
[00:00:00.000 --> 00:00:05.840]   So the tweet was, the parishioner would be like the person involved in the laboratory,
[00:00:05.840 --> 00:00:08.440]   the research institution, and God is the truth.
[00:00:08.440 --> 00:00:14.760]   And if you're not there for the truth, eventually the institution's not going to make it.
[00:00:14.760 --> 00:00:18.760]   It's just going to kind of dissolve, because at the end of the day, if you don't have passion for research,
[00:00:18.760 --> 00:00:22.660]   if you don't have passion for the truth, what's the point?
[00:00:22.660 --> 00:00:26.500]   I don't know, England just seemed like the United States, but whiter.
[00:00:26.500 --> 00:00:30.300]   By the way, can you guess my jati? I mean, I know you're Gujju, right?
[00:00:30.300 --> 00:00:33.900]   Uh-huh. Are you like half Patel, half Bani? I'm just guessing.
[00:00:33.900 --> 00:00:38.300]   Yeah, yeah. Did I guess right? Yeah, yeah, that's exactly right.
[00:00:38.300 --> 00:00:41.900]   How did I do that? I don't even know, man.
[00:00:41.900 --> 00:00:49.500]   All right. Today I have the pleasure of speaking with Razip Khan.
[00:00:49.500 --> 00:00:52.100]   He's one of the top science bloggers in the world.
[00:00:52.100 --> 00:00:56.500]   He writes about genetics, history, and evolution on his blog, Unsupervised Learnings,
[00:00:56.500 --> 00:01:03.100]   and he has a podcast of the same name, and you can find it at razip.substack.com.
[00:01:03.100 --> 00:01:05.700]   So Razip, thanks for coming on the podcast.
[00:01:05.700 --> 00:01:07.700]   That was my pleasure, man. Yeah, yeah.
[00:01:07.700 --> 00:01:12.500]   So can you give my audience a little bit of background about you, how you got into all this stuff?
[00:01:12.500 --> 00:01:21.300]   Yeah, you know, I've always been interested in topics like history, demographics, etc.
[00:01:21.300 --> 00:01:27.300]   And I've also been interested in science. I have a scientific background, scientific training.
[00:01:27.300 --> 00:01:33.900]   And over the last 20 years, genetics has become just a really big deal in terms of,
[00:01:33.900 --> 00:01:39.700]   you know, just as a tool to do various things, whether it's in the biomedical space or historical inference.
[00:01:39.700 --> 00:01:44.500]   And, you know, so obviously I'm interested in demographics, historical inference,
[00:01:44.500 --> 00:01:52.100]   and, you know, genetics is a tool I can use as a geneticist, and so I do.
[00:01:52.100 --> 00:02:00.500]   So, you know, like I was recording right now, I decided to do a bunch of pairwise genetic distances
[00:02:00.500 --> 00:02:05.500]   between populations and stuff just because I could for a post, you know.
[00:02:05.500 --> 00:02:09.500]   So, you know, I do a lot of things by myself.
[00:02:09.500 --> 00:02:15.300]   I replicate what's been done. Yeah. So, I mean, that's a lot of what I do. Yeah.
[00:02:15.300 --> 00:02:19.100]   Okay. Interesting. All right. So I just like to jump into it.
[00:02:19.100 --> 00:02:23.700]   So my first question is, assuming there's no gene editing in the near future,
[00:02:23.700 --> 00:02:26.100]   what is the long-term equilibrium for intelligence look like?
[00:02:26.100 --> 00:02:30.100]   So there's like multiple visions, right? Like one view is like, you know, Charles Marine coming apart.
[00:02:30.100 --> 00:02:33.900]   You have, you know, you have fat tails because there's assortative mating.
[00:02:33.900 --> 00:02:39.900]   Another is there's like a slight dysgenic effect because there's lower fertility among higher intelligence people.
[00:02:39.900 --> 00:02:45.900]   So what does the equilibrium look like if there's no gene editing?
[00:02:45.900 --> 00:02:50.100]   More like the second in terms of not an equilibrium yet.
[00:02:50.100 --> 00:02:57.300]   We're not going to have an equilibrium until, you know, the reproductive differentials equilibrate.
[00:02:57.300 --> 00:03:01.100]   They will at some point, you know, but it could be centuries.
[00:03:01.100 --> 00:03:09.700]   So at this point, people with genes for educational attainment tend to delay childbearing to the point where a lot of them do not have children,
[00:03:09.700 --> 00:03:14.100]   you know, because they invest in educational attainment in the short term.
[00:03:14.100 --> 00:03:21.500]   So, you know, they don't have as many children and their generation times are longer, like the math is difficult there, right?
[00:03:21.500 --> 00:03:28.700]   So right now there's strong negative selection, not strong, there's negative selection on genes for educational attainment.
[00:03:28.700 --> 00:03:32.500]   I mean, everyone who's looked at it says that, at least in the developed world, right?
[00:03:32.500 --> 00:03:34.700]   Yeah. Is this something we can expect in the long term?
[00:03:34.700 --> 00:03:42.700]   Because like naively, I would expect like people who are more intelligent, as long as there's, you know, some sort of selection pressure in the long term.
[00:03:42.700 --> 00:03:58.500]   You know, like there should be selection for, I guess, educated, smart people, because they will just have the cognitive tools to, you know, actually reproduce or, you know, survive and thrive.
[00:03:58.500 --> 00:04:02.200]   Right. As long as like some smart people want to survive and thrive.
[00:04:02.200 --> 00:04:10.300]   Yeah, I mean, survive and thrive is one thing, but have offspring is a different thing.
[00:04:10.300 --> 00:04:16.500]   The incentives in our society are such that a lot of people believe that thriving is being child free.
[00:04:16.500 --> 00:04:24.000]   Or, you know, what usually happens, I think, is people want to establish themselves in their 20s and they don't want to put too much thought.
[00:04:24.000 --> 00:04:27.600]   I mean, at least, you know, professional managerial, you know, college educated people.
[00:04:27.600 --> 00:04:31.500]   And then in their 30s, they start thinking about it. And sometimes people wait too long.
[00:04:31.500 --> 00:04:37.400]   There's fertility issues or they just wait too long. They can't find someone else, you know.
[00:04:37.400 --> 00:04:40.700]   So, yeah, in the long term, obviously, there's a limit.
[00:04:40.700 --> 00:04:46.500]   There's a limiting principle. But you don't need to be that bright to, you know, survive and have a lot of children.
[00:04:46.500 --> 00:04:53.300]   And on the contrary, there's clear evidence that not being bright is good for your reproductive output.
[00:04:53.300 --> 00:05:01.900]   So, you know, yeah. The movie about 2006, the movie is called 2006.
[00:05:01.900 --> 00:05:06.900]   Now, in 2006, Idiocracy. Oh, I see.
[00:05:06.900 --> 00:05:13.300]   What explains the level of endogamy you see in between Indian Jatis, like Indian subcaste?
[00:05:13.300 --> 00:05:15.700]   Because you have a very excellent blog post about this.
[00:05:15.700 --> 00:05:20.800]   And so apparently, as you say, there's genetic evidence that for thousands of years,
[00:05:20.800 --> 00:05:25.200]   these Jatis, like living in the same village, you know, they're not intermarrying.
[00:05:25.200 --> 00:05:30.500]   They're not having kids together. You know, even within the context of like, you know, slaves in America.
[00:05:30.500 --> 00:05:31.800]   This is not a thing that happens, right?
[00:05:31.800 --> 00:05:37.200]   Like you have Sally Hemings, you know, Thomas Jefferson's mistress.
[00:05:37.200 --> 00:05:41.000]   So like, I don't know. How is it possible for thousands of people?
[00:05:41.000 --> 00:05:42.700]   What kind of social structure could lead to this?
[00:05:42.700 --> 00:05:45.100]   Yeah, nobody really knows is a short answer.
[00:05:45.100 --> 00:05:51.700]   So the math is like, you know, there's like there's evidence from from Andhra Pradesh, South India.
[00:05:51.700 --> 00:05:57.500]   David Reich looked at it and it's like if you run the math, it's like, oh, like their endogamy rate is like,
[00:05:57.500 --> 00:06:04.000]   you know, point, you know, it's like ninety nine point five percent per generation, like, you know, super high.
[00:06:04.000 --> 00:06:09.400]   So, I mean, you know, when when I was younger, you know,
[00:06:09.400 --> 00:06:15.500]   the endogamy rate for like black Americans was like ninety five percent, which is high.
[00:06:15.500 --> 00:06:17.600]   And today it's like eighty five percent, you know.
[00:06:17.600 --> 00:06:24.500]   But. You know, five percent is like ten times bigger than what I'm talking about, you know.
[00:06:24.500 --> 00:06:30.000]   Yeah. Like you said, average black Americans, 20 percent European in ancestry, et cetera, et cetera.
[00:06:30.000 --> 00:06:34.300]   So it's just like there's really high barriers in this economy in terms of how it can be maintained.
[00:06:34.300 --> 00:06:38.900]   One thing that I wonder about is infanticide.
[00:06:38.900 --> 00:06:45.300]   Perhaps, I mean, maybe just like the social taboos, reproductive fitness is really low.
[00:06:45.300 --> 00:06:53.200]   I don't know. I it doesn't you know, for humans, it doesn't make sense, but the data is what it is.
[00:06:53.200 --> 00:06:58.400]   Indians just are really good at endogamy for some reason.
[00:06:58.400 --> 00:07:04.800]   You know, whereas in other populations, the general pattern is.
[00:07:04.800 --> 00:07:11.200]   You know, I mean. You see someone, you're like, oh, they're fine, you know?
[00:07:11.200 --> 00:07:17.200]   Yeah. One thing leads to another, you know, I just I guess, you know, this isn't it's not rocket science.
[00:07:17.200 --> 00:07:22.600]   It's universal human nature. Right. But somehow Indians were able to escape that.
[00:07:22.600 --> 00:07:28.600]   No, no, really. No one really knows. I mean, I've had multiple Genesis come up to me and be like, what's up with this?
[00:07:28.600 --> 00:07:30.800]   I don't know. It's like, why are you asking me?
[00:07:30.800 --> 00:07:34.800]   And I'm like, well, I mean, you know, you're brown.
[00:07:34.800 --> 00:07:38.200]   So maybe, you know, it's like they're trying to figure out whether there's a secret sauce here.
[00:07:38.200 --> 00:07:46.900]   Because it's just not it doesn't make any sense for a for a mammal where the males in particular are highly polygynous, you know, and ideal.
[00:07:46.900 --> 00:07:50.600]   So, I mean, are there any hypotheses out there about that?
[00:07:50.600 --> 00:07:53.600]   I try to explain this. Not really.
[00:07:53.600 --> 00:07:58.000]   I mean, you know, it's like, oh, like caste system, blah, blah, blah, you know.
[00:07:58.000 --> 00:08:05.600]   But again, I mean, you know, sexual exploitation of lower caste women by upper caste men has been a thing.
[00:08:05.600 --> 00:08:09.100]   So I do wonder, like, what's up with that? I mean, there are some cases where you see things.
[00:08:09.100 --> 00:08:18.200]   So like on the Nair, the Nair group in Kerala, you know, many of them, many of the women traditionally, not always,
[00:08:18.200 --> 00:08:28.000]   but they have these relationships with Kerala Brahmins, Namatiri Brahmins, that weren't marriages, but there was like consort.
[00:08:28.000 --> 00:08:30.500]   There were consorts and, you know, Kerala.
[00:08:30.500 --> 00:08:32.900]   I think the Nair also did polyandry and other things.
[00:08:32.900 --> 00:08:38.200]   But, you know, you see the Nair, you see like a range of like genetic distance to Namatiri Brahmins.
[00:08:38.200 --> 00:08:41.100]   And that's just because they're biological fathers or fathers.
[00:08:41.100 --> 00:08:46.800]   I mean, I don't know if they call them fathers, but, you know, I mean, are of that group.
[00:08:46.800 --> 00:08:49.200]   So there are exceptions to this.
[00:08:49.200 --> 00:08:56.500]   But, you know, like like you're telling me, yeah, like in general, in general, I can like look at someone.
[00:08:56.500 --> 00:09:05.800]   Most Indians like figure out like what their community, as they say, is from, which is like not like typical, you know, most most of most of the world's not like that.
[00:09:05.800 --> 00:09:11.600]   It's basically like if all of India is populated by people like Ashkenazi Jews.
[00:09:11.600 --> 00:09:16.800]   You know, very, very people because people are like, you know, people are like, oh, well, there's no other example.
[00:09:16.800 --> 00:09:24.400]   And I'm like, actually, there is like Ashkenazi Jews, the Roma who themselves are of part Indian origin.
[00:09:24.400 --> 00:09:31.000]   You know, there's a few examples. The issue is just like having a whole society like this is pretty weird.
[00:09:31.000 --> 00:09:35.800]   Yeah, that is that is the innovation. It's like, oh, let's have a whole society that's stratified.
[00:09:35.800 --> 00:09:44.700]   So, you know. Yeah, it's very interesting. Speaking of Ashkenazi Jews, I thought I thought your I thought your post on that was very interesting.
[00:09:44.700 --> 00:09:52.900]   And, you know, you talk about how, you know, before before Jews were kind of liberated in Europe in the 18th century or sorry, it was the 19th century.
[00:09:52.900 --> 00:09:58.700]   There just wasn't that early 19th. Yep. There wasn't that much Jewish achievement.
[00:09:58.700 --> 00:10:15.700]   And it kind of made me wonder, are there like are there some other population groups in the world today that are that were bottlenecked by a similar process and who are also very endogamous that, you know, once they get to a point of prosperity and and liberation that Jews went through in the 19th century?
[00:10:15.700 --> 00:10:22.700]   You know, in the future, we'll just be talking about how they're outputting a greater portion of the world's cultural heritage.
[00:10:22.700 --> 00:10:30.900]   Like, you know, like parts of the world that are just going through industrialization now and might have like small populations like Ashkenazi Jews.
[00:10:30.900 --> 00:10:35.400]   Right. Is there potential for like a new Ashkenazi Jew in the next century or two?
[00:10:35.400 --> 00:10:40.300]   I guess what I'm asking. So what you need is Ashkenazi Jews are highly endogamous were.
[00:10:40.300 --> 00:10:49.900]   Yeah. And, you know, they they emerged in the context of Central East Europe as a middlemen minority.
[00:10:49.900 --> 00:11:02.000]   You know what the whole thing is like, you know, Haredi Jews dress like Polish nobles, you know, because they worked for these Polish nobles as factors and tax collectors and administrators and whatnot.
[00:11:02.000 --> 00:11:17.300]   So I guess you have to look for something like that. Well, you know, this is a totally equivalent because endogamy is not a big issue here, but like Fujianese, you know, Chinese from Fujian have traditionally done better on examinations going back a thousand years.
[00:11:17.300 --> 00:11:23.700]   Go back to the Song Dynasty. So there were like affirmative action quotas on people from Fujian.
[00:11:23.700 --> 00:11:35.300]   So if you look at like who some Fujian people, basically a lot of the rich Chinese, not all, obviously, but, you know, traditionally, like in Hong Kong,
[00:11:35.300 --> 00:11:51.400]   the elite families are, you know, Shanghais, some Fujianese. And so like these coastal Southeast coastal people in China have traditionally been extremely enterprising and central government in China has often clamped down on them.
[00:11:51.400 --> 00:11:59.400]   Obviously, this government is not the modern economy cannot. And so I think these these populations might come into their own, you know?
[00:11:59.400 --> 00:12:18.400]   Although, didn't you write somewhere else that the Chinese government for a long time, like not just, you know, the CCP, but like, I guess China, you know, in Chinese history, there's been many instances of the government trying to get rid of like genetically distinct groups by, I guess, breeding them into the larger stock.
[00:12:18.400 --> 00:12:25.400]   So potentially that reduces the odds of some, you know, outlier endogamous group.
[00:12:25.400 --> 00:12:48.400]   Yeah, so in China, the only equivalent, like, like you see in Ashkenazi Jews, the Hakka in South China, and the Hakka are descended from northern Chinese migrants. And so they speak like a dialect of Mandarin, northern Chinese, you know, dialect in the South, like in Guangdong, where the Cantonese and Taishanese are, and, you know, they still kind of tend to intermarry.
[00:12:48.400 --> 00:13:14.400]   I mean, they're, they're spatially isolated. But, you know, again, like, the Hakka, the Hakka are not like Ashkenazi Jews and having an ideological reason for their endogamy, you know, Chinese lineages, some sound like Indian lineages, but are paternal, you know, so your identity and who you are, your clan is determined by who your father is.
[00:13:14.400 --> 00:13:28.400]   So, you know, that's, I mean, you might have a lower status if your mother is an ethnic minority, like Zhuang or Uyghur or something like that, but, you know, informally, but still officially you're part of the clan. And so that's, I think, how assimilation has happened.
[00:13:28.400 --> 00:13:47.400]   Genetically, people in Guangdong, like the Cantonese, like they have a minority of, you know, indigenous or South ethnic group, you know, ancestry. Some of their practices are clearly not Han Chinese, especially like certain marriage practices, certain things that women do.
[00:13:47.400 --> 00:14:08.400]   And most of the gene flow is probably from females, from non-Han that were assimilated in the area. So, yeah, the Han identity is very assimilative. North of the Yangtze, pretty much every Han sample that I have has a little bit of West Eurasian ancestry. South of the Yangtze, none of them have it.
[00:14:08.400 --> 00:14:30.400]   And so I think most of that West Eurasian is probably assimilated Mongol and other things like that. Because the Mongols are about 10% assimilated Mongol. Yeah, I think that's what it is. Because the Mongols are about 10% West Eurasian. And the Tao for me is, you know, like about 1% of Northern Chinese Han men have R1A, maybe 0.5%.
[00:14:30.400 --> 00:14:48.400]   It's not super high, but R1A is, you know, mostly found in Iranians and Slavs. And Mongols have it. They have the Indo-Iranian version because they assimilated Scythians and Sarmatians and other Iranian steppe people. So I think that's probably where that comes into the Chinese.
[00:14:48.400 --> 00:15:06.400]   And, you know, you can go back to the Toba Turks and other groups after the fall of the Han dynasty, you know, 1500 or actually 1700 years ago, 1700-1800 years ago. I mean, I think that's when they started introducing that genetic element to Northern China. North of the Yangtze.
[00:15:06.400 --> 00:15:24.400]   Uh-huh. Interesting. By the way, going back to India, there's been a lot of talk about how a lot of American CEOs of big tech companies are Indians and specifically from Brahmin jatis. Is there some particular reason that that seems to be happening?
[00:15:24.400 --> 00:15:27.400]   Wait, what seems to happen? Can you repeat that again?
[00:15:27.400 --> 00:15:35.400]   Why are a lot of big tech CEOs Indians and specifically a lot of them from Brahmins?
[00:15:35.400 --> 00:15:47.400]   Yeah, well, the guy from TikTok's not. He's Baniya. I mean, I think the Indian explanation, which you probably know, is that Brahmins are illiterate. They're symbolic manipulators.
[00:15:47.400 --> 00:15:59.400]   And so obviously, you know, if you were at Microsoft or Google, and they tend to be particularly South Indian Brahmins, actually, as opposed to North Indian Brahmins. There aren't that many of those.
[00:15:59.400 --> 00:16:11.400]   And this goes back to the colonial period, actually. South Indian Brahmins would migrate to the cities in North India to work in the Indian civil service. The reverse would not happen.
[00:16:11.400 --> 00:16:24.400]   So, you know, this is a longstanding issue or issue or phenomenon of South Indian English speaking Brahmin elites in particular, availing themselves of technology, higher education.
[00:16:24.400 --> 00:16:36.400]   You know, Tamil Brahmins, for example, are very well represented in engineering and software. And that's obviously the pipeline that Indian Americans are going in into as CEOs, highly overrepresented.
[00:16:36.400 --> 00:16:48.400]   You know, so I think, you know, the CEO of Microsoft and CEO of Google are both South Indian Brahmins. They're both Telugu Brahmins.
[00:16:48.400 --> 00:16:57.400]   There's some there's some, like, debates, I think, whether whether the guy at Microsoft is a Brahmin or not. I don't know.
[00:16:57.400 --> 00:17:02.400]   I can't tell these sorts of things. I mean, I can, but not like I don't have a good instinct. You know what I'm saying?
[00:17:02.400 --> 00:17:09.400]   But anyway, yeah. So I think Brahmins are, you know, like Ashkenazi Jews. You know, they analogize themselves, particularly South Indian Brahmins.
[00:17:09.400 --> 00:17:18.400]   I think we do have to distinguish that because, you know, one of you heard about a Guju Brahmin or a UP Brahmin, you know, it's like those people just stay where they are.
[00:17:18.400 --> 00:17:30.400]   You know, they're not, you know, they're local landed elites, but they're not like well known outside of the Indian subcontinent or, you know, to be honest, within the Indian subcontinent from what I can see.
[00:17:30.400 --> 00:17:36.400]   Well, what explains that? I mean, so I read I read a part of Satya, he's the Microsoft CEO, Satya Nadella.
[00:17:36.400 --> 00:17:42.400]   And he he talked about how his parents were like these Marxist philosophers, you know, Brahmin philosophers.
[00:17:42.400 --> 00:17:54.400]   But anyway, so what explains what explains why these North Indians were, I guess, complacent and these South Indians were availing themselves of, you know, the resources?
[00:17:54.400 --> 00:18:01.400]   Yeah, I mean, so I think UP and Bihar in particular, the elites, they tend to be they tend to like to be big fish in small ponds.
[00:18:01.400 --> 00:18:07.400]   So it's not like there is like Rajput Thakurs all over the world either from UP, right? Punjab is different.
[00:18:07.400 --> 00:18:17.400]   There's a lot of Punjabis all over the world of various groups, you know, like Jats, agriculturalists, farmers and Central Valley countries all over the place, you know.
[00:18:17.400 --> 00:18:24.400]   In contrast, in UP, Bihar, these North Indian states, there's just like there's less dynamism, less cultural dynamism.
[00:18:24.400 --> 00:18:33.400]   The behavioral economic literature shows like a really strong preference for zero sum gains, wanting to be like at the at the pinnacle of the local.
[00:18:33.400 --> 00:18:46.400]   This is not always true, you know, but they prefer to be at the pinnacle of the local power structure rather than taking a risk going somewhere else where they might not be at the peak.
[00:18:46.400 --> 00:18:51.400]   You know, they might be way more well off in the aggregate, but, you know, they wouldn't be at the peak.
[00:18:51.400 --> 00:18:58.400]   And so, for example, someone like like Chandra Sekhar, Chandra Sekhar Limit, he's a Tamil Brahmin by background.
[00:18:58.400 --> 00:19:05.400]   Obviously, he settled in the United States eventually. But, you know, I think he was born in Lahore.
[00:19:05.400 --> 00:19:13.400]   His dad was working for the Indian Civil Service. And, you know, if you read his biography, they experience like some kind of discrimination,
[00:19:13.400 --> 00:19:21.400]   you know, prejudice being South Indians in the north. And then Chandra Sekhar went to the United States and during the time of segregation,
[00:19:21.400 --> 00:19:28.400]   you know, and they tried to put him in the blacks only area in St. Louis, like for some sports game.
[00:19:28.400 --> 00:19:35.400]   There's like all sorts of things that happen, you know, and then he experienced prejudice at the hands of I think Arthur Eddington in particular,
[00:19:35.400 --> 00:19:40.400]   was was pretty prejudiced against Indians and their ability to contribute to physics.
[00:19:40.400 --> 00:19:46.400]   So is that the guy who approved Einstein's or the approved relativity?
[00:19:46.400 --> 00:19:51.400]   Right. Yeah. Yeah. I think empirically. Yeah. So, yeah. But I mean, at least that's Chandra Sekhar's take.
[00:19:51.400 --> 00:19:55.400]   Like, you know, we don't know if it's like 100 percent true that anything was really, you know, who knows?
[00:19:55.400 --> 00:19:59.400]   Because sometimes it turns out that there's personal beefs going on.
[00:19:59.400 --> 00:20:02.400]   I don't think anything ever told his side. He died a long time ago.
[00:20:02.400 --> 00:20:10.400]   Chandra Sekhar lived until like it wasn't like 10 years ago. I think he died 10 years ago.
[00:20:10.400 --> 00:20:21.400]   Yeah, I think. Oh, no, not 10 years ago, like 1995. So a while ago.
[00:20:21.400 --> 00:20:28.400]   Yeah. Yeah. Ninety five. So 25 years ago. But yeah, I mean, he was still I mean, so he was still around when I was in high school.
[00:20:28.400 --> 00:20:34.400]   I remember someone did a report on him and, you know, it's hard to find information back then, but, you know, you could.
[00:20:34.400 --> 00:20:40.400]   He was still around giving quotes. Yeah. Does the work you do involve a lot of traveling?
[00:20:40.400 --> 00:20:47.400]   I mean, you're writing about all these different areas of the world and, you know, their anthropological and genetic history.
[00:20:47.400 --> 00:20:56.400]   I wonder if that if that requires you or if it helps you to, like, just travel to all these places or are you able to do that just from just from here?
[00:20:56.400 --> 00:21:00.400]   I guess I do most of the United States. I mean, I've traveled a little bit, but not too much. I'm not a big traveler internationally.
[00:21:00.400 --> 00:21:05.400]   I'm not a you know, I'm like, yeah, I don't I don't do that.
[00:21:05.400 --> 00:21:14.400]   Some people do. You know, Spencer Wells, who I worked with, former boss, he's traveled all over the world and, you know, with National Geographic and stuff.
[00:21:14.400 --> 00:21:22.400]   That adds a lot of local color in terms of things you see, things, you know, whenever we talked about the Eurasian steppe, he's been there a lot.
[00:21:22.400 --> 00:21:27.400]   So, you know, he can add a lot to that in a few places.
[00:21:27.400 --> 00:21:32.400]   If if I mean, I don't know if you read the Finland series. I've been to Finland, you know.
[00:21:32.400 --> 00:21:38.400]   So there are certain things that I know about Finland. I've been to Finland, been to Italy.
[00:21:38.400 --> 00:21:47.400]   I don't know. England just seemed like the United States, but whiter. So, I mean, there was there wasn't like, oh, like, whoa, like I really understand the British people now.
[00:21:47.400 --> 00:21:52.400]   I'm just like, OK, they're drinking a lot. I think I I think I am not surprised by that.
[00:21:52.400 --> 00:21:57.400]   Just judging by, you know, like all those British sitcoms and TV shows where they're like drinking in the morning.
[00:21:57.400 --> 00:22:01.400]   I get it now, you know, so. Huh. Yeah.
[00:22:01.400 --> 00:22:07.400]   So that's one thing I was wondering is knowing all that you know about the history of these different places.
[00:22:07.400 --> 00:22:13.400]   Do you feel that when you visit a place or when you learn more about a place, you're like, oh, I what they're doing today.
[00:22:13.400 --> 00:22:21.400]   That makes sense to me, like why why it is the way it is, given what I know about, you know, the the roots of what happened in that place thousands of years ago.
[00:22:21.400 --> 00:22:25.400]   So does it feel that it just kind of random? No, it's not random. Sometimes I do.
[00:22:25.400 --> 00:22:29.400]   I mean, there's sometimes where it's like, you know, someone does this or their family does this.
[00:22:29.400 --> 00:22:38.400]   And I'm like, oh, it's because of this. They're like, well, oh, you know, like they don't because you don't know the antecedents of, you know, we don't know the antecedents of everything we do.
[00:22:38.400 --> 00:22:45.400]   And so a lot of times I do. And, you know, I mean, the thing was, like, you know, for example, like Americans are really ignorant in geography.
[00:22:45.400 --> 00:22:51.400]   So. So 2019, I'm a scientific conference in the American Society of Human Genetics.
[00:22:51.400 --> 00:22:57.400]   You know, I'm meeting these people, you know, you're networking, you're meeting. So I met this Chinese geneticist.
[00:22:57.400 --> 00:23:08.400]   She's officially in grad school in the United States. I was like, oh, where are you from? She was just like, oh, I'm from a city between Beijing and Guangdong, like exactly in the middle.
[00:23:08.400 --> 00:23:14.400]   OK, so here's my my train of thought. So I immediately blurred out Wuhan.
[00:23:14.400 --> 00:23:20.400]   She was like, well, how did you guess that? You know, it's a one. She was shocked that I knew what Wuhan that I knew of Wuhan.
[00:23:20.400 --> 00:23:27.400]   Right. Because most don't to Shanghai's the middle. But if she was from Shanghai, she would say Shanghai.
[00:23:27.400 --> 00:23:37.400]   So it had to be another city. I happen to know that there's a high speed line rail line between Beijing and Guangdong between Guangzhou.
[00:23:37.400 --> 00:23:43.400]   And it's its middle point is Wuhan. So I knew what was exactly in the middle. Right.
[00:23:43.400 --> 00:23:50.400]   And so I was like, you know, these are the sort of things. I mean, it's like, oh, like an American. It's like super amazing because we don't know any geography.
[00:23:50.400 --> 00:23:54.400]   Like her friend was like, you know, I was like looking at him. I was like, oh, you're pretty tall.
[00:23:54.400 --> 00:23:58.400]   Like, you know, are you from the Chinese? Like, yeah, yeah, I'm from the east. I'm like Shandong.
[00:23:58.400 --> 00:24:02.400]   And then he was just like, well, how do you know that? I'm like, what's the easternmost province?
[00:24:02.400 --> 00:24:10.400]   I mean, I mean, it's just an educated. You know, I'm saying if someone's like someone's like has like they're talking about chowder.
[00:24:10.400 --> 00:24:14.400]   You know, a drink a tonic and it's like wicked smart. And I'm just like, are you from Boston?
[00:24:14.400 --> 00:24:19.400]   Like, whoa, that's wicked crazy. How'd you know that?
[00:24:19.400 --> 00:24:30.400]   And I'm just like, you know, but it's because because I know like everybody outside of America rightfully assumes that Americans do not know anything about where they're from.
[00:24:30.400 --> 00:24:38.400]   Like nothing. Yeah. And so it's just like an incredible party trick with an American accent to be like, you are from Praha.
[00:24:38.400 --> 00:24:45.400]   You know, what, you know, but by the way, can you guess my Jati?
[00:24:45.400 --> 00:24:51.400]   I mean, I know you do you right now, but I couldn't I didn't guess it.
[00:24:51.400 --> 00:24:55.400]   I don't know by the name. I mean, you look are you like half Patel, half Bonnie?
[00:24:55.400 --> 00:24:59.400]   I'm just guessing. Yeah. Yeah. Did I guess right? Yeah.
[00:24:59.400 --> 00:25:04.400]   Yeah, that's exactly right. I don't even know.
[00:25:04.400 --> 00:25:10.400]   But my mom, my dad, my dad is Patel and my mom is Maria. Yeah.
[00:25:10.400 --> 00:25:16.400]   OK. All right. That's exactly correct. OK, guys, this is not a conspiracy between us.
[00:25:16.400 --> 00:25:21.400]   Like, he literally just I didn't know that question was going to be asked and I actually didn't have any.
[00:25:21.400 --> 00:25:26.400]   I just like looked at him and I was just like, this is my guess. This is my educated guess.
[00:25:26.400 --> 00:25:31.400]   Right. Right. So that alone should justify your subscription. Yeah.
[00:25:31.400 --> 00:25:39.400]   OK, so I've had this question about, you know, the greater male variance theory for a long time, which is that.
[00:25:39.400 --> 00:25:44.400]   So basically the idea is men produce more geniuses, but also more idiots.
[00:25:44.400 --> 00:25:47.400]   So I've always wondered, like, why is that the case?
[00:25:47.400 --> 00:25:53.400]   Because so is there some there must be some mechanism that, like, just increases the variation,
[00:25:53.400 --> 00:25:59.400]   like, you know, gives you a higher odds of being a genius, but at the cost of higher odds of also being an idiot.
[00:25:59.400 --> 00:26:02.400]   That is, like, more activated than men. Right. Like, why?
[00:26:02.400 --> 00:26:09.400]   What is the tradeoff that involves if you activate the tradeoff, you might have a higher odds of becoming a genius,
[00:26:09.400 --> 00:26:13.400]   but also a higher odds of becoming an idiot. Yeah.
[00:26:13.400 --> 00:26:16.400]   So, I mean, you got to, like, be a little bit of a molecular mechanism.
[00:26:16.400 --> 00:26:21.400]   I haven't looked at in detail in a while. But one of the hypotheses is, for example, we have one X chromosome.
[00:26:21.400 --> 00:26:31.400]   So with X chromosomes, normally with women, there's inactivation of one X chromosome randomly in the cell.
[00:26:31.400 --> 00:26:35.400]   Right. Or in the tissue or whatever, in the tissue, in the bar bodies. Right.
[00:26:35.400 --> 00:26:43.400]   So every cell has an X chromosome and they tend to clump where it's like they're these, like, bar bodies, like X chromosomes that are inactivated.
[00:26:43.400 --> 00:26:51.400]   They're not expressing, they're like, like, euchromatic. People are going to be like, oh, my God, like, he's getting euchromatic and heterochromatic mixed.
[00:26:51.400 --> 00:26:57.400]   I was going to mixed up. OK, I'm not a molecular geneticist. But anyway, so one of the X chromosomes has to inactivate and that's random.
[00:26:57.400 --> 00:27:04.400]   OK, so let's say a woman has a major mutation in the X chromosome.
[00:27:04.400 --> 00:27:12.400]   You know that, like, she has another copy. Right. But, you know, it could be that in that cell there's a malfunction because the other copy is the one that's inactivated.
[00:27:12.400 --> 00:27:18.400]   The one that's functional. Now, if you're a man, there's no choice. It's only one X chromosome.
[00:27:18.400 --> 00:27:23.400]   Mm hmm. Right. So obviously that's limiting the degrees of freedom. Right.
[00:27:23.400 --> 00:27:30.400]   And so if that's a good copy, if it's got some good stuff going on there, well, that's good.
[00:27:30.400 --> 00:27:34.400]   But if it's got bad stuff going on there, well, you're screwed.
[00:27:34.400 --> 00:27:48.400]   So, I mean, the easiest explanation for why at the low end men have problems is probably, OK, well, we have a load of deleterious alleles on our X chromosomes that are not masked because we only have one of them.
[00:27:48.400 --> 00:27:53.400]   Right. So that's one thing in terms of we are the heterogametic sex.
[00:27:53.400 --> 00:27:59.400]   So we're the sex that has like so in birds, I think it's the opposite. I know it is the opposite.
[00:27:59.400 --> 00:28:04.400]   Females are the heterogametic sex. The sex determination happens through them.
[00:28:04.400 --> 00:28:12.400]   And males have like the equivalent of two Xs. ZW. And I think the males are ZZ.
[00:28:12.400 --> 00:28:20.400]   Anyway, so that's one issue. And when you think developmentally, you know, we all start out as females.
[00:28:20.400 --> 00:28:27.400]   The female is the template. And so men have to go through extra processes.
[00:28:27.400 --> 00:28:37.400]   So the end of life is the opposite. Like women go through menopause, which is a proactive physiological shutdown, not just like a long, slow decline like we go through in our reproductive processes.
[00:28:37.400 --> 00:28:43.400]   But at the beginning of life, I think it's the end of the first trimester, we go through this testosterone burst. Right.
[00:28:43.400 --> 00:28:53.400]   SRY, the sex, you know, the sex chromosome, you know, the sex determining region kicks in and we become male, we become masculinized.
[00:28:53.400 --> 00:28:59.400]   So when you have a situation, when you have extra developmental steps, hey, guess what? That can mess things up.
[00:28:59.400 --> 00:29:08.400]   OK, so we also have higher testosterone. Testosterone is antagonistic to immune response.
[00:29:08.400 --> 00:29:20.400]   So there are more males born than females, probably because the Y chromosome of the male, the sperm of a male Y chromosome is lighter than when it has an X chromosome.
[00:29:20.400 --> 00:29:26.400]   OK, so probably male sperm, quote, male sperm have an advantage in speed.
[00:29:26.400 --> 00:29:32.400]   There's about one hundred and four, one hundred and five males born for one hundred females.
[00:29:32.400 --> 00:29:40.400]   But in utero, there's a strong suspicion from people that have done like sampling on miscarriage, miscarried fetuses, that males are overrepresented.
[00:29:40.400 --> 00:29:46.400]   So we actually start out with a bigger advantage and we're already culled because of our genetic abnormality.
[00:29:46.400 --> 00:29:51.400]   Something on the order of like 10 to 50 percent of fetuses miscarry.
[00:29:51.400 --> 00:29:56.400]   It's still kind of not clear with the total numbers. It's really hard to track miscarriages early on.
[00:29:56.400 --> 00:30:04.400]   Right. And so so that that explains the downward, the low end in terms of why there might be more male, quote, geniuses.
[00:30:04.400 --> 00:30:10.400]   I think the way you might want to look at it is. There's really no reproductive value at the high end.
[00:30:10.400 --> 00:30:17.400]   It's just kind of like a freak thing. And if we're less developmentally stable.
[00:30:17.400 --> 00:30:21.400]   We can go off target a little bit more is the way I think of it.
[00:30:21.400 --> 00:30:24.400]   There's no reason you need you need to have your IQ be like once.
[00:30:24.400 --> 00:30:29.400]   There's no reason you need to be able to do algebraic topology easily.
[00:30:29.400 --> 00:30:32.400]   Yeah, there's no reason. There is some evidence.
[00:30:32.400 --> 00:30:43.400]   There is some evidence in the genomic literature now with the most recent work that there is some enrichment for schizophrenia and other things with some of these educational attainment genes.
[00:30:43.400 --> 00:30:46.400]   Like some there's some evidence. Yeah.
[00:30:46.400 --> 00:30:54.400]   But is there some reason in the ancestral environment why I don't know, having a brain capable of algebraic topology would be advantageous?
[00:30:54.400 --> 00:30:57.400]   Like, is there something that a human would need to do?
[00:30:57.400 --> 00:31:02.400]   Yeah. And then a separate question, I guess you can answer at the same time.
[00:31:02.400 --> 00:31:09.400]   Do we have an explanation for why brain size decreased by like, what was it was 10 percent or something like that?
[00:31:09.400 --> 00:31:12.400]   We're just smaller. So our bodies got smaller.
[00:31:12.400 --> 00:31:18.400]   Like when it got warmer, we got smaller, but also agriculture seems to have given us really, really weak bones.
[00:31:18.400 --> 00:31:23.400]   We got more fragile, more gracile. We shrunk some with agriculture.
[00:31:23.400 --> 00:31:33.400]   And so that natural process of that is smaller brains. I bet you average nutrition probably decreased some in terms of like quality as opposed to reliability and consistency.
[00:31:33.400 --> 00:31:39.400]   And that probably meant that, you know, smaller brain sizes are more optimal to survive through the ephemism.
[00:31:39.400 --> 00:31:45.400]   We know smaller body sizes are for sure. We know small, smaller body sizes are.
[00:31:45.400 --> 00:31:51.400]   There's been a lot of negative selection in Southern Europe and in Asia for small body size.
[00:31:51.400 --> 00:32:00.400]   And last I checked, it's pretty clear that people in the eastern part of the Indian subcontinent are shorter genetically.
[00:32:00.400 --> 00:32:05.400]   And some of it is like East Asian ancestry. But I mean, clearly, like Bengalis are just a short people.
[00:32:05.400 --> 00:32:14.400]   You know, if you just like me, like people from Bangladesh or West Bengal in the West and like they're chubby AF because they get a lot to eat.
[00:32:14.400 --> 00:32:19.400]   So it's not like genetics, you know, like I used to when I was not or not genetic, but like environment.
[00:32:19.400 --> 00:32:29.400]   I was like when I was little, people would say, like, oh, well, you know, people, you know, your parents, because my dad's short, your parents didn't eat a lot of meat.
[00:32:29.400 --> 00:32:36.400]   It's like, OK, but now that I know about genetics, nutrition and class background, I'm like, no, like my family.
[00:32:36.400 --> 00:32:40.400]   Like, you know, like people were like obese in my family, like they had enough to eat.
[00:32:40.400 --> 00:32:46.400]   They didn't suffer from the Bengal famine. And also my family's Muslim, so they eat beef and they got protein.
[00:32:46.400 --> 00:32:50.400]   No, they're just short because genetics, you know, and why?
[00:32:50.400 --> 00:32:57.400]   Well, we know the Bangla, the Bengali populations, Bangladesh, they have cholera resistance, obviously, because, you know, the issues with flooding and water.
[00:32:57.400 --> 00:33:01.400]   That's different than others. Indians have continental populations.
[00:33:01.400 --> 00:33:07.400]   There's some reasons why they're small, too. I don't know why, why Bengalis are small, but that's obviously true.
[00:33:07.400 --> 00:33:10.400]   So sorry, what's the link between cholera and height or cholera?
[00:33:10.400 --> 00:33:17.400]   There's no link. I'm just saying that there's been studies in selection. There's selection for resistance to cholera in Bengal.
[00:33:17.400 --> 00:33:26.400]   One of the canonical examples, like the Vibrio, whatever, like that, the microbe, there's clearly strong selection because of the cholera over the last couple of centuries.
[00:33:26.400 --> 00:33:35.400]   Yeah. And then what do you make of the self-domestication hypothesis, the idea that there's like a there's a set of genes that I guess they happen together.
[00:33:35.400 --> 00:33:46.400]   They're associated in many different mammals with domestication, you know, like smaller jaws, males and females looking similar and then, you know, less intelligence.
[00:33:46.400 --> 00:33:51.400]   There's a cluster of other things. But basically, the idea is that the same thing happened to humans during the agricultural revolution.
[00:33:51.400 --> 00:34:00.400]   What do you make of that? I mean, I think it's a plausible I think it hasn't really.
[00:34:00.400 --> 00:34:11.400]   It hasn't really panned out in terms of the genomics, let's put it that way, because this hypothesis has been around for a generation and it hasn't really panned out in terms of the genomics.
[00:34:11.400 --> 00:34:28.400]   So I guess what I would say is like it could be that and let's see.
[00:34:28.400 --> 00:34:34.400]   Let's see. What I would say is it could be humans are special in some distinct ways.
[00:34:34.400 --> 00:34:45.400]   OK, because because like it's been studied in foxes and other organisms extensively, but it hasn't been.
[00:34:45.400 --> 00:34:50.400]   And dogs, you know, and there's a spackling and some of the things let's talk about what you're talking about.
[00:34:50.400 --> 00:34:59.400]   Like there's certain like spackling patterns, floppy ears, just really, really common patterns across mammals because the same developmental pathways are tuned.
[00:34:59.400 --> 00:35:03.400]   We obviously don't have floppy ears and we don't show highball patterning.
[00:35:03.400 --> 00:35:06.400]   So I think I think it's a great idea.
[00:35:06.400 --> 00:35:12.400]   I just don't know for sure like how it how it operationalizes in humans.
[00:35:12.400 --> 00:35:15.400]   Let's put it that way. I mean, it's been it's been a generation.
[00:35:15.400 --> 00:35:21.400]   We have genomic resources and it hasn't really I haven't seen too much advancement in that direction.
[00:35:21.400 --> 00:35:31.400]   Gotcha. OK, so this morning you tweeted, if everyone who attends a church thinks that the point of church is to bask in the work of fellow parishioners rather than worshipping God, the church won't last long.
[00:35:31.400 --> 00:35:37.400]   And then you follow that up with a tweet that said in parentheses, I'm not talking about religion.
[00:35:37.400 --> 00:35:41.400]   So I generally don't know what you're what you're referring to that tweet.
[00:35:41.400 --> 00:35:45.400]   I don't know if you meant to keep it unsaid, but I was just kind of curious.
[00:35:45.400 --> 00:35:48.400]   Yeah, I was being stressed and I was having a discussion.
[00:35:48.400 --> 00:35:52.400]   I'll tell you what it was having a discussion with a scientist friend of mine.
[00:35:52.400 --> 00:35:55.400]   We're talking about collegiality and truth.
[00:35:55.400 --> 00:36:08.400]   And, you know, it's like sometimes sometimes it seems like in science today, and it's just not just online, but just in general, you know, like the community.
[00:36:08.400 --> 00:36:14.400]   And just like, you know, comfort, I guess, I don't know, is like prioritized.
[00:36:14.400 --> 00:36:22.400]   And a lot of it's fake, you know, science is like it is like is like.
[00:36:22.400 --> 00:36:27.400]   It's like it's like management consulting, it's up or out, you know.
[00:36:27.400 --> 00:36:32.400]   So all this stuff about, like, support and it's just fake.
[00:36:32.400 --> 00:36:39.400]   Right. One percent of one percent of incoming graduate students will have like a tenured R1 research.
[00:36:39.400 --> 00:36:48.400]   One top research, one position like, you know, relevant one. Right. So all this stuff about how we're here to support, you know, like we're here to separate the wheat from the chaff.
[00:36:48.400 --> 00:36:59.400]   So that's kind of like fake right there. But, you know, there's a lot of talk about, you know, just kind of community and not making people uncomfortable and inclusion and equity.
[00:36:59.400 --> 00:37:03.400]   And I'm just like science is like super inequitous.
[00:37:03.400 --> 00:37:09.400]   Right. It's not like it's not like it's not like pediatrics or something. Right.
[00:37:09.400 --> 00:37:16.400]   Yeah, there are superstar pediatricians, but look, the average pediatrician makes a difference and pediatricians, a pediatrician and science.
[00:37:16.400 --> 00:37:22.400]   You have like a few superstars who I mean, like it's hyper Pareto principle. Right.
[00:37:22.400 --> 00:37:27.400]   It's not like the twenty eighty. It's like, you know, the five to ninety five, you know.
[00:37:27.400 --> 00:37:33.400]   So anyway, it's just like a little strange there. And, you know, the whole idea is like truth.
[00:37:33.400 --> 00:37:39.400]   And, you know, I've you know, I'm I I've just seen things where it's like, oh, people like that's uncomfortable.
[00:37:39.400 --> 00:37:43.400]   You can't say that. That makes people like one.
[00:37:43.400 --> 00:37:48.400]   It's a very, very winnowing profession. They haven't changed that no matter what you say.
[00:37:48.400 --> 00:37:53.400]   Like you can repeat these mantras, but it doesn't matter. Like it's a winnowing profession.
[00:37:53.400 --> 00:38:05.400]   And the other thing is, you know, like if a science was here for the truth, like if that's not the primary focus, if you're here for like quality of life.
[00:38:05.400 --> 00:38:10.400]   You know, I don't know. Why are we funding it then?
[00:38:10.400 --> 00:38:15.400]   You know, I don't know. Yeah. Yeah. Yeah. Yeah. It makes sense.
[00:38:15.400 --> 00:38:24.400]   It's not only unequal in the sense that there's like a lot of curve where a small minority of scientists make the largest, much larger portion of the total contribution to science, but it's also unequal.
[00:38:24.400 --> 00:38:27.400]   I think it was you who said this or wrote about this. I don't know where I saw this.
[00:38:27.400 --> 00:38:37.400]   But professors, the career professorship has the highest heredity in the sense that the highest correlation between the parent being a professor and child being a professor.
[00:38:37.400 --> 00:38:42.400]   Yeah. Yeah. Yeah. I didn't. I mean, I probably retweeted it.
[00:38:42.400 --> 00:38:46.400]   I mean, that's not that's obvious. I didn't talk about it because I was like, OK, everyone knows this.
[00:38:46.400 --> 00:38:49.400]   Everybody in science knows this. My dad was a professor, by the way.
[00:38:49.400 --> 00:38:56.400]   But anyway, I'm not. I'm just saying that like everybody who. So one thing is I have a friend.
[00:38:56.400 --> 00:39:07.400]   You know, so people in science, people who go into graduate school in science, academic science, they, you know, read the first generation as a thing is because it's so skewed toward professional managerial class people in general.
[00:39:07.400 --> 00:39:13.400]   It's very, very class biased, you know, so. Yeah.
[00:39:13.400 --> 00:39:21.400]   Anyway, it's very class biased, but even people who come from professional managerial backgrounds, if they didn't come from academia, they don't always know everything.
[00:39:21.400 --> 00:39:35.400]   So I have a friend who came from like very upper upper middle class background and, you know, he admitted like, yeah, like he had to learn some things in terms of what you do to make it in science.
[00:39:35.400 --> 00:39:42.400]   Because, you know, his I think his dad's a lawyer. I think his dad's a lawyer. But, you know, so, you know, I mean, it's the same thing in medicine.
[00:39:42.400 --> 00:39:55.400]   Like, I have a friend who's in medicine. I think his parents are engineers. And, you know, he's he said that they told him for medical school interviews, it's going to count against you that your parents aren't doctors because they just assume you don't know as much about like how to make it the profession.
[00:39:55.400 --> 00:40:03.400]   Right. And so there's tacit stuff that gets passed on. Like, I have a friend. He's he is a research one professor. He does have tenure. He has succeeded.
[00:40:03.400 --> 00:40:17.400]   But he comes from a very working class background. And by the time he got to the postdoctoral fellow stage, which is after Ph.D., before professor, he was like talking to people and they were talking about their choices that they made as undergrads and blah, blah, blah.
[00:40:17.400 --> 00:40:27.400]   And he just thought to himself and, you know, he's. He's in like in his field, he's in a top 10 institution, not like Harvard, but he's in a top.
[00:40:27.400 --> 00:40:37.400]   So he's doing really well. So I don't want to under undersell how much he's accomplished. But, you know, he literally told me he was like, you know, I just thought to myself, I was like, I never had a chance.
[00:40:37.400 --> 00:40:47.400]   You know, because he just I mean, he did well, obviously, but like he never planned this way. He never optimized his own life because he just he didn't have that background, you know?
[00:40:47.400 --> 00:41:02.400]   Yeah. I thought he never had a chance. So, you know, that is what on the margin. It makes a big difference. I think this is why there's a lot of virtue signaling from some people who, you know, like some of the most.
[00:41:02.400 --> 00:41:10.400]   There's a there's a professor I can name who it is explicitly, but people who follow academic Twitter probably know who I'm talking about.
[00:41:10.400 --> 00:41:19.400]   They work in biomedical science and, you know, they do periodic virtual signaling, just like standard progressive stuff.
[00:41:19.400 --> 00:41:25.400]   But like I think their uncle was like a Nobel Prize winner and they did research in their uncle's lab when they were in high school.
[00:41:25.400 --> 00:41:37.400]   So, I mean, this is a person who got a huge leg up by family. I mean, they're smart. OK, but OK, like they knew exactly how to succeed in science because they had all the family connections in the world.
[00:41:37.400 --> 00:41:46.400]   And, you know, so now I think they overcompensate. I think she overcompensates, to be honest. That's what everyone assumes privately. That's what they say.
[00:41:46.400 --> 00:41:57.400]   And I think it's probably true. You know, there's other people like that where, you know, online there's a couple there's there's there's one guy online who's super, super progressive.
[00:41:57.400 --> 00:42:06.400]   But a friend of mine told me he's like a notorious dick. To his people in his lab where it's like he's a really bad boss.
[00:42:06.400 --> 00:42:12.400]   He's really mean, really demanding. So obviously he's just covering his butt like on social media.
[00:42:12.400 --> 00:42:33.400]   So anyway, like my tweet was basically alluding to the fact that, like, well, if you're not there for the right reasons, if everyone's just there to collect a salary or they don't know what to do with their life or like they like hanging out with this crew and being on the same, like, I don't know, ideological team, kind of like, OK, like, I mean, what is the what is the point of science then?
[00:42:33.400 --> 00:42:44.400]   You know, what is the point of where why are you here? Why aren't you an accountant or a CPA or something? I don't know. It doesn't it doesn't make sense. You know, you're supposed to be here for a higher calling.
[00:42:44.400 --> 00:42:56.400]   And so, OK, so the tweet was the parishioner, the parishioner would be like the person involved, the laboratory, the research institution. And God is the truth.
[00:42:56.400 --> 00:43:12.400]   And if you're not there for the truth. Eventually, the institution is not going to make it, it's just going to kind of dissolve, because at the end of the day, if you don't have passion for research, if you don't passion for the truth, what's the point?
[00:43:12.400 --> 00:43:24.400]   Yeah, yeah. There's this professor we both know, but obviously I'm not going to say who it is. And so his his his his kids also want to become professors.
[00:43:24.400 --> 00:43:42.400]   So the kid, they just graduated high school and then so they had a peer reviewed published paper while they were in high school, because, you know, obviously the professor had guided guided them so that they would be in a good position to become a professor themselves.
[00:43:42.400 --> 00:43:51.400]   Right. So when you consider that kind of advantage and someone who just goes to college, like, oh, this subject seems interesting to me, maybe I should consider a graduate academic career here. Obviously, there's like no comparison.
[00:43:51.400 --> 00:44:00.400]   And in just the level of advantage you have, if you have been planning it out like. Yeah, I don't know. I don't know what's going on here.
[00:44:00.400 --> 00:44:17.400]   I've talked to multiple friends who are like in postdoc level and they just talk about like they can see over the last 10 years of massive inflation and publication where their postdocs now and like some of their graduate students have like two or three publications coming in.
[00:44:17.400 --> 00:44:24.400]   And like they didn't have any publications until their third, fourth year of graduate school, and they went to the same university. So what's happening here?
[00:44:24.400 --> 00:44:35.400]   You know, and I mean, you know, that's like, OK, like in the past, they didn't publish as much. They did a lot of science. So this is this is like one of those issues when you devise a metric.
[00:44:35.400 --> 00:44:48.400]   To measure something, eventually the metric gets distorted. Right. It's just like the true metrics getting distorted. There are people who are like producing. I mean, look, I mean, some of these researchers who are like who have like 30 papers a year.
[00:44:48.400 --> 00:44:54.400]   What? What? What? You know, I mean, you're not contributing. You're just you're not really contributing to it, you know?
[00:44:54.400 --> 00:45:07.400]   So yeah, yeah. Yeah. There's like there's like full time bloggers who don't output as many blog posts as you are outputting papers. Right. Yeah, yeah, yeah. I mean, yeah, yeah.
[00:45:07.400 --> 00:45:18.400]   OK, so within the spheres that I travel and maybe that you travel as well, like EA just and stuff, there's this idea that we are living in like a very important time in history.
[00:45:18.400 --> 00:45:27.400]   And then there's like there's like a step function. Right. So you have like different steps, like agriculture, domestication, metallurgy, industrialization.
[00:45:27.400 --> 00:45:36.400]   And like we're at another step right now. So from all the people I know, you know the most about history, especially ancient history.
[00:45:36.400 --> 00:45:43.400]   So do you view history as a sort of a series of step functions, each one, the newest one more important than the last?
[00:45:43.400 --> 00:45:53.400]   Or do you view it as just like a sort of a gradual exponential curve? Like what is your view of your long view of history?
[00:45:53.400 --> 00:46:01.400]   I think it's mostly gradual. We reified into a step, but I think we might actually be at a step now.
[00:46:01.400 --> 00:46:08.400]   I mean, if the slope is steep enough, it's a step. Right.
[00:46:08.400 --> 00:46:15.400]   So I think we might be at a step now and there have been steps in the past, but mostly we reify.
[00:46:15.400 --> 00:46:23.400]   So the Industrial Revolution, my understanding from economic history, is really more gradual and exponential than the quote revolution.
[00:46:23.400 --> 00:46:26.400]   Agriculture was probably like that as well.
[00:46:26.400 --> 00:46:31.400]   Peter Turchin's work with some of his collaborators indicates that the Axial Age was actually more gradual.
[00:46:31.400 --> 00:46:37.400]   Some of the coalescing of ideas, it wasn't like a step within one century, you know, around 600 B.C. or whatever.
[00:46:37.400 --> 00:46:44.400]   So, you know, this is just a situation where most of the time I think we we tend to like simplify it as a step.
[00:46:44.400 --> 00:46:54.400]   But, you know, you know, right now we live in an age of miracles that we don't take for granted because, you know, me, you, everybody, we're just scrambling.
[00:46:54.400 --> 00:46:57.400]   You know, we have supercomputers in our pockets. They're called phones.
[00:46:57.400 --> 00:47:08.400]   You know, we're doing, you know, science fictional video stuff. And my kids who are like my oldest kid is, say, 10.
[00:47:08.400 --> 00:47:11.400]   My youngest is like five. Right. Something like that.
[00:47:11.400 --> 00:47:20.400]   So they're they're a little dubious about this idea. Sometimes you use the phone for these non video calls.
[00:47:20.400 --> 00:47:29.400]   And they're very confused why people would do that. And they're very skeptical of this idea that that's what this phone was actually originally designed for.
[00:47:29.400 --> 00:47:38.400]   You know, so I mean, like, this is to the point where it's like, OK, like and like also like they they see a flip phone and they're very confused of how that could be a phone.
[00:47:38.400 --> 00:47:45.400]   Like, what is this ancient technology from 2007? You know, so it's just like, you know, because we have like a like a lot of people.
[00:47:45.400 --> 00:47:50.400]   We have like a desk full of phones, old phones that we never threw away because of whatever. Right.
[00:47:50.400 --> 00:47:55.400]   And so, you know, my kids saw the phone and they were like, what is this thing?
[00:47:55.400 --> 00:48:01.400]   And like, it's cool. And they're like, I'm like, oh, that's a phone. And they're like, no, but a phone square.
[00:48:01.400 --> 00:48:08.400]   You know, and they have an old rotary like toy phone. They traditionally use it as a hammer.
[00:48:08.400 --> 00:48:13.400]   You know, it's like they don't really know what the form factor is. Totally weird for them.
[00:48:13.400 --> 00:48:21.400]   So, you know, we are living through a radical change in terms of, you know, like our social technology or information technology.
[00:48:21.400 --> 00:48:25.400]   Like most of your viewers probably know Kurzweil. Information technology is exponential.
[00:48:25.400 --> 00:48:29.400]   Yeah. So there are some radical changes going on right now.
[00:48:29.400 --> 00:48:38.400]   And we need to think about what that means, because I think we're like, you know, I mean, VR is going to be a big deal.
[00:48:38.400 --> 00:48:43.400]   So I have said like I did say 20 years ago, probably.
[00:48:43.400 --> 00:48:48.400]   Again, like, you know, I know people hope Holden will be OK with him saying I've known Holden for 15 years.
[00:48:48.400 --> 00:48:54.400]   You know, Holden Karnofsky, I think he's, you know, might be one of the people you're talking about, about this century.
[00:48:54.400 --> 00:49:03.400]   And I said, like, this might be like the last century of humans in a way that we will recognize or it might be a century of regression.
[00:49:03.400 --> 00:49:18.400]   I, I think that we are in a metastable state right now where, I mean, I'm looking at you right now and you look like a primate, you know, and you are a primate, you know, but like you have access to all this technology.
[00:49:18.400 --> 00:49:25.400]   Like what's going on, especially or just in general, like I look at myself when I see myself, I don't see a primate.
[00:49:25.400 --> 00:49:35.400]   I see Raziv, you know what I'm saying? But if you look at another person, it's just like really, really like, you know, you think about it can be really, really visible.
[00:49:35.400 --> 00:49:49.400]   It's really, really visible that you're an animal of like that particular lineage, you know, when you look at it, you see where they move, you know, you think about.
[00:49:49.400 --> 00:49:55.400]   So like how long is this going to persist? Like we obviously evolved during the Pleistocene even earlier with a lot of our instincts.
[00:49:55.400 --> 00:50:00.400]   Now we have like the ability to destroy the world, our civilization, like we're not going to exterminate all life on Earth.
[00:50:00.400 --> 00:50:07.400]   Like that's just, you know, probably not even all humans. You know, there's probably going to be people in the Southern Hemisphere for sure that are going to survive.
[00:50:07.400 --> 00:50:19.400]   But it would destroy our civilization and civilizations have destroyed in the past, have been destroyed in the past by, you know, overreach, you know.
[00:50:19.400 --> 00:50:23.400]   But those civilizations had like local collapses, local regressions.
[00:50:23.400 --> 00:50:28.400]   And then they got like we got more robust with some of what you would call social technology. Right.
[00:50:28.400 --> 00:50:35.400]   So, for example, you see the Chinese dynastic cycle keeps shrinking every single time in terms of the chaotic interregnum.
[00:50:35.400 --> 00:50:42.400]   So, one, that means that the previous dynasty was its institutional structure is probably more robust to shocks.
[00:50:42.400 --> 00:50:47.400]   And then it can rewind itself back up relatively easily. Right.
[00:50:47.400 --> 00:50:52.400]   So like the first big first big unwinding is the Zhao dynasty doesn't really count.
[00:50:52.400 --> 00:50:55.400]   But let's do the Zhao. That's like 500 years decline.
[00:50:55.400 --> 00:51:05.400]   You hear the warring states and then the, you know, the Qin Han dynasty. Then there's like like a 300 year period of collapse.
[00:51:05.400 --> 00:51:12.400]   And then there's like like 150 year period of collapse. You know, it just keeps shrinking every single time.
[00:51:12.400 --> 00:51:17.400]   And so showing you that like the cultural or social technology is getting better, information technology is getting better.
[00:51:17.400 --> 00:51:20.400]   But now we're global.
[00:51:20.400 --> 00:51:28.400]   And so even if there's like a 50 year collapse, I mean, you know, there's a lot of stuff that we're going to lose, you know.
[00:51:28.400 --> 00:51:35.400]   And, you know, Sammo and others have talked about the fact that like we can't make rockets the way we could 50 years ago.
[00:51:35.400 --> 00:51:42.400]   Because a lot of those engineers are not, you know, and our military runs on like cobalt cobalt software that barely anyone can read.
[00:51:42.400 --> 00:51:51.400]   It's a cuneiform, you know, so it's like the Babylonians, like we laugh at them for like 2000 years or 1500 years.
[00:51:51.400 --> 00:51:58.400]   2000 years after the last native Sumerian speaker died, they were using Sumerian liturgy, you know.
[00:51:58.400 --> 00:52:04.400]   But we are going to have a situation soon where there's going to be almost no cobalt programmers, but we have cobalt software base.
[00:52:04.400 --> 00:52:13.400]   And so people are going to have to like, you know, train and like learn from these manuals, these ancient texts from the 1970s.
[00:52:13.400 --> 00:52:16.400]   I mean, it's not like that difficult. It's feasible.
[00:52:16.400 --> 00:52:23.400]   But the issue here is like timing and time, because you might not have enough programmers to service all the code that you have.
[00:52:23.400 --> 00:52:33.400]   So these are real issues that I think we have to deal with as primates who've organically developed this technological system and are trying to figure out how to make it work.
[00:52:33.400 --> 00:52:35.400]   Yeah, yeah.
[00:52:35.400 --> 00:52:37.400]   Sustained, sustained.
[00:52:37.400 --> 00:52:42.400]   Right. Yeah, I had Sam on just a little while back to discuss exactly these topics.
[00:52:42.400 --> 00:52:46.400]   So for the listeners who are interested, definitely check out that episode.
[00:52:46.400 --> 00:52:53.400]   Yeah. And even with something like, you know, cobalt or software engineering, like as somebody who started computer science, like even something that's that legible.
[00:52:53.400 --> 00:52:58.400]   You know, you can have sort of implicit knowledge from previous programmers or like how does the entire system work?
[00:52:58.400 --> 00:53:00.400]   And, you know, this is like literally written word, right?
[00:53:00.400 --> 00:53:01.400]   That's what a program is.
[00:53:01.400 --> 00:53:03.400]   So that's super legible.
[00:53:03.400 --> 00:53:05.400]   Compare that to other forms of manufacturing.
[00:53:05.400 --> 00:53:09.400]   I know a guy who worked at a fertilizer plant.
[00:53:09.400 --> 00:53:16.400]   And, you know, maybe I shouldn't say this on air, but he basically said, like, if somebody did something to this fertilizer plant, that's like, OK, that's a famine right there.
[00:53:16.400 --> 00:53:22.400]   Right. We've lost the source of nitrogen here.
[00:53:22.400 --> 00:53:30.400]   So, yeah. Oh, so going back to you made a very interesting comment about like we're at a point where we're using our computers to do magic.
[00:53:30.400 --> 00:53:33.400]   But the person behind that computer is a primate.
[00:53:33.400 --> 00:53:38.400]   So do you see knowing what you know about genetics and the potential malleability of our genetics?
[00:53:38.400 --> 00:53:50.400]   Do you see the future iteration as as us adding on to or modifying or selecting on the same biological substrate or do you see the future iteration?
[00:53:50.400 --> 00:53:56.400]   Do you think it's more feasible that we just move on to entirely virtual?
[00:53:56.400 --> 00:54:00.400]   We're like M's living on computers, like which seems more feasible to you?
[00:54:00.400 --> 00:54:06.400]   I mean, it would be ironic if we're a simulation that uploads ourselves into a computer.
[00:54:06.400 --> 00:54:10.400]   But I don't want to get into that. You know, I don't want to get into that.
[00:54:10.400 --> 00:54:16.400]   What's even more feasible? I think in the OK, like the biological program of redoing ourselves,
[00:54:16.400 --> 00:54:21.400]   I think is like actually it's not straightforward in terms of like, you know, minimal risk.
[00:54:21.400 --> 00:54:26.400]   There's going to be a lot of false starts, which is going to be kind of crazy. But I do think people will improve themselves.
[00:54:26.400 --> 00:54:31.400]   OK, I think they will edit themselves better over the next century.
[00:54:31.400 --> 00:54:37.400]   And but I think that there's going to be some integration with brain computer interfaces.
[00:54:37.400 --> 00:54:42.400]   Yes, I do. I mean, I haven't followed it closely, but, you know, I do move in some of the similar circles.
[00:54:42.400 --> 00:54:46.400]   And I think, you know, brain technology interfaces are going to be a big deal.
[00:54:46.400 --> 00:54:51.400]   And I think they're going to really change the game. And I think that's going to be.
[00:54:51.400 --> 00:54:58.400]   But the issue there is like. So I guess I think of gene editing, to be honest, more as like Smithian growth,
[00:54:58.400 --> 00:55:04.400]   where it's like, you know, increased efficiencies because we have the genetic variation now.
[00:55:04.400 --> 00:55:07.400]   Like we can make him smarter. We have the technology. OK, we're not we're not there yet.
[00:55:07.400 --> 00:55:11.400]   I can see that, though. I mean, we can all like understand the basic logic there.
[00:55:11.400 --> 00:55:14.400]   Like there is John John von Neumann existed. The experiment has been done.
[00:55:14.400 --> 00:55:18.400]   Yeah. So we can aspire to create like a bunch of anomalies. OK, that's great.
[00:55:18.400 --> 00:55:25.400]   Now, the issue is like with like with the with the human computer interfaces, that's never been done.
[00:55:25.400 --> 00:55:32.400]   Right. So that is that is like an innovation. That's like, you know, technology driven growth.
[00:55:32.400 --> 00:55:38.400]   That's increasing like the baseline productivity by like crazy about like that's that could be the possible quantum leap.
[00:55:38.400 --> 00:55:45.400]   So, excuse me, that could be a big deal in a good way or a bad way.
[00:55:45.400 --> 00:55:50.400]   And I think a lot of your listeners know about all the existential risk crisis and artificial.
[00:55:50.400 --> 00:55:55.400]   You know, we talk about like, you know, hostile AI and general artificial intelligence and all this stuff.
[00:55:55.400 --> 00:55:59.400]   But I mean, I mean, perhaps it'll start with us.
[00:55:59.400 --> 00:56:15.400]   You know, perhaps Skynet will be some uploaded crazy kid, you know, where it's like maybe it's going to be a situation where it's like it's like going to the new world where, you know, there were attempts to go to like to the new.
[00:56:15.400 --> 00:56:22.400]   So the new they didn't know the new world was there, but there were people in the Middle Ages who left for the West and they never.
[00:56:22.400 --> 00:56:26.400]   Obviously, the ships just disappeared. You know, they died, you know, they died at sea. Right.
[00:56:26.400 --> 00:56:32.400]   So there's going to be people who do things like going to Mars, high mortality rates, you know, these sorts of things.
[00:56:32.400 --> 00:56:40.400]   Similarly, with like these human computer interfaces, there's going to be high death rates, like just basically people just disappear into the ether.
[00:56:40.400 --> 00:56:52.400]   But then the first person that gets in there. It's going to be like Christopher Columbus or, you know, it's going to be a situation where they may be like actually like a very, very advantageous position.
[00:56:52.400 --> 00:56:59.400]   Instead of being a primitive prototype, they might like basically have all the quote unquote land in the cyberspace. Right.
[00:56:59.400 --> 00:57:06.400]   Where it's like they do all the learnings really early on. They iterate, they pivot. And so they can be like, you know, the god of that universe.
[00:57:06.400 --> 00:57:13.400]   I'm just speculating here, but I'm trying to say that, like, I think the possibilities there are like pretty extensive, pretty high variance.
[00:57:13.400 --> 00:57:21.400]   And in the short term, what is the landscape of just, I guess, gene editing, polygenic selection?
[00:57:21.400 --> 00:57:30.400]   What does that look like in the next 10 or 20 years? So, I mean, is there potential that, you know, you could like raise your kids IQ by one or two standard deviations?
[00:57:30.400 --> 00:57:39.400]   Or are these going to be like marginal improvements? Like by the time I'm ready to have kids, what will it look like?
[00:57:39.400 --> 00:57:43.400]   Yeah, I think with gene editing, the intelligence thing is going to be like 20 years. Let's say 20 years.
[00:57:43.400 --> 00:57:48.400]   Okay. Like I think in the short term, gene editing really will do is probably cure cystic fibrosis, clear sickle cell.
[00:57:48.400 --> 00:57:54.400]   Like these are like Mendelian, quasi-Mendelian diseases with large effect loci and people just have issues.
[00:57:54.400 --> 00:57:59.400]   And so, you know, there's always a delivery problem. There's always a problem with off target effects, which could cause mutations, could cause cancers.
[00:57:59.400 --> 00:58:04.400]   But, you know, if you're cystic fibrosis, you're going to be dead by 45. You're going to take the risk, right?
[00:58:04.400 --> 00:58:11.400]   So, I think that's honestly going to be the first thing. The first thing is going to be transfection or like, you know, gene editing of adults for Mendelian diseases.
[00:58:11.400 --> 00:58:19.400]   So, that's the next 10 years. Okay. It's already happening now. They're already curing people of malaria or sickle cell.
[00:58:19.400 --> 00:58:29.400]   And I think they're working literally right now as we record on cystic fibrosis and ALS, you know, because there's degenerative diseases that kill people in the prime of their lives.
[00:58:29.400 --> 00:58:40.400]   But, you know, 20 years, that's a long time. You know, we have 40-year-old IVF babies now, you know. I think almost 40.
[00:58:40.400 --> 00:58:47.400]   But, so, I think 20 years, yes, you will start to see parents editing the genes of their offspring.
[00:58:47.400 --> 00:58:56.400]   I think intelligence is like difficult because it's a polygenic trait with a lot of different genomic positions.
[00:58:56.400 --> 00:59:00.400]   I wonder if they're going to go for other things first and then kind of work to it.
[00:59:00.400 --> 00:59:11.400]   And then, you know, there's that theory that Armando Roy was talking about it, but other people is like, it's not like what you should do is focus on mutations and other things.
[00:59:11.400 --> 00:59:17.400]   Try to fix those and see if that just inadvertently increases the, you know, intelligence.
[00:59:17.400 --> 00:59:18.400]   Interesting.
[00:59:18.400 --> 00:59:27.400]   Rather than focusing on getting gain-of-function genes, which is like, okay, how do you identify those, fix all of your copy errors because that's a finite number.
[00:59:27.400 --> 00:59:34.400]   Compare to the pedigree of the parents, look at the de novo mutations, look at the parents' de novo mutations against the idealized reference, et cetera, et cetera.
[00:59:34.400 --> 00:59:36.400]   That might be much more feasible. Yeah.
[00:59:36.400 --> 00:59:38.400]   Oh, interesting. Yeah, I didn't know about that.
[00:59:38.400 --> 00:59:43.400]   Do you have an estimate for how many SNPs affect the variation in intelligence between people?
[00:59:43.400 --> 00:59:49.400]   Let's see. How many SNPs?
[00:59:49.400 --> 00:59:52.400]   I think it's going to be an order of thousands.
[00:59:52.400 --> 00:59:53.400]   Okay.
[00:59:53.400 --> 00:59:56.400]   Yeah.
[00:59:56.400 --> 00:59:58.400]   All right.
[00:59:58.400 --> 01:00:00.400]   So just some meta questions to close out on.
[01:00:00.400 --> 01:00:10.400]   So you've, you know, you distinguish yourself in your career by being somebody who's like an expert in history and an expert in genomics and life sciences more generally.
[01:00:10.400 --> 01:00:16.400]   Are there other fields where you think knowledge of history would be very useful in setting up like a separate niche?
[01:00:16.400 --> 01:00:19.400]   Because you have a niche in history and genomics.
[01:00:19.400 --> 01:00:25.400]   But, you know, it would be hard to imagine, for example, somebody knowing a lot about history and computer science having a special niche, right?
[01:00:25.400 --> 01:00:26.400]   Yeah.
[01:00:26.400 --> 01:00:27.400]   Cultural evolution.
[01:00:27.400 --> 01:00:28.400]   Yeah, cultural evolution.
[01:00:28.400 --> 01:00:32.400]   That's what Joe Henrick and some of his people wanted to do.
[01:00:32.400 --> 01:00:33.400]   Yeah.
[01:00:33.400 --> 01:00:43.400]   So I think like in cultural evolution, there's going to be a lot of gains because, you know, Peter Turchin, Joe Henrick, these people are applying evolutionary principles to historical processes.
[01:00:43.400 --> 01:00:52.400]   And to have the empirical data set, to have the empirical data set is really important.
[01:00:52.400 --> 01:00:54.400]   And this is a really new, nascent field.
[01:00:54.400 --> 01:00:58.400]   So I think that that's going to be the big thing that I would think people should focus on.
[01:00:58.400 --> 01:01:00.400]   Peter said, oh, like get anthropology knowledge.
[01:01:00.400 --> 01:01:01.400]   Oh, my God.
[01:01:01.400 --> 01:01:03.400]   I don't think the short term knowledge is super important.
[01:01:03.400 --> 01:01:12.400]   I think having a deep, deep, not deep, thick knowledge about historical arcs would probably be pretty useful.
[01:01:12.400 --> 01:01:13.400]   Yeah.
[01:01:13.400 --> 01:01:14.400]   Interesting.
[01:01:14.400 --> 01:01:15.400]   Interesting.
[01:01:15.400 --> 01:01:17.400]   Joe and his group, they're working on that.
[01:01:17.400 --> 01:01:19.400]   They're moving into history.
[01:01:19.400 --> 01:01:23.400]   They're doing some serious imperialism that's causing problems.
[01:01:23.400 --> 01:01:24.400]   Yeah.
[01:01:24.400 --> 01:01:25.400]   Causing problems.
[01:01:25.400 --> 01:01:26.400]   Oh, yeah.
[01:01:26.400 --> 01:01:30.400]   Just historians do not like the turf, turf infringement.
[01:01:30.400 --> 01:01:31.400]   That's what I'm saying.
[01:01:31.400 --> 01:01:32.400]   I see.
[01:01:32.400 --> 01:01:33.400]   Yeah, yeah.
[01:01:33.400 --> 01:01:34.400]   Yeah.
[01:01:34.400 --> 01:01:35.400]   Right.
[01:01:35.400 --> 01:01:49.400]   And so you're one of the top bloggers on Substack and you have this like deeply technical blog on, you know, the science of genomics and other things, you know, that that's like, you know, you would think beforehand that your prior would be like, oh, like how many people are going to be able to understand this or be interested in this.
[01:01:49.400 --> 01:01:51.400]   But in fact, you're, you know, you're one of the top people on Substack.
[01:01:51.400 --> 01:01:53.400]   Like, what is the experience of that been like?
[01:01:53.400 --> 01:01:57.400]   And like, has it surprised you, the popularity of your work and everything?
[01:01:57.400 --> 01:02:00.400]   Honestly, no.
[01:02:00.400 --> 01:02:02.400]   Well, I mean, it's surprised.
[01:02:02.400 --> 01:02:03.400]   OK, I'm going to be honest.
[01:02:03.400 --> 01:02:06.400]   It probably surprised me, like how many people are willing to pay.
[01:02:06.400 --> 01:02:08.400]   But people have been reading me for a long time.
[01:02:08.400 --> 01:02:10.400]   So I just kind of like professionalized it some.
[01:02:10.400 --> 01:02:16.400]   And yeah, it's it's been great.
[01:02:16.400 --> 01:02:23.400]   And it's really like helped me figure out what people are interested in, in terms of what they're willing to pay for.
[01:02:23.400 --> 01:02:26.400]   And, you know, it's given me some direction, I guess.
[01:02:26.400 --> 01:02:33.400]   But I plan to do I basically do what I continue to have done in various ways in the past and to the future.
[01:02:33.400 --> 01:02:40.400]   And, you know, like thinking of startup world way, I would pivot and iterate is what is what I'm thinking.
[01:02:40.400 --> 01:02:48.400]   And final question, do you have any advice for people who want to write about technical topics in a way that's very interesting to a broad audience?
[01:02:48.400 --> 01:02:55.400]   OK, so you have to make it relevant to them somehow.
[01:02:55.400 --> 01:03:01.400]   So, for example, like let's say you want to write about signal detection.
[01:03:01.400 --> 01:03:06.400]   I think, you know, text to speech type stuff is one.
[01:03:06.400 --> 01:03:09.400]   There are things people are super interested in.
[01:03:09.400 --> 01:03:14.400]   So, for example, people are super interested in Ashkenazi Jewish genetics.
[01:03:14.400 --> 01:03:17.400]   People are super interested in the genetic architecture of skin color.
[01:03:17.400 --> 01:03:26.400]   I mean, OK, why? I can talk about the genetic architecture of like, I don't know, something else, you know, and it wouldn't be a super interesting.
[01:03:26.400 --> 01:03:30.400]   So you have to find the domain that they're interested in and then apply your method.
[01:03:30.400 --> 01:03:44.400]   Right. So if you're interested in. So actually, there's a substack on personality that talks about personality and using machine learning methods to classify personality.
[01:03:44.400 --> 01:03:49.400]   Machine learning is technical, but personality is interesting.
[01:03:49.400 --> 01:03:52.400]   Interesting. OK. Yeah, yeah. It's good advice. All right.
[01:03:52.400 --> 01:04:01.400]   Steve, thanks so much for coming on the podcast. Thanks for your time. This was my pleasure.
[01:04:01.400 --> 01:04:06.400]   Hey, I hope you enjoyed that episode. If you did, you want to help support the podcast.
[01:04:06.400 --> 01:04:11.400]   The most helpful thing you can do is share it on social media and make their friends.
[01:04:11.400 --> 01:04:19.400]   Other than that, please make sure to like and subscribe and click here for more content like this.
[01:04:19.400 --> 01:04:23.400]   Huge thanks to my co-founder, Grayson Raucher, for producing the show.
[01:04:23.400 --> 01:04:39.400]   I'll see you next time. Peace.
[01:04:39.400 --> 01:04:49.400]   [BLANK_AUDIO]

