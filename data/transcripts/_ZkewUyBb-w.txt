
[00:00:00.000 --> 00:00:06.160]   Welcome back everyone.
[00:00:06.160 --> 00:00:07.960]   This is part six in our series on
[00:00:07.960 --> 00:00:10.360]   advanced behavioral testing for NLU.
[00:00:10.360 --> 00:00:13.720]   To this point, we've been focused on adversarial testing.
[00:00:13.720 --> 00:00:16.380]   We're now going to take a more expansive view and think about
[00:00:16.380 --> 00:00:20.380]   the potential benefits of training on adversarial cases.
[00:00:20.380 --> 00:00:22.820]   The foundational entry in this literature is
[00:00:22.820 --> 00:00:25.280]   the ANLI paper and the associated benchmark.
[00:00:25.280 --> 00:00:26.560]   As far as I know,
[00:00:26.560 --> 00:00:28.800]   ANLI is the first attempt to create
[00:00:28.800 --> 00:00:33.620]   a really large train set that is filled with adversarial examples.
[00:00:33.620 --> 00:00:35.820]   That is, with examples that fooled
[00:00:35.820 --> 00:00:39.000]   a top performing model but were intuitive for humans.
[00:00:39.000 --> 00:00:42.800]   I think it's fair to say that ANLI is a direct response to
[00:00:42.800 --> 00:00:44.920]   the adversarial test results that we
[00:00:44.920 --> 00:00:47.720]   reviewed in the previous screencast where we saw
[00:00:47.720 --> 00:00:50.840]   NLI models that were surpassing our estimates for
[00:00:50.840 --> 00:00:54.180]   human performance but nonetheless falling down on
[00:00:54.180 --> 00:00:56.960]   very simple phenomena turning on
[00:00:56.960 --> 00:01:00.120]   systematicity or compositionality in language.
[00:01:00.120 --> 00:01:02.680]   The vision for ANLI is that by
[00:01:02.680 --> 00:01:06.760]   introducing an adversarial dynamic into the train set creation,
[00:01:06.760 --> 00:01:09.220]   we can get models that are more robust.
[00:01:09.220 --> 00:01:11.860]   Here's how data set creation worked.
[00:01:11.860 --> 00:01:15.320]   The annotator is presented with a premise sentence and
[00:01:15.320 --> 00:01:17.740]   a condition that is entailment, contradiction,
[00:01:17.740 --> 00:01:20.180]   or neutral, one of the NLI labels.
[00:01:20.180 --> 00:01:24.000]   The annotator writes a hypothesis and then a state of
[00:01:24.000 --> 00:01:26.040]   the art model makes a prediction about
[00:01:26.040 --> 00:01:28.580]   the resulting premise hypothesis pair.
[00:01:28.580 --> 00:01:31.400]   If the model's prediction matches the condition,
[00:01:31.400 --> 00:01:33.860]   that is, if the model was correct in some sense,
[00:01:33.860 --> 00:01:37.080]   the annotator returns to step 2 to try again.
[00:01:37.080 --> 00:01:39.400]   Whereas if the model was fooled,
[00:01:39.400 --> 00:01:41.700]   the premise hypothesis pair is independently
[00:01:41.700 --> 00:01:44.740]   validated by other human annotators.
[00:01:44.740 --> 00:01:46.840]   The result of this dynamic,
[00:01:46.840 --> 00:01:49.240]   of this interaction with this top performing model,
[00:01:49.240 --> 00:01:53.160]   is a train set that is full of really hard cases,
[00:01:53.160 --> 00:01:55.600]   cases that fooled this top performing model,
[00:01:55.600 --> 00:01:59.000]   in addition to cases that didn't fool that model.
[00:01:59.000 --> 00:02:01.080]   The examples are interesting.
[00:02:01.080 --> 00:02:03.360]   The premises in ANLI tend to be long.
[00:02:03.360 --> 00:02:05.880]   The hypotheses are, of course, challenging.
[00:02:05.880 --> 00:02:09.240]   Interestingly, the dataset also contains these reason texts.
[00:02:09.240 --> 00:02:11.620]   This is the annotator's best attempt to explain
[00:02:11.620 --> 00:02:15.080]   why the model might have struggled with that particular example.
[00:02:15.080 --> 00:02:17.040]   As far as I know, the reason texts
[00:02:17.040 --> 00:02:18.900]   haven't been used very much in the literature,
[00:02:18.900 --> 00:02:20.960]   but they strike me as an interesting source
[00:02:20.960 --> 00:02:23.800]   of indirect supervision about the task.
[00:02:23.800 --> 00:02:25.960]   You might check those out.
[00:02:25.960 --> 00:02:30.080]   This is the core results table for the ANLI paper.
[00:02:30.080 --> 00:02:31.560]   There's a lot of information here,
[00:02:31.560 --> 00:02:33.640]   but I think the story is pretty straightforward.
[00:02:33.640 --> 00:02:35.500]   Let's focus on the BERT model.
[00:02:35.500 --> 00:02:38.540]   The BERT model is doing really well on SNLI and
[00:02:38.540 --> 00:02:40.040]   multi-NLI across all of
[00:02:40.040 --> 00:02:42.760]   these different variants of the training regimes.
[00:02:42.760 --> 00:02:46.280]   When the model is trained only on SNLI and multi-NLI,
[00:02:46.280 --> 00:02:48.680]   it does really poorly on ANLI.
[00:02:48.680 --> 00:02:50.760]   You can see ANLI had three rounds.
[00:02:50.760 --> 00:02:51.880]   When we pool them together,
[00:02:51.880 --> 00:02:55.320]   this model gets around 20 percent accuracy.
[00:02:55.320 --> 00:02:57.560]   As we take that model and augment
[00:02:57.560 --> 00:03:00.920]   its training data with ANLI data from previous rounds,
[00:03:00.920 --> 00:03:04.560]   we do see improvements overall in the ANLI column,
[00:03:04.560 --> 00:03:05.440]   which is encouraging.
[00:03:05.440 --> 00:03:07.680]   It looks like the models are getting better at
[00:03:07.680 --> 00:03:09.200]   the task as they get more of
[00:03:09.200 --> 00:03:12.040]   these adversarial examples as part of training.
[00:03:12.040 --> 00:03:15.480]   But the fundamental insight here is that performance on
[00:03:15.480 --> 00:03:18.820]   ANLI is well below performance for the other benchmarks.
[00:03:18.820 --> 00:03:20.920]   This is a substantial challenge and I
[00:03:20.920 --> 00:03:24.600]   believe that this substantial challenge still stands.
[00:03:24.600 --> 00:03:27.360]   Models do not excel at ANLI
[00:03:27.360 --> 00:03:31.080]   even to this day as far as I know.
[00:03:31.080 --> 00:03:34.600]   One thing I love about ANLI is that it projects
[00:03:34.600 --> 00:03:36.740]   this really interesting vision for
[00:03:36.740 --> 00:03:38.640]   the future development of
[00:03:38.640 --> 00:03:41.040]   train and test assets for the field.
[00:03:41.040 --> 00:03:43.840]   It's actually all credit due to Zellers et al.
[00:03:43.840 --> 00:03:45.680]   They also described this vision in
[00:03:45.680 --> 00:03:48.360]   their papers on SWAG and Hella SWAG.
[00:03:48.360 --> 00:03:51.580]   They write, "A path for NLP progress going forward
[00:03:51.580 --> 00:03:53.760]   towards benchmarks that adversarially
[00:03:53.760 --> 00:03:57.100]   co-evolve with evolving state-of-the-art models."
[00:03:57.100 --> 00:03:59.940]   I didn't have time to tell this full story in details,
[00:03:59.940 --> 00:04:02.240]   but Zellers et al is an interesting story.
[00:04:02.240 --> 00:04:03.520]   There are two papers.
[00:04:03.520 --> 00:04:05.320]   The first one introduced SWAG,
[00:04:05.320 --> 00:04:09.360]   which is a synthetically created train and test environment
[00:04:09.360 --> 00:04:12.960]   for adversarial testing.
[00:04:12.960 --> 00:04:15.040]   They found that it was very difficult,
[00:04:15.040 --> 00:04:16.460]   but when the BERT paper came out,
[00:04:16.460 --> 00:04:19.600]   BERT essentially solved the SWAG problem.
[00:04:19.600 --> 00:04:21.300]   In response to that,
[00:04:21.300 --> 00:04:23.220]   Zellers et al made some adjustments
[00:04:23.220 --> 00:04:26.060]   to the SWAG dataset that produced Hella SWAG.
[00:04:26.060 --> 00:04:28.560]   Hella SWAG was substantially harder for BERT,
[00:04:28.560 --> 00:04:30.500]   and I believe that Hella SWAG remains
[00:04:30.500 --> 00:04:32.580]   a challenging benchmark to this day.
[00:04:32.580 --> 00:04:35.120]   I think that started us on the path of seeing how
[00:04:35.120 --> 00:04:37.980]   productive it could be to create datasets,
[00:04:37.980 --> 00:04:39.580]   use them to develop models,
[00:04:39.580 --> 00:04:41.420]   and then respond when models seem to
[00:04:41.420 --> 00:04:44.500]   succeed with even harder challenges.
[00:04:44.500 --> 00:04:46.180]   In the ANLI paper,
[00:04:46.180 --> 00:04:48.220]   they project this vision very directly.
[00:04:48.220 --> 00:04:52.600]   This process yields a moving post dynamic target for
[00:04:52.600 --> 00:04:54.020]   NLU systems rather than
[00:04:54.020 --> 00:04:56.620]   a static benchmark that will eventually saturate.
[00:04:56.620 --> 00:04:58.460]   This sounds so productive to me.
[00:04:58.460 --> 00:05:01.260]   Throughout the field, large teams of
[00:05:01.260 --> 00:05:04.140]   very talented people spend lots of time and money
[00:05:04.140 --> 00:05:06.340]   getting epsilon more performance
[00:05:06.340 --> 00:05:08.260]   out of our established benchmarks.
[00:05:08.260 --> 00:05:10.300]   Wouldn't it be wonderful if instead,
[00:05:10.300 --> 00:05:12.140]   when we saw the benchmark saturating,
[00:05:12.140 --> 00:05:14.300]   we simply created new benchmarks,
[00:05:14.300 --> 00:05:16.540]   and posed new challenges for ourselves.
[00:05:16.540 --> 00:05:19.420]   I think it's a very safe bet that models would improve
[00:05:19.420 --> 00:05:21.700]   more rapidly and become more capable
[00:05:21.700 --> 00:05:24.860]   if we did this moving post thing.
[00:05:24.860 --> 00:05:27.780]   That really is the vision for Dynabench.
[00:05:27.780 --> 00:05:30.340]   Dynabench is an open-source software effort,
[00:05:30.340 --> 00:05:32.460]   an open-source platform for doing,
[00:05:32.460 --> 00:05:36.780]   among other things, dynamic adversarial data collection.
[00:05:36.780 --> 00:05:40.620]   Dynabench has produced a number of datasets to this point.
[00:05:40.620 --> 00:05:42.700]   ANLI is the first one.
[00:05:42.700 --> 00:05:43.860]   That's the precursor.
[00:05:43.860 --> 00:05:46.940]   We also have Dynabench derived datasets for QA,
[00:05:46.940 --> 00:05:50.860]   for sentiment, and a number of datasets for hate speech,
[00:05:50.860 --> 00:05:53.460]   including counter speech to hate speech.
[00:05:53.460 --> 00:05:56.500]   We have a few on QA and one on German hate speech.
[00:05:56.500 --> 00:05:58.940]   I think this list will continue to grow
[00:05:58.940 --> 00:06:02.620]   and offer us these incredible new resources.
[00:06:02.620 --> 00:06:05.420]   Let me stop there for the next screencast.
[00:06:05.420 --> 00:06:07.340]   I'm going to do a deep dive on
[00:06:07.340 --> 00:06:12.060]   a Dynabench derived dataset that we created called Dynascent.
[00:06:12.060 --> 00:06:22.060]   [BLANK_AUDIO]

