
[00:00:00.000 --> 00:00:01.760]   I don't know what it's like to be an alien.
[00:00:01.760 --> 00:00:03.160]   I would like to know.
[00:00:03.160 --> 00:00:05.760]   Two alien civilizations coexisting on a planet.
[00:00:05.760 --> 00:00:07.240]   What's that look like exactly?
[00:00:07.240 --> 00:00:12.040]   When you see them and they see you, you're assuming they have vision, they have the ability
[00:00:12.040 --> 00:00:14.680]   to construct in 3D and in time.
[00:00:14.680 --> 00:00:16.600]   That's a lot of assumptions we're making.
[00:00:16.600 --> 00:00:19.840]   What human level intelligence has done is quite different.
[00:00:19.840 --> 00:00:24.200]   It's not just that we remember states that the universe has existed in before, it's that
[00:00:24.200 --> 00:00:29.320]   we can imagine ones that have never existed and we can actually make them come into existence.
[00:00:29.320 --> 00:00:32.200]   You can travel back in time sometimes.
[00:00:32.200 --> 00:00:33.200]   Yes.
[00:00:33.200 --> 00:00:36.440]   You travel forward in time to travel back.
[00:00:36.440 --> 00:00:39.000]   Yes.
[00:00:39.000 --> 00:00:42.240]   The following is a conversation with Sarah Walker and Lee Cronin.
[00:00:42.240 --> 00:00:48.400]   They have each been on this podcast once before individually and now for their second time
[00:00:48.400 --> 00:00:50.080]   they're here together.
[00:00:50.080 --> 00:00:54.080]   Sarah is an astrobiologist and theoretical physicist.
[00:00:54.080 --> 00:01:00.160]   Lee is a chemist and if I may say so, the real life manifestation of Rick from Rick
[00:01:00.160 --> 00:01:01.680]   and Morty.
[00:01:01.680 --> 00:01:07.800]   They both are interested in how life originates and develops both life here on earth and alien
[00:01:07.800 --> 00:01:13.320]   life including intelligent alien civilizations out there in the cosmos.
[00:01:13.320 --> 00:01:18.760]   They are colleagues and friends who love to explore, disagree and debate nuanced points
[00:01:18.760 --> 00:01:24.760]   about alien life and so we're calling this an alien debate.
[00:01:24.760 --> 00:01:29.120]   Very few questions to me are as fascinating as what do aliens look like?
[00:01:29.120 --> 00:01:30.740]   How do we recognize them?
[00:01:30.740 --> 00:01:32.260]   How do we talk to them?
[00:01:32.260 --> 00:01:37.040]   And how do we make sense of life here on earth in the context of all possible life forms
[00:01:37.040 --> 00:01:38.840]   that are out there?
[00:01:38.840 --> 00:01:44.120]   Treating these questions with the seriousness and rigor they deserve is what I hope to do
[00:01:44.120 --> 00:01:48.320]   with this conversation and future ones like it.
[00:01:48.320 --> 00:01:50.040]   Our world is shrouded in mystery.
[00:01:50.040 --> 00:01:55.240]   We must first be humble to acknowledge this and then be bold in diving in and trying to
[00:01:55.240 --> 00:01:57.520]   figure things out anyway.
[00:01:57.520 --> 00:01:59.520]   This is the Lex Friedman Podcast.
[00:01:59.520 --> 00:02:03.040]   To support it, please check out our sponsors in the description.
[00:02:03.040 --> 00:02:08.760]   And now dear friends, here's Sarah Walker and Lee Cronin.
[00:02:08.760 --> 00:02:11.160]   First of all, welcome back Sarah, welcome back Lee.
[00:02:11.160 --> 00:02:13.640]   You guys, I'm a huge fan of yours, you're incredible people.
[00:02:13.640 --> 00:02:17.360]   I should say thank you to Sarah for wearing really awesome boots.
[00:02:17.360 --> 00:02:21.680]   We'll probably overlay a picture later on, but why the hell didn't you dress up Lee?
[00:02:21.680 --> 00:02:22.680]   No, I'm just kidding.
[00:02:22.680 --> 00:02:23.680]   This is me dressed up.
[00:02:23.680 --> 00:02:28.480]   You were saying that you're pink, that your thing is pink, my thing is black and white,
[00:02:28.480 --> 00:02:30.400]   the simplicity of it.
[00:02:30.400 --> 00:02:31.400]   Where's the pink?
[00:02:31.400 --> 00:02:34.400]   When did it hit you that pink is your color?
[00:02:34.400 --> 00:02:41.560]   I became pink about, I don't know actually, maybe 2017?
[00:02:41.560 --> 00:02:42.560]   Did you know me when you first-
[00:02:42.560 --> 00:02:43.560]   I think I met you pre-pink.
[00:02:43.560 --> 00:02:44.560]   Yeah, yeah.
[00:02:44.560 --> 00:02:50.480]   So about 2017, I think, I just decided I was boring and I needed to make a statement and
[00:02:50.480 --> 00:02:53.320]   red was too bright, so I went pink, salmon pink.
[00:02:53.320 --> 00:02:59.200]   Well, I think you were always pink, you just found yourself in 2017.
[00:02:59.200 --> 00:03:02.520]   There's an amazing photo of him where there's like everybody in their black gown and he's
[00:03:02.520 --> 00:03:03.520]   just wearing the pink pants.
[00:03:03.520 --> 00:03:05.200]   Oh, that was at the Wagon and University-
[00:03:05.200 --> 00:03:06.200]   It's totally nuts.
[00:03:06.200 --> 00:03:10.440]   100th year anniversary, they got me to give the plenary and they didn't find the outfit
[00:03:10.440 --> 00:03:13.800]   for me, so they were all wearing these silly hats and these gowns and there was me dressed
[00:03:13.800 --> 00:03:16.120]   up in pink looking like a complete idiot.
[00:03:16.120 --> 00:03:21.160]   We're definitely going to have to find that picture and overlay it, big full screen, slow
[00:03:21.160 --> 00:03:22.160]   motion.
[00:03:22.160 --> 00:03:23.760]   All right, let's talk about aliens.
[00:03:23.760 --> 00:03:27.440]   We'll find places we disagree and places we agree.
[00:03:27.440 --> 00:03:31.640]   Life, intelligence, consciousness, universe, all of that.
[00:03:31.640 --> 00:03:37.520]   Let's start with a tweet from Neil deGrasse Tyson stating his skepticism about aliens
[00:03:37.520 --> 00:03:39.680]   wanting to visit Earth.
[00:03:39.680 --> 00:03:45.440]   Quote, "How egocentric of us to think that space aliens who have mastered interstellar
[00:03:45.440 --> 00:03:53.520]   travel across the galaxy would give," pardon the French, "would give a shit about humans
[00:03:53.520 --> 00:03:54.520]   on Earth."
[00:03:54.520 --> 00:03:59.200]   So let me ask you, would aliens care about visiting Earth, observing, communicating with
[00:03:59.200 --> 00:04:00.200]   humans?
[00:04:00.200 --> 00:04:05.160]   Let's take a perspective of aliens, maybe Sarah, first.
[00:04:05.160 --> 00:04:11.040]   Are we interesting in the whole spectrum of life in the universe?
[00:04:11.040 --> 00:04:14.560]   - I'm completely biased, at least as far as I think right now, we're the most interesting
[00:04:14.560 --> 00:04:16.480]   thing in the universe.
[00:04:16.480 --> 00:04:24.120]   So I would expect, based on the intrinsic curiosity that we have and how much I think
[00:04:24.120 --> 00:04:28.880]   that's deeply related to the physics of what we are, that other intelligent aliens would
[00:04:28.880 --> 00:04:34.540]   want to seek out examples of the phenomena they are to understand themselves better.
[00:04:34.540 --> 00:04:38.160]   And I think that's kind of a natural thing to want to do, and I don't think there's any
[00:04:38.160 --> 00:04:42.120]   kind of judgment on it being a lesser being or not.
[00:04:42.120 --> 00:04:45.200]   It's like saying you have nothing to learn by talking to a baby.
[00:04:45.200 --> 00:04:49.280]   You have lots to learn, probably more than you do talking to somebody that's 90.
[00:04:49.280 --> 00:04:51.520]   So I think they absolutely would.
[00:04:51.520 --> 00:04:57.160]   - So whatever the phenomena is that is human, there would be an inkling of the same kind
[00:04:57.160 --> 00:05:00.760]   of phenomena within alien species, and they would be seeking that same--
[00:05:00.760 --> 00:05:04.320]   - I think there's gotta be some features of us that are universal, and I think the ones
[00:05:04.320 --> 00:05:10.840]   that are most interesting, and I hope I live in an interesting universe, are the ones that
[00:05:10.840 --> 00:05:18.720]   are driven by our curiosity and the fact that our intelligence allows us to do things that
[00:05:18.720 --> 00:05:22.920]   the universe wouldn't be able to do without things like us existing.
[00:05:22.920 --> 00:05:24.380]   - We're gonna define a lot of terms.
[00:05:24.380 --> 00:05:25.380]   One of them is interesting.
[00:05:25.380 --> 00:05:26.380]   - Yes.
[00:05:26.380 --> 00:05:29.280]   - That's a very interesting term to try to define.
[00:05:29.280 --> 00:05:30.760]   Lee, what do you think?
[00:05:30.760 --> 00:05:33.840]   Are humans interesting for aliens?
[00:05:33.840 --> 00:05:34.840]   - Let's take it from our perspective.
[00:05:34.840 --> 00:05:37.840]   We wanna go find aliens as a species quite desperately.
[00:05:37.840 --> 00:05:41.480]   So if we put the shoe on the other foot, of course we're interesting.
[00:05:41.480 --> 00:05:46.160]   But I'm wondering, and assuming that we're at the right technological capabilities to
[00:05:46.160 --> 00:05:49.160]   go searching for aliens, then that's interesting.
[00:05:49.160 --> 00:05:55.800]   So what I mean is, if there needs to be a massive leap in technology that we don't have,
[00:05:55.800 --> 00:05:58.920]   how will aliens prioritize coming to Earth and other places?
[00:05:58.920 --> 00:06:03.680]   But I do think that they would come and find us, 'cause they'd wanna find out about our
[00:06:03.680 --> 00:06:06.680]   culture, what things are universal.
[00:06:06.680 --> 00:06:10.320]   What about, I mean, I'm a chemist, so I would say, "Well, is the chemistry universal?"
[00:06:10.320 --> 00:06:14.880]   Are the creatures that we're gonna find making all this commotion, are they made of the same
[00:06:14.880 --> 00:06:17.660]   stuff?
[00:06:17.660 --> 00:06:20.520]   What does their science look like?
[00:06:20.520 --> 00:06:23.400]   Are they off planet yet?
[00:06:23.400 --> 00:06:29.920]   So I think that Neil deGrasse Tyson is being slightly pessimistic and maybe trying to play
[00:06:29.920 --> 00:06:35.280]   the tune that the universe is vast and it's not worth them coming here.
[00:06:35.280 --> 00:06:40.920]   I don't think that, but I just worry that maybe we don't have the ability to talk to
[00:06:40.920 --> 00:06:43.840]   them, we don't have the universal translator, we don't have the right physics.
[00:06:43.840 --> 00:06:45.480]   But sure, they should come.
[00:06:45.480 --> 00:06:46.480]   We are interesting.
[00:06:46.480 --> 00:06:47.480]   I wanna know if they exist.
[00:06:47.480 --> 00:06:50.880]   It would make it easier if they just came.
[00:06:50.880 --> 00:06:56.360]   - So again, I'm gonna use your tweets like it's Shakespeare and analyze it.
[00:06:56.360 --> 00:07:02.000]   So Sarah tweeted, "Thinking about aliens, thinking about aliens."
[00:07:02.000 --> 00:07:07.480]   So how much do you think aliens are thinking about other aliens, including humans?
[00:07:07.480 --> 00:07:15.440]   So you said, "We humans want to visit, we're longing to connect with aliens."
[00:07:15.440 --> 00:07:16.440]   Why is that?
[00:07:16.440 --> 00:07:17.440]   Can you introspect that?
[00:07:17.440 --> 00:07:21.680]   Is that an obvious thing that we should be, what are we hoping to understand by meeting
[00:07:21.680 --> 00:07:24.000]   aliens, exactly?
[00:07:24.000 --> 00:07:28.280]   Something as an introvert, it's like I ask myself this all the time, why go out on a
[00:07:28.280 --> 00:07:30.520]   Friday night to meet people?
[00:07:30.520 --> 00:07:31.520]   What are you hoping to find?
[00:07:31.520 --> 00:07:34.880]   - I think the curiosity, so when I saw Sarah put that tweet, I think I answered it actually
[00:07:34.880 --> 00:07:39.040]   as well, which was, "We are thinking about trying to make contact."
[00:07:39.040 --> 00:07:43.600]   So they almost certainly are, but maybe there's a number of classes.
[00:07:43.600 --> 00:07:49.240]   There are those aliens that have not yet made contact with other aliens, like us.
[00:07:49.240 --> 00:07:53.120]   Those aliens have made contact with just one other alien and maybe it's an anticlimax and
[00:07:53.120 --> 00:07:59.400]   slime, and aliens that have made contact with not just one set of intelligent species, but
[00:07:59.400 --> 00:08:00.400]   several.
[00:08:00.400 --> 00:08:01.400]   That must be amazing actually.
[00:08:01.400 --> 00:08:05.080]   Literally, there are some place in the universe, there must be one alien civilization that's
[00:08:05.080 --> 00:08:10.720]   not made contact with not one, but two other intelligent civilizations.
[00:08:10.720 --> 00:08:12.160]   So they must be thinking about it.
[00:08:12.160 --> 00:08:20.240]   There must be entire degree courses on aliens, thinking about aliens and universal cultural
[00:08:20.240 --> 00:08:21.240]   norms.
[00:08:21.560 --> 00:08:24.640]   - Do you think they will survive the meeting?
[00:08:24.640 --> 00:08:28.680]   And by the way, Lee did respond saying, "That's all the universe wants."
[00:08:28.680 --> 00:08:31.920]   So Sarah said, "Thinking about aliens, thinking about aliens."
[00:08:31.920 --> 00:08:34.440]   Lee said, "That's all the universe wants."
[00:08:34.440 --> 00:08:38.760]   And then Sarah responded, "Cheeky universe we live in."
[00:08:38.760 --> 00:08:43.560]   So cheeky is a cheeky version of the word interesting, all of which we'll try to define
[00:08:43.560 --> 00:08:44.560]   mathematically.
[00:08:44.560 --> 00:08:46.760]   - Cheeky might be harder than interesting.
[00:08:46.760 --> 00:08:48.800]   - 'Cause there's humor in that too.
[00:08:48.800 --> 00:08:49.800]   - Yes.
[00:08:49.800 --> 00:08:53.120]   - I think there's a mathematical definition of humor, but we'll talk about that in a bit.
[00:08:53.120 --> 00:08:54.120]   - Oh, interesting.
[00:08:54.120 --> 00:08:55.120]   - Yeah, sure there is, yeah.
[00:08:55.120 --> 00:09:01.120]   - So if you're a graduate student alien looking at multiple alien civilizations, do you think
[00:09:01.120 --> 00:09:03.560]   they survive the encounters?
[00:09:03.560 --> 00:09:08.160]   - I think there's a tendency to anthropomorphize a lot of the discussions about alien life,
[00:09:08.160 --> 00:09:10.200]   which is a really big challenge.
[00:09:10.200 --> 00:09:15.440]   So usually when I'm trying to think about these problems, I don't try to think about
[00:09:15.440 --> 00:09:19.560]   us as humans, but us as an example of phenomena that exists in the universe that we have yet
[00:09:19.560 --> 00:09:21.800]   to explain.
[00:09:21.800 --> 00:09:28.240]   And it doesn't seem to be the case that if I think about the features I would argue are
[00:09:28.240 --> 00:09:32.640]   most universal about that phenomena, that there's any reason to think that a first encounter
[00:09:32.640 --> 00:09:40.440]   with another lineage or example of life would be antagonistic.
[00:09:40.440 --> 00:09:42.760]   I think, yeah.
[00:09:42.760 --> 00:09:47.880]   And I think there's this kind of assumption, I mean, going back to Neil deGrasse Tyson's
[00:09:47.880 --> 00:09:52.840]   quote, I mean, it kind of bothers me because there's a, I mean, I'm a physicist, so I know
[00:09:52.840 --> 00:09:57.280]   we have a lot of egos about how much we can describe the world, but that there's this
[00:09:57.280 --> 00:10:01.920]   like, because we understand fundamental physics so well, we understand alien life and we can
[00:10:01.920 --> 00:10:02.920]   kind of extrapolate.
[00:10:02.920 --> 00:10:08.960]   And I just think that we don't, and the quest there is really to understand something totally
[00:10:08.960 --> 00:10:11.640]   new about the universe and that thing just happens to be us.
[00:10:11.640 --> 00:10:12.800]   - I agree, I agree.
[00:10:12.800 --> 00:10:13.880]   There's something else more profound.
[00:10:13.880 --> 00:10:17.840]   I think Neil is just being, again, he's just trying to stir the pot.
[00:10:17.840 --> 00:10:23.360]   I would say from a contingency point of view, I want to know how many ways does the universe
[00:10:23.360 --> 00:10:26.520]   build structures, build memories, right?
[00:10:26.520 --> 00:10:30.040]   And then I want to know if those memories can interact with each other.
[00:10:30.040 --> 00:10:35.000]   And if you have two different origins of life and then origins of intelligence, and then
[00:10:35.000 --> 00:10:39.880]   these things become conscious, surely you want to go and talk to them and figure out
[00:10:39.880 --> 00:10:41.560]   what commonalities you share.
[00:10:41.560 --> 00:10:44.960]   And it might be that we're just unable to conceive of what they're going to look like.
[00:10:44.960 --> 00:10:49.480]   They're just going to be completely different infrastructure, but surely we'll want to go
[00:10:49.480 --> 00:10:55.160]   and find out a map and surely curiosity is a property that evolution has made on earth.
[00:10:55.160 --> 00:11:00.240]   And I can't see any reason that it won't happen elsewhere because curiosity probably exists
[00:11:00.240 --> 00:11:03.800]   because we want to find innovations in the environment.
[00:11:03.800 --> 00:11:08.840]   We want to use that information to help our technology.
[00:11:08.840 --> 00:11:12.280]   And also curiosity is like planning for the future, are they going to fight us?
[00:11:12.280 --> 00:11:15.320]   Are we going to be able to trade with them?
[00:11:15.320 --> 00:11:19.440]   So I think that Neil's just, I don't know, maybe, you know, I mean, give a shit.
[00:11:19.440 --> 00:11:22.760]   That's really, I think that's really down on earth, right?
[00:11:22.760 --> 00:11:26.240]   How would aliens categorize humans, do you think?
[00:11:26.240 --> 00:11:27.240]   How would we?
[00:11:27.240 --> 00:11:28.240]   So let's put it the other way around.
[00:11:28.240 --> 00:11:29.240]   Slime category?
[00:11:29.240 --> 00:11:30.760]   Maybe, no, no, no.
[00:11:30.760 --> 00:11:33.400]   Maybe we could, the thing is a bit odd, right?
[00:11:33.400 --> 00:11:36.880]   Look at Instagram, Twitter, all these people taking selfies.
[00:11:36.880 --> 00:11:41.680]   I mean, does the universe, is the ultimate state of consciousness thinking beings that
[00:11:41.680 --> 00:11:45.240]   take photographs themselves and upload them to an internet with other thinking beings
[00:11:45.240 --> 00:11:47.080]   looking at each other's photos?
[00:11:47.080 --> 00:11:49.600]   So I think that they will be-
[00:11:49.600 --> 00:11:50.600]   - What's wrong with that?
[00:11:50.600 --> 00:11:53.680]   - I did not say there was anything wrong with it.
[00:11:53.680 --> 00:11:57.480]   - It's consciousness manifested at scale, selfies on Instagram.
[00:11:57.480 --> 00:12:00.000]   - Yeah, it's like the mirror test at scale.
[00:12:00.000 --> 00:12:05.600]   - Yeah, I do think that curiosity is really the driving force of why we have our technology,
[00:12:05.600 --> 00:12:06.600]   right?
[00:12:06.600 --> 00:12:08.640]   If we weren't curious, we wouldn't have left the cave.
[00:12:08.640 --> 00:12:14.720]   So I think that Neil's got it completely wrong, in fact, actually.
[00:12:14.720 --> 00:12:16.320]   Of course they'd want to come here.
[00:12:16.320 --> 00:12:18.040]   It doesn't mean they are coming here.
[00:12:18.040 --> 00:12:19.440]   We've seen evidence for that.
[00:12:19.440 --> 00:12:22.120]   I guess we can argue about that, right?
[00:12:22.120 --> 00:12:26.800]   But I think that we want, I desperately, and I know that Sarah does too, but I won't speak
[00:12:26.800 --> 00:12:28.400]   for you, you're here.
[00:12:28.400 --> 00:12:32.560]   I desperately want to have missions to look for life in the solar system right now.
[00:12:32.560 --> 00:12:34.920]   I want to map life over the solar system.
[00:12:34.920 --> 00:12:38.880]   And I want to understand how we can go and find life as quickly as possible at the nearest
[00:12:38.880 --> 00:12:44.560]   stars and also at the same time do it in the lab just to compensate.
[00:12:44.560 --> 00:12:45.560]   So sure.
[00:12:45.560 --> 00:12:47.480]   - Yeah, just one more point on this.
[00:12:47.480 --> 00:12:52.920]   If you think about sort of what's driven the most features of our own evolution as a species
[00:12:52.920 --> 00:12:55.040]   you could, and try to map that to alien species.
[00:12:55.040 --> 00:12:58.220]   I always think optimism is what's gonna get us furthest.
[00:12:58.220 --> 00:13:02.280]   And so I think a lot of people always think that it's like war and conflict is gonna be
[00:13:02.280 --> 00:13:06.200]   the way that alien species will expand out into the cosmos.
[00:13:06.200 --> 00:13:09.880]   But if you just look at how we're doing it and how we talk about it, it's always our
[00:13:09.880 --> 00:13:14.480]   future in space is always built from narratives of optimism.
[00:13:14.480 --> 00:13:19.360]   And so it seems to me that if intelligence does get out in the universe that it's gonna
[00:13:19.360 --> 00:13:23.240]   be more optimism and curiosity driving it than war and conflict because those things
[00:13:23.240 --> 00:13:25.620]   end up crushing you.
[00:13:25.620 --> 00:13:28.080]   So there might be some selective filter.
[00:13:28.080 --> 00:13:29.480]   Of course, this is me being an optimist.
[00:13:29.480 --> 00:13:32.000]   I'm a half full kind of person, but.
[00:13:32.000 --> 00:13:36.200]   - Is it obvious that curiosity, not obvious, but what do you think?
[00:13:36.200 --> 00:13:43.440]   Is curiosity a more powerful force in the universe than violence and the will to power?
[00:13:43.440 --> 00:13:50.200]   So 'cause you said you framed curiosity as a way to also plan on how to avoid violence,
[00:13:50.200 --> 00:13:52.680]   which is an interesting framing of curiosity.
[00:13:52.680 --> 00:14:00.600]   But I could also argue that violence is a pretty productive way to operate in the world,
[00:14:00.600 --> 00:14:02.640]   which is like, that's one way to protect yourself.
[00:14:02.640 --> 00:14:05.840]   The best defense is offense.
[00:14:05.840 --> 00:14:07.880]   - I'm not qualified to answer this, but I'll have a go.
[00:14:07.880 --> 00:14:10.720]   I think violence, let's not talk about violence.
[00:14:10.720 --> 00:14:12.240]   - That's the summary of this podcast.
[00:14:12.240 --> 00:14:17.560]   - I would, yeah, maybe, let's not call it violence, but I call it erasure.
[00:14:17.560 --> 00:14:24.480]   So if you think about the way evolution works, or the way, obviously corporate assembly theory,
[00:14:24.480 --> 00:14:29.200]   so if you say you build, curiosity allows you to open up avenues, new graphs, right?
[00:14:29.200 --> 00:14:32.960]   So new features you can play.
[00:14:32.960 --> 00:14:37.600]   What the ability to erase those things allows you to start again and do some pruning.
[00:14:37.600 --> 00:14:40.400]   So the universe, I think curiosity gets you furthest.
[00:14:40.400 --> 00:14:44.920]   Curiosity gets you rockets that land, it gets you robots that can make drugs, it gets you
[00:14:44.920 --> 00:14:48.680]   poetry and art and communication.
[00:14:48.680 --> 00:14:53.480]   And then, I often think, wouldn't it be great in bureaucracy to have another world war,
[00:14:53.480 --> 00:14:57.920]   not literally a world war now, please no world war, but the equivalent so we could get, remove
[00:14:57.920 --> 00:15:01.920]   all the admin bureaucracy, right, all the admin violence, get rid of it and start again.
[00:15:01.920 --> 00:15:03.400]   Do you know what I mean?
[00:15:03.400 --> 00:15:06.320]   Because you get layers and you get redundant systems built.
[00:15:06.320 --> 00:15:14.640]   So actually, a reset, let's not call it violence, a reset in some aspects of our culture and
[00:15:14.640 --> 00:15:20.520]   our technology allows us to then build more important things without the, 'cause how many,
[00:15:20.520 --> 00:15:22.240]   you know, how many cookies do I have to click on?
[00:15:22.240 --> 00:15:27.120]   How many things, how many extra clicks do I have in the future of my life that I could
[00:15:27.120 --> 00:15:33.200]   remove and a bit of a reset would allow us to start again.
[00:15:33.200 --> 00:15:37.320]   And maybe that's how, I suppose, our encounter with aliens will be.
[00:15:37.320 --> 00:15:41.240]   Maybe they will fight with us and say, "Oh, we're not as excited by you as we thought,
[00:15:41.240 --> 00:15:42.240]   we'll just get rid of you."
[00:15:42.240 --> 00:15:44.160]   - Or they might wanna reset Earth.
[00:15:44.160 --> 00:15:45.160]   - Yeah, why not?
[00:15:45.160 --> 00:15:47.440]   - To be like, let's see how the evolution runs again.
[00:15:47.440 --> 00:15:51.840]   This seems like they've, there's nothing new happening here.
[00:15:51.840 --> 00:15:56.040]   They're observing for a while, this is just not, let's keep it more fun.
[00:15:56.040 --> 00:15:57.720]   Let's start with a fish again.
[00:15:57.720 --> 00:16:03.800]   I like how you equated violence to resetting your cookies.
[00:16:03.800 --> 00:16:09.240]   I suppose that's the kind of violence in this modern world where words are violence, resetting
[00:16:09.240 --> 00:16:10.240]   cookies.
[00:16:10.240 --> 00:16:12.320]   - I don't know where that came from.
[00:16:12.320 --> 00:16:13.920]   - That's poetic, really.
[00:16:13.920 --> 00:16:17.800]   Okay, so let's talk about life.
[00:16:17.800 --> 00:16:19.480]   What is life?
[00:16:19.480 --> 00:16:21.100]   What is non-life?
[00:16:21.100 --> 00:16:23.680]   What is the line between life and non-life?
[00:16:23.680 --> 00:16:28.160]   And maybe at any point we can pull in ideas of assembly theory.
[00:16:28.160 --> 00:16:30.120]   How do we start to try to define life?
[00:16:30.120 --> 00:16:37.920]   And for people listening, so Sarah identifies as a physicist and Lee identifies as a chemist.
[00:16:37.920 --> 00:16:44.960]   Of course, they are very interdisciplinary in nature in general.
[00:16:44.960 --> 00:16:45.960]   So what is life?
[00:16:45.960 --> 00:16:46.960]   Sarah.
[00:16:46.960 --> 00:16:47.960]   - Yeah.
[00:16:47.960 --> 00:16:52.040]   - I love asking that question 'cause it's so absurdly big.
[00:16:52.040 --> 00:16:54.200]   - I know, I love it.
[00:16:54.200 --> 00:16:57.080]   It's my absolute favorite question in the whole universe.
[00:16:57.080 --> 00:17:00.680]   So I think I have three ways of describing it right now.
[00:17:00.680 --> 00:17:04.080]   And I like to say all three of them 'cause people latch on to different facets of them.
[00:17:04.080 --> 00:17:08.080]   And so the whole idea of what Lee and I are trying to work on is not to try to define
[00:17:08.080 --> 00:17:11.480]   life but to try to find a more fundamental theory that explains what the phenomena we
[00:17:11.480 --> 00:17:12.480]   call life.
[00:17:12.480 --> 00:17:14.280]   And then it should explain certain attributes.
[00:17:14.280 --> 00:17:17.400]   And you end up having a really different framing than the way people usually talk.
[00:17:17.400 --> 00:17:20.880]   So the way I talk about it, three different ways.
[00:17:20.880 --> 00:17:25.400]   Life is how information structures matter across space and time.
[00:17:25.400 --> 00:17:31.040]   Life is, I don't know, this one's from you actually, simple machines constructing more
[00:17:31.040 --> 00:17:33.080]   complex machines.
[00:17:33.080 --> 00:17:38.720]   And the other one is the physics of existence, so to speak, which is life is the mechanism
[00:17:38.720 --> 00:17:42.720]   the universe has to explore the space of what's possible.
[00:17:42.720 --> 00:17:43.720]   That's my favorite.
[00:17:43.720 --> 00:17:46.560]   - So can I, yeah, yeah, can I add on to that?
[00:17:46.560 --> 00:17:49.720]   - Okay, can you say the physics one again?
[00:17:49.720 --> 00:17:50.720]   - The physics of existence.
[00:17:50.720 --> 00:17:52.160]   - Yeah, the physics of existence.
[00:17:52.160 --> 00:17:53.880]   I don't know what to call it.
[00:17:53.880 --> 00:17:57.520]   If you think of all the things that could exist, only certain things do exist.
[00:17:57.520 --> 00:18:02.740]   And I think life is basically the universe's mechanism of bringing things into physically
[00:18:02.740 --> 00:18:04.880]   existing in the moment now.
[00:18:04.880 --> 00:18:05.880]   - Yeah.
[00:18:05.880 --> 00:18:08.600]   - Yeah, and what's another one?
[00:18:08.600 --> 00:18:09.800]   - We were debating this the other day.
[00:18:09.800 --> 00:18:15.640]   So if you think about a universe that has nothing in it, that's kind of hard to conceive
[00:18:15.640 --> 00:18:16.640]   of, right?
[00:18:16.640 --> 00:18:18.360]   Because, and this is where physicists really go wrong.
[00:18:18.360 --> 00:18:20.960]   They think of a universe with nothing in it, they can't.
[00:18:20.960 --> 00:18:21.960]   And you think--
[00:18:21.960 --> 00:18:23.240]   - So nonexistence is really hard to think of.
[00:18:23.240 --> 00:18:28.400]   - Yeah, and then you think of a universe with everything in it, that's really hard.
[00:18:28.400 --> 00:18:30.320]   And you just have this white blob, right?
[00:18:30.320 --> 00:18:31.320]   This is everything.
[00:18:31.320 --> 00:18:36.320]   But the fact we have discrete stuff in the universe beyond, say, planets, so you've got
[00:18:36.320 --> 00:18:38.640]   stars, space, planet stuff, right?
[00:18:38.640 --> 00:18:40.640]   The boring stuff.
[00:18:40.640 --> 00:18:47.280]   But I would define life or say that life is where there are architectures, any architectures,
[00:18:47.280 --> 00:18:52.320]   and we should stop fixating on what is building the architectures to start with.
[00:18:52.320 --> 00:18:57.880]   And the fact that the universe has discrete things in it is completely mind-blowing.
[00:18:57.880 --> 00:19:04.880]   If you think about it for one second, the fact there's any objects at all, and there's,
[00:19:04.880 --> 00:19:12.960]   because for me, the object is a proxy for a machine that built it, some information
[00:19:12.960 --> 00:19:20.280]   being moved around, actuation, sensing, getting resource, and building these objects.
[00:19:20.280 --> 00:19:25.800]   So for me, everyone's been obsessing about the machine, but I'm like, forget the machine,
[00:19:25.800 --> 00:19:27.480]   let's see the objects.
[00:19:27.480 --> 00:19:32.400]   And I think in a way that assembly theory, we realized maybe a few months ago that assembly
[00:19:32.400 --> 00:19:38.000]   theory actually does account for the soul in the objects, not mystically like, say,
[00:19:38.000 --> 00:19:43.000]   Geldreich's morphic resonance or Leibniz's monodology, seeing souls in things.
[00:19:43.000 --> 00:19:47.600]   But when you see an object, and I've said this before, but this object is evidence of
[00:19:47.600 --> 00:19:51.320]   thought and then there's a lineage of those objects.
[00:19:51.320 --> 00:19:57.360]   So I think what is fascinating is that, you put it much more elegantly, but the barrier
[00:19:57.360 --> 00:20:02.760]   between life and non-life is accruing enough memories to then actuate.
[00:20:02.760 --> 00:20:06.080]   So what that means is there are contingency, there are things that happen in the universe
[00:20:06.080 --> 00:20:10.320]   that get trapped, these memories then have a causal effect on the future.
[00:20:10.320 --> 00:20:13.760]   And then when you get those concentrated in a machine, and you're actually able in real
[00:20:13.760 --> 00:20:20.280]   time, able to integrate the past, the present with the future, and do stuff, that's when
[00:20:20.280 --> 00:20:22.840]   you are most alive.
[00:20:22.840 --> 00:20:23.840]   - You being the machine.
[00:20:23.840 --> 00:20:24.840]   - Yes.
[00:20:24.840 --> 00:20:30.840]   - Wait a minute, why is the object, so one of the ways to define life that Sarah said
[00:20:30.840 --> 00:20:34.360]   is simple machines creating complex machines.
[00:20:34.360 --> 00:20:37.480]   So there's a million questions there.
[00:20:37.480 --> 00:20:40.920]   So how the hell does a simple machine create a complex machine?
[00:20:40.920 --> 00:20:41.920]   - By mutation.
[00:20:41.920 --> 00:20:45.560]   So this is what we were talking about at the beginning, you have the minimum replicator,
[00:20:45.560 --> 00:20:46.560]   so a molecule.
[00:20:46.560 --> 00:20:49.880]   So this is what I was trying to convince Sarah of the mechanism, get there years ago I think,
[00:20:49.880 --> 00:20:55.920]   but then you've been building on it and saying, you have a molecule that can copy itself,
[00:20:55.920 --> 00:20:59.960]   but then there has to be some variability, otherwise it's not gonna get more functional.
[00:20:59.960 --> 00:21:01.960]   So you need to add bits on.
[00:21:01.960 --> 00:21:06.040]   So you have a minimum molecule that can copy itself, but then it can add bits on, and that
[00:21:06.040 --> 00:21:13.080]   can be copied as well, and those add-ons can give you additional function to be able to
[00:21:13.080 --> 00:21:15.920]   acquire more stuff to exist.
[00:21:15.920 --> 00:21:21.480]   So existence is weird, but the fact that there is existence is why there is life, and that's
[00:21:21.480 --> 00:21:27.540]   why I realized a few days ago that there must be, that's why alien life must be everywhere,
[00:21:27.540 --> 00:21:28.920]   because there is existence.
[00:21:28.920 --> 00:21:33.840]   - Is there like a conservation of cheeky stuff happening?
[00:21:33.840 --> 00:21:37.680]   So like, how can you keep injecting more complex things?
[00:21:37.680 --> 00:21:46.200]   Like, doesn't the machine that creates the object need to be as or more powerful than
[00:21:46.200 --> 00:21:48.560]   the things it creates?
[00:21:48.560 --> 00:21:51.920]   So how can you get complexity from simplicity?
[00:21:51.920 --> 00:21:57.800]   - So the way you get complexity from simplicity is that you, I'm just making this up, but
[00:21:57.800 --> 00:22:01.320]   this is kind of my notion, that you have a large volume of stuff, so you're able to get
[00:22:01.320 --> 00:22:08.480]   seeds, if you like, random cues from the environment, so you just use those objects to basically
[00:22:08.480 --> 00:22:17.840]   write on your tape, ones and zeros, whatever, and that is necessarily rich, complex, okay,
[00:22:17.840 --> 00:22:21.680]   but it has a low assembly-ness, but even though it has a high assembly number, we can talk
[00:22:21.680 --> 00:22:26.560]   about that, but then when you start to then integrate that all into a smaller volume,
[00:22:26.560 --> 00:22:33.040]   and over time, and you become more autonomous, you then make the transition.
[00:22:33.040 --> 00:22:34.760]   I don't know what you think about that.
[00:22:34.760 --> 00:22:39.880]   - I think the easiest way to think about it is actually, which I know is a concept you
[00:22:39.880 --> 00:22:43.360]   hate, but I also hate, which is entropy, but people are more familiar with entropy than
[00:22:43.360 --> 00:22:48.880]   what we talk about in assembly theory, and also the idea that, like, say physics as we
[00:22:48.880 --> 00:22:54.560]   know it involves objects that don't exist across time, or as Lee would say, low-memory
[00:22:54.560 --> 00:22:55.560]   objects.
[00:22:55.560 --> 00:22:59.560]   So one of the key distinctions that-- - Low-memory objects.
[00:22:59.560 --> 00:23:03.560]   - Yeah, so physics is all-- - Physicists are low-memory objects.
[00:23:03.560 --> 00:23:04.560]   - Low-memory objects.
[00:23:04.560 --> 00:23:05.560]   - Quick clip.
[00:23:05.560 --> 00:23:09.800]   - Physicists are creators of low-memory objects, or manipulators of low-memory objects.
[00:23:09.800 --> 00:23:10.800]   - Yep, absolutely.
[00:23:10.800 --> 00:23:12.600]   - It's a very nice way of putting it.
[00:23:12.600 --> 00:23:13.600]   Okay, sorry, go ahead.
[00:23:13.600 --> 00:23:14.600]   - Yeah, no, sorry.
[00:23:14.600 --> 00:23:15.600]   - Sorry to keep interrupting.
[00:23:15.600 --> 00:23:16.600]   - No, no, no, it's fine.
[00:23:16.600 --> 00:23:20.880]   I like it, too, it's very funny, but I think it's a good way of phrasing it, because I
[00:23:20.880 --> 00:23:26.760]   think this kind of idea we have in assembly theory is that physics as we know it has basically
[00:23:26.760 --> 00:23:33.160]   removed time as being a physical observable of an object, and the argument I would make
[00:23:33.160 --> 00:23:38.000]   is that when you look at things like water bottles or us, we're actually things that
[00:23:38.000 --> 00:23:44.720]   exist that have a large extent in time, so we actually have a physical size in time,
[00:23:44.720 --> 00:23:50.640]   and we measure that with something called the assembly index in molecules, but presumably
[00:23:50.640 --> 00:23:54.960]   everyone should have sort of a, do you wanna explain what assembly?
[00:23:54.960 --> 00:23:56.560]   - Yeah, you know what?
[00:23:56.560 --> 00:23:59.720]   Let's step back and start at the beginning.
[00:23:59.720 --> 00:24:00.720]   What is assembly theory?
[00:24:00.720 --> 00:24:02.880]   Lee sent me some slides.
[00:24:02.880 --> 00:24:07.520]   There's a big, sexy paper coming out probably, maybe, I don't know.
[00:24:07.520 --> 00:24:10.560]   - We've almost finished it.
[00:24:10.560 --> 00:24:11.720]   - Almost, almost finished it.
[00:24:11.720 --> 00:24:13.720]   - That's also a summary of science.
[00:24:13.720 --> 00:24:14.720]   We're almost done.
[00:24:14.720 --> 00:24:15.720]   - Yes.
[00:24:15.720 --> 00:24:16.720]   - Well, no, no, we're almost done.
[00:24:16.720 --> 00:24:17.720]   - It's the history of science.
[00:24:17.800 --> 00:24:21.360]   - We are ready to start an interesting discussion with our peers.
[00:24:21.360 --> 00:24:25.760]   - Right, you're the machine that created the object, and we'll see what the object takes
[00:24:25.760 --> 00:24:26.760]   us.
[00:24:26.760 --> 00:24:28.640]   All right, so what is assembly theory?
[00:24:28.640 --> 00:24:34.120]   - Yeah, well, I think the easiest way for people to understand it is to think about
[00:24:34.120 --> 00:24:36.520]   assembly in molecules, although the theory is very general.
[00:24:36.520 --> 00:24:40.000]   It doesn't just apply to molecules, and this was really Lee's insight, so it's kind of
[00:24:40.000 --> 00:24:41.800]   funny that I'm explaining it, but--
[00:24:41.800 --> 00:24:42.800]   - I'll mark you.
[00:24:42.800 --> 00:24:43.800]   - Okay, all right, I'm ready, I'm ready, I'm ready.
[00:24:43.800 --> 00:24:47.560]   You have to tell me where I get the check marks minus, but--
[00:24:47.560 --> 00:24:48.560]   - It's your theory as well, so--
[00:24:48.560 --> 00:24:54.840]   - Yeah, I know, but imagine a molecule, and then you can break the molecule apart into
[00:24:54.840 --> 00:24:55.840]   elementary building blocks.
[00:24:55.840 --> 00:24:59.320]   They happen to be bonds, and then you can think of all the ways, for molecular assembly
[00:24:59.320 --> 00:25:02.800]   theory, you can think of all the ways of building up the original molecule, so there's all these
[00:25:02.800 --> 00:25:07.480]   paths that you can assemble it, and the sort of rules of assembly is you can use pieces
[00:25:07.480 --> 00:25:12.360]   that have been generated already, so it has this kind of recursive property to it, and
[00:25:12.360 --> 00:25:16.840]   so that's where kind of memory comes into assembly theory, and then the assembly index
[00:25:16.840 --> 00:25:21.760]   is the shortest path in that space, so it's supposed to be the minimal amount of history
[00:25:21.760 --> 00:25:25.960]   that the universe has to undergo in order to assemble that particular object, and the
[00:25:25.960 --> 00:25:32.520]   reason that this is significant is Lee figured out how to measure that with a mass spec in
[00:25:32.520 --> 00:25:37.560]   the lab, and we had this conjecture that if that minimal number of steps was sufficiently
[00:25:37.560 --> 00:25:41.640]   large, it would indicate that you required a machine or a system that had information
[00:25:41.640 --> 00:25:45.000]   about how to assemble that specific object, because the combinatorial space of possibilities
[00:25:45.000 --> 00:25:49.120]   is getting exponentially large as the assembly index is increasing.
[00:25:49.120 --> 00:25:53.920]   - So just, sorry to interrupt, but so that means there's a sufficiently high assembly
[00:25:53.920 --> 00:26:04.040]   index that if observed in an object, is an indicator that something lifelike created
[00:26:04.040 --> 00:26:06.960]   it, or is the object itself lifelike?
[00:26:06.960 --> 00:26:12.880]   - Both, but you might wanna make the distinction that a water bottle's not life, but it would
[00:26:12.880 --> 00:26:18.520]   still be a signature that you were in that domain of physics, and that I might be alive,
[00:26:18.520 --> 00:26:21.120]   so-- - So there would be potentially
[00:26:21.120 --> 00:26:27.840]   a lot of arguments about where the line, at which assembly index does interesting stuff
[00:26:27.840 --> 00:26:28.840]   start to happen.
[00:26:28.840 --> 00:26:32.840]   - The point is, we can make all the arguments, but it should be experimentally observable,
[00:26:32.840 --> 00:26:36.720]   and Lee can talk more about that part of it, but the point I wanna make about it is, there
[00:26:36.720 --> 00:26:41.160]   was always this intuition that I had that there should be some complexity threshold
[00:26:41.160 --> 00:26:45.800]   in the universe above which you would start to say whatever physics governs life actually
[00:26:45.800 --> 00:26:50.560]   becomes operative, and I think about it a little bit like we have Planck's constant,
[00:26:50.560 --> 00:26:55.800]   and we have the fine structure constant, and then this sort of assembly threshold is basically
[00:26:55.800 --> 00:27:01.760]   another sort of potentially constant of nature, it might depend on specific features of the
[00:27:01.760 --> 00:27:09.040]   system, which we debate about sometimes, but then when you're past that, you have to have
[00:27:09.040 --> 00:27:12.320]   some other explanation than the current explanations we have in physics, 'cause now you're in high
[00:27:12.320 --> 00:27:20.280]   memory, things actually require time for them to exist, and time becomes a physical variable.
[00:27:20.280 --> 00:27:25.440]   - The path to the creation of the object is the memory, so you need to consider that.
[00:27:25.440 --> 00:27:32.320]   - Yeah, but the point is, that's a feature of the object, so when I think of all the
[00:27:32.320 --> 00:27:38.320]   things in this room, we see the projection of them as a water bottle, but assembly theory
[00:27:38.320 --> 00:27:41.960]   would say that this is a causal graph of all the ways the universe can create this thing,
[00:27:41.960 --> 00:27:46.200]   that's what it is as an object, and we're all interacting a causal graph, and most of
[00:27:46.200 --> 00:27:50.720]   the creativity in the biosphere is because a lot of the objects that exist now are huge
[00:27:50.720 --> 00:27:54.960]   in their structure across time, four billion years of evolution to get to us.
[00:27:54.960 --> 00:28:01.600]   - Is it possible to look at me and infer the history that led to me?
[00:28:01.600 --> 00:28:08.560]   - If you, you as an individual, might be hard, you as a representative of a population of
[00:28:08.560 --> 00:28:12.560]   objects that have high assembly, with similar causal history and structure, that you can
[00:28:12.560 --> 00:28:15.760]   communicate with, i.e. other humans, you can infer a lot probably.
[00:28:15.760 --> 00:28:18.640]   - Yeah, also with a-- - Which we do, genomically even, I mean,
[00:28:18.640 --> 00:28:23.360]   it's not like, we have a lot of information in us, we can reconstruct histories from,
[00:28:23.360 --> 00:28:24.840]   probably saying something slightly deeper.
[00:28:24.840 --> 00:28:28.120]   - One thing to add, I mean, it's not just about the object, but the objects that occur,
[00:28:28.120 --> 00:28:32.240]   and not just objects with high assembly number, because you can have random things that have
[00:28:32.240 --> 00:28:36.120]   a high assembly number, but they must have, there must be a number of identical copies,
[00:28:36.120 --> 00:28:40.280]   so you know you're getting away from the random, because you could take a snapshot, this is
[00:28:40.280 --> 00:28:45.000]   why, it's not like I hate entropy, I love entropy, I mean, use correctly, but it's about
[00:28:45.000 --> 00:28:49.960]   the problem of entropy, you have to have a labeler, and so you can label the beginning
[00:28:49.960 --> 00:28:53.840]   and the end, the start and the finish, you know, what you can do in assembly is say,
[00:28:53.840 --> 00:28:58.520]   oh, I have a number of objects in abundance, they all have these features, and then you
[00:28:58.520 --> 00:29:03.080]   can infer, and one of the things that we debated a lot, particularly during lockdown, because
[00:29:03.080 --> 00:29:07.160]   I almost went insane trying to crush the, produce the assembly equation, so we came
[00:29:07.160 --> 00:29:12.480]   up with the assembly equation, I had, just imagine this, so you have this string, where,
[00:29:12.480 --> 00:29:18.760]   oh, actually it makes me sick trying to remember, it was so, it did my head in for a long time.
[00:29:18.760 --> 00:29:19.760]   - Dramatic.
[00:29:19.760 --> 00:29:24.960]   - Yeah, because I couldn't, so if you just have a string of say, words, say, you know,
[00:29:24.960 --> 00:29:29.120]   a series of words, a series of letters, so you just have A, A, A, B, B, B, C, C, C, D,
[00:29:29.120 --> 00:29:34.760]   D, D, and you find that object, and you just have four A's, four B's, four C's, four D's,
[00:29:34.760 --> 00:29:39.480]   together, boom, then, and that, you measured that, you physically measured that string
[00:29:39.480 --> 00:29:46.080]   of letters, then what you could do is you can infer sub-graphs of maybe the four A's,
[00:29:46.080 --> 00:29:50.080]   the four B's, the four C's, and the four C's, but you don't see them in the real world,
[00:29:50.080 --> 00:29:55.080]   you just infer them, and I really got stuck with that, because there's a problem, to try
[00:29:55.080 --> 00:30:00.480]   and work out what's the difference between a long, you know, physical object and the
[00:30:00.480 --> 00:30:04.240]   assembly space of the objects, so that we realised the best way to put that is infer
[00:30:04.240 --> 00:30:09.120]   in time, that, so although we can't infer your entire history, we know at some point
[00:30:09.120 --> 00:30:13.080]   the four A's were made, the four B's were made, the four C's were made, the four D's
[00:30:13.080 --> 00:30:17.720]   were made, and they all got added together, and that's one really interesting thing that's
[00:30:17.720 --> 00:30:25.920]   come out of the theory, but the killer, when we knew we were going beyond standard complexity
[00:30:25.920 --> 00:30:30.880]   theories, it was incredibly successful, is that we realised we could start to measure
[00:30:30.880 --> 00:30:36.580]   these things for real across domains, so the assembly index is actually an intrinsic property
[00:30:36.580 --> 00:30:43.320]   of all stuff that you can break into components, particularly molecules are good, because you
[00:30:43.320 --> 00:30:49.240]   can break them up into smaller molecules, into atoms, the challenge will be making that
[00:30:49.240 --> 00:30:52.440]   more general across all the domains, but we're working on it right now, and I think the theory
[00:30:52.440 --> 00:30:53.440]   will do that.
[00:30:53.440 --> 00:30:58.920]   - So components, domains, so you're talking about basically measuring the complexity of
[00:30:58.920 --> 00:31:06.240]   an object in what, biology, chemistry, physics, that's what you mean by domains?
[00:31:06.240 --> 00:31:10.520]   - Complexity of tests, complexity of computers, complexity of memes, you know.
[00:31:10.520 --> 00:31:11.520]   - Memes?
[00:31:11.520 --> 00:31:12.520]   - Yep.
[00:31:12.520 --> 00:31:13.520]   - What is that, ideas?
[00:31:13.520 --> 00:31:15.520]   - Yeah, I mean, so one of the--
[00:31:15.520 --> 00:31:17.160]   - Ideas are objects in assembly theory.
[00:31:17.160 --> 00:31:18.160]   - Yeah.
[00:31:18.160 --> 00:31:19.160]   - They are.
[00:31:19.160 --> 00:31:21.440]   - They're physical things, they're just features of the causal graph, I mean, the fact I can
[00:31:21.440 --> 00:31:27.380]   talk to you right now is because we're exchanging structure of our assembly space.
[00:31:27.380 --> 00:31:35.240]   - So conversation is the exchanging structures in assembly space, what is assembly space?
[00:31:35.240 --> 00:31:39.080]   - When I started working on Origins of Life, I was writing about something called top-down
[00:31:39.080 --> 00:31:43.000]   causation, which a lot of philosophers are interested in, and people that worry about
[00:31:43.000 --> 00:31:49.840]   the mind-body problem, but the whole idea is, if we have, the microscopic world of physics
[00:31:49.840 --> 00:31:54.280]   is causally complete, it seems like there's no room for higher-level causes, like our
[00:31:54.280 --> 00:31:59.480]   thoughts to actually have any impact on the world, and that seems problematic when you
[00:31:59.480 --> 00:32:04.360]   get to studying life and mind, because it does seem that, quote-unquote, emergent properties
[00:32:04.360 --> 00:32:09.200]   do matter to matter.
[00:32:09.200 --> 00:32:12.480]   And then there's this other sort of paradoxical situation where information looks like it's
[00:32:12.480 --> 00:32:16.280]   disembodied, so we talk about information like it can just move from any physical system
[00:32:16.280 --> 00:32:21.960]   to any other physical system, and it doesn't require, like, you don't have to specify anything
[00:32:21.960 --> 00:32:24.480]   about the substrate to talk about information.
[00:32:24.480 --> 00:32:29.080]   And then there's also, the way we talk about mathematics is also disembodied, right, like
[00:32:29.080 --> 00:32:35.560]   the platonic world of forms, and I think all of those things are hinting that we really
[00:32:35.560 --> 00:32:42.720]   don't know how to think about abstractions as physical things, and really, I think what
[00:32:42.720 --> 00:32:47.320]   assembly theory is pointing to is what we're missing there is the dimension of time, and
[00:32:47.320 --> 00:32:53.000]   if you actually look at an object being extended across time, what we call information and
[00:32:53.000 --> 00:32:57.680]   the things that look abstract are things that are entangled in the histories of those objects.
[00:32:57.680 --> 00:33:00.200]   They're features of the overlapping assembly space.
[00:33:00.200 --> 00:33:06.080]   So they look abstract 'cause they're not part of the current structure, but they're part
[00:33:06.080 --> 00:33:10.120]   of the structure if you thought about it as the philosophical concept of a hyperobject,
[00:33:10.120 --> 00:33:14.120]   an object that's too big in time for us to actually to resolve.
[00:33:14.120 --> 00:33:19.240]   And so I think information's physical, it's just physical in time, not in space.
[00:33:19.240 --> 00:33:24.880]   - Too hyperobject, too difficult for us to resolve, so we're supposed to think about
[00:33:24.880 --> 00:33:30.160]   of life as this thing that stretches through time, and there's a causation chain that led
[00:33:30.160 --> 00:33:37.040]   to that thing, and then you're trying to measure something with the assembly index about--
[00:33:37.040 --> 00:33:41.960]   - The assembly index is the ordering, you could think of it as a partial ordering of
[00:33:41.960 --> 00:33:44.000]   all the things that can happen.
[00:33:44.000 --> 00:33:48.320]   So in thermodynamics, we coarse-grain things by temperature and pressure.
[00:33:48.320 --> 00:33:52.960]   In assembly theory, we coarse-grain by the number of copies of an object and the assembly
[00:33:52.960 --> 00:33:56.560]   index, which is basically, if you think of the space of all possible things, it's like
[00:33:56.560 --> 00:34:00.600]   a depth of how far you've gone into that space and how much time was required to get there.
[00:34:00.600 --> 00:34:04.320]   - In the shortest possible version.
[00:34:04.320 --> 00:34:07.080]   Not average, 'cause can't you just 3D--
[00:34:07.080 --> 00:34:10.920]   - You're gonna kill me with that question.
[00:34:10.920 --> 00:34:13.720]   - Not 3D, can't you always 3D print the thing?
[00:34:13.720 --> 00:34:15.240]   - That's like stabbing in the heart.
[00:34:15.240 --> 00:34:18.960]   - No, because I had such fights, so Sarah's team and my team are writing this paper at
[00:34:18.960 --> 00:34:19.960]   the moment, and--
[00:34:19.960 --> 00:34:21.480]   - It's so funny.
[00:34:21.480 --> 00:34:24.480]   - I think we kinda share the, at the beginning you were like, "No, that's not right, oh yeah,
[00:34:24.480 --> 00:34:26.320]   that's right," and we're doing this for a bit.
[00:34:26.320 --> 00:34:30.400]   And then the problem is when you build a theory and build the intuition, there's some certain
[00:34:30.400 --> 00:34:35.760]   features of the theory that almost felt like I was being religious about, saying, "Right,
[00:34:35.760 --> 00:34:36.760]   you have to do this.
[00:34:36.760 --> 00:34:40.760]   A good assembly theorist does this, does this, does this."
[00:34:40.760 --> 00:34:44.960]   And Sarah's post-doc Daniel and my post-doc Abhishek, and they were both--
[00:34:44.960 --> 00:34:45.960]   - We're both brilliant.
[00:34:45.960 --> 00:34:49.440]   - They're brilliant, but they were both like, "No, we don't buy that."
[00:34:49.440 --> 00:34:51.920]   And I was like, "It is, is."
[00:34:51.920 --> 00:34:56.960]   They were like, "Well, Lee, actually, I thought you were the first to say that if you can't
[00:34:56.960 --> 00:35:00.400]   explain it, and you can't do an experiment, that it doesn't exist."
[00:35:00.400 --> 00:35:03.720]   And that saved me, and I said to Abhishek, Abhishek's my post-doc in Glasgow, Daniel
[00:35:03.720 --> 00:35:09.720]   is Sarah's post-doc in ASU, I was like, "I have the experimental data, so when I basically
[00:35:09.720 --> 00:35:14.000]   take the molecules and chop them up in the mass spec, the assembly number is never the
[00:35:14.000 --> 00:35:16.920]   average, it's always the shortest, it's an intrinsic property."
[00:35:16.920 --> 00:35:18.960]   And then the penny dropped for Abhishek, he said, "Okay."
[00:35:18.960 --> 00:35:23.560]   'Cause I had these things that we had to believe to start with, or to trust, and then we've
[00:35:23.560 --> 00:35:27.040]   done the math, and it comes out, and they now have the shortest path, actually, it's
[00:35:27.040 --> 00:35:29.360]   up, it explains why the shortest path.
[00:35:29.360 --> 00:35:33.040]   Here's why the shortest path's important, not the average.
[00:35:33.040 --> 00:35:37.800]   Shortest path needs you to identify when the universe has basically got a memory, not an
[00:35:37.800 --> 00:35:38.800]   average.
[00:35:38.800 --> 00:35:43.880]   So what you wanna be able to do is to say, "What is the minimum number of features that
[00:35:43.880 --> 00:35:45.620]   I want to be able to see in the universe?
[00:35:45.620 --> 00:35:52.280]   When I find those features, I know the universe has had a coherent memory, and is basically
[00:35:52.280 --> 00:35:54.320]   alive."
[00:35:54.320 --> 00:35:56.840]   And so, that gives you the lower bound.
[00:35:56.840 --> 00:36:01.240]   So that's like, of course there's gonna be other paths, we can be more ridiculous, right?
[00:36:01.240 --> 00:36:03.560]   We can have other paths, but it's just the minimum.
[00:36:03.560 --> 00:36:08.760]   So probabilistically, at the beginning, because assembly theory was built as a measure for
[00:36:08.760 --> 00:36:12.000]   biosignatures, I needed to go there.
[00:36:12.000 --> 00:36:15.820]   And then I realized it was intrinsic, and then Sarah realized it was intrinsic, and
[00:36:15.820 --> 00:36:19.980]   these hyperobjects were coming, and we were kind of fusing that notions together.
[00:36:19.980 --> 00:36:25.740]   And then the team were like, "Yeah, but if I have enough energy, and I have enough resources,
[00:36:25.740 --> 00:36:27.060]   I might not take the shortest path.
[00:36:27.060 --> 00:36:28.060]   I might go a bit longer.
[00:36:28.060 --> 00:36:33.220]   I might take a really long path, because it allows me then to do something else."
[00:36:33.220 --> 00:36:38.340]   So what you can do is, let's say I've got two different objects, A and B, and they both
[00:36:38.340 --> 00:36:43.940]   have different shortest paths to get them, but then, if you want to make A and B together,
[00:36:43.940 --> 00:36:44.940]   they will have a compromise.
[00:36:44.940 --> 00:36:49.740]   So in the joint assembly space, that might be an average, but actually it's the shortest
[00:36:49.740 --> 00:36:54.780]   way you can make both A and B with a minimum amount of resource in time.
[00:36:54.780 --> 00:36:59.860]   So suddenly you then layer these things up, and so the average becomes not important,
[00:36:59.860 --> 00:37:05.460]   but as you literally overlap those sets, you get a new shortest path.
[00:37:05.460 --> 00:37:09.220]   And so what we realized time and time again when we're doing the math, the shortest path
[00:37:09.220 --> 00:37:13.460]   is intrinsic, is fundamental, and is measurable, which is kind of mind-blowing.
[00:37:13.460 --> 00:37:19.460]   - So what we're talking about, some basic ingredients, maybe we'll talk about that,
[00:37:19.460 --> 00:37:23.940]   what those basic ingredients could be, and how many steps, when you say shortest path,
[00:37:23.940 --> 00:37:32.300]   how many steps it takes to turn those basic ingredients into the final meal.
[00:37:32.300 --> 00:37:35.020]   So what's the shortest way to make a pizza?
[00:37:35.020 --> 00:37:36.020]   - Or a pie.
[00:37:36.020 --> 00:37:37.020]   - Or a pie.
[00:37:37.020 --> 00:37:38.020]   - An apple pie.
[00:37:38.020 --> 00:37:39.020]   - An apple pie, that's right.
[00:37:39.020 --> 00:37:40.020]   - And a pizza and a pie together.
[00:37:40.020 --> 00:37:41.020]   - From scratch.
[00:37:41.020 --> 00:37:42.020]   - Yeah.
[00:37:42.020 --> 00:37:45.140]   - So there's a lot of ways.
[00:37:45.140 --> 00:37:49.620]   There's the shortest way, and you take the full spectrum of ways, and there's probably
[00:37:49.620 --> 00:37:56.500]   an average duration for a noob to make an apple pie.
[00:37:56.500 --> 00:37:58.460]   Is the average interesting still?
[00:37:58.460 --> 00:38:05.100]   If you measure the average length of the path to assemble a thing, does that tell you something
[00:38:05.100 --> 00:38:14.020]   about the way nature usually does it, versus something fundamental about the object, which
[00:38:14.020 --> 00:38:16.460]   I think is what you're aiming at with the assembly index.
[00:38:16.460 --> 00:38:19.540]   - Yeah, I mean look, we all have to quantify things.
[00:38:19.540 --> 00:38:22.580]   The minimum path gives you the lower bounds, you know you're detecting something, you know
[00:38:22.580 --> 00:38:24.180]   you're inferring something.
[00:38:24.180 --> 00:38:29.620]   The average tells you about really how the objects are existing in the ecosystem or the
[00:38:29.620 --> 00:38:38.860]   technology, and there has to be more paths explored, because then you can happen upon
[00:38:38.860 --> 00:38:41.220]   other memories, and then condense them down.
[00:38:41.220 --> 00:38:45.260]   I'm not making too much sense, but if you look at, say, let's just say, I mean maybe
[00:38:45.260 --> 00:38:47.380]   we're gonna get to alien civilizations later, right?
[00:38:47.380 --> 00:38:54.300]   But I would argue very strongly that alien civilization A and alien civilization B, they're
[00:38:54.300 --> 00:38:57.860]   different assembly spaces, so they're kind of gonna be a bit messed up if they happen
[00:38:57.860 --> 00:39:02.420]   upon one another, only when they find some joint overlap in their technology, 'cause
[00:39:02.420 --> 00:39:06.900]   if aliens come to us and they don't share any of the causal graph we've showed, but
[00:39:06.900 --> 00:39:12.100]   hopefully they share the periodic table, and bonds and things, that we're gonna have to
[00:39:12.100 --> 00:39:17.500]   really think about the language to talk to us aliens by inferring, by using assembly
[00:39:17.500 --> 00:39:24.580]   theory to infer their language, their technology, and other bits and bobs, and the shortest
[00:39:24.580 --> 00:39:25.940]   path will help you do that quickly.
[00:39:25.940 --> 00:39:32.340]   - All right, so all aliens in this causality graphs have a common ancestor in the--
[00:39:32.340 --> 00:39:35.780]   - If the building blocks are the same, which means they live in the same universe as us.
[00:39:35.780 --> 00:39:36.780]   - So this is the assumption.
[00:39:36.780 --> 00:39:38.580]   - It depends on how far back in time you go, though.
[00:39:38.580 --> 00:39:42.140]   But the universe has all the same building blocks.
[00:39:42.140 --> 00:39:43.140]   - Yeah.
[00:39:43.140 --> 00:39:45.900]   - And we have to assume that.
[00:39:45.900 --> 00:39:53.580]   So there's not different classes of causality graphs, right?
[00:39:53.580 --> 00:39:58.740]   The universe doesn't just say, here, you get the red causality graph, and then you get
[00:39:58.740 --> 00:39:59.740]   the blue one.
[00:39:59.740 --> 00:40:04.740]   These basic ingredients, and they're geographically constrained, or constrained in space or time,
[00:40:04.740 --> 00:40:06.220]   or something like that.
[00:40:06.220 --> 00:40:11.860]   - They're constrained in time, 'cause only by the virtue of the fact that you need enough
[00:40:11.860 --> 00:40:14.720]   time to have passed for some things to exist.
[00:40:14.720 --> 00:40:17.300]   So the universe has to be big enough in time for some things.
[00:40:17.300 --> 00:40:20.260]   So just to one point on the shortest path versus the average path, which I think we'll
[00:40:20.260 --> 00:40:24.980]   get to this, is you had a nice way of saying it's like the minimal compression is the shortest
[00:40:24.980 --> 00:40:26.740]   path for the universe to produce that.
[00:40:26.740 --> 00:40:31.060]   But it's also like the first time in the ordering of events that you might expect to see that
[00:40:31.060 --> 00:40:32.660]   object.
[00:40:32.660 --> 00:40:39.460]   But the average path tells you something about the actual steps that were realized, and that
[00:40:39.460 --> 00:40:43.860]   becomes an emergent property of that object's interaction with other objects.
[00:40:43.860 --> 00:40:47.620]   So it's not an intrinsic feature of that object, it's a feature of the interactions with other
[00:40:47.620 --> 00:40:48.620]   things.
[00:40:48.620 --> 00:40:51.660]   And so one of the nice features of assembly is you've basically gotten rid of, you just
[00:40:51.660 --> 00:40:54.900]   look at the things that exist, and you've gotten rid of the mechanisms for constructing
[00:40:54.900 --> 00:40:56.340]   them in some sense.
[00:40:56.340 --> 00:41:03.140]   Like the machines are not as important in the current construction of the theory, although
[00:41:03.140 --> 00:41:08.140]   I would like to bridge it to some ideas about constructors.
[00:41:08.140 --> 00:41:13.620]   But then you can only communicate with things as Lee was saying if you have some overlap
[00:41:13.620 --> 00:41:15.160]   in the past history.
[00:41:15.160 --> 00:41:19.620]   So if you had an alien species that had absolutely no overlap, then there would be no means of
[00:41:19.620 --> 00:41:21.380]   communication.
[00:41:21.380 --> 00:41:27.500]   But as we progress further and further in time, and more things become possible because
[00:41:27.500 --> 00:41:32.000]   the assembly spaces are larger, because you can have a larger assembly space in terms
[00:41:32.000 --> 00:41:36.580]   of index and also just the size of the space, because it's exponentially growing, then more
[00:41:36.580 --> 00:41:38.340]   things can happen in the future.
[00:41:38.340 --> 00:41:42.700]   And the example I like to give is actually when we made first contact with gravitational
[00:41:42.700 --> 00:41:47.660]   waves, because that's an alien phenomenon that's been permeating our planet, not alien
[00:41:47.660 --> 00:41:52.100]   in life, but alien, like something we had never knew existed.
[00:41:52.100 --> 00:41:57.540]   It's been like, there's gravitational waves rippling through this room right now.
[00:41:57.540 --> 00:42:03.300]   But we had to advance to the level of Einstein writing down his theory of relativity, and
[00:42:03.300 --> 00:42:08.620]   then 100 years of technological development to even quote unquote see that phenomena.
[00:42:08.620 --> 00:42:15.420]   - So the, okay, to see that phenomena, our causal graph had to start intersecting.
[00:42:15.420 --> 00:42:19.100]   - Yeah, we needed the idea to emerge first, the abstraction, right?
[00:42:19.100 --> 00:42:23.820]   And then we had to build the technology that could actually observe features of that abstraction.
[00:42:23.820 --> 00:42:28.900]   - So the nice promising thing is over time the graph can grow so it can start overlapping
[00:42:28.900 --> 00:42:29.900]   eventually.
[00:42:29.900 --> 00:42:34.020]   - Yeah, so the interesting feature of that graph is there was an event 1.4 billion years
[00:42:34.020 --> 00:42:41.660]   away of a black hole merger that we detected on our detector, and now suddenly we're connected
[00:42:41.660 --> 00:42:46.220]   through this communication channel with this distant event in our universe that if you
[00:42:46.220 --> 00:42:49.900]   think about 1.4 billion years ago what was happening on this planet, or even further
[00:42:49.900 --> 00:42:55.580]   back in time, that there's common physics underlying all those events, but even for
[00:42:55.580 --> 00:42:58.020]   those two events to communicate with-- - Now I understand what you were going on
[00:42:58.020 --> 00:42:59.020]   about the other week.
[00:42:59.020 --> 00:43:00.020]   - Yeah, I'm sorry.
[00:43:00.020 --> 00:43:01.020]   - Yeah, yeah, yeah.
[00:43:01.020 --> 00:43:03.260]   - It's a really abstract example, but it's sort of--
[00:43:03.260 --> 00:43:05.100]   - Your causal graphs are now overlapping.
[00:43:05.100 --> 00:43:09.500]   - Yeah, so, well, let's just say now our causal graphs are overlapping in the deep past.
[00:43:09.500 --> 00:43:12.300]   - Oh, I like it, so you made it-- - I totally missed it.
[00:43:12.300 --> 00:43:13.820]   - Oh, well, the 1.6 billion.
[00:43:13.820 --> 00:43:14.820]   - You made a connection with it.
[00:43:14.820 --> 00:43:15.820]   No, I do like that.
[00:43:15.820 --> 00:43:18.620]   - No, you can tell me what your epiphany is now, that's good.
[00:43:18.620 --> 00:43:21.140]   - Because I was-- - And I should get the jokes before 30 seconds
[00:43:21.140 --> 00:43:22.140]   after, so.
[00:43:22.140 --> 00:43:23.820]   - Oh, I get it now.
[00:43:23.820 --> 00:43:25.900]   - No, it's all right, I was slow.
[00:43:25.900 --> 00:43:27.260]   - The joke came two minutes ago.
[00:43:27.260 --> 00:43:28.580]   - I'm slow on the uptake here.
[00:43:28.580 --> 00:43:32.260]   - I wasn't able to comprehend what you were talking about when saying the channel communicating
[00:43:32.260 --> 00:43:38.620]   to the past, but what you're saying is we were able to infer what happened 1.4 billion
[00:43:38.620 --> 00:43:40.380]   years ago.
[00:43:40.380 --> 00:43:41.660]   We detected the gravity wave.
[00:43:41.660 --> 00:43:46.060]   I mean, I think it's amazing that at that time we were just becoming multicellular,
[00:43:46.060 --> 00:43:47.060]   right?
[00:43:47.060 --> 00:43:48.540]   It's like insane.
[00:43:48.540 --> 00:43:55.260]   And then we progressed from multicellularity through to technology and built the detector
[00:43:55.260 --> 00:43:58.980]   and then we just extrapolate backwards.
[00:43:58.980 --> 00:44:02.740]   So although we didn't do anything back to the graph back in time, we understood its
[00:44:02.740 --> 00:44:04.540]   existence and overlapped going forward.
[00:44:04.540 --> 00:44:06.500]   - Well, that's because our graphs are larger.
[00:44:06.500 --> 00:44:09.300]   - Yeah, but that means that has a consequence.
[00:44:09.300 --> 00:44:15.540]   One of the things I was trying to say is I think, I don't know, Sarah might be, she can
[00:44:15.540 --> 00:44:19.740]   correct me, information first and I'm a object first kind of guy.
[00:44:19.740 --> 00:44:23.660]   So I mean, there's things that get constructed, there has to be this transition in random
[00:44:23.660 --> 00:44:25.420]   constructions.
[00:44:25.420 --> 00:44:32.220]   So when the object that's being constructed by the process bakes in that memory and those
[00:44:32.220 --> 00:44:35.700]   memories then add on and add on and add on.
[00:44:35.700 --> 00:44:40.740]   So as it becomes more competent in life is about taking those memories and compressing
[00:44:40.740 --> 00:44:43.860]   them increasing their autonomy.
[00:44:43.860 --> 00:44:47.580]   And so I think that, you know, like the cell that we have in biology on earth is our way
[00:44:47.580 --> 00:44:52.580]   of doing that, that really the maximum ability to take memories and to act on the future.
[00:44:52.580 --> 00:44:55.700]   - Oh, I think that's mathematics.
[00:44:55.700 --> 00:44:58.140]   - No, mathematics doesn't exist.
[00:44:58.140 --> 00:44:59.500]   - No, but that's the point.
[00:44:59.500 --> 00:45:02.060]   The point is that abstractions do exist.
[00:45:02.060 --> 00:45:03.180]   They're real physical things.
[00:45:03.180 --> 00:45:10.660]   We call them abstractions, but the point about mathematics that I think is, so I don't disagree.
[00:45:10.660 --> 00:45:15.140]   I think you're object first and I'm information first, but I think I'm only information first
[00:45:15.140 --> 00:45:20.660]   in the sense that I think the thing that we need to explain is what abstractions are and
[00:45:20.660 --> 00:45:24.820]   what they are as physical things because of all of human history, we've thought that there
[00:45:24.820 --> 00:45:29.900]   were these properties that are disembodied exist outside of the universe.
[00:45:29.900 --> 00:45:34.260]   And really they do exist in the universe and we just don't understand what their physics
[00:45:34.260 --> 00:45:35.260]   is.
[00:45:35.260 --> 00:45:36.820]   So I think mathematics is a really good example.
[00:45:36.820 --> 00:45:43.180]   We do theoretical physics with math, but imagine doing physics of math and then thinking about
[00:45:43.180 --> 00:45:46.340]   math as a physical object and math is super interesting.
[00:45:46.340 --> 00:45:49.820]   I think this is why we think it describes reality so well because it's the most copyable
[00:45:49.820 --> 00:45:50.980]   kind of information.
[00:45:50.980 --> 00:45:54.900]   It retains its properties when you move it between physical media, which means that it's
[00:45:54.900 --> 00:45:59.900]   very deep and so it seems to describe the universe really well, but it probably is because
[00:45:59.900 --> 00:46:06.220]   it's information that's very deep in our past and it's just, we invented a way of communicating
[00:46:06.220 --> 00:46:09.460]   it very effectively between us.
[00:46:09.460 --> 00:46:11.420]   Isn't math more fundamental?
[00:46:11.420 --> 00:46:16.380]   Isn't the assembly of the graph, isn't basically, I'm going to sound, I sound completely boring.
[00:46:16.380 --> 00:46:19.660]   It's like math, assembly theory invented math, but it did.
[00:46:19.660 --> 00:46:21.780]   It has to be.
[00:46:21.780 --> 00:46:22.780]   Okay.
[00:46:22.780 --> 00:46:29.140]   So what, what is, uh, what is math exactly?
[00:46:29.140 --> 00:46:38.140]   It's a, uh, a nice simplification, a simple description of what?
[00:46:38.140 --> 00:46:40.740]   So we have a computer scientist, a physicist and the chemist here.
[00:46:40.740 --> 00:46:42.060]   Walk into a bar.
[00:46:42.060 --> 00:46:45.660]   I think the chemist is going to define math and you guys can correct me.
[00:46:45.660 --> 00:46:46.660]   Go for it.
[00:46:46.660 --> 00:46:49.660]   I would say, lay it honestly.
[00:46:49.660 --> 00:46:50.860]   We're ready.
[00:46:50.940 --> 00:46:57.540]   I think the ability to, um, to label objects and, uh, and place them into classes and then
[00:46:57.540 --> 00:47:01.100]   do operations on the objects is what math is.
[00:47:01.100 --> 00:47:07.060]   So on that point, what does it mean to be object first versus information first?
[00:47:07.060 --> 00:47:11.220]   So what, what's the difference between object and information when you get to that low fundamental
[00:47:11.220 --> 00:47:12.220]   level?
[00:47:12.220 --> 00:47:14.060]   Well, I might change my view.
[00:47:14.060 --> 00:47:16.580]   So I'm stuff first, the stuff.
[00:47:16.580 --> 00:47:22.660]   And then when stuff becomes objects, it has to invent information and then the information
[00:47:22.660 --> 00:47:24.900]   acts on more stuff and becomes more objects.
[00:47:24.900 --> 00:47:29.780]   So I think there is a transition to information that occurs when you go from stuff to objects.
[00:47:29.780 --> 00:47:31.660]   I just mean time though.
[00:47:31.660 --> 00:47:32.660]   Information is emergent.
[00:47:32.660 --> 00:47:35.140]   Not emergent.
[00:47:35.140 --> 00:47:40.480]   Information is actionable memories from the universe.
[00:47:40.480 --> 00:47:44.900]   So when, when memories become actionable, that's information.
[00:47:44.900 --> 00:47:47.460]   But there's always memory, but it's not actionable.
[00:47:47.460 --> 00:47:48.700]   Yeah.
[00:47:48.700 --> 00:47:49.700]   And then it's not information.
[00:47:49.700 --> 00:47:50.700]   Great.
[00:47:50.700 --> 00:47:52.380]   And actionable is what you can create.
[00:47:52.380 --> 00:47:53.740]   You can use it.
[00:47:53.740 --> 00:47:55.580]   If you can't use it, then it's not information.
[00:47:55.580 --> 00:47:59.180]   If you can't transmit it, if you, if it doesn't have any causal consequence.
[00:47:59.180 --> 00:48:00.420]   Falls in the forest.
[00:48:00.420 --> 00:48:03.620]   I don't understand why is that not information?
[00:48:03.620 --> 00:48:04.620]   It's not information.
[00:48:04.620 --> 00:48:10.180]   It's, it's, um, it's, uh, it's stuff happening, but it's not, it's not causal.
[00:48:10.180 --> 00:48:11.180]   Yeah.
[00:48:11.180 --> 00:48:12.180]   Yeah.
[00:48:12.180 --> 00:48:13.180]   We can, this is cool.
[00:48:13.180 --> 00:48:14.180]   But it's happening.
[00:48:14.180 --> 00:48:15.180]   It requires information.
[00:48:15.180 --> 00:48:16.180]   No, no, no, no, no.
[00:48:16.180 --> 00:48:18.180]   Stuff is always happening.
[00:48:18.180 --> 00:48:21.940]   No, this is where the physicists get and the mathematicians get themselves in a loop because
[00:48:21.940 --> 00:48:28.180]   I think the universe, I mean, I think say Max Tegmark and, and is very playful and say
[00:48:28.180 --> 00:48:30.180]   like the universe, universe, just math.
[00:48:30.180 --> 00:48:31.500]   Well, the universe is just math.
[00:48:31.500 --> 00:48:34.980]   Then we might as well not bother having any conversation because the conversation already
[00:48:34.980 --> 00:48:37.540]   written, we just might as well go to the future and say, can you just give us the conversations
[00:48:37.540 --> 00:48:38.780]   happened already?
[00:48:38.780 --> 00:48:43.820]   So I think the problem is that mathematicians are so successful at labeling stuff and so
[00:48:43.820 --> 00:48:46.340]   successful understanding of stuff through those labels.
[00:48:46.340 --> 00:48:51.980]   They forget that actually those labels had to emerge and that information had to be built
[00:48:51.980 --> 00:48:52.980]   on those memories.
[00:48:52.980 --> 00:48:57.820]   So memory in the universe, so constraints graph, when they become actionable and the
[00:48:57.820 --> 00:49:03.100]   graph can loop back on itself or interact with other graphs and they can intersect those
[00:49:03.100 --> 00:49:06.020]   memories become actionable and therefore their information.
[00:49:06.020 --> 00:49:10.540]   And I think you just changed my son, my, my mind on something pretty big, but I'd have
[00:49:10.540 --> 00:49:14.580]   a pen so I can't write, I'm going to write it down later, but roughly the idea is, is
[00:49:14.580 --> 00:49:20.660]   like you've got these, these two graphs of objects of stuff that you have memories and
[00:49:20.660 --> 00:49:25.580]   then when they intersect and then they can act on each other, that's maybe the mechanism
[00:49:25.580 --> 00:49:29.380]   by which information is then, so then you can then abstract.
[00:49:29.380 --> 00:49:33.340]   So when one graph can then build another graph and say, Hey, you don't have to go through
[00:49:33.340 --> 00:49:34.940]   the nonsense we had to go through.
[00:49:34.940 --> 00:49:36.340]   Here's literally the way to do it.
[00:49:36.340 --> 00:49:40.140]   Stuff always comes first, but then when stuff builds the abstraction, the abstraction can
[00:49:40.140 --> 00:49:42.220]   be then teleported onto other stuff.
[00:49:42.220 --> 00:49:43.220]   Abstractions is the looping back.
[00:49:43.220 --> 00:49:44.220]   Yeah.
[00:49:44.220 --> 00:49:45.220]   Or, okay.
[00:49:45.220 --> 00:49:47.820]   Am I making, I dunno, I got stuck.
[00:49:47.820 --> 00:49:48.820]   Yeah.
[00:49:48.820 --> 00:49:52.460]   So first a God made stuff.
[00:49:52.460 --> 00:49:57.460]   Then after that, when you start to be able to form abstractions, that's when God is the
[00:49:57.460 --> 00:50:00.720]   memory week, the universe can remember.
[00:50:00.720 --> 00:50:03.460]   God is the memory of the universe can remember.
[00:50:03.460 --> 00:50:04.460]   Otherwise there's no way.
[00:50:04.460 --> 00:50:08.020]   Did you deciphering that statement hundreds of years from now?
[00:50:08.020 --> 00:50:09.020]   What does that mean?
[00:50:09.020 --> 00:50:10.020]   I'm not going to get into this.
[00:50:10.020 --> 00:50:12.580]   Hey, look, don't, don't diss my, my one liners.
[00:50:12.580 --> 00:50:16.300]   It took me 15 seconds to come up.
[00:50:16.300 --> 00:50:17.780]   I don't know what it means.
[00:50:17.780 --> 00:50:18.780]   What does it mean?
[00:50:18.780 --> 00:50:19.780]   Okay.
[00:50:19.780 --> 00:50:24.540]   Wait, we need to, how do we get onto this?
[00:50:24.540 --> 00:50:30.720]   We were a time causality mathematics.
[00:50:30.720 --> 00:50:43.160]   So what is mathematics in this picture of stuff, objects, memory, and information is
[00:50:43.160 --> 00:50:45.080]   what, what exactly is mathematics?
[00:50:45.080 --> 00:50:49.960]   It's the most efficient labeling scheme that you can apply to lots of different graphs.
[00:50:49.960 --> 00:50:52.520]   Labeling scheme doesn't make it sound useful.
[00:50:52.520 --> 00:50:53.520]   Can I try?
[00:50:53.520 --> 00:50:54.520]   Yep.
[00:50:54.520 --> 00:50:55.520]   Sure.
[00:50:55.520 --> 00:50:56.520]   Please.
[00:50:56.520 --> 00:50:57.520]   Have you rejected my definition of mathematics?
[00:50:57.520 --> 00:50:58.520]   I'm shocked.
[00:50:58.520 --> 00:50:59.520]   Yeah, no, I'm sorry.
[00:50:59.520 --> 00:51:00.520]   But it's correct.
[00:51:00.520 --> 00:51:01.520]   I'm sorry.
[00:51:01.520 --> 00:51:02.520]   Excellent.
[00:51:02.520 --> 00:51:06.440]   No, I mean, I think, I think we have a problem, right?
[00:51:06.440 --> 00:51:10.720]   Because we, we can't not be us, like we're stuck in the shells we are and we're trying
[00:51:10.720 --> 00:51:11.720]   to observe the world.
[00:51:11.720 --> 00:51:13.880]   And so mathematics looks like it has certain properties.
[00:51:13.880 --> 00:51:18.280]   And I guess the thought experiment I find is useful is to try to imagine if you were
[00:51:18.280 --> 00:51:23.280]   outside of us looking at us as physical systems using mathematics, what would be the specific
[00:51:23.280 --> 00:51:29.420]   features you associate to the property of understanding mathematics and being able to
[00:51:29.420 --> 00:51:33.400]   implement it in the universe, right?
[00:51:33.400 --> 00:51:37.360]   And when you do that, mathematics seems to have some really interesting properties relative
[00:51:37.360 --> 00:51:43.200]   to other kinds of abstraction we might talk about, like language or artistic expression.
[00:51:43.200 --> 00:51:47.200]   One of those properties is the one I mentioned already, that is really easy to copy between
[00:51:47.200 --> 00:51:48.200]   physical media.
[00:51:48.200 --> 00:51:51.960]   So if I give you a mathematical statement, you almost immediately know what I mean.
[00:51:51.960 --> 00:51:54.880]   If I tell you the sky is blue, you might say, is it gold ball blue?
[00:51:54.880 --> 00:51:55.880]   Is it azure blue?
[00:51:55.880 --> 00:51:56.880]   What color blue do you mean?
[00:51:56.880 --> 00:52:00.360]   And you have a harder time visualizing what I actually mean.
[00:52:00.360 --> 00:52:03.480]   So mathematics carries a lot of meaning with it when it's copied between physical systems.
[00:52:03.480 --> 00:52:07.040]   It's also the reason we use it to communicate with computers.
[00:52:07.040 --> 00:52:12.080]   And then the second one is it retains its property of actually what it can do in the
[00:52:12.080 --> 00:52:13.920]   universe when it's copied.
[00:52:13.920 --> 00:52:19.680]   So the example I like to give there is think about like Newton's law of gravitation.
[00:52:19.680 --> 00:52:24.680]   It's actually, it's a compressed regularity of a bunch of phenomena that we observe in
[00:52:24.680 --> 00:52:25.680]   the universe.
[00:52:25.680 --> 00:52:29.800]   And then that information actually is a causal in a sense that it allows us to do things
[00:52:29.800 --> 00:52:33.520]   we wouldn't be able to do without that particular knowledge and that particular abstraction.
[00:52:33.520 --> 00:52:36.520]   And in this case, like launch satellites to space or send people to Mars or whatever it
[00:52:36.520 --> 00:52:39.320]   is.
[00:52:39.320 --> 00:52:42.840]   So if you look at us from the outside and you say, what is it for physical systems to
[00:52:42.840 --> 00:52:51.800]   invent a thing called mathematics and then to use and then it to become a physical observable,
[00:52:51.800 --> 00:52:58.400]   this is kind of like the universally copyable information that allows new possibility spaces
[00:52:58.400 --> 00:53:02.440]   to be open in the future because it allows this kind of ability to map one physical system
[00:53:02.440 --> 00:53:05.280]   to another and actually understand that the general principles.
[00:53:05.280 --> 00:53:11.320]   - So is it helping the overlap of causal graphs then by mapping?
[00:53:11.320 --> 00:53:16.000]   - Oh, I think that's the explanation for what it is in terms of the physical theory of assembly
[00:53:16.000 --> 00:53:20.920]   would be some feature of the structure of the assembly spaces of causal graphs and their
[00:53:20.920 --> 00:53:22.200]   relationship to each other.
[00:53:22.200 --> 00:53:26.400]   So for example, and I mean, this is things that we're gonna have to work out over the
[00:53:26.400 --> 00:53:27.400]   next few years.
[00:53:27.400 --> 00:53:33.920]   I mean, we're in totally uncharted conceptual territory here, but as is usual diving off
[00:53:33.920 --> 00:53:35.480]   the deep end.
[00:53:35.480 --> 00:53:39.800]   But I would expect that we would be able to come up with a theory of like, why is it that
[00:53:39.800 --> 00:53:43.200]   some physical systems can communicate with each other?
[00:53:43.200 --> 00:53:44.200]   Like language.
[00:53:44.200 --> 00:53:48.560]   Language is basically because we're objects extended over time and some of the history
[00:53:48.560 --> 00:53:51.160]   of that assembly space actually overlaps.
[00:53:51.160 --> 00:53:54.640]   And when we communicate, it's because we actually have shared structure in our causal history.
[00:53:54.640 --> 00:53:56.520]   - Let me have another quick go at this, right?
[00:53:56.520 --> 00:53:57.520]   So I think we all agree.
[00:53:57.520 --> 00:54:04.840]   So I think we take mathematics for granted because we've gone through this chain, right?
[00:54:04.840 --> 00:54:06.440]   We all share a language now, okay?
[00:54:06.440 --> 00:54:12.560]   And we can, well, we share, so we have languages that we can make interoperable.
[00:54:12.560 --> 00:54:17.640]   And so whether you're speaking, I don't know, all the different dialects of Chinese, all
[00:54:17.640 --> 00:54:22.520]   the different dialects of English, French, German, whatever, you can interconvert them.
[00:54:22.520 --> 00:54:26.160]   The interesting thing about mathematics now is that everybody on planet Earth, every human
[00:54:26.160 --> 00:54:29.960]   being and computers share that common language.
[00:54:29.960 --> 00:54:33.480]   That language was constructed by a process in time.
[00:54:33.480 --> 00:54:38.560]   So what I'm trying to say is assembly invented math is those right from the, you know, mathematics
[00:54:38.560 --> 00:54:42.040]   didn't exist before life.
[00:54:42.040 --> 00:54:44.240]   Abstraction was invented by life, right?
[00:54:44.240 --> 00:54:47.400]   That doesn't mean that the universe wasn't capable of mathematical things.
[00:54:47.400 --> 00:54:48.400]   - Wait, wait a minute.
[00:54:48.400 --> 00:54:52.920]   Can we just ask that old famous question, is math invented or discovered?
[00:54:52.920 --> 00:54:56.840]   So when you say assembly invented or whatever, it means-
[00:54:56.840 --> 00:55:01.240]   - Well, someone might refer to assembly as a mathematical theory, but sorry.
[00:55:01.240 --> 00:55:02.240]   - Right.
[00:55:02.240 --> 00:55:03.240]   - Are we arguing?
[00:55:03.240 --> 00:55:04.240]   - Exactly.
[00:55:04.240 --> 00:55:05.240]   - Are we arguing now?
[00:55:05.240 --> 00:55:06.240]   - That's what it sounds like.
[00:55:06.240 --> 00:55:07.240]   Are we discovering mathematics?
[00:55:07.240 --> 00:55:08.240]   - No.
[00:55:08.240 --> 00:55:09.240]   Well, yes and no.
[00:55:09.240 --> 00:55:10.240]   I would say-
[00:55:10.240 --> 00:55:12.880]   - And you call mathematics a language that you're developing.
[00:55:13.360 --> 00:55:19.000]   - I'm pretty sure that there are some very common seeds of mathematics in the universe,
[00:55:19.000 --> 00:55:20.000]   right?
[00:55:20.000 --> 00:55:25.680]   But actually a lot of the mathematics that we are finding now is not discovered, it's
[00:55:25.680 --> 00:55:26.680]   invented.
[00:55:26.680 --> 00:55:30.480]   But even though, I think those two terms are very triggering and I don't think they're
[00:55:30.480 --> 00:55:35.800]   necessarily useful because I think that what people do, the mathematicians that say, "Oh,
[00:55:35.800 --> 00:55:40.560]   mathematics was discovered," because they live in a universe where there is no time
[00:55:40.560 --> 00:55:42.040]   and it just all exists.
[00:55:42.040 --> 00:55:46.800]   But what I'm saying is, and I think in the same way you can create, let's say I'm going
[00:55:46.800 --> 00:55:51.520]   to go and create and make a piece of art.
[00:55:51.520 --> 00:55:56.160]   Did I make that piece of art or did I discover it?
[00:55:56.160 --> 00:55:57.560]   Like inventing the airplane.
[00:55:57.560 --> 00:55:58.560]   Did I invent the airplane?
[00:55:58.560 --> 00:55:59.560]   Let's stick with the airplane.
[00:55:59.560 --> 00:56:00.560]   The airplane is a good one.
[00:56:00.560 --> 00:56:03.280]   Let's say, did I discover the airplane?
[00:56:03.280 --> 00:56:05.680]   Well in a way, the universe discovered the airplane because it just chucked a load of
[00:56:05.680 --> 00:56:09.880]   atoms together and a load of random human beings, one day stuff, and then we discovered
[00:56:09.880 --> 00:56:12.280]   the airplane in the space of possibilities.
[00:56:12.280 --> 00:56:20.720]   But here's the thing, when the space of possibilities is so vast, infinite almost, and you're able
[00:56:20.720 --> 00:56:24.680]   to actualise one of those in an object, then you are inventing it.
[00:56:24.680 --> 00:56:28.680]   So in mathematics, because there are infinite number of theorems, the fact you're actually
[00:56:28.680 --> 00:56:33.520]   pulling, there's no difference between inventing a mathematical structure and inventing the
[00:56:33.520 --> 00:56:34.520]   airplane.
[00:56:34.520 --> 00:56:35.520]   They're the same thing.
[00:56:35.520 --> 00:56:38.800]   But that doesn't mean that now the airplane exists in the universe, there's something
[00:56:38.800 --> 00:56:41.280]   weird about the universe.
[00:56:41.280 --> 00:56:46.960]   So I think that the more, this is the thing that I, you probably, the more memory required
[00:56:46.960 --> 00:56:50.380]   for the object, the more invented it is.
[00:56:50.380 --> 00:56:56.640]   So when a mathematical theorem needs more bytes to store it, the more invented it is
[00:56:56.640 --> 00:57:00.120]   and the less bytes, the more discovered it is.
[00:57:00.120 --> 00:57:02.160]   - But everything then is invented.
[00:57:02.160 --> 00:57:03.960]   It's just more or less invented.
[00:57:03.960 --> 00:57:04.960]   - Absolutely.
[00:57:04.960 --> 00:57:05.960]   - Okay.
[00:57:05.960 --> 00:57:06.960]   - The universe has to generate everything as it goes.
[00:57:06.960 --> 00:57:06.960]   - Yeah.
[00:57:07.960 --> 00:57:11.160]   And it wasn't there in the beginning.
[00:57:11.160 --> 00:57:14.920]   And the way we're thinking it, when you're thinking about the difference between invented
[00:57:14.920 --> 00:57:18.560]   and discovered is because we're throwing away all the memory.
[00:57:18.560 --> 00:57:24.000]   So if you start to think in terms of causality and time, then those things become the same.
[00:57:24.000 --> 00:57:25.000]   Everything is invented.
[00:57:25.000 --> 00:57:28.280]   - And the idea is to make everything intrinsic to the universe.
[00:57:28.280 --> 00:57:32.280]   So I think one of the features of assembly theory is we don't want to have external observers.
[00:57:32.280 --> 00:57:35.200]   There's been this long tradition in physics of trying to describe the universe from the
[00:57:35.200 --> 00:57:37.880]   outside and not the inside.
[00:57:37.880 --> 00:57:41.440]   And the universe has to generate everything itself if you do it from the inside.
[00:57:41.440 --> 00:57:44.640]   - Assembly theory describes how the universe builds itself.
[00:57:44.640 --> 00:57:48.920]   - Did it take you 15 seconds to say that?
[00:57:48.920 --> 00:57:49.920]   To come up with that also?
[00:57:49.920 --> 00:57:50.920]   - No, I've thought of that before.
[00:57:50.920 --> 00:57:51.920]   - Okay.
[00:57:51.920 --> 00:57:52.920]   That's a good line.
[00:57:52.920 --> 00:57:53.920]   - Are you making fun of me?
[00:57:53.920 --> 00:57:54.920]   - No, I'm not making fun.
[00:57:54.920 --> 00:57:55.920]   I'm having fun.
[00:57:55.920 --> 00:57:56.920]   There's a difference.
[00:57:56.920 --> 00:57:57.920]   - Oh, that's good.
[00:57:57.920 --> 00:57:58.920]   All right.
[00:57:58.920 --> 00:57:59.920]   - She's inventing fun.
[00:57:59.920 --> 00:58:00.920]   - I'm not all intimidated.
[00:58:00.920 --> 00:58:01.920]   - Yes.
[00:58:01.920 --> 00:58:05.080]   - And there's a causal history to that fun.
[00:58:05.080 --> 00:58:12.080]   So you mentioned that there's no way to communicate with aliens until there's overlap in the causal
[00:58:12.080 --> 00:58:14.960]   graph.
[00:58:14.960 --> 00:58:18.800]   Communication includes being able to see them?
[00:58:18.800 --> 00:58:26.520]   And like what are we, this is the question is, is communication any kind of detection?
[00:58:26.520 --> 00:58:33.200]   And if so, what do aliens look like as you get more and more overlap on the causal graph?
[00:58:33.200 --> 00:58:39.680]   You're assuming, let's assume that, so when you see them and they see you, you're assuming
[00:58:39.680 --> 00:58:44.000]   they have vision, they have the ability to construct in 3D and in time.
[00:58:44.000 --> 00:58:45.880]   That's a lot of assumptions we're making.
[00:58:45.880 --> 00:58:46.880]   - What detection?
[00:58:46.880 --> 00:58:47.880]   All right, let's step back.
[00:58:47.880 --> 00:58:49.280]   So yes, okay, you're right.
[00:58:49.280 --> 00:58:53.560]   So when in the English language, when we say the word see, we mean visually, they show
[00:58:53.560 --> 00:58:56.460]   up to a party and it's like, oh wow, that's an alien.
[00:58:56.460 --> 00:59:00.040]   That's visual, that's 3D, that's okay.
[00:59:00.040 --> 00:59:05.720]   And that's also assuming scale, spatial scale of something that's visible to you.
[00:59:05.720 --> 00:59:09.680]   So it can't be microscopic or it can't be so big that you don't even realize that's
[00:59:09.680 --> 00:59:10.680]   an entity.
[00:59:10.680 --> 00:59:14.120]   Okay, but other kinds of detection too.
[00:59:14.120 --> 00:59:18.480]   - I would make it more abstract and go, I was thinking this morning about how to rewrite
[00:59:18.480 --> 00:59:23.280]   the Arecibo message in assembly theory and also to abandon binary.
[00:59:23.280 --> 00:59:27.360]   'Cause I don't think aliens necessarily, why should they have binary?
[00:59:27.360 --> 00:59:32.440]   They have some basic elements with which to do information exchange.
[00:59:32.440 --> 00:59:37.280]   - Let's make it more fundamental, more universal.
[00:59:37.280 --> 00:59:40.800]   So we need to think about what is the universal way of making a memory and then we should
[00:59:40.800 --> 00:59:43.640]   re-encode Arecibo in that way.
[00:59:43.640 --> 00:59:45.840]   - What's more basic than zeros and ones?
[00:59:45.840 --> 00:59:49.560]   - Well, it's really difficult to get out of that causal chain because we're so, so let's
[00:59:49.560 --> 00:59:51.840]   erase the idea of zero for a moment.
[00:59:51.840 --> 00:59:55.040]   It took human beings a long time to come up with the idea of zero.
[00:59:55.040 --> 00:59:56.880]   Now you've got the idea of zero, you can't throw it away.
[00:59:56.880 --> 00:59:57.880]   - That's so useful.
[00:59:57.880 --> 00:59:59.200]   - To discover the idea of zero.
[00:59:59.200 --> 01:00:00.200]   - To discover or invent.
[01:00:00.200 --> 01:00:04.920]   - I don't know, but it took a long time, so it was invented, that's right.
[01:00:04.920 --> 01:00:07.400]   - Yeah, I think zero was invented, exactly.
[01:00:07.400 --> 01:00:10.520]   So it's not a given that aliens know what zero is.
[01:00:10.520 --> 01:00:12.760]   - They just have to warn.
[01:00:12.760 --> 01:00:13.760]   - Massive assumption.
[01:00:13.760 --> 01:00:16.200]   - It's a useful discovery.
[01:00:16.200 --> 01:00:19.760]   You're saying if you break the causal chain, there might be some other more efficient way
[01:00:19.760 --> 01:00:20.760]   of representing.
[01:00:20.760 --> 01:00:23.320]   - That's why I wanna meet him and ask him.
[01:00:23.320 --> 01:00:27.800]   Or a shortcut, but you won't be able to ask him until--
[01:00:27.800 --> 01:00:29.920]   - So I interrupted you and I think you're making a good point.
[01:00:29.920 --> 01:00:31.080]   I was just gonna say, well look--
[01:00:31.080 --> 01:00:32.080]   - Thank you.
[01:00:32.080 --> 01:00:33.080]   - Sorry.
[01:00:33.080 --> 01:00:34.080]   (laughing)
[01:00:34.080 --> 01:00:35.080]   Rather than saying--
[01:00:35.080 --> 01:00:37.360]   - Please internet, tweet at him for the rude interruptions.
[01:00:37.360 --> 01:00:39.040]   Oh, go ahead, I'm sorry.
[01:00:39.040 --> 01:00:40.360]   - No, it's okay.
[01:00:40.360 --> 01:00:41.360]   Maybe it's change.
[01:00:41.360 --> 01:00:46.000]   How do we, so, oh, I don't know what it's like to be an alien.
[01:00:46.000 --> 01:00:47.600]   I would like to know.
[01:00:47.600 --> 01:00:52.600]   - What is the full spectrum of what aliens might look like to us?
[01:00:52.600 --> 01:00:58.920]   Now that we've laid this all on the table of like, all right, so there has to be some
[01:00:58.920 --> 01:01:03.960]   overlap in this causal chain that led to them.
[01:01:03.960 --> 01:01:05.320]   What are we looking for?
[01:01:05.320 --> 01:01:07.040]   What do you think we should be looking for?
[01:01:07.040 --> 01:01:13.040]   So you mentioned mass spec, measuring certain objects that aliens could create, or are aliens
[01:01:13.040 --> 01:01:14.040]   themselves.
[01:01:14.040 --> 01:01:20.360]   We show up to a planet, or maybe not a planet, or maybe, what the hell is the basic object
[01:01:20.360 --> 01:01:21.360]   we're trying to measure--
[01:01:21.360 --> 01:01:22.360]   - Well let's call ourselves a bright--
[01:01:22.360 --> 01:01:23.360]   - What's the index of?
[01:01:23.360 --> 01:01:24.360]   - Let's call ourselves a break.
[01:01:24.360 --> 01:01:30.600]   Let's assume that they are, they're metabolized, they've got an energy source, and they're
[01:01:30.600 --> 01:01:32.800]   a size that we can recognize.
[01:01:32.800 --> 01:01:37.240]   Let's give ourselves a break, 'cause there could be aliens that are so big we won't recognize
[01:01:37.240 --> 01:01:38.240]   we're seeing them.
[01:01:38.240 --> 01:01:41.000]   There might be aliens that are so small we don't yet have the ability to, you know, we
[01:01:41.000 --> 01:01:45.200]   don't have microscopes that can see far enough away that just won't be able to see them.
[01:01:45.200 --> 01:01:46.560]   - So what's a good range?
[01:01:46.560 --> 01:01:50.840]   - So let's just make a range, let's just be very anthropocentric and say we're gonna look
[01:01:50.840 --> 01:01:55.640]   for aliens roughly our size, and technology our size, because we know it's possible on
[01:01:55.640 --> 01:01:56.640]   Earth, right?
[01:01:56.640 --> 01:02:00.360]   I mean a reasonable thing to do would be to find exoplanets that are in the same zone
[01:02:00.360 --> 01:02:06.480]   as Earth in terms of heat and stuff, and then say, hey, if there's that same kind of gravity,
[01:02:06.480 --> 01:02:13.400]   same kind of stuff, we could reasonably assume that the alien life there might use a similar
[01:02:13.400 --> 01:02:16.840]   kind of physical infrastructure, and then we're good.
[01:02:16.840 --> 01:02:23.480]   So then your question becomes really relevant, say, right, let's use vision, sound, touch.
[01:02:23.480 --> 01:02:24.960]   - So okay, that's really nice.
[01:02:24.960 --> 01:02:32.440]   So if there's a lot of aliens out there, there's a good likelihood if you match to the planet
[01:02:32.440 --> 01:02:38.240]   that they're going to be in the same spatial and temporal, operating in the same spatial
[01:02:38.240 --> 01:02:41.160]   and temporal domain as humans.
[01:02:41.160 --> 01:02:48.080]   Given that, what do they look like visually?
[01:02:48.080 --> 01:02:49.080]   What do they sound like?
[01:02:49.080 --> 01:02:53.080]   What do they, oh God, this sounds creepy, taste like?
[01:02:53.080 --> 01:02:56.000]   What do they smell like?
[01:02:56.000 --> 01:02:59.440]   - It sounds like our clubhouse, and it's like, can we have sex with aliens?
[01:02:59.440 --> 01:03:00.840]   Which was basically me saying--
[01:03:00.840 --> 01:03:01.840]   - Passionate, passionate love.
[01:03:01.840 --> 01:03:04.960]   - But it wasn't actually about sex, it was about, is our chemistry compatible, right?
[01:03:04.960 --> 01:03:05.960]   Is there some--
[01:03:05.960 --> 01:03:06.960]   - Yeah.
[01:03:06.960 --> 01:03:09.880]   - Yeah, can we, yeah.
[01:03:09.880 --> 01:03:10.880]   Are they edible too?
[01:03:10.880 --> 01:03:12.880]   - Yeah, they could be very edible, they could be delicious.
[01:03:12.880 --> 01:03:14.840]   - That's why I wanna see some aliens, right?
[01:03:14.840 --> 01:03:21.720]   Because I think evolution, I mean, evolution exploits symmetry, right?
[01:03:21.720 --> 01:03:26.160]   Because why generate memory, why generate storage, the need for storage space when you
[01:03:26.160 --> 01:03:28.640]   can use symmetry?
[01:03:28.640 --> 01:03:33.120]   And symmetry is maybe quite effective in allowing you to mechanically design stuff, right?
[01:03:33.120 --> 01:03:39.920]   So maybe you could be reasonable to assume that aliens could have, they could be bipedal,
[01:03:39.920 --> 01:03:45.240]   they could be symmetric in the same way, might have a couple of eyes, or a couple of senses.
[01:03:45.240 --> 01:03:49.200]   We can make them, perhaps, there's this whole zoo of different aliens out there, and we'll
[01:03:49.200 --> 01:03:53.360]   never get to be able to classify some of the weird aliens we can't interact with, because
[01:03:53.360 --> 01:03:55.160]   they have made such weird stuff.
[01:03:55.160 --> 01:04:00.480]   But we are just going to look at, we're gonna find aliens that look most like us, why not?
[01:04:00.480 --> 01:04:02.520]   - 'Cause those are the first ones we're likely to see.
[01:04:02.520 --> 01:04:03.520]   - Yeah.
[01:04:03.520 --> 01:04:04.520]   - Yeah.
[01:04:04.520 --> 01:04:08.400]   - But I think it's really hard to imagine what the space of aliens is, because the space
[01:04:08.400 --> 01:04:13.200]   is huge, because one of the arguments that you can make about why life emerges in chemistry
[01:04:13.200 --> 01:04:19.200]   is because chemistry is the first scale in terms of building up objects from elementary
[01:04:19.200 --> 01:04:24.320]   objects, that the number of possible things that could exist is larger than the universe
[01:04:24.320 --> 01:04:27.880]   can possibly make all at once, right?
[01:04:27.880 --> 01:04:32.080]   So imagine you have two planets, and they're cooking some geochemistry.
[01:04:32.080 --> 01:04:37.160]   Our planet invented one kind of biochemistry, and presumably, as you start building up the
[01:04:37.160 --> 01:04:41.760]   complexity of the molecules, the chances of the overlap in those trajectories, those causal
[01:04:41.760 --> 01:04:45.360]   chains being built up, is probably very low.
[01:04:45.360 --> 01:04:49.200]   And it gets lower and lower as it gets further advanced along its evolutionary path.
[01:04:49.200 --> 01:04:53.320]   So I think it's very difficult to imagine predicting the technologies that aliens are
[01:04:53.320 --> 01:04:54.320]   gonna have.
[01:04:54.320 --> 01:04:59.760]   I mean, it's so, you're looking at basically, planets have kind of convergent chemistry,
[01:04:59.760 --> 01:05:02.920]   but there's some variability, and then you're looking basically at the outgrowth into the
[01:05:02.920 --> 01:05:04.200]   possibility space for chemistry.
[01:05:04.200 --> 01:05:10.040]   - So do you think we would detect the technology, the objects created by aliens before we detect
[01:05:10.040 --> 01:05:11.040]   the aliens?
[01:05:11.040 --> 01:05:12.040]   - Possibly.
[01:05:12.040 --> 01:05:17.680]   - So when you're talking about measuring assembly index, don't you think we would detect the
[01:05:17.680 --> 01:05:19.360]   garbage first?
[01:05:19.360 --> 01:05:26.120]   Like at the outskirts of alien civilizations, this is gonna be trash.
[01:05:26.120 --> 01:05:28.080]   - I think I would come back to Arecibo.
[01:05:28.080 --> 01:05:33.600]   The Arecibo message sent from the Arecibo telescope built by Drake, I think, and Sagan.
[01:05:33.600 --> 01:05:34.600]   How's Arecibo spelled?
[01:05:34.600 --> 01:05:35.600]   - A-R-E-C-I-B-O.
[01:05:35.600 --> 01:05:36.600]   - Yes, thank you.
[01:05:36.600 --> 01:05:40.200]   - And there we go, they've got it up there.
[01:05:40.200 --> 01:05:42.640]   - That's the telescope that sent the message that you're talking about.
[01:05:42.640 --> 01:05:45.640]   - So that message was sent where?
[01:05:45.640 --> 01:05:52.440]   - It was beamed at a star, a specific star, and it was sent out many years ago.
[01:05:52.440 --> 01:05:56.920]   And what they did, so this is why I was pushing on binary, it's a binary message.
[01:05:56.920 --> 01:06:04.120]   I think it's a semi-prime length number of characters, so I think 73 by 23, I think.
[01:06:04.120 --> 01:06:10.920]   And it basically represents human bit proton, binary, human beings, DNA, male and female.
[01:06:10.920 --> 01:06:12.760]   And it's really cool.
[01:06:12.760 --> 01:06:20.680]   But I'm just wondering if it could be done not making any, 'cause it made assumptions
[01:06:20.680 --> 01:06:22.080]   that aliens speak binary.
[01:06:22.080 --> 01:06:24.320]   Why make that assumption?
[01:06:24.320 --> 01:06:28.600]   Why not just assume that if the difference between physics, chemistry, and biology is
[01:06:28.600 --> 01:06:36.920]   the amount of memory that's recordable by the substrate, then surely the universal thing,
[01:06:36.920 --> 01:06:41.600]   my I'm gonna make some sacrilegious statement, which I think is pretty awesome for people
[01:06:41.600 --> 01:06:42.600]   to argue with.
[01:06:42.600 --> 01:06:48.480]   - So this is, we're looking at an image where it's the entirety of the message encoded in
[01:06:48.480 --> 01:06:54.240]   binary, and then there's probably interpretation of different parts of that image.
[01:06:54.240 --> 01:06:58.000]   There's a person, there's green parts.
[01:06:58.000 --> 01:07:01.760]   It looks like for people just listening like a game of Tetris.
[01:07:01.760 --> 01:07:06.160]   So it's encoding in minimal ways a bunch of cool information probably.
[01:07:06.160 --> 01:07:07.160]   - Representing all of us.
[01:07:07.160 --> 01:07:09.760]   - So at the top it's kind of teaching us how to count, and then it all goes all the way
[01:07:09.760 --> 01:07:14.160]   down teaching you chemistry, and then just says, but it makes so many assumptions.
[01:07:14.160 --> 01:07:19.760]   And I think if we can actually, so look, I think, I mean Sarah's much more eloquent expressing
[01:07:19.760 --> 01:07:22.040]   this, but I'll have a go and you can correct it if you want.
[01:07:22.040 --> 01:07:28.040]   Which is like, one of the things that Sarah has had a profound effect on the way I look
[01:07:28.040 --> 01:07:32.560]   at the origin of life, and this is one of the reasons why we're working together, because
[01:07:32.560 --> 01:07:34.240]   we don't really care about the origin of life.
[01:07:34.240 --> 01:07:37.780]   We wanna make life, make aliens, and find aliens.
[01:07:37.780 --> 01:07:38.840]   Make aliens, find aliens.
[01:07:38.840 --> 01:07:42.400]   I think we might have to make aliens in the lab before we find aliens in the universe,
[01:07:42.400 --> 01:07:43.400]   right?
[01:07:43.400 --> 01:07:45.520]   I think that would be a cool way to do it.
[01:07:45.520 --> 01:07:48.400]   So what is it about the universe that creates aliens?
[01:07:48.400 --> 01:07:52.680]   Well it's selection through assembly theory, creating memories.
[01:07:52.680 --> 01:07:58.120]   Because when you create memories you can then command your domain, you can basically do
[01:07:58.120 --> 01:07:59.120]   stuff.
[01:07:59.120 --> 01:08:00.920]   You can command matter.
[01:08:00.920 --> 01:08:05.600]   So we need to find a way, by understanding what life is, of how the minimal way to command
[01:08:05.600 --> 01:08:10.400]   matter, how that would emerge in the universe, and if we want to communicate, I mean maybe
[01:08:10.400 --> 01:08:13.760]   we don't want to necessarily uniformly communicate.
[01:08:13.760 --> 01:08:18.320]   What I would do perhaps if I had, is I would send out lots of probes away from Earth that
[01:08:18.320 --> 01:08:20.120]   have this magic way of communicating with aliens.
[01:08:20.120 --> 01:08:25.760]   Get them quite far away from Earth, plausibly deniable, and then send out the message that
[01:08:25.760 --> 01:08:29.160]   would then attract all the aliens, and then basically work out if they're a friend or
[01:08:29.160 --> 01:08:30.600]   foe and how they wanna hang out.
[01:08:30.600 --> 01:08:33.440]   The messages being something that has to do with the memories?
[01:08:33.440 --> 01:08:34.440]   Yes.
[01:08:34.440 --> 01:08:39.660]   Like the assembly version of Arecibo, so that everyone in the universe understands what
[01:08:39.660 --> 01:08:41.080]   life is.
[01:08:41.080 --> 01:08:43.440]   So aliens need to work out what they are.
[01:08:43.440 --> 01:08:46.400]   Once they've worked out what they are, they then can work out how to encode what they
[01:08:46.400 --> 01:08:48.680]   are, and then they can go out and send messages.
[01:08:48.680 --> 01:08:55.960]   It's like the universal, the Rosetta Stone for life in the universe is working out how
[01:08:55.960 --> 01:08:56.960]   the memories are built.
[01:08:56.960 --> 01:09:02.880]   I don't know if, Sarah, you have any, well, whether you would agree with that.
[01:09:02.880 --> 01:09:09.280]   No, I wanted to raise a different point, which is about the fact that we can't see the aliens
[01:09:09.280 --> 01:09:11.960]   yet 'cause we haven't gotten the technology.
[01:09:11.960 --> 01:09:15.680]   And presumably we think assembly theory is the right way of doing it, but I don't think
[01:09:15.680 --> 01:09:20.280]   that we know how to go from the kind of data you're describing, Lex, like visual data or
[01:09:20.280 --> 01:09:23.360]   smell to construct the assembly spaces yet.
[01:09:23.360 --> 01:09:28.440]   And in some ways, I think that the problem of life detection really is the same problem
[01:09:28.440 --> 01:09:35.160]   at the foundations of AI that we don't understand how to get machines to see causal graphs,
[01:09:35.160 --> 01:09:38.000]   to see reality in terms of causation.
[01:09:38.000 --> 01:09:44.040]   And so I think assembly and AI are gonna intersect in interesting ways, hopefully.
[01:09:44.040 --> 01:09:49.840]   But the sort of key point, and I've been trying to make this argument more recently, and might
[01:09:49.840 --> 01:09:53.640]   write an essay on it, is people talk about the great filter, right?
[01:09:53.640 --> 01:09:57.760]   Which is, again, this doomsday thing that people wanna say there's no aliens out there
[01:09:57.760 --> 01:10:00.480]   'cause something terrible happened to them.
[01:10:00.480 --> 01:10:05.640]   And it matters whether that's in our past or our future as to the longevity of our species,
[01:10:05.640 --> 01:10:08.240]   presumably, which is why people find it interesting.
[01:10:08.240 --> 01:10:10.840]   But I think it's not a physical filter.
[01:10:10.840 --> 01:10:11.960]   It's not like things go extinct.
[01:10:11.960 --> 01:10:15.800]   I think it's literally we don't have the technology to see them.
[01:10:15.800 --> 01:10:16.800]   And you could see that with microscopes.
[01:10:16.800 --> 01:10:20.600]   I mean, we didn't know there were microbes on this table for, or tables for thousands
[01:10:20.600 --> 01:10:21.600]   of years, or telescopes.
[01:10:21.600 --> 01:10:23.560]   Like, there's so much of the universe we can't see.
[01:10:23.560 --> 01:10:28.040]   And then basically what we have done as a species is outsource our physical perceptions
[01:10:28.040 --> 01:10:33.640]   to technology, building microscopes based on our eyes, and building seismometers based
[01:10:33.640 --> 01:10:36.320]   on our sense of feelings, like feel earthquakes and things.
[01:10:36.320 --> 01:10:39.600]   And AI is basically we're trying to outsource what's actually happening in our thinking
[01:10:39.600 --> 01:10:43.400]   apparatus into machines now, into technological devices.
[01:10:43.400 --> 01:10:47.440]   And maybe that's the key technology that's gonna allow us to see things like us and see
[01:10:47.440 --> 01:10:48.760]   the universe in a totally different way.
[01:10:48.760 --> 01:10:50.160]   - But you kinda mentioned the great filter.
[01:10:50.160 --> 01:10:53.960]   Do you think there's a way through technology to stop being able to see stuff?
[01:10:53.960 --> 01:10:55.760]   So can you take a step backwards?
[01:10:55.760 --> 01:10:56.760]   - I think so, yeah.
[01:10:56.760 --> 01:10:58.480]   - Did you imply that with the great, so like--
[01:10:58.480 --> 01:11:04.840]   - Well, no, I mean, I think there's a great perceptual filter in the sense that a example
[01:11:04.840 --> 01:11:09.280]   of life evolving on a planet over billions of years has to acquire a certain amount of
[01:11:09.280 --> 01:11:14.520]   knowledge and technology to actually recognize the phenomena that it is.
[01:11:14.520 --> 01:11:21.000]   - Well, that's the sense I have, is when you talk with physicists, engineers in general,
[01:11:21.000 --> 01:11:27.680]   there's this kind of idea that we have most of the tools already just to hear the signal.
[01:11:27.680 --> 01:11:32.600]   But to me it feels like we don't have any of the tools to see the signal.
[01:11:32.600 --> 01:11:34.280]   - No, we don't know what we're doing, yeah, I agree.
[01:11:34.280 --> 01:11:35.880]   - That's the biggest, like to hear.
[01:11:35.880 --> 01:11:39.560]   We don't have the tools to really hear, to see.
[01:11:39.560 --> 01:11:41.160]   - Aliens are everywhere, we just don't have the--
[01:11:41.160 --> 01:11:42.160]   - Exactly.
[01:11:42.160 --> 01:11:43.160]   - Yeah, well, that's--
[01:11:43.160 --> 01:11:48.200]   - I mean, I got this in part, actually, 'cause you were like, last time I was here, you were
[01:11:48.200 --> 01:11:49.200]   like, "Look at the carpet."
[01:11:49.200 --> 01:11:52.600]   Could it, like if you had an alien detector, would the carpet be aliens?
[01:11:52.600 --> 01:11:54.080]   I mean, I think we really don't.
[01:11:54.080 --> 01:11:59.640]   - So it would be, but the aliens would nevertheless have a high assembly index, or produce things
[01:11:59.640 --> 01:12:01.320]   of a high assembly index.
[01:12:01.320 --> 01:12:06.960]   And those things of a high assembly index, you have to have a detector that can recognize
[01:12:06.960 --> 01:12:09.280]   high assembly index in all its forms.
[01:12:09.280 --> 01:12:10.280]   - Yeah.
[01:12:10.280 --> 01:12:11.280]   - Yes.
[01:12:11.280 --> 01:12:12.280]   - That's it, that's it.
[01:12:12.280 --> 01:12:13.600]   - Take data, construct assembly space.
[01:12:13.600 --> 01:12:14.600]   - Yeah.
[01:12:14.600 --> 01:12:18.760]   - Those patterns, basically, so one way to think about high assembly index is interesting
[01:12:18.760 --> 01:12:21.520]   patterns, of basic ingredients.
[01:12:21.520 --> 01:12:25.400]   - I can give you an example, 'cause I mean, in molecules we've been talking about, in
[01:12:25.400 --> 01:12:29.160]   objects, but we're also trying to do it in spatial trajectories.
[01:12:29.160 --> 01:12:34.480]   Like imagine you're just, like I always get bothered by the fact that when you look at
[01:12:34.480 --> 01:12:38.880]   birds flocking, you can describe that with a simple Boy's Model, or people use spin glass
[01:12:38.880 --> 01:12:41.920]   to describe animal behavior, and those are really simple physics models.
[01:12:41.920 --> 01:12:48.360]   Yet you're looking at a system that you know has agency, and there's intelligence in those
[01:12:48.360 --> 01:12:49.360]   birds.
[01:12:49.360 --> 01:12:54.000]   And basically, you can't help but think there must be some statistical signatures of the
[01:12:54.000 --> 01:12:59.720]   fact that that's a group of agents, versus, you know, like, I don't know, the physics
[01:12:59.720 --> 01:13:03.840]   example, maybe like, I don't know, Brownian motion or something.
[01:13:03.840 --> 01:13:06.880]   And so what we're trying to do is actually apply assembly to trajectory data, to try
[01:13:06.880 --> 01:13:12.000]   to say there's a minimal amount of causal history to build up certain trajectories for
[01:13:12.000 --> 01:13:15.600]   observed agents, that's like an agency detector for behavior.
[01:13:15.600 --> 01:13:21.840]   - Do you think it's possible to do some, like, Boids, or those kinds of things, like artificial,
[01:13:21.840 --> 01:13:28.400]   like cellular automata, play with those ideas with assembly theory?
[01:13:28.400 --> 01:13:36.620]   Have you found any useful, really simple mathematical, like, simulation tools that allow you to play
[01:13:36.620 --> 01:13:37.620]   with these concepts?
[01:13:37.620 --> 01:13:44.480]   So like, one, of course, you're doing math spec in the physical space with chemistry,
[01:13:44.480 --> 01:13:48.360]   but it just seems, well, I mean, computer science person, maybe, it seems easier to
[01:13:48.360 --> 01:13:49.640]   just-- - I agree with you.
[01:13:49.640 --> 01:13:57.800]   - And sexier in terms of tweeting visual information on Twitter or Instagram, more importantly,
[01:13:57.800 --> 01:14:02.160]   to play like, here's an organism of a low assembly index, and here's an organism of
[01:14:02.160 --> 01:14:08.040]   a high assembly index, and let's watch them create more and more memories, and more and
[01:14:08.040 --> 01:14:09.440]   more complex objects.
[01:14:09.440 --> 01:14:13.560]   And so like, and mathematically, you get to observe what that looks like, to build up
[01:14:13.560 --> 01:14:15.880]   an intuition of what assembly index is like.
[01:14:15.880 --> 01:14:20.000]   We are building a toolkit right now, so I think it's a really good idea, but what we've
[01:14:20.000 --> 01:14:24.720]   gotta do is, I'm kind of still obsessed with the infrastructure required, and one of the
[01:14:24.720 --> 01:14:29.880]   reasons why I was pushing on information and mathematics, when human beings, when human
[01:14:29.880 --> 01:14:34.280]   beings, we take a lot of the infrastructure for granted, and I think we have to strip
[01:14:34.280 --> 01:14:35.960]   that back a bit for going forward.
[01:14:35.960 --> 01:14:40.720]   But you're absolutely right, I would agree that, I think the fact that we exist in the
[01:14:40.720 --> 01:14:45.160]   universe, this is, like, I can see that lots of people would disagree with this statement,
[01:14:45.160 --> 01:14:48.520]   but I don't think, I don't think Sarah will, but I don't know.
[01:14:48.520 --> 01:14:54.000]   The fact that objects exist, I don't think anyone on Earth will disagree that objects
[01:14:54.000 --> 01:14:55.960]   can exist elsewhere, right?
[01:14:55.960 --> 01:14:58.520]   But they will disagree that life can exist elsewhere.
[01:14:58.520 --> 01:15:04.720]   But what perhaps I'm trying to say is that the acquisition, the universe's ability to
[01:15:04.720 --> 01:15:12.540]   acquire memory, is the very first step for building life, and that must be, that's so
[01:15:12.540 --> 01:15:19.840]   easy to happen, so therefore alien life is everywhere, 'cause all alien life is, is those
[01:15:19.840 --> 01:15:24.920]   memories being compressed and minimized, and the alien equivalent of the cell working.
[01:15:24.920 --> 01:15:31.200]   So I think that we will build new technologies to find aliens, but we need to understand
[01:15:31.200 --> 01:15:36.640]   what we are first, and how we go from physics to chemistry to biology.
[01:15:36.640 --> 01:15:41.760]   The most interesting thing, as you say, to these two organisms, different assemblies,
[01:15:41.760 --> 01:15:47.000]   is when you get into biology, biology gets more and more weird, more and more contingent.
[01:15:47.000 --> 01:15:50.400]   Physics is, chemistry is less weird, 'cause the rules of chemistry are smaller than the
[01:15:50.400 --> 01:15:57.420]   rules of biology, and then going away to physics, where you have a very nicely tangible number
[01:15:57.420 --> 01:16:00.440]   of ways of arranging things.
[01:16:00.440 --> 01:16:04.040]   And I think assembly theory just helps you appreciate that.
[01:16:04.040 --> 01:16:09.160]   And so once we get there, my dream is that we are just gonna be able to suddenly, I mean,
[01:16:09.160 --> 01:16:12.360]   I may be just being really arrogant here, I don't mean to be arrogant, it's just, again,
[01:16:12.360 --> 01:16:16.440]   I've just got this hammer called assembly, and everything's a nail, but I think that
[01:16:16.440 --> 01:16:22.520]   once we crack it, we'll be able to use assembly theory plus telescopes to find aliens.
[01:16:22.520 --> 01:16:27.720]   - Do you have, Sarah, do you have disagreements with Lee on the number of aliens that are
[01:16:27.720 --> 01:16:28.720]   out there?
[01:16:28.720 --> 01:16:30.760]   - I do, actually, yeah, well.
[01:16:30.760 --> 01:16:36.760]   - And what they look like, so any of the things we've been talking about, is there nuanced,
[01:16:36.760 --> 01:16:42.960]   it's always nice to discover wisdom through nuanced disagreement.
[01:16:42.960 --> 01:16:49.920]   - Yeah, I don't wholly disagree, but I think, but I do think I disagree, it's kind of, there's
[01:16:49.920 --> 01:16:50.920]   nuance there.
[01:16:50.920 --> 01:16:52.760]   But Lee made-- - You can disagree.
[01:16:52.760 --> 01:16:55.760]   - No, it's fine, it is nuanced, right?
[01:16:55.760 --> 01:17:02.040]   So you made the point earlier that you think, you know, once we discover what life is, we'll
[01:17:02.040 --> 01:17:04.560]   see alien life everywhere.
[01:17:04.560 --> 01:17:07.600]   And I think I agree on some levels in the sense that I think the physics that governs
[01:17:07.600 --> 01:17:12.720]   us is universal, but I don't know how far I would go to say that we're a likely phenomena,
[01:17:12.720 --> 01:17:18.480]   'cause we don't understand all of the features of the transition at the origin of life, which
[01:17:18.480 --> 01:17:25.520]   we would just say in assembly is you go from the no-memory physics to, there's like a critical
[01:17:25.520 --> 01:17:28.920]   transition around the assembly index where assemblyness starts to increase, and that's
[01:17:28.920 --> 01:17:33.000]   what we call the evolution of the biosphere and complexification of the biosphere.
[01:17:33.000 --> 01:17:35.880]   So there's a principle of increasing assemblyness, or that goes back to what I was saying at
[01:17:35.880 --> 01:17:40.280]   the very beginning about the physics of the possible, that the universe basically gets
[01:17:40.280 --> 01:17:45.080]   in this mode of trying to make as much possibilities as possible.
[01:17:45.080 --> 01:17:50.640]   Now how often that transition happens that you get the kind of cascading effect that
[01:17:50.640 --> 01:17:53.120]   we get in our biosphere, I think we don't know.
[01:17:53.120 --> 01:17:55.600]   If we did, we would know the likelihood of life in the universe.
[01:17:55.600 --> 01:17:58.280]   And a lot of people wanna say life is common, but I don't think that we can say that yet
[01:17:58.280 --> 01:18:01.440]   till we have the empirical data, which I think you would agree with.
[01:18:01.440 --> 01:18:06.560]   But then there's this other kind of thought experiment I have, which I don't like, but
[01:18:06.560 --> 01:18:12.840]   I did have it, which is if life emerges on one planet and you get this real high density
[01:18:12.840 --> 01:18:17.760]   of things that can exist on that planet, is it sort of dominating the density of creation
[01:18:17.760 --> 01:18:19.360]   that the universe can actually generate?
[01:18:19.360 --> 01:18:23.080]   So if you're thinking about counting entropy, the universe has a certain amount of stuff
[01:18:23.080 --> 01:18:29.160]   in it, and then assembly is kind of like an entropic principle, it's not entropy.
[01:18:29.160 --> 01:18:35.360]   But the idea is that now transformations among stuff or the actual physical histories of
[01:18:35.360 --> 01:18:40.080]   things now become things that you have to count as far as saying that these things exist
[01:18:40.080 --> 01:18:44.000]   and we're increasing the number of things that exist.
[01:18:44.000 --> 01:18:47.600]   And if you think about that cosmologically, maybe Earth is sucking up all the life potential
[01:18:47.600 --> 01:18:49.480]   of the whole universe, I don't know.
[01:18:49.480 --> 01:18:51.320]   But I have-- - How's that, can you explain that a little
[01:18:51.320 --> 01:18:52.320]   bit?
[01:18:52.320 --> 01:18:57.920]   Why can any one geographical region suck up the creative capacity of the universe?
[01:18:57.920 --> 01:19:01.720]   - It's just like, I know it's a ridiculous thought, I don't actually agree with it, but
[01:19:01.720 --> 01:19:02.720]   it was just a thought experiment.
[01:19:02.720 --> 01:19:07.560]   - I love that you can have thoughts that you don't like and don't agree with, but you have
[01:19:07.560 --> 01:19:09.280]   to think through them anyway.
[01:19:09.280 --> 01:19:10.280]   - Yeah.
[01:19:10.280 --> 01:19:12.000]   - The human mind is fascinating.
[01:19:12.000 --> 01:19:16.640]   - Yeah, I think these sort of counterfactual thought experiments are really good when you're
[01:19:16.640 --> 01:19:20.120]   trying to build new theories, 'cause you have to think through all the consequences.
[01:19:20.120 --> 01:19:25.200]   And there are people that wanna try to account for, say, the degrees of freedom on our planet
[01:19:25.200 --> 01:19:31.040]   in cosmological inventories of talking about the entropy of the universe and when we're
[01:19:31.040 --> 01:19:33.440]   thinking about cosmological arrow of time and things like that.
[01:19:33.440 --> 01:19:36.720]   Now, I think those are pretty superficial proposals as they stand now, but assembly
[01:19:36.720 --> 01:19:38.440]   would give you a way of counting it.
[01:19:38.440 --> 01:19:43.160]   And then the question is, if there's a certain maximal capacity of the universe's speed of
[01:19:43.160 --> 01:19:47.960]   generating stuff, which Lee always has this argument that assembly is about time, the
[01:19:47.960 --> 01:19:49.460]   universe is generating more states.
[01:19:49.460 --> 01:19:53.120]   Really what it's generating is more assembly possibilities.
[01:19:53.120 --> 01:19:57.520]   And then dark energy might be one manifestation of that, that the universe is accelerating
[01:19:57.520 --> 01:20:00.200]   its expansion because that makes more physical space.
[01:20:00.200 --> 01:20:04.160]   And what's happening on our planet is it's accelerating in the expansion of possible
[01:20:04.160 --> 01:20:07.860]   things that exist, and maybe the universe just has a maximal rate of what it can do
[01:20:07.860 --> 01:20:09.500]   to generate things.
[01:20:09.500 --> 01:20:13.200]   And then if there is a maximal rate, maybe only a certain number of planets can actually
[01:20:13.200 --> 01:20:17.600]   do that, or there's a trade-off about the pace of growth on certain planets versus others.
[01:20:17.600 --> 01:20:20.400]   - I have a million questions there, but do you have thoughts on--
[01:20:20.400 --> 01:20:22.400]   - Yeah, just a quick, yeah, I'll just say something very quick.
[01:20:22.400 --> 01:20:23.400]   - I love that experiment.
[01:20:23.400 --> 01:20:25.000]   - No, it's good, I think I get it, I think I get it.
[01:20:25.000 --> 01:20:34.040]   So what I want to say is, when I mean aliens are everywhere, I mean memories are the prerequisite
[01:20:34.040 --> 01:20:40.440]   for aliens via selection and then concentration of selection when selection becomes autonomous.
[01:20:40.440 --> 01:20:44.920]   So what I would love to do is to build, say, a magical telescope that was a memory--
[01:20:44.920 --> 01:20:45.920]   - Magical.
[01:20:45.920 --> 01:20:51.480]   - A magical one, yeah, sorry, or a real one, that would be a memory detector to see selection.
[01:20:51.480 --> 01:20:55.240]   So you could get to exoplanets and say, "That exoplanet looks like there's lots of selection
[01:20:55.240 --> 01:20:56.640]   going on there.
[01:20:56.640 --> 01:20:58.840]   Maybe there's evolution and maybe there's gonna be life."
[01:20:58.840 --> 01:21:01.080]   So what I'm just trying to say is narrow down the regions of space.
[01:21:01.080 --> 01:21:05.920]   We can say, "There's definitely evidence of memory as high assembly there," or not high
[01:21:05.920 --> 01:21:12.440]   assembly 'cause that would be life, but where it's capable of happening, and then that would
[01:21:12.440 --> 01:21:14.360]   also help us frame the search for aliens.
[01:21:14.360 --> 01:21:18.920]   I don't know how likely it is to make the transition to cells and all the other things.
[01:21:18.920 --> 01:21:23.320]   I think you're right, but I think that we just need to get more data.
[01:21:23.320 --> 01:21:26.680]   - Well, I didn't like the thought experiment 'cause I don't like the idea that if the universe
[01:21:26.680 --> 01:21:30.960]   has a maximal limit on the amount it can generate per unit time, that our existence is actually
[01:21:30.960 --> 01:21:32.200]   precluding the existence of other things.
[01:21:32.200 --> 01:21:33.200]   - Well, I'll just say one thing--
[01:21:33.200 --> 01:21:35.680]   - But I think that's probably true anyway 'cause of the resource limitations.
[01:21:35.680 --> 01:21:38.600]   - So I don't like your thought experiment because I think it's wrong.
[01:21:38.600 --> 01:21:41.600]   Well, no, no, no, I do like the thought experiment.
[01:21:41.600 --> 01:21:46.280]   So what you're trying to say is there is a chain of events that goes back that's manifestly
[01:21:46.280 --> 01:21:50.160]   culminated with life on Earth, and you're not saying that life isn't possible elsewhere.
[01:21:50.160 --> 01:21:53.800]   You say that there has been these number of contingent things that have happened that
[01:21:53.800 --> 01:21:56.800]   have allowed life to emerge here.
[01:21:56.800 --> 01:22:00.000]   That doesn't mean that life can't emerge elsewhere, but you're saying that the intersection of
[01:22:00.000 --> 01:22:04.240]   events may be concentrated here, right?
[01:22:04.240 --> 01:22:06.320]   - Not exactly.
[01:22:06.320 --> 01:22:12.960]   It's more like if you look at, say, the causal graphs are fundamental, maybe space is an
[01:22:12.960 --> 01:22:17.000]   emergent property, which is consistent with some proposals in quantum gravity, but also
[01:22:17.000 --> 01:22:19.280]   how we talk about things in assembly theory.
[01:22:19.280 --> 01:22:24.280]   Then the universe is causal graphs generating more structure in causal graphs, right?
[01:22:24.280 --> 01:22:26.320]   So this is how the universe is unfolding.
[01:22:26.320 --> 01:22:31.960]   And maybe there's a cap on the rate of generation, like there's only so much stuff that gets
[01:22:31.960 --> 01:22:34.800]   made per update of the universe.
[01:22:34.800 --> 01:22:38.760]   And then if there's a lot of stuff being made in a particular region that happens to look
[01:22:38.760 --> 01:22:44.280]   the same locally, spatially, that's an after effect of the fact that the whole causal graph
[01:22:44.280 --> 01:22:45.280]   is updating.
[01:22:45.280 --> 01:22:49.160]   - Yeah, I don't know that.
[01:22:49.160 --> 01:22:51.000]   I think that that doesn't work.
[01:22:51.000 --> 01:22:53.840]   - I don't think it works either, but I don't have a good argument in my mind about.
[01:22:53.840 --> 01:22:57.320]   - But I do like the idea of the capacity, 'cause you've got the number of states.
[01:22:57.320 --> 01:22:58.840]   Yeah, we can come back to it.
[01:22:58.840 --> 01:23:00.760]   - Well, let me ask real quick.
[01:23:00.760 --> 01:23:07.680]   Why does different local pockets of the universe start remembering stuff?
[01:23:07.680 --> 01:23:10.600]   How does memory emerge exactly?
[01:23:10.600 --> 01:23:16.240]   So at the origin of the universe, it was very forgetful.
[01:23:16.240 --> 01:23:22.440]   That's when the physicists were happiest, those low memory objects, which is like ultra
[01:23:22.440 --> 01:23:27.320]   low memory objects, which is what the definition of stuff.
[01:23:27.320 --> 01:23:31.120]   So how does memory emerge?
[01:23:31.120 --> 01:23:37.560]   How does the temporal stickiness of objects emerge?
[01:23:37.560 --> 01:23:43.200]   - I'm gonna take a very chemocentric point of view, because I can't imagine any other
[01:23:43.200 --> 01:23:44.560]   way of doing it.
[01:23:44.560 --> 01:23:47.520]   You could think of other ways, maybe.
[01:23:47.520 --> 01:23:53.520]   But I would say heterogeneity in matter is where the memory.
[01:23:53.520 --> 01:23:58.660]   So you must have enough different ways of rearranging matter for there to be a memory.
[01:23:58.660 --> 01:24:04.980]   So what that means is if you've got particles colliding in a box, let's just take some elements
[01:24:04.980 --> 01:24:10.360]   in a box, those elements can combine in a combinatorial set of ways.
[01:24:10.360 --> 01:24:15.600]   So there's a combinatorial explosion of the number of molecules or minerals or solid objects,
[01:24:15.600 --> 01:24:17.240]   bonds being made.
[01:24:17.240 --> 01:24:22.400]   Because there's such a large number, the population of different objects that are possible, this
[01:24:22.400 --> 01:24:27.240]   goes back to assembly theory, where assembly theory, there's four types of universes, right?
[01:24:27.240 --> 01:24:32.880]   So you've got basically, and this is what was up earlier, where one universe where you've
[01:24:32.880 --> 01:24:36.320]   just got everything is possible, so you can take all the atoms and combine them and make
[01:24:36.320 --> 01:24:37.600]   everything.
[01:24:37.600 --> 01:24:43.840]   Then you've got basically what is the assembly combinatorial, where you basically have to
[01:24:43.840 --> 01:24:46.080]   accrue information in steps.
[01:24:46.080 --> 01:24:49.320]   Then you've got assembly observed, right?
[01:24:49.320 --> 01:24:51.580]   And then you've got the object assembly going back.
[01:24:51.580 --> 01:24:56.200]   So what I'm trying to say is if you can take atoms and make bonds, let's say you take a
[01:24:56.200 --> 01:25:00.200]   nitrogen atom and add it to a carbon atom, you find an amino acid, then you add another
[01:25:00.200 --> 01:25:04.280]   carbon atom on it in a particular configuration, then another one, all different molecules,
[01:25:04.280 --> 01:25:07.880]   they all represent different histories.
[01:25:07.880 --> 01:25:13.600]   So I would say for me right now, the most simple route into life seems to be through
[01:25:13.600 --> 01:25:16.200]   recording memories and chemistry.
[01:25:16.200 --> 01:25:20.800]   But that doesn't mean there can't be other ways, there can't be other emergent effects,
[01:25:20.800 --> 01:25:26.360]   but I think if you can make bonds and lots of different bonds, and those molecules can
[01:25:26.360 --> 01:25:29.560]   have a causal effect on the future.
[01:25:29.560 --> 01:25:35.800]   So imagine a box of atoms, and then you combine those atoms in some way, so you make molecule
[01:25:35.800 --> 01:25:44.040]   A from a load of atoms, and then molecule A can go back to the box and influence the
[01:25:44.040 --> 01:25:46.240]   box.
[01:25:46.240 --> 01:25:52.660]   Then you make A prime or AB or ABC, and that process keeps going and that's where the memories
[01:25:52.660 --> 01:25:56.480]   come from, is that heterogeneity in the universe from bonding.
[01:25:56.480 --> 01:25:58.960]   I don't know if that makes any sense.
[01:25:58.960 --> 01:26:04.720]   - They're beginning to flourish at the chemistry level.
[01:26:04.720 --> 01:26:09.360]   So the physicists have no, like not enough.
[01:26:09.360 --> 01:26:20.400]   They're like desperately begging for more freedom and heterogeneous components to play
[01:26:20.400 --> 01:26:21.400]   with.
[01:26:21.400 --> 01:26:22.400]   - Yeah, that's exactly it.
[01:26:22.400 --> 01:26:24.760]   - What do you think about that, Sarah?
[01:26:24.760 --> 01:26:28.960]   - I mentioned already, I think it's significant that whatever physics governs life emerges
[01:26:28.960 --> 01:26:30.220]   actually in chemistry.
[01:26:30.220 --> 01:26:35.160]   It's not relevant at the subatomic scale or even at the atomic scale.
[01:26:35.160 --> 01:26:38.100]   It's in, well, atomic scale 'cause chemistry.
[01:26:38.100 --> 01:26:43.160]   But when you get into this combinatorial diversity that you get from combining things on the
[01:26:43.160 --> 01:26:48.600]   periodic table, that's when selection actually matters, or the fact that some things can
[01:26:48.600 --> 01:26:52.200]   exist and others can't exist actually starts to matter.
[01:26:52.200 --> 01:26:57.200]   So I think of it like you don't study gravity inside the atomic nucleus.
[01:26:57.200 --> 01:27:00.680]   You study it in terms of large-scale structure of the universe or black holes or things like
[01:27:00.680 --> 01:27:01.680]   that.
[01:27:01.680 --> 01:27:05.520]   And whatever we're talking about as physics of information or physics of assembly becomes
[01:27:05.520 --> 01:27:09.320]   relevant at a certain scale of reality.
[01:27:09.320 --> 01:27:13.600]   And the transition that you're talking about, I would think of as just when you get a sufficient
[01:27:13.600 --> 01:27:19.800]   density in terms of the assembly space of like the relationship of the overlap and the
[01:27:19.800 --> 01:27:26.200]   assembly space, which is like a feature of common memory, there is this transition to
[01:27:26.200 --> 01:27:29.400]   assembly-dominated physics, whatever that is.
[01:27:29.400 --> 01:27:32.840]   Like when we're talking about, and we're trying to map out exactly what that transition looks
[01:27:32.840 --> 01:27:33.840]   like.
[01:27:33.840 --> 01:27:37.800]   We're pretty sure of some of its features, but we haven't done all of the--
[01:27:37.800 --> 01:27:41.040]   - Do you think if you were there in the early universe, you would have been able to predict
[01:27:41.040 --> 01:27:43.320]   the emergence of chemistry and biology?
[01:27:43.320 --> 01:27:50.600]   And I ask that because at this stage as humans, do you think we can possibly predict the length
[01:27:50.600 --> 01:27:57.040]   of memory that might be able to be formed later on in this pocket of the universe?
[01:27:57.040 --> 01:28:02.880]   Like how complex is, what is the ceiling of assembly?
[01:28:02.880 --> 01:28:06.480]   - I think as much time as you have in the past is how much you can predict in the future.
[01:28:06.480 --> 01:28:13.800]   'Cause that is actually physical in the system and you have to have enough time for features
[01:28:13.800 --> 01:28:14.800]   of that structure to exist.
[01:28:14.800 --> 01:28:17.800]   - Wait, let me push back on that.
[01:28:17.800 --> 01:28:22.400]   Isn't there somewhere in the universe that's like a shortest path that's been, that stretches
[01:28:22.400 --> 01:28:23.720]   all the way to the beginning?
[01:28:23.720 --> 01:28:24.720]   - Yeah.
[01:28:24.720 --> 01:28:26.200]   - That's building some giant monster?
[01:28:26.200 --> 01:28:27.200]   - Maybe, yeah.
[01:28:27.200 --> 01:28:28.760]   - Yeah, so you can't predict the monster.
[01:28:28.760 --> 01:28:32.480]   - The universe has as much memory as the largest assembly object in the universe.
[01:28:32.480 --> 01:28:33.480]   - Yeah.
[01:28:33.480 --> 01:28:35.400]   - Right, but so you can't predict--
[01:28:35.400 --> 01:28:37.680]   - You can't predict any deeper than that, no.
[01:28:37.680 --> 01:28:43.560]   - Right, so I guess what I'm saying is like what intuition do you have about complexity
[01:28:43.560 --> 01:28:46.680]   living in the world that you'd have today, right?
[01:28:46.680 --> 01:28:54.560]   'Cause you just, you can, I mean I guess how long, does it get more fun?
[01:28:54.560 --> 01:28:58.120]   Isn't there gonna be at some point, 'cause there's a heat death in the universe, isn't
[01:28:58.120 --> 01:29:04.840]   there going to be a point of the most, of the highest assembly of object with the highest
[01:29:04.840 --> 01:29:06.600]   probability being generated?
[01:29:06.600 --> 01:29:09.840]   - When is the universe gonna be the most fun and can we freeze ourselves and then live
[01:29:09.840 --> 01:29:10.840]   then?
[01:29:10.840 --> 01:29:11.840]   - Exactly.
[01:29:11.840 --> 01:29:15.880]   And will you know when you're having the most fun that this is the best time, you're in
[01:29:15.880 --> 01:29:16.880]   your prime?
[01:29:16.880 --> 01:29:20.120]   Are you going to do what everyone does which is deny that you're in your prime and the
[01:29:20.120 --> 01:29:23.320]   best years are still ahead of you?
[01:29:23.320 --> 01:29:26.480]   - What option do you have?
[01:29:26.480 --> 01:29:31.360]   I mean the problem is there's lots of really interesting features here.
[01:29:31.360 --> 01:29:35.320]   I just want to mention one thing that might be, is I do think assembly theory applies
[01:29:35.320 --> 01:29:40.480]   all the way back to subatomic particles and I also think that cosmological selection might
[01:29:40.480 --> 01:29:43.920]   have been actually, there might have been, I would say it's a really boring bit, but
[01:29:43.920 --> 01:29:47.720]   it's really important if you're a cosmologist that universes have gone through.
[01:29:47.720 --> 01:29:49.760]   Was it Lee Smolin who proposed this maybe?
[01:29:49.760 --> 01:29:53.840]   That there is this, that basically the universe evolves, you've got the wrong constants, we'll
[01:29:53.840 --> 01:29:58.200]   start again and the most productive constants where you can allow particles to form in a
[01:29:58.200 --> 01:30:01.240]   certain way, propagate to the next universe and we go again.
[01:30:01.240 --> 01:30:04.800]   So actually selection goes all the way back and there's these cycles of universes and
[01:30:04.800 --> 01:30:10.480]   now this universe has been selected because life can occur and it carries on.
[01:30:10.480 --> 01:30:12.640]   But I've really butchered that.
[01:30:12.640 --> 01:30:13.640]   There's a much more--
[01:30:13.640 --> 01:30:18.360]   - So there's some aspect where through the selection process there's parameters that
[01:30:18.360 --> 01:30:22.980]   are being fine-tuned and we happen to be living in one where there's some level of fine-tuning.
[01:30:22.980 --> 01:30:31.480]   Is there, given that, can you still man the case that we humans are alone in the universe?
[01:30:31.480 --> 01:30:35.000]   We're the highest assembly index object in the universe.
[01:30:35.000 --> 01:30:36.000]   - Yeah I can, I guess.
[01:30:36.000 --> 01:30:37.000]   It's sad though.
[01:30:37.000 --> 01:30:38.320]   I mean, so from a--
[01:30:38.320 --> 01:30:39.320]   - Is it possible?
[01:30:39.320 --> 01:30:41.680]   - Yes, it's possible.
[01:30:41.680 --> 01:30:47.600]   Let's assume, well we know, I mean it's possible.
[01:30:47.600 --> 01:30:55.080]   So let me, so okay, so there is a particular set of elements on Earth in a particular ratio
[01:30:55.080 --> 01:31:00.800]   and the right gravitational constant and the right viscosity of stuff being able to move
[01:31:00.800 --> 01:31:07.840]   around, the right distance from our sun, right number of events where we have a moon, the
[01:31:07.840 --> 01:31:16.000]   Earth is rotating, the late heavy bombardment produced a lot of, brought in the right stuff
[01:31:16.000 --> 01:31:23.120]   and Mars was cooking up the right molecules first so it was habitable before Earth.
[01:31:23.120 --> 01:31:30.280]   It was actually doing the combinatorial search and before Mars kind of became unhabitable,
[01:31:30.280 --> 01:31:36.240]   it seeded Earth with the right molecular replicators and there was just the right stuff on Earth
[01:31:36.240 --> 01:31:41.000]   and that's how the miracle of life occurred.
[01:31:41.000 --> 01:31:48.240]   So I find I'm very uncomfortable with that because actually, because life came so quickly
[01:31:48.240 --> 01:31:50.960]   in the Earth's past.
[01:31:50.960 --> 01:31:55.700]   But that doesn't mean that life is easy elsewhere.
[01:31:55.700 --> 01:32:00.480]   It just might mean that, 'cause chemistry is actually not a long-term thing.
[01:32:00.480 --> 01:32:04.160]   Chemistry can happen quickly so maybe going on with the steel manning of the argument
[01:32:04.160 --> 01:32:08.400]   to say actually the fact that life emerged quickly doesn't mean that life is easy.
[01:32:08.400 --> 01:32:14.120]   It just means that the chemistry was right on Earth and Earth is very special and that's
[01:32:14.120 --> 01:32:16.720]   why there's no life anywhere else in the universe.
[01:32:16.720 --> 01:32:21.760]   - Yeah, so Sarah mentioned this kind of cascading thing.
[01:32:21.760 --> 01:32:29.840]   So what if that's the reason we're lucky is that we got to have a rare cascading of, like
[01:32:29.840 --> 01:32:34.560]   an accelerating cascading effect in terms of the complexity of things?
[01:32:34.560 --> 01:32:40.240]   Like maybe most of the universe is trying to get sticky with the memory and it's not
[01:32:40.240 --> 01:32:43.160]   able to really form it and then we got really lucky in that.
[01:32:43.160 --> 01:32:47.720]   And it has nothing, like there's a lot of Earth-like conditions, let's say, but it's
[01:32:47.720 --> 01:32:51.800]   just you really, really have to get lucky on this.
[01:32:51.800 --> 01:32:54.200]   - But I'm doing experiments right now.
[01:32:54.200 --> 01:32:57.440]   In fact, experiments that Sarah and I are working on 'cause we have some joint funding
[01:32:57.440 --> 01:33:01.520]   for this where we're seeing that the universe can get sticky really quickly.
[01:33:01.520 --> 01:33:06.200]   Now of course we're being very anthropocentric, we're using laboratory tools, we're using
[01:33:06.200 --> 01:33:14.400]   theory, but actually the phenomena of selection, the process of developing heterogeneity, we
[01:33:14.400 --> 01:33:15.400]   can do in the lab.
[01:33:15.400 --> 01:33:17.840]   We're just seeing the very first hints of it.
[01:33:17.840 --> 01:33:26.400]   And wouldn't it be great if we can start to pin down a bit more precisely, becoming good
[01:33:26.400 --> 01:33:30.760]   Bayesianists for this, for the origin of life and the emergence of life, to finding out
[01:33:30.760 --> 01:33:33.640]   what kind of chemistries we really need to look for.
[01:33:33.640 --> 01:33:38.080]   And I'm becoming increasingly confident we'll be able to do that in the next few years.
[01:33:38.080 --> 01:33:43.400]   Make life in the lab or make some selection in the lab from inorganic stuff, from sand,
[01:33:43.400 --> 01:33:45.440]   from rocks, from dead stuff, from moon.
[01:33:45.440 --> 01:33:51.120]   Wouldn't it be great to get stuff from the moon, put it in our origin of life experiment
[01:33:51.120 --> 01:33:56.560]   and make moon life and restrict ourselves to interesting self-replicating stuff that
[01:33:56.560 --> 01:33:57.560]   we find on the moon?
[01:33:57.560 --> 01:34:03.000]   - Sarah, what do you think about this approach of engineering life in order to understand
[01:34:03.000 --> 01:34:04.000]   life?
[01:34:04.000 --> 01:34:05.720]   So building life in the machine.
[01:34:05.720 --> 01:34:14.760]   - Yeah, so, I mean, Lee and I are trying right now to build a vision for a large institute
[01:34:14.760 --> 01:34:17.480]   or experimental program, basically, to do this problem.
[01:34:17.480 --> 01:34:21.080]   But I think of it as like, we need to simulate a planet.
[01:34:21.080 --> 01:34:25.120]   So like the Large Hadron Collider was supposed to be simulating conditions just after the
[01:34:25.120 --> 01:34:26.120]   Big Bang.
[01:34:26.120 --> 01:34:30.240]   Lee's built a lot of technology in his lab to do these kind of selection engines.
[01:34:30.240 --> 01:34:35.080]   But the question you're asking is how many experiments do you need to run?
[01:34:35.080 --> 01:34:40.720]   What volume of chemical space do you need to explore before you actually see an event?
[01:34:40.720 --> 01:34:43.720]   And I like to make an analogy to one of my favorite particle physics experiments, which
[01:34:43.720 --> 01:34:46.640]   is Super Kamiokande that's looking for the decay of the proton.
[01:34:46.640 --> 01:34:51.560]   So this is something that we predicted theoretically, but we've never observed in our universe.
[01:34:51.560 --> 01:34:55.360]   And basically what they're doing is every time they don't see a proton decay event,
[01:34:55.360 --> 01:34:57.840]   they have a longer bound on the lifetime of a proton.
[01:34:57.840 --> 01:35:03.000]   So imagine we built an experiment with the idea in mind of trying to simulate planetary
[01:35:03.000 --> 01:35:04.480]   conditions, physically simulate.
[01:35:04.480 --> 01:35:06.320]   You can't simulate original life in a computer.
[01:35:06.320 --> 01:35:09.000]   You have to do it in an experiment.
[01:35:09.000 --> 01:35:13.200]   Simulate enough planetary conditions to explore the space of what's possible and bound the
[01:35:13.200 --> 01:35:15.160]   probability for an original life event.
[01:35:15.160 --> 01:35:17.600]   Even if you're not observing it, you can talk about the probability.
[01:35:17.600 --> 01:35:27.240]   That we hopefully, life is not exponentially rare and we would then be able to evolve in
[01:35:27.240 --> 01:35:31.800]   an automated system alien life in the lab.
[01:35:31.800 --> 01:35:35.520]   And if we can do that, then we understand the physics as well as we understand what
[01:35:35.520 --> 01:35:37.520]   we can do in particle accelerators.
[01:35:37.520 --> 01:35:44.280]   So keep expanding physically the simulation, the physical simulation until something happens.
[01:35:44.280 --> 01:35:48.480]   Yeah, or just build a big enough volume of chemical experiments and evolve them.
[01:35:48.480 --> 01:35:50.760]   When you say volume, you mean like literally volume.
[01:35:50.760 --> 01:35:54.800]   I mean physical volume in terms of space, but I actually mean volume in terms of the
[01:35:54.800 --> 01:35:57.920]   combinatorial space of chemistry.
[01:35:57.920 --> 01:36:03.080]   How do you nicely control the combinatorial exploration, the search space?
[01:36:03.080 --> 01:36:07.880]   Such that it's always like you keep grabbing the low hanging fruit.
[01:36:07.880 --> 01:36:08.880]   Yeah.
[01:36:08.880 --> 01:36:10.600]   How do you build a search engine for chemistry?
[01:36:10.600 --> 01:36:11.600]   I think you explained it really well.
[01:36:11.600 --> 01:36:12.600]   We should carry on doing this.
[01:36:12.600 --> 01:36:15.240]   We should pretend the physics, be the physicist, you be the chemist.
[01:36:15.240 --> 01:36:22.840]   So the way to do it is, I will always play a joke because I like writing grants to ask
[01:36:22.840 --> 01:36:26.320]   for money to do cool stuff.
[01:36:26.320 --> 01:36:31.200]   Years ago I started wanting to build, so I actually wanted to wear the, so I built this
[01:36:31.200 --> 01:36:36.760]   robot in my lab called the computer, which is this robot you can program to do chemistry.
[01:36:36.760 --> 01:36:42.800]   Now it's a pro, I made a programming language for the computer and made it operate chemical
[01:36:42.800 --> 01:36:43.800]   equipment.
[01:36:43.800 --> 01:36:49.340]   Originally I wrote grants to say, "Hey, I want to make an origin of life system and
[01:36:49.340 --> 01:36:51.760]   no one would give me any money for this.
[01:36:51.760 --> 01:36:52.760]   This is ridiculous.
[01:36:52.760 --> 01:36:54.640]   Why are you wanting to make it?
[01:36:54.640 --> 01:36:55.640]   It's really hard.
[01:36:55.640 --> 01:36:56.640]   It takes forever.
[01:36:56.640 --> 01:36:58.520]   You're not a very good origin of life chemist anyway.
[01:36:58.520 --> 01:37:00.560]   Why would we give you any money?"
[01:37:00.560 --> 01:37:04.440]   And so I turned it around and said, "Can you, can instead, can you give me money to
[01:37:04.440 --> 01:37:07.160]   make robots, to make molecules that are interesting?"
[01:37:07.160 --> 01:37:10.880]   And everyone went, "Yeah, okay, you can do that."
[01:37:10.880 --> 01:37:17.200]   And that's, so actually the funny thing is the computer project, which I have in my lab,
[01:37:17.200 --> 01:37:20.960]   which is very briefly, it's just basically, it's like literally an automated test tube
[01:37:20.960 --> 01:37:26.400]   and we've made a programming language for the test tube, which is cool, has come, has
[01:37:26.400 --> 01:37:28.680]   literally came from this.
[01:37:28.680 --> 01:37:31.680]   I went to my lab one day, I said, "I want to make a search engine to get the origin
[01:37:31.680 --> 01:37:33.680]   of life because I don't have a planet."
[01:37:33.680 --> 01:37:36.280]   And I thought about doing in a microfluidic format.
[01:37:36.280 --> 01:37:41.040]   So microfluidic is very nano, very small channels in device where you can basically have all
[01:37:41.040 --> 01:37:46.120]   the pipes lit up, produced by lithography, and you can have a chamber maybe say between
[01:37:46.120 --> 01:37:48.640]   say 10 and a hundred microns in volume.
[01:37:48.640 --> 01:37:53.000]   And we slot them all together like Lego and we can make an origin of life system.
[01:37:53.000 --> 01:37:55.880]   And I could never get it to work.
[01:37:55.880 --> 01:38:01.320]   And I realized I had to make, do chemistry at the kind of test tube level.
[01:38:01.320 --> 01:38:06.520]   And what you want to be able to do, yeah, it goes back to that tweet in 1981.
[01:38:06.520 --> 01:38:10.280]   - 1981, the computer, we're looking at a tweet from Lee.
[01:38:10.280 --> 01:38:13.160]   In 1981, the computer was a distant dream.
[01:38:13.160 --> 01:38:19.360]   And oh wow, this is the scientist looking back at his, the young boy who dreamed.
[01:38:19.360 --> 01:38:24.320]   In 2018, it was realized, spelled in a British way, realized.
[01:38:24.320 --> 01:38:27.960]   - Yeah, I'm starting to feel sad, but not.
[01:38:27.960 --> 01:38:32.280]   So now there's a system that does the physical manifestation or whatever the programming
[01:38:32.280 --> 01:38:36.920]   language, the spec tells you to do.
[01:38:36.920 --> 01:38:40.120]   - Yeah, well in 1981, I got my first computer, ZX81.
[01:38:40.120 --> 01:38:42.040]   - What was the computer?
[01:38:42.040 --> 01:38:43.040]   - ZX81.
[01:38:43.040 --> 01:38:44.040]   - ZX81.
[01:38:44.040 --> 01:38:46.120]   - Sinclair ZX81.
[01:38:46.120 --> 01:38:53.320]   It was, and I got a chemistry set and I liked the chemistry set and I liked the computer
[01:38:53.320 --> 01:38:54.920]   and I just wanted to put them together.
[01:38:54.920 --> 01:38:57.840]   I thought, wouldn't it be cool if I could just use the computer to control the chemistry
[01:38:57.840 --> 01:38:58.840]   set?
[01:38:58.840 --> 01:39:00.560]   And obviously that was insane.
[01:39:00.560 --> 01:39:04.440]   And I was like, you know, eight years old, right?
[01:39:04.440 --> 01:39:06.960]   Nine years old, getting on nine years old.
[01:39:06.960 --> 01:39:14.920]   And then I invented the computer just because I wanted to build this origin of life grid,
[01:39:14.920 --> 01:39:15.920]   right?
[01:39:15.920 --> 01:39:21.200]   Which is like literally a billion test tubes connected together in real time and real space,
[01:39:21.200 --> 01:39:24.880]   basically throwing a chemical die, dice, throw dice, throw dice, throw dice.
[01:39:24.880 --> 01:39:26.600]   You're going to get lucky.
[01:39:26.600 --> 01:39:30.680]   And that's what we, I think Sarah and I have been thinking very deeply about.
[01:39:30.680 --> 01:39:37.440]   Because there's more money being spent on the origin of the gravity or looking at the
[01:39:37.440 --> 01:39:39.720]   Higgs boson than the origin of life, right?
[01:39:39.720 --> 01:39:44.960]   And the origin of life is the, I think the biggest question or not the biggest question.
[01:39:44.960 --> 01:39:45.960]   It is a big question.
[01:39:45.960 --> 01:39:46.960]   Let's put it that way.
[01:39:46.960 --> 01:39:47.960]   - It is the biggest question.
[01:39:47.960 --> 01:39:48.960]   You're okay saying that.
[01:39:48.960 --> 01:39:49.960]   - Okay, all right.
[01:39:50.040 --> 01:39:56.040]   - Isn't it possible once you figure out the origin of life that that's not going to solve,
[01:39:56.040 --> 01:40:00.400]   that's not actually going to solve the question of what is life?
[01:40:00.400 --> 01:40:01.400]   Like isn't it?
[01:40:01.400 --> 01:40:02.800]   Because you're kind of putting a lot of-
[01:40:02.800 --> 01:40:04.720]   - Yeah, I think that's the same problem.
[01:40:04.720 --> 01:40:10.960]   - But you're putting, is it possible that you're putting too many, too much bets into
[01:40:10.960 --> 01:40:12.240]   this origin part?
[01:40:12.240 --> 01:40:16.640]   Maybe the origin thing isn't, isn't there always a turtle underneath the turtle?
[01:40:16.640 --> 01:40:17.640]   Isn't it a stack of turtles?
[01:40:18.320 --> 01:40:21.600]   Because then if you create it in the lab, maybe you need some other stuff.
[01:40:21.600 --> 01:40:24.440]   - Well, that's not the thing, but the origin-
[01:40:24.440 --> 01:40:26.960]   - Like in the lab, there's still memory.
[01:40:26.960 --> 01:40:27.960]   - Yeah.
[01:40:27.960 --> 01:40:28.960]   Yes.
[01:40:28.960 --> 01:40:29.960]   - Right.
[01:40:29.960 --> 01:40:31.640]   - The experiment is already the product of evolution.
[01:40:31.640 --> 01:40:32.640]   - Right.
[01:40:32.640 --> 01:40:36.160]   In some maybe really deep way, not an obvious way, in some very deep way.
[01:40:36.160 --> 01:40:42.080]   So maybe the haters are always going to be like, well, you have to reconstruct the fold.
[01:40:42.080 --> 01:40:43.080]   You have to build it from scratch.
[01:40:43.080 --> 01:40:46.080]   - Fortunately for us, the haters are not aware of that argument.
[01:40:46.080 --> 01:40:47.640]   - Well, no, I know.
[01:40:47.640 --> 01:40:51.080]   - We're the one making that argument usually, but yeah.
[01:40:51.080 --> 01:40:56.720]   - I just think that if we create life in the lab, it's not obvious that you'll get to the
[01:40:56.720 --> 01:41:02.760]   deep, deep understanding of necessarily what is the line between life and non-life.
[01:41:02.760 --> 01:41:03.760]   - No, I think so.
[01:41:03.760 --> 01:41:05.000]   Look, there's so much here.
[01:41:05.000 --> 01:41:06.400]   - I'm just like playing devil's advocate.
[01:41:06.400 --> 01:41:10.560]   - So much here, but let me play devil's advocate back in a previous conversation, right?
[01:41:10.560 --> 01:41:12.600]   And say, yeah, I will.
[01:41:12.600 --> 01:41:13.600]   Why not?
[01:41:13.600 --> 01:41:14.600]   We've got time.
[01:41:14.600 --> 01:41:15.600]   - Yeah, let's go.
[01:41:15.600 --> 01:41:16.600]   - Cellular automata.
[01:41:16.600 --> 01:41:17.600]   - Yes.
[01:41:17.600 --> 01:41:24.960]   - Cellular automata, these very simple things where you color squares black or white and
[01:41:24.960 --> 01:41:26.880]   implement rules and play them in time.
[01:41:26.880 --> 01:41:31.000]   And you can get these very, very complex patterns coming out.
[01:41:31.000 --> 01:41:35.200]   There's nice rules, there are Turing complete rules.
[01:41:35.200 --> 01:41:41.640]   And I would argue that cellular automata don't really exist on their own.
[01:41:41.640 --> 01:41:45.160]   They have to exist in a computing device.
[01:41:45.160 --> 01:41:48.080]   If that, whether it's computing devices, a piece of paper, an abstraction, a mathematician
[01:41:48.080 --> 01:41:53.680]   drawing a grid or a framework.
[01:41:53.680 --> 01:41:59.360]   Now, so I would argue CAs are beautiful things, simple going complex, but the complexity is
[01:41:59.360 --> 01:42:02.680]   all borrowed from the lithography, the numbers.
[01:42:02.680 --> 01:42:11.240]   Right, now let's take that same argument with the chemistry experiment, origin of life.
[01:42:11.240 --> 01:42:14.200]   What you need to be able to do is go out, and I'm inspired to do this, to go out and
[01:42:14.200 --> 01:42:17.800]   look for CAs that occur in nature.
[01:42:17.800 --> 01:42:24.480]   You know, let's kind of, let's find some CAs that just emerge in our universe.
[01:42:24.480 --> 01:42:29.640]   - And for people, just to interrupt, for people just listening and in general, I think what
[01:42:29.640 --> 01:42:36.200]   we're looking at is a cellular automata where again, as Lee described, there is just binary
[01:42:36.200 --> 01:42:41.720]   black or white squares, and they only have local information, and they're born and they
[01:42:41.720 --> 01:42:42.920]   die.
[01:42:42.920 --> 01:42:46.560]   And you would think nothing interesting would emerge, but actually what we're looking at
[01:42:46.560 --> 01:42:54.560]   is something that I believe is called glider guns, or a glider gun, which is moving objects
[01:42:54.560 --> 01:43:01.320]   in this multi-cell space that look like they're organisms that have much more information,
[01:43:01.320 --> 01:43:06.760]   that have much more complexity than the individual building components, in fact look like they
[01:43:06.760 --> 01:43:10.040]   have a long-term memory.
[01:43:10.040 --> 01:43:14.560]   While the individual components don't seem like they have any memory at all, which is
[01:43:14.560 --> 01:43:15.560]   fascinating.
[01:43:15.560 --> 01:43:20.240]   - Yes, the argument here is that has to exist on all this layer of infrastructure, right,
[01:43:20.240 --> 01:43:21.720]   and though it looks simple.
[01:43:21.720 --> 01:43:24.960]   And then what I would make, the argument I would make if I were you, say, well I think
[01:43:24.960 --> 01:43:29.680]   CAs are really simple and everywhere, is say, show me how they emerge in a substrate.
[01:43:29.680 --> 01:43:32.600]   Now let's go to the origin of life, or machine.
[01:43:32.600 --> 01:43:36.640]   I don't think we want to do the origin of life, just any origin is good.
[01:43:36.640 --> 01:43:41.180]   So what we do, so we literally have our sand shaker, shake the sand, like massive grid
[01:43:41.180 --> 01:43:45.400]   of chemistry experiments, shaking sand, shaking whatever.
[01:43:45.400 --> 01:43:48.840]   And then because we know what we've put in, so we know how we've cheated, and the same
[01:43:48.840 --> 01:43:52.400]   way with CA, we know how we've cheated, we know the number of operations needed, we know
[01:43:52.400 --> 01:43:54.740]   how big a grid we want to get this.
[01:43:54.740 --> 01:44:03.720]   If we could then say, okay, how can we generate this recipe in the lab and make a life form,
[01:44:03.720 --> 01:44:09.480]   what contingency did we need to put in, and we're up front about how we cheated, okay,
[01:44:09.480 --> 01:44:15.880]   say oh you had to shake it, it was periodic, planet rotates, stride comes in and out.
[01:44:15.880 --> 01:44:21.560]   So and then we can start to basically say, okay, how difficult is it for these features
[01:44:21.560 --> 01:44:25.360]   to be found, and then we can look for exoplanets and other features.
[01:44:25.360 --> 01:44:29.360]   So I think Sarah's absolutely right, we want to explain to people we're cheating.
[01:44:29.360 --> 01:44:30.820]   In fact, we have to cheat.
[01:44:30.820 --> 01:44:33.800]   No one has given, I'm good at writing grants, well I used to be, I'm not very good right
[01:44:33.800 --> 01:44:38.140]   now, I keep getting rejected, but writing a grant for a planet in 100 million years,
[01:44:38.140 --> 01:44:43.460]   no grant funder is gonna give me that, but maybe money to make a kind of a grid, a computer
[01:44:43.460 --> 01:44:47.000]   grid, origin of life computer grid.
[01:44:47.000 --> 01:44:48.320]   In physical space.
[01:44:48.320 --> 01:44:49.320]   And just do it.
[01:44:49.320 --> 01:44:56.420]   - So Sarah said something, which is you can't simulate the origin of life in a computer,
[01:44:56.420 --> 01:45:00.220]   so like in simulation, why not?
[01:45:00.220 --> 01:45:05.200]   You said it very confidently, so is it possible?
[01:45:05.200 --> 01:45:06.800]   And why would it be very difficult?
[01:45:06.800 --> 01:45:08.880]   What's your intuition there?
[01:45:08.880 --> 01:45:13.840]   - I think it's very difficult right now 'cause we don't know the physics, but if you go based
[01:45:13.840 --> 01:45:18.180]   on principles of assembly theory and you think every molecule is actually a very large causal
[01:45:18.180 --> 01:45:22.100]   graph, not just the molecule, then you have to simulate all the features of those causal
[01:45:22.100 --> 01:45:25.520]   graphs, and I think it becomes computationally intractable, you might as well just build
[01:45:25.520 --> 01:45:27.140]   the experiment.
[01:45:27.140 --> 01:45:32.860]   - 'Cause you have, in the physical space, you have all the objects with all the memories,
[01:45:32.860 --> 01:45:36.700]   and in the computer, you would have to copy them or reconstruct them.
[01:45:36.700 --> 01:45:43.340]   - That's beautifully put, and I would say that lots of people, you just don't have enough
[01:45:43.340 --> 01:45:45.380]   resources.
[01:45:45.380 --> 01:45:50.460]   It's easier to actually do the physical experiment because we are literally, I would view the
[01:45:50.460 --> 01:45:54.380]   physical experiment almost like a computational experiment.
[01:45:54.380 --> 01:45:57.780]   It's just basically we're just outsourcing all the matrix.
[01:45:57.780 --> 01:46:04.460]   - And on your point about the experiment being also an example of life, it's almost like
[01:46:04.460 --> 01:46:09.820]   you wanna design, it's like all of us are lineages of propagating information across
[01:46:09.820 --> 01:46:14.240]   time, and so everything we do becomes part of life because it's part of that causal chain.
[01:46:14.240 --> 01:46:17.980]   So it's like you wanna try to pinch off as much as you can of the information from your
[01:46:17.980 --> 01:46:22.240]   causal chain that goes into the experiment, but you can't pinch off all of it to move
[01:46:22.240 --> 01:46:23.760]   it to a different timeline.
[01:46:23.760 --> 01:46:27.020]   It's always gonna be part of your timeline, but at least if you can control how much information
[01:46:27.020 --> 01:46:31.540]   you put in, you can try to see how much does that particular trajectory you set up start
[01:46:31.540 --> 01:46:33.900]   generating its own assembly.
[01:46:33.900 --> 01:46:38.780]   So you know where it starts, and then you wanna try to see it take off on its own when
[01:46:38.780 --> 01:46:42.380]   you try to pinch it off as much as possible.
[01:46:42.380 --> 01:46:43.860]   - Got it.
[01:46:43.860 --> 01:46:45.420]   Quick pause, bathroom break.
[01:46:45.420 --> 01:46:46.420]   - Yes.
[01:46:46.420 --> 01:46:47.700]   - All right, cool.
[01:46:47.700 --> 01:46:49.660]   And now we're back.
[01:46:49.660 --> 01:46:54.420]   All right, we talked about the early days of the universe when there was just stuff
[01:46:54.420 --> 01:46:57.240]   and no memory, not even causality.
[01:46:57.240 --> 01:47:00.680]   I think Lee at least implied that causality is emergent somehow.
[01:47:00.680 --> 01:47:02.100]   We could discuss this.
[01:47:02.100 --> 01:47:07.580]   What happened before this all originated?
[01:47:07.580 --> 01:47:09.140]   What's outside the universe?
[01:47:09.140 --> 01:47:11.620]   - Defined by zero.
[01:47:11.620 --> 01:47:16.500]   - Okay, so it's not relevant, not understandable.
[01:47:16.500 --> 01:47:18.660]   Is it useful to even ask the question?
[01:47:18.660 --> 01:47:19.660]   - No.
[01:47:19.660 --> 01:47:22.020]   - Just because it's so hard?
[01:47:22.020 --> 01:47:23.020]   - No, it's not hard.
[01:47:23.020 --> 01:47:24.980]   It's just not a question.
[01:47:24.980 --> 01:47:28.900]   If I can't do an experiment or even think of an experiment, the question doesn't exist.
[01:47:28.900 --> 01:47:34.260]   - Well, no, you can't think of a lot of experiments, no offense.
[01:47:34.260 --> 01:47:35.260]   - What I mean is if I can't--
[01:47:35.260 --> 01:47:40.540]   - 'Cause your causality graph is like, this is what we've been talking about.
[01:47:40.540 --> 01:47:45.660]   There is limits to your ability to construct experiments.
[01:47:45.660 --> 01:47:49.380]   - I agree, but I'll be facetious and I'll try to make a point.
[01:47:49.380 --> 01:47:58.660]   I think that if there is a causal bottleneck through which information can't propagate
[01:47:58.660 --> 01:48:04.980]   in principle, then it's very hard to think of an experiment, even in principle, even
[01:48:04.980 --> 01:48:09.420]   one that's beyond my mediocre intellect, which is fine.
[01:48:09.420 --> 01:48:11.020]   I'm happy to accept that.
[01:48:11.020 --> 01:48:14.780]   But this is one of the things I actually do think there was something before the Big Bang
[01:48:14.780 --> 01:48:19.780]   'cause I would say that I think the Big Bang just couldn't occur and create time.
[01:48:19.780 --> 01:48:21.420]   Time created the Big Bang.
[01:48:21.420 --> 01:48:23.380]   - So there was time before the Big Bang?
[01:48:23.380 --> 01:48:24.380]   - Yeah.
[01:48:24.380 --> 01:48:26.380]   - There was no space, but there was time?
[01:48:26.380 --> 01:48:27.380]   - Yeah.
[01:48:27.380 --> 01:48:28.380]   Yeah.
[01:48:28.380 --> 01:48:31.100]   But I mean, I'm just making that stuff up just to make all the physicists happy, but
[01:48:31.100 --> 01:48:32.100]   I think it's--
[01:48:32.100 --> 01:48:33.940]   - Do you think that would make them happy?
[01:48:33.940 --> 01:48:35.940]   'Cause they would be quite upset, actually.
[01:48:35.940 --> 01:48:37.340]   - Why would they be upset?
[01:48:37.340 --> 01:48:41.740]   - 'Cause they would say that time can't exist before the Big Bang.
[01:48:41.740 --> 01:48:45.500]   - Yeah, I mean, this goes back to an argument that you might not want to have the argument
[01:48:45.500 --> 01:48:46.500]   here.
[01:48:46.500 --> 01:48:50.500]   I was talking to Sarah earlier today about an argument we had about time a long time
[01:48:50.500 --> 01:48:51.500]   ago.
[01:48:51.500 --> 01:48:52.500]   - Yeah.
[01:48:52.500 --> 01:48:53.500]   - A long time in time.
[01:48:53.500 --> 01:48:57.260]   And what I would, it's like, I think there is this thing called time or state creation.
[01:48:57.260 --> 01:49:02.180]   The universe is creating states and it's outside of space, but they create space.
[01:49:02.180 --> 01:49:06.300]   So what I mean is, you can imagine there are states being created all the time and there
[01:49:06.300 --> 01:49:12.700]   is this thing called time, time as a clock, which you can use to measure when things happen.
[01:49:12.700 --> 01:49:17.380]   But that doesn't mean, because you can't measure something, that states aren't being created.
[01:49:17.380 --> 01:49:25.460]   And so you might locally refer to the Big Bang and the Big Bang occurred at some point
[01:49:25.460 --> 01:49:27.820]   in when those states were there.
[01:49:27.820 --> 01:49:31.340]   Probably there had to be enough states for the Big Bang to occur.
[01:49:31.340 --> 01:49:36.380]   And then, but I think that there is something wrong with our conception of how the universe
[01:49:36.380 --> 01:49:41.060]   was created and the Big Bang, because we don't really get time.
[01:49:41.060 --> 01:49:47.420]   Because again, I don't want to become boring and sound like a broken record, but time is
[01:49:47.420 --> 01:49:49.120]   a real thing.
[01:49:49.120 --> 01:49:54.260]   And until I can really explain that more elegantly, I'm just going to get into more trouble.
[01:49:54.260 --> 01:49:59.540]   - We're going to talk about time, because time is a useful measuring device for experiments,
[01:49:59.540 --> 01:50:02.340]   but also time is an ideal, okay.
[01:50:02.340 --> 01:50:04.940]   But let me first ask Sarah, what do you think?
[01:50:04.940 --> 01:50:08.740]   Is it a useful question to ask what happened before the Big Bang?
[01:50:08.740 --> 01:50:15.140]   Is it a useful question to ask what's outside the universe?
[01:50:15.140 --> 01:50:20.420]   - So I would think about it as the Big Bang is an event that we reconstructed as probably
[01:50:20.420 --> 01:50:24.420]   happening in the past of our universe, based on current observational data.
[01:50:24.420 --> 01:50:33.180]   And so the way I like to think about it is, we exist locally in something called a universe.
[01:50:33.180 --> 01:50:37.980]   So and going back to like the physics of existence, we exist locally in the space of all things
[01:50:37.980 --> 01:50:39.320]   that could exist.
[01:50:39.320 --> 01:50:43.660]   And we can infer certain properties of the structure of where we exist locally.
[01:50:43.660 --> 01:50:50.180]   And one of the properties that we've inferred in the past is that there is a thing we call
[01:50:50.180 --> 01:50:51.180]   the Big Bang.
[01:50:51.180 --> 01:50:58.620]   And we have signatures of our local environment that indicate that there was a very low information
[01:50:58.620 --> 01:50:59.900]   event that started our universe.
[01:50:59.900 --> 01:51:07.180]   I think that's actually just an artifact of the structure of the assembly space that when
[01:51:07.180 --> 01:51:13.440]   you start losing all the memory in the objects, it looks like what we call a Big Bang.
[01:51:13.440 --> 01:51:16.260]   So I think it makes sense to talk about where you are locally.
[01:51:16.260 --> 01:51:21.220]   I think it makes sense to talk about counterfactual possibilities, what could exist outside the
[01:51:21.220 --> 01:51:25.620]   universe in the sense that they become part of our reasoning, and therefore part of our
[01:51:25.620 --> 01:51:28.660]   causal chain of things that we can do.
[01:51:28.660 --> 01:51:33.820]   So like the multiverse in my mind exists, but it doesn't exist as a multiverse of possible
[01:51:33.820 --> 01:51:34.820]   universes.
[01:51:34.820 --> 01:51:38.540]   It exists as an idea in our minds that allows us to reason about how physics works, and
[01:51:38.540 --> 01:51:42.240]   then to do physics differently because we reason about it that way.
[01:51:42.240 --> 01:51:48.440]   So I always like to recenter it on things exist, but they don't always exist like we
[01:51:48.440 --> 01:51:50.540]   think they exist.
[01:51:50.540 --> 01:51:54.260]   So when we're thinking about things outside the universe, they absolutely exist because
[01:51:54.260 --> 01:51:59.900]   we're thinking about them, but they don't look like the projections in our minds.
[01:51:59.900 --> 01:52:00.900]   They're something else.
[01:52:00.900 --> 01:52:02.180]   And something you said just gave me an idea.
[01:52:02.180 --> 01:52:04.820]   So I'll go back to your question.
[01:52:04.820 --> 01:52:10.740]   If there was caught, I mean, if something caused the Big Bang, if there was some memory
[01:52:10.740 --> 01:52:14.240]   or some artifact of that, then of course, to answer your question, it's worth going
[01:52:14.240 --> 01:52:19.780]   back to that because that would imply there is something beyond that barrier, that filter.
[01:52:19.780 --> 01:52:20.780]   And that's what you were saying, I guess.
[01:52:20.780 --> 01:52:21.780]   Right.
[01:52:21.780 --> 01:52:23.860]   I'm agnostic to what exists outside the universe.
[01:52:23.860 --> 01:52:24.860]   I just don't think that.
[01:52:24.860 --> 01:52:28.820]   Like I think the most interesting things for us to be doing are finding explanations that
[01:52:28.820 --> 01:52:32.760]   allow us to do more, like that optimism.
[01:52:32.760 --> 01:52:38.540]   So I tend to draw the boundary on questions I ask as being scientific ones because I find
[01:52:38.540 --> 01:52:42.900]   that that's where the most creative potential is to impact the future trajectory of what
[01:52:42.900 --> 01:52:43.900]   we're doing on this planet.
[01:52:43.900 --> 01:52:48.660]   It's interesting to think about the Big Bang is basically from our current perspective
[01:52:48.660 --> 01:52:52.820]   of what we're able to detect, it's the time when things were forgotten.
[01:52:52.820 --> 01:52:53.820]   Yes.
[01:52:53.820 --> 01:52:58.660]   It's the time to reset from our limited perspective.
[01:52:58.660 --> 01:53:03.980]   And so the question is, is it useful to ever study the thing that was forgotten?
[01:53:03.980 --> 01:53:08.460]   Or should we focus just on the memories that are still there?
[01:53:08.460 --> 01:53:11.920]   The point I was trying to make about the experiment is I was trying to say both things.
[01:53:11.920 --> 01:53:16.260]   And I think perhaps yes, from the following point of view, if you could then imagine what
[01:53:16.260 --> 01:53:20.980]   was forgotten and then work forwards, you will have different consequences.
[01:53:20.980 --> 01:53:22.780]   So then it becomes testable.
[01:53:22.780 --> 01:53:26.900]   So as long as we can find tests, and it's definitely worth thinking about, what I don't
[01:53:26.900 --> 01:53:31.300]   like is when physicists say what happened before the Big Bang and before, before, before,
[01:53:31.300 --> 01:53:38.380]   without giving me any credible conjecture about how would we know the difference.
[01:53:38.380 --> 01:53:39.700]   The way you framed it is quite nice.
[01:53:39.700 --> 01:53:40.700]   I like that.
[01:53:40.700 --> 01:53:43.660]   It's like, what have we forgotten?
[01:53:43.660 --> 01:53:47.980]   Is there a room for God in assembly theory?
[01:53:47.980 --> 01:53:49.580]   Who's God?
[01:53:49.580 --> 01:53:52.820]   I like arguments for a necessary being better than God.
[01:53:52.820 --> 01:53:53.820]   Well I think I said it earlier.
[01:53:53.820 --> 01:53:54.820]   What's a necessary being?
[01:53:54.820 --> 01:53:57.220]   Like something that has to exist.
[01:53:57.220 --> 01:54:01.220]   Oh, so you like, I mean, you like the shortest path.
[01:54:01.220 --> 01:54:02.220]   Like does God need?
[01:54:02.220 --> 01:54:03.220]   No, no, no.
[01:54:03.220 --> 01:54:08.340]   I mean, well you can go back to like Thomas Aquinas and arguments for the existence of
[01:54:08.340 --> 01:54:14.020]   God, but I think most of the interesting theological arguments are always about whether something
[01:54:14.020 --> 01:54:17.300]   has to exist or there was a first thing that had to exist.
[01:54:17.300 --> 01:54:20.300]   But I think there's a lot of logical loopholes in those kinds of arguments.
[01:54:20.300 --> 01:54:30.180]   So God here meaning the machine that creates, that generates the stuff.
[01:54:30.180 --> 01:54:31.620]   So what I was trying to say earlier is that-
[01:54:31.620 --> 01:54:32.620]   Isn't that just the universe?
[01:54:32.620 --> 01:54:33.620]   Yeah, yeah.
[01:54:33.620 --> 01:54:38.860]   Well, yeah, well, but there's a difference between, I imagine like a black box, like
[01:54:38.860 --> 01:54:39.860]   a machine.
[01:54:39.860 --> 01:54:40.860]   Yeah.
[01:54:40.860 --> 01:54:45.260]   That's, then I would be more comfortable calling that God, because it's a machine.
[01:54:45.260 --> 01:54:47.260]   You go into a room and there's a thing with a button.
[01:54:47.260 --> 01:54:50.020]   Yeah, I don't like the great programmer in the sky version.
[01:54:50.020 --> 01:54:56.780]   Yeah, but if it's more kind of, like I don't like to think of, if you look at a cellular
[01:54:56.780 --> 01:55:05.020]   automata, if it's the cells and the rules, that doesn't feel like God that generates
[01:55:05.020 --> 01:55:06.340]   a bunch of stuff.
[01:55:06.340 --> 01:55:14.260]   But if there's a machine that runs the cellular automata and set the rules, then that feels
[01:55:14.260 --> 01:55:15.260]   like God.
[01:55:15.260 --> 01:55:19.300]   That sort of, in terms of terminology.
[01:55:19.300 --> 01:55:23.420]   So I wonder if there's like a machine that's required to generate this universe.
[01:55:23.420 --> 01:55:26.940]   It's very sort of important for running this in the lab.
[01:55:26.940 --> 01:55:30.900]   So as I said earlier, I think I said this earlier, that I can't remember the phrase,
[01:55:30.900 --> 01:55:34.540]   but something like, I mean, does God exist in our universe?
[01:55:34.540 --> 01:55:35.540]   Yes.
[01:55:35.540 --> 01:55:36.540]   Where does God exist?
[01:55:36.540 --> 01:55:42.860]   God at least exists in the abstraction in our minds, particularly of people who have
[01:55:42.860 --> 01:55:45.100]   religious faith they believe in.
[01:55:45.100 --> 01:55:49.300]   But let's then take your, but you're talking a little bit more about generic, say, well,
[01:55:49.300 --> 01:55:51.900]   is there a mechanism beyond the universe you're calling God?
[01:55:51.900 --> 01:55:59.900]   I would say God did not exist at the beginning, but he or she does now.
[01:55:59.900 --> 01:56:00.900]   Because I'm saying the mechanism-
[01:56:00.900 --> 01:56:03.600]   Well, you don't know that he didn't exist in the beginning.
[01:56:03.600 --> 01:56:10.060]   So like this could be us in our minds trying to, like just what, listening to gravitational
[01:56:10.060 --> 01:56:12.220]   waves, detecting gravitational waves.
[01:56:12.220 --> 01:56:18.540]   It's the same thing as us trying to go back further and further into our memories to try
[01:56:18.540 --> 01:56:23.420]   to understand the machines that make up us.
[01:56:23.420 --> 01:56:30.260]   So it's possible that we're trying to grasp at possible kind of, what kind of machines
[01:56:30.260 --> 01:56:32.620]   could create.
[01:56:32.620 --> 01:56:34.180]   There's always a tweet.
[01:56:34.180 --> 01:56:35.860]   There's always a tweet.
[01:56:35.860 --> 01:56:41.260]   If the universe is a computer, then God must have built it because computers need creators.
[01:56:41.260 --> 01:56:43.380]   There you go.
[01:56:43.380 --> 01:56:49.660]   And then Joshe Bach replied, "Since there's something rather than nothing, perhaps existence
[01:56:49.660 --> 01:56:50.660]   is the default."
[01:56:50.660 --> 01:56:51.660]   Hmm.
[01:56:51.660 --> 01:56:56.860]   "If existence is the default, then many computers exist.
[01:56:56.860 --> 01:57:00.660]   Creator gods are necessary computers, unnecessarily computers too."
[01:57:00.660 --> 01:57:01.940]   I'm very confused by that.
[01:57:01.940 --> 01:57:05.780]   But that's an interesting idea that existence is the default versus non-existence.
[01:57:05.780 --> 01:57:07.660]   I agree with that, but the rest is-
[01:57:07.660 --> 01:57:12.860]   And then Lee responds, "Perhaps this reasoning is incomplete."
[01:57:12.860 --> 01:57:16.420]   That's how scientists talk trash to each other on Twitter, apparently.
[01:57:16.420 --> 01:57:18.820]   Which part don't you agree with?
[01:57:18.820 --> 01:57:22.580]   When he said, "If existence is the default, then many computers exist," this comes back
[01:57:22.580 --> 01:57:26.300]   to the inventor and discovery argument.
[01:57:26.300 --> 01:57:31.980]   I would say the universe at the beginning wasn't capable of computation because there
[01:57:31.980 --> 01:57:34.940]   wasn't enough technology, enough states.
[01:57:34.940 --> 01:57:38.660]   So what you're saying is if God is a mechanism...
[01:57:38.660 --> 01:57:44.580]   So I might actually agree, but then the thing is lots of people see God as more than a mechanism.
[01:57:44.580 --> 01:57:48.980]   For me, God could be the causal graph in assembly theory that creates all the stuff and the
[01:57:48.980 --> 01:57:49.980]   memories we know.
[01:57:49.980 --> 01:57:53.860]   And the fact that we can even relate to each other is because we have the same...
[01:57:53.860 --> 01:57:54.860]   We share that heritage.
[01:57:54.860 --> 01:58:02.140]   And why we love each other or we like to see God in each other is it's just...
[01:58:02.140 --> 01:58:04.860]   We know we have a shared existence.
[01:58:04.860 --> 01:58:10.180]   So if God is the mechanism that created this whole thing, I think a lot of people see God
[01:58:10.180 --> 01:58:16.620]   in a religious sense as that mechanism also being able to communicate with the objects
[01:58:16.620 --> 01:58:18.340]   it creates.
[01:58:18.340 --> 01:58:23.660]   And if it's just the mechanism, we won't be able to communicate with the objects it creates.
[01:58:23.660 --> 01:58:24.660]   It can only create.
[01:58:24.660 --> 01:58:28.420]   It can't interact with the...
[01:58:28.420 --> 01:58:32.260]   Well, there's versions of God that create the universe and then left.
[01:58:32.260 --> 01:58:33.260]   You know?
[01:58:33.260 --> 01:58:35.420]   Yeah, like spark.
[01:58:35.420 --> 01:58:36.420]   For some religions, but...
[01:58:36.420 --> 01:58:37.420]   The first spark, yeah.
[01:58:37.420 --> 01:58:42.420]   Yeah, but I think I liked your analogy of the machine and the rules, right?
[01:58:42.420 --> 01:58:49.900]   But I think part of the problem is, I mean, we have this conception that we can disentangle
[01:58:49.900 --> 01:58:51.740]   the rules from the physical substrate, right?
[01:58:51.740 --> 01:58:55.180]   And that's the whole thing about software and hardware being separate or the way Newton
[01:58:55.180 --> 01:58:57.380]   wrote his laws that there was some...
[01:58:57.380 --> 01:58:58.380]   They exist outside the universe.
[01:58:58.380 --> 01:58:59.900]   They're not actually a feature of the universe.
[01:58:59.900 --> 01:59:03.180]   They don't have to emerge out of the universe itself.
[01:59:03.180 --> 01:59:08.620]   So I think if you merge your two views, then it gets back to the God is the universe.
[01:59:08.620 --> 01:59:13.300]   And then I think the deeper question is, why does it seem like there's meaning and purpose?
[01:59:13.300 --> 01:59:18.020]   And if I think about the features of the universe that give it the most meaning and purpose,
[01:59:18.020 --> 01:59:21.140]   those are what we would call the living components of the universe.
[01:59:21.140 --> 01:59:25.380]   So if you wanted to say God is a physically real thing, which you were saying is like
[01:59:25.380 --> 01:59:30.540]   an emergent property of our minds, but I would just say, the way the universe creates meaning
[01:59:30.540 --> 01:59:32.900]   and purpose, there is really a physics there.
[01:59:32.900 --> 01:59:35.060]   It's not like a illusory thing.
[01:59:35.060 --> 01:59:39.220]   And that is just what the physics of life is.
[01:59:39.220 --> 01:59:44.620]   Is it possible that we've forgotten much of the mechanisms that created the universe?
[01:59:44.620 --> 01:59:45.620]   Mm-hmm.
[01:59:45.620 --> 01:59:46.620]   Yes.
[01:59:46.620 --> 01:59:52.300]   So basically, whatever God is that mechanism, we just leave parts of that behind.
[01:59:52.300 --> 01:59:54.780]   Well, but the universe is constantly generating itself.
[01:59:54.780 --> 01:59:59.340]   So if God is that mechanism, it would be that that would still be active today.
[01:59:59.340 --> 02:00:06.620]   I'm agnostic, but if I recall the things I believe in God in the way that some people
[02:00:06.620 --> 02:00:12.340]   talk about God, I would say that God is in the universe now.
[02:00:12.340 --> 02:00:16.380]   It's not an absent thing.
[02:00:16.380 --> 02:00:24.940]   So I think there's a mislabeling here because you're, I mean, I'm a professional idiot,
[02:00:24.940 --> 02:00:25.940]   actually.
[02:00:25.940 --> 02:00:26.940]   But-
[02:00:26.940 --> 02:00:29.940]   You should put that on your CV.
[02:00:29.940 --> 02:00:30.940]   Yeah.
[02:00:30.940 --> 02:00:31.940]   Professionally.
[02:00:31.940 --> 02:00:34.820]   Not recreationally or amateur, but professionally.
[02:00:34.820 --> 02:00:39.700]   I think I would say if you were talking about God, I mean, again, I'm way out, way out
[02:00:39.700 --> 02:00:43.420]   my depth here and I'm almost feeling, you know, I feel quite uncomfortable articulating,
[02:00:43.420 --> 02:00:44.420]   but I'll try.
[02:00:44.420 --> 02:00:49.180]   For me, a lot of people that think of God as a consciousness, a reasoning entity that
[02:00:49.180 --> 02:00:53.140]   actually has causal power and you're-
[02:00:53.140 --> 02:00:54.140]   Human like intelligence.
[02:00:54.140 --> 02:00:58.340]   And you're, and so you're like, then you're saying like gravity could be God or time could
[02:00:58.340 --> 02:00:59.340]   be God.
[02:00:59.340 --> 02:01:04.860]   I mean, I think for me, my conception of time is probably as fundamental as God because it
[02:01:04.860 --> 02:01:09.800]   gave rise to human intelligence and consciousness in which we can have this abstract notion
[02:01:09.800 --> 02:01:12.140]   of God.
[02:01:12.140 --> 02:01:18.460]   So I think that you're maybe talking about God in a very mechanistic kind of unsophisticated
[02:01:18.460 --> 02:01:22.380]   sense, whereas other people would say that God is more sophisticated and got all this,
[02:01:22.380 --> 02:01:27.180]   you know, feelings and love and, you know, and this abstracting ability.
[02:01:27.180 --> 02:01:29.500]   So is that what, or do you mean that?
[02:01:29.500 --> 02:01:36.260]   Do you mean God as in this conscious entity that decided to flick the universe into existence?
[02:01:36.260 --> 02:01:44.820]   Well, one of the features that God would have is the ability to flick the universe into
[02:01:44.820 --> 02:01:45.820]   existence.
[02:01:45.820 --> 02:01:52.620]   I, you know, like Windows 95, I don't know if God is Windows 95 or Windows XP or Windows
[02:01:52.620 --> 02:01:53.620]   10.
[02:01:53.620 --> 02:01:55.780]   I don't know the full feature set.
[02:01:55.780 --> 02:02:00.080]   So the very least you have to flick the universe into existence.
[02:02:00.080 --> 02:02:06.620]   And then other features might include ability to interact with that universe in interesting
[02:02:06.620 --> 02:02:07.820]   ways.
[02:02:07.820 --> 02:02:11.780]   And then how do you interact with the universe in interesting ways?
[02:02:11.780 --> 02:02:15.320]   You have to be able to speak the language of its different components.
[02:02:15.320 --> 02:02:24.980]   So in order to interact with humans, you have to know how to act human-like.
[02:02:24.980 --> 02:02:37.220]   So I don't know, but it seems like whatever mechanism created the universe might want
[02:02:37.220 --> 02:02:42.760]   to also generate local pockets of mechanisms that can interact with that.
[02:02:42.760 --> 02:02:43.760]   Like inject.
[02:02:43.760 --> 02:02:46.020]   Like God was lonely?
[02:02:46.020 --> 02:02:47.420]   Yeah, it was lonely.
[02:02:47.420 --> 02:02:51.740]   I mean, it could be just a teenager and another just playing a video game.
[02:02:51.740 --> 02:02:52.740]   Yeah, maybe.
[02:02:52.740 --> 02:02:57.340]   I mean, I don't, so this is referring to our origin of life engine.
[02:02:57.340 --> 02:03:00.860]   It's like, I don't believe in God, but that doesn't mean I don't want to be one.
[02:03:00.860 --> 02:03:01.860]   Right.
[02:03:01.860 --> 02:03:04.580]   Because I want to make a universe and make a life form.
[02:03:04.580 --> 02:03:08.700]   But that may be rude to people who have, you know, dear religious beliefs.
[02:03:08.700 --> 02:03:15.780]   What I mean by that is, if we are able to create an entirely new life form, different
[02:03:15.780 --> 02:03:21.100]   chemistry, different culture, what does it make up?
[02:03:21.100 --> 02:03:22.980]   By that definition, it makes us gods, right?
[02:03:22.980 --> 02:03:23.980]   Well, there is.
[02:03:23.980 --> 02:03:28.180]   I mean, like when you have children, you're like one of the magical things of that is
[02:03:28.180 --> 02:03:29.540]   you're kind of mini gods.
[02:03:29.540 --> 02:03:35.340]   I mean, first of all, from a child's perspective, parents are gods for quite a while.
[02:03:35.340 --> 02:03:39.540]   And then, I mean, in a positive sense, there's a magic to it.
[02:03:39.540 --> 02:03:43.640]   That's why I love robotics, is you instill life into something.
[02:03:43.640 --> 02:03:50.180]   And that makes you feel god-like in a sort of positive way.
[02:03:50.180 --> 02:03:52.140]   Being a creator is a positive feeling.
[02:03:52.140 --> 02:03:53.140]   Yeah, exactly.
[02:03:53.140 --> 02:03:54.140]   And a small scale.
[02:03:54.140 --> 02:03:59.900]   And then god would be a creator at the largest possible scale, I suppose.
[02:03:59.900 --> 02:04:00.900]   Okay.
[02:04:00.900 --> 02:04:03.140]   You mentioned offline the Assembletron.
[02:04:03.140 --> 02:04:04.140]   Assemblytron.
[02:04:04.140 --> 02:04:05.140]   Assemblytron.
[02:04:05.140 --> 02:04:06.140]   Yep.
[02:04:06.140 --> 02:04:07.140]   What's an Assemblytron?
[02:04:07.140 --> 02:04:12.660]   This is an early idea of something you're thinking about.
[02:04:12.660 --> 02:04:19.540]   So Sarah's team, well, I think Sarah's team are interested in using AI to understand life.
[02:04:19.540 --> 02:04:20.540]   My team is.
[02:04:20.540 --> 02:04:27.980]   And I'm wondering if we could apply the principles of assembly theory, that is, the causal structure
[02:04:27.980 --> 02:04:34.700]   that you get with assembly theory, and hybridize it and make a new type of neuron, if you like.
[02:04:34.700 --> 02:04:40.500]   I mean, there are causal neural networks out there, but they are not quite the architecture
[02:04:40.500 --> 02:04:41.860]   like what I would like.
[02:04:41.860 --> 02:04:48.180]   I would like to associate memory bits with, basically, I'd like to make a, rather than
[02:04:48.180 --> 02:04:53.900]   have an ASIC for neural networks, I want to make an ASIC for assembly networks.
[02:04:53.900 --> 02:04:57.540]   Can you say that again?
[02:04:57.540 --> 02:05:00.540]   Assembly networks.
[02:05:00.540 --> 02:05:07.620]   So what is a thing with an input and an output, and it's like a neural network type of thing,
[02:05:07.620 --> 02:05:08.620]   what does it do exactly?
[02:05:08.620 --> 02:05:09.620]   What's the input?
[02:05:09.620 --> 02:05:10.620]   What's the output?
[02:05:10.620 --> 02:05:13.980]   So in this case, if you're talking about a general neural network, I mean, in general
[02:05:13.980 --> 02:05:20.420]   neural network, you can train it on any sort of data, depending on the framework, whether
[02:05:20.420 --> 02:05:26.060]   it's like text or image data or whatnot.
[02:05:26.060 --> 02:05:31.420]   And that's fine, but there's no causal structure associated with that data.
[02:05:31.420 --> 02:05:35.380]   Now just imagine, rather than, let's say we're going to classify a difference between cat
[02:05:35.380 --> 02:05:38.220]   and dog, classic cat and dog neural network.
[02:05:38.220 --> 02:05:45.540]   What about if the system understood the assembly space that created the cat and the dog.
[02:05:45.540 --> 02:05:50.100]   And rather than guessing what was happening and training on those images and not understanding
[02:05:50.100 --> 02:05:59.420]   those features, you almost like, you could imagine going back a step and doing the training.
[02:05:59.420 --> 02:06:01.160]   Going back a step and doing the training.
[02:06:01.160 --> 02:06:03.420]   Going back a step, back a step, back a step.
[02:06:03.420 --> 02:06:09.100]   And I wonder if that is actually the origin of intelligence or how we'll crack intelligence.
[02:06:09.100 --> 02:06:16.820]   Because we need to, because we'll create the entire graph of events and be able to kind
[02:06:16.820 --> 02:06:19.100]   of look at cause and effect across those graphs.
[02:06:19.100 --> 02:06:24.700]   I'm explaining it really badly, but it's a gene of an idea and I'm guessing very smart,
[02:06:24.700 --> 02:06:29.700]   very rich people in AI are already doing this.
[02:06:29.700 --> 02:06:34.460]   Trying to not generate cats and dogs, but trying to generate things of high assembly
[02:06:34.460 --> 02:06:35.460]   index.
[02:06:35.460 --> 02:06:41.700]   - Yeah, and I think, and also using causal graphs in neural networks and machine learning
[02:06:41.700 --> 02:06:44.300]   and deep learning, maybe building a new architecture.
[02:06:44.300 --> 02:06:47.660]   I'm just wondering, is there something we can get out of assembly theory allows us to
[02:06:47.660 --> 02:06:54.860]   rebuild current machine learning architectures to give causation more cheaply.
[02:06:54.860 --> 02:06:58.740]   I mean, I don't know if that's what you, we've been inventing this for a little while, but
[02:06:58.740 --> 02:07:01.540]   we're trying to finish the theory paper first before we do anything else.
[02:07:01.540 --> 02:07:06.940]   - Yeah, you also want to have say goal-directed behavior in neural networks, then assembly
[02:07:06.940 --> 02:07:08.740]   theory is a good framework for doing that.
[02:07:08.740 --> 02:07:10.540]   Daniel's been thinking about that a lot.
[02:07:10.540 --> 02:07:15.540]   And I think it's a really interesting idea that you can map concepts from how neural
[02:07:15.540 --> 02:07:20.220]   networks learn to thinking about goal-directed behavior as a learning process, that you're
[02:07:20.220 --> 02:07:21.340]   learning a specific goal.
[02:07:21.340 --> 02:07:24.580]   The universe is learning a goal when it generates a particular structure and that you could
[02:07:24.580 --> 02:07:27.260]   map that physical structure in a neural network.
[02:07:27.260 --> 02:07:31.980]   - How, what's the goal?
[02:07:31.980 --> 02:07:37.300]   - Well in a neural network, you're designing the goal in biology.
[02:07:37.300 --> 02:07:44.060]   I mean, people are not supposed to use teleological language in biology, which is ridiculous,
[02:07:44.060 --> 02:07:45.220]   'cause goals are real things.
[02:07:45.220 --> 02:07:46.420]   They're just post-selected.
[02:07:46.420 --> 02:07:51.220]   So you can talk about goals after the fact.
[02:07:51.220 --> 02:07:55.260]   Once a goal emerges in the universe, that physical entity has a goal.
[02:07:55.260 --> 02:08:00.620]   So Lee and I came up with a test for, like a Turing test for goal-directed behavior based
[02:08:00.620 --> 02:08:01.620]   on the idea of assembly.
[02:08:01.620 --> 02:08:04.660]   Like we have to formalize this still, but I would like to write a paper on it.
[02:08:04.660 --> 02:08:12.820]   But like the basic idea is like if you had two systems that were completely equivalent,
[02:08:12.820 --> 02:08:16.820]   you know, like in the instantaneous like physical experimental setup, so Lee has to figure out
[02:08:16.820 --> 02:08:18.380]   how to do this.
[02:08:18.380 --> 02:08:21.940]   But there was something that would be different in their future.
[02:08:21.940 --> 02:08:25.260]   There was a symmetry breaking you observe in the present based on that possibility of
[02:08:25.260 --> 02:08:26.660]   that future outcome.
[02:08:26.660 --> 02:08:30.900]   Then you could say that that system had some representation of some kind of goal in mind
[02:08:30.900 --> 02:08:33.580]   about what it wanted to do in the future.
[02:08:33.580 --> 02:08:38.820]   And so goals are interesting because they don't exist as instantaneous things.
[02:08:38.820 --> 02:08:43.180]   They exist across time, which is one of the reasons that assembly theories may be more
[02:08:43.180 --> 02:08:48.260]   naturally able to account for the existence of goals.
[02:08:48.260 --> 02:08:57.220]   So goals are, they only exist in time or they manifest themselves in time through, you said
[02:08:57.220 --> 02:08:58.700]   symmetry breaking.
[02:08:58.700 --> 02:09:05.660]   So it's almost like, imagine like if representations in your mind are real, right?
[02:09:05.660 --> 02:09:09.900]   And you can imagine future possibilities, but imagine everything else is physically
[02:09:09.900 --> 02:09:14.860]   equivalent and the only thing that you actually change your decision based on is what you
[02:09:14.860 --> 02:09:17.540]   model as being the future outcome.
[02:09:17.540 --> 02:09:21.780]   Then somehow that representation in your mind of the future outcome becomes causal to what
[02:09:21.780 --> 02:09:22.780]   you're doing now.
[02:09:22.780 --> 02:09:26.020]   So it's kind of like retrocausal effect, but it's not actually retrocausal.
[02:09:26.020 --> 02:09:32.260]   It's just that your assembly space is actually includes those possibilities as part of the
[02:09:32.260 --> 02:09:33.260]   structure.
[02:09:33.260 --> 02:09:35.780]   It's just, you're not observing all the features of the assembly space in the current moment.
[02:09:35.780 --> 02:09:42.220]   Well, the possibilities exist, but they don't become a goal until they're realized.
[02:09:42.220 --> 02:09:46.740]   So one of the features of assembly space that's super interesting, and it's easier to envision
[02:09:46.740 --> 02:09:51.260]   with like Legos, for example, is if you're thinking about an assembly space, you can't
[02:09:51.260 --> 02:09:54.100]   observe the entire assembly space in any instant in time.
[02:09:54.100 --> 02:09:57.700]   So if you imagine a stack of Legos and you want to look at the assembly space of a stack
[02:09:57.700 --> 02:10:03.140]   of Legos, you have to break the Legos apart and then you look at all the possible ways
[02:10:03.140 --> 02:10:05.660]   of building up the original object.
[02:10:05.660 --> 02:10:09.380]   So now you have in your mind the goal of building that object and you have all the possible
[02:10:09.380 --> 02:10:10.880]   ways of doing it.
[02:10:10.880 --> 02:10:14.700]   And those are actual physical features of that object, but that object doesn't always
[02:10:14.700 --> 02:10:15.700]   exist.
[02:10:15.700 --> 02:10:17.740]   You don't have the possibility of generating it.
[02:10:17.740 --> 02:10:20.580]   And the possibilities are always infinite.
[02:10:20.580 --> 02:10:25.540]   Well for that particular object, it has a well-defined assembly space.
[02:10:25.540 --> 02:10:29.700]   And I guess what I'm saying is that object is the assembly space, but you actually have
[02:10:29.700 --> 02:10:33.420]   to unpack that object across time to view that feature of it.
[02:10:33.420 --> 02:10:35.260]   It's only an observable across time.
[02:10:35.260 --> 02:10:42.140]   The term goal is such an important and difficult to explain concept, right?
[02:10:42.140 --> 02:10:47.060]   Because what you want is a way, it's like, I think only conscious beings can have conscious
[02:10:47.060 --> 02:10:49.620]   goals.
[02:10:49.620 --> 02:10:51.900]   Everything else is doing selection.
[02:10:51.900 --> 02:10:57.500]   But selection does invent goals and in a way that the way that biology reinterprets the
[02:10:57.500 --> 02:11:05.140]   past in the present is kind of helps you to understand there was a goal in the past now,
[02:11:05.140 --> 02:11:06.140]   right?
[02:11:06.140 --> 02:11:09.440]   It's kind of like goals only exist back in time.
[02:11:09.440 --> 02:11:17.220]   So first of all, only conscious beings can have conscious goals.
[02:11:17.220 --> 02:11:19.420]   I'm not even going to touch that one.
[02:11:19.420 --> 02:11:20.420]   Why?
[02:11:20.420 --> 02:11:23.420]   Go for it, come on.
[02:11:23.420 --> 02:11:29.220]   The line between conscious goals and non-conscious goals, exactly.
[02:11:29.220 --> 02:11:36.740]   And also maybe just on top of that, you said a Turing test for goal-directed behavior.
[02:11:36.740 --> 02:11:39.180]   What does a Turing test potentially look like?
[02:11:39.180 --> 02:11:40.940]   So if you've got two objects, we were thinking about this.
[02:11:40.940 --> 02:11:43.900]   So we actually got some funding to work together on two teams.
[02:11:43.900 --> 02:11:48.460]   So I'm trying to do, and part of this is I'm trying to do a bit of theory and Sarah's teaching
[02:11:48.460 --> 02:11:52.140]   me a bit of theory and Sarah's trying to design experiments and I'm teaching experiments.
[02:11:52.140 --> 02:11:58.500]   I think it's really good for us to have that to say, when would a, so that's good.
[02:11:58.500 --> 02:12:02.580]   I like this, I'm sure we're using Dan Dennett essay.
[02:12:02.580 --> 02:12:05.180]   And I can explain why we wouldn't want to call it a Turing test after.
[02:12:05.180 --> 02:12:11.620]   So Dan Dennett wrote this really nice essay about herding cats and free will inflation.
[02:12:11.620 --> 02:12:13.620]   I love the title, the title is so brilliant.
[02:12:13.620 --> 02:12:14.620]   I think that's the actual title.
[02:12:14.620 --> 02:12:15.620]   That's the title, yeah.
[02:12:15.620 --> 02:12:17.620]   Herding cats and free will inflation.
[02:12:17.620 --> 02:12:18.940]   Yeah, something like that.
[02:12:18.940 --> 02:12:20.420]   I mean, it's not, maybe not.
[02:12:20.420 --> 02:12:26.140]   And so if you've got a, let's imagine you've got two objects on a hillside, okay.
[02:12:26.140 --> 02:12:28.260]   And this happens to be a snowy hill.
[02:12:28.260 --> 02:12:34.060]   And let's just say you see an object go rolling down the hill, or you, and it rolls down the
[02:12:34.060 --> 02:12:36.780]   hill, but the start goes to the end.
[02:12:36.780 --> 02:12:38.020]   How do you know that object's had a goal?
[02:12:38.020 --> 02:12:42.660]   Now you unveil the object and you'll see it's actually a skier and the skier starts at the
[02:12:42.660 --> 02:12:44.620]   top and goes down the bottom.
[02:12:44.620 --> 02:12:45.740]   Great.
[02:12:45.740 --> 02:12:48.820]   Then you look at the rock, rock rolls down the hill and goes to the bottom.
[02:12:48.820 --> 02:12:51.300]   How can you tell the difference between the two?
[02:12:51.300 --> 02:12:57.300]   So, and what Dan says is like, well, this is clear the skier's in control and the, because
[02:12:57.300 --> 02:12:58.700]   they're adjusting the trajectory.
[02:12:58.700 --> 02:13:00.300]   So there's some updating going on.
[02:13:00.300 --> 02:13:03.340]   Then the only way you could really do that is you put the skier back to the top of the
[02:13:03.340 --> 02:13:07.780]   hill again, they would tend to start roughly in the same space and probably go take all
[02:13:07.780 --> 02:13:12.060]   that complex set of trajectories and end up pretty much at the same finish point, right?
[02:13:12.060 --> 02:13:13.620]   With plus or minus a few metres.
[02:13:13.620 --> 02:13:18.260]   Whereas if it was just a random rock going down to random trajectory, that wouldn't happen.
[02:13:18.260 --> 02:13:22.420]   And so what Sarah and I were kind of doing when we were writing this grant, we were like,
[02:13:22.420 --> 02:13:29.260]   we need to somehow instantiate the skier and the rock in an experiment and then say, okay,
[02:13:29.260 --> 02:13:33.180]   when does the object, when it, so for an object to have a goal,
[02:13:33.180 --> 02:13:37.580]   it has to have an update, it has to have some sensing and some kind of, you know, inbuilt
[02:13:37.580 --> 02:13:42.820]   actuation to respond to the environment.
[02:13:42.820 --> 02:13:44.340]   And then we just have to iterate on that.
[02:13:44.340 --> 02:13:46.980]   And maybe Sarah, you can then fill in the Turing test part.
[02:13:46.980 --> 02:13:50.060]   Well, yeah, I guess the motivation for me was slightly different.
[02:13:50.060 --> 02:13:54.660]   So I get really frustrated about conversations about consciousness as most people do.
[02:13:54.660 --> 02:13:58.900]   You know, a lot of people are, which is not necessarily related to free will directly
[02:13:58.900 --> 02:14:03.460]   or to this goal-directed behaviour, but I think there's a whole set of bundled and related
[02:14:03.460 --> 02:14:04.460]   topics here.
[02:14:04.460 --> 02:14:09.740]   But I think for me, I was, you know, everybody's always interested in explaining intrinsic
[02:14:09.740 --> 02:14:12.460]   experience and quantifying intrinsic experience.
[02:14:12.460 --> 02:14:16.580]   And there's all sorts of problems with that because you can never actually be another
[02:14:16.580 --> 02:14:17.580]   physical system.
[02:14:17.580 --> 02:14:20.520]   So you can't know what it's like to be another physical system.
[02:14:20.520 --> 02:14:24.740]   So I always thought there must be some way of getting at this problem about if an agent
[02:14:24.740 --> 02:14:29.420]   or an entity is conscious or at least has internal representations, and those are real
[02:14:29.420 --> 02:14:34.780]   physical things, that it must have causal consequences.
[02:14:34.780 --> 02:14:40.900]   So the way I would ask the question of consciousness is not, you know, what is it like intrinsically,
[02:14:40.900 --> 02:14:45.620]   but if things have intrinsic experience, is there any observable difference from the outside
[02:14:45.620 --> 02:14:50.620]   about the kind of causation that that physical system would enact in?
[02:14:50.620 --> 02:14:53.900]   And for me, the most interesting thing that humans do is have imagination.
[02:14:53.900 --> 02:14:57.500]   So like we can imagine rockets centuries before we build them.
[02:14:57.500 --> 02:15:00.780]   They've become real physical things because we imagine them.
[02:15:00.780 --> 02:15:03.780]   And people might disentangle that from conscious experience, but I think a lot of the sort
[02:15:03.780 --> 02:15:07.060]   of imagination we do is actually a conscious process.
[02:15:07.060 --> 02:15:13.700]   So then this becomes a question of if I were observing systems and I said one had an internal
[02:15:13.700 --> 02:15:17.900]   representation, which is slightly different than a conscious experience, obviously, so
[02:15:17.900 --> 02:15:23.500]   I'm entangling some concepts, but it's a loose set of thought experiments.
[02:15:23.500 --> 02:15:28.580]   Can I set them up in a physically equivalent situation?
[02:15:28.580 --> 02:15:34.860]   Would it be the case that there would be experimental observables associated with it?
[02:15:34.860 --> 02:15:40.140]   And that became the idea of trying to actually measure for internal representation or conscious.
[02:15:40.140 --> 02:15:42.460]   So Turing basically didn't want to do that.
[02:15:42.460 --> 02:15:46.140]   He just wanted a machine that could emulate and trick you into having the behavior, but
[02:15:46.140 --> 02:15:51.260]   never dealt with the internal experience because he didn't know how to do that.
[02:15:51.260 --> 02:15:56.100]   And I guess I was wondering, is there a way to set up the experiment where you could actually
[02:15:56.100 --> 02:15:57.820]   test for that?
[02:15:57.820 --> 02:16:00.980]   For imagination that led to the-
[02:16:00.980 --> 02:16:06.620]   That there was something internal going on, some kind of inner world as people say, or
[02:16:06.620 --> 02:16:10.100]   you could say, it actually is an agent.
[02:16:10.100 --> 02:16:11.500]   It's making decisions.
[02:16:11.500 --> 02:16:14.020]   It has an internal representation.
[02:16:14.020 --> 02:16:18.060]   And whether you say that's experience or not is a different thing, but at least the feature
[02:16:18.060 --> 02:16:23.260]   that there's some abstraction it's doing that's not obvious from looking at the physical substrate.
[02:16:23.260 --> 02:16:25.340]   Do you think it's possible to do that kind of thing?
[02:16:25.340 --> 02:16:30.740]   One of the compelling things about the Turing test is that defining intelligent, defining
[02:16:30.740 --> 02:16:38.940]   any complicated concept as a thing, like observing it from the surface and not caring about what's
[02:16:38.940 --> 02:16:42.820]   going on deep inside because how do you know?
[02:16:42.820 --> 02:16:43.820]   That's the point.
[02:16:43.820 --> 02:16:45.060]   So the idea is exactly that.
[02:16:45.060 --> 02:16:49.980]   So what we're trying to do with the Turing test for goal-directedness is literally take
[02:16:49.980 --> 02:16:53.900]   some objects that clearly don't have any internal representation, grains of sand blowing on
[02:16:53.900 --> 02:16:56.300]   the beach or something, right?
[02:16:56.300 --> 02:17:00.700]   And I don't know, a crab wandering around on the beach and then generating an experiment
[02:17:00.700 --> 02:17:06.420]   where we literally, the experiment generates an entity that literally has no internal representation
[02:17:06.420 --> 02:17:07.940]   to sand.
[02:17:07.940 --> 02:17:11.740]   These are oil droplets actually, what I've got in mind, a robot that makes oil droplets.
[02:17:11.740 --> 02:17:16.340]   But then what we want to try and do is train the oil droplets to be like crabs, give them
[02:17:16.340 --> 02:17:21.900]   an internal representation, give them the ability to integrate information from the
[02:17:21.900 --> 02:17:22.900]   environment.
[02:17:22.900 --> 02:17:31.140]   So they remember the past, are in the present and can imagine a future.
[02:17:31.140 --> 02:17:36.220]   And in a very limited way, their kind of game engine, their limited simulation of the world
[02:17:36.220 --> 02:17:38.340]   allows them to then make a decision.
[02:17:38.340 --> 02:17:40.420]   They're objects across time.
[02:17:40.420 --> 02:17:44.700]   So then you would run a bunch of crabs, like over and over and over and over?
[02:17:44.700 --> 02:17:46.460]   How many crabs, Lee?
[02:17:46.460 --> 02:17:51.180]   Is there, what's, because you have to have a large number of crabs, what does your theory
[02:17:51.180 --> 02:17:52.180]   say?
[02:17:52.180 --> 02:17:53.180]   Is there a mathematical?
[02:17:53.180 --> 02:17:54.180]   We're working on it.
[02:17:54.180 --> 02:17:55.180]   I mean, this is literally-
[02:17:55.180 --> 02:17:56.180]   Limit, crab limit.
[02:17:56.180 --> 02:17:57.180]   There's literally a-
[02:17:57.180 --> 02:17:58.180]   Excellent.
[02:17:58.180 --> 02:17:59.180]   There's literally a-
[02:17:59.180 --> 02:18:00.180]   What's the herding cats have to do?
[02:18:00.180 --> 02:18:01.180]   Oh, that's random.
[02:18:01.180 --> 02:18:06.900]   Wait, what's cats in the title by Daniel Dennett, Herding Cats and the Free Will Inflation?
[02:18:06.900 --> 02:18:07.900]   So I-
[02:18:07.900 --> 02:18:09.060]   What does herding cats mean?
[02:18:09.060 --> 02:18:10.940]   What does free will inflation mean?
[02:18:10.940 --> 02:18:18.900]   So this, I love this essay because it explained to me how I can live in a deterministic universe,
[02:18:18.900 --> 02:18:24.700]   but have not free will, but have freedom, you know?
[02:18:24.700 --> 02:18:31.020]   And also it helped me explain that time needed to be a real thing in this universe.
[02:18:31.020 --> 02:18:36.060]   So what basically Dan was saying here is like, how do you, how do these cats appear to just
[02:18:36.060 --> 02:18:37.740]   do what they want?
[02:18:37.740 --> 02:18:38.740]   Right?
[02:18:38.740 --> 02:18:43.300]   And if you live in a deterministic universe, why do the cats do these things?
[02:18:43.300 --> 02:18:46.420]   You know, aren't they just, isn't it all obvious?
[02:18:46.420 --> 02:18:48.780]   And how does free will inflate the universe?
[02:18:48.780 --> 02:18:53.860]   And for me, I mean, probably I love the essay because my interpretation of the essay in
[02:18:53.860 --> 02:18:57.900]   assembly theory makes complete sense.
[02:18:57.900 --> 02:19:05.060]   Because you need an expanding universe in assembly theory to create novelty that you
[02:19:05.060 --> 02:19:09.940]   search for that then when you find something interesting and you keep doing it because
[02:19:09.940 --> 02:19:15.540]   it's cool and it gives you an advantage, then it appears in the past to be a goal.
[02:19:15.540 --> 02:19:21.020]   So what does in assembly theory, the expansion of the universe look like?
[02:19:21.020 --> 02:19:24.460]   What are we talking about?
[02:19:24.460 --> 02:19:29.380]   Why does the expansion of the universe give you more possibilities of novelty and cool
[02:19:29.380 --> 02:19:30.500]   stuff?
[02:19:30.500 --> 02:19:34.060]   So for me, I don't think about the universe in terms of Big Bang and space.
[02:19:34.060 --> 02:19:37.900]   I think about in terms of the big memory expansion.
[02:19:37.900 --> 02:19:42.220]   You only have the ability to store one bit of information, so then you can't do very
[02:19:42.220 --> 02:19:43.220]   much.
[02:19:43.220 --> 02:19:50.300]   So what the universe has been doing since forever, it's been increasing the size of
[02:19:50.300 --> 02:19:51.300]   its RAM.
[02:19:51.300 --> 02:19:52.300]   Okay?
[02:19:52.300 --> 02:19:57.660]   So it's like one megabyte, two megabyte, three megabyte, four megabytes, all the way up.
[02:19:57.660 --> 02:20:04.940]   And so the more RAM you have, the more you can remember about the past, which allows
[02:20:04.940 --> 02:20:07.220]   you to do cooler things in the future.
[02:20:07.220 --> 02:20:11.540]   So if you can remember how to launch a rocket, then you might be able to imagine how to land
[02:20:11.540 --> 02:20:15.100]   a rocket and then relaunch, re-land and carry on.
[02:20:15.100 --> 02:20:21.140]   And so you're able to expand the space and remember the past.
[02:20:21.140 --> 02:20:23.740]   And so that's why I think it's very important.
[02:20:23.740 --> 02:20:26.140]   But not a perfect memory.
[02:20:26.140 --> 02:20:28.900]   - That's an interesting question, whether there's some forgetting that happens that
[02:20:28.900 --> 02:20:30.580]   might increase.
[02:20:30.580 --> 02:20:37.100]   Is the expansion of the forgetting at some point accelerate faster than the remembering?
[02:20:37.100 --> 02:20:40.980]   - I think that that's a very important thing that probably intelligence does, and we're
[02:20:40.980 --> 02:20:42.420]   gonna learn in machine learning about.
[02:20:42.420 --> 02:20:45.980]   Because you want machine learning right now, or artificial intelligence right now, doesn't
[02:20:45.980 --> 02:20:51.620]   have memory right, but you want the ability to, or not for, if you want to get to human-like
[02:20:51.620 --> 02:20:56.380]   consciousness, you need to have the ability I suppose to remember stuff and then to selectively
[02:20:56.380 --> 02:20:59.980]   forget stuff so you can re-remember it and compress it.
[02:20:59.980 --> 02:21:02.340]   Arguably the way that we come up with new physical laws.
[02:21:02.340 --> 02:21:03.340]   - Cryptocurrency.
[02:21:03.340 --> 02:21:05.460]   - Yeah, sorry, you were confused.
[02:21:05.460 --> 02:21:07.500]   - No, no, it's all right, no, I just wanted to.
[02:21:07.500 --> 02:21:15.020]   - I think that there is a great deal to be gained from having the ability to remember
[02:21:15.020 --> 02:21:19.660]   things, but then when you forget them, you can then have a, you can basically do the
[02:21:19.660 --> 02:21:24.020]   simulation again and work out if you get to that compressed representation.
[02:21:24.020 --> 02:21:25.420]   So that it's in cycles.
[02:21:25.420 --> 02:21:31.540]   So cycles of remembering and forgetting are probably important, but there shouldn't be
[02:21:31.540 --> 02:21:34.300]   an excuse to have a universe with no memory in it.
[02:21:34.300 --> 02:21:40.260]   The universe is gonna remember that it forgot, but just not tell you.
[02:21:40.260 --> 02:21:45.140]   - I'm looking at this paper and it's talking about a puppet controlling a puppet controlling
[02:21:45.140 --> 02:21:48.700]   a puppet controlling a puppet controlling a puppet controlling a puppet, conceptually
[02:21:48.700 --> 02:21:52.580]   easy to understand, but physically impossible, it's physically impossible, it's predicting
[02:21:52.580 --> 02:21:53.580]   a fair coin toss.
[02:21:53.580 --> 02:21:58.700]   I don't know what he's talking about, but there's pictures of puppets controlling puppets.
[02:21:58.700 --> 02:22:04.820]   Let me ask you, there's a few things I wanna ask, but we brought up time quite a bit.
[02:22:04.820 --> 02:22:09.220]   You guys tweet about time quite a bit.
[02:22:09.220 --> 02:22:10.780]   What is time in all of this?
[02:22:10.780 --> 02:22:13.500]   We kind of mentioned it a bunch.
[02:22:13.500 --> 02:22:17.020]   Is it not important at all in terms of, is it just a word?
[02:22:17.020 --> 02:22:18.900]   Should we be talking about causality mostly?
[02:22:18.900 --> 02:22:21.580]   Sarah, what do you think?
[02:22:21.580 --> 02:22:24.220]   We've talked about memories.
[02:22:24.220 --> 02:22:28.380]   Is that the fundamental thing that we should be thinking about and time is just a useful
[02:22:28.380 --> 02:22:30.700]   measurement device or something like that?
[02:22:30.700 --> 02:22:32.460]   - Well there's different concepts of time.
[02:22:32.460 --> 02:22:34.900]   So I think in assembly theory, when we're talking about time, we're talking about the
[02:22:34.900 --> 02:22:36.300]   ordering of things.
[02:22:36.300 --> 02:22:38.420]   So that's the causal graph part.
[02:22:38.420 --> 02:22:42.740]   And so then the fundamental structure of the universe is that there is a certain ordering
[02:22:42.740 --> 02:22:44.780]   and certain things can't happen until other things happen.
[02:22:44.780 --> 02:22:51.700]   But usually when we colloquially talk about time, we're talking about the flow of time.
[02:22:51.700 --> 02:22:54.060]   And I guess Lee and I were actually debating about this this morning.
[02:22:54.060 --> 02:22:58.340]   So in talking on it, walking on the river here, which is a very lovely spot for talking
[02:22:58.340 --> 02:23:05.100]   about time, but that when the universe is updating, it's transitioning between things
[02:23:05.100 --> 02:23:09.500]   that exist now and things that exist now.
[02:23:09.500 --> 02:23:11.780]   That's really the flow of time.
[02:23:11.780 --> 02:23:15.780]   So you have to separate out those concepts at bare minimum.
[02:23:15.780 --> 02:23:20.060]   And then there's also an arrow of time that people talk about in physics, which is that
[02:23:20.060 --> 02:23:25.300]   time doesn't appear to have a directionality in fundamental physics, but it does to us.
[02:23:25.300 --> 02:23:26.980]   We can't go backwards in time.
[02:23:26.980 --> 02:23:31.580]   And usually that would be explained in physics in terms of, well, there's a cosmological
[02:23:31.580 --> 02:23:36.220]   arrow of time, but there's also the thermodynamic arrow of time of increasing entropy.
[02:23:36.220 --> 02:23:39.420]   But what we would say in assembly theory is that there is a clear directionality, the
[02:23:39.420 --> 02:23:42.220]   universe only runs in one direction, which is why some things...
[02:23:42.220 --> 02:23:43.220]   It's easy to make...
[02:23:43.220 --> 02:23:47.780]   If the universe runs in one direction, it's easy to make processes look reversible.
[02:23:47.780 --> 02:23:51.260]   For example, if they have no memory, they're easy to run forward and backwards, which is
[02:23:51.260 --> 02:23:55.620]   why the laws of physics that we have now look the way they do, because they involve objects
[02:23:55.620 --> 02:23:56.720]   that have no memory.
[02:23:56.720 --> 02:24:00.500]   But when you get to things like us, it becomes very clear that the universe has a directionality
[02:24:00.500 --> 02:24:01.500]   associated to it.
[02:24:01.500 --> 02:24:04.700]   So it's not reversible at all.
[02:24:04.700 --> 02:24:06.380]   No man ever steps in the same river.
[02:24:06.380 --> 02:24:08.500]   I just have to bring that up because you're walking on the river.
[02:24:08.500 --> 02:24:12.900]   No man ever steps in the same river twice, for it's not the same river and he's not the
[02:24:12.900 --> 02:24:13.900]   same man.
[02:24:13.900 --> 02:24:15.900]   So it's not reversible.
[02:24:15.900 --> 02:24:19.580]   No, but reversibility is an emergent property.
[02:24:19.580 --> 02:24:23.380]   So we think of the reversibility laws as being fundamental and the irreversibility as being
[02:24:23.380 --> 02:24:24.380]   emergent.
[02:24:24.380 --> 02:24:28.060]   But I think what we would say from how we think about it, and certainly it's easy to
[02:24:28.060 --> 02:24:33.540]   give the case for our perception of time, but also what's happening in biological evolution,
[02:24:33.540 --> 02:24:37.380]   you can make things reversible, but it requires work to do it.
[02:24:37.380 --> 02:24:40.780]   And it requires certain machines to run it forward and backward.
[02:24:40.780 --> 02:24:44.380]   And Chiara Marletto is working on some interesting ideas on constructor theory related to that,
[02:24:44.380 --> 02:24:46.420]   which is a totally different set of ideas.
[02:24:46.420 --> 02:24:49.540]   You can travel back in time sometimes.
[02:24:49.540 --> 02:24:50.540]   Yes.
[02:24:50.540 --> 02:24:57.060]   You can't travel actually back in time, but you could reconstruct things that have existed
[02:24:57.060 --> 02:24:58.100]   in the past.
[02:24:58.100 --> 02:25:00.900]   You're always moving forward in time, but you can cycle through.
[02:25:00.900 --> 02:25:03.700]   Can I clarify what you just said?
[02:25:03.700 --> 02:25:04.700]   Yeah, go for it.
[02:25:04.700 --> 02:25:06.460]   Quickly, you travel forward in time to travel back.
[02:25:06.460 --> 02:25:07.460]   Yes.
[02:25:07.460 --> 02:25:08.460]   Thank you.
[02:25:08.460 --> 02:25:09.460]   That really clarified it.
[02:25:09.460 --> 02:25:13.900]   What Sarah is saying is you don't go back in time, you recreate what happened in the
[02:25:13.900 --> 02:25:16.020]   past in the future and inspect it again.
[02:25:16.020 --> 02:25:19.260]   So in that local pocket of time, it's as if you travel back in time.
[02:25:19.260 --> 02:25:22.460]   So how's that not traveling back in time?
[02:25:22.460 --> 02:25:25.540]   Because you're not going back to your same self back in time.
[02:25:25.540 --> 02:25:27.180]   You're creating that in the future.
[02:25:27.180 --> 02:25:30.180]   But everything else is the same as it was in the past.
[02:25:30.180 --> 02:25:31.620]   No, no, no, no.
[02:25:31.620 --> 02:25:32.620]   It's not in registry.
[02:25:32.620 --> 02:25:34.140]   I mean, it goes back to the big question.
[02:25:34.140 --> 02:25:39.740]   I'm saying, I mean, this is something I was trying to look up today when we first had
[02:25:39.740 --> 02:25:44.460]   this discussion and I was talking to Sarah on Skype and said, "By the way, time is the
[02:25:44.460 --> 02:25:45.940]   fundamental thing in the universe."
[02:25:45.940 --> 02:25:47.460]   She almost hung up on me.
[02:25:47.460 --> 02:25:48.460]   Right.
[02:25:48.460 --> 02:25:51.620]   But you can even, I mean, if you want to make an analogy to computation, and I think Charles
[02:25:51.620 --> 02:25:56.180]   Bennett actually has a paper on this about reversible computation and reversible Turing
[02:25:56.180 --> 02:25:57.180]   machines.
[02:25:57.180 --> 02:26:01.420]   In order to make it reversible, you have to store memory to run the process backwards.
[02:26:01.420 --> 02:26:04.220]   So time is always running forward in that.
[02:26:04.220 --> 02:26:05.540]   Because you have to write the memory.
[02:26:05.540 --> 02:26:06.780]   You can't erase the memory.
[02:26:06.780 --> 02:26:11.500]   You can erase the memory, but the point, when you go back to zero, right?
[02:26:11.500 --> 02:26:17.500]   But the whole point is that in order to have a process that even runs in both directions,
[02:26:17.500 --> 02:26:21.740]   you have to start talking about memory to store the information to run it backwards.
[02:26:21.740 --> 02:26:22.740]   I got it.
[02:26:22.740 --> 02:26:27.500]   So you can't really then, you can't have it exactly how it was in the past.
[02:26:27.500 --> 02:26:28.500]   Exactly.
[02:26:28.500 --> 02:26:30.820]   You have extra stuff, extra baggage always.
[02:26:30.820 --> 02:26:34.460]   A really important thing that I want to say on this, I think if I try and get it right,
[02:26:34.460 --> 02:26:41.300]   is to say that if you can think that the universe is expanding in terms of the number of boxes
[02:26:41.300 --> 02:26:45.420]   that it has to store states, right?
[02:26:45.420 --> 02:26:49.020]   And this is where the directionality of the universe comes from, everything comes from.
[02:26:49.020 --> 02:26:53.940]   You could erase what's in those boxes, but the fact you've now got so many boxes at time,
[02:26:53.940 --> 02:26:57.620]   now in this present, there's more of those boxes than there were in the past.
[02:26:57.620 --> 02:27:01.620]   See, but the boxes aren't physical boxes.
[02:27:01.620 --> 02:27:03.500]   It's not space or time.
[02:27:03.500 --> 02:27:05.860]   Why is the number of boxes always expanding?
[02:27:05.860 --> 02:27:09.700]   It's very hard to imagine this because we live in space.
[02:27:09.700 --> 02:27:16.300]   So what I'm saying, which is I think probably correct, is that we just, let's just imagine
[02:27:16.300 --> 02:27:23.940]   for a second, there is a non-local situation, but there are these things called states and
[02:27:23.940 --> 02:27:30.580]   that the universe, irrespective of whether you measure anything, there is a universal,
[02:27:30.580 --> 02:27:33.660]   let's call it a clock or a state creator.
[02:27:33.660 --> 02:27:38.180]   Maybe we can call it, maybe you can call it God, but let's call it a state creator where
[02:27:38.180 --> 02:27:41.700]   the universe is expanding in the number of states it has.
[02:27:41.700 --> 02:27:43.300]   - Why are you saying it's expanding though?
[02:27:43.300 --> 02:27:44.740]   Is that obvious that it's expanding?
[02:27:44.740 --> 02:27:47.980]   - It's obvious because that's where the, because we-
[02:27:47.980 --> 02:27:50.440]   - That's a source of novelty.
[02:27:50.440 --> 02:27:54.820]   - It's a source of novelty and it also explains why the universe is not predictable.
[02:27:54.820 --> 02:27:57.620]   - How do you know it's not predictable?
[02:27:57.620 --> 02:27:59.020]   I just like interrupting you.
[02:27:59.020 --> 02:28:00.020]   Sorry, it's fun.
[02:28:00.020 --> 02:28:01.420]   Because you're struggling.
[02:28:01.420 --> 02:28:05.420]   - I'm struggling because I'm trying to be as concrete as possible and not sound like
[02:28:05.420 --> 02:28:07.740]   I'm insane.
[02:28:07.740 --> 02:28:09.100]   And I'm not insane.
[02:28:09.100 --> 02:28:14.180]   It's obvious because you, I'm a chemist.
[02:28:14.180 --> 02:28:20.140]   So as a chemist, I grew into the world understanding irreversibility.
[02:28:20.140 --> 02:28:22.460]   Irreversibility is all I knew.
[02:28:22.460 --> 02:28:27.360]   And when people start telling me the universe is actually reversible, it's a magic trick.
[02:28:27.360 --> 02:28:28.620]   We can use time to do it.
[02:28:28.620 --> 02:28:38.740]   So what I mean is the second law is really the magical.
[02:28:38.740 --> 02:28:40.020]   But why does it need to be magical?
[02:28:40.020 --> 02:28:41.580]   The universe is just asymmetric.
[02:28:41.580 --> 02:28:47.900]   All I'm saying is the universe is asymmetric in the state production and we can erase those
[02:28:47.900 --> 02:28:50.540]   states, but we just have more computational power.
[02:28:50.540 --> 02:28:56.100]   So what I'm saying is that the universe's deterministic horizon, this is one of the
[02:28:56.100 --> 02:28:59.500]   reasons we can't live in a simulation, by the way.
[02:28:59.500 --> 02:29:01.100]   You can't live in a simulation.
[02:29:01.100 --> 02:29:02.100]   - The irreversibility?
[02:29:02.100 --> 02:29:06.780]   - Yeah, so basically every time you try and simulate the universe, you know, and live
[02:29:06.780 --> 02:29:09.020]   in a simulation, the universe is expanded in states.
[02:29:09.020 --> 02:29:10.020]   You're like, "Oh, damn it.
[02:29:10.020 --> 02:29:11.540]   I need to make my computer bigger again."
[02:29:11.540 --> 02:29:15.260]   And every time you try and contain the universe in the computation, because it's got bigger
[02:29:15.260 --> 02:29:16.820]   in number of states.
[02:29:16.820 --> 02:29:23.620]   And so I'm saying the fact the universe has novelty in it is going to turn out experimentally
[02:29:23.620 --> 02:29:32.640]   to be proof that time, as I've labeled it, is fundamental and exists as a physical thing
[02:29:32.640 --> 02:29:34.380]   that creates space.
[02:29:34.380 --> 02:29:40.340]   - Okay, so if you can prove that novelty is always being created, you're saying that it's
[02:29:40.340 --> 02:29:45.620]   possible to also then prove that it's always expanding in the state space.
[02:29:45.620 --> 02:29:48.260]   Those are things that have to be proven.
[02:29:48.260 --> 02:29:50.140]   - That's what we're working on experiments for, yeah.
[02:29:50.140 --> 02:29:55.260]   - And you're trying to, like, by looking at the sliver of reality, show that there's always
[02:29:55.260 --> 02:29:56.820]   novelty being generated.
[02:29:56.820 --> 02:30:01.940]   - Yeah, because if we go and live in a universe that conventional physicists would live in,
[02:30:01.940 --> 02:30:05.020]   it's a big lookup table of stuff and everything exists.
[02:30:05.020 --> 02:30:10.100]   I want to prove that that book doesn't exist.
[02:30:10.100 --> 02:30:12.380]   It's continuously being added pages on.
[02:30:12.380 --> 02:30:13.380]   That's all I'm saying.
[02:30:13.380 --> 02:30:17.660]   If the universe is a book, we started, the universe at the beginning only had no pages,
[02:30:17.660 --> 02:30:21.220]   or had one page, another page, another page, whereas the physicists would now say all the
[02:30:21.220 --> 02:30:24.580]   pages exist, and we could in principle access them.
[02:30:24.580 --> 02:30:27.260]   I'm saying that is fundamentally incorrect.
[02:30:27.260 --> 02:30:29.340]   - Do you know what's written in this book?
[02:30:29.340 --> 02:30:32.320]   The free will question.
[02:30:32.320 --> 02:30:41.340]   Is there room for free will in this view of the universe as generating novelty and getting
[02:30:41.340 --> 02:30:45.580]   greater and greater assembly structures built?
[02:30:45.580 --> 02:30:46.580]   Sarah?
[02:30:46.580 --> 02:30:47.580]   - Yes.
[02:30:47.580 --> 02:30:48.580]   - Okay.
[02:30:48.580 --> 02:30:49.580]   - Done.
[02:30:49.580 --> 02:30:51.700]   Next question.
[02:30:51.700 --> 02:30:53.540]   - What's the source of free will in this?
[02:30:53.540 --> 02:30:56.580]   - I think it depends on what you mean by free will.
[02:30:56.580 --> 02:31:01.020]   - Yeah, well, please.
[02:31:01.020 --> 02:31:08.460]   - I think what I'm interested in as far as the phenomena of free will is do we have individual
[02:31:08.460 --> 02:31:15.020]   autonomy and agency, and when I do things, is it really me, or is it my atoms that did
[02:31:15.020 --> 02:31:17.940]   it?
[02:31:17.940 --> 02:31:19.140]   That's the part that's interesting to me.
[02:31:19.140 --> 02:31:23.220]   I guess there's also the determinism versus randomness part.
[02:31:23.220 --> 02:31:31.900]   The way I think about it is each of us are a thread or an assembly space through this
[02:31:31.900 --> 02:31:37.340]   giant possibility space, and it's like we're moving on our own trajectory through that
[02:31:37.340 --> 02:31:41.500]   space, and that is defined by our history, so we're sort of causally contingent on our
[02:31:41.500 --> 02:31:42.780]   past.
[02:31:42.780 --> 02:31:49.980]   But also because of the sort of intersection of novelty generation, it's not completely
[02:31:49.980 --> 02:31:52.340]   predetermined by the past.
[02:31:52.340 --> 02:31:59.020]   And so then you have the causal control of the determinism part that you are your causal
[02:31:59.020 --> 02:32:04.580]   history, and there's some determinism from that past, but there's also room for creativity.
[02:32:04.580 --> 02:32:09.660]   And I think it's actually necessary that something like free will exists if the universe is gonna
[02:32:09.660 --> 02:32:18.260]   be as creative as possible, because if I were an all-intelligent being inventing a universe,
[02:32:18.260 --> 02:32:23.140]   and I wanted it to have a maximal number of interesting things happen, again, we should
[02:32:23.140 --> 02:32:26.860]   come up with the metric of interesting, but generating-
[02:32:26.860 --> 02:32:27.860]   - Assembly.
[02:32:27.860 --> 02:32:28.860]   - Yes, I know.
[02:32:28.860 --> 02:32:33.980]   Generating maximal possibilities, then I would want the agents to have free will because
[02:32:33.980 --> 02:32:41.500]   it means that they're more individual, like each entity actually is a different causal
[02:32:41.500 --> 02:32:46.780]   force in the universe, and it's intrinsic and local property of that system.
[02:32:46.780 --> 02:32:52.420]   - There's a greater number of distributed agents, like are you always creating more
[02:32:52.420 --> 02:32:54.860]   and more individuality?
[02:32:54.860 --> 02:32:55.860]   - Kind of.
[02:32:55.860 --> 02:32:58.580]   I would say you're creating more causal power, but-
[02:32:58.580 --> 02:33:04.980]   - So causal power, the word consciousness, is the causal power somehow correlated with
[02:33:04.980 --> 02:33:05.980]   consciousness?
[02:33:05.980 --> 02:33:09.220]   - I mean, that's why I have this conception of consciousness being related to imagination,
[02:33:09.220 --> 02:33:13.580]   'cause the more that we can imagine can happen, and the more counterfactual possibilities
[02:33:13.580 --> 02:33:16.460]   you have in mind, the more you can actually implement.
[02:33:16.460 --> 02:33:21.580]   And somehow free will is also at the intersection of the counterfactual becoming the actual.
[02:33:21.580 --> 02:33:25.820]   - So can you elaborate on that a little bit, that consciousness is imagination?
[02:33:25.820 --> 02:33:30.620]   - I don't know exactly how to articulate it, and I'm sure people will aim at certain things
[02:33:30.620 --> 02:33:35.620]   I'm saying, but I think the language is really imprecise, so I'm not the best way to-
[02:33:35.620 --> 02:33:43.220]   - It's really interesting, like what is imagination, and what role does it play in the human experience,
[02:33:43.220 --> 02:33:45.300]   in experience of any agent?
[02:33:45.300 --> 02:33:49.140]   - I love imagination, I think it's the most amazing thing we do.
[02:33:49.140 --> 02:33:52.900]   But I guess one way I would think about it is, we talked about the transition to life
[02:33:52.900 --> 02:33:57.020]   being the universe acquiring memory, and life does something really interesting, you just
[02:33:57.020 --> 02:34:01.060]   think about biology generally, it remembers states of the past to adapt to things that
[02:34:01.060 --> 02:34:02.700]   happen in the future.
[02:34:02.700 --> 02:34:06.340]   So the longer life has evolved on this planet, the deeper that past is, the more memory we
[02:34:06.340 --> 02:34:08.980]   have, the more kinds of organisms and things.
[02:34:08.980 --> 02:34:13.660]   But what human level intelligence has done is quite different, it's not just that we
[02:34:13.660 --> 02:34:18.300]   remember states that the universe has existed in before, it's that we can imagine ones that
[02:34:18.300 --> 02:34:22.720]   have never existed, and we can actually make them come into existence.
[02:34:22.720 --> 02:34:27.700]   And I think that's the most unique feature about the transition to whatever we are, from
[02:34:27.700 --> 02:34:32.100]   what life on this planet has been doing for the last four billion years.
[02:34:32.100 --> 02:34:34.500]   And I think it's deeply related to the phenomena we call consciousness.
[02:34:34.500 --> 02:34:38.220]   - Yeah, I was gonna just agree with that, I think that consciousness is the ability
[02:34:38.220 --> 02:34:40.480]   to generate those counterfactuals.
[02:34:40.480 --> 02:34:47.820]   Now whether you can say, are there degrees of consciousness, I mean, I'm sorry, panpsychics,
[02:34:47.820 --> 02:34:51.900]   but electrons don't have counterfactuals, although they do have some, they are able
[02:34:51.900 --> 02:34:54.180]   to search the space and pathways.
[02:34:54.180 --> 02:35:02.500]   But I think that there is a very concrete, there's a very specific property that humans
[02:35:02.500 --> 02:35:07.980]   have, and I don't know if it's unique to humans, I mean, maybe dogs can do it, and birds can
[02:35:07.980 --> 02:35:08.980]   do it, right?
[02:35:08.980 --> 02:35:14.580]   And where they are basically solving a problem, 'cause consciousness was invented, or this
[02:35:14.580 --> 02:35:19.440]   abstraction was invented by evolution for a specific reason.
[02:35:19.440 --> 02:35:24.780]   And so look, one of the reasons why I came to the conclusion that time was fundamental
[02:35:24.780 --> 02:35:30.220]   was actually because Sarah and I had a completely different--
[02:35:30.220 --> 02:35:32.420]   - The most heated debate on Skype chat ever.
[02:35:32.420 --> 02:35:33.860]   - No, no, no, no, we had two--
[02:35:33.860 --> 02:35:34.860]   - None of us stopped it?
[02:35:34.860 --> 02:35:37.680]   - No, no, it goes back to the free will thing.
[02:35:37.680 --> 02:35:41.960]   So I think that, although I've changed my view a bit, because there's some really interesting
[02:35:41.960 --> 02:35:48.220]   physicists out there who talks about how the measurement problem in Newtonian space, but
[02:35:48.220 --> 02:35:53.620]   I don't wanna go there just now because I think I'll mess it up, but briefly, I could
[02:35:53.620 --> 02:35:56.060]   not see how we can have free will.
[02:35:56.060 --> 02:36:01.780]   I mean, this is really boring 'cause this is a well-trodden path, but not so boring,
[02:36:01.780 --> 02:36:04.220]   I suppose it's kind of, we just wanna be precise.
[02:36:04.220 --> 02:36:08.060]   If the universe is deterministic, how can we have free will, right?
[02:36:08.060 --> 02:36:16.740]   So Sarah's a physicist, I think she can show that most of the laws we have are deterministic
[02:36:16.740 --> 02:36:21.380]   to some degree, quantum mechanics onto Newtonian stuff, and yet there's Sarah telling me she
[02:36:21.380 --> 02:36:22.380]   believes in free will.
[02:36:22.380 --> 02:36:29.660]   I'm like, your belief system's broken here, right, because you're demanding free will
[02:36:29.660 --> 02:36:38.020]   in a deterministic universe, and then I realized that I agreed with her that I do think that
[02:36:38.020 --> 02:36:42.020]   free will is a thing because we're able to search for novelty, and then that's where
[02:36:42.020 --> 02:36:46.820]   I came to the conclusion that time, the universe is expanding in terms of novelty, and it goes
[02:36:46.820 --> 02:36:51.940]   back to that Dan Dennett essay, they were talking about the free will inflation.
[02:36:51.940 --> 02:36:58.860]   So the past, it did not exist in the past, the past exists in the present.
[02:36:58.860 --> 02:37:03.700]   What I mean is like, there was no past, there is only present.
[02:37:03.700 --> 02:37:08.300]   So that means you are the sum total, everything that occurs in the past is manifestly here
[02:37:08.300 --> 02:37:13.740]   in the present, and then you have this little echo state in your consciousness because you're
[02:37:13.740 --> 02:37:20.140]   able to imagine something without actualization, but the fact you imagine it, that occurs in
[02:37:20.140 --> 02:37:24.860]   electrons and potassium ion flows in your neural network in your brain.
[02:37:24.860 --> 02:37:27.900]   Maybe consciousness is just the present.
[02:37:27.900 --> 02:37:33.860]   So somehow you imagine that, and then by imagining, oh that's good, I'm going to make a robot
[02:37:33.860 --> 02:37:39.180]   do this thing, and program it, and then you physically then go and do it.
[02:37:39.180 --> 02:37:42.300]   So that changes the future.
[02:37:42.300 --> 02:37:44.340]   - What's imagination?
[02:37:44.340 --> 02:37:49.180]   Does it require the past, does it require the future, does it require memory, does it
[02:37:49.180 --> 02:37:53.180]   - It's imagination - Does it only exist in the moment?
[02:37:53.180 --> 02:37:59.260]   - So imagination is, yeah probably it's an instantaneous readout of what's going on.
[02:37:59.260 --> 02:38:04.740]   You can maybe, your subconscious brain has been generating all the bits for it, but no,
[02:38:04.740 --> 02:38:10.660]   imagination occurs when you, in your game engine, you remember the past and you integrate
[02:38:10.660 --> 02:38:14.540]   sensory of the present, and you try and work out what you want to do in the future, and
[02:38:14.540 --> 02:38:16.180]   then you go and make that happen.
[02:38:16.180 --> 02:38:21.380]   So the imagination is, it's like, asking what imagination is about, asking what surfing
[02:38:21.380 --> 02:38:22.540]   is.
[02:38:22.540 --> 02:38:26.220]   You can see, you can surfboard, surfer, wave coming in.
[02:38:26.220 --> 02:38:31.340]   But when you're on that wave and you're surfing, that's where the imagination is.
[02:38:31.340 --> 02:38:35.780]   - I think imagination is just accessing things that aren't the present moment, in the present
[02:38:35.780 --> 02:38:36.780]   moment.
[02:38:36.780 --> 02:38:41.020]   So like I'm sitting here and I'm looking at the table, and I can imagine the river and
[02:38:41.020 --> 02:38:46.940]   things, or whatever it was, and so it seems to be that it's like, it's our ability to
[02:38:46.940 --> 02:38:50.340]   access things that aren't present.
[02:38:50.340 --> 02:38:56.460]   - So conjure up worlds, some of them might be akin to something that happened to you
[02:38:56.460 --> 02:38:57.460]   recently?
[02:38:57.460 --> 02:39:00.740]   - Right, but they don't have to be things that actually happened in your past.
[02:39:00.740 --> 02:39:04.820]   And I think this gets back to assembly theory, like the way I would think about imagination
[02:39:04.820 --> 02:39:11.540]   from an assembly theoretic standpoint is I'm a giant causal graph, and I exist in a present
[02:39:11.540 --> 02:39:18.340]   moment as a particular configuration of Sarah, but there's a lot of, I carry a lot of evolutionary
[02:39:18.340 --> 02:39:22.500]   baggage, I have that whole causal history, and I can access parts of it.
[02:39:22.500 --> 02:39:26.140]   Now when you talk about getting to something as complex as us, having as large an assembly
[02:39:26.140 --> 02:39:31.420]   space as us, there's ways of, like there's a lot of things in that causal graph that
[02:39:31.420 --> 02:39:36.100]   have actually never existed in the past history of the universe, 'cause like the universe
[02:39:36.100 --> 02:39:42.980]   got big enough to contain the three of us in this room in time, but not all the features
[02:39:42.980 --> 02:39:49.420]   of each one of us individually have come into existence as physical objects we would recognize
[02:39:49.420 --> 02:39:50.420]   as individual objects.
[02:39:50.420 --> 02:39:55.380]   This goes back to your point that we actually have to explain why things actually even look
[02:39:55.380 --> 02:40:00.940]   like objects and aren't just a shmear of mess.
[02:40:00.940 --> 02:40:05.060]   And just on the free will in physics thing, when you were talking, I just wanna bring
[02:40:05.060 --> 02:40:09.700]   this up 'cause I think it's a really interesting viewpoint that Nicholas Jisen has that, you
[02:40:09.700 --> 02:40:12.820]   know, like we wanna use the laws of physics and then say you can't have free will, and
[02:40:12.820 --> 02:40:17.100]   his point is you have to have free will in order to even choose to set up an experiment
[02:40:17.100 --> 02:40:18.260]   to test the laws of physics.
[02:40:18.260 --> 02:40:22.380]   So in some sense, free will should be more fundamental than physics is, 'cause to even
[02:40:22.380 --> 02:40:28.500]   do science, there's some assumption that the agents have free will.
[02:40:28.500 --> 02:40:33.900]   And I always thought it was really perplexing that, you know, physics wants to remove agency
[02:40:33.900 --> 02:40:37.460]   because the idea that I could do an experiment here on this part of Earth, and then I can
[02:40:37.460 --> 02:40:41.620]   move somewhere else and prepare an identically, you know, identically prepared experiment,
[02:40:41.620 --> 02:40:45.260]   and then do an experiment again, seems to imply something about the structure of our
[02:40:45.260 --> 02:40:50.020]   universe that is not encoded in the laws that we're testing in those experiments.
[02:40:50.020 --> 02:40:53.700]   - So this kind of dream of physics that you can do multiple experiments, different locations,
[02:40:53.700 --> 02:40:58.740]   and then validate each other, you're saying that's an illusion?
[02:40:58.740 --> 02:41:04.260]   - No, I'm saying that requires decision making and free will to be a real thing, I think.
[02:41:04.260 --> 02:41:09.460]   Like I think the fact that we can do science is not arbitrary, and I think people, you
[02:41:09.460 --> 02:41:12.860]   know, the standard canon in physics would be, well, you could trace all of that back
[02:41:12.860 --> 02:41:16.800]   to the initial condition of the universe, but the whole point of science is I can imagine
[02:41:16.800 --> 02:41:19.700]   doing the experiment and I can do it, and then I can do it again and again and again
[02:41:19.700 --> 02:41:20.700]   all over the planet.
[02:41:20.700 --> 02:41:24.620]   - But to you, imagination is somehow fundamentally generative of novelty.
[02:41:24.620 --> 02:41:25.620]   - Yes.
[02:41:25.620 --> 02:41:28.980]   - So it's not like the universe could have predicted the things you imagined.
[02:41:28.980 --> 02:41:33.340]   - Imagination's super, so coming back to novelty, I think novelty can exist outside of imagination,
[02:41:33.340 --> 02:41:36.340]   but it supercharges it, it's another transition, I think.
[02:41:36.340 --> 02:41:39.900]   I mean, I would say, I mean, this may be a boring statement, but I would say the fact
[02:41:39.900 --> 02:41:40.900]   that--
[02:41:40.900 --> 02:41:42.740]   - I'm not sure, these are hard questions.
[02:41:42.740 --> 02:41:48.460]   - Yeah, I mean, I think the fact that objects exist is yet another proof that time is fundamental
[02:41:48.460 --> 02:41:50.780]   and novelty exists, right?
[02:41:50.780 --> 02:41:55.180]   Because I think, again, if you ask a physicist to write down in their infinite bible of the
[02:41:55.180 --> 02:41:57.220]   universe, let's call it the bible, the--
[02:41:57.220 --> 02:41:58.220]   - The book?
[02:41:58.220 --> 02:41:59.220]   - Yeah, well, I mean--
[02:41:59.220 --> 02:42:00.220]   - That book that we're adding pages to?
[02:42:00.220 --> 02:42:03.580]   - The mathematical universe, whether you're Max Tegmark or Sean Carroll--
[02:42:03.580 --> 02:42:04.580]   - I thought it was a different kind of science.
[02:42:04.580 --> 02:42:05.580]   - Or Steven Wolfram, okay?
[02:42:05.580 --> 02:42:06.580]   - I like that book.
[02:42:06.580 --> 02:42:07.580]   - Yeah, I love it too.
[02:42:07.580 --> 02:42:08.580]   It's lots of pretty pictures.
[02:42:08.580 --> 02:42:09.580]   - It's really interesting that they cope with the enormity of the universe by saying, well,
[02:42:09.580 --> 02:42:10.580]   it's all there, mathematics, it all exists, right?
[02:42:10.580 --> 02:42:11.580]   And I would say that that's why I'm excited about the future of the universe, because
[02:42:11.580 --> 02:42:12.580]   it, although it is somehow dependent upon the past, it is not constrained just by the
[02:42:12.580 --> 02:42:13.580]   fact that it's a part of the universe.
[02:42:13.580 --> 02:42:14.580]   - Yeah, I think that's a really good point.
[02:42:14.580 --> 02:42:15.580]   - Yeah, I think that's a really good point.
[02:42:15.580 --> 02:42:16.580]   - Yeah, I think that's a really good point.
[02:42:16.580 --> 02:42:17.580]   - Yeah, I think that's a really good point.
[02:42:17.580 --> 02:42:18.580]   - Yeah, I think that's a really good point.
[02:42:18.580 --> 02:42:19.580]   - Yeah, I think that's a really good point.
[02:42:19.580 --> 02:42:45.580]   - Yeah, I think that's a really good point.
[02:42:45.580 --> 02:42:56.660]   - I mean, the other thing I would say that's super important for human beings, right?
[02:42:56.660 --> 02:43:00.660]   Human beings have actually very little causal control in the future.
[02:43:00.660 --> 02:43:02.180]   I realized this the other week.
[02:43:02.180 --> 02:43:03.620]   - Oh, the immediate future, you mean.
[02:43:03.620 --> 02:43:06.260]   - Yeah, yeah, so what happened, so this is what I think it is.
[02:43:06.260 --> 02:43:11.860]   The way, by reinterpreting your past, I mean, talk about from a kind of cognitive, psychological
[02:43:11.860 --> 02:43:18.620]   cognitive point of view, by reinterpreting your past in your current mind, you can actually
[02:43:18.620 --> 02:43:22.020]   help you shape your future again.
[02:43:22.020 --> 02:43:27.340]   You have much more freedom to interpret your past, to act in the present, to change your
[02:43:27.340 --> 02:43:29.540]   future than you do to change your future.
[02:43:29.540 --> 02:43:34.420]   It may sound weird, so I'm saying, everybody, imagine your past, think about your past,
[02:43:34.420 --> 02:43:39.500]   reinterpret your past in the nicest way you can, then imagine what you can do next, or
[02:43:39.500 --> 02:43:42.460]   imagine your past in a more negative way, and what you do next, and look at those two
[02:43:42.460 --> 02:43:44.460]   counterfactuals, they're different.
[02:43:44.460 --> 02:43:48.060]   - Yeah, it's fascinating, I mean, Daniel Kahneman talks about this, that most of our life is
[02:43:48.060 --> 02:43:52.660]   lived in our memories, and it's interesting, 'cause you can essentially, in imagination,
[02:43:52.660 --> 02:43:53.660]   choose the life you live.
[02:43:53.660 --> 02:44:01.220]   So maybe free will exists in imagination, choices are made in your imagination, and
[02:44:01.220 --> 02:44:07.300]   that results in you basically able to control how the future unrolls, 'cause you're like,
[02:44:07.300 --> 02:44:11.540]   reinterpreting constantly the things that happen to you.
[02:44:11.540 --> 02:44:16.300]   - Exactly, so if you want to increase your amount of free will, those people that have,
[02:44:16.300 --> 02:44:22.660]   I don't think everyone has equal amounts of agency, because of our sad constraints, where
[02:44:22.660 --> 02:44:29.660]   they're happenstance, health, economic, born in a certain place, right?
[02:44:29.660 --> 02:44:36.180]   But those of us that have the ability to go back and reinterpret our past, and use that
[02:44:36.180 --> 02:44:42.900]   to change the future, are the ones that exert most agency in the present.
[02:44:42.900 --> 02:44:49.300]   And I want to achieve higher degrees of agency, and enable everyone else to do that as well,
[02:44:49.300 --> 02:44:52.020]   to have more fun in the universe.
[02:44:52.020 --> 02:44:54.340]   - Then we'll hit that peak, the maximum fun point.
[02:44:54.340 --> 02:44:58.700]   - I don't think there's ever gonna be a maximum, I think the wonderful thing about the future
[02:44:58.700 --> 02:45:00.220]   is there's always gonna be more fun.
[02:45:00.380 --> 02:45:07.100]   - Yeah, you, I think, again, going back to Twitter, I think you retweeted something about
[02:45:07.100 --> 02:45:11.620]   being a life maximalist, that you want to maximize the number of life, the amount of
[02:45:11.620 --> 02:45:13.740]   life in the universe.
[02:45:13.740 --> 02:45:19.900]   And that's the more general version of that goal, is to maximize the amount of fun in
[02:45:19.900 --> 02:45:24.060]   the universe, 'cause life is a subset of fun, there are all kinds of, I suppose they're
[02:45:24.060 --> 02:45:27.260]   either correlated or exactly equal, I don't know.
[02:45:27.260 --> 02:45:32.380]   Anyway, speaking of fun, let me ask you about alien sightings.
[02:45:32.380 --> 02:45:36.420]   So there's been quite a bit of UFO sightings and all that kind of stuff.
[02:45:36.420 --> 02:45:45.860]   What do you think would be the first time when humans sight aliens, see aliens, in a
[02:45:45.860 --> 02:45:49.540]   sort of unquestionable way?
[02:45:49.540 --> 02:45:56.820]   This extremely strong and arguable way we've made contact with aliens.
[02:45:56.820 --> 02:45:59.460]   Sarah, what would it look like?
[02:45:59.460 --> 02:46:05.420]   Obviously the space of possibility is huge here, but if you were to kind of look into
[02:46:05.420 --> 02:46:07.300]   the future, what would that look like?
[02:46:07.300 --> 02:46:14.420]   Would it be inklings of UFOs here and there that slowly unravel a mystery, or would it
[02:46:14.420 --> 02:46:19.120]   be like an obvious overwhelming signal?
[02:46:19.120 --> 02:46:23.220]   So I think we have an obsession with making contact with events.
[02:46:23.220 --> 02:46:30.180]   So what I mean by that is, people have a UFO sighting, they make contact.
[02:46:30.180 --> 02:46:36.940]   And I always think, what's interesting to me about the UFO narratives right now is not
[02:46:36.940 --> 02:46:41.700]   that I have a disbelief about what people are experiencing or feeling, but the discussion
[02:46:41.700 --> 02:46:45.540]   right now is sort of at the level of modern mythology.
[02:46:45.540 --> 02:46:49.420]   Aliens are our mythos in modern culture.
[02:46:49.420 --> 02:46:55.660]   And when you treat it like that, then I want to think about when do things that we traditionally
[02:46:55.660 --> 02:47:00.300]   only regularize through mythology actually become things that become standard knowledge.
[02:47:00.300 --> 02:47:05.860]   So it used to be variations in the climate were described by some kind of gods or something,
[02:47:05.860 --> 02:47:10.020]   and now it's like our technology picks up an anomaly or someone sees something, we say
[02:47:10.020 --> 02:47:11.220]   it's aliens.
[02:47:11.220 --> 02:47:16.700]   And I think the real thing is, it's not contact with events, but first contact is actually
[02:47:16.700 --> 02:47:20.060]   contact with knowledge of the phenomena or the explanation.
[02:47:20.060 --> 02:47:25.180]   And so this is very subtle and very abstract, but when does it become something that we
[02:47:25.180 --> 02:47:28.500]   actually understand what it is that we're talking about?
[02:47:28.500 --> 02:47:29.500]   That's first contact.
[02:47:29.500 --> 02:47:30.500]   It's not-
[02:47:30.500 --> 02:47:35.260]   Would you make the myth, would you give credit to the myth, the mythology as first contact?
[02:47:35.260 --> 02:47:36.780]   I think, yes.
[02:47:36.780 --> 02:47:40.620]   I think it's the rudimentary that we have some understanding that there's a phenomena
[02:47:40.620 --> 02:47:42.980]   that we have to understand and regularize.
[02:47:42.980 --> 02:47:43.980]   So I think-
[02:47:43.980 --> 02:47:44.980]   To understand that there is weather.
[02:47:44.980 --> 02:47:45.980]   Yes.
[02:47:45.980 --> 02:47:48.300]   You have to construct a mythology around that weather.
[02:47:48.300 --> 02:47:49.300]   Yes.
[02:47:49.300 --> 02:47:50.300]   It's something that's controllable.
[02:47:50.300 --> 02:47:51.300]   Right.
[02:47:51.300 --> 02:47:55.180]   I see mythology basically as like baby knowledge.
[02:47:55.180 --> 02:47:56.180]   Right.
[02:47:56.180 --> 02:48:03.260]   It could be that, although there's lots of alien sight, so-called alien sightings, right?
[02:48:03.260 --> 02:48:04.420]   So there is a number of things you can do.
[02:48:04.420 --> 02:48:07.540]   You could just dismiss them and say they're not true, they're kind of made up.
[02:48:07.540 --> 02:48:09.740]   Or you say, well, there's something interesting here, right?
[02:48:09.740 --> 02:48:11.660]   We keep seeing a commonality, right?
[02:48:11.660 --> 02:48:14.540]   We see the same phenomena again and again and again.
[02:48:14.540 --> 02:48:17.540]   Also there's this interesting thing about human imagination.
[02:48:17.540 --> 02:48:23.820]   Even if they are, let's not say made up, but misappropriated kind of other inputs, the
[02:48:23.820 --> 02:48:29.640]   fact that human consciousness is capable of imagining it contact with aliens, does that
[02:48:29.640 --> 02:48:32.900]   not tell us about something about where we are in our position, in our culture, in our
[02:48:32.900 --> 02:48:33.900]   technology?
[02:48:33.900 --> 02:48:35.500]   It tells us about where in time we are.
[02:48:35.500 --> 02:48:38.780]   Could it be that we're making contact with, let's say that-
[02:48:38.780 --> 02:48:41.580]   So let's say, let's take the most miserable version.
[02:48:41.580 --> 02:48:42.820]   There are no aliens in the universe.
[02:48:42.820 --> 02:48:44.900]   Life is only on Earth.
[02:48:44.900 --> 02:48:48.460]   That then, the interpretation of that is we're desperate to kind of understand why we're
[02:48:48.460 --> 02:48:50.580]   the only life in the universe, right?
[02:48:50.580 --> 02:48:53.460]   The other one is, the other most extreme is that aliens are visiting all the time and
[02:48:53.460 --> 02:48:57.220]   we're just not able to capture them coherently.
[02:48:57.220 --> 02:49:02.780]   Or there's a big conspiracy and there's Area 51 and there are lizards everywhere and there's
[02:49:02.780 --> 02:49:03.780]   that.
[02:49:03.780 --> 02:49:08.540]   Or I'm kind of in favor of the idea that maybe humanity is waking up to the idea that we
[02:49:08.540 --> 02:49:13.420]   aren't alone in the universe and we're just running the simulation and we're seeing some
[02:49:13.420 --> 02:49:14.420]   evidence.
[02:49:14.420 --> 02:49:17.140]   You know, we don't know what life is yet.
[02:49:17.140 --> 02:49:19.840]   We do have some anomalies out there.
[02:49:19.840 --> 02:49:23.220]   We can't explain everything.
[02:49:23.220 --> 02:49:28.820]   And over time, you know, we will start to unpack that.
[02:49:28.820 --> 02:49:35.780]   One very plausible thing we might do, which might be boring for the average alien observer
[02:49:35.780 --> 02:49:40.140]   or believes that aliens, as in intelligent aliens, are visiting Earth, it could be that
[02:49:40.140 --> 02:49:45.420]   we might go to the outer solar system and find a new type of life that has completely
[02:49:45.420 --> 02:49:46.820]   new chemistry.
[02:49:46.820 --> 02:49:51.580]   Bring these cells back to Earth where you could say in my hand, "On Earth, here's RNA,
[02:49:51.580 --> 02:49:54.820]   DNA and proteins and look, cells self-replicating."
[02:49:54.820 --> 02:50:00.060]   From Titan, we have this new set of molecules, new set of cells and we feed it stuff and
[02:50:00.060 --> 02:50:01.060]   it grows.
[02:50:01.060 --> 02:50:08.780]   That, for me, if we were able to do that, would be like the most, that would be my UFO
[02:50:08.780 --> 02:50:09.780]   sign.
[02:50:09.780 --> 02:50:10.780]   That's a good test.
[02:50:10.780 --> 02:50:12.780]   So you feed it and it grows.
[02:50:12.780 --> 02:50:13.780]   Yeah.
[02:50:13.780 --> 02:50:21.660]   We've made, so not until you know how to feed the thing.
[02:50:21.660 --> 02:50:22.740]   It grows somehow.
[02:50:22.740 --> 02:50:25.780]   We can make a comic book, you know, the tiger that came for tea, the alien that came for
[02:50:25.780 --> 02:50:26.780]   tea.
[02:50:27.780 --> 02:50:33.740]   What would you say is between the two of you is the biggest disagreement about aliens,
[02:50:33.740 --> 02:50:35.940]   alien life out there?
[02:50:35.940 --> 02:50:41.540]   Is it from the basic framework of thinking about what is life to maybe what aliens look
[02:50:41.540 --> 02:50:46.020]   like to alien civilizations to UFO sightings?
[02:50:46.020 --> 02:50:47.460]   What would you think?
[02:50:47.460 --> 02:50:55.740]   So I would say the biggest one is that the emergence of life does not have to be, it
[02:50:55.740 --> 02:50:57.580]   can't just happen once on a planet.
[02:50:57.580 --> 02:51:01.460]   That it could be two or more life forms present on a planet at once.
[02:51:01.460 --> 02:51:03.500]   And I think Sarah doesn't agree with that.
[02:51:03.500 --> 02:51:07.900]   I think that's like logically inconsistent.
[02:51:07.900 --> 02:51:08.900]   That's really polite.
[02:51:08.900 --> 02:51:09.900]   You're saying it's nonsense.
[02:51:09.900 --> 02:51:11.300]   But because you think that, yeah.
[02:51:11.300 --> 02:51:12.300]   How likely is that?
[02:51:12.300 --> 02:51:14.860]   So the idea that, what does it look like?
[02:51:14.860 --> 02:51:19.660]   Let's imagine two alien civilizations coexisting on a planet.
[02:51:19.660 --> 02:51:21.360]   What's that look like exactly?
[02:51:21.360 --> 02:51:26.740]   So I would say, I think I've got to get around your argument.
[02:51:26.740 --> 02:51:34.660]   Let's say that on this planet there's lots of available chemistry and one life form emerges
[02:51:34.660 --> 02:51:39.060]   based on carbon and interacts and there's an ecosystem based on carbon.
[02:51:39.060 --> 02:51:45.460]   And there's an orthogonal, and so it's planetary phenomena, which is what you, I think, right?
[02:51:45.460 --> 02:51:48.460]   But there's also one that co-exists on silicon.
[02:51:48.460 --> 02:51:53.780]   And because there's enough energy and there's enough stuff that these life forms might not
[02:51:53.780 --> 02:51:56.580]   actually necessarily compete evolutionarily.
[02:51:56.580 --> 02:52:01.380]   Yeah, but they would have to not interact at all because they're going to be co-constructing
[02:52:01.380 --> 02:52:02.660]   each other's causal chains.
[02:52:02.660 --> 02:52:04.100]   I think that's what you just got me.
[02:52:04.100 --> 02:52:10.220]   So there's no overlap in terms of their causal chains or very limited overlap.
[02:52:10.220 --> 02:52:13.420]   Yeah, so I think the only way I can get away with that is to say, right, life could emerge
[02:52:13.420 --> 02:52:14.420]   on a planet underneath.
[02:52:14.420 --> 02:52:18.620]   The lizard people under the crust of the earth.
[02:52:18.620 --> 02:52:25.060]   I think, I think, I think, let's go, I think, but look, as you can see, we disagree.
[02:52:25.060 --> 02:52:30.260]   And I think Sarah actually has convinced me because of that life is a planetary phenomena,
[02:52:30.260 --> 02:52:33.100]   the emergence of life is a planetary phenomena.
[02:52:33.100 --> 02:52:38.180]   And actually because of the way evolution selection works, that nothing occurs in isolation,
[02:52:38.180 --> 02:52:39.400]   the causal chains interact.
[02:52:39.400 --> 02:52:42.420]   So there is a common, there's a consensus model for life on the earth.
[02:52:42.420 --> 02:52:50.420]   But you don't think you can place aliens from elsewhere onto the, can't you just place multiple
[02:52:50.420 --> 02:52:52.420]   alien civilizations on one planet?
[02:52:52.420 --> 02:52:58.580]   Right, but I think, so you can take two original life events that were independent and co-mingle
[02:52:58.580 --> 02:53:05.220]   them, but I don't think when you're talking about, when you look at the interaction of
[02:53:05.220 --> 02:53:11.340]   that structure, it's like the same idea as like an experiment being an example of life,
[02:53:11.340 --> 02:53:12.340]   right?
[02:53:12.340 --> 02:53:13.340]   It's a very abstract and subtle concept.
[02:53:13.340 --> 02:53:18.340]   And I guess what I'm saying is life is information propagating through matter.
[02:53:18.340 --> 02:53:23.900]   So once you start having things interacting, they in some sense co-mingle and they become
[02:53:23.900 --> 02:53:25.900]   part of the same chain.
[02:53:25.900 --> 02:53:27.860]   - Sarah's a good--
[02:53:27.860 --> 02:53:30.820]   - The co-mingling starts quickly.
[02:53:30.820 --> 02:53:33.260]   We proceed to co-mingle quickly no matter what.
[02:53:33.260 --> 02:53:34.260]   - Right, right.
[02:53:34.260 --> 02:53:38.180]   So you could say, so the question is then, the more interesting question is are there
[02:53:38.180 --> 02:53:39.960]   two distinct origins events?
[02:53:39.960 --> 02:53:43.920]   And I still think that there's reasons that on a single planet you would have one origins
[02:53:43.920 --> 02:53:49.700]   event because of the time scales of cycling of geochemistry on a planet.
[02:53:49.700 --> 02:53:53.540]   And also the fact that I don't think that the origin of life happens in a pool and like
[02:53:53.540 --> 02:53:56.060]   radiates outward through evolutionary processes.
[02:53:56.060 --> 02:53:57.580]   I think it's a multi-scale phenomenon.
[02:53:57.580 --> 02:54:02.020]   It happens at the level of individual molecules interacting, collections of molecules interacting
[02:54:02.020 --> 02:54:04.260]   and entire planetary scale cycles.
[02:54:04.260 --> 02:54:08.260]   So life as we know it has always been multi-scale.
[02:54:08.260 --> 02:54:13.100]   And there's brilliant examples of individual mutations at the genome level changing global
[02:54:13.100 --> 02:54:14.100]   climate.
[02:54:14.100 --> 02:54:21.020]   So there's a tight coupling between things that happen at the largest scale or planetary
[02:54:21.020 --> 02:54:23.940]   scale and the smallest scale that life mediates.
[02:54:23.940 --> 02:54:29.900]   - But it still might be difficult within something you would call as a single alien civilization.
[02:54:29.900 --> 02:54:31.940]   There's species and stuff.
[02:54:31.940 --> 02:54:32.940]   - But I think what--
[02:54:32.940 --> 02:54:33.940]   - Yeah.
[02:54:33.940 --> 02:54:34.940]   - And they might not be able to communicate.
[02:54:34.940 --> 02:54:36.660]   - But you're asking about life, not species, right?
[02:54:36.660 --> 02:54:43.220]   - But what's the difference between one living civilization?
[02:54:43.220 --> 02:54:45.220]   This is almost like a category question.
[02:54:45.220 --> 02:54:46.220]   - Yeah.
[02:54:46.220 --> 02:54:47.220]   - Versus species.
[02:54:47.220 --> 02:54:48.220]   - Right.
[02:54:48.220 --> 02:54:49.220]   - 'Cause it can be very different.
[02:54:49.220 --> 02:54:50.220]   - Right.
[02:54:50.220 --> 02:54:53.780]   - Like evolution, 'cause there's like island, like literally islands that you can evolve
[02:54:53.780 --> 02:54:55.500]   different kinds of turtles and stuff.
[02:54:55.500 --> 02:54:56.500]   - Yeah.
[02:54:56.500 --> 02:54:57.500]   - And they can--
[02:54:57.500 --> 02:54:58.500]   - So I guess what I'm saying is--
[02:54:58.500 --> 02:54:59.500]   - Weird in different ways.
[02:54:59.500 --> 02:55:05.980]   - If you look at the structure of two interacting living things, populations, and you look in
[02:55:05.980 --> 02:55:10.580]   their past and they have independent origins for their causal chain, then you would say
[02:55:10.580 --> 02:55:14.980]   one was alien, they have different independent origins events.
[02:55:14.980 --> 02:55:18.580]   But if you look at their future by virtue of the fact they're interacting, their causal
[02:55:18.580 --> 02:55:20.220]   chains have become co-mingled.
[02:55:20.220 --> 02:55:24.460]   So that in the future, they are not independent.
[02:55:24.460 --> 02:55:25.460]   - Right.
[02:55:25.460 --> 02:55:28.500]   - Right, so that's why you would even define them as alien.
[02:55:28.500 --> 02:55:33.820]   So the structure across time is two examples of life become one example of life because
[02:55:33.820 --> 02:55:36.380]   life is the entire structure across time.
[02:55:36.380 --> 02:55:39.300]   - Right, but there could be a lot of variation within--
[02:55:39.300 --> 02:55:46.220]   - Yeah, so the question we're all interested in is how many independent origins of a complexifying
[02:55:46.220 --> 02:55:49.340]   causal chain are there in the universe?
[02:55:49.340 --> 02:55:55.300]   - But the idea of origin is easy for you to define?
[02:55:55.300 --> 02:55:56.300]   'Cause like--
[02:55:56.300 --> 02:55:57.300]   - Well--
[02:55:57.300 --> 02:56:05.260]   - When the species split in the evolutionary process and you get like a dolphin versus
[02:56:05.260 --> 02:56:09.900]   a human or Neanderthal versus Homo sapien, isn't there--
[02:56:09.900 --> 02:56:13.020]   - Let me make a distinction here quickly.
[02:56:13.020 --> 02:56:20.420]   So I think, sorry to interrupt, what we're saying, I mean, Sarah won that argument 'cause
[02:56:20.420 --> 02:56:22.580]   I think she's right.
[02:56:22.580 --> 02:56:25.860]   Once the causal chains interact and go forward, so we're talking about a number of things.
[02:56:25.860 --> 02:56:28.020]   Let's go all the way back before origin of life.
[02:56:28.020 --> 02:56:29.020]   Origin of life--
[02:56:29.020 --> 02:56:30.020]   - On Earth.
[02:56:30.020 --> 02:56:31.020]   - On Earth.
[02:56:31.020 --> 02:56:35.460]   Chemistry emerges, so there's all these, I would say there's probably mechanistically,
[02:56:35.460 --> 02:56:38.540]   the chemistry's desperately trying to find any way to get replicators.
[02:56:38.540 --> 02:56:42.540]   The ribosome kind of was really rubbish at the beginning and it just competed, competed,
[02:56:42.540 --> 02:56:46.380]   competed and you got better and better ribosome and suddenly that was the technology.
[02:56:46.380 --> 02:56:51.740]   The ribosome is the technology that boom, allowed evolution to start.
[02:56:51.740 --> 02:56:57.940]   So why I interrupted you is say that once evolution has started using that technology,
[02:56:57.940 --> 02:57:04.420]   then you can speciate and I was trying to, and I think what Sarah was convinced me of
[02:57:04.420 --> 02:57:08.100]   'cause I was like, no, we can have lots of different chemistry, shadow biosphere on Earth
[02:57:08.100 --> 02:57:14.140]   and she's like, no, no, no, you have to have this, you have to get to this minimum evolutionary
[02:57:14.140 --> 02:57:18.780]   machine and then when that occurs, speciation occurs.
[02:57:18.780 --> 02:57:24.860]   So it's like dolphins, humans, everything on Earth but when you're looking at aliens
[02:57:24.860 --> 02:57:30.100]   or alien life, there's not gonna be two different types of chemistry because they compete and
[02:57:30.100 --> 02:57:33.400]   interact and cooperate because the causal chains overlap.
[02:57:33.400 --> 02:57:37.260]   One might kill the other, one might combine with the other and then you go on and then
[02:57:37.260 --> 02:57:42.100]   you have this kind of, this average and sure, there might be re-speciation.
[02:57:42.100 --> 02:57:44.640]   It might be you have two types of emerging chemistry.
[02:57:44.640 --> 02:57:50.180]   It almost looks like the origin of life on Earth required two different pre-life forms,
[02:57:50.180 --> 02:57:52.860]   the peptide world and the RNA world.
[02:57:52.860 --> 02:57:57.800]   Somehow they got together and by combining, you got the ribosome and that was the minimum
[02:57:57.800 --> 02:58:00.060]   competent entity for evolution.
[02:58:00.060 --> 02:58:06.900]   - And would all alien civilizations have an evolutionary process on a planet?
[02:58:06.900 --> 02:58:10.580]   So that's one of the, it's almost the definition of life.
[02:58:10.580 --> 02:58:13.980]   To create all those memories, you have to have something--
[02:58:13.980 --> 02:58:15.940]   - You have to change in time.
[02:58:15.940 --> 02:58:19.780]   - But there has to be selection.
[02:58:19.780 --> 02:58:22.140]   That's like an efficient, there's no other way to do it.
[02:58:22.140 --> 02:58:23.140]   - No.
[02:58:23.140 --> 02:58:24.980]   Well, never say never because soon as I say that--
[02:58:24.980 --> 02:58:28.820]   - That's the part that depresses me though, going back to like, I don't know, the earlier
[02:58:28.820 --> 02:58:30.860]   discussion on violence and things.
[02:58:30.860 --> 02:58:35.940]   And I don't know where, somebody was tweeting about this recently but like, how much stuff
[02:58:35.940 --> 02:58:36.940]   had to die.
[02:58:36.940 --> 02:58:37.940]   Maybe it was you.
[02:58:37.940 --> 02:58:38.940]   - Yeah.
[02:58:38.940 --> 02:58:39.940]   - Yeah.
[02:58:39.940 --> 02:58:40.940]   So, yeah, sorry.
[02:58:40.940 --> 02:58:41.940]   - So we were talking about life.
[02:58:41.940 --> 02:58:42.940]   - Yeah.
[02:58:42.940 --> 02:58:48.420]   - And I guess a lot of murder had to occur.
[02:58:48.420 --> 02:58:52.220]   - Right, so selection means things had to be weeded out, right?
[02:58:52.220 --> 02:58:53.220]   So--
[02:58:53.220 --> 02:58:54.220]   - Well, we can celebrate that.
[02:58:54.220 --> 02:58:55.660]   Death makes way for the two of us.
[02:58:55.660 --> 02:59:00.140]   - Yeah, I mean, and also, you know, one of the most interesting features of major extinction
[02:59:00.140 --> 02:59:06.540]   events in the history of our planet is how much novelty emerged immediately after, right?
[02:59:06.540 --> 02:59:09.700]   And of course, a lot of people make arguments, we wouldn't be here if the dinosaurs didn't
[02:59:09.700 --> 02:59:10.900]   go extinct.
[02:59:10.900 --> 02:59:16.700]   So in some ways, we can attribute our existence to all of that.
[02:59:16.700 --> 02:59:22.740]   But I guess I was just wondering and sort of like, if I was gonna build a universe myself,
[02:59:22.740 --> 02:59:25.180]   in the most optimistic way, would I retain that feature?
[02:59:25.180 --> 02:59:26.180]   But it does seem to be a universe.
[02:59:26.180 --> 02:59:27.180]   - I think you have to.
[02:59:27.180 --> 02:59:31.300]   I mean, I think we're probably being over-anthropomorphizing.
[02:59:31.300 --> 02:59:34.700]   I remember watching the blue, I think it was the Blue Planet, David Attenborough was showing
[02:59:34.700 --> 02:59:38.780]   these seals and because of climate change, some seals were falling off a cliff and how
[02:59:38.780 --> 02:59:39.780]   tragic that was.
[02:59:39.780 --> 02:59:43.420]   I was like, I was saying to my son, that's pretty cool.
[02:59:43.420 --> 02:59:46.580]   Look at those ones down there, they've obviously got some kind of mutations, some and they're
[02:59:46.580 --> 02:59:48.140]   not doing that daft thing.
[02:59:48.140 --> 02:59:51.380]   And so that poor gene will be weeded out.
[02:59:51.380 --> 02:59:54.900]   Of course, at the individual level, it looks tragic.
[02:59:54.900 --> 02:59:58.000]   And of course, as human beings, we have the ability to abstract and we empathize.
[02:59:58.000 --> 03:00:01.580]   We don't wanna cause suffering on other human beings and we should retain that.
[03:00:01.580 --> 03:00:08.380]   But we shouldn't look back in time and say, you know, how many butterflies had to die?
[03:00:08.380 --> 03:00:14.860]   I remember, if you think about the caterpillar become the chrysalis and then the butterfly
[03:00:14.860 --> 03:00:20.420]   getting out, how many, if that suffering, we call it suffering, if that process of pruning
[03:00:20.420 --> 03:00:22.980]   had not occurred, we have no butterflies.
[03:00:22.980 --> 03:00:26.820]   So none of the butterfly beauty in the world without all that pruning.
[03:00:26.820 --> 03:00:33.860]   So pruning is required, but we shouldn't anthropomorphize and feel sorry for the biological entities
[03:00:33.860 --> 03:00:37.020]   because that seems to be a backwards way of looking at it.
[03:00:37.020 --> 03:00:41.940]   What we should do is project forward and maybe think about what values we have across our
[03:00:41.940 --> 03:00:45.100]   species and our ecosystem and our fellow human beings.
[03:00:45.100 --> 03:00:51.420]   You know, now that we know that animals suffer at some level, think about humane farming.
[03:00:51.420 --> 03:00:57.340]   When we find that plants can in fact are conscious and can think and have pain, then we'll do
[03:00:57.340 --> 03:00:58.780]   humane gardening.
[03:00:58.780 --> 03:01:01.140]   Until that point, we won't do it, right?
[03:01:01.140 --> 03:01:02.700]   I like this.
[03:01:03.260 --> 03:01:07.540]   Famous chemist endorses the majestic nature of murder.
[03:01:07.540 --> 03:01:08.540]   That's the title.
[03:01:08.540 --> 03:01:09.540]   I didn't say that, but okay.
[03:01:09.540 --> 03:01:10.540]   Well, I just inserted it.
[03:01:10.540 --> 03:01:11.540]   I have a hard time with it, though.
[03:01:11.540 --> 03:01:17.860]   I think the way you put it, it's kind of...
[03:01:17.860 --> 03:01:23.260]   But it's the reality of, it is beautiful.
[03:01:23.260 --> 03:01:27.100]   You know, there's an Instagram account called Nature is Metal.
[03:01:27.100 --> 03:01:31.940]   And I keep following it, unfollowing it because I can't handle it for prolonged periods of
[03:01:31.940 --> 03:01:32.940]   time.
[03:01:32.940 --> 03:01:33.940]   We evolve together, you die alone.
[03:01:33.940 --> 03:01:34.940]   Yeah.
[03:01:34.940 --> 03:01:37.940]   We evolve together, but you die alone.
[03:01:37.940 --> 03:01:39.500]   You live alone, too.
[03:01:39.500 --> 03:01:40.780]   It's the Gatsby thing.
[03:01:40.780 --> 03:01:41.900]   I don't know.
[03:01:41.900 --> 03:01:42.900]   We evolve together.
[03:01:42.900 --> 03:01:43.900]   Where's the together?
[03:01:43.900 --> 03:01:45.700]   The together is the murder and the sex.
[03:01:45.700 --> 03:01:46.700]   The population.
[03:01:46.700 --> 03:01:47.700]   Interaction and the population.
[03:01:47.700 --> 03:01:48.700]   Sex and murder.
[03:01:48.700 --> 03:01:52.940]   My romantic vision of it to try to make me happy, Sarah, instead of sad, Sarah, I talk
[03:01:52.940 --> 03:02:00.100]   in third person when I think very abstractly, sorry, is, you know, like this whole, like
[03:02:00.100 --> 03:02:03.100]   certain things can coexist, so the universe is trying to maximize existence, but there's
[03:02:03.100 --> 03:02:09.940]   some things that just aren't the most productive trajectory together, but it doesn't mean that
[03:02:09.940 --> 03:02:15.660]   they don't exist on another timeline or another chain somewhere else.
[03:02:15.660 --> 03:02:20.660]   Maybe you would call that then some kind of multiverse or things, but what am I saying?
[03:02:20.660 --> 03:02:22.780]   I think you can't, you can't go down a level.
[03:02:22.780 --> 03:02:25.700]   I'm just making stuff up to make myself feel better.
[03:02:25.700 --> 03:02:26.700]   I don't understand.
[03:02:26.700 --> 03:02:27.700]   It's illogical.
[03:02:27.700 --> 03:02:28.700]   No, I know, I know.
[03:02:28.700 --> 03:02:34.100]   If you look at bacteria, if you look at virus, I mean, just the number of organisms that
[03:02:34.100 --> 03:02:37.340]   are constantly, like looking at bacteria, they're just dying nonstop.
[03:02:37.340 --> 03:02:38.340]   It's like a slaughter.
[03:02:38.340 --> 03:02:39.340]   Right.
[03:02:39.340 --> 03:02:41.020]   Well, and this goes back to the conversation about God.
[03:02:41.020 --> 03:02:44.780]   I mean, like there's the whole thing about like, why is the universe enables suffering?
[03:02:44.780 --> 03:02:46.580]   Individuals don't exist, right?
[03:02:46.580 --> 03:02:51.900]   So for this, I think if you think about life as an entity on earth, right?
[03:02:51.900 --> 03:02:53.700]   Let's just go back a second.
[03:02:53.700 --> 03:02:55.700]   I mean, I like to, I'll be ludicrous for a second.
[03:02:55.700 --> 03:02:56.700]   I don't exist.
[03:02:56.700 --> 03:02:58.420]   Evolution exists, right?
[03:02:58.420 --> 03:03:02.580]   But the actions you do, the product of evolution exists, right?
[03:03:02.580 --> 03:03:07.600]   The objects you create exist quantitatively in the real world.
[03:03:07.600 --> 03:03:12.500]   If you then understand life on earth or alien life or any life in the universe as this integrated
[03:03:12.500 --> 03:03:16.420]   entity where you need, you need cells in your body to die.
[03:03:16.420 --> 03:03:20.300]   Otherwise you'd just get really big and you wouldn't be able to walk around.
[03:03:20.300 --> 03:03:21.300]   Right?
[03:03:21.300 --> 03:03:23.300]   So you know, you do.
[03:03:23.300 --> 03:03:24.300]   Yeah.
[03:03:24.300 --> 03:03:25.300]   Yeah.
[03:03:25.300 --> 03:03:26.300]   Yeah.
[03:03:26.300 --> 03:03:27.300]   So I think-
[03:03:27.300 --> 03:03:29.420]   It's the patterns that persist, not the physical thing.
[03:03:29.420 --> 03:03:34.260]   And of course we, you know, we have, we have, we place immense values on fellow human beings
[03:03:34.260 --> 03:03:38.940]   and I'm majestic professor does like other individual human beings.
[03:03:38.940 --> 03:03:40.660]   Now you're talking in third person too.
[03:03:40.660 --> 03:03:41.860]   I know, it happens, right?
[03:03:41.860 --> 03:03:48.180]   So death, would you say, I mean, because you said evolution is a fundamental part of life.
[03:03:48.180 --> 03:03:50.940]   So death is a fundamental part of life.
[03:03:50.940 --> 03:03:51.940]   Yeah.
[03:03:51.940 --> 03:03:54.300]   It might right now, it might not be in the future.
[03:03:54.300 --> 03:03:58.700]   We might hack some aspects of death and we'll evolve in different ways.
[03:03:58.700 --> 03:04:05.100]   But isn't there, I think Sarah mentioned like this life density.
[03:04:05.100 --> 03:04:06.220]   Can't that become a problem?
[03:04:06.220 --> 03:04:11.380]   Like too much, too much bureaucracy, too much baggage builds up.
[03:04:11.380 --> 03:04:12.860]   Like you need to keep erasing stuff.
[03:04:12.860 --> 03:04:15.580]   I think it's okay that we dissipate.
[03:04:15.580 --> 03:04:16.580]   Like I don't think of it like-
[03:04:16.580 --> 03:04:17.580]   Dissipate, yes.
[03:04:17.580 --> 03:04:23.220]   No, but I mean like, like we're so fixated on ourselves as individuals and agents.
[03:04:23.220 --> 03:04:27.500]   And we were talking about this last night actually over dinner, but like, you know,
[03:04:27.500 --> 03:04:30.300]   an individual persists for a certain amount of time.
[03:04:30.300 --> 03:04:35.260]   But what you want to do, like if you're really concerned with immortality is not to live indefinitely
[03:04:35.260 --> 03:04:37.820]   as an individual, but maximize your causal impact.
[03:04:37.820 --> 03:04:41.300]   So like, what are the traces of you that are left?
[03:04:41.300 --> 03:04:45.980]   And you're still a real, I always think of Einstein, like for a period of time, he was
[03:04:45.980 --> 03:04:48.860]   a real physical thing where you identify as a human.
[03:04:48.860 --> 03:04:53.180]   And now we just see echoes of that human in all of the ways that we talk about his, you
[03:04:53.180 --> 03:04:55.940]   know, causal impact or frankly, right, is another great example.
[03:04:55.940 --> 03:04:58.220]   How many Easter eggs could you leave in the future?
[03:04:58.220 --> 03:04:59.940]   It's like, oh, I got you.
[03:04:59.940 --> 03:05:05.380]   So I guess the question is how much do you want to control the localization of a certain
[03:05:05.380 --> 03:05:12.580]   features of say a packet of propagating information we might call a person and keep them localized
[03:05:12.580 --> 03:05:14.580]   to one individual physical structure?
[03:05:14.580 --> 03:05:18.580]   Do you want to, you know, is there a time when that just becomes a dissipated feature
[03:05:18.580 --> 03:05:22.100]   of the society that it once existed in?
[03:05:22.100 --> 03:05:26.060]   And I'm okay with the dissipated feature because I just think that makes more room for more
[03:05:26.060 --> 03:05:28.300]   creativity in the future.
[03:05:28.300 --> 03:05:31.620]   - So you mentioned engineering life in the lab.
[03:05:31.620 --> 03:05:36.100]   Let me take you to computer science world.
[03:05:36.100 --> 03:05:38.180]   What about robots?
[03:05:38.180 --> 03:05:47.660]   So is it possible to engineer, 'cause you're really talking about like engineering life
[03:05:47.660 --> 03:05:54.860]   at the chemistry level, but do you think it's possible to engineer life at the like humanoid
[03:05:54.860 --> 03:05:57.700]   level, at the dog level?
[03:05:57.700 --> 03:06:05.020]   Or is that, like at which level can we instill the magic of life into inanimate stuff?
[03:06:05.020 --> 03:06:07.220]   - No, I think you could do it at every level.
[03:06:07.220 --> 03:06:12.900]   I just think that we're particularly interested in chemistry because it's the origin life
[03:06:12.900 --> 03:06:16.980]   transition that presumably, or at least this is how I feel about it, is gonna give you
[03:06:16.980 --> 03:06:22.900]   the most interesting or deepest insights into the physics.
[03:06:22.900 --> 03:06:26.700]   But presumably everything that we do and build is an example of life.
[03:06:26.700 --> 03:06:30.740]   And the question is just how much do you want to take from things that we have now and put
[03:06:30.740 --> 03:06:35.300]   them into like examples of life and copy them into machines?
[03:06:35.300 --> 03:06:38.340]   - I saw that there was this tweet again.
[03:06:38.340 --> 03:06:42.020]   I think you were at the Mars conference and you were hanging out with a humanoid robot.
[03:06:42.020 --> 03:06:43.020]   - Yes.
[03:06:43.020 --> 03:06:44.020]   That was a fun time.
[03:06:44.020 --> 03:06:46.780]   - Making lots of new friends at Mars 2020.
[03:06:46.780 --> 03:06:51.940]   Did you guys color match ahead of time with the robot or did that accidentally happen?
[03:06:51.940 --> 03:06:54.180]   - Accidentally, I went up and I wanted to say hi.
[03:06:54.180 --> 03:06:57.220]   - Torquoise, would that be the correct name for the color?
[03:06:57.220 --> 03:06:58.220]   - I think so.
[03:06:58.220 --> 03:07:00.340]   We didn't color coordinate our outfits.
[03:07:00.340 --> 03:07:02.700]   - Well, you didn't, maybe the robot did.
[03:07:02.700 --> 03:07:03.700]   - The robot probably did.
[03:07:03.700 --> 03:07:04.700]   Much more stylish.
[03:07:04.700 --> 03:07:10.260]   - So for people who are just listening, there's a picture of Sarah standing next to a humanoid
[03:07:10.260 --> 03:07:11.260]   robot.
[03:07:11.260 --> 03:07:15.140]   I guess you like them with a small head and perfect vision.
[03:07:15.140 --> 03:07:16.140]   - Actually no, I just-
[03:07:16.140 --> 03:07:17.820]   - I did that perfectly.
[03:07:17.820 --> 03:07:18.820]   There's a LIDAR.
[03:07:18.820 --> 03:07:22.300]   - No, I mean, I think I was just deeply interested because-
[03:07:22.300 --> 03:07:24.140]   - Sorry to interrupt.
[03:07:24.140 --> 03:07:25.140]   Was it manual control?
[03:07:25.140 --> 03:07:27.260]   Was it actually stabilizing itself?
[03:07:27.260 --> 03:07:29.100]   - Oh no, it was walking around.
[03:07:29.100 --> 03:07:30.100]   - Oh, nice.
[03:07:30.100 --> 03:07:31.100]   - Yeah.
[03:07:31.100 --> 03:07:32.100]   - Nice.
[03:07:32.100 --> 03:07:33.100]   - It was pretty impressive.
[03:07:33.100 --> 03:07:36.460]   Actually, there's some videos online of Jeff Bezos walking with one of those across the
[03:07:36.460 --> 03:07:38.660]   lawn nearby there.
[03:07:38.660 --> 03:07:40.660]   - It's great.
[03:07:40.660 --> 03:07:41.660]   - Yeah.
[03:07:41.660 --> 03:07:43.660]   - I wasn't invited.
[03:07:43.660 --> 03:07:48.660]   - Yeah, but there you go.
[03:07:48.660 --> 03:07:49.660]   See?
[03:07:49.660 --> 03:07:52.660]   - That's incredible, isn't it?
[03:07:52.660 --> 03:07:53.660]   - Yeah.
[03:07:53.660 --> 03:07:57.780]   - So you look at the walking robot, where did the idea for walking come from?
[03:07:57.780 --> 03:08:00.180]   It was invented by evolution, right?
[03:08:00.180 --> 03:08:03.580]   And us as human beings, able to conceptualize and design and engineer.
[03:08:03.580 --> 03:08:04.580]   The causal chain.
[03:08:04.580 --> 03:08:07.260]   So that robot is evidence of life.
[03:08:07.260 --> 03:08:14.680]   And so I think what's going to happen is we want to find where the spark comes from mechanistically.
[03:08:14.680 --> 03:08:17.860]   How can you literally go from sand to cells?
[03:08:17.860 --> 03:08:20.700]   So that's the first transition that I think, you know, there are a number of problems we
[03:08:20.700 --> 03:08:21.700]   want to do.
[03:08:21.700 --> 03:08:24.380]   Make life in a lab, great.
[03:08:24.380 --> 03:08:28.100]   Then we want to make life in a lab and want to suddenly start to make intelligent life
[03:08:28.100 --> 03:08:31.580]   or life that can start to solve abstract problems.
[03:08:31.580 --> 03:08:34.940]   And then we want to make life that is conscious.
[03:08:34.940 --> 03:08:36.100]   Okay?
[03:08:36.100 --> 03:08:37.100]   - In that order?
[03:08:37.100 --> 03:08:38.740]   - I think it has to happen that order.
[03:08:38.740 --> 03:08:41.620]   You know, getting towards this artificial general intelligence.
[03:08:41.620 --> 03:08:45.740]   I think that artificial general intelligence can't exist in a vacuum.
[03:08:45.740 --> 03:08:49.780]   It has to have a causal chain all the way back to Luca, right?
[03:08:49.780 --> 03:08:57.020]   And so the question I think, I really like the question is to say, what are we, how is,
[03:08:57.020 --> 03:08:59.140]   what is our pursuit of more and more lifelike?
[03:08:59.140 --> 03:09:02.620]   I know you want to, you like robots, you want to project into them, you want to interact
[03:09:02.620 --> 03:09:03.620]   with them.
[03:09:03.620 --> 03:09:07.620]   You, I think you would want, if you have a robot dog and the robot dog does everything
[03:09:07.620 --> 03:09:10.820]   expected of a normal dog and you can't tell the difference, you're not really going to
[03:09:10.820 --> 03:09:14.420]   ask the question anymore if it's a real dog or not, or you've got a personality, you're
[03:09:14.420 --> 03:09:16.100]   interacting with it.
[03:09:16.100 --> 03:09:21.300]   And so I think what would be interesting would be to kind of understand the computational
[03:09:21.300 --> 03:09:25.100]   architecture, how that evolves, because you could then, you know, teleport the personality
[03:09:25.100 --> 03:09:29.620]   from one object to the other and say, right, is it act the same?
[03:09:29.620 --> 03:09:35.700]   And I think that as we go along, we're going to get better and better at integrating our
[03:09:35.700 --> 03:09:37.540]   consciousness into machines.
[03:09:37.540 --> 03:09:43.060]   - Well, let me ask you that question just to linger on it.
[03:09:43.060 --> 03:09:49.740]   I would call that a living conscious thing, potentially, I as a human allegedly, but would
[03:09:49.740 --> 03:09:53.820]   you as a person trying to define life?
[03:09:53.820 --> 03:09:57.060]   If you pass the Turing test, are you a life form?
[03:09:57.060 --> 03:10:03.060]   - One of the reasons I walked up to the robot was because I wanted to meet the robot.
[03:10:03.060 --> 03:10:10.780]   So I, it felt like I was, I base a lot of my interaction with reality on emotion and
[03:10:10.780 --> 03:10:13.580]   feeling, but like, how do you feel about an interaction?
[03:10:13.580 --> 03:10:16.780]   And I always love your point about like, is it enough to have that shared experience with
[03:10:16.780 --> 03:10:17.780]   a robot?
[03:10:17.780 --> 03:10:21.060]   So walking up to it, does it feel like you're interacting with a living thing?
[03:10:21.060 --> 03:10:22.860]   And it did to an extent.
[03:10:22.860 --> 03:10:27.060]   But in some degrees, it feels like you're interacting with a baby living thing.
[03:10:27.060 --> 03:10:30.820]   So I think our relationship with technology in particular, the robots we build is really
[03:10:30.820 --> 03:10:36.260]   interesting because basically they exist as objects in our future in some sense, like
[03:10:36.260 --> 03:10:41.060]   we're a much older evolutionary lineage than robots are, but we're all part of the same
[03:10:41.060 --> 03:10:42.700]   causal chain.
[03:10:42.700 --> 03:10:47.460]   And presumably, you know, they're kind of in their infancy.
[03:10:47.460 --> 03:10:50.940]   So it's almost like you're looking at the future of life when you're looking at them,
[03:10:50.940 --> 03:10:56.900]   but it hasn't really become life in a full manifestation of whatever it is that they're
[03:10:56.900 --> 03:11:00.060]   going to become.
[03:11:00.060 --> 03:11:04.380]   And you know, the more, the example of the walking robot was super interesting, but they
[03:11:04.380 --> 03:11:08.600]   also had a dolphin that they put in the pool at the cocktail party at Mars.
[03:11:08.600 --> 03:11:11.660]   And it looked just like a real dolphin swimming in the pool.
[03:11:11.660 --> 03:11:18.660]   And you know, it's in this kind of uncanny valley because, and I was having this conversation
[03:11:18.660 --> 03:11:22.500]   with a gentleman named Mutu who was super perceptive, but he was basically saying like
[03:11:22.500 --> 03:11:25.940]   it made him feel really uncomfortable.
[03:11:25.940 --> 03:11:26.940]   And I think-
[03:11:26.940 --> 03:11:27.940]   - The dolphin.
[03:11:27.940 --> 03:11:29.620]   - Yeah, and I think a lot of people would have that response.
[03:11:29.620 --> 03:11:33.740]   And I guess my point about it is, it is kind of interesting because you're basically trying
[03:11:33.740 --> 03:11:38.420]   to make a thing that you think is non-living mimic a living thing.
[03:11:38.420 --> 03:11:42.860]   And so the thought experiment I would want to run in that case is imagine we replaced
[03:11:42.860 --> 03:11:47.220]   every living thing on earth with a robot equivalent, like all the dolphins and things.
[03:11:47.220 --> 03:11:51.020]   And in some sense, then you're making, if you think that the robots aren't experiencing
[03:11:51.020 --> 03:11:56.900]   reality for example, in the way that a biologically evolved thing would, you're basically making
[03:11:56.900 --> 03:12:00.700]   the philosophical zombie argument become real.
[03:12:00.700 --> 03:12:05.980]   And basically building reality into a simulation because you've made everything quote unquote
[03:12:05.980 --> 03:12:07.420]   fake in some sense.
[03:12:07.420 --> 03:12:11.020]   You've replaced everything with a physical simulation of it.
[03:12:11.020 --> 03:12:20.260]   - So as opposed to being excited by the possibility of creating something new, you're terrified
[03:12:20.260 --> 03:12:22.380]   of humans being replaced.
[03:12:22.380 --> 03:12:26.940]   - I was just trying to run like what would be the absolute thought experiment, but I
[03:12:26.940 --> 03:12:29.460]   don't think that scenario would actually play out.
[03:12:29.460 --> 03:12:34.300]   I guess what I think is weird for why we feel this kind of uncanny valley interacting with
[03:12:34.300 --> 03:12:38.500]   something like the robot dolphin is we're looking at an object we know is kind of in
[03:12:38.500 --> 03:12:43.020]   the future in the sense of like if everything's ordered in time, but it's borrowing from a
[03:12:43.020 --> 03:12:45.380]   structure that we have common history with.
[03:12:45.380 --> 03:12:50.700]   And it's basically copying in a kind of superficial way things from one part of the causal chain
[03:12:50.700 --> 03:12:51.700]   to another.
[03:12:51.700 --> 03:12:52.700]   Yeah.
[03:12:52.700 --> 03:12:54.980]   - Well, that's a video.
[03:12:54.980 --> 03:12:57.700]   - I never believed it was real.
[03:12:57.700 --> 03:13:00.700]   They look so real.
[03:13:00.700 --> 03:13:04.380]   And obviously the technology was developed for movies.
[03:13:04.380 --> 03:13:08.220]   - But I think we're confusing emotional response and understanding the causal chain of how
[03:13:08.220 --> 03:13:09.740]   we got there, right?
[03:13:09.740 --> 03:13:15.060]   Because the philosophical zombie argument thinks about objects just appearing, right?
[03:13:15.060 --> 03:13:19.460]   That you're facsimiled in some way, whereas there is the causal, the chain of events that
[03:13:19.460 --> 03:13:22.300]   caused the dolphin to be built went for a human being.
[03:13:22.300 --> 03:13:26.100]   - Yeah, would a philosophical zombie still have a high assembly index?
[03:13:26.100 --> 03:13:27.100]   - Yeah.
[03:13:27.100 --> 03:13:31.940]   Because it can't be, philosophical zombies can't like Boltzmann brains just can't appear
[03:13:31.940 --> 03:13:32.940]   out of nowhere.
[03:13:32.940 --> 03:13:36.060]   - Well, I guess my question would be in that scenario where you built all the robots and
[03:13:36.060 --> 03:13:39.700]   replaced everything on earth with robots, would the biosphere be as creative under that
[03:13:39.700 --> 03:13:40.700]   scenario or not?
[03:13:40.700 --> 03:13:41.700]   - Yeah.
[03:13:41.700 --> 03:13:45.500]   - And so are there quantitative differences you would notice over time?
[03:13:45.500 --> 03:13:47.460]   - And it's not obvious either way, right?
[03:13:47.460 --> 03:13:51.380]   - It's not obvious right now because we don't really, we don't understand, we haven't built
[03:13:51.380 --> 03:13:53.300]   into machines how we work.
[03:13:53.300 --> 03:13:58.620]   - So that's I think one of the big missing things that I think that we're both looking
[03:13:58.620 --> 03:13:59.620]   for, right?
[03:13:59.620 --> 03:14:01.180]   - This is a robot, it's a cute robot.
[03:14:01.180 --> 03:14:05.500]   - But the point Sarah is that the biosphere won't be as creative if you did it right now.
[03:14:05.500 --> 03:14:06.500]   - No, of course.
[03:14:06.500 --> 03:14:07.860]   I think that's why people don't like it.
[03:14:07.860 --> 03:14:17.220]   - But in the future, we will be able to solve the problem of origin of life, intelligence
[03:14:17.220 --> 03:14:21.200]   and consciousness because they exist in physical substrates.
[03:14:21.200 --> 03:14:26.380]   We just don't understand enough about the material substrate and the causal chain.
[03:14:26.380 --> 03:14:30.180]   But I'm very confident we will get to an AGI, but it won't be what people think.
[03:14:30.180 --> 03:14:34.140]   It won't be, solution won't be a, we'll get fooled a lot.
[03:14:34.140 --> 03:14:40.060]   And so GPT-3 is getting better at fooling us and GPT-153 might really fool us, but it
[03:14:40.060 --> 03:14:42.700]   won't have the magic we're looking for.
[03:14:42.700 --> 03:14:48.900]   It won't be a creative, but it will help us understand the differences between what we're
[03:14:48.900 --> 03:14:52.140]   - Really though, because isn't that what love is?
[03:14:52.140 --> 03:14:53.140]   Being fooled.
[03:14:53.140 --> 03:15:00.140]   Like what, why, why are you not giving much value to the emotional connection with objects,
[03:15:00.140 --> 03:15:02.340]   with robots, with humans?
[03:15:02.340 --> 03:15:09.500]   Emotion is that thing which happens when your expectation function is dashed and something
[03:15:09.500 --> 03:15:10.500]   else happens.
[03:15:10.500 --> 03:15:11.500]   Right?
[03:15:11.500 --> 03:15:13.060]   I mean, that's what emotion is.
[03:15:13.060 --> 03:15:14.900]   - Is that what love is too?
[03:15:14.900 --> 03:15:15.900]   - Yeah.
[03:15:15.900 --> 03:15:18.700]   - You were expecting one thing and something else happened.
[03:15:18.700 --> 03:15:19.700]   - Yeah.
[03:15:19.700 --> 03:15:20.700]   - I don't know.
[03:15:20.700 --> 03:15:21.700]   - I don't think that's true either.
[03:15:21.700 --> 03:15:22.700]   - Well, what is it then?
[03:15:22.700 --> 03:15:25.700]   I think no, emotion, look, I'm sorry, emotion is that, but that's what we're-
[03:15:25.700 --> 03:15:27.700]   - No, I think love is just fulfilling your purpose.
[03:15:27.700 --> 03:15:30.140]   - No, but I mean, look, look.
[03:15:30.140 --> 03:15:31.140]   - Like whatever that means.
[03:15:31.140 --> 03:15:32.140]   - I mean, really, so, okay.
[03:15:32.140 --> 03:15:33.140]   - But when are you happiest?
[03:15:33.140 --> 03:15:34.140]   It's like when you're-
[03:15:34.140 --> 03:15:35.140]   - All right, all right, all right.
[03:15:35.140 --> 03:15:36.140]   Let me go back.
[03:15:36.140 --> 03:15:37.140]   If you want me to define-
[03:15:37.140 --> 03:15:38.140]   - Follow your bliss.
[03:15:38.140 --> 03:15:39.140]   - Let me define love quickly.
[03:15:39.140 --> 03:15:40.140]   - Okay, go for it.
[03:15:40.140 --> 03:15:42.140]   - In terms of assembly space, right?
[03:15:42.140 --> 03:15:43.140]   - Excellent.
[03:15:43.140 --> 03:15:45.140]   - I didn't think I'd be doing this today.
[03:15:45.140 --> 03:15:49.140]   - I can't wait till assembly theory 101 is taught and the second lecture is assembly
[03:15:49.140 --> 03:15:50.140]   theory of love.
[03:15:50.140 --> 03:15:53.740]   - No, no, but look, well, but actually, but look, but-
[03:15:53.740 --> 03:15:54.740]   - It's being surprised.
[03:15:54.740 --> 03:15:55.740]   The expectation is being broken.
[03:15:55.740 --> 03:15:56.740]   - I'm just, I'm not-
[03:15:56.740 --> 03:15:57.740]   - No, go for it.
[03:15:57.740 --> 03:15:58.740]   I want to hear you-
[03:15:58.740 --> 03:15:59.740]   - I'm not an emotional being.
[03:15:59.740 --> 03:16:04.140]   But I would say, so let's talk, so we'll talk about emotional, but love is more complex.
[03:16:04.140 --> 03:16:07.740]   Love is a very complex set of emotions together and logical stuff.
[03:16:07.740 --> 03:16:12.620]   But if you've got this thing, this person that's on this causal chain that has this
[03:16:12.620 --> 03:16:18.500]   empathy for this other thing, love is being able to project ahead in your assembly space
[03:16:18.500 --> 03:16:23.940]   and work out what the person you're in love with has a need for and to do that for them
[03:16:23.940 --> 03:16:26.660]   without selflessly, right?
[03:16:26.660 --> 03:16:29.620]   Because you can project ahead what they're going to need and they are there and maybe
[03:16:29.620 --> 03:16:33.020]   you can see someone who's going to fall over and you catch them before they fall over.
[03:16:33.020 --> 03:16:36.980]   Or maybe you can anticipate that someone's going to be hungry and without helping you,
[03:16:36.980 --> 03:16:37.980]   you just help them.
[03:16:37.980 --> 03:16:38.980]   That's what love is.
[03:16:38.980 --> 03:16:39.980]   - That just sounds like empathy.
[03:16:39.980 --> 03:16:42.100]   - But it's more complex than that, right?
[03:16:42.100 --> 03:16:43.100]   It's more complex.
[03:16:43.100 --> 03:16:48.220]   It's more about not just empathy, it's understanding, it's about kind of sharing that experience.
[03:16:48.220 --> 03:16:49.700]   - You have an expression of love though.
[03:16:49.700 --> 03:16:51.340]   That's not what it's like to feel love.
[03:16:51.340 --> 03:16:56.740]   Like feeling love is like, I think it's like when you're aligned with things that you feel
[03:16:56.740 --> 03:17:00.660]   like are your purpose or your reason for existing.
[03:17:00.660 --> 03:17:07.780]   - So if you have those feelings towards a robot, why is that robot, I mean, because
[03:17:07.780 --> 03:17:12.460]   you said like the AGI, we'll build an AGI, but there'll be a fundamental difference in
[03:17:12.460 --> 03:17:13.460]   AGI and human.
[03:17:13.460 --> 03:17:15.140]   - I don't think we'll build it, it's going to merge from our technology.
[03:17:15.140 --> 03:17:16.980]   - I think you guys are all arguing the same thing.
[03:17:16.980 --> 03:17:23.620]   I just said that GPT, we do not correctly capture the causal chain that we have.
[03:17:23.620 --> 03:17:24.620]   - Within GPT.
[03:17:24.620 --> 03:17:25.620]   - Yeah, within AI.
[03:17:25.620 --> 03:17:33.980]   - Don't you think it captures, because GPT-3 is fundamentally trained on a corpus of knowledge,
[03:17:33.980 --> 03:17:36.020]   you know, like the internet.
[03:17:36.020 --> 03:17:39.700]   Don't you think it gets better and better and better at capturing the memory of all
[03:17:39.700 --> 03:17:40.700]   of those?
[03:17:40.700 --> 03:17:42.140]   - It will be better at fooling you.
[03:17:42.140 --> 03:17:44.740]   And at some point you won't care.
[03:17:44.740 --> 03:17:48.500]   But when it comes, my guess, this is a quick, this is what I was getting to right before
[03:17:48.500 --> 03:17:51.460]   we got, I got in the love trap.
[03:17:51.460 --> 03:17:52.460]   - Love trap, yeah.
[03:17:52.780 --> 03:17:54.740]   - Lee Cronin in the love trap.
[03:17:54.740 --> 03:17:58.740]   - Sounds like a good band name.
[03:17:58.740 --> 03:18:01.780]   - Sad, okay, sad, assembly space of sad.
[03:18:01.780 --> 03:18:09.020]   No, is that so sure, but I think there are other features that we pull on innovation
[03:18:09.020 --> 03:18:12.620]   that allow us to do more than what we just see in GPT-3.
[03:18:12.620 --> 03:18:14.340]   So if you're being fooled there.
[03:18:14.340 --> 03:18:19.340]   So I think what I mean is human beings have this ability to be surprising and creative,
[03:18:19.340 --> 03:18:28.020]   whereas is it Dali, this thing, or if you take, GPT-3 is not gonna create a new verb.
[03:18:28.020 --> 03:18:29.660]   Shakespeare created new verbs.
[03:18:29.660 --> 03:18:31.900]   You're like, wow.
[03:18:31.900 --> 03:18:36.700]   And that required Shakespeare to think outside of language in a different domain.
[03:18:36.700 --> 03:18:41.100]   So I think having that connections across multiple domains is what you need for AGI.
[03:18:41.100 --> 03:18:49.700]   - Yeah, but I don't know if you need, I don't know if there's any limitations to GPT and
[03:18:49.700 --> 03:18:51.700]   not being able to be cross-domain.
[03:18:51.700 --> 03:19:00.860]   - The number one problem is it's instantiated in a resource-limited substrate in silicon.
[03:19:00.860 --> 03:19:04.900]   The architectures used for training for learning is about fooling.
[03:19:04.900 --> 03:19:07.060]   It's not about understanding.
[03:19:07.060 --> 03:19:12.780]   And I think that there is some understanding that we have that is not yet symbolically
[03:19:12.780 --> 03:19:13.780]   representable.
[03:19:13.780 --> 03:19:19.300]   - Language, learning language and using language seems to be fundamentally about fooling, not
[03:19:19.300 --> 03:19:22.060]   understanding.
[03:19:22.060 --> 03:19:24.180]   Why do you use language exactly?
[03:19:24.180 --> 03:19:27.620]   - I might disagree with that quite fundamentally, actually.
[03:19:27.620 --> 03:19:32.100]   But I don't, I'm not sure I understand how I make a coherent argument for that.
[03:19:32.100 --> 03:19:44.180]   But my feeling is that there is comprehension in reality, in our consciousness below language.
[03:19:44.180 --> 03:19:46.780]   And we use those for language for all sorts of expressions.
[03:19:46.780 --> 03:19:49.140]   And we don't yet understand that there's a gap.
[03:19:49.140 --> 03:19:50.140]   We will get there.
[03:19:50.140 --> 03:19:53.820]   But I'm saying, wouldn't it be interesting, it's a bit like saying, could I facsimile
[03:19:53.820 --> 03:19:58.340]   you or Sarah into a new human being, right?
[03:19:58.340 --> 03:20:04.100]   And let's just say I could copy all your atoms and the positions of all your atoms and electrons
[03:20:04.100 --> 03:20:05.980]   into this other person, they would be you.
[03:20:05.980 --> 03:20:07.460]   The answer is no.
[03:20:07.460 --> 03:20:11.900]   And it's quite easy to show using assembly theory, because actually the feature space
[03:20:11.900 --> 03:20:16.780]   that you have, that graph, the only way to copy you is to create you on that graph.
[03:20:16.780 --> 03:20:20.020]   So everything that's happened to you in your past, we have to have a faithful record for.
[03:20:20.020 --> 03:20:22.540]   If you want another copy of Lex, you have to do the exact thing.
[03:20:22.540 --> 03:20:27.340]   Want another copy of Sarah, want another copy of Lee, the exact past has to be replicated.
[03:20:27.340 --> 03:20:28.940]   - Let me push back on that a little bit.
[03:20:28.940 --> 03:20:35.100]   That's maybe from an assembly theory perspective, but I don't think it's that difficult to recreate
[03:20:35.100 --> 03:20:43.260]   a version of me, like a clone, that would make everybody exactly equally as happy.
[03:20:43.260 --> 03:20:44.860]   Like they wouldn't care which one.
[03:20:44.860 --> 03:20:49.820]   And like there's two of me, and then they get to pick which one, and they'll kill either
[03:20:49.820 --> 03:20:50.980]   one, they'll be fine.
[03:20:50.980 --> 03:20:52.340]   As long as they're forced to kill.
[03:20:52.340 --> 03:20:55.620]   - They'll be fine, but here's what will happen is, let's say we make artificial Lex.
[03:20:55.620 --> 03:20:58.940]   And everyone's like, "Wow, so cool, it looks the same in interact."
[03:20:58.940 --> 03:21:02.060]   Then there'll be this battle of like, "Right, we're gonna tell the difference.
[03:21:02.060 --> 03:21:09.300]   We're gonna basically keep nudging Lex and artificial Lex until we get novelty from one,
[03:21:09.300 --> 03:21:11.140]   and we'll kill the other one."
[03:21:11.140 --> 03:21:12.140]   And I think thank God--
[03:21:12.140 --> 03:21:14.620]   - But you're not, novelty is a fuzzy concept.
[03:21:14.620 --> 03:21:16.580]   That's the whole problem of novelty.
[03:21:16.580 --> 03:21:19.340]   - So I will define novelty, it's not fuzzy.
[03:21:19.340 --> 03:21:28.740]   Novelty is the ability for you to create architectures that are, or create an architecture.
[03:21:28.740 --> 03:21:31.660]   So let's say you've got a corpus of architectures known, you can write down, you've got some
[03:21:31.660 --> 03:21:32.660]   distance measure.
[03:21:32.660 --> 03:21:39.060]   And then I create a new one, and the distance measure's so far away from what you'd expected.
[03:21:39.060 --> 03:21:40.420]   There's no linear algebra gonna get there.
[03:21:40.420 --> 03:21:43.420]   It's like, that is creativity.
[03:21:43.420 --> 03:21:47.220]   And we don't know how to do that yet, on any level.
[03:21:47.220 --> 03:21:50.020]   - Well, I was also thinking about your argument about free will.
[03:21:50.020 --> 03:21:53.660]   You wouldn't be able to know it was, it doesn't work instantaneously.
[03:21:53.660 --> 03:21:57.940]   It's not like a micro level thing, but more a macro level thing over the scale of trajectories
[03:21:57.940 --> 03:21:59.260]   or longer term decisions.
[03:21:59.260 --> 03:22:05.060]   So if you think that the novelty manifests over those longer time scales, it might be
[03:22:05.060 --> 03:22:11.420]   the two Lexes diverge quite a bit over certain time scales of their behavior.
[03:22:11.420 --> 03:22:15.220]   - But nobody would notice the difference.
[03:22:15.220 --> 03:22:17.180]   - They might not.
[03:22:17.180 --> 03:22:20.180]   And the universe, the earth won't notice the difference.
[03:22:20.180 --> 03:22:21.180]   The universe won't notice the difference.
[03:22:21.180 --> 03:22:22.420]   - The universe would notice the difference.
[03:22:22.420 --> 03:22:25.540]   - No, the universe doesn't know about its novelty that's being generated.
[03:22:25.540 --> 03:22:26.940]   That's the whole point of novelty.
[03:22:26.940 --> 03:22:29.420]   - Yeah, but this is what selection is, right?
[03:22:29.420 --> 03:22:34.380]   It's like taking nearly equivalent ones and then deciding, like the universe selects,
[03:22:34.380 --> 03:22:35.380]   right?
[03:22:35.380 --> 03:22:37.980]   So whatever selection is, selects some things to persist in time.
[03:22:37.980 --> 03:22:41.980]   - Yeah, it's gonna select the artificial one, just 'cause it likes that one better.
[03:22:41.980 --> 03:22:43.780]   - But you're mixing up two arguments here.
[03:22:43.780 --> 03:22:44.780]   So let's go back a second.
[03:22:44.780 --> 03:22:45.780]   - What are you basing this argument on, Lex?
[03:22:45.780 --> 03:22:55.940]   - I'm just saying that I kind of don't think, 'cause Lee said that it's not possible, like
[03:22:55.940 --> 03:23:02.340]   if you copy every single molecule in a person's body, that's not going to be the same person.
[03:23:02.340 --> 03:23:06.860]   That they won't have the same assembly index, it won't be the same person.
[03:23:06.860 --> 03:23:13.140]   And I just don't, I think copying, you can compress, not only do I disagree with that,
[03:23:13.140 --> 03:23:17.900]   I think you can even compress a person down to some, where you can fool the universe.
[03:23:17.900 --> 03:23:19.980]   - I'm saying, let me restate it.
[03:23:19.980 --> 03:23:26.780]   It is not possible to copy somebody unless you copy the causal history.
[03:23:26.780 --> 03:23:31.180]   - Also, you can't have two identical, I mean, actually, I really like the idea that everything
[03:23:31.180 --> 03:23:32.180]   in the universe is unique.
[03:23:32.180 --> 03:23:33.740]   So even if there were two Lex's--
[03:23:33.740 --> 03:23:36.940]   - I know you like that idea, 'cause you're human and you think you're unique.
[03:23:36.940 --> 03:23:37.940]   - Yeah, exactly.
[03:23:37.940 --> 03:23:41.780]   But also, I can make a logical argument for it, that even if we could copy all of your
[03:23:41.780 --> 03:23:45.660]   molecules and all their positions, the other you would be there, and you have a different
[03:23:45.660 --> 03:23:46.660]   position in space.
[03:23:46.660 --> 03:23:47.660]   You're distinguishable.
[03:23:47.660 --> 03:23:49.980]   - Yeah, the other thing I was gonna add--
[03:23:49.980 --> 03:23:53.060]   - How unique are you, just by the position in space, really?
[03:23:53.060 --> 03:23:56.340]   - Sure, but then how much does that slight translation of Lex--
[03:23:56.340 --> 03:23:57.340]   - Well, that's not an interesting--
[03:23:57.340 --> 03:23:58.340]   - Affect the future.
[03:23:58.340 --> 03:24:02.540]   - I see, but, no, wait a minute.
[03:24:02.540 --> 03:24:07.460]   Is part of the definition of something being interesting is how much it affects the future?
[03:24:07.460 --> 03:24:08.460]   - Yes.
[03:24:08.460 --> 03:24:09.460]   - Yes.
[03:24:09.460 --> 03:24:10.460]   - But let me come back--
[03:24:10.460 --> 03:24:11.460]   - Don't you agree?
[03:24:11.460 --> 03:24:12.460]   - I disagree.
[03:24:12.460 --> 03:24:13.460]   - One point quickly that you were making.
[03:24:13.460 --> 03:24:15.820]   - Sure, I think I probably agree, yeah.
[03:24:15.820 --> 03:24:16.820]   - There's two Lex's, right?
[03:24:16.820 --> 03:24:22.860]   There's a robot Lex that you just basically, it is a charade, it's a facsimile.
[03:24:22.860 --> 03:24:26.540]   It's just coded to emulate you.
[03:24:26.540 --> 03:24:28.580]   - Are you robot Lex?
[03:24:28.580 --> 03:24:29.580]   - I wouldn't know, right?
[03:24:29.580 --> 03:24:30.580]   - Let's get there.
[03:24:30.580 --> 03:24:31.580]   - That's the point, I wouldn't know.
[03:24:31.580 --> 03:24:32.580]   - But let's get there.
[03:24:32.580 --> 03:24:36.100]   It's a very important point here, because he's ducking and diving between this.
[03:24:36.100 --> 03:24:42.860]   So if I facsimile you into a robot, then your robot might be, would be a representation
[03:24:42.860 --> 03:24:45.740]   of you now, but fundamentally be boring, because you go and have other ideas.
[03:24:45.740 --> 03:24:50.220]   If, however, you built an architecture that itself is capable of generating novelty, you
[03:24:50.220 --> 03:24:55.160]   would diverge in your causal chain, and you'd both be equally interesting to interact with.
[03:24:55.160 --> 03:24:56.380]   We don't know that mechanism.
[03:24:56.380 --> 03:24:58.600]   All I'm trying to say is we don't yet know that mechanism.
[03:24:58.600 --> 03:25:03.740]   We do not know the mechanism that generates novelty, and at the moment, in our AIs, we
[03:25:03.740 --> 03:25:05.360]   are emulating.
[03:25:05.360 --> 03:25:07.320]   We are not generating.
[03:25:07.320 --> 03:25:08.940]   - You don't think we're sneaking up on that?
[03:25:08.940 --> 03:25:09.940]   Do you think it's a fun one?
[03:25:09.940 --> 03:25:10.940]   - No, no.
[03:25:10.940 --> 03:25:15.160]   There is no ghost in the machine, and I want there to be one.
[03:25:15.160 --> 03:25:16.460]   I want the same thing you want.
[03:25:16.460 --> 03:25:17.460]   Sorry, I was interrupted.
[03:25:17.460 --> 03:25:21.660]   - I know you want that as a human, because everything you just said makes you feel more
[03:25:21.660 --> 03:25:22.660]   special than the GPT-2.
[03:25:22.660 --> 03:25:24.420]   - I want to be, no, no, no, screw my specialness.
[03:25:24.420 --> 03:25:26.180]   I just want to be surprised.
[03:25:26.180 --> 03:25:27.180]   If I can--
[03:25:27.180 --> 03:25:28.720]   - You don't think a robot can surprise you?
[03:25:28.720 --> 03:25:36.060]   - If you can produce an algorithm instantiated in a robot to surprise me, I will have one
[03:25:36.060 --> 03:25:37.060]   of those robots.
[03:25:37.060 --> 03:25:39.340]   It'll be brilliant, but it won't surprise me.
[03:25:39.340 --> 03:25:44.620]   - But why is it a problem to think that humans are special?
[03:25:44.620 --> 03:25:46.020]   - Maybe it's not the special, you're right.
[03:25:46.020 --> 03:25:47.980]   It's the better than.
[03:25:47.980 --> 03:25:48.980]   - Yes.
[03:25:48.980 --> 03:25:54.100]   - Because then you start to not recognize the magic in other life forms that you either
[03:25:54.100 --> 03:25:55.980]   have created or you have observed.
[03:25:56.400 --> 03:26:03.540]   I just think there's magic in legged robots moving about, and they are full of surprises.
[03:26:03.540 --> 03:26:04.540]   - Yeah.
[03:26:04.540 --> 03:26:05.540]   - So this--
[03:26:05.540 --> 03:26:06.540]   - And personality.
[03:26:06.540 --> 03:26:07.540]   - Yeah.
[03:26:07.540 --> 03:26:08.540]   So I'm a little--
[03:26:08.540 --> 03:26:11.700]   - I know why you like cellular automata, right?
[03:26:11.700 --> 03:26:18.060]   But the specialness in your robot comes from the roboticist that built it.
[03:26:18.060 --> 03:26:19.060]   - Yeah.
[03:26:19.060 --> 03:26:21.060]   - It's part of the lineage.
[03:26:21.060 --> 03:26:22.380]   - Yeah, and so that's fine.
[03:26:22.380 --> 03:26:23.380]   I'm happy with that.
[03:26:23.380 --> 03:26:26.500]   - That's what I felt like looking at the standing robot was I was looking at four billion years
[03:26:26.500 --> 03:26:27.500]   of evolution.
[03:26:27.500 --> 03:26:28.500]   - Yeah, right.
[03:26:28.500 --> 03:26:29.500]   If it wasn't--
[03:26:29.500 --> 03:26:30.500]   - Yeah.
[03:26:30.500 --> 03:26:31.500]   - So I think I'm happy.
[03:26:31.500 --> 03:26:32.500]   I mean, I'm happy we're gonna coexist.
[03:26:32.500 --> 03:26:33.580]   I'm just saying you're gonna get more excitement.
[03:26:33.580 --> 03:26:36.460]   There's something missing in our understanding of intelligence.
[03:26:36.460 --> 03:26:39.420]   Intelligence isn't just training.
[03:26:39.420 --> 03:26:42.620]   The way the neural network is conceived right now is great, and it's lovely, and it'll be
[03:26:42.620 --> 03:26:44.180]   better, and we'll argue forever.
[03:26:44.180 --> 03:26:48.380]   But you want to know, wouldn't it be great if I said, "Look, I know how to invent an
[03:26:48.380 --> 03:26:51.580]   architecture and I can give it a soul."
[03:26:51.580 --> 03:26:57.780]   And what I mean by a soul is some, I know for real that there is internal reference.
[03:26:57.780 --> 03:27:02.620]   As soon as I, not fake internal reference, and if we could generate that mechanism for
[03:27:02.620 --> 03:27:04.620]   internal reference, that's why our goal direct--
[03:27:04.620 --> 03:27:05.620]   - That's why you have to--
[03:27:05.620 --> 03:27:06.620]   - We can do that.
[03:27:06.620 --> 03:27:07.620]   - Develop a test for goal directness, yeah.
[03:27:07.620 --> 03:27:08.620]   - Get that goal directness.
[03:27:08.620 --> 03:27:13.180]   You would love that robot more than the one that's just made to look like it does because
[03:27:13.180 --> 03:27:18.300]   you'll have more fun with it because you better generate search, other problems, get more
[03:27:18.300 --> 03:27:19.300]   novelty.
[03:27:19.300 --> 03:27:24.700]   You'll fall in love with that robot, for real, but not the one that's faking it.
[03:27:24.700 --> 03:27:26.380]   - What about fake it till you make it?
[03:27:26.380 --> 03:27:35.260]   - Well, I think a lot of people fall in love with fake humans.
[03:27:35.260 --> 03:27:39.660]   It's nice to fall in love with something that's full of novelty, yes.
[03:27:39.660 --> 03:27:44.420]   I could imagine all kinds of robots that I would want to have a close relationship with.
[03:27:44.420 --> 03:27:47.820]   And I don't mean like sexual, I mean like intimacy.
[03:27:47.820 --> 03:27:53.420]   I just don't think that novelty generation is such a special...
[03:27:53.420 --> 03:27:59.580]   Okay, there's like mathematical novelty or something like that, and then there's just
[03:27:59.580 --> 03:28:02.940]   humans being surprised, and I think we're easily surprised.
[03:28:02.940 --> 03:28:03.940]   - That's fine, but that's--
[03:28:03.940 --> 03:28:05.300]   - But you don't think that's a good definition of novelty?
[03:28:05.300 --> 03:28:06.300]   - No, that's good.
[03:28:06.300 --> 03:28:12.820]   I'm happy to be surprised, but not globally surprised 'cause someone else, but I really
[03:28:12.820 --> 03:28:17.780]   want, I was, while I'm a scientist, I really want to be the first to be surprised by something
[03:28:17.780 --> 03:28:23.500]   and the first thing in the universe to create that novelty and to know for sure that that
[03:28:23.500 --> 03:28:26.420]   novelty has never occurred anywhere else.
[03:28:26.420 --> 03:28:27.420]   That's a real buzz, right?
[03:28:27.420 --> 03:28:29.460]   - Is there a way to really know that?
[03:28:29.460 --> 03:28:32.220]   - You have to have a really big look-up table.
[03:28:32.220 --> 03:28:34.460]   You're never gonna know for sure, right?
[03:28:34.460 --> 03:28:39.060]   That's one of the hard things about being a scientist searching for this type of novelty.
[03:28:39.060 --> 03:28:45.100]   Maybe that's why mathematicians love discovery, but actually they are creating, and then when
[03:28:45.100 --> 03:28:51.860]   they create a new mathematical structure that they can then, you can write code to work
[03:28:51.860 --> 03:28:54.940]   out whether that structure exists before.
[03:28:54.940 --> 03:28:58.500]   That's almost why I would love to have been a mathematician from that regard, to invent
[03:28:58.500 --> 03:29:04.300]   new math that really I know pretty much for sure does not exist anywhere else in the universe
[03:29:04.300 --> 03:29:05.300]   'cause it's so contingent.
[03:29:05.300 --> 03:29:09.700]   - Right, but this gets into, you said a few times, and I still really don't understand
[03:29:09.700 --> 03:29:13.900]   how you actually plan to do this, to build an experiment that detects how the universe
[03:29:13.900 --> 03:29:17.180]   is generating novelty or that time is the mechanism.
[03:29:17.180 --> 03:29:21.380]   The problem that we all have, which I think is what Lex is pushing against, is if I build
[03:29:21.380 --> 03:29:27.380]   the experiment, you don't know what you put into it, so you don't know what, unless you
[03:29:27.380 --> 03:29:31.820]   can quantify everything you put in, all of your agency, all the boundary conditions,
[03:29:31.820 --> 03:29:36.300]   you don't know if you somehow biased it in some way.
[03:29:36.300 --> 03:29:40.100]   Is the novelty actually intrinsic to that experiment or to that robot, or is it something
[03:29:40.100 --> 03:29:42.500]   you gave it, but you didn't realize you gave it?
[03:29:42.500 --> 03:29:44.420]   It's gonna asymptote towards that, right?
[03:29:44.420 --> 03:29:49.060]   You're never gonna know for sure, but you can start to take out, you can use good Bayesian
[03:29:49.060 --> 03:29:53.940]   approaches and just keep updating and updating and updating until you point, to all intents
[03:29:53.940 --> 03:29:56.060]   and purposes-- - So you wanna bound on how much novelty
[03:29:56.060 --> 03:29:57.460]   generation could be.
[03:29:57.460 --> 03:29:58.460]   - Yeah.
[03:29:58.460 --> 03:29:59.460]   - Got it.
[03:29:59.460 --> 03:30:04.980]   - So the ability to generate novelty is correlated with high assembly index, with assembly index?
[03:30:04.980 --> 03:30:06.660]   - Yeah, yeah.
[03:30:06.660 --> 03:30:11.620]   - 'Cause the space of possibilities is bigger.
[03:30:11.620 --> 03:30:14.620]   So that's the key.
[03:30:14.620 --> 03:30:18.940]   This could be a good, so I have a running joke of why Lex is single, this could be a
[03:30:18.940 --> 03:30:23.020]   good part four.
[03:30:23.020 --> 03:30:33.660]   So what you're looking for in a robot partner is ability to generate novelty.
[03:30:33.660 --> 03:30:40.740]   And that's, I suppose you would say, it's a good definition of intelligence, too.
[03:30:40.740 --> 03:30:47.740]   - Boy, is novelty a fuzzy concept.
[03:30:47.740 --> 03:30:48.740]   - Is creativity better?
[03:30:48.740 --> 03:30:53.060]   - Yeah, I mean, that's all pretty fuzzy.
[03:30:53.060 --> 03:30:54.660]   - It's kind of the same.
[03:30:54.660 --> 03:30:58.380]   Maybe that's why aliens haven't come yet, is 'cause we're not creating enough novelty.
[03:30:58.380 --> 03:31:01.300]   There's some kind of a hierarchy of novelty in the universe.
[03:31:01.300 --> 03:31:04.220]   - Well, I think novelty is like, things surprise you, right?
[03:31:04.220 --> 03:31:08.020]   So it's a very passive thing, but I guess what I meant by saying creativity is I think
[03:31:08.020 --> 03:31:12.860]   it's much more active, that you think there's a mechanism of the things that exist are generating
[03:31:12.860 --> 03:31:14.220]   the creativity.
[03:31:14.220 --> 03:31:18.260]   Novelty seems to be there's some spontaneous production, it's completely decoupled from
[03:31:18.260 --> 03:31:19.260]   the things that exist.
[03:31:19.260 --> 03:31:20.260]   - No, I understand.
[03:31:20.260 --> 03:31:27.540]   I think creativity is the mechanism, and novelty is the observable.
[03:31:27.540 --> 03:31:32.420]   - Novelty could just be surprise, your model of the world was broken, and not necessarily
[03:31:32.420 --> 03:31:33.500]   in a positive way.
[03:31:33.500 --> 03:31:34.500]   - That's surprise.
[03:31:34.500 --> 03:31:35.500]   So there's three things now.
[03:31:35.500 --> 03:31:36.500]   Let's go back, that's cool.
[03:31:36.500 --> 03:31:41.100]   You've got surprise, which is basically, I mean, I'm surprised all the time 'cause I
[03:31:41.100 --> 03:31:42.540]   don't read very much, I'm pretty dumb.
[03:31:42.540 --> 03:31:44.020]   I was like, "Oh, wow, this is..."
[03:31:44.020 --> 03:31:48.580]   I often used to invent new scientific ideas, and I was really surprised by that, and then
[03:31:48.580 --> 03:31:50.140]   when I look in literature properly, and it's there.
[03:31:50.140 --> 03:31:54.820]   So surprise, that's to the extent that you don't have full information.
[03:31:54.820 --> 03:32:03.740]   Creativity, the act of pushing on that kind of on the causal structure, and novelty, which
[03:32:03.740 --> 03:32:06.620]   is measuring that degree, right?
[03:32:06.620 --> 03:32:09.700]   So I think that's pretty well defined in that regard.
[03:32:09.700 --> 03:32:13.580]   So you want your robot, I mean, and in the end, that's why actually the way the internet
[03:32:13.580 --> 03:32:19.740]   and the printing press share some, I actually think creativity has dropped a bit since the
[03:32:19.740 --> 03:32:23.140]   internet, because everyone's just regurgitating stuff.
[03:32:23.140 --> 03:32:27.380]   But of course, now it's beginning to accelerate again, 'cause everyone's using this tool to
[03:32:27.380 --> 03:32:31.260]   be creative, and boom, it's exploding.
[03:32:31.260 --> 03:32:34.820]   So I think that's what happens when you create these new technologies.
[03:32:34.820 --> 03:32:36.100]   That's really helpful.
[03:32:36.100 --> 03:32:38.980]   There's a difference between novelty and surprise.
[03:32:38.980 --> 03:32:41.060]   I think I was thinking about surprise.
[03:32:41.060 --> 03:32:44.020]   If you give me a toy that surprises me for a bit, that'd be great.
[03:32:44.020 --> 03:32:45.020]   Robot that surprises me.
[03:32:45.020 --> 03:32:46.900]   An experiment that surprises you.
[03:32:46.900 --> 03:32:49.540]   Yeah, I mean, that's why I love doing experiments, 'cause I can't...
[03:32:49.540 --> 03:32:50.540]   It's still exciting.
[03:32:50.540 --> 03:32:53.240]   Surprise is exciting.
[03:32:53.240 --> 03:32:56.580]   Even negative surprise, like some people love drama in relationships.
[03:32:56.580 --> 03:33:01.020]   It's like, "Why the hell, why'd you do this?"
[03:33:01.020 --> 03:33:02.540]   That can be exciting.
[03:33:02.540 --> 03:33:06.980]   I can imagine companies selling updates to their companion robots that just basically
[03:33:06.980 --> 03:33:10.140]   generate negative surprise, just to spice things up a bit.
[03:33:10.140 --> 03:33:12.140]   Yeah, it's the push and pull.
[03:33:12.140 --> 03:33:13.980]   That's one of the components of love.
[03:33:13.980 --> 03:33:16.060]   As you said, love is a complicated thing.
[03:33:16.060 --> 03:33:20.900]   Oh, beauty, I wanted to mention this, 'cause you also tweeted, I think this was Sarah.
[03:33:20.900 --> 03:33:22.140]   No, it might have been Lee.
[03:33:22.140 --> 03:33:23.140]   I don't remember.
[03:33:23.140 --> 03:33:26.860]   But it was a survey published in Nature showing that scientists find...
[03:33:26.860 --> 03:33:28.860]   That was me, yeah.
[03:33:28.860 --> 03:33:30.860]   Anyway, there's a plot.
[03:33:30.860 --> 03:33:37.380]   This is published in Nature of what scientists find beautiful in their work, and it separates
[03:33:37.380 --> 03:33:38.780]   biologists and physicists.
[03:33:38.780 --> 03:33:41.860]   It'd be nice if you showed the full plot.
[03:33:41.860 --> 03:33:47.980]   And there's simplicity, elegance, hidden order, inner logic of systems, symmetry, complexity,
[03:33:47.980 --> 03:33:49.660]   harmony, and so on.
[03:33:49.660 --> 03:33:53.060]   Is there any interesting things that stand out to you?
[03:33:53.060 --> 03:33:57.700]   I think the fact that biologists like complexity and pleasing colors.
[03:33:57.700 --> 03:34:00.820]   Oh, there's pleasing colors on there?
[03:34:00.820 --> 03:34:01.820]   Yeah, yeah, yeah.
[03:34:01.820 --> 03:34:02.820]   Or shapes.
[03:34:02.820 --> 03:34:03.820]   Or shapes.
[03:34:03.820 --> 03:34:08.140]   And then physicists obviously love simplicity above all else.
[03:34:08.140 --> 03:34:09.140]   Simplicity and elegance.
[03:34:09.140 --> 03:34:10.140]   Simplicity, elegance.
[03:34:10.140 --> 03:34:11.140]   They love symmetry.
[03:34:11.140 --> 03:34:15.140]   And then biologists love complexity.
[03:34:15.140 --> 03:34:17.940]   And well, they just love a little bit less.
[03:34:17.940 --> 03:34:20.900]   They love everything a little bit less, but complexity a little bit more.
[03:34:20.900 --> 03:34:21.900]   A little bit more.
[03:34:22.420 --> 03:34:23.420]   That's so interesting.
[03:34:23.420 --> 03:34:25.300]   And pleasing colors or shapes.
[03:34:25.300 --> 03:34:26.300]   Do you think it's a useful...
[03:34:26.300 --> 03:34:29.300]   I forget what your tweet was, that this is missing some of the...
[03:34:29.300 --> 03:34:36.580]   No, I think it's because I think about how explanations become causal to our future.
[03:34:36.580 --> 03:34:44.540]   So I have this whole philosophy that the theories we build and the way we describe reality should
[03:34:44.540 --> 03:34:52.820]   be have the largest breadth of possibilities for the future of what we can accomplish.
[03:34:52.820 --> 03:34:58.180]   So in some sense, it's not like Occam's razor is not for simplicity, it's for optimism or
[03:34:58.180 --> 03:35:00.260]   the kind of future you can build.
[03:35:00.260 --> 03:35:05.460]   And so I think you have to think this way when you're thinking about life and alien
[03:35:05.460 --> 03:35:07.780]   life because ultimately we're trying to build...
[03:35:07.780 --> 03:35:11.140]   I mean, science is just basically our narratives about reality.
[03:35:11.140 --> 03:35:14.060]   And now you're building a narrative that is what we are as physical systems.
[03:35:14.060 --> 03:35:17.380]   It seems to me it needs to be as positive as possible because it's basically going to
[03:35:17.380 --> 03:35:20.180]   shape the future trajectory where we're going.
[03:35:20.180 --> 03:35:26.020]   And we don't use that as a heuristic in theory building because we think theories are about
[03:35:26.020 --> 03:35:28.940]   predicting features of the world, not causing them.
[03:35:28.940 --> 03:35:31.940]   But if you look at the history of all of the development of human thought, it's caused
[03:35:31.940 --> 03:35:33.100]   the things that happen next.
[03:35:33.100 --> 03:35:38.700]   So it's not just about looking at the world and observing it, it's about actually that
[03:35:38.700 --> 03:35:46.420]   feedback loop that's missing and it's not in any of those categories.
[03:35:46.420 --> 03:35:53.580]   What do you think is the most beautiful idea in the physics of life, in the chemistry of
[03:35:53.580 --> 03:36:03.940]   life, in this... through all your exploration with assembly theory, what is the thing that
[03:36:03.940 --> 03:36:11.780]   made you step back and say, "This idea is beautiful or potentially beautiful"?
[03:36:11.780 --> 03:36:14.340]   - For me, it's that the universe is a creative place.
[03:36:14.340 --> 03:36:20.260]   I guess I want to think, and whether it's true or not, is that we are special in some
[03:36:20.260 --> 03:36:26.100]   way and it's not like an arbitrary, added-on, epiphenomenal or ad hoc feature of the universe
[03:36:26.100 --> 03:36:31.260]   that we exist, but it's something deep and intrinsic to the structure of reality.
[03:36:31.260 --> 03:36:35.700]   And to me, the most beautiful ideas that come out of that is that the reason we exist is
[03:36:35.700 --> 03:36:42.780]   for the universe to generate more things and to think about itself and use that as a mechanism
[03:36:42.780 --> 03:36:47.260]   for creating more stuff.
[03:36:47.260 --> 03:36:48.260]   That's for me.
[03:36:48.260 --> 03:36:56.660]   - So like the life that this, however common it is, is an intrinsic part, is a fundamental
[03:36:56.660 --> 03:36:59.860]   part of this universe, at least, that we live in.
[03:36:59.860 --> 03:37:00.860]   - I think so.
[03:37:00.860 --> 03:37:06.780]   It's always interesting to me because we have theories of quantum mechanics and gravity,
[03:37:06.780 --> 03:37:09.820]   and they're supposed to be our most fundamental theories right now.
[03:37:09.820 --> 03:37:14.700]   And they describe things like the interaction of massive bodies or the way that charges
[03:37:14.700 --> 03:37:18.220]   accelerate or all these kind of features.
[03:37:18.220 --> 03:37:21.380]   And they're these really deep theories, and they tell us a lot about how reality works,
[03:37:21.380 --> 03:37:24.140]   but they're completely agnostic to our existence.
[03:37:24.140 --> 03:37:29.540]   And I can't help but think that whatever describes us has to be even deeper than that.
[03:37:29.540 --> 03:37:35.660]   - And I think incorporating memory, I guess, or causality, whatever the term you want to
[03:37:35.660 --> 03:37:38.860]   use, into the physics view of the world might be taking a step in that direction.
[03:37:38.860 --> 03:37:40.820]   - That's the easiest way to do it.
[03:37:40.820 --> 03:37:44.300]   It's the cleanest, so here we go again with the physicist, I'm a physicist.
[03:37:44.300 --> 03:37:48.060]   The cleanest, I was gonna say the simplest, most elegant way of resolving all of the kind
[03:37:48.060 --> 03:37:53.900]   of ways that we have these paradoxes associated with life.
[03:37:53.900 --> 03:37:59.740]   It's not that life is not, current physics is not incompatible with life, but it doesn't
[03:37:59.740 --> 03:38:00.740]   explain life.
[03:38:00.740 --> 03:38:04.380]   And then you want to know where are the explanatory gaps.
[03:38:04.380 --> 03:38:10.060]   And this idea that we have in assembly that time is fundamental and objects actually are
[03:38:10.060 --> 03:38:14.180]   extended in time and have physical extent in time is the cleanest way of resolving a
[03:38:14.180 --> 03:38:16.660]   lot of the explanatory gaps.
[03:38:16.660 --> 03:38:23.180]   - So I've been, I struggle with assembly theory for many years 'cause I could see this gap.
[03:38:23.180 --> 03:38:30.940]   And I think when I first met Sarah and we realized we were kind of talking about the
[03:38:30.940 --> 03:38:34.620]   same problem, but we understood another language.
[03:38:34.620 --> 03:38:37.860]   It was quite hilarious actually, 'cause it's like, I have no idea what you're talking about,
[03:38:37.860 --> 03:38:39.780]   but I think it sounds right.
[03:38:39.780 --> 03:38:44.580]   So for me, the most beautiful thing about assembly theory is I realized the assembly
[03:38:44.580 --> 03:38:50.740]   theory explains why life is a universe developing a memory, but not only that poetically, I
[03:38:50.740 --> 03:38:53.500]   could actually go and measure it.
[03:38:53.500 --> 03:39:00.420]   And I was like, holy shit, we physically measured this thing, this abstract thing, and we could
[03:39:00.420 --> 03:39:01.940]   measure it.
[03:39:01.940 --> 03:39:08.420]   And not only could we measure it, but we can then start to quantify the causal consequences.
[03:39:08.420 --> 03:39:14.420]   And because, I mean, I think as a kind of inventing this together with Sarah and her
[03:39:14.420 --> 03:39:21.260]   team, I thought there was quite a high chance that, we're doing science, there's such a
[03:39:21.260 --> 03:39:23.260]   high probability we're wrong.
[03:39:23.260 --> 03:39:24.260]   - Every day.
[03:39:24.260 --> 03:39:25.260]   - On this.
[03:39:25.260 --> 03:39:34.300]   And I remember kind of trying to go to hard physicists, mathematicians, complexity theorists,
[03:39:34.300 --> 03:39:38.100]   and everyone just kind of giving me such a hard time about it.
[03:39:38.100 --> 03:39:42.660]   And said, you know, this is kind of, you've just done this, you've just done that, you've
[03:39:42.660 --> 03:39:45.780]   just recapitulated an old theory.
[03:39:45.780 --> 03:39:51.380]   And I was unable, I lacked the language to really explain, and I had to, it was a real
[03:39:51.380 --> 03:39:52.380]   struggle.
[03:39:52.380 --> 03:39:58.980]   So this realization that life, what life does, that physics cannot understand or chemistry,
[03:39:58.980 --> 03:40:04.900]   is the universe develops a memory that's causally actionable, and then we can measure it, but
[03:40:04.900 --> 03:40:10.300]   it isn't just one thing, there is this intrinsic property of all the objects in the universe.
[03:40:10.300 --> 03:40:14.420]   Like I've said before, but me holding up this water bottle, it isn't any other water bottle,
[03:40:14.420 --> 03:40:19.180]   but it is a sum total of all the water bottles that have existed, right?
[03:40:19.180 --> 03:40:23.660]   And will likely change the future of water bottles and for other objects.
[03:40:23.660 --> 03:40:30.940]   So it's this kind of, so for me, assembly theory explains the soul in stuff.
[03:40:30.940 --> 03:40:32.780]   - The monadology.
[03:40:32.780 --> 03:40:37.020]   - But it is, monology is not like Sheldrake's morphic resonance, where we have this kind
[03:40:37.020 --> 03:40:39.780]   of wooey thing permeating the universe.
[03:40:39.780 --> 03:40:44.860]   It's the interaction of objects, of other objects, and some objects have more instantaneous
[03:40:44.860 --> 03:40:52.780]   causal power, that's life, living things, and some objects are the instantaneous output
[03:40:52.780 --> 03:40:56.500]   of that causal power, dead objects, but they're part of the lineage.
[03:40:56.500 --> 03:41:00.720]   And that for me is fascinating and really beautiful, and I think that even if we're
[03:41:00.720 --> 03:41:06.980]   determined to be totally wrong, I think it will help us, help hopefully understand what
[03:41:06.980 --> 03:41:09.900]   life is and go into tech life elsewhere and make life in the lab.
[03:41:09.900 --> 03:41:11.700]   - How does that make you feel, by the way?
[03:41:11.700 --> 03:41:17.180]   Does it make you feel less special, that you're so deeply integrated, interconnected to the
[03:41:17.180 --> 03:41:18.180]   lineage?
[03:41:18.180 --> 03:41:21.740]   - I mean, I can on one level, I just wanted in my life as a scientist, I wanted to have
[03:41:21.740 --> 03:41:25.500]   an interesting idea just once or an original idea.
[03:41:25.500 --> 03:41:31.020]   I mean, it was like, you know, so I think that was cool that we had this idea and we
[03:41:31.020 --> 03:41:32.020]   were playing with it.
[03:41:32.020 --> 03:41:37.460]   I think also that I kind of, I mean, it took me ages to realize that Sarah had also had
[03:41:37.460 --> 03:41:41.300]   the same kind of form, coming towards the same formulation, just from a completely different
[03:41:41.300 --> 03:41:45.260]   point because I, but no, it makes me feel special.
[03:41:45.260 --> 03:41:47.020]   And it also makes me feel connected to the universe.
[03:41:47.020 --> 03:41:53.620]   It also makes me feel not just humble about, you know, being a living object in the universe,
[03:41:53.620 --> 03:41:56.940]   but the fact that it makes me really optimistic about what the universe is going to do in
[03:41:56.940 --> 03:42:02.780]   the future, because we're not just isolated phenomena, we are connected.
[03:42:02.780 --> 03:42:07.380]   I will be able to have, you know, one of my small objectives in life is to change the
[03:42:07.380 --> 03:42:12.100]   future of the universe in some profound way just by existing.
[03:42:12.100 --> 03:42:14.540]   - Yeah, that's not ambitious at all.
[03:42:14.540 --> 03:42:22.660]   - I think it's also good because it makes me feel less lonely because I just realized
[03:42:22.660 --> 03:42:26.020]   I'm not like, I mean, I'm a unique assembly structure, but I have so much overlap with
[03:42:26.020 --> 03:42:31.700]   the other entities I interact with that we're not completely individual, right?
[03:42:31.700 --> 03:42:39.820]   - And yet your existence does have a huge amount of impact on how this whole thing unrolls
[03:42:39.820 --> 03:42:41.500]   on the future of the world.
[03:42:41.500 --> 03:42:43.620]   - As individuals, that's, yeah.
[03:42:43.620 --> 03:42:44.620]   - But I was gonna say--
[03:42:44.620 --> 03:42:45.820]   - Local packets of agency.
[03:42:45.820 --> 03:42:49.540]   - I think we all have a profound impact on the future, some more than others, right?
[03:42:49.540 --> 03:42:50.780]   All human beings, all life.
[03:42:50.780 --> 03:42:56.140]   And I mean, that's why I think it's a privilege in a way for, you know, to say, to assert
[03:42:56.140 --> 03:42:59.180]   some degree of ego and agency.
[03:42:59.180 --> 03:43:02.060]   You know, I'm gonna make a computer or make an origin life machine or we're gonna do this
[03:43:02.060 --> 03:43:03.060]   thing.
[03:43:03.060 --> 03:43:07.380]   But actually, it's just like, you know, life's probably living, if there is a God or there's
[03:43:07.380 --> 03:43:11.620]   a soul in everything, it's probably laughing at us going, "I fool these guys by giving
[03:43:11.620 --> 03:43:12.620]   them ego.
[03:43:12.620 --> 03:43:16.660]   So they strive for this stuff and look what it does for, you know, the assembly space
[03:43:16.660 --> 03:43:18.060]   of the universe."
[03:43:18.060 --> 03:43:20.700]   And there's always a possibility that science can't answer all of it.
[03:43:20.700 --> 03:43:23.460]   So that part's challenging for me.
[03:43:23.460 --> 03:43:27.140]   - There may be a limit to this thing.
[03:43:27.140 --> 03:43:33.380]   Let me ask you a bunch of ridiculous questions and I demand relatively short answers.
[03:43:33.380 --> 03:43:41.140]   Lee, what's the scariest thing you've ever done?
[03:43:41.140 --> 03:43:45.260]   Or what's a scary thing that pops to mind?
[03:43:45.260 --> 03:43:48.780]   - Giving seminars in front of other scientists.
[03:43:48.780 --> 03:43:50.900]   - That's, yeah, that is terrifying.
[03:43:50.900 --> 03:43:56.980]   I could, if I had more time, I would ask you about the most embarrassing, but we'll spare
[03:43:56.980 --> 03:43:57.980]   you.
[03:43:57.980 --> 03:43:58.980]   What about you, Sarah?
[03:43:58.980 --> 03:44:02.660]   Scariest thing up there?
[03:44:02.660 --> 03:44:04.940]   Some of the scary things you've done.
[03:44:04.940 --> 03:44:09.620]   - Actually, the scariest for me was deciding I wanted to get divorced because it was like
[03:44:09.620 --> 03:44:11.500]   a totally radical like...
[03:44:11.500 --> 03:44:12.500]   - Life transformation.
[03:44:12.740 --> 03:44:16.220]   - Yeah, because we had been married for a really long time.
[03:44:16.220 --> 03:44:21.500]   And I think it was just so much like, I realized like so much of my individual agency I didn't
[03:44:21.500 --> 03:44:22.500]   realize I had before.
[03:44:22.500 --> 03:44:26.060]   And that was just really like scary, like empowering scary, but like terrifying.
[03:44:26.060 --> 03:44:29.940]   Like you were living in a kind of one way for your whole life and then you realize your
[03:44:29.940 --> 03:44:31.860]   life could be a different way.
[03:44:31.860 --> 03:44:33.780]   - Yeah, there's a between humans.
[03:44:33.780 --> 03:44:38.700]   I mean, that's the beautiful thing about love is the connection you have, but it's also
[03:44:38.700 --> 03:44:43.420]   becomes a dependency and breaking that, whether it's a mentor, with your parents, your close
[03:44:43.420 --> 03:44:44.420]   friends.
[03:44:44.420 --> 03:44:46.860]   - It's almost like waking up, like just there's a different reality.
[03:44:46.860 --> 03:44:48.180]   Yeah, that was scary.
[03:44:48.180 --> 03:44:49.460]   - Reinventing yourself.
[03:44:49.460 --> 03:44:53.940]   Okay, if you could leave, maybe I'll actually alternate.
[03:44:53.940 --> 03:44:59.900]   Sarah, if you could be someone else for a day, someone alive today, you haven't met
[03:44:59.900 --> 03:45:04.060]   yet, or maybe you could do one who you've met, who would it be?
[03:45:04.060 --> 03:45:05.060]   - Kim Kardashian.
[03:45:05.060 --> 03:45:06.060]   No joke.
[03:45:06.060 --> 03:45:07.060]   The woman's brilliant.
[03:45:07.060 --> 03:45:08.060]   I would be her.
[03:45:08.060 --> 03:45:09.060]   - Yeah, she's brilliant.
[03:45:09.060 --> 03:45:14.140]   - I would just like to experience, like, I just, I think she's got such an interesting
[03:45:14.140 --> 03:45:16.740]   and very deep understanding of social reality.
[03:45:16.740 --> 03:45:20.300]   - But you also said you have a appreciation, a love for fashion.
[03:45:20.300 --> 03:45:21.300]   - I do.
[03:45:21.300 --> 03:45:22.980]   But that's actually the same.
[03:45:22.980 --> 03:45:25.780]   Like I just think it's really interesting because we live in a social reality, which
[03:45:25.780 --> 03:45:27.980]   is completely artificially constructed.
[03:45:27.980 --> 03:45:30.180]   And some people are really genius about moving through that.
[03:45:30.180 --> 03:45:31.580]   And I think she's particularly good at it.
[03:45:31.580 --> 03:45:33.860]   - I wonder if she's good at understanding it, if she's-
[03:45:33.860 --> 03:45:35.660]   - I think it's very deeply intrinsic to her.
[03:45:35.660 --> 03:45:36.660]   So I don't know if she-
[03:45:36.660 --> 03:45:37.660]   - She's like surfing a wave.
[03:45:37.660 --> 03:45:40.580]   - I don't know how much cognitive awareness she has of it or how strategic it is, but
[03:45:40.580 --> 03:45:42.220]   I think it's deeply fascinating.
[03:45:42.220 --> 03:45:44.820]   So I guess that's the first one that comes to mind.
[03:45:44.820 --> 03:45:45.820]   - What about you, Lee?
[03:45:45.820 --> 03:45:47.940]   If you could be somebody for a day?
[03:45:47.940 --> 03:45:48.940]   Don't say Yoshi Bach.
[03:45:48.940 --> 03:45:49.940]   - Don't say Kim Kardashian.
[03:45:49.940 --> 03:45:50.940]   - Yeah, right.
[03:45:50.940 --> 03:45:51.940]   Those two are off the table.
[03:45:51.940 --> 03:45:52.940]   - Off the table.
[03:45:52.940 --> 03:45:58.060]   - No, I was gonna say, I would like to be a, does it have to be here today?
[03:45:58.060 --> 03:46:02.540]   I was gonna say, I'd like to be the latest arm processor.
[03:46:02.540 --> 03:46:03.540]   - Interesting.
[03:46:04.100 --> 03:46:07.620]   - I would like to be the latest arm processor.
[03:46:07.620 --> 03:46:14.220]   I'd like to understand, I would like to know what it feel like to basically-
[03:46:14.220 --> 03:46:15.220]   - You like being objects.
[03:46:15.220 --> 03:46:18.620]   - I like being, I've always obsessed with being objects ever since I was a kid.
[03:46:18.620 --> 03:46:21.140]   - What's the best part of being an arm processor for a day?
[03:46:21.140 --> 03:46:24.820]   - I mean, I'd like to understand how I access my memory, what I anticipate is coming next
[03:46:24.820 --> 03:46:26.020]   in clock cycles.
[03:46:26.020 --> 03:46:27.420]   - What about how it feels like?
[03:46:27.420 --> 03:46:29.820]   - Yeah, I wanna know how it feels like to be-
[03:46:29.820 --> 03:46:30.820]   - To be useful.
[03:46:30.820 --> 03:46:31.820]   - To be, thanks for that.
[03:46:31.820 --> 03:46:44.220]   - All right, if, Lee, if everyone on earth disappeared and it was just you left, what
[03:46:44.220 --> 03:46:46.060]   would your days look like?
[03:46:46.060 --> 03:46:47.060]   What would you do?
[03:46:47.060 --> 03:46:53.500]   Nobody else left to impress, nobody, no, probably can't really do any real science at scale.
[03:46:53.500 --> 03:46:55.980]   What would you do with your remaining days?
[03:46:55.980 --> 03:47:02.580]   - Every possible tool I could and put it in my workshop and just make stuff.
[03:47:02.580 --> 03:47:04.220]   - So try to make stuff.
[03:47:04.220 --> 03:47:07.700]   - Just try and make stuff, make companions, I'd probably not making companions probably,
[03:47:07.700 --> 03:47:08.700]   yeah.
[03:47:08.700 --> 03:47:10.060]   - So in the physical space.
[03:47:10.060 --> 03:47:11.060]   - Yeah.
[03:47:11.060 --> 03:47:12.460]   - What about you, Sarah?
[03:47:12.460 --> 03:47:15.660]   What would you, when you're just left alone on earth, you're the last-
[03:47:15.660 --> 03:47:17.740]   - Are there animals in this scenario?
[03:47:17.740 --> 03:47:19.420]   - No, no living beings.
[03:47:19.420 --> 03:47:20.420]   - No plants?
[03:47:20.420 --> 03:47:21.420]   - No plants.
[03:47:21.420 --> 03:47:26.820]   - Oh, interesting, I was gonna say I would just, I would try to walk the entire planet,
[03:47:26.820 --> 03:47:29.020]   at least all the land mass.
[03:47:29.020 --> 03:47:35.140]   - Well, that's true, so you probably don't know if there's stuff, you could be searching
[03:47:35.140 --> 03:47:36.700]   for plants or other humans or other animals.
[03:47:36.700 --> 03:47:37.700]   - And what would I eat?
[03:47:37.700 --> 03:47:44.700]   - It's a, you just have daily just allotment-
[03:47:44.700 --> 03:47:45.700]   - I would just walk all the time, I think.
[03:47:45.700 --> 03:47:46.700]   - Of soylent.
[03:47:46.700 --> 03:47:47.700]   - I don't know why I-
[03:47:47.700 --> 03:47:48.700]   - Just walk.
[03:47:48.700 --> 03:47:49.700]   - That's just what came to mind.
[03:47:49.700 --> 03:47:50.700]   - You're the explorer.
[03:47:50.700 --> 03:47:51.700]   - I'm the explorer.
[03:47:51.700 --> 03:47:55.140]   - And I guess I would make a goal of covering all of the entire earth, 'cause what else
[03:47:55.140 --> 03:47:56.660]   are you gonna do with your time?
[03:47:56.660 --> 03:48:00.620]   - What's an item on your bucket list, Sarah, that you haven't done yet, but you hope to
[03:48:00.620 --> 03:48:01.620]   do?
[03:48:01.620 --> 03:48:08.460]   Skydiving, travel to space.
[03:48:08.460 --> 03:48:09.460]   - I don't know.
[03:48:09.460 --> 03:48:13.580]   You know, it's funny with my bucket list, I only know it was on my bucket list once
[03:48:13.580 --> 03:48:14.580]   I check it off.
[03:48:14.580 --> 03:48:15.580]   - Once you check it off.
[03:48:15.580 --> 03:48:18.060]   So your bucket list is like a fog, it's like a mystery.
[03:48:18.060 --> 03:48:19.060]   - Yeah.
[03:48:19.060 --> 03:48:20.060]   - You're almost by doing it.
[03:48:20.060 --> 03:48:22.860]   - Yeah, so it's very subconsciously driven.
[03:48:22.860 --> 03:48:24.780]   - So it's in your subconscious in there.
[03:48:24.780 --> 03:48:25.780]   - I think so.
[03:48:25.780 --> 03:48:26.780]   - You're bringing it to the surface.
[03:48:26.780 --> 03:48:29.620]   - I think most of the steering of our agency is in our subconscious anyway, so I just kind
[03:48:29.620 --> 03:48:30.620]   of go with the flow.
[03:48:30.620 --> 03:48:31.620]   But I guess, no, seriously.
[03:48:31.620 --> 03:48:32.620]   - Yeah, no, I get it.
[03:48:32.620 --> 03:48:37.660]   - I don't know, I guess, but I would like to go on a submarine, like to the bottom of
[03:48:37.660 --> 03:48:39.100]   the ocean, I think that'd be really cool.
[03:48:39.100 --> 03:48:40.460]   - To the bottom of the ocean.
[03:48:40.460 --> 03:48:42.620]   Are you captivated by the mystery of the ocean?
[03:48:42.620 --> 03:48:43.620]   Like how low?
[03:48:43.620 --> 03:48:44.620]   - I am, yeah.
[03:48:44.620 --> 03:48:47.940]   - Yeah, what about you, Lee?
[03:48:47.940 --> 03:48:50.420]   What item on your bucket list?
[03:48:50.420 --> 03:48:51.900]   - I don't have a bucket list, but I'll just make one.
[03:48:51.900 --> 03:48:58.060]   I would love to take a computer to the moon or Mars and make drugs off world, be the first
[03:48:58.060 --> 03:49:00.780]   chemist to make drugs off world.
[03:49:00.780 --> 03:49:02.620]   - The first drug manufacturer in space.
[03:49:02.620 --> 03:49:03.620]   - Yeah, why not?
[03:49:03.620 --> 03:49:08.860]   - Do they have to be somehow like be able to habitate, like be able to survive on that
[03:49:08.860 --> 03:49:10.700]   particular space?
[03:49:10.700 --> 03:49:14.340]   Or like what's the connection between being on Mars and doing manufacturing?
[03:49:14.340 --> 03:49:21.860]   I just would like to take the ability to have command and control over chemicals programmatically
[03:49:21.860 --> 03:49:24.180]   off earth to somewhere else in the universe.
[03:49:24.180 --> 03:49:29.020]   - That just seems like you like difficulty engineering problems.
[03:49:29.020 --> 03:49:30.020]   - Before I die, if I can do that, that's great.
[03:49:30.020 --> 03:49:31.700]   - Would you travel to space before you die?
[03:49:31.700 --> 03:49:32.980]   - Yeah, yeah, that's what I'm saying.
[03:49:32.980 --> 03:49:35.820]   I'd love to go into space, but not just to be a tourist.
[03:49:35.820 --> 03:49:39.940]   I wanna take a scientific experiment in space and do a thing in space that's never been
[03:49:39.940 --> 03:49:41.500]   done before.
[03:49:41.500 --> 03:49:42.500]   - That's a real possibility.
[03:49:42.500 --> 03:49:43.500]   - Yeah, yeah, yeah.
[03:49:43.500 --> 03:49:48.780]   So that's why there's no point in listing things I can't do.
[03:49:48.780 --> 03:49:56.660]   - All right, what small act of kindness were you once shown that you will never forget?
[03:49:56.660 --> 03:49:59.860]   Small act of kindness, not big.
[03:49:59.860 --> 03:50:03.420]   Somebody was just kind to you, somebody did something sweet.
[03:50:03.420 --> 03:50:12.620]   - When I was a PhD student, someone helped me out with just, I was basically, I needed
[03:50:12.620 --> 03:50:16.980]   a computer, I needed some power, computation power, and someone took pity on me and helped
[03:50:16.980 --> 03:50:20.300]   me and gave me, I was really touched, they didn't have to.
[03:50:20.300 --> 03:50:23.820]   And they were actually quite, they were a disabled scientist, they had other things
[03:50:23.820 --> 03:50:29.260]   to do rather than help some random PhD student, gave me access, taught me a lot of stuff.
[03:50:29.260 --> 03:50:33.460]   - Yeah, actually, when you're a grad student or when you're a student, when you're a student,
[03:50:33.460 --> 03:50:35.740]   the younger it is, the better.
[03:50:35.740 --> 03:50:40.780]   The attention, the support, the love you get from an older person, a teacher, something
[03:50:40.780 --> 03:50:43.340]   like that is super powerful, it's fascinating.
[03:50:43.340 --> 03:50:47.860]   And from the perspective of the teacher, they might not realize the impact I have, but that
[03:50:47.860 --> 03:50:54.380]   little bit, those few words, a little bit of help can have a lot of impact.
[03:50:54.380 --> 03:50:56.380]   What about you, Sarah?
[03:50:56.380 --> 03:50:59.300]   Somebody give you a free Starbucks at some point?
[03:50:59.300 --> 03:51:00.500]   - I love free Starbucks.
[03:51:00.500 --> 03:51:04.900]   I like it when you're in the line at Starbucks and somebody buys your coffee in front of
[03:51:04.900 --> 03:51:06.900]   you and then you buy the next one, I love those.
[03:51:06.900 --> 03:51:09.180]   But that's not my example, but those are great.
[03:51:09.180 --> 03:51:10.180]   - I love that too.
[03:51:10.180 --> 03:51:12.580]   - Okay, and then my kids get all excited when we do it, when we go in.
[03:51:12.580 --> 03:51:18.060]   We're the first ones in line doing it.
[03:51:18.060 --> 03:51:23.060]   I guess I can use a similar example about just being a student.
[03:51:23.060 --> 03:51:30.620]   So Paul Davies is a very well-known theoretical physicist and he was generous enough with
[03:51:30.620 --> 03:51:33.060]   his time to take me on as a postdoc.
[03:51:33.060 --> 03:51:36.940]   But before I became his postdoc, he invited me to a workshop at Arizona State University
[03:51:36.940 --> 03:51:42.300]   Beyond Center and took a walk with me around campus just to talk about ideas after.
[03:51:42.300 --> 03:51:48.500]   And I think there were two things that were completely generous about that.
[03:51:48.500 --> 03:51:53.620]   One is Paul's philosophy is always interacting with young people.
[03:51:53.620 --> 03:51:58.060]   You interact with a mind in the room, it doesn't matter how well-known or whatever.
[03:51:58.060 --> 03:52:01.260]   It's like you evaluate the person for the person.
[03:52:01.260 --> 03:52:05.900]   But he also gave me a book, The Eerie Silence, that he had written and he wrote in it, this
[03:52:05.900 --> 03:52:12.380]   is how E.E. gets to E.T., which was Enantiomeric Excess, which I worked on as a PhD student,
[03:52:12.380 --> 03:52:16.100]   was the origin of homochirality, all the way up to what the book was about, which was Are
[03:52:16.100 --> 03:52:19.500]   We Alone in the Universe and Is There an Intelligent Life Out There?
[03:52:19.500 --> 03:52:25.900]   And it was just so much about the questions I wanted to ask, 'cause it was just everything
[03:52:25.900 --> 03:52:29.180]   about, it was just really, really kind.
[03:52:29.180 --> 03:52:32.060]   - Like that it's okay to ask these questions.
[03:52:32.060 --> 03:52:33.060]   - Yeah.
[03:52:33.060 --> 03:52:34.060]   - And you can actually have strong enough to answer them.
[03:52:34.060 --> 03:52:38.260]   - I think a lot of my career is mostly his encouragement to ask deep questions.
[03:52:38.260 --> 03:52:42.100]   He gave me the space to do it in ways that a lot of previous mentors had.
[03:52:42.100 --> 03:52:47.180]   I've had a good experience with mentors, but it was like go off the deep end, ask the hardest
[03:52:47.180 --> 03:52:48.180]   questions.
[03:52:48.180 --> 03:52:49.980]   And I think that's the best gift you can give somebody.
[03:52:49.980 --> 03:52:57.540]   - What would you, 'cause you're both fascinating minds and non, I would say non-standard in
[03:52:57.540 --> 03:52:59.060]   the best possible way.
[03:52:59.060 --> 03:53:05.500]   Is there advice you can give to young folks how to be non-standard, how to stand out,
[03:53:05.500 --> 03:53:07.260]   novelty, how to generate novelty?
[03:53:07.260 --> 03:53:10.860]   - That's what I want on my tombstone, I have one.
[03:53:10.860 --> 03:53:11.860]   - He generated novelty.
[03:53:11.860 --> 03:53:12.860]   - No, no, how to.
[03:53:12.860 --> 03:53:13.860]   - Oh, how to.
[03:53:13.860 --> 03:53:14.860]   - How still.
[03:53:14.860 --> 03:53:23.380]   I just love doing science, and so when I was younger, I was just wanted to, I mean, I'm
[03:53:23.380 --> 03:53:25.140]   still not sure I'm a real scientist, right?
[03:53:25.140 --> 03:53:26.560]   So I wanna try.
[03:53:26.560 --> 03:53:32.380]   So my advice for the young people is just, if you love asking questions, then don't be
[03:53:32.380 --> 03:53:37.180]   afraid to ask the question, even if it pisses people off, because if you piss people off,
[03:53:37.180 --> 03:53:40.100]   you're probably asking the right question.
[03:53:40.100 --> 03:53:44.620]   What I would say though is don't do what I did, which is just piss everyone off.
[03:53:44.620 --> 03:53:52.980]   Try and work out how to, you know, I think if other people are challenged by your questions,
[03:53:52.980 --> 03:53:58.940]   you will get not only respect, but people will create space for you 'cause you're doing
[03:53:58.940 --> 03:54:01.380]   something really new.
[03:54:01.380 --> 03:54:07.640]   I really try to create space in my academic career with my team, really try and praise
[03:54:07.640 --> 03:54:09.600]   them and push them to do new things.
[03:54:09.600 --> 03:54:17.260]   So my advice is try to do new things, get feedback, and the universe will help you.
[03:54:17.260 --> 03:54:19.820]   - 'Cause the universe likes novelty.
[03:54:19.820 --> 03:54:21.860]   - I think so, I think so, right?
[03:54:21.860 --> 03:54:24.460]   - This one will keep 'em around.
[03:54:24.460 --> 03:54:26.380]   What about you, Sarah?
[03:54:26.380 --> 03:54:30.420]   You too like to ask the really out there big question.
[03:54:30.420 --> 03:54:36.300]   - Yeah, 'cause I have a strong passion for them, so I think it goes back to the love.
[03:54:36.300 --> 03:54:40.620]   Like if you're doing the thing you're supposed to be doing, you should really love it.
[03:54:40.620 --> 03:54:43.420]   So I always tell people that they should do the thing they're most passionate about, but
[03:54:43.420 --> 03:54:49.540]   I think a flip side of that is that's when you become, not to sound cheesy, but like
[03:54:49.540 --> 03:54:51.100]   your best version of yourself.
[03:54:51.100 --> 03:54:56.140]   So I guess like for me, as I become more successful in my career, I feel like I can be more myself
[03:54:56.140 --> 03:54:57.580]   as an individual.
[03:54:57.580 --> 03:55:01.460]   And so there's this, I've always been following the questions I'm most interested in, which
[03:55:01.460 --> 03:55:05.580]   very early on I was discouraged from doing by many people because they thought they were
[03:55:05.580 --> 03:55:06.820]   unanswerable questions.
[03:55:06.820 --> 03:55:10.940]   And I always just thought, well, if no one's even trying to answer them, of course they're
[03:55:10.940 --> 03:55:12.460]   gonna be unanswerable.
[03:55:12.460 --> 03:55:13.900]   And then that was kind of an odd viewpoint.
[03:55:13.900 --> 03:55:19.100]   But the more I found my way in that space, the more I also made a space for myself as
[03:55:19.100 --> 03:55:25.420]   a person, because you're basically generating the niche that you want to exist in.
[03:55:25.420 --> 03:55:31.580]   And so I think that's part of it, is not just to follow your passion, but also think about
[03:55:31.580 --> 03:55:35.060]   like who do you want to be and create that.
[03:55:35.060 --> 03:55:36.060]   Yeah.
[03:55:36.060 --> 03:55:37.060]   Who am I?
[03:55:37.060 --> 03:55:38.060]   Who do you want to be?
[03:55:38.060 --> 03:55:40.380]   I mean, yeah, play temporally with it.
[03:55:40.380 --> 03:55:41.860]   Yeah, who am I now?
[03:55:41.860 --> 03:55:43.060]   Who do I want to be now?
[03:55:43.060 --> 03:55:44.340]   But who do I want to be in the future?
[03:55:44.340 --> 03:55:45.340]   They're not decoupled.
[03:55:45.340 --> 03:55:46.340]   Yeah.
[03:55:46.340 --> 03:55:53.500]   I always wonder if that's like, if I become something, am I finding myself or am I creating
[03:55:53.500 --> 03:55:54.500]   myself?
[03:55:54.500 --> 03:55:55.500]   Yeah.
[03:55:55.500 --> 03:55:57.980]   And I think those are somehow the same kind of thing.
[03:55:57.980 --> 03:56:02.700]   I do feel often like I was always meant to be this kind of thing.
[03:56:02.700 --> 03:56:06.180]   But is that created or discovered?
[03:56:06.180 --> 03:56:07.180]   I don't know.
[03:56:07.180 --> 03:56:09.660]   But basically go towards that direction.
[03:56:09.660 --> 03:56:11.980]   If you were abducted by aliens, Sarah.
[03:56:11.980 --> 03:56:12.980]   Excellent.
[03:56:12.980 --> 03:56:13.980]   I'm waiting.
[03:56:13.980 --> 03:56:14.980]   They can come find me.
[03:56:15.380 --> 03:56:17.660]   They're on a spaceship.
[03:56:17.660 --> 03:56:28.060]   And then they somehow figured out the language you speak and ask you, what are you?
[03:56:28.060 --> 03:56:29.320]   Explain yourself.
[03:56:29.320 --> 03:56:33.660]   Not you, Sarah, but the species.
[03:56:33.660 --> 03:56:38.060]   Life on earth, like we don't have time.
[03:56:38.060 --> 03:56:42.900]   We're busy grad students from another planet.
[03:56:42.900 --> 03:56:45.640]   What's interesting about human civilization?
[03:56:45.640 --> 03:56:47.820]   What's interesting about you?
[03:56:47.820 --> 03:56:48.820]   You specifically too.
[03:56:48.820 --> 03:56:53.820]   They could be very kind of personal and kind of pushy.
[03:56:53.820 --> 03:56:54.820]   And yeah.
[03:56:54.820 --> 03:56:57.820]   How would you begin to describe?
[03:56:57.820 --> 03:56:58.820]   Okay.
[03:56:58.820 --> 03:57:00.620]   I have one.
[03:57:00.620 --> 03:57:05.500]   Because obviously I self-identify as a scientist and a physicist, but intrinsically I feel
[03:57:05.500 --> 03:57:06.500]   more like an artist.
[03:57:06.500 --> 03:57:10.360]   But it's almost like you're an artist that you don't know what you're painting yet.
[03:57:10.360 --> 03:57:13.280]   And I guess I feel like that's humanity.
[03:57:13.280 --> 03:57:19.520]   In some sense, we're creating something I think is profound and potentially very beautiful
[03:57:19.520 --> 03:57:20.960]   in existence of the universe.
[03:57:20.960 --> 03:57:25.400]   But we're just so early.
[03:57:25.400 --> 03:57:26.400]   We're early.
[03:57:26.400 --> 03:57:27.400]   We're young.
[03:57:27.400 --> 03:57:28.400]   We don't know what we're doing yet.
[03:57:28.400 --> 03:57:29.400]   Yeah.
[03:57:29.400 --> 03:57:31.360]   What's with the nuclear weapons is a big question too.
[03:57:31.360 --> 03:57:32.360]   What are you guys?
[03:57:32.360 --> 03:57:34.400]   What are we doing with them?
[03:57:34.400 --> 03:57:37.840]   This creativity that you talk is very nice, but it's...
[03:57:37.840 --> 03:57:39.400]   We're making things that are...
[03:57:39.400 --> 03:57:41.600]   Very destructive and the rockets.
[03:57:41.600 --> 03:57:43.320]   This seems very aggressive.
[03:57:43.320 --> 03:57:44.320]   Yeah.
[03:57:44.320 --> 03:57:45.320]   I know.
[03:57:45.320 --> 03:57:48.320]   This is my blinders on.
[03:57:48.320 --> 03:57:49.320]   I don't know.
[03:57:49.320 --> 03:57:50.880]   I mean, it goes back to the whole conversation about suffering.
[03:57:50.880 --> 03:57:56.720]   I have a hard time regularizing certain aspects of reality into what I want to envision.
[03:57:56.720 --> 03:57:58.480]   And that's obviously problematic.
[03:57:58.480 --> 03:58:03.960]   But nuclear power has also given us a lot of good things.
[03:58:03.960 --> 03:58:04.960]   So both...
[03:58:04.960 --> 03:58:06.240]   That's human nature.
[03:58:06.240 --> 03:58:10.880]   Both human beings and the technology we create has the capacity for evil and the capacity
[03:58:10.880 --> 03:58:11.880]   for good.
[03:58:11.880 --> 03:58:12.880]   Yeah.
[03:58:12.880 --> 03:58:13.880]   And we can't all be good all the time.
[03:58:13.880 --> 03:58:17.400]   I mean, there's this huge misnomer that you need to be liked by everyone universally.
[03:58:17.400 --> 03:58:21.360]   And obviously that's an ideal, but it's physically impossible.
[03:58:21.360 --> 03:58:24.600]   You can't get a group of people in a room and have everyone like each other all the
[03:58:24.600 --> 03:58:25.600]   time.
[03:58:25.600 --> 03:58:32.360]   So I think that kind of tension is actually really important that we have different aesthetics,
[03:58:32.360 --> 03:58:38.720]   different goals, and sometimes conflict comes out of that.
[03:58:38.720 --> 03:58:39.720]   Yeah.
[03:58:39.720 --> 03:58:43.480]   Speaking of which, do you, Lee, and Yosha Bach ever say anything nice to each other?
[03:58:43.480 --> 03:58:45.520]   Or is it always conflict?
[03:58:45.520 --> 03:58:46.520]   We never have conflict.
[03:58:46.520 --> 03:58:50.200]   We argue, but I don't think the arguments are bad.
[03:58:50.200 --> 03:58:51.200]   It's love.
[03:58:51.200 --> 03:58:52.200]   I mean, I think the problem I have...
[03:58:52.200 --> 03:58:53.200]   Not problem.
[03:58:53.200 --> 03:58:54.200]   I think...
[03:58:54.200 --> 03:58:55.200]   Here we go.
[03:58:55.200 --> 03:58:56.200]   And he's not here to defend himself.
[03:58:56.200 --> 03:58:59.680]   No, it's just I don't necessarily understand.
[03:58:59.680 --> 03:59:03.360]   I mean, he's just talking at such a high level.
[03:59:03.360 --> 03:59:04.360]   I'm a dimwit.
[03:59:04.360 --> 03:59:08.160]   So I think a lot of our conflict is not conflict.
[03:59:08.160 --> 03:59:09.240]   We actually have a...
[03:59:09.240 --> 03:59:10.240]   I think...
[03:59:10.240 --> 03:59:11.240]   I mean, I can't speak for Yosha.
[03:59:11.240 --> 03:59:12.640]   I have a deep appreciation for him.
[03:59:12.640 --> 03:59:13.680]   He's brilliant.
[03:59:13.680 --> 03:59:17.680]   But I think I'm kind of frustrated and I'm trying to...
[03:59:17.680 --> 03:59:23.760]   He thinks the universe is a computer and I want to turn the universe into a computer.
[03:59:23.760 --> 03:59:24.760]   Yeah.
[03:59:24.760 --> 03:59:25.920]   That's a small disagreement.
[03:59:25.920 --> 03:59:26.920]   So what would you...
[03:59:26.920 --> 03:59:30.520]   How would you defend your life to an alien when you're being abducted?
[03:59:30.520 --> 03:59:32.520]   Would you focus on the specifics of your life?
[03:59:32.520 --> 03:59:33.520]   No, no, no.
[03:59:33.520 --> 03:59:34.520]   I would be...
[03:59:34.520 --> 03:59:37.160]   I would try to be as random as possible and try and confuse them.
[03:59:37.160 --> 03:59:38.160]   Oh, good.
[03:59:38.160 --> 03:59:39.160]   Good.
[03:59:39.160 --> 03:59:40.160]   Excellent.
[03:59:40.160 --> 03:59:41.160]   That might be the wiser choice.
[03:59:41.160 --> 03:59:42.160]   The Easter eggs in reality.
[03:59:42.160 --> 03:59:43.160]   No, I mean, if aliens abducted me...
[03:59:43.160 --> 03:59:44.160]   Would you play dumb?
[03:59:44.160 --> 03:59:45.160]   No, no.
[03:59:45.160 --> 03:59:46.160]   I would try and be as random as...
[03:59:46.160 --> 03:59:47.160]   I would try and do something that would surprise the hell out of them, which I thought...
[03:59:47.160 --> 03:59:48.160]   I mean, probably risky.
[03:59:48.160 --> 03:59:49.160]   They might kill me.
[03:59:49.160 --> 03:59:50.160]   But I think I would try and be as random as possible.
[03:59:50.160 --> 03:59:51.160]   I would try and be as random as possible.
[03:59:51.160 --> 03:59:53.760]   I would try and do something that would surprise the hell out of them, which I thought...
[03:59:53.760 --> 03:59:54.760]   I mean, probably risky.
[03:59:54.760 --> 03:59:55.760]   They might kill me.
[03:59:55.760 --> 03:59:56.760]   But I think that might be funny.
[03:59:56.760 --> 03:59:57.760]   That might...
[03:59:57.760 --> 04:00:00.400]   Yeah, they might want to study you for prolonged periods of time.
[04:00:00.400 --> 04:00:04.440]   My reasoning is if I wanted to stay alive, okay, so if the thing is, if I wasn't going
[04:00:04.440 --> 04:00:08.280]   back to Earth and the job was to stay alive, if I could be as surprising as possible, they'd
[04:00:08.280 --> 04:00:11.680]   keep me around like a pet, right?
[04:00:11.680 --> 04:00:13.000]   Pet Lee on the alien space ship.
[04:00:13.000 --> 04:00:14.560]   So you'd be okay being a pet?
[04:00:14.560 --> 04:00:15.560]   Well, no, but I mean...
[04:00:15.560 --> 04:00:19.480]   The last human that survives would just be a pet to the aliens.
[04:00:19.480 --> 04:00:20.480]   I don't know.
[04:00:20.480 --> 04:00:26.280]   I think that might be fun because then I might get some feedback from their curiosity.
[04:00:26.280 --> 04:00:27.280]   But yeah.
[04:00:27.280 --> 04:00:28.720]   Let me ask you this question.
[04:00:28.720 --> 04:00:33.420]   Given our conversation has a very different meaning, not a more profound meaning perhaps,
[04:00:33.420 --> 04:00:42.280]   but would you rather lose all of your old memories or never be able to make new ones?
[04:00:42.280 --> 04:00:46.880]   I would have to lose all my old memories.
[04:00:46.880 --> 04:00:49.880]   Again, it's the novelty.
[04:00:49.880 --> 04:00:52.280]   What about you, Sarah?
[04:00:52.280 --> 04:00:54.880]   I'm the same because I don't think...
[04:00:54.880 --> 04:00:56.440]   It's about the future experience, right?
[04:00:56.440 --> 04:01:01.200]   And in some sense, like you were saying earlier, most of our lived experience is actually in
[04:01:01.200 --> 04:01:02.200]   our memories.
[04:01:02.200 --> 04:01:06.560]   So if you can't generate new memories, it's like you're not alive anymore.
[04:01:06.560 --> 04:01:08.240]   That's it.
[04:01:08.240 --> 04:01:10.440]   What comforts you on bad days?
[04:01:10.440 --> 04:01:14.600]   When you look at human civilization, when you look at your own life, what gives you
[04:01:14.600 --> 04:01:20.680]   hope or makes you feel good about what we're doing about life at the small scale of you
[04:01:20.680 --> 04:01:25.520]   as a human and at the big scale of us as a human civilization, maybe the big scale of
[04:01:25.520 --> 04:01:27.520]   the universe?
[04:01:27.520 --> 04:01:30.600]   Children, my kids.
[04:01:30.600 --> 04:01:37.000]   But I also mean that in like a grand sense of like, not a grand, but like future minds
[04:01:37.000 --> 04:01:38.160]   in some sense.
[04:01:38.160 --> 04:01:42.400]   So for me, like the most bleak movie ever, people worry about apocalyptic things like
[04:01:42.400 --> 04:01:46.960]   AI existential risk and climate change, which children of men, the whole premise of the
[04:01:46.960 --> 04:01:50.840]   movie was there can be no children born on the entire planet.
[04:01:50.840 --> 04:01:54.920]   And the youngest person on the planet is like 18 years old or something.
[04:01:54.920 --> 04:01:56.880]   Can you imagine a world without children?
[04:01:56.880 --> 04:01:59.120]   It's just, it's harrowing.
[04:01:59.120 --> 04:02:00.160]   That's the scariest thing.
[04:02:00.160 --> 04:02:08.880]   So I think what gives me hope is always youth and the hope of children and the possibilities
[04:02:08.880 --> 04:02:10.240]   of the future they see.
[04:02:10.240 --> 04:02:14.520]   And they grow up in a completely different reality than adults do.
[04:02:14.520 --> 04:02:19.680]   And I think we have a hard time seeing what their reality actually looks like.
[04:02:19.680 --> 04:02:23.240]   But I think most of the time it's super interesting.
[04:02:23.240 --> 04:02:26.360]   Yeah, they have dreams, they have imagination.
[04:02:26.360 --> 04:02:27.880]   They have this kind of excitement.
[04:02:27.880 --> 04:02:28.880]   It's so cool.
[04:02:28.880 --> 04:02:31.880]   It's so fun to watch.
[04:02:31.880 --> 04:02:38.920]   And yeah, you feel like you're almost getting in the way of all that imagination.
[04:02:38.920 --> 04:02:39.920]   What about you, Lee?
[04:02:39.920 --> 04:02:40.920]   What gives you hope?
[04:02:40.920 --> 04:02:46.040]   So when I go back to my eight year old self, the thing that I dreamed of as my eight year
[04:02:46.040 --> 04:02:50.440]   old self was this world in which technology became programmable and there was internet
[04:02:50.440 --> 04:02:58.440]   and I get information and I would expand my consciousness by just, you know, getting access
[04:02:58.440 --> 04:02:59.880]   to everything that was going on.
[04:02:59.880 --> 04:03:01.480]   And it's happened in my lifetime.
[04:03:01.480 --> 04:03:02.880]   I mean, we really do have that.
[04:03:02.880 --> 04:03:06.800]   I mean, okay, there's some bad things, you know, there's TikTok, everyone just, or whatever,
[04:03:06.800 --> 04:03:08.240]   all the bad things about social media.
[04:03:08.240 --> 04:03:17.040]   But I think, I mean, I can't quite believe my luck being born now.
[04:03:17.040 --> 04:03:18.040]   So amazing.
[04:03:18.040 --> 04:03:20.600]   Being able to program reality in some way.
[04:03:20.600 --> 04:03:21.600]   Yeah.
[04:03:21.600 --> 04:03:26.800]   And the thing that I really find fascinating about human beings is just how ingenious they
[04:03:26.800 --> 04:03:27.800]   are.
[04:03:27.800 --> 04:03:35.200]   I'm, you know, whether it's from my kids, my research group, my peers, other companies,
[04:03:35.200 --> 04:03:37.960]   just how ingenious everyone is.
[04:03:37.960 --> 04:03:43.760]   And I'm pretty sure humanity has a, or our causal chain in which humanity is a vital
[04:03:43.760 --> 04:03:46.600]   part in the future is going to have a lot of fun.
[04:03:46.600 --> 04:03:50.320]   And I'm just, yeah, it's just mind blowing just to watch.
[04:03:50.320 --> 04:03:55.880]   And you know, so humans are ingenious and I hope to help them be more ingenious if I
[04:03:55.880 --> 04:03:56.880]   can.
[04:03:56.880 --> 04:04:02.640]   Well, what gives me hope, what makes me feel good on bad days is the existence of wild
[04:04:02.640 --> 04:04:09.400]   minds like yours, novelty generators, assembly structures that generate novelty and do so
[04:04:09.400 --> 04:04:12.240]   beautifully and then tweet about it.
[04:04:12.240 --> 04:04:15.240]   Sarah, this, I really, really enjoy talking to you.
[04:04:15.240 --> 04:04:16.240]   I enjoy following you.
[04:04:16.240 --> 04:04:17.240]   I'm a huge fan.
[04:04:17.240 --> 04:04:21.800]   Sarah Lee, I hope to talk to you many times in the future, maybe with Yosha Bach.
[04:04:21.800 --> 04:04:22.800]   You're just incredible people.
[04:04:22.800 --> 04:04:24.200]   Thank you for everything you do.
[04:04:24.200 --> 04:04:25.200]   You're awesome.
[04:04:25.200 --> 04:04:26.200]   Thank you for talking today.
[04:04:26.200 --> 04:04:27.200]   I really, really appreciate it.
[04:04:27.200 --> 04:04:28.200]   Thanks.
[04:04:28.200 --> 04:04:29.200]   Yeah, brilliant to be here.
[04:04:29.840 --> 04:04:32.840]   Thanks for listening to this conversation with Sarah Walker and Lee Cronin.
[04:04:32.840 --> 04:04:37.520]   To support this podcast, please check out our sponsors in the description.
[04:04:37.520 --> 04:04:41.280]   Now let me leave you with some words from Arthur C. Clarke.
[04:04:41.280 --> 04:04:43.760]   Two possibilities exist.
[04:04:43.760 --> 04:04:49.120]   Either we are alone in the universe or we are not.
[04:04:49.120 --> 04:04:52.280]   Both are equally terrifying.
[04:04:52.280 --> 04:04:58.800]   And let me, if I may, add to that by saying that both possibilities, at least to me, are
[04:04:58.800 --> 04:05:00.960]   both terrifying and exciting.
[04:05:00.960 --> 04:05:05.920]   And keeping these two feelings in my heart is a fun way to explore, to wander, to think
[04:05:05.920 --> 04:05:10.400]   and to live, always a little bit on the edge of madness.
[04:05:10.400 --> 04:05:11.400]   Thank you for listening.
[04:05:11.400 --> 04:05:13.120]   I hope to see you next time.
[04:05:13.120 --> 04:05:14.120]   Bye.
[04:05:14.120 --> 04:05:15.120]   Bye.
[04:05:15.120 --> 04:05:16.120]   Bye.
[04:05:16.120 --> 04:05:17.120]   Bye.
[04:05:17.120 --> 04:05:18.120]   Bye.
[04:05:18.120 --> 04:05:19.120]   Bye.
[04:05:19.120 --> 04:05:20.120]   Bye.
[04:05:20.120 --> 04:05:21.120]   Bye.

