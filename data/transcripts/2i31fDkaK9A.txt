
[00:00:00.000 --> 00:00:04.740]   I tweet that and then I finished the day and I wake up the next morning and I glance at
[00:00:04.740 --> 00:00:05.740]   my phone.
[00:00:05.740 --> 00:00:11.000]   I'm seeing all these verified accounts that are, you know, quote tweeting it and demanding
[00:00:11.000 --> 00:00:12.400]   a retraction and whatever.
[00:00:12.400 --> 00:00:17.400]   And I go, oh, okay, this looks like it's getting, looks like it's getting some attention.
[00:00:17.400 --> 00:00:21.520]   I then continue about my day around noon.
[00:00:21.520 --> 00:00:29.300]   I hear from my dad that he got a hundred messages from you should have aborted your son to we're
[00:00:29.300 --> 00:00:31.560]   going to find all of you to whatever else.
[00:00:31.560 --> 00:00:32.840]   My dad has no idea what's going on.
[00:00:32.840 --> 00:00:37.880]   He's like, I don't know what this is, but I have a hundred DMS to everything else you
[00:00:37.880 --> 00:00:39.200]   can imagine.
[00:00:39.200 --> 00:00:47.280]   Um, and I start to get emails about, you know, we, you know, your Jewish faith, this and
[00:00:47.280 --> 00:00:48.840]   that and the other thing.
[00:00:48.840 --> 00:00:53.520]   And so at that point to me, I thought this is just going to get worse and worse and worse.
[00:00:53.520 --> 00:00:58.240]   And so I deleted the tweet and I really regret doing that because over the 48 hours that
[00:00:58.240 --> 00:01:01.880]   followed, yes, the attacks escalated.
[00:01:01.880 --> 00:01:07.360]   It went through Candace Owens and then at Fox news.com Newsmax kind of peaking with
[00:01:07.360 --> 00:01:09.400]   with Donald Trump Jr.
[00:01:09.400 --> 00:01:12.640]   And it was horrible.
[00:01:12.640 --> 00:01:16.680]   The following is a conversation with David Pakman, a left wing progressive political
[00:01:16.680 --> 00:01:20.320]   commentator and host of the David Pakman show.
[00:01:20.320 --> 00:01:26.040]   I hope to continue to have many conversations on politics with prominent, insightful, and
[00:01:26.040 --> 00:01:30.160]   sometimes controversial figures across the political spectrum.
[00:01:30.160 --> 00:01:34.320]   David and I have been planning to speak for a long time, and I'm sure we'll speak many
[00:01:34.320 --> 00:01:35.500]   more times.
[00:01:35.500 --> 00:01:39.660]   This conversation was challenging, eye opening, and fun.
[00:01:39.660 --> 00:01:41.400]   This is the Lex Friedman podcast.
[00:01:41.400 --> 00:01:44.680]   To support it, please check out our sponsors in the description.
[00:01:44.680 --> 00:01:49.320]   And now dear friends, here's David Pakman.
[00:01:49.320 --> 00:01:55.400]   Are there interesting differences to you between terms like liberal, Democrat, left wing, leftist,
[00:01:55.400 --> 00:02:00.200]   progressive, socialist, communist, Marxist, far left, center left, all of these labels?
[00:02:00.200 --> 00:02:02.240]   Is there interesting distinctions between them?
[00:02:02.240 --> 00:02:04.440]   Yeah, there's two sets of distinctions.
[00:02:04.440 --> 00:02:08.800]   One is if you just want to say, let's define each of these as political terms.
[00:02:08.800 --> 00:02:09.840]   They're all different terms.
[00:02:09.840 --> 00:02:14.400]   You can be a progressive ideologically, but not be a member of the democratic party.
[00:02:14.400 --> 00:02:17.120]   Many say the democratic party isn't even really very progressive.
[00:02:17.120 --> 00:02:22.660]   So these are certainly terms that we could define in order to have a conversation about
[00:02:22.660 --> 00:02:27.300]   the next thing, kind of as a precursor to a conversation.
[00:02:27.300 --> 00:02:35.720]   Sometimes the terms are used in order to tag someone with a certain ideology that's not
[00:02:35.720 --> 00:02:41.700]   really linked to policy or any particular political question, but they can be used positively
[00:02:41.700 --> 00:02:46.600]   or negatively to just kind of say, here is the image of this individual that I have in
[00:02:46.600 --> 00:02:47.600]   my mind.
[00:02:47.600 --> 00:02:54.740]   The term Marxist is right now very popularly being used by some on the right, um, to attack
[00:02:54.740 --> 00:02:56.060]   Democrats.
[00:02:56.060 --> 00:03:00.440]   There's very few actual Marxists, certainly not in positions of power in the United States,
[00:03:00.440 --> 00:03:02.860]   but even among the general population.
[00:03:02.860 --> 00:03:05.080]   Um, so I think it's important to distinguish.
[00:03:05.080 --> 00:03:09.940]   Are we defining these terms because we want to compare and contrast the ideas that a particular
[00:03:09.940 --> 00:03:15.020]   group might bring to the discussion or are we using them as insults or to stifle conversation?
[00:03:15.020 --> 00:03:18.700]   There are terms that can be used to start a conversation or to stop it.
[00:03:18.700 --> 00:03:22.940]   And the use of those terms is evolving rapidly month by month.
[00:03:22.940 --> 00:03:27.500]   So the term leftist, I think is a relatively popular term now to use in the negative context
[00:03:27.500 --> 00:03:34.940]   to describe, um, what an outraged left wing commentator.
[00:03:34.940 --> 00:03:41.820]   I think what you're kind of grasping onto is that there's probably some set of ideas
[00:03:41.820 --> 00:03:46.340]   that would apply to most of those who consider themselves to be on the left.
[00:03:46.340 --> 00:03:50.940]   The discussion of how that term is mostly being used is not about policy ideas.
[00:03:50.940 --> 00:03:55.060]   You're accurately kind of, uh, uh, identifying that.
[00:03:55.060 --> 00:03:59.900]   And it does seem like progressive is no longer being used as a smear and leftist is being
[00:03:59.900 --> 00:04:01.820]   used as a smear more at this point.
[00:04:01.820 --> 00:04:02.820]   Okay.
[00:04:02.820 --> 00:04:04.460]   But sometimes some of these terms are useful.
[00:04:04.460 --> 00:04:11.780]   Like can we try to pick the terms that are useful, like liberal and progressive and Democrat?
[00:04:11.780 --> 00:04:13.380]   Liberal and progressive.
[00:04:13.380 --> 00:04:17.860]   Is there an interesting definable distinction between liberal and progressive to you?
[00:04:17.860 --> 00:04:20.540]   That's a, maybe one of the most interesting ones.
[00:04:20.540 --> 00:04:24.600]   10 years ago, liberal often meant progress.
[00:04:24.600 --> 00:04:27.540]   What now we mean by progressive.
[00:04:27.540 --> 00:04:34.900]   More recently, the progressive socialist leaning part of the political spectrum has started
[00:04:34.900 --> 00:04:42.680]   to use liberal to mean Joe Biden, to mean someone who is not really left enough.
[00:04:42.680 --> 00:04:46.540]   So liberals, very interesting because I remember talking with my audience years ago, maybe
[00:04:46.540 --> 00:04:51.020]   eight years ago or something like that, where I identified, I'm going to now use the term
[00:04:51.020 --> 00:04:56.580]   progressive more commonly to describe my own beliefs because liberal has now been made
[00:04:56.580 --> 00:04:57.580]   a smear.
[00:04:57.580 --> 00:04:59.540]   It's being shifted into something else.
[00:04:59.540 --> 00:05:02.980]   And it also means more of like a center left politics.
[00:05:02.980 --> 00:05:10.100]   So it's changed in some sense by necessity, by force, and also because the spectrum has
[00:05:10.100 --> 00:05:11.660]   shifted to some degree.
[00:05:11.660 --> 00:05:20.100]   So the term liberal has evolved now liberal meaning some kind of embodiment of the mainstream
[00:05:20.100 --> 00:05:22.100]   Democratic Party almost.
[00:05:22.100 --> 00:05:23.100]   To some degree.
[00:05:23.100 --> 00:05:28.380]   Sometimes I'm called, I'm written off by, you know, within my space, there are all sorts
[00:05:28.380 --> 00:05:33.740]   of shades of gray, which I'm sure we can talk about, about where I am versus should be,
[00:05:33.740 --> 00:05:36.220]   could be, or am wrongly placed.
[00:05:36.220 --> 00:05:42.580]   And sometimes an attack on me is he's just a lib, meaning I'm not left enough.
[00:05:42.580 --> 00:05:46.900]   I'm not progressive socialist wherever else you want to go.
[00:05:46.900 --> 00:05:51.740]   So yeah, the, the problem with a lot of these terms and they're used very casually by people
[00:05:51.740 --> 00:05:56.940]   who call into my show is that unless we actually define them each time, they very often mean
[00:05:56.940 --> 00:06:00.740]   very different things to different people and often come with an agenda attached to
[00:06:00.740 --> 00:06:01.740]   them.
[00:06:01.740 --> 00:06:06.380]   And so I find that they often stifle meaningful conversation rather than encourage it.
[00:06:06.380 --> 00:06:12.020]   Do you sense that there's a drifting of a, what is the threshold to be a progressive
[00:06:12.020 --> 00:06:19.020]   or is there, should we use progressive synonymously with a democratic socialist?
[00:06:19.020 --> 00:06:22.500]   I think we should not use it synonymously with democratic socialist.
[00:06:22.500 --> 00:06:27.720]   And this is where there's another linguistic confusion and a political confusion.
[00:06:27.720 --> 00:06:34.340]   So we'll first talk about the linguistic one, social democracy versus democratic socialism.
[00:06:34.340 --> 00:06:37.500]   Very similar words in a different order.
[00:06:37.500 --> 00:06:38.580]   Okay.
[00:06:38.580 --> 00:06:46.420]   My, the way I operate is democratic socialism is actually a form of socialism where one
[00:06:46.420 --> 00:06:49.760]   would seek to socialize ownership of the means of production.
[00:06:49.760 --> 00:06:55.800]   As an example, social democracy is a very, a highly regulated form of capitalism.
[00:06:55.800 --> 00:06:59.880]   The likes of which we would see in Northern Europe, Denmark, et cetera.
[00:06:59.880 --> 00:07:01.620]   These are very different things.
[00:07:01.620 --> 00:07:08.960]   I associate progressivism in 2023 with social democracy and would consider democratic socialism
[00:07:08.960 --> 00:07:11.360]   a form of actual socialism that is different.
[00:07:11.360 --> 00:07:16.120]   It is, we're no longer talking about a capitalist organization of society.
[00:07:16.120 --> 00:07:22.560]   So transition from one to the other is a fundamental shift in house, in how society operates then.
[00:07:22.560 --> 00:07:23.640]   Absolutely.
[00:07:23.640 --> 00:07:29.420]   And when you talk about social democracy, you're talking about socializing a couple
[00:07:29.420 --> 00:07:36.040]   more things than we could socialize in most modern capitalist countries.
[00:07:36.040 --> 00:07:39.360]   I had this conversation with Patrick bet David recently.
[00:07:39.360 --> 00:07:40.760]   Social democracy is okay.
[00:07:40.760 --> 00:07:44.240]   We've socialized the military already in the United States.
[00:07:44.240 --> 00:07:49.700]   We've socialized some healthcare in the sense of like the VA and Medicaid, et cetera.
[00:07:49.700 --> 00:07:55.880]   We're talking about socializing a couple more things still in a capitalist country.
[00:07:55.880 --> 00:07:58.680]   Democratic socialism would be something beyond that.
[00:07:58.680 --> 00:08:03.680]   And, and as someone who is not a democratic socialist myself, I'm, I'm maybe not the best
[00:08:03.680 --> 00:08:09.580]   advocate for explaining exactly how that system would function, but it would have some version
[00:08:09.580 --> 00:08:13.540]   of socializing ownership of the means of production, businesses, et cetera.
[00:08:13.540 --> 00:08:20.040]   So you mentioned you appeared on the PBD podcast with Patrick bet David.
[00:08:20.040 --> 00:08:22.040]   The debate was pretty intense.
[00:08:22.040 --> 00:08:25.440]   I should say I personally enjoyed it.
[00:08:25.440 --> 00:08:30.760]   I thought actually you did well and I thought Patrick did well and it was a good conversation.
[00:08:30.760 --> 00:08:35.760]   I mean, there was a little bit of tension and I thought that Patrick actually, so I
[00:08:35.760 --> 00:08:36.760]   disagree with the internet.
[00:08:36.760 --> 00:08:39.120]   I thought Patrick just took on a kind of devil's advocate.
[00:08:39.120 --> 00:08:43.880]   Like he was, he was purposely being stubborn to bring out the best in you.
[00:08:43.880 --> 00:08:48.320]   But the internet thought that he's being stubborn, not being open to your ideas.
[00:08:48.320 --> 00:08:53.680]   I thought the tension between ideas, um, I think a lot of the tension had to do probably
[00:08:53.680 --> 00:08:57.200]   with Donald Trump and Trump supporters.
[00:08:57.200 --> 00:08:58.760]   That certainly could be the case.
[00:08:58.760 --> 00:09:02.280]   And people wrote to me after people wrote to me the full gamut of everything you can
[00:09:02.280 --> 00:09:07.140]   imagine from this was your best thing you've ever done in public to you got humiliated
[00:09:07.140 --> 00:09:08.800]   and your mother should have aborted you.
[00:09:08.800 --> 00:09:09.800]   Okay.
[00:09:09.800 --> 00:09:10.800]   So every and everything in between.
[00:09:10.800 --> 00:09:11.960]   So, you know, take your pick.
[00:09:11.960 --> 00:09:17.600]   But um, the most interesting feedback I got was from people who asked me after was it
[00:09:17.600 --> 00:09:23.160]   incredibly tense and awkward and because it seemed so combative and I think for, I'm so
[00:09:23.160 --> 00:09:28.000]   used to those types of tensions in the discussions that I have that it's very comfortable to
[00:09:28.000 --> 00:09:29.000]   me.
[00:09:29.000 --> 00:09:32.060]   It's not like afterwards it's, it's, there's a grudge or it's tense or whatever the case
[00:09:32.060 --> 00:09:33.060]   may be.
[00:09:33.060 --> 00:09:34.060]   I'm very comfortable.
[00:09:34.060 --> 00:09:36.040]   Just, I, I disagree with people and that's it.
[00:09:36.040 --> 00:09:38.320]   So I did not find anything that happened inappropriate.
[00:09:38.320 --> 00:09:40.200]   I disagreed with a lot of the things he said.
[00:09:40.200 --> 00:09:41.200]   Certainly.
[00:09:41.200 --> 00:09:43.560]   Uh, so you also spoke on Michael Knowles.
[00:09:43.560 --> 00:09:47.000]   Um, I think about the idea of what is a woman.
[00:09:47.000 --> 00:09:52.440]   Do you, can you speak broadly about your conversations with people you disagree with?
[00:09:52.440 --> 00:09:56.920]   Uh, you know, some of the cases it feels like it's gone wrong.
[00:09:56.920 --> 00:09:58.680]   The conversations have gone wrong.
[00:09:58.680 --> 00:09:59.680]   Yeah.
[00:09:59.680 --> 00:10:00.680]   Yeah.
[00:10:00.680 --> 00:10:04.720]   I mean, I think there's a couple of different things and I'm the first to tell you that
[00:10:04.720 --> 00:10:11.200]   depending on who I'm talking to, I go in with a different attitude about how quote seriously
[00:10:11.200 --> 00:10:16.120]   I'm taking it in the sense of whether I think it's going to be a deep policy discussion
[00:10:16.120 --> 00:10:20.880]   versus where whether it's going to be more of a performance for an audience that is expecting
[00:10:20.880 --> 00:10:21.880]   a certain thing.
[00:10:21.880 --> 00:10:23.360]   And I think there's different types of shows.
[00:10:23.360 --> 00:10:27.240]   When I was interviewed by this guy, Jesse Lee Peterson in Los Angeles, it's very different.
[00:10:27.240 --> 00:10:32.040]   For example, than when I'm talking to Patrick bet David, just to give two, two examples.
[00:10:32.040 --> 00:10:37.600]   I think the reason I stopped doing the Michael Knowles show was the number of threats I would
[00:10:37.600 --> 00:10:38.800]   get after the fact.
[00:10:38.800 --> 00:10:42.360]   That's really the re, I was glad to engage with him to the extent that the interviews
[00:10:42.360 --> 00:10:46.040]   were interesting and you know, we could organize it reasonably efficiently.
[00:10:46.040 --> 00:10:50.480]   Um, but the reason I stepped away was sort of the aftermath.
[00:10:50.480 --> 00:10:57.360]   But I did find him to be someone who was abundantly clear about his view and where he comes from.
[00:10:57.360 --> 00:11:01.560]   And while I could not possibly disagree more with him in terms of politics and culture
[00:11:01.560 --> 00:11:04.440]   and our backgrounds, everything is just so, so different.
[00:11:04.440 --> 00:11:10.160]   I found it easy to engage in the conversation just because of how upfront and clear he was
[00:11:10.160 --> 00:11:12.460]   about what his beliefs were.
[00:11:12.460 --> 00:11:16.320]   But the number of threats, yeah, yeah, it was just too much.
[00:11:16.320 --> 00:11:21.040]   And this, um, you know, I don't know how much you saw about this recent Twitter dust up
[00:11:21.040 --> 00:11:26.440]   I was involved in that peaked with Donald Trump Jr tweeting about me and then that then
[00:11:26.440 --> 00:11:28.000]   declining from there.
[00:11:28.000 --> 00:11:29.000]   Let's talk through it.
[00:11:29.000 --> 00:11:30.000]   I didn't see it.
[00:11:30.000 --> 00:11:32.000]   I have to understand like a way you study Shakespeare.
[00:11:32.000 --> 00:11:33.360]   I have to study your Twitter.
[00:11:33.360 --> 00:11:36.720]   I have to understand some, how much of it is sarcasm.
[00:11:36.720 --> 00:11:37.720]   It's mostly sarcasm.
[00:11:37.720 --> 00:11:40.880]   I mean, here's the thing, and I know that there are people who will say, David, you're
[00:11:40.880 --> 00:11:43.120]   dealing with such serious issues.
[00:11:43.120 --> 00:11:47.400]   It's really not okay not to take everything you do completely seriously.
[00:11:47.400 --> 00:11:53.620]   But my view is it's so incredible that I've between chance and timing and so different
[00:11:53.620 --> 00:11:58.780]   things fallen into a position where this is what I do professionally and it's a career
[00:11:58.780 --> 00:12:01.920]   and it's financially sustainable and all these different things.
[00:12:01.920 --> 00:12:07.720]   I don't want to end up taking myself too seriously because I recognize the timing and lock and
[00:12:07.720 --> 00:12:08.720]   all of these other things.
[00:12:08.720 --> 00:12:11.040]   And this could have gone a completely different way.
[00:12:11.040 --> 00:12:15.400]   So my approach to a lot of this is let's not take ourselves too seriously.
[00:12:15.400 --> 00:12:19.560]   And in particular on Twitter, a platform that, you know, the degree to which it should be
[00:12:19.560 --> 00:12:23.360]   taken very seriously, maybe has changed over time.
[00:12:23.360 --> 00:12:27.480]   I'm always sort of thinking a little bit tongue in cheek on Twitter.
[00:12:27.480 --> 00:12:31.600]   What happened with Donald Trump Jr. or the full arc of it?
[00:12:31.600 --> 00:12:32.600]   Yeah.
[00:12:32.600 --> 00:12:36.540]   To give you a one minute arc and then we can pick whichever parts we want.
[00:12:36.540 --> 00:12:40.440]   After a mass shooting, now you might say there's like two or three a day.
[00:12:40.440 --> 00:12:41.560]   You're correct.
[00:12:41.560 --> 00:12:49.720]   After the Nashville mass shooting at a Christian school, I tweeted snarkily tongue in cheek
[00:12:49.720 --> 00:12:56.560]   to point that thoughts and prayers not only aren't particularly useful after a shooting,
[00:12:56.560 --> 00:13:02.560]   they also don't prevent shootings, that there's some confusion about how there would be a
[00:13:02.560 --> 00:13:07.560]   shooting at a Christian school, given that it is a place where prayer is taking place.
[00:13:07.560 --> 00:13:12.480]   I think I jokingly said something like, were they not praying enough or correctly?
[00:13:12.480 --> 00:13:17.880]   In my deep journalistic integrity, I have your tweet.
[00:13:17.880 --> 00:13:23.480]   This is the only display of journalistic integrity I will show today.
[00:13:23.480 --> 00:13:25.440]   And I have a couple of responses.
[00:13:25.440 --> 00:13:27.280]   And you deleted the tweet since then.
[00:13:27.280 --> 00:13:28.280]   Which I regret.
[00:13:28.280 --> 00:13:29.280]   Oh, interesting.
[00:13:29.280 --> 00:13:30.280]   And we can talk about that.
[00:13:30.280 --> 00:13:35.160]   I would love to, because it's such an interesting decision.
[00:13:35.160 --> 00:13:39.080]   Because when you tweet something, one of the things I've also learned is you don't often
[00:13:39.080 --> 00:13:42.880]   understand how it's going to be read.
[00:13:42.880 --> 00:13:46.640]   It's going to be analyzed, like I mentioned, Shakespeare.
[00:13:46.640 --> 00:13:51.800]   The use of certain words that you regret saying in a certain kind of way, maybe just because
[00:13:51.800 --> 00:13:58.080]   it wasn't as eloquent, as powerful, it didn't actually convey the thing, or it's a distraction
[00:13:58.080 --> 00:13:59.960]   to the main message, all that kind of stuff.
[00:13:59.960 --> 00:14:04.000]   Okay, the actual tweet is, "Very surprising that there would be a mass shooting at a Christian
[00:14:04.000 --> 00:14:09.240]   school, given that lack of prayer is often blamed for these horrible events.
[00:14:09.240 --> 00:14:15.600]   Is it possible they weren't praying enough or correctly, despite being a Christian school?"
[00:14:15.600 --> 00:14:25.960]   And a lot of people quote retweeted that, which I'm assuming was criticism.
[00:14:25.960 --> 00:14:30.760]   So Colin Wright wrote, "I used to consider you a reasonable progressive, but you clearly
[00:14:30.760 --> 00:14:32.560]   devolved into partisan hackery.
[00:14:32.560 --> 00:14:33.800]   I'm an atheist.
[00:14:33.800 --> 00:14:37.800]   It cannot begin to fathom using the murder of children and adults at a Christian school
[00:14:37.800 --> 00:14:41.000]   as an opportunity to dunk on the concept of prayer."
[00:14:41.000 --> 00:14:45.100]   And you responded, "I'm dunking on the people who send thoughts and prayers and do nothing
[00:14:45.100 --> 00:14:47.200]   else and the shootings continue."
[00:14:47.200 --> 00:14:49.600]   Okay, I'm sure there's a lot of other interactions.
[00:14:49.600 --> 00:14:52.160]   There's a few other hundred thousand.
[00:14:52.160 --> 00:14:55.080]   So do you want the arc leading to the leading?
[00:14:55.080 --> 00:14:59.280]   So basically, do you know what time of day I tweeted the original one?
[00:14:59.280 --> 00:15:04.120]   I feel like it was in the afternoon or evening on a Monday.
[00:15:04.120 --> 00:15:09.760]   3.42 PM on March 27th.
[00:15:09.760 --> 00:15:11.080]   Which was a Monday, okay.
[00:15:11.080 --> 00:15:14.040]   So basically I tweet that, and then I finish the day.
[00:15:14.040 --> 00:15:17.120]   So you tweet, and then you go on with your day.
[00:15:17.120 --> 00:15:22.880]   I might've looked once at Twitter and it had 2,000 likes and a few people saying, "Eh,
[00:15:22.880 --> 00:15:24.160]   this might've missed the mark."
[00:15:24.160 --> 00:15:28.220]   But it's sort of like, it's one of my 20,000 tweets, I don't know.
[00:15:28.220 --> 00:15:33.160]   I wake up the next morning, my baby daughter did not sleep till 7.30 the way I would like,
[00:15:33.160 --> 00:15:35.680]   so she's up at 6 AM and I get up.
[00:15:35.680 --> 00:15:38.120]   And I'm just there starting to make breakfast.
[00:15:38.120 --> 00:15:40.880]   And I glance at my phone and I'm starting to...
[00:15:40.880 --> 00:15:44.240]   This was when verified meant a different thing than it means now.
[00:15:44.240 --> 00:15:50.280]   I'm seeing all these verified accounts that are quote tweeting it and demanding a retraction
[00:15:50.280 --> 00:15:51.280]   and whatever.
[00:15:51.280 --> 00:15:55.920]   And I go, "Uh-oh, okay, this looks like it's getting some attention."
[00:15:55.920 --> 00:15:59.240]   I then continue about my day.
[00:15:59.240 --> 00:16:05.460]   Around noon, I hear from my dad that he got 100 messages from, "You should've aborted
[00:16:05.460 --> 00:16:10.040]   your son," to, "We're gonna find all of you," to whatever else.
[00:16:10.040 --> 00:16:11.320]   My dad has no idea what's going on.
[00:16:11.320 --> 00:16:16.520]   He's like, "I don't know what this is, but I have 100 DMs," to everything else you can
[00:16:16.520 --> 00:16:18.600]   imagine.
[00:16:18.600 --> 00:16:27.320]   And I start to get emails about your Jewish faith, this and that, and the other thing.
[00:16:27.320 --> 00:16:32.000]   And so at that point, to me, I thought, "This is just going to get worse and worse and worse."
[00:16:32.000 --> 00:16:33.700]   And so I deleted the tweet.
[00:16:33.700 --> 00:16:39.320]   And I really regret doing that because over the 48 hours that followed, yes, the attacks
[00:16:39.320 --> 00:16:40.360]   escalated.
[00:16:40.360 --> 00:16:46.040]   It went through Candace Owens, and then at FoxNews.com, Newsmax, kind of peaking with
[00:16:46.040 --> 00:16:47.880]   Donald Trump Jr.
[00:16:47.880 --> 00:16:48.880]   And it was horrible.
[00:16:48.880 --> 00:16:52.200]   I mean, thousands and thousands of the...
[00:16:52.200 --> 00:16:53.200]   Okay.
[00:16:53.200 --> 00:16:59.560]   But once I told my audience about what happened, I got thousands of messages from people saying,
[00:16:59.560 --> 00:17:06.240]   "David, only someone who doesn't know you and is determined to interpret this in the
[00:17:06.240 --> 00:17:12.000]   worst possible faith would think you're blaming kids who died for getting shot.
[00:17:12.000 --> 00:17:13.440]   Of course you weren't doing that.
[00:17:13.440 --> 00:17:19.360]   I wish you hadn't deleted it so that it would still be up and you would now see the tide
[00:17:19.360 --> 00:17:21.360]   kind of turning on it."
[00:17:21.360 --> 00:17:27.720]   This was not a fun three days regardless, but I do regret having deleted it because
[00:17:27.720 --> 00:17:28.720]   it was a...
[00:17:28.720 --> 00:17:34.120]   I wanted to do the quickest thing I thought I could to get people to stop trying to find
[00:17:34.120 --> 00:17:36.560]   family members and send them threats.
[00:17:36.560 --> 00:17:38.840]   And so around noon, that's what I did.
[00:17:38.840 --> 00:17:42.880]   And the truth is the threats didn't stop anyway because everybody had screenshotted it.
[00:17:42.880 --> 00:17:45.400]   And I do wish I had left it up.
[00:17:45.400 --> 00:17:51.920]   Is there some degree, maybe stepping outside yourself, do you regret tweeting that in that
[00:17:51.920 --> 00:17:57.080]   it feeds the mockery engine that fuels Twitter?
[00:17:57.080 --> 00:18:02.560]   So does that tweet really represent what you believe?
[00:18:02.560 --> 00:18:11.600]   It absolutely represents the disgust with a politics that includes saying we can't touch
[00:18:11.600 --> 00:18:12.600]   guns.
[00:18:12.600 --> 00:18:18.420]   We just, we can't, but we're willing to point to mental health or say we need more prayer
[00:18:18.420 --> 00:18:20.120]   in schools or whatever.
[00:18:20.120 --> 00:18:23.820]   1000% it represents that view.
[00:18:23.820 --> 00:18:29.260]   Is it the type of snark and sarcasm that I would use if given an hour to discuss the
[00:18:29.260 --> 00:18:32.880]   topic rather than whatever the number of characters is now on Twitter?
[00:18:32.880 --> 00:18:33.880]   No, definitely not.
[00:18:33.880 --> 00:18:41.280]   And so I am very cognizant of the fact that it was unnecessarily provocative how it was
[00:18:41.280 --> 00:18:42.280]   written.
[00:18:42.280 --> 00:18:46.200]   I think I asked a similar question to Ben Shapiro.
[00:18:46.200 --> 00:18:52.600]   Do you worry that this style of presentation can turn you from being a deeply thoughtful,
[00:18:52.600 --> 00:19:01.200]   subjective political thinker to somebody who's just a partisan hack or partisan, what's
[00:19:01.200 --> 00:19:02.280]   a good word?
[00:19:02.280 --> 00:19:03.280]   Talking head?
[00:19:03.280 --> 00:19:06.840]   Do you mean with regard to Twitter or the format of my show in general?
[00:19:06.840 --> 00:19:09.160]   So Twitter for now, let's start with Twitter for now.
[00:19:09.160 --> 00:19:17.000]   And can you silo your style of communication on Twitter from being a virus that affects
[00:19:17.000 --> 00:19:18.000]   your mind?
[00:19:18.000 --> 00:19:19.000]   Right.
[00:19:19.000 --> 00:19:25.000]   I don't have deep thoughts about the Twitter component beyond, I think across all sorts
[00:19:25.000 --> 00:19:26.000]   of disciplines.
[00:19:26.000 --> 00:19:31.920]   This is not the best way to most effectively solve problems and figure out solutions to
[00:19:31.920 --> 00:19:32.920]   complex issues.
[00:19:32.920 --> 00:19:35.240]   You're talking about Twitter now, right now I'm talking about Twitter.
[00:19:35.240 --> 00:19:36.240]   Yes.
[00:19:36.240 --> 00:19:43.520]   That being said, I think all of us to some degree have to adapt our content to the platform
[00:19:43.520 --> 00:19:46.960]   that we're using in the same way that what I post to YouTube is different than what I
[00:19:46.960 --> 00:19:48.640]   post to tick tock.
[00:19:48.640 --> 00:19:51.500]   What I post to Twitter is also different.
[00:19:51.500 --> 00:19:56.600]   Do I think Twitter has been an unmitigated good for society?
[00:19:56.600 --> 00:19:57.920]   No.
[00:19:57.920 --> 00:20:03.200]   Have I chosen to step into Twitter as one of the ways in which I get my message out
[00:20:03.200 --> 00:20:05.080]   with the good and the bad?
[00:20:05.080 --> 00:20:06.080]   Yes.
[00:20:06.080 --> 00:20:10.400]   And I think that there is a deep conversation to be had there.
[00:20:10.400 --> 00:20:14.320]   I think zooming out a little bit in terms of what I do, and I was hoping this would
[00:20:14.320 --> 00:20:16.680]   come up because I think it's really interesting.
[00:20:16.680 --> 00:20:20.200]   I will often get emails from people who say two things.
[00:20:20.200 --> 00:20:24.560]   I will get the, you would have such a bigger audience if you did X type emails.
[00:20:24.560 --> 00:20:32.040]   And usually they are plays to sensationalism, salacious and titillating content, more pop
[00:20:32.040 --> 00:20:34.800]   culture stuff, et cetera.
[00:20:34.800 --> 00:20:39.440]   On the other hand, it's folks who say, listen, what you're doing really isn't as serious
[00:20:39.440 --> 00:20:41.060]   as it could be.
[00:20:41.060 --> 00:20:46.280]   And it seems like you could do something more serious and you should consider doing deep
[00:20:46.280 --> 00:20:47.280]   dives.
[00:20:47.280 --> 00:20:49.720]   You know, once it was do a deep dive into Calvin Coolidge and I was like, nobody will
[00:20:49.720 --> 00:20:50.720]   watch that.
[00:20:50.720 --> 00:20:56.600]   So there, it's not by accident that my show is the way it is right in an hour.
[00:20:56.600 --> 00:21:01.800]   I'm thinking of all the platforms I'm on and I'm saying, okay, I want to do a relatively
[00:21:01.800 --> 00:21:04.960]   deep dive on the federal budget.
[00:21:04.960 --> 00:21:10.280]   And I want to talk about some of the, um, uh, political tomfoolery going on within the
[00:21:10.280 --> 00:21:11.280]   post office.
[00:21:11.280 --> 00:21:16.420]   And I'm going to do a segment about the wacky rally where Trump said crazy things and made
[00:21:16.420 --> 00:21:19.320]   up three words and said he endorsed a candidate who's named it.
[00:21:19.320 --> 00:21:20.320]   Right.
[00:21:20.320 --> 00:21:25.720]   I'm crafting that in total to find a balance between let's build this audience as much
[00:21:25.720 --> 00:21:31.840]   as I can in order to have a bigger base to get my message out there and include the more
[00:21:31.840 --> 00:21:35.360]   serious stuff with the hope that there's a little bit of something for everyone.
[00:21:35.360 --> 00:21:39.560]   And I'm finding a balance between those two sides of the spectrum.
[00:21:39.560 --> 00:21:44.320]   It's a deliberate thing and I'm aware that if I were producing my show 50 years ago,
[00:21:44.320 --> 00:21:48.340]   the balance would probably be different and it would probably change again if we didn't.
[00:21:48.340 --> 00:21:52.920]   If the show was audio only rather than having all these video platforms, it would also be
[00:21:52.920 --> 00:21:53.920]   different.
[00:21:53.920 --> 00:21:59.560]   But it's a decision that's proactively made to try to get the best and most out of the
[00:21:59.560 --> 00:22:01.160]   hour that I'm creating every day.
[00:22:01.160 --> 00:22:07.080]   Well, it just feels like there's an entire machine fed by Twitter and journalism that
[00:22:07.080 --> 00:22:14.800]   wants to divide people and the drama of that division, highlighting the partisan division,
[00:22:14.800 --> 00:22:20.200]   the drama of that division feels like it's a tension with objective, clear thinking sometimes.
[00:22:20.200 --> 00:22:26.040]   And so that's the, I worry that there's a drug to it.
[00:22:26.040 --> 00:22:33.560]   There's too much fun to mock ridiculous people on the other side.
[00:22:33.560 --> 00:22:35.640]   I think you're right about that.
[00:22:35.640 --> 00:22:43.960]   And the fact that that is true to me supports, I've talked with my audience about, you know,
[00:22:43.960 --> 00:22:47.560]   like the old food pyramid, which I guess was like wrong, but let's imagine that there was
[00:22:47.560 --> 00:22:49.800]   a pyramid that made sense.
[00:22:49.800 --> 00:22:51.040]   The bottom bread.
[00:22:51.040 --> 00:22:53.560]   I think like whole grains maybe.
[00:22:53.560 --> 00:22:54.560]   I don't remember.
[00:22:54.560 --> 00:22:55.560]   It's been a while.
[00:22:55.560 --> 00:22:56.560]   Sugar is on the top.
[00:22:56.560 --> 00:22:57.560]   Okay.
[00:22:57.560 --> 00:22:58.560]   Junk food is at the very top.
[00:22:58.560 --> 00:23:00.400]   I'm very open with my audience.
[00:23:00.400 --> 00:23:05.400]   The vast majority of what I do is the top of that pyramid.
[00:23:05.400 --> 00:23:13.020]   And I tell people very openly, I don't consume a lot of the type of content I produce.
[00:23:13.020 --> 00:23:19.720]   And I think it's really important to, as a base, be doing critical thinking, epistemology,
[00:23:19.720 --> 00:23:24.840]   how do we believe the things we believe, basics about the world.
[00:23:24.840 --> 00:23:30.040]   After that, reading history, economics, philosophy, et cetera.
[00:23:30.040 --> 00:23:32.360]   After that, now we're getting into current events.
[00:23:32.360 --> 00:23:41.480]   I would mostly be looking at consuming, um, primary source reporting things like associated
[00:23:41.480 --> 00:23:42.720]   press, whatever.
[00:23:42.720 --> 00:23:45.760]   I know everybody will have a different list of what counts there.
[00:23:45.760 --> 00:23:51.500]   After that is when I'd say indulge in some of the commentary type stuff that I do.
[00:23:51.500 --> 00:23:56.960]   If you find that I'm thoughtful enough to make it into that, but I'm very open.
[00:23:56.960 --> 00:24:02.280]   And really what I try to do on my show often is in being that at the top of the pyramid,
[00:24:02.280 --> 00:24:07.400]   tell people there's all this other stuff that should be forming your foundation that I hope
[00:24:07.400 --> 00:24:09.800]   you're consuming in addition to just watching me.
[00:24:09.800 --> 00:24:12.140]   And I'm very open with my audience about that.
[00:24:12.140 --> 00:24:17.400]   - What about the shape, the dynamics, the characteristics of your audience?
[00:24:17.400 --> 00:24:26.280]   Is there some degree to which you're through mocking maybe Republicans that there's a lean
[00:24:26.280 --> 00:24:29.000]   to that audience and then you become captured by the audience.
[00:24:29.000 --> 00:24:31.160]   Do you worry about the audience capture?
[00:24:31.160 --> 00:24:33.040]   - I worry about it.
[00:24:33.040 --> 00:24:41.600]   I'm relatively comfortable that it's not shaping the program to a great degree in the sense
[00:24:41.600 --> 00:24:47.480]   that at this point I have a pretty good sense of the things I can say that will upset what
[00:24:47.480 --> 00:24:49.000]   I might call my core audience.
[00:24:49.000 --> 00:24:53.920]   You know, one of the interesting things just to briefly go back to the Twitter thing was
[00:24:53.920 --> 00:24:58.200]   those people who were furious with me on Twitter and they contacted my advertisers and some
[00:24:58.200 --> 00:25:02.160]   advertisers dropped me and on and on and on.
[00:25:02.160 --> 00:25:04.680]   None of them are actually in my audience.
[00:25:04.680 --> 00:25:07.320]   None of them are regular consumers of my audience.
[00:25:07.320 --> 00:25:11.400]   They were kind of weaponized against me by people who said, Hey, look at this.
[00:25:11.400 --> 00:25:15.380]   The people who follow Candace Owens on Twitter, other than for their kind of shock value,
[00:25:15.380 --> 00:25:17.360]   they're not in my audience.
[00:25:17.360 --> 00:25:20.840]   And with my core audience, I know there are things I can talk about that will generate
[00:25:20.840 --> 00:25:23.360]   displeasure.
[00:25:23.360 --> 00:25:27.200]   I guess you could say with my audience, sometimes when I touch the Israeli Palestinian conflict,
[00:25:27.200 --> 00:25:29.600]   that will happen sometimes on vaccines.
[00:25:29.600 --> 00:25:36.100]   There's a portion of my audience that is more generally skeptical of vaccines, sometimes
[00:25:36.100 --> 00:25:43.360]   on some foreign policy issues or, you know, I'm not a big fan of Marianne Williamson nor
[00:25:43.360 --> 00:25:49.660]   Bobby Kennedy Jr's challenges to Joe Biden, not because I love Joe Biden, but because
[00:25:49.660 --> 00:25:51.860]   I don't consider them to be the most serious challengers.
[00:25:51.860 --> 00:25:53.700]   I know there's people in my audience who don't like that.
[00:25:53.700 --> 00:25:57.460]   They get, they get mad at me about that and I'm totally okay with that.
[00:25:57.460 --> 00:26:00.480]   Uh, and that tension with, with my core audience.
[00:26:00.480 --> 00:26:06.320]   So in that sense, I don't feel as though I've had that audience capture take place, but
[00:26:06.320 --> 00:26:11.360]   I know it can happen and I'm very open to, to being told ways in which it may be happening
[00:26:11.360 --> 00:26:12.360]   without me noticing.
[00:26:12.360 --> 00:26:20.140]   Uh, so I've, uh, made a call for questions on Reddit for this conversation.
[00:26:20.140 --> 00:26:23.520]   There's a lot of good questions that I'll probably bring up, but one of them was about
[00:26:23.520 --> 00:26:30.120]   Marianne Williamson, um, asking why David thinks she is a garbage candidate.
[00:26:30.120 --> 00:26:35.880]   Now, which of course I've never said, but perhaps you have more eloquently criticized.
[00:26:35.880 --> 00:26:40.400]   So let's, let's go there to the 2024 election.
[00:26:40.400 --> 00:26:41.400]   Okay.
[00:26:41.400 --> 00:26:44.720]   So Biden, Joe Biden officially announced that he's running again.
[00:26:44.720 --> 00:26:48.560]   Donald Trump officially announced that he's running again.
[00:26:48.560 --> 00:26:52.200]   And if that's the matchup, who do you think wins?
[00:26:52.200 --> 00:26:56.120]   If the elections held today, I think Biden.
[00:26:56.120 --> 00:26:57.120]   Why?
[00:26:57.120 --> 00:27:00.800]   Well, first of all, I believe he won last time.
[00:27:00.800 --> 00:27:07.200]   And if I start with the results from 2020 and I think to myself, what has happened since
[00:27:07.200 --> 00:27:12.800]   then that would push or pull voters one way or the other, I have a hard time making a
[00:27:12.800 --> 00:27:19.480]   case that Trump is in a better position today than he was in November of 2020.
[00:27:19.480 --> 00:27:24.120]   So that's kind of my starting point, which is it's a rematch of an election with a known
[00:27:24.120 --> 00:27:25.640]   outcome.
[00:27:25.640 --> 00:27:27.200]   What has changed?
[00:27:27.200 --> 00:27:33.240]   And I can't make a case for circumstances having changed in Trump's favor to give a
[00:27:33.240 --> 00:27:35.480]   couple of state level examples.
[00:27:35.480 --> 00:27:41.280]   Florida seems to be kind of moving more to the Republican side since 2020, but Trump
[00:27:41.280 --> 00:27:44.160]   won that state already in 2020.
[00:27:44.160 --> 00:27:46.760]   So it wouldn't really change the outcome.
[00:27:46.760 --> 00:27:48.400]   Arizona was close.
[00:27:48.400 --> 00:27:51.080]   I think Arizona has moved to the left since 2020.
[00:27:51.080 --> 00:27:53.720]   So I don't see Trump taking that one.
[00:27:53.720 --> 00:27:56.160]   Wisconsin, I think the same sort of thing applies.
[00:27:56.160 --> 00:28:03.600]   So being very like practical, that would be kind of the start of my reasoning.
[00:28:03.600 --> 00:28:10.280]   Do you think Joe Biden is a better candidate now than he was in 2020?
[00:28:10.280 --> 00:28:14.080]   I think he's a worse candidate.
[00:28:14.080 --> 00:28:19.640]   This is going to sound ageist, but I think he's a worse candidate in that he's even older
[00:28:19.640 --> 00:28:26.640]   and there already seems to be an appetite for younger candidates, particularly on the
[00:28:26.640 --> 00:28:29.480]   Democratic voting side.
[00:28:29.480 --> 00:28:34.360]   So he's going to be four years older and in a sense that could be a liability.
[00:28:34.360 --> 00:28:38.520]   However, he also is going to have four years of accomplishments.
[00:28:38.520 --> 00:28:42.640]   Now, you might not like the things he's done, in which case that would hurt him.
[00:28:42.640 --> 00:28:48.620]   But he has started to accumulate a not insignificant number of accomplishments.
[00:28:48.620 --> 00:28:53.640]   Some of the big things that are known, inflation reduction act and covid stimulus, you know,
[00:28:53.640 --> 00:28:58.240]   but also less well-known things like a bunch of little tweaks to health care, a bunch of
[00:28:58.240 --> 00:29:00.040]   little tweaks to student lending.
[00:29:00.040 --> 00:29:04.420]   There's been a lot of little things at the macro level.
[00:29:04.420 --> 00:29:08.820]   I don't actually think Joe Biden has that much to do with this the same way I didn't
[00:29:08.820 --> 00:29:11.960]   credit or attack Trump for a lot of the macroeconomic stuff.
[00:29:11.960 --> 00:29:14.120]   But inflation has started to come down significantly.
[00:29:14.120 --> 00:29:16.800]   The stock market's quite steady.
[00:29:16.800 --> 00:29:22.980]   These sort of things, I think, looking historically, it's a pretty OK environment for Joe Biden,
[00:29:22.980 --> 00:29:27.940]   with the exception that he was already the oldest president to be inaugurated in 2021.
[00:29:27.940 --> 00:29:31.240]   And he would beat his own record in January of 2025.
[00:29:31.240 --> 00:29:33.840]   And I just don't know how voters are going to see that.
[00:29:33.840 --> 00:29:39.200]   So in terms of just a public human being, how would you compare Trump and Biden?
[00:29:39.200 --> 00:29:46.200]   So if I were to give criticism towards Trump, it would be that he's chaotic, maybe to the
[00:29:46.200 --> 00:29:50.960]   point of being disrespectful to a lot of different groups, to a lot of different ideas, to a
[00:29:50.960 --> 00:29:54.380]   lot of different nations and leaders and all that kind of stuff.
[00:29:54.380 --> 00:30:01.200]   And then the criticism towards Biden would be that he, maybe perhaps because of age or
[00:30:01.200 --> 00:30:07.880]   any other kind of cognitive capabilities, is not really there mentally, as you know,
[00:30:07.880 --> 00:30:11.160]   in the way that perhaps you could say that Barack Obama was there.
[00:30:11.160 --> 00:30:17.440]   Just mentally being able to handle all kinds of aspects of being a public representative
[00:30:17.440 --> 00:30:21.840]   of a nation to the world and to the people of that nation.
[00:30:21.840 --> 00:30:28.280]   So which, in the competition of personality flaws, which do you think is more powerful?
[00:30:28.280 --> 00:30:36.540]   You've laid out fair and I believe accurate assessments of elements of both of those men.
[00:30:36.540 --> 00:30:41.940]   You haven't weighed in on to what degree you value each of those assessments, which is
[00:30:41.940 --> 00:30:47.520]   where I think the kind of meat of this question really is.
[00:30:47.520 --> 00:30:52.280]   I don't see, and I know that, you know, Biden's going to get us into World War Three, World
[00:30:52.280 --> 00:30:55.040]   War Three, that doesn't seem to be happening.
[00:30:55.040 --> 00:31:01.080]   I don't see the Biden deficits you listed, which I agree with you on.
[00:31:01.080 --> 00:31:08.520]   I don't see them as dangerous or threatening to the standing of the United States in this
[00:31:08.520 --> 00:31:15.720]   kind of environment with our Western traditional Western allies and geopolitics, etc.
[00:31:15.720 --> 00:31:22.080]   In the way that the sort of unhinged personality of Trump, combined with his lack of knowledge
[00:31:22.080 --> 00:31:25.800]   about most issues is a threat.
[00:31:25.800 --> 00:31:31.440]   So for me, if those two are the candidates, Biden would be my choice.
[00:31:31.440 --> 00:31:35.200]   Now are there people I would rather see on the Democratic side?
[00:31:35.200 --> 00:31:36.200]   Yes.
[00:31:36.200 --> 00:31:40.240]   If I knew the president would be a Republican, can I think of better options than Trump?
[00:31:40.240 --> 00:31:41.240]   Absolutely.
[00:31:41.240 --> 00:31:45.900]   You know, it's so funny when in 2012 it was Obama versus Romney.
[00:31:45.900 --> 00:31:50.040]   The difference seemed so significant between them.
[00:31:50.040 --> 00:31:54.240]   Looking back, I'm sure I would disagree with Mitt Romney about tax rates and his views
[00:31:54.240 --> 00:31:59.880]   on LGBT or I'm sure I know are different than mine, but it seems without looking at him
[00:31:59.880 --> 00:32:04.840]   with rose colored glasses, so comparatively benign given the four years of Trump.
[00:32:04.840 --> 00:32:06.520]   So that's kind of where I come down.
[00:32:06.520 --> 00:32:12.360]   Even McCain and Obama, the, the, the differences seem quite drastic.
[00:32:12.360 --> 00:32:13.360]   Yeah.
[00:32:13.360 --> 00:32:17.640]   McCain was interesting because Palin as his running mate opened the door to the sort of
[00:32:17.640 --> 00:32:21.920]   cartoonish stuff that we've started to see on the Republican side.
[00:32:21.920 --> 00:32:27.760]   Palin, Trump, Marjorie Taylor Greene, it started going in that direction, which has made the
[00:32:27.760 --> 00:32:31.600]   party a bit of a joke aside from what you believe the tax rate would be.
[00:32:31.600 --> 00:32:32.600]   Right.
[00:32:32.600 --> 00:32:36.680]   You can say taxes are too high, but Jewish space lasers, come on, you know?
[00:32:36.680 --> 00:32:39.640]   So, uh, but, but I agree with you on McCain also.
[00:32:39.640 --> 00:32:42.360]   So go back to the political terms we talked about.
[00:32:42.360 --> 00:32:45.760]   What, where in that spectrum do you place yourself today?
[00:32:45.760 --> 00:32:50.080]   Um, w w which of the label do you think captures your political views?
[00:32:50.080 --> 00:32:54.320]   Progressive social Democrat, um, which, which again is a capitalist.
[00:32:54.320 --> 00:32:56.160]   I own my own business.
[00:32:56.160 --> 00:33:00.120]   I pay the taxes I'm legally required to pay and not a penny more.
[00:33:00.120 --> 00:33:03.360]   And you know, all, all those things, that's where I place myself.
[00:33:03.360 --> 00:33:06.040]   Would you place yourself to the left of Joe Biden?
[00:33:06.040 --> 00:33:07.040]   Yes.
[00:33:07.040 --> 00:33:08.040]   Yes.
[00:33:08.040 --> 00:33:11.680]   Where does the AOC fit into that?
[00:33:11.680 --> 00:33:12.680]   It's a good question.
[00:33:12.680 --> 00:33:15.520]   What do you think about AOC as a candidate?
[00:33:15.520 --> 00:33:17.600]   Do you think she eventually runs?
[00:33:17.600 --> 00:33:25.040]   I think that if she doesn't run into some kind of scandal, and I don't mean scandal
[00:33:25.040 --> 00:33:32.680]   in the sense of some personal impropriety that, you know, but I mean some kind of major
[00:33:32.680 --> 00:33:34.840]   political problem.
[00:33:34.840 --> 00:33:40.000]   It seems that she has the staying power to be an American elected politics for a long
[00:33:40.000 --> 00:33:45.760]   time, whether she would even want to be president versus maybe going to the Senate or being
[00:33:45.760 --> 00:33:47.160]   governor or whatever the case may be.
[00:33:47.160 --> 00:33:52.800]   I have no idea what her ambitions are in that sense, but certainly like policy aside, she
[00:33:52.800 --> 00:34:02.160]   has this combination of charisma, likeability to some, but also something about her personality
[00:34:02.160 --> 00:34:08.400]   that angers the people who don't like her in a way that only fuels her sort of a presence,
[00:34:08.400 --> 00:34:11.720]   which I think applies to Trump as well.
[00:34:11.720 --> 00:34:14.920]   That I do think that she has the potential to be, to have significant staying power in
[00:34:14.920 --> 00:34:15.920]   American politics.
[00:34:15.920 --> 00:34:16.920]   President, I don't know.
[00:34:16.920 --> 00:34:23.320]   Do you think that's the future of political elections and politics in general is people
[00:34:23.320 --> 00:34:27.280]   who are able to skillfully piss off the other side like AOC and Trump did?
[00:34:27.280 --> 00:34:29.040]   I think it's an aspect of it.
[00:34:29.040 --> 00:34:34.640]   I think it's also understanding how to communicate policy ideas.
[00:34:34.640 --> 00:34:39.040]   Trump, I have things I can praise Trump about if we want to get to that segment at some
[00:34:39.040 --> 00:34:43.280]   point, you let me know when that is, but I do think that there are some things Trump
[00:34:43.280 --> 00:34:47.360]   is very good at and this is why it's very hard for me to believe that Ron DeSantis has
[00:34:47.360 --> 00:34:53.120]   what it takes to actually fight Trump in a national primary.
[00:34:53.120 --> 00:34:59.360]   And um, one of those things is Trump has a, even though he often says very strange things
[00:34:59.360 --> 00:35:02.640]   that if you transcribe them, you go, that's what language is that?
[00:35:02.640 --> 00:35:04.320]   That doesn't make any sense whatsoever.
[00:35:04.320 --> 00:35:10.840]   In the moment, the way he relates to, um, adversaries on stage, et cetera, is very good
[00:35:10.840 --> 00:35:14.820]   in that he is very much aware of how it is going to be seen by the audience.
[00:35:14.820 --> 00:35:18.080]   And so that's why a lot of times it's more about, doesn't matter that a word salad came
[00:35:18.080 --> 00:35:22.600]   out of his mouth, how he immediately responded and related to the person who was very good.
[00:35:22.600 --> 00:35:28.560]   So I think that knowing how to be good when clips are shared all the time, often out of
[00:35:28.560 --> 00:35:31.840]   context is extraordinarily important.
[00:35:31.840 --> 00:35:36.520]   Knowing how to use social media, which every election cycle, that means something different,
[00:35:36.520 --> 00:35:39.560]   but understanding how to use social media, very important.
[00:35:39.560 --> 00:35:46.480]   Those things are absolutely so important and whether you're able to do a deep dive on the
[00:35:46.480 --> 00:35:50.680]   deficit, it's certainly useful, but I would say it's a bad thing.
[00:35:50.680 --> 00:35:53.920]   It's becoming less important in terms of figuring out who we want to represent us.
[00:35:53.920 --> 00:35:59.480]   So just lingering on the AOC and then maybe let's throw in Bernie Sanders on that.
[00:35:59.480 --> 00:36:00.480]   Yeah.
[00:36:00.480 --> 00:36:05.840]   So where do you place yourself and how do you do the layout of the land of Bernie Sanders,
[00:36:05.840 --> 00:36:10.280]   AOC, Joe Biden, and David Backman?
[00:36:10.280 --> 00:36:13.840]   My instinct is, and I'm going to answer it.
[00:36:13.840 --> 00:36:20.720]   The thing that makes this tough is Bernie says he's a democratic socialist.
[00:36:20.720 --> 00:36:22.880]   He ran as a social Democrat.
[00:36:22.880 --> 00:36:27.440]   He didn't run on anything that was really socialism.
[00:36:27.440 --> 00:36:31.000]   So I'm going by their public facing platforms.
[00:36:31.000 --> 00:36:34.840]   I've been listening to him for many, many years and all the way back to the Tom Hartman
[00:36:34.840 --> 00:36:40.680]   show and I think using the terms as you've been using them, he has, I don't think ever
[00:36:40.680 --> 00:36:42.560]   been a democratic socialist.
[00:36:42.560 --> 00:36:46.200]   I haven't heard him speak about socialism.
[00:36:46.200 --> 00:36:53.440]   I think I've heard him speak about social programs and the value of social programs
[00:36:53.440 --> 00:36:58.240]   throughout the history of the United States and how they've been beneficial.
[00:36:58.240 --> 00:37:02.080]   My understanding is very similar to yours, although there may be stuff from the seventies
[00:37:02.080 --> 00:37:04.200]   where he really was talking about bonafides.
[00:37:04.200 --> 00:37:06.720]   We all did some shit in the seventies.
[00:37:06.720 --> 00:37:10.720]   You and I even who weren't around, we were doing stuff in the seventies.
[00:37:10.720 --> 00:37:17.620]   My sense would be, Biden is like center left and then I'm to the left of that, but maybe
[00:37:17.620 --> 00:37:22.960]   just inside of where AOC and Bernie are very, very similar to Bernie.
[00:37:22.960 --> 00:37:28.620]   I mean, I identify with a lot of Bernie's ideas, maybe their implementation.
[00:37:28.620 --> 00:37:33.600]   I'm more flexible on, I'll give you one example, Medicare for all.
[00:37:33.600 --> 00:37:38.700]   One way of trying to get healthcare to everybody, which Bernie's very big on is you take the
[00:37:38.700 --> 00:37:42.880]   current Medicare program, you just eliminate the age limit, make it available to everybody,
[00:37:42.880 --> 00:37:44.720]   pay for it through taxation.
[00:37:44.720 --> 00:37:45.720]   Interesting.
[00:37:45.720 --> 00:37:52.200]   However, I'm open to other models if they get everybody healthcare that is good quality
[00:37:52.200 --> 00:37:53.520]   and affordable.
[00:37:53.520 --> 00:37:55.720]   Singapore has an interesting model.
[00:37:55.720 --> 00:37:57.400]   Germany has an interesting model.
[00:37:57.400 --> 00:38:02.880]   I am more agnostic about how we do it than just saying, let's expand Medicare.
[00:38:02.880 --> 00:38:07.600]   Whether that puts me to the right of Bernie, I don't know, but I'm not like exactly right
[00:38:07.600 --> 00:38:09.440]   there on it has to be Medicare for all.
[00:38:09.440 --> 00:38:10.440]   Yeah.
[00:38:10.440 --> 00:38:15.080]   That's more of a, that's more just flexibility versus dogmatism.
[00:38:15.080 --> 00:38:17.000]   So I don't know if that puts you to the left or to the right.
[00:38:17.000 --> 00:38:18.000]   I don't either.
[00:38:18.000 --> 00:38:23.720]   What do you think about the, we could term manipulation or the corruption in the DNC
[00:38:23.720 --> 00:38:27.560]   that perhaps tipped the scales against Bernie in the election?
[00:38:27.560 --> 00:38:29.240]   Do you think there was such a thing?
[00:38:29.240 --> 00:38:32.240]   In 2016 or 2020?
[00:38:32.240 --> 00:38:35.720]   Both, I would say.
[00:38:35.720 --> 00:38:41.320]   In different, the dynamics there were different with Hillary Clinton and the pressure from
[00:38:41.320 --> 00:38:43.360]   Hillary Clinton as a candidate and so on.
[00:38:43.360 --> 00:38:44.360]   Yeah.
[00:38:44.360 --> 00:38:47.800]   So was there, why didn't Bernie win?
[00:38:47.800 --> 00:38:50.120]   I guess is one, one way to ask.
[00:38:50.120 --> 00:38:51.120]   Okay.
[00:38:51.120 --> 00:38:52.720]   I think there's a couple of things here.
[00:38:52.720 --> 00:38:57.880]   First the DNC, I'm not a Democrat, just your audience may not know.
[00:38:57.880 --> 00:38:59.120]   I'm just a independent.
[00:38:59.120 --> 00:39:00.120]   Yeah.
[00:39:00.120 --> 00:39:03.560]   I mostly vote for candidates that end up being Democrats in local elections.
[00:39:03.560 --> 00:39:05.160]   Often there's no party designation.
[00:39:05.160 --> 00:39:06.160]   So, okay.
[00:39:06.160 --> 00:39:07.160]   I'm obviously on the left.
[00:39:07.160 --> 00:39:10.320]   I'm not denying that, but the democratic party as an institution has never really been interesting
[00:39:10.320 --> 00:39:11.320]   to me.
[00:39:11.320 --> 00:39:15.000]   So you're not a rebel that resist belonging to any institution.
[00:39:15.000 --> 00:39:16.000]   Exactly right.
[00:39:16.000 --> 00:39:17.000]   Exactly right.
[00:39:17.000 --> 00:39:19.560]   And whether it matters, I don't know.
[00:39:19.560 --> 00:39:26.640]   The DNC and the RNC really are organizations that to some degree exist to justify their
[00:39:26.640 --> 00:39:32.300]   own existence because if they were no longer necessary, they would go away.
[00:39:32.300 --> 00:39:36.760]   And so they have to assert their value and their importance.
[00:39:36.760 --> 00:39:41.720]   They do this in a number of different ways, organizing the way that the nominee is chosen,
[00:39:41.720 --> 00:39:46.680]   the convention, uh, working with States on everything from redistricting to whatever
[00:39:46.680 --> 00:39:51.100]   else the case may be, setting the order of primaries and having some involvement in how
[00:39:51.100 --> 00:39:52.960]   that's all going to happen.
[00:39:52.960 --> 00:39:55.880]   And also coordinating behind the scenes.
[00:39:55.880 --> 00:40:01.680]   Uh, I guess they would describe it as making sure our candidates don't get in each other's
[00:40:01.680 --> 00:40:02.680]   ways.
[00:40:02.680 --> 00:40:07.400]   And we might see it and say they're picking the, the winner.
[00:40:07.400 --> 00:40:11.200]   There's nothing illegal about them being involved in picking the winner, but we might say it's
[00:40:11.200 --> 00:40:12.920]   not in people's interests.
[00:40:12.920 --> 00:40:16.400]   I think the 2020 primary was really interesting.
[00:40:16.400 --> 00:40:18.840]   Bernie supporter myself.
[00:40:18.840 --> 00:40:24.720]   I started telling my audience after a couple primaries and even before based on polling
[00:40:24.720 --> 00:40:32.260]   and different things, I see a real uphill battle here for Bernie and it's really important.
[00:40:32.260 --> 00:40:36.620]   People in my audience are not the average, you know, union worker in Michigan who is
[00:40:36.620 --> 00:40:41.180]   mostly working and raising a family and then goes to vote on primary day and goes to vote
[00:40:41.180 --> 00:40:42.660]   on election day.
[00:40:42.660 --> 00:40:46.620]   If you spend a lot of time on Reddit and Twitter, you're going to have an inflated sense of
[00:40:46.620 --> 00:40:50.600]   Bernie's popularity within the democratic party.
[00:40:50.600 --> 00:40:52.500]   That was my sense.
[00:40:52.500 --> 00:40:57.040]   And to some degree we saw that in certain States.
[00:40:57.040 --> 00:41:01.780]   I don't have the exact primary order and results in front of me or in my head, but the big
[00:41:01.780 --> 00:41:03.820]   turning point was South Carolina.
[00:41:03.820 --> 00:41:10.560]   South Carolina was when Joe Biden won and won handily understood to be because of the
[00:41:10.560 --> 00:41:14.360]   larger African American population in South Carolina.
[00:41:14.360 --> 00:41:18.420]   And right around that exact same time, I actually don't remember now whether it was the day
[00:41:18.420 --> 00:41:22.460]   after or the day before some of the smaller democratic candidates, smaller in terms of
[00:41:22.460 --> 00:41:27.360]   support got out and said, I'm endorsing Joe Biden.
[00:41:27.360 --> 00:41:32.940]   And to some degree, of course it was all organized and timed to help Joe Biden.
[00:41:32.940 --> 00:41:34.560]   There's no doubt about that.
[00:41:34.560 --> 00:41:37.240]   This is what the DNC does.
[00:41:37.240 --> 00:41:44.020]   It's hard for me to be mad at the DNC because this is sort of like if, if we believe they
[00:41:44.020 --> 00:41:48.820]   were there to be unbiased arbiters and to stay as much on the side as possible, it would
[00:41:48.820 --> 00:41:53.940]   make sense to be furious that they've gone against their stated, you know, kind of mandate.
[00:41:53.940 --> 00:42:00.800]   But we know that the DNC negotiates and is working behind the scenes and has a favorite.
[00:42:00.800 --> 00:42:03.420]   That favorite was Hillary in 2016, 2020.
[00:42:03.420 --> 00:42:08.240]   So I share the frustration about the power that the DNC has, but for people who were
[00:42:08.240 --> 00:42:12.360]   saying they did something illegal or whatever else the case may be, that that doesn't seem
[00:42:12.360 --> 00:42:13.880]   to be the case.
[00:42:13.880 --> 00:42:18.740]   But this is part of why, I mean, I would love there not to be this duopoly of Republicans
[00:42:18.740 --> 00:42:22.880]   and Democrats, and there's probably four major changes that have to happen in order to make
[00:42:22.880 --> 00:42:23.880]   that a reality.
[00:42:23.880 --> 00:42:30.240]   But I share the frustration of folks while recognizing that Reddit was not accurately
[00:42:30.240 --> 00:42:33.200]   representing Bernie's level of popularity.
[00:42:33.200 --> 00:42:37.500]   Still I wish that the bias wasn't towards the, uh, what could be negatively turned the
[00:42:37.500 --> 00:42:42.680]   deep state towards the bureaucracy, towards the momentum of the past, which I think Joe
[00:42:42.680 --> 00:42:48.600]   Biden kind of represents, uh, versus new ideas, which is funny to say that Bernie Sanders
[00:42:48.600 --> 00:42:53.080]   somehow represents new ideas cause he's also an older gentleman.
[00:42:53.080 --> 00:42:55.720]   Well it's a frame, it's a lot of framing.
[00:42:55.720 --> 00:43:02.400]   And the other aspect of that is on paper, Joe Biden's platform was arguably the most
[00:43:02.400 --> 00:43:07.280]   progressive of any democratic candidate who won the nomination.
[00:43:07.280 --> 00:43:11.440]   Now of course there were people who challenged the nominations who were to Joe Biden's left.
[00:43:11.440 --> 00:43:17.400]   A lot of this is perspective and it, you know, that's how you end up saying the guy who's
[00:43:17.400 --> 00:43:21.120]   a couple of years older than Biden is actually the guy with the fresh perspective, which
[00:43:21.120 --> 00:43:23.080]   is interesting because I don't disagree with you.
[00:43:23.080 --> 00:43:24.080]   Yeah.
[00:43:24.080 --> 00:43:27.800]   And then you also have to say the perspective doesn't always align with the policies.
[00:43:27.800 --> 00:43:28.800]   You're right.
[00:43:28.800 --> 00:43:32.960]   And you know, the actual policies of Joe Biden are different than the, maybe the perception
[00:43:32.960 --> 00:43:34.720]   of Joe Biden or what he ran on.
[00:43:34.720 --> 00:43:40.000]   I mean, just two examples I would give are during his campaign, he played up a little
[00:43:40.000 --> 00:43:45.080]   bit his interest in doing student loan forgiveness and something on cannabis.
[00:43:45.080 --> 00:43:46.080]   I never bought it.
[00:43:46.080 --> 00:43:50.960]   I told my audience, I think he's saying this stuff because this is the way the tide is
[00:43:50.960 --> 00:43:54.320]   kind of, the wind is blowing and he's being advised to say this stuff.
[00:43:54.320 --> 00:43:56.360]   I don't think he's going to do very much on either of these things.
[00:43:56.360 --> 00:44:01.960]   He did actually do some student loan stuff, but that would be two examples I think.
[00:44:01.960 --> 00:44:02.960]   Okay.
[00:44:02.960 --> 00:44:08.840]   Let's go to the, something you alluded to, which is the pros and cons of a particular
[00:44:08.840 --> 00:44:09.840]   candidate.
[00:44:09.840 --> 00:44:18.560]   What to you as a critic of Trump, what to you are the pros, the strengths of Donald
[00:44:18.560 --> 00:44:21.440]   Trump and what you are as big as weaknesses.
[00:44:21.440 --> 00:44:26.600]   The strengths of Trump, let's see how I can frame them in a way that is both accurate
[00:44:26.600 --> 00:44:30.200]   and accurately assesses my feeling about it.
[00:44:30.200 --> 00:44:34.440]   And can be taken out of context most masterfully through the clipping process.
[00:44:34.440 --> 00:44:35.440]   Yes.
[00:44:35.440 --> 00:44:43.840]   Trump's strengths are mostly superficial and in terms of presentation, Trump was able
[00:44:43.840 --> 00:44:46.600]   to, I call it a grift.
[00:44:46.600 --> 00:44:51.240]   Some on the right say he's just so good at relating to different types of people.
[00:44:51.240 --> 00:44:57.300]   Trump as a rich guy from New York city was able to convince people that he spent most
[00:44:57.300 --> 00:45:04.080]   of his life trying to be kept isolated from that he had their best interests in mind,
[00:45:04.080 --> 00:45:12.040]   that he knew why they weren't doing well in the 2016 economy and that he had solutions
[00:45:12.040 --> 00:45:14.200]   that he was going to bring forward.
[00:45:14.200 --> 00:45:19.600]   The truth is he never really liked those people and as soon as they weren't useful to him
[00:45:19.600 --> 00:45:24.360]   for a brief period of time, he, you know, that, that love affair with his followers
[00:45:24.360 --> 00:45:27.140]   stopped and then now it's back that he needs them again.
[00:45:27.140 --> 00:45:31.880]   He didn't really understand the causes of the problems that those folks were experiencing
[00:45:31.880 --> 00:45:33.600]   and his solutions were laughable, right?
[00:45:33.600 --> 00:45:37.720]   Like Jared was going to solve the Israeli Palestinian conflict in year one.
[00:45:37.720 --> 00:45:43.240]   He was going to replace Obamacare in 2017, things that were never going anywhere, anywhere.
[00:45:43.240 --> 00:45:50.160]   But what he did really well was he put up a united front of, I know what is ailing you.
[00:45:50.160 --> 00:45:55.680]   I know how to fix it and I know how to fix it, I guess because he's a businessman and
[00:45:55.680 --> 00:46:00.660]   he's been above the fray of politics for so long knowing how to use political donations
[00:46:00.660 --> 00:46:01.660]   to his advantage.
[00:46:01.660 --> 00:46:02.660]   He's called that smart, et cetera.
[00:46:02.660 --> 00:46:04.780]   I think that's his greatest strength.
[00:46:04.780 --> 00:46:10.920]   Why do you say that the, the, the, the Jared plan for Israel, Palestine and the plan for
[00:46:10.920 --> 00:46:14.880]   healthcare to improve Obamacare, why, why do you say that's laughable?
[00:46:14.880 --> 00:46:19.320]   Well, only someone, I would include the North Korea plan as well, which I'm glad to talk
[00:46:19.320 --> 00:46:20.560]   about.
[00:46:20.560 --> 00:46:29.140]   Only someone who doesn't know anything about the size and scope of these issues could so
[00:46:29.140 --> 00:46:32.560]   arrogantly say that they could solve them in that way.
[00:46:32.560 --> 00:46:38.720]   And on that timeframe, I'm all for optimism and, and bringing a new face to things.
[00:46:38.720 --> 00:46:39.720]   Absolutely.
[00:46:39.720 --> 00:46:40.720]   Without a doubt.
[00:46:40.720 --> 00:46:47.040]   But, you know, a wall with Mexico that Mexico will pay for at the end of my first term.
[00:46:47.040 --> 00:46:50.560]   I know there are people who believed it because they would call into my show and say, I'm
[00:46:50.560 --> 00:46:52.520]   voting for Trump because of it.
[00:46:52.520 --> 00:46:57.960]   But it's hard to believe that anybody serious would fall for that unless you were deliberately
[00:46:57.960 --> 00:47:02.580]   wanting to just believe whatever was being fed to you or you just hadn't ever thought
[00:47:02.580 --> 00:47:04.200]   about these issues before.
[00:47:04.200 --> 00:47:10.240]   The healthcare plan, you know, in 2017 they proposed one would have led to 24 million
[00:47:10.240 --> 00:47:12.160]   or so people ending up without healthcare.
[00:47:12.160 --> 00:47:14.120]   Didn't go anywhere cause it was so terrible.
[00:47:14.120 --> 00:47:18.440]   And then in August of 2020 Trump said in, in two weeks I'm going to finally have my
[00:47:18.440 --> 00:47:20.200]   healthcare proposal.
[00:47:20.200 --> 00:47:22.280]   It's 2023 we, we still never got it.
[00:47:22.280 --> 00:47:26.580]   You know, with all of these things, when you think them through, it was just sort of arrogance.
[00:47:26.580 --> 00:47:31.280]   And I get the perspective of, I want optimism and I liked that optimism.
[00:47:31.280 --> 00:47:32.280]   It worked.
[00:47:32.280 --> 00:47:33.280]   I mean, fair.
[00:47:33.280 --> 00:47:36.880]   A lot of people saw it and liked it as someone who followed a lot of those, those issues
[00:47:36.880 --> 00:47:38.260]   closely.
[00:47:38.260 --> 00:47:40.600]   They seemed of course like impossible promises.
[00:47:40.600 --> 00:47:42.000]   Well, it's a double edged sword.
[00:47:42.000 --> 00:47:48.480]   So to push back a little bit, if you look at the things I have a little bit more knowledge
[00:47:48.480 --> 00:47:54.080]   about, which is the space of artificial intelligence, there's a company called DeepMind and there's
[00:47:54.080 --> 00:47:59.400]   a company called OpenAI that we laughed at for a long time when they were talking about
[00:47:59.400 --> 00:48:01.880]   that they're going to solve intelligence.
[00:48:01.880 --> 00:48:08.200]   And now they've made, especially DeepMind and now most recently OpenAI with GPT, they've
[00:48:08.200 --> 00:48:13.400]   made progress that most of the AI community would not have imagined they'd be able to
[00:48:13.400 --> 00:48:14.640]   make.
[00:48:14.640 --> 00:48:21.160]   Everything from AlphaGo beating the WorldGo champion, just all the different steps in
[00:48:21.160 --> 00:48:30.120]   progress they can get into were a surprise to everybody and they are legitimately, fearlessly
[00:48:30.120 --> 00:48:33.720]   pursuing the task of solving intelligence.
[00:48:33.720 --> 00:48:41.160]   The other aspect, he gets a lot of criticism now, but another example is Elon Musk.
[00:48:41.160 --> 00:48:46.600]   I can say a lot of things like SpaceX, so commercial space flight, he was laughed at
[00:48:46.600 --> 00:48:49.800]   for a long time that that's possible.
[00:48:49.800 --> 00:48:55.060]   Same thing with autopilot in Tesla, autonomous vehicles, his approach was harshly criticized
[00:48:55.060 --> 00:49:00.680]   by all the experts and still criticized, to this day deeply criticized.
[00:49:00.680 --> 00:49:06.520]   And I as a person that I believe objectively can look at the progress of autopilot as a
[00:49:06.520 --> 00:49:11.640]   semi-autonomous vehicle system has been incredibly surprising.
[00:49:11.640 --> 00:49:19.880]   The reason I mention that is sometimes it feels like you need the guy or the gal who
[00:49:19.880 --> 00:49:26.200]   makes those preposterous, ambitious statements like, "We're going to solve healthcare this
[00:49:26.200 --> 00:49:29.200]   year."
[00:49:29.200 --> 00:49:35.680]   And then there's experts like yourself that are looking, thinking, "Have you read anything
[00:49:35.680 --> 00:49:37.840]   about the history of Israel-Palestine?"
[00:49:37.840 --> 00:49:40.240]   Israel-Palestine is a good example of that.
[00:49:40.240 --> 00:49:42.720]   Do you know there's a history there?
[00:49:42.720 --> 00:49:47.240]   Do you realize how complicated, how many people have tried, how many people have failed, how
[00:49:47.240 --> 00:49:56.280]   many millions of people hate each other in this little place, in this land?
[00:49:56.280 --> 00:49:58.360]   Sometimes the expertise can really weigh you down.
[00:49:58.360 --> 00:50:04.040]   So to push back, sometimes you have to have the, almost be naive and stupid and just rush
[00:50:04.040 --> 00:50:07.880]   in with an optimism in order to actually make some progress.
[00:50:07.880 --> 00:50:09.360]   I agree with you 100%.
[00:50:09.360 --> 00:50:14.600]   I think it's interesting that all of the examples you gave of successes are from the technology
[00:50:14.600 --> 00:50:15.600]   space.
[00:50:15.600 --> 00:50:16.600]   Not politics, yes.
[00:50:16.600 --> 00:50:20.680]   Not from politics, which, I mean, listen, I would love to be able to make headway on
[00:50:20.680 --> 00:50:22.760]   some of these issues more quickly, without a doubt.
[00:50:22.760 --> 00:50:27.640]   I do think at some point though, when it comes down to voting and saying, "One of these people
[00:50:27.640 --> 00:50:32.400]   is going to be ostensibly in charge for four years through all of the departments and secretaries
[00:50:32.400 --> 00:50:38.600]   and choices that they make," we do want to apply some level of realism with the understanding
[00:50:38.600 --> 00:50:42.160]   that your examples are from the tech space and they're good examples.
[00:50:42.160 --> 00:50:43.600]   There's no question about it.
[00:50:43.600 --> 00:50:48.880]   One thing I'll add to this, I recently read Bradley Hope's new book about North Korea.
[00:50:48.880 --> 00:50:55.380]   And it's really about an activist who, it doesn't even really matter, but in the background
[00:50:55.380 --> 00:51:00.800]   of the book, it's written, much of what is written about happens during the Trump era.
[00:51:00.800 --> 00:51:04.760]   And when Trump did the first and then the second, I guess you'd call them summits with
[00:51:04.760 --> 00:51:06.440]   Kim Jong-un.
[00:51:06.440 --> 00:51:12.720]   And it actually did seem like to some degree, Trump's, "We're going to handle this like
[00:51:12.720 --> 00:51:16.320]   I do a business deal" approach to Kim Jong-un.
[00:51:16.320 --> 00:51:23.960]   In some sense, it actually was logical because of Kim Jong-un and the way that it was so
[00:51:23.960 --> 00:51:29.080]   ego-driven and they both as sort of authoritarian strongmen types to different degrees wanted
[00:51:29.080 --> 00:51:30.080]   that.
[00:51:30.080 --> 00:51:34.440]   There was actually a kernel where I actually thought as I read it, Trump's initial idea
[00:51:34.440 --> 00:51:35.440]   wasn't crazy.
[00:51:35.440 --> 00:51:38.200]   The problem was he knew nothing about the backstory of the relationship.
[00:51:38.200 --> 00:51:43.240]   He fell for all sorts of lies from Kim Jong-un and he made offers that didn't make any sense
[00:51:43.240 --> 00:51:44.240]   to me.
[00:51:44.240 --> 00:51:45.240]   It fell apart, fine.
[00:51:45.240 --> 00:51:50.560]   But that's an example where I think Trump's personality was not actually at its base,
[00:51:50.560 --> 00:51:52.520]   the problem when it came to North Korea.
[00:51:52.520 --> 00:51:56.840]   Well, there's other things of this nature that could go in, or some people argue goes
[00:51:56.840 --> 00:51:59.720]   into the strengths and pros of Donald Trump.
[00:51:59.720 --> 00:52:02.840]   China, for example, tariffs on China.
[00:52:02.840 --> 00:52:08.880]   Can you make the case that there's some positive outcomes of the way Donald Trump acted with
[00:52:08.880 --> 00:52:09.880]   China?
[00:52:09.880 --> 00:52:10.880]   It's really tough.
[00:52:10.880 --> 00:52:11.880]   And I'll give you a couple of reasons why.
[00:52:11.880 --> 00:52:12.880]   Okay, then also cons.
[00:52:12.880 --> 00:52:14.280]   I'll give you, it's tough to make.
[00:52:14.280 --> 00:52:21.960]   So the China thing is really, so just very recently to when we're recording this, Trump
[00:52:21.960 --> 00:52:27.840]   was on Fox News interviewed by a guy named Mark Levin and Trump proposed a new, I call
[00:52:27.840 --> 00:52:29.200]   it a conspiracy theory.
[00:52:29.200 --> 00:52:33.980]   Maybe it will strike you as something different about China, covid and tariffs.
[00:52:33.980 --> 00:52:39.160]   And Trump's suggestion was that the tariffs cost China so much money.
[00:52:39.160 --> 00:52:45.800]   China sent the U.S. so much money in tariffs that they released covid as punishment.
[00:52:45.800 --> 00:52:48.000]   Now there's a couple of problems with that.
[00:52:48.000 --> 00:52:52.000]   One American companies pay the tariffs.
[00:52:52.000 --> 00:52:53.820]   Trump still doesn't seem to know this.
[00:52:53.820 --> 00:52:59.740]   Trump seems to believe that when he puts a tariff on Chinese imports, someone in China
[00:52:59.740 --> 00:53:02.940]   is cutting a check to the United States.
[00:53:02.940 --> 00:53:07.540]   American companies buy the stuff from China and then American companies cut a check to
[00:53:07.540 --> 00:53:10.500]   the United States for the tariff.
[00:53:10.500 --> 00:53:14.300]   Trump doesn't seem to get that, but it still has a sting to the Chinese economy.
[00:53:14.300 --> 00:53:19.200]   You can make the argument that if there is a suitable alternative domestically or from
[00:53:19.200 --> 00:53:24.020]   a different country that it will reduce imports, but it didn't happen.
[00:53:24.020 --> 00:53:30.420]   And we actually have reports now that the tariffs on China cost about a quarter million
[00:53:30.420 --> 00:53:32.520]   American jobs.
[00:53:32.520 --> 00:53:38.820]   The other problem with that idea is China created and released the virus in order to
[00:53:38.820 --> 00:53:41.160]   hurt you.
[00:53:41.160 --> 00:53:47.220]   But as of today, five point seven of the six point eight million deaths were in other countries.
[00:53:47.220 --> 00:53:48.640]   It's a very indirect way.
[00:53:48.640 --> 00:53:52.760]   You're mostly killing people in other countries to hurt Trump.
[00:53:52.760 --> 00:53:57.560]   Maybe there was a, this is the sort of thing where when I think about how Trump dealt with
[00:53:57.560 --> 00:54:03.980]   China, it's very scary because given another four years, who knows what he might do if
[00:54:03.980 --> 00:54:06.720]   he still doesn't understand how tariffs work.
[00:54:06.720 --> 00:54:13.080]   The geopolitics operates in complicated ways with carrots and sticks.
[00:54:13.080 --> 00:54:15.960]   And Henry Kissinger has written quite a lot about this.
[00:54:15.960 --> 00:54:23.040]   And in some sense, the positive aspect here that Donald Trump is willing to take big risks
[00:54:23.040 --> 00:54:29.800]   in a, in the game of geopolitics with this a giant superpower that is China and a lot
[00:54:29.800 --> 00:54:37.800]   of others are too afraid, too afraid to call them out, to come to the table and criticize.
[00:54:37.800 --> 00:54:42.320]   I certainly think that's an argument that can be made.
[00:54:42.320 --> 00:54:47.820]   My question would be what tangible positive outcomes did it lead to?
[00:54:47.820 --> 00:54:51.040]   And it's tough to identify any, but I think it's a great thing.
[00:54:51.040 --> 00:54:57.400]   I mean, listen, one of the things you're kind of getting at maybe indirectly is that there's
[00:54:57.400 --> 00:55:02.080]   been this sense that politics has been done very similarly for a long time.
[00:55:02.080 --> 00:55:07.120]   And even between Democrats and Republicans still, even with some policy differences,
[00:55:07.120 --> 00:55:12.360]   there's still the kind of feeling that it's disconnected folks in DC mostly dealing with
[00:55:12.360 --> 00:55:14.480]   issues that don't directly affect.
[00:55:14.480 --> 00:55:15.480]   I get that.
[00:55:15.480 --> 00:55:16.480]   I'm with you on that.
[00:55:16.480 --> 00:55:22.240]   I think the question as to whether Trump's bluster was positive rather than extraordinarily
[00:55:22.240 --> 00:55:23.960]   humiliating in many ways.
[00:55:23.960 --> 00:55:27.440]   I just come down on, it was an absolute and total humiliation.
[00:55:27.440 --> 00:55:32.440]   But I understand that you can recognize Trump doesn't know a lot of stuff, but his attitude
[00:55:32.440 --> 00:55:34.320]   was refreshing in some way.
[00:55:34.320 --> 00:55:35.960]   That's a reasonable position for someone to take.
[00:55:35.960 --> 00:55:38.320]   I disagree with it, but I understand it.
[00:55:38.320 --> 00:55:42.200]   Well, it's trying and failing better than not trying.
[00:55:42.200 --> 00:55:44.120]   This goes well beyond politics.
[00:55:44.120 --> 00:55:46.440]   You know, Wayne Gretzky is weighed in about this.
[00:55:46.440 --> 00:55:49.320]   Michael Jordan is weighed in about, I mean, this is a, yeah.
[00:55:49.320 --> 00:55:51.280]   Is it better to have tried and failed than never?
[00:55:51.280 --> 00:55:54.280]   Is it better to have loved and lost than never to have loved?
[00:55:54.280 --> 00:55:55.280]   I don't know.
[00:55:55.280 --> 00:55:58.040]   I mean, listen, we live through four years of Trump.
[00:55:58.040 --> 00:56:01.960]   We know what that four year term was like.
[00:56:01.960 --> 00:56:07.960]   And it's very hard for me to say that the things he tried were, were overwhelmingly
[00:56:07.960 --> 00:56:08.960]   reasonable.
[00:56:08.960 --> 00:56:11.520]   But I get the point you're trying to make and I appreciate it.
[00:56:11.520 --> 00:56:14.520]   And it's, if we don't do any of it, then where do we end up?
[00:56:14.520 --> 00:56:15.520]   Sure.
[00:56:15.520 --> 00:56:18.000]   We know where we ended up with, with Trump and it was pretty embarrassing.
[00:56:18.000 --> 00:56:19.240]   Uh, okay.
[00:56:19.240 --> 00:56:22.280]   Let's linger on some more strengths.
[00:56:22.280 --> 00:56:24.660]   We didn't start any new wars.
[00:56:24.660 --> 00:56:26.520]   We didn't start something to that.
[00:56:26.520 --> 00:56:28.240]   Yeah, that's, it's, it's interesting.
[00:56:28.240 --> 00:56:31.680]   There's a few different approaches to dealing with that.
[00:56:31.680 --> 00:56:37.600]   Um, first it's really important to remember that the counterpoint to that from the folks
[00:56:37.600 --> 00:56:41.600]   who like to say that was that Hillary Clinton was going to start three wars.
[00:56:41.600 --> 00:56:45.000]   Sometimes they say four wars, sometimes they say five wars.
[00:56:45.000 --> 00:56:46.000]   Okay.
[00:56:46.000 --> 00:56:50.920]   Um, the geopolitical situation during the four years that Trump was in office, I don't
[00:56:50.920 --> 00:56:56.000]   know that they obviously lent themselves to wars that Trump just barely was able to keep
[00:56:56.000 --> 00:56:57.000]   us out of.
[00:56:57.000 --> 00:57:02.920]   I think the Russia thing is interesting because now it's very popular to go back and say,
[00:57:02.920 --> 00:57:06.920]   you know, the reason Putin didn't do the Ukraine thing when Trump, right.
[00:57:06.920 --> 00:57:10.040]   And to somehow give Trump credit for that.
[00:57:10.040 --> 00:57:15.580]   There's a counterpoint to it, which is Putin under Trump, particularly if Trump got four
[00:57:15.580 --> 00:57:21.240]   more years would have been able to maybe consolidate power in other ways because of his relationship
[00:57:21.240 --> 00:57:22.240]   with Trump.
[00:57:22.240 --> 00:57:23.560]   I'm not coming down on one side or the other.
[00:57:23.560 --> 00:57:28.120]   It's not my area of expertise, but it's not the open shut slam dunk that, you know, Trump
[00:57:28.120 --> 00:57:32.040]   likes to say it is Putin didn't invade because he knew I would crush them.
[00:57:32.040 --> 00:57:33.040]   Okay.
[00:57:33.040 --> 00:57:38.680]   So it's not obvious to me that there were imminently wars that would have started during
[00:57:38.680 --> 00:57:39.680]   that time.
[00:57:39.680 --> 00:57:48.480]   That being said, um, you know, for all the criticism of Obama during Crimea, Trump seemed
[00:57:48.480 --> 00:57:51.840]   to just kind of forget about that after all the criticism and say, I'm not actually going
[00:57:51.840 --> 00:57:53.200]   to do anything about that.
[00:57:53.200 --> 00:57:57.800]   And so there's, there are foreign policy, uh, criticisms that, that could be made, but
[00:57:57.800 --> 00:57:58.880]   it is true.
[00:57:58.880 --> 00:58:01.120]   No new wars were started under Trump.
[00:58:01.120 --> 00:58:02.200]   And I, I like that.
[00:58:02.200 --> 00:58:03.200]   I don't like wars.
[00:58:03.200 --> 00:58:05.800]   What do you think about his handling of COVID?
[00:58:05.800 --> 00:58:09.840]   Can you say what are the pros and cons of his handling of COVID?
[00:58:09.840 --> 00:58:14.440]   The con for him is he'd be president right now if he had handled it differently.
[00:58:14.440 --> 00:58:18.700]   I think it's abundantly clear, um, early on.
[00:58:18.700 --> 00:58:21.800]   And there's now a lot of really good reporting about the conversations he was having with
[00:58:21.800 --> 00:58:23.880]   Jared Kushner and others.
[00:58:23.880 --> 00:58:28.120]   He became convinced either because of things he was being told or because he decided this
[00:58:28.120 --> 00:58:29.480]   is the way it's going to be.
[00:58:29.480 --> 00:58:30.640]   This is going to go away.
[00:58:30.640 --> 00:58:31.640]   Fine.
[00:58:31.640 --> 00:58:32.640]   It's in China.
[00:58:32.640 --> 00:58:33.640]   Okay.
[00:58:33.640 --> 00:58:34.640]   It's in China and Italy.
[00:58:34.640 --> 00:58:35.640]   Okay.
[00:58:35.640 --> 00:58:36.640]   We have 15 cases, but it'll soon be zero.
[00:58:36.640 --> 00:58:40.360]   We'll be open by Easter of 2020.
[00:58:40.360 --> 00:58:42.700]   None of it happened.
[00:58:42.700 --> 00:58:47.760]   If he had handled it in the following way, and I've said this to Rogan and I've said
[00:58:47.760 --> 00:58:52.800]   this to Patrick, bet David, and they tend to all see my side of this.
[00:58:52.800 --> 00:58:59.760]   If Trump had said, listen, we don't know how bad this is going to be, but I care too much
[00:58:59.760 --> 00:59:02.620]   about the American people to take a shot.
[00:59:02.620 --> 00:59:04.480]   So it's not going to be two weeks.
[00:59:04.480 --> 00:59:07.480]   It's going to be a little bit, but I need your help.
[00:59:07.480 --> 00:59:08.960]   We're going to bring everybody together.
[00:59:08.960 --> 00:59:12.960]   I don't care if you're a Democrat or a Republican, we're going to have MAGA masks and you could
[00:59:12.960 --> 00:59:16.440]   have kept 50 cents on the dollar to pay off stormy or whoever.
[00:59:16.440 --> 00:59:17.440]   Right.
[00:59:17.440 --> 00:59:24.280]   But it would have been, I think he wins reelection because the perception was, and reality is
[00:59:24.280 --> 00:59:25.520]   a version of that.
[00:59:25.520 --> 00:59:31.280]   The perception was that he was way too cavalier about it early on.
[00:59:31.280 --> 00:59:34.080]   People died who didn't need to die.
[00:59:34.080 --> 00:59:42.000]   And I think that it was arguably the one area where he could have all but guaranteed that
[00:59:42.000 --> 00:59:43.360]   he was going to get himself reelected.
[00:59:43.360 --> 00:59:47.480]   - Well, to push back on that, I mean, because you mentioned sort of masks and lockdowns
[00:59:47.480 --> 00:59:49.480]   kind of a solution to COVID.
[00:59:49.480 --> 00:59:52.360]   - I didn't mention lockdowns, but I'm glad to talk about policy.
[00:59:52.360 --> 00:59:53.360]   - Sure, quarantine.
[00:59:53.360 --> 00:59:58.040]   But there's several solutions to a pandemic, broadly speaking.
[00:59:58.040 --> 01:00:00.080]   And one of them is vaccine.
[01:00:00.080 --> 01:00:02.640]   And so you didn't mention that.
[01:00:02.640 --> 01:00:05.720]   He fast tracked the development.
[01:00:05.720 --> 01:00:10.720]   His administration fast tracked the development of the vaccine, which surprising, he didn't
[01:00:10.720 --> 01:00:12.400]   really take much credit for.
[01:00:12.400 --> 01:00:13.400]   I think he did.
[01:00:13.400 --> 01:00:14.400]   I think he tried.
[01:00:14.400 --> 01:00:15.400]   There's a couple, there's a lot there.
[01:00:15.400 --> 01:00:23.880]   - Well, to me, it seems like you could make the case with the Trump hand gestures that
[01:00:23.880 --> 01:00:29.280]   his decisions for fast tracking the development of the vaccine saved tens of millions of lives.
[01:00:29.280 --> 01:00:33.560]   You can make, he could in the Trumpian way make that case.
[01:00:33.560 --> 01:00:34.720]   - So couple different things.
[01:00:34.720 --> 01:00:39.860]   I know you don't necessarily follow Trump's rallies as closely as I do, and I'm jealous
[01:00:39.860 --> 01:00:41.080]   of you for that.
[01:00:41.080 --> 01:00:49.920]   But he did tout the vaccine stuff hugely for a while until his audience turned against
[01:00:49.920 --> 01:00:51.000]   him.
[01:00:51.000 --> 01:00:56.400]   And then he had to draw this line where he was going, I made the vaccines, which none
[01:00:56.400 --> 01:00:58.620]   of you have to take, by the way, freedom.
[01:00:58.620 --> 01:01:00.780]   You don't have to take them, but it's fantastic.
[01:01:00.780 --> 01:01:03.360]   And nobody else could have done it, but don't worry, nobody's going to make you take the
[01:01:03.360 --> 01:01:04.360]   vaccine.
[01:01:04.360 --> 01:01:08.220]   And he actually got booed at a couple of his own rallies when talking about the vaccines.
[01:01:08.220 --> 01:01:10.080]   But let's back up a little bit.
[01:01:10.080 --> 01:01:11.080]   Fast tracking.
[01:01:11.080 --> 01:01:15.960]   My understanding of what he did is he did what any president in his situation would
[01:01:15.960 --> 01:01:21.920]   do and what many world leaders elsewhere did as well, which is he agreed to pre-purchase
[01:01:21.920 --> 01:01:27.140]   supply of vaccine in order to provide money to pharmaceutical companies to scale up the
[01:01:27.140 --> 01:01:30.600]   manufacturing, which is absolutely fine.
[01:01:30.600 --> 01:01:35.520]   But he wants, one of the stories he tells is it usually takes 12 years to develop a
[01:01:35.520 --> 01:01:36.520]   vaccine.
[01:01:36.520 --> 01:01:39.840]   We did it in nine months, thanks to me.
[01:01:39.840 --> 01:01:46.040]   Decades of mRNA technology being developed, created the platform in which you can make
[01:01:46.040 --> 01:01:48.640]   a particular vaccine in nine months.
[01:01:48.640 --> 01:01:50.440]   Didn't have anything to do with Trump.
[01:01:50.440 --> 01:01:55.120]   He did pre-fund and say, we will buy huge supply and that provided liquidity to the
[01:01:55.120 --> 01:01:56.120]   pharmaceutical companies.
[01:01:56.120 --> 01:02:05.160]   But he also delegated control to, to people, to experts that enable that kind of fast tracking
[01:02:05.160 --> 01:02:06.540]   of vaccines, right?
[01:02:06.540 --> 01:02:10.540]   He was very eager for the FDA to approve it because he saw that there would be a political
[01:02:10.540 --> 01:02:11.540]   benefit.
[01:02:11.540 --> 01:02:12.540]   He didn't get in the way, I guess.
[01:02:12.540 --> 01:02:13.540]   He didn't get in the way.
[01:02:13.540 --> 01:02:14.540]   Fair.
[01:02:14.540 --> 01:02:15.540]   I think now we're on the same page.
[01:02:15.540 --> 01:02:19.780]   He did not get in the way of vaccines being developed, which is good.
[01:02:19.780 --> 01:02:24.580]   Presidents and bureaucracies have a way of getting in the way.
[01:02:24.580 --> 01:02:26.060]   I don't disagree with that.
[01:02:26.060 --> 01:02:28.580]   I'm not aware of really any governments that got in the way.
[01:02:28.580 --> 01:02:34.260]   I mean, it seemed given the global situation, everybody, European countries were pre-purchasing
[01:02:34.260 --> 01:02:35.260]   vaccine.
[01:02:35.260 --> 01:02:39.920]   African countries were, who were going to be later to receive vaccines were partnering
[01:02:39.920 --> 01:02:42.920]   with the European countries that had pre-purchased.
[01:02:42.920 --> 01:02:46.500]   But the most interesting thing about all of this is Trump did play up the vaccines for
[01:02:46.500 --> 01:02:50.980]   a long time until his, his crowd didn't want to hear about it anymore, which was crazy.
[01:02:50.980 --> 01:02:56.340]   It was sort of like he became a victim of the monster he created to some degree.
[01:02:56.340 --> 01:03:01.020]   One of the effects of all this that makes me truly sad is this division over the vaccines
[01:03:01.020 --> 01:03:03.460]   has created distrust in science.
[01:03:03.460 --> 01:03:11.740]   And also what makes me sad is the scientific leaders, Anthony Fauci being one of the representatives
[01:03:11.740 --> 01:03:15.740]   of that community, I would say completely dropped the ball.
[01:03:15.740 --> 01:03:18.140]   - In what way?
[01:03:18.140 --> 01:03:20.500]   - They spoke with arrogance.
[01:03:20.500 --> 01:03:22.520]   They spoke down to people.
[01:03:22.520 --> 01:03:30.060]   They spoke in a way that a great scientist does not speak, which is they spoke with certainty
[01:03:30.060 --> 01:03:35.540]   without humility, like they have all the wisdom and all of us are too dumb to understand it,
[01:03:35.540 --> 01:03:40.900]   but they're going to be the parent that tells us exactly what to do versus speaking to the
[01:03:40.900 --> 01:03:46.740]   immensity of the problem, the deep core of the problem being the uncertainty.
[01:03:46.740 --> 01:03:49.500]   We don't know what to do.
[01:03:49.500 --> 01:03:53.500]   The terrifying thing about the pandemic, we don't know anything about it as it's happening.
[01:03:53.500 --> 01:03:54.820]   And so you have to make decisions.
[01:03:54.820 --> 01:04:00.780]   You have to take risks about, well, maybe you have to overreact in order to protect
[01:04:00.780 --> 01:04:07.460]   the populace, but it's in the face of uncertainty that you have to do that, not empowered by
[01:04:07.460 --> 01:04:14.780]   science somehow and the deep expertise that somebody like Anthony Fauci claims to have.
[01:04:14.780 --> 01:04:21.460]   So I just am really troubled by, yeah, the distress in science that resulted from that.
[01:04:21.460 --> 01:04:26.380]   And you have to blame the leaders, I mean, to the degree, leaders take responsibility.
[01:04:26.380 --> 01:04:31.860]   And I think Anthony Fauci was the scientific leader behind the American response to the
[01:04:31.860 --> 01:04:38.020]   pandemic and I think he failed as a scientist, as a representative of science.
[01:04:38.020 --> 01:04:44.740]   - I'm less, I don't know if interested is the right word, but the kind of, the Fauci
[01:04:44.740 --> 01:04:50.500]   review is less interesting to me in terms of what comes next than the first part you
[01:04:50.500 --> 01:04:53.700]   mentioned, which is the distrust in science.
[01:04:53.700 --> 01:04:58.980]   And sometimes I'll get voicemails or emails from people in my audience who say that I
[01:04:58.980 --> 01:05:03.660]   have had to backpedal on certain things related to this.
[01:05:03.660 --> 01:05:09.740]   And one of the things I tried to do from the beginning was not speak in certain terms when
[01:05:09.740 --> 01:05:11.460]   we really didn't have complete information.
[01:05:11.460 --> 01:05:16.900]   So there was this period where hydroxychloroquine was first sort of mentioned as a possible
[01:05:16.900 --> 01:05:23.900]   treatment prophylactic or proactive treatment for COVID or active treatment for COVID along
[01:05:23.900 --> 01:05:25.020]   with a bunch of other stuff.
[01:05:25.020 --> 01:05:30.540]   There was ivermectin, there was vitamin D, there were all sorts of different things.
[01:05:30.540 --> 01:05:37.100]   And I tried to be careful to say, right now, we don't have rigorous science that tells
[01:05:37.100 --> 01:05:39.020]   us that some of these things work.
[01:05:39.020 --> 01:05:43.420]   It doesn't mean that won't come in the future, at which point, if there was something as
[01:05:43.420 --> 01:05:49.100]   cheap as hydroxychloroquine that treated COVID effectively, unbelievable, fantastic.
[01:05:49.100 --> 01:05:52.180]   It's not, there's no way it ever will be determined.
[01:05:52.180 --> 01:05:53.740]   We don't have that information right now.
[01:05:53.740 --> 01:05:58.040]   So it's not super wise right now to go and start taking this stuff.
[01:05:58.040 --> 01:06:03.700]   We eventually learned like with vitamin D, having an appropriate vitamin D level does
[01:06:03.700 --> 01:06:10.140]   seem to be based on what I've most recently read, generally protective and a good thing
[01:06:10.140 --> 01:06:12.140]   when it comes to infections of different.
[01:06:12.140 --> 01:06:13.140]   Great.
[01:06:13.140 --> 01:06:14.980]   So that's something we figured out.
[01:06:14.980 --> 01:06:20.900]   One of the really difficult things is that the quote truth about the vaccines did change.
[01:06:20.900 --> 01:06:24.820]   And the original, again, this is all, I don't pretend to be an expert, but just someone
[01:06:24.820 --> 01:06:29.340]   who's synthesizing the medical data and writing about it.
[01:06:29.340 --> 01:06:37.740]   Originally the first vaccine related to the wild type strain did seem to be very effective,
[01:06:37.740 --> 01:06:42.500]   not only at preventing death and serious illness, but also transmission.
[01:06:42.500 --> 01:06:49.340]   There were people then saying it doesn't prevent transmission over time as the variants came
[01:06:49.340 --> 01:06:52.980]   forward, the vaccine became less effective at that.
[01:06:52.980 --> 01:06:56.620]   At that point I started telling my audience something different because as far as I was
[01:06:56.620 --> 01:06:59.540]   concerned, the reality on the ground had changed.
[01:06:59.540 --> 01:07:01.480]   In my mind, that's how science works.
[01:07:01.480 --> 01:07:02.480]   It's not backpedaling.
[01:07:02.480 --> 01:07:06.980]   It's we're adjusting our beliefs to what is taking place in the real world.
[01:07:06.980 --> 01:07:12.980]   Well, to be fair, the scientists, many of whom are my friends, virologists and biologists,
[01:07:12.980 --> 01:07:17.300]   they have way more humility than people like Anthony Fauci who are speaking about this
[01:07:17.300 --> 01:07:20.380]   or the CEO of Pfizer who are speaking about this.
[01:07:20.380 --> 01:07:27.040]   This is the fundamental problem here is the way science works is there's usually a lot
[01:07:27.040 --> 01:07:32.300]   more humility and a lot more transparency about what we know and what we don't know.
[01:07:32.300 --> 01:07:39.820]   And people like Anthony Fauci thought it would be beneficial for the world if he speaks with
[01:07:39.820 --> 01:07:41.940]   more certainty.
[01:07:41.940 --> 01:07:46.020]   But because of the political division that formed around that, that certainty resulted
[01:07:46.020 --> 01:07:51.500]   and became completely counterproductive that people didn't trust anything about the vaccine,
[01:07:51.500 --> 01:07:56.580]   didn't trust any institutions that contained the experts that actually knew what they were
[01:07:56.580 --> 01:08:01.820]   doing and basically didn't trust anything that was coming out of the mouths of scientists.
[01:08:01.820 --> 01:08:04.580]   Some large percent of the population.
[01:08:04.580 --> 01:08:12.340]   So that made you completely ineffective at scale as a society trying to respond to a
[01:08:12.340 --> 01:08:13.740]   terrible pandemic.
[01:08:13.740 --> 01:08:17.100]   And that's where I put a lot of blame on leaders.
[01:08:17.100 --> 01:08:22.260]   So political leaders and scientific leaders are the ones that should inspire us to all
[01:08:22.260 --> 01:08:24.900]   get together and respond.
[01:08:24.900 --> 01:08:26.980]   That should be the case for the pandemic.
[01:08:26.980 --> 01:08:31.300]   That should be the case in a time of war, all this kind of stuff.
[01:08:31.300 --> 01:08:32.660]   I generally agree with you.
[01:08:32.660 --> 01:08:35.220]   And for me, it's really about shared blame.
[01:08:35.220 --> 01:08:39.820]   And there were a lot of different reasons why that the early communication wasn't good.
[01:08:39.820 --> 01:08:45.860]   Part of it was, I mean, for me, I prefer accuracy rather than overconfidence.
[01:08:45.860 --> 01:08:51.580]   I would prefer, listen, we don't really know right now whether masks do X or Y.
[01:08:51.580 --> 01:08:55.260]   What we do know is the supply is really limited of this type of mask.
[01:08:55.260 --> 01:08:57.340]   We're going to try to keep them for the frontline workers.
[01:08:57.340 --> 01:08:58.580]   I love that.
[01:08:58.580 --> 01:09:00.940]   That's the way I want to be communicated to.
[01:09:00.940 --> 01:09:05.740]   A call was made to do it differently, which was to say the masks don't actually help.
[01:09:05.740 --> 01:09:08.420]   But the real reason is they want to keep them for health care workers.
[01:09:08.420 --> 01:09:11.340]   And then later the masks are what's going to solve.
[01:09:11.340 --> 01:09:13.180]   I'm with I'm with you 100 percent.
[01:09:13.180 --> 01:09:19.780]   I think the other layer to it is you can't ignore the political situation at the time.
[01:09:19.780 --> 01:09:24.840]   If Trump had won reelection and the vaccine distribution had taken place while Trump was
[01:09:24.840 --> 01:09:30.360]   president rather than Biden, my belief is that the same number of Democrats would have
[01:09:30.360 --> 01:09:34.780]   gotten vaccinated, but way more Republicans would have as well because they were following
[01:09:34.780 --> 01:09:36.860]   not science, but political leaders.
[01:09:36.860 --> 01:09:41.040]   And when it was Biden in D.C. instead of Trump, a lot of those people said, I don't trust
[01:09:41.040 --> 01:09:44.060]   the vaccine, but wait, it's Trump's vaccine, I thought.
[01:09:44.060 --> 01:09:46.660]   Yeah, but something about the way Biden's distributing it.
[01:09:46.660 --> 01:09:49.420]   So I do think you can't ignore that political layer.
[01:09:49.420 --> 01:09:50.420]   I agree with you.
[01:09:50.420 --> 01:09:53.340]   The communication was a disaster.
[01:09:53.340 --> 01:09:54.760]   Let me ask you about Joe Biden.
[01:09:54.760 --> 01:09:57.900]   What are the strengths and weaknesses of Joe Biden?
[01:09:57.900 --> 01:10:01.600]   Weaknesses, I think, are some of the things you've identified.
[01:10:01.600 --> 01:10:05.640]   He is not seen as high energy.
[01:10:05.640 --> 01:10:12.540]   He is not the same Joe Biden that debated Paul Ryan in 2012 and ran circles around him
[01:10:12.540 --> 01:10:16.600]   and just an incredible debate performance.
[01:10:16.600 --> 01:10:24.680]   He is not inspiring in the way that someone like a Barack Obama was to people coming up
[01:10:24.680 --> 01:10:26.840]   and starting to get interested in politics.
[01:10:26.840 --> 01:10:32.020]   I think a lot of those are fair criticisms, I think, on policy.
[01:10:32.020 --> 01:10:36.640]   He's not interested in a lot of the things that younger voters are interested in.
[01:10:36.640 --> 01:10:39.400]   I mentioned cannabis reform.
[01:10:39.400 --> 01:10:41.260]   I mentioned student loans.
[01:10:41.260 --> 01:10:44.580]   So I think that that's a deficit for Biden.
[01:10:44.580 --> 01:10:53.460]   I think the upside to Biden is when it comes to foreign policy, diplomacy, high level negotiations,
[01:10:53.460 --> 01:10:56.820]   knowing how to engage with allies in a productive way.
[01:10:56.820 --> 01:11:00.120]   It's tough to find someone with more experience than Biden.
[01:11:00.120 --> 01:11:04.420]   I know that there are counterpoints to what I'm saying, and those include that was the
[01:11:04.420 --> 01:11:05.420]   old Biden.
[01:11:05.420 --> 01:11:06.820]   The new Biden doesn't have it.
[01:11:06.820 --> 01:11:10.500]   That includes that's just a sign of rot because he's been around for so long.
[01:11:10.500 --> 01:11:13.140]   Nobody should be around that long in politics.
[01:11:13.140 --> 01:11:15.660]   Perfectly reasonable criticisms to talk about.
[01:11:15.660 --> 01:11:17.840]   But I do see that as one of his strengths.
[01:11:17.840 --> 01:11:25.460]   And he also is good at knowing when he can work with Republicans and when he can't and
[01:11:25.460 --> 01:11:31.020]   not wasting more time than is sort of expected for posturing reasons.
[01:11:31.020 --> 01:11:32.340]   And I think that that's a good thing.
[01:11:32.340 --> 01:11:34.600]   Do you think he's actually there?
[01:11:34.600 --> 01:11:39.380]   So in day to day operation of government, given his cognitive capabilities, do you think
[01:11:39.380 --> 01:11:44.140]   he is an active and practicing executive?
[01:11:44.140 --> 01:11:50.120]   I don't know that I can say that it's because of what may be going on cognitively.
[01:11:50.120 --> 01:11:56.620]   But my sense from the people I talk to is that he's very much involved in the highest
[01:11:56.620 --> 01:12:01.900]   level geopolitical and big domestic economic stuff.
[01:12:01.900 --> 01:12:06.740]   But that a lot of the smaller issues that presidents might or might not be in sort of
[01:12:06.740 --> 01:12:11.260]   plugged into that he's not plugged into the details of a lot of the lower level stuff.
[01:12:11.260 --> 01:12:15.380]   I mean, you could probably apply the same exact criticism even more so towards the Donald
[01:12:15.380 --> 01:12:21.060]   Trump administration in terms of being a practicing active executive who's paying attention.
[01:12:21.060 --> 01:12:30.060]   Like for example, like Vladimir Putin is somebody who loves the role of the executive, has a
[01:12:30.060 --> 01:12:35.180]   huge amount of meetings, has constantly tracking information about agriculture and all the
[01:12:35.180 --> 01:12:38.260]   different subsystems of government.
[01:12:38.260 --> 01:12:41.820]   Stalin, funny enough, was also extremely good at this.
[01:12:41.820 --> 01:12:45.660]   So certain people just love the job of being an executive.
[01:12:45.660 --> 01:12:51.100]   And I'm not sure if Donald Trump did.
[01:12:51.100 --> 01:12:57.260]   And I'm not sure if Joe Biden in his current state has the cognitive capability to.
[01:12:57.260 --> 01:12:58.260]   It's a good question.
[01:12:58.260 --> 01:12:59.780]   Kim Jong-un's another one, by the way.
[01:12:59.780 --> 01:13:04.460]   You know, there's videos of him examining a pottery, you know, a factory where they
[01:13:04.460 --> 01:13:08.940]   make plates and making very specific comments about how the plates should be made.
[01:13:08.940 --> 01:13:13.340]   I think that in that case, there's a lot of propaganda value to it with Trump.
[01:13:13.340 --> 01:13:15.420]   I think you're probably right.
[01:13:15.420 --> 01:13:17.460]   You know, he did get involved in the minutia of things.
[01:13:17.460 --> 01:13:22.300]   I mean, once he pulled out a weather map and with a Sharpie drew a different hurricane
[01:13:22.300 --> 01:13:25.020]   path that was more politically convenient to him.
[01:13:25.020 --> 01:13:27.780]   That's pretty micro, you know, saying the weather channels.
[01:13:27.780 --> 01:13:29.020]   I see what you did there.
[01:13:29.020 --> 01:13:30.020]   This is OK.
[01:13:30.020 --> 01:13:31.020]   I see what you did there.
[01:13:31.020 --> 01:13:32.020]   Micro.
[01:13:32.020 --> 01:13:35.060]   He went to Puerto Rico and he gave out paper towels after a hurricane.
[01:13:35.060 --> 01:13:38.340]   Now, he was shooting them like free throws, which didn't look very good.
[01:13:38.340 --> 01:13:42.100]   So he will get involved in the micro when it's advantageous.
[01:13:42.100 --> 01:13:43.100]   You know what I mean?
[01:13:43.100 --> 01:13:46.300]   But I do agree with you that he wants to just kind of make it so build the wall.
[01:13:46.300 --> 01:13:47.300]   I don't know.
[01:13:47.300 --> 01:13:48.300]   Just build it, figure it out, get it done.
[01:13:48.300 --> 01:13:49.300]   Quick pause.
[01:13:49.300 --> 01:13:50.300]   Bethenbry.
[01:13:50.300 --> 01:13:51.300]   Sure.
[01:13:51.300 --> 01:13:54.300]   Yeah, you're hilarious.
[01:13:54.300 --> 01:13:57.180]   And just for the sake of completeness, I should mention the subreddit.
[01:13:57.180 --> 01:14:02.260]   What Biden has done is also what Trump has done, but it's not as active.
[01:14:02.260 --> 01:14:05.340]   And it has like this master list of all the accomplishments.
[01:14:05.340 --> 01:14:11.700]   It's I recommend people look at it because there's a kind of rigorous and interesting
[01:14:11.700 --> 01:14:14.620]   with links list of all the things he's done.
[01:14:14.620 --> 01:14:16.860]   Just list some of them.
[01:14:16.860 --> 01:14:20.020]   Restored daily press briefings.
[01:14:20.020 --> 01:14:21.500]   Cancel the Keystone pipeline.
[01:14:21.500 --> 01:14:22.980]   Reverse Trump's Muslim ban.
[01:14:22.980 --> 01:14:24.340]   Required masks on federal property.
[01:14:24.340 --> 01:14:26.660]   Rejoins the Paris climate agreement.
[01:14:26.660 --> 01:14:28.860]   Extend student loan payment freeze.
[01:14:28.860 --> 01:14:30.940]   Extend eviction freeze.
[01:14:30.940 --> 01:14:33.500]   Historic stimulus bill, as you mentioned.
[01:14:33.500 --> 01:14:37.620]   Hence funding for border wall and so on and so forth.
[01:14:37.620 --> 01:14:39.660]   Border strengthening of DACA.
[01:14:39.660 --> 01:14:40.980]   Rejoins the World Health Organization.
[01:14:40.980 --> 01:14:44.860]   And the timing of this, of course, is important.
[01:14:44.860 --> 01:14:51.180]   Yeah, several historic stimulus bills, which of course you could criticize or support.
[01:14:51.180 --> 01:14:56.620]   Raised the minimum wage for federal contractors and federal employees for $15.
[01:14:56.620 --> 01:14:57.620]   There's a lot.
[01:14:57.620 --> 01:14:58.620]   There's a lot.
[01:14:58.620 --> 01:15:05.060]   It makes you realize with both Trump and Biden that there's a bunch of small details that
[01:15:05.060 --> 01:15:06.060]   matter.
[01:15:06.060 --> 01:15:07.060]   Yeah.
[01:15:07.060 --> 01:15:11.460]   Like that matter on people's lives, like actual little policies.
[01:15:11.460 --> 01:15:15.580]   Trump did a lot of stuff as far as I heard for the military.
[01:15:15.580 --> 01:15:18.700]   Like not big stuff, but small stuff.
[01:15:18.700 --> 01:15:19.700]   Yeah.
[01:15:19.700 --> 01:15:21.460]   I'd be curious what you're thinking of.
[01:15:21.460 --> 01:15:24.700]   I mean, I know one of the big things under Trump was we're going to get trans people
[01:15:24.700 --> 01:15:27.260]   out of the military.
[01:15:27.260 --> 01:15:28.260]   That's not what I was referring to.
[01:15:28.260 --> 01:15:29.580]   You weren't referring to that.
[01:15:29.580 --> 01:15:33.820]   He Trump that tell you, Trump's hilarious with these stories that he tells.
[01:15:33.820 --> 01:15:37.740]   And one of the story and you get to know them if you, if you follow him at all, he tells
[01:15:37.740 --> 01:15:42.220]   the story, you know, when I came into office, the generals came to me and they said, sir,
[01:15:42.220 --> 01:15:43.440]   the cupboards are bare.
[01:15:43.440 --> 01:15:45.060]   We have no bullets.
[01:15:45.060 --> 01:15:46.500]   And so I rebuilt the military.
[01:15:46.500 --> 01:15:49.940]   You know, the cupboards were bare when Obama left it and it was just terrible.
[01:15:49.940 --> 01:15:52.420]   But I rebuilt it and the generals, I've got the best generals.
[01:15:52.420 --> 01:15:54.660]   They said, sir, it's incredible what you were able to do.
[01:15:54.660 --> 01:15:57.460]   You look into it and it's like, yeah, that's not really true.
[01:15:57.460 --> 01:16:01.020]   Like it is true that there are armaments that just like on a schedule do get replaced and
[01:16:01.020 --> 01:16:02.980]   that's part of the military industrial complex.
[01:16:02.980 --> 01:16:05.380]   But there's nothing like special Trump really did.
[01:16:05.380 --> 01:16:09.520]   But these stories become, they take on like a life of their own.
[01:16:09.520 --> 01:16:12.940]   And it's interesting to sometimes try to dig down and figure out like, was there any policy
[01:16:12.940 --> 01:16:15.460]   connected to that or is that just a story?
[01:16:15.460 --> 01:16:19.420]   Do you think it's possible to have a good conversation with each of them, Donald Trump
[01:16:19.420 --> 01:16:24.500]   and Joe Biden in a pot, in a podcast context or in a debate context?
[01:16:24.500 --> 01:16:25.500]   Absolutely.
[01:16:25.500 --> 01:16:26.500]   Yeah.
[01:16:26.500 --> 01:16:27.900]   You're saying like, could I with either of them?
[01:16:27.900 --> 01:16:28.900]   Oh, a hundred percent.
[01:16:28.900 --> 01:16:29.900]   Yeah.
[01:16:29.900 --> 01:16:30.900]   Joe Biden too.
[01:16:30.900 --> 01:16:31.900]   Sure.
[01:16:31.900 --> 01:16:32.900]   Yeah.
[01:16:32.900 --> 01:16:33.900]   But can you dig into that a little more?
[01:16:33.900 --> 01:16:38.540]   Well, I mean, I don't know what, I think there's maybe something implicit in your, in your question,
[01:16:38.540 --> 01:16:42.260]   but this deeper about the nature of politics and politicians.
[01:16:42.260 --> 01:16:43.260]   Yes.
[01:16:43.260 --> 01:16:48.780]   I think with either of them, I mean the political differences wouldn't be an impediment to having
[01:16:48.780 --> 01:16:52.260]   a good conversation with either of them.
[01:16:52.260 --> 01:16:57.660]   I think one of the things that's really tough in my experience when talking to elected officials
[01:16:57.660 --> 01:17:05.140]   is they could be super interesting about a hundred different topics, but handlers decide
[01:17:05.140 --> 01:17:09.420]   or try to get you to talk about something you don't really care about and something
[01:17:09.420 --> 01:17:13.220]   really narrow, which doesn't bring out your best nor their best.
[01:17:13.220 --> 01:17:14.960]   And that's a frustration.
[01:17:14.960 --> 01:17:18.720]   But I think that given an unstructured three hour conversation, I think it would be interesting
[01:17:18.720 --> 01:17:19.720]   to talk to both.
[01:17:19.720 --> 01:17:25.580]   I mean, listen, with, with Biden, aside from his view on cannabis or whatever, his background
[01:17:25.580 --> 01:17:32.140]   and the incredible unimaginable family tragedy that he had in his first wife and you know,
[01:17:32.140 --> 01:17:33.140]   multiple kids dying.
[01:17:33.140 --> 01:17:39.220]   And I mean, it, it's just incredible, you know, and, and with Trump, I think also you
[01:17:39.220 --> 01:17:40.380]   could have an interesting conversation.
[01:17:40.380 --> 01:17:41.380]   Yeah.
[01:17:41.380 --> 01:17:46.600]   Those, the human beings with a life story there that, and they're some of the most successful
[01:17:46.600 --> 01:17:54.180]   humans who have ever lived to have rose to this highest office in interesting, complex
[01:17:54.180 --> 01:17:55.180]   ways.
[01:17:55.180 --> 01:17:56.180]   Yes.
[01:17:56.180 --> 01:18:00.420]   I mean, one of the things I'm troubled by maybe you can speak to is why we're so negative
[01:18:00.420 --> 01:18:03.120]   towards presidential candidates and presidents.
[01:18:03.120 --> 01:18:08.740]   Why it's just, they go through this shit storm no matter who they are.
[01:18:08.740 --> 01:18:09.740]   Yeah.
[01:18:09.740 --> 01:18:10.740]   They're like hated.
[01:18:11.000 --> 01:18:18.220]   Like all the conspiracy theories and just, just the dynamics of how we talk about them
[01:18:18.220 --> 01:18:19.220]   is vicious.
[01:18:19.220 --> 01:18:25.360]   If you, if you just look at replies to even Barack Obama on Twitter, it's like, what is
[01:18:25.360 --> 01:18:26.360]   going on here?
[01:18:26.360 --> 01:18:27.360]   Why, why?
[01:18:27.360 --> 01:18:31.800]   Like, cause we look at other leaders in other spaces and we're generally positive about
[01:18:31.800 --> 01:18:32.800]   it.
[01:18:32.800 --> 01:18:33.800]   Yeah.
[01:18:33.800 --> 01:18:35.720]   There's a couple different things.
[01:18:35.720 --> 01:18:40.860]   There's this dynamic, which is really unfortunate, which is you ask people, do you approve of
[01:18:40.860 --> 01:18:43.780]   the job a particular president is doing?
[01:18:43.780 --> 01:18:47.700]   And very often if at any point while they were in office, they did something you don't
[01:18:47.700 --> 01:18:49.500]   like, people will say, I don't approve.
[01:18:49.500 --> 01:18:53.020]   And so by its nature, what that means is just like the longer you're in office, the lower
[01:18:53.020 --> 01:18:55.140]   your approval rating is going to be.
[01:18:55.140 --> 01:18:56.980]   And very often that's the way it works.
[01:18:56.980 --> 01:19:02.460]   I mean, there's major events like nine 11 spike George W. Bush's approval to an incredible
[01:19:02.460 --> 01:19:03.460]   level.
[01:19:03.460 --> 01:19:04.800]   Then it came back down with the Iraq war.
[01:19:04.800 --> 01:19:08.140]   But there's this unfortunate thing that when, when people are just asked, you think Biden's
[01:19:08.140 --> 01:19:09.540]   doing a good job?
[01:19:09.540 --> 01:19:14.180]   If four months ago Biden did something on healthcare that somebody didn't like, even
[01:19:14.180 --> 01:19:18.880]   if you like most of it, a lot of people from that point forward will say, I don't approve.
[01:19:18.880 --> 01:19:22.860]   They might still vote for him because they like him better than the alternative or whatever.
[01:19:22.860 --> 01:19:24.900]   It's just the dynamic of politics.
[01:19:24.900 --> 01:19:25.900]   And I agree.
[01:19:25.900 --> 01:19:27.820]   It's, it's very, does it have to be that way?
[01:19:27.820 --> 01:19:33.460]   I don't think it has to be that way, but to unwind it, so many things would have to change.
[01:19:33.460 --> 01:19:39.040]   I think our election system is part of why politics is the way it is, where you have
[01:19:39.040 --> 01:19:44.560]   two choices and it's first past the post and we have this electoral college so that depending
[01:19:44.560 --> 01:19:49.120]   on which state you vote in, the kind of meaning and significance of your vote is different.
[01:19:49.120 --> 01:19:53.600]   If you vote in Montana, it's the Republican candidate's going to win and that changes
[01:19:53.600 --> 01:19:54.640]   the day the dynamics.
[01:19:54.640 --> 01:19:56.680]   I think that's part of it.
[01:19:56.680 --> 01:20:03.220]   I think that at a personal level, I've experienced this in my life a lot.
[01:20:03.220 --> 01:20:07.340]   We've become, and by we, I mean people in the United States to some degree who talk
[01:20:07.340 --> 01:20:08.340]   about this stuff.
[01:20:08.340 --> 01:20:14.100]   We've become uncomfortable when there is disagreement and it bleeds over into now we can't have
[01:20:14.100 --> 01:20:16.620]   a normal interpersonal relationship anymore.
[01:20:16.620 --> 01:20:19.940]   I'm from Argentina and in Argentina it's really common.
[01:20:19.940 --> 01:20:26.220]   Even in my family, there are incredibly heated political debates at the start, the middle,
[01:20:26.220 --> 01:20:27.940]   the end of some kind of gathering.
[01:20:27.940 --> 01:20:32.060]   But then everybody just goes back to like, okay, we disagree on some things, but that's
[01:20:32.060 --> 01:20:37.740]   okay and we can now go and you know, finish cooking the beef or whatever it is that we're
[01:20:37.740 --> 01:20:38.940]   doing.
[01:20:38.940 --> 01:20:43.300]   And I experienced this even with people who come up to me on the street and they go, just
[01:20:43.300 --> 01:20:49.080]   earlier today, a guy came up to me and he said, RFK all the way baby, talking about
[01:20:49.080 --> 01:20:50.820]   Bobby Kennedy Jr.
[01:20:50.820 --> 01:20:55.220]   And I just kind of, you know, I said, oh, all right, you know, let's see what happens.
[01:20:55.220 --> 01:20:59.180]   And then there was another moment where the guy ended up standing next to me for maybe
[01:20:59.180 --> 01:21:00.180]   longer than he thought.
[01:21:00.180 --> 01:21:03.740]   And I could tell this guy's getting so awkward because it was an utterance he thought that
[01:21:03.740 --> 01:21:05.420]   would just be on the fly and he'd be gone.
[01:21:05.420 --> 01:21:08.620]   But now we're standing next to each other waiting for our sandwiches.
[01:21:08.620 --> 01:21:09.620]   It's like no big deal.
[01:21:09.620 --> 01:21:10.880]   You know, it's just, oh, okay.
[01:21:10.880 --> 01:21:12.580]   You like Bobby Kennedy Jr.
[01:21:12.580 --> 01:21:14.900]   I don't plan to vote for it's fine, you know?
[01:21:14.900 --> 01:21:16.460]   And that's like a sociocultural thing.
[01:21:16.460 --> 01:21:17.820]   I think there's lots of other countries.
[01:21:17.820 --> 01:21:20.420]   I've spent time in Italy.
[01:21:20.420 --> 01:21:24.780]   I have relatives in Israel where like shouting at each other is sort of like normal and then
[01:21:24.780 --> 01:21:27.020]   you just go back and finish the, it sounds like shouting.
[01:21:27.020 --> 01:21:29.860]   I'm sort of exaggerating, but very animated.
[01:21:29.860 --> 01:21:33.020]   What seemed like big disagreements and then everybody's cool.
[01:21:33.020 --> 01:21:34.660]   I wish it were more normal.
[01:21:34.660 --> 01:21:40.580]   - So maybe the mechanism of going from shouting to being cool again needs to improve.
[01:21:40.580 --> 01:21:43.060]   Because maybe we can't solve the shouting at each other.
[01:21:43.060 --> 01:21:44.060]   - Maybe not.
[01:21:44.060 --> 01:21:48.380]   - So maybe we need to somehow figure out the deescalation, like making up.
[01:21:48.380 --> 01:21:51.260]   I've had a few recent fights with friends like that.
[01:21:51.260 --> 01:21:52.260]   - Really?
[01:21:52.260 --> 01:21:53.260]   - Yeah.
[01:21:53.260 --> 01:21:54.260]   - For politics?
[01:21:54.260 --> 01:22:00.300]   - No, but political style, emotionally drenched stuff.
[01:22:00.300 --> 01:22:06.260]   And it was interesting to go through that full process and then make up at the end.
[01:22:06.260 --> 01:22:11.300]   But it was a process and it was a process that required being in person and talking
[01:22:11.300 --> 01:22:12.940]   through it.
[01:22:12.940 --> 01:22:15.660]   And it was stressful, the whole thing.
[01:22:15.660 --> 01:22:20.420]   And maybe because most of our interactions are online, we don't get a chance to do that
[01:22:20.420 --> 01:22:22.340]   in person making up again.
[01:22:22.340 --> 01:22:27.460]   - I don't know, but do you think it's a feature or a bug of the system that we're so, we just
[01:22:27.460 --> 01:22:29.680]   hate the powerful?
[01:22:29.680 --> 01:22:31.300]   - You mentioned the online part.
[01:22:31.300 --> 01:22:38.680]   I think it's, you mentioned it earlier perfectly, which is you take contentious political issues,
[01:22:38.680 --> 01:22:45.740]   you create a platform that rewards controversy and disagreement and limits the number of
[01:22:45.740 --> 01:22:48.480]   characters you can use to express yourself.
[01:22:48.480 --> 01:22:53.320]   You kind of throw it into a baking dish and mix the entire thing up.
[01:22:53.320 --> 01:22:54.740]   It's complete and total chaos.
[01:22:54.740 --> 01:22:58.940]   And one of the things that, I've talked before about all the angry emails and threats and
[01:22:58.940 --> 01:22:59.940]   stuff that I get.
[01:22:59.940 --> 01:23:05.500]   I'm acutely aware that if I had in-person conversations with most of these people, the
[01:23:05.500 --> 01:23:09.660]   conversations would basically be like, "Oh, we have different views about how to solve
[01:23:09.660 --> 01:23:11.300]   some of the problems we're facing.
[01:23:11.300 --> 01:23:17.240]   We probably agree about what the problem is and we probably share many values, but on
[01:23:17.240 --> 01:23:23.260]   these particular four issues, we may have very different views, but that's okay."
[01:23:23.260 --> 01:23:25.300]   Online that's not the case.
[01:23:25.300 --> 01:23:29.620]   And it leads to, you know, the mess that we get ourselves in.
[01:23:29.620 --> 01:23:33.500]   But I think that it's a feature of a lot of the systems that are being used to disseminate
[01:23:33.500 --> 01:23:34.500]   information.
[01:23:34.500 --> 01:23:36.260]   - Again, let me linger on that.
[01:23:36.260 --> 01:23:43.620]   Do you regret some of the mockery and the snark you use on Twitter and even in your
[01:23:43.620 --> 01:23:46.980]   show that kind of feeds that division?
[01:23:46.980 --> 01:23:55.620]   - I don't regret it in the sense that it's a calculated part or tool that I use in addition
[01:23:55.620 --> 01:24:00.980]   to figuring out how to simplify complicated concepts and choosing stories that I think
[01:24:00.980 --> 01:24:01.980]   are underrepresented.
[01:24:01.980 --> 01:24:05.360]   And, you know, it's all part of like the package of what I'm doing.
[01:24:05.360 --> 01:24:14.700]   I recognize that my show is not the audio visual version of a peer-reviewed, you know,
[01:24:14.700 --> 01:24:18.340]   randomized controlled trial about views on abortion or whatever the case.
[01:24:18.340 --> 01:24:25.820]   Like I'm very much aware of that, but I don't regret including it as a tool that I've used
[01:24:25.820 --> 01:24:30.300]   to build the community in some total that I've built over the last more than 10 years.
[01:24:30.300 --> 01:24:35.020]   - I guess I could ask about the different trajectories you think your show might take.
[01:24:35.020 --> 01:24:40.580]   So you know, the dynamic you had with Donald Trump Jr. and maybe Candace Owens is the more
[01:24:40.580 --> 01:24:42.500]   appropriate comparison.
[01:24:42.500 --> 01:24:48.220]   Are you okay having that dance for the next few years between you and Candace Owens and
[01:24:48.220 --> 01:24:55.020]   just kind of the mockery, the derision that's a part of that process and taking part in
[01:24:55.020 --> 01:24:56.660]   that, you know?
[01:24:56.660 --> 01:25:07.620]   - I'm fine with it in the sense of personally, I tolerate it well until it crosses the line
[01:25:07.620 --> 01:25:10.300]   and people pull my family in and people, right?
[01:25:10.300 --> 01:25:11.300]   So that's the part that's difficult.
[01:25:11.300 --> 01:25:12.500]   - If you set the family stuff aside.
[01:25:12.500 --> 01:25:13.500]   - If I set that aside.
[01:25:13.500 --> 01:25:14.700]   - Just in the digital space.
[01:25:14.700 --> 01:25:16.180]   - I'm glad to mix it up.
[01:25:16.180 --> 01:25:21.460]   Now the truth is Candace Owens has had me blocked for years up until this incident.
[01:25:21.460 --> 01:25:25.060]   She unblocked me just to tweet about what I tweeted about.
[01:25:25.060 --> 01:25:26.940]   I don't know the backstory of that genuinely.
[01:25:26.940 --> 01:25:28.460]   I have no idea.
[01:25:28.460 --> 01:25:32.440]   So I don't have a sense that she's super interested in engaging with me on that.
[01:25:32.440 --> 01:25:35.660]   But all of these people, I mean, Candace Owens is welcome on my show anytime.
[01:25:35.660 --> 01:25:37.940]   Don Jr.'s welcome on my show anytime.
[01:25:37.940 --> 01:25:40.300]   It's been a decade since I had Ben Shapiro on.
[01:25:40.300 --> 01:25:42.100]   He's welcome at any time.
[01:25:42.100 --> 01:25:46.740]   I'm glad to have these conversations and I think it's an important thing.
[01:25:46.740 --> 01:25:51.540]   And also I wish that everybody was willing to have the conversations in good faith rather
[01:25:51.540 --> 01:25:55.980]   than as performance, it's not even really performance art, rather than being simply
[01:25:55.980 --> 01:25:58.340]   performative for an audience that you have.
[01:25:58.340 --> 01:26:02.380]   - In terms of your motivations, do you see, do you worry about the effects of something
[01:26:02.380 --> 01:26:08.220]   you spoke about offline, like the YouTube algorithm?
[01:26:08.220 --> 01:26:14.980]   Do you, are you driven by the number of views your videos get or are you driven by something
[01:26:14.980 --> 01:26:16.220]   else?
[01:26:16.220 --> 01:26:23.420]   - So in my world, I guess I would say the number of views that any platform generates
[01:26:23.420 --> 01:26:27.620]   is a metric that I can choose how to interpret.
[01:26:27.620 --> 01:26:34.020]   I can choose to interpret it as I've created content that's interesting to people or I've
[01:26:34.020 --> 01:26:37.580]   created content that's really angering people and that's why they're showing up.
[01:26:37.580 --> 01:26:38.580]   They don't actually like it.
[01:26:38.580 --> 01:26:41.020]   It's because they're angry or whatever else the case may be.
[01:26:41.020 --> 01:26:45.140]   But it is true that there are algorithmic changes that can take place.
[01:26:45.140 --> 01:26:49.460]   Something happened in early January that affected us on YouTube or there are periods on Tik
[01:26:49.460 --> 01:26:54.940]   Tok where you can tell we're doing all the same things, something has happened and then
[01:26:54.940 --> 01:26:56.780]   you never usually figure out what it is.
[01:26:56.780 --> 01:27:02.220]   So for me, it's sort of just like a general tool to see what is the level of interest
[01:27:02.220 --> 01:27:06.900]   in what I'm doing and are the numbers so out of whack with what I would expect that I should
[01:27:06.900 --> 01:27:09.100]   look into whether something deeper is happening.
[01:27:09.100 --> 01:27:12.180]   Has there been some change to an algorithm or whatever the case may be?
[01:27:12.180 --> 01:27:18.620]   I had a debate once with someone who accused me of using clickbait to generate views and
[01:27:18.620 --> 01:27:23.460]   we had a really interesting conversation where I said, tell me really what you mean by that.
[01:27:23.460 --> 01:27:30.500]   Is your argument that I'm using titles that don't actually represent what's in the video?
[01:27:30.500 --> 01:27:32.020]   No what's in the title is in the video.
[01:27:32.020 --> 01:27:34.780]   I go, okay, so it's not that the title is dishonest.
[01:27:34.780 --> 01:27:40.660]   Are you saying, saying I'm deliberately picking titles that will garner a larger audience?
[01:27:40.660 --> 01:27:42.220]   And they said, yeah, that's kind of what I mean.
[01:27:42.220 --> 01:27:44.420]   And I said, isn't that kind of what we're all doing?
[01:27:44.420 --> 01:27:48.620]   The alternative would be choosing titles to generate a smaller audience, which seems like
[01:27:48.620 --> 01:27:50.420]   a real kind of waste of time.
[01:27:50.420 --> 01:27:57.620]   So I'm trying to navigate and play the game in a way that's comfortable, but use the metrics
[01:27:57.620 --> 01:28:00.700]   more as a tool than as something to obsess over.
[01:28:00.700 --> 01:28:09.220]   Nevertheless, the metrics are what they are in that they are able to affect your psyche.
[01:28:09.220 --> 01:28:13.480]   It's very difficult, which is why I have a Chrome extension that hides all the views
[01:28:13.480 --> 01:28:17.940]   and all that on YouTube for me.
[01:28:17.940 --> 01:28:22.980]   It's difficult not to let it affect how you think about ideas.
[01:28:22.980 --> 01:28:30.020]   So maybe your extensive exploration of a particular topic like healthcare generated very few views.
[01:28:30.020 --> 01:28:33.620]   It's difficult for you to still care about healthcare.
[01:28:33.620 --> 01:28:38.860]   There's some aspect of the human mind that starts being affected by those views.
[01:28:38.860 --> 01:28:42.020]   And I think that's a really dangerous thing.
[01:28:42.020 --> 01:28:48.560]   Mostly it's probably beneficial because it probably makes you a better presenter.
[01:28:48.560 --> 01:28:54.900]   If you do care about a topic a lot, you become more charismatic, you learn sort of in a Jimmy,
[01:28:54.900 --> 01:28:58.020]   Mr. Beast way how to present the ideas better.
[01:28:58.020 --> 01:29:02.980]   But it also can affect which topics you choose to cover, what you choose to think about those
[01:29:02.980 --> 01:29:06.180]   topics, the audience capture those topics.
[01:29:06.180 --> 01:29:07.820]   And that's a really scary effect.
[01:29:07.820 --> 01:29:10.460]   I'm really worried about my own mind and that.
[01:29:10.460 --> 01:29:13.820]   So I run from that aggressively.
[01:29:13.820 --> 01:29:20.220]   - One of the things that I include in my overall approach is I don't think about any one clip.
[01:29:20.220 --> 01:29:25.060]   I think about an entire show or a week of shows or a month of shows.
[01:29:25.060 --> 01:29:28.620]   And so it's less about does any one clip do well?
[01:29:28.620 --> 01:29:33.700]   My view going in is I'm going to do stuff that won't do that well, but I think it's
[01:29:33.700 --> 01:29:34.860]   really important to do.
[01:29:34.860 --> 01:29:36.580]   And I want to make it part of my show.
[01:29:36.580 --> 01:29:41.700]   And so when I did a clip with 10 ideas for reducing gun violence, I know that that's
[01:29:41.700 --> 01:29:43.780]   not going to get 500,000 or a million views.
[01:29:43.780 --> 01:29:45.700]   I know it's just not going to.
[01:29:45.700 --> 01:29:50.460]   And the first day it'll get 12,000 and I'll go, I don't care.
[01:29:50.460 --> 01:29:51.460]   That's fine.
[01:29:51.460 --> 01:29:54.660]   There's a group of people in my audience that values this stuff and I want to keep doing
[01:29:54.660 --> 01:29:55.660]   this stuff.
[01:29:55.660 --> 01:29:57.780]   I'll end up surprised sometime.
[01:29:57.780 --> 01:30:02.340]   And two weeks later has 150,000 views because it started being shared.
[01:30:02.340 --> 01:30:03.340]   Great.
[01:30:03.340 --> 01:30:08.620]   But I don't go into it thinking these all need to be home runs by that metric.
[01:30:08.620 --> 01:30:13.140]   I always go in saying, I want to put out a diversity of content, including stuff that
[01:30:13.140 --> 01:30:16.580]   is less titillating and salacious, but is important to do.
[01:30:16.580 --> 01:30:19.100]   It's more researched, et cetera.
[01:30:19.100 --> 01:30:22.100]   And so that's the way I try to resist exactly what you're talking about.
[01:30:22.100 --> 01:30:23.980]   - And I think you have to probably know yourself.
[01:30:23.980 --> 01:30:27.140]   Like for me, metrics, I just like numbers too much.
[01:30:27.140 --> 01:30:28.820]   And for me, metrics do affect me.
[01:30:28.820 --> 01:30:30.500]   This is why I don't pay attention at all.
[01:30:30.500 --> 01:30:34.980]   Like I can't, I would love to hire somebody in the team who cares because we currently
[01:30:34.980 --> 01:30:39.900]   have folks who just all of us just don't care.
[01:30:39.900 --> 01:30:44.100]   Because he probably is good to care enough to kind of just do good thumbnails and this
[01:30:44.100 --> 01:30:46.020]   kind of stuff to pay attention.
[01:30:46.020 --> 01:30:51.300]   But to me personally, I just find inner peace and focus if I don't think about the numbers
[01:30:51.300 --> 01:30:52.300]   at all.
[01:30:52.300 --> 01:30:58.540]   Because I find myself, I just remember a long time ago when I started a podcast, I
[01:30:58.540 --> 01:31:03.740]   would think that I failed if it didn't do well.
[01:31:03.740 --> 01:31:08.500]   Like if I didn't celebrate the person well enough, I didn't do a good job enough of a
[01:31:08.500 --> 01:31:09.500]   conversation.
[01:31:09.500 --> 01:31:12.780]   Well, that's not necessarily at all what that means.
[01:31:12.780 --> 01:31:13.780]   It's hard not to--
[01:31:13.780 --> 01:31:14.780]   - This is tough stuff.
[01:31:14.780 --> 01:31:17.060]   I mean, yeah, I know exactly what you're saying.
[01:31:17.060 --> 01:31:23.380]   And part of it is, I mean, it starts in my, you have a little bit of a different situation
[01:31:23.380 --> 01:31:28.860]   than me because you're doing long form conversations with people and the prep is a little bit different.
[01:31:28.860 --> 01:31:32.380]   One of the things in my space, because I'm reacting mostly to what's going on in the
[01:31:32.380 --> 01:31:36.340]   news and then also picking topics to dive into it a little bit more deeply, is I have
[01:31:36.340 --> 01:31:38.580]   very little control over the news cycle.
[01:31:38.580 --> 01:31:45.820]   And there is a metametric or a macro metric that affects me that will quadruple my audience
[01:31:45.820 --> 01:31:51.420]   and then take 75% of it away, which is the seasonality of election cycles.
[01:31:51.420 --> 01:31:56.220]   And the first few election cycles, it's very tough because I go, it's October.
[01:31:56.220 --> 01:32:01.100]   I'm like, at this rate, we're going to have 20 million subscribers by next, these numbers
[01:32:01.100 --> 01:32:02.660]   are unbelievable.
[01:32:02.660 --> 01:32:07.060]   And then it's January 30th, the inauguration's over.
[01:32:07.060 --> 01:32:10.620]   The debate is about the debt ceiling and nothing's going on.
[01:32:10.620 --> 01:32:13.060]   And I go, nobody's watching my content.
[01:32:13.060 --> 01:32:17.380]   I must've forgotten to upload a, something must be wrong.
[01:32:17.380 --> 01:32:19.540]   It's completely beyond my control.
[01:32:19.540 --> 01:32:23.900]   So I just, and I think part of what you're saying is I try to focus on the things I can
[01:32:23.900 --> 01:32:28.600]   control and understand those that I really have no control over whatsoever and try not
[01:32:28.600 --> 01:32:31.460]   to worry about them.
[01:32:31.460 --> 01:32:35.740]   - And try to do the things that make you happy at the end of the day.
[01:32:35.740 --> 01:32:37.460]   You mentioned RFK, the guy you met.
[01:32:37.460 --> 01:32:42.940]   What do you think about some of the other candidates outside of Joe Biden in the Democratic
[01:32:42.940 --> 01:32:45.900]   Party, RFK Jr.?
[01:32:45.900 --> 01:32:47.620]   What do you think of him as a candidate?
[01:32:47.620 --> 01:32:48.620]   - I've met him.
[01:32:48.620 --> 01:32:51.980]   We once had dinner and we have a number of friends in common, which is what makes this
[01:32:51.980 --> 01:32:54.620]   a little more awkward.
[01:32:54.620 --> 01:33:02.880]   But I think his campaign is basically sort of like a chaos candidacy to raise awareness
[01:33:02.880 --> 01:33:06.940]   and maybe raise money either for his book or his anti-vaccine organization, Children's
[01:33:06.940 --> 01:33:09.380]   Defense Fund, I believe it's called.
[01:33:09.380 --> 01:33:13.860]   I think there's some reporting that Steve Bannon really liked the idea of him running
[01:33:13.860 --> 01:33:17.500]   as a Democrat, again, to just generate chaos.
[01:33:17.500 --> 01:33:19.180]   I don't find it super interesting.
[01:33:19.180 --> 01:33:23.500]   I don't find it worthy of that much discussion.
[01:33:23.500 --> 01:33:30.660]   Smart guy, nice guy, has been doing anti-vaccine work that I don't find particularly inspiring.
[01:33:30.660 --> 01:33:34.180]   - So it's not just anti-COVID vaccine, it's more broader than that?
[01:33:34.180 --> 01:33:36.740]   - He's been in that space long before the COVID vaccines.
[01:33:36.740 --> 01:33:37.740]   Yeah, yeah.
[01:33:37.740 --> 01:33:38.740]   I don't find it super interesting.
[01:33:38.740 --> 01:33:44.660]   - Well, he also wrote the book, "The Real Anthony Fauci," is that the name of the book?
[01:33:44.660 --> 01:33:45.660]   - Did he write that?
[01:33:45.660 --> 01:33:46.660]   That's interesting.
[01:33:46.660 --> 01:33:47.660]   I don't know.
[01:33:47.660 --> 01:33:48.660]   I'm not sure about that.
[01:33:48.660 --> 01:33:49.780]   I'm aware of that book.
[01:33:49.780 --> 01:33:50.780]   I didn't know he wrote it.
[01:33:50.780 --> 01:33:55.260]   - I think I did, but it's been on my reading list to get...
[01:33:55.260 --> 01:34:02.060]   I've been trying to get a good balanced reading list about the COVID pandemic to understand
[01:34:02.060 --> 01:34:04.020]   what the hell happened.
[01:34:04.020 --> 01:34:09.820]   And anytime I start to try to go into that place, I'm exhausted by it.
[01:34:09.820 --> 01:34:14.820]   - Well, it's interesting to me that you wouldn't wait longer before delving into those books
[01:34:14.820 --> 01:34:18.460]   to have maybe a more clear hindsight.
[01:34:18.460 --> 01:34:21.580]   - But I think this is a pretty good time.
[01:34:21.580 --> 01:34:22.580]   You don't think so?
[01:34:22.580 --> 01:34:24.540]   This is...
[01:34:24.540 --> 01:34:26.220]   It depends on your goals.
[01:34:26.220 --> 01:34:31.180]   If you're thinking of it as a historical event, yes, you should probably wait longer.
[01:34:31.180 --> 01:34:38.440]   But if you're thinking about understanding what is broken about our system that we responded
[01:34:38.440 --> 01:34:43.860]   so poorly, that there was so much division, what is broken about our political system
[01:34:43.860 --> 01:34:49.340]   that it didn't unite us, it divided us, who's to blame?
[01:34:49.340 --> 01:34:53.940]   There's probably a lot of different narratives, but I feel like the more you learn about this,
[01:34:53.940 --> 01:34:54.940]   the better you can understand.
[01:34:54.940 --> 01:35:01.260]   I read, on just Vladimir Putin, I read like five biographies already, maybe more.
[01:35:01.260 --> 01:35:06.620]   Just it helps to really understand the people involved, the organizations involved.
[01:35:06.620 --> 01:35:07.620]   I don't know.
[01:35:07.620 --> 01:35:11.500]   Everything from the scientists to the political leaders.
[01:35:11.500 --> 01:35:14.540]   It felt like the blog posts and the tweets didn't quite capture this.
[01:35:14.540 --> 01:35:15.540]   - They didn't quite capture.
[01:35:15.540 --> 01:35:17.420]   No, one of the things, I read a ton.
[01:35:17.420 --> 01:35:24.140]   I don't read any modern political books, so I don't read the memoirs of elected officials.
[01:35:24.140 --> 01:35:27.580]   I don't read any, I just feel like I get enough of it in my job.
[01:35:27.580 --> 01:35:30.340]   So my reading list is just other things.
[01:35:30.340 --> 01:35:34.780]   It's history, it's narrative nonfiction, economics, et cetera.
[01:35:34.780 --> 01:35:38.460]   And that's my bias because I'm so overloaded with a lot of the stuff you're talking about.
[01:35:38.460 --> 01:35:40.580]   I haven't read any of Obama's books.
[01:35:40.580 --> 01:35:42.580]   I didn't read John Bolton's book.
[01:35:42.580 --> 01:35:50.660]   I don't read any of that stuff, although I'm sure there is value to be gleaned from it.
[01:35:50.660 --> 01:35:54.820]   - What about the other candidate that, according to the subreddit, and as you mentioned, you've
[01:35:54.820 --> 01:35:59.060]   criticized it a little bit, Marianne Williamson.
[01:35:59.060 --> 01:36:03.700]   Do you think, what are the pros and cons of her as a candidate?
[01:36:03.700 --> 01:36:08.580]   - This is another area where many in my audience really are angry with me.
[01:36:08.580 --> 01:36:10.940]   I don't find her candidacy super interesting.
[01:36:10.940 --> 01:36:13.700]   I'll tell you the pros and the cons as I see them.
[01:36:13.700 --> 01:36:19.020]   I do think that we have elected officials in the US, particularly presidents, from a
[01:36:19.020 --> 01:36:21.060]   really narrow range of backgrounds.
[01:36:21.060 --> 01:36:27.420]   So it's lawyers and sometimes business people, very, very often lawyers.
[01:36:27.420 --> 01:36:30.800]   I think we would benefit from a much greater diversity of backgrounds.
[01:36:30.800 --> 01:36:36.580]   And I once said, and that would include people from education, people from the science world,
[01:36:36.580 --> 01:36:38.780]   people with backgrounds maybe running nonprofits, et cetera.
[01:36:38.780 --> 01:36:42.300]   Now, Marianne Williamson did, I guess at one point, run some kind of small nonprofit.
[01:36:42.300 --> 01:36:46.940]   And some in my audience thought that credential alone would make me fall head over heels in
[01:36:46.940 --> 01:36:49.140]   love with the idea of a Marianne candidacy.
[01:36:49.140 --> 01:36:51.020]   I've interviewed for her.
[01:36:51.020 --> 01:36:53.700]   It's just not for me is the way I like to say it.
[01:36:53.700 --> 01:37:00.500]   It's the background of the woo woo type stuff is a bit off putting to me.
[01:37:00.500 --> 01:37:07.100]   I understand that someone with literal Christian Bible beliefs that also I don't like, maybe
[01:37:07.100 --> 01:37:12.620]   I'm more willing to accept as most of our presidents, of course, have had those views
[01:37:12.620 --> 01:37:15.700]   because they're otherwise more qualified.
[01:37:15.700 --> 01:37:20.180]   But some of the things that she says just strike me as, I just, I just don't know.
[01:37:20.180 --> 01:37:21.920]   I'll give you an example.
[01:37:21.920 --> 01:37:27.520]   When she was on with Russell Brand, she said, there's no such thing as clinical depression.
[01:37:27.520 --> 01:37:31.980]   It just means someone in a clinic told you you have depression.
[01:37:31.980 --> 01:37:33.380]   I don't believe that to be the case.
[01:37:33.380 --> 01:37:36.040]   I think we have an understanding there's two types of depression.
[01:37:36.040 --> 01:37:38.380]   There's like a genetic predisposition depression.
[01:37:38.380 --> 01:37:41.460]   There's like a acute, something's happening in my life.
[01:37:41.460 --> 01:37:42.460]   Temporary depression.
[01:37:42.460 --> 01:37:43.460]   Okay.
[01:37:43.460 --> 01:37:46.900]   And when I asked about it recently, she said, I didn't mean it.
[01:37:46.900 --> 01:37:51.200]   I was just trying to impress Russell Brand.
[01:37:51.200 --> 01:37:55.140]   I don't know if I'm more bothered by the things she first said or that by the fact that she
[01:37:55.140 --> 01:37:59.500]   wanted to impress Russell Brand, but it's just like, it's just really not for me.
[01:37:59.500 --> 01:38:03.960]   And I agree with her on, we need to take the climate more seriously.
[01:38:03.960 --> 01:38:05.820]   We need to expand access to hell.
[01:38:05.820 --> 01:38:07.180]   I'm with all of that stuff.
[01:38:07.180 --> 01:38:10.240]   Now I want to say one other thing about this.
[01:38:10.240 --> 01:38:12.340]   Anybody who wants to run should run.
[01:38:12.340 --> 01:38:17.780]   I am not suggesting there should be an uncontested primary for Joe Biden.
[01:38:17.780 --> 01:38:18.780]   Absolutely.
[01:38:18.780 --> 01:38:20.420]   So you think it should be contested?
[01:38:20.420 --> 01:38:23.860]   Well, what I mean by contested is, so there's two parts to what we mean by contested.
[01:38:23.860 --> 01:38:28.540]   Will the DNC organized debates and we'll get to that in a second, but should, should anybody
[01:38:28.540 --> 01:38:32.340]   who's on the left get out of the way because Joe Biden is president and he's running for
[01:38:32.340 --> 01:38:33.340]   reelection?
[01:38:33.340 --> 01:38:34.420]   Absolutely not.
[01:38:34.420 --> 01:38:37.340]   The question about should there be debates?
[01:38:37.340 --> 01:38:39.520]   I would like there to be debates.
[01:38:39.520 --> 01:38:42.940]   The DNC pretty clearly isn't going to organize them.
[01:38:42.940 --> 01:38:48.700]   I think if you did them, you would have to say at what polling level do you qualify?
[01:38:48.700 --> 01:38:51.940]   And I don't know exactly where you put that number, but I think it would be a great thing
[01:38:51.940 --> 01:38:56.500]   to put Joe Biden on a stage with if you can get what, 6%, 8%.
[01:38:56.500 --> 01:38:58.200]   I'm not really sure what the number would be.
[01:38:58.200 --> 01:39:00.180]   I'm totally all for that.
[01:39:00.180 --> 01:39:05.660]   Why is this set of candidates, at least from my perspective, so weak?
[01:39:05.660 --> 01:39:07.140]   Do you have, do you have an understanding?
[01:39:07.140 --> 01:39:10.440]   There's a lot of different answers to this.
[01:39:10.440 --> 01:39:14.680]   One aspect of this, which I think is more of a sociocultural thing, which I've recently
[01:39:14.680 --> 01:39:20.800]   read about to some degree is the job actually turns off the people who would be best at
[01:39:20.800 --> 01:39:23.880]   it because of what you need to do to become president.
[01:39:23.880 --> 01:39:31.440]   And it includes all but completely abandoning your existing day to day life job, which you
[01:39:31.440 --> 01:39:35.360]   may depend on and family to some degree.
[01:39:35.360 --> 01:39:39.800]   It's horribly negative as we already talked about.
[01:39:39.800 --> 01:39:44.360]   And at the end of all of that, you either lose and then have to rebuild and maybe you're
[01:39:44.360 --> 01:39:47.400]   not in a position to be able to do that or you win.
[01:39:47.400 --> 01:39:52.380]   And then now you've got four years of being one of the most hated people, no matter how
[01:39:52.380 --> 01:39:53.720]   good a job you do.
[01:39:53.720 --> 01:39:58.340]   So I think by its nature, it turns off a lot of people that would otherwise be good.
[01:39:58.340 --> 01:40:03.160]   I also think that there's a lot of posturing from within the parties about, well, you might
[01:40:03.160 --> 01:40:04.620]   be good, but it's not your time yet.
[01:40:04.620 --> 01:40:05.620]   So you should wait.
[01:40:05.620 --> 01:40:08.240]   And then let's talk about maybe a Senate seat here and there.
[01:40:08.240 --> 01:40:10.000]   So it's like, it's like a company essentially.
[01:40:10.000 --> 01:40:12.600]   And they're figuring out where they want to place people.
[01:40:12.600 --> 01:40:16.220]   I think all of these things make it so we end up with candidates.
[01:40:16.220 --> 01:40:17.980]   Most people aren't super thrilled with.
[01:40:17.980 --> 01:40:24.560]   So it's difficult for somebody who's young or an outsider to, uh, to quickly become a
[01:40:24.560 --> 01:40:25.560]   candidate.
[01:40:25.560 --> 01:40:26.560]   I think that that's true.
[01:40:26.560 --> 01:40:31.640]   And I think also in a lot of ways, it's just not, I mean, would you want to be president?
[01:40:31.640 --> 01:40:32.640]   No.
[01:40:32.640 --> 01:40:34.480]   I mean, I can't be, cause I wasn't born in the U S.
[01:40:34.480 --> 01:40:40.320]   So it's easy for me to say, but, um, if everyone says no, then we're get the people that we
[01:40:40.320 --> 01:40:41.320]   have.
[01:40:41.320 --> 01:40:42.400]   No, I understand.
[01:40:42.400 --> 01:40:44.680]   So I would love to help somehow.
[01:40:44.680 --> 01:40:45.680]   Sure.
[01:40:45.680 --> 01:40:50.360]   And I feel like there's not even a mechanism for helping except through the democratic,
[01:40:50.360 --> 01:40:55.600]   through the voting process, but I'm just annoyed how little technology there is in the whole
[01:40:55.600 --> 01:40:59.840]   process, how little innovation there is in the whole process.
[01:40:59.840 --> 01:41:00.840]   All of this.
[01:41:00.840 --> 01:41:06.720]   And the sad thing is this is written about a lot, which is there's this thing called
[01:41:06.720 --> 01:41:09.000]   political hobbyism.
[01:41:09.000 --> 01:41:14.020]   And I think there's a good chance that some of the people in my audience are political
[01:41:14.020 --> 01:41:20.680]   hobbyists in the sense that they follow this stuff as entertainment to some degree.
[01:41:20.680 --> 01:41:26.280]   And I've written a lot about how, uh, I've read a lot and talked a lot about how, okay,
[01:41:26.280 --> 01:41:29.960]   we vote every two years or every four years in our local elections, et cetera.
[01:41:29.960 --> 01:41:32.280]   And then we think about politics all the time.
[01:41:32.280 --> 01:41:36.480]   Uh, Neil Postman wrote about this in his book, amusing ourselves to death.
[01:41:36.480 --> 01:41:41.600]   But what are you actually going to do about the kids starving in this country and the
[01:41:41.600 --> 01:41:43.400]   nuclear buildup in that country?
[01:41:43.400 --> 01:41:44.400]   It's okay.
[01:41:44.400 --> 01:41:48.160]   If everybody refocused their attention on their immediate communities, and that could
[01:41:48.160 --> 01:41:52.320]   mean any number, it could mean the town or city you live in, or it might mean, um, an
[01:41:52.320 --> 01:41:54.520]   athletic club or whatever.
[01:41:54.520 --> 01:41:58.320]   If everybody, this time they spent on political hobbyism, they moved somewhere else, which
[01:41:58.320 --> 01:41:59.440]   might put me out of a job.
[01:41:59.440 --> 01:42:00.440]   That's okay.
[01:42:00.440 --> 01:42:03.080]   I'm willing to lose my job because I think it would be so beneficial.
[01:42:03.080 --> 01:42:07.320]   Then our communities would just be that much better because you can actually affect change
[01:42:07.320 --> 01:42:11.200]   in a much more tangible way locally, whether it's obvious people talk about potholes, but
[01:42:11.200 --> 01:42:13.240]   other things as well.
[01:42:13.240 --> 01:42:14.920]   Yeah.
[01:42:14.920 --> 01:42:18.800]   And I wish our system was more amenable to that kind of contribution.
[01:42:18.800 --> 01:42:20.920]   Hopefully through the digital space, it would be.
[01:42:20.920 --> 01:42:27.080]   Uh, let me ask you about on the Republican side, Ron DeSantis, what do you think of him
[01:42:27.080 --> 01:42:30.080]   as a candidate running against Donald Trump?
[01:42:30.080 --> 01:42:36.480]   I think in the couple of weeks before our discussion today, his campaign, which hasn't
[01:42:36.480 --> 01:42:40.520]   even started, has sort of started to implode.
[01:42:40.520 --> 01:42:45.360]   And this was something that I started thinking about in September, October.
[01:42:45.360 --> 01:42:50.520]   He really doesn't seem ready for prime time in the sense that just being confronted and
[01:42:50.520 --> 01:42:51.960]   confronted is not even the right word.
[01:42:51.960 --> 01:42:57.960]   Just being asked about some topics he didn't really seem to want to talk about.
[01:42:57.960 --> 01:43:02.920]   He responded in such a sort of disproportionate, unhinged way.
[01:43:02.920 --> 01:43:08.800]   During his recent trip to Asia, he was asked about why aren't you or why are you responding,
[01:43:08.800 --> 01:43:12.000]   but in this weird way to Trump's attacks on you.
[01:43:12.000 --> 01:43:16.640]   And he went into this weird bobblehead thing with a weird smile and something came out
[01:43:16.640 --> 01:43:17.720]   that didn't make any sense.
[01:43:17.720 --> 01:43:19.520]   And he sort of got mad at the reporter.
[01:43:19.520 --> 01:43:23.800]   And it was just like, if you can't handle that, you can't be on a debate stage with
[01:43:23.800 --> 01:43:24.800]   Donald Trump.
[01:43:24.800 --> 01:43:28.200]   And again, for all my criticisms of Trump, the guy gets you on a debate stage.
[01:43:28.200 --> 01:43:30.480]   He can make you look pretty silly.
[01:43:30.480 --> 01:43:37.200]   He was recently asked about his role at Guantanamo Bay when he was an officer in, I forget which
[01:43:37.200 --> 01:43:39.200]   branch of the armed forces.
[01:43:39.200 --> 01:43:44.200]   And he just sort of attacked the journalist asking the question and it just looked very
[01:43:44.200 --> 01:43:45.580]   bad.
[01:43:45.580 --> 01:43:49.040]   And there are increasingly big Republican donors who are not fans of Trump and were
[01:43:49.040 --> 01:43:52.680]   sort of hoping to put their eggs in the Rhonda Santas basket who are saying, this guy just
[01:43:52.680 --> 01:43:53.680]   doesn't have what it takes.
[01:43:53.680 --> 01:43:54.880]   I don't think he can do it.
[01:43:54.880 --> 01:43:59.520]   So I don't know if DeSantis will be able to get away once you're polling 20 something
[01:43:59.520 --> 01:44:03.320]   like he is and you haven't even announced it's very attractive.
[01:44:03.320 --> 01:44:08.120]   And he probably to some degree is thinking, if I wait till 2028 I might not have this
[01:44:08.120 --> 01:44:09.840]   opportunity again.
[01:44:09.840 --> 01:44:15.680]   But Trump's polling 52 53 which means even if DeSantis gets all of the current non Trump
[01:44:15.680 --> 01:44:19.520]   vote, he has to figure out how to take something more from Trump.
[01:44:19.520 --> 01:44:21.640]   I just don't know how he does it.
[01:44:21.640 --> 01:44:24.640]   First of all, the implosion aspect, that's part of the process, isn't it?
[01:44:24.640 --> 01:44:29.400]   You kind of implode a bunch of times.
[01:44:29.400 --> 01:44:30.400]   And then rebuild yourself.
[01:44:30.400 --> 01:44:33.440]   And rebuild and because the new cycle kind of forgets.
[01:44:33.440 --> 01:44:34.480]   It's possible.
[01:44:34.480 --> 01:44:41.280]   The problem is if the first debate is in August, so that's only a few months away and the decision
[01:44:41.280 --> 01:44:44.000]   is going to have to be made pretty soon.
[01:44:44.000 --> 01:44:49.640]   And unless he can get a new momentum going, uh, I just don't know how he gets what he
[01:44:49.640 --> 01:44:51.780]   needs in order to really have a shot.
[01:44:51.780 --> 01:44:54.080]   So would anyone else run against Donald Trump?
[01:44:54.080 --> 01:44:55.080]   It's very tough right now.
[01:44:55.080 --> 01:44:56.400]   I mean, there are other people running.
[01:44:56.400 --> 01:45:01.760]   There's this guy, Vivek Ramaswamy, who's running a Nikki Haley is running her campaign basically
[01:45:01.760 --> 01:45:03.200]   dead on arrival.
[01:45:03.200 --> 01:45:05.620]   Um, Trump actually does better in polls.
[01:45:05.620 --> 01:45:10.160]   The more people run when, when it's just him and DeSantis, that's the best scenario for
[01:45:10.160 --> 01:45:11.240]   DeSantis.
[01:45:11.240 --> 01:45:15.000]   It's not great for DeSantis, but it's, it's certainly better.
[01:45:15.000 --> 01:45:20.880]   Um, but I think the difficulty is this is a question for Republicans to figure out the
[01:45:20.880 --> 01:45:25.800]   people who rightly recognized in 2016 that this guy is not good for their party.
[01:45:25.800 --> 01:45:30.800]   Um, still believe this guy is not good for their party, but many of them recognize that
[01:45:30.800 --> 01:45:34.360]   most of the voters are still behind him.
[01:45:34.360 --> 01:45:36.720]   You can always say it's early.
[01:45:36.720 --> 01:45:38.320]   His polling doesn't really mean anything.
[01:45:38.320 --> 01:45:39.320]   Anything could happen.
[01:45:39.320 --> 01:45:42.400]   Anything major would have to happen for Trump to lose that lead.
[01:45:42.400 --> 01:45:46.480]   If he got more, uh, if he was arrested two more times and had more indictments and it
[01:45:46.480 --> 01:45:51.440]   just became like this guy can't even campaign because he's so busy going from court to court,
[01:45:51.440 --> 01:45:52.720]   maybe that would make a difference.
[01:45:52.720 --> 01:45:55.520]   It's really tough to imagine.
[01:45:55.520 --> 01:45:59.880]   You said that there are three categories of people who vote Republican and uh, that Trump
[01:45:59.880 --> 01:46:01.400]   introduced the fourth one.
[01:46:01.400 --> 01:46:04.920]   Well, can you go through the four categories?
[01:46:04.920 --> 01:46:05.920]   Sure.
[01:46:05.920 --> 01:46:09.100]   So you've got like your pro business, low tax Republicans.
[01:46:09.100 --> 01:46:11.520]   These are mostly people like Mitt Romney.
[01:46:11.520 --> 01:46:13.920]   Mitt Romney has a bit of the social conservatism as well.
[01:46:13.920 --> 01:46:15.120]   He's Mormon and that's there.
[01:46:15.120 --> 01:46:18.080]   But Mitt Romney primarily, particularly as a Northeast sort of Republican.
[01:46:18.080 --> 01:46:23.960]   I mean, I know Utah, but governor of Massachusetts, he is like a low tax pro business type guy.
[01:46:23.960 --> 01:46:30.520]   Um, you've got your libertarian type Republicans who are primarily about freedom and liberty.
[01:46:30.520 --> 01:46:34.240]   Often they are actually more socially liberal where they go, I don't care about gay marriage,
[01:46:34.240 --> 01:46:37.360]   liberal, you know, I don't care so much about abortion.
[01:46:37.360 --> 01:46:42.740]   Um, and that overlapped a little bit with the tea party movement in 2010 although tea
[01:46:42.740 --> 01:46:46.700]   party did have a religious component, but sort of like the libertarian freedom minded
[01:46:46.700 --> 01:46:50.720]   folks and then the religious conservatives, people that support candidates like Josh Hawley
[01:46:50.720 --> 01:46:56.180]   or uh, Ted Cruz, et cetera, where their big thing are social issues.
[01:46:56.180 --> 01:47:00.940]   Often they actually want a Christianity being civil government.
[01:47:00.940 --> 01:47:02.880]   They don't want separation of church and state.
[01:47:02.880 --> 01:47:05.460]   Those are traditionally the three Republican groups.
[01:47:05.460 --> 01:47:10.000]   The one that Trump introduced was people who just didn't really pay attention to politics,
[01:47:10.000 --> 01:47:16.140]   but either followed celebrity or had grievances that they didn't yet have a scapegoat for.
[01:47:16.140 --> 01:47:21.720]   Um, and we're sort of right leaning culturally even though they didn't attribute that to
[01:47:21.720 --> 01:47:27.020]   Republicanism and Trump was able to bring them into politics often for the first time
[01:47:27.020 --> 01:47:28.120]   as voters.
[01:47:28.120 --> 01:47:33.680]   They could be part of any of those three groups if they get more into politics or kind of
[01:47:33.680 --> 01:47:34.880]   be their own thing.
[01:47:34.880 --> 01:47:37.940]   But they're more kind of like cult of personality.
[01:47:37.940 --> 01:47:39.400]   I'm here for Trump types.
[01:47:39.400 --> 01:47:43.840]   Did it have to do anything about the culture wars and the identity politics, all that kind
[01:47:43.840 --> 01:47:44.840]   of stuff?
[01:47:44.840 --> 01:47:45.840]   Yeah.
[01:47:45.840 --> 01:47:50.080]   I mean, so in 2016 when Trump mobilized them, those weren't really issues the way they are
[01:47:50.080 --> 01:47:51.080]   now.
[01:47:51.080 --> 01:47:55.160]   So I think at the top at that time it certainly was not a factor.
[01:47:55.160 --> 01:47:56.600]   What was the mobilizing issue?
[01:47:56.600 --> 01:48:00.000]   I know it was a just anti Hillary in 2016 there.
[01:48:00.000 --> 01:48:06.680]   He did a good job on anti Hillary, but a lot of it was identifying real economic problems,
[01:48:06.680 --> 01:48:11.520]   wage depression, lack of jobs in parts of the country, you know, Ohio and Indiana.
[01:48:11.520 --> 01:48:15.320]   Trump rightly identified like we have an issue here, we don't have enough entrepreneurship,
[01:48:15.320 --> 01:48:16.320]   et cetera.
[01:48:16.320 --> 01:48:20.600]   Um, but there was also a lot of scapegoating that was, you know, China and people coming
[01:48:20.600 --> 01:48:25.920]   through the U S Mexico border were popular scapegoats for a lot of those problems.
[01:48:25.920 --> 01:48:29.040]   This gets us kind of to populism.
[01:48:29.040 --> 01:48:35.680]   Populism is a rhetoric and populism as a rhetoric doesn't necessarily come with particular
[01:48:35.680 --> 01:48:36.680]   policies.
[01:48:36.680 --> 01:48:43.000]   You can be a populist, a user of populist rhetoric and propose solutions that would
[01:48:43.000 --> 01:48:46.800]   be more aligned with Bernie or Tucker Carlson.
[01:48:46.800 --> 01:48:50.400]   Populists will often identify the plight of the middle class.
[01:48:50.400 --> 01:48:56.080]   The difference would be Bernie will say, we've got to put some restrictions on how much a
[01:48:56.080 --> 01:49:00.600]   billionaires can make and we've got to reinvest in these social programs.
[01:49:00.600 --> 01:49:07.520]   Tucker will say BLM taking your house and a brown person from Mexico taking your job
[01:49:07.520 --> 01:49:09.680]   are what we need to deal with.
[01:49:09.680 --> 01:49:14.600]   So the, the populist rhetoric can lend its lend itself to very different policy and Trump
[01:49:14.600 --> 01:49:17.120]   used that very effectively in 2016.
[01:49:17.120 --> 01:49:23.000]   What do you think Hillary Clinton was hated as intensely as she was by a certain percent
[01:49:23.000 --> 01:49:24.440]   of the population?
[01:49:24.440 --> 01:49:30.800]   It feels like, um, that's the first election I witnessed where there's a lot of hate.
[01:49:30.800 --> 01:49:32.920]   Maybe I'm misremembering.
[01:49:32.920 --> 01:49:36.080]   I don't remember Obama.
[01:49:36.080 --> 01:49:37.800]   I don't remember the degree of hate.
[01:49:37.800 --> 01:49:42.080]   There was a conspiracy theory that he wasn't born in this country, but I don't remember
[01:49:42.080 --> 01:49:43.960]   hate towards Obama.
[01:49:43.960 --> 01:49:47.360]   Record death threats under Obama more than any previous president.
[01:49:47.360 --> 01:49:48.360]   Towards who?
[01:49:48.360 --> 01:49:49.360]   Towards him.
[01:49:49.360 --> 01:49:50.360]   Yeah.
[01:49:50.360 --> 01:49:53.280]   Do you mean more hate between voters or between voters?
[01:49:53.280 --> 01:49:54.280]   Between voters.
[01:49:54.280 --> 01:50:01.120]   But like, that's, I guess what I was speaking to, but that hate was directed towards, um,
[01:50:01.120 --> 01:50:07.440]   the narrative, the thread that connected all of that in 2016 was Hillary Clinton.
[01:50:07.440 --> 01:50:09.080]   Few different things.
[01:50:09.080 --> 01:50:12.220]   Um, and I'm not ranking these.
[01:50:12.220 --> 01:50:14.920]   These are just all things that come to mind.
[01:50:14.920 --> 01:50:21.560]   One is Hillary Clinton had been around in the political space for a long time from her
[01:50:21.560 --> 01:50:26.560]   time as first lady through a Senator, Secretary of State, et cetera.
[01:50:26.560 --> 01:50:34.040]   So I think that there was enough time for different groups to develop an antipathy towards
[01:50:34.040 --> 01:50:35.440]   her for different reasons.
[01:50:35.440 --> 01:50:36.920]   So time.
[01:50:36.920 --> 01:50:45.040]   Um, secondly, Trump's branding of her as crooked was very effective where there were so many
[01:50:45.040 --> 01:50:48.020]   people demanding that she be imprisoned.
[01:50:48.020 --> 01:50:52.440]   If you ask them what is the crime, they don't know, but she should definitely be locked
[01:50:52.440 --> 01:50:53.440]   up.
[01:50:53.440 --> 01:50:55.480]   That became a very big thing.
[01:50:55.480 --> 01:51:03.720]   The email story as it were, and, uh, James Comey doing a second public event about that
[01:51:03.720 --> 01:51:06.740]   investigation, even though there wasn't any actual news about it, just doing a second
[01:51:06.740 --> 01:51:12.060]   event about it at the last minute, I think hurt her and also generated some hate.
[01:51:12.060 --> 01:51:16.240]   And I don't find Hillary Clinton to be particularly likable, although I voted for her, I thought
[01:51:16.240 --> 01:51:18.280]   she was the better candidate.
[01:51:18.280 --> 01:51:23.720]   And I think that there are others who also didn't find her particularly likable that
[01:51:23.720 --> 01:51:26.040]   those are a lot of impediments to becoming president.
[01:51:26.040 --> 01:51:30.960]   I was trying to understand why there's so many conspiracy theories about Clinton's general
[01:51:30.960 --> 01:51:33.640]   Bill Clinton and Hillary Clinton.
[01:51:33.640 --> 01:51:39.920]   And I, maybe I'm not researched well enough of the why of it.
[01:51:39.920 --> 01:51:44.360]   The why of it, actually the extent of the conspiracy theories, the sort of the, the
[01:51:44.360 --> 01:51:48.240]   conspiracy theories that they've killed a lot of people, this kind of stuff.
[01:51:48.240 --> 01:51:51.440]   It's hard for me to speak to them because I'm aware that they exist, but I'm not an
[01:51:51.440 --> 01:51:57.040]   expert in them because they seem so obviously baseless to the degree that I've researched
[01:51:57.040 --> 01:51:58.840]   them a little bit and then I move on.
[01:51:58.840 --> 01:52:01.440]   And, uh, you know, it's been years since I've looked at this stuff.
[01:52:01.440 --> 01:52:06.480]   I know there's the Seth Rich one and there's the Clinton body count one.
[01:52:06.480 --> 01:52:10.040]   I think there was one connected to Epstein if I recall correctly.
[01:52:10.040 --> 01:52:11.960]   There's all sorts of these different ones.
[01:52:11.960 --> 01:52:17.000]   Um, without speaking to any of them specifically because I'm not the expert on Clinton conspiracies,
[01:52:17.000 --> 01:52:22.800]   uh, it does seem as though this stuff for so long has generated an audience.
[01:52:22.800 --> 01:52:27.880]   I mean, I remember in the supermarket when Bill Clinton was president at the checkout
[01:52:27.880 --> 01:52:32.680]   seeing the tabloids and there were stuff about Hillary birthed an alien baby and you know,
[01:52:32.680 --> 01:52:36.520]   all the, it seems like it's been titillating to people for a very long time.
[01:52:36.520 --> 01:52:41.400]   Well, another question from Reddit, speaking of aliens, uh, I would be curious to hear
[01:52:41.400 --> 01:52:47.280]   David's views on conspiracies and conspiracy theories, the extent to which real conspiracies
[01:52:47.280 --> 01:52:53.960]   happen and why conspiracies that have little evidence behind them managed to be so compelling
[01:52:53.960 --> 01:52:55.440]   to people regardless.
[01:52:55.440 --> 01:52:58.480]   Also, please bring up aliens and UAPs.
[01:52:58.480 --> 01:53:00.640]   Okay, where do we start?
[01:53:00.640 --> 01:53:01.640]   The conspiracy.
[01:53:01.640 --> 01:53:09.640]   What, uh, so what in general as a person who, uh, thinks about politics, uh, thinks about
[01:53:09.640 --> 01:53:13.800]   this world, like where do conspiracy theories fit in for you?
[01:53:13.800 --> 01:53:19.480]   I think there have been conspiracies and by conspiracies I'm using a colloquial definition,
[01:53:19.480 --> 01:53:26.640]   which is basically, uh, individuals working together to, um, in a clandestine way, uh,
[01:53:26.640 --> 01:53:30.840]   impact or affect some kind of event or phenomenon.
[01:53:30.840 --> 01:53:32.640]   Uh, very, very broadly.
[01:53:32.640 --> 01:53:36.040]   I mean, certainly that those things have happened.
[01:53:36.040 --> 01:53:40.440]   Um, the, to pick, to jump around to some of the things that were in there.
[01:53:40.440 --> 01:53:47.520]   I think the reason that conspiracy theories are so compelling is that it's really tough
[01:53:47.520 --> 01:53:50.640]   for a lot of people to accept.
[01:53:50.640 --> 01:53:56.440]   There are random events, not predictable specifically at a stochastic level.
[01:53:56.440 --> 01:54:02.920]   We might be able to predict them, but specifically unpredictable, bad events in many ways.
[01:54:02.920 --> 01:54:06.600]   I could be the victim of one or you could, or my family could.
[01:54:06.600 --> 01:54:09.740]   That's really scary to a lot of people, understandably so.
[01:54:09.740 --> 01:54:15.760]   And for some people it's less scary and more soothing in a way to say there aren't really
[01:54:15.760 --> 01:54:18.160]   random events like this.
[01:54:18.160 --> 01:54:23.000]   Somebody planned it and if we had just known who planned it, it just could have been stopped
[01:54:23.000 --> 01:54:29.640]   because we would have known exactly when that's just a psychological level easier to accept
[01:54:29.640 --> 01:54:30.860]   for people.
[01:54:30.860 --> 01:54:36.900]   And I get that to some degree because listen, it's, it's not the most exciting thing that
[01:54:36.900 --> 01:54:41.440]   everything can just be going fine and something absolutely horrible happens and kills who
[01:54:41.440 --> 01:54:44.040]   knows some number of people or.
[01:54:44.040 --> 01:54:48.040]   So I think that's the biggest attractor to a lot of these conspiracy theories.
[01:54:48.040 --> 01:54:49.440]   It doesn't apply to all of them though.
[01:54:49.440 --> 01:54:55.520]   But yeah, but there's still kind of a basic understanding of human nature where people,
[01:54:55.520 --> 01:55:00.420]   some people are greedy and want power and are corrupted by power.
[01:55:00.420 --> 01:55:09.720]   So there's kind of these compelling narratives that stick that, I don't know, the vaccine
[01:55:09.720 --> 01:55:16.120]   is a, is a opportunity for a powerful billionaire to implant chips into you so he can control
[01:55:16.120 --> 01:55:17.120]   you further.
[01:55:17.120 --> 01:55:18.120]   - Right.
[01:55:18.120 --> 01:55:24.120]   - It doesn't seem, what do I want to say?
[01:55:24.120 --> 01:55:29.280]   It's like for some reason that doesn't seem as crazy as it should.
[01:55:29.280 --> 01:55:30.280]   (laughing)
[01:55:30.280 --> 01:55:33.240]   'Cause you think like maybe Hollywood contributes to that.
[01:55:33.240 --> 01:55:39.160]   But you think, yeah, you could imagine an evil person, a person that wants more control,
[01:55:39.160 --> 01:55:45.240]   more power, and is also at the same time able to convince themselves, as history shows,
[01:55:45.240 --> 01:55:48.880]   that they actually have the best interests of the populace in mind, that they're trying
[01:55:48.880 --> 01:55:51.040]   to do good for the world.
[01:55:51.040 --> 01:55:53.240]   So they do evil while trying to do good.
[01:55:53.240 --> 01:55:55.040]   You can kind of imagine it.
[01:55:55.040 --> 01:55:57.400]   So it's like, why not?
[01:55:57.400 --> 01:56:06.600]   And you listen to people in power, authorities, they kind of look and sound shady, you know?
[01:56:06.600 --> 01:56:12.680]   Like the transparency, especially the older ones, I think younger folks are better at
[01:56:12.680 --> 01:56:18.000]   being like real and transparent and just like revealing their flaws and the basic humanity.
[01:56:18.000 --> 01:56:21.960]   But people that are a little bit older in the positions of power, they're more polished.
[01:56:21.960 --> 01:56:26.520]   They're more like, it feels like they're presenting a narrative where the truth is hidden in the
[01:56:26.520 --> 01:56:27.520]   shadows.
[01:56:27.520 --> 01:56:28.520]   So-
[01:56:28.520 --> 01:56:33.800]   I don't think there's anything wrong with suspecting maybe a public figure isn't giving
[01:56:33.800 --> 01:56:34.800]   me the full story.
[01:56:34.800 --> 01:56:35.800]   Yeah.
[01:56:35.800 --> 01:56:38.320]   Totally reasonable thing to question.
[01:56:38.320 --> 01:56:43.800]   I don't think there's anything wrong with exploring a lot of these different things.
[01:56:43.800 --> 01:56:48.240]   I think the problem becomes, and I know you've talked about this in so many different ways
[01:56:48.240 --> 01:56:53.600]   with other guests, the problem becomes when we lose a shared understanding of how we would
[01:56:53.600 --> 01:57:01.200]   assess whether any of these things are true and then both alleged evidence and an absence
[01:57:01.200 --> 01:57:05.960]   of evidence both become supportive of the conspiracy theory.
[01:57:05.960 --> 01:57:09.520]   Because if there's bad evidence, you manipulate it and say it's good evidence.
[01:57:09.520 --> 01:57:13.280]   If there's no evidence, you say the evidence was obviously hidden by the people who carried
[01:57:13.280 --> 01:57:15.500]   out the thing or whatever.
[01:57:15.500 --> 01:57:19.400]   So unless we can have a shared understanding of how we would determine what's true, these
[01:57:19.400 --> 01:57:23.800]   are common conversations often between atheists and religious folks.
[01:57:23.800 --> 01:57:28.440]   How can we deter, like is, is my faith in something or my desire for something to be
[01:57:28.440 --> 01:57:30.960]   true a good way to evaluate whether it is true?
[01:57:30.960 --> 01:57:32.480]   They're really similar questions.
[01:57:32.480 --> 01:57:38.960]   Well, let me ask you about Trump on that front, about the election, 2020 election, and maybe
[01:57:38.960 --> 01:57:40.920]   the better question is about January 6th.
[01:57:40.920 --> 01:57:44.040]   Do you think January 6th was a big deal?
[01:57:44.040 --> 01:57:45.600]   I do.
[01:57:45.600 --> 01:57:49.000]   How big of a deal compared to what?
[01:57:49.000 --> 01:57:50.000]   Uh, civil war.
[01:57:50.000 --> 01:57:53.440]   I think it was less of a big deal than the civil war.
[01:57:53.440 --> 01:57:54.440]   Okay.
[01:57:54.440 --> 01:57:57.080]   No, I mean, so you, well, it's a very interesting thing though, right?
[01:57:57.080 --> 01:58:00.840]   Because we have not only the, the event that's, that's clever actually.
[01:58:00.840 --> 01:58:07.680]   It's not only the event, but it's what led up to it and what has happened since and did
[01:58:07.680 --> 01:58:17.360]   it change what is considered on the table that citizens can, should or might do if they
[01:58:17.360 --> 01:58:20.220]   disagree with the results of an election.
[01:58:20.220 --> 01:58:25.280]   So I think that there are further reaching consequences than just was the six hour period
[01:58:25.280 --> 01:58:29.320]   on January 6th, a bigger or smaller deal than the civil war.
[01:58:29.320 --> 01:58:32.360]   And there's so much wrapped up into it.
[01:58:32.360 --> 01:58:37.160]   Um, many conspiracy theories flowed from January 6th as well.
[01:58:37.160 --> 01:58:42.280]   Uh, 60 minutes recently featured a guy named Ray Epps who was targeted by some on the right,
[01:58:42.280 --> 01:58:46.520]   um, claiming that he was an instigator or an agent of the FBI or something along those
[01:58:46.520 --> 01:58:47.520]   lines.
[01:58:47.520 --> 01:58:52.760]   Uh, there were people claiming that no real, it was like a no true Scotsman sort of thing.
[01:58:52.760 --> 01:58:55.260]   Like Trump supporters wouldn't riot.
[01:58:55.260 --> 01:59:01.120]   So by definition it must have been Antifa, uh, police let him in or police, you know,
[01:59:01.120 --> 01:59:02.120]   all these different things.
[01:59:02.120 --> 01:59:05.840]   I think it was a big deal in a lot of ways because it completely made us have to go back
[01:59:05.840 --> 01:59:12.120]   to the top to say, okay, what are the parameters of valid discussion and activism in the United
[01:59:12.120 --> 01:59:13.120]   States?
[01:59:13.120 --> 01:59:18.880]   But what aspect of the January 6th was bad for you?
[01:59:18.880 --> 01:59:27.880]   Well, I mean, if you're thinking of from a big philosophical political perspective, so
[01:59:27.880 --> 01:59:35.040]   presumably the number of people hurt and the number of people who died is not the only
[01:59:35.040 --> 01:59:36.720]   metric to consider here.
[01:59:36.720 --> 01:59:37.720]   Absolutely.
[01:59:37.720 --> 01:59:45.480]   I think the sum total of what it means about how the United States operates is what's most
[01:59:45.480 --> 01:59:48.520]   concerning and I'll kind of just like flesh it out a little bit.
[01:59:48.520 --> 01:59:55.600]   So summer of 2020 Trump's already saying they're going to cheat.
[01:59:55.600 --> 02:00:00.960]   Now the polling is close, but it shows that Biden's in a good position.
[02:00:00.960 --> 02:00:02.880]   People aren't happy with Trump.
[02:00:02.880 --> 02:00:07.520]   Any reasonable person would look and say it's going to be close, but Biden certainly wouldn't
[02:00:07.520 --> 02:00:08.520]   be a crazy thing.
[02:00:08.520 --> 02:00:12.400]   If Biden won, Trump's already saying they're going to cheat with mail in ballots or they're
[02:00:12.400 --> 02:00:15.600]   going to cheat with early voting or you're going to cheat with machines or we should
[02:00:15.600 --> 02:00:18.760]   do only in person or whatever else the case may be.
[02:00:18.760 --> 02:00:20.640]   We have the election.
[02:00:20.640 --> 02:00:24.840]   We knew in certain states how the vote count was going to go.
[02:00:24.840 --> 02:00:26.840]   Some states stop counting at 10 PM.
[02:00:26.840 --> 02:00:28.960]   Some states count all of the mail and stuff up front.
[02:00:28.960 --> 02:00:30.320]   Some don't.
[02:00:30.320 --> 02:00:33.120]   Everything was completely predictable.
[02:00:33.120 --> 02:00:36.080]   At 2 a.m. Trump comes out and says, I won.
[02:00:36.080 --> 02:00:37.080]   Okay.
[02:00:37.080 --> 02:00:38.080]   Where are you?
[02:00:38.080 --> 02:00:39.080]   Where are you getting that, sir?
[02:00:39.080 --> 02:00:40.800]   As he claims, people always refer to him.
[02:00:40.800 --> 02:00:42.440]   Where are you getting that?
[02:00:42.440 --> 02:00:49.040]   And with that statement, immediately we see that there is a large portion of this country
[02:00:49.040 --> 02:00:54.920]   that either is unable or unwilling to say, wait a second, the polling all said this was
[02:00:54.920 --> 02:00:57.120]   a real possibility.
[02:00:57.120 --> 02:01:02.680]   The counting schedules are all being adhered to all, but Trump won.
[02:01:02.680 --> 02:01:04.160]   That doesn't make any sense.
[02:01:04.160 --> 02:01:05.520]   That doesn't happen.
[02:01:05.520 --> 02:01:06.600]   It builds.
[02:01:06.600 --> 02:01:11.600]   People are donating millions to Trump for supposed audits, which nobody can define and
[02:01:11.600 --> 02:01:13.160]   lawsuits which go nowhere.
[02:01:13.160 --> 02:01:14.880]   And it builds and builds and builds.
[02:01:14.880 --> 02:01:17.880]   And we have a total separation from a factual reality.
[02:01:17.880 --> 02:01:21.080]   There's no reason to think by December 1st, right?
[02:01:21.080 --> 02:01:23.540]   Give three weeks to look through some of this stuff.
[02:01:23.540 --> 02:01:28.220]   By December 1st, there's no reasonable case to be made that Trump actually won.
[02:01:28.220 --> 02:01:29.520]   But it doesn't end there.
[02:01:29.520 --> 02:01:35.800]   It goes into maybe we can just like send different electors, even though Biden won Arizona.
[02:01:35.800 --> 02:01:36.800]   Let's just like send.
[02:01:36.800 --> 02:01:38.840]   I don't remember how many electors it is in Arizona.
[02:01:38.840 --> 02:01:42.040]   Let's just like send Republican electors to say we vote for Trump.
[02:01:42.040 --> 02:01:43.440]   But that's that's not democracy.
[02:01:43.440 --> 02:01:45.440]   That's not the way the system works.
[02:01:45.440 --> 02:01:49.280]   Let's make sure we're ready, ready for what exactly.
[02:01:49.280 --> 02:01:55.280]   And then it builds to maybe Mike Pence can just like prevent Biden from being president
[02:01:55.280 --> 02:01:58.320]   or maybe we can just interfere in this other way.
[02:01:58.320 --> 02:02:02.560]   And then it gets to let's break into the Capitol.
[02:02:02.560 --> 02:02:10.960]   It's the height of saying we no longer comport ourselves attached to what is a verifiable
[02:02:10.960 --> 02:02:13.000]   factual reality.
[02:02:13.000 --> 02:02:20.000]   And when we no longer do that, we're also willing to commit crimes, property crimes,
[02:02:20.000 --> 02:02:25.840]   violent crimes, different degrees in order to try to have something other than democracy.
[02:02:25.840 --> 02:02:29.840]   It wouldn't be democracy if any of those things had happened.
[02:02:29.840 --> 02:02:32.480]   Yeah, I think it's not the height of it.
[02:02:32.480 --> 02:02:42.240]   I think there's still a case to be made that that did not leave the realm of protest versus
[02:02:42.240 --> 02:02:45.040]   a violation of the principles of democracy.
[02:02:45.040 --> 02:02:51.960]   So to me, the height of what could happen on January 6th is if Donald Trump was much
[02:02:51.960 --> 02:02:55.800]   better executive, he could take control of the military.
[02:02:55.800 --> 02:02:56.800]   If it had succeeded.
[02:02:56.800 --> 02:02:59.200]   No, not even succeeded.
[02:02:59.200 --> 02:03:03.840]   The attempt would have been more empowered.
[02:03:03.840 --> 02:03:04.840]   I understand.
[02:03:04.840 --> 02:03:09.800]   So like the way not to bring up Hitler, every other word, which is something your subreddit
[02:03:09.800 --> 02:03:12.000]   also told me not to do.
[02:03:12.000 --> 02:03:13.720]   It's kind of an important figure.
[02:03:13.720 --> 02:03:17.800]   It's interesting to study that moment in history because it reveals so much about human nature
[02:03:17.800 --> 02:03:20.200]   and that all of us are capable of good and evil.
[02:03:20.200 --> 02:03:25.240]   But thank you, dear subredditor or Redditor for your contribution to the conversation.
[02:03:25.240 --> 02:03:29.880]   I will keep bringing up Hitler and the Third Reich and I'll keep bringing up Stalin.
[02:03:29.880 --> 02:03:32.320]   There's so much to learn from that.
[02:03:32.320 --> 02:03:42.160]   Anyway, an effective practice of authoritarian could roll the tanks out into the city streets
[02:03:42.160 --> 02:03:51.160]   to establish order and in so doing, pause the process of democracy as opposed to a few
[02:03:51.160 --> 02:03:57.000]   protesters breaking in to a questionably protected building.
[02:03:57.000 --> 02:03:58.680]   I agree that what you're saying would be worse.
[02:03:58.680 --> 02:04:03.000]   I don't want to use it to minimize what the protesters were intent on doing.
[02:04:03.000 --> 02:04:04.000]   They failed, fortunately.
[02:04:04.000 --> 02:04:05.120]   I was both to you.
[02:04:05.120 --> 02:04:06.120]   The intention was there.
[02:04:06.120 --> 02:04:10.280]   Well, the intention was Trump should remain president.
[02:04:10.280 --> 02:04:11.280]   That's the intention.
[02:04:11.280 --> 02:04:17.320]   And to what length they would have been willing to go if by the evening, early evening, they,
[02:04:17.320 --> 02:04:18.520]   you know, were sort of like forced out.
[02:04:18.520 --> 02:04:19.520]   I don't know.
[02:04:19.520 --> 02:04:23.680]   I agree with you that Trump trying to use the military would absolutely be worse.
[02:04:23.680 --> 02:04:26.920]   You know, there's these reports that he tried to seize voting machines, which is kind of
[02:04:26.920 --> 02:04:30.600]   funny because it's like once you get the machine at Mar-a-Lago, what do you do with it?
[02:04:30.600 --> 02:04:31.600]   Exactly.
[02:04:31.600 --> 02:04:32.600]   I don't know.
[02:04:32.600 --> 02:04:35.240]   There's a like a comedic element to Trump sitting around with voting machines, but he
[02:04:35.240 --> 02:04:39.080]   did float trying to do some other things.
[02:04:39.080 --> 02:04:43.880]   I don't believe there's reporting that he actually tried to use the military.
[02:04:43.880 --> 02:04:49.520]   I wonder to what degree this opened the door to further things like this with other other
[02:04:49.520 --> 02:04:55.240]   candidates on, you know, even in the Democratic Party also.
[02:04:55.240 --> 02:04:59.040]   Do you think there'll be more and more questioning of the election results?
[02:04:59.040 --> 02:05:00.200]   There has been already.
[02:05:00.200 --> 02:05:02.160]   It's very clearly the playbook.
[02:05:02.160 --> 02:05:03.680]   Carrie Lake lost.
[02:05:03.680 --> 02:05:05.920]   She ran for governor in Arizona, 2022.
[02:05:05.920 --> 02:05:07.000]   She lost.
[02:05:07.000 --> 02:05:09.240]   What I mean by that is her opponent received more votes.
[02:05:09.240 --> 02:05:12.360]   It's like very clear what it means that she lost.
[02:05:12.360 --> 02:05:15.040]   She insists to this day that she won.
[02:05:15.040 --> 02:05:19.920]   To this day, she did the same grift Trump did about donate.
[02:05:19.920 --> 02:05:20.920]   We've got a case.
[02:05:20.920 --> 02:05:21.920]   We won in the case.
[02:05:21.920 --> 02:05:22.920]   You didn't win.
[02:05:22.920 --> 02:05:26.560]   They just set a court date like that's not what doesn't know what you know, lies upon
[02:05:26.560 --> 02:05:28.080]   grift upon lies.
[02:05:28.080 --> 02:05:30.440]   So they did it then.
[02:05:30.440 --> 02:05:36.320]   It is I it's extraordinarily saddening, but it seems like this is now going to be part
[02:05:36.320 --> 02:05:37.320]   of the playbook.
[02:05:37.320 --> 02:05:40.400]   Do you think people on the left will start doing it?
[02:05:40.400 --> 02:05:44.360]   I don't have a reason to believe that that is going to happen, but I'm not going to say
[02:05:44.360 --> 02:05:45.360]   it never could.
[02:05:45.360 --> 02:05:46.360]   Absolutely.
[02:05:46.360 --> 02:05:47.360]   It certainly could.
[02:05:47.360 --> 02:05:49.440]   People on the left could start using it as a tactic right now.
[02:05:49.440 --> 02:05:52.400]   There's not a sign that that's going to happen, but it's certainly good.
[02:05:52.400 --> 02:05:57.760]   My expectation is, and I'm not a betting man, but I would bet money if Joe Biden loses in
[02:05:57.760 --> 02:06:01.720]   November of 2024, he will say I lost.
[02:06:01.720 --> 02:06:03.760]   He will call the winner.
[02:06:03.760 --> 02:06:07.700]   He will concede and he will leave the White House in an orderly fashion.
[02:06:07.700 --> 02:06:10.880]   You don't think there'll be claims of a hacked election?
[02:06:10.880 --> 02:06:17.900]   The ability to hack elections is becoming, uh, more and more effective with the developments
[02:06:17.900 --> 02:06:19.860]   on the artificial intelligence side.
[02:06:19.860 --> 02:06:26.180]   The difficulty is you're basically saying, will something happen without me knowing anything
[02:06:26.180 --> 02:06:27.820]   about the election?
[02:06:27.820 --> 02:06:30.280]   Imagine there really was evidence of a hacked election.
[02:06:30.280 --> 02:06:32.660]   Then I would want those claims to be made.
[02:06:32.660 --> 02:06:36.860]   But the way elections have gone in the past, I don't expect that that's a claim that would
[02:06:36.860 --> 02:06:37.860]   be made.
[02:06:37.860 --> 02:06:38.860]   No.
[02:06:38.860 --> 02:06:44.240]   Speaking of evidence of things, uh, that were claimed, what do you think about the Hunter
[02:06:44.240 --> 02:06:52.020]   Biden laptop or as you tweeted the laptop from hell, the laptop from hell TM, right?
[02:06:52.020 --> 02:06:58.780]   Uh, to what degree was this, uh, laptop story important and to what degree was it not?
[02:06:58.780 --> 02:07:07.340]   At this point, I have said many times if there is any reason to believe that Hunter Biden,
[02:07:07.340 --> 02:07:15.540]   Joe Biden, Naomi Biden, Jill Biden, Hillary Obama, Doug M. If there's any evidence, any
[02:07:15.540 --> 02:07:21.240]   of them committed a crime, they should be investigated, they should be charged and they
[02:07:21.240 --> 02:07:22.860]   should be tried.
[02:07:22.860 --> 02:07:24.260]   Period.
[02:07:24.260 --> 02:07:31.280]   The Hunter Biden laptop thing has been floating around for so long and we still have zero
[02:07:31.280 --> 02:07:40.020]   actual, uh, pieces of evidence of any crime, particularly involving Joe Biden.
[02:07:40.020 --> 02:07:45.540]   There's the claim from some that references to the big guy are about Joe Biden getting
[02:07:45.540 --> 02:07:47.540]   10% for some illicit.
[02:07:47.540 --> 02:07:50.940]   It's been years they've been saying this, that they've not been able to bring forward
[02:07:50.940 --> 02:07:52.780]   any evidence on it.
[02:07:52.780 --> 02:08:02.120]   So my, um, assessment of the Hunter Biden laptop is it seems to mostly be a story about
[02:08:02.120 --> 02:08:08.660]   nude images released without someone's consent, which is illegal in most states and violates
[02:08:08.660 --> 02:08:10.800]   Twitter's own policies.
[02:08:10.800 --> 02:08:12.740]   That's the main story to me.
[02:08:12.740 --> 02:08:17.180]   Beyond that, I don't know how many people have a copy of this hard drive at this point.
[02:08:17.180 --> 02:08:18.180]   Rudy had it.
[02:08:18.180 --> 02:08:22.340]   Tucker, do you remember when Tucker, this, this is, this is unbelievable.
[02:08:22.340 --> 02:08:29.740]   Tucker said that he mailed himself a copy, a USB stick and it got lost in the mail.
[02:08:29.740 --> 02:08:35.700]   You, you have the mother load proving the criminality of Joe and Hunter Biden and I
[02:08:35.700 --> 02:08:38.380]   don't, you just dropped it off with a stamp and it got lost in the mail.
[02:08:38.380 --> 02:08:39.380]   You don't have a backup copy.
[02:08:39.380 --> 02:08:40.380]   You don't.
[02:08:40.380 --> 02:08:43.060]   So I'm ready for the evidence to come forward.
[02:08:43.060 --> 02:08:47.740]   Hunter Biden has nothing to do with Joe Biden's administration, but as a person who, if he
[02:08:47.740 --> 02:08:50.500]   committed a crime, charge him, investigate him, whatever.
[02:08:50.500 --> 02:08:54.180]   But it's, it's getting, it's almost getting satirical the degree to which they're talking
[02:08:54.180 --> 02:08:55.420]   about the Hunter Biden laptop.
[02:08:55.420 --> 02:09:00.220]   Uh, what do you think about the, the social media aspect of this, that that story got
[02:09:00.220 --> 02:09:06.820]   censored and what do you think about censorship in general on social media, that that story
[02:09:06.820 --> 02:09:11.340]   during an important time in the electoral process got censored?
[02:09:11.340 --> 02:09:17.720]   So I, uh, as a matter of principle, I think we have to define what we mean by censorship,
[02:09:17.720 --> 02:09:22.660]   that I'm against censorship short of illegal content, I guess is the way I would put it.
[02:09:22.660 --> 02:09:27.820]   I do respect the company's right to have terms of service and to enforce them as long as
[02:09:27.820 --> 02:09:29.060]   they're not illegal.
[02:09:29.060 --> 02:09:32.140]   If Twitter were to say, we don't publish content from Jewish people.
[02:09:32.140 --> 02:09:35.100]   Okay, now we've got a problem on our hands.
[02:09:35.100 --> 02:09:43.120]   But um, what, what is dubious to me is the claim that had people been able to see Hunter
[02:09:43.120 --> 02:09:48.480]   Biden's genitals, they would have voted for Trump, which I know it's like, David, you're,
[02:09:48.480 --> 02:09:50.200]   you're making light of.
[02:09:50.200 --> 02:09:55.800]   And but at the end of the day, what exactly is the claim that if you had known more about
[02:09:55.800 --> 02:10:02.000]   Hunter Biden, I guess, allegedly hiring prostitutes and having a drug problem and seeing pictures,
[02:10:02.000 --> 02:10:04.040]   you wouldn't have voted for Joe Biden.
[02:10:04.040 --> 02:10:07.880]   I mean, I know me as a voter, I don't feel that way.
[02:10:07.880 --> 02:10:14.760]   I think, uh, it's less about the content of the story and about the actions of, uh, a
[02:10:14.760 --> 02:10:19.680]   social media company to control what you see and what you don't see.
[02:10:19.680 --> 02:10:23.400]   So you can imagine a social media company like Facebook and Twitter making the same
[02:10:23.400 --> 02:10:29.280]   kind of decision about our more impactful story than a few dick pics on a laptop.
[02:10:29.280 --> 02:10:33.420]   Well, I think if that happened, then my view might be different, right?
[02:10:33.420 --> 02:10:38.580]   But I do my, my general view though on the Hunter Biden story is had the articles not
[02:10:38.580 --> 02:10:44.360]   contained those images that were illegal in many states and violated Twitter's policies,
[02:10:44.360 --> 02:10:46.220]   I would say publish it.
[02:10:46.220 --> 02:10:47.220]   Absolutely.
[02:10:47.220 --> 02:10:50.540]   I don't think it would have had an impact, but I would be in favor of it being of the
[02:10:50.540 --> 02:10:52.900]   links being allowed a hundred percent.
[02:10:52.900 --> 02:10:53.900]   Okay.
[02:10:53.900 --> 02:10:54.900]   Uh, you mentioned Tucker.
[02:10:54.900 --> 02:10:57.540]   What do you, what do you think about talking and getting fired from Fox?
[02:10:57.540 --> 02:11:01.620]   Um, you're a media person that works independently.
[02:11:01.620 --> 02:11:02.620]   Yes.
[02:11:02.620 --> 02:11:06.380]   Um, Tucker was a media person who doesn't work independently.
[02:11:06.380 --> 02:11:07.380]   Right.
[02:11:07.380 --> 02:11:08.380]   Uh, yeah.
[02:11:08.380 --> 02:11:12.020]   What do you, what do you think about that particular situation?
[02:11:12.020 --> 02:11:16.220]   Is it representative of some big shift that's happening in mainstream media?
[02:11:16.220 --> 02:11:18.940]   What would the shift be?
[02:11:18.940 --> 02:11:23.700]   Uh, basically mainstream media freaking out because the funding is getting less and less
[02:11:23.700 --> 02:11:29.020]   and less and less, and there's going to give more power to individual commentators.
[02:11:29.020 --> 02:11:32.100]   Basically Tucker Carlson just starting a podcast.
[02:11:32.100 --> 02:11:34.540]   So YouTube channel, I think that's what he should do.
[02:11:34.540 --> 02:11:39.140]   I think that's the most profitable path rather than maybe going to work for Newsmax or whatever
[02:11:39.140 --> 02:11:40.140]   the case may be.
[02:11:40.140 --> 02:11:46.060]   But the firing fundamentally was not a politically oriented firing that suggests Fox news is
[02:11:46.060 --> 02:11:48.220]   changing its tune politically in any way.
[02:11:48.220 --> 02:11:50.100]   There's no evidence of that whatsoever.
[02:11:50.100 --> 02:11:54.060]   Um, Tucker Carlson basically became a legal problem for Fox news.
[02:11:54.060 --> 02:11:56.000]   There's really four points to it.
[02:11:56.000 --> 02:12:03.320]   One is the $787 and a half million settlement with dominion partially was because of the,
[02:12:03.320 --> 02:12:06.960]   um, claims that went out on Tucker Carlson's program.
[02:12:06.960 --> 02:12:15.000]   So to some degree Tucker's program was a prominent, uh, node of the problematic claims that became
[02:12:15.000 --> 02:12:16.160]   the subject of the lawsuit.
[02:12:16.160 --> 02:12:17.160]   That's number one.
[02:12:17.160 --> 02:12:22.160]   Number two, smartmatic, which is another voting machine company, still has a similarly sized
[02:12:22.160 --> 02:12:25.640]   lawsuit against Fox news based on the exact same sorts of claims.
[02:12:25.640 --> 02:12:27.220]   It may cost Fox news again.
[02:12:27.220 --> 02:12:30.540]   So this is now two problems that Tucker's a big contributor to.
[02:12:30.540 --> 02:12:37.300]   Number three, former Tucker staffer has brought a lawsuit and I don't remember the exact claims,
[02:12:37.300 --> 02:12:41.080]   but I know that there are claims of different types of discrimination.
[02:12:41.080 --> 02:12:46.160]   It seems like it has legs and that may be a third payout related to Tucker Carlson.
[02:12:46.160 --> 02:12:50.400]   And based on the 60 minutes piece from a few weeks ago, Ray Epps saying Tucker ruined his
[02:12:50.400 --> 02:12:54.480]   life by fomenting conspiracies about him around January 6th.
[02:12:54.480 --> 02:12:56.400]   That's ripe for another lawsuit.
[02:12:56.400 --> 02:13:04.320]   So to me, Tucker's firing was a risk mitigation strategy, uh, of many that will be employed
[02:13:04.320 --> 02:13:06.160]   as, as these lawsuits come forward.
[02:13:06.160 --> 02:13:11.000]   There's no evidence that it's because Fox didn't like, and what we mean by that, who
[02:13:11.000 --> 02:13:12.000]   are we talking about?
[02:13:12.000 --> 02:13:15.480]   Rupert Murdoch doesn't like, or the PR, I don't know, but I don't have any reason to
[02:13:15.480 --> 02:13:19.320]   believe it's because Tucker's ideas were no longer welcome on Fox.
[02:13:19.320 --> 02:13:20.320]   Certainly the audience liked them.
[02:13:20.320 --> 02:13:21.320]   So interesting.
[02:13:21.320 --> 02:13:23.720]   It's not about, it's not even about the ratings.
[02:13:23.720 --> 02:13:27.240]   It's about just the legal costs.
[02:13:27.240 --> 02:13:28.240]   Fox is interesting.
[02:13:28.240 --> 02:13:33.080]   The ratings question is interesting because Fox, unlike most other or every other cable
[02:13:33.080 --> 02:13:38.480]   news channel, um, they negotiate a fee from every cable subscriber.
[02:13:38.480 --> 02:13:41.760]   If you have Fox news as a channel, even if you don't watch it, Fox gets a little bit
[02:13:41.760 --> 02:13:42.760]   of money.
[02:13:42.760 --> 02:13:47.800]   They are dramatically less dependent on ad revenue than CNN and MSNBC.
[02:13:47.800 --> 02:13:52.400]   So the ratings question is an interesting one, but Fox's position is different on that.
[02:13:52.400 --> 02:14:00.840]   Another question from Reddit, "Both sides are the same" is a meme notion that has spread
[02:14:00.840 --> 02:14:04.320]   far and wide in American political discourse on the internet.
[02:14:04.320 --> 02:14:08.600]   To what extent do you agree or disagree with this notion and why do you think it is so
[02:14:08.600 --> 02:14:09.600]   popular?
[02:14:09.600 --> 02:14:15.580]   Now, this Reddit comment also says that podcasts like Russell Brand and Joe Rogan or the legendary
[02:14:15.580 --> 02:14:21.960]   comic George Carlin are examples of big proponents of this notion, all of which I kind of disagree
[02:14:21.960 --> 02:14:22.960]   with.
[02:14:22.960 --> 02:14:31.000]   Uh, Russell Brand, Joe Rogan, and George Carlin claim that both sides are the same and use
[02:14:31.000 --> 02:14:35.000]   that, you know, all politicians are crooked and suck and this kind of thing.
[02:14:35.000 --> 02:14:38.760]   I don't know if they're, I don't know if that's true.
[02:14:38.760 --> 02:14:40.000]   Maybe George Carlin.
[02:14:40.000 --> 02:14:42.600]   Anyway, leave that aside.
[02:14:42.600 --> 02:14:47.840]   To what degree do you think, uh, do you agree with this notion that both sides are the same?
[02:14:47.840 --> 02:14:53.360]   Left and right, the crooked corrupt politicians, they do what politicians do.
[02:14:53.360 --> 02:14:54.480]   I don't agree that it's the same.
[02:14:54.480 --> 02:15:01.280]   I think there are different factions that like to say that, um, for different reasons.
[02:15:01.280 --> 02:15:06.520]   There are some individuals who want to present themselves as kind of being above the fray
[02:15:06.520 --> 02:15:07.840]   of partisan politics.
[02:15:07.840 --> 02:15:10.680]   And so it's, I call it enlightened centrism.
[02:15:10.680 --> 02:15:13.800]   Um, do you mean that positively or negatively?
[02:15:13.800 --> 02:15:14.800]   No, I mean it negatively.
[02:15:14.800 --> 02:15:15.800]   Yeah.
[02:15:15.800 --> 02:15:16.800]   It's, it's a bit of a pejorative.
[02:15:16.800 --> 02:15:22.840]   I mean, I think that I am not going to fall for being a Democrat or a Republican.
[02:15:22.840 --> 02:15:29.120]   I can see that these are just two sides of the same coin equally bad lying to every,
[02:15:29.120 --> 02:15:30.720]   okay, so that's one.
[02:15:30.720 --> 02:15:35.200]   It's sort of like it's popular at dinner parties in some circles to go, but with all these
[02:15:35.200 --> 02:15:37.120]   politicians, you know, left and right.
[02:15:37.120 --> 02:15:38.800]   So that's one side of it.
[02:15:38.800 --> 02:15:45.960]   The other side of it is that it's often used when, when your side has really stepped in
[02:15:45.960 --> 02:15:47.280]   it.
[02:15:47.280 --> 02:15:55.200]   It's a popular way to acknowledge that your side has done something wrong, but while framing
[02:15:55.200 --> 02:15:59.600]   it as it's not uniquely wrong and it's not worse than what anybody else does.
[02:15:59.600 --> 02:16:05.960]   And I find that it's one of the lamest and most kind of cringe inducing things to hear
[02:16:05.960 --> 02:16:09.440]   because of the what comes next.
[02:16:09.440 --> 02:16:15.520]   And usually what comes next is not a good, accurate criticism of something that took
[02:16:15.520 --> 02:16:19.160]   place and a discussion of how to solve a real problem that we have.
[02:16:19.160 --> 02:16:24.720]   I find that a conversation Stifler, it also is used to kind of suppress voter turnout.
[02:16:24.720 --> 02:16:25.720]   Not actively.
[02:16:25.720 --> 02:16:28.440]   It's not that the people who say that go around saying don't vote.
[02:16:28.440 --> 02:16:32.240]   But the idea of course is the more people that believe that it doesn't really make a
[02:16:32.240 --> 02:16:34.200]   difference who you vote for.
[02:16:34.200 --> 02:16:35.800]   It's going to suppress voter turnout.
[02:16:35.800 --> 02:16:38.920]   And I want voter turnout to be as high as possible, not as low as possible.
[02:16:38.920 --> 02:16:41.480]   So I also dislike it for that reason.
[02:16:41.480 --> 02:16:46.200]   So is it possible to say that one side is worse than the other in, in modern current
[02:16:46.200 --> 02:16:47.760]   political climate?
[02:16:47.760 --> 02:16:50.860]   Listen, I'm a person on the left.
[02:16:50.860 --> 02:16:56.200]   I'm not pretending to come here as, as, and not knowing that my view is biased because
[02:16:56.200 --> 02:16:57.200]   I'm a person of the left.
[02:16:57.200 --> 02:16:59.600]   If you ask Ben Shapiro, he'll tell you something different.
[02:16:59.600 --> 02:17:07.000]   I think in 2023 sum total the influence of the American right wing, if the American right
[02:17:07.000 --> 02:17:12.840]   wing were to get everything it wants, it would be a horrifying reality.
[02:17:12.840 --> 02:17:18.200]   If the left were to get everything it wants, we'd have to figure out a few things, including
[02:17:18.200 --> 02:17:24.360]   exactly how we pay for certain programs, but they're mostly noble goals.
[02:17:24.360 --> 02:17:30.160]   And I believe that they are more supportive of an individual self determining what they
[02:17:30.160 --> 02:17:34.800]   want to do in life and how they want to live and is more in line with the idea of freedom
[02:17:34.800 --> 02:17:38.040]   and liberty than what the right is currently proposing.
[02:17:38.040 --> 02:17:39.780]   That's my view.
[02:17:39.780 --> 02:17:42.440]   And of course people will disagree with me all day.
[02:17:42.440 --> 02:17:45.120]   Now we get to freedom and liberty the way that the right wants to do it.
[02:17:45.120 --> 02:17:47.080]   Okay, well we can have that conversation.
[02:17:47.080 --> 02:17:54.920]   So I think you've implied in your answer, it was kind of focused on policy.
[02:17:54.920 --> 02:17:57.280]   It felt like it was focused on policy.
[02:17:57.280 --> 02:17:59.200]   There's other stuff that people worry about.
[02:17:59.200 --> 02:18:00.200]   Sure.
[02:18:00.200 --> 02:18:06.040]   Particularly with the left, what may be termed the woke mind virus.
[02:18:06.040 --> 02:18:09.680]   Where have I heard, who's using that term a lot now?
[02:18:09.680 --> 02:18:10.680]   I'm trying to think.
[02:18:10.680 --> 02:18:11.680]   I'm not sure.
[02:18:11.680 --> 02:18:12.680]   Not sure.
[02:18:12.680 --> 02:18:15.120]   Not sure where it comes up.
[02:18:15.120 --> 02:18:17.360]   But the cultural aspect of this.
[02:18:17.360 --> 02:18:18.360]   Sure.
[02:18:18.360 --> 02:18:25.080]   That if you give a lot of power to people on the left, as you gave as an example, there
[02:18:25.080 --> 02:18:33.160]   would be a lot of censorship and suppression of speech and a kind of dividing up of a society
[02:18:33.160 --> 02:18:40.120]   of who is allowed to, basically a reallocation of resources not based on merit, but based
[02:18:40.120 --> 02:18:44.200]   on some kind of high ethical notions of what is right.
[02:18:44.200 --> 02:18:50.520]   And only a very small percent of the population gets to decide what is fair, what is right.
[02:18:50.520 --> 02:18:52.500]   Which is, you know.
[02:18:52.500 --> 02:18:55.720]   We already have a small portion of the population deciding fair.
[02:18:55.720 --> 02:18:56.720]   Okay.
[02:18:56.720 --> 02:18:57.720]   Yes.
[02:18:57.720 --> 02:19:02.600]   But I don't know how many different ways I can say kind of a negative characterization
[02:19:02.600 --> 02:19:06.520]   of folks on the left when we're now comparing it.
[02:19:06.520 --> 02:19:07.920]   Just as to play devil's advocate.
[02:19:07.920 --> 02:19:08.920]   Sure.
[02:19:08.920 --> 02:19:10.520]   And that's something that you worry about.
[02:19:10.520 --> 02:19:14.320]   So setting policies aside, wokeism.
[02:19:14.320 --> 02:19:15.320]   Yes.
[02:19:15.320 --> 02:19:16.600]   How big of a problem is it?
[02:19:16.600 --> 02:19:18.040]   This is a great conversation.
[02:19:18.040 --> 02:19:20.400]   So let's, two sides of it.
[02:19:20.400 --> 02:19:21.400]   Okay.
[02:19:21.400 --> 02:19:27.020]   We have new polling that seems to suggest so-called wokeism is kind of more popular
[02:19:27.020 --> 02:19:29.680]   in the United States than anti-wokeism.
[02:19:29.680 --> 02:19:31.520]   And I'll tell you what I mean by that.
[02:19:31.520 --> 02:19:32.720]   This is the less interesting part.
[02:19:32.720 --> 02:19:35.840]   We'll go to the more interesting part second.
[02:19:35.840 --> 02:19:44.160]   Sometimes what people mean by wokeism is an overreaction to a perceived injustice that
[02:19:44.160 --> 02:19:48.600]   goes beyond what would be fair and equitable.
[02:19:48.600 --> 02:19:52.240]   There was this really interesting poll and it asked questions like, for example, do you
[02:19:52.240 --> 02:19:59.080]   believe society has gone too far, not far enough, or just about the right amount in
[02:19:59.080 --> 02:20:03.720]   dealing with issues affecting the trans community?
[02:20:03.720 --> 02:20:09.960]   The woke position, which is society hasn't gone far enough, was far more popular than
[02:20:09.960 --> 02:20:11.440]   we've gone too far.
[02:20:11.440 --> 02:20:14.440]   Now the right wing media narrative is we've gone way too far.
[02:20:14.440 --> 02:20:16.620]   This is out of control.
[02:20:16.620 --> 02:20:18.280]   And there are lots of other similar answers.
[02:20:18.280 --> 02:20:19.360]   It's not a huge margin.
[02:20:19.360 --> 02:20:23.140]   A lot of these are like 58 to 42, 60 to 40.
[02:20:23.140 --> 02:20:28.880]   It's not like 90 to 10, but by a small margin, the so-called woke perspective of we actually
[02:20:28.880 --> 02:20:34.440]   haven't yet done enough to fix some of these issues is a little bit more popular.
[02:20:34.440 --> 02:20:39.800]   So if we went back to DeSantis, this is part of why I think DeSantis is anti-woke agenda
[02:20:39.800 --> 02:20:41.800]   may just be a political misstep.
[02:20:41.800 --> 02:20:43.120]   That's really interesting result.
[02:20:43.120 --> 02:20:49.320]   I wonder how the questions are framed, but it's still interesting nevertheless, no matter
[02:20:49.320 --> 02:20:58.460]   what to hear that people are majority of people in America are woke and not in the negative
[02:20:58.460 --> 02:20:59.560]   sense of the word.
[02:20:59.560 --> 02:21:01.360]   The poll didn't use the term woke, right?
[02:21:01.360 --> 02:21:02.360]   Right.
[02:21:02.360 --> 02:21:03.360]   So this is a critical thing.
[02:21:03.360 --> 02:21:04.360]   Let's use the term woke positively.
[02:21:04.360 --> 02:21:08.440]   The term has kind of been perverted.
[02:21:08.440 --> 02:21:12.200]   Four years ago when the term was started to be used, I would have said, oh yeah, woke
[02:21:12.200 --> 02:21:19.740]   just means like I have become aware of problems that are bigger than any one person can fix
[02:21:19.740 --> 02:21:23.160]   for themselves that relate to the system.
[02:21:23.160 --> 02:21:26.960]   I think that's what, and we might disagree on which problems fall into that category,
[02:21:26.960 --> 02:21:28.320]   but like it was kind of benign.
[02:21:28.320 --> 02:21:36.160]   I think now it just means like outrageously left wing, maybe even with socialist or Marxist
[02:21:36.160 --> 02:21:37.160]   undertones.
[02:21:37.160 --> 02:21:38.560]   It's become a pejorative at this point.
[02:21:38.560 --> 02:21:40.760]   But also like bullies, like people-
[02:21:40.760 --> 02:21:41.760]   Bullies, sure.
[02:21:41.760 --> 02:21:42.760]   Censors.
[02:21:42.760 --> 02:21:50.880]   Yeah, but people that go around calling others racist, sometimes, oftentimes without any
[02:21:50.880 --> 02:21:53.200]   proof of that or justification.
[02:21:53.200 --> 02:21:54.200]   Fair.
[02:21:54.200 --> 02:22:00.200]   And that's a few folks on Twitter you're saying, like the polling is starting to show that
[02:22:00.200 --> 02:22:07.400]   like, no, they're still, most of the Americans still care about these issues and want to
[02:22:07.400 --> 02:22:08.880]   improve and want to make progress.
[02:22:08.880 --> 02:22:12.120]   I think that's the case and they want to do it in a genuine way that doesn't suppress
[02:22:12.120 --> 02:22:13.280]   or oppress anybody.
[02:22:13.280 --> 02:22:18.120]   But now let me get to like, to what degree do I think that actual, when it goes too far
[02:22:18.120 --> 02:22:19.480]   is a problem?
[02:22:19.480 --> 02:22:21.480]   It absolutely exists.
[02:22:21.480 --> 02:22:24.880]   We can find instances of where this exists on the left.
[02:22:24.880 --> 02:22:32.820]   I've been told many times that as a Jewish Argentinian immigrant to the United States,
[02:22:32.820 --> 02:22:42.200]   I actually don't qualify as oppressed enough because Jews are privileged now in the U.S.
[02:22:42.200 --> 02:22:45.640]   and my family had just enough money to leave Argentina.
[02:22:45.640 --> 02:22:50.000]   So there's this kind of like oppression Olympics thing where I've been told you don't get to
[02:22:50.000 --> 02:22:56.440]   comment for example, like a topic in the Latino community now is, are you familiar with Latin
[02:22:56.440 --> 02:22:57.440]   X?
[02:22:57.440 --> 02:22:58.440]   Okay.
[02:22:58.440 --> 02:23:05.200]   In Spanish there's an analogous movement where words by their nature sort of like have a
[02:23:05.200 --> 02:23:06.400]   gender.
[02:23:06.400 --> 02:23:14.200]   So like the word for friend is amigo, but if it's a woman, you would say Amiga.
[02:23:14.200 --> 02:23:16.940]   So right from there you can tell the gender that we're talking about.
[02:23:16.940 --> 02:23:23.080]   And if it's a mixed group, you say amigos, it's the male with an S, but it could include
[02:23:23.080 --> 02:23:24.080]   both.
[02:23:24.080 --> 02:23:27.280]   There's a movement now which wants to do away with that and put the letter Ian.
[02:23:27.280 --> 02:23:28.280]   It's a new word.
[02:23:28.280 --> 02:23:29.280]   Okay.
[02:23:29.280 --> 02:23:30.840]   It's a gender neutral word.
[02:23:30.840 --> 02:23:33.440]   Amigas, totally new.
[02:23:33.440 --> 02:23:35.000]   I don't like that.
[02:23:35.000 --> 02:23:37.720]   And I don't know anyone, no one in my family uses it.
[02:23:37.720 --> 02:23:42.600]   And I think it's kind of like a strange imposition from someone kind of with, with a solution
[02:23:42.600 --> 02:23:44.160]   in search of a problem.
[02:23:44.160 --> 02:23:48.380]   I've been told you moved to the U S long ago and like your English is good and like you
[02:23:48.380 --> 02:23:51.900]   look wide and said like you don't get to weigh in on that.
[02:23:51.900 --> 02:23:56.100]   That I think is an example, if I understand correctly, of the type of thing you're talking
[02:23:56.100 --> 02:23:57.100]   about.
[02:23:57.100 --> 02:23:58.100]   I'm, I'm kind of being bullied.
[02:23:58.100 --> 02:23:59.100]   I'm fine.
[02:23:59.100 --> 02:24:02.360]   I'm surviving fine, but I'm being bullied over it and disqualified and saying you don't get
[02:24:02.360 --> 02:24:03.820]   to speak on this issue.
[02:24:03.820 --> 02:24:08.560]   All of those example, all of that stuff I am completely against.
[02:24:08.560 --> 02:24:12.460]   And I tell people on the left, we're actually hurting our own movement with this stuff.
[02:24:12.460 --> 02:24:16.940]   I just don't think it's as big as some others believe.
[02:24:16.940 --> 02:24:21.180]   You don't think it's an existential threat to our civilization in the West to what?
[02:24:21.180 --> 02:24:22.180]   No, I don't.
[02:24:22.180 --> 02:24:24.620]   And, and I mean, look, we've got a Biden administration.
[02:24:24.620 --> 02:24:27.140]   I see Biden as center left.
[02:24:27.140 --> 02:24:32.700]   Those who see Biden as extreme far left, this stuff has played almost no role whatsoever
[02:24:32.700 --> 02:24:35.100]   in the first two plus years of his administration.
[02:24:35.100 --> 02:24:38.360]   What does people that see him far left as far left?
[02:24:38.360 --> 02:24:44.360]   There's people on the right who, I mean, Trump says Biden's a Marxist socialist communist.
[02:24:44.360 --> 02:24:48.440]   I haven't heard that because I don't think that would stick very much.
[02:24:48.440 --> 02:24:54.240]   I think that every rally, which I love how jealous that you don't watch these things.
[02:24:54.240 --> 02:24:57.160]   It's like how deeply researched you are in Trump.
[02:24:57.160 --> 02:25:00.080]   I can only imagine how good your Trump impression is at this point.
[02:25:00.080 --> 02:25:01.880]   It's not very, sadly, it's not, it's not.
[02:25:01.880 --> 02:25:02.880]   All right.
[02:25:02.880 --> 02:25:06.600]   No, but, and I'll say one other thing on that, you know, take trans because trans, just to
[02:25:06.600 --> 02:25:09.460]   talk about it a little bit, we haven't dealt with it much.
[02:25:09.460 --> 02:25:13.720]   The trans issue has become huge, I believe because the right is obsessed with it.
[02:25:13.720 --> 02:25:16.520]   The right is very much not concerned with gay men anymore.
[02:25:16.520 --> 02:25:20.600]   It used to be that gay men is like, Oh, we have to stop gay men from adopting and unnatural
[02:25:20.600 --> 02:25:21.900]   and pedophiles.
[02:25:21.900 --> 02:25:25.300]   Now it's trans it's drag shows, et cetera.
[02:25:25.300 --> 02:25:34.200]   I do think that there is a fair question to say, how do we deal with trans women in a
[02:25:34.200 --> 02:25:37.660]   very small short list of sports?
[02:25:37.660 --> 02:25:38.660]   That's real.
[02:25:38.660 --> 02:25:39.660]   Okay.
[02:25:39.660 --> 02:25:43.760]   My view though, is I go, okay, we have all issues.
[02:25:43.760 --> 02:25:47.760]   We have issues related to gender and sexual orientation.
[02:25:47.760 --> 02:25:53.440]   We have issues related to trans within that we have specifically sports.
[02:25:53.440 --> 02:25:57.120]   You can eliminate from that a trans men.
[02:25:57.120 --> 02:25:58.440]   Nobody's worried, right?
[02:25:58.440 --> 02:26:02.040]   About women, biological women who are trans.
[02:26:02.040 --> 02:26:06.760]   And then when you say it's only in certain sports that it matters, Hey, I'm right there.
[02:26:06.760 --> 02:26:08.160]   I think it's a complicated question.
[02:26:08.160 --> 02:26:09.800]   I don't know how we deal with it.
[02:26:09.800 --> 02:26:14.780]   I would ask leagues that have experienced with this already and whatever.
[02:26:14.780 --> 02:26:20.940]   The problem I have is pretending that the, the, uh, Vanguard of left-wing politics right
[02:26:20.940 --> 02:26:26.180]   now is trying to force trans women into sports.
[02:26:26.180 --> 02:26:33.100]   It's like, it's just not the big issue that the right is reacting as if it were, but perhaps
[02:26:33.100 --> 02:26:40.060]   because of the right, it's forcing the left, uh, to, to continue discussing it.
[02:26:40.060 --> 02:26:45.240]   I mean, I, I feel like it, uh, even in the institutions, even at universities, it feels
[02:26:45.240 --> 02:26:50.540]   like these ideas of diversity, inclusion, and equity are taking some of the air out
[02:26:50.540 --> 02:26:59.020]   of the room of, um, what a university should also care about, which is, uh, merit.
[02:26:59.020 --> 02:27:05.700]   And it feels like reprioritization is going a little too far the other way.
[02:27:05.700 --> 02:27:13.220]   Meaning, uh, prioritizing this kind of amorphous concept of diversity is moving away, is giving
[02:27:13.220 --> 02:27:15.860]   power to people that don't care about merit.
[02:27:15.860 --> 02:27:26.300]   And I just want to bully people with a big stick that says racism or sexism or, um, anti-diversity.
[02:27:26.300 --> 02:27:34.900]   And it, it, it kind of suffocates the people that, uh, care about merit, about meritocracy,
[02:27:34.900 --> 02:27:38.420]   about inspiring people from all kinds of backgrounds to succeed.
[02:27:38.420 --> 02:27:40.460]   And it's just, you kind of observe that.
[02:27:40.460 --> 02:27:43.020]   I'm sure that happens in all kinds of institutions.
[02:27:43.020 --> 02:27:47.780]   And the concern, I think the people that are concerned about wokeism are concerned about
[02:27:47.780 --> 02:27:52.380]   at scale, what impact does that have on a society when there's so much conversation
[02:27:52.380 --> 02:27:58.620]   about racism and a decre, a pressure not to talk about merit?
[02:27:58.620 --> 02:28:01.740]   Like who's the actual good person in the room?
[02:28:01.740 --> 02:28:03.100]   The best person in the room.
[02:28:03.100 --> 02:28:05.180]   Generically, that's a concern to me.
[02:28:05.180 --> 02:28:11.380]   The degree to which it's happening at different institutions, I think is worthy of exploration.
[02:28:11.380 --> 02:28:16.580]   I know people who work in academia that are getting out of academia because they don't
[02:28:16.580 --> 02:28:19.660]   like the environment on their campuses for exactly the reason you're saying.
[02:28:19.660 --> 02:28:20.780]   So it exists.
[02:28:20.780 --> 02:28:21.820]   There's no question about it.
[02:28:21.820 --> 02:28:27.580]   I also think that the idea of a perfect meritocracy is, is maybe not necessarily the goal in the
[02:28:27.580 --> 02:28:34.340]   sense that, um, when you talk about perfect meritocracy, someone wrote a book about this
[02:28:34.340 --> 02:28:37.900]   who I interviewed about a year and a half ago and whose name escapes me.
[02:28:37.900 --> 02:28:39.700]   There are problems with a perfect meritocracy.
[02:28:39.700 --> 02:28:49.540]   I think what we want to do is generate roughly equal, um, uh, opportunity for people understanding
[02:28:49.540 --> 02:28:55.580]   that there is going to be an outcome on a gradient or a bell curve, allowing people,
[02:28:55.580 --> 02:28:59.940]   generally speaking, to determine the path that they want to take and giving them if
[02:28:59.940 --> 02:29:05.260]   it's possible, the ability to, uh, pursue that without suppressing, limiting.
[02:29:05.260 --> 02:29:12.020]   I mean, this is like relatively uncontroversial stuff among, I would argue 95% of the left
[02:29:12.020 --> 02:29:15.060]   with the caveats of what you're talking about, which I agree exist.
[02:29:15.060 --> 02:29:17.340]   It would be nice to know the actual data.
[02:29:17.340 --> 02:29:19.620]   Sometimes people blow stuff out of proportion.
[02:29:19.620 --> 02:29:24.780]   What is, it's hard, it's hard to measure how much self-censorship happens at university
[02:29:24.780 --> 02:29:25.780]   campuses.
[02:29:25.780 --> 02:29:26.780]   It's hard.
[02:29:26.780 --> 02:29:27.780]   That's true.
[02:29:27.780 --> 02:29:31.940]   I think also it's sort of like the, the pit bull bite stories thing where when a pit bull
[02:29:31.940 --> 02:29:38.100]   bites a person, it's more likely to be reported on because it fits a certain narrative.
[02:29:38.100 --> 02:29:44.040]   And there are right wing publications that are very interested in making this seem as
[02:29:44.040 --> 02:29:45.700]   if it is an epidemic.
[02:29:45.700 --> 02:29:49.540]   I'm the first to say it is happening to a degree.
[02:29:49.540 --> 02:29:51.300]   I don't know the degree that it's happening to.
[02:29:51.300 --> 02:29:57.340]   I know a lot of people in academia, only a couple of them say that it's an issue.
[02:29:57.340 --> 02:29:59.140]   Would they say it though if they believed it?
[02:29:59.140 --> 02:30:02.660]   I think they would say it to me, these are just personal contacts.
[02:30:02.660 --> 02:30:03.940]   It's not like I'm going to go blabbing.
[02:30:03.940 --> 02:30:10.940]   To push back, I kind of agree with you, but at the same time, most, I mean, I'm deeply
[02:30:10.940 --> 02:30:14.580]   connected in academia, I have a huge number of colleagues.
[02:30:14.580 --> 02:30:19.260]   Most people self-censor by not thinking about it at all.
[02:30:19.260 --> 02:30:20.780]   They're like, screw it.
[02:30:20.780 --> 02:30:21.780]   That's deeper.
[02:30:21.780 --> 02:30:22.780]   Whatever.
[02:30:22.780 --> 02:30:25.900]   I'm just going to focus on the thing I love doing, which is the work.
[02:30:25.900 --> 02:30:32.740]   And they don't think about, they basically remove themselves from politics and social
[02:30:32.740 --> 02:30:37.780]   issues and they just kind of say, I'm going to do my engineering, I'm going to do my mathematics.
[02:30:37.780 --> 02:30:41.580]   The problem with that is it's kind of, you can't go anywhere further to figure it out.
[02:30:41.580 --> 02:30:45.700]   It's sort of like, there's this funny clip where Jordan Peterson says, even atheists
[02:30:45.700 --> 02:30:47.860]   are actually religious, they just don't know it.
[02:30:47.860 --> 02:30:49.420]   And it's like, it's hard to test that.
[02:30:49.420 --> 02:30:50.420]   I don't know.
[02:30:50.420 --> 02:30:51.420]   Okay.
[02:30:51.420 --> 02:30:52.940]   I mean, I don't, but it's a fair point.
[02:30:52.940 --> 02:30:57.300]   I mean, there may be some people, if it has become so toxic for some people, they may
[02:30:57.300 --> 02:31:01.620]   have repressed it way down into their subconscious, but I don't know how we would know that.
[02:31:01.620 --> 02:31:08.660]   But you, you, you know, symptoms of it because when certain people speak up kind of lightly
[02:31:08.660 --> 02:31:14.260]   and then a 19 year old or a 20 year old responds and is outraged.
[02:31:14.260 --> 02:31:20.140]   The fact that the administration listens to that 19 and 20 year old and then reprimands
[02:31:20.140 --> 02:31:22.660]   whoever spoke up a little bit.
[02:31:22.660 --> 02:31:25.460]   That's a really dangerous sign to me.
[02:31:25.460 --> 02:31:26.940]   And I don't really care about these.
[02:31:26.940 --> 02:31:28.340]   So I'm more with you.
[02:31:28.340 --> 02:31:31.300]   I don't think it's a big issue, but then I notice it.
[02:31:31.300 --> 02:31:36.140]   I wonder, wait a minute, would this kind of environment allow a young Noam Chomsky to
[02:31:36.140 --> 02:31:37.420]   be around?
[02:31:37.420 --> 02:31:43.860]   Would this environment allow like, I don't know, like what tenure was designed for, which
[02:31:43.860 --> 02:31:50.140]   is to have controversial thinkers and not kind of weird controversial things, but really
[02:31:50.140 --> 02:31:53.500]   people that challenge things that should be challenged.
[02:31:53.500 --> 02:31:54.500]   Yeah.
[02:31:54.500 --> 02:31:56.420]   I sympathize with that significantly.
[02:31:56.420 --> 02:32:02.180]   I always try to look at specific examples and sometimes I'll look at people, I'll ask
[02:32:02.180 --> 02:32:04.220]   for them and people will send me five.
[02:32:04.220 --> 02:32:07.860]   And one of them is a legit bonafide example of what we're talking about.
[02:32:07.860 --> 02:32:12.740]   And four are kind of like, eh, there was a complaint and it was investigated, but the
[02:32:12.740 --> 02:32:14.900]   teacher's tenure was never in jeopardy.
[02:32:14.900 --> 02:32:19.260]   And I don't know that I chalk this up to a big woke event.
[02:32:19.260 --> 02:32:23.300]   What do you think the kind of apparatus of the four year degree in college is going to
[02:32:23.300 --> 02:32:25.220]   look like in 20 years?
[02:32:25.220 --> 02:32:31.460]   Oh, that's, I mean, we're like day by day that seems to be changing with GPT.
[02:32:31.460 --> 02:32:34.340]   I don't know if you've gotten a chance to interact with chat GPT.
[02:32:34.340 --> 02:32:35.340]   Absolutely.
[02:32:35.340 --> 02:32:37.380]   My entire show now is written by chat GPT.
[02:32:37.380 --> 02:32:43.460]   I mean, there's a, that's partially a joke.
[02:32:43.460 --> 02:32:47.240]   It is only because it stopped looking at the internet in 2021.
[02:32:47.240 --> 02:32:49.460]   If it was current, I could completely just tune it out.
[02:32:49.460 --> 02:32:51.700]   No, I'm kidding, but it's a fascinating tool.
[02:32:51.700 --> 02:32:55.980]   And it's changing the nature of how we do homework assignments.
[02:32:55.980 --> 02:33:00.700]   It's changing the nature of how we learn, how we look up new information, how we explore
[02:33:00.700 --> 02:33:02.980]   information, how we care about things we're interested in.
[02:33:02.980 --> 02:33:11.740]   I think it, I don't think we'll have value for university degree in 20 years the way
[02:33:11.740 --> 02:33:12.740]   we do now.
[02:33:12.740 --> 02:33:14.380]   I just think it changes everything.
[02:33:14.380 --> 02:33:22.460]   I think language models, Google search has already, and Wikipedia has already transformed,
[02:33:22.460 --> 02:33:28.360]   I would say our civilization, but it's, there was still a value for basic education.
[02:33:28.360 --> 02:33:32.800]   I don't, I think that starts to dissipate with chat GPT.
[02:33:32.800 --> 02:33:35.240]   So I don't, I don't know.
[02:33:35.240 --> 02:33:40.380]   I really, I really don't think there's a university the way we think of a university in 20, 23
[02:33:40.380 --> 02:33:41.380]   years.
[02:33:41.380 --> 02:33:45.620]   And I have a personal interest in it in that my daughter is 10 months old and I'm doing
[02:33:45.620 --> 02:33:46.840]   the five 29 account.
[02:33:46.840 --> 02:33:52.840]   I'm going through the motions as if, but I also recognize, you know, if she went to the
[02:33:52.840 --> 02:33:59.380]   schools I went to just with the rate of tuition increase, you're talking 200 K a year by the
[02:33:59.380 --> 02:34:01.460]   time she's 18.
[02:34:01.460 --> 02:34:03.700]   And what happens with wages relative to that?
[02:34:03.700 --> 02:34:07.280]   This is like separate from the technological thing.
[02:34:07.280 --> 02:34:12.680]   And in my mind I'm thinking, is this going to continue being the right path?
[02:34:12.680 --> 02:34:17.600]   What I would love to see is so many people that I interact with just by virtue of what
[02:34:17.600 --> 02:34:24.200]   I do have no foundation in critical thinking, epistemology, philosophy, media literacy.
[02:34:24.200 --> 02:34:30.960]   And if there were some way to make that the core of some basic education that everybody's
[02:34:30.960 --> 02:34:38.260]   receiving, which goes beyond, you know, chat GPT can do so many things, but I've not yet
[02:34:38.260 --> 02:34:41.400]   seen good examples of how it can teach you to think.
[02:34:41.400 --> 02:34:47.940]   Maybe you have a different view on how chat GPT can teach a user to think, but those skills
[02:34:47.940 --> 02:34:49.520]   seem to be so lacking.
[02:34:49.520 --> 02:34:53.120]   And so many of the people I interact with, if there's any positive change to come from
[02:34:53.120 --> 02:34:58.240]   a changing dynamic with higher education, I wish it would be to go in that direction.
[02:34:58.240 --> 02:35:04.680]   - Well no, chat GPT is actually much better at helping me think than any educator, even
[02:35:04.680 --> 02:35:09.760]   books that I've encountered, because it's very good at presenting the full picture,
[02:35:09.760 --> 02:35:12.520]   even better than a lot of Wikipedia articles.
[02:35:12.520 --> 02:35:16.840]   You know, on questions like, did the virus leak from a lab?
[02:35:16.840 --> 02:35:18.320]   Did COVID leak from a lab?
[02:35:18.320 --> 02:35:21.760]   It just presents to you all the different hypotheses, the amount of evidence available
[02:35:21.760 --> 02:35:22.760]   to it.
[02:35:22.760 --> 02:35:27.560]   It's like a full, calm, objective picture of it.
[02:35:27.560 --> 02:35:29.080]   There's no partisanship.
[02:35:29.080 --> 02:35:31.640]   It's like a really nice list of things that's available.
[02:35:31.640 --> 02:35:38.100]   - But I guess what I mean is, does it tell you how, as a thinking human, you should evaluate
[02:35:38.100 --> 02:35:41.040]   the strength of each of the paragraphs it presents to you?
[02:35:41.040 --> 02:35:42.120]   - You can literally ask the question.
[02:35:42.120 --> 02:35:44.400]   - You can ask it to do, oh fair, okay, yeah.
[02:35:44.400 --> 02:35:50.080]   - And then it's actually a fun, it's fun to ask chat GPT that question, 'cause you'll
[02:35:50.080 --> 02:35:52.080]   get good answers.
[02:35:52.080 --> 02:36:01.040]   And so you'll basically have a kind of Socratic, like a deep, intimate, like great podcast
[02:36:01.040 --> 02:36:07.040]   style conversation with an AI system every single day for as many hours as you want,
[02:36:07.040 --> 02:36:12.200]   especially as it improves, and as the interfaces by which you communicate with the thing improves.
[02:36:12.200 --> 02:36:17.440]   So yeah, I think it will do exactly that, which is teach you how to think, because you
[02:36:17.440 --> 02:36:26.680]   will offload the memory of facts and equations and whatever else school teaches you, you'll
[02:36:26.680 --> 02:36:29.120]   offload that to AI.
[02:36:29.120 --> 02:36:34.680]   And instead you'll be using your human mind, which is what it, for now, is uniquely good
[02:36:34.680 --> 02:36:40.920]   at, which is asking good questions, thinking through the complexities of issues when there's
[02:36:40.920 --> 02:36:43.080]   multiple perspectives on it, all of that.
[02:36:43.080 --> 02:36:44.280]   - Well then I stand corrected.
[02:36:44.680 --> 02:36:46.880]   Then I don't know what college is gonna be in 20 years.
[02:36:46.880 --> 02:36:52.520]   - Well, but you were sort of commenting, I see, to the financial aspects of it, like
[02:36:52.520 --> 02:36:55.800]   why does it even make sense at this point?
[02:36:55.800 --> 02:37:03.320]   I am thinking about the transformative effects of AI, and what, it starts to ask, what is
[02:37:03.320 --> 02:37:04.320]   even education?
[02:37:04.320 --> 02:37:05.320]   - Right.
[02:37:05.320 --> 02:37:08.920]   - What are you supposed, what is the purpose of education?
[02:37:08.920 --> 02:37:13.800]   So one is to give you kind of a background knowledge on a bunch of different topics,
[02:37:13.800 --> 02:37:17.680]   but the other is to discover the thing you're truly passionate about, and the thing you're
[02:37:17.680 --> 02:37:22.520]   really good at, such that you can make money, and you can contribute to society, and have
[02:37:22.520 --> 02:37:24.160]   a fulfilling life.
[02:37:24.160 --> 02:37:30.480]   - Yeah, and also learning to interact with other people, relationships are built, socializing,
[02:37:30.480 --> 02:37:31.960]   and so many other things as well.
[02:37:31.960 --> 02:37:36.640]   - But is that, that is the big value of university.
[02:37:36.640 --> 02:37:38.940]   And maybe it should be called something else.
[02:37:38.940 --> 02:37:41.360]   - Can you get that for less than 200K a year somewhere else?
[02:37:41.360 --> 02:37:42.720]   Yeah, no, it's a fair question.
[02:37:42.720 --> 02:37:44.000]   - It's a kind of social club.
[02:37:44.000 --> 02:37:47.920]   - And you know, one of the things I think about also is people who are well-connected,
[02:37:47.920 --> 02:37:49.760]   I mean, this has always been, this isn't new, right?
[02:37:49.760 --> 02:37:55.360]   But if you're well-connected, and you have a sort of drive towards entrepreneurship,
[02:37:55.360 --> 02:37:59.120]   and doing your own thing, and you're not pursuing a field that is very licensing dependent,
[02:37:59.120 --> 02:38:04.840]   like medicine or law, getting started four years earlier with some internships can be
[02:38:04.840 --> 02:38:06.040]   a privilege in some cases.
[02:38:06.040 --> 02:38:11.400]   But again, that path is available to the people that would likely do well, regardless of whether
[02:38:11.400 --> 02:38:17.840]   they went to college, and so it's a very privileged self-selected group anyway.
[02:38:17.840 --> 02:38:24.080]   - Another question from Reddit, "Ask David to explain why American-style libertarianism
[02:38:24.080 --> 02:38:26.440]   is an unserious philosophy."
[02:38:26.440 --> 02:38:30.080]   - I don't know what they mean by American-style libertarianism.
[02:38:30.080 --> 02:38:38.040]   I've talked before about these kind of utopian libertarians, where, you know, we have, we
[02:38:38.040 --> 02:38:45.360]   don't have police, you just kind of like hire a for-profit company if you want protection,
[02:38:45.360 --> 02:38:50.280]   and if there's a conflict between two of these private security companies, then I don't know,
[02:38:50.280 --> 02:38:51.440]   you figure it out somehow.
[02:38:51.440 --> 02:38:55.000]   - So it's almost like anarchism, so take it to that degree.
[02:38:55.000 --> 02:38:58.920]   - I don't know what the question means by that American-style libertarianism, but in
[02:38:58.920 --> 02:39:05.340]   general my problems with libertarianism, as it is often presented, come from the work
[02:39:05.340 --> 02:39:11.180]   of sociology as well as human psychology, which is the reality that once you get a group
[02:39:11.180 --> 02:39:18.360]   that's bigger than 150 people, you really have to start centralizing some decisions,
[02:39:18.360 --> 02:39:25.480]   unless you're going to subdivide the 150 endlessly into 275s that now no longer have contact,
[02:39:25.480 --> 02:39:28.120]   but then that's not really one society, now it's two.
[02:39:28.120 --> 02:39:33.000]   I've not seen good evidence, and I've read a fair bit about this, that once you get beyond
[02:39:33.000 --> 02:39:40.560]   150, you can keep all decisions decentralized, and once you say some things need to be centralized,
[02:39:40.560 --> 02:39:45.480]   then it's a matter of how you do it, and it's going to be some version of government that
[02:39:45.480 --> 02:39:48.000]   conflicts with aspects of libertarianism.
[02:39:48.000 --> 02:39:50.160]   - Well, it could be companies, right?
[02:39:50.160 --> 02:39:56.920]   It could be more market-driven, which is the idea of anarchism, that you don't give any
[02:39:56.920 --> 02:40:02.120]   centralized entity a monopoly over violence.
[02:40:02.120 --> 02:40:08.120]   You know, and then if you think that the markets are efficient at delivering, especially in
[02:40:08.120 --> 02:40:15.880]   this 21st century and beyond, where a market could have perfect information about people.
[02:40:15.880 --> 02:40:20.640]   So one of the issues is that you can manipulate markets because there's not perfect information,
[02:40:20.640 --> 02:40:29.000]   but now in the digital age, we can be higher bandwidth participants in the market.
[02:40:29.000 --> 02:40:33.960]   So if you're choosing between different security companies, or you're choosing between different
[02:40:33.960 --> 02:40:39.880]   providers of different services, you can do so more efficiently and more effectively in
[02:40:39.880 --> 02:40:41.320]   the digital space.
[02:40:41.320 --> 02:40:48.240]   So you can kind of imagine it, but we haven't successfully done it without governments.
[02:40:48.240 --> 02:40:56.900]   - Yeah, and I think there's a practical, once you get beyond 150, you also start specializing.
[02:40:56.900 --> 02:41:01.080]   It just is a matter of fact, you don't have, everybody isn't growing their own food.
[02:41:01.080 --> 02:41:03.600]   Some people grow the food and other people do other things.
[02:41:03.600 --> 02:41:09.400]   And you come across a lot of the problems that started at the agricultural revolution.
[02:41:09.400 --> 02:41:13.640]   And whether you say that it's a company that's solving it or a government, the problems are
[02:41:13.640 --> 02:41:15.120]   going to be very similar.
[02:41:15.120 --> 02:41:20.520]   And I've not read anything that to my satisfaction explains how you deal with that.
[02:41:20.520 --> 02:41:25.280]   - Well, there's underlying principles of libertarianism, which is putting priority at the freedom of
[02:41:25.280 --> 02:41:27.760]   the individual, right?
[02:41:27.760 --> 02:41:29.760]   And that's a compelling notion.
[02:41:29.760 --> 02:41:35.720]   - Yeah, whenever I do these various political compass things that put you on two axes, on
[02:41:35.720 --> 02:41:42.060]   the authoritarian libertarian axis, I am way down on the libertarian side as a left libertarian.
[02:41:42.060 --> 02:41:51.340]   So my tendencies are always anti-authoritarian and towards that option when it makes sense.
[02:41:51.340 --> 02:41:53.920]   So I sympathize with that a lot.
[02:41:53.920 --> 02:41:58.380]   - Another question from Reddit, "Ask David what issues he disagrees with you on."
[02:41:58.380 --> 02:41:59.880]   Is there something you-- - I have no idea.
[02:41:59.880 --> 02:42:00.880]   - Okay, that's good.
[02:42:00.880 --> 02:42:01.880]   There you go.
[02:42:01.880 --> 02:42:03.320]   There's no issues.
[02:42:03.320 --> 02:42:04.320]   Perfect agreement.
[02:42:04.320 --> 02:42:06.400]   - What's your view on Tesla?
[02:42:06.400 --> 02:42:08.080]   - That's a good opportunity to ask.
[02:42:08.080 --> 02:42:13.720]   What do you think is strengths and weaknesses of Elon Musk?
[02:42:13.720 --> 02:42:14.720]   You mentioned Twitter.
[02:42:14.720 --> 02:42:15.720]   Have you paid your $8?
[02:42:15.720 --> 02:42:16.960]   - I have not paid my $8.
[02:42:16.960 --> 02:42:18.520]   I don't see the point in paying for it.
[02:42:18.520 --> 02:42:20.160]   I have no problem paying for services.
[02:42:20.160 --> 02:42:21.520]   I use a ton of services.
[02:42:21.520 --> 02:42:22.520]   I'll try the free.
[02:42:22.520 --> 02:42:25.000]   I'll go to the paid.
[02:42:25.000 --> 02:42:33.520]   Right now, so the way I used to use the verified feed was I would post a tweet and then the
[02:42:33.520 --> 02:42:37.400]   next day when I review what's going on in my social media, I would look at the replies
[02:42:37.400 --> 02:42:41.640]   to the tweet, which give me a mix of replies from verified and unverified people.
[02:42:41.640 --> 02:42:49.120]   But then I would also look at the verified and see who that are verified public folks
[02:42:49.120 --> 02:42:54.560]   have responded to me or maybe I want to engage with or whatever the case may be.
[02:42:54.560 --> 02:42:57.640]   I don't even understand why I would look at the verified feed anymore.
[02:42:57.640 --> 02:43:01.640]   So I never do because it's random folks who I don't know.
[02:43:01.640 --> 02:43:04.000]   It sort of lost its utility to me.
[02:43:04.000 --> 02:43:06.120]   - Yeah, sorry to interrupt.
[02:43:06.120 --> 02:43:12.720]   But the idea is if everybody who's human pays the $8, it shows to you that it's not bots.
[02:43:12.720 --> 02:43:14.480]   It's at least humans.
[02:43:14.480 --> 02:43:18.080]   - From the reports about the number of people that have bought the blue check mark, I think
[02:43:18.080 --> 02:43:23.440]   we may be a thousand years from enough signups in order to make that sort of like a reality.
[02:43:23.440 --> 02:43:24.440]   I don't know.
[02:43:24.440 --> 02:43:25.440]   - That was the idea.
[02:43:25.440 --> 02:43:26.440]   - That was the idea.
[02:43:26.440 --> 02:43:27.440]   It's an interesting idea.
[02:43:27.440 --> 02:43:33.360]   Honestly, from my experience, obviously I was seeing all sorts of attack comments, some
[02:43:33.360 --> 02:43:37.520]   of which were I'm sure from bots, but I'm ignoring all of those comments anyway.
[02:43:37.520 --> 02:43:39.400]   So it really didn't affect my experience that much.
[02:43:39.400 --> 02:43:43.240]   I mean, here's the thing about Elon and I say this, people sometimes are like, David,
[02:43:43.240 --> 02:43:46.520]   you obviously hate Elon or you obviously love Elon.
[02:43:46.520 --> 02:43:50.040]   I was an investor in Tesla starting in 2015.
[02:43:50.040 --> 02:43:52.320]   I've since sold all my shares.
[02:43:52.320 --> 02:43:53.860]   Great run.
[02:43:53.860 --> 02:43:56.600]   I'm on my second Tesla right now.
[02:43:56.600 --> 02:44:02.440]   I probably won't get a third one because I think that electric vehicle technology is
[02:44:02.440 --> 02:44:07.560]   now maturing such that when my lease is up, I'm going to have many more options with the
[02:44:07.560 --> 02:44:09.920]   range and charging network that's important to me.
[02:44:09.920 --> 02:44:10.920]   But I could be wrong.
[02:44:10.920 --> 02:44:11.920]   Maybe, you know, I don't know.
[02:44:11.920 --> 02:44:16.480]   I have no, the cults of personality around people, they mean nothing to me.
[02:44:16.480 --> 02:44:19.200]   So for me, it's just like people are people.
[02:44:19.200 --> 02:44:20.920]   Nobody has only good ideas.
[02:44:20.920 --> 02:44:21.920]   Fine.
[02:44:21.920 --> 02:44:27.440]   I think that what Elon Musk did accelerating and pushing forward the battery and electric
[02:44:27.440 --> 02:44:30.200]   vehicle technology is unbelievable.
[02:44:30.200 --> 02:44:36.720]   It's it's a it's a one person wrecking ball in the best sense of saying we're not going
[02:44:36.720 --> 02:44:39.560]   to slow play this and do OK.
[02:44:39.560 --> 02:44:45.160]   Now Toyota has a Toyota hasn't actually entered, but now whoever we've got a 90 mile range
[02:44:45.160 --> 02:44:47.280]   car and next year it'll be 110.
[02:44:47.280 --> 02:44:49.520]   And it's just like we're doing this right now.
[02:44:49.520 --> 02:44:53.000]   You can compete or you can opt out and look at what's happened.
[02:44:53.000 --> 02:44:54.080]   Fantastic.
[02:44:54.080 --> 02:44:58.960]   On the Twitter side of things, I don't really get the whole plan.
[02:44:58.960 --> 02:45:03.800]   I don't know if it started maybe as kind of a goof of some kind and it developed into,
[02:45:03.800 --> 02:45:05.600]   I guess I have to buy it.
[02:45:05.600 --> 02:45:09.920]   And I think something about it ended up with there was a clause invoked where I think he
[02:45:09.920 --> 02:45:12.440]   did try to get out of buying it, but then was forced to to some degree.
[02:45:12.440 --> 02:45:13.440]   He was forced.
[02:45:13.440 --> 02:45:19.200]   So the way Twitter used to work was you followed people and when you looked at your feed, you
[02:45:19.200 --> 02:45:24.480]   either saw the posts from the people you were following in reverse chronological order or
[02:45:24.480 --> 02:45:29.800]   posts from the people you followed algorithmically tailored to what you're most likely to want
[02:45:29.800 --> 02:45:31.440]   to see.
[02:45:31.440 --> 02:45:36.600]   And if you didn't follow someone, you generally wouldn't see their posts unless it was like
[02:45:36.600 --> 02:45:40.800]   a sponsored tweet or someone you follow quoted or retweeted them.
[02:45:40.800 --> 02:45:41.800]   Fine.
[02:45:41.800 --> 02:45:46.920]   Now, the For You feed, Tik Tok, I believe first had a so-called For You feed.
[02:45:46.920 --> 02:45:53.060]   The idea is this is stuff you might like based on, I don't know what, either demographic
[02:45:53.060 --> 02:45:56.040]   data about you, your other habits, whatever.
[02:45:56.040 --> 02:45:57.120]   And so it's useless to me.
[02:45:57.120 --> 02:46:00.400]   It's just, it's just basically mostly right wing content that that is not interesting.
[02:46:00.400 --> 02:46:01.400]   Why do you think that is?
[02:46:01.400 --> 02:46:07.160]   I mean, so the, the signals that are used to generate the For You page is looking at
[02:46:07.160 --> 02:46:14.720]   all your likes, all your comments, all your blocks and mutes and all that.
[02:46:14.720 --> 02:46:15.720]   It should know that.
[02:46:15.720 --> 02:46:16.720]   I mean, I don't know what it's looking at.
[02:46:16.720 --> 02:46:17.720]   Okay.
[02:46:17.720 --> 02:46:20.120]   So it's supposed to be very pleasant for you.
[02:46:20.120 --> 02:46:22.760]   I'm sure other people go, wow, this for you thing is awesome.
[02:46:22.760 --> 02:46:27.480]   And I'll get like if you had insert some right wing or sitting here, they would go, Twitter
[02:46:27.480 --> 02:46:32.480]   used to suppress right wing voices and now finally they're getting the fair shake that
[02:46:32.480 --> 02:46:34.480]   they deserve in the For You feed.
[02:46:34.480 --> 02:46:35.480]   Okay.
[02:46:35.480 --> 02:46:41.880]   So I wonder if there's left wing folks setting their feelings of Elon aside that are enjoying
[02:46:41.880 --> 02:46:43.480]   the For You page.
[02:46:43.480 --> 02:46:46.520]   That's a really important question because it's supposed to be people on the left and
[02:46:46.520 --> 02:46:48.520]   people on the right should be enjoying the For You page.
[02:46:48.520 --> 02:46:49.520]   Sure.
[02:46:49.520 --> 02:46:50.520]   Yeah.
[02:46:50.520 --> 02:46:53.960]   I mean, so for me, my thought on Elon is some incredible successes.
[02:46:53.960 --> 02:46:55.480]   I don't know about Twitter.
[02:46:55.480 --> 02:47:00.400]   I do think that I don't believe Elon is a right winger.
[02:47:00.400 --> 02:47:06.660]   And when you see interviews with him, um, certainly at least socially and in many ways
[02:47:06.660 --> 02:47:11.720]   culturally seems very moderate or even somewhat on the left in my experience.
[02:47:11.720 --> 02:47:13.680]   So I don't think it's Elon's a right winger.
[02:47:13.680 --> 02:47:15.800]   I don't, that's not an interesting critique.
[02:47:15.800 --> 02:47:22.800]   It does seem though that throughout the Twitter escapade, he certainly ended up closer to
[02:47:22.800 --> 02:47:27.080]   some voices that may be influencing him in a particular way.
[02:47:27.080 --> 02:47:30.020]   Uh, that's giving some people that impression, you know?
[02:47:30.020 --> 02:47:35.140]   But as far as like the Elon hate or the Elon love, it's just, it's just a person who's
[02:47:35.140 --> 02:47:39.040]   done some interesting things, some of which I like and some of which I could kind of leave,
[02:47:39.040 --> 02:47:40.040]   leave aside.
[02:47:40.040 --> 02:47:52.200]   I have seen, uh, folks drift towards the right more in response to just the viciousness of
[02:47:52.200 --> 02:47:54.160]   attacks from the left.
[02:47:54.160 --> 02:47:55.160]   Like who?
[02:47:55.160 --> 02:47:56.160]   I, well, Elon and...
[02:47:56.160 --> 02:48:00.260]   So, so you do, you do think he's drifted towards the right?
[02:48:00.260 --> 02:48:07.220]   Uh, in a, so I don't think at the core, but I think on the surface, I think, uh, and I
[02:48:07.220 --> 02:48:09.900]   think Joe Rogan has as well on the surface.
[02:48:09.900 --> 02:48:15.420]   Cause I think maybe you can correct me, but it feels like people on the left attack more
[02:48:15.420 --> 02:48:16.420]   viciously.
[02:48:16.420 --> 02:48:19.700]   Um, that has not been my experience.
[02:48:19.700 --> 02:48:21.620]   Well, it hasn't?
[02:48:21.620 --> 02:48:25.960]   No, this, so, so yeah, let me know because my sense was that they attack people on the
[02:48:25.960 --> 02:48:27.960]   left viciously as well.
[02:48:27.960 --> 02:48:31.640]   Left attacks its own because you're not progressive enough.
[02:48:31.640 --> 02:48:36.280]   You're not, uh, you know, it's just this kind of bullying that happens very intensely.
[02:48:36.280 --> 02:48:45.200]   No, you're a hundred percent right that when the left has attacked me, um, it's almost
[02:48:45.200 --> 02:48:47.840]   as vicious as when the right attacks me.
[02:48:47.840 --> 02:48:53.560]   The difference in my experience is it's a smaller contingent on the left that's willing
[02:48:53.560 --> 02:48:57.160]   to levy those attacks against me, but I'm on the left.
[02:48:57.160 --> 02:49:00.400]   So to some degree you could say, well that that's to be expected.
[02:49:00.400 --> 02:49:04.960]   Um, there is toxicity on the left, but it's intense, isn't it?
[02:49:04.960 --> 02:49:09.480]   Like, and that's what I mean, like the attacks on people who are on the left, just, uh, you're
[02:49:09.480 --> 02:49:10.480]   not left enough.
[02:49:10.480 --> 02:49:13.240]   Yeah, that's no, that's the, and it is a small number of people.
[02:49:13.240 --> 02:49:18.880]   I can't deny that that is absolutely, uh, absolutely a real phenomenon.
[02:49:18.880 --> 02:49:24.880]   And um, it depending on what sort of topics you take on publicly, you are going to suffer
[02:49:24.880 --> 02:49:29.080]   the wrath of that to a greater or, or lesser degree.
[02:49:29.080 --> 02:49:32.960]   But with all of these things, what I always go back to is, you know, I probably would
[02:49:32.960 --> 02:49:36.440]   have more disagreements with Rogan today than the last time I was on his show, which was
[02:49:36.440 --> 02:49:38.400]   like at the beginning of the pandemic.
[02:49:38.400 --> 02:49:42.760]   But there would be zero and I've done clips critical of things that he has said substantive.
[02:49:42.760 --> 02:49:48.080]   Of course, to me it's sort of like, oh yeah, I could sit down with him and do a podcast
[02:49:48.080 --> 02:49:50.520]   and it would be zero big deal.
[02:49:50.520 --> 02:49:53.520]   And I would tell him I stand by everything I said about what you said and I would say
[02:49:53.520 --> 02:49:54.600]   it to you right now.
[02:49:54.600 --> 02:49:59.960]   There are people who write to me and go, oh man, things must be really, really tense now.
[02:49:59.960 --> 02:50:04.480]   If you were to, Rogan would never have you on because you disagreed and it's, he loves
[02:50:04.480 --> 02:50:05.480]   you.
[02:50:05.480 --> 02:50:06.480]   I'm sure he's just not thinking of me.
[02:50:06.480 --> 02:50:08.600]   I'm not the most important thing to Joe Rogan.
[02:50:08.600 --> 02:50:13.920]   I think both of us would be able to sit down and talk about every one of my criticisms.
[02:50:13.920 --> 02:50:17.600]   It would not be taken personally and then we would move on and it would be the next
[02:50:17.600 --> 02:50:18.600]   day.
[02:50:18.600 --> 02:50:19.600]   You get attacked a lot.
[02:50:19.600 --> 02:50:23.160]   How do you not let that break you mentally?
[02:50:23.160 --> 02:50:25.240]   Well, I don't know.
[02:50:25.240 --> 02:50:27.100]   So let's see.
[02:50:27.100 --> 02:50:30.440]   I try to, I mean I'm in a toxic space.
[02:50:30.440 --> 02:50:35.840]   The news and politics, partisan news and politics, partisan news and politics on the internet
[02:50:35.840 --> 02:50:40.640]   with a social media component, just completely and totally toxic from a personal perspective.
[02:50:40.640 --> 02:50:45.720]   When I'm done producing my last show of the week until Monday, I try to completely tune
[02:50:45.720 --> 02:50:48.040]   out from news and politics altogether.
[02:50:48.040 --> 02:50:53.600]   Um, and also make an effort to just not look at feedback and what's going on.
[02:50:53.600 --> 02:50:58.560]   I also really limit my vis my visibility.
[02:50:58.560 --> 02:51:00.600]   Uh, I don't need to read every comment.
[02:51:00.600 --> 02:51:03.800]   I don't need to look at every email or every tweet.
[02:51:03.800 --> 02:51:09.320]   I have 15 minutes each day where I go through my social media platforms, look at generally
[02:51:09.320 --> 02:51:14.800]   what is the reaction been, maybe include that in my assessment of how I want to tackle a
[02:51:14.800 --> 02:51:18.160]   certain issue if I missed a good point or something like that and basically try to move
[02:51:18.160 --> 02:51:22.760]   on when something like we talked about at the beginning happens, it becomes obsessive.
[02:51:22.760 --> 02:51:24.240]   I mean it's unhealthy, right?
[02:51:24.240 --> 02:51:29.000]   Where I'm going, oh my God, who's attacking me now that scrolling, it becomes, uh, you
[02:51:29.000 --> 02:51:30.000]   know, I'm sweating.
[02:51:30.000 --> 02:51:31.120]   It's horrible.
[02:51:31.120 --> 02:51:35.760]   But I think just like limiting exposure to that and remembering that it is impossible
[02:51:35.760 --> 02:51:37.280]   to please everybody.
[02:51:37.280 --> 02:51:43.200]   And so I'd really rather have fresh, genuine views each day rather than views that are
[02:51:43.200 --> 02:51:50.360]   sort of like, uh, restricted and flattened by what I perceive to be people's preferences.
[02:51:50.360 --> 02:51:55.360]   Uh, so just can you speak a little more to the full process of creating the David Pakman
[02:51:55.360 --> 02:51:56.360]   show?
[02:51:56.360 --> 02:51:59.920]   Like what you wake up cause you're doing five shows a week.
[02:51:59.920 --> 02:52:03.440]   I have the Letterman schedule, which means I do five shows in four days.
[02:52:03.440 --> 02:52:07.360]   I shoot Monday to Thursday, but we're doing five episodes.
[02:52:07.360 --> 02:52:11.440]   Basically, uh, our guests, we schedule in advance.
[02:52:11.440 --> 02:52:16.560]   I'm picking six to eight stories each day that are, like I said, a blend of stuff I
[02:52:16.560 --> 02:52:22.280]   think will be interesting things I want to talk about and things where there's, it's
[02:52:22.280 --> 02:52:26.320]   being discussed at one layer and I want to go deeper on it and I feel like I'm able to
[02:52:26.320 --> 02:52:27.320]   do that.
[02:52:27.320 --> 02:52:31.920]   So I choose those stories in the morning record in the early afternoon and we put the show
[02:52:31.920 --> 02:52:34.320]   out by that afternoon.
[02:52:34.320 --> 02:52:38.360]   What the preparation, what's the, how do you take notes?
[02:52:38.360 --> 02:52:41.280]   Uh, are you on a sheet of paper?
[02:52:41.280 --> 02:52:42.480]   No sheets of paper anymore.
[02:52:42.480 --> 02:52:43.840]   I used to do sheets of paper.
[02:52:43.840 --> 02:52:48.160]   I found something about it like it worked, the tactile nature of it.
[02:52:48.160 --> 02:52:53.680]   It became inconvenient for sharing the notes with my team, but basically we use a wiki
[02:52:53.680 --> 02:52:55.320]   type system.
[02:52:55.320 --> 02:52:58.600]   It's called media wiki, which is basically like a Wikipedia clone.
[02:52:58.600 --> 02:52:59.600]   Old school.
[02:52:59.600 --> 02:53:00.600]   Yeah.
[02:53:00.600 --> 02:53:01.600]   Old school.
[02:53:01.600 --> 02:53:03.080]   So we can have pages for every guest, every topic.
[02:53:03.080 --> 02:53:04.080]   Oh, that's interesting.
[02:53:04.080 --> 02:53:05.080]   I haven't heard that.
[02:53:05.080 --> 02:53:06.080]   Yeah.
[02:53:06.080 --> 02:53:07.080]   We, I don't know anyone else who's using it.
[02:53:07.080 --> 02:53:08.080]   It works really well.
[02:53:08.080 --> 02:53:11.520]   It, it's so fast and it takes up almost no space.
[02:53:11.520 --> 02:53:13.840]   So it just is a really good tool.
[02:53:13.840 --> 02:53:19.200]   Uh, when my team, you know, when we book a guest and they have notes from the publicist,
[02:53:19.200 --> 02:53:21.560]   they'll put it in there and then I can access it.
[02:53:21.560 --> 02:53:24.160]   So I'm basically working off of notes rather than a script.
[02:53:24.160 --> 02:53:28.600]   Um, I'll pull any audio visual stuff that I want so that that's available.
[02:53:28.600 --> 02:53:34.560]   Um, and it's, I mean it's, it's really a, uh, very seamless, you know, we're doing this
[02:53:34.560 --> 02:53:36.000]   every day, four days a week.
[02:53:36.000 --> 02:53:37.920]   And so we have it down to a well-oiled machine.
[02:53:37.920 --> 02:53:40.840]   Where do you get ideas from everywhere?
[02:53:40.840 --> 02:53:46.640]   Um, I have a bunch of subreddits that I follow that I think are talking about interesting
[02:53:46.640 --> 02:53:47.640]   things.
[02:53:47.640 --> 02:53:52.160]   I have a curated list for which I still use Twitter and it is very good for this.
[02:53:52.160 --> 02:53:56.880]   It's a curated private list of journalists that I think are doing interesting work.
[02:53:56.880 --> 02:53:58.640]   So I'll see what's there.
[02:53:58.640 --> 02:54:05.720]   Look at the sort of standard news reporting, uh, wire services, AP and Reuters glance at
[02:54:05.720 --> 02:54:11.320]   what, um, everything from drudge to CNN to whoever is covering that day.
[02:54:11.320 --> 02:54:13.580]   Look at Google news.
[02:54:13.580 --> 02:54:17.440]   How do you try to fact check stuff on your show?
[02:54:17.440 --> 02:54:19.440]   So like is there sources or is there a process?
[02:54:19.440 --> 02:54:25.400]   I always try to get to a primary source first and foremost for the facts of the story.
[02:54:25.400 --> 02:54:29.320]   And then I'll use other tools for background research.
[02:54:29.320 --> 02:54:33.840]   Oftentimes Wikipedia is footnotes I find to be useful tools.
[02:54:33.840 --> 02:54:35.840]   Chat GPT is a good one.
[02:54:35.840 --> 02:54:41.080]   It, I, you really have to fact check it, but it'll give you ideas of where to do the fact
[02:54:41.080 --> 02:54:42.680]   checking, which I think is fantastic.
[02:54:42.680 --> 02:54:44.820]   Sometimes it gives me information that's flat out wrong.
[02:54:44.820 --> 02:54:48.320]   And when you ask for the source, it's like, oh yeah, that, that actually is not real.
[02:54:48.320 --> 02:54:51.240]   Um, which is, Hey, it's part, part of the process.
[02:54:51.240 --> 02:54:52.240]   Yeah.
[02:54:52.240 --> 02:54:56.440]   But, um, and then when there's like an expertise type of thing, if it's a breaking legal matter,
[02:54:56.440 --> 02:55:00.400]   I'll just call like a friend who's a lawyer or call a friend who's a doctor or something
[02:55:00.400 --> 02:55:01.400]   like that.
[02:55:01.400 --> 02:55:03.000]   If it lends itself to that.
[02:55:03.000 --> 02:55:05.880]   Let me ask you about the nature of truth.
[02:55:05.880 --> 02:55:12.480]   Do you think it's becoming more and more difficult to know what is true and will become continuously
[02:55:12.480 --> 02:55:16.040]   continue to get more difficult, especially with GPT?
[02:55:16.040 --> 02:55:23.400]   I think the big difficulty is in getting people to agree as to what is a statement of fact
[02:55:23.400 --> 02:55:25.360]   and what is a statement of opinion.
[02:55:25.360 --> 02:55:30.760]   I think once we can do that, reasonable people can more or less agree on how to get to the
[02:55:30.760 --> 02:55:36.520]   truth or if we can't get to it, at least figure out how we would if the information were available.
[02:55:36.520 --> 02:55:42.960]   But the, the bigger challenge I'm having is someone will call in with an opinion, but
[02:55:42.960 --> 02:55:45.680]   say they want to talk about facts.
[02:55:45.680 --> 02:55:49.880]   And I have to explain to them, you're talking about an opinion and not a fact.
[02:55:49.880 --> 02:55:54.360]   And this goes back to the lack of critical thinking and lack of media literacy.
[02:55:54.360 --> 02:55:57.440]   Uh, but that's the bigger challenge for me right now.
[02:55:57.440 --> 02:56:02.920]   But I mean, I think the big statements are always going to be somewhat opinions like,
[02:56:02.920 --> 02:56:09.680]   um, was the elect was the 2020 election fair.
[02:56:09.680 --> 02:56:12.440]   I think any answer to that as an opinion.
[02:56:12.440 --> 02:56:14.140]   I disagree.
[02:56:14.140 --> 02:56:16.000]   If we define fair.
[02:56:16.000 --> 02:56:18.600]   Well, yes.
[02:56:18.600 --> 02:56:23.760]   So then I don't think it's possible to define fair in a way that's not several paragraphs
[02:56:23.760 --> 02:56:27.720]   where each sentence it now has, has facts, right?
[02:56:27.720 --> 02:56:30.360]   So what do you, what do you mean by fair?
[02:56:30.360 --> 02:56:32.560]   Is it a, who could show up to vote?
[02:56:32.560 --> 02:56:35.200]   What was the process of how easy it is to vote?
[02:56:35.200 --> 02:56:40.360]   Uh, was there actual, uh, cheating going on in different, like what is the evidence of
[02:56:40.360 --> 02:56:41.360]   that cheating?
[02:56:41.360 --> 02:56:46.480]   It's hard to actually get to the actual like details of a thing, high level, you know,
[02:56:46.480 --> 02:56:48.760]   uh, everything is just going to be an opinion.
[02:56:48.760 --> 02:56:56.360]   It feels like, and you can approximate that to be like, it's a well founded opinion.
[02:56:56.360 --> 02:56:58.160]   Well, most of science is an opinion.
[02:56:58.160 --> 02:56:59.700]   Even physics is an opinion.
[02:56:59.700 --> 02:57:05.000]   So like, I think there's a threshold beyond which an opinion becomes like, uh, this is
[02:57:05.000 --> 02:57:08.920]   a pretty reliable thing to assume for now that this is true.
[02:57:08.920 --> 02:57:09.920]   Okay.
[02:57:09.920 --> 02:57:10.920]   So let me revise.
[02:57:10.920 --> 02:57:20.240]   Maybe better said, I think that the difficulty, I mean it is the process you described is
[02:57:20.240 --> 02:57:26.400]   probably the right process and is it's exhausting for mundane things and that causes major problems.
[02:57:26.400 --> 02:57:34.360]   If we were to say, is it better for the economy to have a tax rate on people making over a
[02:57:34.360 --> 02:57:37.680]   million dollars, that's 20% or 50%.
[02:57:37.680 --> 02:57:38.680]   Okay.
[02:57:38.680 --> 02:57:41.600]   What do we mean by better for the economy?
[02:57:41.600 --> 02:57:44.280]   It's not an overwhelming task to decide on that.
[02:57:44.280 --> 02:57:47.820]   We could say, well, we'll say it's better for the economy by looking at what was the
[02:57:47.820 --> 02:57:51.180]   unemployment rate based on the tax rate on million, you know, people earning a million
[02:57:51.180 --> 02:57:54.560]   a year and what was GDP and whatever.
[02:57:54.560 --> 02:57:55.560]   Okay.
[02:57:55.560 --> 02:57:58.260]   We've, we've agreed this is a statement.
[02:57:58.260 --> 02:58:03.800]   We are now in the realm of just determining what is given the parameters that we've established.
[02:58:03.800 --> 02:58:06.360]   I think that that's, that's relatively doable.
[02:58:06.360 --> 02:58:12.040]   The issue is with the bigger ones like you're talking about where, what do we mean by a
[02:58:12.040 --> 02:58:18.560]   fair election and fair in whose eyes and, but I am with you that it often devolves into
[02:58:18.560 --> 02:58:24.720]   a conversation about opinions about what is fair rather than an ascertainment of the facts.
[02:58:24.720 --> 02:58:25.720]   Yeah.
[02:58:25.720 --> 02:58:31.640]   And it feels like maybe avoiding some of these big, maybe there's some trigger words to maybe
[02:58:31.640 --> 02:58:36.840]   avoiding them allows you to actually talk about the facts and through that educate yourself
[02:58:36.840 --> 02:58:43.060]   and learn about like whether the virus leaked from a lab or not.
[02:58:43.060 --> 02:58:45.200]   To me it was always a super interesting question.
[02:58:45.200 --> 02:58:48.040]   I don't know why everybody got super touchy about it.
[02:58:48.040 --> 02:58:54.800]   Mostly people I know, colleagues, biologists thought it's pretty good likelihood that it
[02:58:54.800 --> 02:58:56.680]   leaked from a lab given everything.
[02:58:56.680 --> 02:59:00.920]   They just didn't, the evidence is not there for either one.
[02:59:00.920 --> 02:59:06.000]   And so like you should be able to just openly talk about it unless you're in a high political
[02:59:06.000 --> 02:59:09.600]   office where there could be geopolitical consequences to your statements.
[02:59:09.600 --> 02:59:11.360]   But in general it's an interesting question.
[02:59:11.360 --> 02:59:15.040]   You should be able to talk about it, but there's no, first of all there's not many facts around
[02:59:15.040 --> 02:59:18.120]   there unfortunately.
[02:59:18.120 --> 02:59:21.520]   And a lot of very conclusive statements about it, especially in the early days, were just
[02:59:21.520 --> 02:59:22.520]   opinions.
[02:59:22.520 --> 02:59:29.720]   And so you have to, the idea of what is true or not becomes a little, even mentioning the
[02:59:29.720 --> 02:59:33.280]   word truth in that context, it feels divisive.
[02:59:33.280 --> 02:59:37.520]   Yeah, I completely agree with you, which is strange.
[02:59:37.520 --> 02:59:39.680]   Like it really, you think it shouldn't.
[02:59:39.680 --> 02:59:45.000]   One of the really good opening questions that I've had work to my advantage when talking
[02:59:45.000 --> 02:59:50.240]   with people who I know disagree with me about a contentious topic is how do you think we
[02:59:50.240 --> 02:59:52.280]   would figure out X?
[02:59:52.280 --> 02:59:57.560]   And it often gets people thinking first collaboratively.
[02:59:57.560 --> 03:00:01.040]   And obviously we might have very different opinions, but with something like the COVID
[03:00:01.040 --> 03:00:05.240]   lab leak, I think it's an interesting one because if you say, okay, maybe it leaked,
[03:00:05.240 --> 03:00:06.400]   maybe it didn't.
[03:00:06.400 --> 03:00:07.680]   How would we figure that out?
[03:00:07.680 --> 03:00:09.720]   Who would we trust to weigh in on that?
[03:00:09.720 --> 03:00:11.360]   What evidence would count?
[03:00:11.360 --> 03:00:14.220]   Now we're kind of on the same team.
[03:00:14.220 --> 03:00:19.160]   And then if we can establish that, then we're on a search for capital T truth together or
[03:00:19.160 --> 03:00:20.160]   whatever.
[03:00:20.160 --> 03:00:23.880]   It's kind of pie in the sky, but in some conversations I've actually had success with that.
[03:00:23.880 --> 03:00:28.640]   And then you can kind of realize if a, if there's no amount of evidence that's going
[03:00:28.640 --> 03:00:34.000]   to prove a show to you that you're wrong in your current opinion, that that's probably
[03:00:34.000 --> 03:00:35.520]   a really bad sign for you.
[03:00:35.520 --> 03:00:36.520]   It's a waste.
[03:00:36.520 --> 03:00:38.840]   It may be a waste to pursue the conversation further at that point.
[03:00:38.840 --> 03:00:39.840]   Yeah.
[03:00:39.840 --> 03:00:40.840]   I mean, yeah.
[03:00:40.840 --> 03:00:41.840]   What?
[03:00:41.840 --> 03:00:42.840]   Oh, so, okay.
[03:00:42.840 --> 03:00:43.840]   You think Trump was a good president.
[03:00:43.840 --> 03:00:47.860]   How do you determine that and what evidence might exist that would change your mind?
[03:00:47.860 --> 03:00:48.860]   There is no evidence.
[03:00:48.860 --> 03:00:50.220]   Trump was the best president ever.
[03:00:50.220 --> 03:00:53.400]   I think the conversation is probably done except Abraham Lincoln.
[03:00:53.400 --> 03:00:54.400]   Right.
[03:00:54.400 --> 03:00:56.520]   Uh, you mentioned Israel, Palestine.
[03:00:56.520 --> 03:00:59.760]   Well, what do you think about the situation in Israel and Palestine?
[03:00:59.760 --> 03:01:02.520]   Something you've thought about, spoken about for quite a time.
[03:01:02.520 --> 03:01:05.760]   Do you think we'll ever see peace in this part of the world?
[03:01:05.760 --> 03:01:06.760]   I don't know.
[03:01:06.760 --> 03:01:07.760]   Yeah.
[03:01:07.760 --> 03:01:09.920]   I mean, uh, I could say yes.
[03:01:09.920 --> 03:01:10.920]   I could say yes.
[03:01:10.920 --> 03:01:19.040]   I, I, the, you know, one of the problems is, and I'll give, you may not know that the,
[03:01:19.040 --> 03:01:24.760]   there are people on the left of my audience who call me a Netanyahu shill, even though
[03:01:24.760 --> 03:01:28.480]   I've never been a supporter of Netanyahu and I'm, I'm on the left.
[03:01:28.480 --> 03:01:36.300]   I just don't think that some of the, uh, kind of black and white characterizations about
[03:01:36.300 --> 03:01:38.840]   Israel are even remotely accurate.
[03:01:38.840 --> 03:01:42.360]   And I think most people, uh, it's become a sort of litmus test.
[03:01:42.360 --> 03:01:43.960]   Are you criticizing Israel enough?
[03:01:43.960 --> 03:01:45.960]   Are you showing us that you're actually left wing?
[03:01:45.960 --> 03:01:47.040]   I don't do any of that stuff.
[03:01:47.040 --> 03:01:49.400]   I sort of really look at the situation for what it is.
[03:01:49.400 --> 03:01:55.200]   That's become a litmus test in American politics, uh, in the spectrum of American politics.
[03:01:55.200 --> 03:01:56.200]   Yes.
[03:01:56.200 --> 03:02:07.200]   Um, my view, big picture is that, uh, I don't think we're going to really get anywhere until
[03:02:07.200 --> 03:02:14.800]   some pre negotiated terms are set and the parties to do the negotiating are all good
[03:02:14.800 --> 03:02:16.080]   faith parties.
[03:02:16.080 --> 03:02:22.160]   For example, I don't think Israel's right wing party Likud is a particularly good faith
[03:02:22.160 --> 03:02:28.760]   arbiter of peace because I think Likud benefits from there not being peace and the threat
[03:02:28.760 --> 03:02:30.100]   of violence.
[03:02:30.100 --> 03:02:31.100]   And there is violence.
[03:02:31.100 --> 03:02:32.480]   It's not just the threat of violence.
[03:02:32.480 --> 03:02:38.720]   I don't think Hamas is going to be an arbiter of peace for the Palestinian people either.
[03:02:38.720 --> 03:02:40.800]   I think the Palestinian authority is a question mark.
[03:02:40.800 --> 03:02:42.200]   I'm not sure.
[03:02:42.200 --> 03:02:46.960]   So I think that there needs to be some pre conditions that would need to be set with
[03:02:46.960 --> 03:02:52.200]   regard to everything from settlements to a lot of this minutiae.
[03:02:52.200 --> 03:02:57.000]   Big picture though, if I imagine what the most likely solution looks like, it doesn't
[03:02:57.000 --> 03:02:59.840]   mean it's a perfect solution and obviously it's a solution.
[03:02:59.840 --> 03:03:02.560]   Many people will say it's not, it's not going to happen.
[03:03:02.560 --> 03:03:07.400]   I think it's a solution where the borders are similar to what was being discussed in
[03:03:07.400 --> 03:03:10.240]   the Clinton era to some degree.
[03:03:10.240 --> 03:03:16.420]   As many of the settlements as possible have to go understanding that some of the bigger
[03:03:16.420 --> 03:03:20.560]   ones are just not going to go and there's going to have to be meaningful land swaps
[03:03:20.560 --> 03:03:25.520]   with which Yasser Arafat seemed to be amenable to when he weighed in on it.
[03:03:25.520 --> 03:03:27.640]   I believe it was in the 90s.
[03:03:27.640 --> 03:03:33.540]   The topic of the temple Mount and Jerusalem, et cetera, is a complicated one.
[03:03:33.540 --> 03:03:40.240]   But I think that almost certainly, um, East Jerusalem is going to have to be part of an
[03:03:40.240 --> 03:03:41.960]   eventual Palestinian state.
[03:03:41.960 --> 03:03:44.920]   You know, I mean like we can go as kind of as far as, as we want to with a lot of this
[03:03:44.920 --> 03:03:45.920]   stuff.
[03:03:45.920 --> 03:03:50.840]   What role does us have to play in this coming to the table with good faith parties?
[03:03:50.840 --> 03:03:56.160]   I don't know whether the, I go back and forth between believing that the us should play
[03:03:56.160 --> 03:04:03.340]   a big role to the us should play essentially no role whatsoever because, uh, of course
[03:04:03.340 --> 03:04:09.840]   of the funding of, uh, Israel that the us provides, will the you, I don't, it's not
[03:04:09.840 --> 03:04:14.140]   that I have a personal problem with American involvement and somebody like Bill Clinton
[03:04:14.140 --> 03:04:17.920]   was arguably relatively well positioned to try to make something happen.
[03:04:17.920 --> 03:04:22.100]   It's more just, will there, will it be seen as credible on the global stage?
[03:04:22.100 --> 03:04:25.060]   And that's, I think the most important thing because at the end of whatever negotiation
[03:04:25.060 --> 03:04:31.700]   takes place, both sides need to agree that this is where we are renouncing all past claims.
[03:04:31.700 --> 03:04:35.620]   And in the future, if there's a disagreement, we can't go back to that thing from the eighties
[03:04:35.620 --> 03:04:36.620]   or the nineties.
[03:04:36.620 --> 03:04:37.940]   That's just like a critical piece of this.
[03:04:37.940 --> 03:04:38.940]   Yeah.
[03:04:38.940 --> 03:04:48.540]   It has to be stable and uh, you know, um, materialize it to something stable over years.
[03:04:48.540 --> 03:04:50.620]   Yes.
[03:04:50.620 --> 03:04:55.940]   Another difficult conflict going on in the world is the war in Ukraine.
[03:04:55.940 --> 03:05:00.220]   What do you think about the Russian invasion of Ukraine in February, 2022?
[03:05:00.220 --> 03:05:03.380]   I don't pretend to be an expert on this issue.
[03:05:03.380 --> 03:05:05.780]   I think you probably know more about this than I do.
[03:05:05.780 --> 03:05:11.760]   Just from the brief conversation we had before we started filming my view as a general observer
[03:05:11.760 --> 03:05:16.220]   of geopolitics and the way that this area, this part of the world is related to American
[03:05:16.220 --> 03:05:22.580]   presidents over the last several cycles is I don't think it's controversial to say that
[03:05:22.580 --> 03:05:28.340]   this was a war of aggression, an invasion of aggression and active aggression by Vladimir
[03:05:28.340 --> 03:05:29.340]   Putin.
[03:05:29.340 --> 03:05:41.040]   Um, I do believe that if Trump had been reelected, Putin may have seen himself as having other
[03:05:41.040 --> 03:05:48.480]   tools with which to try to, um, expand influence that may have been different than geographical
[03:05:48.480 --> 03:05:51.260]   pursuits, geographic pursuits.
[03:05:51.260 --> 03:05:54.300]   Uh, but I, we don't know that for sure.
[03:05:54.300 --> 03:06:01.560]   Um, I also have a really hard time imagining what the end of this looks like and that's
[03:06:01.560 --> 03:06:10.260]   very scary because sometimes the most benign and seems to be that Putin ends up out of
[03:06:10.260 --> 03:06:17.980]   power either through no longer being alive, uh, or deposed in some way.
[03:06:17.980 --> 03:06:22.220]   It doesn't feel like that ladder is super likely the former there's reports about his
[03:06:22.220 --> 03:06:23.220]   health.
[03:06:23.220 --> 03:06:24.460]   I don't know how accurate they are.
[03:06:24.460 --> 03:06:33.100]   It's just hard to imagine a face saving exit that is going to be, um, even remotely, uh,
[03:06:33.100 --> 03:06:34.100]   what's the word?
[03:06:34.100 --> 03:06:35.260]   It's not even a question of acceptable.
[03:06:35.260 --> 03:06:38.580]   It's just, um, it's not satisfying either.
[03:06:38.580 --> 03:06:43.820]   Just, just not tragic, I guess, is what I'm looking for in terms of, uh, Putin speaking
[03:06:43.820 --> 03:06:48.660]   to the Russian people and being able to figure out what to say, what kind of narrative to
[03:06:48.660 --> 03:06:50.500]   say why this war made sense.
[03:06:50.500 --> 03:06:51.500]   Yes.
[03:06:51.500 --> 03:06:55.500]   Um, the same on the Ukrainian side to figure out how to exit this war.
[03:06:55.500 --> 03:06:56.500]   Yes.
[03:06:56.500 --> 03:07:03.780]   Uh, I mean to some degree it requires Russian troops leaving Ukraine and that is somewhat
[03:07:03.780 --> 03:07:04.780]   under the control.
[03:07:04.780 --> 03:07:10.900]   I mean, if of course it's not up to Ukraine whether the initiative continues, but what
[03:07:10.900 --> 03:07:17.660]   I am not thrilled with are some of the reflexive, you know, if Trump had been in power instead
[03:07:17.660 --> 03:07:24.860]   of Joe Biden, a lot of the reflexive, uh, comments about, oh, you're, if you, if you
[03:07:24.860 --> 03:07:30.780]   say Ukraine is just acting defensively, you're supporting neo Nazis or some of these things
[03:07:30.780 --> 03:07:35.800]   that have come out of the American Republican Party seem both wacky and like they would
[03:07:35.800 --> 03:07:39.660]   be saying completely different things if Trump happened to be in the Oval Office.
[03:07:39.660 --> 03:07:41.820]   They're really proxy attacks on Joe Biden.
[03:07:41.820 --> 03:07:42.820]   Yeah.
[03:07:42.820 --> 03:07:49.220]   Well that in some sense, Ukraine is also kind of political litmus test of how you speak
[03:07:49.220 --> 03:07:50.220]   about it.
[03:07:50.220 --> 03:07:56.020]   I think because of the huge amount of funding that's going, uh, from us to Ukraine.
[03:07:56.020 --> 03:07:59.140]   Um, maybe you can correct me if I'm wrong.
[03:07:59.140 --> 03:08:03.780]   Um, but it seems to, it seems to me that this topic has become politicized already.
[03:08:03.780 --> 03:08:04.860]   A hundred percent.
[03:08:04.860 --> 03:08:08.300]   There are people like Marjorie Taylor green and others saying we should be doing nothing
[03:08:08.300 --> 03:08:09.300]   for Ukraine.
[03:08:09.300 --> 03:08:13.420]   Zelensky is a comedian and we're supporting neo Nazis.
[03:08:13.420 --> 03:08:14.420]   That's it.
[03:08:14.420 --> 03:08:15.420]   Full stop.
[03:08:15.420 --> 03:08:18.060]   And you either subscribe to that, um, or you don't.
[03:08:18.060 --> 03:08:23.660]   And it very quickly becomes, um, it very quickly becomes as partisan as so many other issues.
[03:08:23.660 --> 03:08:28.380]   And it's really the most disappointing thing is that some of these issues become incredibly
[03:08:28.380 --> 03:08:30.780]   the devices divisive, but they're simple.
[03:08:30.780 --> 03:08:33.540]   Like for example, a conspiracy theory that we know isn't true.
[03:08:33.540 --> 03:08:36.420]   It shouldn't be devices divisive because it's so simple.
[03:08:36.420 --> 03:08:41.900]   Other issues become divisive and they are simplified, but in reality they are extraordinarily
[03:08:41.900 --> 03:08:46.900]   complex and you lose the ability to talk about the complexity because they're becoming partisan.
[03:08:46.900 --> 03:08:51.980]   Uh, do you think there will always be war in the world as a bunch of folks in the subreddit
[03:08:51.980 --> 03:08:56.620]   that were interested in your different complex perspectives on foreign policy?
[03:08:56.620 --> 03:08:58.780]   So let's talk about war.
[03:08:58.780 --> 03:09:04.540]   You look at the war in Ukraine, you look at what's going on in Israel and Palestine, you
[03:09:04.540 --> 03:09:06.140]   look at the wars across the world.
[03:09:06.140 --> 03:09:09.460]   Do you think there will always be war as a Redditor put it?
[03:09:09.460 --> 03:09:13.140]   Is it a necessary evil in the game of geopolitics?
[03:09:13.140 --> 03:09:21.180]   I used to have what I now believe is an extremely naive perspective, which is that if we somehow,
[03:09:21.180 --> 03:09:27.860]   if a intelligent aliens arrived here, it would be so momentous for homo sapiens that all
[03:09:27.860 --> 03:09:33.100]   of our differences would immediately be exposed to so insignificant.
[03:09:33.100 --> 03:09:36.640]   We would never fight again and we would realize that intelligent life.
[03:09:36.640 --> 03:09:42.020]   And then I spoke to people who deal in space exploration and other, other scientists and
[03:09:42.020 --> 03:09:44.560]   they all said that David, that's extraordinarily naive.
[03:09:44.560 --> 03:09:49.300]   There would be a period during which this was as momentous as you're imagining and then
[03:09:49.300 --> 03:09:53.800]   it would become normal and then we would go back to many of the same conflicts that we
[03:09:53.800 --> 03:09:56.700]   have now, sectarian, et cetera.
[03:09:56.700 --> 03:10:04.480]   I think that in all likelihood there will always be conflict between factions, whether
[03:10:04.480 --> 03:10:08.060]   it's what we currently think of as war, probably not.
[03:10:08.060 --> 03:10:14.180]   I mean, it seems as though the tactics will evolve and it will be less about missiles
[03:10:14.180 --> 03:10:16.380]   and I don't know where it's going to go.
[03:10:16.380 --> 03:10:20.860]   I don't know whether it's going to become more biological or cyber or certainly something
[03:10:20.860 --> 03:10:22.620]   we haven't even considered yet.
[03:10:22.620 --> 03:10:27.060]   But um, I think there will always be conflicts we would refer to in that way.
[03:10:27.060 --> 03:10:33.540]   Do you agree with Chomsky on the, his general harsh criticism of us foreign policy in war?
[03:10:33.540 --> 03:10:39.940]   That's many actions, military actions in the United States are criminal in nature, almost
[03:10:39.940 --> 03:10:41.500]   terrorist in nature.
[03:10:41.500 --> 03:10:48.460]   I am not, it's been a decade or more since I've read any Chomsky and I don't keep up
[03:10:48.460 --> 03:10:50.780]   with everything that he has recently said.
[03:10:50.780 --> 03:10:54.500]   So I don't want to mischaracterize any of it.
[03:10:54.500 --> 03:11:01.460]   In general, Americans are sold the view that we're the good guys spreading freedom across
[03:11:01.460 --> 03:11:02.460]   the world.
[03:11:02.460 --> 03:11:07.380]   And no, Chomsky takes a perspective that, yeah, but if you look at the number of civilians
[03:11:07.380 --> 03:11:11.460]   you kill while doing it, it's incomparable to any other military actions across the world.
[03:11:11.460 --> 03:11:12.460]   Right.
[03:11:12.460 --> 03:11:19.280]   So I very much disagree with those who take the view that the U S is this, um, wonderful
[03:11:19.280 --> 03:11:24.260]   global police force that's spreading democracy and fixing problems.
[03:11:24.260 --> 03:11:25.860]   Very, very much wrong.
[03:11:25.860 --> 03:11:30.340]   I think where I've had disagreements with Chomsky in the past is more he framed, he
[03:11:30.340 --> 03:11:36.220]   seems to frame the U S as a uniquely bad actor in some of these cases.
[03:11:36.220 --> 03:11:42.740]   And I think it's more an outcropping of the size and wealth of the U S and less about
[03:11:42.740 --> 03:11:45.260]   uniquely negative intentions.
[03:11:45.260 --> 03:11:49.300]   And so I think that would be my general disagreement with Chomsky based on stuff I read a decade
[03:11:49.300 --> 03:11:50.300]   ago.
[03:11:50.300 --> 03:11:54.520]   Well, he says that he lives in the United States, he's an American, and so he feels
[03:11:54.520 --> 03:11:57.020]   his focus of criticism should be in America.
[03:11:57.020 --> 03:12:02.300]   And I think that's one of the great things about being an American and being in America
[03:12:02.300 --> 03:12:04.980]   is the freedom to criticize harshly.
[03:12:04.980 --> 03:12:05.980]   Sure.
[03:12:05.980 --> 03:12:12.140]   While being a university professor, by the way, um, he's basically the embodiment of
[03:12:12.140 --> 03:12:14.100]   why 10 years are really valuable thing.
[03:12:14.100 --> 03:12:15.100]   I agree.
[03:12:15.100 --> 03:12:16.780]   Whether you agree with him or not.
[03:12:16.780 --> 03:12:23.500]   A question from Reddit, ask David what he plans for his garden this year.
[03:12:23.500 --> 03:12:24.500]   Is this a joke or is this a recipe?
[03:12:24.500 --> 03:12:25.500]   It's not a joke.
[03:12:25.500 --> 03:12:27.740]   I got into gardening a few years ago.
[03:12:27.740 --> 03:12:30.660]   Honestly, I'm with the baby.
[03:12:30.660 --> 03:12:32.140]   I can't do a garden this year.
[03:12:32.140 --> 03:12:35.580]   It's just, and I have a lot of travel coming up, so everything would die.
[03:12:35.580 --> 03:12:38.300]   But I did start to try to figure out gardening.
[03:12:38.300 --> 03:12:43.420]   If you're stressed by the toxicity of the social media world, gardening is a great hobby.
[03:12:43.420 --> 03:12:45.340]   It really is.
[03:12:45.340 --> 03:12:49.340]   But it is extraordinarily time consuming, so I have no garden planned this year.
[03:12:49.340 --> 03:12:56.580]   Um, other books, uh, or maybe movies in your life that had a big impact on you that, you
[03:12:56.580 --> 03:13:00.660]   know, you're thinking about that diet.
[03:13:00.660 --> 03:13:05.780]   Has there been stuff you read, forget it, about even just books, like blogs or writers
[03:13:05.780 --> 03:13:14.340]   or just sources of information that had, uh, that molded you into the intellectual, into
[03:13:14.340 --> 03:13:17.060]   the political thinker that you are?
[03:13:17.060 --> 03:13:21.660]   It's so hard to, this is sort of like, you know, you win an Oscar and you want to make
[03:13:21.660 --> 03:13:23.580]   sure you thank all the right people.
[03:13:23.580 --> 03:13:29.180]   I read so much and have been reading for so long that it's really hard to say, but I think
[03:13:29.180 --> 03:13:39.300]   certainly, um, for me, narrative nonfiction has been a fantastic genre to learn not only
[03:13:39.300 --> 03:13:43.180]   about history, but also about people and psychology.
[03:13:43.180 --> 03:13:48.060]   And very often when people say, I don't really read, like, what can you recommend to me that
[03:13:48.060 --> 03:13:49.260]   might be interesting?
[03:13:49.260 --> 03:13:55.180]   I'll, depending on, you know, knowing them to some degree, I'll give recommendations
[03:13:55.180 --> 03:14:00.420]   there in terms of, of, um, just things I picked up recently that, that I think are interesting.
[03:14:00.420 --> 03:14:05.020]   Um, I've been reading, uh, a bunch of Neil Postman.
[03:14:05.020 --> 03:14:11.180]   I read a Jenny Odell has a new book on time and the concept of like saving time, spending
[03:14:11.180 --> 03:14:12.180]   time, et cetera.
[03:14:12.180 --> 03:14:14.120]   She just published it.
[03:14:14.120 --> 03:14:15.120]   Super interesting.
[03:14:15.120 --> 03:14:22.300]   I just read, um, Lansing's book about the Shackleton voyage in Antarctica in 1914, 15
[03:14:22.300 --> 03:14:23.640]   and 16.
[03:14:23.640 --> 03:14:24.640]   Super interesting.
[03:14:24.640 --> 03:14:25.940]   I'm really all over the map.
[03:14:25.940 --> 03:14:27.180]   I have that one on audio book.
[03:14:27.180 --> 03:14:31.780]   I've been meeting to listen to it's very interesting and it seems inconceivable how these guys
[03:14:31.780 --> 03:14:34.620]   survived it in completely inconceivable.
[03:14:34.620 --> 03:14:36.340]   And yet they did.
[03:14:36.340 --> 03:14:42.380]   Oh, it kind of inspires you to think of a space exploration and taking on similar kinds
[03:14:42.380 --> 03:14:44.380]   of risky and dangerous journeys.
[03:14:44.380 --> 03:14:47.100]   Uh, in narrative nonfiction, I grew with you very much.
[03:14:47.100 --> 03:14:53.840]   I've been reading a lot of 20th century history, um, about Stalin and about Hitler rise and
[03:14:53.840 --> 03:14:54.840]   fall of the third Reich.
[03:14:54.840 --> 03:15:02.020]   I've read twice now and I recommend, what did you get out of reading at the second time?
[03:15:02.020 --> 03:15:12.100]   Uh, so what I, uh, so the second time I listened to the audio book as I ran, I get the same
[03:15:12.100 --> 03:15:19.740]   thing from it, uh, as I get maybe reading a man search for meaning, which is, um, all
[03:15:19.740 --> 03:15:25.460]   the troubles of day to day in the modern world kind of, uh, fade away and dissipate when
[03:15:25.460 --> 03:15:33.980]   I'm thinking about the, uh, you know, basically the embodiment of evil at scale, uh, at that
[03:15:33.980 --> 03:15:35.980]   recent time in human history.
[03:15:35.980 --> 03:15:40.180]   So it's, it makes me sort of appreciate all the, it fills me with gratitude to have all
[03:15:40.180 --> 03:15:44.860]   the freedoms, all the just simple joys of life that we have today.
[03:15:44.860 --> 03:15:51.140]   And um, I think the second time I was, uh, as I was reading it, uh, because William Shire
[03:15:51.140 --> 03:15:54.460]   was there, he's the author, he was there through the whole thing.
[03:15:54.460 --> 03:15:57.660]   You start to pick up little details as opposed to like big things.
[03:15:57.660 --> 03:16:04.620]   You start to pick up the little quirks of, uh, how history turns and just like these
[03:16:04.620 --> 03:16:10.220]   little events, you notice of the dynamics between people in a room during a meeting
[03:16:10.220 --> 03:16:11.220]   with Hitler.
[03:16:11.220 --> 03:16:14.820]   You just notice these little things that are mentioned because he was either there directly,
[03:16:14.820 --> 03:16:16.660]   or heard it the next day.
[03:16:16.660 --> 03:16:20.940]   So you get, that's why to me, um, Rise and Fall of the Third Reich is interesting is
[03:16:20.940 --> 03:16:28.100]   because it's by a guy who was there, who's reporting on it, um, versus a sort of a more
[03:16:28.100 --> 03:16:31.540]   distant, uh, displaced, uh, retelling.
[03:16:31.540 --> 03:16:32.540]   And I also like biographies.
[03:16:32.540 --> 03:16:35.060]   I'm a big fan of biographies.
[03:16:35.060 --> 03:16:39.500]   And um, Walter Isaacson has just written some incredible ones, and Steve Jobs and Einstein,
[03:16:39.500 --> 03:16:41.860]   all that kind of stuff.
[03:16:41.860 --> 03:16:46.220]   Um, Victor Frankel's book is one I've read a bunch of times and it's so short and you
[03:16:46.220 --> 03:16:49.620]   know, reading in general, I know a lot of people who read way more than I do.
[03:16:49.620 --> 03:16:51.780]   And I also know a lot of people who don't read at all.
[03:16:51.780 --> 03:16:54.960]   I mean, they haven't read a book since, since college essentially.
[03:16:54.960 --> 03:17:00.040]   To me, it's almost like the amount I get from it, it's almost like a secret weapon where
[03:17:00.040 --> 03:17:05.300]   when I think, you know, in two or three or 400 pages, which I can read in whatever, 10
[03:17:05.300 --> 03:17:12.060]   days or however long it takes reading 30 pages a day, the amount of information insights
[03:17:12.060 --> 03:17:17.580]   into so many aspects of the human psyche that I can get, it's sort of like, it's not like
[03:17:17.580 --> 03:17:20.540]   I'm in a competition for anything in particular with anybody.
[03:17:20.540 --> 03:17:21.540]   I just do my show.
[03:17:21.540 --> 03:17:27.140]   But it's sort of like if I'm reading dozens of books a year and you're reading zero, I'm
[03:17:27.140 --> 03:17:32.560]   exposed to so many different things and ideas that are not even in your universe.
[03:17:32.560 --> 03:17:36.700]   It just seems like the power of reading just seems overwhelming.
[03:17:36.700 --> 03:17:42.300]   And I had, speaking of getting attacked, I had a fun time getting attacked a few months
[03:17:42.300 --> 03:17:45.020]   ago for publishing a reading list.
[03:17:45.020 --> 03:17:52.700]   Some reading at least a book a week, read 18 or 19 books from the beginning of the year.
[03:17:52.700 --> 03:17:55.700]   You got attacked for the books you chose or for this?
[03:17:55.700 --> 03:17:59.340]   I don't know for what, but it became quite viral.
[03:17:59.340 --> 03:18:00.900]   Attacked for reading.
[03:18:00.900 --> 03:18:01.900]   That's something.
[03:18:01.900 --> 03:18:08.740]   So it's basically what happened is that people, I actually don't, it's not worth, folks who
[03:18:08.740 --> 03:18:13.240]   know know, folks who don't, don't even worry about it.
[03:18:13.240 --> 03:18:18.580]   What I really loved about being attacked for it is it shows that you can get attacked for
[03:18:18.580 --> 03:18:19.580]   anything.
[03:18:19.580 --> 03:18:20.580]   Apparently.
[03:18:20.580 --> 03:18:22.380]   So it's not like I did something wrong.
[03:18:22.380 --> 03:18:24.180]   It was kind of a beautiful thing.
[03:18:24.180 --> 03:18:33.860]   It was just the most intensely beautiful display of absurdity of Twitter and the internet that
[03:18:33.860 --> 03:18:38.940]   there would be, there were articles written about me with the book list.
[03:18:38.940 --> 03:18:40.940]   There's no bad books on it.
[03:18:40.940 --> 03:18:47.460]   The thing I was being mocked for is reading Dostoevsky, reading stuff that sounds like
[03:18:47.460 --> 03:18:48.820]   a high school reading list.
[03:18:48.820 --> 03:18:50.060]   Oh, I see.
[03:18:50.060 --> 03:18:57.100]   Or all these kinds of aspects of the reading list which doesn't stand up to any sort of
[03:18:57.100 --> 03:18:58.660]   legitimate kind of criticism.
[03:18:58.660 --> 03:19:03.420]   But the fact that people are just looking for single words, single aspects of a tweet
[03:19:03.420 --> 03:19:05.540]   and so on to criticize.
[03:19:05.540 --> 03:19:12.660]   It actually forced me to, because I released a video about summarizing my takeaways from
[03:19:12.660 --> 03:19:16.020]   one of the books and I've been meaning to do more and more, but every time I start to
[03:19:16.020 --> 03:19:20.940]   like want to record it, I have this negative feeling.
[03:19:20.940 --> 03:19:23.300]   They kind of ruin the fun of sharing with others.
[03:19:23.300 --> 03:19:25.380]   I know exactly what you're talking about.
[03:19:25.380 --> 03:19:29.100]   My advice on that is don't do it.
[03:19:29.100 --> 03:19:30.100]   Just don't record it.
[03:19:30.100 --> 03:19:31.100]   Yeah.
[03:19:31.100 --> 03:19:32.620]   Because, and is that what you basically did?
[03:19:32.620 --> 03:19:37.020]   Yeah, for now, but I think time cures it, but for now I decided not to.
[03:19:37.020 --> 03:19:38.020]   Yes.
[03:19:38.020 --> 03:19:41.220]   It's just until I feel joy when I do it, yeah.
[03:19:41.220 --> 03:19:46.100]   We are in such a privileged position to even be able to do this sort of thing, right?
[03:19:46.100 --> 03:19:51.460]   I have taken on projects and then it sort of sounds good or I end up doing it because
[03:19:51.460 --> 03:19:56.700]   there's some third party that brings the idea and I feel like I can't really say no or whatever.
[03:19:56.700 --> 03:19:59.900]   And then when I get in front of the camera or I have to write something for a while,
[03:19:59.900 --> 03:20:03.540]   I did a newspaper column that I hated doing.
[03:20:03.540 --> 03:20:08.860]   I realized that I'm ruining the exact thing that I have worked to build, which is that
[03:20:08.860 --> 03:20:10.500]   I can just do whatever I want.
[03:20:10.500 --> 03:20:11.500]   Why am I doing this?
[03:20:11.500 --> 03:20:13.180]   And sometimes it takes me a week to realize it.
[03:20:13.180 --> 03:20:16.660]   Sometimes it takes me a year, but just don't do it.
[03:20:16.660 --> 03:20:17.660]   That's the thought.
[03:20:17.660 --> 03:20:21.460]   In this case in particular, it's also that there's a private thing I enjoy, which is
[03:20:21.460 --> 03:20:22.460]   reading.
[03:20:22.460 --> 03:20:23.460]   Right.
[03:20:23.460 --> 03:20:29.300]   And if sharing that private thing you enjoy is not fun, then just don't share it.
[03:20:29.300 --> 03:20:34.820]   That's, yeah, there's certain things, there's certain private things that should remain
[03:20:34.820 --> 03:20:35.820]   private.
[03:20:35.820 --> 03:20:38.780]   That's like, which is one of the first things ever.
[03:20:38.780 --> 03:20:44.340]   I'm the same person privately as I am publicly, but the books, it's like, man, I don't get
[03:20:44.340 --> 03:20:48.740]   to share, I guess through these conversations I can share some of the stuff I'm reading
[03:20:48.740 --> 03:20:50.060]   and enjoying it.
[03:20:50.060 --> 03:20:51.460]   Because it sucks.
[03:20:51.460 --> 03:20:56.420]   It sucks to get attacked for stuff and it sucks to get attacked for stuff you love.
[03:20:56.420 --> 03:20:57.700]   Yeah, especially reading.
[03:20:57.700 --> 03:21:01.300]   I mean, that's the bottom of the barrel.
[03:21:01.300 --> 03:21:07.180]   I have these ideas where I'll go, you know, maybe for my next thing, I'll go from politics,
[03:21:07.180 --> 03:21:09.740]   which is so toxic, I'll go to travel blogging.
[03:21:09.740 --> 03:21:14.380]   Because there's so many travel bloggers I follow and there's so many interesting places.
[03:21:14.380 --> 03:21:18.300]   And then I go, wait a second, I like traveling and just hanging out.
[03:21:18.300 --> 03:21:21.780]   Now traveling is going to be my job and now I've got to bring two cameras with me and
[03:21:21.780 --> 03:21:24.420]   I've got to get shots and I've got to film my food.
[03:21:24.420 --> 03:21:25.420]   I'm not going to do that.
[03:21:25.420 --> 03:21:30.020]   I'm just going to do what I'm doing, but then I'll travel when I want to take a vacation.
[03:21:30.020 --> 03:21:31.260]   And of course some of it could be fun.
[03:21:31.260 --> 03:21:38.220]   I mean, I have to say when I did one video on a book in 1984, I really enjoyed it.
[03:21:38.220 --> 03:21:42.300]   The whole process was fun.
[03:21:42.300 --> 03:21:47.740]   I don't think I've ever thought as hard about a book when I had to make a video about it.
[03:21:47.740 --> 03:21:51.780]   Because I had to like, you know, I read 1984, I don't know how many times, probably five,
[03:21:51.780 --> 03:21:54.620]   ten times, I don't remember.
[03:21:54.620 --> 03:21:57.820]   I read Animal Farm way more.
[03:21:57.820 --> 03:22:02.300]   But I don't think, I was like, what do I think about, what are the key takeaways for me?
[03:22:02.300 --> 03:22:03.300]   I didn't really know.
[03:22:03.300 --> 03:22:07.300]   Like if you ask me what I think about even Animal Farm, because I haven't done that one,
[03:22:07.300 --> 03:22:12.580]   and I've read that one, I don't know, over 50 times, it's probably my favorite book.
[03:22:12.580 --> 03:22:14.380]   I would have to struggle.
[03:22:14.380 --> 03:22:18.740]   And making a video about it, basically a little mini lecture, forced me to actually have an
[03:22:18.740 --> 03:22:23.940]   opinion about the details of it and to do enough research to think like, okay, what
[03:22:23.940 --> 03:22:25.580]   is the historical context of this book?
[03:22:25.580 --> 03:22:31.060]   I mean, it allowed me to say interesting and to think interesting stuff about the book.
[03:22:31.060 --> 03:22:36.820]   I found it to be really rewarding to basically, the old Feynman thing, one of the best ways
[03:22:36.820 --> 03:22:38.180]   to learn is to teach.
[03:22:38.180 --> 03:22:39.180]   Yeah.
[03:22:39.180 --> 03:22:42.140]   I can't think of one thing I would say about Animal Farm.
[03:22:42.140 --> 03:22:45.260]   And I read it, again, not that long ago, but I don't know what commentary I would have.
[03:22:45.260 --> 03:22:50.460]   You kind of have a generic comment about like authoritarianism and so on, whatever.
[03:22:50.460 --> 03:22:56.780]   But there could be interesting quirks of the book and the characters and how corruption
[03:22:56.780 --> 03:22:57.780]   happens.
[03:22:57.780 --> 03:23:01.580]   You could say all kinds of stuff that may be contrasting it.
[03:23:01.580 --> 03:23:09.500]   Like even when 1984 allowed me to contrast with Brave New World and how that 1984 was
[03:23:09.500 --> 03:23:13.180]   politicized and how it's used by the Republican Party of today.
[03:23:13.180 --> 03:23:16.860]   You could say a lot of interesting stuff if you think about it and write it down on a
[03:23:16.860 --> 03:23:17.860]   sheet of paper.
[03:23:17.860 --> 03:23:19.060]   Maybe you don't need to make a video about it.
[03:23:19.060 --> 03:23:22.460]   So I found it to be really rewarding in general.
[03:23:22.460 --> 03:23:27.140]   So I probably will do more of it, but not always, not as a main profession, just like
[03:23:27.140 --> 03:23:28.140]   with the travel blogging.
[03:23:28.140 --> 03:23:29.140]   I agree with that.
[03:23:29.140 --> 03:23:31.260]   I mean, you get threatened a lot.
[03:23:31.260 --> 03:23:32.780]   You get attacked a lot online.
[03:23:32.780 --> 03:23:34.940]   Do you think about your mortality?
[03:23:34.940 --> 03:23:39.420]   Well, the other day I went to the doctor and he said, "Next physical, we're going to be
[03:23:39.420 --> 03:23:42.420]   talking about a lot of new things."
[03:23:42.420 --> 03:23:44.860]   And so I was thinking about it a lot that day.
[03:23:44.860 --> 03:23:45.860]   Yeah.
[03:23:45.860 --> 03:23:48.380]   No, I mean, it's funny.
[03:23:48.380 --> 03:23:51.580]   I recently did a bunch of estate stuff.
[03:23:51.580 --> 03:23:57.300]   And when you have intellectual property, there's a question of like, okay, I have my assets,
[03:23:57.300 --> 03:24:04.960]   but also if I died tomorrow, especially in a particularly fiery death, my YouTube channel
[03:24:04.960 --> 03:24:08.940]   would probably for a while generate more money because it would be like, oh my goodness,
[03:24:08.940 --> 03:24:11.220]   this person died in a terrible...
[03:24:11.220 --> 03:24:13.980]   What happens with a future revenue stream and all these different things?
[03:24:13.980 --> 03:24:19.500]   And it got me thinking about legacy and about the fact that people who do this sort of thing,
[03:24:19.500 --> 03:24:22.060]   it's kind of a new thing in a sense.
[03:24:22.060 --> 03:24:26.300]   And so, you know, if you work at ABC News, at some point you just retire and someone
[03:24:26.300 --> 03:24:28.740]   else fills in for you.
[03:24:28.740 --> 03:24:31.380]   How does my career wind down given...
[03:24:31.380 --> 03:24:32.900]   I don't actually know the answer.
[03:24:32.900 --> 03:24:34.740]   I'm not sure.
[03:24:34.740 --> 03:24:35.740]   What is it?
[03:24:35.740 --> 03:24:41.700]   I just one day stop posting videos, but all my content stays up, getting fewer and fewer
[03:24:41.700 --> 03:24:44.420]   views or do I delete everything?
[03:24:44.420 --> 03:24:46.300]   I don't know.
[03:24:46.300 --> 03:24:47.300]   What is the legacy?
[03:24:47.300 --> 03:24:50.980]   - And if you die, I mean, so my trip to Ukraine, 'cause I knew I was going to the front, is
[03:24:50.980 --> 03:24:56.860]   the first time I did, I recorded a video if I die and I posted it and I gave instructions
[03:24:56.860 --> 03:25:00.340]   to folks what to do.
[03:25:00.340 --> 03:25:02.900]   So like there's a closure.
[03:25:02.900 --> 03:25:04.060]   But it's an interesting process.
[03:25:04.060 --> 03:25:06.900]   Like what happens to your...
[03:25:06.900 --> 03:25:10.900]   At which point does GPT take over and continue tweeting for David Pakman?
[03:25:10.900 --> 03:25:15.100]   - Well, the tweeting I care less about right now, unless blue becomes something unbelievable,
[03:25:15.100 --> 03:25:16.940]   I'm less worried about Twitter.
[03:25:16.940 --> 03:25:20.780]   But some of my audience members have been saying, you know, some of these tools, David,
[03:25:20.780 --> 03:25:29.020]   are getting good enough that we could clone your voice and also make it match video.
[03:25:29.020 --> 03:25:33.420]   And with scripts, you could just keep pumping out content even if you were gone.
[03:25:33.420 --> 03:25:36.700]   And I said, now that I'm interested in, that I want to learn more about.
[03:25:36.700 --> 03:25:40.540]   - Boy, it's going to be a weird future.
[03:25:40.540 --> 03:25:46.140]   What advice do you have to young folks that are facing this future?
[03:25:46.140 --> 03:25:50.540]   - Almost always, it's some version of start right away.
[03:25:50.540 --> 03:25:52.940]   And that applies in so many different ways.
[03:25:52.940 --> 03:25:55.060]   So if you're thinking about...
[03:25:55.060 --> 03:25:57.720]   Oftentimes the context is people want to do what I do.
[03:25:57.720 --> 03:26:03.220]   And I always say, do not sit around for a year thinking about lighting.
[03:26:03.220 --> 03:26:04.580]   This is how you never do anything.
[03:26:04.580 --> 03:26:09.620]   And I, dozens of people who I felt obligated to talk to on the phone because of a personal
[03:26:09.620 --> 03:26:14.900]   connection, I go through all the advice and I can tell they're not going to do it.
[03:26:14.900 --> 03:26:17.820]   They just, it's already sounding too complicated.
[03:26:17.820 --> 03:26:20.980]   And so instead they'll sort of say, well, I got to get the right lighting in the right
[03:26:20.980 --> 03:26:22.700]   room and blah, blah, blah, blah.
[03:26:22.700 --> 03:26:26.020]   The best thing you can do no matter what you're doing is just start right away.
[03:26:26.020 --> 03:26:29.100]   And that applies in this business and in whatever else you're doing.
[03:26:29.100 --> 03:26:34.820]   If you want to learn a new thing, find a new hobby, the ability to get data right away
[03:26:34.820 --> 03:26:38.380]   about what's working, what's not working, and whether you even like this approach that
[03:26:38.380 --> 03:26:42.540]   you're taking is so valuable and it will allow you to iterate.
[03:26:42.540 --> 03:26:47.380]   And the sooner you do it, the cost to a change of direction will also be lower.
[03:26:47.380 --> 03:26:51.100]   If there's any, I don't do like self-help or generic advice type stuff, but the one
[03:26:51.100 --> 03:26:57.860]   thing that applies in so many situations is just try it right away and iterate from there.
[03:26:57.860 --> 03:26:58.860]   Yeah.
[03:26:58.860 --> 03:27:01.820]   Start today and then do it every day.
[03:27:01.820 --> 03:27:03.020]   Or decide, hey, you know what?
[03:27:03.020 --> 03:27:05.020]   I figured out I don't actually want to do it.
[03:27:05.020 --> 03:27:06.020]   Yeah.
[03:27:06.020 --> 03:27:07.020]   Hence iterate.
[03:27:07.020 --> 03:27:08.020]   Yeah.
[03:27:08.020 --> 03:27:12.500]   And usually you'll discover you do.
[03:27:12.500 --> 03:27:16.380]   You think we'll make it out of the century, humanity, human civilization?
[03:27:16.380 --> 03:27:18.140]   Out of the century, so like to 2100?
[03:27:18.140 --> 03:27:19.140]   Yeah.
[03:27:19.140 --> 03:27:20.140]   How much we got, 80 years?
[03:27:20.140 --> 03:27:21.140]   Yeah.
[03:27:21.140 --> 03:27:22.140]   77.
[03:27:22.140 --> 03:27:24.700]   I think we're going to make it.
[03:27:24.700 --> 03:27:25.700]   You think so?
[03:27:25.700 --> 03:27:26.700]   Yeah.
[03:27:26.700 --> 03:27:29.380]   What are the biggest threats facing our civilization?
[03:27:29.380 --> 03:27:31.180]   If not wokeism.
[03:27:31.180 --> 03:27:35.500]   If it's not wokeism, it's hard to say.
[03:27:35.500 --> 03:27:44.460]   I actually think that if you believe that we are on an inflection point of sorts in
[03:27:44.460 --> 03:27:50.740]   changes to society and acceleration of technology, et cetera, I think it's really tough to know
[03:27:50.740 --> 03:27:54.660]   in 2090 what will actually be the biggest threat.
[03:27:54.660 --> 03:27:55.660]   So I don't know.
[03:27:55.660 --> 03:28:02.620]   It's so cliche to say nuclear and climate change and another pandemic.
[03:28:02.620 --> 03:28:06.980]   That world might look so different that it's almost unimaginable.
[03:28:06.980 --> 03:28:09.580]   What it means to be human is unimaginable.
[03:28:09.580 --> 03:28:16.140]   And also the degree that we make progress out into space is also unimaginable.
[03:28:16.140 --> 03:28:20.360]   I think space is super interesting and there's people on both the left and right who for
[03:28:20.360 --> 03:28:24.580]   different reasons are kind of not into the whole space exploration thing.
[03:28:24.580 --> 03:28:29.140]   The people I hear from, the ones on the right think it's just kind of dumb.
[03:28:29.140 --> 03:28:33.380]   The ones on the left think it's an excuse not to fix problems here.
[03:28:33.380 --> 03:28:34.380]   I think you-
[03:28:34.380 --> 03:28:36.820]   And also they say it's the play thing of billionaires.
[03:28:36.820 --> 03:28:37.820]   Sure.
[03:28:37.820 --> 03:28:39.460]   Which is another funny kind of concept.
[03:28:39.460 --> 03:28:40.460]   Yeah.
[03:28:40.460 --> 03:28:43.700]   I mean, someone's got to pay for it.
[03:28:43.700 --> 03:28:45.300]   Why not be people who have a lot of money to-
[03:28:45.300 --> 03:28:50.540]   You could either be billionaires or governments that are trillionaires.
[03:28:50.540 --> 03:28:54.340]   Somebody has to pay for big ambitious moonshot projects.
[03:28:54.340 --> 03:29:00.740]   To me, the most interesting thing is that in getting closer to the next step of space
[03:29:00.740 --> 03:29:07.540]   exploration, we may well learn things that can then be used to improve circumstances
[03:29:07.540 --> 03:29:08.540]   here.
[03:29:08.540 --> 03:29:09.540]   For me, it's not one or the other.
[03:29:09.540 --> 03:29:14.620]   And I recently read a long piece about why, why not Mars?
[03:29:14.620 --> 03:29:17.940]   Because it's terrible in every way for supporting life.
[03:29:17.940 --> 03:29:18.940]   Okay.
[03:29:18.940 --> 03:29:24.180]   So like that's one perspective, but still in so exploring, who knows what we might end
[03:29:24.180 --> 03:29:25.180]   up learning.
[03:29:25.180 --> 03:29:26.180]   So I'm big on it.
[03:29:26.180 --> 03:29:29.100]   I don't share the view of some on the left about it.
[03:29:29.100 --> 03:29:33.060]   So I guess to add to your advice to young people, if a thing seems terrible, you still
[03:29:33.060 --> 03:29:34.300]   might want to consider doing it.
[03:29:34.300 --> 03:29:35.660]   I would say so.
[03:29:35.660 --> 03:29:36.660]   Yeah.
[03:29:36.660 --> 03:29:37.660]   How many things I've seen?
[03:29:37.660 --> 03:29:46.140]   I mean, listen, there are so many trips where the day before I say, why am I doing this?
[03:29:46.140 --> 03:29:49.260]   The jet lag, I've got to do this and that.
[03:29:49.260 --> 03:29:53.820]   And now who, if my guest host falls through, I should just stay and work.
[03:29:53.820 --> 03:29:55.780]   And I go, hold on.
[03:29:55.780 --> 03:29:57.460]   You do this every time.
[03:29:57.460 --> 03:29:58.460]   Just go.
[03:29:58.460 --> 03:29:59.460]   You never regret it.
[03:29:59.460 --> 03:30:00.860]   You learn something, you try something.
[03:30:00.860 --> 03:30:04.820]   I never regret the trip.
[03:30:04.820 --> 03:30:07.540]   This hopefully applies to the conversation we had today.
[03:30:07.540 --> 03:30:09.820]   David, I'm a big fan of yours.
[03:30:09.820 --> 03:30:12.420]   Thank you so much for talking today.
[03:30:12.420 --> 03:30:13.580]   Thank you for being patient with me.
[03:30:13.580 --> 03:30:15.900]   We tried to talk earlier.
[03:30:15.900 --> 03:30:17.300]   Please continue doing what you're doing.
[03:30:17.300 --> 03:30:22.380]   Please continue being objective and thoughtful and fearless on the internet.
[03:30:22.380 --> 03:30:23.380]   Thank you.
[03:30:23.380 --> 03:30:24.380]   I'm a big fan of yours as well.
[03:30:24.380 --> 03:30:25.380]   I appreciate it.
[03:30:25.380 --> 03:30:28.380]   Thanks for listening to this conversation with David Pakman.
[03:30:28.380 --> 03:30:32.660]   To support this podcast, please check out our sponsors in the description.
[03:30:32.660 --> 03:30:36.700]   And now let me leave you with some words from Mahatma Gandhi.
[03:30:36.700 --> 03:30:42.140]   What difference does it make to the dead, the orphans and the homeless, whether the
[03:30:42.140 --> 03:30:47.700]   mad destruction is wrought under the name of totalitarianism or in the holy name of
[03:30:47.700 --> 03:30:50.900]   liberty or democracy?
[03:30:50.900 --> 03:30:53.980]   Thank you for listening and hope to see you next time.
[03:30:53.980 --> 03:31:03.980]   [END]

