
[00:00:00.000 --> 00:00:06.840]   What percentage of your time spent using technology is dedicated to services that make you feel worse?
[00:00:06.840 --> 00:00:13.560]   Why do we end up entangled with tools that don't seem to have our best interest in mind?
[00:00:13.560 --> 00:00:19.360]   How do we escape this fate and take back control of our attention and reclaim our lives?
[00:00:19.360 --> 00:00:22.640]   Today, we're going to tackle these issues.
[00:00:22.640 --> 00:00:26.440]   I'm Cal Newport, and this is Deep Questions.
[00:00:26.960 --> 00:00:30.740]   The show about cultivating a deep life in a distracted world.
[00:00:30.740 --> 00:00:36.000]   Today's episode, what technology wants and how to push back.
[00:00:36.000 --> 00:00:51.780]   Part one, a tale of two technologies.
[00:00:53.360 --> 00:01:00.620]   So, Jesse, I don't know if you noticed this, but several people sent us this same Substack post recently.
[00:01:00.620 --> 00:01:02.700]   It had an arresting photo.
[00:01:02.700 --> 00:01:05.920]   I'm going to put this up on the screen for people who are watching instead of just listening.
[00:01:05.920 --> 00:01:08.860]   You see in this photo something you don't normally see.
[00:01:08.860 --> 00:01:15.340]   The young kid laying on the couch talking on an old-fashioned hardwired landline.
[00:01:15.340 --> 00:01:18.180]   The whole curly wire and everything.
[00:01:19.060 --> 00:01:24.260]   Here's what the author of this essay, Priscilla Harvey, wrote.
[00:01:24.260 --> 00:01:31.200]   I bought my kids an old-fashioned phone to keep smartphones out of their hands while still letting them chat with friends.
[00:01:31.200 --> 00:01:34.460]   But it has turned into the sweetest, most unexpected surprise.
[00:01:34.460 --> 00:01:39.020]   My son's new daily conversation with his grandmothers.
[00:01:39.580 --> 00:01:45.880]   This is how I keep finding him, stretched out on the couch, phone pressed to his ear, intently focused and listening to the matriarchs of our family.
[00:01:45.880 --> 00:01:50.320]   I hear laughter and sense a deeper connection blossoming in these conversations.
[00:01:50.320 --> 00:01:54.480]   There's no scrolling, no distractions, no comparisons, no dopamine hits to chase.
[00:01:54.480 --> 00:02:02.580]   Instead, he is just listening to stories, asking questions, and having the comfort of knowing someone who loves him is listening on the other end of the line.
[00:02:03.300 --> 00:02:13.060]   Now, clearly, Harvey wrote this article to make a point about kids and phones.
[00:02:13.060 --> 00:02:16.700]   That's an important point and what I talk about often on this show.
[00:02:16.700 --> 00:02:25.820]   But embedded in here is an even deeper idea, an idea that I think is fundamental to our broken relationship with technology.
[00:02:25.820 --> 00:02:30.600]   I want you to think about the two phones that are referenced in this article.
[00:02:30.600 --> 00:02:47.960]   The telephone, in that original hard plastic curly wired form, is an example of what we might call an additive technology because its goal is to take something you value, like talking to people you know, and make this activity easier and more accessible in your life.
[00:02:47.960 --> 00:02:48.820]   You want to talk to your grandmother?
[00:02:48.820 --> 00:02:50.740]   Type in some numbers.
[00:02:50.740 --> 00:02:51.940]   There's your voice.
[00:02:51.940 --> 00:02:54.900]   Clear in your ear as if you were right next door.
[00:02:54.900 --> 00:03:01.080]   So the phone, in its original form, seeks strictly to add value to your life.
[00:03:01.080 --> 00:03:07.200]   Now, let's compare this to something modern, like the Instagram app you might be using on a smartphone.
[00:03:07.200 --> 00:03:09.900]   Now, here, the value proposition becomes suddenly more muddled.
[00:03:09.900 --> 00:03:12.920]   I mean, there might be aspects of this app you enjoy.
[00:03:12.920 --> 00:03:17.020]   It can be distracting or occasionally you find out something interesting going on with your friend.
[00:03:17.020 --> 00:03:20.380]   But those joys come with endless sorrows as well.
[00:03:20.380 --> 00:03:22.520]   The scrolling can become addictive.
[00:03:22.920 --> 00:03:30.680]   The content can devolve into one of those digital slurries, equal parts mind-nummy and anxiety-producing.
[00:03:31.100 --> 00:03:43.280]   So unlike the straightforward benefits of something like an old-fashioned landline, become clear that a tool like Instagram on a new smartphone doesn't have your best interest as its primary goal.
[00:03:44.000 --> 00:03:45.180]   It's using you.
[00:03:45.180 --> 00:03:53.240]   It's making itself just compelling enough that you'll pick it up, at which point it can monetize every last ounce of your time and your data.
[00:03:53.240 --> 00:04:03.600]   Because of this, I would call the modern smartphone with Instagram an extractive technology, as it seeks to extract value from you instead of trying to provide it.
[00:04:03.600 --> 00:04:19.780]   Now, this tension between additive and extractive technology, how we move away from the former, how we get entangled in the latter and what we can do about it, this is the core of what I want to talk to you today about.
[00:04:19.780 --> 00:04:33.620]   And I think a good place to start this conversation is in trying to understand why, if extractive technologies can be so unpleasant or cruel to us, why do we end up using them at all?
[00:04:34.620 --> 00:04:38.420]   To answer this question, I think a good place to start is taking a step back in time.
[00:04:38.420 --> 00:04:43.060]   This brings us to part two, thefacebook.com.
[00:04:43.060 --> 00:04:49.820]   Jesse, do you remember, it's a question that Gen Z would not be able to answer, us millennials might be able to.
[00:04:49.820 --> 00:04:51.740]   Do you remember first encountering Facebook?
[00:04:51.740 --> 00:04:53.340]   Yes.
[00:04:53.340 --> 00:04:54.760]   Was it like three weeks ago?
[00:04:54.760 --> 00:04:57.400]   Let's be honest, you're like, what is this?
[00:04:57.400 --> 00:05:00.240]   There's faces on this device.
[00:05:00.240 --> 00:05:01.840]   What magic is this?
[00:05:01.840 --> 00:05:04.600]   What do you remember about it when you first, this would have been like,
[00:05:04.600 --> 00:05:06.520]   college right after college, right?
[00:05:06.520 --> 00:05:08.040]   Yeah, basically senior year.
[00:05:08.040 --> 00:05:13.600]   Yeah, so like, do you remember what people were attracted to?
[00:05:13.600 --> 00:05:18.800]   I remember a lot of people were getting accounts and me and a couple of my buddies were pretty late to the game.
[00:05:18.800 --> 00:05:22.360]   I was late to the game too, sort of famously, but yeah, that's my memory as well.
[00:05:22.360 --> 00:05:23.960]   We're the same age, senior year of college.
[00:05:23.960 --> 00:05:26.240]   It was called thefacebook.com back then.
[00:05:26.240 --> 00:05:30.180]   People were delighted by it, was my memory, right?
[00:05:30.180 --> 00:05:34.400]   Here's what I'm trying to remember here, but here's what I think people cared about.
[00:05:34.400 --> 00:05:37.640]   Looking up boyfriend and girlfriends from like high school.
[00:05:37.640 --> 00:05:39.060]   Like, what are they up to?
[00:05:39.060 --> 00:05:41.060]   Relationship status.
[00:05:41.060 --> 00:05:42.020]   Remember that?
[00:05:42.020 --> 00:05:42.440]   Yeah.
[00:05:42.440 --> 00:05:43.080]   That was critical.
[00:05:43.080 --> 00:05:46.500]   Like, is my old girlfriend in a relationship now?
[00:05:46.500 --> 00:05:50.840]   Or like the guy down the hall, like what's going on with, like that was really critical information.
[00:05:51.240 --> 00:05:57.000]   And then as it evolved over the next couple of years and more people got on it, you're like, oh, like I might know what like my cousin's up to.
[00:05:57.000 --> 00:06:05.820]   Like they might post something on my wall, like I'm going to be in town or my uncle will post a picture of my nephew.
[00:06:06.060 --> 00:06:11.860]   So it was strictly sort of additive and useful and kind of fun.
[00:06:11.860 --> 00:06:14.660]   It wasn't a great, you know, it's kind of a simple old website, but it was fun.
[00:06:14.660 --> 00:06:18.040]   Then it got worse, right?
[00:06:18.040 --> 00:06:22.140]   If we fast forward to today, who feels delight for Facebook?
[00:06:22.140 --> 00:06:27.040]   Like who's walking around being like, hey, let's do some Facebook and someone, hey, let's high five.
[00:06:27.040 --> 00:06:27.760]   And then everyone cheers.
[00:06:27.760 --> 00:06:30.320]   Like no one is really super happy about Facebook anymore.
[00:06:30.720 --> 00:06:41.720]   So what happened here, we can get some good insight by reading a few excerpts from a recent court appearance by Mark Zuckerberg.
[00:06:41.720 --> 00:06:46.380]   So Mark Zuckerberg had to go under oath back in April for an antitrust suit.
[00:06:46.380 --> 00:06:51.040]   And while under oath, they asked him questions about the evolution of Facebook.
[00:06:51.040 --> 00:06:57.660]   And if we hear some key quotes about this testimony, we begin to get an idea about what happened to Facebook.
[00:06:57.660 --> 00:07:00.600]   Why do people no longer feel delighted by it?
[00:07:00.600 --> 00:07:01.800]   So I want to read here.
[00:07:01.800 --> 00:07:05.500]   This is a quote from an article that appeared in Yahoo News.
[00:07:05.500 --> 00:07:10.640]   As time change, so do mission statements, especially in the fast and loose world of tech.
[00:07:10.640 --> 00:07:19.560]   In recent months, we've seen Google walk back its pledge to do no evil and open AI, quietly delete a policy prohibiting its software use for military technology.
[00:07:19.560 --> 00:07:21.780]   Mark Zuckerberg's Facebook is no exception.
[00:07:22.620 --> 00:07:29.580]   It's 2008 motto, Facebook helps you connect and share with the people in your life, is now a distant memory.
[00:07:29.580 --> 00:07:35.860]   According to Zuckerberg himself, who testified this week that Facebook's main purpose, quote,
[00:07:35.860 --> 00:07:39.320]   wasn't really to connect with friends anymore, end quote.
[00:07:39.320 --> 00:07:43.540]   The friend part has gone down quite a bit, Zuckerberg said.
[00:07:44.320 --> 00:07:46.480]   All right, here's another quote about this trial.
[00:07:46.480 --> 00:07:50.300]   This comes from a recent Vanity Fair article, which reported,
[00:07:50.300 --> 00:08:01.080]   Zuckerberg testified that just 20% of the content people consume on Facebook and 10% of the content they see on Instagram is from their friends.
[00:08:02.400 --> 00:08:04.020]   Now, this is very different.
[00:08:04.020 --> 00:08:11.600]   Original Facebook, the whole point, as was in that motto from 2008, was to see what was going on with people you knew or your friends,
[00:08:11.600 --> 00:08:19.180]   to be connected to their life or to sort of, in a mild, mild sort of amusing way, cyber stalk someone you used to know and see what they're up to.
[00:08:19.180 --> 00:08:20.260]   That was what they offered.
[00:08:20.260 --> 00:08:24.800]   Fast forward to today when Mark Zuckerberg is testifying and he said 20% of the stuff you see is from friends.
[00:08:24.920 --> 00:08:32.880]   Everything else is something else, selected by algorithms, stuff that you may or may not like, stuff that might be interesting or stuff that might push your buttons.
[00:08:32.880 --> 00:08:37.620]   There's a very simple reason why they made this shift.
[00:08:37.620 --> 00:08:49.580]   One way to get there is to jump back a decade and hear when a former Facebook founder, Sean Parker, famously played in the social network by Justin Timberlake,
[00:08:49.660 --> 00:08:55.320]   that's how probably people know him, gave a talk at Stanford and he said something I want to play right now.
[00:08:55.320 --> 00:09:05.680]   Thought process that went into building these applications, Facebook being the first of them to really understand it.
[00:09:05.680 --> 00:09:14.120]   That thought process was all about how do we consume as much of your time and conscious attention as possible.
[00:09:15.120 --> 00:09:18.620]   That was a shift from how Facebook started.
[00:09:18.620 --> 00:09:27.040]   When Jesse and I were first encountering the Facebook.com in our senior year of college, its entire goal was to get as many users as possible.
[00:09:27.040 --> 00:09:29.020]   And it did that by being as useful as possible.
[00:09:29.020 --> 00:09:33.900]   That's why most people I know in my generation signed up for Facebook, because it was useful.
[00:09:33.900 --> 00:09:42.360]   By 2017, when Parker was giving that speech, they had shifted towards another strategy, which was how do we get people to use this as much as possible?
[00:09:42.980 --> 00:09:46.100]   How do we get as much as possible of their time and their data?
[00:09:46.100 --> 00:09:47.960]   The switch here is an obvious one.
[00:09:47.960 --> 00:09:49.880]   They needed to start making money.
[00:09:49.880 --> 00:09:52.640]   And this is when the experience of Facebook shifted.
[00:09:52.640 --> 00:09:55.520]   Why we want to show you algorithmic content?
[00:09:55.520 --> 00:09:58.160]   Because you're going to stay on the app more.
[00:09:58.160 --> 00:10:01.400]   More people are going to produce content for it to make money off of you.
[00:10:01.400 --> 00:10:02.540]   We can get more data from you.
[00:10:02.540 --> 00:10:03.840]   We can put more ads into it.
[00:10:03.840 --> 00:10:05.120]   We can make more money off of you.
[00:10:05.420 --> 00:10:18.060]   So Facebook became an extractive technology later, after so many people had already began to use it and began to find it to be something that was indispensable to their life.
[00:10:18.060 --> 00:10:20.360]   There's a theory for this process.
[00:10:20.360 --> 00:10:22.040]   I'm not the first one to notice this.
[00:10:22.040 --> 00:10:24.780]   There's a now sort of famous blog post.
[00:10:24.860 --> 00:10:27.360]   I'll load it on the screen here for people who are watching.
[00:10:27.360 --> 00:10:34.080]   It came from Corey Doctrow, the novelist, nonfiction writer, the essayist, the critical thinker.
[00:10:34.860 --> 00:10:44.140]   On his blog, Pluralistic, back in 2022, he put out this now famous essay that's titled, How Monopoly Enshittified Amazon.
[00:10:44.140 --> 00:10:58.720]   Now, this is a big article about a lot of issues that happen with Amazon, about how Amazon made a shift to make the experience sort of worse for users with ads and worse for sellers by adding all these extra fees on top of them and making the whole experience more expensive.
[00:10:58.720 --> 00:11:02.420]   He was talking about a similar sort of shift that we saw with Facebook.
[00:11:02.420 --> 00:11:10.780]   It started as a sort of utopian place for users who could get books sent right to their door and for sellers who could sell to anyone at low cost.
[00:11:10.780 --> 00:11:18.500]   And then it became worse for everyone except for the shareholders of Amazon, where it started making them more money.
[00:11:18.500 --> 00:11:21.760]   So he called this process cleverly enshittification.
[00:11:21.760 --> 00:11:25.400]   And he said this is something that's somewhat inevitable in the internet digital economy.
[00:11:25.400 --> 00:11:30.980]   You bring in the users, then you make it worse to extract money from them.
[00:11:30.980 --> 00:11:34.860]   I want to read you a quote from this article where he's sort of summarizing this idea.
[00:11:34.860 --> 00:11:45.220]   He says, entertainment and tech companies all want to be the only game in town for their creative labor force because that lets them turn the screws on those workers, moving value from labor to shareholders.
[00:11:45.840 --> 00:11:51.960]   Now, here's the important part for a lot of new consumer facing digital technology like the Instagram example we just talked about.
[00:11:51.960 --> 00:11:54.220]   You are the creative labor force.
[00:11:54.220 --> 00:11:55.900]   You are who they're talking about.
[00:11:55.900 --> 00:12:04.100]   You are who create for free all the content that they use to get other people to look at it and to make money off of them as well as make money off of you.
[00:12:04.100 --> 00:12:11.460]   You are the target that is having the screws turned into by these companies once they already have you hooked in to their services.
[00:12:12.460 --> 00:12:20.320]   So, though Dr. Rao didn't use my exact terms, enshittification helps explain well why we get so tangled in extractive technologies.
[00:12:20.320 --> 00:12:22.700]   When we first started using them, they were additive.
[00:12:22.700 --> 00:12:25.620]   And then they get worse.
[00:12:25.620 --> 00:12:26.900]   They become extractive.
[00:12:26.900 --> 00:12:28.740]   And again and again, we see that happen.
[00:12:29.220 --> 00:12:39.260]   A lot of our modern digital consumer tools are like a rotary phone that hooks you on ketamine and then forces you to dial 1-900 numbers to get your next fix.
[00:12:39.260 --> 00:12:42.440]   Except I guess with like more emojis or something like that.
[00:12:42.440 --> 00:12:44.320]   I think it's a pretty good metaphor, don't you think?
[00:12:44.320 --> 00:12:44.820]   Good analogy.
[00:12:44.820 --> 00:12:45.500]   Yeah.
[00:12:45.500 --> 00:12:50.700]   So we have a good general explanation for why extractive technologies end up in our lives.
[00:12:50.700 --> 00:12:53.000]   But this motivates a follow-up question.
[00:12:53.000 --> 00:12:56.400]   How do companies get away with this?
[00:12:56.820 --> 00:13:01.040]   Like why do we stick around if these products suddenly get much worse?
[00:13:01.040 --> 00:13:05.460]   This brings us to part three of this story, the road to exploitation.
[00:13:05.460 --> 00:13:12.540]   So as I've been looking closer at this question of the transition from additive to extractive technologies in the digital technology space,
[00:13:12.540 --> 00:13:20.540]   two strategies emerged as being relatively primary for the creators of these technologies to use to make this shift succeed.
[00:13:21.280 --> 00:13:27.120]   In other words, to minimize the people who leave when their product gets more extractive.
[00:13:27.120 --> 00:13:30.100]   The first and obvious one is slowness.
[00:13:30.100 --> 00:13:41.840]   There wasn't some point where Facebook was all friends and relationship statuses and the next day was 80% Fox News articles that was being sent you by an algorithm.
[00:13:42.200 --> 00:13:44.120]   That happened way more slowly.
[00:13:44.120 --> 00:13:46.800]   I went back at some point and looked through this whole timeline.
[00:13:46.800 --> 00:13:48.620]   There's a bunch of things that were added.
[00:13:48.620 --> 00:13:54.860]   Like the first part, actually the first step towards extractive technology was not about adding new content.
[00:13:54.860 --> 00:13:57.420]   It was about adding new social indicators to content.
[00:13:57.420 --> 00:14:00.400]   Sean Barker talks about that in that 2017 speech.
[00:14:00.540 --> 00:14:02.400]   The first move was to add likes.
[00:14:02.400 --> 00:14:09.180]   And now suddenly you could get this reinforced, intermittent reinforcement.
[00:14:09.180 --> 00:14:12.460]   Oh, I wonder how many likes the thing I just got, I just posted got.
[00:14:12.460 --> 00:14:13.700]   Was it popular?
[00:14:13.700 --> 00:14:14.240]   Was it not?
[00:14:14.240 --> 00:14:14.900]   Are people upset?
[00:14:14.900 --> 00:14:16.700]   And that got people back into the app.
[00:14:16.700 --> 00:14:18.640]   So once they moved it to the app, they added likes.
[00:14:18.640 --> 00:14:20.780]   And now people were using the app a lot more.
[00:14:20.780 --> 00:14:22.900]   This is sort of where they got their first fix.
[00:14:22.900 --> 00:14:28.720]   It's like they took those extra opioids after the back surgery and began looking around for fentanyl after they couldn't find any more.
[00:14:28.720 --> 00:14:31.940]   They got their fix with the like buttons and people pressing the app button more.
[00:14:31.940 --> 00:14:34.240]   And then they started working on the content itself.
[00:14:34.240 --> 00:14:35.680]   Then they started realizing, you know what?
[00:14:35.680 --> 00:14:39.080]   People, for the most part, the stuff they're posting is not that interesting.
[00:14:39.080 --> 00:14:43.740]   So if your feed is mainly the stuff that your friends are posting, that's not a super fascinating feed.
[00:14:43.740 --> 00:14:46.360]   You'll still come back to see if they like your stuff, but only so often.
[00:14:46.360 --> 00:14:52.040]   So let's put stuff in there that's going to be more interesting to you so that once you're in here, you go slack jawed.
[00:14:52.040 --> 00:14:58.100]   And the drool begins to pool as you swipe from one thing to the other that's pressing whatever buttons the algorithm learns actually works.
[00:14:58.100 --> 00:15:07.760]   And so that's how they ended up where we get today, where only 20% of Facebook and 10% of Instagram content even has anything to do with the people you know and follow and actually care about.
[00:15:07.760 --> 00:15:09.240]   So they go slow.
[00:15:09.240 --> 00:15:12.500]   And then by the time you look up, you don't realize what is happening.
[00:15:12.500 --> 00:15:19.500]   This is why, like, when I really started yelling about these issues in 2018 or 2019, like, hey, social media is a problem.
[00:15:19.500 --> 00:15:21.040]   You got to be wary about it.
[00:15:21.560 --> 00:15:23.100]   People were kind of too far gone.
[00:15:23.100 --> 00:15:25.600]   It'd been a half decade of them using these things.
[00:15:25.600 --> 00:15:26.960]   It'd become a background of the culture.
[00:15:26.960 --> 00:15:35.020]   And instead, what we got was the only natural reaction, especially coming out of the media and other types of people, was just like, well, how do we, like, fix these?
[00:15:35.020 --> 00:15:35.920]   How do we make them better?
[00:15:35.920 --> 00:15:37.520]   How do we hold the leaders to account?
[00:15:37.680 --> 00:15:43.360]   The very idea that you would just stop using Facebook or Twitter or Instagram at the time wasn't even on the table.
[00:15:43.360 --> 00:15:53.460]   It had been around long enough, had got enough people using it, made it enough of the background conversation that getting rid of these tools would seem like back in the 1950s, getting rid of your landline phone.
[00:15:53.460 --> 00:15:55.440]   People weren't even considering it.
[00:15:56.160 --> 00:15:59.040]   What's the second strategy that they use to get you to stick around?
[00:15:59.040 --> 00:16:01.760]   Well, this one is even more insidious.
[00:16:01.760 --> 00:16:10.160]   And I want to go back to Corey Doctrow, who talked about this strategy in a talk he gave earlier this year at CloudFest.
[00:16:10.160 --> 00:16:18.920]   I actually have two short clips I want to play with Corey talking about this second strategy for keeping us around as technology to become more extractive.
[00:16:18.920 --> 00:16:22.560]   So, Jesse, let's play both those clips more or less back to back here.
[00:16:23.060 --> 00:16:26.540]   It is a Pareto-optimal decision.
[00:16:26.540 --> 00:16:33.520]   Markets don't discipline tech companies because tech companies don't compete with their rivals.
[00:16:33.520 --> 00:16:35.260]   They buy their rivals.
[00:16:35.260 --> 00:16:43.260]   Now, that's a quote from Mark Zuckerberg, who famously said, it is better to buy than to compete, which is why Mark...
[00:16:43.260 --> 00:16:44.500]   That's clip one.
[00:16:44.500 --> 00:16:46.160]   We've got a second clip here.
[00:16:49.640 --> 00:16:53.680]   Because a sector with 100 SMEs is a rabble.
[00:16:53.680 --> 00:16:55.160]   They are at each other's throat.
[00:16:55.160 --> 00:16:56.760]   They can't agree on anything.
[00:16:56.760 --> 00:16:58.360]   They can't agree on how to lobby.
[00:16:58.360 --> 00:17:03.680]   But when a sector turns into five companies, it is a conspiracy in waiting.
[00:17:03.680 --> 00:17:09.940]   A sector that has been boiled down to a mere handful of firms can agree on a common lobbying position.
[00:17:10.280 --> 00:17:22.720]   And they are so insulated from what Peter Thiel calls "wasteful competition" that they are a slosh in cash, and they mobilize that cash to turn their regulatory priorities into regulations.
[00:17:23.600 --> 00:17:26.640]   So what we see here is an even more advanced strategy.
[00:17:26.640 --> 00:17:28.480]   They reduce competition.
[00:17:28.480 --> 00:17:36.080]   So especially when we're talking about like the big tech companies who are producing the things that are most capturing our attention, particularly on our phones and smart screens.
[00:17:37.160 --> 00:17:38.760]   They want the competition to be small.
[00:17:38.760 --> 00:17:39.920]   They don't want to be competing.
[00:17:39.920 --> 00:17:50.400]   If you are in a large competitive marketplace where there's a hundred different apps trying to help you connect to your friends, you'll just say, hey, this one is in shitifying.
[00:17:50.400 --> 00:17:51.680]   I'll go use this other one.
[00:17:51.680 --> 00:17:57.040]   Like, why do I want to use this service that's showing me all these Fox News articles that I didn't ask for?
[00:17:57.040 --> 00:17:59.060]   I want to go to the one where I can actually see what my friend is doing.
[00:17:59.140 --> 00:18:05.860]   But when there's not competition, you just assume, I guess, this is like more or less the best I'm going to get, and these companies were very good at that.
[00:18:05.860 --> 00:18:16.300]   As Dr. Rell put in his second clip there, this also allows them to avoid the types of regulations that might actually constrain some of the worst externality.
[00:18:16.300 --> 00:18:23.820]   So some of the effects that make them the most money that have the least value to users, typically a part where regulation can come in and fight on behalf of the users.
[00:18:24.300 --> 00:18:33.520]   But in this monopolistic marketplace, they have tight, tight-focused lobbying targets, and they can prevent that from happening.
[00:18:33.520 --> 00:18:37.880]   I have another example of someone talking about exactly these effects.
[00:18:37.880 --> 00:18:46.480]   This is a clip that comes from Ed Zitron, who has a fantastic technology criticism podcast called Better Offline.
[00:18:46.480 --> 00:18:51.520]   This is an episode where he was talking about Google and why it got worse.
[00:18:51.520 --> 00:18:54.200]   It's an episode that he won a Webby Award for.
[00:18:54.200 --> 00:18:56.280]   So this was an episode that he just won an award for.
[00:18:56.280 --> 00:18:58.400]   Anyways, I want to play you a quick clip from this.
[00:18:58.400 --> 00:18:59.680]   That doesn't matter to Google.
[00:18:59.680 --> 00:19:01.740]   Google can now say they've got AI in their shit.
[00:19:01.740 --> 00:19:03.760]   Google Search, they can fix it.
[00:19:03.760 --> 00:19:10.320]   I believe that Google, with all of their money and all of the very smart people that work there and have worked there, could fight SEO.
[00:19:10.320 --> 00:19:12.960]   They could find a way to rank things properly.
[00:19:12.960 --> 00:19:18.000]   I just don't think they care because they're doing $10-$20 billion in profit every quarter.
[00:19:18.620 --> 00:19:35.520]   So what he's arguing there is search results have gotten worse on Google, and he gets really into the details in that episode, in part because there's a ton of ads, in part because they have this sort of terrible AI-generated things up front, but in part because SEO or search engine optimization puts a lot of bad results up front that don't really help you.
[00:19:36.220 --> 00:19:39.500]   Deuteron does the research in this episode and says, they could fix that.
[00:19:39.500 --> 00:19:41.160]   They could fix that.
[00:19:41.160 --> 00:19:42.200]   But why?
[00:19:42.200 --> 00:19:48.400]   Having more results and bad results, you have to scroll a few more screens to get to the results you're looking for just to be sure.
[00:19:48.400 --> 00:19:50.280]   Strictly more money.
[00:19:50.280 --> 00:19:54.060]   And they are the only game in town with search, so why make it better?
[00:19:54.480 --> 00:19:57.760]   There's not 16 other big competitors that they have to keep pace with.
[00:19:57.760 --> 00:20:02.900]   This is not Mercedes looking over their shoulders to see what Rolls-Royce just did with their radio.
[00:20:02.900 --> 00:20:08.660]   And so they can let the service slowly get worse without really worrying that this is going to be a problem.
[00:20:08.660 --> 00:20:11.280]   Now, here's a key question.
[00:20:11.740 --> 00:20:26.520]   Are these type of strategies, this sort of shifting from additive to extractive, is this still happening or is this something that is a snapshot of a strategy that is finishing out that really was a product of the 2010s and the sort of consumer internet burst?
[00:20:26.520 --> 00:20:28.520]   Here's the bad news.
[00:20:28.520 --> 00:20:29.980]   It absolutely is still happening.
[00:20:29.980 --> 00:20:33.320]   We absolutely still need to worry about it.
[00:20:33.320 --> 00:20:38.100]   So I want to read to you from an article that appeared recently.
[00:20:38.100 --> 00:20:39.220]   I'm an e-marketer.
[00:20:39.220 --> 00:20:41.480]   There's been a lot of other articles that said something similar.
[00:20:41.480 --> 00:20:50.520]   The opening of this article should give you some pause about the coming shifts to extractive technology we can expect in the future.
[00:20:50.520 --> 00:20:52.260]   All right, so let me read this here.
[00:20:52.260 --> 00:21:04.220]   OpenAI has hired its first-ever chief marketing officer, Kate Rauch, signaling an effort to step up its brand messaging and bring advertising to its products.
[00:21:04.220 --> 00:21:10.040]   Rauch has deep ties in the digital advertising industry, having worked at Meta for 11 years,
[00:21:10.040 --> 00:21:14.620]   where she eventually served as global head of brand marketing for its suites of apps.
[00:21:14.620 --> 00:21:17.380]   All right, here's why we should be worried about it.
[00:21:17.380 --> 00:21:22.680]   AI is probably doing the exact same thing that the social media companies did 10 years ago.
[00:21:23.680 --> 00:21:31.280]   Think about your relationship right now to a lot of these consumer-facing AI tools like the chatbot interfaces like ChatGPT.
[00:21:32.420 --> 00:21:34.420]   They premiere in late 2022, early 2023.
[00:21:34.420 --> 00:21:35.160]   They're delightful.
[00:21:35.160 --> 00:21:35.880]   They're interesting.
[00:21:35.880 --> 00:21:37.060]   You experiment with them.
[00:21:37.060 --> 00:21:37.960]   Cool stuff is happening.
[00:21:37.960 --> 00:21:39.380]   And suddenly they go to GPT-4.
[00:21:39.380 --> 00:21:40.900]   The cool stuff you can do explodes.
[00:21:40.900 --> 00:21:45.800]   It doesn't cost any money or it costs like a relatively small fee if you're using it a lot.
[00:21:45.800 --> 00:21:47.980]   It's just every week something new.
[00:21:47.980 --> 00:21:48.800]   Hey, you can do this.
[00:21:48.800 --> 00:21:49.780]   Or now we can do images.
[00:21:49.780 --> 00:21:51.660]   Or we tuned it to be a little better at this or that.
[00:21:51.720 --> 00:21:52.840]   Now your code is looking better.
[00:21:52.840 --> 00:21:54.840]   It all seems exciting.
[00:21:54.840 --> 00:21:56.740]   So you start using it.
[00:21:56.740 --> 00:22:00.520]   And they want people to think about it like now they think about Google or they think about Facebook.
[00:22:00.520 --> 00:22:08.380]   That there's parts of your life or your process or your rituals or your business that hook into these tools while it's still delightful and easy and seems really cool.
[00:22:08.380 --> 00:22:10.900]   But they just hired a CMO from Meta.
[00:22:10.900 --> 00:22:15.280]   Someone who understood how Facebook made money.
[00:22:15.280 --> 00:22:18.600]   That means they're looking towards that second phase.
[00:22:18.600 --> 00:22:21.040]   They are losing a lot of money.
[00:22:21.040 --> 00:22:23.280]   Every one of these companies is losing a lot of money.
[00:22:23.280 --> 00:22:25.060]   OpenAI is losing a lot of money.
[00:22:25.060 --> 00:22:26.380]   Microsoft is losing a lot of money.
[00:22:26.380 --> 00:22:28.240]   Google and Meta are losing a lot of money on AI.
[00:22:28.240 --> 00:22:30.320]   It's not a super profitable thing right now.
[00:22:30.320 --> 00:22:37.320]   The biggest amount of money made right now is that OpenAI has this collection of users that hooked up early to their service
[00:22:37.320 --> 00:22:40.500]   and are paying like the nominal, somewhat nominal fee to use the pro version.
[00:22:40.500 --> 00:22:41.680]   They have these like 5 million users.
[00:22:41.680 --> 00:22:44.600]   The other services don't have anywhere near this many users.
[00:22:44.600 --> 00:22:48.260]   And Zitron points this out, which tells you that that's more the result of marketing than necessity.
[00:22:48.260 --> 00:22:49.720]   So they're not making a ton of money.
[00:22:49.720 --> 00:22:50.780]   They need to make more money.
[00:22:50.780 --> 00:22:57.860]   They're bringing in an advertiser who's an expert on using data, your data, to figure out how to target ads for you.
[00:22:57.860 --> 00:23:06.260]   You don't have to have a crystal ball to start to realize, oh, these services, once we're hooked in, mightn't shitify.
[00:23:06.720 --> 00:23:11.200]   They might go from something that is quite additive in my life to something that's quite extractive.
[00:23:11.200 --> 00:23:14.780]   And it might happen in a way where there's no other real competitors to move to.
[00:23:14.780 --> 00:23:17.460]   And it happens slowly enough that I don't realize it's going on.
[00:23:17.460 --> 00:23:20.200]   So these type of transitions are happening right now.
[00:23:20.200 --> 00:23:26.860]   There could be technologies you love today that tomorrow are going to be something that is a net negative in your life.
[00:23:27.860 --> 00:23:32.240]   So this brings us to our fourth and final part of this story, what we can do about it.
[00:23:32.240 --> 00:23:34.840]   Call this final part fighting back.
[00:23:34.840 --> 00:23:36.640]   All right.
[00:23:36.640 --> 00:23:42.840]   I have three ideas here about how to tilt the balance of technology in your life towards additive and away from extractive.
[00:23:42.840 --> 00:23:49.760]   One, do the periodic digital declutter that I suggest in my book, Digital Minimalism.
[00:23:49.760 --> 00:23:56.080]   A digital declutter, and I got to say this three times an episode because otherwise people get it wrong.
[00:23:56.080 --> 00:23:58.960]   A digital declutter is not a detox.
[00:23:59.520 --> 00:24:09.420]   The goal is not to take a break from technology so that you can hear the birds sing and have great insights and figure out the meaning of life and then tick-tock again until your eyes bleed.
[00:24:09.420 --> 00:24:11.860]   That's not the goal of a digital declutter.
[00:24:11.860 --> 00:24:20.360]   The goal is to gain insight that you then use to make permanent changes to what technologies retain a role in your life.
[00:24:21.240 --> 00:24:24.020]   Now, if you read my book, Digital Minimalism, I go through the details of this.
[00:24:24.020 --> 00:24:26.280]   I ran 1,600 people through this process.
[00:24:26.280 --> 00:24:27.560]   There's a lot of insights to share.
[00:24:27.560 --> 00:24:28.800]   But here's the big idea.
[00:24:28.800 --> 00:24:32.320]   30 days where you're stepping away from optional digital technologies.
[00:24:32.320 --> 00:24:37.100]   This will include many of the technologies that right now are additive but will probably become extractive in the near future.
[00:24:37.100 --> 00:24:41.920]   And you then aggressively experiment with new ways to spend your time connecting with people.
[00:24:41.920 --> 00:24:43.580]   You don't just sit there and white knuckle.
[00:24:43.580 --> 00:24:45.840]   You try to get back in touch with what's important.
[00:24:45.840 --> 00:24:53.100]   And then when the declutter is over, you say, which of these technologies I stepped away from do I want to put back?
[00:24:53.100 --> 00:24:55.580]   They don't get back by default.
[00:24:55.580 --> 00:25:00.020]   They have to earn their way back by having some sort of real value they offer to your life.
[00:25:00.020 --> 00:25:01.560]   Then you'll let it back in your life.
[00:25:01.560 --> 00:25:05.780]   But now you'll put gates around there, fences around it, so that you only use it in the ways that are useful.
[00:25:05.780 --> 00:25:14.220]   When you do this on a semi-regular basis, it is a way to sweep away technologies that once were additive that have become extractive.
[00:25:14.240 --> 00:25:18.820]   Because once they become extractive, when you run it through this test, hey, do I really miss using this app?
[00:25:18.820 --> 00:25:19.740]   And you say, well, wait a second.
[00:25:19.740 --> 00:25:23.200]   I used to use that to keep up with my, like, college girlfriends.
[00:25:23.200 --> 00:25:25.620]   That was, like, six years ago.
[00:25:25.620 --> 00:25:27.280]   Now it just makes me upset.
[00:25:27.280 --> 00:25:28.760]   No, you don't earn your way back.
[00:25:28.760 --> 00:25:29.780]   No, that's not coming back.
[00:25:29.780 --> 00:25:32.580]   And so it's a declutter on a semi-regular basis.
[00:25:32.580 --> 00:25:37.520]   It can be, like, defragmentation of extractive technologies in your life.
[00:25:37.520 --> 00:25:39.580]   All right, idea number two.
[00:25:39.580 --> 00:25:43.840]   Have better filter questions you ask before you bring a tool into your life.
[00:25:43.900 --> 00:25:49.080]   And I think one of the key ones for trying to avoid extractive technologies is to say, how much does this cost?
[00:25:49.080 --> 00:25:55.560]   If you don't have a number to answer that question, it's probably costing more over time than you want to spend.
[00:25:56.420 --> 00:26:02.900]   So what I'm trying to say here is you should be biased towards products that have a clear financial cost.
[00:26:02.900 --> 00:26:04.640]   It costs this much money for this product.
[00:26:04.640 --> 00:26:08.240]   The value we get from you is having a product that is good enough for you to pay for.
[00:26:08.240 --> 00:26:08.940]   You paid for it.
[00:26:08.940 --> 00:26:09.660]   Here you go.
[00:26:11.100 --> 00:26:15.600]   You are safe from the shift to extractive technologies in that type of situation.
[00:26:15.980 --> 00:26:21.120]   If the answer to this is, like, oh, it's free or it's nominal, they've got to get paid some time.
[00:26:21.120 --> 00:26:22.380]   And guess what?
[00:26:22.380 --> 00:26:23.400]   Who's going to be doing the pain?
[00:26:23.400 --> 00:26:25.160]   It's going to be you.
[00:26:25.160 --> 00:26:28.600]   And it's going to be in ways that you don't choose and in ways that might be much worse.
[00:26:28.600 --> 00:26:30.440]   It's why, for example, I'm thinking about my own life.
[00:26:30.440 --> 00:26:33.580]   I use Scrivener for writing articles and books.
[00:26:33.700 --> 00:26:35.800]   What I like about Scrivener is you buy a site license.
[00:26:35.800 --> 00:26:37.380]   It costs a non-trivial amount of money.
[00:26:37.380 --> 00:26:38.940]   Then you have the software.
[00:26:38.940 --> 00:26:41.880]   They'll send you updates as it updates, but you've bought it.
[00:26:41.880 --> 00:26:42.560]   That's it.
[00:26:42.560 --> 00:26:43.380]   You have the site license.
[00:26:43.380 --> 00:26:44.700]   You can share it on a couple machines.
[00:26:44.700 --> 00:26:46.900]   It's not a monthly thing.
[00:26:46.900 --> 00:26:47.900]   It's not free with ads.
[00:26:47.900 --> 00:26:52.780]   Another thing I like is the to-do application, Things 3.
[00:26:52.780 --> 00:26:53.460]   Same idea.
[00:26:53.460 --> 00:26:55.540]   They're like, yeah, it's $10 for your phone.
[00:26:55.540 --> 00:26:56.780]   And what do you get?
[00:26:56.780 --> 00:26:57.260]   You get the app.
[00:26:57.260 --> 00:26:58.340]   You went on your Mac.
[00:26:58.340 --> 00:26:59.200]   It's $30 on the map.
[00:26:59.200 --> 00:27:00.000]   You buy it.
[00:27:00.000 --> 00:27:00.760]   It's like the old days.
[00:27:00.760 --> 00:27:02.060]   You get a box and there's a disk in it.
[00:27:02.060 --> 00:27:02.660]   You open the disk.
[00:27:02.660 --> 00:27:03.040]   That's yours.
[00:27:03.040 --> 00:27:03.520]   You own it.
[00:27:04.460 --> 00:27:09.180]   So the value they get from you is in convincing you that this application was useful enough
[00:27:09.180 --> 00:27:09.900]   for you to buy.
[00:27:09.900 --> 00:27:12.840]   They've already have their money from you once you buy it.
[00:27:12.840 --> 00:27:15.960]   You're much less likely for that to become extractive.
[00:27:15.960 --> 00:27:16.120]   All right.
[00:27:16.120 --> 00:27:19.800]   The third idea is to hack the technologies you have that you worry about becoming extractive.
[00:27:19.800 --> 00:27:23.580]   Again, this is an idea from my book, Digital Minimalism, where I talk about joining the
[00:27:23.580 --> 00:27:25.200]   quote, attention resistance.
[00:27:25.200 --> 00:27:29.660]   The idea is you take technologies where there's a value you need out of it, but they're also
[00:27:29.660 --> 00:27:31.900]   furiously trying to extract value from you.
[00:27:31.900 --> 00:27:37.740]   And you use rules and technological solutions to get your value and prevent them from getting
[00:27:37.740 --> 00:27:38.120]   theirs.
[00:27:38.120 --> 00:27:44.660]   So you transform extractive technologies back to their additive form.
[00:27:44.660 --> 00:27:45.780]   There's a lot of ways this could be.
[00:27:45.780 --> 00:27:49.820]   There's people who have to use social media, for example, for various business things who
[00:27:49.820 --> 00:27:55.280]   use it on their laptop and they use it with plugins that scrapes free all the content.
[00:27:55.280 --> 00:27:56.700]   They can't see what other people are posting.
[00:27:56.700 --> 00:28:00.940]   And they go on there for 20 minutes twice a week to post stuff that they wrote in Google
[00:28:00.940 --> 00:28:02.660]   Docs or hire someone to do it on their behalf.
[00:28:02.660 --> 00:28:03.500]   It doesn't take that much time.
[00:28:03.500 --> 00:28:06.580]   It doesn't cost much money or someone to keep track of their comments for them.
[00:28:06.580 --> 00:28:08.720]   They're getting some business value out of social media.
[00:28:08.720 --> 00:28:11.060]   Social media is not getting their time or attention.
[00:28:11.060 --> 00:28:12.300]   I've seen people do this with YouTube.
[00:28:12.540 --> 00:28:14.560]   I want to look up how to fix my car on YouTube.
[00:28:14.560 --> 00:28:15.280]   I have a plugin.
[00:28:15.280 --> 00:28:16.180]   I do it on my computer.
[00:28:16.180 --> 00:28:17.360]   It gets rid of the recommendations.
[00:28:17.360 --> 00:28:18.300]   Now it's a library.
[00:28:18.300 --> 00:28:23.660]   I search for videos about fixing my car and all that comes up is videos about fixing my
[00:28:23.660 --> 00:28:25.100]   car and I find one and I use it.
[00:28:25.100 --> 00:28:26.040]   It's a free library.
[00:28:26.040 --> 00:28:26.780]   I get value.
[00:28:26.780 --> 00:28:27.220]   They don't.
[00:28:27.220 --> 00:28:29.620]   So that's the attention resistance.
[00:28:29.620 --> 00:28:32.100]   And there's any number of things like that you can do.
[00:28:33.060 --> 00:28:38.800]   Even looking for open source or indie versions of tools, I can use DuckDuckGo instead of
[00:28:38.800 --> 00:28:39.980]   Google, right?
[00:28:39.980 --> 00:28:41.100]   It's an indie version of the tool.
[00:28:41.100 --> 00:28:46.080]   They just don't have the same profit motive or the energy, quite frankly, to be as annoying.
[00:28:46.080 --> 00:28:48.380]   I don't trust open AI.
[00:28:48.380 --> 00:28:55.360]   Wait, there's going to be, as the technology matures, lower cost versions, indie versions
[00:28:55.360 --> 00:28:58.280]   of these tools that aren't going to be as powerful as the frontier models, but you don't
[00:28:58.280 --> 00:28:58.880]   need them to be.
[00:28:58.880 --> 00:29:01.500]   And they're probably going to be a lot less extractive, right?
[00:29:01.500 --> 00:29:02.980]   So indie tools is another thing to do.
[00:29:03.920 --> 00:29:05.560]   All right, so let's wrap up this conversation.
[00:29:05.560 --> 00:29:09.340]   What I'm trying to say here is that when it comes to creating a deep life in a distracted
[00:29:09.340 --> 00:29:12.560]   world, you have to be wary of the technology that you use.
[00:29:12.560 --> 00:29:16.580]   This reality is often more complicated than just saying, hey, stay away from bad tools.
[00:29:16.580 --> 00:29:20.700]   Only use good tools because sometimes these roles can change.
[00:29:20.700 --> 00:29:27.240]   So next time you despair about all the rabbit holes your smartphone has thrown you down,
[00:29:27.240 --> 00:29:32.160]   remember that kid on the couch having a great time with an old piece of plastic,
[00:29:32.580 --> 00:29:33.580]   talking to his grandma.
[00:29:33.580 --> 00:29:36.020]   There you go, Jesse.
[00:29:36.020 --> 00:29:38.220]   Additive versus extractive.
[00:29:38.220 --> 00:29:39.400]   I wrote about this in my newsletter.
[00:29:39.400 --> 00:29:43.680]   So if you don't subscribe to my newsletter, calnewport.com, well, you should.
[00:29:43.680 --> 00:29:45.680]   It's a good example.
[00:29:45.680 --> 00:29:51.900]   My alternative topic, which I would have done two hours on, was me attending the Nats A's
[00:29:51.900 --> 00:29:57.040]   game last night where the Nats broke a six-game winning streak with a walk-off hit by CJ Abrams
[00:29:57.040 --> 00:29:57.800]   in the bottom of the ninth.
[00:29:57.800 --> 00:29:59.240]   I was there.
[00:29:59.240 --> 00:30:01.080]   Now, let me give you another data point.
[00:30:01.080 --> 00:30:03.280]   I was not there for the day game today.
[00:30:03.280 --> 00:30:05.380]   Nats lost 06.
[00:30:05.840 --> 00:30:10.220]   So I think, and I'm trying to explain this to my wife, I probably need to go to every
[00:30:10.220 --> 00:30:10.900]   home Nats game.
[00:30:10.900 --> 00:30:11.800]   Yeah.
[00:30:11.800 --> 00:30:13.900]   I mean, if they're going to get their record around.
[00:30:13.900 --> 00:30:16.060]   So that was what I was going to talk about.
[00:30:16.060 --> 00:30:21.780]   And what I was going to spend like 20 minutes on was the fact that, and only like 17 people
[00:30:21.780 --> 00:30:22.860]   right now will appreciate this.
[00:30:23.720 --> 00:30:30.480]   In the bottom of the ninth, with one man on and no outs, Jacob Young bunted with two strikes
[00:30:30.480 --> 00:30:31.980]   and fouled out.
[00:30:31.980 --> 00:30:37.800]   And the woman in front of us turned around and was like, so what happened there?
[00:30:37.800 --> 00:30:38.820]   And we told her.
[00:30:38.820 --> 00:30:40.040]   And she's like, is that a new rule?
[00:30:40.040 --> 00:30:41.220]   We're like, no, that's an old rule.
[00:30:41.220 --> 00:30:43.540]   And she's like, so then why did he bunt with two strikes?
[00:30:43.540 --> 00:30:44.860]   He shouldn't have.
[00:30:44.860 --> 00:30:46.240]   He really just shouldn't have.
[00:30:46.240 --> 00:30:47.620]   So that's going to be next week.
[00:30:47.620 --> 00:30:51.320]   It's going to be Jacob Young's faults in his bunting strategy.
[00:30:51.320 --> 00:30:53.460]   Anyways, we've got a good rest of the show to come.
[00:30:53.520 --> 00:30:54.300]   We've got some good questions.
[00:30:54.300 --> 00:30:57.940]   And because this is the first episode, that's not the first, it's our second episode of August,
[00:30:57.940 --> 00:30:58.240]   probably.
[00:30:58.240 --> 00:31:02.640]   We forgot to do this last week, but we will go over the books I read last month.
[00:31:02.640 --> 00:31:08.700]   But first, because it's the most fun thing we do on the show, let's hear from a sponsor.
[00:31:08.700 --> 00:31:15.820]   Look, Jesse, going online without ExpressVPN is like forgetting to put yourself on mute when
[00:31:15.820 --> 00:31:16.540]   you're in a Zoom meeting.
[00:31:16.540 --> 00:31:20.480]   Do you really want your coworkers to hear you trash talking to them?
[00:31:20.480 --> 00:31:21.120]   True story, Jesse.
[00:31:21.120 --> 00:31:23.320]   I was once on a Zoom call with Jesse Skeleton.
[00:31:23.320 --> 00:31:25.240]   You forgot to press mute.
[00:31:25.240 --> 00:31:27.280]   He said some terrible things about you.
[00:31:27.280 --> 00:31:34.960]   I don't want to stir the waters, but all I got to do is I just got to say he had some
[00:31:34.960 --> 00:31:36.860]   choice words about Tufts lacrosse.
[00:31:39.240 --> 00:31:44.060]   And I don't want to use the black sock scandal, but he did compare it to that.
[00:31:44.060 --> 00:31:48.320]   So I'm just saying, you don't want to be on Zoom without mute.
[00:31:48.320 --> 00:31:50.060]   Well, you don't want to use the internet without a VPN.
[00:31:50.060 --> 00:31:51.120]   And here's the thing.
[00:31:51.120 --> 00:31:52.780]   You got to have a VPN.
[00:31:52.780 --> 00:31:54.300]   Here's what really happens here.
[00:31:54.300 --> 00:31:58.280]   When you connect to the internet, you send all the data in these little bundles that are
[00:31:58.280 --> 00:31:58.780]   called packets.
[00:31:59.040 --> 00:32:03.640]   And yeah, the data itself could be secure, could be encrypted, but the address, like where
[00:32:03.640 --> 00:32:07.680]   these packets are going, that has to be out in the open because the internet routers need
[00:32:07.680 --> 00:32:09.280]   to know like where to send the bundles.
[00:32:09.280 --> 00:32:10.480]   So what does this mean?
[00:32:10.480 --> 00:32:15.020]   Anyone nearby can see what sites and services you're using.
[00:32:15.020 --> 00:32:18.920]   If you're at a coffee shop or whatever, anyone with a radio antenna on their computer with the
[00:32:18.920 --> 00:32:21.980]   right piece of software can see exactly what sites and services you're using.
[00:32:21.980 --> 00:32:25.620]   Your ISP at home, they know exactly what sites and services you're using.
[00:32:25.700 --> 00:32:26.540]   They can sell that data.
[00:32:26.540 --> 00:32:30.260]   There's a lot of internet service providers who actually do.
[00:32:30.260 --> 00:32:31.680]   So a VPN protects you from this.
[00:32:31.680 --> 00:32:35.980]   The way it works is instead of talking to a site or service directly, you talk to the VPN,
[00:32:35.980 --> 00:32:39.320]   you send that a secret message and say, here's who I really want to talk to.
[00:32:39.320 --> 00:32:42.660]   They talk to that site or service on your behalf, encrypt it, send it back.
[00:32:42.660 --> 00:32:47.160]   Now the guy trying to sniff your packets with the laptop nearby and the coffee shop or your
[00:32:47.160 --> 00:32:51.060]   ISP, all they learn is that you're talking to a VPN, privacy regained.
[00:32:51.060 --> 00:32:55.140]   So if you're going to use a VPN, use the one I suggest, which is ExpressVPN.
[00:32:55.140 --> 00:32:56.560]   It's easy to use.
[00:32:56.560 --> 00:33:01.620]   It works on all of your devices and it is rated number one by top tech reviewers like CNET
[00:33:01.620 --> 00:33:03.160]   and The Verge.
[00:33:03.160 --> 00:33:04.580]   So what are you waiting for?
[00:33:04.580 --> 00:33:09.500]   Protect your online privacy today by visiting expressvpn.com slash deep.
[00:33:09.500 --> 00:33:13.300]   That's E-X-P-R-E-S-S-V-P-N dot com slash deep.
[00:33:13.620 --> 00:33:20.220]   To find out how you can get up to four extra months for free, that's expressvpn.com slash
[00:33:20.220 --> 00:33:20.640]   deep.
[00:33:20.640 --> 00:33:24.740]   I don't know, Jesse, something about the phrase, the guy next to you in the coffee shop is sniffing
[00:33:24.740 --> 00:33:25.260]   your packets.
[00:33:25.260 --> 00:33:30.200]   Feel somehow like the type of thing you don't want to be heard on a Zoom when your mute's
[00:33:30.200 --> 00:33:31.060]   off or something like that.
[00:33:31.060 --> 00:33:31.860]   I agree with that.
[00:33:31.860 --> 00:33:32.860]   It's a technical topic.
[00:33:32.860 --> 00:33:35.240]   Trust me, it is.
[00:33:35.240 --> 00:33:35.680]   All right.
[00:33:35.680 --> 00:33:37.020]   I also want to talk about our friends at Oracle.
[00:33:37.020 --> 00:33:37.680]   Here's the thing.
[00:33:37.680 --> 00:33:42.300]   In business, you can have better, cheaper, or faster, but you only get to pick two.
[00:33:42.300 --> 00:33:43.440]   That's a saying I invented, Jesse.
[00:33:43.440 --> 00:33:46.660]   I don't know if you know about that, but I have a lot of insights like that.
[00:33:46.660 --> 00:33:48.740]   Now, what if you could have all three at the same time?
[00:33:48.740 --> 00:33:55.260]   Well, that's exactly what Cohere, Thomas Reuters, and Specialized Bikes have since they upgraded
[00:33:55.260 --> 00:33:58.680]   to the next generation of the cloud, Oracle Cloud Infrastructure.
[00:33:58.680 --> 00:34:05.040]   OCI is the blazing fast platform for your infrastructure, database, application development, and AI needs,
[00:34:05.040 --> 00:34:11.080]   where you can run any workload in a high availability, consistently high performance environment,
[00:34:11.080 --> 00:34:13.800]   and spend less than you would with other clouds.
[00:34:13.800 --> 00:34:15.140]   How is it faster?
[00:34:15.140 --> 00:34:18.180]   OCI's block storage gives you more operations per second.
[00:34:18.180 --> 00:34:18.800]   How is it cheaper?
[00:34:18.800 --> 00:34:26.140]   OCI costs up to 50% less for compute and 70% less for storage, and a whopping 80% less for
[00:34:26.140 --> 00:34:26.500]   networking.
[00:34:26.500 --> 00:34:27.460]   How is it better?
[00:34:27.460 --> 00:34:33.920]   Test after test, OCI customers report lower latency and higher bandwidth versus other clouds.
[00:34:34.320 --> 00:34:37.600]   This is the cloud built for AI and all of your biggest workloads.
[00:34:37.600 --> 00:34:39.120]   I've got good news.
[00:34:39.120 --> 00:34:43.620]   Right now, with zero commitment, you can try OCI for free.
[00:34:43.620 --> 00:34:46.280]   Head to oracle.com slash deep questions.
[00:34:46.280 --> 00:34:48.840]   That's oracle.com slash deep questions.
[00:34:49.720 --> 00:34:52.360]   All right, Jesse, let's do some questions of our own.
[00:34:52.360 --> 00:34:55.660]   First question is from Erica.
[00:34:55.660 --> 00:34:58.200]   I'm a computational biologist.
[00:34:58.200 --> 00:35:03.000]   We have systems where project leads have to update a row on a project tractor ahead of meetings.
[00:35:03.000 --> 00:35:08.340]   This typically consists of a status and a short description of work completed and what's coming up.
[00:35:08.340 --> 00:35:11.020]   We then review this information together at the meeting.
[00:35:11.020 --> 00:35:12.240]   This has some positives.
[00:35:12.240 --> 00:35:15.140]   However, the trackers really build up and it takes a lot of time.
[00:35:15.140 --> 00:35:16.840]   What are your thoughts on these systems?
[00:35:16.960 --> 00:35:18.340]   Do you have any alternatives?
[00:35:18.340 --> 00:35:19.940]   All right, do we understand?
[00:35:19.940 --> 00:35:22.620]   I want to make sure we fully understand how this system works.
[00:35:22.620 --> 00:35:24.840]   Let's try to imagine this, Jesse, like if we were using it.
[00:35:24.840 --> 00:35:28.180]   So there's a spreadsheet and there's a project lead.
[00:35:28.180 --> 00:35:34.640]   So like if we were working on some project, like adding an animatronic child and a Jesse skeleton,
[00:35:34.640 --> 00:35:40.060]   you as the project lead would update a row in a project tracker.
[00:35:40.060 --> 00:35:44.480]   So I guess that's like a spreadsheet that would have a status and a short description of the work
[00:35:44.480 --> 00:35:46.180]   completed but also the work coming up.
[00:35:46.180 --> 00:35:52.020]   So you would actually probably have to say, like, I worked on this.
[00:35:52.020 --> 00:35:56.160]   I worked on the, like the Arduino connections today and tomorrow I'm going to work on whatever.
[00:35:56.160 --> 00:36:00.120]   And so you're like kind of updating not just what you're working on, but like specifically
[00:36:00.120 --> 00:36:02.280]   what on that task you did and what's coming up.
[00:36:02.280 --> 00:36:05.820]   So here's what I think about, if I'm understanding that right, I think that's right.
[00:36:05.820 --> 00:36:06.280]   Yeah.
[00:36:06.400 --> 00:36:09.740]   So Erica, the intent here is great.
[00:36:09.740 --> 00:36:17.000]   By having project tracking, you're stopping unscheduled communication that might require replies,
[00:36:17.000 --> 00:36:21.700]   which of course is the productivity poison that afflicts most of knowledge work, having to respond to unscheduled messages.
[00:36:21.700 --> 00:36:23.320]   Now you have to context shift constantly.
[00:36:23.320 --> 00:36:24.240]   You run out of energy.
[00:36:24.340 --> 00:36:25.660]   Nothing gets miserable and nothing gets done.
[00:36:25.660 --> 00:36:31.080]   Also, this type of system will succeed in making work and work loads more transparent.
[00:36:31.080 --> 00:36:32.540]   Who's working on what and what's going on?
[00:36:32.540 --> 00:36:37.740]   So it's not just this sort of informal dance of messaging and you never really know who's doing what until you bother them.
[00:36:38.740 --> 00:36:40.980]   The problem is you're tracking too much.
[00:36:40.980 --> 00:36:42.780]   I think you're picking this up in your question here.
[00:36:42.780 --> 00:36:45.540]   Tracking actual work accomplished, work you want to do.
[00:36:45.540 --> 00:36:49.500]   So getting into the granularity about the specific stuff you're doing towards a larger goal.
[00:36:49.500 --> 00:36:51.300]   I think that is too much friction.
[00:36:51.300 --> 00:36:56.240]   If that friction adds up, it becomes a choke point and then you stop making progress altogether.
[00:36:56.240 --> 00:37:01.000]   I think software developers have found a better middle ground in this.
[00:37:01.000 --> 00:37:06.480]   When they're sort of agile systems like Scrum, they think about work in terms of, they call it features.
[00:37:06.660 --> 00:37:12.860]   So I guess the equivalent here for a computational biologist might be like a task, right?
[00:37:12.860 --> 00:37:19.640]   So a project would be too big, but a task, it might require like multiple steps, but something that you can maybe tackle in a day or so.
[00:37:19.640 --> 00:37:26.460]   And what they do in software development is they track who's working on what task and more importantly, what tasks need to be done but no one's working on yet.
[00:37:26.460 --> 00:37:28.060]   This way you can control the workload.
[00:37:28.060 --> 00:37:31.100]   Stuff can be tracked without it having to be on people's plates.
[00:37:31.340 --> 00:37:38.180]   The way they update the statuses is they have a daily 15-minute stand-up meeting where everyone is like, go.
[00:37:38.180 --> 00:37:41.040]   And you're like, here's what I'm working on.
[00:37:41.040 --> 00:37:41.660]   Here's where I'm stuck.
[00:37:41.660 --> 00:37:42.320]   Here's what I need.
[00:37:42.320 --> 00:37:43.020]   Here's how it's going.
[00:37:43.020 --> 00:37:44.240]   It's 45 seconds.
[00:37:44.240 --> 00:37:44.600]   Go.
[00:37:44.600 --> 00:37:48.820]   They call them standing meetings because they're trying to emphasize they're quick.
[00:37:48.820 --> 00:37:50.520]   They're not meant to be how's your day?
[00:37:50.520 --> 00:37:51.260]   What's going on?
[00:37:51.260 --> 00:37:52.240]   What happened with your skeleton?
[00:37:52.400 --> 00:37:54.040]   It's like, go, go, go, go.
[00:37:54.040 --> 00:38:00.180]   It turns out that this is a very low-friction way of getting people up to speed as well as figuring out who needs what from whom.
[00:38:00.180 --> 00:38:04.300]   I know it doesn't sound like it should take so much time to write that down, but it freezes people.
[00:38:04.300 --> 00:38:05.640]   Talking's easier.
[00:38:05.640 --> 00:38:14.500]   So they've really kind of honed this in in software development that that granularity of task and that method of updating, that's the right way to do it.
[00:38:14.540 --> 00:38:16.460]   So I would look for something like that.
[00:38:16.460 --> 00:38:22.100]   In my book, A World Without Email, which you should get, I talk in there somewhere.
[00:38:22.100 --> 00:38:23.600]   You'll have to find what chapter it is.
[00:38:23.600 --> 00:38:34.240]   I talk in there somewhere if I get an interesting research report written by a computer scientist at the University of Maryland who talks about adapting that sort of software development granularity to his research group.
[00:38:34.240 --> 00:38:37.520]   And he gets some ideas about why it works or how to adapt that to that world.
[00:38:37.520 --> 00:38:38.220]   So check that out.
[00:38:38.220 --> 00:38:39.320]   You're on the right track.
[00:38:39.320 --> 00:38:40.700]   You got too many frictions.
[00:38:40.700 --> 00:38:42.420]   There's too much friction going on there.
[00:38:42.420 --> 00:38:44.520]   Too much friction going on there.
[00:38:44.520 --> 00:38:50.520]   I mean, I require Jesse to narrate everything he's doing and then transcribe those narrations.
[00:38:50.520 --> 00:38:57.580]   And then I prefer them if you transcribe the French and then back to English, I feel like it makes it a little bit more concise.
[00:38:57.580 --> 00:39:01.180]   And then what we do is we chisel that in the clay because it's a little bit more durable.
[00:39:01.180 --> 00:39:03.640]   And then he gives me those clay tablets once every 17 minutes.
[00:39:03.640 --> 00:39:08.240]   We do that, but that might be more friction than what some other people want to do.
[00:39:08.240 --> 00:39:10.720]   So take a page out of the software developers.
[00:39:10.720 --> 00:39:11.760]   All right, who we got?
[00:39:11.760 --> 00:39:12.980]   Next up is Hunter.
[00:39:12.980 --> 00:39:18.940]   Can you talk about the alternative route to a Walt Disney story where folks might get obsessed with something,
[00:39:18.940 --> 00:39:22.660]   but it just remains an obsession and never is a catalyst for productive action?
[00:39:22.660 --> 00:39:27.740]   How can folks know if a project will spur something or just indulge a dorky obsession?
[00:39:27.740 --> 00:39:29.700]   I mean, dorky is okay.
[00:39:30.620 --> 00:39:31.980]   Yeah, don't be obsessed.
[00:39:31.980 --> 00:39:40.800]   Like, I don't have a, when it comes to projects you're doing in your leisure time, just a sort of engineer wonder, like we talked about a few episodes ago.
[00:39:40.800 --> 00:39:43.180]   Don't be obsessive about it.
[00:39:43.180 --> 00:39:49.820]   I know that sounds like a bit of a cop-out or a tautological response, but you know when something is becoming obsessive.
[00:39:50.960 --> 00:39:53.860]   All I'm doing, it's getting in the way of other things.
[00:39:53.860 --> 00:39:57.680]   I'm staying up all night, reposing my skeletons.
[00:39:57.680 --> 00:39:58.640]   I mean, that one's okay.
[00:39:58.640 --> 00:39:59.900]   That one's okay.
[00:39:59.900 --> 00:40:05.680]   But if you're taking those skeletons on dates, then things are getting a little bit more obsessive.
[00:40:05.680 --> 00:40:06.480]   Then you got to be careful.
[00:40:07.020 --> 00:40:09.180]   So, you know, have schedules, like, yeah, that's what I'm working on.
[00:40:09.180 --> 00:40:11.260]   I have some, I put aside some time when I need it.
[00:40:11.260 --> 00:40:16.100]   I'm really happy when I get to work on it, but you know, I'm not disappointed when I don't.
[00:40:16.100 --> 00:40:16.800]   Life happens.
[00:40:16.800 --> 00:40:22.080]   It's like this thing you have to look forward to, and you make some time to work on it when you can.
[00:40:22.080 --> 00:40:24.420]   It's kind of the way I think about myself with movies.
[00:40:24.420 --> 00:40:25.300]   I really like movies.
[00:40:25.300 --> 00:40:27.880]   I only have so many occasions to watch them.
[00:40:27.880 --> 00:40:30.980]   And there's certain times of life or year where it's like, oh, this is great.
[00:40:30.980 --> 00:40:32.920]   Like, I have a light day coming up.
[00:40:32.920 --> 00:40:34.600]   I'm going to block off the afternoon.
[00:40:34.720 --> 00:40:38.420]   I'm working from home, and I'm going to watch, you know, like whatever movie I've been waiting to watch.
[00:40:38.420 --> 00:40:39.040]   And it's like fun.
[00:40:39.040 --> 00:40:42.200]   Or my friends and I get in a rhythm of seeing some of the new movies at night.
[00:40:42.200 --> 00:40:44.460]   And then like a month will go by where I just really don't see anything.
[00:40:44.460 --> 00:40:45.580]   It's the summer.
[00:40:45.580 --> 00:40:46.660]   We're doing stuff with the kids.
[00:40:46.660 --> 00:40:48.240]   It's, you know, it's light out late.
[00:40:48.240 --> 00:40:49.860]   People are around.
[00:40:49.860 --> 00:40:51.240]   And we don't see as many movies or whatever.
[00:40:51.240 --> 00:40:52.800]   But it's something I always enjoy.
[00:40:52.800 --> 00:40:54.560]   And when I get a chance to read about them, I do.
[00:40:54.560 --> 00:40:56.420]   And when I get a chance to watch them, I'm very happy.
[00:40:56.420 --> 00:41:02.040]   And occasionally there's parts of my life where I am putting aside time to, like, I'm going to go read this textbook.
[00:41:02.260 --> 00:41:06.140]   I'm going to make a point of going to see this movie in this particular format.
[00:41:06.140 --> 00:41:07.040]   And sometimes I'm not.
[00:41:07.040 --> 00:41:08.080]   And that's great.
[00:41:08.080 --> 00:41:10.460]   That's really where things just sort of be.
[00:41:10.460 --> 00:41:12.860]   You know, I love this thing.
[00:41:12.860 --> 00:41:13.840]   I do it when I can.
[00:41:13.840 --> 00:41:14.740]   Sometimes I put aside time.
[00:41:14.740 --> 00:41:15.880]   It's okay if I can't get to it.
[00:41:15.880 --> 00:41:17.380]   Sometimes it's been more than others.
[00:41:17.820 --> 00:41:22.000]   Once it starts to get obsessive, like, if it's causing problems, it's defeating the purpose.
[00:41:22.000 --> 00:41:24.020]   And that's actually what happened with Disney and his railroad.
[00:41:24.020 --> 00:41:25.720]   He was still working his job.
[00:41:25.720 --> 00:41:27.400]   He was still running Disney Studios.
[00:41:27.400 --> 00:41:29.240]   All that stuff was still going on.
[00:41:29.240 --> 00:41:31.360]   And there'd be points where he spent a lot of time on these projects.
[00:41:31.360 --> 00:41:33.500]   And then other points where a lot of time would go on.
[00:41:33.500 --> 00:41:34.560]   He couldn't really work on it.
[00:41:34.560 --> 00:41:36.500]   He'd hire people to work on it.
[00:41:36.500 --> 00:41:37.480]   And then you'd check in with them.
[00:41:37.480 --> 00:41:40.180]   And it'd give them a little bit of a reprieve of an otherwise stressful day.
[00:41:40.420 --> 00:41:41.560]   He loved the project.
[00:41:41.560 --> 00:41:42.600]   It gave him back wonder.
[00:41:42.600 --> 00:41:44.520]   But it wasn't like it was taking over his life.
[00:41:44.520 --> 00:41:46.340]   It wasn't like he stopped his job.
[00:41:46.340 --> 00:41:48.140]   It wasn't like he stopped spending time with his daughters.
[00:41:48.140 --> 00:41:49.140]   So I don't know.
[00:41:49.140 --> 00:41:51.180]   I guess my answer is be cool.
[00:41:51.180 --> 00:41:52.220]   Just be cool.
[00:41:52.220 --> 00:41:56.840]   There was a lot of good positive feedback from that Engineering Wonder episode.
[00:41:56.840 --> 00:41:58.840]   Yeah, a lot of people do something like this.
[00:41:58.840 --> 00:42:03.940]   Yeah, I got a lot of messages from people who have had good experiences do that.
[00:42:03.940 --> 00:42:09.120]   Is that feedback an invitation for me to get into a lot of detail
[00:42:09.120 --> 00:42:12.120]   about the really technical Imagineering stuff I've been reading about in Disney?
[00:42:12.120 --> 00:42:13.660]   You think that's what they're saying?
[00:42:13.660 --> 00:42:18.320]   Like we want to hear about the format that was innovated
[00:42:18.320 --> 00:42:23.560]   based off of technology that was coming out of missile control from the Navy.
[00:42:23.560 --> 00:42:27.780]   How they figured out how to use sound encoding of movement patterns
[00:42:27.780 --> 00:42:30.520]   on one-inch tape for the Enchanted Tiki Room decoration.
[00:42:30.520 --> 00:42:32.280]   Because I could do a lot on that.
[00:42:32.280 --> 00:42:34.180]   I could do a lot on that.
[00:42:34.180 --> 00:42:37.020]   It's using, they use a reed decoder.
[00:42:37.180 --> 00:42:40.440]   So actual vibrations of reeds at different friction closes a circuit
[00:42:40.440 --> 00:42:44.460]   and allows them to actually transform tones on a multi-track tape.
[00:42:44.460 --> 00:42:48.960]   ASCAM was a very new company into actuated movement.
[00:42:48.960 --> 00:42:50.100]   It's very interesting.
[00:42:50.100 --> 00:42:53.240]   Pirates of the Caribbean, they went to physical disk because of durability issues.
[00:42:53.240 --> 00:42:54.640]   It's all very interesting.
[00:42:56.720 --> 00:42:58.140]   That'll be another episode coming up.
[00:42:58.140 --> 00:42:59.060]   All right, who do we got?
[00:42:59.060 --> 00:43:00.640]   Next up is Dominic.
[00:43:00.640 --> 00:43:04.080]   I'm interested in your thoughts on how people should concern themselves
[00:43:04.080 --> 00:43:05.600]   with the folks they interact with.
[00:43:05.600 --> 00:43:08.620]   How should we think about the influence of our partner or housemates
[00:43:08.620 --> 00:43:10.440]   on our ability to pursue depth?
[00:43:10.440 --> 00:43:14.620]   I mean, it helps to have some people in your life,
[00:43:14.620 --> 00:43:18.260]   friends, for example, who are on your same wavelength
[00:43:18.260 --> 00:43:20.060]   in terms of how you think about like depth and ambition
[00:43:20.060 --> 00:43:21.900]   and pursuing projects or whatever, right?
[00:43:21.900 --> 00:43:24.160]   It's good to be around some people like that.
[00:43:24.160 --> 00:43:26.400]   You don't need everyone in your life to be that way.
[00:43:26.400 --> 00:43:28.740]   And probably it's good if everyone in your life is not that way
[00:43:28.740 --> 00:43:31.420]   because you also need people to fulfill other roles in your life.
[00:43:31.420 --> 00:43:36.440]   If you're in a monotype, things get weird.
[00:43:36.440 --> 00:43:39.440]   If everyone is the same way around you, things get weird.
[00:43:39.440 --> 00:43:42.780]   If no one around you is like you, things get lonely or frustrating.
[00:43:43.060 --> 00:43:45.380]   If everyone's like you, things get weird.
[00:43:45.380 --> 00:43:48.820]   My case in point is, I mean, hang out with people
[00:43:48.820 --> 00:43:50.300]   who've been in Silicon Valley too long.
[00:43:50.300 --> 00:43:52.860]   They seem innocent.
[00:43:52.860 --> 00:43:54.820]   Like, oh, it's like a cool gingham shirt you have on.
[00:43:54.820 --> 00:43:58.080]   And the next thing you know, they're talking about how they're,
[00:43:58.080 --> 00:44:03.040]   you know, only on soylent green to prepare for having the chip
[00:44:03.040 --> 00:44:06.840]   implanted in their head so that they can have their like messianic
[00:44:06.840 --> 00:44:10.540]   AI fuel singularity moment that is like coming next week for sure.
[00:44:10.900 --> 00:44:13.820]   Like people go weird when everyone is the same way.
[00:44:13.820 --> 00:44:15.820]   So you want some people to be around.
[00:44:15.820 --> 00:44:17.800]   Like I have a bunch of author friends.
[00:44:17.800 --> 00:44:18.760]   We talk all the time.
[00:44:18.760 --> 00:44:21.060]   We deal with the same issues.
[00:44:21.060 --> 00:44:21.900]   We have the same job.
[00:44:21.900 --> 00:44:22.640]   We have the same ambitions.
[00:44:22.640 --> 00:44:24.120]   But I don't want everyone being like that.
[00:44:24.120 --> 00:44:28.200]   I've become unbearable or weird or unusual.
[00:44:28.200 --> 00:44:30.140]   Or God knows what would happen if I hung out with writers.
[00:44:30.140 --> 00:44:30.720]   It's not great.
[00:44:30.720 --> 00:44:31.860]   It's not great.
[00:44:31.860 --> 00:44:33.660]   So have other people in your life to fill other roles,
[00:44:33.660 --> 00:44:35.040]   to bring other things out about you.
[00:44:35.040 --> 00:44:38.180]   So then what becomes the filter you're looking for about whether someone
[00:44:38.180 --> 00:44:39.420]   that should be in your life or not?
[00:44:40.560 --> 00:44:44.740]   If they make your life better, if they make you better,
[00:44:44.740 --> 00:44:46.140]   they're good to be in your life.
[00:44:46.140 --> 00:44:48.160]   If they make your life worse or make you unhappy, they're not.
[00:44:48.160 --> 00:44:51.080]   And there's a lot of different ways that people can make your life better
[00:44:51.080 --> 00:44:53.700]   and make you better because there's all sorts of parts of life.
[00:44:53.700 --> 00:44:55.100]   So you need a lot of people around you.
[00:44:55.100 --> 00:44:57.640]   Make sure you have some people on your wavelength, but not too many.
[00:44:57.640 --> 00:44:59.380]   Things do get weird.
[00:44:59.380 --> 00:45:00.820]   All right.
[00:45:00.820 --> 00:45:01.520]   We got one more here.
[00:45:01.900 --> 00:45:02.820]   One more from Ryan.
[00:45:02.820 --> 00:45:06.760]   How do you think about the differences in relationships between your podcast,
[00:45:06.760 --> 00:45:08.720]   YouTube channel, and newsletter?
[00:45:08.720 --> 00:45:11.800]   I'm considering starting a similar offering in my niche,
[00:45:11.800 --> 00:45:15.920]   but I want to feel absolutely clear on what I'm trying to achieve with each platform.
[00:45:16.920 --> 00:45:21.240]   Ryan's forgetting one of my platforms, which is my TikTok,
[00:45:21.240 --> 00:45:27.340]   where I give Halloween decoration advice in the form of K-pop dances.
[00:45:27.340 --> 00:45:32.360]   So that one, believe it or not, I have 175 million followers.
[00:45:34.560 --> 00:45:36.000]   I do it without pants on, too.
[00:45:36.000 --> 00:45:36.400]   So I don't know.
[00:45:36.400 --> 00:45:37.540]   I think that's a TikTok thing.
[00:45:37.540 --> 00:45:38.280]   That's what they told me.
[00:45:38.280 --> 00:45:38.660]   I don't know.
[00:45:38.660 --> 00:45:42.440]   I don't know if that's unusual or not, but different platforms play different roles.
[00:45:42.440 --> 00:45:42.680]   Okay.
[00:45:42.680 --> 00:45:43.820]   So how do I think about these?
[00:45:43.820 --> 00:45:51.860]   Podcast, I think about as a combination of more story focus because I have time, right?
[00:45:51.860 --> 00:45:54.520]   You have a longer engagement with someone in their ears.
[00:45:54.520 --> 00:45:56.940]   They're in the middle of driving to work or something like this,
[00:45:56.940 --> 00:45:58.600]   and you have time to lay out a story.
[00:45:58.600 --> 00:46:02.520]   And because I have time, I can get more detailed with advice.
[00:46:02.520 --> 00:46:05.620]   So we do questions or in the final parts of my deep dives.
[00:46:05.620 --> 00:46:08.860]   We can take time to, like, walk through particular advice.
[00:46:08.860 --> 00:46:12.820]   So it's a format that breathes, and it's a format that it's narrative.
[00:46:12.820 --> 00:46:14.100]   So story works well.
[00:46:14.100 --> 00:46:17.540]   Newsletter is much shorter, right?
[00:46:17.540 --> 00:46:18.680]   It's a much shorter format.
[00:46:18.680 --> 00:46:21.720]   I mean, some people write very long newsletters, but that's not my style.
[00:46:21.720 --> 00:46:24.380]   I usually have, like, one big idea, maybe one example.
[00:46:24.380 --> 00:46:26.460]   So I have one thing to deliver to you in this newsletter.
[00:46:26.460 --> 00:46:27.500]   That's my style.
[00:46:27.500 --> 00:46:29.100]   Oh, that's an interesting story.
[00:46:29.100 --> 00:46:29.680]   What's the takeaway?
[00:46:29.680 --> 00:46:31.180]   Oh, I see how that connects.
[00:46:31.180 --> 00:46:34.280]   This takeaway is something that's useful to me or it's like an idea that's useful to me.
[00:46:34.280 --> 00:46:36.660]   So it's much more shorter and focused.
[00:46:36.660 --> 00:46:39.560]   I'm also more likely to be technical.
[00:46:39.560 --> 00:46:41.180]   I'll do more technical stuff in the newsletter.
[00:46:41.180 --> 00:46:42.940]   I'll do more, like, let's get into weeds, AI.
[00:46:42.940 --> 00:46:48.600]   That's fine in the written format in a way that I think maybe plays less good.
[00:46:48.600 --> 00:46:53.820]   There's only so much you want to hear me talk about, you know, reinforcement learning
[00:46:53.820 --> 00:46:57.620]   and tuning of chain of thought type patterns of inferences.
[00:46:57.620 --> 00:46:59.680]   That doesn't play as well in audio.
[00:46:59.680 --> 00:47:01.040]   So I think about that a little bit differently.
[00:47:01.040 --> 00:47:05.540]   YouTube is just my podcast or stuff that comes out of my podcast for the most part.
[00:47:05.540 --> 00:47:09.940]   So, like, I think about YouTube as there's a whole potential audience for my podcast who
[00:47:09.940 --> 00:47:11.380]   doesn't use podcast players.
[00:47:11.380 --> 00:47:12.140]   They use YouTube.
[00:47:12.140 --> 00:47:15.160]   That's where they listen to their podcast on the YouTube app.
[00:47:15.160 --> 00:47:20.140]   They find out about content and people that they like, like authors, on YouTube.
[00:47:20.140 --> 00:47:23.020]   So we really just, we move my podcast over there.
[00:47:23.020 --> 00:47:28.040]   And, like, what we're trying to do is, like, clip out the podcast and allow what we're talking
[00:47:28.040 --> 00:47:34.120]   about here to find people who are a little bit younger usually who don't necessarily consume
[00:47:34.120 --> 00:47:35.180]   content through the podcast app.
[00:47:35.220 --> 00:47:39.440]   So I don't think about, it's the same, there's no content there that's not on YouTube for
[00:47:39.440 --> 00:47:41.260]   the most part, that's not on my podcast right now.
[00:47:41.260 --> 00:47:45.160]   So that's just, like, to me, that goes together with podcasting.
[00:47:45.160 --> 00:47:45.860]   It's broadcasting.
[00:47:45.860 --> 00:47:47.200]   And I think video is important.
[00:47:47.200 --> 00:47:48.940]   That's where video goes these days.
[00:47:48.940 --> 00:47:50.040]   That might change in the future.
[00:47:50.040 --> 00:47:51.600]   But I see those as being very similar.
[00:47:51.600 --> 00:47:53.240]   Does that match how you think about it?
[00:47:53.240 --> 00:47:53.920]   Yeah.
[00:47:53.920 --> 00:47:54.840]   It's kind of what we do, right?
[00:47:54.840 --> 00:47:57.000]   Do you think there'll ever be another medium?
[00:47:57.000 --> 00:48:01.560]   I just think there's going to be, I think the future of video is not going to be YouTube.
[00:48:01.560 --> 00:48:02.060]   Yeah, you said that.
[00:48:02.060 --> 00:48:03.620]   Or it's not going to be YouTube in its current form.
[00:48:03.620 --> 00:48:06.560]   I think this, this is an extractive technology.
[00:48:06.560 --> 00:48:09.960]   Let's go back to the original deep dive discussion.
[00:48:09.960 --> 00:48:13.640]   YouTube executes now in a way that's sort of an extractive technology.
[00:48:13.640 --> 00:48:16.400]   The additive part was like, hey, I can, it's easy to upload video.
[00:48:16.400 --> 00:48:16.800]   It's free.
[00:48:16.800 --> 00:48:17.520]   It's free to watch.
[00:48:17.520 --> 00:48:20.100]   And there's like a lot of really interesting stuff here, a lot of innovation.
[00:48:20.100 --> 00:48:23.440]   The extractive part is now it's algorithms are pushing you.
[00:48:23.440 --> 00:48:26.960]   They want you to get stuck in these rabbit holes of just following content that just like
[00:48:26.960 --> 00:48:28.320]   vaguely keeps you engaged.
[00:48:28.320 --> 00:48:31.840]   That makes them more money, but that doesn't necessarily make the experience better.
[00:48:31.840 --> 00:48:35.480]   So I think video dominates audio historically.
[00:48:35.480 --> 00:48:37.400]   We love visual input.
[00:48:37.400 --> 00:48:39.180]   We like it more than audio input.
[00:48:39.180 --> 00:48:40.160]   Radio is awesome.
[00:48:40.160 --> 00:48:42.900]   TV ate its lunch, even though it's more expensive, less convenient.
[00:48:42.900 --> 00:48:46.360]   And when it first came out, it was like a really small black and white screen.
[00:48:46.360 --> 00:48:47.020]   You could barely see.
[00:48:47.020 --> 00:48:47.560]   Didn't matter.
[00:48:47.560 --> 00:48:48.340]   That's what we want.
[00:48:48.340 --> 00:48:49.940]   I'd rather do that than listen.
[00:48:49.940 --> 00:48:51.440]   We want to see people.
[00:48:51.860 --> 00:48:55.740]   So this is one of the reasons why we've always committed to video, even if there's parts
[00:48:55.740 --> 00:48:59.140]   about YouTube we don't like, video has got to be the future.
[00:48:59.140 --> 00:49:00.300]   And I don't know what it's going to be.
[00:49:00.300 --> 00:49:06.440]   Some sort of diversification of streaming so that like in addition to you have like indie
[00:49:06.440 --> 00:49:07.540]   streaming, that might be it.
[00:49:07.540 --> 00:49:10.300]   Like a Netflix type experience that you curate.
[00:49:10.300 --> 00:49:12.900]   It's episodes and carousels, pretty high production quality.
[00:49:13.300 --> 00:49:16.000]   And you're curating and it's stuff you really, really like.
[00:49:16.000 --> 00:49:17.420]   And it's made from independent creators.
[00:49:17.420 --> 00:49:21.800]   And so instead of it's Netflix spending $8 million to produce a show, it might be someone
[00:49:21.800 --> 00:49:23.660]   who spent $100,000 to produce a show.
[00:49:23.660 --> 00:49:28.400]   And but the economics work out and it's not a recommendation based environment.
[00:49:28.400 --> 00:49:33.020]   It's more like a Netflix subscription based service with, you know, you find your niches.
[00:49:33.020 --> 00:49:37.660]   I don't like I don't think the hopping around from random video clips that are only connected
[00:49:37.660 --> 00:49:38.280]   through algorithms.
[00:49:38.280 --> 00:49:40.340]   That's not the optimal way to do video.
[00:49:40.420 --> 00:49:45.100]   I think people like the idea of coherence, there's something about the parts of the linear
[00:49:45.100 --> 00:49:46.240]   model that I think are good.
[00:49:46.240 --> 00:49:52.460]   Not the ads, not the I have to watch the show when it comes on, but something about, oh, I
[00:49:52.460 --> 00:49:53.420]   like this channel.
[00:49:53.420 --> 00:49:57.560]   And there's like content on this channel, there's shows on this channel and some of them I know
[00:49:57.560 --> 00:50:00.800]   and look forward to and other shows I learn about because they're on this channel and I
[00:50:00.800 --> 00:50:01.800]   like their point of view.
[00:50:01.800 --> 00:50:04.620]   That is a, that's a pretty good model.
[00:50:04.620 --> 00:50:07.000]   And right now we don't have it right now.
[00:50:07.000 --> 00:50:10.040]   It's just, you're going to jump from this to something completely unrelated to something else
[00:50:10.040 --> 00:50:10.780]   completely unrelated.
[00:50:10.780 --> 00:50:14.740]   There's a lot of context shifting, a lot of, it makes it hard to be loyal towards a
[00:50:14.740 --> 00:50:16.800]   particular like brand or just style of creation.
[00:50:16.800 --> 00:50:21.680]   So I don't think YouTube as it is, is a great fit for what people want.
[00:50:21.680 --> 00:50:26.440]   It's what their lizard brain wants to produce a lot of money, but I think there's going to
[00:50:26.440 --> 00:50:27.740]   be a future of independent video.
[00:50:27.740 --> 00:50:28.360]   It's going to be huge.
[00:50:28.500 --> 00:50:32.620]   And I want to make sure that we are ready for it because I have a lot of pants list, Halloween
[00:50:32.620 --> 00:50:35.740]   decoration, TikTok videos that we got to get to the masses.
[00:50:35.740 --> 00:50:37.440]   We've got to get those spread.
[00:50:37.440 --> 00:50:38.060]   All right.
[00:50:38.060 --> 00:50:38.880]   Do we have a call this week?
[00:50:38.880 --> 00:50:39.440]   We do.
[00:50:39.440 --> 00:50:39.740]   All right.
[00:50:39.740 --> 00:50:40.360]   Let's hear this.
[00:50:41.560 --> 00:50:42.100]   Hi, Cal.
[00:50:42.100 --> 00:50:43.020]   My name's Jonathan.
[00:50:43.020 --> 00:50:47.120]   I'm a cardiology resident based in London, and I'm about to start a PhD, which I think
[00:50:47.120 --> 00:50:51.120]   will mark a significant shift in the way that I go about my work on a day-to-day basis.
[00:50:51.620 --> 00:50:56.000]   I've followed your work for many years, and I wondered if you had any advice for someone
[00:50:56.000 --> 00:51:04.300]   who was going from a very reactive, busy environment to an academic setting, how I could develop
[00:51:04.300 --> 00:51:10.840]   systems to better manage my time, and how I could shift my mode of thinking from the hospital
[00:51:10.840 --> 00:51:11.840]   into academia.
[00:51:11.840 --> 00:51:12.760]   Thank you.
[00:51:12.760 --> 00:51:14.080]   All right.
[00:51:14.080 --> 00:51:15.160]   Here's what I think is going to happen.
[00:51:15.160 --> 00:51:20.360]   I don't know what subject you're studying as a doctoral student, but almost certainly what
[00:51:20.360 --> 00:51:27.840]   you're going to find is that being a cardiology resident is so much harder than being a doctoral
[00:51:27.840 --> 00:51:31.760]   student that you're going to worry that something is wrong, and you're going to invent work to
[00:51:31.760 --> 00:51:32.680]   try to make your life worse.
[00:51:32.680 --> 00:51:35.160]   That is the number one thing I am afraid of for you.
[00:51:35.160 --> 00:51:40.340]   It is not hard to be a doctoral student in the same way that it's hard to be a resident.
[00:51:40.340 --> 00:51:41.620]   It's hard in other ways, right?
[00:51:41.620 --> 00:51:46.700]   It's hard in the sense of you have to produce like original thought, and if you can't, you're
[00:51:46.700 --> 00:51:47.220]   out of luck.
[00:51:47.220 --> 00:51:50.400]   It's hard in the sense that you can't just hard work your way to success.
[00:51:50.400 --> 00:51:54.740]   It's hard in the sense that it might not go well, or you might turn out you don't like
[00:51:54.740 --> 00:51:54.820]   it.
[00:51:54.820 --> 00:51:58.780]   It's a very rarefied, weird, specific, eccentric type of thing to do.
[00:51:58.780 --> 00:52:00.400]   And for some people, it just doesn't fit.
[00:52:00.400 --> 00:52:03.940]   And for some people, they love the idea, but it's just not the way their brain works, so
[00:52:03.940 --> 00:52:06.600]   they don't have the right preparation for it.
[00:52:06.700 --> 00:52:12.380]   It could be very hit or miss, but it's not hard in the sense of I'm on like hour 16 of
[00:52:12.380 --> 00:52:16.120]   a shift doing really difficult life and death work, and I have an eight-hour shift coming
[00:52:16.120 --> 00:52:16.600]   up next.
[00:52:16.600 --> 00:52:20.260]   It's going to seem easy, and you're going to feel like you should be doing more work,
[00:52:20.260 --> 00:52:21.200]   and that's okay.
[00:52:21.200 --> 00:52:22.240]   Take a breather.
[00:52:22.240 --> 00:52:27.100]   What you were just doing was more physically demanding, was probably more mentally exhausting.
[00:52:27.100 --> 00:52:28.500]   That's okay.
[00:52:28.500 --> 00:52:30.140]   Do your courses.
[00:52:30.140 --> 00:52:30.620]   Do well.
[00:52:30.620 --> 00:52:31.080]   They're easy.
[00:52:31.160 --> 00:52:32.860]   They want you to do well in the doctoral program.
[00:52:32.860 --> 00:52:33.840]   Focus on research.
[00:52:33.840 --> 00:52:35.860]   Work with the best advisor you can.
[00:52:35.860 --> 00:52:39.700]   Work backwards always from the question of how do I get from here to producing stuff that
[00:52:39.700 --> 00:52:40.660]   matters that people cite.
[00:52:40.660 --> 00:52:41.460]   That's all that matters.
[00:52:41.460 --> 00:52:43.180]   Get into the process of research.
[00:52:43.180 --> 00:52:43.640]   Love it.
[00:52:43.640 --> 00:52:44.480]   Master topics.
[00:52:44.480 --> 00:52:46.160]   Be around people doing research.
[00:52:46.160 --> 00:52:47.480]   Be around really good researchers.
[00:52:47.480 --> 00:52:48.920]   Build up your affinity for it.
[00:52:48.920 --> 00:52:50.700]   And also just enjoy your free time.
[00:52:50.700 --> 00:52:51.980]   You better get in pretty good shape.
[00:52:51.980 --> 00:52:52.780]   You have a lot of time.
[00:52:52.780 --> 00:52:54.260]   I got in good shape as a doctoral student.
[00:52:54.260 --> 00:52:56.220]   You better have a hobby you get really good at.
[00:52:56.220 --> 00:52:57.040]   You have a lot of time.
[00:52:57.040 --> 00:52:58.100]   Don't waste that opportunity.
[00:52:58.100 --> 00:53:00.640]   So don't, I mean, that's my main worry.
[00:53:00.860 --> 00:53:03.740]   Don't fear if it feels too easy compared to what you were just doing.
[00:53:03.740 --> 00:53:06.660]   That's how it's supposed to feel.
[00:53:06.660 --> 00:53:10.560]   If you came from doing something like being a resident, you'll be okay and enjoy it.
[00:53:10.560 --> 00:53:12.060]   All right.
[00:53:12.060 --> 00:53:14.280]   Well, we want to get into the books I read last month.
[00:53:14.280 --> 00:53:17.400]   But before we get to that final segment, we'll do something even more exciting.
[00:53:17.400 --> 00:53:19.400]   Let's hear from a sponsor.
[00:53:19.400 --> 00:53:22.820]   Jesse, let me tell you something that I'm not very good at.
[00:53:22.820 --> 00:53:23.620]   I think you'll agree.
[00:53:23.620 --> 00:53:26.280]   The process of hiring people.
[00:53:26.280 --> 00:53:27.560]   I'm not sure.
[00:53:27.560 --> 00:53:28.940]   Do you remember how I hired you?
[00:53:28.940 --> 00:53:30.420]   I do.
[00:53:30.560 --> 00:53:31.240]   Yeah, true story.
[00:53:31.240 --> 00:53:31.960]   I dressed up.
[00:53:31.960 --> 00:53:35.380]   I was outside of Motor Cat here in Tacoma Park, dressed up like a police detective.
[00:53:35.380 --> 00:53:36.960]   Saw Jesse walking by.
[00:53:36.960 --> 00:53:42.620]   Sir, there was a larceny, grand larceny, third degree.
[00:53:42.620 --> 00:53:43.640]   Heard in this restaurant.
[00:53:43.640 --> 00:53:44.200]   We need a question.
[00:53:44.200 --> 00:53:45.120]   We set up a little room up here.
[00:53:45.120 --> 00:53:45.820]   See what you saw.
[00:53:45.820 --> 00:53:47.280]   We're trying to piece together what happened.
[00:53:47.280 --> 00:53:48.340]   So Jesse came up.
[00:53:48.340 --> 00:53:49.480]   A little nervous, right?
[00:53:49.480 --> 00:53:50.480]   He's like, okay, I'm not in trouble.
[00:53:50.580 --> 00:53:53.500]   We're like, no, no, we just want to, we want to just see what you saw.
[00:53:53.500 --> 00:53:56.040]   We're trying to, we're trying to collaborate, corroborate different stories here.
[00:53:56.040 --> 00:53:56.780]   Brought Jesse up.
[00:53:56.780 --> 00:53:58.960]   Came up here to the, to the HQ.
[00:53:59.700 --> 00:54:02.700]   And he was like, what, why are there so many Halloween decorations up here?
[00:54:02.700 --> 00:54:04.280]   And I was like, welcome to the rest of your life.
[00:54:04.280 --> 00:54:05.140]   Chain that door.
[00:54:05.140 --> 00:54:07.240]   Jesse's never left these offices.
[00:54:07.240 --> 00:54:09.600]   Now that's one way to hire.
[00:54:09.600 --> 00:54:11.280]   I don't recommend it.
[00:54:11.800 --> 00:54:15.180]   What I do recommend is using Indeed.
[00:54:15.180 --> 00:54:18.080]   When it comes to hiring, this is something I just thought of.
[00:54:18.080 --> 00:54:20.600]   Indeed is all you need.
[00:54:20.600 --> 00:54:23.560]   Stop struggling to get your job posts seen by other job sites.
[00:54:23.560 --> 00:54:28.080]   You need to use Indeed's sponsored jobs to help you stand out and hire fast.
[00:54:28.080 --> 00:54:29.600]   This really works.
[00:54:29.600 --> 00:54:37.260]   Like they have data that shows that their sponsored job posts have 50 or 45 rather percent more applicants
[00:54:37.260 --> 00:54:38.400]   than the non-sponsored jobs.
[00:54:38.400 --> 00:54:41.540]   So right away, you're getting a big boost in the applications you get.
[00:54:42.200 --> 00:54:46.140]   Plus with Indeed's sponsored jobs, there are no monthly subscriptions, no long-term contracts.
[00:54:46.140 --> 00:54:48.020]   You only pay for results.
[00:54:48.020 --> 00:54:50.080]   There's no need to wait any longer.
[00:54:50.080 --> 00:54:52.560]   Speed up your hiring right now with Indeed.
[00:54:52.560 --> 00:54:58.900]   And listeners of this show are going to get a $75 sponsored job credit to get your job more
[00:54:58.900 --> 00:55:03.100]   visibility if they go to Indeed.com slash deep.
[00:55:03.100 --> 00:55:07.940]   Just go to Indeed.com slash deep right now and support our show by saying that you heard
[00:55:07.940 --> 00:55:09.940]   about Indeed on this podcast.
[00:55:10.580 --> 00:55:11.840]   Indeed.com slash deep.
[00:55:11.840 --> 00:55:13.420]   Terms and conditions apply.
[00:55:13.420 --> 00:55:17.100]   Hiring Indeed is all you need.
[00:55:17.100 --> 00:55:21.580]   Hey, guys in my audience, wouldn't you like to look a little younger?
[00:55:21.580 --> 00:55:23.880]   Look, you got two options to do so.
[00:55:23.880 --> 00:55:31.840]   One, you can carry a skateboard and use the terms fire and Ohio all the time.
[00:55:32.540 --> 00:55:33.380]   That's what Jesse does.
[00:55:33.380 --> 00:55:36.040]   Or two, you can take care of your skin.
[00:55:36.040 --> 00:55:39.980]   And if you're going with option two, which is what I recommend, you need to check out the
[00:55:39.980 --> 00:55:43.140]   offerings from our sponsor, Caldera Labs.
[00:55:43.140 --> 00:55:46.540]   Their high-performance skincare is designed specifically for men.
[00:55:47.100 --> 00:55:49.260]   It's simple, effective, and backed by science.
[00:55:49.260 --> 00:55:53.700]   In a consumer study, 100% of men said their skin looks smoother and healthier.
[00:55:53.700 --> 00:55:57.500]   96.9% noticed improved hydration and texture.
[00:55:57.500 --> 00:56:01.900]   And 93.8% reported a more youthful experience.
[00:56:02.660 --> 00:56:08.180]   Their products include The Good, which is an award-winning serum packed with 27 botanicals,
[00:56:08.180 --> 00:56:13.000]   and The Eye Serum, which helped reduce the appearance of tired eyes, dark circles, and puffiness.
[00:56:13.000 --> 00:56:19.180]   The one I particularly like and think all men need is The Base Layer, which is a daily moisturizer
[00:56:19.180 --> 00:56:21.720]   to keep your skin hydrated without the shine.
[00:56:21.720 --> 00:56:24.000]   Jesse, I didn't know you were supposed to hydrate your skin.
[00:56:24.000 --> 00:56:28.260]   I think women figure this out, like, roughly around the time they acquire language.
[00:56:28.260 --> 00:56:30.460]   I think I first heard about this, like, last week.
[00:56:30.460 --> 00:56:31.520]   So let me get the word out.
[00:56:31.520 --> 00:56:32.940]   The men out there, you need to hydrate your skin.
[00:56:32.940 --> 00:56:35.320]   Use The Base Layer from Caldera Labs.
[00:56:35.320 --> 00:56:39.200]   Skincare doesn't have to be complicated, but it should be good.
[00:56:39.200 --> 00:56:44.180]   Upgrade your routine with Caldera Lab and see the difference for yourself.
[00:56:44.180 --> 00:56:49.980]   Go to calderalab.com slash deep and use deep at checkout for 20% off your first order.
[00:56:49.980 --> 00:56:50.700]   All right, Jesse.
[00:56:50.700 --> 00:56:54.340]   The order of our business next—this is really smooth.
[00:56:54.340 --> 00:56:58.200]   The order of our next business is our final segment.
[00:56:58.200 --> 00:57:00.040]   I nailed that.
[00:57:00.040 --> 00:57:00.880]   You did.
[00:57:00.880 --> 00:57:01.200]   I did.
[00:57:01.200 --> 00:57:02.960]   That was just—that was smooth broadcasting.
[00:57:02.960 --> 00:57:05.340]   I know Howard Stern is retiring, right?
[00:57:05.340 --> 00:57:07.020]   I didn't hear that.
[00:57:07.020 --> 00:57:07.840]   I just heard that.
[00:57:07.840 --> 00:57:11.240]   And I don't want to make you nervous, but after that last transition,
[00:57:11.240 --> 00:57:13.880]   they might be coming to me to take that job.
[00:57:13.880 --> 00:57:15.080]   He works for Sirius with Mad Dog.
[00:57:15.080 --> 00:57:16.380]   That's right.
[00:57:16.380 --> 00:57:18.600]   And he's paid a ton of money.
[00:57:18.600 --> 00:57:21.640]   I'm going to say this, and I don't want to speak out of turn,
[00:57:21.640 --> 00:57:23.920]   and maybe I'm exaggerating, but I would guess
[00:57:23.920 --> 00:57:25.900]   Howard Stern makes more money than we do.
[00:57:25.900 --> 00:57:29.180]   His show is more profitable than ours, if I had to guess.
[00:57:29.180 --> 00:57:31.980]   All right, so I like to—
[00:57:31.980 --> 00:57:35.300]   First episode of every month, or second episode because I forgot.
[00:57:35.300 --> 00:57:37.700]   I like to talk about the books I read in the past month.
[00:57:37.700 --> 00:57:38.440]   I have my list here.
[00:57:38.760 --> 00:57:40.700]   My goal is always to read five books.
[00:57:40.700 --> 00:57:45.360]   Helps keep me on track with keeping my mind sharp in this increasingly distracted world.
[00:57:45.360 --> 00:57:48.640]   Talked about before, my strategy is just to mix in books.
[00:57:48.640 --> 00:57:50.080]   You just sound fascinating.
[00:57:50.080 --> 00:57:50.700]   Go random.
[00:57:50.700 --> 00:57:51.400]   Go weird.
[00:57:51.400 --> 00:57:52.640]   Hard books.
[00:57:52.640 --> 00:57:53.120]   Easy books.
[00:57:53.120 --> 00:57:53.640]   Cool books.
[00:57:53.640 --> 00:57:54.100]   Weird books.
[00:57:54.100 --> 00:57:54.520]   I don't care.
[00:57:54.520 --> 00:57:55.300]   Reading's good.
[00:57:55.560 --> 00:57:57.960]   All right, here are the books I read in July 2025.
[00:57:57.960 --> 00:58:02.520]   These were essentially all read during my trip up north.
[00:58:02.520 --> 00:58:08.980]   The first book, and I don't know, you know, what Joker gave this one to me,
[00:58:08.980 --> 00:58:12.540]   but the first book I read was L.A. Story by Bill Plunkett.
[00:58:12.540 --> 00:58:13.780]   I don't know where I got that book, Jesse.
[00:58:13.780 --> 00:58:17.160]   That was a birthday gift from Jesse.
[00:58:17.160 --> 00:58:19.400]   I knew very little about Shoyotani.
[00:58:19.400 --> 00:58:24.920]   I think I'd basically been blacklisting the Dodgers ever since they beat us in the NLDS,
[00:58:24.920 --> 00:58:27.020]   whenever that was, like 2014.
[00:58:27.020 --> 00:58:28.540]   So I didn't know the story.
[00:58:28.540 --> 00:58:29.760]   Like, I did not know the details.
[00:58:29.760 --> 00:58:34.820]   Turns out, and this is a spoiler alert, Shoyotani is good at baseball.
[00:58:34.820 --> 00:58:36.860]   This is one of the things I learned from that book.
[00:58:36.860 --> 00:58:40.040]   The next book I read, I talked about this in one of my newsletters,
[00:58:40.040 --> 00:58:44.360]   so you may have heard me talk about it, was Disney's Land by Richard Snow.
[00:58:44.360 --> 00:58:48.560]   And it's a book just about Disneyland, because I went to Disneyland in June.
[00:58:48.560 --> 00:58:49.560]   I was like, oh, I want to read about it.
[00:58:49.560 --> 00:58:56.160]   Then I read Ingenious, the new biography of Benjamin Franklin, written by Richard Munson.
[00:58:56.160 --> 00:59:00.020]   It focuses more on his science.
[00:59:00.020 --> 00:59:00.820]   That's kind of interesting.
[00:59:00.820 --> 00:59:05.040]   Often the scientific achievements of Franklin get short shrift,
[00:59:05.040 --> 00:59:05.980]   but it's actually pretty interesting.
[00:59:05.980 --> 00:59:06.600]   I like that book.
[00:59:06.600 --> 00:59:10.440]   Then I listened to an audio, Original Sin.
[00:59:10.440 --> 00:59:13.800]   I think that was Jake Tapper and Alex Thompson wrote that.
[00:59:13.860 --> 00:59:19.600]   This is about Joe Biden and his cognitive decline and what people did or did not know.
[00:59:19.600 --> 00:59:21.680]   Not like a happy, fun book.
[00:59:21.680 --> 00:59:23.720]   That's the way I'd review it.
[00:59:23.720 --> 00:59:24.620]   But interesting.
[00:59:24.620 --> 00:59:26.840]   Things did not go well.
[00:59:26.840 --> 00:59:27.740]   Again, spoiler alert.
[00:59:27.740 --> 00:59:30.600]   That story did not end up well for Joe Biden.
[00:59:31.260 --> 00:59:34.360]   I finally read the book Wool by Hugh Howey.
[00:59:34.360 --> 00:59:37.240]   One of my sons was reading it, and he read it and liked it.
[00:59:37.240 --> 00:59:38.220]   And he recommended it to me.
[00:59:38.220 --> 00:59:38.680]   And I liked it.
[00:59:38.680 --> 00:59:43.800]   It's an originally, I would say self-published book, but that's not even right.
[00:59:43.800 --> 00:59:46.900]   It was, he was just like, I guess it was.
[00:59:46.900 --> 00:59:49.880]   He was publishing parts of it on Kindle Direct, but not all at once.
[00:59:50.700 --> 00:59:53.340]   And it's just a dystopian book in the future.
[00:59:53.340 --> 00:59:58.520]   And there's people living in a giant silo in the ground after some sort of disaster.
[00:59:58.520 --> 01:00:01.280]   And they have all these weird sort of like rules and rituals, and you don't know much
[01:00:01.280 --> 01:00:01.760]   about it.
[01:00:01.760 --> 01:00:04.800]   And then sort of mysteries unveil, and you learn more about what's really going on.
[01:00:04.800 --> 01:00:09.520]   It's like a great, I think it's like a fantastic, kind of like Andy Weir, a little bit turned
[01:00:09.520 --> 01:00:10.360]   down hard sci-fi.
[01:00:10.360 --> 01:00:12.880]   They don't get that much into the details of science like he would have.
[01:00:12.880 --> 01:00:20.180]   But that sort of sense of like neorealistic dystopian sci-fi, very plot driven, very plot
[01:00:20.180 --> 01:00:23.160]   focused, which makes sense because he's writing this in dispatches.
[01:00:23.160 --> 01:00:26.280]   And the plot just rolls, you know, like, I want to know what happens next.
[01:00:26.280 --> 01:00:27.100]   This world is interesting.
[01:00:27.100 --> 01:00:28.000]   I want to know about it.
[01:00:28.000 --> 01:00:29.180]   And it's a great story.
[01:00:29.180 --> 01:00:33.360]   I heard the Hugh Howey story originally wasn't quite accurate.
[01:00:33.360 --> 01:00:37.040]   I heard it as this was like this project he had.
[01:00:37.040 --> 01:00:38.100]   That's how he became a writer.
[01:00:38.100 --> 01:00:38.460]   All right.
[01:00:38.460 --> 01:00:40.420]   It turns out he had published many books.
[01:00:40.720 --> 01:00:45.960]   He had written a bunch of novels and was self-publishing pieces of this as just like a project to help
[01:00:45.960 --> 01:00:47.400]   mourn the death of his dog.
[01:00:47.400 --> 01:00:49.280]   It just was in that mood.
[01:00:49.280 --> 01:00:52.440]   But it was, it just hit a, hit a vein.
[01:00:52.440 --> 01:00:54.100]   Like something about this is right.
[01:00:54.100 --> 01:00:56.240]   And the series, you know, Random House bought it.
[01:00:56.240 --> 01:00:58.740]   Is it related to the Apple show?
[01:00:58.740 --> 01:00:59.260]   Silo?
[01:00:59.260 --> 01:01:00.340]   Silo is based off of it.
[01:01:00.340 --> 01:01:00.680]   Okay.
[01:01:00.680 --> 01:01:01.000]   Yeah.
[01:01:01.000 --> 01:01:02.480]   So Silo is based off of these books here.
[01:01:02.480 --> 01:01:08.140]   So it became a big, you know, premium TV show on, on Apple and the books sold all the
[01:01:08.140 --> 01:01:08.480]   copies.
[01:01:08.480 --> 01:01:10.300]   And so it became very successful.
[01:01:10.300 --> 01:01:12.440]   But he was, I didn't know that piece.
[01:01:12.440 --> 01:01:13.620]   He had written a bunch of books before then.
[01:01:13.620 --> 01:01:15.020]   Did you watch the Apple show?
[01:01:15.020 --> 01:01:17.000]   I'm going to, I'm going to watch with my son who read it.
[01:01:17.000 --> 01:01:20.420]   It was my, my 12 year old doesn't like these books.
[01:01:20.420 --> 01:01:22.180]   He, he likes World War II nonfiction.
[01:01:22.180 --> 01:01:23.640]   Who knows why?
[01:01:24.960 --> 01:01:27.000]   But my 10 year old loves sci-fi.
[01:01:27.000 --> 01:01:28.440]   So I'm going to watch with him.
[01:01:28.440 --> 01:01:30.640]   But I figure I, he read the whole series.
[01:01:30.640 --> 01:01:33.080]   I only read the first one, but I should probably check out the show.
[01:01:33.080 --> 01:01:36.460]   I guess it's, if he read the book, I guess I shouldn't be too worried about the show, but
[01:01:36.460 --> 01:01:37.000]   I'll check it out.
[01:01:37.000 --> 01:01:42.800]   Just to be, not a lot of like beheadings in the first opening or something like that.
[01:01:42.800 --> 01:01:44.320]   So I'll, I'll check that out.
[01:01:45.020 --> 01:01:46.860]   Um, there you go.
[01:01:46.860 --> 01:01:47.540]   Whoa.
[01:01:47.540 --> 01:01:48.640]   I'm going to read the second one.
[01:01:48.640 --> 01:01:49.280]   Shift.
[01:01:49.280 --> 01:01:51.600]   I haven't got to it, but I'm going to, I'm going to get to it.
[01:01:51.600 --> 01:01:52.200]   It's on my list.
[01:01:52.200 --> 01:01:53.300]   It's all plot.
[01:01:53.300 --> 01:01:53.980]   It's just good plot.
[01:01:53.980 --> 01:01:55.140]   Just, Hey, that's a cool world.
[01:01:55.140 --> 01:01:55.980]   The plot moves.
[01:01:55.980 --> 01:01:56.820]   You want to know what happens.
[01:01:56.820 --> 01:01:57.940]   Characters are fine.
[01:01:57.940 --> 01:01:59.200]   You like care about the plot.
[01:01:59.200 --> 01:02:01.740]   Sometimes when you're on vacation, that's the type of book you need.
[01:02:01.740 --> 01:02:02.680]   All right.
[01:02:02.680 --> 01:02:07.660]   Speaking of vacation, I think that's all the time we have for today, but we might, speaking
[01:02:07.660 --> 01:02:10.860]   of vacation, we might be doing a, something special next week, right?
[01:02:11.420 --> 01:02:16.120]   Yeah, so you might notice the next week's episode might be a little bit different because
[01:02:16.120 --> 01:02:19.280]   both Jesse and I are away at the same time at a skeleton convention.
[01:02:19.280 --> 01:02:21.240]   Um, so we might do something different.
[01:02:21.240 --> 01:02:22.060]   So stay tuned for that.
[01:02:22.060 --> 01:02:24.320]   Just for one episode, then we'll be back into our fall season.
[01:02:24.320 --> 01:02:25.560]   I call it a season.
[01:02:25.560 --> 01:02:27.640]   It sounds more efficient, Jesse, but we just do these all the time.
[01:02:27.640 --> 01:02:29.680]   But the fall season, be excited.
[01:02:29.680 --> 01:02:31.360]   Special guests will be here.
[01:02:31.360 --> 01:02:34.280]   So until then, as always stay deep.
[01:02:34.280 --> 01:02:40.620]   Hey, if you liked today's discussion of additive versus extractive technologies, you might
[01:02:40.620 --> 01:02:46.100]   also like episode 363, which was about escaping the digital doldrums.
[01:02:46.100 --> 01:02:46.700]   Check it out.
[01:02:46.700 --> 01:02:47.580]   I think you'll like it.
[01:02:47.580 --> 01:02:53.200]   I'm going to start with a little known story from the life of Walt Disney that I came across
[01:02:53.200 --> 01:02:57.040]   recently while reading Richard Snow's book, Disney's Land.
[01:02:57.040 --> 01:03:04.780]   I am then going to connect this tale from the 1940s to our current moment in the 2020s.
[01:03:04.780 --> 01:03:10.520]   And I am going to extract from it a useful strategy for trying to combat that all too
[01:03:10.520 --> 01:03:16.460]   common sense that we are sleepwalking through life subservient to our screens.

