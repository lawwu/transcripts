
[00:00:00.000 --> 00:00:01.840]   A lot of people have said for many years
[00:00:01.840 --> 00:00:06.320]   that there will come a time when we want to pause a little bit.
[00:00:06.320 --> 00:00:07.200]   That time is now.
[00:00:07.200 --> 00:00:13.600]   The following is a conversation with Max Tegmark,
[00:00:13.600 --> 00:00:15.440]   his third time on the podcast.
[00:00:15.440 --> 00:00:18.800]   In fact, his first appearance was episode number one
[00:00:18.800 --> 00:00:20.560]   of this very podcast.
[00:00:20.560 --> 00:00:24.000]   He is a physicist and artificial intelligence researcher
[00:00:24.000 --> 00:00:27.120]   at MIT, co-founder of Future of Life Institute,
[00:00:27.120 --> 00:00:31.400]   and author of Life 3.0, Being Human in the Age
[00:00:31.400 --> 00:00:33.480]   of Artificial Intelligence.
[00:00:33.480 --> 00:00:36.320]   Most recently, he's a key figure in spearheading
[00:00:36.320 --> 00:00:39.120]   the open letter calling for a six-month pause
[00:00:39.120 --> 00:00:43.960]   on giant AI experiments, like training GPT-4.
[00:00:43.960 --> 00:00:47.440]   The letter reads, "We're calling for a pause
[00:00:47.440 --> 00:00:51.880]   on training of models larger than GPT-4 for six months.
[00:00:51.880 --> 00:00:55.000]   This does not imply a pause or ban on all AI research
[00:00:55.000 --> 00:00:58.080]   and development or the use of systems that have already
[00:00:58.080 --> 00:00:59.800]   been placed on the market.
[00:00:59.800 --> 00:01:02.440]   Our call is specific and addresses
[00:01:02.440 --> 00:01:06.920]   a very small pool of actors who possesses this capability."
[00:01:06.920 --> 00:01:09.960]   The letter has been signed by over 50,000 individuals,
[00:01:09.960 --> 00:01:14.480]   including 1,800 CEOs and over 1,500 professors.
[00:01:14.480 --> 00:01:17.840]   Signatories include Yoshua Bengio, Stuart Russell,
[00:01:17.840 --> 00:01:21.880]   Elon Musk, Steve Wozniak, Yuval Noah Harari, Andrew Yang,
[00:01:21.880 --> 00:01:23.600]   and many others.
[00:01:23.600 --> 00:01:26.040]   This is a defining moment in the history
[00:01:26.040 --> 00:01:29.000]   of human civilization, where the balance of power
[00:01:29.000 --> 00:01:32.880]   between human and AI begins to shift.
[00:01:32.880 --> 00:01:36.760]   And Max's mind and his voice is one of the most valuable
[00:01:36.760 --> 00:01:39.520]   and powerful in a time like this.
[00:01:39.520 --> 00:01:43.680]   His support, his wisdom, his friendship has been a gift
[00:01:43.680 --> 00:01:46.720]   I'm forever deeply grateful for.
[00:01:46.720 --> 00:01:48.840]   This is the Alex Friedman Podcast.
[00:01:48.840 --> 00:01:50.680]   To support it, please check out our sponsors
[00:01:50.680 --> 00:01:51.840]   in the description.
[00:01:51.840 --> 00:01:55.600]   And now, dear friends, here's Max Tegmark.
[00:01:55.600 --> 00:01:59.280]   You were the first ever guest on this podcast,
[00:01:59.280 --> 00:02:00.600]   episode number one.
[00:02:00.600 --> 00:02:03.760]   So first of all, Max, I just have to say,
[00:02:03.760 --> 00:02:05.240]   thank you for giving me a chance.
[00:02:05.240 --> 00:02:06.640]   Thank you for starting this journey.
[00:02:06.640 --> 00:02:07.840]   It's been an incredible journey.
[00:02:07.840 --> 00:02:11.200]   Just thank you for sitting down with me
[00:02:11.200 --> 00:02:14.280]   and just acting like I'm somebody who matters,
[00:02:14.280 --> 00:02:16.680]   that I'm somebody who's interesting to talk to.
[00:02:16.680 --> 00:02:18.880]   And thank you for doing it.
[00:02:18.880 --> 00:02:19.920]   That meant a lot.
[00:02:20.200 --> 00:02:24.280]   - Thanks to you for putting your heart and soul into this.
[00:02:24.280 --> 00:02:26.880]   I know when you delve into controversial topics,
[00:02:26.880 --> 00:02:30.720]   it's inevitable to get hit by what Hamlet talks about,
[00:02:30.720 --> 00:02:32.360]   the slings and arrows and stuff.
[00:02:32.360 --> 00:02:33.800]   And I really admire this.
[00:02:33.800 --> 00:02:37.400]   It's in an era where YouTube videos are too long
[00:02:37.400 --> 00:02:39.960]   and now it has to be like a 20-minute TikTok,
[00:02:39.960 --> 00:02:41.880]   20-second TikTok clip.
[00:02:41.880 --> 00:02:44.280]   It's just so refreshing to see you going exactly
[00:02:44.280 --> 00:02:47.520]   against all of the advice and doing these really long form
[00:02:47.520 --> 00:02:49.720]   things and the people appreciate it.
[00:02:49.720 --> 00:02:51.360]   Reality is nuanced.
[00:02:51.360 --> 00:02:55.840]   And thanks for sharing it that way.
[00:02:55.840 --> 00:02:57.280]   - So let me ask you again,
[00:02:57.280 --> 00:02:59.520]   the first question I've ever asked on this podcast,
[00:02:59.520 --> 00:03:02.200]   episode number one, talking to you,
[00:03:02.200 --> 00:03:04.920]   do you think there's intelligent life out there
[00:03:04.920 --> 00:03:05.840]   in the universe?
[00:03:05.840 --> 00:03:07.320]   Let's revisit that question.
[00:03:07.320 --> 00:03:08.800]   Do you have any updates?
[00:03:08.800 --> 00:03:12.360]   What's your view when you look out to the stars?
[00:03:12.360 --> 00:03:14.120]   - So when we look out to the stars,
[00:03:14.120 --> 00:03:18.920]   if you define our universe the way most astrophysicists do,
[00:03:18.920 --> 00:03:22.560]   not as all of space, but the spherical region of space
[00:03:22.560 --> 00:03:23.880]   that we can see with our telescopes,
[00:03:23.880 --> 00:03:25.920]   from which light has a time to reach us,
[00:03:25.920 --> 00:03:29.400]   since our Big Bang, I'm in the minority.
[00:03:29.400 --> 00:03:34.360]   I estimate that we are the only life
[00:03:34.360 --> 00:03:39.360]   in this spherical volume that has invented internet,
[00:03:39.360 --> 00:03:41.600]   radios, gotten our level of tech.
[00:03:41.600 --> 00:03:43.680]   And if that's true,
[00:03:43.680 --> 00:03:47.880]   then it puts a lot of responsibility on us
[00:03:47.880 --> 00:03:49.960]   to not mess this one up.
[00:03:49.960 --> 00:03:54.400]   Because if it's true, it means that life is quite rare.
[00:03:54.400 --> 00:03:58.160]   And we are stewards of this one spark
[00:03:58.160 --> 00:04:01.320]   of advanced consciousness, which if we nurture it
[00:04:01.320 --> 00:04:05.440]   and help it grow, eventually life can spread from here
[00:04:05.440 --> 00:04:06.800]   out into much of our universe.
[00:04:06.800 --> 00:04:08.560]   And we can have this just amazing future.
[00:04:08.560 --> 00:04:11.360]   Whereas if we instead are reckless
[00:04:11.360 --> 00:04:14.080]   with the technology we build and just snuff it out
[00:04:14.080 --> 00:04:17.360]   due to stupidity or infighting,
[00:04:17.360 --> 00:04:22.360]   then maybe the rest of cosmic history in our universe
[00:04:22.360 --> 00:04:24.800]   is just gonna be a play for empty benches.
[00:04:24.800 --> 00:04:28.960]   But I do think that we are actually very likely
[00:04:28.960 --> 00:04:33.240]   to get visited by aliens, alien intelligence quite soon.
[00:04:33.240 --> 00:04:34.560]   But I think we are gonna be building
[00:04:34.560 --> 00:04:36.680]   that alien intelligence.
[00:04:36.680 --> 00:04:40.600]   - So we're going to give birth
[00:04:40.600 --> 00:04:43.400]   to an intelligent alien civilization.
[00:04:44.200 --> 00:04:47.200]   Unlike anything that human,
[00:04:47.200 --> 00:04:49.620]   that evolution here on Earth was able to create
[00:04:49.620 --> 00:04:52.800]   in terms of the path, the biological path it took.
[00:04:52.800 --> 00:04:56.000]   - Yeah, and it's gonna be much more alien
[00:04:56.000 --> 00:05:00.920]   than a cat or even the most exotic animal
[00:05:00.920 --> 00:05:02.720]   on the planet right now.
[00:05:02.720 --> 00:05:05.480]   Because it will not have been created
[00:05:05.480 --> 00:05:07.620]   through the usual Darwinian competition
[00:05:07.620 --> 00:05:11.080]   where it necessarily cares about self-preservation,
[00:05:11.080 --> 00:05:15.120]   afraid of death, any of those things.
[00:05:15.120 --> 00:05:18.600]   The space of alien minds that you can build
[00:05:18.600 --> 00:05:22.100]   is just so much vaster than what evolution will give you.
[00:05:22.100 --> 00:05:24.880]   And with that also comes a great responsibility
[00:05:24.880 --> 00:05:28.240]   for us to make sure that the kind of minds we create
[00:05:28.240 --> 00:05:32.240]   are the kind of minds that it's good to create.
[00:05:32.240 --> 00:05:36.520]   Minds that will share our values
[00:05:36.520 --> 00:05:39.480]   and be good for humanity and life.
[00:05:39.480 --> 00:05:42.020]   And also create minds that don't suffer.
[00:05:42.020 --> 00:05:46.900]   - Do you try to visualize the full space
[00:05:46.900 --> 00:05:49.800]   of alien minds that AI could be?
[00:05:49.800 --> 00:05:51.920]   Do you try to consider all the different kinds
[00:05:51.920 --> 00:05:55.480]   of intelligences, sort of generalizing
[00:05:55.480 --> 00:05:58.600]   what humans are able to do to the full spectrum
[00:05:58.600 --> 00:06:01.920]   of what intelligent creatures, entities could do?
[00:06:01.920 --> 00:06:05.180]   - I try, but I would say I fail.
[00:06:05.180 --> 00:06:08.800]   I mean, it's very difficult for a human mind
[00:06:08.800 --> 00:06:13.800]   to really grapple with something so completely alien.
[00:06:13.800 --> 00:06:17.040]   I mean, even for us, right?
[00:06:17.040 --> 00:06:18.680]   If we just try to imagine, how would it feel
[00:06:18.680 --> 00:06:23.480]   if we were completely indifferent towards death
[00:06:23.480 --> 00:06:25.060]   or individuality?
[00:06:25.060 --> 00:06:29.320]   Even if you just imagine that, for example,
[00:06:29.320 --> 00:06:34.520]   you could just copy my knowledge of how to speak Swedish.
[00:06:34.520 --> 00:06:36.120]   Boom, now you can speak Swedish.
[00:06:37.280 --> 00:06:39.680]   And you could copy any of my cool experiences
[00:06:39.680 --> 00:06:41.160]   and then you could delete the ones you didn't like
[00:06:41.160 --> 00:06:43.160]   in your own life, just like that.
[00:06:43.160 --> 00:06:45.800]   It would already change quite a lot
[00:06:45.800 --> 00:06:48.520]   about how you feel as a human being, right?
[00:06:48.520 --> 00:06:51.680]   You probably spend less effort studying things
[00:06:51.680 --> 00:06:52.600]   if you just copy them.
[00:06:52.600 --> 00:06:54.660]   And you might be less afraid of death
[00:06:54.660 --> 00:06:58.300]   because if the plane you're on starts to crash,
[00:06:58.300 --> 00:07:01.600]   you'd just be like, "Oh, shucks, I haven't backed
[00:07:01.600 --> 00:07:04.640]   "my brain up for four hours.
[00:07:04.640 --> 00:07:08.320]   "So I'm gonna lose all these wonderful experiences
[00:07:08.320 --> 00:07:09.700]   "of this flight."
[00:07:09.700 --> 00:07:16.140]   We might also start feeling more compassionate,
[00:07:16.140 --> 00:07:18.560]   maybe with other people, if we can so readily share
[00:07:18.560 --> 00:07:20.840]   each other's experiences and our knowledge
[00:07:20.840 --> 00:07:23.480]   and feel more like a hive mind.
[00:07:23.480 --> 00:07:24.760]   It's very hard, though.
[00:07:24.760 --> 00:07:29.760]   I really feel very humble about this, to grapple with it,
[00:07:29.760 --> 00:07:33.080]   how it might actually feel.
[00:07:33.080 --> 00:07:35.280]   - The one thing which is so obvious, though,
[00:07:35.280 --> 00:07:38.360]   which I think is just really worth reflecting on
[00:07:38.360 --> 00:07:42.400]   is because the mind space of possible intelligences
[00:07:42.400 --> 00:07:45.600]   is so different from ours, it's very dangerous
[00:07:45.600 --> 00:07:47.200]   if we assume they're gonna be like us,
[00:07:47.200 --> 00:07:48.420]   or anything like us.
[00:07:48.420 --> 00:07:54.480]   - Well, the entirety of human written history
[00:07:54.480 --> 00:07:57.440]   has been through poetry, through novels,
[00:07:57.440 --> 00:08:00.960]   been trying to describe through philosophy,
[00:08:00.960 --> 00:08:03.080]   trying to describe the human condition
[00:08:03.080 --> 00:08:04.480]   and what's entailed in it.
[00:08:04.480 --> 00:08:05.720]   Just like you said, fear of death
[00:08:05.720 --> 00:08:07.680]   and all those kinds of things, what is love,
[00:08:07.680 --> 00:08:11.200]   and all of that changes if you have a different
[00:08:11.200 --> 00:08:13.280]   kind of intelligence, all of it.
[00:08:13.280 --> 00:08:16.480]   The entirety, all those poems, they're trying to sneak up
[00:08:16.480 --> 00:08:18.360]   to what the hell it means to be human.
[00:08:18.360 --> 00:08:19.880]   All of that changes.
[00:08:19.880 --> 00:08:24.880]   How AI concerns and existential crises that AI experiences,
[00:08:24.880 --> 00:08:29.800]   how that clashes with the human existential crisis,
[00:08:29.800 --> 00:08:34.480]   the human condition, that's hard to fathom, hard to predict.
[00:08:34.480 --> 00:08:37.960]   - It's hard, but it's fascinating to think about also.
[00:08:37.960 --> 00:08:42.180]   Even in the best case scenario where we don't lose control
[00:08:42.180 --> 00:08:44.960]   over the ever more powerful AI that we're building
[00:08:44.960 --> 00:08:49.120]   to other humans whose goals we think are horrible,
[00:08:49.120 --> 00:08:51.840]   and where we don't lose control to the machines,
[00:08:51.840 --> 00:08:56.320]   and AI provides the things we want,
[00:08:56.320 --> 00:08:59.660]   even then, you get into the questions you touched here.
[00:08:59.660 --> 00:09:03.120]   Maybe it's the struggle that it's actually hard
[00:09:03.120 --> 00:09:04.760]   to do things is part of the things
[00:09:04.760 --> 00:09:07.120]   that gives us meaning as well.
[00:09:07.120 --> 00:09:09.900]   For example, I found it so shocking
[00:09:09.900 --> 00:09:14.240]   that this new Microsoft GPT-4 commercial
[00:09:14.240 --> 00:09:18.000]   that they put together has this woman talking about,
[00:09:18.000 --> 00:09:20.620]   showing this demo of how she's gonna give
[00:09:20.620 --> 00:09:23.820]   a graduation speech to her beloved daughter,
[00:09:23.820 --> 00:09:25.780]   and she asks GPT-4 to write it.
[00:09:25.780 --> 00:09:28.880]   If it's frigging 200 words or so,
[00:09:28.880 --> 00:09:31.840]   if I realized that my parents couldn't be bothered
[00:09:31.840 --> 00:09:35.000]   struggling a little bit to write 200 words
[00:09:35.000 --> 00:09:36.760]   and outsource that to their computer,
[00:09:36.760 --> 00:09:39.020]   I would feel really offended, actually.
[00:09:39.020 --> 00:09:44.400]   And so I wonder if eliminating too much
[00:09:44.400 --> 00:09:46.360]   of the struggle from our existence,
[00:09:46.360 --> 00:09:53.240]   do you think that would also take away a little bit of what--
[00:09:53.240 --> 00:09:55.060]   - It means to be human, yeah.
[00:09:55.060 --> 00:09:57.860]   We can't even predict.
[00:09:57.860 --> 00:10:02.380]   I had somebody mention to me that they started using
[00:10:02.380 --> 00:10:06.400]   Chad GPT with a 3.5 and not 4.0
[00:10:06.400 --> 00:10:12.580]   to write what they really feel to a person,
[00:10:12.580 --> 00:10:14.220]   and they have a temper issue,
[00:10:14.220 --> 00:10:17.800]   and they're basically trying to get Chad GPT
[00:10:17.800 --> 00:10:19.760]   to rewrite it in a nicer way,
[00:10:19.760 --> 00:10:22.620]   to get the point across, but rewrite it in a nicer way.
[00:10:22.620 --> 00:10:26.180]   So we're even removing the inner asshole
[00:10:26.180 --> 00:10:27.680]   from our communication.
[00:10:27.680 --> 00:10:31.840]   So there's some positive aspects of that,
[00:10:31.840 --> 00:10:34.200]   but mostly it's just the transformation
[00:10:34.200 --> 00:10:35.780]   of how humans communicate.
[00:10:35.780 --> 00:10:40.780]   And it's scary because so much of our society
[00:10:40.780 --> 00:10:44.640]   is based on this glue of communication.
[00:10:44.640 --> 00:10:49.080]   And if we're now using AI as the medium of communication
[00:10:49.080 --> 00:10:51.140]   that does the language for us,
[00:10:51.140 --> 00:10:55.520]   so much of the emotion that's laden in human communication,
[00:10:55.520 --> 00:10:59.320]   so much of the intent that's going to be handled
[00:10:59.320 --> 00:11:02.140]   by outsourced AI, how does that change everything?
[00:11:02.140 --> 00:11:03.940]   How does that change the internal state
[00:11:03.940 --> 00:11:06.500]   of how we feel about other human beings?
[00:11:06.500 --> 00:11:08.980]   What makes us lonely, what makes us excited?
[00:11:08.980 --> 00:11:11.160]   What makes us afraid, how we fall in love,
[00:11:11.160 --> 00:11:12.180]   all that kind of stuff.
[00:11:12.180 --> 00:11:15.060]   - Yeah, for me personally, I have to confess,
[00:11:15.060 --> 00:11:16.500]   the challenge is one of the things
[00:11:16.500 --> 00:11:21.500]   that really makes my life feel meaningful.
[00:11:22.660 --> 00:11:26.120]   If I go hike a mountain with my wife, Maya,
[00:11:26.120 --> 00:11:28.220]   I don't want to just press a button and be at the top.
[00:11:28.220 --> 00:11:30.240]   I want to struggle and come up there sweaty
[00:11:30.240 --> 00:11:32.360]   and feel, wow, we did this.
[00:11:32.360 --> 00:11:37.360]   In the same way, I want to constantly work on myself
[00:11:37.360 --> 00:11:39.320]   to become a better person.
[00:11:39.320 --> 00:11:42.680]   If I say something in anger that I regret,
[00:11:42.680 --> 00:11:46.480]   I want to go back and really work on myself
[00:11:46.480 --> 00:11:49.720]   rather than just tell an AI from now on
[00:11:49.720 --> 00:11:51.080]   always filter what I write
[00:11:51.080 --> 00:11:53.620]   so I don't have to work on myself
[00:11:53.620 --> 00:11:55.840]   'cause then I'm not growing.
[00:11:55.840 --> 00:11:59.840]   - Yeah, but then again, it could be like with chess.
[00:11:59.840 --> 00:12:04.680]   An AI, once it significantly, obviously,
[00:12:04.680 --> 00:12:06.840]   supersedes the performance of humans,
[00:12:06.840 --> 00:12:08.800]   it will live in its own world
[00:12:08.800 --> 00:12:12.600]   and provide maybe a flourishing civilization for humans,
[00:12:12.600 --> 00:12:15.120]   but we humans will continue hiking mountains
[00:12:15.120 --> 00:12:18.240]   and playing our games even though AI is so much smarter,
[00:12:18.240 --> 00:12:21.120]   so much stronger, so much superior in every single way,
[00:12:21.120 --> 00:12:22.280]   just like with chess.
[00:12:22.280 --> 00:12:26.720]   That's one possible hopeful trajectory here
[00:12:26.720 --> 00:12:28.560]   is that humans will continue to human
[00:12:28.560 --> 00:12:34.240]   and AI will just be a kind of
[00:12:34.240 --> 00:12:45.320]   a medium that enables the human experience to flourish.
[00:12:45.600 --> 00:12:50.600]   - Yeah, I would phrase that as rebranding ourselves
[00:12:50.600 --> 00:12:53.920]   from homo sapiens to homo sentiens.
[00:12:53.920 --> 00:12:58.200]   Right now, sapiens, the ability to be intelligent,
[00:12:58.200 --> 00:13:00.280]   we've even put it in our species name.
[00:13:00.280 --> 00:13:05.120]   We're branding ourselves as the smartest
[00:13:05.120 --> 00:13:08.600]   information processing entity on the planet.
[00:13:08.600 --> 00:13:12.940]   That's clearly gonna change if AI continues ahead.
[00:13:14.200 --> 00:13:16.520]   So maybe we should focus on the experience instead,
[00:13:16.520 --> 00:13:20.440]   the subjective experience that we have with homo sentiens
[00:13:20.440 --> 00:13:23.080]   and say that's what's really valuable,
[00:13:23.080 --> 00:13:25.400]   the love, the connection, the other things.
[00:13:25.400 --> 00:13:31.240]   Get off our high horses and get rid of this hubris
[00:13:31.240 --> 00:13:35.160]   that only we can do integrals.
[00:13:35.160 --> 00:13:37.880]   - So consciousness, the subjective experience
[00:13:37.880 --> 00:13:42.120]   is a fundamental value to what it means to be human.
[00:13:42.120 --> 00:13:44.200]   Make that the priority.
[00:13:44.200 --> 00:13:47.480]   - That feels like a hopeful direction to me,
[00:13:47.480 --> 00:13:50.920]   but that also requires more compassion,
[00:13:50.920 --> 00:13:53.680]   not just towards other humans because they happen
[00:13:53.680 --> 00:13:55.720]   to be the smartest on the planet,
[00:13:55.720 --> 00:13:57.760]   but also towards all our other fellow creatures
[00:13:57.760 --> 00:13:58.880]   on this planet.
[00:13:58.880 --> 00:14:01.320]   I personally feel right now we're treating
[00:14:01.320 --> 00:14:03.480]   a lot of farm animals horribly, for example,
[00:14:03.480 --> 00:14:04.880]   and the excuse we're using is,
[00:14:04.880 --> 00:14:06.720]   oh, they're not as smart as us.
[00:14:06.720 --> 00:14:10.040]   But if we admit that we're not that smart
[00:14:10.040 --> 00:14:13.040]   in the grand scheme of things either in the post-AI epoch,
[00:14:13.040 --> 00:14:17.840]   then surely we should value the subjective experience
[00:14:17.840 --> 00:14:19.520]   of a cow also.
[00:14:19.520 --> 00:14:23.960]   - Well, allow me to briefly look at the book,
[00:14:23.960 --> 00:14:26.340]   which at this point is becoming more and more visionary
[00:14:26.340 --> 00:14:28.880]   that you've written, I guess over five years ago,
[00:14:28.880 --> 00:14:29.880]   Life 3.0.
[00:14:29.880 --> 00:14:35.640]   So first of all, 3.0, what's 1.0, what's 2.0, what's 3.0?
[00:14:35.640 --> 00:14:38.880]   And how's that vision sort of evolve,
[00:14:38.880 --> 00:14:41.200]   the vision in the book evolve to today?
[00:14:41.200 --> 00:14:45.160]   - Life 1.0 is really dumb, like bacteria,
[00:14:45.160 --> 00:14:46.960]   in that it can't actually learn anything at all
[00:14:46.960 --> 00:14:47.880]   during the lifetime.
[00:14:47.880 --> 00:14:51.800]   The learning just comes from this genetic process
[00:14:51.800 --> 00:14:55.200]   from one generation to the next.
[00:14:55.200 --> 00:15:00.200]   Life 2.0 is us and other animals which have brains,
[00:15:00.200 --> 00:15:04.080]   which can learn during their lifetime a great deal.
[00:15:06.960 --> 00:15:11.960]   And you were born without being able to speak English.
[00:15:11.960 --> 00:15:13.520]   And at some point you decided,
[00:15:13.520 --> 00:15:15.320]   hey, I wanna upgrade my software.
[00:15:15.320 --> 00:15:17.560]   Let's install an English speaking module.
[00:15:17.560 --> 00:15:19.280]   - So you did.
[00:15:19.280 --> 00:15:23.840]   - And Life 3.0, which does not exist yet,
[00:15:23.840 --> 00:15:27.400]   can replace not only its software the way we can,
[00:15:27.400 --> 00:15:28.500]   but also its hardware.
[00:15:28.500 --> 00:15:33.400]   And that's where we're heading towards at high speed.
[00:15:33.400 --> 00:15:34.840]   We're already maybe 2.1,
[00:15:34.840 --> 00:15:38.960]   'cause we can put in an artificial knee,
[00:15:38.960 --> 00:15:42.520]   pacemaker, et cetera, et cetera.
[00:15:42.520 --> 00:15:45.760]   And if Neuralink and other companies succeed,
[00:15:45.760 --> 00:15:48.000]   we'll be Life 2.2, et cetera.
[00:15:48.000 --> 00:15:52.080]   But what the company's trying to build, AGI,
[00:15:52.080 --> 00:15:54.720]   or trying to make is, of course, full 3.0.
[00:15:54.720 --> 00:15:56.000]   And you can put that intelligence
[00:15:56.000 --> 00:15:57.720]   into something that also has no
[00:15:57.720 --> 00:16:02.760]   biological basis whatsoever.
[00:16:02.760 --> 00:16:05.400]   - So less constraints and more capabilities,
[00:16:05.400 --> 00:16:08.720]   just like the leap from 1.0 to 2.0.
[00:16:08.720 --> 00:16:10.080]   There is, nevertheless,
[00:16:10.080 --> 00:16:12.120]   you speaking so harshly about bacteria,
[00:16:12.120 --> 00:16:14.300]   so disrespectfully about bacteria,
[00:16:14.300 --> 00:16:18.240]   there is still the same kind of magic there
[00:16:18.240 --> 00:16:22.480]   that permeates Life 2.0 and 3.0.
[00:16:22.480 --> 00:16:26.520]   It seems like maybe the thing that's truly powerful
[00:16:26.520 --> 00:16:29.400]   about life, intelligence, and consciousness
[00:16:29.400 --> 00:16:31.960]   was already there in 1.0.
[00:16:31.960 --> 00:16:32.840]   Is it possible?
[00:16:32.840 --> 00:16:37.960]   - I think we should be humble and not be so quick
[00:16:37.960 --> 00:16:42.120]   to make everything binary and say either it's there
[00:16:42.120 --> 00:16:42.960]   or it's not.
[00:16:42.960 --> 00:16:44.960]   Clearly, there's a great spectrum.
[00:16:44.960 --> 00:16:48.960]   And there is even controversy about whether some unicellular
[00:16:48.960 --> 00:16:51.520]   organisms like amoebas can maybe learn a little bit
[00:16:51.520 --> 00:16:53.440]   after all.
[00:16:53.440 --> 00:16:56.200]   So apologies if I offended any bacteria here.
[00:16:56.200 --> 00:16:57.040]   It wasn't my intent.
[00:16:57.040 --> 00:16:59.960]   It was more that I wanted to talk up how cool it is
[00:16:59.960 --> 00:17:01.420]   to actually have a brain,
[00:17:01.420 --> 00:17:04.680]   where you can learn dramatically within your lifetime.
[00:17:04.680 --> 00:17:05.800]   - Typical human.
[00:17:05.800 --> 00:17:09.240]   - And the higher up you get from 1.0 to 2.0 to 3.0,
[00:17:09.240 --> 00:17:12.480]   the more you become the captain of your own ship,
[00:17:12.480 --> 00:17:13.960]   the master of your own destiny,
[00:17:13.960 --> 00:17:15.520]   and the less you become a slave
[00:17:15.520 --> 00:17:17.540]   to whatever evolution gave you, right?
[00:17:17.540 --> 00:17:20.240]   By upgrading our software,
[00:17:20.240 --> 00:17:22.640]   we can be so different from previous generations
[00:17:22.640 --> 00:17:24.560]   and even from our parents,
[00:17:24.560 --> 00:17:27.160]   much more so than even a bacterium.
[00:17:27.160 --> 00:17:29.180]   You know, no offense to them.
[00:17:29.180 --> 00:17:32.080]   And if you can also swap out your hardware
[00:17:32.080 --> 00:17:33.840]   and take any physical form you want,
[00:17:33.840 --> 00:17:36.800]   of course, really the sky's the limit.
[00:17:36.800 --> 00:17:40.680]   - Yeah, so it accelerates the rate
[00:17:40.680 --> 00:17:43.560]   at which you can perform the computation
[00:17:43.560 --> 00:17:45.520]   that determines your destiny.
[00:17:45.520 --> 00:17:48.760]   - Yeah, and I think it's worth commenting a bit
[00:17:48.760 --> 00:17:50.560]   on what you means in this context also,
[00:17:50.560 --> 00:17:52.640]   if you swap things out a lot, right?
[00:17:52.640 --> 00:17:58.520]   This is controversial, but my current
[00:17:59.380 --> 00:18:04.380]   understanding is that life is best thought of
[00:18:04.380 --> 00:18:10.860]   not as a bag of meat or even a bag of elementary particles,
[00:18:10.860 --> 00:18:16.820]   but rather as a system which can process information
[00:18:16.820 --> 00:18:19.580]   and retain its own complexity,
[00:18:19.580 --> 00:18:21.580]   even though nature is always trying to mess it up.
[00:18:21.580 --> 00:18:25.100]   So it's all about information processing.
[00:18:25.100 --> 00:18:28.500]   And that makes it a lot like something
[00:18:28.500 --> 00:18:29.880]   like a wave in the ocean,
[00:18:29.880 --> 00:18:33.600]   which is not its water molecules, right?
[00:18:33.600 --> 00:18:35.120]   The water molecules bob up and down,
[00:18:35.120 --> 00:18:36.240]   but the wave moves forward.
[00:18:36.240 --> 00:18:37.800]   It's an information pattern.
[00:18:37.800 --> 00:18:40.540]   In the same way, you, Lex,
[00:18:40.540 --> 00:18:43.520]   you're not the same atoms as during the first
[00:18:43.520 --> 00:18:44.600]   time you did with me. - Time we talked, yeah.
[00:18:44.600 --> 00:18:47.840]   - You've swapped out most of them, but still you.
[00:18:47.840 --> 00:18:51.080]   - Yeah. - And the information pattern
[00:18:51.080 --> 00:18:52.320]   is still there.
[00:18:52.320 --> 00:18:55.840]   And if you could swap out your arms
[00:18:55.840 --> 00:19:00.840]   and whatever, you can still have this kind of continuity.
[00:19:00.840 --> 00:19:03.480]   It becomes much more sophisticated,
[00:19:03.480 --> 00:19:04.860]   sort of wave forward in time
[00:19:04.860 --> 00:19:06.880]   where the information lives on.
[00:19:06.880 --> 00:19:11.440]   I lost both of my parents since our last podcast.
[00:19:11.440 --> 00:19:13.880]   And it actually gives me a lot of solace
[00:19:13.880 --> 00:19:17.060]   that this way of thinking about them,
[00:19:17.060 --> 00:19:21.400]   they haven't entirely died because a lot of mommy
[00:19:21.400 --> 00:19:24.940]   and daddies, sorry, I'm getting a little emotional here,
[00:19:24.940 --> 00:19:28.840]   but a lot of their values and ideas
[00:19:28.840 --> 00:19:33.320]   and even jokes and so on, they haven't gone away, right?
[00:19:33.320 --> 00:19:34.160]   Some of them live on.
[00:19:34.160 --> 00:19:35.760]   I can carry on some of them.
[00:19:35.760 --> 00:19:38.920]   And they also live on a lot of other people.
[00:19:38.920 --> 00:19:41.900]   So in this sense, even with Life 2.0,
[00:19:41.900 --> 00:19:45.880]   we can, to some extent, already transcend
[00:19:45.880 --> 00:19:49.160]   our physical bodies and our death.
[00:19:49.160 --> 00:19:53.920]   And particularly if you can share your own information,
[00:19:53.920 --> 00:19:57.320]   your own ideas with many others like you do
[00:19:57.320 --> 00:20:02.320]   in your podcast, then that's the closest
[00:20:02.320 --> 00:20:06.840]   to immortality we can get with our bio-bodies.
[00:20:06.840 --> 00:20:10.280]   - You carry a little bit of them in you in some sense.
[00:20:10.280 --> 00:20:11.120]   - Yeah, yeah.
[00:20:11.120 --> 00:20:13.000]   - Do you miss them?
[00:20:13.000 --> 00:20:14.080]   You miss your mom and dad?
[00:20:14.080 --> 00:20:15.680]   - Of course, of course.
[00:20:15.680 --> 00:20:17.240]   - What did you learn about life from them
[00:20:17.240 --> 00:20:20.560]   if it can take a bit of a tangent?
[00:20:21.520 --> 00:20:22.640]   - So many things.
[00:20:22.640 --> 00:20:28.920]   For starters, my fascination for math
[00:20:28.920 --> 00:20:32.360]   and the physical mysteries of our universe.
[00:20:32.360 --> 00:20:34.960]   I got a lot of that from my dad.
[00:20:34.960 --> 00:20:38.520]   But I think my obsession for fairly big questions
[00:20:38.520 --> 00:20:40.000]   and consciousness and so on,
[00:20:40.000 --> 00:20:42.880]   that actually came mostly from my mom.
[00:20:42.880 --> 00:20:47.120]   And what I got from both of them,
[00:20:47.120 --> 00:20:49.840]   which is a very core part of really who I am,
[00:20:49.840 --> 00:20:53.360]   I think is this,
[00:20:53.360 --> 00:21:02.000]   just feeling comfortable with not buying
[00:21:02.000 --> 00:21:07.360]   into what everybody else is saying.
[00:21:07.360 --> 00:21:11.280]   Doing what I think is right.
[00:21:11.280 --> 00:21:19.560]   They both very much just did their own thing.
[00:21:19.800 --> 00:21:21.160]   And sometimes they got flack for it,
[00:21:21.160 --> 00:21:22.320]   and they did it anyway.
[00:21:22.320 --> 00:21:25.920]   - That's why you've always been an inspiration to me,
[00:21:25.920 --> 00:21:27.400]   that you're at the top of your field
[00:21:27.400 --> 00:21:31.400]   and you're still willing to tackle
[00:21:31.400 --> 00:21:35.160]   the big questions in your own way.
[00:21:35.160 --> 00:21:40.160]   You're one of the people that represents MIT best to me.
[00:21:40.160 --> 00:21:41.960]   You've always been an inspiration in that.
[00:21:41.960 --> 00:21:44.000]   So it's good to hear that you got that from your mom and dad.
[00:21:44.000 --> 00:21:44.960]   - Yeah, you're too kind.
[00:21:44.960 --> 00:21:49.720]   But yeah, I mean, the good reason to do science
[00:21:49.720 --> 00:21:51.520]   is because you're really curious,
[00:21:51.520 --> 00:21:53.800]   you wanna figure out the truth.
[00:21:53.800 --> 00:21:57.200]   If you think this is how it is,
[00:21:57.200 --> 00:21:58.960]   and everyone else says, no, no, that's bullshit,
[00:21:58.960 --> 00:22:00.360]   and it's that way, you know,
[00:22:00.360 --> 00:22:04.240]   you stick with what you think is true.
[00:22:04.240 --> 00:22:09.480]   And even if everybody else keeps thinking it's bullshit,
[00:22:09.480 --> 00:22:10.360]   there's a certain,
[00:22:10.360 --> 00:22:15.960]   I always root for the underdog when I watch movies.
[00:22:15.960 --> 00:22:18.800]   And my dad once, one time, for example,
[00:22:18.800 --> 00:22:22.160]   when I wrote one of my craziest papers ever,
[00:22:22.160 --> 00:22:24.280]   talking about our universe ultimately being mathematical,
[00:22:24.280 --> 00:22:25.680]   which we're not gonna get into today,
[00:22:25.680 --> 00:22:28.280]   I got this email from a quite famous professor saying,
[00:22:28.280 --> 00:22:29.520]   this is not only bullshit,
[00:22:29.520 --> 00:22:31.160]   but it's gonna ruin your career.
[00:22:31.160 --> 00:22:33.280]   You should stop doing this kind of stuff.
[00:22:33.280 --> 00:22:34.800]   I sent it to my dad.
[00:22:34.800 --> 00:22:35.640]   Do you know what he said?
[00:22:35.640 --> 00:22:36.800]   - What did he say?
[00:22:36.800 --> 00:22:39.600]   - He replied with a quote from Dante.
[00:22:39.600 --> 00:22:42.720]   (speaking in foreign language)
[00:22:42.720 --> 00:22:45.480]   Follow your own path and let the people talk.
[00:22:45.480 --> 00:22:48.240]   Go dad!
[00:22:48.240 --> 00:22:49.080]   This is the kind of thing,
[00:22:49.080 --> 00:22:53.000]   you know, he's dead, but that attitude is not.
[00:22:53.000 --> 00:22:59.240]   - How did losing them as a man, as a human being change you?
[00:22:59.240 --> 00:23:01.160]   How did it expand your thinking about the world?
[00:23:01.160 --> 00:23:03.340]   How did it expand your thinking about,
[00:23:03.340 --> 00:23:05.720]   you know, this thing we're talking about,
[00:23:05.720 --> 00:23:09.800]   which is humans creating another living,
[00:23:09.800 --> 00:23:12.120]   sentient, perhaps, being?
[00:23:12.120 --> 00:23:14.600]   - I think it,
[00:23:18.040 --> 00:23:19.400]   mainly did two things.
[00:23:19.400 --> 00:23:23.760]   One of them, just going through all their stuff
[00:23:23.760 --> 00:23:25.840]   after they had passed away and so on,
[00:23:25.840 --> 00:23:28.840]   just drove home to me how important it is to ask ourselves,
[00:23:28.840 --> 00:23:31.520]   why are we doing these things we do?
[00:23:31.520 --> 00:23:34.040]   Because it's inevitable that you look at some things
[00:23:34.040 --> 00:23:36.200]   they spent an enormous time on and you ask,
[00:23:36.200 --> 00:23:38.680]   in hindsight, would they really have spent
[00:23:38.680 --> 00:23:40.400]   so much time on this?
[00:23:40.400 --> 00:23:42.920]   Would they have done something that was more meaningful?
[00:23:42.920 --> 00:23:46.520]   So I've been looking more in my life now and asking,
[00:23:46.520 --> 00:23:48.680]   you know, why am I doing what I'm doing?
[00:23:48.680 --> 00:23:50.120]   And I feel,
[00:23:50.120 --> 00:23:56.680]   it should either be something I really enjoy doing
[00:23:56.680 --> 00:23:58.840]   or it should be something that I find
[00:23:58.840 --> 00:24:02.840]   really, really meaningful because it helps humanity.
[00:24:02.840 --> 00:24:09.480]   And if it's in none of those two categories,
[00:24:09.480 --> 00:24:12.480]   maybe I should spend less time on it, you know?
[00:24:12.480 --> 00:24:17.000]   The other thing is dealing with death up in person like this,
[00:24:17.000 --> 00:24:20.600]   it's actually made me less afraid of,
[00:24:20.600 --> 00:24:24.560]   even less afraid of other people telling me
[00:24:24.560 --> 00:24:27.960]   that I'm an idiot, you know, which happens regularly.
[00:24:27.960 --> 00:24:31.280]   And just, I'm gonna live my life, do my thing, you know?
[00:24:31.280 --> 00:24:38.360]   And it's made it a little bit easier for me to focus
[00:24:38.360 --> 00:24:40.600]   on what I feel is really important.
[00:24:40.600 --> 00:24:42.440]   - What about fear of your own death?
[00:24:42.440 --> 00:24:45.960]   Has it made it more real that this is,
[00:24:45.960 --> 00:24:49.480]   that this is something that happens?
[00:24:49.480 --> 00:24:51.400]   - Yeah, it's made it extremely real.
[00:24:51.400 --> 00:24:54.200]   And I'm next in line in our family now, right?
[00:24:54.200 --> 00:24:56.080]   It's me and my younger brother.
[00:24:56.080 --> 00:25:01.080]   But they both handled it with such dignity.
[00:25:01.080 --> 00:25:04.600]   That was a true inspiration also.
[00:25:04.600 --> 00:25:06.880]   They never complained about things.
[00:25:06.880 --> 00:25:08.600]   And you know, when you're old
[00:25:08.600 --> 00:25:10.200]   and your body starts falling apart,
[00:25:10.200 --> 00:25:11.400]   it's more and more to complain about.
[00:25:11.400 --> 00:25:13.160]   They looked at what could they still do
[00:25:13.160 --> 00:25:14.760]   that was meaningful.
[00:25:14.760 --> 00:25:17.880]   And they focused on that rather than wasting time
[00:25:17.880 --> 00:25:22.120]   talking about or even thinking much
[00:25:22.120 --> 00:25:24.480]   about things they were disappointed in.
[00:25:24.480 --> 00:25:26.360]   I think anyone can make themselves depressed
[00:25:26.360 --> 00:25:30.800]   if they start their morning by making a list of grievances.
[00:25:30.800 --> 00:25:34.160]   Whereas if you start your day with a little meditation
[00:25:34.160 --> 00:25:36.680]   and just things you're grateful for,
[00:25:36.680 --> 00:25:39.840]   you basically choose to be a happy person.
[00:25:39.840 --> 00:25:42.480]   - Because you only have a finite number of days
[00:25:42.480 --> 00:25:43.440]   you should spend them.
[00:25:43.440 --> 00:25:44.640]   - Make it count.
[00:25:44.640 --> 00:25:45.760]   - Being grateful.
[00:25:45.760 --> 00:25:46.600]   - Yeah.
[00:25:46.600 --> 00:25:52.840]   - Well, you do happen to be working on a thing
[00:25:52.840 --> 00:25:57.840]   which seems to have potentially some of the greatest impact
[00:25:57.840 --> 00:26:00.800]   on human civilization of anything humans have ever created,
[00:26:00.800 --> 00:26:02.200]   which is artificial intelligence.
[00:26:02.200 --> 00:26:05.240]   This is on the both detailed technical level
[00:26:05.240 --> 00:26:08.280]   and in the high philosophical level you work on.
[00:26:08.280 --> 00:26:12.760]   So you've mentioned to me that there's an open letter
[00:26:12.760 --> 00:26:15.040]   that you're working on.
[00:26:15.040 --> 00:26:18.920]   - It's actually going live in a few hours.
[00:26:18.920 --> 00:26:20.040]   (Lex laughing)
[00:26:20.040 --> 00:26:22.760]   I've been having late nights and early mornings.
[00:26:22.760 --> 00:26:24.840]   It's been very exciting actually.
[00:26:24.840 --> 00:26:29.840]   In short, have you seen "Don't Look Up", the film?
[00:26:29.840 --> 00:26:32.400]   - Yes, yes.
[00:26:32.400 --> 00:26:34.040]   - I don't wanna be the movie spoiler
[00:26:34.040 --> 00:26:36.400]   for anyone watching this who hasn't seen it.
[00:26:36.400 --> 00:26:37.600]   But if you're watching this,
[00:26:37.600 --> 00:26:40.440]   you haven't seen it, watch it.
[00:26:40.440 --> 00:26:43.360]   Because we are actually acting out,
[00:26:43.360 --> 00:26:45.640]   it's life imitating art.
[00:26:45.640 --> 00:26:47.880]   Humanity is doing exactly that right now,
[00:26:47.880 --> 00:26:52.480]   except it's an asteroid that we are building ourselves.
[00:26:52.480 --> 00:26:54.840]   Almost nobody is talking about it.
[00:26:54.840 --> 00:26:56.680]   People are squabbling across the planet
[00:26:56.680 --> 00:26:58.840]   about all sorts of things which seem very minor
[00:26:58.840 --> 00:27:02.320]   compared to the asteroid that's about to hit us, right?
[00:27:02.320 --> 00:27:04.440]   Most politicians don't even have their radar,
[00:27:04.440 --> 00:27:05.280]   this on the radar,
[00:27:05.280 --> 00:27:07.680]   they think maybe in 100 years or whatever.
[00:27:07.680 --> 00:27:11.400]   Right now, we're at a fork in the road.
[00:27:11.400 --> 00:27:14.960]   This is the most important fork humanity has reached
[00:27:14.960 --> 00:27:17.840]   in its over 100,000 years on this planet.
[00:27:17.840 --> 00:27:21.720]   We're building effectively a new species
[00:27:21.720 --> 00:27:22.880]   that's smarter than us.
[00:27:22.880 --> 00:27:25.440]   It doesn't look so much like a species yet
[00:27:25.440 --> 00:27:27.560]   'cause it's mostly not embodied in robots,
[00:27:27.560 --> 00:27:32.080]   but that's a technicality which will soon be changed.
[00:27:32.080 --> 00:27:37.080]   And this arrival of artificial general intelligence
[00:27:37.080 --> 00:27:39.280]   that can do all our jobs as well as us,
[00:27:39.280 --> 00:27:43.120]   and probably shortly thereafter, superintelligence,
[00:27:43.120 --> 00:27:46.360]   which greatly exceeds our cognitive abilities,
[00:27:46.360 --> 00:27:48.720]   it's gonna either be the best thing ever to happen
[00:27:48.720 --> 00:27:50.000]   to humanity or the worst.
[00:27:50.000 --> 00:27:52.000]   I'm really quite confident that there is
[00:27:52.000 --> 00:27:55.160]   not that much middle ground there.
[00:27:55.160 --> 00:27:58.080]   - But it would be fundamentally transformative
[00:27:58.080 --> 00:27:59.880]   to human civilization. - Of course.
[00:27:59.920 --> 00:28:01.400]   Utterly and totally.
[00:28:01.400 --> 00:28:04.560]   Again, we branded ourselves as homo sapiens
[00:28:04.560 --> 00:28:06.080]   'cause it seemed like the basic thing.
[00:28:06.080 --> 00:28:09.000]   We're the king of the castle on this planet.
[00:28:09.000 --> 00:28:10.160]   We're the smart ones.
[00:28:10.160 --> 00:28:11.840]   If we can control everything else,
[00:28:11.840 --> 00:28:15.520]   this could very easily change.
[00:28:15.520 --> 00:28:18.360]   We're certainly not gonna be the smartest
[00:28:18.360 --> 00:28:22.720]   on the planet for very long unless AI progress just halts.
[00:28:22.720 --> 00:28:25.400]   And we can talk more about why I think that's true
[00:28:25.400 --> 00:28:26.880]   'cause it's controversial.
[00:28:28.400 --> 00:28:29.920]   And then we can also talk about
[00:28:29.920 --> 00:28:35.160]   reasons you might think it's gonna be the best thing ever
[00:28:35.160 --> 00:28:39.120]   and the reason you think it's gonna be the end of humanity,
[00:28:39.120 --> 00:28:41.480]   which is, of course, super controversial.
[00:28:41.480 --> 00:28:46.480]   But what I think we can, anyone who's working on advanced AI
[00:28:46.480 --> 00:28:52.720]   can agree on is it's much like the film "Don't Look Up"
[00:28:52.720 --> 00:28:57.640]   in that it's just really comical how little serious
[00:28:57.640 --> 00:29:01.440]   public debate there is about it given how huge it is.
[00:29:01.440 --> 00:29:06.960]   - So what we're talking about is a development
[00:29:06.960 --> 00:29:09.200]   of currently things like GPT-4
[00:29:09.200 --> 00:29:14.840]   and the signs it's showing of rapid improvement
[00:29:14.840 --> 00:29:18.440]   that may in the near term lead to development
[00:29:18.440 --> 00:29:23.160]   of super intelligent AGI, AI, general AI systems,
[00:29:23.160 --> 00:29:26.040]   and what kind of impact that has on society.
[00:29:26.040 --> 00:29:26.880]   - Exactly.
[00:29:26.880 --> 00:29:28.440]   - The whole thing is achieves
[00:29:28.440 --> 00:29:30.680]   general human level intelligence
[00:29:30.680 --> 00:29:34.680]   and then beyond that, general super human level intelligence.
[00:29:34.680 --> 00:29:38.900]   There's a lot of questions to explore here.
[00:29:38.900 --> 00:29:41.640]   So one, you mentioned halt.
[00:29:41.640 --> 00:29:44.800]   Is that the content of the letter?
[00:29:44.800 --> 00:29:45.640]   - Yeah.
[00:29:45.640 --> 00:29:47.880]   - Is to suggest that maybe we should pause
[00:29:47.880 --> 00:29:49.680]   the development of these systems.
[00:29:49.680 --> 00:29:50.520]   - Exactly.
[00:29:50.520 --> 00:29:52.020]   So this is very controversial.
[00:29:54.480 --> 00:29:56.000]   When we talked the first time,
[00:29:56.000 --> 00:29:57.400]   we talked about how I was involved
[00:29:57.400 --> 00:29:59.040]   in starting the Future Life Institute
[00:29:59.040 --> 00:30:02.120]   and we worked very hard on 2014, 2015
[00:30:02.120 --> 00:30:04.080]   was the mainstream AI safety.
[00:30:04.080 --> 00:30:07.280]   The idea that there even could be risks
[00:30:07.280 --> 00:30:09.720]   and that you could do things about them.
[00:30:09.720 --> 00:30:11.640]   Before then, a lot of people thought it was just really
[00:30:11.640 --> 00:30:14.560]   kooky to even talk about it and a lot of AI researchers
[00:30:14.560 --> 00:30:18.600]   felt worried that this was too flaky
[00:30:18.600 --> 00:30:20.080]   and could be bad for funding
[00:30:20.080 --> 00:30:21.440]   and that the people who talked about it
[00:30:21.440 --> 00:30:24.120]   were just not, didn't understand AI.
[00:30:24.120 --> 00:30:28.800]   I'm very, very happy with how that's gone
[00:30:28.800 --> 00:30:32.160]   and that now, it's completely mainstream.
[00:30:32.160 --> 00:30:34.840]   You go in any AI conference and people talk about AI safety
[00:30:34.840 --> 00:30:37.920]   and it's a nerdy technical field full of equations
[00:30:37.920 --> 00:30:39.520]   and similar and blah, blah.
[00:30:39.520 --> 00:30:42.600]   As it should be.
[00:30:42.600 --> 00:30:47.520]   But there's this other thing which has been quite taboo
[00:30:47.520 --> 00:30:50.560]   up until now, calling for slowdown.
[00:30:50.560 --> 00:30:54.440]   So what we've constantly been saying,
[00:30:54.440 --> 00:30:56.520]   including myself, I've been biting my tongue a lot,
[00:30:56.520 --> 00:31:01.520]   is that we don't need to slow down AI development,
[00:31:01.520 --> 00:31:05.520]   we just need to win this race, the wisdom race,
[00:31:05.520 --> 00:31:07.840]   between the growing power of the AI
[00:31:07.840 --> 00:31:12.280]   and the growing wisdom with which we manage it.
[00:31:12.280 --> 00:31:14.280]   And rather than trying to slow down AI,
[00:31:14.280 --> 00:31:16.880]   let's just try to accelerate the wisdom.
[00:31:16.880 --> 00:31:18.720]   Do all this technical work to figure out
[00:31:18.720 --> 00:31:21.280]   how you can actually ensure that your powerful AI
[00:31:21.280 --> 00:31:23.320]   is gonna do what you want it to do
[00:31:23.320 --> 00:31:28.320]   and have society adapt also with incentives and regulations
[00:31:28.320 --> 00:31:31.080]   so that these things get put to good use.
[00:31:31.080 --> 00:31:34.840]   Sadly, that didn't pan out.
[00:31:34.840 --> 00:31:39.960]   The progress on technical AI and capabilities
[00:31:39.960 --> 00:31:44.960]   has gone a lot faster than many people thought
[00:31:46.080 --> 00:31:49.080]   back when we started this in 2014.
[00:31:49.080 --> 00:31:50.960]   Turned out to be easier to build
[00:31:50.960 --> 00:31:52.720]   really advanced AI than we thought.
[00:31:52.720 --> 00:31:58.360]   And on the other side, it's gone much slower
[00:31:58.360 --> 00:32:03.360]   than we hoped with getting policy makers and others
[00:32:03.360 --> 00:32:06.840]   to actually put incentives in place
[00:32:06.840 --> 00:32:10.600]   to steer this in the good direction.
[00:32:10.600 --> 00:32:12.400]   Maybe we should unpack it and talk a little bit about each.
[00:32:12.400 --> 00:32:15.920]   So why did it go faster than a lot of people thought?
[00:32:16.920 --> 00:32:21.920]   In hindsight, it's exactly like building flying machines.
[00:32:21.920 --> 00:32:24.360]   People spent a lot of time wondering
[00:32:24.360 --> 00:32:26.560]   about how do birds fly?
[00:32:26.560 --> 00:32:28.800]   And that turned out to be really hard.
[00:32:28.800 --> 00:32:31.760]   Have you seen the TED Talk with a flying bird?
[00:32:31.760 --> 00:32:33.200]   - Like a flying robotic bird?
[00:32:33.200 --> 00:32:35.240]   - Yeah, it flies around the audience.
[00:32:35.240 --> 00:32:38.440]   But it took 100 years longer to figure out how to do that
[00:32:38.440 --> 00:32:40.440]   than for the Wright brothers to build the first airplane
[00:32:40.440 --> 00:32:43.080]   because it turned out there was a much easier way to fly.
[00:32:43.080 --> 00:32:45.640]   And evolution picked the more complicated one
[00:32:45.640 --> 00:32:48.000]   because it had its hands tied.
[00:32:48.000 --> 00:32:51.040]   It could only build a machine that could assemble itself,
[00:32:51.040 --> 00:32:53.920]   which the Wright brothers didn't care about.
[00:32:53.920 --> 00:32:55.040]   They can only build a machine
[00:32:55.040 --> 00:32:58.320]   that used only the most common atoms in the periodic table.
[00:32:58.320 --> 00:32:59.780]   Wright brothers didn't care about that.
[00:32:59.780 --> 00:33:03.280]   They could use steel, iron atoms.
[00:33:03.280 --> 00:33:05.920]   And it had to be able to repair itself
[00:33:05.920 --> 00:33:08.520]   and it also had to be incredibly fuel efficient.
[00:33:08.520 --> 00:33:13.360]   A lot of birds use less than half the fuel
[00:33:13.360 --> 00:33:16.880]   of a remote control plane flying the same distance.
[00:33:16.880 --> 00:33:18.880]   For humans, just throw a little more,
[00:33:18.880 --> 00:33:20.400]   put a little more fuel in a roof.
[00:33:20.400 --> 00:33:22.360]   There you go, 100 years earlier.
[00:33:22.360 --> 00:33:24.400]   That's exactly what's happening now
[00:33:24.400 --> 00:33:26.000]   with these large language models.
[00:33:26.000 --> 00:33:29.960]   The brain is incredibly complicated.
[00:33:29.960 --> 00:33:31.800]   Many people made the mistake of thinking
[00:33:31.800 --> 00:33:35.320]   we had to figure out how the brain does human level AI first
[00:33:35.320 --> 00:33:37.520]   before we could build in a machine.
[00:33:37.520 --> 00:33:39.000]   That was completely wrong.
[00:33:39.000 --> 00:33:44.000]   You can take an incredibly simple computational system
[00:33:44.000 --> 00:33:45.760]   called a transformer network
[00:33:45.760 --> 00:33:48.480]   and just train it to do something incredibly dumb.
[00:33:48.480 --> 00:33:50.760]   Just read a gigantic amount of texts
[00:33:50.760 --> 00:33:52.400]   and try to predict the next word.
[00:33:52.400 --> 00:33:57.240]   And it turns out if you just throw a ton of compute at that
[00:33:57.240 --> 00:34:01.520]   and a ton of data, it gets to be frighteningly good
[00:34:01.520 --> 00:34:03.840]   like GPT-4, which I've been playing with so much
[00:34:03.840 --> 00:34:04.740]   since it came out.
[00:34:06.080 --> 00:34:09.120]   And there's still some debate
[00:34:09.120 --> 00:34:11.040]   about whether that can get you all the way
[00:34:11.040 --> 00:34:13.680]   to full human level or not.
[00:34:13.680 --> 00:34:16.240]   But yeah, we can come back to the details of that
[00:34:16.240 --> 00:34:17.760]   and how you might get to human level AI
[00:34:17.760 --> 00:34:22.200]   even if large language models don't.
[00:34:22.200 --> 00:34:24.560]   - Can you briefly, if it's just a small tangent,
[00:34:24.560 --> 00:34:27.120]   comment on your feelings about GPT-4?
[00:34:27.120 --> 00:34:31.280]   Suggest that you're impressed by this rate of progress,
[00:34:31.280 --> 00:34:32.720]   but where is it?
[00:34:32.760 --> 00:34:35.480]   Can GPT-4 reason?
[00:34:35.480 --> 00:34:37.800]   What are the intuitions?
[00:34:37.800 --> 00:34:40.600]   What are human interpretable words you can assign
[00:34:40.600 --> 00:34:42.280]   to the capabilities of GPT-4
[00:34:42.280 --> 00:34:45.040]   that makes you so damn impressed with it?
[00:34:45.040 --> 00:34:48.400]   - I'm both very excited about it and terrified.
[00:34:48.400 --> 00:34:52.040]   It's an interesting mixture of emotions.
[00:34:52.040 --> 00:34:55.320]   - All the best things in life include those two somehow.
[00:34:55.320 --> 00:34:57.360]   - Yeah, it can absolutely reason.
[00:34:57.360 --> 00:34:59.580]   Anyone who hasn't played with it,
[00:34:59.580 --> 00:35:02.040]   I highly recommend doing that before dissing it.
[00:35:03.040 --> 00:35:06.440]   It can do quite remarkable reasoning.
[00:35:06.440 --> 00:35:08.760]   I've had it do a lot of things,
[00:35:08.760 --> 00:35:12.200]   which I realized I couldn't do that myself that well even.
[00:35:12.200 --> 00:35:16.040]   And obviously does it dramatically faster than we do too
[00:35:16.040 --> 00:35:17.920]   when you watch it type.
[00:35:17.920 --> 00:35:20.720]   And it's doing that while servicing a massive number
[00:35:20.720 --> 00:35:23.080]   of other humans at the same time.
[00:35:23.080 --> 00:35:28.080]   At the same time, it cannot reason as well as a human can
[00:35:28.080 --> 00:35:30.280]   on some tasks.
[00:35:30.280 --> 00:35:32.320]   Just because it's obviously a limitation
[00:35:32.320 --> 00:35:33.360]   from its architecture.
[00:35:33.360 --> 00:35:36.560]   We have in our heads what in geek speak
[00:35:36.560 --> 00:35:38.720]   is called a recurrent neural network.
[00:35:38.720 --> 00:35:39.560]   There are loops.
[00:35:39.560 --> 00:35:41.480]   Information can go from this neuron to this neuron
[00:35:41.480 --> 00:35:43.240]   to this neuron and then back to this one.
[00:35:43.240 --> 00:35:44.880]   You can like ruminate on something for a while.
[00:35:44.880 --> 00:35:46.740]   You can self-reflect a lot.
[00:35:46.740 --> 00:35:50.320]   These large language models that are,
[00:35:50.320 --> 00:35:52.400]   they cannot, like GPT-4.
[00:35:52.400 --> 00:35:55.200]   It's a so-called transformer where it's just like
[00:35:55.200 --> 00:35:57.880]   a one-way street of information basically.
[00:35:57.880 --> 00:36:01.120]   In geek speak, it's called a feed-forward neural network.
[00:36:01.120 --> 00:36:03.080]   And it's only so deep.
[00:36:03.080 --> 00:36:05.840]   So it can only do logic that's that many steps
[00:36:05.840 --> 00:36:06.680]   and that deep.
[00:36:06.680 --> 00:36:12.480]   And you can create the problems which will fail to solve
[00:36:12.480 --> 00:36:13.760]   for that reason.
[00:36:13.760 --> 00:36:20.280]   But the fact that it can do so amazing things
[00:36:20.280 --> 00:36:23.640]   with this incredibly simple architecture already
[00:36:23.640 --> 00:36:24.880]   is quite stunning.
[00:36:24.880 --> 00:36:27.280]   And what we see in my lab at MIT
[00:36:27.280 --> 00:36:30.600]   when we look inside large language models
[00:36:30.600 --> 00:36:32.280]   to try to figure out how they're doing it,
[00:36:32.280 --> 00:36:35.520]   that's the key core focus of our research.
[00:36:35.520 --> 00:36:40.520]   It's called mechanistic interpretability in geek speak.
[00:36:40.520 --> 00:36:42.920]   You have this machine that does something smart.
[00:36:42.920 --> 00:36:44.400]   You try to reverse engineer it.
[00:36:44.400 --> 00:36:45.480]   See how does it do it?
[00:36:45.480 --> 00:36:49.840]   Or you think of it also as artificial neuroscience.
[00:36:49.840 --> 00:36:50.680]   (laughing)
[00:36:50.680 --> 00:36:51.520]   'Cause that's exactly what neuroscientists do
[00:36:51.520 --> 00:36:52.360]   with actual brains.
[00:36:52.360 --> 00:36:54.360]   But here you have the advantage that you can,
[00:36:54.360 --> 00:36:56.200]   you don't have to worry about measurement errors.
[00:36:56.200 --> 00:36:59.560]   You can see what every neuron is doing all the time.
[00:36:59.560 --> 00:37:02.360]   And a recurrent thing we see again and again,
[00:37:02.360 --> 00:37:06.200]   there's been a number of beautiful papers quite recently
[00:37:06.200 --> 00:37:09.120]   by a lot of researchers, some of them here,
[00:37:09.120 --> 00:37:12.080]   I mean in this area, is where when they figure out
[00:37:12.080 --> 00:37:14.280]   how something is done, you can say,
[00:37:14.280 --> 00:37:16.840]   "Oh man, that's such a dumb way of doing it."
[00:37:16.840 --> 00:37:18.840]   And you immediately see how it can be improved.
[00:37:18.840 --> 00:37:22.200]   Like for example, there was a beautiful paper recently
[00:37:22.200 --> 00:37:24.640]   where they figured out how a large language model
[00:37:24.640 --> 00:37:28.800]   stores certain facts, like Eiffel Tower is in Paris.
[00:37:28.800 --> 00:37:31.680]   And they figured out exactly how it's stored.
[00:37:31.680 --> 00:37:34.200]   The proof that they understood it was they could edit it.
[00:37:34.200 --> 00:37:37.800]   They changed some of the synapses in it.
[00:37:37.800 --> 00:37:39.680]   And then they asked it, "Where's the Eiffel Tower?"
[00:37:39.680 --> 00:37:41.440]   And it said, "It's in Rome."
[00:37:41.440 --> 00:37:43.680]   And then they asked you, "How do you get there?"
[00:37:43.680 --> 00:37:45.720]   "Oh, how do you get there from Germany?"
[00:37:45.720 --> 00:37:47.000]   "Oh, you take this train,
[00:37:47.000 --> 00:37:51.120]   and the Roma Termini train station, and this and that."
[00:37:51.120 --> 00:37:52.880]   And what might you see if you're in front of it?
[00:37:52.880 --> 00:37:55.880]   "Oh, you might see the Colosseum."
[00:37:55.880 --> 00:37:57.040]   So they had edited it.
[00:37:57.040 --> 00:37:59.160]   - So they literally moved it to Rome.
[00:37:59.160 --> 00:38:01.800]   - But the way that it's storing this information,
[00:38:01.800 --> 00:38:03.360]   it's incredibly dumb.
[00:38:03.360 --> 00:38:07.800]   For any fellow nerds listening to this,
[00:38:07.800 --> 00:38:11.720]   there was a big matrix, and roughly speaking,
[00:38:11.720 --> 00:38:13.200]   there are certain row and column vectors
[00:38:13.200 --> 00:38:15.600]   which encode these things, and they correspond
[00:38:15.600 --> 00:38:17.880]   very hand-wavily to principal components.
[00:38:17.880 --> 00:38:21.360]   And it would be much more efficient for a sparse matrix
[00:38:21.360 --> 00:38:23.680]   to just store in the database.
[00:38:23.680 --> 00:38:27.800]   But in everything so far, we've figured out
[00:38:27.800 --> 00:38:29.960]   how these things do, or ways where you can see
[00:38:29.960 --> 00:38:31.320]   they can easily be improved.
[00:38:31.320 --> 00:38:34.240]   And the fact that this particular architecture
[00:38:34.240 --> 00:38:37.680]   has some roadblocks built into it is in no way
[00:38:37.680 --> 00:38:40.960]   gonna prevent crafty researchers
[00:38:40.960 --> 00:38:42.600]   from quickly finding workarounds
[00:38:42.600 --> 00:38:47.480]   and making other kinds of architectures
[00:38:47.480 --> 00:38:48.640]   sort of go all the way.
[00:38:50.120 --> 00:38:54.360]   In short, it's turned out to be a lot easier
[00:38:54.360 --> 00:38:58.240]   to build close to human intelligence than we thought,
[00:38:58.240 --> 00:39:00.240]   and that means our runway as a species
[00:39:00.240 --> 00:39:05.240]   to get our shit together has shortened.
[00:39:05.240 --> 00:39:07.960]   - And it seems like the scary thing
[00:39:07.960 --> 00:39:11.680]   about the effectiveness of large language models,
[00:39:11.680 --> 00:39:14.920]   so Sam Altman I recently had a conversation with,
[00:39:14.920 --> 00:39:19.840]   and he really showed that the leap from GPT-3
[00:39:19.840 --> 00:39:23.200]   to GPT-4 has to do with just a bunch of hacks,
[00:39:23.200 --> 00:39:28.200]   a bunch of little explorations with smart researchers
[00:39:28.200 --> 00:39:30.560]   doing a few little fixes here and there.
[00:39:30.560 --> 00:39:34.240]   It's not some fundamental leap and transformation
[00:39:34.240 --> 00:39:35.680]   in the architecture.
[00:39:35.680 --> 00:39:37.720]   - And more data and more compute.
[00:39:37.720 --> 00:39:39.680]   - And more data and compute, but he said the big leaps
[00:39:39.680 --> 00:39:42.840]   has to do with not the data and the compute,
[00:39:42.840 --> 00:39:46.800]   but just learning this new discipline, just like you said.
[00:39:46.800 --> 00:39:48.920]   So researchers are going to look at these architectures,
[00:39:48.920 --> 00:39:52.480]   and there might be big leaps where you realize,
[00:39:52.480 --> 00:39:54.440]   wait, why are we doing this in this dumb way?
[00:39:54.440 --> 00:39:57.160]   And all of a sudden this model is 10x smarter,
[00:39:57.160 --> 00:39:59.400]   and that can happen on any one day,
[00:39:59.400 --> 00:40:02.080]   on any one Tuesday or Wednesday afternoon,
[00:40:02.080 --> 00:40:03.640]   and then all of a sudden you have a system
[00:40:03.640 --> 00:40:05.740]   that's 10x smarter.
[00:40:05.740 --> 00:40:07.440]   It seems like it's such a new discipline.
[00:40:07.440 --> 00:40:10.320]   It's such a new, like we understand so little
[00:40:10.320 --> 00:40:12.540]   about why this thing works so damn well
[00:40:12.540 --> 00:40:16.200]   that the linear improvement of compute, or exponential,
[00:40:16.200 --> 00:40:17.720]   but the steady improvement of compute,
[00:40:17.720 --> 00:40:19.320]   steady improvement of the data,
[00:40:19.320 --> 00:40:21.520]   may not be the thing that even leads to the next leap.
[00:40:21.520 --> 00:40:24.240]   It could be a surprise little hack that improves everything.
[00:40:24.240 --> 00:40:25.800]   - Or a lot of little leaps here and there,
[00:40:25.800 --> 00:40:29.960]   because so much of this is out in the open also.
[00:40:29.960 --> 00:40:33.640]   So many smart people are looking at this
[00:40:33.640 --> 00:40:35.680]   and trying to figure out little leaps here and there,
[00:40:35.680 --> 00:40:39.120]   and it becomes this sort of collective race
[00:40:39.120 --> 00:40:40.600]   where a lot of people feel,
[00:40:40.600 --> 00:40:42.640]   if I don't take the leap, someone else will.
[00:40:42.640 --> 00:40:45.880]   And this is actually very crucial for the other part of it.
[00:40:45.880 --> 00:40:47.140]   Why do we want to slow this down?
[00:40:47.140 --> 00:40:50.040]   So again, what this open letter is calling for
[00:40:50.040 --> 00:40:54.040]   is just pausing all training of systems
[00:40:54.040 --> 00:40:59.400]   that are more powerful than GPT-4 for six months.
[00:40:59.400 --> 00:41:04.400]   Just give a chance for the labs to coordinate a bit
[00:41:04.400 --> 00:41:08.080]   on safety and for society to adapt,
[00:41:08.080 --> 00:41:09.840]   give the right incentives to the labs.
[00:41:09.840 --> 00:41:14.200]   'Cause you've interviewed a lot of these people
[00:41:14.200 --> 00:41:16.720]   who lead these labs, and you know just as well as I do
[00:41:16.720 --> 00:41:18.980]   that they're good people, they're idealistic people.
[00:41:18.980 --> 00:41:21.300]   They're doing this first and foremost
[00:41:21.300 --> 00:41:23.580]   because they believe that AI has a huge potential
[00:41:23.580 --> 00:41:24.700]   to help humanity.
[00:41:24.700 --> 00:41:29.520]   But at the same time, they are trapped
[00:41:29.520 --> 00:41:33.100]   in this horrible race to the bottom.
[00:41:33.100 --> 00:41:40.820]   Have you read "Meditations on Moloch" by Scott Alexander?
[00:41:40.820 --> 00:41:41.660]   - Yes.
[00:41:41.660 --> 00:41:44.560]   - Yeah, it's a beautiful essay on this poem by Ginzburg
[00:41:44.560 --> 00:41:47.960]   where he interprets it as being about this monster.
[00:41:47.960 --> 00:41:53.520]   It's this game theory monster that pits people
[00:41:53.520 --> 00:41:55.980]   against each other in this race to the bottom
[00:41:55.980 --> 00:41:58.140]   where everybody ultimately loses.
[00:41:58.140 --> 00:41:59.700]   The evil thing about this monster is
[00:41:59.700 --> 00:42:02.140]   even though everybody sees it and understands,
[00:42:02.140 --> 00:42:03.980]   they still can't get out of the race.
[00:42:03.980 --> 00:42:08.420]   A good fraction of all the bad things that we humans do
[00:42:08.420 --> 00:42:09.980]   are caused by Moloch.
[00:42:10.040 --> 00:42:15.040]   I like Scott Alexander's naming of the monster
[00:42:15.040 --> 00:42:19.220]   so we humans can think of it as a thing.
[00:42:19.220 --> 00:42:23.120]   If you look at why do we have overfishing,
[00:42:23.120 --> 00:42:26.360]   why do we have more generally the tragedy of the commons,
[00:42:26.360 --> 00:42:29.560]   why is it that, so Liv Borre,
[00:42:29.560 --> 00:42:31.200]   I don't know if you've had her on your podcast.
[00:42:31.200 --> 00:42:33.080]   - Yeah, she's become a friend, yeah.
[00:42:33.080 --> 00:42:36.560]   - Great, she made this awesome point recently
[00:42:36.560 --> 00:42:40.740]   that beauty filters that a lot of female influencers
[00:42:40.740 --> 00:42:46.420]   feel pressure to use are exactly Moloch in action again.
[00:42:46.420 --> 00:42:47.820]   First, nobody was using them
[00:42:47.820 --> 00:42:51.060]   and people saw them just the way they were
[00:42:51.060 --> 00:42:52.960]   and then some of them started using it
[00:42:52.960 --> 00:42:56.660]   and becoming ever more plastic fantastic
[00:42:56.660 --> 00:42:58.460]   and then the other ones that weren't using it
[00:42:58.460 --> 00:43:01.820]   started to realize that if they wanna just keep
[00:43:01.820 --> 00:43:05.860]   their market share, they have to start using it too
[00:43:05.860 --> 00:43:09.080]   and then you're in a situation where they're all using it
[00:43:09.080 --> 00:43:11.280]   and none of them has any more market share
[00:43:11.280 --> 00:43:12.240]   or less than before.
[00:43:12.240 --> 00:43:15.280]   So nobody gained anything, everybody lost
[00:43:15.280 --> 00:43:17.920]   and they have to keep becoming
[00:43:17.920 --> 00:43:20.440]   ever more plastic fantastic also, right?
[00:43:20.440 --> 00:43:25.000]   But nobody can go back to the old way
[00:43:25.000 --> 00:43:28.660]   'cause it's just too costly, right?
[00:43:28.660 --> 00:43:34.560]   Moloch is everywhere and Moloch is not a new arrival
[00:43:34.840 --> 00:43:36.220]   on the scene either.
[00:43:36.220 --> 00:43:39.340]   We humans have developed a lot of collaboration mechanisms
[00:43:39.340 --> 00:43:41.540]   to help us fight back against Moloch
[00:43:41.540 --> 00:43:45.340]   through various kinds of constructive collaboration.
[00:43:45.340 --> 00:43:47.380]   The Soviet Union and the United States
[00:43:47.380 --> 00:43:51.780]   did sign a number of arms control treaties
[00:43:51.780 --> 00:43:53.980]   against Moloch who is trying to stoke them
[00:43:53.980 --> 00:43:58.220]   into unnecessarily risky nuclear arms races, et cetera,
[00:43:58.220 --> 00:44:00.340]   et cetera and this is exactly what's happening
[00:44:00.340 --> 00:44:01.980]   on the AI front.
[00:44:01.980 --> 00:44:05.360]   This time, it's a little bit geopolitics
[00:44:05.360 --> 00:44:07.360]   but it's mostly money where there's just
[00:44:07.360 --> 00:44:08.680]   so much commercial pressure.
[00:44:08.680 --> 00:44:12.700]   If you take any of these leaders of the top tech companies,
[00:44:12.700 --> 00:44:16.600]   if they just say, this is too risky,
[00:44:16.600 --> 00:44:19.400]   I wanna pause for six months,
[00:44:19.400 --> 00:44:20.800]   they're gonna get a lot of pressure
[00:44:20.800 --> 00:44:22.380]   from shareholders and others.
[00:44:22.380 --> 00:44:26.240]   They're like, well, if you pause,
[00:44:26.240 --> 00:44:27.700]   but those guys don't pause,
[00:44:27.700 --> 00:44:31.400]   we don't wanna get our lunch eaten.
[00:44:31.400 --> 00:44:32.240]   - Yeah.
[00:44:32.240 --> 00:44:34.160]   - And shareholders even have the power
[00:44:34.160 --> 00:44:37.360]   to replace the executives in the worst case, right?
[00:44:37.360 --> 00:44:42.320]   So we did this open letter because we wanna help
[00:44:42.320 --> 00:44:44.600]   these idealistic tech executives
[00:44:44.600 --> 00:44:47.480]   to do what their heart tells them
[00:44:47.480 --> 00:44:49.280]   by providing enough public pressure
[00:44:49.280 --> 00:44:52.000]   on the whole sector to just pause
[00:44:52.000 --> 00:44:55.560]   so that they can all pause in a coordinated fashion.
[00:44:55.560 --> 00:44:57.320]   And I think without the public pressure,
[00:44:57.320 --> 00:44:59.800]   none of them can do it alone,
[00:44:59.800 --> 00:45:02.440]   push back against their shareholders,
[00:45:02.440 --> 00:45:05.280]   no matter how good-hearted they are.
[00:45:05.280 --> 00:45:07.280]   'Cause Moloch is a really powerful foe.
[00:45:07.280 --> 00:45:11.660]   - So the idea is to,
[00:45:11.660 --> 00:45:15.040]   for the major developers of AI systems like this,
[00:45:15.040 --> 00:45:17.120]   so we're talking about Microsoft, Google,
[00:45:17.120 --> 00:45:21.920]   Meta, and anyone else?
[00:45:21.920 --> 00:45:25.200]   - Well, OpenAI is very close with Microsoft now, of course.
[00:45:25.200 --> 00:45:28.640]   And there are plenty of smaller players.
[00:45:28.640 --> 00:45:32.480]   For example, Anthropic is very impressive.
[00:45:32.480 --> 00:45:33.640]   There's Conjecture.
[00:45:33.640 --> 00:45:35.040]   There's many, many, many players.
[00:45:35.040 --> 00:45:37.680]   I don't wanna make a long list to leave anyone out.
[00:45:37.680 --> 00:45:41.920]   And for that reason, it's so important
[00:45:41.920 --> 00:45:44.560]   that some coordination happens,
[00:45:44.560 --> 00:45:46.840]   that there's external pressure on all of them
[00:45:46.840 --> 00:45:48.760]   saying you all need to pause.
[00:45:48.760 --> 00:45:52.800]   'Cause then the people, the researchers in these organizations
[00:45:52.800 --> 00:45:54.880]   who the leaders who wanna slow down a little bit,
[00:45:54.880 --> 00:45:56.640]   they can say to their shareholders,
[00:45:56.640 --> 00:46:01.240]   everybody's slowing down because of this pressure,
[00:46:01.240 --> 00:46:03.320]   and it's the right thing to do.
[00:46:03.320 --> 00:46:07.400]   - Have you seen in history their examples
[00:46:07.400 --> 00:46:09.240]   where it's possible to pause the Moloch?
[00:46:09.240 --> 00:46:10.080]   - Absolutely.
[00:46:10.080 --> 00:46:12.640]   Like human cloning, for example.
[00:46:12.640 --> 00:46:14.940]   You could make so much money on human cloning.
[00:46:14.940 --> 00:46:19.640]   Why aren't we doing it?
[00:46:19.640 --> 00:46:23.960]   Because biologists thought hard about this
[00:46:23.960 --> 00:46:27.560]   and felt like this is way too risky.
[00:46:27.560 --> 00:46:30.960]   They got together in the '70s in the Selomar
[00:46:30.960 --> 00:46:34.640]   and decided even to stop a lot more stuff also,
[00:46:34.640 --> 00:46:36.320]   just editing the human germline,
[00:46:36.320 --> 00:46:42.000]   gene editing that goes in to our offspring,
[00:46:42.000 --> 00:46:44.800]   and decided let's not do this
[00:46:44.800 --> 00:46:48.120]   because it's too unpredictable what it's gonna lead to.
[00:46:48.120 --> 00:46:51.920]   We could lose control over what happens to our species.
[00:46:51.920 --> 00:46:52.840]   So they paused.
[00:46:53.840 --> 00:46:56.280]   There was a ton of money to be made there.
[00:46:56.280 --> 00:46:57.800]   So it's very doable,
[00:46:57.800 --> 00:47:02.240]   but you need a public awareness of what the risks are
[00:47:02.240 --> 00:47:05.160]   and the broader community coming in and saying,
[00:47:05.160 --> 00:47:06.800]   hey, let's slow down.
[00:47:06.800 --> 00:47:09.720]   And another common pushback I get today
[00:47:09.720 --> 00:47:13.700]   is we can't stop in the West because China.
[00:47:13.700 --> 00:47:17.320]   And in China, undoubtedly,
[00:47:17.320 --> 00:47:20.440]   they also get told we can't slow down because the West,
[00:47:20.440 --> 00:47:22.800]   because both sides think they're the good guy.
[00:47:22.800 --> 00:47:25.420]   But look at human cloning.
[00:47:25.420 --> 00:47:28.920]   Did China forge ahead with human cloning?
[00:47:28.920 --> 00:47:30.520]   There's been exactly one human cloning
[00:47:30.520 --> 00:47:32.440]   that's actually been done that I know of.
[00:47:32.440 --> 00:47:34.160]   It was done by a Chinese guy.
[00:47:34.160 --> 00:47:36.080]   Do you know where he is now?
[00:47:36.080 --> 00:47:36.920]   In jail.
[00:47:36.920 --> 00:47:39.640]   And you know who put him there?
[00:47:39.640 --> 00:47:40.480]   - Who?
[00:47:40.480 --> 00:47:42.160]   - Chinese government.
[00:47:42.160 --> 00:47:45.680]   Not because Westerners said, China, look, this is...
[00:47:45.680 --> 00:47:47.040]   No, the Chinese government put him there
[00:47:47.040 --> 00:47:50.480]   because they also felt they like control,
[00:47:50.480 --> 00:47:51.760]   the Chinese government.
[00:47:51.760 --> 00:47:54.080]   If anything, maybe they are even more concerned
[00:47:54.080 --> 00:47:57.160]   about having control than Western governments
[00:47:57.160 --> 00:47:59.800]   have no incentive of just losing control
[00:47:59.800 --> 00:48:01.600]   over where everything is going.
[00:48:01.600 --> 00:48:03.320]   And you can also see the Ernie bot
[00:48:03.320 --> 00:48:07.200]   that was released by, I believe, Baidu recently.
[00:48:07.200 --> 00:48:08.920]   They got a lot of pushback from the government
[00:48:08.920 --> 00:48:11.400]   and had to rein it in in a big way.
[00:48:11.400 --> 00:48:15.240]   I think once this basic message comes out
[00:48:15.240 --> 00:48:16.560]   that this isn't an arms race,
[00:48:16.560 --> 00:48:17.840]   it's a suicide race,
[00:48:17.840 --> 00:48:23.600]   where everybody loses if anybody's AI goes out of control,
[00:48:23.600 --> 00:48:25.560]   it really changes the whole dynamic.
[00:48:25.560 --> 00:48:32.080]   I'll say this again, 'cause this is a very basic point
[00:48:32.080 --> 00:48:34.200]   I think a lot of people get wrong.
[00:48:34.200 --> 00:48:38.240]   Because a lot of people dismiss the whole idea
[00:48:38.240 --> 00:48:42.080]   that AI can really get very superhuman,
[00:48:42.080 --> 00:48:43.880]   because they think there's something really magical
[00:48:43.880 --> 00:48:46.320]   about intelligence such that it can only exist
[00:48:46.320 --> 00:48:47.160]   in the human mind.
[00:48:47.160 --> 00:48:48.360]   Because they believe that,
[00:48:48.360 --> 00:48:51.000]   they think it's gonna kind of get to just more or less
[00:48:51.000 --> 00:48:54.560]   GPT-4++ and then that's it.
[00:48:54.560 --> 00:48:58.520]   They don't see it as a suicide race.
[00:48:58.520 --> 00:49:00.000]   They think whoever gets that first,
[00:49:00.000 --> 00:49:02.560]   they're gonna control the world, they're gonna win.
[00:49:02.560 --> 00:49:04.160]   That's not how it's gonna be.
[00:49:04.160 --> 00:49:08.320]   And we can talk again about the scientific arguments
[00:49:08.320 --> 00:49:09.600]   from why it's not gonna stop there.
[00:49:09.600 --> 00:49:13.600]   But the way it's gonna be is if anybody completely
[00:49:13.600 --> 00:49:16.600]   loses control and you don't care,
[00:49:16.600 --> 00:49:21.840]   if someone manages to take over the world
[00:49:21.840 --> 00:49:24.200]   who really doesn't share your goals,
[00:49:24.200 --> 00:49:25.800]   you probably don't really even care very much
[00:49:25.800 --> 00:49:27.120]   about what nationality they have.
[00:49:27.120 --> 00:49:29.800]   You're not gonna like it, much worse than today.
[00:49:29.800 --> 00:49:34.120]   If you live in Orwellian dystopia,
[00:49:34.120 --> 00:49:36.720]   what do you care who created it, right?
[00:49:36.720 --> 00:49:38.960]   And if someone, if it goes farther
[00:49:38.960 --> 00:49:43.360]   and we just lose control even to the machines,
[00:49:44.360 --> 00:49:47.360]   so that it's not us versus them, it's us versus it,
[00:49:47.360 --> 00:49:52.320]   what do you care who created this underlying entity
[00:49:52.320 --> 00:49:55.320]   which has goals different from humans ultimately
[00:49:55.320 --> 00:49:58.560]   and we get marginalized, we get made obsolete,
[00:49:58.560 --> 00:49:59.600]   we get replaced?
[00:49:59.600 --> 00:50:04.960]   That's what I mean when I say it's a suicide race.
[00:50:04.960 --> 00:50:09.040]   It's kind of like we're rushing towards this cliff,
[00:50:09.040 --> 00:50:10.560]   but the closer to the cliff we get,
[00:50:10.560 --> 00:50:13.240]   the more scenic the views are and the more money we make.
[00:50:13.240 --> 00:50:16.920]   The more money there is there, so we keep going,
[00:50:16.920 --> 00:50:19.200]   but we have to also stop at some point, right?
[00:50:19.200 --> 00:50:20.760]   Quit while we're ahead.
[00:50:20.760 --> 00:50:25.760]   And it's a suicide race which cannot be won,
[00:50:25.760 --> 00:50:33.440]   but the way to really benefit from it
[00:50:33.440 --> 00:50:38.440]   is to continue developing awesome AI, a little bit slower,
[00:50:38.440 --> 00:50:41.640]   so we make it safe, make sure it does the things
[00:50:41.640 --> 00:50:44.760]   we always want and create a condition where everybody wins.
[00:50:44.760 --> 00:50:49.440]   Technology has shown us that geopolitics
[00:50:49.440 --> 00:50:54.160]   and politics in general is not a zero-sum game at all.
[00:50:54.160 --> 00:50:57.200]   - So there is some rate of development that will lead
[00:50:57.200 --> 00:51:01.960]   us as a human species to lose control of this thing
[00:51:01.960 --> 00:51:05.200]   and the hope you have is that there's some lower level
[00:51:05.200 --> 00:51:09.840]   of development which will not allow us to lose control.
[00:51:09.840 --> 00:51:11.080]   This is an interesting thought you have
[00:51:11.080 --> 00:51:14.360]   about losing control, so if you are somebody
[00:51:14.360 --> 00:51:17.160]   like Sander Parchai or Sam Altman at the head
[00:51:17.160 --> 00:51:20.240]   of a company like this, you're saying if they develop
[00:51:20.240 --> 00:51:23.280]   an AGI, they too will lose control of it.
[00:51:23.280 --> 00:51:26.720]   So no one person can maintain control,
[00:51:26.720 --> 00:51:29.120]   no group of individuals can maintain control.
[00:51:29.120 --> 00:51:33.760]   - If it's created very, very soon and is a big black box
[00:51:33.760 --> 00:51:36.000]   that we don't understand, like the large language models,
[00:51:36.000 --> 00:51:39.040]   yeah, then I'm very confident they're gonna lose control.
[00:51:39.040 --> 00:51:40.760]   But this isn't just me saying it.
[00:51:40.760 --> 00:51:44.000]   Sam Altman and Demis Hassabis have both said
[00:51:44.000 --> 00:51:47.880]   and themselves acknowledged that there's really great risks
[00:51:47.880 --> 00:51:50.240]   with this and they wanna slow down once they feel
[00:51:50.240 --> 00:51:53.960]   like it's scary, but it's clear that they're stuck
[00:51:53.960 --> 00:51:57.800]   and again, Moloch is forcing them to go a little faster
[00:51:57.800 --> 00:51:59.680]   than they're comfortable with because of pressure
[00:51:59.680 --> 00:52:01.860]   from just commercial pressures, right?
[00:52:01.860 --> 00:52:06.600]   To get a bit optimistic here, of course this is a problem
[00:52:06.600 --> 00:52:08.100]   that can be ultimately solved.
[00:52:10.320 --> 00:52:14.440]   It's just to win this wisdom race, it's clear that what
[00:52:14.440 --> 00:52:17.160]   we hoped that was gonna happen hasn't happened.
[00:52:17.160 --> 00:52:20.000]   The capability progress has gone faster
[00:52:20.000 --> 00:52:22.800]   than a lot of people thought and the progress
[00:52:22.800 --> 00:52:25.120]   in the public sphere of policymaking and so on
[00:52:25.120 --> 00:52:26.480]   has gone slower than we thought.
[00:52:26.480 --> 00:52:29.060]   Even the technical AI safety has gone slower.
[00:52:29.060 --> 00:52:32.060]   A lot of the technical safety research was kind of banking
[00:52:32.060 --> 00:52:35.800]   on that large language models and other poorly understood
[00:52:35.800 --> 00:52:38.560]   systems couldn't get us all the way, that you had to build
[00:52:38.560 --> 00:52:41.240]   more of a kind of intelligence that you could understand,
[00:52:41.240 --> 00:52:45.600]   maybe it could prove itself safe, things like this.
[00:52:45.600 --> 00:52:50.360]   And I'm quite confident that this can be done
[00:52:50.360 --> 00:52:53.680]   so we can reap all the benefits, but we cannot do it
[00:52:53.680 --> 00:52:58.680]   as quickly as this out of control express train we are
[00:52:58.680 --> 00:53:00.200]   on now is gonna get the AGI.
[00:53:00.200 --> 00:53:02.600]   That's why we need a little more time, I feel.
[00:53:02.600 --> 00:53:05.640]   - Is there something to be said,
[00:53:05.640 --> 00:53:07.720]   what Sam Altman talked about, which is,
[00:53:07.720 --> 00:53:12.720]   while we're in the pre-AGI stage, to release often
[00:53:12.720 --> 00:53:17.400]   and as transparently as possible to learn a lot.
[00:53:17.400 --> 00:53:21.360]   So as opposed to being extremely cautious, release a lot.
[00:53:21.360 --> 00:53:25.880]   Don't invest in a closed development where you focus
[00:53:25.880 --> 00:53:30.280]   on AI safety while it's somewhat dumb, quote unquote.
[00:53:30.280 --> 00:53:33.240]   Release as often as possible.
[00:53:33.240 --> 00:53:38.240]   And as you start to see signs of human level intelligence
[00:53:38.240 --> 00:53:41.960]   or superhuman level intelligence, then you put a halt on it.
[00:53:41.960 --> 00:53:45.480]   - Well, what a lot of safety researchers have been saying
[00:53:45.480 --> 00:53:48.520]   for many years is that the most dangerous things you can do
[00:53:48.520 --> 00:53:52.220]   with an AI is, first of all, teach it to write code.
[00:53:52.220 --> 00:53:53.060]   - Yeah.
[00:53:53.060 --> 00:53:54.520]   - 'Cause that's the first step towards recursive
[00:53:54.520 --> 00:53:56.440]   self-improvement, which can take it from AGI
[00:53:56.440 --> 00:53:58.600]   to much higher levels.
[00:53:58.600 --> 00:54:01.560]   Okay, oops, we've done that.
[00:54:01.560 --> 00:54:05.840]   And another thing, high risk is connected to the internet.
[00:54:05.840 --> 00:54:08.640]   Let it go to websites, download stuff on its own,
[00:54:08.640 --> 00:54:09.740]   talk to people.
[00:54:09.740 --> 00:54:12.480]   Oops, we've done that already.
[00:54:12.480 --> 00:54:13.600]   You know, Eliezer Yudkowsky,
[00:54:13.600 --> 00:54:15.160]   you said you interviewed him recently, right?
[00:54:15.160 --> 00:54:16.000]   - Yes, yes.
[00:54:16.000 --> 00:54:19.200]   - He had this tweet recently, which gave me one
[00:54:19.200 --> 00:54:20.920]   of the best laughs in a while, where he was like,
[00:54:20.920 --> 00:54:22.820]   "Hey, people used to make fun of me and say,
[00:54:22.820 --> 00:54:25.120]   "you're so stupid, Eliezer, 'cause you're saying
[00:54:25.120 --> 00:54:28.320]   "you have to worry."
[00:54:28.320 --> 00:54:32.760]   Obviously, developers, once they get to really strong AI,
[00:54:32.760 --> 00:54:34.760]   first thing you're gonna do is never connect it
[00:54:34.760 --> 00:54:37.400]   to the internet, keep it in a box
[00:54:37.400 --> 00:54:39.440]   where you can really study it.
[00:54:39.440 --> 00:54:43.280]   So he had written it in the meme form,
[00:54:43.280 --> 00:54:46.520]   so it's like, "then," and then that.
[00:54:46.520 --> 00:54:51.520]   Now, "lol, let's make a chatbot."
[00:54:51.520 --> 00:54:53.920]   (both laughing)
[00:54:53.920 --> 00:54:55.880]   And the third thing, Stuart Russell,
[00:54:56.480 --> 00:55:00.360]   you know, amazing AI researcher,
[00:55:00.360 --> 00:55:05.360]   he has argued for a while that we should never teach AI
[00:55:05.360 --> 00:55:08.400]   anything about humans.
[00:55:08.400 --> 00:55:10.400]   Above all, we should never let it learn
[00:55:10.400 --> 00:55:13.020]   about human psychology and how you manipulate humans.
[00:55:13.020 --> 00:55:16.240]   That's the most dangerous kind of knowledge you can give it.
[00:55:16.240 --> 00:55:18.320]   Yeah, you can teach it all it needs to know
[00:55:18.320 --> 00:55:19.960]   about how to cure cancer and stuff like that,
[00:55:19.960 --> 00:55:23.400]   but don't let it read Daniel Kahneman's book
[00:55:23.400 --> 00:55:25.520]   about cognitive biases and all that.
[00:55:25.520 --> 00:55:30.160]   And then, "oops, lol, let's invent social media
[00:55:30.160 --> 00:55:34.720]   recommender algorithms," which do exactly that.
[00:55:34.720 --> 00:55:39.720]   They get so good at knowing us and pressing our buttons
[00:55:39.720 --> 00:55:43.440]   that we're starting to create a world now
[00:55:43.440 --> 00:55:45.440]   where we just have ever more hatred
[00:55:45.440 --> 00:55:48.760]   'cause they figured out that these algorithms,
[00:55:48.760 --> 00:55:51.920]   not out of evil, but just to make money on advertising,
[00:55:51.920 --> 00:55:56.040]   that the best way to get more engagement, the euphemism,
[00:55:56.040 --> 00:55:58.240]   get people glued to their little rectangles,
[00:55:58.240 --> 00:56:00.080]   but it's just to make them pissed off.
[00:56:00.080 --> 00:56:03.520]   - Well, that's really interesting that a large AI system
[00:56:03.520 --> 00:56:06.440]   that's doing the recommender system kind of task
[00:56:06.440 --> 00:56:09.880]   on social media is basically just studying human beings
[00:56:09.880 --> 00:56:14.780]   because it's a bunch of us rats giving it signal,
[00:56:14.780 --> 00:56:15.920]   nonstop signal.
[00:56:15.920 --> 00:56:17.960]   It'll show a thing, and then we give signal
[00:56:17.960 --> 00:56:20.880]   on whether we spread that thing, we like that thing,
[00:56:20.880 --> 00:56:22.440]   that thing increases our engagement,
[00:56:22.440 --> 00:56:24.240]   gets us to return to the platform.
[00:56:24.240 --> 00:56:26.200]   It has that on the scale of hundreds of millions
[00:56:26.200 --> 00:56:27.840]   of people constantly.
[00:56:27.840 --> 00:56:29.760]   So it's just learning and learning and learning.
[00:56:29.760 --> 00:56:32.080]   And presumably, if the number of parameters
[00:56:32.080 --> 00:56:34.240]   in the neural network that's doing the learning,
[00:56:34.240 --> 00:56:38.280]   the more end-to-end the learning is,
[00:56:38.280 --> 00:56:41.240]   the more it's able to just basically encode
[00:56:41.240 --> 00:56:43.960]   how to manipulate human behavior,
[00:56:43.960 --> 00:56:45.360]   how to control humans at scale.
[00:56:45.360 --> 00:56:47.080]   - Exactly, and that is not something
[00:56:47.080 --> 00:56:49.400]   I think is in humanity's interest.
[00:56:49.400 --> 00:56:50.240]   - Yes.
[00:56:50.240 --> 00:56:52.440]   - Now it's mainly letting some humans
[00:56:52.440 --> 00:56:56.820]   manipulate other humans for profit and power,
[00:56:56.820 --> 00:57:00.860]   which already caused a lot of damage.
[00:57:00.860 --> 00:57:03.840]   And eventually that's a sort of skill
[00:57:03.840 --> 00:57:07.880]   that can make AIs persuade humans to let them escape
[00:57:07.880 --> 00:57:10.320]   whatever safety precautions we have.
[00:57:10.320 --> 00:57:12.480]   But there was a really nice article
[00:57:12.480 --> 00:57:16.880]   in the New York Times recently by Yuval Noah Harari
[00:57:16.880 --> 00:57:19.680]   and two co-authors, including Tristan Harris
[00:57:19.680 --> 00:57:20.920]   from "The Social Dilemma."
[00:57:20.920 --> 00:57:25.160]   They have this phrase in there I love.
[00:57:25.160 --> 00:57:28.280]   Humanity's first contact with advanced AI
[00:57:28.280 --> 00:57:30.720]   was social media.
[00:57:30.720 --> 00:57:32.780]   And we lost that one.
[00:57:32.780 --> 00:57:38.200]   We now live in a country where there's much more hate
[00:57:38.200 --> 00:57:41.000]   in the world where there's much more hate, in fact.
[00:57:41.000 --> 00:57:43.960]   And in our democracy, we're having this conversation
[00:57:43.960 --> 00:57:46.860]   and people can't even agree on who won the last election.
[00:57:47.920 --> 00:57:50.680]   And we humans often point fingers at other humans
[00:57:50.680 --> 00:57:51.600]   and say it's their fault.
[00:57:51.600 --> 00:57:55.020]   But it's really Moloch and these AI algorithms.
[00:57:55.020 --> 00:57:59.900]   We got the algorithms and then Moloch
[00:57:59.900 --> 00:58:02.200]   pitted the social media companies against each other
[00:58:02.200 --> 00:58:04.160]   so nobody could have a less creepy algorithm
[00:58:04.160 --> 00:58:05.720]   'cause then they would lose out on revenue
[00:58:05.720 --> 00:58:07.040]   to the other company.
[00:58:07.040 --> 00:58:08.840]   - Is there any way to win that battle back
[00:58:08.840 --> 00:58:11.760]   just if we just linger on this one battle
[00:58:11.760 --> 00:58:13.920]   that we've lost in terms of social media?
[00:58:13.920 --> 00:58:16.920]   Is it possible to redesign social media,
[00:58:16.920 --> 00:58:20.920]   this very medium in which we use as a civilization
[00:58:20.920 --> 00:58:22.480]   to communicate with each other,
[00:58:22.480 --> 00:58:24.240]   to have these kinds of conversations,
[00:58:24.240 --> 00:58:25.760]   to have discourse to try to figure out
[00:58:25.760 --> 00:58:28.360]   how to solve the biggest problems in the world,
[00:58:28.360 --> 00:58:32.120]   whether that's nuclear war or the development of AGI?
[00:58:32.120 --> 00:58:35.920]   Is it possible to do social media correctly?
[00:58:35.920 --> 00:58:38.920]   - I think it's not only possible, but it's necessary.
[00:58:38.920 --> 00:58:40.880]   Who are we kidding that we're gonna be able to solve
[00:58:40.880 --> 00:58:42.840]   all these other challenges if we can't even have
[00:58:42.840 --> 00:58:44.000]   a conversation with each other?
[00:58:44.000 --> 00:58:45.480]   It's constructive.
[00:58:45.480 --> 00:58:47.880]   The whole idea, the key idea of democracy
[00:58:47.880 --> 00:58:50.400]   is that you get a bunch of people together
[00:58:50.400 --> 00:58:52.000]   and they have a real conversation,
[00:58:52.000 --> 00:58:53.920]   the ones you try to foster on this podcast
[00:58:53.920 --> 00:58:57.120]   where you respectfully listen to people you disagree with.
[00:58:57.120 --> 00:58:59.440]   And you realize, actually, there are some things,
[00:58:59.440 --> 00:59:01.040]   actually, some common ground we have,
[00:59:01.040 --> 00:59:04.680]   and we both agree, let's not have a nuclear war,
[00:59:04.680 --> 00:59:07.600]   let's not do that, et cetera, et cetera.
[00:59:07.600 --> 00:59:12.480]   We're kidding ourselves thinking we can face off
[00:59:12.480 --> 00:59:16.400]   the second contact with ever more powerful AI
[00:59:16.400 --> 00:59:19.160]   that's happening now with these large language models
[00:59:19.160 --> 00:59:23.640]   if we can't even have a functional conversation
[00:59:23.640 --> 00:59:25.520]   in the public space.
[00:59:25.520 --> 00:59:28.400]   That's why I started the Improve the News project,
[00:59:28.400 --> 00:59:29.920]   improvethenews.org.
[00:59:29.920 --> 00:59:33.560]   But I'm an optimist, fundamentally,
[00:59:33.560 --> 00:59:38.560]   in that there is a lot of intrinsic goodness in people
[00:59:41.760 --> 00:59:45.200]   and that what makes the difference
[00:59:45.200 --> 00:59:48.000]   between someone doing good things for humanity
[00:59:48.000 --> 00:59:51.600]   and bad things is not some sort of fairy tale thing
[00:59:51.600 --> 00:59:53.720]   that this person was born with an evil gene
[00:59:53.720 --> 00:59:55.440]   and this one was not born with a good gene.
[00:59:55.440 --> 00:59:58.240]   No, I think it's whether we put,
[00:59:58.240 --> 01:00:01.600]   whether people find themselves in situations
[01:00:01.600 --> 01:00:03.800]   that bring out the best in them
[01:00:03.800 --> 01:00:05.880]   or that bring out the worst in them.
[01:00:05.880 --> 01:00:10.080]   And I feel we're building an internet and a society
[01:00:10.080 --> 01:00:14.320]   that brings out the worst in us.
[01:00:14.320 --> 01:00:16.080]   - But it doesn't have to be that way.
[01:00:16.080 --> 01:00:16.920]   - No, it does not.
[01:00:16.920 --> 01:00:18.880]   - It's possible to create incentives
[01:00:18.880 --> 01:00:22.680]   and also create incentives that make money,
[01:00:22.680 --> 01:00:24.800]   that both make money and bring out the best in people.
[01:00:24.800 --> 01:00:25.880]   - I mean, in the long term,
[01:00:25.880 --> 01:00:27.880]   it's not a good investment for anyone
[01:00:27.880 --> 01:00:30.400]   to have a nuclear war, for example.
[01:00:30.400 --> 01:00:32.720]   And is it a good investment for humanity
[01:00:32.720 --> 01:00:35.680]   if we just ultimately replace all humans by machines
[01:00:35.680 --> 01:00:37.440]   and then we're so obsolete that eventually
[01:00:37.440 --> 01:00:39.720]   there are no humans left?
[01:00:40.600 --> 01:00:43.000]   - Well, it depends, I guess, on how you do the math.
[01:00:43.000 --> 01:00:46.640]   But I would say by any reasonable economics,
[01:00:46.640 --> 01:00:48.440]   if you look at the future income of humans
[01:00:48.440 --> 01:00:51.000]   and there aren't any, that's not a good investment.
[01:00:51.000 --> 01:00:56.360]   Moreover, why can't we have a little bit of pride
[01:00:56.360 --> 01:00:58.120]   in our species, damn it?
[01:00:58.120 --> 01:00:59.800]   Why should we just build another species
[01:00:59.800 --> 01:01:01.840]   that gets rid of us?
[01:01:01.840 --> 01:01:03.440]   If we were Neanderthals,
[01:01:03.440 --> 01:01:07.240]   would we really consider it a smart move
[01:01:07.240 --> 01:01:11.560]   if we had really advanced biotech to build Homo sapiens?
[01:01:11.560 --> 01:01:15.840]   You might say, "Hey, Max, yeah, let's build
[01:01:15.840 --> 01:01:18.920]   these Homo sapiens.
[01:01:18.920 --> 01:01:20.280]   They're gonna be smarter than us.
[01:01:20.280 --> 01:01:23.400]   Maybe they can help us defend us better against predators
[01:01:23.400 --> 01:01:27.200]   and help fix up our caves, make them nicer.
[01:01:27.200 --> 01:01:29.000]   We'll control them undoubtedly."
[01:01:29.000 --> 01:01:31.840]   So then they build a couple, a little baby girl,
[01:01:31.840 --> 01:01:32.840]   a little baby boy.
[01:01:35.960 --> 01:01:39.400]   And then you have some wise old Neanderthal elders like,
[01:01:39.400 --> 01:01:44.400]   "Hmm, I'm scared that we're opening a Pandora's box here
[01:01:44.400 --> 01:01:46.800]   and that we're gonna get outsmarted
[01:01:46.800 --> 01:01:51.800]   by these super Neanderthal intelligences
[01:01:51.800 --> 01:01:55.280]   and there won't be any Neanderthals left."
[01:01:55.280 --> 01:01:56.640]   But then you have a bunch of others in the cave,
[01:01:56.640 --> 01:01:58.920]   "Well, yeah, you're such a Luddite scaremonger.
[01:01:58.920 --> 01:02:00.920]   Of course, they're gonna wanna keep us around
[01:02:00.920 --> 01:02:02.320]   'cause we are their creators.
[01:02:02.320 --> 01:02:05.320]   I think the smarter they get,
[01:02:05.320 --> 01:02:06.280]   the nicer they're gonna get.
[01:02:06.280 --> 01:02:07.400]   They're gonna leave us.
[01:02:07.400 --> 01:02:11.480]   They're gonna want us around and it's gonna be fine.
[01:02:11.480 --> 01:02:14.400]   And besides, look at these babies, they're so cute.
[01:02:14.400 --> 01:02:16.000]   Clearly, they're totally harmless."
[01:02:16.000 --> 01:02:19.160]   That's exactly, those babies are exactly GPT-4.
[01:02:19.160 --> 01:02:24.160]   It's not, I wanna be clear, it's not GPT-4 that's terrifying.
[01:02:24.160 --> 01:02:27.800]   It's the GPT-4 is a baby technology.
[01:02:27.800 --> 01:02:31.440]   You know, and Microsoft even had a paper recently out
[01:02:33.040 --> 01:02:36.720]   with the title something like "Sparkles of AGI."
[01:02:36.720 --> 01:02:39.840]   Well, they were basically saying this is baby AI,
[01:02:39.840 --> 01:02:41.640]   like these little Neanderthal babies.
[01:02:41.640 --> 01:02:44.600]   And it's gonna grow up.
[01:02:44.600 --> 01:02:48.520]   There's gonna be other systems from the same company,
[01:02:48.520 --> 01:02:51.000]   from other companies, they'll be way more powerful
[01:02:51.000 --> 01:02:53.120]   but they're gonna take all the things,
[01:02:53.120 --> 01:02:55.440]   ideas from these babies.
[01:02:55.440 --> 01:02:58.600]   And before we know it, we're gonna be like
[01:02:58.600 --> 01:03:02.680]   those last Neanderthals who were pretty disappointed.
[01:03:02.680 --> 01:03:06.240]   And when they realized that they were getting replaced.
[01:03:06.240 --> 01:03:07.920]   - Well, this interesting point you make,
[01:03:07.920 --> 01:03:10.160]   which is the programming, it's entirely possible
[01:03:10.160 --> 01:03:13.200]   that GPT-4 is already the kind of system
[01:03:13.200 --> 01:03:18.200]   that can change everything by writing programs.
[01:03:18.200 --> 01:03:21.880]   - Like three, yeah, because it's Life 2.0,
[01:03:21.880 --> 01:03:25.280]   the systems I'm afraid of are gonna look nothing
[01:03:25.280 --> 01:03:27.840]   like a large language model and they're not gonna.
[01:03:29.080 --> 01:03:32.880]   But once it or other people figure out a way
[01:03:32.880 --> 01:03:35.080]   of using this tech to make much better tech,
[01:03:35.080 --> 01:03:38.160]   it's just constantly replacing its software.
[01:03:38.160 --> 01:03:42.040]   And from everything we've seen about how these work
[01:03:42.040 --> 01:03:45.520]   under the hood, they're like the minimum viable intelligence.
[01:03:45.520 --> 01:03:47.680]   They do everything in the dumbest way
[01:03:47.680 --> 01:03:49.080]   that still works sort of.
[01:03:49.080 --> 01:03:49.920]   - Yeah.
[01:03:49.920 --> 01:03:54.280]   - So they are Life 3.0, except when they replace
[01:03:54.280 --> 01:03:56.440]   their software, it's a lot faster
[01:03:56.600 --> 01:03:59.120]   than when you decide to learn Swedish.
[01:03:59.120 --> 01:04:04.680]   And moreover, they think a lot faster than us too.
[01:04:04.680 --> 01:04:09.680]   So when, we don't think on how one logical step
[01:04:09.680 --> 01:04:18.400]   every nanosecond or so the way they do.
[01:04:18.400 --> 01:04:21.920]   And we can't also just suddenly scale up our hardware
[01:04:21.920 --> 01:04:24.840]   massively in the cloud, we're so limited, right?
[01:04:26.160 --> 01:04:31.160]   So they are also Life, can soon become a little bit more
[01:04:31.160 --> 01:04:36.040]   like Life 3.0 in that if they need more hardware,
[01:04:36.040 --> 01:04:38.160]   hey, just rent it in the cloud, you know?
[01:04:38.160 --> 01:04:39.000]   How do you pay for it?
[01:04:39.000 --> 01:04:41.000]   Well, with all the services you provide.
[01:04:41.000 --> 01:04:49.760]   - And what we haven't seen yet, which could change a lot,
[01:04:49.760 --> 01:04:53.920]   is entire software systems.
[01:04:53.920 --> 01:04:57.480]   So right now, programming is done sort of in bits and pieces
[01:04:57.480 --> 01:05:01.240]   as an assistant tool to humans.
[01:05:01.240 --> 01:05:03.120]   But I do a lot of programming,
[01:05:03.120 --> 01:05:05.720]   and with the kind of stuff that GPT-4 is able to do,
[01:05:05.720 --> 01:05:07.900]   I mean, it's replacing a lot what I'm able to do.
[01:05:07.900 --> 01:05:10.760]   But you still need a human in the loop
[01:05:10.760 --> 01:05:13.680]   to kind of manage the design of things,
[01:05:13.680 --> 01:05:15.760]   manage like what are the prompts
[01:05:15.760 --> 01:05:17.320]   that generate the kind of stuff,
[01:05:17.320 --> 01:05:19.360]   to do some basic adjustment of the code,
[01:05:19.360 --> 01:05:21.120]   just do some debugging.
[01:05:21.120 --> 01:05:25.440]   But if it's possible to add on top of GPT-4
[01:05:25.440 --> 01:05:30.440]   kind of feedback loop of self-debugging, improving the code,
[01:05:30.440 --> 01:05:35.440]   and then you launch that system onto the wild,
[01:05:35.440 --> 01:05:37.400]   on the internet, because everything is connected,
[01:05:37.400 --> 01:05:39.920]   and have it do things, have it interact with humans,
[01:05:39.920 --> 01:05:41.240]   and then get that feedback,
[01:05:41.240 --> 01:05:44.720]   now you have this giant ecosystem of humans.
[01:05:44.720 --> 01:05:47.720]   That's one of the things that Elon Musk recently
[01:05:47.720 --> 01:05:51.920]   sort of tweeted as a case why everyone needs to pay $7
[01:05:51.920 --> 01:05:53.080]   or whatever for Twitter.
[01:05:53.080 --> 01:05:54.760]   - To make sure they're real.
[01:05:54.760 --> 01:05:55.600]   - Make sure they're real.
[01:05:55.600 --> 01:05:57.480]   We're now going to be living in a world
[01:05:57.480 --> 01:06:01.120]   where the bots are getting smarter and smarter and smarter
[01:06:01.120 --> 01:06:05.880]   to a degree where you can't tell the difference
[01:06:05.880 --> 01:06:06.920]   between a human and a bot.
[01:06:06.920 --> 01:06:07.760]   - That's right.
[01:06:07.760 --> 01:06:10.620]   - And now you can have bots outnumber humans
[01:06:10.620 --> 01:06:14.840]   by one million to one, which is why he's making a case
[01:06:14.840 --> 01:06:17.360]   why you have to pay to prove you're human,
[01:06:17.360 --> 01:06:19.400]   which is one of the only mechanisms to prove,
[01:06:19.400 --> 01:06:21.280]   which is depressing.
[01:06:21.280 --> 01:06:24.480]   - And I feel we have to remember,
[01:06:24.480 --> 01:06:27.920]   as individuals, we should, from time to time,
[01:06:27.920 --> 01:06:29.920]   ask ourselves why are we doing what we're doing,
[01:06:29.920 --> 01:06:33.000]   right, and as a species, we need to do that too.
[01:06:33.000 --> 01:06:36.240]   So if we're building, as you say,
[01:06:36.240 --> 01:06:39.280]   machines that are outnumbering us
[01:06:39.280 --> 01:06:41.360]   and more and more outsmarting us
[01:06:41.360 --> 01:06:43.080]   and replacing us on the job market,
[01:06:43.080 --> 01:06:46.480]   not just for the dangerous and boring tasks,
[01:06:46.480 --> 01:06:49.480]   but also for writing poems and doing art
[01:06:49.480 --> 01:06:52.160]   and things that a lot of people find really meaningful,
[01:06:52.160 --> 01:06:53.760]   gotta ask ourselves, why?
[01:06:53.760 --> 01:06:54.900]   Why are we doing this?
[01:06:54.900 --> 01:07:01.900]   The answer is Moloch is tricking us into doing it.
[01:07:01.900 --> 01:07:03.120]   And it's such a clever trick
[01:07:03.120 --> 01:07:04.520]   that even though we see the trick,
[01:07:04.520 --> 01:07:07.360]   we still have no choice but to fall for it, right?
[01:07:07.360 --> 01:07:14.680]   Also, the thing you said about you using co-pilot AI tools
[01:07:15.720 --> 01:07:17.400]   to program faster, how many times,
[01:07:17.400 --> 01:07:20.360]   what factor faster would you say you code now?
[01:07:20.360 --> 01:07:22.560]   Does it go twice as fast or?
[01:07:22.560 --> 01:07:25.960]   - I don't really, because it's such a new tool.
[01:07:25.960 --> 01:07:27.040]   - Yeah.
[01:07:27.040 --> 01:07:29.580]   - I don't know if speed is significantly improved,
[01:07:29.580 --> 01:07:33.200]   but it feels like I'm a year away
[01:07:33.200 --> 01:07:36.960]   from being five to 10 times faster.
[01:07:36.960 --> 01:07:39.680]   - So if that's typical for programmers,
[01:07:39.680 --> 01:07:43.260]   then you're already seeing another kind of self,
[01:07:44.240 --> 01:07:45.680]   recursive self-improvement, right?
[01:07:45.680 --> 01:07:50.480]   Because previously, one major generation of improvement
[01:07:50.480 --> 01:07:53.440]   of the code would happen on the human R&D timescale.
[01:07:53.440 --> 01:07:55.400]   And now if that's five times shorter,
[01:07:55.400 --> 01:07:57.960]   then it's gonna take five times less time
[01:07:57.960 --> 01:08:00.320]   than it otherwise would to develop the next level
[01:08:00.320 --> 01:08:02.520]   of these tools and so on.
[01:08:02.520 --> 01:08:06.440]   So this is exactly the beginning
[01:08:06.440 --> 01:08:09.040]   of an intelligence explosion.
[01:08:09.040 --> 01:08:11.760]   There can be humans in the loop a lot in the early stages,
[01:08:11.760 --> 01:08:14.480]   and then eventually humans are needed less and less,
[01:08:14.480 --> 01:08:16.440]   and the machines can more kind of go along.
[01:08:16.440 --> 01:08:19.600]   But what you said there is just an exact example
[01:08:19.600 --> 01:08:20.720]   of these sort of things.
[01:08:20.720 --> 01:08:22.120]   Another thing which,
[01:08:22.120 --> 01:08:27.520]   I was kind of lying on my psychiatrist,
[01:08:27.520 --> 01:08:29.680]   imagining I'm on a psychiatrist's couch here,
[01:08:29.680 --> 01:08:31.480]   saying, "What are my fears that people would do
[01:08:31.480 --> 01:08:33.040]   "with AI systems?"
[01:08:33.040 --> 01:08:37.080]   So I mentioned three that I had fears about many years ago
[01:08:37.080 --> 01:08:40.620]   that they would do, namely teach you the code,
[01:08:41.680 --> 01:08:42.760]   connect it to the internet,
[01:08:42.760 --> 01:08:45.200]   then teach it to manipulate humans.
[01:08:45.200 --> 01:08:48.200]   A fourth one is building an API
[01:08:48.200 --> 01:08:52.800]   where code can control this super powerful thing, right?
[01:08:52.800 --> 01:08:54.560]   That's very unfortunate,
[01:08:54.560 --> 01:08:58.520]   because one thing that systems like GPT-4
[01:08:58.520 --> 01:09:00.840]   have going for them is that they are an oracle
[01:09:00.840 --> 01:09:04.200]   in the sense that they just answer questions.
[01:09:04.200 --> 01:09:07.080]   There is no robot connected to GPT-4.
[01:09:07.080 --> 01:09:10.480]   GPT-4 can't go and do stock trading based on its thinking.
[01:09:10.480 --> 01:09:13.000]   It is not an agent.
[01:09:13.000 --> 01:09:14.480]   An intelligent agent is something
[01:09:14.480 --> 01:09:16.560]   that takes in information from the world,
[01:09:16.560 --> 01:09:20.520]   processes it to figure out what action to take
[01:09:20.520 --> 01:09:22.360]   based on its goals that it has,
[01:09:22.360 --> 01:09:26.460]   and then does something back on the world.
[01:09:26.460 --> 01:09:29.800]   But once you have an API, for example, GPT-4,
[01:09:29.800 --> 01:09:33.040]   nothing stops Joe Schmo and a lot of other people
[01:09:33.040 --> 01:09:35.680]   from building real agents,
[01:09:35.680 --> 01:09:38.080]   which just keep making calls somewhere
[01:09:38.080 --> 01:09:39.480]   in some inner loop somewhere
[01:09:39.480 --> 01:09:41.720]   to these powerful oracle systems,
[01:09:41.720 --> 01:09:45.600]   which makes them themselves much more powerful.
[01:09:45.600 --> 01:09:48.920]   That's another kind of unfortunate development,
[01:09:48.920 --> 01:09:53.360]   which I think we would have been better off delaying.
[01:09:53.360 --> 01:09:55.040]   I don't want to pick on any particular companies.
[01:09:55.040 --> 01:09:58.260]   I think they're all under a lot of pressure to make money.
[01:09:58.260 --> 01:10:04.480]   And again, the reason we're calling for this pause
[01:10:04.480 --> 01:10:06.320]   is to give them all cover
[01:10:06.320 --> 01:10:08.840]   to do what they know is the right thing,
[01:10:08.840 --> 01:10:10.280]   slow down a little bit at this point.
[01:10:10.280 --> 01:10:12.860]   But everything we've talked about,
[01:10:12.860 --> 01:10:17.920]   I hope we'll make it clear to people watching this
[01:10:17.920 --> 01:10:22.000]   why these sort of human level tools
[01:10:22.000 --> 01:10:23.640]   can cause a gradual acceleration.
[01:10:23.640 --> 01:10:25.440]   You keep using yesterday's technology
[01:10:25.440 --> 01:10:27.320]   to build tomorrow's technology.
[01:10:27.320 --> 01:10:30.320]   And when you do that over and over again,
[01:10:30.320 --> 01:10:32.360]   you naturally get an explosion.
[01:10:32.360 --> 01:10:34.880]   That's the definition of an explosion in science.
[01:10:36.520 --> 01:10:41.520]   If you have two people and they fall in love,
[01:10:41.520 --> 01:10:44.080]   now you have four people,
[01:10:44.080 --> 01:10:46.120]   and then they can make more babies,
[01:10:46.120 --> 01:10:47.120]   and now you have eight people,
[01:10:47.120 --> 01:10:50.840]   and then you have 16, 32, 64, et cetera.
[01:10:50.840 --> 01:10:53.200]   We call that a population explosion,
[01:10:53.200 --> 01:10:55.160]   where it's just that each,
[01:10:55.160 --> 01:10:59.560]   if it's instead free neutrons in a nuclear reaction,
[01:10:59.560 --> 01:11:02.080]   if each one can make more than one,
[01:11:02.080 --> 01:11:03.600]   then you get an exponential growth in that.
[01:11:03.600 --> 01:11:05.800]   We call it a nuclear explosion.
[01:11:05.800 --> 01:11:06.920]   All explosions are like that.
[01:11:06.920 --> 01:11:08.040]   And an intelligence explosion,
[01:11:08.040 --> 01:11:09.440]   it's just exactly the same principle,
[01:11:09.440 --> 01:11:11.240]   that some amount of intelligence
[01:11:11.240 --> 01:11:14.040]   can make more intelligence than that,
[01:11:14.040 --> 01:11:15.440]   and then repeat.
[01:11:15.440 --> 01:11:17.680]   You always get exponentials.
[01:11:17.680 --> 01:11:19.040]   - What's your intuition why it does?
[01:11:19.040 --> 01:11:21.000]   You mentioned there's some technical reasons
[01:11:21.000 --> 01:11:23.680]   why it doesn't stop at a certain point.
[01:11:23.680 --> 01:11:24.720]   What's your intuition?
[01:11:24.720 --> 01:11:28.360]   And do you have any intuition why it might stop?
[01:11:28.360 --> 01:11:29.400]   - It's obviously gonna stop
[01:11:29.400 --> 01:11:31.720]   when it bumps up against the laws of physics.
[01:11:31.720 --> 01:11:32.960]   There are some things you just can't do
[01:11:32.960 --> 01:11:34.480]   no matter how smart you are, right?
[01:11:34.480 --> 01:11:35.320]   - Allegedly.
[01:11:36.200 --> 01:11:41.080]   - 'Cause we don't know the full laws of physics yet, right?
[01:11:41.080 --> 01:11:42.680]   - Seth Lloyd wrote a really cool paper
[01:11:42.680 --> 01:11:46.080]   on the physical limits on computation, for example.
[01:11:46.080 --> 01:11:49.000]   If you put too much energy into it,
[01:11:49.000 --> 01:11:51.920]   then in finite space, it'll turn into a black hole.
[01:11:51.920 --> 01:11:53.320]   You can't move information around
[01:11:53.320 --> 01:11:54.920]   faster than the speed of light, stuff like that.
[01:11:54.920 --> 01:11:58.680]   But it's hard to store way more
[01:11:58.680 --> 01:12:02.720]   than a modest number of bits per atom, et cetera.
[01:12:02.720 --> 01:12:06.920]   But those limits are just astronomically above,
[01:12:06.920 --> 01:12:09.320]   like 30 orders of magnitude above where we are now.
[01:12:09.320 --> 01:12:14.720]   Bigger jump in intelligence
[01:12:14.720 --> 01:12:18.480]   than if you go from an ant to a human.
[01:12:18.480 --> 01:12:23.560]   I think, of course, what we want to do
[01:12:23.560 --> 01:12:26.720]   is have a controlled thing.
[01:12:26.720 --> 01:12:28.680]   A nuclear reactor, you put moderators in
[01:12:28.680 --> 01:12:31.480]   to make sure exactly it doesn't blow up out of control.
[01:12:32.480 --> 01:12:37.480]   When we do experiments with biology and cells and so on,
[01:12:37.480 --> 01:12:41.160]   we also try to make sure it doesn't get out of control.
[01:12:41.160 --> 01:12:44.440]   We can do this with AI, too.
[01:12:44.440 --> 01:12:47.360]   The thing is, we haven't succeeded yet.
[01:12:47.360 --> 01:12:51.680]   And Moloch is exactly doing the opposite,
[01:12:51.680 --> 01:12:54.400]   just fueling, just egging everybody on,
[01:12:54.400 --> 01:12:56.360]   faster, faster, faster,
[01:12:56.360 --> 01:12:58.280]   or the other company is gonna catch up with you,
[01:12:58.280 --> 01:13:00.680]   or the other country is gonna catch up with you.
[01:13:01.840 --> 01:13:03.400]   We have to want this stuff.
[01:13:03.400 --> 01:13:06.360]   I don't believe in this,
[01:13:06.360 --> 01:13:09.400]   just asking people to look into their hearts
[01:13:09.400 --> 01:13:10.880]   and do the right thing.
[01:13:10.880 --> 01:13:12.680]   It's easier for others to say that,
[01:13:12.680 --> 01:13:14.680]   but if you're in a situation
[01:13:14.680 --> 01:13:17.360]   where your company is gonna get screwed,
[01:13:17.360 --> 01:13:23.840]   by other companies that are not stopping,
[01:13:23.840 --> 01:13:26.080]   you're putting people in a very hard situation.
[01:13:26.080 --> 01:13:26.920]   The right thing to do
[01:13:26.920 --> 01:13:29.920]   is change the whole incentive structure instead.
[01:13:29.920 --> 01:13:31.520]   And this is not an old...
[01:13:31.520 --> 01:13:34.360]   Maybe I should say one more thing about this,
[01:13:34.360 --> 01:13:37.440]   'cause Moloch has been around as humanity's
[01:13:37.440 --> 01:13:40.720]   number one or number two enemy
[01:13:40.720 --> 01:13:42.320]   since the beginning of civilization.
[01:13:42.320 --> 01:13:46.600]   And we came up with some really cool countermeasures.
[01:13:46.600 --> 01:13:49.760]   First of all, already over 100,000 years ago,
[01:13:49.760 --> 01:13:52.960]   evolution realized that it was very unhelpful
[01:13:52.960 --> 01:13:55.520]   that people kept killing each other all the time.
[01:13:55.520 --> 01:13:59.240]   So it genetically gave us compassion.
[01:14:00.240 --> 01:14:03.240]   And made it so that if you get two drunk dudes
[01:14:03.240 --> 01:14:05.040]   getting into a pointless bar fight,
[01:14:05.040 --> 01:14:07.920]   they might give each other black eyes,
[01:14:07.920 --> 01:14:10.600]   but they have a lot of inhibition
[01:14:10.600 --> 01:14:12.760]   towards just killing each other.
[01:14:12.760 --> 01:14:18.160]   And similarly, if you find a baby lying on the street
[01:14:18.160 --> 01:14:20.520]   when you go out for your morning jog tomorrow,
[01:14:20.520 --> 01:14:22.120]   you're gonna stop and pick it up, right?
[01:14:22.120 --> 01:14:25.880]   Even though it may make you late for your next podcast.
[01:14:25.880 --> 01:14:28.080]   So evolution gave us these genes
[01:14:28.080 --> 01:14:32.320]   that make our own egoistic incentives more aligned
[01:14:32.320 --> 01:14:35.520]   with what's good for the greater group we're part of.
[01:14:35.520 --> 01:14:39.760]   And then as we got a bit more sophisticated
[01:14:39.760 --> 01:14:43.640]   and developed language, we invented gossip,
[01:14:43.640 --> 01:14:45.800]   which is also a fantastic anti-Moloch.
[01:14:45.800 --> 01:14:51.680]   'Cause now it really discourages liars,
[01:14:51.680 --> 01:14:57.040]   moochers, cheaters, because their own incentive now
[01:14:57.040 --> 01:15:00.880]   is not to do this, because word quickly gets around
[01:15:00.880 --> 01:15:02.880]   and then suddenly people aren't gonna invite them
[01:15:02.880 --> 01:15:05.640]   to their dinners anymore or trust them.
[01:15:05.640 --> 01:15:07.560]   And then when we got still more sophisticated
[01:15:07.560 --> 01:15:11.440]   and bigger societies, invented the legal system,
[01:15:11.440 --> 01:15:14.240]   where even strangers who couldn't rely on gossip
[01:15:14.240 --> 01:15:16.640]   and things like this would treat each other,
[01:15:16.640 --> 01:15:17.880]   would have an incentive.
[01:15:17.880 --> 01:15:19.240]   Now those guys in the bar fight,
[01:15:19.240 --> 01:15:21.080]   even if someone is so drunk
[01:15:21.080 --> 01:15:24.680]   that he actually wants to kill the other guy,
[01:15:26.160 --> 01:15:28.080]   he also has a little thought in the back of his head
[01:15:28.080 --> 01:15:30.480]   that, "Do I really wanna spend the next 10 years
[01:15:30.480 --> 01:15:34.680]   eating really crappy food in a small room?
[01:15:34.680 --> 01:15:38.760]   I'm just gonna chill out."
[01:15:38.760 --> 01:15:40.840]   And we similarly have tried to give these incentives
[01:15:40.840 --> 01:15:44.360]   to our corporations by having regulation
[01:15:44.360 --> 01:15:45.760]   and all sorts of oversight,
[01:15:45.760 --> 01:15:48.480]   so that their incentives are aligned with the greater good.
[01:15:48.480 --> 01:15:49.560]   We tried really hard.
[01:15:49.560 --> 01:15:54.480]   And the big problem that we're failing now
[01:15:55.640 --> 01:15:57.480]   is not that we haven't tried before,
[01:15:57.480 --> 01:16:00.040]   but it's just that the tech is growing much,
[01:16:00.040 --> 01:16:01.480]   is developing much faster
[01:16:01.480 --> 01:16:03.440]   than the regulators have been able to keep up.
[01:16:03.440 --> 01:16:06.720]   So regulators, it's kind of comical,
[01:16:06.720 --> 01:16:10.160]   like European Union right now is doing this AI act, right?
[01:16:10.160 --> 01:16:13.040]   Which, in the beginning,
[01:16:13.040 --> 01:16:16.040]   they had a little opt-out exception
[01:16:16.040 --> 01:16:19.600]   that GPT-4 would be completely excluded from regulation.
[01:16:19.600 --> 01:16:21.680]   Brilliant idea.
[01:16:21.680 --> 01:16:23.240]   - What's the logic behind that?
[01:16:24.240 --> 01:16:27.400]   - Some lobbyists pushed successfully for this.
[01:16:27.400 --> 01:16:28.600]   So we were actually quite involved
[01:16:28.600 --> 01:16:30.080]   with the Future Life Institute,
[01:16:30.080 --> 01:16:34.160]   Mark Brackel, Christo Ouk, Anthony Aguirre, and others.
[01:16:34.160 --> 01:16:38.160]   We're quite involved with educating various people
[01:16:38.160 --> 01:16:39.120]   involved in this process
[01:16:39.120 --> 01:16:42.960]   about these general purpose AI models coming
[01:16:42.960 --> 01:16:45.360]   and pointing out that they would become the laughingstock
[01:16:45.360 --> 01:16:46.800]   if they didn't put it in.
[01:16:46.800 --> 01:16:48.840]   So the French started pushing for it.
[01:16:48.840 --> 01:16:50.840]   It got put in to the draft,
[01:16:50.840 --> 01:16:52.520]   and it looked like all was good.
[01:16:52.520 --> 01:16:56.800]   And then there was a huge counter push from lobbyists.
[01:16:56.800 --> 01:16:59.520]   There were more lobbyists in Brussels from tech companies
[01:16:59.520 --> 01:17:02.440]   than from oil companies, for example.
[01:17:02.440 --> 01:17:04.000]   And it looked like it might,
[01:17:04.000 --> 01:17:06.680]   is it gonna maybe get taken out again?
[01:17:06.680 --> 01:17:09.000]   And now GPT-4 happened,
[01:17:09.000 --> 01:17:10.560]   and I think it's gonna stay in.
[01:17:10.560 --> 01:17:12.320]   But this just shows, you know,
[01:17:12.320 --> 01:17:14.240]   Moloch can be defeated,
[01:17:14.240 --> 01:17:18.080]   but the challenge we're facing is that the tech
[01:17:18.080 --> 01:17:23.080]   is generally much faster than what the policymakers are.
[01:17:23.080 --> 01:17:25.640]   And a lot of the policymakers
[01:17:25.640 --> 01:17:28.160]   also don't have a tech background.
[01:17:28.160 --> 01:17:31.200]   So it's, you know, we really need to work hard
[01:17:31.200 --> 01:17:34.800]   to educate them on what's taking place here.
[01:17:34.800 --> 01:17:39.240]   So we're getting the situation where the first kind of non,
[01:17:39.240 --> 01:17:41.320]   so I define artificial intelligence
[01:17:41.320 --> 01:17:43.520]   just as non-biological intelligence, right?
[01:17:44.680 --> 01:17:46.160]   And by that definition,
[01:17:46.160 --> 01:17:50.560]   a company, a corporation is also an artificial intelligence
[01:17:50.560 --> 01:17:53.960]   because the corporation isn't, it's humans, it's a system.
[01:17:53.960 --> 01:17:56.040]   If its CEO decides,
[01:17:56.040 --> 01:17:58.760]   if the CEO of a tobacco company decides one morning
[01:17:58.760 --> 01:18:01.000]   that she or he doesn't wanna sell cigarettes anymore,
[01:18:01.000 --> 01:18:02.840]   they'll just put another CEO in there.
[01:18:02.840 --> 01:18:08.080]   It's not enough to align the incentives of individual people
[01:18:08.080 --> 01:18:12.920]   or align individual computers' incentives to their owners,
[01:18:12.920 --> 01:18:16.120]   which is what technically AI safety research is about.
[01:18:16.120 --> 01:18:18.800]   You also have to align the incentives of corporations
[01:18:18.800 --> 01:18:19.840]   with the greater good.
[01:18:19.840 --> 01:18:23.040]   And some corporations have gotten so big and so powerful
[01:18:23.040 --> 01:18:26.400]   very quickly that in many cases,
[01:18:26.400 --> 01:18:30.440]   their lobbyists instead align the regulators
[01:18:30.440 --> 01:18:33.000]   to what they want rather than the other way around.
[01:18:33.000 --> 01:18:35.600]   It's a classic regulatory capture.
[01:18:35.600 --> 01:18:40.400]   - All right, is the thing that the slowdown hopes to achieve
[01:18:40.400 --> 01:18:43.560]   is give enough time to the regulators to catch up
[01:18:43.560 --> 01:18:46.280]   or enough time to the companies themselves to breathe
[01:18:46.280 --> 01:18:48.880]   and understand how to do AI safety correctly?
[01:18:48.880 --> 01:18:52.000]   - I think both, but I think that the vision,
[01:18:52.000 --> 01:18:55.240]   the path to success I see is first you give a breather
[01:18:55.240 --> 01:18:58.040]   actually to the people in these companies,
[01:18:58.040 --> 01:19:00.240]   their leadership who wants to do the right thing
[01:19:00.240 --> 01:19:03.080]   and they all have safety teams and so on on their companies.
[01:19:03.080 --> 01:19:07.240]   Give them a chance to get together with the other companies
[01:19:08.720 --> 01:19:11.320]   and the outside pressure can also help catalyze that
[01:19:11.320 --> 01:19:17.280]   and work out what is it that's,
[01:19:17.280 --> 01:19:21.200]   what are the reasonable safety requirements
[01:19:21.200 --> 01:19:25.040]   one should put on future systems before they get rolled out?
[01:19:25.040 --> 01:19:27.520]   There are a lot of people also in academia
[01:19:27.520 --> 01:19:29.240]   and elsewhere outside of these companies
[01:19:29.240 --> 01:19:30.320]   who can be brought into this
[01:19:30.320 --> 01:19:32.760]   and have a lot of very good ideas.
[01:19:32.760 --> 01:19:35.480]   And then I think it's very realistic
[01:19:35.480 --> 01:19:39.880]   that within six months you can get these people coming up,
[01:19:39.880 --> 01:19:40.880]   so here's a white paper,
[01:19:40.880 --> 01:19:43.440]   here's where we all think it's reasonable.
[01:19:43.440 --> 01:19:45.160]   You know, you didn't,
[01:19:45.160 --> 01:19:46.760]   just because cars killed a lot of people,
[01:19:46.760 --> 01:19:48.080]   you didn't ban cars,
[01:19:48.080 --> 01:19:50.200]   but they got together a bunch of people and decided,
[01:19:50.200 --> 01:19:52.320]   you know, in order to be allowed to sell a car,
[01:19:52.320 --> 01:19:53.920]   it has to have a seatbelt in it.
[01:19:53.920 --> 01:19:58.160]   They're the analogous things that you can start requiring
[01:19:58.160 --> 01:20:03.080]   a future AI systems so that they are safe.
[01:20:03.080 --> 01:20:08.080]   And once this heavy lifting,
[01:20:08.080 --> 01:20:11.760]   this intellectual work has been done by experts in the field,
[01:20:11.760 --> 01:20:13.520]   which can be done quickly,
[01:20:13.520 --> 01:20:16.200]   I think it's going to be quite easy to get policymakers
[01:20:16.200 --> 01:20:19.360]   to see, yeah, this is a good idea.
[01:20:19.360 --> 01:20:24.360]   And it's, you know, for the companies to fight Moloch,
[01:20:24.360 --> 01:20:27.760]   they want, and I believe Sam Altman
[01:20:27.760 --> 01:20:29.120]   has explicitly called for this,
[01:20:29.120 --> 01:20:31.000]   they want the regulators to actually adopt it
[01:20:31.000 --> 01:20:33.840]   so that their competition is going to abide by it too, right?
[01:20:33.840 --> 01:20:38.840]   You don't want to be enacting all these principles
[01:20:38.840 --> 01:20:40.760]   and then you abide by them,
[01:20:40.760 --> 01:20:43.760]   and then there's this one little company
[01:20:43.760 --> 01:20:46.880]   that doesn't sign on to it,
[01:20:46.880 --> 01:20:49.640]   and then now they can gradually overtake you.
[01:20:49.640 --> 01:20:51.080]   Then the companies will get,
[01:20:51.080 --> 01:20:54.280]   be able to sleep secure,
[01:20:54.280 --> 01:20:56.680]   knowing that everybody's playing by the same rules.
[01:20:56.680 --> 01:20:59.600]   - So do you think it's possible to develop guardrails
[01:21:00.800 --> 01:21:05.800]   that keep the systems from basically
[01:21:05.800 --> 01:21:09.200]   damaging irreparably humanity
[01:21:09.200 --> 01:21:12.240]   while still enabling sort of the capitalist-fueled
[01:21:12.240 --> 01:21:13.640]   competition between companies
[01:21:13.640 --> 01:21:16.960]   as they develop how to best make money with this AI?
[01:21:16.960 --> 01:21:18.040]   You think there's a balancing--
[01:21:18.040 --> 01:21:19.160]   - Totally. - That's possible?
[01:21:19.160 --> 01:21:20.560]   - Absolutely, I mean, we've seen that
[01:21:20.560 --> 01:21:23.240]   in many other sectors where you've had the free market
[01:21:23.240 --> 01:21:27.360]   produce quite good things without causing particular harm.
[01:21:28.600 --> 01:21:30.800]   When the guardrails are there and they work,
[01:21:30.800 --> 01:21:35.360]   capitalism is a very good way of optimizing
[01:21:35.360 --> 01:21:38.120]   for just getting the same things done more efficiently.
[01:21:38.120 --> 01:21:40.840]   But it was good, and in hindsight,
[01:21:40.840 --> 01:21:42.360]   and I've never met anyone,
[01:21:42.360 --> 01:21:48.160]   even on parties way over on the right,
[01:21:48.160 --> 01:21:51.720]   in any country who thinks it was a terrible idea
[01:21:51.720 --> 01:21:55.200]   to ban child labor, for example.
[01:21:55.200 --> 01:21:57.880]   - Yeah, but it seems like this particular technology
[01:21:57.880 --> 01:22:02.560]   has gotten so good so fast, become powerful
[01:22:02.560 --> 01:22:05.400]   to a degree where you could see in the near term
[01:22:05.400 --> 01:22:07.800]   the ability to make a lot of money
[01:22:07.800 --> 01:22:10.440]   and to put guardrails, to develop guardrails quickly
[01:22:10.440 --> 01:22:12.960]   in that kind of context seems to be tricky.
[01:22:12.960 --> 01:22:16.640]   It's not similar to cars or child labor.
[01:22:16.640 --> 01:22:19.960]   It seems like the opportunity to make a lot of money here
[01:22:19.960 --> 01:22:22.840]   very quickly is right here before us.
[01:22:22.840 --> 01:22:24.920]   - So again, there's this cliff.
[01:22:24.920 --> 01:22:27.200]   - Yeah, it gets quite scenic.
[01:22:27.200 --> 01:22:29.000]   The closer to the cliff you go,
[01:22:29.000 --> 01:22:32.720]   the more money there is, the more gold ingots
[01:22:32.720 --> 01:22:34.280]   there are on the ground you can pick up or whatever
[01:22:34.280 --> 01:22:36.080]   if you want to drive there very fast.
[01:22:36.080 --> 01:22:38.720]   But it's not in anyone's incentive that we go over the cliff
[01:22:38.720 --> 01:22:40.920]   and it's not like everybody's in their own car.
[01:22:40.920 --> 01:22:43.680]   All the cars are connected together with a chain.
[01:22:43.680 --> 01:22:48.160]   So if anyone goes over, they'll start dragging the others down too.
[01:22:48.160 --> 01:22:52.560]   And so ultimately, it's in the selfish interests
[01:22:52.560 --> 01:22:56.200]   also of the people in the companies to slow down
[01:22:56.200 --> 01:22:59.280]   when you start seeing the contours of the cliff
[01:22:59.280 --> 01:23:00.600]   there in front of you.
[01:23:00.600 --> 01:23:03.080]   The problem is that even though the people
[01:23:03.080 --> 01:23:07.400]   who are building the technology and the CEOs,
[01:23:07.400 --> 01:23:10.080]   they really get it, the shareholders
[01:23:10.080 --> 01:23:12.400]   and these other market forces,
[01:23:12.400 --> 01:23:16.040]   they are people who don't honestly understand
[01:23:16.040 --> 01:23:16.880]   that the cliff is there.
[01:23:16.880 --> 01:23:17.960]   They usually don't.
[01:23:17.960 --> 01:23:19.600]   You have to get quite into the weeds
[01:23:19.600 --> 01:23:22.560]   to really appreciate how powerful this is and how fast.
[01:23:22.560 --> 01:23:24.160]   And a lot of people are even still stuck again
[01:23:24.160 --> 01:23:29.160]   in this idea that in this carbon chauvinism,
[01:23:29.160 --> 01:23:31.800]   as I like to call it, that you can only have
[01:23:31.800 --> 01:23:34.800]   our level of intelligence in humans,
[01:23:34.800 --> 01:23:36.000]   that there's something magical about it.
[01:23:36.000 --> 01:23:38.200]   Whereas the people in the tech companies
[01:23:38.200 --> 01:23:41.440]   who build this stuff, they all realize
[01:23:41.440 --> 01:23:45.720]   that intelligence is information processing of a certain kind.
[01:23:45.720 --> 01:23:48.000]   And it really doesn't matter at all
[01:23:48.000 --> 01:23:50.200]   whether the information is processed by carbon atoms
[01:23:50.280 --> 01:23:55.000]   in neurons in brains or by silicon atoms
[01:23:55.000 --> 01:23:56.840]   in some technology we build.
[01:23:56.840 --> 01:24:00.720]   So you brought up capitalism earlier,
[01:24:00.720 --> 01:24:02.560]   and there are a lot of people who love capitalism
[01:24:02.560 --> 01:24:07.560]   and a lot of people who really, really don't.
[01:24:07.560 --> 01:24:12.960]   And it struck me recently that what's happening
[01:24:12.960 --> 01:24:16.360]   with capitalism here is exactly analogous
[01:24:16.360 --> 01:24:19.120]   to the way in which superintelligence might wipe us out.
[01:24:20.120 --> 01:24:25.120]   So, you know, I studied economics for my undergrad,
[01:24:25.120 --> 01:24:28.320]   Stockholm School of Economics, yay.
[01:24:28.320 --> 01:24:29.760]   (laughing)
[01:24:29.760 --> 01:24:31.000]   - Well, no, I tell me.
[01:24:31.000 --> 01:24:34.080]   - So I was very interested in how you could use
[01:24:34.080 --> 01:24:37.040]   market forces to just get stuff done more efficiently,
[01:24:37.040 --> 01:24:38.880]   but give the right incentives to the market
[01:24:38.880 --> 01:24:41.520]   so that it wouldn't do really bad things.
[01:24:41.520 --> 01:24:44.760]   So Dylan Hadfield-Manel, who's a professor
[01:24:44.760 --> 01:24:46.480]   and colleague of mine at MIT,
[01:24:47.360 --> 01:24:49.400]   wrote this really interesting paper
[01:24:49.400 --> 01:24:51.520]   with some collaborators recently,
[01:24:51.520 --> 01:24:54.480]   where they proved mathematically that if you just take
[01:24:54.480 --> 01:24:57.880]   one goal that you just optimize for,
[01:24:57.880 --> 01:24:59.720]   on and on and on indefinitely,
[01:24:59.720 --> 01:25:03.400]   that you think is gonna bring you in the right direction.
[01:25:03.400 --> 01:25:05.440]   What basically always happens is,
[01:25:05.440 --> 01:25:08.680]   in the beginning, it will make things better for you.
[01:25:08.680 --> 01:25:11.320]   But if you keep going, at some point,
[01:25:11.320 --> 01:25:13.720]   it's gonna start making things worse for you again.
[01:25:13.720 --> 01:25:15.000]   And then gradually it's gonna make it
[01:25:15.000 --> 01:25:16.400]   really, really terrible.
[01:25:16.400 --> 01:25:20.520]   So just as a simple, the way I think of the proof is,
[01:25:20.520 --> 01:25:25.520]   suppose you wanna go from here back to Austin, for example,
[01:25:25.520 --> 01:25:29.400]   and you're like, okay, yeah, let's just, let's go south,
[01:25:29.400 --> 01:25:30.760]   but you put in exactly the right,
[01:25:30.760 --> 01:25:32.120]   sort of the right direction.
[01:25:32.120 --> 01:25:34.120]   Just optimize that, south is possible.
[01:25:34.120 --> 01:25:35.920]   You get closer and closer to Austin,
[01:25:35.920 --> 01:25:41.360]   but there's always some little error.
[01:25:41.360 --> 01:25:44.240]   So you're not going exactly towards Austin,
[01:25:44.240 --> 01:25:45.440]   but you get pretty close.
[01:25:45.440 --> 01:25:47.240]   But eventually you start going away again,
[01:25:47.240 --> 01:25:50.160]   and eventually you're gonna be leaving the solar system.
[01:25:50.160 --> 01:25:51.000]   - Yeah.
[01:25:51.000 --> 01:25:53.440]   - And they proved, it's a beautiful mathematical proof.
[01:25:53.440 --> 01:25:57.800]   This happens generally, and this is very important for AI,
[01:25:57.800 --> 01:26:02.240]   because even though Stuart Russell has written a book
[01:26:02.240 --> 01:26:06.000]   and given a lot of talks on why it's a bad idea
[01:26:06.000 --> 01:26:08.440]   to have AI just blindly optimize something,
[01:26:08.440 --> 01:26:10.720]   that's what pretty much all our systems do.
[01:26:10.720 --> 01:26:12.280]   We have something called the loss function
[01:26:12.280 --> 01:26:14.160]   that we're just minimizing, or reward function,
[01:26:14.160 --> 01:26:15.680]   we're just maximizing.
[01:26:15.680 --> 01:26:21.920]   And capitalism is exactly like that too.
[01:26:21.920 --> 01:26:26.240]   We wanted to get stuff done more efficiently
[01:26:26.240 --> 01:26:27.560]   than people wanted.
[01:26:27.560 --> 01:26:30.440]   So we introduced the free market.
[01:26:30.440 --> 01:26:34.360]   Things got done much more efficiently than they did
[01:26:34.360 --> 01:26:38.760]   in say, communism, right?
[01:26:38.760 --> 01:26:39.760]   And it got better.
[01:26:39.760 --> 01:26:43.320]   But then it just kept optimizing.
[01:26:43.840 --> 01:26:45.320]   And kept optimizing.
[01:26:45.320 --> 01:26:46.480]   And you got ever bigger companies
[01:26:46.480 --> 01:26:48.640]   and ever more efficient information processing,
[01:26:48.640 --> 01:26:51.080]   and now also very much powered by IT.
[01:26:51.080 --> 01:26:55.360]   And eventually a lot of people are beginning to feel,
[01:26:55.360 --> 01:26:57.320]   wait, we're kind of optimizing a bit too much.
[01:26:57.320 --> 01:26:59.920]   Like, why did we just chop down half the rainforest?
[01:26:59.920 --> 01:27:03.480]   And why did suddenly these regulators
[01:27:03.480 --> 01:27:07.360]   get captured by lobbyists and so on?
[01:27:07.360 --> 01:27:08.600]   It's just the same optimization
[01:27:08.600 --> 01:27:11.040]   that's been running for too long.
[01:27:11.040 --> 01:27:15.000]   If you have an AI that actually has power over the world
[01:27:15.000 --> 01:27:16.200]   and you just give it one goal
[01:27:16.200 --> 01:27:18.240]   and just keep optimizing that,
[01:27:18.240 --> 01:27:20.040]   most likely everybody's gonna be like,
[01:27:20.040 --> 01:27:21.320]   yay, this is great in the beginning,
[01:27:21.320 --> 01:27:23.480]   things are getting better.
[01:27:23.480 --> 01:27:27.760]   But it's almost impossible to give it exactly
[01:27:27.760 --> 01:27:29.920]   the right direction to optimize in.
[01:27:29.920 --> 01:27:34.680]   And then eventually all hay breaks loose, right?
[01:27:34.680 --> 01:27:37.440]   Nick Bostrom and others have given examples
[01:27:37.440 --> 01:27:38.440]   that sound quite silly.
[01:27:38.840 --> 01:27:43.800]   What if you just wanna tell it to cure cancer or something,
[01:27:43.800 --> 01:27:45.120]   and that's all you tell it?
[01:27:45.120 --> 01:27:50.120]   Maybe it's gonna decide to take over entire continents
[01:27:50.120 --> 01:27:53.600]   just so it can get more supercomputer facilities in there
[01:27:53.600 --> 01:27:55.960]   and figure out how to cure cancer backwards.
[01:27:55.960 --> 01:27:58.440]   And then you're like, wait, that's not what I wanted, right?
[01:27:58.440 --> 01:28:02.960]   And the issue with capitalism
[01:28:02.960 --> 01:28:04.200]   and the issue with the front-end AI
[01:28:04.200 --> 01:28:05.480]   have kind of merged now
[01:28:05.600 --> 01:28:08.600]   because the Moloch I talked about
[01:28:08.600 --> 01:28:10.400]   is exactly the capitalist Moloch
[01:28:10.400 --> 01:28:12.360]   that we have built an economy
[01:28:12.360 --> 01:28:16.680]   that is optimizing for only one thing, profit.
[01:28:16.680 --> 01:28:20.960]   And that worked great back when things were very inefficient
[01:28:20.960 --> 01:28:22.720]   and then now it's getting done better.
[01:28:22.720 --> 01:28:24.760]   And it worked great as long as the companies
[01:28:24.760 --> 01:28:28.080]   were small enough that they couldn't capture the regulators.
[01:28:28.080 --> 01:28:32.520]   But that's not true anymore, but they keep optimizing.
[01:28:32.520 --> 01:28:37.000]   And now they realize that these companies
[01:28:37.000 --> 01:28:39.520]   can make even more profit by building ever more powerful AI
[01:28:39.520 --> 01:28:40.680]   even if it's reckless,
[01:28:40.680 --> 01:28:46.040]   but optimize more and more and more and more and more.
[01:28:46.040 --> 01:28:50.280]   So this is Moloch again, showing up.
[01:28:50.280 --> 01:28:54.200]   And I just wanna, anyone here who has any concerns
[01:28:54.200 --> 01:28:59.200]   about late-stage capitalism having gone a little too far,
[01:28:59.200 --> 01:29:02.400]   you should worry about superintelligence
[01:29:02.400 --> 01:29:06.120]   'cause it's the same villain in both cases, it's Moloch.
[01:29:06.120 --> 01:29:10.040]   - And optimizing one objective function
[01:29:10.040 --> 01:29:13.560]   aggressively, blindly is going to take us there.
[01:29:13.560 --> 01:29:16.080]   - Yeah, we have this pause from time to time
[01:29:16.080 --> 01:29:20.560]   and look into our hearts and ask, why are we doing this?
[01:29:20.560 --> 01:29:23.360]   Is this, am I still going towards Austin
[01:29:23.360 --> 01:29:25.320]   or have I gone too far?
[01:29:25.320 --> 01:29:27.400]   Maybe we should change direction.
[01:29:27.400 --> 01:29:30.920]   - And that is the idea behind a halt for six months.
[01:29:30.920 --> 01:29:32.300]   Why six months? - Yeah.
[01:29:32.300 --> 01:29:34.200]   - That seems like a very short period.
[01:29:34.200 --> 01:29:37.680]   Can we just linger and explore different ideas here?
[01:29:37.680 --> 01:29:40.160]   Because this feels like a really important moment
[01:29:40.160 --> 01:29:42.960]   in human history where pausing would actually
[01:29:42.960 --> 01:29:46.160]   have a significant positive effect.
[01:29:46.160 --> 01:29:50.480]   - We said six months because we figured
[01:29:50.480 --> 01:29:54.360]   the number one pushback we were gonna get in the West
[01:29:54.360 --> 01:29:55.820]   was like, but China.
[01:29:57.960 --> 01:30:01.040]   And everybody knows there's no way that China
[01:30:01.040 --> 01:30:03.520]   is gonna catch up with the West on this in six months.
[01:30:03.520 --> 01:30:05.800]   So that argument goes off the table
[01:30:05.800 --> 01:30:08.200]   and you can forget about geopolitical competition
[01:30:08.200 --> 01:30:11.360]   and just focus on the real issue.
[01:30:11.360 --> 01:30:12.600]   That's why we put this.
[01:30:12.600 --> 01:30:13.800]   - That's really interesting.
[01:30:13.800 --> 01:30:18.000]   But you've already made the case that even for China,
[01:30:18.000 --> 01:30:20.640]   if you actually wanna take on that argument,
[01:30:20.640 --> 01:30:25.400]   China too would not be bothered by a longer halt
[01:30:25.400 --> 01:30:26.960]   because they don't wanna lose control,
[01:30:26.960 --> 01:30:28.560]   even more than the West doesn't.
[01:30:28.560 --> 01:30:30.400]   - That's what I think.
[01:30:30.400 --> 01:30:32.280]   - That's a really interesting argument.
[01:30:32.280 --> 01:30:33.960]   I have to actually really think about that,
[01:30:33.960 --> 01:30:36.920]   which the kind of thing people assume
[01:30:36.920 --> 01:30:40.040]   is if you develop an AGI, that open AI,
[01:30:40.040 --> 01:30:42.200]   if they're the ones that do it, for example,
[01:30:42.200 --> 01:30:44.000]   they're going to win.
[01:30:44.000 --> 01:30:47.400]   But you're saying, no, everybody loses.
[01:30:47.400 --> 01:30:49.840]   - Yeah, it's gonna get better and better and better
[01:30:49.840 --> 01:30:52.080]   and then kaboom, we all lose.
[01:30:52.080 --> 01:30:53.120]   That's what's gonna happen.
[01:30:53.120 --> 01:30:55.000]   - When lose and win are defined in a metric
[01:30:55.000 --> 01:31:00.000]   of basically quality of life for human civilization
[01:31:00.000 --> 01:31:01.360]   and for Sam Altman.
[01:31:01.360 --> 01:31:05.280]   - To be blunt, my personal guess,
[01:31:05.280 --> 01:31:06.320]   and people can quibble with this,
[01:31:06.320 --> 01:31:08.680]   is that we're just gonna, there won't be any humans.
[01:31:08.680 --> 01:31:10.560]   That's it, that's what I mean by lose.
[01:31:10.560 --> 01:31:15.000]   We can see in history, once you have some species
[01:31:15.000 --> 01:31:18.180]   or some group of people who aren't needed anymore,
[01:31:18.180 --> 01:31:22.640]   doesn't usually work out so well for them, right?
[01:31:22.640 --> 01:31:23.480]   - Yeah.
[01:31:24.320 --> 01:31:26.440]   - There were a lot of horses that were used
[01:31:26.440 --> 01:31:29.080]   for traffic in Boston and then the car got invented
[01:31:29.080 --> 01:31:33.040]   and most of them got, yeah, well, we don't need to go there.
[01:31:33.040 --> 01:31:36.040]   And if you look at humans, right now,
[01:31:36.040 --> 01:31:45.360]   why did the labor movement succeed
[01:31:45.360 --> 01:31:46.720]   after the Industrial Revolution?
[01:31:46.720 --> 01:31:47.900]   Because it was needed.
[01:31:47.900 --> 01:31:52.920]   Even though we had a lot of Molochs
[01:31:52.920 --> 01:31:54.840]   and there was child labor and so on,
[01:31:54.840 --> 01:31:58.680]   the company still needed to have workers
[01:31:58.680 --> 01:32:02.640]   and that's why strikes had power and so on.
[01:32:02.640 --> 01:32:05.120]   If we get to the point where most humans
[01:32:05.120 --> 01:32:07.760]   aren't needed anymore, I think it's quite naive
[01:32:07.760 --> 01:32:10.600]   to think that they're gonna still be treated well.
[01:32:10.600 --> 01:32:13.200]   We say that, yeah, yeah, everybody's equal
[01:32:13.200 --> 01:32:15.540]   and the government will always, we'll always protect them.
[01:32:15.540 --> 01:32:17.460]   But if you look in practice,
[01:32:17.460 --> 01:32:19.480]   groups that are very disenfranchised
[01:32:19.480 --> 01:32:21.480]   and don't have any actual power,
[01:32:22.320 --> 01:32:24.200]   usually get screwed.
[01:32:24.200 --> 01:32:29.200]   And now, in the beginning, so Industrial Revolution,
[01:32:29.200 --> 01:32:30.920]   we automated away muscle work.
[01:32:30.920 --> 01:32:35.560]   But that worked out pretty well eventually
[01:32:35.560 --> 01:32:36.880]   because we educated ourselves
[01:32:36.880 --> 01:32:38.680]   and started working with our brains instead
[01:32:38.680 --> 01:32:42.520]   and got usually more interesting, better paid jobs.
[01:32:42.520 --> 01:32:44.280]   But now we're beginning to replace brain work.
[01:32:44.280 --> 01:32:46.320]   So we replaced a lot of boring stuff,
[01:32:46.320 --> 01:32:48.800]   like we got the pocket calculator
[01:32:48.800 --> 01:32:50.920]   so you don't have people adding stuff
[01:32:50.920 --> 01:32:53.720]   and adding, multiplying numbers anymore at work.
[01:32:53.720 --> 01:32:56.280]   Fine, there were better jobs they could get.
[01:32:56.280 --> 01:33:01.280]   But now, GPT-4 and the stable diffusion
[01:33:01.280 --> 01:33:02.800]   and techniques like this,
[01:33:02.800 --> 01:33:06.000]   they're really beginning to blow away
[01:33:06.000 --> 01:33:08.720]   some jobs that people really love having.
[01:33:08.720 --> 01:33:10.600]   There was a heartbreaking article just,
[01:33:10.600 --> 01:33:13.120]   post just yesterday on social media I saw
[01:33:13.120 --> 01:33:17.320]   about this guy who was doing 3D modeling for gaming
[01:33:17.320 --> 01:33:20.960]   and all of a sudden now they got this new software,
[01:33:20.960 --> 01:33:24.760]   he just says prompts and he feels his whole job
[01:33:24.760 --> 01:33:27.320]   that he loved just lost its meaning.
[01:33:27.320 --> 01:33:32.320]   And I asked GPT-4 to rewrite "Twinkle, Twinkle Little Star"
[01:33:32.320 --> 01:33:34.600]   in the style of Shakespeare.
[01:33:34.600 --> 01:33:37.720]   I couldn't have done such a good job.
[01:33:37.720 --> 01:33:39.920]   It was really impressive.
[01:33:39.920 --> 01:33:42.160]   You've seen a lot of the art coming out here.
[01:33:42.160 --> 01:33:47.160]   So I'm all for automating away the dangerous jobs
[01:33:47.160 --> 01:33:48.520]   and boring jobs.
[01:33:48.520 --> 01:33:51.840]   But I think you hear a lot,
[01:33:51.840 --> 01:33:53.200]   some arguments which are too glib.
[01:33:53.200 --> 01:33:54.040]   Sometimes people say,
[01:33:54.040 --> 01:33:55.120]   "Well, that's all that's gonna happen.
[01:33:55.120 --> 01:33:57.600]   "We're getting rid of the boring,
[01:33:57.600 --> 01:33:59.160]   "tedious, dangerous jobs."
[01:33:59.160 --> 01:34:00.160]   It's just not true.
[01:34:00.160 --> 01:34:01.880]   There are a lot of really interesting jobs
[01:34:01.880 --> 01:34:03.240]   that are being taken away now.
[01:34:03.240 --> 01:34:05.960]   Journalism is gonna get crushed.
[01:34:05.960 --> 01:34:08.760]   Coding is gonna get crushed.
[01:34:08.760 --> 01:34:12.120]   I predict the job market for programmers,
[01:34:12.120 --> 01:34:14.360]   salaries are gonna start dropping.
[01:34:15.360 --> 01:34:18.080]   You said you can code five times faster
[01:34:18.080 --> 01:34:20.160]   than you need five times fewer programmers.
[01:34:20.160 --> 01:34:22.880]   Maybe there'll be more output also,
[01:34:22.880 --> 01:34:27.080]   but you'll still end up needing fewer programmers than today.
[01:34:27.080 --> 01:34:28.320]   And I love coding.
[01:34:28.320 --> 01:34:29.960]   I think it's super cool.
[01:34:29.960 --> 01:34:35.000]   So we need to stop and ask ourselves why again
[01:34:35.000 --> 01:34:36.720]   are we doing this as humans?
[01:34:36.720 --> 01:34:43.960]   I feel that AI should be built by humanity for humanity.
[01:34:44.520 --> 01:34:45.960]   And let's not forget that.
[01:34:45.960 --> 01:34:48.800]   It shouldn't be by Moloch for Moloch.
[01:34:48.800 --> 01:34:53.240]   Or what it really is now is kind of by humanity for Moloch,
[01:34:53.240 --> 01:34:54.880]   which doesn't make any sense.
[01:34:54.880 --> 01:34:57.680]   It's for us that we're doing it.
[01:34:57.680 --> 01:35:00.280]   And it would make a lot more sense
[01:35:00.280 --> 01:35:04.000]   if we develop, figure out gradually and safely
[01:35:04.000 --> 01:35:04.920]   how to make all this tech.
[01:35:04.920 --> 01:35:06.760]   And then we think about what are the kind of jobs
[01:35:06.760 --> 01:35:08.880]   that people really don't wanna have?
[01:35:08.880 --> 01:35:10.640]   Automate them all away.
[01:35:10.640 --> 01:35:11.480]   And then we ask,
[01:35:11.480 --> 01:35:15.240]   what are the jobs that people really find meaning in?
[01:35:15.240 --> 01:35:20.240]   Like maybe taking care of children in the daycare center,
[01:35:20.240 --> 01:35:23.320]   maybe doing art, et cetera, et cetera.
[01:35:23.320 --> 01:35:26.760]   And even if it were possible to automate that way,
[01:35:26.760 --> 01:35:28.600]   we don't need to do that, right?
[01:35:28.600 --> 01:35:30.320]   We built these machines.
[01:35:30.320 --> 01:35:33.760]   - Well, it's possible that we redefine
[01:35:33.760 --> 01:35:36.680]   or rediscover what are the jobs that give us meaning.
[01:35:36.680 --> 01:35:40.200]   So for me, the thing, it is really sad.
[01:35:40.200 --> 01:35:43.920]   Like I, half the time I'm excited,
[01:35:43.920 --> 01:35:48.680]   half the time I'm crying as I'm generating code
[01:35:48.680 --> 01:35:52.640]   because I kind of love programming.
[01:35:52.640 --> 01:35:55.240]   It's an act of creation.
[01:35:55.240 --> 01:35:58.240]   You have an idea, you design it,
[01:35:58.240 --> 01:36:00.080]   and then you bring it to life and it does something,
[01:36:00.080 --> 01:36:02.400]   especially if there's some intelligence that it does something.
[01:36:02.400 --> 01:36:04.120]   It doesn't even have to have intelligence.
[01:36:04.120 --> 01:36:06.240]   Printing "Hello World" on screen,
[01:36:06.240 --> 01:36:10.240]   you made a little machine and it comes to life.
[01:36:10.240 --> 01:36:11.080]   - Yeah.
[01:36:11.080 --> 01:36:13.840]   - And there's a bunch of tricks you learn along the way
[01:36:13.840 --> 01:36:17.440]   'cause you've been doing it for many, many years.
[01:36:17.440 --> 01:36:19.920]   And then to see AI be able to generate
[01:36:19.920 --> 01:36:21.920]   all the tricks you thought were special.
[01:36:21.920 --> 01:36:22.760]   - Yeah.
[01:36:22.760 --> 01:36:29.040]   - I don't know, it's very, it's scary.
[01:36:29.040 --> 01:36:34.080]   It's almost painful, like a loss of innocence maybe.
[01:36:34.080 --> 01:36:36.520]   Like maybe when I was younger,
[01:36:36.520 --> 01:36:39.960]   I remember before I learned that sugar's bad for you,
[01:36:39.960 --> 01:36:41.720]   you should be on a diet.
[01:36:41.720 --> 01:36:44.320]   I remember I enjoyed candy deeply,
[01:36:44.320 --> 01:36:47.120]   in a way I just can't anymore,
[01:36:47.120 --> 01:36:48.760]   that I know is bad for me.
[01:36:48.760 --> 01:36:53.760]   I enjoyed it unapologetically, fully, just intensely.
[01:36:53.760 --> 01:36:55.840]   And I just, I lost that.
[01:36:55.840 --> 01:36:59.400]   Now I feel like a little bit of that is lost for me
[01:36:59.400 --> 01:37:01.520]   with programming, or being lost with programming,
[01:37:01.520 --> 01:37:06.440]   similar as it is for the 3D modeler,
[01:37:06.440 --> 01:37:10.000]   no longer being able to really enjoy the art of modeling
[01:37:10.000 --> 01:37:11.840]   3D things for gaming.
[01:37:11.840 --> 01:37:13.400]   I don't know, I don't know what to make sense of that.
[01:37:13.400 --> 01:37:15.960]   Maybe I would rediscover that the true magic
[01:37:15.960 --> 01:37:16.960]   of what it means to be human
[01:37:16.960 --> 01:37:18.160]   is connecting with other humans,
[01:37:18.160 --> 01:37:19.800]   to have conversations like this.
[01:37:19.800 --> 01:37:24.040]   I don't know, to have sex, to eat food,
[01:37:24.040 --> 01:37:28.240]   to really intensify the value from conscious experiences
[01:37:28.240 --> 01:37:30.320]   versus like creating other stuff.
[01:37:30.320 --> 01:37:32.360]   - You're pitching the rebranding again
[01:37:32.360 --> 01:37:34.000]   from Homo sapiens to Homo sentiens.
[01:37:34.000 --> 01:37:34.960]   - Homo sentiens, yeah.
[01:37:34.960 --> 01:37:36.480]   - The meaningful experiences.
[01:37:36.480 --> 01:37:38.400]   And just to inject some optimism in this here,
[01:37:38.400 --> 01:37:40.640]   so we don't sound like a bunch of gloomers,
[01:37:40.640 --> 01:37:43.080]   we can totally have our cake and eat it.
[01:37:43.080 --> 01:37:45.080]   You hear a lot of totally bullshit claims
[01:37:45.080 --> 01:37:47.800]   that we can't afford having more teachers,
[01:37:47.800 --> 01:37:49.440]   have to cut the number of nurses.
[01:37:49.440 --> 01:37:51.680]   That's just nonsense, obviously.
[01:37:51.680 --> 01:37:57.800]   With anything, even quite far short of AGI,
[01:37:57.800 --> 01:38:01.720]   we can dramatically improve, grow the GDP
[01:38:01.720 --> 01:38:05.600]   and produce a wealth of goods and services.
[01:38:05.600 --> 01:38:07.160]   It's very easy to create a world
[01:38:07.160 --> 01:38:09.160]   where everybody's better off than today,
[01:38:09.160 --> 01:38:13.560]   including the richest people can be better off as well.
[01:38:13.560 --> 01:38:17.000]   It's not a zero-sum game in technology.
[01:38:17.000 --> 01:38:20.440]   Again, you can have two countries like Sweden and Denmark
[01:38:20.440 --> 01:38:23.480]   have all these ridiculous wars century after century.
[01:38:25.360 --> 01:38:28.800]   And sometimes Sweden got a little better off
[01:38:28.800 --> 01:38:29.720]   'cause it got a little bit bigger.
[01:38:29.720 --> 01:38:31.280]   And then Denmark got a little bit better off
[01:38:31.280 --> 01:38:33.320]   'cause Sweden got a little bit smaller.
[01:38:33.320 --> 01:38:35.000]   But then technology came along
[01:38:35.000 --> 01:38:37.240]   and we both got just dramatically wealthier
[01:38:37.240 --> 01:38:38.640]   without taking it away from anyone else.
[01:38:38.640 --> 01:38:40.960]   It was just a total win for everyone.
[01:38:40.960 --> 01:38:44.480]   And AI can do that on steroids.
[01:38:44.480 --> 01:38:47.960]   If you can build safe AGI,
[01:38:47.960 --> 01:38:49.800]   if you can build super intelligence,
[01:38:49.800 --> 01:38:55.000]   basically all the limitations that cause harm today
[01:38:55.000 --> 01:38:57.840]   can be completely eliminated.
[01:38:57.840 --> 01:39:00.720]   It's a wonderful possibility.
[01:39:00.720 --> 01:39:01.920]   And this is not sci-fi.
[01:39:01.920 --> 01:39:03.760]   This is something which is clearly possible
[01:39:03.760 --> 01:39:05.640]   according to the laws of physics.
[01:39:05.640 --> 01:39:09.440]   And we can talk about ways of making it safe also.
[01:39:09.440 --> 01:39:13.480]   But unfortunately, that'll only happen
[01:39:13.480 --> 01:39:14.720]   if we steer in that direction.
[01:39:14.720 --> 01:39:17.120]   That's absolutely not the default outcome.
[01:39:17.120 --> 01:39:22.000]   That's why income inequality keeps going up.
[01:39:22.000 --> 01:39:23.960]   That's why the life expectancy in the US
[01:39:23.960 --> 01:39:25.000]   has been going down now.
[01:39:25.000 --> 01:39:27.240]   I think it's four years in a row.
[01:39:27.240 --> 01:39:30.760]   I just read a heartbreaking study from the CDC
[01:39:30.760 --> 01:39:33.480]   about how something like one third
[01:39:33.480 --> 01:39:36.400]   of all teenage girls in the US
[01:39:36.400 --> 01:39:37.840]   been thinking about suicide.
[01:39:37.840 --> 01:39:42.600]   Those are steps in totally the wrong direction.
[01:39:42.600 --> 01:39:45.880]   And it's important to keep our eyes on the prize here
[01:39:45.880 --> 01:39:50.880]   that we have the power now for the first time
[01:39:50.880 --> 01:39:53.240]   in the history of our species.
[01:39:53.960 --> 01:39:55.680]   To harness artificial intelligence,
[01:39:55.680 --> 01:39:58.120]   to help us really flourish
[01:39:58.120 --> 01:40:03.360]   and help bring out the best in our humanity
[01:40:03.360 --> 01:40:05.840]   rather than the worst of it.
[01:40:05.840 --> 01:40:10.240]   To help us have really fulfilling experiences
[01:40:10.240 --> 01:40:11.480]   that feel truly meaningful.
[01:40:11.480 --> 01:40:13.680]   And you and I shouldn't sit here
[01:40:13.680 --> 01:40:15.520]   and dictate to future generations what they will be.
[01:40:15.520 --> 01:40:16.360]   Let them figure it out,
[01:40:16.360 --> 01:40:18.680]   but let's give them a chance to live
[01:40:18.680 --> 01:40:21.320]   and not foreclose all these possibilities for them
[01:40:21.320 --> 01:40:23.040]   by just messing things up, right?
[01:40:23.040 --> 01:40:25.800]   - And for that, we'll have to solve the AI safety problem.
[01:40:25.800 --> 01:40:27.080]   It would be nice if we can linger
[01:40:27.080 --> 01:40:29.520]   on exploring that a little bit.
[01:40:29.520 --> 01:40:33.760]   So one interesting way to enter that discussion
[01:40:33.760 --> 01:40:37.920]   is you tweeted and Elon replied.
[01:40:37.920 --> 01:40:40.400]   You tweeted, "Let's not just focus on whether GPT-4
[01:40:40.400 --> 01:40:42.580]   "will do more harm or good on the job market,
[01:40:42.580 --> 01:40:44.580]   "but also whether its coding skills
[01:40:44.580 --> 01:40:47.480]   "will hasten the arrival of superintelligence."
[01:40:47.480 --> 01:40:49.440]   That's something we've been talking about, right?
[01:40:49.440 --> 01:40:51.560]   So Elon proposed one thing in the reply,
[01:40:51.560 --> 01:40:53.320]   saying, "Maximum truth-seeking
[01:40:53.320 --> 01:40:55.960]   "is my best guess for AI safety."
[01:40:55.960 --> 01:40:59.400]   Can you maybe steelman the case
[01:40:59.400 --> 01:41:04.400]   for this objective function of truth
[01:41:04.400 --> 01:41:06.760]   and maybe make an argument against it?
[01:41:06.760 --> 01:41:09.960]   And in general, what are your different ideas
[01:41:09.960 --> 01:41:12.720]   to start approaching the solution to AI safety?
[01:41:12.720 --> 01:41:14.400]   - I didn't see that reply, actually.
[01:41:14.400 --> 01:41:16.120]   - Oh, interesting.
[01:41:16.120 --> 01:41:18.240]   - But I really resonate with it because
[01:41:19.240 --> 01:41:20.600]   AI is not evil.
[01:41:20.600 --> 01:41:23.000]   It caused people around the world
[01:41:23.000 --> 01:41:24.520]   to hate each other much more,
[01:41:24.520 --> 01:41:27.960]   but that's because we made it in a certain way.
[01:41:27.960 --> 01:41:28.800]   It's a tool.
[01:41:28.800 --> 01:41:30.480]   We can use it for great things and bad things.
[01:41:30.480 --> 01:41:33.240]   And we could just as well have AI systems.
[01:41:33.240 --> 01:41:36.840]   And this is part of my vision for success here,
[01:41:36.840 --> 01:41:41.840]   truth-seeking AI that really brings us together again.
[01:41:41.840 --> 01:41:43.800]   Why do people hate each other so much
[01:41:43.800 --> 01:41:46.080]   between countries and within countries?
[01:41:46.080 --> 01:41:49.640]   It's because they each have totally different versions
[01:41:49.640 --> 01:41:50.840]   of the truth, right?
[01:41:50.840 --> 01:41:54.560]   If they all had the same truth
[01:41:54.560 --> 01:41:56.240]   that they trusted for good reason,
[01:41:56.240 --> 01:41:57.800]   'cause they could check it and verify it
[01:41:57.800 --> 01:41:58.640]   and not have to believe
[01:41:58.640 --> 01:42:00.840]   in some self-proclaimed authority, right?
[01:42:00.840 --> 01:42:03.960]   There wouldn't be nearly as much hate.
[01:42:03.960 --> 01:42:06.040]   There'd be a lot more understanding instead.
[01:42:06.040 --> 01:42:09.160]   And this is, I think,
[01:42:09.160 --> 01:42:11.200]   something AI can help enormously with.
[01:42:11.200 --> 01:42:14.840]   For example, if you're a journalist,
[01:42:14.960 --> 01:42:18.640]   for example, a little baby step in this direction
[01:42:18.640 --> 01:42:21.040]   is this website called Metaculous,
[01:42:21.040 --> 01:42:25.320]   where people bet and make predictions,
[01:42:25.320 --> 01:42:29.000]   not for money, but just for their own reputation.
[01:42:29.000 --> 01:42:30.480]   And it's kind of funny, actually.
[01:42:30.480 --> 01:42:32.400]   You treat the humans like you treat AI,
[01:42:32.400 --> 01:42:35.560]   as you have a loss function where they get penalized
[01:42:35.560 --> 01:42:37.880]   if they're super confident on something
[01:42:37.880 --> 01:42:39.440]   and then the opposite happens.
[01:42:39.440 --> 01:42:43.120]   Whereas if you're kind of humble and then you're like,
[01:42:43.120 --> 01:42:45.360]   I think it's 51% chance this is gonna happen,
[01:42:45.360 --> 01:42:48.600]   and then the other happens, you don't get penalized much.
[01:42:48.600 --> 01:42:50.000]   And what you can see is that some people
[01:42:50.000 --> 01:42:52.400]   are much better at predicting than others.
[01:42:52.400 --> 01:42:54.040]   They've earned your trust, right?
[01:42:54.040 --> 01:42:57.680]   One project that I'm working on right now
[01:42:57.680 --> 01:42:59.320]   is an outgrowth of Improve the News Foundation
[01:42:59.320 --> 01:43:00.520]   together with the Metaculous folks
[01:43:00.520 --> 01:43:03.040]   is seeing if we can really scale this up a lot
[01:43:03.040 --> 01:43:04.560]   with more powerful AI.
[01:43:04.560 --> 01:43:06.320]   'Cause I would love it.
[01:43:06.320 --> 01:43:07.400]   I would love for there to be
[01:43:07.400 --> 01:43:09.600]   a really powerful truth-seeking system
[01:43:10.560 --> 01:43:14.320]   where that is trustworthy
[01:43:14.320 --> 01:43:17.120]   because it keeps being right about stuff.
[01:43:17.120 --> 01:43:19.240]   And people who come to it
[01:43:19.240 --> 01:43:24.160]   and maybe look at its latest trust ranking
[01:43:24.160 --> 01:43:27.480]   of different pundits and newspapers, et cetera,
[01:43:27.480 --> 01:43:29.840]   if they wanna know why someone got a low score,
[01:43:29.840 --> 01:43:32.440]   they can click on it and see all the predictions
[01:43:32.440 --> 01:43:35.040]   that they actually made and how they turned out.
[01:43:35.040 --> 01:43:38.160]   This is how we do it in science.
[01:43:38.160 --> 01:43:40.560]   You trust scientists like Einstein who said something
[01:43:40.560 --> 01:43:44.200]   everybody thought was bullshit and turned out to be right.
[01:43:44.200 --> 01:43:45.560]   You get a lot of trust points
[01:43:45.560 --> 01:43:47.440]   and he did it multiple times even.
[01:43:47.440 --> 01:43:53.800]   I think AI has the power to really heal
[01:43:53.800 --> 01:43:58.400]   a lot of the rifts we're seeing by creating trust systems.
[01:43:58.400 --> 01:44:02.520]   It has to get away from this idea today
[01:44:02.520 --> 01:44:03.880]   with some fact-checking science
[01:44:03.880 --> 01:44:05.760]   which might themselves have an agenda
[01:44:05.760 --> 01:44:08.160]   and you just trust it because of its reputation.
[01:44:08.160 --> 01:44:13.080]   You wanna have it so these sort of systems,
[01:44:13.080 --> 01:44:16.320]   they earn their trust and they're completely transparent.
[01:44:16.320 --> 01:44:18.480]   This I think would actually help a lot.
[01:44:18.480 --> 01:44:21.400]   That can, I think, help heal
[01:44:21.400 --> 01:44:24.920]   the very dysfunctional conversation that humanity has
[01:44:24.920 --> 01:44:28.880]   about how it's gonna deal with all its biggest challenges
[01:44:28.880 --> 01:44:30.160]   in the world today.
[01:44:31.520 --> 01:44:35.920]   - And then on the technical side,
[01:44:35.920 --> 01:44:39.400]   another common sort of gloom comment I get
[01:44:39.400 --> 01:44:40.920]   from people who are saying, "We're just screwed.
[01:44:40.920 --> 01:44:44.120]   "There's no hope," is, well, things like GPT-4
[01:44:44.120 --> 01:44:47.160]   are way too complicated for a human to ever understand
[01:44:47.160 --> 01:44:49.240]   and prove that they can be trustworthy.
[01:44:49.240 --> 01:44:51.600]   They're forgetting that AI can help us
[01:44:51.600 --> 01:44:53.480]   prove that things work.
[01:44:53.480 --> 01:44:58.240]   There's this very fundamental fact that in math,
[01:44:58.240 --> 01:45:01.760]   it's much harder to come up with a proof
[01:45:01.760 --> 01:45:04.920]   than it is to verify that the proof is correct.
[01:45:04.920 --> 01:45:07.040]   You can actually write a little proof-checking code
[01:45:07.040 --> 01:45:10.640]   which is quite short, but you can, as a human, understand it.
[01:45:10.640 --> 01:45:12.960]   And then it can check the most monstrously long proof
[01:45:12.960 --> 01:45:14.920]   ever generated even by a computer and say,
[01:45:14.920 --> 01:45:16.280]   "Yeah, this is valid."
[01:45:16.280 --> 01:45:22.560]   So right now, we have this approach
[01:45:26.880 --> 01:45:29.680]   with virus-checking software that it looks to see
[01:45:29.680 --> 01:45:31.800]   if there's something, if you should not trust it.
[01:45:31.800 --> 01:45:33.160]   And if it can prove to itself
[01:45:33.160 --> 01:45:35.600]   that you should not trust that code, it warns you.
[01:45:35.600 --> 01:45:40.000]   What if you flip this around?
[01:45:40.000 --> 01:45:44.240]   And this is an idea I give credit to Steve on Mahindra for.
[01:45:44.240 --> 01:45:47.360]   So that it will only run the code if it can prove,
[01:45:47.360 --> 01:45:49.000]   instead of not running it, if it can prove
[01:45:49.000 --> 01:45:51.520]   that it's not trustworthy, if it will only run it
[01:45:51.520 --> 01:45:52.920]   if it can prove that it's trustworthy.
[01:45:52.920 --> 01:45:55.120]   So it asks the code, "Prove to me that you're gonna do
[01:45:55.120 --> 01:45:56.640]   "what you say you're gonna do."
[01:45:57.400 --> 01:46:00.640]   And it gives you this proof.
[01:46:00.640 --> 01:46:03.880]   And you, a little proof-taker, can check it.
[01:46:03.880 --> 01:46:06.480]   Now you can actually trust an AI
[01:46:06.480 --> 01:46:08.880]   that's much more intelligent than you are, right?
[01:46:08.880 --> 01:46:13.440]   Because it's a problem to come up with this proof
[01:46:13.440 --> 01:46:16.160]   that you could never have found, but you should trust it.
[01:46:16.160 --> 01:46:17.760]   - So this is the interesting point.
[01:46:17.760 --> 01:46:21.760]   I agree with you, but this is where Eliezer Yakovsky
[01:46:21.760 --> 01:46:23.200]   might disagree with you.
[01:46:23.200 --> 01:46:25.760]   His claim, not with you, but with this idea.
[01:46:26.760 --> 01:46:30.680]   His claim is a super-intelligent AI
[01:46:30.680 --> 01:46:34.800]   would be able to know how to lie to you with such a proof.
[01:46:34.800 --> 01:46:37.840]   - How to lie to you and give me a proof
[01:46:37.840 --> 01:46:39.640]   that I'm gonna think is correct?
[01:46:39.640 --> 01:46:40.480]   - Yeah.
[01:46:40.480 --> 01:46:41.920]   - But it's not me that's lying to you.
[01:46:41.920 --> 01:46:44.240]   That's the trick, my proof-checker.
[01:46:44.240 --> 01:46:45.160]   It's just a piece of code.
[01:46:45.160 --> 01:46:50.120]   - So his general idea is a super-intelligent system
[01:46:50.120 --> 01:46:53.040]   can lie to a dumber proof-checker.
[01:46:54.000 --> 01:46:56.600]   So you're going to have, as a system
[01:46:56.600 --> 01:46:57.760]   becomes more and more intelligent,
[01:46:57.760 --> 01:46:59.840]   there's going to be a threshold
[01:46:59.840 --> 01:47:01.560]   where a super-intelligent system
[01:47:01.560 --> 01:47:03.120]   would be able to effectively lie
[01:47:03.120 --> 01:47:05.400]   to a slightly dumber AGI system.
[01:47:05.400 --> 01:47:11.680]   He really focuses on this weak AGI to strong AGI jump
[01:47:11.680 --> 01:47:15.760]   where the strong AGI can make all the weak AGIs
[01:47:15.760 --> 01:47:19.960]   think that it's just one of them, but it's no longer that.
[01:47:19.960 --> 01:47:23.760]   And that leap is when it runs away from you.
[01:47:23.760 --> 01:47:25.720]   - I don't buy that argument.
[01:47:25.720 --> 01:47:29.320]   I think no matter how super-intelligent an AI is,
[01:47:29.320 --> 01:47:30.880]   it's never gonna be able to prove to me
[01:47:30.880 --> 01:47:33.560]   that there are only finitely many primes, for example.
[01:47:33.560 --> 01:47:36.800]   It just can't.
[01:47:36.800 --> 01:47:40.000]   And it can try to snow me by making up
[01:47:40.000 --> 01:47:42.840]   all sorts of new weird rules of deduction
[01:47:42.840 --> 01:47:47.840]   that say, trust me, the way your proof-checker works
[01:47:47.840 --> 01:47:51.760]   is too limited, and we have this new hyper-math,
[01:47:51.760 --> 01:47:53.040]   and it's true.
[01:47:53.040 --> 01:47:55.560]   But then I would just take the attitude,
[01:47:55.560 --> 01:47:58.080]   okay, I'm gonna forfeit some of these
[01:47:58.080 --> 01:48:00.000]   supposedly super-cool technologies.
[01:48:00.000 --> 01:48:01.840]   I'm only gonna go with the ones that I can prove
[01:48:01.840 --> 01:48:03.880]   with my own trusted proof-checker.
[01:48:03.880 --> 01:48:05.320]   Then I think it's fine.
[01:48:05.320 --> 01:48:08.520]   There's still, of course, this is not something
[01:48:08.520 --> 01:48:10.360]   anyone has successfully implemented at this point,
[01:48:10.360 --> 01:48:14.680]   but I think I just give it as an example of hope.
[01:48:14.680 --> 01:48:17.160]   We don't have to do all the work ourselves.
[01:48:17.160 --> 01:48:19.880]   This is exactly the sort of very boring and tedious task
[01:48:19.880 --> 01:48:22.720]   that's perfect to outsource to an AI.
[01:48:22.720 --> 01:48:24.680]   And this is a way in which less powerful
[01:48:24.680 --> 01:48:26.960]   and less intelligent agents like us
[01:48:26.960 --> 01:48:29.720]   can actually continue to control
[01:48:29.720 --> 01:48:31.760]   and trust more powerful ones.
[01:48:31.760 --> 01:48:33.800]   - So build AGI systems that help us defend
[01:48:33.800 --> 01:48:35.840]   against other AGI systems.
[01:48:35.840 --> 01:48:39.160]   - Well, for starters, begin with a simple problem
[01:48:39.160 --> 01:48:41.120]   of just making sure that the system that you own
[01:48:41.120 --> 01:48:44.320]   or that's supposed to be loyal to you
[01:48:44.320 --> 01:48:46.440]   has to prove to itself that it's always gonna do
[01:48:46.440 --> 01:48:48.240]   the things that you actually want it to do.
[01:48:48.240 --> 01:48:51.040]   And if it can't prove it, maybe it's still gonna do it,
[01:48:51.040 --> 01:48:52.480]   but you won't run it.
[01:48:52.480 --> 01:48:54.400]   So you just forfeit some aspects
[01:48:54.400 --> 01:48:56.520]   of all the cool things AI can do.
[01:48:56.520 --> 01:48:58.480]   I bet you dollars and donuts it can still do
[01:48:58.480 --> 01:49:00.520]   some incredibly cool stuff for you.
[01:49:00.520 --> 01:49:01.440]   - Yeah.
[01:49:01.440 --> 01:49:02.600]   - There are other things too
[01:49:02.600 --> 01:49:03.880]   that we shouldn't sweep under the rug,
[01:49:03.880 --> 01:49:06.880]   like not every human agrees on exactly
[01:49:06.880 --> 01:49:09.440]   what direction we should go with humanity, right?
[01:49:09.440 --> 01:49:10.840]   - Yes.
[01:49:10.840 --> 01:49:13.760]   - And you've talked a lot about geopolitical things
[01:49:13.760 --> 01:49:16.280]   on your podcast to this effect,
[01:49:16.280 --> 01:49:19.120]   but I think that shouldn't distract us
[01:49:19.120 --> 01:49:21.680]   from the fact that there are actually a lot of things
[01:49:21.680 --> 01:49:25.920]   that everybody in the world virtually agrees on
[01:49:25.920 --> 01:49:29.000]   that, hey, you know, like having no humans on the planet
[01:49:29.000 --> 01:49:35.120]   in a near future, let's not do that, right?
[01:49:35.120 --> 01:49:36.000]   You look at something like
[01:49:36.000 --> 01:49:39.360]   the United Nations Sustainable Development Goals,
[01:49:39.360 --> 01:49:42.280]   some of them are quite ambitious,
[01:49:42.280 --> 01:49:44.960]   and basically all the countries agree,
[01:49:44.960 --> 01:49:47.960]   US, China, Russia, Ukraine, they all agree.
[01:49:47.960 --> 01:49:50.960]   So instead of quibbling about the little things
[01:49:50.960 --> 01:49:53.120]   we don't agree on, let's start with the things
[01:49:53.120 --> 01:49:56.720]   we do agree on and get them done.
[01:49:56.720 --> 01:49:59.200]   Instead of being so distracted by all these things
[01:49:59.200 --> 01:50:02.840]   we disagree on that Moloch wins,
[01:50:02.840 --> 01:50:07.840]   because frankly, Moloch going wild now,
[01:50:07.840 --> 01:50:12.000]   it feels like a war on life playing out in front of our eyes.
[01:50:12.000 --> 01:50:15.960]   If you just look at it from space, you know,
[01:50:15.960 --> 01:50:20.720]   we're on this planet, beautiful, vibrant ecosystem.
[01:50:20.720 --> 01:50:24.680]   Now we start chopping down big parts of it,
[01:50:24.680 --> 01:50:27.760]   even though most people thought that was a bad idea.
[01:50:27.760 --> 01:50:30.480]   Always start doing ocean acidification,
[01:50:30.480 --> 01:50:33.000]   wiping out all sorts of species.
[01:50:33.000 --> 01:50:34.800]   Oh, now we have all these close calls,
[01:50:34.800 --> 01:50:36.720]   we almost had a nuclear war.
[01:50:36.720 --> 01:50:39.880]   And we're replacing more and more of the biosphere
[01:50:39.880 --> 01:50:42.880]   with non-living things.
[01:50:42.880 --> 01:50:45.600]   We're also replacing in our social lives
[01:50:45.600 --> 01:50:49.120]   a lot of the things which were so valuable to humanity.
[01:50:49.120 --> 01:50:51.360]   A lot of social interactions now are replaced
[01:50:51.360 --> 01:50:54.320]   by people staring into their rectangles, right?
[01:50:54.320 --> 01:50:58.640]   And I'm not a psychologist, I'm out of my depth here,
[01:50:58.640 --> 01:51:02.640]   but I suspect that part of the reason why teen suicide
[01:51:02.640 --> 01:51:04.760]   and suicide in general in the US,
[01:51:04.760 --> 01:51:08.080]   the record breaking levels is actually caused by,
[01:51:08.080 --> 01:51:11.600]   again, AI technologies and social media,
[01:51:11.600 --> 01:51:13.080]   making people spend less time
[01:51:13.080 --> 01:51:16.280]   and actually just human interaction.
[01:51:16.320 --> 01:51:19.840]   We've all seen a bunch of good looking people
[01:51:19.840 --> 01:51:22.240]   in restaurants staring into the rectangles
[01:51:22.240 --> 01:51:24.680]   instead of looking into each other's eyes, right?
[01:51:24.680 --> 01:51:28.160]   So that's also a part of the war on life,
[01:51:28.160 --> 01:51:31.760]   that we're replacing so many
[01:51:31.760 --> 01:51:38.200]   really life affirming things by technology.
[01:51:38.200 --> 01:51:41.640]   We're putting technology between us.
[01:51:41.640 --> 01:51:43.800]   The technology that was supposed to connect us
[01:51:43.800 --> 01:51:47.000]   is actually distancing us, ourselves from each other.
[01:51:47.000 --> 01:51:50.680]   And then we're giving ever more power
[01:51:50.680 --> 01:51:52.680]   to things which are not alive.
[01:51:52.680 --> 01:51:55.600]   These large corporations are not living things, right?
[01:51:55.600 --> 01:51:57.480]   They're just maximizing profit.
[01:51:57.480 --> 01:52:01.960]   I wanna win the war on life.
[01:52:01.960 --> 01:52:06.280]   I think we humans, together with all our fellow living things
[01:52:06.280 --> 01:52:08.880]   on this planet, will be better off if we can
[01:52:08.880 --> 01:52:12.920]   remain in control over the non-living things
[01:52:12.920 --> 01:52:15.120]   and make sure that they work for us.
[01:52:15.120 --> 01:52:17.560]   I really think it can be done.
[01:52:17.560 --> 01:52:19.840]   - Can you just linger on this,
[01:52:19.840 --> 01:52:23.160]   maybe high level philosophical disagreement
[01:52:23.160 --> 01:52:24.920]   with Eliezer Yudkowsky
[01:52:24.920 --> 01:52:30.080]   in the hope you're stating?
[01:52:30.080 --> 01:52:31.960]   So he is very sure,
[01:52:31.960 --> 01:52:35.560]   he puts a very high probability,
[01:52:35.560 --> 01:52:38.560]   very close to one, depending on the day he puts it at one,
[01:52:39.400 --> 01:52:42.800]   that AI is going to kill humans.
[01:52:42.800 --> 01:52:47.320]   That there's just, he does not see a trajectory
[01:52:47.320 --> 01:52:50.920]   which it doesn't end up with that conclusion.
[01:52:50.920 --> 01:52:54.360]   What trajectory do you see that doesn't end up there?
[01:52:54.360 --> 01:52:57.680]   And maybe can you see the point he's making?
[01:52:57.680 --> 01:53:01.300]   And can you also see a way out?
[01:53:01.300 --> 01:53:07.480]   - First of all, I tremendously respect Eliezer Yudkowsky
[01:53:07.480 --> 01:53:10.040]   and his thinking.
[01:53:10.040 --> 01:53:13.280]   Second, I do share his view
[01:53:13.280 --> 01:53:14.840]   that there's a pretty large chance
[01:53:14.840 --> 01:53:16.840]   that we're not gonna make it as humans,
[01:53:16.840 --> 01:53:19.800]   that there won't be any humans on the planet
[01:53:19.800 --> 01:53:20.840]   in the not-too-distant future.
[01:53:20.840 --> 01:53:22.280]   And that makes me very sad.
[01:53:22.280 --> 01:53:24.840]   We just had a little baby, and I keep asking myself,
[01:53:24.840 --> 01:53:34.080]   how old is he even gonna get?
[01:53:34.080 --> 01:53:35.960]   And I ask myself,
[01:53:37.360 --> 01:53:39.200]   it feels, I said to my wife recently,
[01:53:39.200 --> 01:53:40.960]   it feels a little bit like I was just diagnosed
[01:53:40.960 --> 01:53:43.560]   with some sort of cancer,
[01:53:43.560 --> 01:53:48.000]   which has some risk of dying from
[01:53:48.000 --> 01:53:49.360]   and some risk of surviving,
[01:53:49.360 --> 01:53:53.560]   except this is the kind of cancer
[01:53:53.560 --> 01:53:54.720]   which will kill all of humanity.
[01:53:54.720 --> 01:53:59.220]   So I completely take seriously his concerns.
[01:53:59.220 --> 01:54:05.360]   I think, but I absolutely don't think it's hopeless.
[01:54:05.360 --> 01:54:10.360]   I think there is, first of all, a lot of momentum now,
[01:54:10.360 --> 01:54:15.200]   for the first time, actually,
[01:54:15.200 --> 01:54:16.960]   since the many, many years that have passed,
[01:54:16.960 --> 01:54:20.000]   since I and many others started warning about this,
[01:54:20.000 --> 01:54:23.040]   I feel most people are getting it now.
[01:54:23.040 --> 01:54:30.400]   Just talking to this guy in the gas station
[01:54:30.400 --> 01:54:32.200]   near our house the other day,
[01:54:34.440 --> 01:54:37.880]   and he's like, "I think we're getting replaced."
[01:54:37.880 --> 01:54:44.360]   So that's positive, that we're finally seeing this reaction,
[01:54:44.360 --> 01:54:47.640]   which is the first step towards solving the problem.
[01:54:47.640 --> 01:54:50.480]   Second, I really think that this vision
[01:54:50.480 --> 01:54:52.280]   of only running AIs,
[01:54:52.280 --> 01:54:55.720]   if the stakes are really high,
[01:54:55.720 --> 01:54:57.880]   that can prove to us that they're safe,
[01:54:57.880 --> 01:55:00.280]   it's really just virus checking in reverse again.
[01:55:00.280 --> 01:55:02.720]   I think it's scientifically doable.
[01:55:03.680 --> 01:55:05.120]   I don't think it's hopeless.
[01:55:05.120 --> 01:55:08.960]   We might have to forfeit some of the technology
[01:55:08.960 --> 01:55:12.320]   that we could get if we were putting blind faith in our AIs,
[01:55:12.320 --> 01:55:14.360]   but we're still gonna get amazing stuff.
[01:55:14.360 --> 01:55:16.080]   - Do you envision a process with a proof checker,
[01:55:16.080 --> 01:55:18.840]   like something like GPT-4 or GPT-5,
[01:55:18.840 --> 01:55:21.840]   would go through a process of rigorous interrogation?
[01:55:21.840 --> 01:55:23.200]   - No, I think it's hopeless.
[01:55:23.200 --> 01:55:25.840]   That's like trying to prove Vero about five spaghetti.
[01:55:25.840 --> 01:55:27.720]   (laughing)
[01:55:27.720 --> 01:55:31.440]   What I think, well, the vision I have for success
[01:55:31.440 --> 01:55:34.600]   is instead that just like we human beings
[01:55:34.600 --> 01:55:36.720]   were able to look at our brains
[01:55:36.720 --> 01:55:38.160]   and distill out the key knowledge.
[01:55:38.160 --> 01:55:42.520]   Galileo, when his dad threw him an apple when he was a kid,
[01:55:42.520 --> 01:55:43.360]   he was able to catch it
[01:55:43.360 --> 01:55:45.840]   'cause his brain could in this funny spaghetti kind of way,
[01:55:45.840 --> 01:55:48.040]   predict how parabolas are gonna move.
[01:55:48.040 --> 01:55:49.640]   His Kahneman system won, right?
[01:55:49.640 --> 01:55:53.200]   Then he got older and it's like, wait, this is a parabola.
[01:55:53.200 --> 01:55:55.640]   It's Y equals X squared.
[01:55:55.640 --> 01:55:56.960]   I can distill this knowledge out,
[01:55:56.960 --> 01:55:59.680]   and today you can easily program it into a computer
[01:55:59.680 --> 01:56:01.640]   and it can simulate not just that,
[01:56:01.640 --> 01:56:04.240]   but how to get to Mars and so on, right?
[01:56:04.240 --> 01:56:05.640]   I envision a similar process
[01:56:05.640 --> 01:56:09.120]   where we use the amazing learning power of neural networks
[01:56:09.120 --> 01:56:12.320]   to discover the knowledge in the first place,
[01:56:12.320 --> 01:56:16.880]   but we don't stop with a black box and use that.
[01:56:16.880 --> 01:56:19.160]   We then do a second round of AI
[01:56:19.160 --> 01:56:21.600]   where we use automated systems to extract out the knowledge
[01:56:21.600 --> 01:56:24.120]   and see what are the insights it's had.
[01:56:24.120 --> 01:56:28.280]   And then we put that knowledge
[01:56:28.280 --> 01:56:31.800]   into a completely different kind of architecture
[01:56:31.800 --> 01:56:33.640]   or programming language or whatever
[01:56:33.640 --> 01:56:37.040]   that's made in a way that it can be both really efficient
[01:56:37.040 --> 01:56:41.480]   and also is more amenable to very formal verification.
[01:56:41.480 --> 01:56:44.280]   That's my vision.
[01:56:44.280 --> 01:56:46.880]   I'm not sitting here saying I'm confident,
[01:56:46.880 --> 01:56:48.600]   100% sure that it's gonna work,
[01:56:48.600 --> 01:56:51.960]   but I don't think, the chance is certainly not zero either,
[01:56:51.960 --> 01:56:53.560]   and it will certainly be possible to do
[01:56:53.560 --> 01:56:57.280]   for a lot of really cool AI applications
[01:56:57.280 --> 01:56:58.920]   that we're not using now.
[01:56:58.920 --> 01:57:02.240]   So we can have a lot of the fun that we're excited about
[01:57:02.240 --> 01:57:03.840]   if we do this.
[01:57:03.840 --> 01:57:05.680]   We're gonna need a little bit of time.
[01:57:05.680 --> 01:57:08.560]   That's why it's good to pause
[01:57:08.560 --> 01:57:12.440]   and put in place requirements.
[01:57:12.440 --> 01:57:15.640]   One more thing also, I think,
[01:57:15.640 --> 01:57:17.960]   someone might think,
[01:57:17.960 --> 01:57:20.680]   well, 0% chance we're gonna survive.
[01:57:20.680 --> 01:57:22.680]   Let's just give up, right?
[01:57:22.680 --> 01:57:23.800]   That's very dangerous
[01:57:26.240 --> 01:57:29.840]   because there's no more guaranteed way to fail
[01:57:29.840 --> 01:57:33.280]   than to convince yourself that it's impossible and not try.
[01:57:33.280 --> 01:57:39.280]   When you study history and military history,
[01:57:39.280 --> 01:57:40.920]   the first thing you learn is
[01:57:40.920 --> 01:57:44.680]   that that's how you do psychological warfare.
[01:57:44.680 --> 01:57:47.120]   You persuade the other side that it's hopeless
[01:57:47.120 --> 01:57:48.360]   so they don't even fight.
[01:57:48.360 --> 01:57:51.520]   And then, of course, you win, right?
[01:57:51.520 --> 01:57:55.120]   Let's not do this psychological warfare on ourselves
[01:57:55.120 --> 01:57:56.760]   and say there's 100% probability
[01:57:56.760 --> 01:57:58.400]   we're all screwed anyway.
[01:57:58.400 --> 01:58:03.480]   It's sadly, I do get that a little bit sometimes
[01:58:03.480 --> 01:58:06.680]   from some young people who are so convinced
[01:58:06.680 --> 01:58:08.160]   that we're all screwed that they're like,
[01:58:08.160 --> 01:58:12.000]   I'm just gonna play computer games and do drugs
[01:58:12.000 --> 01:58:14.000]   'cause we're screwed anyway, right?
[01:58:14.000 --> 01:58:17.560]   It's important to keep the hope alive
[01:58:17.560 --> 01:58:20.120]   because it actually has a causal impact
[01:58:20.120 --> 01:58:22.720]   and makes it more likely that we're gonna succeed.
[01:58:22.720 --> 01:58:25.680]   - It seems like the people that actually build solutions
[01:58:25.680 --> 01:58:28.760]   to a problem seemingly impossible to solve problems
[01:58:28.760 --> 01:58:31.200]   are the ones that believe.
[01:58:31.200 --> 01:58:33.040]   They're the ones who are the optimists.
[01:58:33.040 --> 01:58:36.560]   And it seems like there's some fundamental law
[01:58:36.560 --> 01:58:38.720]   to the universe where fake it 'til you make it
[01:58:38.720 --> 01:58:40.000]   kind of works.
[01:58:40.000 --> 01:58:43.480]   Like, believe it's possible and it becomes possible.
[01:58:43.480 --> 01:58:46.000]   - Yeah, was it Henry Ford who said that
[01:58:46.000 --> 01:58:50.900]   if you tell yourself that it's impossible, it is?
[01:58:52.480 --> 01:58:54.040]   Let's not make that mistake.
[01:58:54.040 --> 01:58:56.560]   And this is a big mistake society is making,
[01:58:56.560 --> 01:58:57.400]   I think all in all.
[01:58:57.400 --> 01:58:59.920]   Everybody's so gloomy and the media are also very biased
[01:58:59.920 --> 01:59:02.400]   towards if it bleeds, it leads and gloom and doom.
[01:59:02.400 --> 01:59:07.400]   So most visions of the future we have
[01:59:07.400 --> 01:59:12.600]   are dystopian, which really demotivates people.
[01:59:12.600 --> 01:59:16.000]   We wanna really, really, really focus on the upside also
[01:59:16.000 --> 01:59:18.800]   to give people the willingness to fight for it.
[01:59:18.800 --> 01:59:23.800]   And for AI, you and I mostly talked about gloom here again,
[01:59:23.800 --> 01:59:30.000]   but let's not forget that we have probably both lost
[01:59:30.000 --> 01:59:33.920]   someone we really cared about to some disease
[01:59:33.920 --> 01:59:35.680]   that we were told was incurable.
[01:59:35.680 --> 01:59:37.160]   Well, it's not.
[01:59:37.160 --> 01:59:38.480]   There's no law of physics saying
[01:59:38.480 --> 01:59:40.400]   you have to die of that cancer or whatever.
[01:59:40.400 --> 01:59:42.280]   Of course you can cure it.
[01:59:42.280 --> 01:59:44.160]   And there are so many other things
[01:59:44.160 --> 01:59:45.920]   that we with our human intelligence
[01:59:45.920 --> 01:59:49.360]   have also failed to solve on this planet,
[01:59:49.360 --> 01:59:52.280]   which AI could also very much help us with.
[01:59:52.280 --> 01:59:56.760]   So if we can get this right, just be a little more chill
[01:59:56.760 --> 01:59:59.160]   and slow down a little bit till we get it right.
[01:59:59.160 --> 02:00:04.480]   It's mind blowing how awesome our future can be.
[02:00:04.480 --> 02:00:08.000]   We talked a lot about stuff on earth, it can be great.
[02:00:08.000 --> 02:00:09.920]   But even if you really get ambitious
[02:00:09.920 --> 02:00:11.280]   and look up into the skies,
[02:00:11.280 --> 02:00:13.680]   there's no reason we have to be stuck on this planet
[02:00:13.680 --> 02:00:16.800]   for the rest of the remaining,
[02:00:16.800 --> 02:00:19.160]   for billions of years to come.
[02:00:19.160 --> 02:00:22.480]   We totally understand now as laws of physics
[02:00:22.480 --> 02:00:26.480]   let life spread out into space to other solar systems,
[02:00:26.480 --> 02:00:27.840]   to other galaxies and flourish
[02:00:27.840 --> 02:00:29.960]   for billions and billions of years.
[02:00:29.960 --> 02:00:32.800]   And this to me is a very, very hopeful vision
[02:00:32.800 --> 02:00:37.880]   that really motivates me to fight.
[02:00:37.880 --> 02:00:39.040]   And coming back to the end,
[02:00:39.040 --> 02:00:40.680]   something you talked about again,
[02:00:40.680 --> 02:00:42.920]   the struggle, how the human struggle
[02:00:42.920 --> 02:00:45.240]   is one of the things that also really gives meaning
[02:00:45.240 --> 02:00:46.440]   to our lives.
[02:00:46.440 --> 02:00:50.080]   If there's ever been an epic struggle, this is it.
[02:00:50.080 --> 02:00:53.320]   And isn't it even more epic if you're the underdog?
[02:00:53.320 --> 02:00:55.760]   If most people are telling you this is gonna fail,
[02:00:55.760 --> 02:00:57.480]   it's impossible, right?
[02:00:57.480 --> 02:01:01.040]   And you persist and you succeed.
[02:01:01.040 --> 02:01:05.280]   That's what we can do together as a species on this one.
[02:01:05.280 --> 02:01:08.800]   A lot of pundits are ready to count this out.
[02:01:08.800 --> 02:01:11.560]   - Both in the battle to keep AI safe
[02:01:11.560 --> 02:01:13.680]   and becoming a multi-planetary species.
[02:01:13.680 --> 02:01:16.480]   - Yeah, and they're the same challenge.
[02:01:16.480 --> 02:01:17.640]   If we can keep AI safe,
[02:01:17.640 --> 02:01:21.600]   that's how we're gonna get multi-planetary very efficiently.
[02:01:21.600 --> 02:01:23.600]   - I have some sort of technical questions
[02:01:23.600 --> 02:01:24.720]   about how to get it right.
[02:01:24.720 --> 02:01:28.800]   So one idea that I'm not even sure
[02:01:28.800 --> 02:01:31.200]   what the right answer is to is,
[02:01:31.200 --> 02:01:35.000]   should systems like GPT-4 be open sourced
[02:01:35.000 --> 02:01:36.600]   in whole or in part?
[02:01:36.600 --> 02:01:38.880]   Can you see the case for either?
[02:01:40.680 --> 02:01:42.600]   - I think the answer right now is no.
[02:01:42.600 --> 02:01:45.880]   I think the answer early on was yes.
[02:01:45.880 --> 02:01:50.200]   So we could bring in all the wonderful,
[02:01:50.200 --> 02:01:53.160]   great thought process of everybody on this.
[02:01:53.160 --> 02:01:56.600]   But asking, should we open source GPT-4 now
[02:01:56.600 --> 02:01:57.880]   is just the same as if you say,
[02:01:57.880 --> 02:01:58.720]   well, is it good?
[02:01:58.720 --> 02:02:02.320]   Should we open source how to build
[02:02:02.320 --> 02:02:04.280]   really small nuclear weapons?
[02:02:04.280 --> 02:02:08.680]   Should we open source how to make bioweapons?
[02:02:09.680 --> 02:02:13.600]   Should we open source how to make a new virus
[02:02:13.600 --> 02:02:15.440]   that kills 90% of everybody who gets it?
[02:02:15.440 --> 02:02:17.400]   Of course we shouldn't.
[02:02:17.400 --> 02:02:19.600]   - So it's already that powerful.
[02:02:19.600 --> 02:02:22.720]   It's already that powerful that we have to respect
[02:02:22.720 --> 02:02:26.680]   the power of the systems we've built.
[02:02:26.680 --> 02:02:29.000]   - The knowledge that you get
[02:02:29.000 --> 02:02:32.520]   from open sourcing everything we do now
[02:02:32.520 --> 02:02:35.120]   might very well be powerful enough
[02:02:35.120 --> 02:02:36.640]   that people looking at that
[02:02:38.040 --> 02:02:39.960]   can use it to build the things
[02:02:39.960 --> 02:02:41.600]   that are really threatening.
[02:02:41.600 --> 02:02:43.280]   Remember, open AI is,
[02:02:43.280 --> 02:02:46.440]   GPT-4 is a baby AI.
[02:02:46.440 --> 02:02:50.720]   Baby, sort of baby proto, almost little bit AGI
[02:02:50.720 --> 02:02:53.920]   according to what Microsoft's recent paper said.
[02:02:53.920 --> 02:02:55.640]   It's not that that we're scared of.
[02:02:55.640 --> 02:02:57.880]   What we're scared about is people taking that
[02:02:57.880 --> 02:03:01.280]   who might be a lot less responsible
[02:03:01.280 --> 02:03:02.680]   than the company that made it
[02:03:02.680 --> 02:03:06.000]   and just going to town with it.
[02:03:06.000 --> 02:03:10.480]   That's why we want to,
[02:03:10.480 --> 02:03:12.200]   it's an information hazard.
[02:03:12.200 --> 02:03:15.120]   There are many things which are not open sourced
[02:03:15.120 --> 02:03:17.760]   right now in society for a very good reason.
[02:03:17.760 --> 02:03:18.960]   Like how do you make
[02:03:18.960 --> 02:03:23.600]   certain kind of very powerful toxins
[02:03:23.600 --> 02:03:27.000]   out of stuff you can buy at Home Depot,
[02:03:27.000 --> 02:03:29.560]   you don't open source those things for a reason.
[02:03:29.560 --> 02:03:32.480]   And this is really no different.
[02:03:32.480 --> 02:03:34.920]   - So open-- - And I'm saying that,
[02:03:34.920 --> 02:03:38.120]   I have to say it feels in a way a bit weird to say it
[02:03:38.120 --> 02:03:42.400]   because MIT is like the cradle of the open source movement.
[02:03:42.400 --> 02:03:44.320]   And I love open source in general,
[02:03:44.320 --> 02:03:46.080]   power to the people, let's say.
[02:03:46.080 --> 02:03:50.920]   But there's always gonna be some stuff
[02:03:50.920 --> 02:03:52.320]   that you don't open source.
[02:03:52.320 --> 02:03:55.480]   And it's just like you don't open source.
[02:03:55.480 --> 02:03:56.880]   So we have a three month old baby, right?
[02:03:56.880 --> 02:03:58.480]   When he gets a little bit older,
[02:03:58.480 --> 02:03:59.640]   we're not gonna open source to him
[02:03:59.640 --> 02:04:02.080]   all the most dangerous things he can do in the house.
[02:04:02.080 --> 02:04:02.920]   - Yeah. - Right?
[02:04:04.400 --> 02:04:07.040]   But it does, it's a weird feeling
[02:04:07.040 --> 02:04:10.600]   because this is one of the first moments in history
[02:04:10.600 --> 02:04:13.240]   where there's a strong case to be made
[02:04:13.240 --> 02:04:15.740]   not to open source software.
[02:04:15.740 --> 02:04:19.720]   This is when the software has become too dangerous.
[02:04:19.720 --> 02:04:21.160]   - Yeah, but it's not the first time
[02:04:21.160 --> 02:04:23.080]   that we didn't wanna open source a technology.
[02:04:23.080 --> 02:04:24.040]   - Technology, yeah.
[02:04:24.040 --> 02:04:28.400]   Is there something to be said
[02:04:28.400 --> 02:04:30.980]   about how to get the release of such systems right?
[02:04:30.980 --> 02:04:32.980]   Like GPT-4 and GPT-5.
[02:04:33.980 --> 02:04:37.820]   So OpenAI went through a pretty rigorous effort
[02:04:37.820 --> 02:04:39.140]   for several months.
[02:04:39.140 --> 02:04:40.300]   You could say it could be longer,
[02:04:40.300 --> 02:04:42.900]   but nevertheless it's longer than you would have expected
[02:04:42.900 --> 02:04:44.540]   of trying to test the system
[02:04:44.540 --> 02:04:46.660]   to see like what are the ways it goes wrong
[02:04:46.660 --> 02:04:49.300]   to make it very difficult for people,
[02:04:49.300 --> 02:04:51.860]   somewhat difficult for people to ask things,
[02:04:51.860 --> 02:04:54.300]   how do I make a bomb for $1?
[02:04:54.300 --> 02:05:00.180]   Or how do I say I hate a certain group on Twitter
[02:05:00.180 --> 02:05:02.260]   in a way that doesn't get me blocked from Twitter,
[02:05:02.260 --> 02:05:05.380]   banned from Twitter, those kinds of questions.
[02:05:05.380 --> 02:05:08.940]   So you basically use the system to do harm.
[02:05:08.940 --> 02:05:13.340]   Is there something you could say about ideas
[02:05:13.340 --> 02:05:15.460]   you have just on looking,
[02:05:15.460 --> 02:05:17.740]   having thought about this problem of AI safety,
[02:05:17.740 --> 02:05:18.940]   how to release such system,
[02:05:18.940 --> 02:05:21.000]   how to test such systems when you have them
[02:05:21.000 --> 02:05:22.280]   inside the company?
[02:05:22.280 --> 02:05:29.900]   - Yeah, so a lot of people say
[02:05:29.900 --> 02:05:33.220]   that the two biggest risks from large language models are
[02:05:33.220 --> 02:05:40.020]   it's spreading disinformation,
[02:05:40.020 --> 02:05:42.380]   harmful information of various types.
[02:05:42.380 --> 02:05:48.620]   And second, being used for offensive cyber weapon.
[02:05:48.620 --> 02:05:53.300]   So I think those are not the two greatest threats.
[02:05:53.300 --> 02:05:55.020]   They're very serious threats and it's wonderful
[02:05:55.020 --> 02:05:57.400]   that people are trying to mitigate them.
[02:05:58.500 --> 02:06:00.220]   A much bigger elephant in the room
[02:06:00.220 --> 02:06:02.620]   is how is this just gonna disrupt our economy
[02:06:02.620 --> 02:06:03.620]   in a huge way, obviously,
[02:06:03.620 --> 02:06:06.300]   and maybe take away a lot of the most meaningful jobs.
[02:06:06.300 --> 02:06:08.820]   And an even bigger one is the one we spent
[02:06:08.820 --> 02:06:10.180]   so much time talking about here,
[02:06:10.180 --> 02:06:15.180]   that this becomes the bootloader
[02:06:15.180 --> 02:06:17.860]   for the more powerful AI.
[02:06:17.860 --> 02:06:21.120]   - Write code, connect it to the internet, manipulate humans.
[02:06:21.120 --> 02:06:23.900]   - Yeah, and before we know it, we have something else,
[02:06:23.900 --> 02:06:25.700]   which is not at all a large language model.
[02:06:25.700 --> 02:06:26.860]   It looks nothing like it,
[02:06:26.860 --> 02:06:29.860]   but which is way more intelligent and capable and has goals.
[02:06:29.860 --> 02:06:33.720]   And that's the elephant in the room.
[02:06:33.720 --> 02:06:36.220]   And obviously, no matter how hard
[02:06:36.220 --> 02:06:37.920]   any of these companies have tried,
[02:06:37.920 --> 02:06:41.220]   that's not something that's easy for them to verify
[02:06:41.220 --> 02:06:42.460]   with large language models.
[02:06:42.460 --> 02:06:45.660]   And the only way to really lower that risk a lot
[02:06:45.660 --> 02:06:48.940]   would be to not let, for example,
[02:06:48.940 --> 02:06:52.020]   never let it read any code, not train on that,
[02:06:52.020 --> 02:06:53.680]   and not put it into an API,
[02:06:54.400 --> 02:06:59.400]   and not give it access to so much information
[02:06:59.400 --> 02:07:01.960]   about how to manipulate humans.
[02:07:01.960 --> 02:07:05.840]   But that doesn't mean you still can't make
[02:07:05.840 --> 02:07:08.280]   a ton of money on them.
[02:07:08.280 --> 02:07:13.720]   We're gonna just watch now this coming year,
[02:07:13.720 --> 02:07:17.680]   Microsoft is rolling out the new Office suite
[02:07:17.680 --> 02:07:21.480]   where you go into Microsoft Word and give it a prompt,
[02:07:21.480 --> 02:07:23.160]   and it writes the whole text for you,
[02:07:23.160 --> 02:07:24.600]   and then you edit it.
[02:07:24.600 --> 02:07:25.440]   And then you're like,
[02:07:25.440 --> 02:07:26.680]   "Oh, give me a PowerPoint version of this,"
[02:07:26.680 --> 02:07:27.520]   and it makes it.
[02:07:27.520 --> 02:07:31.440]   And now take the spreadsheet and blah, blah.
[02:07:31.440 --> 02:07:32.920]   All of those things, I think,
[02:07:32.920 --> 02:07:35.920]   you can debate the economic impact of it
[02:07:35.920 --> 02:07:39.280]   and whether society is prepared to deal with this disruption,
[02:07:39.280 --> 02:07:41.040]   but those are not the things which,
[02:07:41.040 --> 02:07:43.560]   that's not the elephant of the room
[02:07:43.560 --> 02:07:46.200]   that keeps me awake at night for wiping out humanity.
[02:07:46.200 --> 02:07:51.200]   And I think that's the biggest misunderstanding we have.
[02:07:51.200 --> 02:07:52.640]   A lot of people think that we're scared
[02:07:52.640 --> 02:07:55.680]   of automatic spreadsheets.
[02:07:55.680 --> 02:07:56.560]   That's not the case.
[02:07:56.560 --> 02:07:59.720]   That's not what Eliezer was freaked out about either.
[02:07:59.720 --> 02:08:03.600]   - Is there, in terms of the actual mechanism
[02:08:03.600 --> 02:08:06.720]   of how AI might kill all humans,
[02:08:06.720 --> 02:08:09.600]   so something you've been outspoken about,
[02:08:09.600 --> 02:08:13.720]   you've talked about a lot, is autonomous weapon systems.
[02:08:13.720 --> 02:08:17.200]   So the use of AI in war.
[02:08:17.200 --> 02:08:21.200]   Is that one of the things that still you carry concern for
[02:08:21.200 --> 02:08:23.120]   as these systems become more and more powerful?
[02:08:23.120 --> 02:08:24.120]   - I carry concern for it,
[02:08:24.120 --> 02:08:26.480]   not that all humans are going to get killed by slaughterbots,
[02:08:26.480 --> 02:08:31.480]   but rather just as express route into Orwellian dystopia
[02:08:31.480 --> 02:08:35.080]   where it becomes much easier for very few to kill very many
[02:08:35.080 --> 02:08:36.400]   and therefore it becomes very easy
[02:08:36.400 --> 02:08:38.200]   for very few to dominate very many.
[02:08:38.200 --> 02:08:43.920]   If you want to know how AI could kill all people,
[02:08:43.920 --> 02:08:45.400]   just ask yourself,
[02:08:45.400 --> 02:08:47.720]   humans have driven a lot of species extinct.
[02:08:47.720 --> 02:08:48.660]   How do we do it?
[02:08:49.960 --> 02:08:51.320]   We were smarter than them.
[02:08:51.320 --> 02:08:55.520]   Usually we didn't do it even systematically
[02:08:55.520 --> 02:08:58.000]   by going around one after the other
[02:08:58.000 --> 02:09:00.080]   and stepping on them or shooting them or anything like that.
[02:09:00.080 --> 02:09:02.160]   We just like chopped down their habitat
[02:09:02.160 --> 02:09:04.240]   'cause we needed it for something else.
[02:09:04.240 --> 02:09:08.080]   In some cases, we did it by putting more carbon dioxide
[02:09:08.080 --> 02:09:10.840]   in the atmosphere because of some reason
[02:09:10.840 --> 02:09:13.680]   that those animals didn't even understand
[02:09:13.680 --> 02:09:15.800]   and now they're gone, right?
[02:09:15.800 --> 02:09:20.800]   So if you're an AI and you just want to figure something out
[02:09:20.800 --> 02:09:26.480]   then you decide, we just really need the space here
[02:09:26.480 --> 02:09:28.520]   to build more compute facilities.
[02:09:28.520 --> 02:09:34.240]   If that's the only goal it has,
[02:09:34.240 --> 02:09:37.920]   we are just the sort of accidental roadkill along the way.
[02:09:37.920 --> 02:09:38.960]   And you could totally imagine,
[02:09:38.960 --> 02:09:40.840]   yeah, maybe this oxygen is kind of annoying
[02:09:40.840 --> 02:09:42.160]   'cause it caused more corrosion,
[02:09:42.160 --> 02:09:44.360]   so let's get rid of the oxygen
[02:09:44.360 --> 02:09:46.480]   and good luck surviving after that.
[02:09:46.480 --> 02:09:48.200]   I'm not particularly concerned
[02:09:48.200 --> 02:09:49.920]   that they would want to kill us
[02:09:49.920 --> 02:09:54.920]   just because that would be a goal in itself.
[02:09:54.920 --> 02:10:02.320]   We've driven a number of the elephant species extinct.
[02:10:02.320 --> 02:10:04.420]   It wasn't 'cause we didn't like elephants.
[02:10:04.420 --> 02:10:11.040]   The basic problem is you just don't want to give,
[02:10:11.040 --> 02:10:13.960]   you don't want to cede control over your planet
[02:10:13.960 --> 02:10:17.120]   to some other more intelligent entity
[02:10:17.120 --> 02:10:18.440]   that doesn't share your goals.
[02:10:18.440 --> 02:10:19.280]   It's that simple.
[02:10:19.280 --> 02:10:23.720]   So which brings us to another key challenge
[02:10:23.720 --> 02:10:25.880]   which AI safety researchers have been grappling with
[02:10:25.880 --> 02:10:27.440]   for a long time.
[02:10:27.440 --> 02:10:31.720]   How do you make AI first of all understand our goals
[02:10:31.720 --> 02:10:32.760]   and then adopt our goals
[02:10:32.760 --> 02:10:35.160]   and then retain them as they get smarter, right?
[02:10:41.520 --> 02:10:44.080]   All three of those are really hard, right?
[02:10:44.080 --> 02:10:49.080]   Like a human child, first they're just not smart enough
[02:10:49.080 --> 02:10:50.640]   to understand our goals.
[02:10:50.640 --> 02:10:53.020]   They can't even talk.
[02:10:53.020 --> 02:10:56.240]   And then eventually they're teenagers
[02:10:56.240 --> 02:10:57.640]   and understand our goals just fine,
[02:10:57.640 --> 02:10:59.080]   but they don't share.
[02:10:59.080 --> 02:11:03.760]   But there is fortunately a magic phase in the middle
[02:11:03.760 --> 02:11:05.440]   where they're smart enough to understand our goals
[02:11:05.440 --> 02:11:06.840]   and malleable enough that we can hopefully
[02:11:06.840 --> 02:11:09.580]   with good parenting teach them right from wrong
[02:11:09.580 --> 02:11:12.280]   and instill good goals in them, right?
[02:11:12.280 --> 02:11:17.960]   So those are all tough challenges with computers.
[02:11:17.960 --> 02:11:20.560]   And then even if you teach your kids good goals
[02:11:20.560 --> 02:11:22.300]   when they're little, they might outgrow them too.
[02:11:22.300 --> 02:11:25.720]   And that's a challenge for machines to keep improving.
[02:11:25.720 --> 02:11:30.380]   So these are a lot of hard challenges we're up for,
[02:11:30.380 --> 02:11:33.240]   but I don't think any of them are insurmountable.
[02:11:33.240 --> 02:11:37.980]   The fundamental reason why Eliezer looked so depressed
[02:11:37.980 --> 02:11:39.800]   when he last saw him was because he felt
[02:11:39.800 --> 02:11:42.060]   there just wasn't enough time.
[02:11:42.060 --> 02:11:44.600]   - Oh, not that it was unsolvable.
[02:11:44.600 --> 02:11:46.000]   - Correct. - There's just not enough time.
[02:11:46.000 --> 02:11:48.240]   - He was hoping that humanity was gonna take this threat
[02:11:48.240 --> 02:11:50.800]   more seriously so we would have more time.
[02:11:50.800 --> 02:11:51.640]   - Yeah.
[02:11:51.640 --> 02:11:53.360]   - And now we don't have more time.
[02:11:53.360 --> 02:11:56.360]   That's why the open letter is calling for more time.
[02:11:56.360 --> 02:12:02.880]   - But even with time, the AI alignment problem
[02:12:02.880 --> 02:12:06.360]   seems to be really difficult.
[02:12:06.360 --> 02:12:07.200]   - Oh yeah.
[02:12:08.200 --> 02:12:11.660]   - But it's also the most worthy problem,
[02:12:11.660 --> 02:12:14.220]   the most important problem for humanity to ever solve.
[02:12:14.220 --> 02:12:15.940]   Because if we solve that one, Lex,
[02:12:15.940 --> 02:12:20.740]   that aligned AI can help us solve all the other problems.
[02:12:20.740 --> 02:12:23.940]   - 'Cause it seems like it has to have constant humility
[02:12:23.940 --> 02:12:26.440]   about its goal, constantly questioning the goal.
[02:12:26.440 --> 02:12:31.220]   Because as you optimize towards a particular goal
[02:12:31.220 --> 02:12:32.580]   and you start to achieve it,
[02:12:32.580 --> 02:12:34.320]   that's when you have the unintended consequences,
[02:12:34.320 --> 02:12:35.940]   all the things you mentioned about.
[02:12:35.940 --> 02:12:40.000]   So how do you enforce and code a constant humility
[02:12:40.000 --> 02:12:42.920]   as your ability become better and better and better and better
[02:12:42.920 --> 02:12:44.760]   - Stewart, Professor Stewart Russell at Berkeley,
[02:12:44.760 --> 02:12:49.400]   who's also one of the driving forces behind this letter,
[02:12:49.400 --> 02:12:54.320]   he has a whole research program about this.
[02:12:54.320 --> 02:12:59.080]   I think of it as AI humility, exactly.
[02:12:59.080 --> 02:13:01.320]   Although he calls it inverse reinforcement learning
[02:13:01.320 --> 02:13:02.840]   and other nerdy terms.
[02:13:02.840 --> 02:13:04.120]   But it's about exactly that.
[02:13:04.140 --> 02:13:06.100]   Instead of telling the AI, here's this goal,
[02:13:06.100 --> 02:13:08.920]   go optimize the bejesus out of it.
[02:13:08.920 --> 02:13:15.220]   You tell it, okay, do what I want you to do,
[02:13:15.220 --> 02:13:16.900]   but I'm not gonna tell you right now what it is
[02:13:16.900 --> 02:13:19.260]   I want you to do, you need to figure it out.
[02:13:19.260 --> 02:13:21.700]   So then you give the incentive to be very humble
[02:13:21.700 --> 02:13:23.360]   and keep asking you questions along the way.
[02:13:23.360 --> 02:13:24.580]   Is this what you really meant?
[02:13:24.580 --> 02:13:25.660]   Is this what you wanted?
[02:13:25.660 --> 02:13:28.140]   And oh, this other thing I tried didn't work,
[02:13:28.140 --> 02:13:29.340]   seemed like it didn't work out right,
[02:13:29.340 --> 02:13:31.400]   should I try it differently?
[02:13:33.240 --> 02:13:35.600]   What's nice about this is it's not just philosophical
[02:13:35.600 --> 02:13:38.320]   mumbo jumbo, it's theorems and technical work
[02:13:38.320 --> 02:13:40.860]   that with more time I think you can make a lot of progress.
[02:13:40.860 --> 02:13:43.320]   And there are a lot of brilliant people now
[02:13:43.320 --> 02:13:44.560]   working on AI safety.
[02:13:44.560 --> 02:13:47.840]   We just need to give them a bit more time.
[02:13:47.840 --> 02:13:50.800]   - But also not that many relative to the scale of the problem.
[02:13:50.800 --> 02:13:51.960]   - No, exactly.
[02:13:51.960 --> 02:13:56.400]   There should be, at least just like every university
[02:13:56.400 --> 02:13:59.120]   worth its name has some cancer research going on
[02:13:59.120 --> 02:14:01.520]   in its biology department, right?
[02:14:01.520 --> 02:14:03.800]   Every university that does computer science
[02:14:03.800 --> 02:14:07.060]   should have a real effort in this area
[02:14:07.060 --> 02:14:09.300]   and it's nowhere near that.
[02:14:09.300 --> 02:14:11.980]   This is something I hope is changing now
[02:14:11.980 --> 02:14:13.780]   thanks to the GPT-4, right?
[02:14:13.780 --> 02:14:17.180]   So I think if there's a silver lining
[02:14:17.180 --> 02:14:20.820]   to what's happening here, even though I think many people
[02:14:20.820 --> 02:14:24.420]   would wish it would have been rolled out more carefully,
[02:14:24.420 --> 02:14:26.900]   is that this might be the wake up call
[02:14:26.900 --> 02:14:30.540]   that humanity needed to really
[02:14:31.540 --> 02:14:35.080]   stop fantasising about this being 100 years off
[02:14:35.080 --> 02:14:37.600]   and stop fantasising about this being completely
[02:14:37.600 --> 02:14:41.960]   controllable and predictable because it's so obvious
[02:14:41.960 --> 02:14:45.240]   it's not predictable, you know?
[02:14:45.240 --> 02:14:50.240]   Why is it that, I think it was ChatGPT
[02:14:50.240 --> 02:14:54.880]   tried to persuade a journalist,
[02:14:54.880 --> 02:15:00.080]   or was it ChatGPT-4, to divorce his wife?
[02:15:00.080 --> 02:15:02.780]   It was not 'cause the engineers that built it
[02:15:02.780 --> 02:15:06.940]   was like, "Heh heh heh heh heh, let's put this in here
[02:15:06.940 --> 02:15:09.720]   "and screw a little bit with people."
[02:15:09.720 --> 02:15:11.800]   They hadn't predicted it at all.
[02:15:11.800 --> 02:15:13.540]   They built the giant black box,
[02:15:13.540 --> 02:15:16.560]   trained to predict the next word,
[02:15:16.560 --> 02:15:18.120]   got all these emergent properties,
[02:15:18.120 --> 02:15:20.540]   and oops, it did this, you know?
[02:15:20.540 --> 02:15:26.440]   I think this is a very powerful wake up call
[02:15:26.440 --> 02:15:29.840]   and anyone watching this who's not scared,
[02:15:29.840 --> 02:15:31.960]   I would encourage them to just play a bit more
[02:15:31.960 --> 02:15:36.020]   with these tools that are out there now, like GPT-4.
[02:15:36.020 --> 02:15:42.240]   It's a wake up call is first step.
[02:15:42.240 --> 02:15:45.600]   Once you've woken up, then gotta slow down a little bit
[02:15:45.600 --> 02:15:48.800]   the risky stuff to give a chance to all,
[02:15:48.800 --> 02:15:51.440]   everyone who's woken up, to catch up
[02:15:51.440 --> 02:15:52.600]   with us on the safety front.
[02:15:52.600 --> 02:15:55.480]   - You know, what's interesting is, you know, MIT,
[02:15:55.480 --> 02:15:58.680]   that's computer science in general,
[02:15:58.680 --> 02:16:01.920]   but let's just even say computer science curriculum.
[02:16:01.920 --> 02:16:04.400]   How does the computer science curriculum change now?
[02:16:04.400 --> 02:16:06.300]   You mentioned programming.
[02:16:06.300 --> 02:16:13.600]   When I was coming up, programming is a prestigious position.
[02:16:13.600 --> 02:16:17.520]   Like, why would you be dedicating crazy amounts of time
[02:16:17.520 --> 02:16:19.240]   to become an excellent programmer?
[02:16:19.240 --> 02:16:21.800]   Like, the nature of programming is fundamentally changing.
[02:16:21.800 --> 02:16:24.760]   - The nature of our entire education system
[02:16:24.760 --> 02:16:28.480]   is completely turned on its head.
[02:16:28.480 --> 02:16:30.840]   Has anyone been able to like load that in
[02:16:30.840 --> 02:16:33.960]   and like think about, 'cause it's really turning.
[02:16:33.960 --> 02:16:36.160]   - I mean, some English professors, some English teachers
[02:16:36.160 --> 02:16:38.360]   are beginning to really freak out now.
[02:16:38.360 --> 02:16:40.560]   Right, like they give an essay assignment
[02:16:40.560 --> 02:16:42.640]   and they get back all this fantastic prose,
[02:16:42.640 --> 02:16:44.880]   like this is the style of Hemingway.
[02:16:44.880 --> 02:16:48.080]   And then they realize they have to completely rethink.
[02:16:48.080 --> 02:16:52.920]   And even, you know, just like we stopped teaching,
[02:16:52.920 --> 02:16:57.880]   writing a script, is that what you say in English?
[02:16:57.880 --> 02:16:59.160]   - Yeah, handwritten, yeah.
[02:16:59.160 --> 02:17:01.200]   - Yeah, when everybody started typing, you know,
[02:17:01.200 --> 02:17:04.080]   like so much of what we teach our kids today.
[02:17:04.080 --> 02:17:09.960]   - Yeah, I mean, that's,
[02:17:09.960 --> 02:17:15.440]   everything is changing and it's changing very,
[02:17:15.440 --> 02:17:17.920]   it's changing very quickly.
[02:17:17.920 --> 02:17:20.680]   And so much of us understanding how to deal
[02:17:20.680 --> 02:17:21.840]   with the big problems of the world
[02:17:21.840 --> 02:17:23.960]   is through the education system.
[02:17:23.960 --> 02:17:26.620]   And if the education system is being turned on its head,
[02:17:26.620 --> 02:17:27.960]   then what's next?
[02:17:27.960 --> 02:17:30.600]   It feels like having these kinds of conversations
[02:17:30.600 --> 02:17:32.920]   is essential to try to figure it out.
[02:17:32.920 --> 02:17:35.480]   And everything's happening so rapidly.
[02:17:35.480 --> 02:17:38.280]   I don't think there's even, speaking of safety,
[02:17:38.280 --> 02:17:40.880]   what the broad AI safety defined,
[02:17:40.880 --> 02:17:44.400]   I don't think most universities have courses on AI safety.
[02:17:44.400 --> 02:17:46.280]   It's like a philosophy seminar.
[02:17:46.280 --> 02:17:48.680]   - And like, I'm an educator myself,
[02:17:48.680 --> 02:17:50.560]   so it pains me to see this, say this,
[02:17:50.560 --> 02:17:52.280]   but I feel our education right now
[02:17:52.280 --> 02:17:56.380]   is completely obsoleted by what's happening.
[02:17:56.380 --> 02:17:58.620]   You know, you put a kid into first grade
[02:17:58.620 --> 02:18:01.580]   and then you're envisioning,
[02:18:01.580 --> 02:18:03.020]   and then they're gonna come out of high school
[02:18:03.020 --> 02:18:06.380]   12 years later, and you've already pre-planned now
[02:18:06.380 --> 02:18:08.700]   what they're gonna learn when you're not even sure
[02:18:08.700 --> 02:18:11.340]   if there's gonna be any world left to come out to.
[02:18:11.340 --> 02:18:16.500]   Clearly, you need to have a much more
[02:18:16.500 --> 02:18:17.980]   opportunistic education system
[02:18:17.980 --> 02:18:20.220]   that keeps adapting itself very rapidly
[02:18:20.220 --> 02:18:22.720]   as society readapts.
[02:18:22.720 --> 02:18:25.200]   The skills that were really useful
[02:18:25.200 --> 02:18:26.520]   when the curriculum was written,
[02:18:26.520 --> 02:18:28.520]   I mean, how many of those skills
[02:18:28.520 --> 02:18:31.240]   are gonna get you a job in 12 years?
[02:18:31.240 --> 02:18:32.560]   I mean, seriously.
[02:18:32.560 --> 02:18:36.160]   - If we just linger on the GPT-4 system a little bit,
[02:18:36.160 --> 02:18:41.920]   you kind of hinted at it, especially talking about
[02:18:41.920 --> 02:18:46.480]   the importance of consciousness in the human mind
[02:18:46.480 --> 02:18:48.380]   with homo sentience.
[02:18:48.380 --> 02:18:51.520]   Do you think GPT-4 is conscious?
[02:18:51.520 --> 02:18:53.960]   - Love this question.
[02:18:53.960 --> 02:18:57.560]   So, let's define consciousness first,
[02:18:57.560 --> 02:19:00.880]   because in my experience, like 90% of all arguments
[02:19:00.880 --> 02:19:03.320]   about consciousness boil down to the two people
[02:19:03.320 --> 02:19:05.060]   arguing, having totally different definitions
[02:19:05.060 --> 02:19:08.240]   of what it is, and they're just shouting past each other.
[02:19:08.240 --> 02:19:13.720]   I define consciousness as subjective experience.
[02:19:13.720 --> 02:19:17.740]   Right now, I'm experiencing colors and sounds
[02:19:17.740 --> 02:19:21.560]   and emotions, but does a self-driving car
[02:19:21.560 --> 02:19:22.680]   experience anything?
[02:19:22.680 --> 02:19:26.400]   That's the question about whether it's conscious or not.
[02:19:26.400 --> 02:19:30.280]   Other people think you should define consciousness
[02:19:30.280 --> 02:19:33.520]   differently, fine by me, but then maybe
[02:19:33.520 --> 02:19:34.960]   use a different word for it.
[02:19:34.960 --> 02:19:38.700]   I'm gonna use consciousness for this, at least.
[02:19:38.700 --> 02:19:43.800]   But if people hate the, yeah.
[02:19:43.800 --> 02:19:46.680]   So, is GPT-4 conscious?
[02:19:46.680 --> 02:19:50.060]   Does GPT-4 have subjective experience?
[02:19:50.060 --> 02:19:53.240]   Short answer, I don't know, because we still don't know
[02:19:53.240 --> 02:19:56.680]   what it is that gives us wonderful subjective experience
[02:19:56.680 --> 02:19:59.240]   that is kind of the meaning of our life, right?
[02:19:59.240 --> 02:20:01.040]   'Cause meaning itself, feeling a meaning
[02:20:01.040 --> 02:20:02.620]   is a subjective experience.
[02:20:02.620 --> 02:20:04.120]   Joy is a subjective experience.
[02:20:04.120 --> 02:20:05.720]   Love is a subjective experience.
[02:20:05.720 --> 02:20:08.660]   We don't know what it is.
[02:20:08.660 --> 02:20:11.560]   I've written some papers about this.
[02:20:11.560 --> 02:20:12.680]   A lot of people have.
[02:20:13.760 --> 02:20:18.480]   Giulio Tononi, a professor, has stuck his neck
[02:20:18.480 --> 02:20:20.040]   out the farthest and written down, actually,
[02:20:20.040 --> 02:20:23.080]   a very bold mathematical conjecture
[02:20:23.080 --> 02:20:26.960]   for what's the essence of conscious information processing.
[02:20:26.960 --> 02:20:29.080]   He might be wrong, he might be right,
[02:20:29.080 --> 02:20:30.180]   but we should test it.
[02:20:30.180 --> 02:20:34.440]   He postulates that consciousness has to do
[02:20:34.440 --> 02:20:37.360]   with loops in the information processing.
[02:20:37.360 --> 02:20:38.640]   So, our brain has loops.
[02:20:38.640 --> 02:20:41.400]   Information can go round and round.
[02:20:41.400 --> 02:20:43.600]   In computer science nerd speak,
[02:20:43.600 --> 02:20:45.460]   you call it a recurrent neural network
[02:20:45.460 --> 02:20:48.440]   where some of the output gets fed back in again.
[02:20:48.440 --> 02:20:53.440]   And with his mathematical formalism,
[02:20:53.440 --> 02:20:56.360]   if it's a feed-forward neural network
[02:20:56.360 --> 02:20:58.680]   where information only goes in one direction,
[02:20:58.680 --> 02:21:01.480]   like from your eye, retina, into the back of your brain,
[02:21:01.480 --> 02:21:03.040]   for example, that's not conscious.
[02:21:03.040 --> 02:21:04.600]   So, he would predict that your retina itself
[02:21:04.600 --> 02:21:09.460]   isn't conscious of anything, or a video camera.
[02:21:09.460 --> 02:21:11.360]   Now, the interesting thing about GPT-4
[02:21:11.360 --> 02:21:14.800]   is it's also just one-way flow of information.
[02:21:14.800 --> 02:21:19.800]   So, if Tononi is right, GPT-4 is a very intelligent zombie
[02:21:19.800 --> 02:21:22.440]   that can do all this smart stuff
[02:21:22.440 --> 02:21:24.040]   but isn't experiencing anything.
[02:21:24.040 --> 02:21:30.120]   And this is both a relief if it's true,
[02:21:30.120 --> 02:21:32.280]   in that you don't have to feel guilty
[02:21:32.280 --> 02:21:35.600]   about turning off GPT-4 and wiping its memory
[02:21:35.600 --> 02:21:37.840]   whenever a new user comes along.
[02:21:37.840 --> 02:21:40.320]   I wouldn't like if someone did that to me,
[02:21:40.320 --> 02:21:42.260]   neuralized me like in Men in Black.
[02:21:42.260 --> 02:21:48.240]   But it's also creepy that you can have
[02:21:48.240 --> 02:21:51.120]   very high intelligence, perhaps, that's not conscious.
[02:21:51.120 --> 02:21:53.400]   Because if we get replaced by machines,
[02:21:53.400 --> 02:21:58.960]   and it's sad enough that humanity isn't here anymore,
[02:21:58.960 --> 02:22:00.520]   'cause I kind of like humanity.
[02:22:00.520 --> 02:22:04.280]   But at least if the machines were conscious,
[02:22:04.280 --> 02:22:06.200]   I could be like, well, but they're our descendants
[02:22:06.200 --> 02:22:09.280]   and maybe they have our values, they're our children.
[02:22:09.280 --> 02:22:13.000]   But if Tononi is right, and these are all transformers
[02:22:13.000 --> 02:22:19.260]   that are not in the sense of Hollywood,
[02:22:19.260 --> 02:22:21.920]   but in the sense of these one-way direction
[02:22:21.920 --> 02:22:24.880]   neural networks, so they're all the zombies.
[02:22:24.880 --> 02:22:26.680]   That's the ultimate zombie apocalypse now.
[02:22:26.680 --> 02:22:28.320]   We have this universe that goes on
[02:22:28.320 --> 02:22:30.440]   with great construction projects and stuff,
[02:22:30.440 --> 02:22:32.580]   but there's no one experiencing anything.
[02:22:32.580 --> 02:22:37.240]   That would be like the ultimate depressing future.
[02:22:37.240 --> 02:22:40.920]   So I actually think as we move forward
[02:22:40.920 --> 02:22:42.920]   to building more advanced AI,
[02:22:42.920 --> 02:22:44.880]   we should do more research on figuring out
[02:22:44.880 --> 02:22:47.220]   what kind of information processing actually has experience,
[02:22:47.220 --> 02:22:49.620]   because I think that's what it's all about.
[02:22:49.620 --> 02:22:54.280]   And I completely don't buy the dismissal that some people,
[02:22:54.280 --> 02:22:56.480]   some people will say, well, this is all bullshit
[02:22:56.480 --> 02:22:58.600]   because consciousness equals intelligence.
[02:22:58.600 --> 02:23:01.280]   That's obviously not true.
[02:23:01.280 --> 02:23:03.160]   You can have a lot of conscious experience
[02:23:03.160 --> 02:23:06.160]   when you're not really accomplishing any goals at all,
[02:23:06.160 --> 02:23:08.800]   you're just reflecting on something.
[02:23:08.800 --> 02:23:12.840]   And you can sometimes have things,
[02:23:12.840 --> 02:23:14.160]   doing things that require intelligence
[02:23:14.160 --> 02:23:16.160]   probably without being conscious.
[02:23:16.160 --> 02:23:18.900]   - But I also worry that we humans won't,
[02:23:18.900 --> 02:23:22.560]   will discriminate against AI systems
[02:23:22.560 --> 02:23:24.800]   that clearly exhibit consciousness,
[02:23:24.800 --> 02:23:29.120]   that we will not allow AI systems to have consciousness.
[02:23:29.120 --> 02:23:32.600]   We'll come up with theories about measuring consciousness
[02:23:32.600 --> 02:23:35.200]   that will say this is a lesser being.
[02:23:35.200 --> 02:23:37.120]   And this is why I worry about that,
[02:23:37.120 --> 02:23:40.640]   because maybe we humans will create something
[02:23:40.640 --> 02:23:43.800]   that is better than us humans
[02:23:43.800 --> 02:23:47.080]   in the way that we find beautiful,
[02:23:47.080 --> 02:23:51.320]   which is they have a deeper subjective experience
[02:23:51.320 --> 02:23:52.220]   of reality.
[02:23:52.220 --> 02:23:55.600]   Not only are they smarter, but they feel deeper.
[02:23:55.600 --> 02:23:58.580]   And we humans will hate them for it.
[02:23:58.580 --> 02:24:02.100]   As human history has shown,
[02:24:02.100 --> 02:24:04.800]   they'll be the other, we'll try to suppress it,
[02:24:04.800 --> 02:24:07.680]   they'll create conflict, they'll create war, all of this.
[02:24:07.680 --> 02:24:09.320]   I worry about this too.
[02:24:09.320 --> 02:24:11.400]   - Are you saying that we humans sometimes
[02:24:11.400 --> 02:24:13.520]   come up with self-serving arguments?
[02:24:13.520 --> 02:24:15.640]   No, we would never do that, would we?
[02:24:15.640 --> 02:24:16.960]   - Well, that's the danger here,
[02:24:16.960 --> 02:24:19.480]   is even in this early stages,
[02:24:19.480 --> 02:24:21.520]   we might create something beautiful
[02:24:21.520 --> 02:24:24.960]   and we'll erase its memory.
[02:24:24.960 --> 02:24:28.440]   - I was horrified as a kid
[02:24:28.440 --> 02:24:33.280]   when someone started boiling lobsters.
[02:24:33.280 --> 02:24:36.000]   I'm like, "Oh my God, that's so cruel."
[02:24:36.000 --> 02:24:38.520]   And some grownup there back in Sweden said,
[02:24:38.520 --> 02:24:40.000]   "Oh, it doesn't feel pain."
[02:24:40.000 --> 02:24:41.400]   I'm like, "How do you know that?"
[02:24:41.400 --> 02:24:43.160]   "Oh, scientists have shown that."
[02:24:43.160 --> 02:24:46.200]   And then there was a recent study
[02:24:46.200 --> 02:24:48.480]   where they show that lobsters actually do feel pain
[02:24:48.480 --> 02:24:49.580]   when you boil them.
[02:24:49.580 --> 02:24:51.880]   So they banned lobster boiling in Switzerland now,
[02:24:51.880 --> 02:24:54.480]   to kill them in a different way first.
[02:24:54.480 --> 02:24:56.600]   Presumably, that scientific research
[02:24:56.600 --> 02:24:58.520]   boiled down to someone asked the lobster,
[02:24:58.520 --> 02:24:59.520]   "Does this hurt?"
[02:24:59.520 --> 02:25:00.680]   (both laugh)
[02:25:00.680 --> 02:25:01.520]   - Survey, self-report.
[02:25:01.800 --> 02:25:03.880]   - We do the same thing with cruelty to farm animals,
[02:25:03.880 --> 02:25:07.640]   also all these self-serving arguments for why they're fine.
[02:25:07.640 --> 02:25:10.160]   Yeah, so we should certainly be watchful.
[02:25:10.160 --> 02:25:12.000]   I think step one is just be humble
[02:25:12.000 --> 02:25:13.800]   and acknowledge that consciousness
[02:25:13.800 --> 02:25:16.040]   is not the same thing as intelligence.
[02:25:16.040 --> 02:25:18.600]   And I believe that consciousness still is
[02:25:18.600 --> 02:25:20.160]   a form of information processing
[02:25:20.160 --> 02:25:22.320]   where it's really information being aware of itself
[02:25:22.320 --> 02:25:23.160]   in a certain way.
[02:25:23.160 --> 02:25:26.040]   And let's study it and give ourselves a little bit of time.
[02:25:26.040 --> 02:25:28.200]   And I think we will be able to figure out
[02:25:28.200 --> 02:25:31.240]   actually what it is that causes consciousness.
[02:25:31.240 --> 02:25:34.560]   Then we can make probably unconscious robots
[02:25:34.560 --> 02:25:37.600]   that do the boring jobs that we would feel immoral
[02:25:37.600 --> 02:25:38.440]   to give to machines.
[02:25:38.440 --> 02:25:42.080]   But if you have a companion robot taking care of your mom
[02:25:42.080 --> 02:25:44.160]   or something like that,
[02:25:44.160 --> 02:25:45.760]   she would probably want it to be conscious, right?
[02:25:45.760 --> 02:25:49.640]   So that the emotions it seems to display aren't fake.
[02:25:49.640 --> 02:25:53.720]   All these things can be done in a good way
[02:25:53.720 --> 02:25:55.720]   if we give ourselves a little bit of time
[02:25:55.720 --> 02:25:59.400]   and don't run and take on this challenge.
[02:25:59.400 --> 02:26:02.000]   - Is there something you could say to the timeline
[02:26:02.000 --> 02:26:05.920]   that you think about, about the development of AGI?
[02:26:05.920 --> 02:26:09.160]   Depending on the day, I'm sure that changes for you.
[02:26:09.160 --> 02:26:13.560]   But when do you think there'll be a really big leap
[02:26:13.560 --> 02:26:16.400]   in intelligence where you would definitively say
[02:26:16.400 --> 02:26:17.920]   we have built AGI?
[02:26:17.920 --> 02:26:19.400]   Do you think it's one year from now,
[02:26:19.400 --> 02:26:23.160]   five years from now, 10, 20, 50?
[02:26:23.160 --> 02:26:24.280]   What's your gut say?
[02:26:27.680 --> 02:26:32.520]   - Honestly, for the past decade,
[02:26:32.520 --> 02:26:34.720]   I've deliberately given very long timelines
[02:26:34.720 --> 02:26:35.760]   just because I didn't want to fuel
[02:26:35.760 --> 02:26:37.760]   some kind of stupid Moloch race.
[02:26:37.760 --> 02:26:42.000]   But I think that cat has really left the bag now.
[02:26:42.000 --> 02:26:46.600]   I think we might be very, very close.
[02:26:46.600 --> 02:26:50.640]   I don't think the Microsoft paper is totally off
[02:26:50.640 --> 02:26:54.960]   when they say that there are some glimmers of AGI.
[02:26:54.960 --> 02:26:56.800]   It's not AGI yet.
[02:26:56.800 --> 02:26:57.720]   It's not an agent.
[02:26:57.720 --> 02:26:59.760]   There's a lot of things it can't do.
[02:26:59.760 --> 02:27:03.800]   But I wouldn't bet very strongly
[02:27:03.800 --> 02:27:07.160]   against it happening very soon.
[02:27:07.160 --> 02:27:09.720]   That's why we decided to do this open letter
[02:27:09.720 --> 02:27:14.280]   because if there's ever been a time to pause, it's today.
[02:27:14.280 --> 02:27:19.360]   - There's a feeling like this GPT-4 is a big transition
[02:27:19.360 --> 02:27:23.560]   into waking everybody up to the effectiveness
[02:27:23.560 --> 02:27:24.400]   of these systems.
[02:27:24.400 --> 02:27:28.520]   And so the next version will be big.
[02:27:28.520 --> 02:27:31.440]   - Yeah, and if that next one isn't AGI,
[02:27:31.440 --> 02:27:33.040]   maybe the next next one will.
[02:27:33.040 --> 02:27:35.440]   And there are many companies trying to do these things.
[02:27:35.440 --> 02:27:37.840]   And the basic architecture of them
[02:27:37.840 --> 02:27:39.840]   is not some sort of super well-kept secret.
[02:27:39.840 --> 02:27:43.000]   So this is a time to,
[02:27:43.000 --> 02:27:45.960]   a lot of people have said for many years
[02:27:45.960 --> 02:27:46.920]   that there will come a time
[02:27:46.920 --> 02:27:48.680]   when we want to pause a little bit.
[02:27:48.680 --> 02:27:51.440]   That time is now.
[02:27:54.280 --> 02:27:58.920]   - You have spoken about and thought about nuclear war a lot
[02:27:58.920 --> 02:28:01.480]   over the past year.
[02:28:01.480 --> 02:28:06.480]   We've seemingly have come closest
[02:28:06.480 --> 02:28:09.520]   to the precipice of nuclear war,
[02:28:09.520 --> 02:28:11.540]   then at least in my lifetime.
[02:28:11.540 --> 02:28:13.700]   - Yeah.
[02:28:13.700 --> 02:28:15.880]   - What do you learn about human nature from that?
[02:28:15.880 --> 02:28:19.240]   - It's our old friend Moloch again.
[02:28:19.240 --> 02:28:22.480]   It's really scary to see it where
[02:28:23.480 --> 02:28:26.800]   America doesn't want there to be a nuclear war.
[02:28:26.800 --> 02:28:30.000]   Russia doesn't want there to be a global nuclear war either.
[02:28:30.000 --> 02:28:32.040]   We both know that it's just being others.
[02:28:32.040 --> 02:28:33.480]   If we just try to do it,
[02:28:33.480 --> 02:28:35.760]   both sides try to launch first,
[02:28:35.760 --> 02:28:37.640]   it's just another suicide race, right?
[02:28:37.640 --> 02:28:40.240]   So why is it the way you said
[02:28:40.240 --> 02:28:43.240]   that this is the closest we've come since 1962?
[02:28:43.240 --> 02:28:44.720]   In fact, I think we've come closer now
[02:28:44.720 --> 02:28:47.000]   than even the Cuban Missile Crisis.
[02:28:47.000 --> 02:28:48.080]   It's 'cause of Moloch.
[02:28:48.080 --> 02:28:50.760]   You have these other forces.
[02:28:51.640 --> 02:28:54.920]   On one hand, you have the West
[02:28:54.920 --> 02:28:59.920]   saying that we have to drive Russia out of Ukraine.
[02:28:59.920 --> 02:29:01.080]   It's a matter of pride.
[02:29:01.080 --> 02:29:04.640]   We've staked so much on it
[02:29:04.640 --> 02:29:08.360]   that it would be seen as a huge loss
[02:29:08.360 --> 02:29:10.080]   of the credibility of the West
[02:29:10.080 --> 02:29:12.880]   if we don't drive Russia out entirely of the Ukraine.
[02:29:12.880 --> 02:29:17.320]   And on the other hand, you have Russia
[02:29:20.440 --> 02:29:22.400]   and you have the Russian leadership
[02:29:22.400 --> 02:29:24.840]   who knows that if they get completely driven
[02:29:24.840 --> 02:29:29.840]   out of Ukraine, it's not just gonna be very humiliating
[02:29:29.840 --> 02:29:32.440]   for them, but they might,
[02:29:32.440 --> 02:29:36.400]   it often happens when countries lose wars
[02:29:36.400 --> 02:29:39.640]   that things don't go so well for their leadership either.
[02:29:39.640 --> 02:29:42.480]   You remember when Argentina invaded the Falkland Islands?
[02:29:42.480 --> 02:29:48.240]   The military junta ordered that, right?
[02:29:48.240 --> 02:29:50.400]   People were cheering on the streets at first
[02:29:50.400 --> 02:29:51.840]   when they took it.
[02:29:51.840 --> 02:29:56.680]   And then when they got their butt kicked by the British,
[02:29:56.680 --> 02:29:58.520]   you know what happened to those guys?
[02:29:58.520 --> 02:30:01.320]   They were out.
[02:30:01.320 --> 02:30:04.600]   And I believe those who are still alive are in jail now.
[02:30:04.600 --> 02:30:09.320]   So the Russian leadership is entirely cornered
[02:30:09.320 --> 02:30:14.320]   where they know that just getting driven out of Ukraine
[02:30:14.320 --> 02:30:15.600]   is not an option.
[02:30:17.160 --> 02:30:22.160]   And so this to me is a typical example of Moloch.
[02:30:22.160 --> 02:30:27.040]   You have these incentives of the two parties
[02:30:27.040 --> 02:30:29.600]   where both of them are just driven
[02:30:29.600 --> 02:30:30.920]   to escalate more and more, right?
[02:30:30.920 --> 02:30:33.960]   If Russia starts losing in the conventional warfare,
[02:30:33.960 --> 02:30:36.480]   the only thing they can do
[02:30:36.480 --> 02:30:39.280]   since they're back against the war is to keep escalating.
[02:30:39.280 --> 02:30:43.040]   And the West has put itself in the situation now
[02:30:43.040 --> 02:30:45.520]   where we've sort of already committed to drive Russia out.
[02:30:45.520 --> 02:30:48.560]   So the only option the West has is to call Russia's bluff
[02:30:48.560 --> 02:30:50.200]   and keep sending in more weapons.
[02:30:50.200 --> 02:30:52.160]   This really bothers me
[02:30:52.160 --> 02:30:55.480]   because Moloch can sometimes drive competing parties
[02:30:55.480 --> 02:30:57.840]   to do something which is ultimately just really bad
[02:30:57.840 --> 02:30:58.880]   for both of them.
[02:30:58.880 --> 02:31:02.720]   And what makes me even more worried
[02:31:02.720 --> 02:31:07.720]   is not just that it's difficult to see an ending,
[02:31:07.720 --> 02:31:12.320]   a quick, peaceful ending to this tragedy
[02:31:12.320 --> 02:31:15.480]   that doesn't involve some horrible escalation,
[02:31:15.480 --> 02:31:19.000]   but also that we understand more clearly now
[02:31:19.000 --> 02:31:21.640]   just how horrible it would be.
[02:31:21.640 --> 02:31:23.960]   There was an amazing paper that was published
[02:31:23.960 --> 02:31:27.080]   in Nature Food this August
[02:31:27.080 --> 02:31:30.480]   by some of the top researchers
[02:31:30.480 --> 02:31:31.960]   who've been studying nuclear winter for a long time.
[02:31:31.960 --> 02:31:36.960]   And what they basically did was they combined climate models
[02:31:38.120 --> 02:31:42.800]   with food agricultural models.
[02:31:42.800 --> 02:31:45.280]   So instead of just saying, yeah, it gets really cold,
[02:31:45.280 --> 02:31:46.700]   blah, blah, blah, they figured out actually
[02:31:46.700 --> 02:31:49.160]   how many people would die in different countries.
[02:31:49.160 --> 02:31:52.280]   And it's pretty mind-blowing.
[02:31:52.280 --> 02:31:54.480]   So basically what happens is the thing that kills
[02:31:54.480 --> 02:31:56.000]   the most people is not the explosions,
[02:31:56.000 --> 02:31:59.800]   it's not the radioactivity, it's not the EMP mayhem,
[02:31:59.800 --> 02:32:04.120]   it's not the rampaging mobs foraging for food.
[02:32:04.120 --> 02:32:06.680]   No, it's the fact that you get so much smoke
[02:32:06.680 --> 02:32:09.720]   coming up from the burning cities and stratosphere
[02:32:09.720 --> 02:32:14.720]   that spreads around the earth from the jet streams.
[02:32:14.720 --> 02:32:19.000]   So in typical models, you get like 10 years or so
[02:32:19.000 --> 02:32:20.440]   where it's just crazy cold.
[02:32:20.440 --> 02:32:25.960]   During the first year after the war,
[02:32:25.960 --> 02:32:30.960]   and their models, the temperature drops in Nebraska
[02:32:30.960 --> 02:32:35.440]   and in the Ukraine breadbaskets by like 20,
[02:32:36.560 --> 02:32:38.920]   Celsius or so if I remember.
[02:32:38.920 --> 02:32:42.600]   No, yeah, 20, 30 Celsius, depending on where you are,
[02:32:42.600 --> 02:32:46.160]   40 Celsius in some places, which is 40 Fahrenheit
[02:32:46.160 --> 02:32:48.840]   to 80 Fahrenheit colder than what it would normally be.
[02:32:48.840 --> 02:32:53.840]   So I'm not good at farming, but if it's snowing,
[02:32:53.840 --> 02:32:58.080]   if it drops below freezing pretty much most days in July,
[02:32:58.080 --> 02:32:59.160]   and then that's not good.
[02:32:59.160 --> 02:33:02.080]   So they worked out, they put this into their farming models.
[02:33:02.080 --> 02:33:04.120]   And what they found was really interesting.
[02:33:04.120 --> 02:33:06.080]   The countries that get the most hard hit
[02:33:06.080 --> 02:33:08.080]   are the ones in the Northern hemisphere.
[02:33:08.080 --> 02:33:12.680]   So in the US and one model,
[02:33:12.680 --> 02:33:16.360]   they had about 99% of all Americans starving to death.
[02:33:16.360 --> 02:33:18.160]   In Russia and China and Europe,
[02:33:18.160 --> 02:33:21.120]   also about 99%, 98% starving to death.
[02:33:21.120 --> 02:33:24.760]   So you might be like, oh, it's kind of poetic justice
[02:33:24.760 --> 02:33:28.040]   that both the Russians and the Americans,
[02:33:28.040 --> 02:33:29.720]   99% of them have to pay for it
[02:33:29.720 --> 02:33:31.360]   'cause it was their bombs that did it.
[02:33:31.360 --> 02:33:35.280]   But that doesn't particularly cheer people up in Sweden
[02:33:35.280 --> 02:33:37.280]   or other random countries
[02:33:37.280 --> 02:33:38.920]   that have nothing to do with it.
[02:33:38.920 --> 02:33:45.880]   I think it hasn't entered the mainstream,
[02:33:45.880 --> 02:33:53.200]   not understanding very much just like how bad this is.
[02:33:53.200 --> 02:33:55.720]   Most people, especially a lot of people
[02:33:55.720 --> 02:33:58.040]   in decision-making positions still think of nuclear weapons
[02:33:58.040 --> 02:33:59.880]   as something that makes you powerful.
[02:33:59.880 --> 02:34:05.000]   Scary, powerful, they don't think of it as something
[02:34:05.000 --> 02:34:09.760]   where, yeah, just to within a percent or two,
[02:34:09.760 --> 02:34:11.880]   we're all just gonna starve to death.
[02:34:11.880 --> 02:34:17.040]   - And starving to death is the worst way to die,
[02:34:17.040 --> 02:34:25.120]   as a lot of more, as all the famines in history show,
[02:34:25.120 --> 02:34:27.160]   the torture involved in that.
[02:34:27.160 --> 02:34:29.520]   - Probably brings out the worst in people also,
[02:34:29.520 --> 02:34:32.760]   when people are desperate like this.
[02:34:34.240 --> 02:34:37.120]   I've heard some people say that
[02:34:37.120 --> 02:34:39.240]   if that's what's gonna happen,
[02:34:39.240 --> 02:34:42.000]   they'd rather be at ground zero and just get vaporized.
[02:34:42.000 --> 02:34:49.760]   But I think people underestimate the risk of this
[02:34:49.760 --> 02:34:53.400]   because they aren't afraid of Moloch.
[02:34:53.400 --> 02:34:54.800]   They think, oh, it's just gonna be,
[02:34:54.800 --> 02:34:56.600]   'cause humans don't want this, so it's not gonna happen.
[02:34:56.600 --> 02:34:58.080]   That's the whole point of Moloch,
[02:34:58.080 --> 02:35:00.360]   that things happen that nobody wanted.
[02:35:00.360 --> 02:35:02.440]   - And that applies to nuclear weapons,
[02:35:02.440 --> 02:35:04.360]   and that applies to AGI.
[02:35:04.360 --> 02:35:09.000]   - Exactly, and it applies to some of the things
[02:35:09.000 --> 02:35:12.440]   that people have gotten most upset with capitalism for also,
[02:35:12.440 --> 02:35:14.920]   where everybody was just kind of trapped.
[02:35:14.920 --> 02:35:18.640]   It's not to see if some company does something
[02:35:18.640 --> 02:35:23.400]   that causes a lot of harm.
[02:35:23.400 --> 02:35:25.560]   Not that the CEO is a bad person,
[02:35:25.560 --> 02:35:29.160]   but she or he knew that all the other companies
[02:35:29.160 --> 02:35:30.000]   were doing this too.
[02:35:30.000 --> 02:35:32.480]   So Moloch is a formidable foe.
[02:35:32.480 --> 02:35:40.320]   I hope someone makes a good movie
[02:35:40.320 --> 02:35:42.160]   so we can see who the real enemy is.
[02:35:42.160 --> 02:35:45.680]   We're not fighting against each other.
[02:35:45.680 --> 02:35:48.400]   Moloch makes us fight against each other.
[02:35:48.400 --> 02:35:50.720]   That's what Moloch's superpower is.
[02:35:50.720 --> 02:35:55.440]   The hope here is any kind of technology
[02:35:55.440 --> 02:35:58.440]   or other mechanism that lets us instead realize
[02:35:59.560 --> 02:36:01.400]   that we're fighting the wrong enemy.
[02:36:01.400 --> 02:36:04.120]   - It's such a fascinating battle.
[02:36:04.120 --> 02:36:06.480]   - It's not us versus them, it's us versus it.
[02:36:06.480 --> 02:36:11.840]   - We are fighting, Moloch, for human survival.
[02:36:11.840 --> 02:36:13.000]   We as a civilization.
[02:36:13.000 --> 02:36:16.320]   - Have you seen the movie "Needful Things"?
[02:36:16.320 --> 02:36:17.960]   It's a Stephen King novel.
[02:36:17.960 --> 02:36:21.040]   I love Stephen King and Max von Sydow,
[02:36:21.040 --> 02:36:23.840]   Swedish actor, is playing the guy.
[02:36:23.840 --> 02:36:25.000]   It's brilliant.
[02:36:25.000 --> 02:36:27.540]   I just hadn't thought about that until now,
[02:36:27.540 --> 02:36:31.840]   but that's the closest I've seen to a movie about Moloch.
[02:36:31.840 --> 02:36:33.280]   I don't want to spoil the film for anyone
[02:36:33.280 --> 02:36:36.120]   who wants to watch it, but basically,
[02:36:36.120 --> 02:36:39.560]   it's about this guy who turns out to,
[02:36:39.560 --> 02:36:41.600]   you can interpret him as the devil or whatever,
[02:36:41.600 --> 02:36:44.200]   but he doesn't actually ever go around and kill people
[02:36:44.200 --> 02:36:47.120]   or torture people with burning coal or anything.
[02:36:47.120 --> 02:36:49.200]   He makes everybody fight each other,
[02:36:49.200 --> 02:36:51.160]   makes everybody fear each other, hate each other,
[02:36:51.160 --> 02:36:53.120]   and then kill each other.
[02:36:53.120 --> 02:36:56.400]   So that's the movie about Moloch.
[02:36:56.400 --> 02:36:57.460]   - Love is the answer.
[02:36:57.460 --> 02:37:02.460]   That seems to be one of the ways to fight Moloch
[02:37:02.460 --> 02:37:08.140]   is by compassion, by seeing the common humanity.
[02:37:08.140 --> 02:37:09.980]   - Yes, yes.
[02:37:09.980 --> 02:37:12.300]   And to not sound, so we don't sound like,
[02:37:12.300 --> 02:37:15.380]   like what's it, Kumbaya tree huggers here, right?
[02:37:15.380 --> 02:37:16.780]   (Lex laughing)
[02:37:16.780 --> 02:37:19.540]   We're not just saying love and peace, man.
[02:37:19.540 --> 02:37:21.860]   We're trying to actually help people
[02:37:21.860 --> 02:37:25.380]   understand the true facts about the other side.
[02:37:26.360 --> 02:37:31.360]   And feel the compassion because the truth
[02:37:31.360 --> 02:37:35.940]   makes you more compassionate, right?
[02:37:35.940 --> 02:37:42.080]   So that's why I really like using AI
[02:37:42.080 --> 02:37:46.300]   for truth-seeking technologies that can,
[02:37:46.300 --> 02:37:53.400]   as a result, get us more love than hate.
[02:37:53.400 --> 02:37:56.680]   And even if you can't get love,
[02:37:56.680 --> 02:37:59.800]   settle for some understanding,
[02:37:59.800 --> 02:38:01.240]   which already gives compassion.
[02:38:01.240 --> 02:38:06.120]   If someone is like, "I really disagree with you, Lex,
[02:38:06.120 --> 02:38:07.860]   "but I can see where you're coming from.
[02:38:07.860 --> 02:38:12.700]   "You're not a bad person who needs to be destroyed,
[02:38:12.700 --> 02:38:13.560]   "but I disagree with you,
[02:38:13.560 --> 02:38:15.920]   "and I'm happy to have an argument about it."
[02:38:15.920 --> 02:38:18.760]   That's a lot of progress compared to where we are
[02:38:18.760 --> 02:38:22.120]   at 2023 in the public space, wouldn't you say?
[02:38:22.120 --> 02:38:26.560]   - If we solve the AI safety problem, as we've talked about,
[02:38:26.560 --> 02:38:31.040]   and then you, Max Tegmark, who has been talking about this
[02:38:31.040 --> 02:38:35.120]   for many years, get to sit down with the AGI,
[02:38:35.120 --> 02:38:38.220]   with the early AGI system, on a beach with a drink,
[02:38:38.220 --> 02:38:41.760]   what kind of, what would you ask her?
[02:38:41.760 --> 02:38:42.760]   What kind of question would you ask?
[02:38:42.760 --> 02:38:44.060]   What would you talk about?
[02:38:44.060 --> 02:38:47.600]   Something so much smarter than you.
[02:38:47.600 --> 02:38:49.560]   Would you be afraid--
[02:38:49.560 --> 02:38:50.720]   - I knew you were gonna get me
[02:38:50.720 --> 02:38:53.720]   with a really zinger of a question.
[02:38:53.720 --> 02:38:54.560]   That's a good one.
[02:38:54.560 --> 02:38:58.360]   - Would you be afraid to ask some questions?
[02:38:58.360 --> 02:38:59.680]   - No.
[02:38:59.680 --> 02:39:01.040]   I'm not afraid of the truth.
[02:39:01.040 --> 02:39:01.880]   (laughing)
[02:39:01.880 --> 02:39:02.760]   I'm very humble.
[02:39:02.760 --> 02:39:05.440]   I know I'm just a meat bag with all these flaws,
[02:39:05.440 --> 02:39:09.920]   but I have, we talked a lot about homo sentiens.
[02:39:09.920 --> 02:39:12.920]   I've already tried that for a long time with myself.
[02:39:12.920 --> 02:39:16.400]   So that is what's really valuable about being alive for me,
[02:39:16.400 --> 02:39:19.760]   is that I have these meaningful experiences.
[02:39:19.760 --> 02:39:24.400]   It's not that I'm good at this or good at that or whatever.
[02:39:24.400 --> 02:39:25.720]   There's so much I suck at.
[02:39:25.720 --> 02:39:28.480]   - So you're not afraid for the system
[02:39:28.480 --> 02:39:29.920]   to show you just how dumb you are?
[02:39:29.920 --> 02:39:30.760]   - No, no.
[02:39:30.760 --> 02:39:34.160]   In fact, my son reminds me of that pretty frequently.
[02:39:34.160 --> 02:39:36.440]   - You could find out how dumb you are in terms of physics,
[02:39:36.440 --> 02:39:38.720]   how little we humans understand.
[02:39:38.720 --> 02:39:40.200]   - I'm cool with that.
[02:39:40.200 --> 02:39:45.200]   I think, so I can't waffle my way out of this question.
[02:39:45.200 --> 02:39:47.760]   It's a fair one.
[02:39:49.280 --> 02:39:52.480]   I think, given that I'm a really, really curious person,
[02:39:52.480 --> 02:39:57.240]   that's really the defining part of who I am.
[02:39:57.240 --> 02:39:58.440]   I'm so curious.
[02:39:58.440 --> 02:40:05.520]   I have some physics questions.
[02:40:05.520 --> 02:40:06.720]   (laughing)
[02:40:06.720 --> 02:40:09.800]   I love to understand.
[02:40:09.800 --> 02:40:12.240]   I have some questions about consciousness,
[02:40:12.240 --> 02:40:13.360]   about the nature of reality.
[02:40:13.360 --> 02:40:15.960]   I would just really, really love to understand also.
[02:40:15.960 --> 02:40:18.720]   I could tell you one, for example,
[02:40:18.720 --> 02:40:21.000]   that I've been obsessing about a lot recently.
[02:40:21.000 --> 02:40:27.720]   So I believe that, so suppose Tononi is right.
[02:40:27.720 --> 02:40:30.720]   And suppose there are some information processing systems
[02:40:30.720 --> 02:40:32.560]   that are conscious and some that are not.
[02:40:32.560 --> 02:40:34.480]   Suppose you can even make reasonably smart things
[02:40:34.480 --> 02:40:36.400]   like GPT-4 that are not conscious,
[02:40:36.400 --> 02:40:38.840]   but you can also make them conscious.
[02:40:38.840 --> 02:40:41.280]   Here's the question that keeps me awake at night.
[02:40:41.280 --> 02:40:47.800]   Is it the case that the unconscious zombie systems
[02:40:47.800 --> 02:40:50.040]   that are really intelligent are also really efficient?
[02:40:50.040 --> 02:40:51.600]   So they're really inefficient?
[02:40:51.600 --> 02:40:54.960]   So that when you try to make things more efficient,
[02:40:54.960 --> 02:40:57.200]   which there'll naturally be a pressure to do,
[02:40:57.200 --> 02:40:59.120]   they become conscious.
[02:40:59.120 --> 02:41:02.480]   I'm kind of hoping that that's correct.
[02:41:02.480 --> 02:41:05.480]   And do you want me to give you a hand wavey argument for it?
[02:41:05.480 --> 02:41:11.160]   In my lab, again, every time we look at
[02:41:11.160 --> 02:41:13.680]   how these large language models do something,
[02:41:13.680 --> 02:41:15.200]   we see that they do it in really dumb ways
[02:41:15.200 --> 02:41:17.640]   and you could make it better.
[02:41:17.640 --> 02:41:22.640]   If you, we have loops in our computer language for a reason.
[02:41:22.640 --> 02:41:25.640]   The code would get way, way longer
[02:41:25.640 --> 02:41:27.400]   if you weren't allowed to use them.
[02:41:27.400 --> 02:41:29.840]   It's more efficient to have the loops.
[02:41:29.840 --> 02:41:34.240]   And in order to have self-reflection,
[02:41:34.240 --> 02:41:37.000]   whether it's conscious or not,
[02:41:37.000 --> 02:41:39.560]   even an operating system knows things about itself.
[02:41:39.560 --> 02:41:43.040]   You need to have loops already.
[02:41:44.080 --> 02:41:48.360]   So I think, I'm waving my hands a lot,
[02:41:48.360 --> 02:41:53.240]   but I suspect that the most efficient way
[02:41:53.240 --> 02:41:55.840]   of implementing a given level of intelligence
[02:41:55.840 --> 02:42:01.840]   has loops in it, self-reflection, and will be conscious.
[02:42:01.840 --> 02:42:04.240]   - Isn't that great news?
[02:42:04.240 --> 02:42:06.080]   - Yes, if it's true, it's wonderful.
[02:42:06.080 --> 02:42:07.920]   'Cause then we don't have to fear
[02:42:07.920 --> 02:42:09.560]   the ultimate zombie apocalypse.
[02:42:09.560 --> 02:42:12.160]   And I think if you look at our brains, actually,
[02:42:12.160 --> 02:42:16.920]   our brains are part zombie and part conscious.
[02:42:16.920 --> 02:42:24.960]   When I open my eyes, I immediately take all these pixels
[02:42:24.960 --> 02:42:27.800]   that hit my retina, right?
[02:42:27.800 --> 02:42:29.880]   And I'm like, "Oh, that's Lex."
[02:42:29.880 --> 02:42:32.800]   But I have no freaking clue of how I did that computation.
[02:42:32.800 --> 02:42:34.280]   It's actually quite complicated, right?
[02:42:34.280 --> 02:42:36.640]   It was only relatively recently
[02:42:36.640 --> 02:42:39.720]   we could even do it well with machines, right?
[02:42:39.720 --> 02:42:42.160]   You get a bunch of information processing happening
[02:42:42.160 --> 02:42:44.480]   in my retina, and then it goes to the lateral geniculate
[02:42:44.480 --> 02:42:48.520]   nucleus, my thalamus, and the area V1, V2, V4,
[02:42:48.520 --> 02:42:51.040]   and the fusiform face area here that Nancy Kenwisher
[02:42:51.040 --> 02:42:53.400]   at MIT invented, and blah, blah, blah, blah, blah.
[02:42:53.400 --> 02:42:56.320]   And I have no freaking clue how that worked, right?
[02:42:56.320 --> 02:42:59.520]   It feels to me subjectively like my conscious module
[02:42:59.520 --> 02:43:02.760]   just got a little email saying,
[02:43:05.480 --> 02:43:10.480]   "Facial processing task complete. It's Lex."
[02:43:10.480 --> 02:43:13.120]   - Yeah.
[02:43:13.120 --> 02:43:15.080]   - I'm gonna just go with that, right?
[02:43:15.080 --> 02:43:18.440]   So this fits perfectly with Tononi's model
[02:43:18.440 --> 02:43:23.060]   because this was all one-way information processing mainly.
[02:43:23.060 --> 02:43:28.160]   And it turned out for that particular task,
[02:43:28.160 --> 02:43:30.200]   that's all you needed, and it probably was
[02:43:30.200 --> 02:43:32.560]   kind of the most efficient way to do it.
[02:43:32.560 --> 02:43:34.080]   But there were a lot of other things
[02:43:34.080 --> 02:43:36.120]   that we associated with higher intelligence
[02:43:36.120 --> 02:43:38.000]   and planning and so on and so forth,
[02:43:38.000 --> 02:43:40.160]   where you kind of want to have loops
[02:43:40.160 --> 02:43:42.240]   and be able to ruminate and self-reflect
[02:43:42.240 --> 02:43:46.840]   and introspect and so on, where my hunch is
[02:43:46.840 --> 02:43:49.120]   that if you want to fake that with a zombie system
[02:43:49.120 --> 02:43:52.240]   that just all goes one way, you have to unroll those loops
[02:43:52.240 --> 02:43:53.360]   and it gets really, really long,
[02:43:53.360 --> 02:43:55.840]   and it's much more inefficient.
[02:43:55.840 --> 02:43:59.320]   So I'm actually hopeful that AI, if in the future
[02:43:59.320 --> 02:44:01.800]   we have all these very sublime and interesting machines
[02:44:01.800 --> 02:44:04.600]   that do cool things and are aligned with us,
[02:44:04.600 --> 02:44:08.680]   that they will also have consciousness
[02:44:08.680 --> 02:44:11.680]   for the kind of these things that we do.
[02:44:11.680 --> 02:44:14.520]   - That great intelligence is also correlated
[02:44:14.520 --> 02:44:18.480]   to great consciousness or a deep kind of consciousness.
[02:44:18.480 --> 02:44:20.000]   - Yes.
[02:44:20.000 --> 02:44:21.760]   So that's a happy thought for me
[02:44:21.760 --> 02:44:24.760]   'cause the zombie apocalypse really is my worst nightmare
[02:44:24.760 --> 02:44:27.160]   of all, to be like adding insult to injury.
[02:44:27.160 --> 02:44:29.040]   Not only did we get replaced,
[02:44:29.040 --> 02:44:32.240]   but we frigging replaced ourselves by zombies.
[02:44:32.240 --> 02:44:34.120]   How dumb can we be?
[02:44:34.120 --> 02:44:35.480]   - That's such a beautiful vision,
[02:44:35.480 --> 02:44:37.080]   and that's actually a provable one.
[02:44:37.080 --> 02:44:40.200]   That's one that we humans can intuit and prove
[02:44:40.200 --> 02:44:42.520]   that those two things are correlated
[02:44:42.520 --> 02:44:45.240]   as we start to understand what it means to be intelligent
[02:44:45.240 --> 02:44:46.880]   and what it means to be conscious,
[02:44:46.880 --> 02:44:51.120]   which these systems, early AGI-like systems
[02:44:51.120 --> 02:44:52.400]   will help us understand.
[02:44:52.400 --> 02:44:53.760]   - And I just want to say one more thing,
[02:44:53.760 --> 02:44:55.200]   which is super important.
[02:44:55.200 --> 02:44:57.440]   Most of my colleagues, when I started going on
[02:44:57.440 --> 02:44:59.360]   about consciousness, tell me that it's all bullshit
[02:44:59.360 --> 02:45:01.120]   and I should stop talking about it.
[02:45:01.120 --> 02:45:04.240]   I hear a little inner voice from my father
[02:45:04.240 --> 02:45:06.960]   and from my mom saying, "Keep talking about it,"
[02:45:06.960 --> 02:45:08.040]   'cause I think they're wrong.
[02:45:08.040 --> 02:45:13.040]   And the main way to convince people like that,
[02:45:13.040 --> 02:45:17.120]   that they're wrong, if they say that consciousness
[02:45:17.120 --> 02:45:19.560]   is just equal to intelligence, is to ask them,
[02:45:19.560 --> 02:45:21.280]   "What's wrong with torture?"
[02:45:21.280 --> 02:45:22.920]   Or, "Why are you against torture?"
[02:45:23.960 --> 02:45:28.920]   If it's just about these particles moving this way
[02:45:28.920 --> 02:45:31.680]   rather than that way, and there is no such thing
[02:45:31.680 --> 02:45:34.320]   as subjective experience, what's wrong with torture?
[02:45:34.320 --> 02:45:36.520]   Do you have a good comeback to that?
[02:45:36.520 --> 02:45:40.480]   - No, it seems like suffering imposed onto other humans
[02:45:40.480 --> 02:45:44.200]   is somehow deeply wrong in a way
[02:45:44.200 --> 02:45:46.120]   that intelligence doesn't quite explain.
[02:45:46.120 --> 02:45:50.720]   - And if someone tells me, "Well, it's just an illusion,
[02:45:50.720 --> 02:45:55.720]   "consciousness, whatever," I like to invite them
[02:45:55.720 --> 02:45:58.920]   the next time they're having surgery
[02:45:58.920 --> 02:46:00.360]   to do it without anesthesia.
[02:46:00.360 --> 02:46:03.760]   What is anesthesia really doing?
[02:46:03.760 --> 02:46:05.800]   If you have it, you can have it at local anesthesia
[02:46:05.800 --> 02:46:06.620]   when you're awake.
[02:46:06.620 --> 02:46:07.800]   I had that when they fixed my shoulder.
[02:46:07.800 --> 02:46:09.060]   It was super entertaining.
[02:46:09.060 --> 02:46:12.560]   What was it that it did?
[02:46:12.560 --> 02:46:15.560]   It just removed my subjective experience of pain.
[02:46:15.560 --> 02:46:17.760]   It didn't change anything about what was actually happening
[02:46:17.760 --> 02:46:18.860]   in my shoulder, right?
[02:46:20.120 --> 02:46:22.120]   So if someone says that's all bullshit,
[02:46:22.120 --> 02:46:24.960]   skip the anesthesia is my advice.
[02:46:24.960 --> 02:46:26.680]   This is incredibly central.
[02:46:26.680 --> 02:46:30.080]   - It could be fundamental to whatever this thing
[02:46:30.080 --> 02:46:31.320]   we have going on here.
[02:46:31.320 --> 02:46:36.080]   - It is fundamental because what we feel is so fundamental
[02:46:36.080 --> 02:46:41.080]   is suffering and joy and pleasure and meaning.
[02:46:41.080 --> 02:46:46.580]   That's all, those are all subjective experiences there.
[02:46:47.880 --> 02:46:50.160]   And let's not, those are the elephant in the room.
[02:46:50.160 --> 02:46:51.840]   That's what makes life worth living
[02:46:51.840 --> 02:46:53.040]   and that's what can make it horrible
[02:46:53.040 --> 02:46:54.420]   if it's just the way you're suffering.
[02:46:54.420 --> 02:46:56.640]   So let's not make the mistake of saying
[02:46:56.640 --> 02:46:58.400]   that that's all bullshit.
[02:46:58.400 --> 02:47:02.640]   - And let's not make the mistake of not instilling
[02:47:02.640 --> 02:47:07.640]   the AI systems with that same thing that makes us special.
[02:47:07.640 --> 02:47:09.600]   - Yeah.
[02:47:09.600 --> 02:47:12.800]   - Max, it's a huge honor that you will sit down to me
[02:47:12.800 --> 02:47:16.240]   the first time on the first episode of this podcast.
[02:47:16.240 --> 02:47:18.280]   It's a huge honor you sit down with me again
[02:47:18.280 --> 02:47:21.400]   and talk about this, what I think is the most important
[02:47:21.400 --> 02:47:25.080]   topic, the most important problem that we humans
[02:47:25.080 --> 02:47:28.740]   have to face and hopefully solve.
[02:47:28.740 --> 02:47:31.600]   - Yeah, well the honor is all mine and I'm so grateful
[02:47:31.600 --> 02:47:34.960]   to you for making more people aware of the fact
[02:47:34.960 --> 02:47:37.320]   that humanity has reached the most important fork
[02:47:37.320 --> 02:47:38.960]   in the road ever in its history
[02:47:38.960 --> 02:47:41.700]   and let's turn in the correct direction.
[02:47:41.700 --> 02:47:44.240]   - Thanks for listening to this conversation
[02:47:44.240 --> 02:47:45.440]   with Max Tegmark.
[02:47:45.440 --> 02:47:47.840]   To support this podcast, please check out our sponsors
[02:47:47.840 --> 02:47:49.440]   in the description.
[02:47:49.440 --> 02:47:51.440]   And now let me leave you with some words
[02:47:51.440 --> 02:47:52.880]   from Frank Herbert.
[02:47:52.880 --> 02:47:56.440]   History is a constant race
[02:47:56.440 --> 02:47:59.120]   between invention and catastrophe.
[02:47:59.120 --> 02:48:03.480]   Thank you for listening and hope to see you next time.
[02:48:03.480 --> 02:48:06.060]   (upbeat music)
[02:48:06.060 --> 02:48:08.640]   (upbeat music)
[02:48:08.640 --> 02:48:18.640]   [BLANK_AUDIO]

