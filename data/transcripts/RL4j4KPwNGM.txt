
[00:00:00.000 --> 00:00:02.840]   The following is a conversation with Max Tegmark,
[00:00:02.840 --> 00:00:04.760]   his second time on the podcast.
[00:00:04.760 --> 00:00:07.120]   In fact, the previous conversation
[00:00:07.120 --> 00:00:10.960]   was episode number one of this very podcast.
[00:00:10.960 --> 00:00:14.800]   He is a physicist and artificial intelligence researcher
[00:00:14.800 --> 00:00:18.840]   at MIT, co-founder of the Future of Life Institute,
[00:00:18.840 --> 00:00:21.360]   and author of "Life 3.0",
[00:00:21.360 --> 00:00:24.560]   Being Human in the Age of Artificial Intelligence.
[00:00:24.560 --> 00:00:26.560]   He's also the head of a bunch of other
[00:00:26.560 --> 00:00:28.440]   huge fascinating projects,
[00:00:28.440 --> 00:00:30.560]   and has written a lot of different things
[00:00:30.560 --> 00:00:32.120]   that you should definitely check out.
[00:00:32.120 --> 00:00:34.480]   He has been one of the key humans
[00:00:34.480 --> 00:00:36.400]   who has been outspoken about long-term
[00:00:36.400 --> 00:00:38.160]   existential risks of AI,
[00:00:38.160 --> 00:00:40.480]   and also its exciting possibilities
[00:00:40.480 --> 00:00:42.880]   and solutions to real world problems.
[00:00:42.880 --> 00:00:46.400]   Most recently at the intersection of AI and physics,
[00:00:46.400 --> 00:00:49.960]   and also in re-engineering the algorithms
[00:00:49.960 --> 00:00:53.160]   that divide us by controlling the information we see,
[00:00:53.160 --> 00:00:55.000]   and thereby creating bubbles
[00:00:55.000 --> 00:00:58.240]   and all other kinds of complex social phenomena
[00:00:58.240 --> 00:00:59.640]   that we see today.
[00:00:59.640 --> 00:01:01.440]   In general, he's one of the most passionate
[00:01:01.440 --> 00:01:04.360]   and brilliant people I have the fortune of knowing.
[00:01:04.360 --> 00:01:06.200]   I hope to talk to him many more times
[00:01:06.200 --> 00:01:08.280]   on this podcast in the future.
[00:01:08.280 --> 00:01:10.000]   Quick mention of our sponsors,
[00:01:10.000 --> 00:01:12.200]   the Jordan Harbinger Show,
[00:01:12.200 --> 00:01:14.360]   Four Sigmatic Mushroom Coffee,
[00:01:14.360 --> 00:01:16.200]   BetterHelp Online Therapy,
[00:01:16.200 --> 00:01:18.480]   and ExpressVPN.
[00:01:18.480 --> 00:01:23.480]   So the choices, wisdom, caffeine, sanity, or privacy.
[00:01:23.480 --> 00:01:25.000]   Choose wisely, my friends.
[00:01:25.000 --> 00:01:27.480]   And if you wish, click the sponsor links below
[00:01:27.480 --> 00:01:30.560]   to get a discount and to support this podcast.
[00:01:30.560 --> 00:01:32.160]   As a side note, let me say that
[00:01:32.160 --> 00:01:35.400]   much of the researchers in the machine learning
[00:01:35.400 --> 00:01:37.760]   and artificial intelligence communities
[00:01:37.760 --> 00:01:40.400]   do not spend much time thinking deeply
[00:01:40.400 --> 00:01:42.720]   about existential risks of AI.
[00:01:42.720 --> 00:01:46.160]   Because our current algorithms are seen as useful but dumb,
[00:01:46.160 --> 00:01:49.240]   it's difficult to imagine how they may become destructive
[00:01:49.240 --> 00:01:51.240]   to the fabric of human civilization
[00:01:51.240 --> 00:01:53.000]   in the foreseeable future.
[00:01:53.000 --> 00:01:56.120]   I understand this mindset, but it's very troublesome.
[00:01:56.120 --> 00:01:58.320]   To me, this is both a dangerous
[00:01:58.320 --> 00:02:00.480]   and uninspiring perspective,
[00:02:00.480 --> 00:02:03.960]   reminiscent of a lobster sitting in a pot of lukewarm water
[00:02:03.960 --> 00:02:06.160]   that a minute ago was cold.
[00:02:06.160 --> 00:02:08.640]   I feel a kinship with this lobster.
[00:02:08.640 --> 00:02:10.560]   I believe that already the algorithms
[00:02:10.560 --> 00:02:12.960]   that drive our interaction on social media
[00:02:12.960 --> 00:02:15.000]   have an intelligence and power
[00:02:15.000 --> 00:02:17.360]   that far outstrip the intelligence and power
[00:02:17.360 --> 00:02:19.200]   of any one human being.
[00:02:19.200 --> 00:02:21.640]   Now really is the time to think about this,
[00:02:21.640 --> 00:02:24.920]   to define the trajectory of the interplay of technology
[00:02:24.920 --> 00:02:26.960]   and human beings in our society.
[00:02:26.960 --> 00:02:29.680]   I think that the future of human civilization
[00:02:29.680 --> 00:02:32.840]   very well may be at stake over this very question
[00:02:32.840 --> 00:02:36.240]   of the role of artificial intelligence in our society.
[00:02:36.240 --> 00:02:38.160]   If you enjoy this thing, subscribe on YouTube,
[00:02:38.160 --> 00:02:40.960]   review it on Apple Podcasts, follow on Spotify,
[00:02:40.960 --> 00:02:43.840]   support on Patreon, or connect with me on Twitter,
[00:02:43.840 --> 00:02:45.260]   @lexfriedman.
[00:02:45.260 --> 00:02:48.840]   And now, here's my conversation with Max Tagmark.
[00:02:48.840 --> 00:02:51.440]   So people might not know this,
[00:02:51.440 --> 00:02:55.280]   but you were actually episode number one of this podcast
[00:02:55.280 --> 00:02:59.280]   just a couple of years ago, and now we're back.
[00:02:59.280 --> 00:03:01.800]   And it so happens that a lot of exciting things
[00:03:01.800 --> 00:03:05.600]   happened in both physics and artificial intelligence,
[00:03:05.600 --> 00:03:08.480]   both fields that you're super passionate about.
[00:03:08.480 --> 00:03:11.640]   Can we try to catch up to some of the exciting things
[00:03:11.640 --> 00:03:14.040]   happening in artificial intelligence,
[00:03:14.040 --> 00:03:17.740]   especially in the context of the way it's cracking open
[00:03:17.740 --> 00:03:20.020]   the different problems of the sciences?
[00:03:21.180 --> 00:03:25.760]   - Yeah, I'd love to, especially now as we start 2021 here.
[00:03:25.760 --> 00:03:27.400]   It's a really fun time to think about
[00:03:27.400 --> 00:03:30.840]   what were the biggest breakthroughs in AI?
[00:03:30.840 --> 00:03:33.040]   Not the ones necessarily that media wrote about,
[00:03:33.040 --> 00:03:34.440]   but that really matter.
[00:03:34.440 --> 00:03:37.000]   And what does that mean for our ability
[00:03:37.000 --> 00:03:38.640]   to do better science?
[00:03:38.640 --> 00:03:41.240]   What does it mean for our ability
[00:03:41.240 --> 00:03:44.320]   to help people around the world?
[00:03:44.320 --> 00:03:47.720]   And what does it mean for new problems
[00:03:47.720 --> 00:03:49.640]   that they could cause if we're not smart enough
[00:03:49.640 --> 00:03:50.480]   to avoid them?
[00:03:50.480 --> 00:03:53.260]   You know, what do we learn basically from this?
[00:03:53.260 --> 00:03:54.100]   - Yes, absolutely.
[00:03:54.100 --> 00:03:56.220]   So one of the amazing things you're part of
[00:03:56.220 --> 00:03:58.980]   is the AI Institute for Artificial Intelligence
[00:03:58.980 --> 00:04:02.020]   and Fundamental Interactions.
[00:04:02.020 --> 00:04:03.580]   What's up with this institute?
[00:04:03.580 --> 00:04:05.020]   What are you working on?
[00:04:05.020 --> 00:04:06.300]   What are you thinking about?
[00:04:06.300 --> 00:04:10.420]   - The idea is something I'm very on fire with,
[00:04:10.420 --> 00:04:13.340]   which is basically AI meets physics.
[00:04:13.340 --> 00:04:16.740]   And you know, it's been almost five years now
[00:04:16.740 --> 00:04:19.680]   since I shifted my own MIT research
[00:04:19.680 --> 00:04:22.120]   from physics to machine learning.
[00:04:22.120 --> 00:04:24.200]   And in the beginning, I noticed a lot of my colleagues,
[00:04:24.200 --> 00:04:25.760]   even though they were polite about it,
[00:04:25.760 --> 00:04:29.120]   were like kind of, "What is Max doing?
[00:04:29.120 --> 00:04:30.320]   What is this weird stuff?"
[00:04:30.320 --> 00:04:32.280]   - He's lost his mind.
[00:04:32.280 --> 00:04:36.600]   - But then gradually, I, together with some colleagues,
[00:04:36.600 --> 00:04:39.080]   were able to persuade more and more
[00:04:39.080 --> 00:04:42.640]   of the other professors in our physics department
[00:04:42.640 --> 00:04:43.760]   to get interested in this.
[00:04:44.140 --> 00:04:47.620]   And now we got this amazing NSF center,
[00:04:47.620 --> 00:04:50.620]   so 20 million bucks for the next five years,
[00:04:50.620 --> 00:04:54.420]   MIT and a bunch of neighboring universities here also.
[00:04:54.420 --> 00:04:56.500]   And I noticed now those colleagues
[00:04:56.500 --> 00:04:58.860]   who were looking at me funny have stopped asking
[00:04:58.860 --> 00:05:01.620]   what the point is of this,
[00:05:01.620 --> 00:05:03.700]   because it's becoming more clear.
[00:05:03.700 --> 00:05:06.780]   And I really believe that, of course,
[00:05:06.780 --> 00:05:09.820]   AI can help physics a lot to do better physics,
[00:05:10.720 --> 00:05:14.280]   but physics can also help AI a lot,
[00:05:14.280 --> 00:05:17.600]   both by building better hardware.
[00:05:17.600 --> 00:05:19.840]   My colleague, Marin Solzhachich, for example,
[00:05:19.840 --> 00:05:22.960]   is working on an optical chip
[00:05:22.960 --> 00:05:24.400]   for much faster machine learning
[00:05:24.400 --> 00:05:26.420]   where the computation is done
[00:05:26.420 --> 00:05:29.000]   not by moving electrons around,
[00:05:29.000 --> 00:05:31.340]   but by moving photons around,
[00:05:31.340 --> 00:05:33.820]   dramatically less energy use, faster, better.
[00:05:33.820 --> 00:05:38.600]   We can also help AI a lot, I think,
[00:05:38.600 --> 00:05:43.600]   by having a different set of tools
[00:05:43.600 --> 00:05:46.680]   and a different, maybe more audacious attitude.
[00:05:46.680 --> 00:05:51.040]   AI has, to a significant extent,
[00:05:51.040 --> 00:05:52.480]   been an engineering discipline
[00:05:52.480 --> 00:05:55.080]   where you're just trying to make things that work
[00:05:55.080 --> 00:05:57.520]   and being more interested in maybe selling them
[00:05:57.520 --> 00:06:01.200]   than in figuring out exactly how they work
[00:06:01.200 --> 00:06:04.560]   and proving theorems about that they will always work.
[00:06:04.560 --> 00:06:06.080]   Contrast that with physics.
[00:06:06.080 --> 00:06:08.600]   When Elon Musk sends a rocket
[00:06:08.600 --> 00:06:11.020]   to the International Space Station,
[00:06:11.020 --> 00:06:12.800]   they didn't just train with machine learning,
[00:06:12.800 --> 00:06:14.720]   oh, let's fire it a little bit more to the left,
[00:06:14.720 --> 00:06:15.560]   a bit more to the right,
[00:06:15.560 --> 00:06:17.600]   oh, that also missed, let's try here.
[00:06:17.600 --> 00:06:21.840]   No, we figured out Newton's laws of gravitation
[00:06:21.840 --> 00:06:24.040]   and other things
[00:06:24.040 --> 00:06:26.440]   and got a really deep fundamental understanding.
[00:06:26.440 --> 00:06:31.480]   And that's what gives us such confidence in rockets.
[00:06:31.480 --> 00:06:36.480]   And my vision is that in the future,
[00:06:36.480 --> 00:06:38.200]   all machine learning systems
[00:06:38.200 --> 00:06:40.640]   that actually have impact on people's lives
[00:06:40.640 --> 00:06:43.720]   will be understood at a really, really deep level.
[00:06:43.720 --> 00:06:47.560]   So we trust them not 'cause some sales rep told us to,
[00:06:47.560 --> 00:06:49.400]   but because they've earned our trust.
[00:06:49.400 --> 00:06:53.200]   And really safety-critical things even prove
[00:06:53.200 --> 00:06:56.200]   that they will always do what we expect them to do.
[00:06:56.200 --> 00:06:57.640]   That's very much the physics mindset.
[00:06:57.640 --> 00:07:00.760]   So it's interesting, if you look at big breakthroughs
[00:07:00.760 --> 00:07:03.200]   that have happened in machine learning this year,
[00:07:03.200 --> 00:07:08.800]   from dancing robots, it's pretty fantastic,
[00:07:08.800 --> 00:07:09.880]   not just because it's cool,
[00:07:09.880 --> 00:07:13.480]   but if you just think about not that many years ago,
[00:07:13.480 --> 00:07:16.120]   this YouTube video at this DARPA challenge
[00:07:16.120 --> 00:07:19.800]   where the MIT robot comes out of the car and face plants,
[00:07:19.800 --> 00:07:24.400]   how far we've come in just a few years.
[00:07:24.400 --> 00:07:27.440]   Similarly, AlphaFold2,
[00:07:29.560 --> 00:07:31.760]   crushing the protein folding problem.
[00:07:31.760 --> 00:07:33.640]   We can talk more about implications
[00:07:33.640 --> 00:07:34.960]   for medical research and stuff,
[00:07:34.960 --> 00:07:38.320]   but hey, that's huge progress.
[00:07:38.320 --> 00:07:44.840]   You can look at GPT-3 that can spout off English texts,
[00:07:44.840 --> 00:07:49.440]   which sometimes really, really blows you away.
[00:07:49.440 --> 00:07:53.480]   You can look at the Google at DeepMind's MuZero,
[00:07:53.480 --> 00:07:58.480]   which doesn't just kick our butt in Go and Chess and Shogi,
[00:07:58.480 --> 00:08:00.360]   but also in all these Atari games,
[00:08:00.360 --> 00:08:03.560]   and you don't even have to teach it the rules now.
[00:08:03.560 --> 00:08:05.840]   What all of those have in common is,
[00:08:05.840 --> 00:08:07.480]   besides being powerful,
[00:08:07.480 --> 00:08:10.000]   is we don't fully understand how they work.
[00:08:10.000 --> 00:08:13.760]   And that's fine if it's just some dancing robots,
[00:08:13.760 --> 00:08:17.040]   and the worst thing that can happen is they face plant,
[00:08:17.040 --> 00:08:18.600]   or if they're playing Go,
[00:08:18.600 --> 00:08:19.680]   and the worst thing that can happen
[00:08:19.680 --> 00:08:22.040]   is that they make a bad move and lose the game.
[00:08:22.040 --> 00:08:25.400]   It's less fine if that's what's controlling
[00:08:25.400 --> 00:08:28.720]   your self-driving car or your nuclear power plant.
[00:08:28.720 --> 00:08:33.680]   And we've seen already that even though Hollywood
[00:08:33.680 --> 00:08:35.720]   had all these movies where they try to make us worry
[00:08:35.720 --> 00:08:39.160]   about the wrong things, like machines turning evil,
[00:08:39.160 --> 00:08:42.840]   the actual bad things that have happened with automation
[00:08:42.840 --> 00:08:45.560]   have not been machines turning evil.
[00:08:45.560 --> 00:08:48.400]   They've been caused by overtrust
[00:08:48.400 --> 00:08:50.160]   in things we didn't understand
[00:08:50.160 --> 00:08:51.400]   as well as we thought we did.
[00:08:51.400 --> 00:08:54.280]   Even very simple automated systems,
[00:08:54.280 --> 00:08:59.120]   like what Boeing put into the 737 MAX,
[00:08:59.120 --> 00:09:00.400]   killed a lot of people.
[00:09:00.400 --> 00:09:02.920]   Was it that that little simple system was evil?
[00:09:02.920 --> 00:09:03.920]   Of course not.
[00:09:03.920 --> 00:09:07.360]   But we didn't understand it as well as we should have.
[00:09:07.360 --> 00:09:10.600]   - And we trusted without understanding.
[00:09:10.600 --> 00:09:11.440]   - Exactly.
[00:09:11.440 --> 00:09:12.400]   - Hence the overtrust.
[00:09:12.400 --> 00:09:15.680]   - We didn't even understand that we didn't understand.
[00:09:15.680 --> 00:09:19.800]   The humility is really at the core of being a scientist.
[00:09:19.800 --> 00:09:21.880]   I think step one, if you wanna be a scientist,
[00:09:21.880 --> 00:09:24.160]   is don't ever fool yourself into thinking
[00:09:24.160 --> 00:09:26.120]   you understand things when you actually don't.
[00:09:26.120 --> 00:09:26.960]   - Yes.
[00:09:26.960 --> 00:09:27.800]   - Right?
[00:09:27.800 --> 00:09:29.480]   - That's probably good advice for humans in general.
[00:09:29.480 --> 00:09:31.320]   - I think humility in general can do us good.
[00:09:31.320 --> 00:09:33.080]   But in science, it's so spectacular.
[00:09:33.080 --> 00:09:35.880]   Like why did we have the wrong theory of gravity
[00:09:35.880 --> 00:09:40.520]   ever from Aristotle onward until Galileo's time?
[00:09:40.520 --> 00:09:42.520]   Why would we believe something so dumb
[00:09:42.520 --> 00:09:44.600]   as that if I throw this water bottle,
[00:09:44.600 --> 00:09:47.280]   it's gonna go up with constant speed
[00:09:47.280 --> 00:09:49.760]   until it realizes that its natural motion is down.
[00:09:49.760 --> 00:09:51.000]   - It changes its mind.
[00:09:51.040 --> 00:09:55.320]   - Because people just kind of assumed Aristotle was right,
[00:09:55.320 --> 00:09:57.680]   he's an authority, we understand that.
[00:09:57.680 --> 00:09:59.320]   Why did we believe things like
[00:09:59.320 --> 00:10:01.880]   that the sun is going around the earth?
[00:10:01.880 --> 00:10:04.600]   Why did we believe that time flows at the same rate
[00:10:04.600 --> 00:10:06.400]   for everyone until Einstein?
[00:10:06.400 --> 00:10:08.560]   Same exact mistake over and over again.
[00:10:08.560 --> 00:10:11.760]   We just weren't humble enough to acknowledge
[00:10:11.760 --> 00:10:13.920]   that we actually didn't know for sure.
[00:10:13.920 --> 00:10:15.720]   We assumed we knew.
[00:10:15.720 --> 00:10:17.280]   So we didn't discover the truth
[00:10:17.280 --> 00:10:18.840]   because we assumed there was nothing there
[00:10:18.840 --> 00:10:20.560]   to be discovered, right?
[00:10:20.560 --> 00:10:24.360]   There was something to be discovered about the 737 MAX.
[00:10:24.360 --> 00:10:26.520]   And if you had been a bit more suspicious
[00:10:26.520 --> 00:10:28.680]   and tested it better, we would have found it.
[00:10:28.680 --> 00:10:30.640]   And it's the same thing with most harm
[00:10:30.640 --> 00:10:33.720]   that's been done by automation so far, I would say.
[00:10:33.720 --> 00:10:35.520]   So I don't know if you, did you hear of a company
[00:10:35.520 --> 00:10:37.200]   called Knight Capital?
[00:10:37.200 --> 00:10:38.040]   - No.
[00:10:38.040 --> 00:10:40.520]   - So good, that means you didn't invest in them earlier.
[00:10:40.520 --> 00:10:42.080]   (both laughing)
[00:10:42.080 --> 00:10:44.440]   They deployed this automated rating system.
[00:10:44.440 --> 00:10:45.600]   - Yes.
[00:10:45.600 --> 00:10:46.960]   - All nice and shiny.
[00:10:46.960 --> 00:10:49.480]   They didn't understand it as well as they thought.
[00:10:49.480 --> 00:10:53.040]   And it went about losing 10 million bucks per minute
[00:10:53.040 --> 00:10:54.720]   for 44 minutes straight.
[00:10:54.720 --> 00:10:55.560]   - No.
[00:10:55.560 --> 00:10:56.960]   - Until someone presumably was like,
[00:10:56.960 --> 00:10:58.760]   "Oh no, shut this off."
[00:10:58.760 --> 00:11:00.480]   You know, was it evil?
[00:11:00.480 --> 00:11:03.480]   No, it was again, misplaced trust.
[00:11:03.480 --> 00:11:05.240]   Something they didn't fully understand, right?
[00:11:05.240 --> 00:11:08.480]   And there have been so many,
[00:11:08.480 --> 00:11:11.000]   even when people have been killed by robots,
[00:11:11.000 --> 00:11:14.280]   which is quite rare still, but in factory accidents,
[00:11:14.280 --> 00:11:17.560]   it's in every single case been not malice,
[00:11:17.560 --> 00:11:19.120]   just that the robot didn't understand
[00:11:19.120 --> 00:11:22.200]   that a human is different from an auto part or whatever.
[00:11:22.200 --> 00:11:28.040]   So this is where I think there's so much opportunity
[00:11:28.040 --> 00:11:31.400]   for a physics approach, where you just aim
[00:11:31.400 --> 00:11:33.680]   for a higher level of understanding.
[00:11:33.680 --> 00:11:37.040]   And if you look at all these systems that we talked about,
[00:11:37.040 --> 00:11:42.040]   from reinforcement learning systems and dancing robots
[00:11:42.040 --> 00:11:46.240]   to all these neural networks that power GPT-3
[00:11:46.240 --> 00:11:49.560]   and Go playing software and stuff,
[00:11:49.560 --> 00:11:52.640]   they're all basically black boxes,
[00:11:52.640 --> 00:11:54.480]   much like not so different
[00:11:54.480 --> 00:11:55.880]   from if you teach a human something,
[00:11:55.880 --> 00:11:58.080]   you have no idea how their brain works, right?
[00:11:58.080 --> 00:12:02.200]   Except the human brain at least has been error corrected
[00:12:02.200 --> 00:12:04.440]   during many, many centuries of evolution
[00:12:04.440 --> 00:12:07.480]   in a way that some of these systems have not, right?
[00:12:07.480 --> 00:12:10.600]   And my MIT research is entirely focused
[00:12:10.600 --> 00:12:12.760]   on demystifying this black box.
[00:12:12.760 --> 00:12:15.960]   Intelligible intelligence is my slogan.
[00:12:15.960 --> 00:12:18.440]   - That's a good line, intelligible intelligence.
[00:12:18.440 --> 00:12:20.360]   - Yeah, that we shouldn't settle for something
[00:12:20.360 --> 00:12:23.080]   that seems intelligent, but it should be intelligible
[00:12:23.080 --> 00:12:26.520]   so that we actually trust it because we understand it, right?
[00:12:26.520 --> 00:12:28.920]   Like, again, Elon trusts his rockets
[00:12:28.920 --> 00:12:30.640]   because he understands Newton's laws
[00:12:30.640 --> 00:12:32.880]   and thrust and how everything works.
[00:12:32.880 --> 00:12:35.160]   And let me tell you why,
[00:12:35.160 --> 00:12:36.880]   can I tell you why I'm optimistic about this?
[00:12:36.880 --> 00:12:37.720]   - Yes.
[00:12:37.720 --> 00:12:41.280]   - I think we've made a bit of a mistake
[00:12:41.280 --> 00:12:44.280]   where some people still think that somehow
[00:12:44.280 --> 00:12:46.520]   we're never gonna understand neural networks
[00:12:46.520 --> 00:12:49.520]   and we're just gonna have to learn to live with this.
[00:12:49.520 --> 00:12:52.240]   It's this very powerful black box.
[00:12:52.240 --> 00:12:55.880]   Basically, for those who haven't spent time
[00:12:55.880 --> 00:12:59.000]   building their own, it's super simple what happens inside.
[00:12:59.000 --> 00:13:01.280]   You send in a long list of numbers
[00:13:01.280 --> 00:13:04.880]   and then you do a bunch of operations on them,
[00:13:04.880 --> 00:13:06.960]   multiply by matrices, et cetera, et cetera,
[00:13:06.960 --> 00:13:09.920]   and some other numbers come out, that's the output of it.
[00:13:09.920 --> 00:13:12.480]   And then there are a bunch of knobs you can tune.
[00:13:13.600 --> 00:13:16.720]   And when you change them, it affects the computation,
[00:13:16.720 --> 00:13:18.120]   the input-output relation.
[00:13:18.120 --> 00:13:21.000]   And then you just give the computer some definition of good
[00:13:21.000 --> 00:13:22.640]   and it keeps optimizing these knobs
[00:13:22.640 --> 00:13:24.720]   until it performs as good as possible.
[00:13:24.720 --> 00:13:27.120]   And often you go like, wow, that's really good.
[00:13:27.120 --> 00:13:28.680]   This robot can dance,
[00:13:28.680 --> 00:13:30.840]   or this machine is beating me at chess now.
[00:13:30.840 --> 00:13:33.440]   And in the end, you have something
[00:13:33.440 --> 00:13:35.120]   which even though you can look inside it,
[00:13:35.120 --> 00:13:38.680]   you have very little idea of how it works.
[00:13:38.680 --> 00:13:40.240]   You can print out tables
[00:13:40.240 --> 00:13:43.200]   of all the millions of parameters in there.
[00:13:43.200 --> 00:13:44.880]   Is it crystal clear now how it's working?
[00:13:44.880 --> 00:13:46.840]   No, of course not, right?
[00:13:46.840 --> 00:13:48.920]   Many of my colleagues seem willing to settle for that.
[00:13:48.920 --> 00:13:51.720]   And I'm like, no, that's like the halfway point.
[00:13:51.720 --> 00:13:57.560]   Some have even gone as far as sort of guessing
[00:13:57.560 --> 00:14:00.480]   that the instrutability of this
[00:14:00.480 --> 00:14:02.760]   is where some of the power comes from
[00:14:02.760 --> 00:14:05.040]   and some sort of mysticism.
[00:14:05.040 --> 00:14:06.800]   I think that's total nonsense.
[00:14:06.800 --> 00:14:10.240]   I think the real power of neural networks
[00:14:10.240 --> 00:14:12.160]   comes not from instrutability,
[00:14:12.160 --> 00:14:15.040]   but from differentiability.
[00:14:15.040 --> 00:14:17.960]   And what I mean by that is simply that
[00:14:17.960 --> 00:14:23.880]   the output changes only smoothly if you tweak your knobs.
[00:14:23.880 --> 00:14:26.640]   And then you can use all these powerful methods
[00:14:26.640 --> 00:14:28.360]   we have for optimization in science.
[00:14:28.360 --> 00:14:29.520]   We can just tweak them a little bit
[00:14:29.520 --> 00:14:31.680]   and see did that get better or worse?
[00:14:31.680 --> 00:14:33.920]   That's the fundamental idea of machine learning,
[00:14:33.920 --> 00:14:36.080]   that the machine itself can keep optimizing
[00:14:36.080 --> 00:14:37.240]   until it gets better.
[00:14:37.240 --> 00:14:41.880]   Suppose you wrote this algorithm instead in Python
[00:14:41.880 --> 00:14:43.640]   or some other programming language.
[00:14:43.640 --> 00:14:45.600]   And then what the knobs did was
[00:14:45.600 --> 00:14:48.280]   they just changed random letters in your code.
[00:14:48.280 --> 00:14:51.440]   Now it would just epically fail.
[00:14:51.440 --> 00:14:53.400]   You change one thing and instead of saying print,
[00:14:53.400 --> 00:14:56.800]   it says sint, syntax error.
[00:14:56.800 --> 00:14:58.720]   You don't even know, was that for the better
[00:14:58.720 --> 00:14:59.880]   or for the worse, right?
[00:14:59.880 --> 00:15:02.440]   This to me is, this is what I believe
[00:15:02.440 --> 00:15:05.240]   is the fundamental power of neural networks.
[00:15:05.240 --> 00:15:07.440]   - And just to clarify, the changing of the different letters
[00:15:07.440 --> 00:15:10.640]   in a program would not be a differentiable process.
[00:15:10.640 --> 00:15:13.760]   - It would make it an invalid program, typically.
[00:15:13.760 --> 00:15:16.760]   And then you wouldn't even know if you changed more letters
[00:15:16.760 --> 00:15:18.560]   if it would make it work again.
[00:15:18.560 --> 00:15:22.120]   - So that's the magic of neural networks,
[00:15:22.120 --> 00:15:23.360]   the inscrutability.
[00:15:23.360 --> 00:15:26.040]   - The differentiability, that every setting
[00:15:26.040 --> 00:15:27.320]   of the parameters is a program
[00:15:27.320 --> 00:15:29.320]   and you can tell is it better or worse.
[00:15:29.320 --> 00:15:32.840]   - So you don't like the poetry of the mystery
[00:15:32.840 --> 00:15:35.120]   of neural networks as the source of its power?
[00:15:35.120 --> 00:15:39.160]   - I generally like poetry, but not in this case.
[00:15:39.160 --> 00:15:42.880]   It's so misleading and above all, it shortchanges us.
[00:15:42.880 --> 00:15:46.440]   It makes us underestimate the good things
[00:15:46.440 --> 00:15:48.640]   we can accomplish because, so what we've been doing
[00:15:48.640 --> 00:15:51.440]   in my group is basically step one,
[00:15:51.440 --> 00:15:54.900]   train the mysterious neural network to do something well.
[00:15:54.900 --> 00:15:59.580]   And then step two, do some additional AI techniques
[00:15:59.580 --> 00:16:02.680]   to see if we can now transform this black box
[00:16:02.680 --> 00:16:05.320]   into something equally intelligent
[00:16:05.320 --> 00:16:07.160]   that you can actually understand.
[00:16:07.160 --> 00:16:08.540]   So for example, I'll give you one example.
[00:16:08.540 --> 00:16:11.600]   This AI Feynman project that we just published.
[00:16:11.600 --> 00:16:16.600]   So we took the 100 most famous or complicated equations
[00:16:16.600 --> 00:16:21.160]   from one of my favorite physics textbooks.
[00:16:21.160 --> 00:16:22.600]   In fact, the one that got me into physics
[00:16:22.600 --> 00:16:25.920]   in the first place, the Feynman lectures on physics.
[00:16:25.920 --> 00:16:29.480]   And so you have a formula, maybe it has,
[00:16:29.480 --> 00:16:34.200]   what goes into the formula is six different variables
[00:16:34.200 --> 00:16:36.000]   and then what comes out is one.
[00:16:36.000 --> 00:16:38.080]   So then you can make like a giant Excel spreadsheet
[00:16:38.080 --> 00:16:39.420]   of seven columns.
[00:16:39.420 --> 00:16:41.680]   You put in just random numbers for the six columns
[00:16:41.680 --> 00:16:44.340]   for those six input variables and then you calculate
[00:16:44.340 --> 00:16:46.800]   with a formula of the seventh column, the output.
[00:16:46.800 --> 00:16:50.440]   So maybe it's like the force equals in the last column
[00:16:50.440 --> 00:16:51.680]   some function of the other.
[00:16:51.680 --> 00:16:53.860]   And now the task is, okay, if I don't tell you
[00:16:53.860 --> 00:16:57.340]   what the formula was, can you figure that out
[00:16:57.340 --> 00:16:59.020]   from looking at my spreadsheet I gave you?
[00:16:59.020 --> 00:17:00.100]   - Yes.
[00:17:00.100 --> 00:17:03.500]   - This problem is called symbolic regression.
[00:17:03.500 --> 00:17:06.260]   If I tell you that the formula is what we call
[00:17:06.260 --> 00:17:09.020]   a linear formula, so it's just that the output is
[00:17:09.020 --> 00:17:15.840]   some sum of all the things input, the times some constants,
[00:17:15.840 --> 00:17:18.680]   that's the famous easy problem we can solve.
[00:17:18.680 --> 00:17:21.340]   We do it all the time in science and engineering.
[00:17:21.340 --> 00:17:24.500]   But the general one, if it's more complicated functions
[00:17:24.500 --> 00:17:26.940]   with logarithms or cosines or other math,
[00:17:26.940 --> 00:17:30.600]   it's a very, very hard one and probably impossible
[00:17:30.600 --> 00:17:34.620]   to do fast in general just because the number of formulas
[00:17:34.620 --> 00:17:37.780]   with n symbols just grows exponentially.
[00:17:37.780 --> 00:17:39.380]   Just like the number of passwords you can make
[00:17:39.380 --> 00:17:41.320]   grow dramatically with length.
[00:17:41.320 --> 00:17:46.380]   But we had this idea that if you first have
[00:17:46.380 --> 00:17:48.980]   a neural network that can actually approximate the formula,
[00:17:48.980 --> 00:17:50.740]   you just trained it, even if you don't understand
[00:17:50.740 --> 00:17:54.940]   how it works, that can be a first step
[00:17:54.940 --> 00:17:57.340]   towards actually understanding how it works.
[00:17:57.340 --> 00:17:59.420]   So that's what we do first.
[00:17:59.420 --> 00:18:03.220]   And then we study that neural network now
[00:18:03.220 --> 00:18:05.340]   and put in all sorts of other data that wasn't
[00:18:05.340 --> 00:18:08.620]   in the original training data and use that to discover
[00:18:08.620 --> 00:18:11.420]   simplifying properties of the formula.
[00:18:11.420 --> 00:18:13.140]   And that lets us break it apart often
[00:18:13.140 --> 00:18:15.540]   into many simpler pieces in a kind of divide
[00:18:15.540 --> 00:18:17.420]   and conquer approach.
[00:18:17.420 --> 00:18:20.020]   So we were able to solve all of those 100 formulas,
[00:18:20.020 --> 00:18:22.140]   discover them automatically, plus a whole bunch
[00:18:22.140 --> 00:18:22.980]   of other ones.
[00:18:22.980 --> 00:18:28.020]   It's actually kind of humbling to see that this code,
[00:18:28.020 --> 00:18:30.260]   which anyone who wants now, who's listening to this,
[00:18:30.260 --> 00:18:33.700]   can type pip install AI Feynman on the computer
[00:18:33.700 --> 00:18:34.540]   and run it.
[00:18:34.540 --> 00:18:38.020]   It can actually do what Johannes Kepler spent four years
[00:18:38.020 --> 00:18:40.180]   doing when he stared at Mars data.
[00:18:40.180 --> 00:18:42.980]   Until he's like, "Finally, Eureka, this is an ellipse!"
[00:18:42.980 --> 00:18:46.900]   This will do it automatically for you in one hour, right?
[00:18:46.900 --> 00:18:51.020]   Or Max Planck, he was looking at how much radiation
[00:18:51.020 --> 00:18:54.140]   comes out at different wavelengths from a hot object
[00:18:54.140 --> 00:18:57.180]   and discovered the famous blackbody formula.
[00:18:57.180 --> 00:18:58.980]   This discovers it automatically.
[00:18:59.980 --> 00:19:04.980]   I'm actually excited about seeing if we can discover
[00:19:04.980 --> 00:19:09.660]   not just old formulas again, but new formulas
[00:19:09.660 --> 00:19:11.940]   that no one has seen before.
[00:19:11.940 --> 00:19:14.660]   - I do like this process of using kind of a neural network
[00:19:14.660 --> 00:19:18.380]   to find some basic insights and then dissecting
[00:19:18.380 --> 00:19:21.620]   the neural network to then gain the final.
[00:19:21.620 --> 00:19:25.060]   So that's, in that way, you've forcing
[00:19:27.260 --> 00:19:32.020]   the explainability issue of really trying to analyze
[00:19:32.020 --> 00:19:35.580]   a neural network for the things it knows
[00:19:35.580 --> 00:19:38.340]   in order to come up with the final, beautiful,
[00:19:38.340 --> 00:19:42.220]   simple theory underlying the initial system
[00:19:42.220 --> 00:19:43.060]   that you were looking at.
[00:19:43.060 --> 00:19:44.180]   - I love that.
[00:19:44.180 --> 00:19:48.420]   And the reason I'm so optimistic that it can be generalized
[00:19:48.420 --> 00:19:52.020]   to so much more is because that's exactly what we do
[00:19:52.020 --> 00:19:53.500]   as human scientists.
[00:19:53.500 --> 00:19:55.660]   Think of Galileo, whom we mentioned, right?
[00:19:55.660 --> 00:19:57.060]   I bet when he was a little kid,
[00:19:57.060 --> 00:19:59.540]   if his dad threw him an apple, he would catch it.
[00:19:59.540 --> 00:20:01.860]   Why?
[00:20:01.860 --> 00:20:04.420]   Because he had a neural network in his brain
[00:20:04.420 --> 00:20:07.220]   that he had trained to predict the parabolic orbit
[00:20:07.220 --> 00:20:09.900]   of apples that are thrown under gravity.
[00:20:09.900 --> 00:20:11.940]   If you throw a tennis ball to a dog,
[00:20:11.940 --> 00:20:15.380]   it also has this same ability of deep learning
[00:20:15.380 --> 00:20:18.220]   to figure out how the ball is gonna move and catch it.
[00:20:18.220 --> 00:20:21.980]   But Galileo went one step further when he got older.
[00:20:21.980 --> 00:20:24.020]   He went back and was like, "Wait a minute.
[00:20:24.100 --> 00:20:26.100]   (Lex laughs)
[00:20:26.100 --> 00:20:27.900]   "I can write down a formula for this.
[00:20:27.900 --> 00:20:31.660]   "Y equals X squared, a parabola."
[00:20:31.660 --> 00:20:36.580]   And he helped revolutionize physics as we know it, right?
[00:20:36.580 --> 00:20:38.820]   - So there was a basic neural network in there
[00:20:38.820 --> 00:20:43.380]   from childhood that captured the experiences
[00:20:43.380 --> 00:20:46.460]   of observing different kinds of trajectories.
[00:20:46.460 --> 00:20:48.260]   And then he was able to go back in
[00:20:48.260 --> 00:20:51.020]   with another extra little neural network
[00:20:51.020 --> 00:20:53.180]   and analyze all those experiences
[00:20:53.180 --> 00:20:54.540]   and be like, "Wait a minute.
[00:20:54.540 --> 00:20:56.300]   "There's a deeper rule here."
[00:20:56.300 --> 00:20:57.140]   - Exactly.
[00:20:57.140 --> 00:21:00.740]   He was able to distill out in symbolic form
[00:21:00.740 --> 00:21:04.020]   what that complicated black box neural network was doing.
[00:21:04.020 --> 00:21:06.940]   Not only did the formula he got
[00:21:06.940 --> 00:21:09.100]   ultimately become more accurate,
[00:21:09.100 --> 00:21:11.860]   and similarly, this is how Newton got Newton's laws,
[00:21:11.860 --> 00:21:15.660]   which is why Elon can send rockets to the space station now.
[00:21:15.660 --> 00:21:17.260]   So it's not only more accurate,
[00:21:17.260 --> 00:21:20.140]   but it's also simpler, much simpler.
[00:21:20.140 --> 00:21:22.340]   And it's so simple that we can actually describe it
[00:21:22.340 --> 00:21:24.940]   to our friends and each other, right?
[00:21:24.940 --> 00:21:28.780]   We've talked about it just in the context of physics now,
[00:21:28.780 --> 00:21:31.340]   but hey, isn't this what we're doing
[00:21:31.340 --> 00:21:33.420]   when we're talking to each other also?
[00:21:33.420 --> 00:21:35.500]   We go around with our neural networks,
[00:21:35.500 --> 00:21:38.740]   just like dogs and cats and chipmunks and blue jays,
[00:21:38.740 --> 00:21:41.860]   and we experience things in the world.
[00:21:41.860 --> 00:21:44.460]   But then we humans do this additional step on top of that,
[00:21:44.460 --> 00:21:48.780]   where we then distill out certain high-level knowledge
[00:21:48.780 --> 00:21:50.740]   that we've extracted from this in a way
[00:21:50.740 --> 00:21:52.900]   that we can communicate it to each other
[00:21:52.900 --> 00:21:56.660]   in a symbolic form, in English in this case, right?
[00:21:56.660 --> 00:21:59.240]   So if we can do it,
[00:21:59.240 --> 00:22:02.900]   and we believe that we are information processing entities,
[00:22:02.900 --> 00:22:04.860]   then we should be able to make machine learning
[00:22:04.860 --> 00:22:05.920]   that does it also.
[00:22:05.920 --> 00:22:10.140]   - Well, do you think the entire thing could be learning?
[00:22:10.140 --> 00:22:14.140]   Because this dissection process, like for AI Feynman,
[00:22:14.140 --> 00:22:19.140]   the secondary stage feels like something like reasoning,
[00:22:19.220 --> 00:22:21.300]   and the initial step feels like more like
[00:22:21.300 --> 00:22:25.300]   the more basic kind of differentiable learning.
[00:22:25.300 --> 00:22:29.100]   Do you think the whole thing could be differentiable learning?
[00:22:29.100 --> 00:22:30.500]   Do you think the whole thing could be
[00:22:30.500 --> 00:22:32.340]   basically neural networks on top of each other?
[00:22:32.340 --> 00:22:33.820]   It's like turtles all the way down?
[00:22:33.820 --> 00:22:35.940]   Can it be neural networks all the way down?
[00:22:35.940 --> 00:22:37.900]   - I mean, that's a really interesting question.
[00:22:37.900 --> 00:22:39.560]   We know that in your case,
[00:22:39.560 --> 00:22:41.320]   it is neural networks all the way down,
[00:22:41.320 --> 00:22:42.940]   because that's all you have in your skull,
[00:22:42.940 --> 00:22:45.900]   is a bunch of neurons doing their thing, right?
[00:22:45.900 --> 00:22:50.380]   But if you ask the question more generally,
[00:22:50.380 --> 00:22:54.180]   what algorithms are being used in your brain?
[00:22:54.180 --> 00:22:56.180]   I think it's super interesting to compare.
[00:22:56.180 --> 00:22:58.740]   I think we've gone a little bit backwards historically,
[00:22:58.740 --> 00:23:03.100]   because we humans first discovered good old-fashioned AI,
[00:23:03.100 --> 00:23:06.900]   the logic-based AI that we often called Go-Fi,
[00:23:06.900 --> 00:23:08.300]   for good old-fashioned AI.
[00:23:08.300 --> 00:23:12.780]   And then more recently, we did machine learning,
[00:23:12.780 --> 00:23:14.220]   because it required bigger computers,
[00:23:14.220 --> 00:23:16.020]   so we had to discover it later.
[00:23:16.020 --> 00:23:19.300]   So we think of machine learning with neural networks
[00:23:19.300 --> 00:23:20.580]   as the modern thing,
[00:23:20.580 --> 00:23:23.180]   and the logic-based AI as the old-fashioned thing.
[00:23:23.180 --> 00:23:27.820]   But if you look at evolution on Earth,
[00:23:27.820 --> 00:23:29.900]   it's actually been the other way around.
[00:23:29.900 --> 00:23:32.780]   I would say that, for example,
[00:23:32.780 --> 00:23:37.020]   an eagle has a better vision system than I have,
[00:23:37.020 --> 00:23:42.460]   and dogs are just as good at casting tennis balls as I am.
[00:23:42.460 --> 00:23:46.060]   All this stuff which is done by training a neural network
[00:23:46.060 --> 00:23:48.020]   and not interpreting it in words,
[00:23:48.020 --> 00:23:51.900]   is something so many of our animal friends can do,
[00:23:51.900 --> 00:23:53.780]   at least as well as us, right?
[00:23:53.780 --> 00:23:55.460]   What is it that we humans can do
[00:23:55.460 --> 00:23:58.180]   that the chipmunks and the eagles cannot?
[00:23:58.180 --> 00:24:01.620]   It's more to do with this logic-based stuff, right,
[00:24:01.620 --> 00:24:06.620]   where we can extract out information in symbols,
[00:24:06.620 --> 00:24:10.260]   in language, and now even with equations,
[00:24:10.260 --> 00:24:12.260]   if you're a scientist, right?
[00:24:12.260 --> 00:24:13.620]   So basically what happened was,
[00:24:13.620 --> 00:24:14.900]   first we built these computers
[00:24:14.900 --> 00:24:18.180]   that could multiply numbers real fast and manipulate symbols,
[00:24:18.180 --> 00:24:20.660]   and we felt they were pretty dumb.
[00:24:20.660 --> 00:24:22.740]   And then we made neural networks
[00:24:22.740 --> 00:24:25.060]   that can see as well as a cat can
[00:24:25.060 --> 00:24:29.240]   and do a lot of this inscrutable black box neural networks.
[00:24:29.240 --> 00:24:31.860]   What we humans can do also
[00:24:31.860 --> 00:24:34.140]   is put the two together in a useful way.
[00:24:34.140 --> 00:24:36.260]   - Yes, in our own brain.
[00:24:36.260 --> 00:24:37.420]   - Yes, in our own brain.
[00:24:37.420 --> 00:24:40.980]   So if we ever wanna get artificial general intelligence
[00:24:40.980 --> 00:24:45.180]   that can do all jobs as well as humans can, right,
[00:24:45.180 --> 00:24:47.180]   then that's what's gonna be required,
[00:24:47.180 --> 00:24:52.180]   to be able to combine the neural networks with symbolic,
[00:24:52.180 --> 00:24:55.340]   combine the old AI with the new AI in a good way.
[00:24:55.340 --> 00:24:57.380]   We do it in our brains,
[00:24:57.380 --> 00:24:59.860]   and there seems to be basically two strategies
[00:24:59.860 --> 00:25:01.140]   I see in industry now.
[00:25:01.140 --> 00:25:03.680]   One scares the heebie-jeebies out of me,
[00:25:03.680 --> 00:25:05.940]   and the other one I find much more encouraging.
[00:25:05.940 --> 00:25:07.180]   - Okay, which one?
[00:25:07.180 --> 00:25:08.420]   Can we break them apart?
[00:25:08.420 --> 00:25:09.740]   Which of the two? (laughs)
[00:25:09.740 --> 00:25:11.740]   - The one that scares the heebie-jeebies out of me
[00:25:11.740 --> 00:25:13.280]   is this attitude that we're just gonna make
[00:25:13.280 --> 00:25:15.940]   ever bigger systems that we still don't understand
[00:25:15.940 --> 00:25:18.420]   until they can be as smart as humans.
[00:25:18.420 --> 00:25:22.340]   What could possibly go wrong, right?
[00:25:22.340 --> 00:25:24.260]   I think it's just such a reckless thing to do.
[00:25:24.260 --> 00:25:28.340]   And unfortunately, and if we actually succeed as a species
[00:25:28.340 --> 00:25:30.180]   to build artificial general intelligence,
[00:25:30.180 --> 00:25:31.940]   then we still have no clue how it works,
[00:25:31.940 --> 00:25:35.260]   I think at least 50% chance
[00:25:35.260 --> 00:25:37.100]   we're gonna be extinct before too long.
[00:25:37.100 --> 00:25:40.540]   It's just gonna be an utter epic own goal.
[00:25:40.540 --> 00:25:45.380]   - So it's that 44-minute losing money problem
[00:25:45.380 --> 00:25:47.380]   or the paperclip problem
[00:25:47.380 --> 00:25:49.500]   where we don't understand how it works,
[00:25:49.500 --> 00:25:51.300]   and it just, in a matter of seconds,
[00:25:51.300 --> 00:25:52.800]   runs away in some kind of direction
[00:25:52.800 --> 00:25:54.500]   that's going to be very problematic.
[00:25:54.500 --> 00:25:56.820]   - Even long before you have to worry
[00:25:56.820 --> 00:25:59.180]   about the machines themselves
[00:25:59.180 --> 00:26:02.660]   somehow deciding to do things to us,
[00:26:02.660 --> 00:26:06.900]   we have to worry about people using machines.
[00:26:06.900 --> 00:26:09.900]   They're short of AI, AGI, and power to do bad things.
[00:26:09.900 --> 00:26:11.800]   I mean, just take a moment,
[00:26:11.800 --> 00:26:18.100]   and if anyone is not worried particularly about advanced AI,
[00:26:18.100 --> 00:26:21.060]   just take 10 seconds and just think about
[00:26:21.060 --> 00:26:23.780]   your least favorite leader on the planet right now.
[00:26:23.780 --> 00:26:25.220]   Don't tell me who it is.
[00:26:25.220 --> 00:26:26.840]   I'm gonna keep this apolitical.
[00:26:26.840 --> 00:26:28.900]   But just see the face in front of you,
[00:26:28.900 --> 00:26:30.540]   that person, for 10 seconds.
[00:26:30.540 --> 00:26:32.980]   Now imagine that that person has
[00:26:32.980 --> 00:26:36.660]   this incredibly powerful AI under their control
[00:26:36.660 --> 00:26:38.780]   and can use it to impose their will on the whole planet.
[00:26:38.780 --> 00:26:40.180]   How does that make you feel?
[00:26:40.180 --> 00:26:43.780]   - Yeah.
[00:26:43.780 --> 00:26:48.720]   Can we break that apart just briefly?
[00:26:48.720 --> 00:26:51.820]   For the 50% chance that we'll run
[00:26:51.820 --> 00:26:53.680]   to trouble with this approach,
[00:26:53.680 --> 00:26:56.940]   do you see the bigger worry in that leader
[00:26:56.940 --> 00:27:00.700]   or humans using the system to do damage,
[00:27:00.700 --> 00:27:03.420]   or are you more worried,
[00:27:03.420 --> 00:27:05.440]   and I think I'm in this camp,
[00:27:05.440 --> 00:27:07.540]   more worried about accidental,
[00:27:07.540 --> 00:27:10.940]   unintentional destruction of everything?
[00:27:10.940 --> 00:27:12.980]   So humans trying to do good,
[00:27:12.980 --> 00:27:17.500]   and in a way where everyone agrees it's kinda good,
[00:27:17.500 --> 00:27:18.940]   it's just that they're trying to do good
[00:27:18.940 --> 00:27:20.160]   without understanding.
[00:27:20.160 --> 00:27:22.580]   'Cause I think every evil leader in history
[00:27:22.580 --> 00:27:24.500]   thought they're, to some degree,
[00:27:24.500 --> 00:27:25.700]   thought they were trying to do good.
[00:27:25.700 --> 00:27:28.140]   - Oh yeah, I'm sure Hitler thought he was doing good.
[00:27:28.140 --> 00:27:29.620]   - Yeah, Stalin.
[00:27:29.620 --> 00:27:31.220]   I've been reading a lot about Stalin.
[00:27:31.220 --> 00:27:34.820]   I'm sure Stalin, he legitimately thought
[00:27:34.820 --> 00:27:36.700]   that communism was good for the world,
[00:27:36.700 --> 00:27:37.880]   and that he was doing good.
[00:27:37.880 --> 00:27:39.700]   - I think Mao Zedong thought what he was doing
[00:27:39.700 --> 00:27:41.380]   with the Great Leap Forward was good too.
[00:27:41.380 --> 00:27:42.220]   Yeah.
[00:27:42.220 --> 00:27:45.660]   I'm actually concerned about both of those.
[00:27:45.660 --> 00:27:48.460]   Before, I promised to answer this in detail,
[00:27:48.460 --> 00:27:49.780]   but before we do that,
[00:27:49.780 --> 00:27:51.300]   let me finish answering the first question,
[00:27:51.300 --> 00:27:53.460]   'cause I told you that there were two different routes
[00:27:53.460 --> 00:27:55.140]   we could get to artificial general intelligence,
[00:27:55.140 --> 00:27:57.140]   and one scares the hippies out of me,
[00:27:57.140 --> 00:27:59.300]   which is this one where we build something,
[00:27:59.300 --> 00:28:01.020]   we just say bigger neural networks,
[00:28:01.020 --> 00:28:02.060]   ever more hardware,
[00:28:02.060 --> 00:28:03.820]   and just train the heck out of more data,
[00:28:03.820 --> 00:28:06.180]   and poof, now it's very powerful.
[00:28:06.180 --> 00:28:11.880]   That, I think, is the most unsafe and reckless approach.
[00:28:11.880 --> 00:28:15.340]   The alternative to that is the intelligent,
[00:28:15.340 --> 00:28:18.060]   intelligible intelligence approach instead,
[00:28:18.060 --> 00:28:23.060]   where we say neural networks is just a tool
[00:28:23.060 --> 00:28:27.060]   like for the first step to get the intuition,
[00:28:27.060 --> 00:28:30.660]   but then we're gonna spend also serious resources
[00:28:30.660 --> 00:28:35.260]   on other AI techniques for demystifying this black box
[00:28:35.260 --> 00:28:37.620]   and figuring out what it's actually doing
[00:28:37.620 --> 00:28:39.820]   so we can convert it into something
[00:28:39.820 --> 00:28:41.040]   that's equally intelligent,
[00:28:41.040 --> 00:28:44.100]   but that we actually understand what it's doing.
[00:28:44.100 --> 00:28:45.980]   Maybe we can even prove theorems about it,
[00:28:45.980 --> 00:28:50.140]   that this car here will never be hacked when it's driving,
[00:28:50.140 --> 00:28:51.420]   because here's the proof.
[00:28:51.420 --> 00:28:55.180]   There is a whole science of this.
[00:28:55.180 --> 00:28:57.100]   It doesn't work for neural networks.
[00:28:57.100 --> 00:28:58.140]   There are big black boxes,
[00:28:58.140 --> 00:29:01.020]   but it works well in certain other kinds of codes.
[00:29:01.020 --> 00:29:05.180]   That approach, I think, is much more promising.
[00:29:05.180 --> 00:29:07.180]   That's exactly why I'm working on it, frankly,
[00:29:07.180 --> 00:29:09.460]   not just because I think it's cool for science,
[00:29:09.460 --> 00:29:14.120]   but because I think the more we understand these systems,
[00:29:14.120 --> 00:29:16.740]   the better the chances that we can make them
[00:29:16.740 --> 00:29:18.420]   do the things that are good for us,
[00:29:18.420 --> 00:29:21.600]   that are actually intended, not unintended.
[00:29:21.600 --> 00:29:24.300]   - So you think it's possible to prove things
[00:29:24.300 --> 00:29:27.440]   about something as complicated as a neural network?
[00:29:27.440 --> 00:29:28.540]   That's the hope?
[00:29:28.540 --> 00:29:30.820]   - Well, ideally, there's no reason
[00:29:30.820 --> 00:29:33.500]   there has to be a neural network in the end either.
[00:29:33.500 --> 00:29:34.340]   Right?
[00:29:34.340 --> 00:29:36.580]   Like, we discovered that Newton's laws of gravity
[00:29:36.580 --> 00:29:39.740]   with neural network in Newton's head.
[00:29:39.740 --> 00:29:40.580]   - Yes.
[00:29:40.580 --> 00:29:41.500]   - But that's not the way it's programmed
[00:29:41.500 --> 00:29:46.460]   into the navigation system of Elon Musk's rocket anymore.
[00:29:46.460 --> 00:29:47.300]   - Right.
[00:29:47.300 --> 00:29:48.820]   - It's written in C++,
[00:29:48.820 --> 00:29:51.140]   or I don't know what language he uses exactly.
[00:29:51.140 --> 00:29:51.980]   - Yeah.
[00:29:51.980 --> 00:29:52.820]   - And then there are software tools
[00:29:52.820 --> 00:29:54.660]   called symbolic verification,
[00:29:54.660 --> 00:29:57.860]   DARPA and the US military
[00:29:57.860 --> 00:30:00.560]   has done a lot of really great research on this,
[00:30:00.560 --> 00:30:01.960]   'cause they really want to understand
[00:30:01.960 --> 00:30:03.800]   that when they build weapon systems,
[00:30:03.800 --> 00:30:07.600]   they don't just go fire at random or malfunction, right?
[00:30:07.600 --> 00:30:11.520]   And there's even a whole operating system called Cell 3
[00:30:11.520 --> 00:30:12.960]   that's been developed by a DARPA grant
[00:30:12.960 --> 00:30:16.200]   where you can actually mathematically prove
[00:30:16.200 --> 00:30:18.040]   that this thing can never be hacked.
[00:30:18.040 --> 00:30:20.400]   - Wow.
[00:30:20.400 --> 00:30:22.680]   - One day, I hope that will be something you can say
[00:30:22.680 --> 00:30:25.220]   about the OS that's running on our laptops too.
[00:30:25.220 --> 00:30:26.980]   As you know, we're not there,
[00:30:26.980 --> 00:30:29.900]   but I think we should be ambitious, frankly.
[00:30:29.900 --> 00:30:30.740]   - Yeah.
[00:30:30.740 --> 00:30:34.220]   - And if we can use machine learning
[00:30:34.220 --> 00:30:36.380]   to help do the proofs and so on as well, right,
[00:30:36.380 --> 00:30:40.140]   then it's much easier to verify that a proof is correct
[00:30:40.140 --> 00:30:43.060]   than to come up with a proof in the first place.
[00:30:43.060 --> 00:30:45.140]   That's really the core idea here.
[00:30:45.140 --> 00:30:47.540]   If someone comes on your podcast and says
[00:30:47.540 --> 00:30:49.820]   they proved the Riemann hypothesis
[00:30:49.820 --> 00:30:52.100]   or some new sensational new theorem,
[00:30:52.920 --> 00:30:57.680]   it's much easier for someone else,
[00:30:57.680 --> 00:31:00.000]   take some smart math grad students to check,
[00:31:00.000 --> 00:31:02.600]   oh, there's an error here in equation five,
[00:31:02.600 --> 00:31:04.040]   or this really checks out
[00:31:04.040 --> 00:31:06.060]   than it was to discover the proof.
[00:31:06.060 --> 00:31:09.040]   - Yeah, although some of those proofs are pretty complicated,
[00:31:09.040 --> 00:31:11.120]   but yes, it's still nevertheless much easier
[00:31:11.120 --> 00:31:12.920]   to verify the proof.
[00:31:12.920 --> 00:31:14.500]   I love the optimism.
[00:31:14.500 --> 00:31:17.520]   You know, we kinda, even with the security of systems,
[00:31:17.520 --> 00:31:21.780]   there's a kinda cynicism that pervades people
[00:31:21.780 --> 00:31:24.960]   who think about this, which is like, oh, it's hopeless.
[00:31:24.960 --> 00:31:26.040]   I mean, in the same sense,
[00:31:26.040 --> 00:31:27.920]   exactly like you're saying when you own networks,
[00:31:27.920 --> 00:31:30.480]   oh, it's hopeless to understand what's happening.
[00:31:30.480 --> 00:31:32.080]   With security, people are just like,
[00:31:32.080 --> 00:31:37.080]   well, there's always going to be attack vectors,
[00:31:37.080 --> 00:31:40.840]   like ways to attack the system.
[00:31:40.840 --> 00:31:42.240]   But you're right, we're just very new
[00:31:42.240 --> 00:31:44.120]   with these computational systems.
[00:31:44.120 --> 00:31:46.440]   We're new with these intelligent systems,
[00:31:46.440 --> 00:31:49.600]   and it's not out of the realm of possibility.
[00:31:49.600 --> 00:31:51.900]   Just like people didn't understand the movement
[00:31:51.900 --> 00:31:53.740]   of the stars and the planets and so on.
[00:31:53.740 --> 00:31:54.580]   - Yeah.
[00:31:54.580 --> 00:31:58.340]   - It's entirely possible that within, hopefully soon,
[00:31:58.340 --> 00:32:00.420]   but it could be within 100 years,
[00:32:00.420 --> 00:32:03.660]   we start to have an obvious laws of gravity
[00:32:03.660 --> 00:32:07.820]   about intelligence, and God forbid,
[00:32:07.820 --> 00:32:09.100]   about consciousness, too.
[00:32:09.100 --> 00:32:11.020]   That one is--
[00:32:11.020 --> 00:32:12.340]   - Agreed.
[00:32:12.340 --> 00:32:15.300]   I think, of course, if you're selling computers
[00:32:15.300 --> 00:32:17.340]   that get hacked a lot, that's in your interest as a company
[00:32:17.340 --> 00:32:19.400]   that people think it's impossible to make it safe,
[00:32:19.400 --> 00:32:21.140]   so nobody's going to get the idea of suing you.
[00:32:21.140 --> 00:32:23.700]   But I want to really inject optimism here.
[00:32:23.700 --> 00:32:29.540]   It's absolutely possible to do much better
[00:32:29.540 --> 00:32:30.380]   than we're doing now.
[00:32:30.380 --> 00:32:34.900]   And your laptop does so much stuff.
[00:32:34.900 --> 00:32:38.020]   You don't need the music player to be super safe
[00:32:38.020 --> 00:32:42.200]   in your future self-driving car, right?
[00:32:42.200 --> 00:32:45.200]   If someone hacks it and starts playing music you don't like,
[00:32:45.200 --> 00:32:47.940]   the world won't end.
[00:32:47.940 --> 00:32:49.600]   But what you can do is you can break out
[00:32:49.600 --> 00:32:53.120]   and say the drive computer that controls your safety
[00:32:53.120 --> 00:32:55.960]   must be completely physically decoupled entirely
[00:32:55.960 --> 00:32:57.640]   from the entertainment system,
[00:32:57.640 --> 00:33:01.140]   and it must physically be such that it can't take on
[00:33:01.140 --> 00:33:03.240]   over-the-air updates while you're driving.
[00:33:03.240 --> 00:33:10.000]   It can have, ultimately, some operating system on it
[00:33:10.000 --> 00:33:12.320]   which is symbolically verified and proven
[00:33:12.320 --> 00:33:16.840]   that it's always going to do what it's supposed to do.
[00:33:17.880 --> 00:33:19.100]   We can basically have,
[00:33:19.100 --> 00:33:20.660]   and companies should take that attitude too.
[00:33:20.660 --> 00:33:22.180]   They should look at everything they do
[00:33:22.180 --> 00:33:25.880]   and say what are the few systems in our company
[00:33:25.880 --> 00:33:27.440]   that threaten the whole life of the company
[00:33:27.440 --> 00:33:28.720]   if they get hacked, you know,
[00:33:28.720 --> 00:33:31.840]   and have the highest standards for them.
[00:33:31.840 --> 00:33:34.600]   And then they can save money by going for the El Cheapo,
[00:33:34.600 --> 00:33:36.980]   poorly understood stuff for the rest.
[00:33:36.980 --> 00:33:38.960]   This is very feasible, I think.
[00:33:38.960 --> 00:33:41.760]   And coming back to the bigger question
[00:33:41.760 --> 00:33:43.200]   that you worried about,
[00:33:43.200 --> 00:33:46.320]   that there'll be unintentional failures, I think,
[00:33:46.320 --> 00:33:48.280]   there are two quite separate risks here, right?
[00:33:48.280 --> 00:33:49.600]   We talked a lot about one of them,
[00:33:49.600 --> 00:33:52.640]   which is that the goals are noble of the human.
[00:33:52.640 --> 00:33:56.920]   The human says, "I want this airplane to not crash
[00:33:56.920 --> 00:33:58.640]   'cause this is not Mohammed Atta
[00:33:58.640 --> 00:34:00.480]   now flying the airplane," right?
[00:34:00.480 --> 00:34:03.240]   And now there's this technical challenge
[00:34:03.240 --> 00:34:05.500]   of making sure that the autopilot
[00:34:05.500 --> 00:34:08.320]   is actually gonna behave as the pilot wants.
[00:34:08.320 --> 00:34:13.360]   If you set that aside, there's also the separate question.
[00:34:13.360 --> 00:34:17.400]   How do you make sure that the goals of the pilot
[00:34:17.400 --> 00:34:19.640]   are actually aligned with the goals of the passenger?
[00:34:19.640 --> 00:34:22.420]   How do you make sure very much more broadly
[00:34:22.420 --> 00:34:24.580]   that if we can all agree as a species
[00:34:24.580 --> 00:34:26.120]   that we would like things to kind of go well
[00:34:26.120 --> 00:34:28.080]   for humanity as a whole,
[00:34:28.080 --> 00:34:31.480]   that the goals are aligned here, the alignment problem.
[00:34:31.480 --> 00:34:35.960]   And yeah, there's been a lot of progress
[00:34:35.960 --> 00:34:39.080]   in the sense that there's suddenly
[00:34:39.080 --> 00:34:42.000]   huge amounts of research going on about it.
[00:34:42.000 --> 00:34:43.360]   I'm very grateful to Elon Musk
[00:34:43.360 --> 00:34:44.920]   for giving us that money five years ago
[00:34:44.920 --> 00:34:46.640]   so we could launch the first research program
[00:34:46.640 --> 00:34:49.480]   on technical AI safety and alignment.
[00:34:49.480 --> 00:34:51.240]   There's a lot of stuff happening.
[00:34:51.240 --> 00:34:54.100]   But I think we need to do more
[00:34:54.100 --> 00:34:55.640]   than just make sure little machines
[00:34:55.640 --> 00:34:57.280]   do always what their owners do.
[00:34:57.280 --> 00:35:00.120]   That wouldn't have prevented September 11th
[00:35:00.120 --> 00:35:03.000]   if Mohammed Atta said, "Okay, autopilot,
[00:35:03.000 --> 00:35:05.640]   please fly into World Trade Center."
[00:35:05.640 --> 00:35:07.680]   And it's like, okay.
[00:35:07.680 --> 00:35:11.800]   That even happened in a different situation.
[00:35:11.800 --> 00:35:15.680]   There was this depressed pilot named Andreas Lubitz,
[00:35:15.680 --> 00:35:17.600]   who told his Germanwings passenger jet
[00:35:17.600 --> 00:35:19.040]   to fly into the Alps.
[00:35:19.040 --> 00:35:21.600]   He just told the computer to change the altitude
[00:35:21.600 --> 00:35:23.280]   to 100 meters or something like that.
[00:35:23.280 --> 00:35:25.360]   And you know what the computer said?
[00:35:25.360 --> 00:35:26.560]   Okay.
[00:35:26.560 --> 00:35:29.520]   And it had the freaking topographical map of the Alps
[00:35:29.520 --> 00:35:31.440]   in there, it had GPS, everything.
[00:35:31.440 --> 00:35:33.080]   No one had bothered teaching it
[00:35:33.080 --> 00:35:35.560]   even the basic kindergarten ethics of like,
[00:35:35.560 --> 00:35:39.560]   no, we never want airplanes to fly into mountains
[00:35:39.560 --> 00:35:40.980]   under any circumstances.
[00:35:41.980 --> 00:35:46.980]   And so we have to think beyond just the technical issues
[00:35:46.980 --> 00:35:52.100]   and think about how do we align in general incentives
[00:35:52.100 --> 00:35:54.740]   on this planet for the greater good?
[00:35:54.740 --> 00:35:56.580]   So starting with simple stuff like that,
[00:35:56.580 --> 00:35:59.140]   every airplane that has a computer in it
[00:35:59.140 --> 00:36:01.900]   should be taught whatever kindergarten ethics
[00:36:01.900 --> 00:36:03.220]   that's smart enough to understand.
[00:36:03.220 --> 00:36:05.980]   Like, no, don't fly into fixed objects
[00:36:05.980 --> 00:36:08.260]   if the pilot tells you to do so,
[00:36:08.260 --> 00:36:12.980]   then go on autopilot mode, send an email to the cops
[00:36:12.980 --> 00:36:16.300]   and land at the latest airport, nearest airport.
[00:36:16.300 --> 00:36:19.660]   Any car with a forward facing camera
[00:36:19.660 --> 00:36:22.180]   should just be programmed by the manufacturer
[00:36:22.180 --> 00:36:25.040]   so that it will never accelerate into a human ever.
[00:36:25.040 --> 00:36:30.300]   That would avoid things like the Nice attack
[00:36:30.300 --> 00:36:32.620]   and many horrible terrorist vehicle attacks
[00:36:32.620 --> 00:36:35.140]   where they deliberately did that, right?
[00:36:35.140 --> 00:36:36.540]   This was not some sort of thing,
[00:36:36.540 --> 00:36:39.860]   oh, you know, US and China, different views on,
[00:36:39.860 --> 00:36:42.940]   no, there was not a single car manufacturer
[00:36:42.940 --> 00:36:45.460]   in the world who wanted the cars to do this.
[00:36:45.460 --> 00:36:47.180]   They just hadn't thought to do the alignment.
[00:36:47.180 --> 00:36:49.180]   And if you look at more broadly,
[00:36:49.180 --> 00:36:50.980]   problems that happen on this planet,
[00:36:50.980 --> 00:36:55.020]   the vast majority have to do with poor alignment.
[00:36:55.020 --> 00:36:58.380]   I mean, think about, let's go back really big
[00:36:58.380 --> 00:37:00.380]   'cause I know you're so good at that.
[00:37:00.380 --> 00:37:01.220]   - Let's go big, yeah.
[00:37:01.220 --> 00:37:05.100]   - Yeah, so long ago in evolution, we had these genes
[00:37:05.100 --> 00:37:07.620]   and they wanted to make copies of themselves.
[00:37:07.620 --> 00:37:09.380]   That's really all they cared about.
[00:37:09.380 --> 00:37:14.380]   So some genes said, hey, I'm gonna build a brain
[00:37:14.380 --> 00:37:17.420]   on this body I'm in so that I can get better
[00:37:17.420 --> 00:37:18.900]   at making copies of myself.
[00:37:18.900 --> 00:37:21.820]   And then they decided for their benefit
[00:37:21.820 --> 00:37:24.860]   to get copied more, to align your brain's incentives
[00:37:24.860 --> 00:37:26.020]   with their incentives.
[00:37:26.020 --> 00:37:29.640]   So it didn't want you to starve to death,
[00:37:29.640 --> 00:37:33.260]   so it gave you an incentive to eat
[00:37:33.260 --> 00:37:36.580]   and it wanted you to make copies of the genes.
[00:37:36.580 --> 00:37:38.620]   So it gave you incentive to fall in love
[00:37:38.620 --> 00:37:42.460]   and do all sorts of naughty things
[00:37:42.460 --> 00:37:45.340]   to make copies of itself, right?
[00:37:45.340 --> 00:37:49.140]   So that was successful value alignment done on the genes.
[00:37:49.140 --> 00:37:51.620]   They created something more intelligent than themselves,
[00:37:51.620 --> 00:37:54.140]   but they made sure to try to align the values.
[00:37:54.140 --> 00:37:57.060]   But then something went a little bit wrong
[00:37:57.060 --> 00:37:59.600]   against the idea of what the genes wanted
[00:37:59.600 --> 00:38:01.700]   because a lot of humans discovered,
[00:38:01.700 --> 00:38:05.580]   hey, yeah, we really like this business about sex
[00:38:05.580 --> 00:38:07.660]   that the genes have made us enjoy,
[00:38:07.660 --> 00:38:10.260]   but we don't wanna have babies right now.
[00:38:10.260 --> 00:38:14.700]   So we're gonna hack the genes and use birth control.
[00:38:14.700 --> 00:38:19.220]   And I really feel like drinking a Coca-Cola right now,
[00:38:19.220 --> 00:38:20.660]   but I don't wanna get a potbelly,
[00:38:20.660 --> 00:38:22.540]   so I'm gonna drink Diet Coke.
[00:38:22.540 --> 00:38:25.660]   We have all these things we've figured out
[00:38:25.660 --> 00:38:26.980]   because we're smarter than the genes,
[00:38:26.980 --> 00:38:29.700]   how we can actually subvert their intentions.
[00:38:29.700 --> 00:38:33.500]   So it's not surprising that we humans now,
[00:38:33.500 --> 00:38:34.860]   when we're in the role of these genes,
[00:38:34.860 --> 00:38:37.700]   creating other non-human entities with a lot of power,
[00:38:37.700 --> 00:38:39.500]   have to face the same exact challenge.
[00:38:39.500 --> 00:38:41.780]   How do we make other powerful entities
[00:38:41.780 --> 00:38:44.580]   have incentives that are aligned with ours
[00:38:44.580 --> 00:38:47.060]   so they won't hack them?
[00:38:47.060 --> 00:38:48.780]   Corporations, for example, right?
[00:38:48.780 --> 00:38:51.360]   We humans decided to create corporations
[00:38:51.360 --> 00:38:53.520]   because it can benefit us greatly.
[00:38:53.520 --> 00:38:55.180]   Now all of a sudden there's a supermarket.
[00:38:55.180 --> 00:38:56.340]   I can go buy food there.
[00:38:56.340 --> 00:38:57.260]   I don't have to hunt.
[00:38:57.260 --> 00:38:58.100]   Awesome.
[00:38:59.580 --> 00:39:02.900]   And then to make sure that this corporation
[00:39:02.900 --> 00:39:05.980]   would do things that were good for us and not bad for us,
[00:39:05.980 --> 00:39:08.300]   we created institutions to keep them in check.
[00:39:08.300 --> 00:39:12.540]   Like if the local supermarket sells poisonous food,
[00:39:12.540 --> 00:39:17.540]   then the owners of the supermarket
[00:39:17.540 --> 00:39:22.180]   have to spend some years reflecting behind bars, right?
[00:39:22.180 --> 00:39:25.720]   So we created incentives to align them.
[00:39:25.720 --> 00:39:27.480]   But of course, just like we were able to see
[00:39:27.480 --> 00:39:29.460]   through this thing and you,
[00:39:29.460 --> 00:39:31.860]   birth control, if you're a powerful corporation,
[00:39:31.860 --> 00:39:35.100]   you also have an incentive to try to hack the institutions
[00:39:35.100 --> 00:39:36.340]   that are supposed to govern you
[00:39:36.340 --> 00:39:38.180]   'cause you ultimately as a corporation
[00:39:38.180 --> 00:39:40.940]   have an incentive to maximize your profit.
[00:39:40.940 --> 00:39:42.080]   Just like you have an incentive
[00:39:42.080 --> 00:39:44.180]   to maximize the enjoyment your brain has,
[00:39:44.180 --> 00:39:46.020]   not for your genes.
[00:39:46.020 --> 00:39:50.460]   So if they can figure out a way of bribing regulators,
[00:39:50.460 --> 00:39:52.380]   then they're gonna do that.
[00:39:52.380 --> 00:39:54.420]   In the US, we kind of caught onto that
[00:39:54.420 --> 00:39:57.220]   and made laws against corruption and bribery.
[00:39:58.580 --> 00:40:03.580]   Then in the late 1800s, Teddy Roosevelt realized that,
[00:40:03.580 --> 00:40:05.340]   no, we were still being kind of hacked
[00:40:05.340 --> 00:40:07.280]   'cause the Massachusetts Railroad Companies
[00:40:07.280 --> 00:40:10.140]   had like a bigger budget than the state of Massachusetts
[00:40:10.140 --> 00:40:13.600]   and they were doing a lot of very corrupt stuff.
[00:40:13.600 --> 00:40:15.500]   So he did the whole trust busting thing
[00:40:15.500 --> 00:40:18.460]   to try to align these other non-human entities,
[00:40:18.460 --> 00:40:21.660]   the companies, again, more with the incentives
[00:40:21.660 --> 00:40:23.040]   of Americans as a whole.
[00:40:23.040 --> 00:40:26.100]   It's not surprising though that this is a battle
[00:40:26.100 --> 00:40:27.180]   you have to keep fighting.
[00:40:27.180 --> 00:40:31.460]   Now we have even larger companies than we ever had before.
[00:40:31.460 --> 00:40:34.340]   And of course, they're gonna try to, again,
[00:40:34.340 --> 00:40:39.640]   subvert the institutions, not because,
[00:40:39.640 --> 00:40:43.100]   I think people make a mistake of getting all too,
[00:40:43.100 --> 00:40:47.940]   black, thinking about things in terms of good and evil,
[00:40:47.940 --> 00:40:51.380]   like arguing about whether corporations are good or evil
[00:40:51.380 --> 00:40:54.020]   or whether robots are good or evil.
[00:40:54.020 --> 00:40:57.020]   A robot isn't good or evil.
[00:40:57.020 --> 00:40:59.300]   It's a tool and you can use it for great things
[00:40:59.300 --> 00:41:02.020]   like robotic surgery or for bad things.
[00:41:02.020 --> 00:41:05.140]   And a corporation also is a tool, of course.
[00:41:05.140 --> 00:41:07.460]   And if you have good incentives to the corporation,
[00:41:07.460 --> 00:41:09.620]   it'll do great things like start a hospital
[00:41:09.620 --> 00:41:10.980]   or a grocery store.
[00:41:10.980 --> 00:41:12.680]   If you have really bad incentives,
[00:41:12.680 --> 00:41:16.780]   then it's gonna start maybe marketing addictive drugs
[00:41:16.780 --> 00:41:19.440]   to people and you'll have an opioid epidemic, right?
[00:41:19.440 --> 00:41:25.620]   It's all about, we should not make the mistake
[00:41:25.620 --> 00:41:27.380]   of getting into some sort of fairy tale,
[00:41:27.380 --> 00:41:30.500]   good, evil thing about corporations or robots.
[00:41:30.500 --> 00:41:33.420]   We should focus on putting the right incentives in place.
[00:41:33.420 --> 00:41:35.740]   My optimistic vision is that if we can do that,
[00:41:35.740 --> 00:41:38.460]   then we can really get good things.
[00:41:38.460 --> 00:41:40.580]   We're not doing so great with that right now,
[00:41:40.580 --> 00:41:44.380]   either on AI, I think, or on other intelligent,
[00:41:44.380 --> 00:41:46.540]   non-human entities like big companies.
[00:41:46.540 --> 00:41:50.180]   We just have a new Secretary of Defense
[00:41:50.180 --> 00:41:53.620]   that's gonna start up now in the Biden administration
[00:41:53.620 --> 00:41:58.140]   who was an active member of the board of Raytheon,
[00:41:58.140 --> 00:41:59.220]   for example. - I hope, yeah.
[00:41:59.220 --> 00:42:03.300]   - So I have nothing against Raytheon.
[00:42:03.300 --> 00:42:05.660]   I'm not a pacifist,
[00:42:05.660 --> 00:42:08.540]   but there's an obvious conflict of interest
[00:42:08.540 --> 00:42:12.340]   if someone is in the job where they decide
[00:42:12.340 --> 00:42:14.260]   who they're gonna contract with.
[00:42:14.260 --> 00:42:16.700]   And I think somehow we have,
[00:42:16.700 --> 00:42:19.540]   maybe we need another Teddy Roosevelt to come along again
[00:42:19.540 --> 00:42:23.460]   and say, "Hey, we want what's good for all Americans.
[00:42:23.460 --> 00:42:26.540]   "And we need to go do some serious realigning again
[00:42:26.540 --> 00:42:29.820]   "of the incentives that we're giving to these big companies.
[00:42:29.820 --> 00:42:33.900]   "And then we're gonna be better off."
[00:42:33.900 --> 00:42:35.820]   - It seems that naturally with human beings,
[00:42:35.820 --> 00:42:37.740]   just like you beautifully described the history
[00:42:37.740 --> 00:42:40.780]   of this whole thing, it all started with the genes,
[00:42:40.780 --> 00:42:42.660]   and they're probably pretty upset
[00:42:42.660 --> 00:42:45.600]   by all the unintended consequences that happened since.
[00:42:45.600 --> 00:42:48.700]   But it seems that it kinda works out.
[00:42:48.700 --> 00:42:51.140]   Like it's in this collective intelligence
[00:42:51.140 --> 00:42:53.500]   that emerges at the different levels.
[00:42:53.500 --> 00:42:56.920]   It seems to find sometimes last minute
[00:42:56.920 --> 00:43:00.940]   a way to realign the values or keep the values aligned.
[00:43:00.940 --> 00:43:03.820]   It's almost, it finds a way.
[00:43:03.820 --> 00:43:07.580]   Like different leaders, different humans pop up
[00:43:07.580 --> 00:43:10.700]   all over the place that reset the system.
[00:43:10.700 --> 00:43:15.260]   Do you want, I mean, do you have an explanation why that is?
[00:43:15.260 --> 00:43:17.260]   Or is that just survivor bias?
[00:43:17.260 --> 00:43:19.620]   And also, is that different,
[00:43:19.620 --> 00:43:23.100]   somehow fundamentally different than with AI systems,
[00:43:23.100 --> 00:43:26.420]   where you're no longer dealing with something
[00:43:26.420 --> 00:43:30.180]   that was a direct, maybe companies are the same,
[00:43:30.180 --> 00:43:33.340]   a direct byproduct of the evolutionary process?
[00:43:33.340 --> 00:43:36.180]   - I think there is one thing which has changed.
[00:43:36.180 --> 00:43:40.280]   That's why I'm not all optimistic.
[00:43:40.280 --> 00:43:42.260]   That's why I think there's about a 50% chance
[00:43:42.260 --> 00:43:46.100]   if we take the dumb route with artificial intelligence
[00:43:46.100 --> 00:43:50.240]   that humanity will be extinct in this century.
[00:43:50.240 --> 00:43:53.580]   First, just the big picture, yeah,
[00:43:53.580 --> 00:43:56.720]   companies need to have the right incentives.
[00:43:56.720 --> 00:43:58.980]   Even governments, right?
[00:43:58.980 --> 00:44:00.900]   We used to have governments,
[00:44:00.900 --> 00:44:04.220]   usually there were just some king, you know,
[00:44:04.220 --> 00:44:07.180]   who was the king because his dad was the king, you know?
[00:44:07.180 --> 00:44:10.580]   And then there were some benefits
[00:44:10.580 --> 00:44:15.280]   of having this powerful kingdom or empire of any sort,
[00:44:15.280 --> 00:44:17.980]   because then it could prevent a lot of local squabbles.
[00:44:17.980 --> 00:44:19.380]   So at least everybody in that region
[00:44:19.380 --> 00:44:20.780]   would stop warring against each other.
[00:44:20.780 --> 00:44:24.180]   And their incentives of different cities in the kingdom
[00:44:24.180 --> 00:44:25.140]   became more aligned, right?
[00:44:25.140 --> 00:44:27.220]   That was the whole selling point.
[00:44:27.220 --> 00:44:31.500]   Harari, Noah Ural Harari has a beautiful piece
[00:44:31.500 --> 00:44:35.340]   on how empires were collaboration enablers.
[00:44:35.340 --> 00:44:37.620]   And then we also, Harari says, invented money
[00:44:37.620 --> 00:44:40.660]   for that reason, so we could have better alignment
[00:44:40.660 --> 00:44:44.140]   and we could do trade even with people we didn't know.
[00:44:44.140 --> 00:44:45.840]   So this sort of stuff has been playing out
[00:44:45.840 --> 00:44:47.840]   since time immemorial, right?
[00:44:47.840 --> 00:44:51.520]   What's changed is that it happens on ever larger scales,
[00:44:51.520 --> 00:44:53.480]   right, technology keeps getting better
[00:44:53.480 --> 00:44:54.760]   because science gets better.
[00:44:54.760 --> 00:44:57.600]   So now we can communicate over larger distances,
[00:44:57.600 --> 00:44:59.840]   transport things fast over larger distances.
[00:44:59.840 --> 00:45:02.960]   And so the entities get ever bigger,
[00:45:02.960 --> 00:45:05.480]   but our planet is not getting bigger anymore.
[00:45:05.480 --> 00:45:08.120]   So in the past, you could have one experiment
[00:45:08.120 --> 00:45:11.080]   that just totally screwed up, like Easter Island,
[00:45:12.020 --> 00:45:15.220]   where they actually managed to have such poor alignment
[00:45:15.220 --> 00:45:17.660]   that when they went extinct, people there,
[00:45:17.660 --> 00:45:20.620]   there was no one else to come back and replace them, right?
[00:45:20.620 --> 00:45:24.060]   If Elon Musk doesn't get us to Mars,
[00:45:24.060 --> 00:45:27.740]   and then we go extinct on a global scale,
[00:45:27.740 --> 00:45:29.020]   then we're not coming back.
[00:45:29.020 --> 00:45:31.540]   That's the fundamental difference.
[00:45:31.540 --> 00:45:34.740]   And that's a mistake I would rather that we don't make
[00:45:34.740 --> 00:45:35.860]   for that reason.
[00:45:35.860 --> 00:45:39.860]   In the past, of course, history is full of fiascos, right?
[00:45:39.860 --> 00:45:42.200]   But it was never the whole planet.
[00:45:42.200 --> 00:45:46.000]   And then, okay, now there's this nice uninhabited land here,
[00:45:46.000 --> 00:45:49.440]   some other people could move in and organize things better.
[00:45:49.440 --> 00:45:50.760]   This is different.
[00:45:50.760 --> 00:45:52.720]   The second thing which is also different
[00:45:52.720 --> 00:45:57.720]   is that technology gives us so much more empowerment,
[00:45:57.720 --> 00:46:00.560]   right, both to do good things and also to screw up.
[00:46:00.560 --> 00:46:02.960]   In the Stone Age, even if you had someone
[00:46:02.960 --> 00:46:04.800]   whose goals were really poorly aligned,
[00:46:04.800 --> 00:46:06.720]   like maybe he was really pissed off
[00:46:06.720 --> 00:46:08.760]   because his Stone Age girlfriend dumped him
[00:46:08.760 --> 00:46:09.940]   and he just wanted to,
[00:46:09.940 --> 00:46:12.660]   if he wanted to kill as many people as he could,
[00:46:12.660 --> 00:46:15.180]   how many could he really take out with a rock and a stick
[00:46:15.180 --> 00:46:17.220]   before he was overpowered, right?
[00:46:17.220 --> 00:46:18.960]   Just a handful, right?
[00:46:18.960 --> 00:46:23.780]   Now, with today's technology,
[00:46:23.780 --> 00:46:25.660]   if we have an accidental nuclear war
[00:46:25.660 --> 00:46:27.900]   between Russia and the US,
[00:46:27.900 --> 00:46:31.100]   which we almost have about a dozen times,
[00:46:31.100 --> 00:46:32.340]   and then we have a nuclear winter,
[00:46:32.340 --> 00:46:34.780]   it could take out seven billion people,
[00:46:34.780 --> 00:46:37.300]   or six billion people, we don't know.
[00:46:37.300 --> 00:46:40.460]   So the scale of the damage is bigger than we can do.
[00:46:40.460 --> 00:46:41.300]   And if,
[00:46:41.300 --> 00:46:46.060]   there's obviously no law of physics that says
[00:46:46.060 --> 00:46:48.100]   that technology will never get powerful enough
[00:46:48.100 --> 00:46:51.740]   that we could wipe out our species entirely.
[00:46:51.740 --> 00:46:53.640]   That would just be fantasy to think
[00:46:53.640 --> 00:46:55.100]   that science is somehow doomed
[00:46:55.100 --> 00:46:57.220]   to not get more powerful than that, right?
[00:46:57.220 --> 00:47:00.260]   And it's not at all unfeasible in our lifetime
[00:47:00.260 --> 00:47:03.100]   that someone could design a designer pandemic
[00:47:03.100 --> 00:47:04.660]   which spreads as easily as COVID,
[00:47:04.660 --> 00:47:06.880]   but just basically kills everybody.
[00:47:06.880 --> 00:47:08.500]   We already had smallpox.
[00:47:08.500 --> 00:47:10.700]   It killed one third of everybody who got it.
[00:47:10.700 --> 00:47:14.420]   - What do you think of the,
[00:47:14.420 --> 00:47:16.820]   here's an intuition, maybe it's completely naive,
[00:47:16.820 --> 00:47:18.980]   and this optimistic intuition I have,
[00:47:18.980 --> 00:47:22.860]   which it seems, and maybe it's a biased experience
[00:47:22.860 --> 00:47:25.940]   that I have, but it seems like the most brilliant people
[00:47:25.940 --> 00:47:29.940]   I've met in my life all are really
[00:47:29.940 --> 00:47:33.700]   like fundamentally good human beings.
[00:47:33.700 --> 00:47:35.860]   And not like naive good,
[00:47:35.860 --> 00:47:38.020]   like they really want to do good for the world
[00:47:38.020 --> 00:47:39.920]   in a way that, well, maybe is aligned
[00:47:39.920 --> 00:47:41.820]   to my sense of what good means.
[00:47:41.820 --> 00:47:45.260]   And so I have a sense that the,
[00:47:45.260 --> 00:47:48.980]   the people that will be defining
[00:47:48.980 --> 00:47:51.020]   the very cutting edge of technology,
[00:47:51.020 --> 00:47:53.980]   there'll be much more of the ones that are doing good
[00:47:53.980 --> 00:47:55.860]   versus the ones that are doing evil.
[00:47:55.860 --> 00:48:00.180]   So the race, I'm optimistic on the,
[00:48:00.180 --> 00:48:03.100]   us always like last minute coming up with a solution.
[00:48:03.100 --> 00:48:06.500]   So if there's an engineered pandemic
[00:48:06.500 --> 00:48:09.300]   that has the capability to destroy
[00:48:09.300 --> 00:48:11.660]   most of the human civilization,
[00:48:11.660 --> 00:48:15.900]   it feels like to me, either leading up to that before
[00:48:15.900 --> 00:48:19.260]   or as it's going on, there will be,
[00:48:19.260 --> 00:48:22.500]   we're able to rally the collective genius
[00:48:22.500 --> 00:48:23.820]   of the human species.
[00:48:23.820 --> 00:48:26.140]   I can tell by your smile that you're
[00:48:26.140 --> 00:48:30.060]   at least some percentage doubtful.
[00:48:30.060 --> 00:48:35.020]   But could that be a fundamental law of human nature?
[00:48:35.020 --> 00:48:36.860]   That evolution only creates,
[00:48:36.860 --> 00:48:40.900]   it creates like karma is beneficial,
[00:48:40.900 --> 00:48:44.300]   good is beneficial, and therefore we'll be all right.
[00:48:44.300 --> 00:48:46.780]   - I hope you're right.
[00:48:46.780 --> 00:48:48.700]   I really would love it if you're right,
[00:48:48.700 --> 00:48:50.540]   if there's some sort of law of nature
[00:48:50.540 --> 00:48:51.820]   that says that we always get lucky
[00:48:51.820 --> 00:48:53.560]   in the last second because of karma.
[00:48:53.560 --> 00:48:57.620]   But you know, I prefer,
[00:48:57.620 --> 00:49:00.980]   I prefer not playing it so close
[00:49:00.980 --> 00:49:03.060]   and gambling on that.
[00:49:03.060 --> 00:49:06.500]   And I think, in fact, I think it can be dangerous
[00:49:06.500 --> 00:49:08.140]   to have too strong faith in that
[00:49:08.140 --> 00:49:10.780]   because it makes us complacent.
[00:49:10.780 --> 00:49:12.500]   Like if someone tells you, you never have to worry
[00:49:12.500 --> 00:49:13.740]   about your house burning down,
[00:49:13.740 --> 00:49:15.340]   then you're not gonna put in a smoke detector
[00:49:15.340 --> 00:49:16.980]   'cause why would you need to, right?
[00:49:16.980 --> 00:49:19.040]   Even if it's sometimes very simple precautions,
[00:49:19.040 --> 00:49:20.000]   we don't take them.
[00:49:20.000 --> 00:49:23.500]   If you're like, oh, the government is gonna take care
[00:49:23.500 --> 00:49:25.820]   of everything for us, I can always trust my politicians.
[00:49:25.820 --> 00:49:28.700]   We can always, we abdicate our own responsibility.
[00:49:28.700 --> 00:49:30.300]   I think it's a healthier attitude to say,
[00:49:30.300 --> 00:49:32.180]   yeah, maybe things will work out.
[00:49:32.180 --> 00:49:34.700]   Maybe I'm actually gonna have to myself step up
[00:49:34.700 --> 00:49:36.320]   and take responsibility.
[00:49:36.320 --> 00:49:39.660]   And the stakes are so huge.
[00:49:39.660 --> 00:49:41.420]   I mean, if we do this right,
[00:49:41.420 --> 00:49:44.860]   we can develop all this ever more powerful technology
[00:49:44.860 --> 00:49:47.580]   and cure all diseases and create a future
[00:49:47.580 --> 00:49:49.300]   where humanity is healthy and wealthy
[00:49:49.300 --> 00:49:50.920]   for not just the next election cycle,
[00:49:50.920 --> 00:49:53.880]   but like billions of years throughout our universe.
[00:49:53.880 --> 00:49:55.700]   That's really worth working hard for.
[00:49:55.700 --> 00:49:58.860]   And not just sitting and hoping
[00:49:58.860 --> 00:50:00.380]   for some sort of fairy tale karma.
[00:50:00.380 --> 00:50:02.460]   - Well, I just mean, so you're absolutely right.
[00:50:02.460 --> 00:50:03.940]   From the perspective of the individual,
[00:50:03.940 --> 00:50:06.420]   like for me, the primary thing should be
[00:50:06.420 --> 00:50:10.580]   to take responsibility and to build the solutions
[00:50:10.580 --> 00:50:12.060]   that your skillset allows to build.
[00:50:12.060 --> 00:50:13.620]   - Yeah, which is a lot.
[00:50:13.620 --> 00:50:15.360]   I think we underestimate often very much
[00:50:15.360 --> 00:50:17.300]   how much good we can do.
[00:50:17.300 --> 00:50:22.220]   If you or anyone listening to this is completely confident
[00:50:22.220 --> 00:50:24.460]   that our government would do a perfect job
[00:50:24.460 --> 00:50:28.620]   on handling any future crisis with engineered pandemics
[00:50:28.620 --> 00:50:31.020]   or future AI, I actually-
[00:50:31.020 --> 00:50:33.300]   - The one or two people out there.
[00:50:33.300 --> 00:50:35.780]   - On what actually happened in 2020.
[00:50:35.780 --> 00:50:40.340]   Do you feel that the government by and large
[00:50:40.340 --> 00:50:43.340]   around the world has handled this flawlessly?
[00:50:43.340 --> 00:50:45.780]   - That's a really sad and disappointing reality
[00:50:45.780 --> 00:50:49.340]   that hopefully is a wake up call for everybody.
[00:50:49.340 --> 00:50:52.820]   For the scientists, for the engineers,
[00:50:52.820 --> 00:50:54.780]   for the researchers in AI especially.
[00:50:54.780 --> 00:50:59.780]   It was disappointing to see how inefficient we were
[00:50:59.780 --> 00:51:04.660]   at collecting the right amount of data
[00:51:04.660 --> 00:51:07.660]   in a privacy-preserving way and spreading that data
[00:51:07.660 --> 00:51:09.780]   and utilizing that data to make decisions,
[00:51:09.780 --> 00:51:10.980]   all that kind of stuff.
[00:51:10.980 --> 00:51:13.900]   - Yeah, I think when something bad happens to me,
[00:51:13.900 --> 00:51:17.780]   I made myself a promise many years ago
[00:51:17.780 --> 00:51:21.540]   that I would not be a whiner.
[00:51:22.420 --> 00:51:24.180]   And when something bad happens to me,
[00:51:24.180 --> 00:51:27.780]   of course it's a process of disappointment,
[00:51:27.780 --> 00:51:31.060]   but then I try to focus on what did I learn from this
[00:51:31.060 --> 00:51:33.100]   that can make me a better person in the future?
[00:51:33.100 --> 00:51:36.260]   And there's usually something to be learned when I fail.
[00:51:36.260 --> 00:51:38.740]   And I think we should all ask ourselves,
[00:51:38.740 --> 00:51:42.020]   what can we learn from the pandemic
[00:51:42.020 --> 00:51:43.460]   about how we can do better in the future?
[00:51:43.460 --> 00:51:46.420]   And you mentioned there a really good lesson.
[00:51:46.420 --> 00:51:49.540]   We were not as resilient as we thought we were
[00:51:50.540 --> 00:51:54.020]   and we were not as prepared maybe as we wish we were.
[00:51:54.020 --> 00:51:57.340]   You can even see very stark contrast around the planet.
[00:51:57.340 --> 00:52:01.820]   South Korea, they have over 50 million people.
[00:52:01.820 --> 00:52:03.060]   Do you know how many deaths they have
[00:52:03.060 --> 00:52:04.620]   from COVID last time I checked?
[00:52:04.620 --> 00:52:07.220]   - No. - It's about 500.
[00:52:07.220 --> 00:52:10.340]   Why is that?
[00:52:10.340 --> 00:52:15.340]   Well, the short answer is that they had prepared.
[00:52:15.340 --> 00:52:19.260]   They were incredibly quick,
[00:52:19.260 --> 00:52:23.500]   incredibly quick to get on it with very rapid testing
[00:52:23.500 --> 00:52:25.540]   and contact tracing and so on,
[00:52:25.540 --> 00:52:28.100]   which is why they never had more cases
[00:52:28.100 --> 00:52:30.100]   than they could contract trace effectively.
[00:52:30.100 --> 00:52:32.100]   They never even had to have the kind of big lockdowns
[00:52:32.100 --> 00:52:33.740]   we had in the West.
[00:52:33.740 --> 00:52:36.460]   But the deeper answer to it,
[00:52:36.460 --> 00:52:39.100]   it's not just the Koreans are just somehow better people.
[00:52:39.100 --> 00:52:40.860]   The reason I think they were better prepared
[00:52:40.860 --> 00:52:45.380]   was because they had already had a pretty bad hit
[00:52:45.380 --> 00:52:49.940]   from the SARS pandemic, which never became a pandemic,
[00:52:49.940 --> 00:52:52.100]   something like 17 years ago, I think.
[00:52:52.100 --> 00:52:53.700]   So it was a kind of fresh memory that,
[00:52:53.700 --> 00:52:55.980]   we need to be prepared for pandemics.
[00:52:55.980 --> 00:52:57.020]   So they were, right?
[00:52:57.020 --> 00:53:01.260]   And so maybe this is a lesson here
[00:53:01.260 --> 00:53:03.300]   for all of us to draw from COVID
[00:53:03.300 --> 00:53:06.340]   that rather than just wait for the next pandemic
[00:53:06.340 --> 00:53:09.820]   or the next problem with AI getting out of control
[00:53:09.820 --> 00:53:13.060]   or anything else, maybe we should just actually
[00:53:14.100 --> 00:53:16.820]   set aside a tiny fraction of our GDP
[00:53:16.820 --> 00:53:20.460]   to have people very systematically do some horizon scanning
[00:53:20.460 --> 00:53:23.340]   and say, okay, what are the things that could go wrong?
[00:53:23.340 --> 00:53:25.820]   And let's duke it out and see which are the more likely ones
[00:53:25.820 --> 00:53:28.780]   and which are the ones that are actually actionable
[00:53:28.780 --> 00:53:29.820]   and then be prepared.
[00:53:29.820 --> 00:53:36.540]   - So one of the observations as one little ant/human
[00:53:36.540 --> 00:53:40.100]   that I am of disappointment is the political division
[00:53:40.940 --> 00:53:45.540]   over information that has been observed,
[00:53:45.540 --> 00:53:48.940]   that I observed this year, that it seemed
[00:53:48.940 --> 00:53:50.860]   the discussion was less about
[00:53:50.860 --> 00:53:59.020]   sort of what happened and understanding what happened deeply
[00:53:59.020 --> 00:54:04.060]   and more about there's different truths out there.
[00:54:04.060 --> 00:54:05.420]   And it's like a argument,
[00:54:05.420 --> 00:54:07.620]   my truth is better than your truth.
[00:54:07.620 --> 00:54:10.460]   And it's like red versus blue or different,
[00:54:10.460 --> 00:54:13.260]   like it was like this ridiculous discourse
[00:54:13.260 --> 00:54:16.540]   that doesn't seem to get at any kind of notion of the truth.
[00:54:16.540 --> 00:54:18.980]   It's not like some kind of scientific process.
[00:54:18.980 --> 00:54:21.020]   Even science got politicized in ways
[00:54:21.020 --> 00:54:23.500]   that's very heartbreaking to me.
[00:54:23.500 --> 00:54:28.660]   You have an exciting project on the AI front
[00:54:28.660 --> 00:54:33.660]   of trying to rethink, you mentioned corporations,
[00:54:33.660 --> 00:54:37.340]   there's one of the other collective intelligence systems
[00:54:37.340 --> 00:54:40.540]   that have emerged through all of this is social networks.
[00:54:40.540 --> 00:54:42.580]   And just the spread, the internet,
[00:54:42.580 --> 00:54:46.380]   is the spread of information on the internet,
[00:54:46.380 --> 00:54:48.300]   our ability to share that information,
[00:54:48.300 --> 00:54:50.620]   there's all different kinds of news sources and so on.
[00:54:50.620 --> 00:54:53.180]   And so you said like that's from first principles,
[00:54:53.180 --> 00:54:57.300]   let's rethink how we think about the news,
[00:54:57.300 --> 00:54:59.060]   how we think about information.
[00:54:59.060 --> 00:55:02.500]   Can you talk about this amazing effort
[00:55:02.500 --> 00:55:03.660]   that you're undertaking?
[00:55:03.660 --> 00:55:04.580]   - Oh, I'd love to.
[00:55:04.580 --> 00:55:06.420]   This has been my big COVID project.
[00:55:06.420 --> 00:55:10.740]   I've spent nights and weekends on ever since the lockdown.
[00:55:10.740 --> 00:55:13.140]   To segue into this actually,
[00:55:13.140 --> 00:55:14.540]   let me come back to what you said earlier,
[00:55:14.540 --> 00:55:17.100]   that you had this hope that in your experience,
[00:55:17.100 --> 00:55:18.860]   people who you felt were very talented
[00:55:18.860 --> 00:55:21.260]   or often idealistic and wanted to do good.
[00:55:21.260 --> 00:55:26.060]   Frankly, I feel the same about all people by and large.
[00:55:26.060 --> 00:55:27.140]   There are always exceptions,
[00:55:27.140 --> 00:55:29.420]   but I think the vast majority of everybody,
[00:55:29.420 --> 00:55:31.380]   regardless of education and whatnot,
[00:55:31.380 --> 00:55:34.260]   really are fundamentally good, right?
[00:55:34.260 --> 00:55:36.620]   So how can it be that people still do
[00:55:36.620 --> 00:55:39.060]   so much nasty stuff, right?
[00:55:39.060 --> 00:55:41.060]   I think it has everything to do with this,
[00:55:41.060 --> 00:55:43.700]   with the information that we're given.
[00:55:43.700 --> 00:55:47.140]   If you go into Sweden 500 years ago
[00:55:47.140 --> 00:55:48.420]   and you start telling all the farmers
[00:55:48.420 --> 00:55:51.900]   that those Danes in Denmark, they're so terrible people,
[00:55:51.900 --> 00:55:53.460]   and we have to invade them
[00:55:53.460 --> 00:55:56.340]   because they've done all these terrible things
[00:55:56.340 --> 00:55:58.300]   that you can't fact check yourself.
[00:55:58.300 --> 00:56:00.740]   A lot of people, Swedes did that, right?
[00:56:00.740 --> 00:56:05.740]   And we're seeing so much of this today in the world,
[00:56:05.740 --> 00:56:11.700]   both geopolitically, where we are told
[00:56:11.700 --> 00:56:15.100]   that China is bad and Russia is bad and Venezuela is bad,
[00:56:15.100 --> 00:56:17.100]   and people in those countries are often told
[00:56:17.100 --> 00:56:18.380]   that we are bad.
[00:56:18.380 --> 00:56:21.820]   And we also see it at a micro level,
[00:56:21.820 --> 00:56:22.980]   where people are told that,
[00:56:22.980 --> 00:56:25.740]   "Oh, those who voted for the other party are bad people."
[00:56:25.740 --> 00:56:27.540]   It's not just an intellectual disagreement,
[00:56:27.540 --> 00:56:30.660]   but they're bad people.
[00:56:30.660 --> 00:56:33.820]   And we're getting ever more divided.
[00:56:33.820 --> 00:56:35.860]   And so how do you reconcile this
[00:56:35.860 --> 00:56:40.300]   with this intrinsic goodness in people?
[00:56:40.300 --> 00:56:42.620]   I think it's pretty obvious that it has, again,
[00:56:42.620 --> 00:56:44.500]   to do with this, with the information
[00:56:44.500 --> 00:56:47.300]   that we're fed and given, right?
[00:56:47.300 --> 00:56:50.620]   We evolved to live in small groups
[00:56:50.620 --> 00:56:52.900]   where you might know 30 people in total, right?
[00:56:52.900 --> 00:56:56.300]   So you then had a system that was quite good
[00:56:56.300 --> 00:56:58.540]   for assessing who you could trust and who you could not.
[00:56:58.540 --> 00:57:03.500]   And if someone told you that Joe there is a jerk,
[00:57:03.500 --> 00:57:05.780]   but you had interacted with him yourself
[00:57:05.780 --> 00:57:07.300]   and seen him in action,
[00:57:07.300 --> 00:57:09.100]   and you would quickly realize maybe
[00:57:09.100 --> 00:57:12.460]   that that's actually not quite accurate, right?
[00:57:12.460 --> 00:57:14.260]   But now that the most people on the planet
[00:57:14.260 --> 00:57:16.020]   are people who've never met,
[00:57:16.020 --> 00:57:17.220]   it's very important that we have a way
[00:57:17.220 --> 00:57:19.460]   of trusting the information we're given.
[00:57:19.460 --> 00:57:23.180]   So, okay, so where does the news project come in?
[00:57:23.180 --> 00:57:26.580]   Well, throughout history, you can go read Machiavelli
[00:57:26.580 --> 00:57:28.700]   from the 1400s and you'll see how already then
[00:57:28.700 --> 00:57:30.100]   they were busy manipulating people
[00:57:30.100 --> 00:57:31.660]   with propaganda and stuff.
[00:57:31.660 --> 00:57:35.740]   Propaganda is not new at all.
[00:57:35.740 --> 00:57:37.740]   And the incentives to manipulate people
[00:57:37.740 --> 00:57:40.060]   is just not new at all.
[00:57:40.060 --> 00:57:41.260]   What is it that's new?
[00:57:41.260 --> 00:57:45.780]   What's new is machine learning meets propaganda.
[00:57:45.780 --> 00:57:46.820]   That's what's new.
[00:57:46.820 --> 00:57:49.100]   That's why this has gotten so much worse.
[00:57:49.100 --> 00:57:51.740]   Some people like to blame certain individuals,
[00:57:51.740 --> 00:57:54.220]   like in my liberal university bubble,
[00:57:54.220 --> 00:57:56.980]   many people blame Donald Trump and say it was his fault.
[00:57:56.980 --> 00:57:59.220]   I see it differently.
[00:57:59.220 --> 00:58:04.860]   I think Donald Trump just had this extreme skill
[00:58:04.860 --> 00:58:08.620]   at playing this game in the machine learning algorithm age,
[00:58:08.620 --> 00:58:12.740]   a game he couldn't have played 10 years ago.
[00:58:12.740 --> 00:58:13.700]   So what's changed?
[00:58:13.700 --> 00:58:16.020]   What's changed is, well, Facebook and Google
[00:58:16.020 --> 00:58:19.420]   and other companies, and I'm not badmouthing them,
[00:58:19.420 --> 00:58:21.660]   I have a lot of friends who work for these companies,
[00:58:21.660 --> 00:58:25.540]   good people, they deployed machine learning algorithms
[00:58:25.540 --> 00:58:27.100]   just to increase their profit a little bit,
[00:58:27.100 --> 00:58:31.300]   to just maximize the time people spent watching ads.
[00:58:31.300 --> 00:58:33.300]   And they had totally underestimated
[00:58:33.300 --> 00:58:35.100]   how effective they were gonna be.
[00:58:35.100 --> 00:58:36.900]   This was, again, the black box,
[00:58:36.900 --> 00:58:38.940]   non-intelligible intelligence.
[00:58:38.940 --> 00:58:42.220]   They just noticed, oh, we're getting more ad revenue, great.
[00:58:42.220 --> 00:58:44.860]   It took a long time until they even realized why and how
[00:58:44.860 --> 00:58:48.140]   and how damaging this was for society.
[00:58:48.140 --> 00:58:51.420]   'Cause of course, what the machine learning figured out was
[00:58:51.420 --> 00:58:54.500]   that the by far most effective way of gluing you
[00:58:54.500 --> 00:58:57.420]   to your little rectangle was to show you things
[00:58:57.420 --> 00:59:01.780]   that triggered strong emotions, anger, et cetera, resentment.
[00:59:01.780 --> 00:59:07.700]   And if it was true or not, didn't really matter.
[00:59:07.700 --> 00:59:10.580]   It was also easier to find stories that weren't true.
[00:59:10.580 --> 00:59:12.140]   If you weren't limited, that's just a limitation.
[00:59:12.140 --> 00:59:15.340]   - Right, that's a very limiting factor.
[00:59:15.340 --> 00:59:19.980]   - And before long, we got these amazing filter bubbles
[00:59:19.980 --> 00:59:21.940]   on a scale we had never seen before.
[00:59:21.940 --> 00:59:23.820]   A couple of this to the fact that also
[00:59:23.820 --> 00:59:28.540]   the online news media was so effective
[00:59:28.540 --> 00:59:30.780]   that they killed a lot of print journalism.
[00:59:30.780 --> 00:59:35.180]   There's less than half as many journalists now in America,
[00:59:35.180 --> 00:59:39.300]   I believe, as there was a generation ago.
[00:59:39.300 --> 00:59:42.780]   You just couldn't compete with the online advertising.
[00:59:42.780 --> 00:59:47.580]   So all of a sudden, most people are not getting,
[00:59:47.580 --> 00:59:48.660]   even reading newspapers.
[00:59:48.660 --> 00:59:51.300]   They get their news from social media.
[00:59:51.300 --> 00:59:54.980]   And most people only get news in their little bubble.
[00:59:54.980 --> 00:59:58.460]   So along comes now some people like Donald Trump
[00:59:58.460 --> 01:00:01.580]   who figured out, among the first successful politicians
[01:00:01.580 --> 01:00:04.180]   to figure out how to really play this new game
[01:00:04.180 --> 01:00:06.020]   and become very, very influential.
[01:00:06.020 --> 01:00:08.580]   But I think Donald Trump was a simple,
[01:00:08.580 --> 01:00:11.020]   well, he took advantage of it.
[01:00:11.020 --> 01:00:15.020]   He didn't create, the fundamental conditions were created
[01:00:15.020 --> 01:00:19.020]   by machine learning taking over the news media.
[01:00:19.020 --> 01:00:22.940]   So this is what motivated my little COVID project here.
[01:00:22.940 --> 01:00:27.140]   So I said before, machine learning and tech in general,
[01:00:27.140 --> 01:00:29.060]   it's not evil, but it's also not good.
[01:00:29.060 --> 01:00:31.580]   It's just a tool that you can use
[01:00:31.580 --> 01:00:32.700]   for good things or bad things.
[01:00:32.700 --> 01:00:36.020]   And as it happens, machine learning and news
[01:00:36.020 --> 01:00:39.700]   was mainly used by the big players, big tech,
[01:00:39.700 --> 01:00:43.220]   to manipulate people into watch as many ads as possible,
[01:00:43.220 --> 01:00:44.860]   which had this unintended consequence
[01:00:44.860 --> 01:00:46.660]   of really screwing up our democracy
[01:00:46.660 --> 01:00:49.500]   and fragmenting it into filter bubbles.
[01:00:49.500 --> 01:00:53.180]   So I thought, well, machine learning algorithms
[01:00:53.180 --> 01:00:54.260]   were basically free.
[01:00:54.260 --> 01:00:56.220]   They can run on your smartphone for free also
[01:00:56.220 --> 01:00:57.880]   if someone gives them away to you, right?
[01:00:57.880 --> 01:01:00.740]   There's no reason why they only have to help the big guy
[01:01:00.740 --> 01:01:03.020]   to manipulate the little guy.
[01:01:03.020 --> 01:01:05.300]   They can just as well help the little guy
[01:01:05.300 --> 01:01:07.900]   to see through all the manipulation attempts
[01:01:07.900 --> 01:01:08.740]   from the big guy.
[01:01:08.740 --> 01:01:10.660]   So did this project, it's called,
[01:01:10.660 --> 01:01:12.820]   you can go to improvethenews.org.
[01:01:12.820 --> 01:01:14.300]   The first thing we've built
[01:01:14.300 --> 01:01:16.620]   is this little news aggregator.
[01:01:16.620 --> 01:01:17.860]   Looks a bit like Google News,
[01:01:17.860 --> 01:01:19.420]   except it has these sliders on it
[01:01:19.420 --> 01:01:21.740]   to help you break out of your filter bubble.
[01:01:21.740 --> 01:01:24.420]   So if you're reading, you can click, click,
[01:01:24.420 --> 01:01:25.900]   and go to your favorite topic.
[01:01:25.900 --> 01:01:31.460]   And then if you just slide the left-right slider
[01:01:31.460 --> 01:01:32.580]   all the way over to the left.
[01:01:32.580 --> 01:01:33.700]   - There's two sliders, right?
[01:01:33.700 --> 01:01:36.280]   - Yeah, there's the one, the most obvious one
[01:01:36.280 --> 01:01:38.780]   is the one that has left-right labeled on it.
[01:01:38.780 --> 01:01:40.580]   You go to left, you get one set of articles,
[01:01:40.580 --> 01:01:43.380]   you go to the right, you see a very different truth
[01:01:43.380 --> 01:01:44.220]   appearing.
[01:01:44.220 --> 01:01:46.700]   - Oh, that's literally left and right on the--
[01:01:46.700 --> 01:01:47.660]   - Political spectrum.
[01:01:47.660 --> 01:01:48.500]   - On the political spectrum.
[01:01:48.500 --> 01:01:50.780]   - Yeah, so if you're reading about immigration,
[01:01:50.780 --> 01:01:55.580]   for example, it's very, very noticeable.
[01:01:55.580 --> 01:01:57.100]   And I think step one, always,
[01:01:57.100 --> 01:01:59.260]   if you wanna not get manipulated,
[01:01:59.260 --> 01:02:02.980]   is just to be able to recognize the techniques people use.
[01:02:02.980 --> 01:02:04.940]   So it's very helpful to just see
[01:02:04.940 --> 01:02:06.800]   how they spin things on the two sides.
[01:02:08.200 --> 01:02:11.280]   I think many people are under the misconception
[01:02:11.280 --> 01:02:14.120]   that the main problem is fake news.
[01:02:14.120 --> 01:02:14.960]   It's not.
[01:02:14.960 --> 01:02:18.560]   I had an amazing team of MIT students
[01:02:18.560 --> 01:02:20.280]   where we did an academic project
[01:02:20.280 --> 01:02:22.240]   to use machine learning to detect
[01:02:22.240 --> 01:02:23.920]   the main kinds of bias over the summer.
[01:02:23.920 --> 01:02:26.640]   And yes, of course, sometimes there's fake news
[01:02:26.640 --> 01:02:30.800]   where someone just claims something that's false, right?
[01:02:30.800 --> 01:02:33.840]   Like, oh, Hillary Clinton just got divorced or something.
[01:02:33.840 --> 01:02:37.140]   But what we see much more of is actually just omissions.
[01:02:38.140 --> 01:02:41.700]   - If you go to, there's some stories
[01:02:41.700 --> 01:02:45.500]   which just won't be mentioned by the left or the right
[01:02:45.500 --> 01:02:47.420]   because it doesn't suit their agenda.
[01:02:47.420 --> 01:02:50.680]   And then they'll mention other ones very, very, very much.
[01:02:50.680 --> 01:02:55.680]   So for example, we've had a number of stories
[01:02:55.680 --> 01:02:59.600]   about the Trump family's financial dealings.
[01:02:59.600 --> 01:03:02.540]   And then there's been some stories
[01:03:02.540 --> 01:03:06.240]   about the Biden family's, Hunter Biden's financial dealings.
[01:03:06.240 --> 01:03:08.540]   Surprise, surprise, they don't get equal coverage
[01:03:08.540 --> 01:03:10.420]   on the left and the right.
[01:03:10.420 --> 01:03:14.300]   One side loves to cover Hunter Biden's stuff
[01:03:14.300 --> 01:03:16.380]   and one side loves to cover the Trump.
[01:03:16.380 --> 01:03:18.940]   You can never guess which is which, right?
[01:03:18.940 --> 01:03:22.440]   But the great news is if you're a normal American citizen
[01:03:22.440 --> 01:03:25.000]   and you dislike corruption in all its forms,
[01:03:25.000 --> 01:03:29.420]   then slide, slide, you can just look at both sides
[01:03:29.420 --> 01:03:33.340]   and you'll see all those political corruption stories.
[01:03:33.340 --> 01:03:38.340]   It's really liberating to just take in the both sides,
[01:03:38.340 --> 01:03:40.480]   the spin on both sides.
[01:03:40.480 --> 01:03:43.800]   It somehow unlocks your mind to think on your own,
[01:03:43.800 --> 01:03:47.080]   to realize that, I don't know,
[01:03:47.080 --> 01:03:49.160]   it's the same thing that was useful
[01:03:49.160 --> 01:03:53.880]   in the Soviet Union times for when everybody
[01:03:53.880 --> 01:03:58.080]   was much more aware that they're surrounded by propaganda.
[01:03:58.080 --> 01:04:01.560]   - That is so interesting what you're saying, actually.
[01:04:01.560 --> 01:04:04.980]   So Noam Chomsky, used to be our MIT colleague,
[01:04:04.980 --> 01:04:08.460]   once said that propaganda is to democracy
[01:04:08.460 --> 01:04:12.740]   what violence is to totalitarianism.
[01:04:12.740 --> 01:04:15.060]   And what he means by that is
[01:04:15.060 --> 01:04:17.460]   if you have a really totalitarian government,
[01:04:17.460 --> 01:04:18.820]   you don't need propaganda.
[01:04:18.820 --> 01:04:23.740]   People will do what you want them to do anyway
[01:04:23.740 --> 01:04:25.140]   out of fear, right?
[01:04:25.140 --> 01:04:28.860]   But otherwise, you need propaganda.
[01:04:28.860 --> 01:04:30.700]   So I would say actually that the propaganda
[01:04:30.700 --> 01:04:33.320]   is much higher quality in democracies,
[01:04:33.320 --> 01:04:34.800]   much more believable.
[01:04:34.800 --> 01:04:36.400]   And it's really-- - That's brilliant.
[01:04:36.400 --> 01:04:37.520]   - It's really striking.
[01:04:37.520 --> 01:04:40.040]   When I talk to colleagues, science colleagues,
[01:04:40.040 --> 01:04:41.880]   like from Russia and China and so on,
[01:04:41.880 --> 01:04:45.680]   I notice they are actually much more aware
[01:04:45.680 --> 01:04:47.760]   of the propaganda in their own media
[01:04:47.760 --> 01:04:49.400]   than many of my American colleagues are
[01:04:49.400 --> 01:04:51.680]   about the propaganda in Western media.
[01:04:51.680 --> 01:04:52.520]   - That's brilliant.
[01:04:52.520 --> 01:04:54.400]   That means the propaganda in the Western media
[01:04:54.400 --> 01:04:56.080]   is just better. - Yes!
[01:04:56.080 --> 01:04:57.360]   - That's so brilliant. - Everything's better
[01:04:57.360 --> 01:04:58.800]   in the West, even the propaganda.
[01:04:58.800 --> 01:05:00.640]   (laughing)
[01:05:00.640 --> 01:05:02.400]   - So, but there's--
[01:05:02.400 --> 01:05:05.080]   (laughing)
[01:05:05.080 --> 01:05:05.920]   - That's good.
[01:05:05.920 --> 01:05:07.420]   - But once you realize that,
[01:05:07.420 --> 01:05:09.340]   you realize there's also something very optimistic there
[01:05:09.340 --> 01:05:10.500]   that you can do about it, right?
[01:05:10.500 --> 01:05:12.980]   Because first of all, omissions,
[01:05:12.980 --> 01:05:16.960]   as long as there's no outright censorship,
[01:05:16.960 --> 01:05:18.540]   you can just look at both sides
[01:05:18.540 --> 01:05:22.780]   and pretty quickly piece together
[01:05:22.780 --> 01:05:26.180]   a much more accurate idea of what's actually going on, right?
[01:05:26.180 --> 01:05:28.100]   - And develop a natural skepticism, too.
[01:05:28.100 --> 01:05:28.940]   - Yeah, yeah.
[01:05:28.940 --> 01:05:31.660]   - Develop an analytical, scientific mind
[01:05:31.660 --> 01:05:32.940]   about the way you're taking the information.
[01:05:32.940 --> 01:05:35.540]   - Yeah, and I think, I have to say,
[01:05:35.540 --> 01:05:38.540]   sometimes I feel that some of us in the academic bubble
[01:05:38.540 --> 01:05:41.500]   are too arrogant about this and somehow think,
[01:05:41.500 --> 01:05:44.660]   oh, it's just people who aren't as educated
[01:05:44.660 --> 01:05:45.860]   as us, who are fooled.
[01:05:45.860 --> 01:05:48.260]   When we are often just as gullible also,
[01:05:48.260 --> 01:05:52.140]   we read only our media and don't see through things.
[01:05:52.140 --> 01:05:54.620]   Anyone who looks at both sides like this in comparison,
[01:05:54.620 --> 01:05:56.380]   well, we immediately start noticing
[01:05:56.380 --> 01:05:58.100]   the shenanigans being pulled.
[01:05:58.100 --> 01:06:01.900]   And I think what I tried to do with this app
[01:06:01.900 --> 01:06:05.820]   is that the big tech has to some extent
[01:06:05.820 --> 01:06:09.020]   tried to blame the individual for being manipulated,
[01:06:09.020 --> 01:06:12.360]   much like big tobacco tried to blame the individuals
[01:06:12.360 --> 01:06:13.740]   entirely for smoking.
[01:06:13.740 --> 01:06:16.940]   And then later on, our government stepped up and said,
[01:06:16.940 --> 01:06:19.620]   actually, you can't just blame little kids
[01:06:19.620 --> 01:06:20.460]   for starting to smoke.
[01:06:20.460 --> 01:06:22.440]   We have to have more responsible advertising
[01:06:22.440 --> 01:06:23.540]   and this and that.
[01:06:23.540 --> 01:06:24.660]   I think it's a bit the same here.
[01:06:24.660 --> 01:06:27.620]   It's very convenient for a big tech to blame.
[01:06:27.620 --> 01:06:30.120]   So it's just people who are so dumb and get fooled.
[01:06:30.120 --> 01:06:34.180]   The blame usually comes in saying,
[01:06:34.180 --> 01:06:36.000]   oh, it's just human psychology.
[01:06:36.000 --> 01:06:38.360]   People just wanna hear what they already believe.
[01:06:38.360 --> 01:06:43.140]   But Professor David Rand at MIT actually partly debunked that
[01:06:43.140 --> 01:06:45.300]   with a really nice study showing that people
[01:06:45.300 --> 01:06:47.620]   tend to be interested in hearing things
[01:06:47.620 --> 01:06:49.900]   that go against what they believe
[01:06:49.900 --> 01:06:52.700]   if it's presented in a respectful way.
[01:06:52.700 --> 01:06:57.560]   Suppose, for example, that you have a company
[01:06:57.560 --> 01:06:59.180]   and you're just about to launch this project
[01:06:59.180 --> 01:07:00.340]   and you're convinced it's gonna work.
[01:07:00.340 --> 01:07:04.380]   And someone says, you know, Lex, I hate to tell you this,
[01:07:04.380 --> 01:07:06.700]   but this is gonna fail and here's why.
[01:07:06.700 --> 01:07:08.940]   Would you be like, shut up, I don't wanna hear it.
[01:07:08.940 --> 01:07:10.660]   La la la la la la la la la.
[01:07:10.660 --> 01:07:11.500]   Would you?
[01:07:11.500 --> 01:07:13.020]   You would be interested, right?
[01:07:13.020 --> 01:07:15.520]   And also, if you're on an airplane,
[01:07:15.520 --> 01:07:19.060]   back in the pre-COVID times, you know,
[01:07:19.060 --> 01:07:22.900]   and the guy next to you is clearly from the opposite side
[01:07:22.900 --> 01:07:26.780]   of the political spectrum, but is very respectful
[01:07:26.780 --> 01:07:29.040]   and polite to you, wouldn't you be kind of interested
[01:07:29.040 --> 01:07:32.940]   to hear a bit about how he or she thinks about things?
[01:07:32.940 --> 01:07:33.900]   - Of course.
[01:07:33.900 --> 01:07:36.380]   - But it's not so easy to find out
[01:07:36.380 --> 01:07:38.880]   respectful disagreement now because, for example,
[01:07:38.880 --> 01:07:42.020]   if you are a Democrat and you're like,
[01:07:42.020 --> 01:07:43.640]   oh, I wanna see something on the other side,
[01:07:43.640 --> 01:07:46.160]   so you just go Breitbart.com.
[01:07:46.160 --> 01:07:48.040]   And then after the first 10 seconds,
[01:07:48.040 --> 01:07:50.260]   you feel deeply insulted by something.
[01:07:50.260 --> 01:07:53.440]   It's not gonna work.
[01:07:53.440 --> 01:07:55.540]   Or if you take someone who votes Republican
[01:07:56.620 --> 01:07:58.380]   and they go to something on the left
[01:07:58.380 --> 01:08:01.100]   and they just get very offended very quickly
[01:08:01.100 --> 01:08:03.180]   by them having put a deliberately ugly picture
[01:08:03.180 --> 01:08:05.260]   of Donald Trump on the front page or something,
[01:08:05.260 --> 01:08:06.580]   it doesn't really work.
[01:08:06.580 --> 01:08:10.760]   So this news aggregator also has a nuance slider,
[01:08:10.760 --> 01:08:13.300]   which you can pull to the right and then,
[01:08:13.300 --> 01:08:16.000]   to make it easier to get exposed to actually more
[01:08:16.000 --> 01:08:18.140]   sort of academic style or more respectful
[01:08:18.140 --> 01:08:22.300]   portrayals of different views.
[01:08:22.300 --> 01:08:25.420]   And finally, the one kind of bias I think
[01:08:25.420 --> 01:08:28.340]   people are mostly aware of is the left-right,
[01:08:28.340 --> 01:08:29.180]   because it's so obvious,
[01:08:29.180 --> 01:08:33.420]   because both left and right are very powerful here.
[01:08:33.420 --> 01:08:36.760]   Both of them have well-funded TV stations and newspapers,
[01:08:36.760 --> 01:08:38.380]   and it's kind of hard to miss.
[01:08:38.380 --> 01:08:41.940]   But there's another one, the establishment slider,
[01:08:41.940 --> 01:08:44.100]   which is also really fun.
[01:08:44.100 --> 01:08:45.820]   I love to play with it.
[01:08:45.820 --> 01:08:47.620]   That's more about corruption.
[01:08:47.620 --> 01:08:51.380]   Because if you have a society
[01:08:53.260 --> 01:08:57.340]   where almost all the powerful entities
[01:08:57.340 --> 01:08:59.500]   want you to believe a certain thing,
[01:08:59.500 --> 01:09:01.860]   that's what you're gonna read in both the big media,
[01:09:01.860 --> 01:09:04.620]   mainstream media on the left and on the right, of course.
[01:09:04.620 --> 01:09:08.220]   And powerful companies can push back very hard,
[01:09:08.220 --> 01:09:10.820]   like tobacco companies pushed back very hard back in the day
[01:09:10.820 --> 01:09:13.540]   when some newspapers started writing articles
[01:09:13.540 --> 01:09:15.380]   about tobacco being dangerous.
[01:09:15.380 --> 01:09:18.420]   So it was hard to get a lot of coverage about it initially.
[01:09:18.420 --> 01:09:20.860]   And also if you look geopolitically, right?
[01:09:20.860 --> 01:09:23.100]   Of course, in any country, when you read their media,
[01:09:23.100 --> 01:09:24.900]   you're mainly gonna be reading a lot of articles
[01:09:24.900 --> 01:09:27.380]   about how our country is the good guy,
[01:09:27.380 --> 01:09:30.380]   and the other countries are the bad guys, right?
[01:09:30.380 --> 01:09:33.340]   So if you wanna have a really more nuanced understanding,
[01:09:33.340 --> 01:09:36.500]   like the Germans used to be told that the,
[01:09:36.500 --> 01:09:37.580]   the British used to be told
[01:09:37.580 --> 01:09:38.820]   that the French were the bad guys,
[01:09:38.820 --> 01:09:39.860]   and the French used to be told
[01:09:39.860 --> 01:09:41.900]   that the British were the bad guys.
[01:09:41.900 --> 01:09:45.700]   Now they visit each other's countries a lot
[01:09:45.700 --> 01:09:47.380]   and have a much more nuanced understanding.
[01:09:47.380 --> 01:09:48.860]   I don't think there's gonna be any more wars
[01:09:48.860 --> 01:09:50.180]   between France and Germany.
[01:09:50.340 --> 01:09:54.260]   On the geopolitical scale, it's just as much as ever,
[01:09:54.260 --> 01:09:57.620]   you know, big Cold War now, US, China, and so on.
[01:09:57.620 --> 01:10:01.180]   And if you wanna get a more nuanced understanding
[01:10:01.180 --> 01:10:03.500]   of what's happening geopolitically,
[01:10:03.500 --> 01:10:05.940]   then it's really fun to look at this establishment slider,
[01:10:05.940 --> 01:10:09.380]   because it turns out there are tons of little newspapers,
[01:10:09.380 --> 01:10:11.340]   both on the left and on the right,
[01:10:11.340 --> 01:10:14.460]   who sometimes challenge establishment and say,
[01:10:14.460 --> 01:10:17.780]   you know, maybe we shouldn't actually invade Iraq right now.
[01:10:17.780 --> 01:10:20.380]   Maybe this weapons of mass destruction thing is BS.
[01:10:20.380 --> 01:10:23.660]   If you look at the journalism research afterwards,
[01:10:23.660 --> 01:10:25.140]   you can actually see that quite clearly,
[01:10:25.140 --> 01:10:29.180]   that both CNN and Fox were very pro,
[01:10:29.180 --> 01:10:30.620]   let's get rid of Saddam,
[01:10:30.620 --> 01:10:32.580]   there are weapons of mass destruction.
[01:10:32.580 --> 01:10:34.660]   Then there were a lot of smaller newspapers.
[01:10:34.660 --> 01:10:36.200]   They were like, wait a minute,
[01:10:36.200 --> 01:10:39.340]   this evidence seems a bit sketchy, and maybe we,
[01:10:39.340 --> 01:10:42.260]   but of course, they were so hard to find.
[01:10:42.260 --> 01:10:44.580]   Most people didn't even know they existed, right?
[01:10:44.580 --> 01:10:47.420]   Yet it would have been better for American national security
[01:10:47.420 --> 01:10:50.140]   if those voices had also come up.
[01:10:50.140 --> 01:10:52.540]   I think it harmed America's national security, actually,
[01:10:52.540 --> 01:10:53.780]   that we invaded Iraq.
[01:10:53.780 --> 01:10:55.540]   - And arguably, there's a lot more interest
[01:10:55.540 --> 01:11:00.460]   in that kind of thinking, too, from those small sources.
[01:11:00.460 --> 01:11:02.620]   So like, when you say big,
[01:11:02.620 --> 01:11:07.620]   it's more about kind of the reach of the broadcast,
[01:11:07.620 --> 01:11:12.060]   but it's not big in terms of the interest.
[01:11:12.060 --> 01:11:14.100]   I think there's a lot of interest
[01:11:14.100 --> 01:11:16.220]   in that kind of anti-establishment,
[01:11:16.220 --> 01:11:20.380]   or like skepticism towards out-of-the-box thinking.
[01:11:20.380 --> 01:11:22.020]   There's a lot of interest in that kind of thing.
[01:11:22.020 --> 01:11:26.900]   Do you see this news project or something like it
[01:11:26.900 --> 01:11:30.580]   being basically taken over the world
[01:11:30.580 --> 01:11:32.940]   as the main way we consume information?
[01:11:32.940 --> 01:11:35.140]   Like, how do we get there?
[01:11:35.140 --> 01:11:37.340]   Like, how do we, you know?
[01:11:37.340 --> 01:11:39.100]   So, okay, the idea is brilliant.
[01:11:39.100 --> 01:11:43.980]   You're calling it your little project in 2020,
[01:11:43.980 --> 01:11:48.500]   but how does that become the new way we consume information?
[01:11:48.500 --> 01:11:51.020]   - I hope, first of all, just to plant a little seed there,
[01:11:51.020 --> 01:11:55.340]   because normally, the big barrier of doing anything
[01:11:55.340 --> 01:11:57.260]   in media is you need a ton of money,
[01:11:57.260 --> 01:11:59.300]   but this costs no money at all.
[01:11:59.300 --> 01:12:01.100]   I've just been paying myself,
[01:12:01.100 --> 01:12:03.100]   pay a tiny amount of money each month to Amazon
[01:12:03.100 --> 01:12:04.700]   to run the thing in their cloud.
[01:12:04.700 --> 01:12:06.940]   There will never be any ads.
[01:12:06.940 --> 01:12:09.340]   The point is not to make any money off of it,
[01:12:09.340 --> 01:12:11.660]   and we just train machine learning algorithms
[01:12:11.660 --> 01:12:13.180]   to classify the articles and stuff,
[01:12:13.180 --> 01:12:14.860]   so it just kind of runs by itself.
[01:12:14.860 --> 01:12:17.740]   So if it actually gets good enough at some point
[01:12:17.740 --> 01:12:20.700]   that it starts catching on, it could scale,
[01:12:20.700 --> 01:12:23.100]   and if other people carbon copy it
[01:12:23.100 --> 01:12:24.940]   and make other versions that are better,
[01:12:24.940 --> 01:12:28.180]   that's the more the merrier.
[01:12:28.180 --> 01:12:32.900]   I think there's a real opportunity for machine learning
[01:12:32.900 --> 01:12:35.220]   to empower the individual
[01:12:35.220 --> 01:12:38.860]   against the list of the powerful players.
[01:12:38.860 --> 01:12:41.420]   As I said in the beginning here,
[01:12:41.420 --> 01:12:43.140]   it's been mostly the other way around so far.
[01:12:43.140 --> 01:12:44.860]   The big players have the AI,
[01:12:44.860 --> 01:12:48.100]   and then they tell people, "This is the truth.
[01:12:48.100 --> 01:12:49.660]   This is how it is,"
[01:12:49.660 --> 01:12:52.220]   but it can just as well go the other way around.
[01:12:52.220 --> 01:12:53.860]   And when the internet was born, actually,
[01:12:53.860 --> 01:12:54.980]   a lot of people had this hope
[01:12:54.980 --> 01:12:57.540]   that maybe this will be a great thing for democracy,
[01:12:57.540 --> 01:12:59.620]   make it easier to find out about things,
[01:12:59.620 --> 01:13:01.420]   and maybe machine learning and things like this
[01:13:01.420 --> 01:13:03.780]   can actually help again.
[01:13:03.780 --> 01:13:07.140]   And I have to say, I think it's more important than ever now
[01:13:07.140 --> 01:13:12.140]   because this is very linked also to the whole future of life
[01:13:12.180 --> 01:13:13.900]   as we discussed earlier.
[01:13:13.900 --> 01:13:16.060]   We're getting this ever more powerful tech.
[01:13:16.060 --> 01:13:19.980]   Frank, it's pretty clear if you look on the one
[01:13:19.980 --> 01:13:21.940]   or two generation, three generation timescale
[01:13:21.940 --> 01:13:24.940]   that there are only two ways this can end geopolitically.
[01:13:24.940 --> 01:13:27.740]   Either it ends great for all humanity
[01:13:27.740 --> 01:13:31.700]   or it ends terribly for all of us.
[01:13:31.700 --> 01:13:33.780]   There's really no in between.
[01:13:33.780 --> 01:13:36.420]   And we're so stuck in,
[01:13:36.420 --> 01:13:39.180]   because technology knows no borders,
[01:13:39.180 --> 01:13:42.300]   and you can't have people fighting
[01:13:42.300 --> 01:13:45.220]   when the weapons just keep getting ever more powerful
[01:13:45.220 --> 01:13:47.100]   indefinitely.
[01:13:47.100 --> 01:13:50.420]   Eventually, the luck runs out.
[01:13:50.420 --> 01:13:55.420]   And right now we have, I love America,
[01:13:55.420 --> 01:13:59.900]   but the fact of the matter is what's good for America
[01:13:59.900 --> 01:14:01.740]   is not opposite in the long term
[01:14:01.740 --> 01:14:03.660]   to what's good for other countries.
[01:14:03.660 --> 01:14:07.420]   It would be if this was some sort of zero-sum game,
[01:14:07.420 --> 01:14:10.060]   like it was thousands of years ago
[01:14:10.060 --> 01:14:13.300]   when the only way one country could get more resources
[01:14:13.300 --> 01:14:15.020]   was to take land from other countries,
[01:14:15.020 --> 01:14:17.740]   'cause that was basically the resource.
[01:14:17.740 --> 01:14:18.940]   Look at the map of Europe.
[01:14:18.940 --> 01:14:21.420]   Some countries kept getting bigger and smaller,
[01:14:21.420 --> 01:14:22.380]   endless wars.
[01:14:22.380 --> 01:14:25.420]   But then since 1945,
[01:14:25.420 --> 01:14:27.260]   there hasn't been any war in Western Europe,
[01:14:27.260 --> 01:14:29.980]   and they all got way richer because of tech.
[01:14:29.980 --> 01:14:34.900]   So the optimistic outcome is that the big winner
[01:14:34.900 --> 01:14:38.260]   in this century is gonna be America and China and Russia
[01:14:38.260 --> 01:14:39.100]   and everybody else,
[01:14:39.100 --> 01:14:41.820]   because technology just makes us all healthier and wealthier
[01:14:41.820 --> 01:14:44.700]   and we just find some way of keeping the peace
[01:14:44.700 --> 01:14:45.540]   on this planet.
[01:14:45.540 --> 01:14:48.620]   But I think, unfortunately,
[01:14:48.620 --> 01:14:50.500]   there are some pretty powerful forces right now
[01:14:50.500 --> 01:14:52.620]   that are pushing in exactly the opposite direction
[01:14:52.620 --> 01:14:54.620]   and trying to demonize other countries,
[01:14:54.620 --> 01:14:56.540]   which just makes it more likely
[01:14:56.540 --> 01:14:59.340]   that this ever more powerful tech we're building
[01:14:59.340 --> 01:15:02.260]   is gonna be used in disastrous ways.
[01:15:02.260 --> 01:15:04.580]   - Yeah, for aggression versus cooperation,
[01:15:04.580 --> 01:15:05.420]   that kind of thing.
[01:15:05.420 --> 01:15:07.860]   - Yeah, even look at just military AI now, right?
[01:15:07.860 --> 01:15:12.180]   It was so awesome to see these dancing robots.
[01:15:12.180 --> 01:15:13.940]   I loved it, right?
[01:15:13.940 --> 01:15:17.500]   But one of the biggest growth areas in robotics now
[01:15:17.500 --> 01:15:19.300]   is, of course, autonomous weapons.
[01:15:19.300 --> 01:15:23.620]   And 2020 was like the best marketing year ever
[01:15:23.620 --> 01:15:24.420]   for autonomous weapons,
[01:15:24.420 --> 01:15:27.620]   because in both Libya, it's a civil war,
[01:15:27.620 --> 01:15:29.660]   and in Nagorno-Karabakh,
[01:15:29.660 --> 01:15:34.540]   they made the decisive difference, right?
[01:15:34.540 --> 01:15:36.260]   And everybody else is like watching this.
[01:15:36.260 --> 01:15:39.020]   Oh yeah, we wanna build autonomous weapons too.
[01:15:39.020 --> 01:15:44.020]   In Libya, you had, on one hand,
[01:15:44.020 --> 01:15:46.500]   our ally, the United Arab Emirates,
[01:15:46.500 --> 01:15:48.420]   that were flying their autonomous weapons
[01:15:48.420 --> 01:15:52.100]   that they bought from China, bombing Libyans.
[01:15:52.100 --> 01:15:54.300]   And on the other side, you had our other ally, Turkey,
[01:15:54.300 --> 01:15:56.220]   flying their drones.
[01:15:56.220 --> 01:16:01.660]   They had no skin in the game, any of these other countries.
[01:16:01.660 --> 01:16:04.260]   And of course, it was the Libyans who really got screwed.
[01:16:04.260 --> 01:16:08.020]   In Nagorno-Karabakh, you had actually,
[01:16:08.020 --> 01:16:12.260]   again, Turkey is sending drones built by this company
[01:16:12.260 --> 01:16:15.300]   that was actually founded by a guy
[01:16:15.300 --> 01:16:17.100]   who went to MIT AeroAstro.
[01:16:17.100 --> 01:16:17.940]   Do you know that?
[01:16:17.940 --> 01:16:19.580]   - No. - Bakr Atiyar, yeah.
[01:16:19.580 --> 01:16:22.740]   So MIT has a direct responsibility for ultimately this,
[01:16:22.740 --> 01:16:24.380]   and a lot of civilians were killed there.
[01:16:24.380 --> 01:16:28.500]   And so because it was militarily so effective,
[01:16:28.500 --> 01:16:31.260]   now suddenly there's a huge push.
[01:16:31.260 --> 01:16:35.740]   Oh yeah, yeah, let's go build ever more autonomy
[01:16:35.740 --> 01:16:39.460]   into these weapons, and it's gonna be great.
[01:16:39.460 --> 01:16:42.900]   And I think actually,
[01:16:42.900 --> 01:16:45.580]   people who are obsessed about
[01:16:45.580 --> 01:16:48.300]   some sort of future Terminator scenario right now
[01:16:48.300 --> 01:16:52.460]   should start focusing on the fact that we have
[01:16:52.460 --> 01:16:55.060]   two much more urgent threats happening from machine learning.
[01:16:55.060 --> 01:16:57.940]   One of them is the whole destruction of democracy
[01:16:57.940 --> 01:16:59.940]   that we've talked about now,
[01:16:59.940 --> 01:17:03.620]   where our flow of information is being manipulated
[01:17:03.620 --> 01:17:04.460]   by machine learning.
[01:17:04.460 --> 01:17:06.980]   And the other one is that right now,
[01:17:06.980 --> 01:17:09.860]   this is the year when the big arms race,
[01:17:09.860 --> 01:17:12.140]   out-of-control arms race in at least autonomous weapons
[01:17:12.140 --> 01:17:14.700]   is gonna start, or it's gonna stop.
[01:17:14.700 --> 01:17:16.500]   - So you have a sense that there is,
[01:17:16.500 --> 01:17:20.700]   like 2020 was a instrumental catalyst
[01:17:20.700 --> 01:17:24.260]   for the race of, for the autonomous weapons race.
[01:17:24.260 --> 01:17:25.460]   - Yeah, 'cause it was the first year
[01:17:25.460 --> 01:17:28.380]   when they proved decisive in the battlefield.
[01:17:28.380 --> 01:17:31.060]   And these ones are still not fully autonomous,
[01:17:31.060 --> 01:17:32.660]   mostly they're remote controlled, right?
[01:17:32.660 --> 01:17:37.660]   But we could very quickly make things about
[01:17:37.660 --> 01:17:41.820]   the size and cost of a smartphone,
[01:17:41.820 --> 01:17:44.300]   which you just put in the GPS coordinates
[01:17:44.300 --> 01:17:45.780]   or the face of the one you wanna kill,
[01:17:45.780 --> 01:17:48.660]   a skin color or whatever, and it flies away and does it.
[01:17:48.660 --> 01:17:53.180]   The real good reason why the US
[01:17:53.180 --> 01:17:57.140]   and all the other superpowers should put the kibosh on this
[01:17:57.140 --> 01:17:59.420]   is the same reason we decided
[01:17:59.420 --> 01:18:01.660]   to put the kibosh on bioweapons.
[01:18:01.660 --> 01:18:05.060]   So we gave the Future of Life Award
[01:18:05.060 --> 01:18:06.340]   that we can talk more about later,
[01:18:06.340 --> 01:18:08.100]   Matthew Messelson from Harvard before
[01:18:08.100 --> 01:18:10.340]   for convincing Nixon to ban bioweapons.
[01:18:10.340 --> 01:18:13.580]   And I asked him, "How did you do it?"
[01:18:13.580 --> 01:18:15.580]   And he was like, "Well, I just said,
[01:18:15.580 --> 01:18:19.340]   "look, we don't want there to be a $500 weapon
[01:18:19.340 --> 01:18:24.040]   "of mass destruction that all our enemies can afford,
[01:18:24.040 --> 01:18:25.620]   "even non-state actors."
[01:18:26.660 --> 01:18:31.660]   And Nixon was like, "Good point."
[01:18:31.660 --> 01:18:34.300]   It's in America's interest that the powerful weapons
[01:18:34.300 --> 01:18:37.580]   are all really expensive, so only we can afford them,
[01:18:37.580 --> 01:18:40.220]   or maybe some more stable adversaries, right?
[01:18:40.220 --> 01:18:42.960]   Nuclear weapons are like that,
[01:18:42.960 --> 01:18:44.920]   but bioweapons were not like that.
[01:18:44.920 --> 01:18:46.460]   That's why we banned them.
[01:18:46.460 --> 01:18:48.380]   And that's why you never hear about them now.
[01:18:48.380 --> 01:18:50.320]   That's why we love biology.
[01:18:50.320 --> 01:18:52.900]   - So you have a sense that it's possible
[01:18:52.900 --> 01:18:57.900]   for the big powerhouses in terms of the big nations
[01:18:57.900 --> 01:19:00.380]   in the world to agree that autonomous weapons
[01:19:00.380 --> 01:19:03.740]   is not a race we wanna be on, that it doesn't end well.
[01:19:03.740 --> 01:19:05.560]   - Yeah, because we know it's just gonna end
[01:19:05.560 --> 01:19:08.460]   in mass proliferation, and every terrorist everywhere
[01:19:08.460 --> 01:19:10.260]   is gonna have these super cheap weapons
[01:19:10.260 --> 01:19:11.760]   that they will use against us.
[01:19:11.760 --> 01:19:15.940]   And our politicians have to constantly worry
[01:19:15.940 --> 01:19:18.220]   about being assassinated every time they go outdoors
[01:19:18.220 --> 01:19:20.900]   by some anonymous little mini-drone.
[01:19:20.900 --> 01:19:21.820]   We don't want that.
[01:19:21.820 --> 01:19:25.900]   And even if the US and China and everyone else
[01:19:25.900 --> 01:19:28.520]   could just agree that you can only build these weapons
[01:19:28.520 --> 01:19:31.220]   if they cost at least 10 million bucks,
[01:19:31.220 --> 01:19:34.700]   that would be a huge win for the superpowers,
[01:19:34.700 --> 01:19:36.200]   and frankly for everybody.
[01:19:36.200 --> 01:19:40.420]   People often push back and say,
[01:19:40.420 --> 01:19:43.180]   well, it's so hard to prevent cheating.
[01:19:43.180 --> 01:19:45.760]   But hey, you could say the same about bioweapons.
[01:19:45.760 --> 01:19:49.300]   Take any of your RMIT colleagues in biology.
[01:19:49.300 --> 01:19:51.980]   Of course they could build some nasty bioweapon
[01:19:51.980 --> 01:19:54.060]   if they really wanted to, but first of all,
[01:19:54.060 --> 01:19:56.380]   they don't want to 'cause they think it's disgusting
[01:19:56.380 --> 01:19:58.340]   'cause of the stigma, and second,
[01:19:58.340 --> 01:20:02.020]   even if there's some sort of nutcase and want to,
[01:20:02.020 --> 01:20:04.120]   it's very likely that some of their grad students
[01:20:04.120 --> 01:20:05.160]   or someone would rat them out
[01:20:05.160 --> 01:20:07.900]   because everyone else thinks it's so disgusting.
[01:20:07.900 --> 01:20:10.420]   And in fact, we now know there was even
[01:20:10.420 --> 01:20:13.460]   a fair bit of cheating on the bioweapons ban,
[01:20:13.460 --> 01:20:17.460]   but no countries used them because it was so stigmatized
[01:20:17.460 --> 01:20:22.340]   that it just wasn't worth revealing that they had cheated.
[01:20:22.340 --> 01:20:26.060]   - You talk about drones, but you kind of think
[01:20:26.060 --> 01:20:28.900]   that drones is the remote operation.
[01:20:28.900 --> 01:20:30.620]   - Which they are mostly still.
[01:20:30.620 --> 01:20:34.500]   - But you're not taking the next intellectual step
[01:20:34.500 --> 01:20:36.260]   of like, where does this go?
[01:20:36.260 --> 01:20:38.660]   You're kind of saying the problem with drones
[01:20:38.660 --> 01:20:42.340]   is that you're removing yourself from direct violence,
[01:20:42.340 --> 01:20:44.900]   therefore you're not able to sort of maintain
[01:20:44.900 --> 01:20:46.380]   the common humanity required
[01:20:46.380 --> 01:20:48.740]   to make the proper decisions strategically.
[01:20:48.740 --> 01:20:51.380]   But that's the criticism as opposed to like,
[01:20:51.380 --> 01:20:55.500]   if this is automated, and just exactly as you said,
[01:20:55.500 --> 01:20:58.660]   if you automate it and there's a race,
[01:20:58.660 --> 01:21:01.280]   then the technology's gonna get better and better and better,
[01:21:01.280 --> 01:21:03.740]   which means getting cheaper and cheaper and cheaper.
[01:21:03.740 --> 01:21:06.060]   And unlike perhaps nuclear weapons,
[01:21:06.060 --> 01:21:10.260]   which is connected to resources in a way,
[01:21:10.260 --> 01:21:11.780]   like it's hard to get the--
[01:21:11.780 --> 01:21:13.740]   - It's hard to engineer.
[01:21:13.740 --> 01:21:17.620]   - It feels like there's too much overlap
[01:21:17.620 --> 01:21:20.420]   between the tech industry and autonomous weapons
[01:21:20.420 --> 01:21:24.420]   to where you could have smartphone type of cheapness.
[01:21:24.420 --> 01:21:29.300]   If you look at drones, for $1,000,
[01:21:29.300 --> 01:21:30.820]   you can have an incredible system
[01:21:30.820 --> 01:21:34.620]   that's able to maintain flight autonomously for you
[01:21:34.620 --> 01:21:36.260]   and take pictures and stuff.
[01:21:36.260 --> 01:21:39.560]   You could see that going into the autonomous weapon space.
[01:21:41.420 --> 01:21:43.260]   But why is that not thought about
[01:21:43.260 --> 01:21:45.660]   or discussed enough in the public, do you think?
[01:21:45.660 --> 01:21:48.980]   You see those dancing Boston Dynamics robots,
[01:21:48.980 --> 01:21:50.740]   and everybody has this kind of,
[01:21:50.740 --> 01:21:55.340]   like as if this is a far future.
[01:21:55.340 --> 01:21:58.620]   They have this fear, like, oh, this'll be Terminator
[01:21:58.620 --> 01:22:03.060]   in some, I don't know, unspecified 20, 30, 40 years.
[01:22:03.060 --> 01:22:04.380]   And they don't think about, well,
[01:22:04.380 --> 01:22:09.140]   this is some much less dramatic version of that
[01:22:09.140 --> 01:22:11.140]   is actually happening now.
[01:22:11.140 --> 01:22:14.840]   It's not gonna be legged, it's not gonna be dancing,
[01:22:14.840 --> 01:22:17.180]   but it already has the capability
[01:22:17.180 --> 01:22:20.260]   to use artificial intelligence to kill humans.
[01:22:20.260 --> 01:22:22.900]   - Yeah, the Boston Dynamics leg robots,
[01:22:22.900 --> 01:22:24.980]   I think the reason we imagine them holding guns
[01:22:24.980 --> 01:22:28.420]   is just 'cause you've all seen Arnold Schwarzenegger.
[01:22:28.420 --> 01:22:30.580]   That's our reference point.
[01:22:30.580 --> 01:22:32.700]   That's pretty useless.
[01:22:32.700 --> 01:22:35.340]   That's not gonna be the main military use of them.
[01:22:35.340 --> 01:22:38.700]   They might be useful in law enforcement in the future,
[01:22:38.700 --> 01:22:40.260]   and there's a whole debate about,
[01:22:40.260 --> 01:22:42.660]   do you want robots showing up at your house with guns
[01:22:42.660 --> 01:22:45.460]   telling you who'll be perfectly obedient
[01:22:45.460 --> 01:22:47.540]   to whatever dictator controls them?
[01:22:47.540 --> 01:22:49.220]   But let's leave that aside for a moment
[01:22:49.220 --> 01:22:51.300]   and look at what's actually relevant now.
[01:22:51.300 --> 01:22:54.780]   So there's a spectrum of things you can do
[01:22:54.780 --> 01:22:55.780]   with AI in the military.
[01:22:55.780 --> 01:22:57.580]   And again, to put my card on the table,
[01:22:57.580 --> 01:22:58.740]   I'm not the pacifist.
[01:22:58.740 --> 01:23:01.240]   I think we should have good defense.
[01:23:01.240 --> 01:23:07.540]   So for example, a Predator drone
[01:23:07.540 --> 01:23:10.540]   is basically a fancy little remote-controlled airplane.
[01:23:10.540 --> 01:23:14.420]   There's a human piloting it,
[01:23:14.420 --> 01:23:17.100]   and the decision ultimately about whether to kill somebody
[01:23:17.100 --> 01:23:19.420]   with it is made by a human still.
[01:23:19.420 --> 01:23:23.900]   And this is a line I think we should never cross.
[01:23:23.900 --> 01:23:25.900]   There's a current DoD policy.
[01:23:25.900 --> 01:23:27.940]   Again, you have to have a human in the loop.
[01:23:27.940 --> 01:23:31.540]   I think algorithms should never make life or death decisions.
[01:23:31.540 --> 01:23:33.040]   They should be left to humans.
[01:23:34.140 --> 01:23:37.740]   Now, why might we cross that line?
[01:23:37.740 --> 01:23:40.540]   Well, first of all, these are expensive, right?
[01:23:40.540 --> 01:23:45.540]   So for example, when Azerbaijan had all these drones
[01:23:45.540 --> 01:23:47.640]   and Armenia didn't have any,
[01:23:47.640 --> 01:23:51.060]   they started trying to jerry-rig little cheap things,
[01:23:51.060 --> 01:23:54.060]   fly around, but then of course, the Armenians would jam them
[01:23:54.060 --> 01:23:55.660]   or the Azeris would jam them.
[01:23:55.660 --> 01:23:58.340]   And remote-controlled things can be jammed.
[01:23:58.340 --> 01:24:00.060]   That makes them inferior.
[01:24:00.060 --> 01:24:02.980]   Also, there's a bit of a time delay between,
[01:24:02.980 --> 01:24:07.480]   if we're piloting something from far away, speed of light,
[01:24:07.480 --> 01:24:09.580]   and the human has a reaction time as well,
[01:24:09.580 --> 01:24:12.500]   it would be nice to eliminate that jamming possibility
[01:24:12.500 --> 01:24:15.300]   in the time delay by having it fully autonomous.
[01:24:15.300 --> 01:24:17.980]   But now you might be, so then if you do,
[01:24:17.980 --> 01:24:20.260]   but now you might be crossing that exact line.
[01:24:20.260 --> 01:24:23.220]   You might program it to just, oh yeah, the air drone,
[01:24:23.220 --> 01:24:26.160]   go hover over this country for a while.
[01:24:26.160 --> 01:24:29.980]   And whenever you find someone who is a bad guy,
[01:24:29.980 --> 01:24:30.820]   kill them.
[01:24:31.760 --> 01:24:34.280]   Now the machine is making these sort of decisions.
[01:24:34.280 --> 01:24:36.680]   And some people who defend this still say,
[01:24:36.680 --> 01:24:40.560]   well, that's morally fine because we are the good guys
[01:24:40.560 --> 01:24:43.580]   and we will tell it the definition of bad guy
[01:24:43.580 --> 01:24:44.920]   that we think is moral.
[01:24:44.920 --> 01:24:49.280]   But now it would be very naive to think
[01:24:49.280 --> 01:24:52.080]   that if ISIS buys that same drone,
[01:24:52.080 --> 01:24:54.640]   that they're gonna use our definition of bad guy.
[01:24:54.640 --> 01:24:56.440]   Maybe for them, bad guy is someone wearing
[01:24:56.440 --> 01:24:57.800]   a US Army uniform.
[01:24:58.720 --> 01:25:03.720]   Or maybe there will be some weird ethnic group
[01:25:03.720 --> 01:25:08.760]   who decides that someone of another ethnic group,
[01:25:08.760 --> 01:25:10.240]   they are the bad guys.
[01:25:10.240 --> 01:25:14.480]   The thing is, human soldiers, with all our faults,
[01:25:14.480 --> 01:25:17.080]   we still have some basic wiring in us.
[01:25:17.080 --> 01:25:21.100]   Like, no, it's not okay to kill kids and civilians.
[01:25:21.100 --> 01:25:24.840]   And Thomas Weapon has none of that.
[01:25:24.840 --> 01:25:26.640]   It's just gonna do whatever is programmed.
[01:25:26.640 --> 01:25:30.520]   It's like the perfect Adolf Eichmann on steroids.
[01:25:30.520 --> 01:25:33.400]   Like, they told him, Adolf Eichmann,
[01:25:33.400 --> 01:25:34.840]   you wanted to do this and this and this
[01:25:34.840 --> 01:25:36.400]   to make the Holocaust more efficient.
[01:25:36.400 --> 01:25:38.760]   And he was like, "Jawohl."
[01:25:38.760 --> 01:25:40.720]   And off he went and did it, right?
[01:25:40.720 --> 01:25:43.800]   Do we really wanna make machines that are like that?
[01:25:43.800 --> 01:25:46.980]   Like completely amoral and will take the user's definition
[01:25:46.980 --> 01:25:48.520]   of who's the bad guy?
[01:25:48.520 --> 01:25:50.720]   And do we then wanna make them so cheap
[01:25:50.720 --> 01:25:52.400]   that all our adversaries can have them?
[01:25:52.400 --> 01:25:55.460]   Like, what could possibly go wrong?
[01:25:55.460 --> 01:26:00.200]   That's, I think, the big argument for why we wanna,
[01:26:00.200 --> 01:26:03.520]   this year, really put the kibosh on this.
[01:26:03.520 --> 01:26:08.280]   And I think you can tell there's a lot of very active debate
[01:26:08.280 --> 01:26:10.160]   even going on within the US military,
[01:26:10.160 --> 01:26:13.120]   and undoubtedly in other militaries around the world also,
[01:26:13.120 --> 01:26:13.960]   about whether we should have
[01:26:13.960 --> 01:26:15.680]   some sort of international agreement
[01:26:15.680 --> 01:26:18.920]   to at least require that these weapons
[01:26:18.920 --> 01:26:21.900]   have to be above a certain size and cost,
[01:26:21.900 --> 01:26:26.900]   so that things just don't totally spiral out of control.
[01:26:26.900 --> 01:26:31.680]   And finally, just for your question,
[01:26:31.680 --> 01:26:33.560]   but is it possible to stop it?
[01:26:33.560 --> 01:26:35.960]   'Cause some people tell me, "Oh, just give up."
[01:26:35.960 --> 01:26:41.440]   But again, so Matthew Messelson, again, from Harvard,
[01:26:41.440 --> 01:26:43.580]   right, the bioweapons hero,
[01:26:43.580 --> 01:26:47.760]   he had exactly this criticism also with bioweapons.
[01:26:47.760 --> 01:26:49.920]   People were like, "How can you check for sure
[01:26:49.920 --> 01:26:51.720]   that the Russians aren't cheating?"
[01:26:51.720 --> 01:26:57.980]   And he told me this, I think, really ingenious insight.
[01:26:57.980 --> 01:27:01.540]   He said, "You know, Max, some people think
[01:27:01.540 --> 01:27:03.660]   you have to have inspections and things,
[01:27:03.660 --> 01:27:05.660]   and you have to make sure that people,
[01:27:05.660 --> 01:27:08.960]   you can catch the cheaters with 100% chance.
[01:27:08.960 --> 01:27:10.820]   You don't need 100%, he said.
[01:27:10.820 --> 01:27:14.100]   1% is usually enough."
[01:27:14.100 --> 01:27:19.100]   Because if it's an enemy, if it's another big state,
[01:27:19.100 --> 01:27:22.020]   like suppose China and the US have signed the treaty,
[01:27:22.020 --> 01:27:24.380]   drawing a certain line and saying,
[01:27:24.380 --> 01:27:26.260]   "Yeah, these kind of drones are okay,
[01:27:26.260 --> 01:27:28.900]   but these fully autonomous ones are not."
[01:27:28.900 --> 01:27:33.900]   Now suppose you are China and you have cheated
[01:27:33.900 --> 01:27:36.000]   and secretly developed some clandestine little thing,
[01:27:36.000 --> 01:27:37.580]   or you're thinking about doing it.
[01:27:37.580 --> 01:27:39.220]   What's your calculation that you do?
[01:27:39.220 --> 01:27:41.900]   Well, you're like, "Okay, what's the probability
[01:27:41.900 --> 01:27:43.400]   that we're gonna get caught?"
[01:27:43.400 --> 01:27:47.980]   If the probability is 100%, of course we're not gonna do it.
[01:27:48.980 --> 01:27:52.620]   But if the probability is 5% that we're gonna get caught,
[01:27:52.620 --> 01:27:55.140]   then it's gonna be a huge embarrassment for us.
[01:27:55.140 --> 01:28:00.060]   We still have our nuclear weapons anyway,
[01:28:00.060 --> 01:28:04.520]   so it doesn't really make an enormous difference
[01:28:04.520 --> 01:28:06.520]   in terms of deterring the US.
[01:28:06.520 --> 01:28:11.580]   - And that feeds the stigma that you kind of establish,
[01:28:11.580 --> 01:28:14.660]   like this fabric, this universal stigma over the thing.
[01:28:14.660 --> 01:28:16.580]   - Exactly, it's very reasonable for them to say,
[01:28:16.580 --> 01:28:19.660]   "Well, we probably get away with it, but if we don't,
[01:28:19.660 --> 01:28:21.780]   then the US will know we cheated,
[01:28:21.780 --> 01:28:23.740]   and then they're gonna go full tilt with their program
[01:28:23.740 --> 01:28:25.020]   and say, 'Look, the Chinese are cheaters,
[01:28:25.020 --> 01:28:27.100]   and now we have all these weapons against us,
[01:28:27.100 --> 01:28:27.940]   and that's bad.'"
[01:28:27.940 --> 01:28:32.100]   The stigma alone is very, very powerful.
[01:28:32.100 --> 01:28:34.540]   And again, look what happened with bioweapons.
[01:28:34.540 --> 01:28:36.940]   It's been 50 years now.
[01:28:36.940 --> 01:28:40.180]   When was the last time you read about a bioterrorism attack?
[01:28:40.180 --> 01:28:42.700]   The only deaths I really know about with bioweapons
[01:28:42.700 --> 01:28:45.540]   that have happened, when we Americans managed to kill
[01:28:45.540 --> 01:28:46.980]   some of our own with anthrax,
[01:28:46.980 --> 01:28:49.300]   you know, the idiot who sent them to Tom Daschle
[01:28:49.300 --> 01:28:50.900]   and others in letters, right?
[01:28:50.900 --> 01:28:55.900]   And similarly, in Sverdlovsk in the Soviet Union,
[01:28:55.900 --> 01:28:57.900]   they had some anthrax in some lab there.
[01:28:57.900 --> 01:29:00.020]   Maybe they were cheating or who knows,
[01:29:00.020 --> 01:29:02.540]   and it leaked out and killed a bunch of Russians.
[01:29:02.540 --> 01:29:04.460]   I'd say that's a pretty good success, right?
[01:29:04.460 --> 01:29:08.420]   50 years, just two own goals by the superpowers,
[01:29:08.420 --> 01:29:09.580]   and then nothing.
[01:29:09.580 --> 01:29:12.060]   And that's why whenever I ask anyone
[01:29:12.060 --> 01:29:15.380]   what they think about biology, they think it's great.
[01:29:15.380 --> 01:29:18.140]   Associated with new cures, new diseases,
[01:29:18.140 --> 01:29:19.780]   maybe a good vaccine.
[01:29:19.780 --> 01:29:22.180]   This is how I want to think about AI in the future.
[01:29:22.180 --> 01:29:24.900]   And I want others to think about AI too,
[01:29:24.900 --> 01:29:27.900]   as a source of all these great solutions to our problems,
[01:29:27.900 --> 01:29:31.940]   not as, "Oh, AI, oh yeah, that's the reason
[01:29:31.940 --> 01:29:34.660]   I feel scared going outside these days."
[01:29:34.660 --> 01:29:37.980]   - Yeah, it's kind of brilliant that the bioweapons
[01:29:37.980 --> 01:29:40.820]   and nuclear weapons, we've figured out,
[01:29:40.820 --> 01:29:43.380]   I mean, of course, they're still a huge source of danger,
[01:29:43.380 --> 01:29:47.780]   but we figured out some way of creating rules
[01:29:47.780 --> 01:29:51.460]   and social stigma over these weapons
[01:29:51.460 --> 01:29:54.620]   that then creates a stability to our,
[01:29:54.620 --> 01:29:57.180]   whatever that game theoretic stability is, of course.
[01:29:57.180 --> 01:29:58.020]   - Exactly.
[01:29:58.020 --> 01:29:59.220]   - And we don't have that with AI.
[01:29:59.220 --> 01:30:03.780]   And you're kind of screaming from the top of the mountain
[01:30:03.780 --> 01:30:05.540]   about this, that we need to find that
[01:30:05.540 --> 01:30:09.620]   because just like, it's very possible
[01:30:09.620 --> 01:30:12.220]   with the future of life, as you've pointed out,
[01:30:12.220 --> 01:30:17.220]   Institute Awards pointed out that with nuclear weapons,
[01:30:17.220 --> 01:30:21.260]   we could have destroyed ourselves quite a few times.
[01:30:21.260 --> 01:30:26.260]   And it's a learning experience that is very costly.
[01:30:26.260 --> 01:30:31.100]   - We gave this future of life award,
[01:30:31.100 --> 01:30:34.860]   we gave it the first time to this guy, Vasily Arkhipov.
[01:30:34.860 --> 01:30:37.700]   He was on, most people haven't even heard of him.
[01:30:37.700 --> 01:30:38.860]   - Yeah, can you say who he is?
[01:30:38.860 --> 01:30:40.100]   - Vasily Arkhipov.
[01:30:41.980 --> 01:30:46.980]   Has, in my opinion, made the greatest positive contribution
[01:30:46.980 --> 01:30:50.180]   to humanity of any human in modern history.
[01:30:50.180 --> 01:30:52.100]   And maybe it sounds like hyperbole here,
[01:30:52.100 --> 01:30:54.700]   like I'm just over the top, but let me tell you the story
[01:30:54.700 --> 01:30:56.280]   and I think maybe you'll agree.
[01:30:56.280 --> 01:30:58.380]   So during the Cuban Missile Crisis,
[01:30:58.380 --> 01:31:02.020]   we Americans first didn't know
[01:31:02.020 --> 01:31:04.440]   that the Russians had sent four submarines,
[01:31:04.440 --> 01:31:06.900]   but we caught two of them.
[01:31:06.900 --> 01:31:09.340]   And we didn't know that,
[01:31:09.340 --> 01:31:11.260]   so we dropped practice depth charges
[01:31:11.260 --> 01:31:12.580]   on the one that he was on,
[01:31:12.580 --> 01:31:14.140]   try to force it to the surface.
[01:31:14.140 --> 01:31:17.900]   But we didn't know that this nuclear submarine
[01:31:17.900 --> 01:31:20.780]   actually was a nuclear submarine with a nuclear torpedo.
[01:31:20.780 --> 01:31:22.900]   We also didn't know that they had authorization
[01:31:22.900 --> 01:31:25.340]   to launch it without clearance from Moscow.
[01:31:25.340 --> 01:31:26.260]   And we also didn't know
[01:31:26.260 --> 01:31:28.460]   that they were running out of electricity.
[01:31:28.460 --> 01:31:30.100]   Their batteries were almost dead.
[01:31:30.100 --> 01:31:32.060]   They were running out of oxygen.
[01:31:32.060 --> 01:31:34.540]   Sailors were fainting left and right.
[01:31:34.540 --> 01:31:39.380]   The temperature was about 110, 120 Fahrenheit on board.
[01:31:39.380 --> 01:31:41.180]   It was really hellish conditions,
[01:31:41.180 --> 01:31:43.500]   really just a kind of doomsday.
[01:31:43.500 --> 01:31:46.520]   And at that point, these giant explosions start happening
[01:31:46.520 --> 01:31:48.420]   from Americans dropping these.
[01:31:48.420 --> 01:31:50.940]   The captain thought World War III had begun.
[01:31:50.940 --> 01:31:52.760]   They decided that they were gonna launch
[01:31:52.760 --> 01:31:53.940]   the nuclear torpedo.
[01:31:53.940 --> 01:31:56.380]   And one of them shouted, "We're all gonna die,
[01:31:56.380 --> 01:31:59.180]   "but we're not gonna disgrace our Navy."
[01:31:59.180 --> 01:32:00.340]   We don't know what would have happened
[01:32:00.340 --> 01:32:03.660]   if there had been a giant mushroom cloud all of a sudden
[01:32:03.660 --> 01:32:05.020]   against the Americans,
[01:32:05.020 --> 01:32:07.580]   but since everybody had their hands on the triggers,
[01:32:07.580 --> 01:32:10.700]   you don't have to be too creative
[01:32:10.700 --> 01:32:13.300]   to think that it could have led to an all-out nuclear war,
[01:32:13.300 --> 01:32:14.420]   in which case we wouldn't be having
[01:32:14.420 --> 01:32:15.900]   this conversation now, right?
[01:32:15.900 --> 01:32:18.780]   What actually took place was they needed three people
[01:32:18.780 --> 01:32:21.260]   to approve this.
[01:32:21.260 --> 01:32:22.420]   The captain had said yes.
[01:32:22.420 --> 01:32:24.340]   There was the Communist Party political officer.
[01:32:24.340 --> 01:32:26.220]   He also said, "Yes, let's do it."
[01:32:26.220 --> 01:32:29.260]   And the third man was this guy, Vasily Arkhipov,
[01:32:29.260 --> 01:32:30.100]   who said, "Nyet."
[01:32:30.100 --> 01:32:34.500]   For some reason, he was just more chill than the others,
[01:32:34.500 --> 01:32:36.100]   and he was the right man at the right time.
[01:32:36.100 --> 01:32:39.980]   I don't want us as a species rely on the right person
[01:32:39.980 --> 01:32:41.540]   being there at the right time.
[01:32:41.540 --> 01:32:46.980]   We tracked down his family living in relative poverty
[01:32:46.980 --> 01:32:48.180]   outside Moscow.
[01:32:48.180 --> 01:32:50.660]   We flew his daughter.
[01:32:50.660 --> 01:32:51.660]   He had passed away.
[01:32:51.660 --> 01:32:54.580]   And we flew them to London.
[01:32:54.580 --> 01:32:55.900]   They had never been to the West even.
[01:32:55.900 --> 01:32:59.140]   It was incredibly moving to get to honor them for this.
[01:32:59.140 --> 01:33:01.620]   The next year, we gave this Future Life Award
[01:33:01.620 --> 01:33:03.940]   to Stanislav Petrov.
[01:33:03.940 --> 01:33:04.780]   Have you heard of him?
[01:33:04.780 --> 01:33:05.600]   - Yes.
[01:33:05.600 --> 01:33:09.740]   - So he was in charge of the Soviet early warning station,
[01:33:09.740 --> 01:33:12.580]   which was built with Soviet technology
[01:33:12.580 --> 01:33:14.500]   and honestly not that reliable.
[01:33:14.500 --> 01:33:17.020]   It said that there were five US missiles coming in.
[01:33:17.020 --> 01:33:21.140]   Again, if they had launched at that point,
[01:33:21.140 --> 01:33:23.180]   we probably wouldn't be having this conversation.
[01:33:23.180 --> 01:33:28.180]   He decided, based on just mainly gut instinct,
[01:33:28.180 --> 01:33:32.500]   to just not escalate this.
[01:33:32.500 --> 01:33:35.060]   And I'm very glad he wasn't replaced by an AI
[01:33:35.060 --> 01:33:37.500]   that was just automatically following orders.
[01:33:37.500 --> 01:33:39.700]   And then we gave the third one to Matthew Messelson.
[01:33:39.700 --> 01:33:44.180]   Last year, we gave this award to these guys
[01:33:44.180 --> 01:33:46.580]   who actually use technology for good,
[01:33:46.580 --> 01:33:49.980]   not avoiding something bad, but for something good.
[01:33:49.980 --> 01:33:51.960]   The guys who eliminated this disease,
[01:33:51.960 --> 01:33:53.560]   which is way worse than COVID,
[01:33:53.560 --> 01:33:56.940]   that had killed half a billion people
[01:33:56.940 --> 01:33:59.380]   in its final century, smallpox.
[01:33:59.380 --> 01:34:01.140]   So we mentioned it earlier.
[01:34:01.140 --> 01:34:05.220]   COVID, on average, kills less than 1% of people who get it.
[01:34:05.220 --> 01:34:08.180]   Smallpox, about 30%.
[01:34:08.180 --> 01:34:13.180]   And ultimately, Viktor Zhdanov and Bill Fahy,
[01:34:13.180 --> 01:34:17.500]   most of my colleagues have never heard of either of them,
[01:34:17.500 --> 01:34:22.020]   one American, one Russian, they did this amazing effort.
[01:34:22.020 --> 01:34:25.220]   Not only was Zhdanov able to get the US and the Soviet Union
[01:34:25.220 --> 01:34:27.980]   to team up against smallpox during the Cold War,
[01:34:27.980 --> 01:34:30.340]   but Bill Fahy came up with this ingenious strategy
[01:34:30.340 --> 01:34:34.740]   for making it actually go all the way to defeat the disease
[01:34:34.740 --> 01:34:37.620]   without funding for vaccinating everyone.
[01:34:37.620 --> 01:34:42.380]   And as a result, we went from 15 million deaths
[01:34:42.380 --> 01:34:44.420]   the year I was born in smallpox,
[01:34:44.420 --> 01:34:45.660]   so what do we have in COVID now?
[01:34:45.660 --> 01:34:47.100]   A little bit short of 2 million, right?
[01:34:47.100 --> 01:34:48.140]   - Yes.
[01:34:48.140 --> 01:34:51.940]   - To zero deaths, of course, this year and forever.
[01:34:51.940 --> 01:34:54.820]   There have been 200 million people, they estimate,
[01:34:54.820 --> 01:34:57.220]   who would have died since then by smallpox
[01:34:57.220 --> 01:34:58.100]   had it not been for this.
[01:34:58.100 --> 01:34:59.780]   So isn't science awesome?
[01:34:59.780 --> 01:35:00.620]   - Yeah, it does.
[01:35:00.620 --> 01:35:01.620]   - When you use it for good.
[01:35:01.620 --> 01:35:04.300]   And the reason we wanna celebrate these sort of people
[01:35:04.300 --> 01:35:05.700]   is to remind them of this.
[01:35:05.700 --> 01:35:10.140]   Science is so awesome when you use it for good.
[01:35:10.140 --> 01:35:13.500]   - And those awards actually, the variety there,
[01:35:13.500 --> 01:35:14.900]   paints a very interesting picture.
[01:35:14.900 --> 01:35:19.340]   So the first two are looking at,
[01:35:19.340 --> 01:35:22.700]   it's kind of exciting to think that these average humans,
[01:35:22.700 --> 01:35:26.180]   in some sense, that are products of billions
[01:35:26.180 --> 01:35:30.180]   of other humans that came before them, evolution,
[01:35:30.180 --> 01:35:33.380]   and some little, you said gut, you know,
[01:35:33.380 --> 01:35:35.300]   but there's something in there
[01:35:35.300 --> 01:35:40.300]   that stopped the annihilation of the human race.
[01:35:40.300 --> 01:35:43.060]   And that's a magical thing,
[01:35:43.060 --> 01:35:45.260]   but that's like this deeply human thing.
[01:35:45.260 --> 01:35:47.420]   And then there's the other aspect
[01:35:47.420 --> 01:35:49.820]   where it's also very human,
[01:35:49.820 --> 01:35:54.460]   which is to build solution to the existential crises
[01:35:54.460 --> 01:35:56.340]   that we're facing, like to build it,
[01:35:56.340 --> 01:35:58.780]   to take the responsibility and to come up
[01:35:58.780 --> 01:36:00.620]   with different technologies and so on.
[01:36:00.660 --> 01:36:04.100]   And both of those are deeply human,
[01:36:04.100 --> 01:36:07.020]   the gut and the mind, whatever that is.
[01:36:07.020 --> 01:36:08.660]   - Yeah, and the best is when they work together.
[01:36:08.660 --> 01:36:11.420]   Arkhipov, I wish I could have met him, of course,
[01:36:11.420 --> 01:36:13.260]   but he had passed away.
[01:36:13.260 --> 01:36:16.740]   He was really a fantastic military officer,
[01:36:16.740 --> 01:36:19.860]   combining all the best traits that we in America admire
[01:36:19.860 --> 01:36:21.940]   in our military, because first of all,
[01:36:21.940 --> 01:36:23.180]   he was very loyal, of course.
[01:36:23.180 --> 01:36:26.100]   He never even told anyone about this during his whole life,
[01:36:26.100 --> 01:36:28.420]   even though you think he had some bragging rights, right?
[01:36:28.420 --> 01:36:29.980]   But he just was like, this is just business,
[01:36:29.980 --> 01:36:31.540]   just doing my job.
[01:36:31.540 --> 01:36:34.300]   It only came out later after his death.
[01:36:34.300 --> 01:36:37.100]   And second, the reason he did the right thing
[01:36:37.100 --> 01:36:39.220]   was not 'cause he was some sort of liberal,
[01:36:39.220 --> 01:36:43.940]   or some sort of, not because he was just,
[01:36:43.940 --> 01:36:47.340]   oh, you know, peace and love.
[01:36:47.340 --> 01:36:49.780]   It was partly because he had been the captain
[01:36:49.780 --> 01:36:53.060]   on another submarine that had a nuclear reactor meltdown.
[01:36:53.060 --> 01:36:58.020]   And it was his heroism that helped contain this.
[01:36:58.020 --> 01:36:59.740]   That's why he died of cancer later also.
[01:36:59.740 --> 01:37:01.460]   But he's seen many of his crew members die.
[01:37:01.460 --> 01:37:04.140]   And I think for him, that gave him this gut feeling
[01:37:04.140 --> 01:37:06.940]   that if there's a nuclear war between the US
[01:37:06.940 --> 01:37:11.060]   and the Soviet Union, the whole world is gonna go through
[01:37:11.060 --> 01:37:13.740]   what I saw my dear crew members suffer through.
[01:37:13.740 --> 01:37:15.820]   It wasn't just an abstract thing for him.
[01:37:15.820 --> 01:37:17.660]   I think it was real.
[01:37:17.660 --> 01:37:20.620]   And second, though, not just the gut, the mind, right?
[01:37:20.620 --> 01:37:23.940]   He was, for some reason, just a very level-headed personality
[01:37:23.940 --> 01:37:27.220]   and a very smart guy, which is exactly what we want
[01:37:28.180 --> 01:37:30.100]   our best fighter pilots to be also, right?
[01:37:30.100 --> 01:37:32.860]   I never forget Neil Armstrong when he's landing on the moon
[01:37:32.860 --> 01:37:34.540]   and almost running out of gas.
[01:37:34.540 --> 01:37:37.420]   And he doesn't even change, when they say 30 seconds,
[01:37:37.420 --> 01:37:39.660]   he doesn't even change the tone of voice, just keeps going.
[01:37:39.660 --> 01:37:41.820]   Arkhipov, I think, was just like that.
[01:37:41.820 --> 01:37:43.460]   So when the explosions start going off
[01:37:43.460 --> 01:37:44.580]   and his captain is screaming,
[01:37:44.580 --> 01:37:47.420]   and we should nuke them and all, he's like,
[01:37:47.420 --> 01:37:54.300]   I don't think the Americans are trying to sink us.
[01:37:54.300 --> 01:37:56.500]   I think they're trying to send us a message.
[01:37:58.060 --> 01:38:00.500]   That's pretty badass coolness.
[01:38:00.500 --> 01:38:02.700]   'Cause he said, if they wanted to sink us,
[01:38:02.700 --> 01:38:06.900]   and he said, listen, listen, it's alternating.
[01:38:06.900 --> 01:38:10.180]   One loud explosion on the left, one on the right.
[01:38:10.180 --> 01:38:12.100]   One on the left, one on the right.
[01:38:12.100 --> 01:38:14.260]   He was the only one to notice this pattern.
[01:38:14.260 --> 01:38:18.700]   And he's like, I think this is them trying to send us
[01:38:18.700 --> 01:38:22.820]   a signal that they want us to surface,
[01:38:22.820 --> 01:38:24.340]   and they're not gonna sink us.
[01:38:25.740 --> 01:38:30.740]   And somehow, this is how he then managed to ultimately,
[01:38:30.740 --> 01:38:34.620]   with his combination of gut,
[01:38:34.620 --> 01:38:37.940]   and also just cool analytical thinking,
[01:38:37.940 --> 01:38:40.140]   was able to deescalate the whole thing.
[01:38:40.140 --> 01:38:44.220]   And yeah, so this is some of the best in humanity.
[01:38:44.220 --> 01:38:45.820]   I guess coming back to what we talked about earlier,
[01:38:45.820 --> 01:38:48.580]   is the combination of the neural network, the instinctive,
[01:38:48.580 --> 01:38:51.660]   with, I'm tearing up here, I'm getting emotional.
[01:38:51.660 --> 01:38:54.780]   But he was just, he is one of my superheroes.
[01:38:55.780 --> 01:39:00.460]   Having both the heart and the mind combined.
[01:39:00.460 --> 01:39:03.700]   - Especially in that time, there's something about the,
[01:39:03.700 --> 01:39:05.380]   I mean, this is a very, in America,
[01:39:05.380 --> 01:39:09.100]   people are used to this kind of idea of being the individual,
[01:39:09.100 --> 01:39:11.180]   of on your own thinking.
[01:39:11.180 --> 01:39:15.500]   I think in the Soviet Union, under communism,
[01:39:15.500 --> 01:39:17.620]   it's actually much harder to do that.
[01:39:17.620 --> 01:39:19.980]   - Oh yeah, he didn't even, he even got,
[01:39:19.980 --> 01:39:21.860]   he didn't get any accolades either
[01:39:21.860 --> 01:39:24.260]   when he came back for this, right?
[01:39:24.260 --> 01:39:25.900]   They just wanted to hush the whole thing up.
[01:39:25.900 --> 01:39:27.980]   - Yeah, there's echoes of that with Chernobyl,
[01:39:27.980 --> 01:39:32.460]   there's all kinds of, that's one,
[01:39:32.460 --> 01:39:34.380]   that's a really hopeful thing,
[01:39:34.380 --> 01:39:37.580]   that amidst big centralized powers,
[01:39:37.580 --> 01:39:39.960]   whether it's companies or states,
[01:39:39.960 --> 01:39:42.500]   there's still the power of the individual
[01:39:42.500 --> 01:39:43.900]   to think on their own, to act.
[01:39:43.900 --> 01:39:46.940]   - But I think we need to think of people like this,
[01:39:46.940 --> 01:39:50.180]   not as a panacea we can always count on,
[01:39:50.180 --> 01:39:54.120]   but rather as a wake-up call.
[01:39:54.120 --> 01:39:58.580]   Because of them, because of Arkhipov,
[01:39:58.580 --> 01:40:01.380]   we are alive to learn from this lesson,
[01:40:01.380 --> 01:40:02.700]   to learn from the fact that we shouldn't
[01:40:02.700 --> 01:40:03.740]   keep playing Russian roulette
[01:40:03.740 --> 01:40:06.660]   and almost have a nuclear war by mistake now and then,
[01:40:06.660 --> 01:40:09.620]   'cause relying on luck is not a good long-term strategy.
[01:40:09.620 --> 01:40:11.420]   If you keep playing Russian roulette over and over again,
[01:40:11.420 --> 01:40:12.540]   the probability of surviving
[01:40:12.540 --> 01:40:15.100]   just drops exponentially with time.
[01:40:15.100 --> 01:40:16.720]   And if you have some probability
[01:40:16.720 --> 01:40:18.700]   of having an accidental nuclear war every year,
[01:40:18.700 --> 01:40:20.180]   the probability of not having one
[01:40:20.180 --> 01:40:21.220]   also drops exponentially.
[01:40:21.220 --> 01:40:22.860]   I think we can do better than that.
[01:40:22.860 --> 01:40:26.020]   So I think the message is very clear,
[01:40:26.020 --> 01:40:27.900]   once in a while shit happens,
[01:40:27.900 --> 01:40:31.380]   and there's a lot of very concrete things we can do
[01:40:31.380 --> 01:40:34.580]   to reduce the risk of things like that
[01:40:34.580 --> 01:40:36.580]   happening in the first place.
[01:40:36.580 --> 01:40:41.020]   - On the AI front, if we just link on that for a second.
[01:40:41.020 --> 01:40:44.140]   So you're friends with, you often talk with Elon Musk,
[01:40:44.140 --> 01:40:46.700]   throughout history, you've did a lot
[01:40:46.700 --> 01:40:48.300]   of interesting things together.
[01:40:48.700 --> 01:40:52.300]   He has a set of fears about the future
[01:40:52.300 --> 01:40:54.980]   of artificial intelligence, AGI.
[01:40:54.980 --> 01:40:59.740]   Do you have a sense, we've already talked about
[01:40:59.740 --> 01:41:01.580]   the things we should be worried about with AI.
[01:41:01.580 --> 01:41:05.420]   Do you have a sense of the shape of his fears in particular
[01:41:05.420 --> 01:41:10.160]   about AI, of which subset of what we've talked about,
[01:41:10.160 --> 01:41:15.160]   whether it's creating, it's that direction of creating
[01:41:15.160 --> 01:41:17.500]   sort of these giant competition systems
[01:41:17.500 --> 01:41:19.140]   that are not explainable,
[01:41:19.140 --> 01:41:21.820]   that they're not intelligible intelligence,
[01:41:21.820 --> 01:41:26.820]   or is it the, and then as a branch of that,
[01:41:26.820 --> 01:41:31.800]   is it the manipulation by big corporations of that
[01:41:31.800 --> 01:41:35.340]   or individual evil people to use that for destruction
[01:41:35.340 --> 01:41:37.420]   or the unintentional consequences?
[01:41:37.420 --> 01:41:40.260]   Do you have a sense of where his thinking is on this?
[01:41:40.260 --> 01:41:42.460]   - From my many conversations with Elon,
[01:41:42.460 --> 01:41:47.460]   yeah, I certainly have a model of how he thinks.
[01:41:47.460 --> 01:41:49.900]   It's actually very much like the way I think also.
[01:41:49.900 --> 01:41:51.100]   I'll elaborate on it a bit.
[01:41:51.100 --> 01:41:54.660]   I just want to push back on when you said evil people.
[01:41:54.660 --> 01:41:59.660]   I don't think it's a very helpful concept, evil people.
[01:41:59.660 --> 01:42:02.380]   Sometimes people do very, very bad things,
[01:42:02.380 --> 01:42:05.460]   but they usually do it because they think it's a good thing
[01:42:05.460 --> 01:42:07.800]   because somehow other people had told them
[01:42:07.800 --> 01:42:09.980]   that that was a good thing or given them
[01:42:09.980 --> 01:42:13.420]   incorrect information or whatever, right?
[01:42:15.540 --> 01:42:18.420]   I believe in the fundamental goodness of humanity
[01:42:18.420 --> 01:42:21.700]   that if we educate people well
[01:42:21.700 --> 01:42:24.300]   and they find out how things really are,
[01:42:24.300 --> 01:42:27.280]   people generally want to do good and be good.
[01:42:27.280 --> 01:42:30.380]   - Hence the value alignment.
[01:42:30.380 --> 01:42:31.220]   - Yes.
[01:42:31.220 --> 01:42:33.700]   - It's about information, about knowledge,
[01:42:33.700 --> 01:42:35.380]   and then once we have that,
[01:42:35.380 --> 01:42:40.380]   we'll likely be able to do good in the way that's aligned
[01:42:40.380 --> 01:42:42.180]   with everybody else who thinks the same way.
[01:42:42.180 --> 01:42:44.060]   - Yeah, and it's not just the individual people
[01:42:44.060 --> 01:42:45.020]   we have to align.
[01:42:45.020 --> 01:42:49.620]   So we don't just want people to be educated
[01:42:49.620 --> 01:42:51.220]   to know the way things actually are
[01:42:51.220 --> 01:42:53.260]   and to treat each other well,
[01:42:53.260 --> 01:42:56.300]   but we also would need to align other non-human entities.
[01:42:56.300 --> 01:42:58.580]   We talked about corporations, there has to be institutions
[01:42:58.580 --> 01:42:59.980]   so that what they do is actually good
[01:42:59.980 --> 01:43:00.940]   for the country they're in,
[01:43:00.940 --> 01:43:03.500]   and we should make sure that what the countries do
[01:43:03.500 --> 01:43:07.820]   is actually good for the species as a whole, et cetera.
[01:43:07.820 --> 01:43:08.700]   Coming back to Elon,
[01:43:08.700 --> 01:43:13.660]   yeah, my understanding of how Elon sees this
[01:43:13.660 --> 01:43:15.260]   is really quite similar to my own,
[01:43:15.260 --> 01:43:18.240]   which is one of the reasons I like him so much
[01:43:18.240 --> 01:43:19.380]   and enjoy talking with him so much.
[01:43:19.380 --> 01:43:22.980]   I feel he's quite different from most people
[01:43:22.980 --> 01:43:27.740]   in that he thinks much more than most people
[01:43:27.740 --> 01:43:29.860]   about the really big picture,
[01:43:29.860 --> 01:43:32.540]   not just what's going to happen in the next election cycle,
[01:43:32.540 --> 01:43:36.040]   but in millennia, millions and billions of years from now.
[01:43:36.040 --> 01:43:39.280]   And when you look in this more cosmic perspective,
[01:43:39.280 --> 01:43:43.060]   it's so obvious that we're gazing out into this universe
[01:43:43.060 --> 01:43:46.220]   that as far as we can tell is mostly dead
[01:43:46.220 --> 01:43:50.500]   with life being an almost imperceptibly tiny perturbation.
[01:43:50.500 --> 01:43:52.580]   And he sees this enormous opportunity
[01:43:52.580 --> 01:43:54.260]   for our universe to come alive,
[01:43:54.260 --> 01:43:56.460]   for us to become an interplanetary species.
[01:43:56.460 --> 01:44:01.460]   Mars is obviously just first stop on this cosmic journey.
[01:44:01.460 --> 01:44:04.960]   And precisely because he thinks more long-term,
[01:44:04.960 --> 01:44:09.520]   it's much more clear to him than to most people
[01:44:09.520 --> 01:44:11.300]   that what we do with this Russian roulette thing
[01:44:11.300 --> 01:44:15.300]   we keep playing with our nukes is a really poor strategy,
[01:44:15.300 --> 01:44:16.700]   really reckless strategy.
[01:44:16.700 --> 01:44:18.580]   And also that we're just building
[01:44:18.580 --> 01:44:20.260]   these ever more powerful AI systems
[01:44:20.260 --> 01:44:21.620]   that we don't understand
[01:44:21.620 --> 01:44:23.780]   is also a really reckless strategy.
[01:44:23.780 --> 01:44:26.620]   I feel Elon is very much a humanist
[01:44:26.620 --> 01:44:30.860]   in the sense that he wants an awesome future for humanity.
[01:44:30.860 --> 01:44:35.860]   He wants it to be us that control the machines
[01:44:35.860 --> 01:44:38.240]   rather than the machines that control us.
[01:44:39.380 --> 01:44:42.060]   And why shouldn't we insist on that?
[01:44:42.060 --> 01:44:44.540]   We're building them after all, right?
[01:44:44.540 --> 01:44:46.500]   Why should we build things that just make us
[01:44:46.500 --> 01:44:48.400]   into some little cog in the machinery
[01:44:48.400 --> 01:44:50.220]   that has no further say in the matter, right?
[01:44:50.220 --> 01:44:54.540]   That's not my idea of an inspiring future either.
[01:44:54.540 --> 01:44:57.860]   - Yeah, if you think on the cosmic scale
[01:44:57.860 --> 01:44:59.800]   in terms of both time and space,
[01:44:59.800 --> 01:45:02.620]   so much is put into perspective.
[01:45:02.620 --> 01:45:04.220]   - Yeah.
[01:45:04.220 --> 01:45:06.420]   Whenever I have a bad day, that's what I think about.
[01:45:06.420 --> 01:45:09.260]   It immediately makes me feel better.
[01:45:09.260 --> 01:45:13.500]   - It makes me sad that for us individual humans,
[01:45:13.500 --> 01:45:16.020]   at least for now, the ride ends too quickly.
[01:45:16.020 --> 01:45:20.060]   We don't get to experience the cosmic scale.
[01:45:20.060 --> 01:45:22.220]   - Yeah, I mean, I think of our universe sometimes
[01:45:22.220 --> 01:45:25.120]   as an organism that has only begun to wake up a tiny bit.
[01:45:25.120 --> 01:45:30.100]   Just like the very first little glimmers of consciousness
[01:45:30.100 --> 01:45:32.140]   you have in the morning when you start coming around.
[01:45:32.140 --> 01:45:33.180]   - Before the coffee.
[01:45:33.180 --> 01:45:34.300]   - Before the coffee.
[01:45:34.300 --> 01:45:35.820]   Even before you get out of bed,
[01:45:35.820 --> 01:45:37.540]   before you even open your eyes.
[01:45:37.540 --> 01:45:40.100]   You start to wake up a little bit.
[01:45:40.100 --> 01:45:41.500]   Oh, there's something here.
[01:45:41.500 --> 01:45:47.140]   That's very much how I think of what we are.
[01:45:47.140 --> 01:45:48.580]   All those galaxies out there,
[01:45:48.580 --> 01:45:51.180]   I think they're really beautiful.
[01:45:51.180 --> 01:45:52.820]   But why are they beautiful?
[01:45:52.820 --> 01:45:55.060]   They're beautiful because conscious entities
[01:45:55.060 --> 01:45:56.980]   are actually observing them,
[01:45:56.980 --> 01:45:59.100]   experiencing them through our telescopes.
[01:45:59.100 --> 01:46:05.860]   I define consciousness as subjective experience,
[01:46:05.860 --> 01:46:09.380]   whether it be colors or emotions or sounds.
[01:46:09.380 --> 01:46:13.740]   So beauty is an experience, meaning is an experience,
[01:46:13.740 --> 01:46:15.820]   purpose is an experience.
[01:46:15.820 --> 01:46:18.940]   If there was no conscious experience observing these galaxies
[01:46:18.940 --> 01:46:20.260]   they wouldn't be beautiful.
[01:46:20.260 --> 01:46:24.900]   If we do something dumb with advanced AI in the future here
[01:46:24.900 --> 01:46:29.340]   and Earth originating, life goes extinct.
[01:46:29.340 --> 01:46:30.460]   And that was it for this.
[01:46:30.460 --> 01:46:33.540]   If there is nothing else with telescopes in our universe,
[01:46:33.540 --> 01:46:36.900]   then it's kind of game over for beauty and meaning
[01:46:36.900 --> 01:46:38.100]   and purpose in the whole universe.
[01:46:38.100 --> 01:46:40.980]   And I think that would be just such an opportunity lost,
[01:46:40.980 --> 01:46:41.820]   frankly.
[01:46:41.820 --> 01:46:46.060]   And I think when Elon points this out,
[01:46:46.060 --> 01:46:49.620]   he gets very unfairly maligned in the media
[01:46:49.620 --> 01:46:52.420]   for all the dumb media bias reasons we talked about.
[01:46:52.420 --> 01:46:55.660]   They want to print precisely the things about Elon
[01:46:55.660 --> 01:46:58.300]   out of context that are really clickbaity.
[01:46:58.300 --> 01:47:00.460]   Like he has gotten so much flack
[01:47:00.460 --> 01:47:03.420]   for this summoning the demon statement.
[01:47:03.420 --> 01:47:07.700]   I happen to know exactly the context
[01:47:07.700 --> 01:47:09.740]   'cause I was in the front row when he gave that talk.
[01:47:09.740 --> 01:47:11.300]   It was at MIT, you'll be pleased to know.
[01:47:11.300 --> 01:47:13.860]   It was the AeroAstro anniversary.
[01:47:13.860 --> 01:47:16.780]   They had Buzz Aldrin there from the moon landing,
[01:47:16.780 --> 01:47:19.020]   the whole house, Kresge Auditorium,
[01:47:19.020 --> 01:47:20.860]   packed with MIT students.
[01:47:20.860 --> 01:47:23.940]   And he had this amazing Q&A, might've gone for an hour.
[01:47:23.940 --> 01:47:27.180]   And they talked about rockets and Mars and everything.
[01:47:27.180 --> 01:47:29.620]   At the very end, this one student
[01:47:29.620 --> 01:47:33.220]   who's actually in my class asked him, "What about AI?"
[01:47:33.220 --> 01:47:35.220]   Elon makes this one comment
[01:47:35.220 --> 01:47:39.420]   and they take this out of context, print it, goes viral.
[01:47:39.420 --> 01:47:41.660]   - Was it like with AI, we're summoning the demons,
[01:47:41.660 --> 01:47:42.500]   something like that?
[01:47:42.500 --> 01:47:43.900]   - Mm-hmm, and try to cast him
[01:47:43.900 --> 01:47:47.460]   as some sort of doom and gloom dude.
[01:47:47.460 --> 01:47:48.700]   You know Elon.
[01:47:48.700 --> 01:47:49.660]   - Yes.
[01:47:49.660 --> 01:47:51.980]   - He's not the doom and gloom dude.
[01:47:51.980 --> 01:47:53.980]   He is such a positive visionary.
[01:47:53.980 --> 01:47:55.660]   And the whole reason he warns about this
[01:47:55.660 --> 01:47:57.700]   is because he realizes more than most
[01:47:57.700 --> 01:47:59.860]   what the opportunity cost is of screwing up,
[01:47:59.860 --> 01:48:02.340]   that there is so much awesomeness in the future
[01:48:02.340 --> 01:48:05.460]   that we can and our descendants can enjoy
[01:48:05.460 --> 01:48:07.740]   if we don't screw up, right?
[01:48:07.740 --> 01:48:10.340]   I get so pissed off when people try to cast him
[01:48:10.340 --> 01:48:14.300]   as some sort of technophobic Luddite.
[01:48:14.300 --> 01:48:18.460]   And at this point, it's kind of ludicrous
[01:48:18.460 --> 01:48:20.500]   when I hear people say that people who worry
[01:48:20.500 --> 01:48:24.560]   about artificial general intelligence are Luddites,
[01:48:24.560 --> 01:48:26.980]   because of course, if you look more closely,
[01:48:26.980 --> 01:48:31.980]   you have some of the most outspoken people making warnings
[01:48:31.980 --> 01:48:35.660]   are people like Professor Stuart Russell from Berkeley
[01:48:35.660 --> 01:48:38.380]   who's written the best-selling AI textbook, you know.
[01:48:38.380 --> 01:48:44.260]   So claiming that he's a Luddite who doesn't understand AI,
[01:48:44.260 --> 01:48:46.500]   the joke is really on the people who said it,
[01:48:46.500 --> 01:48:48.220]   but I think more broadly,
[01:48:48.220 --> 01:48:50.780]   this message is really not sunk in at all,
[01:48:50.780 --> 01:48:52.660]   what it is that people worry about.
[01:48:52.660 --> 01:48:56.660]   They think that Elon and Stuart Russell and others
[01:48:56.660 --> 01:49:01.660]   are worried about the dancing robots picking up an AR-15
[01:49:01.660 --> 01:49:04.340]   and going on a rampage, right?
[01:49:04.340 --> 01:49:08.420]   They think they're worried about robots turning evil.
[01:49:08.420 --> 01:49:10.020]   They're not, I'm not.
[01:49:10.020 --> 01:49:15.020]   The risk is not malice, it's competence.
[01:49:15.020 --> 01:49:17.540]   The risk is just that we build some systems
[01:49:17.540 --> 01:49:18.780]   that are incredibly competent,
[01:49:18.780 --> 01:49:20.020]   which means they're always gonna get
[01:49:20.020 --> 01:49:24.060]   their goals accomplished, even if they clash with our goals.
[01:49:24.060 --> 01:49:25.060]   That's the risk.
[01:49:25.900 --> 01:49:30.900]   Why did we humans drive the West African black rhino extinct?
[01:49:30.900 --> 01:49:35.460]   Is it because we're malicious, evil rhinoceros haters?
[01:49:35.460 --> 01:49:38.700]   No, it's just 'cause our goals didn't align
[01:49:38.700 --> 01:49:39.900]   with the goals of those rhinos
[01:49:39.900 --> 01:49:41.860]   and tough luck for the rhinos.
[01:49:41.860 --> 01:49:47.340]   So the point is just we don't wanna put ourselves
[01:49:47.340 --> 01:49:48.740]   in the position of those rhinos,
[01:49:48.740 --> 01:49:51.860]   creating something more powerful than us
[01:49:51.860 --> 01:49:54.540]   if we haven't first figured out how to align the goals.
[01:49:54.540 --> 01:49:55.540]   And I am optimistic.
[01:49:55.540 --> 01:49:57.540]   I think we could do it if we worked really hard on it
[01:49:57.540 --> 01:50:01.460]   because I spent a lot of time around intelligent entities
[01:50:01.460 --> 01:50:04.620]   that were more intelligent than me, my mom and my dad.
[01:50:04.620 --> 01:50:08.180]   And I was little and that was fine
[01:50:08.180 --> 01:50:09.740]   'cause their goals were actually aligned
[01:50:09.740 --> 01:50:10.780]   with mine quite well.
[01:50:10.780 --> 01:50:14.660]   But we've seen today many examples
[01:50:14.660 --> 01:50:16.900]   of where the goals of our powerful systems
[01:50:16.900 --> 01:50:17.820]   are not so aligned.
[01:50:17.820 --> 01:50:22.820]   So those click-through optimization algorithms
[01:50:22.820 --> 01:50:25.140]   that are polarized social media, right?
[01:50:25.140 --> 01:50:26.740]   They were actually pretty poorly aligned
[01:50:26.740 --> 01:50:29.580]   with what was good for democracy, it turned out.
[01:50:29.580 --> 01:50:32.220]   And again, almost all problems we've had
[01:50:32.220 --> 01:50:34.380]   in the machine learning, again, came so far,
[01:50:34.380 --> 01:50:36.180]   not from malice, but from poor alignment.
[01:50:36.180 --> 01:50:38.940]   And that's exactly why that's why we should be concerned
[01:50:38.940 --> 01:50:39.980]   about it in the future.
[01:50:39.980 --> 01:50:43.900]   - Do you think it's possible that with systems
[01:50:43.900 --> 01:50:47.260]   like Neuralink and brain-computer interfaces,
[01:50:47.260 --> 01:50:49.980]   again, thinking of the cosmic scale,
[01:50:49.980 --> 01:50:51.780]   Elon has talked about this,
[01:50:51.780 --> 01:50:54.540]   but others have as well throughout history
[01:50:54.540 --> 01:50:57.900]   of figuring out how the exact mechanism
[01:50:57.900 --> 01:50:59.980]   of how to achieve that kind of alignment.
[01:50:59.980 --> 01:51:03.140]   So one of them is having a symbiosis with AI,
[01:51:03.140 --> 01:51:05.580]   which is like coming up with clever ways
[01:51:05.580 --> 01:51:10.340]   where we're like stuck together in this weird relationship,
[01:51:10.340 --> 01:51:14.180]   whether it's biological or in some kind of other way.
[01:51:14.180 --> 01:51:17.220]   Do you think that's a possibility
[01:51:17.220 --> 01:51:19.220]   of having that kind of symbiosis?
[01:51:19.220 --> 01:51:20.940]   Or do we wanna instead kind of focus
[01:51:20.940 --> 01:51:25.940]   on this distinct entities of us humans
[01:51:25.940 --> 01:51:31.740]   talking to these intelligible, self-doubting AIs,
[01:51:31.740 --> 01:51:33.580]   maybe like Stuart Russell thinks about it,
[01:51:33.580 --> 01:51:37.620]   like we're self-doubting and full of uncertainty,
[01:51:37.620 --> 01:51:39.740]   and then have our AI systems that are full of uncertainty,
[01:51:39.740 --> 01:51:41.540]   we communicate back and forth,
[01:51:41.540 --> 01:51:43.740]   and in that way achieve symbiosis?
[01:51:43.740 --> 01:51:46.220]   - I honestly don't know.
[01:51:46.220 --> 01:51:48.580]   I would say that because we don't know for sure
[01:51:48.580 --> 01:51:52.220]   what if any of our, which of any of our ideas will work,
[01:51:52.220 --> 01:51:55.220]   but we do know that if we don't,
[01:51:55.220 --> 01:51:56.700]   I'm pretty convinced that if we don't get
[01:51:56.700 --> 01:51:59.860]   any of these things to work and just barge ahead,
[01:51:59.860 --> 01:52:03.740]   then our species is probably gonna go extinct this century.
[01:52:03.740 --> 01:52:05.540]   I think-- - This century.
[01:52:05.540 --> 01:52:10.540]   You think we're facing this crisis is a 21st century crisis.
[01:52:10.540 --> 01:52:12.460]   - Oh yeah. - This century
[01:52:12.460 --> 01:52:16.100]   will be remembered. (laughs)
[01:52:16.100 --> 01:52:18.540]   - On a hard drive somewhere. - On a hard drive somewhere.
[01:52:18.540 --> 01:52:22.300]   - Or maybe by future generations as like,
[01:52:22.300 --> 01:52:26.260]   like there'll be future, Future of Life Institute awards
[01:52:26.260 --> 01:52:30.700]   for people that have done something about AI.
[01:52:30.700 --> 01:52:31.900]   - It could also end even worse,
[01:52:31.900 --> 01:52:33.740]   where there is, we're not superseded
[01:52:33.740 --> 01:52:35.300]   by leaving any AI behind either.
[01:52:35.300 --> 01:52:38.500]   We just totally wipe out, you know, like on Easter Island.
[01:52:38.500 --> 01:52:39.900]   Our century is long.
[01:52:39.900 --> 01:52:44.300]   No, there are still 79 years left of it, right?
[01:52:44.300 --> 01:52:47.700]   Think about how far we've come just in the last 30 years.
[01:52:47.700 --> 01:52:52.700]   So we can talk more about what might go wrong,
[01:52:52.700 --> 01:52:54.580]   but you asked me this really good question
[01:52:54.580 --> 01:52:55.780]   about what's the best strategy.
[01:52:55.780 --> 01:52:59.780]   Is it Neuralink or Russell's approach or whatever?
[01:52:59.780 --> 01:53:04.780]   I think, you know, when we did the Manhattan Project,
[01:53:04.780 --> 01:53:08.460]   we didn't know if any of our four ideas
[01:53:08.460 --> 01:53:11.740]   for enriching uranium and getting out the uranium-235
[01:53:11.740 --> 01:53:12.900]   were gonna work.
[01:53:12.900 --> 01:53:14.780]   But we felt this was really important
[01:53:14.780 --> 01:53:16.700]   to get it before Hitler did.
[01:53:16.700 --> 01:53:19.500]   So, you know, what we did, we tried all four of them.
[01:53:19.500 --> 01:53:24.100]   Here, I think it's analogous where there's the greatest
[01:53:24.100 --> 01:53:25.940]   threat that's ever faced our species.
[01:53:25.940 --> 01:53:29.260]   And of course, US national security by implication.
[01:53:29.260 --> 01:53:31.500]   We don't know, we don't have any method
[01:53:31.500 --> 01:53:34.660]   that's guaranteed to work, but we have a lot of ideas.
[01:53:34.660 --> 01:53:36.860]   So we should invest pretty heavily in pursuing all of them
[01:53:36.860 --> 01:53:40.540]   with an open mind and hope that one of them at least works.
[01:53:40.540 --> 01:53:45.340]   These are, the good news is the century is long, you know,
[01:53:45.340 --> 01:53:48.860]   and it might take decades until we have
[01:53:48.860 --> 01:53:50.180]   artificial general intelligence.
[01:53:50.180 --> 01:53:52.740]   So we have some time, hopefully.
[01:53:52.740 --> 01:53:55.260]   But it takes a long time for us to solve
[01:53:55.260 --> 01:53:57.100]   these very, very difficult problems.
[01:53:57.100 --> 01:53:59.140]   It's gonna actually be, it's the most difficult problem
[01:53:59.140 --> 01:54:01.340]   we were ever trying to solve as a species.
[01:54:01.340 --> 01:54:04.260]   So we have to start now, so we don't have,
[01:54:04.260 --> 01:54:05.860]   rather than, you know, begin thinking about it
[01:54:05.860 --> 01:54:08.740]   the night before some people who've had too much Red Bull
[01:54:08.740 --> 01:54:09.580]   switch it on.
[01:54:09.580 --> 01:54:11.860]   And we have to, coming back to your question,
[01:54:11.860 --> 01:54:14.260]   we have to pursue all of these different avenues and see.
[01:54:14.260 --> 01:54:16.820]   If you were my investment advisor,
[01:54:16.820 --> 01:54:19.900]   and I was trying to invest in the future,
[01:54:19.900 --> 01:54:22.140]   how do you think the human species
[01:54:22.140 --> 01:54:27.540]   is most likely to destroy itself in this century?
[01:54:27.540 --> 01:54:33.540]   Yeah, so if the crises, many of the crises we're facing
[01:54:33.540 --> 01:54:37.260]   are really before us within the next 100 years,
[01:54:37.260 --> 01:54:42.260]   how do we make explicit the,
[01:54:42.340 --> 01:54:46.660]   make known the unknowns and solve those problems
[01:54:46.660 --> 01:54:48.220]   to avoid the biggest,
[01:54:48.220 --> 01:54:51.940]   starting with the biggest existential crisis?
[01:54:51.940 --> 01:54:53.180]   - So as your investment advisor,
[01:54:53.180 --> 01:54:54.740]   how are you planning to make money
[01:54:54.740 --> 01:54:57.340]   on us destroying ourselves, I have to ask?
[01:54:57.340 --> 01:54:58.180]   - I don't know.
[01:54:58.180 --> 01:55:00.100]   It might be the Russian origins.
[01:55:00.100 --> 01:55:02.860]   Somehow it's involved.
[01:55:02.860 --> 01:55:04.740]   - At the micro level of detailed strategies,
[01:55:04.740 --> 01:55:06.700]   of course, these are unsolved problems.
[01:55:06.700 --> 01:55:12.260]   For AI alignment, we can break it into three sub-problems.
[01:55:12.260 --> 01:55:14.420]   That are all unsolved, I think.
[01:55:14.420 --> 01:55:18.380]   You want first to make machines understand our goals,
[01:55:18.380 --> 01:55:23.380]   then adopt our goals, and then retain our goals.
[01:55:23.380 --> 01:55:26.140]   So to hit on all three real quickly.
[01:55:26.140 --> 01:55:31.100]   The problem when Andreas Lubitz told his autopilot
[01:55:31.100 --> 01:55:34.340]   to fly into the Alps was that the computer
[01:55:34.340 --> 01:55:39.060]   didn't even understand anything about his goals.
[01:55:39.060 --> 01:55:40.500]   It was too dumb.
[01:55:40.500 --> 01:55:41.980]   It could have understood, actually.
[01:55:41.980 --> 01:55:45.300]   But you would have had to put some effort in
[01:55:45.300 --> 01:55:46.860]   as the system designer.
[01:55:46.860 --> 01:55:48.860]   Don't fly into mountains.
[01:55:48.860 --> 01:55:49.940]   So that's the first challenge.
[01:55:49.940 --> 01:55:52.140]   How do you program into computers
[01:55:52.140 --> 01:55:55.300]   human values, human goals?
[01:55:55.300 --> 01:55:58.260]   We could start, rather than saying,
[01:55:58.260 --> 01:56:00.340]   oh, it's so hard, we should start with the simple stuff,
[01:56:00.340 --> 01:56:01.180]   as I said.
[01:56:01.180 --> 01:56:04.100]   Self-driving cars, airplanes,
[01:56:04.100 --> 01:56:07.220]   just put in all the goals that we all agree on already.
[01:56:07.220 --> 01:56:10.500]   And then have a habit of whenever machines get smarter,
[01:56:10.500 --> 01:56:14.140]   so they can understand one level higher goals,
[01:56:14.140 --> 01:56:16.900]   put them in too.
[01:56:16.900 --> 01:56:20.780]   The second challenge is getting them to adopt the goals.
[01:56:20.780 --> 01:56:22.260]   It's easy for situations like that,
[01:56:22.260 --> 01:56:23.220]   where you just program it in.
[01:56:23.220 --> 01:56:25.940]   But when you have self-learning systems like children,
[01:56:25.940 --> 01:56:32.380]   any parent knows that there's a difference
[01:56:32.380 --> 01:56:33.980]   between getting our kids to understand
[01:56:33.980 --> 01:56:34.820]   what we want them to do,
[01:56:34.820 --> 01:56:37.620]   and to actually adopt our goals.
[01:56:37.620 --> 01:56:39.580]   With humans, with children,
[01:56:39.580 --> 01:56:44.020]   unfortunately, they go through this phase.
[01:56:44.020 --> 01:56:46.260]   First, they're too dumb to understand what we want,
[01:56:46.260 --> 01:56:47.100]   our goals are.
[01:56:47.100 --> 01:56:50.420]   And then they have this period of some years,
[01:56:50.420 --> 01:56:52.100]   when they're both smart enough to understand them,
[01:56:52.100 --> 01:56:53.540]   and malleable enough that we have a chance
[01:56:53.540 --> 01:56:55.420]   to raise them well.
[01:56:55.420 --> 01:56:56.860]   And then they become teenagers,
[01:56:56.860 --> 01:56:59.220]   and it's kind of too late.
[01:56:59.220 --> 01:57:00.620]   But we have this window.
[01:57:00.620 --> 01:57:01.940]   With machines, the challenges,
[01:57:01.940 --> 01:57:04.140]   the intelligence might grow so fast
[01:57:04.140 --> 01:57:05.940]   that that window is pretty short.
[01:57:05.940 --> 01:57:08.540]   So that's a research problem.
[01:57:08.540 --> 01:57:11.380]   The third one is, how do you make sure they keep the goals,
[01:57:11.380 --> 01:57:14.580]   if they keep learning more and getting smarter?
[01:57:14.580 --> 01:57:17.460]   Many sci-fi movies are about how you have something
[01:57:17.460 --> 01:57:18.540]   which initially was aligned,
[01:57:18.540 --> 01:57:20.380]   but then things kind of go off the heel.
[01:57:20.380 --> 01:57:24.700]   And my kids were very, very excited
[01:57:24.700 --> 01:57:27.420]   about their Legos when they were little.
[01:57:27.420 --> 01:57:29.820]   Now they're just gathering dust in the basement.
[01:57:29.820 --> 01:57:32.620]   If we create machines that are really on board
[01:57:32.620 --> 01:57:34.380]   with the goal of taking care of humanity,
[01:57:34.380 --> 01:57:36.340]   we don't want them to get as bored with us,
[01:57:36.340 --> 01:57:39.540]   as my kids got with Legos.
[01:57:39.540 --> 01:57:41.940]   So this is another research challenge.
[01:57:41.940 --> 01:57:43.460]   How can you make some sort of recursively
[01:57:43.460 --> 01:57:47.480]   self-improving system retain certain basic goals?
[01:57:47.480 --> 01:57:50.940]   - That said, a lot of adult people still play with Legos.
[01:57:50.940 --> 01:57:52.780]   So maybe we succeeded with the Legos.
[01:57:52.780 --> 01:57:54.660]   - Maybe, I like your optimism.
[01:57:54.660 --> 01:57:55.500]   (laughing)
[01:57:55.500 --> 01:57:58.820]   - So not all AI systems have to maintain the goals, right?
[01:57:58.820 --> 01:58:00.220]   Some just some fraction.
[01:58:00.220 --> 01:58:04.940]   - Yeah, so there's a lot of talented AI researchers now
[01:58:04.940 --> 01:58:07.300]   who have heard of this and wanna work on it.
[01:58:07.300 --> 01:58:08.940]   Not so much funding for it yet.
[01:58:08.940 --> 01:58:13.900]   Of the billions that go into building AI more powerful,
[01:58:13.900 --> 01:58:16.260]   it's only a minuscule fraction.
[01:58:16.260 --> 01:58:18.340]   So for going into the safety research,
[01:58:18.340 --> 01:58:20.080]   my attitude is generally we should not try
[01:58:20.080 --> 01:58:21.580]   to slow down the technology,
[01:58:21.580 --> 01:58:23.420]   but we should greatly accelerate the investment
[01:58:23.420 --> 01:58:25.020]   in this sort of safety research.
[01:58:25.020 --> 01:58:29.380]   And also make sure, this was very embarrassing last year,
[01:58:29.380 --> 01:58:33.720]   but the NSF decided to give out six of these big institutes.
[01:58:33.720 --> 01:58:37.100]   We got one of them for AI and science, you asked me about.
[01:58:37.100 --> 01:58:39.800]   Another one was supposed to be for AI safety research.
[01:58:39.800 --> 01:58:43.580]   And they gave it to people studying oceans
[01:58:43.580 --> 01:58:44.980]   and climate and stuff.
[01:58:44.980 --> 01:58:49.380]   So I'm all for studying oceans and climates,
[01:58:49.380 --> 01:58:51.220]   but we need to actually have some money
[01:58:51.220 --> 01:58:53.460]   that actually goes into AI safety research also
[01:58:53.460 --> 01:58:55.500]   and doesn't just get grabbed by whatever.
[01:58:55.500 --> 01:58:58.020]   That's a fantastic investment.
[01:58:58.020 --> 01:59:00.580]   And then at the higher level, you asked this question,
[01:59:00.580 --> 01:59:02.760]   okay, what can we do?
[01:59:02.760 --> 01:59:04.120]   What are the biggest risks?
[01:59:04.120 --> 01:59:08.840]   I think we cannot just consider this
[01:59:08.840 --> 01:59:11.080]   to be only a technical problem.
[01:59:11.080 --> 01:59:13.720]   Again, 'cause if you solve only the technical problem,
[01:59:13.720 --> 01:59:14.720]   can I play with your robot?
[01:59:14.720 --> 01:59:15.560]   - Yes, please.
[01:59:15.560 --> 01:59:20.620]   - If we can get our machines to just blindly obey
[01:59:20.620 --> 01:59:21.940]   the orders we give them,
[01:59:21.940 --> 01:59:26.180]   so we can always trust that it will do what we want,
[01:59:26.180 --> 01:59:28.480]   that might be great for the owner of the robot,
[01:59:28.480 --> 01:59:31.420]   but it might not be so great for the rest of humanity
[01:59:31.420 --> 01:59:34.140]   if that person is that least favorite world leader
[01:59:34.140 --> 01:59:35.740]   or whatever you imagine, right?
[01:59:35.740 --> 01:59:40.060]   So we have to also take a look at the apply alignment,
[01:59:40.060 --> 01:59:42.020]   not just to machines,
[01:59:42.020 --> 01:59:44.600]   but to all the other powerful structures.
[01:59:44.600 --> 01:59:45.780]   That's why it's so important
[01:59:45.780 --> 01:59:47.060]   to strengthen our democracy again.
[01:59:47.060 --> 01:59:48.580]   As I said, to have institutions,
[01:59:48.580 --> 01:59:51.460]   make sure that the playing field is not rigged
[01:59:51.460 --> 01:59:54.860]   so that corporations are given the right incentives
[01:59:54.860 --> 01:59:57.280]   to do the things that both make profit
[01:59:57.280 --> 01:59:58.900]   and are good for people,
[01:59:58.900 --> 02:00:00.980]   to make sure that countries have incentives
[02:00:00.980 --> 02:00:03.380]   to do things that are both good for their people
[02:00:03.380 --> 02:00:06.860]   and don't screw up the rest of the world.
[02:00:06.860 --> 02:00:10.300]   And this is not just something for AI nerds to geek out on.
[02:00:10.300 --> 02:00:13.100]   This is an interesting challenge for political scientists,
[02:00:13.100 --> 02:00:16.820]   economists, and so many other thinkers.
[02:00:16.820 --> 02:00:20.940]   - So one of the magical things that perhaps makes
[02:00:22.860 --> 02:00:27.280]   this earth quite unique is that it's home
[02:00:27.280 --> 02:00:28.840]   to conscious beings.
[02:00:28.840 --> 02:00:30.440]   So you mentioned consciousness.
[02:00:30.440 --> 02:00:35.000]   Perhaps as a small aside,
[02:00:35.000 --> 02:00:36.720]   because we didn't really get specific
[02:00:36.720 --> 02:00:39.440]   to how we might do the alignment.
[02:00:39.440 --> 02:00:41.080]   Like you said, it's just a really important
[02:00:41.080 --> 02:00:41.920]   research problem.
[02:00:41.920 --> 02:00:44.720]   But do you think engineering consciousness
[02:00:44.720 --> 02:00:49.720]   into AI systems is a possibility?
[02:00:49.880 --> 02:00:53.060]   Is something that we might one day do?
[02:00:53.060 --> 02:00:56.820]   Or is there something fundamental to consciousness
[02:00:56.820 --> 02:00:59.860]   that is, is there something about consciousness
[02:00:59.860 --> 02:01:02.360]   that is fundamental to humans and humans only?
[02:01:02.360 --> 02:01:04.620]   - I think it's possible.
[02:01:04.620 --> 02:01:08.340]   I think both consciousness and intelligence
[02:01:08.340 --> 02:01:10.740]   are information processing,
[02:01:10.740 --> 02:01:13.480]   certain types of information processing.
[02:01:13.480 --> 02:01:15.980]   And that fundamentally, it doesn't matter
[02:01:15.980 --> 02:01:19.020]   whether the information is processed by carbon atoms
[02:01:19.020 --> 02:01:22.660]   in neurons and brains or by silicon atoms
[02:01:22.660 --> 02:01:25.940]   and so on in our technology.
[02:01:25.940 --> 02:01:28.260]   Some people disagree.
[02:01:28.260 --> 02:01:30.200]   This is what I think as a physicist.
[02:01:30.200 --> 02:01:34.980]   - That consciousness is the same kind of,
[02:01:34.980 --> 02:01:37.700]   you said consciousness is information processing.
[02:01:37.700 --> 02:01:42.700]   So meaning, I think you had a quote of something like,
[02:01:42.700 --> 02:01:47.740]   it's information knowing itself, that kind of thing.
[02:01:47.740 --> 02:01:51.060]   - I think consciousness is the way information feels
[02:01:51.060 --> 02:01:51.980]   when it's being processed.
[02:01:51.980 --> 02:01:52.820]   - Once people die, yeah.
[02:01:52.820 --> 02:01:53.660]   - In complex ways.
[02:01:53.660 --> 02:01:56.140]   We don't know exactly what those complex ways are.
[02:01:56.140 --> 02:01:59.260]   It's clear that most of the information processing
[02:01:59.260 --> 02:02:01.740]   in our brains does not create an experience.
[02:02:01.740 --> 02:02:03.620]   We're not even aware of it.
[02:02:03.620 --> 02:02:06.340]   Like for example, you're not aware
[02:02:06.340 --> 02:02:07.900]   of your heartbeat regulation right now,
[02:02:07.900 --> 02:02:10.660]   even though it's clearly being done by your body.
[02:02:10.660 --> 02:02:12.140]   It's just kind of doing its own thing.
[02:02:12.140 --> 02:02:15.340]   When you go jogging, there's a lot of complicated stuff
[02:02:15.340 --> 02:02:17.860]   about how you put your foot down.
[02:02:17.860 --> 02:02:18.740]   And we know it's hard.
[02:02:18.740 --> 02:02:20.620]   That's why robots used to fall over so much.
[02:02:20.620 --> 02:02:22.760]   But you're mostly unaware about it.
[02:02:22.760 --> 02:02:25.780]   Your brain, your CEO consciousness module
[02:02:25.780 --> 02:02:28.140]   just sends an email, hey, I'm gonna keep jogging
[02:02:28.140 --> 02:02:29.180]   along this path.
[02:02:29.180 --> 02:02:31.620]   The rest is on autopilot, right?
[02:02:31.620 --> 02:02:33.220]   So most of it is not conscious,
[02:02:33.220 --> 02:02:36.660]   but somehow there is some of the information processing,
[02:02:36.660 --> 02:02:41.660]   which is we don't know what exactly.
[02:02:41.660 --> 02:02:44.140]   I think this is a science problem
[02:02:44.140 --> 02:02:47.660]   that I hope one day we'll have some equation for
[02:02:47.660 --> 02:02:49.060]   or something so we can be able to build
[02:02:49.060 --> 02:02:50.980]   a consciousness detector and say, yeah,
[02:02:50.980 --> 02:02:53.900]   here there is some consciousness, here there is not.
[02:02:53.900 --> 02:02:56.620]   Oh, don't boil that lobster because it's feeling pain
[02:02:56.620 --> 02:02:59.860]   or it's okay because it's not feeling pain.
[02:02:59.860 --> 02:03:02.460]   Right now we treat this as sort of just metaphysics,
[02:03:02.460 --> 02:03:06.900]   but it would be very useful in emergency rooms
[02:03:06.900 --> 02:03:09.740]   to know if a patient has locked in syndrome
[02:03:09.740 --> 02:03:14.580]   and is conscious or if they are actually just out.
[02:03:14.580 --> 02:03:17.740]   And in the future, if you build a very, very intelligent
[02:03:17.740 --> 02:03:20.100]   helper robot to take care of you,
[02:03:20.100 --> 02:03:22.500]   I think you'd like to know if you should feel guilty
[02:03:22.500 --> 02:03:26.180]   by shutting it down or if it's just like a zombie
[02:03:26.180 --> 02:03:28.880]   going through the motions like a fancy tape recorder.
[02:03:28.880 --> 02:03:34.060]   Once we can make progress on the science of consciousness
[02:03:34.060 --> 02:03:38.340]   and figure out what is conscious and what isn't,
[02:03:38.340 --> 02:03:43.340]   then we, assuming we wanna create positive experiences
[02:03:43.340 --> 02:03:48.900]   and not suffering, we'll probably choose to build
[02:03:48.900 --> 02:03:51.780]   some machines that are deliberately unconscious
[02:03:51.780 --> 02:03:56.780]   that do incredibly boring, repetitive jobs
[02:03:56.780 --> 02:03:59.700]   in an iron mine somewhere or whatever.
[02:03:59.700 --> 02:04:03.120]   And maybe we'll choose to create helper robots
[02:04:03.120 --> 02:04:05.340]   for the elderly that are conscious
[02:04:05.340 --> 02:04:07.060]   so that people don't just feel creeped out,
[02:04:07.060 --> 02:04:10.180]   that the robot is just faking it
[02:04:10.180 --> 02:04:12.140]   when it acts like it's sad or happy.
[02:04:12.140 --> 02:04:14.500]   - Like you said, elderly, I think everybody
[02:04:14.500 --> 02:04:16.900]   gets pretty deeply lonely in this world.
[02:04:16.900 --> 02:04:19.660]   And so there's a place, I think, for everybody
[02:04:19.660 --> 02:04:21.660]   to have a connection with conscious beings,
[02:04:21.660 --> 02:04:24.400]   whether they're human or otherwise.
[02:04:24.400 --> 02:04:28.820]   - But I know for sure that I would, if I had a robot,
[02:04:28.820 --> 02:04:31.060]   if I was gonna develop any kind of personal,
[02:04:31.060 --> 02:04:33.820]   emotional connection with it, I would be very creeped out
[02:04:33.820 --> 02:04:35.260]   if I knew it at an intellectual level
[02:04:35.260 --> 02:04:36.820]   that the whole thing was just a fraud.
[02:04:36.820 --> 02:04:41.820]   Now, today you can buy a little talking doll for a kid,
[02:04:41.820 --> 02:04:45.340]   which will say things, and the little child
[02:04:45.340 --> 02:04:47.820]   will often think that this is actually conscious
[02:04:47.820 --> 02:04:50.420]   and even real secrets to it that then go on the internet
[02:04:50.420 --> 02:04:52.580]   and with all sorts of creepy repercussions.
[02:04:52.580 --> 02:04:58.060]   I would not wanna be just hacked and tricked like this.
[02:04:58.060 --> 02:05:01.580]   If I was gonna be developing real emotional connections
[02:05:01.580 --> 02:05:05.420]   with a robot, I would wanna know that this is actually real.
[02:05:05.420 --> 02:05:08.100]   It's acting conscious, acting happy
[02:05:08.100 --> 02:05:09.900]   because it actually feels it.
[02:05:09.900 --> 02:05:11.420]   And I think this is not sci-fi.
[02:05:11.420 --> 02:05:13.580]   I think-- - It's possible to measure,
[02:05:13.580 --> 02:05:15.560]   to come up with tools and make,
[02:05:15.560 --> 02:05:17.540]   after we understand the science of consciousness,
[02:05:17.540 --> 02:05:19.780]   you're saying we'll be able to come up with tools
[02:05:19.780 --> 02:05:23.020]   that can measure consciousness and definitively say
[02:05:23.020 --> 02:05:26.060]   this thing is experiencing the things
[02:05:26.060 --> 02:05:28.300]   it says it's experiencing. - Yeah, kind of by definition.
[02:05:28.300 --> 02:05:31.540]   If it is a physical phenomenon, information processing,
[02:05:31.540 --> 02:05:34.020]   and we know that some information processing is conscious
[02:05:34.020 --> 02:05:35.980]   and some isn't, well, then there is something there
[02:05:35.980 --> 02:05:38.020]   to be discovered with the methods of science.
[02:05:38.020 --> 02:05:41.100]   Giulio Tononi has stuck his neck out the farthest
[02:05:41.100 --> 02:05:43.620]   and written down some equations for a theory.
[02:05:43.620 --> 02:05:45.700]   Maybe that's right, maybe it's wrong.
[02:05:45.700 --> 02:05:47.060]   We certainly don't know.
[02:05:47.060 --> 02:05:50.460]   But I applaud that kind of efforts to sort of take this,
[02:05:50.460 --> 02:05:53.940]   say this is not just something that philosophers
[02:05:53.940 --> 02:05:56.340]   can have beer and muse about,
[02:05:56.340 --> 02:05:58.740]   but something we can measure and study.
[02:05:58.740 --> 02:06:00.580]   And coming, bringing that back to us,
[02:06:00.580 --> 02:06:02.980]   I think what we would probably choose to do, as I said,
[02:06:02.980 --> 02:06:04.580]   is if we cannot figure this out,
[02:06:04.580 --> 02:06:09.020]   choose to make, be quite mindful
[02:06:09.020 --> 02:06:11.300]   about what sort of consciousness, if any,
[02:06:11.300 --> 02:06:13.740]   we put in different machines that we have.
[02:06:13.740 --> 02:06:19.020]   And certainly, we wouldn't wanna make,
[02:06:19.020 --> 02:06:21.700]   should not be making a bunch of machines that suffer
[02:06:21.700 --> 02:06:23.580]   without us even knowing it, right?
[02:06:23.580 --> 02:06:28.260]   And if at any point someone decides to upload themselves,
[02:06:28.260 --> 02:06:30.060]   like Ray Kurzweil wants to do,
[02:06:30.060 --> 02:06:31.420]   I don't know if you've had him on your show.
[02:06:31.420 --> 02:06:32.900]   - We agree, but then COVID happened,
[02:06:32.900 --> 02:06:34.580]   so we're waiting it out a little bit.
[02:06:34.580 --> 02:06:38.460]   - Suppose he uploads himself into this robo-Ray,
[02:06:38.460 --> 02:06:42.100]   and it talks like him and acts like him and laughs like him,
[02:06:42.100 --> 02:06:44.780]   and before he powers off his biological body,
[02:06:44.780 --> 02:06:47.660]   he would probably be pretty disturbed
[02:06:47.660 --> 02:06:49.540]   if he realized that there's no one home.
[02:06:49.540 --> 02:06:52.760]   This robot is not having any subjective experience, right?
[02:06:52.760 --> 02:06:58.740]   If humanity gets replaced by machine descendants
[02:06:59.820 --> 02:07:02.260]   which do all these cool things and build spaceships
[02:07:02.260 --> 02:07:05.620]   and go to intergalactic rock concerts,
[02:07:05.620 --> 02:07:09.980]   and it turns out that they are all unconscious,
[02:07:09.980 --> 02:07:11.420]   just going through the motions,
[02:07:11.420 --> 02:07:16.180]   wouldn't that be like the ultimate zombie apocalypse, right?
[02:07:16.180 --> 02:07:18.020]   Just a play for empty benches?
[02:07:18.020 --> 02:07:21.180]   - Yeah, I have a sense that there's some kind of,
[02:07:21.180 --> 02:07:22.780]   once we understand consciousness better,
[02:07:22.780 --> 02:07:25.620]   we'll understand that there's some kind of continuum,
[02:07:25.620 --> 02:07:28.020]   and it would be a greater appreciation.
[02:07:28.020 --> 02:07:30.460]   And we'll probably understand, just like you said,
[02:07:30.460 --> 02:07:32.420]   it'd be unfortunate if it's a trick.
[02:07:32.420 --> 02:07:33.940]   We'll probably definitively understand
[02:07:33.940 --> 02:07:37.780]   that love is indeed a trick that we play on each other,
[02:07:37.780 --> 02:07:40.960]   that we humans are, we convince ourselves we're conscious,
[02:07:40.960 --> 02:07:45.220]   but we're really, us and trees and dolphins
[02:07:45.220 --> 02:07:46.620]   are all the same kind of consciousness.
[02:07:46.620 --> 02:07:48.140]   - Can I try to cheer you up a little bit
[02:07:48.140 --> 02:07:50.260]   with a philosophical thought here about the love part?
[02:07:50.260 --> 02:07:51.360]   - Yes, let's do it.
[02:07:51.360 --> 02:07:54.380]   - You might say, okay, yeah,
[02:07:54.380 --> 02:07:56.960]   love is just a collaboration enabler.
[02:07:57.960 --> 02:08:01.840]   And then maybe you can go and get depressed about that.
[02:08:01.840 --> 02:08:05.080]   But I think that would be the wrong conclusion, actually.
[02:08:05.080 --> 02:08:08.680]   I know that the only reason I enjoy food
[02:08:08.680 --> 02:08:11.040]   is because my genes hacked me,
[02:08:11.040 --> 02:08:13.760]   and they don't want me to starve to death.
[02:08:13.760 --> 02:08:16.760]   Not because they care about me
[02:08:16.760 --> 02:08:19.080]   consciously enjoying succulent delights
[02:08:19.080 --> 02:08:21.120]   of pistachio ice cream,
[02:08:21.120 --> 02:08:23.360]   but they just want me to make copies of them.
[02:08:23.360 --> 02:08:24.560]   The whole thing, so in a sense,
[02:08:24.560 --> 02:08:29.000]   the whole enjoyment of food is also a scam like this.
[02:08:29.000 --> 02:08:31.320]   But does that mean I shouldn't take pleasure
[02:08:31.320 --> 02:08:32.600]   in this pistachio ice cream?
[02:08:32.600 --> 02:08:34.960]   I love pistachio ice cream, and I can tell you,
[02:08:34.960 --> 02:08:38.240]   I know this is an experimental fact,
[02:08:38.240 --> 02:08:41.640]   I enjoy pistachio ice cream every bit as much,
[02:08:41.640 --> 02:08:43.680]   even though I scientifically know exactly
[02:08:43.680 --> 02:08:46.880]   what kind of scam this was.
[02:08:46.880 --> 02:08:48.680]   - Your genes really appreciate
[02:08:48.680 --> 02:08:50.480]   that you like the pistachio ice cream.
[02:08:50.480 --> 02:08:53.120]   - Well, but my mind appreciates it too,
[02:08:53.120 --> 02:08:55.840]   and I have a conscious experience right now.
[02:08:55.840 --> 02:08:58.680]   Ultimately, all of my brain is also just something
[02:08:58.680 --> 02:09:01.360]   the genes built to copy themselves, but so what?
[02:09:01.360 --> 02:09:05.000]   I'm grateful that, yeah, thanks genes for doing this,
[02:09:05.000 --> 02:09:07.640]   but now it's my brain that's in charge here,
[02:09:07.640 --> 02:09:09.560]   and I'm gonna enjoy my conscious experience,
[02:09:09.560 --> 02:09:12.480]   thank you very much, and not just the pistachio ice cream,
[02:09:12.480 --> 02:09:15.480]   but also the love I feel for my amazing wife,
[02:09:15.480 --> 02:09:19.400]   and all the other delights of being conscious.
[02:09:20.240 --> 02:09:25.080]   Actually, Richard Feynman, I think, said this so well,
[02:09:25.080 --> 02:09:27.960]   he is also the guy who really got me into physics.
[02:09:27.960 --> 02:09:31.240]   Some art friend said that,
[02:09:31.240 --> 02:09:34.520]   oh, science kind of just is the party pooper,
[02:09:34.520 --> 02:09:36.280]   it kind of ruins the fun, right?
[02:09:36.280 --> 02:09:39.680]   When like, you have a beautiful flower, says the artist,
[02:09:39.680 --> 02:09:41.600]   and then the scientist is gonna deconstruct that
[02:09:41.600 --> 02:09:44.160]   into just a blob of quarks and electrons,
[02:09:44.160 --> 02:09:46.080]   and Feynman just pushed back on that
[02:09:46.080 --> 02:09:47.480]   in such a beautiful way,
[02:09:47.480 --> 02:09:49.920]   which I think also can be used to push back
[02:09:49.920 --> 02:09:53.200]   and make you not feel guilty about falling in love.
[02:09:53.200 --> 02:09:54.880]   So here's what Feynman basically said,
[02:09:54.880 --> 02:09:56.920]   he said to his friend,
[02:09:56.920 --> 02:09:58.880]   yeah, I can also, as a scientist,
[02:09:58.880 --> 02:10:00.960]   see that this is a beautiful flower, thank you very much.
[02:10:00.960 --> 02:10:03.280]   Maybe I can't draw as good a painting as you,
[02:10:03.280 --> 02:10:04.560]   'cause I'm not as talented an artist,
[02:10:04.560 --> 02:10:06.800]   but yeah, I can really see the beauty in it,
[02:10:06.800 --> 02:10:09.360]   and it also looks beautiful to me.
[02:10:09.360 --> 02:10:12.200]   But in addition to that, Feynman said, as a scientist,
[02:10:12.200 --> 02:10:16.960]   I see even more beauty that the artist did not see, right?
[02:10:16.960 --> 02:10:21.080]   Suppose this is a flower on a blossoming apple tree,
[02:10:21.080 --> 02:10:23.840]   you could say this tree has more beauty in it
[02:10:23.840 --> 02:10:26.400]   than just the colors and the fragrance.
[02:10:26.400 --> 02:10:29.040]   This tree is made of air, Feynman wrote.
[02:10:29.040 --> 02:10:31.240]   This is one of my favorite Feynman quotes ever.
[02:10:31.240 --> 02:10:33.760]   And it took the carbon out of the air
[02:10:33.760 --> 02:10:36.160]   and bound it in using the flaming heat of the sun,
[02:10:36.160 --> 02:10:38.600]   you know, to turn the air into a tree,
[02:10:38.600 --> 02:10:42.760]   and when you burn logs in your fireplace,
[02:10:42.760 --> 02:10:45.120]   it's really beautiful to think that this is being reversed.
[02:10:45.120 --> 02:10:48.560]   Now the tree is going, the wood is going back into air,
[02:10:48.560 --> 02:10:52.520]   and in this flaming, beautiful dance of the fire
[02:10:52.520 --> 02:10:55.880]   that the artist can see is the flaming light of the sun
[02:10:55.880 --> 02:10:59.120]   that was bound in to turn the air into tree,
[02:10:59.120 --> 02:11:01.440]   and then the ashes is the little residue
[02:11:01.440 --> 02:11:02.560]   that didn't come from the air,
[02:11:02.560 --> 02:11:04.280]   that the tree sucked out of the ground.
[02:11:04.280 --> 02:11:06.160]   Feynman said, these are beautiful things,
[02:11:06.160 --> 02:11:10.000]   and science just adds, it doesn't subtract.
[02:11:10.000 --> 02:11:12.760]   And I feel exactly that way about love
[02:11:12.760 --> 02:11:14.840]   and about pistachio ice cream also.
[02:11:14.840 --> 02:11:18.720]   I can understand that there is even more nuance
[02:11:18.720 --> 02:11:20.520]   to the whole thing, right?
[02:11:20.520 --> 02:11:23.680]   At this very visceral level, you can fall in love
[02:11:23.680 --> 02:11:26.680]   just as much as someone who knows nothing about neuroscience
[02:11:26.680 --> 02:11:32.800]   but you can also appreciate this even greater beauty in it.
[02:11:32.800 --> 02:11:35.640]   Isn't it remarkable that it came about
[02:11:35.640 --> 02:11:38.600]   from this completely lifeless universe,
[02:11:38.600 --> 02:11:43.120]   just a bunch of hot blob of plasma expanding?
[02:11:43.120 --> 02:11:46.200]   And then over the eons, gradually,
[02:11:46.200 --> 02:11:48.480]   first the strong nuclear force decided
[02:11:48.480 --> 02:11:50.960]   to combine quarks together into nuclei,
[02:11:50.960 --> 02:11:53.080]   and then the electric force bound in electrons
[02:11:53.080 --> 02:11:55.280]   and made atoms, and then they clustered from gravity,
[02:11:55.280 --> 02:11:57.760]   and you got planets and stars and this and that,
[02:11:57.760 --> 02:12:00.080]   and then natural selection came along,
[02:12:00.080 --> 02:12:01.840]   and the genes had their little thing,
[02:12:01.840 --> 02:12:04.680]   and you started getting what went from seeming
[02:12:04.680 --> 02:12:06.280]   like a completely pointless universe
[02:12:06.280 --> 02:12:08.080]   that was just trying to increase entropy
[02:12:08.080 --> 02:12:10.200]   and approach heat death into something
[02:12:10.200 --> 02:12:11.760]   that looked more goal-oriented.
[02:12:11.760 --> 02:12:13.280]   Isn't that kind of beautiful?
[02:12:13.280 --> 02:12:15.800]   And then this goal-orientedness through evolution
[02:12:15.800 --> 02:12:18.760]   got ever more sophisticated where you got ever more,
[02:12:18.760 --> 02:12:20.160]   and then you started getting this thing
[02:12:20.160 --> 02:12:25.160]   which is kind of like DeepMind's mu zero and steroids,
[02:12:25.160 --> 02:12:29.440]   the ultimate self-play is not what DeepMind's AI
[02:12:29.440 --> 02:12:32.120]   does against itself to get better at the go.
[02:12:32.120 --> 02:12:34.480]   It's what all these little quark blobs did
[02:12:34.480 --> 02:12:38.960]   against each other in the game of survival of the fittest.
[02:12:38.960 --> 02:12:42.320]   Now, when you had really dumb bacteria living
[02:12:42.320 --> 02:12:45.200]   in a simple environment, there wasn't much incentive
[02:12:45.200 --> 02:12:49.520]   to get intelligent, but then the life made environment
[02:12:49.520 --> 02:12:52.080]   more complex, and then there was more incentive
[02:12:52.080 --> 02:12:56.000]   to get even smarter, and that gave the other organisms
[02:12:56.000 --> 02:12:57.560]   more of incentive to also get smarter,
[02:12:57.560 --> 02:13:02.560]   and then here we are now, just like mu zero learned
[02:13:02.560 --> 02:13:05.560]   to become world master at the go and chess
[02:13:05.560 --> 02:13:08.600]   from playing against itself, by just playing against itself.
[02:13:08.600 --> 02:13:11.360]   All the quarks here on our planet and electrons
[02:13:11.360 --> 02:13:16.360]   have created giraffes and elephants and humans and love.
[02:13:16.360 --> 02:13:21.200]   I just find that really beautiful, and to me,
[02:13:21.200 --> 02:13:24.240]   that just adds to the enjoyment of love.
[02:13:24.240 --> 02:13:25.720]   It doesn't subtract anything.
[02:13:25.720 --> 02:13:27.400]   Do you feel a little more cheerful now?
[02:13:27.400 --> 02:13:29.000]   - I feel way better.
[02:13:29.000 --> 02:13:30.680]   That was incredible.
[02:13:30.680 --> 02:13:34.600]   So this self-play of quarks, taking back
[02:13:34.600 --> 02:13:37.080]   to the beginning of our conversation a little bit,
[02:13:37.080 --> 02:13:39.560]   there's so many exciting possibilities
[02:13:39.560 --> 02:13:40.840]   about artificial intelligence,
[02:13:40.840 --> 02:13:44.260]   understanding the basic laws of physics.
[02:13:44.260 --> 02:13:47.420]   Do you think AI will help us unlock,
[02:13:47.420 --> 02:13:49.280]   there's been quite a bit of excitement
[02:13:49.280 --> 02:13:51.720]   throughout the history of physics of coming up
[02:13:51.720 --> 02:13:55.880]   with more and more general, simple laws
[02:13:55.880 --> 02:13:58.440]   that explain the nature of our reality,
[02:13:58.440 --> 02:14:01.120]   and then the ultimate of that would be a theory
[02:14:01.120 --> 02:14:03.680]   of everything that combines everything together.
[02:14:03.680 --> 02:14:07.440]   Do you think it's possible that, well, one, we humans,
[02:14:07.440 --> 02:14:12.440]   but perhaps AI systems will figure out a theory of physics
[02:14:12.440 --> 02:14:16.200]   that unifies all the laws of physics?
[02:14:16.200 --> 02:14:19.920]   - Yeah, I think it's absolutely possible.
[02:14:19.920 --> 02:14:22.820]   I think it's very clear that we're gonna see
[02:14:22.820 --> 02:14:24.960]   a great boost to science.
[02:14:24.960 --> 02:14:26.720]   We're already seeing a boost, actually,
[02:14:26.720 --> 02:14:28.760]   from machine learning helping science.
[02:14:28.760 --> 02:14:30.520]   Alpha fold was an example,
[02:14:30.520 --> 02:14:33.340]   the decades-old protein folding problem.
[02:14:33.340 --> 02:14:38.160]   So, and gradually, yeah, unless we go extinct
[02:14:38.160 --> 02:14:39.720]   by doing something dumb like we discussed,
[02:14:39.720 --> 02:14:44.720]   I think it's very likely that our understanding
[02:14:44.720 --> 02:14:48.080]   of physics will become so good
[02:14:48.080 --> 02:14:53.080]   that our technology will no longer be limited
[02:14:53.080 --> 02:14:56.280]   by human intelligence, but instead be limited
[02:14:56.280 --> 02:14:57.440]   by the laws of physics.
[02:14:57.440 --> 02:15:00.120]   So our tech today is limited
[02:15:00.120 --> 02:15:02.160]   by what we've been able to invent, right?
[02:15:02.160 --> 02:15:05.840]   I think as AI progresses, it'll just be limited
[02:15:05.840 --> 02:15:09.280]   by the speed of light and other physical limits,
[02:15:09.280 --> 02:15:13.120]   which will mean it's gonna be just dramatically
[02:15:13.120 --> 02:15:15.320]   beyond where we are now.
[02:15:15.320 --> 02:15:18.600]   - Do you think it's a fundamentally mathematical pursuit
[02:15:18.600 --> 02:15:22.120]   of trying to understand the laws
[02:15:22.120 --> 02:15:25.760]   that govern our universe from a mathematical perspective?
[02:15:25.760 --> 02:15:28.000]   It's almost like if it's AI,
[02:15:28.000 --> 02:15:31.640]   it's exploring the space of theorems
[02:15:31.640 --> 02:15:33.520]   and those kinds of things.
[02:15:33.520 --> 02:15:38.520]   Or is there some other more computational ideas,
[02:15:38.520 --> 02:15:41.280]   more sort of empirical ideas?
[02:15:41.280 --> 02:15:43.120]   - They're both, I would say.
[02:15:43.120 --> 02:15:45.920]   It's really interesting to look out at the landscape
[02:15:45.920 --> 02:15:48.000]   of everything we call science today.
[02:15:48.000 --> 02:15:50.200]   So here you come now with this big new hammer.
[02:15:50.200 --> 02:15:51.480]   It says machine learning on it,
[02:15:51.480 --> 02:15:53.400]   and that's, you know, where are there some nails
[02:15:53.400 --> 02:15:56.640]   that you can help with here that you can hammer?
[02:15:56.640 --> 02:16:00.160]   Ultimately, if machine learning gets to the point
[02:16:00.160 --> 02:16:02.840]   that it can do everything better than us,
[02:16:02.840 --> 02:16:06.040]   it will be able to help across the whole space of science.
[02:16:06.040 --> 02:16:08.160]   But maybe we can anchor it by starting a little bit
[02:16:08.160 --> 02:16:11.680]   right now near term and see how we kind of move forward.
[02:16:11.680 --> 02:16:14.880]   So like right now, first of all,
[02:16:14.880 --> 02:16:17.400]   you have a lot of big data science, right?
[02:16:17.400 --> 02:16:19.400]   Where, for example, with telescopes,
[02:16:19.400 --> 02:16:24.120]   we are able to collect way more data every hour
[02:16:24.120 --> 02:16:26.720]   than a grad student can just pour over
[02:16:26.720 --> 02:16:28.760]   like in the old times, right?
[02:16:28.760 --> 02:16:31.040]   And machine learning is already being used very effectively,
[02:16:31.040 --> 02:16:32.120]   even at MIT, right?
[02:16:32.120 --> 02:16:34.680]   To find planets around other stars,
[02:16:34.680 --> 02:16:36.560]   to detect exciting new signatures
[02:16:36.560 --> 02:16:38.760]   of new particle physics in the sky,
[02:16:38.760 --> 02:16:42.960]   to detect the ripples in the fabric of space-time
[02:16:42.960 --> 02:16:44.640]   that we call gravitational waves
[02:16:44.640 --> 02:16:47.720]   caused by enormous black holes crashing into each other
[02:16:47.720 --> 02:16:49.920]   halfway across our observable universe.
[02:16:49.920 --> 02:16:52.680]   Machine learning is running and taking it right now,
[02:16:52.680 --> 02:16:53.800]   doing all these things,
[02:16:53.800 --> 02:16:57.560]   and it's really helping all these experimental fields.
[02:16:57.560 --> 02:17:03.240]   There is a separate front of physics, computational physics,
[02:17:03.240 --> 02:17:05.680]   which is getting an enormous boost also.
[02:17:05.680 --> 02:17:09.520]   So we had to do all our computations by hand, right?
[02:17:09.520 --> 02:17:11.240]   People would have these giant books
[02:17:11.240 --> 02:17:14.200]   with tables of logarithms, and oh my God,
[02:17:15.440 --> 02:17:17.880]   it pains me to even think how long time
[02:17:17.880 --> 02:17:19.920]   it would have taken to do simple stuff.
[02:17:19.920 --> 02:17:23.600]   Then we started to get little calculators and computers
[02:17:23.600 --> 02:17:26.560]   that could do some basic math for us.
[02:17:26.560 --> 02:17:28.880]   Now, what we're starting to see is
[02:17:28.880 --> 02:17:35.640]   kind of a shift from Go-Fi computational physics
[02:17:35.640 --> 02:17:40.040]   to neural network computational physics.
[02:17:40.040 --> 02:17:44.560]   What I mean by that is most computational physics
[02:17:44.560 --> 02:17:48.520]   would be done by humans programming in
[02:17:48.520 --> 02:17:50.240]   the intelligence of how to do the computation
[02:17:50.240 --> 02:17:51.200]   into the computer.
[02:17:51.200 --> 02:17:55.440]   Just as when Garry Kasparov got his posterior kicked
[02:17:55.440 --> 02:17:56.920]   by IBM's Deep Blue in chess,
[02:17:56.920 --> 02:17:59.920]   humans had programmed in exactly how to play chess.
[02:17:59.920 --> 02:18:01.200]   Intelligence came from the humans.
[02:18:01.200 --> 02:18:02.440]   It wasn't learned, right?
[02:18:02.440 --> 02:18:08.520]   Mu zero can be not only Kasparov in chess,
[02:18:08.520 --> 02:18:09.880]   but also Stockfish,
[02:18:09.880 --> 02:18:12.600]   which is the best sort of Go-Fi chess program.
[02:18:13.560 --> 02:18:17.640]   By learning, and we're seeing more of that now,
[02:18:17.640 --> 02:18:19.400]   that shift beginning to happen in physics.
[02:18:19.400 --> 02:18:21.520]   Let me give you an example.
[02:18:21.520 --> 02:18:25.080]   So lattice QCD is an area of physics
[02:18:25.080 --> 02:18:28.320]   whose goal is basically to take the periodic table
[02:18:28.320 --> 02:18:31.080]   and just compute the whole thing from first principles.
[02:18:31.080 --> 02:18:34.840]   This is not the search for theory of everything.
[02:18:34.840 --> 02:18:38.320]   We already know the theory that's supposed to produce
[02:18:38.320 --> 02:18:40.680]   this output, the periodic table,
[02:18:40.680 --> 02:18:43.320]   which atoms are stable, how heavy they are,
[02:18:43.320 --> 02:18:45.440]   all that good stuff, their spectral lines.
[02:18:45.440 --> 02:18:48.720]   It's a theory, lattice QCD,
[02:18:48.720 --> 02:18:50.600]   you can put it on your t-shirt.
[02:18:50.600 --> 02:18:51.760]   Our colleague, Frank Wilczek,
[02:18:51.760 --> 02:18:53.720]   got the Nobel prize for working on it.
[02:18:53.720 --> 02:18:57.160]   But the math is just too hard for us to solve.
[02:18:57.160 --> 02:18:59.240]   We have not been able to start with these equations
[02:18:59.240 --> 02:19:02.080]   and solve them to the extent that we can predict, oh yeah.
[02:19:02.080 --> 02:19:03.960]   And then there is carbon,
[02:19:03.960 --> 02:19:07.640]   and this is what the spectrum of the carbon atom looks like.
[02:19:07.640 --> 02:19:10.000]   But awesome people are building
[02:19:10.000 --> 02:19:12.080]   these super computer simulations
[02:19:12.080 --> 02:19:15.000]   where you just put in these equations
[02:19:15.000 --> 02:19:20.000]   and you make a big cubic lattice of space,
[02:19:20.000 --> 02:19:22.120]   or actually it's a very small lattice
[02:19:22.120 --> 02:19:25.680]   because you're going down to the subatomic scale,
[02:19:25.680 --> 02:19:26.920]   and you try to solve it.
[02:19:26.920 --> 02:19:29.000]   But it's just so computationally expensive
[02:19:29.000 --> 02:19:31.840]   that we still haven't been able to calculate things
[02:19:31.840 --> 02:19:35.000]   as accurately as we measure them in many cases.
[02:19:35.000 --> 02:19:37.560]   And now machine learning is really revolutionizing this.
[02:19:37.560 --> 02:19:40.080]   So my colleague, Fiola Shanahan at MIT, for example,
[02:19:40.080 --> 02:19:44.640]   she's been using this really cool machine learning technique
[02:19:44.640 --> 02:19:47.600]   called normalizing flows,
[02:19:47.600 --> 02:19:49.360]   where she's realized she can actually
[02:19:49.360 --> 02:19:52.240]   speed up the calculation dramatically
[02:19:52.240 --> 02:19:54.680]   by having the AI learn how to do things faster.
[02:19:54.680 --> 02:20:00.720]   Another area like this where we suck up
[02:20:00.720 --> 02:20:05.080]   an enormous amount of super computer time to do physics
[02:20:05.080 --> 02:20:07.200]   is black hole collisions.
[02:20:07.200 --> 02:20:08.640]   So now that we've done the sexy stuff
[02:20:08.640 --> 02:20:10.480]   of detecting a bunch of this,
[02:20:10.480 --> 02:20:11.720]   LIGO and other experiments,
[02:20:11.720 --> 02:20:15.120]   we want to be able to know what we're seeing.
[02:20:15.120 --> 02:20:18.160]   And so it's a very simple conceptual problem.
[02:20:18.160 --> 02:20:19.560]   It's the two-body problem.
[02:20:19.560 --> 02:20:24.720]   Newton solved it for classical gravity hundreds of years ago,
[02:20:24.720 --> 02:20:27.800]   but the two-body problem is still not fully solved.
[02:20:27.800 --> 02:20:29.200]   - For black holes.
[02:20:29.200 --> 02:20:30.760]   - Yes, and Einstein's gravity,
[02:20:30.760 --> 02:20:33.560]   because they won't just orbit each other forever anymore,
[02:20:33.560 --> 02:20:36.040]   two things, they give off gravitational waves,
[02:20:36.040 --> 02:20:37.800]   and eventually they crash into each other.
[02:20:37.800 --> 02:20:39.440]   And the game, what you want to do
[02:20:39.440 --> 02:20:41.520]   is you want to figure out, okay,
[02:20:41.520 --> 02:20:43.480]   what kind of wave comes out
[02:20:43.480 --> 02:20:46.320]   as a function of the masses of the two black holes,
[02:20:46.320 --> 02:20:48.120]   as a function of how they're spinning
[02:20:48.120 --> 02:20:50.720]   relative to each other, et cetera.
[02:20:50.720 --> 02:20:52.080]   And that is so hard.
[02:20:52.080 --> 02:20:54.200]   It can take months of super computer time
[02:20:54.200 --> 02:20:56.160]   and massive numbers of cores to do it.
[02:20:56.160 --> 02:21:00.400]   Wouldn't it be great if you can use machine learning
[02:21:00.400 --> 02:21:03.280]   to greatly speed that up, right?
[02:21:04.760 --> 02:21:09.360]   Now you can use the expensive old Gophi calculation
[02:21:09.360 --> 02:21:11.920]   as the truth, and then see if machine learning
[02:21:11.920 --> 02:21:13.600]   can figure out a smarter, faster way
[02:21:13.600 --> 02:21:15.000]   of getting the right answer.
[02:21:15.000 --> 02:21:20.000]   Yet another area of computational physics.
[02:21:20.000 --> 02:21:22.280]   These are probably the big three
[02:21:22.280 --> 02:21:24.240]   that suck up the most computer time,
[02:21:24.240 --> 02:21:27.160]   lattice QCD, black hole collisions,
[02:21:27.160 --> 02:21:29.600]   and cosmological simulations,
[02:21:29.600 --> 02:21:32.320]   where you take not a subatomic thing
[02:21:32.320 --> 02:21:34.440]   and try to figure out the mass of the proton,
[02:21:34.440 --> 02:21:37.720]   but you take something that's enormous
[02:21:37.720 --> 02:21:41.360]   and try to look at how all the galaxies get formed in there.
[02:21:41.360 --> 02:21:44.760]   There again, there are a lot of very cool ideas right now
[02:21:44.760 --> 02:21:46.440]   about how you can use machine learning
[02:21:46.440 --> 02:21:48.040]   to do this sort of stuff better.
[02:21:48.040 --> 02:21:51.600]   The difference between this and the big data
[02:21:51.600 --> 02:21:54.600]   is you kind of make the data yourself, right?
[02:21:54.600 --> 02:21:58.480]   So, and then finally,
[02:21:58.480 --> 02:22:00.240]   we're looking over the physics landscape
[02:22:00.240 --> 02:22:02.160]   and seeing what can we hammer with machine learning, right?
[02:22:02.160 --> 02:22:05.520]   So we talked about experimental data, big data,
[02:22:05.520 --> 02:22:07.880]   discovering cool stuff that we humans
[02:22:07.880 --> 02:22:09.520]   then look more closely at.
[02:22:09.520 --> 02:22:13.440]   Then we talked about taking the expensive computations
[02:22:13.440 --> 02:22:15.520]   we're doing now and figuring out how to do them
[02:22:15.520 --> 02:22:18.560]   much faster and better with AI.
[02:22:18.560 --> 02:22:20.880]   And finally, let's go really theoretical.
[02:22:20.880 --> 02:22:24.000]   So things like discovering equations,
[02:22:24.000 --> 02:22:27.520]   having deep fundamental insights.
[02:22:28.800 --> 02:22:33.000]   This is something closest to what I've been doing
[02:22:33.000 --> 02:22:33.840]   in my group.
[02:22:33.840 --> 02:22:35.880]   We talked earlier about the whole AI Feynman project
[02:22:35.880 --> 02:22:37.880]   where if you just have some data,
[02:22:37.880 --> 02:22:39.800]   how do you automatically discover equations
[02:22:39.800 --> 02:22:42.160]   that seem to describe this well
[02:22:42.160 --> 02:22:44.080]   that you can then go back as a human
[02:22:44.080 --> 02:22:46.600]   and work with and test and explore?
[02:22:46.600 --> 02:22:51.160]   And you asked a really good question also
[02:22:51.160 --> 02:22:54.880]   about if this is sort of a search problem in some sense.
[02:22:54.880 --> 02:22:57.720]   That's very deep actually what you said, because it is.
[02:22:57.720 --> 02:23:01.040]   Suppose I ask you to prove some mathematical theorem.
[02:23:01.040 --> 02:23:03.680]   What is a proof in math?
[02:23:03.680 --> 02:23:05.520]   It's just a long string of steps,
[02:23:05.520 --> 02:23:08.720]   logical steps that you can write out with symbols.
[02:23:08.720 --> 02:23:10.960]   And once you find it, it's very easy to write a program
[02:23:10.960 --> 02:23:13.760]   to check whether it's a valid proof or not.
[02:23:13.760 --> 02:23:16.880]   So why is it so hard to prove it then?
[02:23:16.880 --> 02:23:19.760]   Well, because there are ridiculously many possible
[02:23:19.760 --> 02:23:22.400]   candidate proofs you could write down, right?
[02:23:22.400 --> 02:23:26.240]   If the proof contains 10,000 symbols,
[02:23:26.240 --> 02:23:28.560]   even if there are only 10 options
[02:23:28.560 --> 02:23:29.960]   for what each symbol could be,
[02:23:29.960 --> 02:23:34.200]   that's 10 to the power of 1,000 possible proofs,
[02:23:34.200 --> 02:23:36.800]   which is way more than there are atoms in our universe.
[02:23:36.800 --> 02:23:39.200]   So you could say it's trivial to prove these things.
[02:23:39.200 --> 02:23:42.000]   You just write a computer, generate all strings,
[02:23:42.000 --> 02:23:43.560]   and then check, is this a valid proof?
[02:23:43.560 --> 02:23:44.920]   Eh, no.
[02:23:44.920 --> 02:23:46.000]   Is this a valid proof?
[02:23:46.000 --> 02:23:46.840]   Eh, no.
[02:23:46.840 --> 02:23:51.020]   And then you just keep doing this forever.
[02:23:51.020 --> 02:23:53.160]   But there are a lot of,
[02:23:53.160 --> 02:23:55.120]   but it is fundamentally a search problem.
[02:23:55.120 --> 02:23:59.920]   You just want to search the space of all strings of symbols
[02:23:59.920 --> 02:24:02.960]   to find the one, find one that is the proof, right?
[02:24:02.960 --> 02:24:08.840]   And there's a whole area of machine learning called search.
[02:24:08.840 --> 02:24:10.600]   How do you search through some giant space
[02:24:10.600 --> 02:24:12.360]   to find the needle in the haystack?
[02:24:12.360 --> 02:24:17.200]   It's easier in cases where there's a clear measure of good,
[02:24:17.200 --> 02:24:18.840]   like you're not just right or wrong,
[02:24:18.840 --> 02:24:20.680]   but this is better and this is worse,
[02:24:20.680 --> 02:24:21.840]   so you can maybe get some hints
[02:24:21.840 --> 02:24:23.840]   as to which direction to go in.
[02:24:23.840 --> 02:24:27.040]   That's why we talked about neural networks work so well.
[02:24:27.040 --> 02:24:30.720]   - I mean, that's such a human thing
[02:24:30.720 --> 02:24:32.320]   of that moment of genius,
[02:24:32.320 --> 02:24:37.320]   of figuring out the intuition of good, essentially.
[02:24:37.320 --> 02:24:38.800]   I mean, we thought that that was--
[02:24:38.800 --> 02:24:40.160]   - Or is it?
[02:24:40.160 --> 02:24:41.360]   - Maybe it's not, right?
[02:24:41.360 --> 02:24:42.760]   We thought that about chess, right?
[02:24:42.760 --> 02:24:43.880]   - Exactly.
[02:24:43.880 --> 02:24:46.880]   - That the ability to see like 10, 15,
[02:24:46.880 --> 02:24:50.720]   sometimes 20 steps ahead was not a calculation
[02:24:50.720 --> 02:24:51.800]   that humans were performing.
[02:24:51.800 --> 02:24:53.760]   It was some kind of weird intuition
[02:24:53.760 --> 02:24:57.320]   about different patterns, about board positions,
[02:24:57.320 --> 02:24:58.760]   about the relative positions.
[02:24:58.760 --> 02:24:59.600]   - Exactly.
[02:24:59.600 --> 02:25:01.680]   - Somehow stitching stuff together.
[02:25:01.680 --> 02:25:03.960]   And a lot of it is just like intuition.
[02:25:03.960 --> 02:25:06.320]   But then you have like Alpha, I guess,
[02:25:06.320 --> 02:25:10.680]   Zero be the first one that did the self-play.
[02:25:10.680 --> 02:25:12.200]   It just came up with this.
[02:25:12.200 --> 02:25:14.600]   It was able to learn through self-play mechanism,
[02:25:14.600 --> 02:25:15.800]   this kind of intuition.
[02:25:15.800 --> 02:25:16.840]   - Exactly.
[02:25:16.840 --> 02:25:20.000]   - But just like you said, it's so fascinating to think
[02:25:20.000 --> 02:25:24.640]   within the space of totally new ideas,
[02:25:24.640 --> 02:25:28.960]   can that be done in developing theorems?
[02:25:28.960 --> 02:25:30.800]   - We know it can be done by neural networks
[02:25:30.800 --> 02:25:32.280]   'cause we did it with the neural networks
[02:25:32.280 --> 02:25:36.240]   in the cranium of the great mathematicians of humanity.
[02:25:36.240 --> 02:25:38.600]   And I'm so glad you brought up Alpha, Zero
[02:25:38.600 --> 02:25:39.960]   'cause that's the counter example.
[02:25:39.960 --> 02:25:41.840]   It turned out we were flattering ourselves
[02:25:41.840 --> 02:25:45.000]   when we said intuition is something different.
[02:25:45.000 --> 02:25:46.520]   It's only humans can do it.
[02:25:46.520 --> 02:25:48.120]   It's not information processing.
[02:25:49.240 --> 02:25:52.320]   If you, if it used to be that way,
[02:25:52.320 --> 02:25:56.200]   again, it's really instructive, I think,
[02:25:56.200 --> 02:25:58.480]   to compare the chess computer, Deep Blue,
[02:25:58.480 --> 02:26:02.040]   that beat Kasparov with Alpha, Zero
[02:26:02.040 --> 02:26:04.280]   that beat Lissadol at the go.
[02:26:04.280 --> 02:26:08.640]   Because for Deep Blue, there was no intuition.
[02:26:08.640 --> 02:26:12.000]   There was some pro, humans had programmed in some intuition.
[02:26:12.000 --> 02:26:13.600]   After humans had played a lot of games,
[02:26:13.600 --> 02:26:15.040]   they told the computer, you know,
[02:26:15.040 --> 02:26:18.720]   count the pawn as one point, the bishop as three points,
[02:26:18.720 --> 02:26:20.320]   the rook as five points and so on.
[02:26:20.320 --> 02:26:22.480]   You add it all up and then you add some extra points
[02:26:22.480 --> 02:26:25.480]   for past pawns and subtract if the opponent has it
[02:26:25.480 --> 02:26:27.340]   and blah, blah, blah, blah.
[02:26:27.340 --> 02:26:31.600]   And then what Deep Blue did was just search.
[02:26:31.600 --> 02:26:35.000]   Just very brute force, tried many, many moves ahead,
[02:26:35.000 --> 02:26:37.440]   all these combinations in a pruned tree search.
[02:26:37.440 --> 02:26:40.480]   And it could think much faster than Kasparov
[02:26:40.480 --> 02:26:41.520]   and it won, right?
[02:26:41.520 --> 02:26:45.480]   And that, I think, inflated our egos
[02:26:45.480 --> 02:26:46.600]   in a way it shouldn't have
[02:26:46.600 --> 02:26:48.800]   'cause people started to say, yeah, yeah,
[02:26:48.800 --> 02:26:51.400]   it's just brute force search but it has no intuition.
[02:26:51.400 --> 02:26:57.320]   Alpha, Zero really popped our bubble there
[02:26:57.320 --> 02:27:00.880]   because what Alpha, Zero does,
[02:27:00.880 --> 02:27:03.900]   yes, it does also do some of that tree search,
[02:27:03.900 --> 02:27:06.560]   but it also has this intuition module
[02:27:06.560 --> 02:27:09.560]   which in geek speak is called a value function
[02:27:09.560 --> 02:27:11.120]   where it just looks at the board
[02:27:11.120 --> 02:27:13.960]   and comes up with a number for how good is that position.
[02:27:15.160 --> 02:27:17.960]   The difference was no human told it
[02:27:17.960 --> 02:27:19.280]   how good the position is.
[02:27:19.280 --> 02:27:21.080]   It just learned it.
[02:27:21.080 --> 02:27:26.820]   And Mu Zero is the coolest or scariest of all,
[02:27:26.820 --> 02:27:28.200]   depending on your mood,
[02:27:28.200 --> 02:27:32.040]   because the same basic AI system
[02:27:32.040 --> 02:27:35.320]   will learn what the good board position is
[02:27:35.320 --> 02:27:38.640]   regardless of whether it's chess or Go or Shogi
[02:27:38.640 --> 02:27:42.920]   or Pac-Man or Lady Pac-Man or Breakout or Space Invaders
[02:27:42.920 --> 02:27:45.000]   or any number, a bunch of other games.
[02:27:45.000 --> 02:27:45.840]   You don't tell it anything
[02:27:45.840 --> 02:27:49.760]   and it gets this intuition after a while for what's good.
[02:27:49.760 --> 02:27:52.760]   So this is very hopeful for science, I think,
[02:27:52.760 --> 02:27:55.240]   because if it can get intuition
[02:27:55.240 --> 02:27:57.280]   for what's a good position there,
[02:27:57.280 --> 02:27:58.880]   maybe it can also get intuition
[02:27:58.880 --> 02:28:00.620]   for what are some good directions to go
[02:28:00.620 --> 02:28:02.480]   if you're trying to prove something.
[02:28:02.480 --> 02:28:06.400]   One of the most fun things in my science career
[02:28:06.400 --> 02:28:08.640]   is when I've been able to prove some theorem about something
[02:28:08.640 --> 02:28:12.200]   and it's very heavily intuition guided, of course.
[02:28:12.200 --> 02:28:14.280]   I don't sit and try all random strings.
[02:28:14.280 --> 02:28:17.720]   I have a hunch that this reminds me a little bit
[02:28:17.720 --> 02:28:20.000]   about this other proof I've seen for this thing.
[02:28:20.000 --> 02:28:22.600]   So maybe I first, what if I try this?
[02:28:22.600 --> 02:28:23.960]   Nah, that didn't work out.
[02:28:23.960 --> 02:28:25.920]   But this reminds me, actually,
[02:28:25.920 --> 02:28:28.600]   the way this failed reminds me of that.
[02:28:28.600 --> 02:28:30.520]   So combining the intuition
[02:28:30.520 --> 02:28:34.880]   with all these brute force capabilities,
[02:28:34.880 --> 02:28:38.600]   I think it's gonna be able to help physics too.
[02:28:38.600 --> 02:28:42.960]   - Do you think there'll be a day when an AI system
[02:28:42.960 --> 02:28:45.120]   being the primary contributor,
[02:28:45.120 --> 02:28:48.260]   let's say 90% plus, wins a Nobel Prize in physics?
[02:28:48.260 --> 02:28:52.000]   Obviously, they'll give it to the humans
[02:28:52.000 --> 02:28:54.840]   'cause we humans don't like to give prizes to machines.
[02:28:54.840 --> 02:28:57.600]   It'll give it to the humans behind the system.
[02:28:57.600 --> 02:28:59.960]   You could argue that AI has already been involved
[02:28:59.960 --> 02:29:01.600]   in some Nobel Prizes, probably,
[02:29:01.600 --> 02:29:03.600]   maybe some to the black holes and stuff like that.
[02:29:03.600 --> 02:29:07.200]   - Yeah, we don't like giving prizes to other life forms.
[02:29:07.200 --> 02:29:09.760]   If someone wins a horse racing contest,
[02:29:09.760 --> 02:29:11.360]   they don't give the prize to horse either.
[02:29:11.360 --> 02:29:12.200]   - It's true.
[02:29:13.160 --> 02:29:16.040]   But do you think that we might be able to see
[02:29:16.040 --> 02:29:19.240]   something like that in our lifetimes when AI?
[02:29:19.240 --> 02:29:21.880]   So the first system, I would say,
[02:29:21.880 --> 02:29:25.400]   that makes us think about a Nobel Prize seriously
[02:29:25.400 --> 02:29:28.760]   is like AlphaFold is making us think about,
[02:29:28.760 --> 02:29:31.960]   in medicine physiology, a Nobel Prize,
[02:29:31.960 --> 02:29:34.080]   perhaps discoveries that are a direct result
[02:29:34.080 --> 02:29:36.640]   of something that's discovered by AlphaFold.
[02:29:36.640 --> 02:29:38.760]   Do you think in physics,
[02:29:38.760 --> 02:29:41.520]   we might be able to see that in our lifetimes?
[02:29:41.520 --> 02:29:43.520]   - I think what's probably gonna happen
[02:29:43.520 --> 02:29:46.880]   is more of a blurring of the distinctions.
[02:29:46.880 --> 02:29:51.880]   So today, if somebody uses a computer
[02:29:51.880 --> 02:29:54.920]   to do a computation that gives them the Nobel Prize,
[02:29:54.920 --> 02:29:57.160]   nobody's gonna dream of giving the prize to the computer.
[02:29:57.160 --> 02:29:59.000]   They're gonna be like, "That was just a tool."
[02:29:59.000 --> 02:30:02.120]   I think for these things, also,
[02:30:02.120 --> 02:30:04.000]   people are just gonna, for a long time,
[02:30:04.000 --> 02:30:06.080]   view the computer as a tool.
[02:30:06.080 --> 02:30:11.080]   But what's gonna change is the ubiquity of machine learning.
[02:30:11.320 --> 02:30:16.160]   I think at some point in my lifetime,
[02:30:16.160 --> 02:30:19.880]   finding a human physicist
[02:30:19.880 --> 02:30:22.240]   who knows nothing about machine learning
[02:30:22.240 --> 02:30:24.760]   is gonna be almost as hard as it is today
[02:30:24.760 --> 02:30:26.760]   finding a human physicist who doesn't,
[02:30:26.760 --> 02:30:29.160]   says, "Oh, I don't know anything about computers,"
[02:30:29.160 --> 02:30:30.880]   or, "I don't use math."
[02:30:30.880 --> 02:30:32.880]   That would just be a ridiculous concept.
[02:30:32.880 --> 02:30:35.400]   - You see, but the thing is,
[02:30:35.400 --> 02:30:40.080]   there is a magic moment, though, like with AlphaZero,
[02:30:40.080 --> 02:30:42.960]   when the system surprises us in a way
[02:30:42.960 --> 02:30:45.640]   where the best people in the world
[02:30:45.640 --> 02:30:48.960]   truly learn something from the system
[02:30:48.960 --> 02:30:52.480]   in a way where you feel like it's another entity.
[02:30:52.480 --> 02:30:54.920]   Like the way people, the way Magnus Carlsen,
[02:30:54.920 --> 02:30:58.080]   the way certain people are looking at the work of AlphaZero,
[02:30:58.080 --> 02:31:03.080]   it's like it truly is no longer a tool
[02:31:03.080 --> 02:31:06.680]   in the sense that it doesn't feel like a tool.
[02:31:06.680 --> 02:31:08.920]   It feels like some other entity.
[02:31:08.920 --> 02:31:12.480]   So there is a magic difference where you're like,
[02:31:12.480 --> 02:31:17.320]   if an AI system is able to come up with an insight
[02:31:17.320 --> 02:31:22.320]   that surprises everybody in some major way
[02:31:22.320 --> 02:31:25.960]   that's a phase shift in our understanding
[02:31:25.960 --> 02:31:27.760]   of some particular science
[02:31:27.760 --> 02:31:30.040]   or some particular aspect of physics,
[02:31:30.040 --> 02:31:32.640]   I feel like that is no longer a tool.
[02:31:32.640 --> 02:31:34.840]   And then you can start to say
[02:31:34.840 --> 02:31:38.720]   that it perhaps deserves the prize.
[02:31:38.720 --> 02:31:40.680]   So for sure, the more important
[02:31:40.680 --> 02:31:43.120]   and the more fundamental transformation
[02:31:43.120 --> 02:31:46.640]   of the 21st century science is exactly what you're saying,
[02:31:46.640 --> 02:31:48.800]   which is probably everybody
[02:31:48.800 --> 02:31:51.560]   will be doing machine learning to some degree.
[02:31:51.560 --> 02:31:54.760]   Like if you want to be successful
[02:31:54.760 --> 02:31:57.560]   at unlocking the mysteries of science,
[02:31:57.560 --> 02:31:58.800]   you should be doing machine learning.
[02:31:58.800 --> 02:32:01.440]   But it's just exciting to think about
[02:32:01.440 --> 02:32:03.080]   whether there'll be one that comes along
[02:32:03.080 --> 02:32:08.080]   that's super surprising and they'll make us question
[02:32:08.240 --> 02:32:10.360]   who the real inventors are in this world.
[02:32:10.360 --> 02:32:12.200]   - Yeah, yeah.
[02:32:12.200 --> 02:32:15.440]   I think the question of isn't if it's gonna happen,
[02:32:15.440 --> 02:32:16.720]   but when.
[02:32:16.720 --> 02:32:19.440]   But it's important, honestly, in my mind,
[02:32:19.440 --> 02:32:22.280]   the time when that happens is also more or less
[02:32:22.280 --> 02:32:25.600]   the same time when we get artificial general intelligence.
[02:32:25.600 --> 02:32:28.200]   And then we have a lot bigger things to worry about
[02:32:28.200 --> 02:32:31.680]   than whether we should get the Nobel Prize or not, right?
[02:32:31.680 --> 02:32:35.040]   Because when you have machines
[02:32:35.040 --> 02:32:37.960]   that can outperform our best scientists
[02:32:37.960 --> 02:32:41.040]   at science, they can probably outperform us
[02:32:41.040 --> 02:32:43.600]   at a lot of other stuff as well,
[02:32:43.600 --> 02:32:46.440]   which can at a minimum make them
[02:32:46.440 --> 02:32:49.440]   incredibly powerful agents in the world.
[02:32:49.440 --> 02:32:52.760]   And I think it's a mistake
[02:32:52.760 --> 02:32:54.600]   to think we only have to start worrying
[02:32:54.600 --> 02:32:58.960]   about loss of control when machines get to AGI
[02:32:58.960 --> 02:33:02.360]   across the board, when they can do everything, all our jobs.
[02:33:02.360 --> 02:33:03.640]   Long before that,
[02:33:05.160 --> 02:33:07.920]   they'll be hugely influential.
[02:33:07.920 --> 02:33:12.600]   We talked at length about how the hacking of our minds
[02:33:12.600 --> 02:33:17.600]   with algorithms trying to get us glued to our screens,
[02:33:17.600 --> 02:33:22.400]   has already had a big impact on society.
[02:33:22.400 --> 02:33:24.120]   That was an incredibly dumb algorithm
[02:33:24.120 --> 02:33:25.840]   in the grand scheme of things, right?
[02:33:25.840 --> 02:33:27.880]   The supervised machine learning,
[02:33:27.880 --> 02:33:29.560]   yet it had huge impact.
[02:33:29.560 --> 02:33:32.120]   So I just don't want us to be lulled
[02:33:32.120 --> 02:33:33.320]   into a false sense of security
[02:33:33.320 --> 02:33:35.600]   and think there won't be any societal impact
[02:33:35.600 --> 02:33:37.080]   until things reach human level,
[02:33:37.080 --> 02:33:38.320]   'cause it's happening already.
[02:33:38.320 --> 02:33:40.640]   And I was just thinking the other week,
[02:33:40.640 --> 02:33:44.920]   when I see some scaremonger going,
[02:33:44.920 --> 02:33:47.120]   oh, the robots are coming,
[02:33:47.120 --> 02:33:50.000]   the implication is always that they're coming to kill us.
[02:33:50.000 --> 02:33:52.400]   And maybe you shouldn't have worried about that
[02:33:52.400 --> 02:33:54.760]   if you were in Nagorno-Karabakh
[02:33:54.760 --> 02:33:55.760]   during the recent war there.
[02:33:55.760 --> 02:33:57.760]   But more seriously,
[02:33:57.760 --> 02:34:01.480]   the robots are coming right now,
[02:34:01.480 --> 02:34:03.200]   but they're mainly not coming to kill us.
[02:34:03.200 --> 02:34:04.480]   They're coming to hack us.
[02:34:04.480 --> 02:34:08.240]   They're coming to hack our minds
[02:34:08.240 --> 02:34:11.320]   into buying things that maybe we didn't need,
[02:34:11.320 --> 02:34:13.240]   to vote for people who may not have
[02:34:13.240 --> 02:34:15.400]   our best interest in mind.
[02:34:15.400 --> 02:34:17.640]   And it's kind of humbling, I think,
[02:34:17.640 --> 02:34:19.640]   actually, as a human being,
[02:34:19.640 --> 02:34:22.080]   to admit that it turns out that our minds
[02:34:22.080 --> 02:34:24.800]   are actually much more hackable than we thought.
[02:34:24.800 --> 02:34:27.080]   And the ultimate insult is that we are actually
[02:34:27.080 --> 02:34:30.440]   getting hacked by the machine learning algorithms
[02:34:30.440 --> 02:34:31.600]   that are in some objective sense,
[02:34:31.600 --> 02:34:34.000]   much dumber than us.
[02:34:34.000 --> 02:34:35.800]   But maybe we shouldn't be so surprised
[02:34:35.800 --> 02:34:40.600]   because how do you feel about the cute puppies?
[02:34:40.600 --> 02:34:41.640]   - Love them.
[02:34:41.640 --> 02:34:44.400]   - So you would probably argue that
[02:34:44.400 --> 02:34:46.200]   in some across the board measure,
[02:34:46.200 --> 02:34:47.720]   you're more intelligent than they are,
[02:34:47.720 --> 02:34:51.160]   but boy, are our cute puppies good at hacking us, right?
[02:34:51.160 --> 02:34:52.520]   They move into our house,
[02:34:52.520 --> 02:34:54.640]   persuade us to feed them and do all these things.
[02:34:54.640 --> 02:34:56.640]   What do they ever do for us?
[02:34:56.640 --> 02:34:57.480]   - Yeah.
[02:34:57.480 --> 02:35:00.600]   - Other than being cute and making us feel good, right?
[02:35:00.600 --> 02:35:03.120]   So if puppies can hack us,
[02:35:03.120 --> 02:35:04.960]   maybe we shouldn't be so surprised
[02:35:04.960 --> 02:35:09.080]   if pretty dumb machine learning algorithms can hack us too.
[02:35:09.080 --> 02:35:11.760]   - Not to speak of cats, which is another level.
[02:35:11.760 --> 02:35:13.440]   And I think we should,
[02:35:13.440 --> 02:35:15.680]   to counter your previous point about there,
[02:35:15.680 --> 02:35:18.080]   let us not think about evil creatures in this world.
[02:35:18.080 --> 02:35:20.520]   We can all agree that cats are as close
[02:35:20.520 --> 02:35:23.000]   to objective evil as we can get.
[02:35:23.000 --> 02:35:24.440]   But that's just me saying that.
[02:35:24.440 --> 02:35:25.280]   Okay, so you--
[02:35:25.280 --> 02:35:27.360]   - Have you seen the cartoon?
[02:35:27.360 --> 02:35:29.960]   I think it's maybe The Onion.
[02:35:30.720 --> 02:35:33.720]   Where this incredibly cute kitten,
[02:35:33.720 --> 02:35:34.560]   and it just says,
[02:35:34.560 --> 02:35:37.280]   underneath something about,
[02:35:37.280 --> 02:35:38.960]   thinks about murder all day.
[02:35:38.960 --> 02:35:41.560]   - Exactly.
[02:35:41.560 --> 02:35:43.080]   That's accurate.
[02:35:43.080 --> 02:35:45.200]   You've mentioned offline that there might be a link
[02:35:45.200 --> 02:35:47.960]   between post-biological AGI and SETI.
[02:35:47.960 --> 02:35:51.280]   So last time we talked,
[02:35:51.280 --> 02:35:54.920]   you've talked about this intuition
[02:35:54.920 --> 02:35:59.280]   that we humans might be quite unique
[02:35:59.280 --> 02:36:02.320]   in our galactic neighborhood.
[02:36:02.320 --> 02:36:03.680]   Perhaps our galaxy,
[02:36:03.680 --> 02:36:06.360]   perhaps the entirety of the observable universe,
[02:36:06.360 --> 02:36:10.680]   we might be the only intelligent civilization here.
[02:36:10.680 --> 02:36:17.720]   And you argue pretty well for that thought.
[02:36:17.720 --> 02:36:21.240]   So I have a few little questions around this.
[02:36:21.240 --> 02:36:24.680]   One, the scientific question.
[02:36:24.680 --> 02:36:29.240]   In which way would you be,
[02:36:29.240 --> 02:36:33.120]   if you were wrong in that intuition,
[02:36:33.120 --> 02:36:36.680]   in which way do you think you would be surprised?
[02:36:36.680 --> 02:36:38.520]   Like why were you wrong?
[02:36:38.520 --> 02:36:41.600]   We find out that you ended up being wrong.
[02:36:41.600 --> 02:36:43.880]   Like in which dimension?
[02:36:43.880 --> 02:36:48.420]   So like, is it because we can't see them?
[02:36:48.420 --> 02:36:51.320]   Is it because the nature of their intelligence
[02:36:51.320 --> 02:36:54.760]   or the nature of their life is totally different
[02:36:54.760 --> 02:36:56.760]   than we can possibly imagine?
[02:36:56.760 --> 02:37:00.680]   Is it because the,
[02:37:00.680 --> 02:37:02.640]   I mean, something about the great filters
[02:37:02.640 --> 02:37:04.480]   and surviving them?
[02:37:04.480 --> 02:37:08.800]   Or maybe because we're being protected from signals?
[02:37:08.800 --> 02:37:13.800]   All those explanations for why we haven't heard
[02:37:13.800 --> 02:37:20.160]   a big, loud, like red light that says we're here.
[02:37:20.400 --> 02:37:21.720]   - Yeah.
[02:37:21.720 --> 02:37:23.560]   So there are actually two separate things there
[02:37:23.560 --> 02:37:24.720]   that I could be wrong about,
[02:37:24.720 --> 02:37:26.800]   two separate claims that I made, right?
[02:37:26.800 --> 02:37:27.620]   Not them.
[02:37:27.620 --> 02:37:32.240]   One of them is, I made the claim,
[02:37:32.240 --> 02:37:35.500]   I think most civilizations,
[02:37:35.500 --> 02:37:41.800]   when you're going from simple bacteria-like things
[02:37:41.800 --> 02:37:46.800]   to space colonizing civilizations,
[02:37:47.840 --> 02:37:50.840]   they spend only a very, very tiny fraction
[02:37:50.840 --> 02:37:55.200]   of their life being where we are.
[02:37:55.200 --> 02:37:57.280]   That I could be wrong about.
[02:37:57.280 --> 02:37:58.740]   The other one I could be wrong about
[02:37:58.740 --> 02:38:00.800]   is a quite different statement that I think
[02:38:00.800 --> 02:38:02.520]   that actually I'm guessing
[02:38:02.520 --> 02:38:04.680]   that we are the only civilization
[02:38:04.680 --> 02:38:06.120]   in our observable universe
[02:38:06.120 --> 02:38:08.240]   from which light has reached us so far
[02:38:08.240 --> 02:38:12.320]   that's actually gotten far enough to invent telescopes.
[02:38:12.320 --> 02:38:14.000]   So let's talk about maybe both of them in turn
[02:38:14.000 --> 02:38:14.980]   'cause they really are different.
[02:38:14.980 --> 02:38:19.860]   The first one, if you look at the N equals one,
[02:38:19.860 --> 02:38:22.100]   the data point we have on this planet,
[02:38:22.100 --> 02:38:25.900]   so we spent four and a half billion years
[02:38:25.900 --> 02:38:28.260]   futzing around on this planet with life, right?
[02:38:28.260 --> 02:38:32.100]   We got, and most of it was pretty lame stuff
[02:38:32.100 --> 02:38:33.700]   from an intelligence perspective.
[02:38:33.700 --> 02:38:38.160]   Bacteria and then the dinosaurs spent,
[02:38:38.160 --> 02:38:41.300]   then the things gradually accelerated, right?
[02:38:41.300 --> 02:38:43.620]   Then the dinosaurs spent over 100 million years
[02:38:43.620 --> 02:38:46.980]   stomping around here without even inventing smartphones.
[02:38:46.980 --> 02:38:49.780]   And then very recently,
[02:38:49.780 --> 02:38:55.340]   we've only spent 400 years going from Newton to us, right?
[02:38:55.340 --> 02:38:56.500]   In terms of technology.
[02:38:56.500 --> 02:38:59.260]   And look what we've done even,
[02:38:59.260 --> 02:39:02.620]   when I was a little kid, there was no internet even.
[02:39:02.620 --> 02:39:07.080]   So I think it's pretty likely for in this case
[02:39:07.080 --> 02:39:08.180]   of this planet, right?
[02:39:08.180 --> 02:39:12.180]   That we're either gonna really get our act together
[02:39:12.180 --> 02:39:15.100]   and start spreading life into space, the century,
[02:39:15.100 --> 02:39:16.460]   and doing all sorts of great things,
[02:39:16.460 --> 02:39:18.060]   or we're gonna wipe out.
[02:39:18.060 --> 02:39:19.900]   It's a little hard.
[02:39:19.900 --> 02:39:23.500]   I couldn't be wrong in the sense that maybe
[02:39:23.500 --> 02:39:25.780]   what happened on this Earth is very atypical.
[02:39:25.780 --> 02:39:28.540]   And for some reason, what's more common on other planets
[02:39:28.540 --> 02:39:31.440]   is that they spend an enormously long time
[02:39:31.440 --> 02:39:33.720]   futzing around with the ham radio and things,
[02:39:33.720 --> 02:39:36.220]   but they just never really take it to the next level
[02:39:36.220 --> 02:39:38.380]   for reasons I haven't understood.
[02:39:38.380 --> 02:39:40.200]   I'm humble and open to that.
[02:39:40.200 --> 02:39:42.860]   But I would bet at least 10 to one
[02:39:42.860 --> 02:39:45.140]   that our situation is more typical,
[02:39:45.140 --> 02:39:46.780]   because the whole thing with Moore's law
[02:39:46.780 --> 02:39:48.220]   and accelerating technology,
[02:39:48.220 --> 02:39:50.180]   it's pretty obvious why it's happening.
[02:39:50.180 --> 02:39:52.940]   Everything that grows exponentially,
[02:39:52.940 --> 02:39:54.120]   we call it an explosion,
[02:39:54.120 --> 02:39:56.660]   whether it's a population explosion or a nuclear explosion,
[02:39:56.660 --> 02:39:58.020]   it's always caused by the same thing.
[02:39:58.020 --> 02:40:01.500]   It's that the next step triggers a step after that.
[02:40:01.500 --> 02:40:06.500]   So today's technology enables tomorrow's technology,
[02:40:06.500 --> 02:40:09.100]   and that enables the next level.
[02:40:09.100 --> 02:40:13.820]   And because the technology's always better,
[02:40:13.820 --> 02:40:17.020]   of course, the steps can come faster and faster.
[02:40:17.020 --> 02:40:19.200]   On the other question that I might be wrong about,
[02:40:19.200 --> 02:40:22.380]   that's the much more controversial one, I think.
[02:40:22.380 --> 02:40:24.960]   But before we close out on this thing about,
[02:40:24.960 --> 02:40:28.360]   if the first one, if it's true that most civilizations
[02:40:28.360 --> 02:40:32.060]   spend only a very short amount of their total time
[02:40:32.060 --> 02:40:35.460]   in the stage, say, between inventing
[02:40:37.660 --> 02:40:40.820]   telescopes or mastering electricity
[02:40:40.820 --> 02:40:43.420]   and doing space travel,
[02:40:43.420 --> 02:40:46.220]   if that's actually generally true,
[02:40:46.220 --> 02:40:49.020]   then that should apply also elsewhere out there.
[02:40:49.020 --> 02:40:52.940]   So we should be very, very surprised
[02:40:52.940 --> 02:40:55.540]   if we find some random civilization
[02:40:55.540 --> 02:40:56.980]   and we happen to catch them exactly
[02:40:56.980 --> 02:40:58.820]   in that very, very short stage.
[02:40:58.820 --> 02:41:00.460]   It's much more likely that we find
[02:41:00.460 --> 02:41:03.020]   this planet full of bacteria,
[02:41:03.020 --> 02:41:05.580]   or that we find some civilization
[02:41:05.580 --> 02:41:08.740]   that's already post-biological and has done
[02:41:08.740 --> 02:41:11.860]   some really cool galactic construction projects
[02:41:11.860 --> 02:41:13.340]   in their galaxy.
[02:41:13.340 --> 02:41:15.200]   - Would we be able to recognize them, do you think?
[02:41:15.200 --> 02:41:17.460]   Is it possible that we just can't?
[02:41:17.460 --> 02:41:20.060]   I mean, this post-biological world,
[02:41:20.060 --> 02:41:24.140]   could it be just existing in some other dimension?
[02:41:24.140 --> 02:41:26.300]   Could it be just all a virtual reality game
[02:41:26.300 --> 02:41:28.500]   for them or something, I don't know,
[02:41:28.500 --> 02:41:32.900]   that it changes completely where we won't be able to detect?
[02:41:32.900 --> 02:41:35.100]   - We have to be, honestly, very humble about this.
[02:41:35.100 --> 02:41:37.020]   I think I said earlier,
[02:41:37.020 --> 02:41:39.780]   the number one principle of being a scientist
[02:41:39.780 --> 02:41:42.220]   is you have to be humble and willing to acknowledge
[02:41:42.220 --> 02:41:45.060]   that everything we think, guess, might be totally wrong.
[02:41:45.060 --> 02:41:46.940]   Of course, you can imagine some civilization
[02:41:46.940 --> 02:41:48.660]   where they all decide to become Buddhists
[02:41:48.660 --> 02:41:51.060]   and very inward-looking and just move
[02:41:51.060 --> 02:41:52.380]   into their little virtual reality
[02:41:52.380 --> 02:41:55.100]   and not disturb the flora and fauna around them
[02:41:55.100 --> 02:41:58.100]   and we might not notice them.
[02:41:58.100 --> 02:41:59.960]   But this is a numbers game, right?
[02:41:59.960 --> 02:42:02.260]   If you have millions of civilizations out there
[02:42:02.260 --> 02:42:03.700]   or billions of them,
[02:42:03.700 --> 02:42:08.100]   all it takes is one with a more ambitious mentality
[02:42:08.100 --> 02:42:10.260]   that decides, hey, we are gonna go out
[02:42:10.260 --> 02:42:15.260]   and settle a bunch of other solar systems
[02:42:15.260 --> 02:42:17.580]   and maybe galaxies,
[02:42:17.580 --> 02:42:18.460]   and then it doesn't matter
[02:42:18.460 --> 02:42:19.620]   if they're a bunch of quiet Buddhists.
[02:42:19.620 --> 02:42:23.020]   We're still gonna notice that expansionist one, right?
[02:42:23.020 --> 02:42:26.580]   And it seems like quite the stretch to assume that,
[02:42:26.580 --> 02:42:28.140]   now, we know even in our own galaxy
[02:42:28.140 --> 02:42:33.140]   that there are probably a billion or more planets
[02:42:33.260 --> 02:42:34.540]   that are pretty Earth-like
[02:42:34.540 --> 02:42:37.860]   and many of them were formed over a billion years
[02:42:37.860 --> 02:42:40.820]   before ours, so it had a big head start.
[02:42:40.820 --> 02:42:45.060]   So if you actually assume also that life happens
[02:42:45.060 --> 02:42:47.540]   kind of automatically on an Earth-like planet,
[02:42:47.540 --> 02:42:52.260]   I think it's quite the stretch to then go and say,
[02:42:52.260 --> 02:42:55.460]   okay, so there are another billion civilizations out there
[02:42:55.460 --> 02:42:57.020]   that also have our level of tech
[02:42:57.020 --> 02:42:59.460]   and they all decided to become Buddhists
[02:42:59.460 --> 02:43:03.020]   and not a single one decided to go Hitler on the galaxy
[02:43:03.020 --> 02:43:05.420]   and say, we need to go out and colonize
[02:43:05.420 --> 02:43:08.980]   or not a single one decided for more benevolent reasons
[02:43:08.980 --> 02:43:10.660]   to go out and get more resources.
[02:43:10.660 --> 02:43:13.980]   That seems like a bit of a stretch, frankly.
[02:43:13.980 --> 02:43:16.700]   And this leads into the second thing
[02:43:16.700 --> 02:43:18.700]   you challenged me to be, that I might be wrong about,
[02:43:18.700 --> 02:43:21.180]   how rare or common is life?
[02:43:21.180 --> 02:43:25.300]   So Francis Drake, when he wrote down the Drake equation,
[02:43:25.300 --> 02:43:27.700]   multiplied together a huge number of factors
[02:43:27.700 --> 02:43:29.500]   and said, we don't know any of them,
[02:43:29.500 --> 02:43:31.660]   so we know even less about what you get
[02:43:31.660 --> 02:43:33.940]   when you multiply together the whole product.
[02:43:33.940 --> 02:43:37.420]   Since then, a lot of those factors
[02:43:37.420 --> 02:43:39.100]   have become much better known.
[02:43:39.100 --> 02:43:41.060]   One of his big uncertainties was,
[02:43:41.060 --> 02:43:44.580]   how common is it that a solar system even has a planet?
[02:43:44.580 --> 02:43:46.500]   Well, now we know it's very common.
[02:43:46.500 --> 02:43:48.540]   - Earth-like planets, we know we have better--
[02:43:48.540 --> 02:43:50.620]   - There are a dime a dozen, there are many, many of them,
[02:43:50.620 --> 02:43:52.300]   even in our galaxy.
[02:43:52.300 --> 02:43:55.220]   At the same time, we have, thanks to,
[02:43:55.220 --> 02:43:59.060]   I'm a big supporter of the SETI project and its cousins,
[02:43:59.060 --> 02:44:00.740]   and I think we should keep doing this,
[02:44:00.740 --> 02:44:02.580]   and we've learned a lot.
[02:44:02.580 --> 02:44:04.020]   We've learned that so far,
[02:44:04.020 --> 02:44:08.260]   all we have is still unconvincing hints, nothing more.
[02:44:08.260 --> 02:44:10.540]   And there are certainly many scenarios
[02:44:10.540 --> 02:44:12.580]   where it would be dead obvious.
[02:44:12.580 --> 02:44:18.260]   If there were 100 million other human-like civilizations
[02:44:18.260 --> 02:44:20.380]   in our galaxy, it would not be that hard
[02:44:20.380 --> 02:44:22.860]   to notice some of them with today's technology,
[02:44:22.860 --> 02:44:23.700]   and we haven't.
[02:44:23.700 --> 02:44:27.940]   So what we can say is, well, okay,
[02:44:29.640 --> 02:44:32.320]   we can rule out that there is a human-level civilization
[02:44:32.320 --> 02:44:36.000]   on the moon, and in fact, the many nearby solar systems,
[02:44:36.000 --> 02:44:39.320]   where we cannot rule out, of course,
[02:44:39.320 --> 02:44:43.360]   that there is something like Earth sitting in a galaxy
[02:44:43.360 --> 02:44:44.860]   five billion light years away.
[02:44:44.860 --> 02:44:48.200]   But we've ruled out a lot,
[02:44:48.200 --> 02:44:50.240]   and that's already kind of shocking,
[02:44:50.240 --> 02:44:52.120]   given that there are all these planets there.
[02:44:52.120 --> 02:44:53.320]   So where are they?
[02:44:53.320 --> 02:44:54.160]   Where are they all?
[02:44:54.160 --> 02:44:56.500]   That's the classic Fermi paradox.
[02:44:56.500 --> 02:44:57.340]   - Yeah.
[02:44:58.840 --> 02:45:01.120]   So my argument, which might very well be wrong,
[02:45:01.120 --> 02:45:03.240]   it's very simple, really, it just goes like this.
[02:45:03.240 --> 02:45:04.880]   Okay, we have no clue about this.
[02:45:04.880 --> 02:45:09.640]   It could be the probability of getting life
[02:45:09.640 --> 02:45:13.080]   on a random planet, it could be 10 to the minus one,
[02:45:13.080 --> 02:45:15.400]   a priori, or 10 to the minus 10,
[02:45:15.400 --> 02:45:19.180]   10 to the minus 20, 10 to the minus 30, 10 to the minus 40.
[02:45:19.180 --> 02:45:20.560]   Basically, every order of magnitude
[02:45:20.560 --> 02:45:21.920]   is about equally likely.
[02:45:21.920 --> 02:45:24.680]   When you then do the math,
[02:45:24.680 --> 02:45:27.460]   and ask how close is our nearest neighbor,
[02:45:27.460 --> 02:45:30.560]   it's again equally likely that it's 10 to the 10 meters away,
[02:45:30.560 --> 02:45:33.480]   10 to the 20 meters away, 10 to the 30 meters away.
[02:45:33.480 --> 02:45:35.680]   We have some nerdy ways of talking about this
[02:45:35.680 --> 02:45:38.120]   with Bayesian statistics and a uniform log prior,
[02:45:38.120 --> 02:45:39.400]   but that's irrelevant.
[02:45:39.400 --> 02:45:42.080]   This is the simple basic argument.
[02:45:42.080 --> 02:45:43.760]   And now comes the data, so we can say,
[02:45:43.760 --> 02:45:46.880]   okay, there are all these orders of magnitude.
[02:45:46.880 --> 02:45:49.300]   10 to the 26 meters away,
[02:45:49.300 --> 02:45:52.000]   there's the edge of our observable universe.
[02:45:52.000 --> 02:45:52.880]   If it's farther than that,
[02:45:52.880 --> 02:45:54.900]   light hasn't even reached us yet.
[02:45:54.900 --> 02:45:58.080]   If it's less than 10 to the 16 meters away,
[02:45:58.080 --> 02:46:02.360]   well, it's within Earth's,
[02:46:02.360 --> 02:46:03.860]   it's no farther away than the sun.
[02:46:03.860 --> 02:46:05.460]   We can definitely rule that out.
[02:46:05.460 --> 02:46:08.560]   So I think about it like this.
[02:46:08.560 --> 02:46:10.920]   A priori, before we looked with telescopes,
[02:46:10.920 --> 02:46:14.360]   it could be 10 to the 10 meters, 10 to the 20,
[02:46:14.360 --> 02:46:15.720]   10 to the 30, 10 to the 40, 10 to the 50,
[02:46:15.720 --> 02:46:16.560]   10 to the blah, blah, blah,
[02:46:16.560 --> 02:46:18.080]   equally likely anywhere here.
[02:46:18.080 --> 02:46:22.760]   And now we've ruled out this chunk.
[02:46:22.760 --> 02:46:23.600]   - Yeah.
[02:46:24.060 --> 02:46:25.460]   Most of it is outside.
[02:46:25.460 --> 02:46:28.900]   - And here is the edge of our observable universe already.
[02:46:28.900 --> 02:46:30.580]   So I'm certainly not saying I don't think
[02:46:30.580 --> 02:46:32.460]   there's any life elsewhere in space.
[02:46:32.460 --> 02:46:33.700]   If space is infinite,
[02:46:33.700 --> 02:46:36.740]   then you're basically 100% guaranteed that there is.
[02:46:36.740 --> 02:46:39.220]   But the probability that there is life,
[02:46:39.220 --> 02:46:42.300]   that the nearest neighbor,
[02:46:42.300 --> 02:46:43.800]   it happens to be in this little region
[02:46:43.800 --> 02:46:47.100]   between where we would have seen it already
[02:46:47.100 --> 02:46:48.740]   and where we will never see it,
[02:46:48.740 --> 02:46:51.980]   there's actually significantly less than one, I think.
[02:46:52.240 --> 02:46:54.280]   And I think there's a moral lesson from this,
[02:46:54.280 --> 02:46:55.840]   which is really important,
[02:46:55.840 --> 02:47:00.120]   which is to be good stewards of this planet
[02:47:00.120 --> 02:47:01.440]   and this shot we've had.
[02:47:01.440 --> 02:47:03.760]   It can be very dangerous to say,
[02:47:03.760 --> 02:47:07.680]   oh, it's fine if we nuke our planet or ruin the climate
[02:47:07.680 --> 02:47:10.280]   or mess it up with unaligned AI,
[02:47:10.280 --> 02:47:15.160]   because I know there is this nice Star Trek fleet out there.
[02:47:15.160 --> 02:47:18.040]   They're gonna swoop in and take over where we failed.
[02:47:18.040 --> 02:47:19.840]   Just like it wasn't the big deal
[02:47:19.840 --> 02:47:23.040]   that the Easter Island losers wiped themselves out.
[02:47:23.040 --> 02:47:25.180]   That's a dangerous way of loading yourself
[02:47:25.180 --> 02:47:26.640]   into false sense of security.
[02:47:26.640 --> 02:47:32.020]   If it's actually the case that it might be up to us
[02:47:32.020 --> 02:47:35.000]   and only us, the whole future of intelligent life
[02:47:35.000 --> 02:47:36.360]   in our observable universe,
[02:47:36.360 --> 02:47:40.440]   then I think it's both,
[02:47:40.440 --> 02:47:43.120]   it really puts a lot of responsibility on our shoulders.
[02:47:43.120 --> 02:47:45.240]   - Inspiring, it's a little bit terrifying,
[02:47:45.240 --> 02:47:46.480]   but it's also inspiring.
[02:47:46.480 --> 02:47:48.240]   - But it's empowering, I think, most of all,
[02:47:48.240 --> 02:47:50.240]   because the biggest problem today is,
[02:47:50.240 --> 02:47:51.960]   I see this even when I teach,
[02:47:51.960 --> 02:47:55.440]   so many people feel that it doesn't matter
[02:47:55.440 --> 02:47:58.800]   what they do or we do, we feel disempowered.
[02:47:58.800 --> 02:48:00.160]   Oh, it makes no difference.
[02:48:00.160 --> 02:48:05.080]   This is about as far from that as you can come.
[02:48:05.080 --> 02:48:06.920]   But we realize that what we do
[02:48:06.920 --> 02:48:12.220]   on our little spinning ball here in our lifetime
[02:48:12.220 --> 02:48:13.880]   could make the difference
[02:48:13.880 --> 02:48:17.120]   for the entire future of life in our universe.
[02:48:17.120 --> 02:48:18.720]   How empowering is that?
[02:48:18.720 --> 02:48:20.280]   - Yeah, survival of consciousness.
[02:48:20.280 --> 02:48:25.280]   I mean, the other, a very similar kind of empowering aspect
[02:48:25.280 --> 02:48:27.680]   of the Drake equation is,
[02:48:27.680 --> 02:48:31.120]   say there is a huge number of intelligent civilizations
[02:48:31.120 --> 02:48:32.920]   that spring up everywhere,
[02:48:32.920 --> 02:48:34.800]   but because of the Drake equation,
[02:48:34.800 --> 02:48:38.000]   which is the lifetime of a civilization,
[02:48:38.000 --> 02:48:39.880]   maybe many of them hit a wall.
[02:48:39.880 --> 02:48:43.360]   And just like you said, it's clear that that,
[02:48:43.360 --> 02:48:45.920]   for us, the great filter,
[02:48:45.920 --> 02:48:49.040]   the one possible great filter seems to be coming
[02:48:49.040 --> 02:48:51.240]   in the next 100 years.
[02:48:51.240 --> 02:48:53.720]   So it's also empowering to say,
[02:48:53.720 --> 02:48:58.720]   okay, well, we have a chance to not,
[02:48:58.720 --> 02:49:00.120]   I mean, the way great filters work,
[02:49:00.120 --> 02:49:02.080]   it is just get most of them.
[02:49:02.080 --> 02:49:02.920]   - Exactly.
[02:49:02.920 --> 02:49:06.080]   Nick Bostrom has articulated this really beautifully too.
[02:49:06.080 --> 02:49:09.480]   Every time yet another search for life on Mars
[02:49:09.480 --> 02:49:14.480]   comes back negative or something, I'm like, yes, yes!
[02:49:14.760 --> 02:49:17.880]   Our odds for us surviving this is the best.
[02:49:17.880 --> 02:49:21.000]   You already made the argument in broad brush there, right?
[02:49:21.000 --> 02:49:22.600]   But just to unpack it, right?
[02:49:22.600 --> 02:49:24.560]   The point is, we already know
[02:49:24.560 --> 02:49:29.680]   there is a crap ton of planets out there that are Earth-like
[02:49:29.680 --> 02:49:33.600]   and we also know that most of them do not seem to have
[02:49:33.600 --> 02:49:35.120]   anything like our kind of life on them.
[02:49:35.120 --> 02:49:37.280]   So what went wrong?
[02:49:37.280 --> 02:49:39.520]   There's clearly one step along the evolutionary,
[02:49:39.520 --> 02:49:43.840]   at least one filter roadblock in going from no life
[02:49:43.840 --> 02:49:45.640]   to spacefaring life.
[02:49:45.640 --> 02:49:48.200]   And where is it?
[02:49:48.200 --> 02:49:50.760]   Is it in front of us or is it behind us, right?
[02:49:50.760 --> 02:49:54.080]   If there's no filter behind us
[02:49:54.080 --> 02:49:59.080]   and we keep finding all sorts of little mice on Mars
[02:49:59.080 --> 02:50:01.920]   and whatever, right?
[02:50:01.920 --> 02:50:03.120]   That's actually very depressing
[02:50:03.120 --> 02:50:04.480]   because that makes it much more likely
[02:50:04.480 --> 02:50:06.280]   that the filter is in front of us.
[02:50:06.280 --> 02:50:08.080]   And that what actually is going on
[02:50:08.080 --> 02:50:11.120]   is like the ultimate dark joke
[02:50:11.120 --> 02:50:14.400]   that whenever a civilization invents
[02:50:14.400 --> 02:50:15.640]   sufficiently powerful tech,
[02:50:15.640 --> 02:50:18.160]   it just sets their clock and then after a little while
[02:50:18.160 --> 02:50:21.840]   it goes poof for one reason or other and wipes itself out.
[02:50:21.840 --> 02:50:24.320]   Now wouldn't that be like utterly depressing
[02:50:24.320 --> 02:50:26.120]   if we're actually doomed?
[02:50:26.120 --> 02:50:30.800]   Whereas if it turns out that there is a great filter
[02:50:30.800 --> 02:50:32.880]   early on that for whatever reason
[02:50:32.880 --> 02:50:34.840]   seems to be really hard to get to the stage
[02:50:34.840 --> 02:50:39.120]   of sexually reproducing organisms
[02:50:39.120 --> 02:50:43.280]   or even the first ribosome or whatever, right?
[02:50:43.280 --> 02:50:47.120]   Or maybe you have lots of planets with dinosaurs and cows
[02:50:47.120 --> 02:50:48.840]   but for some reason they tend to get stuck there
[02:50:48.840 --> 02:50:50.800]   and never invent smartphones.
[02:50:50.800 --> 02:50:55.120]   All of those are huge boosts for our own odds
[02:50:55.120 --> 02:50:58.800]   because been there, done that.
[02:50:58.800 --> 02:51:01.680]   It doesn't matter how hard or unlikely it was
[02:51:01.680 --> 02:51:04.760]   that we got past that roadblock because we already did.
[02:51:04.760 --> 02:51:07.520]   And then that makes it likely
[02:51:07.520 --> 02:51:09.680]   that the future is in our own hands.
[02:51:09.680 --> 02:51:11.440]   We're not doomed.
[02:51:11.440 --> 02:51:16.440]   So that's why I think the fact that life is rare
[02:51:16.440 --> 02:51:19.000]   in the universe, it's not just something
[02:51:19.000 --> 02:51:20.560]   that there is some evidence for
[02:51:20.560 --> 02:51:24.560]   but also something we should actually hope for.
[02:51:24.560 --> 02:51:29.880]   - So that's the end, the mortality,
[02:51:29.880 --> 02:51:31.520]   the death of human civilization
[02:51:31.520 --> 02:51:33.120]   that we've been discussing in life,
[02:51:33.120 --> 02:51:36.680]   maybe prospering beyond any kind of great filter.
[02:51:36.680 --> 02:51:39.400]   Do you think about your own death?
[02:51:39.400 --> 02:51:44.400]   Does it make you sad that you may not witness some of the,
[02:51:44.400 --> 02:51:47.880]   you lead a research group on working
[02:51:47.880 --> 02:51:51.080]   some of the biggest questions in the universe actually,
[02:51:51.080 --> 02:51:53.720]   both on the physics and the AI side.
[02:51:53.720 --> 02:51:55.560]   Does it make you sad that you may not be able
[02:51:55.560 --> 02:51:58.800]   to see some of these exciting things come to fruition
[02:51:58.800 --> 02:52:00.640]   that we've been talking about?
[02:52:00.640 --> 02:52:02.640]   - Of course, of course it sucks,
[02:52:02.640 --> 02:52:04.840]   the fact that I'm gonna die.
[02:52:04.840 --> 02:52:07.200]   I remember once when I was much younger,
[02:52:07.200 --> 02:52:10.840]   my dad made this remark that life is fundamentally tragic.
[02:52:10.840 --> 02:52:13.120]   And I'm like, what are you talking about, daddy?
[02:52:13.120 --> 02:52:15.640]   And then many years later, I felt,
[02:52:15.640 --> 02:52:17.360]   now I feel I totally understand what he means.
[02:52:17.360 --> 02:52:19.040]   You know, we grow up, we're little kids
[02:52:19.040 --> 02:52:21.960]   and everything is infinite and it's so cool.
[02:52:21.960 --> 02:52:24.760]   And then suddenly we find out that actually, you know,
[02:52:24.760 --> 02:52:27.480]   you got to start only, this is,
[02:52:27.480 --> 02:52:30.320]   you're gonna get game over at some point.
[02:52:30.360 --> 02:52:35.360]   So of course it's something that's sad.
[02:52:35.360 --> 02:52:37.280]   - Are you afraid?
[02:52:37.280 --> 02:52:46.040]   - No, not in the sense that I think anything terrible
[02:52:46.040 --> 02:52:48.280]   is gonna happen after I die or anything like that.
[02:52:48.280 --> 02:52:51.000]   No, I think it's really gonna be game over,
[02:52:51.000 --> 02:52:56.000]   but it's more that it makes me very acutely aware
[02:52:56.000 --> 02:52:57.960]   of what a wonderful gift this is
[02:52:57.960 --> 02:53:00.240]   that I get to be alive right now.
[02:53:00.240 --> 02:53:04.720]   And it's a steady reminder to just live life to the fullest
[02:53:04.720 --> 02:53:08.080]   and really enjoy it because it is finite, you know.
[02:53:08.080 --> 02:53:12.200]   And I think actually, and we all get the regular reminders
[02:53:12.200 --> 02:53:14.360]   when someone near and dear to us dies,
[02:53:14.360 --> 02:53:19.360]   that one day it's gonna be our turn.
[02:53:19.360 --> 02:53:21.560]   It adds this kind of focus.
[02:53:21.560 --> 02:53:23.760]   I wonder what it would feel like actually
[02:53:23.760 --> 02:53:27.040]   to be an immortal being, if they might even enjoy
[02:53:27.040 --> 02:53:29.480]   some of the wonderful things of life a little bit less,
[02:53:29.480 --> 02:53:33.480]   just because there isn't that--
[02:53:33.480 --> 02:53:34.360]   - Finiteness?
[02:53:34.360 --> 02:53:35.200]   - Yeah.
[02:53:35.200 --> 02:53:38.080]   - Do you think that could be a feature, not a bug,
[02:53:38.080 --> 02:53:42.080]   the fact that we beings are finite?
[02:53:42.080 --> 02:53:44.360]   Maybe there's lessons for engineering
[02:53:44.360 --> 02:53:47.000]   and artificial intelligence systems as well
[02:53:47.000 --> 02:53:48.440]   that are conscious.
[02:53:48.440 --> 02:53:53.440]   I do think it makes, is it possible
[02:53:53.440 --> 02:53:57.000]   that the reason the pistachio ice cream is delicious
[02:53:57.000 --> 02:53:59.200]   is the fact that you're going to die one day?
[02:54:00.000 --> 02:54:03.800]   And you will not have all the pistachio ice cream
[02:54:03.800 --> 02:54:06.240]   that you could eat because of that fact.
[02:54:06.240 --> 02:54:07.640]   - Well, let me say two things.
[02:54:07.640 --> 02:54:09.720]   First of all, it's actually quite profound
[02:54:09.720 --> 02:54:10.560]   what you're saying.
[02:54:10.560 --> 02:54:12.840]   I do think I appreciate the pistachio ice cream a lot more
[02:54:12.840 --> 02:54:16.000]   knowing that I will, there's only a finite number of times
[02:54:16.000 --> 02:54:17.840]   I get to enjoy that.
[02:54:17.840 --> 02:54:19.960]   And I can only remember a finite number of times
[02:54:19.960 --> 02:54:20.800]   in the past.
[02:54:20.800 --> 02:54:25.180]   And moreover, my life is not so long
[02:54:25.180 --> 02:54:26.360]   that it just starts to feel like things
[02:54:26.360 --> 02:54:28.160]   are repeating themselves in general.
[02:54:28.160 --> 02:54:30.560]   It's so new and fresh.
[02:54:30.560 --> 02:54:35.560]   I also think though that death is a little bit overrated
[02:54:35.560 --> 02:54:41.400]   in the sense that it comes from a sort of outdated view
[02:54:41.400 --> 02:54:45.640]   of physics and what life actually is.
[02:54:45.640 --> 02:54:49.120]   Because if you ask, okay, what is it that's going to die
[02:54:49.120 --> 02:54:52.040]   exactly, what am I really?
[02:54:52.040 --> 02:54:56.000]   When I say I feel sad about the idea of myself dying,
[02:54:56.000 --> 02:54:59.200]   am I really sad that this skin cell here is gonna die?
[02:54:59.200 --> 02:55:01.600]   Of course not, 'cause it's gonna die next week anyway
[02:55:01.600 --> 02:55:04.020]   and I'll grow a new one, right?
[02:55:04.020 --> 02:55:08.440]   And it's not any of my cells that I'm associating really
[02:55:08.440 --> 02:55:12.400]   with who I really am, nor is it any of my atoms
[02:55:12.400 --> 02:55:14.040]   or quarks or electrons.
[02:55:14.040 --> 02:55:19.400]   In fact, basically all of my atoms get replaced
[02:55:19.400 --> 02:55:20.560]   on a regular basis, right?
[02:55:20.560 --> 02:55:22.880]   So what is it that's really me
[02:55:22.880 --> 02:55:24.320]   from a more modern physics perspective?
[02:55:24.320 --> 02:55:25.360]   It's the information.
[02:55:25.880 --> 02:55:30.880]   In processing, that's where my memory, that's my memories,
[02:55:30.880 --> 02:55:36.480]   that's my values, my dreams, my passion, my love.
[02:55:36.480 --> 02:55:43.560]   That's what's really fundamentally me.
[02:55:43.560 --> 02:55:48.560]   And frankly, not all of that will die when my body dies.
[02:55:48.560 --> 02:55:51.800]   Like Richard Feynman, for example,
[02:55:51.800 --> 02:55:55.120]   his body died of cancer, you know?
[02:55:55.120 --> 02:55:59.760]   But many of his ideas that he felt made him very him
[02:55:59.760 --> 02:56:01.440]   actually live on.
[02:56:01.440 --> 02:56:03.200]   This is my own little personal tribute
[02:56:03.200 --> 02:56:04.120]   to Richard Feynman, right?
[02:56:04.120 --> 02:56:07.520]   I try to keep a little bit of him alive in myself.
[02:56:07.520 --> 02:56:09.640]   I've even quoted him today, right?
[02:56:09.640 --> 02:56:11.760]   - Yeah, he almost came alive for a brief moment
[02:56:11.760 --> 02:56:13.360]   in this conversation, yeah.
[02:56:13.360 --> 02:56:17.240]   - Yeah, and this honestly gives me some solace.
[02:56:17.240 --> 02:56:19.360]   You know, when I work as a teacher,
[02:56:19.360 --> 02:56:24.360]   I feel if I can actually share a bit about myself,
[02:56:25.360 --> 02:56:30.360]   that my students feel worthy enough to copy and adopt
[02:56:30.360 --> 02:56:32.240]   as a part of things that they know
[02:56:32.240 --> 02:56:35.680]   or they believe or aspire to,
[02:56:35.680 --> 02:56:38.280]   now I live on also a little bit in them, right?
[02:56:38.280 --> 02:56:44.280]   And so being a teacher is a little bit of what I,
[02:56:44.280 --> 02:56:51.280]   that's something also that contributes
[02:56:51.760 --> 02:56:55.720]   to making me a little teeny bit less mortal, right?
[02:56:55.720 --> 02:56:58.680]   Because I'm not, at least not all gonna die all at once,
[02:56:58.680 --> 02:56:59.560]   right?
[02:56:59.560 --> 02:57:01.240]   And I find that a beautiful tribute
[02:57:01.240 --> 02:57:02.920]   to people we didn't respect.
[02:57:02.920 --> 02:57:06.960]   If we can remember them and carry in us
[02:57:06.960 --> 02:57:12.240]   the things that we felt was the most awesome about them,
[02:57:12.240 --> 02:57:13.480]   right, then they live on.
[02:57:13.480 --> 02:57:17.240]   And I'm getting a bit emotionally over it,
[02:57:17.240 --> 02:57:19.800]   but it's a very beautiful idea you bring up there.
[02:57:19.800 --> 02:57:23.160]   I think we should stop this old-fashioned materialism
[02:57:23.160 --> 02:57:28.160]   and just equate who we are with our quarks and electrons.
[02:57:28.160 --> 02:57:31.360]   There's no scientific basis for that really.
[02:57:31.360 --> 02:57:34.620]   And it's also very uninspiring.
[02:57:34.620 --> 02:57:40.520]   Now, if you look a little bit towards the future, right,
[02:57:40.520 --> 02:57:44.180]   one thing which really sucks about humans dying
[02:57:44.180 --> 02:57:46.200]   is that even though some of their teachings
[02:57:46.200 --> 02:57:49.920]   and memories and stories and ethics and so on
[02:57:49.920 --> 02:57:52.960]   will be copied by those around them, hopefully,
[02:57:52.960 --> 02:57:55.600]   a lot of it can't be copied and just dies with them,
[02:57:55.600 --> 02:57:57.160]   with their brain, and that really sucks.
[02:57:57.160 --> 02:58:00.900]   That's the fundamental reason why we find it so tragic
[02:58:00.900 --> 02:58:03.760]   when someone goes from having all this information there
[02:58:03.760 --> 02:58:07.300]   to the more just gone, ruined, right?
[02:58:07.300 --> 02:58:11.680]   With more post-biological intelligence,
[02:58:11.680 --> 02:58:14.120]   that's gonna shift a lot, right?
[02:58:15.000 --> 02:58:18.320]   The only reason it's so hard to make a backup of your brain
[02:58:18.320 --> 02:58:19.600]   in its entirety is exactly
[02:58:19.600 --> 02:58:21.840]   because it wasn't built for that, right?
[02:58:21.840 --> 02:58:25.760]   If you have a future machine intelligence,
[02:58:25.760 --> 02:58:28.400]   there's no reason for why it has to die at all
[02:58:28.400 --> 02:58:32.600]   if you want to copy it, whatever,
[02:58:32.600 --> 02:58:36.680]   into some other quark blob, right?
[02:58:36.680 --> 02:58:39.520]   You can copy not just some of it, but all of it, right?
[02:58:39.520 --> 02:58:42.180]   And so in that sense,
[02:58:42.820 --> 02:58:45.820]   you can get immortality because all the information
[02:58:45.820 --> 02:58:49.180]   can be copied out of any individual entity.
[02:58:49.180 --> 02:58:51.500]   And it's not just mortality that will change
[02:58:51.500 --> 02:58:54.100]   if we get more post-biological life.
[02:58:54.100 --> 02:58:59.100]   It's also with that, very much the whole individualism
[02:58:59.100 --> 02:59:01.220]   we have now, right?
[02:59:01.220 --> 02:59:02.980]   The reason that we make such a big difference
[02:59:02.980 --> 02:59:06.220]   between me and you is exactly because
[02:59:06.220 --> 02:59:08.100]   we're a little bit limited in how much we can copy.
[02:59:08.100 --> 02:59:10.460]   Like I would just love to go back to the beginning
[02:59:10.460 --> 02:59:13.340]   and copy, like I would just love to go like this
[02:59:13.340 --> 02:59:17.820]   and copy your Russian skills, Russian speaking skills.
[02:59:17.820 --> 02:59:18.860]   Wouldn't it be awesome?
[02:59:18.860 --> 02:59:20.500]   But I can't.
[02:59:20.500 --> 02:59:22.020]   I have to actually work for years
[02:59:22.020 --> 02:59:23.940]   if I want to get better on it.
[02:59:23.940 --> 02:59:27.980]   But if we were robots--
[02:59:27.980 --> 02:59:31.860]   - Just copying paste freely, then that loses completely,
[02:59:31.860 --> 02:59:35.180]   it washes away the sense of what immortality is.
[02:59:35.180 --> 02:59:37.500]   - And also individuality a little bit, right?
[02:59:37.500 --> 02:59:39.340]   We would start feeling much more,
[02:59:39.340 --> 02:59:43.540]   maybe we would feel much more collaborative with each other
[02:59:43.540 --> 02:59:45.660]   if we can just, hey, I'll give you my Russian,
[02:59:45.660 --> 02:59:48.740]   you can give me your Russian and I'll give you whatever,
[02:59:48.740 --> 02:59:50.220]   and suddenly you can speak Swedish.
[02:59:50.220 --> 02:59:52.060]   Maybe that's less a bad trade for you,
[02:59:52.060 --> 02:59:54.660]   but whatever else you want from my brain, right?
[02:59:54.660 --> 02:59:58.100]   And there've been a lot of sci-fi stories
[02:59:58.100 --> 03:00:02.180]   about hive minds and so on where experiences
[03:00:02.180 --> 03:00:03.660]   can be more broadly shared.
[03:00:05.540 --> 03:00:10.220]   And I think, I don't pretend to know
[03:00:10.220 --> 03:00:15.220]   what it would feel like to be a super intelligent machine,
[03:00:15.220 --> 03:00:20.460]   but I'm quite confident that however it feels
[03:00:20.460 --> 03:00:22.460]   about mortality and individuality
[03:00:22.460 --> 03:00:25.020]   will be very, very different from how it is for us.
[03:00:25.020 --> 03:00:30.540]   - Well, for us, mortality and finiteness
[03:00:30.540 --> 03:00:34.140]   seems to be pretty important at this particular moment.
[03:00:34.140 --> 03:00:37.460]   And so all good things must come to an end,
[03:00:37.460 --> 03:00:39.180]   just like this conversation, Max.
[03:00:39.180 --> 03:00:40.660]   - I saw that coming.
[03:00:40.660 --> 03:00:44.660]   - Sorry, this is the world's worst translation.
[03:00:44.660 --> 03:00:45.820]   I could talk to you forever.
[03:00:45.820 --> 03:00:49.140]   It's such a huge honor that you spent time with me.
[03:00:49.140 --> 03:00:50.100]   - Honor is mine.
[03:00:50.100 --> 03:00:52.860]   - Thank you so much for getting me,
[03:00:52.860 --> 03:00:54.580]   essentially to start this podcast
[03:00:54.580 --> 03:00:55.960]   by doing the first conversation,
[03:00:55.960 --> 03:00:58.500]   making me realize falling in love
[03:00:58.500 --> 03:01:01.140]   with conversation in itself.
[03:01:01.140 --> 03:01:03.220]   And thank you so much for inspiring
[03:01:03.220 --> 03:01:05.380]   so many people in the world with your books,
[03:01:05.380 --> 03:01:07.740]   with your research, with your talking,
[03:01:07.740 --> 03:01:12.740]   and with the other, like this ripple effect of friends,
[03:01:12.740 --> 03:01:15.460]   including Elon and everybody else that you inspire.
[03:01:15.460 --> 03:01:18.140]   So thank you so much for talking today.
[03:01:18.140 --> 03:01:18.980]   - Thank you.
[03:01:18.980 --> 03:01:23.620]   I feel so fortunate that you're doing this podcast
[03:01:23.620 --> 03:01:27.780]   and getting so many interesting voices out there
[03:01:27.780 --> 03:01:30.940]   into the ether and not just the five-second sound bites,
[03:01:30.940 --> 03:01:33.060]   but so many of the interviews I've watched you do.
[03:01:33.060 --> 03:01:36.140]   You really let people go in into depth
[03:01:36.140 --> 03:01:38.380]   in a way which we sorely need in this day and age.
[03:01:38.380 --> 03:01:41.740]   And that I got to be number one, I feel super honored.
[03:01:41.740 --> 03:01:43.500]   - Yeah, you started it.
[03:01:43.500 --> 03:01:44.660]   Thank you so much, Max.
[03:01:44.660 --> 03:01:48.260]   Thanks for listening to this conversation with Max Tegmark.
[03:01:48.260 --> 03:01:50.220]   And thank you to our sponsors,
[03:01:50.220 --> 03:01:52.540]   the Jordan Harbinger Show,
[03:01:52.540 --> 03:01:54.820]   Four Sigmatic Mushroom Coffee,
[03:01:54.820 --> 03:01:58.940]   BetterHelp Online Therapy, and ExpressVPN.
[03:01:58.940 --> 03:02:03.020]   So the choices, wisdom, caffeine, sanity,
[03:02:03.020 --> 03:02:04.420]   or privacy.
[03:02:04.420 --> 03:02:05.820]   Choose wisely, my friends.
[03:02:05.820 --> 03:02:08.740]   And if you wish, click the sponsor links below
[03:02:08.740 --> 03:02:11.860]   to get a discount and to support this podcast.
[03:02:11.860 --> 03:02:15.100]   And now let me leave you with some words from Max Tegmark.
[03:02:15.100 --> 03:02:18.860]   If consciousness is the way that information feels
[03:02:18.860 --> 03:02:21.380]   when it's processed in certain ways,
[03:02:21.380 --> 03:02:24.220]   then it must be substrate independent.
[03:02:24.220 --> 03:02:26.660]   It's only the structure of information processing
[03:02:26.660 --> 03:02:29.100]   that matters, not the structure of the matter
[03:02:29.100 --> 03:02:31.900]   doing the information processing.
[03:02:31.900 --> 03:02:34.980]   Thank you for listening and hope to see you next time.
[03:02:34.980 --> 03:02:37.580]   (upbeat music)
[03:02:37.580 --> 03:02:40.180]   (upbeat music)
[03:02:40.180 --> 03:02:50.180]   [BLANK_AUDIO]

