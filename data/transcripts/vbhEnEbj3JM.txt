
[00:00:00.000 --> 00:00:04.720]   Hello everybody, my name is Chris and today we're going to talk about transfer learning.
[00:00:04.720 --> 00:00:12.680]   I think transfer learning is really cool because it lets you take a small data set and actually create a really accurate model.
[00:00:12.680 --> 00:00:15.040]   We're going to do this by leveraging
[00:00:15.040 --> 00:00:22.700]   very large networks that were trained for many hours or even days on much larger data sets than ours and
[00:00:22.700 --> 00:00:29.280]   actually transfer that knowledge into our own network made specifically for our classification problem.
[00:00:30.280 --> 00:00:37.160]   So today we're going to be working with the Freiburg grocery data set, which is a small data set about 4,000 images of
[00:00:37.160 --> 00:00:40.160]   various grocery products and
[00:00:40.160 --> 00:00:46.800]   we want to train our classifier to tell us what type of grocery products those are. So let's take a look at the data.
[00:00:46.800 --> 00:00:49.760]   So first we're going to import
[00:00:49.760 --> 00:00:56.780]   our Keras model layers so we can we can build a Keras model and we're also importing
[00:00:57.800 --> 00:00:59.800]   ResNet50 which is a
[00:00:59.800 --> 00:01:05.120]   image classification network actually out of Microsoft Research
[00:01:05.120 --> 00:01:10.820]   that's very large and we're going to be able to transfer knowledge from it into our own network.
[00:01:10.820 --> 00:01:13.320]   So first we need to load our training data.
[00:01:13.320 --> 00:01:19.520]   So here we're splitting our training data into a train and a test
[00:01:19.520 --> 00:01:24.560]   as well as extracting the class names from this utility library called groceries.
[00:01:25.120 --> 00:01:27.780]   And let's take a look at one of those images actually looks like.
[00:01:27.780 --> 00:01:34.480]   There you go, a jar of pickles.
[00:01:34.480 --> 00:01:38.560]   Hopefully we can train our machine to tell us that.
[00:01:38.560 --> 00:01:42.240]   So let's look at what the other classes in the data set look like.
[00:01:42.240 --> 00:01:45.200]   We got beans, cake,
[00:01:45.200 --> 00:01:48.840]   pasta, and my favorite, vinegar.
[00:01:48.840 --> 00:01:54.240]   All right, let's see how the data is actually distributed.
[00:01:54.520 --> 00:01:56.520]   So
[00:01:56.520 --> 00:02:00.400]   as you can see some of the classes don't have nearly as many examples as others.
[00:02:00.400 --> 00:02:04.160]   Hopefully transfer learning can help to compensate for this.
[00:02:04.160 --> 00:02:11.440]   So before we can train our model we need to convert our categories which are going to be
[00:02:11.440 --> 00:02:20.560]   numbers between 0 and 25 into one hot encoded vectors. So we're calling two categorical on our labels.
[00:02:21.680 --> 00:02:27.880]   And now just to see how we can perform on this data set with a very simple perceptron.
[00:02:27.880 --> 00:02:34.880]   Let's go ahead normalize our data and then just create a single layer perceptron model.
[00:02:34.880 --> 00:02:42.880]   We're going to use categorical cross entropy for our loss because this is a multi-class classification problem. Our good old friend the atom optimizer.
[00:02:42.880 --> 00:02:48.640]   And we also want to view accuracy so we can have a better metric to comprehend what's going on.
[00:02:50.440 --> 00:02:55.880]   Lastly we're calling WMB.init so we can visualize our metrics and let's go ahead and train this model.
[00:02:55.880 --> 00:03:03.320]   Okay, so
[00:03:03.320 --> 00:03:05.720]   looks like we
[00:03:05.720 --> 00:03:07.840]   aren't doing so well.
[00:03:07.840 --> 00:03:10.840]   Our validation accuracy is
[00:03:10.840 --> 00:03:17.000]   0.04%. This is
[00:03:18.280 --> 00:03:20.880]   this is very troubling.
[00:03:20.880 --> 00:03:29.840]   Our accuracy on the training data is even lower. I mean I look at this and I feel ill.
[00:03:29.840 --> 00:03:32.680]   There has to be a better way.
[00:03:32.680 --> 00:03:40.120]   So Keras makes it really easy to leverage the research community's progress in computer vision models.
[00:03:40.120 --> 00:03:47.440]   So here we're going to import ResNet-50 and actually download the pre-trained weights from training on ImageNet
[00:03:47.440 --> 00:03:51.280]   which is an image data set with millions of images that takes
[00:03:51.280 --> 00:03:53.960]   many days to train.
[00:03:53.960 --> 00:03:58.720]   So with this one line, we're pulling in cutting-edge computer vision research.
[00:03:58.720 --> 00:04:04.120]   Let's go ahead and take a look at a model summary to see what this network looks like.
[00:04:04.120 --> 00:04:08.440]   Oh man, so many layers!
[00:04:08.440 --> 00:04:14.240]   ResNet-50 is much more complicated than our simple perceptron.
[00:04:14.920 --> 00:04:17.200]   You can see things like batch normalization,
[00:04:17.200 --> 00:04:21.540]   many different convolutions, and then even this funny add layer.
[00:04:21.540 --> 00:04:25.040]   So what ResNet does is it actually
[00:04:25.040 --> 00:04:30.840]   branches off and takes features from earlier in the network and adds them back in in later layers.
[00:04:30.840 --> 00:04:37.360]   And this helps the network train better and allows researchers to make an even deeper network which gives it more
[00:04:37.360 --> 00:04:39.880]   expressibility and accuracy.
[00:04:39.880 --> 00:04:43.640]   I can just keep on scrolling.
[00:04:44.640 --> 00:04:50.320]   So to see what this network can actually do, let's run it on a picture of an elephant because why not?
[00:04:50.320 --> 00:04:53.000]   So here we're loading in our elephant.
[00:04:53.000 --> 00:04:58.800]   We're changing its size to 224 pixels by 224 pixels because the network expects that size.
[00:04:58.800 --> 00:05:02.440]   Then we're expanding the dimensions because we need to
[00:05:02.440 --> 00:05:06.160]   include our batch dimension.
[00:05:06.160 --> 00:05:11.680]   And we call this really important function pre-process input. So when they trained ResNet,
[00:05:12.040 --> 00:05:16.760]   the researchers used a very specific way of pre-processing the images.
[00:05:16.760 --> 00:05:19.880]   And we're going to use their exact same
[00:05:19.880 --> 00:05:26.000]   logic to do that on our own data so that we can have high accuracy results coming out of the model.
[00:05:26.000 --> 00:05:28.560]   Lastly, we just call predict and
[00:05:28.560 --> 00:05:33.000]   we're using this nice helper method decode predictions, which are going to change the
[00:05:33.000 --> 00:05:39.840]   various indices into the last layer and tell us exactly what category that it's predicting.
[00:05:39.840 --> 00:05:46.840]   And look at that! The network output a tusker with 49% accuracy, an Indian elephant with 34% accuracy,
[00:05:46.840 --> 00:05:49.840]   and there's a slight chance we're looking at an African elephant.
[00:05:49.840 --> 00:05:54.840]   Now, I personally probably wouldn't be able to tell you the difference between these three kinds of elephants,
[00:05:54.840 --> 00:05:58.840]   but a network this powerful is actually able to do it with a high degree of accuracy.
[00:05:58.840 --> 00:06:04.840]   But we don't want this network to tell us the categories that it had trained on.
[00:06:04.840 --> 00:06:07.840]   We want it to tell us our categories for our grocery data set.
[00:06:07.840 --> 00:06:11.840]   So let's look at a way that we can actually do that.
[00:06:11.840 --> 00:06:18.840]   First, let's take our grocery data set and pre-process it exactly the same way that the ResNet authors did.
[00:06:18.840 --> 00:06:26.840]   Now we can actually go into the ResNet model and pull out specific layers that we want to use.
[00:06:26.840 --> 00:06:29.840]   In this case, we're going to pull out the second to last layer,
[00:06:29.840 --> 00:06:31.840]   which is called the average pool layer.
[00:06:31.840 --> 00:06:36.840]   And now we can create a new model with the same input to our ResNet model,
[00:06:36.840 --> 00:06:38.840]   but now instead of outputting a thousand categories,
[00:06:38.840 --> 00:06:42.840]   we're going to output this last layer as our final category.
[00:06:42.840 --> 00:06:45.840]   So let's take a look and see what this model actually looks like.
[00:06:45.840 --> 00:06:50.840]   Still a massive model, but now instead of a thousand categories at the bottom,
[00:06:50.840 --> 00:06:54.840]   we have a 2048 link to the average pool layer.
[00:06:54.840 --> 00:06:57.840]   So now we can actually take our pre-processed grocery data set
[00:06:57.840 --> 00:07:03.840]   and run it through this new model that we've created and actually extract the features.
[00:07:03.840 --> 00:07:11.840]   So now we're going to transform our images into 2048 linked vectors of our data set.
[00:07:11.840 --> 00:07:13.840]   So let's go ahead and run this model.
[00:07:13.840 --> 00:07:17.840]   And now we can see that we have a 2048 linked vector.
[00:07:17.840 --> 00:07:25.840]   So now we're going to transform our images into 2048 linked vectors of numbers
[00:07:25.840 --> 00:07:27.840]   that we can use to train a new model on.
[00:07:27.840 --> 00:07:33.840]   And we hope that ResNet has created features that are going to be much easier to learn from
[00:07:33.840 --> 00:07:35.840]   than our original image data.
[00:07:35.840 --> 00:07:40.840]   We're going to do the same for our test data.
[00:07:40.840 --> 00:07:44.840]   And then finally, we can create a new model, which is a simple perceptron again,
[00:07:44.840 --> 00:07:50.840]   with 25 categories for our data set, using the same loss and optimizer as we did earlier.
[00:07:50.840 --> 00:07:58.840]   Let's go ahead and fit it and see if we can get better accuracy than our first try.
[00:07:58.840 --> 00:08:06.840]   Look at that. Right off the bat, we're getting into 80% validation accuracy.
[00:08:06.840 --> 00:08:13.840]   You might also notice that we have a bit of an overfitting problem.
[00:08:13.840 --> 00:08:18.840]   But there are actually additional techniques we can use to ensure that the network generalizes well
[00:08:18.840 --> 00:08:21.840]   across our data set, and we can fix this issue.
[00:08:21.840 --> 00:08:28.840]   So instead of just extracting the features, which is great because it actually makes our model train really fast,
[00:08:28.840 --> 00:08:34.840]   a disadvantage is now if we actually deploy this model, we're going to have to deploy two models side by side
[00:08:34.840 --> 00:08:42.840]   and always put our input imagery through all of ResNet and then separately pass that output into the next model.
[00:08:42.840 --> 00:08:47.840]   Keras makes it really easy for us to make a single model where the output of the ResNet model
[00:08:47.840 --> 00:08:50.840]   can just go directly into our perceptron.
[00:08:50.840 --> 00:08:53.840]   So we do this here by creating a new model.
[00:08:53.840 --> 00:08:58.840]   We add our ResNet layers and our new final dense layer.
[00:08:58.840 --> 00:09:04.840]   Then we turn all of our layers to be trainable equals false in the ResNet network.
[00:09:04.840 --> 00:09:09.840]   So when we're training this network, we don't want any of the layers to train in ResNet.
[00:09:09.840 --> 00:09:14.840]   Instead, we're just going to tune the weights in our final dense layer.
[00:09:14.840 --> 00:09:23.840]   So now you can see there are 23 million parameters in this network, but only 51,000 of them are trainable.
[00:09:23.840 --> 00:09:32.840]   Now, if we run training, you'll see that it actually takes a lot longer to train.
[00:09:32.840 --> 00:09:38.840]   This is because every batch, we're passing that data all the way through the ResNet network
[00:09:38.840 --> 00:09:42.840]   and doing all of those convolutions and different arithmetic.
[00:09:42.840 --> 00:09:48.840]   So it's taking much longer as opposed to using the cached output features that we had used before.
[00:09:48.840 --> 00:09:56.840]   But the advantage of this is now you have a single model that you can use to continue to retrain
[00:09:56.840 --> 00:10:02.840]   as your dataset maybe grows or you change different labels in your dataset,
[00:10:02.840 --> 00:10:06.840]   as well as it's much easier to deploy your models.
[00:10:06.840 --> 00:10:12.840]   But you see we're getting essentially the same accuracy as we were getting by just extracting the features
[00:10:12.840 --> 00:10:15.840]   and then training this last layer.
[00:10:15.840 --> 00:10:21.840]   So there's one more technique we can use with transfer learning that will actually give us even more accuracy.
[00:10:21.840 --> 00:10:24.840]   This is known as fine-tuning.
[00:10:24.840 --> 00:10:29.840]   So instead of just training our layers that we added at the end of the network,
[00:10:29.840 --> 00:10:37.840]   we can actually take a subset of the layers in the ResNet network and allow them to train as well.
[00:10:37.840 --> 00:10:44.840]   So the reason behind this is the way these networks tend to learn is that the layers much higher up
[00:10:44.840 --> 00:10:47.840]   tend to extract much more higher-level features,
[00:10:47.840 --> 00:10:51.840]   things that would be shared common amongst all the classes in your dataset,
[00:10:51.840 --> 00:10:55.840]   whereas the layers lower in the network tend to be much more specific
[00:10:55.840 --> 00:11:06.840]   and are looking at shapes and different edges that are going to be very specific to your classes,
[00:11:06.840 --> 00:11:10.840]   or in this case the classes that ResNet was trained on.
[00:11:10.840 --> 00:11:14.840]   So we can actually take these final layers and fine-tune them,
[00:11:14.840 --> 00:11:21.840]   enable them to change their weights so that they're better suited for our classes,
[00:11:21.840 --> 00:11:26.840]   while still enabling the very generic layers at the top of the network
[00:11:26.840 --> 00:11:31.840]   to pass down the most meaningful information for our new classifier.
[00:11:31.840 --> 00:11:36.840]   So to do this, we actually set the ResNet to be trainable now,
[00:11:36.840 --> 00:11:41.840]   and we go into that network and actually in this case just say
[00:11:41.840 --> 00:11:47.840]   the final 11 layers out of the 100 or so that are in ResNet we're going to allow to train,
[00:11:47.840 --> 00:11:51.840]   whereas the first layers are all not going to be trainable.
[00:11:51.840 --> 00:12:03.840]   So one thing to note when you're fine-tuning is that because the weights have been trained
[00:12:03.840 --> 00:12:10.840]   on a very large dataset and are going to be very specific to the ResNet dataset,
[00:12:10.840 --> 00:12:14.840]   when we start to fine-tune those weights and move them,
[00:12:14.840 --> 00:12:20.840]   we're likely going to want to do that much more slowly than we would in a normal network.
[00:12:20.840 --> 00:12:24.840]   So this is a case where instead of just setting optimizer equal to atom,
[00:12:24.840 --> 00:12:29.840]   you would want to actually instantiate a new instance of the optimizer
[00:12:29.840 --> 00:12:31.840]   and slow down the learning rate.
[00:12:31.840 --> 00:12:38.840]   So we really want to move those weights in the last layers a little by little,
[00:12:38.840 --> 00:12:41.840]   and this can really prevent overfitting on our dataset,
[00:12:41.840 --> 00:12:45.840]   which can be easy given it's so small and we have so many parameters.
[00:13:03.840 --> 00:13:08.840]   And look at that. With only a few lines, we were able to leverage cutting-edge models
[00:13:08.840 --> 00:13:15.840]   to actually get 72% accuracy on our Freiburg grocery dataset.
[00:13:15.840 --> 00:13:19.840]   Remember we started at less than 1% accuracy.
[00:13:19.840 --> 00:13:22.840]   I'd say that's a pretty good day at the office.
[00:13:22.840 --> 00:13:23.840]   Thank you.
[00:13:24.840 --> 00:13:26.840]   [ End ]
[00:13:26.840 --> 00:13:30.840]   [ Silence ]
[00:13:30.840 --> 00:13:40.840]   [BLANK_AUDIO]

