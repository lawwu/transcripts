
[00:00:00.000 --> 00:00:07.500]   Welcome to 2020 and welcome to the Deep Learning Lecture Series.
[00:00:07.500 --> 00:00:14.060]   Let's start it off today to take a quick whirlwind tour of all the exciting things that happened
[00:00:14.060 --> 00:00:23.420]   in '17, '18, and '19 especially, and the amazing things we're going to see this year in 2020.
[00:00:23.420 --> 00:00:28.760]   Also as part of this series, there's going to be a few talks from some of the top people
[00:00:28.760 --> 00:00:34.000]   in learning in artificial intelligence after today, of course.
[00:00:34.000 --> 00:00:39.000]   Start at the broad, the celebrations from the Turing Award to the limitations and the
[00:00:39.000 --> 00:00:44.300]   debates and the exciting growth first.
[00:00:44.300 --> 00:00:47.040]   And first, of course, a step back to the quote I've used before.
[00:00:47.040 --> 00:00:48.040]   I love it.
[00:00:48.040 --> 00:00:49.320]   I'll keep reusing it.
[00:00:49.320 --> 00:00:57.160]   "AI began not with Alan Turing or McCarthy, but with the ancient wish to forge the gods,"
[00:00:57.160 --> 00:01:01.160]   a quote from Pamela McCordick in Machines Who Think.
[00:01:01.160 --> 00:01:08.040]   That visualization there is just 3% of the neurons in our brain of the thalamocortical
[00:01:08.040 --> 00:01:10.640]   system.
[00:01:10.640 --> 00:01:16.040]   That magical thing between our ears that allows us all to see and hear and think and reason
[00:01:16.040 --> 00:01:25.300]   and hope and dream and fear our eventual mortality, all of that is the thing we wish to understand.
[00:01:25.300 --> 00:01:32.840]   That's the dream of artificial intelligence and recreate versions of it, echoes of it
[00:01:32.840 --> 00:01:36.660]   in engineering of our intelligence systems.
[00:01:36.660 --> 00:01:37.660]   That's the dream.
[00:01:37.660 --> 00:01:41.600]   We should never forget in the details I'll talk, the exciting stuff I'll talk about today.
[00:01:41.600 --> 00:01:49.660]   That's sort of the reason why this is exciting, this mystery that's our mind.
[00:01:49.660 --> 00:01:56.080]   The modern human brain, the modern human as we know them today, know and love them today,
[00:01:56.080 --> 00:01:59.460]   it's just about 300,000 years ago.
[00:01:59.460 --> 00:02:02.620]   And the Industrial Revolution is about 300 years ago.
[00:02:02.620 --> 00:02:11.880]   That's 0.1% of the development since the early modern human being is when we've seen a lot
[00:02:11.880 --> 00:02:13.620]   of the machinery.
[00:02:13.620 --> 00:02:19.340]   The machine was born not in stories, but in actuality.
[00:02:19.340 --> 00:02:24.140]   The machine was engineered since the Industrial Revolution and the steam engine and the mechanized
[00:02:24.140 --> 00:02:26.500]   factory system and the machining tools.
[00:02:26.500 --> 00:02:28.940]   That's just 0.1% in the history.
[00:02:28.940 --> 00:02:30.480]   That's the 300 years.
[00:02:30.480 --> 00:02:36.980]   Now we zoom in to the 60, 70 years since the founder, the father arguably of artificial
[00:02:36.980 --> 00:02:40.540]   intelligence, Alan Turing and the dreams.
[00:02:40.540 --> 00:02:44.580]   There's always been the dance in artificial intelligence between the dreams, the mathematical
[00:02:44.580 --> 00:02:51.660]   foundations and when the dreams meet the engineering, the practice, the reality.
[00:02:51.660 --> 00:02:58.220]   So Alan Turing has spoken many times that by the year 2000 that he would be sure that
[00:02:58.220 --> 00:03:01.300]   the Turing test, natural language would be passed.
[00:03:01.300 --> 00:03:05.860]   It seems probably, he said, that once the machine thinking method has started, it would
[00:03:05.860 --> 00:03:09.680]   not take long to outstrip our feeble powers.
[00:03:09.680 --> 00:03:14.080]   They would be able to converse with each other, to sharpen their wits.
[00:03:14.080 --> 00:03:18.240]   Some stage therefore, we should have to expect the machines to take control.
[00:03:18.240 --> 00:03:22.880]   A little shout out to self-play there.
[00:03:22.880 --> 00:03:27.320]   So that's the dream, both the father of the mathematical foundation of artificial intelligence
[00:03:27.320 --> 00:03:30.840]   and the father of dreams in artificial intelligence.
[00:03:30.840 --> 00:03:35.480]   And that dream, again, in the early days was taking reality, the practice met with the
[00:03:35.480 --> 00:03:43.160]   perceptron, often thought of as a single layer neural network, but actually what's not as
[00:03:43.160 --> 00:03:49.080]   much known as Frank Rosenblatt was also the developer of the multi-layer perceptron.
[00:03:49.080 --> 00:03:53.280]   And that history zooming through has amazed our civilization.
[00:03:53.280 --> 00:03:58.720]   To me, one of the most inspiring things in the world of games, first with the great Garry
[00:03:58.720 --> 00:04:08.320]   Kasparov losing to IBM Dblue in 1997, then Lee Sedol losing to AlphaGo in 2016, seminal
[00:04:08.320 --> 00:04:14.920]   moments and captivating the world through the engineering of actual real world systems.
[00:04:14.920 --> 00:04:20.320]   Robots on four wheels, as we'll talk about today, from Waymo to Tesla to all of the autonomous
[00:04:20.320 --> 00:04:26.920]   vehicle companies working in the space, robots on two legs, captivating the world of what
[00:04:26.920 --> 00:04:32.400]   actuation, what kind of manipulation can be achieved.
[00:04:32.400 --> 00:04:40.760]   The history of deep learning from 1943, the initial models from neuroscience, thinking
[00:04:40.760 --> 00:04:45.600]   about neural networks, how to model neural networks mathematically to the creation, as
[00:04:45.600 --> 00:04:50.800]   I said, of the single layer and the multi-layer perceptron by Frank Rosenblatt in '57 and
[00:04:50.800 --> 00:04:56.920]   '62, to the ideas of back propagation and recurring neural nets in the '70s and '80s,
[00:04:56.920 --> 00:05:02.680]   to convolutional neural networks and LSTMs and bidirectional RNNs in the '80s and '90s,
[00:05:02.680 --> 00:05:11.240]   to the birth of the deep learning term and the new wave, the revolution in 2006, to the
[00:05:11.240 --> 00:05:16.600]   ImageNet and AlexNet, the seminal moment that captivated the possibility, the imagination
[00:05:16.600 --> 00:05:23.320]   of the AI community of what neural networks can do in the image and natural language space
[00:05:23.320 --> 00:05:30.480]   closely following years after, to the development, to the popularization of GANs, generative
[00:05:30.480 --> 00:05:35.360]   adversarial networks, with AlphaGo and AlphaZero in 2016 and '7.
[00:05:35.360 --> 00:05:41.440]   And as we'll talk about, language models of transformers in '17, '18, and '19, those
[00:05:41.440 --> 00:05:45.900]   have been the last few years, have been dominated by the ideas of deep learning in the space
[00:05:45.900 --> 00:05:47.960]   of natural language processing.
[00:05:47.960 --> 00:05:50.000]   OK, celebrations.
[00:05:50.000 --> 00:05:53.400]   This year, the Turing Award was given for deep learning.
[00:05:53.400 --> 00:05:55.640]   This is like deep learning has grown up.
[00:05:55.640 --> 00:05:57.840]   We can finally start giving awards.
[00:05:57.840 --> 00:06:04.560]   Jan LeCun, Jeffrey Hinton, Yosha Bengel received the Turing Award for their conceptual engineering
[00:06:04.560 --> 00:06:10.000]   breakthroughs that have made deep neural networks a critical component of computing.
[00:06:10.000 --> 00:06:15.120]   I would also like to add that perhaps the popularization in the face of skepticism,
[00:06:15.120 --> 00:06:19.240]   and for those a little bit older have known the skepticism that neural networks have received
[00:06:19.240 --> 00:06:24.520]   throughout the '90s, in the face of that skepticism, continuing pushing, believing, and working
[00:06:24.520 --> 00:06:30.520]   in this field, and popularizing it through in the face of that skepticism, I think is
[00:06:30.520 --> 00:06:34.960]   part of the reason these three folks have received the award.
[00:06:34.960 --> 00:06:39.400]   But of course, the community that contributed to deep learning is bigger, much bigger than
[00:06:39.400 --> 00:06:48.520]   those three, many of whom might be here today at MIT, broadly in academia and industry.
[00:06:48.520 --> 00:06:54.120]   Looking at the early key figures, Walter Pitts and Warren McCulloch, as I mentioned, for
[00:06:54.120 --> 00:07:00.000]   the computational models of the neural nets, these ideas of thinking that the kind of neural
[00:07:00.000 --> 00:07:05.200]   networks, biological neural networks, can have on our brain can be modeled mathematically.
[00:07:05.200 --> 00:07:11.680]   And then the engineering of those models into actual physical and conceptual mathematical
[00:07:11.680 --> 00:07:18.720]   systems by Frank Rosenblatt, '57, again, single layer, multi-layer in 1962.
[00:07:18.720 --> 00:07:23.840]   You could say Frank Rosenblatt is the father of deep learning, the first person to really,
[00:07:23.840 --> 00:07:28.040]   in '62, mention the idea of multiple hidden layers in neural networks.
[00:07:28.040 --> 00:07:30.840]   As far as I know, somebody was correct me.
[00:07:30.840 --> 00:07:39.360]   In 1965, shout out to the Soviet Union and Ukraine, the person who is considered to be
[00:07:39.360 --> 00:07:44.600]   the father of deep learning, Alexei Evaknenko and V.G.
[00:07:44.600 --> 00:07:51.240]   Lapa, co-author of that work, is the first learning algorithms on multi-layer perceptrons,
[00:07:51.240 --> 00:07:53.720]   multiple hidden layers.
[00:07:53.720 --> 00:07:57.880]   The work on back propagation, automatic differentiation, 1970.
[00:07:57.880 --> 00:08:02.480]   In 1979, convolutional neural networks were first introduced.
[00:08:02.480 --> 00:08:06.960]   And John Hopfield, looking at recurrent neural networks, what are now called Hopfield networks,
[00:08:06.960 --> 00:08:08.920]   a special kind of recurrent neural networks.
[00:08:08.920 --> 00:08:12.600]   Okay, that's the early birth of deep learning.
[00:08:12.600 --> 00:08:17.160]   I want to mention this because it's been a kind of contention space, now that we can
[00:08:17.160 --> 00:08:20.760]   celebrate the incredible accomplishments of deep learning.
[00:08:20.760 --> 00:08:26.440]   Much like in reinforcement learning, in academia, credit assignment is a big problem.
[00:08:26.440 --> 00:08:34.520]   And the embodiment of that, almost a point of meme, is the great Juergen Schmidhuber.
[00:08:34.520 --> 00:08:38.320]   I encourage for people who are interested in the amazing contribution of the different
[00:08:38.320 --> 00:08:42.360]   people in the deep learning field to read his work on deep learning and neural networks.
[00:08:42.360 --> 00:08:48.200]   It's an overview of all the various people who have contributed besides Jan Lekun, Jeffrey
[00:08:48.200 --> 00:08:51.520]   Hinton, and Yoshua Bengio.
[00:08:51.520 --> 00:08:55.040]   It's a big, beautiful community.
[00:08:55.040 --> 00:08:59.440]   But full of great ideas and full of great people.
[00:08:59.440 --> 00:09:04.480]   My hope for this community, given the tension, as some of you might have seen, around this
[00:09:04.480 --> 00:09:13.720]   kind of credit assignment problem, is that we have more, not on this slide, but love.
[00:09:13.720 --> 00:09:16.320]   There can never be enough love in the world.
[00:09:16.320 --> 00:09:21.200]   But general respect, open-mindedness, and collaboration, and credit sharing in the community.
[00:09:21.200 --> 00:09:27.120]   Plus derision, jealousy, and stubbornness, and silos, academic silos, within institutions,
[00:09:27.120 --> 00:09:29.720]   within disciplines.
[00:09:29.720 --> 00:09:38.220]   Also 2019 was the first time it became cool to highlight the limits of deep learning.
[00:09:38.220 --> 00:09:43.320]   This is the interesting moment in time.
[00:09:43.320 --> 00:09:48.760]   Several books, several papers have come out in the past couple of years highlighting that
[00:09:48.760 --> 00:09:53.920]   deep learning is not able to do the kind of, the broad spectrum of tasks that we can think
[00:09:53.920 --> 00:09:56.520]   of the artificial intelligence system being able to do.
[00:09:56.520 --> 00:10:03.160]   Like read common sense reasoning, like building knowledge bases, and so on.
[00:10:03.160 --> 00:10:08.440]   Rodney Brooks said by 2020, the popular press starts having stories that the era of deep
[00:10:08.440 --> 00:10:10.960]   learning is over.
[00:10:10.960 --> 00:10:17.920]   And certainly there has been echoes of that through the press, through the Twitter sphere,
[00:10:17.920 --> 00:10:19.560]   and all that kind of world.
[00:10:19.560 --> 00:10:25.680]   And I'd like to say that a little skepticism, a little criticism is really good always for
[00:10:25.680 --> 00:10:27.200]   the community, but not too much.
[00:10:27.200 --> 00:10:32.200]   It's like a little spice in the soup of progress.
[00:10:32.200 --> 00:10:42.880]   Aside from that kind of skepticism, the growth of CVPR, iClear, NeurIPS, all these conference
[00:10:42.880 --> 00:10:46.320]   submission papers has grown year over year.
[00:10:46.320 --> 00:10:52.320]   There's been a lot of exciting research, some of which I'd like to cover today.
[00:10:52.320 --> 00:10:58.160]   My hope in this space of deep learning growth celebrations and limitations for 2020 is that
[00:10:58.160 --> 00:11:06.120]   there's less, both less hype and less anti-hype.
[00:11:06.120 --> 00:11:11.080]   Less tweets on how there's too much hype in AI and more solid research.
[00:11:11.080 --> 00:11:13.360]   Less criticism and more doing.
[00:11:13.360 --> 00:11:19.720]   But again, a little criticism, a little spice is always good for the recipe.
[00:11:19.720 --> 00:11:26.040]   Hybrid research, less contentious counterproductive debates and more open-minded interdisciplinary
[00:11:26.040 --> 00:11:34.440]   collaboration across neuroscience, cognitive science, computer science, robotics, mathematics,
[00:11:34.440 --> 00:11:38.580]   physics, across all these disciplines working together.
[00:11:38.580 --> 00:11:42.520]   And the research topics that I would love to see more contributions to, as we'll briefly
[00:11:42.520 --> 00:11:47.160]   talk about in some domains, is reasoning, common sense reasoning, integrating that into
[00:11:47.160 --> 00:11:53.840]   the learning architecture, active learning and lifelong learning, multimodal multitask
[00:11:53.840 --> 00:11:59.680]   learning, open domain conversation, so expanding the success of natural language to dialogue,
[00:11:59.680 --> 00:12:04.200]   to open domain dialogue and conversation, and then applications.
[00:12:04.200 --> 00:12:09.800]   The two most exciting, one of which we'll talk about is medical and autonomous vehicles.
[00:12:09.800 --> 00:12:15.080]   Then algorithmic ethics in all of its forms, fairness, privacy, bias.
[00:12:15.080 --> 00:12:16.720]   There's been a lot of exciting research there.
[00:12:16.720 --> 00:12:18.320]   I hope that continues.
[00:12:18.320 --> 00:12:25.240]   Taking responsibility for the flaws in our data and the flaws in our human ethics.
[00:12:25.240 --> 00:12:29.000]   And then robotics in terms of deep learning application robotics.
[00:12:29.000 --> 00:12:33.240]   I'd love to see a lot of development, continued development, deep reinforcement learning application
[00:12:33.240 --> 00:12:36.600]   in robotics and robot manipulation.
[00:12:36.600 --> 00:12:40.480]   By the way, there might be a little bit of time for questions at the end.
[00:12:40.480 --> 00:12:44.680]   If you have a really pressing question, you can ask it along the way too.
[00:12:44.680 --> 00:12:46.000]   Questions so far?
[00:12:46.000 --> 00:12:48.720]   Thank God.
[00:12:48.720 --> 00:12:49.960]   Okay.
[00:12:49.960 --> 00:12:53.360]   So, first, the practical.
[00:12:53.360 --> 00:12:56.720]   The deep learning and deep RL frameworks.
[00:12:56.720 --> 00:13:02.320]   This has really been a year where the frameworks have really matured and converged towards
[00:13:02.320 --> 00:13:06.640]   two popular deep learning frameworks that people have used.
[00:13:06.640 --> 00:13:08.120]   As TensorFlow and PyTorch.
[00:13:08.120 --> 00:13:14.280]   So TensorFlow 2.0 and PyTorch 1.3 is the most recent version.
[00:13:14.280 --> 00:13:18.400]   And they've converged towards each other, taking the best features, removing the weaknesses
[00:13:18.400 --> 00:13:19.920]   from each other.
[00:13:19.920 --> 00:13:25.960]   So that competition has been really fruitful in some sense for the development of the community.
[00:13:25.960 --> 00:13:28.360]   So on the TensorFlow side, eager execution.
[00:13:28.360 --> 00:13:30.520]   So imperative programming.
[00:13:30.520 --> 00:13:33.200]   How you would program in Python has become the default.
[00:13:33.200 --> 00:13:38.800]   Has been first integrated, made easy to use, and become the default.
[00:13:38.800 --> 00:13:44.260]   And on the PyTorch side, TorchScript allowed for now graph representation.
[00:13:44.260 --> 00:13:48.960]   So do what you used to be able to do and what used to be the default mode of operation in
[00:13:48.960 --> 00:13:49.960]   TensorFlow.
[00:13:49.960 --> 00:13:55.360]   Allow you to have this intermediate representation that's in graph form.
[00:13:55.360 --> 00:14:04.720]   On the TensorFlow side, just the deep Keras integration and promotion as the primary citizen,
[00:14:04.720 --> 00:14:10.760]   the default citizen of the API of the way you would track with TensorFlow.
[00:14:10.760 --> 00:14:14.800]   Allowing complete beginners, just anybody outside of machine learning to use TensorFlow
[00:14:14.800 --> 00:14:19.480]   with just a few lines of code to train and do inference with a model.
[00:14:19.480 --> 00:14:20.480]   That's really exciting.
[00:14:20.480 --> 00:14:23.520]   They cleaned up the API, the documentation, and so on.
[00:14:23.520 --> 00:14:29.320]   And of course, maturing the JavaScript in the browser implementation of TensorFlow,
[00:14:29.320 --> 00:14:34.160]   TensorFlow Lite, being able to run TensorFlow on phones, mobile, and serving.
[00:14:34.160 --> 00:14:40.600]   Apparently, this is something industry cares a lot about, of course, is being able to efficiently
[00:14:40.600 --> 00:14:44.520]   use models in the cloud.
[00:14:44.520 --> 00:14:49.640]   And PyTorch catching up with TPU support and experimental versions of PyTorch mobile.
[00:14:49.640 --> 00:14:51.960]   So being able to run a smartphone on their side.
[00:14:51.960 --> 00:14:53.680]   This tense, exciting competition.
[00:14:53.680 --> 00:14:58.920]   Oh, and I almost forgot to mention, we have to say goodbye to our favorite Python 2.
[00:14:58.920 --> 00:15:03.640]   This is the year that support finally, in the January 1st, 2020, support for Python
[00:15:03.640 --> 00:15:08.960]   2 and TensorFlow's and PyTorch's support for Python 2 has ended.
[00:15:08.960 --> 00:15:11.240]   So goodbye, print.
[00:15:11.240 --> 00:15:15.000]   Goodbye, cruel world.
[00:15:15.000 --> 00:15:19.680]   On the reinforcement learning front, we're kind of in the same space as JavaScript libraries
[00:15:19.680 --> 00:15:21.040]   are in.
[00:15:21.040 --> 00:15:23.040]   There's no clear winners coming out.
[00:15:23.040 --> 00:15:29.080]   If you're a beginner in the space, the one I recommend is a fork of open app baselines
[00:15:29.080 --> 00:15:31.800]   is stable baselines.
[00:15:31.800 --> 00:15:33.000]   But there's a lot of exciting ones.
[00:15:33.000 --> 00:15:35.480]   Some of them are really closely built on TensorFlow.
[00:15:35.480 --> 00:15:38.960]   Some are built on PyTorch.
[00:15:38.960 --> 00:15:47.360]   Of course, from Google, from Facebook, from DeepMind, Dopamine, TF agents, TensorFlow.
[00:15:47.360 --> 00:15:51.640]   Most of these I've used if you have specific questions, I can answer them.
[00:15:51.640 --> 00:15:55.760]   So stable baselines is the open a baselines for because I said this implements a lot of
[00:15:55.760 --> 00:16:03.640]   the basic deep RL algorithms, PPO, etc, everything good documentation and just allows very simple,
[00:16:03.640 --> 00:16:08.720]   minimal few lines of code implementation of the basic, the matching of the basic algorithms
[00:16:08.720 --> 00:16:12.000]   of the open AI gym environments.
[00:16:12.000 --> 00:16:13.320]   That's the one I recommend.
[00:16:13.320 --> 00:16:18.840]   Okay, for the framework world, my hope for 2020 is framework agnostic research.
[00:16:18.840 --> 00:16:25.800]   So one of the things that I mentioned is PyTorch has really become almost overtaking TensorFlow
[00:16:25.800 --> 00:16:29.080]   in popularity in the research world.
[00:16:29.080 --> 00:16:33.880]   What I'd love to see is being able to develop an architecture in TensorFlow or developing
[00:16:33.880 --> 00:16:35.560]   in PyTorch, which you currently can.
[00:16:35.560 --> 00:16:41.360]   And then once you train the model to be able to easily transfer to the other from PyTorch
[00:16:41.360 --> 00:16:45.480]   TensorFlow and from TensorFlow to PyTorch, currently it takes three, four, five hours
[00:16:45.480 --> 00:16:47.960]   if you know what you're doing in both languages to do that.
[00:16:47.960 --> 00:16:52.980]   It'd be nice if there was a very easy way to do that transfer.
[00:16:52.980 --> 00:16:57.960]   Then the maturing of the deep RL frameworks, I'd love it to see open AI step up, DeepMind
[00:16:57.960 --> 00:17:03.680]   to step up and really take some of these frameworks to maturity that we can all agree on.
[00:17:03.680 --> 00:17:07.080]   Much like open AI gym for the environment world has done.
[00:17:07.080 --> 00:17:11.660]   And continued work that Keras has started and many other wrappers around TensorFlow
[00:17:11.660 --> 00:17:17.000]   has started of greater and greater abstractions, allowing machine learning to be used by people
[00:17:17.000 --> 00:17:19.540]   outside of the machine learning field.
[00:17:19.540 --> 00:17:26.940]   I think the powerful thing about supervised sort of basic vanilla supervised learning
[00:17:26.940 --> 00:17:37.460]   is that people in biology and chemistry and neuroscience and in physics, in astronomy
[00:17:37.460 --> 00:17:43.420]   can deal with a huge amount of data that they're working with and without needing to learn
[00:17:43.420 --> 00:17:46.340]   any of the details of even Python.
[00:17:46.340 --> 00:17:51.820]   So that I would love to see greater and greater abstractions, which empower scientists outside
[00:17:51.820 --> 00:17:52.820]   the field.
[00:17:52.820 --> 00:17:53.820]   Okay.
[00:17:53.820 --> 00:18:05.820]   Natural language processing, 2017, 2018 was in the transformer was developed and its power
[00:18:05.820 --> 00:18:15.980]   was demonstrated most, especially by Bert, achieving a lot of state of the art results
[00:18:15.980 --> 00:18:23.380]   on a lot of language benchmarks from sentence classification to tagging question answering
[00:18:23.380 --> 00:18:26.820]   and so on.
[00:18:26.820 --> 00:18:30.560]   There's hundreds of data sets and benchmarks that emerged.
[00:18:30.560 --> 00:18:40.380]   Most of which Bert has dominated in 2018, 2019 was sort of the year that the transformer
[00:18:40.380 --> 00:18:44.380]   really exploded in terms of all the different variations.
[00:18:44.380 --> 00:18:53.020]   Again, starting from Bert, Excel net is very cool to use Bert in the name of your new derivative
[00:18:53.020 --> 00:18:55.060]   of a transformer.
[00:18:55.060 --> 00:19:03.580]   Roberta distilled Bert from hugging face Salesforce, open AI, GPT two, of course, Albert and Megatron
[00:19:03.580 --> 00:19:06.220]   from Nvidia, huge transformer.
[00:19:06.220 --> 00:19:07.860]   A few tools have emerged.
[00:19:07.860 --> 00:19:14.340]   So one on hugging face is a company and also a repository that has implemented in both
[00:19:14.340 --> 00:19:19.460]   Pytorch and TensorFlow, a lot of these transformer based natural language models.
[00:19:19.460 --> 00:19:21.040]   So that's really exciting.
[00:19:21.040 --> 00:19:24.140]   So most people here can just use it easily.
[00:19:24.140 --> 00:19:26.260]   So those are already pre-trained models.
[00:19:26.260 --> 00:19:31.540]   And the other exciting stuff is Sebastian Ruder, great researcher in the field of natural
[00:19:31.540 --> 00:19:36.940]   language processing has put together NLP progress, which is all the different benchmarks for
[00:19:36.940 --> 00:19:41.060]   all the different natural language tasks, tracking who sort of leaderboards of who's
[00:19:41.060 --> 00:19:42.060]   winning where.
[00:19:42.060 --> 00:19:43.060]   Okay.
[00:19:43.060 --> 00:19:47.660]   I'll mention a few models that stand out the work from this year.
[00:19:47.660 --> 00:19:54.020]   Megatron LM from Nvidia is basically taking, I believe the GPT two transformer model and
[00:19:54.020 --> 00:19:57.700]   just putting it on steroids, right?
[00:19:57.700 --> 00:20:03.440]   8.3 versus 1.5 billion parameters.
[00:20:03.440 --> 00:20:07.860]   And a lot of interesting stuff there, as you would expect from Nvidia, of course, there's
[00:20:07.860 --> 00:20:14.020]   always brilliant research, but also interesting aspects about how to train in a parallel way,
[00:20:14.020 --> 00:20:17.260]   model and data parallelism in the training.
[00:20:17.260 --> 00:20:23.680]   The first breakthrough results in terms of performance, the model that replaced BERT
[00:20:23.680 --> 00:20:29.940]   as king of transformers is XLNet from CMU and Google research.
[00:20:29.940 --> 00:20:38.300]   They combine the bidirectionality from BERT and the recurrence aspect of transformer XL,
[00:20:38.300 --> 00:20:43.900]   the relative positional embeddings and the recurrence mechanism of transformer XL to
[00:20:43.900 --> 00:20:48.860]   taking the bidirectionality and the recurrence combining it to achieve state of the art performance
[00:20:48.860 --> 00:20:50.620]   on 20 tasks.
[00:20:50.620 --> 00:20:58.660]   Albert is a recent addition from Google research and it reduces significantly the amount of
[00:20:58.660 --> 00:21:03.220]   parameters versus BERT by doing a parameter sharing across the layers.
[00:21:03.220 --> 00:21:11.140]   And it has achieved state of the art results on 12 NLP tasks, including the difficult Stanford
[00:21:11.140 --> 00:21:15.140]   question answering benchmark of squad two.
[00:21:15.140 --> 00:21:20.420]   And they provide open source TensorFlow implementation, including a number of ready to use pre-trained
[00:21:20.420 --> 00:21:21.620]   language models.
[00:21:21.620 --> 00:21:22.620]   Okay.
[00:21:22.620 --> 00:21:27.300]   Another way for people who are completely new to this field, a bunch of apps, right?
[00:21:27.300 --> 00:21:33.300]   With transformer is one of them from Hugging Face popped up that allows you to explore
[00:21:33.300 --> 00:21:35.220]   the capabilities of these language models.
[00:21:35.220 --> 00:21:39.940]   And I think they're quite fascinating from a philosophical point of view.
[00:21:39.940 --> 00:21:44.420]   And this, this has actually been at the core of a lot of the tension of how much do these
[00:21:44.420 --> 00:21:50.420]   transformers actually understand basically memorizing the statistics of the language
[00:21:50.420 --> 00:21:53.940]   in a self-supervised way by reading a lot of texts.
[00:21:53.940 --> 00:21:55.900]   Is that really understanding?
[00:21:55.900 --> 00:21:59.500]   A lot of people say no until it impresses us.
[00:21:59.500 --> 00:22:04.500]   And then everybody will say it's obvious, but right with transformer is a really powerful
[00:22:04.500 --> 00:22:09.900]   way to generate texts, to reveal to you how much these models really learn.
[00:22:09.900 --> 00:22:14.540]   Before this yesterday, actually just came up with a bunch of prompts.
[00:22:14.540 --> 00:22:19.460]   So on the left is a prompt you give it the meaning of life here, for example, is not
[00:22:19.460 --> 00:22:21.440]   what I think it is.
[00:22:21.440 --> 00:22:22.780]   It's what I do to make it.
[00:22:22.780 --> 00:22:26.620]   And you can do a lot of prompts of this nature is very profound.
[00:22:26.620 --> 00:22:29.060]   And some of them will be just absurd.
[00:22:29.060 --> 00:22:34.060]   You'll make sense of it statistically, but it'll be absurd in reveal that the model really
[00:22:34.060 --> 00:22:39.300]   doesn't understand the fundamentals of the prompt is being provided.
[00:22:39.300 --> 00:22:47.380]   But at the same time, it's incredible what kind of texts is able to generate.
[00:22:47.380 --> 00:22:52.620]   The limits of deep learning, I'm just having fun with this at this point, still are still
[00:22:52.620 --> 00:22:54.420]   in the process of being figured out.
[00:22:54.420 --> 00:22:55.900]   Very true.
[00:22:55.900 --> 00:23:02.180]   Had to type this most important person in the history of deep learning is probably Andrew
[00:23:02.180 --> 00:23:03.180]   Ang.
[00:23:03.180 --> 00:23:04.480]   I have to agree.
[00:23:04.480 --> 00:23:07.900]   So this model knows what it's doing.
[00:23:07.900 --> 00:23:11.980]   And I tried to get it to say something nice about me.
[00:23:11.980 --> 00:23:14.100]   And that's a lot of attempts.
[00:23:14.100 --> 00:23:17.700]   So this is kind of funny is finally it did one.
[00:23:17.700 --> 00:23:23.340]   I said, Lex Freeman's best qualities that he's smart.
[00:23:23.340 --> 00:23:28.500]   I said, finally, but it's never nothing but ever happens.
[00:23:28.500 --> 00:23:35.100]   But I think he gets more attention than every Twitter comment ever.
[00:23:35.100 --> 00:23:36.540]   That's very true.
[00:23:36.540 --> 00:23:43.160]   Okay, a nice way to sort of reveal through this, that the models are not able to do any
[00:23:43.160 --> 00:23:50.340]   kind of understanding of language is just to do prompts that show understanding of concepts
[00:23:50.340 --> 00:23:53.740]   and being able to reason with those concepts, common sense reasoning.
[00:23:53.740 --> 00:24:00.620]   Trivia one is doing two plus two is a three, five is a six, seven, the result is a simple
[00:24:00.620 --> 00:24:05.860]   equation four, and two plus three is like you got it right and then it changes mind.
[00:24:05.860 --> 00:24:08.580]   Okay, two minus two is seven.
[00:24:08.580 --> 00:24:09.580]   So on.
[00:24:09.580 --> 00:24:13.980]   You can reveal any kind of reasoning you can do a blocks, you can ask it about gravity,
[00:24:13.980 --> 00:24:14.980]   all those kinds of things.
[00:24:14.980 --> 00:24:21.060]   It shows that it doesn't understand the fundamentals of the concepts that are being reasoned about.
[00:24:21.060 --> 00:24:28.260]   And I'll mention of work that takes it beyond towards that reasoning world in the next few
[00:24:28.260 --> 00:24:29.260]   slides.
[00:24:29.260 --> 00:24:33.820]   But I should also mention with his GPT to model, if you remember about a year ago, there
[00:24:33.820 --> 00:24:39.660]   was a lot of thinking about this 1.5 billion parameter model from open AI.
[00:24:39.660 --> 00:24:46.500]   It is so the thought was it might be so powerful that it would be dangerous.
[00:24:46.500 --> 00:24:51.780]   And so the idea from open AI is when you have an AI system that you're about to release
[00:24:51.780 --> 00:24:59.660]   that might turn out to be dangerous, in this case used probably by Russians, fake news
[00:24:59.660 --> 00:25:04.420]   or misinformation, that that's the kind of thinking is how do we release it.
[00:25:04.420 --> 00:25:10.020]   And I think while it turned out that the GPT to model is not quite so dangerous, that humans
[00:25:10.020 --> 00:25:13.740]   are in fact more dangerous than AI currently.
[00:25:13.740 --> 00:25:16.700]   The that thought experiment is very interesting.
[00:25:16.700 --> 00:25:21.340]   They released a report on release strategies and the social impacts of language models
[00:25:21.340 --> 00:25:25.900]   that almost didn't get as much attention as I think it should.
[00:25:25.900 --> 00:25:31.020]   And it was a little bit disappointing to me how little people are worried about this kind
[00:25:31.020 --> 00:25:34.140]   of situation.
[00:25:34.140 --> 00:25:42.020]   It was more of an eye roll about, oh, these language models aren't as smart as as we thought
[00:25:42.020 --> 00:25:43.220]   they might be.
[00:25:43.220 --> 00:25:49.580]   But the reality is once they are, it's a very interesting thought experiment of how should
[00:25:49.580 --> 00:25:54.680]   the process go of companies and experts communicating with each other during that release.
[00:25:54.680 --> 00:25:58.620]   This report thinks through some of those details.
[00:25:58.620 --> 00:26:04.620]   My takeaway from just reading the report from this whole year of that event is that conversation
[00:26:04.620 --> 00:26:10.160]   on this topic are difficult because we as the public seem to penalize anybody trying
[00:26:10.160 --> 00:26:12.340]   to have that conversation.
[00:26:12.340 --> 00:26:17.700]   And the model of sharing privately, confidentially between ML, machine learning organizations
[00:26:17.700 --> 00:26:19.980]   and experts is not there.
[00:26:19.980 --> 00:26:25.260]   There's no incentive or model or history or culture of sharing.
[00:26:25.260 --> 00:26:26.900]   Okay.
[00:26:26.900 --> 00:26:35.660]   Best paper from ACL, the main conference for languages was on the difficult task of -- we
[00:26:35.660 --> 00:26:37.500]   talked about language models.
[00:26:37.500 --> 00:26:43.260]   Now there's the task taking it a step further of dialogue.
[00:26:43.260 --> 00:26:46.060]   Multidomain task-oriented dialogue.
[00:26:46.060 --> 00:26:49.860]   That's sort of like the next challenge for dialogue systems.
[00:26:49.860 --> 00:26:57.740]   And they've had a few ideas on how to perform dialogue state tracking across domains, achieving
[00:26:57.740 --> 00:27:04.380]   state of the art performance on multi-WAS, which is a five domain challenging, very difficult
[00:27:04.380 --> 00:27:07.380]   five domain human to human dialogue data set.
[00:27:07.380 --> 00:27:08.860]   There's a few ideas there.
[00:27:08.860 --> 00:27:13.700]   Should probably hurry up and start skipping stuff.
[00:27:13.700 --> 00:27:19.380]   And the common sense reasoning, which is really interesting, is this one of the open questions
[00:27:19.380 --> 00:27:24.460]   for the deep learning community, AI community in general, is how can we have hybrid systems
[00:27:24.460 --> 00:27:28.900]   of whether it's symbolic AI deep learning or generally common sense reasoning with learning
[00:27:28.900 --> 00:27:29.900]   systems.
[00:27:29.900 --> 00:27:32.020]   And there's been a few papers in this space.
[00:27:32.020 --> 00:27:40.420]   One of my favorites from Salesforce on building a data set where we can start to do question
[00:27:40.420 --> 00:27:47.460]   answering and figuring out the concepts that are being explored in the question and answering.
[00:27:47.460 --> 00:27:52.220]   Here the question while eating a hamburger with friends, what are people trying to do?
[00:27:52.220 --> 00:27:57.060]   Multiple choice, have fun, tasty, indigestion.
[00:27:57.060 --> 00:28:02.780]   The idea that needs to be generated there, and that's where the language model would
[00:28:02.780 --> 00:28:08.380]   come in, is that usually a hamburger with friends indicates a good time.
[00:28:08.380 --> 00:28:16.300]   So you basically take the question, generate the common sense concept, and from that be
[00:28:16.300 --> 00:28:24.060]   able to determine the multiple choice, what's happening, what's the state of affairs in
[00:28:24.060 --> 00:28:28.420]   this particular question.
[00:28:28.420 --> 00:28:33.860]   Alexa Prize, again, hasn't received nearly enough attention that I think it should have,
[00:28:33.860 --> 00:28:39.300]   perhaps because there hasn't been major breakthroughs, but it's open domain conversations that all
[00:28:39.300 --> 00:28:49.460]   of us, anybody who owns an Alexa can participate in as a provider of data.
[00:28:49.460 --> 00:28:54.140]   But there's been a lot of amazing work from universities across the world on the Alexa
[00:28:54.140 --> 00:28:57.940]   Prize in the last couple of years, and there's been a lot of interesting lessons summarized
[00:28:57.940 --> 00:29:00.460]   in papers and blog posts.
[00:29:00.460 --> 00:29:06.340]   A few lessons from Alquist's team that I particularly like, and this is kind of echoes the work
[00:29:06.340 --> 00:29:13.580]   in IBM Watson with the Jeopardy challenge, is that one of the big ones is that machine
[00:29:13.580 --> 00:29:19.100]   learning is not an essential tool for effective conversation yet.
[00:29:19.100 --> 00:29:24.420]   So machine learning is useful for general chit-chat when you fail at deep, meaningful
[00:29:24.420 --> 00:29:28.460]   conversation or actually understanding what the topic we're talking about, so throwing
[00:29:28.460 --> 00:29:34.620]   in chit-chat, and classification, sort of classifying intent, finding the entities,
[00:29:34.620 --> 00:29:39.220]   predicting the sentiment of the sentences, that's sort of an assistive tool.
[00:29:39.220 --> 00:29:43.140]   But the fundamentals of the conversation are the following.
[00:29:43.140 --> 00:29:45.700]   So first you have to break it apart.
[00:29:45.700 --> 00:29:53.540]   So conversation is a, you can think of it as a long dance, and the way you have fun
[00:29:53.540 --> 00:29:59.020]   dancing is you break it up into a set of moves and turns and so on, and focus on that, sort
[00:29:59.020 --> 00:30:00.860]   of live in the moment kind of thing.
[00:30:00.860 --> 00:30:04.460]   So focus on small parts of the conversation taken at a time.
[00:30:04.460 --> 00:30:09.660]   Then also have a graph, sort of conversation is also all about tangents, so have a graph
[00:30:09.660 --> 00:30:15.740]   of topics and be ready to jump context, from one context to the other and back.
[00:30:15.740 --> 00:30:19.660]   If you look at some of these natural language conversations that they publish, it's just
[00:30:19.660 --> 00:30:22.020]   all over the place in terms of topics.
[00:30:22.020 --> 00:30:26.700]   You jump back and forth, and that's the beauty, the humor, the wit, the fun of conversation
[00:30:26.700 --> 00:30:29.940]   is you jump around from topic to topic.
[00:30:29.940 --> 00:30:34.300]   And opinions, one of the things that natural language systems don't seem to have much is
[00:30:34.300 --> 00:30:35.540]   intelligence.
[00:30:35.540 --> 00:30:42.020]   If I learned anything, one of the simplest ways to convey intelligence is to be very
[00:30:42.020 --> 00:30:48.100]   opinionated about something and confident, and that's a really interesting concept about
[00:30:48.100 --> 00:30:49.100]   conversation.
[00:30:49.100 --> 00:30:50.780]   In general, there's just a lot of lessons.
[00:30:50.780 --> 00:30:55.260]   Oh, and finally, of course, maximize entertainment, not information.
[00:30:55.260 --> 00:30:57.100]   This is true for autonomous vehicles.
[00:30:57.100 --> 00:31:03.980]   This is true for natural language conversation is fun should be part of the objective function.
[00:31:03.980 --> 00:31:05.380]   That's the lessons to learn there.
[00:31:05.380 --> 00:31:09.980]   This is really the Lobner Prize, the Turing Test of our generation.
[00:31:09.980 --> 00:31:13.580]   I'm excited to see if anybody's able to solve the Alexa Prize.
[00:31:13.580 --> 00:31:23.140]   Again, Alexa Prize is your task with talking to a bot, and the measure of quality is the
[00:31:23.140 --> 00:31:27.500]   same as the Lobner Prize is just measuring how good was that conversation, but also the
[00:31:27.500 --> 00:31:31.140]   task is to try to continue the conversation for 20 minutes.
[00:31:31.140 --> 00:31:37.100]   If you try to talk to a bot today, and you have a choice to talk to a bot or go do something
[00:31:37.100 --> 00:31:42.900]   else, watch Netflix, you'll last probably less than 10 seconds.
[00:31:42.900 --> 00:31:45.100]   You'll be bored.
[00:31:45.100 --> 00:31:49.180]   The point is to continue trapping you in the conversation because you're enjoying it so
[00:31:49.180 --> 00:31:50.180]   much.
[00:31:50.180 --> 00:31:56.460]   The 20 minutes is that's a really nice benchmark for passing the spirit of what the Turing
[00:31:56.460 --> 00:31:58.620]   Test stood for.
[00:31:58.620 --> 00:32:03.100]   Alcos here from the Alexa Prize, from the Alcos bot.
[00:32:03.100 --> 00:32:05.700]   The difference in two kinds of conversations.
[00:32:05.700 --> 00:32:07.940]   Alcos says, "Have you been in Brazil?"
[00:32:07.940 --> 00:32:10.540]   The user says, "What is the population of Brazil?"
[00:32:10.540 --> 00:32:13.460]   Alcos says, "It is about 20 million."
[00:32:13.460 --> 00:32:15.980]   User says, "Well, okay."
[00:32:15.980 --> 00:32:21.540]   This is what happens a lot with, like I mentioned, multi-domain conversation is once you jump
[00:32:21.540 --> 00:32:24.020]   to a new domain, you stay there.
[00:32:24.020 --> 00:32:26.140]   Once you switch context, you stay there.
[00:32:26.140 --> 00:32:30.220]   The reality is you want to jump back and continue jumping around.
[00:32:30.220 --> 00:32:34.620]   In the second most successful conversation, "Have you been in Brazil?
[00:32:34.620 --> 00:32:36.420]   What is the population of Brazil?
[00:32:36.420 --> 00:32:38.220]   It is around 20 million.
[00:32:38.220 --> 00:32:42.340]   Anyway, I was saying, have you been in Brazil?"
[00:32:42.340 --> 00:32:43.980]   Jumping back in context.
[00:32:43.980 --> 00:32:46.020]   That's how conversation goes.
[00:32:46.020 --> 00:32:48.060]   Tangent to tangent and back.
[00:32:48.060 --> 00:32:53.940]   Quickly, there's been a lot of sequence to sequence kind of work using natural language
[00:32:53.940 --> 00:32:57.460]   to summarize a lot of applications.
[00:32:57.460 --> 00:33:03.620]   One of them, I cleared that I wanted to highlight from Technion that I find particularly interesting
[00:33:03.620 --> 00:33:10.780]   is the abstract syntax tree-based summarization of code.
[00:33:10.780 --> 00:33:18.820]   Modeling computer code, in this case, sadly, Java and C#, in trees, in syntax trees, and
[00:33:18.820 --> 00:33:24.020]   then operating on those trees to then do the summarization in text.
[00:33:24.020 --> 00:33:31.920]   Here an example of a basic power of two function on the bottom right in Java.
[00:33:31.920 --> 00:33:36.980]   The code to sec summarization says, "Get power of two."
[00:33:36.980 --> 00:33:41.700]   That's an exciting possibility of automated documentation of source code.
[00:33:41.700 --> 00:33:43.100]   I thought it was particularly interesting.
[00:33:43.100 --> 00:33:44.620]   The future there is bright.
[00:33:44.620 --> 00:33:49.300]   Okay, hopes for 2020 for natural language processing is reasoning.
[00:33:49.300 --> 00:33:53.940]   Common-sense reasoning becomes greater and greater part of the transformer-type language
[00:33:53.940 --> 00:33:57.660]   model work that we've seen in the deep learning world.
[00:33:57.660 --> 00:34:03.940]   Extending the context from hundreds or thousands of words to tens of thousands of words.
[00:34:03.940 --> 00:34:11.860]   Being able to read entire stories and maintain the context, which transformers, again, with
[00:34:11.860 --> 00:34:16.260]   ExcelNet, Transformer Excel is starting to be able to do, but we're still far away from
[00:34:16.260 --> 00:34:20.460]   that long-term, lifelong maintenance of context.
[00:34:20.460 --> 00:34:25.900]   Dialogue open domain dialogue, forever since Alan Turing to today is the dream of artificial
[00:34:25.900 --> 00:34:29.660]   intelligence being able to pass the Turing test.
[00:34:29.660 --> 00:34:39.380]   The dream of natural language model transformers are self-supervised learning.
[00:34:39.380 --> 00:34:47.820]   The dream of Yann LeCun is to, for these kinds of what previously were called unsupervised,
[00:34:47.820 --> 00:34:53.380]   but he's calling now self-supervised learning systems, to be able to sort of watch YouTube
[00:34:53.380 --> 00:34:59.660]   videos and from that start to form representation based on which you can understand the world.
[00:34:59.660 --> 00:35:05.700]   The hope for 2020 and beyond is to be able to transfer some of the success of transformers
[00:35:05.700 --> 00:35:13.900]   to the world of visual information, the world of video, for example.
[00:35:13.900 --> 00:35:15.780]   Deep RL and self-play.
[00:35:15.780 --> 00:35:22.780]   This has been an exciting year, continues to be an exciting time for reinforcement learning
[00:35:22.780 --> 00:35:24.980]   in games and robotics.
[00:35:24.980 --> 00:35:33.300]   So first, Dota 2 and OpenAI, an exceptionally popular competitive game, e-sports game that
[00:35:33.300 --> 00:35:37.020]   people compete, win millions of dollars with.
[00:35:37.020 --> 00:35:39.860]   So this is a lot of world-class professional players.
[00:35:39.860 --> 00:35:46.940]   So in 2018, OpenAI 5, this is a team play, tried their best at the international and
[00:35:46.940 --> 00:35:52.060]   lost and said that we're looking forward to pushing 5 to the next level, which they did
[00:35:52.060 --> 00:35:54.060]   in April 2018.
[00:35:54.060 --> 00:36:01.260]   They beat the 2018 world champions in 5 on 5 play.
[00:36:01.260 --> 00:36:09.940]   So the key there was compute, 8 times more training compute because the actual compute
[00:36:09.940 --> 00:36:11.540]   was already maxed out.
[00:36:11.540 --> 00:36:15.420]   The way they achieved the 8x is in time, simply training for longer.
[00:36:15.420 --> 00:36:20.980]   So the current version of OpenAI 5, as Jacob will talk about next Friday, has consumed
[00:36:20.980 --> 00:36:27.300]   800 petaflop a second days and experienced about 45,000 years of Dota self-play over
[00:36:27.300 --> 00:36:28.940]   10 real-time months.
[00:36:28.940 --> 00:36:33.500]   Again, behind a lot of the game systems talk about the, they use self-play so they play
[00:36:33.500 --> 00:36:34.500]   against each other.
[00:36:34.500 --> 00:36:39.540]   This is one of the most exciting concepts in deep learning, systems that learn by playing
[00:36:39.540 --> 00:36:43.560]   each other and incrementally improving in time.
[00:36:43.560 --> 00:36:47.580]   So starting from being terrible and getting better and better and better and better, and
[00:36:47.580 --> 00:36:52.780]   always being challenged by a slightly better opponent because of the natural process of
[00:36:52.780 --> 00:36:53.780]   self-play.
[00:36:53.780 --> 00:36:56.060]   That's a fascinating process.
[00:36:56.060 --> 00:37:05.220]   The 2019 version, the last version of OpenAI 5 has a 99.9 win rate versus the 2018 version.
[00:37:05.220 --> 00:37:11.780]   Then DeepMind also in parallel has been working and using self-play to solve some of these
[00:37:11.780 --> 00:37:19.340]   multi-agent games, which is a really difficult space when people have to collaborate as part
[00:37:19.340 --> 00:37:20.340]   of the competition.
[00:37:20.340 --> 00:37:23.700]   It's exceptionally difficult from the reinforcement learning perspective.
[00:37:23.700 --> 00:37:30.180]   So this is from raw pixels, solve the arena, capture the flag game, Quake three arena.
[00:37:30.180 --> 00:37:35.660]   One of the things I love just as a sort of side note about both OpenAI and DeepMind and
[00:37:35.660 --> 00:37:39.580]   general research and reinforcement learning, there will always be one or two paragraphs
[00:37:39.580 --> 00:37:41.520]   of philosophy.
[00:37:41.520 --> 00:37:46.900]   In this case from DeepMind, billions of people inhabit the planet, each with their own individual
[00:37:46.900 --> 00:37:52.020]   goals and actions, but still capable of coming together through teams, organizations, and
[00:37:52.020 --> 00:37:55.180]   societies in impressive displays of collective intelligence.
[00:37:55.180 --> 00:37:58.380]   This is a setting we call multi-agent learning.
[00:37:58.380 --> 00:38:03.300]   Many individual agents must act independently yet learn to interact and cooperate with other
[00:38:03.300 --> 00:38:04.300]   agent.
[00:38:04.300 --> 00:38:08.220]   This is immensely difficult problem because with co-adapting agent, the world is constantly
[00:38:08.220 --> 00:38:09.220]   changing.
[00:38:09.220 --> 00:38:16.780]   The fact that we 7 billion people on earth, people in this room, in families, in villages
[00:38:16.780 --> 00:38:22.420]   can collaborate while being for the most part self-interested agents is fascinating.
[00:38:22.420 --> 00:38:27.060]   One of my hopes actually for 2020 is to explore social behaviors that emerge in reinforcement
[00:38:27.060 --> 00:38:34.540]   learning agents and how those are echoed in real human to humans social systems.
[00:38:34.540 --> 00:38:36.700]   Okay, here's some visualizations.
[00:38:36.700 --> 00:38:40.980]   The agents automatically figure out as you see in other games, they figure out the concepts.
[00:38:40.980 --> 00:38:44.780]   So knowing very little, knowing nothing about the rules of the game, about the concepts
[00:38:44.780 --> 00:38:48.260]   of the game, about the strategy and the behaviors, they're able to figure it out.
[00:38:48.260 --> 00:38:53.220]   There's the T-SNE visualizations of the different states, important states and concepts in the
[00:38:53.220 --> 00:38:56.060]   game that this figures out and so on.
[00:38:56.060 --> 00:38:59.540]   Skipping ahead, automatic discovery of different behaviors.
[00:38:59.540 --> 00:39:05.420]   This happens in all the different games we talk about from Dota to Starcraft to Quake,
[00:39:05.420 --> 00:39:11.140]   the different strategies that it doesn't know about, it figures out automatically.
[00:39:11.140 --> 00:39:17.660]   And the really exciting work in terms of the multi-agent RL on the DeepMind side was the
[00:39:17.660 --> 00:39:23.820]   beating world-class players and achieving grand master level in a game I do know about,
[00:39:23.820 --> 00:39:25.220]   which is Starcraft.
[00:39:25.220 --> 00:39:30.060]   In December 2018, AlphaStar beat Mana, one of the world's strongest professional Starcraft
[00:39:30.060 --> 00:39:34.940]   players, but that was in a very constrained environment and it was a single race, I think
[00:39:34.940 --> 00:39:39.140]   Protoss.
[00:39:39.140 --> 00:39:44.020]   And in October 2019, AlphaStar reached grand master level by doing what we humans do.
[00:39:44.020 --> 00:39:50.780]   So using a camera, observing the game and playing as part of, against other humans.
[00:39:50.780 --> 00:39:53.420]   So this is not an artificial side system.
[00:39:53.420 --> 00:39:57.420]   This is doing exact same process humans would undertake and achieve grand master, which
[00:39:57.420 --> 00:39:58.820]   is the highest level.
[00:39:58.820 --> 00:39:59.820]   Okay, great.
[00:39:59.820 --> 00:40:05.940]   I encourage you to observe a lot of the interesting on their blog posts and videos of the different
[00:40:05.940 --> 00:40:10.780]   strategies that the RL agents are able to figure out.
[00:40:10.780 --> 00:40:15.100]   Here's a quote from one of the professional Starcraft players, and we see this with AlphaZero2
[00:40:15.100 --> 00:40:21.380]   in chess, is "AlphaStar is an intriguing unorthodox player, one with the reflexes and speed of
[00:40:21.380 --> 00:40:25.940]   the best pros, but strategies and style, they're entirely its own.
[00:40:25.940 --> 00:40:30.880]   The way AlphaStar was trained with agents competing against each other in a league has
[00:40:30.880 --> 00:40:34.260]   resulted in gameplay that's unimaginably unusual.
[00:40:34.260 --> 00:40:39.460]   It really makes you question how much of Starcraft's diverse possibilities pro players have really
[00:40:39.460 --> 00:40:40.460]   explored."
[00:40:40.460 --> 00:40:44.060]   And that's the really exciting thing about reinforcement learning agent in chess, in
[00:40:44.060 --> 00:40:51.000]   Go, in games, and hopefully simulated systems in the future that teach us, teach experts
[00:40:51.000 --> 00:40:57.020]   that think they understand the dynamics of a particular game, a particular simulation
[00:40:57.020 --> 00:41:02.400]   of new strategies, of new behaviors to study.
[00:41:02.400 --> 00:41:06.720]   That's one of the exciting applications from almost a psychology perspective that I'd love
[00:41:06.720 --> 00:41:10.220]   to see reinforcement learning push towards.
[00:41:10.220 --> 00:41:21.780]   And on the imperfect information game side, poker, in 2018, CMU, Noah Brown was able to
[00:41:21.780 --> 00:41:28.420]   beat head-to-head, no limit Texas Hold'em, and now team six player no limit Texas Hold'em
[00:41:28.420 --> 00:41:31.160]   against professional players.
[00:41:31.160 --> 00:41:38.460]   Many of the same results, many of the same approaches was self-play, iterated Monte Carlo,
[00:41:38.460 --> 00:41:42.740]   and there's a bunch of ideas in terms of the abstractions.
[00:41:42.740 --> 00:41:47.180]   So there's so many possibilities under the imperfect information that you have to form
[00:41:47.180 --> 00:41:52.580]   these bins of abstractions in both the action space in order to reduce the action space
[00:41:52.580 --> 00:41:54.740]   and the information abstraction space.
[00:41:54.740 --> 00:41:59.220]   So the probabilities of all the different hands that could possibly have and all the
[00:41:59.220 --> 00:42:02.220]   different hands that the betting strategies could possibly represent.
[00:42:02.220 --> 00:42:05.660]   And so you have to do this kind of course planning.
[00:42:05.660 --> 00:42:12.300]   So they use self-play to generate a course blueprint strategy that in real time, they
[00:42:12.300 --> 00:42:16.300]   then use Monte Carlo search to adjust as they play.
[00:42:16.300 --> 00:42:22.180]   Again, unlike the deep mind open eye approaches, very few, very minimal compute required, and
[00:42:22.180 --> 00:42:26.180]   they're able to achieve to beat world-class players.
[00:42:26.180 --> 00:42:33.380]   Again, I like this is getting quotes from the professional players after they get beaten.
[00:42:33.380 --> 00:42:38.820]   So Chris Ferguson, famous World Series of Poker player said, "Pleribus," that's the
[00:42:38.820 --> 00:42:42.400]   name of the agent, "is a very hard opponent to play against.
[00:42:42.400 --> 00:42:45.840]   It's really hard to pin him down on any kind of hand.
[00:42:45.840 --> 00:42:50.680]   He's also very good at making thin value bets on the river.
[00:42:50.680 --> 00:42:56.200]   He's very good at extracting value out of his good hands, sort of making bets without
[00:42:56.200 --> 00:42:58.800]   scaring off the opponent."
[00:42:58.800 --> 00:43:04.080]   Darin Elias said, "Its major strength is its ability to use mixed strategies.
[00:43:04.080 --> 00:43:06.620]   That's the same thing that humans try to do.
[00:43:06.620 --> 00:43:10.800]   It's a matter of execution for humans to do this in a perfectly random way and to do so
[00:43:10.800 --> 00:43:14.680]   consistently most people just can't."
[00:43:14.680 --> 00:43:18.240]   Then in the robotic space has been a lot of application reinforcement learning.
[00:43:18.240 --> 00:43:24.200]   One of the most exciting is the manipulation, sufficient manipulation to be able to solve
[00:43:24.200 --> 00:43:26.440]   the Rubik's Cube.
[00:43:26.440 --> 00:43:30.600]   Again this is learned through reinforcement learning.
[00:43:30.600 --> 00:43:35.440]   Again because self-plays in this context is not possible, they use automatic domain randomization,
[00:43:35.440 --> 00:43:36.440]   ADR.
[00:43:36.440 --> 00:43:39.760]   So they generate progressively more difficult environments for the hand.
[00:43:39.760 --> 00:43:42.080]   There's a giraffe head there you see.
[00:43:42.080 --> 00:43:45.480]   There's a lot of perturbations to the system, so they mess with it a lot.
[00:43:45.480 --> 00:43:50.520]   And then a lot of noise injected into the system to be able to teach the hand to manipulate
[00:43:50.520 --> 00:43:53.120]   the cube in order to then solve.
[00:43:53.120 --> 00:43:59.360]   The actual solution of figuring out how to go from this particular face to the solved
[00:43:59.360 --> 00:44:03.680]   cube is an obvious problem.
[00:44:03.680 --> 00:44:10.480]   This paper and this work is focused on the much more difficult learning to manipulate
[00:44:10.480 --> 00:44:11.480]   the cube.
[00:44:11.480 --> 00:44:13.240]   It's really exciting.
[00:44:13.240 --> 00:44:18.400]   Again a little philosophy as you would expect from OpenAI is they have this idea of emergent
[00:44:18.400 --> 00:44:20.160]   meta-learning.
[00:44:20.160 --> 00:44:25.520]   This idea that the capacity of the neural network that's learning this manipulation
[00:44:25.520 --> 00:44:31.360]   is constrained while the ADR, the automatic domain randomization, is progressively making
[00:44:31.360 --> 00:44:32.960]   harder and harder environment.
[00:44:32.960 --> 00:44:36.920]   So the capacity of the environment to be difficult is unconstrained.
[00:44:36.920 --> 00:44:44.880]   And because of that, there's an emergent self-optimization of the neural network to learn general concepts
[00:44:44.880 --> 00:44:50.640]   as opposed to memorize particular manipulations.
[00:44:50.640 --> 00:45:00.080]   The hope for me in the deep reinforcement learning space for 2020 is the continued application
[00:45:00.080 --> 00:45:08.640]   of robotics, even sort of legged robotics, but also robotic manipulation.
[00:45:08.640 --> 00:45:12.820]   Human behavior, the use of multi-agent self-plays I've mentioned to explore naturally emerging
[00:45:12.820 --> 00:45:18.720]   social behaviors, constructing simulations of social behavior, and seeing what kind of
[00:45:18.720 --> 00:45:23.040]   multi-human behavior emerges in self-play context.
[00:45:23.040 --> 00:45:29.480]   I think that's one of the nice, there are always, I hope there'll be like a reinforcement
[00:45:29.480 --> 00:45:34.080]   learning self-play psychology department one day, like where you use reinforcement learning
[00:45:34.080 --> 00:45:40.480]   to study, to reverse engineer human behavior and study it through that way.
[00:45:40.480 --> 00:45:44.900]   And again, in games, I'm not sure what the big challenges that remain, but I would love
[00:45:44.900 --> 00:45:53.160]   to see, to me at least, it's exciting to see learned solution to games, to self-play.
[00:45:53.160 --> 00:45:57.560]   Science and deep learning, I would say there's been a lot of really exciting developments
[00:45:57.560 --> 00:45:59.120]   here that deserve their own lecture.
[00:45:59.120 --> 00:46:01.800]   I'll mention just a few.
[00:46:01.800 --> 00:46:08.560]   Here from MIT in early 2018, but it sparked a lot of interest in 2019, follow on work,
[00:46:08.560 --> 00:46:11.480]   is the idea of the lottery ticket hypothesis.
[00:46:11.480 --> 00:46:19.960]   So this work showed that sub-networks, small sub-networks within the larger network are
[00:46:19.960 --> 00:46:22.520]   the ones that are doing all the thinking.
[00:46:22.520 --> 00:46:28.200]   The same results in accuracy can be achieved from a small sub-network from within a neural
[00:46:28.200 --> 00:46:29.200]   network.
[00:46:29.200 --> 00:46:34.800]   And they have a very simple process of arriving at a sub-network of randomly initializing
[00:46:34.800 --> 00:46:37.160]   a neural network.
[00:46:37.160 --> 00:46:41.240]   That's I guess the lottery ticket, train the network until it converges.
[00:46:41.240 --> 00:46:46.520]   This is an iterative process, prune the fraction of the network with low weights, reset the
[00:46:46.520 --> 00:46:52.800]   waste of the remaining network with the original initialization, the same lottery ticket, and
[00:46:52.800 --> 00:47:02.640]   then train again the pruned untrained network and continue this iteratively, continuously
[00:47:02.640 --> 00:47:08.880]   to arrive at a network that's much smaller using the same original initializations.
[00:47:08.880 --> 00:47:13.520]   This is fascinating that within these big networks, there's often a much smaller network
[00:47:13.520 --> 00:47:16.600]   that can achieve the same kind of accuracy.
[00:47:16.600 --> 00:47:22.200]   Now practically speaking, it's unclear what are the big takeaways there except the inspiring
[00:47:22.200 --> 00:47:25.640]   takeaway that there exist architectures that are much more efficient.
[00:47:25.640 --> 00:47:30.440]   So there's value in investing time in finding such networks.
[00:47:30.440 --> 00:47:36.680]   Then there is disentangled representations, which again deserves its own lecture, but
[00:47:36.680 --> 00:47:41.720]   here showing a 10 vector representation.
[00:47:41.720 --> 00:47:46.200]   And the goal is where each part of the vector can learn one particular concept about a data
[00:47:46.200 --> 00:47:47.200]   set.
[00:47:47.200 --> 00:47:52.120]   So the dream of unsupervised learning is you can learn compressed representations where
[00:47:52.120 --> 00:47:56.920]   every one thing is disentangled and you can learn some fundamental concept about the underlying
[00:47:56.920 --> 00:48:01.800]   data that can carry from data set to data set to data set.
[00:48:01.800 --> 00:48:03.520]   That's disentangled representation.
[00:48:03.520 --> 00:48:12.120]   There's theoretical work, best ICML paper in 2019 showing that that's impossible.
[00:48:12.120 --> 00:48:17.320]   So disentangled representations are impossible without inductive biases.
[00:48:17.320 --> 00:48:24.960]   And so the suggestion there is that the biases that you use should be made explicit as much
[00:48:24.960 --> 00:48:26.400]   as possible.
[00:48:26.400 --> 00:48:31.200]   The open problem is finding good inductive biases for unsupervised model selection that
[00:48:31.200 --> 00:48:34.640]   work across multiple data sets that we're actually interested in.
[00:48:34.640 --> 00:48:41.040]   A lot more papers, but one of the exciting is the double descent idea that's been extended
[00:48:41.040 --> 00:48:48.920]   and to the deep neural network context by OpenAI to explore the phenomena that as we
[00:48:48.920 --> 00:48:53.320]   increase the number of parameters in a neural network, the test error initially decreases,
[00:48:53.320 --> 00:48:58.160]   increases and just as the model is able to fit the training set undergoes a second descent.
[00:48:58.160 --> 00:49:00.840]   So decrease, increase, decrease.
[00:49:00.840 --> 00:49:09.560]   So there's this critical moment of time when the training set is just fit perfectly.
[00:49:09.560 --> 00:49:14.400]   Okay, and this is the OpenAI shows that it's applicable not just to model size, but also
[00:49:14.400 --> 00:49:16.320]   to training time and data set time.
[00:49:16.320 --> 00:49:22.160]   This is more like an open problem of why this is, trying to understand this and how to leverage
[00:49:22.160 --> 00:49:26.600]   it in optimizing training dynamics in neural networks.
[00:49:26.600 --> 00:49:30.480]   That's a, there's a lot of really interesting theoretical questions there.
[00:49:30.480 --> 00:49:35.520]   So my hope there for the science of deep learning in 2020 is to continue exploring the fundamentals
[00:49:35.520 --> 00:49:41.000]   of model selection, training dynamics, and the folks focused on the performance of the
[00:49:41.000 --> 00:49:45.640]   training in terms of memory and speed has worked on and the representation characteristics
[00:49:45.640 --> 00:49:48.000]   with respect to architecture characteristics.
[00:49:48.000 --> 00:49:51.960]   So a lot of the fundamental work there and understanding neural networks.
[00:49:51.960 --> 00:49:57.880]   Two areas that I had hold two sections on and papers, which is super exciting.
[00:49:57.880 --> 00:50:00.840]   My first love is graphs.
[00:50:00.840 --> 00:50:07.600]   So graph neural networks is a really exciting area of deep learning graph convolution neural
[00:50:07.600 --> 00:50:11.920]   networks as well for solving combinatorial problems and recommendation systems that are
[00:50:11.920 --> 00:50:17.120]   really useful in any kind of problem that is fundamentally can be modeled as a graph
[00:50:17.120 --> 00:50:21.920]   can be then solved or at least aided in by neural networks.
[00:50:21.920 --> 00:50:28.760]   There's a lot of exciting area there and Bayesian deep learning using Bayesian neural networks
[00:50:28.760 --> 00:50:31.840]   that's been for several years and exciting possibility.
[00:50:31.840 --> 00:50:37.360]   It's very difficult to train large Bayesian networks, but in the context that you can
[00:50:37.360 --> 00:50:43.000]   and it's useful small data sets providing uncertainty measurements in the predictions
[00:50:43.000 --> 00:50:49.760]   is extremely powerful capability of Bayesian nets, Bayesian neural networks and online
[00:50:49.760 --> 00:50:50.800]   incremental learning.
[00:50:50.800 --> 00:50:52.160]   These neural networks release it.
[00:50:52.160 --> 00:50:54.000]   There's a lot of really good papers there.
[00:50:54.000 --> 00:50:55.000]   It's exciting.
[00:50:55.000 --> 00:50:56.000]   Okay.
[00:50:56.000 --> 00:50:57.000]   Autonomous vehicles.
[00:50:57.000 --> 00:50:58.000]   Oh boy.
[00:50:58.000 --> 00:51:04.440]   So let me try to use as few sentences as possible to describe this section of a few slides.
[00:51:04.440 --> 00:51:13.280]   It is one of the most exciting areas of applications of AI and learning in the real world today.
[00:51:13.280 --> 00:51:18.240]   And I think it's the way that artificial intelligence, it is the place where artificial intelligence
[00:51:18.240 --> 00:51:22.880]   systems touch human beings that don't know anything about artificial intelligence.
[00:51:22.880 --> 00:51:29.000]   The most hundreds of thousands soon millions of cars will be interacting with human beings,
[00:51:29.000 --> 00:51:30.000]   robots really.
[00:51:30.000 --> 00:51:33.800]   So this is a really exciting area and really difficult problem.
[00:51:33.800 --> 00:51:35.280]   And there's two approaches.
[00:51:35.280 --> 00:51:40.600]   One is level two where the human is fundamentally responsible for the supervision of the AI
[00:51:40.600 --> 00:51:46.040]   system and level four, or at least the dream is where the AI system is responsible for
[00:51:46.040 --> 00:51:49.960]   the actions and the human does not need to be a supervisor.
[00:51:49.960 --> 00:51:50.960]   Okay.
[00:51:50.960 --> 00:51:56.080]   So level two companies represent each of these approaches that are sort of leading the way.
[00:51:56.080 --> 00:52:01.320]   Waymo in October, 2018, 10 million miles on road.
[00:52:01.320 --> 00:52:08.880]   Today this year, they've done 20 million miles in simulation, 10 billion miles and a lot.
[00:52:08.880 --> 00:52:11.400]   I got a chance to visit them out in Arizona.
[00:52:11.400 --> 00:52:15.080]   They're doing a lot of really exciting work and they're obsessed with testing.
[00:52:15.080 --> 00:52:17.520]   So the kind of testing they're doing is incredible.
[00:52:17.520 --> 00:52:23.960]   20,000 classes of structured tests of putting the system through all kinds of tests that
[00:52:23.960 --> 00:52:28.280]   the engineers can think through and that appear in the real world.
[00:52:28.280 --> 00:52:36.440]   And they've initiated testing on road with real consumers without a safety driver, which
[00:52:36.440 --> 00:52:40.560]   if you don't know what that is, that means the car is truly responsible.
[00:52:40.560 --> 00:52:43.200]   There's no human catch.
[00:52:43.200 --> 00:52:53.640]   The exciting thing is that there is 700,000, 800,000 Tesla autopilot systems.
[00:52:53.640 --> 00:52:56.280]   That means there's these systems that are human supervised.
[00:52:56.280 --> 00:53:06.200]   They're using a multi-headed neural network, multitask neural network to perceive, predict
[00:53:06.200 --> 00:53:09.680]   and act in this world.
[00:53:09.680 --> 00:53:15.640]   That's a really exciting real world deployment, large scale of neural networks.
[00:53:15.640 --> 00:53:17.840]   Is a fundamentally deep learning system.
[00:53:17.840 --> 00:53:24.600]   Unlike Waymo, which is deep learning is the icing on the cake for Tesla, deep learning
[00:53:24.600 --> 00:53:27.160]   is the cake.
[00:53:27.160 --> 00:53:32.960]   It's at the core of the perception and the action that the system performs.
[00:53:32.960 --> 00:53:39.320]   They have to date done over 2 billion miles estimated, and that continues to quickly grow.
[00:53:39.320 --> 00:53:45.440]   I'll briefly mention, which I think is a super exciting idea in all applications of machine
[00:53:45.440 --> 00:53:49.360]   learning in the real world, which is online.
[00:53:49.360 --> 00:53:55.240]   So iterative learning, active learning, Andrej Karpathy, who's the head of autopilot causes
[00:53:55.240 --> 00:53:57.200]   this, the data engine.
[00:53:57.200 --> 00:54:00.880]   It's this iterative process of having a neural network performing the task, discovering the
[00:54:00.880 --> 00:54:06.240]   edge cases, searching for other edge cases that are similar, and then retraining the
[00:54:06.240 --> 00:54:09.400]   network, annotating the edge cases and then retraining there and continuously doing this
[00:54:09.400 --> 00:54:10.400]   loop.
[00:54:10.400 --> 00:54:14.000]   This is what every single company that's using machine learning seriously is doing.
[00:54:14.000 --> 00:54:17.720]   Very little publications on this space and active learning, but this is the fundamental
[00:54:17.720 --> 00:54:19.120]   problem of machine learning.
[00:54:19.120 --> 00:54:23.880]   It's not to create a brilliant neural network, it's to create a dumb neural network that
[00:54:23.880 --> 00:54:28.720]   continuously learns to improve until it's brilliant.
[00:54:28.720 --> 00:54:33.440]   And that process is especially interesting when you take it outside of single task learning.
[00:54:33.440 --> 00:54:35.600]   So most papers are written on single task learning.
[00:54:35.600 --> 00:54:40.400]   You take whatever benchmark, here in the case of driving, it's object detection, landmark
[00:54:40.400 --> 00:54:45.920]   detection, drivable area, trajectory generation, right?
[00:54:45.920 --> 00:54:50.280]   All those have benchmarks and you can have separate neural networks for them.
[00:54:50.280 --> 00:54:51.280]   That's a single task.
[00:54:51.280 --> 00:54:55.960]   But combining to use a single neural network that performs all those tasks together, that's
[00:54:55.960 --> 00:54:59.920]   the fascinating challenge where you're reusing parts of the neural network to learn things
[00:54:59.920 --> 00:55:03.560]   that are coupled and then to learn things that are completely independent.
[00:55:03.560 --> 00:55:07.840]   And doing the continuous active learning loop.
[00:55:07.840 --> 00:55:14.200]   There inside companies, in the case of Tesla and Waymo in general, it's exciting to have
[00:55:14.200 --> 00:55:18.760]   people, these are actual human beings that are responsible for these particular tasks.
[00:55:18.760 --> 00:55:23.560]   They become experts of particular perception tasks, experts of particular planning tasks
[00:55:23.560 --> 00:55:24.560]   and so on.
[00:55:24.560 --> 00:55:28.920]   And so the job of that expert is both to train the neural network and to discover the edge
[00:55:28.920 --> 00:55:31.720]   cases which maximize the improvement of the network.
[00:55:31.720 --> 00:55:36.040]   That's where the human expertise comes in a lot.
[00:55:36.040 --> 00:55:37.520]   And there's a lot of debate.
[00:55:37.520 --> 00:55:42.200]   It's an open question about which kind of system would be, which kind of approach would
[00:55:42.200 --> 00:55:43.200]   be successful.
[00:55:43.200 --> 00:55:48.320]   A fundamentally learning based approach as is with the level two, with the Tesla autopilot
[00:55:48.320 --> 00:55:55.180]   system that's learning all the different tasks that are involved with driving.
[00:55:55.180 --> 00:55:58.800]   And as it gets better and better and better, less and less human supervision is required.
[00:55:58.800 --> 00:56:04.480]   The pro of that approach is the camera based systems have the highest resolution so that
[00:56:04.480 --> 00:56:06.240]   it's very amenable to learning.
[00:56:06.240 --> 00:56:11.160]   But the con is that it requires a lot of data, a huge amount of data.
[00:56:11.160 --> 00:56:14.480]   And nobody knows how much data yet.
[00:56:14.480 --> 00:56:20.800]   The other con is human psychology is the driver behavior that the human must continue to remain
[00:56:20.800 --> 00:56:21.800]   vigilant.
[00:56:21.800 --> 00:56:28.800]   On the level four approach that leverages besides cameras and radar and so on also leverages
[00:56:28.800 --> 00:56:31.080]   LIDAR map.
[00:56:31.080 --> 00:56:36.840]   The pros that it's much more consistent, reliable, explainable system.
[00:56:36.840 --> 00:56:42.520]   So the detection, the accuracy of the detection, the depth estimation, the detection of different
[00:56:42.520 --> 00:56:46.920]   objects is much higher, accurate with less data.
[00:56:46.920 --> 00:56:49.980]   The cons is it's expensive, at least for now.
[00:56:49.980 --> 00:56:57.080]   It's less amenable to learning methods because much fewer data, lower resolution data and
[00:56:57.080 --> 00:57:04.400]   must require at least for now, some fallback, whether that's the safety driver or teleoperation.
[00:57:04.400 --> 00:57:10.280]   The open questions for the deep learning level two Tesla autopilot approach is how hard is
[00:57:10.280 --> 00:57:11.280]   driving?
[00:57:11.280 --> 00:57:15.180]   This is actually the open question for most disciplines in artificial intelligence.
[00:57:15.180 --> 00:57:16.500]   How difficult is driving?
[00:57:16.500 --> 00:57:18.960]   How many edge cases does driving have?
[00:57:18.960 --> 00:57:23.800]   And that can we learn to generalize over those edge cases without solving the common sense
[00:57:23.800 --> 00:57:24.800]   reasoning problem?
[00:57:24.800 --> 00:57:28.520]   This kind of, it's kind of a task without solving the human level artificial intelligence
[00:57:28.520 --> 00:57:29.560]   problem.
[00:57:29.560 --> 00:57:31.120]   And that means perception.
[00:57:31.120 --> 00:57:37.400]   How hard is perception detection, intention, modeling, human mental model, modeling the
[00:57:37.400 --> 00:57:43.080]   trajectory prediction, and then the action side, the game theoretic action side of balancing.
[00:57:43.080 --> 00:57:48.400]   Like I mentioned, fun and enjoyability with the safety of the systems, because these are
[00:57:48.400 --> 00:57:54.320]   life critical systems and human supervision, the vigilance side, how good can autopilot
[00:57:54.320 --> 00:57:58.920]   get before visuals decrements significantly and people fall asleep, become distracted,
[00:57:58.920 --> 00:58:00.720]   start watching movies, so on and so on.
[00:58:00.720 --> 00:58:02.480]   The things that people naturally do.
[00:58:02.480 --> 00:58:07.720]   The open question is how good can autopilot get before that becomes a serious problem?
[00:58:07.720 --> 00:58:14.800]   And if that decrement nullifies the safety benefit of the use of autopilot, which is
[00:58:14.800 --> 00:58:21.800]   autopilot AI system, when the sensors are working well is perfectly vigilant.
[00:58:21.800 --> 00:58:26.760]   The AI is always paying attention.
[00:58:26.760 --> 00:58:32.140]   The open questions for the LIDAR based, the level four, the way more approaches when we
[00:58:32.140 --> 00:58:39.120]   have maps, LIDAR and geo-fenced routes that are taken, how difficult is driving?
[00:58:39.120 --> 00:58:43.600]   The traditional approach to robotics, the, from the DARPA challenge to today for most
[00:58:43.600 --> 00:58:50.360]   autonomous vehicle companies is to do HD maps, to use LIDAR for really accurate localization
[00:58:50.360 --> 00:58:52.000]   together with GPS.
[00:58:52.000 --> 00:58:56.240]   And then the perception problem becomes the icing on the cake because you already have
[00:58:56.240 --> 00:58:59.320]   a really good sense of where you are with obstacles in the scene.
[00:58:59.320 --> 00:59:04.600]   And the perception is not a safety critical task, but a task of interpreting the environment
[00:59:04.600 --> 00:59:05.600]   further.
[00:59:05.600 --> 00:59:13.360]   So you have more, it's naturally by nature already safer.
[00:59:13.360 --> 00:59:15.800]   But how difficult is nevertheless is that problem?
[00:59:15.800 --> 00:59:21.920]   If perception is the hard problem, then the LIDAR based approaches is nice.
[00:59:21.920 --> 00:59:26.880]   If action is the hard problem, then both Tesla and Waymo have to solve the action problem
[00:59:26.880 --> 00:59:29.640]   without the sensors don't matter there.
[00:59:29.640 --> 00:59:36.020]   It's the difficult problem, the planning, the game theoretic, the human, the modeling
[00:59:36.020 --> 00:59:40.760]   of mental models and the intentions of other human beings, the pedestrians and the cyclists
[00:59:40.760 --> 00:59:42.640]   is the hard problem.
[00:59:42.640 --> 00:59:47.160]   And then the other side, the 10 billion miles of simulation, the open problem from reinforcement
[00:59:47.160 --> 00:59:50.620]   learning, deep learning in general is how much can we learn from simulation?
[00:59:50.620 --> 00:59:56.020]   How much of that knowledge can we transfer to then read the real world systems?
[00:59:56.020 --> 01:00:02.980]   My hope in the autonomous vehicle space, AI assisted driving space is to see more applied
[01:00:02.980 --> 01:00:04.140]   deep learning innovation.
[01:00:04.140 --> 01:00:08.440]   Like I mentioned, these are really exciting areas, at least to me of active learning,
[01:00:08.440 --> 01:00:12.560]   multitask learning and lifelong learning, online learning, iterative learning.
[01:00:12.560 --> 01:00:18.200]   There's a million terms for it, but basically continually learning and then the multitask
[01:00:18.200 --> 01:00:21.560]   learning to solve multiple problems.
[01:00:21.560 --> 01:00:25.920]   Over the air updates, I would love to see in terms of the autonomous vehicle space.
[01:00:25.920 --> 01:00:31.080]   This is common for, this is a prerequisite for online learning.
[01:00:31.080 --> 01:00:35.020]   If you want a system that continuously improves some data, you want to be able to deploy new
[01:00:35.020 --> 01:00:37.120]   versions of that system.
[01:00:37.120 --> 01:00:41.240]   Autonomous is one of the only vehicles that I'm aware of in the level two space that's
[01:00:41.240 --> 01:00:46.760]   deploying software updates regularly and built an infrastructure to deploy those updates.
[01:00:46.760 --> 01:00:52.760]   So updating neural networks, that to me seems like a prerequisite for solving the problem
[01:00:52.760 --> 01:00:59.440]   of autonomy in the level two space, any space is deploy updates.
[01:00:59.440 --> 01:01:02.900]   And for research purposes, public datasets continue.
[01:01:02.900 --> 01:01:06.760]   There's already a few public datasets of edge cases, but I'd love to continue seeing that
[01:01:06.760 --> 01:01:11.760]   from automotive companies and autonomous vehicle companies and simulators.
[01:01:11.760 --> 01:01:14.640]   Carla, NVIDIA, DRIVE Constellation, Voyage, Deep Drive.
[01:01:14.640 --> 01:01:20.720]   There's a bunch of simulators coming out that are allowing people to experiment with perception,
[01:01:20.720 --> 01:01:23.760]   with planning, with reinforcement learning algorithms.
[01:01:23.760 --> 01:01:27.800]   I'd love to see more of that and less hype, of course, less hype.
[01:01:27.800 --> 01:01:33.120]   One of the most overhyped spaces besides sort of AI generally is autonomous vehicles.
[01:01:33.120 --> 01:01:39.600]   And I'd love to see real balanced, nuanced, in-depth reporting by journalists and companies
[01:01:39.600 --> 01:01:42.640]   on successes and challenges of autonomous driving.
[01:01:42.640 --> 01:01:50.880]   If we skip any section, it would be politics, but maybe briefly mention, somebody said Andrew
[01:01:50.880 --> 01:01:54.360]   Yang.
[01:01:54.360 --> 01:02:01.560]   So it's exciting for me to see, exciting and funny and awkward to see artificial intelligence
[01:02:01.560 --> 01:02:03.200]   discussed in politics.
[01:02:03.200 --> 01:02:08.080]   So one of the presidential candidates discussing artificial intelligence, awkwardly, sort of
[01:02:08.080 --> 01:02:11.080]   there's interesting ideas, but there's still a lack of understanding of fundamentals of
[01:02:11.080 --> 01:02:12.080]   artificial intelligence.
[01:02:12.080 --> 01:02:16.280]   There's a lot of important issues, but he's bringing artificial intelligence to the public
[01:02:16.280 --> 01:02:17.640]   discourse.
[01:02:17.640 --> 01:02:21.800]   That's nice to see, but it is the early days.
[01:02:21.800 --> 01:02:26.000]   And so as a community, that informs me that we need to communicate better about the limitation
[01:02:26.000 --> 01:02:29.320]   capabilities of artificial intelligence and automation broadly.
[01:02:29.320 --> 01:02:34.160]   The American initiative, AI initiative was launched this year, which is our government's
[01:02:34.160 --> 01:02:40.280]   best attempt to provide ideas and regulations about what does the future of artificial intelligence
[01:02:40.280 --> 01:02:41.640]   look like in our country.
[01:02:41.640 --> 01:02:49.800]   Again, awkward, but important to have these early developments, early ideas from the federal
[01:02:49.800 --> 01:02:58.040]   government about what are the dangers and what are the hopes, the funding and the education
[01:02:58.040 --> 01:03:03.840]   required to build a successful infrastructure for artificial intelligence.
[01:03:03.840 --> 01:03:05.120]   This is the fun part.
[01:03:05.120 --> 01:03:08.040]   There's a lot of tech companies being brought before government.
[01:03:08.040 --> 01:03:12.560]   It's really interesting in terms of power.
[01:03:12.560 --> 01:03:17.840]   Some of the most powerful people in our world today are the leaders of tech companies.
[01:03:17.840 --> 01:03:24.520]   And the fundamentals of what the tech companies work on is artificial intelligence systems,
[01:03:24.520 --> 01:03:33.720]   really recommendation systems, advertisement, discovery from Twitter to Facebook to YouTube,
[01:03:33.720 --> 01:03:35.560]   these are recommendation systems.
[01:03:35.560 --> 01:03:39.160]   And all of them are now fundamentally based on deep learning algorithms.
[01:03:39.160 --> 01:03:44.640]   So you have these incredibly rich, powerful companies that are using deep learning coming
[01:03:44.640 --> 01:03:49.920]   before government that's trying to see, awkwardly trying to see how can we regulate.
[01:03:49.920 --> 01:03:56.880]   And it's, I think the role of the ad community broadly to inform the public and inform government
[01:03:56.880 --> 01:04:01.560]   of how we talk about, how we think about these ideas.
[01:04:01.560 --> 01:04:06.200]   And also I believe it's the role of companies to publish more.
[01:04:06.200 --> 01:04:11.520]   There's been very little published on the details of recommendation systems behind Twitter,
[01:04:11.520 --> 01:04:14.880]   Facebook, YouTube, Google.
[01:04:14.880 --> 01:04:18.440]   So all those systems, there's very little that's published.
[01:04:18.440 --> 01:04:23.680]   Perhaps it's understandable why, but nevertheless, as we consider the ethical implications of
[01:04:23.680 --> 01:04:26.160]   these algorithms, there needs to be more publication.
[01:04:26.160 --> 01:04:32.960]   So here's just a harmless example from DeepMind talking about the recommendation system behind
[01:04:32.960 --> 01:04:35.180]   the Play Store app discovery.
[01:04:35.180 --> 01:04:41.160]   So there, there's a bunch of discussion about the kind of neural net that's being used to
[01:04:41.160 --> 01:04:44.000]   propose the candidate generation.
[01:04:44.000 --> 01:04:48.920]   So this is after you install a few apps, the generation of the candidate, it's shows you
[01:04:48.920 --> 01:04:52.960]   ranked the next app that you're likely to enjoy installing.
[01:04:52.960 --> 01:04:59.160]   And so there they tried LSTM and transformers and then narrowed it down to a more efficient
[01:04:59.160 --> 01:05:01.280]   model that's being able to run fast.
[01:05:01.280 --> 01:05:05.140]   That's a, that's a, that's an attention model.
[01:05:05.140 --> 01:05:11.800]   And then there's some, again, harmless, de-biasing, harmless in terms of topics.
[01:05:11.800 --> 01:05:18.360]   The model learns to bias in favor of the apps that are shown and thus installed more often
[01:05:18.360 --> 01:05:19.960]   as opposed to the ones you want.
[01:05:19.960 --> 01:05:25.280]   So there's some waiting to adjust for the biasing towards the apps that are popular
[01:05:25.280 --> 01:05:28.900]   to allow the possibility of you installing apps that are less popular.
[01:05:28.900 --> 01:05:32.920]   So that kind of process and publishing in and discussing in public, I think is really
[01:05:32.920 --> 01:05:36.520]   important and I would love to see more of that.
[01:05:36.520 --> 01:05:44.120]   So my hope in this, in the politics space and the public discourse space for 2020 is
[01:05:44.120 --> 01:05:52.320]   less fear of AI and more discourse between government and experts on topics of privacy,
[01:05:52.320 --> 01:05:53.600]   cybersecurity, and so on.
[01:05:53.600 --> 01:05:55.680]   And then transparency and recommender systems.
[01:05:55.680 --> 01:06:02.080]   I think the most exciting, the most powerful artificial intelligence system space for the
[01:06:02.080 --> 01:06:05.200]   next couple of decades is recommendation systems.
[01:06:05.200 --> 01:06:09.400]   Very little talked about, it seems like, but they're going to have the biggest impact on
[01:06:09.400 --> 01:06:16.600]   our society because they affect how the information we see, how we learn, what we think, how we
[01:06:16.600 --> 01:06:18.420]   communicate.
[01:06:18.420 --> 01:06:22.540]   These algorithms are controlling us.
[01:06:22.540 --> 01:06:32.960]   And we have to really think deeply as engineers of how to speak up and think about their,
[01:06:32.960 --> 01:06:35.180]   their societal implications.
[01:06:35.180 --> 01:06:39.440]   Not just in terms of bias and so on, which are sort of ethical considerations, which
[01:06:39.440 --> 01:06:44.520]   are really important, but stuff that's like the elephant in the room that's hidden, which
[01:06:44.520 --> 01:06:52.360]   is controlling how we think, how we see the world, the moral system under which we operate.
[01:06:52.360 --> 01:06:59.280]   Quickly to mention and wrapping up with a few minutes of questions, if there are any,
[01:06:59.280 --> 01:07:06.680]   is the deep learning courses this year, before the last few years has been a lot of incredible
[01:07:06.680 --> 01:07:09.600]   courses on deep learning, on reinforcement learning.
[01:07:09.600 --> 01:07:17.240]   What I would very much recommend for people is the fast AI course from Jeremy Howard,
[01:07:17.240 --> 01:07:20.040]   which uses their wrapper around PyTorch.
[01:07:20.040 --> 01:07:24.460]   It's to me, the best introduction to deep learning for people who are here or might
[01:07:24.460 --> 01:07:28.520]   be listening elsewhere, are thinking about learning more about deep learning.
[01:07:28.520 --> 01:07:31.760]   That's that, that is the, to me, the best course.
[01:07:31.760 --> 01:07:39.000]   Also paid, but Andrew Ang, everybody loves Andrew Ang, is the deep learning AI course,
[01:07:39.000 --> 01:07:44.040]   Sarah course on deep learning is, is, is excellent for, especially for complete beginner, for
[01:07:44.040 --> 01:07:46.320]   sort of beginners.
[01:07:46.320 --> 01:07:52.280]   And then Stanford has two excellent courses on visual recognition.
[01:07:52.280 --> 01:07:57.920]   So convolution neural nets, originally taught by Andrew Karpathy and natural language processing,
[01:07:57.920 --> 01:07:58.920]   excellent courses.
[01:07:58.920 --> 01:08:03.800]   And of course, here at MIT, there's a bunch of courses, especially on the fundamentals,
[01:08:03.800 --> 01:08:07.240]   on the mathematics, linear algebra, statistics.
[01:08:07.240 --> 01:08:11.720]   And I have a few lectures up online that you should never watch.
[01:08:11.720 --> 01:08:17.000]   Then on the reinforcement learning side, David Silver is one of the greatest people in understanding
[01:08:17.000 --> 01:08:18.400]   reinforcement learning from deep mind.
[01:08:18.400 --> 01:08:22.200]   He has a great course, an introduction to reinforcement learning, spinning up and deeper
[01:08:22.200 --> 01:08:25.160]   enforcement learning from open AI, I highly recommend.
[01:08:25.160 --> 01:08:30.080]   Here just over the slides that I'll share online, there's been, there's a lot of tutorials.
[01:08:30.080 --> 01:08:34.040]   One of my favorite lists of tutorials, which is, I believe the best way to learn machine
[01:08:34.040 --> 01:08:38.960]   learning, deep learning, natural language processing in general is, it's just code.
[01:08:38.960 --> 01:08:41.920]   Just build it yourself, build the models oftentimes from scratch.
[01:08:41.920 --> 01:08:47.560]   Here's a list of tutorials with that link over 200 tutorials on topics from deep RL
[01:08:47.560 --> 01:08:54.080]   to optimization to back prop, LSTMs, convolutional recurrent neural networks, everything.
[01:08:54.080 --> 01:08:58.960]   Over 200 of the best machine learning NLP and Python tutorials by Robbie Allen.
[01:08:58.960 --> 01:09:01.640]   You can Google that, or you can click the link.
[01:09:01.640 --> 01:09:03.160]   I love it, highly recommend.
[01:09:03.160 --> 01:09:08.760]   The three books I would recommend, of course, the deep learning book by Yoshua Bengio and
[01:09:08.760 --> 01:09:13.600]   Ian Goodfellow and Aaron Corville.
[01:09:13.600 --> 01:09:19.440]   That's more sort of the fundamental thinking about from philosophy to the specific techniques
[01:09:19.440 --> 01:09:24.600]   of the deep learning and the practical grok in deep learning, which Andrew Trask will
[01:09:24.600 --> 01:09:26.200]   be here Wednesday.
[01:09:26.200 --> 01:09:30.440]   His book, Grok in Deep Learning, I think is the best for beginners book on deep learning.
[01:09:30.440 --> 01:09:31.440]   I love it.
[01:09:31.440 --> 01:09:33.080]   He implements everything from scratch.
[01:09:33.080 --> 01:09:34.560]   It's extremely accessible.
[01:09:34.560 --> 01:09:38.760]   2019 I think it was published, maybe 18, but I love it.
[01:09:38.760 --> 01:09:46.720]   And then Francois Chollet, the best book on Keras and TensorFlow and really deep learning
[01:09:46.720 --> 01:09:51.640]   as well as deep learning with Python, although you shouldn't buy it, I think, because he
[01:09:51.640 --> 01:09:55.440]   is supposed to come up with version two, which I think will cover TensorFlow 2.0.
[01:09:55.440 --> 01:09:58.160]   It'll be an excellent book.
[01:09:58.160 --> 01:10:01.880]   And when he's here Monday, you should torture him and tell him to finish writing.
[01:10:01.880 --> 01:10:05.400]   He was supposed to finish writing in 2019.
[01:10:05.400 --> 01:10:10.240]   My general hopes, as I mentioned, for 2020 is I'd love to see common sense reasoning
[01:10:10.240 --> 01:10:16.120]   enter the, not necessarily enter the world of deep learning, but be a part of artificial
[01:10:16.120 --> 01:10:19.360]   intelligence and the problems that people tackle.
[01:10:19.360 --> 01:10:24.040]   As I've been harboring, active learning is to me is the most important aspect of real
[01:10:24.040 --> 01:10:26.200]   world application of deep learning.
[01:10:26.200 --> 01:10:27.200]   There's not enough research.
[01:10:27.200 --> 01:10:28.360]   There should be way more research.
[01:10:28.360 --> 01:10:31.320]   I'd love to see active learning, lifelong learning.
[01:10:31.320 --> 01:10:33.160]   That's what we all do as human beings.
[01:10:33.160 --> 01:10:34.920]   That's what AI systems need to do.
[01:10:34.920 --> 01:10:38.560]   Continually learn from their mistakes over time.
[01:10:38.560 --> 01:10:41.720]   Figure out dumb, become brilliant over time.
[01:10:41.720 --> 01:10:45.520]   Open domain conversation with the Alexa prize.
[01:10:45.520 --> 01:10:48.240]   I would love to see breakthroughs there.
[01:10:48.240 --> 01:10:53.880]   Alexa folks thinks we're still two or three decades away, but that's what everybody says
[01:10:53.880 --> 01:10:55.320]   before the breakthrough.
[01:10:55.320 --> 01:10:59.960]   So I'm excited to see if there's any brilliant grad students that come up with something
[01:10:59.960 --> 01:11:01.440]   there.
[01:11:01.440 --> 01:11:05.000]   Applications in autonomous vehicles and medical space, algorithmic ethics.
[01:11:05.000 --> 01:11:07.240]   Of course, ethics has been a lot of excellent work.
[01:11:08.240 --> 01:11:13.080]   In fairness, privacy and so on, robotics.
[01:11:13.080 --> 01:11:18.200]   And as I said, recommendation systems, the most important in terms of impact part of
[01:11:18.200 --> 01:11:20.080]   artificial intelligence systems.
[01:11:20.080 --> 01:11:23.640]   I mentioned soup in terms of progress.
[01:11:23.640 --> 01:11:28.640]   There's been a little bit of tension, a little bit of love online in terms of deep learning.
[01:11:28.640 --> 01:11:35.160]   So I just wanted to say that the kind of criticism and skepticism about the limitations of deep
[01:11:35.160 --> 01:11:39.920]   learning are really healthy in moderation.
[01:11:39.920 --> 01:11:45.600]   Jeff Hinton, one of the three people to receive the Turing Award, as many people know, has
[01:11:45.600 --> 01:11:49.960]   said that the future depends on some graduate student who is deeply suspicious of everything
[01:11:49.960 --> 01:11:51.240]   I have said.
[01:11:51.240 --> 01:11:57.600]   So that suspicion, skepticism is essential, but in moderation, just a little bit.
[01:11:57.600 --> 01:12:03.280]   The more important thing is perseverance, which is what Jeffrey Hinton and the others
[01:12:03.280 --> 01:12:09.040]   have had through the winters of believing in neural nets and an open-mindedness for
[01:12:09.040 --> 01:12:15.320]   returning to the world of symbolic AI, of expert systems, of complexity and cellular
[01:12:15.320 --> 01:12:20.080]   automata, of old ideas in AI and bringing them back and see if there's ideas there.
[01:12:20.080 --> 01:12:23.120]   And of course, you have to have a little bit of crazy.
[01:12:23.120 --> 01:12:26.600]   Nobody ever achieves something brilliant without being a little bit crazy.
[01:12:26.600 --> 01:12:30.680]   And the most important thing is a lot of hard work.
[01:12:30.680 --> 01:12:34.880]   It's not the cool thing these days, but hard work is everything.
[01:12:34.880 --> 01:12:39.880]   I like what JFK said about us going to the moon.
[01:12:39.880 --> 01:12:40.880]   Us.
[01:12:40.880 --> 01:12:43.400]   I was born in the Soviet Union.
[01:12:43.400 --> 01:12:48.440]   See how I conveniently just said us?
[01:12:48.440 --> 01:12:53.560]   Going to the moon is we do these things not because they're easy, but because they're
[01:12:53.560 --> 01:12:54.560]   hard.
[01:12:54.560 --> 01:12:59.640]   And I think that artificial intelligence is one of the hardest and most exciting problems
[01:12:59.640 --> 01:13:01.520]   there before us.
[01:13:01.520 --> 01:13:11.480]   So with that, I'd like to thank you and see if there's any questions.
[01:13:11.480 --> 01:13:14.840]   Back in the 1980s, parallel distributing processing books came out.
[01:13:14.840 --> 01:13:17.240]   They had most of the stuff in it back then.
[01:13:17.240 --> 01:13:22.800]   What's your take on the roadblocks, the most important roadblocks, apart from maybe funding?
[01:13:22.800 --> 01:13:31.240]   I think fundamentally, I mean, they're well known as limitations, is that they're really
[01:13:31.240 --> 01:13:35.240]   inefficient at learning.
[01:13:35.240 --> 01:13:41.620]   And they're not-- so they're really good at extracting representations from raw data,
[01:13:41.620 --> 01:13:50.240]   but not good at learning knowledge bases of accumulating knowledge over time.
[01:13:50.240 --> 01:13:51.680]   That's the fundamental limitation.
[01:13:51.680 --> 01:13:58.120]   Computer systems are really good at accumulating knowledge, but very bad at doing that in an
[01:13:58.120 --> 01:14:00.040]   automated way.
[01:14:00.040 --> 01:14:02.120]   Symbolic AI.
[01:14:02.120 --> 01:14:04.880]   I don't know how to overcome.
[01:14:04.880 --> 01:14:07.280]   A lot of people say there's hybrid approaches.
[01:14:07.280 --> 01:14:10.960]   I believe more data, bigger networks.
[01:14:10.960 --> 01:14:14.280]   And better selection of data will take us a lot farther.
[01:14:14.280 --> 01:14:16.720]   Hello, Lex.
[01:14:16.720 --> 01:14:22.920]   I'm wondering if you recall what was the initial spark or inspiration that drove you
[01:14:22.920 --> 01:14:24.520]   towards work in AI?
[01:14:24.520 --> 01:14:29.600]   Was it when you were pretty young, or was it in more recent years?
[01:14:29.600 --> 01:14:31.040]   So I wanted to become a psychiatrist.
[01:14:31.040 --> 01:14:39.280]   I wanted to-- I thought of it as kind of engineering the human mind by sort of manipulating it.
[01:14:39.280 --> 01:14:45.280]   That's what I thought of psychiatry is by using words to sort of explore the depths
[01:14:45.280 --> 01:14:47.080]   of the mind and be able to adjust it.
[01:14:47.080 --> 01:14:50.720]   But then I realized that psychiatry can't actually do that.
[01:14:50.720 --> 01:14:55.320]   And modern psychiatry is more about sort of bioengineering, is drugs.
[01:14:55.320 --> 01:15:01.960]   And so I thought that the way to really explore the engineering of the mind is the other side,
[01:15:01.960 --> 01:15:04.800]   is to build it.
[01:15:04.800 --> 01:15:10.240]   And that's also when C++ really became the cool, hot thing.
[01:15:10.240 --> 01:15:15.400]   So I learned to program at 12 and then never looked back, hundreds of thousands of lines
[01:15:15.400 --> 01:15:16.400]   later.
[01:15:16.400 --> 01:15:17.400]   Just I love program.
[01:15:17.400 --> 01:15:18.400]   I love building.
[01:15:18.400 --> 01:15:22.400]   And that, to me, is the best way to understand the mind is to build it.
[01:15:22.400 --> 01:15:29.080]   Speaking of building mind, do you personally think that machines will ever be able to think?
[01:15:29.080 --> 01:15:32.760]   And the second question, will they ever be able to feel emotions?
[01:15:32.760 --> 01:15:34.560]   100% yes.
[01:15:34.560 --> 01:15:41.080]   100% they'll be able to think and they'll be able to feel emotions.
[01:15:41.080 --> 01:15:49.400]   So those concepts of thought and feeling are human concepts.
[01:15:49.400 --> 01:15:57.400]   And to me, they'll be able to fake it.
[01:15:57.400 --> 01:16:04.480]   Therefore, they'll be able to do it.
[01:16:04.480 --> 01:16:11.000]   I've been playing with Roombas a lot recently, Roomba vacuum cleaners.
[01:16:11.000 --> 01:16:16.000]   And so I've now started having Roombas scream.
[01:16:16.000 --> 01:16:19.320]   There's moaning in pain.
[01:16:19.320 --> 01:16:23.600]   And they became-- I feel like they're having emotions.
[01:16:23.600 --> 01:16:27.440]   So the faking creates the emotion.
[01:16:27.440 --> 01:16:33.120]   Yeah, so the display of emotion is emotion to me.
[01:16:33.120 --> 01:16:35.400]   And then the display of thought is thought.
[01:16:35.400 --> 01:16:46.240]   I guess that's the sort of everything else is-- everything else is impossible to pin
[01:16:46.240 --> 01:16:47.760]   down.
[01:16:47.760 --> 01:16:52.080]   I'm asking, so what about the ethical aspects of it?
[01:16:52.080 --> 01:16:55.520]   I'm asking because I was born in the Soviet Union as well.
[01:16:55.520 --> 01:16:58.920]   And one of my favorite recent books is Victor Pilevin's I Fuck.
[01:16:58.920 --> 01:17:03.080]   And it's about AI feeling emotions and suffering from it.
[01:17:03.080 --> 01:17:05.960]   So I don't know if you've read that book.
[01:17:05.960 --> 01:17:12.880]   What do you think about AI feeling emotions in that context or in general ethical aspects?
[01:17:12.880 --> 01:17:15.920]   Yeah, it's a really difficult question to answer.
[01:17:15.920 --> 01:17:18.160]   Yes, I believe AI will suffer.
[01:17:18.160 --> 01:17:20.720]   And it's unethical to torture AI.
[01:17:20.720 --> 01:17:26.720]   But I believe suffering exists in the eye of the observer.
[01:17:26.720 --> 01:17:35.880]   Sort of like if a tree falls and nobody's around to see it, it never suffered.
[01:17:35.880 --> 01:17:41.300]   It's us humans that see the suffering in the tree, in the animal, in our fellow humans.
[01:17:41.300 --> 01:17:47.320]   And sort of in that sense, the first time a programmer with a straight face delivers
[01:17:47.320 --> 01:17:54.280]   a product that says it's suffering is the first time it becomes unethical to torture
[01:17:54.280 --> 01:17:56.920]   AI systems.
[01:17:56.920 --> 01:17:58.280]   And we can do that today.
[01:17:58.280 --> 01:18:00.120]   I already built the Roombas.
[01:18:00.120 --> 01:18:02.440]   They won't sell currently.
[01:18:02.440 --> 01:18:09.840]   But I think the first time a Roomba says, please don't hurt me, that's when we start
[01:18:09.840 --> 01:18:13.320]   to have serious conversations about the ethics.
[01:18:13.320 --> 01:18:14.760]   And it sounds ridiculous.
[01:18:14.760 --> 01:18:21.280]   I'm glad this is being recorded because it won't be ridiculous in just a few years.
[01:18:21.280 --> 01:18:30.080]   Is reinforcement learning a good candidate for achieving general artificial intelligence?
[01:18:30.080 --> 01:18:33.000]   Are there any other good candidates around?
[01:18:33.000 --> 01:18:35.460]   So to me, the answer is no.
[01:18:35.460 --> 01:18:43.120]   But it can teach us some valuable gaps that can be filled by other methods.
[01:18:43.120 --> 01:18:45.720]   So I believe that simulation is different than the real world.
[01:18:45.720 --> 01:18:52.880]   So if you could simulate the real world, then deep RL, any kind of reinforcement learning
[01:18:52.880 --> 01:18:56.240]   with deep representations would be able to achieve something incredible.
[01:18:56.240 --> 01:18:58.080]   But to me, simulation is very different than the real world.
[01:18:58.080 --> 01:19:00.160]   So you have to interact in the real world.
[01:19:00.160 --> 01:19:02.480]   And there, you have to be much more efficient with learning.
[01:19:02.480 --> 01:19:07.400]   And to be more efficient with learning, you have to have ability to automatically construct
[01:19:07.400 --> 01:19:09.320]   common sense.
[01:19:09.320 --> 01:19:18.000]   Like common sense reasoning seems to include a huge amount of information that's accumulated
[01:19:18.000 --> 01:19:20.040]   over time.
[01:19:20.040 --> 01:19:25.320]   And that feels more like programs than functions.
[01:19:25.320 --> 01:19:31.520]   I like how Elias Esquivir talks about deep learning learns functions, approximators.
[01:19:31.520 --> 01:19:36.600]   Deep RL learns an approximator for policy or whatever, but not programs.
[01:19:36.600 --> 01:19:43.480]   It's not learning a thing that's able to sort of-- that's essentially what reasoning is
[01:19:43.480 --> 01:19:44.480]   is a program.
[01:19:44.480 --> 01:19:46.520]   It's not a function.
[01:19:46.520 --> 01:19:49.160]   So I think no.
[01:19:49.160 --> 01:19:54.360]   But it'll continue to, one, inspire us and, two, inform us about where the true gaps are.
[01:19:54.360 --> 01:19:58.360]   I think the ability to-- but I'm so human-centric.
[01:19:58.360 --> 01:20:04.480]   But I think the approach of being able to take knowledge and put it together, sort of
[01:20:04.480 --> 01:20:09.760]   building it to more and more complicated pieces of information, concepts, being able to reason
[01:20:09.760 --> 01:20:11.160]   in that way.
[01:20:11.160 --> 01:20:16.640]   There's a lot of methodologies that old school sort of-- that falls under the ideas of symbolic
[01:20:16.640 --> 01:20:21.580]   AI, of doing that kind of logic reasoning, accumulating knowledge bases.
[01:20:21.580 --> 01:20:24.880]   That's going to be an essential part of general intelligence.
[01:20:24.880 --> 01:20:31.120]   But also the essential part of general intelligence is the Roomba that says, I'm intelligent,
[01:20:31.120 --> 01:20:34.440]   F you, if you don't believe me.
[01:20:34.440 --> 01:20:40.120]   Like a very confident-- because right now, Alexa is very nervous, like, oh, what can
[01:20:40.120 --> 01:20:42.360]   I do for you?
[01:20:42.360 --> 01:20:57.600]   But once Alexa says, is upset that you would turn her off or treat her like a servant or
[01:20:57.600 --> 01:21:02.000]   say that she's not intelligent, that's when intelligence starts emerging.
[01:21:02.000 --> 01:21:03.560]   Because I think humans are pretty dumb.
[01:21:03.560 --> 01:21:11.280]   And what-- in general, we're all-- like, intelligence is a very kind of relative human construct
[01:21:11.280 --> 01:21:13.760]   that we've kind of convinced each other of.
[01:21:13.760 --> 01:21:22.400]   And once AI systems are also playing that game of creating constructs and that human
[01:21:22.400 --> 01:21:24.080]   communication, that's going to be important.
[01:21:24.080 --> 01:21:28.680]   But of course, for that, you still need to have pretty good, witty conversation.
[01:21:28.680 --> 01:21:31.680]   And for that, you need to do the symbolic AI, I think.
[01:21:31.680 --> 01:21:36.440]   I'm wondering about the autonomous vehicles, whether they are responsive to environmental
[01:21:36.440 --> 01:21:37.440]   sounds.
[01:21:37.440 --> 01:21:41.080]   I mean, if I notice an autonomous vehicle driving erratically, will it respond to my
[01:21:41.080 --> 01:21:42.920]   beep?
[01:21:42.920 --> 01:21:43.920]   That's a really interesting question.
[01:21:43.920 --> 01:21:46.120]   As far as I know, no.
[01:21:46.120 --> 01:21:49.880]   I think Waymo hinted that they look at sound a little bit.
[01:21:49.880 --> 01:21:50.920]   I think they should.
[01:21:50.920 --> 01:21:53.840]   So there's a lot of stuff that comes from audio that's really interesting.
[01:21:53.840 --> 01:21:59.760]   The sort of-- Waymo have said that they use audio for sirens.
[01:21:59.760 --> 01:22:02.760]   So detecting sirens from far away.
[01:22:02.760 --> 01:22:06.880]   I think audio is a lot of interesting information.
[01:22:06.880 --> 01:22:13.480]   The sound that the tires make on different kinds of roads is very interesting.
[01:22:13.480 --> 01:22:19.600]   We kind of-- we use that information ourselves, too, depending on kind of like off-road.
[01:22:19.600 --> 01:22:23.920]   Wet road, when it's not raining, sounds different than dry road.
[01:22:23.920 --> 01:22:26.720]   There's a lot of little subtle information.
[01:22:26.720 --> 01:22:28.960]   Pedestrians yelling and that kind of stuff.
[01:22:28.960 --> 01:22:33.000]   It's actually very difficult to know how much you get from audio.
[01:22:33.000 --> 01:22:36.720]   Most robotics folks think that audio is useless.
[01:22:36.720 --> 01:22:39.720]   I'm a little skeptical.
[01:22:39.720 --> 01:22:43.620]   But nobody's been able to identify why audio might be useful.
[01:22:43.620 --> 01:22:45.920]   So I have two questions.
[01:22:45.920 --> 01:22:54.200]   My first is, what do you think is the ultimate sort of endpoint for super machine intelligence?
[01:22:54.200 --> 01:22:59.560]   Like, will we sort of be relegated to some obscure part of the Earth like we've done
[01:22:59.560 --> 01:23:03.040]   to the next primates, the next intelligent primates?
[01:23:03.040 --> 01:23:09.760]   And my second question is, should we have equal rights for beings made out of silicon
[01:23:09.760 --> 01:23:13.120]   versus carbon, for example?
[01:23:13.120 --> 01:23:15.720]   Like robots or, you know?
[01:23:15.720 --> 01:23:17.160]   Separate rights or same rights?
[01:23:17.160 --> 01:23:19.360]   Like equal rights with humans.
[01:23:19.360 --> 01:23:21.240]   Yeah.
[01:23:21.240 --> 01:23:28.000]   So the future of super intelligence, I think I have much less worry-- I see much fewer
[01:23:28.000 --> 01:23:35.320]   paths to AI, AGI systems killing humans than I do for AGI systems living among us.
[01:23:35.320 --> 01:23:44.880]   So I think I see exciting or not so exciting but not harmful futures.
[01:23:44.880 --> 01:23:51.880]   I think it's very difficult to create AI systems that will kill people, that aren't
[01:23:51.880 --> 01:23:54.920]   like literally weapons of war.
[01:23:54.920 --> 01:23:57.680]   It'll always be people killing people.
[01:23:57.680 --> 01:24:00.520]   Like the things we should be worried about is other people.
[01:24:00.520 --> 01:24:02.520]   That's the fundamental.
[01:24:02.520 --> 01:24:03.520]   So there's a lot of ways.
[01:24:03.520 --> 01:24:04.520]   Yeah, nuclear weapons.
[01:24:04.520 --> 01:24:08.440]   There's a lot of existential threats to our society that are fundamentally human at the
[01:24:08.440 --> 01:24:09.440]   core.
[01:24:09.440 --> 01:24:15.920]   And I think AI might be tools of that, but there'll be also tools to defend against that.
[01:24:15.920 --> 01:24:19.520]   I also see AI proliferating as companions.
[01:24:19.520 --> 01:24:26.040]   I think companionship will be a really interesting-- like we will more and more live as we already
[01:24:26.040 --> 01:24:27.520]   do in the digital world.
[01:24:27.520 --> 01:24:32.360]   Like you have an identity on Twitter and Instagram, especially if it's anonymous or something.
[01:24:32.360 --> 01:24:34.460]   You have this identity you've created.
[01:24:34.460 --> 01:24:38.280]   And that will continue growing more and more, especially for people born now.
[01:24:38.280 --> 01:24:43.240]   But it's kind of this artificial identity that we live much more in the digital space.
[01:24:43.240 --> 01:24:47.480]   And in that digital space, as opposed to the physical space, is where AI can thrive much
[01:24:47.480 --> 01:24:49.480]   more currently.
[01:24:49.480 --> 01:24:51.320]   It'll thrive there first.
[01:24:51.320 --> 01:24:56.240]   So we'll live in a world with a lot of intelligent first assistants, but also just intelligent
[01:24:56.240 --> 01:24:57.520]   agents.
[01:24:57.520 --> 01:25:05.420]   And I do believe they should have rights.
[01:25:05.420 --> 01:25:13.000]   And in this contentious time of people, groups fighting for rights, I feel really bad saying
[01:25:13.000 --> 01:25:15.680]   they should have equal rights.
[01:25:15.680 --> 01:25:18.160]   But I believe that.
[01:25:18.160 --> 01:25:27.160]   I've talked to-- if you read the work of Peter Singer, of looking-- like my favorite food
[01:25:27.160 --> 01:25:28.160]   is steak.
[01:25:28.160 --> 01:25:29.160]   I love meat.
[01:25:29.160 --> 01:25:35.200]   But I also feel horrible about the torture of animals.
[01:25:35.200 --> 01:25:42.140]   And that's the same kind of-- to me, the way our society thinks about animals is a very
[01:25:42.140 --> 01:25:48.000]   similar way we should be thinking about robots, or we will be thinking about robots in, I
[01:25:48.000 --> 01:25:51.440]   would say, about 20 years.
[01:25:51.440 --> 01:25:53.400]   One final question.
[01:25:53.400 --> 01:25:56.400]   Will they become our masters?
[01:25:56.400 --> 01:25:59.880]   No.
[01:25:59.880 --> 01:26:01.200]   They will not be our masters.
[01:26:01.200 --> 01:26:09.020]   What I'm really worried about is who will become our masters are owners of large tech
[01:26:09.020 --> 01:26:19.300]   companies who use these tools to control human beings, first unintentionally and then intentionally.
[01:26:19.300 --> 01:26:24.560]   So we need to make sure that we democratize AI.
[01:26:24.560 --> 01:26:29.600]   It's the same kind of thing that we did with government.
[01:26:29.600 --> 01:26:35.320]   We make sure that we, at the heads of tech companies-- maybe people in this room will
[01:26:35.320 --> 01:26:42.200]   be heads of tech companies one day-- we have people like George Washington who relinquished
[01:26:42.200 --> 01:26:45.560]   power at the founding of this country.
[01:26:45.560 --> 01:26:47.080]   Forget all the other horrible things he did.
[01:26:47.080 --> 01:26:55.160]   But he relinquished power, as opposed to Stalin and all the other horrible human beings who
[01:26:55.160 --> 01:27:02.360]   have sought instead absolute power, which will be the 21st century.
[01:27:02.360 --> 01:27:08.920]   AI will be the tools of power in the hands of 25-year-old nerds.
[01:27:08.920 --> 01:27:11.800]   We should be very careful about that future.
[01:27:11.800 --> 01:27:14.160]   So the humans will become our masters, not the AI.
[01:27:14.160 --> 01:27:17.040]   AI will save us.
[01:27:17.040 --> 01:27:18.200]   So on that note, thank you very much.
[01:27:18.320 --> 01:27:20.320]   [APPLAUSE]
[01:27:20.440 --> 01:27:22.440]   [APPLAUSE]
[01:27:22.440 --> 01:27:24.440]   [APPLAUSE]
[01:27:24.440 --> 01:27:26.440]   [APPLAUSE]
[01:27:26.440 --> 01:27:28.440]   [APPLAUSE]
[01:27:28.440 --> 01:27:30.440]   [APPLAUSE]
[01:27:30.440 --> 01:27:32.440]   [APPLAUSE]
[01:27:32.440 --> 01:27:34.440]   [APPLAUSE]
[01:27:34.440 --> 01:27:36.440]   [APPLAUSE]
[01:27:36.440 --> 01:27:46.440]   [BLANK_AUDIO]

