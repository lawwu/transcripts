
[00:00:00.000 --> 00:00:02.000]   And we're live.
[00:00:02.000 --> 00:00:09.000]   Hey, everybody.
[00:00:09.000 --> 00:00:13.000]   I am assuming I'm live on YouTube right now.
[00:00:13.000 --> 00:00:15.000]   That's what Angelica has told me.
[00:00:15.000 --> 00:00:21.000]   It's hard for me to screen share and also have a look at the YouTube comments at the same time.
[00:00:21.000 --> 00:00:36.000]   So, Angelica, we've had some technical difficulties this week, and instead what we've decided to do with Fastbook is we're just streaming live on YouTube this week, which means it's going to be a bit of a different setup than all the past weeks.
[00:00:36.000 --> 00:00:53.000]   But hopefully we'll get through this together. Also, a huge shout out to Ravi for finding this out and mentioning us and letting us know about the Zoom links and the technical difficulty.
[00:00:53.000 --> 00:01:03.000]   So what we'll do this week, then, we will continue, we will wrap up Chapter 5, and we will start with Chapter 6, if time permits.
[00:01:03.000 --> 00:01:08.000]   We're going to follow this report. So we go to 1db.me/fastbook7.
[00:01:08.000 --> 00:01:17.000]   So this is where -- please, please, please, if you have any questions, just go to this link, 1db.me/fastbook7.
[00:01:17.000 --> 00:01:26.000]   Angelica, I'm not monitoring YouTube, but if you could please share this in the live chat, that would be wonderful.
[00:01:26.000 --> 00:01:31.000]   So if you go to this link, this will bring us to Fastbook Reading Group Week 7.
[00:01:31.000 --> 00:01:38.000]   So if you're there, please just wave at me or just maybe reply to this comment.
[00:01:38.000 --> 00:01:43.000]   I just want to feel that you're still there and you can hear me well.
[00:01:43.000 --> 00:01:49.000]   And yeah, we're good. And then we'll just continue.
[00:01:49.000 --> 00:01:54.000]   I'm really sorry about the Zoom webinar. We don't know what's happened over there.
[00:01:54.000 --> 00:02:00.000]   So we will have to investigate. And unfortunately, today, we didn't have enough time to fix it.
[00:02:00.000 --> 00:02:05.000]   So we just came up with this solution that we will just go live on YouTube instead.
[00:02:05.000 --> 00:02:10.000]   Should be fixed by next week, Akash. Excellent. Thanks, Abhishek.
[00:02:10.000 --> 00:02:17.000]   Okay, let's get started. And this is going to be a really quiet Zoom meeting for me.
[00:02:17.000 --> 00:02:25.000]   Excellent. That's really good to see.
[00:02:25.000 --> 00:02:31.000]   Okay, perfect. So let's do this. Let's start.
[00:02:31.000 --> 00:02:34.000]   This week has been a really, really wonderful week.
[00:02:34.000 --> 00:02:45.000]   And I'm so excited to say many of you have been writing and it's been really lovely to see that you've been writing about all these different topics that we decided that we covered last week.
[00:02:45.000 --> 00:02:51.000]   So Ravi Mastroos has gone in and he's written about understanding cross entropy loss.
[00:02:51.000 --> 00:02:56.000]   So thanks, Ravi, for writing this blog. And the cherry on the top is Jeremy Howard retweeted,
[00:02:56.000 --> 00:03:03.000]   which means there's extra 1,000 followers who get this link, who get to see Ravi's awesome work.
[00:03:03.000 --> 00:03:07.000]   So well done, Ravi. This is amazing.
[00:03:07.000 --> 00:03:12.000]   Ravi Chandra has again come back this week and he's written about deep learning for coders
[00:03:12.000 --> 00:03:15.000]   and he's provided a good summary about his last week's learning vlogs.
[00:03:15.000 --> 00:03:20.000]   So thanks, Ravi Chandra, for writing again and coming to us week after week.
[00:03:20.000 --> 00:03:25.000]   I do hope that all of us continue with the same spirit every week.
[00:03:25.000 --> 00:03:30.000]   We have a new member this week who started writing just this week.
[00:03:30.000 --> 00:03:38.000]   He says he'll give writing a shot and wrote a comparison of writing the Pets classifier using Vanilla PyTorch and Fast.ai.
[00:03:38.000 --> 00:03:43.000]   I really love it, Akash. Really good blog. I read it and I quite enjoyed reading.
[00:03:43.000 --> 00:03:49.000]   And one of the things that even excited me is that you've used Fast.pages to set up the blog.
[00:03:49.000 --> 00:03:55.000]   So maybe share in the chat with everybody how easy it was to get set up with Fast.pages.
[00:03:55.000 --> 00:04:00.000]   It's really lovely to see new members. So welcome.
[00:04:00.000 --> 00:04:06.000]   Sai Amrit, he's come back another week and he's written about cross entropy loss.
[00:04:06.000 --> 00:04:12.000]   Anjali, I'm assuming you're sharing all these links. Unfortunately, I can't.
[00:04:12.000 --> 00:04:16.000]   So Sai Amrit, thank you for coming back this week and again writing about cross entropy.
[00:04:16.000 --> 00:04:24.000]   Really lovely blog. And you've also written about KL divergence, which has been one of those topics that has always, always confused me.
[00:04:24.000 --> 00:04:31.000]   So this is a great blog, actually. Vinayak's come back again this week and he's written about his week six summary.
[00:04:31.000 --> 00:04:40.000]   And he's also written about regular expressions. So I'm really excited that many of you have now started picking different areas that you want to write about.
[00:04:40.000 --> 00:04:44.000]   Somebody's picking up cross entropy. Somebody's writing about KL divergence.
[00:04:44.000 --> 00:04:49.000]   And somebody's picking up even summarizing chapter four or writing about rejects.
[00:04:49.000 --> 00:04:57.000]   So this is really, really inspiring for me. And it's really great to see so many blog posts this week.
[00:04:57.000 --> 00:05:06.000]   We have Korean. He's come back. He's one of our constants and he's come back this week again and he shared his log six learning fast book along with the study group.
[00:05:06.000 --> 00:05:13.000]   So thanks, Korean. I really love the visualizations and thanks for writing this week again.
[00:05:13.000 --> 00:05:20.000]   We have new member, Niazi. He's come back. Niazi's been writing about chapter four.
[00:05:20.000 --> 00:05:27.000]   So this is, again, welcome, Niazi, and hope you keep writing.
[00:05:27.000 --> 00:05:32.000]   Another new member, we have Anand. So Anand's written about chapter four again.
[00:05:32.000 --> 00:05:38.000]   So we've had people catching up with the fast with the study group, with the fast book study group.
[00:05:38.000 --> 00:05:45.000]   And it's really great to see that Jeremy's also kind of he's he's helping and he's retweeting some of your tweets.
[00:05:45.000 --> 00:05:50.000]   So more people get attention. And I've also been trying my level best that your work gets highlighted.
[00:05:50.000 --> 00:05:55.000]   So thanks, everybody, for writing this week.
[00:05:55.000 --> 00:06:03.000]   And so I'll just check comments a lot more this week. So if you have anything to say to me, because I don't have Zoom links.
[00:06:03.000 --> 00:06:13.000]   So, yeah, please continue writing your comments here. I would feel comfortable knowing that you're all there and you're still able to hear me.
[00:06:13.000 --> 00:06:23.000]   Okay. So this week, then, the next step, we kind of finished last week in chapter five, pet breeds.
[00:06:23.000 --> 00:06:29.000]   And we finished last week at model interpretation. So we're just going to get started from there.
[00:06:29.000 --> 00:06:35.000]   So we finished it. We finished last week at this point where we were able to create a confusion matrix.
[00:06:35.000 --> 00:06:42.000]   And we were also able to interpret the most confused. So we could see that our model, we would try to classify pet breeds.
[00:06:42.000 --> 00:06:48.000]   And we could see that a model was most confused between an American Bulldog and miniature Pinscher.
[00:06:48.000 --> 00:06:54.000]   And we can Google them and you can see how those images, how those breeds actually look very similar to each other.
[00:06:54.000 --> 00:07:01.000]   So that's that's satisfying to know that, you know, the model's done a good job at understanding things.
[00:07:01.000 --> 00:07:06.000]   Actually, these results are different because I didn't train the model very well. So don't look at these results.
[00:07:06.000 --> 00:07:15.000]   You would have different results in your terminals. Okay. So then this week, then, we're starting with improving our model.
[00:07:15.000 --> 00:07:23.000]   And we're going to basically pick up where we left off. So until now, we know that we can train a model.
[00:07:23.000 --> 00:07:28.000]   We can train our basic models. We can train we have good baselines, basically.
[00:07:28.000 --> 00:07:33.000]   But where do we go from here? How do we even make our models even better?
[00:07:33.000 --> 00:07:39.000]   So that's what we're going to start looking into today. And if time permits, so we're going to wrap up Chapter 5.
[00:07:39.000 --> 00:07:45.000]   And if there's still time left, I do hope to get started at least with multilabel classification.
[00:07:45.000 --> 00:07:58.000]   So that's our agenda for today is to wrap up Chapter 5. And then hopefully we can cover till binary cross entropy in multi classification in the next chapter.
[00:07:58.000 --> 00:08:04.000]   So improving a model. So what are the ways where we can improve the models?
[00:08:04.000 --> 00:08:16.000]   And one of the ways is a learning rate finder. So we kind of saw so let me bring up one node.
[00:08:16.000 --> 00:08:28.000]   Don't worry about this. Okay. So we kind of know that what is a learning rate we have.
[00:08:28.000 --> 00:08:36.000]   Basically, if our loss curve looks something like that, and we start out at a random point at this loss curve, right?
[00:08:36.000 --> 00:08:42.000]   And then what we do is we want to go towards the point with the minimum loss.
[00:08:42.000 --> 00:08:48.000]   And we start taking steps. Now, how big these steps are, that's what is kind of defined by the learning rate.
[00:08:48.000 --> 00:09:00.000]   And this is something we've looked at before. So the step in a way is P minus equal to P.grad star LR.
[00:09:00.000 --> 00:09:08.000]   Where this grad is the gradient, which we already know how to calculate just by using PyTorch backward propagation.
[00:09:08.000 --> 00:09:13.000]   And then this learning rate kind of tells us how big this step needs to be.
[00:09:13.000 --> 00:09:18.000]   So if this learning rate is too high, that means the steps are going to be too big.
[00:09:18.000 --> 00:09:24.000]   And in fact, we're going to kind of diverge at these different points. Our loss is going to get worse and worse.
[00:09:24.000 --> 00:09:31.000]   And if the learning rate is too small, then it's going to take us forever to get to the point of lowest loss.
[00:09:31.000 --> 00:09:40.000]   So that's just a quick refresher about learning rate. So learning rate is actually a very, very important hyperparameter that we need to worry about.
[00:09:40.000 --> 00:09:53.000]   So for that, Fast.ai folks already has a learning rate finder. So let's see how that works.
[00:09:53.000 --> 00:10:11.000]   So right now, we already had our data loaders. So these data loaders are the PETS data loaders. Sorry, one second.
[00:10:11.000 --> 00:10:17.000]   So this is just running same as last week.
[00:10:17.000 --> 00:10:23.000]   We saw what cross entropy loss is. So I'm just creating my data loaders to this point.
[00:10:23.000 --> 00:10:33.000]   And now, let me see if I need to zoom in a little bit. Is zooming in a little bit?
[00:10:33.000 --> 00:10:37.000]   I hope that's better.
[00:10:37.000 --> 00:10:51.000]   Okay. So now we can just pass in our data loaders and we can create our CNN learner. So whenever that will run.
[00:10:51.000 --> 00:10:59.000]   I'm just checking why it's taking a bit long. Okay. So the model stuck in training. That's what's taking long. That's fine.
[00:10:59.000 --> 00:11:06.000]   Okay. So then what is this learning rate finder? Basically, once you create your learner, once you've got your data loaders ready,
[00:11:06.000 --> 00:11:15.000]   and the process of getting the data loaders ready is you first create a data block, which is the easiest way to get your data loaders ready.
[00:11:15.000 --> 00:11:24.000]   So we've already looked at this last week. And once you have your data loaders ready, then you can pass your data loaders to the CNN learner object.
[00:11:24.000 --> 00:11:33.000]   And you can pass in the model that you want to train. So ResNet-34, if you don't know what a ResNet-34 is, it's just a type of a model.
[00:11:33.000 --> 00:11:44.000]   And if you're really interested about reading more about ResNet-34, then, of course, the easiest way to do that in a Jupyter Notebook is just to press question mark, question mark.
[00:11:44.000 --> 00:11:51.000]   So right now, I've just created I've passed in my data loaders. I've said I want to monitor the edit rate.
[00:11:51.000 --> 00:11:56.000]   So this is a human readable metric, which makes it really easy for us to interpret our models.
[00:11:56.000 --> 00:12:03.000]   And then I'm just going to fine tune my model at some really big base learning rate.
[00:12:03.000 --> 00:12:11.000]   So you can see how the error rate is really, really high. It's like 0.96. It's because I took my learning rate to be too high.
[00:12:11.000 --> 00:12:16.000]   So the question is, then, how do we know what is a good learning rate?
[00:12:16.000 --> 00:12:26.000]   One way could be that we can just guess a learning rate. We can try with 0.1. We can try with 0.2. We can try with 0.1. Sorry, 0.01.
[00:12:26.000 --> 00:12:31.000]   We can try with all these different numbers. And then we train all of these different models.
[00:12:31.000 --> 00:12:36.000]   And then we just pick the model that performs the best. So that's the learning rate. That's one way.
[00:12:36.000 --> 00:12:42.000]   But you can see how that means that we're wasting a lot of computation.
[00:12:42.000 --> 00:12:49.000]   So doing that is not actually computationally good because we're wasting a lot of resources.
[00:12:49.000 --> 00:12:52.000]   We're wasting a lot of time to find this one parameter.
[00:12:52.000 --> 00:13:02.000]   So something that has been only around in 2015 by a researcher called Leslie Smith has been this idea of a learning rate finder.
[00:13:02.000 --> 00:13:05.000]   And Fast.ai has implemented that learning rate finder.
[00:13:05.000 --> 00:13:12.000]   So any time, anywhere in Fast.ai when you have your learner object, you can call this method called LR find.
[00:13:12.000 --> 00:13:17.000]   So if I call that, it returns something that's a suggested LR.
[00:13:17.000 --> 00:13:24.000]   And then I could just say, okay, please tell me what's this suggested LR. And it will print my suggested point.
[00:13:24.000 --> 00:13:29.000]   So right now, for us, Fast.ai is trying to find what a good learning rate is.
[00:13:29.000 --> 00:13:40.000]   Basically what Fast.ai does underneath is that for each batch, you start with a small learning rate.
[00:13:40.000 --> 00:13:45.000]   And then you keep increasing it until the point your loss starts to get bad.
[00:13:45.000 --> 00:13:53.000]   So when you reach a point where your loss starts to get bad, that's when this learning rate finder kind of knows that's a bad learning rate.
[00:13:53.000 --> 00:13:57.000]   So see how we started with a really small learning rate at first.
[00:13:57.000 --> 00:14:03.000]   And then we continued to find we continued to increase our learning rate.
[00:14:03.000 --> 00:14:10.000]   And then we saw, okay, at this point, at a learning rate of one, basically, our loss has become really bad.
[00:14:10.000 --> 00:14:16.000]   So that's not a good loss curve. Let me just do it again. Let me run this again.
[00:14:16.000 --> 00:14:28.000]   I just want to show you a good loss curve that you can then see in real world use cases.
[00:14:28.000 --> 00:14:32.000]   So I'm just going to wait until that runs so I can explain what's going on.
[00:14:32.000 --> 00:14:35.000]   Okay. So that's a better looking loss curve.
[00:14:35.000 --> 00:14:39.000]   So basically what happens is you start out with a really, really small learning rate.
[00:14:39.000 --> 00:14:44.000]   And then as you're increasing your learning rate, your loss will start to decrease. Right?
[00:14:44.000 --> 00:14:55.000]   At this curve, we pretty much want to find a point where this decrease in loss is either the steepest or a little bit less than that.
[00:14:55.000 --> 00:14:59.000]   So this point that's suggested here is this 2.51 ENEG 5.
[00:14:59.000 --> 00:15:03.000]   So that's the point, this point here is the suggested LR.
[00:15:03.000 --> 00:15:08.000]   And we can really just pick this up.
[00:15:08.000 --> 00:15:12.000]   So I can just pick this learning rate. I can just put it in.
[00:15:12.000 --> 00:15:14.000]   And now I can train my model.
[00:15:14.000 --> 00:15:22.000]   So this way we know that the learning rate that we've picked up is a good learning rate.
[00:15:22.000 --> 00:15:25.000]   So that's basically one of the first steps that you can do.
[00:15:25.000 --> 00:15:31.000]   The first thing you want to do is you want to pick up a good learning rate so you can use the learning rate finder in Fast.ai.
[00:15:31.000 --> 00:15:33.000]   And then that will give you a suggested learning rate.
[00:15:33.000 --> 00:15:41.000]   And you can see how now the error is starting to basically going to reduce if we keep continuing training with this learning rate.
[00:15:41.000 --> 00:15:43.000]   So I'll wait for that to run.
[00:15:43.000 --> 00:15:46.000]   And when that's running, so that's the first step.
[00:15:46.000 --> 00:15:49.000]   So we've just trained a model based on that learning rate.
[00:15:49.000 --> 00:15:53.000]   The next step is unfreezing and transfer learning.
[00:15:53.000 --> 00:15:55.000]   So what is unfreezing and what is transfer learning?
[00:15:55.000 --> 00:16:01.000]   These are things I've kind of touched on in the past.
[00:16:01.000 --> 00:16:07.000]   So when you have your model, you have your deep learning model, we know that a deep learning model has many, many layers.
[00:16:07.000 --> 00:16:11.000]   So it's not like -- it's basically multiple layers in a deep learning model.
[00:16:11.000 --> 00:16:13.000]   So this is my earlier layers.
[00:16:13.000 --> 00:16:15.000]   I'm just going to call them L2, L3, and so on.
[00:16:15.000 --> 00:16:17.000]   I'm just going to call this L5.
[00:16:17.000 --> 00:16:19.000]   So these here are called earlier layers.
[00:16:19.000 --> 00:16:22.000]   And these here are called later layers.
[00:16:22.000 --> 00:16:31.000]   So if you remember, when we were looking at the earlier layers, we know that these earlier layers only find things like edges.
[00:16:31.000 --> 00:16:36.000]   Or they find things like -- so we've already visualized what these layers can learn.
[00:16:36.000 --> 00:16:41.000]   And that paper was in a chapter intro before.
[00:16:41.000 --> 00:16:43.000]   So that's back in chapter intros.
[00:16:43.000 --> 00:16:45.000]   That's the first chapter that we looked at.
[00:16:45.000 --> 00:16:53.000]   And we know that the earlier layers can only find things like edges or they can find diagonals or they can find horizontal edges or vertical edges.
[00:16:53.000 --> 00:16:55.000]   Or just basically very basic patterns.
[00:16:55.000 --> 00:17:02.000]   And it's not until the later layers that you start to find complex patterns like eyes or you start to find actually recognizing faces.
[00:17:02.000 --> 00:17:13.000]   So what happens is because this deep learning model has been trained on something called ImageNet, which is 1 million images of various different things,
[00:17:13.000 --> 00:17:17.000]   so we still want to keep the weights of these earlier layers.
[00:17:17.000 --> 00:17:26.000]   So we still want to keep the weights of these earlier layers.
[00:17:26.000 --> 00:17:36.000]   So what we do is -- and we want to discard the weights of these later layers.
[00:17:36.000 --> 00:17:40.000]   So what we do is we cut off this model at a point.
[00:17:40.000 --> 00:17:44.000]   So we keep the body.
[00:17:44.000 --> 00:17:51.000]   And we cut off the head.
[00:17:51.000 --> 00:17:54.000]   So we keep the body and we cut off the head.
[00:17:54.000 --> 00:18:03.000]   And basically what we do is we freeze the body.
[00:18:03.000 --> 00:18:05.000]   And we just train the head.
[00:18:05.000 --> 00:18:11.000]   So this is -- freezing means we're not going to train this part of the model.
[00:18:11.000 --> 00:18:13.000]   So we just train the head first.
[00:18:13.000 --> 00:18:21.000]   And what I mean by the head is because ImageNet was trained on 1,000 classes.
[00:18:21.000 --> 00:18:28.000]   And in pets, you have about 37 classes, which means your final layers are going to look a little bit different.
[00:18:28.000 --> 00:18:33.000]   So then what you do is you can cut off the points that classify into these 1,000 layers.
[00:18:33.000 --> 00:18:36.000]   So you can cut off all those weights because you don't need them.
[00:18:36.000 --> 00:18:40.000]   Because these layers are actually good at classifying things that are in ImageNet.
[00:18:40.000 --> 00:18:44.000]   But we want layers that are good at classifying pets.
[00:18:44.000 --> 00:18:49.000]   So that's the reason why we cut off and we still keep the body.
[00:18:49.000 --> 00:18:50.000]   So that's the first step.
[00:18:50.000 --> 00:18:55.000]   So you start with -- you freeze your body and you train your head at some learning rate.
[00:18:55.000 --> 00:18:58.000]   So now your new head can learn things.
[00:18:58.000 --> 00:19:13.000]   And then as you go forward in the next thing, what you can do is you can train both your body and head.
[00:19:13.000 --> 00:19:18.000]   So this is now you unfreeze the body.
[00:19:18.000 --> 00:19:21.000]   Unfreezing means you're going to now train the body.
[00:19:21.000 --> 00:19:24.000]   But you also train the body and now you also train the head.
[00:19:24.000 --> 00:19:27.000]   So that's the idea of unfreezing.
[00:19:27.000 --> 00:19:29.000]   So let's go back.
[00:19:29.000 --> 00:19:39.000]   By the way, this whole process of having a model that first trains on a bigger data set and then is training on the data set that you care about,
[00:19:39.000 --> 00:19:45.000]   in our case, this is the pets data set, this process is called fine-tuning.
[00:19:45.000 --> 00:19:51.000]   And this is the process of -- sorry, transfer learning.
[00:19:51.000 --> 00:20:05.000]   So remember we visualized all the layers from this Sila and Ferguson paper.
[00:20:05.000 --> 00:20:12.000]   Just going to wait for that to open for a second.
[00:20:12.000 --> 00:20:15.000]   Okay.
[00:20:15.000 --> 00:20:17.000]   There's a few questions.
[00:20:17.000 --> 00:20:23.000]   Does the learning rate finder restore the model weights after running the test?
[00:20:23.000 --> 00:20:27.000]   Because I guess at the end, the test, we've kind of messed up the parameters.
[00:20:27.000 --> 00:20:29.000]   No, it doesn't.
[00:20:29.000 --> 00:20:35.000]   So that's why we kind of -- after the learning rate finder, you start -- now you just create your learner again.
[00:20:35.000 --> 00:20:37.000]   This is just the fresh weights.
[00:20:37.000 --> 00:20:45.000]   And then you can train with the learning rate that's been suggested.
[00:20:45.000 --> 00:20:46.000]   Oh.
[00:20:46.000 --> 00:20:50.000]   Kevin said it restores the weights from before the LR.
[00:20:50.000 --> 00:20:51.000]   Okay.
[00:20:51.000 --> 00:20:52.000]   Thanks, Kevin.
[00:20:52.000 --> 00:20:53.000]   Maybe I'm wrong over here.
[00:20:53.000 --> 00:20:54.000]   I'll have to double check, actually.
[00:20:54.000 --> 00:21:05.000]   That's why I checked the comments, because I wasn't -- I was always under the understanding that it kind of had to be reset with a new CNN learner.
[00:21:05.000 --> 00:21:08.000]   I will get back to you on this, Ravi.
[00:21:08.000 --> 00:21:11.000]   LR should be at the bottom of the curve instead of that point.
[00:21:11.000 --> 00:21:12.000]   Okay.
[00:21:12.000 --> 00:21:17.000]   So I guess it's important to understand and interpret this chart.
[00:21:17.000 --> 00:21:23.000]   So if it's at the bottom, which is this point, after this point, that's when the loss starts getting worse.
[00:21:23.000 --> 00:21:34.000]   So let me copy/paste this in OneNote.
[00:21:34.000 --> 00:21:38.000]   So what point do you want to pick from this loss curve?
[00:21:38.000 --> 00:21:45.000]   So what's being suggested is we pick this point, which is the lowest, because that's the point where the loss is the lowest right now.
[00:21:45.000 --> 00:21:46.000]   But imagine this.
[00:21:46.000 --> 00:21:52.000]   Like at this point, at this learning rate, you don't really want to pick this learning rate, because it's already too high.
[00:21:52.000 --> 00:21:59.000]   Because if you go even a small bit more than this learning rate, then the loss is going to get -- start to get worse.
[00:21:59.000 --> 00:22:05.000]   So you actually want to pick a learning rate where the loss is decreasing at the maximum.
[00:22:05.000 --> 00:22:08.000]   What that means is at this point, the loss is not really decreasing.
[00:22:08.000 --> 00:22:10.000]   It's just reached a minimum point already.
[00:22:10.000 --> 00:22:14.000]   So it's important to understand this curve and interpret it in that way.
[00:22:14.000 --> 00:22:20.000]   You want to pick a point where the loss is decreasing and not the point where the loss has already decreased.
[00:22:20.000 --> 00:22:23.000]   Because this is the point that's an optimal point.
[00:22:23.000 --> 00:22:27.000]   Because that's where we know the loss is going to decrease if we train with that learning rate.
[00:22:27.000 --> 00:22:35.000]   But at this point, we know that if we train with this learning rate, it's already too high that the loss is going to start to get worse.
[00:22:35.000 --> 00:22:40.000]   So I hope that answers the question.
[00:22:40.000 --> 00:22:43.000]   Good question. Thanks for asking.
[00:22:43.000 --> 00:22:46.000]   Why are we not picking up the LR?
[00:22:46.000 --> 00:22:55.000]   Okay. Thanks, Rashmi. I hope that answers the point.
[00:22:55.000 --> 00:23:03.000]   I think Aman may need to rerun the suggested LR.valley sale to get an updated value.
[00:23:03.000 --> 00:23:07.000]   Did I miss something?
[00:23:07.000 --> 00:23:12.000]   Oh, okay.
[00:23:12.000 --> 00:23:16.000]   My bad.
[00:23:16.000 --> 00:23:21.000]   Thanks for picking that up, Kevin.
[00:23:21.000 --> 00:23:28.000]   Question from Vinayak. Does LRFinder test the loss for the same batch by changing the learning rates?
[00:23:28.000 --> 00:23:33.000]   Does one batch may not be able to have all the images if my targets are this?
[00:23:33.000 --> 00:23:38.000]   Can you please elaborate on how LRFinder works with respect to the above?
[00:23:38.000 --> 00:23:44.000]   So basically what LRFinder, as I understand, does is you know how you have a data set.
[00:23:44.000 --> 00:23:49.000]   So if I have a big enough data set, which is then divided into batches.
[00:23:49.000 --> 00:23:52.000]   So the first batch gets some learning rate.
[00:23:52.000 --> 00:23:54.000]   Then the second batch gets a bigger learning rate.
[00:23:54.000 --> 00:23:57.000]   Then the third batch gets an even bigger learning rate.
[00:23:57.000 --> 00:23:59.000]   And the fourth batch gets an even bigger learning rate.
[00:23:59.000 --> 00:24:05.000]   And you keep going all the way to the last batch or till the point that the loss really starts to get worse.
[00:24:05.000 --> 00:24:10.000]   So that's the point. So I think the question you're asking is, is this the same batch or are these different batches?
[00:24:10.000 --> 00:24:12.000]   So these are all different batches.
[00:24:12.000 --> 00:24:21.000]   You start from a smaller learning rate at this point and you go at a higher and higher learning rate at different batches.
[00:24:21.000 --> 00:24:28.000]   In the context of deep learning, is it wrong to define transfer learning as fine tuning the head of the model?
[00:24:28.000 --> 00:24:34.000]   It is not. I just wanted to stick with transfer learning as the term that's been mentioned in the book.
[00:24:34.000 --> 00:24:39.000]   But I don't think it's wrong to call it fine tuning because that's what we're kind of doing.
[00:24:39.000 --> 00:24:49.000]   But the process in then, there's no difference in my head between fine tuning because we're just fine tuning the model and the process is just called transfer learning.
[00:24:49.000 --> 00:24:53.000]   Okay. So with that being said, let's get back to the model.
[00:24:53.000 --> 00:24:57.000]   Oh, there we go. Choosing a good learning rate.
[00:24:57.000 --> 00:25:13.000]   I can see that now. Sorry, one second.
[00:25:13.000 --> 00:25:21.000]   Okay, this is where we were. So now we picked up a good learning rate, which is the suggested point was 1.45 in a three.
[00:25:21.000 --> 00:25:26.000]   And as you can see, training at that learning rate really makes our error rate come down.
[00:25:26.000 --> 00:25:33.000]   So that's a really good learning rate, which means if our error rate is 7%, that means our model is now 93% accurate.
[00:25:33.000 --> 00:25:39.000]   So that's the first thing. Whenever you start with a model, you want to try and pick up a good learning rate.
[00:25:39.000 --> 00:25:43.000]   For those that have been asking, why don't we pick up the point of a learning rate?
[00:25:43.000 --> 00:25:50.000]   I've already explained, but also a good experiment would be just rerun the cell with all these different points and see what you learn.
[00:25:50.000 --> 00:25:55.000]   So I think a good experiment would be at this point would be.
[00:25:55.000 --> 00:26:03.000]   Sorry, that's happened again. One second. A good experiment at this point would be just picking up all the different learning rates in the loss curve
[00:26:03.000 --> 00:26:10.000]   and then see how the loss is kind of responding or how the model is training, because you really want to get a feel of things.
[00:26:10.000 --> 00:26:15.000]   In deep learning, you really want to understand how the model behaves by trying out different experiments.
[00:26:15.000 --> 00:26:23.000]   That would make a really good experiment and a really good blog post idea is how each of these learning rates affect the model differently.
[00:26:23.000 --> 00:26:28.000]   So try picking up a really big one, try picking up a really small one. Train the really big one for two epochs.
[00:26:28.000 --> 00:26:36.000]   Train the really small one for 200 epochs. See if they kind of match the performance or what's happening.
[00:26:36.000 --> 00:26:41.000]   Basically, you want to improve your understanding of the learning rate.
[00:26:41.000 --> 00:26:46.000]   OK, so we were just looking at what's the point of transfer learning and what we're doing.
[00:26:46.000 --> 00:26:52.000]   So I was just bringing up this paper by Siler and Ferguson. So remember, we've already looked at this.
[00:26:52.000 --> 00:26:58.000]   The earlier layers kind of look at patterns that are like diagonals or circles.
[00:26:58.000 --> 00:27:04.000]   And then it's the later layers. Sorry, my laptop is playing a little bit today.
[00:27:04.000 --> 00:27:09.000]   So things are a bit slow. I think it's something's weird.
[00:27:09.000 --> 00:27:14.000]   But anyway, we'll continue. And the later layers are the ones that actually pick up the complex patterns.
[00:27:14.000 --> 00:27:19.000]   So that's why we want to keep the earlier layers because they've been trained on one million images.
[00:27:19.000 --> 00:27:26.000]   And the way of doing that. So we can just. So this is again, we're just starting with a model.
[00:27:26.000 --> 00:27:37.000]   And then what we want to do is we unfreeze. So what this unfreeze command in Fast.ai will do, as I've already explained, it will unfreeze my body.
[00:27:37.000 --> 00:27:41.000]   So we go from we've already done this bit. We go and we unfreeze.
[00:27:41.000 --> 00:27:47.000]   And this is what this is, the point that we reach where the body is unfrozen. We're training both the body and head.
[00:27:47.000 --> 00:27:56.000]   So this is what happens. And then when you call LR, learn.lr find at this point, see how this curve is different.
[00:27:56.000 --> 00:28:06.000]   Because this model has already been trained. So at this point, you want to pick up basically a learning rate where the loss curve is pretty much flat.
[00:28:06.000 --> 00:28:11.000]   So this is a trained model. So previously, when we looked at the loss curve, this was an untrained model.
[00:28:11.000 --> 00:28:15.000]   That's why we could see we have areas where the loss is decreasing.
[00:28:15.000 --> 00:28:21.000]   But now we've already reached a point where our loss is quite low because we've already trained a model.
[00:28:21.000 --> 00:28:27.000]   And now we're just unfreezing the body. At this point, you can see how this loss curve is different.
[00:28:27.000 --> 00:28:33.000]   And then you want to pick a point that is significantly lower, but you know that the loss is kind of a steady curve around.
[00:28:33.000 --> 00:28:40.000]   So these are the points where loss is minimum. Sorry, loss is low and it's steady.
[00:28:40.000 --> 00:28:45.000]   This is the point you want to pick in this learning curve. And then you can train your model again at that.
[00:28:45.000 --> 00:28:52.000]   Basically pick the point or that suggested LR. So in this case, you pick up a smaller number and you can pick up one ENECH4,
[00:28:52.000 --> 00:28:57.000]   which is basically a general rule of thumb that I follow is when I'm fine tuning my model,
[00:28:57.000 --> 00:29:02.000]   I make my learning rate 10 or 100 times smaller than the one I picked up.
[00:29:02.000 --> 00:29:09.000]   So in the last one, if we were going with. If my learning rate was 1.45 ENECH3,
[00:29:09.000 --> 00:29:15.000]   then a good learning rate that I'll pick up over here is just one ENECH4 or maybe even one ENECH5.
[00:29:15.000 --> 00:29:21.000]   So then you train your model again and you can see how the error rate reduces even further.
[00:29:21.000 --> 00:29:25.000]   So that's the main idea of freezing, unfreezing and transfer learning.
[00:29:25.000 --> 00:29:28.000]   So this is how you can make your models even better.
[00:29:28.000 --> 00:29:44.000]   The next thing I want to touch upon is. Sorry, guys, one second.
[00:29:44.000 --> 00:29:48.000]   I'm really sorry for that. I hope you can still hear and see me now.
[00:29:48.000 --> 00:29:52.000]   OK, so the next point we want to pick up is discriminative learning rates.
[00:29:52.000 --> 00:29:56.000]   That's the next way where we can improve our model.
[00:29:56.000 --> 00:29:59.000]   So what is discriminative learning rate?
[00:29:59.000 --> 00:30:11.000]   So as you know, our model is then divided into a body and a head and the head is the part that learns more complex patterns.
[00:30:11.000 --> 00:30:19.000]   And the body is the part that learns more general patterns like diagonals or it learns like horizontal edges or vertical edges.
[00:30:19.000 --> 00:30:26.000]   And as we go from earlier layers to later layers, the patterns it's learning become more and more complex.
[00:30:26.000 --> 00:30:34.000]   So what we really want to do is in most data sets, it would be really helpful still to know or in almost every data set,
[00:30:34.000 --> 00:30:40.000]   it would be really helpful to know what a vertical edge is or what a horizontal edge is.
[00:30:40.000 --> 00:30:44.000]   So it's that point, right? The really early layers.
[00:30:44.000 --> 00:30:49.000]   These are the ones that are like edges or vertical edges or like a circle or those things like that.
[00:30:49.000 --> 00:30:56.000]   So these, that's the very early layers. And then there's these intermediate layers or the layers in the middle.
[00:30:56.000 --> 00:31:03.000]   And then you have the layers at the end that learn really complex patterns like faces or like pet animals.
[00:31:03.000 --> 00:31:13.000]   So what you want to do is when you start from an ImageNet data set, this head, we already cut it off and it's been trained a little bit.
[00:31:13.000 --> 00:31:22.000]   But when you get to this point, when you unfreeze the body, you really want to train this body at a lower learning rate than the head.
[00:31:22.000 --> 00:31:29.000]   So you practically, because it's just going to train like edges, it's just going to find circles.
[00:31:29.000 --> 00:31:34.000]   We don't want to disturb those weights. We still think that's very useful.
[00:31:34.000 --> 00:31:37.000]   So we don't want to change those weights drastically.
[00:31:37.000 --> 00:31:44.000]   Whereas for the head, we want to change those weights because it started with something like ImageNet, which is a completely different data set.
[00:31:44.000 --> 00:31:49.000]   It's completely different classes. And we wanted now to recognize pets.
[00:31:49.000 --> 00:31:53.000]   So this is where all the complex patterns need to change.
[00:31:53.000 --> 00:32:01.000]   But this basic idea of like finding circles or edges, we still want that to stay because that model has been trained on one million images.
[00:32:01.000 --> 00:32:04.000]   So we still want to respect those weights. So how do you do that?
[00:32:04.000 --> 00:32:09.000]   One way is you have like a really small learning rate for this part.
[00:32:09.000 --> 00:32:13.000]   Then you have a slightly bigger learning rate for a different part of the model.
[00:32:13.000 --> 00:32:18.000]   And then you have an even bigger learning rate for the head.
[00:32:18.000 --> 00:32:26.000]   So your body has lower learning rate than the head.
[00:32:26.000 --> 00:32:29.000]   I hope this makes sense.
[00:32:29.000 --> 00:32:34.000]   If you have any questions, please put a comment on the report. I will check soon.
[00:32:34.000 --> 00:32:41.000]   But this idea of having like different learning rates in the model is called discriminative.
[00:32:41.000 --> 00:32:45.000]   I'll just show you actually. It's here. It's called discriminative learning rates.
[00:32:45.000 --> 00:32:50.000]   That's the idea. And how do you do that in Fast.ai?
[00:32:50.000 --> 00:32:54.000]   You can just pass in a slice.
[00:32:54.000 --> 00:32:59.000]   So see how I changed this. Actually, I was just doing some experiments.
[00:32:59.000 --> 00:33:04.000]   Don't worry about that at the top. This is the cell you want to worry about because that's what's in the book.
[00:33:04.000 --> 00:33:08.000]   So you first start with your model. You create a CNN learner.
[00:33:08.000 --> 00:33:12.000]   Right now, this model is a fresh model with new weights.
[00:33:12.000 --> 00:33:17.000]   So I can now call my LR find. I just want to show you the whole process at once.
[00:33:17.000 --> 00:33:26.000]   So I can call my LR find. Then say you train this model for two epochs based on a learning rate.
[00:33:26.000 --> 00:33:35.000]   Then you unfreeze the model. That's your next step. So I'm just saying this is my step zero.
[00:33:35.000 --> 00:33:42.000]   This is my step one. Find learning rate.
[00:33:42.000 --> 00:33:50.000]   This is my step two. Train frozen model. Train frozen body.
[00:33:50.000 --> 00:33:56.000]   Basically, oh sorry, train just the head. I'll just call it that.
[00:33:56.000 --> 00:34:02.000]   So the learning rate we pick up is somewhere around 1E neg 3. So let's pick up that learning rate, which is 3E neg 3.
[00:34:02.000 --> 00:34:10.000]   And we train that. Then you go step three is unfreeze the body.
[00:34:10.000 --> 00:34:18.000]   And finally, use discriminative learning rates.
[00:34:18.000 --> 00:34:24.000]   So what this means is when you're passing a slice to the learning rate in Fast.ai,
[00:34:24.000 --> 00:34:30.000]   what it's going to do is it's going to take 1E neg 6 as the learning rate.
[00:34:30.000 --> 00:34:35.000]   So 1E neg 6 is what goes here and 1E neg 3 is what goes here.
[00:34:35.000 --> 00:34:43.000]   And then everything else is in between. So that's this idea of using discriminative learning rates.
[00:34:43.000 --> 00:34:47.000]   So I just want to show you how things look like. So let's just this training finish.
[00:34:47.000 --> 00:34:58.000]   I just want to show you all the steps together. So when you go back this week, you can actually fine tune your models really well.
[00:34:58.000 --> 00:35:04.000]   So you can see how we are at 0.07 error rate, which means 93 percent accurate. So I unfreeze my model.
[00:35:04.000 --> 00:35:11.000]   And then let's call lrfind again. So now this is a little bit of this. This model's been trained, right?
[00:35:11.000 --> 00:35:18.000]   We just chopped off the head and this model's been trained. So now I can call learn.lrfind.
[00:35:18.000 --> 00:35:25.000]   And we'll see how that loss curve is very different from the one before for the untrained model.
[00:35:25.000 --> 00:35:30.000]   Then it still suggests me a point and we pick that point up.
[00:35:30.000 --> 00:35:35.000]   So it's saying that this is the point which means around 1E neg 4. So that's what goes here.
[00:35:35.000 --> 00:35:38.000]   Then you divide that by 100 pretty much, which is your earliest layer.
[00:35:38.000 --> 00:35:43.000]   So you provide in a slice and then that you fit for even longer.
[00:35:43.000 --> 00:35:50.000]   And you can see how this will really work. And it will really make your model train for discriminative learning rates.
[00:35:50.000 --> 00:35:55.000]   So that's the next idea. And then you can pretty much just plot your losses.
[00:35:55.000 --> 00:36:00.000]   And I will do that when that runs. That's going to take some time to run.
[00:36:00.000 --> 00:36:10.000]   So we have time to look at questions for discriminative learning rates.
[00:36:10.000 --> 00:36:13.000]   All right. We finished last here.
[00:36:13.000 --> 00:36:20.000]   Is it the same concept that LR rate is different for different batches and different layers?
[00:36:20.000 --> 00:36:27.000]   No, I think. Yeah. Thanks, Kevin. So I won't go into LR scheduling.
[00:36:27.000 --> 00:36:31.000]   But Durga, it's different. It's both are very different.
[00:36:31.000 --> 00:36:42.000]   OK. So you divide your model into like these three parts.
[00:36:42.000 --> 00:36:49.000]   Your earlier layers have, say, 1E neg 6 and then your later layers have 1E neg 3.
[00:36:49.000 --> 00:36:59.000]   This is called discriminative learning rate. Then what you do ideally is you train this for, say, 10 epochs.
[00:36:59.000 --> 00:37:04.000]   Right now, your model has trained for 10 epochs.
[00:37:04.000 --> 00:37:12.000]   Then what most practitioners do is.
[00:37:12.000 --> 00:37:16.000]   After 10 epochs, you reduce the learning rate even more.
[00:37:16.000 --> 00:37:24.000]   So you go, say, your earliest layer goes 1E neg 8 and this goes 1E neg 5.
[00:37:24.000 --> 00:37:28.000]   So you reduce the learning rate even further. So that's a completely different idea.
[00:37:28.000 --> 00:37:31.000]   That's called LR scheduling, as Kevin's already mentioned.
[00:37:31.000 --> 00:37:36.000]   But the idea is that you keep reducing your learning rate further and further.
[00:37:36.000 --> 00:37:41.000]   So this is something I touched upon. Actually, there's different kinds of schedulers.
[00:37:41.000 --> 00:37:52.000]   So if you go Torch Optim scheduler, you'll find a whole list that will show you how to adjust the learning rate.
[00:37:52.000 --> 00:37:57.000]   So you will see how there's all these different schedulers, Lambda LR, Multiplicative LR.
[00:37:57.000 --> 00:38:02.000]   Actually, FastAI has a really good list of LR schedulers.
[00:38:02.000 --> 00:38:09.000]   Let me see if I can bring that up. Here it is.
[00:38:09.000 --> 00:38:15.000]   There you go. So there's all these different ways you can change your learning rate at every epoch.
[00:38:15.000 --> 00:38:21.000]   So there's like cosine scheduling, polynomial scheduling, and they can combine different schedulers.
[00:38:21.000 --> 00:38:27.000]   But that's a different concept. We're not talking about this right now.
[00:38:27.000 --> 00:38:33.000]   Block Perth showing different LR metrics. Let's have a look. Oh, it's Noveta. It's one by Zach.
[00:38:33.000 --> 00:38:40.000]   Did Zach write this? I'd be curious if he did. He did. It is by Zach Miller.
[00:38:40.000 --> 00:38:47.000]   Thanks for sharing. Could you please also share this, Angelica, on the YouTube live chat with everybody?
[00:38:47.000 --> 00:38:52.000]   Thanks for pointing out the resource, Alan.
[00:38:52.000 --> 00:38:57.000]   When we unfreeze the model, are we training with the complete network? Yes.
[00:38:57.000 --> 00:39:00.000]   No, there's no question that's a bad question. We're still training the whole network.
[00:39:00.000 --> 00:39:03.000]   It's just that the learning rates are different for different parts of the model.
[00:39:03.000 --> 00:39:06.000]   That's why it's called discriminative learning rate.
[00:39:06.000 --> 00:39:10.000]   So even when we unfreeze, we're still training the model.
[00:39:10.000 --> 00:39:14.000]   That's actually training the whole model is what is called unfreezing.
[00:39:14.000 --> 00:39:23.000]   Because when you have your weights as frozen, that's when you're not training the whole model.
[00:39:23.000 --> 00:39:30.000]   I hope that made sense. How to discriminate these parts of the model.
[00:39:30.000 --> 00:39:32.000]   You don't have to worry about discriminating these parts.
[00:39:32.000 --> 00:39:37.000]   When you pass to Fast.ai, when you pass a learning rate that's a slice,
[00:39:37.000 --> 00:39:42.000]   Fast.ai will already know that you're trying to discriminate the learning rates for different parts of the models.
[00:39:42.000 --> 00:39:45.000]   And internally, it will break the model into chunks.
[00:39:45.000 --> 00:39:50.000]   And it will train each of these different parts of the model for different learning rates.
[00:39:50.000 --> 00:39:53.000]   So this is not something you have to worry about.
[00:39:53.000 --> 00:39:57.000]   So you can see how now training by different discriminative learning rates,
[00:39:57.000 --> 00:40:05.000]   we are able to get about 90 or about 94.2% accuracy because the error rate has really, really reduced.
[00:40:05.000 --> 00:40:09.000]   And then when that finishes, how many epochs are we at? Ten.
[00:40:09.000 --> 00:40:12.000]   So it's just about to finish. It's the last one.
[00:40:12.000 --> 00:40:18.000]   Excellent. And then we can plot the loss and we can see how the train loss is reducing, how the valid loss is reducing.
[00:40:18.000 --> 00:40:23.000]   But this is the point you want to get to where the valid loss is kind of steady.
[00:40:23.000 --> 00:40:29.000]   Because if the validation loss is still reducing, which means you still continue to train your model,
[00:40:29.000 --> 00:40:34.000]   which actually brings me to this point of selecting the number of epochs.
[00:40:34.000 --> 00:40:36.000]   So how do you select the number of epochs?
[00:40:36.000 --> 00:40:42.000]   So that's another different hyperparameter is like how many epochs do you train for?
[00:40:42.000 --> 00:40:49.000]   Like we trained for 12 epochs over here and we trained for three epochs over here.
[00:40:49.000 --> 00:40:53.000]   So how do we know how many epochs we want to train for?
[00:40:53.000 --> 00:41:00.000]   A good rule of thumb is if your loss is still decreasing, not your train loss, but your validation,
[00:41:00.000 --> 00:41:03.000]   because your train loss is always going to decrease.
[00:41:03.000 --> 00:41:06.000]   Remember this guys, train and validation are two very different things.
[00:41:06.000 --> 00:41:15.000]   When you have your whole data set, this is my whole data set, you divide it into train and validation.
[00:41:15.000 --> 00:41:25.000]   The model only sees this part and then we validate the model on the validation set.
[00:41:25.000 --> 00:41:29.000]   It never sees it. It has never seen that part of the data.
[00:41:29.000 --> 00:41:34.000]   We just test the model. We just make the model do predictions on some part of the data,
[00:41:34.000 --> 00:41:39.000]   which could be say 20% or 10% depending on different data sets.
[00:41:39.000 --> 00:41:46.000]   You just make predictions on this validation data set and then you test your model and see how good your model is performing.
[00:41:46.000 --> 00:41:52.000]   So what we really care about when training and validation loss, if we plot the training and validation loss
[00:41:52.000 --> 00:41:57.000]   and say the training curve, because the training curve is always going to decrease,
[00:41:57.000 --> 00:42:00.000]   we say that's the training curve and then that's the validation curve,
[00:42:00.000 --> 00:42:04.000]   because that's what you'll see commonly will happen.
[00:42:04.000 --> 00:42:09.000]   At this point onward, the model is starting to overfit.
[00:42:09.000 --> 00:42:18.000]   I guess right now what I'm trying to do is I'm trying to help you with the intuition of all these different loss curves,
[00:42:18.000 --> 00:42:22.000]   all these different train and validation loss curves that you'll see.
[00:42:22.000 --> 00:42:28.000]   If you see something like the validation loss is decreasing and then it starts to increase,
[00:42:28.000 --> 00:42:36.000]   but your train loss is continuing to decrease, what that means is your model is starting to memorize your training data.
[00:42:36.000 --> 00:42:39.000]   What does it mean that it's starting to memorize your training data?
[00:42:39.000 --> 00:42:44.000]   It means that it's starting to capture not the general patterns, but it's starting to capture,
[00:42:44.000 --> 00:42:48.000]   okay, fur is brown fur or like cat fur, whatever my training data is,
[00:42:48.000 --> 00:42:53.000]   it's starting to capture those things rather than, okay, fur should just be like,
[00:42:53.000 --> 00:42:58.000]   just basically a general pattern about fur, which can be generalized to different images.
[00:42:58.000 --> 00:43:00.000]   So we want the model to generalize.
[00:43:00.000 --> 00:43:06.000]   And when you see a curve like this, where the validation loss is first decreasing and then it starts to increase,
[00:43:06.000 --> 00:43:09.000]   you know that at this point onward, your model is starting to overfit.
[00:43:09.000 --> 00:43:18.000]   So you pretty much retrain your model and you pick up a model where the validation loss is not going any further down.
[00:43:18.000 --> 00:43:22.000]   So you keep training until that point that you've reached a point.
[00:43:22.000 --> 00:43:26.000]   So because you know that after this point, the model is going to get worse,
[00:43:26.000 --> 00:43:29.000]   so you can retrain the model for however many epochs.
[00:43:29.000 --> 00:43:34.000]   So you say first you train for 100 epochs and then you train for 100 epochs.
[00:43:34.000 --> 00:43:38.000]   And you saw that after the 80th epoch, my model is starting to get worse.
[00:43:38.000 --> 00:43:43.000]   So what do you do the next time is you train only for 80 epochs.
[00:43:43.000 --> 00:43:48.000]   And then now you know that your model is not going to get any worse and it's just going to not overfit.
[00:43:48.000 --> 00:43:52.000]   So that's how you pick the number of epochs.
[00:43:52.000 --> 00:43:55.000]   So that's another section.
[00:43:55.000 --> 00:44:04.000]   Which is that. So that's how you can even further improve the performance of your models is by picking up a good number of epochs.
[00:44:04.000 --> 00:44:09.000]   The last thing you can do is you pick up deeper architectures.
[00:44:09.000 --> 00:44:13.000]   So actually, let me just quickly go into questions after.
[00:44:13.000 --> 00:44:17.000]   This is the last section of this chapter.
[00:44:17.000 --> 00:44:20.000]   After this, we'll move into.
[00:44:20.000 --> 00:44:24.000]   Chapter six of the categories.
[00:44:24.000 --> 00:44:27.000]   So it looks like there's no questions. Excellent.
[00:44:27.000 --> 00:44:32.000]   I'm just going to assume everybody understands everything until now.
[00:44:32.000 --> 00:44:39.000]   So the next thing we're doing is we can do is until now we've picked up a ResNet-34.
[00:44:39.000 --> 00:44:45.000]   Until before this, what you can do, you can just pick up a deeper architecture with more number of layers.
[00:44:45.000 --> 00:44:52.000]   What that means is that a deeper architecture has a bigger capacity, which means it can learn things better.
[00:44:52.000 --> 00:45:01.000]   But beware. The fact that a bigger model can learn things better kind of means that a bigger model can overfit quickly.
[00:45:01.000 --> 00:45:05.000]   So you have to really understand what I'm trying to say here. I'll repeat one more time.
[00:45:05.000 --> 00:45:14.000]   The fact that a bigger model can learn things better means that a bigger model can overfit more than a smaller model.
[00:45:14.000 --> 00:45:23.000]   What that means is. If I have a bigger model that I'm training on my training set and I train it for really longer,
[00:45:23.000 --> 00:45:27.000]   then it will start to memorize the training data a lot more quickly than a smaller model.
[00:45:27.000 --> 00:45:31.000]   So I'm just going to call this a smaller model M2 and this is my model one.
[00:45:31.000 --> 00:45:35.000]   So it will take a lot longer for this smaller model to start memorizing.
[00:45:35.000 --> 00:45:38.000]   So that's another thing you have to be careful about.
[00:45:38.000 --> 00:45:45.000]   Is when you when you pick up bigger architectures, but generally you will see that when you pick up bigger architectures,
[00:45:45.000 --> 00:45:52.000]   you will see that the performance will be better. So if I train with this bigger architecture instead of ResNet-34,
[00:45:52.000 --> 00:46:02.000]   which only got me to point 0. It got me to the best error rate is around point five eight or point six,
[00:46:02.000 --> 00:46:06.000]   which means around 94 percent accuracy or ninety four point two.
[00:46:06.000 --> 00:46:13.000]   You will see that when we pick a bigger model like ResNet-50, we can actually get a better performance.
[00:46:13.000 --> 00:46:19.000]   So it's like 95 percent. But something you have to notice is now we're training for less epochs.
[00:46:19.000 --> 00:46:25.000]   If I trained for more epochs, if we're not even training for 12 epochs anymore, we're just training for six epochs.
[00:46:25.000 --> 00:46:30.000]   So it's like this whole balance of epochs, model size, image size, all these different things that you have to worry about.
[00:46:30.000 --> 00:46:37.000]   And you only become a better practitioner as you keep going week by week, as you try different experiments,
[00:46:37.000 --> 00:46:46.000]   as you train all of these different models. But those are the main things. And that's chapter five.
[00:46:46.000 --> 00:46:53.000]   So just as a summary today, right now, we've looked at learning rate finder.
[00:46:53.000 --> 00:46:59.000]   We saw what unfreezing and transfer learning means. We saw what discriminative learning rates are,
[00:46:59.000 --> 00:47:02.000]   which means it's just different learning rates for different parts of the model.
[00:47:02.000 --> 00:47:11.000]   We saw how you want to pick up the number of epochs. So you want to pick up a point where your validation loss doesn't overfit
[00:47:11.000 --> 00:47:16.000]   or basically your model doesn't overfit. Otherwise, the number of epochs is too high.
[00:47:16.000 --> 00:47:23.000]   Or if you pick up less number of epochs, that means the number of epochs is too low because your validation loss is still decreasing.
[00:47:23.000 --> 00:47:30.000]   And then we saw that even picking deeper architectures can help improve the model.
[00:47:30.000 --> 00:47:43.000]   So as something that you should do or basically what I follow, a common practice for me is first create a simple baseline.
[00:47:43.000 --> 00:47:49.000]   So follow everything until before this point of improving our model. So you start with a smaller model.
[00:47:49.000 --> 00:47:55.000]   You start. You don't do anything. There's no fine tuning. You just pick up a random learning rate or like a small learning rate,
[00:47:55.000 --> 00:48:03.000]   like 1-E-neg-3 or 1-E-neg-4 that you know is not too high, not too small. You pick up with that and you train your model and you get a baseline.
[00:48:03.000 --> 00:48:08.000]   What that means is that's your baseline. You're only going to get better from that point on.
[00:48:08.000 --> 00:48:21.000]   Then you try all of these different tricks and that's how you see, OK, I now fine tune my learning rate or basically I pick up a learning rate that's more suitable.
[00:48:21.000 --> 00:48:26.000]   I fine tune my hyperparameter, which means I'm picking up a learning rate that's more suitable.
[00:48:26.000 --> 00:48:29.000]   I'm picking up the number of epochs that's more suitable for my dataset.
[00:48:29.000 --> 00:48:34.000]   I'm trying all of these different methods and then you compare the performance of the model.
[00:48:34.000 --> 00:48:37.000]   Ideally, it should be much better than your baseline.
[00:48:37.000 --> 00:48:44.000]   But it's important to know the baseline because say you picked up the number of epochs as 100 instead of 60,
[00:48:44.000 --> 00:48:49.000]   which were the optimal number of epochs, then the performance is going to be worse than the baseline.
[00:48:49.000 --> 00:48:56.000]   So you need to be able to compare. You need to have a set of standards that you can't go below that.
[00:48:56.000 --> 00:49:00.000]   So that's why you need a baseline. And that's a small process that I follow.
[00:49:00.000 --> 00:49:10.000]   And you can maybe write about, like in terms of blog post ideas for this section of the chapter, write about learning rate finder.
[00:49:10.000 --> 00:49:16.000]   Try different learning rates and experiment with different learning rates in different parts of the batches and see how the model behaves,
[00:49:16.000 --> 00:49:20.000]   how you have to train for different number of epochs. What if you pick up a really small learning rate?
[00:49:20.000 --> 00:49:27.000]   Plot the loss curves. So you plot these train and validation curves for all these different hyper parameters.
[00:49:27.000 --> 00:49:31.000]   How does it look like for a deeper architecture? How does it look like for a smaller architecture?
[00:49:31.000 --> 00:49:36.000]   How does it look like if you don't do discriminated learning rates? How does it look like for a fine-tuned model?
[00:49:36.000 --> 00:49:39.000]   So these are all these things. These are all insights that you're going to get.
[00:49:39.000 --> 00:49:45.000]   And trust me, these insights are going to really help because people don't notice these insights.
[00:49:45.000 --> 00:49:51.000]   And when you share them as a blog post to the world, it's going to be good content that you end up writing.
[00:49:51.000 --> 00:50:00.000]   So that's Chapter 5. I'll just go quickly to the reports and see if there's any questions.
[00:50:00.000 --> 00:50:09.000]   If you're training a model and see that we've overshot the minimum validation loss, that it starts increasing again.
[00:50:09.000 --> 00:50:16.000]   Do we generally restart the training from scratch and reset the number of epochs to a low number?
[00:50:16.000 --> 00:50:22.000]   Or somehow we can rewind? OK. There's different ideas about this.
[00:50:22.000 --> 00:50:29.000]   So let me give you a bit of a feedback, a bit of a history.
[00:50:29.000 --> 00:50:34.000]   So say your loss is always going to decrease. There's never going to be a point that you'll see.
[00:50:34.000 --> 00:50:41.000]   So I'm not going to worry about the loss curve. But let's say for the validation loss, it decreases and then it's going to increase.
[00:50:41.000 --> 00:50:53.000]   So I guess the question is. At this point onwards, because you're always updating the model, remember, a model has weights, right?
[00:50:53.000 --> 00:51:00.000]   So a model has these weights. So at this point onward, you've already made the weights of the model worse.
[00:51:00.000 --> 00:51:06.000]   So because your model is going to train all the way, which means the final weights that the model is going to get.
[00:51:06.000 --> 00:51:12.000]   Let's say these are different epochs. So let's say this is the 0th epoch. Let's say this is the 60th epoch.
[00:51:12.000 --> 00:51:19.000]   And let's say this is the 100th epoch. And because you train for 100 epochs, which means your model weights are really bad.
[00:51:19.000 --> 00:51:29.000]   So you can't really use them. So unless you had some way of saving the weights or you saved the weights at the 60th epoch,
[00:51:29.000 --> 00:51:36.000]   then you could pretty much just reload those weights at the 60th epoch and you could use them.
[00:51:36.000 --> 00:51:42.000]   But generally what I found in practice, because you have all these different learning rate schedulers,
[00:51:42.000 --> 00:51:48.000]   you have all these different things that you're doing and FastAI kind of automatically does them for you.
[00:51:48.000 --> 00:51:56.000]   It makes really smart decisions for you. Typically, what has helped for me is to retrain the model only for 60 epochs.
[00:51:56.000 --> 00:52:02.000]   I think you'll find that at this point, the loss will be lower than whatever the loss at this point was.
[00:52:02.000 --> 00:52:11.000]   But that's what I found from practice. But feel free to save the model weights at all these different epochs and see if that's actually the case.
[00:52:11.000 --> 00:52:21.000]   But at least what I found in practice is retraining is the best strategy forward. So I hope that answers the question.
[00:52:21.000 --> 00:52:34.000]   OK, with that being said, let's move to multi-label classification.
[00:52:34.000 --> 00:52:42.000]   I just need a minute. One second.
[00:52:42.000 --> 00:52:47.000]   All right. So multi-label classification, then.
[00:52:47.000 --> 00:52:53.000]   It's very different from what we've done so far. So let's forget everything that we've learned.
[00:52:53.000 --> 00:53:07.000]   Not forget, but kind of like whatever we've done till now is we basically had, say, zero to thousand images.
[00:53:07.000 --> 00:53:16.000]   So let's say those are my thousand images. Then each image had a label like this was maybe that of a cat.
[00:53:16.000 --> 00:53:25.000]   This was a dog. It was a horse and so on. So each image only belonged to one category.
[00:53:25.000 --> 00:53:31.000]   But what if there are images that belong that have more labels in them?
[00:53:31.000 --> 00:53:41.000]   What that means is, let me show you some examples. But basically an image could have a piano and a chair and a sofa and all of these more labels.
[00:53:41.000 --> 00:53:47.000]   It's not always that in every image you just find one object or you'll only find one category.
[00:53:47.000 --> 00:53:55.000]   It's very possible that a single image has multiple categories. So how do you train that sort of model?
[00:53:55.000 --> 00:54:02.000]   The good news is there's not a lot of things that you have to do differently thanks to the way Fast.ai is built.
[00:54:02.000 --> 00:54:10.000]   There's not a lot of things that you have to do differently. And you don't really have to learn a lot of new things, which is amazing.
[00:54:10.000 --> 00:54:18.000]   So that's the good news. There's no bad news, by the way. The only thing is we have to update the loss curve.
[00:54:18.000 --> 00:54:27.000]   We have to update the loss function. So those are things that we will learn as part of this chapter, as part of multi-label classification.
[00:54:27.000 --> 00:54:34.000]   So let's see what the kind of data looks like. So there's a data called Pascal 2007.
[00:54:34.000 --> 00:54:42.000]   So until now, we've been using like ImageNet or ImageNet data sets or smaller, which have each image has one category.
[00:54:42.000 --> 00:54:53.000]   So if you search Pascal 2007, you'll see that's a data set, which is a visual object detection classes,
[00:54:53.000 --> 00:54:59.000]   which basically means then a single image can have multiple objects.
[00:54:59.000 --> 00:55:05.000]   Like this can have two different pet breeds or this can have a train, this can have a chair, this can have sofa.
[00:55:05.000 --> 00:55:14.000]   So you have multiple categories. So let's download that. I guess the first thing is to import Fastbook.
[00:55:14.000 --> 00:55:20.000]   Let me restart that. And we just import everything and we untar.
[00:55:20.000 --> 00:55:27.000]   So you remember what untar data did? Untar data, this is URLs that Pascal 2007 is just a URL,
[00:55:27.000 --> 00:55:31.000]   which means that this data set sits at this place in the Internet.
[00:55:31.000 --> 00:55:37.000]   And what untar data will do is it will download it and it will provide us with a path where this data sits.
[00:55:37.000 --> 00:55:44.000]   So now if I go path.ls, I can see how there's all of these different things that have been downloaded for me.
[00:55:44.000 --> 00:55:52.000]   One of the things that has been downloaded is also a train.csv. So I can use a library called pandas or pandas.
[00:55:52.000 --> 00:55:59.000]   And we can read the CSV. So if you're new to what a CSV is, it's comma separated values,
[00:55:59.000 --> 00:56:05.000]   or you're new to pandas, in the next sidebar, there's really good.
[00:56:05.000 --> 00:56:09.000]   This is basically just telling you what a data frame is.
[00:56:09.000 --> 00:56:15.000]   And then there's this really good book called Python for Data Analysis, which will it's by O'Reilly.
[00:56:15.000 --> 00:56:20.000]   And it will teach you everything there is to know about the pandas library.
[00:56:20.000 --> 00:56:26.000]   Or you could also check out pandas documentation.
[00:56:26.000 --> 00:56:29.000]   This will really come in handy because you need to know pandas.
[00:56:29.000 --> 00:56:32.000]   It will really, really help and boost your data science careers.
[00:56:32.000 --> 00:56:36.000]   There's a getting started guides, there's API reference.
[00:56:36.000 --> 00:56:42.000]   So you could go getting started. And then this will have lots of tutorials on how you can work with tabular data, basically.
[00:56:42.000 --> 00:56:50.000]   So this kind of data set where, you know, like an Excel sheet, when you have a data, which is the most common kind of data,
[00:56:50.000 --> 00:56:53.000]   you'll come across in all of these different companies.
[00:56:53.000 --> 00:56:56.000]   When you have that kind of data set, it's called tabular data.
[00:56:56.000 --> 00:57:04.000]   And the library in Python that's the most followed to work with this kind of data set, which is tabular data, is pandas.
[00:57:04.000 --> 00:57:08.000]   So pandas and NumPy are two libraries that go really well together.
[00:57:08.000 --> 00:57:13.000]   But for a background, refer to that book, refer to the documentation.
[00:57:13.000 --> 00:57:18.000]   It could make a good blog post if you do end up writing about pandas or like all of these different operations,
[00:57:18.000 --> 00:57:22.000]   or you ended up writing your own tutorials. That's something you could also do.
[00:57:22.000 --> 00:57:26.000]   But that's not part of what we're going to learn today.
[00:57:26.000 --> 00:57:29.000]   We're just going to see how to work with this.
[00:57:29.000 --> 00:57:38.000]   So see how I have these different images, but this image, which is 9.jpg, has two labels.
[00:57:38.000 --> 00:57:42.000]   It has a horse and a person. So like this is what multi-category is.
[00:57:42.000 --> 00:57:45.000]   This is how it is different from the past data set.
[00:57:45.000 --> 00:57:48.000]   Like there's more labels per row.
[00:57:48.000 --> 00:57:52.000]   So then in that case, what do you do? How do you construct a data block?
[00:57:52.000 --> 00:57:55.000]   Remember, the first thing you want to do is you construct your data loader.
[00:57:55.000 --> 00:58:00.000]   Sorry, you construct your data block. Then you construct your data loader, which is the easiest way in Fast.ai.
[00:58:00.000 --> 00:58:05.000]   And then you construct your data loader. You pass that to the model, and then you can train the model.
[00:58:05.000 --> 00:58:10.000]   These are the four steps that you will see are constant for doesn't matter what kind of task you're doing,
[00:58:10.000 --> 00:58:14.000]   whether you're doing natural language processing, whether you're doing multi-label classification.
[00:58:14.000 --> 00:58:20.000]   These are the kind of steps you'll see. So we just start with a very empty data block.
[00:58:20.000 --> 00:58:26.000]   And then I can call this data sets function, passing in my data frame.
[00:58:26.000 --> 00:58:34.000]   So my data frame, remember, is just this table of data, which has three columns, file name, labels, and is valid.
[00:58:34.000 --> 00:58:41.000]   Don't worry about how these react or what happens, but I can just know that I can create my data set.
[00:58:41.000 --> 00:58:48.000]   And then what that will do is, remember, data block is going to split these files into parts on its own.
[00:58:48.000 --> 00:58:53.000]   So the first part is called train and the second part is called valid.
[00:58:53.000 --> 00:59:01.000]   So you can see how train has about 4000 items and valid has about 1000 items.
[00:59:01.000 --> 00:59:07.000]   This is something you we've already looked at what data sets, data loaders and what everything is.
[00:59:07.000 --> 00:59:15.000]   But basically, a data set returns two things. It returns X and Y.
[00:59:15.000 --> 00:59:23.000]   So your Y is the label. Your X is either if you're doing image classification, it's image.
[00:59:23.000 --> 00:59:32.000]   If you're doing test classification, it's text. If you're doing basically table of data, I mean, this is the part that you use to predict the label.
[00:59:32.000 --> 00:59:39.000]   So in this case, then, this is what a data set returns. It returns these two items.
[00:59:39.000 --> 00:59:44.000]   So anything that can return these two things, you can think of that as a data set.
[00:59:44.000 --> 00:59:54.000]   So I can just go and I can have a look at my train. So you can see how the train, remember, had 4009 items.
[00:59:54.000 --> 00:59:58.000]   I can just grab the first item from that train and I can call it X and Y.
[00:59:58.000 --> 01:00:04.000]   And if you see what it is returning, it's actually returning the same thing.
[01:00:04.000 --> 01:00:11.000]   It's returning an F name of this. It's returning the label and it's returning an is valid.
[01:00:11.000 --> 01:00:18.000]   And these are the two same things. So what's happening underneath in Fast.ai, when you called this function,
[01:00:18.000 --> 01:00:22.000]   you didn't really tell Fast.ai how to split this, what to do with this.
[01:00:22.000 --> 01:00:28.000]   You just passed in an item. And then Fast.ai, what it does is internally, because the data set has to return two things,
[01:00:28.000 --> 01:00:33.000]   because it doesn't know how to make your X or it doesn't know how to make your Y.
[01:00:33.000 --> 01:00:38.000]   It returns both of them as the same. That's what Fast.ai does.
[01:00:38.000 --> 01:00:46.000]   But what we really care about, remember, for the X, we care about this file name, because then that's the image.
[01:00:46.000 --> 01:00:50.000]   That's where the image sits. Right. So for my X, I actually want an image.
[01:00:50.000 --> 01:00:55.000]   So I could actually go in and I could say my X, I could say the file name.
[01:00:55.000 --> 01:01:01.000]   This is again, I'm just using pandas or I'm just kind of this is just a row of a pandas data frame.
[01:01:01.000 --> 01:01:05.000]   So I can just go X file name, which returns the file name to me.
[01:01:05.000 --> 01:01:11.000]   And I can just say for my Y, I can just I care about the label. Right.
[01:01:11.000 --> 01:01:16.000]   So right now, these X and Y are the same thing. They're the same row.
[01:01:16.000 --> 01:01:22.000]   But I can extract the important information that I need. So for my X, I need the file name for my Y.
[01:01:22.000 --> 01:01:28.000]   I need the labels. Right. In Python, there's something called as a lambda.
[01:01:28.000 --> 01:01:37.000]   What is a lambda? A lambda. If I want to define a function like F of O, that returns O plus one.
[01:01:37.000 --> 01:01:46.000]   And I do F of X for X in range five.
[01:01:46.000 --> 01:02:08.000]   If I do something like that. If I do something like that, then you can see how.
[01:02:08.000 --> 01:02:18.000]   My NP that arranges just a list of five elements, zero, one, two, three, four, and I'm just calling this function for each of the elements.
[01:02:18.000 --> 01:02:23.000]   So now the returned list is one, two, three, four, five, which is one item.
[01:02:23.000 --> 01:02:36.000]   Sorry, one unit added to all of them. Instead of like defining a function like this in Python, you could say lambda O, O plus one.
[01:02:36.000 --> 01:02:47.000]   For O. I've just made some small error.
[01:02:47.000 --> 01:02:54.000]   What have I done? I'm just trying to figure this out.
[01:02:54.000 --> 01:02:59.000]   Anyway, leave that for me. I think that's a I've just made some syntax error. I'm really sorry for that.
[01:02:59.000 --> 01:03:10.000]   But basically the main idea is you can define a function by passing in some item. You can just define a function and you can say, okay, that's my input and that's my output.
[01:03:10.000 --> 01:03:19.000]   So I can just say that's my input and that's my output.
[01:03:19.000 --> 01:03:32.000]   I know. So I could just call my lambda O, O plus one. And I'm just trying to fix that error because and then I can just go.
[01:03:32.000 --> 01:03:43.000]   Yeah, this time it works. So basically, instead of like writing my function as define F of O and then you return O plus one,
[01:03:43.000 --> 01:03:52.000]   I can just define my function using lambda in Python and I can just say my input is something called O and my output is just O plus one.
[01:03:52.000 --> 01:03:59.000]   So that's my function. And then I can call this function for all of my elements in my list.
[01:03:59.000 --> 01:04:07.000]   So that's just what lambda does. Lambda is just a really easy and concise way of writing a function in Python.
[01:04:07.000 --> 01:04:18.000]   Okay. So now basically then we can say from the row, remember what this data block is going to receive are the two same rows, right?
[01:04:18.000 --> 01:04:28.000]   X and Y are the same. So I can just say whatever goes in to my X or like from X, what I want you to do is I want you to extract the F name.
[01:04:28.000 --> 01:04:36.000]   Right. So you're going to get the row as the input and you're going to output row F name or the file name, basically.
[01:04:36.000 --> 01:04:44.000]   So I can just say lambda row, row F name, which means the input is row and the output is the F name column of that row.
[01:04:44.000 --> 01:04:52.000]   Similarly for Y, I can say my input because it's going to be the same row. I can say my input is row and my output is row labels.
[01:04:52.000 --> 01:04:59.000]   So now when I call my data set, we're just defining how to get the X and Y in our data block.
[01:04:59.000 --> 01:05:10.000]   Now when I call my data sets on this, you can see how it returns the file name for my X and it returns the label for my Y.
[01:05:10.000 --> 01:05:20.000]   And I could have gotten my, like you can check all these different elements of my train and you can see how some images have more than one labels.
[01:05:20.000 --> 01:05:28.000]   So that's like the basic idea of using lambdas in here. But you could have also done the same thing in a more verbose way,
[01:05:28.000 --> 01:05:35.000]   which means you can define your get X and you can define your get Y. And then in data blocks, you're just saying, OK,
[01:05:35.000 --> 01:05:42.000]   the way you get X from the inputs is you're going to get a row, every row, and then you just return row file name.
[01:05:42.000 --> 01:05:49.000]   And you're going to do, you're going to return the row labels or like that's the column that you're going to return for my Y.
[01:05:49.000 --> 01:05:55.000]   So I can do the same thing and then you can see how that kind of returns the same thing that we want.
[01:05:55.000 --> 01:06:03.000]   So instead of just using lambdas like this, I can just define the functions.
[01:06:03.000 --> 01:06:07.000]   It's usually better to define the functions and not use lambdas.
[01:06:07.000 --> 01:06:13.000]   The reason for that is when you're trying to pickle it or when you're trying to basically save your data block,
[01:06:13.000 --> 01:06:20.000]   this kind of way you'll be able to save your data block to a file much easily than when you're defining lambdas.
[01:06:20.000 --> 01:06:26.000]   So that's just something if you ever feel like you're getting some weird saving errors or like it says,
[01:06:26.000 --> 01:06:31.000]   oh, I can't find this or Python complains about like I can't save data block,
[01:06:31.000 --> 01:06:36.000]   then that's the reason why you might have used lambda somewhere.
[01:06:36.000 --> 01:06:44.000]   OK, so now we have a training in a way like the train now returns my file name and my Y returns labels,
[01:06:44.000 --> 01:06:48.000]   but we're still not ready. Like we're still not ready.
[01:06:48.000 --> 01:06:55.000]   This is not something that can be fed to a model. A model accepts an image and it accepts a label.
[01:06:55.000 --> 01:06:59.000]   So my label is not like it's not trained.
[01:06:59.000 --> 01:07:02.000]   Like right now it's just a TV monitor or like it's a string.
[01:07:02.000 --> 01:07:08.000]   The model doesn't know what to do with it. So we need to further define what what's going to happen.
[01:07:08.000 --> 01:07:13.000]   And to convert this into an image. First, we need to provide the full path.
[01:07:13.000 --> 01:07:19.000]   So remember. All our files that we downloaded were in this point.
[01:07:19.000 --> 01:07:24.000]   And then if I go path/train, that's the folder where all of these images are.
[01:07:24.000 --> 01:07:45.000]   So I can just say. If I just import if I want to open this image 0024.jpg, I have to provide the full path.
[01:07:45.000 --> 01:07:50.000]   So that's how that image will be opened. You have to have the full path to the file.
[01:07:50.000 --> 01:07:57.000]   So then that way, what you do is now your X, instead of just being the file name, becomes the full path.
[01:07:57.000 --> 01:08:04.000]   So I'm just showing you right now we're not learning anything about we're not learning anything about multilabel classification.
[01:08:04.000 --> 01:08:12.000]   We're just seeing the steps it takes to create a data block for a multilabel classification pipeline.
[01:08:12.000 --> 01:08:15.000]   Or basically when you're doing multilabel classification.
[01:08:15.000 --> 01:08:22.000]   And when specifically you have your file name labels and is valid in a data frame, in a pandas data frame.
[01:08:22.000 --> 01:08:27.000]   So until now, we've just worked with folders or like all images have been in different folders.
[01:08:27.000 --> 01:08:34.000]   But we haven't really looked at this example of like, how does this work when you have your data frame?
[01:08:34.000 --> 01:08:40.000]   So we're just looking. We're just learning fast AI right now. We're not learning multilabel classification so far.
[01:08:40.000 --> 01:08:47.000]   OK, so I can just say now return my full basically my path.
[01:08:47.000 --> 01:08:52.000]   And remember when we were looking at, let's see if I have an example. Here it is.
[01:08:52.000 --> 01:08:59.000]   When we were looking at the labels for an image that had more than one category in them, then they were space limited.
[01:08:59.000 --> 01:09:06.000]   Or basically they were separated by space. So what I need to do is I need to split them by space.
[01:09:06.000 --> 01:09:15.000]   So if I update my getX and getY like that, then you can see now my X is returning that whole image path.
[01:09:15.000 --> 01:09:21.000]   And my Y is now a label. So let's see if I can find one with more than one label. Yes, there it is.
[01:09:21.000 --> 01:09:29.000]   So instead of now being motorbike, person, car as one string, if I go and I check this, it's now a list of three items.
[01:09:29.000 --> 01:09:32.000]   That's much easier for us to work with.
[01:09:32.000 --> 01:09:37.000]   Because then what we can do is we can convert each of these into a number that can then be fed to a model.
[01:09:37.000 --> 01:09:41.000]   So that's why there's some extra steps when you're working with multilabel classification.
[01:09:41.000 --> 01:09:48.000]   But the reason why I split this is because you need to know that there's like three items in this rather than if you didn't split it,
[01:09:48.000 --> 01:09:52.000]   then each string would be a different length or like all these different things.
[01:09:52.000 --> 01:10:00.000]   So that's just I'm just trying to give you an intuition or provide some background into why you have to do all of these steps.
[01:10:00.000 --> 01:10:08.000]   So now I can just so now I'm returning my path and I'm returning like my labels as a list in data block.
[01:10:08.000 --> 01:10:12.000]   Remember, we've already used image block and multi category block.
[01:10:12.000 --> 01:10:22.000]   What fast AI does is it takes this first item and it passes it through the image block and it takes the second item and it passes that through the multi category block.
[01:10:22.000 --> 01:10:29.000]   So in fast AI when you're doing multilabel classification, remember, the blocks are like a tuple or they're like a list
[01:10:29.000 --> 01:10:33.000]   where the first block is for your X and your second block is for your Y.
[01:10:33.000 --> 01:10:38.000]   And because we're doing multilabel classification and the first thing is an image or path to an image,
[01:10:38.000 --> 01:10:44.000]   the first block is image block and the second block is multi category block.
[01:10:44.000 --> 01:10:50.000]   So I can just do that. And now if I say data sets on train zero, see what the first thing is.
[01:10:50.000 --> 01:11:00.000]   My first thing now is an image. So if I extract the first item from this, see how that's an image.
[01:11:00.000 --> 01:11:10.000]   And if I extract the basically my Y. See how that's been converted into something like this.
[01:11:10.000 --> 01:11:20.000]   So they were about say. In this data set, we have about 20 categories that we're trying to classify.
[01:11:20.000 --> 01:11:25.000]   And each column represents a category because you're doing multilabel classification.
[01:11:25.000 --> 01:11:34.000]   Then have a look at this example. Some because you're going to have rows where it's going to be a dog and a chair or like a sofa and a chair.
[01:11:34.000 --> 01:11:39.000]   In that case, you're going to have multiple places where this category or this number is going to be one.
[01:11:39.000 --> 01:11:44.000]   In this case, because we only have an image of a dog, I believe that's why this is one.
[01:11:44.000 --> 01:11:55.000]   If I go for my second item in my data set. See how that is one at multiple places, which means there's two objects in this.
[01:11:55.000 --> 01:12:01.000]   So let's have a look at the image. So that's right. Maybe they're trying to classify a person and a class.
[01:12:01.000 --> 01:12:06.000]   I'm not exactly sure what those categories are right now, but you don't have to worry about either.
[01:12:06.000 --> 01:12:09.000]   I'm just trying to tell you the process. So like this is what's going on.
[01:12:09.000 --> 01:12:16.000]   This is why this has been converted into like a one hot encoded is what this is called.
[01:12:16.000 --> 01:12:24.000]   So whenever you see every category being represented as a column of its own, that is what is called as one hot encoded.
[01:12:24.000 --> 01:12:36.000]   So let me try and explain that a little bit. So say until now, you've only looked at things like cat, dog and horse.
[01:12:36.000 --> 01:12:41.000]   Right. So you could have your wise if it's an image of a cat, you could say it's zero.
[01:12:41.000 --> 01:12:45.000]   If it's an image of a dog, you could say one. And if it's an image of a horse, you could say two.
[01:12:45.000 --> 01:12:51.000]   And you could have your whole labels that way, like one, two, zero, one, two, two, one.
[01:12:51.000 --> 01:13:01.000]   So that becomes your label. Right. And another way to do it would be you could just make it one at the spot where it is and everything else is zero.
[01:13:01.000 --> 01:13:08.000]   Or like this could be zero one zero and this could be zero zero one because this one is an image of a horse.
[01:13:08.000 --> 01:13:16.000]   So that means that's one. So when you represent categories like this, where each category becomes its own column,
[01:13:16.000 --> 01:13:26.000]   this way of representing categories is called one hot encoding.
[01:13:26.000 --> 01:13:38.000]   And the reason why you have to do one hot encoding in multilabel classification and you don't have to do it when you're doing classification is because you can have multiple labels per image.
[01:13:38.000 --> 01:13:46.000]   Right. So your images are like this list. Right. Still like a list. And this could be a horse and a cat.
[01:13:46.000 --> 01:13:51.000]   Maybe this is just a cat and this is a dog. And this is again a horse, cat and dog. Right.
[01:13:51.000 --> 01:13:59.000]   So if you represented all of these as numbers, then this could be say one and two. This is zero. This is three.
[01:13:59.000 --> 01:14:04.000]   So horse is one. Cat is two. And this is three. So that's how you could represent this.
[01:14:04.000 --> 01:14:12.000]   But what's going to if you did a hot encode, but what's going to happen is like these length of these items are going to be different.
[01:14:12.000 --> 01:14:20.000]   So this is length two. This is length one, one and three. And this won't work because PyTorch expects your labels to be of the same length.
[01:14:20.000 --> 01:14:34.000]   So how do you fix that is your horse becomes horse, cat, dog. Basically, each becomes a row. Zero, one, two.
[01:14:34.000 --> 01:14:46.000]   I've labeled them wrong. So let's say in this case, my horse was zero, cat one, cat one, dog is two, horse zero, cat one and dog two.
[01:14:46.000 --> 01:14:55.000]   Let's say that's the labels. So this won't work. But what you could do is you could instead, when you're trying to represent this, you could say, OK, horse, yes.
[01:14:55.000 --> 01:15:02.000]   Cat. Is he present? Dog is not. Or you could say horse is for the second image.
[01:15:02.000 --> 01:15:10.000]   You could say horse is not present. Cat is present. Dog is not. In this case, you could say horse not present. Cat not present. Dog is yes.
[01:15:10.000 --> 01:15:19.000]   But see how now you're still representing this -- sorry. One second.
[01:15:19.000 --> 01:15:25.000]   You're still representing all of this information. But now the lengths of your labels are still the same.
[01:15:25.000 --> 01:15:31.000]   So this is what will work with PyTorch. And this is why you have to do all of these steps of one hot encoding.
[01:15:31.000 --> 01:15:39.000]   But by the time you've reached this point, we are now ready. So this is your train data set ready so far.
[01:15:39.000 --> 01:15:48.000]   So let me just have a look at some quick questions in case there are any in the report.
[01:15:48.000 --> 01:15:59.000]   Yes, thanks, Kevin. I kind of messed that up. But you're defining a function and then calling in the item. Thanks for that.
[01:15:59.000 --> 01:16:09.000]   Do you think pandas will ever be brought into the Python Native Library? I'm not sure. To be honest, it's something I use in my daily basis.
[01:16:09.000 --> 01:16:17.000]   But you're kind of surprised it isn't native. I am, too. I'm really surprised. But I guess it's its own package, which is very well supported.
[01:16:17.000 --> 01:16:26.000]   So those are just the questions. I guess there's no more questions for multilabel classification that we're doing so far.
[01:16:26.000 --> 01:16:37.000]   We will kind of wrap up until this point, until we reach binary cross entropy today.
[01:16:37.000 --> 01:16:43.000]   So we're going to look at binary cross entropy next. But right now, then, see how now you've converted everything.
[01:16:43.000 --> 01:16:50.000]   Like this is how you've kind of now made your labels look like this.
[01:16:50.000 --> 01:16:56.000]   What if you want to find, if you have labels like this, what if you want to find what labels for this are?
[01:16:56.000 --> 01:17:02.000]   In Python, or sorry, in PyTorch, you have something called a torch.where.
[01:17:02.000 --> 01:17:10.000]   So let's see my data sets. If I grab my data sets.train and I grab the first item of my data set, then this becomes my image.
[01:17:10.000 --> 01:17:17.000]   This becomes my label. And I want to see where is this label basically one or which categories are there.
[01:17:17.000 --> 01:17:26.000]   So if I just try and break this up, this is what something you should be doing on your own as well when you're trying to experiment things.
[01:17:26.000 --> 01:17:32.000]   You want to understand what this does is you break it up. So you do like this, data sets.train.
[01:17:32.000 --> 01:17:40.000]   So it's telling me at this index is where it is true, or basically that's the place where this category is one.
[01:17:40.000 --> 01:17:47.000]   So then you could say torch.where at this point.
[01:17:47.000 --> 01:17:53.000]   And it will return the index. It will say, OK, that's the index number 11.
[01:17:53.000 --> 01:18:01.000]   So you could actually basically torch.where just returns the index of all the places where things are.
[01:18:01.000 --> 01:18:10.000]   Oh, it expects things to be tensor, not list. One second.
[01:18:10.000 --> 01:18:16.000]   Right. So it just returns the indexes of things where things are true and it won't return the index of things where things are false.
[01:18:16.000 --> 01:18:21.000]   If I made that true as well, it's going to say at index zero, at index one, index two, things are true.
[01:18:21.000 --> 01:18:27.000]   And if all were false, it shouldn't return anything.
[01:18:27.000 --> 01:18:32.000]   So it's saying, OK, there's no place or there's no index in this list where things are true.
[01:18:32.000 --> 01:18:38.000]   So that's what we're doing. We're just finding give me the indexes of the places where my category is one.
[01:18:38.000 --> 01:18:47.000]   And it says, OK. The index where the thing is one is basically index 11.
[01:18:47.000 --> 01:18:52.000]   And then my data sets or my train data set has something called as a bocap.
[01:18:52.000 --> 01:18:59.000]   So this what it does is I remember we started with labels like this.
[01:18:59.000 --> 01:19:04.000]   We started with TV monitor. We started with train. Basically, they were strings.
[01:19:04.000 --> 01:19:09.000]   And because and we started with labels like motorbike, person and car.
[01:19:09.000 --> 01:19:15.000]   And because PyTorch or like your deep learning models can't accept strings, they need to be converted to numbers.
[01:19:15.000 --> 01:19:21.000]   So what happened underneath what PyTorch did is it created a list like this.
[01:19:21.000 --> 01:19:25.000]   Everywhere it saw airplane, that's what became one and everywhere it saw.
[01:19:25.000 --> 01:19:33.000]   So basically, all of these are like different indexes. So your TV monitor in this is your final index in your bocap.
[01:19:33.000 --> 01:19:38.000]   Which is, I guess, would be the 19th index. So that's what's happening underneath.
[01:19:38.000 --> 01:19:49.000]   So you can in this bocap, then you can pass that index and you can say, OK, the category or like the category for this.
[01:19:49.000 --> 01:19:56.000]   The category for my label, that's one hard encoded this way is a dog.
[01:19:56.000 --> 01:20:05.000]   That's the we're getting to the final bits now. So until now, we've been able to convert everything and we've been able to use every column.
[01:20:05.000 --> 01:20:11.000]   But if you remember, my data frame also had this column called is valid.
[01:20:11.000 --> 01:20:33.000]   So if we don't pass a splitter, which is if we don't pass this and we just do something like this.
[01:20:33.000 --> 01:20:43.000]   I'm just not using the splitter and I'm trying to show you something. One second, please.
[01:20:43.000 --> 01:20:56.000]   So if I just do something like this and I don't use my splitter, then what it does is it's actually going to randomly split my data set into train and validation.
[01:20:56.000 --> 01:21:05.000]   But what we and you can see how 80 percent of that goes into training and basically 20 percent of that goes into valid.
[01:21:05.000 --> 01:21:13.000]   But then what's happening is like we want to split things by this column, which is the df is valid.
[01:21:13.000 --> 01:21:27.000]   So what we can do is we want to split things by this column. So what we can do is we can have the indexes.
[01:21:27.000 --> 01:21:36.000]   Of every row where this is basically my training set, because if it's valid is true, that means it's not the training set.
[01:21:36.000 --> 01:21:43.000]   It means it's the validation set. So we could have the list of indexes everywhere this is false because that becomes your training set.
[01:21:43.000 --> 01:21:51.000]   And then we could also have a list of indexes everywhere it is true that becomes your validation set.
[01:21:51.000 --> 01:21:56.000]   So when you have your list of indexes like this, your train and val, this is what it is doing.
[01:21:56.000 --> 01:22:08.000]   It's basically just returning indexes. And then when you pass that into your splitter, which is your list of two indexes, it will divide your training and validation set that way.
[01:22:08.000 --> 01:22:20.000]   So now if I do that. One second, I just want to show you. This and I want to show you the length of.
[01:22:20.000 --> 01:22:33.000]   My train and length of my valid. So you can see how now I should expect 2500 in my train and 2510 basically 2510 in my valid.
[01:22:33.000 --> 01:22:42.000]   So let's check that and see if the length of my training and validation sets are that.
[01:22:42.000 --> 01:22:50.000]   I keep calling it well, it should be valid. So there it is. So now that has worked. So basically we've been able to split our data set using this column.
[01:22:50.000 --> 01:22:55.000]   So what a splitter. So these are all things we're just going deeper and deeper into fast AI.
[01:22:55.000 --> 01:23:03.000]   So now you know exactly what splitter is. Now you know what the blocks are used for. Now you know what getX and getY are.
[01:23:03.000 --> 01:23:09.000]   And splitter is then just providing an indexes that that's the way how you split your models.
[01:23:09.000 --> 01:23:17.000]   And then finally, this is everything together. So now we can create our data block by passing in my image block multi category block.
[01:23:17.000 --> 01:23:26.000]   I can pass in my splitter, my getX function that returns the file name, my get sorry the path to my file, my getY that returns the label.
[01:23:26.000 --> 01:23:33.000]   And then something else I want to do is just because I want all of my images to be of the same size in fast AI.
[01:23:33.000 --> 01:23:40.000]   Remember, we've already looked at item transforms. We've already looked at batch transforms. So I can just pass in an item transform.
[01:23:40.000 --> 01:23:46.000]   And now I can say DLs.showBatch and that will work. It will say this is my image.
[01:23:46.000 --> 01:23:51.000]   I want to do showBatch again because I want to find an example where there's two labels.
[01:23:51.000 --> 01:23:57.000]   And there it is a chair in person in this one. Bus here, cat here. We can look at more examples.
[01:23:57.000 --> 01:24:07.000]   So dog in person in this image. So right now, that's it. We've been able to now create our data loader for multi-label classification.
[01:24:07.000 --> 01:24:15.000]   We will look at how things how we will train this in our next session next week.
[01:24:15.000 --> 01:24:23.000]   We will look at how the loss needs to change between multi-label classification and standard single label classification.
[01:24:23.000 --> 01:24:30.000]   So that's next week. Let me see if I have some questions. How to read a downloaded zip file of images?
[01:24:30.000 --> 01:24:39.000]   You can just use Untar Data. Untar Data will unzip it for you. And you should be able to basically get a path.
[01:24:39.000 --> 01:24:43.000]   So yeah, try Untar Data for that.
[01:24:43.000 --> 01:24:49.000]   All right. Then in terms of homework or things you can do this week is please read until this point.
[01:24:49.000 --> 01:24:55.000]   Read the PetBreeze chapter. See if you can find good points. Try and experiment with learning rate finder experiment,
[01:24:55.000 --> 01:24:58.000]   with transfer learning experiment, with all of these different things.
[01:24:58.000 --> 01:25:05.000]   And then when you're doing multi-label, then read until this point that you can construct your own data blocks.
[01:25:05.000 --> 01:25:11.000]   But this is the point where now you can start experimenting and playing around with different data sets.
[01:25:11.000 --> 01:25:17.000]   But that's something we will talk about next week. We will talk about a little bit about Kaggle
[01:25:17.000 --> 01:25:22.000]   and how when we finish this chapter, then you should be able to pick up different data sets
[01:25:22.000 --> 01:25:27.000]   and you should be able to experiment with Kaggle. So we'll talk about that next week.
[01:25:27.000 --> 01:25:31.000]   So let me see if there's any questions. No more questions. Cool.
[01:25:31.000 --> 01:25:36.000]   Everybody, thank you very much for meeting us today.
[01:25:36.000 --> 01:25:41.000]   Again, apologies for the technical difficulties, but that should be fixed by next week.
[01:25:41.000 --> 01:25:50.000]   Thanks, Angelica, for setting the YouTube live link for us today. And thanks, Ravi, for pointing that out to us.
[01:25:50.000 --> 01:25:52.000]   So thanks, everybody. See you next week.
[01:25:52.000 --> 01:25:57.000]   [END]
[01:25:57.000 --> 01:26:02.000]   [END]
[01:26:02.000 --> 01:26:07.000]   [END]
[01:26:07.000 --> 01:26:12.000]   Thank you.

