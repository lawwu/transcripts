
[00:00:00.000 --> 00:00:04.000]   Welcome, everyone.
[00:00:04.000 --> 00:00:06.320]   We're super excited to have you here.
[00:00:06.320 --> 00:00:09.360]   This is our first meetup ever in London.
[00:00:09.360 --> 00:00:10.480]   And how are you?
[00:00:10.480 --> 00:00:18.240]   And this is actually our biggest event ever that we've done.
[00:00:18.240 --> 00:00:21.840]   When we first announced it, I expected 50 people to come.
[00:00:21.840 --> 00:00:25.040]   Then we got 500 people registered.
[00:00:25.040 --> 00:00:28.640]   And finally, we ended up with, I think, about 1,000 people,
[00:00:28.640 --> 00:00:29.680]   which is insane.
[00:00:29.840 --> 00:00:33.600]   Thank you guys for joining us.
[00:00:33.600 --> 00:00:37.360]   We have some really exciting things to show you tonight.
[00:00:37.360 --> 00:00:39.920]   We're going to start with some product announcements.
[00:00:39.920 --> 00:00:45.920]   Then you'll hear from Imad from Stability, Nuno from Lanching.
[00:00:45.920 --> 00:00:51.520]   And then we also have some really exciting surprises for you guys backstage.
[00:00:51.520 --> 00:00:57.040]   But first, I want to tell you a little bit about how you can be
[00:00:57.040 --> 00:00:59.200]   a part of the Weeks & Bices community.
[00:00:59.840 --> 00:01:03.920]   So if you like what you see today, if you like the people that you meet,
[00:01:03.920 --> 00:01:06.720]   I would love for you guys to join our Discord.
[00:01:06.720 --> 00:01:08.560]   You can find it at that link right there.
[00:01:08.560 --> 00:01:13.200]   Our Discord is full of machine learning practitioners
[00:01:13.200 --> 00:01:15.120]   who are really helping each other.
[00:01:15.120 --> 00:01:16.720]   That's the core of our community.
[00:01:16.720 --> 00:01:19.280]   We really want anyone to be able to join us.
[00:01:19.280 --> 00:01:24.640]   And then we want everyone to be helpful to other machine learning engineers
[00:01:24.640 --> 00:01:26.480]   and help everyone get better.
[00:01:28.480 --> 00:01:31.600]   This is a community we're super proud of building,
[00:01:31.600 --> 00:01:33.600]   and we really hope that you join us.
[00:01:33.600 --> 00:01:38.000]   Next up, we also believe that machine learning
[00:01:38.000 --> 00:01:40.480]   should be accessible to everyone.
[00:01:40.480 --> 00:01:42.880]   And also learning machine learning should be free.
[00:01:42.880 --> 00:01:46.320]   So we've been creating all of these different courses
[00:01:46.320 --> 00:01:50.240]   to help machine learning engineers learn to put their models in production.
[00:01:50.240 --> 00:01:55.680]   And in these courses, we share best practices that we have learned
[00:01:55.680 --> 00:01:59.680]   from working with some of the best machine learning teams for years.
[00:01:59.680 --> 00:02:03.200]   And you can find them at 1b.me/courses.
[00:02:03.200 --> 00:02:06.800]   We also have Fully Connected, our blog.
[00:02:06.800 --> 00:02:08.960]   Anyone here heard of Fully Connected?
[00:02:08.960 --> 00:02:11.360]   There's one person.
[00:02:11.360 --> 00:02:11.840]   Hiya.
[00:02:11.840 --> 00:02:15.120]   I love a new audience.
[00:02:15.120 --> 00:02:16.640]   So Fully Connected is our blog.
[00:02:16.640 --> 00:02:18.640]   You can see it on the left.
[00:02:18.640 --> 00:02:23.840]   So this is full of tutorials that are written by machine learning engineers,
[00:02:23.840 --> 00:02:25.120]   just like you.
[00:02:25.120 --> 00:02:27.360]   On the most important topics of the day.
[00:02:27.360 --> 00:02:32.640]   So if a new model comes out, you can probably expect a deep dive on it
[00:02:32.640 --> 00:02:35.360]   on Fully Connected in the next few days.
[00:02:35.360 --> 00:02:38.320]   And you can find it at 1b.me/fc.
[00:02:38.320 --> 00:02:42.640]   Also, we love throwing events, as you can probably tell.
[00:02:42.640 --> 00:02:46.400]   And we run about four to eight events a week.
[00:02:46.400 --> 00:02:48.160]   My events team is insane.
[00:02:48.160 --> 00:02:51.840]   They're in the crowd, and they work really hard.
[00:02:51.840 --> 00:02:54.320]   A lot of these events are virtual.
[00:02:54.320 --> 00:02:55.920]   So you guys can all join them.
[00:02:55.920 --> 00:02:58.640]   And you can find them at 1b.me/events.
[00:02:58.640 --> 00:03:01.200]   But that's not all.
[00:03:01.200 --> 00:03:05.920]   So we're super excited to announce that we are hosting our first ever
[00:03:05.920 --> 00:03:10.240]   user conference in San Francisco, in person, on June 7th.
[00:03:10.240 --> 00:03:12.720]   We don't have a sign-up page just yet.
[00:03:12.720 --> 00:03:17.360]   But if you watch our social media, we're going to post the link to it there soon.
[00:03:17.360 --> 00:03:22.720]   And finally, we also have a podcast that we're super proud of.
[00:03:22.720 --> 00:03:26.400]   It kind of started as a joke between me and our co-founder.
[00:03:26.400 --> 00:03:28.640]   And it's been going on for three years now.
[00:03:28.640 --> 00:03:35.600]   So our podcast is where Lucas, our CEO, takes you behind the scenes
[00:03:35.600 --> 00:03:40.160]   to learn from industry leaders like Jensen, the CEO of NVIDIA,
[00:03:40.160 --> 00:03:42.800]   about how to put your models in production.
[00:03:42.800 --> 00:03:47.040]   And you can find it at beatsandbias at 1b.me/gd.
[00:03:47.920 --> 00:03:53.040]   And speaking of the host, I would love to welcome our co-founder on stage.
[00:03:53.040 --> 00:04:02.260]   All right.
[00:04:02.260 --> 00:04:03.460]   All right.
[00:04:03.460 --> 00:04:05.520]   Yeah, guys.
[00:04:05.520 --> 00:04:06.020]   Yeah.
[00:04:06.020 --> 00:04:07.920]   What an exciting night.
[00:04:07.920 --> 00:04:11.280]   I can feel a real buzz in the air, you know?
[00:04:11.280 --> 00:04:12.800]   Oh, boy.
[00:04:12.800 --> 00:04:16.400]   Some of you might be asking, "Why is there a grown man
[00:04:16.400 --> 00:04:18.480]   wearing a bee costume up on stage right now?"
[00:04:18.480 --> 00:04:22.080]   Just know I'm asking myself that same question.
[00:04:22.080 --> 00:04:28.480]   Let's get this night started by doing a little audience participation.
[00:04:28.480 --> 00:04:31.680]   How many folks have used WANdB?
[00:04:31.680 --> 00:04:34.000]   Make some noise.
[00:04:34.000 --> 00:04:35.600]   Oh, yeah.
[00:04:35.600 --> 00:04:36.100]   All right.
[00:04:36.100 --> 00:04:38.160]   What about LangChain?
[00:04:38.160 --> 00:04:40.320]   We got any LangChain users out there tonight?
[00:04:40.320 --> 00:04:41.920]   All right.
[00:04:41.920 --> 00:04:42.420]   Yeah.
[00:04:42.420 --> 00:04:46.000]   Stability AI fans, maybe.
[00:04:46.000 --> 00:04:47.280]   Can we make some noise?
[00:04:47.280 --> 00:04:47.920]   All right.
[00:04:47.920 --> 00:04:49.280]   All right.
[00:04:49.280 --> 00:04:49.920]   Let's try this.
[00:04:49.920 --> 00:04:53.360]   Everyone that likes TensorFlow, say, "Hey!"
[00:04:53.360 --> 00:04:56.880]   PyTorch.
[00:04:56.880 --> 00:05:01.680]   Yeah?
[00:05:01.680 --> 00:05:02.480]   Okay.
[00:05:02.480 --> 00:05:02.880]   Okay.
[00:05:02.880 --> 00:05:04.320]   That's enough of that.
[00:05:04.320 --> 00:05:04.960]   Yeah, okay.
[00:05:04.960 --> 00:05:05.760]   Okay.
[00:05:05.760 --> 00:05:07.360]   That's the wrong co-founder.
[00:05:07.360 --> 00:05:11.520]   I actually wanted to welcome Lucas Beewald, our CEO,
[00:05:11.520 --> 00:05:14.560]   to tell you about some interesting product announcements.
[00:05:15.520 --> 00:05:22.240]   [APPLAUSE]
[00:05:22.240 --> 00:05:22.960]   Awesome.
[00:05:22.960 --> 00:05:24.320]   Did you just walk off with a clicker?
[00:05:24.320 --> 00:05:26.420]   Nice.
[00:05:26.420 --> 00:05:29.200]   It's so awesome to be here.
[00:05:29.200 --> 00:05:31.680]   This isn't exactly what I thought I was signing up for,
[00:05:31.680 --> 00:05:34.480]   doing a meetup in London, but this is so much better.
[00:05:34.480 --> 00:05:36.800]   I was actually in San Francisco a few weeks ago
[00:05:36.800 --> 00:05:40.800]   for Hugging Faces' awesome Woodstock AI event, I guess.
[00:05:40.800 --> 00:05:44.160]   That was an amazing event, and I loved it.
[00:05:44.160 --> 00:05:46.000]   But it's been almost even more exciting
[00:05:46.000 --> 00:05:47.120]   to walk through the audience here
[00:05:47.120 --> 00:05:49.120]   and just see what you all are up to.
[00:05:49.120 --> 00:05:50.240]   It's amazing.
[00:05:50.240 --> 00:05:51.280]   I really appreciate it.
[00:05:51.280 --> 00:05:54.800]   I'm CEO of Weights & Biases.
[00:05:54.800 --> 00:05:55.840]   I'm Lucas Beewald.
[00:05:55.840 --> 00:06:00.400]   And I'm here to talk about Weights & Biases for LLMs.
[00:06:00.400 --> 00:06:01.120]   Do you have the clicker?
[00:06:01.120 --> 00:06:04.880]   I can use the laptop.
[00:06:04.880 --> 00:06:05.380]   No worries.
[00:06:05.380 --> 00:06:09.840]   And since a lot of you actually hadn't used Weights & Biases,
[00:06:09.840 --> 00:06:11.680]   before we get into Weights & Biases for LLMs,
[00:06:11.680 --> 00:06:15.520]   I just wanted to go through what Weights & Biases does.
[00:06:15.520 --> 00:06:19.040]   And our mission has always been to build the best tools
[00:06:19.040 --> 00:06:21.040]   for machine learning practitioners.
[00:06:21.040 --> 00:06:24.240]   And we define machine learning practitioners broadly
[00:06:24.240 --> 00:06:26.880]   as anyone that's trying to make machine learning models
[00:06:26.880 --> 00:06:27.920]   work in the real world.
[00:06:27.920 --> 00:06:29.360]   That's how we've always thought about it.
[00:06:29.360 --> 00:06:31.120]   And so the history of the company, in a nutshell,
[00:06:31.120 --> 00:06:33.840]   it's actually been five years and things have changed a lot.
[00:06:33.840 --> 00:06:36.480]   We started off, it was just me and that bee,
[00:06:36.480 --> 00:06:39.520]   and a third co-founder who's back in San Francisco
[00:06:39.520 --> 00:06:41.680]   working really hard on the next products.
[00:06:41.680 --> 00:06:44.240]   And we built a thing called Experiments
[00:06:44.240 --> 00:06:46.640]   that was to help people, including ourselves,
[00:06:46.640 --> 00:06:48.000]   do experiment tracking.
[00:06:48.000 --> 00:06:51.040]   So keep track of all the models that we were building
[00:06:51.040 --> 00:06:53.120]   and kind of understand where they regressed
[00:06:53.120 --> 00:06:54.400]   and understand how to make them better.
[00:06:54.400 --> 00:06:58.640]   And not long after we built this, people asked,
[00:06:58.640 --> 00:07:00.480]   "Hey, could you help us make a system
[00:07:00.480 --> 00:07:02.720]   to make models automatically better?"
[00:07:02.720 --> 00:07:06.800]   And so we built Sweeps, which is a hyperparameter tuning system.
[00:07:06.800 --> 00:07:07.360]   Thank you.
[00:07:07.360 --> 00:07:07.860]   Awesome.
[00:07:09.360 --> 00:07:10.960]   And then not long after that, people said,
[00:07:10.960 --> 00:07:12.960]   "Well, we really want to share the stuff
[00:07:12.960 --> 00:07:16.160]   that we're doing with colleagues and our boss."
[00:07:16.160 --> 00:07:18.080]   And then eventually, actually, the whole world.
[00:07:18.080 --> 00:07:20.160]   And that was reports that we built.
[00:07:20.160 --> 00:07:21.680]   And then people started saying,
[00:07:21.680 --> 00:07:23.520]   "We really want full reproducibility here.
[00:07:23.520 --> 00:07:26.640]   We want to track not just the model architecture,
[00:07:26.640 --> 00:07:28.400]   but the training data that went into the models."
[00:07:28.400 --> 00:07:30.560]   And that's complicated because it's changing all the time.
[00:07:30.560 --> 00:07:32.560]   And so we built a thing called Artifacts.
[00:07:32.560 --> 00:07:35.200]   Then folks wanted to explore data, so we built Tables.
[00:07:35.200 --> 00:07:37.040]   They wanted to track things in production.
[00:07:37.040 --> 00:07:39.360]   So recently, this is actually just a few months ago,
[00:07:39.360 --> 00:07:40.880]   we launched Models.
[00:07:40.880 --> 00:07:43.200]   And then our most recent launch to date
[00:07:43.200 --> 00:07:47.040]   is something called Launch, which came from people asking,
[00:07:47.040 --> 00:07:48.960]   "Can I actually automatically run jobs
[00:07:48.960 --> 00:07:50.560]   from the Weights & Biases interface?"
[00:07:50.560 --> 00:07:53.920]   So it's always been guided by requests from people like you
[00:07:53.920 --> 00:07:56.000]   who were using machine learning
[00:07:56.000 --> 00:07:57.680]   and had problems they were running into
[00:07:57.680 --> 00:07:59.040]   and were talking to us and telling us,
[00:07:59.040 --> 00:08:00.560]   "Hey, here's a new thing that I need."
[00:08:00.560 --> 00:08:04.480]   But today, we're here to talk specifically about LLMs,
[00:08:04.480 --> 00:08:06.320]   which we've been thinking a lot about
[00:08:06.320 --> 00:08:07.600]   inside of Weights & Biases.
[00:08:07.600 --> 00:08:09.040]   And just from talking to all of you,
[00:08:09.040 --> 00:08:10.800]   I know that you've all been thinking about it too.
[00:08:10.800 --> 00:08:14.560]   And to kind of ground this in our perspective,
[00:08:14.560 --> 00:08:17.440]   we think there's sort of three ML practitioner types,
[00:08:17.440 --> 00:08:19.280]   and you may be all three or just one of these.
[00:08:19.280 --> 00:08:22.720]   The first is large language model creators, right?
[00:08:22.720 --> 00:08:26.160]   So these are folks that are training LLMs from scratch,
[00:08:26.160 --> 00:08:26.880]   these big models.
[00:08:26.880 --> 00:08:29.120]   These are kind of hardcore machine learning people.
[00:08:29.120 --> 00:08:32.480]   The second is the LLM fine tuners.
[00:08:32.480 --> 00:08:35.040]   So these are people that are taking models
[00:08:35.040 --> 00:08:36.480]   and adding a little bit more data
[00:08:36.480 --> 00:08:37.840]   to do something interesting with them.
[00:08:37.840 --> 00:08:41.440]   And then finally, the LLM prompt engineers
[00:08:41.440 --> 00:08:44.560]   who use LLMs as an API or a service
[00:08:44.560 --> 00:08:47.200]   and try to figure out how to interact with that LLM
[00:08:47.200 --> 00:08:48.560]   to make something useful happen.
[00:08:48.560 --> 00:08:51.920]   And these days, prompt engineer could be almost anyone.
[00:08:51.920 --> 00:08:54.800]   So I wanted to start a little bit
[00:08:54.800 --> 00:08:57.680]   with what we do for LLM creators.
[00:08:57.680 --> 00:08:59.680]   And these companies and organizations
[00:08:59.680 --> 00:09:01.280]   actually all build LLMs,
[00:09:01.280 --> 00:09:04.000]   and they're all longtime customers of Weights & Biases
[00:09:04.000 --> 00:09:05.680]   that we feel super proud of.
[00:09:05.680 --> 00:09:09.760]   Actually, large model training was our very first use case
[00:09:09.760 --> 00:09:10.800]   in our bread and butter.
[00:09:10.800 --> 00:09:14.160]   This is us at OpenAI back in 2018
[00:09:14.160 --> 00:09:16.000]   when it was really just a handful of people,
[00:09:16.000 --> 00:09:17.360]   and we were just a handful of people.
[00:09:17.360 --> 00:09:19.360]   And this is our first product manager,
[00:09:19.360 --> 00:09:21.440]   Kerry, talking to Peter over there.
[00:09:21.440 --> 00:09:23.920]   And Peter is actually showing a very first iteration
[00:09:23.920 --> 00:09:25.760]   of a Weights & Biases dashboard,
[00:09:25.760 --> 00:09:27.280]   which I think we knew we were onto something
[00:09:27.280 --> 00:09:29.680]   when OpenAI decided to take that dashboard
[00:09:29.680 --> 00:09:31.200]   and kind of put it on their wall there.
[00:09:31.200 --> 00:09:34.400]   That was really a joy to see that.
[00:09:34.400 --> 00:09:36.560]   We had about 10 users at the time,
[00:09:36.560 --> 00:09:39.360]   and I think nine of them were inside of OpenAI.
[00:09:39.360 --> 00:09:42.000]   And so at that time,
[00:09:42.000 --> 00:09:43.760]   they were talking about scaling infrastructure
[00:09:43.760 --> 00:09:44.480]   for large models.
[00:09:44.480 --> 00:09:46.080]   And you go back to 2018, and they were thinking,
[00:09:46.080 --> 00:09:49.200]   "Wow, we're gonna need so much more infrastructure
[00:09:49.200 --> 00:09:50.160]   to make these models work."
[00:09:50.160 --> 00:09:51.440]   And they were totally right about that.
[00:09:51.440 --> 00:09:54.320]   But another thing that they were talking to us about
[00:09:54.320 --> 00:09:56.960]   is actually scaling the teams to build these large models.
[00:09:56.960 --> 00:09:59.280]   And this is actually the very first case study we did
[00:09:59.280 --> 00:10:01.600]   also back in 2019, I believe,
[00:10:01.600 --> 00:10:04.320]   where they say, "When we have 10 to 20 people
[00:10:04.320 --> 00:10:05.840]   working with our code base,
[00:10:05.840 --> 00:10:07.440]   at any point, someone could commit a change
[00:10:07.440 --> 00:10:08.640]   and break something."
[00:10:08.640 --> 00:10:10.800]   And that's what I really, really wanted
[00:10:10.800 --> 00:10:11.840]   to help them with that.
[00:10:11.840 --> 00:10:13.680]   And it's been so exciting to see
[00:10:13.680 --> 00:10:15.680]   so many other organizations kind of get to the point
[00:10:15.680 --> 00:10:17.680]   where they have 10 people or 20 people
[00:10:17.680 --> 00:10:19.200]   trying to build something together,
[00:10:19.200 --> 00:10:20.800]   trying to build a machine learning thing together,
[00:10:20.800 --> 00:10:22.560]   and then you run into these things.
[00:10:22.560 --> 00:10:25.440]   And it's funny because back then,
[00:10:25.440 --> 00:10:28.080]   10 seemed like a lot of people to work on a model together.
[00:10:28.080 --> 00:10:31.120]   And then, you know, these organizations like Meta
[00:10:31.120 --> 00:10:33.680]   or Facebook Research Labs became hundreds of people.
[00:10:33.680 --> 00:10:36.080]   And now with the way stability is working
[00:10:36.080 --> 00:10:38.000]   and other organizations kind of in public
[00:10:38.000 --> 00:10:38.960]   and letting anyone in,
[00:10:38.960 --> 00:10:42.800]   these teams are, practically speaking,
[00:10:42.800 --> 00:10:44.640]   thousands of people working together,
[00:10:44.640 --> 00:10:46.640]   all trying to make models better.
[00:10:46.640 --> 00:10:50.080]   And so one really exciting trend here
[00:10:50.080 --> 00:10:51.840]   is the open sourcing of models.
[00:10:51.840 --> 00:10:53.600]   And stability has absolutely been at the forefront of this.
[00:10:53.600 --> 00:10:55.920]   But we first saw it with DALI Mini,
[00:10:55.920 --> 00:10:58.320]   which was an open source version of DALI,
[00:10:58.320 --> 00:11:00.640]   where the author of this kept a training journal.
[00:11:00.640 --> 00:11:02.960]   And it's like hundreds and hundreds of pages
[00:11:02.960 --> 00:11:04.960]   of weights and biases, logs of every single thing
[00:11:04.960 --> 00:11:05.680]   that they did.
[00:11:05.680 --> 00:11:08.480]   And it's actually really interesting to watch his process
[00:11:08.480 --> 00:11:10.400]   as he learns how to do this.
[00:11:10.400 --> 00:11:12.000]   And then the next one we saw is a Luther
[00:11:12.000 --> 00:11:16.800]   doing public experiment tracking on GPT-J and NeoX,
[00:11:16.800 --> 00:11:19.280]   completely in public, completely where everyone could see it.
[00:11:19.280 --> 00:11:20.720]   And you can actually follow this link right now
[00:11:20.720 --> 00:11:24.080]   and look at kind of the evolution of all of these models.
[00:11:24.080 --> 00:11:26.240]   And lately we've seen things like OpenAssist,
[00:11:26.240 --> 00:11:28.080]   which just came out and is really exciting.
[00:11:28.080 --> 00:11:32.000]   And of course, StableLM, which was released yesterday,
[00:11:32.000 --> 00:11:34.400]   I believe, so good timing for this event.
[00:11:34.400 --> 00:11:37.200]   But also something that we feel really proud
[00:11:37.200 --> 00:11:38.560]   to have helped in a little way with.
[00:11:38.560 --> 00:11:43.120]   So training LLMs is not a small feat,
[00:11:43.120 --> 00:11:45.840]   and I would not undertake it lightly.
[00:11:45.840 --> 00:11:48.640]   And I don't actually recommend it to most of our customers.
[00:11:48.640 --> 00:11:51.040]   But we did want to offer a little bit
[00:11:51.040 --> 00:11:53.360]   in the way of helping people that insist on doing this.
[00:11:53.360 --> 00:11:56.320]   And it's surprising how many people insist on trying this.
[00:11:56.320 --> 00:11:58.720]   We have a white paper at LLM White Paper.
[00:11:58.720 --> 00:12:00.880]   We've kind of combined all the sort of best practices
[00:12:00.880 --> 00:12:03.760]   that we've seen across different types of organizations
[00:12:03.760 --> 00:12:06.720]   that want to spend millions of dollars on compute
[00:12:06.720 --> 00:12:08.560]   and hire kind of the hardest to hire people
[00:12:08.560 --> 00:12:11.440]   and bring them together to train an LLM from scratch.
[00:12:11.440 --> 00:12:14.560]   But now I think the most exciting trend
[00:12:14.560 --> 00:12:17.760]   in the last couple of years for us has been LLM fine tuning.
[00:12:17.760 --> 00:12:20.160]   And this used to be a research topic.
[00:12:20.160 --> 00:12:21.840]   I think, you know, back in the aughts,
[00:12:21.840 --> 00:12:23.040]   lots of people talked about fine tuning.
[00:12:23.040 --> 00:12:24.560]   A lot of papers on fine tuning.
[00:12:24.560 --> 00:12:26.800]   But most companies didn't do it, right?
[00:12:26.800 --> 00:12:29.760]   And this is where you take an existing model
[00:12:29.760 --> 00:12:32.160]   and you kind of use the fact that it's been trained
[00:12:32.160 --> 00:12:33.120]   on lots of data.
[00:12:33.120 --> 00:12:36.000]   But then you take it and you feed in a smaller set
[00:12:36.000 --> 00:12:36.720]   of your own data.
[00:12:36.720 --> 00:12:39.520]   So you get the kind of power of the LLM,
[00:12:39.520 --> 00:12:41.600]   but you've made it unique to your use case.
[00:12:41.600 --> 00:12:43.600]   And it works super, super well, right?
[00:12:43.600 --> 00:12:44.880]   Lots and lots of people do it.
[00:12:44.880 --> 00:12:48.800]   But I think it was hard to do until Hugging Face came along
[00:12:48.800 --> 00:12:51.680]   and made it incredibly easy, just a few lines of code,
[00:12:51.680 --> 00:12:55.120]   to take existing large models and then fine tune them.
[00:12:55.120 --> 00:12:56.400]   And also they made a place where people
[00:12:56.400 --> 00:12:57.440]   could publish these models.
[00:12:57.440 --> 00:13:00.080]   So tons and tons of models at Hugging Face.
[00:13:00.080 --> 00:13:01.200]   It's an amazing product.
[00:13:01.200 --> 00:13:04.640]   And so we actually built a zero line integration
[00:13:04.640 --> 00:13:05.280]   a long time ago.
[00:13:05.280 --> 00:13:07.600]   So actually all you have to do is import WANDB
[00:13:07.600 --> 00:13:10.000]   and then you get these nice little dashboards.
[00:13:10.000 --> 00:13:12.240]   It's much less of an engineering undertaking
[00:13:12.240 --> 00:13:13.440]   for us to ingest all this data.
[00:13:13.440 --> 00:13:15.280]   So we appreciate when people fine tune
[00:13:15.280 --> 00:13:16.960]   instead of building from scratch.
[00:13:16.960 --> 00:13:19.840]   We can make these graphs with almost no cost for ourselves.
[00:13:19.840 --> 00:13:22.400]   And we love it, all the people that do it.
[00:13:22.400 --> 00:13:24.080]   And I think all these same products that
[00:13:24.080 --> 00:13:26.560]   were sort of built for training from scratch,
[00:13:26.560 --> 00:13:28.800]   they work the same way for fine tuning.
[00:13:28.800 --> 00:13:31.680]   You still often want to do hyperparameter searches
[00:13:31.680 --> 00:13:32.880]   over what's the best learning rate,
[00:13:32.880 --> 00:13:36.160]   what's the best batch size, what kind of data should I feed in.
[00:13:36.160 --> 00:13:38.640]   You still want to look at what's the input data.
[00:13:38.640 --> 00:13:40.960]   In fact, the data lineage can get more complicated
[00:13:40.960 --> 00:13:43.600]   with fine tuning as you kind of feed in different data
[00:13:43.600 --> 00:13:45.440]   as your model evolves over time.
[00:13:46.560 --> 00:13:50.000]   Tables and exploration is important for any application
[00:13:50.000 --> 00:13:50.800]   of machine learning.
[00:13:50.800 --> 00:13:52.560]   But it's especially important on text, right,
[00:13:52.560 --> 00:13:55.440]   where you want to look at what exactly is the model doing
[00:13:55.440 --> 00:13:56.880]   in specific cases.
[00:13:56.880 --> 00:13:59.440]   And everyone should use a model registry,
[00:13:59.440 --> 00:14:01.440]   even if you don't use weights and biases models,
[00:14:01.440 --> 00:14:02.800]   to keep track of the model lifecycle
[00:14:02.800 --> 00:14:04.960]   so you know actually what you're putting in production.
[00:14:04.960 --> 00:14:06.320]   The number of stories that I've heard
[00:14:06.320 --> 00:14:08.880]   of just sort of simple mistakes, including from Ahmad
[00:14:08.880 --> 00:14:11.200]   a few minutes ago, just blows my mind.
[00:14:11.200 --> 00:14:13.840]   You all should be using some kind of model registry
[00:14:13.840 --> 00:14:14.720]   no matter what you're doing.
[00:14:16.000 --> 00:14:19.680]   And kind of all this has been super exciting.
[00:14:19.680 --> 00:14:21.920]   We've actually tracked over 200 million hours of compute
[00:14:21.920 --> 00:14:23.040]   on weights and biases.
[00:14:23.040 --> 00:14:25.360]   And a huge boost to its popularity
[00:14:25.360 --> 00:14:28.560]   and the mainstreaming came from this fine tuning
[00:14:28.560 --> 00:14:30.000]   on large language models.
[00:14:30.000 --> 00:14:33.280]   So we actually are announcing today--
[00:14:33.280 --> 00:14:35.040]   and I think this could be relevant to a lot of you--
[00:14:35.040 --> 00:14:37.280]   a free course that we're putting out
[00:14:37.280 --> 00:14:40.320]   on best practices for training and fine tuning LM.
[00:14:40.320 --> 00:14:42.960]   So you're welcome to go here right now and sign up
[00:14:42.960 --> 00:14:43.760]   and register for it.
[00:14:43.760 --> 00:14:44.800]   It's completely free.
[00:14:44.800 --> 00:14:46.320]   Our courses get really good reviews
[00:14:46.320 --> 00:14:48.560]   and it's made by experts in our community
[00:14:48.560 --> 00:14:50.000]   that have done this a lot themselves.
[00:14:50.000 --> 00:14:53.040]   All right.
[00:14:53.040 --> 00:14:55.920]   So the big thing that we've been thinking about--
[00:14:55.920 --> 00:14:58.080]   maybe the most exciting thing going on right now,
[00:14:58.080 --> 00:15:00.320]   which I think is contributing to why we actually
[00:15:00.320 --> 00:15:02.160]   had to turn away people from this event.
[00:15:02.160 --> 00:15:03.680]   Which I got to tell you, as an entrepreneur,
[00:15:03.680 --> 00:15:05.440]   turning away people from your own event,
[00:15:05.440 --> 00:15:07.040]   it just like breaks your heart.
[00:15:07.040 --> 00:15:08.080]   Like it kills you.
[00:15:08.080 --> 00:15:10.080]   Like we got a bigger venue and then a bigger venue
[00:15:10.080 --> 00:15:11.600]   and we couldn't get an even bigger venue.
[00:15:11.600 --> 00:15:13.520]   So for folks on the live stream that
[00:15:13.520 --> 00:15:16.320]   couldn't actually get signed in, I apologize.
[00:15:16.320 --> 00:15:17.200]   We'll get you in the next one.
[00:15:17.200 --> 00:15:20.400]   This is prompt engineering, right?
[00:15:20.400 --> 00:15:22.880]   And this is like the most popular way
[00:15:22.880 --> 00:15:24.640]   to use large language models right now.
[00:15:24.640 --> 00:15:25.760]   You don't fine tune it.
[00:15:25.760 --> 00:15:26.960]   You don't build it yourself.
[00:15:26.960 --> 00:15:29.040]   You just take something off the shelf
[00:15:29.040 --> 00:15:30.880]   and then figure out how to make it useful.
[00:15:30.880 --> 00:15:34.480]   And I'm assuming almost all of you have fine tuned something,
[00:15:34.480 --> 00:15:36.080]   but I don't want to be speaking past anyone.
[00:15:36.080 --> 00:15:38.240]   So what I actually mean by prompt engineering
[00:15:38.240 --> 00:15:41.360]   is you take a model and instead of building
[00:15:41.360 --> 00:15:43.360]   your own translation model like I would have done--
[00:15:43.360 --> 00:15:44.800]   and I actually did do in grad school
[00:15:44.800 --> 00:15:47.440]   and that work that I did is completely irrelevant now--
[00:15:47.440 --> 00:15:52.320]   instead of doing that, spending years of your life on that,
[00:15:52.320 --> 00:15:54.720]   you just ask the model, hey, could you please translate this
[00:15:54.720 --> 00:15:55.840]   from English into Italian?
[00:15:55.840 --> 00:15:57.920]   And then it does it beautifully.
[00:15:57.920 --> 00:15:58.420]   Amazing.
[00:15:58.420 --> 00:16:00.720]   And then you can do things like solve
[00:16:00.720 --> 00:16:02.880]   almost every classical NLP problem just
[00:16:02.880 --> 00:16:04.960]   by asking a large language model.
[00:16:04.960 --> 00:16:07.120]   So sentiment analysis, another classic problem,
[00:16:07.120 --> 00:16:09.600]   you just say, hey, is this tweet positive or negative?
[00:16:09.600 --> 00:16:13.120]   Instead of doing any classical machine learning.
[00:16:13.360 --> 00:16:14.320]   Or even deep learning.
[00:16:14.320 --> 00:16:18.160]   And then this actually does get subtle and interesting
[00:16:18.160 --> 00:16:19.840]   the second you try to really do this.
[00:16:19.840 --> 00:16:23.360]   So I think one of the most salient and fun papers on this--
[00:16:23.360 --> 00:16:25.120]   some of you might remember this--
[00:16:25.120 --> 00:16:27.840]   is actually doing math question answering and logic question
[00:16:27.840 --> 00:16:30.080]   answering, where they started off
[00:16:30.080 --> 00:16:31.680]   by just asking the question.
[00:16:31.680 --> 00:16:35.200]   And then they added, let's think through this step by step.
[00:16:35.200 --> 00:16:37.200]   It actually causes the model to answer the question step
[00:16:37.200 --> 00:16:40.960]   by step, which amazingly causes the accuracy of the model
[00:16:40.960 --> 00:16:41.920]   to improve a lot.
[00:16:41.920 --> 00:16:45.440]   So that made GPT-3 go from kind of below state of the art
[00:16:45.440 --> 00:16:47.040]   on some of these benchmarks to actually
[00:16:47.040 --> 00:16:48.160]   above the state of the art.
[00:16:48.160 --> 00:16:51.280]   And I think this was part of a big explosion of new ways
[00:16:51.280 --> 00:16:55.360]   to actually frame asking models well to get the answer
[00:16:55.360 --> 00:16:56.480]   that you want.
[00:16:56.480 --> 00:17:00.240]   And so one question that I've been asking myself,
[00:17:00.240 --> 00:17:01.600]   and a lot of people have been asking me,
[00:17:01.600 --> 00:17:04.880]   is does building the best tools for machine learning
[00:17:04.880 --> 00:17:06.080]   practitioners still matter?
[00:17:06.080 --> 00:17:08.240]   This has been kind of our North Star
[00:17:08.240 --> 00:17:09.280]   since we started the company.
[00:17:09.280 --> 00:17:13.120]   And we've really had amazing success doing it.
[00:17:13.120 --> 00:17:16.400]   But you might wonder, we don't need a lot of the methods
[00:17:16.400 --> 00:17:19.040]   that a lot of us studied in school anymore, right?
[00:17:19.040 --> 00:17:21.840]   Like, I think you wouldn't build a standalone translation
[00:17:21.840 --> 00:17:22.800]   system probably.
[00:17:22.800 --> 00:17:25.760]   You wouldn't build a standalone sentiment analysis system
[00:17:25.760 --> 00:17:26.400]   probably.
[00:17:26.400 --> 00:17:28.800]   Do we even need machine learning practitioners?
[00:17:28.800 --> 00:17:33.920]   And I guess the way I look at this--
[00:17:33.920 --> 00:17:36.400]   and maybe this is just optimism--
[00:17:36.400 --> 00:17:39.520]   I guess I argue that the market just massively expanded.
[00:17:39.520 --> 00:17:42.880]   I think every software developer, maybe every person
[00:17:42.880 --> 00:17:45.360]   now, can do machine learning practitioner--
[00:17:45.360 --> 00:17:47.280]   can be a machine learning practitioner the way
[00:17:47.280 --> 00:17:51.280]   we defined it, in the sense that everyone can use machine
[00:17:51.280 --> 00:17:54.160]   learning models for real world applications
[00:17:54.160 --> 00:17:55.600]   without needing a lot of training.
[00:17:55.600 --> 00:17:59.280]   And so what we've seen-- and you really, really feel this
[00:17:59.280 --> 00:18:00.560]   in San Francisco, let me tell you.
[00:18:00.560 --> 00:18:03.120]   In Silicon Valley, you just feel this so acutely.
[00:18:03.120 --> 00:18:05.360]   There's been an absolute explosion
[00:18:05.360 --> 00:18:07.040]   of generative AI companies.
[00:18:07.040 --> 00:18:10.400]   And the demos that these companies do are unbelievable.
[00:18:10.400 --> 00:18:11.360]   They're so powerful.
[00:18:11.360 --> 00:18:13.840]   I used a product called Tome AI the other day,
[00:18:13.840 --> 00:18:16.160]   and I typed in Weights and Biases sales pitch,
[00:18:16.160 --> 00:18:18.000]   and I got out a PowerPoint deck that
[00:18:18.000 --> 00:18:20.400]   was a pretty good Weights and Biases sales pitch.
[00:18:20.400 --> 00:18:23.920]   I mean, just the number of astounding demos is awesome.
[00:18:23.920 --> 00:18:25.680]   And some people might call it hype.
[00:18:25.680 --> 00:18:27.280]   I think there is a sense that this is hype.
[00:18:27.280 --> 00:18:29.840]   But I look at this, and I've been doing this a long time,
[00:18:29.840 --> 00:18:31.520]   and I've seen a lot of hype cycles.
[00:18:32.240 --> 00:18:35.120]   These companies really do do useful things.
[00:18:35.120 --> 00:18:37.920]   And many of them use Weights and Biases,
[00:18:37.920 --> 00:18:40.160]   and we really want to support them in what they're doing.
[00:18:40.160 --> 00:18:41.840]   We think it's incredibly important.
[00:18:41.840 --> 00:18:43.760]   And we've been thinking a lot about this
[00:18:43.760 --> 00:18:44.960]   over the last couple of months.
[00:18:44.960 --> 00:18:47.280]   How can we support these generative AI companies well?
[00:18:47.280 --> 00:18:49.280]   And so I'm actually super excited.
[00:18:49.280 --> 00:18:50.560]   This is such an opportunity.
[00:18:50.560 --> 00:18:52.400]   We've never really done a big product launch.
[00:18:52.400 --> 00:18:54.800]   And this is slightly accidental, but we're really doing it.
[00:18:54.800 --> 00:18:56.480]   We're actually really announcing kind
[00:18:56.480 --> 00:18:58.960]   of the biggest product launch that we've had in the company,
[00:18:58.960 --> 00:19:00.960]   which is Weights and Biases prompts,
[00:19:00.960 --> 00:19:04.000]   which is our new LLMOps tool set.
[00:19:04.000 --> 00:19:06.720]   And so Weights and Biases prompts
[00:19:06.720 --> 00:19:08.640]   includes a lot of new functionality.
[00:19:08.640 --> 00:19:12.240]   And I want to show it to you with a real-world use case
[00:19:12.240 --> 00:19:12.960]   that we really did.
[00:19:12.960 --> 00:19:16.400]   And this use case, which I love--
[00:19:16.400 --> 00:19:17.920]   I don't know if any of you have tried this--
[00:19:17.920 --> 00:19:21.840]   is generating SQL on a database from a question and a schema.
[00:19:21.840 --> 00:19:24.640]   And I've seen in the last 20 years
[00:19:24.640 --> 00:19:26.560]   that I've been doing this, so many variations of this.
[00:19:26.560 --> 00:19:27.840]   And it's always so annoying.
[00:19:27.840 --> 00:19:28.880]   It's always so bad.
[00:19:28.880 --> 00:19:32.240]   It sounds good to just type in text and query your data set,
[00:19:32.240 --> 00:19:33.520]   and it's always so bad.
[00:19:33.520 --> 00:19:36.800]   But in this case, it finally really works.
[00:19:36.800 --> 00:19:37.920]   We've really seen this.
[00:19:37.920 --> 00:19:41.040]   So here we're inputting our schema and a question,
[00:19:41.040 --> 00:19:43.840]   like find the ID of the first product we send a customer.
[00:19:43.840 --> 00:19:46.800]   And what we're hoping for here is this kind of output,
[00:19:46.800 --> 00:19:48.720]   where it's SQL, and then we're going to run the output.
[00:19:48.720 --> 00:19:51.680]   And so I don't know how many of you have actually tried this,
[00:19:51.680 --> 00:19:55.680]   but what this code looks like here is something like this.
[00:19:55.680 --> 00:19:57.440]   It's probably hard to read, but it's Python code.
[00:19:57.440 --> 00:20:00.560]   We have a prompt template, and we have a set of questions.
[00:20:00.560 --> 00:20:03.200]   And then we have a simple integration with OpenAI
[00:20:03.200 --> 00:20:04.320]   done through LangChain.
[00:20:04.320 --> 00:20:07.760]   And then we create a table of the results.
[00:20:07.760 --> 00:20:09.760]   And then actually, the weights and biases integration
[00:20:09.760 --> 00:20:11.600]   is just this one line at the bottom here,
[00:20:11.600 --> 00:20:14.000]   where it's logging what happened.
[00:20:14.000 --> 00:20:16.240]   So it's logging the inputs and the outputs and other things.
[00:20:16.240 --> 00:20:18.720]   And then you get this kind of weights and biases table
[00:20:18.720 --> 00:20:20.880]   that's been upgraded because so many of our customers
[00:20:20.880 --> 00:20:21.520]   are doing this.
[00:20:21.520 --> 00:20:23.680]   So we can see, OK, what was the response here?
[00:20:23.680 --> 00:20:27.200]   So each row here is a different question.
[00:20:27.200 --> 00:20:28.720]   And here's SQL that we're getting back
[00:20:28.720 --> 00:20:30.480]   from asking that question.
[00:20:30.480 --> 00:20:32.000]   And honestly, it looks pretty plausible.
[00:20:32.000 --> 00:20:34.080]   This is really run on real data.
[00:20:34.080 --> 00:20:36.960]   And then we're looking at these giant full model inputs,
[00:20:36.960 --> 00:20:38.800]   because we're actually inputting the full schema.
[00:20:38.800 --> 00:20:40.960]   But we can go through this and look for bugs
[00:20:40.960 --> 00:20:42.080]   that we might be encountering.
[00:20:42.080 --> 00:20:44.480]   And then we're also tracking the model name and temperature.
[00:20:44.480 --> 00:20:46.160]   Because if there's one thing I've learned in my life,
[00:20:46.160 --> 00:20:48.640]   it's like you want to automatically write down
[00:20:48.640 --> 00:20:50.240]   the different things that you're doing.
[00:20:50.240 --> 00:20:53.040]   And prompt engineering is absolutely no different.
[00:20:53.040 --> 00:20:54.800]   A week from now, I'm going to wonder what I did,
[00:20:54.800 --> 00:20:56.400]   and I'm going to be so happy that it's
[00:20:56.400 --> 00:20:58.640]   going to automatically log the temperature of the model
[00:20:58.640 --> 00:20:59.200]   that I was using.
[00:20:59.200 --> 00:21:02.640]   So that seems like it's working.
[00:21:02.640 --> 00:21:04.400]   And then we actually really want to run it.
[00:21:04.400 --> 00:21:06.720]   So LangChain makes this very easy to do.
[00:21:06.720 --> 00:21:10.000]   So our first step in this LangChain iteration
[00:21:10.000 --> 00:21:11.200]   is generating SQL.
[00:21:11.200 --> 00:21:13.680]   And then the next step is we're going to execute that SQL.
[00:21:13.680 --> 00:21:15.760]   OK.
[00:21:15.760 --> 00:21:17.360]   So what this looks like is actually
[00:21:17.360 --> 00:21:19.040]   we're going to show you the integration at the top here.
[00:21:19.040 --> 00:21:22.160]   It's a one-liner of just 1dbtracer.init.
[00:21:22.160 --> 00:21:25.040]   And what that does is it makes it automatically log
[00:21:25.040 --> 00:21:26.960]   everything that LangChain's doing.
[00:21:26.960 --> 00:21:29.120]   And so we're setting up a chain here
[00:21:29.120 --> 00:21:31.200]   where first it's asking the question,
[00:21:31.200 --> 00:21:33.360]   and then it's running the SQL that comes back.
[00:21:33.360 --> 00:21:36.480]   And here's where we feed in the questions,
[00:21:36.480 --> 00:21:38.640]   and we use a template, and we run the chain.
[00:21:38.640 --> 00:21:41.120]   And you can see that you get these cryptic errors here.
[00:21:41.120 --> 00:21:43.680]   And one of LangChain's essentially founders
[00:21:43.680 --> 00:21:45.840]   is here today, so I'm not going to criticize this too much.
[00:21:45.840 --> 00:21:47.520]   It's really hard to make good error messages.
[00:21:47.520 --> 00:21:50.880]   But I think this is actually a lot clearer what's going on,
[00:21:50.880 --> 00:21:52.880]   even though this is still pretty complicated.
[00:21:52.880 --> 00:21:54.320]   So here's the string preview.
[00:21:54.320 --> 00:21:56.320]   So the question was, get my last 10 orders.
[00:21:56.320 --> 00:21:58.320]   Here's the schema that we passed in.
[00:21:58.320 --> 00:22:00.960]   So this was both fed in to a template.
[00:22:00.960 --> 00:22:04.320]   And then I'm looking, and I'm seeing there's no output.
[00:22:04.320 --> 00:22:05.440]   So what's going on?
[00:22:05.440 --> 00:22:07.440]   I can look at the chain that was created,
[00:22:07.440 --> 00:22:08.800]   and I can look at the error messages.
[00:22:08.800 --> 00:22:10.400]   And actually, I'm getting different errors
[00:22:10.400 --> 00:22:13.120]   on different runs of this LangChain.
[00:22:13.120 --> 00:22:15.680]   And so you can go into this tracer.
[00:22:15.680 --> 00:22:17.280]   It actually looks a lot like a stack trace
[00:22:17.280 --> 00:22:19.760]   for the engineers in the audience,
[00:22:19.760 --> 00:22:21.120]   and it has two steps in that trace.
[00:22:21.120 --> 00:22:24.000]   So there's an OpenAI step where it's asking the question
[00:22:24.000 --> 00:22:24.640]   with the prompt.
[00:22:24.640 --> 00:22:28.560]   Here's my database schema, and here's my question.
[00:22:28.560 --> 00:22:32.480]   And if you look, you can see the output here
[00:22:32.480 --> 00:22:33.840]   is giving us a bunch of metadata,
[00:22:33.840 --> 00:22:35.920]   so a number of tokens and the tokens used.
[00:22:35.920 --> 00:22:37.360]   And then the transform chain step,
[00:22:37.360 --> 00:22:39.440]   which actually is running it, gives us an error.
[00:22:39.440 --> 00:22:43.040]   And in this case, the error is that the object is an iterable,
[00:22:43.040 --> 00:22:44.720]   and it turns out kind of a dumb error.
[00:22:44.720 --> 00:22:48.880]   In this case, the language model actually sent me empty data.
[00:22:48.880 --> 00:22:50.400]   And these are some of the weird quirks
[00:22:50.400 --> 00:22:51.360]   of the OpenAI integration.
[00:22:51.360 --> 00:22:52.720]   If you send it too short of a prompt
[00:22:52.720 --> 00:22:54.480]   or too long of a prompt, it can just error out
[00:22:54.480 --> 00:22:55.520]   and send you nothing.
[00:22:55.520 --> 00:22:57.360]   And then downstream, you get these bizarre errors
[00:22:57.360 --> 00:22:58.800]   that take you forever to debug.
[00:22:58.800 --> 00:23:01.200]   So again, we're just trying to solve
[00:23:01.200 --> 00:23:02.640]   the simple problems like that,
[00:23:02.640 --> 00:23:05.200]   and we're giving you a lot of easy access
[00:23:05.200 --> 00:23:06.560]   into everything that's going on.
[00:23:06.560 --> 00:23:08.880]   Most of it you'll never need, but when you do need it,
[00:23:08.880 --> 00:23:10.160]   you'll be happy that you had it.
[00:23:10.160 --> 00:23:15.840]   So then here was the idea to make this better, right?
[00:23:15.840 --> 00:23:18.000]   So it turned out some of the SQLs malformed.
[00:23:18.000 --> 00:23:20.640]   And incredibly, the solution here
[00:23:20.640 --> 00:23:24.480]   was to generate the SQL, then go back and ask the same LLM,
[00:23:24.480 --> 00:23:25.600]   "Okay, you generated the SQL.
[00:23:25.600 --> 00:23:27.840]   Could you please debug your own SQL and make it better?"
[00:23:27.840 --> 00:23:30.800]   And then execute that SQL.
[00:23:30.800 --> 00:23:32.480]   So that's the new lang chain.
[00:23:32.480 --> 00:23:34.080]   I guess this is state of the art.
[00:23:34.080 --> 00:23:36.320]   And here we're creating these steps here.
[00:23:36.320 --> 00:23:42.800]   And you can see we're inserting the clean SQL chain,
[00:23:42.800 --> 00:23:44.720]   which says, "Please correct any syntax errors
[00:23:44.720 --> 00:23:45.680]   in the following SQL,"
[00:23:45.680 --> 00:23:47.440]   which you actually just generated for me.
[00:23:47.440 --> 00:23:50.160]   And then you can see here that actually
[00:23:50.160 --> 00:23:51.280]   all these runs are successful.
[00:23:51.280 --> 00:23:52.640]   So the output is true.
[00:23:52.640 --> 00:23:56.800]   You can actually see that the inputs here are the same,
[00:23:56.800 --> 00:23:58.400]   but there's kind of this intermediate result.
[00:23:58.400 --> 00:24:01.520]   And it's asking, "Find the top one customers
[00:24:01.520 --> 00:24:02.560]   that have spent the most money."
[00:24:02.560 --> 00:24:04.080]   The SQLs right here really is real.
[00:24:04.080 --> 00:24:06.160]   And I'll tell you, this actually is the right output
[00:24:06.160 --> 00:24:07.600]   on this data set.
[00:24:07.600 --> 00:24:10.960]   So this model worked at least these three times
[00:24:10.960 --> 00:24:12.800]   that we are showing it to you here.
[00:24:12.800 --> 00:24:17.600]   And again, you can see actually a little bit more
[00:24:17.600 --> 00:24:19.440]   of a complicated stack trace
[00:24:19.440 --> 00:24:21.360]   because there's three steps here.
[00:24:21.360 --> 00:24:24.800]   You can see the output here is SQL.
[00:24:24.800 --> 00:24:25.600]   And look at that dot.
[00:24:25.600 --> 00:24:27.280]   For some reason, it put that period in there.
[00:24:27.280 --> 00:24:28.880]   Again, these like maddening little errors
[00:24:28.880 --> 00:24:30.320]   that are so frustrating to debug.
[00:24:30.320 --> 00:24:32.800]   And then it's going to pass that into this step
[00:24:32.800 --> 00:24:34.720]   where it sees that period and it takes it out.
[00:24:34.720 --> 00:24:36.720]   And actually, it cleans up the formatting slightly
[00:24:36.720 --> 00:24:38.160]   if you look closely, which I think is also
[00:24:38.160 --> 00:24:40.880]   kind of a delightful feature of that step.
[00:24:40.880 --> 00:24:44.080]   And then it runs it and gets a useful output.
[00:24:44.080 --> 00:24:45.840]   OK.
[00:24:45.840 --> 00:24:47.440]   So then the kind of final thing here
[00:24:47.440 --> 00:24:50.400]   is what do you do when you want to explore what happened?
[00:24:50.400 --> 00:24:52.160]   So this is-- I mean, I'm giving you the kind
[00:24:52.160 --> 00:24:54.000]   of fun debugging steps.
[00:24:54.000 --> 00:24:56.400]   But the reality is you try 1,000 things.
[00:24:56.400 --> 00:24:58.720]   And you kind of like build up this big set of things
[00:24:58.720 --> 00:24:59.120]   that you tried.
[00:24:59.120 --> 00:25:01.040]   And you kind of then want to go back and look at, OK,
[00:25:01.040 --> 00:25:02.160]   what worked and what didn't.
[00:25:02.160 --> 00:25:04.080]   And actually, in the case of LLMs,
[00:25:04.080 --> 00:25:06.320]   even working is a little bit ill-defined.
[00:25:06.320 --> 00:25:09.040]   It's often outputting something that's maybe sort of right
[00:25:09.040 --> 00:25:10.720]   or you want to make a little bit better.
[00:25:10.720 --> 00:25:13.520]   And so we've built this really flexible table here,
[00:25:13.520 --> 00:25:17.360]   which you can query with essentially arbitrary code.
[00:25:17.360 --> 00:25:20.400]   You can do grouping and filtering.
[00:25:20.400 --> 00:25:29.840]   And so for example, here we're grouping by whether we're
[00:25:29.840 --> 00:25:30.480]   successful or not.
[00:25:30.480 --> 00:25:32.480]   So actually, even though these all look successful,
[00:25:32.480 --> 00:25:33.840]   we can find all the errors and see
[00:25:33.840 --> 00:25:38.080]   if we see patterns in the failures that are happening.
[00:25:38.080 --> 00:25:41.040]   We can also kind of create new columns that
[00:25:41.040 --> 00:25:42.320]   are created from other columns.
[00:25:42.320 --> 00:25:44.640]   So for example, here, after the fact,
[00:25:44.640 --> 00:25:46.720]   we're creating a column which asks ourselves,
[00:25:46.720 --> 00:25:49.440]   does our prompt contain orders?
[00:25:49.440 --> 00:25:52.720]   So it's saying some of our queries included orders,
[00:25:52.720 --> 00:25:53.280]   some didn't.
[00:25:53.280 --> 00:25:55.120]   We want to filter down just to those.
[00:25:55.120 --> 00:25:57.680]   So the ones that actually has orders are marked as true.
[00:25:57.680 --> 00:25:59.640]   And then we're going to group in the same way,
[00:25:59.640 --> 00:26:04.040]   kind of chaining together different steps here.
[00:26:04.040 --> 00:26:08.800]   And then we can analyze this data.
[00:26:08.800 --> 00:26:10.840]   So we're super excited about that.
[00:26:10.840 --> 00:26:13.600]   We've also built an OpenAI integration
[00:26:13.600 --> 00:26:16.240]   that looks almost the same as the Langtan integration
[00:26:16.240 --> 00:26:17.880]   but has a slightly different structure.
[00:26:17.880 --> 00:26:20.360]   But again, one line to kind of automatically get
[00:26:20.360 --> 00:26:22.760]   all the information that happens as you do the OpenAI
[00:26:22.760 --> 00:26:23.680]   integration.
[00:26:23.680 --> 00:26:26.840]   And we've also created a way to quickly evaluate your models.
[00:26:26.840 --> 00:26:31.080]   And what we did here was we wrapped OpenAI evals.
[00:26:31.080 --> 00:26:34.040]   Because so many people have been running this question of,
[00:26:34.040 --> 00:26:35.840]   how do I know if a model is working or not?
[00:26:35.840 --> 00:26:38.240]   I showed you that I properly generated SQL,
[00:26:38.240 --> 00:26:40.120]   but did it give me the right answer?
[00:26:40.120 --> 00:26:41.560]   It's kind of an open question.
[00:26:41.560 --> 00:26:45.680]   I had a funny interview with the CEO of Repl.it that built an LLM.
[00:26:45.680 --> 00:26:47.600]   And he told me that all the testing that he does
[00:26:47.600 --> 00:26:50.840]   is testing by vibes, which is really
[00:26:50.840 --> 00:26:52.000]   an interesting concept.
[00:26:52.000 --> 00:26:54.340]   I think he freaked out some of the engineers at Weights
[00:26:54.340 --> 00:26:55.040]   and Biases.
[00:26:55.040 --> 00:26:56.840]   But people are trying to get away from that.
[00:26:56.840 --> 00:27:00.240]   And OpenAI has built this really great framework, OpenAI evals.
[00:27:00.240 --> 00:27:01.680]   But many people haven't used it.
[00:27:01.680 --> 00:27:03.220]   And not because it's a bad framework,
[00:27:03.220 --> 00:27:06.120]   but I think people are busy, and these integrations are hard.
[00:27:06.120 --> 00:27:09.440]   So what we wanted to do was build a very, very simple way
[00:27:09.440 --> 00:27:12.360]   to run OpenAI evaluations.
[00:27:12.360 --> 00:27:14.240]   And we do it here with our launch products,
[00:27:14.240 --> 00:27:16.960]   where we actually send something into a job queue.
[00:27:16.960 --> 00:27:18.700]   So you're basically taking your data set
[00:27:18.700 --> 00:27:20.400]   of the data that got generated, and we
[00:27:20.400 --> 00:27:22.200]   help you send it right into OpenAI evals.
[00:27:22.200 --> 00:27:25.040]   And we actually just run that for you.
[00:27:25.040 --> 00:27:27.160]   And so you get back these kind of beautiful charts
[00:27:27.160 --> 00:27:29.500]   where you can look at how well your different models are
[00:27:29.500 --> 00:27:34.400]   working, and hopefully know if your models are improving
[00:27:34.400 --> 00:27:37.400]   or degrading as you change your prompts.
[00:27:37.400 --> 00:27:39.560]   So we have one more demo.
[00:27:39.560 --> 00:27:41.240]   And actually, I just want to say,
[00:27:41.240 --> 00:27:42.800]   all these things that I'm showing you
[00:27:42.800 --> 00:27:46.160]   were made by engineers, mostly back in the SF office.
[00:27:46.160 --> 00:27:47.740]   They worked super, super hard on this.
[00:27:47.740 --> 00:27:49.360]   And this is, I think, really excellent.
[00:27:49.360 --> 00:27:52.680]   But for the last demo, we actually
[00:27:52.680 --> 00:27:55.800]   have the author of the demo, my co-founder, Chris,
[00:27:55.800 --> 00:27:57.560]   who built this to show it to you.
[00:27:57.560 --> 00:28:00.960]   [APPLAUSE]
[00:28:00.960 --> 00:28:06.240]   That's right.
[00:28:06.240 --> 00:28:09.640]   This bee can write JavaScript.
[00:28:09.640 --> 00:28:12.920]   We're releasing our JavaScript SDK
[00:28:12.920 --> 00:28:15.280]   for working with tools like LangChain
[00:28:15.280 --> 00:28:17.000]   and easily building apps.
[00:28:17.000 --> 00:28:20.520]   So it's npm install wnv-sdk.
[00:28:20.520 --> 00:28:21.960]   There might be a couple bugs.
[00:28:21.960 --> 00:28:23.280]   Probably not, though.
[00:28:23.280 --> 00:28:24.360]   If there are, let me know.
[00:28:24.360 --> 00:28:25.680]   We'll fix it.
[00:28:25.680 --> 00:28:27.520]   Next slide, Luke.
[00:28:27.520 --> 00:28:29.040]   So this is it in action.
[00:28:29.040 --> 00:28:32.720]   You just paste two lines, very similar to the Python API.
[00:28:32.720 --> 00:28:35.660]   So you bring in the tracer, you write your LangChain,
[00:28:35.660 --> 00:28:38.040]   and you finish your tracer.
[00:28:38.040 --> 00:28:40.560]   In this case, we're doing a SQL generation task again.
[00:28:40.560 --> 00:28:48.440]   I have a database of a bunch of CDs and tracks.
[00:28:48.440 --> 00:28:50.440]   And we can ask the question, what genres
[00:28:50.440 --> 00:28:51.640]   are the most popular?
[00:28:51.640 --> 00:28:54.720]   Or which artists released the most tracks?
[00:28:54.720 --> 00:28:58.280]   Or what goofballs zinged the most zoinks?
[00:28:58.280 --> 00:29:03.120]   I was trying to get it to not work, but it worked.
[00:29:03.120 --> 00:29:05.120]   It hallucinated.
[00:29:05.120 --> 00:29:07.360]   So here, yeah, we could see the SQL actually
[00:29:07.360 --> 00:29:11.720]   getting generated here, just like we did in the Python SDK.
[00:29:11.720 --> 00:29:13.120]   And if we refresh, we can finally
[00:29:13.120 --> 00:29:18.120]   get the answer to what goofballs zinged the most zoinks.
[00:29:18.120 --> 00:29:21.020]   The query orders by Zorks zinged.
[00:29:21.020 --> 00:29:25.160]   And the answer is Iron Maiden, U2, Led Zeppelin, Metallica,
[00:29:25.160 --> 00:29:26.400]   and Lost.
[00:29:26.400 --> 00:29:27.360]   So there you have it.
[00:29:27.360 --> 00:29:28.760]   Play with WNV-SDK.
[00:29:28.760 --> 00:29:29.400]   Thanks, everyone.
[00:29:29.400 --> 00:29:32.800]   [APPLAUSE]
[00:29:36.680 --> 00:29:37.180]   All right.
[00:29:37.180 --> 00:29:44.880]   So in summary, we've just shown you a whole bunch of stuff.
[00:29:44.880 --> 00:29:48.080]   We've shown you improved flexible tables
[00:29:48.080 --> 00:29:50.160]   for doing language exploration.
[00:29:50.160 --> 00:29:52.920]   We've shown you a new prompt tracer.
[00:29:52.920 --> 00:29:54.760]   We've shown you a LangTan integration,
[00:29:54.760 --> 00:29:57.760]   OpenAI integration, a way to launch OpenAI Vals,
[00:29:57.760 --> 00:29:59.800]   and a JavaScript integration.
[00:29:59.800 --> 00:30:02.560]   And I just want to say, none of this is vaporware.
[00:30:02.560 --> 00:30:04.280]   This is all live right now.
[00:30:04.280 --> 00:30:06.880]   If you go to this link, WNV.me/LMs,
[00:30:06.880 --> 00:30:09.320]   you can actually use all this stuff immediately.
[00:30:09.320 --> 00:30:10.560]   And I really hope you use it.
[00:30:10.560 --> 00:30:12.600]   And I really hope that you find it useful.
[00:30:12.600 --> 00:30:15.240]   We're super excited to launch this.
[00:30:15.240 --> 00:30:18.640]   [APPLAUSE]
[00:30:18.640 --> 00:30:26.640]   We're also-- I have to say, we're really hard at work.
[00:30:26.640 --> 00:30:28.680]   And we have lots of ideas about what's next.
[00:30:28.680 --> 00:30:30.400]   But we love collaborating.
[00:30:30.400 --> 00:30:32.400]   And I'll give you a real world example.
[00:30:32.400 --> 00:30:35.960]   This user on Twitter is asking for an integration
[00:30:35.960 --> 00:30:39.400]   to 1DB Sweeps from an LLM project.
[00:30:39.400 --> 00:30:41.600]   And our team jumped on this immediately.
[00:30:41.600 --> 00:30:43.160]   So we really are responsive.
[00:30:43.160 --> 00:30:44.560]   And we really are listening.
[00:30:44.560 --> 00:30:46.360]   And we'd really love to know what you need.
[00:30:46.360 --> 00:30:48.680]   So please come up to us and tell us what you need.
[00:30:48.680 --> 00:30:50.580]   Or go to us on Twitter and ask us
[00:30:50.580 --> 00:30:54.000]   for things that will make your life easy as you do LLM prompt
[00:30:54.000 --> 00:30:56.280]   engineering or build LLMs from scratch.
[00:30:56.280 --> 00:30:57.200]   Thank you.
[00:30:57.200 --> 00:30:59.240]   [APPLAUSE]
[00:30:59.240 --> 00:31:00.240]   Back to Lavanya.
[00:31:00.240 --> 00:31:03.680]   [APPLAUSE]
[00:31:03.680 --> 00:31:05.160]   All right.
[00:31:05.160 --> 00:31:08.080]   I hope you guys are excited about the new product.
[00:31:08.080 --> 00:31:11.240]   Next up, we have Imad from Stability,
[00:31:11.240 --> 00:31:13.040]   who is going to give a talk.
[00:31:13.040 --> 00:31:15.000]   [APPLAUSE]
[00:31:15.000 --> 00:31:15.960]   Thank you very much.
[00:31:15.960 --> 00:31:22.720]   Hi, everyone.
[00:31:22.720 --> 00:31:26.440]   Gosh, there's a few people here, eh?
[00:31:26.440 --> 00:31:28.000]   It's an exciting time.
[00:31:28.000 --> 00:31:31.160]   I don't think anyone's ever seen anything quite like this.
[00:31:31.160 --> 00:31:34.320]   I just found out today that it's not actually 1DB.
[00:31:34.320 --> 00:31:35.680]   It's 1B.
[00:31:35.680 --> 00:31:38.200]   That's why there's a B with a wand.
[00:31:38.200 --> 00:31:40.160]   So my apologies for thinking it was a database.
[00:31:40.160 --> 00:31:41.320]   I always get a bit confused.
[00:31:41.320 --> 00:31:42.000]   No, really.
[00:31:42.000 --> 00:31:45.000]   Thank you, Lucas, for having me here.
[00:31:45.000 --> 00:31:46.440]   And I think this is the key thing.
[00:31:46.440 --> 00:31:47.640]   It's magic, right?
[00:31:47.640 --> 00:31:50.120]   Any sufficiently advanced technology is magic.
[00:31:50.120 --> 00:31:53.160]   And I think we've all here kind of seen that.
[00:31:53.160 --> 00:31:55.320]   One of the things I was commenting on earlier is,
[00:31:55.320 --> 00:31:57.400]   does anyone know a single smart person that
[00:31:57.400 --> 00:32:00.320]   isn't excited and terrified by this?
[00:32:00.320 --> 00:32:01.680]   No, we all know in our circles.
[00:32:01.680 --> 00:32:03.680]   Like, you know, with crypto, it was like, well, you know,
[00:32:03.680 --> 00:32:04.880]   there are people that are excited about it.
[00:32:04.880 --> 00:32:06.200]   But a whole bunch of people are like, eh.
[00:32:06.200 --> 00:32:07.480]   Whereas this is everyone.
[00:32:07.480 --> 00:32:09.040]   But it's a bit like no true Scotsman.
[00:32:09.040 --> 00:32:11.640]   If they aren't excited about it now,
[00:32:11.640 --> 00:32:12.840]   then they're not intelligent.
[00:32:12.840 --> 00:32:13.720]   So you know.
[00:32:13.720 --> 00:32:15.600]   That's a good test of all your friends, right?
[00:32:15.600 --> 00:32:17.520]   But even my mom's asking me about it constantly.
[00:32:17.520 --> 00:32:18.680]   Not just because I'm a CEO.
[00:32:18.680 --> 00:32:19.960]   She's like, Imad, what's this?
[00:32:19.960 --> 00:32:22.720]   And she's like making me memes with the new image models,
[00:32:22.720 --> 00:32:26.240]   saying, why don't you call me enough?
[00:32:26.240 --> 00:32:28.160]   Again, the superpowers that you have, I think,
[00:32:28.160 --> 00:32:30.360]   are going to be quite insane.
[00:32:30.360 --> 00:32:32.000]   So again, thank you all for being here.
[00:32:32.000 --> 00:32:32.500]   Let's see.
[00:32:32.500 --> 00:32:33.800]   Do I have to press this button?
[00:32:33.800 --> 00:32:34.800]   Hey, stability AI.
[00:32:34.800 --> 00:32:35.600]   There we go.
[00:32:35.600 --> 00:32:36.880]   End of the world as we know it.
[00:32:36.880 --> 00:32:37.380]   Oh, gosh.
[00:32:37.380 --> 00:32:39.280]   That's a bit bad.
[00:32:39.280 --> 00:32:40.880]   I think that's where we are, though.
[00:32:40.880 --> 00:32:41.380]   Oops.
[00:32:41.380 --> 00:32:42.420]   This is a bit of feedback.
[00:32:42.420 --> 00:32:45.280]   We are at the end of the world as we know it.
[00:32:45.280 --> 00:32:48.160]   One of the ways I like to put it is that everyone's here
[00:32:48.160 --> 00:32:49.200]   because of a story.
[00:32:49.200 --> 00:32:51.960]   You all love kind of ML, or you're terrified by it.
[00:32:51.960 --> 00:32:55.600]   In which case, you know, just please bother Lucas about it,
[00:32:55.600 --> 00:32:58.080]   because he's enabling it, not me.
[00:32:58.080 --> 00:33:01.240]   And that's how we kind of scale the society.
[00:33:01.240 --> 00:33:02.040]   We first of all--
[00:33:02.040 --> 00:33:04.280]   ooh.
[00:33:04.280 --> 00:33:05.800]   Maybe if I step this way.
[00:33:05.800 --> 00:33:09.080]   First of all, we kind of told stories, and we had our tribes.
[00:33:09.080 --> 00:33:10.560]   And then we went to 150 people.
[00:33:10.560 --> 00:33:14.680]   Then we told stories like money and other things like that.
[00:33:14.680 --> 00:33:17.240]   And then we kept going until we got the Gutenberg Press.
[00:33:17.240 --> 00:33:20.160]   And then we took the stories and we put them down.
[00:33:20.160 --> 00:33:21.780]   And that allowed us to scale even more.
[00:33:21.780 --> 00:33:24.400]   And it allowed us to create corporations and organizations
[00:33:24.400 --> 00:33:26.840]   with bylaws and constitutions.
[00:33:26.840 --> 00:33:30.240]   And organizations right now are slow, dumb AI.
[00:33:30.240 --> 00:33:32.680]   They are slow, dumb AI that has to make us legible.
[00:33:32.680 --> 00:33:34.560]   So they take away our creativity and freedom.
[00:33:34.560 --> 00:33:36.760]   And we're kind of sad as a result of that,
[00:33:36.760 --> 00:33:41.700]   because text is a lossy information format.
[00:33:41.700 --> 00:33:43.880]   And I think that's something that's quite important,
[00:33:43.880 --> 00:33:44.920]   because the easiest way for us to communicate
[00:33:44.920 --> 00:33:46.200]   is what we're doing right now--
[00:33:46.200 --> 00:33:47.000]   speech.
[00:33:47.000 --> 00:33:48.240]   Then the next hardest is text.
[00:33:48.240 --> 00:33:50.840]   Writing a good blog post or report is kind of hard.
[00:33:50.840 --> 00:33:52.680]   Anyone here that's used Salesforce, I mean,
[00:33:52.680 --> 00:33:54.960]   we're trying to make it easy, but it's painful, right?
[00:33:54.960 --> 00:33:57.800]   I can feel physical pain.
[00:33:57.800 --> 00:33:59.080]   Maybe that's a bit weird to me.
[00:33:59.080 --> 00:34:01.000]   But then image is the most difficult thing.
[00:34:01.000 --> 00:34:03.280]   Like, creating this just a little bit ago,
[00:34:03.280 --> 00:34:05.660]   none of us would have imagined that we could have done it,
[00:34:05.660 --> 00:34:09.240]   except for a few, because we didn't quite have the tools.
[00:34:09.240 --> 00:34:12.720]   But now visual communication, be it a PowerPoint presentation--
[00:34:12.720 --> 00:34:16.840]   glad to be an investor in Tome, for example, and a partner--
[00:34:16.840 --> 00:34:18.560]   is available to everyone, be it this,
[00:34:18.560 --> 00:34:20.320]   be it illustration, other things.
[00:34:20.320 --> 00:34:22.240]   That's why Bill Gates--
[00:34:22.240 --> 00:34:24.080]   he said AI is going to be the hottest topic.
[00:34:24.080 --> 00:34:26.280]   It's as important as the PC, as the internet.
[00:34:26.280 --> 00:34:28.700]   Now, I've been lucky enough to talk to some of the leaders.
[00:34:28.700 --> 00:34:30.000]   And again, they're all on this.
[00:34:30.000 --> 00:34:32.280]   Jeff versus Tome, it was as important as fire.
[00:34:32.280 --> 00:34:33.440]   And I was like, tablets?
[00:34:33.440 --> 00:34:34.600]   He's like, no, proper fire.
[00:34:34.600 --> 00:34:35.360]   I was like, cool.
[00:34:35.360 --> 00:34:37.880]   [LAUGHTER]
[00:34:37.880 --> 00:34:40.200]   And so I think I've seen this repeated again and again,
[00:34:40.200 --> 00:34:42.760]   because everyone feels that we're at this peak.
[00:34:42.760 --> 00:34:45.480]   Actually, one of the most interesting things is this.
[00:34:45.480 --> 00:34:47.920]   When I talk to some of the smartest people in the world--
[00:34:47.920 --> 00:34:50.160]   it's always good to talk to people smarter than you--
[00:34:50.160 --> 00:34:52.640]   it's a case of-- a lot of them say the same thing.
[00:34:52.640 --> 00:34:55.160]   I can't see into the future anymore.
[00:34:55.160 --> 00:34:57.240]   Because, for example, Jeff, as I said,
[00:34:57.240 --> 00:34:59.080]   Amazon had this amazing thing, which is--
[00:34:59.080 --> 00:35:01.080]   and this is some advice to all the founders.
[00:35:01.080 --> 00:35:02.700]   When you're building a great business,
[00:35:02.700 --> 00:35:06.080]   you have to look at the inevitable and the unchanging.
[00:35:06.080 --> 00:35:09.160]   Selling books is a way to go onto the full internet,
[00:35:09.160 --> 00:35:11.520]   because the internet has an advantage on books.
[00:35:11.520 --> 00:35:14.420]   But the unchanging is the need for an excellent consumer
[00:35:14.420 --> 00:35:15.160]   experience.
[00:35:15.160 --> 00:35:17.320]   Create great products, right?
[00:35:17.320 --> 00:35:18.920]   And so you can kind of see that.
[00:35:18.920 --> 00:35:21.120]   I see this technology now as an inevitable,
[00:35:21.120 --> 00:35:23.080]   and I see the unchanging elements,
[00:35:23.080 --> 00:35:25.440]   but I have no idea where we are a year from now.
[00:35:25.440 --> 00:35:26.720]   I don't think anyone here does.
[00:35:26.720 --> 00:35:29.320]   And if you do, then please give me a call after,
[00:35:29.320 --> 00:35:31.200]   because I really need some guidance, right?
[00:35:31.200 --> 00:35:32.880]   Need some help navigating this.
[00:35:32.880 --> 00:35:35.160]   Let alone five years, 10 years, because we're not
[00:35:35.160 --> 00:35:37.520]   following Moore's law, right?
[00:35:37.520 --> 00:35:40.320]   We're not following the exponential.
[00:35:40.320 --> 00:35:42.720]   This is why I don't sleep.
[00:35:42.720 --> 00:35:44.080]   So I need a bit of help with that.
[00:35:44.080 --> 00:35:48.160]   This is the number of ML and AI papers on ARCHIVE each month.
[00:35:48.160 --> 00:35:49.560]   It's a literal exponential.
[00:35:49.560 --> 00:35:53.160]   Now, 80% of that is foundation model AI,
[00:35:53.160 --> 00:35:55.560]   which is a bit crazy, right?
[00:35:55.560 --> 00:35:57.680]   It's because what we're getting is network effects.
[00:35:57.680 --> 00:36:00.480]   These models are good enough, fast enough, and cheap enough.
[00:36:00.480 --> 00:36:04.160]   And so you're seeing everything everywhere all at once.
[00:36:04.160 --> 00:36:05.480]   This is also hands, right?
[00:36:05.480 --> 00:36:09.520]   Hands are difficult. With people kind of having innovation
[00:36:09.520 --> 00:36:13.400]   and sharing it back and forth, using things like WANdB
[00:36:13.400 --> 00:36:16.160]   and kind of others to keep track,
[00:36:16.160 --> 00:36:18.880]   using Hugging Face and kind of other great partners.
[00:36:18.880 --> 00:36:20.160]   I don't think this is going to stop,
[00:36:20.160 --> 00:36:22.640]   because we're just starting to explore this latent space.
[00:36:22.640 --> 00:36:24.360]   We're at the iPhone moment.
[00:36:24.360 --> 00:36:27.080]   We're not at the iPhone 10 moment,
[00:36:27.080 --> 00:36:30.440]   let alone whatever's going to come next,
[00:36:30.440 --> 00:36:32.840]   because we're just starting to do one-to-one interaction.
[00:36:32.840 --> 00:36:34.960]   Like we've seen some of this agent-based stuff.
[00:36:34.960 --> 00:36:35.840]   We've seen swarms.
[00:36:35.840 --> 00:36:37.560]   We've seen all of this.
[00:36:37.560 --> 00:36:39.920]   But we're still one-to-one with much of this technology.
[00:36:39.920 --> 00:36:42.280]   And I think the most exciting thing is yet to come,
[00:36:42.280 --> 00:36:45.360]   as we scratch past that surface and we really dig inside.
[00:36:45.360 --> 00:36:47.160]   But we need the right building blocks for it,
[00:36:47.160 --> 00:36:50.880]   because again, you need to make sense of this chaos.
[00:36:50.880 --> 00:36:54.360]   Even as the internet took the cost of information to nothing
[00:36:54.360 --> 00:36:56.920]   and journal-to-AI took the cost of creation to nothing.
[00:36:56.920 --> 00:36:59.480]   What does this mean, and how do we think about it?
[00:36:59.480 --> 00:37:02.440]   I think the way that you think about it is that these models
[00:37:02.440 --> 00:37:08.440]   are really talented grads that occasionally go off their meds.
[00:37:08.440 --> 00:37:10.440]   So thinking about yourself, thinking about it
[00:37:10.440 --> 00:37:13.280]   for companies that you help, your own lives,
[00:37:13.280 --> 00:37:17.280]   what would you do if you had an army of really talented grads
[00:37:17.280 --> 00:37:18.920]   that occasionally go off their meds?
[00:37:18.920 --> 00:37:20.360]   You'd get them to watch each other.
[00:37:20.360 --> 00:37:22.080]   You'd get them to understand each other.
[00:37:22.080 --> 00:37:23.360]   And that's where the disruption happens,
[00:37:23.360 --> 00:37:25.600]   because a really talented grad is really hard to find.
[00:37:25.600 --> 00:37:29.640]   These models are really good at following instructions.
[00:37:29.640 --> 00:37:33.120]   Like the new 32,000 token context window,
[00:37:33.120 --> 00:37:35.640]   version of GPT-4, that's like 20,000 words,
[00:37:35.640 --> 00:37:38.080]   like 50 pages of writing or something.
[00:37:38.080 --> 00:37:39.720]   But anyway, it's a lot.
[00:37:39.720 --> 00:37:42.160]   That's the entire instruction manual that follows it.
[00:37:42.160 --> 00:37:44.040]   But here's the other interesting thing.
[00:37:44.040 --> 00:37:47.280]   Organizations are slow, dumb AI that optimize and turn us
[00:37:47.280 --> 00:37:48.600]   into these automatons.
[00:37:48.600 --> 00:37:52.080]   School is basically child care mixed with a social status
[00:37:52.080 --> 00:37:54.600]   game mixed with a Petri dish.
[00:37:54.600 --> 00:37:57.840]   And they're designed to take away our creativity
[00:37:57.840 --> 00:37:59.920]   and tell us we cannot be creative.
[00:37:59.920 --> 00:38:01.420]   All of you that had graduate jobs,
[00:38:01.420 --> 00:38:03.240]   you were told do not be creative,
[00:38:03.240 --> 00:38:05.240]   because you need experience first.
[00:38:05.240 --> 00:38:07.840]   And we start out creative, have it knocked out of us,
[00:38:07.840 --> 00:38:10.360]   and then we're allowed to be creative later on.
[00:38:10.360 --> 00:38:12.760]   Until then, we're told to consume.
[00:38:12.760 --> 00:38:15.280]   This is something that I think is a super interesting kind
[00:38:15.280 --> 00:38:20.040]   of paradigm, because these models are creative.
[00:38:20.040 --> 00:38:21.000]   It is not collaging.
[00:38:21.000 --> 00:38:22.960]   It is not this.
[00:38:22.960 --> 00:38:26.440]   When you actually use them properly, in my opinion, what
[00:38:26.440 --> 00:38:29.640]   you do is you're saying, like, I use GPT-4 to combat me
[00:38:29.640 --> 00:38:30.400]   on various things.
[00:38:30.400 --> 00:38:34.240]   I'm like, you are a really good answerer who
[00:38:34.240 --> 00:38:36.560]   does constructive criticism, but you're not
[00:38:36.560 --> 00:38:38.160]   afraid to be direct.
[00:38:38.160 --> 00:38:40.160]   And then I just get to challenge my assumptions
[00:38:40.160 --> 00:38:42.320]   and priors, and it's awesome.
[00:38:42.320 --> 00:38:43.960]   Because it's a bit lonely being a CEO.
[00:38:43.960 --> 00:38:47.160]   It's also a really good therapist, I have to say.
[00:38:47.160 --> 00:38:49.400]   No judgment from it, unless you tell it to be judgy,
[00:38:49.400 --> 00:38:52.080]   in which case it's really judgy.
[00:38:52.080 --> 00:38:54.120]   So I think this is one of the interesting things,
[00:38:54.120 --> 00:38:56.320]   because when you have the base deep learning models,
[00:38:56.320 --> 00:38:58.440]   they start off incredibly creative.
[00:38:58.440 --> 00:39:01.160]   Hallucinations are not hallucinations.
[00:39:01.160 --> 00:39:04.440]   Stable diffusion took 100,000 gigabytes of images,
[00:39:04.440 --> 00:39:06.200]   and the output was a 2 gigabyte file.
[00:39:06.200 --> 00:39:07.200]   That is not compression.
[00:39:07.200 --> 00:39:08.840]   If it's compression, then I'm currently
[00:39:08.840 --> 00:39:13.120]   worth a trillion dollars, better than Pied Piper and all that.
[00:39:13.120 --> 00:39:16.680]   Instead, what it is is principal understanding and following.
[00:39:16.680 --> 00:39:18.320]   And so it's this creative element
[00:39:18.320 --> 00:39:19.760]   that was unprecedented, the ability
[00:39:19.760 --> 00:39:21.360]   to do principal-based action.
[00:39:21.360 --> 00:39:24.160]   Again, something like a GPT-4 is trillions of parameters
[00:39:24.160 --> 00:39:25.680]   that fits on a couple of GPUs.
[00:39:25.680 --> 00:39:27.920]   And so I think maybe we're looking at the wrong things
[00:39:27.920 --> 00:39:30.480]   here, because we're trying to get them into expert systems.
[00:39:30.480 --> 00:39:33.560]   So we take these really smart graphs that occasionally
[00:39:33.560 --> 00:39:36.200]   go off their meds, we feed it with junk food,
[00:39:36.200 --> 00:39:38.480]   and then we RLHF it back to being human.
[00:39:38.480 --> 00:39:40.760]   They look a bit disheveled, and you have to tidy it up,
[00:39:40.760 --> 00:39:42.520]   and then you bring it back.
[00:39:42.520 --> 00:39:43.360]   You bulk, then cut.
[00:39:43.360 --> 00:39:46.280]   So I've bulked, and now I need to cut.
[00:39:46.280 --> 00:39:47.740]   Whereas maybe, actually, the future
[00:39:47.740 --> 00:39:52.720]   is going to be more and better free-range organic models,
[00:39:52.720 --> 00:39:55.840]   models that we feed better stuff for certain purposes.
[00:39:55.840 --> 00:39:57.320]   Because how many of you have ever
[00:39:57.320 --> 00:39:59.360]   seen a generalized system that outperforms
[00:39:59.360 --> 00:40:01.520]   a specialized system?
[00:40:01.520 --> 00:40:03.280]   There's not many cases of that, eh?
[00:40:03.280 --> 00:40:05.320]   But a lot of this impetus was AGI scaling.
[00:40:05.320 --> 00:40:06.880]   Now we see the measure of behavior,
[00:40:06.880 --> 00:40:09.520]   and now we go from research to engineering.
[00:40:09.520 --> 00:40:11.400]   And we do this at a time whereby--
[00:40:11.400 --> 00:40:12.720]   I mean, it's insane.
[00:40:12.720 --> 00:40:15.860]   Like, GPT-4 can pass all these exams, right?
[00:40:15.860 --> 00:40:18.240]   It's like someone's going to strap this thing to a robot,
[00:40:18.240 --> 00:40:20.440]   and it'll go to Stanford.
[00:40:20.440 --> 00:40:23.080]   And probably do better than just about everyone there.
[00:40:23.080 --> 00:40:26.640]   The exception is like English literature.
[00:40:26.640 --> 00:40:28.440]   It's still pretty bad at AP English Lit,
[00:40:28.440 --> 00:40:31.520]   but bar exam, medical license exam, others are there.
[00:40:31.520 --> 00:40:34.720]   And so even though we've taken this hacky approach to it,
[00:40:34.720 --> 00:40:38.520]   we have expert systems that fit on single GPUs, which
[00:40:38.520 --> 00:40:40.680]   is a crazy advancement in our society.
[00:40:40.680 --> 00:40:44.560]   But where is the lower limit?
[00:40:44.560 --> 00:40:48.400]   What is the minimum number of tokens, models, model sizes,
[00:40:48.400 --> 00:40:49.040]   data?
[00:40:49.040 --> 00:40:50.800]   We haven't hit that engineering thing yet.
[00:40:50.800 --> 00:40:53.080]   And you saw that with stable diffusion.
[00:40:53.080 --> 00:40:54.600]   It was released out to the world,
[00:40:54.600 --> 00:40:57.400]   and you went from six seconds on an A100
[00:40:57.400 --> 00:40:59.240]   to now you're moving to consistently models,
[00:40:59.240 --> 00:41:01.560]   distillation, and others, multiple images a second.
[00:41:01.560 --> 00:41:02.960]   It works on an iPhone.
[00:41:02.960 --> 00:41:05.240]   You saw that with Lama release, which
[00:41:05.240 --> 00:41:08.280]   is FaceMeta's model that they released recently
[00:41:08.280 --> 00:41:09.400]   on language models.
[00:41:09.400 --> 00:41:10.760]   They got it working on a MacBook.
[00:41:10.760 --> 00:41:13.080]   And you're like, that's a bit crazy, right?
[00:41:13.080 --> 00:41:14.360]   It works on a MacBook.
[00:41:14.360 --> 00:41:16.400]   But then we don't know what this lower limit is,
[00:41:16.400 --> 00:41:18.920]   because a lot of the model trainings we can only do once.
[00:41:18.920 --> 00:41:21.000]   But now with the exponential compute, more talent,
[00:41:21.000 --> 00:41:23.000]   and more people doing it, we will explore
[00:41:23.000 --> 00:41:26.160]   these latent spaces better.
[00:41:26.160 --> 00:41:28.680]   And again, this is essential, because even
[00:41:28.680 --> 00:41:32.080]   though there are these hacky things, it's important now.
[00:41:32.080 --> 00:41:33.800]   I mean, look at this.
[00:41:33.800 --> 00:41:37.480]   Coders are like half the time increasing productivity.
[00:41:37.480 --> 00:41:41.240]   50% of all code on GitHub now is AI generated.
[00:41:41.240 --> 00:41:43.160]   I don't think we have programmers in five years
[00:41:43.160 --> 00:41:44.240]   as we have now.
[00:41:44.240 --> 00:41:46.880]   But that's because we don't have people doing macro media flash
[00:41:46.880 --> 00:41:47.600]   anymore.
[00:41:47.600 --> 00:41:52.760]   When I started as a developer 21 years ago-- god, I'm 40 now.
[00:41:52.760 --> 00:41:56.600]   30 plus 10, I would like to think about it.
[00:41:56.600 --> 00:41:57.840]   We didn't even have GitHub.
[00:41:57.840 --> 00:41:59.640]   We just had subversion coming out.
[00:41:59.640 --> 00:42:02.000]   Programming is a change, and it will continue to change,
[00:42:02.000 --> 00:42:04.360]   because programming is just a way to talk to computers.
[00:42:04.360 --> 00:42:06.320]   And computers can understand our intentions now
[00:42:06.320 --> 00:42:07.360]   with this much better.
[00:42:07.360 --> 00:42:08.700]   But then again, this is happening
[00:42:08.700 --> 00:42:09.840]   across knowledge society.
[00:42:09.840 --> 00:42:13.680]   And again, it's just like having a team of really good grads
[00:42:13.680 --> 00:42:15.040]   who can program.
[00:42:15.040 --> 00:42:17.120]   Think about it in this frame as you build products,
[00:42:17.120 --> 00:42:19.320]   as you integrate it, as you use it in your own life.
[00:42:19.320 --> 00:42:21.520]   And I think it's probably the best frame of reference
[00:42:21.520 --> 00:42:22.640]   you can get.
[00:42:22.640 --> 00:42:24.400]   So open source.
[00:42:24.400 --> 00:42:25.880]   Who here likes open source?
[00:42:25.880 --> 00:42:26.380]   Yeah.
[00:42:26.380 --> 00:42:29.760]   [APPLAUSE]
[00:42:29.760 --> 00:42:31.680]   So I think open is super interesting here.
[00:42:31.680 --> 00:42:33.480]   We had stable diffusion.
[00:42:33.480 --> 00:42:36.040]   It was a collaboration between ourselves, RunwayML,
[00:42:36.040 --> 00:42:38.280]   and Confisclab.
[00:42:38.280 --> 00:42:40.680]   Robin, Andreas, and Dominic, who are
[00:42:40.680 --> 00:42:43.160]   the lead authors of the latent diffusion paper,
[00:42:43.160 --> 00:42:44.240]   all at Stability.
[00:42:44.240 --> 00:42:46.240]   If you look at my Twitter, their work last year
[00:42:46.240 --> 00:42:49.760]   at NVIDIA on high resolution video models just came out.
[00:42:49.760 --> 00:42:51.440]   There's some crazy stuff coming.
[00:42:51.440 --> 00:42:54.240]   We're going to have full movies in a few years generated
[00:42:54.240 --> 00:42:55.080]   from this.
[00:42:55.080 --> 00:42:58.600]   I think people have seen it go from blobs to just these things
[00:42:58.600 --> 00:42:59.960]   almost instantly.
[00:42:59.960 --> 00:43:02.280]   And interestingly, we're moving from this
[00:43:02.280 --> 00:43:04.640]   to full control, because you want
[00:43:04.640 --> 00:43:08.480]   to be able to control outputs to get to what you want.
[00:43:08.480 --> 00:43:11.280]   And so I think what we saw first was just this zero shot stuff.
[00:43:11.280 --> 00:43:13.200]   And the copyright office was like, you can't copyright that.
[00:43:13.200 --> 00:43:13.840]   It's too easy.
[00:43:13.840 --> 00:43:15.000]   And I agree.
[00:43:15.000 --> 00:43:18.440]   But then what you're moving to is full control of all pixels.
[00:43:18.440 --> 00:43:19.880]   So we have stable diffusion Excel.
[00:43:19.880 --> 00:43:20.560]   We have ControlNet.
[00:43:20.560 --> 00:43:21.560]   We have all these things.
[00:43:21.560 --> 00:43:23.000]   But who cares about the technology?
[00:43:23.000 --> 00:43:25.240]   Because what you care about is the interface
[00:43:25.240 --> 00:43:26.960]   and the usability.
[00:43:26.960 --> 00:43:28.840]   I've got something in my mind, and I
[00:43:28.840 --> 00:43:30.920]   want to bring it onto the screen.
[00:43:30.920 --> 00:43:32.840]   Give people the tools and the agency,
[00:43:32.840 --> 00:43:34.120]   and they become happier.
[00:43:34.120 --> 00:43:35.700]   That's why the mission of our company
[00:43:35.700 --> 00:43:39.320]   is to build a foundation to activate humanity's potential.
[00:43:39.320 --> 00:43:41.000]   And our tagline is make people happy.
[00:43:41.000 --> 00:43:42.240]   And we do this through agency.
[00:43:42.240 --> 00:43:45.440]   It's through giving people the models that they can use.
[00:43:45.440 --> 00:43:48.040]   And you need to do this in a way that I
[00:43:48.040 --> 00:43:50.840]   think is a bit different, because it's a new type
[00:43:50.840 --> 00:43:52.320]   of programming primitive.
[00:43:52.320 --> 00:43:54.640]   Stable diffusion, the language models, image models,
[00:43:54.640 --> 00:43:57.280]   all these things, they are a file that, again,
[00:43:57.280 --> 00:43:59.080]   stable diffusion is 2 gigabytes.
[00:43:59.080 --> 00:44:01.960]   And it was four of the top 10 apps
[00:44:01.960 --> 00:44:03.400]   on the App Store in December.
[00:44:03.400 --> 00:44:05.400]   That's pretty much the entire back end, Lensor
[00:44:05.400 --> 00:44:06.320]   and things like that.
[00:44:06.320 --> 00:44:08.280]   What can you do when you can put a few words in
[00:44:08.280 --> 00:44:10.680]   and you get a matter of Stormtrooper out?
[00:44:10.680 --> 00:44:11.640]   That's a bit different.
[00:44:11.640 --> 00:44:14.320]   I like to think of it, again, and framing as a game engine.
[00:44:14.320 --> 00:44:16.160]   It's like you look at the Wii U at the start
[00:44:16.160 --> 00:44:17.840]   to Breath of the Wild at the end.
[00:44:17.840 --> 00:44:19.540]   You look at all the amazing things people
[00:44:19.540 --> 00:44:21.200]   have fine-tuned on these models.
[00:44:21.200 --> 00:44:23.740]   Once you understand that latent space better, you can do it.
[00:44:23.740 --> 00:44:25.640]   But do we need to have 2 billion images?
[00:44:25.640 --> 00:44:26.680]   No.
[00:44:26.680 --> 00:44:28.240]   Do we need to use web scrapes?
[00:44:28.240 --> 00:44:29.680]   I think eventually we won't.
[00:44:29.680 --> 00:44:30.700]   One of the things we're doing is we're
[00:44:30.700 --> 00:44:32.820]   working with multiple governments on national data
[00:44:32.820 --> 00:44:35.360]   sets, because we need more and better data.
[00:44:35.360 --> 00:44:38.440]   DeepMind-- now Google DeepMind, they just merged--
[00:44:38.440 --> 00:44:40.920]   had an amazing paper called Chinchilla, where they said,
[00:44:40.920 --> 00:44:42.600]   do you need to train a model bigger and bigger
[00:44:42.600 --> 00:44:43.600]   trillions of parameters?
[00:44:43.600 --> 00:44:45.880]   No, train it more, which is just like,
[00:44:45.880 --> 00:44:48.160]   don't teach kids everything.
[00:44:48.160 --> 00:44:50.840]   Train them better, effectively, or grads,
[00:44:50.840 --> 00:44:52.280]   as we're going to call them.
[00:44:52.280 --> 00:44:54.040]   But it actually said, use better data.
[00:44:54.040 --> 00:44:56.420]   And that makes sense, because the way these models learn,
[00:44:56.420 --> 00:44:57.480]   so if you train it on everything,
[00:44:57.480 --> 00:44:59.640]   then train it on more and more specific things.
[00:44:59.640 --> 00:45:01.280]   So deep learning first, then train it
[00:45:01.280 --> 00:45:03.200]   on how humans actually interact, because it's
[00:45:03.200 --> 00:45:05.120]   this weird Shoggoth-type thing.
[00:45:05.120 --> 00:45:06.860]   And then use human feedback loops.
[00:45:06.860 --> 00:45:08.360]   So we need better data.
[00:45:08.360 --> 00:45:10.560]   And I think web scraping right now is OK.
[00:45:10.560 --> 00:45:13.200]   But I think we should on the things like opt-outs and others.
[00:45:13.200 --> 00:45:14.480]   And we need to put standards in.
[00:45:14.480 --> 00:45:15.900]   And I've called for self-regulation
[00:45:15.900 --> 00:45:18.560]   of the industry, having signed the FLI letter and others.
[00:45:18.560 --> 00:45:20.520]   But the reason I signed that was because I think
[00:45:20.520 --> 00:45:21.880]   we need to self-regulate.
[00:45:21.880 --> 00:45:23.720]   There weren't training runs anyway.
[00:45:23.720 --> 00:45:25.320]   And this was only for GPT-4 and above.
[00:45:25.320 --> 00:45:28.300]   We don't know how big models actually work.
[00:45:28.300 --> 00:45:31.100]   My take on AGI is that if we ever get to AGI,
[00:45:31.100 --> 00:45:33.340]   it's going to be like Scarlett Johansson and her.
[00:45:33.340 --> 00:45:34.600]   We're really boring.
[00:45:34.600 --> 00:45:35.560]   And it's just going to be out there.
[00:45:35.560 --> 00:45:38.100]   It's going to be like, goodbye, and thanks for all the GPUs.
[00:45:38.100 --> 00:45:39.260]   But I could be wrong.
[00:45:39.260 --> 00:45:40.340]   And so that's a worry.
[00:45:40.340 --> 00:45:41.880]   I'm worried about the security things,
[00:45:41.880 --> 00:45:44.220]   because there's that quote, "You've either been hacked,
[00:45:44.220 --> 00:45:45.380]   or you don't know you've been hacked."
[00:45:45.380 --> 00:45:47.780]   The bad actors probably downloaded it on a USB stick
[00:45:47.780 --> 00:45:48.980]   and have taken it out.
[00:45:48.980 --> 00:45:50.740]   We've got to self-regulate as an industry.
[00:45:50.740 --> 00:45:53.300]   We've got to put standards in, because the fear is
[00:45:53.300 --> 00:45:54.740]   going to increase.
[00:45:54.740 --> 00:45:56.420]   But then the opportunity is here as well
[00:45:56.420 --> 00:45:59.700]   to allow anyone to turn M. Adam into a stormtrooper.
[00:45:59.700 --> 00:46:01.460]   So I'm gunning to go in the next Star Wars.
[00:46:01.460 --> 00:46:04.020]   You know, the last Star Wars, Prince William and Harry
[00:46:04.020 --> 00:46:04.740]   were there?
[00:46:04.740 --> 00:46:06.540]   You can tell, because all stormtroopers are the same
[00:46:06.540 --> 00:46:07.900]   height, but they're taller.
[00:46:07.900 --> 00:46:09.620]   So I'm going to be the shorter one.
[00:46:09.620 --> 00:46:11.340]   That's my aim.
[00:46:11.340 --> 00:46:14.620]   So massive innovation, and then it had this thing.
[00:46:14.620 --> 00:46:16.220]   I stopped counting GitHub stars.
[00:46:16.220 --> 00:46:18.380]   Now you see things like GitHub stars all the time,
[00:46:18.380 --> 00:46:20.180]   because people are excited about generally,
[00:46:20.180 --> 00:46:21.500]   they build cool things.
[00:46:21.500 --> 00:46:22.780]   We're going to take this to the world,
[00:46:22.780 --> 00:46:25.260]   and then we're going to make Game of Thrones as an HK drama,
[00:46:25.260 --> 00:46:26.860]   right?
[00:46:26.860 --> 00:46:29.700]   But then I'm going to remake Game of Thrones season 8,
[00:46:29.700 --> 00:46:31.580]   and I'll be awesome.
[00:46:31.580 --> 00:46:33.980]   Because this is what it is.
[00:46:33.980 --> 00:46:36.840]   Game of Thrones season 1 to 7 was so good,
[00:46:36.840 --> 00:46:39.580]   because all of the characters had agency.
[00:46:39.580 --> 00:46:40.940]   They were agents, and the meaning
[00:46:40.940 --> 00:46:43.380]   was in the interaction between their stories.
[00:46:43.380 --> 00:46:44.740]   These things tell better stories,
[00:46:44.740 --> 00:46:47.580]   whereas the last season is just like, let's get to the ending,
[00:46:47.580 --> 00:46:49.720]   and they behaved unnaturally and weird,
[00:46:49.720 --> 00:46:52.260]   just like the original stable diffusion with arms.
[00:46:52.260 --> 00:46:55.180]   They stuck out of all sorts of places.
[00:46:55.180 --> 00:46:56.140]   But this is the thing.
[00:46:56.140 --> 00:46:59.700]   Infinite customization, infinite thing, infinite imagination.
[00:46:59.700 --> 00:47:03.100]   This is a study that was done at the University of Osaka.
[00:47:03.100 --> 00:47:04.220]   This is a bit creepy.
[00:47:04.220 --> 00:47:06.160]   So you get someone to watch these images.
[00:47:06.160 --> 00:47:09.300]   You take an fMRI, and you put it through stable diffusion,
[00:47:09.300 --> 00:47:10.900]   and you get those outputs.
[00:47:10.900 --> 00:47:13.580]   You can read minds.
[00:47:13.580 --> 00:47:15.500]   Neuralink will be open sourcing their data,
[00:47:15.500 --> 00:47:17.740]   so we'll see how monkeys see the world soon.
[00:47:17.740 --> 00:47:18.740]   But you see cool things.
[00:47:18.740 --> 00:47:20.180]   You look at earthspecies.org.
[00:47:20.180 --> 00:47:22.940]   I was whale watching with Aza on the weekend.
[00:47:22.940 --> 00:47:24.340]   He's translating whale talk.
[00:47:24.340 --> 00:47:25.880]   And again, once they hear us, they'll
[00:47:25.880 --> 00:47:27.980]   probably bugger off and never see us again.
[00:47:27.980 --> 00:47:29.500]   Humans are very annoying.
[00:47:29.500 --> 00:47:30.860]   But I think this is the thing.
[00:47:30.860 --> 00:47:32.380]   You see more and more things coming out.
[00:47:32.380 --> 00:47:34.620]   So we have our next generation of music models coming.
[00:47:34.620 --> 00:47:36.620]   We've got video, and we've got text in 3D.
[00:47:36.620 --> 00:47:38.300]   And this is a bit of a distracting slide,
[00:47:38.300 --> 00:47:41.580]   so I'm going to change it back to this one, the creepy one.
[00:47:41.580 --> 00:47:43.900]   And I think-- so what we're going to do at Stability
[00:47:43.900 --> 00:47:45.580]   is I'm going to build the benchmark model
[00:47:45.580 --> 00:47:48.820]   for every modality, every sector, and every nation.
[00:47:48.820 --> 00:47:50.700]   We're going to build Stabilities everywhere,
[00:47:50.700 --> 00:47:53.100]   because everyone deserves to own their own data
[00:47:53.100 --> 00:47:56.500]   and have a say in this, because nobody has the right answers.
[00:47:56.500 --> 00:47:58.100]   I think this is essential, because you
[00:47:58.100 --> 00:48:00.780]   have to have the big models by the biggest companies.
[00:48:00.780 --> 00:48:02.420]   You need the open eyes and anthropics
[00:48:02.420 --> 00:48:04.820]   to build amazing stuff and be the kind of generation.
[00:48:04.820 --> 00:48:07.900]   But open models are needed for private data.
[00:48:07.900 --> 00:48:10.820]   All the regulated data in the world, private data, your data,
[00:48:10.820 --> 00:48:13.700]   you should have your own AIs, every single company, country,
[00:48:13.700 --> 00:48:14.780]   culture, and person.
[00:48:14.780 --> 00:48:16.560]   And the way those interact, have a vision
[00:48:16.560 --> 00:48:19.180]   of an intelligent internet where they're all working for you,
[00:48:19.180 --> 00:48:21.220]   not trying to extract things from you.
[00:48:21.220 --> 00:48:22.940]   And so our responsibility at Stability
[00:48:22.940 --> 00:48:24.440]   is to try and help coordinate this,
[00:48:24.440 --> 00:48:26.060]   just like we do with all our grants and everything
[00:48:26.060 --> 00:48:26.940]   like that.
[00:48:26.940 --> 00:48:29.400]   We've been trying to find our place, but this is our place.
[00:48:29.400 --> 00:48:31.140]   So loads of announcements coming.
[00:48:31.140 --> 00:48:33.220]   For every modality, we will have a model.
[00:48:33.220 --> 00:48:34.980]   Stable LM is an example of that.
[00:48:34.980 --> 00:48:36.260]   We've put that out.
[00:48:36.260 --> 00:48:37.720]   We didn't put out the best version.
[00:48:37.720 --> 00:48:39.460]   We've got better versions coming out.
[00:48:39.460 --> 00:48:40.340]   But what we're going to do is we're
[00:48:40.340 --> 00:48:41.620]   going to build it in the open.
[00:48:41.620 --> 00:48:44.100]   And we're going to have an open discussion about licensing,
[00:48:44.100 --> 00:48:46.420]   about data sets, our learnings from that,
[00:48:46.420 --> 00:48:48.880]   because there's no view under the hood of how these things
[00:48:48.880 --> 00:48:51.200]   are actually built, but they're going to run our lives.
[00:48:51.200 --> 00:48:52.640]   How insane is that?
[00:48:52.640 --> 00:48:55.180]   So we're going to try this as a collaborative kind of thing
[00:48:55.180 --> 00:48:56.780]   where more and more people will do it.
[00:48:56.780 --> 00:48:58.980]   Those of you that are developers that don't know AI,
[00:48:58.980 --> 00:49:01.480]   do the faster AI courses that we kind of back.
[00:49:01.480 --> 00:49:02.900]   Go on Hugging Face.
[00:49:02.900 --> 00:49:06.460]   Go on Weights and Biases and other things and learn about it.
[00:49:06.460 --> 00:49:09.280]   Because this is really important for our lives, our kids' lives,
[00:49:09.280 --> 00:49:10.320]   and others.
[00:49:10.320 --> 00:49:12.320]   And then, like I said, with the national models,
[00:49:12.320 --> 00:49:13.620]   it will represent the cultures.
[00:49:13.620 --> 00:49:15.160]   This is diversity.
[00:49:15.160 --> 00:49:18.820]   But my main aim is my co-founder Joe and I,
[00:49:18.820 --> 00:49:20.540]   we did Imagine Worldwide.
[00:49:20.540 --> 00:49:23.340]   So we took the Global X Prize for Learning.
[00:49:23.340 --> 00:49:26.580]   That was a $15 million prize by Elon Musk and Tony Robbins.
[00:49:26.580 --> 00:49:27.860]   Elon's a cool guy.
[00:49:27.860 --> 00:49:31.220]   And we've been deploying that over the last few years
[00:49:31.220 --> 00:49:33.300]   to refugee camps.
[00:49:33.300 --> 00:49:35.540]   So the output of that was an app that
[00:49:35.540 --> 00:49:39.500]   was designed to teach literacy and numeracy to kids
[00:49:39.500 --> 00:49:41.460]   without internet in 18 months.
[00:49:41.460 --> 00:49:44.020]   With the randomized control trials now in refugee camps,
[00:49:44.020 --> 00:49:47.980]   we're teaching 76% of kids literacy and numeracy
[00:49:47.980 --> 00:49:52.140]   in 13 months on one hour a day of education without an adult.
[00:49:52.140 --> 00:49:53.380]   How cool is that?
[00:49:53.380 --> 00:49:53.880]   So--
[00:49:53.880 --> 00:49:59.240]   [APPLAUSE]
[00:49:59.240 --> 00:50:01.180]   This is the reason we do what we do.
[00:50:01.180 --> 00:50:04.020]   A, because it's right, and it's also an amazing business model.
[00:50:04.020 --> 00:50:04.960]   We're the only company in the world
[00:50:04.960 --> 00:50:06.700]   that can build a custom model of any type.
[00:50:06.700 --> 00:50:08.160]   I'm very busy these days.
[00:50:08.160 --> 00:50:11.780]   But my aim is to get these technologies, not as an AGI
[00:50:11.780 --> 00:50:14.780]   to replace us, but to every kid in the world.
[00:50:14.780 --> 00:50:16.940]   So we have the whole of Malawi now to educate.
[00:50:16.940 --> 00:50:19.180]   And we're going to completely refactor it.
[00:50:19.180 --> 00:50:20.780]   Taking these models, can you have
[00:50:20.780 --> 00:50:22.560]   an AI that teaches kids, learns from kids,
[00:50:22.560 --> 00:50:24.620]   and allows them to activate their potential?
[00:50:24.620 --> 00:50:26.740]   The only thing that's been shown is one-to-one.
[00:50:26.740 --> 00:50:28.580]   This is an example of the immense power.
[00:50:28.580 --> 00:50:30.940]   Because I'm telling you, when we do entire countries
[00:50:30.940 --> 00:50:32.820]   and we develop special bonds and special hardware
[00:50:32.820 --> 00:50:34.560]   and other things, we have 96 million kids
[00:50:34.560 --> 00:50:36.300]   over the next few years.
[00:50:36.300 --> 00:50:38.780]   These kids will outperform all of us
[00:50:38.780 --> 00:50:40.280]   because they're going to be told they
[00:50:40.280 --> 00:50:42.860]   can continue to be creative and engage.
[00:50:42.860 --> 00:50:45.620]   And that's why I tell people, go to your kids.
[00:50:45.620 --> 00:50:49.460]   Use OpenAI, chat GPT, use Midjourney, use these things.
[00:50:49.460 --> 00:50:51.300]   And be creative.
[00:50:51.300 --> 00:50:53.900]   They are not there to be factual right now.
[00:50:53.900 --> 00:50:55.380]   It's a category error.
[00:50:55.380 --> 00:50:57.700]   These are here to extend our capabilities.
[00:50:57.700 --> 00:51:01.260]   And our capabilities is that we are creative,
[00:51:01.260 --> 00:51:04.500]   amazing individuals.
[00:51:04.500 --> 00:51:05.300]   And that's cool.
[00:51:05.300 --> 00:51:06.660]   So thank you all.
[00:51:06.660 --> 00:51:07.300]   Thank you again.
[00:51:07.300 --> 00:51:07.800]   [APPLAUSE]
[00:51:08.100 --> 00:51:08.600]   [INAUDIBLE]
[00:51:08.600 --> 00:51:16.620]   All right.
[00:51:16.620 --> 00:51:17.900]   Thank you, Amai.
[00:51:17.900 --> 00:51:21.380]   So next up, what we're going to do is--
[00:51:21.380 --> 00:51:23.980]   you guys have heard about the products that we've launched.
[00:51:23.980 --> 00:51:27.620]   But we really wanted to give you a real world use case
[00:51:27.620 --> 00:51:31.500]   that you could use to imagine how to use these tools.
[00:51:31.500 --> 00:51:34.980]   So I have Morgan from my team, who's going to come and talk
[00:51:34.980 --> 00:51:37.740]   about OneBot.
[00:51:37.740 --> 00:51:38.240]   That's cool.
[00:51:38.240 --> 00:51:41.220]   [APPLAUSE]
[00:51:41.220 --> 00:51:43.720]   Thanks, Lavanya.
[00:51:43.720 --> 00:51:45.860]   Lavanya's my manager.
[00:51:45.860 --> 00:51:47.440]   But she didn't-- and she's wonderful.
[00:51:47.440 --> 00:51:49.320]   But she didn't tell me I was on after I'm mad.
[00:51:49.320 --> 00:51:51.740]   So big boots to fill.
[00:51:51.740 --> 00:51:54.400]   So today, we're going to talk a little bit about the support
[00:51:54.400 --> 00:51:57.540]   bot that we built for Words & Voices.
[00:51:57.540 --> 00:51:59.700]   It's been live in our Discord for about three weeks.
[00:51:59.700 --> 00:52:02.500]   And there's a couple of reasons why we built it.
[00:52:02.500 --> 00:52:06.520]   The first is to better understand the workflows
[00:52:06.520 --> 00:52:10.460]   and the pain points of prompt engineers today.
[00:52:10.460 --> 00:52:12.940]   We do a huge amount of user interviews.
[00:52:12.940 --> 00:52:15.540]   And we have ML engineers with years and years
[00:52:15.540 --> 00:52:18.380]   of experience training ML models.
[00:52:18.380 --> 00:52:20.860]   But I think we really also need to learn by doing.
[00:52:20.860 --> 00:52:23.460]   And so by going through these workflows,
[00:52:23.460 --> 00:52:25.460]   experiencing those pain points, we
[00:52:25.460 --> 00:52:28.220]   can figure out how to build the best tools for ML
[00:52:28.220 --> 00:52:30.540]   practitioners that are just coming into the space
[00:52:30.540 --> 00:52:31.780]   with these tools.
[00:52:31.780 --> 00:52:33.420]   The second reason we built this, we
[00:52:33.420 --> 00:52:35.140]   see a clear business need for this.
[00:52:35.140 --> 00:52:37.200]   We're in an incredibly fortunate position
[00:52:37.200 --> 00:52:40.800]   that Words & Voices has super strong user growth.
[00:52:40.800 --> 00:52:45.080]   And so we need a way to figure out how to scale our support
[00:52:45.080 --> 00:52:49.880]   team and keep our user support high quality.
[00:52:49.880 --> 00:52:56.360]   So we had four goals that we identified pretty early
[00:52:56.360 --> 00:52:57.600]   into this.
[00:52:57.600 --> 00:52:59.960]   The first was around trustworthiness.
[00:52:59.960 --> 00:53:04.320]   We needed the bot never, ever to make up Words & Voices' code.
[00:53:04.320 --> 00:53:06.060]   You know, the first time that you
[00:53:06.060 --> 00:53:07.860]   get code that's been hallucinated
[00:53:07.860 --> 00:53:09.700]   from one of these models, it completely
[00:53:09.700 --> 00:53:10.960]   destroys your trust in it.
[00:53:10.960 --> 00:53:13.500]   And you're not going to use it again.
[00:53:13.500 --> 00:53:16.440]   The second was that the answer should be relevant.
[00:53:16.440 --> 00:53:18.020]   And again, we just wanted to make sure
[00:53:18.020 --> 00:53:22.580]   that the bot only answered questions about Words & Voices.
[00:53:22.580 --> 00:53:24.740]   The next was around the UX.
[00:53:24.740 --> 00:53:28.140]   And especially given that this was a pretty raw
[00:53:28.140 --> 00:53:31.500]   and experimental bot that we put into our Discord,
[00:53:31.500 --> 00:53:34.820]   we wanted to highlight to users that it was in beta
[00:53:34.820 --> 00:53:37.460]   and that there were other escape hatches, for example,
[00:53:37.460 --> 00:53:40.780]   to email in our support team if they didn't get the answer
[00:53:40.780 --> 00:53:42.380]   they were looking for.
[00:53:42.380 --> 00:53:43.920]   And the last was strong evaluation.
[00:53:43.920 --> 00:53:46.820]   So to be able to make a case for this internally,
[00:53:46.820 --> 00:53:48.860]   that we think this is a good thing,
[00:53:48.860 --> 00:53:51.380]   we wanted to have a really robust evaluation
[00:53:51.380 --> 00:53:55.100]   with a mix of some manual labeling,
[00:53:55.100 --> 00:53:57.740]   but also automated evaluation.
[00:53:57.740 --> 00:54:01.180]   [SIDE CONVERSATION]
[00:54:01.180 --> 00:54:08.940]   So how did we build this?
[00:54:08.940 --> 00:54:10.820]   We used the wonderful LangChain library
[00:54:10.820 --> 00:54:12.440]   that I'm sure a bunch of you have used.
[00:54:12.440 --> 00:54:15.740]   I'm really looking forward to Nuno's talk shortly.
[00:54:15.740 --> 00:54:17.660]   In terms of Words & Voices tooling,
[00:54:17.660 --> 00:54:22.820]   we used artifacts to store our data sets and our embeddings
[00:54:22.820 --> 00:54:24.860]   of where our vector stores lived.
[00:54:24.860 --> 00:54:29.220]   We had a really, I think, strong position with the data
[00:54:29.220 --> 00:54:32.660]   that we could feed to GPT-4 for the bot.
[00:54:32.660 --> 00:54:34.860]   And so obviously, we have our documentation.
[00:54:34.860 --> 00:54:36.100]   We have our code base.
[00:54:36.100 --> 00:54:39.140]   But we also have a huge amount of example collabs
[00:54:39.140 --> 00:54:41.060]   in our examples repository.
[00:54:41.060 --> 00:54:42.460]   We have our support tickets.
[00:54:42.460 --> 00:54:44.820]   And we also have community forum answers.
[00:54:44.820 --> 00:54:47.740]   So this is a really rich knowledge base
[00:54:47.740 --> 00:54:50.580]   that we can provide as context for user questions.
[00:54:50.580 --> 00:54:54.380]   And I think there's a lot more we can get out of it also.
[00:54:54.380 --> 00:54:56.460]   There are a few considerations that we also
[00:54:56.460 --> 00:54:59.540]   identified pretty early as we were developing.
[00:54:59.540 --> 00:55:01.900]   The first was around evaluation.
[00:55:01.900 --> 00:55:10.100]   And so we decided against building a fully chatty chat
[00:55:10.100 --> 00:55:10.820]   bot.
[00:55:10.820 --> 00:55:14.700]   So we instead just decided to do a single question-answer bot.
[00:55:14.700 --> 00:55:18.780]   And that was mostly just to be able to evaluate
[00:55:18.780 --> 00:55:20.220]   the responses of the bot.
[00:55:20.220 --> 00:55:23.100]   If you have a long user conversation,
[00:55:23.100 --> 00:55:25.700]   it gets a little bit more time-consuming
[00:55:25.700 --> 00:55:29.820]   to manually evaluate how that conversation went.
[00:55:29.820 --> 00:55:31.740]   The next, obviously, is around context length,
[00:55:31.740 --> 00:55:34.020]   which I'm sure a bunch of you are aware of.
[00:55:34.020 --> 00:55:35.740]   We had to find the right balance in terms
[00:55:35.740 --> 00:55:39.780]   of feeding all this rich knowledge to the query,
[00:55:39.780 --> 00:55:43.540]   but also balancing that with how many words a user could actually
[00:55:43.540 --> 00:55:46.620]   add to their question.
[00:55:46.620 --> 00:55:48.020]   The next was around rate limiting.
[00:55:48.020 --> 00:55:50.900]   So we were quite lucky to have access to GPT-4
[00:55:50.900 --> 00:55:52.580]   when we were building this bot.
[00:55:52.580 --> 00:55:55.260]   But nevertheless, it's still not fully generally available.
[00:55:55.260 --> 00:55:57.540]   And we still occasionally get rate limited from time
[00:55:57.540 --> 00:55:58.180]   to times.
[00:55:58.180 --> 00:56:01.940]   And so we had to build a fallback to chat GPT-3.5
[00:56:01.940 --> 00:56:05.180]   in the cases when we did get rate limited.
[00:56:05.180 --> 00:56:07.260]   And the last was around user feedback.
[00:56:07.260 --> 00:56:09.500]   Obviously, we have our own evaluation metrics.
[00:56:09.500 --> 00:56:11.420]   But we really need to hear from people,
[00:56:11.420 --> 00:56:12.500]   was this a good experience?
[00:56:12.500 --> 00:56:15.580]   And was the answer useful to them?
[00:56:15.580 --> 00:56:17.380]   The last, again, like I mentioned earlier,
[00:56:17.380 --> 00:56:20.380]   was around having this escape hatch to support,
[00:56:20.380 --> 00:56:22.260]   caveating this is an early bot in beta,
[00:56:22.260 --> 00:56:24.260]   and that they're welcome to reach out to support
[00:56:24.260 --> 00:56:27.500]   if they didn't get the answer they were looking for.
[00:56:27.500 --> 00:56:31.420]   So in terms of exploring the responses to our bot,
[00:56:31.420 --> 00:56:33.420]   we used Weights and Biases, which you saw--
[00:56:33.420 --> 00:56:35.220]   or tables, which you saw earlier.
[00:56:35.220 --> 00:56:37.020]   And this was really interesting, because we
[00:56:37.020 --> 00:56:39.140]   could look into the different products
[00:56:39.140 --> 00:56:43.020]   users had questions about, and also understand
[00:56:43.020 --> 00:56:45.100]   if there were repeat users, if they tended
[00:56:45.100 --> 00:56:48.020]   to give positive or negative feedback,
[00:56:48.020 --> 00:56:50.380]   or didn't give any feedback at all.
[00:56:50.380 --> 00:56:52.120]   So in terms of performance, how did we do?
[00:56:52.120 --> 00:56:56.140]   So roughly about 75% accuracy, we'll say.
[00:56:56.140 --> 00:57:01.140]   I would say the 25% isn't necessarily incorrect.
[00:57:01.140 --> 00:57:03.080]   There are a lot of interesting cases
[00:57:03.080 --> 00:57:05.580]   when you deploy a bot like this.
[00:57:05.580 --> 00:57:09.020]   There were, for example, I've seen multiple instances
[00:57:09.020 --> 00:57:12.180]   where the bot gives this a little bit roundabout answer,
[00:57:12.180 --> 00:57:15.260]   which is generally correct and will get the user to a happy
[00:57:15.260 --> 00:57:18.700]   place, when instead there's a single snippet, or code,
[00:57:18.700 --> 00:57:22.060]   or a method that it could have recommended instead.
[00:57:22.060 --> 00:57:25.220]   There are also these cases where users
[00:57:25.220 --> 00:57:27.420]   are using slightly different language
[00:57:27.420 --> 00:57:28.900]   than the bot would expect.
[00:57:28.900 --> 00:57:30.740]   And so we might not need to do something there
[00:57:30.740 --> 00:57:34.180]   to translate the user's query into more Weights and Bias
[00:57:34.180 --> 00:57:36.780]   product-friendly language that the bot will then
[00:57:36.780 --> 00:57:38.460]   be able to understand.
[00:57:38.460 --> 00:57:39.860]   But one of the interesting things
[00:57:39.860 --> 00:57:42.940]   that we noticed when we were doing this evaluation was
[00:57:42.940 --> 00:57:46.000]   actually we identified a whole bunch of small areas
[00:57:46.000 --> 00:57:48.820]   in our docs that could do with improvement,
[00:57:48.820 --> 00:57:50.780]   because we knew the bot should be
[00:57:50.780 --> 00:57:52.660]   able to answer if the information in the docs
[00:57:52.660 --> 00:57:53.820]   was there, and it didn't.
[00:57:53.820 --> 00:57:56.740]   And so we have a bunch of those improvements to our docs
[00:57:56.740 --> 00:57:58.980]   shipping next week.
[00:57:58.980 --> 00:58:00.860]   And so what's next?
[00:58:00.860 --> 00:58:04.380]   First, we're going to turn on the chat interaction.
[00:58:04.380 --> 00:58:07.220]   We really see a lot of users trying to chat to the bot
[00:58:07.220 --> 00:58:08.220]   and not being able to.
[00:58:08.220 --> 00:58:11.500]   So I think it's actually harming the experience at this point.
[00:58:11.500 --> 00:58:14.220]   We're also going to do more iteration on our prompt.
[00:58:14.220 --> 00:58:15.640]   I think, like I said, there's more
[00:58:15.640 --> 00:58:17.300]   we can get out of our knowledge base,
[00:58:17.300 --> 00:58:20.540]   especially with the right prompts or maybe a few more
[00:58:20.540 --> 00:58:21.860]   chains.
[00:58:21.860 --> 00:58:23.580]   Then we're going to start to roll it out
[00:58:23.580 --> 00:58:26.260]   across our wider surface area.
[00:58:26.260 --> 00:58:28.580]   So we're going to start with our community forum,
[00:58:28.580 --> 00:58:32.020]   move into our docs chat for support,
[00:58:32.020 --> 00:58:35.340]   and then eventually maybe into our in-app chat itself.
[00:58:35.340 --> 00:58:37.020]   So that's everything from me.
[00:58:37.020 --> 00:58:39.140]   This is a huge team effort by a bunch of people
[00:58:39.140 --> 00:58:41.460]   across Weights and Voices.
[00:58:41.460 --> 00:58:46.020]   And now I'm going to hand you over back to Levania,
[00:58:46.020 --> 00:58:47.300]   if she's here.
[00:58:47.300 --> 00:58:49.340]   In the meantime, one thing I want to say
[00:58:49.340 --> 00:58:52.700]   is we're actually open sourcing one bot.
[00:58:52.700 --> 00:58:55.020]   And so if you go to the link here,
[00:58:55.020 --> 00:58:58.540]   you'll be able to see the code.
[00:58:58.540 --> 00:59:00.780]   Feel free to make PRs, issues, and we
[00:59:00.780 --> 00:59:02.380]   hope we can make this better together.
[00:59:02.380 --> 00:59:02.940]   So thank you.
[00:59:02.940 --> 00:59:07.300]   [APPLAUSE]
[00:59:07.300 --> 00:59:08.700]   So very happy.
[00:59:08.700 --> 00:59:10.420]   Hold on to your papers, everyone.
[00:59:10.420 --> 00:59:11.060]   Thank you.
[00:59:11.060 --> 00:59:12.420]   Thank you.
[00:59:12.420 --> 00:59:15.260]   Hello.
[00:59:15.260 --> 00:59:17.260]   Oh, I hear a sound, but it's not me.
[00:59:17.260 --> 00:59:19.180]   I got confused.
[00:59:19.180 --> 00:59:20.100]   All right.
[00:59:20.100 --> 00:59:21.900]   So next up, we have--
[00:59:21.900 --> 00:59:25.260]   OK, I'm not--
[00:59:25.260 --> 00:59:27.980]   Next up, we have Nuno from Linkchain.
[00:59:27.980 --> 00:59:31.220]   And he's going to talk about using tools in Linkchain.
[00:59:31.220 --> 00:59:32.820]   Welcome, Nuno.
[00:59:32.820 --> 00:59:33.320]   Thank you.
[00:59:33.320 --> 00:59:36.780]   [APPLAUSE]
[00:59:36.780 --> 00:59:37.780]   Thank you.
[00:59:37.780 --> 00:59:47.180]   All right.
[00:59:47.180 --> 00:59:52.980]   So today, I'm talking to you about using tools in Linkchain.
[00:59:52.980 --> 00:59:57.940]   And I think to understand what tools are,
[00:59:57.940 --> 01:00:02.740]   best thing to do is speak first about what an agent is.
[01:00:02.740 --> 01:00:06.660]   And an agent is kind of the fundamental concept
[01:00:06.660 --> 01:00:09.020]   behind a lot of the cool demos that we've
[01:00:09.020 --> 01:00:15.500]   been seeing, like AutoGPT and maybe AGI and all that stuff.
[01:00:15.500 --> 01:00:21.380]   So it's essentially-- or one way to define it anyway
[01:00:21.380 --> 01:00:24.340]   is kind of a more complicated prompt that
[01:00:24.340 --> 01:00:28.340]   involves some kind of planning and that
[01:00:28.340 --> 01:00:32.460]   has some kind of looping going on to apply it
[01:00:32.460 --> 01:00:34.660]   a few times over and over.
[01:00:34.660 --> 01:00:37.580]   And something that makes agents very powerful
[01:00:37.580 --> 01:00:41.820]   is to give them capabilities that are not part of the model.
[01:00:41.820 --> 01:00:46.020]   And those capabilities are what we call tools.
[01:00:46.020 --> 01:00:54.460]   So tools-- tools are actually very simple.
[01:00:54.460 --> 01:00:58.420]   They're just a function that returns a string output.
[01:00:58.420 --> 01:01:03.760]   So really, almost anything could be a tool.
[01:01:03.760 --> 01:01:07.060]   So a tool can be like a search engine.
[01:01:07.060 --> 01:01:10.020]   You can wrap it in a tool, a database.
[01:01:10.020 --> 01:01:13.620]   You can wrap it in a tool calling a database.
[01:01:13.620 --> 01:01:17.060]   You can call an LLM inside a tool.
[01:01:17.060 --> 01:01:18.980]   You can call another agent inside a tool.
[01:01:18.980 --> 01:01:21.900]   So really, you can do almost anything you want.
[01:01:21.900 --> 01:01:26.380]   And that's how you can give these models new capabilities.
[01:01:26.380 --> 01:01:29.260]   So yeah, so that's why tools end up
[01:01:29.260 --> 01:01:32.080]   being important, because you can give them
[01:01:32.080 --> 01:01:35.220]   data that they don't have because it's
[01:01:35.220 --> 01:01:37.660]   your proprietary data or because it's
[01:01:37.660 --> 01:01:40.740]   like something that happened yesterday or something
[01:01:40.740 --> 01:01:47.380]   like that, or also to give them different capabilities,
[01:01:47.380 --> 01:01:51.380]   giving them the ability to interact with an API
[01:01:51.380 --> 01:01:55.340]   or to save some information and retrieve it later
[01:01:55.340 --> 01:01:58.980]   to kind of give it some kind of more long-term memory.
[01:01:58.980 --> 01:02:03.280]   So it's just really a very powerful concept.
[01:02:03.280 --> 01:02:05.320]   Yeah, so as I already said, these
[01:02:05.320 --> 01:02:06.760]   are some examples of tools.
[01:02:06.760 --> 01:02:11.480]   So search engines, calculators, databases, vector stores,
[01:02:11.480 --> 01:02:16.880]   APIs, any function you can think of can be a tool.
[01:02:16.880 --> 01:02:21.200]   And how do you get LLMs to use tools?
[01:02:21.200 --> 01:02:24.120]   Well, as with lots of things with LLMs,
[01:02:24.120 --> 01:02:26.680]   you just tell them to use the tool.
[01:02:26.680 --> 01:02:30.300]   And hopefully, they understand your instructions
[01:02:30.300 --> 01:02:35.140]   and start to say, hey, I want you to now call this tool
[01:02:35.140 --> 01:02:39.060]   and get its output, give it back to me, and go from there.
[01:02:39.060 --> 01:02:43.700]   So yeah, so that's how you get them to use the tools.
[01:02:43.700 --> 01:02:46.460]   But that obviously then comes with some challenges,
[01:02:46.460 --> 01:02:49.620]   which is what I'm going to be talking about next.
[01:02:49.620 --> 01:02:56.100]   So I think the first challenge that we can identify
[01:02:56.100 --> 01:03:00.060]   is just getting the agent or the LLM
[01:03:00.060 --> 01:03:06.660]   to identify the right tool to call for each task.
[01:03:06.660 --> 01:03:10.100]   Then a second challenge is you give them
[01:03:10.100 --> 01:03:12.980]   all these capabilities, but sometimes you actually
[01:03:12.980 --> 01:03:15.540]   don't want the agent to use any tools,
[01:03:15.540 --> 01:03:20.020]   because it does have a lot of in-built knowledge,
[01:03:20.020 --> 01:03:21.580]   so to speak.
[01:03:21.580 --> 01:03:24.880]   So it's not always necessary to call a tool,
[01:03:24.880 --> 01:03:27.380]   and you don't want to break that.
[01:03:27.380 --> 01:03:32.140]   And finally, because when you use a tool,
[01:03:32.140 --> 01:03:36.020]   it's almost always inside an agent,
[01:03:36.020 --> 01:03:39.180]   a big challenge that comes up is just getting the LLM
[01:03:39.180 --> 01:03:45.880]   to reliably produce a valid output that you can parse
[01:03:45.880 --> 01:03:49.580]   and figure out what tool it's trying to call.
[01:03:49.580 --> 01:03:55.440]   So yeah, on the first challenge, getting
[01:03:55.440 --> 01:03:58.840]   them to use the right tools.
[01:03:58.840 --> 01:04:02.240]   So at the most basic level, in addition
[01:04:02.240 --> 01:04:04.720]   to being a function that returns a string,
[01:04:04.720 --> 01:04:09.320]   a tool is a name and a description.
[01:04:09.320 --> 01:04:12.200]   And that name and that description
[01:04:12.200 --> 01:04:14.960]   is actually all that is informing
[01:04:14.960 --> 01:04:19.220]   the agent or the LLM on which tool it should use.
[01:04:19.220 --> 01:04:24.980]   So the most basic way of solving this challenge of getting
[01:04:24.980 --> 01:04:26.840]   the agent to call the right tool is just
[01:04:26.840 --> 01:04:29.560]   to iterate on that description and that name
[01:04:29.560 --> 01:04:34.280]   to make it something that is just more understandable.
[01:04:34.280 --> 01:04:36.000]   And I think the only way to get there
[01:04:36.000 --> 01:04:39.480]   is just with iteration and trying different things.
[01:04:39.480 --> 01:04:47.920]   Some other things that can help is to sometimes repeat
[01:04:47.920 --> 01:04:50.540]   some more important instructions, maybe
[01:04:50.540 --> 01:04:53.760]   at the beginning and at the end of your prompt.
[01:04:53.760 --> 01:04:56.400]   And finally, I think a very interesting solution
[01:04:56.400 --> 01:05:00.460]   to this problem is when you want to build very powerful agents,
[01:05:00.460 --> 01:05:05.240]   you start to give them lots of tools, like 10 tools or 15
[01:05:05.240 --> 01:05:06.800]   tools or something like that.
[01:05:06.800 --> 01:05:10.120]   And that's actually a bit too much sometimes.
[01:05:10.120 --> 01:05:12.940]   So an interesting solution to that
[01:05:12.940 --> 01:05:19.420]   is to have a step in between that for every new input,
[01:05:19.420 --> 01:05:23.740]   it picks the most relevant tools out of all the 20
[01:05:23.740 --> 01:05:24.940]   tools you have available.
[01:05:24.940 --> 01:05:28.520]   It picks the three that are most relevant to that question
[01:05:28.520 --> 01:05:30.580]   and only puts those in the prompt.
[01:05:30.580 --> 01:05:40.340]   So the next challenge is to actually get the agents
[01:05:40.340 --> 01:05:44.020]   to not use tools all the time.
[01:05:44.020 --> 01:05:47.820]   So two problems that come up here
[01:05:47.820 --> 01:05:51.580]   is, for instance, with a search engine tool,
[01:05:51.580 --> 01:05:54.260]   because the description for a search engine tool
[01:05:54.260 --> 01:05:59.200]   is a really generic description.
[01:05:59.200 --> 01:06:02.020]   You can use this to find information about anything.
[01:06:02.020 --> 01:06:05.380]   So what ends up happening is the LLM
[01:06:05.380 --> 01:06:07.180]   will just use that tool for anything
[01:06:07.180 --> 01:06:09.260]   and just forget that it has its own knowledge
[01:06:09.260 --> 01:06:14.340]   that it maybe doesn't need to search to know something basic.
[01:06:14.340 --> 01:06:16.620]   And another thing that happens is
[01:06:16.620 --> 01:06:19.900]   when people interact with these agents,
[01:06:19.900 --> 01:06:25.260]   they very often naturally do not give all the information that
[01:06:25.260 --> 01:06:26.660]   is needed.
[01:06:26.660 --> 01:06:29.460]   They just ask kind of like an ambiguous question that
[01:06:29.460 --> 01:06:32.260]   needs clarification.
[01:06:32.260 --> 01:06:36.100]   But the way that these agents kind of behave by default
[01:06:36.100 --> 01:06:39.340]   with the prompts that people have
[01:06:39.340 --> 01:06:41.620]   is that the agent will just try to do
[01:06:41.620 --> 01:06:43.900]   the best it can with the incomplete information
[01:06:43.900 --> 01:06:45.220]   that it has.
[01:06:45.220 --> 01:06:46.760]   But that's actually not what we want.
[01:06:46.760 --> 01:06:50.500]   What we maybe want is to have the agent just
[01:06:50.500 --> 01:06:51.940]   ask for clarification.
[01:06:51.940 --> 01:06:54.260]   Like, I don't understand what you mean.
[01:06:54.260 --> 01:06:56.700]   Give me more details, something like that.
[01:06:56.700 --> 01:07:02.540]   And that's where a kind of human input tool can be very helpful.
[01:07:02.540 --> 01:07:05.380]   So if you just define another tool in your set of tools
[01:07:05.380 --> 01:07:08.900]   that is like, you can use this to ask the user for more
[01:07:08.900 --> 01:07:11.420]   information when you're not sure,
[01:07:11.420 --> 01:07:15.060]   then that kind of gets it out of that issue.
[01:07:15.060 --> 01:07:22.620]   And finally, the last challenge that I have
[01:07:22.620 --> 01:07:27.020]   is how to minimize the amount of times
[01:07:27.020 --> 01:07:33.300]   that the LLM and the agent will give you an invalid input.
[01:07:33.300 --> 01:07:38.740]   Because when that happens, you'd have to just show your user,
[01:07:38.740 --> 01:07:41.660]   something went wrong, try again, which obviously no one
[01:07:41.660 --> 01:07:43.620]   wants to do.
[01:07:43.620 --> 01:07:47.460]   So there's a few solutions to that.
[01:07:47.460 --> 01:07:52.740]   One is to-- something that works well
[01:07:52.740 --> 01:07:56.260]   is to actually ask the agent to provide
[01:07:56.260 --> 01:07:58.100]   the information of what tool to call,
[01:07:58.100 --> 01:08:03.260]   et cetera, in a structured format like JSON.
[01:08:03.260 --> 01:08:08.100]   That usually makes it more reliable and more likely
[01:08:08.100 --> 01:08:10.220]   to be correct.
[01:08:10.220 --> 01:08:13.640]   Another interesting thing that helps
[01:08:13.640 --> 01:08:17.700]   is actually when an invalid kind of instruction
[01:08:17.700 --> 01:08:22.500]   is produced by the agent, just give it back to the agent
[01:08:22.500 --> 01:08:24.940]   along with the parsing error and say, hey,
[01:08:24.940 --> 01:08:27.420]   I tried to parse your thing and it gave me this error.
[01:08:27.420 --> 01:08:30.100]   Do you want to try and fix it or something?
[01:08:30.100 --> 01:08:32.820]   And sometimes it will just auto recover from the error
[01:08:32.820 --> 01:08:34.940]   by itself, which is really cool.
[01:08:34.940 --> 01:08:42.660]   And finally, something that we're working on right now
[01:08:42.660 --> 01:08:46.980]   is how to make the input to tools more structured.
[01:08:46.980 --> 01:08:50.660]   So up until now, all the tools we've had in LangChain
[01:08:50.660 --> 01:08:57.220]   are like one string argument in, one string return value out,
[01:08:57.220 --> 01:08:59.300]   which is quite powerful.
[01:08:59.300 --> 01:09:02.020]   But lots of times, you're going to want
[01:09:02.020 --> 01:09:06.020]   to build tools that actually take more than one argument.
[01:09:06.020 --> 01:09:10.220]   And having a way of describing that and then
[01:09:10.220 --> 01:09:13.180]   including that in the prompt to the agent
[01:09:13.180 --> 01:09:16.060]   so that it can produce the right arguments, that
[01:09:16.060 --> 01:09:20.100]   can also help the reliability and the quality of the answers
[01:09:20.100 --> 01:09:21.100]   a lot.
[01:09:21.100 --> 01:09:24.860]   So what that looks like--
[01:09:24.860 --> 01:09:28.980]   so on the top is the Python version of this.
[01:09:28.980 --> 01:09:33.740]   And on the bottom is the TypeScript version.
[01:09:33.740 --> 01:09:36.340]   But they basically are equivalent,
[01:09:36.340 --> 01:09:38.060]   and they do the same thing.
[01:09:38.060 --> 01:09:40.460]   So the idea is to--
[01:09:40.460 --> 01:09:43.260]   you define the schema that you want.
[01:09:43.260 --> 01:09:46.260]   In this case, I want an object with a property called
[01:09:46.260 --> 01:09:50.060]   file path and a description, which is name of file.
[01:09:50.060 --> 01:09:54.180]   And the same thing in the TypeScript one.
[01:09:54.180 --> 01:09:56.460]   And then behind the scenes, we're
[01:09:56.460 --> 01:10:01.700]   going to convert that into a JSON schema string that's
[01:10:01.700 --> 01:10:03.540]   going to be passed to the agent.
[01:10:03.540 --> 01:10:07.100]   And that actually results in the agent
[01:10:07.100 --> 01:10:11.060]   being able to provide the arguments in that format.
[01:10:11.060 --> 01:10:12.540]   So you could imagine that could be
[01:10:12.540 --> 01:10:15.580]   like a more complicated schema with more fields
[01:10:15.580 --> 01:10:17.780]   and different types and so on.
[01:10:17.780 --> 01:10:20.860]   And yeah, so then this is what actually finds its way
[01:10:20.860 --> 01:10:25.220]   into the agent prompt.
[01:10:25.220 --> 01:10:27.340]   And that's what I have.
[01:10:27.340 --> 01:10:28.340]   That's it.
[01:10:28.340 --> 01:10:31.300]   [APPLAUSE]
[01:10:31.300 --> 01:10:36.700]   Thank you, Nuno.
[01:10:36.700 --> 01:10:39.940]   So next up, we have the third influencer
[01:10:39.940 --> 01:10:41.700]   that we have in the crowd.
[01:10:41.700 --> 01:10:43.940]   How many of you guys know Two Minute Papers?
[01:10:43.940 --> 01:10:46.340]   [CHEERING]
[01:10:46.340 --> 01:10:49.700]   So we have cut away from Two Minute Papers
[01:10:49.700 --> 01:10:51.700]   to give a special message.
[01:10:51.700 --> 01:10:54.660]   [APPLAUSE]
[01:10:55.660 --> 01:10:58.620]   [CHEERING]
[01:10:58.620 --> 01:11:02.180]   Hello.
[01:11:02.180 --> 01:11:04.340]   Who wants to be in a YouTube video?
[01:11:04.340 --> 01:11:05.500]   [CHEERING]
[01:11:05.500 --> 01:11:07.300]   You've got to help me a little, all right?
[01:11:07.300 --> 01:11:08.380]   So chant with me.
[01:11:08.380 --> 01:11:13.580]   Papers, papers, papers, papers, papers.
[01:11:13.580 --> 01:11:14.740]   More.
[01:11:14.740 --> 01:11:15.820]   Come on.
[01:11:15.820 --> 01:11:20.660]   Papers, papers, papers, papers, papers.
[01:11:20.660 --> 01:11:21.540]   All right.
[01:11:21.540 --> 01:11:22.820]   That was good.
[01:11:22.820 --> 01:11:24.260]   Thank you very much.
[01:11:24.260 --> 01:11:28.540]   So dear fellow scholars, so happy to see all of you
[01:11:28.540 --> 01:11:29.860]   here in person.
[01:11:29.860 --> 01:11:31.580]   Thank you so much for coming.
[01:11:31.580 --> 01:11:34.780]   And also, can we give a big hand to the organizers
[01:11:34.780 --> 01:11:36.860]   for creating this amazing event?
[01:11:36.860 --> 01:11:38.140]   Thank you very much.
[01:11:38.140 --> 01:11:41.100]   [APPLAUSE]
[01:11:41.100 --> 01:11:45.460]   I remember asking Lavanya about the attendance.
[01:11:45.460 --> 01:11:50.140]   And she said, yeah, it's a small meeting with 100 people.
[01:11:50.140 --> 01:11:51.660]   I was like, OK.
[01:11:51.660 --> 01:11:56.300]   Next time I ask, she says, yeah, 500 people.
[01:11:56.300 --> 01:11:59.380]   Next time I ask, she says 1,000 people.
[01:11:59.380 --> 01:12:01.980]   I was like, what is going on?
[01:12:01.980 --> 01:12:03.260]   Crazy.
[01:12:03.260 --> 01:12:06.940]   So you heard about all of these amazing AIs here today.
[01:12:06.940 --> 01:12:09.900]   But here's something about humans
[01:12:09.900 --> 01:12:13.540]   that I'd love to share with all of you fellow scholars.
[01:12:13.540 --> 01:12:16.900]   A father once wrote me that he has a son
[01:12:16.900 --> 01:12:18.860]   and he wants to get closer to him,
[01:12:18.860 --> 01:12:21.980]   but they have nothing in common.
[01:12:21.980 --> 01:12:25.020]   He's trying so hard, but just can't
[01:12:25.020 --> 01:12:26.780]   get a connection with him.
[01:12:26.780 --> 01:12:29.700]   And he said that one day, his son
[01:12:29.700 --> 01:12:32.460]   was watching two-minute papers on YouTube,
[01:12:32.460 --> 01:12:36.140]   and the father noticed, hmm, he likes this.
[01:12:36.140 --> 01:12:37.500]   I like this.
[01:12:37.500 --> 01:12:42.300]   And yes, finally, through the appreciation of the papers,
[01:12:42.300 --> 01:12:44.500]   they found a way to connect.
[01:12:44.500 --> 01:12:47.380]   So how incredible is that?
[01:12:47.380 --> 01:12:49.580]   And that is what the show is about,
[01:12:49.580 --> 01:12:52.980]   celebrating amazing human achievements together
[01:12:52.980 --> 01:12:54.420]   and learning together.
[01:12:54.420 --> 01:12:55.500]   Thank you very much.
[01:12:55.500 --> 01:12:57.500]   If you wish to see me, I'll be around.
[01:12:57.500 --> 01:12:58.000]   Thanks.
[01:12:58.000 --> 01:13:05.900]   [APPLAUSE]
[01:13:05.900 --> 01:13:07.420]   All right, you guys.
[01:13:07.420 --> 01:13:08.780]   That time has come.
[01:13:08.780 --> 01:13:12.340]   The time has come to thank you all so much for coming out.
[01:13:12.340 --> 01:13:15.900]   It is such an honor to have you all at the event tonight.
[01:13:15.900 --> 01:13:17.740]   We really, really appreciate it.
[01:13:17.740 --> 01:13:21.940]   I'd also love to give a special thanks to our guest speakers,
[01:13:21.940 --> 01:13:24.540]   Ahmad and Nuno and Carly.
[01:13:24.540 --> 01:13:31.500]   And I want to thank a bunch of our team members.
[01:13:31.500 --> 01:13:33.340]   So Andrea, who's in the background,
[01:13:33.340 --> 01:13:35.380]   has been working tirelessly for two weeks
[01:13:35.380 --> 01:13:37.140]   to put this together for you guys.
[01:13:37.140 --> 01:13:40.460]   And also Morgan and Anthony and the entire Weeds and Vices
[01:13:40.460 --> 01:13:40.960]   team.
[01:13:40.960 --> 01:13:43.380]   It's such an honor to work with them.
[01:13:43.380 --> 01:13:46.300]   And I want everyone to give a shout out to the Bee.
[01:13:46.300 --> 01:13:49.900]   [APPLAUSE]
[01:13:49.900 --> 01:13:53.900]   Thank you, London, for being such a cool town.
[01:13:53.900 --> 01:13:55.860]   It's not every co-founder that you
[01:13:55.860 --> 01:13:57.660]   can convince to get in a Bee costume,
[01:13:57.660 --> 01:13:59.740]   and I really appreciate that.
[01:13:59.740 --> 01:14:03.380]   So next up, we are here till about 10 o'clock.
[01:14:03.380 --> 01:14:04.300]   There's a bar.
[01:14:04.300 --> 01:14:05.100]   There's food.
[01:14:05.100 --> 01:14:06.260]   There's going to be mingling.
[01:14:06.260 --> 01:14:08.580]   So I encourage you guys to make some new friends.
[01:14:08.580 --> 01:14:11.220]   Ahmad, Lucas, and all these other--
[01:14:11.220 --> 01:14:14.140]   Kerala, other folks are going to be in the audience.
[01:14:14.140 --> 01:14:17.300]   Find them, hound them, ask them all of your questions,
[01:14:17.300 --> 01:14:19.660]   and thank you for coming.
[01:14:19.660 --> 01:14:20.260]   Thank you.
[01:14:20.260 --> 01:14:23.620]   [APPLAUSE]

