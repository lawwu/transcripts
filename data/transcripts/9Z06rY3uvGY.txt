
[00:00:00.000 --> 00:00:04.240]   - Welcome to MIT course 6S099,
[00:00:04.240 --> 00:00:06.020]   Artificial General Intelligence.
[00:00:06.020 --> 00:00:09.960]   Today we have Ray Kurzweil.
[00:00:09.960 --> 00:00:12.040]   He is one of the world's leading inventors,
[00:00:12.040 --> 00:00:13.480]   thinkers, and futurists,
[00:00:13.480 --> 00:00:16.800]   with a 30-year track record of accurate predictions.
[00:00:16.800 --> 00:00:19.640]   Called the restless genius by the Wall Street Journal
[00:00:19.640 --> 00:00:22.960]   and the ultimate thinking machine by Forbes magazine.
[00:00:22.960 --> 00:00:25.840]   He was selected as one of the top entrepreneurs
[00:00:25.840 --> 00:00:28.280]   by Inc. Magazine, which described him
[00:00:28.280 --> 00:00:31.520]   as the rightful heir to Thomas Edison.
[00:00:31.520 --> 00:00:34.360]   PBS selected him as one of the 16 revolutionaries
[00:00:34.360 --> 00:00:35.380]   who made America.
[00:00:35.380 --> 00:00:38.940]   Ray was the principal investigator
[00:00:38.940 --> 00:00:41.060]   of the first CCD flatbed scanner,
[00:00:41.060 --> 00:00:44.320]   the first omni-font optical character recognition,
[00:00:44.320 --> 00:00:47.000]   the first point-to-speech reading machine for the blind,
[00:00:47.000 --> 00:00:49.100]   the first text-to-speech synthesizer,
[00:00:49.100 --> 00:00:52.000]   the first music synthesizer capable of recreating
[00:00:52.000 --> 00:00:55.040]   the grand piano and other orchestral instruments,
[00:00:55.040 --> 00:00:56.920]   and the first commercially marketed
[00:00:56.920 --> 00:00:59.400]   large vocabulary speech recognition.
[00:00:59.400 --> 00:01:02.620]   Among Ray's many honors, he received a Grammy Award
[00:01:02.620 --> 00:01:05.000]   for Outstanding Achievements in Music Technology.
[00:01:05.000 --> 00:01:07.720]   He is the recipient of the National Medal of Technology,
[00:01:07.720 --> 00:01:10.920]   was inducted into the National Inventors Hall of Fame,
[00:01:10.920 --> 00:01:14.320]   holds 21 honorary doctorates and honors
[00:01:14.320 --> 00:01:16.940]   from three US presidents.
[00:01:16.940 --> 00:01:20.240]   Ray has written five national best-selling books,
[00:01:20.240 --> 00:01:23.160]   including the New York Times bestsellers
[00:01:23.160 --> 00:01:25.640]   The Singularity is Near from 2005,
[00:01:25.640 --> 00:01:28.840]   and How to Create a Mind from 2012.
[00:01:28.840 --> 00:01:33.120]   He is co-founder and chancellor of Singularity University
[00:01:33.120 --> 00:01:36.260]   and a director of engineering at Google,
[00:01:36.260 --> 00:01:38.720]   heading up a team developing machine intelligence
[00:01:38.720 --> 00:01:40.600]   and natural language understanding.
[00:01:40.600 --> 00:01:42.640]   Please give Ray a warm welcome.
[00:01:42.640 --> 00:01:45.800]   (audience applauding)
[00:01:45.800 --> 00:01:52.640]   - It's good to be back.
[00:01:52.640 --> 00:01:55.800]   I've been in this lecture hall many times
[00:01:55.800 --> 00:01:57.760]   and walked the infinite corridor.
[00:01:57.760 --> 00:02:02.200]   I came here as an undergraduate in 1965.
[00:02:02.200 --> 00:02:05.100]   Within a year of my being here,
[00:02:05.100 --> 00:02:08.900]   they started a new major called computer science.
[00:02:08.900 --> 00:02:12.600]   It did not get its own course number.
[00:02:12.600 --> 00:02:15.000]   It's 6.1.
[00:02:15.000 --> 00:02:18.600]   Even biotechnology recently got its own course number.
[00:02:18.600 --> 00:02:21.260]   How many of you are CS majors?
[00:02:22.560 --> 00:02:25.680]   Okay, how many of you do work in deep learning?
[00:02:25.680 --> 00:02:29.680]   How many of you have heard of deep learning?
[00:02:29.680 --> 00:02:36.880]   I came here first in 1962 when I was 14.
[00:02:36.880 --> 00:02:42.000]   I became excited about artificial intelligence.
[00:02:42.000 --> 00:02:45.440]   It had only gotten its name six years earlier,
[00:02:45.440 --> 00:02:48.280]   the 1956 Dartmouth Conference
[00:02:48.280 --> 00:02:51.900]   by Marvin Minsky and John McCarthy.
[00:02:52.660 --> 00:02:56.500]   So I wrote Minsky a letter.
[00:02:56.500 --> 00:02:58.780]   There was no email back then.
[00:02:58.780 --> 00:03:00.540]   And he invited me up.
[00:03:00.540 --> 00:03:03.100]   He spent all day with me as if he had nothing else to do.
[00:03:03.100 --> 00:03:05.740]   He was a consummate educator.
[00:03:05.740 --> 00:03:11.580]   And the AI field had already bifurcated
[00:03:11.580 --> 00:03:13.220]   into two warring camps,
[00:03:13.220 --> 00:03:18.740]   the symbolic school which Minsky was associated with
[00:03:18.740 --> 00:03:22.780]   and the connectionist school was not widely known.
[00:03:22.780 --> 00:03:24.380]   In fact, I think it's still not widely known
[00:03:24.380 --> 00:03:29.300]   that Minsky actually invented the neural net in 1953.
[00:03:29.300 --> 00:03:32.100]   But he had become negative about it,
[00:03:32.100 --> 00:03:34.120]   largely 'cause there was a lot of hype
[00:03:34.120 --> 00:03:37.680]   that these giant brains could solve any problem.
[00:03:37.680 --> 00:03:43.540]   So the first popular neural net, the perceptron,
[00:03:45.040 --> 00:03:49.280]   was being promulgated by Frank Rosenblatt at Cornell.
[00:03:49.280 --> 00:03:50.920]   So Minsky said, "Oh, where are you going now?"
[00:03:50.920 --> 00:03:54.920]   And I said to see Rosenblatt at Cornell.
[00:03:54.920 --> 00:03:56.920]   He said, "Don't bother doing that."
[00:03:56.920 --> 00:04:01.920]   And I went there and Rosenblatt was touting the perceptron
[00:04:01.920 --> 00:04:04.980]   that it ultimately would be able to solve any problem.
[00:04:04.980 --> 00:04:08.120]   So I brought some printed letters that had the camera
[00:04:08.120 --> 00:04:10.960]   and it did a perfect job of recognizing them
[00:04:10.960 --> 00:04:13.080]   as long as they were carrier 10,
[00:04:14.080 --> 00:04:16.840]   different type style didn't work at all.
[00:04:16.840 --> 00:04:18.320]   And he said, "But don't worry.
[00:04:18.320 --> 00:04:20.560]   "We can take the output of the perceptron
[00:04:20.560 --> 00:04:22.560]   "and feed it as the input to another perceptron
[00:04:22.560 --> 00:04:25.460]   "and take the output of that and feed it to a third layer.
[00:04:25.460 --> 00:04:26.660]   "And as we add more layers,
[00:04:26.660 --> 00:04:29.520]   "it'll get smarter and smarter and generalized."
[00:04:29.520 --> 00:04:30.960]   And I said, "That's interesting.
[00:04:30.960 --> 00:04:31.960]   "Have you tried that?"
[00:04:31.960 --> 00:04:35.240]   Well, no, but it's high on our research agenda.
[00:04:35.240 --> 00:04:38.800]   Things did not move quite as quickly back then
[00:04:38.800 --> 00:04:39.720]   as they do now.
[00:04:39.720 --> 00:04:43.720]   He died nine years later, never having tried that idea.
[00:04:44.160 --> 00:04:46.040]   Turns out to be remarkably prescient.
[00:04:46.040 --> 00:04:49.520]   I mean, he never tried multi-layer neural nets
[00:04:49.520 --> 00:04:52.440]   and all the excitement that we see now about deep learning
[00:04:52.440 --> 00:04:58.320]   comes from a combination of two things,
[00:04:58.320 --> 00:05:04.480]   many layer neural nets and the law of accelerating returns,
[00:05:04.480 --> 00:05:07.120]   which I'll get to a little bit later,
[00:05:07.120 --> 00:05:10.440]   which is basically the exponential growth of computing
[00:05:10.440 --> 00:05:12.580]   so that we can run these massive nets
[00:05:12.580 --> 00:05:15.180]   and handle massive amounts of data.
[00:05:15.180 --> 00:05:20.600]   It would be decades before that idea was tried.
[00:05:20.600 --> 00:05:24.980]   Several decades later, three-level neural nets were tried.
[00:05:24.980 --> 00:05:25.940]   They were a little bit better.
[00:05:25.940 --> 00:05:28.280]   They could deal with multiple type styles,
[00:05:28.280 --> 00:05:30.400]   still weren't very flexible.
[00:05:30.400 --> 00:05:33.580]   Now, it's not hard to add other layers.
[00:05:33.580 --> 00:05:36.260]   It's a very straightforward concept.
[00:05:36.260 --> 00:05:37.800]   There was a math problem,
[00:05:37.800 --> 00:05:41.700]   the disappearing gradient or the exploding gradient,
[00:05:41.700 --> 00:05:43.900]   which I'm sure many of you are familiar with.
[00:05:43.900 --> 00:05:50.000]   Basically, you need to take maximum advantage
[00:05:50.000 --> 00:05:56.260]   of the range of values in the gradients,
[00:05:56.260 --> 00:06:00.260]   not let them explode or disappear and lose the resolution.
[00:06:00.260 --> 00:06:02.600]   That's a fairly straightforward
[00:06:02.600 --> 00:06:04.760]   mathematical transformation.
[00:06:04.760 --> 00:06:06.780]   With that insight, we could now go to
[00:06:06.780 --> 00:06:08.620]   100-layer neural nets.
[00:06:09.680 --> 00:06:13.880]   And that's behind sort of all the fantastic gains
[00:06:13.880 --> 00:06:16.140]   that we've seen recently.
[00:06:16.140 --> 00:06:22.560]   AlphaGo trained on every online game
[00:06:22.560 --> 00:06:27.920]   and then became a fair Go player.
[00:06:27.920 --> 00:06:30.960]   It then trained itself by playing itself
[00:06:30.960 --> 00:06:33.380]   and soared past the best human.
[00:06:33.380 --> 00:06:37.160]   AlphaGo Zero started with no human input at all.
[00:06:37.160 --> 00:06:42.160]   Within hours of iteration, soared past AlphaGo,
[00:06:42.160 --> 00:06:47.480]   also soared past the best chess programs.
[00:06:47.480 --> 00:06:49.080]   They had another innovation.
[00:06:49.080 --> 00:06:53.400]   Basically, you need to evaluate the quality of the board
[00:06:53.400 --> 00:06:58.000]   at each point, and they used another 100-layer neural net
[00:06:58.000 --> 00:06:59.880]   to do that evaluation.
[00:06:59.880 --> 00:07:06.220]   So, there's still a problem in the field.
[00:07:07.220 --> 00:07:09.980]   Which is, there's a motto that life begins
[00:07:09.980 --> 00:07:11.460]   at a billion examples.
[00:07:11.460 --> 00:07:14.100]   One of the reasons I'm at Google
[00:07:14.100 --> 00:07:16.140]   is we have a billion examples.
[00:07:16.140 --> 00:07:19.220]   For example, there's pictures of dogs and cats
[00:07:19.220 --> 00:07:21.460]   that are labeled, so you got a picture of a cat
[00:07:21.460 --> 00:07:23.420]   and it says cat, and then you can learn from it,
[00:07:23.420 --> 00:07:24.780]   and you need a lot of them.
[00:07:24.780 --> 00:07:29.780]   AlphaGo trained on a million online moves.
[00:07:29.780 --> 00:07:32.460]   That's how many we had of Master Games.
[00:07:33.540 --> 00:07:37.740]   And that only created a sort of fair Go player,
[00:07:37.740 --> 00:07:39.660]   a good amateur could defeat it.
[00:07:39.660 --> 00:07:43.700]   So, they worked around that in the case of Go
[00:07:43.700 --> 00:07:48.700]   by basically generating an infinite amount of data
[00:07:48.700 --> 00:07:50.980]   by having the system play itself.
[00:07:50.980 --> 00:07:55.500]   Had a chat with Demis Hassabis.
[00:07:55.500 --> 00:07:58.700]   What kind of situations can you do that with?
[00:07:58.700 --> 00:08:01.740]   You have to have some way of simulating the world.
[00:08:01.740 --> 00:08:04.740]   So, Go or Chess are, even though Go
[00:08:04.740 --> 00:08:07.020]   is considered a difficult game,
[00:08:07.020 --> 00:08:11.780]   the definition of it exists on one page.
[00:08:11.780 --> 00:08:13.700]   So, you can simulate it.
[00:08:13.700 --> 00:08:15.140]   That applies to math.
[00:08:15.140 --> 00:08:19.860]   I mean, math axioms can be contained on a page or two.
[00:08:19.860 --> 00:08:22.340]   It's not very complicated.
[00:08:22.340 --> 00:08:26.020]   Gets more difficult when you have real life situations,
[00:08:26.020 --> 00:08:27.660]   like biology.
[00:08:27.660 --> 00:08:29.460]   So, we have biological simulators,
[00:08:29.460 --> 00:08:30.980]   but the simulators aren't perfect.
[00:08:30.980 --> 00:08:33.100]   So, learning from the simulators
[00:08:33.100 --> 00:08:35.580]   will only be as good as the simulators.
[00:08:35.580 --> 00:08:38.140]   That's actually the key to being able
[00:08:38.140 --> 00:08:40.460]   to do deep learning on biology.
[00:08:40.460 --> 00:08:45.980]   Autonomous vehicles, you need real life data.
[00:08:45.980 --> 00:08:50.460]   So, the Waymo systems have gone three and a half
[00:08:50.460 --> 00:08:52.380]   million miles.
[00:08:52.380 --> 00:08:56.660]   That's enough data to then create a very good simulator.
[00:08:56.660 --> 00:08:58.500]   So, the simulator is really quite realistic
[00:08:58.500 --> 00:09:01.660]   because they had a lot of real world experience
[00:09:01.660 --> 00:09:06.100]   and they've gone a billion miles in the simulator.
[00:09:06.100 --> 00:09:09.140]   But we don't always have that opportunity
[00:09:09.140 --> 00:09:12.820]   to either create the data or have the data around.
[00:09:12.820 --> 00:09:17.820]   Humans can learn from a small number of examples.
[00:09:17.820 --> 00:09:21.580]   Your significant other, your professor, your boss,
[00:09:21.580 --> 00:09:24.860]   your investor can tell you something once or twice
[00:09:24.860 --> 00:09:27.020]   and you might actually learn from that.
[00:09:27.020 --> 00:09:29.780]   Some humans have been reported to do that.
[00:09:29.780 --> 00:09:34.780]   And that's kind of the remaining advantage of humans.
[00:09:34.780 --> 00:09:38.340]   Now, there's actually no back propagation
[00:09:38.340 --> 00:09:39.580]   in the human brain.
[00:09:39.580 --> 00:09:41.780]   It doesn't use deep learning.
[00:09:41.780 --> 00:09:43.580]   It uses a different architecture.
[00:09:43.580 --> 00:09:47.100]   That same year, in 1962, I wrote a paper,
[00:09:47.100 --> 00:09:49.500]   How I Thought the Human Brain Worked.
[00:09:49.500 --> 00:09:52.420]   There was actually very little neuroscience to go on.
[00:09:52.420 --> 00:09:55.340]   There was one neuroscientist, Vernon Mountcastle,
[00:09:55.340 --> 00:09:59.260]   that had something relevant to say, which is he did...
[00:09:59.260 --> 00:10:01.820]   I mean, there was the common wisdom at the time,
[00:10:01.820 --> 00:10:04.060]   and there's still a lot of neuroscientists that say this,
[00:10:04.060 --> 00:10:06.380]   that we have all these different regions of the brain,
[00:10:06.380 --> 00:10:10.580]   they do different things, they must be different.
[00:10:10.580 --> 00:10:12.180]   There's V1 in the back of the head
[00:10:12.180 --> 00:10:14.140]   where the optic nerve spills into,
[00:10:14.140 --> 00:10:18.460]   that can tell that that's a curved line,
[00:10:18.460 --> 00:10:20.140]   that's a straight line,
[00:10:20.140 --> 00:10:23.820]   does these simple feature extractions on visual images.
[00:10:23.820 --> 00:10:26.140]   That's actually a large part of the neocortex.
[00:10:26.140 --> 00:10:28.580]   There's a fusiform gyrus up here,
[00:10:28.580 --> 00:10:31.340]   which can recognize faces.
[00:10:31.340 --> 00:10:33.460]   We know that because if it gets knocked out
[00:10:33.460 --> 00:10:36.500]   through injury or stroke, people can't recognize faces.
[00:10:36.500 --> 00:10:37.500]   They will learn it again
[00:10:37.500 --> 00:10:39.900]   with a different region of the neocortex.
[00:10:39.900 --> 00:10:41.740]   There's the famous frontal cortex,
[00:10:41.740 --> 00:10:46.340]   which does language and poetry and music.
[00:10:46.340 --> 00:10:49.220]   So these must work on different principles.
[00:10:49.220 --> 00:10:51.340]   He did autopsies on the neocortex
[00:10:51.340 --> 00:10:52.540]   and all these different regions
[00:10:52.540 --> 00:10:53.780]   and found they all looked the same.
[00:10:53.780 --> 00:10:55.900]   They had the same repeating pattern,
[00:10:55.900 --> 00:10:57.780]   same interconnections.
[00:10:57.780 --> 00:10:59.780]   He said neocortex is neocortex.
[00:10:59.780 --> 00:11:01.980]   So I had that hint.
[00:11:01.980 --> 00:11:05.780]   Otherwise, I could actually observe human brains in action,
[00:11:05.780 --> 00:11:07.980]   which I did from time to time.
[00:11:07.980 --> 00:11:11.100]   And there's a lot of hints that you can get that way.
[00:11:11.100 --> 00:11:14.140]   For example, if I ask you to recite the alphabet,
[00:11:14.140 --> 00:11:15.980]   you actually don't do it from A to Z,
[00:11:15.980 --> 00:11:18.580]   you do it as a sequence of sequences,
[00:11:18.580 --> 00:11:22.420]   A, B, C, D, E, F, G, H, I, J, K.
[00:11:22.420 --> 00:11:26.180]   So we learn things as forward sequences of sequences.
[00:11:26.180 --> 00:11:29.380]   Forward, because if I ask you to recite the alphabet
[00:11:29.380 --> 00:11:31.980]   backwards, you can't do it unless you learn that
[00:11:31.980 --> 00:11:33.580]   as a new sequence.
[00:11:33.580 --> 00:11:36.220]   So these are all interesting hints.
[00:11:36.220 --> 00:11:40.300]   I wrote a paper that the neocortex is organized
[00:11:40.300 --> 00:11:41.860]   as a hierarchy of modules,
[00:11:41.860 --> 00:11:44.660]   and each module can learn a simple pattern.
[00:11:44.660 --> 00:11:48.620]   And that's how I got to meet President Johnson.
[00:11:48.620 --> 00:11:52.380]   And that initiated a half century
[00:11:52.380 --> 00:11:54.420]   of thinking about this issue.
[00:11:54.420 --> 00:11:58.060]   I came to MIT to study with Marvin Minsky.
[00:11:58.060 --> 00:11:59.340]   Actually, I came for two reasons.
[00:11:59.340 --> 00:12:02.260]   One, that Minsky became my mentor,
[00:12:02.260 --> 00:12:05.900]   which was a mentorship that lasted for over 50 years.
[00:12:05.900 --> 00:12:07.780]   The fact that MIT was so advanced,
[00:12:07.780 --> 00:12:09.260]   it actually had a computer,
[00:12:09.260 --> 00:12:13.380]   which the other colleges I considered didn't have.
[00:12:14.540 --> 00:12:19.540]   It was an IBM 7094, 32K of 36-bit words,
[00:12:19.540 --> 00:12:22.420]   so it's 150K of core storage,
[00:12:22.420 --> 00:12:24.180]   two microsecond cycle time,
[00:12:24.180 --> 00:12:26.660]   two cycles for instruction, so a quarter of a MIP.
[00:12:26.660 --> 00:12:30.460]   And thousands of students and professors
[00:12:30.460 --> 00:12:32.060]   shared that one machine.
[00:12:32.060 --> 00:12:36.500]   In 2012, I wrote a book about this thesis.
[00:12:36.500 --> 00:12:39.780]   It's now actually an explosion of neuroscience evidence
[00:12:39.780 --> 00:12:40.980]   to support it.
[00:12:40.980 --> 00:12:43.900]   The European Brain Reverse Engineering Project
[00:12:43.900 --> 00:12:47.420]   has identified a repeating module of about 100 neurons.
[00:12:47.420 --> 00:12:50.100]   It's repeated 300 million times,
[00:12:50.100 --> 00:12:53.140]   so it's about 30 billion neurons in the neocortex.
[00:12:53.140 --> 00:12:55.340]   The neocortex is the outer layer of the brain.
[00:12:55.340 --> 00:12:58.820]   That's the part where we do our thinking.
[00:12:58.820 --> 00:13:00.420]   And they can see in each module,
[00:13:00.420 --> 00:13:04.660]   axons coming in from another module.
[00:13:04.660 --> 00:13:08.020]   And then the output, the single output axon of that module
[00:13:08.020 --> 00:13:10.260]   goes as the input to another module.
[00:13:10.260 --> 00:13:13.300]   So we can see it organized as a hierarchy.
[00:13:13.300 --> 00:13:16.340]   It's not a physical hierarchy.
[00:13:16.340 --> 00:13:18.660]   The hierarchy comes from these connections.
[00:13:18.660 --> 00:13:20.740]   The neocortex is a very thin structure.
[00:13:20.740 --> 00:13:22.980]   It's actually one module thick.
[00:13:22.980 --> 00:13:25.340]   There's six layers of neurons,
[00:13:25.340 --> 00:13:28.060]   but it constitutes one module.
[00:13:28.060 --> 00:13:31.540]   And we can see that it learns a simple pattern.
[00:13:31.540 --> 00:13:34.780]   And for various reasons, I cite in the book,
[00:13:34.780 --> 00:13:37.580]   the pattern recognition model it's using
[00:13:37.580 --> 00:13:40.380]   is basically a hidden Markov model.
[00:13:40.380 --> 00:13:43.820]   How many of you have worked with Markov models?
[00:13:43.820 --> 00:13:44.660]   Okay.
[00:13:44.660 --> 00:13:49.860]   That's usually no hands go up when I ask that question.
[00:13:49.860 --> 00:13:54.540]   But a Markov model is not, it is learned,
[00:13:54.540 --> 00:13:56.780]   but it's not back propagation.
[00:13:56.780 --> 00:13:58.700]   It can learn local features.
[00:13:58.700 --> 00:14:00.500]   So it's very good for speech recognition.
[00:14:00.500 --> 00:14:03.900]   And the speech recognition work I did in the 80s
[00:14:03.900 --> 00:14:08.220]   used these Markov models that became the standard approach
[00:14:08.220 --> 00:14:11.540]   because it can deal with local variations.
[00:14:11.540 --> 00:14:15.460]   So the fact that a vowel is stretched,
[00:14:15.460 --> 00:14:18.100]   you can learn that in a Markov model.
[00:14:18.100 --> 00:14:21.740]   It doesn't learn long distance relationships.
[00:14:21.740 --> 00:14:24.340]   That's handled by the hierarchy.
[00:14:24.340 --> 00:14:26.340]   And something we don't fully understand yet
[00:14:26.340 --> 00:14:30.460]   is exactly how the neocortex creates that hierarchy.
[00:14:30.460 --> 00:14:33.740]   But we have figured out how it can connect
[00:14:33.740 --> 00:14:36.020]   this module to this module.
[00:14:36.020 --> 00:14:37.340]   Does it then grow?
[00:14:37.340 --> 00:14:39.580]   I mean, there's no virtual communication
[00:14:39.580 --> 00:14:41.180]   or wireless communication.
[00:14:41.180 --> 00:14:42.940]   It's an actual connection.
[00:14:42.940 --> 00:14:47.060]   So does it grow an axon from one place to another,
[00:14:47.060 --> 00:14:50.020]   which could be inches apart?
[00:14:50.020 --> 00:14:53.780]   Actually, all these connections are there from birth,
[00:14:53.780 --> 00:14:57.700]   like the streets and avenues of Manhattan.
[00:14:57.700 --> 00:14:59.620]   There's vertical and horizontal connections.
[00:14:59.620 --> 00:15:03.860]   So if it decides and how it makes that decision
[00:15:03.860 --> 00:15:05.740]   is still not fully understood,
[00:15:05.740 --> 00:15:08.060]   but it wants to connect this module to this module,
[00:15:08.060 --> 00:15:09.620]   there's already a vertical,
[00:15:09.620 --> 00:15:11.300]   horizontal and a vertical connection.
[00:15:11.300 --> 00:15:12.740]   It just activates them.
[00:15:12.740 --> 00:15:15.500]   We can actually see that now.
[00:15:15.500 --> 00:15:17.940]   And it can see that happening in real time
[00:15:17.940 --> 00:15:21.460]   on non-invasive brain scans.
[00:15:21.460 --> 00:15:23.140]   So there's a tremendous amount of evidence
[00:15:23.140 --> 00:15:28.140]   that's in fact, the neocortex is a hierarchy of modules
[00:15:28.140 --> 00:15:31.900]   that can learn, each module learns
[00:15:31.900 --> 00:15:34.980]   a simple sequential pattern.
[00:15:34.980 --> 00:15:38.100]   And even though the patterns we perceive
[00:15:38.100 --> 00:15:39.700]   don't seem like sequences,
[00:15:39.700 --> 00:15:42.980]   they may seem three-dimensional or even more complicated,
[00:15:42.980 --> 00:15:46.220]   they are in fact represented as sequences,
[00:15:46.220 --> 00:15:48.620]   but the complexity comes in with the hierarchy.
[00:15:48.620 --> 00:15:54.460]   So the neocortex emerged 200 million years ago with mammals.
[00:15:54.460 --> 00:15:57.020]   All mammals have a neocortex.
[00:15:57.020 --> 00:16:00.740]   That's one of the distinguishing features of mammals.
[00:16:00.740 --> 00:16:02.740]   These first mammals were small.
[00:16:02.740 --> 00:16:04.980]   They were rodents,
[00:16:04.980 --> 00:16:07.780]   but they were capable of a new type of thinking.
[00:16:07.780 --> 00:16:11.220]   Other non-mammalian animals had fixed behaviors,
[00:16:11.220 --> 00:16:14.100]   but those fixed behaviors were very well adapted
[00:16:14.100 --> 00:16:16.180]   for their ecological niche.
[00:16:16.180 --> 00:16:20.340]   But these new mammals could invent a new behavior.
[00:16:20.340 --> 00:16:21.980]   So creativity and innovation
[00:16:21.980 --> 00:16:24.340]   was one feature of the neocortex.
[00:16:24.340 --> 00:16:27.020]   So a mouse is escaping a predator,
[00:16:27.020 --> 00:16:29.380]   its usual escape path is blocked,
[00:16:29.380 --> 00:16:32.700]   it will invent a new behavior to deal with it.
[00:16:32.700 --> 00:16:33.540]   Probably wouldn't work,
[00:16:33.540 --> 00:16:35.180]   but if it did work, it would remember it
[00:16:35.180 --> 00:16:36.980]   and it would have a new behavior.
[00:16:36.980 --> 00:16:40.180]   And that behavior could spread virally through the community.
[00:16:40.180 --> 00:16:43.020]   Another mouse watching this would say to itself,
[00:16:43.020 --> 00:16:45.060]   hmm, that was really clever going around that rock,
[00:16:45.060 --> 00:16:47.140]   I'm gonna remember to do that.
[00:16:47.140 --> 00:16:51.260]   And it would have a new behavior.
[00:16:51.260 --> 00:16:54.020]   Didn't help these early mammals that much
[00:16:54.020 --> 00:16:56.700]   because as I say, the non-mammalian animals
[00:16:56.700 --> 00:16:59.980]   were very well adapted to their niches
[00:17:01.620 --> 00:17:05.580]   and nothing much happened for 135 million years.
[00:17:05.580 --> 00:17:09.660]   But then 65 million years ago, something did happen.
[00:17:09.660 --> 00:17:12.900]   There was a sudden violent change to the environment.
[00:17:12.900 --> 00:17:15.980]   We now call it the Cretaceous extinction event.
[00:17:15.980 --> 00:17:18.460]   There's been debate as to whether it was a meteor
[00:17:18.460 --> 00:17:23.460]   or an asteroid, I mean a meteor or a volcanic eruption.
[00:17:23.460 --> 00:17:29.500]   The asteroid or meteor hypothesis is in the ascendancy.
[00:17:29.500 --> 00:17:32.380]   But if you dig down to an area of rock
[00:17:32.380 --> 00:17:34.460]   reflecting 65 million years ago,
[00:17:34.460 --> 00:17:37.060]   the geologists will explain that it shows
[00:17:37.060 --> 00:17:39.860]   a very violent sudden change to the environment.
[00:17:39.860 --> 00:17:42.420]   And we see it all around the globe.
[00:17:42.420 --> 00:17:44.780]   So it was a worldwide phenomenon.
[00:17:44.780 --> 00:17:48.060]   The reason we call it an extinction event
[00:17:48.060 --> 00:17:51.300]   is that's when the dinosaurs went extinct.
[00:17:51.300 --> 00:17:54.700]   That's when 75% of all the animal
[00:17:54.700 --> 00:17:56.660]   and plant species went extinct.
[00:17:56.660 --> 00:18:00.340]   And that's when mammals overtook their ecological niche.
[00:18:00.340 --> 00:18:03.380]   So to anthropomorphize biological evolution
[00:18:03.380 --> 00:18:06.420]   said to itself, this neocortex is pretty good stuff
[00:18:06.420 --> 00:18:07.940]   and it began to grow it.
[00:18:07.940 --> 00:18:10.740]   So now mammals got bigger, their brains got bigger
[00:18:10.740 --> 00:18:13.220]   at an even faster pace, taking up a larger fraction
[00:18:13.220 --> 00:18:14.500]   of their body.
[00:18:14.500 --> 00:18:17.700]   The neocortex got bigger even faster than that
[00:18:17.700 --> 00:18:20.380]   and developed these curvatures that are distinctive
[00:18:20.380 --> 00:18:25.380]   of a primate brain basically to increase its surface area.
[00:18:25.540 --> 00:18:27.700]   But if you stretched it out,
[00:18:27.700 --> 00:18:29.900]   the human neocortex is still a flat structure.
[00:18:29.900 --> 00:18:33.580]   It's about the size of a table napkin, just as thin.
[00:18:33.580 --> 00:18:39.740]   And it basically created primates
[00:18:39.740 --> 00:18:45.900]   which became dominant in their ecological niche.
[00:18:45.900 --> 00:18:49.620]   Then something else happened 2 million years ago.
[00:18:49.620 --> 00:18:53.260]   Biological evolution decided to increase
[00:18:53.260 --> 00:18:56.140]   the neocortex further and increase the size
[00:18:56.140 --> 00:19:00.260]   of the enclosure and basically filled up the frontal cortex
[00:19:00.260 --> 00:19:03.900]   with our big skulls with more neocortex.
[00:19:03.900 --> 00:19:06.940]   And up until recently it was felt, as I said,
[00:19:06.940 --> 00:19:09.860]   that this was, the frontal cortex was different
[00:19:09.860 --> 00:19:12.540]   'cause it does these qualitatively different things.
[00:19:12.540 --> 00:19:18.980]   But we now realize that it's really
[00:19:18.980 --> 00:19:21.420]   just additional neocortex.
[00:19:22.220 --> 00:19:27.220]   So remember what we did with it.
[00:19:27.220 --> 00:19:30.100]   We're already doing a very good job of being primates.
[00:19:30.100 --> 00:19:33.580]   So we put it at the top of the neocortical hierarchy
[00:19:33.580 --> 00:19:36.700]   and we increased the size of the hierarchy.
[00:19:36.700 --> 00:19:39.020]   It was maybe 20% more neocortex,
[00:19:39.020 --> 00:19:41.940]   but it doubled or tripled the number of levels
[00:19:41.940 --> 00:19:43.180]   'cause as you go up the hierarchy,
[00:19:43.180 --> 00:19:44.340]   it's kind of like a pyramid.
[00:19:44.340 --> 00:19:46.140]   There's fewer and fewer modules.
[00:19:47.380 --> 00:19:50.500]   And that was the enabling factor for us
[00:19:50.500 --> 00:19:53.860]   to invent language and art, music.
[00:19:53.860 --> 00:19:57.300]   Every human culture we've ever discovered has music.
[00:19:57.300 --> 00:19:59.780]   No primate culture really has music.
[00:19:59.780 --> 00:20:03.020]   There's debate about that, but it's really true.
[00:20:03.020 --> 00:20:07.500]   Invention, technology.
[00:20:07.500 --> 00:20:10.620]   Technology required another evolutionary adaptation,
[00:20:10.620 --> 00:20:13.940]   which is this humble appendage here.
[00:20:13.940 --> 00:20:15.220]   No other animal has that.
[00:20:15.220 --> 00:20:16.220]   If you look at a chimpanzee,
[00:20:16.220 --> 00:20:17.420]   it looks like they have a similar hand,
[00:20:17.420 --> 00:20:19.100]   but the thumb is actually down here.
[00:20:19.100 --> 00:20:20.620]   Doesn't work very well if you watch them
[00:20:20.620 --> 00:20:22.740]   trying to grab a stick.
[00:20:22.740 --> 00:20:26.100]   So we could imagine creative solutions.
[00:20:26.100 --> 00:20:29.460]   Yeah, I could take that branch and strip off the leaves
[00:20:29.460 --> 00:20:31.060]   and put a point on it.
[00:20:31.060 --> 00:20:36.020]   We could actually carry out these ideas and create tools
[00:20:36.020 --> 00:20:38.420]   and then use tools to create new tools
[00:20:38.420 --> 00:20:41.260]   and started a whole other evolutionary process
[00:20:41.260 --> 00:20:42.740]   of tool making.
[00:20:42.740 --> 00:20:45.620]   And that all came with the neocortex.
[00:20:46.620 --> 00:20:51.620]   So Larry Page read my book in 2012 and liked it.
[00:20:51.620 --> 00:20:55.460]   So I met with him and asked him for an investment
[00:20:55.460 --> 00:20:58.820]   in a company I'd started actually a couple of weeks earlier
[00:20:58.820 --> 00:21:01.660]   to develop those ideas commercially
[00:21:01.660 --> 00:21:03.140]   'cause that's how I went about things
[00:21:03.140 --> 00:21:05.380]   as a serial entrepreneur.
[00:21:05.380 --> 00:21:08.380]   And he said, "Well, we'll invest,
[00:21:08.380 --> 00:21:09.540]   "but let me give you a better idea.
[00:21:09.540 --> 00:21:11.500]   "Why don't you do it here at Google?
[00:21:11.500 --> 00:21:13.780]   "We have a billion pictures of dogs and cats
[00:21:13.780 --> 00:21:15.420]   "and we've got a lot of other data
[00:21:15.420 --> 00:21:17.660]   "and lots of computers and lots of talent,
[00:21:17.660 --> 00:21:19.540]   "all of which is true."
[00:21:19.540 --> 00:21:20.380]   And I says, "Well, I don't know.
[00:21:20.380 --> 00:21:23.860]   "I just started this company to develop this."
[00:21:23.860 --> 00:21:25.140]   He says, "Well, buy your company."
[00:21:25.140 --> 00:21:26.940]   And I said, "How are you gonna value a company
[00:21:26.940 --> 00:21:28.580]   "that hasn't done anything?
[00:21:28.580 --> 00:21:30.020]   "It just started a couple of weeks ago."
[00:21:30.020 --> 00:21:32.380]   And he said, "We can value anything."
[00:21:32.380 --> 00:21:38.460]   So I took my first job five years ago
[00:21:38.460 --> 00:21:41.740]   and I've been basically applying this model,
[00:21:41.740 --> 00:21:46.620]   this hierarchical model to understanding language,
[00:21:46.620 --> 00:21:50.500]   which I think really is the holy grail of AI.
[00:21:50.500 --> 00:21:54.700]   I think Turing was correct in designating
[00:21:54.700 --> 00:21:56.740]   basically text communication
[00:21:56.740 --> 00:22:01.260]   as what we now call a Turing-complete problem
[00:22:01.260 --> 00:22:04.420]   that requires, there's no simple NLP tricks
[00:22:04.420 --> 00:22:08.380]   that you can apply to pass a valid Turing test
[00:22:08.380 --> 00:22:10.860]   with an emphasis on the word valid.
[00:22:10.860 --> 00:22:13.740]   Mitch Caper and I had a six-month debate
[00:22:13.740 --> 00:22:15.020]   on what the rules should be
[00:22:15.020 --> 00:22:18.820]   'cause if you read Turing's 1950 paper,
[00:22:18.820 --> 00:22:20.700]   he describes this in a few paragraphs
[00:22:20.700 --> 00:22:22.380]   and doesn't really describe how to go about it.
[00:22:22.380 --> 00:22:25.180]   But if it's a valid Turing test,
[00:22:25.180 --> 00:22:27.620]   meaning it's really convincing you
[00:22:27.620 --> 00:22:32.620]   through interrogation and dialogue that it's a human,
[00:22:32.620 --> 00:22:37.820]   that requires a full range of human intelligence.
[00:22:37.820 --> 00:22:41.460]   And I think that test has stood the test of time.
[00:22:41.460 --> 00:22:44.260]   We're making very good progress on that.
[00:22:44.260 --> 00:22:46.180]   I mean, just last week, you may have read
[00:22:46.180 --> 00:22:51.180]   that two systems passed a paragraph comprehension test.
[00:22:51.180 --> 00:22:55.940]   It's really very impressive.
[00:22:55.940 --> 00:22:57.980]   When I came to Google,
[00:22:57.980 --> 00:23:01.740]   we were trying to pass these paragraph comprehension tests.
[00:23:01.740 --> 00:23:05.820]   We aced the first grade test.
[00:23:05.820 --> 00:23:08.820]   Second grade test, we kind of got average performance.
[00:23:08.820 --> 00:23:12.100]   And the third grade test had too much inference.
[00:23:12.100 --> 00:23:15.420]   Already you had to know some common sense knowledge
[00:23:15.420 --> 00:23:19.260]   as it's called and make implications of things
[00:23:19.260 --> 00:23:21.380]   that were in different parts of the paragraph.
[00:23:21.380 --> 00:23:24.380]   And there's too much inference and it really didn't work.
[00:23:24.380 --> 00:23:27.500]   So this is now adult level,
[00:23:27.500 --> 00:23:30.660]   just slightly surpassed average human performance.
[00:23:30.660 --> 00:23:33.700]   But we've seen that once something,
[00:23:33.700 --> 00:23:37.180]   an AI does something at average human levels,
[00:23:37.180 --> 00:23:41.180]   it doesn't take long for it to surpassed average human levels.
[00:23:41.180 --> 00:23:42.700]   I think it'll take longer in language
[00:23:42.700 --> 00:23:44.980]   than it did in sort of simple games like Go,
[00:23:44.980 --> 00:23:47.140]   but it's actually very impressive
[00:23:47.140 --> 00:23:50.660]   that it surpasses now average human performance.
[00:23:50.660 --> 00:23:55.940]   It's used in LSTM, long, short temporal memory.
[00:23:55.940 --> 00:23:58.500]   But if you look at the adult test,
[00:23:58.500 --> 00:24:00.060]   in order to answer these questions,
[00:24:00.060 --> 00:24:03.260]   it has to put together inferences and implications
[00:24:03.260 --> 00:24:05.660]   of several different things in the paragraph
[00:24:05.660 --> 00:24:08.980]   with some common sense knowledge that's not explicitly stated.
[00:24:08.980 --> 00:24:14.460]   So that's, I think, a pretty impressive milestone.
[00:24:14.460 --> 00:24:19.300]   So I've been developing, I've got a team of about 45 people,
[00:24:19.300 --> 00:24:24.060]   and we've been developing this hierarchical model.
[00:24:24.060 --> 00:24:26.060]   We don't use Markov models
[00:24:26.060 --> 00:24:30.220]   'cause we can use deep learning for each module.
[00:24:30.220 --> 00:24:32.340]   And so we create an embedding for each word
[00:24:32.340 --> 00:24:35.180]   and we create an embedding for each sentence.
[00:24:35.180 --> 00:24:38.340]   This, we have a, I can talk about it
[00:24:38.340 --> 00:24:40.420]   'cause we have a published paper on it.
[00:24:40.420 --> 00:24:43.780]   It can take into consideration context.
[00:24:43.780 --> 00:24:49.620]   If you use Smart Reply on, if you use Gmail on your phone,
[00:24:49.620 --> 00:24:53.100]   you'll see it gives you three suggestions for responses.
[00:24:53.100 --> 00:24:55.500]   That's called Smart Reply.
[00:24:55.500 --> 00:24:58.980]   They're simple suggestions,
[00:24:58.980 --> 00:25:01.180]   but it has to actually understand
[00:25:01.180 --> 00:25:03.460]   perhaps a complicated email.
[00:25:03.460 --> 00:25:06.540]   And the quality of the suggestions is really quite good,
[00:25:06.540 --> 00:25:08.140]   quite on point.
[00:25:08.140 --> 00:25:12.380]   That's for my team using this kind of hierarchical model.
[00:25:12.380 --> 00:25:15.060]   So instead of Markov models, it uses embeddings
[00:25:15.060 --> 00:25:20.780]   'cause we can use backpropagation, we might as well use it.
[00:25:20.780 --> 00:25:25.540]   But I think what's missing from deep learning
[00:25:25.540 --> 00:25:29.100]   is this hierarchical aspect of understanding
[00:25:29.100 --> 00:25:30.580]   'cause the world is hierarchical.
[00:25:30.580 --> 00:25:35.300]   That's why evolution developed a hierarchical brain structure
[00:25:35.300 --> 00:25:39.180]   to understand the natural hierarchy in the world.
[00:25:39.180 --> 00:25:44.780]   And there's several problems with big, deep neural nets.
[00:25:44.780 --> 00:25:48.100]   One is the fact that you really do need a billion examples
[00:25:48.100 --> 00:25:50.260]   and we don't, sometimes we can generate them
[00:25:50.260 --> 00:25:52.500]   as in the case of Go,
[00:25:52.500 --> 00:25:54.660]   or if we have a really good simulator
[00:25:54.660 --> 00:25:56.500]   as in the case of autonomous vehicles,
[00:25:56.500 --> 00:25:58.700]   not quite the case yet in biology.
[00:25:59.980 --> 00:26:02.820]   Very often you don't have a billion examples.
[00:26:02.820 --> 00:26:05.420]   We certainly have billions of examples of language,
[00:26:05.420 --> 00:26:06.660]   but they're not annotated.
[00:26:06.660 --> 00:26:08.220]   And how would you annotate it anyway
[00:26:08.220 --> 00:26:09.980]   with more language that we can't understand
[00:26:09.980 --> 00:26:10.820]   in the first place?
[00:26:10.820 --> 00:26:13.780]   So it's kind of a chicken and an egg problem.
[00:26:13.780 --> 00:26:17.100]   So I believe this hierarchical structure is needed.
[00:26:17.100 --> 00:26:19.340]   Another criticism of deep neural nets
[00:26:19.340 --> 00:26:21.180]   is they don't explain themselves very well.
[00:26:21.180 --> 00:26:25.780]   It's a big black box that gives you
[00:26:25.780 --> 00:26:27.260]   pretty remarkable answers.
[00:26:27.380 --> 00:26:29.500]   I mean, in the case of these games,
[00:26:29.500 --> 00:26:32.940]   Demis described it's playing in both Go and chess
[00:26:32.940 --> 00:26:34.580]   as almost an alien intelligence
[00:26:34.580 --> 00:26:37.940]   'cause we do things that were shocking to human experts
[00:26:37.940 --> 00:26:42.020]   like sacrificing a queen and a bishop at the same time
[00:26:42.020 --> 00:26:46.100]   or in close succession, which shocked everybody,
[00:26:46.100 --> 00:26:47.700]   but then went on to win,
[00:26:47.700 --> 00:26:48.940]   or early in a Go game,
[00:26:48.940 --> 00:26:52.180]   putting a piece at the corner of the board,
[00:26:52.180 --> 00:26:55.220]   which is kind of crazy to most experts
[00:26:55.220 --> 00:26:57.740]   'cause you really wanna start controlling territory.
[00:26:57.740 --> 00:27:00.940]   And yet on reflection, that was the brilliant move
[00:27:00.940 --> 00:27:02.940]   that enabled it to win that game.
[00:27:02.940 --> 00:27:07.620]   But it doesn't really explain how it does these things.
[00:27:07.620 --> 00:27:09.940]   So if you have a hierarchy,
[00:27:09.940 --> 00:27:11.380]   it's much better at explaining it
[00:27:11.380 --> 00:27:13.180]   'cause you could look at the content
[00:27:13.180 --> 00:27:16.300]   of the modules in the hierarchy
[00:27:16.300 --> 00:27:19.300]   and they'll explain what they're doing.
[00:27:20.340 --> 00:27:25.340]   And just to end on the first application
[00:27:25.340 --> 00:27:28.340]   of applying this to health and medicine,
[00:27:28.340 --> 00:27:29.660]   this will get into high gear
[00:27:29.660 --> 00:27:31.500]   and we're gonna really see us break out
[00:27:31.500 --> 00:27:35.540]   of the linear extension to longevity
[00:27:35.540 --> 00:27:37.340]   that we've experienced.
[00:27:37.340 --> 00:27:39.260]   I believe we're only about a decade away
[00:27:39.260 --> 00:27:41.380]   from longevity escape velocity.
[00:27:41.380 --> 00:27:44.580]   We're adding more time than is going by,
[00:27:44.580 --> 00:27:46.340]   not just to infant life expectancy,
[00:27:46.340 --> 00:27:49.340]   but to your remaining life expectancy.
[00:27:49.340 --> 00:27:50.740]   I think if someone is diligent,
[00:27:50.740 --> 00:27:52.340]   they can be there already.
[00:27:52.340 --> 00:27:56.060]   I think I've at longevity escape velocity.
[00:27:56.060 --> 00:28:01.060]   Now, a word on what life expectancy means.
[00:28:01.060 --> 00:28:03.780]   It used to be assumed that not much would happen.
[00:28:03.780 --> 00:28:07.900]   So whatever your life expectancy is
[00:28:07.900 --> 00:28:09.780]   with or without scientific progress,
[00:28:09.780 --> 00:28:11.260]   it really didn't matter.
[00:28:11.260 --> 00:28:12.500]   Now it matters a lot.
[00:28:12.500 --> 00:28:14.740]   So life expectancy really means,
[00:28:14.740 --> 00:28:16.220]   how long would you live?
[00:28:17.380 --> 00:28:20.940]   What's the, in terms of a statistical likelihood,
[00:28:20.940 --> 00:28:24.340]   if there were not continued scientific progress?
[00:28:24.340 --> 00:28:26.260]   But that's a very inaccurate assumption.
[00:28:26.260 --> 00:28:29.220]   The scientific progress is extremely rapid.
[00:28:29.220 --> 00:28:31.460]   I mean, just as an AI in biotech,
[00:28:31.460 --> 00:28:33.860]   there are advances now every week.
[00:28:33.860 --> 00:28:35.980]   It's quite stunning.
[00:28:35.980 --> 00:28:41.260]   Now, you could have a computed life expectancy,
[00:28:41.260 --> 00:28:46.180]   let's say 30 years, 50 years, 70 years from now,
[00:28:46.180 --> 00:28:49.420]   you could still be hit by the proverbial bus tomorrow.
[00:28:49.420 --> 00:28:52.540]   We're working on that with self-driving vehicles.
[00:28:52.540 --> 00:28:56.540]   But we'll get to a point,
[00:28:56.540 --> 00:28:57.580]   I think if you're diligent,
[00:28:57.580 --> 00:29:01.180]   you can be there now in terms of basically advancing
[00:29:01.180 --> 00:29:04.580]   your own statistical life expectancy,
[00:29:04.580 --> 00:29:08.860]   at least to keep pace with the passage of time.
[00:29:08.860 --> 00:29:12.780]   I think it will be there for most of the population,
[00:29:12.780 --> 00:29:17.060]   at least if they're diligent within about a decade.
[00:29:17.060 --> 00:29:19.260]   So if you can hang in there,
[00:29:19.260 --> 00:29:21.700]   we may get to see the remarkable century ahead.
[00:29:21.700 --> 00:29:23.340]   Thank you very much.
[00:29:23.340 --> 00:29:26.500]   (audience applauding)
[00:29:26.500 --> 00:29:29.140]   - A question, please raise your hand,
[00:29:29.140 --> 00:29:30.180]   we'll get you a mic.
[00:29:30.180 --> 00:29:37.780]   - Hi, so you mentioned both neural network models
[00:29:37.780 --> 00:29:39.460]   and symbolic models.
[00:29:39.460 --> 00:29:42.780]   And I was wondering how far have you been thinking
[00:29:42.780 --> 00:29:45.020]   about combining these two approaches,
[00:29:45.020 --> 00:29:47.940]   creating a symbiosis between neural models
[00:29:47.940 --> 00:29:49.140]   and symbolic ones?
[00:29:49.140 --> 00:29:54.380]   - I don't think we wanna use symbolic models
[00:29:54.380 --> 00:29:56.780]   as they've been used.
[00:29:56.780 --> 00:30:00.980]   How many are familiar with the Psych Project?
[00:30:00.980 --> 00:30:06.220]   That was a very diligent effort in Texas
[00:30:06.220 --> 00:30:09.060]   to define all of common sense reasoning.
[00:30:09.060 --> 00:30:12.460]   And it kind of collapsed on itself.
[00:30:13.460 --> 00:30:15.740]   And became impossible to debug
[00:30:15.740 --> 00:30:17.820]   'cause you'd fix one thing
[00:30:17.820 --> 00:30:19.940]   and it would break three other things.
[00:30:19.940 --> 00:30:24.620]   That complexity ceiling has become typical
[00:30:24.620 --> 00:30:29.620]   of trying to define things through logical rules.
[00:30:29.620 --> 00:30:34.220]   Now it does seem that humans can understand logical rules.
[00:30:34.220 --> 00:30:37.100]   We have logical rules written down for things like law
[00:30:37.100 --> 00:30:41.780]   and game playing and so on.
[00:30:41.780 --> 00:30:46.580]   But you can actually define a connectionist system
[00:30:46.580 --> 00:30:51.580]   to have such a high reliability on a certain type of action
[00:30:51.580 --> 00:30:55.780]   that it looks like it's a symbolic rule
[00:30:55.780 --> 00:31:00.780]   even though it's represented in a connectionist way.
[00:31:00.780 --> 00:31:05.500]   And connection systems can both capture the soft edges
[00:31:05.500 --> 00:31:09.580]   'cause many things in life are not sharply defined.
[00:31:09.580 --> 00:31:12.060]   They can also generate exceptions.
[00:31:12.060 --> 00:31:15.340]   So you don't wanna sacrifice your queen in chess
[00:31:15.340 --> 00:31:18.980]   except certain situations that might be a good idea.
[00:31:18.980 --> 00:31:21.260]   So you can capture that kind of complexity.
[00:31:21.260 --> 00:31:27.580]   So we do wanna be able to learn from accumulated human wisdom
[00:31:27.580 --> 00:31:29.700]   that looks like it's symbolic.
[00:31:29.700 --> 00:31:33.300]   But I think we'll do it with a connectionist system.
[00:31:33.300 --> 00:31:37.020]   But again, I think that connectionist systems
[00:31:37.020 --> 00:31:40.420]   should develop a sense of hierarchy
[00:31:40.420 --> 00:31:43.820]   and not just be one big massive neural net.
[00:31:43.820 --> 00:31:48.420]   - So I understand how we wanna use the neocortex
[00:31:48.420 --> 00:31:51.460]   to extract useful stuff and commercialize that.
[00:31:51.460 --> 00:31:54.820]   But I'm wondering how our middle brain
[00:31:54.820 --> 00:31:57.140]   and the organs that are below the neocortex
[00:31:57.140 --> 00:32:01.780]   will be useful for turning that into what you wanna do.
[00:32:01.780 --> 00:32:05.940]   - Well, the cerebellum is an interesting case in point.
[00:32:05.940 --> 00:32:10.220]   It actually has more neurons than the neocortex.
[00:32:10.220 --> 00:32:15.140]   And it's used to govern most of our behavior.
[00:32:15.140 --> 00:32:18.180]   Some things, if you write a signature,
[00:32:18.180 --> 00:32:20.020]   that's actually controlled by the cerebellum.
[00:32:20.020 --> 00:32:25.020]   So a simple sequence is stored in the cerebellum.
[00:32:25.020 --> 00:32:28.340]   But there's not any reasoning to it.
[00:32:28.340 --> 00:32:29.980]   It's basically a script.
[00:32:29.980 --> 00:32:33.900]   And most of our movement now
[00:32:33.900 --> 00:32:36.380]   has actually been migrated from the cerebellum
[00:32:36.380 --> 00:32:37.940]   to the neocortex.
[00:32:37.940 --> 00:32:39.660]   Cerebellum is still there.
[00:32:39.660 --> 00:32:45.660]   Some people, entire cerebellum is destroyed through disease.
[00:32:45.660 --> 00:32:48.340]   They still function fairly normally.
[00:32:48.340 --> 00:32:50.860]   Their movement might be a little erratic
[00:32:50.860 --> 00:32:53.500]   'cause our movement is largely controlled
[00:32:53.500 --> 00:32:54.380]   by the neocortex.
[00:32:54.380 --> 00:32:58.500]   But some of the subtlety is a kind of pre-programmed script.
[00:32:58.500 --> 00:33:01.380]   And so they'll look a little clumsy,
[00:33:01.380 --> 00:33:03.380]   but they actually function okay.
[00:33:03.980 --> 00:33:06.940]   A lot of other areas of the brain
[00:33:06.940 --> 00:33:09.940]   control autonomic functions like breathing.
[00:33:09.940 --> 00:33:14.420]   But our thinking really is controlled by the neocortex.
[00:33:14.420 --> 00:33:18.780]   In terms of mastering intelligence,
[00:33:18.780 --> 00:33:23.780]   I think the neocortex is the brain region we wanna study.
[00:33:23.780 --> 00:33:29.220]   - I'm curious what you think might happen
[00:33:29.220 --> 00:33:31.300]   after the singularity is reached
[00:33:31.300 --> 00:33:34.420]   in terms of this exponential growth of information.
[00:33:34.420 --> 00:33:37.860]   Yeah, do you think it will continue
[00:33:37.860 --> 00:33:40.060]   or will there be a whole paradigm shift?
[00:33:40.060 --> 00:33:41.220]   What do you predict?
[00:33:41.220 --> 00:33:44.140]   - Well, in the singularity's near,
[00:33:44.140 --> 00:33:47.100]   I talk about the atomic limits.
[00:33:47.100 --> 00:33:50.100]   Based on molecular computing as we understand it,
[00:33:50.100 --> 00:33:53.540]   and it can actually go well past 2045
[00:33:53.540 --> 00:33:57.220]   and actually go to trillions of trillions of times
[00:33:57.220 --> 00:34:00.300]   greater computational capacity than we have today.
[00:34:01.300 --> 00:34:05.540]   So I don't see that stopping any time soon
[00:34:05.540 --> 00:34:09.500]   and will go way beyond what we can imagine.
[00:34:09.500 --> 00:34:13.780]   And it becomes an interesting discussion
[00:34:13.780 --> 00:34:18.780]   what the impact on human civilization will be.
[00:34:18.780 --> 00:34:22.580]   So to take a maybe slightly more mundane issue
[00:34:22.580 --> 00:34:25.060]   that comes up is, oh, it's gonna eliminate
[00:34:25.060 --> 00:34:26.940]   most jobs or all jobs.
[00:34:28.180 --> 00:34:30.140]   The point I make is it's not the first time
[00:34:30.140 --> 00:34:32.180]   in human history we've done that.
[00:34:32.180 --> 00:34:35.500]   How many jobs circa 1900 exist today?
[00:34:35.500 --> 00:34:39.700]   And that was the feeling of the Luddites,
[00:34:39.700 --> 00:34:42.820]   which was an actual society that formed in 1800
[00:34:42.820 --> 00:34:47.340]   after the automation of the textile industry in England.
[00:34:47.340 --> 00:34:48.780]   They looked at all these jobs going away
[00:34:48.780 --> 00:34:51.340]   and felt, oh, employment's gonna be
[00:34:51.340 --> 00:34:52.900]   just limited to an elite.
[00:34:52.900 --> 00:34:57.980]   Indeed, those jobs did go away, but new jobs were created.
[00:34:57.980 --> 00:35:00.820]   So if I were a prescient futurist in 1900,
[00:35:00.820 --> 00:35:04.500]   I would say, well, 38% of you work on farms
[00:35:04.500 --> 00:35:06.260]   and 25% work in factories.
[00:35:06.260 --> 00:35:09.060]   That's 2/3 of the working force.
[00:35:09.060 --> 00:35:16.260]   But I predict by 2015, 115 years from now,
[00:35:16.260 --> 00:35:19.500]   it's gonna be 2% on farms and 9% in factories
[00:35:19.500 --> 00:35:21.260]   and everybody would go, oh my God,
[00:35:21.260 --> 00:35:22.660]   we're gonna be out of work.
[00:35:22.660 --> 00:35:23.580]   And I said, well, don't worry.
[00:35:23.580 --> 00:35:25.700]   For all these jobs we eliminate through automation,
[00:35:25.700 --> 00:35:27.620]   we're gonna invent new jobs.
[00:35:27.620 --> 00:35:29.540]   And people say, oh, really, what new jobs?
[00:35:29.540 --> 00:35:30.940]   And I'd say, well, I don't know.
[00:35:30.940 --> 00:35:32.380]   We haven't invented them yet.
[00:35:32.380 --> 00:35:34.860]   That's the political problem.
[00:35:34.860 --> 00:35:38.060]   We can see jobs very clearly going away fairly soon,
[00:35:38.060 --> 00:35:39.860]   like driving a car or a truck.
[00:35:39.860 --> 00:35:42.860]   And the new jobs haven't been invented.
[00:35:42.860 --> 00:35:45.220]   I mean, just look at the last five or six years.
[00:35:45.220 --> 00:35:47.980]   A lot of the increase in employment
[00:35:47.980 --> 00:35:51.900]   has been through mobile app-related
[00:35:51.900 --> 00:35:53.380]   types of ways of making money
[00:35:53.380 --> 00:35:56.460]   that just weren't contemplated even six years ago.
[00:35:56.460 --> 00:35:59.460]   If I really prescient, I would say,
[00:35:59.460 --> 00:36:01.620]   well, you're gonna get jobs creating mobile apps
[00:36:01.620 --> 00:36:04.860]   and websites and doing data analytics
[00:36:04.860 --> 00:36:07.820]   and self-driving cars.
[00:36:07.820 --> 00:36:09.020]   Cars, what's a car?
[00:36:09.020 --> 00:36:12.700]   Nobody would have any idea what I'm talking about.
[00:36:12.700 --> 00:36:16.740]   Now, the new job, some people say,
[00:36:16.740 --> 00:36:19.020]   yeah, we created new jobs, but it's not as many.
[00:36:19.020 --> 00:36:23.220]   Actually, we've gone from 24 million jobs in 1900
[00:36:23.220 --> 00:36:25.420]   to 142 million jobs today,
[00:36:25.420 --> 00:36:28.900]   from 30% of the population to 45% of the population.
[00:36:28.900 --> 00:36:32.940]   The new jobs pay 11 times as much in constant dollars.
[00:36:32.940 --> 00:36:34.260]   And they're more interesting.
[00:36:34.260 --> 00:36:37.820]   I mean, as I talk to people starting out their career now,
[00:36:37.820 --> 00:36:40.500]   they really want a career that gives them some
[00:36:40.500 --> 00:36:43.620]   life definition and purpose and gratification.
[00:36:43.620 --> 00:36:46.060]   We're moving up Maslow's hierarchy.
[00:36:46.060 --> 00:36:49.340]   100 years ago, you were happy if you had a backbreaking job
[00:36:49.340 --> 00:36:51.260]   that put food on your family's table.
[00:36:52.220 --> 00:36:55.180]   And we couldn't do these new jobs
[00:36:55.180 --> 00:36:57.700]   without enhancing our intelligence.
[00:36:57.700 --> 00:36:59.900]   So we've been doing that, well,
[00:36:59.900 --> 00:37:03.020]   for most of the last 100 years through education.
[00:37:03.020 --> 00:37:07.020]   We've expanded K through 12 in constant dollars tenfold.
[00:37:07.020 --> 00:37:11.380]   We've gone from 38,000 college students in 1870
[00:37:11.380 --> 00:37:13.740]   to 15 million today.
[00:37:13.740 --> 00:37:17.860]   More recently, we have brain extenders.
[00:37:17.860 --> 00:37:20.300]   They're not yet connected directly in our brain,
[00:37:20.300 --> 00:37:21.900]   but they're very close at hand.
[00:37:21.900 --> 00:37:23.980]   When I was here at MIT, I had to take my bicycle
[00:37:23.980 --> 00:37:26.340]   across campus to get to the computer
[00:37:26.340 --> 00:37:28.460]   and show an ID to get in the building.
[00:37:28.460 --> 00:37:33.460]   Now we carry them in our pockets and on our belts.
[00:37:33.460 --> 00:37:36.780]   They're gonna go inside our bodies and brains.
[00:37:36.780 --> 00:37:41.380]   I think that's not a really important distinction.
[00:37:41.380 --> 00:37:43.100]   So we're basically gonna be continuing
[00:37:43.100 --> 00:37:46.780]   to enhance our capability through merging with AI.
[00:37:46.780 --> 00:37:49.740]   And that's the, I think, ultimate answer
[00:37:49.740 --> 00:37:54.500]   to the kind of dystopian view we see in future movies
[00:37:54.500 --> 00:37:57.180]   where it's the AI versus a brave band of humans
[00:37:57.180 --> 00:37:58.740]   for control of humanity.
[00:37:58.740 --> 00:38:01.340]   We don't have one or two AIs in the world today.
[00:38:01.340 --> 00:38:03.140]   We have several billion,
[00:38:03.140 --> 00:38:05.900]   three billion smartphones at last count.
[00:38:05.900 --> 00:38:08.460]   It'll be six billion in just a couple of years
[00:38:08.460 --> 00:38:10.460]   according to the projections.
[00:38:10.460 --> 00:38:13.540]   So we're already deeply integrated with this.
[00:38:13.540 --> 00:38:16.100]   And I think that's gonna continue.
[00:38:16.100 --> 00:38:18.100]   And it's gonna continue to do things
[00:38:18.100 --> 00:38:19.860]   which you can't even imagine today.
[00:38:19.860 --> 00:38:23.260]   Just as we are doing today things we couldn't imagine
[00:38:23.260 --> 00:38:24.580]   even 20 years ago.
[00:38:24.580 --> 00:38:29.700]   - You showed many graphs that go through exponential growth
[00:38:29.700 --> 00:38:31.140]   but I haven't seen one that isn't.
[00:38:31.140 --> 00:38:33.660]   So I would be very interested in hearing--
[00:38:33.660 --> 00:38:35.060]   - You haven't seen one that, what?
[00:38:35.060 --> 00:38:36.820]   - That is not exponential.
[00:38:36.820 --> 00:38:40.420]   So tell me about regions that you've investigated
[00:38:40.420 --> 00:38:43.100]   that have not seen exponential growth
[00:38:43.100 --> 00:38:44.940]   and why do you think that's the case?
[00:38:45.980 --> 00:38:49.180]   - Well, price performance and capacity
[00:38:49.180 --> 00:38:53.580]   of information technology invariably follows exponential.
[00:38:53.580 --> 00:38:58.500]   When it impacts human society it can be linear.
[00:38:58.500 --> 00:39:00.860]   So for example, the growth of democracy
[00:39:00.860 --> 00:39:06.060]   has been linear but still pretty steady.
[00:39:06.060 --> 00:39:07.660]   You could count the number of democracies
[00:39:07.660 --> 00:39:10.380]   on the fingers of one hand a century ago.
[00:39:10.380 --> 00:39:12.900]   Two centuries ago you could count the number of democracies
[00:39:12.900 --> 00:39:14.980]   in the world on the fingers of one finger.
[00:39:15.980 --> 00:39:17.380]   Now there are dozens of them
[00:39:17.380 --> 00:39:20.060]   and it's become kind of a consensus
[00:39:20.060 --> 00:39:22.460]   that that's how we should be governed.
[00:39:22.460 --> 00:39:29.060]   So the, and I attribute all this to the growth
[00:39:29.060 --> 00:39:32.500]   in information technology, communication in particular
[00:39:32.500 --> 00:39:37.500]   for progression of social and cultural institutions.
[00:39:37.500 --> 00:39:42.580]   But information technology,
[00:39:42.580 --> 00:39:47.460]   because it ultimately depends on a vanishingly small
[00:39:47.460 --> 00:39:51.580]   energy and material requirement,
[00:39:51.580 --> 00:39:54.620]   grows exponentially and will for a long time.
[00:39:54.620 --> 00:39:57.180]   There was recently a criticism that,
[00:39:57.180 --> 00:39:58.820]   well, chess scores have,
[00:39:58.820 --> 00:40:03.820]   it's actually a remarkably straight linear progression.
[00:40:03.820 --> 00:40:08.260]   So humans, I think it's like 2,800
[00:40:08.260 --> 00:40:11.300]   and it just soared past that in 1997 with Deep Blue
[00:40:11.300 --> 00:40:12.580]   and it's kept going.
[00:40:12.580 --> 00:40:15.620]   And remarkably straight and saying,
[00:40:15.620 --> 00:40:17.300]   well, this is linear, not exponential.
[00:40:17.300 --> 00:40:20.940]   But the chess score is a logarithmic measurement.
[00:40:20.940 --> 00:40:26.620]   So it really is exponential progression.
[00:40:26.620 --> 00:40:31.060]   - So philosophers like to think a lot about
[00:40:31.060 --> 00:40:33.420]   the meaning of things, especially in the 20th century.
[00:40:33.420 --> 00:40:37.580]   So for instance, Martin Heidegger gave a couple of speeches
[00:40:37.580 --> 00:40:40.340]   and lectures on the relationship of human society
[00:40:40.340 --> 00:40:43.980]   to technology and he particularly distinguished
[00:40:43.980 --> 00:40:47.540]   between the mode of thinking which is calculating thinking
[00:40:47.540 --> 00:40:49.900]   and a mode of thinking which is reflective thinking
[00:40:49.900 --> 00:40:52.180]   or meditative thinking.
[00:40:52.180 --> 00:40:54.020]   And he posed this question,
[00:40:54.020 --> 00:40:56.340]   what is the meaning and purpose
[00:40:56.340 --> 00:40:58.140]   of technological development?
[00:40:58.140 --> 00:40:59.820]   And he couldn't find an answer.
[00:40:59.820 --> 00:41:03.740]   He recommended to remain open to what he called,
[00:41:03.740 --> 00:41:07.100]   he called this an openness to the mystery.
[00:41:07.100 --> 00:41:09.540]   I wonder whether you have any thoughts on this.
[00:41:09.540 --> 00:41:12.980]   Is there a meaning of purpose to technological development
[00:41:12.980 --> 00:41:17.260]   and is there a way for us humans to access that meaning?
[00:41:17.260 --> 00:41:26.460]   - Well, we started using technology to shore up weaknesses
[00:41:26.460 --> 00:41:28.860]   in our own capabilities.
[00:41:28.860 --> 00:41:33.260]   So physically, I mean, who here could build this building?
[00:41:33.260 --> 00:41:36.620]   So we've leveraged the power of our muscles with machines.
[00:41:38.740 --> 00:41:41.100]   And we're in fact very bad at doing things
[00:41:41.100 --> 00:41:44.140]   that the simplest computers can do,
[00:41:44.140 --> 00:41:48.060]   like factor numbers or even just multiply
[00:41:48.060 --> 00:41:52.420]   to eight digit numbers.
[00:41:52.420 --> 00:41:54.780]   Computers can do that trivially, we can't do it.
[00:41:54.780 --> 00:41:57.940]   So we originally started using computers
[00:41:57.940 --> 00:42:00.540]   to make up for that weakness.
[00:42:00.540 --> 00:42:05.180]   I think the essence of what I've been writing about
[00:42:05.180 --> 00:42:09.340]   is to master the unique strengths of humanity,
[00:42:09.340 --> 00:42:14.220]   creating loving expressions in poetry and music
[00:42:14.220 --> 00:42:17.100]   and the kinds of things we associate
[00:42:17.100 --> 00:42:20.740]   with the better qualities of humanity with machines.
[00:42:20.740 --> 00:42:23.140]   That's the true promise of AI.
[00:42:23.140 --> 00:42:25.460]   We're not there yet,
[00:42:25.460 --> 00:42:28.220]   but we're making pretty stunning progress.
[00:42:28.220 --> 00:42:31.140]   Just in the last year, there's so many milestones
[00:42:31.140 --> 00:42:34.020]   that are really significant, including in language.
[00:42:34.900 --> 00:42:39.900]   But I think of technology as an expression of humanity.
[00:42:39.900 --> 00:42:43.500]   It's part of who we are.
[00:42:43.500 --> 00:42:45.620]   And the human species is already
[00:42:45.620 --> 00:42:50.180]   a biological technological civilization.
[00:42:50.180 --> 00:42:51.620]   And it's part of who we are.
[00:42:51.620 --> 00:42:54.180]   And AI is part of humans.
[00:42:54.180 --> 00:42:59.700]   So AI is human and it's part
[00:42:59.700 --> 00:43:02.660]   of the technological expression of humanity.
[00:43:02.660 --> 00:43:06.940]   And we use technology to extend our reach.
[00:43:06.940 --> 00:43:07.980]   I couldn't reach that fruit
[00:43:07.980 --> 00:43:09.540]   at that higher branch a thousand years ago.
[00:43:09.540 --> 00:43:12.420]   So we invented a tool to extend our physical reach.
[00:43:12.420 --> 00:43:14.820]   And we now extend our mental reach.
[00:43:14.820 --> 00:43:19.380]   We can access all of human knowledge with a few keystrokes.
[00:43:19.380 --> 00:43:25.540]   And we're gonna make ourselves literally smarter
[00:43:25.540 --> 00:43:27.300]   by merging with AI.
[00:43:31.460 --> 00:43:36.460]   - Hi, first of all, honored to hear you speak here.
[00:43:36.460 --> 00:43:38.700]   So I first read The Singularity
[00:43:38.700 --> 00:43:42.340]   as near nine years ago or so.
[00:43:42.340 --> 00:43:45.380]   And it changed the way I thought entirely.
[00:43:45.380 --> 00:43:48.140]   But something I think it caused me
[00:43:48.140 --> 00:43:53.140]   to over steeply discount was tail risk in geopolitics,
[00:43:53.140 --> 00:44:00.980]   in systems that span the entire globe.
[00:44:01.100 --> 00:44:06.100]   And my concern is that there is obviously
[00:44:06.100 --> 00:44:11.020]   the possibility of tail risk,
[00:44:11.020 --> 00:44:14.900]   existential level events,
[00:44:14.900 --> 00:44:19.900]   swamping all of these trends that are otherwise war proof,
[00:44:19.900 --> 00:44:22.500]   climate proof, you name it.
[00:44:22.500 --> 00:44:26.860]   So my question for you is what steps do you think
[00:44:26.860 --> 00:44:31.460]   we can take in designing engineered systems,
[00:44:31.460 --> 00:44:35.380]   in designing social and economic institutions
[00:44:35.380 --> 00:44:39.940]   to kind of minimize our exposure to these tail risks
[00:44:39.940 --> 00:44:44.940]   and survive to make it to a beautiful mind filled future?
[00:44:44.940 --> 00:44:53.420]   - Yeah, well, the world was first introduced
[00:44:54.140 --> 00:44:57.900]   to a human made existential risk
[00:44:57.900 --> 00:45:00.100]   when I was in elementary school,
[00:45:00.100 --> 00:45:02.300]   we would have these civil defense drills,
[00:45:02.300 --> 00:45:04.620]   get under our desk and put our hands behind our head
[00:45:04.620 --> 00:45:07.700]   to protect us from a thermonuclear war.
[00:45:07.700 --> 00:45:11.700]   And it worked, we made it through.
[00:45:11.700 --> 00:45:15.420]   But that was really the first introduction
[00:45:15.420 --> 00:45:17.140]   to an existential risk.
[00:45:17.140 --> 00:45:21.100]   And those weapons are still there, by the way,
[00:45:21.100 --> 00:45:23.980]   and they're still on a hair trigger.
[00:45:23.980 --> 00:45:26.940]   And they don't get that much attention.
[00:45:26.940 --> 00:45:28.660]   There's been a lot of discussion,
[00:45:28.660 --> 00:45:32.140]   much of which I've been in the forefront of initiating
[00:45:32.140 --> 00:45:35.420]   on the existential risks of what's sometimes referred to
[00:45:35.420 --> 00:45:39.180]   as GNR, G for genetics, which is biotechnology,
[00:45:39.180 --> 00:45:43.620]   N for nanotechnology, and gray goo, robotics, which is AI.
[00:45:43.620 --> 00:45:48.540]   And I've been accused of being an optimist.
[00:45:48.540 --> 00:45:49.940]   And I think you have to be an optimist
[00:45:49.940 --> 00:45:51.180]   to be an entrepreneur.
[00:45:51.180 --> 00:45:53.500]   If you knew all the problems you were gonna encounter,
[00:45:53.500 --> 00:45:55.460]   you'd never start any project.
[00:45:55.460 --> 00:46:00.460]   But I've written a lot about the downsides.
[00:46:00.460 --> 00:46:01.980]   I remain optimistic.
[00:46:01.980 --> 00:46:06.740]   There are specific paradigms, they're not foolproof,
[00:46:06.740 --> 00:46:09.740]   that we can follow to keep these technologies safe.
[00:46:09.740 --> 00:46:13.940]   So for example, over 40 years ago,
[00:46:13.940 --> 00:46:18.140]   some visionaries recognized the revolutionary potential,
[00:46:18.140 --> 00:46:22.500]   both for promise and peril, of biotechnology.
[00:46:22.500 --> 00:46:25.900]   Neither the promise nor peril was feasible 40 years ago.
[00:46:25.900 --> 00:46:28.340]   But they had a conference
[00:46:28.340 --> 00:46:30.900]   at the Asilomar Conference Center in California
[00:46:30.900 --> 00:46:36.700]   to develop both professional ethics and strategies
[00:46:36.700 --> 00:46:38.820]   to keep biotechnology safe.
[00:46:38.820 --> 00:46:41.300]   And they've been known as the Asilomar Guidelines.
[00:46:41.300 --> 00:46:46.100]   They've been refined through successive Asilomar conferences.
[00:46:46.100 --> 00:46:48.380]   Much of that's baked into law.
[00:46:48.380 --> 00:46:51.180]   And in my opinion, it's worked quite well.
[00:46:51.180 --> 00:46:54.140]   We're now, as I mentioned, getting profound benefit.
[00:46:54.140 --> 00:46:58.300]   It's a trickle today, it'll be a flood over the next decade.
[00:46:58.300 --> 00:47:00.740]   And the number of people who have been harmed,
[00:47:00.740 --> 00:47:03.420]   either through intentional or accidental abuse
[00:47:03.420 --> 00:47:06.100]   of biotechnology, so far is zero.
[00:47:06.100 --> 00:47:07.180]   Actually, I take that back.
[00:47:07.180 --> 00:47:11.060]   There was one boy who died in gene therapy trials
[00:47:11.060 --> 00:47:12.420]   about 12 years ago.
[00:47:12.420 --> 00:47:13.860]   And there was congressional hearings
[00:47:13.860 --> 00:47:18.820]   and they canceled all research for gene therapy
[00:47:18.820 --> 00:47:20.700]   for a number of years.
[00:47:20.700 --> 00:47:22.980]   You could do an interesting master's thesis
[00:47:22.980 --> 00:47:25.580]   and demonstrate that 300,000 people died
[00:47:25.580 --> 00:47:28.420]   as a result of that delay, but you can't name them.
[00:47:28.420 --> 00:47:30.740]   They can't go on CNN, so we don't know who they are.
[00:47:30.740 --> 00:47:34.340]   But that has to do with the balancing of risk.
[00:47:34.340 --> 00:47:38.260]   But in large measure, virtually no one has been hurt
[00:47:38.260 --> 00:47:39.500]   by biotechnology.
[00:47:39.500 --> 00:47:41.780]   Now, that doesn't mean you can cross it off our list.
[00:47:41.780 --> 00:47:43.940]   Okay, we took care of that one
[00:47:43.940 --> 00:47:46.660]   because the technology keeps getting more sophisticated.
[00:47:46.660 --> 00:47:49.300]   CRISPR's a great opportunity.
[00:47:49.300 --> 00:47:53.260]   There's hundreds of trials of CRISPR technologies
[00:47:53.260 --> 00:47:56.500]   to overcome disease, but it could be abused.
[00:47:56.500 --> 00:47:58.860]   You can easily describe scenarios,
[00:47:58.860 --> 00:48:00.580]   so we have to keep reinventing it.
[00:48:00.580 --> 00:48:06.020]   January, we had our first Asilomar Conference on AI ethics.
[00:48:06.020 --> 00:48:08.100]   And so I think this is a good paradigm.
[00:48:08.100 --> 00:48:09.540]   It's not foolproof.
[00:48:09.540 --> 00:48:16.180]   I think the best way we can assure a democratic future
[00:48:18.660 --> 00:48:20.260]   that includes our ideas of liberty
[00:48:20.260 --> 00:48:22.580]   is to practice that in the world today
[00:48:22.580 --> 00:48:24.820]   'cause the future world of the singularity,
[00:48:24.820 --> 00:48:26.940]   which is a merger of biological
[00:48:26.940 --> 00:48:28.660]   and non-biological intelligence,
[00:48:28.660 --> 00:48:31.100]   is not gonna come from Mars.
[00:48:31.100 --> 00:48:34.460]   I mean, it's gonna emerge from our society today.
[00:48:34.460 --> 00:48:36.340]   So if we practice these ideals today,
[00:48:36.340 --> 00:48:39.580]   it's gonna have a higher chance of us practicing them
[00:48:39.580 --> 00:48:41.980]   as we get more enhanced with technology.
[00:48:41.980 --> 00:48:45.140]   That doesn't sound like a foolproof solution.
[00:48:45.140 --> 00:48:48.460]   It isn't, but I think that's the best approach.
[00:48:48.460 --> 00:48:50.740]   In terms of technological solutions,
[00:48:50.740 --> 00:48:52.500]   I mean, AI is the most daunting.
[00:48:52.500 --> 00:48:56.100]   You can imagine there are technical solutions
[00:48:56.100 --> 00:49:00.260]   to biotechnology and nanotechnology.
[00:49:00.260 --> 00:49:02.020]   There's really no subroutine you can put
[00:49:02.020 --> 00:49:07.020]   in your AI software that will assure that it remains safe.
[00:49:07.020 --> 00:49:10.940]   Intelligence is inherently not controllable.
[00:49:10.940 --> 00:49:13.060]   If there's some AI that's much smarter than you
[00:49:13.060 --> 00:49:15.180]   that's out for your destruction,
[00:49:15.180 --> 00:49:16.380]   the best way to deal with that
[00:49:16.380 --> 00:49:19.700]   is not to get in that situation in the first place.
[00:49:19.700 --> 00:49:22.380]   If you are in that situation,
[00:49:22.380 --> 00:49:24.700]   find some AI that will be on your side.
[00:49:24.700 --> 00:49:29.100]   But basically, it's going to,
[00:49:29.100 --> 00:49:33.220]   I believe we have been headed through technology
[00:49:33.220 --> 00:49:35.660]   to a better reality.
[00:49:35.660 --> 00:49:37.580]   I go around the world
[00:49:37.580 --> 00:49:39.940]   and people really think things are getting worse.
[00:49:39.940 --> 00:49:42.380]   And I think that's 'cause our information
[00:49:42.380 --> 00:49:43.580]   about what's wrong with the world
[00:49:43.580 --> 00:49:46.060]   is getting exponentially better.
[00:49:46.060 --> 00:49:47.620]   They say, "Oh, this is the most peaceful time
[00:49:47.620 --> 00:49:48.460]   "in human history."
[00:49:48.460 --> 00:49:49.980]   And people say, "What are you, crazy?
[00:49:49.980 --> 00:49:52.860]   "Didn't you hear about the event yesterday and last week?"
[00:49:52.860 --> 00:49:56.540]   Well, 100 years ago, there could be a battle
[00:49:56.540 --> 00:49:57.780]   that wiped out the next village
[00:49:57.780 --> 00:50:00.100]   and you wouldn't even hear about it for months.
[00:50:00.100 --> 00:50:04.980]   I have all these graphs on education and literacy
[00:50:04.980 --> 00:50:09.340]   has gone from like 10% to 90% over a century
[00:50:09.340 --> 00:50:14.340]   and health, wealth,
[00:50:15.340 --> 00:50:20.340]   poverty has declined 95% in Asia over the last 25 years,
[00:50:20.340 --> 00:50:22.660]   it's documented by the World Bank.
[00:50:22.660 --> 00:50:24.820]   All these trends are very smoothly getting better
[00:50:24.820 --> 00:50:27.620]   and everybody thinks things are getting worse.
[00:50:27.620 --> 00:50:30.380]   But you're right, like on violence,
[00:50:30.380 --> 00:50:34.100]   that curve could be quite disrupted
[00:50:34.100 --> 00:50:36.660]   if there's an existential event.
[00:50:36.660 --> 00:50:39.140]   As I say, I'm optimistic,
[00:50:39.140 --> 00:50:43.300]   but I think that is something that we need to deal with
[00:50:43.300 --> 00:50:45.020]   and a lot of it is not technological,
[00:50:45.020 --> 00:50:48.540]   it's dealing with our social, cultural institutions.
[00:50:48.540 --> 00:50:54.620]   - So you mentioned also exponential growth of software
[00:50:54.620 --> 00:50:56.540]   and ideas, I guess, related to software.
[00:50:56.540 --> 00:50:59.420]   So one of the reasons for which you said
[00:50:59.420 --> 00:51:02.420]   that information technology costs is exponential
[00:51:02.420 --> 00:51:05.900]   is because of fundamental properties of matter and energy.
[00:51:05.900 --> 00:51:07.420]   But in the case of ideas,
[00:51:07.420 --> 00:51:10.020]   why would it have to be exponential?
[00:51:10.020 --> 00:51:12.900]   - Well, a lot of ideas produce exponential,
[00:51:12.900 --> 00:51:14.700]   exponential gains.
[00:51:14.700 --> 00:51:18.820]   They don't increase performance linearly.
[00:51:18.820 --> 00:51:22.020]   There was actually a study during the Obama administration
[00:51:22.020 --> 00:51:27.020]   by the Scientific Advisory Board on assessing this question,
[00:51:27.020 --> 00:51:31.540]   how much gains on 23 classical engineering problems
[00:51:31.540 --> 00:51:36.740]   were gained through hardware improvements
[00:51:36.740 --> 00:51:38.940]   over the last decade and software improvements.
[00:51:38.940 --> 00:51:41.460]   And there's about a thousand to one improvements,
[00:51:41.460 --> 00:51:44.100]   it's about doubling every year from hardware.
[00:51:44.100 --> 00:51:47.580]   There was an average of something like 26,000 to one
[00:51:47.580 --> 00:51:50.500]   through software improvements, algorithmic improvements.
[00:51:50.500 --> 00:51:56.060]   So we do see both, and apparently,
[00:51:56.060 --> 00:51:57.380]   if you come up with an advance,
[00:51:57.380 --> 00:52:01.420]   it doubles the performance or multiplies it by 10.
[00:52:01.420 --> 00:52:05.060]   We see basically exponential growth from each innovation.
[00:52:05.060 --> 00:52:11.060]   So, and we certainly see that in deep learning,
[00:52:11.060 --> 00:52:13.500]   the architectures are getting better
[00:52:13.500 --> 00:52:16.220]   while we also have more data and more computation
[00:52:16.220 --> 00:52:19.660]   and more memory to throw at these algorithms.
[00:52:19.660 --> 00:52:20.500]   - Thank you very much.
[00:52:20.500 --> 00:52:21.900]   Let's give a very big hand.
[00:52:21.900 --> 00:52:25.060]   (audience applauding)
[00:52:25.060 --> 00:52:28.100]   Thank you for being here.

