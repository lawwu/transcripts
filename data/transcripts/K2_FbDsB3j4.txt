
[00:00:00.000 --> 00:00:05.540]   I've never seen computer systems architecture and systems optimization be as interesting
[00:00:05.540 --> 00:00:07.400]   as it is right now, right?
[00:00:07.400 --> 00:00:11.720]   Because it was a period of researching this, it was just about making microprocessors faster,
[00:00:11.720 --> 00:00:13.560]   making a little bit better compilers.
[00:00:13.560 --> 00:00:17.980]   But now that we have to specialize and there's this really exciting application space with
[00:00:17.980 --> 00:00:22.360]   machine learning that offers so many opportunities for optimizations, and you have things like
[00:00:22.360 --> 00:00:25.360]   FPGAs, and it's getting easier to design chips.
[00:00:25.360 --> 00:00:30.320]   It creates all sorts of opportunities for academic research and also for industrial innovation.
[00:00:30.320 --> 00:00:34.680]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:34.680 --> 00:00:36.800]   and I'm your host, Lukas Biewald.
[00:00:36.800 --> 00:00:44.360]   Luis Cezzi is co-founder and CEO of OctoML, founder of the Apache TVM project, and a professor
[00:00:44.360 --> 00:00:47.480]   of computer science at the University of Washington.
[00:00:47.480 --> 00:00:54.040]   He is an expert in making machine learning run efficiently on a variety of hardware systems,
[00:00:54.040 --> 00:00:57.800]   something that I'm super fascinated by and don't know a lot about, and so I could not
[00:00:57.800 --> 00:00:59.560]   be more excited to talk to him today.
[00:00:59.560 --> 00:01:01.960]   Why don't we just jump right in?
[00:01:01.960 --> 00:01:08.120]   I guess you're the CEO of OctoML, and that's based on the Apache TVM project that I think
[00:01:08.120 --> 00:01:09.120]   you also authored.
[00:01:09.120 --> 00:01:13.880]   Can you just, for people who don't know, give a description of what that is?
[00:01:13.880 --> 00:01:14.880]   Yeah, sure.
[00:01:14.880 --> 00:01:15.880]   And maybe a quick intro.
[00:01:15.880 --> 00:01:16.880]   Yeah.
[00:01:16.880 --> 00:01:21.520]   I wear two hats, so I'm a CEO of OctoML and also a professor of computer science engineering
[00:01:21.520 --> 00:01:24.720]   at the University of Washington, and I have many machine learning friends.
[00:01:24.720 --> 00:01:28.160]   And on the area, so I mean machine learning systems.
[00:01:28.160 --> 00:01:29.160]   So what does that mean?
[00:01:29.160 --> 00:01:33.320]   It means building computer systems that make machine learning applications run fast and
[00:01:33.320 --> 00:01:37.720]   efficient and do what they're supposed to do in the easiest way possible.
[00:01:37.720 --> 00:01:41.360]   And often we use machine learning in making machine learning systems better, which is
[00:01:41.360 --> 00:01:43.960]   something that we should touch on at some point.
[00:01:43.960 --> 00:01:45.200]   It's an interesting topic.
[00:01:45.200 --> 00:01:49.120]   So Apache TVM stands for TensorFlow Virtual Machine.
[00:01:49.120 --> 00:01:53.520]   It started in our research group at the University of Washington about five years or so ago.
[00:01:53.520 --> 00:01:55.280]   And the context there was the following.
[00:01:55.280 --> 00:01:59.040]   Five years ago, which in machine learning time is just like eons ago, right?
[00:01:59.040 --> 00:02:04.080]   So there was already a growing set of machine learning models that people care about, and
[00:02:04.080 --> 00:02:07.160]   a faster and faster growing set of those.
[00:02:07.160 --> 00:02:11.240]   The fragmentation in the software ecosystem was just starting, the TensorFlow and PyTorch
[00:02:11.240 --> 00:02:13.560]   and the MaxNet, Keras and so on.
[00:02:13.560 --> 00:02:17.640]   And then harder targets, at that time were mainly CPUs in the beginning of GPUs and a
[00:02:17.640 --> 00:02:19.680]   little bit of accelerators back then.
[00:02:19.680 --> 00:02:23.600]   But our observation then was that, well, we have a growing set of models, a growing set
[00:02:23.600 --> 00:02:28.680]   of harder targets, and then this fragmentation that is either you have a software stack that's
[00:02:28.680 --> 00:02:32.320]   specific to the hardware that you want to deploy your model to, or they're specific
[00:02:32.320 --> 00:02:36.240]   to use cases like computer vision or NLP and so on.
[00:02:36.240 --> 00:02:41.080]   We wanted to create a clean abstraction that would free data scientists or machine learning
[00:02:41.080 --> 00:02:44.320]   engineers from having to worry about how to get their models deployed.
[00:02:44.320 --> 00:02:49.200]   So we wanted to have them focus on the statistical properties of the model, and then target a
[00:02:49.200 --> 00:02:54.000]   clean single pane of glass, clean abstraction across all of the systems and hardware such
[00:02:54.000 --> 00:03:01.200]   that it can deploy your model and make the most of the hardware targets as much as possible
[00:03:01.200 --> 00:03:03.040]   from the hardware target you're deploying to.
[00:03:03.040 --> 00:03:07.320]   As you all know here, and since a lot of machine learning practitioners that listen to this,
[00:03:07.320 --> 00:03:10.520]   machine learning code is extremely sensitive to performance, right?
[00:03:10.520 --> 00:03:15.640]   So it uses a lot of memory, use a lot of memory bandwidth, which means that use a lot of the
[00:03:15.640 --> 00:03:18.960]   ability of moving the data from memory to your compute engine and back, and also use
[00:03:18.960 --> 00:03:21.280]   a lot of raw compute power, right?
[00:03:21.280 --> 00:03:27.700]   So that's why machine learning, hardware is good for machine learning today more and more
[00:03:27.700 --> 00:03:32.920]   look like supercomputers of not too long ago, like vector processing and matrix, tensor
[00:03:32.920 --> 00:03:34.320]   cores and all of these things, right?
[00:03:34.320 --> 00:03:36.560]   So a lot of linear algebra.
[00:03:36.560 --> 00:03:39.440]   So making the most out of that is really, really hard.
[00:03:39.440 --> 00:03:41.000]   I mean, code optimization is already hard.
[00:03:41.000 --> 00:03:44.560]   Now, if you're optimizing code for something that's performance sensitive as machine learning,
[00:03:44.560 --> 00:03:46.640]   you're talking about a really hard job, right?
[00:03:46.640 --> 00:03:48.280]   So anyways, I'm getting there.
[00:03:48.280 --> 00:03:50.200]   I know it's a long story, but I hopefully it'll be worth it.
[00:03:50.200 --> 00:03:54.620]   So what TVM did, what we started as a research question was that, can we automate the process
[00:03:54.620 --> 00:03:59.280]   of tuning your machine learning model and the actual code to the hardware targets that
[00:03:59.280 --> 00:04:00.680]   you want to deploy it to?
[00:04:00.680 --> 00:04:01.680]   Okay?
[00:04:01.680 --> 00:04:06.600]   Instead of having to rely on hand-tuned libraries or relying on a lot of artists and coding to
[00:04:06.600 --> 00:04:11.360]   get your model to run fast enough, we wanted to use machine learning to automate that process.
[00:04:11.360 --> 00:04:16.560]   And the way that works is TVM runs a bunch of little experiments to build really a profile,
[00:04:16.560 --> 00:04:21.120]   a personality of how your hardware behaves and use that to guide a very large optimization
[00:04:21.120 --> 00:04:23.480]   space to tune your model and your code.
[00:04:23.480 --> 00:04:27.840]   So the end result from a user point of view is that you pick a model as input in TVM,
[00:04:27.840 --> 00:04:32.600]   you choose a hardware target, and then what TVM does is it finds just the right way of
[00:04:32.600 --> 00:04:37.520]   tuning your model and compiling to a very efficient binary on your hardware targets.
[00:04:37.520 --> 00:04:39.960]   And I guess when I think of like-
[00:04:39.960 --> 00:04:41.120]   Does that answer your question what TVM is?
[00:04:41.120 --> 00:04:42.640]   I know it's long, but I hope it's useful.
[00:04:42.640 --> 00:04:43.640]   Yeah.
[00:04:43.640 --> 00:04:44.640]   Yeah, no, it's great.
[00:04:44.640 --> 00:04:46.600]   I want to ask some more clarifying questions, I guess.
[00:04:46.600 --> 00:04:49.480]   And I'm not a hardware expert at all.
[00:04:49.480 --> 00:04:54.040]   And I guess what I've observed trying to make ML models run on various hardware types is
[00:04:54.040 --> 00:05:00.160]   that it seems like it's harder and harder to abstract away the hardware.
[00:05:00.160 --> 00:05:05.200]   It seems like people are really building models with specific hardware in mind sometimes,
[00:05:05.200 --> 00:05:07.400]   specific memory sizes and things like that.
[00:05:07.400 --> 00:05:08.960]   I guess my first question is-
[00:05:08.960 --> 00:05:09.960]   And that's what we want to change.
[00:05:09.960 --> 00:05:10.960]   That's what we want to change.
[00:05:10.960 --> 00:05:14.360]   We want to remove that worry from the model builders.
[00:05:14.360 --> 00:05:17.160]   We want them to focus on building the best statistical properties possible.
[00:05:17.160 --> 00:05:21.360]   And then everything else should be left for engines like TVM and the optimizer.
[00:05:21.360 --> 00:05:22.840]   I can tell you more about later.
[00:05:22.840 --> 00:05:23.840]   Yeah.
[00:05:23.840 --> 00:05:26.240]   And so this TVM though, is it actually like a virtual machine?
[00:05:26.240 --> 00:05:32.680]   Is it doing kind of real time compiling to the hardware as the model runs?
[00:05:32.680 --> 00:05:33.680]   That's part of the work.
[00:05:33.680 --> 00:05:34.680]   Yeah.
[00:05:34.680 --> 00:05:38.360]   So TVM by and large, we call it JITing, JITing time compilation.
[00:05:38.360 --> 00:05:42.280]   So the reason the JITing time compilation is important is because, well, you learn more
[00:05:42.280 --> 00:05:46.160]   about the model as you run it, as you evaluate it.
[00:05:46.160 --> 00:05:50.960]   And then second, you can do measurements of performance and make decisions about how we're
[00:05:50.960 --> 00:05:54.000]   going to tune the rest of your population.
[00:05:54.000 --> 00:05:55.000]   I see.
[00:05:55.000 --> 00:05:56.000]   Yeah.
[00:05:56.000 --> 00:05:58.000]   It is a virtual machine in the sense that it offers a clean abstraction.
[00:05:58.000 --> 00:06:01.640]   It's not a virtual machine in the VMware sense.
[00:06:01.640 --> 00:06:05.640]   It's more like a virtual machine in the Java virtual machine sense, which you can get.
[00:06:05.640 --> 00:06:07.560]   It could be a whole different conversation.
[00:06:07.560 --> 00:06:12.240]   It's even closer to my world as a computer systems architect is thinking about those
[00:06:12.240 --> 00:06:13.560]   kinds of abstractions.
[00:06:13.560 --> 00:06:17.840]   But TVM is a virtual machine in the sense that it exposes a well-defined interface for
[00:06:17.840 --> 00:06:22.680]   you to express what your model does and gets that, what we call, lower down to the hardware
[00:06:22.680 --> 00:06:23.680]   target.
[00:06:23.680 --> 00:06:24.680]   Yeah.
[00:06:24.680 --> 00:06:31.200]   And then is this typically for deployment or could it also apply for training time?
[00:06:31.200 --> 00:06:32.200]   Great question.
[00:06:32.200 --> 00:06:35.840]   So TVM so far, by and large, it's used as being for inference.
[00:06:35.840 --> 00:06:37.800]   So you have a model that's being trained.
[00:06:37.800 --> 00:06:40.960]   You often have done quantization too by then and so on.
[00:06:40.960 --> 00:06:45.600]   And then you run it through TVM because we see that as a strength is that you apply all
[00:06:45.600 --> 00:06:49.920]   the optimizations that could change statistical proper of your model and you validate your
[00:06:49.920 --> 00:06:50.920]   model that way.
[00:06:51.160 --> 00:06:56.000]   And then whatever we do from there on should be seen as a process that preserves exactly
[00:06:56.000 --> 00:06:57.000]   what your model does.
[00:06:57.000 --> 00:06:59.720]   We don't want to change anything because we see that as complimentary to all of the other
[00:06:59.720 --> 00:07:03.880]   optimization that model builders would apply before then.
[00:07:03.880 --> 00:07:06.680]   So then what TVM does is really like a compiler, right?
[00:07:06.680 --> 00:07:11.160]   So it's a compiler plus code generator, plus a runtime system.
[00:07:11.160 --> 00:07:16.060]   And we specialize everything to your model and the hardware target.
[00:07:16.060 --> 00:07:20.520]   We really produce a custom package that is ready to be deployed that has custom everything.
[00:07:20.520 --> 00:07:25.000]   It's a custom line of operators for your model, has a custom runtime system for your model,
[00:07:25.000 --> 00:07:28.800]   and then wraps it up into a package that you can just go and deploy.
[00:07:28.800 --> 00:07:29.800]   Got it.
[00:07:29.800 --> 00:07:35.840]   And are you picturing typically, is this kind of like edge and low power compute environments
[00:07:35.840 --> 00:07:38.000]   or is this more for servers?
[00:07:38.000 --> 00:07:39.360]   Yeah.
[00:07:39.360 --> 00:07:40.360]   Great question.
[00:07:40.360 --> 00:07:44.360]   So remember that I was telling you about automating the process and using machine learning to discover
[00:07:44.360 --> 00:07:50.200]   what the hardware can do and can do well and use that to guide the optimization.
[00:07:50.200 --> 00:07:54.520]   That frees us from having to make that choice because essentially, as long as there's no
[00:07:54.520 --> 00:07:59.440]   magic involved, obviously, if you have a giant GPT-3 like model you want to run on a one
[00:07:59.440 --> 00:08:02.120]   milliwatt power microcontroller, this is just simply not going to work, right?
[00:08:02.120 --> 00:08:03.120]   So that's the obvious.
[00:08:03.120 --> 00:08:08.320]   But in terms of the actual basic flow of having what we call cost models for the hardware
[00:08:08.320 --> 00:08:12.480]   target and use those predictive models to guide how to optimize the model for that specific
[00:08:12.480 --> 00:08:17.920]   hardware target, essentially the same from teeny microcontrollers all the way to giant
[00:08:17.920 --> 00:08:23.240]   beefy GPUs or accelerators that we, or FPGA based stuff that we support as well.
[00:08:23.240 --> 00:08:26.800]   That means that TVM doesn't have any preference of either, right?
[00:08:26.800 --> 00:08:31.440]   So we've had use cases both in the open source community, in the research space as well that
[00:08:31.440 --> 00:08:33.120]   we support and we still do it ourselves.
[00:08:33.120 --> 00:08:37.240]   Other into our current customers that are up to ML, we have customers for both edge
[00:08:37.240 --> 00:08:41.960]   deployment and cloud deployment because the basic technology is effectively the same.
[00:08:41.960 --> 00:08:45.720]   Some of the actual deployment aspects and the plumbing changes a bit.
[00:08:45.720 --> 00:08:48.720]   If you're going to deploy it on a tiny device, you might not even have an operating system,
[00:08:48.720 --> 00:08:49.720]   for example.
[00:08:49.720 --> 00:08:50.720]   So we support some of that.
[00:08:50.720 --> 00:08:54.760]   That's different than a server deployment, but the core aspect of how to make your model
[00:08:54.760 --> 00:08:57.040]   run fast on hardware targets is essentially the same.
[00:08:57.040 --> 00:08:58.040]   I see.
[00:08:58.040 --> 00:09:07.240]   And so I guess for server level deployments, I feel like with the exception of TPUs and
[00:09:07.240 --> 00:09:13.600]   a few companies, it seems like almost everyone deploys onto Nvidia stuff.
[00:09:13.600 --> 00:09:19.320]   Is this sort of like outside of CUDA and CUDA NAND or does it translate into something that
[00:09:19.320 --> 00:09:20.880]   can then be compiled by CUDA?
[00:09:20.880 --> 00:09:21.880]   How does that work?
[00:09:21.880 --> 00:09:23.360]   Yeah, this is an excellent question.
[00:09:23.360 --> 00:09:26.960]   So first let's think about just a world with Nvidia and then just free ourselves from that
[00:09:26.960 --> 00:09:27.960]   with Ethereum.
[00:09:27.960 --> 00:09:28.960]   Okay.
[00:09:28.960 --> 00:09:30.600]   Which is part of the action, is part of the goal here too.
[00:09:30.600 --> 00:09:35.920]   I love Nvidia, many friends there admire what they do, but people should have a choice.
[00:09:35.920 --> 00:09:38.640]   And there's a lot of really good non-Nvidia hardware.
[00:09:38.640 --> 00:09:39.640]   Nvidia makes great hardware.
[00:09:39.640 --> 00:09:41.880]   There's a lot of really great non-Nvidia hardware there.
[00:09:41.880 --> 00:09:43.480]   So let's start with Nvidia.
[00:09:43.480 --> 00:09:46.340]   Let's imagine a world that all you care about is deploying on Nvidia.
[00:09:46.340 --> 00:09:52.240]   So Nvidia has at a very lower level of their compilation stack, they do not expose what
[00:09:52.240 --> 00:09:53.640]   we call the instruction set.
[00:09:53.640 --> 00:09:56.680]   So that's actually, it's kept secret, they don't expose it.
[00:09:56.680 --> 00:09:59.560]   You have to program using CUDA, that's the lowest level.
[00:09:59.560 --> 00:10:01.840]   So and there's CUDA NAND on top.
[00:10:01.840 --> 00:10:05.080]   And it is also parallel to that, you have TensorRT, for example, which is more of a
[00:10:05.080 --> 00:10:07.680]   compiler that you compile a model to the hardware target.
[00:10:07.680 --> 00:10:12.280]   So TVM can be parallel, but at the same time use those.
[00:10:12.280 --> 00:10:13.720]   So here's what I mean, right?
[00:10:13.720 --> 00:10:21.040]   So both CUDA NAND and TensorRT are generally guided and tuned and improved based on models
[00:10:21.040 --> 00:10:23.640]   that people care about and moves with where the models are going.
[00:10:23.640 --> 00:10:28.080]   There's some fair amount of tuning that moves with where the models go, right?
[00:10:28.080 --> 00:10:31.640]   So whereas TVM again, generates fresh code for every fresh model.
[00:10:31.640 --> 00:10:36.000]   So that means that in some cases we do better than TensorRT and CUDA NAND, just because
[00:10:36.000 --> 00:10:41.440]   we can specialize enough in a fully automatic way to the specific GPU, Nvidia GPU that you
[00:10:41.440 --> 00:10:44.680]   have, and then we generate raw CUDA code that you just compile down, right?
[00:10:44.680 --> 00:10:48.040]   So essentially you run your models through TVM, which is a ton of CUDA codes, and then
[00:10:48.040 --> 00:10:51.880]   you compile that into a deployable binary on that specific Nvidia GPU.
[00:10:51.880 --> 00:10:55.480]   But in the process of doing that, TVM, I mean, we do not take a dogmatic view that you should
[00:10:55.480 --> 00:10:56.480]   only use TVM.
[00:10:56.480 --> 00:11:01.280]   In some cases, of course, Nvidia's libraries or Nvidia's compiler like TensorRT can do
[00:11:01.280 --> 00:11:03.920]   better and we want to be able to use that too.
[00:11:03.920 --> 00:11:06.640]   So what TVM does, it does what we call best of all worlds.
[00:11:06.640 --> 00:11:10.680]   That in the process of exploring how to compile your model for parts of your model, say a
[00:11:10.680 --> 00:11:15.840]   set of operators, it sees TVM's version and then CUDA NAND and then TensorRT and it's
[00:11:15.840 --> 00:11:18.120]   like, "Oh, this operator is better to use CUDA NAND."
[00:11:18.120 --> 00:11:19.720]   You just go and put it in.
[00:11:19.720 --> 00:11:25.760]   And then we link the whole thing together such that what we produce for you could be
[00:11:25.760 --> 00:11:26.920]   a Franken binary, right?
[00:11:26.920 --> 00:11:33.340]   So bits and pieces are parts of a CUDA NAND, maybe TensorRT or TVM generated code and produces
[00:11:33.340 --> 00:11:37.480]   a package that essentially specialized to your model, including the choice of whether
[00:11:37.480 --> 00:11:40.440]   you should or should not use Nvidia's own software stack.
[00:11:40.440 --> 00:11:41.440]   Okay.
[00:11:41.440 --> 00:11:43.400]   So did I answer your question on Nvidia?
[00:11:43.400 --> 00:11:44.400]   So this is how-
[00:11:44.400 --> 00:11:45.400]   Yeah, yeah, totally.
[00:11:45.400 --> 00:11:46.600]   And by the way, this is just TVM.
[00:11:46.600 --> 00:11:48.200]   We should talk about the optimizer later.
[00:11:48.200 --> 00:11:51.360]   The optimizer, we want to abstract all of that away even further, which is you upload
[00:11:51.360 --> 00:11:55.040]   your model and then you can choose, you have a checkbox of all sorts of hardwares, Intel
[00:11:55.040 --> 00:12:00.400]   CPUs, AMD CPUs, Nvidia GPUs, soon AMD GPUs, and then Raspberry Pis and all of this stuff
[00:12:00.400 --> 00:12:02.200]   and you can let it tune for all of those.
[00:12:02.200 --> 00:12:06.840]   And in some cases you might choose to run and use a native stack for you.
[00:12:06.840 --> 00:12:08.560]   So we don't even have to think about that.
[00:12:08.560 --> 00:12:09.560]   That's really what we want to offer.
[00:12:09.560 --> 00:12:11.240]   We do not have to worry about it.
[00:12:11.240 --> 00:12:16.480]   So now, okay, TVM, Apache TVM, let's just focus on the open source now, has got quite
[00:12:16.480 --> 00:12:19.960]   a bit of traction in both end users and hardware vendors.
[00:12:19.960 --> 00:12:26.000]   End users, companies like Microsoft, Amazon, Facebook, and so on have used it, some of
[00:12:26.000 --> 00:12:27.960]   them using heavily today.
[00:12:27.960 --> 00:12:32.360]   But now hardware vendors got more and more into the TVM world.
[00:12:32.360 --> 00:12:37.720]   ARM built their CPU, GPU, and NPU compiler and software stack on top of TVM.
[00:12:37.720 --> 00:12:41.320]   We're working with AMD to build one for AMD GPUs as well.
[00:12:41.320 --> 00:12:42.320]   So GPUs and CPUs.
[00:12:42.320 --> 00:12:46.840]   Qualcomm has built their software stack with TVM and we are working with them to further
[00:12:46.840 --> 00:12:49.640]   broaden the reach of the hardware that's supported by that.
[00:12:49.640 --> 00:12:55.060]   The reason I'm telling you this is that as we enable hardware like AMD GPUs to be used
[00:12:55.060 --> 00:13:01.800]   very effectively via TVM, I think we will start offering users a meaningful choice and
[00:13:01.800 --> 00:13:05.040]   they should go with the hardware to better serve them without having to necessarily choose
[00:13:05.040 --> 00:13:07.640]   that based on the software stack.
[00:13:07.640 --> 00:13:10.360]   Can I ask a couple of specific questions?
[00:13:10.360 --> 00:13:12.040]   Is that a question or does that make sense?
[00:13:12.040 --> 00:13:15.520]   No, that makes total sense.
[00:13:15.520 --> 00:13:20.800]   We do a lot of work with Qualcomm and they talk a lot about ONNX, which I think my understanding
[00:13:20.800 --> 00:13:25.960]   is that's sort of a translation layer between models and places like hardware that it could
[00:13:25.960 --> 00:13:27.160]   deploy on.
[00:13:27.160 --> 00:13:30.160]   So how does that connect with TVM?
[00:13:30.160 --> 00:13:31.160]   Yeah.
[00:13:31.160 --> 00:13:35.640]   So there's no visuals, I could show you, but think of it as this is a stack, right?
[00:13:35.640 --> 00:13:39.280]   So lowest level hardware, and then you add what you have or compile and operate this
[00:13:39.280 --> 00:13:40.640]   and then you have your code generator.
[00:13:40.640 --> 00:13:42.480]   So that's where our libraries, right?
[00:13:42.480 --> 00:13:44.360]   So two, and that's where TVM sits.
[00:13:44.360 --> 00:13:48.040]   And then on top of that, you have your model framework like TensorFlow, PyTorch, Keras,
[00:13:48.040 --> 00:13:49.800]   MXNet and so on.
[00:13:49.800 --> 00:13:53.800]   ONNX as a spec is wonderful, right?
[00:13:53.800 --> 00:13:57.800]   So essentially it's a common language for you to describe models.
[00:13:57.800 --> 00:14:03.600]   And TVM takes as input models written, specified in ONNX.
[00:14:03.600 --> 00:14:07.720]   But it also takes native TensorFlow, native PyTorch, native Keras, MXNet and so on.
[00:14:07.720 --> 00:14:13.680]   But ONNX is like, if you go to the optimizer service today, you can upload an ONNX model.
[00:14:13.680 --> 00:14:19.000]   And then in the guts of the optimizer, you go and call TVM to import the model and do
[00:14:19.000 --> 00:14:20.000]   its magic, right?
[00:14:20.000 --> 00:14:23.080]   But think of ONNX as a language to describe models.
[00:14:23.080 --> 00:14:28.200]   And so do you think that, I mean, I feel like one of the reasons that I've heard that NVIDIA
[00:14:28.200 --> 00:14:33.000]   has been so hard to displace as sort of the main way people deploy most of their stuff
[00:14:33.000 --> 00:14:35.960]   is because the could and end libraries is so effective.
[00:14:35.960 --> 00:14:41.600]   Like, do you sort of imagine that as TVM gets more powerful, it opens things up to other
[00:14:41.600 --> 00:14:42.600]   hardware companies?
[00:14:42.600 --> 00:14:43.600]   That's right.
[00:14:43.600 --> 00:14:44.600]   Yeah.
[00:14:44.600 --> 00:14:50.120]   And I think NVIDIA has been brilliant in offering, I mean, they have a really, really good software
[00:14:50.120 --> 00:14:53.760]   stack and they of course have good hardware too, but the fact that they have a usable
[00:14:53.760 --> 00:14:59.560]   and broad, and I would say arguably some of the best low-level machine learning system
[00:14:59.560 --> 00:15:02.280]   software stack there gives them a huge advantage, right?
[00:15:02.280 --> 00:15:07.080]   So some other hardware could be just as good in terms of raw processing power, model memory
[00:15:07.080 --> 00:15:09.080]   and kinds of architecture and so on.
[00:15:09.080 --> 00:15:12.880]   If you don't have a good software stack, they're simply not competitive, right?
[00:15:12.880 --> 00:15:17.040]   And we definitely see TVM as offering that choice too, right?
[00:15:17.040 --> 00:15:20.520]   And again, I don't want to sound like we are going to go and compete with NVIDIA.
[00:15:20.520 --> 00:15:21.520]   That's not the point.
[00:15:21.520 --> 00:15:23.120]   I'm just thinking, just think about this, right?
[00:15:23.120 --> 00:15:25.880]   So today, forget machine learning.
[00:15:25.880 --> 00:15:27.360]   Let's just think about operating systems, right?
[00:15:27.360 --> 00:15:28.360]   So you have Linux.
[00:15:28.360 --> 00:15:31.320]   Linux runs in pretty much all the hardware that you care about, right?
[00:15:31.320 --> 00:15:32.320]   Sure.
[00:15:32.320 --> 00:15:33.920]   You might still choose to run Windows, right?
[00:15:33.920 --> 00:15:37.000]   But at least in the same hardware you can choose to run Windows or Linux.
[00:15:37.000 --> 00:15:40.120]   Think of it as TVM is offering a choice of what kind of operating system you'd run on
[00:15:40.120 --> 00:15:44.240]   your hardware, except that you don't have to choose a proprietary one.
[00:15:44.240 --> 00:15:49.320]   And in the machine learning world with NVIDIA, there's essentially no choice today unless
[00:15:49.320 --> 00:15:53.120]   you're going to go and write CUDA code directly, right?
[00:15:53.120 --> 00:15:56.680]   So I guess one of the things, and this is probably the part of the show where I ask
[00:15:56.680 --> 00:16:00.420]   the dumb questions that my team is going to make fun of me for, but kind of in the back
[00:16:00.420 --> 00:16:04.640]   of my head, I feel like I always have this mystery where a new version of CUDA then comes
[00:16:04.640 --> 00:16:10.720]   out and the models get way faster with just a better library.
[00:16:10.720 --> 00:16:17.220]   And I think about what a model does, like a convolution or a matrix multiplication.
[00:16:17.220 --> 00:16:19.280]   It seems so simple to me.
[00:16:19.280 --> 00:16:23.720]   It just seems, and that's kind of how it seems because I feel like I come from more of a
[00:16:23.720 --> 00:16:24.720]   math background.
[00:16:24.720 --> 00:16:30.780]   And I'm just like, how could many years in to making a library, like CUDA in a few years,
[00:16:30.780 --> 00:16:34.700]   how could there be a 20% speed up on a matrix multiplication?
[00:16:34.700 --> 00:16:35.700]   What's going on?
[00:16:35.700 --> 00:16:36.700]   That's a brilliant question.
[00:16:36.700 --> 00:16:37.700]   Yeah.
[00:16:37.700 --> 00:16:38.700]   Great question, Lukas.
[00:16:38.700 --> 00:16:39.700]   All right.
[00:16:39.700 --> 00:16:40.700]   So we should take a whiteboard out and I'll show it to you.
[00:16:40.700 --> 00:16:43.940]   So because then it gets even closer to my world.
[00:16:43.940 --> 00:16:45.780]   Let's think about computer architecture for a second, right?
[00:16:45.780 --> 00:16:51.260]   So let's say that you are an execution engine, like a processor or a core in a GPU, right?
[00:16:51.260 --> 00:16:53.140]   So you have to grab, let's start with one reason.
[00:16:53.140 --> 00:16:55.100]   Like you have to grab data from somewhere in memory, right?
[00:16:55.100 --> 00:17:00.620]   So it turns out that computers, memory is organized in ways that depending on where
[00:17:00.620 --> 00:17:05.820]   the data is in memory, which actual address physically in your memory it is, it gives
[00:17:05.820 --> 00:17:09.300]   you much better performance than others, like by a huge margin, right?
[00:17:09.300 --> 00:17:14.400]   So because depending on how you lay it out, the data, you can actually make the most use
[00:17:14.400 --> 00:17:18.260]   of the wires between your memory and your processor, between your cache and your actual
[00:17:18.260 --> 00:17:20.520]   execution engine in the silicon itself.
[00:17:20.520 --> 00:17:25.460]   And figuring out where that goes becomes a combinatorial problem because not only you
[00:17:25.460 --> 00:17:29.060]   have to choose where the data structure go, but also when you have a bunch of nested loops
[00:17:29.060 --> 00:17:34.540]   that implements your convolution, you have to choose, like if you have a four deep nested
[00:17:34.540 --> 00:17:36.220]   loop in which order should you execute them?
[00:17:36.220 --> 00:17:37.220]   Any order is valid.
[00:17:37.220 --> 00:17:38.220]   Which order should you execute them?
[00:17:38.220 --> 00:17:41.260]   And then within those, you might want to traverse like what size of blocks are you going to
[00:17:41.260 --> 00:17:42.560]   traverse that?
[00:17:42.560 --> 00:17:47.540]   So and all of that is highly dependent on the parameters of your convolution.
[00:17:47.540 --> 00:17:48.900]   I'm just speaking convolution, right?
[00:17:49.280 --> 00:17:52.200]   So even just general matrix multiplication, right?
[00:17:52.200 --> 00:17:56.100]   So long story short, for any given operator, there's literally potentially billions of
[00:17:56.100 --> 00:18:00.840]   ways in which you can compile the same bit by bit equivalent program in terms of outputs.
[00:18:00.840 --> 00:18:04.880]   But one of them is going to be potentially a thousand times faster than this lowest one.
[00:18:04.880 --> 00:18:06.900]   So picking the right one is hard.
[00:18:06.900 --> 00:18:11.800]   Often this is done today by human intuition and some amount of automatic tuning called
[00:18:11.800 --> 00:18:12.800]   auto tuning, right?
[00:18:12.800 --> 00:18:17.320]   So what's happening in CUDNN as you see your model gets faster is that you have, NVIDIA
[00:18:17.320 --> 00:18:19.480]   can afford a large number of programmers, right?
[00:18:19.480 --> 00:18:22.720]   So a lot of really talented soft engineers, they observe where the models are going.
[00:18:22.720 --> 00:18:24.520]   There's some models that matters to them.
[00:18:24.520 --> 00:18:27.680]   They're going to go, they're going to look at the model, see the parameters of all of
[00:18:27.680 --> 00:18:28.880]   the operators, how they're stitched together.
[00:18:28.880 --> 00:18:31.680]   And then you're going to start tuning the libraries to make sure that they do better
[00:18:31.680 --> 00:18:32.680]   data layouts.
[00:18:32.680 --> 00:18:33.840]   They make better loop ordering.
[00:18:33.840 --> 00:18:36.320]   They do better tiling of how the data structure works.
[00:18:36.320 --> 00:18:39.840]   They choose the direction which they're traversing, the data structures and so on.
[00:18:39.840 --> 00:18:41.520]   And that's just one type.
[00:18:41.520 --> 00:18:42.560]   That's just one operator.
[00:18:42.560 --> 00:18:45.280]   But now models, operators talk to other operators, right?
[00:18:45.280 --> 00:18:46.920]   So that's why there's something called operator fusion.
[00:18:46.920 --> 00:18:51.560]   If you fuse two operators, for example, like a matrix multiplication, a convolution to
[00:18:51.560 --> 00:18:52.960]   a single operator.
[00:18:52.960 --> 00:18:57.080]   So now you can generate code in a way that it can keep data as close to your processing
[00:18:57.080 --> 00:18:58.280]   engine as much as possible.
[00:18:58.280 --> 00:19:01.360]   So you make much better use of your memory hierarchy.
[00:19:01.360 --> 00:19:04.120]   And that's yet another significant performance bump, right?
[00:19:04.120 --> 00:19:05.760]   So it might give you a general sense like this is what-
[00:19:05.760 --> 00:19:06.760]   Totally, that was really helpful.
[00:19:06.760 --> 00:19:07.760]   Yeah.
[00:19:07.760 --> 00:19:11.720]   So I guess you can't actually decompose the problem down into, I was sort of picturing
[00:19:11.720 --> 00:19:14.960]   that each step in the compute graph, you could optimize it separately, but actually you have
[00:19:14.960 --> 00:19:15.960]   to-
[00:19:15.960 --> 00:19:16.960]   Yeah, you want to do it together.
[00:19:16.960 --> 00:19:17.960]   In fact, yeah.
[00:19:17.960 --> 00:19:23.360]   If you read the TVM, there were like two, actually three PhD theses.
[00:19:23.360 --> 00:19:29.480]   At the very least, those are the ones that I've been involved in on the core of TVM.
[00:19:29.480 --> 00:19:32.920]   And if you read the first paper, it's been around for several years now.
[00:19:32.920 --> 00:19:36.680]   One of the key messages there, the highest level was the following, is that by doing
[00:19:36.680 --> 00:19:40.480]   high level graph optimization together with code optimization, that's where a lot of the
[00:19:40.480 --> 00:19:41.480]   power comes from.
[00:19:41.480 --> 00:19:46.600]   So essentially, say if you choose to fuse two operators in the graph, now we need to
[00:19:46.600 --> 00:19:47.960]   generate really good code for it.
[00:19:47.960 --> 00:19:52.160]   So now you're going to go and use our automatic highly specialized code generator that uses
[00:19:52.160 --> 00:19:55.200]   machine learning to do the search for this new operator that fused the two with different
[00:19:55.200 --> 00:19:56.200]   parameters.
[00:19:56.200 --> 00:20:00.560]   So by combining high level graph optimizations with low level code generation that specialize
[00:20:00.560 --> 00:20:04.960]   to that, you have significant multiplicative optimization opportunities.
[00:20:04.960 --> 00:20:05.960]   Interesting.
[00:20:05.960 --> 00:20:07.400]   Does that give you...
[00:20:07.400 --> 00:20:08.400]   Yeah.
[00:20:08.400 --> 00:20:09.400]   So that's-
[00:20:09.400 --> 00:20:10.400]   No, no, that's really helpful.
[00:20:10.400 --> 00:20:11.400]   Yeah.
[00:20:11.400 --> 00:20:19.280]   So does the new TPU architectures change anything about this optimization or does it change
[00:20:19.280 --> 00:20:20.440]   what you're doing at all?
[00:20:20.440 --> 00:20:24.000]   Well, it's a different hardware architecture, so you need to go in tune for it as well.
[00:20:24.000 --> 00:20:27.720]   But you remember that TPUs are also made of a bunch of transistor functional units and
[00:20:27.720 --> 00:20:31.240]   floating point units and vector units, and they have wires, they have memories organized
[00:20:31.240 --> 00:20:34.880]   in a certain way that you want to make the most of.
[00:20:34.880 --> 00:20:41.200]   In a sense, a lot of these specialized architectures, what they do, and in fact, TVM also has an
[00:20:41.200 --> 00:20:45.200]   open source CPU-like accelerator that's fully open source hardware.
[00:20:45.200 --> 00:20:47.000]   You can actually stamp it out in FPGA.
[00:20:47.000 --> 00:20:49.360]   Some folks have stamped it out in actual custom silicon.
[00:20:49.360 --> 00:20:52.320]   That gives you a template of how you think about these accelerators.
[00:20:52.320 --> 00:20:56.560]   And they also have parameters, so the different sizes of memories and buffers, what data types
[00:20:56.560 --> 00:21:02.080]   you support, how many functional units you have to have the right throughput.
[00:21:02.080 --> 00:21:05.800]   And it's all a balance of how you organize your memory, how much of your silicon you're
[00:21:05.800 --> 00:21:10.200]   going to devote to compute versus storage, how many wires and how is your interconnection
[00:21:10.200 --> 00:21:11.880]   network to move data around is connected.
[00:21:11.880 --> 00:21:16.200]   The reason I'm telling you this is that many times the trade-off here is the following.
[00:21:16.200 --> 00:21:21.680]   You might make the hardware more complicated to program, harder to program, but immensely
[00:21:21.680 --> 00:21:23.880]   more efficient.
[00:21:23.880 --> 00:21:29.000]   But that means that now you need to rely even more on a compiler to make really good code
[00:21:29.000 --> 00:21:31.920]   generation and specialize how you're going to compile your code to that specific hardware
[00:21:31.920 --> 00:21:32.920]   target.
[00:21:32.920 --> 00:21:33.920]   Right, right.
[00:21:33.920 --> 00:21:35.480]   Because that's a fair trade-off.
[00:21:35.480 --> 00:21:38.600]   A compilation you do once, it might be complicated.
[00:21:38.600 --> 00:21:41.960]   But then if you zoom harder, they have to do every time as data is flowing, it's much
[00:21:41.960 --> 00:21:43.360]   better to do it ahead of time.
[00:21:43.360 --> 00:21:49.800]   I mean, I'm digging deep into my computer science education, but I feel like the story
[00:21:49.800 --> 00:21:57.120]   with computer, the non-deep learning chips, hasn't it been simpler, smaller instruction
[00:21:57.120 --> 00:22:00.160]   sets and trying to simplify things?
[00:22:00.160 --> 00:22:04.280]   It seems the opposite direction of adding complexity into the hardware and then relying
[00:22:04.280 --> 00:22:06.360]   on the compiler to deal with it.
[00:22:06.360 --> 00:22:07.360]   Yeah, yeah.
[00:22:07.360 --> 00:22:08.360]   It's a great question.
[00:22:08.360 --> 00:22:11.120]   There's so many, and I think there could be a whole other conversation too.
[00:22:11.120 --> 00:22:15.720]   But the whole, when the risk versus assist debate happened in the computer architecture
[00:22:15.720 --> 00:22:19.840]   class that I teach, at a graduate level, I actually have them have debates.
[00:22:19.840 --> 00:22:25.200]   So the key aspect there was that by going to a simpler instruction set, you had simpler
[00:22:25.200 --> 00:22:27.520]   hardware so you can actually clock it faster.
[00:22:27.520 --> 00:22:30.840]   So you could have lots of little instructions, but you execute a lot of them per unit of
[00:22:30.840 --> 00:22:32.960]   time so you can run faster.
[00:22:32.960 --> 00:22:37.840]   So it turns out that even complex instruction computers today, like x86 and Intel, they
[00:22:37.840 --> 00:22:39.600]   break it down automatically into teeny instructions.
[00:22:39.600 --> 00:22:43.200]   It still looks like a risk computer inside.
[00:22:43.200 --> 00:22:47.280]   But fast forward to today, what's going on is that there was a huge change in the trends
[00:22:47.280 --> 00:22:51.400]   and we've seen performance coming from in different computer architectures.
[00:22:51.400 --> 00:22:57.560]   So as we get closer and closer to the limits of scaling of transistor technology, what
[00:22:57.560 --> 00:22:58.920]   happens is the following.
[00:22:58.920 --> 00:23:03.400]   You have a certain number of transistors, they're getting ever smaller and more power
[00:23:03.400 --> 00:23:04.400]   efficient.
[00:23:04.400 --> 00:23:08.640]   There was a change that transistors are getting smaller, but not necessarily much more power
[00:23:08.640 --> 00:23:12.740]   efficient, which means that you can pack more transistors on the chip, but it cannot turn
[00:23:12.740 --> 00:23:14.820]   all of them on at the same time.
[00:23:14.820 --> 00:23:17.000]   You might be saying, "Why am I telling you this?"
[00:23:17.000 --> 00:23:22.280]   Because that's the whole justification for going more and more specialized and have a
[00:23:22.280 --> 00:23:27.760]   big chip, a bigger chip with lots of different, quote unquote, more specialized function units.
[00:23:27.760 --> 00:23:31.360]   They're not general, but they're much more efficient because every time you add generality
[00:23:31.360 --> 00:23:34.600]   in the hardware, you fundamentally are adding more switches.
[00:23:34.600 --> 00:23:38.000]   So for example, general purpose CPU that can do anything.
[00:23:38.000 --> 00:23:42.280]   There's a large fraction, more than half of the transistors there are just simply sitting
[00:23:42.280 --> 00:23:43.920]   there asking questions, "Am I doing this or that?
[00:23:43.920 --> 00:23:45.760]   If I'm doing this, I do this."
[00:23:45.760 --> 00:23:49.400]   And then have to make decisions about the data that's flowing through because it's supposed
[00:23:49.400 --> 00:23:50.720]   to be general.
[00:23:50.720 --> 00:23:53.960]   So the trend that we're seeing now is that, well, we need to make this thing much more
[00:23:53.960 --> 00:23:57.740]   efficient, otherwise we can't afford the power to run a global infrastructure or you can't
[00:23:57.740 --> 00:24:01.040]   afford the power to run machine learning.
[00:24:01.040 --> 00:24:03.080]   You have to squeeze efficiency from somewhere.
[00:24:03.080 --> 00:24:06.920]   The way you squeeze efficiency, you remove all of these transistors just sitting there
[00:24:06.920 --> 00:24:10.720]   wondering what they should do, which is the only one thing and one thing very, very, very
[00:24:10.720 --> 00:24:11.720]   well.
[00:24:11.720 --> 00:24:15.400]   Sure, it makes it harder to program because now you have to figure out when and how you
[00:24:15.400 --> 00:24:19.240]   should use this specialized function units, but it's immensely more efficient in terms
[00:24:19.240 --> 00:24:23.480]   of performance per watt and immensely faster than general purpose computers.
[00:24:23.480 --> 00:24:26.160]   Did I answer your question or did I make it more complicated?
[00:24:26.160 --> 00:24:27.820]   Did I confuse you or did I help?
[00:24:27.820 --> 00:24:28.820]   No, this is incredible.
[00:24:28.820 --> 00:24:31.880]   I feel like I'm finally getting clear answers to questions that have been in my head for
[00:24:31.880 --> 00:24:32.880]   a long time.
[00:24:32.880 --> 00:24:34.800]   So I'm actually really, really enjoying this.
[00:24:34.800 --> 00:24:38.940]   But I guess, what should I be imagining as a specialized instruction?
[00:24:38.940 --> 00:24:43.440]   I hear on the M1 laptop, there's a specialized thing to play videos.
[00:24:43.440 --> 00:24:46.560]   What does a specialized instruction look like?
[00:24:46.560 --> 00:24:50.800]   There's a convolution instruction that I could pass through?
[00:24:50.800 --> 00:24:54.800]   For example, it's an eight by eight matrix multiply, single instruction.
[00:24:54.800 --> 00:24:55.800]   Really?
[00:24:55.800 --> 00:24:58.080]   Yeah, you can invoke that.
[00:24:58.080 --> 00:25:02.560]   You set up, you put all the data in the right place and you say, eight by eight matrix multiply,
[00:25:02.560 --> 00:25:03.560]   boom, it happens.
[00:25:03.560 --> 00:25:04.560]   In one tick?
[00:25:04.560 --> 00:25:07.680]   Yeah, not exactly in one tick.
[00:25:07.680 --> 00:25:10.960]   It's one instruction, which means that you give it one command.
[00:25:10.960 --> 00:25:14.840]   It could be broken down into multiple cycles, depends on how it's scheduled.
[00:25:14.840 --> 00:25:20.200]   But from your programming point of view, there's hardware there, essentially an arrangement
[00:25:20.200 --> 00:25:24.880]   of your transistors that implements your functional units and your memories, organized in such
[00:25:24.880 --> 00:25:26.240]   a way there's something called a systolic array.
[00:25:26.240 --> 00:25:30.160]   I don't know if you've heard this term before, but systolic arrays, it's an array of multiply
[00:25:30.160 --> 00:25:31.520]   and accumulate, I think it that way.
[00:25:31.520 --> 00:25:34.960]   So you can just flow data in a specific way that if you just arrange it just right and
[00:25:34.960 --> 00:25:39.240]   you flow it to any one flow, you've done an eight by eight GMF.
[00:25:39.240 --> 00:25:41.920]   But to do that, you have to arrange all the data in the right place and then click go.
[00:25:41.920 --> 00:25:44.600]   So not click, issue an instruction and go.
[00:25:44.600 --> 00:25:50.800]   But now to answer your video compression question or video codec, so we call it an instruction,
[00:25:50.800 --> 00:25:55.160]   but more likely it's essentially a piece of hardware that's just sitting there, knows
[00:25:55.160 --> 00:25:56.560]   where to read data from.
[00:25:56.560 --> 00:25:59.600]   And what you do is just configure it.
[00:25:59.600 --> 00:26:04.120]   The program for real is actually in the actual function specific hardware.
[00:26:04.120 --> 00:26:07.320]   And all you do in your code is to say, activate that now.
[00:26:07.320 --> 00:26:08.640]   Here's the data stream, activate that.
[00:26:08.640 --> 00:26:11.880]   And then you have a fixed function hardware that just starts crunching through that and
[00:26:11.880 --> 00:26:15.560]   decoding your video, for example, or applying a certain computation.
[00:26:15.560 --> 00:26:19.120]   Another thing that people are doing in hardware is activation functions.
[00:26:19.120 --> 00:26:22.480]   Some activation functions is so popular, people use it all the time, but why are you going
[00:26:22.480 --> 00:26:25.160]   to break it down into 30, 40 instructions?
[00:26:25.160 --> 00:26:28.080]   You can design a piece of hardware that does that and just that.
[00:26:28.080 --> 00:26:30.960]   And all you're doing is when you call that activation function, you just activate that
[00:26:30.960 --> 00:26:31.960]   piece of hardware.
[00:26:31.960 --> 00:26:32.960]   Wow.
[00:26:32.960 --> 00:26:41.520]   So I guess if it's laws of physics that are pushing this trend, it seems like you probably
[00:26:41.520 --> 00:26:44.520]   expect this trend to continue for a long time.
[00:26:44.520 --> 00:26:46.840]   And if it does, where would it go?
[00:26:46.840 --> 00:26:51.960]   Would there be even more and more complicated instructions possible in the hardware?
[00:26:51.960 --> 00:26:54.400]   And wouldn't that make research harder?
[00:26:54.400 --> 00:26:58.640]   What if you wanted to do a new activation function that wasn't available in your hardware?
[00:26:58.640 --> 00:26:59.640]   Yeah.
[00:26:59.640 --> 00:27:00.640]   So that's a really great question, Lucas.
[00:27:00.640 --> 00:27:06.600]   So let me try this, the first big question first, and then we can branch down to these
[00:27:06.600 --> 00:27:09.640]   other sub questions about research and how do we continue advancing this.
[00:27:09.640 --> 00:27:14.160]   So yeah, so that's the reality right now.
[00:27:14.160 --> 00:27:18.480]   We already have quite a bit of diversity, not just a different hardware chips and hardware
[00:27:18.480 --> 00:27:19.480]   parts.
[00:27:19.480 --> 00:27:20.960]   Just look at all the AI chip companies out there.
[00:27:20.960 --> 00:27:25.320]   Just look at what's happening to the general purpose processors, like Intel processors
[00:27:25.320 --> 00:27:29.680]   getting specialized instructions that are relevant to machine learning and so on.
[00:27:29.680 --> 00:27:34.160]   So that's going to continue going because honestly, there's just no other way to get
[00:27:34.160 --> 00:27:36.840]   efficiency unless now I'm going to...
[00:27:36.840 --> 00:27:41.040]   Let me open a nerdy, not joke, but a nerdy speculation.
[00:27:41.040 --> 00:27:45.640]   Unless we can teach atoms to rearrange themselves at the atomic level to go, "Let's reconfigure
[00:27:45.640 --> 00:27:49.280]   where your wires are," and therefore you have your chip doing a new thing.
[00:27:49.280 --> 00:27:50.280]   Isn't that like a...
[00:27:50.280 --> 00:27:52.080]   There's a kind of chip like that, right?
[00:27:52.080 --> 00:27:53.080]   Like a FPGA or something.
[00:27:53.080 --> 00:27:54.080]   Is that what it is?
[00:27:54.080 --> 00:27:55.080]   Yeah, but I'm going to get there.
[00:27:55.080 --> 00:27:56.080]   But there's no magic.
[00:27:56.080 --> 00:27:58.320]   FPGAs is just, there's a bunch of wires that are there.
[00:27:58.320 --> 00:28:02.840]   You're just inserting data to tell you which wires you should use, but the wires are always
[00:28:02.840 --> 00:28:03.840]   there.
[00:28:03.840 --> 00:28:06.760]   And just the fact that you have a table that tells you, "If I have this bit on, I'm going
[00:28:06.760 --> 00:28:07.760]   to use this wire.
[00:28:07.760 --> 00:28:08.980]   If I have this bit on, I'm going to use the other wire."
[00:28:08.980 --> 00:28:10.660]   Just that causes inefficiency.
[00:28:10.660 --> 00:28:11.660]   So it's always a trade-off.
[00:28:11.660 --> 00:28:15.880]   Think of it as a trade-off between how general or hard is your...
[00:28:15.880 --> 00:28:17.840]   So there's a generality of a specialized curve.
[00:28:17.840 --> 00:28:20.560]   More general, less energy efficient, easier to program.
[00:28:20.560 --> 00:28:21.920]   More specialized, right?
[00:28:21.920 --> 00:28:25.400]   So more efficient, harder to program, and so on.
[00:28:25.400 --> 00:28:26.840]   But then you have FPGAs.
[00:28:26.840 --> 00:28:27.840]   How about FPGAs?
[00:28:27.840 --> 00:28:33.440]   FPGAs are essentially, they are very general fabric with a very complicated programming
[00:28:33.440 --> 00:28:38.880]   model because FPGAs, what they are, they are a fabric of their bag of wires and little
[00:28:38.880 --> 00:28:44.620]   routing tables and sprinkled some multiply and accumulates or more and more activation
[00:28:44.620 --> 00:28:49.760]   functions and other popular compute elements that you just sprinkle in an even fabric.
[00:28:49.760 --> 00:28:53.340]   And then you just set bits to figure out how you're going to route the data.
[00:28:53.340 --> 00:28:57.080]   So that looks like, the way you program, that looks like how you design hardware.
[00:28:57.080 --> 00:28:59.280]   And they can be very efficient if you do it right.
[00:28:59.280 --> 00:29:03.880]   But fundamentally, they're not going to be more efficient than true fixed function chips.
[00:29:03.880 --> 00:29:08.580]   You're never going to see an FPGA competing with a GPU on the very same task.
[00:29:08.580 --> 00:29:15.480]   You see FPGAs competing with things like GPUs and so on, when you can specialize to your
[00:29:15.480 --> 00:29:19.800]   application and even with the efficiency hit of the hardware, you still have a win.
[00:29:19.800 --> 00:29:20.800]   Does that make sense?
[00:29:20.800 --> 00:29:25.080]   So for example, let's say if you decide that you want two bits data flow for like, let's
[00:29:25.080 --> 00:29:29.160]   say the quantization to two bits here in one layer, three bits on the other layer, and
[00:29:29.160 --> 00:29:30.740]   I know one bit on the other layer, right?
[00:29:30.740 --> 00:29:35.440]   So if it just so happens, there's no existing silicon that can do that for an existing CPU
[00:29:35.440 --> 00:29:41.000]   or GPU that can do that for you, chances are you're going to be living with an eight bits
[00:29:41.000 --> 00:29:44.840]   data plane and you're going to ignore some bits there and then you're going to waste
[00:29:44.840 --> 00:29:47.720]   efficiency there, or you're going to do inefficient packing.
[00:29:47.720 --> 00:29:52.080]   But with an FPGA, you can organize it such that you only activate, you only route your
[00:29:52.080 --> 00:29:54.400]   circuits to use the two bits or one bit or two bits.
[00:29:54.400 --> 00:29:59.280]   In that case, because the data type is more unique, you can specialize to your model and
[00:29:59.280 --> 00:30:02.640]   then you can do really well with an FPGA.
[00:30:02.640 --> 00:30:03.640]   That makes sense.
[00:30:03.640 --> 00:30:05.560]   And now on research, answer your question on research.
[00:30:05.560 --> 00:30:07.600]   Yeah, so research I think is getting more interesting, honestly.
[00:30:07.600 --> 00:30:12.720]   I haven't seen, maybe I'm getting old and a curmudgeon here, but I feel like, I'm not
[00:30:12.720 --> 00:30:16.760]   going to say like curmudgeon, I'm being old and optimistic here, is that I've never seen
[00:30:16.760 --> 00:30:19.880]   computer systems architecture and systems optimization being as interesting as it is
[00:30:19.880 --> 00:30:21.440]   right now, right?
[00:30:21.440 --> 00:30:25.760]   Because it was a period of researching this, it was just about making microprocessors faster,
[00:30:25.760 --> 00:30:27.600]   making a little bit better compilers.
[00:30:27.600 --> 00:30:32.000]   But now that we have to specialize and there's this really exciting application space with
[00:30:32.000 --> 00:30:36.400]   machine learning that offers so many opportunities for optimizations and you have things like
[00:30:36.400 --> 00:30:39.400]   FPGAs and it's getting easier to design chips.
[00:30:39.400 --> 00:30:43.640]   It creates all sorts of opportunities for academic research and also for industrial
[00:30:43.640 --> 00:30:44.640]   innovation.
[00:30:44.640 --> 00:30:50.240]   Hence, we see all of these wonderful new chips and Xilinx with new FPGAs and new FPGA companies
[00:30:50.240 --> 00:30:56.240]   and Sembanova with reconfigurable fabrics and all of these cool hardware targets, right?
[00:30:56.240 --> 00:31:00.020]   I guess, I mean, this is a little bit, maybe this, well, I guess I'm curious, it seems
[00:31:00.020 --> 00:31:05.160]   like ML is becoming a bigger and bigger fraction of data centers and data centers are becoming
[00:31:05.160 --> 00:31:09.000]   a bigger and bigger fraction of global energy use.
[00:31:09.000 --> 00:31:13.760]   Do you feel like there's an environmental impact that you can have by making these things
[00:31:13.760 --> 00:31:14.760]   run more efficiently?
[00:31:14.760 --> 00:31:15.760]   Absolutely, yeah.
[00:31:15.760 --> 00:31:18.440]   And we're not the only ones to make the claim.
[00:31:18.440 --> 00:31:22.880]   Essentially, every time you make an algorithm faster in the same hardware, you're saving
[00:31:22.880 --> 00:31:24.560]   energy, you're saving trees, right?
[00:31:24.560 --> 00:31:26.800]   So you're reducing resource pressure.
[00:31:26.800 --> 00:31:31.000]   So performance optimization is this wonderful thing that you can reap the benefits in so
[00:31:31.000 --> 00:31:32.000]   many ways.
[00:31:32.000 --> 00:31:34.440]   If you make it faster, you're going to make your users happy.
[00:31:34.440 --> 00:31:38.720]   But also, even if it's not latency sensitive, you're going to make your finance folks happy
[00:31:38.720 --> 00:31:40.420]   because you're going to spend less on cloud bills.
[00:31:40.420 --> 00:31:44.560]   But in the end, you're going to be using less energy and that really matters, right?
[00:31:44.560 --> 00:31:50.440]   So now, what we're teaching about environmental impact specifically is that, as you pointed
[00:31:50.440 --> 00:31:54.680]   out, there's a growing fraction of energy in the world that's devoted to computing,
[00:31:54.680 --> 00:31:55.680]   right?
[00:31:55.680 --> 00:31:57.460]   So I'm not going to get into cryptocurrencies.
[00:31:57.460 --> 00:31:58.680]   We're not going to go there right now.
[00:31:58.680 --> 00:31:59.680]   That's a whole separate topic.
[00:31:59.680 --> 00:32:01.240]   Think about the energy cost of that.
[00:32:01.240 --> 00:32:05.640]   Let's just think about the energy cost of machine learning infrastructure that includes
[00:32:05.640 --> 00:32:07.720]   training and deploying models at scale, right?
[00:32:07.720 --> 00:32:11.840]   So it's fair to say that in a typical application you use machine learning today, the majority
[00:32:11.840 --> 00:32:14.420]   of the cycles will go to the machine learning computation, right?
[00:32:14.420 --> 00:32:17.480]   So even memory that you have to keep alive with energy, right?
[00:32:17.480 --> 00:32:22.480]   So anything that you can do to make the hardware more efficient, to make your model more efficient
[00:32:22.480 --> 00:32:27.680]   at the model layer or make it by compiling and optimizing the model specific hardware
[00:32:27.680 --> 00:32:31.840]   is a win, both in terms of user experience and energy efficiencies.
[00:32:31.840 --> 00:32:36.400]   And by making more energy efficient, you make it much less environmentally impactful, right?
[00:32:36.400 --> 00:32:37.400]   So absolutely.
[00:32:37.400 --> 00:32:43.280]   So you should take every opportunity you can to reduce the energy that your models use,
[00:32:43.280 --> 00:32:44.560]   especially if it's deployed at scale.
[00:32:44.560 --> 00:32:47.000]   Even if it doesn't matter from a user experience point of view, you should do it because that's
[00:32:47.000 --> 00:32:49.240]   just the right thing to do, right?
[00:32:49.240 --> 00:32:57.360]   Did you really separate the model compiling and performance and the way that the models
[00:32:57.360 --> 00:32:58.360]   design?
[00:32:58.360 --> 00:33:02.400]   It feels like a lot of the performance improvements in models come from sort of relaxing the constraint
[00:33:02.400 --> 00:33:05.960]   that you need to exactly do the convolution or the matrix.
[00:33:05.960 --> 00:33:13.280]   Just for example, quantization, where you do it in a ludicrously small level of precision
[00:33:13.280 --> 00:33:14.520]   seems to work really well.
[00:33:14.520 --> 00:33:15.520]   No, absolutely.
[00:33:16.080 --> 00:33:19.920]   I did not mean to imply that we should only do model compilation.
[00:33:19.920 --> 00:33:24.800]   Remember that I said, I'm assuming that you're going to come with your model tuned for the
[00:33:24.800 --> 00:33:26.880]   least amount of computation you possibly use.
[00:33:26.880 --> 00:33:27.880]   That's the ideal case.
[00:33:27.880 --> 00:33:32.560]   But you're absolutely right that there are optimizations at the model level that actually
[00:33:32.560 --> 00:33:35.160]   changes the statistical property of the model that enables new optimization.
[00:33:35.160 --> 00:33:39.800]   We can do that too, but TVM does have support, growing support for quantization.
[00:33:39.800 --> 00:33:43.240]   But what I'm particularly interested in, in general, is how do you put things like TVM
[00:33:43.240 --> 00:33:46.640]   in the whole network architecture search loop?
[00:33:46.640 --> 00:33:50.600]   So as you make decisions about your model architecture, and as you retrain for different
[00:33:50.600 --> 00:33:54.520]   model architectures, and you can make new optimization decisions on the model layer
[00:33:54.520 --> 00:33:58.400]   and change the convolution, the data types, and doing all sorts of things like pruning
[00:33:58.400 --> 00:34:02.480]   and compression, deep compression, et cetera.
[00:34:02.480 --> 00:34:07.040]   Put a compiler in the loop to like TVM and measure what's the performance that you're
[00:34:07.040 --> 00:34:11.360]   getting as part of your search loop, because then you really get the synergies.
[00:34:11.360 --> 00:34:16.120]   You're right that you can decouple them in principle, and you're still going to do relatively
[00:34:16.120 --> 00:34:17.120]   well.
[00:34:17.120 --> 00:34:21.400]   But if you do both of them together, I think you're up for more than the addition of either
[00:34:21.400 --> 00:34:23.520]   of them in terms of potential opportunities.
[00:34:23.520 --> 00:34:25.960]   That's what TVM did in terms of high-level graph and low-level optimization.
[00:34:25.960 --> 00:34:29.040]   By doing them together, we show that we can do better.
[00:34:29.040 --> 00:34:31.320]   And I do think that the same thing...
[00:34:31.320 --> 00:34:35.320]   I have data points that shows that the same thing could happen if you do model building
[00:34:35.320 --> 00:34:43.400]   and tuning decisions together with model compilation and hardware tuning together.
[00:34:43.400 --> 00:34:44.960]   Are there trade-offs between...
[00:34:44.960 --> 00:34:50.000]   With GCC, you can optimize for memory or you can optimize for speed.
[00:34:50.000 --> 00:34:56.920]   Is there a latency memory size trade-off here, or are they both aligned with each other?
[00:34:56.920 --> 00:34:57.920]   Yeah.
[00:34:57.920 --> 00:34:58.920]   That's a great question.
[00:34:58.920 --> 00:35:03.600]   Of course, one optimization that definitely impacts memory usage specifically is when
[00:35:03.600 --> 00:35:05.680]   you do model compression or if you do quantization.
[00:35:05.680 --> 00:35:11.200]   If you go from FP32 to INT8, you already have a 4x footprint reduction in your...
[00:35:11.200 --> 00:35:13.000]   You go from 32 bits to 8 bits.
[00:35:13.000 --> 00:35:14.960]   But that'll also make it run faster, right?
[00:35:14.960 --> 00:35:20.400]   So there's no real trade-off there if the quantization keeps the performance you want,
[00:35:20.400 --> 00:35:21.400]   right?
[00:35:21.400 --> 00:35:22.400]   Potentially.
[00:35:22.400 --> 00:35:24.120]   If you're assuming quantization, that's just like you have the same model architecture,
[00:35:24.120 --> 00:35:26.360]   you just change the data type and go.
[00:35:26.360 --> 00:35:30.520]   But that's sort of the easy, lazy quantization.
[00:35:30.520 --> 00:35:33.240]   The right way of doing it, in my opinion, is that once you change the data type, you're
[00:35:33.240 --> 00:35:36.440]   not given an opportunity to actually go and retrain it and some parts of your model become
[00:35:36.440 --> 00:35:37.440]   less...
[00:35:37.440 --> 00:35:40.160]   I think the right way of doing quantization is not just quantize the data type and forget
[00:35:40.160 --> 00:35:41.160]   about it, right?
[00:35:41.160 --> 00:35:46.280]   It's actually close the loop and put it on a network architecture search such that as
[00:35:46.280 --> 00:35:49.520]   you change the data type, you actually allow for different types of...
[00:35:49.520 --> 00:35:54.120]   And then in that case, I think you're up for significant changes to the model that would
[00:35:54.120 --> 00:35:58.640]   make quantization potentially even more effective, right?
[00:35:58.640 --> 00:36:00.920]   So you're asking, but I did not answer your question.
[00:36:00.920 --> 00:36:03.080]   So what's the trade-off between latency and footprint?
[00:36:03.080 --> 00:36:04.360]   Well, it could be that.
[00:36:04.360 --> 00:36:07.920]   It could be that you actually quantize your model, but then you actually make it deeper
[00:36:07.920 --> 00:36:14.320]   to actually make up for some accuracy loss, which might make your model actually potentially
[00:36:14.320 --> 00:36:16.360]   slower, right?
[00:36:16.360 --> 00:36:17.440]   But use a lot less memory.
[00:36:17.440 --> 00:36:19.400]   So there is that trade-off there too.
[00:36:19.400 --> 00:36:24.120]   I guess my experience of deploying models, and I'm just an amateur at this, but I love
[00:36:24.120 --> 00:36:26.320]   my Raspberry Pis and other cheap hardware.
[00:36:26.320 --> 00:36:32.200]   Yeah, and we support Raspberry Pis pretty well in TVM, but you should try it out too.
[00:36:32.200 --> 00:36:33.880]   I will definitely try it after this.
[00:36:33.880 --> 00:36:38.160]   So I did it kind of in the early days of trying to get TensorFlow to run when even that was
[00:36:38.160 --> 00:36:39.160]   a challenge.
[00:36:39.160 --> 00:36:42.980]   And I felt like basically with models, it was sort of binary where either I could fit
[00:36:42.980 --> 00:36:46.680]   it in the Pi's memory and it would run, or I couldn't fit it in the Pi's memory and it
[00:36:46.680 --> 00:36:47.680]   wouldn't run.
[00:36:47.680 --> 00:36:51.480]   So it seemed like less about sort of like optimizing and just either I'm sort of stuck
[00:36:51.480 --> 00:36:52.560]   or I'm not.
[00:36:52.560 --> 00:36:54.120]   Is that a common situation?
[00:36:54.120 --> 00:36:56.760]   It's hard to say if it's common.
[00:36:56.760 --> 00:37:00.440]   Often at least for the models that we get, they get to the point where we pay attention
[00:37:00.440 --> 00:37:04.560]   to them and we know that they run now, but they typically don't run, say the frame rates
[00:37:04.560 --> 00:37:08.560]   that you want, you get half a frame per second and they can well not show you a path to 20
[00:37:08.560 --> 00:37:09.560]   frames per second, right?
[00:37:09.560 --> 00:37:17.000]   So by that time, the model already fits, you're optimizing for performance.
[00:37:17.000 --> 00:37:22.680]   But often this performance optimization comes also with model size reduction.
[00:37:22.680 --> 00:37:27.680]   So let's say if you can just go from FB16 to int8 and it works well, boom, you do that,
[00:37:27.680 --> 00:37:33.120]   you save, you probably improve performance and you also reduce model size, right?
[00:37:33.120 --> 00:37:37.600]   But I've seen plenty of cases where the model already runs and what's hard is actually get
[00:37:37.600 --> 00:37:42.280]   to a target latency that would actually enable the model to be useful.
[00:37:42.280 --> 00:37:44.880]   That's actually by and large what we tend to see is you get your model to run, you hack
[00:37:44.880 --> 00:37:47.160]   it enough to go there, but then it's never fast enough.
[00:37:47.160 --> 00:37:50.880]   And then you're going to go and you need it, and we're on the 10X ahead of you for it to
[00:37:50.880 --> 00:37:52.200]   actually be useful, right?
[00:37:52.200 --> 00:37:53.200]   So totally.
[00:37:53.200 --> 00:37:54.200]   Well, cool.
[00:37:54.200 --> 00:37:56.640]   I don't want to not ask you about your company, OctoML.
[00:37:56.640 --> 00:38:00.880]   I feel like you're one in a growing line of people that are talking to their professors
[00:38:00.880 --> 00:38:03.080]   and they're like starting companies.
[00:38:03.080 --> 00:38:06.440]   What inspired you to build this company?
[00:38:06.440 --> 00:38:07.720]   Yeah, great question.
[00:38:07.720 --> 00:38:11.160]   So first of all, it's one of those moments where all the stars are aligned.
[00:38:11.160 --> 00:38:13.880]   So TVM had gotten quite a bit...
[00:38:13.880 --> 00:38:17.480]   We started a company just about a little under two years ago.
[00:38:17.480 --> 00:38:21.280]   TVM had quite a bit of adoption by then already.
[00:38:21.280 --> 00:38:25.720]   And we saw more and more hardware vendors starting to choose TVM as their chosen software
[00:38:25.720 --> 00:38:26.720]   stack.
[00:38:26.720 --> 00:38:30.760]   And we ran our second conference here in Seattle and I saw like a lot of people like, "Oh, there's
[00:38:30.760 --> 00:38:35.560]   an opportunity here to make TVM even more broadly...
[00:38:35.560 --> 00:38:38.960]   What TVM can do more broadly accessible."
[00:38:38.960 --> 00:38:42.920]   And then the stars are set aligning because I was looking to start another company and
[00:38:42.920 --> 00:38:46.200]   I had become full professor a couple of years before then.
[00:38:46.200 --> 00:38:50.160]   A lot of the core PhD students in TVM were graduating and now instead of coming just
[00:38:50.160 --> 00:38:55.480]   like that, and one of our big champions of TVM, Jason Knight, was at Intel at that time,
[00:38:55.480 --> 00:38:59.200]   was one of our co-founders, was also looking to starting something.
[00:38:59.200 --> 00:39:00.200]   And then we just like...
[00:39:00.200 --> 00:39:02.760]   All the stars aligned in form and I couldn't be...
[00:39:02.760 --> 00:39:08.320]   I feel extremely lucky that we had that group of people ready to start a company and then
[00:39:08.320 --> 00:39:10.000]   we work really well together.
[00:39:10.000 --> 00:39:11.680]   There's a lot of synergy there.
[00:39:11.680 --> 00:39:14.040]   But then, so that's sort of like the stars aligned part.
[00:39:14.040 --> 00:39:19.440]   Now, in terms of technology, it became really clear to all of us that, look, you have this
[00:39:19.440 --> 00:39:24.680]   cross product between model and hardware and there's such a huge opportunity to create
[00:39:24.680 --> 00:39:30.000]   a clean abstraction there and at the same time automate away what's becoming harder
[00:39:30.000 --> 00:39:33.520]   and harder about making machine learning truly useful and deployable.
[00:39:33.520 --> 00:39:38.400]   So honestly, in the DML ops, and I don't love that term because it means so many things,
[00:39:38.400 --> 00:39:44.640]   but going from data to a deployed model, it's clear that the tools to create models got
[00:39:44.640 --> 00:39:45.640]   good pretty fast.
[00:39:45.640 --> 00:39:46.640]   We have a lot of...
[00:39:46.640 --> 00:39:51.160]   There are a lot of people that can create models today and good models, a large repository
[00:39:51.160 --> 00:39:52.880]   of models to start from.
[00:39:52.880 --> 00:39:57.280]   But after interviewing a bunch of potential customers, we realized that, hey, well, people
[00:39:57.280 --> 00:40:01.080]   have actually have a lot of difficulties in getting models to be deployed precisely because
[00:40:01.080 --> 00:40:05.360]   of the software engineering required and the level of performance requirement and cost
[00:40:05.360 --> 00:40:07.000]   requirement to make it viable.
[00:40:07.000 --> 00:40:11.760]   So we formed OctoML to essentially make TVM even more accessible or technologies like
[00:40:11.760 --> 00:40:17.680]   TVM even more accessible to a broad set of model builders and also make it part of the
[00:40:17.680 --> 00:40:18.680]   flow.
[00:40:18.680 --> 00:40:20.800]   Let me just tell briefly what the Optimizer is.
[00:40:20.800 --> 00:40:24.160]   So the Optimizer is a machine learning acceleration platform.
[00:40:24.160 --> 00:40:26.840]   So it has TVM at its heart.
[00:40:26.840 --> 00:40:31.880]   So you have a really clean either API, just a couple of calls, upload model, choose and
[00:40:31.880 --> 00:40:33.560]   then download optimized model.
[00:40:33.560 --> 00:40:35.240]   And you can choose a harder target.
[00:40:35.240 --> 00:40:38.360]   And then you upload a model, you can choose the harder targets that you want.
[00:40:38.360 --> 00:40:42.120]   And then the Optimizer calls TVM or also can use Onyx Runtime.
[00:40:42.120 --> 00:40:44.920]   And we're going to keep adding more.
[00:40:44.920 --> 00:40:49.320]   Again, we want to offer the user the abstraction that you upload a model and you get the fastest
[00:40:49.320 --> 00:40:52.160]   possible model ready to be deployed on your hard and fully automated fashion.
[00:40:52.160 --> 00:40:58.800]   You either get a Python wheel ready to download, or we're working on gRPC, gRPC package so
[00:40:58.800 --> 00:41:01.480]   you can deploy in the cloud or in the cloud functions and so on.
[00:41:01.480 --> 00:41:06.060]   And then so the value add here is all this automation that we provide on top of TVM.
[00:41:06.060 --> 00:41:10.520]   And also the fact that, as I mentioned, TVM uses machine learning for machine learning.
[00:41:10.520 --> 00:41:14.040]   And we have a data set for a lot of the core harder targets that the world cares about
[00:41:14.040 --> 00:41:15.040]   just ready to go.
[00:41:15.040 --> 00:41:17.320]   So you don't have to go and collect it yourself.
[00:41:17.320 --> 00:41:23.340]   I would think running OctoML, you'd have real visibility into how the different hardware
[00:41:23.340 --> 00:41:25.440]   platforms can compare with each other.
[00:41:25.440 --> 00:41:30.300]   I'm sure you don't want to offend the hardware partners, but do you sort of have first pass
[00:41:30.300 --> 00:41:34.280]   recommendations for what people should be targeting in different situations?
[00:41:34.280 --> 00:41:35.280]   Yeah.
[00:41:35.280 --> 00:41:37.660]   So, and that's one of the things that I want the numbers to speak for themselves.
[00:41:37.660 --> 00:41:42.420]   So what you can do is you can, if you come to the optimizing facts, we are open for early
[00:41:42.420 --> 00:41:46.440]   access and we actually have some real users already using it regularly.
[00:41:46.440 --> 00:41:48.960]   So you upload a model, you can choose all sorts of harder targets.
[00:41:48.960 --> 00:41:52.760]   And then you're going to get a dashboard saying, "Here's your model.
[00:41:52.760 --> 00:41:55.000]   Here's the latency of each one of these hardware targets."
[00:41:55.000 --> 00:41:59.160]   And we can compare TVM with other runtimes like Onyx runtime, for example, and we're
[00:41:59.160 --> 00:42:02.640]   going to show you which one you should use and you can choose based on that.
[00:42:02.640 --> 00:42:06.600]   Of course, we're making it that we're working hard to improve the interface to enable users
[00:42:06.600 --> 00:42:08.600]   to make decisions about costs too, for example.
[00:42:08.600 --> 00:42:12.120]   You might want to get the highest throughput per dollar, for example.
[00:42:12.120 --> 00:42:19.400]   So I would say that it's fair to say that models vary so much that it's hard to say
[00:42:19.400 --> 00:42:20.920]   up front like this is going to be the best.
[00:42:20.920 --> 00:42:27.280]   You should run there, run it through the optimizer, get the most efficient version and binder of
[00:42:27.280 --> 00:42:29.360]   your model out and then measure that.
[00:42:29.360 --> 00:42:30.360]   Cool.
[00:42:30.360 --> 00:42:35.320]   Well, I guess that actually leads me into the two questions that we always end with,
[00:42:35.320 --> 00:42:37.640]   which I want to give you time to chew on.
[00:42:37.640 --> 00:42:39.400]   And I haven't asked you about a lot of your research.
[00:42:39.400 --> 00:42:44.280]   It seems super fascinating, but I guess I wanted to ask you, what do you think is a
[00:42:44.280 --> 00:42:48.560]   topic in machine learning that doesn't get enough attention?
[00:42:48.560 --> 00:42:52.680]   If you had an extra time to just work on something that you're interested in, maybe you would
[00:42:52.680 --> 00:42:54.680]   pick to go deeper on.
[00:42:54.680 --> 00:42:58.560]   Yeah, so honestly, it's getting more and more attention now, but I've always been, and a
[00:42:58.560 --> 00:43:02.640]   lot of my research has been in automating systems design with machine learning and for
[00:43:02.640 --> 00:43:03.640]   machine learning.
[00:43:03.640 --> 00:43:09.680]   So TVM is one example of using machine learning to enable better model optimization and compilation,
[00:43:09.680 --> 00:43:14.160]   but also doing hardware design and programming FPGAs, for example, is really hard and machine
[00:43:14.160 --> 00:43:15.680]   learning could have a huge place there.
[00:43:15.680 --> 00:43:20.400]   So I'd say designing, what I want is really model in and automatic hardware plus software
[00:43:20.400 --> 00:43:22.680]   out, ready to be deployed.
[00:43:22.680 --> 00:43:25.720]   I think that's one that I'm passionate about.
[00:43:25.720 --> 00:43:30.440]   And I think you can have quite a bit of impact precisely because you can reap the benefits
[00:43:30.440 --> 00:43:31.440]   in so many ways.
[00:43:31.440 --> 00:43:35.720]   You get new experiences because you enable new applications, but also making more energy
[00:43:35.720 --> 00:43:36.720]   efficient.
[00:43:36.720 --> 00:43:40.600]   So I think we should actually always look at what is the energy cost of deploying this
[00:43:40.600 --> 00:43:43.160]   at scale, if it's going to be deployed at scale.
[00:43:43.160 --> 00:43:44.440]   In rich countries, you don't think about it, right?
[00:43:44.440 --> 00:43:46.640]   You just go pay the energy, even if it's high.
[00:43:46.640 --> 00:43:50.200]   But now if you really actually think about the environmental impact of running this at
[00:43:50.200 --> 00:43:53.040]   scale is something that one should pay attention to.
[00:43:53.040 --> 00:43:57.800]   So this is actually using machine learning to optimize the model.
[00:43:57.800 --> 00:44:01.760]   Using machine learning to optimize not just the model, but also the system.
[00:44:01.760 --> 00:44:02.760]   Yes.
[00:44:02.760 --> 00:44:07.360]   The model and the system that runs your model such that you get better behavior out.
[00:44:07.360 --> 00:44:11.920]   They can be faster, higher throughput per dollar, but also much lower energy use.
[00:44:11.920 --> 00:44:16.100]   And I think it's definitely incredibly exciting and possible to do, right?
[00:44:16.100 --> 00:44:17.480]   So that's one of them.
[00:44:17.480 --> 00:44:20.220]   Now let's see, one that doesn't get as much attention, but now it's getting more attention
[00:44:20.220 --> 00:44:26.120]   that's dear to my heart that we're not touching too, is the role of machine learning in molecular
[00:44:26.120 --> 00:44:27.120]   biology.
[00:44:27.120 --> 00:44:28.120]   Oh, yeah, me too.
[00:44:28.120 --> 00:44:29.520]   I totally agree.
[00:44:29.520 --> 00:44:34.400]   So as part of my research personality, for the past six years or so, I've been heavily
[00:44:34.400 --> 00:44:40.800]   involved in an effort to design systems for using DNA molecules for data storage and for
[00:44:40.800 --> 00:44:41.800]   simple forms of computation.
[00:44:41.800 --> 00:44:44.040]   Some of it is actually related to machine learning.
[00:44:44.040 --> 00:44:47.720]   For example, we recently demonstrated the ability of doing similarity search directly
[00:44:47.720 --> 00:44:49.440]   as a chemical reaction.
[00:44:49.440 --> 00:44:55.240]   What's cool about that is that not only it's cool, definitely pushing as a new device technology
[00:44:55.240 --> 00:44:59.160]   alternative that's very viable and has been time tested by nature, right?
[00:44:59.160 --> 00:45:00.640]   Time tested for sure.
[00:45:00.640 --> 00:45:01.640]   Yeah.
[00:45:01.640 --> 00:45:04.400]   It can be extremely energy efficient.
[00:45:04.400 --> 00:45:09.080]   And fundamentally, the design of molecular systems is so complex that I cannot imagine
[00:45:09.080 --> 00:45:13.320]   any other way to design them than using machine learning to actually design those models.
[00:45:13.320 --> 00:45:14.320]   And we do it all the time.
[00:45:14.320 --> 00:45:16.880]   We had a paper that you might find cool late last year.
[00:45:16.880 --> 00:45:19.640]   It was in Nature Communications called Porcupine.
[00:45:19.640 --> 00:45:23.640]   We use machine learning to design DNA molecules in such a way that they look so different
[00:45:23.640 --> 00:45:24.640]   to a DNA sequencer.
[00:45:24.640 --> 00:45:27.680]   They're not going to be natural DNA, but we can use this to tag things.
[00:45:27.680 --> 00:45:30.000]   So we design these molecules.
[00:45:30.000 --> 00:45:31.720]   You can go and tag art or tag clothes.
[00:45:31.720 --> 00:45:35.080]   And so you take a quick sample, you run through a sequencer, and you can authenticate that
[00:45:35.080 --> 00:45:36.960]   based on these molecular traces.
[00:45:36.960 --> 00:45:40.920]   But that was made possible because of machine learning in designing the molecule and actually
[00:45:40.920 --> 00:45:43.680]   interpreting the signal out of the DNA sequencer and so on.
[00:45:43.680 --> 00:45:46.280]   And I feel this space, it's not getting enough attention.
[00:45:46.280 --> 00:45:50.360]   I think it's getting more and more now precisely because of the pandemic and all of the other
[00:45:50.360 --> 00:45:51.960]   reasons why molecular biology matters.
[00:45:51.960 --> 00:45:53.640]   But I find it incredibly exciting.
[00:45:53.640 --> 00:45:58.600]   And it's a lot of the high level motivation for things that I do both in research and
[00:45:58.600 --> 00:46:01.360]   in industry is enabling use cases like that.
[00:46:01.360 --> 00:46:05.560]   And the things that require so much computation that wouldn't be possible before without a
[00:46:05.560 --> 00:46:07.760]   very efficient, very fast system.
[00:46:07.760 --> 00:46:08.760]   Cool.
[00:46:08.760 --> 00:46:13.320]   And I guess the question we always end with, which you've touched on a lot in this conversation,
[00:46:13.320 --> 00:46:19.320]   is really what you see the big challenges are today of getting machine learning working
[00:46:19.320 --> 00:46:20.320]   in the real world.
[00:46:20.320 --> 00:46:25.240]   Maybe when you talk to your customers and they optimize their models, what are the other
[00:46:25.240 --> 00:46:29.140]   challenges that they run into when they're trying to get their optimized model just deployed
[00:46:29.140 --> 00:46:32.360]   and working for some end use case?
[00:46:32.360 --> 00:46:33.360]   Yeah.
[00:46:33.360 --> 00:46:37.440]   So, well, of course, I'm devoting a good chunk of my life into the deployment and ultimately
[00:46:37.440 --> 00:46:41.320]   the engineering involving deployment, but I don't want to sound too self-serving to
[00:46:41.320 --> 00:46:42.320]   say that's the biggest problem.
[00:46:42.320 --> 00:46:43.320]   I think that's a huge problem.
[00:46:43.320 --> 00:46:47.440]   It's a huge impediment in terms of skills, the skill set, because it requires people
[00:46:47.440 --> 00:46:50.520]   that know about software engineering, about low level system software, and know about
[00:46:50.520 --> 00:46:51.520]   machine learning.
[00:46:51.520 --> 00:46:52.520]   So that's super hard.
[00:46:52.520 --> 00:46:54.720]   So that's one, definitely getting a model ready for deployment.
[00:46:54.720 --> 00:46:57.320]   But then there's other ones, which is just making sure that your model is behaving the
[00:46:57.320 --> 00:47:00.280]   way it's expected post deployment, like observability.
[00:47:00.280 --> 00:47:04.200]   It's making sure that there's not unexpected inputs to make your model misbehave, have
[00:47:04.200 --> 00:47:06.360]   fail safe behavior, and so on.
[00:47:06.360 --> 00:47:11.760]   I think that's one that is no news probably to this community, that some applications
[00:47:11.760 --> 00:47:15.520]   require either because it's the right thing to do when a model is making decisions that
[00:47:15.520 --> 00:47:18.400]   are super important, you want to understand how they're done and making sure that they
[00:47:18.400 --> 00:47:21.040]   actually hold in unexpected inputs.
[00:47:21.040 --> 00:47:26.560]   So I think that's one of the harder ones because like any engineer that's thinking about the
[00:47:26.560 --> 00:47:29.680]   whole system, you want to think about the weakest link in the system failure.
[00:47:29.680 --> 00:47:33.200]   And I worry that if you don't do something proactively, the weakest link in the systems
[00:47:33.200 --> 00:47:37.320]   are going to start being the models that you can't really reason them in a principled way.
[00:47:37.320 --> 00:47:38.320]   Yeah.
[00:47:38.320 --> 00:47:39.320]   Awesome.
[00:47:39.320 --> 00:47:40.320]   Well, thanks for your time.
[00:47:40.320 --> 00:47:41.320]   This was a lot of fun.
[00:47:41.320 --> 00:47:42.320]   Of course.
[00:47:42.320 --> 00:47:43.320]   Thank you.
[00:47:43.320 --> 00:47:44.320]   Really appreciate it.
[00:47:44.320 --> 00:47:45.320]   This was awesome.
[00:47:45.320 --> 00:47:46.320]   Yeah.
[00:47:46.320 --> 00:47:47.320]   So I enjoyed it immensely.
[00:47:47.320 --> 00:47:48.320]   Thank you.
[00:47:48.320 --> 00:47:51.840]   If you're enjoying Gradient Descent, I'd really love for you to check out Fully Connected,
[00:47:51.840 --> 00:47:57.120]   which is an inclusive machine learning community that we're building to let everyone know about
[00:47:57.120 --> 00:48:01.280]   all the stuff going on in ML and all the new research coming out.
[00:48:01.280 --> 00:48:06.680]   If you go to wmv.ai/fc, you can see all the different stuff that we do, including Gradient
[00:48:06.680 --> 00:48:12.320]   Descent, but also salons where we talk about new research and folks share insights, AMAs
[00:48:12.320 --> 00:48:16.680]   where you can directly connect with members of our community, and a Slack channel where
[00:48:16.680 --> 00:48:22.400]   you can get answers to everything from very basic questions about ML to bug reports on
[00:48:22.400 --> 00:48:26.240]   weights and biases to how to hire an ML team.
[00:48:26.240 --> 00:48:27.360]   We're looking forward to meeting you.

