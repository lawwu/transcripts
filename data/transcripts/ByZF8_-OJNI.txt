
[00:00:00.000 --> 00:00:04.700]   All right, so the human side of AI.
[00:00:04.700 --> 00:00:11.400]   So how do we turn this camera back in on the human?
[00:00:11.400 --> 00:00:14.500]   So we've been talking about perception,
[00:00:14.500 --> 00:00:20.200]   how to detect cats and dogs, pedestrians, lanes,
[00:00:20.200 --> 00:00:23.900]   how to steer a vehicle based on the external environment.
[00:00:23.900 --> 00:00:29.700]   The thing that's really fascinating and severely understudied
[00:00:30.100 --> 00:00:31.700]   is the human side.
[00:00:31.700 --> 00:00:34.600]   So you know, you talk about the Tesla,
[00:00:34.600 --> 00:00:39.000]   we have cameras in 17 Teslas driving around Cambridge
[00:00:39.000 --> 00:00:43.500]   because Tesla is one of the only vehicles allowing you
[00:00:43.500 --> 00:00:49.900]   to experience in the real way on the road
[00:00:49.900 --> 00:00:53.300]   the interaction between the human and the machine.
[00:00:53.300 --> 00:00:57.400]   And the thing that we don't have,
[00:00:57.800 --> 00:01:01.200]   that deep learning needs on the human side
[00:01:01.200 --> 00:01:05.300]   of semi-autonomous vehicles and fully autonomous vehicles
[00:01:05.300 --> 00:01:08.000]   is video of drivers.
[00:01:08.000 --> 00:01:10.000]   That's what we're collecting.
[00:01:10.000 --> 00:01:12.800]   That's what my work is in,
[00:01:12.800 --> 00:01:17.500]   is looking at billions of video frames of human beings
[00:01:17.500 --> 00:01:21.700]   driving 60 miles an hour plus on the highway
[00:01:21.700 --> 00:01:24.100]   in their semi-autonomous Tesla.
[00:01:25.500 --> 00:01:28.200]   What are the things that we want to know about the human?
[00:01:28.200 --> 00:01:33.300]   If we're a deep learning therapist
[00:01:33.300 --> 00:01:37.800]   and we try to break apart the different things
[00:01:37.800 --> 00:01:40.300]   we can detect from this raw set of pixels,
[00:01:40.300 --> 00:01:43.700]   we can look here from the green to red
[00:01:43.700 --> 00:01:45.100]   is the different detection problems,
[00:01:45.100 --> 00:01:47.000]   the different computer vision detection problems.
[00:01:47.000 --> 00:01:51.200]   Green means it's less challenging,
[00:01:52.500 --> 00:01:56.000]   it's feasible even under poor lighting conditions,
[00:01:56.000 --> 00:02:01.300]   variable pose, noisy environment, poor resolution.
[00:02:01.300 --> 00:02:05.300]   Red means it's really hard no matter what you do.
[00:02:05.300 --> 00:02:09.500]   That's starting on the left with face detection and body pose,
[00:02:09.500 --> 00:02:11.300]   one of the best studied
[00:02:11.300 --> 00:02:13.500]   and one of the easier computer vision problems.
[00:02:13.500 --> 00:02:16.700]   We have huge data sets for these.
[00:02:16.700 --> 00:02:19.300]   And then there is micro saccades,
[00:02:19.300 --> 00:02:22.100]   the slight tremors of the eye that happen in one
[00:02:22.900 --> 00:02:25.900]   at a rate of a thousand times a second.
[00:02:25.900 --> 00:02:30.500]   Let's look at,
[00:02:30.500 --> 00:02:36.500]   well first, why do we even care about the human in the car?
[00:02:36.500 --> 00:02:39.100]   One is trust.
[00:02:39.100 --> 00:02:40.700]   This trust part is,
[00:02:40.700 --> 00:02:42.300]   so you think about it,
[00:02:42.300 --> 00:02:45.200]   to build trust,
[00:02:45.200 --> 00:02:47.900]   the car needs to have some awareness
[00:02:47.900 --> 00:02:51.900]   of the biological thing it's carrying inside,
[00:02:52.000 --> 00:02:52.800]   the human inside.
[00:02:52.800 --> 00:02:55.000]   You kind of assume the car knows about you
[00:02:55.000 --> 00:02:56.800]   because you're like sitting there controlling it.
[00:02:56.800 --> 00:02:59.300]   But if you think about it,
[00:02:59.300 --> 00:03:01.800]   almost every single car on the road today
[00:03:01.800 --> 00:03:04.900]   has no sensors with which it's perceiving you.
[00:03:04.900 --> 00:03:09.000]   It knows some cars have a pressure sensor on the steering wheel
[00:03:09.000 --> 00:03:12.600]   and a pressure sensor or some kind of sensor
[00:03:12.600 --> 00:03:14.900]   detecting that you're sitting in the seat.
[00:03:14.900 --> 00:03:17.000]   That's the only thing it knows about you.
[00:03:17.000 --> 00:03:18.800]   That's it.
[00:03:19.300 --> 00:03:21.200]   So how is the car supposed to,
[00:03:21.200 --> 00:03:25.200]   this same car that's driving 70 miles an hour
[00:03:25.200 --> 00:03:27.000]   on the highway autonomously,
[00:03:27.000 --> 00:03:29.800]   how is it supposed to build trust with you
[00:03:29.800 --> 00:03:31.100]   if it doesn't perceive you?
[00:03:31.100 --> 00:03:33.800]   That's one of the critical things here.
[00:03:33.800 --> 00:03:36.800]   So if I'm constantly advocating something,
[00:03:36.800 --> 00:03:39.900]   it's that we should have a driver-facing camera in every car.
[00:03:39.900 --> 00:03:43.000]   And that despite the privacy concerns,
[00:03:43.000 --> 00:03:44.900]   you have a camera on your phone
[00:03:44.900 --> 00:03:47.900]   and you don't have as much of a privacy concern there,
[00:03:48.300 --> 00:03:51.100]   is it despite the privacy concerns,
[00:03:51.100 --> 00:03:54.400]   the safety benefits are huge.
[00:03:54.400 --> 00:03:57.000]   The trust benefits are huge.
[00:03:57.000 --> 00:04:01.900]   So let's start with the easy one, body pose,
[00:04:01.900 --> 00:04:03.200]   detecting body pose.
[00:04:03.200 --> 00:04:05.200]   Why do we care?
[00:04:05.200 --> 00:04:07.600]   So there's seatbelt design.
[00:04:07.600 --> 00:04:10.600]   There's these dummies,
[00:04:10.600 --> 00:04:12.500]   crash test dummies with which,
[00:04:12.500 --> 00:04:16.200]   which are used to design the safety system,
[00:04:16.600 --> 00:04:18.700]   the passive safety systems in our cars.
[00:04:18.700 --> 00:04:21.700]   And they make certain assumptions about body shapes,
[00:04:21.700 --> 00:04:25.000]   male, female, child, body shapes.
[00:04:25.000 --> 00:04:27.000]   But they also make assumptions about
[00:04:27.000 --> 00:04:29.000]   the position of your body in the seat.
[00:04:29.000 --> 00:04:32.000]   They have the optimal position,
[00:04:32.000 --> 00:04:34.100]   the position they assume you take.
[00:04:34.100 --> 00:04:37.400]   The reality is, in a Tesla,
[00:04:37.400 --> 00:04:39.800]   when the car is driving itself,
[00:04:39.800 --> 00:04:42.500]   the variability, if you remember the cat,
[00:04:42.500 --> 00:04:43.700]   the deformable cat,
[00:04:44.300 --> 00:04:46.900]   you start doing a little bit more of that.
[00:04:46.900 --> 00:04:49.200]   You start to reach back in the back seat,
[00:04:49.200 --> 00:04:52.600]   in your purse, your bag, for your cell phone,
[00:04:52.600 --> 00:04:53.900]   these kinds of things.
[00:04:53.900 --> 00:04:55.600]   And that's when the crashes happen.
[00:04:55.600 --> 00:04:58.700]   And we need to know how often that happens.
[00:04:58.700 --> 00:05:01.100]   The car needs to know that you're in that position.
[00:05:01.100 --> 00:05:05.300]   That's critical for that very serious moment
[00:05:05.300 --> 00:05:06.700]   when the actual crash happens.
[00:05:06.700 --> 00:05:10.200]   How do you do?
[00:05:10.200 --> 00:05:12.700]   This is deep learning class, right?
[00:05:12.800 --> 00:05:15.200]   So this deep learning of the rescue.
[00:05:15.200 --> 00:05:19.100]   Whenever you have these kind of tasks of detecting,
[00:05:19.100 --> 00:05:21.000]   for example, body poses,
[00:05:21.000 --> 00:05:22.700]   you're detecting points of the shoulders,
[00:05:22.700 --> 00:05:23.600]   points of the head,
[00:05:23.600 --> 00:05:27.200]   5, 10 points along the arms, the skeleton.
[00:05:27.200 --> 00:05:30.600]   How do you do that?
[00:05:30.600 --> 00:05:34.100]   You have a CNN, convolutional neural network,
[00:05:34.100 --> 00:05:37.600]   that takes this input image and takes as an output,
[00:05:37.600 --> 00:05:38.600]   it's a regressor.
[00:05:38.600 --> 00:05:41.200]   It gives an XY position of the,
[00:05:41.200 --> 00:05:42.400]   whatever you're looking for,
[00:05:42.600 --> 00:05:43.900]   the left shoulder or the right shoulder.
[00:05:43.900 --> 00:05:46.100]   And then you have a cascade of regressors
[00:05:46.100 --> 00:05:48.200]   that give you all these points,
[00:05:48.200 --> 00:05:50.300]   that give you the shoulders, the arms and so on.
[00:05:50.300 --> 00:05:52.100]   And then you have,
[00:05:52.100 --> 00:05:55.700]   through time, on every single frame you make that prediction.
[00:05:55.700 --> 00:05:58.900]   And then you optimize,
[00:05:58.900 --> 00:06:00.300]   you know,
[00:06:00.300 --> 00:06:03.800]   you can make certain assumptions about physics,
[00:06:03.800 --> 00:06:07.300]   that you can't, your arm can't be in this place in one frame
[00:06:07.300 --> 00:06:09.300]   and then the next frame be over here.
[00:06:09.300 --> 00:06:11.300]   It moves smoothly through space.
[00:06:11.600 --> 00:06:12.900]   So under those constraints,
[00:06:12.900 --> 00:06:15.400]   you can then minimize the error,
[00:06:15.400 --> 00:06:20.100]   the temporal error from frame to frame.
[00:06:20.100 --> 00:06:23.700]   Or you can just dump all the frames
[00:06:23.700 --> 00:06:25.200]   as if there are different channels,
[00:06:25.200 --> 00:06:26.800]   like RGB is three channels,
[00:06:26.800 --> 00:06:29.000]   you could think of those channels as in time.
[00:06:29.000 --> 00:06:31.000]   You can dump all those frames together
[00:06:31.000 --> 00:06:34.400]   in what are called 3D convolutional neural networks.
[00:06:34.400 --> 00:06:36.100]   You dump them all together
[00:06:36.100 --> 00:06:38.100]   and then you estimate the body pose
[00:06:38.100 --> 00:06:39.800]   in all the frames at once.
[00:06:40.400 --> 00:06:42.600]   And there are some data sets for sports
[00:06:42.600 --> 00:06:44.700]   and we're building our own.
[00:06:44.700 --> 00:06:46.600]   I don't know who that guy is.
[00:06:46.600 --> 00:06:51.600]   Let's fly through this a little bit.
[00:06:51.600 --> 00:06:54.800]   So what's called gaze classification.
[00:06:54.800 --> 00:06:57.300]   Gaze is another word for glance, right?
[00:06:57.300 --> 00:07:00.600]   It's a classification problem.
[00:07:00.600 --> 00:07:04.600]   Here's one of the TAs for this class.
[00:07:04.600 --> 00:07:07.000]   Again, not here,
[00:07:07.000 --> 00:07:08.100]   because you can't see it.
[00:07:08.200 --> 00:07:09.500]   Again, not here,
[00:07:09.500 --> 00:07:10.500]   because he's married,
[00:07:10.500 --> 00:07:11.500]   had to be home.
[00:07:11.500 --> 00:07:13.900]   I know where his priorities are at.
[00:07:13.900 --> 00:07:14.800]   This is on camera,
[00:07:14.800 --> 00:07:15.600]   should be here.
[00:07:15.600 --> 00:07:18.700]   There's five cameras.
[00:07:18.700 --> 00:07:20.100]   This is what we're recording in the Tesla.
[00:07:20.100 --> 00:07:21.200]   This is a Tesla vehicle.
[00:07:21.200 --> 00:07:24.100]   There's in the bottom right,
[00:07:24.100 --> 00:07:25.900]   there's a blue icon that lights up,
[00:07:25.900 --> 00:07:27.100]   automatically detected
[00:07:27.100 --> 00:07:28.700]   if it's operating under autopilot.
[00:07:28.700 --> 00:07:30.900]   That means the car is currently driving itself.
[00:07:30.900 --> 00:07:32.200]   There's five cameras,
[00:07:32.200 --> 00:07:33.400]   one of the forward roadway,
[00:07:33.400 --> 00:07:34.700]   one in the instrument cluster,
[00:07:34.700 --> 00:07:35.700]   one of the center stack,
[00:07:35.700 --> 00:07:36.500]   steering wheel,
[00:07:36.500 --> 00:07:37.500]   his face.
[00:07:37.800 --> 00:07:40.400]   And then it's a classification problem.
[00:07:40.400 --> 00:07:41.700]   You dump the raw pixels
[00:07:41.700 --> 00:07:43.400]   into a convolutional neural network,
[00:07:43.400 --> 00:07:45.500]   have six classes,
[00:07:45.500 --> 00:07:46.700]   forward roadway,
[00:07:46.700 --> 00:07:49.000]   you're predicting where the person is looking,
[00:07:49.000 --> 00:07:50.500]   forward roadway, left, right,
[00:07:50.500 --> 00:07:54.400]   center stack, instrument cluster,
[00:07:54.400 --> 00:07:55.600]   rear view mirror,
[00:07:55.600 --> 00:07:58.700]   and you give it millions of frames
[00:07:58.700 --> 00:07:59.500]   for every class.
[00:07:59.500 --> 00:08:00.300]   Simple.
[00:08:00.300 --> 00:08:05.900]   And it does incredibly well at predicting
[00:08:06.800 --> 00:08:08.000]   where the driver is looking.
[00:08:08.000 --> 00:08:09.700]   And the process is the same
[00:08:09.700 --> 00:08:13.300]   for majority of the driver state problems
[00:08:13.300 --> 00:08:14.400]   that have to do with the face.
[00:08:14.400 --> 00:08:16.300]   The face has so much information,
[00:08:16.300 --> 00:08:18.300]   where you're looking,
[00:08:18.300 --> 00:08:19.300]   emotion,
[00:08:19.300 --> 00:08:20.400]   drowsiness,
[00:08:20.400 --> 00:08:22.800]   so different degrees of frustration.
[00:08:22.800 --> 00:08:24.200]   I'll fly through those as well.
[00:08:24.200 --> 00:08:25.600]   But the process is the same.
[00:08:25.600 --> 00:08:27.200]   There's some pre-processing.
[00:08:27.200 --> 00:08:29.300]   So this is in the wild data.
[00:08:29.300 --> 00:08:31.400]   There's a lot of crazy light going on.
[00:08:31.400 --> 00:08:32.200]   There's noise,
[00:08:32.200 --> 00:08:33.800]   there's vibration from the vehicle.
[00:08:33.800 --> 00:08:35.500]   So first you have to
[00:08:36.100 --> 00:08:37.100]   video stabilization.
[00:08:37.100 --> 00:08:38.800]   You have to remove all that vibration,
[00:08:38.800 --> 00:08:40.600]   all that noise as best as you can.
[00:08:40.600 --> 00:08:41.300]   There's a lot of
[00:08:41.300 --> 00:08:43.000]   algorithms,
[00:08:43.000 --> 00:08:44.900]   non neural network algorithms.
[00:08:44.900 --> 00:08:48.000]   Boring but they work
[00:08:48.000 --> 00:08:50.800]   for removing the noise,
[00:08:50.800 --> 00:08:51.900]   removing the effects of
[00:08:51.900 --> 00:08:53.700]   sudden light variations
[00:08:53.700 --> 00:08:55.100]   and the vibrations of the vehicle.
[00:08:55.100 --> 00:08:57.300]   There's the automated calibration.
[00:08:57.300 --> 00:08:59.700]   So you have to estimate the frame of the camera,
[00:08:59.700 --> 00:09:00.800]   the position of the camera,
[00:09:00.800 --> 00:09:02.200]   and
[00:09:02.200 --> 00:09:05.400]   estimate the identity of the person you're looking at.
[00:09:06.000 --> 00:09:08.000]   The more you can specialize the network
[00:09:08.000 --> 00:09:09.500]   to the identity of the person
[00:09:09.500 --> 00:09:12.300]   and the identity of the car the person is riding in,
[00:09:12.300 --> 00:09:15.700]   the better the performance for the different driver state classification.
[00:09:15.700 --> 00:09:18.000]   So you personalize the network.
[00:09:18.000 --> 00:09:20.100]   You have a background model that works on everyone
[00:09:20.100 --> 00:09:21.900]   and you specialize each individual.
[00:09:21.900 --> 00:09:22.900]   This is transfer learning.
[00:09:22.900 --> 00:09:26.000]   You specialize each individual network to that one individual.
[00:09:26.000 --> 00:09:27.200]   All right.
[00:09:27.200 --> 00:09:29.900]   There is face frontalization.
[00:09:29.900 --> 00:09:31.500]   Fancy name
[00:09:31.500 --> 00:09:32.700]   for the fact that
[00:09:32.700 --> 00:09:34.000]   no matter where they're looking,
[00:09:34.000 --> 00:09:35.200]   you want to transfer that face
[00:09:35.300 --> 00:09:36.000]   so the eyes
[00:09:36.000 --> 00:09:38.700]   and nose are the exact same position in the image.
[00:09:38.700 --> 00:09:41.400]   That way if you want to look at the eyes
[00:09:41.400 --> 00:09:45.200]   and you want to study the subtle movement of the eyes,
[00:09:45.200 --> 00:09:46.500]   the subtle blinking,
[00:09:46.500 --> 00:09:49.000]   the dynamics of the eyelid,
[00:09:49.000 --> 00:09:50.600]   the velocity of the eyelid,
[00:09:50.600 --> 00:09:51.900]   it's always in the same place.
[00:09:51.900 --> 00:09:53.500]   You can really focus in,
[00:09:53.500 --> 00:09:56.400]   remove all effects of any other motion of the head.
[00:09:56.400 --> 00:09:59.500]   And then you just,
[00:09:59.500 --> 00:10:01.300]   this is the beauty of deep learning, right?
[00:10:01.300 --> 00:10:03.100]   You don't, there is some pre-processing
[00:10:03.100 --> 00:10:04.600]   because this is
[00:10:05.500 --> 00:10:06.600]   real-world data,
[00:10:06.600 --> 00:10:08.800]   but you just dump the raw pixels in.
[00:10:08.800 --> 00:10:12.400]   You dump the raw pixels in and predict whatever you need.
[00:10:12.400 --> 00:10:13.500]   What do you need?
[00:10:13.500 --> 00:10:14.900]   One is emotion.
[00:10:14.900 --> 00:10:15.700]   You can have,
[00:10:15.700 --> 00:10:19.300]   so we had a study where people
[00:10:19.300 --> 00:10:21.700]   used a crappy and a good
[00:10:21.700 --> 00:10:24.000]   voice-based navigation system.
[00:10:24.000 --> 00:10:26.400]   So the crappy one got them really frustrated
[00:10:26.400 --> 00:10:29.500]   and they self-reported as a frustrating experience or not
[00:10:29.500 --> 00:10:30.500]   on a scale of 1 to 10.
[00:10:30.500 --> 00:10:31.900]   So that gives us ground truth.
[00:10:31.900 --> 00:10:34.400]   But it had a bunch of people use the system
[00:10:34.800 --> 00:10:38.100]   and you know, they put themselves as frustrated or not.
[00:10:38.100 --> 00:10:39.600]   And so then we can predict,
[00:10:39.600 --> 00:10:42.200]   we can train a convolutional neural network to predict
[00:10:42.200 --> 00:10:43.700]   is this person frustrated or not.
[00:10:43.700 --> 00:10:45.400]   I think we've seen a video of that.
[00:10:45.400 --> 00:10:49.400]   Turns out smiling is a strong indication of frustration.
[00:10:49.400 --> 00:10:51.600]   You can also predict drowsiness in this way,
[00:10:51.600 --> 00:10:54.400]   gaze estimation in this way,
[00:10:54.400 --> 00:10:55.600]   cognitive load.
[00:10:55.600 --> 00:10:57.400]   I'll briefly look at that.
[00:10:57.400 --> 00:10:59.200]   And the process is all the same.
[00:10:59.200 --> 00:11:00.700]   You detect the face,
[00:11:00.700 --> 00:11:02.700]   you find the landmark points in the face
[00:11:02.800 --> 00:11:04.800]   for the face alignment, face frontalization.
[00:11:04.800 --> 00:11:08.000]   And then you dump the raw pixels in
[00:11:08.000 --> 00:11:09.600]   for classification, step 5.
[00:11:09.600 --> 00:11:11.500]   You can use SVMs there
[00:11:11.500 --> 00:11:13.700]   or you can use what everyone uses now,
[00:11:13.700 --> 00:11:15.000]   convolutional neural networks.
[00:11:15.000 --> 00:11:21.400]   This is the one part where CNNs have still struggled to compete,
[00:11:21.400 --> 00:11:23.700]   is the alignment problem.
[00:11:23.700 --> 00:11:27.800]   This is where I talked about the cascade of regressors,
[00:11:27.800 --> 00:11:31.200]   is finding the landmarks
[00:11:31.300 --> 00:11:39.300]   on the eyebrows, the nose, the jawline, the mouth.
[00:11:39.300 --> 00:11:42.100]   There's certain constraints there
[00:11:42.100 --> 00:11:47.100]   and so algorithms that can utilize those constraints effectively
[00:11:47.100 --> 00:11:50.900]   can often perform better than end-to-end regressors
[00:11:50.900 --> 00:11:53.800]   that just don't have any concept of what a face is shaped like.
[00:11:53.800 --> 00:11:57.500]   And there's huge datasets and we're a part
[00:11:58.100 --> 00:12:01.900]   of the awesome community that's building those datasets
[00:12:01.900 --> 00:12:03.400]   for face alignment.
[00:12:03.400 --> 00:12:06.900]   Okay, so this is, again, the TA in his younger form.
[00:12:06.900 --> 00:12:13.700]   This is live in the car, real-time system,
[00:12:13.700 --> 00:12:15.500]   predicting where they're looking.
[00:12:15.500 --> 00:12:20.700]   This is taking slow steps towards the
[00:12:20.700 --> 00:12:25.600]   exciting direction that machine learning is headed,
[00:12:25.600 --> 00:12:27.000]   which is unsupervised learning.
[00:12:27.900 --> 00:12:31.500]   The less you have to have humans look through the data
[00:12:31.500 --> 00:12:32.900]   and annotate that data,
[00:12:32.900 --> 00:12:36.700]   the more power these machine learning algorithms get.
[00:12:36.700 --> 00:12:38.200]   Right, so,
[00:12:38.200 --> 00:12:41.700]   currently supervised learning is what's needed.
[00:12:41.700 --> 00:12:44.900]   You need human beings to label a cat and label a dog.
[00:12:44.900 --> 00:12:49.800]   But if you can only have a human being label 1%,
[00:12:49.800 --> 00:12:52.200]   1/10 of a percent of a dataset,
[00:12:52.200 --> 00:12:54.300]   only the hard cases,
[00:12:54.300 --> 00:12:56.600]   so the machine can come to the human and be like,
[00:12:56.700 --> 00:12:59.700]   "I don't know what I'm looking at these pictures."
[00:12:59.700 --> 00:13:02.000]   Because of the partial light occlusions,
[00:13:02.000 --> 00:13:04.700]   we're not good at dealing with occlusions,
[00:13:04.700 --> 00:13:07.700]   whether it's your own arm or because of light conditions.
[00:13:07.700 --> 00:13:11.100]   We're not good with crazy light,
[00:13:11.100 --> 00:13:12.800]   drowning out the image.
[00:13:12.800 --> 00:13:15.300]   This is what Google self-driving car actually struggle with
[00:13:15.300 --> 00:13:17.000]   when they're trying to use their vision sensors.
[00:13:17.000 --> 00:13:18.600]   Moving out of frame,
[00:13:18.600 --> 00:13:22.100]   so just all kinds of occlusions are really hard
[00:13:22.100 --> 00:13:25.100]   for computer vision algorithms.
[00:13:26.200 --> 00:13:27.200]   And in those case,
[00:13:27.200 --> 00:13:29.600]   we want a machine to step in and say,
[00:13:29.600 --> 00:13:31.800]   and pass that image on to the human,
[00:13:31.800 --> 00:13:33.000]   be like, "Help me out with this."
[00:13:33.000 --> 00:13:36.400]   And the other corner cases is,
[00:13:36.400 --> 00:13:38.200]   so in driving for example,
[00:13:38.200 --> 00:13:40.200]   90+% of the time,
[00:13:40.200 --> 00:13:42.700]   all you're doing is staring forward at the roadway in the same way.
[00:13:42.700 --> 00:13:44.400]   That's where the machine shines.
[00:13:44.400 --> 00:13:46.100]   That's where machine annotation,
[00:13:46.100 --> 00:13:48.800]   automated annotation shines.
[00:13:48.800 --> 00:13:51.000]   Because it's seen that face
[00:13:51.000 --> 00:13:53.800]   for hundreds of millions of frames already
[00:13:53.800 --> 00:13:55.200]   in that exact position.
[00:13:55.600 --> 00:13:57.800]   So it can do all the hard work of annotation for you.
[00:13:57.800 --> 00:14:00.800]   It's in the transition away from those positions
[00:14:00.800 --> 00:14:02.000]   that it needs a little bit help.
[00:14:02.000 --> 00:14:03.400]   Just to make sure,
[00:14:03.400 --> 00:14:06.300]   that this person just start looking away
[00:14:06.300 --> 00:14:07.900]   from the road to the rear view
[00:14:07.900 --> 00:14:10.000]   and you bring those points up.
[00:14:10.000 --> 00:14:10.800]   So you're,
[00:14:10.800 --> 00:14:13.700]   there's using optical flow,
[00:14:13.700 --> 00:14:15.200]   putting the optical flow
[00:14:15.200 --> 00:14:17.100]   in the convolutional neural network.
[00:14:17.100 --> 00:14:21.500]   You use that to predict when something has changed.
[00:14:21.500 --> 00:14:22.800]   And when something has changed,
[00:14:22.800 --> 00:14:24.800]   you bring that to the machine for annotation.
[00:14:25.300 --> 00:14:27.500]   All of this is to build a giant,
[00:14:27.500 --> 00:14:29.900]   billions of frames,
[00:14:29.900 --> 00:14:31.500]   annotated dataset
[00:14:31.500 --> 00:14:32.800]   of ground truth.
[00:14:32.800 --> 00:14:34.800]   I want you to train your
[00:14:34.800 --> 00:14:36.600]   driver state algorithms.
[00:14:36.600 --> 00:14:39.300]   And in this way, you can control.
[00:14:39.300 --> 00:14:41.500]   On the X-axis is the fraction of frames
[00:14:41.500 --> 00:14:42.700]   that a human has to annotate.
[00:14:42.700 --> 00:14:44.700]   0% on the left,
[00:14:44.700 --> 00:14:46.700]   10% on the right.
[00:14:46.700 --> 00:14:49.500]   And then the accuracy trade-off.
[00:14:49.500 --> 00:14:50.900]   The more the human annotates,
[00:14:50.900 --> 00:14:51.900]   the higher the accuracy.
[00:14:51.900 --> 00:14:53.500]   You approach 100% accuracy.
[00:14:54.200 --> 00:14:55.400]   But you can still do pretty good.
[00:14:55.400 --> 00:14:57.600]   This is for the gaze classification task.
[00:14:57.600 --> 00:14:59.400]   When,
[00:14:59.400 --> 00:15:03.900]   with an 84%,
[00:15:03.900 --> 00:15:05.900]   84 fold,
[00:15:05.900 --> 00:15:08.100]   so almost two orders of magnitude reduction
[00:15:08.100 --> 00:15:09.300]   in human annotation.
[00:15:09.300 --> 00:15:11.200]   This is the future of machine learning.
[00:15:11.200 --> 00:15:13.300]   And hopefully one day,
[00:15:13.300 --> 00:15:15.100]   no human annotation.
[00:15:15.100 --> 00:15:21.100]   And the result
[00:15:21.100 --> 00:15:23.800]   is millions of images like this.
[00:15:24.300 --> 00:15:25.100]   Video frames.
[00:15:25.100 --> 00:15:27.300]   Same thing,
[00:15:27.300 --> 00:15:28.600]   driver frustration.
[00:15:28.600 --> 00:15:30.100]   This is what I was talking about.
[00:15:30.100 --> 00:15:31.600]   The frustrated driver is the one
[00:15:31.600 --> 00:15:32.800]   that's on the bottom.
[00:15:32.800 --> 00:15:36.600]   So a lot of movement of the eyebrows
[00:15:36.600 --> 00:15:37.600]   and a lot of smiling.
[00:15:37.600 --> 00:15:39.800]   And that's true subject after subject.
[00:15:39.800 --> 00:15:42.600]   And the happy, the satisfied,
[00:15:42.600 --> 00:15:43.500]   I don't say happy.
[00:15:43.500 --> 00:15:45.300]   The satisfied driver
[00:15:45.300 --> 00:15:47.400]   is cold and stoic.
[00:15:47.400 --> 00:15:50.000]   And that's true for subject after subject.
[00:15:50.000 --> 00:15:51.800]   Because driving is a boring experience
[00:15:51.800 --> 00:15:53.000]   and you want it to stay that way.
[00:15:53.300 --> 00:15:54.000]   Yes, question.
[00:15:54.000 --> 00:16:00.300]   Great, great, great question.
[00:16:00.300 --> 00:16:01.000]   They're not.
[00:16:01.000 --> 00:16:04.900]   Absolutely, that's a great question.
[00:16:04.900 --> 00:16:06.100]   There is a,
[00:16:06.100 --> 00:16:08.100]   so this is cars owned by MIT.
[00:16:08.100 --> 00:16:10.000]   There is somebody in the back.
[00:16:10.000 --> 00:16:17.500]   So,
[00:16:17.500 --> 00:16:19.200]   the comment was
[00:16:19.200 --> 00:16:21.900]   my emotions might then have nothing to do
[00:16:21.900 --> 00:16:23.200]   with the driving experience.
[00:16:23.900 --> 00:16:25.700]   Yes, let me continue that comment is
[00:16:25.700 --> 00:16:27.700]   your emotions are often,
[00:16:27.700 --> 00:16:31.800]   you're an actor on the stage for others
[00:16:31.800 --> 00:16:32.700]   with your emotion.
[00:16:32.700 --> 00:16:33.600]   So when you're alone,
[00:16:33.600 --> 00:16:35.200]   you might not express emotion.
[00:16:35.200 --> 00:16:37.400]   You're really expressing emotion oftentimes
[00:16:37.400 --> 00:16:38.200]   for others.
[00:16:38.200 --> 00:16:39.700]   Like you're frustrated.
[00:16:39.700 --> 00:16:41.200]   So it's like, oh, what the heck.
[00:16:41.200 --> 00:16:43.200]   That's for the passenger
[00:16:43.200 --> 00:16:44.600]   and that's absolutely right.
[00:16:44.600 --> 00:16:46.900]   So one of the cool things
[00:16:46.900 --> 00:16:49.600]   we're doing,
[00:16:49.600 --> 00:16:50.500]   as I said,
[00:16:50.500 --> 00:16:52.300]   we now have over a billion video frames
[00:16:52.300 --> 00:16:53.000]   in the Tesla.
[00:16:53.300 --> 00:16:56.100]   We're collecting huge amounts of data in the Tesla
[00:16:56.100 --> 00:16:57.200]   and it's,
[00:16:57.200 --> 00:16:59.100]   emotion is complex thing, right?
[00:16:59.100 --> 00:17:00.400]   In this case,
[00:17:00.400 --> 00:17:00.800]   we can,
[00:17:00.800 --> 00:17:01.800]   we know the ground truth
[00:17:01.800 --> 00:17:03.000]   how frustrated they were.
[00:17:03.000 --> 00:17:04.900]   In naturalistic data,
[00:17:04.900 --> 00:17:06.600]   when it's just people driving around,
[00:17:06.600 --> 00:17:07.500]   we don't know
[00:17:07.500 --> 00:17:09.400]   how they're really feeling at the moment.
[00:17:09.400 --> 00:17:11.600]   We're not asking them to like enter in an app.
[00:17:11.600 --> 00:17:12.700]   How are you feeling right now?
[00:17:12.700 --> 00:17:15.900]   But we do know certain things,
[00:17:15.900 --> 00:17:17.900]   like we know that people sing a lot.
[00:17:17.900 --> 00:17:22.200]   That has to be a paper at some point.
[00:17:22.500 --> 00:17:23.200]   It's awesome.
[00:17:23.200 --> 00:17:24.300]   People love singing.
[00:17:24.300 --> 00:17:27.900]   So that doesn't happen in this kind of data
[00:17:27.900 --> 00:17:29.300]   because there's somebody sitting in the car
[00:17:29.300 --> 00:17:31.400]   and I think the expression of a frustration
[00:17:31.400 --> 00:17:32.200]   is also the same.
[00:17:32.200 --> 00:17:43.500]   Yes, so
[00:17:43.500 --> 00:17:46.700]   yeah, the question is,
[00:17:46.700 --> 00:17:48.500]   yeah, so or the comment is that
[00:17:48.500 --> 00:17:49.500]   the data set,
[00:17:49.500 --> 00:17:50.600]   the solo data set
[00:17:50.600 --> 00:17:51.900]   is probably gonna be very different
[00:17:52.200 --> 00:17:54.200]   from a data set that's non-solo
[00:17:54.200 --> 00:17:55.100]   with a passenger.
[00:17:55.100 --> 00:17:56.200]   And it's very true.
[00:17:56.200 --> 00:17:57.800]   The tricky thing about driving,
[00:17:57.800 --> 00:17:59.300]   and this is why it's a huge challenge
[00:17:59.300 --> 00:18:00.500]   for self-driving cars,
[00:18:00.500 --> 00:18:02.200]   for the external facing sensors
[00:18:02.200 --> 00:18:03.800]   and for the internal facing sensors
[00:18:03.800 --> 00:18:05.100]   analyzing human behavior,
[00:18:05.100 --> 00:18:08.500]   is like 99.9% of driving
[00:18:08.500 --> 00:18:09.500]   is the same thing.
[00:18:09.500 --> 00:18:10.900]   It's really boring.
[00:18:10.900 --> 00:18:12.600]   So finding the interesting bits
[00:18:12.600 --> 00:18:14.000]   is actually pretty complicated.
[00:18:14.000 --> 00:18:16.400]   So that has to do with emotion.
[00:18:16.400 --> 00:18:18.100]   That has to do with,
[00:18:18.100 --> 00:18:20.100]   so singing is easy to find.
[00:18:20.100 --> 00:18:22.100]   So we can track the mouth pretty well.
[00:18:22.100 --> 00:18:23.400]   So whenever you're talking or singing,
[00:18:23.400 --> 00:18:24.200]   we can find that.
[00:18:24.200 --> 00:18:25.300]   But how do you find
[00:18:25.300 --> 00:18:26.800]   the subtle expressions of emotion?
[00:18:26.800 --> 00:18:27.700]   It's hard
[00:18:27.700 --> 00:18:30.500]   when you're solo.
[00:18:30.500 --> 00:18:33.500]   And cognitive load,
[00:18:33.500 --> 00:18:35.400]   that's
[00:18:35.400 --> 00:18:38.500]   that's a fascinating thing.
[00:18:38.500 --> 00:18:39.500]   I mean, similar to emotion,
[00:18:39.500 --> 00:18:42.000]   it's a little more concrete
[00:18:42.000 --> 00:18:44.800]   in the sense that there's a lot of good science
[00:18:44.800 --> 00:18:46.600]   and ways to measure cognitive load,
[00:18:46.600 --> 00:18:48.500]   cognitive workload,
[00:18:48.500 --> 00:18:50.100]   how occupied your mind is,
[00:18:51.100 --> 00:18:52.900]   mental workload is another term used.
[00:18:52.900 --> 00:18:55.700]   And so the window to the soul,
[00:18:55.700 --> 00:18:58.400]   the cognitive workload soul
[00:18:58.400 --> 00:18:59.400]   is the eyes.
[00:18:59.400 --> 00:19:01.200]   So pupil,
[00:19:01.200 --> 00:19:02.600]   so first of all,
[00:19:02.600 --> 00:19:04.300]   the eyes move in two different ways.
[00:19:04.300 --> 00:19:05.400]   They move a lot of ways,
[00:19:05.400 --> 00:19:06.300]   but two major ways.
[00:19:06.300 --> 00:19:07.500]   It's saccades,
[00:19:07.500 --> 00:19:08.900]   these are these ballistic movements,
[00:19:08.900 --> 00:19:09.700]   they jump around.
[00:19:09.700 --> 00:19:11.800]   Whenever you look around the room,
[00:19:11.800 --> 00:19:13.800]   they're actually just jumping around.
[00:19:13.800 --> 00:19:14.400]   When you read,
[00:19:14.400 --> 00:19:15.700]   their eyes are jumping around.
[00:19:15.700 --> 00:19:17.000]   And when
[00:19:17.000 --> 00:19:18.900]   you follow,
[00:19:18.900 --> 00:19:20.800]   you just follow this bottle with your eyes.
[00:19:21.300 --> 00:19:23.400]   That your eyes are actually gonna move smoothly,
[00:19:23.400 --> 00:19:24.900]   smooth pursuit.
[00:19:24.900 --> 00:19:26.500]   Somebody actually just told me today
[00:19:26.500 --> 00:19:28.700]   that probably has to do with our hunting background
[00:19:28.700 --> 00:19:30.100]   or as animals.
[00:19:30.100 --> 00:19:34.100]   I don't know how that helps,
[00:19:34.100 --> 00:19:36.100]   like frogs track flies
[00:19:36.100 --> 00:19:37.100]   really well.
[00:19:37.100 --> 00:19:38.700]   So that you have to like,
[00:19:38.700 --> 00:19:39.200]   I don't know.
[00:19:39.200 --> 00:19:40.900]   Anyway, the point is
[00:19:40.900 --> 00:19:42.800]   there's smooth pursuit movements
[00:19:42.800 --> 00:19:44.600]   where the eyes move smoothly.
[00:19:44.600 --> 00:19:46.600]   And those are all indications
[00:19:46.600 --> 00:19:48.600]   of certain aspects of cognitive load.
[00:19:49.000 --> 00:19:51.300]   And then there is very subtle movements
[00:19:51.300 --> 00:19:53.600]   which are almost imperceptible for computer vision.
[00:19:53.600 --> 00:19:54.700]   And these are
[00:19:54.700 --> 00:19:56.600]   micro saccades,
[00:19:56.600 --> 00:19:57.900]   these are tremors of the eye.
[00:19:57.900 --> 00:20:00.200]   Here,
[00:20:00.200 --> 00:20:02.100]   work from here from Bill Freeman
[00:20:02.100 --> 00:20:04.200]   magnifying those subtle movements.
[00:20:04.200 --> 00:20:05.600]   These are taken at
[00:20:05.600 --> 00:20:07.500]   500 frames a second.
[00:20:07.500 --> 00:20:14.200]   And so cognitive load,
[00:20:14.200 --> 00:20:17.200]   when the pupil,
[00:20:17.200 --> 00:20:18.700]   that black dot in the middle,
[00:20:18.800 --> 00:20:20.600]   just in case we don't know what a pupil is,
[00:20:20.600 --> 00:20:21.600]   in the middle of the eye.
[00:20:21.600 --> 00:20:23.100]   When it gets larger,
[00:20:23.100 --> 00:20:25.500]   that's an indicator of high cognitive load.
[00:20:25.500 --> 00:20:26.900]   But it also gets
[00:20:26.900 --> 00:20:28.700]   larger when the light is dim.
[00:20:28.700 --> 00:20:31.200]   So there's like this complex interplay.
[00:20:31.200 --> 00:20:32.800]   So we can't rely in the wild,
[00:20:32.800 --> 00:20:33.500]   outside,
[00:20:33.500 --> 00:20:34.500]   in the car,
[00:20:34.500 --> 00:20:36.400]   or just in general outdoors
[00:20:36.400 --> 00:20:38.500]   on using the pupil size.
[00:20:38.500 --> 00:20:40.800]   Even though pupil size has been used effectively
[00:20:40.800 --> 00:20:42.500]   in a lab to measure cognitive load,
[00:20:42.500 --> 00:20:44.700]   it can't be reliably used in the car.
[00:20:44.700 --> 00:20:46.100]   And the same with blinks.
[00:20:47.900 --> 00:20:49.600]   When there's higher cognitive load,
[00:20:49.600 --> 00:20:51.200]   your blink rate decreases
[00:20:51.200 --> 00:20:52.800]   and your blink duration shortens.
[00:20:52.800 --> 00:20:54.500]   Okay.
[00:20:54.500 --> 00:20:58.000]   I think I'm just repeating the same thing over and over.
[00:20:58.000 --> 00:20:59.800]   But you can imagine
[00:20:59.800 --> 00:21:01.700]   how we can predict cognitive load, right?
[00:21:01.700 --> 00:21:04.200]   We extract
[00:21:04.200 --> 00:21:05.800]   video of the eye.
[00:21:05.800 --> 00:21:08.400]   Here is
[00:21:08.400 --> 00:21:10.500]   the primary eye
[00:21:10.500 --> 00:21:13.100]   of the person the system is observing.
[00:21:13.100 --> 00:21:16.600]   Happens to be the same TA once again.
[00:21:17.400 --> 00:21:19.400]   We take the sequence of a hundred,
[00:21:19.400 --> 00:21:20.900]   oh, it's 90 images.
[00:21:20.900 --> 00:21:21.900]   So that's six seconds,
[00:21:21.900 --> 00:21:23.400]   16 frames a second,
[00:21:23.400 --> 00:21:24.700]   15 frames a second.
[00:21:24.700 --> 00:21:27.900]   And we dump that into a 3D convolutional neural network.
[00:21:27.900 --> 00:21:29.000]   Again,
[00:21:29.000 --> 00:21:31.200]   that means it's 90 channels
[00:21:31.200 --> 00:21:33.700]   of,
[00:21:33.700 --> 00:21:35.400]   it's not 90 frames,
[00:21:35.400 --> 00:21:36.200]   grayscale.
[00:21:36.200 --> 00:21:38.100]   And then the prediction is
[00:21:38.100 --> 00:21:39.100]   one of three
[00:21:39.100 --> 00:21:41.400]   classes of cognitive load
[00:21:41.400 --> 00:21:42.900]   is the same.
[00:21:42.900 --> 00:21:43.900]   So we can predict
[00:21:43.900 --> 00:21:44.900]   the same thing.
[00:21:44.900 --> 00:21:46.900]   Classes of cognitive load.
[00:21:46.900 --> 00:21:48.500]   Low cognitive load,
[00:21:48.500 --> 00:21:49.900]   medium cognitive load,
[00:21:49.900 --> 00:21:50.900]   and high cognitive load.
[00:21:50.900 --> 00:21:52.400]   And there's ground truth for that
[00:21:52.400 --> 00:21:53.700]   because we have people,
[00:21:53.700 --> 00:21:55.600]   over 500 different people
[00:21:55.600 --> 00:21:56.700]   do different tasks
[00:21:56.700 --> 00:21:58.300]   of various cognitive load.
[00:21:58.300 --> 00:22:01.500]   And after some frontalization again,
[00:22:01.500 --> 00:22:02.800]   where you see the eyes are,
[00:22:02.800 --> 00:22:05.300]   no matter where the person is looking,
[00:22:05.300 --> 00:22:09.300]   the image of the face is transposed in such a way that
[00:22:09.300 --> 00:22:11.000]   the eyes, the corners of the eyes
[00:22:11.000 --> 00:22:12.400]   remain always in the same position.
[00:22:13.800 --> 00:22:15.200]   After the frontalization,
[00:22:15.200 --> 00:22:18.200]   we find the eye,
[00:22:18.200 --> 00:22:20.100]   active appearance models,
[00:22:20.100 --> 00:22:22.000]   find 39 points
[00:22:22.000 --> 00:22:25.600]   of the eye, of the eyelids,
[00:22:25.600 --> 00:22:27.400]   the iris,
[00:22:27.400 --> 00:22:29.400]   and four points on the pupil.
[00:22:29.400 --> 00:22:34.200]   Putting all of that
[00:22:34.200 --> 00:22:35.900]   into a 3D CNN model,
[00:22:35.900 --> 00:22:37.400]   they're positioned,
[00:22:37.400 --> 00:22:39.000]   image eye sequence on the left,
[00:22:39.000 --> 00:22:40.900]   3D CNN model in the middle,
[00:22:40.900 --> 00:22:42.800]   cognitive load prediction on the right.
[00:22:43.600 --> 00:22:44.700]   This code, by the way, is
[00:22:44.700 --> 00:22:47.600]   freely available online.
[00:22:47.600 --> 00:22:51.000]   All you have to do,
[00:22:51.000 --> 00:22:52.500]   dump a webcam
[00:22:52.500 --> 00:22:55.700]   from the video stream,
[00:22:55.700 --> 00:22:58.300]   CNN runs in faster than real time,
[00:22:58.300 --> 00:22:59.600]   predicts cognitive load.
[00:22:59.600 --> 00:23:03.800]   Same process as detecting the identity of the face,
[00:23:03.800 --> 00:23:06.600]   same process as detecting where the driver is looking,
[00:23:06.600 --> 00:23:08.600]   same process as detecting emotion.
[00:23:08.600 --> 00:23:12.400]   And all of those require very little hyperparameter tuning
[00:23:12.400 --> 00:23:13.700]   on the convolutional neural networks.
[00:23:13.700 --> 00:23:16.400]   They only require
[00:23:16.400 --> 00:23:18.400]   huge amounts of data.
[00:23:18.400 --> 00:23:21.300]   And why do we care
[00:23:21.300 --> 00:23:24.100]   about detecting what the driver is doing?
[00:23:24.100 --> 00:23:26.200]   And I think Eric has mentioned
[00:23:26.200 --> 00:23:28.200]   this.
[00:23:28.200 --> 00:23:29.000]   Is
[00:23:29.000 --> 00:23:31.100]   on the,
[00:23:31.100 --> 00:23:34.000]   oh man, this is the comeback of the slide.
[00:23:34.000 --> 00:23:42.000]   I was criticized for this being a very cheesy slide.
[00:23:43.000 --> 00:23:44.000]   In
[00:23:44.000 --> 00:23:46.000]   in the
[00:23:46.000 --> 00:23:48.000]   path towards
[00:23:48.000 --> 00:23:49.700]   full automation,
[00:23:49.700 --> 00:23:55.500]   we're likely to take gradual steps towards that.
[00:23:55.500 --> 00:23:59.600]   It's enough of that. This is better.
[00:23:59.600 --> 00:24:01.700]   And
[00:24:01.700 --> 00:24:03.100]   especially
[00:24:03.100 --> 00:24:04.300]   given
[00:24:04.300 --> 00:24:05.600]   that
[00:24:05.600 --> 00:24:07.600]   this is
[00:24:07.600 --> 00:24:09.300]   given today,
[00:24:09.300 --> 00:24:10.800]   our new president,
[00:24:11.800 --> 00:24:13.700]   this is pickup truck country.
[00:24:13.700 --> 00:24:20.500]   This is manually controlled vehicle country
[00:24:20.500 --> 00:24:21.700]   for quite a little while.
[00:24:21.700 --> 00:24:22.900]   Will I control?
[00:24:22.900 --> 00:24:25.400]   And control
[00:24:25.400 --> 00:24:29.700]   being given to somebody else, to the machine,
[00:24:29.700 --> 00:24:31.200]   will be a gradual process.
[00:24:31.200 --> 00:24:34.300]   It's a gradual process of that machine earning trust.
[00:24:34.300 --> 00:24:36.500]   And through that process,
[00:24:36.500 --> 00:24:37.700]   the machine,
[00:24:37.700 --> 00:24:39.000]   like the Tesla,
[00:24:39.000 --> 00:24:40.700]   like the BMW,
[00:24:40.900 --> 00:24:42.700]   like the Mercedes, the Volvo,
[00:24:42.700 --> 00:24:45.100]   that's now playing with these ideas,
[00:24:45.100 --> 00:24:48.400]   is going to need to see what the human is doing.
[00:24:48.400 --> 00:24:54.400]   And for that,
[00:24:54.400 --> 00:24:57.800]   to see what the human is doing,
[00:24:57.800 --> 00:24:59.500]   we have
[00:24:59.500 --> 00:25:03.600]   billions of miles of forward-facing data.
[00:25:03.600 --> 00:25:05.000]   What we need
[00:25:05.000 --> 00:25:08.600]   is billions of miles of driver-facing data as well.
[00:25:09.800 --> 00:25:11.500]   We're in the process of collecting that.
[00:25:11.500 --> 00:25:13.800]   And this is a pitch
[00:25:13.800 --> 00:25:16.200]   for automakers
[00:25:16.200 --> 00:25:18.100]   and everybody to
[00:25:18.100 --> 00:25:21.400]   buy cars that have a driver-facing camera.
[00:25:21.400 --> 00:25:25.600]   And let me
[00:25:25.600 --> 00:25:26.600]   sort of
[00:25:26.600 --> 00:25:28.400]   close.
[00:25:28.400 --> 00:25:30.200]   So I said we need a lot of data.
[00:25:30.200 --> 00:25:34.400]   But I think this class has been
[00:25:34.400 --> 00:25:37.300]   and through your own
[00:25:37.800 --> 00:25:42.500]   research, you'll find that we're in the very early stages of
[00:25:42.500 --> 00:25:47.100]   discovering the power of deep learning.
[00:25:47.100 --> 00:25:49.900]   For example,
[00:25:49.900 --> 00:25:51.600]   you know, as recently,
[00:25:51.600 --> 00:25:54.000]   like Jan LeCun said,
[00:25:54.000 --> 00:25:57.900]   that it seems
[00:25:57.900 --> 00:26:00.700]   that the deeper the network,
[00:26:00.700 --> 00:26:02.300]   the better the results
[00:26:02.300 --> 00:26:04.100]   in a lot of really
[00:26:04.100 --> 00:26:05.900]   important cases.
[00:26:07.400 --> 00:26:09.600]   Even though the data is not increasing.
[00:26:09.600 --> 00:26:13.700]   So why does a deeper network give better results?
[00:26:13.700 --> 00:26:16.900]   This is a mysterious thing we don't understand.
[00:26:16.900 --> 00:26:20.000]   There's these hundreds of millions of parameters
[00:26:20.000 --> 00:26:23.800]   and from them is emerging some kind of
[00:26:23.800 --> 00:26:26.800]   structure, some kind of representation of
[00:26:26.800 --> 00:26:28.400]   the knowledge that we're giving it.
[00:26:28.400 --> 00:26:32.800]   One of my favorite examples of this emergent concept
[00:26:32.800 --> 00:26:35.100]   is Conway's Game of Life.
[00:26:36.100 --> 00:26:38.700]   For those of you who know what this is,
[00:26:38.700 --> 00:26:43.300]   will probably criticize me for it being as cheesy as a stairway slide.
[00:26:43.300 --> 00:26:46.700]   But I think it's actually such a simple
[00:26:46.700 --> 00:26:48.900]   and brilliant example
[00:26:48.900 --> 00:26:50.400]   of how
[00:26:50.400 --> 00:26:54.800]   like a neuron in a neural network is a really simple computational unit.
[00:26:54.800 --> 00:26:59.400]   And then incredible power emerges when you just combine a lot of them in a network.
[00:26:59.400 --> 00:27:01.000]   In the same way,
[00:27:01.000 --> 00:27:03.900]   this is called a super-computational network.
[00:27:04.400 --> 00:27:06.300]   This is called a cellular automata.
[00:27:06.300 --> 00:27:09.000]   That's a weird pronunciation.
[00:27:09.000 --> 00:27:15.100]   And every single cell is operating under a simple rule.
[00:27:15.100 --> 00:27:18.100]   You can think of it as a cell living and dying.
[00:27:18.100 --> 00:27:22.000]   It's filled in black when it's alive
[00:27:22.000 --> 00:27:23.500]   and white when it's dead.
[00:27:23.500 --> 00:27:26.800]   And when it has two or three, if it's alive
[00:27:26.800 --> 00:27:29.000]   and has two or three neighbors,
[00:27:29.000 --> 00:27:31.400]   it survives to the next time slot.
[00:27:31.400 --> 00:27:34.000]   Otherwise it dies.
[00:27:35.000 --> 00:27:39.300]   And if it has exactly three neighbors, it's dead.
[00:27:39.300 --> 00:27:42.000]   It comes back to life.
[00:27:42.000 --> 00:27:43.500]   If it has exactly three neighbors.
[00:27:43.500 --> 00:27:44.600]   That's a simple rule.
[00:27:44.600 --> 00:27:46.800]   Whatever, you can just imagine.
[00:27:46.800 --> 00:27:47.400]   It's just simple.
[00:27:47.400 --> 00:27:51.800]   All it's doing is operating under this very local process.
[00:27:51.800 --> 00:27:53.300]   Same as a neuron.
[00:27:53.300 --> 00:27:58.700]   Or in the way we're currently training neural networks
[00:27:58.700 --> 00:28:01.400]   in this local gradient.
[00:28:01.400 --> 00:28:03.500]   We're optimizing over a local gradient.
[00:28:03.900 --> 00:28:05.600]   Same, local rules.
[00:28:05.600 --> 00:28:09.300]   And what happens if you run this system
[00:28:09.300 --> 00:28:12.600]   operating under really local rules,
[00:28:12.600 --> 00:28:13.900]   when you get on the right,
[00:28:13.900 --> 00:28:17.800]   it's not, again, you have to go home.
[00:28:17.800 --> 00:28:20.600]   Hopefully no drugs involved.
[00:28:20.600 --> 00:28:22.900]   But you have to open up your mind
[00:28:22.900 --> 00:28:27.500]   and see how amazing that is.
[00:28:27.500 --> 00:28:29.100]   Because what happens is
[00:28:29.100 --> 00:28:32.100]   it's a local computational unit
[00:28:32.800 --> 00:28:35.000]   that knows very little about the world.
[00:28:35.000 --> 00:28:38.800]   But somehow really complex patterns emerge.
[00:28:38.800 --> 00:28:41.000]   And we don't understand why.
[00:28:41.000 --> 00:28:43.900]   In fact, under different rules,
[00:28:43.900 --> 00:28:45.600]   incredible patterns emerge.
[00:28:45.600 --> 00:28:47.700]   And it feels like it's living creatures,
[00:28:47.700 --> 00:28:49.100]   like communicating.
[00:28:49.100 --> 00:28:50.900]   Like when you just watch it.
[00:28:50.900 --> 00:28:52.700]   Not these examples.
[00:28:52.700 --> 00:28:54.900]   This is the original.
[00:28:54.900 --> 00:28:57.700]   They get like complex and interesting.
[00:28:57.700 --> 00:28:58.900]   But even these examples,
[00:28:58.900 --> 00:29:00.800]   this complex geometric patterns that emerge,
[00:29:00.800 --> 00:29:01.700]   it's incredible.
[00:29:01.700 --> 00:29:02.700]   We don't understand why.
[00:29:03.100 --> 00:29:04.000]   Same with neural networks.
[00:29:04.000 --> 00:29:05.000]   We don't understand why.
[00:29:05.000 --> 00:29:06.000]   And we need to.
[00:29:06.000 --> 00:29:07.500]   In order to see how these networks
[00:29:07.500 --> 00:29:08.600]   will be able to reason.
[00:29:08.600 --> 00:29:11.200]   Okay, so what's next?
[00:29:11.200 --> 00:29:16.700]   I encourage you to read the deep learning book.
[00:29:16.700 --> 00:29:20.700]   It's available online, deeplearningbook.org.
[00:29:20.700 --> 00:29:23.200]   As I mentioned to a few people,
[00:29:23.200 --> 00:29:25.000]   you should, well, first,
[00:29:25.000 --> 00:29:26.400]   there's a ton of amazing papers
[00:29:26.400 --> 00:29:28.000]   every day coming out on archive.
[00:29:28.000 --> 00:29:31.000]   I'll put these links up,
[00:29:31.100 --> 00:29:33.400]   but there's a lot of good collections
[00:29:33.400 --> 00:29:35.600]   of strong paper, list of papers.
[00:29:35.600 --> 00:29:38.200]   There is the literally awesome list,
[00:29:38.200 --> 00:29:41.000]   the awesome deep learning papers on GitHub.
[00:29:41.000 --> 00:29:43.300]   It's calling itself awesome,
[00:29:43.300 --> 00:29:44.800]   but it happens to be awesome.
[00:29:44.800 --> 00:29:47.900]   And there is a lot of blogs,
[00:29:47.900 --> 00:29:49.300]   that are just amazing.
[00:29:49.300 --> 00:29:53.400]   That's how I recommend you learn machine learning,
[00:29:53.400 --> 00:29:54.400]   is on blogs.
[00:29:54.400 --> 00:29:57.600]   And if you're interested
[00:29:57.600 --> 00:29:59.500]   in the application of deep learning
[00:29:59.500 --> 00:30:00.600]   in the automotive space,
[00:30:01.100 --> 00:30:03.700]   you can come do research in our group.
[00:30:03.700 --> 00:30:05.300]   Just email me.
[00:30:05.300 --> 00:30:09.300]   Anyway, we have three winners.
[00:30:09.300 --> 00:30:15.400]   Jeffrey Hu, Michael Gump.
[00:30:15.400 --> 00:30:19.400]   And how do you, are you here?
[00:30:19.400 --> 00:30:22.800]   Hey, how do you say your name?
[00:30:22.800 --> 00:30:25.800]   No, this is not my name.
[00:30:25.800 --> 00:30:29.300]   All right.
[00:30:29.300 --> 00:30:31.500]   This is, so my name is Forna.
[00:30:31.500 --> 00:30:32.500]   Forna.
[00:30:32.500 --> 00:30:33.700]   He stands for Forna.
[00:30:33.700 --> 00:30:35.000]   And Doli is,
[00:30:35.000 --> 00:30:36.900]   Oh, I see.
[00:30:36.900 --> 00:30:39.900]   They, like, they, they, I'm sorry, I'm not,
[00:30:39.900 --> 00:30:44.600]   Well, anyway, here.
[00:30:55.700 --> 00:31:01.300]   So he achieved the stunning speed of,
[00:31:01.300 --> 00:31:04.100]   so I, this is kind of incredible.
[00:31:04.100 --> 00:31:05.200]   So I didn't know what kind of speed
[00:31:05.200 --> 00:31:06.400]   we're going to be able to achieve.
[00:31:06.400 --> 00:31:08.200]   I thought 73 was unbeatable,
[00:31:08.200 --> 00:31:09.800]   because we played with it for a while.
[00:31:09.800 --> 00:31:11.100]   We couldn't achieve 73.
[00:31:11.100 --> 00:31:13.200]   We designed a deterministic algorithm,
[00:31:13.200 --> 00:31:15.200]   that was able to achieve 74, I believe.
[00:31:15.200 --> 00:31:17.900]   Meaning, like, it's cheating,
[00:31:17.900 --> 00:31:20.200]   with a cheating algorithm that got 74.
[00:31:20.200 --> 00:31:22.900]   And so folks have come up
[00:31:24.100 --> 00:31:26.000]   with algorithms that have done,
[00:31:26.000 --> 00:31:28.600]   that have beaten 73 and then 74.
[00:31:28.600 --> 00:31:29.700]   So this is really incredible.
[00:31:29.700 --> 00:31:32.300]   And the other two guys,
[00:31:32.300 --> 00:31:34.800]   so all three of you get a free term
[00:31:34.800 --> 00:31:37.800]   at the Udacity Self-Driving Car Engineering degree.
[00:31:37.800 --> 00:31:40.300]   Thanks to those guys for giving it,
[00:31:40.300 --> 00:31:42.100]   giving that award,
[00:31:42.100 --> 00:31:44.500]   and bringing their army of brilliant,
[00:31:44.500 --> 00:31:47.100]   so they have people who are obsessed
[00:31:47.100 --> 00:31:48.200]   about self-driving cars.
[00:31:48.200 --> 00:31:52.800]   And we've received over 2,000 submissions
[00:31:53.400 --> 00:31:54.500]   for this competition.
[00:31:54.500 --> 00:31:56.800]   A lot of them from those guys.
[00:31:56.800 --> 00:31:58.100]   And they're just brilliant.
[00:31:58.100 --> 00:32:01.900]   So it's really exciting
[00:32:01.900 --> 00:32:04.100]   to have such a big community of deep learning folks
[00:32:04.100 --> 00:32:05.800]   working in this field.
[00:32:05.800 --> 00:32:09.100]   So this is, for the rest of eternity,
[00:32:09.100 --> 00:32:11.600]   we're going to change this up a little bit,
[00:32:11.600 --> 00:32:15.400]   but this is actually the three neural networks,
[00:32:15.400 --> 00:32:18.500]   the three winning neural networks
[00:32:18.500 --> 00:32:19.800]   running side by side.
[00:32:19.800 --> 00:32:22.500]   And you can see the number of cars passed there.
[00:32:22.800 --> 00:32:24.200]   The first place is on the left,
[00:32:24.200 --> 00:32:26.600]   second place and third place.
[00:32:26.600 --> 00:32:29.600]   And in fact, the third place is almost,
[00:32:29.600 --> 00:32:32.300]   wait, no, second place is winning currently.
[00:32:32.300 --> 00:32:35.200]   But that just tells you that
[00:32:35.200 --> 00:32:40.400]   the random nature of competition,
[00:32:40.400 --> 00:32:42.600]   sometimes you win, sometimes you lose.
[00:32:42.600 --> 00:32:46.400]   So there's a,
[00:32:46.400 --> 00:32:49.600]   the actual evaluation process runs through a lot
[00:32:50.000 --> 00:32:53.900]   of a lot of iterations and takes the medium evaluation.
[00:32:53.900 --> 00:32:59.400]   With that, let me thank you guys so much for,
[00:32:59.400 --> 00:33:01.100]   well, wait, wait, wait, you have a question?
[00:33:01.100 --> 00:33:03.600]   Do I have to be winning networks at all?
[00:33:03.600 --> 00:33:06.200]   Yeah, so,
[00:33:06.200 --> 00:33:10.800]   all three guys wrote me a note
[00:33:10.800 --> 00:33:13.100]   about how their networks work.
[00:33:13.100 --> 00:33:15.100]   I did not read that note.
[00:33:15.100 --> 00:33:18.300]   So I'll post,
[00:33:18.300 --> 00:33:20.500]   this tells you how crazy this has been.
[00:33:20.500 --> 00:33:24.100]   I'll post their winning networks
[00:33:24.100 --> 00:33:27.400]   online.
[00:33:27.400 --> 00:33:30.300]   And I encourage you to continue competing
[00:33:30.300 --> 00:33:32.400]   and continue submitting networks.
[00:33:32.400 --> 00:33:35.900]   This will run for a while and we're working on a journal paper
[00:33:35.900 --> 00:33:40.100]   for this game.
[00:33:40.100 --> 00:33:43.100]   We're trying to find the optimal solutions.
[00:33:43.100 --> 00:33:45.900]   Okay, so this is the first time
[00:33:45.900 --> 00:33:47.500]   I've ever taught a class.
[00:33:48.200 --> 00:33:51.500]   And the first time obviously teaching this class.
[00:33:51.500 --> 00:33:54.500]   And so thank you so much for being a part of it.
[00:33:54.500 --> 00:33:59.900]   Thank you, Eric.
[00:33:59.900 --> 00:34:06.800]   If you didn't get a shirt, please come back,
[00:34:06.800 --> 00:34:08.500]   please come down and get a shirt.
[00:34:08.500 --> 00:34:11.500]   Just write your email on the note,
[00:34:11.500 --> 00:34:13.100]   on the index note.
[00:34:13.100 --> 00:34:15.300]   Thank you.
[00:34:15.600 --> 00:34:16.100]   Bye.
[00:34:16.900 --> 00:34:17.400]   Thank you.
[00:34:17.700 --> 00:34:18.200]   Thank you.
[00:34:18.200 --> 00:34:32.200]   [audience chatter]

