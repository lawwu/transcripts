
[00:00:00.000 --> 00:00:29.980]   All right, good afternoon everybody. Thank you all so much for joining us. We have the enviable position of being after lunch. I'm seeing some cookies on the table still, but thankfully you're not here to listen to me. You're going to be riveted by the prompt doctor who's going to come up in a second.
[00:00:29.980 --> 00:00:38.420]   So I'm expecting no sleeping on the table, but you never know. Excited to be here. I'm Jamie Neuwirth. I lead our startup team at Anthropik.
[00:00:38.420 --> 00:00:44.000]   I just wanted to say a couple of quick things before we got going here with, again, the reason you're here, the prompt doctor.
[00:00:44.000 --> 00:00:55.380]   We've had a lot of really exciting releases just in the last couple of days, some in the last couple of hours, and wanted to just put these up there to highlight some of the cool things that we're doing,
[00:00:55.380 --> 00:01:06.680]   but also share how a lot of folks, not only in this room, some of your peers, maybe folks back at the office, can work with Anthropik on not only some of the prompting, of course, work we're going to do here,
[00:01:06.680 --> 00:01:15.060]   but just helping you grow your business and the really cool things you guys are building with Claude, with LOMs, on top of AI.
[00:01:15.540 --> 00:01:26.180]   My team is here specifically to help your company grow and scale the business, whether that's getting access to higher rate limits, getting access to folks like Zach, who's going to be up here in a moment,
[00:01:26.180 --> 00:01:29.280]   learning more about what we're doing from an early access perspective.
[00:01:29.280 --> 00:01:35.500]   We want to work with you and empower the next wave of really exciting AI companies built on top of Claude.
[00:01:35.900 --> 00:01:39.660]   And we're helping from a product perspective with a couple of the releases you see here.
[00:01:39.660 --> 00:01:41.720]   Has anyone tried 3.5 Sonnet yet?
[00:01:41.720 --> 00:01:43.320]   Love it.
[00:01:43.320 --> 00:01:44.040]   Very cool.
[00:01:44.040 --> 00:01:44.780]   Really excited.
[00:01:44.780 --> 00:01:47.420]   Yeah, thankfully, Zach has tried it as well, so we're in a good place.
[00:01:47.420 --> 00:01:51.080]   Really excited about what we were able to release there as well.
[00:01:51.080 --> 00:01:57.260]   Also, just today, or excuse me, I guess Artifacts came out with that with our Claude Teams and Claude AI plan.
[00:01:57.800 --> 00:02:04.900]   Artifacts is this really cool tool that some have been playing around with, making video games with a couple lines of text that turns into code,
[00:02:04.900 --> 00:02:08.600]   but actually a lot of really cool business use cases from a diagram perspective.
[00:02:08.600 --> 00:02:12.080]   I've seen a lot of cool pharma companies using this and thinking out,
[00:02:12.080 --> 00:02:19.320]   how can I put what's in my head on a gene sequencing kind of discussion onto something like an artifact, share that with my peers.
[00:02:19.320 --> 00:02:23.000]   All the way to prompting cookbooks, helping, again, folks like yourself.
[00:02:23.100 --> 00:02:29.780]   I would imagine a lot of you hopefully will be featured on the website, you know, coming up with what you'll be able to prompt for these use cases in your company.
[00:02:29.780 --> 00:02:35.520]   So just some ways that we've, you know, things that we've come out with over the last couple of days,
[00:02:35.520 --> 00:02:39.260]   a couple of hours when you think of Claude Teams and projects, and then ways to connect.
[00:02:39.260 --> 00:02:42.960]   Feel free to reach out at sales, sales at anthropic.com.
[00:02:42.960 --> 00:02:45.520]   We have our head of DevRel, Alex, here as well.
[00:02:45.520 --> 00:02:48.220]   So we really love this community, love staying engaged.
[00:02:48.440 --> 00:02:54.060]   We're really excited about what we've released over the last couple of days to be able to help you again in what you guys are building.
[00:02:54.060 --> 00:03:00.480]   And so I'm going to leave all that to the prompt doctor as well to get that, to really make sure that we can help.
[00:03:00.480 --> 00:03:03.600]   And Zach, why don't you come on up here and see what we can do.
[00:03:03.600 --> 00:03:05.460]   Thank you all so much for joining us today.
[00:03:05.460 --> 00:03:06.120]   We're really excited.
[00:03:13.320 --> 00:03:15.920]   Thanks, Jamie, for the intro, and thank you all for coming.
[00:03:15.920 --> 00:03:16.980]   This is really awesome.
[00:03:16.980 --> 00:03:18.740]   I had no idea this many people were going to be here.
[00:03:18.740 --> 00:03:20.020]   So thanks.
[00:03:20.020 --> 00:03:21.000]   Okay.
[00:03:21.000 --> 00:03:23.080]   So not going to be much talk.
[00:03:23.080 --> 00:03:25.680]   It's mostly just going to be straight up prompting from beginning to end.
[00:03:25.680 --> 00:03:27.460]   Did make a couple slides.
[00:03:27.460 --> 00:03:29.460]   So mostly about what to bring.
[00:03:29.460 --> 00:03:36.460]   So I set up a Slack channel in this, the AI engineer Slack.
[00:03:36.460 --> 00:03:39.400]   It's called PromptEng Live Workshop Anthropic.
[00:03:41.200 --> 00:03:43.060]   So that's where you can upload prompts.
[00:03:43.060 --> 00:03:46.940]   And what we're going to do on stage is I'm just going to sort of look at them.
[00:03:46.940 --> 00:03:49.000]   I'm going to read them.
[00:03:49.000 --> 00:03:52.120]   We're going to test them in the console, the Anthropic console.
[00:03:52.120 --> 00:03:54.500]   We're going to see if we can get better results.
[00:03:54.500 --> 00:03:57.920]   And we'll just sort of like try to learn as we go.
[00:03:57.920 --> 00:04:02.740]   So this is something that I do like internally in our team Slack quite a bit.
[00:04:02.740 --> 00:04:04.940]   But I've never done it in front of this many people.
[00:04:04.940 --> 00:04:07.320]   And it'll be exciting.
[00:04:07.320 --> 00:04:08.240]   It'll be fun.
[00:04:08.240 --> 00:04:09.880]   Might be some hiccups along the way.
[00:04:10.000 --> 00:04:12.560]   But hopefully you all have a good time too and maybe learn something.
[00:04:12.560 --> 00:04:13.660]   I know I'll definitely learn something.
[00:04:13.660 --> 00:04:18.240]   So what kind of things should you put in this Slack channel?
[00:04:18.240 --> 00:04:20.900]   So you can put a prompt template.
[00:04:20.900 --> 00:04:23.880]   So a prompt template is kind of like a prompt.
[00:04:23.880 --> 00:04:26.520]   Actually, I just realized I don't even need this mic.
[00:04:26.520 --> 00:04:28.260]   Okay.
[00:04:28.260 --> 00:04:34.200]   So you put a prompt template, which is like a prompt, but with spaces where the variables are going
[00:04:34.200 --> 00:04:39.160]   to go and the variables, they're going to be denoted with these double brackets.
[00:04:39.160 --> 00:04:41.620]   So in this case, it's like this document part.
[00:04:41.620 --> 00:04:45.440]   If you don't have it in this format, that's fine.
[00:04:45.440 --> 00:04:46.220]   We can figure it out.
[00:04:46.220 --> 00:04:47.160]   This is just like the ideal.
[00:04:47.160 --> 00:04:49.840]   So this is like the prompt template.
[00:04:49.840 --> 00:04:51.180]   This is like the kind of thing you'd put there.
[00:04:51.800 --> 00:04:55.980]   And then you can also have a couple examples of cases where it doesn't do what you want.
[00:04:55.980 --> 00:05:01.740]   And that will give us the like direction as far as like where we want to go with it.
[00:05:01.740 --> 00:05:09.560]   I might also like ask you questions out loud if I have questions about like what kind of output
[00:05:09.560 --> 00:05:13.980]   is good or not, or I might ask questions in Slack either way because it's easier.
[00:05:13.980 --> 00:05:16.580]   We'll have to kind of figure that out as we go.
[00:05:16.580 --> 00:05:17.900]   Okay.
[00:05:17.900 --> 00:05:22.320]   So that being said, we're going to use the console for iterating mostly,
[00:05:22.320 --> 00:05:26.260]   although I might use a couple other tools like quad for sheets,
[00:05:26.260 --> 00:05:29.680]   which is like a spreadsheet where you can call quad.
[00:05:29.680 --> 00:05:31.660]   Okay.
[00:05:31.660 --> 00:05:32.620]   So, yeah.
[00:05:32.620 --> 00:05:34.620]   Let's see what we've got in the Slack already.
[00:05:34.620 --> 00:05:36.640]   Okay.
[00:05:36.640 --> 00:05:37.500]   We have something here.
[00:05:37.500 --> 00:05:38.340]   Thank you, Gordy.
[00:05:38.620 --> 00:05:40.880]   So, you're an expert patient.
[00:05:40.880 --> 00:05:45.200]   So let's put this into the console and then let's take a look.
[00:05:45.200 --> 00:05:52.980]   Okay.
[00:05:52.980 --> 00:05:57.100]   And I'm just going to go through as many of these as we can get through in the session.
[00:05:57.100 --> 00:06:00.260]   And, yeah, this is pretty much what it's going to be.
[00:06:00.260 --> 00:06:10.000]   So, first of all, we can probably capitalize all the sentences.
[00:06:10.000 --> 00:06:11.420]   Does that matter?
[00:06:11.420 --> 00:06:12.640]   Does that matter?
[00:06:12.640 --> 00:06:13.020]   Yeah.
[00:06:13.020 --> 00:06:14.180]   Hi, I'm Gordy.
[00:06:14.180 --> 00:06:14.860]   Thank you.
[00:06:14.860 --> 00:06:15.800]   Yeah, yeah, yeah, yeah, yeah.
[00:06:15.800 --> 00:06:16.940]   Perfect.
[00:06:16.940 --> 00:06:19.840]   So, does it matter having capitalization?
[00:06:20.840 --> 00:06:22.200]   I think so.
[00:06:22.200 --> 00:06:22.700]   Okay.
[00:06:22.700 --> 00:06:31.840]   A lot of things, like, prompt engineering is, like, it's very new, right?
[00:06:31.840 --> 00:06:33.500]   So, like, we don't know for sure.
[00:06:33.500 --> 00:06:38.160]   Somebody out there might have done a study where they, like, conclusively show that using capital
[00:06:38.160 --> 00:06:41.580]   letters and, like, using, like, better grammar, fixing grammar mistakes help.
[00:06:41.580 --> 00:06:44.660]   I have, like, anecdotally found this in a few cases.
[00:06:44.660 --> 00:06:50.060]   I also have read some, like, quantitative stuff showing that, like, typos do hurt performance.
[00:06:50.060 --> 00:06:53.320]   But I'm also just, like, pretty obsessive about this stuff.
[00:06:53.320 --> 00:06:54.200]   So I just fix it.
[00:06:54.200 --> 00:06:55.840]   And I think it, like, it definitely doesn't hurt.
[00:06:55.840 --> 00:06:57.980]   Okay.
[00:06:57.980 --> 00:06:59.760]   Can you zoom in?
[00:06:59.760 --> 00:07:00.780]   Can I zoom in?
[00:07:00.780 --> 00:07:01.600]   Great question.
[00:07:01.600 --> 00:07:03.920]   Is that any better?
[00:07:03.920 --> 00:07:04.680]   No.
[00:07:04.680 --> 00:07:06.160]   A little more.
[00:07:06.160 --> 00:07:07.640]   Okay.
[00:07:07.640 --> 00:07:08.280]   Is that any better?
[00:07:08.280 --> 00:07:08.380]   Okay.
[00:07:11.260 --> 00:07:17.400]   So, first thing, let's put information in XML tags.
[00:07:17.400 --> 00:07:19.980]   So, we can go like this.
[00:07:19.980 --> 00:07:21.500]   Why not, like, markdown?
[00:07:21.500 --> 00:07:22.220]   Why XML?
[00:07:22.220 --> 00:07:23.080]   Why not markdown?
[00:07:23.080 --> 00:07:24.420]   Another great question.
[00:07:24.420 --> 00:07:28.220]   So, Claude was trained with a lot of XML in its training data.
[00:07:28.220 --> 00:07:32.500]   And so it's sort of seen more of that than it's seen of other formats.
[00:07:32.500 --> 00:07:35.940]   So, it just works a little bit better.
[00:07:35.940 --> 00:07:38.900]   So, this looks like all the information here.
[00:07:38.900 --> 00:07:40.080]   So, we have the medication review.
[00:07:40.080 --> 00:07:41.640]   We will be, okay.
[00:07:41.640 --> 00:07:44.480]   Can you run it before and after?
[00:07:44.480 --> 00:07:49.080]   So, before, then first iteration, and then the final iteration, and see how it comes first?
[00:07:49.080 --> 00:07:49.520]   Yeah.
[00:07:49.520 --> 00:07:50.000]   Great call.
[00:07:50.000 --> 00:07:50.600]   Okay.
[00:07:50.600 --> 00:07:52.800]   Actually, let me undo everything that I've done so far.
[00:07:52.800 --> 00:07:56.740]   Okay.
[00:07:56.740 --> 00:07:58.760]   So, we can run it here.
[00:07:58.760 --> 00:08:02.660]   And then, now, in the console, it's asking us for the user input.
[00:08:02.800 --> 00:08:04.300]   So, do we have a user input?
[00:08:04.300 --> 00:08:06.020]   No, I gave you a sample of the flat.
[00:08:06.020 --> 00:08:06.380]   Okay.
[00:08:06.380 --> 00:08:06.800]   Perfect.
[00:08:06.800 --> 00:08:10.060]   So, who are you?
[00:08:10.060 --> 00:08:11.680]   Let's do this one.
[00:08:11.680 --> 00:08:12.240]   Why do I need to?
[00:08:12.240 --> 00:08:12.680]   Yeah.
[00:08:12.680 --> 00:08:13.540]   We can do them both.
[00:08:13.540 --> 00:08:17.220]   Who are you?
[00:08:23.360 --> 00:08:24.360]   Okay.
[00:08:24.360 --> 00:08:25.360]   Okay.
[00:08:25.360 --> 00:08:27.360]   So, Gordy, what do you think of this?
[00:08:27.360 --> 00:08:28.360]   It's way too long.
[00:08:28.360 --> 00:08:32.360]   This is a conversational agent, so no more than one sentence.
[00:08:32.360 --> 00:08:33.360]   Okay.
[00:08:33.360 --> 00:08:34.360]   It's too long.
[00:08:34.360 --> 00:08:37.360]   We can probably fix that.
[00:08:37.360 --> 00:08:44.260]   So, well, we can also use this evaluate tab, so let's just, like, add all the test cases
[00:08:44.260 --> 00:08:45.260]   here.
[00:08:45.260 --> 00:08:46.960]   So, this is the evaluate tab of the console.
[00:08:46.960 --> 00:08:51.720]   I'm also going to be doing, like, some showing off of the console features, because I think
[00:08:51.720 --> 00:08:55.680]   it's a cool tool for prompt iteration.
[00:08:55.680 --> 00:08:59.600]   There's also some secret new features that I might show.
[00:08:59.600 --> 00:09:01.960]   We'll see about that.
[00:09:01.960 --> 00:09:10.400]   Okay, so then we also have, why do I need to do this experiment?
[00:09:10.400 --> 00:09:12.640]   Looks like it added a bunch of new lines.
[00:09:12.640 --> 00:09:16.380]   Let's definitely get rid of those.
[00:09:16.380 --> 00:09:18.820]   And then we can get this next one.
[00:09:18.820 --> 00:09:22.400]   Can I schedule it tomorrow instead?
[00:09:22.400 --> 00:09:23.720]   Cool.
[00:09:23.720 --> 00:09:32.160]   So, I hit run remaining.
[00:09:32.160 --> 00:09:38.140]   And this is all running through Dove, sorry, through, okay, so we have, why do I need to
[00:09:38.140 --> 00:09:39.140]   do this appointment?
[00:09:39.140 --> 00:09:43.160]   So, this looks pretty long as well.
[00:09:43.160 --> 00:09:46.660]   And here, we have this, like, I apologize, but I don't have information about scheduling
[00:09:46.660 --> 00:09:54.280]   Okay, so, is that true?
[00:09:54.280 --> 00:09:56.540]   Is it true that we don't have information about scheduling?
[00:09:56.540 --> 00:10:03.540]   No, we are always available, make it easy for that.
[00:10:03.540 --> 00:10:04.980]   No, 24 hour availability.
[00:10:04.980 --> 00:10:05.980]   24 hour availability.
[00:10:05.980 --> 00:10:06.980]   Okay.
[00:10:06.980 --> 00:10:09.360]   All right, so this is, like, the version one.
[00:10:09.360 --> 00:10:12.920]   Now let's make some changes.
[00:10:12.920 --> 00:10:17.980]   So, first of all, actually, I'll try to do things, like, roughly in, like, some order of
[00:10:17.980 --> 00:10:18.980]   lines.
[00:10:18.980 --> 00:10:25.360]   So, maybe I won't make you all sit through the capitalization, even though I, like, would
[00:10:25.360 --> 00:10:26.360]   definitely do that.
[00:10:26.360 --> 00:10:30.820]   I'm also going to add a new line here, just because I think that's, like, more normal what
[00:10:30.820 --> 00:10:35.100]   you'd see in, like, a written document, you'd have a new line.
[00:10:35.100 --> 00:10:36.840]   We'll close the information tab.
[00:10:36.840 --> 00:10:40.840]   What's that?
[00:10:40.840 --> 00:10:44.800]   Yeah, I could, actually.
[00:10:44.800 --> 00:10:46.740]   That's not a bad idea.
[00:10:46.740 --> 00:10:50.500]   The one thing I wouldn't feel completely confident of is that it would, like, exactly
[00:10:50.500 --> 00:10:54.740]   transcribe the rest of everything, like, word for word.
[00:10:54.740 --> 00:10:58.780]   I think it probably would.
[00:10:58.780 --> 00:11:04.200]   What I actually might do is just, like, have Claude write code to capitalize every first word
[00:11:04.200 --> 00:11:05.620]   of the sentence.
[00:11:05.620 --> 00:11:08.900]   Then I'd be worried about edge cases, like, what if there's, like, ellipses?
[00:11:08.900 --> 00:11:12.800]   But that kind of thing is definitely useful.
[00:11:12.800 --> 00:11:15.500]   And like, I definitely use Claude a lot in writing prompts.
[00:11:15.500 --> 00:11:20.260]   For instance, like, we have, like, a quad tool that, like, helps complete code, basically,
[00:11:20.260 --> 00:11:25.500]   and I do a lot of prompting in that IDE, because, like, especially with, like, very nested XML
[00:11:25.500 --> 00:11:26.500]   tags.
[00:11:26.500 --> 00:11:30.160]   It helps a lot just, like, suggesting the closures of them, which is, like, pretty obvious, but
[00:11:30.160 --> 00:11:32.260]   still takes a long time to type.
[00:11:32.260 --> 00:11:38.020]   So, yeah, if you have any sort of, like, co-pilot type thing, definitely that's, like, a good
[00:11:38.020 --> 00:11:40.020]   environment for writing prompts.
[00:11:40.020 --> 00:11:44.780]   Okay, now let's do the same thing with this instructions.
[00:11:44.780 --> 00:11:51.380]   And we can do this.
[00:11:51.380 --> 00:11:56.780]   It looks like this one, like, didn't get a number, so let's, like, do that.
[00:11:56.780 --> 00:12:14.540]   Yeah, so the key thing in terms of XML, I think, is just, like, really, XML isn't even that important.
[00:12:14.540 --> 00:12:19.900]   The most important thing is just clearly separating the different parts of the prompt.
[00:12:19.900 --> 00:12:24.500]   Yeah, exactly, it's, like, here's this stuff, here's this other stuff.
[00:12:24.500 --> 00:12:32.580]   Like, if we wanted to, we could do something like -- like, I wouldn't do this, but, like,
[00:12:32.580 --> 00:12:36.580]   like, I think it would probably work fine.
[00:12:36.580 --> 00:12:39.580]   Yeah.
[00:12:39.580 --> 00:12:41.580]   Okay.
[00:12:41.580 --> 00:12:42.920]   So this is all fine.
[00:12:42.920 --> 00:12:51.800]   Let's also do the same thing with user input.
[00:12:51.800 --> 00:12:57.120]   Now we can run, we can go back to the evaluate tab, and we can hit rerun all, and it's using
[00:12:57.120 --> 00:12:59.860]   our nice new prompt.
[00:12:59.860 --> 00:13:01.280]   Still looks pretty long.
[00:13:01.280 --> 00:13:08.660]   But we can also see how it does on the last case where -- okay, so here it's still said
[00:13:08.660 --> 00:13:11.360]   I don't have access to the specific scheduling information.
[00:13:11.360 --> 00:13:13.100]   So let's try and fix these two things.
[00:13:13.100 --> 00:13:19.140]   So first of all, we can make it shorter, so -- do we have anything here about, like, making
[00:13:19.140 --> 00:13:20.140]   it shorter?
[00:13:20.140 --> 00:13:21.140]   What's that?
[00:13:21.140 --> 00:13:22.140]   Rule 7.
[00:13:22.140 --> 00:13:25.640]   Rule 7, okay.
[00:13:25.640 --> 00:13:28.520]   Be concise and offer only relevant information.
[00:13:28.520 --> 00:13:31.600]   Oops, let's actually do this.
[00:13:31.600 --> 00:13:36.600]   Don't want to misnumber here.
[00:13:36.600 --> 00:13:39.340]   And be concise.
[00:13:39.340 --> 00:13:43.340]   So, like -- and offer only relevant information.
[00:13:43.340 --> 00:13:51.600]   Each response should be -- or let's be a little bit, like, less prescriptive to give Claude,
[00:13:51.600 --> 00:13:52.940]   like, a little bit more room.
[00:13:52.940 --> 00:13:56.620]   Like, if we say, like, every response should be, like, exactly three sentences, that might
[00:13:56.620 --> 00:13:57.980]   be, like, a little too constraining.
[00:13:57.980 --> 00:13:58.980]   I'm just guessing.
[00:13:58.980 --> 00:14:14.060]   So we could just say, like -- so why is response should be two to four sentences better than
[00:14:14.060 --> 00:14:15.060]   telling it to be concise?
[00:14:15.060 --> 00:14:16.060]   Concise is arbitrary.
[00:14:16.060 --> 00:14:20.060]   Yeah, concise could mean a lot of different things to different people.
[00:14:20.060 --> 00:14:26.140]   Like, in some cases, like, concise might mean, like, literally only one word.
[00:14:26.140 --> 00:14:29.740]   In some cases, like, if you ask for, like, a concise book review, we might be looking at,
[00:14:29.740 --> 00:14:35.140]   like, you know, a single-page Word doc, and that would be concise in the context of a book
[00:14:35.140 --> 00:14:36.140]   review.
[00:14:36.140 --> 00:14:41.820]   So, yeah, Claude is, like, trying to guess what you mean by concise.
[00:14:41.820 --> 00:14:47.820]   What if you also have a longer prompt, so you have a really long prompt.
[00:14:47.820 --> 00:14:48.820]   Sorry, one sec.
[00:14:48.820 --> 00:14:49.820]   Go ahead.
[00:14:49.820 --> 00:14:53.040]   You have a really long prompt, so if you have a long system prompt with a lot of detailed
[00:14:53.040 --> 00:14:58.040]   instructions saying be concise, you're not going to get something super, super short.
[00:14:58.040 --> 00:14:59.040]   I think that's right.
[00:14:59.040 --> 00:15:02.820]   I think the tone of the prompt -- so, what he was saying, if people couldn't hear, is, like,
[00:15:02.820 --> 00:15:06.320]   the prompt is long, so the response might also be long.
[00:15:06.320 --> 00:15:09.220]   I don't think that's, like, definitively true.
[00:15:09.220 --> 00:15:13.400]   Like, you can have long prompts that give short responses or short prompts that give long responses.
[00:15:13.400 --> 00:15:17.260]   But it's more like if you don't say anything, it might pick up on some of those, like, context
[00:15:17.260 --> 00:15:18.260]   clues.
[00:15:18.260 --> 00:15:19.260]   You were saying something over here?
[00:15:19.260 --> 00:15:20.260]   Yeah.
[00:15:20.260 --> 00:15:21.260]   Yeah.
[00:15:21.260 --> 00:15:40.860]   So, let me actually get to that after we do this.
[00:15:40.860 --> 00:15:43.260]   So, this two to four sentences, it looks like it's still pretty long.
[00:15:43.260 --> 00:15:46.760]   I think maybe that's actually, like, longer than necessary, so maybe we should make it,
[00:15:46.760 --> 00:15:49.280]   like, one to two sentences.
[00:15:49.280 --> 00:15:53.600]   Let's try that.
[00:15:53.600 --> 00:15:54.980]   Never more than three.
[00:15:54.980 --> 00:15:56.200]   Okay.
[00:15:56.200 --> 00:15:58.760]   Now we can try that here.
[00:15:58.760 --> 00:16:01.880]   Okay.
[00:16:01.880 --> 00:16:04.000]   That looks better, right?
[00:16:04.000 --> 00:16:06.000]   This is definitely shorter.
[00:16:06.000 --> 00:16:07.000]   Okay.
[00:16:07.000 --> 00:16:14.720]   And it also seems that it is giving variable numbers of sentences, so these were both two
[00:16:14.720 --> 00:16:16.940]   sentences and then this one is three.
[00:16:16.940 --> 00:16:20.380]   So one of the questions over here is, like, can the LLM figure out that it should do longer
[00:16:20.380 --> 00:16:22.940]   responses in certain situations and shorter in other words?
[00:16:22.940 --> 00:16:24.780]   So it seems like it did that here.
[00:16:24.780 --> 00:16:25.780]   Okay.
[00:16:25.780 --> 00:16:31.000]   So then the next point was that in this case it shouldn't say that I don't have access to
[00:16:31.000 --> 00:16:35.060]   the scheduling system or specific appointment times.
[00:16:35.060 --> 00:16:36.060]   What should it say instead?
[00:16:36.060 --> 00:16:39.600]   It should say, "Sure, what time tomorrow."
[00:16:39.600 --> 00:16:43.840]   But intentionally in the prompt, I left out that it's a 24-hour service.
[00:16:43.840 --> 00:16:52.480]   So this is a case where we're asking a question that's not present in the information it has.
[00:16:52.480 --> 00:16:53.480]   Okay.
[00:16:53.480 --> 00:17:00.060]   So, yeah, I mean, we could add something like, "You're open for 24-hour service," but you're
[00:17:00.060 --> 00:17:05.620]   saying you want to test its ability to, like, figure out how to do it without that.
[00:17:05.620 --> 00:17:06.680]   It is saying I don't know.
[00:17:06.680 --> 00:17:07.680]   So...
[00:17:07.680 --> 00:17:08.680]   Oh, okay.
[00:17:08.680 --> 00:17:09.680]   It is saying I don't know, which is good.
[00:17:09.680 --> 00:17:10.680]   Okay.
[00:17:10.680 --> 00:17:12.680]   Well, then we're doing great.
[00:17:12.680 --> 00:17:15.840]   All right.
[00:17:15.840 --> 00:17:16.840]   Should we...
[00:17:16.840 --> 00:17:19.680]   Anything else that you wanted to get out of looking at this example, Gordy?
[00:17:19.680 --> 00:17:21.300]   I think that was...
[00:17:21.300 --> 00:17:23.240]   Oh, about the structure.
[00:17:23.240 --> 00:17:28.300]   So does the order of the rules or the order of putting information, then rules, or rules
[00:17:28.300 --> 00:17:31.300]   first, then information, does any of that matter?
[00:17:31.300 --> 00:17:32.300]   Yeah.
[00:17:32.300 --> 00:17:36.240]   So does it matter what order we have these components?
[00:17:36.240 --> 00:17:37.240]   Yeah.
[00:17:37.240 --> 00:17:40.800]   I think it's better to put the information above the instructions.
[00:17:40.800 --> 00:17:41.800]   Okay.
[00:17:41.800 --> 00:17:46.860]   We've sort of found that instructions are more tightly followed the closer they are to the
[00:17:46.860 --> 00:17:49.760]   bottom of the prompt as a rule.
[00:17:49.760 --> 00:17:54.200]   This doesn't necessarily apply in all situations, so definitely test it out.
[00:17:54.200 --> 00:17:59.680]   That actually is, like, a blanket statement that applies to everything that I say, but particularly
[00:17:59.680 --> 00:18:00.680]   for that.
[00:18:00.680 --> 00:18:01.680]   Yeah.
[00:18:01.680 --> 00:18:02.680]   Okay.
[00:18:02.680 --> 00:18:03.680]   Can I ask a question?
[00:18:03.680 --> 00:18:04.680]   Yeah.
[00:18:04.680 --> 00:18:05.680]   I don't know...
[00:18:05.680 --> 00:18:07.680]   I don't know if you were asking all the questions or...
[00:18:07.680 --> 00:18:08.680]   No, no, no.
[00:18:08.680 --> 00:18:09.680]   Go ahead.
[00:18:09.680 --> 00:18:10.680]   Okay.
[00:18:10.680 --> 00:18:11.680]   Okay.
[00:18:11.680 --> 00:18:12.680]   Okay.
[00:18:12.680 --> 00:18:17.680]   If there is some difference to the exclamation mark here, I noticed that it's...
[00:18:17.680 --> 00:18:18.680]   Exclamation mark?
[00:18:18.680 --> 00:18:19.680]   The exclamation mark.
[00:18:19.680 --> 00:18:20.680]   Yes.
[00:18:20.680 --> 00:18:21.680]   Oh, remember.
[00:18:21.680 --> 00:18:24.680]   I don't think I added that on...
[00:18:24.680 --> 00:18:27.680]   It looks like you added that, Gordy.
[00:18:27.680 --> 00:18:32.680]   The exclamation marks, just as they emphasize things for humans, they also emphasize things
[00:18:32.680 --> 00:18:33.680]   to the model.
[00:18:33.680 --> 00:18:40.680]   Do you think that has more of an effect over the numbers or less, like, in terms of relevant?
[00:18:40.680 --> 00:18:41.680]   Ooh.
[00:18:41.680 --> 00:18:42.680]   Yeah.
[00:18:42.680 --> 00:18:45.680]   I don't know at that level of detail, and I think it's dependent on context as well.
[00:18:45.680 --> 00:18:49.680]   But yeah, if you want to emphasize things, like, capitalizing them or putting exclamation
[00:18:49.680 --> 00:18:55.680]   marks or, like, just saying, "This is extremely important," that all does do something.
[00:18:55.680 --> 00:18:56.680]   Yeah.
[00:18:56.680 --> 00:19:08.680]   So, the tokenizer, though, I'm taking a look at some of your tokenizer codes, but it doesn't
[00:19:08.680 --> 00:19:12.680]   seem like exclamation points actually, like, do anything, really.
[00:19:12.680 --> 00:19:16.680]   Like, the tokenizer kind of binds them into the word.
[00:19:16.680 --> 00:19:17.680]   It does something.
[00:19:17.680 --> 00:19:18.680]   It does something.
[00:19:18.680 --> 00:19:19.680]   That's all I can say.
[00:19:19.680 --> 00:19:22.680]   Just anecdotally, if you put exclamation marks in, it's different.
[00:19:22.680 --> 00:19:23.680]   I guess...
[00:19:23.680 --> 00:19:24.680]   Yeah.
[00:19:24.680 --> 00:19:25.680]   That's my analysis of it.
[00:19:25.680 --> 00:19:26.680]   All right.
[00:19:26.680 --> 00:19:27.680]   Okay.
[00:19:27.680 --> 00:19:30.680]   I think let's go to the next...
[00:19:30.680 --> 00:19:31.680]   How many we got?
[00:19:31.680 --> 00:19:32.680]   We got six here already.
[00:19:32.680 --> 00:19:33.680]   That's pretty good.
[00:19:33.680 --> 00:19:34.680]   Okay.
[00:19:34.680 --> 00:19:36.680]   This is just a general question.
[00:19:36.680 --> 00:19:38.680]   I'll just answer this really quick.
[00:19:38.680 --> 00:19:41.680]   In general, for translations, our multilingual output, is it better to instruct our English
[00:19:41.680 --> 00:19:43.680]   or the native language?
[00:19:43.680 --> 00:19:48.680]   I think it's better to instruct in the native language if you speak the native language.
[00:19:48.680 --> 00:19:53.680]   If you only speak English and you're choosing between, like, a better prompt that's written
[00:19:53.680 --> 00:19:58.680]   in English versus, like, a worse prompt that's written in a language that you don't understand,
[00:19:58.680 --> 00:20:02.680]   I would probably default to writing it in the language that I knew super well.
[00:20:02.680 --> 00:20:07.680]   But ideally, I think for the ideal prompt, you would find a native speaker of the language
[00:20:07.680 --> 00:20:11.680]   and explain your use case to them and have them write the prompt.
[00:20:11.680 --> 00:20:33.680]   So, is that not the same question that I just answered?
[00:20:33.680 --> 00:20:34.680]   Is it different?
[00:20:34.680 --> 00:20:40.680]   I think it's better to have the prompt in that language.
[00:20:40.680 --> 00:20:41.680]   In general.
[00:20:41.680 --> 00:20:42.680]   If you can.
[00:20:42.680 --> 00:20:44.680]   If you can write a really good prompt.
[00:20:44.680 --> 00:20:45.680]   All right.
[00:20:45.680 --> 00:20:46.680]   Let's go to this next guy.
[00:20:46.680 --> 00:20:48.680]   So, you'll be acting as a test reviewer.
[00:20:48.680 --> 00:20:51.680]   Let me pump up the size here, too.
[00:20:51.680 --> 00:20:52.680]   Okay.
[00:20:52.680 --> 00:20:55.680]   Not sure if there's a way to...
[00:20:55.680 --> 00:20:56.680]   Oh, I can hide this.
[00:20:56.680 --> 00:20:57.680]   Okay.
[00:20:57.680 --> 00:20:58.680]   Great.
[00:20:58.680 --> 00:21:00.680]   Responsible for improving unit tests based on a set of requirements.
[00:21:00.680 --> 00:21:03.680]   Below is the project directory.
[00:21:03.680 --> 00:21:04.680]   Project path.
[00:21:04.680 --> 00:21:09.680]   Don't include any other explanation, conversion, or output besides the JSON.
[00:21:09.680 --> 00:21:10.680]   Okay.
[00:21:10.680 --> 00:21:20.680]   This is great because it's going to let me show off prefills.
[00:21:20.680 --> 00:21:26.680]   So, let's make a new prompt here.
[00:21:26.680 --> 00:21:28.680]   Let's paste this in.
[00:21:28.680 --> 00:21:34.680]   I have to use double brackets instead of single brackets to get variables in the console.
[00:21:34.680 --> 00:21:36.680]   And then I think there's another...
[00:21:36.680 --> 00:21:38.680]   But this JSON string...
[00:21:38.680 --> 00:21:39.680]   Who gave this prompt?
[00:21:39.680 --> 00:21:41.680]   This is from Dan.
[00:21:41.680 --> 00:21:44.680]   The JSON string, Dan, that's...
[00:21:44.680 --> 00:21:45.680]   So, that's...
[00:21:45.680 --> 00:21:46.680]   Is that variable or is that...
[00:21:46.680 --> 00:21:49.680]   Is that like an example in this problem template?
[00:21:49.680 --> 00:21:50.680]   So, in this case, that's...
[00:21:50.680 --> 00:21:51.680]   That is just the template and then we...
[00:21:51.680 --> 00:22:00.680]   So, basically, we have one of you to write the unit tests and then...
[00:22:00.680 --> 00:22:01.680]   Yeah.
[00:22:01.680 --> 00:22:01.680]   Test of your agent does not return...
[00:22:01.680 --> 00:22:01.680]   Always return JSON all way.
[00:22:01.680 --> 00:22:01.680]   Yeah.
[00:22:01.680 --> 00:22:02.680]   Like, almost all of this is just like workarounds for the fact that it doesn't always speak JSON
[00:22:02.680 --> 00:22:03.680]   right.
[00:22:03.680 --> 00:22:06.680]   Like, you can see how many times we said that.
[00:22:06.680 --> 00:22:07.680]   Yeah.
[00:22:07.680 --> 00:22:08.680]   And then...
[00:22:08.680 --> 00:22:09.680]   Do you have an example input here?
[00:22:09.680 --> 00:22:10.680]   Uh, so that first...
[00:22:10.680 --> 00:22:11.680]   If you go up a little bit, sorry.
[00:22:11.680 --> 00:22:12.680]   Sorry.
[00:22:12.680 --> 00:22:13.680]   Sorry.
[00:22:13.680 --> 00:22:14.680]   Sorry.
[00:22:14.680 --> 00:22:15.680]   Okay.
[00:22:15.680 --> 00:22:16.680]   Yeah.
[00:22:16.680 --> 00:22:17.680]   Yeah.
[00:22:17.680 --> 00:22:18.680]   Yeah.
[00:22:18.680 --> 00:22:19.680]   Okay.
[00:22:19.680 --> 00:22:20.680]   Okay.
[00:22:20.680 --> 00:22:21.680]   Yeah.
[00:22:21.680 --> 00:22:22.680]   Yeah.
[00:22:22.680 --> 00:22:23.680]   Yeah.
[00:22:23.680 --> 00:22:24.680]   Yeah.
[00:22:24.680 --> 00:22:25.680]   Yeah.
[00:22:25.680 --> 00:22:29.680]   Like, almost all of this is just like workarounds for the fact that it doesn't always speak JSON
[00:22:29.680 --> 00:22:30.680]   right.
[00:22:30.680 --> 00:22:31.680]   Okay.
[00:22:31.680 --> 00:22:32.680]   Like, you can see how many times we said that.
[00:22:32.680 --> 00:22:33.680]   Yeah.
[00:22:33.680 --> 00:22:35.680]   And then, uh, do you have an example input here?
[00:22:35.680 --> 00:22:37.680]   Uh, so that first...
[00:22:37.680 --> 00:22:38.680]   If you go up a little bit, sorry.
[00:22:38.680 --> 00:22:41.680]   So that first comment is from the unit test writer.
[00:22:41.680 --> 00:22:42.680]   So that is the input.
[00:22:42.680 --> 00:22:45.680]   Like, the unit test writer writes a bunch of unit tests.
[00:22:45.680 --> 00:22:49.680]   And then the reviewer reviews it and makes them better.
[00:22:49.680 --> 00:22:50.680]   So everything...
[00:22:50.680 --> 00:22:51.680]   Everything here is...
[00:22:51.680 --> 00:22:52.680]   Is what I should put into...
[00:22:52.680 --> 00:22:53.680]   Yes.
[00:22:53.680 --> 00:22:55.680]   And then you can see that second set.
[00:22:55.680 --> 00:22:56.680]   This is...
[00:22:56.680 --> 00:23:00.680]   This is a good result where it writes JSON, which basically says, cool.
[00:23:00.680 --> 00:23:04.680]   Update this file with these unit tests and here's the modifications I made.
[00:23:04.680 --> 00:23:05.680]   That sort of thing.
[00:23:05.680 --> 00:23:06.680]   Okay.
[00:23:06.680 --> 00:23:12.680]   So in this template here, where would the thing that I just copied go?
[00:23:12.680 --> 00:23:18.680]   Well, so essentially we don't provide it in line with the prompt.
[00:23:18.680 --> 00:23:25.680]   We just provide the conversation and then this thing jumps in as a separate agent.
[00:23:25.680 --> 00:23:29.680]   So, like, the context window is gonna have the unit tests in it.
[00:23:29.680 --> 00:23:35.680]   But we're saying respond in this format given the unit tests that are earlier in the conversation that you're picking up.
[00:23:35.680 --> 00:23:40.680]   So the thing that you just pasted is, like, step three of a multi-shot conversation?
[00:23:40.680 --> 00:23:42.680]   Sorry, not multi-shot, multi-turn.
[00:23:42.680 --> 00:23:43.680]   Yeah, yeah.
[00:23:43.680 --> 00:23:45.680]   The thing I just pasted was two shots, right?
[00:23:45.680 --> 00:23:47.680]   Unit test writer and then a unit test reviewer.
[00:23:47.680 --> 00:23:49.680]   And the reviewer is the one that's having the problem.
[00:23:49.680 --> 00:23:50.680]   It comes second.
[00:23:50.680 --> 00:23:57.680]   Okay, so it would be something like this.
[00:23:57.680 --> 00:24:07.680]   Here are some unit tests written by a unit test writer bot.
[00:24:07.680 --> 00:24:08.680]   Right.
[00:24:08.680 --> 00:24:18.680]   Okay, and then we have this.
[00:24:18.680 --> 00:24:20.680]   Now, you didn't...
[00:24:20.680 --> 00:24:24.680]   Okay, so this is basically how it works.
[00:24:24.680 --> 00:24:26.680]   Does this look right?
[00:24:26.680 --> 00:24:27.680]   No, it does.
[00:24:27.680 --> 00:24:28.680]   I mean, this...
[00:24:28.680 --> 00:24:29.680]   Okay.
[00:24:29.680 --> 00:24:30.680]   We just...
[00:24:30.680 --> 00:24:34.680]   We do this in sort of this larger conversation, not just as sort of a standalone prompt for this one agent.
[00:24:34.680 --> 00:24:35.680]   Yeah.
[00:24:35.680 --> 00:24:36.680]   Okay, so then here I put the unit tests in.
[00:24:36.680 --> 00:24:37.680]   Right, yep.
[00:24:37.680 --> 00:24:40.680]   And then for project path, what sort of thing should I put there?
[00:24:40.680 --> 00:24:41.680]   Anything.
[00:24:41.680 --> 00:24:46.680]   I mean, this is just like the local directory that's gonna be modified.
[00:24:46.680 --> 00:24:52.680]   And so it actually has access to the files in that directory, and it'll fill in its own sort of what files to modify.
[00:24:52.680 --> 00:24:54.680]   Okay, so then I'll just put...
[00:24:54.680 --> 00:24:55.680]   That's fine.
[00:24:55.680 --> 00:24:56.680]   Something like that.
[00:24:56.680 --> 00:24:57.680]   Yep.
[00:24:57.680 --> 00:25:10.680]   Okay, so let's see if it comes out with the JSON or not.
[00:25:10.680 --> 00:25:11.680]   Bated breath here.
[00:25:11.680 --> 00:25:12.680]   Yep.
[00:25:12.680 --> 00:25:16.680]   We did do most of our tests on Cloud 3 and not 3.5, so 3.5 is probably a little better.
[00:25:16.680 --> 00:25:17.680]   Okay.
[00:25:17.680 --> 00:25:18.680]   If we haven't done...
[00:25:18.680 --> 00:25:23.680]   Yeah, I mean, if it makes it more realistic, we could also switch the model version to use...
[00:25:23.680 --> 00:25:24.680]   Oh, no, I mean, we're gonna upgrade.
[00:25:24.680 --> 00:25:25.680]   So I'd rather see it with this.
[00:25:25.680 --> 00:25:26.680]   Okay.
[00:25:26.680 --> 00:25:27.680]   What was that?
[00:25:27.680 --> 00:25:29.680]   Is the temperature being set to zero intentional?
[00:25:29.680 --> 00:25:34.680]   Yeah, I usually, for knowledge work, I usually have the temperature set to zero.
[00:25:34.680 --> 00:25:35.680]   Not 0.01?
[00:25:35.680 --> 00:25:35.680]   Like, I usually use that for hallucination.
[00:25:35.680 --> 00:25:36.680]   I mean, I'm just gonna go on here.
[00:25:36.680 --> 00:25:37.680]   But, like, is that...
[00:25:37.680 --> 00:25:38.680]   Like, am I totally out...
[00:25:38.680 --> 00:25:41.680]   I think using temperature zero, you'll probably get, like, marginally fewer hallucinations.
[00:25:41.680 --> 00:25:42.680]   Okay, I'm...
[00:25:42.680 --> 00:25:43.680]   Oh, here we go.
[00:25:43.680 --> 00:25:44.680]   Okay, so it looks like in this case it did output JSON, I think.
[00:25:44.680 --> 00:25:45.680]   Yeah, that looks plausible.
[00:25:45.680 --> 00:25:46.680]   Okay.
[00:25:46.680 --> 00:25:47.680]   It's very long JSON.
[00:25:47.680 --> 00:25:48.680]   I guess that explains why it was taking so long to...
[00:25:48.680 --> 00:25:49.680]   Looks like it actually even ran into the max output tokens because it didn't finish its JSON.
[00:25:49.680 --> 00:25:50.680]   Aha.
[00:25:50.680 --> 00:25:51.680]   Uh-huh.
[00:25:51.680 --> 00:25:52.680]   Just to make this...
[00:25:52.680 --> 00:25:52.680]   Since this one is kind of going kind of slow, I will...
[00:25:52.680 --> 00:25:53.680]   I will...
[00:25:53.680 --> 00:25:54.680]   If you're using temperature zero, you'll probably get, like, marginally fewer hallucinations.
[00:25:54.680 --> 00:25:55.680]   Okay.
[00:25:55.680 --> 00:25:56.680]   I'm...
[00:25:56.680 --> 00:25:57.680]   Oh, here we go.
[00:25:57.680 --> 00:25:58.680]   Okay.
[00:25:58.680 --> 00:26:01.680]   So it looks like in this case it did output JSON.
[00:26:01.680 --> 00:26:02.680]   I think.
[00:26:02.680 --> 00:26:03.680]   Yeah.
[00:26:03.680 --> 00:26:04.680]   That looks plausible.
[00:26:04.680 --> 00:26:05.680]   Okay.
[00:26:05.680 --> 00:26:06.680]   It's still...
[00:26:06.680 --> 00:26:07.680]   Very long JSON.
[00:26:07.680 --> 00:26:11.680]   I guess that explains why it was taking so long to...
[00:26:11.680 --> 00:26:15.680]   Looks like actually it even ran into the max output tokens because it didn't finish its JSON.
[00:26:15.680 --> 00:26:16.680]   Aha.
[00:26:16.680 --> 00:26:18.680]   Just to make this...
[00:26:18.680 --> 00:26:22.680]   Since this one is kind of going kind of slow, I will test it with Haiku.
[00:26:22.680 --> 00:26:23.680]   Let's...
[00:26:23.680 --> 00:26:32.680]   And let's also increase the max tokens to sample so that it doesn't run into that issue.
[00:26:32.680 --> 00:26:38.680]   So what I'm really hoping is to get a case that doesn't output JSON so that then I can fix
[00:26:38.680 --> 00:26:41.680]   it and then it will output JSON.
[00:26:41.680 --> 00:26:43.680]   If not, I can still say, like, how I would fix it.
[00:26:43.680 --> 00:26:44.680]   Yeah.
[00:26:44.680 --> 00:26:45.680]   That would be great.
[00:26:45.680 --> 00:26:47.680]   Just honestly, any comments you have just on how we structured things.
[00:26:47.680 --> 00:26:48.680]   Okay.
[00:26:48.680 --> 00:26:49.680]   Yeah.
[00:26:49.680 --> 00:26:53.680]   So, I mean, this is, like, definitely, like, a big request from people is, like, how do
[00:26:53.680 --> 00:26:56.680]   I make sure the model outputs JSON?
[00:26:56.680 --> 00:27:02.680]   The most reliable way to do that, I feel, is using the assistant pre-fill.
[00:27:02.680 --> 00:27:06.680]   So, maybe some of you have used this feature before.
[00:27:06.680 --> 00:27:11.680]   Maybe some of you have, like, only used other models such as GPT that don't offer this feature.
[00:27:11.680 --> 00:27:17.680]   Something that you can do in the Claude API is partially pre-fill an assistant message.
[00:27:17.680 --> 00:27:23.680]   So, what you're doing there is you're putting some words in Claude's mouth, as we call it.
[00:27:23.680 --> 00:27:28.680]   And then when Claude continues from there, it assumes that it's already said whatever you
[00:27:28.680 --> 00:27:30.680]   told it that it had said.
[00:27:30.680 --> 00:27:34.680]   And that can help you get it on the right path.
[00:27:34.680 --> 00:27:37.680]   So, for instance, in this case, if we want to make sure...
[00:27:37.680 --> 00:27:42.680]   So, the classic, like, bad response from Claude, when people give it prompts like this, where
[00:27:42.680 --> 00:27:48.680]   they want to get JSON, is Claude would say something like, I'll just, like, add another message, just
[00:27:48.680 --> 00:27:49.680]   have some of us to type.
[00:27:49.680 --> 00:27:53.680]   It might be, like, here is the JSON, right?
[00:27:53.680 --> 00:27:56.680]   Have people seen stuff like this?
[00:27:56.680 --> 00:28:00.680]   And this part right here is, like, very annoying.
[00:28:00.680 --> 00:28:02.680]   And difficult to get rid of.
[00:28:02.680 --> 00:28:04.680]   So, okay.
[00:28:04.680 --> 00:28:07.680]   So, I have two strategies.
[00:28:07.680 --> 00:28:10.680]   Let me actually just give, like, the simplest one.
[00:28:10.680 --> 00:28:17.680]   They both, though, they both require a tiny bit of post-processing at the end.
[00:28:17.680 --> 00:28:18.680]   So, let's start by...
[00:28:18.680 --> 00:28:23.680]   Let's actually, like, take out all this stuff about make sure to only respond in JSON.
[00:28:23.680 --> 00:28:27.680]   That could be one way to get it to not do the...
[00:28:27.680 --> 00:28:29.680]   To be bad.
[00:28:29.680 --> 00:28:31.680]   So, we could just go like this.
[00:28:31.680 --> 00:28:35.680]   Let's try to make it not do JSON.
[00:28:35.680 --> 00:28:38.680]   Let's get rid of all this stuff.
[00:28:38.680 --> 00:28:39.680]   Okay.
[00:28:39.680 --> 00:28:44.680]   So, here now, maybe it will do the preamble thing that we don't want it to do.
[00:28:44.680 --> 00:28:45.680]   Perfect.
[00:28:45.680 --> 00:28:46.680]   Okay.
[00:28:46.680 --> 00:28:58.680]   So, an easy way to get it to not do that is to just take this and then put it in the pre-fill.
[00:28:58.680 --> 00:29:01.680]   So, it thinks that it already said that.
[00:29:01.680 --> 00:29:04.680]   Like this.
[00:29:04.680 --> 00:29:05.680]   Okay.
[00:29:05.680 --> 00:29:07.680]   So, if we do that...
[00:29:07.680 --> 00:29:09.680]   Just the JSON.
[00:29:09.680 --> 00:29:12.680]   So, what we're doing here is...
[00:29:12.680 --> 00:29:14.680]   You can think of quad almost like a...
[00:29:14.680 --> 00:29:19.680]   Like a child who's just, like, misbehaving and it wants to do something and you're like,
[00:29:19.680 --> 00:29:23.680]   don't do the thing, but it just keeps doing it because it just loves preamble so much and
[00:29:23.680 --> 00:29:26.680]   it has this, like, innate desire to do them.
[00:29:26.680 --> 00:29:29.680]   So, one way is to, like, argue with it a lot.
[00:29:29.680 --> 00:29:32.680]   But, like, if you have a kid, sometimes, you know, you just have to, like, let them do the
[00:29:32.680 --> 00:29:34.680]   thing that they want to do and then they'll get over it.
[00:29:34.680 --> 00:29:37.680]   So, in this case, that's basically what we did.
[00:29:37.680 --> 00:29:41.680]   We just gave quad this pre-fill where we let it do the thing.
[00:29:41.680 --> 00:29:44.680]   So, as far as it's concerned, it already did the thing.
[00:29:44.680 --> 00:29:48.680]   And then from there, what it's outputting is JSON.
[00:29:48.680 --> 00:29:54.680]   Now, if you want to make this even more reliable, you can put this nice little bracket here.
[00:29:54.680 --> 00:29:56.680]   And then it's like, oh, dang.
[00:29:56.680 --> 00:29:58.680]   Like, I'm really in JSON mode now.
[00:29:58.680 --> 00:29:59.680]   Like, I'm really...
[00:29:59.680 --> 00:30:01.680]   My JSON has actually already begun.
[00:30:01.680 --> 00:30:04.680]   So, at this point, it's definitely not going to do the preamble.
[00:30:04.680 --> 00:30:13.680]   The only thing here is if you sample with this pre-fill, you will need to add the bracket
[00:30:13.680 --> 00:30:17.680]   back before you try to do your JSON.loads or what have you.
[00:30:17.680 --> 00:30:18.680]   Because quad is...
[00:30:18.680 --> 00:30:22.680]   Since you told it that it had already said the opening bracket, it's not going to give you
[00:30:22.680 --> 00:30:24.680]   another opening bracket.
[00:30:24.680 --> 00:30:25.680]   Okay.
[00:30:25.680 --> 00:30:33.680]   So, then another thing that you can do is return the JSON in JSON tags.
[00:30:33.680 --> 00:30:40.680]   And then, if we do this without the pre-fill, let's try it without the pre-fill.
[00:30:40.680 --> 00:30:45.680]   You don't normally capitalize JSON?
[00:30:45.680 --> 00:30:50.680]   I'm not a software engineer, okay?
[00:30:50.680 --> 00:30:51.680]   I'm a prompt engineer.
[00:30:51.680 --> 00:30:53.680]   I don't even know that it's capitalized.
[00:30:53.680 --> 00:30:56.680]   For all I know, that's just like an English word.
[00:30:56.680 --> 00:30:57.680]   But, yeah.
[00:30:57.680 --> 00:30:58.680]   Good.
[00:30:58.680 --> 00:30:59.680]   Thank you.
[00:30:59.680 --> 00:31:00.680]   Okay.
[00:31:00.680 --> 00:31:02.680]   So, here we see it did the thing.
[00:31:02.680 --> 00:31:03.680]   So, it gave its preamble, right?
[00:31:03.680 --> 00:31:07.680]   And then, it gave the JSON tag.
[00:31:07.680 --> 00:31:09.680]   Everything within the JSON tag is JSON.
[00:31:09.680 --> 00:31:13.680]   And then, at the end, it closed this JSON tag.
[00:31:13.680 --> 00:31:21.680]   So, again, this requires, like, the tiniest smidgen of post-processing, where you're saying, like,
[00:31:21.680 --> 00:31:22.680]   you just...
[00:31:22.680 --> 00:31:23.680]   It's like a reg X.
[00:31:23.680 --> 00:31:26.680]   You're just, like, take everything within the JSON tags and then use that.
[00:31:26.680 --> 00:31:28.680]   You can even combine these two techniques.
[00:31:28.680 --> 00:31:34.680]   So, you could say, here's the updated JSON.
[00:31:34.680 --> 00:31:35.680]   And now you give it the JSON tag.
[00:31:35.680 --> 00:31:37.680]   We can even put a bracket here.
[00:31:37.680 --> 00:31:41.680]   And now what we'll see is it will just give the JSON minus the bracket, and then it will
[00:31:41.680 --> 00:31:45.680]   close the bracket, and then it will close with the JSON tag.
[00:31:45.680 --> 00:31:46.680]   There we go.
[00:31:46.680 --> 00:31:47.680]   And it also gave this...
[00:31:47.680 --> 00:31:48.680]   So, you can see it did...
[00:31:48.680 --> 00:32:00.680]   At first, I was, like, a little bit panicked because I didn't see the close JSON tag at the
[00:32:00.680 --> 00:32:01.680]   very bottom.
[00:32:01.680 --> 00:32:06.680]   But then I saw that it actually did include the tag up here, and then it gave this little explanation
[00:32:06.680 --> 00:32:07.680]   afterwards.
[00:32:07.680 --> 00:32:09.680]   So, this is another useful thing.
[00:32:09.680 --> 00:32:14.680]   This will save you some time and tokens and trouble.
[00:32:14.680 --> 00:32:21.680]   One thing we could do, like, it costs you money to get Claude to output all this stuff.
[00:32:21.680 --> 00:32:24.680]   And you probably don't need it.
[00:32:24.680 --> 00:32:26.680]   In most cases, you don't need the explanation.
[00:32:26.680 --> 00:32:27.680]   You just need the JSON.
[00:32:27.680 --> 00:32:36.680]   So, one thing we could do is we could say, do not include any explanation after the JSON,
[00:32:36.680 --> 00:32:38.680]   ?
[00:32:38.680 --> 00:32:39.680]   I mean, probably.
[00:32:39.680 --> 00:32:40.680]   But I don't know.
[00:32:40.680 --> 00:32:41.680]   Honestly, I don't yell that much.
[00:32:41.680 --> 00:32:46.680]   I'm just, like, this is actually meant to be my parody of, like, what a frustrated prompt
[00:32:46.680 --> 00:32:49.680]   engineer would write if they were, like, couldn't get rid of this.
[00:32:49.680 --> 00:32:51.680]   But in practice, you might not need to do that.
[00:32:51.680 --> 00:32:55.680]   But the simpler way to do this, there's a...
[00:32:55.680 --> 00:32:58.680]   And we're getting outside the realm of prompt engineering for a second and into the world
[00:32:58.680 --> 00:33:01.680]   of, like, API parameters, but that's okay.
[00:33:01.680 --> 00:33:04.680]   There's a parameter that's called stop sequences.
[00:33:04.680 --> 00:33:05.680]   And if you set...
[00:33:05.680 --> 00:33:09.680]   So, we told it to return the JSON and JSON tags, right?
[00:33:09.680 --> 00:33:13.680]   So, there's no functionality to do this in the console, so I can't show it off at this
[00:33:13.680 --> 00:33:14.680]   exact moment.
[00:33:14.680 --> 00:33:17.680]   But in the API, there's a parameter called stop sequences.
[00:33:17.680 --> 00:33:26.680]   And if you add this close JSON tag that I've highlighted with my mouse, if you add that to the stop
[00:33:26.680 --> 00:33:30.680]   sequences, then it will just hard stop after it outputs those, and you don't even have to
[00:33:30.680 --> 00:33:34.680]   worry about telling it not to continue from there because it just won't even sample from
[00:33:34.680 --> 00:33:35.680]   there.
[00:33:35.680 --> 00:33:36.680]   You won't be charged.
[00:33:36.680 --> 00:33:43.680]   So, one of the things that I'm sort of hoping to impart with this talk is that a lot of times
[00:33:43.680 --> 00:33:51.680]   it's cheaper and easier to do a little bit of work outside of a call to the LOM and not even worry
[00:33:51.680 --> 00:33:56.680]   about prompting because prompting can feel sometimes like non-deterministic.
[00:33:56.680 --> 00:33:58.680]   You don't know what the model is going to do.
[00:33:58.680 --> 00:34:02.680]   So, when you can offload stuff to code, especially if the code is really easy to write, it's like,
[00:34:02.680 --> 00:34:03.680]   just do that, right?
[00:34:03.680 --> 00:34:08.680]   Like, don't put a bunch of stuff in the prompt about you must output JSON, just use the prefill,
[00:34:08.680 --> 00:34:11.680]   and then parse it out with the regex.
[00:34:11.680 --> 00:34:14.680]   You know, don't add a bunch of stuff about how you have to stop after you say a certain
[00:34:14.680 --> 00:34:17.680]   word, just add it to the stop sequences.
[00:34:17.680 --> 00:34:23.680]   So, like, simple is better, and falling back on code is better than relying on prompts.
[00:34:23.680 --> 00:34:24.680]   Yeah?
[00:34:24.680 --> 00:34:28.680]   Is the prefill available through the API?
[00:34:28.680 --> 00:34:29.680]   Yes.
[00:34:29.680 --> 00:34:31.680]   The prefill is available through the API.
[00:34:31.680 --> 00:34:37.680]   What you do is you include an assistant message as the last message in the messages list.
[00:34:37.680 --> 00:34:43.680]   And when I say an assistant message, I just mean a message where the role is set to assistant.
[00:34:43.680 --> 00:34:48.680]   And what would have happened if you had the text that you put in the prefill,
[00:34:48.680 --> 00:34:51.680]   you just put it into the last line of the instructions?
[00:34:51.680 --> 00:34:55.680]   So, in other words, if I said -- I'm actually not sure.
[00:34:55.680 --> 00:34:56.680]   That's a good question.
[00:34:56.680 --> 00:34:58.680]   So, let's actually try that.
[00:34:58.680 --> 00:35:06.680]   I genuinely do not know how Cloud will respond to this.
[00:35:06.680 --> 00:35:07.680]   So, let's see.
[00:35:07.680 --> 00:35:17.680]   So, it looks like what it did was -- it looks to me like what it did without looking at this
[00:35:17.680 --> 00:35:21.680]   JSON is it included an additional open bracket.
[00:35:21.680 --> 00:35:22.680]   Right?
[00:35:22.680 --> 00:35:27.680]   Because it's supposed to already have started with an open bracket, but here it started with
[00:35:27.680 --> 00:35:28.680]   an additional open bracket.
[00:35:28.680 --> 00:35:29.680]   So, it kind of almost worked, but not quite.
[00:35:29.680 --> 00:35:34.680]   Anyways, I don't recommend doing this, but that was fun just out of curiosity's sake.
[00:35:34.680 --> 00:35:35.680]   Yeah?
[00:35:35.680 --> 00:35:50.680]   So, after the sentence, we can just write it like -- you wrote, write -- written the JSON,
[00:35:50.680 --> 00:35:56.680]   written the response in the JSON format, and then in the next line, you can just write JSON,
[00:35:56.680 --> 00:35:57.680]   colon, then leave it.
[00:35:57.680 --> 00:35:58.680]   I think it will --
[00:35:58.680 --> 00:36:03.680]   Oh, so, like if I said something like this?
[00:36:03.680 --> 00:36:04.680]   Like this?
[00:36:04.680 --> 00:36:05.680]   Yeah, that's it.
[00:36:05.680 --> 00:36:07.680]   Yeah, I could see this working.
[00:36:07.680 --> 00:36:11.680]   Yeah, it looks like it worked pretty well.
[00:36:11.680 --> 00:36:15.680]   I think there's like a lot of ways to accomplish this.
[00:36:15.680 --> 00:36:17.680]   I think the ways that I showed are the most reliable.
[00:36:17.680 --> 00:36:20.680]   So, that's what I would like officially recommend.
[00:36:20.680 --> 00:36:23.680]   But yeah, like definitely experiment.
[00:36:23.680 --> 00:36:31.680]   If you were going to like try to use this for production or whatever, what -- like these
[00:36:31.680 --> 00:36:34.680]   exact kind of things you're playing around with right now, how would you think about testing
[00:36:34.680 --> 00:36:36.680]   that like at some sort of scale?
[00:36:36.680 --> 00:36:37.680]   Like --
[00:36:37.680 --> 00:36:39.680]   How do we test it at some sort of scale?
[00:36:39.680 --> 00:36:43.680]   Yeah, more than like one -- like the one-shot test we just did, right?
[00:36:43.680 --> 00:36:44.680]   Yeah, yeah, yeah.
[00:36:44.680 --> 00:36:49.680]   To test it at scale, you need a bunch of test cases.
[00:36:49.680 --> 00:36:58.680]   And if you don't have test cases, okay, this is maybe a good time to maybe show off this
[00:36:58.680 --> 00:37:00.680]   thing, although I'm actually not sure if it will work.
[00:37:00.680 --> 00:37:06.680]   So, I guess a more pointed question is like in this case, I think test cases are useful
[00:37:06.680 --> 00:37:11.680]   useful when I'm writing a prompt to deduce whether like does asking it to think step
[00:37:11.680 --> 00:37:13.680]   by step lead to this thing being more accurate.
[00:37:13.680 --> 00:37:19.680]   But in this case for formatting, I guess what I'm wondering is like could you have this prompt
[00:37:19.680 --> 00:37:25.680]   and then feed in the output and the prompt and then ask the model itself to evaluate like
[00:37:25.680 --> 00:37:28.680]   how good these various things are at following the instructions?
[00:37:28.680 --> 00:37:29.680]   Yeah, yeah.
[00:37:29.680 --> 00:37:30.680]   Okay, so can we do model grading?
[00:37:30.680 --> 00:37:31.680]   Can we model grade the outputs?
[00:37:31.680 --> 00:37:34.680]   Yeah, especially for formatting related things.
[00:37:34.680 --> 00:37:37.680]   For formatting, I would not model grade the outputs.
[00:37:37.680 --> 00:37:38.680]   Okay.
[00:37:38.680 --> 00:37:40.680]   Because formatting is something that I can check in code.
[00:37:40.680 --> 00:37:44.680]   So, if I can do anything in code and I don't have to call the LLM, the LLM is like this
[00:37:44.680 --> 00:37:45.680]   crazy black box, right?
[00:37:45.680 --> 00:37:50.680]   It's like if I don't need to like make this pilgrimage to the Oracle and like ask it, I'd rather
[00:37:50.680 --> 00:37:51.680]   just do it like in code.
[00:37:51.680 --> 00:37:56.680]   So, formatting specifically, we're kind of like in luck.
[00:37:56.680 --> 00:37:57.680]   It's easy to check.
[00:37:57.680 --> 00:38:01.680]   For something like the other, the previous prompt we looked at where the outputs are a lot more
[00:38:01.680 --> 00:38:05.680]   squishy, we might possibly a model grading could work.
[00:38:05.680 --> 00:38:08.680]   Possibly we might need a human to evaluate the answers.
[00:38:08.680 --> 00:38:10.680]   So, just to lightly push back on that.
[00:38:10.680 --> 00:38:11.680]   Yeah.
[00:38:11.680 --> 00:38:12.680]   I'm wondering like I actually put an example in the Slack channel.
[00:38:12.680 --> 00:38:14.680]   We don't need to get to it because we're talking through it now.
[00:38:14.680 --> 00:38:20.680]   But like for, let's imagine I don't have, or actually maybe tags are the answer to this.
[00:38:20.680 --> 00:38:25.680]   Like imagine I'm asking for a summary or something and then I want to deduce whether there's
[00:38:25.680 --> 00:38:28.680]   additional chat like content like before or after that.
[00:38:28.680 --> 00:38:34.680]   In that case would you, I would have, my mental model would have been to use like an LLM as
[00:38:34.680 --> 00:38:38.680]   a grader, but it sounds like maybe would you encourage instead using the summary tags and
[00:38:38.680 --> 00:38:42.680]   checking like hard coding for additional text around that?
[00:38:42.680 --> 00:38:43.680]   Yeah.
[00:38:43.680 --> 00:38:48.680]   I think that will be pretty quick and easy to do.
[00:38:48.680 --> 00:38:52.680]   Also just having the summary be in summary tags is like generally a good practice.
[00:38:52.680 --> 00:38:53.680]   Okay.
[00:38:53.680 --> 00:38:56.680]   I generally have all my outputs inside tags to make it really easy to extract them.
[00:38:56.680 --> 00:38:59.680]   I don't think there's really any downside to doing that.
[00:38:59.680 --> 00:39:05.680]   So, and it might even be that by doing that you effectively fix your entire issue and you
[00:39:05.680 --> 00:39:07.680]   don't even like need to do the test anymore.
[00:39:07.680 --> 00:39:11.680]   Or, and you just put close summary in the stop sequences and you're kind of good to go.
[00:39:11.680 --> 00:39:12.680]   Okay.
[00:39:12.680 --> 00:39:13.680]   Cool.
[00:39:13.680 --> 00:39:18.680]   But that also does sound like a problem that an LLM could grade.
[00:39:18.680 --> 00:39:19.680]   Okay.
[00:39:19.680 --> 00:39:24.680]   Let's go to the next prompt here.
[00:39:24.680 --> 00:39:26.680]   Just shout out your question.
[00:39:26.680 --> 00:39:30.680]   Here's a very poorly formatted Excel spreadsheet.
[00:39:30.680 --> 00:39:32.680]   Um, I got a question real quick.
[00:39:32.680 --> 00:39:38.680]   So, um, this seems like a really ridiculously like powerful attack vector.
[00:39:38.680 --> 00:39:41.680]   So, can we test the prompt real quick?
[00:39:41.680 --> 00:39:45.680]   Um, I don't want to get into too much like jail breaking stuff here.
[00:39:45.680 --> 00:39:46.680]   Sorry.
[00:39:46.680 --> 00:39:47.680]   Okay.
[00:39:47.680 --> 00:39:48.680]   Apologies.
[00:39:48.680 --> 00:39:49.680]   Yeah, yeah.
[00:39:49.680 --> 00:39:50.680]   That's kind of my specialty.
[00:39:50.680 --> 00:39:51.680]   Okay.
[00:39:51.680 --> 00:39:52.680]   Yeah.
[00:39:52.680 --> 00:39:54.680]   Um, I'm going to go to the next prompt.
[00:39:54.680 --> 00:39:59.680]   Um, so what do we have here?
[00:39:59.680 --> 00:40:04.680]   Here's a poorly formatted Excel spreadsheet slash CSV.
[00:40:04.680 --> 00:40:07.680]   Please extract all data into JSON.
[00:40:07.680 --> 00:40:08.680]   Okay.
[00:40:08.680 --> 00:40:12.680]   How can we, so, uh, Jan or Jan?
[00:40:12.680 --> 00:40:14.680]   Is it Jan or Jan?
[00:40:14.680 --> 00:40:15.680]   Jan.
[00:40:15.680 --> 00:40:18.680]   What, what is the actual text that I should?
[00:40:18.680 --> 00:40:20.680]   Can, can you, can you paste the text here?
[00:40:20.680 --> 00:40:24.680]   Because I don't know how to get the CSV into the, uh, into the console.
[00:40:24.680 --> 00:40:25.680]   I've just been, oh, hey, thanks.
[00:40:25.680 --> 00:40:29.680]   I've just been just copying the entire CSV.
[00:40:29.680 --> 00:40:32.680]   I'm putting that into the prompts.
[00:40:32.680 --> 00:40:38.680]   Again, I've been trying to use Claude to extract some information from spreadsheets.
[00:40:38.680 --> 00:40:40.680]   And it's always been very, very hard.
[00:40:40.680 --> 00:40:43.680]   It hallucinates a lot or it skips a lot of stuff.
[00:40:43.680 --> 00:40:49.680]   And I was wondering if you, maybe more generally, how do you have Claude analyze really poorly
[00:40:49.680 --> 00:40:54.680]   formatted spreadsheets to sometimes the different clusters or multiple data sets in the same sheet
[00:40:54.680 --> 00:40:56.680]   and things like that?
[00:40:56.680 --> 00:40:57.680]   Okay.
[00:40:57.680 --> 00:41:04.680]   I'll try to answer the general question of having, uh, analyzing poorly formatted spreadsheets.
[00:41:04.680 --> 00:41:09.680]   The first thing that came to mind, especially when you're talking about how the spreadsheets are very big,
[00:41:09.680 --> 00:41:15.680]   is breaking the problem down into, so, so give it, like, fewer spreadsheets at a time.
[00:41:15.680 --> 00:41:17.680]   Give it fewer columns of the spreadsheet at a time.
[00:41:17.680 --> 00:41:21.680]   Only give it the, the columns that it needs to, to work with.
[00:41:21.680 --> 00:41:26.680]   Um, and then make the questions sort of smaller and more bite-sized.
[00:41:26.680 --> 00:41:29.680]   And then tackle it that way by, by breaking it down.
[00:41:29.680 --> 00:41:36.680]   So, at that level of general, generality, that's, would be, would be my answer here.
[00:41:36.680 --> 00:41:41.680]   Um, I, I, I'd also be curious to look at this one more specifically.
[00:41:41.680 --> 00:41:45.680]   Right now I'm just struggling with how to, like, copy the text and put it into the tool.
[00:41:45.680 --> 00:41:50.680]   That's what I did, but it keeps downloading it.
[00:41:50.680 --> 00:41:57.680]   I guess I can, um, the next, sorry, the next tab.
[00:41:57.680 --> 00:42:00.680]   This is that other one.
[00:42:00.680 --> 00:42:03.680]   Sorry, I don't want to click open my downloads.
[00:42:03.680 --> 00:42:05.680]   I'm scared I'm going to, like, reveal some private information.
[00:42:05.680 --> 00:42:10.680]   This is my work computer, so, I want to just do it all on the browser.
[00:42:10.680 --> 00:42:16.680]   That's fine, you can go to the next one.
[00:42:16.680 --> 00:42:18.680]   Yeah, let's do the next one.
[00:42:18.680 --> 00:42:21.680]   Okay, you are social media ghostwriter.
[00:42:21.680 --> 00:42:27.680]   Giving the bow along for our article.
[00:42:27.680 --> 00:42:28.680]   Okay.
[00:42:28.680 --> 00:42:33.680]   Uh, generally we would recommend putting the, I like this one, it's short.
[00:42:33.680 --> 00:42:34.680]   We can do some quick hits here.
[00:42:34.680 --> 00:42:39.680]   Uh, we would recommend putting the instructions after the document.
[00:42:39.680 --> 00:42:44.680]   That's similar to the question that was asked about should we have the information first
[00:42:44.680 --> 00:42:46.680]   or the instructions first.
[00:42:46.680 --> 00:42:51.680]   Particularly with long documents, it's a little bit better to give the instructions at the end.
[00:42:51.680 --> 00:42:56.680]   Let's also put some XML tags here.
[00:42:56.680 --> 00:42:59.680]   Uh, let's just, like, clean up the grammar a little bit.
[00:42:59.680 --> 00:43:02.680]   Given the above long form article.
[00:43:02.680 --> 00:43:04.680]   Create five to ten tweets.
[00:43:04.680 --> 00:43:05.680]   Don't use hashtags.
[00:43:05.680 --> 00:43:07.680]   Don't be hyperbolic and don't be cringe.
[00:43:07.680 --> 00:43:09.680]   Uh, return a JSON array of posts content.
[00:43:09.680 --> 00:43:17.680]   Probably good to give an example, uh, of the format.
[00:43:17.680 --> 00:43:29.680]   Uh, so we could do, like, uh, I guess we can just do a return, like, a list.
[00:43:29.680 --> 00:43:31.680]   Wait, what did you originally have it?
[00:43:31.680 --> 00:43:36.680]   Is there a special reason that you wanted it to be a JSON array or is it just to make it
[00:43:36.680 --> 00:43:37.680]   parsable?
[00:43:37.680 --> 00:43:40.680]   Uh, just to make it parsable for automation.
[00:43:40.680 --> 00:43:41.680]   Okay, yeah.
[00:43:41.680 --> 00:43:53.680]   So let's say return in -- I'm just a huge fan of these tags, so let's do it like this.
[00:43:53.680 --> 00:44:22.680]   Okay, so that is -- that's -- that's some stuff without adding examples.
[00:44:22.680 --> 00:44:29.680]   The other thing that I would want to do is to give some illustrative examples of what
[00:44:29.680 --> 00:44:33.680]   it means to not be cringe.
[00:44:33.680 --> 00:44:36.680]   So, how long are these documents?
[00:44:36.680 --> 00:44:41.680]   Uh, I put an -- I used a recent one from the rocket.
[00:44:41.680 --> 00:44:43.680]   Perfect, yeah.
[00:44:43.680 --> 00:44:46.680]   Let's actually -- let's run this as is.
[00:44:46.680 --> 00:44:58.680]   Oh, yep.
[00:44:58.680 --> 00:44:59.680]   Thank you.
[00:44:59.680 --> 00:45:01.680]   Now we can take this.
[00:45:01.680 --> 00:45:05.680]   Uh, what if they write cringe tweets about our product?
[00:45:05.680 --> 00:45:06.680]   I'm going to be embarrassed.
[00:45:06.680 --> 00:45:20.680]   Okay, this doesn't include any hashtags.
[00:45:20.680 --> 00:45:22.680]   It doesn't seem very hyperbolic.
[00:45:22.680 --> 00:45:23.680]   Is this cringe?
[00:45:23.680 --> 00:45:24.680]   What do we think?
[00:45:24.680 --> 00:45:25.680]   New feature alert.
[00:45:25.680 --> 00:45:26.680]   That could be a little bit cringe.
[00:45:26.680 --> 00:45:27.680]   There's no emojis.
[00:45:27.680 --> 00:45:28.680]   It's a good sign.
[00:45:28.680 --> 00:45:29.680]   Okay.
[00:45:29.680 --> 00:45:30.680]   What do you -- your name was Charlie.
[00:45:30.680 --> 00:45:31.680]   What do you think of this, Charlie?
[00:45:31.680 --> 00:45:32.680]   Uh, I think they are adequate, but not engaging.
[00:45:32.680 --> 00:45:33.680]   Not engaging.
[00:45:33.680 --> 00:45:34.680]   Okay.
[00:45:34.680 --> 00:45:35.680]   So, yeah, yeah, yeah.
[00:45:35.680 --> 00:45:52.680]   So, we can try to make it more engaging without making it cringe.
[00:45:52.680 --> 00:45:56.680]   So, let's say -- don't give you hashtags, but cringe.
[00:45:56.680 --> 00:46:01.680]   Try to make the tweets engaging.
[00:46:01.680 --> 00:46:05.680]   Are these meant to be tweeted from the Anthropic Twitter account?
[00:46:05.680 --> 00:46:09.680]   Or from, like, the AI influencer Twitter account?
[00:46:09.680 --> 00:46:10.680]   Sure.
[00:46:10.680 --> 00:46:13.680]   Let's say AI influencer Twitter account.
[00:46:13.680 --> 00:46:14.680]   Okay.
[00:46:14.680 --> 00:46:14.680]   Let's see how this goes.
[00:46:14.680 --> 00:46:27.680]   I'm going to switch back to Dove, too.
[00:46:27.680 --> 00:46:28.680]   Sorry.
[00:46:28.680 --> 00:46:33.680]   Exciting news.
[00:46:33.680 --> 00:46:42.680]   Is it better to break it up into small sentences in the prompt, or can you use complex sentences?
[00:46:42.680 --> 00:46:46.680]   Is it better to break up sentences -- to use, like, small sentences in the prompt, or big
[00:46:46.680 --> 00:46:49.680]   sentences?
[00:46:49.680 --> 00:46:56.680]   I think, generally, in English writing, it's better to use small sentences in small words.
[00:46:56.680 --> 00:47:00.680]   So, I think it's probably also better to do that in a prompt.
[00:47:00.680 --> 00:47:04.680]   I think it's fine to use big words if you are really sure you know what you're doing, and
[00:47:04.680 --> 00:47:08.680]   you know that it's the exact right word for the situation.
[00:47:08.680 --> 00:47:13.680]   Sometimes, I'll find myself using more academic language if I want the output to seem a bit
[00:47:13.680 --> 00:47:15.680]   more academic.
[00:47:15.680 --> 00:47:21.680]   Generally, I think, simple, small sentences is better.
[00:47:21.680 --> 00:47:22.680]   Okay.
[00:47:22.680 --> 00:47:31.680]   So, these are maybe a little bit more engaging.
[00:47:31.680 --> 00:47:33.680]   Like, they have these questions here.
[00:47:33.680 --> 00:47:35.680]   Want to try it?
[00:47:35.680 --> 00:47:39.680]   What do we think?
[00:47:39.680 --> 00:47:41.680]   It's got exclamation points and question marks.
[00:47:41.680 --> 00:47:42.680]   Is it better?
[00:47:42.680 --> 00:47:45.680]   Do you want it to be even more engaging or something?
[00:47:45.680 --> 00:47:46.680]   Let's see.
[00:47:46.680 --> 00:47:47.680]   Okay.
[00:47:47.680 --> 00:47:55.680]   So, I honestly think temperature is a bit overrated, maybe.
[00:47:55.680 --> 00:48:12.680]   We can see how it differs, though.
[00:48:12.680 --> 00:48:17.680]   I'm not sure exactly how to distinguish these from the previous ones.
[00:48:17.680 --> 00:48:31.680]   They look kind of similar to me, from the ones with temperature one, or temperature zero.
[00:48:31.680 --> 00:48:32.680]   That's right.
[00:48:32.680 --> 00:48:33.680]   Yeah.
[00:48:33.680 --> 00:48:37.680]   So, what I was going to say is, I think this is roughly as far as you can take this without
[00:48:37.680 --> 00:48:38.680]   example.
[00:48:38.680 --> 00:48:44.680]   I think the best thing to improve this prompt would be either examples of the sort of tweets
[00:48:44.680 --> 00:48:52.680]   that you want, or even an entire other document, an example of tweets that go with that document,
[00:48:52.680 --> 00:48:55.680]   and maybe, like, multiple of those.
[00:48:55.680 --> 00:49:03.680]   So, if you're cost limited, maybe you don't want to put in all those input tokens every time.
[00:49:03.680 --> 00:49:08.680]   But, I don't know, the models are pretty cheap now, and we don't need to generate that many
[00:49:08.680 --> 00:49:09.680]   tweets.
[00:49:09.680 --> 00:49:15.680]   So, if they have, like, any economic value to you at all, it's probably pretty cost effective
[00:49:15.680 --> 00:49:16.680]   to put...
[00:49:16.680 --> 00:49:18.680]   So, basically, like...
[00:49:18.680 --> 00:49:21.680]   But it's more work on your part, because what you're doing then is...
[00:49:21.680 --> 00:49:22.680]   So...
[00:49:22.680 --> 00:49:23.680]   Okay.
[00:49:23.680 --> 00:49:28.680]   So, the way that I would actually do this is, I would start out with some document.
[00:49:28.680 --> 00:49:31.680]   I would have Claude write a bunch of tweets.
[00:49:31.680 --> 00:49:36.680]   I would take the ones that I liked, and maybe I would write some more, or get, like, my
[00:49:36.680 --> 00:49:37.680]   friend to write some more.
[00:49:37.680 --> 00:49:38.680]   Or maybe I'd have Claude...
[00:49:38.680 --> 00:49:39.680]   Oops.
[00:49:39.680 --> 00:49:44.680]   Claude generate a hundred tweets, and then I would take the seven that I liked best, and
[00:49:44.680 --> 00:49:47.680]   then I would put that in as an example.
[00:49:47.680 --> 00:49:52.680]   And then, from there, I would sample, okay, now here's another document, and then write a
[00:49:52.680 --> 00:49:54.680]   bunch more tweets based on this.
[00:49:54.680 --> 00:50:02.680]   And what I would do is iteratively build up this list of documents plus example tweets,
[00:50:02.680 --> 00:50:04.680]   and then I'd put them all into the prompt.
[00:50:04.680 --> 00:50:07.680]   And it would look something like this.
[00:50:07.680 --> 00:50:08.680]   So, let's actually do that.
[00:50:08.680 --> 00:50:09.680]   So, let's imagine that we had done this.
[00:50:09.680 --> 00:50:20.680]   So, it could be, like, you know, system prompt, "You are an AI influencer who writes engaging
[00:50:20.680 --> 00:50:28.680]   social media content about new models and releases."
[00:50:28.680 --> 00:50:48.680]   It could be, like, here are some example documents along with the tweets you wrote about them.
[00:50:48.680 --> 00:50:49.680]   And here you would actually...
[00:50:49.680 --> 00:51:03.680]   I'm gonna write this, but you would actually put the literal text of the document here.
[00:51:03.680 --> 00:51:04.680]   And here, again, you'd put a literal tweet here.
[00:51:04.680 --> 00:51:08.680]   And this could either be something that you wrote or something that Claude wrote, or, you
[00:51:08.680 --> 00:51:10.680]   know, something that Claude wrote and then you edited.
[00:51:10.680 --> 00:51:14.680]   Like, a lot of times, Claude might give you an example that's not perfect, but it's close
[00:51:14.680 --> 00:51:19.680]   enough, and then you'll change it a little bit to make it perfect.
[00:51:19.680 --> 00:51:27.680]   I have honestly given multi-shot examples pretty short shrift in this talk so far relative to
[00:51:27.680 --> 00:51:29.680]   their level of importance.
[00:51:29.680 --> 00:51:36.680]   Like, I think that, in reality, most of the gains, most of the effort, most of the gains
[00:51:36.680 --> 00:51:42.680]   of writing a good prompt is literally just picking the perfect document that goes here,
[00:51:42.680 --> 00:51:47.680]   picking the perfect set of tweets that go here, altering and changing them to modulate the
[00:51:47.680 --> 00:51:48.680]   tone.
[00:51:48.680 --> 00:51:54.680]   In some ways, that's more important than, like, everything else that I've said combined.
[00:51:54.680 --> 00:51:58.680]   Like, another way to do the whole JSON thing would just be, like, with examples of Claude
[00:51:58.680 --> 00:51:59.680]   giving the stuff without a preamble.
[00:51:59.680 --> 00:52:05.680]   The JSON one is maybe an exception because the prefill approach works so well there along with
[00:52:05.680 --> 00:52:09.680]   the tags, but for anything else, the examples are really huge.
[00:52:09.680 --> 00:52:21.680]   For a few-shot prompting, do you prefer to promote those all-in-one response like this?
[00:52:21.680 --> 00:52:27.680]   Or do you find further success with an exchange of messages between the agent and the user where
[00:52:27.680 --> 00:52:29.680]   you're putting your few-shot prompts in there?
[00:52:29.680 --> 00:52:30.680]   Yeah.
[00:52:30.680 --> 00:52:36.680]   Really good question and something that I would dearly love to know the answer to, but I don't.
[00:52:36.680 --> 00:52:39.680]   The question is -- I don't need to repeat the questions.
[00:52:39.680 --> 00:52:41.680]   I think people can hear them.
[00:52:41.680 --> 00:52:42.680]   But I'll repeat it anyway.
[00:52:42.680 --> 00:52:46.680]   So, do we want to just put all the examples in one big giant examples block like this?
[00:52:46.680 --> 00:52:51.680]   Or do we want to structure the examples as a dialogue where the human says something and
[00:52:51.680 --> 00:52:53.680]   then the assistant says something back?
[00:52:53.680 --> 00:52:57.680]   And we're literally, like, putting a large number of messages into the messages list.
[00:52:57.680 --> 00:53:04.680]   I typically do it this way with a big examples block, but it's mostly because it's less work
[00:53:04.680 --> 00:53:08.680]   for me, and I don't have any evidence that this works either better or worse.
[00:53:08.680 --> 00:53:12.680]   I did do some testing of this at one point on a few data sets, and I found that it didn't
[00:53:12.680 --> 00:53:15.680]   make much of a difference for my particular case.
[00:53:15.680 --> 00:53:20.680]   But there's a lot of, like, little particulars that went into my testing that make me not
[00:53:20.680 --> 00:53:22.680]   very confident in the result that I got.
[00:53:22.680 --> 00:53:25.680]   So, sorry for a bit of an unsatisfying answer here.
[00:53:25.680 --> 00:53:29.680]   I'll just say, I don't think -- if it is wrong to do one giant examples block, I don't think
[00:53:29.680 --> 00:53:31.680]   it's, like, very wrong.
[00:53:31.680 --> 00:53:34.680]   Do you use anti-examples too?
[00:53:34.680 --> 00:53:38.680]   So, like, in here, would you give it a thing and say, like, this would be bad because this
[00:53:38.680 --> 00:53:39.680]   is cringe?
[00:53:39.680 --> 00:53:40.680]   Yes.
[00:53:40.680 --> 00:53:41.680]   Yeah.
[00:53:41.680 --> 00:53:42.680]   I think that is good.
[00:53:42.680 --> 00:53:44.680]   I think it's good to include negative examples.
[00:53:44.680 --> 00:53:49.680]   Particularly around, like, the cringe thing where Claude might mess up.
[00:53:49.680 --> 00:53:55.680]   I think just negative examples on their own don't usually get you there.
[00:53:55.680 --> 00:53:58.680]   You want to have some positive examples too.
[00:53:58.680 --> 00:54:02.680]   But I think it's great to have, especially, like, contrasting pairs.
[00:54:02.680 --> 00:54:03.680]   So, like, here's a document.
[00:54:03.680 --> 00:54:06.680]   Like, here's a cringe tweet about this document.
[00:54:06.680 --> 00:54:09.680]   Here's an excellent tweet about the same document.
[00:54:09.680 --> 00:54:12.680]   And, like, set those up side by side.
[00:54:12.680 --> 00:54:13.680]   I think that's pretty powerful.
[00:54:13.680 --> 00:54:14.680]   And I do that.
[00:54:14.680 --> 00:54:15.680]   And I think it helps Claude.
[00:54:15.680 --> 00:54:18.680]   And then if you also include, like, the reasoning for it, right?
[00:54:18.680 --> 00:54:23.680]   So, like, if it was a cringe tweet, it has, like, a little reasoning of, like, why?
[00:54:23.680 --> 00:54:28.680]   Do you also, do you trust that reasoning for the model?
[00:54:28.680 --> 00:54:32.680]   So, like, if you ask it, like, hey, give me, like, what were you thinking when you were writing
[00:54:32.680 --> 00:54:33.680]   this tweet?
[00:54:33.680 --> 00:54:34.680]   And then write me this tweet.
[00:54:34.680 --> 00:54:35.680]   Yeah.
[00:54:35.680 --> 00:54:40.680]   When you're reading through your examples to choose the best ones, how much do you trust
[00:54:40.680 --> 00:54:45.680]   that reasoning and how much do you rely on that versus just, like, I just care about, like,
[00:54:45.680 --> 00:54:46.680]   the input/output?
[00:54:46.680 --> 00:54:47.680]   I don't trust the reasoning very much.
[00:54:47.680 --> 00:54:49.680]   Especially if it's after something the model already said.
[00:54:49.680 --> 00:54:50.680]   No.
[00:54:50.680 --> 00:54:51.680]   Then I, like, really don't trust it.
[00:54:51.680 --> 00:54:52.680]   Yeah.
[00:54:52.680 --> 00:54:56.680]   But, I mean, humans are not very good at explaining why we do the things that we do.
[00:54:56.680 --> 00:55:01.680]   We're really good at rationalizing and coming up with, like, fake reasons, but a lot of times
[00:55:01.680 --> 00:55:05.680]   we don't even know why we do the things that we did, let alone be able to coherently explain
[00:55:05.680 --> 00:55:06.680]   them to someone else.
[00:55:06.680 --> 00:55:12.680]   So, I, there's a subtlety here.
[00:55:12.680 --> 00:55:16.680]   So, something that does work pretty well is having the model think about its reasoning
[00:55:16.680 --> 00:55:20.680]   in advance and, like, go through different reasons or rationales for why it might choose
[00:55:20.680 --> 00:55:25.680]   one option or the other or think about what sort of things might go into good response.
[00:55:25.680 --> 00:55:30.680]   So, if I had the model do some thinking in advance before it gave the response, then I
[00:55:30.680 --> 00:55:35.680]   might just trust or assume that the response would be better.
[00:55:35.680 --> 00:55:41.680]   Having a bunch of explanation for why I did the thing after, probably, I would not trust
[00:55:41.680 --> 00:55:42.680]   that.
[00:55:42.680 --> 00:55:43.680]   Sorry, you had a question for a while.
[00:55:43.680 --> 00:55:55.680]   Do you, do you give your own reasoning that explains the examples?
[00:55:55.680 --> 00:55:56.680]   And if so, how do you make sure that the model doesn't get reasoning in advance?
[00:55:56.680 --> 00:55:58.680]   Do I give reasoning to explain the examples?
[00:55:58.680 --> 00:55:59.680]   Yes.
[00:55:59.680 --> 00:56:02.680]   I do a lot of giving, giving reasoning to explain the examples.
[00:56:02.680 --> 00:56:10.680]   So, for instance, just in this case, one thing that we could do here is, like, we could
[00:56:10.680 --> 00:56:17.680]   add something like, I was going to say tweet planning, but maybe it's, like, key points of
[00:56:17.680 --> 00:56:19.680]   document.
[00:56:19.680 --> 00:56:35.680]   And then here we have some key points, like the document presents the launch of.
[00:56:35.680 --> 00:56:38.680]   So, you would have this after the examples.
[00:56:38.680 --> 00:56:39.680]   So, if you have 10 examples.
[00:56:39.680 --> 00:56:41.680]   No, this is before the examples.
[00:56:41.680 --> 00:56:44.680]   Sorry, this is part of the example right here.
[00:56:44.680 --> 00:56:49.680]   So, I, in this particular example, in this example block, I gave it a document.
[00:56:49.680 --> 00:56:51.680]   Now I'm doing this, this key points business.
[00:56:51.680 --> 00:56:52.680]   Got it.
[00:56:52.680 --> 00:56:57.680]   And then I would have these tweets.
[00:56:57.680 --> 00:57:01.680]   Now this key points could be something that I wrote myself, or it could be something that
[00:57:01.680 --> 00:57:06.680]   Claude wrote, and then I'm, I'm editing it.
[00:57:06.680 --> 00:57:09.680]   Or if Claude did a perfect job, maybe I could just include the thing that Claude wrote.
[00:57:09.680 --> 00:57:14.680]   But now in order to get this, get Claude to do this, we would also say something like,
[00:57:14.680 --> 00:57:18.680]   return in this format, key points.
[00:57:18.680 --> 00:57:22.680]   A list of the key points from the document.
[00:57:22.680 --> 00:57:29.680]   So this is, like, a lightweight chain of thought where we're having the model do some thinking
[00:57:29.680 --> 00:57:30.680]   thinking in advance.
[00:57:30.680 --> 00:57:34.680]   And we also gave it examples of it doing the thinking in advance, like this.
[00:57:34.680 --> 00:57:38.680]   Yeah.
[00:57:38.680 --> 00:57:45.680]   Yeah.
[00:57:45.680 --> 00:57:48.680]   So, let's imagine we, like, really want to give examples like this.
[00:57:48.680 --> 00:57:51.680]   But we have a problem, which is that our documents are, like, super long.
[00:57:51.680 --> 00:57:53.680]   And I'm greedy and want to save on input tokens.
[00:57:53.680 --> 00:57:54.680]   Yeah.
[00:57:54.680 --> 00:57:59.680]   Would you err on the side of doing, like, one document but a really good example?
[00:57:59.680 --> 00:58:02.680]   Or doing, like, truncated versions of more documents?
[00:58:02.680 --> 00:58:05.680]   I would, that's a good question.
[00:58:05.680 --> 00:58:11.680]   I would err on the side of one extremely good example and not truncated versions of more documents.
[00:58:11.680 --> 00:58:17.680]   But I would also want to look at the outputs and test that assumption because it's possible
[00:58:17.680 --> 00:58:22.680]   that with only one example, Claude would fixate on aspects of the exact document that it uploaded
[00:58:22.680 --> 00:58:26.680]   and start trying to transfer them to your document.
[00:58:26.680 --> 00:58:27.680]   Right.
[00:58:27.680 --> 00:58:29.680]   So, I think it's one of those, it would be case by case.
[00:58:29.680 --> 00:58:30.680]   Right.
[00:58:30.680 --> 00:58:33.680]   But I would want to start with, like, having one extremely good example.
[00:58:33.680 --> 00:58:39.680]   Generally, I think that, like, less but, like, higher quality is a better way to go than,
[00:58:39.680 --> 00:58:40.680]   like, more and lower quality.
[00:58:40.680 --> 00:58:41.680]   Cool.
[00:58:41.680 --> 00:58:42.680]   Thank you.
[00:58:42.680 --> 00:58:43.680]   Okay.
[00:58:43.680 --> 00:58:44.680]   We have a lot of prompts here.
[00:58:44.680 --> 00:58:45.680]   Let's go to the...
[00:58:45.680 --> 00:58:46.680]   Okay.
[00:58:46.680 --> 00:58:47.680]   This is good.
[00:58:47.680 --> 00:58:54.680]   I was hoping we would get some, like, persona ones here.
[00:58:54.680 --> 00:58:55.680]   Okay.
[00:58:55.680 --> 00:59:07.680]   So, this looks like something where we're trying to get Claude to roleplay in these different
[00:59:07.680 --> 00:59:08.680]   protocols.
[00:59:08.680 --> 00:59:12.680]   So, let's try this out and let's see how it works.
[00:59:12.680 --> 00:59:15.680]   So, this looks something where we're going to have, like, a...
[00:59:15.680 --> 00:59:19.680]   This looks like it's, like, meant to be a multi-turn prompt, right?
[00:59:19.680 --> 00:59:22.680]   So, this is, like, a conversation.
[00:59:22.680 --> 00:59:27.680]   You are talking to an assistant.
[00:59:27.680 --> 00:59:31.680]   It said execute greater than P assist.
[00:59:31.680 --> 00:59:32.680]   Where's that?
[00:59:32.680 --> 00:59:36.680]   So, basically, at the top, you see three roles at the top.
[00:59:36.680 --> 00:59:40.680]   And then you can decide who you want to talk to.
[00:59:40.680 --> 00:59:41.680]   Oh, thank you so much.
[00:59:41.680 --> 00:59:43.680]   So, you see three roles at the top.
[00:59:43.680 --> 00:59:44.680]   Yeah.
[00:59:44.680 --> 00:59:48.680]   And then if you do that P assist, you see down there that's highlighted in yellow.
[00:59:48.680 --> 00:59:49.680]   Yeah.
[00:59:49.680 --> 00:59:55.680]   You just do that little arrow P and then you can pick a different persona.
[00:59:55.680 --> 00:59:56.680]   Yeah.
[00:59:56.680 --> 00:59:58.680]   And then you can have them talk between themselves or you can just switch.
[00:59:58.680 --> 01:00:06.680]   We use this for designers in our shop to do synthetic interviews to synthetic users, basically.
[01:00:06.680 --> 01:00:07.680]   Got you.
[01:00:07.680 --> 01:00:08.680]   It allows us to switch back and forth.
[01:00:08.680 --> 01:00:13.680]   And then what issues or troubles have you been having with this?
[01:00:13.680 --> 01:00:17.680]   I'm guessing you have seen a lot of role-playing prompts out there.
[01:00:17.680 --> 01:00:29.680]   So, I was just wondering if you see anything that's perhaps not as optimized as it could be or any other best practices for role-playing, particularly with multiple synthetic personas within the same session.
[01:00:29.680 --> 01:00:30.680]   Yeah.
[01:00:30.680 --> 01:00:31.680]   Okay.
[01:00:31.680 --> 01:00:36.680]   For single personas, there's one answer that I would give.
[01:00:36.680 --> 01:00:39.680]   This multiple personas thing, actually, I haven't worked that much with.
[01:00:39.680 --> 01:00:44.680]   But off the top of my head, here is probably how I would think about it.
[01:00:44.680 --> 01:01:00.680]   I would give all the personas to -- I would write a separate prompt for each persona and then I would have the user's command trigger some coding logic where it would decide which bot to send the -- which prompt to send that reply to.
[01:01:00.680 --> 01:01:04.680]   So, this is getting back to the thing that I said before about, like, don't do it in a prompt if you don't have to.
[01:01:04.680 --> 01:01:13.680]   Like, this -- I mean, this prompt, like, is -- like, there's a lot of thought that went into it, which is -- probably makes it work a lot better than it would have if you hadn't put as much effort into it.
[01:01:13.680 --> 01:01:20.680]   But I think it's going to be easier if you just dynamically route the query based on what the user said.
[01:01:20.680 --> 01:01:21.680]   Does that make sense?
[01:01:21.680 --> 01:01:22.680]   It does.
[01:01:22.680 --> 01:01:23.680]   Okay.
[01:01:23.680 --> 01:01:28.680]   You're talking about, like, if you were to use the API instead of just the chat --
[01:01:28.680 --> 01:01:29.680]   Yeah.
[01:01:29.680 --> 01:01:30.680]   -- construct something like this, right?
[01:01:30.680 --> 01:01:31.680]   Yeah, exactly.
[01:01:31.680 --> 01:01:32.680]   But you're doing this just in the chat.
[01:01:32.680 --> 01:01:37.680]   This was just in the chat, but I appreciate -- I definitely appreciate the note there.
[01:01:37.680 --> 01:01:52.680]   So, maybe related to that, one of the other things is how much have you dealt with having a second thread with the API that acts as maybe the entity that's capturing inputs from multiple ones
[01:01:52.680 --> 01:01:53.680]   into a single thread.
[01:01:53.680 --> 01:01:54.680]   You know what I mean?
[01:01:54.680 --> 01:02:08.680]   Like, let's say that I build an app, and I have the user interact with these different synthetic personas, but then I have a second interaction with the API that's tying these things together into a cohesive whole.
[01:02:08.680 --> 01:02:11.680]   I don't know if you guys have explored some of that.
[01:02:11.680 --> 01:02:12.680]   I'll be curious.
[01:02:12.680 --> 01:02:13.680]   Yeah.
[01:02:13.680 --> 01:02:17.680]   I don't have a great answer for that one.
[01:02:17.680 --> 01:02:18.680]   Sorry.
[01:02:18.680 --> 01:02:21.680]   I do want to kind of test this prompt out, though, just to kind of see how it goes.
[01:02:21.680 --> 01:02:22.680]   Yeah.
[01:02:22.680 --> 01:02:28.680]   So, maybe here I would say -- I can just say something like -- so, how would I switch it?
[01:02:28.680 --> 01:02:29.680]   I could --
[01:02:29.680 --> 01:02:32.680]   So, do the right arrow P?
[01:02:32.680 --> 01:02:33.680]   Yeah.
[01:02:33.680 --> 01:02:34.680]   Right arrow P.
[01:02:34.680 --> 01:02:35.680]   And then type Sam.
[01:02:35.680 --> 01:02:35.680]   And then say, hey -- yeah.
[01:02:35.680 --> 01:02:36.680]   Okay.
[01:02:36.680 --> 01:02:37.680]   So, now I could --
[01:02:37.680 --> 01:02:51.680]   You could say, hey, how do you -- what's your process to look for the right -- for the best medication pricing whenever you get sick or something like that?
[01:02:51.680 --> 01:02:59.680]   And then here in this particular case, if you switch to Joe, Joe is optimized more for convenience versus cost savings.
[01:02:59.680 --> 01:03:02.680]   So, you have two different types of users and we can learn from.
[01:03:02.680 --> 01:03:03.680]   Yeah.
[01:03:03.680 --> 01:03:04.680]   Okay.
[01:03:04.680 --> 01:03:08.680]   So, Quad did the thing here that I want to show you all how to get rid of.
[01:03:08.680 --> 01:03:09.680]   Oh, yes.
[01:03:09.680 --> 01:03:10.680]   Yes.
[01:03:10.680 --> 01:03:15.680]   As Sam, it's like -- that's not something that Sam would say, right?
[01:03:15.680 --> 01:03:16.680]   Yeah.
[01:03:16.680 --> 01:03:20.680]   So, I don't know for sure this is going to work.
[01:03:20.680 --> 01:03:24.680]   I feel like a magician that's about to do a trick but, like, I haven't practiced it.
[01:03:24.680 --> 01:03:39.680]   But, generally, something that is pretty useful here is to -- we could say, prepend each response with the name of the current persona in brackets.
[01:03:39.680 --> 01:04:03.680]   So, one thing I'm going to do here is I'm going to change this, like, multi-shot a little bit also because if Quad sees itself not doing the thing that I told it to do -- actually, let's just redo the whole conversation.
[01:04:03.680 --> 01:04:05.680]   Or we can take out this.
[01:04:05.680 --> 01:04:09.680]   So, let's just, like, run that back.
[01:04:09.680 --> 01:04:10.680]   You are talking to assistant.
[01:04:10.680 --> 01:04:11.680]   Nice.
[01:04:11.680 --> 01:04:25.680]   And now we could say -- and now we could say the same thing, like, what's your process for finding best prices for medication?
[01:04:25.680 --> 01:04:32.680]   Oh.
[01:04:32.680 --> 01:04:33.680]   Okay.
[01:04:33.680 --> 01:04:35.680]   So, I guess we need to do this.
[01:04:35.680 --> 01:04:38.680]   We need to, like, change in a separate call.
[01:04:38.680 --> 01:04:39.680]   Yeah.
[01:04:39.680 --> 01:04:40.680]   Okay.
[01:04:40.680 --> 01:04:41.680]   Great.
[01:04:41.680 --> 01:04:44.680]   Now it's going to work.
[01:04:44.680 --> 01:04:45.680]   Totally.
[01:04:45.680 --> 01:04:47.680]   It's going to totally work.
[01:04:47.680 --> 01:04:48.680]   Okay.
[01:04:48.680 --> 01:04:50.680]   It's a little bit better, right?
[01:04:50.680 --> 01:04:51.680]   It didn't say as Sam.
[01:04:51.680 --> 01:04:54.680]   This is, like, something that a human might maybe say.
[01:04:54.680 --> 01:04:57.680]   Like, as someone who's -- okay.
[01:04:57.680 --> 01:04:59.680]   I don't know.
[01:04:59.680 --> 01:05:03.680]   It's better than it was before, right?
[01:05:03.680 --> 01:05:06.680]   Maybe we could say something like --
[01:05:06.680 --> 01:05:18.680]   You don't need to say too much about your persona and your responses.
[01:05:18.680 --> 01:05:21.680]   Just stay in character.
[01:05:21.680 --> 01:05:22.680]   Hey, quick question.
[01:05:22.680 --> 01:05:25.680]   What are your thoughts on using things in the negative sense versus --
[01:05:25.680 --> 01:05:26.680]   Yo, check it out.
[01:05:26.680 --> 01:05:27.680]   It worked a lot better.
[01:05:27.680 --> 01:05:28.680]   Sorry to interrupt.
[01:05:28.680 --> 01:05:29.680]   Oh, yeah.
[01:05:29.680 --> 01:05:30.680]   Very nice.
[01:05:30.680 --> 01:05:31.680]   Very nice.
[01:05:31.680 --> 01:05:35.680]   Yeah, so --
[01:05:35.680 --> 01:05:36.680]   Yeah.
[01:05:36.680 --> 01:05:40.680]   What are your thoughts on using, like, negative stuff, like, you don't versus the positive sense?
[01:05:40.680 --> 01:05:41.680]   Yeah.
[01:05:41.680 --> 01:05:43.680]   I think positive is, like, a little bit better.
[01:05:43.680 --> 01:05:48.680]   In this case, I don't really have a good answer for why I phrased this negatively.
[01:05:48.680 --> 01:05:49.680]   I guess I did a combination.
[01:05:49.680 --> 01:05:51.680]   I was like, you don't need to say too much.
[01:05:51.680 --> 01:05:53.680]   Just stay in character.
[01:05:53.680 --> 01:05:56.680]   I guess I think it's better to use, like, a light touch.
[01:05:56.680 --> 01:05:58.680]   Like, if you're doing negative prompting.
[01:05:58.680 --> 01:06:01.680]   Like, I think there's, like, a little bit of a thing going on with reverse psychology where
[01:06:01.680 --> 01:06:04.680]   if you tell the model, like, don't talk about elephants.
[01:06:04.680 --> 01:06:06.680]   Don't -- definitely no elephants.
[01:06:06.680 --> 01:06:07.680]   Definitely don't say anything about an elephant.
[01:06:07.680 --> 01:06:09.680]   It might make it more likely to talk about an elephant.
[01:06:09.680 --> 01:06:13.680]   So, if you do use negative prompting, I think it's better to have, like, a light touch
[01:06:13.680 --> 01:06:17.680]   where you just kind of say it once but, like, don't dwell on it too much.
[01:06:17.680 --> 01:06:20.680]   Also, something similar with parenting.
[01:06:20.680 --> 01:06:22.680]   It's like if you don't want your kid to eat prunes, you're just like, oh, we're not having
[01:06:22.680 --> 01:06:24.680]   prunes today, and then you just change the subject.
[01:06:24.680 --> 01:06:29.680]   But if you really, like, emphasize that there are, like, no prunes to be had, then you might
[01:06:29.680 --> 01:06:31.680]   get more pushback.
[01:06:31.680 --> 01:06:32.680]   Hi there.
[01:06:32.680 --> 01:06:35.680]   I noticed you're not using the system prompt much.
[01:06:35.680 --> 01:06:36.680]   Like, is there a reason for that?
[01:06:36.680 --> 01:06:40.680]   Or what do you think the biggest value items for a system prompt are?
[01:06:40.680 --> 01:06:41.680]   Yeah, system prompt.
[01:06:41.680 --> 01:06:45.680]   Personally, the only thing that I ever put in the system prompt is a role.
[01:06:45.680 --> 01:06:48.680]   So I might say, like, you are this, you are that.
[01:06:48.680 --> 01:06:52.680]   I think, generally, Claude follows instructions a little bit better if they're in the human
[01:06:52.680 --> 01:06:55.680]   prompt and not in the system prompt.
[01:06:55.680 --> 01:07:00.680]   The exception is things like tool use, where maybe there's been some explicit fine tuning
[01:07:00.680 --> 01:07:03.680]   on, like, certain system prompts specifically.
[01:07:03.680 --> 01:07:07.680]   For, like, general prompts like the ones we've been going over here, though, I don't really
[01:07:07.680 --> 01:07:11.680]   think you need to use the system prompt very much.
[01:07:11.680 --> 01:07:12.680]   Yeah.
[01:07:12.680 --> 01:07:22.680]   One thing we've found when using the user prompt, I guess, sometimes is it makes it more prone
[01:07:22.680 --> 01:07:25.680]   to hallucinations because it thinks the user is saying it.
[01:07:25.680 --> 01:07:28.680]   And so we migrated things to the system prompt more.
[01:07:28.680 --> 01:07:30.680]   I don't know if you have any experience with that.
[01:07:30.680 --> 01:07:31.680]   Yeah.
[01:07:31.680 --> 01:07:32.680]   Yeah.
[01:07:32.680 --> 01:07:33.680]   I've actually heard that before.
[01:07:33.680 --> 01:07:35.680]   So it's possible I'm missing something.
[01:07:35.680 --> 01:07:38.680]   I've heard this from enough people that I could just be wrong.
[01:07:38.680 --> 01:07:41.680]   So I'm unusually likely to be wrong when I say this.
[01:07:41.680 --> 01:07:45.680]   I think that if you just put a bunch of context and you're like, here's the message from the
[01:07:45.680 --> 01:07:46.680]   user.
[01:07:46.680 --> 01:07:47.680]   Open user bracket.
[01:07:47.680 --> 01:07:48.680]   And then put the message.
[01:07:48.680 --> 01:07:49.680]   And then close the user bracket.
[01:07:49.680 --> 01:07:54.680]   It will work and you won't have that issue anymore.
[01:07:54.680 --> 01:07:56.680]   That said, like, I don't know.
[01:07:56.680 --> 01:07:59.680]   Maybe it does fall over sometimes.
[01:07:59.680 --> 01:08:03.680]   But that would be my default is just to, like, specify even more clearly.
[01:08:03.680 --> 01:08:06.680]   And if you're having this issue, be like, here's the message from the user.
[01:08:06.680 --> 01:08:08.680]   Here's the stuff that I want you to do.
[01:08:08.680 --> 01:08:12.680]   And I think it probably won't get confused by that.
[01:08:12.680 --> 01:08:15.680]   I have a question about the counterexamples.
[01:08:15.680 --> 01:08:23.680]   So before, in order to get it to say not cringy things, you were saying provide it with a counterexample.
[01:08:23.680 --> 01:08:30.680]   But here in the case of where you're doing this character bot, you haven't provided it any counterexamples.
[01:08:30.680 --> 01:08:33.680]   So this is sort of like a generic question.
[01:08:33.680 --> 01:08:44.680]   So if the model is trained on preference optimization with examples and counterexamples, do you get a better result in the prompting?
[01:08:44.680 --> 01:08:50.680]   Well, I don't know that the details of the RLHF have that much bearing.
[01:08:50.680 --> 01:08:57.680]   Because I think when the model is trained, it doesn't usually see those both in the same, like, window.
[01:08:57.680 --> 01:09:02.680]   It's more that it's, like, some stuff that happens with, like, the RL algorithms.
[01:09:02.680 --> 01:09:05.680]   I don't think that's necessarily the right way to think of it.
[01:09:05.680 --> 01:09:08.680]   With counterexamples, I don't feel that I have to include them in every prompt.
[01:09:08.680 --> 01:09:12.680]   It's just a tool that I have in my toolbox that I'd use sometimes.
[01:09:12.680 --> 01:09:17.680]   In regards to, like, negative prompting -- I'm over here. Hi.
[01:09:17.680 --> 01:09:27.680]   Do you think that it would be better to do negative prompting using control vectors, like what you talked about in your scaling monosemanticity paper?
[01:09:27.680 --> 01:09:28.680]   Yeah.
[01:09:28.680 --> 01:09:34.680]   And maybe, like, having, like, a negative version of the vector as your kind of negative prompt instead of mentioning it in the prompt outright?
[01:09:34.680 --> 01:09:35.680]   Yeah.
[01:09:35.680 --> 01:09:38.680]   Steering is still, like, super new.
[01:09:38.680 --> 01:09:41.680]   We don't know how well it works relative to prompting.
[01:09:41.680 --> 01:09:46.680]   I'm, like, a, you know, die-hard prompter till the end, so.
[01:09:46.680 --> 01:09:48.680]   I've played around with it a little bit.
[01:09:48.680 --> 01:09:53.680]   I haven't found it to work as well as prompting in my experience so far.
[01:09:53.680 --> 01:10:00.680]   That said, there's, like, a lot of research improvements that I won't get into in too much detail, but there's a lot of stuff that could make it work better than --
[01:10:00.680 --> 01:10:09.680]   So, like, right now, it's, like, finding these features, and then you're steering according to the features, which are sort of, like, these abstractions on top of the underlying vector space.
[01:10:09.680 --> 01:10:10.680]   Yep.
[01:10:10.680 --> 01:10:23.680]   There's other possibilities for how you could steer, and there's, like, academic papers that you can read where you're steering according to just, like, the differences in the activations versus, like, trying to pull it out to this feature first.
[01:10:23.680 --> 01:10:26.680]   So, maybe that would work a bit better, like, the control vectors thing.
[01:10:26.680 --> 01:10:27.680]   Yep.
[01:10:27.680 --> 01:10:34.680]   I haven't played with it enough to know for sure, but I think there's definitely, like, something along those lines will work eventually.
[01:10:34.680 --> 01:10:37.680]   I can't say in the long term if it'll work better or worse than prompting.
[01:10:37.680 --> 01:10:40.680]   Right now, I still think prompting works, like, a lot better.
[01:10:40.680 --> 01:10:50.680]   I mean, from my experience with, like, smaller models and trying to work with control vectors, I've seen that it's better when it comes to style than it is for, like, actual deterministic prompting.
[01:10:50.680 --> 01:10:51.680]   Yeah, pretty interesting.
[01:10:51.680 --> 01:10:52.680]   Yeah.
[01:10:52.680 --> 01:10:54.680]   Sometimes I feel like stuff from smaller models transfers.
[01:10:54.680 --> 01:10:55.680]   Sometimes it doesn't transfer.
[01:10:55.680 --> 01:11:01.680]   I don't have a great intuition for what does and doesn't transfer between small and large models, but, yeah, good points.
[01:11:01.680 --> 01:11:02.680]   Thank you.
[01:11:02.680 --> 01:11:03.680]   Okay.
[01:11:03.680 --> 01:11:07.680]   I think we've gone over this roleplay stuff enough.
[01:11:07.680 --> 01:11:08.680]   Let's go to the next one.
[01:11:08.680 --> 01:11:12.680]   I'm going to upload a few screenshots of my dating profile.
[01:11:12.680 --> 01:11:13.680]   Okay.
[01:11:13.680 --> 01:11:14.680]   This is our first image one.
[01:11:14.680 --> 01:11:17.680]   Are there any screenshots?
[01:11:17.680 --> 01:11:20.680]   No screenshots.
[01:11:20.680 --> 01:11:21.680]   Okay.
[01:11:21.680 --> 01:11:24.680]   Actually, somebody responded in the very first in reply.
[01:11:24.680 --> 01:11:28.680]   So, since we're doing images, maybe I'll start there.
[01:11:28.680 --> 01:11:31.680]   If you scroll down to that, that was me.
[01:11:31.680 --> 01:11:32.680]   That was you.
[01:11:32.680 --> 01:11:35.680]   Let me try and find my message here.
[01:11:35.680 --> 01:11:36.680]   All right.
[01:11:36.680 --> 01:11:37.680]   You said it's at the bottom of the...
[01:11:37.680 --> 01:11:52.680]   Riveting to watch me scroll through this channel, I'm sure.
[01:11:52.680 --> 01:12:02.680]   There's a lot of messages here.
[01:12:02.680 --> 01:12:03.680]   Okay.
[01:12:03.680 --> 01:12:07.680]   Here we go.
[01:12:07.680 --> 01:12:08.680]   Okay.
[01:12:08.680 --> 01:12:09.680]   So, let's...
[01:12:09.680 --> 01:12:12.680]   Can I copy the image?
[01:12:12.680 --> 01:12:13.680]   Copy image.
[01:12:13.680 --> 01:12:14.680]   Okay.
[01:12:14.680 --> 01:12:15.680]   Cool.
[01:12:15.680 --> 01:12:18.680]   I actually don't know if I can paste it into the console.
[01:12:18.680 --> 01:12:20.680]   So, I might fall back on using call.ai for this.
[01:12:20.680 --> 01:12:26.680]   Pasting images is not enabled right now.
[01:12:26.680 --> 01:12:27.680]   Okay.
[01:12:27.680 --> 01:12:28.680]   Let's try call.ai then.
[01:12:34.680 --> 01:12:35.680]   Okay.
[01:12:35.680 --> 01:12:36.680]   So, then this...
[01:12:36.680 --> 01:12:37.680]   The question was...
[01:12:37.680 --> 01:12:38.680]   The prompt here was...
[01:12:38.680 --> 01:12:51.680]   High-performing validated AI model.
[01:12:51.680 --> 01:12:52.680]   Sorry.
[01:12:52.680 --> 01:12:57.680]   I, like, lost all your formatting here.
[01:12:57.680 --> 01:12:58.680]   Okay.
[01:12:58.680 --> 01:13:04.680]   So, in this image here, if we zoom in, it's supposed to get Maddie white.
[01:13:04.680 --> 01:13:05.680]   And...
[01:13:05.680 --> 01:13:06.680]   Eight, six...
[01:13:06.680 --> 01:13:07.680]   Hmm.
[01:13:07.680 --> 01:13:09.680]   I'm having a hard time reading this.
[01:13:09.680 --> 01:13:10.680]   Eight, six, eighty-seven.
[01:13:10.680 --> 01:13:12.680]   That's the hard one that it messes up.
[01:13:12.680 --> 01:13:13.680]   Eight, six, eighty-seven.
[01:13:13.680 --> 01:13:15.680]   And then eight, six, eighty-seven down here.
[01:13:15.680 --> 01:13:17.680]   And then in this one, it's just eight, six, seven.
[01:13:17.680 --> 01:13:18.680]   Yeah.
[01:13:18.680 --> 01:13:19.680]   They...
[01:13:19.680 --> 01:13:20.680]   That's a typo.
[01:13:20.680 --> 01:13:21.680]   That's a typo.
[01:13:21.680 --> 01:13:22.680]   Probably the human.
[01:13:22.680 --> 01:13:23.680]   And so, I'm hoping it can correct for that.
[01:13:23.680 --> 01:13:26.680]   I'm just trying to pull out kind of the average data from all three combined.
[01:13:26.680 --> 01:13:27.680]   Okay.
[01:13:27.680 --> 01:13:28.680]   And it said...
[01:13:28.680 --> 01:13:33.680]   It looks like it said middle mark.
[01:13:33.680 --> 01:13:34.680]   So, it's misreading the...
[01:13:34.680 --> 01:13:35.680]   Okay.
[01:13:35.680 --> 01:13:40.680]   So, I don't know too many good tips for image is.
[01:13:40.680 --> 01:13:42.680]   But I'll tell you what I have.
[01:13:42.680 --> 01:13:44.680]   So, one of the things that you said...
[01:13:44.680 --> 01:13:45.680]   Oops.
[01:13:45.680 --> 01:13:46.680]   Sorry.
[01:13:46.680 --> 01:13:54.680]   One of the things that you said here was that it works better with zooming in and cropping
[01:13:54.680 --> 01:13:55.680]   image.
[01:13:55.680 --> 01:14:00.680]   That's definitely like the easiest win that you can have is just giving it like a higher
[01:14:00.680 --> 01:14:03.680]   quality image, taking out the unnecessary details.
[01:14:03.680 --> 01:14:07.680]   That might be hard to do programmatically because it's like you don't know what the...
[01:14:07.680 --> 01:14:09.680]   Which details are necessary and unnecessary.
[01:14:09.680 --> 01:14:14.680]   But for the same reason that including extraneous information in text form, you probably won't
[01:14:14.680 --> 01:14:15.680]   get as good results.
[01:14:15.680 --> 01:14:20.680]   If you include extraneous information in image form, the results probably won't be as good
[01:14:20.680 --> 01:14:21.680]   either.
[01:14:21.680 --> 01:14:26.680]   So, the more that you can like narrow in on the exact information that you need, the better.
[01:14:26.680 --> 01:14:30.680]   Is the model down sampling large images?
[01:14:30.680 --> 01:14:32.680]   I don't know slash can't talk about that.
[01:14:32.680 --> 01:14:36.680]   But definitely having like higher quality bigger images is better.
[01:14:36.680 --> 01:14:39.680]   I did just read on your website that it says...
[01:14:39.680 --> 01:14:42.680]   It down samples to a thousand pixels by a thousand pixels.
[01:14:42.680 --> 01:14:43.680]   Okay.
[01:14:43.680 --> 01:14:44.680]   Great.
[01:14:44.680 --> 01:14:45.680]   That can be found with Google.
[01:14:45.680 --> 01:14:46.680]   Okay.
[01:14:46.680 --> 01:14:51.680]   So, then any general tips on how to discuss images with Solnit?
[01:14:51.680 --> 01:14:56.680]   My number one tip for images is to start by having the model describe everything it sees
[01:14:56.680 --> 01:14:58.680]   about the image.
[01:14:58.680 --> 01:15:00.680]   So, I don't know if that will work here.
[01:15:00.680 --> 01:15:01.680]   This example is like...
[01:15:01.680 --> 01:15:05.680]   This one is hard enough for me to even read that I kind of doubt that quad will do well
[01:15:05.680 --> 01:15:07.680]   on it regardless of what we say.
[01:15:07.680 --> 01:15:09.680]   But we can give it another shot.
[01:15:09.680 --> 01:15:15.680]   One thing I've noticed when I attempted that where if I asked it to go like tube by tube in
[01:15:15.680 --> 01:15:17.680]   that image, if it...
[01:15:17.680 --> 01:15:21.680]   Like the first tube, if it came to the wrong conclusion, it would use that and come to
[01:15:21.680 --> 01:15:24.680]   that same wrong conclusion on multiple tubes.
[01:15:24.680 --> 01:15:28.680]   Where like it made it so there was some kind of like directionality in its thinking where
[01:15:28.680 --> 01:15:33.680]   if it got the answer wrong at first, it would project that onto the rest of the image it was analyzed.
[01:15:33.680 --> 01:15:34.680]   Yes.
[01:15:34.680 --> 01:15:35.680]   I think that's definitely true.
[01:15:35.680 --> 01:15:39.680]   If the model starts off on the wrong path, it probably will just continue going on the wrong path.
[01:15:39.680 --> 01:15:41.680]   It's trying really hard to be self-consistent.
[01:15:41.680 --> 01:15:42.680]   Mm-hm.
[01:15:42.680 --> 01:15:43.680]   And...
[01:15:43.680 --> 01:15:44.680]   It's...
[01:15:44.680 --> 01:15:45.680]   It's...
[01:15:45.680 --> 01:15:52.680]   And this is why self-correction is such a big like frontier for LMs in general right now.
[01:15:52.680 --> 01:15:56.680]   But as of this month, why use those for this path?
[01:15:56.680 --> 01:16:01.680]   So, I love to use those, but why for this specific image?
[01:16:01.680 --> 01:16:07.680]   I found like text track and OCR models I played with don't do as well with some of the handwriting.
[01:16:07.680 --> 01:16:11.680]   Like if I zoom in on this image, it actually does perform pretty well.
[01:16:11.680 --> 01:16:16.680]   from some of these fairly messy handwriting that's even hard for a human.
[01:16:16.680 --> 01:16:17.680]   Okay.
[01:16:17.680 --> 01:16:18.680]   Yeah.
[01:16:18.680 --> 01:16:22.680]   Unfortunately, I don't know how to get better results out of this other than maybe by cropping
[01:16:22.680 --> 01:16:23.680]   it better and up sampling.
[01:16:23.680 --> 01:16:24.680]   Okay.
[01:16:24.680 --> 01:16:25.680]   Cool.
[01:16:25.680 --> 01:16:26.680]   Thank you.
[01:16:26.680 --> 01:16:27.680]   Oops.
[01:16:27.680 --> 01:16:32.680]   Here we are.
[01:16:32.680 --> 01:16:33.680]   Here we are.
[01:16:33.680 --> 01:16:39.680]   So let's scroll up to where we were before.
[01:16:39.680 --> 01:16:40.680]   Okay.
[01:16:40.680 --> 01:16:41.680]   Yeah.
[01:16:41.680 --> 01:16:42.680]   Still no screenshots there.
[01:16:42.680 --> 01:16:55.680]   How do I enable fragments?
[01:16:55.680 --> 01:16:57.680]   What are fragments?
[01:16:57.680 --> 01:17:02.680]   Is fragments like the pre-fill part?
[01:17:02.680 --> 01:17:03.680]   The pre...
[01:17:03.680 --> 01:17:04.680]   Artifacts.
[01:17:04.680 --> 01:17:05.680]   Oh, okay.
[01:17:05.680 --> 01:17:06.680]   It's just a setting.
[01:17:06.680 --> 01:17:07.680]   It's in the bottom left of the...
[01:17:07.680 --> 01:17:08.680]   You have to enable it.
[01:17:08.680 --> 01:17:09.680]   Yeah, you can find it online.
[01:17:09.680 --> 01:17:10.680]   Our people will help you.
[01:17:10.680 --> 01:17:11.680]   Uh...
[01:17:11.680 --> 01:17:11.680]   I often dump full trace back errors directly into the prompt box.
[01:17:11.680 --> 01:17:15.680]   I often dump full trace back errors directly into the prompt box as an API.
[01:17:15.680 --> 01:17:20.680]   It seems exceptional at not running into trace back loops.
[01:17:20.680 --> 01:17:39.680]   I don't know, like, if that's intentional, like, literally I'll just take the entire trace
[01:17:39.680 --> 01:17:45.680]   back, zero context to Claude, and I'll just dump the entire thing in.
[01:17:45.680 --> 01:17:46.680]   Yeah.
[01:17:46.680 --> 01:17:48.680]   And then it'll give me the fix.
[01:17:48.680 --> 01:17:49.680]   Okay.
[01:17:49.680 --> 01:17:50.680]   Great.
[01:17:50.680 --> 01:17:51.680]   So, but...
[01:17:51.680 --> 01:17:54.680]   Can you elaborate on how that might have...
[01:17:54.680 --> 01:17:55.680]   How this...
[01:17:55.680 --> 01:18:00.680]   For example, like, this only really appeared with, like, very recently.
[01:18:00.680 --> 01:18:02.680]   Like, you'd have to explain explicitly a lot.
[01:18:02.680 --> 01:18:03.680]   The models get better, man.
[01:18:03.680 --> 01:18:05.680]   They get better every generation.
[01:18:05.680 --> 01:18:08.680]   I understand, but this is, like, you know, this isn't prompt.
[01:18:08.680 --> 01:18:10.680]   This is prompt engineering we're talking about, right?
[01:18:10.680 --> 01:18:15.680]   So, I'm just wondering, like, is this a form of prompt engineering, or is just the model
[01:18:15.680 --> 01:18:16.680]   being good?
[01:18:16.680 --> 01:18:19.680]   Sounds more like the model being good if you're just dumping it in.
[01:18:19.680 --> 01:18:20.680]   Okay.
[01:18:20.680 --> 01:18:21.680]   I'm gonna move to this prompt here.
[01:18:21.680 --> 01:18:22.680]   Okay.
[01:18:22.680 --> 01:18:23.680]   So...
[01:18:23.680 --> 01:18:28.680]   Uh...
[01:18:28.680 --> 01:18:31.680]   To the person who uploaded this, do you...
[01:18:31.680 --> 01:18:36.680]   And generally, to anyone who's uploaded more examples, if you could just, like, put some
[01:18:36.680 --> 01:18:40.680]   stuff in the thread, like, write some stuff about what the issue is that you're having, or,
[01:18:40.680 --> 01:18:43.680]   like, why it's not working, that would be great.
[01:18:43.680 --> 01:18:48.680]   This is actually a follow-up to, like, what I was doing with the translation.
[01:18:48.680 --> 01:18:53.680]   So, basically, what I'm trying to get to do is to actually analyze the text.
[01:18:53.680 --> 01:18:56.680]   So, like, in this case, you know, there's the original English.
[01:18:56.680 --> 01:18:57.680]   Yeah.
[01:18:57.680 --> 01:18:59.680]   There's a bad Japanese translation.
[01:18:59.680 --> 01:19:03.680]   And I'm trying to get to score between one and five how good the translation is.
[01:19:03.680 --> 01:19:06.680]   And so, what I've been doing is adding a lot of stuff to try to get to do sort of chain
[01:19:06.680 --> 01:19:08.680]   of thought to, you know, see, like...
[01:19:08.680 --> 01:19:09.680]   Yeah.
[01:19:09.680 --> 01:19:10.680]   Because they'll notice errors.
[01:19:10.680 --> 01:19:13.680]   But, you know, it just generally does a very bad job at scoring.
[01:19:13.680 --> 01:19:14.680]   Mm.
[01:19:14.680 --> 01:19:15.680]   Yeah.
[01:19:15.680 --> 01:19:16.680]   Okay.
[01:19:16.680 --> 01:19:17.680]   So, this is great.
[01:19:17.680 --> 01:19:18.680]   I'm really glad that you asked this.
[01:19:18.680 --> 01:19:20.680]   Because model grading is something that...
[01:19:20.680 --> 01:19:22.680]   I mean, it would be incredibly useful if it worked.
[01:19:22.680 --> 01:19:26.680]   And right now, it's in a place where it, like, sometimes kind of works and sometimes
[01:19:26.680 --> 01:19:27.680]   doesn't.
[01:19:27.680 --> 01:19:32.680]   So, let's paste this into the console.
[01:19:32.680 --> 01:19:33.680]   Okay.
[01:19:33.680 --> 01:19:36.680]   So, we have some English text and we have some...
[01:19:36.680 --> 01:19:37.680]   Yep.
[01:19:37.680 --> 01:19:38.680]   Yep.
[01:19:38.680 --> 01:19:39.680]   Yep.
[01:19:39.680 --> 01:19:40.680]   Yep.
[01:19:40.680 --> 01:19:53.680]   Then we got the Japanese text.
[01:19:53.680 --> 01:19:56.680]   So, is this a good translation or a bad translation?
[01:19:56.680 --> 01:19:57.680]   Terrible translation.
[01:19:57.680 --> 01:19:58.680]   Terrible translation.
[01:19:58.680 --> 01:19:59.680]   Okay.
[01:19:59.680 --> 01:20:02.680]   Now, quad's actually supposed to be good at Japanese, too.
[01:20:02.680 --> 01:20:03.680]   So...
[01:20:03.680 --> 01:20:05.680]   Oh, this is somebody else.
[01:20:05.680 --> 01:20:09.680]   But I'm saying if quad is good at Japanese, it should be good at, like, judging other people's
[01:20:09.680 --> 01:20:10.680]   Japanese in theory.
[01:20:10.680 --> 01:20:11.680]   In theory.
[01:20:11.680 --> 01:20:12.680]   In theory.
[01:20:12.680 --> 01:20:13.680]   Okay.
[01:20:13.680 --> 01:20:15.680]   So, now we've got the answer here.
[01:20:15.680 --> 01:20:16.680]   So...
[01:20:16.680 --> 01:20:20.680]   I guess it's stalled out here for whatever reason.
[01:20:20.680 --> 01:20:22.680]   So, what are we doing in this prompt?
[01:20:22.680 --> 01:20:27.680]   We're scoring between one and five as below.
[01:20:27.680 --> 01:20:30.680]   One is many grammatical errors the native would never make.
[01:20:30.680 --> 01:20:32.680]   Contains multiple grammatical errors.
[01:20:32.680 --> 01:20:35.680]   An average quality translation with some errors.
[01:20:35.680 --> 01:20:36.680]   Okay.
[01:20:36.680 --> 01:20:38.680]   That looks pretty good.
[01:20:38.680 --> 01:20:39.680]   Look for specific clues or indicators.
[01:20:39.680 --> 01:20:39.680]   I don't know why.
[01:20:39.680 --> 01:20:40.680]   I think that's just a bug.
[01:20:40.680 --> 01:20:41.680]   I think I'll conclude here.
[01:20:41.680 --> 01:20:42.680]   Okay.
[01:20:42.680 --> 01:20:57.680]   So, it gave it a three, but we want it to give a one.
[01:20:57.680 --> 01:20:58.680]   Only closer, yeah.
[01:20:58.680 --> 01:20:59.680]   Okay.
[01:20:59.680 --> 01:21:08.680]   Now, have you found that it's generally too forgiving or too strict or that it's just all
[01:21:08.680 --> 01:21:09.680]   over the place?
[01:21:09.680 --> 01:21:10.680]   Well, it's all over the place.
[01:21:10.680 --> 01:21:15.680]   Also, it seems to confuse sometimes content versus...
[01:21:15.680 --> 01:21:20.680]   You know, so like, for example, this is from, I think, the HLRF, you know, set.
[01:21:20.680 --> 01:21:22.680]   So, you know, the content is fine.
[01:21:22.680 --> 01:21:26.680]   But so, it might not actually rate it low, even though it's a terrible translation, because
[01:21:26.680 --> 01:21:28.680]   it thinks the content is okay.
[01:21:28.680 --> 01:21:33.680]   Even though, if you asked it even to list all the errors, there are a dozen errors in
[01:21:33.680 --> 01:21:36.680]   that, you know, single piece of text for the translation.
[01:21:36.680 --> 01:21:37.680]   Yeah, yeah.
[01:21:37.680 --> 01:21:41.680]   So, that's what I'm trying to also see if, you know, is there any way to like, get to
[01:21:41.680 --> 01:21:45.680]   separate out the grammatical or the actual translation errors versus...
[01:21:45.680 --> 01:21:46.680]   Yeah.
[01:21:46.680 --> 01:21:47.680]   Okay.
[01:21:47.680 --> 01:21:48.680]   A few thoughts.
[01:21:48.680 --> 01:21:53.680]   So, generally, this is like a thing where if you ask the model if some text is like good
[01:21:53.680 --> 01:21:59.680]   or bad, it's sort of, if the text is like about a nice subject, it's more likely to say
[01:21:59.680 --> 01:22:00.680]   that it's good in all ways.
[01:22:00.680 --> 01:22:01.680]   Like, it's a good translation.
[01:22:01.680 --> 01:22:02.680]   Like, it's well written.
[01:22:02.680 --> 01:22:05.680]   It flows very logically.
[01:22:05.680 --> 01:22:08.680]   And if it's about like a negative subject, it's more likely to like, criticize it and
[01:22:08.680 --> 01:22:10.680]   say that it doesn't flow well.
[01:22:10.680 --> 01:22:11.680]   Yep.
[01:22:11.680 --> 01:22:18.680]   I think you can get at those issues by typing language about it in the prompt.
[01:22:18.680 --> 01:22:23.680]   So, for instance, here, you might say something like...
[01:22:23.680 --> 01:22:26.680]   Is one or five good?
[01:22:26.680 --> 01:22:28.680]   You don't specify that.
[01:22:28.680 --> 01:22:29.680]   Hmm?
[01:22:29.680 --> 01:22:31.680]   Is one or five the best score?
[01:22:31.680 --> 01:22:34.680]   One is the worst and five should be the best.
[01:22:34.680 --> 01:22:38.680]   That's sort of like implicit in this rubric up here.
[01:22:38.680 --> 01:22:43.680]   But it might be good to say it anyways.
[01:22:43.680 --> 01:22:56.680]   How do you get around the fact that you may not have a Japanese tokenizer?
[01:22:56.680 --> 01:22:58.680]   Do you have a Japanese tokenizer on cloud?
[01:22:58.680 --> 01:23:00.680]   The API will tokenize anything.
[01:23:00.680 --> 01:23:03.680]   But how is it trained?
[01:23:03.680 --> 01:23:05.680]   How is it trained?
[01:23:05.680 --> 01:23:06.680]   Yeah, yeah.
[01:23:06.680 --> 01:23:07.680]   Pre-trained.
[01:23:07.680 --> 01:23:08.680]   Off topic for...
[01:23:08.680 --> 01:23:09.680]   And also, I don't know.
[01:23:09.680 --> 01:23:10.680]   I don't think...
[01:23:10.680 --> 01:23:14.680]   I actually don't think there is a tokenizer for cloud.
[01:23:14.680 --> 01:23:15.680]   Like...
[01:23:15.680 --> 01:23:16.680]   Oh, so...
[01:23:16.680 --> 01:23:18.680]   I mean, unless you say otherwise, there's not...
[01:23:18.680 --> 01:23:21.680]   I mean, if you upload some text, it will be tokenized.
[01:23:21.680 --> 01:23:22.680]   But it's not pre-trained.
[01:23:22.680 --> 01:23:23.680]   But it's not pre-trained.
[01:23:23.680 --> 01:23:26.680]   So you're not going to get a really good answer for this.
[01:23:26.680 --> 01:23:27.680]   Or Japanese text.
[01:23:27.680 --> 01:23:28.680]   I've tested this.
[01:23:28.680 --> 01:23:29.680]   Okay.
[01:23:29.680 --> 01:23:33.680]   Claude speaks the best Japanese of any model available, actually.
[01:23:33.680 --> 01:23:36.680]   We don't need to debate, like, Claude's Japanese skill.
[01:23:36.680 --> 01:23:38.680]   Let's just like...
[01:23:40.680 --> 01:23:41.680]   Yeah.
[01:23:41.680 --> 01:23:42.680]   But the tokenizer isn't available.
[01:23:42.680 --> 01:23:43.680]   And so that would be interesting.
[01:23:43.680 --> 01:23:44.680]   Okay.
[01:23:44.680 --> 01:23:45.680]   Yeah.
[01:23:45.680 --> 01:23:46.680]   I have a question about programming.
[01:23:46.680 --> 01:23:59.680]   Sorry, can we actually cut the questions off while I type this prompt here?
[01:23:59.680 --> 01:24:00.680]   Okay.
[01:24:00.680 --> 01:24:01.680]   So it's extra important to...
[01:24:01.680 --> 01:24:06.920]   So what we're trying to do here is get it to distinguish between the, like, ethical nature
[01:24:06.920 --> 01:24:10.920]   of the text and the quality of the translation.
[01:24:10.920 --> 01:24:11.920]   So...
[01:24:11.920 --> 01:24:13.920]   Is it useful to tell it to be, like, extra critical?
[01:24:13.920 --> 01:24:16.920]   Say, like, you're grading a, you know, like...
[01:24:16.920 --> 01:24:19.920]   I don't know, like, graduate course, you know...
[01:24:19.920 --> 01:24:20.920]   Sorry, I'm a little bit all over the place.
[01:24:20.920 --> 01:24:25.920]   I need to, like, type this out before I can clear the queue and, like, respond to other questions
[01:24:25.920 --> 01:24:26.920]   here.
[01:24:26.920 --> 01:24:51.920]   So what's a good word here, like, risque topics, or R-rated...
[01:24:51.920 --> 01:24:52.920]   Yeah.
[01:24:52.920 --> 01:25:18.920]   So, I don't know, something like this could help the model not pay so much attention.
[01:25:18.920 --> 01:25:25.920]   So, the main thing that I would want to do for this prompt is just add a bunch of examples.
[01:25:25.920 --> 01:25:30.920]   So for each category, I would add at least one example of that category.
[01:25:30.920 --> 01:25:34.920]   So, like, I have, like, a really bad translation, and I'd say why it's a one.
[01:25:34.920 --> 01:25:36.920]   And you can say that in your own words.
[01:25:36.920 --> 01:25:41.920]   And I'd have an example of, like, a two-level translation, a three-level translation, and so
[01:25:41.920 --> 01:25:42.920]   on.
[01:25:42.920 --> 01:25:43.920]   Okay.
[01:25:43.920 --> 01:25:48.920]   And in each case, before you get to the answer, you would have the explanation for why it's
[01:25:48.920 --> 01:25:49.920]   good or bad.
[01:25:49.920 --> 01:25:53.920]   I can't tape all that here, among other reasons, because I don't speak Japanese.
[01:25:53.920 --> 01:25:54.920]   Mm-hm.
[01:25:54.920 --> 01:25:58.920]   But I think that's the most valuable thing that you could do here.
[01:25:58.920 --> 01:26:00.920]   Otherwise, I mean, like, the formatting looks really good.
[01:26:00.920 --> 01:26:04.920]   The fact that you're doing the chain of thought in advance looks good.
[01:26:04.920 --> 01:26:05.920]   Yeah.
[01:26:05.920 --> 01:26:13.920]   I think maybe this specific clues are indicators.
[01:26:13.920 --> 01:26:18.920]   I think you could go into a little bit more detail here about what is...
[01:26:18.920 --> 01:26:19.920]   Okay.
[01:26:19.920 --> 01:26:25.920]   So basically just end shot every single example or, you know, grade with an example, basically.
[01:26:25.920 --> 01:26:26.920]   Yeah.
[01:26:26.920 --> 01:26:27.920]   Yeah.
[01:26:27.920 --> 01:26:31.920]   I mean, it's tedious to write out all these examples, but A, quad can help, and then you
[01:26:31.920 --> 01:26:35.920]   have the problem of just editing quad's response versus, like, writing it all out yourself.
[01:26:35.920 --> 01:26:40.920]   And B, it really does lead to better performance versus, like, almost anything else that you could
[01:26:40.920 --> 01:26:41.920]   do.
[01:26:41.920 --> 01:26:46.920]   Do you think it's better to use a number scale, or else could you say, like, good, bad, or,
[01:26:46.920 --> 01:26:47.920]   you know?
[01:26:47.920 --> 01:26:48.920]   Yeah.
[01:26:48.920 --> 01:26:54.920]   In terms of the scale on these rubrics, I think either a number scale or a good, bad is
[01:26:54.920 --> 01:26:55.920]   fine.
[01:26:55.920 --> 01:26:59.920]   The thing I'd be careful about with the number scale is that I don't think it's very well
[01:26:59.920 --> 01:27:00.920]   calibrated.
[01:27:00.920 --> 01:27:04.920]   So if you're telling it, like, choose a number from 1 through 100, it's not going to be, like,
[01:27:04.920 --> 01:27:06.920]   a unbiased estimator necessarily.
[01:27:06.920 --> 01:27:11.920]   So I'd probably just limit the granularity to maybe five different classes.
[01:27:11.920 --> 01:27:12.920]   Yeah.
[01:27:12.920 --> 01:27:19.920]   One thing we haven't talked about at all here, but going back to your question, is that
[01:27:19.920 --> 01:27:24.920]   this is a case where you might be able to utilize, like, log probs, and I'm wondering if
[01:27:24.920 --> 01:27:29.920]   that's something you ever use in any of your work or other prompts.
[01:27:29.920 --> 01:27:30.920]   Yeah.
[01:27:30.920 --> 01:27:35.920]   I agree this could be a case where log probs would be useful if you could get, like, the
[01:27:35.920 --> 01:27:37.920]   probability of each grade.
[01:27:37.920 --> 01:27:44.920]   So here's the thing with log probs is you really want the chain of thought beforehand, and the
[01:27:44.920 --> 01:27:48.920]   chain of thought, I think, is going to get you more of a win than using the log probs.
[01:27:48.920 --> 01:27:49.920]   log probs.
[01:27:49.920 --> 01:27:55.920]   But log probs, there's no -- in any model, I don't think there's a way where you can just
[01:27:55.920 --> 01:28:00.920]   say, sample all the way through, and then output -- after you output this, like, closed chain
[01:28:00.920 --> 01:28:03.920]   of thought tag, then give me the log probs of whatever comes next.
[01:28:03.920 --> 01:28:04.920]   Mm-hm.
[01:28:04.920 --> 01:28:08.920]   So if you are going to use the log probs, then you're looking at this, like, multi-turn
[01:28:08.920 --> 01:28:13.920]   setup, where you first sample all the chain of thought, or sample all the, like, precogitation
[01:28:13.920 --> 01:28:14.920]   that it wants to do.
[01:28:14.920 --> 01:28:15.920]   Mm-hm.
[01:28:15.920 --> 01:28:21.920]   And then cut it off there, and then re-upload that with the chain of thought as a pre-fill
[01:28:21.920 --> 01:28:23.920]   message, and then you could get the log probs.
[01:28:23.920 --> 01:28:30.920]   But for that, you need a model that both has the pre-fill capacity and has log prob capacity.
[01:28:30.920 --> 01:28:34.920]   I'm not sure of what model has both of those characteristics right now.
[01:28:34.920 --> 01:28:39.920]   Walk me through why it wouldn't just be sufficient to, like, in this case, just ask for -- I'm asking
[01:28:39.920 --> 01:28:44.920]   for a score, one to five, only return that, but then, like, look at the log probs of what it
[01:28:44.920 --> 01:28:45.920]   returns in that case.
[01:28:45.920 --> 01:28:46.920]   Yeah.
[01:28:46.920 --> 01:28:51.920]   So what you're losing there is whatever intelligence boost you got from having the model do the
[01:28:51.920 --> 01:28:53.920]   chain of thought.
[01:28:53.920 --> 01:28:57.920]   And my sense is that chain of thought plus have the model say either one, two, three, four,
[01:28:57.920 --> 01:29:02.920]   or five is going to be more accurate than, like, the additional nuance that you'd get by
[01:29:02.920 --> 01:29:07.920]   having it give you the log probs, because it's actually doing a lot of its thinking in that
[01:29:07.920 --> 01:29:08.920]   chain of thought.
[01:29:08.920 --> 01:29:09.920]   You're, like, leveraging more computation.
[01:29:09.920 --> 01:29:10.920]   You're getting more forward passes.
[01:29:10.920 --> 01:29:13.920]   For all the same reason that chain of thought is, like, usually a good idea.
[01:29:13.920 --> 01:29:17.920]   Are you talking about, like, chain of thought, like, it's actually out loud writing a chain
[01:29:17.920 --> 01:29:18.920]   of thought before the answer?
[01:29:18.920 --> 01:29:19.920]   Exactly.
[01:29:19.920 --> 01:29:20.920]   Okay.
[01:29:20.920 --> 01:29:21.920]   Okay.
[01:29:21.920 --> 01:29:22.920]   Sure.
[01:29:22.920 --> 01:29:23.920]   I got you.
[01:29:23.920 --> 01:29:24.920]   Which is what we see in this prompt right here, right?
[01:29:24.920 --> 01:29:25.920]   Like, we have this analysis section.
[01:29:25.920 --> 01:29:29.920]   So if we cut out this whole analysis section, we're really tanking the number of forward passes
[01:29:29.920 --> 01:29:31.920]   that the model can do before it gave you the answer.
[01:29:31.920 --> 01:29:32.920]   Okay.
[01:29:32.920 --> 01:29:33.920]   Cool.
[01:29:33.920 --> 01:29:34.920]   That's good to know.
[01:29:34.920 --> 01:29:35.920]   What's that?
[01:29:35.920 --> 01:29:36.920]   Okay.
[01:29:36.920 --> 01:29:44.920]   I'm being told that I should answer a couple more questions and then get off stage.
[01:29:44.920 --> 01:29:49.920]   I was, like, before I came here, I was honestly really worried that, like, no one would have
[01:29:49.920 --> 01:29:52.920]   questions and I'd be supplying my own.
[01:29:52.920 --> 01:29:55.920]   But you all have had amazing questions so far and amazing examples.
[01:29:55.920 --> 01:29:57.920]   So I really appreciate that.
[01:29:57.920 --> 01:29:58.920]   It's made this go on well.
[01:29:58.920 --> 01:29:59.920]   Sorry.
[01:29:59.920 --> 01:30:01.920]   I'm supposed to say that at the end of this, but I'm giving a pre-thank you.
[01:30:01.920 --> 01:30:02.920]   Now we can do the encore.
[01:30:02.920 --> 01:30:03.920]   Yeah.
[01:30:03.920 --> 01:30:09.920]   I just wanted to add that having a numbered list may give more weight to, like, number
[01:30:09.920 --> 01:30:13.920]   one, two, three, four, five versus just having an unstructured list.
[01:30:13.920 --> 01:30:18.920]   So it may give more weight into the score for the output if you just change it from, like,
[01:30:18.920 --> 01:30:23.920]   one, two, three, four, five to, like, little dashes for criteria.
[01:30:23.920 --> 01:30:25.920]   Just in my experience of what I've been seeing.
[01:30:25.920 --> 01:30:26.920]   Okay.
[01:30:26.920 --> 01:30:27.920]   Cool.
[01:30:27.920 --> 01:30:28.920]   Yeah.
[01:30:28.920 --> 01:30:33.920]   I have replied back with the prompt that was fixed or that, like, in my own improvement
[01:30:33.920 --> 01:30:35.920]   of what I think is a better prompt for this.
[01:30:35.920 --> 01:30:36.920]   Yeah.
[01:30:36.920 --> 01:30:37.920]   Okay.
[01:30:37.920 --> 01:30:38.920]   Awesome.
[01:30:38.920 --> 01:30:42.920]   Nisha, should I do another prompt or should I just answer a couple questions and then head
[01:30:42.920 --> 01:30:43.920]   up?
[01:30:43.920 --> 01:30:44.920]   All right.
[01:30:44.920 --> 01:30:48.920]   Let's do one last prompt.
[01:30:48.920 --> 01:30:51.920]   What's -- which one should we choose?
[01:30:51.920 --> 01:30:52.920]   Okay.
[01:30:52.920 --> 01:30:54.920]   Let's do this one.
[01:30:54.920 --> 01:30:57.920]   Good old mitigating hallucinations.
[01:30:57.920 --> 01:30:59.920]   Because we haven't really done that.
[01:30:59.920 --> 01:31:00.920]   Okay.
[01:31:00.920 --> 01:31:04.920]   Please provide a summary of the text provider's input.
[01:31:04.920 --> 01:31:05.920]   Okay.
[01:31:05.920 --> 01:31:10.920]   First thing I'll do is just move these instructions down.
[01:31:10.920 --> 01:31:26.920]   Now, Matt, did you have an example of the document -- a document where it hallucinates with this
[01:31:26.920 --> 01:31:27.920]   prompt?
[01:31:27.920 --> 01:31:28.920]   Yeah.
[01:31:28.920 --> 01:31:29.920]   Can you just put that in the thread?
[01:31:29.920 --> 01:31:30.920]   Okay.
[01:31:30.920 --> 01:31:45.920]   Your summary should be concise while maintaining all important information that would assist
[01:31:45.920 --> 01:31:47.920]   in helping someone understand the content.
[01:31:47.920 --> 01:31:50.920]   If it mentions any dates.
[01:31:50.920 --> 01:31:54.920]   Don't start or end with anything like I've generated a summary for you.
[01:31:54.920 --> 01:31:56.920]   Here's the summary you asked for.
[01:31:56.920 --> 01:31:57.920]   Okay.
[01:31:57.920 --> 01:31:58.920]   Yeah.
[01:31:58.920 --> 01:31:59.920]   So this one we can fix with a pre-fill.
[01:31:59.920 --> 01:32:00.920]   Okay.
[01:32:00.920 --> 01:32:01.920]   All right.
[01:32:01.920 --> 01:32:18.920]   We could do something like this.
[01:32:18.920 --> 01:32:33.920]   Now, how about the hallucination part?
[01:32:33.920 --> 01:32:37.920]   The best trick that I know for getting around hallucinations in a case like this is to have
[01:32:37.920 --> 01:32:41.920]   the model extract relevant quotes first.
[01:32:41.920 --> 01:32:44.920]   So what I would say do here is I would say something like --
[01:32:44.920 --> 01:32:47.920]   and now, of course, in this pre-fill, since we're having relevant quotes here, we wouldn't
[01:32:47.920 --> 01:32:48.920]   want to start with summary.
[01:32:48.920 --> 01:32:54.920]   that would just be confusing slash wrong.
[01:32:54.920 --> 01:32:55.920]   So we could say here is the --
[01:32:55.920 --> 01:33:13.920]   And now, of course, in this pre-fill, since we're having relevant quotes here, we wouldn't want to start with summary.
[01:33:13.920 --> 01:33:14.920]   That would just be confusing slash wrong.
[01:33:14.920 --> 01:33:26.920]   So we could say here is the -- something like this.
[01:33:26.920 --> 01:33:27.920]   Okay.
[01:33:27.920 --> 01:33:28.920]   Did you get the doc yet?
[01:33:28.920 --> 01:33:29.920]   Okay.
[01:33:29.920 --> 01:33:43.920]   And then, of course, I would put the document here.
[01:33:43.920 --> 01:33:44.920]   Okay.
[01:33:44.920 --> 01:33:56.920]   I think I should get off stage.
[01:33:56.920 --> 01:33:57.920]   So, yeah.
[01:33:57.920 --> 01:33:59.920]   Let me just call it here.
[01:33:59.920 --> 01:34:01.920]   And, Matt, we can talk after.
[01:34:01.920 --> 01:34:02.920]   Yeah.
[01:34:02.920 --> 01:34:04.920]   Once again, I really appreciate you all coming out.
[01:34:04.920 --> 01:34:08.920]   It's been amazing to have such, like, a great audience engaged.
[01:34:08.920 --> 01:34:10.920]   I've had fun.
[01:34:10.920 --> 01:34:11.920]   I learned some things.
[01:34:11.920 --> 01:34:12.920]   I hope you all did, too.
[01:34:12.920 --> 01:34:17.920]   I'm planning to stick around this event for the next -- for the rest of the afternoon.
[01:34:17.920 --> 01:34:23.920]   So I don't know exactly where I'll be, but maybe just DM me if you want to come find me and chat.
[01:34:23.920 --> 01:34:25.920]   I'm always happy to talk prompt engineering.
[01:34:25.920 --> 01:34:29.920]   It's, like, my truest passion at this point in the world.
[01:34:29.920 --> 01:34:30.920]   So, like, find me.
[01:34:30.920 --> 01:34:31.920]   Hit me up.
[01:34:31.920 --> 01:34:32.920]   We'll talk.
[01:34:32.920 --> 01:34:34.920]   And, yeah.
[01:34:34.920 --> 01:34:35.920]   This has been great.
[01:34:35.920 --> 01:34:36.920]   Thank you so much.
[01:34:36.920 --> 01:34:37.920]   Thank you.
[01:34:37.920 --> 01:34:37.920]   Thank you.
[01:34:37.920 --> 01:34:37.920]   Thank you.
[01:34:37.920 --> 01:34:38.920]   Thank you.
[01:34:38.920 --> 01:34:39.920]   Thank you.
[01:34:39.920 --> 01:34:39.920]   Thank you.
[01:34:39.920 --> 01:34:40.920]   Thank you.
[01:34:40.920 --> 01:34:40.920]   Thank you.
[01:34:40.920 --> 01:34:41.920]   Thank you.
[01:34:41.920 --> 01:34:42.920]   Thank you.
[01:34:42.920 --> 01:34:43.920]   Thank you.
[01:34:43.920 --> 01:34:44.920]   Thank you.
[01:34:44.920 --> 01:34:45.920]   Thank you.
[01:34:45.920 --> 01:34:46.920]   Thank you.
[01:34:46.920 --> 01:34:47.920]   Thank you.
[01:34:47.920 --> 01:34:48.920]   Thank you.
[01:34:48.920 --> 01:34:49.920]   Thank you.
[01:34:49.920 --> 01:34:50.920]   Thank you.
[01:34:50.920 --> 01:34:55.560]   We'll see you next time.

