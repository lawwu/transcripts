
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:05.160]   (upbeat music)
[00:00:05.160 --> 00:00:07.740]   (upbeat music)
[00:00:08.260 --> 00:00:10.840]   (upbeat music)
[00:00:10.840 --> 00:00:13.420]   (upbeat music)
[00:00:14.120 --> 00:00:16.700]   (upbeat music)
[00:00:16.700 --> 00:00:19.280]   (upbeat music)
[00:00:19.280 --> 00:00:21.860]   (upbeat music)
[00:00:21.860 --> 00:00:25.440]   (upbeat music)
[00:00:25.440 --> 00:00:29.360]   (upbeat music)
[00:00:29.360 --> 00:00:32.900]   (upbeat music)
[00:00:32.900 --> 00:00:36.520]   (upbeat music)
[00:00:36.520 --> 00:00:40.080]   (upbeat music)
[00:00:40.080 --> 00:00:43.580]   (upbeat music)
[00:00:43.580 --> 00:00:47.120]   (upbeat music)
[00:00:48.060 --> 00:00:50.640]   (upbeat music)
[00:00:50.640 --> 00:00:54.220]   (upbeat music)
[00:00:54.220 --> 00:00:57.820]   (upbeat music)
[00:00:57.820 --> 00:01:01.400]   (upbeat music)
[00:01:01.400 --> 00:01:04.980]   (upbeat music)
[00:01:04.980 --> 00:01:08.600]   (upbeat music)
[00:01:08.600 --> 00:01:12.220]   (upbeat music)
[00:01:12.220 --> 00:01:15.700]   (upbeat music)
[00:01:16.840 --> 00:01:19.420]   (upbeat music)
[00:01:19.420 --> 00:01:23.000]   (upbeat music)
[00:01:23.000 --> 00:01:26.480]   (upbeat music)
[00:01:26.480 --> 00:01:30.020]   (upbeat music)
[00:01:30.020 --> 00:01:33.560]   (upbeat music)
[00:01:33.560 --> 00:01:37.260]   (upbeat music)
[00:01:37.260 --> 00:01:40.800]   (upbeat music)
[00:01:40.800 --> 00:01:44.400]   (upbeat music)
[00:01:45.400 --> 00:01:47.980]   (upbeat music)
[00:01:47.980 --> 00:01:52.560]   (upbeat music)
[00:01:52.560 --> 00:01:59.280]   (speaking in foreign language)
[00:01:59.280 --> 00:02:15.360]   (speaking in foreign language)
[00:02:15.360 --> 00:02:17.600]   - Maria from Wayden Bias.
[00:02:17.600 --> 00:02:21.520]   (speaking in foreign language)
[00:02:21.520 --> 00:02:22.860]   - Hello everyone.
[00:02:22.860 --> 00:02:26.780]   (speaking in foreign language)
[00:02:26.780 --> 00:02:30.700]   (speaking in foreign language)
[00:02:30.700 --> 00:02:34.620]   (speaking in foreign language)
[00:02:34.620 --> 00:02:38.540]   (speaking in foreign language)
[00:02:38.540 --> 00:02:42.460]   (speaking in foreign language)
[00:02:42.460 --> 00:02:46.380]   (speaking in foreign language)
[00:02:46.380 --> 00:02:52.360]   (speaking in foreign language)
[00:02:52.360 --> 00:02:56.280]   (speaking in foreign language)
[00:02:56.280 --> 00:03:00.200]   (speaking in foreign language)
[00:03:00.200 --> 00:03:04.120]   (speaking in foreign language)
[00:03:04.120 --> 00:03:08.040]   (speaking in foreign language)
[00:03:08.040 --> 00:03:12.900]   (speaking in foreign language)
[00:03:12.900 --> 00:03:20.820]   (speaking in foreign language)
[00:03:20.820 --> 00:03:25.580]   (speaking in foreign language)
[00:03:31.800 --> 00:03:35.720]   (speaking in foreign language)
[00:03:36.080 --> 00:03:40.000]   (speaking in foreign language)
[00:03:40.000 --> 00:03:43.920]   (speaking in foreign language)
[00:03:43.920 --> 00:03:47.840]   (speaking in foreign language)
[00:03:48.660 --> 00:03:52.580]   (speaking in foreign language)
[00:03:52.580 --> 00:03:56.500]   (speaking in foreign language)
[00:03:56.500 --> 00:03:58.420]   from Wayden Bias.
[00:03:58.420 --> 00:04:02.340]   (speaking in foreign language)
[00:04:02.340 --> 00:04:06.260]   (speaking in foreign language)
[00:04:06.260 --> 00:04:10.180]   (speaking in foreign language)
[00:04:10.180 --> 00:04:14.100]   (speaking in foreign language)
[00:04:14.100 --> 00:04:18.020]   (speaking in foreign language)
[00:04:18.980 --> 00:04:22.900]   (speaking in foreign language)
[00:04:22.900 --> 00:04:26.820]   (speaking in foreign language)
[00:04:26.820 --> 00:04:30.740]   (speaking in foreign language)
[00:04:30.740 --> 00:04:34.660]   (speaking in foreign language)
[00:04:34.660 --> 00:04:38.580]   (speaking in foreign language)
[00:04:38.580 --> 00:04:42.500]   (speaking in foreign language)
[00:04:42.500 --> 00:04:46.420]   (speaking in foreign language)
[00:04:46.420 --> 00:05:08.220]   - Hello, Morgan, can you hear me?
[00:05:08.220 --> 00:05:11.080]   - Yes, hello, hello, everybody.
[00:05:12.420 --> 00:05:16.060]   Yes, you can start your screen and start your lecture.
[00:05:16.060 --> 00:05:20.300]   - Amazing, great, thanks so much, Stella,
[00:05:20.300 --> 00:05:24.500]   and the rest of the Deep Learning crew for organizing this.
[00:05:24.500 --> 00:05:28.460]   Super excited to be here and to meet
[00:05:28.460 --> 00:05:32.380]   the virtual South Korean community.
[00:05:32.380 --> 00:05:35.140]   I will share my screen.
[00:05:35.140 --> 00:05:37.580]   Okay, can you see this?
[00:05:37.580 --> 00:05:39.180]   - Yes, I can see it.
[00:05:40.100 --> 00:05:44.340]   - Great, let's start this show.
[00:05:44.340 --> 00:05:48.580]   Great, so to kick off,
[00:05:48.580 --> 00:05:51.380]   I'd like to first chat a little bit
[00:05:51.380 --> 00:05:54.980]   about reproducible machine learning,
[00:05:54.980 --> 00:05:57.100]   and then maybe show you a quick idea
[00:05:57.100 --> 00:05:58.980]   of how Weights & Biases can help you
[00:05:58.980 --> 00:06:05.020]   kind of track and reproduce your ML workflows.
[00:06:05.020 --> 00:06:09.540]   So I'm Morgan, I'm a machine learning engineer
[00:06:09.540 --> 00:06:12.300]   on our growth team at Weights & Biases,
[00:06:12.300 --> 00:06:15.220]   and it's really nice to meet you all.
[00:06:15.220 --> 00:06:19.460]   So if you are curious about Weights & Biases
[00:06:19.460 --> 00:06:21.620]   and you don't want to listen to me speak
[00:06:21.620 --> 00:06:23.700]   and you just want to look at some code,
[00:06:23.700 --> 00:06:27.420]   you can go to this link, wandb.me/start,
[00:06:27.420 --> 00:06:30.620]   and you can start to immediately run a Colab
[00:06:30.620 --> 00:06:33.740]   and look at our documentation and play around.
[00:06:35.260 --> 00:06:39.540]   So first, why do we need reproducibility
[00:06:39.540 --> 00:06:40.780]   in machine learning?
[00:06:40.780 --> 00:06:47.060]   And so deep learning is unusual in that
[00:06:47.060 --> 00:06:49.940]   even when you're training the same model
[00:06:49.940 --> 00:06:51.700]   with the same hyperparameter,
[00:06:51.700 --> 00:06:53.420]   you might not always get the same results.
[00:06:53.420 --> 00:06:58.420]   And when I first was learning deep learning,
[00:06:58.420 --> 00:07:00.980]   this was kind of quite surprising to me,
[00:07:00.980 --> 00:07:02.220]   but there's a whole bunch of reasons
[00:07:02.220 --> 00:07:04.060]   why this might not be the case,
[00:07:04.060 --> 00:07:08.180]   and you might have fluctuations in your final metric
[00:07:08.180 --> 00:07:09.020]   that you care about.
[00:07:09.020 --> 00:07:13.460]   And like you say here, it could be to do with the datasets.
[00:07:13.460 --> 00:07:14.700]   Obviously, when we're training,
[00:07:14.700 --> 00:07:17.580]   when we're shuffling our training data,
[00:07:17.580 --> 00:07:20.740]   and that can result in slight changes
[00:07:20.740 --> 00:07:23.020]   in how the model learns.
[00:07:23.020 --> 00:07:25.780]   There might be results in terms of the differences
[00:07:25.780 --> 00:07:28.060]   in hardwares or GPUs you're using,
[00:07:28.060 --> 00:07:30.660]   like newer GPUs, bigger GPUs.
[00:07:31.980 --> 00:07:34.060]   Also differences in library versions
[00:07:34.060 --> 00:07:39.060]   and potentially how potential operations are encoded,
[00:07:39.060 --> 00:07:43.460]   and also small differences
[00:07:43.460 --> 00:07:48.460]   between machine learning frameworks.
[00:07:48.460 --> 00:07:53.460]   And a little example is the value for the epsilon
[00:07:53.460 --> 00:07:59.340]   in a layer norm module in PyTorch and TensorFlow
[00:07:59.500 --> 00:08:02.540]   is different by a small amount,
[00:08:02.540 --> 00:08:05.100]   but can result in slightly different results
[00:08:05.100 --> 00:08:06.140]   in your training.
[00:08:06.140 --> 00:08:09.940]   Of course, initialization of your weights
[00:08:09.940 --> 00:08:13.540]   is usually random, and so that's gonna have an impact too.
[00:08:13.540 --> 00:08:16.660]   And then other sources of randomness, such as dropout,
[00:08:16.660 --> 00:08:21.060]   for example, would also result in small changes
[00:08:21.060 --> 00:08:26.060]   that you mightn't expect in your final results.
[00:08:26.060 --> 00:08:29.060]   And so we want to track all of this
[00:08:29.060 --> 00:08:34.060]   because ML models are becoming more and more important
[00:08:34.060 --> 00:08:38.100]   and relevant throughout our daily lives
[00:08:38.100 --> 00:08:40.820]   and can impact us more and more in the real world.
[00:08:40.820 --> 00:08:44.380]   And there's already models in production,
[00:08:44.380 --> 00:08:47.820]   like in banks that decide who gets a loan.
[00:08:47.820 --> 00:08:50.140]   Companies are working with resume reviews
[00:08:50.140 --> 00:08:52.860]   to screen resumes faster.
[00:08:52.860 --> 00:08:55.660]   And other machine learning models out there
[00:08:55.660 --> 00:09:00.660]   that try and measure some measure of trust or responsibility.
[00:09:00.660 --> 00:09:07.660]   And so it's very important that when these models go wrong
[00:09:07.660 --> 00:09:12.140]   or make a mistake, we can exactly identify
[00:09:12.140 --> 00:09:15.860]   what the cause was and how we can maybe improve
[00:09:15.860 --> 00:09:18.380]   and avoid making those mistakes in future.
[00:09:18.380 --> 00:09:22.380]   And so the classic example we have
[00:09:22.380 --> 00:09:24.740]   is the self-driving car.
[00:09:24.740 --> 00:09:28.380]   You know, if anything happens,
[00:09:28.380 --> 00:09:32.100]   if a self-driving car gets into an accident,
[00:09:32.100 --> 00:09:36.780]   the car manufacturer and the regulators
[00:09:36.780 --> 00:09:39.780]   and potentially the police will want to know
[00:09:39.780 --> 00:09:42.140]   why the car crashed.
[00:09:42.140 --> 00:09:47.140]   And maybe it was because the model wasn't trained
[00:09:47.140 --> 00:09:52.980]   on enough examples of the data for the event that happened.
[00:09:53.940 --> 00:09:58.940]   Maybe the model that was selected over indexes
[00:09:58.940 --> 00:10:03.620]   on detecting cars and humans, but not on roads.
[00:10:03.620 --> 00:10:06.420]   And so you need to be able to look at the entire pipeline
[00:10:06.420 --> 00:10:07.900]   of how that model was trained,
[00:10:07.900 --> 00:10:11.020]   be able to see the data it was trained on,
[00:10:11.020 --> 00:10:12.820]   see how that data was processed,
[00:10:12.820 --> 00:10:15.940]   and then see all of the hyperparameters
[00:10:15.940 --> 00:10:18.660]   that went into training the model to understand
[00:10:18.660 --> 00:10:21.300]   where the failure might be
[00:10:21.460 --> 00:10:24.820]   and where you need to improve your processes for the future.
[00:10:24.820 --> 00:10:28.980]   And obviously this has a huge impact in the real world.
[00:10:28.980 --> 00:10:33.980]   And so having reproducibility in your work here is key.
[00:10:33.980 --> 00:10:39.540]   And so what we see is a reproducibility gap
[00:10:39.540 --> 00:10:41.500]   in deep learning.
[00:10:41.500 --> 00:10:45.500]   And I'm sure many of you have experienced this already.
[00:10:45.500 --> 00:10:49.300]   There's a bunch of different reasons
[00:10:49.300 --> 00:10:52.820]   that there might be a difficulty with reproducibility.
[00:10:52.820 --> 00:10:57.100]   And one of the most cited ones would be
[00:10:57.100 --> 00:10:58.940]   the first item here,
[00:10:58.940 --> 00:11:02.700]   lack of access to the same training data.
[00:11:02.700 --> 00:11:05.340]   So some companies just have a lot of proprietary data
[00:11:05.340 --> 00:11:09.380]   that they can train their models on
[00:11:09.380 --> 00:11:10.660]   that isn't released to the public.
[00:11:10.660 --> 00:11:14.580]   And so you'll never be able to reproduce the models
[00:11:14.580 --> 00:11:16.700]   those companies are releasing
[00:11:16.700 --> 00:11:18.980]   because you don't have the same training data.
[00:11:18.980 --> 00:11:21.740]   Other issues can be around
[00:11:21.740 --> 00:11:25.660]   the lack of code that's shared.
[00:11:25.660 --> 00:11:28.700]   So there might be an amazing paper that they released
[00:11:28.700 --> 00:11:33.700]   and they potentially point to a GitHub in their paper,
[00:11:33.700 --> 00:11:37.180]   but that GitHub stays empty
[00:11:37.180 --> 00:11:38.420]   because the authors are too busy
[00:11:38.420 --> 00:11:39.580]   and they move on to the next thing
[00:11:39.580 --> 00:11:41.220]   and they never release the code.
[00:11:41.220 --> 00:11:43.820]   And so trying to reproduce that author's work
[00:11:44.700 --> 00:11:47.780]   can be difficult and very difficult
[00:11:47.780 --> 00:11:50.100]   if they don't give a good clear explanation
[00:11:50.100 --> 00:11:52.260]   in their paper of how they,
[00:11:52.260 --> 00:11:55.820]   the technique that they were describing.
[00:11:55.820 --> 00:11:58.780]   Other issues we see are
[00:11:58.780 --> 00:12:02.900]   under-specification of the metrics used.
[00:12:02.900 --> 00:12:05.940]   And so like poorly defined metrics can be an issue
[00:12:05.940 --> 00:12:08.900]   as well as like,
[00:12:08.900 --> 00:12:13.900]   not doing like sufficient analysis on your results
[00:12:14.740 --> 00:12:17.260]   you know, there's like issues around like P-hacking
[00:12:17.260 --> 00:12:19.060]   and trying to, you know, just like tweak everything
[00:12:19.060 --> 00:12:20.940]   until you get what you think might be
[00:12:20.940 --> 00:12:23.300]   a like statistically significant results.
[00:12:23.300 --> 00:12:26.460]   And then there's other issues around like,
[00:12:26.460 --> 00:12:28.580]   yeah, selective reporting results,
[00:12:28.580 --> 00:12:30.660]   you know, potentially like in the deep learning world,
[00:12:30.660 --> 00:12:32.260]   you know, maybe there's a danger.
[00:12:32.260 --> 00:12:36.300]   Some people are tempted to do some training on the test set
[00:12:36.300 --> 00:12:40.020]   to try and like improve their results a little bit.
[00:12:40.020 --> 00:12:43.820]   And so can result in like a gap in reproducibility.
[00:12:43.820 --> 00:12:46.860]   There, and so there's a whole, you know,
[00:12:46.860 --> 00:12:48.420]   bunch of issues that we see
[00:12:48.420 --> 00:12:53.660]   and these are all kind of from a paper by Joel Pino
[00:12:53.660 --> 00:12:57.140]   and yeah, Christophe Sina,
[00:12:57.140 --> 00:13:01.140]   who basically kind of like wrote about this
[00:13:01.140 --> 00:13:04.020]   in back in, I think, 2015.
[00:13:04.020 --> 00:13:07.820]   And so I would love,
[00:13:07.820 --> 00:13:12.820]   we can, I'm curious to anyone in the chat,
[00:13:13.620 --> 00:13:15.580]   either in the Zoom or YouTube,
[00:13:15.580 --> 00:13:21.460]   curious to hear if folks have had an issue
[00:13:21.460 --> 00:13:25.220]   with reproducing their models.
[00:13:25.220 --> 00:13:28.700]   Sorry, I think I've just lost my slides.
[00:13:28.700 --> 00:13:29.540]   Apologies.
[00:13:29.540 --> 00:13:35.740]   - Actually, it's gonna be in my presentation today,
[00:13:35.740 --> 00:13:39.180]   but I've had a case where I originally thought
[00:13:39.180 --> 00:13:43.380]   that my model was four times faster than my original model,
[00:13:43.380 --> 00:13:48.380]   but then it turned out to be maybe double or less
[00:13:48.380 --> 00:13:50.700]   or maybe just zero.
[00:13:50.700 --> 00:13:55.700]   So I've actually tried and initial results were amazing.
[00:13:55.700 --> 00:14:03.140]   I tried to reproduce, so, and I just published all that.
[00:14:03.140 --> 00:14:05.740]   And then I tried to reproduce
[00:14:05.740 --> 00:14:08.940]   and the results were like subpar
[00:14:08.940 --> 00:14:12.460]   when I tried to get the performance again.
[00:14:12.460 --> 00:14:15.540]   So I'm actually guilty of a lot of the things
[00:14:15.540 --> 00:14:17.820]   you mentioned just now.
[00:14:17.820 --> 00:14:19.140]   - Yeah, okay, exactly.
[00:14:19.140 --> 00:14:20.340]   That's a great example.
[00:14:20.340 --> 00:14:23.100]   And thanks so much for sharing.
[00:14:23.100 --> 00:14:24.980]   And so it's a super common issue,
[00:14:24.980 --> 00:14:29.140]   both across academia and industry,
[00:14:29.140 --> 00:14:31.500]   even when you're like kind of starting out
[00:14:31.500 --> 00:14:32.900]   on your deep learning journey
[00:14:32.900 --> 00:14:35.820]   and also like for like experienced practitioners as well.
[00:14:35.820 --> 00:14:40.940]   Deep learning, you all know, it's just not a mature field.
[00:14:40.940 --> 00:14:43.420]   Like it's not as mature as like software engineering
[00:14:43.420 --> 00:14:46.540]   or like other areas in computer science.
[00:14:46.540 --> 00:14:49.540]   And so we don't have, a lot of us just don't have the tools
[00:14:49.540 --> 00:14:54.540]   or like the training to like easily be able to like track
[00:14:54.540 --> 00:14:56.460]   and reproduce our work.
[00:14:56.460 --> 00:15:02.420]   And so this question was asked in an article
[00:15:02.420 --> 00:15:05.180]   that was published in Nature.
[00:15:05.180 --> 00:15:09.900]   And so like across pretty much like every area of science,
[00:15:09.900 --> 00:15:13.340]   there was like pretty strong agreement
[00:15:13.340 --> 00:15:17.420]   that like people struggled to reproduce other people's work,
[00:15:17.420 --> 00:15:19.980]   but often also like their own work,
[00:15:19.980 --> 00:15:21.620]   which was quite interesting to see.
[00:15:21.620 --> 00:15:25.860]   And so some of the main issues around this,
[00:15:25.860 --> 00:15:28.540]   and obviously this is more focused on academia,
[00:15:28.540 --> 00:15:30.780]   but some of the main issues that were identified
[00:15:30.780 --> 00:15:32.420]   across like all of the fields
[00:15:32.420 --> 00:15:35.340]   was like the pressure to publish.
[00:15:35.340 --> 00:15:38.460]   And I'm sure some of you experienced this,
[00:15:38.460 --> 00:15:41.460]   coming up to like a conference submission deadline,
[00:15:41.460 --> 00:15:43.700]   there can be quite a rush
[00:15:43.700 --> 00:15:45.260]   to get like your last experiments done,
[00:15:45.260 --> 00:15:50.260]   and maybe it's easy to forget to track every hyperparameter
[00:15:50.260 --> 00:15:51.940]   or every like tweak that you used
[00:15:51.940 --> 00:15:54.020]   to get your final results.
[00:15:54.020 --> 00:15:58.460]   Other issues I see are poor analysis of results
[00:15:58.460 --> 00:16:02.780]   or not doing, or not, again,
[00:16:02.780 --> 00:16:06.260]   like not sharing code and things like that.
[00:16:06.260 --> 00:16:09.860]   So that's like a really like interesting study to see,
[00:16:09.860 --> 00:16:12.060]   and kind of like alliance with what we see
[00:16:12.060 --> 00:16:13.900]   in the deep learning world as well.
[00:16:13.900 --> 00:16:20.540]   So I'm curious today,
[00:16:20.540 --> 00:16:23.500]   do you folks have a process
[00:16:23.500 --> 00:16:26.180]   to make your models producible?
[00:16:26.180 --> 00:16:31.180]   And feel free to drop a comment in the YouTube chat.
[00:16:31.180 --> 00:16:35.020]   And Stella, maybe you could help me translate
[00:16:35.020 --> 00:16:38.340]   if there's any Korean comments in there.
[00:16:38.340 --> 00:16:44.140]   (speaking in foreign language)
[00:16:44.140 --> 00:16:47.380]   - You mean, is there any question?
[00:16:47.380 --> 00:16:51.820]   I get that yet, but I will tell you if there is.
[00:16:51.820 --> 00:16:52.660]   - Good, great.
[00:16:52.660 --> 00:16:56.700]   Sorry, was there a question?
[00:16:56.700 --> 00:17:02.100]   Oh no, okay, cool.
[00:17:02.100 --> 00:17:02.940]   Good, good, cool.
[00:17:02.940 --> 00:17:06.460]   And so what we see in,
[00:17:06.460 --> 00:17:11.900]   when we're speaking to new users of Weights & Biases,
[00:17:11.900 --> 00:17:16.700]   or else, and they could be like, again,
[00:17:16.700 --> 00:17:17.700]   like they can be like students,
[00:17:17.700 --> 00:17:19.860]   they can be like advanced researchers,
[00:17:19.860 --> 00:17:24.340]   or they can be people doing deep learning in industry,
[00:17:24.340 --> 00:17:25.980]   in like large companies,
[00:17:25.980 --> 00:17:28.780]   large like modern like tech companies,
[00:17:28.780 --> 00:17:30.460]   whose products are powered by ML.
[00:17:31.500 --> 00:17:35.780]   A lot of people either don't have like a clear process,
[00:17:35.780 --> 00:17:39.860]   or they have their own like personal process
[00:17:39.860 --> 00:17:43.700]   that is like almost unique to each person in the team.
[00:17:43.700 --> 00:17:46.140]   And some people are using Google Sheets,
[00:17:46.140 --> 00:17:51.100]   some people are printing results to a text file,
[00:17:51.100 --> 00:17:55.980]   some people have their own kind of custom versioning
[00:17:55.980 --> 00:17:58.220]   and tracking pipelines,
[00:17:58.220 --> 00:18:00.820]   with all their own like little code.
[00:18:00.820 --> 00:18:03.020]   But again, it's all quite fragile.
[00:18:03.020 --> 00:18:06.300]   And none of this really scales across a team
[00:18:06.300 --> 00:18:08.820]   and across like a broader like group
[00:18:08.820 --> 00:18:10.420]   of machine learning practitioners.
[00:18:10.420 --> 00:18:13.980]   And so when someone leaves a team
[00:18:13.980 --> 00:18:16.020]   or someone like joins the team,
[00:18:16.020 --> 00:18:18.820]   it can be very hard for them to like quickly understand,
[00:18:18.820 --> 00:18:21.180]   you know, what are the experiments that were run?
[00:18:21.180 --> 00:18:24.300]   What were the hyperparameters used?
[00:18:24.300 --> 00:18:26.620]   Where are the model weights stored?
[00:18:26.620 --> 00:18:29.860]   You know, like it's still a little bit
[00:18:29.860 --> 00:18:31.220]   of the wild west, you know,
[00:18:31.220 --> 00:18:35.060]   and that you would never have that with ideally,
[00:18:35.060 --> 00:18:36.100]   at least you would never have that
[00:18:36.100 --> 00:18:38.660]   in a software development context.
[00:18:38.660 --> 00:18:40.820]   You know, if you were to join an engineering team,
[00:18:40.820 --> 00:18:44.820]   you know, everyone probably works out at GitHub or GitLab,
[00:18:44.820 --> 00:18:47.180]   you can see everyone's commits,
[00:18:47.180 --> 00:18:50.380]   probably they're using a task management tool, maybe JIRA.
[00:18:50.380 --> 00:18:52.940]   You can see the tickets that were filed,
[00:18:52.940 --> 00:18:55.060]   you can see the context in those tickets,
[00:18:55.060 --> 00:18:59.180]   but we don't really have equivalent for that in machine.
[00:18:59.180 --> 00:19:01.900]   And so that's something that weights and biases
[00:19:01.900 --> 00:19:03.620]   is hoping to address.
[00:19:03.620 --> 00:19:06.140]   And so just on the same study quickly,
[00:19:06.140 --> 00:19:09.700]   the same respondents were asked this question of,
[00:19:09.700 --> 00:19:12.580]   do they have a process to make your models reproducible?
[00:19:12.580 --> 00:19:16.180]   And 34% said, no, they didn't.
[00:19:16.180 --> 00:19:18.460]   And so it's, again, there was an issue we see
[00:19:18.460 --> 00:19:21.900]   across a whole bunch of fields.
[00:19:21.900 --> 00:19:25.420]   I'm going to jump through a few slides
[00:19:25.420 --> 00:19:28.060]   'cause I'm just conscious of time here.
[00:19:28.060 --> 00:19:33.060]   And so this, I'll take you through a quick high level view
[00:19:33.060 --> 00:19:39.020]   of what weights and biases does and like hopes to achieve.
[00:19:39.020 --> 00:19:43.700]   And so speaking about traditional like software development
[00:19:43.700 --> 00:19:45.580]   at the top row here,
[00:19:45.580 --> 00:19:47.620]   you can see like an example of some of the like
[00:19:47.620 --> 00:19:51.180]   most common tools that are used as part of like a workflow
[00:19:51.180 --> 00:19:55.300]   when like writing software, you know, software 1.0
[00:19:55.300 --> 00:19:57.540]   to traditional software engineering.
[00:19:57.540 --> 00:19:59.980]   And you see obviously people use GitHub
[00:19:59.980 --> 00:20:01.540]   or GitLab to version their code.
[00:20:01.540 --> 00:20:03.340]   Like I mentioned, like JIRA
[00:20:03.340 --> 00:20:05.580]   for like collaboration and task management,
[00:20:05.580 --> 00:20:09.340]   to, you know, tools for deployment and CI/CD.
[00:20:09.340 --> 00:20:13.180]   But there's no common standard
[00:20:13.180 --> 00:20:18.180]   for the equivalent steps in machine learning
[00:20:18.180 --> 00:20:23.180]   or, you know, if you want to call it software 2.0.
[00:20:23.180 --> 00:20:24.260]   And like I said, you know,
[00:20:24.260 --> 00:20:28.660]   people have different ways to version their data
[00:20:28.660 --> 00:20:29.740]   and their models.
[00:20:29.740 --> 00:20:32.340]   You know, they can be a bunch of like files
[00:20:32.340 --> 00:20:34.500]   in like an S3 bucket somewhere.
[00:20:34.500 --> 00:20:38.420]   You know, it depends on like how well those files are named
[00:20:38.420 --> 00:20:42.060]   to like to know exactly what's in them.
[00:20:42.060 --> 00:20:44.220]   Maybe the person who put them in there left
[00:20:44.220 --> 00:20:46.420]   and has never documented that this is where
[00:20:46.420 --> 00:20:47.900]   all the model weights lives.
[00:20:47.900 --> 00:20:49.580]   And so there's like a risk there
[00:20:49.580 --> 00:20:52.260]   because no one knows what's in that bucket anymore.
[00:20:52.260 --> 00:20:54.540]   Again, a lot of like text files
[00:20:54.540 --> 00:20:55.740]   all the way through the process
[00:20:55.740 --> 00:20:58.300]   in terms of like managing their experiments.
[00:20:58.300 --> 00:21:01.020]   We see a lot of people with, you know,
[00:21:01.020 --> 00:21:03.340]   and it's a common joke in data science, right?
[00:21:03.340 --> 00:21:06.660]   You know, tens and hundreds of notebooks
[00:21:06.660 --> 00:21:11.180]   and notebook, untitled notebook underscore, you know,
[00:21:11.180 --> 00:21:15.100]   1001 where they've done their experiments there.
[00:21:15.100 --> 00:21:18.060]   People take screenshots of charts they've plotted
[00:21:18.060 --> 00:21:21.020]   in notebooks and place them into, you know,
[00:21:21.020 --> 00:21:24.620]   Google Docs or other places to be able to share.
[00:21:24.620 --> 00:21:29.620]   And so there's no set of like tools
[00:21:29.620 --> 00:21:33.580]   or the rather there's no like standard set of tools
[00:21:33.580 --> 00:21:36.380]   that we see that like kind of covers a lot of these steps
[00:21:36.380 --> 00:21:39.700]   in the ML pipeline.
[00:21:39.700 --> 00:21:42.580]   And so this is what Weights & Biases hopes to fix.
[00:21:42.580 --> 00:21:47.340]   And so this is an idea of where we are at the moment.
[00:21:47.340 --> 00:21:52.340]   And so from artifacts all the way up to being able
[00:21:52.340 --> 00:21:55.460]   to share your results, kind of Weights & Biases
[00:21:55.460 --> 00:21:59.540]   has tools to cover the machine learning pipeline.
[00:21:59.540 --> 00:22:01.340]   And so we'll be working on, you can see on the far right,
[00:22:01.340 --> 00:22:03.260]   we'll be working on production monitoring
[00:22:03.260 --> 00:22:05.380]   and hopefully later this year.
[00:22:05.380 --> 00:22:07.700]   And so Weights & Biases can help you, you know,
[00:22:07.700 --> 00:22:11.500]   yeah, version your data, visualize your data,
[00:22:11.500 --> 00:22:14.940]   track all of your experiments and like hyper parameters
[00:22:14.940 --> 00:22:17.060]   and results and like, you know, loss curves
[00:22:17.060 --> 00:22:19.100]   and all of the metrics you care about.
[00:22:19.100 --> 00:22:22.540]   We can also help you with hyper parameter optimization
[00:22:22.540 --> 00:22:25.220]   to like get the most out of your experiments.
[00:22:25.220 --> 00:22:28.180]   And then finally we have reports where you can share
[00:22:28.180 --> 00:22:32.220]   either, you know, small, quick insights that you got
[00:22:32.220 --> 00:22:34.420]   from a particular experiment or, you know,
[00:22:34.420 --> 00:22:39.420]   a lot longer, more detailed, like richer documents as well.
[00:22:39.420 --> 00:22:44.580]   And so Weights & Biases is we're library agnostic,
[00:22:44.580 --> 00:22:46.060]   we're framework agnostic.
[00:22:46.060 --> 00:22:48.420]   And so we have integrations with, yeah,
[00:22:48.420 --> 00:22:52.300]   Hugging Face and YOLOv5 and Spacey and PyTorch Lightning.
[00:22:52.300 --> 00:22:56.380]   And we also work across, you know, every framework,
[00:22:56.380 --> 00:22:58.300]   PyTorch, Keras, TensorFlow, JAX.
[00:22:58.300 --> 00:23:04.140]   And we have a bunch of partnerships with other folks too.
[00:23:04.140 --> 00:23:09.100]   And so I'm going to jump into a quick demo
[00:23:09.100 --> 00:23:12.300]   of what Weights & Biases actually looks like.
[00:23:12.300 --> 00:23:16.900]   And so this is, let's do this.
[00:23:16.900 --> 00:23:18.100]   Oh God, cool.
[00:23:18.100 --> 00:23:24.580]   This is an example of what a Weights & Biases workspace
[00:23:24.580 --> 00:23:25.420]   looks like.
[00:23:25.420 --> 00:23:31.420]   And so on the left here, you have all of the experiments
[00:23:31.420 --> 00:23:32.820]   that you've run.
[00:23:32.820 --> 00:23:35.900]   On the right, we have different panels
[00:23:35.900 --> 00:23:38.060]   to give us some information about what's going on.
[00:23:38.060 --> 00:23:40.660]   And so we've got a few markdown text panels
[00:23:40.660 --> 00:23:43.460]   just to describe this workspace.
[00:23:43.460 --> 00:23:48.460]   We've got some larger KPI panels to give us a quick snapshot
[00:23:48.460 --> 00:23:53.780]   of what the most, the best performing model is in this case.
[00:23:53.780 --> 00:23:58.380]   And we're taking the max of the AUC score
[00:23:58.380 --> 00:24:02.740]   and you can see the experiment name is given here.
[00:24:02.740 --> 00:24:05.700]   And all the experiment names are generated randomly,
[00:24:05.700 --> 00:24:08.020]   but you can also like define the name
[00:24:08.020 --> 00:24:09.300]   of your experiments also.
[00:24:10.420 --> 00:24:15.420]   We have our good old curve metric and loss curves here
[00:24:15.420 --> 00:24:17.500]   for all of these experiments.
[00:24:17.500 --> 00:24:21.100]   And you can, for each one of these particular curves,
[00:24:21.100 --> 00:24:24.340]   you can click, you can see it being highlighted on the left
[00:24:24.340 --> 00:24:26.420]   and you can click into this and look deeper.
[00:24:26.420 --> 00:24:28.420]   We will do that in a second.
[00:24:28.420 --> 00:24:33.340]   We have a bunch of custom charts that you can log to.
[00:24:33.340 --> 00:24:35.020]   So in this case, like a rock chart,
[00:24:35.020 --> 00:24:37.140]   we have a feature importance chart.
[00:24:37.140 --> 00:24:40.820]   We have a confusion matrices and a whole bunch of others.
[00:24:40.820 --> 00:24:43.660]   If you don't find the chart you're looking for,
[00:24:43.660 --> 00:24:47.180]   you can also like log your matplotlib charts directly
[00:24:47.180 --> 00:24:48.020]   to weights and biases.
[00:24:48.020 --> 00:24:49.900]   And we'll also display those.
[00:24:49.900 --> 00:24:53.820]   And so you're not limited by, yeah,
[00:24:53.820 --> 00:24:56.460]   by like our set of like inbuilt charts.
[00:24:56.460 --> 00:25:01.820]   And so if we're going to kind of start thinking
[00:25:01.820 --> 00:25:06.060]   about how we approach our experiments, we can say,
[00:25:06.060 --> 00:25:10.820]   "Okay, let's look at the best performing run we have here,
[00:25:10.820 --> 00:25:12.420]   which is this Azure Galaxy."
[00:25:12.420 --> 00:25:15.020]   And so we can click on that run name.
[00:25:15.020 --> 00:25:20.020]   And so now we see the same charts for this individual run.
[00:25:20.020 --> 00:25:28.100]   And we also see the custom charts that we mentioned here.
[00:25:28.100 --> 00:25:30.220]   But we might want to know,
[00:25:30.220 --> 00:25:33.260]   okay, like what exactly was this model trained on?
[00:25:33.260 --> 00:25:37.580]   And so we can go into the details of this particular run
[00:25:37.580 --> 00:25:41.140]   and we get a lot more rich information.
[00:25:41.140 --> 00:25:41.980]   And in this case, you know,
[00:25:41.980 --> 00:25:44.620]   everything from like the OS that was trained on,
[00:25:44.620 --> 00:25:49.260]   the Python version, even a Colab link.
[00:25:49.260 --> 00:25:53.260]   And so weights and biases has a feature called code saving,
[00:25:53.260 --> 00:25:56.980]   and it's turned off by default for privacy reasons.
[00:25:56.980 --> 00:25:59.940]   But if you turn it on in your user settings,
[00:25:59.940 --> 00:26:03.860]   weights and biases will also actually log the script
[00:26:03.860 --> 00:26:06.620]   or the notebook where weights and biases was called.
[00:26:06.620 --> 00:26:10.900]   And so you can get some really nice reproducibility here,
[00:26:10.900 --> 00:26:12.380]   not only of like the, like I mentioned,
[00:26:12.380 --> 00:26:14.740]   the hyper parameters that was used,
[00:26:14.740 --> 00:26:17.380]   but actually the code itself that was like used
[00:26:17.380 --> 00:26:18.700]   to train this model.
[00:26:18.700 --> 00:26:23.220]   And so then in this same section,
[00:26:23.220 --> 00:26:25.020]   you can then also, okay, here,
[00:26:25.020 --> 00:26:28.300]   see all of the configurations that were used
[00:26:28.300 --> 00:26:30.020]   to train this particular model,
[00:26:30.020 --> 00:26:35.020]   as well as all of the results and metrics
[00:26:35.020 --> 00:26:37.980]   for this particular model.
[00:26:37.980 --> 00:26:40.300]   The other thing that's logged here
[00:26:40.300 --> 00:26:43.620]   are the, what we call the artifacts from this model.
[00:26:43.620 --> 00:26:45.340]   So essentially these are, you know,
[00:26:45.340 --> 00:26:50.340]   just either data sets or model weights or model files.
[00:26:50.340 --> 00:26:52.900]   And so you can see on the right,
[00:26:52.900 --> 00:26:55.900]   we have artifact inputs and inputs
[00:26:55.900 --> 00:26:57.580]   into this particular training one,
[00:26:57.580 --> 00:26:59.340]   was this data set.
[00:26:59.340 --> 00:27:01.540]   And you can see, this is a versioned data set.
[00:27:01.540 --> 00:27:03.380]   There's like a V2 here,
[00:27:03.380 --> 00:27:06.820]   and it's been used 19 times in different,
[00:27:06.820 --> 00:27:08.740]   in other training sets.
[00:27:08.740 --> 00:27:11.700]   And you can see the outputs of this experiment
[00:27:11.700 --> 00:27:15.180]   were a model, a model file,
[00:27:15.180 --> 00:27:17.700]   as well as some other weights and biases tables
[00:27:17.700 --> 00:27:21.060]   to help with the visualization of results.
[00:27:21.060 --> 00:27:25.940]   You can also see here the system metrics.
[00:27:25.940 --> 00:27:29.260]   And so when you log anything with weights and biases,
[00:27:29.260 --> 00:27:33.300]   the system metrics are logged automatically.
[00:27:33.300 --> 00:27:34.860]   You don't have to do any extra,
[00:27:34.860 --> 00:27:39.340]   write any extra code to log those.
[00:27:39.340 --> 00:27:41.420]   And so in this case,
[00:27:41.420 --> 00:27:44.100]   this model was just trained on a CPU.
[00:27:44.100 --> 00:27:47.020]   And so you can see the CPU utilization,
[00:27:47.020 --> 00:27:49.900]   the system memory, threads,
[00:27:49.900 --> 00:27:52.140]   and a bunch of other useful things.
[00:27:52.140 --> 00:27:54.660]   If you were to look at this on a GPU,
[00:27:54.660 --> 00:27:56.180]   and we will shortly,
[00:27:56.180 --> 00:27:58.860]   you would also see like your GPU memory usage.
[00:27:58.860 --> 00:28:03.860]   And so that's a super popular chart with all of our users,
[00:28:03.860 --> 00:28:06.940]   because they can understand if they're fully utilizing
[00:28:06.940 --> 00:28:09.700]   their like expensive GPU resources,
[00:28:09.700 --> 00:28:12.060]   and they can understand,
[00:28:12.060 --> 00:28:14.860]   whether they can maybe use a slightly larger model
[00:28:14.860 --> 00:28:17.460]   or increase their batch size a little bit more
[00:28:17.460 --> 00:28:21.300]   to really like get the most out of their GPU resources.
[00:28:21.300 --> 00:28:24.060]   Great.
[00:28:24.060 --> 00:28:27.020]   So if we take a step back now,
[00:28:27.020 --> 00:28:31.580]   we will say, okay, we've run a bunch of experiments,
[00:28:31.580 --> 00:28:34.220]   kind of like manually changing things
[00:28:34.220 --> 00:28:37.140]   to understand how to improve our model.
[00:28:37.140 --> 00:28:43.340]   But now we really want to focus on optimizing this model,
[00:28:43.340 --> 00:28:45.740]   you know, and getting the most of it.
[00:28:45.740 --> 00:28:47.940]   And maybe this is for something
[00:28:47.940 --> 00:28:50.740]   that you want to deploy into production,
[00:28:50.740 --> 00:28:54.380]   or maybe it's for a paper you're hoping to publish.
[00:28:54.380 --> 00:28:55.780]   And so, you know, you want to really like,
[00:28:55.780 --> 00:28:57.940]   really get the most out of your metric.
[00:28:57.940 --> 00:29:01.300]   And so weights and biases has the sweeps too,
[00:29:01.300 --> 00:29:02.220]   like I mentioned.
[00:29:02.220 --> 00:29:05.820]   And so with weights and biases sweeps,
[00:29:05.820 --> 00:29:08.820]   you can quickly do a lot of hyperparameter search
[00:29:08.820 --> 00:29:13.820]   to like find the best hyperparameters for your model.
[00:29:13.820 --> 00:29:16.780]   The nice thing with weights and biases sweeps
[00:29:16.780 --> 00:29:19.420]   is that you can either use our algorithms,
[00:29:19.420 --> 00:29:20.940]   and there's, you know, we have, obviously,
[00:29:20.940 --> 00:29:23.860]   like a random search, we have a grid search,
[00:29:23.860 --> 00:29:27.660]   and we have a Bayesian optimization algorithm.
[00:29:27.660 --> 00:29:30.660]   But you don't have to, if you want, you know,
[00:29:30.660 --> 00:29:33.300]   to use like other, like more advanced algorithms,
[00:29:33.300 --> 00:29:37.100]   maybe from like Optuna or Raytune.
[00:29:37.100 --> 00:29:39.700]   We also have integrations with those libraries
[00:29:39.700 --> 00:29:44.020]   that will let you use, you know,
[00:29:44.020 --> 00:29:47.460]   those like specialized algorithms
[00:29:47.460 --> 00:29:49.460]   with weights and biases sweeps.
[00:29:49.460 --> 00:29:52.540]   And so you can get the best of both worlds.
[00:29:52.540 --> 00:29:55.020]   And so this is an example of where I've done
[00:29:55.020 --> 00:29:58.980]   about a thousand sweeps to try and like find
[00:29:58.980 --> 00:30:00.780]   the best hyperparameters here.
[00:30:00.780 --> 00:30:03.100]   And again, we see our main metrics,
[00:30:03.100 --> 00:30:05.700]   we see our loss curves.
[00:30:05.700 --> 00:30:11.340]   And this is the chart I'd like to spend some time on.
[00:30:11.340 --> 00:30:15.380]   And so here you can see, it looks a little bit crazy,
[00:30:15.380 --> 00:30:17.140]   but this is essentially showing
[00:30:17.140 --> 00:30:22.140]   the hyperparameter values that were used
[00:30:22.140 --> 00:30:27.500]   in each of the vertical columns,
[00:30:27.500 --> 00:30:31.020]   along with the final metric that we care about,
[00:30:31.020 --> 00:30:34.140]   in this case, the AUC metric.
[00:30:34.140 --> 00:30:39.060]   And so you can like highlight a particular run and see,
[00:30:39.060 --> 00:30:44.060]   okay, this run had a value like early stopping rounds of 20,
[00:30:45.180 --> 00:30:50.180]   used a gamma of 0.9, a learning rate of 1.0, excuse me,
[00:30:50.180 --> 00:30:54.380]   and a min child weight of 86.
[00:30:54.380 --> 00:30:58.100]   And so we plot this for every one
[00:30:58.100 --> 00:30:59.580]   of the experiments we've done.
[00:30:59.580 --> 00:31:00.620]   And so this is quite noisy
[00:31:00.620 --> 00:31:02.780]   and we can't really get much of a signal out of it,
[00:31:02.780 --> 00:31:06.140]   but probably we only care about looking at the runs
[00:31:06.140 --> 00:31:06.980]   that perform the best.
[00:31:06.980 --> 00:31:09.260]   And so we can do like a little filtering here.
[00:31:09.260 --> 00:31:12.340]   And now we get a little bit more signal
[00:31:12.340 --> 00:31:13.900]   and a little bit more understanding
[00:31:13.900 --> 00:31:16.900]   of like what a hyperparameters are important to us.
[00:31:16.900 --> 00:31:21.500]   And so now you can see some ranges seem to be important,
[00:31:21.500 --> 00:31:22.820]   others may be less so.
[00:31:22.820 --> 00:31:25.500]   And so you can see a learning rate,
[00:31:25.500 --> 00:31:29.020]   you know, of between 0.8 and 0.85,
[00:31:29.020 --> 00:31:30.780]   looks like the kind of sweet spot
[00:31:30.780 --> 00:31:32.740]   for this particular model and data set.
[00:31:32.740 --> 00:31:37.860]   A gamma, you know, between maybe 0.5 and 0.7,
[00:31:37.860 --> 00:31:40.460]   look useful for this,
[00:31:40.460 --> 00:31:43.340]   but then other values like early stopping rounds,
[00:31:43.340 --> 00:31:45.780]   doesn't seem to matter if it's 10 or if it's 20
[00:31:45.780 --> 00:31:49.140]   or if it's 40 and the same for min child weight,
[00:31:49.140 --> 00:31:50.420]   there's a lot more,
[00:31:50.420 --> 00:31:53.900]   a lot less sensitivity there in min child weight.
[00:31:53.900 --> 00:31:56.660]   And you can like, yeah,
[00:31:56.660 --> 00:31:58.820]   it's less important to like try and like fine tune
[00:31:58.820 --> 00:32:02.060]   that hyperparameter by the looks of it.
[00:32:02.060 --> 00:32:04.620]   And so this is like one really nice way to try
[00:32:04.620 --> 00:32:07.140]   and really understand, you know,
[00:32:07.140 --> 00:32:09.180]   how to get the most out of your model.
[00:32:09.180 --> 00:32:12.300]   Okay, moving on.
[00:32:12.300 --> 00:32:16.460]   So if we have, we'll say we have done our step one,
[00:32:16.460 --> 00:32:21.060]   which was, you know, to do like a lot of kind of like
[00:32:21.060 --> 00:32:23.100]   manual experimentation to understand, you know,
[00:32:23.100 --> 00:32:25.540]   the problem space,
[00:32:25.540 --> 00:32:27.940]   then we've got to come on to our step two here.
[00:32:27.940 --> 00:32:29.740]   We've done like some hyperparameter optimization
[00:32:29.740 --> 00:32:32.580]   to like really find the hyperparameters
[00:32:32.580 --> 00:32:36.700]   that will result in the highest metric that we care about.
[00:32:36.700 --> 00:32:39.500]   Maybe like a step three we want to do
[00:32:39.500 --> 00:32:42.860]   is to do some like deeper evaluation
[00:32:42.860 --> 00:32:46.460]   of like how that particular model is performing.
[00:32:46.460 --> 00:32:49.140]   And so we can use weights and biases tables
[00:32:49.140 --> 00:32:50.220]   for this evaluation.
[00:32:50.220 --> 00:32:53.340]   So I've just selected, this is a new project,
[00:32:53.340 --> 00:32:55.540]   but we can consider it the same.
[00:32:55.540 --> 00:32:59.620]   And so here I've just selected a single run.
[00:32:59.620 --> 00:33:00.740]   And so in weights and biases,
[00:33:00.740 --> 00:33:05.740]   these little eyes show and hide the experiment results.
[00:33:05.740 --> 00:33:08.980]   And so here's all of our, excuse me,
[00:33:08.980 --> 00:33:12.380]   all of our experiments as expected.
[00:33:12.380 --> 00:33:14.580]   This is also a, was trained on a GPU.
[00:33:14.580 --> 00:33:16.620]   So we can look at the system metrics here
[00:33:16.620 --> 00:33:21.180]   and you can see that, where is it?
[00:33:21.180 --> 00:33:24.180]   Here are our GPU metrics.
[00:33:24.180 --> 00:33:25.980]   And so you can see, for example,
[00:33:25.980 --> 00:33:30.980]   the GPU utilization of this model was actually pretty low.
[00:33:30.980 --> 00:33:34.980]   And the memory allocation here, like was also pretty low,
[00:33:34.980 --> 00:33:36.380]   like 58%.
[00:33:36.380 --> 00:33:38.580]   So probably if it made sense,
[00:33:38.580 --> 00:33:41.420]   you could do like a larger batch size
[00:33:41.420 --> 00:33:42.860]   or maybe a larger model.
[00:33:42.860 --> 00:33:47.580]   And you get the most out of your GPU resources,
[00:33:47.580 --> 00:33:48.420]   like I mentioned.
[00:33:48.420 --> 00:33:50.940]   We can go back to our project.
[00:33:50.940 --> 00:33:54.340]   I'm sorry, I'll just hide these again.
[00:33:54.340 --> 00:33:57.500]   Great, so like I said,
[00:33:57.500 --> 00:34:01.340]   we want to evaluate how this model is performing.
[00:34:01.340 --> 00:34:04.620]   So this is where weights and biases tables
[00:34:04.620 --> 00:34:06.220]   can be super useful.
[00:34:06.220 --> 00:34:08.220]   And so tables, you can think of tables
[00:34:08.220 --> 00:34:11.340]   like a data frame, like a pandas data frame
[00:34:11.340 --> 00:34:13.980]   that can also support rich media.
[00:34:13.980 --> 00:34:16.580]   And that can be images, that can be video,
[00:34:16.580 --> 00:34:21.580]   it can be audio, 3D point clouds,
[00:34:21.580 --> 00:34:24.460]   sorry, point clouds, 3D molecules,
[00:34:24.460 --> 00:34:27.460]   and a whole bunch of like other
[00:34:27.460 --> 00:34:30.100]   like rich media visualizations.
[00:34:30.100 --> 00:34:35.460]   If you don't, if tables doesn't support the visualization
[00:34:35.460 --> 00:34:38.900]   that you like, you please feel free to reach out to us.
[00:34:38.900 --> 00:34:40.060]   And we would love to add it
[00:34:40.060 --> 00:34:43.100]   because we really want tables to be,
[00:34:43.100 --> 00:34:46.420]   to work across, you know, almost every like data format.
[00:34:46.420 --> 00:34:50.940]   And so you can have that rich media alongside your text
[00:34:50.940 --> 00:34:52.780]   and your integers and your floats
[00:34:52.780 --> 00:34:54.100]   and your categorical data,
[00:34:54.100 --> 00:34:58.300]   just like you would in a pandas data frame.
[00:34:58.300 --> 00:34:59.580]   And so here, what we're looking at
[00:34:59.580 --> 00:35:03.580]   are the results on an evaluation data set.
[00:35:04.660 --> 00:35:06.940]   And in this case, the status of this,
[00:35:06.940 --> 00:35:11.940]   trying to predict the kind of category of organism
[00:35:11.940 --> 00:35:14.500]   from the photo here.
[00:35:14.500 --> 00:35:19.100]   And so you can see, you can,
[00:35:19.100 --> 00:35:22.100]   so you can, with, sorry, with tables,
[00:35:22.100 --> 00:35:25.460]   then you can like kind of do a lot of the actions
[00:35:25.460 --> 00:35:30.740]   that you would expect, like in a regular data.
[00:35:30.740 --> 00:35:32.540]   And so you can do sorts.
[00:35:32.540 --> 00:35:35.980]   And so in this case, we have,
[00:35:35.980 --> 00:35:39.220]   we've logged the score at the output from the model
[00:35:39.220 --> 00:35:41.340]   for each one of the classes that we care about.
[00:35:41.340 --> 00:35:42.940]   I think there are eight classes.
[00:35:42.940 --> 00:35:49.260]   And so by sorting by the amphibia score, for example,
[00:35:49.260 --> 00:35:51.940]   you know, on the predictions side of things,
[00:35:51.940 --> 00:35:55.260]   we see a lot of frogs, which makes sense.
[00:35:55.260 --> 00:35:57.700]   Frogs, they're part of the amphibia family.
[00:35:57.700 --> 00:36:01.740]   And we can say, see the same for the Aves.
[00:36:02.620 --> 00:36:05.780]   And we can see, okay, this looks like a lot of birds here.
[00:36:05.780 --> 00:36:09.700]   And so, okay, obviously when the model is super confident
[00:36:09.700 --> 00:36:11.540]   that it's a bird, it's a bird.
[00:36:11.540 --> 00:36:13.340]   And so that's kind of interesting,
[00:36:13.340 --> 00:36:16.660]   but like not super useful to like scale.
[00:36:16.660 --> 00:36:19.940]   And so one thing you might be interested in
[00:36:19.940 --> 00:36:22.940]   is understanding the performance of your model
[00:36:22.940 --> 00:36:24.420]   for a particular class.
[00:36:24.420 --> 00:36:27.300]   'Cause obviously we want a decent performance
[00:36:27.300 --> 00:36:29.380]   across like every one of our classes.
[00:36:29.380 --> 00:36:32.500]   And so you might want to like filter this table.
[00:36:32.500 --> 00:36:36.060]   And we want to filter for, excuse me,
[00:36:36.060 --> 00:36:41.060]   all of the birds in our validation data sets.
[00:36:41.060 --> 00:36:46.300]   And so we can filter the truth column
[00:36:46.300 --> 00:36:51.300]   to be equal to Aves.
[00:36:51.300 --> 00:36:52.300]   We click apply.
[00:36:52.300 --> 00:36:56.620]   Cool, and now we're just looking at when the,
[00:36:56.620 --> 00:37:01.620]   all of the instances where the ground truth is a bird.
[00:37:01.620 --> 00:37:06.860]   And so now we can look at the cases
[00:37:06.860 --> 00:37:10.820]   when our model is predicting the wrong thing.
[00:37:10.820 --> 00:37:14.060]   And so we can like insert columns into tables
[00:37:14.060 --> 00:37:16.780]   and then we can like edit exactly
[00:37:16.780 --> 00:37:17.780]   what should be in this column.
[00:37:17.780 --> 00:37:18.980]   And so in this case,
[00:37:18.980 --> 00:37:22.580]   we want to look at all of the incorrect predictions.
[00:37:22.580 --> 00:37:26.340]   So we look at the times where the truth value
[00:37:26.340 --> 00:37:30.780]   is not equal to the prediction or the guess.
[00:37:30.780 --> 00:37:33.300]   We can name this columns.
[00:37:33.300 --> 00:37:36.020]   We'll say this column is for like incorrect.
[00:37:36.020 --> 00:37:39.540]   Actually, no, let's do it.
[00:37:39.540 --> 00:37:41.260]   It's easier, I think, to understand
[00:37:41.260 --> 00:37:42.860]   just for the times when it's correct, right?
[00:37:42.860 --> 00:37:44.220]   So we say correct.
[00:37:44.220 --> 00:37:50.300]   And tables always tries to make a guess
[00:37:50.300 --> 00:37:54.740]   at what data type this column is gonna be.
[00:37:54.740 --> 00:37:56.580]   And so this is gonna be a Boolean column.
[00:37:56.580 --> 00:37:58.420]   And so we're gonna, we can leave that.
[00:37:58.420 --> 00:38:00.180]   But if it made sense, you know,
[00:38:00.180 --> 00:38:01.860]   it could also be a string,
[00:38:01.860 --> 00:38:05.140]   but this column should be a Boolean.
[00:38:05.140 --> 00:38:06.540]   So we leave it as a Boolean.
[00:38:06.540 --> 00:38:10.580]   Cool, and so now we can see all of the times
[00:38:10.580 --> 00:38:12.780]   when the prediction was correct.
[00:38:12.780 --> 00:38:17.020]   We can again sort and see the times
[00:38:17.020 --> 00:38:19.300]   when the prediction was like incorrect.
[00:38:19.300 --> 00:38:23.740]   And so here, and sorry, these images are a little small,
[00:38:23.740 --> 00:38:24.700]   but for example, here, you know,
[00:38:24.700 --> 00:38:28.980]   you've got a guess in that this duck
[00:38:28.980 --> 00:38:33.980]   looks like a mollusk was predicted to be a mollusk.
[00:38:33.980 --> 00:38:35.780]   This is a bird, a sad bird.
[00:38:35.780 --> 00:38:39.500]   Okay, cool, so now we have an idea of,
[00:38:39.500 --> 00:38:41.740]   on a sample by sample basis,
[00:38:41.740 --> 00:38:44.420]   which predictions are correct or incorrect.
[00:38:44.420 --> 00:38:46.860]   But we wanna look at the aggregate performance
[00:38:46.860 --> 00:38:48.700]   of our predictions.
[00:38:48.700 --> 00:38:51.580]   And so we can then group by this column
[00:38:51.580 --> 00:38:54.700]   and you can group by, you know, any categorical column,
[00:38:54.700 --> 00:38:58.300]   and then immediately get a much richer picture
[00:38:58.300 --> 00:39:02.660]   of how we're doing on the wider side of things.
[00:39:02.660 --> 00:39:05.060]   And so this shows all of the cases
[00:39:05.060 --> 00:39:10.060]   when our predictions are incorrect or correct down here.
[00:39:10.060 --> 00:39:14.700]   And so you can look at the incorrect predictions
[00:39:14.700 --> 00:39:18.180]   and, you know, try and better understand
[00:39:18.180 --> 00:39:19.500]   where your model might be wrong.
[00:39:19.500 --> 00:39:23.100]   And so in this case, you know, the model was incorrect.
[00:39:23.100 --> 00:39:24.740]   It didn't predict a bird,
[00:39:24.740 --> 00:39:26.460]   but you can see our data isn't great.
[00:39:26.460 --> 00:39:28.580]   You know, it's kind of very hard to see
[00:39:28.580 --> 00:39:30.340]   the birds in these photos.
[00:39:30.340 --> 00:39:33.980]   And so this might be a case where you can kind of dive deep,
[00:39:33.980 --> 00:39:36.180]   deeper into your labeling or your data sets.
[00:39:36.180 --> 00:39:40.220]   And, you know, maybe you wanna remove these images
[00:39:40.220 --> 00:39:42.660]   or maybe you wanna crop them to be more focused
[00:39:42.660 --> 00:39:43.780]   on the bird itself.
[00:39:43.780 --> 00:39:47.460]   Like in this case, I don't even see a bird.
[00:39:47.460 --> 00:39:50.500]   Oh yeah, so there's like a bird here and a bird here,
[00:39:50.500 --> 00:39:53.220]   but it's very fuzzy and it's not super clear
[00:39:53.220 --> 00:39:54.660]   that this is an image of a bird.
[00:39:54.660 --> 00:39:58.820]   So you can't really blame the model that it's struggling.
[00:39:58.820 --> 00:40:04.980]   And so when in tables, when you group by a column,
[00:40:04.980 --> 00:40:08.180]   all of your other columns get grouped by two.
[00:40:08.180 --> 00:40:11.700]   And so you can see for the predictions that were incorrect,
[00:40:11.700 --> 00:40:16.700]   the distribution of classes, predictions.
[00:40:17.540 --> 00:40:19.300]   And so you can see here,
[00:40:19.300 --> 00:40:23.300]   there was the most confusion between birds and reptiles
[00:40:23.300 --> 00:40:27.780]   in our data, but also, you know,
[00:40:27.780 --> 00:40:30.180]   mammals and then like plants here.
[00:40:30.180 --> 00:40:33.300]   We only have a few samples here, so it's not great.
[00:40:33.300 --> 00:40:38.260]   And then you can also see all of the scores as well.
[00:40:38.260 --> 00:40:40.940]   And so you can see, if you wanna look at the,
[00:40:40.940 --> 00:40:45.300]   if we say, if we look at the predictions for birds,
[00:40:46.300 --> 00:40:49.860]   obviously we would like the values for birds
[00:40:49.860 --> 00:40:51.740]   to be high in this case.
[00:40:51.740 --> 00:40:54.260]   But like we said before, the model struggled.
[00:40:54.260 --> 00:40:59.260]   And so it was only given like low scores to the bird class.
[00:40:59.260 --> 00:41:06.940]   And so that's a quick tour through Weights and Biases tables
[00:41:06.940 --> 00:41:09.460]   and a quick tour through the product.
[00:41:09.460 --> 00:41:13.020]   And excuse me, I didn't cover our artifacts feature,
[00:41:13.020 --> 00:41:18.020]   which lets you join up your data sets and models
[00:41:18.020 --> 00:41:22.620]   to your experiments to have a nice data lineage,
[00:41:22.620 --> 00:41:24.860]   but happy to talk about it later
[00:41:24.860 --> 00:41:27.980]   in the, during the competition or in the Q&A,
[00:41:27.980 --> 00:41:28.860]   if it makes sense.
[00:41:28.860 --> 00:41:32.980]   And yeah, Stella, do we,
[00:41:32.980 --> 00:41:35.820]   should we take any question or two,
[00:41:35.820 --> 00:41:37.180]   or I'm happy to wrap up here.
[00:41:37.180 --> 00:41:41.820]   I don't want to slow down the rest of the seminar for folks.
[00:41:42.820 --> 00:41:47.420]   - Yes, thank you for introducing NICE team for Weights and Biases.
[00:41:47.420 --> 00:41:51.020]   There is some question, or if there is some question,
[00:41:51.020 --> 00:41:54.100]   please open your mic and tell him.
[00:41:54.100 --> 00:41:57.660]   Yeah, is there any question among the organizers?
[00:41:57.660 --> 00:42:04.100]   - I see Andrea posted a couple of questions.
[00:42:04.100 --> 00:42:05.740]   So I could maybe take those first.
[00:42:05.740 --> 00:42:10.220]   And so the first question was comparing TensorBoard,
[00:42:10.220 --> 00:42:13.740]   which Weights and Biases function would be the most powerful.
[00:42:13.740 --> 00:42:19.260]   And so, yeah, you know, TensorBoard is like a great tool
[00:42:19.260 --> 00:42:23.500]   to be able to like quickly monitor your experiments.
[00:42:23.500 --> 00:42:26.820]   One of the disadvantages of TensorBoard is that it's,
[00:42:26.820 --> 00:42:28.860]   unless you know, you'd be like,
[00:42:28.860 --> 00:42:30.900]   especially set up like a remote server,
[00:42:30.900 --> 00:42:35.380]   like all of your results in TensorBoard are stored locally.
[00:42:35.380 --> 00:42:38.220]   And so it's much harder to share those results
[00:42:38.220 --> 00:42:40.100]   with other folks.
[00:42:40.100 --> 00:42:42.260]   For example, in Weights and Biases,
[00:42:42.260 --> 00:42:44.940]   if I wanted to share this chart with someone,
[00:42:44.940 --> 00:42:46.700]   we have like reports,
[00:42:46.700 --> 00:42:50.460]   which I also didn't get a chance to share it,
[00:42:50.460 --> 00:42:52.460]   but I can say, you know,
[00:42:52.460 --> 00:42:56.820]   "Hey, look at my report."
[00:42:56.820 --> 00:43:02.060]   And then you can say, "Interesting results."
[00:43:02.060 --> 00:43:06.100]   And I can hit save on this.
[00:43:06.100 --> 00:43:10.860]   And then, yeah, let's do this for the fun.
[00:43:10.860 --> 00:43:12.580]   I'm gonna create a view only link
[00:43:12.580 --> 00:43:15.380]   that everyone will be able to see.
[00:43:15.380 --> 00:43:18.900]   And I can like drop that in the chat there,
[00:43:18.900 --> 00:43:21.780]   and I'll drop it in the YouTube chat as well.
[00:43:21.780 --> 00:43:22.620]   There we go.
[00:43:22.620 --> 00:43:23.460]   And then like straight away,
[00:43:23.460 --> 00:43:26.980]   I'm able to share this chart with whoever I want.
[00:43:26.980 --> 00:43:29.620]   And so TensorBoard, you know,
[00:43:29.620 --> 00:43:31.300]   you can do the same in TensorBoard,
[00:43:31.300 --> 00:43:33.220]   but you would have to take a screenshot,
[00:43:34.220 --> 00:43:37.460]   put it into a Slack message or a Google Doc,
[00:43:37.460 --> 00:43:42.260]   and then that screenshot has no context
[00:43:42.260 --> 00:43:45.540]   about the actual project and the source it came from.
[00:43:45.540 --> 00:43:47.100]   So that's one nice thing I see
[00:43:47.100 --> 00:43:49.220]   over Weights and Biases versus TensorBoard.
[00:43:49.220 --> 00:43:51.780]   The other would be tools like tables,
[00:43:51.780 --> 00:43:53.660]   and that's like data exploration,
[00:43:53.660 --> 00:43:58.140]   which TensorBoard can't do.
[00:43:58.140 --> 00:44:00.220]   But we also support TensorBoard.
[00:44:00.220 --> 00:44:03.980]   And so a lot of people have their code already instrumented,
[00:44:03.980 --> 00:44:04.940]   which then TensorBoard.
[00:44:04.940 --> 00:44:07.380]   And so we don't want to, you know,
[00:44:07.380 --> 00:44:09.220]   force people away from that.
[00:44:09.220 --> 00:44:11.620]   And so if you go to like our documentation,
[00:44:11.620 --> 00:44:14.740]   and I'll drop the links again in the Zoom chat
[00:44:14.740 --> 00:44:17.380]   and the YouTube chat here.
[00:44:17.380 --> 00:44:19.660]   You know, basically there is like,
[00:44:19.660 --> 00:44:22.340]   when you start a Weights and Biases run,
[00:44:22.340 --> 00:44:24.100]   and so you start a Weights and Biases run
[00:44:24.100 --> 00:44:26.100]   calling like wannabe.init,
[00:44:26.100 --> 00:44:29.980]   you call this one single argument,
[00:44:29.980 --> 00:44:34.620]   and that will basically like search
[00:44:34.620 --> 00:44:39.140]   for any of the TensorBoard logs that TensorBoard generates,
[00:44:39.140 --> 00:44:42.260]   and it'll like plot those in Weights and Biases.
[00:44:42.260 --> 00:44:44.660]   And so you can, I don't know if it's a good screenshot here,
[00:44:44.660 --> 00:44:47.140]   but you'll be able to see your like, you know,
[00:44:47.140 --> 00:44:49.060]   completely normal TensorBoard view
[00:44:49.060 --> 00:44:51.940]   in a Weights and Biases workspace.
[00:44:51.940 --> 00:44:55.140]   And then that also like makes TensorBoard easy then,
[00:44:55.140 --> 00:44:57.020]   you know, to share with your colleagues.
[00:44:57.020 --> 00:45:00.900]   So you can use TensorBoard and Weights and Biases.
[00:45:00.900 --> 00:45:04.020]   It's, you don't have to choose between one or the other
[00:45:04.020 --> 00:45:05.140]   if you don't want to.
[00:45:05.140 --> 00:45:08.220]   I hope that answers your question.
[00:45:08.220 --> 00:45:10.780]   Another one, a quick question.
[00:45:10.780 --> 00:45:12.300]   Is there a limit on the number of projects
[00:45:12.300 --> 00:45:14.140]   which can be created?
[00:45:14.140 --> 00:45:17.500]   There is no limit on the number of projects
[00:45:17.500 --> 00:45:18.580]   that can be created.
[00:45:18.580 --> 00:45:22.660]   So I can go to my projects section here,
[00:45:22.660 --> 00:45:25.460]   and I've like got a whole bunch of projects,
[00:45:25.460 --> 00:45:27.620]   207 projects.
[00:45:27.620 --> 00:45:29.820]   There are, we have what are called teams.
[00:45:29.820 --> 00:45:32.980]   And so when you want to like collaborate with people,
[00:45:32.980 --> 00:45:35.580]   you can invite people to your team,
[00:45:35.580 --> 00:45:38.580]   and then you can like all log to the same project.
[00:45:38.580 --> 00:45:41.740]   There is a limit, say if you're,
[00:45:41.740 --> 00:45:44.260]   so we offer free teams for academics.
[00:45:44.260 --> 00:45:46.460]   So if you're like a student or a researcher,
[00:45:46.460 --> 00:45:50.420]   or also if you have like an open source repository,
[00:45:50.420 --> 00:45:55.100]   we offer free academic teams to those people.
[00:45:55.100 --> 00:45:59.860]   We, they are limited by two to one team.
[00:45:59.860 --> 00:46:01.780]   And then if you have a,
[00:46:01.780 --> 00:46:04.220]   if you're a company and you want to use Weights and Biases,
[00:46:04.220 --> 00:46:06.460]   and you're paying for Weights and Biases,
[00:46:06.460 --> 00:46:08.940]   there are different plans for different numbers of teams.
[00:46:08.940 --> 00:46:11.820]   So I think the basic plan comes with, you know, one team,
[00:46:11.820 --> 00:46:14.420]   but then you can go to the more advanced plans,
[00:46:14.420 --> 00:46:16.060]   which have like, I think five teams,
[00:46:16.060 --> 00:46:17.540]   and then advanced again,
[00:46:17.540 --> 00:46:19.380]   which have like multiple teams again.
[00:46:19.380 --> 00:46:24.180]   - Actually, I was a bit curious about this
[00:46:24.180 --> 00:46:29.180]   because the costs of Weights and Biases is like non-trivial,
[00:46:29.180 --> 00:46:34.460]   even for reasonably well-funded startups.
[00:46:34.460 --> 00:46:37.980]   And this is, I think that this is part of the reason
[00:46:37.980 --> 00:46:41.700]   that like Weights and Biases might not have had
[00:46:41.700 --> 00:46:44.460]   quite as much adoption as hoped for,
[00:46:44.460 --> 00:46:47.700]   because it's even more expensive than PyCharm
[00:46:47.700 --> 00:46:51.780]   or many IDEs and other software support systems.
[00:46:51.780 --> 00:46:55.220]   So do you have any ideas?
[00:46:55.220 --> 00:47:00.420]   - Yeah, I don't work in our sales team,
[00:47:00.420 --> 00:47:02.220]   and so they can probably give you a better answer,
[00:47:02.220 --> 00:47:04.300]   but one good way to look at it is that
[00:47:04.300 --> 00:47:09.500]   it looks expensive upfront,
[00:47:09.500 --> 00:47:12.460]   but if you consider the cost of
[00:47:12.460 --> 00:47:17.420]   the, your machine learning engineers' time,
[00:47:17.420 --> 00:47:18.420]   obviously it depends, again,
[00:47:18.420 --> 00:47:20.260]   like where in the world you're based.
[00:47:21.260 --> 00:47:25.380]   But if you were to consider the cost of,
[00:47:25.380 --> 00:47:28.020]   say, their one person's salary,
[00:47:28.020 --> 00:47:30.180]   or maybe like half a person's salary,
[00:47:30.180 --> 00:47:34.020]   if one of your core team members leaves
[00:47:34.020 --> 00:47:37.580]   and they take with them all of the knowledge
[00:47:37.580 --> 00:47:41.700]   of where your, again, your experiments are saved,
[00:47:41.700 --> 00:47:42.620]   where your results are saved,
[00:47:42.620 --> 00:47:44.260]   where your model weights are saved,
[00:47:44.260 --> 00:47:46.340]   and that slows down your team
[00:47:46.340 --> 00:47:48.180]   because now they need to dig through
[00:47:48.180 --> 00:47:50.300]   and figure out where all that is,
[00:47:50.300 --> 00:47:54.900]   you can probably work out the time and hours spent
[00:47:54.900 --> 00:47:59.340]   trying to do some data archeology
[00:47:59.340 --> 00:48:00.780]   to try and understand it,
[00:48:00.780 --> 00:48:04.300]   or find the work and understand what's happened
[00:48:04.300 --> 00:48:07.860]   versus continuing to iterate and build on your experiment
[00:48:07.860 --> 00:48:10.780]   and weigh up the pros and cons like that.
[00:48:10.780 --> 00:48:13.380]   That's one mental model to frame it,
[00:48:13.380 --> 00:48:15.220]   and so it mightn't,
[00:48:15.220 --> 00:48:18.420]   and you mightn't see that benefit
[00:48:18.420 --> 00:48:20.500]   in the first three months or six months,
[00:48:20.500 --> 00:48:21.980]   but at some point,
[00:48:21.980 --> 00:48:24.220]   everyone's gonna move on to their next job,
[00:48:24.220 --> 00:48:28.940]   and if you don't have a good way in a team, in a company,
[00:48:28.940 --> 00:48:30.220]   of passing on that knowledge
[00:48:30.220 --> 00:48:32.620]   and having good processes to share knowledge,
[00:48:32.620 --> 00:48:36.980]   everyone's gonna face that issue at some point of saying,
[00:48:36.980 --> 00:48:39.940]   "Okay, which notebook was that result in
[00:48:39.940 --> 00:48:42.020]   "and that screenshot in that I need to find
[00:48:42.020 --> 00:48:44.740]   "in this model that we trained six months ago
[00:48:44.740 --> 00:48:45.820]   "that's now in production
[00:48:45.820 --> 00:48:47.820]   "and the person who trained it has left?"
[00:48:47.820 --> 00:48:51.460]   Yeah, but I mean, yeah, I guess as well,
[00:48:51.460 --> 00:48:55.700]   it's always a conversation with our sales teams as well,
[00:48:55.700 --> 00:48:59.260]   and they're always happy to talk more about pricing
[00:48:59.260 --> 00:49:02.380]   and come to arrangements for each customer as well.
[00:49:02.380 --> 00:49:04.300]   So if you have concerns about pricing,
[00:49:04.300 --> 00:49:07.700]   I would definitely still engage with sales,
[00:49:07.700 --> 00:49:11.780]   and they can, A, give more context around
[00:49:11.780 --> 00:49:13.020]   why the price is what it is,
[00:49:13.020 --> 00:49:15.900]   and then, B, if there's anything they can do
[00:49:15.900 --> 00:49:19.500]   in terms of discounts or adding more things in
[00:49:19.500 --> 00:49:21.660]   for the current price.
[00:49:21.660 --> 00:49:22.540]   Hope that helps.
[00:49:22.540 --> 00:49:26.580]   Okay, cool.
[00:49:26.580 --> 00:49:28.420]   Stella, sorry, I know there's a few more questions,
[00:49:28.420 --> 00:49:32.380]   but should we maybe pause and move on to the next speaker?
[00:49:32.380 --> 00:49:34.980]   - Yeah, thank you for your kind answering.
[00:49:34.980 --> 00:49:36.820]   If there is more question,
[00:49:36.820 --> 00:49:40.220]   you can communicate via YouTube channel,
[00:49:40.220 --> 00:49:41.980]   so please use it.
[00:49:41.980 --> 00:49:44.740]   So yeah, let's end up your session.
[00:49:44.740 --> 00:49:46.340]   Yeah, thank you, Morgan.
[00:49:46.340 --> 00:49:47.180]   - Great, yeah, thanks.
[00:49:47.180 --> 00:49:49.740]   I'll jump into the comments in the YouTube channel
[00:49:49.740 --> 00:49:51.940]   and answer some questions there if I can.
[00:49:51.940 --> 00:49:54.020]   - Yeah, thank you very much.
[00:49:54.020 --> 00:49:54.860]   - Super. - Okay.
[00:49:54.860 --> 00:49:55.700]   - Thanks, everyone.
[00:49:55.700 --> 00:49:57.020]   - Yeah, bye.
[00:49:57.020 --> 00:50:03.300]   Okay, then let's go to next session,
[00:50:03.300 --> 00:50:06.020]   and it is my session.
[00:50:06.020 --> 00:50:09.940]   (speaking in foreign language)
[00:50:09.940 --> 00:50:13.860]   (speaking in foreign language)
[00:50:13.860 --> 00:50:17.780]   (speaking in foreign language)
[00:50:17.780 --> 00:50:21.700]   (speaking in foreign language)
[00:50:21.700 --> 00:50:25.620]   (speaking in foreign language)
[00:50:25.620 --> 00:50:29.020]   (computer mouse clicking)
[00:50:29.020 --> 00:50:37.940]   (speaking in foreign language)
[00:50:37.940 --> 00:50:41.860]   (speaking in foreign language)
[00:50:42.380 --> 00:50:46.300]   (speaking in foreign language)
[00:50:46.300 --> 00:50:50.220]   (speaking in foreign language)
[00:50:50.220 --> 00:50:54.140]   (speaking in foreign language)
[00:50:54.140 --> 00:50:58.060]   (speaking in foreign language)
[00:50:58.060 --> 00:51:01.980]   (speaking in foreign language)
[00:51:01.980 --> 00:51:05.900]   (speaking in foreign language)
[00:51:05.900 --> 00:51:11.340]   (speaking in foreign language)
[00:51:11.340 --> 00:51:15.260]   (speaking in foreign language)
[00:51:15.260 --> 00:51:19.180]   (speaking in foreign language)
[00:51:20.140 --> 00:51:24.060]   (speaking in foreign language)
[00:51:24.060 --> 00:51:27.980]   (speaking in foreign language)
[00:51:28.940 --> 00:51:32.860]   (speaking in foreign language)
[00:51:32.860 --> 00:51:36.780]   (speaking in foreign language)
[00:51:36.780 --> 00:51:40.700]   (speaking in foreign language)
[00:51:40.700 --> 00:51:44.620]   (speaking in foreign language)
[00:51:44.620 --> 00:51:48.540]   (speaking in foreign language)
[00:51:48.540 --> 00:51:52.460]   (speaking in foreign language)
[00:51:52.460 --> 00:51:56.380]   (speaking in foreign language)
[00:51:56.380 --> 00:52:00.300]   (speaking in foreign language)
[00:52:00.300 --> 00:52:04.220]   (speaking in foreign language)
[00:52:04.220 --> 00:52:08.140]   (speaking in foreign language)
[00:52:08.140 --> 00:52:12.060]   (speaking in foreign language)
[00:52:12.060 --> 00:52:15.980]   (speaking in foreign language)
[00:52:15.980 --> 00:52:19.900]   (speaking in foreign language)
[00:52:19.900 --> 00:52:23.820]   (speaking in foreign language)
[00:52:23.820 --> 00:52:27.740]   (speaking in foreign language)
[00:52:27.740 --> 00:52:31.660]   (speaking in foreign language)
[00:52:31.660 --> 00:52:35.580]   (speaking in foreign language)
[00:52:35.580 --> 00:52:39.500]   (speaking in foreign language)
[00:52:39.500 --> 00:52:43.420]   (speaking in foreign language)
[00:52:43.420 --> 00:52:47.340]   (speaking in foreign language)
[00:52:47.340 --> 00:52:51.260]   (speaking in foreign language)
[00:52:51.260 --> 00:52:55.180]   (speaking in foreign language)
[00:52:55.180 --> 00:52:59.100]   (speaking in foreign language)
[00:52:59.100 --> 00:53:03.020]   (speaking in foreign language)
[00:53:03.020 --> 00:53:06.940]   (speaking in foreign language)
[00:53:06.940 --> 00:53:10.860]   (speaking in foreign language)
[00:53:11.820 --> 00:53:15.740]   (speaking in foreign language)
[00:53:15.740 --> 00:53:19.660]   (speaking in foreign language)
[00:53:20.580 --> 00:53:24.500]   (speaking in foreign language)
[00:53:24.500 --> 00:53:28.420]   (speaking in foreign language)
[00:53:28.420 --> 00:53:32.340]   (speaking in foreign language)
[00:53:32.340 --> 00:53:36.260]   (speaking in foreign language)
[00:53:37.180 --> 00:53:41.100]   (speaking in foreign language)
[00:53:41.100 --> 00:53:45.020]   (speaking in foreign language)
[00:53:45.020 --> 00:53:48.940]   (speaking in foreign language)
[00:53:49.860 --> 00:53:53.780]   (speaking in foreign language)
[00:53:53.780 --> 00:53:57.700]   (speaking in foreign language)
[00:53:58.620 --> 00:54:02.540]   (speaking in foreign language)
[00:54:02.540 --> 00:54:06.460]   (speaking in foreign language)
[00:54:06.460 --> 00:54:10.380]   (speaking in foreign language)
[00:54:10.380 --> 00:54:14.300]   (speaking in foreign language)
[00:54:14.300 --> 00:54:18.220]   (speaking in foreign language)
[00:54:18.220 --> 00:54:22.140]   (speaking in foreign language)
[00:54:22.140 --> 00:54:26.060]   (speaking in foreign language)
[00:54:26.060 --> 00:54:29.980]   (speaking in foreign language)
[00:54:29.980 --> 00:54:33.900]   (speaking in foreign language)
[00:54:33.900 --> 00:54:37.820]   (speaking in foreign language)
[00:54:38.740 --> 00:54:42.660]   (speaking in foreign language)
[00:54:42.660 --> 00:54:46.580]   (speaking in foreign language)
[00:54:47.500 --> 00:54:51.420]   (speaking in foreign language)
[00:54:51.420 --> 00:54:55.340]   (speaking in foreign language)
[00:54:56.260 --> 00:55:00.180]   (speaking in foreign language)
[00:55:00.180 --> 00:55:04.100]   (speaking in foreign language)
[00:55:04.100 --> 00:55:08.020]   (speaking in foreign language)
[00:55:08.940 --> 00:55:12.860]   (speaking in foreign language)
[00:55:12.860 --> 00:55:16.780]   (speaking in foreign language)
[00:55:16.780 --> 00:55:20.700]   (speaking in foreign language)
[00:55:20.700 --> 00:55:24.620]   (speaking in foreign language)
[00:55:24.620 --> 00:55:28.540]   (speaking in foreign language)
[00:55:28.540 --> 00:55:32.460]   (speaking in foreign language)
[00:55:32.460 --> 00:55:36.380]   (speaking in foreign language)
[00:55:36.380 --> 00:55:40.300]   (speaking in foreign language)
[00:55:40.300 --> 00:55:44.220]   (speaking in foreign language)
[00:55:44.220 --> 00:55:48.140]   (speaking in foreign language)
[00:55:48.140 --> 00:55:52.060]   (speaking in foreign language)
[00:56:15.980 --> 00:56:19.900]   (speaking in foreign language)
[00:56:19.900 --> 00:56:23.820]   (speaking in foreign language)
[00:56:23.820 --> 00:56:27.740]   (speaking in foreign language)
[00:56:27.740 --> 00:56:31.660]   (speaking in foreign language)
[00:56:31.660 --> 00:56:35.580]   (speaking in foreign language)
[00:56:35.660 --> 00:56:39.580]   (speaking in foreign language)
[00:56:39.580 --> 00:56:43.500]   (speaking in foreign language)
[00:56:43.500 --> 00:56:47.420]   (speaking in foreign language)
[00:56:47.420 --> 00:56:51.340]   (speaking in foreign language)
[00:56:51.340 --> 00:56:55.260]   (speaking in foreign language)
[00:56:55.260 --> 00:56:59.180]   (speaking in foreign language)
[00:56:59.180 --> 00:57:03.100]   (speaking in foreign language)
[00:57:04.060 --> 00:57:07.980]   (speaking in foreign language)
[00:57:07.980 --> 00:57:11.900]   (speaking in foreign language)
[00:57:11.900 --> 00:57:15.820]   (speaking in foreign language)
[00:57:15.820 --> 00:57:19.740]   (speaking in foreign language)
[00:57:19.740 --> 00:57:23.660]   (speaking in foreign language)
[00:57:23.660 --> 00:57:27.580]   (speaking in foreign language)
[00:57:27.580 --> 00:57:31.500]   (speaking in foreign language)
[00:57:31.500 --> 00:57:35.420]   (speaking in foreign language)
[00:57:35.420 --> 00:57:39.340]   (speaking in foreign language)
[00:57:39.340 --> 00:57:43.260]   (speaking in foreign language)
[00:57:43.260 --> 00:57:47.180]   (speaking in foreign language)
[00:57:47.180 --> 00:57:51.100]   (speaking in foreign language)
[00:57:51.100 --> 00:57:55.020]   (speaking in foreign language)
[00:57:55.020 --> 00:57:58.940]   (speaking in foreign language)
[00:57:58.940 --> 00:58:02.860]   (speaking in foreign language)
[00:58:02.860 --> 00:58:06.780]   (speaking in foreign language)
[00:58:06.780 --> 00:58:10.700]   (speaking in foreign language)
[00:58:10.700 --> 00:58:14.620]   (speaking in foreign language)
[00:58:14.620 --> 00:58:18.540]   (speaking in foreign language)
[00:58:18.540 --> 00:58:22.460]   (speaking in foreign language)
[00:58:23.420 --> 00:58:27.340]   (speaking in foreign language)
[00:58:27.340 --> 00:58:31.260]   (speaking in foreign language)
[00:58:31.260 --> 00:58:35.180]   (speaking in foreign language)
[00:58:35.180 --> 00:58:39.100]   (speaking in foreign language)
[00:58:39.100 --> 00:58:43.020]   (speaking in foreign language)
[00:58:43.020 --> 00:58:46.940]   (speaking in foreign language)
[00:58:46.940 --> 00:58:50.860]   (speaking in foreign language)
[00:58:50.860 --> 00:58:54.780]   (speaking in foreign language)
[00:58:54.780 --> 00:58:58.700]   (speaking in foreign language)
[00:58:58.700 --> 00:59:02.620]   (speaking in foreign language)
[00:59:02.620 --> 00:59:06.540]   (speaking in foreign language)
[00:59:06.540 --> 00:59:10.460]   (speaking in foreign language)
[00:59:10.460 --> 00:59:14.380]   (speaking in foreign language)
[00:59:14.380 --> 00:59:18.300]   (speaking in foreign language)
[00:59:18.300 --> 00:59:22.220]   (speaking in foreign language)
[00:59:22.220 --> 00:59:26.140]   (speaking in foreign language)
[00:59:26.140 --> 00:59:30.060]   (speaking in foreign language)
[00:59:30.060 --> 00:59:33.980]   (speaking in foreign language)
[00:59:33.980 --> 00:59:37.900]   (speaking in foreign language)
[00:59:37.900 --> 00:59:41.820]   (speaking in foreign language)
[00:59:41.820 --> 00:59:45.740]   (speaking in foreign language)
[00:59:45.740 --> 00:59:49.660]   (speaking in foreign language)
[00:59:49.660 --> 00:59:53.580]   (speaking in foreign language)
[00:59:53.580 --> 00:59:56.580]   (keyboard clicking)
[00:59:56.580 --> 01:00:01.500]   (speaking in foreign language)
[01:00:01.500 --> 01:00:05.420]   (speaking in foreign language)
[01:00:05.420 --> 01:00:09.340]   (speaking in foreign language)
[01:00:09.340 --> 01:00:12.340]   (keyboard clicking)
[01:00:12.340 --> 01:00:16.260]   (speaking in foreign language)
[01:00:16.260 --> 01:00:20.180]   (speaking in foreign language)
[01:00:20.180 --> 01:00:29.100]   (speaking in foreign language)
[01:00:29.100 --> 01:00:40.900]   (speaking in foreign language)
[01:00:40.900 --> 01:00:44.820]   (speaking in foreign language)
[01:00:45.740 --> 01:00:49.660]   (speaking in foreign language)
[01:00:49.660 --> 01:00:53.580]   (speaking in foreign language)
[01:00:54.500 --> 01:00:58.420]   (speaking in foreign language)
[01:00:58.420 --> 01:01:02.340]   (speaking in foreign language)
[01:01:02.340 --> 01:01:06.260]   (speaking in foreign language)
[01:01:06.260 --> 01:01:10.180]   (speaking in foreign language)
[01:01:10.180 --> 01:01:14.100]   (speaking in foreign language)
[01:01:14.100 --> 01:01:18.020]   (speaking in foreign language)
[01:01:18.020 --> 01:01:21.940]   (speaking in foreign language)
[01:01:22.860 --> 01:01:26.860]   (speaking in foreign language)
[01:01:27.780 --> 01:01:31.780]   (speaking in foreign language)
[01:01:31.780 --> 01:01:35.700]   (speaking in foreign language)
[01:01:35.700 --> 01:01:39.620]   (speaking in foreign language)
[01:01:40.540 --> 01:01:44.540]   (speaking in foreign language)
[01:01:45.460 --> 01:01:49.460]   (speaking in foreign language)
[01:01:50.380 --> 01:01:54.380]   (speaking in foreign language)
[01:01:55.300 --> 01:01:59.300]   (speaking in foreign language)
[01:01:59.300 --> 01:02:03.220]   (speaking in foreign language)
[01:02:04.140 --> 01:02:08.140]   (speaking in foreign language)
[01:02:08.140 --> 01:02:12.060]   (speaking in foreign language)
[01:02:12.060 --> 01:02:15.980]   (speaking in foreign language)
[01:02:15.980 --> 01:02:19.900]   (speaking in foreign language)
[01:02:19.900 --> 01:02:23.820]   (speaking in foreign language)
[01:02:23.820 --> 01:02:27.740]   (speaking in foreign language)
[01:02:27.740 --> 01:02:31.660]   (speaking in foreign language)
[01:02:31.660 --> 01:02:35.580]   (speaking in foreign language)
[01:02:35.580 --> 01:02:39.500]   (speaking in foreign language)
[01:02:39.500 --> 01:02:43.420]   (speaking in foreign language)
[01:02:43.420 --> 01:02:47.340]   (speaking in foreign language)
[01:02:47.340 --> 01:02:51.260]   (speaking in foreign language)
[01:03:17.260 --> 01:03:21.260]   (speaking in foreign language)
[01:03:21.260 --> 01:03:25.180]   (speaking in foreign language)
[01:03:25.180 --> 01:03:29.100]   (speaking in foreign language)
[01:03:29.100 --> 01:03:33.020]   (speaking in foreign language)
[01:03:33.020 --> 01:03:36.940]   (speaking in foreign language)
[01:03:36.940 --> 01:03:40.860]   (speaking in foreign language)
[01:03:40.860 --> 01:03:44.780]   (speaking in foreign language)
[01:03:44.780 --> 01:03:48.700]   (speaking in foreign language)
[01:03:48.700 --> 01:03:52.620]   (speaking in foreign language)
[01:03:52.620 --> 01:03:56.540]   (speaking in foreign language)
[01:03:56.540 --> 01:04:00.460]   (speaking in foreign language)
[01:04:00.460 --> 01:04:04.380]   (speaking in foreign language)
[01:04:04.380 --> 01:04:09.300]   (speaking in foreign language)
[01:04:09.300 --> 01:04:13.220]   (speaking in foreign language)
[01:04:13.220 --> 01:04:17.140]   (speaking in foreign language)
[01:04:17.140 --> 01:04:21.060]   (speaking in foreign language)
[01:04:21.060 --> 01:04:24.980]   (speaking in foreign language)
[01:04:24.980 --> 01:04:28.900]   (speaking in foreign language)
[01:04:28.900 --> 01:04:32.820]   (speaking in foreign language)
[01:04:32.820 --> 01:04:36.740]   (speaking in foreign language)
[01:04:36.740 --> 01:04:40.660]   (speaking in foreign language)
[01:04:40.660 --> 01:04:44.580]   (speaking in foreign language)
[01:04:45.500 --> 01:04:49.420]   (speaking in foreign language)
[01:04:49.420 --> 01:04:53.340]   (speaking in foreign language)
[01:04:53.340 --> 01:04:57.260]   (speaking in foreign language)
[01:04:57.260 --> 01:05:01.180]   (speaking in foreign language)
[01:05:02.100 --> 01:05:06.020]   (speaking in foreign language)
[01:05:06.020 --> 01:05:09.940]   (speaking in foreign language)
[01:05:09.940 --> 01:05:13.860]   (speaking in foreign language)
[01:05:14.780 --> 01:05:18.700]   (speaking in foreign language)
[01:05:18.700 --> 01:05:22.620]   (speaking in foreign language)
[01:05:22.620 --> 01:05:26.540]   (speaking in foreign language)
[01:05:26.540 --> 01:05:30.460]   (speaking in foreign language)
[01:05:30.460 --> 01:05:34.380]   (speaking in foreign language)
[01:05:34.380 --> 01:05:38.300]   (speaking in foreign language)
[01:05:38.300 --> 01:05:42.220]   (speaking in foreign language)
[01:05:42.220 --> 01:05:46.140]   (speaking in foreign language)
[01:05:47.060 --> 01:05:50.980]   (speaking in foreign language)
[01:05:51.900 --> 01:05:55.820]   (speaking in foreign language)
[01:05:55.820 --> 01:05:59.740]   (speaking in foreign language)
[01:05:59.740 --> 01:06:03.660]   (speaking in foreign language)
[01:06:03.660 --> 01:06:07.580]   (speaking in foreign language)
[01:06:08.500 --> 01:06:12.420]   (speaking in foreign language)
[01:06:12.420 --> 01:06:16.340]   (speaking in foreign language)
[01:06:16.340 --> 01:06:20.260]   (speaking in foreign language)
[01:06:20.260 --> 01:06:24.180]   (speaking in foreign language)
[01:06:24.180 --> 01:06:28.100]   (speaking in foreign language)
[01:06:28.100 --> 01:06:32.020]   (speaking in foreign language)
[01:06:32.020 --> 01:06:35.940]   (speaking in foreign language)
[01:06:36.860 --> 01:06:40.780]   (speaking in foreign language)
[01:06:41.700 --> 01:06:45.620]   (speaking in foreign language)
[01:06:45.620 --> 01:06:49.540]   (speaking in foreign language)
[01:06:49.540 --> 01:06:53.460]   (speaking in foreign language)
[01:06:53.460 --> 01:06:57.380]   (speaking in foreign language)
[01:06:57.380 --> 01:07:01.300]   (speaking in foreign language)
[01:07:02.220 --> 01:07:06.140]   (speaking in foreign language)
[01:07:07.140 --> 01:07:11.060]   (speaking in foreign language)
[01:07:11.060 --> 01:07:14.980]   (speaking in foreign language)
[01:07:14.980 --> 01:07:18.900]   (speaking in foreign language)
[01:07:18.900 --> 01:07:22.820]   (speaking in foreign language)
[01:07:22.820 --> 01:07:26.740]   (speaking in foreign language)
[01:07:26.740 --> 01:07:30.660]   (speaking in foreign language)
[01:07:30.660 --> 01:07:34.580]   (speaking in foreign language)
[01:07:34.580 --> 01:07:38.500]   (speaking in foreign language)
[01:07:39.420 --> 01:07:43.420]   (speaking in foreign language)
[01:07:43.420 --> 01:07:47.340]   (speaking in foreign language)
[01:07:48.260 --> 01:07:52.260]   (speaking in foreign language)
[01:07:52.260 --> 01:07:56.180]   (speaking in foreign language)
[01:07:56.180 --> 01:08:00.100]   (speaking in foreign language)
[01:08:01.020 --> 01:08:05.020]   (speaking in foreign language)
[01:08:05.020 --> 01:08:08.940]   (speaking in foreign language)
[01:08:08.940 --> 01:08:12.860]   (speaking in foreign language)
[01:08:12.860 --> 01:08:16.780]   (speaking in foreign language)
[01:08:16.780 --> 01:08:20.700]   (speaking in foreign language)
[01:08:20.700 --> 01:08:24.620]   (speaking in foreign language)
[01:08:25.540 --> 01:08:29.540]   (speaking in foreign language)
[01:08:29.540 --> 01:08:33.460]   (speaking in foreign language)
[01:08:33.460 --> 01:08:37.380]   (speaking in foreign language)
[01:08:37.380 --> 01:08:41.300]   (speaking in foreign language)
[01:08:41.300 --> 01:08:45.220]   (speaking in foreign language)
[01:08:45.220 --> 01:08:49.140]   (speaking in foreign language)
[01:08:50.060 --> 01:08:54.060]   (speaking in foreign language)
[01:08:54.980 --> 01:08:58.980]   (speaking in foreign language)
[01:09:23.980 --> 01:09:27.900]   (speaking in foreign language)
[01:09:27.900 --> 01:09:31.820]   (speaking in foreign language)
[01:09:31.820 --> 01:09:35.740]   (speaking in foreign language)
[01:09:35.740 --> 01:09:39.660]   (speaking in foreign language)
[01:09:39.660 --> 01:09:53.660]   (speaking in foreign language)
[01:09:54.660 --> 01:09:58.580]   (speaking in foreign language)
[01:09:58.580 --> 01:10:02.500]   (speaking in foreign language)
[01:10:03.500 --> 01:10:07.420]   (speaking in foreign language)
[01:10:07.420 --> 01:10:11.340]   (speaking in foreign language)
[01:10:11.340 --> 01:10:15.260]   (speaking in foreign language)
[01:10:16.260 --> 01:10:20.180]   (speaking in foreign language)
[01:10:20.180 --> 01:10:24.100]   (speaking in foreign language)
[01:10:24.100 --> 01:10:28.020]   (speaking in foreign language)
[01:10:28.020 --> 01:10:31.940]   (speaking in foreign language)
[01:10:32.940 --> 01:10:36.860]   (speaking in foreign language)
[01:10:36.860 --> 01:10:40.780]   (speaking in foreign language)
[01:10:40.780 --> 01:10:44.700]   (speaking in foreign language)
[01:10:44.700 --> 01:10:48.620]   (speaking in foreign language)
[01:10:48.620 --> 01:10:52.540]   (speaking in foreign language)
[01:10:52.540 --> 01:10:56.460]   (speaking in foreign language)
[01:10:56.460 --> 01:11:00.380]   (speaking in foreign language)
[01:11:00.380 --> 01:11:04.300]   (speaking in foreign language)
[01:11:04.300 --> 01:11:08.220]   (speaking in foreign language)
[01:11:08.220 --> 01:11:12.140]   (speaking in foreign language)
[01:11:12.140 --> 01:11:16.060]   (speaking in foreign language)
[01:11:16.060 --> 01:11:19.980]   (speaking in foreign language)
[01:11:19.980 --> 01:11:23.900]   (speaking in foreign language)
[01:11:24.820 --> 01:11:28.820]   (speaking in foreign language)
[01:11:28.820 --> 01:11:32.740]   (speaking in foreign language)
[01:11:32.740 --> 01:11:36.660]   (speaking in foreign language)
[01:11:37.580 --> 01:11:41.580]   (speaking in foreign language)
[01:11:41.580 --> 01:11:45.500]   (speaking in foreign language)
[01:11:45.500 --> 01:11:49.420]   (speaking in foreign language)
[01:11:49.420 --> 01:11:53.340]   (speaking in foreign language)
[01:11:53.340 --> 01:11:57.260]   (speaking in foreign language)
[01:11:57.260 --> 01:12:01.180]   (speaking in foreign language)
[01:12:01.180 --> 01:12:05.100]   (speaking in foreign language)
[01:12:05.100 --> 01:12:09.020]   (speaking in foreign language)
[01:12:09.020 --> 01:12:12.940]   (speaking in foreign language)
[01:12:12.940 --> 01:12:16.860]   (speaking in foreign language)
[01:12:16.860 --> 01:12:20.780]   (speaking in foreign language)
[01:12:20.780 --> 01:12:24.700]   (speaking in foreign language)
[01:12:24.700 --> 01:12:28.620]   (speaking in foreign language)
[01:12:28.620 --> 01:12:32.540]   (speaking in foreign language)
[01:12:32.540 --> 01:12:36.460]   (speaking in foreign language)
[01:12:36.460 --> 01:12:40.380]   (speaking in foreign language)
[01:12:40.380 --> 01:12:44.300]   (speaking in foreign language)
[01:12:45.220 --> 01:12:49.220]   (speaking in foreign language)
[01:12:49.220 --> 01:12:53.140]   (speaking in foreign language)
[01:12:53.140 --> 01:12:57.060]   (speaking in foreign language)
[01:12:57.060 --> 01:13:00.980]   (speaking in foreign language)
[01:13:00.980 --> 01:13:04.900]   (speaking in foreign language)
[01:13:04.900 --> 01:13:08.820]   (speaking in foreign language)
[01:13:08.820 --> 01:13:12.740]   (speaking in foreign language)
[01:13:13.660 --> 01:13:17.660]   (speaking in foreign language)
[01:13:17.660 --> 01:13:21.580]   (speaking in foreign language)
[01:13:22.500 --> 01:13:26.500]   (speaking in foreign language)
[01:13:26.500 --> 01:13:30.420]   (speaking in foreign language)
[01:13:30.420 --> 01:13:34.340]   (speaking in foreign language)
[01:13:34.340 --> 01:13:38.260]   (speaking in foreign language)
[01:13:38.260 --> 01:13:42.180]   (speaking in foreign language)
[01:13:43.100 --> 01:13:47.100]   (speaking in foreign language)
[01:13:47.100 --> 01:13:51.020]   (speaking in foreign language)
[01:13:51.940 --> 01:13:55.940]   (speaking in foreign language)
[01:13:56.860 --> 01:14:00.860]   (speaking in foreign language)
[01:14:00.860 --> 01:14:04.780]   (speaking in foreign language)
[01:14:04.780 --> 01:14:08.700]   (speaking in foreign language)
[01:14:08.700 --> 01:14:12.620]   (speaking in foreign language)
[01:14:13.540 --> 01:14:17.540]   (speaking in foreign language)
[01:14:18.460 --> 01:14:22.460]   (speaking in foreign language)
[01:14:22.460 --> 01:14:26.380]   (speaking in foreign language)
[01:14:26.380 --> 01:14:30.300]   (speaking in foreign language)
[01:14:31.220 --> 01:14:35.220]   (speaking in foreign language)
[01:14:35.220 --> 01:14:39.140]   (speaking in foreign language)
[01:14:40.060 --> 01:14:44.060]   (speaking in foreign language)
[01:14:44.060 --> 01:14:47.980]   (speaking in foreign language)
[01:14:47.980 --> 01:14:51.900]   (speaking in foreign language)
[01:14:51.900 --> 01:14:55.820]   (speaking in foreign language)
[01:14:55.820 --> 01:14:59.740]   (speaking in foreign language)
[01:14:59.740 --> 01:15:03.660]   (speaking in foreign language)
[01:15:04.580 --> 01:15:08.580]   (speaking in foreign language)
[01:15:08.580 --> 01:15:12.500]   (speaking in foreign language)
[01:15:12.500 --> 01:15:16.420]   (speaking in foreign language)
[01:15:17.340 --> 01:15:21.340]   (speaking in foreign language)
[01:15:21.340 --> 01:15:25.260]   (speaking in foreign language)
[01:15:25.260 --> 01:15:29.180]   (speaking in foreign language)
[01:15:29.180 --> 01:15:33.100]   (speaking in foreign language)
[01:15:34.020 --> 01:15:38.020]   (speaking in foreign language)
[01:15:38.020 --> 01:15:41.940]   (speaking in foreign language)
[01:15:42.860 --> 01:15:46.860]   (speaking in foreign language)
[01:15:46.860 --> 01:15:50.780]   (speaking in foreign language)
[01:15:50.780 --> 01:15:54.700]   (speaking in foreign language)
[01:15:54.700 --> 01:15:58.620]   (speaking in foreign language)
[01:15:58.620 --> 01:16:02.540]   (speaking in foreign language)
[01:16:02.540 --> 01:16:06.460]   (speaking in foreign language)
[01:16:06.460 --> 01:16:10.380]   (speaking in foreign language)
[01:16:10.380 --> 01:16:14.300]   (speaking in foreign language)
[01:16:15.300 --> 01:16:19.220]   (speaking in foreign language)
[01:16:19.220 --> 01:16:23.140]   (speaking in foreign language)
[01:16:24.140 --> 01:16:28.140]   (speaking in foreign language)
[01:16:28.140 --> 01:16:32.060]   (speaking in foreign language)
[01:16:32.060 --> 01:16:35.980]   (speaking in foreign language)
[01:16:36.900 --> 01:16:40.900]   (speaking in foreign language)
[01:16:40.900 --> 01:16:44.820]   (speaking in foreign language)
[01:16:44.820 --> 01:17:09.460]   - Three minute break.
[01:17:10.460 --> 01:17:13.460]   Yes, let's take a three minute break.
[01:17:13.460 --> 01:17:14.300]   Yeah.
[01:17:14.300 --> 01:17:18.220]   (speaking in foreign language)
[01:17:18.220 --> 01:17:22.140]   (speaking in foreign language)
[01:17:22.140 --> 01:17:26.060]   (speaking in foreign language)
[01:17:26.060 --> 01:17:29.980]   (speaking in foreign language)
[01:17:29.980 --> 01:17:33.900]   (speaking in foreign language)
[01:17:33.900 --> 01:17:37.820]   (speaking in foreign language)
[01:17:37.820 --> 01:17:41.740]   (speaking in foreign language)
[01:17:41.740 --> 01:17:45.660]   (speaking in foreign language)
[01:17:45.660 --> 01:17:49.580]   (speaking in foreign language)
[01:17:49.580 --> 01:17:53.500]   (speaking in foreign language)
[01:17:53.500 --> 01:17:57.420]   (speaking in foreign language)
[01:17:57.420 --> 01:18:21.000]   (upbeat music)
[01:18:21.000 --> 01:18:23.580]   (upbeat music)
[01:18:23.580 --> 01:18:26.160]   (upbeat music)
[01:18:26.160 --> 01:18:28.740]   (upbeat music)
[01:18:28.740 --> 01:18:31.320]   (upbeat music)
[01:18:31.320 --> 01:18:33.900]   (upbeat music)
[01:18:33.900 --> 01:18:36.480]   (upbeat music)
[01:18:36.480 --> 01:18:39.060]   (upbeat music)
[01:18:39.060 --> 01:18:41.640]   (upbeat music)
[01:18:41.640 --> 01:18:44.220]   (upbeat music)
[01:18:44.220 --> 01:18:46.800]   (upbeat music)
[01:18:46.800 --> 01:18:49.380]   (upbeat music)
[01:18:49.380 --> 01:18:51.960]   (upbeat music)
[01:18:51.960 --> 01:18:54.540]   (upbeat music)
[01:18:54.540 --> 01:18:57.120]   (upbeat music)
[01:18:57.120 --> 01:18:59.700]   (upbeat music)
[01:18:59.700 --> 01:19:02.280]   (upbeat music)
[01:19:02.280 --> 01:19:04.860]   (upbeat music)
[01:19:04.860 --> 01:19:07.440]   (upbeat music)
[01:19:07.440 --> 01:19:10.020]   (upbeat music)
[01:19:10.020 --> 01:19:12.600]   (upbeat music)
[01:19:12.600 --> 01:19:17.180]   (upbeat music)
[01:19:17.180 --> 01:19:22.760]   (upbeat music)
[01:19:22.760 --> 01:19:27.260]   (upbeat music)
[01:19:27.260 --> 01:19:35.680]   (soft music)
[01:19:35.680 --> 01:19:38.100]   (soft music)
[01:19:38.100 --> 01:19:40.520]   (soft music)
[01:19:40.520 --> 01:19:42.940]   (soft music)
[01:19:42.940 --> 01:20:10.140]   (upbeat music)
[01:20:11.140 --> 01:20:13.720]   (upbeat music)
[01:20:13.720 --> 01:20:18.300]   (upbeat music)
[01:20:18.300 --> 01:20:23.840]   (upbeat music)
[01:20:23.840 --> 01:20:30.760]   (upbeat music)
[01:20:30.760 --> 01:20:37.340]   (upbeat music)
[01:20:38.340 --> 01:20:40.920]   (upbeat music)
[01:20:40.920 --> 01:20:48.500]   (upbeat music)
[01:20:48.500 --> 01:20:58.000]   (upbeat music)
[01:20:58.000 --> 01:21:00.580]   (upbeat music)
[01:21:00.580 --> 01:21:13.420]   (soft music)
[01:21:13.420 --> 01:21:15.840]   (soft music)
[01:21:15.840 --> 01:21:33.840]   (soft music)
[01:21:33.840 --> 01:21:36.260]   (soft music)
[01:21:36.260 --> 01:21:54.420]   (upbeat music)
[01:21:54.420 --> 01:21:57.000]   (upbeat music)
[01:21:57.000 --> 01:22:13.840]   (soft music)
[01:22:13.840 --> 01:22:21.920]   (speaking in foreign language)
[01:22:21.920 --> 01:22:25.840]   (speaking in foreign language)
[01:22:25.840 --> 01:22:29.760]   (speaking in foreign language)
[01:22:29.760 --> 01:22:33.680]   (speaking in foreign language)
[01:22:34.180 --> 01:22:38.100]   (speaking in foreign language)
[01:22:38.100 --> 01:22:42.020]   (speaking in foreign language)
[01:22:42.020 --> 01:22:45.940]   (speaking in foreign language)
[01:22:45.940 --> 01:22:49.860]   (speaking in foreign language)
[01:22:49.860 --> 01:22:53.780]   (speaking in foreign language)
[01:22:53.780 --> 01:22:57.700]   (speaking in foreign language)
[01:22:57.700 --> 01:23:01.620]   (speaking in foreign language)
[01:23:01.620 --> 01:23:05.540]   (speaking in foreign language)
[01:23:05.540 --> 01:23:09.460]   (speaking in foreign language)
[01:23:09.460 --> 01:23:13.380]   (speaking in foreign language)
[01:23:14.300 --> 01:23:18.220]   (speaking in foreign language)
[01:23:19.140 --> 01:23:23.060]   (speaking in foreign language)
[01:23:23.060 --> 01:23:26.980]   (speaking in foreign language)
[01:23:27.900 --> 01:23:31.820]   (speaking in foreign language)
[01:23:32.740 --> 01:23:36.660]   (speaking in foreign language)
[01:23:36.660 --> 01:23:40.580]   (speaking in foreign language)
[01:23:40.580 --> 01:23:44.500]   (speaking in foreign language)
[01:23:45.420 --> 01:23:49.340]   (speaking in foreign language)
[01:23:49.340 --> 01:23:53.260]   (speaking in foreign language)
[01:23:53.260 --> 01:23:57.180]   (speaking in foreign language)
[01:23:57.180 --> 01:24:01.100]   (speaking in foreign language)
[01:24:01.100 --> 01:24:05.020]   (speaking in foreign language)
[01:24:05.020 --> 01:24:08.940]   (speaking in foreign language)
[01:24:08.940 --> 01:24:12.860]   (speaking in foreign language)
[01:24:12.860 --> 01:24:16.780]   (speaking in foreign language)
[01:24:16.780 --> 01:24:20.700]   (speaking in foreign language)
[01:24:20.700 --> 01:24:24.620]   (speaking in foreign language)
[01:24:24.620 --> 01:24:28.540]   (speaking in foreign language)
[01:24:28.540 --> 01:24:32.460]   (speaking in foreign language)
[01:24:32.460 --> 01:24:36.380]   (speaking in foreign language)
[01:24:36.380 --> 01:24:40.300]   (speaking in foreign language)
[01:24:40.300 --> 01:24:44.220]   (speaking in foreign language)
[01:24:44.220 --> 01:24:48.140]   (speaking in foreign language)
[01:24:48.140 --> 01:24:52.060]   (speaking in foreign language)
[01:24:52.060 --> 01:24:55.980]   (speaking in foreign language)
[01:24:56.900 --> 01:25:00.820]   (speaking in foreign language)
[01:25:00.820 --> 01:25:04.740]   (speaking in foreign language)
[01:25:04.740 --> 01:25:08.660]   (speaking in foreign language)
[01:25:08.660 --> 01:25:12.580]   (speaking in foreign language)
[01:25:12.580 --> 01:25:16.500]   (speaking in foreign language)
[01:25:16.500 --> 01:25:20.420]   (speaking in foreign language)
[01:25:20.420 --> 01:25:24.340]   (speaking in foreign language)
[01:25:24.340 --> 01:25:28.260]   (speaking in foreign language)
[01:25:28.260 --> 01:25:32.180]   (speaking in foreign language)
[01:25:32.180 --> 01:25:36.100]   (speaking in foreign language)
[01:25:36.100 --> 01:25:40.020]   (speaking in foreign language)
[01:25:40.020 --> 01:25:43.940]   (speaking in foreign language)
[01:25:43.940 --> 01:25:47.860]   (speaking in foreign language)
[01:25:47.860 --> 01:25:51.780]   (speaking in foreign language)
[01:25:52.780 --> 01:25:56.700]   (speaking in foreign language)
[01:25:56.700 --> 01:26:00.620]   (speaking in foreign language)
[01:26:00.620 --> 01:26:04.540]   (speaking in foreign language)
[01:26:04.540 --> 01:26:08.460]   (speaking in foreign language)
[01:26:08.460 --> 01:26:12.380]   (speaking in foreign language)
[01:26:12.380 --> 01:26:16.300]   (speaking in foreign language)
[01:26:16.300 --> 01:26:20.220]   (speaking in foreign language)
[01:26:20.220 --> 01:26:24.140]   (speaking in foreign language)
[01:26:24.140 --> 01:26:28.060]   (speaking in foreign language)
[01:26:28.060 --> 01:26:31.980]   (speaking in foreign language)
[01:26:31.980 --> 01:26:35.900]   (speaking in foreign language)
[01:26:35.900 --> 01:26:39.820]   (speaking in foreign language)
[01:26:39.820 --> 01:26:43.740]   (speaking in foreign language)
[01:26:43.740 --> 01:26:47.660]   (speaking in foreign language)
[01:26:47.660 --> 01:26:51.580]   (speaking in foreign language)
[01:26:51.580 --> 01:26:55.500]   (speaking in foreign language)
[01:26:55.500 --> 01:26:59.420]   (speaking in foreign language)
[01:26:59.420 --> 01:27:03.340]   (speaking in foreign language)
[01:27:03.340 --> 01:27:07.260]   (speaking in foreign language)
[01:27:08.260 --> 01:27:12.180]   (speaking in foreign language)
[01:27:12.180 --> 01:27:16.100]   (speaking in foreign language)
[01:27:16.100 --> 01:27:20.020]   (speaking in foreign language)
[01:27:20.020 --> 01:27:23.940]   (speaking in foreign language)
[01:27:23.940 --> 01:27:27.860]   (speaking in foreign language)
[01:27:27.860 --> 01:27:31.780]   (speaking in foreign language)
[01:27:31.780 --> 01:27:35.700]   (speaking in foreign language)
[01:27:35.700 --> 01:27:39.620]   (speaking in foreign language)
[01:27:39.620 --> 01:27:43.540]   (speaking in foreign language)
[01:27:43.540 --> 01:27:47.460]   (speaking in foreign language)
[01:27:47.460 --> 01:27:51.380]   (speaking in foreign language)
[01:27:51.380 --> 01:27:55.300]   (speaking in foreign language)
[01:27:55.300 --> 01:27:59.220]   (speaking in foreign language)
[01:27:59.220 --> 01:28:03.140]   (speaking in foreign language)
[01:28:03.140 --> 01:28:07.060]   (speaking in foreign language)
[01:28:07.060 --> 01:28:10.980]   (speaking in foreign language)
[01:28:10.980 --> 01:28:14.900]   (speaking in foreign language)
[01:28:14.900 --> 01:28:18.820]   (speaking in foreign language)
[01:28:18.820 --> 01:28:22.740]   (speaking in foreign language)
[01:28:22.740 --> 01:28:26.660]   (speaking in foreign language)
[01:28:26.660 --> 01:28:30.580]   (speaking in foreign language)
[01:28:31.500 --> 01:28:35.420]   (speaking in foreign language)
[01:28:35.420 --> 01:28:39.340]   (speaking in foreign language)
[01:28:39.340 --> 01:28:43.260]   (speaking in foreign language)
[01:28:43.260 --> 01:28:47.180]   (speaking in foreign language)
[01:28:47.180 --> 01:28:51.100]   (speaking in foreign language)
[01:28:51.100 --> 01:28:55.020]   (speaking in foreign language)
[01:28:55.020 --> 01:28:58.940]   (speaking in foreign language)
[01:28:58.940 --> 01:29:02.860]   (speaking in foreign language)
[01:29:02.860 --> 01:29:06.780]   (speaking in foreign language)
[01:29:06.780 --> 01:29:10.700]   (speaking in foreign language)
[01:29:10.700 --> 01:29:14.620]   (speaking in foreign language)
[01:29:14.620 --> 01:29:18.540]   (speaking in foreign language)
[01:29:18.540 --> 01:29:22.460]   (speaking in foreign language)
[01:29:22.460 --> 01:29:26.380]   (speaking in foreign language)
[01:29:26.380 --> 01:29:30.300]   (speaking in foreign language)
[01:29:30.300 --> 01:29:34.220]   (speaking in foreign language)
[01:29:34.220 --> 01:29:38.140]   (speaking in foreign language)
[01:29:38.140 --> 01:29:42.060]   (speaking in foreign language)
[01:29:42.060 --> 01:29:45.980]   (speaking in foreign language)
[01:29:46.700 --> 01:29:50.620]   (speaking in foreign language)
[01:29:50.620 --> 01:29:54.540]   (speaking in foreign language)
[01:29:54.540 --> 01:29:58.460]   (speaking in foreign language)
[01:29:59.420 --> 01:30:03.340]   (speaking in foreign language)
[01:30:03.340 --> 01:30:07.260]   (speaking in foreign language)
[01:30:07.260 --> 01:30:11.180]   (speaking in foreign language)
[01:30:11.180 --> 01:30:15.100]   (speaking in foreign language)
[01:30:16.100 --> 01:30:20.020]   (speaking in foreign language)
[01:30:20.020 --> 01:30:23.940]   (speaking in foreign language)
[01:30:23.940 --> 01:30:27.860]   (speaking in foreign language)
[01:30:27.860 --> 01:30:31.780]   (speaking in foreign language)
[01:30:31.780 --> 01:30:35.700]   (speaking in foreign language)
[01:30:35.700 --> 01:30:39.620]   (speaking in foreign language)
[01:30:39.620 --> 01:30:43.540]   (speaking in foreign language)
[01:30:43.540 --> 01:30:47.460]   (speaking in foreign language)
[01:30:47.460 --> 01:30:51.380]   (speaking in foreign language)
[01:30:51.380 --> 01:30:55.300]   (speaking in foreign language)
[01:30:55.300 --> 01:30:59.220]   (speaking in foreign language)
[01:30:59.220 --> 01:31:03.140]   (speaking in foreign language)
[01:31:03.140 --> 01:31:07.060]   (speaking in foreign language)
[01:31:07.060 --> 01:31:10.980]   (speaking in foreign language)
[01:31:10.980 --> 01:31:14.900]   (speaking in foreign language)
[01:31:14.900 --> 01:31:18.820]   (speaking in foreign language)
[01:31:18.820 --> 01:31:22.740]   (speaking in foreign language)
[01:31:22.740 --> 01:31:26.660]   (speaking in foreign language)
[01:31:26.660 --> 01:31:30.580]   (speaking in foreign language)
[01:31:30.580 --> 01:31:34.500]   (speaking in foreign language)
[01:31:34.500 --> 01:31:38.420]   (speaking in foreign language)
[01:32:03.420 --> 01:32:07.340]   (speaking in foreign language)
[01:32:07.340 --> 01:32:11.260]   (speaking in foreign language)
[01:32:11.260 --> 01:32:15.180]   (speaking in foreign language)
[01:32:15.180 --> 01:32:19.100]   (speaking in foreign language)
[01:32:19.100 --> 01:32:23.020]   (speaking in foreign language)
[01:32:23.020 --> 01:32:26.940]   (speaking in foreign language)
[01:32:26.940 --> 01:32:30.860]   (speaking in foreign language)
[01:32:30.860 --> 01:32:34.780]   (speaking in foreign language)
[01:32:35.780 --> 01:32:39.700]   (speaking in foreign language)
[01:32:39.700 --> 01:32:43.620]   (speaking in foreign language)
[01:32:43.620 --> 01:32:47.540]   (speaking in foreign language)
[01:32:47.540 --> 01:32:51.460]   (speaking in foreign language)
[01:32:51.460 --> 01:32:55.380]   (speaking in foreign language)
[01:32:55.380 --> 01:32:59.300]   (speaking in foreign language)
[01:33:00.300 --> 01:33:04.220]   (speaking in foreign language)
[01:33:04.220 --> 01:33:08.140]   (speaking in foreign language)
[01:33:09.060 --> 01:33:13.060]   (speaking in foreign language)
[01:33:13.060 --> 01:33:16.980]   (speaking in foreign language)
[01:33:16.980 --> 01:33:20.900]   (speaking in foreign language)
[01:33:20.900 --> 01:33:24.820]   (speaking in foreign language)
[01:33:24.820 --> 01:33:28.740]   (speaking in foreign language)
[01:33:28.740 --> 01:33:32.660]   (speaking in foreign language)
[01:33:32.660 --> 01:33:36.580]   (speaking in foreign language)
[01:33:36.580 --> 01:33:40.500]   (speaking in foreign language)
[01:33:40.500 --> 01:33:44.420]   (speaking in foreign language)
[01:33:44.420 --> 01:33:48.340]   (speaking in foreign language)
[01:33:48.340 --> 01:33:52.260]   (speaking in foreign language)
[01:33:52.260 --> 01:33:56.180]   (speaking in foreign language)
[01:33:56.180 --> 01:34:00.100]   (speaking in foreign language)
[01:34:00.100 --> 01:34:04.020]   (speaking in foreign language)
[01:34:04.020 --> 01:34:07.940]   (speaking in foreign language)
[01:34:07.940 --> 01:34:11.860]   (speaking in foreign language)
[01:34:11.860 --> 01:34:15.780]   (speaking in foreign language)
[01:34:15.780 --> 01:34:19.700]   (speaking in foreign language)
[01:34:19.700 --> 01:34:23.620]   (speaking in foreign language)
[01:34:23.620 --> 01:34:27.540]   (speaking in foreign language)
[01:34:27.540 --> 01:34:31.460]   (speaking in foreign language)
[01:34:31.460 --> 01:34:35.380]   (speaking in foreign language)
[01:34:35.380 --> 01:34:39.300]   (speaking in foreign language)
[01:34:39.300 --> 01:34:43.220]   (speaking in foreign language)
[01:34:43.220 --> 01:34:47.140]   (speaking in foreign language)
[01:34:47.140 --> 01:34:51.060]   (speaking in foreign language)
[01:34:51.060 --> 01:34:54.980]   (speaking in foreign language)
[01:34:55.820 --> 01:34:59.740]   (speaking in foreign language)
[01:35:00.580 --> 01:35:04.500]   (speaking in foreign language)
[01:35:04.500 --> 01:35:08.420]   (speaking in foreign language)
[01:35:08.420 --> 01:35:12.340]   (speaking in foreign language)
[01:35:12.340 --> 01:35:16.260]   (speaking in foreign language)
[01:35:17.260 --> 01:35:21.180]   (speaking in foreign language)
[01:35:21.180 --> 01:35:25.100]   (speaking in foreign language)
[01:35:25.100 --> 01:35:29.020]   (speaking in foreign language)
[01:35:29.020 --> 01:35:32.940]   (speaking in foreign language)
[01:35:32.940 --> 01:35:36.860]   (speaking in foreign language)
[01:35:36.860 --> 01:35:40.780]   (speaking in foreign language)
[01:35:40.780 --> 01:35:44.700]   (speaking in foreign language)
[01:35:45.620 --> 01:35:49.540]   (speaking in foreign language)
[01:35:49.540 --> 01:35:53.460]   (speaking in foreign language)
[01:35:53.460 --> 01:35:57.380]   (speaking in foreign language)
[01:35:57.380 --> 01:36:01.300]   (speaking in foreign language)
[01:36:01.300 --> 01:36:05.220]   (speaking in foreign language)
[01:36:06.220 --> 01:36:10.140]   (speaking in foreign language)
[01:36:10.140 --> 01:36:14.060]   (speaking in foreign language)
[01:36:14.060 --> 01:36:17.980]   (speaking in foreign language)
[01:36:17.980 --> 01:36:21.900]   (speaking in foreign language)
[01:36:21.900 --> 01:36:25.820]   (speaking in foreign language)
[01:36:25.820 --> 01:36:29.740]   (speaking in foreign language)
[01:36:29.740 --> 01:36:33.660]   (speaking in foreign language)
[01:36:33.660 --> 01:36:37.580]   (speaking in foreign language)
[01:36:37.580 --> 01:36:41.500]   (speaking in foreign language)
[01:36:42.420 --> 01:36:46.340]   (speaking in foreign language)
[01:36:46.340 --> 01:36:50.260]   (speaking in foreign language)
[01:36:51.180 --> 01:36:55.100]   (speaking in foreign language)
[01:36:55.940 --> 01:36:59.860]   (speaking in foreign language)
[01:36:59.860 --> 01:37:03.780]   (speaking in foreign language)
[01:37:03.780 --> 01:37:07.700]   (speaking in foreign language)
[01:37:07.700 --> 01:37:11.620]   (speaking in foreign language)
[01:37:12.540 --> 01:37:16.460]   (speaking in foreign language)
[01:37:16.460 --> 01:37:20.380]   (speaking in foreign language)
[01:37:20.380 --> 01:37:24.300]   (speaking in foreign language)
[01:37:24.300 --> 01:37:28.220]   (speaking in foreign language)
[01:37:28.220 --> 01:37:32.140]   (speaking in foreign language)
[01:37:32.140 --> 01:37:36.060]   (speaking in foreign language)
[01:37:36.060 --> 01:37:39.980]   (speaking in foreign language)
[01:37:39.980 --> 01:37:43.900]   (speaking in foreign language)
[01:37:44.820 --> 01:37:48.740]   (speaking in foreign language)
[01:37:48.740 --> 01:37:52.660]   (speaking in foreign language)
[01:37:52.660 --> 01:37:56.580]   (speaking in foreign language)
[01:37:56.580 --> 01:38:00.500]   (speaking in foreign language)
[01:38:00.500 --> 01:38:04.420]   (speaking in foreign language)
[01:38:04.420 --> 01:38:08.340]   (speaking in foreign language)
[01:38:08.340 --> 01:38:12.260]   (speaking in foreign language)
[01:38:12.260 --> 01:38:16.180]   (speaking in foreign language)
[01:38:16.180 --> 01:38:20.100]   (speaking in foreign language)
[01:38:20.940 --> 01:38:24.860]   (speaking in foreign language)
[01:38:25.460 --> 01:38:29.380]   (speaking in foreign language)
[01:38:29.380 --> 01:38:33.300]   (speaking in foreign language)
[01:38:33.300 --> 01:38:37.220]   (speaking in foreign language)
[01:38:38.060 --> 01:38:41.980]   (speaking in foreign language)
[01:38:41.980 --> 01:38:45.900]   (speaking in foreign language)
[01:38:45.900 --> 01:38:49.820]   (speaking in foreign language)
[01:38:49.820 --> 01:38:53.740]   (speaking in foreign language)
[01:38:54.260 --> 01:38:58.180]   (speaking in foreign language)
[01:38:58.180 --> 01:39:02.100]   (speaking in foreign language)
[01:39:02.100 --> 01:39:06.020]   (speaking in foreign language)
[01:39:06.700 --> 01:39:10.620]   (speaking in foreign language)
[01:39:10.620 --> 01:39:14.540]   (speaking in foreign language)
[01:39:14.540 --> 01:39:18.460]   (speaking in foreign language)
[01:39:18.460 --> 01:39:22.380]   (speaking in foreign language)
[01:39:22.380 --> 01:39:26.300]   (speaking in foreign language)
[01:39:26.300 --> 01:39:30.220]   (speaking in foreign language)
[01:39:31.140 --> 01:39:35.060]   (speaking in foreign language)
[01:39:35.980 --> 01:39:39.900]   (speaking in foreign language)
[01:39:40.740 --> 01:39:44.660]   (speaking in foreign language)
[01:39:44.660 --> 01:39:48.580]   (speaking in foreign language)
[01:39:48.580 --> 01:39:52.500]   (speaking in foreign language)
[01:39:52.500 --> 01:39:56.420]   (speaking in foreign language)
[01:39:56.420 --> 01:40:00.340]   (speaking in foreign language)
[01:40:00.340 --> 01:40:04.260]   (speaking in foreign language)
[01:40:05.100 --> 01:40:09.020]   (speaking in foreign language)
[01:40:09.020 --> 01:40:12.940]   (speaking in foreign language)
[01:40:13.540 --> 01:40:17.460]   (speaking in foreign language)
[01:40:17.460 --> 01:40:21.380]   (speaking in foreign language)
[01:40:21.380 --> 01:40:25.300]   (speaking in foreign language)
[01:40:25.300 --> 01:40:29.220]   (speaking in foreign language)
[01:40:29.220 --> 01:40:33.140]   (speaking in foreign language)
[01:40:33.140 --> 01:40:37.060]   (speaking in foreign language)
[01:40:37.060 --> 01:40:40.980]   (speaking in foreign language)
[01:40:40.980 --> 01:40:44.900]   (speaking in foreign language)
[01:40:44.900 --> 01:40:48.820]   (speaking in foreign language)
[01:40:49.740 --> 01:40:53.660]   (speaking in foreign language)
[01:40:53.660 --> 01:40:57.580]   (speaking in foreign language)
[01:40:57.580 --> 01:41:01.500]   (speaking in foreign language)
[01:41:01.500 --> 01:41:05.420]   (speaking in foreign language)
[01:41:05.420 --> 01:41:09.340]   (speaking in foreign language)
[01:41:09.340 --> 01:41:13.260]   (speaking in foreign language)
[01:41:13.260 --> 01:41:17.180]   (speaking in foreign language)
[01:41:17.180 --> 01:41:21.100]   (speaking in foreign language)
[01:41:21.100 --> 01:41:25.020]   (speaking in foreign language)
[01:41:25.020 --> 01:41:28.940]   (speaking in foreign language)
[01:41:28.940 --> 01:41:32.860]   (speaking in foreign language)
[01:41:33.700 --> 01:41:37.620]   (speaking in foreign language)
[01:41:38.620 --> 01:41:42.540]   (speaking in foreign language)
[01:41:42.540 --> 01:41:46.460]   (speaking in foreign language)
[01:41:47.460 --> 01:41:51.380]   (speaking in foreign language)
[01:41:51.380 --> 01:41:55.300]   (speaking in foreign language)
[01:41:56.300 --> 01:42:00.220]   (speaking in foreign language)
[01:42:00.220 --> 01:42:04.140]   (speaking in foreign language)
[01:42:04.140 --> 01:42:08.060]   (speaking in foreign language)
[01:42:08.060 --> 01:42:11.980]   (speaking in foreign language)
[01:42:11.980 --> 01:42:15.900]   (speaking in foreign language)
[01:42:15.900 --> 01:42:19.820]   (speaking in foreign language)
[01:42:19.820 --> 01:42:23.740]   (speaking in foreign language)
[01:42:24.740 --> 01:42:28.660]   (speaking in foreign language)
[01:42:28.660 --> 01:42:32.580]   (speaking in foreign language)
[01:42:32.580 --> 01:42:36.500]   (speaking in foreign language)
[01:42:36.500 --> 01:42:40.420]   (speaking in foreign language)
[01:42:40.420 --> 01:42:44.340]   (speaking in foreign language)
[01:42:44.340 --> 01:42:48.260]   (speaking in foreign language)
[01:42:48.260 --> 01:42:52.180]   (speaking in foreign language)
[01:42:52.180 --> 01:42:56.100]   (speaking in foreign language)
[01:42:56.100 --> 01:43:00.020]   (speaking in foreign language)
[01:43:00.020 --> 01:43:03.940]   (speaking in foreign language)
[01:43:03.940 --> 01:43:07.860]   (speaking in foreign language)
[01:43:07.860 --> 01:43:11.780]   (speaking in foreign language)
[01:43:11.780 --> 01:43:15.700]   (speaking in foreign language)
[01:43:15.700 --> 01:43:19.620]   (speaking in foreign language)
[01:43:19.620 --> 01:43:23.540]   (speaking in foreign language)
[01:43:23.540 --> 01:43:27.460]   (speaking in foreign language)
[01:43:27.460 --> 01:43:31.380]   (speaking in foreign language)
[01:43:31.380 --> 01:43:35.300]   (speaking in foreign language)
[01:43:36.220 --> 01:43:40.140]   (speaking in foreign language)
[01:43:40.140 --> 01:43:44.060]   (speaking in foreign language)
[01:43:44.980 --> 01:43:48.900]   (speaking in foreign language)
[01:43:48.900 --> 01:43:52.820]   (speaking in foreign language)
[01:43:53.820 --> 01:43:57.740]   (speaking in foreign language)
[01:43:57.740 --> 01:44:01.660]   (speaking in foreign language)
[01:44:01.660 --> 01:44:05.580]   (speaking in foreign language)
[01:44:05.580 --> 01:44:09.500]   (speaking in foreign language)
[01:44:09.500 --> 01:44:13.420]   (speaking in foreign language)
[01:44:13.420 --> 01:44:17.340]   (speaking in foreign language)
[01:44:17.340 --> 01:44:21.260]   (speaking in foreign language)
[01:44:21.260 --> 01:44:25.180]   (speaking in foreign language)
[01:44:25.180 --> 01:44:29.100]   (speaking in foreign language)
[01:44:29.100 --> 01:44:33.020]   (speaking in foreign language)
[01:44:33.020 --> 01:44:36.940]   (speaking in foreign language)
[01:44:36.940 --> 01:44:40.860]   (speaking in foreign language)
[01:44:40.860 --> 01:44:44.780]   (speaking in foreign language)
[01:44:44.780 --> 01:44:48.700]   (speaking in foreign language)
[01:44:48.700 --> 01:44:52.620]   (speaking in foreign language)
[01:44:52.620 --> 01:44:56.540]   (speaking in foreign language)
[01:44:56.540 --> 01:45:00.460]   (speaking in foreign language)
[01:45:01.380 --> 01:45:05.300]   (speaking in foreign language)
[01:45:05.300 --> 01:45:09.220]   (speaking in foreign language)
[01:45:09.220 --> 01:45:13.140]   (speaking in foreign language)
[01:45:13.140 --> 01:45:17.060]   (speaking in foreign language)
[01:45:17.060 --> 01:45:20.980]   (speaking in foreign language)
[01:45:20.980 --> 01:45:24.900]   (speaking in foreign language)
[01:45:24.900 --> 01:45:28.820]   (speaking in foreign language)
[01:45:28.820 --> 01:45:32.740]   (speaking in foreign language)
[01:45:32.740 --> 01:45:36.660]   (speaking in foreign language)
[01:45:37.580 --> 01:45:41.500]   (speaking in foreign language)
[01:45:41.500 --> 01:45:45.420]   (speaking in foreign language)
[01:45:46.260 --> 01:45:50.180]   (speaking in foreign language)
[01:45:50.180 --> 01:45:54.100]   (speaking in foreign language)
[01:45:54.100 --> 01:45:58.020]   (speaking in foreign language)
[01:45:59.020 --> 01:46:02.940]   (speaking in foreign language)
[01:46:02.940 --> 01:46:06.860]   (speaking in foreign language)
[01:46:06.860 --> 01:46:10.780]   (speaking in foreign language)
[01:46:10.780 --> 01:46:14.700]   (speaking in foreign language)
[01:46:14.700 --> 01:46:18.620]   (speaking in foreign language)
[01:46:18.620 --> 01:46:22.540]   (speaking in foreign language)
[01:46:23.460 --> 01:46:27.380]   (speaking in foreign language)
[01:46:27.380 --> 01:46:31.300]   (speaking in foreign language)
[01:46:32.220 --> 01:46:36.140]   (speaking in foreign language)
[01:46:36.140 --> 01:46:40.060]   (speaking in foreign language)
[01:46:40.060 --> 01:46:43.980]   (speaking in foreign language)
[01:46:44.980 --> 01:46:48.900]   (speaking in foreign language)
[01:46:48.900 --> 01:46:52.820]   (speaking in foreign language)
[01:46:52.820 --> 01:46:56.740]   (speaking in foreign language)
[01:46:56.740 --> 01:47:00.660]   (speaking in foreign language)
[01:47:00.660 --> 01:47:04.580]   (speaking in foreign language)
[01:47:05.500 --> 01:47:09.420]   (speaking in foreign language)
[01:47:09.420 --> 01:47:13.340]   (speaking in foreign language)
[01:47:14.260 --> 01:47:18.180]   (speaking in foreign language)
[01:47:19.020 --> 01:47:22.940]   (speaking in foreign language)
[01:47:22.940 --> 01:47:26.860]   (speaking in foreign language)
[01:47:26.860 --> 01:47:30.780]   (speaking in foreign language)
[01:47:30.780 --> 01:47:34.700]   (speaking in foreign language)
[01:47:34.700 --> 01:47:38.620]   (speaking in foreign language)
[01:47:38.620 --> 01:47:42.540]   (speaking in foreign language)
[01:47:42.540 --> 01:47:46.460]   (speaking in foreign language)
[01:47:46.460 --> 01:47:50.380]   (speaking in foreign language)
[01:47:50.380 --> 01:47:54.300]   (speaking in foreign language)
[01:47:54.300 --> 01:48:00.220]   (speaking in foreign language)
[01:48:20.220 --> 01:48:24.140]   (speaking in foreign language)
[01:48:24.140 --> 01:48:28.060]   (speaking in foreign language)
[01:48:28.060 --> 01:48:31.980]   (speaking in foreign language)
[01:48:31.980 --> 01:48:42.980]   (speaking in foreign language)
[01:48:42.980 --> 01:48:47.740]   (speaking in foreign language)
[01:48:47.740 --> 01:48:54.460]   (speaking in foreign language)
[01:48:54.460 --> 01:48:58.380]   (speaking in foreign language)
[01:48:58.380 --> 01:49:23.500]   Five minutes, take a rest.
[01:49:23.500 --> 01:49:24.340]   Yeah.
[01:49:24.340 --> 01:49:38.080]   (upbeat music)
[01:49:38.080 --> 01:49:40.660]   (upbeat music)
[01:49:40.660 --> 01:49:43.240]   (upbeat music)
[01:49:43.240 --> 01:49:45.820]   (upbeat music)
[01:49:45.820 --> 01:49:48.400]   (upbeat music)
[01:49:48.400 --> 01:49:50.980]   (upbeat music)
[01:49:50.980 --> 01:49:53.560]   (upbeat music)
[01:49:53.560 --> 01:49:56.140]   (upbeat music)
[01:49:56.140 --> 01:49:58.720]   (upbeat music)
[01:49:58.720 --> 01:50:01.300]   (upbeat music)
[01:50:01.300 --> 01:50:03.880]   (upbeat music)
[01:50:03.880 --> 01:50:06.460]   (upbeat music)
[01:50:06.460 --> 01:50:11.040]   (upbeat music)
[01:50:11.040 --> 01:50:15.540]   (upbeat music)
[01:50:15.540 --> 01:50:20.040]   (upbeat music)
[01:50:20.040 --> 01:50:24.620]   (upbeat music)
[01:50:24.620 --> 01:50:29.040]   (upbeat music)
[01:50:31.040 --> 01:50:33.620]   (upbeat music)
[01:50:33.620 --> 01:50:38.200]   (upbeat music)
[01:50:38.200 --> 01:50:42.620]   (upbeat music)
[01:50:42.620 --> 01:50:47.200]   (upbeat music)
[01:50:47.200 --> 01:50:51.780]   (upbeat music)
[01:50:51.780 --> 01:50:56.200]   (upbeat music)
[01:50:58.200 --> 01:51:00.780]   (upbeat music)
[01:51:00.780 --> 01:51:05.780]   (upbeat music)
[01:51:05.780 --> 01:51:12.780]   (upbeat music)
[01:51:12.780 --> 01:51:19.780]   (upbeat music)
[01:51:19.780 --> 01:51:25.780]   (upbeat music)
[01:51:26.780 --> 01:51:29.360]   (upbeat music)
[01:51:29.360 --> 01:51:34.360]   (upbeat music)
[01:51:34.360 --> 01:51:39.360]   (upbeat music)
[01:51:39.360 --> 01:51:45.360]   (upbeat music)
[01:51:45.360 --> 01:51:51.360]   (upbeat music)
[01:51:54.780 --> 01:51:57.360]   (upbeat music)
[01:51:58.360 --> 01:52:00.940]   (upbeat music)
[01:52:00.940 --> 01:52:05.940]   (upbeat music)
[01:52:05.940 --> 01:52:11.940]   (upbeat music)
[01:52:11.940 --> 01:52:17.940]   (upbeat music)
[01:52:17.940 --> 01:52:23.940]   (upbeat music)
[01:52:23.940 --> 01:52:26.520]   (upbeat music)
[01:52:26.520 --> 01:52:31.520]   (upbeat music)
[01:52:31.520 --> 01:52:38.520]   (upbeat music)
[01:52:38.520 --> 01:52:44.520]   (upbeat music)
[01:52:44.520 --> 01:52:50.520]   (upbeat music)
[01:52:51.520 --> 01:52:54.100]   (upbeat music)
[01:52:54.100 --> 01:53:00.100]   (upbeat music)
[01:53:00.100 --> 01:53:05.100]   (upbeat music)
[01:53:05.100 --> 01:53:11.100]   (upbeat music)
[01:53:11.100 --> 01:53:17.100]   (upbeat music)
[01:53:17.100 --> 01:53:19.680]   (upbeat music)
[01:53:19.680 --> 01:53:24.680]   (upbeat music)
[01:53:24.680 --> 01:53:29.680]   (upbeat music)
[01:53:29.680 --> 01:53:34.680]   (upbeat music)
[01:53:34.680 --> 01:53:40.680]   (upbeat music)
[01:53:40.680 --> 01:53:43.260]   (upbeat music)
[01:53:43.260 --> 01:53:45.840]   (upbeat music)
[01:53:45.840 --> 01:53:49.760]   (speaking in foreign language)
[01:53:49.760 --> 01:53:53.680]   (speaking in foreign language)
[01:53:53.680 --> 01:53:57.600]   (speaking in foreign language)
[01:53:57.600 --> 01:54:01.520]   (speaking in foreign language)
[01:54:01.520 --> 01:54:05.440]   (speaking in foreign language)
[01:54:05.440 --> 01:54:29.640]   - Hello world.
[01:54:29.640 --> 01:54:33.360]   And before I start, I'd like to thank Weights & Biases
[01:54:33.360 --> 01:54:35.320]   for this great opportunity
[01:54:35.320 --> 01:54:38.600]   and to our speakers and organizers for the great talk
[01:54:38.600 --> 01:54:40.240]   and their hard work.
[01:54:40.240 --> 01:54:44.480]   And today, my project,
[01:54:44.480 --> 01:54:47.480]   I'm going to introduce a project that I've been working on
[01:54:47.480 --> 01:54:49.480]   that I've dubbed Crescent.
[01:54:49.480 --> 01:54:53.440]   And the purpose is to make reproducible
[01:54:53.440 --> 01:54:56.240]   and maintainable deep learning projects
[01:54:56.240 --> 01:54:58.760]   easier to create and to maintain.
[01:54:58.760 --> 01:55:02.760]   And I hope that this is in line with the idea
[01:55:02.760 --> 01:55:06.080]   that Weights & Biases has with their product.
[01:55:06.080 --> 01:55:09.840]   So a brief introduction about myself.
[01:55:09.840 --> 01:55:11.560]   My name is Junhyung Lee.
[01:55:11.560 --> 01:55:16.560]   You can call me John Young Lee if that's my Anglicized name.
[01:55:16.560 --> 01:55:20.240]   I chose it because it was an anagram of my Korean name.
[01:55:20.240 --> 01:55:22.680]   And many of you might have heard me
[01:55:22.680 --> 01:55:24.600]   during the weekly archive talk
[01:55:24.640 --> 01:55:29.640]   where I work as a moderator.
[01:55:29.640 --> 01:55:32.120]   And okay.
[01:55:32.120 --> 01:55:35.320]   So before introducing Crescent,
[01:55:35.320 --> 01:55:38.520]   I wanted to talk about some issues
[01:55:38.520 --> 01:55:39.880]   that I think many of you,
[01:55:39.880 --> 01:55:44.400]   especially in corporate settings are very familiar with.
[01:55:44.400 --> 01:55:47.840]   And that is the, it works on my machine issue.
[01:55:47.840 --> 01:55:52.640]   And I think that even more than for ordinary deployments
[01:55:52.640 --> 01:55:54.720]   and developments in software engineering,
[01:55:54.720 --> 01:55:57.320]   that this issue is really serious
[01:55:57.320 --> 01:56:00.400]   for deep learning and machine learning in general.
[01:56:00.400 --> 01:56:05.400]   And so I think many of you here
[01:56:05.400 --> 01:56:09.560]   were probably surprised to add
[01:56:09.560 --> 01:56:11.720]   how hard CUDA is to configure.
[01:56:11.720 --> 01:56:14.960]   When I first went into graduate school,
[01:56:14.960 --> 01:56:21.320]   it took me over a day to just get my configuration
[01:56:21.600 --> 01:56:23.920]   on my computer to get working.
[01:56:23.920 --> 01:56:26.800]   And that was before I could install TensorFlow.
[01:56:26.800 --> 01:56:30.400]   And likewise, many people here,
[01:56:30.400 --> 01:56:32.120]   whenever you get a new machine,
[01:56:32.120 --> 01:56:35.040]   I think many of you will have a very hard time
[01:56:35.040 --> 01:56:38.960]   installing CUDA and getting the drivers correctly,
[01:56:38.960 --> 01:56:41.680]   getting all the packages and dependencies.
[01:56:41.680 --> 01:56:46.040]   And then after like many, many long hours later,
[01:56:46.040 --> 01:56:48.760]   you can finally get started with your project.
[01:56:48.760 --> 01:56:50.360]   And even when you do that,
[01:56:50.360 --> 01:56:53.600]   you have to keep doing that for every new machine
[01:56:53.600 --> 01:56:54.440]   that you come to.
[01:56:54.440 --> 01:56:55.760]   And for every new project,
[01:56:55.760 --> 01:56:58.640]   you have to tweak your configurations
[01:56:58.640 --> 01:57:00.880]   and settings and dependencies.
[01:57:00.880 --> 01:57:04.520]   And even if you do all this hard work,
[01:57:04.520 --> 01:57:07.560]   your projects become unreproducible after a few months
[01:57:07.560 --> 01:57:12.560]   because the dependency libraries get updated.
[01:57:12.560 --> 01:57:14.840]   Some things reach end of life
[01:57:14.840 --> 01:57:17.240]   and they have to be deprecated.
[01:57:17.240 --> 01:57:22.200]   You originally built your library on Python 3.6,
[01:57:22.200 --> 01:57:24.920]   but now some other library that you have to use
[01:57:24.920 --> 01:57:27.880]   uses Python 3.8 and you have to update,
[01:57:27.880 --> 01:57:31.600]   but now you have to go through all the documentation
[01:57:31.600 --> 01:57:34.320]   to find how to build this.
[01:57:34.320 --> 01:57:39.320]   And for me, it was very frustrating, even as a researcher.
[01:57:39.320 --> 01:57:44.240]   And especially now that I work in a corporate setting,
[01:57:44.240 --> 01:57:46.640]   I found it very difficult
[01:57:46.640 --> 01:57:50.080]   because there was just too much engineering load
[01:57:50.080 --> 01:57:51.600]   and too much technical depth
[01:57:51.600 --> 01:57:54.720]   that was being caused by all my projects.
[01:57:54.720 --> 01:57:59.640]   And I really wanted some sort of solution
[01:57:59.640 --> 01:58:02.140]   that could take care of this kind of thing for me.
[01:58:02.140 --> 01:58:08.080]   So I decided to write the fruits of my frustration
[01:58:08.080 --> 01:58:14.160]   into a little project that I dubbed Cressit.
[01:58:14.160 --> 01:58:16.800]   And why did I call it Cressit?
[01:58:16.800 --> 01:58:19.600]   Well, the word Cressit is an old word,
[01:58:19.600 --> 01:58:24.600]   meaning a vessel or cup that is used for bearing a torch.
[01:58:24.600 --> 01:58:28.640]   And I thought that since,
[01:58:28.640 --> 01:58:33.640]   so the name means that it's a layer of abstraction
[01:58:33.640 --> 01:58:39.480]   or a cup that is designed to hold any project inside,
[01:58:39.480 --> 01:58:41.520]   no matter what's underneath.
[01:58:41.520 --> 01:58:46.240]   Now, so I decided,
[01:58:46.240 --> 01:58:47.960]   so this is the repository
[01:58:47.960 --> 01:58:52.000]   and I've shared the project URL with everyone.
[01:58:52.000 --> 01:58:55.320]   So I hope that you're watching
[01:58:55.320 --> 01:58:57.200]   because I think it's a little easier
[01:58:57.200 --> 01:58:59.880]   to understand the presentation
[01:58:59.880 --> 01:59:03.640]   if you're also looking at the source code at the same time.
[01:59:03.640 --> 01:59:08.360]   And so to introduce Cressit, what is Cressit?
[01:59:08.360 --> 01:59:12.360]   Like the Cressit is a template repository.
[01:59:12.360 --> 01:59:16.080]   So it's not a single project,
[01:59:16.080 --> 01:59:20.760]   but sort of a starting point for your projects,
[01:59:20.760 --> 01:59:25.760]   a baseline to reduce a lot of your boilerplate code
[01:59:25.760 --> 01:59:31.520]   and to handle a lot of the engineering details
[01:59:31.520 --> 01:59:36.520]   about CUDA, MKL, CUDNN, and that kind of thing for you.
[01:59:36.760 --> 01:59:41.160]   And it's a system for easily reproducible
[01:59:41.160 --> 01:59:44.040]   and easily maintainable projects.
[01:59:44.040 --> 01:59:46.880]   So even, for example,
[01:59:46.880 --> 01:59:50.800]   if you want to update only your Python version
[01:59:50.800 --> 01:59:53.640]   and you want to build everything
[01:59:53.640 --> 01:59:57.160]   that you compiled previously, for example, PyTorch,
[01:59:57.160 --> 02:00:01.280]   and then maybe OpenCV with CUDA,
[02:00:01.280 --> 02:00:04.080]   all you have to do is just change the Python flag
[02:00:04.080 --> 02:00:07.400]   and everything will just build from the beginning.
[02:00:07.400 --> 02:00:12.400]   So it's a slightly miniature CI system,
[02:00:12.400 --> 02:00:16.120]   but specialized to machine learning
[02:00:16.120 --> 02:00:17.680]   and especially deep learning.
[02:00:17.680 --> 02:00:21.720]   So I think this is a difference
[02:00:21.720 --> 02:00:25.600]   between people in academia and in industry.
[02:00:25.600 --> 02:00:27.680]   But in academia, most people
[02:00:27.680 --> 02:00:30.240]   still just use local environments,
[02:00:30.240 --> 02:00:33.640]   but in industry, using Docker,
[02:00:33.640 --> 02:00:36.040]   this is simply not good enough.
[02:00:36.040 --> 02:00:39.800]   And Docker containers are the standard solution.
[02:00:39.800 --> 02:00:42.600]   For those of you who are not familiar with what Docker is,
[02:00:42.600 --> 02:00:44.200]   I will get into it soon.
[02:00:44.200 --> 02:00:48.960]   However, the way that most people in research
[02:00:48.960 --> 02:00:51.000]   or research engineering use Docker
[02:00:51.000 --> 02:00:56.560]   often makes it even harder to get things off the ground.
[02:00:56.560 --> 02:00:59.760]   People often use lots of shell scripts
[02:00:59.760 --> 02:01:03.800]   and they just commit their Docker containers
[02:01:03.800 --> 02:01:07.120]   into new images until there is just no difference
[02:01:07.120 --> 02:01:11.480]   between using a local environment and using a container.
[02:01:11.480 --> 02:01:16.040]   And this was something that I wished to solve
[02:01:16.040 --> 02:01:18.440]   using the Crescent template.
[02:01:18.440 --> 02:01:23.080]   So reproducibility was a really big key here.
[02:01:23.080 --> 02:01:27.720]   And so, although there are a lot of other utilities,
[02:01:27.720 --> 02:01:32.720]   there are only about four important components.
[02:01:32.720 --> 02:01:36.600]   And these are the requirements file, the Docker file,
[02:01:36.600 --> 02:01:39.840]   the Docker compose file, and the .env file.
[02:01:39.840 --> 02:01:44.840]   And most of these already have very reasonable defaults.
[02:01:44.840 --> 02:01:48.880]   So the Docker file is designed to get
[02:01:48.880 --> 02:01:53.080]   whatever version of NVIDIA CUDA you want.
[02:01:53.080 --> 02:01:55.200]   The compose file is already set up
[02:01:55.200 --> 02:01:59.520]   to start a interactive shell for your environments.
[02:01:59.520 --> 02:02:01.980]   And the requirements files,
[02:02:01.980 --> 02:02:06.800]   you will have to customize for your use case now.
[02:02:06.800 --> 02:02:11.800]   So that was a very, very brief introduction.
[02:02:11.800 --> 02:02:15.240]   And I know that what I'm talking about
[02:02:15.240 --> 02:02:19.920]   will not be very easy to understand at one try.
[02:02:19.920 --> 02:02:22.240]   So for those of you who are not,
[02:02:22.240 --> 02:02:27.240]   so I'll try to explain bit by bit what I'm trying to do.
[02:02:27.240 --> 02:02:28.520]   So what is Docker?
[02:02:28.520 --> 02:02:34.360]   Docker is similar to a virtual machine,
[02:02:34.360 --> 02:02:36.600]   but not quite the same.
[02:02:36.600 --> 02:02:40.080]   So Docker has this idea of containers,
[02:02:40.080 --> 02:02:44.840]   and you can package to this all kinds of software
[02:02:44.840 --> 02:02:48.320]   inside a Docker container and just deploy it
[02:02:48.320 --> 02:02:51.420]   regardless of the operating system underneath.
[02:02:51.420 --> 02:02:55.720]   However, the container, so,
[02:02:55.720 --> 02:02:59.440]   but the problem is that when you use NVIDIA GPUs
[02:02:59.440 --> 02:03:02.240]   or any other kind of GPU system,
[02:03:02.240 --> 02:03:06.060]   the dependency management simply can't be isolated
[02:03:06.060 --> 02:03:07.300]   from the hardware.
[02:03:07.300 --> 02:03:12.120]   And when you have updated hardware,
[02:03:12.120 --> 02:03:16.000]   for CPUs, the compatibility is rather good actually,
[02:03:16.000 --> 02:03:20.480]   but for GPUs and other pieces of specialized hardware,
[02:03:20.480 --> 02:03:25.200]   the compatibility is really, NVIDIA tries its best,
[02:03:25.200 --> 02:03:28.200]   but the compatibility can go out of date
[02:03:28.200 --> 02:03:29.840]   really, really quickly.
[02:03:29.840 --> 02:03:34.840]   So, but using Docker containers for environment management
[02:03:34.840 --> 02:03:38.600]   can create very useful,
[02:03:38.600 --> 02:03:42.080]   a baseline for getting reproducible results
[02:03:42.080 --> 02:03:44.400]   and environments for your project,
[02:03:44.400 --> 02:03:45.960]   or whether you're doing research
[02:03:45.960 --> 02:03:49.640]   or whether you're trying to deploy a research project
[02:03:49.640 --> 02:03:52.080]   into engineering mode.
[02:03:52.080 --> 02:03:56.200]   So a bit less familiar might be Docker Compose,
[02:03:56.200 --> 02:03:58.240]   even for people in industry.
[02:03:58.240 --> 02:03:59.920]   And what is Docker Compose?
[02:03:59.920 --> 02:04:03.760]   I just mentioned that most Docker users
[02:04:03.760 --> 02:04:05.960]   actually use lots of shell scripts
[02:04:05.960 --> 02:04:08.040]   to build their Docker image,
[02:04:08.040 --> 02:04:11.040]   and then create, and then run their containers
[02:04:11.040 --> 02:04:14.540]   for with certain configurations,
[02:04:14.540 --> 02:04:17.720]   like which ports to use, which volumes to use.
[02:04:17.720 --> 02:04:19.960]   But this is actually not a great idea
[02:04:19.960 --> 02:04:22.560]   because there is always,
[02:04:22.560 --> 02:04:24.900]   then everyone has to use their own shell script
[02:04:24.900 --> 02:04:26.400]   for every machine.
[02:04:26.400 --> 02:04:30.200]   And this also becomes very difficult to manage and scale up.
[02:04:30.200 --> 02:04:33.360]   And I think the people at Weights & Biases
[02:04:33.360 --> 02:04:37.360]   can agree with me that everyone's custom solution,
[02:04:37.360 --> 02:04:39.320]   this might look trivial at first,
[02:04:39.320 --> 02:04:41.940]   but once this starts to get,
[02:04:44.360 --> 02:04:46.100]   going into legacy,
[02:04:46.100 --> 02:04:48.920]   then we have a crisis of reproducibility.
[02:04:48.920 --> 02:04:51.080]   Even if the code itself is fine,
[02:04:51.080 --> 02:04:54.080]   it might not work because the environment dependencies
[02:04:54.080 --> 02:04:58.560]   are off, or some library has been updated
[02:04:58.560 --> 02:05:00.120]   without anyone knowing.
[02:05:00.120 --> 02:05:05.040]   And using Docker Compose, you can explicitly fix this
[02:05:05.040 --> 02:05:09.940]   and make it a lot easier to read and see what's going on,
[02:05:09.940 --> 02:05:12.080]   and improve consistency.
[02:05:13.760 --> 02:05:16.320]   And now this might sound very difficult,
[02:05:16.320 --> 02:05:19.560]   but Docker Compose is just a YAML file
[02:05:19.560 --> 02:05:22.880]   with a slightly unique syntax.
[02:05:22.880 --> 02:05:27.480]   And this is an example for an entire service
[02:05:27.480 --> 02:05:31.960]   based on an official NVIDIA image.
[02:05:31.960 --> 02:05:34.760]   And as you can see, it's not very long,
[02:05:34.760 --> 02:05:39.720]   especially when compared to Kubernetes or any other system,
[02:05:39.720 --> 02:05:44.720]   but using this, you can just create a entire environment
[02:05:44.720 --> 02:05:48.360]   and you can just download an entire operating system.
[02:05:48.360 --> 02:05:53.360]   For example, Ubuntu 20.04 with the latest CUDA drivers,
[02:05:53.360 --> 02:05:56.160]   with the latest CUDA installed,
[02:05:56.160 --> 02:05:58.400]   and create an interactive environment.
[02:05:58.400 --> 02:06:03.560]   And although Kubernetes is currently the market leader
[02:06:03.560 --> 02:06:08.560]   for MLOps, and the next session is actually concerned
[02:06:09.160 --> 02:06:13.040]   with using Kubeflow and other tools for deployment.
[02:06:13.040 --> 02:06:16.780]   Now, I think that Compose could be a reasonable solution
[02:06:16.780 --> 02:06:19.620]   for small to medium-sized projects
[02:06:19.620 --> 02:06:24.200]   that has been neglected, I think, unfortunately,
[02:06:24.200 --> 02:06:28.340]   but which I hope that more people might be interested
[02:06:28.340 --> 02:06:29.740]   in using later on.
[02:06:29.740 --> 02:06:34.540]   So why should I use Crescent?
[02:06:34.540 --> 02:06:39.540]   This is actually the most number one question that I get.
[02:06:39.540 --> 02:06:41.380]   I already have a workflow.
[02:06:41.380 --> 02:06:43.540]   Why should I use your workflow
[02:06:43.540 --> 02:06:47.180]   when I have to do extra work to get used to it?
[02:06:47.180 --> 02:06:52.020]   I hope that I can persuade many of you
[02:06:52.020 --> 02:06:54.500]   that you can create easy
[02:06:54.500 --> 02:06:57.180]   and reproducible environment configurations
[02:06:57.180 --> 02:07:03.420]   that can be easily shared across teams and between teammates.
[02:07:03.780 --> 02:07:06.100]   And using the Docker Compose syntax,
[02:07:06.100 --> 02:07:11.100]   you can cycle faster from research to deployment,
[02:07:11.100 --> 02:07:15.260]   which is also a neglected barrier
[02:07:15.260 --> 02:07:18.340]   to machine learning research
[02:07:18.340 --> 02:07:21.260]   that can actually be very helpful,
[02:07:21.260 --> 02:07:24.020]   especially for corporate users,
[02:07:24.020 --> 02:07:27.180]   and obviously source builds.
[02:07:27.180 --> 02:07:31.680]   Now, and who might benefit most from using Crescent?
[02:07:31.680 --> 02:07:34.140]   So I think that the general idea
[02:07:34.140 --> 02:07:37.860]   of using a simple configuration file,
[02:07:37.860 --> 02:07:40.620]   whether it be YAML or some other form,
[02:07:40.620 --> 02:07:43.540]   to maintain all of your dependencies
[02:07:43.540 --> 02:07:47.900]   and continuously check them is already well-established.
[02:07:47.900 --> 02:07:51.580]   And I think, especially in the continuous integration
[02:07:51.580 --> 02:07:53.780]   and the continuous deployment trends,
[02:07:57.980 --> 02:08:02.600]   and I think that Crescent is just part of that trend
[02:08:02.600 --> 02:08:05.680]   that is somewhat more specialized for machine learning,
[02:08:05.680 --> 02:08:09.120]   and especially for the small-scale users
[02:08:09.120 --> 02:08:13.520]   who do not have the benefit of 2,048 GPUs
[02:08:13.520 --> 02:08:15.560]   just lying around somewhere,
[02:08:15.560 --> 02:08:20.160]   and expert engineers who can handle all of their issues,
[02:08:20.160 --> 02:08:24.360]   but the ordinary grad student or the startup employee
[02:08:24.360 --> 02:08:28.800]   who might want to use a single or maybe a few servers
[02:08:28.800 --> 02:08:32.560]   with maybe dozens, up to maybe a dozen GPUs.
[02:08:32.560 --> 02:08:35.680]   Also, people who have to,
[02:08:35.680 --> 02:08:38.860]   people who are core PyTorch contributors
[02:08:38.860 --> 02:08:42.640]   or other CUDA contributors might find this very useful
[02:08:42.640 --> 02:08:46.560]   because since the entire build process is automated,
[02:08:46.560 --> 02:08:50.400]   and at least in my opinion, well-debugged,
[02:08:50.400 --> 02:08:53.160]   they can just keep modifying their code
[02:08:53.160 --> 02:08:54.680]   without having to worry about
[02:08:54.680 --> 02:08:59.600]   whether their compilation process is buggy
[02:08:59.600 --> 02:09:02.420]   or whether their builds are unreproducible.
[02:09:02.420 --> 02:09:07.880]   And like I mentioned, people like me
[02:09:07.880 --> 02:09:11.780]   who find Kubernetes to be simply too difficult
[02:09:11.780 --> 02:09:15.080]   or too much work for their individual projects,
[02:09:15.080 --> 02:09:19.440]   I think that Crescent can be a reasonable solution
[02:09:19.440 --> 02:09:21.240]   to their problems.
[02:09:21.240 --> 02:09:23.440]   And finally, about two days ago,
[02:09:23.440 --> 02:09:28.440]   NVIDIA recently announced that the H100 GPU
[02:09:28.440 --> 02:09:31.440]   with the Hopper architecture,
[02:09:31.440 --> 02:09:35.700]   and currently the official PyTorch binaries,
[02:09:35.700 --> 02:09:38.720]   I think will not work on these GPUs,
[02:09:38.720 --> 02:09:41.760]   and you will have to build these from source.
[02:09:41.760 --> 02:09:43.500]   And if you're an early adopter
[02:09:43.500 --> 02:09:46.840]   who just can't wait for the official binaries to come out,
[02:09:46.840 --> 02:09:51.200]   I think that you'll find my project to be somewhat useful
[02:09:51.200 --> 02:09:55.740]   instead of having to learn
[02:09:55.740 --> 02:09:59.580]   how to build a PyTorch from source all on your own.
[02:09:59.580 --> 02:10:05.780]   Okay, and I mentioned this during the tutorial,
[02:10:05.780 --> 02:10:10.640]   and this is somewhat embarrassing on my part,
[02:10:10.640 --> 02:10:14.280]   but many of you listening to the talk
[02:10:14.280 --> 02:10:17.840]   may have already known about my project.
[02:10:17.840 --> 02:10:21.360]   I've worked very hard to advertise it,
[02:10:21.360 --> 02:10:23.800]   but originally it was designed
[02:10:23.800 --> 02:10:26.800]   to be a PyTorch source build template
[02:10:26.800 --> 02:10:29.540]   that would hopefully improve runtime speeds.
[02:10:29.540 --> 02:10:32.160]   And I am not lying.
[02:10:32.160 --> 02:10:33.420]   When I first tried it out,
[02:10:33.420 --> 02:10:35.960]   I really did find a four times increase
[02:10:35.960 --> 02:10:38.760]   in the runtime compute speed,
[02:10:38.760 --> 02:10:42.200]   but when I tried to reproduce it,
[02:10:42.200 --> 02:10:47.200]   I found maybe about 70 to maybe double
[02:10:48.160 --> 02:10:51.520]   time difference, and most of that could be
[02:10:51.520 --> 02:10:53.960]   simply narrowed down to versioning issues.
[02:10:53.960 --> 02:10:59.760]   And I think people who know me from the archive talk
[02:10:59.760 --> 02:11:04.000]   will know that I have a very negative opinion
[02:11:04.000 --> 02:11:07.920]   about false positives and the research community
[02:11:07.920 --> 02:11:10.880]   always producing interesting things,
[02:11:10.880 --> 02:11:14.240]   but they're not being reproducible.
[02:11:14.240 --> 02:11:17.340]   And I hope that this confession of mine
[02:11:17.340 --> 02:11:21.160]   will encourage others to also be more honest
[02:11:21.160 --> 02:11:24.160]   about their mishaps about reproducibility.
[02:11:24.160 --> 02:11:30.280]   And these are the benchmarks that I tried out,
[02:11:30.280 --> 02:11:34.640]   but as you can see, the left is a source build
[02:11:34.640 --> 02:11:36.640]   and the right is a naive install.
[02:11:36.640 --> 02:11:41.640]   And there is a bit of a time difference, but it depends.
[02:11:42.280 --> 02:11:47.280]   Likewise, this is a benchmark on a RTX 380
[02:11:47.280 --> 02:11:52.080]   on a source build and on from the latest install.
[02:11:52.080 --> 02:11:54.420]   The source build has two minutes
[02:11:54.420 --> 02:11:57.080]   and the install from the PyTorch GitHub repo
[02:11:57.080 --> 02:11:59.960]   has three minutes, but to be honest,
[02:11:59.960 --> 02:12:03.800]   this was also cherry picked to be the most,
[02:12:03.800 --> 02:12:07.920]   with the most noticeable difference.
[02:12:07.920 --> 02:12:11.320]   So whether a source builds help or not
[02:12:11.320 --> 02:12:13.920]   really, really depends on your hardware
[02:12:13.920 --> 02:12:16.900]   and on your computational load.
[02:12:16.900 --> 02:12:22.200]   However, that still, however,
[02:12:22.200 --> 02:12:24.120]   a Crescent can still help a lot
[02:12:24.120 --> 02:12:26.160]   with your environment management.
[02:12:26.160 --> 02:12:31.160]   And so I've decided to create a little bit,
[02:12:31.160 --> 02:12:36.160]   a few videos to help understanding and let me see.
[02:12:39.160 --> 02:12:44.160]   Okay, so first you install the repository from GitHub
[02:12:44.160 --> 02:12:48.800]   and then you move in and I've shown,
[02:12:48.800 --> 02:12:52.160]   some configuration files are visible above
[02:12:52.160 --> 02:12:55.320]   and you enter the makeenv command
[02:12:55.320 --> 02:12:58.640]   and then you edit the environment file.
[02:12:58.640 --> 02:13:01.920]   You just paste in a bunch of options that you want
[02:13:01.920 --> 02:13:06.920]   and then you type make up and that's it.
[02:13:08.840 --> 02:13:13.720]   Then you take a break, depending on which option you use,
[02:13:13.720 --> 02:13:15.600]   whether you're using,
[02:13:15.600 --> 02:13:18.560]   whether you want to build a PyTorch from source
[02:13:18.560 --> 02:13:22.360]   or whether you just want to install packages
[02:13:22.360 --> 02:13:24.280]   or if you've already done this before
[02:13:24.280 --> 02:13:25.920]   and it's already in the cache,
[02:13:25.920 --> 02:13:31.480]   you can, this, at most it takes about 20 minutes
[02:13:31.480 --> 02:13:33.800]   to get an up and running environment.
[02:13:33.800 --> 02:13:37.120]   And even if you build a PyTorch from source,
[02:13:37.120 --> 02:13:42.120]   it takes about an hour and a half on my laptop
[02:13:42.120 --> 02:13:45.440]   to install about 20 gigabytes
[02:13:45.440 --> 02:13:47.240]   and then build PyTorch from source.
[02:13:47.240 --> 02:13:51.120]   So yeah, it can be, it's a lot,
[02:13:51.120 --> 02:13:54.520]   it makes setting up your development environments
[02:13:54.520 --> 02:13:56.240]   very, very easy.
[02:13:56.240 --> 02:13:58.880]   And I hope people in the research community
[02:13:58.880 --> 02:14:02.080]   who might not be as interested in reproducibility
[02:14:02.080 --> 02:14:06.080]   and maintainability are still interested
[02:14:06.080 --> 02:14:11.080]   in the ease of setting up for each project.
[02:14:11.080 --> 02:14:16.480]   So after you install, so you do a make up command
[02:14:16.480 --> 02:14:20.920]   and after waiting for a bit,
[02:14:20.920 --> 02:14:25.200]   you find that the image will be built
[02:14:25.200 --> 02:14:26.760]   and your environment will be set up.
[02:14:26.760 --> 02:14:31.280]   But this still sounds really hard to do.
[02:14:31.280 --> 02:14:33.480]   So all you have to do is a make exec
[02:14:33.480 --> 02:14:35.800]   and you're inside your container.
[02:14:35.800 --> 02:14:39.920]   Oh, that lasted a lot shorter than I hoped for.
[02:14:39.920 --> 02:14:42.720]   But once you run the make exec command,
[02:14:42.720 --> 02:14:45.720]   you actually have an interactive terminal
[02:14:45.720 --> 02:14:47.200]   inside your container.
[02:14:47.200 --> 02:14:51.600]   So this is just like using a remote shell,
[02:14:51.600 --> 02:14:54.400]   remote terminal to code.
[02:14:54.400 --> 02:14:57.520]   And from a user experience point of view,
[02:14:57.520 --> 02:15:02.160]   there's just no difference from using your local terminal,
[02:15:02.160 --> 02:15:07.160]   your local terminal and using a Crescent terminal.
[02:15:07.160 --> 02:15:11.400]   Maybe that you'll like the UI better.
[02:15:11.400 --> 02:15:16.040]   I really worked hard to get the UI to my liking.
[02:15:16.040 --> 02:15:19.200]   So, and if you want to add another dependency,
[02:15:19.200 --> 02:15:21.880]   for example, let's say weights and biases
[02:15:21.880 --> 02:15:27.800]   and you add the dependency and you think,
[02:15:27.800 --> 02:15:30.320]   do I have to wait another hour
[02:15:30.320 --> 02:15:33.000]   just to get another reproducible environment?
[02:15:33.000 --> 02:15:35.280]   The answer is no, since, oh,
[02:15:35.280 --> 02:15:37.680]   since everything has already been cached,
[02:15:37.680 --> 02:15:42.360]   all you have to do is type in the make rebuild command
[02:15:42.360 --> 02:15:46.160]   and you will have another perfectly reproducible environment
[02:15:46.160 --> 02:15:49.800]   where everything just works.
[02:15:49.800 --> 02:15:53.920]   And that Docker image can be used to deploy on a cloud
[02:15:53.920 --> 02:15:58.720]   or sent to someone else so that they can run your project
[02:15:58.720 --> 02:16:00.680]   or you can upload it on your,
[02:16:00.680 --> 02:16:03.840]   upload it onto Docker Hub with a link
[02:16:03.840 --> 02:16:07.520]   onto your GitHub project so that other researchers
[02:16:07.520 --> 02:16:12.520]   can try out your code without having to worry about
[02:16:12.520 --> 02:16:17.600]   which PyTorch version or which dependency was what version
[02:16:17.600 --> 02:16:20.840]   and which settings were wrong and that kind of thing.
[02:16:20.840 --> 02:16:25.840]   And, oh, okay, okay.
[02:16:28.320 --> 02:16:29.800]   A few limitations.
[02:16:29.800 --> 02:16:34.640]   Right now, the template still requires users
[02:16:34.640 --> 02:16:39.640]   to install the NVIDIA driver, Docker and Docker Compose.
[02:16:39.640 --> 02:16:43.920]   And you still have to read the code a bit
[02:16:43.920 --> 02:16:48.120]   to find out which configurations you need.
[02:16:48.120 --> 02:16:50.920]   And there are some other limitations
[02:16:50.920 --> 02:16:53.840]   for source code editing.
[02:16:55.520 --> 02:17:00.520]   And to be fair, I updated the documentation last night
[02:17:00.520 --> 02:17:05.040]   at 2 a.m., I think, and at morning today.
[02:17:05.040 --> 02:17:10.040]   So the documentation is not quite as up to scratch
[02:17:10.040 --> 02:17:14.600]   as I would like, but please raise an issue
[02:17:14.600 --> 02:17:17.600]   or send a chat in the discussions
[02:17:17.600 --> 02:17:21.200]   and I think I can help you out with your issue.
[02:17:21.200 --> 02:17:23.880]   And that was the end of my talk.
[02:17:23.880 --> 02:17:25.000]   Thank you for listening.
[02:17:25.000 --> 02:17:28.360]   And the image here that I'd like to show
[02:17:28.360 --> 02:17:32.760]   while ending my talk is that of Sisyphus.
[02:17:32.760 --> 02:17:35.560]   And why did I include him here?
[02:17:35.560 --> 02:17:39.240]   I included him here because watching everyone
[02:17:39.240 --> 02:17:42.600]   just struggling with getting to research
[02:17:42.600 --> 02:17:46.960]   and then to deployment and then maintaining their projects
[02:17:46.960 --> 02:17:48.920]   or just setting up CUDA,
[02:17:48.920 --> 02:17:53.240]   whether you're a graduate student who doesn't know
[02:17:53.240 --> 02:17:56.920]   what exactly CUDA is or how to set it up
[02:17:56.920 --> 02:17:58.800]   to get it working efficiently,
[02:17:58.800 --> 02:18:02.880]   or whether you're a research engineer
[02:18:02.880 --> 02:18:06.040]   trying to turn research code into deployment code,
[02:18:06.040 --> 02:18:11.040]   I keep hearing lots of things from people in industry
[02:18:11.040 --> 02:18:14.960]   and in academia, and I thought that it was such a lot
[02:18:14.960 --> 02:18:17.320]   of work for so little benefit.
[02:18:17.320 --> 02:18:20.680]   And I hope that my project can lift that rock
[02:18:20.680 --> 02:18:23.520]   off everyone's shoulders so that they can get back
[02:18:23.520 --> 02:18:28.520]   to doing great research or making great products.
[02:18:28.520 --> 02:18:33.680]   And I hope that this will really help everyone out.
[02:18:33.680 --> 02:18:36.920]   Thank you for listening.
[02:18:36.920 --> 02:18:43.480]   Yep.
[02:18:43.480 --> 02:18:46.800]   Does anyone have any questions?
[02:18:46.800 --> 02:18:50.320]   I think I finished a bit early.
[02:18:51.160 --> 02:18:53.320]   (silence)
[02:18:53.320 --> 02:19:02.320]   No questions?
[02:19:02.320 --> 02:19:12.560]   - I just have more of a comment.
[02:19:12.560 --> 02:19:15.280]   I really liked that.
[02:19:15.280 --> 02:19:17.560]   Yeah.
[02:19:18.240 --> 02:19:23.240]   We, on the way on our team side or like within Grudge,
[02:19:23.240 --> 02:19:25.240]   yeah, we're thinking a little bit more
[02:19:25.240 --> 02:19:29.560]   in the next quarter or two in terms of, yeah,
[02:19:29.560 --> 02:19:31.760]   the use of like Docker and containers like that
[02:19:31.760 --> 02:19:34.680]   to like improve like reproducibility,
[02:19:34.680 --> 02:19:36.840]   alongside all of the goodness
[02:19:36.840 --> 02:19:38.400]   that the Weights and Weights brings.
[02:19:38.400 --> 02:19:41.680]   So yeah, super, super relevant talk.
[02:19:41.680 --> 02:19:44.320]   - Thank you.
[02:19:47.280 --> 02:19:48.120]   Yeah.
[02:19:48.120 --> 02:19:51.400]   If you like it, please start the repository.
[02:19:51.400 --> 02:19:53.960]   Thanks.
[02:19:53.960 --> 02:19:59.920]   Actually, I recently submitted this to DockerCon,
[02:19:59.920 --> 02:20:04.920]   but I just got a rejection notice during Stella's talk.
[02:20:04.920 --> 02:20:08.840]   Apparently it was a very narrow reject.
[02:20:08.840 --> 02:20:11.840]   So if you know anyone at DockerCon,
[02:20:11.840 --> 02:20:15.120]   please guide them to the right path, you know?
[02:20:16.160 --> 02:20:17.200]   - Yeah.
[02:20:17.200 --> 02:20:19.200]   Anyone who has influence.
[02:20:19.200 --> 02:20:23.320]   Apparently if someone else drops out,
[02:20:23.320 --> 02:20:28.320]   I might be accepted according to the rejection notice.
[02:20:28.320 --> 02:20:30.240]   I don't know whether this is true.
[02:20:30.240 --> 02:20:34.120]   - Yep.
[02:20:34.120 --> 02:20:35.400]   Hopefully you can make it.
[02:20:35.400 --> 02:20:38.000]   I actually do a question.
[02:20:38.000 --> 02:20:41.560]   So I'm sorry you mentioned this and I didn't catch it.
[02:20:43.240 --> 02:20:48.240]   NVIDIA, you know, like also have Dockers that they offer
[02:20:48.240 --> 02:20:51.240]   that are, you know, like optimized
[02:20:51.240 --> 02:20:54.920]   for like a particular framework or like machine.
[02:20:54.920 --> 02:20:59.280]   Can this interact with that?
[02:20:59.280 --> 02:21:00.200]   Or like can basically,
[02:21:00.200 --> 02:21:03.040]   can you get the benefit of both worlds?
[02:21:03.040 --> 02:21:05.560]   - Actually, if you look at the repository,
[02:21:05.560 --> 02:21:10.560]   I have a Compose service for the official NVIDIA NGC image.
[02:21:13.120 --> 02:21:17.080]   And one of the key points that I couldn't get into
[02:21:17.080 --> 02:21:19.520]   during the presentation was that
[02:21:19.520 --> 02:21:22.800]   this was generally applicable to any,
[02:21:22.800 --> 02:21:27.800]   the idea behind it is generally applicable to any project,
[02:21:27.800 --> 02:21:29.080]   not just this one.
[02:21:29.080 --> 02:21:32.240]   So this is a very project independent idea,
[02:21:32.240 --> 02:21:37.240]   but someone had to get a specific example up and running
[02:21:37.240 --> 02:21:40.120]   to persuade people to use it.
[02:21:40.120 --> 02:21:43.680]   And, you know, to brag a little bit,
[02:21:43.680 --> 02:21:48.680]   it already has 578 stars and Yannick Kilcher
[02:21:48.680 --> 02:21:53.240]   and Djorko Borovic at PyTorch Lightning.
[02:21:53.240 --> 02:21:57.960]   And many people have already starred the repository.
[02:21:57.960 --> 02:22:01.680]   So I think you might find it interesting.
[02:22:01.680 --> 02:22:02.520]   - Lovely.
[02:22:02.520 --> 02:22:04.160]   Yeah, we'd take a closer look for sure.
[02:22:04.160 --> 02:22:05.040]   - Yeah.
[02:22:05.040 --> 02:22:08.320]   And if anyone is interested in translations
[02:22:08.320 --> 02:22:11.240]   or suggestions or contributions,
[02:22:11.240 --> 02:22:13.880]   please feel free to make an issue
[02:22:13.880 --> 02:22:16.440]   or comment on the discussions.
[02:22:16.440 --> 02:22:18.120]   Or if you just want to take it
[02:22:18.120 --> 02:22:21.120]   and use it inside your company, feel free to do so.
[02:22:21.120 --> 02:22:24.600]   Like there are already people in Korea,
[02:22:24.600 --> 02:22:26.600]   lots of startups who are already using
[02:22:26.600 --> 02:22:30.480]   this project internally.
[02:22:30.480 --> 02:22:35.000]   One minus, one limitation on my fault.
[02:22:36.840 --> 02:22:41.600]   One thing that I did not do very well was documentation.
[02:22:41.600 --> 02:22:46.600]   Like I said, I revamped the documentation yesterday
[02:22:46.600 --> 02:22:48.520]   and today morning.
[02:22:48.520 --> 02:22:51.440]   So the documentation quality,
[02:22:51.440 --> 02:22:54.200]   even yesterday was not quite as good,
[02:22:54.200 --> 02:22:55.720]   not very good, frankly.
[02:22:55.720 --> 02:22:57.640]   And a lot of it was out of date,
[02:22:57.640 --> 02:23:02.520]   but I've officially created a version 0.2 release.
[02:23:02.520 --> 02:23:05.080]   So I think it might be a lot easier to use.
[02:23:06.720 --> 02:23:07.720]   Just try it out.
[02:23:07.720 --> 02:23:11.160]   And if something goes wrong, please feel free to comment.
[02:23:11.160 --> 02:23:14.720]   Thank you.
[02:23:14.720 --> 02:23:20.160]   (speaking in foreign language)
[02:23:20.160 --> 02:23:24.080]   (speaking in foreign language)
[02:23:24.080 --> 02:23:28.000]   (speaking in foreign language)
[02:23:29.000 --> 02:23:32.920]   (speaking in foreign language)
[02:23:33.840 --> 02:23:37.760]   (speaking in foreign language)
[02:23:37.760 --> 02:23:41.680]   (speaking in foreign language)
[02:23:41.680 --> 02:23:45.600]   (speaking in foreign language)
[02:23:45.600 --> 02:23:48.200]   (upbeat music)
[02:23:48.200 --> 02:23:50.800]   (upbeat music)
[02:23:50.800 --> 02:23:54.720]   (speaking in foreign language)
[02:23:54.720 --> 02:24:14.760]   (upbeat music)
[02:24:14.760 --> 02:24:17.360]   (upbeat music)
[02:24:17.360 --> 02:24:19.960]   (upbeat music)
[02:24:19.960 --> 02:24:22.560]   (upbeat music)
[02:24:23.560 --> 02:24:26.160]   (upbeat music)
[02:24:26.160 --> 02:24:28.760]   (upbeat music)
[02:24:28.760 --> 02:24:39.360]   (upbeat music)
[02:24:39.360 --> 02:24:52.960]   (upbeat music)
[02:24:53.560 --> 02:24:56.160]   (upbeat music)
[02:24:56.160 --> 02:25:02.760]   (upbeat music)
[02:25:02.760 --> 02:25:08.760]   (upbeat music)
[02:25:08.760 --> 02:25:15.760]   (upbeat music)
[02:25:15.760 --> 02:25:22.360]   (upbeat music)
[02:25:22.560 --> 02:25:25.160]   (upbeat music)
[02:25:25.160 --> 02:25:35.160]   (upbeat music)
[02:25:35.160 --> 02:25:45.160]   (upbeat music)
[02:25:45.360 --> 02:25:47.960]   (upbeat music)
[02:25:47.960 --> 02:26:00.960]   (upbeat music)
[02:26:13.360 --> 02:26:15.960]   (upbeat music)
[02:26:15.960 --> 02:26:36.160]   (upbeat music)
[02:26:36.360 --> 02:26:38.960]   (upbeat music)
[02:26:38.960 --> 02:26:46.960]   (upbeat music)
[02:26:46.960 --> 02:26:56.960]   (upbeat music)
[02:27:04.360 --> 02:27:06.960]   (upbeat music)
[02:27:06.960 --> 02:27:21.760]   (soft music)
[02:27:22.760 --> 02:27:25.160]   (soft music)
[02:27:26.160 --> 02:27:28.560]   (soft music)
[02:27:54.160 --> 02:27:56.560]   (soft music)
[02:27:56.560 --> 02:28:21.560]   (soft music)
[02:28:21.560 --> 02:28:23.960]   (soft music)
[02:28:23.960 --> 02:28:33.960]   (soft music)
[02:28:33.960 --> 02:28:43.960]   (soft music)
[02:28:43.960 --> 02:28:46.360]   (soft music)
[02:28:46.360 --> 02:29:00.960]   (speaking in foreign language)
[02:29:00.960 --> 02:29:04.860]   (speaking in foreign language)
[02:29:04.860 --> 02:29:08.760]   (speaking in foreign language)
[02:29:08.760 --> 02:29:12.660]   (speaking in foreign language)
[02:29:13.660 --> 02:29:17.560]   (speaking in foreign language)
[02:29:17.560 --> 02:29:21.460]   (speaking in foreign language)
[02:29:21.460 --> 02:29:25.360]   (speaking in foreign language)
[02:29:25.360 --> 02:29:29.260]   (speaking in foreign language)
[02:29:53.960 --> 02:29:56.060]   - I'm prepared to show the video,
[02:29:56.060 --> 02:29:57.660]   so please wait a little bit.
[02:29:57.660 --> 02:30:20.460]   Just a little bit more time seems to be take.
[02:30:21.460 --> 02:30:24.460]   (keyboard clacking)
[02:30:24.460 --> 02:30:27.460]   (keyboard clacking)
[02:30:27.460 --> 02:30:33.460]   Oops.
[02:30:33.460 --> 02:30:36.460]   (keyboard clacking)
[02:30:37.460 --> 02:30:40.460]   (keyboard clacking)
[02:30:40.460 --> 02:31:03.460]   To.
[02:31:04.460 --> 02:31:07.460]   (keyboard clacking)
[02:31:07.460 --> 02:31:19.960]   Hi everyone, nice to meet you.
[02:31:19.960 --> 02:31:22.960]   My name is Chansung Park from South Korea,
[02:31:22.960 --> 02:31:27.260]   and I'm a Google developers expert for machine learning.
[02:31:27.260 --> 02:31:29.460]   Today I wanna give a talk about the topic,
[02:31:29.460 --> 02:31:32.460]   continuously adapting machine learning system
[02:31:32.460 --> 02:31:34.560]   and GCP and TFX.
[02:31:34.560 --> 02:31:37.460]   What will be covered then?
[02:31:37.460 --> 02:31:41.160]   First of all, let's define what is machine learning operation
[02:31:41.160 --> 02:31:45.260]   and why we need the CI/CD pipeline for machine learning.
[02:31:45.260 --> 02:31:47.160]   Then we'll review the key components
[02:31:47.160 --> 02:31:52.160]   to make MLOps possible in Google Cloud Platform or GCP.
[02:31:52.160 --> 02:31:57.060]   Particularly, we'll see machine learning pipeline
[02:31:57.060 --> 02:31:59.960]   is not the only one to realize MLOps.
[02:31:59.960 --> 02:32:02.160]   There are many more to consider.
[02:32:02.160 --> 02:32:04.960]   Also, we'll see some of the underlying details,
[02:32:04.960 --> 02:32:07.460]   how machine learning pipeline works as well.
[02:32:07.460 --> 02:32:10.860]   By introducing the key components in the cloud,
[02:32:10.860 --> 02:32:13.960]   you might wonder how and where to get started.
[02:32:13.960 --> 02:32:15.760]   In the third section of this session,
[02:32:15.760 --> 02:32:18.860]   I'll briefly go over some of the recommendations,
[02:32:18.860 --> 02:32:22.660]   how you can get started with the MLOps in GCP.
[02:32:22.660 --> 02:32:24.760]   In the final section, I'll close this session
[02:32:24.760 --> 02:32:27.260]   by giving five specific sample
[02:32:27.260 --> 02:32:30.260]   and somewhat complicated examples.
[02:32:30.260 --> 02:32:32.760]   The first and the second examples will see
[02:32:32.760 --> 02:32:37.260]   how to leverage AutoML capabilities to realize MLOps.
[02:32:37.260 --> 02:32:38.960]   In this specific examples,
[02:32:38.960 --> 02:32:42.560]   I have built a simple yet interesting example MLOp system
[02:32:42.560 --> 02:32:45.460]   that could adapt to the changes of my daughter's face
[02:32:45.460 --> 02:32:47.160]   as she grows up.
[02:32:47.160 --> 02:32:50.960]   The third example will see a very common MLOp scenario,
[02:32:50.960 --> 02:32:53.760]   which is known as layered predictions.
[02:32:53.760 --> 02:32:58.660]   The fourth example, we'll move on to more complex example
[02:32:58.660 --> 02:33:02.460]   about how you can build MLOp system evolving
[02:33:02.460 --> 02:33:04.760]   as the underlying code base changes.
[02:33:04.760 --> 02:33:09.860]   You will see some of the FX usages here as well.
[02:33:09.860 --> 02:33:12.060]   In the final example, we'll take a look
[02:33:12.060 --> 02:33:16.360]   at how to build MLOp system evolving as the data changes.
[02:33:16.360 --> 02:33:23.660]   Let's dive in why CI/CD for machine learning.
[02:33:23.660 --> 02:33:25.860]   There are lots of many different situations
[02:33:25.860 --> 02:33:27.160]   that you need this,
[02:33:27.160 --> 02:33:32.160]   but let me tell you this with some simple examples.
[02:33:32.160 --> 02:33:34.960]   We often claim that we got a state-of-the-art image
[02:33:34.960 --> 02:33:38.360]   classification or natural language processing models.
[02:33:38.360 --> 02:33:41.360]   However, there's no universally best model.
[02:33:41.360 --> 02:33:44.460]   The best model you get is an optimal model
[02:33:44.460 --> 02:33:47.960]   from a snapshot of a certain period of time.
[02:33:47.960 --> 02:33:49.560]   So when the timeline moves,
[02:33:49.560 --> 02:33:53.160]   the snapshot is not the optimal snapshot anymore.
[02:33:53.160 --> 02:33:54.860]   As you see from this slide,
[02:33:54.860 --> 02:33:58.560]   you can clearly see the fashion trend has been changed
[02:33:58.560 --> 02:34:01.060]   over time and we know it.
[02:34:01.060 --> 02:34:05.060]   So even if you get the best model to recognize clothes
[02:34:05.060 --> 02:34:08.160]   in 2010 to 2019 period,
[02:34:08.160 --> 02:34:11.760]   it's highly possible that it won't be the best model
[02:34:11.760 --> 02:34:12.860]   in the near future.
[02:34:12.860 --> 02:34:17.260]   We can see similar phenomenon from the languages
[02:34:17.260 --> 02:34:19.660]   that we use in our daily life.
[02:34:19.660 --> 02:34:23.060]   Our children don't know the language from our teenage,
[02:34:23.060 --> 02:34:25.560]   and we don't know their languages too.
[02:34:25.560 --> 02:34:28.860]   And as you might notice from time to time,
[02:34:28.860 --> 02:34:33.860]   you often see some new slang expressions from SNSs.
[02:34:33.860 --> 02:34:35.160]   If you don't know,
[02:34:35.160 --> 02:34:37.760]   and if your model was trained in your age,
[02:34:37.760 --> 02:34:39.660]   your model doesn't know them either.
[02:34:39.660 --> 02:34:43.860]   This kind of situation is often cited
[02:34:43.860 --> 02:34:46.760]   as data drift or concept drift.
[02:34:46.760 --> 02:34:49.160]   Machine learning model is like a black box
[02:34:49.160 --> 02:34:53.860]   and you don't know when the performance degradation occurs.
[02:34:53.860 --> 02:34:56.560]   Like we can't insert if and else statement
[02:34:56.560 --> 02:34:59.360]   for exception handling inside of the model.
[02:34:59.360 --> 02:35:04.360]   That's why our changes to model is called silent killer.
[02:35:04.360 --> 02:35:07.460]   So what we need is monitoring capabilities,
[02:35:07.460 --> 02:35:10.060]   meaning monitoring and evaluating
[02:35:10.060 --> 02:35:12.360]   the model performance periodically
[02:35:12.360 --> 02:35:15.060]   by giving a newly collected dataset
[02:35:15.060 --> 02:35:18.360]   that the model has never seen from the training phase.
[02:35:18.360 --> 02:35:21.460]   Basically, there are two common situations
[02:35:21.460 --> 02:35:23.960]   when the machine learning system has to evolve.
[02:35:23.960 --> 02:35:26.460]   First one is based on dataset changes,
[02:35:26.460 --> 02:35:30.560]   meaning data or concept drift has occurred.
[02:35:30.560 --> 02:35:34.160]   Second one is based on code-based changes,
[02:35:34.160 --> 02:35:37.660]   meaning we get better technologies over time.
[02:35:37.660 --> 02:35:40.560]   For instance, we witnessed data pre-processing,
[02:35:40.560 --> 02:35:42.560]   data engineering strategies,
[02:35:42.560 --> 02:35:46.160]   and modeling strategies changes so quickly in these days,
[02:35:46.160 --> 02:35:47.760]   especially in academia.
[02:35:47.760 --> 02:35:54.160]   And the evolution of MLO system has to be done automatically.
[02:35:54.160 --> 02:35:56.960]   Let's go through what are the key components
[02:35:56.960 --> 02:36:01.460]   in GCP environment to enable such MLO system.
[02:36:01.460 --> 02:36:05.160]   First of all, we need to define a machine learning pipeline
[02:36:05.160 --> 02:36:09.160]   and there are two main ways to write it in Vertex AI platform.
[02:36:09.160 --> 02:36:12.760]   We can think of Kubeflow pipeline as a naive approach
[02:36:12.760 --> 02:36:17.860]   while TFX is a high-level wrapper of the Kubeflow pipeline.
[02:36:17.860 --> 02:36:20.860]   Since it wraps up the underlying complexities,
[02:36:20.860 --> 02:36:24.060]   it also comes with many standard components
[02:36:24.060 --> 02:36:27.260]   where we can build powerful custom components easily.
[02:36:27.260 --> 02:36:30.760]   But how does it really work?
[02:36:30.760 --> 02:36:33.460]   Pipeline consists of a bunch of components
[02:36:33.460 --> 02:36:37.860]   from data ingestion, data pre-processing, model training,
[02:36:37.860 --> 02:36:40.760]   to publishing the trained or blessed model.
[02:36:40.760 --> 02:36:43.660]   The most important is not how they work,
[02:36:43.660 --> 02:36:48.160]   but how they scale with different types of hardware resources
[02:36:48.160 --> 02:36:52.160]   because they all have different kind of requirements.
[02:36:52.160 --> 02:36:56.160]   For example, you want cheap and lightweight hardware resources
[02:36:56.160 --> 02:36:59.360]   for regular tasks such as data ingestion,
[02:36:59.360 --> 02:37:01.960]   or you want high-performance CPU
[02:37:01.960 --> 02:37:05.660]   and high RAM capacity for data pre-processing,
[02:37:05.660 --> 02:37:10.360]   or you probably need multiple GPUs, TPUs for model training,
[02:37:10.360 --> 02:37:14.360]   or even model prediction if your model is big enough.
[02:37:14.360 --> 02:37:17.860]   Google Kubernetes Engine is the underlying technology
[02:37:17.860 --> 02:37:19.260]   for enabling this.
[02:37:19.260 --> 02:37:21.960]   You have different types of node pools
[02:37:21.960 --> 02:37:23.760]   and based on the request,
[02:37:23.760 --> 02:37:29.060]   it allocates appropriate one to each job or component.
[02:37:29.060 --> 02:37:32.260]   The problem with the GKE is it doesn't know
[02:37:32.260 --> 02:37:35.860]   how to process jobs with dependencies in between.
[02:37:35.860 --> 02:37:39.560]   For example, data ingestion has to be finished
[02:37:39.560 --> 02:37:42.460]   before the data pre-processing and so on.
[02:37:42.460 --> 02:37:45.860]   You can write up such dependencies all by yourself,
[02:37:45.860 --> 02:37:48.460]   but it is non-trivial to write such codes
[02:37:48.460 --> 02:37:51.760]   in manageable and generalizable ways.
[02:37:51.760 --> 02:37:53.860]   Instead, you can leverage Kubeflow
[02:37:53.860 --> 02:37:56.660]   with the Kubeflow Pipeline or TFX.
[02:37:56.660 --> 02:37:58.960]   You just write up codes for each component
[02:37:58.960 --> 02:38:00.860]   in an isolated manner.
[02:38:00.860 --> 02:38:03.560]   Then you can easily hook up each component
[02:38:03.560 --> 02:38:08.760]   to ensure certain job has to be run beforehand or afterward.
[02:38:08.760 --> 02:38:11.960]   Also, it makes sure each job is containerized
[02:38:11.960 --> 02:38:15.760]   so that GKE can handle it.
[02:38:15.760 --> 02:38:18.260]   As you see, you pass a set of job specs
[02:38:18.260 --> 02:38:20.960]   for TFX in this example.
[02:38:20.960 --> 02:38:23.960]   Then it ensures that data ingestion happens first
[02:38:23.960 --> 02:38:26.260]   and model serving is handled at last.
[02:38:26.260 --> 02:38:29.560]   Nice thing about Google Kubernetes Engine
[02:38:29.560 --> 02:38:31.860]   is that you can define a node pool
[02:38:31.860 --> 02:38:36.060]   and you can allocate more or less resources dynamically.
[02:38:36.060 --> 02:38:38.360]   That means it runs in a scalable manner
[02:38:38.360 --> 02:38:41.460]   and it lets the runs from different pipelines
[02:38:41.460 --> 02:38:45.160]   or runs from the same pipeline launch parallel.
[02:38:45.160 --> 02:38:47.660]   For instance, you might want to run a number
[02:38:47.660 --> 02:38:49.860]   of experiments in parallel
[02:38:49.860 --> 02:38:52.460]   or colleagues from your organization
[02:38:52.460 --> 02:38:56.260]   can run their own experiments in parallel.
[02:38:56.260 --> 02:38:57.660]   But that's not the end.
[02:38:57.660 --> 02:39:01.460]   You can delegate certain jobs to dedicated GCP services.
[02:39:01.460 --> 02:39:04.160]   For instance, data transform, preprocessing
[02:39:04.160 --> 02:39:07.560]   can be done with multiple nodes by collaborating together.
[02:39:07.560 --> 02:39:10.660]   Also, model training can be done in a similar way.
[02:39:10.660 --> 02:39:15.260]   Since GKE nodes doesn't know how to collaborate with,
[02:39:15.260 --> 02:39:17.460]   you need dedicated set of servers
[02:39:17.460 --> 02:39:18.760]   with special settings.
[02:39:18.760 --> 02:39:22.160]   Of course, Kubeflow knows how to do this,
[02:39:22.160 --> 02:39:26.660]   but GCP comes with more optimal services for this purpose.
[02:39:26.660 --> 02:39:31.360]   Luckily, GCP provides such dedicated services.
[02:39:31.360 --> 02:39:33.260]   Dataflow for data handling,
[02:39:33.260 --> 02:39:35.560]   Vertex AI training for model training,
[02:39:35.560 --> 02:39:38.660]   Vertex AI tuner for hyperparameter search,
[02:39:38.660 --> 02:39:41.860]   and Vertex AI prediction for model serving.
[02:39:41.860 --> 02:39:44.360]   Also, you can disaggregate the data sources
[02:39:44.360 --> 02:39:46.460]   with a fully managed services
[02:39:46.460 --> 02:39:49.460]   like Google Cloud Storage or BigQuery.
[02:39:49.460 --> 02:39:52.460]   TFX lets you ingest data from those services
[02:39:52.460 --> 02:39:55.260]   directly into the pipeline.
[02:39:55.260 --> 02:39:59.360]   Okay, you have a full E2E, end-to-end machine learning pipeline,
[02:39:59.360 --> 02:40:02.360]   but how do you run it based on the two triggering events
[02:40:02.360 --> 02:40:03.960]   that we talked about?
[02:40:03.960 --> 02:40:07.560]   Cloud Function has a built-in feature to run Python codes
[02:40:07.560 --> 02:40:11.460]   by detecting events of certain data sources has been changed.
[02:40:11.460 --> 02:40:15.260]   GitHub Action and Cloud Build can run unit tests
[02:40:15.260 --> 02:40:20.760]   and integrate new code bases into the currently deployed one.
[02:40:20.760 --> 02:40:22.960]   There are even more takeaways.
[02:40:22.960 --> 02:40:28.160]   For example, GCS comes with data version controls feature
[02:40:28.160 --> 02:40:33.260]   and Vertex AI is able to handle model version control in nature.
[02:40:33.260 --> 02:40:38.060]   So you can go through some of the other benefits
[02:40:38.060 --> 02:40:42.360]   of the Google Cloud Platform.
[02:40:42.360 --> 02:40:46.460]   And these are very nice features that we can get out of the box
[02:40:46.460 --> 02:40:48.360]   from Google Cloud Platform.
[02:40:48.360 --> 02:40:55.960]   However, many cloud users complain about the expenditure on the cloud.
[02:40:55.960 --> 02:41:01.060]   What would be the best practice to minimize then?
[02:41:01.060 --> 02:41:03.560]   You can make a plan moving from the core lab,
[02:41:03.560 --> 02:41:08.060]   which is the simplest solution to local machine to Vertex AI on the cloud.
[02:41:08.060 --> 02:41:11.860]   In this case, you can ensure at least the unit test of your code base
[02:41:11.860 --> 02:41:14.660]   has no error at the initial phase.
[02:41:14.660 --> 02:41:16.860]   When you put everything on the cloud,
[02:41:16.860 --> 02:41:20.660]   then the only left part is debugging the adjustment
[02:41:20.660 --> 02:41:23.360]   between local and cloud environment.
[02:41:23.360 --> 02:41:28.360]   TFX can support three scenarios by providing different contexts.
[02:41:28.360 --> 02:41:33.560]   In a notebook context, you can run each TFX component cell by cell.
[02:41:33.560 --> 02:41:37.460]   And when you're ready, you can easily export the cells from the notebook
[02:41:37.460 --> 02:41:39.160]   to an external file.
[02:41:39.160 --> 02:41:43.360]   Then you can use local runner to test out if everything works
[02:41:43.360 --> 02:41:46.160]   as a standalone Python application.
[02:41:46.160 --> 02:41:50.460]   Then you can use a Vertex engine with a Kubeflow V2 runner
[02:41:50.460 --> 02:41:53.660]   for the cloud environment.
[02:41:53.660 --> 02:41:55.460]   For the debugging of the cloud environment,
[02:41:55.460 --> 02:42:02.360]   you can always use free GCP credit, which is $300 that you can always get.
[02:42:02.360 --> 02:42:07.660]   As you'll see, every example that I will introduce you today
[02:42:07.660 --> 02:42:12.160]   has been done only with a free GCP credit.
[02:42:12.160 --> 02:42:14.760]   Let's see some example pipelines then.
[02:42:14.760 --> 02:42:19.360]   For the number one, I have built a simple Kubeflow pipeline example
[02:42:19.360 --> 02:42:21.360]   with auto ML capability.
[02:42:21.360 --> 02:42:23.360]   So let's see what it is.
[02:42:23.360 --> 02:42:27.160]   I have brainstormed an idea to show a bit of ML ops.
[02:42:27.160 --> 02:42:29.960]   And I have found that it would be great to show
[02:42:29.960 --> 02:42:33.860]   how the model can adapt to the changes my daughter's face.
[02:42:33.860 --> 02:42:37.760]   One good reason was I have shoot a lot of photos of her,
[02:42:37.760 --> 02:42:39.360]   just like other parents.
[02:42:39.360 --> 02:42:44.960]   In Vertex AI, I have been able to import about 100 images of her
[02:42:44.960 --> 02:42:48.960]   and label them right in the browser.
[02:42:48.960 --> 02:42:51.860]   Then I have built a simple Kubeflow pipeline.
[02:42:51.860 --> 02:42:54.660]   What it does is to import the mesh data set,
[02:42:54.660 --> 02:42:58.060]   then hand it over to the auto ML image training job
[02:42:58.060 --> 02:43:01.260]   for auto ML object detection testing,
[02:43:01.260 --> 02:43:05.060]   then deploy the trained model on the cloud.
[02:43:05.060 --> 02:43:09.660]   From the initial run, I have encountered two unexpected results.
[02:43:09.660 --> 02:43:14.360]   First of all, the trained model could catch a three-year-old face,
[02:43:14.360 --> 02:43:19.660]   even though the model was trained with a one to two-year-old face.
[02:43:19.660 --> 02:43:23.060]   The second unexpected result was due to the COVID-19.
[02:43:23.060 --> 02:43:29.160]   As you know, these days, everyone, including children, always wears a mask.
[02:43:29.160 --> 02:43:33.360]   In this case, the trained model could not find her at all.
[02:43:33.360 --> 02:43:35.860]   So I have built a simple workflow.
[02:43:35.860 --> 02:43:40.660]   This slide shows what I've explained so far for the initial pipeline run.
[02:43:40.660 --> 02:43:42.860]   First, collect and label data set.
[02:43:42.860 --> 02:43:45.260]   You provide a list of data,
[02:43:45.260 --> 02:43:50.860]   then you can label them using web-based labeling tool in Vertex AI.
[02:43:50.860 --> 02:43:54.760]   After labeling, you can export the data spec in JSONL format
[02:43:54.760 --> 02:43:57.660]   to the specific GCS location.
[02:43:57.660 --> 02:44:03.360]   Second, in Jupyter Notebook, which is run as an instance of Vertex AI Notebook,
[02:44:03.360 --> 02:44:07.760]   you write an experiment with some codes to build a pipeline.
[02:44:07.760 --> 02:44:10.960]   Then you can generate JSON spec for the pipeline
[02:44:10.960 --> 02:44:14.360]   to instantiate the same pipeline whenever needed.
[02:44:14.360 --> 02:44:19.160]   And run the pipeline job in the Vertex AI by referring the JSON spec.
[02:44:19.160 --> 02:44:22.660]   Also, I have trig integrated the cloud function,
[02:44:22.660 --> 02:44:29.460]   which detects any changes of the data spec, JSONL from GCS.
[02:44:29.460 --> 02:44:36.660]   Then relaunch the pipeline run by referring to the pipeline JSON spec file.
[02:44:36.660 --> 02:44:41.160]   So that was the initial setup, and this slide shows the operational flow.
[02:44:41.160 --> 02:44:46.460]   You collect more data and label them on the web browser using Vertex AI.
[02:44:46.460 --> 02:44:53.960]   When you're done labeling, you can export JSONL data spec file to the specific GCS location.
[02:44:53.960 --> 02:44:58.760]   Remember, you got the same name of JSONL file at the initial phase,
[02:44:58.760 --> 02:45:01.860]   so you're updating it with more of new data.
[02:45:01.860 --> 02:45:08.260]   And that is it. Cloud Function notices there is a change under the specific GCS bucket
[02:45:08.260 --> 02:45:13.760]   that it launches the pipeline by referring the JSON pipeline spec file.
[02:45:13.760 --> 02:45:19.360]   When the pipelines run, it'll look up the JSONL data spec to create a new managed data set,
[02:45:19.360 --> 02:45:24.060]   and AutoML for object detection learns the new data with a new data set.
[02:45:24.060 --> 02:45:26.860]   A newly trained model will be deployed.
[02:45:26.860 --> 02:45:33.860]   This is a very simple yet powerful example because it leverages the power of AutoML.
[02:45:33.860 --> 02:45:36.760]   There are a couple of nice things about AutoML.
[02:45:36.760 --> 02:45:41.360]   The internal AutoML algorithms are maintained by Google.
[02:45:41.360 --> 02:45:45.660]   That means you're always ensured to use the best AutoML algorithm.
[02:45:45.660 --> 02:45:49.260]   You don't even notice or have to use different APIs.
[02:45:49.260 --> 02:45:59.660]   Everything can stay, you know, as is, but you will get the best trained model based on your budget.
[02:45:59.660 --> 02:46:03.860]   This slide shows the minimum amount of code which has to be written for Cloud Function.
[02:46:03.860 --> 02:46:07.360]   The Cloud Function works based on a very simple code indeed.
[02:46:07.360 --> 02:46:15.860]   As mentioned, it detects any changes happening in the specific GCS bucket that it filters out a file whose extension is JSONL.
[02:46:15.860 --> 02:46:24.760]   Then it launches the pipeline by run, run by calling create run from job spec method of AI platform client library,
[02:46:24.760 --> 02:46:33.460]   which is essentially a wrapper SDK for gRPC protocol buffer specs for Vertex AI.
[02:46:33.460 --> 02:46:36.660]   Vertex AI also comes with built-in monitoring features.
[02:46:36.660 --> 02:46:41.960]   As you can see from this slide, you can monitor how many predictions are done per second,
[02:46:41.960 --> 02:46:52.160]   prediction error percentage caused by failures, how many requests were there per second, and prediction latency.
[02:46:52.160 --> 02:46:59.760]   So with this, I have added more of our face, but with mass querying once at this time and exported the JSONL data spec.
[02:46:59.760 --> 02:47:03.360]   Then Cloud Function took care of the rest.
[02:47:03.360 --> 02:47:11.860]   And as a result, the newly trained model could detect not only every single of our face, but also mass querying once.
[02:47:11.860 --> 02:47:14.360]   Nice, right?
[02:47:14.360 --> 02:47:18.960]   That was an interesting example, and it was too simple to be realistic too.
[02:47:18.960 --> 02:47:25.260]   So I have expanded the first example further. Let's see what changes I have made based on the first one.
[02:47:25.260 --> 02:47:31.760]   The second example shows how to overcome some shortcomings from the first example.
[02:47:31.760 --> 02:47:39.560]   First of all, people sometimes want to add dataset label without Vertex AI dataset labeling toolbox.
[02:47:39.560 --> 02:47:47.060]   For instance, you may want to label with AI or with other open source labeling projects.
[02:47:47.060 --> 02:47:56.160]   Also, you can manage the dataset all by yourself, which means you can organize GCS bucket tree in a more meaningful way.
[02:47:56.160 --> 02:48:01.160]   Second, the first example created new Vertex AI dataset every time.
[02:48:01.160 --> 02:48:10.160]   It would be great to add more data into the existing Vertex AI dataset instance because it is more modular and cost effective.
[02:48:10.160 --> 02:48:15.460]   Third, the first example trained AutoML model from scratch all the time.
[02:48:15.460 --> 02:48:21.360]   However, it would be nice to train a new model based on the previously trained one.
[02:48:21.360 --> 02:48:27.460]   And this way, we can likely get a much better model than the one from scratch.
[02:48:27.460 --> 02:48:30.760]   And maybe it is more cost effective as well.
[02:48:30.760 --> 02:48:39.060]   This pipeline graph clearly shows that it has two main branches, one that is triggered if the dataset is new,
[02:48:39.060 --> 02:48:43.760]   and the other that is triggered if the dataset already exists.
[02:48:43.760 --> 02:48:53.060]   Also note that from the second branch, it checks if there is a previously trained model to determine to get a new model based on it or not.
[02:48:53.060 --> 02:49:03.960]   One important thing to notice here when you are using Vertex AI pipeline is that it comes with a predefined artifacts such as a vertex dataset, vertex model, etc.
[02:49:03.960 --> 02:49:15.760]   In order to leverage those artifacts so that Vertex AI can notice them, you have to add some appropriate properties in the returning artifacts like,
[02:49:15.760 --> 02:49:24.060]   as you can see from the bottom right side, I have set resource name in the metadata dictionary to special string format.
[02:49:24.060 --> 02:49:30.160]   And on the left side of the slide, you can get the idea how the flow is controlled by conditions.
[02:49:30.160 --> 02:49:40.260]   Basically, all you need to do is put condition statement in the kfb.dsl package.
[02:49:40.260 --> 02:49:44.460]   The next example is something called dual deployment.
[02:49:44.460 --> 02:49:51.660]   The idea of this project is borrowed from a popular machine learning book, Machine Learning Design Patterns.
[02:49:51.660 --> 02:49:56.460]   In this book, we can see lots of very common patterns in real world application.
[02:49:56.460 --> 02:50:06.960]   And this project is the showcase of something called a layered prediction and on and off line predictions pattern.
[02:50:06.960 --> 02:50:12.960]   In the layered prediction scenario, we train two models from the same dataset for different tasks,
[02:50:12.960 --> 02:50:18.160]   then deploy one of them in the cloud and embed the other one on device.
[02:50:18.160 --> 02:50:25.460]   Since cloud model can leverage much powerful computing resources, it could do more complex tasks,
[02:50:25.460 --> 02:50:28.460]   while on device model is somewhat limited.
[02:50:28.460 --> 02:50:32.160]   So it takes care of relatively simple tasks.
[02:50:32.160 --> 02:50:43.760]   For instance, the on device model can classify this sound recording in a broad manner, like if it belongs to music or voice, etc.
[02:50:43.760 --> 02:50:48.960]   Then the on device model sends a prediction, the results to the cloud model.
[02:50:48.960 --> 02:50:55.860]   Cloud model makes more specific predictions based on the results from the on device model.
[02:50:55.860 --> 02:51:04.160]   For instance, it tries to classify which instruments is playing if the sound is classified as music.
[02:51:04.160 --> 02:51:11.160]   In the on and off prediction scenario, we train two models from the same dataset for the same task,
[02:51:11.160 --> 02:51:15.660]   then deploy one of them in the cloud and embed the other one on device.
[02:51:15.660 --> 02:51:20.560]   For instance, we want to classify images as accurately as possible.
[02:51:20.560 --> 02:51:27.060]   Then the cloud model should do the job since it can access better and more powerful resources.
[02:51:27.060 --> 02:51:29.760]   Plus, we can have bigger model.
[02:51:29.760 --> 02:51:35.960]   However, the thing is your mobile device or IoT device can go offline all the time.
[02:51:35.960 --> 02:51:40.460]   In this case, you still want to make predictions as much as possible.
[02:51:40.460 --> 02:51:47.360]   So we can leverage the on device model, even though it is not guaranteed to get the best results yet.
[02:51:47.360 --> 02:51:56.260]   To make those more realistic, this project leveraged the Firebase ML service to host on device models.
[02:51:56.260 --> 02:52:03.360]   Firebase ML is more practical way to embed model on device than just exporting the tf.light model.
[02:52:03.360 --> 02:52:11.660]   Because the tf.light model file, you have to compile your app every time whenever you need to upgrade the model.
[02:52:11.660 --> 02:52:23.760]   But with Firebase ML, your model can be replaced without actually compiling and redeploying the app once your app is deployed.
[02:52:23.760 --> 02:52:32.660]   I didn't put much details with the actual code base for this project since it is really not a big deal to implement it by yourself.
[02:52:32.660 --> 02:52:37.960]   However, I just want to point out about the extra feature in Kubeflow pipeline.
[02:52:37.960 --> 02:52:43.160]   That is kfb.dsl.parallel_for.
[02:52:43.160 --> 02:52:46.760]   This lets you run multiple jobs in parallel.
[02:52:46.760 --> 02:52:53.460]   So with this feature, you can run multiple experiments or multiple training jobs for different deployment environments,
[02:52:53.460 --> 02:52:57.260]   just like dual deployment project.
[02:52:57.260 --> 02:52:59.060]   Let's see the fourth example.
[02:52:59.060 --> 02:53:02.960]   It's called model training as a CI/CD system with a TFX.
[02:53:02.960 --> 02:53:08.860]   And it basically says machine learning system evolves when the new code is available.
[02:53:08.860 --> 02:53:17.760]   This and the fifth examples are somewhat more complicated, and it doesn't use AutoML, but TFX with custom model.
[02:53:17.760 --> 02:53:21.460]   So why we need such a system anyways?
[02:53:21.460 --> 02:53:24.760]   This basically borrows a concept from DevOps world.
[02:53:24.760 --> 02:53:29.360]   There are multi parts in machine learning pipeline that could be changed over time.
[02:53:29.360 --> 02:53:33.760]   For instance, you might want to apply new feature engineering techniques,
[02:53:33.760 --> 02:53:39.060]   or you may want to test out newly emerged state of the art models from academia.
[02:53:39.060 --> 02:53:44.260]   In both situations, you have to write bits of code to reflect them.
[02:53:44.260 --> 02:53:46.460]   CI/CD starts from this point.
[02:53:46.460 --> 02:53:50.360]   It always listens to any changes in code base.
[02:53:50.360 --> 02:53:53.060]   Simply, this can be done with GitHub action.
[02:53:53.060 --> 02:53:59.560]   If you are a GitHub user, then it runs a set of unit tests and build custom TFX Docker image.
[02:53:59.560 --> 02:54:04.860]   If the tests are all passed, you can do this in GitHub action as well.
[02:54:04.860 --> 02:54:10.260]   But Cloud Builds is another option, which is more flexible, manageable, powerful.
[02:54:10.260 --> 02:54:13.260]   And that's how it's done for this project.
[02:54:13.260 --> 02:54:18.460]   When the Docker image is built, then it is pushed to Google Cloud Registry.
[02:54:18.460 --> 02:54:23.660]   The TFX pipeline will get compiled and it'll generate a JSON spec file,
[02:54:23.660 --> 02:54:27.860]   which every information is packed to run the pipeline.
[02:54:27.860 --> 02:54:32.860]   Each component of the pipeline is run based on the given TFX Docker image.
[02:54:32.860 --> 02:54:41.760]   So when compiling the pipeline, it makes sure every components to be run on the latest code base.
[02:54:41.760 --> 02:54:46.460]   While doing this project, me and my collaborator have discovered
[02:54:46.460 --> 02:54:50.760]   that TFX provides TFX CLI, which is a very nice toolbox.
[02:54:50.760 --> 02:54:58.060]   Here's why. With TFX CLI, you can create a complete template project for machine learning pipeline out of the box,
[02:54:58.060 --> 02:55:01.060]   which is structured from example gen to pusher.
[02:55:01.060 --> 02:55:08.160]   For your information, example gen is a component to inject data into the pipeline,
[02:55:08.160 --> 02:55:14.960]   and the pusher is a component to push the model, plus possibly deploy the model as well.
[02:55:14.960 --> 02:55:19.060]   And you can even run the pipeline with a single run of CLI.
[02:55:19.060 --> 02:55:26.660]   Also, you can experiment a variety of tests of integration with the other GCP services as well,
[02:55:26.660 --> 02:55:32.760]   like Vertex AI training, Vertex AI model, Vertex AI prediction, or even data flow.
[02:55:32.760 --> 02:55:41.460]   These settings can be adjusted by only changing the configuration file already provided when creating a template project.
[02:55:41.460 --> 02:55:47.660]   You don't even need to touch the codes. Because you can get the end-to-end pipeline instantly,
[02:55:47.660 --> 02:55:51.660]   you can focus on MLOps perspective as much as possible.
[02:55:51.660 --> 02:55:56.760]   You may want to experiment with many MLOps scenarios at the high level view,
[02:55:56.760 --> 02:56:03.660]   but if you don't have end-to-end pipeline, there are too many moving parts to consider or even implement.
[02:56:03.660 --> 02:56:08.060]   With TFX template project created by TFX CLI,
[02:56:08.060 --> 02:56:15.260]   you can get your hands dirty by creating lots of different MLOps scenarios before you get knocked out
[02:56:15.260 --> 02:56:23.260]   by just setting up a whole bunch of open source projects for different parts of MLOps.
[02:56:23.260 --> 02:56:32.460]   This slide shows TFX CLI 101. With TFX template copy CLI, you can create an end-to-end TFX pipeline project.
[02:56:32.460 --> 02:56:37.160]   There are two models currently available, and it is specified in model option.
[02:56:37.160 --> 02:56:43.860]   I have created a template project with text model, and as you see, it creates everything we need.
[02:56:43.860 --> 02:56:49.360]   With TFX pipeline create CLI, you can compile the pipeline.
[02:56:49.360 --> 02:56:56.860]   It generates a JSON pipeline spec file, and you can even provide a Docker file to build a custom Docker image.
[02:56:56.860 --> 02:57:06.360]   Finally, TFX run create CLI lets you run the pipeline with the value of vertex in engine option.
[02:57:06.360 --> 02:57:10.260]   The pipeline will run on the Google Cloud platform.
[02:57:10.260 --> 02:57:17.660]   Also, it is good to notice that we have to provide project ID, original information as in the options.
[02:57:17.660 --> 02:57:23.460]   This basically says vertex AI pipeline is a serverless platform.
[02:57:23.460 --> 02:57:29.060]   If it's not a serverless, you have to create a Kubernetes cluster all by yourself,
[02:57:29.060 --> 02:57:35.460]   and you have to pay even if it is being used or not.
[02:57:35.460 --> 02:57:43.260]   The entire flow that I introduced from unit testing to triggering the pipeline is written in the Cloud Build script.
[02:57:43.260 --> 02:57:52.560]   As you can see, I did nothing special, but just used TFX CLI to build image, run the pipeline, etc.
[02:57:52.560 --> 02:58:01.560]   It would be better if we avoid Docker image building as much as possible because it takes unexpectedly longer time,
[02:58:01.560 --> 02:58:08.060]   and we usually make changes in code base for data pre-processing modeling only.
[02:58:08.060 --> 02:58:11.560]   It is uncommon to change the other parts frequently.
[02:58:11.560 --> 02:58:18.060]   So we need to decouple the data pre-processing modeling parts in modular way,
[02:58:18.060 --> 02:58:21.960]   and isolate unit test of the decoupled modules,
[02:58:21.960 --> 02:58:31.260]   and find a way to refer to decoupled modules in transform and trainer component without building a new Docker image.
[02:58:31.260 --> 02:58:37.760]   Unfortunately, trainer and transform components have special parameter called module file.
[02:58:37.760 --> 02:58:42.460]   It lets the data pre-processing modeling jobs run on the fly.
[02:58:42.460 --> 02:58:46.960]   We just need to specify the location where the scripts are for them.
[02:58:46.960 --> 02:58:51.560]   And luckily, we can put Python scripts in Google Cloud Storage as well.
[02:58:51.560 --> 02:58:56.460]   There's no special extra job to make it work with Google Cloud Storage,
[02:58:56.460 --> 02:59:02.960]   but just to write down the location in the module file parameter.
[02:59:02.960 --> 02:59:06.060]   The Cloud Build script is very similar to the previous one,
[02:59:06.060 --> 02:59:14.560]   but at this time, we only unit test the data pre-processing and modeling modules that run the pipeline.
[02:59:14.560 --> 02:59:23.460]   The final example project is something called continuous adaptation for machine learning system to data changes with TFX.
[02:59:23.460 --> 02:59:27.660]   So this is the counterpart of the previous project.
[02:59:27.660 --> 02:59:37.160]   Do you remember that I said the ML Ops should make sure your model is up to date by adapting to the changes in both of code base and data?
[02:59:37.160 --> 02:59:43.060]   The previous project showed the first case, while this project shows the latter case.
[02:59:43.060 --> 02:59:46.660]   So let's see what it really looks like.
[02:59:46.660 --> 02:59:50.960]   I have basically built two machine learning pipelines collaborating back and forth.
[02:59:50.960 --> 02:59:56.360]   The first pipeline basically does what we have seen so far, model training.
[02:59:56.360 --> 03:00:02.160]   The only difference is it trains a new model based on a selected range of data set.
[03:00:02.160 --> 03:00:06.860]   The second pipeline is a batch prediction pipeline.
[03:00:06.860 --> 03:00:15.360]   It evaluates the currently deployed model from the first pipeline with newly collected or sampled data.
[03:00:15.360 --> 03:00:28.760]   Then if the performance of the current model is not good enough, it pushes the new data into different directory by giving a special name like span one or span two, etc.
[03:00:28.760 --> 03:00:34.260]   Finally, it triggers the first pipeline to run based on the pushed data set.
[03:00:34.260 --> 03:00:47.960]   Or you can specify the range of data set if you want a new model based on the new data set as well as the previous data set or something.
[03:00:47.960 --> 03:00:52.160]   Let's see the batch prediction pipeline in a bit more detail.
[03:00:52.160 --> 03:00:56.360]   There is a cloud scheduler, which is basically managed the cron job.
[03:00:56.360 --> 03:01:01.060]   It periodically checks if there is enough number of new data.
[03:01:01.060 --> 03:01:04.360]   Let's say maybe thousands of new images.
[03:01:04.360 --> 03:01:10.160]   Actually, the actual logic for the cloud scheduler is implemented in a cloud function.
[03:01:10.160 --> 03:01:24.660]   So if the cloud function determines that there is enough number of data, it triggers the batch prediction pipeline to evaluate the newly collected data.
[03:01:24.660 --> 03:01:34.560]   Inside the batch prediction pipeline, first, it performs batch predictions on the newly collected data set by calling the currently deployed models endpoint.
[03:01:34.560 --> 03:01:42.860]   Second, it evaluates the batch prediction results by checking if the metric is above or below the threshold.
[03:01:42.860 --> 03:01:55.560]   Third, if it thinks the model should be retrained, it pushes the newly collected data set into the GCS location where the model training pipeline looks for the trained data.
[03:01:55.560 --> 03:01:58.560]   Fourth, it triggers the model training pipeline.
[03:01:58.560 --> 03:02:07.860]   Especially, it passes which data to look up as in runtime parameter when triggering the model training pipeline.
[03:02:07.860 --> 03:02:19.860]   For this set of sequences, I have built custom TFX components of FileListGen, BatchPredictionGen, PerformanceEvaluator, SpamPreparator, PipelineTrigger.
[03:02:19.860 --> 03:02:27.860]   The names are straightforward, so I won't go over explaining what they do once again.
[03:02:27.860 --> 03:02:32.260]   One nice feature of the TFX to notice is the resolver.
[03:02:32.260 --> 03:02:43.460]   Actually, whatever you do in a machine learning pipeline, everything is recorded in terms of artifacts, and those artifacts are reusable.
[03:02:43.460 --> 03:02:50.060]   So as you can see from this slide, whenever you process some data, you can reuse it afterwards.
[03:02:50.060 --> 03:02:57.460]   And at this time, it doesn't have to be processed once again, but just referring as needed.
[03:02:57.460 --> 03:02:59.960]   This feature is something called rolling windows.
[03:02:59.960 --> 03:03:06.760]   This slide conceptually shows how this project works in the perspective of the changes of data set.
[03:03:06.760 --> 03:03:10.760]   We have initial data set, which is stored under span number one.
[03:03:10.760 --> 03:03:15.760]   Then we have more data set with the same distribution from the span number one.
[03:03:15.760 --> 03:03:22.760]   So in this case, we can add more data set under the span number one, but it's not considered as data drift.
[03:03:22.760 --> 03:03:27.760]   Then we have more data set with the out of distribution comparing to the span number one.
[03:03:27.760 --> 03:03:31.760]   In this case, these additional data set goes into span number two.
[03:03:31.760 --> 03:03:34.760]   Then the model gets updated accordingly.
[03:03:34.760 --> 03:03:39.760]   One thing to notice is artifacts are reusable, as I mentioned.
[03:03:39.760 --> 03:03:47.760]   So if we see this trend goes backward, then we can just simply pull out the past data set or model.
[03:03:47.760 --> 03:03:52.760]   This is a rollback in machine learning world.
[03:03:52.760 --> 03:03:56.760]   So that's it for my session.
[03:03:56.760 --> 03:04:05.760]   We have briefly gone through the basics of MLOps, and I have introduced some exciting or boring examples afterwards.
[03:04:05.760 --> 03:04:11.760]   If you have any questions about my presentations, please contact via my Twitter account.
[03:04:11.760 --> 03:04:15.760]   I have two Twitter accounts, but you can contact any one of those.
[03:04:15.760 --> 03:04:19.760]   Thanks for having me in this wonderful event.
[03:04:19.760 --> 03:04:20.760]   Thank you.
[03:04:20.760 --> 03:04:46.760]   [silence]
[03:04:46.760 --> 03:04:56.760]   [speaking in Korean]
[03:04:56.760 --> 03:05:19.760]   [speaking in Korean]
[03:05:19.760 --> 03:05:47.760]   [speaking in Korean]
[03:05:47.760 --> 03:05:52.760]   So for taking picture, please turn your camera, please.
[03:05:52.760 --> 03:05:59.760]   [speaking in Korean]
[03:05:59.760 --> 03:06:06.760]   After 30 minutes, we'll take a picture for memory this time.
[03:06:06.760 --> 03:06:08.760]   So thank you for waiting.
[03:06:08.760 --> 03:06:21.760]   [silence]
[03:06:21.760 --> 03:06:24.760]   [speaking in Korean]
[03:06:24.760 --> 03:06:26.760]   [speaking in Korean]
[03:06:26.760 --> 03:06:34.760]   [silence]
[03:06:34.760 --> 03:06:38.760]   [speaking in Korean]
[03:06:38.760 --> 03:06:43.760]   [silence]
[03:06:43.760 --> 03:06:50.760]   [speaking in Korean]
[03:06:50.760 --> 03:06:57.760]   [speaking in Korean]
[03:06:57.760 --> 03:07:04.760]   [silence]
[03:07:04.760 --> 03:07:06.760]   [speaking in Korean]
[03:07:06.760 --> 03:07:07.760]   [laughter]
[03:07:07.760 --> 03:07:09.760]   Thank you for...
[03:07:09.760 --> 03:07:12.760]   [silence]
[03:07:12.760 --> 03:07:14.760]   [speaking in Korean]
[03:07:14.760 --> 03:07:16.760]   [silence]
[03:07:16.760 --> 03:07:23.760]   [speaking in Korean]
[03:07:23.760 --> 03:07:26.760]   [speaking in Korean]
[03:07:26.760 --> 03:07:32.280]   Yes. Uh, so now I'm gonna send out the competition.
[03:07:32.280 --> 03:07:37.560]   Uh, firstly, what kind of competition have we prepared?
[03:07:37.560 --> 03:07:45.480]   It's a competition that you can take a look at Kaggle and 1 DB tracking.
[03:07:45.480 --> 03:07:49.500]   Uh, it's a competition, classification...
[03:07:49.500 --> 03:07:51.500]   We prepared this competition.
[03:07:51.500 --> 03:07:54.360]   If you enter here in top 10,
[03:07:54.360 --> 03:07:57.960]   We give you a gift at Wayden Bycess.
[03:07:57.960 --> 03:08:02.360]   Uh, I hope you can participate in a comfortable and fun way.
[03:08:02.360 --> 03:08:07.800]   Yes, before we start the competition,
[03:08:07.800 --> 03:08:11.080]   Uh, now, the first thing you announced at our seminar,
[03:08:11.080 --> 03:08:17.240]   Uh, 1 DB's Morgan will give you a brief guide on how to participate in the competition.
[03:08:17.240 --> 03:08:22.120]   And then, uh, if you share our call app,
[03:08:22.120 --> 03:08:27.000]   You can participate in the competition for 40 minutes.
[03:08:27.000 --> 03:08:30.040]   And while the competition is going on,
[03:08:30.040 --> 03:08:35.000]   Uh, 1 DB wants to hire a Korean big learning engineer.
[03:08:35.000 --> 03:08:37.000]   There's also a seminar.
[03:08:37.000 --> 03:08:43.880]   So, uh, if you have any questions about employment at 1 DB,
[03:08:43.880 --> 03:08:49.400]   Uh, we're going to have a Q&A time while we're in the competition.
[03:08:49.400 --> 03:08:52.120]   Uh, please ask a lot of questions in the YouTube comments.
[03:08:52.120 --> 03:08:57.640]   Then I'll read it for you and answer it.
[03:08:57.640 --> 03:09:01.560]   You can ask questions in English or in Korean.
[03:09:01.560 --> 03:09:05.720]   I'll translate it and deliver it to Morgan.
[03:09:05.720 --> 03:09:10.440]   And, uh, before the competition starts,
[03:09:10.440 --> 03:09:15.320]   We need a Kaggle account and a 1 DB account.
[03:09:15.320 --> 03:09:20.280]   So, uh, I'd like you to prepare that account now.
[03:09:20.280 --> 03:09:22.040]   If you don't have an account yet,
[03:09:22.040 --> 03:09:28.120]   Uh, I'll explain it in English for a moment and bring it to Morgan.
[03:09:28.120 --> 03:09:31.800]   Uh, I'd appreciate it if you could prepare your account in the meantime.
[03:09:31.800 --> 03:09:33.080]   Yes.
[03:09:33.080 --> 03:09:37.080]   Um, so now I'm going to speak in English.
[03:09:37.080 --> 03:09:39.800]   Hello, everyone. I am Yewon Kang.
[03:09:39.800 --> 03:09:43.160]   I am one of the organizers of the Deep Learning Playground.
[03:09:43.160 --> 03:09:47.240]   Uh, thank you so much for participating in our seminar.
[03:09:47.240 --> 03:09:48.200]   It was quite long.
[03:09:48.200 --> 03:09:54.520]   Um, so, uh, now we're going to have a simple classification competition
[03:09:54.520 --> 03:09:59.800]   to give everyone a taste of Kaggle and how to use, uh, weight and biases tracking.
[03:09:59.800 --> 03:10:05.080]   Um, I'm gonna bring Morgan back and he will guide you about the competition.
[03:10:05.080 --> 03:10:07.080]   So before you join the competition,
[03:10:07.080 --> 03:10:10.920]   you need a Kaggle account and a weight and biases account.
[03:10:10.920 --> 03:10:13.000]   So if you don't have one, please sign up.
[03:10:13.000 --> 03:10:19.880]   Uh, we're also going to have a Q&A session about employment in weight and biases.
[03:10:19.880 --> 03:10:25.800]   So if you have any questions, please, uh, write down on our YouTube live stream
[03:10:25.800 --> 03:10:27.080]   and I'll read it for you.
[03:10:27.080 --> 03:10:30.680]   Um, Morgan, are you ready?
[03:10:30.680 --> 03:10:34.120]   Yes. Yes. Let's, let's do it.
[03:10:34.120 --> 03:10:39.240]   Um, okay. I will share my screen again.
[03:10:39.880 --> 03:10:42.280]   Cool. Competition time.
[03:10:42.280 --> 03:10:47.640]   You want, will I explain a little and then do you want to translate?
[03:10:47.640 --> 03:10:48.760]   Would that be a good way to do it?
[03:10:48.760 --> 03:10:52.120]   Uh, sure. Uh, okay. I will.
[03:10:52.120 --> 03:10:56.120]   Cool. Yeah. I won't say too much.
[03:10:56.120 --> 03:11:01.480]   Um, so we're going to run the competition in Kaggle.
[03:11:01.480 --> 03:11:05.560]   The, it's a community competition in Kaggle.
[03:11:05.560 --> 03:11:06.520]   It's super simple.
[03:11:07.160 --> 03:11:10.360]   Um, all you have to do is submit your predictions here.
[03:11:10.360 --> 03:11:13.640]   There's no data. I'm sorry. Yeah.
[03:11:13.640 --> 03:11:16.920]   I'm sorry. I'm sorry. Uh, I don't know.
[03:11:16.920 --> 03:11:17.420]   Okay.
[03:11:17.420 --> 03:11:33.240]   Super. So the, we'll be using a Colab notebook.
[03:11:33.240 --> 03:11:36.040]   So all of the data gets downloaded in the notebook.
[03:11:36.040 --> 03:11:41.960]   So all you have to do in the competition is to make your predictions or submissions
[03:11:41.960 --> 03:11:43.640]   and, and look at the leaderboard.
[03:12:03.800 --> 03:12:07.720]   So this is the notebook we're going to share.
[03:12:07.720 --> 03:12:12.600]   We'll share a link in one or two minutes when we start the competition.
[03:12:12.600 --> 03:12:16.600]   The data is downloaded here.
[03:12:16.600 --> 03:12:22.360]   There is a simple training loop here.
[03:12:22.360 --> 03:12:30.440]   And there is a, some code to generate the predictions and submission files down here.
[03:12:31.080 --> 03:12:53.080]   [Foreign Language]
[03:12:53.080 --> 03:12:58.280]   Great. And finally, the, the link to the Kaggle competition is also here.
[03:12:58.280 --> 03:13:12.040]   [Foreign Language]
[03:13:12.040 --> 03:13:16.520]   Great. Um, I think we're maybe ready to, to go.
[03:13:16.520 --> 03:13:23.160]   Um, maybe we can share the, uh, link in the, uh, in the YouTube chat.
[03:13:23.160 --> 03:13:28.120]   Um, and I can, I can put up, uh, the link here as well.
[03:13:28.120 --> 03:13:35.080]   Great. So this is the, the link to the, um, Colab.
[03:13:35.080 --> 03:13:39.320]   And, uh, yeah, let's, let's start the competition.
[03:13:39.320 --> 03:13:42.520]   So let's run it for, uh, 45 minutes.
[03:13:42.520 --> 03:13:46.600]   So we finish at 7 PM Korean time.
[03:13:46.600 --> 03:14:16.520]   [Foreign Language]
[03:14:16.520 --> 03:14:34.440]   [Foreign Language]
[03:14:34.440 --> 03:14:38.120]   So I asked them if you have any questions about
[03:14:38.120 --> 03:14:45.880]   weights and biases, uh, employment, they can, uh, uh, they can share their questions and I'll read
[03:14:45.880 --> 03:14:48.520]   to you and you can answer.
[03:14:48.520 --> 03:14:55.560]   Super. That's great. Thanks. And, uh, yeah, if people have any other questions about weights
[03:14:55.560 --> 03:15:00.840]   and biases in, in general, um, we're happy to talk about the product as well.
[03:15:00.840 --> 03:15:06.920]   And obviously if there's any issues with the Colab or making submissions, yeah,
[03:15:06.920 --> 03:15:09.400]   please let us know as well. Hopefully it all works.
[03:15:09.400 --> 03:15:23.720]   Um, uh, there's a little bit of delay between our Zoom and the YouTube live stream.
[03:15:23.720 --> 03:15:28.360]   So, uh, we can just wait a little bit for the question.
[03:15:28.360 --> 03:15:29.820]   Cool.
[03:15:34.280 --> 03:15:37.160]   Uh, okay. Oh, here's one question. Um,
[03:15:37.160 --> 03:15:42.120]   what's the primary reason for hiring engineers in Korea?
[03:15:42.120 --> 03:15:53.560]   So we're just to give some more context about, um, what our team does, um, where they are,
[03:15:53.560 --> 03:16:02.920]   the growth team. And so our role is to, um, grow our user base and also, um, uh, maintain and
[03:16:02.920 --> 03:16:08.040]   improve the engagement, um, of our users. So we want people to sign up and we want people to
[03:16:08.040 --> 03:16:12.440]   come back and, you know, get the value out of weights and biases, um, every day, every week,
[03:16:12.440 --> 03:16:24.680]   every month. Um, and so we have a large user base in Korea, um, but we don't have any Korean
[03:16:24.680 --> 03:16:30.600]   speakers within the company. And so we feel maybe a little disconnected from the community in Korea.
[03:16:30.600 --> 03:16:35.480]   And we would like to, uh, you know, work a little closer with the community there.
[03:16:35.480 --> 03:16:41.960]   Um, and that's for both people, you know, uh, starting now with deep learning, uh, students,
[03:16:41.960 --> 03:16:48.440]   researchers, um, but also people in, um, industry. Um, and so we have some customers in Korea, but
[03:16:48.440 --> 03:16:53.960]   there's so much, um, you know, I may like amazing tech companies coming out of Korea
[03:16:53.960 --> 03:16:58.760]   that we, from a sales point of view, you know, we also want to be able to, uh, engage with,
[03:16:58.760 --> 03:17:02.040]   with, uh, those folks too. Okay. Thank you.
[03:17:02.520 --> 03:17:03.320]   Um,
[03:17:03.320 --> 03:17:05.660]   [SPEAKING KOREAN]
[03:17:05.660 --> 03:17:07.660]   [SPEAKING KOREAN]
[03:17:07.660 --> 03:17:09.660]   [SPEAKING KOREAN]
[03:17:09.660 --> 03:17:11.660]   [SPEAKING KOREAN]
[03:17:11.660 --> 03:17:13.660]   [SPEAKING KOREAN]
[03:17:13.660 --> 03:17:15.660]   [SPEAKING KOREAN]
[03:17:41.660 --> 03:17:47.220]   How many international employees are working in ONDB?
[03:17:47.220 --> 03:17:49.660]   It's a good question.
[03:17:49.660 --> 03:17:53.780]   I would say-- I'd say is it percentage?
[03:17:53.780 --> 03:18:02.060]   If I had to guess, maybe 30%, 40%, maybe 30%.
[03:18:02.060 --> 03:18:05.700]   So my team, the Andreas team, the growth team,
[03:18:05.700 --> 03:18:12.340]   we're probably closer to 60%, 70% international.
[03:18:12.340 --> 03:18:16.180]   So we have a lot of great people in India.
[03:18:16.180 --> 03:18:17.820]   There's a bunch of us in Europe.
[03:18:17.820 --> 03:18:18.980]   I'm in Dublin.
[03:18:18.980 --> 03:18:20.220]   I'm in Ireland.
[03:18:20.220 --> 03:18:25.540]   And then we have folks in California as well.
[03:18:25.540 --> 03:18:28.340]   So the growth team is probably more international
[03:18:28.340 --> 03:18:32.180]   than other teams in the company.
[03:18:32.180 --> 03:18:38.180]   But most teams are probably still 30% international.
[03:18:38.180 --> 03:18:40.460]   Oh, I have one question for that.
[03:18:40.460 --> 03:18:42.980]   Do you have any particular reason
[03:18:42.980 --> 03:18:47.020]   why the growth team has the largest percentage
[03:18:47.020 --> 03:18:50.180]   of the international employees?
[03:18:50.180 --> 03:18:53.020]   Yeah, we were probably one of the first teams
[03:18:53.020 --> 03:19:02.140]   to be comfortable with hiring people remotely.
[03:19:02.140 --> 03:19:06.100]   And so a lot of work around growth
[03:19:06.100 --> 03:19:08.860]   is interacting with different communities across the world.
[03:19:08.860 --> 03:19:12.900]   And so it doesn't really matter where you're based.
[03:19:12.900 --> 03:19:14.420]   A lot of that interaction happens
[03:19:14.420 --> 03:19:17.500]   on Twitter or YouTube.
[03:19:17.500 --> 03:19:22.780]   And so you can be in Antarctica but have
[03:19:22.780 --> 03:19:30.660]   an amazing Twitter account and reach a lot of people.
[03:19:30.660 --> 03:19:34.180]   And so we're happy to employ people in Antarctica
[03:19:34.180 --> 03:19:37.300]   if they have that.
[03:19:37.300 --> 03:19:43.740]   There were some, for more context, like our CTO,
[03:19:43.740 --> 03:19:49.140]   I think we used to prefer that the engineers all sit together
[03:19:49.140 --> 03:19:52.300]   in the one office so they can collaborate
[03:19:52.300 --> 03:19:53.820]   like that.
[03:19:53.820 --> 03:19:56.500]   But then COVID came and we couldn't do it anyways.
[03:19:56.500 --> 03:20:00.460]   And so we're now like a remote first company.
[03:20:00.460 --> 03:20:02.140]   We still have an office in San Francisco
[03:20:02.140 --> 03:20:05.540]   and people nearby can work there if they like.
[03:20:05.540 --> 03:20:08.980]   But most people are pretty much remote most of the time.
[03:20:08.980 --> 03:20:10.820]   OK.
[03:20:10.820 --> 03:20:12.900]   [SPEAKING KOREAN]
[03:20:12.900 --> 03:20:14.900]   [SPEAKING KOREAN]
[03:20:15.900 --> 03:20:17.900]   [SPEAKING KOREAN]
[03:20:17.900 --> 03:20:19.900]   [SPEAKING KOREAN]
[03:20:19.900 --> 03:20:21.900]   [SPEAKING KOREAN]
[03:20:21.900 --> 03:20:50.900]   [SPEAKING KOREAN]
[03:20:50.900 --> 03:20:54.900]   There is one question about the competition.
[03:20:54.900 --> 03:20:58.820]   How many submissions are allowed in the contest?
[03:20:58.820 --> 03:21:02.420]   Yeah, so the answer in the chat is all yeah,
[03:21:02.420 --> 03:21:05.140]   but a maximum 20 submissions.
[03:21:05.140 --> 03:21:08.380]   And that's just the maximum Kaggle allows you.
[03:21:08.380 --> 03:21:12.540]   Like I don't think there should be a max, but yeah,
[03:21:12.540 --> 03:21:14.380]   20 is the most Kaggle allow us.
[03:21:14.380 --> 03:21:15.380]   OK.
[03:21:15.380 --> 03:21:17.380]   [SPEAKING KOREAN]
[03:21:17.380 --> 03:21:26.820]   [SPEAKING KOREAN]
[03:21:26.820 --> 03:21:31.580]   Can you share how to work remotely in 1DB?
[03:21:31.580 --> 03:21:33.060]   Sure.
[03:21:33.060 --> 03:21:37.380]   So we use, I guess, a lot of the similar tools
[03:21:37.380 --> 03:21:41.100]   that most remote companies use.
[03:21:41.100 --> 03:21:42.660]   I'm not a remote, I guess, too.
[03:21:42.660 --> 03:21:44.340]   But yeah, so a lot of our work is
[03:21:44.340 --> 03:21:48.500]   done asynchronously over Slack.
[03:21:48.500 --> 03:21:56.820]   We, as a loose policy, there are maybe like two or three team
[03:21:56.820 --> 03:22:01.140]   or company meetings a week that everyone is expected to attend.
[03:22:01.140 --> 03:22:05.180]   But besides that, we're super flexible.
[03:22:05.180 --> 03:22:12.500]   We just-- we want people to do the best work that they can.
[03:22:12.500 --> 03:22:16.860]   And so we don't care if you're working from 9 to 5
[03:22:16.860 --> 03:22:22.340]   or from midnight to 8 AM, whatever works for you.
[03:22:22.340 --> 03:22:25.380]   That's fine with us.
[03:22:25.380 --> 03:22:29.980]   We do monthly-- in the growth team,
[03:22:29.980 --> 03:22:34.020]   we do kind of monthly games sessions.
[03:22:34.020 --> 03:22:36.460]   And so we do some team games because it's
[03:22:36.460 --> 03:22:40.660]   nice just all to be together and play games
[03:22:40.660 --> 03:22:43.020]   and not talk about work.
[03:22:43.020 --> 03:22:48.140]   We have daily-- in our calendars,
[03:22:48.140 --> 03:22:52.740]   we have daily coffee chat times.
[03:22:52.740 --> 03:22:55.940]   And there's usually-- not everyone
[03:22:55.940 --> 03:22:58.700]   goes to those every day, but usually there's like two or three
[03:22:58.700 --> 03:23:00.940]   people there on any day.
[03:23:00.940 --> 03:23:04.700]   And you can, again, just drop in for 15 minutes or 20 minutes,
[03:23:04.700 --> 03:23:06.420]   or however you like.
[03:23:06.420 --> 03:23:08.860]   And just drop in and chat.
[03:23:08.860 --> 03:23:10.860]   And again, you get to connect to people
[03:23:10.860 --> 03:23:13.780]   because especially when you're remote,
[03:23:13.780 --> 03:23:18.620]   it's always nice to kind of connect to people.
[03:23:18.620 --> 03:23:21.980]   I think those are probably the main features.
[03:23:21.980 --> 03:23:24.740]   Yeah, if there's any more specific questions about remote
[03:23:24.740 --> 03:23:28.020]   work or the tools we use, happy to chat about that again.
[03:23:31.180 --> 03:24:00.540]   [SPEAKING KOREAN]
[03:24:00.540 --> 03:24:13.420]   Cool.
[03:24:13.420 --> 03:24:16.500]   Could you explain major differences or superiors
[03:24:16.500 --> 03:24:20.180]   of 1DB and the competitors?
[03:24:20.180 --> 03:24:20.860]   Sure, yeah.
[03:24:20.860 --> 03:24:21.620]   I'd love to.
[03:24:21.620 --> 03:24:24.500]   Maybe we can pause for 10 seconds
[03:24:24.500 --> 03:24:27.500]   and see if there's anyone on the leaderboard.
[03:24:27.500 --> 03:24:28.660]   Yes.
[03:24:28.660 --> 03:24:30.260]   Refresh.
[03:24:30.260 --> 03:24:31.500]   OK, cool.
[03:24:31.500 --> 03:24:33.740]   Too much on.
[03:24:33.740 --> 03:24:34.260]   Great.
[03:24:34.260 --> 03:24:35.900]   And so, sorry, I should have mentioned,
[03:24:35.900 --> 03:24:38.780]   but it's here on the leaderboard.
[03:24:38.780 --> 03:24:43.460]   10% of the test data goes towards the leaderboard
[03:24:43.460 --> 03:24:44.020]   competition.
[03:24:44.020 --> 03:24:46.780]   So at the end, we'll be scoring everything
[03:24:46.780 --> 03:24:48.860]   on the private leaderboard submission.
[03:24:48.860 --> 03:24:54.860]   So be careful not to overfit too much on your public leaderboard
[03:24:54.860 --> 03:24:56.820]   score.
[03:24:56.820 --> 03:24:59.900]   10% of the prediction?
[03:24:59.900 --> 03:25:02.780]   Yeah, 10% of the test set.
[03:25:02.780 --> 03:25:26.700]   [SPEAKING KOREAN]
[03:25:26.700 --> 03:25:33.940]   You can keep on explaining.
[03:25:33.940 --> 03:25:34.620]   Cool.
[03:25:34.620 --> 03:25:35.260]   Thanks, Yoan.
[03:25:35.260 --> 03:25:36.340]   That was everything.
[03:25:36.340 --> 03:25:38.500]   I just wanted to have a look.
[03:25:38.500 --> 03:25:43.180]   But yeah, we can go back to the questions.
[03:25:43.180 --> 03:25:45.340]   So yeah, so the major differences,
[03:25:45.340 --> 03:25:47.420]   weights and biases and the competitors,
[03:25:47.420 --> 03:25:53.340]   well, we kind of joke internally,
[03:25:53.340 --> 03:25:57.620]   but it's true, Google Sheets or Excel
[03:25:57.620 --> 03:25:59.940]   are actually probably our biggest competitors
[03:25:59.940 --> 03:26:02.100]   because that's actually where most people put
[03:26:02.100 --> 03:26:07.020]   in their experiment results.
[03:26:07.020 --> 03:26:12.580]   But in terms of dedicated machine learning and tracking,
[03:26:12.580 --> 03:26:16.460]   like I mentioned earlier, we have a few advantages
[03:26:16.460 --> 03:26:18.180]   over TensorBoard, for example, in terms
[03:26:18.180 --> 03:26:23.180]   of being able to log remotely and store your results remotely.
[03:26:23.180 --> 03:26:28.580]   We have the capabilities around modeling and data set
[03:26:28.580 --> 03:26:29.420]   versioning.
[03:26:29.420 --> 03:26:33.540]   We have the data and predictions like visualization,
[03:26:33.540 --> 03:26:35.460]   like I showed earlier.
[03:26:35.460 --> 03:26:39.500]   So I'll be against TensorBoard very quickly.
[03:26:39.500 --> 03:26:42.340]   Against the competitors, I mean, it's
[03:26:42.340 --> 03:26:47.220]   about what tools you enjoy to use.
[03:26:47.220 --> 03:26:50.260]   Feedback we've gotten compared to some of our competitors
[03:26:50.260 --> 03:26:53.660]   is that our product just feels a little bit more polished.
[03:26:53.660 --> 03:27:01.740]   Yeah, I think, again, there's a lot of reasons
[03:27:01.740 --> 03:27:05.220]   why people choose different tools.
[03:27:05.220 --> 03:27:10.220]   But we have a strong and growing customer base
[03:27:10.220 --> 03:27:13.420]   who have evaluated all of the tools out there
[03:27:13.420 --> 03:27:14.940]   and chosen weights and biases.
[03:27:14.940 --> 03:27:18.940]   Yeah, I mean, I guess--
[03:27:18.940 --> 03:27:26.820]   yeah, I don't-- I mean, I think it's important that we not--
[03:27:26.820 --> 03:27:28.700]   I don't want to put any other tools down.
[03:27:28.700 --> 03:27:31.220]   I mean, everyone's working quite hard in this space.
[03:27:31.220 --> 03:27:33.260]   And it's quite an evolving space.
[03:27:33.260 --> 03:27:35.980]   And a lot of people make a lot of new products
[03:27:35.980 --> 03:27:38.340]   that are quite cool.
[03:27:38.340 --> 03:27:40.940]   But yeah, if you have a specific question
[03:27:40.940 --> 03:27:44.340]   about a specific product or feature that's
[03:27:44.340 --> 03:27:48.740]   in our competitors, I'm happy to maybe talk about that.
[03:27:48.740 --> 03:27:49.540]   OK.
[03:27:49.540 --> 03:27:51.100]   [SPEAKING KOREAN]
[03:28:16.020 --> 03:28:18.740]   How much deep learning experience are you guys
[03:28:18.740 --> 03:28:21.020]   looking to ML engineers?
[03:28:21.020 --> 03:28:25.460]   Yeah, it's a good question.
[03:28:25.460 --> 03:28:32.780]   So for this particular role, for someone from Korea--
[03:28:32.780 --> 03:28:37.060]   and again, they don't have to be based in Korea.
[03:28:37.060 --> 03:28:40.700]   You could be a Korean living in the US or Europe.
[03:28:40.700 --> 03:28:43.900]   That's fine, too.
[03:28:43.900 --> 03:28:45.940]   That experience, I think we're looking for maybe
[03:28:45.940 --> 03:28:50.140]   like two or three years of deep learning experience.
[03:28:50.140 --> 03:28:52.620]   So you don't have to be super senior.
[03:28:52.620 --> 03:28:57.020]   But you have seen enough working in a company
[03:28:57.020 --> 03:29:02.580]   that you understand the main issues and pain
[03:29:02.580 --> 03:29:08.540]   points of doing machine learning in a team in production.
[03:29:11.700 --> 03:29:13.700]   [SPEAKING KOREAN]
[03:29:37.780 --> 03:29:40.060]   What kind of skill sets are you expecting
[03:29:40.060 --> 03:29:43.500]   from qualified candidates?
[03:29:43.500 --> 03:29:46.020]   So it's a great question.
[03:29:46.020 --> 03:29:52.220]   The ML engineers we have within growth
[03:29:52.220 --> 03:29:55.500]   are maybe having a slightly different skill
[03:29:55.500 --> 03:29:57.980]   set than a lot of--
[03:29:57.980 --> 03:29:59.900]   than the majority of ML engineers.
[03:29:59.900 --> 03:30:02.980]   Because generally, we look for people
[03:30:02.980 --> 03:30:08.500]   who are interacting with the community in some way.
[03:30:08.500 --> 03:30:12.140]   And that could look--
[03:30:12.140 --> 03:30:14.020]   there's a few different ways that can look.
[03:30:14.020 --> 03:30:18.060]   There's A, making open source contributions
[03:30:18.060 --> 03:30:18.860]   to other libraries.
[03:30:18.860 --> 03:30:23.980]   Or like Crescent, having a strong open source library
[03:30:23.980 --> 03:30:25.180]   yourself.
[03:30:25.180 --> 03:30:31.540]   There's people who are super strong and active on Twitter
[03:30:31.540 --> 03:30:36.020]   and YouTube and have a good social media presence like that.
[03:30:36.020 --> 03:30:39.060]   We also have people who are great writers
[03:30:39.060 --> 03:30:40.580]   and communicators.
[03:30:40.580 --> 03:30:45.620]   And so not only understand ML and deep learning,
[03:30:45.620 --> 03:30:49.180]   but able to communicate it really, really nicely.
[03:30:49.180 --> 03:30:52.820]   And then the same goes for those people running events
[03:30:52.820 --> 03:30:55.820]   and explaining different machine learning
[03:30:55.820 --> 03:30:58.020]   concepts or frameworks.
[03:30:58.020 --> 03:31:02.860]   So we're looking for someone with the strong technical
[03:31:02.860 --> 03:31:06.940]   experience, but also the ability to communicate and interact
[03:31:06.940 --> 03:31:08.380]   with the community.
[03:31:08.380 --> 03:31:09.380]   Oh, OK.
[03:31:09.380 --> 03:31:11.020]   I see.
[03:31:11.020 --> 03:31:12.700]   [SPEAKING KOREAN]
[03:31:12.700 --> 03:31:16.060]   [SPEAKING KOREAN]
[03:31:16.060 --> 03:31:17.420]   [SPEAKING KOREAN]
[03:31:17.420 --> 03:31:18.780]   [SPEAKING KOREAN]
[03:31:19.580 --> 03:31:20.940]   [SPEAKING KOREAN]
[03:31:20.940 --> 03:31:22.180]   [SPEAKING KOREAN]
[03:31:22.180 --> 03:31:23.420]   [SPEAKING KOREAN]
[03:31:23.420 --> 03:31:24.660]   [SPEAKING KOREAN]
[03:31:24.660 --> 03:31:25.900]   [SPEAKING KOREAN]
[03:31:25.900 --> 03:31:55.260]   [SPEAKING KOREAN]
[03:31:55.260 --> 03:31:59.540]   And I might just add to that also.
[03:31:59.540 --> 03:32:06.700]   The way I look at it is that we are helping build this team
[03:32:06.700 --> 03:32:11.340]   and build this company through ML.
[03:32:11.340 --> 03:32:17.220]   And so we don't have a research team, at least not yet.
[03:32:17.220 --> 03:32:23.860]   So we don't do six or 12 months of research and publish.
[03:32:23.860 --> 03:32:29.820]   The product isn't powered by machine learning.
[03:32:29.820 --> 03:32:32.500]   So the machine learning comes in in terms
[03:32:32.500 --> 03:32:37.980]   of being able to take that understanding and knowledge,
[03:32:37.980 --> 03:32:40.780]   wrap White's and Bice's product around it,
[03:32:40.780 --> 03:32:42.980]   and then communicate that to the world.
[03:32:42.980 --> 03:32:45.820]   Or in terms of our integrations team,
[03:32:45.820 --> 03:32:49.900]   maybe make some code integrations also.
[03:32:49.900 --> 03:32:51.740]   But yeah, so if you have an interest
[03:32:51.740 --> 03:32:57.300]   in helping build this startup and make the startup a success,
[03:32:57.300 --> 03:32:59.180]   that's also something we're looking for.
[03:32:59.180 --> 03:33:00.620]   [SPEAKING KOREAN]
[03:33:19.140 --> 03:33:23.660]   About the question before, difference
[03:33:23.660 --> 03:33:27.740]   between the Wangdevia and the competitors,
[03:33:27.740 --> 03:33:34.100]   Jiho asked more specific about comparing with MLflow.
[03:33:34.100 --> 03:33:35.820]   Sure.
[03:33:35.820 --> 03:33:45.460]   So if you've used MLflow, you'll know that they probably--
[03:33:45.460 --> 03:33:48.260]   well, one of the main differences we see
[03:33:48.260 --> 03:33:51.100]   is that they have a model registry.
[03:33:51.100 --> 03:33:57.340]   And I know that's important for a lot of users in industry
[03:33:57.340 --> 03:34:00.100]   to be able to keep track of their models
[03:34:00.100 --> 03:34:04.380]   and know which ones are being staged
[03:34:04.380 --> 03:34:06.940]   and which ones are being experimented on
[03:34:06.940 --> 03:34:09.900]   and which models are in production.
[03:34:09.900 --> 03:34:11.660]   So that's probably the biggest difference.
[03:34:11.660 --> 03:34:14.740]   We are building a model registry.
[03:34:14.740 --> 03:34:18.180]   And it's actually in beta at the moment.
[03:34:18.180 --> 03:34:21.540]   And if you go to our docs, you can see it.
[03:34:21.540 --> 03:34:25.420]   It's going to change a lot in the next couple of months.
[03:34:25.420 --> 03:34:28.020]   But it's down here in model management.
[03:34:28.020 --> 03:34:30.060]   And I'll share this link in the chat too.
[03:34:30.060 --> 03:34:31.700]   Sorry.
[03:34:31.700 --> 03:34:35.020]   And so we're building a model registry there
[03:34:35.020 --> 03:34:40.820]   because we do see the request from users and customers
[03:34:40.820 --> 03:34:42.820]   for that.
[03:34:42.820 --> 03:34:45.740]   Other comparisons to MLflow--
[03:34:45.740 --> 03:34:50.540]   I think, like small things, we've
[03:34:50.540 --> 03:34:56.260]   seen our documentation, or we're told it is a little better.
[03:34:56.260 --> 03:35:03.060]   Some people are concerned that, again, MLflow is local.
[03:35:03.060 --> 03:35:06.020]   So it's just a bit harder to share results
[03:35:06.020 --> 03:35:08.500]   unless maybe you're in a company and then you
[03:35:08.500 --> 03:35:12.740]   have a hosted solution and multiple people can work on it.
[03:35:13.060 --> 03:35:18.380]   And yeah, that's kind of the main comparisons.
[03:35:18.380 --> 03:35:23.620]   I guess MLflow is also ideally part of the wider parent
[03:35:23.620 --> 03:35:27.700]   companies' offerings for other ML services.
[03:35:27.700 --> 03:35:31.460]   So they're maybe also trying to push you towards those
[03:35:31.460 --> 03:35:35.140]   as part of the MLflow offering.
[03:35:35.140 --> 03:35:36.780]   Yeah, hopefully that helps.
[03:35:36.780 --> 03:35:37.260]   OK.
[03:35:37.660 --> 03:35:38.140]   OK.
[03:35:38.140 --> 03:36:04.620]   So one of them is asking, so in short,
[03:36:04.620 --> 03:36:07.700]   is Weights & Biases looking for some sort
[03:36:07.700 --> 03:36:13.380]   of tech evangelist type of ML engineers in Korea?
[03:36:13.380 --> 03:36:18.140]   Yes, yeah, that's a good way of putting it.
[03:36:18.140 --> 03:36:26.380]   We also appreciate people who have a product mindset also.
[03:36:26.380 --> 03:36:30.020]   So if you look at Weights & Biases, at the product,
[03:36:30.020 --> 03:36:34.260]   if you have ideas around how it can be improved,
[03:36:34.260 --> 03:36:36.940]   how the user experience can be better,
[03:36:36.940 --> 03:36:40.100]   how we can reduce friction, yeah,
[03:36:40.100 --> 03:36:41.580]   we're also looking for those people.
[03:36:41.580 --> 03:36:46.500]   But in general, yeah, evangelist is a good description
[03:36:46.500 --> 03:36:48.780]   of what we're looking for in Korea.
[03:36:48.780 --> 03:37:10.940]   [SPEAKING KOREAN]
[03:37:10.940 --> 03:37:15.060]   What is the major programming language and platform for 1DB?
[03:37:15.060 --> 03:37:18.500]   Do you need a kind of on-device AI engineering,
[03:37:18.500 --> 03:37:19.580]   like a mobile API?
[03:37:19.580 --> 03:37:28.020]   So a lot of our app is--
[03:37:28.020 --> 03:37:33.260]   or rather, the back end is written in Go.
[03:37:33.260 --> 03:37:38.020]   The front end is written in TypeScript, I believe.
[03:37:38.020 --> 03:37:43.380]   And in terms of the mobile on-device, it's interesting.
[03:37:43.380 --> 03:37:46.020]   But no, not for now.
[03:37:46.020 --> 03:37:50.300]   A lot of the-- or all of the compute that happens,
[03:37:50.300 --> 03:37:55.980]   to render the charts or do the visualizations with tables,
[03:37:55.980 --> 03:37:58.660]   all happens on our servers on the back end.
[03:37:58.660 --> 03:38:03.220]   So we don't do much work on device today.
[03:38:03.220 --> 03:38:05.180]   But thanks for the question.
[03:38:05.180 --> 03:38:28.860]   [SPEAKING KOREAN]
[03:38:28.860 --> 03:38:33.940]   And Yubin said she just wanted to say thank you
[03:38:33.940 --> 03:38:37.140]   for the amazing work.
[03:38:37.140 --> 03:38:42.460]   And Yubin is working on three ML projects for which 1DB just
[03:38:42.460 --> 03:38:44.220]   works like a charm.
[03:38:44.220 --> 03:38:47.020]   It does a 90-grit--
[03:38:47.020 --> 03:38:50.900]   it does the nitty gritty, heavy lifting jobs for me.
[03:38:50.900 --> 03:38:53.980]   And Yubin loves it.
[03:38:53.980 --> 03:38:55.260]   Yeah, just wants to say that.
[03:38:55.260 --> 03:38:56.220]   Great.
[03:38:56.220 --> 03:38:56.820]   Glory, thank you.
[03:38:56.820 --> 03:38:57.340]   Thank you.
[03:38:57.340 --> 03:38:58.260]   Good to hear.
[03:38:58.260 --> 03:38:59.660]   We'll pass on that feedback.
[03:38:59.660 --> 03:39:01.780]   We have a channel internally.
[03:39:01.780 --> 03:39:03.220]   We share all this positive feedback
[03:39:03.220 --> 03:39:07.500]   because it's great to share it with the engineers and the product
[03:39:07.500 --> 03:39:10.700]   team who don't interact with the community every day.
[03:39:10.700 --> 03:39:12.740]   So thank you.
[03:39:12.740 --> 03:39:14.980]   OK, thank you.
[03:39:14.980 --> 03:39:16.700]   One more question.
[03:39:16.700 --> 03:39:20.260]   What is the current short-term and if possibly long-term
[03:39:20.260 --> 03:39:21.820]   objectives for 1DB?
[03:39:21.820 --> 03:39:29.100]   It's a good question.
[03:39:29.100 --> 03:39:34.060]   I guess I can talk about long-term
[03:39:34.060 --> 03:39:36.020]   because that's easier.
[03:39:36.020 --> 03:39:41.940]   And so in my presentation earlier--
[03:39:41.940 --> 03:39:45.500]   and if you missed it, I'll go to the slide here.
[03:39:45.500 --> 03:39:51.540]   So we see this as the pipeline for ML experimentation.
[03:39:51.540 --> 03:39:57.300]   What we don't have at the moment is production monitoring.
[03:39:57.300 --> 03:40:00.540]   So that's probably the biggest thing that we'll
[03:40:00.540 --> 03:40:04.140]   be working on later this year.
[03:40:04.140 --> 03:40:07.740]   We also-- maybe I'll pause there, actually,
[03:40:07.740 --> 03:40:11.580]   because this might be a long answer.
[03:40:11.580 --> 03:40:16.460]   Can you explain more about the short-term objectives?
[03:40:16.460 --> 03:40:24.860]   Yeah, short-term, we have the model registry
[03:40:24.860 --> 03:40:27.740]   that we want to ship, and we have
[03:40:27.740 --> 03:40:32.300]   a new product called Launch.
[03:40:32.300 --> 03:40:35.020]   And so it's about--
[03:40:35.020 --> 03:40:40.180]   the model registry is important but incremental
[03:40:40.180 --> 03:40:43.500]   and built on top of artifacts.
[03:40:43.500 --> 03:40:47.180]   Launch is a newer, more ambitious project
[03:40:47.180 --> 03:40:51.180]   that will open the door to a lot more interesting things.
[03:40:51.180 --> 03:40:53.060]   But I can talk about that more in a second.
[03:40:54.060 --> 03:40:56.060]   [SPEAKING KOREAN]
[03:40:56.860 --> 03:40:58.860]   [SPEAKING KOREAN]
[03:40:59.660 --> 03:41:01.660]   [SPEAKING KOREAN]
[03:41:01.660 --> 03:41:03.660]   [SPEAKING KOREAN]
[03:41:04.660 --> 03:41:06.660]   [SPEAKING KOREAN]
[03:41:06.660 --> 03:41:30.140]   OK.
[03:41:30.140 --> 03:41:35.140]   So that's about it.
[03:41:35.140 --> 03:41:39.980]   And if you have more questions, please write down
[03:41:39.980 --> 03:41:41.420]   through our YouTube channel.
[03:41:41.420 --> 03:41:50.620]   So we have about 17 minutes left.
[03:41:50.620 --> 03:41:52.620]   [SPEAKING KOREAN]
[03:41:52.620 --> 03:41:54.620]   [SPEAKING KOREAN]
[03:41:54.620 --> 03:42:22.380]   So yeah, we have 19 submissions, some quite strong
[03:42:22.380 --> 03:42:26.820]   scores, and a very clear leader at the moment,
[03:42:26.820 --> 03:42:32.380]   with only one entry as well, which is super interesting.
[03:42:32.380 --> 03:42:34.300]   That's very nice.
[03:42:34.300 --> 03:42:36.300]   [SPEAKING KOREAN]
[03:42:36.300 --> 03:42:38.300]   [SPEAKING KOREAN]
[03:42:38.300 --> 03:42:40.300]   [SPEAKING KOREAN]
[03:42:40.300 --> 03:42:42.300]   [SPEAKING KOREAN]
[03:42:42.300 --> 03:42:44.300]   [SPEAKING KOREAN]
[03:42:44.300 --> 03:42:46.300]   [SPEAKING KOREAN]
[03:42:46.300 --> 03:42:48.300]   [SPEAKING KOREAN]
[03:42:49.300 --> 03:42:51.300]   [SPEAKING KOREAN]
[03:42:51.300 --> 03:42:53.300]   [SPEAKING KOREAN]
[03:43:19.300 --> 03:43:23.220]   We can talk a little bit more about Weights and Biases
[03:43:23.220 --> 03:43:24.420]   Launch, maybe?
[03:43:24.420 --> 03:43:27.420]   Yeah, sure, sure.
[03:43:27.420 --> 03:43:29.420]   [SPEAKING KOREAN]
[03:43:29.420 --> 03:43:41.340]   Cool.
[03:43:41.340 --> 03:43:46.820]   So the idea with Weights and Biases Launch
[03:43:46.820 --> 03:43:53.500]   is that you can schedule and trigger
[03:43:53.500 --> 03:43:57.740]   training runs or evaluation runs from the Weights and Biases
[03:43:57.740 --> 03:43:58.940]   dashboard.
[03:43:58.940 --> 03:44:04.180]   And so Launch will work with whatever infrastructure
[03:44:04.180 --> 03:44:06.580]   you're using today.
[03:44:06.580 --> 03:44:11.940]   And so if you're using GCP or AWS or Azure
[03:44:11.940 --> 03:44:14.940]   or whatever machines, Launch will
[03:44:14.940 --> 03:44:21.660]   be able to spin up those machines,
[03:44:21.660 --> 03:44:25.180]   kick off a training run, and then log all of those results
[03:44:25.180 --> 03:44:28.380]   back to Weights and Biases.
[03:44:28.380 --> 03:44:29.900]   Maybe I'll pause there for a second.
[03:44:29.900 --> 03:44:37.740]   [SPEAKING KOREAN]
[03:44:38.740 --> 03:44:40.740]   [SPEAKING KOREAN]
[03:44:40.740 --> 03:44:55.260]   Do you have more to say about the Launch?
[03:44:55.260 --> 03:44:55.860]   Yeah.
[03:44:55.860 --> 03:45:02.660]   And so one example would be if you have trained a model
[03:45:02.660 --> 03:45:07.140]   and then you've done run and evaluation run.
[03:45:07.140 --> 03:45:11.260]   On that model, on your validation data set,
[03:45:11.260 --> 03:45:14.900]   you'll be able to take that evaluation run
[03:45:14.900 --> 03:45:19.140]   and relaunch it if you need to.
[03:45:19.140 --> 03:45:24.020]   And so for example, if you collect more data
[03:45:24.020 --> 03:45:26.980]   and you want to understand how this model performs
[03:45:26.980 --> 03:45:31.100]   on this new data, you'll just be able to take
[03:45:31.100 --> 03:45:34.420]   the same evaluation run and point it
[03:45:34.420 --> 03:45:38.780]   towards a different data set, which also lives in Weights
[03:45:38.780 --> 03:45:42.260]   and Biases, and say, no, evaluate on this data set,
[03:45:42.260 --> 03:45:45.620]   please, and then push Play.
[03:45:45.620 --> 03:45:48.820]   And then Launch will be able to spin up some machines
[03:45:48.820 --> 03:45:52.060]   and run the exact same training script with all
[03:45:52.060 --> 03:45:55.340]   of the same hyperparameters with the same model weights
[03:45:55.340 --> 03:45:59.500]   and the same logging and create a new evaluation
[03:45:59.500 --> 03:46:04.100]   run on the new data set.
[03:46:04.100 --> 03:46:06.020]   [SPEAKING KOREAN]
[03:46:06.020 --> 03:46:08.020]   [SPEAKING KOREAN]
[03:46:08.020 --> 03:46:36.020]   [SPEAKING KOREAN]
[03:46:36.020 --> 03:46:37.500]   Cool.
[03:46:37.500 --> 03:46:38.820]   Yeah, that's about it.
[03:46:38.820 --> 03:46:42.180]   So both Launch and the Model Registry
[03:46:42.180 --> 03:46:44.140]   are in beta at the moment.
[03:46:44.140 --> 03:46:47.940]   So just a warning, if you're using them,
[03:46:47.940 --> 03:46:50.420]   there might be some things that don't work so well
[03:46:50.420 --> 03:46:52.340]   at the moment.
[03:46:52.340 --> 03:46:52.840]   OK.
[03:46:52.840 --> 03:47:00.340]   How are we doing?
[03:47:00.340 --> 03:47:01.340]   [SPEAKING KOREAN]
[03:47:01.340 --> 03:47:09.940]   People are moving up.
[03:47:09.940 --> 03:47:19.940]   So we have 12 minutes left, I think.
[03:47:19.940 --> 03:47:20.460]   Yes.
[03:47:20.460 --> 03:47:20.960]   Great.
[03:47:20.960 --> 03:47:22.460]   [SPEAKING KOREAN]
[03:47:22.460 --> 03:47:48.700]   [SPEAKING KOREAN]
[03:47:48.700 --> 03:48:17.540]   [SPEAKING KOREAN]
[03:48:17.540 --> 03:48:20.260]   No, not weights and biases, but just, well,
[03:48:20.260 --> 03:48:23.500]   either an ML startup or just a cool startup
[03:48:23.500 --> 03:48:26.380]   that use a lot of ML.
[03:48:26.380 --> 03:48:28.660]   Oh, OK.
[03:48:28.660 --> 03:48:31.980]   There are a lot of startups.
[03:48:31.980 --> 03:48:40.260]   I think many people can answer that instead of me.
[03:48:43.780 --> 03:48:51.900]   One startup that I thought was interesting was Altera.
[03:48:51.900 --> 03:48:56.540]   Also, Andrew Un have mentioned it before.
[03:48:56.540 --> 03:49:02.540]   And this predicts the fire from the mountain.
[03:49:02.540 --> 03:49:08.260]   And I heard that it has a really good prediction for that.
[03:49:11.180 --> 03:49:17.060]   That's one of the companies that I looked very interested.
[03:49:17.060 --> 03:49:17.560]   Cool.
[03:49:17.560 --> 03:49:19.540]   [SPEAKING KOREAN]
[03:49:19.540 --> 03:49:41.340]   [SPEAKING KOREAN]
[03:49:41.340 --> 03:49:56.300]   [SPEAKING KOREAN]
[03:49:56.300 --> 03:49:59.540]   So that comment in the YouTube channel,
[03:49:59.540 --> 03:50:02.780]   the company is called here?
[03:50:02.780 --> 03:50:03.900]   Interesting.
[03:50:09.660 --> 03:50:11.140]   Oh, here.
[03:50:11.140 --> 03:50:14.660]   I think he means 1 DB.
[03:50:14.660 --> 03:50:16.020]   Ah, sorry.
[03:50:16.020 --> 03:50:21.780]   OK, cool.
[03:50:21.780 --> 03:50:25.900]   There's a startup that uses vision AI to get personal color.
[03:50:25.900 --> 03:50:28.260]   I thought it was quite interesting.
[03:50:28.260 --> 03:50:31.660]   [SPEAKING KOREAN] Can you tell us
[03:50:31.660 --> 03:50:33.340]   the name of the startup company?
[03:50:33.340 --> 03:50:51.620]   I'm curious.
[03:50:51.620 --> 03:51:00.620]   In the meantime, do you have any other events, deep learning
[03:51:00.620 --> 03:51:05.100]   playground events lined up in the next few weeks or months?
[03:51:05.100 --> 03:51:08.140]   So this is actually our first event.
[03:51:08.140 --> 03:51:15.220]   So we don't have anything yet.
[03:51:15.220 --> 03:51:17.620]   Oh, cool.
[03:51:17.620 --> 03:51:21.060]   Maybe we'll hold more events after.
[03:51:21.060 --> 03:51:23.140]   Yeah, great.
[03:51:23.140 --> 03:51:28.060]   For anyone watching, it's been amazing
[03:51:28.060 --> 03:51:30.980]   to work with Stella and Yuan and the entire team.
[03:51:30.980 --> 03:51:35.660]   So I highly recommend working with the DL Playground
[03:51:35.660 --> 03:51:37.420]   to organize events.
[03:51:37.420 --> 03:51:41.620]   Yes, we are also recruiting the new members for our community.
[03:51:41.620 --> 03:51:46.740]   So speaking of that, I'll advertise them for a second.
[03:51:49.380 --> 03:51:51.380]   [SPEAKING KOREAN]
[03:51:51.380 --> 03:51:53.380]   [SPEAKING KOREAN]
[03:51:53.380 --> 03:51:55.380]   [SPEAKING KOREAN]
[03:51:55.380 --> 03:51:57.380]   [SPEAKING KOREAN]
[03:51:58.380 --> 03:52:00.380]   [SPEAKING KOREAN]
[03:52:00.380 --> 03:52:02.380]   [SPEAKING KOREAN]
[03:52:02.380 --> 03:52:04.380]   [SPEAKING KOREAN]
[03:52:04.380 --> 03:52:06.380]   [SPEAKING KOREAN]
[03:52:06.380 --> 03:52:08.380]   [SPEAKING KOREAN]
[03:52:08.380 --> 03:52:10.380]   [SPEAKING KOREAN]
[03:52:10.380 --> 03:52:12.380]   [SPEAKING KOREAN]
[03:52:12.380 --> 03:52:14.380]   [SPEAKING KOREAN]
[03:52:14.380 --> 03:52:16.380]   [SPEAKING KOREAN]
[03:52:16.380 --> 03:52:18.380]   [SPEAKING KOREAN]
[03:52:18.380 --> 03:52:20.380]   [SPEAKING KOREAN]
[03:52:20.380 --> 03:52:22.380]   [SPEAKING KOREAN]
[03:52:22.380 --> 03:52:24.380]   [SPEAKING KOREAN]
[03:52:25.380 --> 03:52:27.380]   [SPEAKING KOREAN]
[03:52:27.380 --> 03:52:29.380]   [SPEAKING KOREAN]
[03:52:56.380 --> 03:53:03.220]   Do-- is there a DL Playground Twitter handle?
[03:53:03.220 --> 03:53:04.380]   Pardon?
[03:53:04.380 --> 03:53:08.660]   Is there a Twitter account for a deep learning playground?
[03:53:08.660 --> 03:53:10.060]   We haven't made that yet.
[03:53:10.060 --> 03:53:11.980]   We have Instagram, we have Facebook,
[03:53:11.980 --> 03:53:14.580]   we have YouTube, no Twitter yet.
[03:53:14.580 --> 03:53:17.220]   Oh, we'll make that.
[03:53:17.220 --> 03:53:20.620]   Do you see a lot of machine learning on Instagram?
[03:53:20.620 --> 03:53:25.020]   It's not an area we look at at the moment.
[03:53:25.020 --> 03:53:27.980]   On Instagram?
[03:53:27.980 --> 03:53:29.820]   Actually, we just made an account.
[03:53:29.820 --> 03:53:32.940]   We haven't activated the account.
[03:53:32.940 --> 03:53:37.020]   So I have nothing to say for that.
[03:53:37.020 --> 03:53:45.660]   About the company that uses vision AI to get
[03:53:45.660 --> 03:53:50.540]   a personal color, it's called CodeCorn.
[03:53:50.540 --> 03:53:51.580]   Oh, I heard about that.
[03:53:52.220 --> 03:53:52.700]   Nice.
[03:53:52.700 --> 03:53:59.180]   OK, there's also Glassdome.
[03:53:59.180 --> 03:54:08.100]   Oh, thank you, Jinmoo.
[03:54:08.100 --> 03:54:13.860]   Great.
[03:54:13.860 --> 03:54:15.900]   So I think we have about five minutes left
[03:54:15.900 --> 03:54:18.540]   in the competition.
[03:54:18.540 --> 03:54:22.740]   So if anyone wants to try one last idea or two,
[03:54:22.740 --> 03:54:25.780]   now's the time to go.
[03:54:25.780 --> 03:54:27.780]   [SPEAKING KOREAN]
[03:54:27.780 --> 03:54:46.060]   Oh, I saw-- just to speak in personal,
[03:54:46.060 --> 03:54:49.060]   I saw your Twitter before our seminar.
[03:54:49.060 --> 03:54:55.820]   And in your area, it was 5 AM, I saw.
[03:54:55.820 --> 03:54:56.780]   Yeah, yeah.
[03:54:56.780 --> 03:55:01.780]   We started this event at 6 AM Irish time.
[03:55:01.780 --> 03:55:04.860]   Sorry, I got up at 5.
[03:55:04.860 --> 03:55:08.900]   Well, thank you for joining for our seminar.
[03:55:08.900 --> 03:55:09.820]   No, it's been amazing.
[03:55:09.820 --> 03:55:10.860]   Yeah, thanks very much.
[03:55:10.860 --> 03:55:14.100]   And it's been great to get in touch with everyone here.
[03:55:14.100 --> 03:55:16.580]   And some of the talks are really interesting as well.
[03:55:16.580 --> 03:55:18.500]   And yeah, I mean, I feel like I've
[03:55:18.500 --> 03:55:22.140]   done a full day's work, and it's only 10 AM.
[03:55:22.140 --> 03:55:24.820]   I've got the entire day.
[03:55:24.820 --> 03:55:28.580]   Oh, it's almost 7 PM here in Korea.
[03:55:28.580 --> 03:55:35.900]   Oh, Leslie, can you show us how to apply for a job for 1DB,
[03:55:35.900 --> 03:55:39.140]   like through the LinkedIn?
[03:55:39.140 --> 03:55:40.300]   Yeah, exactly.
[03:55:40.300 --> 03:55:43.740]   So we can go to the link.
[03:55:43.740 --> 03:55:45.420]   So this is the link again.
[03:55:45.420 --> 03:55:48.500]   And I'll just drop it in the chat one more time.
[03:55:48.500 --> 03:55:51.740]   So if we go to this link, this will take us to LinkedIn.
[03:55:51.740 --> 03:55:58.580]   And yeah, you can just hit the Apply button.
[03:55:58.580 --> 03:56:08.580]   And so that actually just takes us to our careers page.
[03:56:08.580 --> 03:56:11.780]   And so we're hiring for this role.
[03:56:11.780 --> 03:56:15.580]   We would love to have someone from Korea for this role.
[03:56:15.580 --> 03:56:20.460]   But we're also hiring for this role in other locations
[03:56:20.460 --> 03:56:20.980]   as well.
[03:56:20.980 --> 03:56:23.300]   We would like to hire maybe a few more deep learning
[03:56:23.300 --> 03:56:25.140]   engineers across growth in general.
[03:56:25.140 --> 03:56:30.940]   I think that's about it.
[03:56:30.940 --> 03:56:33.820]   Can you also share the link at the YouTube?
[03:56:33.820 --> 03:56:39.660]   Oh, yeah, I think I did, right?
[03:56:39.660 --> 03:56:40.260]   Can you see it?
[03:56:41.260 --> 03:56:44.340]   [SPEAKING KOREAN]
[03:57:09.260 --> 03:57:12.060]   And one other thing-- so yeah, two minutes left
[03:57:12.060 --> 03:57:14.340]   for the competition.
[03:57:14.340 --> 03:57:17.940]   Looking forward to seeing what happens when we get there.
[03:57:17.940 --> 03:57:19.660]   But one thing I'd like to show quickly
[03:57:19.660 --> 03:57:25.220]   is we just relaunched our version 2 of our blog
[03:57:25.220 --> 03:57:26.340]   just yesterday.
[03:57:26.340 --> 03:57:29.300]   And so it's called Fully Connected.
[03:57:29.300 --> 03:57:35.940]   It has a lot of articles, some written by [INAUDIBLE]
[03:57:35.940 --> 03:57:38.780]   machine learning engineers, some contributed
[03:57:38.780 --> 03:57:44.060]   by the community on a really wide variety of topics,
[03:57:44.060 --> 03:57:47.300]   some intro level stuff, some more advanced stuff,
[03:57:47.300 --> 03:57:49.060]   some paper reviews.
[03:57:49.060 --> 03:57:50.620]   And so we would love if people wanted
[03:57:50.620 --> 03:57:55.780]   to write any reports here, we'd love to show them.
[03:57:55.780 --> 03:57:59.140]   And so you can submit a report just like this.
[03:57:59.140 --> 03:58:06.420]   Our content team will review it and then hopefully publish it.
[03:58:06.420 --> 03:58:08.340]   But there's also different sections.
[03:58:08.340 --> 03:58:10.540]   So you can see projects.
[03:58:10.540 --> 03:58:15.260]   And again, if there are specific projects
[03:58:15.260 --> 03:58:18.940]   you'd like to showcase, you can also add your own projects
[03:58:18.940 --> 03:58:20.660]   here.
[03:58:20.660 --> 03:58:24.940]   Oh, yeah, so in this Share button project here.
[03:58:24.940 --> 03:58:29.380]   We also showed the trend of if it's a GitHub library, who's
[03:58:29.380 --> 03:58:33.260]   using or how many people are using that library.
[03:58:33.260 --> 03:58:34.860]   We have a news section.
[03:58:34.860 --> 03:58:36.820]   We have an events section.
[03:58:36.820 --> 03:58:41.500]   You can see our webinars and also rewatch past events
[03:58:41.500 --> 03:58:42.980]   and webinars.
[03:58:42.980 --> 03:58:45.860]   We have all of the links to all of our podcasts
[03:58:45.860 --> 03:58:47.780]   on the Gradient Descent.
[03:58:47.780 --> 03:58:51.100]   And we have a link that goes out to our forum.
[03:58:51.100 --> 03:58:55.300]   So I'll share this link too.
[03:58:55.300 --> 03:58:57.180]   That's amazing.
[03:58:57.180 --> 03:59:07.180]   [SPEAKING KOREAN]
[03:59:07.180 --> 03:59:17.180]   [SPEAKING KOREAN]
[03:59:17.180 --> 03:59:27.180]   [SPEAKING KOREAN]
[03:59:27.180 --> 03:59:37.180]   [SPEAKING KOREAN]
[03:59:37.180 --> 03:59:47.180]   [SPEAKING KOREAN]
[03:59:47.180 --> 03:59:57.180]   [SPEAKING KOREAN]
[03:59:57.180 --> 03:59:57.700]   Cool.
[03:59:57.700 --> 04:00:01.180]   I think we can finish the contest.
[04:00:01.180 --> 04:00:16.180]   [SPEAKING KOREAN]
[04:00:16.180 --> 04:00:17.860]   Can you show us the top 10?
[04:00:17.860 --> 04:00:19.820]   Yeah, so I just refreshed.
[04:00:19.820 --> 04:00:21.180]   This is the private leaderboard.
[04:00:21.180 --> 04:00:22.900]   This is our final results.
[04:00:22.900 --> 04:00:25.420]   So in first place, congrats, Suwon.
[04:00:25.420 --> 04:00:36.420]   [SPEAKING KOREAN]
[04:00:36.420 --> 04:00:38.620]   Well done, everybody.
[04:00:38.620 --> 04:00:43.740]   Yeah, I hope this was useful and a bit of fun.
[04:00:43.740 --> 04:00:45.420]   And it gives you a bit of an idea
[04:00:45.420 --> 04:00:49.620]   of how to use the Weights & Biases.
[04:00:49.620 --> 04:00:52.900]   Let me share the slides.
[04:00:52.900 --> 04:00:55.380]   Can you start sharing your screen?
[04:00:55.380 --> 04:00:58.700]   So that I can show them how to email.
[04:01:10.100 --> 04:01:40.060]   [SPEAKING KOREAN]
[04:01:40.060 --> 04:01:46.060]   [SPEAKING KOREAN]
[04:01:46.060 --> 04:01:51.180]   And, Yoan, I see, looking at the leaderboard,
[04:01:51.180 --> 04:01:57.780]   we actually have two scores for 10 and 11.
[04:01:57.780 --> 04:02:00.900]   So we'll give out swag to the top 11.
[04:02:00.900 --> 04:02:22.340]   [SPEAKING KOREAN]
[04:02:22.340 --> 04:02:27.340]   [SPEAKING KOREAN]
[04:02:27.340 --> 04:02:44.780]   [SPEAKING KOREAN]
[04:02:44.780 --> 04:03:12.580]   [SPEAKING KOREAN]
[04:03:12.580 --> 04:03:16.980]   So now, thank you for attending at our seminar.
[04:03:16.980 --> 04:03:21.740]   So that's about it.
[04:03:21.740 --> 04:03:23.340]   And thank you.
[04:03:23.340 --> 04:03:26.820]   Yeah, thanks very much for hosting us.
[04:03:26.820 --> 04:03:27.780]   We really enjoyed this.
[04:03:27.780 --> 04:03:30.420]   And yeah, like I said, if folks want
[04:03:30.420 --> 04:03:33.380]   to look at the YouTube chat, there's all the links there.
[04:03:33.380 --> 04:03:36.300]   If you want to look at Fully Connected, some of the new
[04:03:36.300 --> 04:03:38.620]   products we're launching, if you're interested in working
[04:03:38.620 --> 04:03:42.220]   for us, we'd love to hear from you.
[04:03:42.220 --> 04:03:44.980]   If you have any issues about weights and biases,
[04:03:44.980 --> 04:03:49.060]   the product in general, you can email support@onedb.com.
[04:03:49.060 --> 04:03:51.900]   And I'll add that in the YouTube chat as well.
[04:03:51.900 --> 04:03:55.620]   But yeah, no, huge thanks to the entire deep learning
[04:03:55.620 --> 04:03:59.060]   playground team, and to Stella, to Yoan, and Jiwoo,
[04:03:59.060 --> 04:03:59.940]   and everyone else.
[04:03:59.940 --> 04:04:05.580]   And yeah, looking forward to doing some more fun events
[04:04:05.580 --> 04:04:09.500]   with the Korean ML community at some point soon.
[04:04:09.500 --> 04:04:10.580]   OK, thank you.
[04:04:10.580 --> 04:04:24.540]   [SPEAKING KOREAN]
[04:04:24.540 --> 04:04:26.300]   [SPEAKING KOREAN]
[04:04:26.300 --> 04:04:29.960]   [END PLAYBACK]
[04:04:30.520 --> 04:04:33.080]   [VIDEO PLAYBACK]
[04:04:33.600 --> 04:04:36.160]   [END PLAYBACK]
[04:04:36.160 --> 04:04:46.160]   [BLANK_AUDIO]

