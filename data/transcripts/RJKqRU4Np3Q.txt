
[00:00:00.000 --> 00:00:03.720]   So I'm here to talk about designing an ML project.
[00:00:03.720 --> 00:00:08.160]   And hopefully this is relevant based on all the questions
[00:00:08.160 --> 00:00:09.080]   that were being asked.
[00:00:09.080 --> 00:00:11.200]   I think there'll be some material in here
[00:00:11.200 --> 00:00:11.860]   that's relevant.
[00:00:11.860 --> 00:00:15.040]   But if it's not, please interrupt me or interject.
[00:00:15.040 --> 00:00:16.120]   This is for you guys.
[00:00:16.120 --> 00:00:20.080]   So please interrupt and ask questions as you have them
[00:00:20.080 --> 00:00:23.900]   or guide the conversation in one direction.
[00:00:23.900 --> 00:00:26.200]   I think the only reason Josh invited me today
[00:00:26.200 --> 00:00:28.120]   is because we, about every three months,
[00:00:28.120 --> 00:00:31.440]   go and get coffee and talk about my crazy machine learning
[00:00:31.440 --> 00:00:36.920]   ideas that are mostly terrible, but he seems to tolerate them.
[00:00:36.920 --> 00:00:38.720]   But yeah, hopefully this is all relevant.
[00:00:38.720 --> 00:00:42.200]   So I always think it's important to start--
[00:00:42.200 --> 00:00:45.320]   will this work-- with a little bit of background on me.
[00:00:45.320 --> 00:00:47.120]   So you guys are like, oh, Neil, who are you?
[00:00:47.120 --> 00:00:49.040]   What do you do?
[00:00:49.040 --> 00:00:51.000]   How many ML projects have you done in your life?
[00:00:51.000 --> 00:00:52.960]   What qualifies you to give this talk?
[00:00:52.960 --> 00:00:54.920]   And the answer, of course, is nothing.
[00:00:54.920 --> 00:00:56.280]   I'm just friends with Josh.
[00:00:56.280 --> 00:00:58.800]   And so that's the only reason I'm here.
[00:00:58.800 --> 00:01:00.800]   I'm definitely the least qualified speaker
[00:01:00.800 --> 00:01:02.600]   you're going to have.
[00:01:02.600 --> 00:01:05.680]   But if you do want a little information on me,
[00:01:05.680 --> 00:01:07.200]   this is about me.
[00:01:07.200 --> 00:01:09.200]   This actually isn't really a timeline.
[00:01:09.200 --> 00:01:11.000]   I just was playing around with Google Slides
[00:01:11.000 --> 00:01:12.280]   and I like the graphic.
[00:01:12.280 --> 00:01:17.280]   And so the story I have is, actually in 2014,
[00:01:17.280 --> 00:01:21.040]   I was an intern on a team called Cerebra inside Google.
[00:01:21.040 --> 00:01:24.280]   And there I was working on 3D reconstruction and computer
[00:01:24.280 --> 00:01:24.780]   vision.
[00:01:24.780 --> 00:01:28.560]   So the idea is, you have two images of the same scene.
[00:01:28.560 --> 00:01:32.360]   Can you infer the underlying 3D structure?
[00:01:32.360 --> 00:01:33.780]   And then I returned back to Google
[00:01:33.780 --> 00:01:37.880]   and I worked what was then called the Applied Brain Team.
[00:01:37.880 --> 00:01:40.440]   And the idea was, there's all this stuff going on
[00:01:40.440 --> 00:01:41.720]   in deep learning.
[00:01:41.720 --> 00:01:45.000]   How do we actually make money off of it for Google?
[00:01:45.000 --> 00:01:47.680]   And so they deployed me on the really important problem
[00:01:47.680 --> 00:01:51.640]   of getting people to watch more videos on YouTube.
[00:01:51.640 --> 00:01:54.300]   And that was a really good experience
[00:01:54.300 --> 00:01:56.940]   in terms of learning how you take research and turn it
[00:01:56.940 --> 00:01:58.720]   into practical problems.
[00:01:58.720 --> 00:02:01.360]   But I really didn't like working in a big company.
[00:02:01.360 --> 00:02:04.480]   And so as Josh said, after I finished at Stanford,
[00:02:04.480 --> 00:02:06.480]   where I did both my undergrad and master's,
[00:02:06.480 --> 00:02:08.820]   I went and did the crazy thing of starting
[00:02:08.820 --> 00:02:10.800]   two health care companies.
[00:02:10.800 --> 00:02:13.040]   They both are machine learning projects.
[00:02:13.040 --> 00:02:15.760]   The first-- I'll give you guys one sentence on each
[00:02:15.760 --> 00:02:16.680]   because I have to.
[00:02:16.680 --> 00:02:17.520]   I'm an entrepreneur.
[00:02:17.520 --> 00:02:18.840]   I'm obliged.
[00:02:18.840 --> 00:02:21.520]   So we're using-- at Curai, we're using machine learning
[00:02:21.520 --> 00:02:24.500]   to try and make primary health care available to everyone
[00:02:24.500 --> 00:02:25.700]   through your phone.
[00:02:25.700 --> 00:02:28.180]   And at Totemic, we basically ask the question
[00:02:28.180 --> 00:02:30.600]   of, how do we build technology tools that
[00:02:30.600 --> 00:02:32.200]   enable the 100 million people who
[00:02:32.200 --> 00:02:36.580]   are going to be over the age of 65 to live independently?
[00:02:36.580 --> 00:02:39.000]   So if any of you take care of an older adult in your life,
[00:02:39.000 --> 00:02:40.740]   you know how much effort goes into that.
[00:02:40.740 --> 00:02:43.720]   And we've been using machine learning
[00:02:43.720 --> 00:02:45.660]   in a bunch of interesting ways, particularly
[00:02:45.660 --> 00:02:49.120]   around sensing and perception to try and keep old people safe
[00:02:49.120 --> 00:02:52.340]   and in their homes.
[00:02:52.340 --> 00:02:55.680]   So on to the meat of the talk.
[00:02:55.680 --> 00:02:57.940]   I think the most important thing that most people forget
[00:02:57.940 --> 00:03:00.100]   to do when they start every machine learning project
[00:03:00.100 --> 00:03:02.060]   is to frame their project.
[00:03:02.060 --> 00:03:04.900]   And so I always tell people, understand
[00:03:04.900 --> 00:03:06.140]   why you're doing the project.
[00:03:06.140 --> 00:03:07.300]   Are you doing it to learn?
[00:03:07.300 --> 00:03:09.340]   Are you doing it to start a company?
[00:03:09.340 --> 00:03:14.020]   Are you doing it to do a crazy science experiment
[00:03:14.020 --> 00:03:18.020]   or to get a job or just to work with friends?
[00:03:18.020 --> 00:03:21.260]   There are so many good reasons to start a machine learning
[00:03:21.260 --> 00:03:22.860]   company--
[00:03:22.860 --> 00:03:24.940]   or sorry, to work on a machine learning project--
[00:03:24.940 --> 00:03:27.100]   a Freudian slip.
[00:03:27.100 --> 00:03:30.100]   But you should be clear about which one of those this is.
[00:03:30.100 --> 00:03:33.420]   I think the interesting thing here is a lot of the times
[00:03:33.420 --> 00:03:35.700]   you start with one and you end up in another.
[00:03:35.700 --> 00:03:37.700]   So Waymo is a great example of this.
[00:03:37.700 --> 00:03:40.780]   This was a science project that they started inside Google.
[00:03:40.780 --> 00:03:42.700]   And then it turns out, oh, this could
[00:03:42.700 --> 00:03:45.340]   be a really large company.
[00:03:45.340 --> 00:03:46.960]   Usually, it works the other way around.
[00:03:46.960 --> 00:03:48.620]   So you start a machine learning company,
[00:03:48.620 --> 00:03:50.420]   and it turns out to just be a project.
[00:03:50.420 --> 00:03:55.540]   But sometimes you get lucky, and this happens.
[00:03:55.540 --> 00:03:57.820]   This talk, I'm mostly trying to help
[00:03:57.820 --> 00:04:00.520]   people who are trying to do applied machine learning.
[00:04:00.520 --> 00:04:03.020]   I think if you're trying to do science projects or research,
[00:04:03.020 --> 00:04:04.260]   that's fucking awesome.
[00:04:04.260 --> 00:04:07.860]   Sorry, am I allowed to swear?
[00:04:07.860 --> 00:04:11.420]   But the reality is then is that you're
[00:04:11.420 --> 00:04:13.500]   doing it for your own intellectual interest,
[00:04:13.500 --> 00:04:15.700]   or at least that's why you should be doing it.
[00:04:15.700 --> 00:04:19.160]   And my guidance isn't going to be as helpful.
[00:04:19.160 --> 00:04:23.300]   So the other thing I'll say is being clear
[00:04:23.300 --> 00:04:24.960]   about what your constraints are when
[00:04:24.960 --> 00:04:27.300]   you start a machine learning project is really important.
[00:04:27.300 --> 00:04:29.300]   And every-- or sorry, what your components are.
[00:04:29.300 --> 00:04:32.480]   Every machine learning project has all five of these.
[00:04:32.480 --> 00:04:33.960]   You need a problem statement.
[00:04:33.960 --> 00:04:35.960]   You need data.
[00:04:35.960 --> 00:04:38.040]   You need an approach.
[00:04:38.040 --> 00:04:40.000]   And you certainly need measures of success.
[00:04:40.000 --> 00:04:41.600]   This is something that a lot of people
[00:04:41.600 --> 00:04:44.260]   miss, which is like, OK, I'm going to go out there and build
[00:04:44.260 --> 00:04:45.480]   this classifier.
[00:04:45.480 --> 00:04:49.200]   How do I know that I did something good?
[00:04:49.200 --> 00:04:51.240]   You can spend your whole life-- and I'm
[00:04:51.240 --> 00:04:53.160]   sure Josh will tell you this-- you
[00:04:53.160 --> 00:04:55.240]   spend your whole life building different models
[00:04:55.240 --> 00:04:59.280]   and just watching some sort of accuracy score
[00:04:59.280 --> 00:05:01.360]   come out the other end of the pipeline.
[00:05:01.360 --> 00:05:03.040]   But is that your measure of success?
[00:05:03.040 --> 00:05:04.880]   And I think it's really important to come
[00:05:04.880 --> 00:05:06.160]   into these projects with that.
[00:05:06.160 --> 00:05:09.800]   And the last part is also really overlooked a lot,
[00:05:09.800 --> 00:05:12.640]   which is for most people, unless you're just
[00:05:12.640 --> 00:05:15.960]   doing this as a class project, try and understand
[00:05:15.960 --> 00:05:18.680]   what your iteration process and your feedback loop
[00:05:18.680 --> 00:05:20.640]   looks like so that you can get better.
[00:05:20.640 --> 00:05:22.640]   I mean, this is the word "learning" in machine
[00:05:22.640 --> 00:05:23.440]   learning.
[00:05:23.440 --> 00:05:27.600]   And whether that's just the process of science,
[00:05:27.600 --> 00:05:30.480]   like we make a hypothesis about a modeling technique,
[00:05:30.480 --> 00:05:33.920]   we try it, we see how it works, we improve,
[00:05:33.920 --> 00:05:37.640]   there's tons of methodology you can use for--
[00:05:37.640 --> 00:05:39.720]   and I'm not going to cover it in this talk--
[00:05:39.720 --> 00:05:43.720]   but having that process can be important.
[00:05:43.720 --> 00:05:48.320]   And so be clear about having all five of these components
[00:05:48.320 --> 00:05:51.120]   when you set out to start a project.
[00:05:51.120 --> 00:05:56.520]   Part two is how do you brainstorm ideas?
[00:05:56.520 --> 00:06:01.040]   This is really relevant, given what Josh just
[00:06:01.040 --> 00:06:01.840]   put on the screen.
[00:06:01.840 --> 00:06:04.920]   But if you don't want to do one of his really boring ideas,
[00:06:04.920 --> 00:06:08.160]   you should pay attention here.
[00:06:08.160 --> 00:06:10.840]   I'm going to focus this on how I generate ideas.
[00:06:10.840 --> 00:06:14.280]   This is an art, not a science.
[00:06:14.280 --> 00:06:16.360]   I've found that constraints are really helpful.
[00:06:16.360 --> 00:06:19.120]   So if you go back to these components,
[00:06:19.120 --> 00:06:20.680]   these are all constraints.
[00:06:20.680 --> 00:06:23.800]   If you need all these things, your project
[00:06:23.800 --> 00:06:26.000]   has to be designed in a way that has these things.
[00:06:26.000 --> 00:06:28.840]   And in design in general, constraints
[00:06:28.840 --> 00:06:30.880]   are really helpful because they reduce
[00:06:30.880 --> 00:06:32.640]   the number of possibilities.
[00:06:32.640 --> 00:06:34.560]   And they introduce problems that you have
[00:06:34.560 --> 00:06:37.480]   to creatively solve around.
[00:06:37.480 --> 00:06:39.880]   And so there's a couple of different ways
[00:06:39.880 --> 00:06:41.360]   I like to think of the constraints
[00:06:41.360 --> 00:06:43.520]   and how they can help you generate ideas.
[00:06:43.520 --> 00:06:45.120]   And the first one's the most intuitive,
[00:06:45.120 --> 00:06:47.840]   which is you take data and you generate a project
[00:06:47.840 --> 00:06:48.680]   idea for that.
[00:06:48.680 --> 00:06:50.520]   So this is the first machine learning project
[00:06:50.520 --> 00:06:52.600]   I think I ever did, which was--
[00:06:52.600 --> 00:06:54.840]   this is now through 2018, but this
[00:06:54.840 --> 00:06:57.800]   was a data set from 2009 to 2011--
[00:06:57.800 --> 00:07:01.280]   which was every football play ever run in the NFL.
[00:07:01.280 --> 00:07:02.800]   It was just a log.
[00:07:02.800 --> 00:07:05.400]   And it would say this play was a five-yard run to the right
[00:07:05.400 --> 00:07:08.280]   or a 10-yard run or a pass to the left.
[00:07:08.280 --> 00:07:10.880]   And I looked at this data set.
[00:07:10.880 --> 00:07:13.080]   And I was like, I like football.
[00:07:13.080 --> 00:07:14.520]   I like machine learning.
[00:07:14.520 --> 00:07:16.360]   I'll try and figure out something to do.
[00:07:16.360 --> 00:07:19.240]   And so I constructed a classifier
[00:07:19.240 --> 00:07:21.320]   that was logistic regression that
[00:07:21.320 --> 00:07:23.080]   could predict whether the next play was
[00:07:23.080 --> 00:07:24.680]   going to be a pass or a run.
[00:07:24.680 --> 00:07:28.520]   Totally contrived problem generated from a data set
[00:07:28.520 --> 00:07:31.800]   that existed, but a very valid way to come up with an idea.
[00:07:31.800 --> 00:07:33.720]   And really interesting, it turns out
[00:07:33.720 --> 00:07:36.320]   with very, very thin features, you
[00:07:36.320 --> 00:07:39.200]   can predict with 80-plus percent accuracy
[00:07:39.200 --> 00:07:40.840]   whether the next play in the NFL is
[00:07:40.840 --> 00:07:43.560]   going to be a pass or a run.
[00:07:43.560 --> 00:07:45.320]   So this is one method that I actually
[00:07:45.320 --> 00:07:47.000]   think is a pretty good one.
[00:07:47.000 --> 00:07:50.000]   And I linked the data set there if any of you like football.
[00:07:50.000 --> 00:07:53.360]   But that's one method I like.
[00:07:53.360 --> 00:07:58.120]   The second is algorithm to data to project.
[00:07:58.120 --> 00:08:00.880]   So this just introduces one extra constraint
[00:08:00.880 --> 00:08:03.080]   at the beginning, which is maybe you just
[00:08:03.080 --> 00:08:05.000]   want to learn about a particular algorithm
[00:08:05.000 --> 00:08:07.560]   or use a particular algorithm first.
[00:08:07.560 --> 00:08:09.400]   And so this was a project that I actually
[00:08:09.400 --> 00:08:11.400]   did in college, which was like, take
[00:08:11.400 --> 00:08:14.680]   a bunch of images of Zappos shoes and try and-- we
[00:08:14.680 --> 00:08:16.200]   called it Shoe-Zam.
[00:08:16.200 --> 00:08:18.760]   So it was like, you could take a picture of your shoes,
[00:08:18.760 --> 00:08:21.680]   and we'd show you shoes that looked like it,
[00:08:21.680 --> 00:08:25.200]   and hopefully then let you purchase them.
[00:08:25.200 --> 00:08:27.920]   This is actually a pretty good way to generate ideas, too.
[00:08:27.920 --> 00:08:30.720]   And the only reason we thought of this is we, at the time,
[00:08:30.720 --> 00:08:32.720]   it was probably like 2013 or '14,
[00:08:32.720 --> 00:08:37.040]   wanted to learn how to use convolutional neural nets.
[00:08:37.040 --> 00:08:39.960]   And so then we were like, OK, what are interesting images?
[00:08:39.960 --> 00:08:42.520]   And I'll actually talk about this later.
[00:08:42.520 --> 00:08:44.880]   But we didn't have this data set.
[00:08:44.880 --> 00:08:46.680]   This is released by University of Texas.
[00:08:46.680 --> 00:08:51.640]   So we ended up having to scrape all of Zappos for this data set
[00:08:51.640 --> 00:08:55.520]   and then we defined the project based on that.
[00:08:55.520 --> 00:08:58.680]   So that's another really great way to generate project ideas
[00:08:58.680 --> 00:09:00.960]   is starting with an algorithm.
[00:09:00.960 --> 00:09:05.160]   The third way I'll highlight is what I call process to project.
[00:09:05.160 --> 00:09:07.080]   And this is basically what Google
[00:09:07.080 --> 00:09:10.220]   did with everything they've ever done with search, which
[00:09:10.220 --> 00:09:13.480]   is you take a human process and things that are already
[00:09:13.480 --> 00:09:16.000]   occurring naturally, and then you try and turn that
[00:09:16.000 --> 00:09:19.040]   into some sort of prediction or automation challenge.
[00:09:19.040 --> 00:09:21.600]   So with PageRank, that started with all these people
[00:09:21.600 --> 00:09:24.160]   are putting these links on the web.
[00:09:24.160 --> 00:09:29.480]   How can we learn to rank relevance based on that?
[00:09:29.480 --> 00:09:34.640]   And their next insight, of course,
[00:09:34.640 --> 00:09:37.000]   was like when we serve content, people
[00:09:37.000 --> 00:09:39.880]   click-- when they enter a query, they click on different links.
[00:09:39.880 --> 00:09:43.400]   And we can learn to re-rank based on what they're
[00:09:43.400 --> 00:09:45.640]   clicking.
[00:09:45.640 --> 00:09:49.800]   And one really big advantage of this approach
[00:09:49.800 --> 00:09:51.420]   is that most people don't want to think
[00:09:51.420 --> 00:09:52.840]   this far outside of the box.
[00:09:52.840 --> 00:09:56.600]   And so these kinds of projects are far less frequently done.
[00:09:56.600 --> 00:09:58.700]   So if you want to do something unique,
[00:09:58.700 --> 00:10:01.240]   try and figure out where there are things going on
[00:10:01.240 --> 00:10:05.840]   in a public setting that you can observe or scrape or construct.
[00:10:05.840 --> 00:10:08.420]   There's plenty of places where you can create little games--
[00:10:08.420 --> 00:10:10.680]   and I wish I had better examples--
[00:10:10.680 --> 00:10:14.160]   and little meme toys on the internet.
[00:10:14.160 --> 00:10:16.040]   And people do a lot of things.
[00:10:16.040 --> 00:10:18.800]   You can record that data and turn it
[00:10:18.800 --> 00:10:20.480]   into a prediction challenge.
[00:10:20.480 --> 00:10:22.560]   There's one really interesting one from MIT
[00:10:22.560 --> 00:10:26.280]   that was like put a bunch of pictures
[00:10:26.280 --> 00:10:28.080]   of a bunch of urinals and asked people
[00:10:28.080 --> 00:10:30.000]   to click which one they would go to.
[00:10:30.000 --> 00:10:32.240]   And they tried to learn the social rules of which
[00:10:32.240 --> 00:10:35.040]   urinal you should stand in.
[00:10:35.040 --> 00:10:37.240]   I thought it was kind of interesting.
[00:10:37.240 --> 00:10:39.320]   The challenge here is you have to think creatively
[00:10:39.320 --> 00:10:41.000]   about how to generate that data set.
[00:10:41.000 --> 00:10:44.640]   And that requires more than just going on an ImageNet
[00:10:44.640 --> 00:10:46.760]   and just stealing the images and the labels.
[00:10:46.760 --> 00:10:49.280]   So those are the trade-offs.
[00:10:49.280 --> 00:10:51.800]   And then I'll quickly highlight the last one,
[00:10:51.800 --> 00:10:53.680]   which I call Promna Project.
[00:10:53.680 --> 00:10:55.680]   I think this is most interesting.
[00:10:55.680 --> 00:10:57.680]   This is what we're working on at Curai.
[00:10:57.680 --> 00:11:00.280]   So this is another shameless plug.
[00:11:00.280 --> 00:11:01.780]   Effectively, what we've done is we've
[00:11:01.780 --> 00:11:04.040]   taken a human problem, which is people
[00:11:04.040 --> 00:11:05.780]   don't have access to health care,
[00:11:05.780 --> 00:11:10.420]   and tried to formulate it as a machine learning project.
[00:11:10.420 --> 00:11:14.320]   So we now run about 1,000 consultations
[00:11:14.320 --> 00:11:16.840]   between physicians and patients every day.
[00:11:16.840 --> 00:11:20.880]   And the idea is can we structure information that's going on?
[00:11:20.880 --> 00:11:22.600]   This is just a chat.
[00:11:22.600 --> 00:11:25.640]   And turn it into a tractable machine learning problem.
[00:11:25.640 --> 00:11:29.640]   And that's in line with our mission.
[00:11:29.640 --> 00:11:31.680]   But there are plenty of other places
[00:11:31.680 --> 00:11:34.520]   where people have taken interesting approaches to say,
[00:11:34.520 --> 00:11:37.160]   how do you take a problem, solve it,
[00:11:37.160 --> 00:11:39.280]   and start to automate it over time?
[00:11:39.280 --> 00:11:43.600]   So I'm going to move on.
[00:11:43.600 --> 00:11:46.000]   Unless people have questions about that section,
[00:11:46.000 --> 00:11:49.200]   I'll talk about how you find data.
[00:11:49.200 --> 00:11:49.700]   Yeah?
[00:11:49.700 --> 00:11:53.320]   Did you try something different idea?
[00:11:53.320 --> 00:11:56.080]   Yes.
[00:11:56.080 --> 00:11:57.880]   No, I did.
[00:11:57.880 --> 00:12:00.880]   I actually, a long, long time ago, probably about 10 years
[00:12:00.880 --> 00:12:04.520]   ago now, had an internship in analytics with the 49ers.
[00:12:04.520 --> 00:12:06.720]   And so I tried to convince them to do it.
[00:12:06.720 --> 00:12:08.760]   It turns out you're not allowed to use computers
[00:12:08.760 --> 00:12:11.440]   on the sideline in the NFL.
[00:12:11.440 --> 00:12:13.960]   You're only allowed to look at still images on Microsoft
[00:12:13.960 --> 00:12:16.560]   surfaces, because Microsoft paid a lot of money
[00:12:16.560 --> 00:12:18.160]   to get that sponsorship.
[00:12:18.160 --> 00:12:20.460]   But they don't want any of the teams
[00:12:20.460 --> 00:12:22.320]   to have an analytics advantage in game.
[00:12:22.320 --> 00:12:29.260]   All right, how do I find data?
[00:12:29.260 --> 00:12:30.880]   Josh kind of talked about the first one,
[00:12:30.880 --> 00:12:32.060]   which is public data sets.
[00:12:32.060 --> 00:12:34.240]   They are everywhere.
[00:12:34.240 --> 00:12:35.140]   There's tons of these.
[00:12:35.140 --> 00:12:37.620]   They're popping up all over the place.
[00:12:37.620 --> 00:12:42.380]   The ones up here, this was a data set
[00:12:42.380 --> 00:12:47.540]   of chest x-rays released by NIH.
[00:12:47.540 --> 00:12:49.580]   Images and labels, I believe.
[00:12:49.580 --> 00:12:52.620]   There are self-driving data sets coming up.
[00:12:52.620 --> 00:12:54.300]   There's data sets on images.
[00:12:54.300 --> 00:12:58.100]   I mean, ImageNet is the classic one, but there's tons of others.
[00:12:58.100 --> 00:12:59.140]   They're interesting.
[00:12:59.140 --> 00:13:00.900]   They're everywhere.
[00:13:00.900 --> 00:13:04.560]   There's even large repositories on GitHub that gather them.
[00:13:04.560 --> 00:13:08.260]   So I linked one here, which just has,
[00:13:08.260 --> 00:13:09.860]   like, if you go through it, there's
[00:13:09.860 --> 00:13:12.540]   like 10,000 public data sets.
[00:13:12.540 --> 00:13:14.540]   And so these are interesting.
[00:13:14.540 --> 00:13:18.780]   As Josh said, people tend to have really
[00:13:18.780 --> 00:13:20.300]   mined the hell out of them.
[00:13:20.300 --> 00:13:23.260]   And so it's harder to come up with unique projects.
[00:13:23.260 --> 00:13:26.020]   But again, if you know your goal and your goal is just
[00:13:26.020 --> 00:13:28.900]   to learn or try something cool, you can do that.
[00:13:28.900 --> 00:13:32.620]   And that's a worthwhile endeavor.
[00:13:32.620 --> 00:13:35.380]   And kind of one step above that is company data sets.
[00:13:35.380 --> 00:13:37.980]   So the original one here was the Netflix Challenge,
[00:13:37.980 --> 00:13:39.580]   where they offered a million bucks
[00:13:39.580 --> 00:13:42.820]   to anybody who could improve their video recommendations.
[00:13:42.820 --> 00:13:44.900]   But this is now a thing.
[00:13:44.900 --> 00:13:51.100]   So Yelp has released metadata data on all their businesses.
[00:13:51.100 --> 00:13:53.140]   I tried to do a project a while back that was like,
[00:13:53.140 --> 00:13:58.420]   can you summarize a consensus opinion about a business given
[00:13:58.420 --> 00:14:01.100]   the set of reviews?
[00:14:01.100 --> 00:14:04.460]   Summarization turns out to be a pretty hard problem.
[00:14:04.460 --> 00:14:08.540]   But it is a really interesting data set.
[00:14:08.540 --> 00:14:12.100]   Amazon's releasing data sets from Alexa, Google's.
[00:14:12.100 --> 00:14:13.860]   This is their-- what is it called?
[00:14:13.860 --> 00:14:14.580]   YouTube 8M?
[00:14:14.580 --> 00:14:17.940]   Yeah, it's like 6 million videos from YouTube
[00:14:17.940 --> 00:14:19.340]   with some associated metadata.
[00:14:19.340 --> 00:14:21.060]   It's pretty cool.
[00:14:21.060 --> 00:14:22.940]   These data sets are all over the internet.
[00:14:22.940 --> 00:14:25.540]   Basically, if you just type your favorite company name
[00:14:25.540 --> 00:14:27.420]   and then data set, something will come up.
[00:14:27.420 --> 00:14:33.500]   And then, obviously, another great place is Kaggle.
[00:14:33.500 --> 00:14:35.300]   And this is like a hybrid between the two.
[00:14:35.300 --> 00:14:40.980]   But I'm assuming most of you are familiar with or have
[00:14:40.980 --> 00:14:43.260]   participated in Kaggle competitions.
[00:14:43.260 --> 00:14:45.020]   There's really good data on it.
[00:14:45.020 --> 00:14:46.660]   And there's no reason you have to be
[00:14:46.660 --> 00:14:47.740]   a part of the competition.
[00:14:47.740 --> 00:14:50.080]   You can just steal the data and do your own thing
[00:14:50.080 --> 00:14:51.500]   if you want to.
[00:14:51.500 --> 00:14:53.080]   It is a good way to get a job, though,
[00:14:53.080 --> 00:14:54.660]   if you want a job in machine learning.
[00:14:54.660 --> 00:14:57.340]   People do look at these things.
[00:14:57.340 --> 00:14:58.820]   That's the third category.
[00:14:58.820 --> 00:15:02.340]   And if you really get desperate, then you can just steal data.
[00:15:02.340 --> 00:15:04.420]   And that's like the scraping thing.
[00:15:04.420 --> 00:15:07.460]   And scraping is totally underrated
[00:15:07.460 --> 00:15:10.300]   because there is data everywhere on the web.
[00:15:10.300 --> 00:15:11.940]   And there's so much of it.
[00:15:11.940 --> 00:15:13.540]   Like, if you pay attention, there's
[00:15:13.540 --> 00:15:16.580]   tables and graphs and unstructured data
[00:15:16.580 --> 00:15:18.220]   and semi-structured-- like Wikipedia
[00:15:18.220 --> 00:15:19.580]   is full of semi-structured data.
[00:15:19.580 --> 00:15:20.900]   It's everywhere.
[00:15:20.900 --> 00:15:24.660]   This is a great way to get unique projects
[00:15:24.660 --> 00:15:28.140]   and unique data sets because most people don't want to do
[00:15:28.140 --> 00:15:31.140]   it, and scraping is really easy, too.
[00:15:31.140 --> 00:15:33.900]   So this is a great way to go do that data to projects thing.
[00:15:33.900 --> 00:15:38.500]   And yeah, if you get really desperate on ideas,
[00:15:38.500 --> 00:15:40.020]   you can also steal ideas.
[00:15:40.020 --> 00:15:44.540]   And so there are a bunch of Stanford classes in particular
[00:15:44.540 --> 00:15:47.380]   that have like 100 or 200 machine learning
[00:15:47.380 --> 00:15:49.700]   projects every three months.
[00:15:49.700 --> 00:15:53.060]   And you can just troll through those, see what people did.
[00:15:53.060 --> 00:15:54.140]   You get to see what works.
[00:15:54.140 --> 00:15:57.100]   So you get to know roughly how hard it is,
[00:15:57.100 --> 00:15:59.100]   which is a huge benefit.
[00:15:59.100 --> 00:16:02.260]   But this is a great way to get ideas.
[00:16:02.260 --> 00:16:04.340]   Conferences are also great for this, too.
[00:16:04.340 --> 00:16:06.460]   If you look at recent NIPS papers, which
[00:16:06.460 --> 00:16:08.940]   are more on the research end, but there are also
[00:16:08.940 --> 00:16:13.020]   more applied conferences like Rexis.
[00:16:13.020 --> 00:16:15.340]   There's ones in particular domains.
[00:16:15.340 --> 00:16:18.820]   So we publish in Machine Learning for Health Care.
[00:16:18.820 --> 00:16:21.260]   That's a more applied domain.
[00:16:21.260 --> 00:16:26.220]   So there's a bunch of interesting stuff there.
[00:16:26.220 --> 00:16:28.980]   And I'll highlight a couple of interesting areas
[00:16:28.980 --> 00:16:31.780]   in terms of project ideas, since Josh and I have
[00:16:31.780 --> 00:16:34.700]   had a lot of these conversations.
[00:16:34.700 --> 00:16:36.500]   If you're looking for inspiration,
[00:16:36.500 --> 00:16:38.740]   the first is more along the research lines
[00:16:38.740 --> 00:16:41.660]   is like reinforcement learning.
[00:16:41.660 --> 00:16:44.980]   Really, really great, exciting space.
[00:16:44.980 --> 00:16:49.700]   I think the pros are there's a lot of new environments coming
[00:16:49.700 --> 00:16:51.500]   up for reinforcement learning, whether it's
[00:16:51.500 --> 00:16:55.060]   Dota 2 or StarCraft 2 or Jim or Universe
[00:16:55.060 --> 00:16:57.340]   or any of these things.
[00:16:57.340 --> 00:16:59.260]   You can do really cool projects.
[00:16:59.260 --> 00:17:01.780]   They are very exciting.
[00:17:01.780 --> 00:17:05.580]   The challenges are you need a lot of compute, typically,
[00:17:05.580 --> 00:17:10.220]   and the algorithms are more on the edge.
[00:17:10.220 --> 00:17:13.140]   And so the trade-off with reinforcement learning
[00:17:13.140 --> 00:17:15.740]   is it really is more of a research project.
[00:17:15.740 --> 00:17:17.980]   But I do think there's a lot of really interesting work
[00:17:17.980 --> 00:17:18.480]   to be done.
[00:17:18.480 --> 00:17:21.220]   And obviously, robotics is another great area
[00:17:21.220 --> 00:17:23.380]   for reinforcement learning.
[00:17:23.380 --> 00:17:25.780]   A second area I am really excited about
[00:17:25.780 --> 00:17:27.860]   is unsupervised learning.
[00:17:27.860 --> 00:17:31.860]   But in particular, I think a lot of the most interesting
[00:17:31.860 --> 00:17:33.940]   unsupervised problems actually get formulated
[00:17:33.940 --> 00:17:35.140]   as supervised problems.
[00:17:35.140 --> 00:17:38.540]   So I'm assuming people here are familiar with Word2Vec.
[00:17:38.540 --> 00:17:39.580]   Yeah, roughly.
[00:17:39.580 --> 00:17:42.220]   This is an unsupervised data set formulated
[00:17:42.220 --> 00:17:43.820]   as a supervised problem, where you're
[00:17:43.820 --> 00:17:45.900]   trying to predict one word given the context.
[00:17:45.900 --> 00:17:51.980]   I think that's really a clever way.
[00:17:51.980 --> 00:17:56.620]   And again, if you can cleverly think about your data,
[00:17:56.620 --> 00:17:59.060]   you get to be less clever about your algorithms.
[00:17:59.060 --> 00:18:02.020]   And given that everybody else thinks about their models,
[00:18:02.020 --> 00:18:06.140]   it's a way to differentiate and do cool things.
[00:18:06.140 --> 00:18:09.900]   The challenges here-- one, obviously, the algorithms
[00:18:09.900 --> 00:18:12.940]   are less robust, especially for purely unsupervised learning.
[00:18:12.940 --> 00:18:20.540]   Even for supervised learning, measurement is really hard.
[00:18:20.540 --> 00:18:23.120]   What the hell does it mean to have trained a successful word
[00:18:23.120 --> 00:18:23.620]   vector?
[00:18:23.620 --> 00:18:24.980]   Nobody really knows.
[00:18:24.980 --> 00:18:27.260]   They kind of defined it as, can we
[00:18:27.260 --> 00:18:29.580]   do these mathematical operations on words?
[00:18:29.580 --> 00:18:32.260]   But if that didn't apply, would we say it was a failure?
[00:18:32.260 --> 00:18:34.780]   I don't know.
[00:18:34.780 --> 00:18:37.260]   So that's one of the challenges.
[00:18:37.260 --> 00:18:40.460]   Interestingly, these problems pop up everywhere.
[00:18:40.460 --> 00:18:42.660]   So the YouTube prediction problem
[00:18:42.660 --> 00:18:45.000]   that I worked on at Google was the same thing.
[00:18:45.000 --> 00:18:46.500]   They give you a history from a user,
[00:18:46.500 --> 00:18:49.740]   and they say, given the first five videos they watched,
[00:18:49.740 --> 00:18:50.980]   predict the sixth.
[00:18:50.980 --> 00:18:52.780]   And the sixth one you have the label for,
[00:18:52.780 --> 00:18:54.180]   because they actually watched it.
[00:18:54.180 --> 00:18:58.140]   And that's how you can predict a sequence of videos.
[00:18:58.140 --> 00:19:00.940]   So these problems are everywhere.
[00:19:00.940 --> 00:19:02.340]   And they're pretty interesting.
[00:19:02.340 --> 00:19:07.020]   And then the next area I'll talk about is long-term.
[00:19:07.020 --> 00:19:09.100]   And it's longer projects and probably
[00:19:09.100 --> 00:19:10.420]   beyond the scope of this class.
[00:19:10.420 --> 00:19:12.500]   But it's what I call data capture loops.
[00:19:12.500 --> 00:19:19.340]   And in particular, if you don't want to compete with Google
[00:19:19.340 --> 00:19:24.420]   and OpenAI, compete on data and not on compute and resources
[00:19:24.420 --> 00:19:25.660]   and engineers.
[00:19:25.660 --> 00:19:29.260]   And so if you want to do work that's new and original,
[00:19:29.260 --> 00:19:34.260]   think about how you can generate a data set from scratch.
[00:19:34.260 --> 00:19:38.900]   And this is something that I'm particularly bullish on.
[00:19:38.900 --> 00:19:39.740]   Is there anything?
[00:19:39.740 --> 00:19:41.340]   Oh, one thing I'll add.
[00:19:41.340 --> 00:19:43.740]   When you do this, you don't have to spend as much time
[00:19:43.740 --> 00:19:46.620]   cleaning data, which I think everybody prefers,
[00:19:46.620 --> 00:19:49.860]   because you structure it properly up front.
[00:19:49.860 --> 00:19:51.500]   But most machine learning scientists
[00:19:51.500 --> 00:19:53.460]   don't like spending time thinking about data.
[00:19:53.460 --> 00:19:55.660]   They just want to think about models.
[00:19:55.660 --> 00:19:59.780]   So this is-- and I would say, in my experience,
[00:19:59.780 --> 00:20:02.180]   our most effective machine learning researchers
[00:20:02.180 --> 00:20:07.220]   and engineers work a lot and think very hard
[00:20:07.220 --> 00:20:10.060]   about the end-to-end lifecycle of data.
[00:20:10.060 --> 00:20:12.820]   So it's like, we introduce an algorithm.
[00:20:12.820 --> 00:20:14.560]   This changes the data that comes out
[00:20:14.560 --> 00:20:16.440]   on the other end in this way.
[00:20:16.440 --> 00:20:18.380]   And then we feed that back in.
[00:20:18.380 --> 00:20:20.580]   And particularly, if you have human processes,
[00:20:20.580 --> 00:20:23.760]   like a user has to click thing or someone on your customer
[00:20:23.760 --> 00:20:26.180]   support team has to respond, most people
[00:20:26.180 --> 00:20:27.980]   don't want to deal with those complications.
[00:20:27.980 --> 00:20:31.660]   But it makes you infinitely more effective at getting products
[00:20:31.660 --> 00:20:32.860]   into use.
[00:20:32.860 --> 00:20:34.980]   So if you do want to be an applied machine learning
[00:20:34.980 --> 00:20:37.920]   researcher or engineer, I would highly
[00:20:37.920 --> 00:20:40.980]   suggest thinking about this.
[00:20:40.980 --> 00:20:43.540]   The last thing I'll say is a caveat to machine learning
[00:20:43.540 --> 00:20:47.020]   projects is, Andrew Ng has this thing about the machine
[00:20:47.020 --> 00:20:50.500]   learning hammer, which is, once you learn to classify things
[00:20:50.500 --> 00:20:53.580]   with math, you just want to hit everything.
[00:20:53.580 --> 00:20:55.460]   And not everything is a nail.
[00:20:55.460 --> 00:20:58.060]   So be careful.
[00:20:58.060 --> 00:21:01.060]   And this is why a lot of projects
[00:21:01.060 --> 00:21:04.820]   turn into dumb classifiers that don't do anything.
[00:21:04.820 --> 00:21:07.820]   And this is also why measures of success is really important.
[00:21:10.940 --> 00:21:15.100]   Any questions on that section, or should I keep moving?
[00:21:15.100 --> 00:21:19.020]   So you did a lot of medical projects.
[00:21:19.020 --> 00:21:19.900]   Yeah.
[00:21:19.900 --> 00:21:23.340]   Around getting data for medical purposes,
[00:21:23.340 --> 00:21:26.740]   did you have contracts with doctors, or how did you have--
[00:21:26.740 --> 00:21:28.340]   Sure.
[00:21:28.340 --> 00:21:30.860]   So the best way, I think, for people
[00:21:30.860 --> 00:21:32.820]   is to look for public data sets.
[00:21:32.820 --> 00:21:33.900]   And there's a lot of them.
[00:21:33.900 --> 00:21:38.780]   So-- did I include-- no, I didn't include that section.
[00:21:38.780 --> 00:21:40.300]   OK.
[00:21:40.300 --> 00:21:42.700]   There are a bunch of medical data
[00:21:42.700 --> 00:21:45.220]   sets that have been published by the federal government.
[00:21:45.220 --> 00:21:46.460]   There's one called NAMCS.
[00:21:46.460 --> 00:21:48.460]   There's one called MIMIC-II.
[00:21:48.460 --> 00:21:49.580]   There's ChexNet.
[00:21:49.580 --> 00:21:52.140]   I can share the list of them if they're interesting.
[00:21:52.140 --> 00:21:56.300]   They're all over the place, because it's really hot
[00:21:56.300 --> 00:21:58.060]   in medicine right now to try and figure out
[00:21:58.060 --> 00:22:01.380]   how to use machine learning.
[00:22:01.380 --> 00:22:03.820]   Health care medicine, in particular, is a tricky one,
[00:22:03.820 --> 00:22:06.940]   because you have to think really hard about data.
[00:22:06.940 --> 00:22:10.220]   So the example-- and ethics, I think, in particular.
[00:22:10.220 --> 00:22:14.900]   So the example I give is you have a data set about how long--
[00:22:14.900 --> 00:22:18.700]   given a set of symptoms that a patient comes in the emergency
[00:22:18.700 --> 00:22:22.180]   department with, whether or not they lived.
[00:22:22.180 --> 00:22:25.820]   And you build a classifier that says, OK, we can actually
[00:22:25.820 --> 00:22:28.060]   triage people when they walk in the door.
[00:22:28.060 --> 00:22:30.780]   And given what symptoms they have that the nurse gathers,
[00:22:30.780 --> 00:22:35.020]   we'll prioritize how long until the doctor sees them,
[00:22:35.020 --> 00:22:38.300]   based on how likely they are to survive.
[00:22:38.300 --> 00:22:42.180]   The challenge with that is that if you did that in a real ED
[00:22:42.180 --> 00:22:44.780]   today, in an emergency department, what you would
[00:22:44.780 --> 00:22:51.140]   learn is, oh, people who come in with chest pain often live.
[00:22:51.140 --> 00:22:53.380]   And the reason that is is because if you walk into an
[00:22:53.380 --> 00:22:56.220]   emergency department-- so if you ever want to be treated
[00:22:56.220 --> 00:22:58.340]   right away in an emergency department, just say, I have
[00:22:58.340 --> 00:22:59.420]   chest pain.
[00:22:59.420 --> 00:23:01.740]   And they will treat you right away, because they'll think
[00:23:01.740 --> 00:23:03.260]   you're having a heart attack.
[00:23:03.260 --> 00:23:07.300]   And so the problem with that classifier is that you learn
[00:23:07.300 --> 00:23:09.220]   to predict that people are more likely to live with
[00:23:09.220 --> 00:23:11.420]   chest pain, when it's really just that
[00:23:11.420 --> 00:23:13.100]   they get seen first.
[00:23:13.100 --> 00:23:15.700]   And so in medicine, you do have to be careful.
[00:23:15.700 --> 00:23:17.740]   And you also have to be careful about how it fits into
[00:23:17.740 --> 00:23:21.420]   the human processes and factors in health care.
[00:23:21.420 --> 00:23:23.420]   But the data sets are everywhere.
[00:23:23.420 --> 00:23:24.380]   You can--
[00:23:24.380 --> 00:23:27.500]   and there is a lot of proprietary data still.
[00:23:27.500 --> 00:23:28.780]   It's really hard to get.
[00:23:28.780 --> 00:23:32.620]   So I personally spent 18 months flying to Boston once a month
[00:23:32.620 --> 00:23:35.980]   to try and get data from a hospital system out there.
[00:23:35.980 --> 00:23:39.940]   But it was a pain in the ass.
[00:23:39.940 --> 00:23:42.140]   So I would suggest, if you're interested in health care,
[00:23:42.140 --> 00:23:45.180]   looking at public data sets.
[00:23:45.180 --> 00:23:47.740]   So if you're interested in one particular project that does
[00:23:47.740 --> 00:23:53.060]   not have public data sets, how easy would it be to get one
[00:23:53.060 --> 00:23:54.020]   doctor from a hospital?
[00:23:54.020 --> 00:23:57.260]   Is there existing relationships?
[00:23:57.260 --> 00:24:01.300]   Your best bet is to associate with a research university.
[00:24:01.300 --> 00:24:03.020]   I would not--
[00:24:03.020 --> 00:24:05.940]   if there isn't public data sets, and you're trying to do
[00:24:05.940 --> 00:24:08.340]   it for this class, is that the--
[00:24:08.340 --> 00:24:10.700]   how long is the class?
[00:24:10.700 --> 00:24:11.940]   Don't do it.
[00:24:11.940 --> 00:24:13.900]   [LAUGHTER]
[00:24:13.900 --> 00:24:15.900]   Can you speak a little about your experience with
[00:24:15.900 --> 00:24:18.020]   allergies and [INAUDIBLE]?
[00:24:18.020 --> 00:24:19.260]   Yes.
[00:24:19.260 --> 00:24:25.500]   A mechanical turk is absolutely terrible.
[00:24:25.500 --> 00:24:29.140]   That's my experience in one sentence.
[00:24:29.140 --> 00:24:31.860]   It is really, really hard.
[00:24:31.860 --> 00:24:35.540]   I think if you are trying to curate or annotate manually,
[00:24:35.540 --> 00:24:37.860]   it is hard to scale.
[00:24:37.860 --> 00:24:43.340]   So at Totemic, for example, we've used--
[00:24:43.340 --> 00:24:44.740]   is Lucas here?
[00:24:44.740 --> 00:24:47.260]   So we use Cloud Factory.
[00:24:47.260 --> 00:24:47.780]   I know.
[00:24:47.780 --> 00:24:49.380]   I'm sorry.
[00:24:49.380 --> 00:24:53.060]   And we've been through--
[00:24:53.060 --> 00:24:54.420]   it's a pain in the ass.
[00:24:54.420 --> 00:24:58.660]   And we looked at all of the annotation services.
[00:24:58.660 --> 00:25:01.900]   You can get started, and you can get somewhere.
[00:25:01.900 --> 00:25:04.860]   But if you really want to build a system at scale,
[00:25:04.860 --> 00:25:07.860]   you need to find a way to then get into a natural learning
[00:25:07.860 --> 00:25:09.620]   loop, right?
[00:25:09.620 --> 00:25:11.500]   Because it's more of a cold start problem,
[00:25:11.500 --> 00:25:12.660]   at least in my experience.
[00:25:12.660 --> 00:25:13.220]   I don't know.
[00:25:13.220 --> 00:25:14.420]   I don't want to offend.
[00:25:14.420 --> 00:25:22.260]   But I do think those services are much, much better
[00:25:22.260 --> 00:25:23.460]   than mechanical turk.
[00:25:23.460 --> 00:25:25.820]   They at least give you high quality annotations.
[00:25:25.820 --> 00:25:27.740]   I've had terrible, terrible experiences
[00:25:27.740 --> 00:25:31.100]   with mechanical turk for anything that's not really,
[00:25:31.100 --> 00:25:34.100]   really simple, like is this red or blue?
[00:25:34.100 --> 00:25:36.620]   If it's that simple, you can do it.
[00:25:36.620 --> 00:25:42.020]   If it even takes more than half a second of cognitive thinking,
[00:25:42.020 --> 00:25:42.900]   good luck.
[00:25:42.900 --> 00:25:45.540]   So what is the approximate time that you
[00:25:45.540 --> 00:25:48.940]   take to see classical and new labels,
[00:25:48.940 --> 00:25:55.660]   10,000 users using one of these services?
[00:25:55.660 --> 00:25:57.700]   Well, it depends how much you're willing to pay.
[00:25:57.700 --> 00:25:59.420]   10,000 images, they could do pretty fast.
[00:25:59.420 --> 00:26:01.180]   I mean, you guys have a much better expert.
[00:26:01.180 --> 00:26:02.980]   [INAUDIBLE]
[00:26:02.980 --> 00:26:05.060]   I don't have a number off the top of my head.
[00:26:05.060 --> 00:26:07.700]   [INAUDIBLE]
[00:26:07.700 --> 00:26:09.020]   Yeah, you could do that.
[00:26:09.020 --> 00:26:12.580]   I mean, have you ever tried to label images yourself?
[00:26:12.580 --> 00:26:14.260]   It's cumbersome.
[00:26:14.260 --> 00:26:16.700]   It is cumbersome, but I've done it.
[00:26:16.700 --> 00:26:18.860]   I've labeled thousands of images myself.
[00:26:18.860 --> 00:26:20.980]   Actually, for the original Zappos projects we did,
[00:26:20.980 --> 00:26:27.340]   I labeled about 10,000 images in about 12 hours.
[00:26:27.340 --> 00:26:29.420]   It was a pretty simple cognitive task.
[00:26:29.420 --> 00:26:32.300]   You just sit there, and you build a tool, and you hit--
[00:26:32.300 --> 00:26:35.700]   so does that answer the question?
[00:26:35.700 --> 00:26:38.140]   Yeah, except that if you label it yourself,
[00:26:38.140 --> 00:26:41.020]   your [INAUDIBLE]
[00:26:41.020 --> 00:26:42.660]   Sure.
[00:26:42.660 --> 00:26:44.420]   And it depends on what your goal is, right?
[00:26:44.420 --> 00:26:49.580]   All right, so selecting ideas.
[00:26:49.580 --> 00:26:51.740]   How do you select ideas?
[00:26:51.740 --> 00:26:54.060]   So we talked about the components.
[00:26:54.060 --> 00:26:56.900]   I think once you have a list of ideas,
[00:26:56.900 --> 00:26:58.820]   hopefully you have like 2 million ideas.
[00:26:58.820 --> 00:27:02.660]   You're like, how do I narrow these down?
[00:27:02.660 --> 00:27:06.140]   We've talked about data sets being a constraining factor.
[00:27:06.140 --> 00:27:08.100]   We've talked about timeline.
[00:27:08.100 --> 00:27:09.860]   If you have seven weeks to build a project,
[00:27:09.860 --> 00:27:12.100]   you should be realistic about what you can build.
[00:27:12.100 --> 00:27:15.260]   And we've talked about compute.
[00:27:15.260 --> 00:27:19.500]   You can't probably build OpenAI 5 in this class
[00:27:19.500 --> 00:27:23.260]   because you don't have enough GPUs.
[00:27:23.260 --> 00:27:26.580]   But I think the complexity of the problem is something that--
[00:27:26.580 --> 00:27:28.020]   and this came up in the question.
[00:27:28.020 --> 00:27:30.980]   It's like, how do you estimate the complexity of a problem?
[00:27:30.980 --> 00:27:34.940]   And so I have one heuristic that works really well for me
[00:27:34.940 --> 00:27:40.260]   for trying to estimate what's possible and what's hard.
[00:27:40.260 --> 00:27:42.620]   And that is the size of the state space.
[00:27:42.620 --> 00:27:46.740]   As a heuristic, if you have p possible inputs and q
[00:27:46.740 --> 00:27:50.900]   possible outputs, you have p cross q possible
[00:27:50.900 --> 00:27:52.980]   transformations.
[00:27:52.980 --> 00:27:57.380]   And so this, at some level-- and I'll get into some caveats.
[00:27:57.380 --> 00:28:01.020]   This has been a rough, rough mathematical equation
[00:28:01.020 --> 00:28:04.220]   that helps me estimate how hard a problem is.
[00:28:04.220 --> 00:28:07.780]   And this is not a precise mathematical formula.
[00:28:07.780 --> 00:28:11.940]   It ends up being an intuition because there
[00:28:11.940 --> 00:28:16.300]   are all sorts of complicating factors in this being--
[00:28:16.300 --> 00:28:19.100]   actually understanding the size of a state space.
[00:28:19.100 --> 00:28:22.100]   So as a practical matter, if you look at two examples,
[00:28:22.100 --> 00:28:24.780]   if you look at sentiment analysis and language modeling,
[00:28:24.780 --> 00:28:27.500]   sentiment analysis is like a pretty--
[00:28:27.500 --> 00:28:30.060]   there are pretty good models out there for it.
[00:28:30.060 --> 00:28:32.860]   And if you think about it, even though your input is
[00:28:32.860 --> 00:28:36.860]   like the sequence of potentially the entire English language,
[00:28:36.860 --> 00:28:39.180]   it's like an independent sentence.
[00:28:39.180 --> 00:28:42.140]   And you've got three possible output classes.
[00:28:42.140 --> 00:28:45.360]   Whereas when you try and solve language modeling,
[00:28:45.360 --> 00:28:48.180]   you could have paragraphs of dependencies.
[00:28:48.180 --> 00:28:49.720]   And it's the entire English language.
[00:28:49.720 --> 00:28:52.740]   And your next word is also the entire English language.
[00:28:52.740 --> 00:28:55.780]   And so even though these two problems are both natural
[00:28:55.780 --> 00:29:00.580]   language problems, they are very, very different
[00:29:00.580 --> 00:29:03.360]   in how hard they are.
[00:29:03.360 --> 00:29:07.260]   And this is why I say it's kind of a nuanced thing
[00:29:07.260 --> 00:29:12.780]   that you have to think carefully about to really get
[00:29:12.780 --> 00:29:14.180]   a sense of how to estimate it.
[00:29:14.180 --> 00:29:15.780]   And a lot of it comes with experience
[00:29:15.780 --> 00:29:17.220]   in thinking through these problems.
[00:29:17.220 --> 00:29:19.660]   I think to give a demonstration of why this is tricky,
[00:29:19.660 --> 00:29:23.020]   if you look at ImageNet, you might say, OK,
[00:29:23.020 --> 00:29:25.860]   the state space or the state-- the possible inputs
[00:29:25.860 --> 00:29:30.500]   is the height times the width of the image times 256 cubed,
[00:29:30.500 --> 00:29:32.700]   which is like you have your RGB channels.
[00:29:32.700 --> 00:29:34.180]   And that's the number of pixels.
[00:29:34.180 --> 00:29:37.580]   And so that's the total number of possible inputs you have.
[00:29:37.580 --> 00:29:39.180]   But that's not true.
[00:29:39.180 --> 00:29:41.020]   So images aren't random in ImageNet.
[00:29:41.020 --> 00:29:41.860]   They have structure.
[00:29:41.860 --> 00:29:43.900]   So if you have a picture of genes,
[00:29:43.900 --> 00:29:46.380]   you have a section of the image that's blue.
[00:29:46.380 --> 00:29:49.660]   And the problem is easier because it
[00:29:49.660 --> 00:29:51.940]   has these hidden constraints that
[00:29:51.940 --> 00:29:53.580]   reduce the size of the state space.
[00:29:53.580 --> 00:29:55.780]   And this is a great tip.
[00:29:55.780 --> 00:29:58.020]   Think about how you reduce the size of the state
[00:29:58.020 --> 00:30:02.660]   space of your problem by introducing constraints.
[00:30:02.660 --> 00:30:06.020]   And so if you think about GANs, for example,
[00:30:06.020 --> 00:30:07.740]   this is, at least in my opinion, part
[00:30:07.740 --> 00:30:10.980]   of what makes them possible and tractable.
[00:30:10.980 --> 00:30:13.180]   When you see GANs make mistakes, or at least early on,
[00:30:13.180 --> 00:30:16.140]   a lot of times it was like three eyes on a chicken.
[00:30:16.140 --> 00:30:20.700]   And it was just because it learned this structure of an eye
[00:30:20.700 --> 00:30:22.780]   and just replicated it many times.
[00:30:22.780 --> 00:30:25.900]   And it took more for it to learn the overall structure.
[00:30:25.900 --> 00:30:28.860]   And this is a problem in images in general.
[00:30:28.860 --> 00:30:32.380]   This is why random noise breaks ImageNet, at least
[00:30:32.380 --> 00:30:34.420]   is one of my hypotheses.
[00:30:34.420 --> 00:30:36.900]   You learn an underlying structure of the images.
[00:30:36.900 --> 00:30:39.700]   And as soon as you introduce imperceptible noise to you
[00:30:39.700 --> 00:30:42.500]   and I, you're breaking some of that structure.
[00:30:42.500 --> 00:30:46.780]   And so you get these funky-- what's
[00:30:46.780 --> 00:30:51.940]   the word I'm looking for-- these adversarial examples.
[00:30:51.940 --> 00:30:57.860]   So that's why I think about the size of the state space.
[00:30:57.860 --> 00:30:59.900]   I think the size of your data set
[00:30:59.900 --> 00:31:02.020]   actually matters quite a bit.
[00:31:02.020 --> 00:31:03.260]   And it's an added wrinkle.
[00:31:03.260 --> 00:31:06.460]   So if you have a million possible transformations
[00:31:06.460 --> 00:31:08.660]   in your state space, you've got a million data points,
[00:31:08.660 --> 00:31:11.980]   versus 1,000 and 100,000.
[00:31:11.980 --> 00:31:16.140]   Which of these problems do you think is going to be easier?
[00:31:16.140 --> 00:31:21.060]   And this is obviously why some problems are not solvable,
[00:31:21.060 --> 00:31:24.060]   yet we can solve OpenAI5 or robotics.
[00:31:24.060 --> 00:31:26.900]   Robotics is a great example of this, too.
[00:31:26.900 --> 00:31:29.500]   Your arm has all these actuators.
[00:31:29.500 --> 00:31:32.700]   It can move in a million billion different ways.
[00:31:32.700 --> 00:31:35.540]   And that's what makes the problem so hard.
[00:31:35.540 --> 00:31:38.180]   With enough data, they've shown that you can actually
[00:31:38.180 --> 00:31:40.540]   solve this reasonably well.
[00:31:40.540 --> 00:31:45.620]   And OpenAI5, to me, solves an impossible problem,
[00:31:45.620 --> 00:31:47.300]   if you really think of it.
[00:31:47.300 --> 00:31:49.460]   It makes no sense that this can be
[00:31:49.460 --> 00:31:51.460]   solved as some sort of policy.
[00:31:51.460 --> 00:31:53.300]   But it's just enough data.
[00:31:53.300 --> 00:31:56.420]   And there's, I think, a pretty reasonable theory
[00:31:56.420 --> 00:32:00.660]   that with enough data, you can solve-- or in simulation,
[00:32:00.660 --> 00:32:05.060]   even-- you can solve basically any current behavior
[00:32:05.060 --> 00:32:08.820]   or challenge with current methodologies.
[00:32:08.820 --> 00:32:13.860]   So I call the OpenAI5 thing the monkeys
[00:32:13.860 --> 00:32:15.860]   writing Shakespeare thing.
[00:32:15.860 --> 00:32:19.260]   Is that offensive?
[00:32:19.260 --> 00:32:21.780]   And eventually, they're going to hit Shakespeare.
[00:32:21.780 --> 00:32:23.380]   And that's what you have here.
[00:32:23.380 --> 00:32:25.980]   But it does highlight that these methods are a lot more
[00:32:25.980 --> 00:32:29.220]   powerful than we thought.
[00:32:29.220 --> 00:32:31.860]   So in summary, I do really believe
[00:32:31.860 --> 00:32:35.700]   that getting an intuition for the size of state space
[00:32:35.700 --> 00:32:38.180]   is the best way to estimate what's
[00:32:38.180 --> 00:32:41.500]   possible given your data set.
[00:32:41.500 --> 00:32:44.300]   Does this make sense to people?
[00:32:44.300 --> 00:32:46.860]   OK, I just totally lied to you.
[00:32:46.860 --> 00:32:51.540]   The actual best way is to look at research.
[00:32:51.540 --> 00:32:53.780]   This is the real shortcut.
[00:32:53.780 --> 00:32:56.540]   If you look at what other people have done,
[00:32:56.540 --> 00:32:58.340]   they've gone through the challenge
[00:32:58.340 --> 00:33:00.260]   of introducing these artificial constraints.
[00:33:00.260 --> 00:33:05.060]   So the original OpenAI5 was only certain heroes--
[00:33:05.060 --> 00:33:06.820]   I forget, they had a bunch of other rules
[00:33:06.820 --> 00:33:07.780]   that made it possible.
[00:33:07.780 --> 00:33:10.700]   Am I running over time?
[00:33:10.700 --> 00:33:11.540]   I'm almost done.
[00:33:11.540 --> 00:33:12.040]   But--
[00:33:12.040 --> 00:33:12.740]   You're OK.
[00:33:12.740 --> 00:33:15.100]   OK.
[00:33:15.100 --> 00:33:17.740]   So in general, this is a great way
[00:33:17.740 --> 00:33:20.340]   to learn what's actually possible given
[00:33:20.340 --> 00:33:22.700]   the newest methods.
[00:33:22.700 --> 00:33:25.740]   But actually, this is also a lie.
[00:33:25.740 --> 00:33:29.980]   The larger point is that you need to build intuition.
[00:33:29.980 --> 00:33:33.260]   And this is why machine learning scientists get paid so much.
[00:33:33.260 --> 00:33:36.060]   Because the same project that you're going to start on,
[00:33:36.060 --> 00:33:39.100]   even though you have the same base level of knowledge,
[00:33:39.100 --> 00:33:41.260]   you're going to take way more time than Jeff Dean is.
[00:33:41.260 --> 00:33:43.300]   And that's because he has a really good intuition
[00:33:43.300 --> 00:33:46.180]   for approaches and how to constrain the problem
[00:33:46.180 --> 00:33:48.780]   and how to attack it.
[00:33:48.780 --> 00:33:52.380]   There's a guy I work with who's like, nowadays, anybody
[00:33:52.380 --> 00:33:54.660]   trains a model in TensorFlow and throws up a TensorBoard
[00:33:54.660 --> 00:33:56.740]   and calls themselves a machine learning scientist,
[00:33:56.740 --> 00:33:59.340]   and he gets really offended when people do that.
[00:33:59.340 --> 00:34:07.340]   And I think that the nuance comes in this expertise.
[00:34:07.340 --> 00:34:09.260]   And so hopefully, these heuristics
[00:34:09.260 --> 00:34:10.540]   can actually help, though.
[00:34:10.540 --> 00:34:14.820]   They are very helpful for me, personally.
[00:34:14.820 --> 00:34:16.580]   But as much as possible, it's really
[00:34:16.580 --> 00:34:18.580]   about trying to understand what worked and didn't
[00:34:18.580 --> 00:34:22.060]   for other people's specific approaches.
[00:34:22.060 --> 00:34:24.380]   And then quickly-- I'll go quickly and get to questions.
[00:34:24.380 --> 00:34:28.100]   Part four on managing projects, once you've started them.
[00:34:28.100 --> 00:34:30.060]   The first thing is, go end to end first.
[00:34:30.060 --> 00:34:31.780]   And Josh talked about this.
[00:34:31.780 --> 00:34:35.020]   Get the crappiest classifier in the world working first,
[00:34:35.020 --> 00:34:37.780]   so that you can modularly look at the different parts
[00:34:37.780 --> 00:34:38.740]   and then improve it.
[00:34:38.740 --> 00:34:40.660]   This is the number one thing I would suggest.
[00:34:40.660 --> 00:34:43.460]   If you walk away with nothing else from this talk that
[00:34:43.460 --> 00:34:46.260]   will help you in this class, please, for the love of God,
[00:34:46.260 --> 00:34:49.660]   make your project end to end, and then start improving it.
[00:34:49.660 --> 00:34:51.860]   Don't spend three weeks trying to build models,
[00:34:51.860 --> 00:34:55.580]   and then get to week six and be like, oh, shit,
[00:34:55.580 --> 00:34:59.460]   nothing works, and the data's completely broken.
[00:34:59.460 --> 00:35:01.900]   Tip number two, don't just build a classifier.
[00:35:01.900 --> 00:35:04.540]   And this goes back to your success metrics.
[00:35:04.540 --> 00:35:08.100]   Don't just build a little toy, and then not benchmark it
[00:35:08.100 --> 00:35:11.220]   versus other approaches.
[00:35:11.220 --> 00:35:14.020]   Not think about whether or not it's successful.
[00:35:14.020 --> 00:35:16.340]   If you're thinking about real world applications,
[00:35:16.340 --> 00:35:18.480]   your metrics are probably going to lie.
[00:35:18.480 --> 00:35:24.220]   So 100% classification accuracy on ImageNet
[00:35:24.220 --> 00:35:26.980]   does not mean you can take a picture of anything
[00:35:26.980 --> 00:35:28.980]   and classify it perfectly.
[00:35:28.980 --> 00:35:31.460]   Metrics lie everywhere in machine learning,
[00:35:31.460 --> 00:35:34.900]   and so be careful when you think about that.
[00:35:34.900 --> 00:35:38.780]   And then my last thing is build teams, work together.
[00:35:38.780 --> 00:35:41.260]   As I say, if Jeff Dean pair programs,
[00:35:41.260 --> 00:35:44.460]   you're not too good to do it.
[00:35:44.460 --> 00:35:46.380]   And they really help you sanity check.
[00:35:46.380 --> 00:35:49.300]   This is totally underrated by most machine learning
[00:35:49.300 --> 00:35:51.060]   scientists, in my opinion.
[00:35:51.060 --> 00:35:58.260]   And not everyone is going to do the glory modeling work.
[00:35:58.260 --> 00:36:00.260]   You're going to have to do all the parts, which
[00:36:00.260 --> 00:36:02.860]   is data cleaning, pipeline, benchmarking,
[00:36:02.860 --> 00:36:05.060]   if you're trying to productionize it in any way.
[00:36:05.060 --> 00:36:07.020]   These all take a lot of work.
[00:36:07.020 --> 00:36:08.860]   Split it up, work together on these things.
[00:36:08.860 --> 00:36:14.780]   So yeah, we're hiring.
[00:36:14.780 --> 00:36:16.580]   Yeah, I have to say this.
[00:36:16.580 --> 00:36:19.420]   So when you finish this course, come talk to me.
[00:36:19.420 --> 00:36:24.220]   I'm happy to share these slides, and you can click on the email.
[00:36:24.220 --> 00:36:24.720]   Questions?
[00:36:24.720 --> 00:36:33.500]   Also related to what the stuff you said about data gathering,
[00:36:33.500 --> 00:36:35.500]   so you talk about the capital TARP on one end,
[00:36:35.500 --> 00:36:38.060]   and then you think about the problem well enough
[00:36:38.060 --> 00:36:41.860]   to create supervised data out of unsupervised,
[00:36:41.860 --> 00:36:42.940]   the other ideal.
[00:36:42.940 --> 00:36:45.140]   And maybe in the middle, have you
[00:36:45.140 --> 00:36:50.100]   had experience with trying to use active learning
[00:36:50.100 --> 00:36:52.100]   to speed this up?
[00:36:52.100 --> 00:36:55.580]   Yeah, and that's where I think the human labeling
[00:36:55.580 --> 00:37:02.340]   services like CrowdFlower and CloudFactory, they do best.
[00:37:02.340 --> 00:37:07.900]   What we found is you can significantly speed things up,
[00:37:07.900 --> 00:37:10.820]   but that also is a process.
[00:37:10.820 --> 00:37:12.140]   None of that happens overnight.
[00:37:13.140 --> 00:37:14.140]   Other questions?
[00:37:14.140 --> 00:37:15.420]   I know everyone wants to leave.
[00:37:15.420 --> 00:37:15.900]   It's 8:30.
[00:37:15.900 --> 00:37:16.400]   So--
[00:37:16.400 --> 00:37:23.380]   You said you work on the YouTube application system.
[00:37:23.380 --> 00:37:24.380]   Yeah.
[00:37:24.380 --> 00:37:29.860]   So could you please explain why there's a tendency for YouTube
[00:37:29.860 --> 00:37:31.860]   to send you--
[00:37:31.860 --> 00:37:34.180]   I don't get to take that much credit for that.
[00:37:34.180 --> 00:37:38.620]   I think it's actually very simple.
[00:37:38.620 --> 00:37:39.700]   It's what people click on.
[00:37:39.700 --> 00:37:41.420]   I think it's actually very simple.
[00:37:41.420 --> 00:37:43.220]   It's what people click on.
[00:37:43.220 --> 00:37:45.220]   If you think about the way the model is trained,
[00:37:45.220 --> 00:37:48.100]   it's learned what people click on.
[00:37:48.100 --> 00:37:52.700]   And if you say you're not clicking on the conspiracy
[00:37:52.700 --> 00:37:54.220]   theory video, you're just lying.
[00:37:54.220 --> 00:38:00.900]   All right.
[00:38:00.900 --> 00:38:02.500]   That's all I got.
[00:38:02.500 --> 00:38:04.260]   Thanks, folks.
[00:38:04.260 --> 00:38:06.700]   [APPLAUSE]

