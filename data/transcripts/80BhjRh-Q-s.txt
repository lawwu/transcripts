
[00:00:00.000 --> 00:00:03.380]   The big future AI in the singularity looks back and says,
[00:00:03.380 --> 00:00:06.040]   "Hey, who gets the most credit for this genomics revolution
[00:00:06.040 --> 00:00:07.800]   that happened in the early 21st century?"
[00:00:07.800 --> 00:00:10.120]   That AI is gonna find these papers on the archive
[00:00:10.120 --> 00:00:12.040]   in which we prove this is possible.
[00:00:12.040 --> 00:00:14.040]   - What advice did Richard Feynman give you
[00:00:14.040 --> 00:00:15.220]   about picking up girls?
[00:00:15.220 --> 00:00:21.200]   - It's very funny because most wokest people today
[00:00:21.200 --> 00:00:23.000]   hate this stuff.
[00:00:23.000 --> 00:00:27.380]   But most progressives like Margaret Sanger or, you know,
[00:00:27.380 --> 00:00:30.380]   well, in some sense, the forebears of today's wokest,
[00:00:30.380 --> 00:00:31.560]   in the early 20th century,
[00:00:31.560 --> 00:00:34.560]   they were all what we would call today eugenicists.
[00:00:34.560 --> 00:00:36.880]   - Today, I have the pleasure of speaking with Steve Hsu.
[00:00:36.880 --> 00:00:38.180]   Steve, thanks for coming on the podcast.
[00:00:38.180 --> 00:00:39.340]   I'm excited about this.
[00:00:39.340 --> 00:00:40.180]   - Hey, it's my pleasure.
[00:00:40.180 --> 00:00:41.020]   I'm excited too.
[00:00:41.020 --> 00:00:41.840]   And I just wanna say,
[00:00:41.840 --> 00:00:44.680]   I've listened to some of your earlier interviews
[00:00:44.680 --> 00:00:46.140]   and thought you were very insightful,
[00:00:46.140 --> 00:00:47.340]   which is why I was really excited
[00:00:47.340 --> 00:00:49.260]   to have a conversation with you.
[00:00:49.260 --> 00:00:50.980]   - That means a lot for me to hear you say
[00:00:50.980 --> 00:00:53.060]   because I'm a big fan of your podcast.
[00:00:53.060 --> 00:00:54.260]   My first question is,
[00:00:54.260 --> 00:00:56.260]   what advice did Richard Feynman give you
[00:00:56.260 --> 00:00:57.480]   about picking up girls?
[00:00:57.480 --> 00:01:00.180]   - Wow.
[00:01:00.180 --> 00:01:05.520]   So one day in the spring of my senior year,
[00:01:05.520 --> 00:01:07.760]   I was walking across campus
[00:01:07.760 --> 00:01:10.440]   and I see Feynman coming toward me
[00:01:10.440 --> 00:01:13.440]   and we knew each other from various things.
[00:01:13.440 --> 00:01:15.220]   And it's a small campus.
[00:01:15.220 --> 00:01:17.060]   And I was a physics major and he was my hero.
[00:01:17.060 --> 00:01:19.560]   So I guess I had known him since my freshman year.
[00:01:19.560 --> 00:01:23.020]   So he sees me and, you know, he's got this,
[00:01:23.020 --> 00:01:25.020]   I don't know if it's long, I guess it's a Long Island
[00:01:25.020 --> 00:01:27.480]   or it's some kind of New York borough accent.
[00:01:27.480 --> 00:01:31.260]   And he says, "Hey Shu."
[00:01:31.260 --> 00:01:33.440]   This is how he says my name, "Hey Shu."
[00:01:33.440 --> 00:01:35.860]   And I'm like, "Hi, Professor Feynman."
[00:01:35.860 --> 00:01:39.180]   And so we start talking.
[00:01:39.180 --> 00:01:43.760]   And he says to me, "Wow, you're kind of a big guy."
[00:01:43.760 --> 00:01:45.980]   And I was a lot bigger then 'cause I played on the,
[00:01:45.980 --> 00:01:48.220]   I was a linebacker on the Celtic football team.
[00:01:48.220 --> 00:01:51.060]   So I was about almost 200 pounds.
[00:01:51.060 --> 00:01:53.400]   I'm just over six feet tall.
[00:01:53.400 --> 00:01:57.020]   And so I was pretty like a gym rat at that time.
[00:01:57.020 --> 00:02:00.060]   And so he was like, I was much bigger than him, obviously.
[00:02:00.060 --> 00:02:03.600]   He was like, "Wow, you're a big guy, Steve.
[00:02:03.600 --> 00:02:06.100]   I gotta ask you something."
[00:02:06.100 --> 00:02:08.260]   And Feynman was born in like 1918.
[00:02:08.260 --> 00:02:11.180]   So he's not really like from the modern era.
[00:02:11.180 --> 00:02:13.900]   Like he was, I guess he was going through graduate school
[00:02:13.900 --> 00:02:16.640]   when the Second World War started.
[00:02:16.640 --> 00:02:21.140]   And so to him, the whole concept of a health club,
[00:02:21.140 --> 00:02:24.720]   a gym was like totally, couldn't understand it.
[00:02:24.720 --> 00:02:26.800]   And that was the era, this was the '80s.
[00:02:26.800 --> 00:02:28.460]   So that was the era when Gold's Gym
[00:02:28.460 --> 00:02:30.840]   was like becoming a world national franchise.
[00:02:30.840 --> 00:02:32.460]   And so there were gyms all over the place,
[00:02:32.460 --> 00:02:34.760]   24 hour fitness and stuff like this.
[00:02:34.760 --> 00:02:35.920]   So he didn't know what it was.
[00:02:35.920 --> 00:02:37.800]   And he's a very interesting guy.
[00:02:37.800 --> 00:02:40.260]   So his suspicion, he says to me,
[00:02:40.260 --> 00:02:42.380]   "What do you guys do there?
[00:02:42.380 --> 00:02:45.720]   Is it just a thing to meet chicks, to meet girls?
[00:02:45.720 --> 00:02:48.960]   Or do you guys actually, is it really for training?
[00:02:48.960 --> 00:02:51.740]   Do you guys really go there to get buff, to get big?"
[00:02:51.740 --> 00:02:53.280]   And so I started explaining to him.
[00:02:53.280 --> 00:02:56.360]   I said, "Yes, people are there to get big,
[00:02:56.360 --> 00:02:58.320]   but people are also checking out the girls.
[00:02:58.320 --> 00:03:00.120]   And there is a lot of stuff happening
[00:03:00.120 --> 00:03:03.620]   at the health club or in the weight room."
[00:03:03.620 --> 00:03:06.360]   And so he grills me on this for a long time.
[00:03:06.360 --> 00:03:08.580]   And one of the famous things about Feynman
[00:03:08.580 --> 00:03:11.320]   is that he has this laser-like focus.
[00:03:11.320 --> 00:03:13.320]   So if there's something he really doesn't understand
[00:03:13.320 --> 00:03:15.580]   and he wants to get to the bottom of it,
[00:03:15.580 --> 00:03:18.860]   he will just focus in on you and just start questioning you
[00:03:18.860 --> 00:03:19.860]   and get to the bottom of it.
[00:03:19.860 --> 00:03:21.180]   That's the way his brain works.
[00:03:21.180 --> 00:03:23.220]   So he did that to me for like, I don't know how long.
[00:03:23.220 --> 00:03:26.340]   We were talking about lifting weights and everything,
[00:03:26.340 --> 00:03:27.520]   'cause he didn't know anything about it.
[00:03:27.520 --> 00:03:30.800]   And at the end, he says to me,
[00:03:30.800 --> 00:03:32.800]   "Wow, Steve, I really appreciate that.
[00:03:32.800 --> 00:03:37.980]   Let me give you some good advice."
[00:03:37.980 --> 00:03:42.980]   And so then he starts telling me about how to pick up girls,
[00:03:42.980 --> 00:03:47.420]   and which I guess he's kind of an expert on.
[00:03:47.420 --> 00:03:49.500]   And he says to me, he goes,
[00:03:49.500 --> 00:03:51.560]   one of the things he says to me is like,
[00:03:51.560 --> 00:03:53.820]   "I don't know how much girls really like guys
[00:03:53.820 --> 00:03:55.180]   that are as big as you."
[00:03:55.180 --> 00:03:58.820]   He thought it might be a turnoff, actually.
[00:03:58.820 --> 00:04:00.340]   And he said, "But you know what?
[00:04:00.340 --> 00:04:02.260]   You have a nice smile."
[00:04:02.260 --> 00:04:05.180]   So that was the one compliment he gives me.
[00:04:05.180 --> 00:04:06.940]   "You have a nice smile."
[00:04:06.940 --> 00:04:08.080]   And then he starts telling me, he says,
[00:04:08.080 --> 00:04:12.660]   "You know, the main thing is it's a numbers game, okay?
[00:04:12.660 --> 00:04:14.760]   You have to divorce your,
[00:04:14.760 --> 00:04:16.980]   you have to be totally rational about it.
[00:04:16.980 --> 00:04:20.220]   You're never gonna see that girl again, right?
[00:04:20.220 --> 00:04:24.140]   You're in an airport lounge, or you're at a bar,
[00:04:24.140 --> 00:04:28.060]   it's Saturday night in Pasadena or Westwood,
[00:04:28.060 --> 00:04:30.100]   and you're talking to some girl."
[00:04:30.100 --> 00:04:33.700]   And he says, "You're never gonna see her again.
[00:04:33.700 --> 00:04:35.820]   This is your one interaction with her,
[00:04:35.820 --> 00:04:37.900]   five-minute interaction.
[00:04:37.900 --> 00:04:39.940]   Do what you have to do.
[00:04:39.940 --> 00:04:42.620]   And if she, for some reason, doesn't like you,
[00:04:42.620 --> 00:04:43.860]   just go to the next one."
[00:04:43.860 --> 00:04:46.420]   And that's what he says.
[00:04:46.420 --> 00:04:51.420]   So, you know, and he gives some colorful details and stuff.
[00:04:51.420 --> 00:04:52.880]   But the point is, he's like,
[00:04:52.880 --> 00:04:54.660]   "You should not care what they think of you.
[00:04:54.660 --> 00:04:56.260]   You're trying to do your thing."
[00:04:56.260 --> 00:04:57.820]   And you know, he's a pretty,
[00:04:57.820 --> 00:05:00.540]   he had a kind of a reputation at Caltech as a womanizer.
[00:05:00.540 --> 00:05:02.020]   I could go into that too,
[00:05:02.020 --> 00:05:04.820]   but I heard this from the secretaries and stuff.
[00:05:04.820 --> 00:05:06.460]   But-
[00:05:06.460 --> 00:05:07.940]   - With the students or with like-
[00:05:07.940 --> 00:05:10.820]   - No, no, with secretaries, mostly secretaries,
[00:05:10.820 --> 00:05:12.980]   who were almost all female at that time.
[00:05:12.980 --> 00:05:15.100]   He had thought about this a lot.
[00:05:15.100 --> 00:05:17.680]   And he was just like, "Look, it's a numbers game.
[00:05:17.680 --> 00:05:20.100]   Just, I guess the PUA type,
[00:05:20.100 --> 00:05:22.220]   are you familiar with PUA culture?"
[00:05:22.220 --> 00:05:23.260]   - Yeah, yeah, yeah.
[00:05:23.260 --> 00:05:24.960]   - So the PUA guys would say like,
[00:05:24.960 --> 00:05:27.200]   "Yeah, don't, you know, it's like a operation.
[00:05:27.200 --> 00:05:28.620]   Like you're just doing something.
[00:05:28.620 --> 00:05:31.880]   You follow the algorithm and whatever happens,
[00:05:31.880 --> 00:05:34.860]   it's not a reflection on your self-esteem
[00:05:34.860 --> 00:05:36.380]   or your internal self-image.
[00:05:36.380 --> 00:05:38.020]   It's just, that's what happened.
[00:05:38.020 --> 00:05:39.620]   And you just go on to the next one."
[00:05:39.620 --> 00:05:42.860]   And that was basically the advice he was giving me.
[00:05:42.860 --> 00:05:43.820]   You know, and he said other things
[00:05:43.820 --> 00:05:44.640]   which were pretty standard.
[00:05:44.640 --> 00:05:45.660]   Like, "You know, be funny.
[00:05:45.660 --> 00:05:46.620]   You're a funny guy.
[00:05:46.620 --> 00:05:47.660]   You know, girls like that.
[00:05:47.660 --> 00:05:48.500]   Be confident."
[00:05:48.500 --> 00:05:49.940]   You know, just basic stuff.
[00:05:49.940 --> 00:05:53.340]   But the main thing I remember was the operationalization
[00:05:53.340 --> 00:05:54.580]   of it as an algorithm.
[00:05:54.580 --> 00:05:57.260]   And that you should just not internalize
[00:05:57.260 --> 00:05:58.820]   whatever happens if you get rejected.
[00:05:58.820 --> 00:05:59.700]   'Cause that's what really hurts.
[00:05:59.700 --> 00:06:01.060]   That you know you're a guy, right?
[00:06:01.060 --> 00:06:04.240]   When you go across the bar to talk to that girl,
[00:06:04.240 --> 00:06:05.940]   maybe that doesn't happen in your generation.
[00:06:05.940 --> 00:06:07.180]   Maybe you just like swipe.
[00:06:07.180 --> 00:06:09.700]   But we had to go, it was terrifying.
[00:06:09.700 --> 00:06:13.740]   We had to go across the bar and talk to some lady
[00:06:13.740 --> 00:06:17.180]   and it's loud and you got like a few minutes
[00:06:17.180 --> 00:06:19.300]   to make your case basically.
[00:06:19.300 --> 00:06:23.440]   And nothing hurts more and nothing is more scary
[00:06:23.440 --> 00:06:26.260]   than walking across up to the girl,
[00:06:26.260 --> 00:06:28.860]   maybe she and her friends or something, right?
[00:06:28.860 --> 00:06:30.020]   So he was just saying like,
[00:06:30.020 --> 00:06:31.700]   you gotta train yourself out of that.
[00:06:31.700 --> 00:06:33.260]   Like, you're never gonna see them again.
[00:06:33.260 --> 00:06:36.580]   This face space of humanity is so big,
[00:06:36.580 --> 00:06:38.260]   you'll never encounter them again.
[00:06:38.260 --> 00:06:39.300]   And it just doesn't matter.
[00:06:39.300 --> 00:06:40.460]   So just do your best.
[00:06:40.460 --> 00:06:42.260]   - Yeah, that's interesting.
[00:06:42.260 --> 00:06:43.940]   Because I wonder when he was,
[00:06:43.940 --> 00:06:46.140]   I mean, in the 40s, when he was at that age,
[00:06:46.140 --> 00:06:46.980]   was he doing this?
[00:06:46.980 --> 00:06:49.020]   I don't know what the cultural conventions were at the time,
[00:06:49.020 --> 00:06:52.100]   but I don't know, were there bars in the 40s
[00:06:52.100 --> 00:06:53.940]   where you could just go hit on girls or?
[00:06:53.940 --> 00:06:55.740]   - Oh yeah, absolutely, absolutely.
[00:06:55.740 --> 00:06:57.700]   I mean, if you read literature from that time
[00:06:57.700 --> 00:06:59.100]   or even a little bit earlier,
[00:06:59.100 --> 00:07:02.260]   like Hemingway or John O'Hara or, you know,
[00:07:02.260 --> 00:07:05.140]   they talk about, you know, how men and women interacted
[00:07:05.140 --> 00:07:08.700]   in bars and stuff like this and in New York City and,
[00:07:08.700 --> 00:07:11.300]   you know, yeah, so that was a thing.
[00:07:11.300 --> 00:07:12.540]   That was much more of a thing
[00:07:12.540 --> 00:07:14.140]   than I think for your generation.
[00:07:14.140 --> 00:07:16.060]   That's what I can't figure out with my kids.
[00:07:16.060 --> 00:07:17.060]   Like, what is going on?
[00:07:17.060 --> 00:07:20.180]   Like, how do boys and girls meet these days?
[00:07:20.180 --> 00:07:21.940]   But back in the day, it was like,
[00:07:21.940 --> 00:07:24.300]   the guy had to do all the work
[00:07:24.300 --> 00:07:27.940]   and it was like the most terrifying thing you could do.
[00:07:27.940 --> 00:07:30.460]   And, you know, and you just have
[00:07:30.460 --> 00:07:32.220]   to train yourself out of that.
[00:07:32.220 --> 00:07:33.060]   - Right.
[00:07:33.060 --> 00:07:35.700]   By the way, when, for the context for the audience,
[00:07:35.700 --> 00:07:37.500]   when about five minutes as you were a big guy,
[00:07:37.500 --> 00:07:39.740]   like you were a football player at Caltech, right?
[00:07:39.740 --> 00:07:42.980]   And then there's a picture of you actually on your website
[00:07:42.980 --> 00:07:45.820]   where maybe this was after college or something,
[00:07:45.820 --> 00:07:48.340]   but yeah, you look like pretty ripped.
[00:07:48.340 --> 00:07:50.700]   And it's kind of, I mean,
[00:07:50.700 --> 00:07:52.940]   today it seems more common because of gym culture and stuff,
[00:07:52.940 --> 00:07:54.220]   but I don't know, back then,
[00:07:54.220 --> 00:07:58.660]   I don't know how common that kind of body physique was.
[00:07:58.660 --> 00:08:00.420]   - It's amazing that you asked this question.
[00:08:00.420 --> 00:08:03.860]   I'll tell you a funny story because I was,
[00:08:03.860 --> 00:08:06.260]   one of the reasons Feynman found this so weird
[00:08:06.260 --> 00:08:10.220]   was because the way bodybuilding entered the United States
[00:08:10.220 --> 00:08:12.380]   or became widespread was a very interesting story
[00:08:12.380 --> 00:08:15.340]   because at first they were regarded as freaks
[00:08:15.340 --> 00:08:17.660]   and homosexuals and all kinds of stuff.
[00:08:17.660 --> 00:08:22.660]   And I remember growing up, our high school football coach,
[00:08:22.660 --> 00:08:23.940]   swimming was different.
[00:08:23.940 --> 00:08:26.300]   Swimming, because it was international,
[00:08:26.300 --> 00:08:29.540]   swimming picked up a lot of advanced training techniques
[00:08:29.540 --> 00:08:31.620]   from the Russians and from East Germans and stuff,
[00:08:31.620 --> 00:08:35.100]   but football was more, you know,
[00:08:35.100 --> 00:08:38.900]   more kind of just American and not very international.
[00:08:38.900 --> 00:08:42.660]   And so our football coach used to tell us not to lift weights
[00:08:42.660 --> 00:08:45.300]   when we were, maybe when I was in junior high school.
[00:08:45.300 --> 00:08:46.900]   And they said, it makes you slow.
[00:08:46.900 --> 00:08:48.500]   You're being bulky is no good.
[00:08:48.500 --> 00:08:52.940]   You gotta be, you know, you gotta be fast in football.
[00:08:52.940 --> 00:08:54.900]   And then something changed around the time
[00:08:54.900 --> 00:08:58.060]   I was in high school where the coaches figured out,
[00:08:58.060 --> 00:08:59.220]   'cause the swimmers, as a swimmer,
[00:08:59.220 --> 00:09:01.120]   I'd been lifting weights since I was an age group swimmer,
[00:09:01.120 --> 00:09:04.100]   like maybe at age 12 or 14, I started lifting weights.
[00:09:04.100 --> 00:09:07.100]   So, but then the football coaches got into it
[00:09:07.100 --> 00:09:08.820]   and mainly because the University of Nebraska,
[00:09:08.820 --> 00:09:11.060]   University of Nebraska had a very famous strength program
[00:09:11.060 --> 00:09:13.280]   that really popularized it.
[00:09:13.280 --> 00:09:16.080]   And so at the time though,
[00:09:16.080 --> 00:09:18.060]   there just weren't a lot of big guys
[00:09:18.060 --> 00:09:20.100]   and the people who knew how to train,
[00:09:20.100 --> 00:09:21.980]   the way everybody like you probably go to the gym
[00:09:21.980 --> 00:09:24.100]   and train using what would be considered
[00:09:24.100 --> 00:09:27.740]   kind of advanced knowledge back in the '80s, okay?
[00:09:27.740 --> 00:09:30.740]   Like how to do a split routine or squat on one day
[00:09:30.740 --> 00:09:32.540]   and do your upper body on the other day, next day.
[00:09:32.540 --> 00:09:35.780]   That was like considered advanced knowledge at that time.
[00:09:35.780 --> 00:09:38.660]   And so I remember once I had an injury
[00:09:38.660 --> 00:09:39.820]   and it was in the trainer's room
[00:09:39.820 --> 00:09:42.580]   at the Caltech athletic facility.
[00:09:42.580 --> 00:09:44.740]   And the lady was looking, it was a female trainer,
[00:09:44.740 --> 00:09:47.300]   and she was looking at my quadriceps
[00:09:47.300 --> 00:09:49.020]   and 'cause I'd pulled a muscle
[00:09:49.020 --> 00:09:51.700]   and she was looking at the, if you know your anatomy,
[00:09:51.700 --> 00:09:52.780]   like right above your kneecap,
[00:09:52.780 --> 00:09:55.820]   your quadriceps kind of insert right above your kneecap.
[00:09:55.820 --> 00:09:57.300]   And if you have well-developed quads,
[00:09:57.300 --> 00:10:01.220]   you actually have a bulge, a bump right above your kneecap.
[00:10:01.220 --> 00:10:02.660]   And she was looking at it from this angle
[00:10:02.660 --> 00:10:03.620]   where she was in front of me
[00:10:03.620 --> 00:10:05.740]   and she was looking at my leg from the front.
[00:10:05.740 --> 00:10:07.940]   And she's like, "Wow, it's really swollen."
[00:10:07.940 --> 00:10:11.020]   And I was like, "That's not the injury,
[00:10:11.020 --> 00:10:12.860]   "that's my quadricep muscle."
[00:10:12.860 --> 00:10:14.580]   And she was a trainer.
[00:10:14.580 --> 00:10:16.660]   So, you know, and at that time,
[00:10:16.660 --> 00:10:18.020]   like I could probably squat,
[00:10:18.020 --> 00:10:19.900]   I could maybe squat 400 pounds at that time.
[00:10:19.900 --> 00:10:21.180]   So I was pretty strong, right?
[00:10:21.180 --> 00:10:22.940]   And I had big legs.
[00:10:22.940 --> 00:10:24.580]   And so anyway, the fact that the trainer
[00:10:24.580 --> 00:10:27.780]   didn't really understand like what well-developed anatomy
[00:10:27.780 --> 00:10:29.620]   was supposed to look like was just blew my mind.
[00:10:29.620 --> 00:10:31.940]   I was like, "No, that's my quadricep.
[00:10:31.940 --> 00:10:32.860]   "We build that up."
[00:10:32.860 --> 00:10:34.940]   And she's like, "Oh, I thought that was the injury."
[00:10:34.940 --> 00:10:36.700]   I was like, "What, what are you talking about?"
[00:10:36.700 --> 00:10:38.700]   So anyway, we've come a long way.
[00:10:38.700 --> 00:10:40.420]   This is one of these things where you gotta be old
[00:10:40.420 --> 00:10:44.820]   to have any kind of understanding of how this stuff evolved
[00:10:44.820 --> 00:10:47.820]   over the last, you know, 30, 40 years.
[00:10:47.820 --> 00:10:51.220]   - But, you know, I wonder if that was a phenomenon
[00:10:51.220 --> 00:10:53.020]   of that particular time or if like,
[00:10:53.020 --> 00:10:54.060]   if throughout human history,
[00:10:54.060 --> 00:10:56.580]   people have just not been that muscular or like,
[00:10:56.580 --> 00:10:58.420]   'cause you hear stories of like Roman soldiers
[00:10:58.420 --> 00:11:02.220]   who are carrying like 80 pounds for 10 or 20 miles a day.
[00:11:02.220 --> 00:11:04.140]   And I mean, there's like a lot of sculptures
[00:11:04.140 --> 00:11:06.020]   in the ancient world where, I mean, not that ancient,
[00:11:06.020 --> 00:11:07.540]   but like the people look like
[00:11:07.540 --> 00:11:09.220]   they have a well-developed musculature.
[00:11:09.220 --> 00:11:11.260]   - So the Greeks were very special
[00:11:11.260 --> 00:11:14.500]   because they were the first to really think about
[00:11:14.500 --> 00:11:18.940]   the word gymnasium and there's a thing called the palestra,
[00:11:18.940 --> 00:11:21.140]   which where they would train like wrestling and boxing
[00:11:21.140 --> 00:11:22.300]   and stuff like this.
[00:11:22.300 --> 00:11:24.420]   They were the first people who were really seriously
[00:11:24.420 --> 00:11:27.260]   into physical culture and training,
[00:11:27.260 --> 00:11:30.260]   specific training for athletic competition.
[00:11:30.260 --> 00:11:32.740]   But if you look at like, even in the '70s,
[00:11:32.740 --> 00:11:36.060]   so when I was a little kid and I remember in the '70s
[00:11:36.060 --> 00:11:38.860]   and now when I look back at old photos from the '70s,
[00:11:38.860 --> 00:11:41.620]   it's very apparent, guys are skinny.
[00:11:41.620 --> 00:11:43.060]   Guys are so skinny.
[00:11:43.060 --> 00:11:44.900]   You know, the guys who went off and fought World War II,
[00:11:44.900 --> 00:11:47.060]   whether they were on the German side or the American side,
[00:11:47.060 --> 00:11:48.940]   they were like five, eight, five, nine,
[00:11:48.940 --> 00:11:51.300]   and they weighed like 130 pounds, 140 pounds.
[00:11:51.300 --> 00:11:56.020]   They were totally different than what modern U.S. Marines
[00:11:56.020 --> 00:11:57.860]   you would think of look like, right?
[00:11:57.860 --> 00:12:01.380]   So yeah, physical culture was a new thing.
[00:12:01.380 --> 00:12:03.460]   Of course, the Romans and the Greeks had it to some degree,
[00:12:03.460 --> 00:12:06.700]   but it was kind of lost for a long time.
[00:12:06.700 --> 00:12:09.660]   And it was just coming back in the U.S. when I was growing up
[00:12:09.660 --> 00:12:14.660]   and so yeah, if you were 200 pounds of fairly lean 200 pounds
[00:12:14.660 --> 00:12:17.340]   and you could bench over 300,
[00:12:17.340 --> 00:12:19.500]   that was pretty rare back in those days.
[00:12:20.700 --> 00:12:22.020]   - Yeah, yeah, yeah.
[00:12:22.020 --> 00:12:26.020]   Okay, so let's talk about your company, Genomic Prediction.
[00:12:26.020 --> 00:12:27.780]   Yeah, do you wanna talk about what this company does?
[00:12:27.780 --> 00:12:30.340]   Do you wanna give an intro into what this is?
[00:12:30.340 --> 00:12:34.700]   - Yeah, so if you don't mind what I should say,
[00:12:34.700 --> 00:12:36.020]   there are two ways to introduce it.
[00:12:36.020 --> 00:12:38.380]   One is the scientific view
[00:12:38.380 --> 00:12:41.100]   and then the other is the IVF view
[00:12:41.100 --> 00:12:42.540]   and I can kind of do a little of both.
[00:12:42.540 --> 00:12:44.820]   So scientifically, the issue is
[00:12:44.820 --> 00:12:48.500]   we have more and more genomic data.
[00:12:48.500 --> 00:12:51.060]   If you give me the genomes of a bunch of people
[00:12:51.060 --> 00:12:53.300]   and then you give me some information about each person
[00:12:53.300 --> 00:12:56.340]   like do they or do they not have diabetes
[00:12:56.340 --> 00:13:01.260]   or how tall are they or what's their IQ score or something,
[00:13:01.260 --> 00:13:03.300]   then all of your listeners will be familiar
[00:13:03.300 --> 00:13:05.060]   with AI and machine learning.
[00:13:05.060 --> 00:13:07.740]   It's a natural AI machine learning problem
[00:13:07.740 --> 00:13:11.300]   to figure out which features in the DNA variation
[00:13:11.300 --> 00:13:13.940]   between people are predictive
[00:13:13.940 --> 00:13:16.100]   of whatever variable you're trying to predict,
[00:13:16.100 --> 00:13:19.300]   whatever biological term is phenotype.
[00:13:19.300 --> 00:13:22.380]   So this is an ancient scientific question
[00:13:22.380 --> 00:13:25.460]   of how do you relate the genotype of the organism,
[00:13:25.460 --> 00:13:28.580]   the specific DNA pattern to the phenotype,
[00:13:28.580 --> 00:13:30.980]   the actual expressed characteristics of the organism.
[00:13:30.980 --> 00:13:33.380]   And if you think about it, this is what biology is.
[00:13:33.380 --> 00:13:35.060]   Like once we had the molecular revolution
[00:13:35.060 --> 00:13:37.060]   and people figured out that DNA is the thing
[00:13:37.060 --> 00:13:40.460]   which stores the information which is passed along
[00:13:40.460 --> 00:13:45.420]   and evolution selects on the variation in the DNA
[00:13:45.420 --> 00:13:47.860]   as it's expressed as phenotype
[00:13:47.860 --> 00:13:50.980]   and as that phenotype affects fitness, okay,
[00:13:50.980 --> 00:13:53.140]   or reproductive success.
[00:13:53.140 --> 00:13:55.780]   That's the whole ball game for biology.
[00:13:55.780 --> 00:13:58.300]   And I'm lucky that as a physicist who's trained
[00:13:58.300 --> 00:14:00.620]   in kind of mathematics and computation,
[00:14:00.620 --> 00:14:03.060]   I arrived on the scene at a time
[00:14:03.060 --> 00:14:06.500]   when we're gonna solve this basic fundamental problem
[00:14:06.500 --> 00:14:10.060]   of biology through brute force AI and machine learning.
[00:14:10.060 --> 00:14:12.620]   So that's how I kind of got into this, right?
[00:14:12.620 --> 00:14:15.020]   Now you ask as an entrepreneur, like, okay, fine,
[00:14:15.020 --> 00:14:16.740]   Steve, you're doing this in your office
[00:14:16.740 --> 00:14:19.540]   with your postdocs and collaborators
[00:14:19.540 --> 00:14:21.660]   on your computers and stuff, but what use is it, right?
[00:14:21.660 --> 00:14:24.060]   What use is all this stuff?
[00:14:24.060 --> 00:14:29.060]   The most direct application of this
[00:14:29.060 --> 00:14:30.940]   is in the following setting.
[00:14:30.940 --> 00:14:33.420]   Every year around the world,
[00:14:33.420 --> 00:14:37.300]   there are millions of families that go through IVF,
[00:14:37.300 --> 00:14:39.380]   typically because they're having some fertility issues
[00:14:39.380 --> 00:14:42.900]   and also mainly typically because the mother is older,
[00:14:42.900 --> 00:14:45.580]   like typically in her 30s or maybe 40s.
[00:14:45.580 --> 00:14:48.700]   And in the process of IVF,
[00:14:48.700 --> 00:14:50.660]   because they use hormone stimulation,
[00:14:50.660 --> 00:14:54.700]   they generally produce more eggs
[00:14:54.700 --> 00:14:56.940]   instead of one per cycle they might produce
[00:14:56.940 --> 00:14:59.340]   depending on the age of the woman,
[00:14:59.340 --> 00:15:01.460]   anywhere between five or 10 or 20,
[00:15:01.460 --> 00:15:03.980]   or even I recently learned for young women
[00:15:03.980 --> 00:15:07.100]   who are hormonally stimulated, if they're egg donors,
[00:15:07.100 --> 00:15:11.500]   they can produce 60 or 100 eggs in one retrieval cycle.
[00:15:11.500 --> 00:15:13.100]   And then it's trivial, as you know,
[00:15:13.100 --> 00:15:16.900]   men produce sperm all the time, we're just producing it.
[00:15:16.900 --> 00:15:21.980]   You can fertilize those eggs pretty easily in a little dish
[00:15:21.980 --> 00:15:26.060]   and you get a bunch of embryos, which they grow.
[00:15:26.060 --> 00:15:28.580]   They just start growing once they're fertilized.
[00:15:28.580 --> 00:15:31.100]   Now, the problem is if you're a family
[00:15:31.100 --> 00:15:35.380]   and you produce more embryos than you're going to use,
[00:15:35.380 --> 00:15:38.220]   you have what we call the embryo choice problem.
[00:15:38.220 --> 00:15:39.660]   You have to figure out like, okay,
[00:15:39.660 --> 00:15:41.100]   I have these 20 viable embryos,
[00:15:41.100 --> 00:15:43.260]   which one am I going to use?
[00:15:43.260 --> 00:15:44.980]   And so the most direct application
[00:15:44.980 --> 00:15:46.940]   of the science that I described is,
[00:15:46.940 --> 00:15:51.940]   well, we can now genotype those embryos from a small biopsy.
[00:15:51.940 --> 00:15:55.140]   And I can tell you things about the embryos.
[00:15:55.140 --> 00:15:57.980]   I could tell you, hey, number four is an outlier
[00:15:57.980 --> 00:15:59.420]   for breast cancer risk.
[00:15:59.420 --> 00:16:01.980]   I would think carefully about using number four.
[00:16:01.980 --> 00:16:06.460]   Number 10 is an outlier for cardiovascular disease risk.
[00:16:06.460 --> 00:16:09.380]   You might want to think about not using that one.
[00:16:09.380 --> 00:16:11.500]   The other ones are okay.
[00:16:11.500 --> 00:16:13.980]   And so that is what genomic prediction does.
[00:16:13.980 --> 00:16:18.980]   And I think we work with two or 300 different IVF clinics
[00:16:18.980 --> 00:16:21.740]   on six continents now.
[00:16:21.740 --> 00:16:22.580]   - Yeah, yeah.
[00:16:22.580 --> 00:16:24.020]   So the super fascinating thing about this
[00:16:24.020 --> 00:16:26.460]   is that the diseases you just talked about,
[00:16:26.460 --> 00:16:30.140]   or at least their risk profiles, they're polygenic.
[00:16:30.140 --> 00:16:33.300]   So you can have thousands of SNPs,
[00:16:33.300 --> 00:16:35.260]   single nucleotide polymorphisms,
[00:16:35.260 --> 00:16:37.260]   that determine whether you're going to get
[00:16:37.260 --> 00:16:38.420]   this disease or not.
[00:16:38.420 --> 00:16:41.060]   And so I'm really curious to learn
[00:16:41.060 --> 00:16:44.740]   how you were able to transition to space
[00:16:44.740 --> 00:16:47.700]   and how your knowledge of mathematics and physics
[00:16:47.700 --> 00:16:49.220]   was able to help you figure out
[00:16:49.220 --> 00:16:50.820]   how to make sense of all this data.
[00:16:50.820 --> 00:16:51.900]   - Yeah, that's a great question.
[00:16:51.900 --> 00:16:55.940]   So first of all, again, I was kind of stressing
[00:16:55.940 --> 00:16:59.340]   the fundamental scientific importance of all this stuff.
[00:16:59.340 --> 00:17:01.740]   If you go into a slightly higher level of detail,
[00:17:01.740 --> 00:17:03.900]   which you were getting at with the individual SNPs
[00:17:03.900 --> 00:17:06.900]   or polymorphisms, those are individual locations
[00:17:06.900 --> 00:17:09.660]   in the genome where I might differ from you
[00:17:09.660 --> 00:17:11.620]   and you might differ from another person.
[00:17:11.620 --> 00:17:14.900]   And typically, if you just take pairs of individuals,
[00:17:14.900 --> 00:17:16.780]   each human, each pair of individuals
[00:17:16.780 --> 00:17:19.740]   will differ at a few million places in the genome.
[00:17:19.740 --> 00:17:21.220]   Okay, and that's what's controlling.
[00:17:21.220 --> 00:17:22.780]   That's why I look a little different than you.
[00:17:22.780 --> 00:17:25.180]   And, you know, so-
[00:17:25.180 --> 00:17:26.220]   - Just a little.
[00:17:26.220 --> 00:17:28.220]   - Just, yeah, a little bit.
[00:17:28.220 --> 00:17:30.580]   I mean, you look better than me, but you know.
[00:17:30.580 --> 00:17:35.580]   The question is the following.
[00:17:35.580 --> 00:17:38.300]   So a lot of times what theoretical physicists do
[00:17:38.300 --> 00:17:40.020]   is they have a little spare energy.
[00:17:40.020 --> 00:17:42.540]   They have some spare cycles and they get tired
[00:17:42.540 --> 00:17:44.620]   of thinking about quarks or something.
[00:17:44.620 --> 00:17:47.580]   And they want to like maybe dabble in biology
[00:17:47.580 --> 00:17:49.140]   or they want to dabble in computer science
[00:17:49.140 --> 00:17:50.380]   or some other field.
[00:17:50.380 --> 00:17:51.940]   And the thing that we always have to do
[00:17:51.940 --> 00:17:53.540]   as theoretical physicists, we always feel like,
[00:17:53.540 --> 00:17:55.060]   oh, I have a lot of horsepower.
[00:17:55.060 --> 00:17:56.580]   I can figure a lot of stuff out.
[00:17:56.580 --> 00:17:58.220]   Like for example, Feynman helped design
[00:17:58.220 --> 00:18:01.020]   the first parallel processors at thinking machines.
[00:18:01.020 --> 00:18:02.260]   I got to figure out which problems
[00:18:02.260 --> 00:18:04.100]   I can actually make an impact on
[00:18:04.100 --> 00:18:05.740]   because I can waste a lot of time.
[00:18:05.740 --> 00:18:08.100]   Some people spend their whole lives studying one problem,
[00:18:08.100 --> 00:18:12.340]   like one molecule or something or one biological system.
[00:18:12.340 --> 00:18:13.660]   And I don't have time for that.
[00:18:13.660 --> 00:18:15.060]   I'm just going to jump in and jump out.
[00:18:15.060 --> 00:18:15.980]   I'm a physicist, right?
[00:18:15.980 --> 00:18:18.980]   That's a typical attitude among theoretical physicists.
[00:18:18.980 --> 00:18:21.460]   So the thing that I had to confront about 10 years ago
[00:18:21.460 --> 00:18:25.340]   was I knew the rate at which sequencing costs
[00:18:25.340 --> 00:18:26.820]   were going down.
[00:18:26.820 --> 00:18:30.220]   So I could anticipate we would get to the day today
[00:18:30.220 --> 00:18:32.220]   when there are millions of genomes
[00:18:32.220 --> 00:18:35.700]   with good phenotype data available for analysis.
[00:18:35.700 --> 00:18:37.980]   So that a typical run for us, a training run,
[00:18:37.980 --> 00:18:40.340]   might involve almost a million genomes
[00:18:40.340 --> 00:18:42.300]   or half a million genomes or something.
[00:18:42.300 --> 00:18:44.740]   So the mathematical question is,
[00:18:44.740 --> 00:18:48.300]   what is the most effective algorithm
[00:18:48.300 --> 00:18:53.060]   given a set of genomes and phenotype information
[00:18:53.060 --> 00:18:55.220]   to build the best predictor, right?
[00:18:55.220 --> 00:18:57.740]   So it can be boiled down to a very well-defined
[00:18:57.740 --> 00:18:59.020]   machine learning problem.
[00:19:00.020 --> 00:19:04.620]   And it turns out for some subset of algorithms,
[00:19:04.620 --> 00:19:05.620]   there are theorems,
[00:19:05.620 --> 00:19:09.140]   there are actually performance guarantees that tell you,
[00:19:09.140 --> 00:19:12.460]   they give you a bound on how much data you need
[00:19:12.460 --> 00:19:17.460]   to capture almost all of the variation in the features.
[00:19:17.460 --> 00:19:22.420]   And so I spent actually a fair amount of time,
[00:19:22.420 --> 00:19:26.100]   like probably a year or two studying these results,
[00:19:26.100 --> 00:19:27.020]   very famous results.
[00:19:27.020 --> 00:19:28.860]   Some of them were proved by a guy called Terence Tao,
[00:19:28.860 --> 00:19:30.740]   who's a Fields Medalist.
[00:19:30.740 --> 00:19:33.660]   And these are results on something called compressed sensing,
[00:19:33.660 --> 00:19:38.580]   which is a penalized form of high-dimensional regression,
[00:19:38.580 --> 00:19:41.500]   which tries to build sparse predictors.
[00:19:41.500 --> 00:19:44.460]   Machine learning people might know it
[00:19:44.460 --> 00:19:47.620]   as L1 penalized optimization.
[00:19:47.620 --> 00:19:51.140]   And anyway, so the point is the very first paper
[00:19:51.140 --> 00:19:56.140]   we wrote on this was to prove that using real genomic data,
[00:19:56.140 --> 00:19:58.180]   that these theorems that were very abstract
[00:19:58.180 --> 00:20:01.380]   could be applied in order to predict how much data
[00:20:01.380 --> 00:20:05.020]   you would need to quote solve individual human traits.
[00:20:05.020 --> 00:20:08.100]   So we showed that you would need at least a few,
[00:20:08.100 --> 00:20:12.020]   around a few hundred thousand individuals and their heights,
[00:20:12.020 --> 00:20:14.780]   their genomes and their heights to solve height
[00:20:14.780 --> 00:20:16.980]   as a phenotype.
[00:20:16.980 --> 00:20:20.820]   And we proved that in a paper using all this fancy math
[00:20:20.820 --> 00:20:25.620]   in 2012, I wanna say the paper came out around 2012.
[00:20:25.620 --> 00:20:29.340]   And then around 2017, when we got a hold of
[00:20:29.340 --> 00:20:31.900]   half a million genomes,
[00:20:31.900 --> 00:20:35.060]   we were able to implement it in practical terms
[00:20:35.060 --> 00:20:37.380]   and show that our mathematical result
[00:20:37.380 --> 00:20:39.100]   from some years ago was correct.
[00:20:39.100 --> 00:20:43.140]   And we, the transition from low performance
[00:20:43.140 --> 00:20:44.740]   of the predictor to high performance,
[00:20:44.740 --> 00:20:46.860]   there's a kind of what we call a phase transition boundary
[00:20:46.860 --> 00:20:48.180]   between those two domains,
[00:20:48.180 --> 00:20:50.580]   occurred just where we said it was gonna occur.
[00:20:50.580 --> 00:20:52.100]   So some of these technical details
[00:20:52.100 --> 00:20:53.220]   are really just not understood,
[00:20:53.220 --> 00:20:55.260]   even by practitioners in computational genomics
[00:20:55.260 --> 00:20:57.940]   who are not quite that mathematical.
[00:20:57.940 --> 00:20:59.780]   They don't understand actually these results
[00:20:59.780 --> 00:21:01.220]   that in our earlier papers,
[00:21:01.220 --> 00:21:03.020]   they don't really know why we can do stuff
[00:21:03.020 --> 00:21:05.580]   that other people can't do or why we can predict
[00:21:05.580 --> 00:21:07.620]   how much data we're gonna need to do stuff.
[00:21:07.620 --> 00:21:10.020]   It's not well appreciated even in the field.
[00:21:10.020 --> 00:21:11.860]   But if you look carefully,
[00:21:11.860 --> 00:21:16.380]   when the big future AI in our future in the singularity
[00:21:16.380 --> 00:21:18.460]   looks back and says, "Hey, who gets the most credit
[00:21:18.460 --> 00:21:20.140]   "for this genomics revolution that happened
[00:21:20.140 --> 00:21:21.700]   "in the early 21st century?"
[00:21:21.700 --> 00:21:24.020]   They're gonna find, that AI is gonna find these papers
[00:21:24.020 --> 00:21:26.620]   on the archive in which we prove this is possible.
[00:21:26.620 --> 00:21:28.740]   And then five years later, we did it,
[00:21:28.740 --> 00:21:30.700]   and et cetera, et cetera.
[00:21:30.700 --> 00:21:31.900]   Right now it's underappreciated,
[00:21:31.900 --> 00:21:35.100]   but the future AI that Rocco's basilisk AI,
[00:21:35.100 --> 00:21:35.940]   when he looks back,
[00:21:35.940 --> 00:21:38.460]   is gonna give me a little bit of credit for it.
[00:21:38.460 --> 00:21:41.180]   - Yeah, yeah, so I was kind of a little interested
[00:21:41.180 --> 00:21:42.540]   in this a few years ago.
[00:21:42.540 --> 00:21:43.540]   And then at that time,
[00:21:43.540 --> 00:21:45.420]   I looked into like how these polygenic risk scores
[00:21:45.420 --> 00:21:46.420]   are calculated.
[00:21:46.420 --> 00:21:47.300]   And it was basically,
[00:21:47.300 --> 00:21:50.100]   you just find the correlation between the phenotype
[00:21:50.100 --> 00:21:53.460]   and the alleles that correlate with it.
[00:21:53.460 --> 00:21:55.100]   And you just add up how many copies
[00:21:55.100 --> 00:21:56.180]   of this allele do you have?
[00:21:56.180 --> 00:21:57.340]   What is the correlation?
[00:21:57.340 --> 00:21:59.380]   So it seemed like, and you just do a weighted sum of that.
[00:21:59.380 --> 00:22:01.340]   So that seemed like a very,
[00:22:01.340 --> 00:22:03.700]   it just seems super simple,
[00:22:03.700 --> 00:22:04.860]   especially in an era where we have
[00:22:04.860 --> 00:22:06.420]   all this machine learning.
[00:22:06.420 --> 00:22:08.340]   But it seemed like they were getting
[00:22:08.340 --> 00:22:09.620]   good predictive results out of that.
[00:22:09.620 --> 00:22:12.300]   So what is the delta between how good you can get
[00:22:12.300 --> 00:22:13.820]   with all this fancy mathematics
[00:22:13.820 --> 00:22:17.740]   versus just like a very simple sum of correlations?
[00:22:17.740 --> 00:22:19.980]   - Yeah, so you're absolutely right
[00:22:19.980 --> 00:22:22.780]   that the ultimate models that are used
[00:22:22.780 --> 00:22:24.260]   when you've done all the training
[00:22:24.260 --> 00:22:26.220]   and the dust settles,
[00:22:26.220 --> 00:22:27.500]   the models are very simple.
[00:22:27.500 --> 00:22:30.300]   They have an additive structure.
[00:22:30.300 --> 00:22:32.020]   So it's basically like,
[00:22:32.020 --> 00:22:34.500]   I either assign a non-zero weight
[00:22:34.500 --> 00:22:38.580]   to this particular region in the genome, or I don't.
[00:22:38.580 --> 00:22:40.500]   And then I need to know what is the weighting,
[00:22:40.500 --> 00:22:43.140]   but then the function is a linear function of,
[00:22:43.140 --> 00:22:46.220]   it's an additive function of the state of your genome
[00:22:46.220 --> 00:22:48.340]   at some subset of positions.
[00:22:48.340 --> 00:22:50.460]   So the ultimate model that you get is very simple.
[00:22:50.460 --> 00:22:53.580]   Now, if you go back 10 years when we were doing this,
[00:22:53.580 --> 00:22:54.540]   there were lots of claims
[00:22:54.540 --> 00:22:56.180]   that it was gonna be super non-linear,
[00:22:56.180 --> 00:22:58.060]   that it wasn't gonna be additive
[00:22:58.060 --> 00:22:59.420]   the way I just described it.
[00:22:59.420 --> 00:23:01.060]   There are gonna be lots of interaction terms
[00:23:01.060 --> 00:23:02.260]   between regions.
[00:23:02.260 --> 00:23:04.140]   Some biologists are still convinced that's true,
[00:23:04.140 --> 00:23:06.540]   even though we already know we have predictors
[00:23:06.540 --> 00:23:08.660]   that don't have interactions.
[00:23:08.660 --> 00:23:11.060]   Okay, the other question, which is more technical,
[00:23:11.060 --> 00:23:14.580]   is that in any small region of your genome,
[00:23:14.580 --> 00:23:17.700]   the state of the individual variance is highly correlated
[00:23:17.700 --> 00:23:20.740]   because you inherit them in chunks.
[00:23:20.740 --> 00:23:22.700]   And so you need to figure out
[00:23:22.700 --> 00:23:24.380]   which one of those you want to use.
[00:23:24.380 --> 00:23:25.660]   You don't want to activate all of them
[00:23:25.660 --> 00:23:27.340]   because you might be overcounting.
[00:23:27.340 --> 00:23:31.420]   So that's where this L1 penalization sparse methods,
[00:23:31.420 --> 00:23:36.060]   they force the predictor to be sparse.
[00:23:36.060 --> 00:23:37.580]   And that is a key step.
[00:23:37.580 --> 00:23:39.420]   Otherwise, you might overcount.
[00:23:39.420 --> 00:23:42.340]   You might have 10 different variants close by
[00:23:42.340 --> 00:23:45.100]   that have roughly the same statistical significance
[00:23:45.100 --> 00:23:47.220]   if you just do some simple regression math.
[00:23:47.220 --> 00:23:49.100]   But then you don't know which one of those tend to use
[00:23:49.100 --> 00:23:50.580]   and you might be overcounting effects
[00:23:50.580 --> 00:23:51.420]   or undercounting effects.
[00:23:51.420 --> 00:23:52.980]   So what you end up doing
[00:23:52.980 --> 00:23:55.100]   is a super high dimensional optimization
[00:23:55.100 --> 00:23:59.420]   where you only activate, you grudgingly activate a SNP
[00:23:59.420 --> 00:24:00.740]   when the signal is strong enough.
[00:24:00.740 --> 00:24:02.420]   And once you activate that one,
[00:24:02.420 --> 00:24:04.580]   the algorithm has to be smart enough to penalize
[00:24:04.580 --> 00:24:06.380]   the other ones nearby and not activate them
[00:24:06.380 --> 00:24:08.900]   'cause you're overcounting effects if you do that.
[00:24:08.900 --> 00:24:11.500]   So there's a little bit of subtlety in it,
[00:24:11.500 --> 00:24:13.700]   but the main point which you made,
[00:24:13.700 --> 00:24:15.620]   which is that the ultimate predictors,
[00:24:15.620 --> 00:24:17.340]   which are very simple and additive,
[00:24:17.340 --> 00:24:20.900]   just sums over effect sizes times states,
[00:24:20.900 --> 00:24:21.940]   actually works really well.
[00:24:21.940 --> 00:24:24.060]   And that is related to a deep statement
[00:24:24.060 --> 00:24:27.100]   about the additive structure
[00:24:27.100 --> 00:24:30.580]   of the genetic architecture of individual differences.
[00:24:30.580 --> 00:24:33.140]   So in other words, it's kind of weird
[00:24:33.140 --> 00:24:35.940]   that the ways that I differ from you
[00:24:35.940 --> 00:24:38.340]   are merely just 'cause I have more of something
[00:24:38.340 --> 00:24:39.740]   and you have less of something.
[00:24:39.740 --> 00:24:41.940]   And it's not like, oh, these things are interacting
[00:24:41.940 --> 00:24:45.860]   in some super incredibly un-understandable way.
[00:24:45.860 --> 00:24:47.900]   And so that's a very deep thing,
[00:24:47.900 --> 00:24:50.460]   which again is not appreciated that much by biologists yet,
[00:24:50.460 --> 00:24:52.220]   but over time, I think they're gonna figure out
[00:24:52.220 --> 00:24:54.300]   that there's something interesting here.
[00:24:54.300 --> 00:24:56.660]   - Right, no, I thought that was super fascinating.
[00:24:56.660 --> 00:24:59.380]   And I commented about that on Twitter.
[00:24:59.380 --> 00:25:02.820]   What is really interesting about that is,
[00:25:02.820 --> 00:25:03.660]   I guess two things.
[00:25:03.660 --> 00:25:06.140]   One is you have this really interesting evolutionary argument
[00:25:06.140 --> 00:25:09.180]   about why that would be the case, you might wanna explain.
[00:25:09.180 --> 00:25:12.140]   And the second is, it makes you wonder
[00:25:12.140 --> 00:25:14.260]   if just becoming more intelligent
[00:25:14.260 --> 00:25:16.940]   is just a matter of like turning on certain snips.
[00:25:16.940 --> 00:25:20.380]   It's not a matter of like all this incredible optimization
[00:25:20.380 --> 00:25:24.940]   that it's like solving a Sudoku puzzle or anything.
[00:25:24.940 --> 00:25:29.100]   If that's the case, then why aren't we already,
[00:25:29.100 --> 00:25:30.940]   why hasn't the human population already been selected
[00:25:30.940 --> 00:25:33.460]   to be maxed out on all of these traits
[00:25:33.460 --> 00:25:35.540]   if it's just a matter of a bit flip?
[00:25:35.540 --> 00:25:36.380]   - Yeah, so, okay.
[00:25:36.380 --> 00:25:40.220]   So the first issue, which is how, why is this,
[00:25:40.220 --> 00:25:42.300]   why is this genetic architecture so simple,
[00:25:42.300 --> 00:25:43.740]   surprisingly simple?
[00:25:43.740 --> 00:25:44.780]   And again, 10 years ago,
[00:25:44.780 --> 00:25:46.100]   we didn't know it was gonna be simple.
[00:25:46.100 --> 00:25:49.220]   So when we were, when I was checking to see
[00:25:49.220 --> 00:25:51.020]   whether this is a field that I should go into
[00:25:51.020 --> 00:25:52.420]   because either we are capable
[00:25:52.420 --> 00:25:54.740]   or not capable of making progress,
[00:25:54.740 --> 00:25:56.940]   we had to study the more general problem
[00:25:56.940 --> 00:25:59.220]   of the nonlinear possibilities as well.
[00:25:59.220 --> 00:26:00.580]   But eventually we realized
[00:26:00.580 --> 00:26:02.020]   that probably most of the variance
[00:26:02.020 --> 00:26:03.700]   was gonna be captured in an additive way.
[00:26:03.700 --> 00:26:06.660]   So we could narrow down the problem quite a bit.
[00:26:06.660 --> 00:26:08.140]   There are evolutionary reasons for this.
[00:26:08.140 --> 00:26:10.060]   There's a famous theorem by Fisher,
[00:26:10.060 --> 00:26:12.580]   who's the father of population genetics
[00:26:12.580 --> 00:26:16.820]   and also of really what you call frequentist statistics.
[00:26:16.820 --> 00:26:19.380]   And so Fisher proved something called the fundamental,
[00:26:19.380 --> 00:26:21.180]   Fisher's fundamental theorem of natural selection,
[00:26:21.180 --> 00:26:25.020]   which says that if you impose some selection pressure
[00:26:25.020 --> 00:26:29.740]   on a population, the rate at which that population responds
[00:26:29.740 --> 00:26:30.740]   to the selection pressure,
[00:26:30.860 --> 00:26:34.500]   say like it's the bigger rats
[00:26:34.500 --> 00:26:37.500]   that outcompete the smaller rats,
[00:26:37.500 --> 00:26:39.420]   at what rate does the rat population
[00:26:39.420 --> 00:26:41.220]   then start getting bigger?
[00:26:41.220 --> 00:26:45.220]   He showed that it's dominated by the additive variance,
[00:26:45.220 --> 00:26:47.580]   that that dominates the rate of evolution.
[00:26:47.580 --> 00:26:48.700]   And it's easy to understand why,
[00:26:48.700 --> 00:26:50.700]   if it's a nonlinear mechanism
[00:26:50.700 --> 00:26:52.620]   that you need to make the rat bigger,
[00:26:52.620 --> 00:26:55.940]   when you sexually reproduce and that gets chopped apart,
[00:26:55.940 --> 00:26:57.740]   you might break the mechanism.
[00:26:57.740 --> 00:27:00.020]   Whereas if each little allele
[00:27:00.020 --> 00:27:01.340]   has its own independent effect,
[00:27:01.340 --> 00:27:03.860]   you can just inherit them
[00:27:03.860 --> 00:27:06.620]   without worrying about breaking the mechanisms.
[00:27:06.620 --> 00:27:09.060]   So it was well known for,
[00:27:09.060 --> 00:27:10.620]   at least among a tiny population
[00:27:10.620 --> 00:27:12.860]   of theoretical population biologists,
[00:27:12.860 --> 00:27:15.580]   that additive variance was the dominant way
[00:27:15.580 --> 00:27:18.060]   that populations would respond to selection.
[00:27:18.060 --> 00:27:19.500]   So that was already known.
[00:27:19.500 --> 00:27:22.500]   And the other thing is that humans have been through
[00:27:22.500 --> 00:27:24.420]   a pretty tight bottleneck
[00:27:24.420 --> 00:27:26.500]   and we're not that different from each other.
[00:27:26.500 --> 00:27:29.260]   So it's very plausible to me
[00:27:29.260 --> 00:27:32.820]   that if I wanted to edit a human embryo
[00:27:32.820 --> 00:27:34.660]   and make it into a frog,
[00:27:34.660 --> 00:27:36.540]   then there's all kinds of nonlinear,
[00:27:36.540 --> 00:27:38.220]   subtle things I have to do.
[00:27:38.220 --> 00:27:39.780]   But all those very nonlinear,
[00:27:39.780 --> 00:27:42.500]   complicated subsystems are fixed in humans.
[00:27:42.500 --> 00:27:44.340]   You have the same system as I do.
[00:27:44.340 --> 00:27:47.780]   You have the human not frog or ape not frog
[00:27:47.780 --> 00:27:50.980]   version of that region of DNA, and so do I.
[00:27:50.980 --> 00:27:53.460]   But the small ways in which we differ
[00:27:53.460 --> 00:27:55.420]   are just these little additive switches,
[00:27:55.420 --> 00:27:57.740]   mostly little additive switches.
[00:27:57.740 --> 00:28:02.580]   And so that's the deep scientific discovery
[00:28:02.580 --> 00:28:06.380]   from the last, say, five, 10 years of work in this area.
[00:28:06.380 --> 00:28:10.060]   Now, you were asking about why evolution
[00:28:10.060 --> 00:28:13.060]   hasn't completely, quote, optimized
[00:28:13.060 --> 00:28:14.420]   all traits in humans already.
[00:28:14.420 --> 00:28:16.820]   Now, I don't know if you ever do deep learning
[00:28:16.820 --> 00:28:18.340]   or very high-dimensional optimization,
[00:28:18.340 --> 00:28:20.820]   but you realize in that high-dimensional space,
[00:28:20.820 --> 00:28:23.900]   you're often moving on a surface
[00:28:23.900 --> 00:28:26.540]   which is slightly tilted, so you're getting gains,
[00:28:26.540 --> 00:28:27.860]   but it's also kind of flat.
[00:28:27.860 --> 00:28:30.140]   So even though you scale up your compute
[00:28:30.140 --> 00:28:32.540]   or data size by an order of magnitude,
[00:28:32.540 --> 00:28:33.860]   you don't move that much farther.
[00:28:33.860 --> 00:28:35.700]   You get some gains, but you're never really
[00:28:35.700 --> 00:28:38.340]   at the global max of anything
[00:28:38.340 --> 00:28:40.300]   in these high-dimensional spaces.
[00:28:40.300 --> 00:28:41.340]   I don't know if that makes sense to you,
[00:28:41.340 --> 00:28:44.180]   but it's quite plausible to me
[00:28:44.180 --> 00:28:45.940]   that two things are important here.
[00:28:45.940 --> 00:28:48.540]   One is evolution has not had that much time
[00:28:48.540 --> 00:28:51.140]   to optimize humans.
[00:28:51.140 --> 00:28:52.540]   And what do you mean by optimization?
[00:28:52.540 --> 00:28:54.180]   Because the environment that humans live in
[00:28:54.180 --> 00:28:56.980]   has changed radically in the last 10,000 years.
[00:28:56.980 --> 00:28:58.380]   For a while, we didn't have agriculture.
[00:28:58.380 --> 00:28:59.420]   Now we have agriculture.
[00:28:59.420 --> 00:29:02.260]   Now we have swipe left if you want to have sex tonight.
[00:29:02.260 --> 00:29:06.700]   The environment didn't stay fixed.
[00:29:06.700 --> 00:29:09.300]   And so when you say fully optimized for the environment,
[00:29:09.300 --> 00:29:10.460]   what do you mean?
[00:29:10.460 --> 00:29:13.180]   The ability to diagonalize matrices
[00:29:13.180 --> 00:29:17.620]   might not have been very adaptive 10,000 years ago.
[00:29:17.620 --> 00:29:20.180]   It might not even be adaptive now.
[00:29:20.180 --> 00:29:23.340]   But anyway, so it's a complicated question.
[00:29:23.340 --> 00:29:26.820]   One can't reason that naively about,
[00:29:26.820 --> 00:29:29.220]   oh, well, if God wanted us to be 10 feet tall,
[00:29:29.220 --> 00:29:30.300]   we'd be 10 feet tall.
[00:29:30.300 --> 00:29:31.980]   Or if it's better to be smart,
[00:29:31.980 --> 00:29:34.340]   my brain would be like this big or something.
[00:29:34.340 --> 00:29:38.820]   So you can't reason that naively about stuff like that.
[00:29:38.820 --> 00:29:39.660]   - I see.
[00:29:39.660 --> 00:29:40.780]   Yeah, okay, so I guess it could make sense,
[00:29:40.780 --> 00:29:42.180]   for example, with certain health risks,
[00:29:42.180 --> 00:29:43.820]   like the thing that makes you more likely
[00:29:43.820 --> 00:29:46.380]   to get diabetes or heart disease today might be,
[00:29:46.380 --> 00:29:49.180]   I don't know what the pliatoric effect of that could be,
[00:29:49.180 --> 00:29:50.820]   but maybe that's not that important
[00:29:50.820 --> 00:29:51.900]   when you're not that obese.
[00:29:51.900 --> 00:29:54.860]   - Let me just point out that most of the diseases
[00:29:54.860 --> 00:29:57.140]   that we care about now, most of them,
[00:29:57.140 --> 00:29:59.180]   not the rare ones, but the common ones,
[00:29:59.180 --> 00:30:01.780]   they manifest when you're like 50, 60, 70 years old.
[00:30:01.780 --> 00:30:05.580]   And there was never any evolutionary big advantage,
[00:30:05.580 --> 00:30:07.860]   I think, of being super long-lived, right?
[00:30:07.860 --> 00:30:10.140]   So there's even a debate about whether like,
[00:30:10.140 --> 00:30:12.740]   okay, if the grandparents are around to help raise the kids,
[00:30:12.740 --> 00:30:15.940]   that raises the fitness a little bit of the family unit.
[00:30:15.940 --> 00:30:19.140]   But most of the time in the past,
[00:30:19.140 --> 00:30:20.180]   and most of our evolutionary past,
[00:30:20.180 --> 00:30:22.140]   humans just died fairly early.
[00:30:22.140 --> 00:30:23.140]   And so a lot of these diseases
[00:30:23.140 --> 00:30:26.100]   would never have been optimized against evolutionarily,
[00:30:26.100 --> 00:30:27.020]   but we see them now
[00:30:27.020 --> 00:30:29.020]   because we live under such good conditions.
[00:30:29.020 --> 00:30:32.260]   We can, people regularly approach 80 or 90 years.
[00:30:32.260 --> 00:30:34.580]   - Regarding the linearity and additivity point,
[00:30:34.580 --> 00:30:36.260]   I was gonna make the analogy,
[00:30:36.260 --> 00:30:37.580]   and I'm curious if this is valid,
[00:30:37.580 --> 00:30:39.900]   but when you're programming,
[00:30:39.900 --> 00:30:41.420]   one thing that's good practice
[00:30:41.420 --> 00:30:44.380]   is to have all the implementation details
[00:30:44.380 --> 00:30:46.580]   in separate function calls or separate programs
[00:30:46.580 --> 00:30:47.460]   or something,
[00:30:47.460 --> 00:30:52.300]   and then have your main loop of operation
[00:30:52.300 --> 00:30:54.060]   just be called different functions,
[00:30:54.060 --> 00:30:55.300]   like do this, do that,
[00:30:55.300 --> 00:30:57.260]   so that you can easily comment stuff away
[00:30:57.260 --> 00:30:59.500]   or change arguments.
[00:30:59.500 --> 00:31:01.540]   And this seemed very similar to that,
[00:31:01.540 --> 00:31:04.620]   where just by turning these things on and off,
[00:31:04.620 --> 00:31:09.060]   you can change what the next offering is gonna be,
[00:31:09.060 --> 00:31:10.780]   and you don't have to worry about
[00:31:10.820 --> 00:31:12.140]   actually implementing
[00:31:12.140 --> 00:31:15.860]   whatever the underlying mechanism is.
[00:31:15.860 --> 00:31:17.500]   - Well, what you said is related
[00:31:17.500 --> 00:31:19.500]   to what Fisher proved in his theorems,
[00:31:19.500 --> 00:31:23.660]   which is that if suddenly it becomes advantageous
[00:31:23.660 --> 00:31:27.060]   to have X, like white fur instead of black fur or something,
[00:31:27.060 --> 00:31:30.660]   it would be best if there were little levers
[00:31:30.660 --> 00:31:32.140]   that you could move somebody
[00:31:32.140 --> 00:31:34.940]   from black fur to white fur continuously
[00:31:34.940 --> 00:31:38.180]   by just modifying those switches in an additive way.
[00:31:38.180 --> 00:31:40.860]   It just turns out for sexually reproducing species
[00:31:40.860 --> 00:31:43.740]   where the DNA gets scrambled up in every generation,
[00:31:43.740 --> 00:31:45.900]   it's better to have switches of that kind.
[00:31:45.900 --> 00:31:52.100]   And so the other point related to your software analogy
[00:31:52.100 --> 00:31:57.860]   is that there seem to be modular,
[00:31:57.860 --> 00:32:02.580]   fairly modular things going on in the genome.
[00:32:02.580 --> 00:32:04.180]   So when we looked at,
[00:32:04.180 --> 00:32:05.540]   we were the first group to,
[00:32:05.540 --> 00:32:09.140]   I think we had initially like say 20 major disease conditions
[00:32:09.140 --> 00:32:10.700]   we had decent predictors for.
[00:32:10.700 --> 00:32:13.060]   And we just started looking carefully at the,
[00:32:13.060 --> 00:32:17.220]   just something as trivial as the overlap of,
[00:32:17.220 --> 00:32:19.780]   my sparsely trained predictor turns out,
[00:32:19.780 --> 00:32:22.660]   uses these features for diabetes,
[00:32:22.660 --> 00:32:26.340]   but it uses these features for schizophrenia.
[00:32:26.340 --> 00:32:27.460]   And how much overlap,
[00:32:27.460 --> 00:32:29.940]   just the stupidest metric is like how much overlap
[00:32:29.940 --> 00:32:31.540]   or variance accounted for overlap
[00:32:31.540 --> 00:32:34.740]   is there between pairs of disease conditions.
[00:32:34.740 --> 00:32:35.980]   And it's very modest.
[00:32:35.980 --> 00:32:39.180]   It's actually the opposite of what naive biologists would say
[00:32:39.180 --> 00:32:42.420]   when they talk about pleiotropy or they're just disjoint.
[00:32:42.420 --> 00:32:45.500]   They're just disjoint regions of your genome
[00:32:45.500 --> 00:32:47.660]   that are governing certain things.
[00:32:47.660 --> 00:32:48.540]   And so why not?
[00:32:48.540 --> 00:32:49.980]   You have 3 billion base pairs.
[00:32:49.980 --> 00:32:51.260]   There's a lot you can do in there.
[00:32:51.260 --> 00:32:52.820]   There's a lot of information in there.
[00:32:52.820 --> 00:32:53.940]   So you can have,
[00:32:53.940 --> 00:32:57.220]   if you need a thousand to control diabetes risk,
[00:32:57.220 --> 00:32:58.700]   I can have, I think I estimated
[00:32:58.700 --> 00:33:02.900]   you can easily have a thousand roughly independent traits
[00:33:02.900 --> 00:33:06.500]   that are just disjoint in their genetic dependencies.
[00:33:06.500 --> 00:33:08.020]   And so if you think about like D&D,
[00:33:08.020 --> 00:33:10.580]   like your strength and your decks and your wisdom
[00:33:10.580 --> 00:33:12.900]   and your intelligence and charisma,
[00:33:12.900 --> 00:33:13.900]   those are all disjoint.
[00:33:13.900 --> 00:33:15.940]   They're all just independent variables.
[00:33:15.940 --> 00:33:18.220]   So it's like a seven dimensional space
[00:33:18.220 --> 00:33:19.740]   that your character lives in.
[00:33:19.740 --> 00:33:21.620]   Well, there's enough information
[00:33:21.620 --> 00:33:25.500]   in the few million differences between me and you.
[00:33:25.500 --> 00:33:28.740]   There's enough for a thousand dimensional space
[00:33:28.740 --> 00:33:30.820]   of variation.
[00:33:30.820 --> 00:33:32.140]   Like, oh, how big is your spleen?
[00:33:32.140 --> 00:33:33.860]   My big, my spleen's a little bit smaller.
[00:33:33.860 --> 00:33:34.700]   Yours is a little bit bigger.
[00:33:34.700 --> 00:33:36.620]   That can vary independently of your IQ.
[00:33:36.620 --> 00:33:37.940]   Oh, it was big surprise.
[00:33:37.940 --> 00:33:39.860]   The size of your spleen can vary independently
[00:33:39.860 --> 00:33:41.140]   of the size of your big toe.
[00:33:41.140 --> 00:33:41.980]   Oh yeah, yeah.
[00:33:41.980 --> 00:33:43.420]   There's about a thousand.
[00:33:43.420 --> 00:33:45.020]   If you just do information theory,
[00:33:45.020 --> 00:33:47.340]   there's about a thousand different parameters
[00:33:47.340 --> 00:33:49.980]   I can vary independently with the number of variants
[00:33:49.980 --> 00:33:51.900]   that I have between me and you.
[00:33:51.900 --> 00:33:54.180]   So, and this thing,
[00:33:54.180 --> 00:33:56.660]   because you understand some information theory
[00:33:56.660 --> 00:33:58.660]   is kind of trivial to explain,
[00:33:58.660 --> 00:34:00.420]   but try explaining to a biologist.
[00:34:00.420 --> 00:34:02.580]   You won't get very far.
[00:34:02.580 --> 00:34:03.420]   - Yeah, yeah.
[00:34:03.420 --> 00:34:05.660]   Do the log two of the number of,
[00:34:05.660 --> 00:34:07.700]   is that basically how you do it?
[00:34:07.700 --> 00:34:08.540]   Yeah, okay.
[00:34:08.540 --> 00:34:09.380]   - That's all it is.
[00:34:09.380 --> 00:34:11.420]   I mean, well, I mean, well, we, it's in our,
[00:34:11.420 --> 00:34:12.260]   it's in our paper.
[00:34:12.260 --> 00:34:14.460]   Like we basically look at, okay, how many,
[00:34:14.460 --> 00:34:16.740]   how many variants are typically accounting
[00:34:16.740 --> 00:34:19.860]   for most of the variation for any of these major traits?
[00:34:19.860 --> 00:34:21.940]   And then imagine that they're mostly disjoint.
[00:34:21.940 --> 00:34:23.860]   Well, just how much length of DN,
[00:34:23.860 --> 00:34:25.300]   how many variants do you need then
[00:34:25.300 --> 00:34:29.020]   to independently vary a thousand traits?
[00:34:29.020 --> 00:34:31.660]   Well, it's a few million differences
[00:34:31.660 --> 00:34:33.500]   between me and you are enough, right?
[00:34:33.500 --> 00:34:35.900]   So it's trivial.
[00:34:35.900 --> 00:34:36.860]   It's very trivial math.
[00:34:36.860 --> 00:34:38.340]   Once you understand the base,
[00:34:38.340 --> 00:34:40.380]   how to reason about information theory,
[00:34:40.380 --> 00:34:41.900]   then it's very trivial,
[00:34:41.900 --> 00:34:45.900]   but it ain't trivial for theoretical biologists,
[00:34:45.900 --> 00:34:47.740]   as far as I can tell.
[00:34:47.740 --> 00:34:49.420]   - But the result is so interesting
[00:34:49.420 --> 00:34:51.180]   because I remember reading in "The Selfish Gene"
[00:34:51.180 --> 00:34:54.300]   that like he hypothesizes the reason we have aging,
[00:34:54.300 --> 00:34:56.060]   or one of the possible reasons we have aging
[00:34:56.060 --> 00:34:59.180]   is that there's antagonistic cliotropy.
[00:34:59.180 --> 00:35:02.380]   There's something that makes you healthier
[00:35:02.380 --> 00:35:03.820]   when you're young and fertile
[00:35:03.820 --> 00:35:05.900]   that makes you unhealthy when you're old.
[00:35:05.900 --> 00:35:08.860]   And evolution would have selected for such a trade-off
[00:35:08.860 --> 00:35:10.740]   because when you're young and fertile
[00:35:10.740 --> 00:35:13.300]   is when evolution and your genes care about you.
[00:35:13.300 --> 00:35:16.500]   And so, but if there's enough space in the genome
[00:35:16.500 --> 00:35:19.540]   before you, where these trade-offs are not necessary,
[00:35:19.540 --> 00:35:21.660]   then this may be like a bad explanation for aging.
[00:35:21.660 --> 00:35:23.700]   Or do you think I'm straining the analogy?
[00:35:23.700 --> 00:35:26.500]   - No, no, I love your interviews
[00:35:26.500 --> 00:35:29.500]   because the point you're making here is really good.
[00:35:29.500 --> 00:35:33.220]   So Dawkins, who is a kind of evolutionary theorist,
[00:35:33.220 --> 00:35:34.700]   but from the old school,
[00:35:34.700 --> 00:35:37.140]   when they had almost no data, okay, to deal,
[00:35:37.140 --> 00:35:40.740]   you can imagine how much data they had compared to today.
[00:35:40.740 --> 00:35:44.340]   He would like to tell you a story about a particular gene
[00:35:44.340 --> 00:35:48.180]   that maybe it has this positive effect when you're young,
[00:35:48.180 --> 00:35:50.620]   but it makes you age faster, so there's a trade-off.
[00:35:50.620 --> 00:35:53.340]   And we know about things like sickle cell anemia
[00:35:53.340 --> 00:35:55.380]   and we know stories like that.
[00:35:55.380 --> 00:35:58.500]   And no doubt there are stories like that,
[00:35:58.500 --> 00:36:02.380]   which are true about specific variants in your genome,
[00:36:02.380 --> 00:36:04.300]   but that's not the general story.
[00:36:04.300 --> 00:36:05.140]   The general story,
[00:36:05.140 --> 00:36:07.060]   which we only discovered in the last five years
[00:36:07.060 --> 00:36:08.620]   is that almost every trade
[00:36:08.620 --> 00:36:10.580]   is controlled by thousands of variants.
[00:36:10.580 --> 00:36:12.620]   And those variants tend to be disjoint
[00:36:12.620 --> 00:36:15.020]   from the ones that control the other trait.
[00:36:15.020 --> 00:36:16.940]   So they weren't wrong,
[00:36:16.940 --> 00:36:19.460]   but they didn't have the big picture.
[00:36:19.460 --> 00:36:20.380]   - Yeah, I see.
[00:36:21.460 --> 00:36:23.940]   And then, yeah, so you had this paper,
[00:36:23.940 --> 00:36:26.100]   I think it was polygenic health index,
[00:36:26.100 --> 00:36:27.740]   general health and disease risk.
[00:36:27.740 --> 00:36:30.020]   And then you showed that with 10 embryos,
[00:36:30.020 --> 00:36:33.980]   you could increase disability-adjusted life years by four,
[00:36:33.980 --> 00:36:35.540]   which is like a huge increase of like,
[00:36:35.540 --> 00:36:36.380]   if you think about like,
[00:36:36.380 --> 00:36:37.780]   if you could just live four years longer
[00:36:37.780 --> 00:36:38.940]   in a healthy state.
[00:36:38.940 --> 00:36:40.260]   - Yeah, what's the value of that?
[00:36:40.260 --> 00:36:42.660]   What would you pay to buy that for your kid?
[00:36:42.660 --> 00:36:43.620]   - Right, yeah.
[00:36:43.620 --> 00:36:46.460]   But I don't know, this seems like this,
[00:36:46.460 --> 00:36:48.500]   going back to that earlier question about the trade-offs
[00:36:48.500 --> 00:36:51.060]   or like about why this hasn't already been selected for.
[00:36:51.060 --> 00:36:54.700]   If you're right, and there's no like trade-off to do this,
[00:36:54.700 --> 00:36:55.780]   just living four years older,
[00:36:55.780 --> 00:36:57.140]   even if that's beyond your fertility,
[00:36:57.140 --> 00:36:59.140]   just like being a grandpa or something,
[00:36:59.140 --> 00:37:01.780]   that seems like an unmitigated good.
[00:37:01.780 --> 00:37:03.740]   So why, it's kind of mysterious
[00:37:03.740 --> 00:37:07.900]   that that hasn't already been selected for.
[00:37:07.900 --> 00:37:11.420]   - So no, I'm glad you're really asking about these questions
[00:37:11.420 --> 00:37:15.020]   'cause these are things that people are very confused about,
[00:37:15.020 --> 00:37:16.380]   you know, even in the field.
[00:37:16.380 --> 00:37:19.740]   So, first of all, let me say,
[00:37:19.740 --> 00:37:23.340]   if you have a trait that's controlled by 10,000 variants,
[00:37:23.340 --> 00:37:26.420]   okay, like height is controlled by a border 10,000 variants
[00:37:26.420 --> 00:37:28.720]   and probably cognitive ability a little bit more,
[00:37:28.720 --> 00:37:33.720]   the square root of 10,000 is 100, okay?
[00:37:33.720 --> 00:37:37.580]   So if I could come to this little embryo and I said,
[00:37:37.580 --> 00:37:41.160]   I wanna give it one extra standard deviation of height,
[00:37:41.160 --> 00:37:43.100]   plus one standard deviation of height,
[00:37:43.100 --> 00:37:44.820]   I only need to edit 100.
[00:37:46.160 --> 00:37:49.920]   I only need to flip 100 minus variants to plus variants.
[00:37:49.920 --> 00:37:50.960]   These are very rough numbers,
[00:37:50.960 --> 00:37:52.920]   but you know, one standard deviation
[00:37:52.920 --> 00:37:54.620]   is like the square root of N, right?
[00:37:54.620 --> 00:37:59.080]   If I flip a coin N times and I want a better outcome
[00:37:59.080 --> 00:38:01.120]   in terms of number ratio of heads to tails,
[00:38:01.120 --> 00:38:03.240]   and I wanna increase it by one standard deviation,
[00:38:03.240 --> 00:38:07.160]   I only need to flip square root of N heads
[00:38:07.160 --> 00:38:08.920]   because if you flip a lot,
[00:38:08.920 --> 00:38:10.520]   you're gonna get a very narrow distribution
[00:38:10.520 --> 00:38:12.760]   peaked around half and the width of that distribution
[00:38:12.760 --> 00:38:14.480]   is square root of N, okay?
[00:38:14.480 --> 00:38:17.560]   So once I tell you, hey, your, you know,
[00:38:17.560 --> 00:38:19.840]   height is controlled by 10,000 variants
[00:38:19.840 --> 00:38:22.720]   and I only need to flip 100 genetic variants
[00:38:22.720 --> 00:38:24.500]   to make you one standard deviation for a male,
[00:38:24.500 --> 00:38:25.560]   that would be three inches tall,
[00:38:25.560 --> 00:38:27.480]   two and a half or three inches taller.
[00:38:27.480 --> 00:38:28.620]   Suddenly you realize, wait a minute,
[00:38:28.620 --> 00:38:30.960]   there's a lot of variants up for grabs there.
[00:38:30.960 --> 00:38:35.960]   I mean, if I could flip 500 variants in your genome,
[00:38:35.960 --> 00:38:39.880]   I would make you five standard deviations taller.
[00:38:39.880 --> 00:38:41.280]   You'd be like seven feet tall
[00:38:41.280 --> 00:38:42.620]   and I didn't have to do that much work.
[00:38:42.620 --> 00:38:45.680]   And there's a lot more variation where that came from, okay?
[00:38:45.680 --> 00:38:47.680]   Because I only flipped 500 out of 10,000,
[00:38:47.680 --> 00:38:49.960]   I could have flipped even more, right?
[00:38:49.960 --> 00:38:54.960]   So there's this kind of quasi-infinite well of variation
[00:38:54.960 --> 00:39:00.040]   which evolution or genetic engineers could act on.
[00:39:00.040 --> 00:39:03.040]   And again, the early population geneticists
[00:39:03.040 --> 00:39:07.600]   who breed corn, who breed animals, they know this.
[00:39:07.600 --> 00:39:10.720]   This is actually something they explicitly know about
[00:39:10.720 --> 00:39:12.460]   'cause they've done calculations.
[00:39:12.460 --> 00:39:14.440]   Now, interestingly, the human geneticists
[00:39:14.440 --> 00:39:16.640]   who are mainly concerned with diseases and stuff
[00:39:16.640 --> 00:39:19.320]   are often not familiar with what the math
[00:39:19.320 --> 00:39:21.840]   that the animal breeders already know.
[00:39:21.840 --> 00:39:23.120]   And you might be interested to know
[00:39:23.120 --> 00:39:24.920]   that the milk you drink comes
[00:39:24.920 --> 00:39:27.400]   from heavily genetically optimized cows
[00:39:27.400 --> 00:39:29.760]   who are actually bred artificially using,
[00:39:29.760 --> 00:39:32.440]   and they're using almost exactly the same technologies
[00:39:32.440 --> 00:39:33.960]   that we use at Genomic Prediction,
[00:39:33.960 --> 00:39:35.960]   but they're doing it to optimize milk production
[00:39:35.960 --> 00:39:37.100]   and stuff like this.
[00:39:37.100 --> 00:39:40.440]   So there is a big well of variance.
[00:39:40.440 --> 00:39:45.440]   It's a consequence of this super multi-polygenicity
[00:39:45.440 --> 00:39:46.700]   of the trait.
[00:39:46.700 --> 00:39:50.660]   And it does look like people could,
[00:39:50.660 --> 00:39:54.300]   coming back to your question about longevity,
[00:39:54.300 --> 00:39:56.620]   it does look like people could, quote,
[00:39:56.620 --> 00:40:00.940]   "Be engineered to live much longer than they currently do"
[00:40:00.940 --> 00:40:06.020]   by just say flipping the variance that reduce risk
[00:40:06.020 --> 00:40:08.740]   for individual diseases that tend to shorten your life.
[00:40:08.740 --> 00:40:10.720]   And then the question is back to,
[00:40:10.720 --> 00:40:13.000]   well, why didn't evolution give us lifespans
[00:40:13.000 --> 00:40:13.840]   of a thousand years?
[00:40:13.840 --> 00:40:14.660]   Like back in the Bible,
[00:40:14.660 --> 00:40:16.200]   people in the Bible used to live for a thousand years.
[00:40:16.200 --> 00:40:17.040]   Why don't we?
[00:40:17.040 --> 00:40:18.620]   I mean, that probably didn't really happen.
[00:40:18.620 --> 00:40:21.720]   But the question is,
[00:40:21.720 --> 00:40:25.840]   you have this very high dimensional space
[00:40:25.840 --> 00:40:27.640]   and you have a fitness function,
[00:40:27.640 --> 00:40:30.800]   and how big is the slope
[00:40:30.800 --> 00:40:33.200]   in a particular direction of that fitness function?
[00:40:33.200 --> 00:40:37.000]   Like how much more successful reproductively
[00:40:37.000 --> 00:40:40.640]   would Joe have been, Joe Caveman have been,
[00:40:40.640 --> 00:40:45.400]   if he lived to be 150 instead of only 100 or something?
[00:40:45.400 --> 00:40:47.840]   And there just hasn't been enough time
[00:40:47.840 --> 00:40:52.320]   to explore this super high dimensional space.
[00:40:52.320 --> 00:40:53.680]   That's the actual answer.
[00:40:53.680 --> 00:40:56.440]   But now we have the technology,
[00:40:56.440 --> 00:40:59.840]   we're gonna fucking explore it fast now.
[00:40:59.840 --> 00:41:01.840]   That's the point that people,
[00:41:01.840 --> 00:41:03.960]   the big light bulbs should go off.
[00:41:03.960 --> 00:41:07.360]   Like, no, we're mapping this space out now.
[00:41:07.360 --> 00:41:10.600]   Pretty confident in 10 years or so,
[00:41:10.600 --> 00:41:12.400]   the CRISPR gene editing technologies
[00:41:12.400 --> 00:41:16.400]   will be ready for massively multiplexed edits.
[00:41:16.400 --> 00:41:18.440]   And we're gonna start navigating
[00:41:18.440 --> 00:41:21.520]   in this high dimensional space as we like.
[00:41:21.520 --> 00:41:25.200]   So that's the more long-term consequence
[00:41:25.200 --> 00:41:28.060]   of these scientific insights.
[00:41:28.060 --> 00:41:29.120]   - Yeah, that's super interesting.
[00:41:29.120 --> 00:41:31.240]   And what do you think will be the plateau
[00:41:31.240 --> 00:41:34.200]   for a trait like, you know, how long you live?
[00:41:34.200 --> 00:41:36.240]   Well, 'cause four is, I guess,
[00:41:36.240 --> 00:41:38.160]   with the current data and techniques,
[00:41:38.160 --> 00:41:40.200]   you think it could be significantly greater than that or?
[00:41:40.200 --> 00:41:42.000]   - Well, we did a very simple calculation,
[00:41:42.000 --> 00:41:45.840]   which amazing that it gives the kind of the right result.
[00:41:45.840 --> 00:41:50.840]   We said like, given this polygenic predictor that we built,
[00:41:50.840 --> 00:41:52.360]   which isn't perfect, I mean, we're gonna,
[00:41:52.360 --> 00:41:54.720]   it's gonna improve a lot as we get more data.
[00:41:54.720 --> 00:41:57.920]   Given this polygenic predictor for overall health,
[00:41:59.080 --> 00:42:01.920]   which is used in selecting embryos today,
[00:42:01.920 --> 00:42:04.040]   if you just say like, well, out of a billion people,
[00:42:04.040 --> 00:42:06.040]   what's the best person typically?
[00:42:06.040 --> 00:42:08.280]   What would their score be on this index?
[00:42:08.280 --> 00:42:10.280]   And then how long would they be predicted to live?
[00:42:10.280 --> 00:42:11.580]   It's about 120 years.
[00:42:11.580 --> 00:42:13.880]   So it's actually spot on.
[00:42:13.880 --> 00:42:16.680]   It's basically, that's one in a billion type person
[00:42:16.680 --> 00:42:19.300]   lives to be like 120 years old or so, roughly.
[00:42:19.300 --> 00:42:22.920]   How much better can you do?
[00:42:22.920 --> 00:42:24.740]   Probably a lot better.
[00:42:24.740 --> 00:42:26.800]   Probably, I mean, I don't wanna speculate,
[00:42:26.800 --> 00:42:29.240]   but 'cause other effects, nonlinear effects,
[00:42:29.240 --> 00:42:30.480]   things that we're not taking into account
[00:42:30.480 --> 00:42:31.880]   will start to play a role at some point.
[00:42:31.880 --> 00:42:35.000]   So it's a little bit hard to estimate what the limiting,
[00:42:35.000 --> 00:42:37.400]   what the true limiting factors will be.
[00:42:37.400 --> 00:42:40.000]   But the one statement which is super robust,
[00:42:40.000 --> 00:42:42.960]   and I'll stand by it, I'll debate any Nobel laureate
[00:42:42.960 --> 00:42:45.620]   in biology or whatever who wants to talk about it.
[00:42:45.620 --> 00:42:49.120]   There's clearly a lot of variants available
[00:42:49.120 --> 00:42:51.200]   to be selected on or edited.
[00:42:51.200 --> 00:42:52.600]   That's just, there's no question about that.
[00:42:52.600 --> 00:42:55.080]   And that's been established in animal breeding
[00:42:55.080 --> 00:42:59.160]   and plant breeding for a long time now.
[00:42:59.160 --> 00:43:02.880]   So we can, if you want a chicken that grows to be this big
[00:43:02.880 --> 00:43:04.200]   instead of this big, you can do it.
[00:43:04.200 --> 00:43:06.660]   If you want a cow that produces literally 10 times
[00:43:06.660 --> 00:43:11.480]   or 100 times more milk than a regular cow, you can do it.
[00:43:11.480 --> 00:43:14.660]   The egg you ate for breakfast this morning,
[00:43:14.660 --> 00:43:18.220]   those bioengineered chickens, they lay almost an egg a day.
[00:43:18.220 --> 00:43:22.080]   A chicken in the wild lays like an egg a month.
[00:43:22.080 --> 00:43:23.720]   How the hell did we do that?
[00:43:23.720 --> 00:43:26.560]   By genetic engineering, that's how we did it, so.
[00:43:26.560 --> 00:43:27.400]   - Yeah, yeah.
[00:43:27.400 --> 00:43:30.920]   And that was just brute artificial selection.
[00:43:30.920 --> 00:43:33.000]   No fancy machine learning there.
[00:43:33.000 --> 00:43:34.800]   - Last 10 years, last 10 years,
[00:43:34.800 --> 00:43:37.400]   it's gotten sophisticated machine learning,
[00:43:37.400 --> 00:43:42.400]   genotyping of chickens, artificial insemination,
[00:43:42.400 --> 00:43:47.240]   modeling of the traits using ML.
[00:43:47.240 --> 00:43:50.660]   Last 10 years, it's basically, for cow breeding now,
[00:43:50.660 --> 00:43:53.520]   it's totally done by ML now.
[00:43:53.520 --> 00:43:54.360]   - I had no idea.
[00:43:54.360 --> 00:43:55.520]   That's super interesting.
[00:43:55.520 --> 00:44:01.360]   So you mentioned that you're accumulating data
[00:44:01.360 --> 00:44:02.880]   and improving your techniques over time.
[00:44:02.880 --> 00:44:04.560]   Is there a first mover advantage
[00:44:04.560 --> 00:44:07.480]   to a genomic prediction company like this,
[00:44:07.480 --> 00:44:11.560]   or is it just whoever has a newest, best algorithm
[00:44:11.560 --> 00:44:13.320]   for going through the biobank data?
[00:44:13.320 --> 00:44:18.080]   - That's another super question.
[00:44:18.080 --> 00:44:23.080]   So for your entrepreneurs in your audience,
[00:44:23.080 --> 00:44:25.080]   I would say in the short run, if you ask like,
[00:44:25.080 --> 00:44:29.360]   what should the valuation of GP be?
[00:44:29.360 --> 00:44:31.400]   That's how the venture guys
[00:44:31.400 --> 00:44:33.940]   would want me to answer the question.
[00:44:33.940 --> 00:44:35.400]   There is a huge first mover advantage
[00:44:35.400 --> 00:44:39.540]   because very important is the channel relationships
[00:44:39.540 --> 00:44:41.620]   between us and the clinics.
[00:44:41.620 --> 00:44:44.080]   And nobody's gonna be able to get in there very easily
[00:44:44.080 --> 00:44:47.020]   when they come later, because we're developing trust
[00:44:47.020 --> 00:44:49.880]   and a big track record with clinics all over the world,
[00:44:49.880 --> 00:44:50.920]   and we're well known.
[00:44:52.320 --> 00:44:56.680]   Could 23andMe or some company that has a huge amount of data
[00:44:56.680 --> 00:44:59.520]   and if they were to actually get better AI/ML people
[00:44:59.520 --> 00:45:02.640]   working on this, could they kind of blow us away
[00:45:02.640 --> 00:45:04.120]   a little bit and build better predictors
[00:45:04.120 --> 00:45:06.440]   'cause they have just much more data than we do?
[00:45:06.440 --> 00:45:07.840]   Possibly, yes.
[00:45:07.840 --> 00:45:12.320]   Now there's a core expertise that we have
[00:45:12.320 --> 00:45:14.560]   in doing this kind of work for years and years and years
[00:45:14.560 --> 00:45:16.480]   that we're just really good at it.
[00:45:16.480 --> 00:45:19.440]   And so even though we don't have as much data as 23andMe,
[00:45:19.440 --> 00:45:21.240]   we might still, our predictors are better
[00:45:21.240 --> 00:45:22.360]   than theirs right now.
[00:45:22.360 --> 00:45:27.240]   And I'm out there all the time working with biobanks
[00:45:27.240 --> 00:45:29.440]   all around the world, like in countries like,
[00:45:29.440 --> 00:45:30.440]   I don't wanna say all the names,
[00:45:30.440 --> 00:45:32.820]   but other countries trying to get my hands
[00:45:32.820 --> 00:45:34.240]   on as much data as I can.
[00:45:34.240 --> 00:45:40.340]   But there may not be a lasting advantage
[00:45:40.340 --> 00:45:43.240]   beyond the actual business channel connections
[00:45:43.240 --> 00:45:44.480]   to that particular market.
[00:45:44.480 --> 00:45:49.000]   It may not be a defensible, purely scientific moat
[00:45:49.000 --> 00:45:50.340]   around the company.
[00:45:50.340 --> 00:45:52.360]   We do have patents on specific technologies
[00:45:52.360 --> 00:45:54.440]   about how to do the genotyping
[00:45:54.440 --> 00:45:58.320]   or how to do error correction on embryo DNA
[00:45:58.320 --> 00:45:59.160]   and stuff like this.
[00:45:59.160 --> 00:46:00.400]   We do have patents on stuff like that,
[00:46:00.400 --> 00:46:02.520]   but this general idea of who's gonna be the best
[00:46:02.520 --> 00:46:04.640]   at predicting human traits from DNA,
[00:46:04.640 --> 00:46:06.880]   it's unclear who's gonna be the winner in that race.
[00:46:06.880 --> 00:46:10.400]   Maybe it'll be the Chinese government in 50 years.
[00:46:10.400 --> 00:46:12.080]   Who knows?
[00:46:12.080 --> 00:46:13.080]   Yeah, that's interesting.
[00:46:13.080 --> 00:46:15.840]   I mean, if you think about a company like Google,
[00:46:15.840 --> 00:46:17.080]   theoretically it's possible you could come up
[00:46:17.080 --> 00:46:19.780]   with a better algorithm than PageRank and then beat them,
[00:46:19.780 --> 00:46:21.880]   but it just seems like probably the engineer at Google
[00:46:21.880 --> 00:46:24.120]   is gonna be the one that comes up with whatever edge case
[00:46:24.120 --> 00:46:26.800]   or whatever improvement is possible.
[00:46:26.800 --> 00:46:29.520]   - That's exactly, see, it's exactly what I would say.
[00:46:29.520 --> 00:46:31.480]   I would say like, yeah, maybe, I mean,
[00:46:31.480 --> 00:46:33.800]   PageRank actually by now is totally deprecated,
[00:46:33.800 --> 00:46:37.780]   but even if somebody else comes up
[00:46:37.780 --> 00:46:40.400]   with a somewhat better algorithm or somewhat better,
[00:46:40.400 --> 00:46:42.280]   maybe they have a little bit more data,
[00:46:42.280 --> 00:46:44.520]   if you have a team that's been doing this for a long time
[00:46:44.520 --> 00:46:45.720]   and you're really focused and good,
[00:46:45.720 --> 00:46:46.680]   it's still tough to beat you,
[00:46:46.680 --> 00:46:48.880]   especially if you have a lead in the market.
[00:46:48.880 --> 00:46:52.300]   - Yeah, yeah, and then, so what layer of the stack do you,
[00:46:52.300 --> 00:46:54.380]   so you guys are, are you guys doing the actual biopsy
[00:46:54.380 --> 00:46:55.820]   or is it just that they upload the genome
[00:46:55.820 --> 00:46:57.140]   and you're the one processing
[00:46:57.140 --> 00:46:58.380]   and just giving recommendations?
[00:46:58.380 --> 00:47:01.500]   Is it just like an API call basically or?
[00:47:01.500 --> 00:47:03.620]   - So it's great, I love your question.
[00:47:03.620 --> 00:47:07.460]   So it is totally standard.
[00:47:07.460 --> 00:47:10.540]   Every good IVF clinic in the world
[00:47:10.540 --> 00:47:12.460]   regularly takes embryo biopsies.
[00:47:12.460 --> 00:47:13.500]   So that's totally standard.
[00:47:13.500 --> 00:47:16.480]   It's like a lab tech doing that, okay?
[00:47:16.480 --> 00:47:18.820]   And then what happens is they take the little sample
[00:47:18.820 --> 00:47:21.100]   and they put it on ice and they just ship it.
[00:47:21.100 --> 00:47:25.980]   And the DNA as a molecule is extremely robust and stable.
[00:47:25.980 --> 00:47:28.360]   In fact, my other startup solves crimes
[00:47:28.360 --> 00:47:31.660]   that are a hundred years old from DNA
[00:47:31.660 --> 00:47:35.420]   that we get from some semen stain on some rape victim,
[00:47:35.420 --> 00:47:37.900]   you know, serial killer victims bra strap.
[00:47:37.900 --> 00:47:39.660]   We've done stuff like that.
[00:47:39.660 --> 00:47:42.780]   - Jack the Ripper, when are we gonna solve that mystery?
[00:47:42.780 --> 00:47:45.640]   - If they can give me samples, we can get into that.
[00:47:47.520 --> 00:47:49.920]   For example, we just learned that you can recover DNA
[00:47:49.920 --> 00:47:53.280]   pretty well from the, like if someone licks a stamp
[00:47:53.280 --> 00:47:55.400]   and puts it on their correspondence.
[00:47:55.400 --> 00:47:58.720]   So that, I mean, if you can do Neanderthals,
[00:47:58.720 --> 00:48:00.700]   you can do a lot for solving crimes.
[00:48:00.700 --> 00:48:06.700]   So in the IVF workflow, our lab, which is in New Jersey,
[00:48:06.700 --> 00:48:10.280]   can service every clinic in the world
[00:48:10.280 --> 00:48:12.560]   because they just take the biopsy,
[00:48:12.560 --> 00:48:14.480]   they put it in a standard shipping container
[00:48:14.480 --> 00:48:16.340]   and they just send it to us.
[00:48:16.340 --> 00:48:19.640]   And then we actually genotype the DNA in our lab,
[00:48:19.640 --> 00:48:23.280]   but we've actually trained a few of the bigger clinics
[00:48:23.280 --> 00:48:25.400]   to actually do the genotyping on their site.
[00:48:25.400 --> 00:48:26.220]   And at that point,
[00:48:26.220 --> 00:48:28.160]   it's just like they upload some data into the cloud
[00:48:28.160 --> 00:48:30.480]   and then they get back some stuff from our platform.
[00:48:30.480 --> 00:48:33.480]   So at that point where it's gonna be the whole world, man,
[00:48:33.480 --> 00:48:37.040]   every human who wants their kid to be healthy
[00:48:37.040 --> 00:48:39.440]   and get the best they can,
[00:48:39.440 --> 00:48:41.340]   it's gonna, that data is gonna come up to us
[00:48:41.340 --> 00:48:43.160]   and the report is gonna come back down
[00:48:43.160 --> 00:48:44.860]   to their IVF physician.
[00:48:44.860 --> 00:48:47.400]   - Right, yeah, which is great if you think,
[00:48:47.400 --> 00:48:50.840]   let's say you think that there's a potential
[00:48:50.840 --> 00:48:53.720]   that this technology might get regulated in some way.
[00:48:53.720 --> 00:48:56.040]   You could just have like, just go to Mexico or something,
[00:48:56.040 --> 00:48:59.600]   have them upload the genome.
[00:48:59.600 --> 00:49:01.160]   You don't care where they upload it from.
[00:49:01.160 --> 00:49:04.080]   And then you can just get the recommendations there.
[00:49:04.080 --> 00:49:06.200]   - Yeah, I think we're gonna evolve to a point where,
[00:49:06.200 --> 00:49:08.000]   because the genotyping technology
[00:49:08.000 --> 00:49:09.280]   is getting better and better.
[00:49:09.280 --> 00:49:11.640]   Eventually we are gonna be out of the wet
[00:49:12.480 --> 00:49:14.700]   part of this business and only in the bit
[00:49:14.700 --> 00:49:16.260]   and cloud part of this business.
[00:49:16.260 --> 00:49:19.540]   Because eventually the clinic, no matter where it is,
[00:49:19.540 --> 00:49:22.740]   they're gonna have a little sequencer, which is this big.
[00:49:22.740 --> 00:49:23.920]   And their tech is gonna do it
[00:49:23.920 --> 00:49:25.740]   and then they're just gonna hit upload.
[00:49:25.740 --> 00:49:29.060]   And then they get the report back like three seconds later
[00:49:29.060 --> 00:49:31.380]   from us for the physician to look at
[00:49:31.380 --> 00:49:34.300]   and the parents can look at it on their phone or whatever.
[00:49:34.300 --> 00:49:37.100]   Actually, we're basically there actually with some clinics.
[00:49:37.100 --> 00:49:40.420]   So yeah, it's gonna be tough to regulate
[00:49:40.420 --> 00:49:42.340]   because it's just bits, right?
[00:49:42.340 --> 00:49:44.360]   So you have the bits
[00:49:44.360 --> 00:49:47.760]   and you're in some repressive, terrible country
[00:49:47.760 --> 00:49:51.740]   that doesn't allow you to select for some special traits
[00:49:51.740 --> 00:49:53.380]   that people are nervous about.
[00:49:53.380 --> 00:49:56.740]   But you just upload it to some vendor who's in Singapore
[00:49:56.740 --> 00:49:59.780]   or in some free country.
[00:49:59.780 --> 00:50:02.540]   And they give you the report back.
[00:50:02.540 --> 00:50:03.580]   It doesn't have to be us.
[00:50:03.580 --> 00:50:06.100]   I mean, we don't do the edgy stuff.
[00:50:06.100 --> 00:50:08.740]   We only do the health related stuff right now.
[00:50:08.740 --> 00:50:12.220]   But if you wanna know how tall this embryo is gonna be,
[00:50:12.220 --> 00:50:13.700]   I'll tell you a mind blower.
[00:50:13.700 --> 00:50:18.820]   When you do face recognition in AI,
[00:50:18.820 --> 00:50:20.740]   you're basically mapping someone's face
[00:50:20.740 --> 00:50:22.380]   into a parameter space of like,
[00:50:22.380 --> 00:50:25.380]   I think it's on the order of hundreds of parameters, right?
[00:50:25.380 --> 00:50:29.000]   Each of those parameters is super heritable.
[00:50:29.000 --> 00:50:30.860]   So in other words, if I take two twins
[00:50:30.860 --> 00:50:34.580]   and I photograph them and the algorithm gives me the value
[00:50:34.580 --> 00:50:36.140]   of that parameter for twin one and twin two,
[00:50:36.140 --> 00:50:37.440]   they're very close, obviously.
[00:50:37.440 --> 00:50:39.940]   That's why I can't tell the two twins apart.
[00:50:39.940 --> 00:50:42.780]   The face recognition can ultimately tell the twins apart.
[00:50:42.780 --> 00:50:44.460]   They're really good face recognition.
[00:50:44.460 --> 00:50:46.460]   But you can just conclude
[00:50:46.460 --> 00:50:47.980]   that almost all these parameters are the same
[00:50:47.980 --> 00:50:48.980]   for those twins.
[00:50:48.980 --> 00:50:50.460]   So it's highly heritable.
[00:50:50.460 --> 00:50:54.620]   So we're gonna get to a point soon
[00:50:54.620 --> 00:50:57.540]   where I can do the inverse problem
[00:50:57.540 --> 00:51:02.380]   where I have your DNA and I predict each of those parameters
[00:51:02.380 --> 00:51:04.380]   in the face recognition algorithm.
[00:51:04.380 --> 00:51:07.340]   And then from that, I reconstruct the face.
[00:51:07.340 --> 00:51:10.260]   So I say like this embryo, when she's 16,
[00:51:10.260 --> 00:51:11.740]   this is what she's gonna look like.
[00:51:11.740 --> 00:51:14.220]   When she's 32, this is what she's gonna look like.
[00:51:14.220 --> 00:51:16.460]   And I'll be able to do that for sure.
[00:51:16.460 --> 00:51:20.600]   It's just a data, it's just an AI/ML problem right now.
[00:51:20.600 --> 00:51:23.300]   But the basic biology is clearly gonna work.
[00:51:23.300 --> 00:51:26.420]   So then you're gonna be able to say like,
[00:51:26.420 --> 00:51:27.820]   oh, look, here's a report.
[00:51:27.820 --> 00:51:29.860]   Look, let's, embryo four is so cute.
[00:51:29.860 --> 00:51:31.900]   Why don't we, you know.
[00:51:31.900 --> 00:51:33.020]   - Before you get married.
[00:51:33.020 --> 00:51:36.580]   - Now, we don't do that, but it's gonna be possible.
[00:51:36.580 --> 00:51:37.620]   - Right, before you get married,
[00:51:37.620 --> 00:51:40.040]   you'll wanna see what their genotype implies
[00:51:40.040 --> 00:51:43.900]   about their face's longevity.
[00:51:43.900 --> 00:51:44.740]   Yeah, it's interesting.
[00:51:44.740 --> 00:51:46.180]   You hear stories about these cartel leaders
[00:51:46.180 --> 00:51:49.580]   who will get plastic surgery or something to evade the law.
[00:51:49.580 --> 00:51:51.980]   You could just have a check where you're like,
[00:51:51.980 --> 00:51:54.820]   lick this, lick this, lick this slab.
[00:51:54.820 --> 00:51:57.460]   And then, yeah, does that match the face
[00:51:57.460 --> 00:51:58.940]   that you would have had five years ago
[00:51:58.940 --> 00:52:00.840]   when we caught you on tape?
[00:52:00.840 --> 00:52:03.660]   - Yeah, well, and also, you don't,
[00:52:03.660 --> 00:52:05.540]   it's, there's a little bit back to old school Gattaca,
[00:52:05.540 --> 00:52:07.460]   but you don't even need the face.
[00:52:07.460 --> 00:52:10.260]   You can just say like, I'm gonna take a few molecules
[00:52:10.260 --> 00:52:12.900]   of, you know, skin cells from you.
[00:52:12.900 --> 00:52:15.220]   I'm gonna take a few skin cells off of you
[00:52:15.220 --> 00:52:16.060]   and just phenotype you.
[00:52:16.060 --> 00:52:17.940]   I know exactly who you are.
[00:52:17.940 --> 00:52:22.940]   So, I've had conversations with the spooky intel folks
[00:52:22.940 --> 00:52:26.060]   about, you know, they're very interested in like,
[00:52:26.060 --> 00:52:29.620]   oh, if some Russian diplomat comes in
[00:52:29.620 --> 00:52:31.100]   and we think he's actually a spy,
[00:52:31.100 --> 00:52:33.180]   but he's, you know, with the embassy over there
[00:52:33.180 --> 00:52:36.140]   and he has a coffee with me and I save the cup
[00:52:36.140 --> 00:52:38.820]   and send it to my brother, my buddy at Langley,
[00:52:38.820 --> 00:52:40.140]   can we figure out who this guy is
[00:52:40.140 --> 00:52:43.440]   and that he has a daughter who's going to Choate?
[00:52:43.440 --> 00:52:46.380]   You know, can do all that now.
[00:52:46.380 --> 00:52:49.860]   - Oh, that's, I, it seems to me,
[00:52:49.860 --> 00:52:51.540]   if that's true, then like in the future,
[00:52:51.540 --> 00:52:54.600]   people will be so concerned, like world leaders
[00:52:54.600 --> 00:52:56.740]   or something, when they're visiting a foreign country,
[00:52:56.740 --> 00:52:58.700]   they're not gonna wanna eat anything, drink it.
[00:52:58.700 --> 00:53:00.940]   Like, they'll be wearing like a hazmat suit
[00:53:00.940 --> 00:53:02.700]   to make sure they don't like lose a hair follicle.
[00:53:02.700 --> 00:53:04.580]   - The next time Pelosi goes,
[00:53:04.580 --> 00:53:08.460]   she's gonna be in like a space suit, if she cares.
[00:53:08.460 --> 00:53:10.780]   Or the other thing is they're just gonna give in.
[00:53:10.780 --> 00:53:13.060]   They're just gonna be like, yeah, my DNA is everywhere.
[00:53:13.060 --> 00:53:15.760]   If I'm a public figure, my DNA, I can't track it.
[00:53:15.760 --> 00:53:16.600]   It's all over.
[00:53:16.600 --> 00:53:18.780]   - Yeah, but the thing is, like, there's so much as you,
[00:53:18.780 --> 00:53:19.980]   I'm sure you know, there's like speculation
[00:53:19.980 --> 00:53:21.940]   that Putin might have cancer or something.
[00:53:21.940 --> 00:53:23.780]   If we have his DNA, we can just see like,
[00:53:23.780 --> 00:53:25.580]   oh, actually like his probability of having cancer
[00:53:25.580 --> 00:53:28.820]   at age 70 or whatever he is, is, you know, 85%.
[00:53:28.820 --> 00:53:31.140]   So yeah, that's like a verified rumor.
[00:53:31.140 --> 00:53:32.020]   You know, that would be interesting.
[00:53:32.020 --> 00:53:35.060]   - I don't think that would be very definitive.
[00:53:35.060 --> 00:53:36.460]   Like, I don't think we'll reach that point
[00:53:36.460 --> 00:53:38.180]   where you could say, yeah, Putin definitely has cancer
[00:53:38.180 --> 00:53:41.180]   because of his DNA, which I could have known
[00:53:41.180 --> 00:53:42.180]   when he was an embryo.
[00:53:42.180 --> 00:53:43.580]   I don't think it's gonna reach that level,
[00:53:43.580 --> 00:53:45.500]   but we could say, yeah, he is high risk
[00:53:45.500 --> 00:53:46.920]   for this kind of cancer.
[00:53:46.920 --> 00:53:48.140]   - Yeah. - You could say that.
[00:53:48.140 --> 00:53:49.620]   - Yeah, yeah, yeah.
[00:53:49.620 --> 00:53:54.220]   So if this like reaches, if like, let's say in 50 years
[00:53:54.220 --> 00:53:56.740]   or 100 years, all the majority of population is doing this.
[00:53:56.740 --> 00:53:58.900]   And if that means that the diseases
[00:53:58.900 --> 00:54:03.460]   that are highly heritable get pruned out of the population,
[00:54:03.460 --> 00:54:05.580]   does that mean we'll only be left with the diseases
[00:54:05.580 --> 00:54:07.020]   that are like lifestyle diseases?
[00:54:07.020 --> 00:54:09.100]   So you won't get like breast cancer anymore,
[00:54:09.100 --> 00:54:12.580]   but you will still get, you know, you'll still get like fat
[00:54:12.580 --> 00:54:15.100]   or I don't know, whatever, lung cancer
[00:54:15.100 --> 00:54:17.100]   from smoking or something.
[00:54:17.100 --> 00:54:21.020]   - I think it's hard to discuss the asymptotic limit
[00:54:21.020 --> 00:54:22.520]   of what's gonna happen here.
[00:54:22.520 --> 00:54:27.380]   I'm not very confident about making predictions like that.
[00:54:27.380 --> 00:54:31.420]   You know, it could be that we'll get to the point
[00:54:31.420 --> 00:54:35.220]   where everybody is, well, everybody who's rich
[00:54:35.220 --> 00:54:38.540]   or who's been through this stuff for a while,
[00:54:38.540 --> 00:54:41.260]   especially if we get the editing working,
[00:54:41.260 --> 00:54:45.180]   is super low risk for all the, like the top 20 killers,
[00:54:45.180 --> 00:54:47.060]   the diseases which, you know,
[00:54:47.060 --> 00:54:49.660]   have most life expectancy impact.
[00:54:49.660 --> 00:54:51.140]   And yeah, maybe those people live
[00:54:51.140 --> 00:54:53.140]   to be 300 years old naturally.
[00:54:53.140 --> 00:54:55.860]   I don't think that's excluded at all.
[00:54:55.860 --> 00:54:59.140]   So I think that's within the realm of possibility.
[00:54:59.140 --> 00:55:02.420]   But you know, it's gonna happen for a few lucky,
[00:55:02.420 --> 00:55:04.900]   you know, Elon Musk-like people before it happens
[00:55:04.900 --> 00:55:07.360]   for schlubs like you and me.
[00:55:07.360 --> 00:55:09.260]   (laughing)
[00:55:09.260 --> 00:55:12.380]   You know, there's gonna be very angry inequality protesters
[00:55:12.380 --> 00:55:14.540]   about, you know, the Trump grandchildren
[00:55:14.540 --> 00:55:18.380]   who models predict will live to be 200 years old.
[00:55:18.380 --> 00:55:19.500]   - Right. - You know.
[00:55:19.500 --> 00:55:20.340]   - Yeah, yeah, yeah.
[00:55:20.340 --> 00:55:22.540]   - Like people are not gonna be happy about that.
[00:55:22.540 --> 00:55:24.100]   - Yeah, that's so interesting.
[00:55:25.100 --> 00:55:26.620]   And okay, so one way to think
[00:55:26.620 --> 00:55:28.620]   about these different embryos is,
[00:55:28.620 --> 00:55:31.700]   like if you're gonna produce multiple embryos,
[00:55:31.700 --> 00:55:33.020]   you can get to select from one of them.
[00:55:33.020 --> 00:55:34.900]   Each of them is like,
[00:55:34.900 --> 00:55:36.860]   each of them is like a call option, right?
[00:55:36.860 --> 00:55:39.820]   And therefore, you probably wanna optimize
[00:55:39.820 --> 00:55:42.260]   for volatility as much or if not more
[00:55:42.260 --> 00:55:44.500]   than just the expected value of the trait.
[00:55:44.500 --> 00:55:46.820]   And so I'm wondering if there's mechanisms
[00:55:46.820 --> 00:55:49.500]   where you can, I don't know, like increase the volatility
[00:55:49.500 --> 00:55:51.180]   in meiosis or in some other process.
[00:55:51.180 --> 00:55:53.340]   So you just get a higher variance
[00:55:53.340 --> 00:55:55.620]   you can just select from the tail better.
[00:55:55.620 --> 00:55:58.260]   - Well, I'll tell you something related to that,
[00:55:58.260 --> 00:55:59.340]   which is quite amusing.
[00:55:59.340 --> 00:56:03.820]   So I had conversations with some pretty senior people
[00:56:03.820 --> 00:56:08.820]   at the company that owns all the dating apps.
[00:56:08.820 --> 00:56:12.060]   So you can look up, you can figure out what company this is,
[00:56:12.060 --> 00:56:15.540]   but they own Tinder and Match and stuff like this.
[00:56:15.540 --> 00:56:19.420]   And they're kind of interested in,
[00:56:19.420 --> 00:56:21.580]   wow, what if we have a special feature
[00:56:21.580 --> 00:56:23.900]   where instead of Tinder Gold or Platinum,
[00:56:23.900 --> 00:56:28.380]   you upload your genome and you match,
[00:56:28.380 --> 00:56:31.100]   we talk about how well you match the other person
[00:56:31.100 --> 00:56:32.620]   based on your genome.
[00:56:32.620 --> 00:56:33.860]   Actually, one person told me something
[00:56:33.860 --> 00:56:35.460]   which was really shocking is that
[00:56:35.460 --> 00:56:38.780]   apparently guys lie about their height on these apps.
[00:56:38.780 --> 00:56:41.060]   And if you could have a DNA verified-
[00:56:41.060 --> 00:56:42.460]   - Truly shocked.
[00:56:42.460 --> 00:56:43.300]   Truly shocked.
[00:56:43.300 --> 00:56:47.140]   - And if you could have a DNA verified height on,
[00:56:47.140 --> 00:56:49.660]   'cause our accuracy is like an inch or something.
[00:56:49.660 --> 00:56:54.540]   So it would prevent like really gross distortions.
[00:56:54.540 --> 00:56:57.940]   Like someone claims they're 6'2" and they're actually 5'9".
[00:56:57.940 --> 00:57:00.540]   Probably the DNA could say that's unlikely actually.
[00:57:00.540 --> 00:57:04.220]   But no, the application to what you were discussing
[00:57:04.220 --> 00:57:08.420]   is more like, let's suppose that we're selecting
[00:57:08.420 --> 00:57:10.980]   on intelligence or something.
[00:57:10.980 --> 00:57:14.260]   And let's suppose that the regions
[00:57:14.260 --> 00:57:19.260]   where your girlfriend has all the plus stuff,
[00:57:19.580 --> 00:57:22.140]   is complementary to the regions
[00:57:22.140 --> 00:57:24.740]   where you have your plus stuff.
[00:57:24.740 --> 00:57:28.340]   So we could model that and say like your kids,
[00:57:28.340 --> 00:57:32.140]   just because of the complementarity
[00:57:32.140 --> 00:57:34.020]   of the structure of your genome
[00:57:34.020 --> 00:57:36.460]   in the regions that affect intelligence,
[00:57:36.460 --> 00:57:38.540]   you're very likely to have some super smart kids
[00:57:38.540 --> 00:57:43.420]   way above the mean of you and your girlfriend's values.
[00:57:43.420 --> 00:57:44.980]   So you could actually say things like,
[00:57:44.980 --> 00:57:47.660]   yeah, it's better for you to marry that girl
[00:57:47.660 --> 00:57:48.580]   than that girl.
[00:57:49.580 --> 00:57:52.260]   As long as you're going to go through embryo selection,
[00:57:52.260 --> 00:57:54.900]   we can throw out the bad outliers.
[00:57:54.900 --> 00:57:55.740]   - That is so fascinating.
[00:57:55.740 --> 00:57:57.700]   - So all that's technically feasible.
[00:57:57.700 --> 00:57:59.860]   And I think actually it's true
[00:57:59.860 --> 00:58:03.260]   that one of the earliest patent applications,
[00:58:03.260 --> 00:58:05.660]   they'll all deny it now.
[00:58:05.660 --> 00:58:07.380]   What's her name?
[00:58:07.380 --> 00:58:08.940]   Gosh, I can't remember her name.
[00:58:08.940 --> 00:58:12.220]   The CEO of 23andMe, Wojcicki.
[00:58:12.220 --> 00:58:13.620]   - Yeah.
[00:58:13.620 --> 00:58:14.900]   - She'll deny it now.
[00:58:14.900 --> 00:58:17.260]   But I think if you look in the patent database,
[00:58:17.260 --> 00:58:20.940]   one of the very earliest patents that 23andMe filed
[00:58:20.940 --> 00:58:22.420]   when they were like still a small startup
[00:58:22.420 --> 00:58:24.620]   was about exactly this.
[00:58:24.620 --> 00:58:28.780]   It's like advising parents about mating
[00:58:28.780 --> 00:58:30.660]   and how their kids would turn out and stuff like this.
[00:58:30.660 --> 00:58:33.700]   So we don't even go that far in GP.
[00:58:33.700 --> 00:58:34.860]   We don't even talk about stuff like that,
[00:58:34.860 --> 00:58:35.820]   but they were thinking about it
[00:58:35.820 --> 00:58:37.580]   when they founded 23andMe.
[00:58:37.580 --> 00:58:39.420]   - That is unbelievably interesting.
[00:58:39.420 --> 00:58:42.540]   But by the way, speaking of pie,
[00:58:42.540 --> 00:58:45.060]   this just occurred to me,
[00:58:45.060 --> 00:58:47.340]   but it's supposed to be highly heritable,
[00:58:47.340 --> 00:58:49.700]   but especially people in Asian countries
[00:58:49.700 --> 00:58:52.500]   who we have the experience of having grandparents
[00:58:52.500 --> 00:58:53.420]   that are much shorter than us
[00:58:53.420 --> 00:58:55.220]   and then parents that are shorter than us,
[00:58:55.220 --> 00:58:56.620]   which is just that the environment
[00:58:56.620 --> 00:58:57.740]   has a big part to play in it,
[00:58:57.740 --> 00:59:00.140]   just like malnutrition or something.
[00:59:00.140 --> 00:59:03.340]   Yeah, so how do you scare that the fact
[00:59:03.340 --> 00:59:05.100]   that often our parents are shorter than us
[00:59:05.100 --> 00:59:06.700]   with the idea that height
[00:59:06.700 --> 00:59:08.420]   is supposed to be super heritable?
[00:59:08.420 --> 00:59:09.540]   - Another great observation.
[00:59:09.540 --> 00:59:13.300]   So the real correct scientific statement
[00:59:13.300 --> 00:59:18.300]   is we can predict height for people
[00:59:18.300 --> 00:59:23.180]   who will be born and raised in a favorable environment.
[00:59:23.180 --> 00:59:26.020]   So in other words, if you live close to a McDonald's
[00:59:26.020 --> 00:59:28.820]   and you can afford all the food you want,
[00:59:28.820 --> 00:59:34.860]   then the height phenotype becomes super heritable
[00:59:34.860 --> 00:59:36.700]   because the environmental variation now
[00:59:36.700 --> 00:59:38.700]   doesn't matter very much.
[00:59:38.700 --> 00:59:41.740]   But you and I both know,
[00:59:41.740 --> 00:59:43.940]   if we go back to where our ancestors came from,
[00:59:43.940 --> 00:59:45.460]   people are a lot smaller.
[00:59:45.460 --> 00:59:46.980]   And also, if you look at how much food,
[00:59:46.980 --> 00:59:49.620]   how many calories and how much protein and calcium they eat,
[00:59:49.620 --> 00:59:50.860]   it's totally different than what I ate
[00:59:50.860 --> 00:59:51.940]   and what you ate growing up.
[00:59:51.940 --> 00:59:53.700]   So we're not saying,
[00:59:53.700 --> 00:59:57.060]   we're never saying the environmental effects are zero.
[00:59:57.060 --> 00:59:59.820]   We're saying for people raised
[00:59:59.820 --> 01:00:02.100]   in a certain very favorable environment,
[01:00:02.100 --> 01:00:06.300]   maybe the genes are a cap on what can be achieved.
[01:00:06.300 --> 01:00:09.540]   And we can estimate, you know, we can predict that.
[01:00:09.540 --> 01:00:10.900]   - Yeah.
[01:00:10.900 --> 01:00:13.300]   - In fact, in our data, actually, we have,
[01:00:13.300 --> 01:00:16.900]   like I have data from Asia where, yeah, you can see
[01:00:16.900 --> 01:00:20.740]   there clearly are much bigger environmental effects,
[01:00:20.740 --> 01:00:23.620]   age effects, actually, just older people
[01:00:23.620 --> 01:00:26.460]   for fixed polygenic score on the trait
[01:00:26.460 --> 01:00:28.460]   are much shorter than younger people.
[01:00:28.460 --> 01:00:31.660]   - Oh, okay, interesting.
[01:00:31.660 --> 01:00:33.540]   Yeah, that actually raises the next question
[01:00:33.540 --> 01:00:35.020]   I was about to ask, which was,
[01:00:36.020 --> 01:00:39.940]   how applicable are these scores across,
[01:00:39.940 --> 01:00:42.220]   you know, different ancestral populations?
[01:00:42.220 --> 01:00:45.140]   - Huge, huge problem right now
[01:00:45.140 --> 01:00:48.220]   because most of the data is from Europeans.
[01:00:48.220 --> 01:00:51.460]   And what happens is that as you,
[01:00:51.460 --> 01:00:54.140]   if you train a predictor in this ancestry group
[01:00:54.140 --> 01:00:57.060]   and you go to a more distant ancestry group,
[01:00:57.060 --> 01:00:59.980]   there's a fall off in the quality of prediction.
[01:00:59.980 --> 01:01:03.940]   And this is, again, this is like frontier questions.
[01:01:03.940 --> 01:01:05.420]   So we don't know the answer for sure,
[01:01:05.420 --> 01:01:08.140]   but most people believe or many people believe
[01:01:08.140 --> 01:01:10.380]   that what happens is that there's a certain
[01:01:10.380 --> 01:01:12.460]   correlational structure in each population
[01:01:12.460 --> 01:01:15.380]   where if I know the state of this SNP,
[01:01:15.380 --> 01:01:18.460]   I can predict the state of these neighboring SNPs.
[01:01:18.460 --> 01:01:21.100]   And that is a product of the mating patterns
[01:01:21.100 --> 01:01:24.180]   and the ancestry, you know, of that group.
[01:01:24.180 --> 01:01:26.460]   And sometimes the predictor,
[01:01:26.460 --> 01:01:28.980]   which is just using statistical power to figure things out,
[01:01:28.980 --> 01:01:31.580]   will grab one of these SNPs as a tag
[01:01:31.580 --> 01:01:35.140]   for the truly causal SNP that's in there.
[01:01:35.140 --> 01:01:37.060]   It doesn't really know which one is truly causal.
[01:01:37.060 --> 01:01:38.940]   It's just grabbing a tag,
[01:01:38.940 --> 01:01:40.980]   but the tagging quality falls off
[01:01:40.980 --> 01:01:42.380]   if you then go to another population.
[01:01:42.380 --> 01:01:45.740]   Like this was a very good tag for the truly causal SNP
[01:01:45.740 --> 01:01:46.820]   in the British population,
[01:01:46.820 --> 01:01:50.060]   but it's not so good a tag in the South Asian population
[01:01:50.060 --> 01:01:53.780]   for the truly causal SNP, which we hypothesize is the same.
[01:01:53.780 --> 01:01:56.820]   It's the same underlying genetic architecture
[01:01:56.820 --> 01:01:58.180]   in these different ancestry groups.
[01:01:58.180 --> 01:01:59.820]   We don't know that's a hypothesis,
[01:01:59.820 --> 01:02:03.700]   but even so the tagging quality falls off.
[01:02:03.700 --> 01:02:07.740]   So my group, you know, we've spent a lot of our time
[01:02:07.740 --> 01:02:11.300]   looking at performance of predictor trained in population A
[01:02:11.300 --> 01:02:12.780]   on distant population B,
[01:02:12.780 --> 01:02:14.740]   and doing all this stuff,
[01:02:14.740 --> 01:02:15.940]   modeling it, trying to figure out,
[01:02:15.940 --> 01:02:17.220]   trying to test hypotheses
[01:02:17.220 --> 01:02:19.780]   as to whether it's just the tagging decay,
[01:02:19.780 --> 01:02:21.620]   which is responsible for most of the fall off.
[01:02:21.620 --> 01:02:24.420]   So all of this is an area of very active investigation.
[01:02:24.420 --> 01:02:29.260]   I think, you know, it'll probably be solved in five years.
[01:02:29.260 --> 01:02:32.180]   The first really big biobanks that are non-European
[01:02:32.180 --> 01:02:33.620]   are coming online.
[01:02:33.620 --> 01:02:35.660]   And so we're, I think we're going to solve it,
[01:02:35.660 --> 01:02:37.580]   you know, in some number of years.
[01:02:37.580 --> 01:02:39.980]   - Oh, what does the solution look like, I guess?
[01:02:39.980 --> 01:02:41.980]   'Cause if you don't know,
[01:02:41.980 --> 01:02:43.980]   unless you can identify like the causal mechanism
[01:02:43.980 --> 01:02:46.060]   by which each SNP is having an effect,
[01:02:46.060 --> 01:02:47.740]   how can you know that something is a tag
[01:02:47.740 --> 01:02:51.700]   or whether it's the actual underlying, you know, switch?
[01:02:51.700 --> 01:02:56.100]   - The resolution will be, again,
[01:02:56.100 --> 01:03:00.260]   the nature of reality determines how this is going to go.
[01:03:00.260 --> 01:03:03.740]   So, and we don't know the underlying biology.
[01:03:03.740 --> 01:03:06.700]   If it's true, and this is the amazing thing,
[01:03:06.700 --> 01:03:11.020]   like people argue about like human biodiversity
[01:03:11.020 --> 01:03:13.220]   and all this stuff, and we don't even know
[01:03:13.220 --> 01:03:18.220]   whether the specific mechanisms that say predispose you
[01:03:18.220 --> 01:03:21.340]   to being tall or to having heart disease
[01:03:21.340 --> 01:03:24.100]   are the same in these different ancestry groups.
[01:03:24.100 --> 01:03:26.500]   We assume that it is, but we don't know that.
[01:03:26.500 --> 01:03:28.180]   And, you know, like as we get further away,
[01:03:28.180 --> 01:03:29.940]   like to Neanderthals or Homo erectus,
[01:03:29.940 --> 01:03:30.820]   you might be like, yeah,
[01:03:30.820 --> 01:03:33.500]   they have a slightly different architecture there than we do.
[01:03:33.500 --> 01:03:38.500]   But let's assume that the causal structure is the same
[01:03:38.500 --> 01:03:42.420]   for South Asians and for British people, okay?
[01:03:42.420 --> 01:03:45.420]   And then it's a matter of improving the tags.
[01:03:45.420 --> 01:03:47.380]   And you might say, wait, wait, wait a minute, Steve.
[01:03:47.380 --> 01:03:48.300]   How do I know?
[01:03:48.300 --> 01:03:51.340]   How do I know if I don't know which one is causal?
[01:03:51.340 --> 01:03:54.220]   How, what do you mean by improving the tags?
[01:03:54.220 --> 01:03:55.900]   This is a machine learning problem.
[01:03:55.900 --> 01:03:57.900]   So the question is, if there's a SNP,
[01:03:57.900 --> 01:04:00.220]   which when I use it across multiple ancestry groups
[01:04:00.220 --> 01:04:03.300]   is always coming up as very significant,
[01:04:03.300 --> 01:04:05.660]   maybe that one's truly causal.
[01:04:05.660 --> 01:04:08.420]   As I vary the tagging correlations
[01:04:08.420 --> 01:04:10.380]   in the neighborhood of that SNP,
[01:04:10.380 --> 01:04:14.380]   I always find that that one is in the intersection,
[01:04:14.380 --> 01:04:16.780]   the intersection of all these different sets.
[01:04:16.780 --> 01:04:20.180]   That makes me think that one's gonna actually be causal.
[01:04:20.180 --> 01:04:22.940]   So that's a process we're engaged in now
[01:04:22.940 --> 01:04:26.140]   is to try to basically do that.
[01:04:26.140 --> 01:04:28.420]   It's basically just a machine learning problem,
[01:04:28.420 --> 01:04:29.500]   but we need data.
[01:04:29.500 --> 01:04:31.220]   That's the main issue.
[01:04:31.220 --> 01:04:33.420]   - Yeah, I was kinda hoping that wouldn't be possible
[01:04:33.420 --> 01:04:36.140]   because one worry you might have about this research
[01:04:36.140 --> 01:04:39.940]   is that it'll in itself become taboo
[01:04:39.940 --> 01:04:42.260]   or cause other sorts of bad social consequences
[01:04:42.260 --> 01:04:45.740]   if you can definitively show that on certain traits,
[01:04:45.740 --> 01:04:48.980]   there's differences between ancestral populations, right?
[01:04:48.980 --> 01:04:50.300]   And I was kinda hoping that maybe
[01:04:50.300 --> 01:04:52.260]   there's just a evasion button where like,
[01:04:52.260 --> 01:04:54.380]   yeah, we can't say because they're just tags
[01:04:54.380 --> 01:04:55.260]   and the tags might be different
[01:04:55.260 --> 01:04:56.700]   between different ancestral populations,
[01:04:56.700 --> 01:04:58.540]   but I guess with better machine learning, we'll know.
[01:04:58.540 --> 01:05:00.060]   - That's a situation we're in now
[01:05:00.060 --> 01:05:02.100]   where you have to do some fancy analysis.
[01:05:02.100 --> 01:05:03.980]   If you wanna claim like,
[01:05:03.980 --> 01:05:08.980]   Italians literally have lower height potential than Nordics,
[01:05:08.980 --> 01:05:11.660]   which is possible.
[01:05:11.660 --> 01:05:13.660]   And there's been a ton of research about this
[01:05:13.660 --> 01:05:15.780]   because there's signals of selection.
[01:05:15.780 --> 01:05:18.140]   It looks like the alleles,
[01:05:18.140 --> 01:05:22.180]   which are activated in height predictors,
[01:05:22.180 --> 01:05:25.380]   it looks like they've been under some selection
[01:05:25.380 --> 01:05:29.660]   between North and South Europe over the last 5,000 years
[01:05:29.660 --> 01:05:31.020]   for whatever reason, we don't know the reason,
[01:05:31.020 --> 01:05:32.540]   but there are some.
[01:05:32.540 --> 01:05:34.340]   But this is a thing which is debated
[01:05:34.340 --> 01:05:36.900]   by people who study molecular evolution.
[01:05:36.900 --> 01:05:40.060]   But suppose it's true, okay?
[01:05:40.060 --> 01:05:41.220]   And then what that would mean
[01:05:41.220 --> 01:05:42.980]   is that when we finally get to the bottom of it
[01:05:42.980 --> 01:05:47.340]   and we find all the causal loci for height,
[01:05:47.340 --> 01:05:52.260]   literally the average value for the Italians is lower
[01:05:52.260 --> 01:05:55.500]   than the average value for the people living in Stockholm.
[01:05:55.500 --> 01:05:56.700]   And that might be true.
[01:05:56.700 --> 01:05:59.540]   People don't get that excited.
[01:05:59.540 --> 01:06:01.780]   They get a little bit excited about height,
[01:06:01.780 --> 01:06:03.180]   but they would get really excited
[01:06:03.180 --> 01:06:04.700]   if this were true for some other traits, right?
[01:06:04.700 --> 01:06:08.100]   Suppose like your extraversion,
[01:06:08.100 --> 01:06:12.300]   the causal variance affecting your level of extraversion
[01:06:12.300 --> 01:06:15.580]   is systematic, that the average value of those,
[01:06:15.580 --> 01:06:19.500]   the weighted average of those states is different
[01:06:19.500 --> 01:06:24.500]   in Japan versus Sicily, right?
[01:06:24.500 --> 01:06:27.500]   People might freak out over that.
[01:06:27.500 --> 01:06:28.340]   - Right.
[01:06:28.340 --> 01:06:31.780]   - I'm supposed to just say that's obviously not true.
[01:06:31.780 --> 01:06:32.780]   It's obviously not true.
[01:06:32.780 --> 01:06:33.620]   It can't be true.
[01:06:33.620 --> 01:06:34.780]   How could it possibly be true?
[01:06:34.780 --> 01:06:37.980]   Because there hasn't been enough evolutionary time
[01:06:37.980 --> 01:06:39.220]   for those differences to arise.
[01:06:39.220 --> 01:06:40.900]   After all, it's not possible
[01:06:40.900 --> 01:06:44.540]   that despite what looks to be the case for height
[01:06:44.540 --> 01:06:46.580]   over the last 5,000 years in Europe,
[01:06:46.580 --> 01:06:48.740]   no other traits could possibly
[01:06:48.740 --> 01:06:50.020]   have been differentially selected
[01:06:50.020 --> 01:06:52.020]   for over the last 5,000 years.
[01:06:52.020 --> 01:06:53.220]   That's the really dangerous thing.
[01:06:53.220 --> 01:06:57.020]   So there are few people who understand this field
[01:06:57.020 --> 01:07:02.020]   well enough to understand what you and I just discussed
[01:07:02.020 --> 01:07:04.460]   and who are so alarmed by it
[01:07:04.460 --> 01:07:06.020]   that they're just trying to suppress everything.
[01:07:06.020 --> 01:07:07.860]   There are people like that.
[01:07:07.860 --> 01:07:09.860]   But most of them actually don't really follow it
[01:07:09.860 --> 01:07:13.180]   at the technical level that you and I are discussing it.
[01:07:13.180 --> 01:07:16.460]   So they're just instinctively negative about it,
[01:07:16.460 --> 01:07:18.860]   but they don't really understand it very well.
[01:07:18.860 --> 01:07:21.140]   - That's good to hear, 'cause that's...
[01:07:21.140 --> 01:07:23.020]   Yeah, in a lot of other spaces you see this pattern
[01:07:23.020 --> 01:07:25.780]   that by the time that somebody might wanna regulate
[01:07:25.780 --> 01:07:30.780]   or in some way interfere with some technology
[01:07:30.780 --> 01:07:34.740]   or some information, it already has achieved wide adoption.
[01:07:34.740 --> 01:07:36.780]   You could argue that that's the case with crypto today.
[01:07:36.780 --> 01:07:39.340]   But if it's true that a bunch of IVF clinics
[01:07:39.340 --> 01:07:42.820]   across the world are using these scores
[01:07:42.820 --> 01:07:45.900]   to do selection and other things.
[01:07:45.900 --> 01:07:47.820]   Yeah, by the time that people realize the implications
[01:07:47.820 --> 01:07:51.580]   of this data for other kinds of social questions,
[01:07:51.580 --> 01:07:54.340]   by that time, this would already be
[01:07:54.340 --> 01:07:57.420]   like an actual consumer technology, hopefully.
[01:07:57.420 --> 01:07:59.260]   - I think that's true.
[01:07:59.260 --> 01:08:03.580]   I think the main outcry will be if it turns out
[01:08:03.580 --> 01:08:05.620]   that there are really big gains to be had
[01:08:05.620 --> 01:08:09.300]   and only the billionaires are getting them.
[01:08:09.300 --> 01:08:12.660]   But that might have the consequence of causing countries
[01:08:12.660 --> 01:08:17.660]   to make this free part of their national healthcare system.
[01:08:17.660 --> 01:08:23.100]   So Denmark, Israel, they pay for IVF for infertile couples.
[01:08:23.100 --> 01:08:28.660]   So it's part of their national healthcare system.
[01:08:28.660 --> 01:08:31.220]   And they're pretty aggressive about genetic testing.
[01:08:31.220 --> 01:08:35.060]   In Denmark, one in 10 babies born now is born through IVF.
[01:08:35.060 --> 01:08:35.900]   - Right.
[01:08:35.900 --> 01:08:40.100]   - So yeah, so it's not clear how it's gonna go,
[01:08:40.100 --> 01:08:43.980]   but yeah, I mean, we're in for some fun times.
[01:08:43.980 --> 01:08:45.220]   There's no doubt about it.
[01:08:45.220 --> 01:08:47.020]   - Yeah, well, I guess one way it could go
[01:08:47.020 --> 01:08:49.260]   is some countries decided to ban it altogether.
[01:08:49.260 --> 01:08:51.300]   And another way it could go is countries decide
[01:08:51.300 --> 01:08:53.180]   to give everybody free access to it.
[01:08:53.180 --> 01:08:54.020]   - Naturalized.
[01:08:54.020 --> 01:08:54.860]   - Yeah, exactly.
[01:08:54.860 --> 01:08:56.020]   If you had to choose between the two,
[01:08:56.020 --> 01:08:58.540]   I guess you would wanna go for the second one,
[01:08:58.540 --> 01:09:00.020]   which I guess would be the hope.
[01:09:00.020 --> 01:09:03.660]   And maybe only those two are compatible with people's,
[01:09:03.660 --> 01:09:06.620]   I don't know, their moral intuitions
[01:09:06.620 --> 01:09:08.220]   about this kind of stuff.
[01:09:08.220 --> 01:09:12.220]   - It's very funny because most wokest people today
[01:09:12.220 --> 01:09:14.020]   hate this stuff.
[01:09:14.020 --> 01:09:18.180]   But most progressives, like Margaret Sanger or, you know,
[01:09:18.180 --> 01:09:21.140]   anybody who was progressive, the intellectual,
[01:09:21.140 --> 01:09:24.100]   well, in some sense, the forebears of today's wokest,
[01:09:24.100 --> 01:09:25.300]   in the early 20th century,
[01:09:25.300 --> 01:09:27.980]   they were all what we would call today eugenicists
[01:09:27.980 --> 01:09:30.780]   because they were like, oh, shoot, thanks to Darwin,
[01:09:30.780 --> 01:09:32.220]   we now know how this all works
[01:09:32.220 --> 01:09:35.060]   and we should take steps to keep society healthy
[01:09:35.060 --> 01:09:37.460]   and not in a negative way
[01:09:37.460 --> 01:09:38.780]   where we kill people we don't like,
[01:09:38.780 --> 01:09:41.500]   but we should just help society do healthy things
[01:09:41.500 --> 01:09:44.220]   and when they reproduce and, you know, have healthy kids.
[01:09:44.220 --> 01:09:47.100]   And so now this whole thing has just been flipped over
[01:09:47.100 --> 01:09:48.980]   among progressives, so.
[01:09:48.980 --> 01:09:49.820]   - Yeah.
[01:09:49.820 --> 01:09:50.780]   - It's sad.
[01:09:50.780 --> 01:09:54.220]   - Yeah, even in India, like that was like very recently,
[01:09:54.220 --> 01:09:55.820]   less than 50 years ago or whatever.
[01:09:55.820 --> 01:09:58.180]   Indira Gandhi, you know, she's like the left side
[01:09:58.180 --> 01:10:00.100]   of India's political spectrum.
[01:10:00.100 --> 01:10:01.860]   And yeah, she obviously, she was infamous
[01:10:01.860 --> 01:10:04.500]   for putting on these like forced sterilization programs
[01:10:04.500 --> 01:10:08.780]   and yeah, so, you know, I don't wanna credit the person,
[01:10:08.780 --> 01:10:11.140]   but somebody made an interesting comment.
[01:10:11.140 --> 01:10:13.260]   They wouldn't want their name associated with this maybe,
[01:10:13.260 --> 01:10:15.180]   but somebody made an interesting comment about this
[01:10:15.180 --> 01:10:17.500]   where they said, they were asked like,
[01:10:17.500 --> 01:10:20.940]   oh, is it true that progressives in history,
[01:10:20.940 --> 01:10:22.700]   the history always tilts towards progressives
[01:10:22.700 --> 01:10:25.700]   and if so, isn't everybody else doomed?
[01:10:25.700 --> 01:10:27.060]   Aren't their views doomed?
[01:10:27.060 --> 01:10:28.660]   And the person made a really interesting point,
[01:10:28.660 --> 01:10:31.260]   which is that, yes, whatever we consider left
[01:10:31.260 --> 01:10:32.900]   at the time tends to be winning,
[01:10:32.900 --> 01:10:35.860]   but what is left changes a lot over time, right?
[01:10:35.860 --> 01:10:38.180]   So in the early 20th century,
[01:10:38.180 --> 01:10:40.940]   prohibition was a left cause, right?
[01:10:40.940 --> 01:10:42.100]   It was a progressive cause.
[01:10:42.100 --> 01:10:43.780]   And you know, that changed and now that's no longer,
[01:10:43.780 --> 01:10:45.060]   I mean, the opposite is a left cause.
[01:10:45.060 --> 01:10:47.900]   - Yeah, now legalizing pot is progressive.
[01:10:47.900 --> 01:10:50.820]   - Exactly, so the way, if the, you know,
[01:10:50.820 --> 01:10:52.820]   if conquest second law is true and everything just tilts
[01:10:52.820 --> 01:10:55.140]   left over time, just change what left is, right?
[01:10:55.140 --> 01:10:55.980]   That's a solution.
[01:10:55.980 --> 01:10:57.700]   - Yeah, absolutely.
[01:10:57.700 --> 01:11:01.220]   I mean, the, I, of course one can't demand
[01:11:01.220 --> 01:11:03.340]   that any of these woke guys
[01:11:03.340 --> 01:11:05.940]   be like intellectually self-consistent
[01:11:05.940 --> 01:11:09.740]   or even like say the same things from one year to another,
[01:11:09.740 --> 01:11:11.580]   but if one could, you know,
[01:11:11.580 --> 01:11:12.700]   you wonder what they think about
[01:11:12.700 --> 01:11:16.020]   these literally communist Chinese.
[01:11:16.020 --> 01:11:17.580]   I mean, these are literally communists.
[01:11:17.580 --> 01:11:20.660]   They're recycling huge parts of their GDP to help the poor
[01:11:20.660 --> 01:11:23.460]   and do all this other stuff that, you know,
[01:11:23.460 --> 01:11:25.100]   medicine is free, everything, you know,
[01:11:25.100 --> 01:11:26.220]   education is free, right?
[01:11:26.220 --> 01:11:27.540]   They're literally socialists.
[01:11:27.540 --> 01:11:29.940]   They're literally communists.
[01:11:29.940 --> 01:11:33.820]   But in Chinese, the Chinese characters for eugenics
[01:11:33.820 --> 01:11:35.300]   is a totally positive thing.
[01:11:35.300 --> 01:11:37.380]   It's just like healthy production.
[01:11:37.380 --> 01:11:38.340]   It means healthy.
[01:11:38.340 --> 01:11:39.860]   Well, that's actually what it means in Greek too,
[01:11:39.860 --> 01:11:43.940]   but more or less, but the whole viewpoint on all this stuff
[01:11:43.940 --> 01:11:48.420]   is like 180 degrees off in East Asia compared to here.
[01:11:48.420 --> 01:11:52.380]   And even in the, among the literal communists, you know,
[01:11:52.380 --> 01:11:54.140]   so go figure.
[01:11:54.140 --> 01:11:55.500]   - Yeah, very based.
[01:11:56.540 --> 01:11:58.180]   So let's talk about one of the traits
[01:11:58.180 --> 01:12:00.220]   that people might be interested
[01:12:00.220 --> 01:12:02.940]   in potentially selecting for, which is intelligence.
[01:12:02.940 --> 01:12:06.820]   Do, or do we, what is the potential
[01:12:06.820 --> 01:12:08.820]   that we'll be able to actually acquire the data
[01:12:08.820 --> 01:12:13.260]   to be able to correlate the genotype with intelligence?
[01:12:13.260 --> 01:12:17.340]   - Well, that's the most personally frustrating aspect
[01:12:17.340 --> 01:12:18.420]   of all of this stuff.
[01:12:18.420 --> 01:12:21.380]   Like if you ask me like 10 years ago
[01:12:21.380 --> 01:12:22.620]   when I started doing this stuff,
[01:12:22.620 --> 01:12:24.940]   what did I think we were gonna get?
[01:12:24.940 --> 01:12:29.940]   I think everything is gone kind of on the optimistic side
[01:12:29.940 --> 01:12:31.060]   of what I would have predicted.
[01:12:31.060 --> 01:12:33.060]   So everything's good, you know,
[01:12:33.060 --> 01:12:35.380]   didn't turn out to be interactively non-linear.
[01:12:35.380 --> 01:12:38.580]   It didn't turn out to be interactively polytropic.
[01:12:38.580 --> 01:12:39.660]   You know, all these good things,
[01:12:39.660 --> 01:12:42.300]   which nobody could have known a priori how they would work,
[01:12:42.300 --> 01:12:44.380]   turned out to be good for gene engineers
[01:12:44.380 --> 01:12:45.780]   of the 21st century.
[01:12:45.780 --> 01:12:51.340]   The one thing that's frustrating is because of crazy wokeism
[01:12:51.340 --> 01:12:54.220]   and fear of crazy wokists,
[01:12:54.220 --> 01:12:56.660]   the most interesting, what I consider
[01:12:56.660 --> 01:13:00.500]   the most interesting phenotype of all is lagging
[01:13:00.500 --> 01:13:02.500]   because everybody's afraid.
[01:13:02.500 --> 01:13:04.380]   Even though there are very good reasons
[01:13:04.380 --> 01:13:07.260]   for medical researchers to want to know
[01:13:07.260 --> 01:13:11.940]   the cognitive ability of people in their studies.
[01:13:11.940 --> 01:13:13.700]   For example, when you wanna study aging
[01:13:13.700 --> 01:13:18.140]   or decline of cognitive function, memory in older people,
[01:13:18.140 --> 01:13:20.260]   you wanna have baseline measurements
[01:13:20.260 --> 01:13:21.860]   of how good their cognitive function was
[01:13:21.860 --> 01:13:23.180]   when they were younger, right?
[01:13:23.180 --> 01:13:26.740]   So very good reasons for why you wanna have all this data.
[01:13:26.740 --> 01:13:30.060]   But researchers are afraid because it's also linked
[01:13:30.060 --> 01:13:32.900]   to all these controversial social issues.
[01:13:32.900 --> 01:13:37.220]   And so the amount, there's just a ginormous amount
[01:13:37.220 --> 01:13:40.140]   of genomic data where there's actually
[01:13:40.140 --> 01:13:45.140]   no cognitive measurement attached as a field to that data,
[01:13:45.140 --> 01:13:47.420]   which would have been very cheap to measure.
[01:13:47.420 --> 01:13:50.300]   Again, wokists hate this, but I can measure your IQ
[01:13:50.300 --> 01:13:53.380]   on like a 12 minute test, no problem, right?
[01:13:53.380 --> 01:13:54.900]   I mean, not with perfect accuracy,
[01:13:54.900 --> 01:13:58.340]   but I can get a pretty, I can get a very useful measurement.
[01:13:58.340 --> 01:14:00.060]   If I just take, like the NFL has this thing
[01:14:00.060 --> 01:14:03.020]   called the Wunderlich, which every player
[01:14:03.020 --> 01:14:04.420]   that's being considered for the draft
[01:14:04.420 --> 01:14:05.620]   is asked to take this Wunderlich.
[01:14:05.620 --> 01:14:07.220]   You can go back and look at the Wunderlich scores
[01:14:07.220 --> 01:14:08.900]   of every NFL player.
[01:14:08.900 --> 01:14:11.900]   It's a short test, it's like 12 minutes long or something.
[01:14:11.900 --> 01:14:13.740]   And it's pretty highly correlated.
[01:14:13.740 --> 01:14:16.020]   It's like probably correlates 0.8 or 0.9,
[01:14:16.020 --> 01:14:20.980]   0.8 maybe with a more fulsome IQ measurement.
[01:14:20.980 --> 01:14:25.660]   So it would be trivial and inexpensive to gather this data.
[01:14:25.660 --> 01:14:28.660]   And then once we have my prediction
[01:14:28.660 --> 01:14:30.860]   from this earlier math that I was talking about
[01:14:30.860 --> 01:14:32.500]   is that when you get to a border of a million,
[01:14:32.500 --> 01:14:35.100]   it could be 1 million, it could be 2 million,
[01:14:35.100 --> 01:14:39.700]   well phenotyped people and genomes,
[01:14:39.700 --> 01:14:42.340]   we would be able to build a pretty decent IQ predictor
[01:14:42.340 --> 01:14:43.660]   that might have a standard error
[01:14:43.660 --> 01:14:45.260]   of maybe 10 points or something.
[01:14:46.020 --> 01:14:50.100]   So that would be incredibly, for science,
[01:14:50.100 --> 01:14:53.420]   just unlimited interesting stuff in there,
[01:14:53.420 --> 01:14:56.100]   but not getting done.
[01:14:56.100 --> 01:14:59.300]   - Yeah, and if there are differences between,
[01:14:59.300 --> 01:15:02.060]   I mean, differences in how things are tagged
[01:15:02.060 --> 01:15:04.100]   between different ancestral groups,
[01:15:04.100 --> 01:15:05.660]   not talking about like average differences or anything,
[01:15:05.660 --> 01:15:07.820]   just how the genotype is tagged.
[01:15:07.820 --> 01:15:09.900]   And if the Chinese do this first,
[01:15:09.900 --> 01:15:12.260]   then that's like, they have an advantage
[01:15:12.260 --> 01:15:15.220]   that can't be transferred over, I guess, right?
[01:15:15.220 --> 01:15:16.660]   Because it's only applicable
[01:15:16.660 --> 01:15:21.660]   or advantageously applicable to their population.
[01:15:21.660 --> 01:15:24.300]   - No, that's a great point.
[01:15:24.300 --> 01:15:26.500]   You can easily imagine, even in a small country
[01:15:26.500 --> 01:15:30.860]   like Singapore or Taiwan has enough data to do this,
[01:15:30.860 --> 01:15:33.180]   no problem, Estonia.
[01:15:33.180 --> 01:15:36.300]   And they could do it and have this thing working
[01:15:36.300 --> 01:15:39.220]   and just not share it with anybody.
[01:15:39.220 --> 01:15:41.180]   So it's certainly possible.
[01:15:41.180 --> 01:15:44.380]   Now that's a little bit too science fictiony
[01:15:44.380 --> 01:15:46.100]   because the leaders who run these countries
[01:15:46.100 --> 01:15:49.820]   are not transhumanist, rationalist people
[01:15:49.820 --> 01:15:52.620]   who read my blog posts on the internet.
[01:15:52.620 --> 01:15:53.460]   They are not.
[01:15:53.460 --> 01:15:54.300]   - They're not dominant comings.
[01:15:54.300 --> 01:15:56.860]   - So I don't think anything that exciting is gonna happen,
[01:15:56.860 --> 01:15:58.540]   but maybe it will.
[01:15:58.540 --> 01:16:00.340]   - Yeah, and do you think the potential
[01:16:00.340 --> 01:16:04.140]   for a pleiotropy is higher with intelligence?
[01:16:04.140 --> 01:16:06.780]   I mean, with certain populations,
[01:16:06.780 --> 01:16:08.140]   oh, of course, by the way, disclaimer,
[01:16:08.140 --> 01:16:10.180]   5,000 years is not enough, blah, blah, blah.
[01:16:10.180 --> 01:16:11.780]   But given that-
[01:16:11.780 --> 01:16:12.820]   - Obviously. - Obviously.
[01:16:12.820 --> 01:16:13.980]   - Obviously.
[01:16:13.980 --> 01:16:16.340]   But given that you see with certain populations
[01:16:16.340 --> 01:16:19.700]   like Ashkenazi Jews, you have a higher incidence of,
[01:16:19.700 --> 01:16:22.780]   is it nervous system disorders?
[01:16:22.780 --> 01:16:25.060]   You know, like Tay-Sachs and other things.
[01:16:25.060 --> 01:16:29.140]   And that seems potentially to be the trade-off
[01:16:29.140 --> 01:16:32.380]   of the higher average intelligence.
[01:16:32.380 --> 01:16:33.860]   You think that maybe the pleiotropy
[01:16:33.860 --> 01:16:36.940]   has a higher chance of occurring with intelligence?
[01:16:36.940 --> 01:16:41.740]   - It can only be speculation, okay, at this stage.
[01:16:41.740 --> 01:16:45.380]   Now, with the history of the Ashkenazi Jews,
[01:16:45.380 --> 01:16:46.220]   they also went through
[01:16:46.220 --> 01:16:48.860]   some very narrow population bottlenecks.
[01:16:48.860 --> 01:16:51.940]   So there's some special aspects of their genetics.
[01:16:51.940 --> 01:16:54.900]   And whether it's related to cognitive function or not,
[01:16:54.900 --> 01:16:56.380]   you know, we don't really know for sure,
[01:16:56.380 --> 01:16:59.500]   but there are lots of reasons why they have
[01:16:59.500 --> 01:17:01.780]   fairly high proportion of inherited diseases
[01:17:01.780 --> 01:17:03.100]   and things like that that they're dealing with.
[01:17:03.100 --> 01:17:06.100]   This is one of the reasons why Israel is so progressive
[01:17:06.100 --> 01:17:07.540]   when it comes to genetic screening
[01:17:07.540 --> 01:17:09.100]   and IVF and things like this.
[01:17:09.780 --> 01:17:11.940]   One thing people talk a lot about is schizophrenia.
[01:17:11.940 --> 01:17:12.780]   So they say like,
[01:17:12.780 --> 01:17:15.220]   "Oh, schizophrenia could be correlated with creativity.
[01:17:15.220 --> 01:17:16.700]   "So if your brother's schizophrenic,
[01:17:16.700 --> 01:17:18.460]   "maybe you're more likely to be creative."
[01:17:18.460 --> 01:17:19.620]   And he's super creative,
[01:17:19.620 --> 01:17:21.300]   but we don't know what he's talking about.
[01:17:21.300 --> 01:17:23.540]   (laughing)
[01:17:23.540 --> 01:17:26.380]   So people say like,
[01:17:26.380 --> 01:17:28.220]   "Oh, if you start screening against schizophrenia,
[01:17:28.220 --> 01:17:30.340]   "maybe we won't get creative geniuses."
[01:17:30.340 --> 01:17:33.300]   So there's all kinds of pleiotropic things
[01:17:33.300 --> 01:17:34.620]   that are possibly true.
[01:17:34.620 --> 01:17:36.060]   But the thing I keep wanting going,
[01:17:36.060 --> 01:17:37.100]   I want to go back to this,
[01:17:37.100 --> 01:17:42.100]   is that if it's 10 or 20,000 different genetic variants,
[01:17:42.100 --> 01:17:44.380]   locations in your genome
[01:17:44.380 --> 01:17:45.620]   that are more or less determining
[01:17:45.620 --> 01:17:48.260]   your genetic cognitive potential,
[01:17:48.260 --> 01:17:51.020]   I can go around.
[01:17:51.020 --> 01:17:52.340]   - Ah, yes, yes, yes.
[01:17:52.340 --> 01:17:54.140]   - It's a high dimensional space.
[01:17:54.140 --> 01:17:55.820]   If I find out this little cluster,
[01:17:55.820 --> 01:17:57.740]   okay, you can make someone smart in this little,
[01:17:57.740 --> 01:17:59.180]   using this stuff in this cluster,
[01:17:59.180 --> 01:18:02.540]   but it makes them dull or it makes them autistic
[01:18:02.540 --> 01:18:04.940]   or it makes them, they don't have big muscles.
[01:18:04.940 --> 01:18:06.860]   Like, okay, I'll just go around.
[01:18:06.860 --> 01:18:08.060]   I don't need to use those.
[01:18:08.060 --> 01:18:09.060]   I have plenty more.
[01:18:09.060 --> 01:18:09.900]   Look over here.
[01:18:09.900 --> 01:18:11.580]   Those 500, I don't need to use.
[01:18:11.580 --> 01:18:13.860]   I will use these 500.
[01:18:13.860 --> 01:18:16.900]   And this is why it's important to look at
[01:18:16.900 --> 01:18:19.580]   historical geniuses
[01:18:19.580 --> 01:18:23.620]   who were pretty normal.
[01:18:23.620 --> 01:18:25.460]   And maybe they were even good athletes
[01:18:25.460 --> 01:18:28.100]   and maybe they even were good with the ladies.
[01:18:28.100 --> 01:18:30.380]   Okay, these people existed.
[01:18:30.380 --> 01:18:32.940]   So you have these existence proofs
[01:18:32.940 --> 01:18:34.780]   that I can, if I need to,
[01:18:34.780 --> 01:18:36.660]   if I'm a really good genetic engineer
[01:18:36.660 --> 01:18:39.620]   and I can operate in this 10,000 dimensional space,
[01:18:39.620 --> 01:18:41.140]   whatever obstacle you put for me,
[01:18:41.140 --> 01:18:43.420]   I will just drive around it.
[01:18:43.420 --> 01:18:45.180]   And I just need some good data.
[01:18:45.180 --> 01:18:46.020]   I need lots of data.
[01:18:46.020 --> 01:18:49.780]   I need lots of AI, ML, and I'll do it.
[01:18:49.780 --> 01:18:50.900]   And that's the answer,
[01:18:50.900 --> 01:18:52.700]   which again, most people don't really get this,
[01:18:52.700 --> 01:18:53.700]   but it's true.
[01:18:53.700 --> 01:18:54.540]   - Right, yeah.
[01:18:54.540 --> 01:18:56.020]   So, I mean, there's a thing where
[01:18:56.020 --> 01:18:58.740]   if two traits are correlated at the ends,
[01:18:58.740 --> 01:19:01.380]   the person who is like, for example, the smartest
[01:19:01.380 --> 01:19:05.420]   will not necessarily be the person who is as strong.
[01:19:05.420 --> 01:19:06.740]   I guess these aren't necessarily correlated,
[01:19:06.740 --> 01:19:08.940]   but the person who has the highest mathematical ability
[01:19:08.940 --> 01:19:10.460]   will not be the person who has the highest verbal ability,
[01:19:10.460 --> 01:19:11.860]   even though the two are correlated.
[01:19:11.860 --> 01:19:13.260]   And at some point it'll be interesting
[01:19:13.260 --> 01:19:14.700]   'cause parents will have to make that trade-off
[01:19:14.700 --> 01:19:17.740]   even if two things are extraordinarily correlated.
[01:19:17.740 --> 01:19:19.260]   And it'll be interesting to see how they make the trade-off.
[01:19:19.260 --> 01:19:20.900]   - Eventually, you're really gonna have to trust
[01:19:20.900 --> 01:19:23.820]   your friendly neighborhood genetic engineer to advise you.
[01:19:23.820 --> 01:19:26.660]   It's gonna be like a lot of modeling
[01:19:26.660 --> 01:19:27.700]   going on in the background.
[01:19:27.700 --> 01:19:28.540]   - Right.
[01:19:28.900 --> 01:19:29.740]   Right.
[01:19:29.740 --> 01:19:31.700]   Now, I guess for the time being,
[01:19:31.700 --> 01:19:34.940]   we're stuck with educational attainment as a correlate.
[01:19:34.940 --> 01:19:38.300]   And that concerns me because educational attainment
[01:19:38.300 --> 01:19:40.060]   also probably correlates with other things
[01:19:40.060 --> 01:19:43.220]   that somebody might want or they might not want,
[01:19:43.220 --> 01:19:45.980]   which are conscientiousness and conformity,
[01:19:45.980 --> 01:19:47.820]   which is, you know, if you're Brian Kaplan,
[01:19:47.820 --> 01:19:48.780]   in "The Case Against Education,"
[01:19:48.780 --> 01:19:50.660]   he says that the three things education signals
[01:19:50.660 --> 01:19:52.820]   are conscientiousness, conformity, and intelligence.
[01:19:52.820 --> 01:19:54.100]   You want the intelligence.
[01:19:54.100 --> 01:19:55.340]   Probably most parents actually do want
[01:19:55.340 --> 01:19:57.140]   conscientiousness and conformity,
[01:19:57.140 --> 01:19:58.460]   but some might not, right?
[01:19:58.460 --> 01:20:02.420]   So, yeah, could you, I guess,
[01:20:02.420 --> 01:20:05.100]   hopefully we can get the direct intelligence data itself,
[01:20:05.100 --> 01:20:07.540]   but if we don't, is there some way to segment out
[01:20:07.540 --> 01:20:11.060]   the conformity part of that educational attainment data?
[01:20:11.060 --> 01:20:12.140]   - Well, here's the thing.
[01:20:12.140 --> 01:20:13.980]   Like, in my dream world,
[01:20:13.980 --> 01:20:16.740]   like if I were the CEO of 23andMe or something,
[01:20:16.740 --> 01:20:18.100]   what would I do?
[01:20:18.100 --> 01:20:20.900]   Oh, warning, they're actually secretly doing this,
[01:20:20.900 --> 01:20:23.180]   but you didn't hear that from me.
[01:20:23.180 --> 01:20:26.140]   I would have little surveys on the site that's like,
[01:20:26.140 --> 01:20:28.980]   "Oh, can you do a personality survey?"
[01:20:28.980 --> 01:20:31.740]   And one of the categories will be conscientiousness
[01:20:31.740 --> 01:20:33.820]   and one will be extroversion, right?
[01:20:33.820 --> 01:20:35.260]   And one will be like,
[01:20:35.260 --> 01:20:38.700]   conformity is not a traditional big five thing,
[01:20:38.700 --> 01:20:40.900]   but you could have questions that kind of measure
[01:20:40.900 --> 01:20:42.860]   like how conformist someone is.
[01:20:42.860 --> 01:20:45.220]   And of course, we know how to do a little math,
[01:20:45.220 --> 01:20:49.300]   so we can diagonalize the matrix of correlated measurements
[01:20:49.300 --> 01:20:51.620]   of all these different things.
[01:20:51.620 --> 01:20:56.420]   So I might be able to remove the chunk within EA,
[01:20:56.420 --> 01:20:58.660]   which is due to conformism,
[01:20:58.660 --> 01:21:01.860]   remove the chunk, which is due to conscientiousness
[01:21:01.860 --> 01:21:04.420]   and leave behind the chunk, which, oh, wow,
[01:21:04.420 --> 01:21:05.460]   and that correlates really highly
[01:21:05.460 --> 01:21:07.660]   with my separate IQ predictor,
[01:21:07.660 --> 01:21:09.220]   G predictor that I built separately
[01:21:09.220 --> 01:21:11.060]   using a different method.
[01:21:11.060 --> 01:21:12.900]   All these things are very,
[01:21:12.900 --> 01:21:16.740]   at our level, these are understood,
[01:21:16.740 --> 01:21:18.340]   the solutions to these problems are understood,
[01:21:18.340 --> 01:21:20.580]   it's just a data problem, okay?
[01:21:21.580 --> 01:21:22.700]   I'll tell you an interesting thing.
[01:21:22.700 --> 01:21:25.660]   So we, my group was the first to do,
[01:21:25.660 --> 01:21:29.620]   there are 20,000 sibling pairs in the UK Biobank.
[01:21:29.620 --> 01:21:32.700]   So we were the first to say,
[01:21:32.700 --> 01:21:35.740]   you know, this is like three years ago or more,
[01:21:35.740 --> 01:21:37.380]   you know, some people don't really understand
[01:21:37.380 --> 01:21:39.580]   these polygenic scores and they're very skeptical
[01:21:39.580 --> 01:21:41.900]   and they think, oh, we're capturing,
[01:21:41.900 --> 01:21:45.140]   we're not really capturing the real stuff, et cetera, et cetera.
[01:21:45.140 --> 01:21:45.980]   Well, you know what?
[01:21:45.980 --> 01:21:48.380]   I will just look to see how well we can predict
[01:21:48.380 --> 01:21:50.540]   which of the two brothers who experienced
[01:21:50.540 --> 01:21:53.780]   the same environment is gonna be taller.
[01:21:53.780 --> 01:21:55.580]   How well does my predictor do that?
[01:21:55.580 --> 01:21:59.540]   I'm gonna predict which of these two brothers has diabetes.
[01:21:59.540 --> 01:22:01.220]   Does the diabetes predictor really do that?
[01:22:01.220 --> 01:22:04.020]   And you're modding out all the environmental shit
[01:22:04.020 --> 01:22:06.060]   because they grew up in the same family, right?
[01:22:06.060 --> 01:22:11.060]   So, and we showed that the predictive power fall off
[01:22:11.060 --> 01:22:13.140]   if you're trying to do this trick
[01:22:13.140 --> 01:22:15.140]   with unrelated pairs of people
[01:22:15.140 --> 01:22:18.180]   versus brothers who grew up in the same house or sisters
[01:22:18.180 --> 01:22:21.460]   is minor, it's a small fall off in predictive power.
[01:22:21.460 --> 01:22:26.460]   So basically we are getting the true genetic stuff, okay?
[01:22:26.460 --> 01:22:29.860]   One of the interesting things is when you look at EA,
[01:22:29.860 --> 01:22:33.660]   if you ask, I built an EA predictor,
[01:22:33.660 --> 01:22:36.180]   does it work better or worse when I try to predict
[01:22:36.180 --> 01:22:39.260]   which of the two brothers got more education?
[01:22:39.260 --> 01:22:42.820]   It turns out it works much worse
[01:22:42.820 --> 01:22:45.780]   because part of what that predictor is capturing
[01:22:45.780 --> 01:22:48.180]   is some maybe property of the parents
[01:22:48.180 --> 01:22:50.260]   who beat them and made them go to school,
[01:22:50.260 --> 01:22:53.260]   but both brothers got beaten and had to go to school.
[01:22:53.260 --> 01:22:58.260]   So the reduction in quality of EA prediction for brothers
[01:22:58.260 --> 01:23:03.180]   is quite a bit higher
[01:23:03.180 --> 01:23:05.020]   than if you're just trying to predict G.
[01:23:05.020 --> 01:23:08.260]   So we have predictors we built that just predict G
[01:23:08.260 --> 01:23:11.700]   and those have a much smaller reduction in quality
[01:23:11.700 --> 01:23:15.620]   when you apply them to brothers of sibs
[01:23:15.620 --> 01:23:17.980]   than in unrelated pairs.
[01:23:17.980 --> 01:23:20.020]   And so I went through that a little fast
[01:23:20.020 --> 01:23:21.420]   so people can go look up the paper,
[01:23:21.420 --> 01:23:25.700]   but the point is we can see EA is a very different trait
[01:23:25.700 --> 01:23:28.180]   than G from these kinds of results.
[01:23:28.180 --> 01:23:29.140]   - That's super fascinating.
[01:23:29.140 --> 01:23:31.140]   - And again, like people who criticize us
[01:23:31.140 --> 01:23:33.980]   have no idea how sophisticated the work is.
[01:23:33.980 --> 01:23:35.820]   They just don't, they don't read our papers.
[01:23:35.820 --> 01:23:38.740]   If they try to read our papers, they can't understand them,
[01:23:38.740 --> 01:23:40.820]   but we've done all this stuff.
[01:23:40.820 --> 01:23:45.260]   So it's, now a guy who comes from a physics background
[01:23:45.260 --> 01:23:46.740]   or from an AIML background,
[01:23:46.740 --> 01:23:47.860]   if I just start explaining to them,
[01:23:47.860 --> 01:23:49.140]   they're like, oh yeah, okay, cool.
[01:23:49.140 --> 01:23:49.980]   You guys are doing that.
[01:23:49.980 --> 01:23:51.260]   Yeah, that's how it works out.
[01:23:51.260 --> 01:23:52.220]   You can absorb it.
[01:23:52.220 --> 01:23:54.420]   But a lot of our critics just can't absorb it.
[01:23:54.420 --> 01:23:56.580]   It's literally a G, it's literally a G thing.
[01:23:56.580 --> 01:23:57.420]   They can't absorb it.
[01:23:57.420 --> 01:24:01.220]   So, but they just want to keep criticizing us forever.
[01:24:01.220 --> 01:24:02.820]   So, you know.
[01:24:02.820 --> 01:24:03.660]   - Yeah, yeah.
[01:24:03.660 --> 01:24:05.460]   The funny thing is when I read your papers,
[01:24:05.460 --> 01:24:07.460]   I have a much easier time,
[01:24:07.460 --> 01:24:10.500]   like the pros part and the explanation in the organization
[01:24:10.500 --> 01:24:12.700]   is I don't know if it's your physics background or whatever,
[01:24:12.700 --> 01:24:14.260]   but I noticed with Scott Aronson's papers as well,
[01:24:14.260 --> 01:24:16.180]   it's like, they're written like essays.
[01:24:16.180 --> 01:24:17.980]   They're so easy to,
[01:24:17.980 --> 01:24:19.780]   as long as you understand the underlying ideas,
[01:24:19.780 --> 01:24:20.900]   they're so easy to absorb.
[01:24:20.900 --> 01:24:24.460]   Whereas if I just read like a random thing on bio archive,
[01:24:24.460 --> 01:24:26.660]   it just, I don't even know where to get started with this.
[01:24:26.660 --> 01:24:28.420]   It just ran so turgidly.
[01:24:28.420 --> 01:24:30.700]   - I'm totally with you.
[01:24:30.700 --> 01:24:33.100]   I mean, of course there are multiple reasons for this,
[01:24:33.100 --> 01:24:36.420]   but one is that, yeah, maybe I'm a outsider.
[01:24:36.420 --> 01:24:38.660]   So I'm trying to write it very clearly and conceptually,
[01:24:38.660 --> 01:24:40.860]   maybe like a theoretical physicist would write it.
[01:24:40.860 --> 01:24:43.060]   But also it's like, it's a slightly selected population.
[01:24:43.060 --> 01:24:46.860]   Like Scott has a enormously popular blog
[01:24:46.860 --> 01:24:49.700]   and he writes these huge posts all the time.
[01:24:49.700 --> 01:24:50.580]   And I have a blog too.
[01:24:50.580 --> 01:24:53.180]   So we are a little bit better at expressing ourselves
[01:24:53.180 --> 01:24:55.500]   or clarifying ideas than the average scientist
[01:24:55.500 --> 01:24:56.940]   who's just trying to get the thing out
[01:24:56.940 --> 01:24:58.500]   and get it published in nature.
[01:24:58.500 --> 01:25:00.540]   - Awesome.
[01:25:00.540 --> 01:25:02.820]   Okay, so let's talk a little bit about
[01:25:02.820 --> 01:25:05.180]   what consumers will actually want.
[01:25:05.180 --> 01:25:09.940]   Goren has this really detailed post about embryo selection.
[01:25:09.940 --> 01:25:13.140]   And he writes in it,
[01:25:13.140 --> 01:25:15.980]   "My belief is that the total uptake will be fairly modest
[01:25:15.980 --> 01:25:17.500]   as a fraction of the population."
[01:25:17.500 --> 01:25:19.460]   And he's talking about embryo selection here.
[01:25:19.460 --> 01:25:22.140]   "A large fraction of the population expresses hostility
[01:25:22.140 --> 01:25:25.420]   towards any new fertility related technology whatsoever.
[01:25:25.420 --> 01:25:27.300]   And the people open to the possibility
[01:25:27.300 --> 01:25:28.980]   will be deterred by the necessity
[01:25:28.980 --> 01:25:30.540]   of advanced family planning,
[01:25:30.540 --> 01:25:32.580]   the large financial cost of IVF,
[01:25:32.580 --> 01:25:36.580]   and the fact that the IVF process is lengthy and painful."
[01:25:36.580 --> 01:25:39.140]   So yeah, he seems like very pessimistic
[01:25:39.140 --> 01:25:39.980]   about the possibility
[01:25:39.980 --> 01:25:43.220]   that this is something that millions of people are using.
[01:25:43.220 --> 01:25:44.060]   What do you think?
[01:25:44.060 --> 01:25:46.580]   What's your reaction to his take here?
[01:25:46.580 --> 01:25:51.340]   - There are two perspectives
[01:25:51.340 --> 01:25:53.860]   that you could adopt in looking at this.
[01:25:53.860 --> 01:25:56.940]   One is a perspective of a venture capitalist
[01:25:56.940 --> 01:25:59.820]   where you say, "How big is this market?
[01:25:59.820 --> 01:26:01.820]   What's it worth to dominate this market?
[01:26:01.820 --> 01:26:06.620]   What valuation should I accept from these pirates at GP?"
[01:26:08.860 --> 01:26:10.460]   The other perspective is,
[01:26:10.460 --> 01:26:11.900]   "Hey, I'm really worried that humans
[01:26:11.900 --> 01:26:15.340]   are all gonna engineer themselves to be blonde and 6'4"
[01:26:15.340 --> 01:26:18.140]   and we're gonna be suddenly susceptible
[01:26:18.140 --> 01:26:19.260]   to all kinds of diseases
[01:26:19.260 --> 01:26:22.860]   and one single cold virus will kill all of us."
[01:26:22.860 --> 01:26:25.740]   So there's like two different perspectives
[01:26:25.740 --> 01:26:27.820]   on like what level of penetration
[01:26:27.820 --> 01:26:29.100]   this technology will have, right?
[01:26:29.100 --> 01:26:30.060]   They're two different perspectives.
[01:26:30.060 --> 01:26:33.860]   So from the venture guy's perspective,
[01:26:33.860 --> 01:26:34.780]   I will just say this,
[01:26:34.820 --> 01:26:39.020]   one out of 10 babies in Denmark is born this way.
[01:26:39.020 --> 01:26:41.260]   Would you like to capture a market
[01:26:41.260 --> 01:26:44.860]   that interfaces with one out of 10 families?
[01:26:44.860 --> 01:26:46.620]   And that's gonna grow, of course, right?
[01:26:46.620 --> 01:26:49.260]   One out of 10 families in all developed countries,
[01:26:49.260 --> 01:26:50.540]   maybe including China,
[01:26:50.540 --> 01:26:57.220]   you have the genome of mom and dad and the kid.
[01:26:57.220 --> 01:27:00.220]   And maybe you can sell them some health services later on.
[01:27:00.220 --> 01:27:04.140]   Maybe it's sticky, your relationship with these people.
[01:27:04.140 --> 01:27:07.340]   Okay, so that's for the venture guys, okay?
[01:27:07.340 --> 01:27:09.220]   You know how to get in touch with me.
[01:27:09.220 --> 01:27:14.540]   From the, "Oh, I'm really worried about human evolution."
[01:27:14.540 --> 01:27:17.340]   Or, "When are we gonna get another von Neumann?"
[01:27:17.340 --> 01:27:19.660]   That's a different question.
[01:27:19.660 --> 01:27:24.380]   And it may be that it'll never be more than 10 or 20%
[01:27:24.380 --> 01:27:27.420]   of the population that's using IVF
[01:27:27.420 --> 01:27:30.620]   and then through IVF embryo selection
[01:27:30.620 --> 01:27:33.020]   and maybe potentially editing someday.
[01:27:33.980 --> 01:27:35.740]   So in that sense, why worry?
[01:27:35.740 --> 01:27:37.420]   There's always gonna be this natural reservoir
[01:27:37.420 --> 01:27:39.660]   of the wild type, you know,
[01:27:39.660 --> 01:27:43.900]   that have much more genetic diversity, et cetera, et cetera.
[01:27:43.900 --> 01:27:46.060]   So I think there's a very,
[01:27:46.060 --> 01:27:48.540]   maybe this is like the Goldilocks world.
[01:27:48.540 --> 01:27:52.060]   But imagine the Goldilocks world where, you know,
[01:27:52.060 --> 01:27:53.620]   there's plenty of wild type people.
[01:27:53.620 --> 01:27:54.620]   And then there's plenty of people
[01:27:54.620 --> 01:27:56.220]   using these advanced technologies.
[01:27:56.220 --> 01:27:58.740]   And everybody's happy, including our investors.
[01:28:00.020 --> 01:28:00.860]   - Yeah.
[01:28:00.860 --> 01:28:05.100]   Something tells me that that won't not be satisfying enough
[01:28:05.100 --> 01:28:07.380]   to the people who are concerned about.
[01:28:07.380 --> 01:28:10.020]   I have a sense that this whole argument about like the,
[01:28:10.020 --> 01:28:11.980]   "Oh, we're not gonna have the evolutionary diversity,"
[01:28:11.980 --> 01:28:13.100]   or whatever, that's just a front
[01:28:13.100 --> 01:28:16.060]   for just like a moral reservation about this technology.
[01:28:16.060 --> 01:28:16.900]   - Yeah, exactly.
[01:28:16.900 --> 01:28:18.540]   It's a front for people who just hate it.
[01:28:18.540 --> 01:28:20.660]   But what is Guern saying?
[01:28:20.660 --> 01:28:22.860]   Like, is he saying that like,
[01:28:22.860 --> 01:28:26.100]   "Well, you know, these 10% of babies born in Denmark,
[01:28:26.100 --> 01:28:27.900]   "they're already mostly screened
[01:28:27.900 --> 01:28:30.900]   "for chromosomal abnormalities.
[01:28:30.900 --> 01:28:32.500]   "And if I take that same data
[01:28:32.500 --> 01:28:34.180]   "and I can generate this other report,
[01:28:34.180 --> 01:28:36.180]   "are you really not gonna look at that report?
[01:28:36.180 --> 01:28:38.340]   "Are you gonna say like,
[01:28:38.340 --> 01:28:41.300]   "Well, you know, one of these kids
[01:28:41.300 --> 01:28:44.340]   "is gonna be super high risk for, you know,
[01:28:44.340 --> 01:28:46.660]   "macular degeneration or something, you know, something,
[01:28:46.660 --> 01:28:48.580]   "but I'm not gonna look at which one,
[01:28:48.580 --> 01:28:49.580]   "but I'm already screening them
[01:28:49.580 --> 01:28:51.380]   "for chromosomal abnormalities."
[01:28:51.380 --> 01:28:53.140]   Is that really gonna happen?
[01:28:53.140 --> 01:28:53.980]   I don't think so.
[01:28:53.980 --> 01:28:57.140]   I think that 10% of the population that's using IVF
[01:28:57.140 --> 01:28:59.100]   is gonna look at the report,
[01:28:59.100 --> 01:29:01.940]   which can be generated by the, you know,
[01:29:01.940 --> 01:29:05.580]   at the cost of running some bits through AWS server, right?
[01:29:05.580 --> 01:29:09.260]   So I'm not sure what he means by that.
[01:29:09.260 --> 01:29:13.260]   Like, I mean, Guern, I admire him a lot,
[01:29:13.260 --> 01:29:14.420]   but what does he mean by that?
[01:29:14.420 --> 01:29:16.220]   It's not very many people are gonna adopt it.
[01:29:16.220 --> 01:29:18.660]   Does he mean like the percentage adoption
[01:29:18.660 --> 01:29:22.100]   within IVF families or the fraction of the population
[01:29:22.100 --> 01:29:23.020]   that's already doing IVF?
[01:29:23.020 --> 01:29:25.260]   Because those are already big numbers.
[01:29:25.260 --> 01:29:27.140]   So I don't know what he means, but.
[01:29:27.140 --> 01:29:27.980]   - Yeah, yeah.
[01:29:27.980 --> 01:29:28.820]   You know, it's interesting.
[01:29:28.820 --> 01:29:30.740]   Like, I guess one way to think about generic prediction,
[01:29:30.740 --> 01:29:32.140]   given your earlier statement that, you know,
[01:29:32.140 --> 01:29:33.260]   these Scandinavian countries,
[01:29:33.260 --> 01:29:35.940]   a lot, there's a huge amounts of IVF happening there.
[01:29:35.940 --> 01:29:37.620]   And part of that is because of how old people are
[01:29:37.620 --> 01:29:39.140]   when they're having babies.
[01:29:39.140 --> 01:29:40.980]   A venture capitalist can think of your company
[01:29:40.980 --> 01:29:43.780]   as a way to get exposure to demographic collapse, right?
[01:29:43.780 --> 01:29:46.100]   (laughing)
[01:29:46.100 --> 01:29:48.020]   - Yes, it's been mentioned.
[01:29:48.020 --> 01:29:50.820]   (laughing)
[01:29:50.820 --> 01:29:53.940]   By the way, it's like three to 5% in the US.
[01:29:53.940 --> 01:29:55.700]   So it ain't small.
[01:29:55.700 --> 01:29:58.140]   Like if you go to a kindergarten,
[01:29:58.140 --> 01:30:01.020]   there are some IVF babies running around in the playground.
[01:30:01.020 --> 01:30:04.300]   So it's not small.
[01:30:04.300 --> 01:30:06.540]   So I don't know whether the perspective is,
[01:30:06.540 --> 01:30:08.540]   is this a big enough market for you to make money in it?
[01:30:08.540 --> 01:30:10.540]   Or is this like gonna change the future
[01:30:10.540 --> 01:30:12.380]   of the human species?
[01:30:12.380 --> 01:30:14.140]   You know, you can have different perspectives.
[01:30:14.140 --> 01:30:17.620]   - Yeah, by the way, Guern is such an interesting character.
[01:30:17.620 --> 01:30:18.820]   I've been reading for him for a long time,
[01:30:18.820 --> 01:30:21.780]   but obviously like his persona is very mysterious.
[01:30:21.780 --> 01:30:24.900]   I don't know if you have like something,
[01:30:24.900 --> 01:30:26.820]   obviously nothing that isn't already public,
[01:30:26.820 --> 01:30:28.260]   but like, what is going on here?
[01:30:28.260 --> 01:30:31.140]   Like how did this person get into,
[01:30:31.140 --> 01:30:33.580]   like, it's a really interesting and detailed report
[01:30:33.580 --> 01:30:34.780]   that he published in every selection.
[01:30:34.780 --> 01:30:36.620]   And it's super interesting.
[01:30:36.620 --> 01:30:37.820]   What is going on here?
[01:30:37.820 --> 01:30:41.380]   - Well, Guern is a super smart guy.
[01:30:41.380 --> 01:30:45.820]   And he, you know, I know a lot of scholars
[01:30:45.820 --> 01:30:48.900]   and serious scientists and intellectuals in the academy
[01:30:48.900 --> 01:30:51.580]   and outside.
[01:30:51.580 --> 01:30:54.420]   And I will pay, even though I didn't quite agree
[01:30:54.420 --> 01:30:57.420]   with his take that you just mentioned.
[01:30:57.420 --> 01:30:59.340]   I mean, it might not be technically wrong
[01:30:59.340 --> 01:31:01.100]   'cause he used words there that I'm not sure
[01:31:01.100 --> 01:31:02.140]   what he means by those words.
[01:31:02.140 --> 01:31:04.460]   But, and I'm not sure he would disagree
[01:31:04.460 --> 01:31:07.780]   with the quantitative things I just mentioned to you.
[01:31:07.780 --> 01:31:11.300]   So, but I just wanna say some positive things about Guern
[01:31:11.300 --> 01:31:13.500]   because I like to read his stuff.
[01:31:13.500 --> 01:31:14.980]   And so in the early days,
[01:31:14.980 --> 01:31:16.660]   he was following a lot of this stuff
[01:31:16.660 --> 01:31:19.540]   about genomic prediction and embryo selection.
[01:31:19.540 --> 01:31:21.180]   And, you know, he's written stuff on that.
[01:31:21.180 --> 01:31:25.460]   He's written stuff on GPT-3 and alignment risk.
[01:31:25.460 --> 01:31:27.980]   He's written lots and lots of insightful things.
[01:31:27.980 --> 01:31:31.980]   And I think he's quite impressive,
[01:31:31.980 --> 01:31:34.620]   even if you compare him to like the most, you know,
[01:31:34.620 --> 01:31:38.260]   famous academic scholars, like whether it's a Steve Pinker
[01:31:38.260 --> 01:31:41.180]   or, you know, somebody who just has written a lot of stuff
[01:31:41.180 --> 01:31:44.980]   that people read and has obviously been thinking deeply
[01:31:44.980 --> 01:31:46.620]   about a lot of different things
[01:31:46.620 --> 01:31:48.740]   during the course of a very serious life,
[01:31:48.740 --> 01:31:50.500]   reading and thinking and writing.
[01:31:50.500 --> 01:31:51.780]   I think Guern is super awesome.
[01:31:51.780 --> 01:31:54.340]   I think he's right up there with those guys.
[01:31:54.340 --> 01:31:56.460]   So I think it's awesome that we live in this internet age
[01:31:56.460 --> 01:31:59.740]   that some totally anonymous dude
[01:31:59.740 --> 01:32:02.180]   can produce really good thinking
[01:32:02.180 --> 01:32:03.900]   about a wide variety of things.
[01:32:03.900 --> 01:32:04.740]   And he's not wrong.
[01:32:04.740 --> 01:32:06.460]   Most of the stuff he writes about embryo selection
[01:32:06.460 --> 01:32:07.780]   is pretty much right.
[01:32:07.780 --> 01:32:12.140]   So yeah, I have very high opinion of Guern.
[01:32:12.140 --> 01:32:14.820]   - And yeah, so it's interesting with people like Guern,
[01:32:14.820 --> 01:32:17.180]   you can, it's almost in the model,
[01:32:17.180 --> 01:32:20.820]   you can think of early 20th century or late 19th century,
[01:32:20.820 --> 01:32:24.340]   these gentlemen scholars who would just pontificate
[01:32:24.340 --> 01:32:26.220]   about a lot of different subjects.
[01:32:26.220 --> 01:32:29.660]   I wonder if we're gonna see a return
[01:32:29.660 --> 01:32:31.900]   of this sort of generalist thinker.
[01:32:31.900 --> 01:32:34.460]   And maybe we've over-indexed on specialists,
[01:32:34.460 --> 01:32:36.220]   but now it's like, now it's the time for,
[01:32:36.220 --> 01:32:37.060]   like somebody like you, right?
[01:32:37.060 --> 01:32:38.060]   It's like theoretical physics,
[01:32:38.060 --> 01:32:39.820]   bringing all of that computational
[01:32:39.820 --> 01:32:43.020]   and mathematical knowledge to genomics.
[01:32:43.020 --> 01:32:45.140]   Is that the new trend in science,
[01:32:45.140 --> 01:32:46.580]   at least at the upper levels?
[01:32:46.580 --> 01:32:49.380]   - I don't think it's a trend.
[01:32:49.380 --> 01:32:54.380]   So in terms of Guern having a platform,
[01:32:54.380 --> 01:32:57.180]   so first of all, he's there, he's thinking really,
[01:32:57.180 --> 01:32:59.260]   you can tell, he's thinking, he's reading a lot,
[01:32:59.260 --> 01:33:02.860]   he's thinking, and then he's writing very insightful stuff.
[01:33:02.860 --> 01:33:04.820]   And he has an audience thanks to the internet, right?
[01:33:04.820 --> 01:33:05.940]   So people can read it.
[01:33:06.940 --> 01:33:10.100]   That is an amazing positive trend,
[01:33:10.100 --> 01:33:11.420]   which I think will continue.
[01:33:11.420 --> 01:33:13.180]   So I think we're in a kind of, in a way,
[01:33:13.180 --> 01:33:17.660]   we're kind of in a golden age for intellectual exchange.
[01:33:17.660 --> 01:33:19.740]   Even this conversation that you and I are having
[01:33:19.740 --> 01:33:20.820]   is an example of that.
[01:33:20.820 --> 01:33:25.100]   The thing I'm afraid is not gonna happen
[01:33:25.100 --> 01:33:28.100]   just because science is so specialized now.
[01:33:28.100 --> 01:33:33.500]   And it takes so much money and resources
[01:33:33.500 --> 01:33:35.940]   and institutional support within a university
[01:33:35.940 --> 01:33:38.740]   or lab or something to get stuff done.
[01:33:38.740 --> 01:33:41.660]   I think it's getting less and less common
[01:33:41.660 --> 01:33:45.940]   to find polymathic people who are actually able
[01:33:45.940 --> 01:33:48.380]   to do things at the frontier
[01:33:48.380 --> 01:33:51.460]   where they really make a significant contribution
[01:33:51.460 --> 01:33:55.700]   and it's recognized by the natives in that subspecialty.
[01:33:55.700 --> 01:33:58.220]   That's becoming rarer and rarer.
[01:33:58.220 --> 01:34:00.940]   It was much less rare in the time of like Feynman
[01:34:00.940 --> 01:34:02.420]   and von Neumann and people like that,
[01:34:02.420 --> 01:34:04.260]   just because the science was smaller.
[01:34:04.260 --> 01:34:08.020]   Feynman played around with some molecular biology.
[01:34:08.020 --> 01:34:09.700]   When molecular biology would become a big thing,
[01:34:09.700 --> 01:34:12.500]   he was friends with Francis Crick,
[01:34:12.500 --> 01:34:13.620]   who was down in San Diego.
[01:34:13.620 --> 01:34:15.420]   And so he would do stuff like that.
[01:34:15.420 --> 01:34:16.980]   And now it's almost impossible.
[01:34:16.980 --> 01:34:18.020]   And people would tell me,
[01:34:18.020 --> 01:34:20.420]   Steve, like serious theoretical physicists would be like,
[01:34:20.420 --> 01:34:22.100]   "Steve, why are you fucking around with this stuff?
[01:34:22.100 --> 01:34:23.660]   "You're wasting your talent."
[01:34:23.660 --> 01:34:26.020]   They'll literally say stuff like that to me.
[01:34:26.020 --> 01:34:28.700]   So I don't think the trends are good for that.
[01:34:28.700 --> 01:34:31.820]   But for general intellectual exchange,
[01:34:31.820 --> 01:34:33.180]   I think the trend is good.
[01:34:33.180 --> 01:34:36.020]   - Yeah, that's interesting.
[01:34:36.020 --> 01:34:38.860]   Going back to IVF,
[01:34:38.860 --> 01:34:40.700]   do you think the gains will be greater
[01:34:40.700 --> 01:34:43.100]   in any given trait you could think about
[01:34:43.100 --> 01:34:45.500]   for parents who are already high in that trait
[01:34:45.500 --> 01:34:48.940]   or for parents who are lower in that trait
[01:34:48.940 --> 01:34:51.540]   compared to the average of the population?
[01:34:51.540 --> 01:34:54.900]   - I don't think that the base level of mom and dad
[01:34:54.900 --> 01:34:58.620]   is a very big factor actually.
[01:34:58.620 --> 01:35:01.740]   The big factor is how good are your predictors
[01:35:01.740 --> 01:35:03.380]   and how many embryos are you looking at
[01:35:03.380 --> 01:35:05.220]   or how good are your editing tools?
[01:35:05.220 --> 01:35:08.540]   By the way, I just want to reinforce something
[01:35:08.540 --> 01:35:09.380]   I recently learned.
[01:35:09.380 --> 01:35:11.420]   It was so amazing that it freaked me out
[01:35:11.420 --> 01:35:13.460]   'cause I thought, "Oh, I'm kind of in this field,
[01:35:13.460 --> 01:35:15.940]   "so in this industry, so I kind of know about it."
[01:35:15.940 --> 01:35:20.940]   But our company was having some conversations
[01:35:20.940 --> 01:35:26.020]   with a company that handles egg donation.
[01:35:26.020 --> 01:35:28.700]   So it's in the IVF space.
[01:35:28.700 --> 01:35:30.940]   And the egg donors are typically young women,
[01:35:30.940 --> 01:35:33.980]   like 22, 23, they could even be college-age women
[01:35:33.980 --> 01:35:35.980]   who are paid a fair sum of money
[01:35:35.980 --> 01:35:37.820]   to go through an IVF cycle
[01:35:37.820 --> 01:35:41.260]   and just donate the eggs to some billionaire family
[01:35:41.260 --> 01:35:43.900]   or whoever needs the eggs.
[01:35:43.900 --> 01:35:50.060]   And I was told that 60 to 100 eggs per cycle is not unknown.
[01:35:50.060 --> 01:35:55.100]   So it's totally shocking
[01:35:55.100 --> 01:35:56.460]   because usually it's an older woman
[01:35:56.460 --> 01:35:59.380]   who's in her 30s or 40s who's going through it
[01:35:59.380 --> 01:36:04.220]   and they're struggling just to get some viable embryos.
[01:36:04.220 --> 01:36:07.660]   And then so then when you run that same process
[01:36:07.660 --> 01:36:10.540]   with a 19-year-old, what do you get?
[01:36:10.540 --> 01:36:12.420]   And I was kind of shocked at how high these numbers were.
[01:36:12.420 --> 01:36:16.340]   So in principle, let's just imagine
[01:36:16.340 --> 01:36:21.340]   you're a billionaire oligarch, but very tech savvy,
[01:36:21.340 --> 01:36:24.860]   and you want to have a large family
[01:36:24.860 --> 01:36:28.500]   and you want to have really high quality kids,
[01:36:28.500 --> 01:36:30.900]   maybe very long-lived, healthy kids.
[01:36:30.900 --> 01:36:33.620]   You might be selecting the best out of hundreds.
[01:36:33.620 --> 01:36:38.580]   Like there are a hundred parallel universes I could live in.
[01:36:38.580 --> 01:36:41.420]   I get to peek into each one and then choose,
[01:36:41.420 --> 01:36:43.660]   I'm going to step through door number 742
[01:36:43.660 --> 01:36:45.380]   because that's the outcome I like.
[01:36:45.380 --> 01:36:48.020]   Not that expensive actually,
[01:36:48.020 --> 01:36:52.180]   but amazing that people can now do this.
[01:36:52.180 --> 01:36:55.420]   - I guess that'll imply that the returns of being young
[01:36:55.420 --> 01:36:58.220]   when you have kids are gonna increase,
[01:36:58.220 --> 01:37:00.180]   'cause IVF is theoretically supposed to be,
[01:37:00.180 --> 01:37:01.940]   oh, you can have kids when you're old as well now, right?
[01:37:01.940 --> 01:37:04.460]   So it's evening the playing field.
[01:37:04.460 --> 01:37:07.060]   The addition of this with the additional embryos
[01:37:07.060 --> 01:37:08.100]   for somebody who's young is like,
[01:37:08.100 --> 01:37:09.180]   no, it's actually, we're tilting it
[01:37:09.180 --> 01:37:10.660]   way in favor of young now.
[01:37:10.660 --> 01:37:13.620]   At least if you care about those kinds of traits
[01:37:13.620 --> 01:37:16.580]   that IVF could, sorry, genetic screening
[01:37:16.580 --> 01:37:18.500]   could help you figure out.
[01:37:18.500 --> 01:37:22.500]   So let me just ask you what you think
[01:37:22.500 --> 01:37:23.460]   about some of the possibilities
[01:37:23.460 --> 01:37:25.660]   that Goran talks about in that post.
[01:37:25.660 --> 01:37:27.980]   One is that we might be able to turn
[01:37:27.980 --> 01:37:33.500]   induced pluripotent stem cells into embryos,
[01:37:33.500 --> 01:37:35.380]   and then we'll be able to select across
[01:37:35.380 --> 01:37:38.300]   hundreds of embryos without having to harvest eggs.
[01:37:38.300 --> 01:37:42.620]   - Yeah, so eggs are the limiting factor.
[01:37:42.620 --> 01:37:43.540]   Sperm is cheap.
[01:37:43.540 --> 01:37:50.260]   And the technology, the stem cell technology
[01:37:50.260 --> 01:37:55.260]   to take a skin cell and revert it
[01:37:55.340 --> 01:37:58.020]   to the pluripotent state
[01:37:58.020 --> 01:38:01.780]   so that it can become some other kind of cell,
[01:38:01.780 --> 01:38:03.780]   not a skin cell, but maybe an egg cell.
[01:38:03.780 --> 01:38:07.980]   That technology has been more or less mastered
[01:38:07.980 --> 01:38:10.540]   for mice and rats, I believe.
[01:38:10.540 --> 01:38:13.540]   At least, maybe rat is the most common model system.
[01:38:13.540 --> 01:38:20.100]   So there's a few labs, like in Japan,
[01:38:20.100 --> 01:38:21.500]   where they seem to have fully mastered this,
[01:38:21.500 --> 01:38:23.660]   and they've done multiple generations of rat
[01:38:24.740 --> 01:38:27.020]   using induced pluripotency to make the eggs.
[01:38:27.020 --> 01:38:31.100]   And so my guess would be to get it working in humans
[01:38:31.100 --> 01:38:32.100]   is not that hard.
[01:38:32.100 --> 01:38:33.300]   It's a matter of some years
[01:38:33.300 --> 01:38:36.140]   of just slaving away in the lab to get it working.
[01:38:36.140 --> 01:38:39.340]   And I know of startups that are actually working on this.
[01:38:39.340 --> 01:38:44.340]   And now there's gonna be some trepidation initially.
[01:38:44.340 --> 01:38:46.620]   Like, why would you do that
[01:38:46.620 --> 01:38:48.940]   if you can just pay some 19-year-old
[01:38:48.940 --> 01:38:51.420]   to be your egg donor or something?
[01:38:52.500 --> 01:38:55.580]   For example, some gay couples really wanna do it
[01:38:55.580 --> 01:38:57.660]   because maybe they think they can also,
[01:38:57.660 --> 01:39:01.660]   maybe they can take their partner's skin
[01:39:01.660 --> 01:39:03.900]   and make it into an egg, okay.
[01:39:03.900 --> 01:39:07.700]   So there are reasons why you would do it.
[01:39:07.700 --> 01:39:10.060]   But for a lot of people, I think they would say,
[01:39:10.060 --> 01:39:15.340]   "That egg was made through a new and untested process.
[01:39:15.340 --> 01:39:16.860]   "I'd rather have an egg
[01:39:16.860 --> 01:39:18.340]   "where I don't have that additional risk
[01:39:18.340 --> 01:39:20.420]   "in this whole thing."
[01:39:20.420 --> 01:39:22.380]   So I don't know, adoption-wise,
[01:39:22.380 --> 01:39:24.260]   what's gonna happen there.
[01:39:24.260 --> 01:39:28.220]   But I do think that it's just a technological prediction.
[01:39:28.220 --> 01:39:29.460]   It will be possible.
[01:39:29.460 --> 01:39:30.660]   It won't, it isn't,
[01:39:30.660 --> 01:39:32.780]   we're not that far from being able to do it.
[01:39:32.780 --> 01:39:34.740]   I mean, the fact that we can do it in rat
[01:39:34.740 --> 01:39:37.020]   means I think we're not too far.
[01:39:37.020 --> 01:39:38.620]   And yeah, it could have huge implications
[01:39:38.620 --> 01:39:39.460]   for natural selection
[01:39:39.460 --> 01:39:42.900]   if you really wanted to be able to select
[01:39:42.900 --> 01:39:45.140]   from best of 1,000 embryos.
[01:39:45.140 --> 01:39:46.060]   There's no technical,
[01:39:46.060 --> 01:39:47.980]   I mean, eventually there's no technical barrier.
[01:39:47.980 --> 01:39:52.700]   Now, I would say that on roughly the same timescale
[01:39:52.700 --> 01:39:56.780]   for the pluripotent production of eggs
[01:39:56.780 --> 01:40:01.060]   to get mature, to be tested
[01:40:01.060 --> 01:40:03.580]   so that people are confident in it,
[01:40:03.580 --> 01:40:05.460]   I think on that same timescale,
[01:40:05.460 --> 01:40:10.460]   multiplex, very accurate sort of CRISPR-based editing
[01:40:10.460 --> 01:40:14.860]   will also arrive.
[01:40:14.860 --> 01:40:16.260]   And so at that point, it's like,
[01:40:16.260 --> 01:40:17.660]   "Why are you fooling around with this?
[01:40:17.660 --> 01:40:19.220]   "I'll just go in and dig it in,
[01:40:19.220 --> 01:40:20.940]   "make the changes I need to make."
[01:40:20.940 --> 01:40:24.620]   And over that same timescale,
[01:40:24.620 --> 01:40:26.900]   I think it's roughly the timescale
[01:40:26.900 --> 01:40:27.940]   over which we're gonna figure out
[01:40:27.940 --> 01:40:29.180]   where the real causal-
[01:40:29.180 --> 01:40:30.020]   - Yeah, yeah.
[01:40:30.020 --> 01:40:30.860]   'Cause you- - Process are.
[01:40:30.860 --> 01:40:31.700]   Is what you really need. - Exactly.
[01:40:31.700 --> 01:40:32.620]   I was just about to ask 'cause-
[01:40:32.620 --> 01:40:33.460]   - Yeah.
[01:40:33.460 --> 01:40:35.060]   - Otherwise you're just changing the tag, yeah.
[01:40:35.060 --> 01:40:35.900]   - Yeah.
[01:40:35.900 --> 01:40:39.380]   So all of this is stuff that,
[01:40:39.380 --> 01:40:40.340]   you're younger than me,
[01:40:40.340 --> 01:40:43.180]   so I'm fully confident you're gonna see it all.
[01:40:43.180 --> 01:40:46.500]   I may not see all of it,
[01:40:46.500 --> 01:40:49.140]   but I'll see it, the technology perfected.
[01:40:49.140 --> 01:40:52.020]   I won't necessarily see this impact on society,
[01:40:52.020 --> 01:40:52.860]   but you'll probably see it all.
[01:40:52.860 --> 01:40:55.060]   - I'm hoping it's ready by the time I'm ready to have kids,
[01:40:55.060 --> 01:40:57.020]   which is still a while away, so.
[01:40:57.020 --> 01:40:58.940]   Another possibility that Gwern discusses
[01:40:58.940 --> 01:41:01.540]   is iterated embryo selection,
[01:41:01.540 --> 01:41:03.420]   where you just, you can keep,
[01:41:03.420 --> 01:41:05.100]   I'll let you describe how it actually works,
[01:41:05.100 --> 01:41:06.940]   but what do you think about this possibility?
[01:41:06.940 --> 01:41:08.580]   - Yeah, so there it's like,
[01:41:08.580 --> 01:41:10.740]   you make the embryo, you make a bunch of embryos,
[01:41:10.740 --> 01:41:13.380]   and then you decide which ones you like.
[01:41:13.380 --> 01:41:15.660]   And then before you actually make it into a person,
[01:41:15.660 --> 01:41:17.620]   so then that person grows up and reproduces,
[01:41:17.620 --> 01:41:21.380]   you actually reproduce just using iteration of embryos.
[01:41:21.380 --> 01:41:22.620]   That's also plausible too.
[01:41:22.620 --> 01:41:26.740]   So I think all of these, you know,
[01:41:26.740 --> 01:41:31.060]   very molecular technologies have a chance of working.
[01:41:31.060 --> 01:41:33.500]   I don't know anybody who's working on that actually,
[01:41:33.500 --> 01:41:35.820]   really like spending all their time working on that.
[01:41:35.820 --> 01:41:38.260]   But yeah, that could work as well.
[01:41:38.260 --> 01:41:40.100]   Well, I just do wanna say that, you know,
[01:41:40.100 --> 01:41:43.620]   like I made these jokes about the wokes and progressives
[01:41:43.620 --> 01:41:46.060]   and people like that who hate us.
[01:41:46.060 --> 01:41:48.900]   And I actually just feel it's kind of wrong-headed of them.
[01:41:48.900 --> 01:41:50.540]   I think actually the goals,
[01:41:50.540 --> 01:41:53.740]   like I actually consider myself a progressive.
[01:41:53.740 --> 01:41:55.460]   I don't consider myself woke,
[01:41:55.460 --> 01:41:58.380]   but the goals of having healthy people,
[01:41:58.380 --> 01:41:59.780]   maybe healthy, beautiful people
[01:41:59.780 --> 01:42:01.660]   who live to be 200 years old,
[01:42:01.660 --> 01:42:03.060]   who's against that?
[01:42:03.060 --> 01:42:06.820]   You know, like I'm also against inequality in society.
[01:42:06.820 --> 01:42:10.940]   I think, you know, consistent with growth and advancement
[01:42:10.940 --> 01:42:12.460]   in science and technology,
[01:42:12.460 --> 01:42:14.820]   we should try to have a fairly egalitarian society.
[01:42:14.820 --> 01:42:16.100]   I'm for all those things.
[01:42:16.100 --> 01:42:20.620]   So I think if you're a wokester
[01:42:20.620 --> 01:42:21.740]   who's watching this interview
[01:42:21.740 --> 01:42:24.980]   to just like hate Steve Hsu or something,
[01:42:24.980 --> 01:42:25.820]   think about it.
[01:42:25.820 --> 01:42:27.620]   Think about why you're angry at me.
[01:42:27.620 --> 01:42:30.740]   Like I'm actually exploring how the world actually is.
[01:42:30.740 --> 01:42:33.700]   And don't you wanna know how the world actually is?
[01:42:33.700 --> 01:42:36.380]   If we have an inequality problem
[01:42:36.380 --> 01:42:39.900]   because some people don't do well in school,
[01:42:39.900 --> 01:42:42.460]   don't you wanna give those families these resources
[01:42:42.460 --> 01:42:44.660]   so they can fix it for the next generation?
[01:42:44.660 --> 01:42:47.580]   Isn't that the ultimate goal of what you want?
[01:42:47.580 --> 01:42:48.700]   I mean, just think about it.
[01:42:48.700 --> 01:42:50.140]   - Yeah, yeah, yeah.
[01:42:50.140 --> 01:42:51.060]   I guess to steel,
[01:42:51.060 --> 01:42:52.900]   I guess to steel man them a little bit,
[01:42:52.900 --> 01:42:54.140]   somebody might say,
[01:42:54.140 --> 01:42:56.220]   "Listen, one of the things that prevents
[01:42:56.220 --> 01:43:00.620]   "just runaway divergence between families over time
[01:43:00.620 --> 01:43:03.620]   "in the model of like Piketty or something
[01:43:03.620 --> 01:43:07.100]   "is just reversion to the mean."
[01:43:07.100 --> 01:43:08.820]   And, you know, I listened to your conversation
[01:43:08.820 --> 01:43:10.020]   with Gregory Clark where he says
[01:43:10.020 --> 01:43:12.500]   this is kind of already the case.
[01:43:12.500 --> 01:43:15.540]   But to the extent that it doesn't get magnified over time,
[01:43:15.540 --> 01:43:20.300]   the reason is, yeah, it's hard to maintain elite genetics
[01:43:20.300 --> 01:43:22.020]   because of reversion to the mean.
[01:43:22.020 --> 01:43:23.580]   If you can keep that up,
[01:43:23.580 --> 01:43:27.060]   and if there's increasing returns to having good genes
[01:43:27.060 --> 01:43:29.820]   because you can then afford these kinds of treatments,
[01:43:29.820 --> 01:43:32.260]   then the possibility of society,
[01:43:32.260 --> 01:43:35.260]   instead of a normal distribution for society,
[01:43:35.260 --> 01:43:37.780]   you can just have a bimodal distribution
[01:43:37.780 --> 01:43:40.460]   that keeps getting further and further apart.
[01:43:40.460 --> 01:43:42.220]   That is a potential possibility.
[01:43:42.220 --> 01:43:43.980]   - The Morlocks and the Eloy.
[01:43:43.980 --> 01:43:50.020]   Yeah, I mean, I think that is a fair concern
[01:43:50.020 --> 01:43:55.100]   that this could lead to grotesque, huge inequality.
[01:43:55.100 --> 01:43:58.660]   And that is a risk of the technology in it.
[01:43:58.660 --> 01:44:00.300]   A lot of that depends on society too.
[01:44:00.300 --> 01:44:03.180]   I mean, like when someone confronts me with that,
[01:44:03.180 --> 01:44:05.380]   I will acknowledge it as a legitimate concern,
[01:44:05.380 --> 01:44:06.340]   but then I'll say like,
[01:44:06.340 --> 01:44:08.540]   "You know, we live in a country which is the rich,
[01:44:08.540 --> 01:44:10.140]   "in some sense, the richest country in the world.
[01:44:10.140 --> 01:44:10.980]   "And there are plenty of people
[01:44:10.980 --> 01:44:12.940]   "who don't even have healthcare.
[01:44:12.940 --> 01:44:14.500]   "Are you worried about that inequality?"
[01:44:14.500 --> 01:44:17.940]   Like, you know, like we have a lot of inequality.
[01:44:17.940 --> 01:44:19.460]   There's a lot of things for you to worry about
[01:44:19.460 --> 01:44:20.540]   when it comes to inequality.
[01:44:20.540 --> 01:44:23.540]   And this is some technology which could contribute to it,
[01:44:23.540 --> 01:44:24.940]   but it doesn't have to.
[01:44:24.940 --> 01:44:27.300]   - Actually, maybe this might not be globally beneficial,
[01:44:27.300 --> 01:44:29.340]   but for at least this particular debate,
[01:44:29.340 --> 01:44:31.700]   it might be beneficial if the case was,
[01:44:31.700 --> 01:44:32.540]   when I asked you like,
[01:44:32.540 --> 01:44:34.660]   "Oh, do people who are lower on some trait
[01:44:34.660 --> 01:44:38.260]   "have a greater potential for increasing that trait
[01:44:38.260 --> 01:44:40.180]   "than somebody who's higher up on it?"
[01:44:40.180 --> 01:44:42.140]   If that was the case, then you could just say like,
[01:44:42.140 --> 01:44:44.020]   "Listen, the smart people
[01:44:44.020 --> 01:44:45.460]   "are just gonna asymptote at some point,
[01:44:45.460 --> 01:44:49.460]   "whereas the dumb people can just catch up over time, right?"
[01:44:49.460 --> 01:44:52.900]   - Well, I think again, like if you're more of a left guy
[01:44:52.900 --> 01:44:54.580]   and you like government intervention,
[01:44:54.580 --> 01:44:57.780]   and so this becomes part of the government healthcare system
[01:44:57.780 --> 01:44:59.340]   and it's free.
[01:44:59.340 --> 01:45:02.300]   And you say, "We will allow more aggressive edits
[01:45:02.300 --> 01:45:06.220]   "or more embryos to be produced for below average families."
[01:45:06.220 --> 01:45:08.260]   There's a very natural way you can redistribute,
[01:45:08.260 --> 01:45:10.940]   just like you're gonna forcibly take a bunch of money
[01:45:10.940 --> 01:45:14.020]   from me when I die that I would rather pass on to my kids,
[01:45:14.020 --> 01:45:15.980]   you're gonna forcibly take it from me.
[01:45:15.980 --> 01:45:19.860]   Well, you can forcibly give more genomic prediction
[01:45:19.860 --> 01:45:23.060]   sources to people who need them.
[01:45:23.060 --> 01:45:24.140]   It's easy.
[01:45:24.140 --> 01:45:25.860]   - Okay, so in your,
[01:45:25.860 --> 01:45:28.060]   just to shift topics quite a bit here,
[01:45:29.860 --> 01:45:33.820]   you had an interesting post on that recent Twitter viral meme
[01:45:33.820 --> 01:45:36.500]   about the word cells and shape rotators,
[01:45:36.500 --> 01:45:40.940]   about how actually the concept of a shape rotator
[01:45:40.940 --> 01:45:42.860]   is combining two separate abilities,
[01:45:42.860 --> 01:45:46.500]   math and spatial ability that are,
[01:45:46.500 --> 01:45:48.380]   yeah, when you do like principal component analysis
[01:45:48.380 --> 01:45:50.260]   and psychometrics, they turn up to be different,
[01:45:50.260 --> 01:45:51.100]   but correlated.
[01:45:51.100 --> 01:45:56.060]   As a programmer, I'm really curious about which of those
[01:45:56.060 --> 01:45:58.500]   is the one that is required more
[01:45:58.500 --> 01:46:00.300]   for that particular skillset.
[01:46:00.300 --> 01:46:02.300]   Because I'm the kind of person,
[01:46:02.300 --> 01:46:05.300]   when we're talking about abstractions and data structures
[01:46:05.300 --> 01:46:07.420]   and the flow of a program,
[01:46:07.420 --> 01:46:08.780]   I'm the kind of person that intuitively
[01:46:08.780 --> 01:46:10.340]   likes to think about it from,
[01:46:10.340 --> 01:46:13.180]   I just imagine what it looks like visually.
[01:46:13.180 --> 01:46:15.540]   Whereas I know friends who,
[01:46:15.540 --> 01:46:17.180]   I said, "Okay, so clearly programming
[01:46:17.180 --> 01:46:20.060]   "is a visuospatial ability."
[01:46:20.060 --> 01:46:21.980]   And they said that actually they don't imagine it
[01:46:21.980 --> 01:46:24.660]   visually at all, that for them it's much more of
[01:46:24.660 --> 01:46:25.900]   just looking through the loop
[01:46:25.900 --> 01:46:27.980]   and what's gonna happen next, what's gonna happen next.
[01:46:27.980 --> 01:46:29.540]   So yeah, I'm curious, which of these is
[01:46:29.540 --> 01:46:31.780]   a better description of what programming is like?
[01:46:31.780 --> 01:46:35.380]   - Yeah, I think your description captured the whole story
[01:46:35.380 --> 01:46:38.980]   that people are very different in the way they attack,
[01:46:38.980 --> 01:46:40.500]   even though they're attacking the same problem,
[01:46:40.500 --> 01:46:43.060]   the way that their brain does it.
[01:46:43.060 --> 01:46:44.660]   I think that's one of the most fascinating things
[01:46:44.660 --> 01:46:47.180]   about this field of psychometrics and psychology
[01:46:47.180 --> 01:46:50.500]   is that we're really trying to get into that.
[01:46:50.500 --> 01:46:52.900]   One of the things that fascinated me
[01:46:52.900 --> 01:46:55.380]   when I was being educated and going through training
[01:46:55.380 --> 01:46:57.900]   as in theoretical physics and math is like,
[01:46:57.900 --> 01:47:00.940]   looking at how my, whatever, classmates at Caltech
[01:47:00.940 --> 01:47:02.980]   or Richard Feynman or somebody approached a problem,
[01:47:02.980 --> 01:47:05.300]   which might be totally different than the way I would do it
[01:47:05.300 --> 01:47:07.540]   or the way that we would communicate about the solution
[01:47:07.540 --> 01:47:09.220]   once we got it.
[01:47:09.220 --> 01:47:11.220]   And there clearly are people who are visual,
[01:47:11.220 --> 01:47:13.260]   like Feynman was a very visual thinker.
[01:47:13.260 --> 01:47:18.380]   Other people are more kind of logic-go-verbal
[01:47:18.380 --> 01:47:20.940]   where they're like stepping through things
[01:47:20.940 --> 01:47:23.380]   and it might even be like they hear the arguments
[01:47:23.380 --> 01:47:25.020]   as they're stepping through it or something.
[01:47:25.020 --> 01:47:27.140]   So everybody's different
[01:47:27.140 --> 01:47:29.620]   and I think those things are super fascinating.
[01:47:29.620 --> 01:47:34.220]   Something that's kind of gone out of fashion now,
[01:47:34.220 --> 01:47:37.500]   but was very in very standard when I was growing up
[01:47:37.500 --> 01:47:39.620]   is like when I took shop class,
[01:47:39.620 --> 01:47:41.300]   I don't know if you had to take shop class
[01:47:41.300 --> 01:47:43.100]   in junior high or high school,
[01:47:43.100 --> 01:47:44.940]   but we had to take like shop class,
[01:47:44.940 --> 01:47:46.620]   which where you go and bend metal
[01:47:46.620 --> 01:47:48.740]   and literally they have machines that would,
[01:47:48.740 --> 01:47:51.500]   I made like an ashtray or something out of steel
[01:47:51.500 --> 01:47:52.620]   or something.
[01:47:52.620 --> 01:47:56.100]   So yet in that class, which is very spatially loaded,
[01:47:56.100 --> 01:47:57.100]   like you could have guys,
[01:47:57.100 --> 01:48:01.140]   like I had a friend who had a very high SAT score
[01:48:01.140 --> 01:48:03.700]   and went to Princeton to study English.
[01:48:03.700 --> 01:48:06.820]   That guy could not spatially rotate at all.
[01:48:06.820 --> 01:48:09.900]   He was totally lost in figuring out how to like do the bends
[01:48:09.900 --> 01:48:12.260]   to make the ashtray or whatever, right?
[01:48:12.260 --> 01:48:14.980]   So you see that very clearly.
[01:48:14.980 --> 01:48:18.140]   And in those old days when things were more based,
[01:48:18.140 --> 01:48:19.420]   when you went to shop class,
[01:48:19.420 --> 01:48:21.820]   sometimes they just give you a standardized test,
[01:48:21.820 --> 01:48:24.700]   which was a standardized test of spatial ability.
[01:48:24.700 --> 01:48:26.820]   So we're all like, my generation is like,
[01:48:26.820 --> 01:48:30.300]   you don't have to lie to me about all these things.
[01:48:30.300 --> 01:48:31.500]   We saw how it works.
[01:48:31.500 --> 01:48:33.340]   We saw people take the standardized test
[01:48:33.340 --> 01:48:37.100]   for spatial visualization.
[01:48:37.100 --> 01:48:41.420]   And then we saw people try to fucking work the machine,
[01:48:41.420 --> 01:48:42.660]   the metal bending machine.
[01:48:42.660 --> 01:48:43.980]   And some people just couldn't do it.
[01:48:43.980 --> 01:48:45.820]   Like they couldn't actually make the thing look the way
[01:48:45.820 --> 01:48:47.500]   the product looked the way it was supposed to look.
[01:48:47.500 --> 01:48:52.500]   So in the real economy of atoms
[01:48:52.500 --> 01:48:54.860]   and grams of steel, kilograms of steel,
[01:48:54.860 --> 01:48:57.260]   which has all moved to China now or something,
[01:48:57.260 --> 01:49:00.020]   that all this stuff is super important.
[01:49:00.020 --> 01:49:01.900]   Like you can't just like theorize about like, okay,
[01:49:01.900 --> 01:49:03.660]   then I have this module that does this
[01:49:03.660 --> 01:49:05.260]   and this function is gonna have these types.
[01:49:05.260 --> 01:49:06.900]   And well, that's nice.
[01:49:06.900 --> 01:49:09.740]   That's super valuable in this part of the economy,
[01:49:09.740 --> 01:49:12.220]   but somebody has got to get this plant working
[01:49:12.220 --> 01:49:13.820]   and it's got to be efficient.
[01:49:13.820 --> 01:49:16.020]   And we got to put the machines here and here
[01:49:16.020 --> 01:49:18.180]   so we don't have to carry the shit too far from here.
[01:49:18.180 --> 01:49:19.740]   You know, there's a lot of like,
[01:49:19.740 --> 01:49:21.940]   that's very spatially loaded stuff,
[01:49:21.940 --> 01:49:24.900]   which used to be part of the American economy
[01:49:24.900 --> 01:49:25.780]   and education system.
[01:49:25.780 --> 01:49:28.060]   And now I think it's all gone, but it's real.
[01:49:28.060 --> 01:49:28.980]   It's not fake.
[01:49:28.980 --> 01:49:30.580]   No, people are not making this up.
[01:49:30.580 --> 01:49:33.180]   And psychometricians of the 1950s and '60s
[01:49:33.180 --> 01:49:36.540]   would have been like, yeah, here's my 10 volume treaties
[01:49:36.540 --> 01:49:41.380]   on measuring spatial visualization ability or something.
[01:49:41.380 --> 01:49:43.380]   - Yeah, even if you read the biography
[01:49:43.380 --> 01:49:44.340]   of somebody like Einstein,
[01:49:44.340 --> 01:49:46.940]   I mean, he was especially known for being a spatial thinker.
[01:49:46.940 --> 01:49:49.220]   - Oh, he was incredibly visual, incredibly visual.
[01:49:49.220 --> 01:49:50.420]   - Right, just like thought experiments
[01:49:50.420 --> 01:49:52.300]   that are just basically what does it look like
[01:49:52.300 --> 01:49:55.260]   or what does it feel like to be moving at this speed
[01:49:55.260 --> 01:49:56.580]   or whatever.
[01:49:56.580 --> 01:49:57.860]   Yeah, that's interesting.
[01:49:57.860 --> 01:50:01.140]   Yeah, so I guess in the case of programmers,
[01:50:01.140 --> 01:50:02.460]   I'm not sure I got your answer,
[01:50:02.460 --> 01:50:04.500]   but for that particular discipline,
[01:50:04.500 --> 01:50:07.900]   which do you think is the more pertinent skill?
[01:50:07.900 --> 01:50:11.820]   - I was gonna say people are gonna do it different ways.
[01:50:11.820 --> 01:50:16.220]   I do think that if you compare the category of engineers
[01:50:16.220 --> 01:50:18.980]   to the category of software developers,
[01:50:18.980 --> 01:50:20.740]   engineers generally, I think have higher,
[01:50:20.740 --> 01:50:23.520]   on average, higher spatial ability and they're using it.
[01:50:23.520 --> 01:50:26.940]   Whereas you can be an awesome programmer with like zero,
[01:50:26.940 --> 01:50:29.100]   I think zero spatial ability.
[01:50:29.100 --> 01:50:30.420]   That's my guess.
[01:50:30.420 --> 01:50:32.340]   - Yeah, I wonder if the,
[01:50:32.340 --> 01:50:34.540]   you know, when you're studying history or something,
[01:50:34.540 --> 01:50:36.820]   you notice that some people are really attracted
[01:50:36.820 --> 01:50:38.540]   to the military history aspect of it
[01:50:38.540 --> 01:50:40.780]   and seeing how the units are moving and stuff.
[01:50:40.780 --> 01:50:42.020]   And I wonder if that's because
[01:50:42.020 --> 01:50:43.540]   they have a higher spatial ability
[01:50:43.540 --> 01:50:46.580]   and they just need to be able to understand
[01:50:46.580 --> 01:50:48.980]   how the units are moving and so on.
[01:50:48.980 --> 01:50:50.780]   - I was gonna say, this is a very weird thing
[01:50:50.780 --> 01:50:51.620]   for me to review,
[01:50:51.620 --> 01:50:54.140]   but like sometimes when I'm having trouble falling asleep,
[01:50:54.140 --> 01:50:57.160]   I'll be visualizing, like recently I was thinking about,
[01:50:57.160 --> 01:51:01.120]   you know, how I would use a ballistic missile
[01:51:01.120 --> 01:51:03.000]   to target like an aircraft carrier, right?
[01:51:03.000 --> 01:51:05.580]   But like have the Chinese actually solve this problem
[01:51:05.580 --> 01:51:08.040]   or, you know, and sometimes if I'm trying to go to sleep,
[01:51:08.040 --> 01:51:10.400]   I'll just like be visualizing like, okay,
[01:51:10.400 --> 01:51:12.920]   when you're at about an altitude of, you know,
[01:51:12.920 --> 01:51:16.160]   five kilometers, what can your radar see
[01:51:16.160 --> 01:51:18.340]   and how much resolution do you need?
[01:51:18.340 --> 01:51:20.260]   And then how much time do you have for course correction
[01:51:20.260 --> 01:51:21.100]   to hit the ship?
[01:51:21.100 --> 01:51:26.060]   And, you know, like I'll be thinking about stuff like that
[01:51:26.060 --> 01:51:30.100]   for relaxation, but I'm, and it is highly visual
[01:51:30.100 --> 01:51:31.500]   but you're, and also quantitative
[01:51:31.500 --> 01:51:32.620]   'cause you have to make some estimates.
[01:51:32.620 --> 01:51:35.340]   But like, I think that would be typical
[01:51:35.340 --> 01:51:36.840]   of like a lot of physicists.
[01:51:36.840 --> 01:51:38.260]   'Cause like, if we start talking about it,
[01:51:38.260 --> 01:51:39.300]   we'd be like, oh yeah, right.
[01:51:39.300 --> 01:51:41.920]   And you've only got about point of order,
[01:51:41.920 --> 01:51:43.220]   a 10th of a second to do this.
[01:51:43.220 --> 01:51:45.180]   And you, but your thing can do it in milliseconds.
[01:51:45.180 --> 01:51:46.380]   So we're okay.
[01:51:46.380 --> 01:51:49.700]   And then, anyway, that kind of thinking is very prevalent
[01:51:49.700 --> 01:51:50.660]   amongst certain types.
[01:51:50.660 --> 01:51:51.500]   - Right, right.
[01:51:51.500 --> 01:51:57.100]   Now I'm curious why it's the case that people from physics
[01:51:57.100 --> 01:52:00.100]   so often transition to finance.
[01:52:00.100 --> 01:52:02.620]   I think that was something you were considering at one point
[01:52:02.620 --> 01:52:05.140]   is the underlying knowledge and mathematics
[01:52:05.140 --> 01:52:08.100]   is just the same or is it just such a credible signal
[01:52:08.100 --> 01:52:12.200]   of mathematical ability and G that, you know,
[01:52:12.200 --> 01:52:14.220]   of the quant forms and whatever
[01:52:14.220 --> 01:52:15.860]   they wanna hire physics students?
[01:52:15.860 --> 01:52:20.820]   - The answer is a little bit complicated.
[01:52:20.820 --> 01:52:23.180]   I think all the factors you mentioned are true,
[01:52:23.180 --> 01:52:28.180]   but one of the things was that in the early phase,
[01:52:28.180 --> 01:52:32.920]   like in the '80s and '90s, when a lot of people
[01:52:32.920 --> 01:52:35.200]   in my generation went into finance,
[01:52:35.200 --> 01:52:38.360]   a lot of them went to do, to trade derivatives.
[01:52:38.360 --> 01:52:41.040]   And if you look at options pricing theory,
[01:52:41.040 --> 01:52:42.520]   it looks a lot like physics.
[01:52:42.520 --> 01:52:46.000]   It's kind of like the mathematics of random walks basically.
[01:52:46.000 --> 01:52:49.840]   And so there was a very tight, not tight connection,
[01:52:49.840 --> 01:52:54.280]   but the concepts were strongly related that were necessary.
[01:52:54.280 --> 01:52:56.840]   Now, if you brought it out a little bit more to say like,
[01:52:56.840 --> 01:52:59.480]   okay, but nowadays, if you go to really big quant funds
[01:52:59.480 --> 01:53:02.840]   and they're looking for signal and analyzing tons of data,
[01:53:02.840 --> 01:53:03.920]   and they're not trading derivatives,
[01:53:03.920 --> 01:53:05.840]   they're trading, you know, just actual names
[01:53:05.840 --> 01:53:10.080]   like stocks or whatever, a lot,
[01:53:10.080 --> 01:53:11.880]   I think there's more loading on machine learning
[01:53:11.880 --> 01:53:15.480]   and CS background now, and the physicists who go in,
[01:53:15.480 --> 01:53:18.160]   they're having to, they're using that subset
[01:53:18.160 --> 01:53:22.120]   of their skills, but the funds would just as soon hire a CS
[01:53:22.120 --> 01:53:24.420]   or ML type guy to do it.
[01:53:24.420 --> 01:53:27.160]   So it's a little bit of a complicated answer.
[01:53:27.160 --> 01:53:28.240]   - Yeah, that's super interesting.
[01:53:28.240 --> 01:53:31.440]   'Cause I mean, back in the '90s and early 2000s,
[01:53:31.440 --> 01:53:34.840]   I don't know, I was watching, I read that book
[01:53:34.840 --> 01:53:36.920]   about the fall of long-term capital management,
[01:53:36.920 --> 01:53:39.320]   and obviously, I guess this is a cautionary tale,
[01:53:39.320 --> 01:53:40.600]   but still, it's kind of cool to-
[01:53:40.600 --> 01:53:41.800]   - Actually, there are two books.
[01:53:41.800 --> 01:53:43.840]   There's one called "When Genius Failed,"
[01:53:43.840 --> 01:53:45.880]   and then there's another, there's actually three books,
[01:53:45.880 --> 01:53:48.880]   at least three books, but they're all good.
[01:53:48.880 --> 01:53:50.200]   - Yeah, and then you just hear about like,
[01:53:50.200 --> 01:53:52.360]   obviously, the people who created options,
[01:53:52.360 --> 01:53:55.280]   pricing theory there, but the applying, you know,
[01:53:55.280 --> 01:53:57.080]   calculus to random walks and stuff,
[01:53:57.080 --> 01:53:58.880]   stuff I don't understand, but just super cool
[01:53:58.880 --> 01:54:01.080]   that you have these mathematicians that are just coming in
[01:54:01.080 --> 01:54:05.120]   and applying these ideas to finance.
[01:54:05.120 --> 01:54:07.120]   - I do want to say one thing about physicists,
[01:54:07.120 --> 01:54:09.120]   which is a little different from mathematicians
[01:54:09.120 --> 01:54:10.480]   and computer science guys,
[01:54:10.480 --> 01:54:13.680]   maybe not so different from data science guys,
[01:54:13.680 --> 01:54:15.840]   but definitely different from most computer science guys
[01:54:15.840 --> 01:54:19.680]   and most math guys, is that we spend a lot of time
[01:54:19.680 --> 01:54:22.200]   looking at bad, noisy data.
[01:54:22.200 --> 01:54:23.920]   So even if you're a theorist,
[01:54:23.920 --> 01:54:26.540]   you had to go through these lab courses where,
[01:54:26.540 --> 01:54:29.360]   I mean, for me, those lab courses were among the hardest,
[01:54:29.360 --> 01:54:31.020]   like the worst, 'cause you had to go in
[01:54:31.020 --> 01:54:34.200]   and build some electronic equipment to take some data.
[01:54:34.200 --> 01:54:36.360]   And, you know, it could be extremely noisy,
[01:54:36.360 --> 01:54:38.920]   like you're measuring muon cosmic rays
[01:54:38.920 --> 01:54:41.560]   coming through the roof and hitting your detector.
[01:54:41.560 --> 01:54:43.600]   And then you have to analyze the data.
[01:54:43.600 --> 01:54:47.080]   And when you're building this thing, you screw it up.
[01:54:47.080 --> 01:54:49.480]   And so like you get data that makes no sense,
[01:54:49.480 --> 01:54:52.760]   or something about the amplifier wasn't right,
[01:54:52.760 --> 01:54:57.760]   or you're used to seeing data that sucks.
[01:54:57.760 --> 01:54:59.580]   And you have this theoretical view
[01:54:59.580 --> 01:55:00.960]   of what should be happening,
[01:55:00.960 --> 01:55:02.480]   like maybe you're visualizing it,
[01:55:02.480 --> 01:55:05.160]   like the muon comes in and it does this,
[01:55:05.160 --> 01:55:08.160]   and interpolating between the theoretical view
[01:55:08.160 --> 01:55:09.560]   of what should be happening
[01:55:09.560 --> 01:55:11.800]   with the particles and the systems,
[01:55:11.800 --> 01:55:13.960]   and what the actual data looks like,
[01:55:13.960 --> 01:55:16.080]   and saying like, oh shit, we didn't do this,
[01:55:16.080 --> 01:55:17.440]   or we didn't shield this part,
[01:55:17.440 --> 01:55:19.380]   so that's why we're getting that.
[01:55:19.380 --> 01:55:22.500]   That's something physicists are very, very used to doing.
[01:55:22.500 --> 01:55:25.240]   And mathematicians are often shitty at it.
[01:55:25.240 --> 01:55:28.120]   They just accept, oh, I just accept this is the data,
[01:55:28.120 --> 01:55:30.800]   now I'll reason with this data.
[01:55:30.800 --> 01:55:33.800]   And the same could be true for computer science people.
[01:55:33.800 --> 01:55:36.200]   But you need someone who's actually had to deal
[01:55:36.200 --> 01:55:38.720]   with shitty data, and tried to connect it
[01:55:38.720 --> 01:55:41.280]   to a very elegant mathematical model.
[01:55:41.280 --> 01:55:43.920]   That's something physicists kind of uniquely are used to.
[01:55:43.920 --> 01:55:46.080]   - No, but I think that's also true of CS people,
[01:55:46.080 --> 01:55:50.200]   which is that you have, obviously in debugging,
[01:55:50.200 --> 01:55:52.600]   there's many potential problems that could happen.
[01:55:52.600 --> 01:55:54.460]   One of them, obviously, is just you wrote the code wrong,
[01:55:54.460 --> 01:55:57.320]   but often you get the actual implementation just right.
[01:55:57.320 --> 01:56:00.800]   There's so many layers of abstraction beneath you,
[01:56:00.800 --> 01:56:03.840]   and above the actual hardware,
[01:56:03.840 --> 01:56:06.720]   that you have to figure out why is the correspondence
[01:56:06.720 --> 01:56:08.820]   between this idea I had,
[01:56:08.820 --> 01:56:11.960]   and the actual program output not the same.
[01:56:11.960 --> 01:56:13.800]   - I think that's fair.
[01:56:13.800 --> 01:56:16.400]   Because yeah, when you debug your code,
[01:56:16.400 --> 01:56:18.300]   there are many different ways it could have failed.
[01:56:18.300 --> 01:56:21.960]   And you have to actually, in a sense, step back and model.
[01:56:21.960 --> 01:56:24.300]   Like, oh, maybe this module's feeding me
[01:56:24.300 --> 01:56:26.320]   something back wrong, and that's what's causing the problem,
[01:56:26.320 --> 01:56:27.480]   or it's this other layer.
[01:56:27.480 --> 01:56:30.280]   So that is very analogous to when we have to deal
[01:56:30.280 --> 01:56:32.180]   with a physical experiment in the lab.
[01:56:32.180 --> 01:56:37.160]   The thing with physics, though, is that we're really,
[01:56:37.160 --> 01:56:40.480]   really geared toward getting toward the underlying reality.
[01:56:40.480 --> 01:56:42.600]   Like, if it's really late at night,
[01:56:42.600 --> 01:56:45.300]   and my buddy, my lab partner, and I just wanna get out
[01:56:45.300 --> 01:56:49.360]   and go to sleep, we can't tell ourselves
[01:56:49.360 --> 01:56:50.600]   that things are okay.
[01:56:50.600 --> 01:56:53.680]   We didn't actually screw up the shielding on that.
[01:56:53.680 --> 01:56:55.880]   It's okay, we'll just bring the data home and look at it.
[01:56:55.880 --> 01:56:58.240]   No, we gotta actually decide.
[01:56:58.240 --> 01:56:59.760]   Do we have to spend three more hours
[01:56:59.760 --> 01:57:02.200]   ripping this thing apart and reshielding it?
[01:57:02.200 --> 01:57:04.960]   Or, we have to get to the real underlying reality.
[01:57:04.960 --> 01:57:05.800]   We can't fake it.
[01:57:05.800 --> 01:57:07.360]   We can't just pretend.
[01:57:07.360 --> 01:57:10.880]   Like, oh, this admission scheme will work perfectly.
[01:57:10.880 --> 01:57:14.400]   You know, like, we can't lie to ourselves about it.
[01:57:14.400 --> 01:57:16.680]   And I guess that's true for coders too.
[01:57:16.680 --> 01:57:18.520]   But anyway, it's just very different
[01:57:18.520 --> 01:57:20.000]   from like social scientists and stuff,
[01:57:20.000 --> 01:57:22.880]   where they can just decide, ah, I don't like that reality.
[01:57:22.880 --> 01:57:25.760]   I'll just make up this model for how society behaves,
[01:57:25.760 --> 01:57:26.880]   and then I'm done.
[01:57:26.880 --> 01:57:27.880]   We can't do that.
[01:57:27.880 --> 01:57:29.400]   - Yeah, so given the skillset
[01:57:29.400 --> 01:57:32.360]   that theoretical physicists have, as you just mentioned,
[01:57:32.360 --> 01:57:33.200]   is it potentially the case,
[01:57:33.200 --> 01:57:36.680]   I mean, obviously, the common criticism
[01:57:36.680 --> 01:57:37.880]   of physics as a community
[01:57:37.880 --> 01:57:40.960]   is that they're absorbing too much talent.
[01:57:40.960 --> 01:57:42.840]   You know, like, three or four standard deviations
[01:57:42.840 --> 01:57:45.000]   above average intelligence people
[01:57:45.000 --> 01:57:48.720]   are working on a field that, I guess,
[01:57:48.720 --> 01:57:49.800]   in popular convention, at least,
[01:57:49.800 --> 01:57:52.000]   seems like isn't making as much progress.
[01:57:52.000 --> 01:57:54.880]   And then, so should more people in physics
[01:57:54.880 --> 01:57:56.760]   be making the step that you made,
[01:57:56.760 --> 01:57:57.600]   which is just like, yeah,
[01:57:57.600 --> 01:57:59.640]   I learned all these skills in theoretical physics.
[01:57:59.640 --> 01:58:01.520]   I want to move out of it.
[01:58:01.520 --> 01:58:02.680]   Maybe finance is one way
[01:58:02.680 --> 01:58:04.640]   in which we're getting these pro-social benefits
[01:58:04.640 --> 01:58:06.520]   from the skills that physics builds.
[01:58:06.520 --> 01:58:08.280]   But also, yeah, just like stepping into fields
[01:58:08.280 --> 01:58:10.840]   like genomics or things like that.
[01:58:10.840 --> 01:58:13.840]   Should more physicists be just using their skills elsewhere?
[01:58:13.840 --> 01:58:20.240]   - Yeah, so number one, the attrition rate is super high.
[01:58:20.240 --> 01:58:22.520]   So even if you cut, you say like,
[01:58:22.520 --> 01:58:25.240]   take this set of kids that are plus three
[01:58:25.240 --> 01:58:27.760]   or four standard deviations in ability,
[01:58:27.760 --> 01:58:30.080]   and they enter a physics major
[01:58:30.080 --> 01:58:32.760]   at Princeton or MIT or something,
[01:58:32.760 --> 01:58:34.000]   what fraction of them actually end up
[01:58:34.000 --> 01:58:35.160]   as practicing physicists?
[01:58:35.160 --> 01:58:36.120]   It's pretty small.
[01:58:36.120 --> 01:58:38.960]   So they're bleeding off at all points.
[01:58:38.960 --> 01:58:40.680]   Like, you know, Bezos started in physics,
[01:58:40.680 --> 01:58:42.120]   and toward the end of his Princeton career,
[01:58:42.120 --> 01:58:43.440]   he switched to computer science.
[01:58:43.440 --> 01:58:47.320]   And, you know, Elon was in graduate school in physics,
[01:58:47.320 --> 01:58:49.200]   at applied physics or physics at Stanford,
[01:58:49.200 --> 01:58:50.320]   and he bled out.
[01:58:50.320 --> 01:58:54.520]   So it's already the case that, for me,
[01:58:54.520 --> 01:58:56.920]   one way to say it is the education is phenomenal.
[01:58:56.920 --> 01:58:58.440]   You should try to get that education.
[01:58:58.440 --> 01:58:59.920]   It'll pay off for you later.
[01:58:59.920 --> 01:59:01.760]   And probably you're gonna bleed out.
[01:59:01.760 --> 01:59:04.240]   You're gonna attrit away and do something else.
[01:59:04.240 --> 01:59:05.320]   Now, if you say like, okay,
[01:59:05.320 --> 01:59:08.000]   of the thousands of theoretical physicists
[01:59:08.000 --> 01:59:11.160]   or physicists who do fundamental research,
[01:59:11.160 --> 01:59:13.000]   including the experimentalists around the world,
[01:59:13.000 --> 01:59:14.240]   there are tens of thousands,
[01:59:14.240 --> 01:59:16.720]   and maybe some of those guys should also be doing
[01:59:16.720 --> 01:59:20.760]   some more cancer research or doing financial modeling.
[01:59:20.760 --> 01:59:21.880]   Yeah, maybe so, maybe so.
[01:59:21.880 --> 01:59:24.000]   I mean, maybe even some of those guys should,
[01:59:24.000 --> 01:59:25.200]   you know, we should tear off,
[01:59:25.200 --> 01:59:26.800]   you know, we should remove even more of those guys
[01:59:26.800 --> 01:59:28.600]   and have them do more applied stuff.
[01:59:28.600 --> 01:59:31.040]   There's still some argument in favor of that.
[01:59:31.040 --> 01:59:32.320]   But we do need a core of people
[01:59:32.320 --> 01:59:34.760]   that are trying to do these really hard fundamental,
[01:59:34.760 --> 01:59:36.920]   answer these hard fundamental questions about nature.
[01:59:36.920 --> 01:59:39.440]   - By the way, the Bezos example is really interesting.
[01:59:39.440 --> 01:59:41.680]   And obviously, by the way,
[01:59:41.680 --> 01:59:43.320]   for the people in the audience who might not know,
[01:59:43.320 --> 01:59:46.160]   is he was asked once why,
[01:59:46.160 --> 01:59:47.280]   I mean, his original plan, I think,
[01:59:47.280 --> 01:59:49.000]   was to become a theoretical physicist.
[01:59:49.000 --> 01:59:50.160]   And the reason he didn't pursue it
[01:59:50.160 --> 01:59:52.080]   is that he noticed one of his friends
[01:59:52.080 --> 01:59:54.600]   was just so much obviously more gifted than him
[01:59:54.600 --> 01:59:57.640]   at that skill that the story he tells
[01:59:57.640 --> 02:00:00.520]   is like Bezos was working on a problem
[02:00:00.520 --> 02:00:03.600]   for many hours and making no progress.
[02:00:03.600 --> 02:00:05.680]   And one of his friends just looks at it in an instant.
[02:00:05.680 --> 02:00:07.520]   He's like, oh, the answer is, I don't know what it was,
[02:00:07.520 --> 02:00:10.280]   but like, blah, blah, blah, cosine of something.
[02:00:10.280 --> 02:00:11.600]   And then he's just like, yeah,
[02:00:11.600 --> 02:00:12.720]   I just eliminated all the terms.
[02:00:12.720 --> 02:00:14.520]   I recognize a similar problem.
[02:00:14.520 --> 02:00:16.080]   And so basically it's like, okay,
[02:00:16.080 --> 02:00:18.760]   this is not my competitive advantage.
[02:00:18.760 --> 02:00:21.800]   - Can I just say one, I got to add one anecdote.
[02:00:21.800 --> 02:00:24.880]   The guy, that guy,
[02:00:24.880 --> 02:00:26.360]   so I know a lot of the guys
[02:00:26.360 --> 02:00:28.880]   that were in Bezos' Eating Club and were also,
[02:00:28.880 --> 02:00:31.160]   'cause we're very similar in vintage,
[02:00:31.160 --> 02:00:33.360]   and who took all these classes with him.
[02:00:33.360 --> 02:00:34.880]   And a lot of them were late,
[02:00:34.880 --> 02:00:36.560]   one was a friend of mine from high school,
[02:00:36.560 --> 02:00:37.600]   but another were guys,
[02:00:37.600 --> 02:00:38.720]   a whole another set were guys
[02:00:38.720 --> 02:00:39.880]   that I went to grad school at Berkeley,
[02:00:39.880 --> 02:00:41.560]   'cause there's a whole Princeton contingent
[02:00:41.560 --> 02:00:42.920]   that would go to Berkeley for grad school
[02:00:42.920 --> 02:00:45.080]   that I knew that were Bezos' classmates.
[02:00:45.080 --> 02:00:46.000]   So I know all these guys,
[02:00:46.000 --> 02:00:48.120]   and I know all these Bezos stories.
[02:00:48.120 --> 02:00:51.040]   The funny thing is the guy you're talking about,
[02:00:51.040 --> 02:00:53.200]   whose name I believe is Yasanta,
[02:00:53.200 --> 02:00:55.680]   is an Indian guy, Sri Lankan guy.
[02:00:55.680 --> 02:00:58.320]   He went to grad school at Caltech.
[02:00:58.320 --> 02:01:00.480]   And so actually he and I,
[02:01:00.480 --> 02:01:02.920]   I don't remember how, I looked him up at one point.
[02:01:02.920 --> 02:01:04.240]   I think we met up at Caltech
[02:01:04.240 --> 02:01:05.600]   when I was visiting at one point,
[02:01:05.600 --> 02:01:06.680]   and he was in grad school.
[02:01:06.680 --> 02:01:08.040]   And so I actually met this guy
[02:01:08.040 --> 02:01:10.120]   and talked to him about Bezos,
[02:01:10.120 --> 02:01:11.280]   'cause we had friends and other,
[02:01:11.280 --> 02:01:12.880]   we weren't focused on Bezos
[02:01:12.880 --> 02:01:14.080]   since we had other friends in common,
[02:01:14.080 --> 02:01:15.120]   but I actually met this guy
[02:01:15.120 --> 02:01:17.280]   that is in that anecdote that you just mentioned.
[02:01:17.280 --> 02:01:19.880]   - Oh, no, 'cause actually that's really good to know,
[02:01:19.880 --> 02:01:22.560]   because that is relevant to my question.
[02:01:22.560 --> 02:01:26.680]   My friend and I have this continual debate
[02:01:26.680 --> 02:01:29.320]   about the importance of intelligence
[02:01:29.320 --> 02:01:34.320]   at the peaks of entrepreneurial ability
[02:01:34.320 --> 02:01:36.240]   or engineering ability.
[02:01:36.240 --> 02:01:39.400]   And he tries to use that anecdote to say that,
[02:01:39.400 --> 02:01:41.760]   "Oh, clearly Bezos was not smart enough
[02:01:41.760 --> 02:01:42.800]   "to be a theoretical physicist,
[02:01:42.800 --> 02:01:45.960]   "so therefore intelligence is not that important
[02:01:45.960 --> 02:01:48.920]   "beyond a certain, not especially high point,
[02:01:48.920 --> 02:01:49.760]   "and afterwards, as such,
[02:01:49.760 --> 02:01:51.520]   "Bezos was creative," or blah, blah, blah.
[02:01:51.520 --> 02:01:52.640]   He was hardworking.
[02:01:52.640 --> 02:01:56.000]   And I don't know, my perception of the story was like,
[02:01:56.000 --> 02:01:57.920]   "Okay, he's not smart enough to be a theoretical physicist.
[02:01:57.920 --> 02:01:59.600]   "He's below five standard deviations
[02:01:59.600 --> 02:02:01.600]   "or four standard deviations."
[02:02:01.600 --> 02:02:05.320]   But clearly, just studying physics at Princeton
[02:02:05.320 --> 02:02:07.120]   is itself a testament that he's probably
[02:02:07.120 --> 02:02:08.920]   at least two or three standard deviations above,
[02:02:08.920 --> 02:02:11.000]   well, at least three, I mean.
[02:02:11.000 --> 02:02:13.120]   But okay, so can you tell me more
[02:02:13.120 --> 02:02:15.600]   about what was the perception of those people
[02:02:15.600 --> 02:02:19.160]   you talked to at Princeton about Jeff Bezos?
[02:02:19.160 --> 02:02:21.880]   Is it that he just was super high
[02:02:21.880 --> 02:02:23.640]   in other trades like hardworking or creative,
[02:02:23.640 --> 02:02:25.280]   or it's actually intelligence was super high,
[02:02:25.280 --> 02:02:27.680]   just not high enough to be a theoretical physicist?
[02:02:27.680 --> 02:02:30.480]   - Yeah, this is a great topic that,
[02:02:30.480 --> 02:02:33.680]   I think a lot of people are interested in this topic,
[02:02:33.680 --> 02:02:34.960]   and even among my close friends,
[02:02:34.960 --> 02:02:37.280]   including these friends who know Bezos,
[02:02:37.280 --> 02:02:39.320]   or knew Bezos in school,
[02:02:39.320 --> 02:02:40.520]   we all talk about this kind of stuff.
[02:02:40.520 --> 02:02:42.680]   So first of all, you gotta make a distinction
[02:02:42.680 --> 02:02:45.840]   between the very abstract kind of intelligence,
[02:02:45.840 --> 02:02:47.720]   which is useful in physics and math,
[02:02:47.720 --> 02:02:49.320]   or maybe computer science,
[02:02:49.320 --> 02:02:53.240]   versus a more kind of generalist intelligence.
[02:02:53.240 --> 02:02:55.480]   And those are correlated, but they're not the same thing.
[02:02:55.480 --> 02:02:59.880]   And so, I would say Bezos is probably very off-scale
[02:02:59.880 --> 02:03:03.720]   for ability to work hard, take risk,
[02:03:03.720 --> 02:03:05.880]   function under pressure, be focused,
[02:03:05.880 --> 02:03:07.400]   and generalist intelligence.
[02:03:07.400 --> 02:03:09.520]   So he's just probably off-scale on.
[02:03:09.520 --> 02:03:11.640]   If you're just, since these traits
[02:03:11.640 --> 02:03:13.680]   are at least somewhat uncorrelated,
[02:03:13.680 --> 02:03:17.360]   if you're top 10% in each of these five simultaneously,
[02:03:17.360 --> 02:03:18.920]   you're already a pretty rare individual, right?
[02:03:18.920 --> 02:03:20.600]   'Cause plenty of the physics guys
[02:03:20.600 --> 02:03:23.280]   who did better than Bezos in the physics classes,
[02:03:23.280 --> 02:03:24.840]   they could not lead a company.
[02:03:24.840 --> 02:03:26.840]   They could not put together a presentation
[02:03:26.840 --> 02:03:29.040]   that would convince a venture capitalist to invest.
[02:03:29.040 --> 02:03:33.000]   So it's sort of different skill sets
[02:03:33.000 --> 02:03:34.080]   that we're talking about.
[02:03:34.080 --> 02:03:38.160]   I think the idea that there's a unidimensional measure
[02:03:38.160 --> 02:03:42.080]   of cognitive ability is just not that useful.
[02:03:42.080 --> 02:03:43.240]   You know, I'm probably guilty.
[02:03:43.240 --> 02:03:44.880]   People would say, wait, Steve Hsu just said that,
[02:03:44.880 --> 02:03:46.320]   but he's the guy most responsible
[02:03:46.320 --> 02:03:48.440]   for promulgating this perspective.
[02:03:48.440 --> 02:03:50.880]   But it's only because it's the simplest thing to talk about,
[02:03:50.880 --> 02:03:53.280]   is if you compress it to one general factor,
[02:03:53.280 --> 02:03:54.360]   it's just easier to talk about.
[02:03:54.360 --> 02:03:56.520]   It doesn't mean that the other components
[02:03:56.520 --> 02:03:57.360]   are not meaningful.
[02:03:57.360 --> 02:03:59.640]   We just got done talking about verbal versus spatial,
[02:03:59.640 --> 02:04:03.200]   versus some more generalized mathematical talent.
[02:04:03.200 --> 02:04:05.760]   So obviously it's a much, it's a high dimensional,
[02:04:05.760 --> 02:04:06.800]   not that high dimensional,
[02:04:06.800 --> 02:04:08.720]   but it's at least a multidimensional space
[02:04:08.720 --> 02:04:10.640]   of abilities that we're talking about.
[02:04:10.640 --> 02:04:13.960]   Now, the point about Bezos,
[02:04:13.960 --> 02:04:15.240]   I think, which is non-trivial though,
[02:04:15.240 --> 02:04:17.320]   which I think is directly relevant
[02:04:17.320 --> 02:04:20.440]   to like the life experiences of like physicists
[02:04:20.440 --> 02:04:22.280]   who leave physics and do other stuff,
[02:04:22.280 --> 02:04:26.400]   is that very often in an engineering setting
[02:04:26.400 --> 02:04:28.280]   or a startup setting, people will be like,
[02:04:28.280 --> 02:04:29.280]   you don't know shit about that.
[02:04:29.280 --> 02:04:31.640]   What are you talking about, right?
[02:04:31.640 --> 02:04:34.920]   But the reality is people who do perform,
[02:04:34.920 --> 02:04:36.240]   I'm on a technical problem, okay?
[02:04:36.240 --> 02:04:40.040]   Not about what's the right way to get a good,
[02:04:40.040 --> 02:04:42.600]   a warm intro to this VC, not something like that,
[02:04:42.600 --> 02:04:45.760]   but some technical problem that the startup has to solve.
[02:04:45.760 --> 02:04:49.280]   Like in Bezos' case, it was often like optimization
[02:04:49.280 --> 02:04:50.480]   of some supply chain thing,
[02:04:50.480 --> 02:04:52.480]   or optimization of some sorting process,
[02:04:52.480 --> 02:04:53.760]   or reducing the error rate,
[02:04:53.760 --> 02:04:55.800]   and some like, you know, address labeling.
[02:04:55.800 --> 02:04:57.880]   You know, it was a very well-defined thing
[02:04:57.880 --> 02:05:00.360]   once you, operations problem.
[02:05:00.360 --> 02:05:02.960]   And the people in the company uniformly say,
[02:05:02.960 --> 02:05:04.800]   like when Bezos comes in the room,
[02:05:04.800 --> 02:05:08.040]   he will give us very good feedback
[02:05:08.040 --> 02:05:11.360]   on the solution to this ops problem
[02:05:11.360 --> 02:05:12.800]   that, you know, it could be out of the blue
[02:05:12.800 --> 02:05:14.200]   better than what we said,
[02:05:14.200 --> 02:05:16.480]   or at least he finds the problems with what we said,
[02:05:16.480 --> 02:05:19.440]   or if we did a good job on it, he gets it right away,
[02:05:19.440 --> 02:05:22.160]   which is some executives might not get it right away.
[02:05:22.160 --> 02:05:26.760]   So my point is that people who have these super high,
[02:05:27.760 --> 02:05:31.760]   just raw G abilities, they generally can be useful
[02:05:31.760 --> 02:05:33.440]   in these technological environments,
[02:05:33.440 --> 02:05:35.000]   even if they don't have a lot of background.
[02:05:35.000 --> 02:05:36.480]   Like they can still come in and be helpful,
[02:05:36.480 --> 02:05:37.760]   and sometimes they can solve problems
[02:05:37.760 --> 02:05:41.120]   that the people who are well-trained in the area
[02:05:41.120 --> 02:05:42.160]   are having trouble with.
[02:05:42.160 --> 02:05:43.440]   I think that is fair.
[02:05:43.440 --> 02:05:46.600]   But it's not fair to say there's just some,
[02:05:46.600 --> 02:05:49.480]   you know, unidimensional measure of intelligence,
[02:05:49.480 --> 02:05:51.200]   and this guy always beats this guy,
[02:05:51.200 --> 02:05:53.680]   and this guy always beats, it doesn't work like that.
[02:05:53.680 --> 02:05:56.080]   But it's just that some of these off-scale guys
[02:05:56.080 --> 02:05:57.520]   are just generally more useful
[02:05:57.520 --> 02:06:00.040]   than the critics would like to give them credit for.
[02:06:00.040 --> 02:06:02.680]   - Your life story is kind of an example of that.
[02:06:02.680 --> 02:06:05.480]   But, you know, I had another experience of this,
[02:06:05.480 --> 02:06:08.680]   which was, I recently interviewed Sam Becker-Fried,
[02:06:08.680 --> 02:06:11.440]   who is the CEO of FTX, on my podcast.
[02:06:11.440 --> 02:06:14.920]   And one interesting, like, for that interview,
[02:06:14.920 --> 02:06:16.720]   and I guess generally for all interviews,
[02:06:16.720 --> 02:06:19.480]   I try to come up with questions that I think
[02:06:19.480 --> 02:06:23.560]   that the guest has probably not heard before.
[02:06:23.560 --> 02:06:25.040]   And in that one, in that case,
[02:06:25.040 --> 02:06:26.480]   I tried really hard to come up with questions
[02:06:26.480 --> 02:06:27.440]   that he might not have heard before,
[02:06:27.440 --> 02:06:28.280]   and that might've been, like,
[02:06:28.280 --> 02:06:30.600]   really interesting and challenging.
[02:06:30.600 --> 02:06:32.840]   You know, I listened to all the interviews he's ever done,
[02:06:32.840 --> 02:06:35.040]   and then, yeah, I prepped for a long time.
[02:06:35.040 --> 02:06:37.560]   And if you listen to that interview,
[02:06:37.560 --> 02:06:39.280]   the thing you'll notice is,
[02:06:39.280 --> 02:06:40.520]   the way he answers these questions,
[02:06:40.520 --> 02:06:41.600]   it, like, sounds like he was just,
[02:06:41.600 --> 02:06:43.160]   "Oh, I was just talking to somebody about that.
[02:06:43.160 --> 02:06:45.080]   "Let me just say again what I was just thinking."
[02:06:45.080 --> 02:06:48.000]   It's, like, no matter how creative a question
[02:06:48.000 --> 02:06:49.280]   I could try to throw at him,
[02:06:49.280 --> 02:06:51.560]   it's just his ability to grok,
[02:06:53.200 --> 02:06:56.440]   like, all the context explained in the most,
[02:06:56.440 --> 02:06:59.000]   in the way that an audience would understand.
[02:06:59.000 --> 02:07:00.360]   It was kind of exceptional.
[02:07:00.360 --> 02:07:06.400]   - Being a super successful founder
[02:07:06.400 --> 02:07:11.080]   selects for the ability to figure out,
[02:07:11.080 --> 02:07:13.880]   "Okay, this guy's, this investor is from private equity.
[02:07:13.880 --> 02:07:15.280]   "This is how he's gonna think about the problem,
[02:07:15.280 --> 02:07:16.680]   "and this is how I should explain it to him.
[02:07:16.680 --> 02:07:19.080]   "This guy's from a very tech-heavy venture fund.
[02:07:19.080 --> 02:07:21.120]   "This is how I gotta talk to him about the problem.
[02:07:21.120 --> 02:07:23.080]   "This is the due diligence guy that they sent me,
[02:07:23.080 --> 02:07:24.960]   "and he's a computer science professor at Stanford.
[02:07:24.960 --> 02:07:27.200]   "I gotta talk to him in this language."
[02:07:27.200 --> 02:07:30.240]   So founders are very selected population
[02:07:30.240 --> 02:07:32.840]   for being very good multi-band communicators
[02:07:32.840 --> 02:07:35.000]   across different cultures and stuff like this.
[02:07:35.000 --> 02:07:37.880]   This is a dumb investment banker from Goldman,
[02:07:37.880 --> 02:07:39.320]   and, I mean, they're not dumb,
[02:07:39.320 --> 02:07:40.440]   but he's not technical at all,
[02:07:40.440 --> 02:07:42.160]   and so I gotta explain it to him this way.
[02:07:42.160 --> 02:07:43.000]   But this guy's a lawyer.
[02:07:43.000 --> 02:07:44.000]   I gotta talk to him that way.
[02:07:44.000 --> 02:07:46.240]   So it's not surprising to me that,
[02:07:46.240 --> 02:07:50.480]   this guy would have those capabilities.
[02:07:50.480 --> 02:07:53.640]   It's not, it's selected for in that population.
[02:07:53.640 --> 02:07:54.760]   - Right, yeah.
[02:07:54.760 --> 02:07:58.360]   Okay, so you are a practitioner of jujitsu
[02:07:58.360 --> 02:08:03.240]   and other martial arts, and one of the things in (laughs)
[02:08:03.240 --> 02:08:06.840]   one, you know, obviously one notable aspect
[02:08:06.840 --> 02:08:10.040]   of those disciplines is that you can really punch
[02:08:10.040 --> 02:08:11.040]   above your weight, right?
[02:08:11.040 --> 02:08:14.240]   And that, you know, Royce Gracie in the UFC
[02:08:14.240 --> 02:08:15.600]   is a great example of this.
[02:08:18.160 --> 02:08:21.080]   Is that possible with a trait like intelligence?
[02:08:21.080 --> 02:08:22.600]   Is it possible that we have techniques
[02:08:22.600 --> 02:08:26.440]   or other ways of compensating for your,
[02:08:26.440 --> 02:08:29.280]   I guess analogously, your just raw weight
[02:08:29.280 --> 02:08:34.280]   that what jujitsu is to fighting?
[02:08:34.280 --> 02:08:37.360]   - That's a great question.
[02:08:37.360 --> 02:08:41.840]   So in a way, jujitsu is like applied physics
[02:08:41.840 --> 02:08:43.640]   because you're thinking about like,
[02:08:43.640 --> 02:08:45.760]   I have two arms, you have two arms.
[02:08:47.360 --> 02:08:50.240]   Is it easier for you to punch me and knock me out
[02:08:50.240 --> 02:08:51.600]   before I can close the distance
[02:08:51.600 --> 02:08:53.480]   and force you to grapple with me?
[02:08:53.480 --> 02:08:56.160]   It really is, the reason I like jujitsu so much
[02:08:56.160 --> 02:08:58.120]   is because it's very rational.
[02:08:58.120 --> 02:09:00.640]   It's basically scientific analysis
[02:09:00.640 --> 02:09:03.320]   of what two humans can do to each other.
[02:09:03.320 --> 02:09:04.840]   And so it's a technology.
[02:09:04.840 --> 02:09:08.920]   And in terms of what technologies people can use
[02:09:08.920 --> 02:09:12.480]   like to amplify their brain power,
[02:09:12.480 --> 02:09:13.840]   obviously we're surrounded by it.
[02:09:13.840 --> 02:09:16.040]   So like, here's an interesting thing.
[02:09:16.040 --> 02:09:19.240]   Suppose you and your girlfriend
[02:09:19.240 --> 02:09:21.920]   are trying to like get the answer to some question
[02:09:21.920 --> 02:09:23.800]   and you're both using Google.
[02:09:23.800 --> 02:09:25.400]   There's an enormous variance
[02:09:25.400 --> 02:09:28.200]   in who immediately puts the search term in
[02:09:28.200 --> 02:09:29.120]   that gets the right answer.
[02:09:29.120 --> 02:09:30.640]   Like the top hit is the answer,
[02:09:30.640 --> 02:09:31.920]   direct answer to your question.
[02:09:31.920 --> 02:09:34.880]   And that's very G, that's very G loaded,
[02:09:34.880 --> 02:09:39.880]   but you could use technologies to improve yourself.
[02:09:39.880 --> 02:09:42.200]   If you train, training is not the right word,
[02:09:42.200 --> 02:09:44.680]   if you kind of get good at using certain technologies
[02:09:44.680 --> 02:09:48.360]   or certain information channels,
[02:09:48.360 --> 02:09:50.200]   you can amplify your ability
[02:09:50.200 --> 02:09:54.000]   beyond just what the raw capability.
[02:09:54.000 --> 02:09:57.200]   So I think my answer is that there are tools,
[02:09:57.200 --> 02:10:02.120]   but there's nobody who like,
[02:10:02.120 --> 02:10:03.640]   there's no dojo where you can go
[02:10:03.640 --> 02:10:05.520]   and Henzo Gracie just like starts teaching you
[02:10:05.520 --> 02:10:07.240]   immediately like this, do this, this, this, and this.
[02:10:07.240 --> 02:10:08.720]   And then you're gonna, the guy is bigger than you,
[02:10:08.720 --> 02:10:10.960]   but you're gonna take him down and choke him out.
[02:10:10.960 --> 02:10:14.320]   There isn't something like that for cognition.
[02:10:14.320 --> 02:10:15.280]   - Right.
[02:10:15.280 --> 02:10:17.760]   - But I can see like people can amplify
[02:10:17.760 --> 02:10:19.400]   their capabilities in different,
[02:10:19.400 --> 02:10:21.760]   either more or less effectively.
[02:10:21.760 --> 02:10:23.480]   - Now you had a blog post a long time ago
[02:10:23.480 --> 02:10:24.880]   about elite education.
[02:10:24.880 --> 02:10:27.320]   And in it, you talk about how,
[02:10:27.320 --> 02:10:29.080]   even if you control for SAT,
[02:10:29.080 --> 02:10:31.920]   at the very top jobs,
[02:10:31.920 --> 02:10:35.160]   the people from elite schools are overrepresented.
[02:10:35.160 --> 02:10:39.400]   And so I'm curious, do you think this is because
[02:10:39.400 --> 02:10:44.240]   of a selection effect based on like Harvard selecting
[02:10:44.240 --> 02:10:45.760]   based on personality as well?
[02:10:45.760 --> 02:10:49.320]   And that selects for a certain high achievers,
[02:10:49.320 --> 02:10:51.440]   or is it something about being at Harvard
[02:10:51.440 --> 02:10:53.040]   that makes you a high achiever?
[02:10:53.040 --> 02:10:54.040]   What is going on?
[02:10:54.040 --> 02:11:01.320]   - So first of all, I researched this question
[02:11:01.320 --> 02:11:03.840]   pretty aggressively when I was first,
[02:11:03.840 --> 02:11:06.200]   when I first became an entrepreneur,
[02:11:06.200 --> 02:11:09.520]   because I was like, well, we can raise this much money.
[02:11:09.520 --> 02:11:11.480]   We can get these meetings with these funds,
[02:11:11.480 --> 02:11:13.840]   but how the hell did this guy get,
[02:11:13.840 --> 02:11:15.760]   raise a hundred million for this stupid idea?
[02:11:15.760 --> 02:11:16.920]   Like what the hell?
[02:11:16.920 --> 02:11:19.200]   And then so I would start looking into this guy's background.
[02:11:19.200 --> 02:11:20.720]   I'd be like, well, he went to Harvard
[02:11:20.720 --> 02:11:23.880]   and oh, he was in Skull and Bone, you know, whatever.
[02:11:23.880 --> 02:11:26.640]   So I got intensely interested in like,
[02:11:26.640 --> 02:11:28.120]   okay, these super outlier guys,
[02:11:28.120 --> 02:11:30.680]   like how did this guy get a job writing for the Simpsons?
[02:11:30.680 --> 02:11:32.720]   You know, like what I would like to write,
[02:11:32.720 --> 02:11:34.000]   you know, this other guy would like to write
[02:11:34.000 --> 02:11:35.600]   for the Simpsons, but he went to Ohio State.
[02:11:35.600 --> 02:11:37.920]   So he's like 10 layers of social networking
[02:11:37.920 --> 02:11:38.840]   away from the Simpsons.
[02:11:38.840 --> 02:11:40.400]   But the Harvard guy's not actually,
[02:11:40.400 --> 02:11:41.360]   his buddy's at the Crimson,
[02:11:41.360 --> 02:11:44.920]   I'll write for the, you know, the Simpsons, right?
[02:11:44.920 --> 02:11:48.720]   So there are multiple factors why,
[02:11:48.720 --> 02:11:52.240]   take two kids, they both scored 1580 on the SAT.
[02:11:52.240 --> 02:11:56.000]   One goes to Ohio State on the Ohio Regents Scholarship
[02:11:56.000 --> 02:11:57.400]   for engineering.
[02:11:57.400 --> 02:11:58.720]   And the other one says, no, fuck no,
[02:11:58.720 --> 02:11:59.560]   I'm gonna go to Harvard,
[02:11:59.560 --> 02:12:01.160]   even though the engineering school there sucks,
[02:12:01.160 --> 02:12:03.080]   but I'm gonna go to Harvard instead, okay?
[02:12:03.080 --> 02:12:05.200]   So what's the difference in their lives?
[02:12:05.200 --> 02:12:07.640]   One, somehow, maybe the guy went to Harvard
[02:12:07.640 --> 02:12:09.200]   'cause he kinda understands how the world works
[02:12:09.200 --> 02:12:11.560]   a little better than the other dude, okay?
[02:12:11.560 --> 02:12:13.040]   Two, when he gets to Harvard,
[02:12:13.040 --> 02:12:16.760]   he's gonna meet a lot of super ambitious,
[02:12:16.760 --> 02:12:19.400]   aggressive, smart kids.
[02:12:19.400 --> 02:12:22.520]   Some of those kids are children of super wealthy people.
[02:12:22.520 --> 02:12:26.000]   Some of them are children of super influential people.
[02:12:26.000 --> 02:12:28.320]   And all of them are trying to get ahead.
[02:12:28.320 --> 02:12:29.640]   They're super ambitious.
[02:12:29.640 --> 02:12:32.000]   They know what it means to like make managing director
[02:12:32.000 --> 02:12:33.720]   at Goldman or become a partner at McKinsey.
[02:12:33.720 --> 02:12:35.080]   They know what those things are.
[02:12:35.080 --> 02:12:36.480]   Okay, and if you didn't know them
[02:12:36.480 --> 02:12:37.760]   'cause you grew up in Ohio,
[02:12:37.760 --> 02:12:39.280]   you learn them right away
[02:12:39.280 --> 02:12:42.960]   'cause you see what Joe, who was two years ahead of me,
[02:12:42.960 --> 02:12:44.400]   but had the room across the hall,
[02:12:44.400 --> 02:12:46.200]   he interviewed, now he's at McKinsey,
[02:12:46.200 --> 02:12:47.800]   now he's doing this.
[02:12:47.800 --> 02:12:49.840]   You just get a better view of what's possible
[02:12:49.840 --> 02:12:53.240]   in the elite sector of society from that exposure.
[02:12:53.240 --> 02:12:54.640]   So there are multiple factors.
[02:12:54.640 --> 02:12:56.920]   Networking, some of these Harvard kids
[02:12:56.920 --> 02:12:58.720]   come from super wealthy families.
[02:12:58.720 --> 02:13:00.760]   Some of them, their dad used to play golf
[02:13:00.800 --> 02:13:05.800]   with the head of the fund
[02:13:05.800 --> 02:13:08.240]   that he's trying to get a meeting with, right?
[02:13:08.240 --> 02:13:12.640]   So it's all those things together.
[02:13:12.640 --> 02:13:13.640]   I'm not saying it's good,
[02:13:13.640 --> 02:13:15.800]   but I kind of want to understand how the world works.
[02:13:15.800 --> 02:13:17.600]   I kind of understand why this other dude
[02:13:17.600 --> 02:13:19.280]   can raise so much more money than I can raise
[02:13:19.280 --> 02:13:21.080]   or get meetings that I can't get, right?
[02:13:21.080 --> 02:13:24.240]   So that's how I was initially interested in this question.
[02:13:24.240 --> 02:13:29.320]   - Why are China and India underrepresented
[02:13:29.320 --> 02:13:31.520]   in, like massively underrepresented
[02:13:31.520 --> 02:13:33.680]   in Nobel Prizes per capita?
[02:13:33.680 --> 02:13:37.160]   And even in computer science,
[02:13:37.160 --> 02:13:40.200]   when I would try to find papers on certain subjects,
[02:13:40.200 --> 02:13:43.640]   often those papers, it was rare that they would come
[02:13:43.640 --> 02:13:45.080]   from China or something like that.
[02:13:45.080 --> 02:13:48.400]   And when they did, it was just the quality was much worse
[02:13:48.400 --> 02:13:49.800]   than the ones that I could find
[02:13:49.800 --> 02:13:52.080]   from a professor in the US.
[02:13:52.080 --> 02:13:54.200]   And I'm curious why you think that is.
[02:13:54.200 --> 02:13:58.240]   So obviously, it's clear that it can't just be
[02:13:58.240 --> 02:13:59.800]   the population or anything like that,
[02:13:59.800 --> 02:14:02.200]   because when those researchers come to the US,
[02:14:02.200 --> 02:14:04.400]   they're producing stellar research.
[02:14:04.400 --> 02:14:05.440]   What is happening here?
[02:14:05.440 --> 02:14:08.200]   That like, why is this effect real?
[02:14:08.200 --> 02:14:10.120]   And if so, what is the explanation?
[02:14:10.120 --> 02:14:12.600]   - Well, the easy answer to that question
[02:14:12.600 --> 02:14:14.800]   is many of the things or almost all the things
[02:14:14.800 --> 02:14:17.280]   you mentioned are lagging indicators.
[02:14:17.280 --> 02:14:20.520]   So they reflect the fact that the West was developed
[02:14:20.520 --> 02:14:24.960]   and had a strong scientific and engineering tradition
[02:14:24.960 --> 02:14:28.200]   when China and India were desperately poor.
[02:14:28.200 --> 02:14:30.480]   And just didn't have any of that.
[02:14:30.480 --> 02:14:33.200]   And in my own life, I went in the last 20 years
[02:14:33.200 --> 02:14:36.560]   from when I would visit a university in China
[02:14:36.560 --> 02:14:38.920]   or even like South Korea and Taiwan,
[02:14:38.920 --> 02:14:40.760]   I could see them go from,
[02:14:40.760 --> 02:14:43.960]   they had plenty of talented undergraduates,
[02:14:43.960 --> 02:14:45.200]   but the best of those undergraduates
[02:14:45.200 --> 02:14:47.640]   always wanna come to the US for their PhD.
[02:14:47.640 --> 02:14:50.760]   They went from that to now some
[02:14:50.760 --> 02:14:53.120]   of the best undergraduates stay there.
[02:14:53.120 --> 02:14:55.560]   And the researchers who are professors there
[02:14:55.560 --> 02:14:57.480]   are becoming world-class.
[02:14:57.480 --> 02:15:00.840]   But that happened only in my adult lifetime.
[02:15:00.840 --> 02:15:04.440]   So you can see it's a heavily lagging indicator.
[02:15:04.440 --> 02:15:07.040]   Interestingly, like in my physics career,
[02:15:07.040 --> 02:15:10.320]   I knew several, I think the Indian term is called toppers.
[02:15:10.320 --> 02:15:11.720]   I don't know if you know this term toppers.
[02:15:11.720 --> 02:15:13.880]   So the people who take the IIT exams,
[02:15:13.880 --> 02:15:15.760]   they literally rank every kid in the country
[02:15:15.760 --> 02:15:16.800]   who takes the exam, right?
[02:15:16.800 --> 02:15:20.080]   So I knew guys who were number one or number two
[02:15:20.080 --> 02:15:25.080]   or number five on the IIT entrance exam.
[02:15:25.080 --> 02:15:26.720]   But they ended up going to Caltech
[02:15:26.720 --> 02:15:28.480]   or they ended up going to MIT or, you know.
[02:15:28.480 --> 02:15:31.840]   So there's this huge brain drain.
[02:15:31.840 --> 02:15:34.320]   I mean, it's super powerful, elite brain drain.
[02:15:34.320 --> 02:15:38.320]   And MIT recently has just been recruiting.
[02:15:38.320 --> 02:15:40.000]   If you win one of these Olympiads,
[02:15:40.000 --> 02:15:41.840]   you get a gold medal in the Informatics Olympiad
[02:15:41.840 --> 02:15:42.680]   or the Math Olympiad.
[02:15:42.680 --> 02:15:44.360]   MIT will try to get you to come to MIT.
[02:15:44.360 --> 02:15:46.840]   So there's this huge sucking of talent
[02:15:46.840 --> 02:15:49.240]   into the United States, which is great, I think.
[02:15:49.240 --> 02:15:52.800]   But that's why when you go to IIT,
[02:15:52.800 --> 02:15:55.480]   even though the undergraduates are super smart,
[02:15:55.480 --> 02:15:56.560]   the professors are actually,
[02:15:56.560 --> 02:15:58.120]   no offense to my colleagues who teach there,
[02:15:58.120 --> 02:16:00.640]   but the professors there would generally,
[02:16:00.640 --> 02:16:03.040]   if they get a bid from UCLA,
[02:16:03.040 --> 02:16:05.760]   they'll move to UCLA on average.
[02:16:05.760 --> 02:16:07.280]   So that's the difference.
[02:16:07.280 --> 02:16:09.080]   But that's gradually evening out.
[02:16:09.080 --> 02:16:11.200]   - Are there any downsides to the fact
[02:16:11.200 --> 02:16:16.200]   that we can pay researchers or postdocs in the US less
[02:16:16.200 --> 02:16:22.000]   because we're partially paying foreign workers in visas?
[02:16:22.000 --> 02:16:25.240]   Is that just a market arbitrage that has, you know,
[02:16:25.240 --> 02:16:28.000]   that's just like positive externalities for the economy?
[02:16:28.000 --> 02:16:30.440]   Or is there some downside to the fact
[02:16:30.440 --> 02:16:33.640]   that it's not competitive for native born workers?
[02:16:33.640 --> 02:16:37.280]   - Good for the US overall on average.
[02:16:37.280 --> 02:16:38.960]   Bad for developing countries
[02:16:38.960 --> 02:16:40.800]   'cause you're stealing their talent.
[02:16:40.800 --> 02:16:45.520]   Bad for native born Americans who have to compete
[02:16:45.520 --> 02:16:48.480]   against the best brains from all over the world.
[02:16:48.480 --> 02:16:51.480]   So much harder for an American kid to, you know,
[02:16:51.480 --> 02:16:53.520]   get the job he deserves, you know,
[02:16:53.520 --> 02:16:55.480]   at these elite levels where he's strongly impacted
[02:16:55.480 --> 02:16:56.400]   by immigration.
[02:16:56.400 --> 02:16:59.760]   So, you know, you got winners and losers.
[02:16:59.760 --> 02:17:06.120]   Whether there's a long-term problem for America.
[02:17:06.120 --> 02:17:09.680]   So now like there's some guys who are super obsessed
[02:17:09.680 --> 02:17:11.400]   who like comment on my blog every now and then
[02:17:11.400 --> 02:17:14.840]   who study like where are all the IMO,
[02:17:14.840 --> 02:17:17.800]   you know, international math Olympiad winners going?
[02:17:17.800 --> 02:17:18.640]   Where are they?
[02:17:18.640 --> 02:17:19.520]   What, you know?
[02:17:19.520 --> 02:17:22.080]   And they claim they're seeing this huge drop-off
[02:17:22.080 --> 02:17:25.840]   in like kids who grew up in America
[02:17:25.840 --> 02:17:28.760]   who are not like first, like children of immigrants,
[02:17:28.760 --> 02:17:30.680]   but they've been here a while.
[02:17:30.680 --> 02:17:33.080]   They just never win these competitions anymore.
[02:17:33.080 --> 02:17:37.760]   So ultimately you might be kind of discouraging
[02:17:37.760 --> 02:17:42.640]   the native talent pool by just letting the door,
[02:17:42.640 --> 02:17:43.880]   opening the door and bringing in
[02:17:43.880 --> 02:17:46.000]   like all these super talented people from outside.
[02:17:46.000 --> 02:17:47.720]   So there could be some second order effects
[02:17:47.720 --> 02:17:48.560]   that aren't so good.
[02:17:48.560 --> 02:17:49.520]   - Although it's interesting
[02:17:49.520 --> 02:17:51.960]   when you look at an industry like tech,
[02:17:51.960 --> 02:17:55.560]   where there's a similar aspect of foreign competition
[02:17:55.560 --> 02:17:57.960]   being allowed in because of H1B visas,
[02:17:57.960 --> 02:18:02.320]   but the compensation has remained really competitive.
[02:18:02.320 --> 02:18:04.000]   Is it just because tech is a super,
[02:18:04.000 --> 02:18:09.000]   it's like super inelastic demand for the talent?
[02:18:09.000 --> 02:18:12.680]   - Yeah, because you're maybe a little more focused
[02:18:12.680 --> 02:18:15.200]   on things like software development and ML and stuff.
[02:18:15.200 --> 02:18:17.000]   But if you look at like more kind of
[02:18:17.000 --> 02:18:20.000]   traditional engineering fields, which aren't as hot,
[02:18:20.000 --> 02:18:22.880]   probably those guys like an engineer at Boeing
[02:18:22.880 --> 02:18:24.560]   or those guys would probably say like,
[02:18:24.560 --> 02:18:27.480]   "No, my fucking salary is heavily suppressed
[02:18:27.480 --> 02:18:31.160]   by the existence of hungry engineers from India and China
[02:18:31.160 --> 02:18:32.000]   and stuff like that."
[02:18:32.000 --> 02:18:36.920]   So software, because it's been so hot for so long,
[02:18:36.920 --> 02:18:38.640]   doesn't feel this effect so much.
[02:18:38.640 --> 02:18:41.240]   It's got plenty of elasticity.
[02:18:41.240 --> 02:18:42.160]   - Awesome, okay.
[02:18:42.160 --> 02:18:43.600]   Steve, this is so much fun.
[02:18:43.600 --> 02:18:46.680]   I really, really enjoyed this conversation
[02:18:46.680 --> 02:18:48.600]   and in preparing for it and in talking to you,
[02:18:48.600 --> 02:18:50.840]   I like really got to learn a lot more about this subject
[02:18:50.840 --> 02:18:53.440]   that I was interested in for a long amount of time.
[02:18:53.440 --> 02:18:56.120]   Is there anything else that we should touch upon
[02:18:56.120 --> 02:18:57.760]   on any of the subjects we've covered today
[02:18:57.760 --> 02:18:59.400]   or have failed to cover today?
[02:18:59.400 --> 02:19:00.600]   - Wow, we've covered so much
[02:19:00.600 --> 02:19:03.640]   and I just, I really think you're a great interviewer
[02:19:03.640 --> 02:19:05.960]   because your questions are like always getting at
[02:19:05.960 --> 02:19:09.320]   a key thing that I think a lot of people are confused about
[02:19:09.320 --> 02:19:11.240]   and there's a lot of depth there.
[02:19:11.240 --> 02:19:12.720]   So I thought it was great.
[02:19:12.720 --> 02:19:15.080]   There's plenty more that we could talk about.
[02:19:15.080 --> 02:19:16.760]   We should just get together and do this some other time,
[02:19:16.760 --> 02:19:19.120]   but I don't think you've left anything out.
[02:19:19.120 --> 02:19:21.880]   - If you're willing, I would love to do a version two
[02:19:21.880 --> 02:19:24.200]   of this where we talk some about your physics work
[02:19:24.200 --> 02:19:26.240]   and yeah, the other subjects we might've missed
[02:19:26.240 --> 02:19:27.400]   this time around.
[02:19:27.400 --> 02:19:29.640]   - Yeah, we got to talk about many worlds
[02:19:29.640 --> 02:19:30.880]   and quantum computers.
[02:19:30.880 --> 02:19:31.800]   - Yeah, yeah.
[02:19:31.800 --> 02:19:32.640]   This will be fun.
[02:19:32.640 --> 02:19:38.680]   In the meantime, do you want to give people your website,
[02:19:38.680 --> 02:19:40.760]   your podcast and your Twitter
[02:19:40.760 --> 02:19:42.320]   so they know where to find you?
[02:19:43.800 --> 02:19:47.800]   - Yeah, so my, well, my last name is HSU.
[02:19:47.800 --> 02:19:49.200]   That's like the hardest thing for people
[02:19:49.200 --> 02:19:53.400]   'cause it's kind of anti-phonetic, H then S then U.
[02:19:53.400 --> 02:19:56.000]   And just search for me.
[02:19:56.000 --> 02:19:58.240]   I'm on Twitter, I have a blog
[02:19:58.240 --> 02:20:00.200]   and I have a podcast called Manifold,
[02:20:00.200 --> 02:20:02.920]   which doesn't have a huge listenership,
[02:20:02.920 --> 02:20:04.920]   but I try to keep the quality level really high
[02:20:04.920 --> 02:20:07.640]   where I try to get really best in class kind of guests
[02:20:07.640 --> 02:20:09.720]   and then we're willing to go into some depth.
[02:20:09.720 --> 02:20:11.800]   So it's got a very kind of niche audience,
[02:20:11.800 --> 02:20:15.520]   but if you like the conversation that we just had here,
[02:20:15.520 --> 02:20:17.240]   you'll probably like Manifold.
[02:20:17.240 --> 02:20:20.240]   So you can look for that in all the usual places
[02:20:20.240 --> 02:20:22.240]   you get your podcast and also on YouTube.
[02:20:22.240 --> 02:20:26.360]   - The podcast is like, it's similar.
[02:20:26.360 --> 02:20:28.520]   It's exactly what I'm trying to do here
[02:20:28.520 --> 02:20:30.840]   in the sense that it's like really in-depth conversations,
[02:20:30.840 --> 02:20:33.880]   but you just know so much about so many different fields.
[02:20:33.880 --> 02:20:35.200]   So it's so fun to listen to
[02:20:35.200 --> 02:20:37.800]   where you're having expert level conversations
[02:20:37.800 --> 02:20:40.560]   in everything from social science to foreign policy
[02:20:40.560 --> 02:20:42.960]   to, yeah, obviously your fields,
[02:20:42.960 --> 02:20:47.960]   but that is a podcast I really recommend.
[02:20:47.960 --> 02:20:52.840]   So yeah, Manifold podcast is one you should check out.
[02:20:52.840 --> 02:20:53.840]   - Yeah, my pleasure.
[02:20:53.840 --> 02:20:56.240]   - Thanks for watching.
[02:20:56.240 --> 02:20:58.040]   I hope you enjoyed that episode.
[02:20:58.040 --> 02:21:00.920]   If you did and you wanna support the podcast,
[02:21:00.920 --> 02:21:04.120]   the most helpful thing you can do is share it
[02:21:04.120 --> 02:21:06.560]   on social media and with your friends.
[02:21:06.560 --> 02:21:10.400]   Other than that, please like and subscribe on YouTube
[02:21:10.400 --> 02:21:13.360]   and leave good reviews on podcast platforms.
[02:21:13.360 --> 02:21:15.560]   Cheers, I'll see you next time.
[02:21:15.560 --> 02:21:18.140]   (upbeat music)
[02:21:18.140 --> 02:21:20.720]   (upbeat music)
[02:21:20.720 --> 02:21:24.720]   [music]

