
[00:00:00.000 --> 00:00:09.000]   All right, first we got Muhammad Hamza joining us from Pakistan.
[00:00:09.000 --> 00:00:11.480]   Welcome Muhammad.
[00:00:11.480 --> 00:00:16.240]   We're also, I think we're now live on YouTube, so hopefully we can get some comments from
[00:00:16.240 --> 00:00:23.000]   the folks watching us there.
[00:00:23.000 --> 00:00:41.240]   I also have a quick little question I wanted to ask the folks who were joining us.
[00:00:41.240 --> 00:00:46.400]   I'm curious which frameworks people are using for their experiments.
[00:00:46.400 --> 00:00:52.420]   So if you've been training models recently, if you've been doing some deep learning and
[00:00:52.420 --> 00:01:00.280]   machine learning, go to this link, itempool.com/wnb/live.
[00:01:00.280 --> 00:01:06.160]   And then you can enter your answer there.
[00:01:06.160 --> 00:01:07.560]   There's a couple different options.
[00:01:07.560 --> 00:01:13.800]   And we'll come back and give people a second to look there and put their answers in.
[00:01:13.800 --> 00:01:16.120]   And we'll see what people have said.
[00:01:16.120 --> 00:01:22.620]   So some folks, probably a lot of folks using PyTorch.
[00:01:22.620 --> 00:01:25.220]   Oh, TensorFlow should have been an option there.
[00:01:25.220 --> 00:01:26.940]   So maybe a lot of people are going to say something else.
[00:01:26.940 --> 00:01:28.420]   They're going to say it means TensorFlow/Keras.
[00:01:28.420 --> 00:01:31.940]   I don't know how that one didn't get included.
[00:01:31.940 --> 00:01:35.220]   But yeah, so we've already got two answers.
[00:01:35.220 --> 00:01:40.300]   And we'll come back and see what the remaining answers are maybe after this talk.
[00:01:40.300 --> 00:01:41.900]   Hey, Charles, really quick.
[00:01:41.900 --> 00:01:45.160]   I'm going to start and stop the recording again just so I can do a local version of
[00:01:45.160 --> 00:01:46.160]   it.
[00:01:46.160 --> 00:01:52.960]   So you might just get a loud person saying that this meeting's been recorded.
[00:01:52.960 --> 00:01:53.960]   Great.
[00:01:53.960 --> 00:01:54.960]   Great.
[00:01:54.960 --> 00:01:58.960]   OK, we are all set.
[00:01:58.960 --> 00:01:59.960]   Thanks.
[00:01:59.960 --> 00:02:00.960]   Cool.
[00:02:00.960 --> 00:02:01.960]   Thanks, Kayla.
[00:02:01.960 --> 00:02:02.960]   All right.
[00:02:02.960 --> 00:02:09.560]   So let's get started with our first talk.
[00:02:09.560 --> 00:02:14.860]   So for our first talk, we have Mohamed Reza Amirian, who is joining us from Switzerland.
[00:02:14.860 --> 00:02:20.660]   He's doing a joint PhD at the University in Ulm and at the Zurich University of Applied
[00:02:20.660 --> 00:02:21.660]   Sciences.
[00:02:21.660 --> 00:02:25.540]   He's another person who came into machine learning and deep learning a little bit from
[00:02:25.540 --> 00:02:31.980]   more the biology side, working on biophysical signals and signal processing, and in particular
[00:02:31.980 --> 00:02:34.860]   on signals involving pain.
[00:02:34.860 --> 00:02:40.680]   And now he's doing some work on computer vision and in particular on radial basis function
[00:02:40.680 --> 00:02:44.960]   networks and how they can be integrated with convolutional networks.
[00:02:44.960 --> 00:02:49.240]   So I'm really excited to see this sort of classical machine learning method combined
[00:02:49.240 --> 00:02:52.040]   with newer state-of-the-art techniques.
[00:02:52.040 --> 00:02:54.640]   So Mohamed Reza, take it away.
[00:02:54.640 --> 00:02:55.700]   All right.
[00:02:55.700 --> 00:02:57.760]   Thank you very much, Charles and Kayla.
[00:02:57.760 --> 00:03:05.180]   So I'm quite excited to present our recent research work jointly done at Zurich University
[00:03:05.180 --> 00:03:09.380]   of Applied Sciences and Ulm University.
[00:03:09.380 --> 00:03:17.100]   This work is about integrating radial basis function networks to convolutional neural
[00:03:17.100 --> 00:03:20.540]   networks and using them for computer vision.
[00:03:20.540 --> 00:03:25.640]   This research is guided with my PhD advisor Friedhelm Schwenker from Ulm University and
[00:03:25.640 --> 00:03:34.640]   motivated by my second PhD advisor, Tilo Stadelmann from Zurich University of Applied Sciences.
[00:03:34.640 --> 00:03:40.860]   So the radial basis function neural networks are three-layer neural networks.
[00:03:40.860 --> 00:03:44.400]   They have been introduced first in 1988.
[00:03:44.400 --> 00:03:51.800]   For a given input feature vector in the forward pass, first we compute the distances of these
[00:03:51.800 --> 00:03:56.940]   features to some cluster centers which are encoded in the hidden layer.
[00:03:56.940 --> 00:04:05.220]   And afterwards, we apply activation function to these distances and then train a output
[00:04:05.220 --> 00:04:08.920]   weight in order to specify the classes.
[00:04:08.920 --> 00:04:13.100]   Radial basis function can also be used for regression.
[00:04:13.100 --> 00:04:20.540]   In this research, we adopted the training process of the RBFs and also proposed an activation
[00:04:20.540 --> 00:04:25.100]   function in order that they can be integrated into CNNs.
[00:04:25.100 --> 00:04:30.780]   And as an advantage, we actually can learn a similarity distance metric and we could
[00:04:30.780 --> 00:04:40.220]   also interpret the decision process of the computer vision models using RBFs by looking
[00:04:40.220 --> 00:04:47.980]   at the clusters and distribution of the training and test samples around the clusters.
[00:04:47.980 --> 00:04:49.660]   The related works regarding--
[00:04:49.660 --> 00:04:50.660]   Oh, real quick.
[00:04:50.660 --> 00:04:56.140]   Are you intending to share your screen, those slides you had?
[00:04:56.140 --> 00:04:57.200]   Yes.
[00:04:57.200 --> 00:04:59.500]   Can you see the slides or--
[00:04:59.500 --> 00:05:01.100]   I currently don't see the slides.
[00:05:01.100 --> 00:05:04.460]   I think your screen sharing is turned off.
[00:05:04.460 --> 00:05:05.460]   OK.
[00:05:05.460 --> 00:05:08.340]   I think it's sharing, but perhaps--
[00:05:08.340 --> 00:05:09.780]   It seems like it's sharing.
[00:05:09.780 --> 00:05:11.420]   It's working from my end.
[00:05:11.420 --> 00:05:12.420]   Oh, OK.
[00:05:12.420 --> 00:05:13.820]   I figured it out.
[00:05:13.820 --> 00:05:14.820]   My bad.
[00:05:14.820 --> 00:05:15.820]   Keep going.
[00:05:15.820 --> 00:05:16.820]   OK.
[00:05:16.820 --> 00:05:17.820]   Can you see the slides now?
[00:05:17.820 --> 00:05:18.820]   Yes.
[00:05:18.820 --> 00:05:19.820]   OK, great.
[00:05:19.820 --> 00:05:20.820]   Thanks.
[00:05:20.820 --> 00:05:23.060]   You're welcome.
[00:05:23.060 --> 00:05:30.220]   And the related works related to the RBF networks are divided into three categories.
[00:05:30.220 --> 00:05:33.660]   So some works are focusing on the training process.
[00:05:33.660 --> 00:05:40.780]   There are different type of training process, one, two, and three phase learning for RBFs.
[00:05:40.780 --> 00:05:47.740]   And here, we actually combined two phases of training the RBFs by combining supervised
[00:05:47.740 --> 00:05:49.700]   and unsupervised learning.
[00:05:49.700 --> 00:05:54.700]   There are several different activation functions proposed in the literature for RBF networks
[00:05:54.700 --> 00:05:57.140]   based on different applications.
[00:05:57.140 --> 00:06:03.180]   And here, we propose a quadratic function in order that we can have a completely linear
[00:06:03.180 --> 00:06:08.380]   computational graph for efficient gradient flow for CNNs.
[00:06:08.380 --> 00:06:15.380]   And in terms of applications, CNNs are used for classification, regression, and even mathematical
[00:06:15.380 --> 00:06:17.500]   function interpolation.
[00:06:17.500 --> 00:06:26.100]   So here, we present the first attempt of using RBFs in conjunction with CNNs for computer
[00:06:26.100 --> 00:06:32.980]   vision.
[00:06:32.980 --> 00:06:39.900]   So here, I explain the training process of radial basis networks.
[00:06:39.900 --> 00:06:44.020]   In the hidden layer, as I mentioned earlier, the cluster centers are encoded.
[00:06:44.020 --> 00:06:50.300]   Here, you can see an unsupervised loss inspired by K-means algorithm.
[00:06:50.300 --> 00:06:54.860]   The first sum shows an average over the clusters.
[00:06:54.860 --> 00:07:03.540]   And the second is aimed at minimizing the distance between feature vectors and a cluster
[00:07:03.540 --> 00:07:04.540]   center.
[00:07:04.540 --> 00:07:09.900]   Traditionally, the feature space used to be fixed for radial basis function networks.
[00:07:09.900 --> 00:07:15.220]   But in the architecture that we use, we feed the embeddings of CNNs, which are trained
[00:07:15.220 --> 00:07:18.060]   during the training process.
[00:07:18.060 --> 00:07:21.380]   So the input features of the RBFs are not fixed anymore.
[00:07:21.380 --> 00:07:27.580]   Therefore, we include this loss function in the training in order that the cluster centers
[00:07:27.580 --> 00:07:35.980]   are updated during the training process of the RBF networks and CNNs.
[00:07:35.980 --> 00:07:37.660]   Then we have to compute the distance.
[00:07:37.660 --> 00:07:42.660]   As you can see, the distance can be defined based on a distance metric.
[00:07:42.660 --> 00:07:49.340]   If the distance metric could be Euclidean distance, if we train the main diagonal of
[00:07:49.340 --> 00:07:54.180]   the covariance metrics, then we end up with having a Mahalanobis distance.
[00:07:54.180 --> 00:07:56.980]   And the entire covariance metrics can also be learned.
[00:07:56.980 --> 00:08:03.380]   And we can write it in terms of matrix multiplication, as you can see over here.
[00:08:03.380 --> 00:08:07.300]   Then we apply activations.
[00:08:07.300 --> 00:08:13.140]   And at the end, we can estimate the ground truth label shown by Y by a multiplication
[00:08:13.140 --> 00:08:18.700]   of output weights and the activations.
[00:08:18.700 --> 00:08:25.260]   And initializing the clusters are basically the second phase of the optimization.
[00:08:25.260 --> 00:08:33.900]   And the one-phase training algorithm of RBF only computes the weights of the output layer.
[00:08:33.900 --> 00:08:45.020]   And the third phase is fine-tuning the model using gradient descent end-to-end.
[00:08:45.020 --> 00:08:54.020]   In order to adapt the RBFs to CNNs, as we said, we connect the backbone of the CNNs
[00:08:54.020 --> 00:08:58.780]   via a fully connected layer to the RBFs.
[00:08:58.780 --> 00:09:04.620]   So in principle, we flatten the features and use a fully connected without any type of
[00:09:04.620 --> 00:09:06.260]   activation here.
[00:09:06.260 --> 00:09:13.580]   So experimentally, I noticed that RBFs don't work very well with dropout.
[00:09:13.580 --> 00:09:18.900]   Therefore, this layer is necessary.
[00:09:18.900 --> 00:09:25.540]   And since we have parameters both in the hidden layer and output layer, if we use the original
[00:09:25.540 --> 00:09:29.380]   feature space, we easily undergo overfitting.
[00:09:29.380 --> 00:09:37.340]   So a fully connected layer to convert the output of the CNNs into a lower dimensional
[00:09:37.340 --> 00:09:39.660]   space is necessary here.
[00:09:39.660 --> 00:09:45.740]   Afterwards, we have the computation of the distance using matrix multiplication.
[00:09:45.740 --> 00:09:55.980]   And our proposed activation is just an addition and division by constants, which shows the
[00:09:55.980 --> 00:09:59.300]   width of the kernel.
[00:09:59.300 --> 00:10:08.020]   And at the end, during the optimization, we optimize the unsupervised loss and minimize
[00:10:08.020 --> 00:10:12.420]   it, as I mentioned earlier, and we will have a supervised loss, which could be a normal
[00:10:12.420 --> 00:10:18.340]   softmax cross entropy or any type of other loss that we would like to optimize for classification
[00:10:18.340 --> 00:10:23.220]   or regression.
[00:10:23.220 --> 00:10:31.320]   In this slide, I would like to show you how the training process works in practice.
[00:10:31.320 --> 00:10:34.420]   For this slide, I used the MNIST dataset.
[00:10:34.420 --> 00:10:40.380]   And on right and left, you can see the two dimensional representation of first the input
[00:10:40.380 --> 00:10:49.500]   of the RBF, which is the embeddings of the CNNs and the activations of the clusters,
[00:10:49.500 --> 00:10:54.080]   basically after computing the distance and applying the activation functions.
[00:10:54.080 --> 00:11:01.020]   On left, you can see that the unsupervised loss is more prominent, means that during
[00:11:01.020 --> 00:11:08.300]   the training process, the data samples are divided into clusters which are not corresponding
[00:11:08.300 --> 00:11:13.580]   to the ground truth labels.
[00:11:13.580 --> 00:11:20.020]   On right, the other loss is more prominent, means supervised loss.
[00:11:20.020 --> 00:11:26.500]   And you can see that the data samples are dividing into clusters during the training
[00:11:26.500 --> 00:11:31.780]   process based on the ground truth labels.
[00:11:31.780 --> 00:11:38.260]   In the middle figure, I demonstrated the samples around a cluster center.
[00:11:38.260 --> 00:11:42.420]   So you can imagine that the center of the cluster is at zero and zero.
[00:11:42.420 --> 00:11:48.020]   Then the training samples are distributed with a random angle based on their distance
[00:11:48.020 --> 00:11:50.340]   to the center of this cluster.
[00:11:50.340 --> 00:11:53.820]   Here is where both losses interact.
[00:11:53.820 --> 00:12:01.460]   The unsupervised loss tries to bring all the samples as close as possible to the center,
[00:12:01.460 --> 00:12:04.140]   as I explained earlier.
[00:12:04.140 --> 00:12:11.500]   And the supervised loss tries to put the samples from the same clusters with the same space
[00:12:11.500 --> 00:12:12.900]   from the cluster center.
[00:12:12.900 --> 00:12:18.740]   So this is why you can see some circles with the samples from same clusters around this
[00:12:18.740 --> 00:12:21.220]   cluster.
[00:12:21.220 --> 00:12:31.060]   And this process continues during the training of the CNN and RBF architecture.
[00:12:31.060 --> 00:12:45.260]   Furthermore, we used some benchmark computer vision datasets in order to confirm that this
[00:12:45.260 --> 00:12:50.580]   architecture can work for more complicated problems.
[00:12:50.580 --> 00:13:00.460]   So we noticed that picking the correct set of hyperparameter, including number of clusters,
[00:13:00.460 --> 00:13:07.180]   as well as the dimensionality of the input and dimensionality of the RBF is not all the
[00:13:07.180 --> 00:13:08.180]   time trivial.
[00:13:08.180 --> 00:13:17.420]   So we had to use the Vade and Biases toolbox in order to have a hyperparameter search.
[00:13:17.420 --> 00:13:25.740]   And we also used the auto-augment for augmenting our images to improve the performance.
[00:13:25.740 --> 00:13:31.700]   So at the end, we noticed that the radial basis function networks can work with a wide
[00:13:31.700 --> 00:13:39.700]   range of CNN backbones, such as efficient net and networks, including inception blocks,
[00:13:39.700 --> 00:13:42.340]   as well as residual connections.
[00:13:42.340 --> 00:13:49.820]   So there is a small gap between the performances that we can achieve using RBFs on the top
[00:13:49.820 --> 00:13:55.460]   of CNNs and the state of the art, which is actually due to the overfitting.
[00:13:55.460 --> 00:14:04.100]   So we noticed that the training dataset can be learned very well, but we need to have
[00:14:04.100 --> 00:14:11.620]   novel methods for regularization of the RBFs.
[00:14:11.620 --> 00:14:21.180]   At the end, the metric that we learned based on the architecture of the RBFs can be used
[00:14:21.180 --> 00:14:23.940]   to find similar and dissimilar images.
[00:14:23.940 --> 00:14:31.940]   You can see that we applied to the pet dataset, the dataset of aircrafts, as well as birds.
[00:14:31.940 --> 00:14:38.620]   And we can take a look at the position of the test images and corresponding closer train
[00:14:38.620 --> 00:14:41.560]   images around every cluster.
[00:14:41.560 --> 00:14:48.500]   This is not necessarily at the moment interpretable based on the ground truth label, since as
[00:14:48.500 --> 00:14:55.980]   I visualized in the training process as well, these clusters are learned completely unsupervised
[00:14:55.980 --> 00:15:03.460]   and they are not necessarily, the position of the samples are not necessarily related
[00:15:03.460 --> 00:15:08.340]   to their labels.
[00:15:08.340 --> 00:15:16.380]   At the end, we had to modify the activation of the RBFs as well as its training process
[00:15:16.380 --> 00:15:19.980]   in order to integrate it into CNNs.
[00:15:19.980 --> 00:15:26.620]   We have comparable results with the state of the arts, but there is still a gap and
[00:15:26.620 --> 00:15:38.140]   the RBFs provide us with the opportunity to have a more interpretable methods and decision
[00:15:38.140 --> 00:15:40.620]   making process.
[00:15:40.620 --> 00:15:49.460]   So maybe one of the most important questions for further investigation would be that the
[00:15:49.460 --> 00:15:55.900]   regularization techniques for RBF in order to fill the gap between our performances and
[00:15:55.900 --> 00:15:59.240]   the state of the art.
[00:15:59.240 --> 00:16:05.900]   Thank you very much for your attention and I'm very willing to hear your questions.
[00:16:05.900 --> 00:16:06.900]   Great.
[00:16:06.900 --> 00:16:13.980]   So if folks have questions, either on YouTube or on Zoom, type them into your respective
[00:16:13.980 --> 00:16:19.660]   live chats and we will pass them on to Mohamed Reza.
[00:16:19.660 --> 00:16:24.500]   My first question was about the readout layer on these things.
[00:16:24.500 --> 00:16:30.100]   You said you could use a cross entropy loss or something else.
[00:16:30.100 --> 00:16:35.500]   Could you talk a little bit more about those choices and how you implement them and how
[00:16:35.500 --> 00:16:36.500]   you use them?
[00:16:36.500 --> 00:16:42.860]   Yeah, I mean, the output layer is actually very similar to the normal output layers that
[00:16:42.860 --> 00:16:46.740]   we kind of have for normal CNNs.
[00:16:46.740 --> 00:16:53.860]   So depending on the task that you have, you can also use mean square error as well for
[00:16:53.860 --> 00:16:56.380]   regression for instance.
[00:16:56.380 --> 00:17:01.740]   So any type of loss function which can be used in conjunction with CNNs can be used
[00:17:01.740 --> 00:17:05.860]   with this architecture as well.
[00:17:05.860 --> 00:17:06.860]   I see.
[00:17:06.860 --> 00:17:13.100]   So the output isn't necessarily those locations of those cluster centers that you were showing.
[00:17:13.100 --> 00:17:14.940]   That's not the final output layer of the network?
[00:17:14.940 --> 00:17:22.300]   No, like the output layer exactly has the same structure as the output of the CNN.
[00:17:22.300 --> 00:17:29.140]   So it has the same number as the number of classes and in principle, the training process
[00:17:29.140 --> 00:17:31.060]   is very the same.
[00:17:31.060 --> 00:17:32.060]   I see.
[00:17:32.060 --> 00:17:35.300]   So then what you were showing was the hidden layer of the RBF.
[00:17:35.300 --> 00:17:37.700]   Those are those cluster locations you were showing.
[00:17:37.700 --> 00:17:38.700]   Yeah, exactly.
[00:17:38.700 --> 00:17:42.540]   So this is basically the activation and not the output layer.
[00:17:42.540 --> 00:17:47.980]   So I visualized the activations.
[00:17:47.980 --> 00:17:48.980]   I see.
[00:17:48.980 --> 00:17:49.980]   Yeah.
[00:17:49.980 --> 00:17:53.740]   So I'm just, one reason I'm asking about that is I know folks have looked into ways to do
[00:17:53.740 --> 00:17:59.140]   classification that don't use softmax because various issues like the calibration of the
[00:17:59.140 --> 00:18:01.540]   softmax layer can be very difficult.
[00:18:01.540 --> 00:18:05.660]   It can be difficult to motivate why we're using a softmax in the first place.
[00:18:05.660 --> 00:18:11.340]   So it seems like those cluster centers that you have have a natural interpretation as
[00:18:11.340 --> 00:18:12.860]   basically class labels, right?
[00:18:12.860 --> 00:18:15.060]   Which cluster it is closest to.
[00:18:15.060 --> 00:18:22.540]   So is that a way that you can sort of train these networks or validate these networks
[00:18:22.540 --> 00:18:26.900]   or is it important to include that readout and softmax?
[00:18:26.900 --> 00:18:30.300]   Yeah, actually that's definitely true.
[00:18:30.300 --> 00:18:38.140]   So besides unsupervised initialization of the clusters, we can also do it based on supervised
[00:18:38.140 --> 00:18:39.340]   methods.
[00:18:39.340 --> 00:18:46.380]   So at the moment, the way that we completely use unsupervised learning on these clusters,
[00:18:46.380 --> 00:18:52.740]   but in principle you can divide your classes into subclasses or superclasses even, and
[00:18:52.740 --> 00:18:56.780]   then you don't need the softmax layer at the end as well.
[00:18:56.780 --> 00:19:04.140]   So it's possible to just finish the network in the cluster level as well.
[00:19:04.140 --> 00:19:08.540]   Interesting.
[00:19:08.540 --> 00:19:14.260]   And so you mentioned your interest in explainability and interpretable AI.
[00:19:14.260 --> 00:19:24.300]   So what are some of the ways that you see this as being directly enabling greater explainability
[00:19:24.300 --> 00:19:28.260]   for CNNs relative to the baseline?
[00:19:28.260 --> 00:19:36.900]   So I would say as soon as we have these supervised clusters, we will have a much better interpretability
[00:19:36.900 --> 00:19:43.380]   because at the moment, the clusters doesn't really show anything interpretable based on
[00:19:43.380 --> 00:19:46.100]   common sense knowledge of humans.
[00:19:46.100 --> 00:19:54.420]   But in principle, we can divide every class into subclusters or even into superclasses,
[00:19:54.420 --> 00:20:01.460]   and then we can kind of interpret where these test sample really goes and interpret these
[00:20:01.460 --> 00:20:05.620]   based on the ground truth labels and subcategories, of course.
[00:20:05.620 --> 00:20:07.620]   I see.
[00:20:07.620 --> 00:20:14.300]   So it gives you maybe a little bit more insight into that last bit of the CNN before the readout
[00:20:14.300 --> 00:20:19.020]   layer, like sort of enforcing this unsupervised learning stuff.
[00:20:19.020 --> 00:20:20.020]   Yeah, that's true.
[00:20:20.020 --> 00:20:21.020]   Cool.
[00:20:21.020 --> 00:20:22.020]   All right.
[00:20:22.020 --> 00:20:28.900]   Well, thanks for answering those questions.
[00:20:28.900 --> 00:20:33.820]   And it looks like, oh, we've got one from Han Li here.
[00:20:33.820 --> 00:20:35.700]   So let me read it out to you.
[00:20:35.700 --> 00:20:40.260]   So how does the latent space transform between the latent space output of the CNN backbone
[00:20:40.260 --> 00:20:42.380]   versus afterwards?
[00:20:42.380 --> 00:20:48.540]   So what, like, how does that, I guess he wants a little bit of insight into what changes
[00:20:48.540 --> 00:20:54.020]   about that last layer of the CNN and what comes out of your RBF network.
[00:20:54.020 --> 00:20:55.380]   All right.
[00:20:55.380 --> 00:21:01.340]   I think it's basically just about how we can visualize the decision making.
[00:21:01.340 --> 00:21:07.420]   So I would say these cluster centers work as support points.
[00:21:07.420 --> 00:21:17.640]   And we kind of can explain the transformation of the CNNs based on these support points.
[00:21:17.640 --> 00:21:22.420]   So at the moment, since we do it completely unsupervised, they don't say anything about
[00:21:22.420 --> 00:21:26.460]   any kind of human interpretable concept.
[00:21:26.460 --> 00:21:31.780]   But as soon as we involve the ground truth labels into learning the clusters, then you
[00:21:31.780 --> 00:21:39.020]   can basically see the network made this decision because it's close to cluster center A, which
[00:21:39.020 --> 00:21:45.060]   contains a specific attribute of the image.
[00:21:45.060 --> 00:21:46.060]   I see.
[00:21:46.060 --> 00:21:47.060]   Yeah, that's interesting.
[00:21:47.060 --> 00:21:52.740]   And I guess one last question for me.
[00:21:52.740 --> 00:21:57.900]   So how immediately extensible is this idea to other kinds of-- so like stacking it on
[00:21:57.900 --> 00:22:03.580]   the end of a network that's fully connected or a recurrent network on its readout layer
[00:22:03.580 --> 00:22:06.500]   or some other kind of network?
[00:22:06.500 --> 00:22:09.300]   Is there an immediate way to translate from the work you've done to that?
[00:22:09.300 --> 00:22:12.180]   Or is that another project?
[00:22:12.180 --> 00:22:18.620]   I would say that with some reasonable effort, it could be possible to integrate RBFs as
[00:22:18.620 --> 00:22:23.100]   the configuration that we proposed here to other types of networks.
[00:22:23.100 --> 00:22:28.940]   I mean, LSTMs or recurrent networks are a little bit tricky to train.
[00:22:28.940 --> 00:22:35.620]   But I still think that the model that we have is basically ready for plug and play to any
[00:22:35.620 --> 00:22:39.820]   type of deep learning method.
[00:22:39.820 --> 00:22:46.540]   I would say the challenge would be to somewhat have a hyperparameter search as well as regularization
[00:22:46.540 --> 00:22:47.540]   for sure.
[00:22:47.540 --> 00:22:52.180]   So maybe at the very beginning, it takes some time to reproduce the same performance with
[00:22:52.180 --> 00:22:53.180]   more interpretability.
[00:22:53.180 --> 00:22:54.180]   Yeah, no free lunch.
[00:22:54.180 --> 00:22:55.180]   Yeah.
[00:22:55.180 --> 00:22:56.180]   Yeah.
[00:22:56.180 --> 00:22:57.180]   Cool.
[00:22:57.180 --> 00:22:58.180]   All right.
[00:22:58.180 --> 00:23:04.300]   Well, it's very, very late at night/early in the morning in Switzerland.
[00:23:04.300 --> 00:23:05.300]   So we'll let you go.
[00:23:05.300 --> 00:23:06.700]   Thanks for presenting your research.
[00:23:06.700 --> 00:23:10.180]   So thank you very much for your interest and amazing questions.
[00:23:10.180 --> 00:23:13.420]   It was great to be with you.
[00:23:13.420 --> 00:23:15.020]   And thanks a lot for your invite.
[00:23:15.020 --> 00:23:16.020]   Yep.
[00:23:16.020 --> 00:23:17.020]   Take care.
[00:23:17.020 --> 00:23:18.020]   So thank you very much.
[00:23:18.020 --> 00:23:20.620]   Have a good afternoon.
[00:23:20.620 --> 00:23:21.620]   Thank you.
[00:23:21.620 --> 00:23:27.540]   So next up, we've got my coworker, Stacey Svetlichnaya.
[00:23:27.540 --> 00:23:31.980]   So she's talked at these salons in the past.
[00:23:31.980 --> 00:23:37.340]   So I'll give her a second to get her slides up.
[00:23:37.340 --> 00:23:43.620]   While she does that, let's look at our responses on item pool.
[00:23:43.620 --> 00:23:45.180]   Let's see what people said.
[00:23:45.180 --> 00:23:46.900]   All right.
[00:23:46.900 --> 00:23:52.540]   So it looks like two folks said PyTorch.
[00:23:52.540 --> 00:23:54.740]   One person said PyTorch and something else.
[00:23:54.740 --> 00:23:57.860]   And one person said something else.
[00:23:57.860 --> 00:24:03.900]   I would guess, as we said at the start, that the absence of TensorFlow/Keras on that list
[00:24:03.900 --> 00:24:07.740]   means that something else is probably that.
[00:24:07.740 --> 00:24:09.740]   So it's interesting to see.
[00:24:09.740 --> 00:24:15.380]   It looks like we're a little bit evenly divided between our PyTorch and TensorFlow users.
[00:24:15.380 --> 00:24:17.700]   And yeah.
[00:24:17.700 --> 00:24:21.620]   So much like the community at large.
[00:24:21.620 --> 00:24:25.320]   I think I had another question.
[00:24:25.320 --> 00:24:29.040]   This one is a little bit more freeform.
[00:24:29.040 --> 00:24:35.580]   So there's been a lot of excitement and interest on Twitter and especially certain parts of
[00:24:35.580 --> 00:24:40.100]   venture capital and machine learning Twitter over the last couple of weeks about the GPT-3
[00:24:40.100 --> 00:24:47.260]   model, the general purpose transformer 3 model from OpenAI.
[00:24:47.260 --> 00:24:56.260]   So this next question, which you can provide your opinion on by going to itempool.com/w&b/live,
[00:24:56.260 --> 00:25:07.660]   which I will drop into the chat on Zoom and on YouTube, is will more than 20% of the text
[00:25:07.660 --> 00:25:14.100]   on the internet be generated by GPT-3 or similar deep language models by 2025?
[00:25:14.100 --> 00:25:22.480]   So there's a lot of possibilities, chat bots, generating content, all kinds of things.
[00:25:22.480 --> 00:25:27.860]   And it could end up that the very data set that was used to train GPT-3 gets filled up
[00:25:27.860 --> 00:25:32.560]   with neural network content, which will be an interesting problem.
[00:25:32.560 --> 00:25:35.280]   So we're already starting to see a few answers coming in.
[00:25:35.280 --> 00:25:38.580]   So make sure to check that out.
[00:25:38.580 --> 00:25:47.460]   And we'll see at the end of Stacey's talk what people said.
[00:25:47.460 --> 00:25:50.420]   So you can go ahead.
[00:25:50.420 --> 00:25:52.460]   I'm done with my little question.
[00:25:52.460 --> 00:25:53.460]   >> That's great.
[00:25:53.460 --> 00:25:57.100]   Can you just confirm that right now you're seeing slides and right now you're seeing
[00:25:57.100 --> 00:25:58.100]   a report?
[00:25:58.100 --> 00:25:59.100]   >> Yeah.
[00:25:59.100 --> 00:26:02.300]   It takes a second for the report to load.
[00:26:02.300 --> 00:26:03.300]   But yeah.
[00:26:03.300 --> 00:26:04.300]   >> Okay.
[00:26:04.300 --> 00:26:06.900]   I'm going to make sure that it's switching tabs.
[00:26:06.900 --> 00:26:08.900]   >> Oh, yeah.
[00:26:08.900 --> 00:26:09.900]   >> Great.
[00:26:09.900 --> 00:26:10.900]   Awesome.
[00:26:10.900 --> 00:26:12.900]   Then I can get started.
[00:26:12.900 --> 00:26:13.900]   Great.
[00:26:13.900 --> 00:26:14.900]   Hi, everyone.
[00:26:14.900 --> 00:26:15.900]   I'm Stacey.
[00:26:15.900 --> 00:26:22.060]   And I'm a deep learning engineer here at Weights & Biases.
[00:26:22.060 --> 00:26:27.220]   And today I'm going to talk to you about some amazing visualization tools that we have for
[00:26:27.220 --> 00:26:34.780]   self-driving tasks on Weights & Biases or autonomous navigation more generally.
[00:26:34.780 --> 00:26:40.140]   And as an outline, I'll dive into some specific subproblems in self-driving and the tools
[00:26:40.140 --> 00:26:47.460]   we have for those, semantic segmentation, 2D object detection, and 3D object detection.
[00:26:47.460 --> 00:26:51.780]   I'll then give an overview of some of the collaboration tools that are generally useful
[00:26:51.780 --> 00:26:58.860]   for anything you're working on and do a deep dive on a report that I have on a particular
[00:26:58.860 --> 00:27:04.700]   semantic segmentation problem and set of models and how I used Weights & Biases to help figure
[00:27:04.700 --> 00:27:05.700]   that out.
[00:27:05.700 --> 00:27:07.020]   And I'd love your questions.
[00:27:07.020 --> 00:27:10.740]   I generally might keep this a little bit on the shorter side and see what people want
[00:27:10.740 --> 00:27:15.460]   to dive into and also have interactive examples.
[00:27:15.460 --> 00:27:18.820]   So first, focusing on semantic segmentation.
[00:27:18.820 --> 00:27:25.300]   This is a task where given an image, in this case, it'll be a dashboard scene from the
[00:27:25.300 --> 00:27:27.420]   dash of a car.
[00:27:27.420 --> 00:27:34.580]   You need to label every pixel in the image as belonging to a particular class or category.
[00:27:34.580 --> 00:27:39.180]   My models have been training with 20 different of these categories or labels.
[00:27:39.180 --> 00:27:47.540]   So that's car or tree or building or road or human, bicycle, et cetera.
[00:27:47.540 --> 00:27:51.820]   And different models have different labels, but those are sort of the main ones that we
[00:27:51.820 --> 00:27:52.820]   care about.
[00:27:52.820 --> 00:27:56.180]   You can also imagine doing this on videos.
[00:27:56.180 --> 00:27:59.460]   And I have some links here if you want to check out some code examples.
[00:27:59.460 --> 00:28:05.100]   There's a cool Kaggle competition for this from CVPR 2018.
[00:28:05.100 --> 00:28:09.140]   You can also extend this to segmenting drivable area.
[00:28:09.140 --> 00:28:13.660]   So knowing which lane is available for a car to switch into.
[00:28:13.660 --> 00:28:21.300]   And this is encapsulated in a great dataset, Berkeley Deep Drive 100K.
[00:28:21.300 --> 00:28:22.300]   I've been using this a lot.
[00:28:22.300 --> 00:28:28.100]   It has lots of self-driving car-related tasks and datasets.
[00:28:28.100 --> 00:28:31.500]   And the solution that we provide for this at Weights and Biases is semantic segmentation
[00:28:31.500 --> 00:28:36.340]   masks, which let you easily and interactively compare model predictions to ground truth.
[00:28:36.340 --> 00:28:37.340]   Let's see if this works.
[00:28:37.340 --> 00:28:39.980]   I'm going to jump into a report that I made for this.
[00:28:39.980 --> 00:28:44.300]   And you can check this out and try it yourself in a Colab notebook here.
[00:28:44.300 --> 00:28:51.420]   But just to give you an overview, this lets you separately log the examples that you're
[00:28:51.420 --> 00:28:54.060]   using the raw photos for your model.
[00:28:54.060 --> 00:28:57.340]   And then the class annotations for them.
[00:28:57.340 --> 00:29:03.660]   So in this case, you can see that this human is not tagged here.
[00:29:03.660 --> 00:29:08.140]   But here we can see them outlined in orange.
[00:29:08.140 --> 00:29:12.260]   And you'll also see that this should be road, but it's classified as sidewalk.
[00:29:12.260 --> 00:29:16.220]   And I think this is because it's overfitting on lots of examples where humans are generally
[00:29:16.220 --> 00:29:19.860]   on or next to the sidewalk.
[00:29:19.860 --> 00:29:27.300]   And one -- the most amazing aspect of this feature is that you don't need to log fixed
[00:29:27.300 --> 00:29:28.780]   layers in advance.
[00:29:28.780 --> 00:29:34.340]   You can interactively decide which classes you care about and how to overlay them over
[00:29:34.340 --> 00:29:37.700]   your training images.
[00:29:37.700 --> 00:29:43.740]   So here you can see more of these examples interactively.
[00:29:43.740 --> 00:29:51.140]   And from the controls here, you can turn on and off any of the classes.
[00:29:51.140 --> 00:29:55.580]   These are ground truth and prediction masks overlaid with the initial results and the
[00:29:55.580 --> 00:29:56.580]   final results.
[00:29:56.580 --> 00:30:01.780]   So you can see, say, the prediction of the human converging much more on the ground truth
[00:30:01.780 --> 00:30:03.220]   label.
[00:30:03.220 --> 00:30:11.340]   Another way you can lay this out is just looking at the final predictions for each model.
[00:30:11.340 --> 00:30:16.220]   And let's jump into the results for a particular one.
[00:30:16.220 --> 00:30:21.820]   Here all three -- so the raw photo, the model's prediction, and the ground truth, all three
[00:30:21.820 --> 00:30:25.300]   of those masks are overlaid on top of each other.
[00:30:25.300 --> 00:30:28.340]   Which looks pretty cool, but might be a little hard to read.
[00:30:28.340 --> 00:30:31.860]   Here we see that the ground truth annotation gets the bus.
[00:30:31.860 --> 00:30:35.100]   But it looks like we've labeled it in a different color.
[00:30:35.100 --> 00:30:42.460]   So one thing we can do is split these up so that we get all three masks separately.
[00:30:42.460 --> 00:30:48.860]   And now we can see that the correct labeling is bus, but it looks like this example actually
[00:30:48.860 --> 00:30:50.740]   has it labeled as a truck.
[00:30:50.740 --> 00:30:55.700]   Which we can confirm by toggling the truck on and off.
[00:30:55.700 --> 00:30:59.340]   And then we can sort of scroll through these and see how it's generally doing.
[00:30:59.340 --> 00:31:04.380]   Here's another example where the truck is actually labeled as a car.
[00:31:04.380 --> 00:31:05.380]   And that's understandable.
[00:31:05.380 --> 00:31:09.140]   That's a pretty easy confusion to make.
[00:31:09.140 --> 00:31:13.700]   Plus you can see the details like the traffic poles that it's actually doing pretty well
[00:31:13.700 --> 00:31:16.740]   on here.
[00:31:16.740 --> 00:31:23.540]   But maybe not getting some of the detail, especially in faraway cars.
[00:31:23.540 --> 00:31:27.140]   And so I'm just showing a bunch of different examples here.
[00:31:27.140 --> 00:31:31.140]   Let's hop into a different feature.
[00:31:31.140 --> 00:31:37.980]   2D object detection, which is another classic problem in self driving.
[00:31:37.980 --> 00:31:43.460]   What we want to do is draw a box, a bounding box around an object that we care about.
[00:31:43.460 --> 00:31:45.260]   Say a car.
[00:31:45.260 --> 00:31:46.260]   Or a human.
[00:31:46.260 --> 00:31:49.180]   Or again, a traffic sign.
[00:31:49.180 --> 00:31:50.740]   And you can do this in 3D as well.
[00:31:50.740 --> 00:31:56.860]   There's a great lift data set for this on Kaggle.
[00:31:56.860 --> 00:32:03.180]   And what we do for this in Weights and Biases is enable you to see bounding boxes with an
[00:32:03.180 --> 00:32:05.660]   interactive slider for confidence level.
[00:32:05.660 --> 00:32:12.660]   And let me again hop into a report where I am again training on the Berkeley Deep Drive.
[00:32:12.660 --> 00:32:15.300]   This time it's not semantic segmentation.
[00:32:15.300 --> 00:32:19.600]   It's a YOLOv3 network for object detection.
[00:32:19.600 --> 00:32:23.140]   And here you can just at a glance see a bunch of the bounding boxes.
[00:32:23.140 --> 00:32:27.980]   And you can zoom into it a little bit like this.
[00:32:27.980 --> 00:32:30.660]   And I can set the score slider.
[00:32:30.660 --> 00:32:31.660]   You know, maybe it's too noisy.
[00:32:31.660 --> 00:32:34.660]   I'm going to set it higher.
[00:32:34.660 --> 00:32:38.380]   And you can see that a lot of the low confidence ones disappear.
[00:32:38.380 --> 00:32:40.180]   You can toggle them on and off.
[00:32:40.180 --> 00:32:42.340]   You'll notice this bus, for example.
[00:32:42.340 --> 00:32:45.460]   Blinking in and out.
[00:32:45.460 --> 00:32:46.820]   What else does this model detect?
[00:32:46.820 --> 00:32:48.660]   Benches, fire hydrants, airplanes.
[00:32:48.660 --> 00:32:52.460]   I don't think there's any airplanes in the validation data set.
[00:32:52.460 --> 00:32:57.660]   And you can, of course, show this zoomed in and use this to debug the details of your
[00:32:57.660 --> 00:32:58.660]   model.
[00:32:58.660 --> 00:33:01.940]   You know, this is doing really well here on people.
[00:33:01.940 --> 00:33:05.460]   Even getting this one in the background.
[00:33:05.460 --> 00:33:09.460]   But it does, for example, miss this tiny fire hydrant here.
[00:33:09.460 --> 00:33:13.220]   You can see that there's some overlap between trucks and cars.
[00:33:13.220 --> 00:33:18.460]   And you can read the confidence score here.
[00:33:18.460 --> 00:33:22.980]   And if you check out these reports, you can see that the syntax for logging this is really
[00:33:22.980 --> 00:33:23.980]   easy.
[00:33:23.980 --> 00:33:28.780]   And we take bounding boxes as both relative and absolute coordinates.
[00:33:28.780 --> 00:33:32.780]   And it's all nicely done for you.
[00:33:32.780 --> 00:33:35.660]   And hopping back into a different feature.
[00:33:35.660 --> 00:33:38.300]   3D object detection.
[00:33:38.300 --> 00:33:41.820]   And here we're looking at point clouds generated from LiDAR.
[00:33:41.820 --> 00:33:43.900]   This is, again, from the LIFT data set.
[00:33:43.900 --> 00:33:48.020]   It's really cool to play with this data.
[00:33:48.020 --> 00:33:52.900]   And what we let you do in Weights and Biases is actually log and interact with the point
[00:33:52.900 --> 00:33:54.820]   cloud.
[00:33:54.820 --> 00:33:58.060]   And we'll see if this works in present mode.
[00:33:58.060 --> 00:34:02.700]   But basically, I can expand one of these.
[00:34:02.700 --> 00:34:03.700]   Oh.
[00:34:03.700 --> 00:34:04.700]   Oh.
[00:34:04.700 --> 00:34:06.060]   This might be a little much.
[00:34:06.060 --> 00:34:10.100]   But you can -- I think that's gonna be too much for the browser.
[00:34:10.100 --> 00:34:14.260]   But basically, you can just trust me that you can pan and zoom and rotate around the
[00:34:14.260 --> 00:34:19.820]   map and see how the bounding boxes -- here the ground truth is in green and the predictions
[00:34:19.820 --> 00:34:21.580]   are in yellow.
[00:34:21.580 --> 00:34:24.020]   Sometimes they overlap.
[00:34:24.020 --> 00:34:27.700]   Sometimes the model doesn't quite get the correct orientation for the car or thinks
[00:34:27.700 --> 00:34:31.220]   the tree is a person, et cetera.
[00:34:31.220 --> 00:34:35.580]   So this is another fun logging tool.
[00:34:35.580 --> 00:34:38.940]   And then hopping back.
[00:34:38.940 --> 00:34:41.700]   This is a link to the report.
[00:34:41.700 --> 00:34:44.700]   And maybe you can see here a more detailed example.
[00:34:44.700 --> 00:34:49.660]   When the boxes are lined up, that means our model got a pretty good prediction of the
[00:34:49.660 --> 00:34:50.660]   ground truth.
[00:34:50.660 --> 00:34:53.900]   But then there's cases in both directions, lots of predictions that are actually picking
[00:34:53.900 --> 00:34:55.500]   up on some noise.
[00:34:55.500 --> 00:34:59.940]   Although this here could be a car that we've detected that was unlabeled in the original
[00:34:59.940 --> 00:35:03.500]   data set.
[00:35:03.500 --> 00:35:08.140]   Now zooming out to collaboration tools.
[00:35:08.140 --> 00:35:16.220]   The sweeps, workspace, and reports functionality are really useful for self-driving problems
[00:35:16.220 --> 00:35:17.220]   I found.
[00:35:17.220 --> 00:35:18.220]   But they're also useful generally.
[00:35:18.220 --> 00:35:26.140]   I think from here I'll just hop into a report of mine that focuses on semantic segmentation.
[00:35:26.140 --> 00:35:29.500]   And we'll wait for it to load.
[00:35:29.500 --> 00:35:34.740]   Reports are specifically very useful to write down intermediate results and share them with
[00:35:34.740 --> 00:35:39.100]   colleagues or other folks who might be interested in your work.
[00:35:39.100 --> 00:35:43.980]   Here I first describe the task, which is taking these raw photos and then generating these
[00:35:43.980 --> 00:35:45.300]   predictions.
[00:35:45.300 --> 00:35:47.580]   And I can compare them to the ground truth.
[00:35:47.580 --> 00:35:49.340]   You'll notice that this is the old style.
[00:35:49.340 --> 00:35:53.820]   I'm not using the fancy interactive image masks because those didn't exist when I made
[00:35:53.820 --> 00:35:55.620]   this report.
[00:35:55.620 --> 00:35:57.100]   And you can see the model's doing pretty well.
[00:35:57.100 --> 00:35:59.700]   For example, it's getting these humans.
[00:35:59.700 --> 00:36:06.740]   But you can see here that this bicycle rider is not identified as a bike and a rider.
[00:36:06.740 --> 00:36:08.820]   It's just a human.
[00:36:08.820 --> 00:36:14.740]   And some of the details of these traffic posts aren't detected correctly.
[00:36:14.740 --> 00:36:17.220]   And you'll see there's some haziness here.
[00:36:17.220 --> 00:36:23.980]   And that's probably because the light quality is pretty weak in this original photo.
[00:36:23.980 --> 00:36:29.980]   Yeah, so you can see it's a little bit trickier to compare across these than having the overlaid
[00:36:29.980 --> 00:36:30.980]   masks.
[00:36:30.980 --> 00:36:34.020]   Maybe someday I'll upgrade this report.
[00:36:34.020 --> 00:36:36.300]   And here I talk a little bit through the task.
[00:36:36.300 --> 00:36:41.980]   I'm using code made by my colleague Boris that's a unit in Fast.ai.
[00:36:41.980 --> 00:36:47.060]   And I'm using different encoders to see basically how well I can identify different classes.
[00:36:47.060 --> 00:36:53.420]   And in the process of working on this, I noticed that although I was getting a high average
[00:36:53.420 --> 00:36:59.140]   accuracy, it was doing really well on cars and traffic signs, but really poorly on humans
[00:36:59.140 --> 00:37:02.820]   when I split it up into per class accuracy.
[00:37:02.820 --> 00:37:07.300]   So here, for example, you can see that all of the car accuracies for three different
[00:37:07.300 --> 00:37:13.460]   models in these solid lines are, you know, in the low 90s, even as high as 95, which
[00:37:13.460 --> 00:37:15.700]   is pretty good.
[00:37:15.700 --> 00:37:17.540]   The dashed lines are traffic.
[00:37:17.540 --> 00:37:20.220]   And that's like the most detailed and the hardest to get.
[00:37:20.220 --> 00:37:23.000]   So they're relatively lower across models.
[00:37:23.000 --> 00:37:25.020]   And then the dots is the overall accuracy.
[00:37:25.020 --> 00:37:30.460]   And that's, you know, coming in and around the high 80s across models, which is pretty
[00:37:30.460 --> 00:37:31.460]   good.
[00:37:31.460 --> 00:37:34.180]   But if you look at human accuracy, it's incredibly low.
[00:37:34.180 --> 00:37:39.060]   And partially this is because human pixels don't take up that much space in the data,
[00:37:39.060 --> 00:37:42.900]   you know, compared to the number of pixels that are occupied by cars or roads.
[00:37:42.900 --> 00:37:44.980]   Humans are just a very tiny fraction of that.
[00:37:44.980 --> 00:37:50.340]   So it also doesn't get to see that many examples.
[00:37:50.340 --> 00:37:53.900]   So one thing that I found a change that was really helpful was to look at intersection
[00:37:53.900 --> 00:37:57.500]   over union instead as the metric.
[00:37:57.500 --> 00:38:00.260]   And intersection over union is sort of like a Venn diagram.
[00:38:00.260 --> 00:38:04.420]   So instead of looking at the percentage of pixels that are correct, you look at the overlap
[00:38:04.420 --> 00:38:09.740]   between the predictions and the ground truth for a given class.
[00:38:09.740 --> 00:38:14.460]   And this helped me train models that were much more accurate overall.
[00:38:14.460 --> 00:38:20.420]   So one interesting detail that I found when I varied encoders for my unit is that the
[00:38:20.420 --> 00:38:27.820]   best performing model or encoder type for a human IOU was AlexNet.
[00:38:27.820 --> 00:38:33.780]   But overall, it was less good than a ResNet because it would predict these -- this is
[00:38:33.780 --> 00:38:39.940]   a representative example here -- these blocky humans and get a lot of intersection, right?
[00:38:39.940 --> 00:38:46.340]   But not actually capture any of the details and definitely not capture, you know, this
[00:38:46.340 --> 00:38:49.060]   tiny, tiny human in the background here.
[00:38:49.060 --> 00:38:53.140]   Maybe it's even a pair.
[00:38:53.140 --> 00:38:58.100]   And you know, this just shows some of the awesome report features to just do a diff
[00:38:58.100 --> 00:39:00.860]   of two of my model configs.
[00:39:00.860 --> 00:39:02.760]   Some more examples from AlexNet.
[00:39:02.760 --> 00:39:06.100]   Another mode that it would get into is just pick up on the very, very fine details of
[00:39:06.100 --> 00:39:12.100]   the photo instead of the broad categories.
[00:39:12.100 --> 00:39:13.100]   There's more examples here.
[00:39:13.100 --> 00:39:17.020]   And I'll scroll through to the part where I run a sweep.
[00:39:17.020 --> 00:39:22.620]   And a sweep is a nice UI for hyperparameter search from weights and biases where you can
[00:39:22.620 --> 00:39:27.220]   just specify some of the hyperparameters you want to explore, like learning decay -- or
[00:39:27.220 --> 00:39:33.780]   sorry -- learning rate, weight decay, training stages, and then launch agents that will basically
[00:39:33.780 --> 00:39:38.340]   try your same training script but with different hyperparameter values.
[00:39:38.340 --> 00:39:42.820]   And then you'll get nice visualizations of that training over time and this parallel
[00:39:42.820 --> 00:39:44.500]   coordinates plot.
[00:39:44.500 --> 00:39:51.340]   And every experiment that I ran is a line here that's connecting values on these vertical
[00:39:51.340 --> 00:39:52.660]   number lines.
[00:39:52.660 --> 00:39:56.820]   And you can see that the high accuracies are in yellow and the lower ones are in purple.
[00:39:56.820 --> 00:40:01.180]   And these are just failed experiments.
[00:40:01.180 --> 00:40:04.100]   And you can see that there's not a clear relationship with weight decay.
[00:40:04.100 --> 00:40:06.140]   It's sort of all over the place.
[00:40:06.140 --> 00:40:08.860]   We don't have a strong signal.
[00:40:08.860 --> 00:40:13.560]   For training stages, it looks like the lower ones are better, but also I tried a lot more
[00:40:13.560 --> 00:40:15.620]   experiments with lower values.
[00:40:15.620 --> 00:40:20.060]   And then for learning rate, it seems like the lower learning rate generally correlates
[00:40:20.060 --> 00:40:21.060]   with higher accuracy.
[00:40:21.060 --> 00:40:26.580]   And it's nice because you can click into any one of these experiments.
[00:40:26.580 --> 00:40:32.740]   I then -- the last awesome bit about sweeps is that we run a random forest for you in
[00:40:32.740 --> 00:40:35.780]   the background on your hyperparameters and your target metric.
[00:40:35.780 --> 00:40:40.780]   So here for human IOU, if that's the one I care about, it turns out that the AlexNet
[00:40:40.780 --> 00:40:47.980]   encoder is the highest predictor of good human IOU and it's very highly correlated here while
[00:40:47.980 --> 00:40:52.420]   decreasing the learning rate as we just saw from the sweep is the next most important
[00:40:52.420 --> 00:40:53.420]   thing.
[00:40:53.420 --> 00:40:56.340]   And it increased the number of training examples, et cetera.
[00:40:56.340 --> 00:41:00.540]   And weight decay, as you can see, doesn't really matter that much.
[00:41:00.540 --> 00:41:05.060]   What's cool is I can switch this and pick a different target metric, like average IOU
[00:41:05.060 --> 00:41:08.420]   across all of my classes, not just humans.
[00:41:08.420 --> 00:41:11.980]   And here we'll see that the learning rate matters a lot.
[00:41:11.980 --> 00:41:14.500]   The number of examples matters a lot.
[00:41:14.500 --> 00:41:19.260]   And then actually ResNet is more useful because it's a much more advanced model and more precise
[00:41:19.260 --> 00:41:20.260]   overall.
[00:41:20.260 --> 00:41:27.380]   Yeah, so that's that particular report.
[00:41:27.380 --> 00:41:30.980]   And these are just some screenshots of what I showed.
[00:41:30.980 --> 00:41:36.620]   I -- there's a really cool visual in sweeps that I want to briefly show which plots every
[00:41:36.620 --> 00:41:40.860]   experiment over time as it comes in.
[00:41:40.860 --> 00:41:43.020]   These are just some metrics for some of my experiments.
[00:41:43.020 --> 00:41:45.340]   So training loss, car accuracy.
[00:41:45.340 --> 00:41:48.940]   And it's awesome that you can customize these in a dashboard.
[00:41:48.940 --> 00:41:49.940]   Here it is.
[00:41:49.940 --> 00:41:54.620]   These are all the experiments I ran, all the different combinations of hyperparameters.
[00:41:54.620 --> 00:41:59.460]   And you can see the IOU, which is my target metric, improving over time.
[00:41:59.460 --> 00:42:02.060]   And the best run is this one.
[00:42:02.060 --> 00:42:08.820]   And I can find that configuration of hyperparameters and use it in my future experiments.
[00:42:08.820 --> 00:42:12.060]   So all of this is ongoing work.
[00:42:12.060 --> 00:42:15.780]   And I'm happy to take any questions.
[00:42:15.780 --> 00:42:22.740]   But it's a reasonable overview of the kinds of things you can do with Bias's tools for
[00:42:22.740 --> 00:42:23.740]   self-driving.
[00:42:23.740 --> 00:42:28.100]   Though, of course, I have a whole appendix of other fun subproblems in self-driving that
[00:42:28.100 --> 00:42:30.060]   can go on for much longer.
[00:42:30.060 --> 00:42:31.060]   Wow.
[00:42:31.060 --> 00:42:32.060]   Yeah.
[00:42:32.060 --> 00:42:39.900]   I'm always impressed by just the, like, breadth of problems that you've tackled using the
[00:42:39.900 --> 00:42:40.900]   tool.
[00:42:40.900 --> 00:42:41.900]   It's -- yeah.
[00:42:41.900 --> 00:42:49.460]   As somebody who has done a lot of more theoretical machine learning work in which it's endless
[00:42:49.460 --> 00:42:56.380]   and C-far most of the time, it's wild to see just how much else there is out there.
[00:42:56.380 --> 00:43:06.420]   So what would you say is the biggest insight that you've gained from using the Sweeps tool
[00:43:06.420 --> 00:43:11.060]   on your self-driving problems?
[00:43:11.060 --> 00:43:15.880]   Or I guess -- or even more broadly, can you think of one specific instance of really strong
[00:43:15.880 --> 00:43:20.980]   insight driven by -- in any of the examples that you showed that you want to highlight?
[00:43:20.980 --> 00:43:21.980]   Yeah.
[00:43:21.980 --> 00:43:22.980]   Definitely.
[00:43:22.980 --> 00:43:23.980]   That's a great question.
[00:43:23.980 --> 00:43:28.800]   So initially, I was running these Sweeps to maximize accuracy.
[00:43:28.800 --> 00:43:33.860]   And I was then logging separately my metrics for accuracy on different classes.
[00:43:33.860 --> 00:43:38.380]   And I was noticing that I was trying a bunch of stuff, but basically there was no signal
[00:43:38.380 --> 00:43:40.380]   for human accuracy.
[00:43:40.380 --> 00:43:46.980]   And when I switched to IOUs, I actually set my search space wide enough that some of the
[00:43:46.980 --> 00:43:51.340]   Sweeps generated signal for human IOU.
[00:43:51.340 --> 00:44:00.740]   And these were combinations of sampling and settings that would actually give, you know,
[00:44:00.740 --> 00:44:02.180]   some signal on this.
[00:44:02.180 --> 00:44:03.860]   And that was really cool to see.
[00:44:03.860 --> 00:44:07.420]   Although a lot of them have AlexNet as the encoder.
[00:44:07.420 --> 00:44:12.020]   So then there's a question of, you know, focusing in on the search space, seeing what the best
[00:44:12.020 --> 00:44:14.580]   ResNet ones are.
[00:44:14.580 --> 00:44:20.100]   But it's really awesome to be able to let the Sweep run and then discover a direction
[00:44:20.100 --> 00:44:24.780]   to follow based on what that Sweep finds instead of manually trying it myself.
[00:44:24.780 --> 00:44:25.780]   I see.
[00:44:25.780 --> 00:44:26.780]   Yeah.
[00:44:26.780 --> 00:44:28.540]   But in a very sort of iterative way.
[00:44:28.540 --> 00:44:29.540]   Like you try something.
[00:44:29.540 --> 00:44:30.940]   There's a whole bunch of things.
[00:44:30.940 --> 00:44:35.700]   Maybe, you know, a lot of your runs fail, you know, some more than others.
[00:44:35.700 --> 00:44:39.540]   But then that tells you where to go next, maybe.
[00:44:39.540 --> 00:44:40.540]   Yeah, exactly.
[00:44:40.540 --> 00:44:43.540]   And even configure, I call this iterative Sweeps.
[00:44:43.540 --> 00:44:47.460]   You know, you have some findings from a stage of experiments, and then you set a new search
[00:44:47.460 --> 00:44:50.380]   space or go deeper in one direction.
[00:44:50.380 --> 00:44:51.380]   Exactly.
[00:44:51.380 --> 00:44:52.380]   Yeah.
[00:44:52.380 --> 00:44:59.580]   So, folks in the Zoom or on the YouTube, if you have questions, go ahead and submit them.
[00:44:59.580 --> 00:45:03.160]   But while you're doing that, I'm going to take a look at the responses to our survey
[00:45:03.160 --> 00:45:09.380]   and we'll come back to them, come back to Stacey with any questions that pop up.
[00:45:09.380 --> 00:45:15.740]   So, it looks like we got an exactly equal number of answers for the two possibilities.
[00:45:15.740 --> 00:45:19.460]   So, the reveal step here is maybe a little bit less exciting than it could be.
[00:45:19.460 --> 00:45:21.740]   But let's see what people said.
[00:45:21.740 --> 00:45:23.860]   All right.
[00:45:23.860 --> 00:45:28.100]   It was an option actually to pick both yes and no, if you wanted.
[00:45:28.100 --> 00:45:34.380]   So, that maybe, you know, it does not compute sort of divide by zero kind of answer.
[00:45:34.380 --> 00:45:39.780]   But yeah, it looks like people picked people are about equally about an equal number of
[00:45:39.780 --> 00:45:43.140]   people think it's definitely going to happen and it's definitely not going to happen by
[00:45:43.140 --> 00:45:44.140]   2025.
[00:45:44.140 --> 00:45:49.060]   I'd be interested to know, maybe people can drop this in the chat somewhere, whether people
[00:45:49.060 --> 00:45:56.420]   think that maybe my date was too aggressive or that maybe it's 2030 or 2035 or if people
[00:45:56.420 --> 00:45:57.420]   think that that's totally impossible.
[00:45:57.420 --> 00:46:01.540]   So, if you have thoughts about that, drop that in the chat.
[00:46:01.540 --> 00:46:04.540]   We do have a question in the Q&A for Stacey.
[00:46:04.540 --> 00:46:10.260]   So, aside from AlexNet, which CNN backbone did you find that provides the best results
[00:46:10.260 --> 00:46:11.860]   for your image segmentation tasks?
[00:46:11.860 --> 00:46:17.820]   And do you think that there's sort of like a single answer to that for all kinds of tasks?
[00:46:17.820 --> 00:46:22.740]   I think it's hard to say that it'll be for all kinds of tasks.
[00:46:22.740 --> 00:46:32.340]   The ones that I tried here were AlexNet, ResNet-34, and a smaller number for ResNet-18, I believe.
[00:46:32.340 --> 00:46:38.020]   And you know, ResNet-34 does the best across classes and AlexNet has this edge on humans
[00:46:38.020 --> 00:46:42.560]   because it actually predicts these large blocky regions.
[00:46:42.560 --> 00:46:48.980]   One interesting thing is with semantic segmentation, my batch size needs to be a lot smaller than
[00:46:48.980 --> 00:46:53.220]   the kinds of CNN problems that I'm used to because there's just so much signal of prediction
[00:46:53.220 --> 00:46:55.740]   for each pixel.
[00:46:55.740 --> 00:47:01.900]   And more complicated encoders would just require a little bit more optimization in order to
[00:47:01.900 --> 00:47:06.460]   train at a reasonable speed on my GPUs.
[00:47:06.460 --> 00:47:08.020]   Interesting.
[00:47:08.020 --> 00:47:18.060]   Yeah, it seems plausible to me that, yeah, there's not necessarily one right answer.
[00:47:18.060 --> 00:47:25.580]   That's probably a good idea to try out multiple models from the model zoo for every task,
[00:47:25.580 --> 00:47:27.580]   every time you come up with something new.
[00:47:27.580 --> 00:47:28.580]   Definitely.
[00:47:28.580 --> 00:47:33.580]   And I tried a bunch of the default available ones in Fast.ai and you could customize from
[00:47:33.580 --> 00:47:35.580]   there for sure.
[00:47:35.580 --> 00:47:36.580]   Interesting.
[00:47:36.580 --> 00:47:37.580]   Yeah.
[00:47:37.580 --> 00:47:43.840]   But the thing about AlexNet predicting relatively blocky regions, that's an architectural constraint
[00:47:43.840 --> 00:47:54.260]   of AlexNet, right, just due to the numbers on the convolutions in the pools?
[00:47:54.260 --> 00:48:02.260]   I think we could get into more finer architectural distinctions, but I think overall it does
[00:48:02.260 --> 00:48:07.380]   less well as an older and much simpler architecture, or at least that's the way that I understand
[00:48:07.380 --> 00:48:08.380]   it.
[00:48:08.380 --> 00:48:12.980]   So when we're looking at these giant images and these fine details, it's not generalizing
[00:48:12.980 --> 00:48:13.980]   as well.
[00:48:13.980 --> 00:48:14.980]   I see.
[00:48:14.980 --> 00:48:15.980]   Yeah.
[00:48:15.980 --> 00:48:20.660]   Just curious if that AlexNet finding would be just like a general property that would
[00:48:20.660 --> 00:48:24.660]   be sometimes good, sometimes bad, depending on your task.
[00:48:24.660 --> 00:48:25.660]   Yeah.
[00:48:25.660 --> 00:48:26.660]   Yeah.
[00:48:26.660 --> 00:48:34.540]   And it's possible to fine tune this particular AlexNet version for this encoder in a better
[00:48:34.540 --> 00:48:35.540]   way.
[00:48:35.540 --> 00:48:37.540]   I'm just taking the pre-trained version.
[00:48:37.540 --> 00:48:38.540]   Gotcha.
[00:48:38.540 --> 00:48:39.540]   Cool.
[00:48:39.540 --> 00:48:40.540]   All right.
[00:48:40.540 --> 00:48:47.140]   Thanks, Stacy, and thanks, Han, for the great question.
[00:48:47.140 --> 00:48:51.220]   And I think it is my turn.
[00:48:51.220 --> 00:48:54.400]   So all right.
[00:48:54.400 --> 00:48:58.580]   So this talk, let me pull up my slides.
[00:48:58.580 --> 00:49:03.940]   Think I've got that going for everybody.
[00:49:03.940 --> 00:49:09.580]   So taking us a little bit out of the realm of applications that Mohamed Reza and Stacy
[00:49:09.580 --> 00:49:16.540]   had us in, I wanted to talk about a core algorithm, a core idea from linear algebra that shows
[00:49:16.540 --> 00:49:20.420]   up in machine learning quite a bit, the singular value decomposition.
[00:49:20.420 --> 00:49:24.640]   And what I want to convince you by the end of this talk is that the singular value decomposition
[00:49:24.640 --> 00:49:28.000]   is a refactor.
[00:49:28.000 --> 00:49:32.460]   So the singular value decomposition, if you're not familiar with it, it's a very important
[00:49:32.460 --> 00:49:35.420]   algorithm, just some examples of the SVD in action.
[00:49:35.420 --> 00:49:38.300]   Principal components analysis is an application of it.
[00:49:38.300 --> 00:49:43.780]   The discrete Fourier transform is an application of singular value decomposition, which includes
[00:49:43.780 --> 00:49:47.860]   JPEG, the image encoding technique.
[00:49:47.860 --> 00:49:51.380]   Computing the pseudo inverse, which actually came up, I don't know if you noticed it, in
[00:49:51.380 --> 00:49:55.940]   Mohamed Reza's talk, computing the pseudo inverse was an important part of the RBF training
[00:49:55.940 --> 00:49:56.940]   step.
[00:49:56.940 --> 00:50:02.100]   And computing the pseudo inverse is typically done via the singular value decomposition.
[00:50:02.100 --> 00:50:06.700]   That is related to why it shows up when we try and solve linear regression.
[00:50:06.700 --> 00:50:11.420]   If all those examples seem a little bit too perhaps academic for you, video for round
[00:50:11.420 --> 00:50:19.260]   isolation, recommendation engines, page rank, and more, all these very useful application
[00:50:19.260 --> 00:50:24.080]   domains, the singular value decomposition shows up.
[00:50:24.080 --> 00:50:28.580]   And despite that broad array of applications, it's actually deceptively simple.
[00:50:28.580 --> 00:50:33.260]   The singular value decomposition simply states that we can break a matrix down into three
[00:50:33.260 --> 00:50:39.500]   particular pieces, which usually go by the names U, sigma, and V transpose, where U is
[00:50:39.500 --> 00:50:42.180]   a matrix that's usually kind of tall and skinny.
[00:50:42.180 --> 00:50:47.600]   V transpose is a matrix that's kind of wide and short, a squat.
[00:50:47.600 --> 00:50:53.500]   And then in between is a matrix sigma that is a diagonal matrix.
[00:50:53.500 --> 00:50:56.680]   So this is the singular value decomposition.
[00:50:56.680 --> 00:51:01.140]   And this way of writing it sort of emphasizes the idea that this matrix M on the left is
[00:51:01.140 --> 00:51:08.220]   equal to the product of the matrices on the right.
[00:51:08.220 --> 00:51:11.180]   But I think there's a slightly different view that I want to take that's going to lead us
[00:51:11.180 --> 00:51:18.420]   to this view of SVD not as an algebraic equation, but instead as a process akin to refactoring
[00:51:18.420 --> 00:51:19.420]   and programming.
[00:51:19.420 --> 00:51:22.940]   And first, we need to change a little bit the way we think about matrices.
[00:51:22.940 --> 00:51:28.140]   So matrix vector multiplication is more like function application than it is like the multiplication
[00:51:28.140 --> 00:51:29.720]   of scalars.
[00:51:29.720 --> 00:51:33.100]   We should think of a matrix not just as an array of numbers, though sometimes thinking
[00:51:33.100 --> 00:51:38.580]   of it that way is helpful, but instead as an object that takes in certain kinds of inputs
[00:51:38.580 --> 00:51:41.260]   and returns certain kinds of outputs.
[00:51:41.260 --> 00:51:47.020]   So on this slide, I'm showing a matrix M in red, taking in a vector X in blue, and returning
[00:51:47.020 --> 00:51:55.540]   an output, which we call M times X, that is in purple and is also an array.
[00:51:55.540 --> 00:51:58.940]   So this view, sometimes it's useful to think of matrices as a bunch of numbers.
[00:51:58.940 --> 00:52:04.760]   Other times, it's useful to think of them as functions that act on vectors.
[00:52:04.760 --> 00:52:09.840]   So this singular value decomposition is a decomposition of a function.
[00:52:09.840 --> 00:52:14.980]   It's taking the matrix M and undoing the process of composing it.
[00:52:14.980 --> 00:52:17.140]   Functions are composed.
[00:52:17.140 --> 00:52:20.860]   They're combined together, one function after another.
[00:52:20.860 --> 00:52:25.340]   And the end result of composing a bunch of functions is also a function.
[00:52:25.340 --> 00:52:29.340]   So that's expressed by this diagram on the right-hand side here.
[00:52:29.340 --> 00:52:36.800]   So the way to read a diagram like this, let me pull up my laser pointer, is that we start
[00:52:36.800 --> 00:52:42.120]   up here in the top left with an array with N entries, and M is a function represented
[00:52:42.120 --> 00:52:51.120]   by an arrow that takes us from N entry arrays to M entry arrays, lowercase M on the size
[00:52:51.120 --> 00:52:53.020]   of the array.
[00:52:53.020 --> 00:52:58.820]   But we can alternatively think of this arrow here as a sequence of arrows, as a sequence
[00:52:58.820 --> 00:53:04.420]   of steps, V transpose followed by sigma followed by U.
[00:53:04.420 --> 00:53:09.240]   So the content that is on the left-hand side here expresses an equation is also expressed
[00:53:09.240 --> 00:53:10.820]   by this diagram here.
[00:53:10.820 --> 00:53:16.020]   These diagrams, commutative diagrams is what they're called, say that if I follow these
[00:53:16.020 --> 00:53:22.980]   arrows and end up at the same place as following any other path, then I must get the same result.
[00:53:22.980 --> 00:53:30.780]   So that's saying that V transpose followed by sigma followed by U is the same as M.
[00:53:30.780 --> 00:53:38.660]   So if we think of our singular value decomposition as a decomposition, as the reversal of composition,
[00:53:38.660 --> 00:53:42.460]   the connection to refactoring becomes a little bit more clear.
[00:53:42.460 --> 00:53:45.380]   Refactoring programs involves a couple of common tricks.
[00:53:45.380 --> 00:53:49.220]   Separation of concerns to say this function does one thing and this function does another,
[00:53:49.220 --> 00:53:55.100]   I don't mix them all together, peas and carrots together, cats and dogs lying together, these
[00:53:55.100 --> 00:53:58.440]   sorts of awful things.
[00:53:58.440 --> 00:54:00.620]   We also sort of remove code that doesn't do anything.
[00:54:00.620 --> 00:54:02.460]   Oh, I don't actually need to check that.
[00:54:02.460 --> 00:54:05.240]   It's guaranteed not to have that value.
[00:54:05.240 --> 00:54:06.300]   This code is dead.
[00:54:06.300 --> 00:54:08.700]   It no longer refers to anything of use.
[00:54:08.700 --> 00:54:10.980]   Those are examples of refactoring tricks.
[00:54:10.980 --> 00:54:15.760]   Breaking up into functions is another important refactoring trick, decomposing functions that
[00:54:15.760 --> 00:54:19.500]   have been already sort of pushed together too much.
[00:54:19.500 --> 00:54:22.620]   And these all have direct equivalence for matrices.
[00:54:22.620 --> 00:54:27.580]   So for separation of concerns, we have eigenvectors and eigendecomposition.
[00:54:27.580 --> 00:54:32.740]   Sort of split up a matrix from a whole bunch of things happening all at once to, you know,
[00:54:32.740 --> 00:54:37.440]   n things happening on n different eigenspaces or eigenvectors.
[00:54:37.440 --> 00:54:41.200]   For throwing out code that doesn't do anything, we have low rank approximation.
[00:54:41.200 --> 00:54:42.620]   This part of the matrix is unimportant.
[00:54:42.620 --> 00:54:43.860]   I will throw it out.
[00:54:43.860 --> 00:54:45.380]   This part of the code doesn't do anything.
[00:54:45.380 --> 00:54:46.640]   I will delete it.
[00:54:46.640 --> 00:54:52.740]   And for breaking up into functions or decomposing, we have singular value decomposition.
[00:54:52.740 --> 00:54:56.540]   When we refactor software, this decomposition step looks something like this.
[00:54:56.540 --> 00:55:00.000]   I have some function here on the right that's very tersely written.
[00:55:00.000 --> 00:55:06.340]   This one here returns the string true if the input is odd and the string false if the input
[00:55:06.340 --> 00:55:07.340]   is not odd.
[00:55:07.340 --> 00:55:12.060]   I can break that up into three steps, just sort of looking at this guy and saying, okay,
[00:55:12.060 --> 00:55:14.380]   what are the pieces of this function?
[00:55:14.380 --> 00:55:21.860]   First, we use mod two to sort of check whether the input is divisible by two or not.
[00:55:21.860 --> 00:55:26.780]   Then we treat that value as a Boolean, not just as a number.
[00:55:26.780 --> 00:55:29.900]   Not just as zero or one, but as true or false.
[00:55:29.900 --> 00:55:31.380]   That's in the if else step.
[00:55:31.380 --> 00:55:34.380]   And then finally, we return a string for our final output.
[00:55:34.380 --> 00:55:40.420]   So we return true if the Boolean is true and we return false as a string if the Boolean
[00:55:40.420 --> 00:55:41.420]   is false.
[00:55:41.420 --> 00:55:44.100]   We can break those out and make those all separate functions.
[00:55:44.100 --> 00:55:48.780]   And now our is odd function at the top is a sequence of three operations that are written
[00:55:48.780 --> 00:55:55.260]   out rather than the sort of tightly compacted thing that's on the left.
[00:55:55.260 --> 00:55:58.300]   It's a matter of taste, maybe which of these is the right way to write it.
[00:55:58.300 --> 00:56:00.260]   Maybe neither is really the right way to write it.
[00:56:00.260 --> 00:56:02.500]   But the important thing is that we can do it.
[00:56:02.500 --> 00:56:04.800]   We can split this function up if we want.
[00:56:04.800 --> 00:56:09.300]   And we can take this function, this set of functions, and we can collapse them down into
[00:56:09.300 --> 00:56:10.300]   a single one.
[00:56:10.300 --> 00:56:15.020]   And this is the process of refactoring and rewriting software.
[00:56:15.020 --> 00:56:20.240]   And in this case, one function is being decomposed into three pieces.
[00:56:20.240 --> 00:56:24.720]   So this should remind you a little bit of that diagram of the SVD I showed just a few
[00:56:24.720 --> 00:56:25.720]   slides ago.
[00:56:25.720 --> 00:56:29.660]   So we start off with an integer that we want to check whether it's odd.
[00:56:29.660 --> 00:56:33.460]   And if we feed it to the function is odd, what we should get out is a string, true or
[00:56:33.460 --> 00:56:34.460]   false.
[00:56:34.460 --> 00:56:40.220]   Maybe we're printing this to a user who wants to know whether their input is odd or not.
[00:56:40.220 --> 00:56:44.140]   And what this diagram is saying is that we can apply three separate functions and get
[00:56:44.140 --> 00:56:45.620]   the exact same result.
[00:56:45.620 --> 00:56:51.700]   So this way of writing is odd really emphasizes that it is implemented in this particular
[00:56:51.700 --> 00:56:55.820]   way as mod two, then two bool, then two string.
[00:56:55.820 --> 00:57:02.300]   Mod two, the two bool function checks whether that mod two output is equal to one.
[00:57:02.300 --> 00:57:08.280]   And the two string one just takes that Boolean and turns it into a string.
[00:57:08.280 --> 00:57:11.280]   So let's talk about what those three pieces are.
[00:57:11.280 --> 00:57:16.920]   So the way of understanding those three pieces in a way that is going to generalize is to
[00:57:16.920 --> 00:57:19.280]   say that they do three separate things.
[00:57:19.280 --> 00:57:23.240]   That mod two picks out sort of representative examples.
[00:57:23.240 --> 00:57:25.740]   That two bool reversibly renames them.
[00:57:25.740 --> 00:57:30.500]   And that two string then gives them the right type for the output.
[00:57:30.500 --> 00:57:35.080]   So to be more specific, when I say that mod two picks representatives, what I mean is
[00:57:35.080 --> 00:57:38.520]   that we need a representative for each output.
[00:57:38.520 --> 00:57:43.960]   One representative integer that is the example of an odd number.
[00:57:43.960 --> 00:57:46.640]   And one that is the example of an even number.
[00:57:46.640 --> 00:57:51.960]   So mod two corresponds to picking as our examples zero and one.
[00:57:51.960 --> 00:57:54.640]   Somebody else might use one and two as their examples.
[00:57:54.640 --> 00:57:57.080]   They would have a different function than mod two.
[00:57:57.080 --> 00:58:00.880]   But they would be doing in this decomposition the same thing.
[00:58:00.880 --> 00:58:06.200]   I want to pick out a representative even number and odd number.
[00:58:06.200 --> 00:58:10.240]   Because that's what -- those are the things that I give different answers for.
[00:58:10.240 --> 00:58:12.000]   And it simplifies our function down.
[00:58:12.000 --> 00:58:16.520]   We just need to know how it works on two different inputs rather than on every possible input.
[00:58:16.520 --> 00:58:23.160]   And then we need to know which group each input falls into.
[00:58:23.160 --> 00:58:27.360]   So then two bool reversibly renames those inputs.
[00:58:27.360 --> 00:58:31.960]   So it associates each representative with its output, sort of one to one.
[00:58:31.960 --> 00:58:36.800]   In a way such that each output is targeted by exactly one representative.
[00:58:36.800 --> 00:58:38.960]   So we have zero and one.
[00:58:38.960 --> 00:58:44.040]   We have an example odd number and we have an example even number.
[00:58:44.040 --> 00:58:48.320]   And now we just need to sort of be like, okay, which one of these is which?
[00:58:48.320 --> 00:58:51.120]   Is one the odd number or is zero the odd number?
[00:58:51.120 --> 00:58:52.400]   So one is the odd number.
[00:58:52.400 --> 00:58:53.400]   So it goes to true.
[00:58:53.400 --> 00:58:55.240]   And zero goes to false.
[00:58:55.240 --> 00:58:58.000]   And that's the step here, two bool.
[00:58:58.000 --> 00:59:00.360]   And then finally we need to put them in the correct type.
[00:59:00.360 --> 00:59:03.400]   So our output there was not necessarily the right type.
[00:59:03.400 --> 00:59:05.960]   It was just true and false.
[00:59:05.960 --> 00:59:10.800]   And so we need to recognize the output of our function inside of our output type.
[00:59:10.800 --> 00:59:13.560]   There are many strings besides just true and false.
[00:59:13.560 --> 00:59:16.520]   There's through and tals for one.
[00:59:16.520 --> 00:59:22.200]   There's the -- you know, the -- all kinds of books and the content of this presentation.
[00:59:22.200 --> 00:59:24.400]   All of that is inside the type string.
[00:59:24.400 --> 00:59:26.960]   Lots of stuff besides true and false.
[00:59:26.960 --> 00:59:31.080]   So we need to find inside true, we have to find the outputs that we want to associate
[00:59:31.080 --> 00:59:32.600]   with our two values.
[00:59:32.600 --> 00:59:36.120]   So we just say the boolean true becomes the string true.
[00:59:36.120 --> 00:59:38.720]   The boolean false becomes the string false.
[00:59:38.720 --> 00:59:40.200]   Pretty straightforward.
[00:59:40.200 --> 00:59:42.480]   Relatively simple step.
[00:59:42.480 --> 00:59:45.360]   And this may seem a little bit trivial with this function.
[00:59:45.360 --> 00:59:50.640]   But any function, you can do this three-step composition where I take some function func
[00:59:50.640 --> 00:59:55.440]   and first I do an onto transformation where I pick out representatives.
[00:59:55.440 --> 01:00:00.200]   So I have a representative for each type of output I'm going to give.
[01:00:00.200 --> 01:00:04.080]   Then I apply a reversible function that says, okay, this representative is the one that
[01:00:04.080 --> 01:00:05.480]   gets this output.
[01:00:05.480 --> 01:00:07.840]   You know, one is an even number.
[01:00:07.840 --> 01:00:12.840]   Two -- or zero is -- sorry, one is an odd number.
[01:00:12.840 --> 01:00:14.760]   Zero is an even number.
[01:00:14.760 --> 01:00:16.680]   That's the reversible step.
[01:00:16.680 --> 01:00:21.760]   And then lastly, we have a one-to-one step where we sort of recognize that output that
[01:00:21.760 --> 01:00:26.600]   we had, which might not be every possible value of the output type.
[01:00:26.600 --> 01:00:30.040]   We recognize that as a subset of the output type.
[01:00:30.040 --> 01:00:32.080]   So that's the final step here.
[01:00:32.080 --> 01:00:39.680]   I broke it down into foobarbaz for cultural reasons of how programmers like to write these
[01:00:39.680 --> 01:00:42.160]   things.
[01:00:42.160 --> 01:00:48.360]   But those three steps onto reversible one-to-one are a general way to break down literally
[01:00:48.360 --> 01:00:49.840]   any function.
[01:00:49.840 --> 01:00:54.200]   So you can -- you know, if you need to refactor a function, consider this three-step breakdown
[01:00:54.200 --> 01:00:58.760]   as a way of breaking down the function.
[01:00:58.760 --> 01:01:02.080]   So any matrix can also be decomposed into three pieces.
[01:01:02.080 --> 01:01:08.120]   And because matrices are simpler than any possible function, we can do a lot more than
[01:01:08.120 --> 01:01:14.200]   just breaking them down into those three very generic types of pieces.
[01:01:14.200 --> 01:01:21.480]   So if we apply this decomposition to a matrix, we get that -- we can split it up into three
[01:01:21.480 --> 01:01:22.480]   pieces.
[01:01:22.480 --> 01:01:26.600]   A wide matrix, a square matrix, and a tall matrix.
[01:01:26.600 --> 01:01:30.240]   The wide matrix has inputs that are bigger than its outputs.
[01:01:30.240 --> 01:01:33.200]   How wide a matrix is tell you how big its inputs are.
[01:01:33.200 --> 01:01:36.760]   How tall it is tell you how big its outputs are.
[01:01:36.760 --> 01:01:42.480]   And so what this is doing is it's doing the same thing of picking representatives that
[01:01:42.480 --> 01:01:46.440]   was done in the previous example by mod two.
[01:01:46.440 --> 01:01:50.400]   So for linear functions, for things that are implemented by matrices, if two inputs are
[01:01:50.400 --> 01:01:55.160]   sent to the same output by the matrix, both of them are sent to zero.
[01:01:55.160 --> 01:01:58.440]   And zero is sent to zero by any linear function.
[01:01:58.440 --> 01:02:04.040]   And so what this is basically doing is this matrix is going to take everything that gets
[01:02:04.040 --> 01:02:10.160]   sent to zero by m and send it to zero, but basically not do much else.
[01:02:10.160 --> 01:02:11.600]   So that's the first step.
[01:02:11.600 --> 01:02:14.220]   That's the pick representatives step.
[01:02:14.220 --> 01:02:17.120]   Then we have the reversible relabeling step.
[01:02:17.120 --> 01:02:18.680]   So that's the matrix in the middle.
[01:02:18.680 --> 01:02:22.480]   Here it's B. That one's -- it's reversible, so it's square.
[01:02:22.480 --> 01:02:24.840]   Its inputs are the same size as its outputs.
[01:02:24.840 --> 01:02:28.560]   If it were not square, then there would be no way to match inputs and outputs.
[01:02:28.560 --> 01:02:33.840]   Sort of a you can't put toothpaste back in the tube kind of principle.
[01:02:33.840 --> 01:02:37.240]   So this is our relabeling step, this middle one.
[01:02:37.240 --> 01:02:40.920]   This is where we're sort of really doing most of the meat of the work.
[01:02:40.920 --> 01:02:45.640]   And then finally, we have this tall matrix where the outputs can be bigger than the inputs.
[01:02:45.640 --> 01:02:50.760]   And this one basically finds a copy of all arrays with some number of entries among the
[01:02:50.760 --> 01:02:53.800]   set of all arrays with a larger number of entries.
[01:02:53.800 --> 01:02:58.680]   So I could do that where, like, maybe the first r entries are all -- all have a value,
[01:02:58.680 --> 01:03:03.240]   and then the last m minus r are all zero, or they're all two, or they're all whatever
[01:03:03.240 --> 01:03:04.840]   I want.
[01:03:04.840 --> 01:03:09.800]   And then -- or I could do it where, you know, the last r are the ones that have entries.
[01:03:09.800 --> 01:03:15.320]   It's going to depend on exactly the details of the matrix m and what function it does.
[01:03:15.320 --> 01:03:24.060]   But basically, A is going to find a copy of this r dimensional space that our middle matrix
[01:03:24.060 --> 01:03:26.440]   mapped onto inside the outputs.
[01:03:26.440 --> 01:03:31.680]   And it will -- it's going to do that in such a way that it gets the same answers as m applied
[01:03:31.680 --> 01:03:34.160]   to the original input.
[01:03:34.160 --> 01:03:37.720]   So this is the part where we recognize that, okay, true and false aren't the only strings
[01:03:37.720 --> 01:03:38.760]   that exist.
[01:03:38.760 --> 01:03:41.040]   There are many strings out there.
[01:03:41.040 --> 01:03:45.000]   But -- so let's take true and false and turn them into strings, even though we know our
[01:03:45.000 --> 01:03:51.040]   function, you know, can't produce all possible strings.
[01:03:51.040 --> 01:03:54.400]   So in this breakdown, that number r there is the rank of the matrix.
[01:03:54.400 --> 01:04:00.040]   It's the dimension of things that don't get mapped to zero by that matrix.
[01:04:00.040 --> 01:04:06.000]   And that is the -- that's going to be the size of the input of that last array there,
[01:04:06.000 --> 01:04:07.240]   A.
[01:04:07.240 --> 01:04:10.600]   And so in order to get the singular value decomposition, we just need to make some special
[01:04:10.600 --> 01:04:11.600]   choices, right?
[01:04:11.600 --> 01:04:14.000]   I said A, B, and C were pretty free to change around.
[01:04:14.000 --> 01:04:18.840]   I could multiply C by a number and divide A by a number, and I would get the same result.
[01:04:18.840 --> 01:04:22.240]   So if we make some specific choices, then we get the singular value decomposition.
[01:04:22.240 --> 01:04:29.640]   If we make B diagonal and we make A and C unitary, then we get this SVD.
[01:04:29.640 --> 01:04:33.680]   So unitary means no growing or shrinking of anything.
[01:04:33.680 --> 01:04:40.000]   So one motivation for doing that is that our diagonal matrix in the middle, that B, can
[01:04:40.000 --> 01:04:41.920]   do all the growing and shrinking for us, right?
[01:04:41.920 --> 01:04:47.560]   Our first step was just picking out representatives, and our last step was just recognizing our
[01:04:47.560 --> 01:04:51.680]   outputs in -- like, our outputs inside another type.
[01:04:51.680 --> 01:04:54.120]   And so there's no need to really grow or shrink there.
[01:04:54.120 --> 01:05:01.040]   So we can put that all in the middle in that diagonal matrix.
[01:05:01.040 --> 01:05:05.640]   So the growing, shrinking -- so no growing and shrinking, but maybe some turning around
[01:05:05.640 --> 01:05:08.800]   and some reflecting and things like that.
[01:05:08.800 --> 01:05:10.520]   That's what's allowed by unitary transformation.
[01:05:10.520 --> 01:05:16.520]   And if we do that, that gives this generic style of decomposition that can be applied
[01:05:16.520 --> 01:05:18.840]   to any possible function.
[01:05:18.840 --> 01:05:23.240]   If we make these specific choices, then we get the singular value decomposition as the
[01:05:23.240 --> 01:05:25.560]   output.
[01:05:25.560 --> 01:05:30.200]   As a technical note, this specific choice of the exact shapes here is what's called
[01:05:30.200 --> 01:05:31.440]   a compact SVD.
[01:05:31.440 --> 01:05:35.920]   So you might see something slightly different, but the principle of breaking a matrix down
[01:05:35.920 --> 01:05:41.240]   into these three pieces that are doing effectively the same three things is going to hold no
[01:05:41.240 --> 01:05:43.840]   matter what kind of SVD it is that you're computing.
[01:05:43.840 --> 01:05:48.000]   The compact SVD happens to be most useful when you're doing maybe a little bit more
[01:05:48.000 --> 01:05:52.120]   algebra type stuff, whereas the other kinds are maybe more useful when you're doing actual
[01:05:52.120 --> 01:05:55.760]   numerical computing.
[01:05:55.760 --> 01:06:01.840]   So this is just sort of one perspective on the SVD, and it's an uncommon one.
[01:06:01.840 --> 01:06:06.280]   I came up with it while trying to understand pseudoinverses and then found a couple of
[01:06:06.280 --> 01:06:11.000]   other people talking about this, but it doesn't seem to be the most common way to think about
[01:06:11.000 --> 01:06:12.000]   the SVD.
[01:06:12.000 --> 01:06:16.400]   But I do think it gives me a sort of different and interesting insight into what's going
[01:06:16.400 --> 01:06:18.280]   on.
[01:06:18.280 --> 01:06:22.960]   And I guess the only last point I'd make is that this style of decomposition comes from
[01:06:22.960 --> 01:06:28.200]   something called the first isomorphism theorem, which gets used in abstract algebra all over
[01:06:28.200 --> 01:06:32.520]   the place and actually gives you different insights depending on which structure it is
[01:06:32.520 --> 01:06:34.000]   you apply it to.
[01:06:34.000 --> 01:06:40.280]   And it tends to show up and sort of develop really interesting concepts no matter what
[01:06:40.280 --> 01:06:43.880]   it is you apply it to, whether it's group theory or fields or ring theory or whatever
[01:06:43.880 --> 01:06:45.680]   it is you're doing with your algebra.
[01:06:45.680 --> 01:06:51.520]   This first isomorphism theorem shows up, and it gets generalized very broadly in category
[01:06:51.520 --> 01:06:57.040]   theory to be applicable to just an absolute bewildering array of mathematical objects.
[01:06:57.040 --> 01:07:01.760]   And every time it gets applied, a new interesting concept pops up.
[01:07:01.760 --> 01:07:05.880]   So it's cool to me that this singular value decomposition, which is also this workhorse
[01:07:05.880 --> 01:07:15.160]   algorithm, shows up among these as this deep mathematical concept.
[01:07:15.160 --> 01:07:20.920]   So with that, I'll close out my talk.
[01:07:20.920 --> 01:07:30.720]   And I will take any questions if folks have them.
[01:07:30.720 --> 01:07:40.560]   Let me pull up the right Zoom setting.
[01:07:40.560 --> 01:07:45.600]   Looks like the YouTube is a few seconds behind us, maybe even 30 seconds.
[01:07:45.600 --> 01:07:51.440]   So it might take a little bit of time for them to catch up.
[01:07:51.440 --> 01:07:58.400]   Looks like-- oh, my old co-host, Lavanya, has shown up in the top chat to say she did
[01:07:58.400 --> 01:07:59.400]   great talk.
[01:07:59.400 --> 01:08:00.400]   Thanks, Lavanya.
[01:08:00.400 --> 01:08:06.680]   It feels a little bit about like when you're-- if your mother calls you a handsome young
[01:08:06.680 --> 01:08:10.800]   man, it's like, thanks for the compliment.
[01:08:10.800 --> 01:08:11.800]   It means a lot.
[01:08:11.800 --> 01:08:12.800]   All right.
[01:08:12.800 --> 01:08:18.980]   Well, it doesn't look like there are any questions.
[01:08:18.980 --> 01:08:25.860]   So I'll just-- before we close out, I'll share my screen really quickly and just say that--
[01:08:25.860 --> 01:08:30.400]   so my job here at Weights and Biases is to be a deep learning educator, to sort of reach
[01:08:30.400 --> 01:08:37.160]   out to our community of deep learning engineers, practitioners, enthusiasts, and amateurs,
[01:08:37.160 --> 01:08:41.580]   and help them learn a little bit more about deep learning, and ideally also how to use
[01:08:41.580 --> 01:08:43.800]   the Weights and Biases experiment tracking tool.
[01:08:43.800 --> 01:08:49.800]   So please reach out to me at Charles@WMB.com or on Twitter, twitter.com/Charles_IRL.
[01:08:49.800 --> 01:08:54.680]   I'd be happy to answer questions, talk with you about work you've been doing, learn about
[01:08:54.680 --> 01:09:00.360]   your projects, and just talk about mathematics and machine learning and, I don't know, Dungeons
[01:09:00.360 --> 01:09:03.320]   and Dragons if you're also into that.
[01:09:03.320 --> 01:09:16.560]   So with that said, I think we are done with our salon for the week.
[01:09:16.560 --> 01:09:22.240]   So I will catch you all in two weeks at our next salon.

