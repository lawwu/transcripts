
[00:00:00.000 --> 00:00:06.600]   The challenge is like, well, shit, I didn't sign up for this.
[00:00:06.600 --> 00:00:08.240]   I wanted to do AI research.
[00:00:08.240 --> 00:00:13.560]   I didn't want to do AI research plus societal ethics and geopolitics.
[00:00:13.560 --> 00:00:14.560]   That's also not my expertise.
[00:00:14.560 --> 00:00:17.000]   I think that's a very reasonable point.
[00:00:17.000 --> 00:00:25.160]   Unfortunately, there isn't another crap team of people hiding behind some wall to entirely
[00:00:25.160 --> 00:00:27.080]   shoulder the burden of this.
[00:00:27.080 --> 00:00:30.920]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:30.920 --> 00:00:32.600]   models work in the real world.
[00:00:32.600 --> 00:00:34.600]   I'm your host, Lukas Biewald.
[00:00:34.600 --> 00:00:38.920]   Jack Clark is the strategy and communications director at OpenAI.
[00:00:38.920 --> 00:00:43.240]   Before that, he was the world's only neural network reporter at Bloomberg.
[00:00:43.240 --> 00:00:48.960]   He's also one of the smartest people I know thinking about policy and AI and ethics.
[00:00:48.960 --> 00:00:51.080]   I really am excited to talk to him.
[00:00:51.080 --> 00:00:58.720]   I feel like I typically get nervous when people ask me big policy questions about AI.
[00:00:58.720 --> 00:01:02.200]   I never really feel like I have much smart to say.
[00:01:02.200 --> 00:01:07.080]   I think the goal of this podcast is mainly to talk about people doing AI and production.
[00:01:07.080 --> 00:01:12.360]   But then when I started writing down questions I wanted to ask you, I was like, "Wait a second.
[00:01:12.360 --> 00:01:16.880]   I want to ask you all the policy questions and all the weird questions that everyone
[00:01:16.880 --> 00:01:21.840]   asks me because I have no idea."
[00:01:21.840 --> 00:01:24.920]   So this question I seriously want to know because I feel like you think about this a
[00:01:24.920 --> 00:01:25.920]   lot.
[00:01:25.920 --> 00:01:30.360]   I mean, this is such a cliche question, but I'm actually fascinated by how you're going
[00:01:30.360 --> 00:01:35.920]   to answer it, which is what probability do you put on AI apocalypse?
[00:01:35.920 --> 00:01:36.920]   Oh, good.
[00:01:36.920 --> 00:01:41.920]   So we'll start with the really easy question and go from there.
[00:01:41.920 --> 00:01:42.920]   Yeah.
[00:01:42.920 --> 00:01:47.320]   What's your, is it like one in 10, nine out of 10, one in a million?
[00:01:47.320 --> 00:01:53.480]   I think the chance of an AI apocalypse is quite high.
[00:01:53.480 --> 00:01:54.480]   It's above 50%.
[00:01:54.480 --> 00:02:00.520]   But that's only because the chance of most apocalypses, if they get to the point where
[00:02:00.520 --> 00:02:04.440]   they're happening, like say a global pandemic, which is something that we're currently going
[00:02:04.440 --> 00:02:10.840]   through, it's quite clear that most of today's governments don't have the capacity or capabilities
[00:02:10.840 --> 00:02:13.120]   to deal with these really hard challenges.
[00:02:13.120 --> 00:02:19.880]   So if you end up in some scenario where you've got large autonomous broken machines doing
[00:02:19.880 --> 00:02:26.840]   bad stuff, then I think your chance of being able to right the ship is relatively low and
[00:02:26.840 --> 00:02:30.680]   you don't have a super positive outlook.
[00:02:30.680 --> 00:02:36.680]   I think the chance that we have to avert that and get ahead of it is actually quite high.
[00:02:36.680 --> 00:02:42.360]   But I think your question is more like, if something wakes up and we enter this very,
[00:02:42.360 --> 00:02:45.120]   very weird territory, what are our chances?
[00:02:45.120 --> 00:02:50.560]   And I think if we don't do anything today, then our chances are extremely poor.
[00:02:50.560 --> 00:02:55.200]   I think maybe I agree, our chances of surviving an AI apocalypse are probably low.
[00:02:55.200 --> 00:02:59.320]   But I think my question actually is, what do you think the chances are of actually entering
[00:02:59.320 --> 00:03:00.320]   the AI apocalypse?
[00:03:00.760 --> 00:03:06.640]   And remember that all apocalypse scenarios, they can't sum to more than one.
[00:03:06.640 --> 00:03:16.560]   So in a way, like a pandemic apocalypse, unless you think they're sort of linked, that should
[00:03:16.560 --> 00:03:18.600]   make the AI apocalypse actually lower.
[00:03:18.600 --> 00:03:26.960]   I think this is kind of like at the beginning of when you started to do massive amounts
[00:03:26.960 --> 00:03:31.160]   of computer trading on the stock market, saying what's the chance we're going to enter into
[00:03:31.160 --> 00:03:34.360]   a high frequency trading apocalypse?
[00:03:34.360 --> 00:03:39.440]   And I think how someone would have answered that is, it's really high, we'll have problems,
[00:03:39.440 --> 00:03:44.240]   but it's fairly unlikely that the whole system will topple over due to high frequency trading.
[00:03:44.240 --> 00:03:47.960]   And I think that my answer on AI is pretty similar.
[00:03:47.960 --> 00:03:53.040]   Like it's really high, but we're going to have some problems because it's a massively
[00:03:53.040 --> 00:03:57.000]   scaled technology that does weird things really, really, really quickly.
[00:03:57.000 --> 00:04:02.560]   And we'll do stuff in areas where finance has also deployed huge amounts of capital.
[00:04:02.560 --> 00:04:06.000]   So the opportunity for big things going wrong is kind of high.
[00:04:06.000 --> 00:04:12.120]   But the chance of a total apocalypse feels a little fantastical to me.
[00:04:12.120 --> 00:04:18.800]   And that's partly because I think for a real apocalypse, like a really bad, severe one,
[00:04:18.800 --> 00:04:22.400]   you need the ability for AI to take a lot of actions in the world, which means you need
[00:04:22.400 --> 00:04:27.360]   robotics and robotics, as you and I both know, is terrible and actually protects us from
[00:04:27.360 --> 00:04:31.360]   a huge amount of many parts of the like ornate apocalypse scenarios.
[00:04:31.360 --> 00:04:39.760]   The way that I think about this is you develop a load of radical technology and some of the
[00:04:39.760 --> 00:04:44.880]   greatest risks you face aren't the technology deciding of its own volition to do bad stuff.
[00:04:44.880 --> 00:04:45.880]   That very rarely happens.
[00:04:45.880 --> 00:04:46.880]   It's even unlikely here.
[00:04:46.880 --> 00:04:52.480]   There's a chance that you kind of get black mold for technology.
[00:04:52.480 --> 00:04:56.240]   Like somewhere in your house, you have not been cleaning it efficiently and you don't
[00:04:56.240 --> 00:05:01.600]   have good systems in place and something problematic starts developing in a kind of emergent way
[00:05:01.600 --> 00:05:02.600]   that you barely notice.
[00:05:02.600 --> 00:05:03.600]   But that thing has really adverse effects on you.
[00:05:03.600 --> 00:05:11.560]   And it's also hard to diagnose the source of the problem and why it's occurring.
[00:05:11.560 --> 00:05:21.080]   That seems like a much more concrete scenario.
[00:05:21.080 --> 00:05:22.080]   What form might that take?
[00:05:22.080 --> 00:05:26.200]   I mean, it sounds like you're mostly worried about the things we're doing now.
[00:05:26.200 --> 00:05:30.680]   We get better at doing these bad things and that causes big problems.
[00:05:30.680 --> 00:05:33.600]   What are top of mind concerns for you?
[00:05:33.600 --> 00:05:40.840]   I guess I'd frame my concern as we're currently pretty blind to most of the places this could
[00:05:40.840 --> 00:05:45.840]   show up and we kind of need something that looks a lot like weather forecasting and radar
[00:05:45.840 --> 00:05:50.640]   and sensors for looking at evolutions in this domain.
[00:05:50.640 --> 00:05:55.520]   The sorts of things that I'd be worried about are scaled up versions of what we already
[00:05:55.520 --> 00:05:56.520]   have.
[00:05:56.520 --> 00:06:01.840]   Recommendation systems pushing people towards kind of increasingly odd areas of sort of
[00:06:01.840 --> 00:06:07.440]   content or subject matter that we aren't realizing are quietly radicalizing people or making
[00:06:07.440 --> 00:06:10.200]   people behave differently with each other.
[00:06:10.200 --> 00:06:16.560]   I worry quite a lot about sort of AI capabilities interacting with economics.
[00:06:16.560 --> 00:06:22.800]   So you have some economic incentive today to create entertaining disinformation or misinformation.
[00:06:22.800 --> 00:06:26.360]   I think we think about what happens when those things collide.
[00:06:26.360 --> 00:06:31.840]   You have good AI tools for creating misinformation or disinformation and an economic incentive
[00:06:31.840 --> 00:06:33.640]   and stuff starts showing up.
[00:06:33.640 --> 00:06:37.240]   I think there are going to be relatively few grand evil plans.
[00:06:37.240 --> 00:06:42.120]   I think there are going to be lots and lots of like accidental screw ups that happen at
[00:06:42.120 --> 00:06:46.720]   really, really large scales and that happen really, really quickly with self-reinforcing
[00:06:46.720 --> 00:06:47.720]   cycles.
[00:06:47.720 --> 00:06:52.680]   I think that that's the challenge is you not only need to spot something, but you're going
[00:06:52.680 --> 00:06:55.480]   to need to take actions quite quickly.
[00:06:55.480 --> 00:06:59.960]   That's something that we're traditionally just really, really bad at doing as people.
[00:06:59.960 --> 00:07:06.520]   We can observe bad things happening, but our ability to act against them is quite low.
[00:07:06.520 --> 00:07:12.120]   You do a lot of work on ethics and AI and a lot of thinking about it.
[00:07:12.120 --> 00:07:15.480]   But it sort of seems like those scenarios sort of feel...
[00:07:15.480 --> 00:07:16.480]   Is AI special there?
[00:07:16.480 --> 00:07:21.080]   It seems like there's kind of a lot that might be just sort of general technology risk.
[00:07:21.080 --> 00:07:23.920]   Do you think AI makes this sort of different?
[00:07:23.920 --> 00:07:28.800]   I think the difference is delegation.
[00:07:28.800 --> 00:07:31.520]   Technology allows us to delegate certain things.
[00:07:31.520 --> 00:07:38.400]   Technology up until many sort of practical forms of AI lets us delegate highly specific
[00:07:38.400 --> 00:07:42.880]   things that we can write down in a sort of procedural way.
[00:07:42.880 --> 00:07:48.520]   And AI allows us to delegate things which have a bit more inherent freedom in how you
[00:07:48.520 --> 00:07:52.480]   choose to approach the problem that's being delegated to you.
[00:07:52.480 --> 00:07:54.520]   Make sure people watch more videos on my website.
[00:07:54.520 --> 00:07:56.560]   It's kind of a fuzzy problem.
[00:07:56.560 --> 00:08:01.040]   You're giving a larger space for the system to sort of think about it.
[00:08:01.040 --> 00:08:06.840]   And so I think the ethics there, they're not something that humans haven't encountered
[00:08:06.840 --> 00:08:12.960]   before, but it's a form of ethics which has a lot in common with military or how you do
[00:08:12.960 --> 00:08:18.640]   administrative states in the old days, which is the sort of ethical nature of giving someone
[00:08:18.640 --> 00:08:23.080]   the ability to delegate increasingly broad tasks to hundreds or thousands of capable
[00:08:23.080 --> 00:08:24.080]   people.
[00:08:24.080 --> 00:08:28.760]   That's like a classic ethical problem that people have dealt with for hundreds or thousands
[00:08:28.760 --> 00:08:29.760]   of years.
[00:08:29.760 --> 00:08:33.520]   But with AI, now almost everyone gets to do that delegation.
[00:08:33.520 --> 00:08:35.320]   And that really hasn't happened before.
[00:08:35.320 --> 00:08:39.800]   We haven't had this scale of delegation and this ease with which people can kind of scale
[00:08:39.800 --> 00:08:40.800]   themselves up.
[00:08:40.800 --> 00:08:46.560]   And so lots of the ethical challenges are like, okay, people now have much greater capabilities
[00:08:46.560 --> 00:08:49.360]   to do good and harm than they did before.
[00:08:49.360 --> 00:08:53.400]   They have these automated tools that kind of extend their ability to do this.
[00:08:53.400 --> 00:08:58.000]   How do you think about the role of the tool developer in that context?
[00:08:58.000 --> 00:09:04.440]   Because sure, you're building just like iterations on previous tools, but the scope of which
[00:09:04.440 --> 00:09:08.840]   those tools will be used, the areas in which they'll be used is much, much broader than
[00:09:08.840 --> 00:09:10.160]   you've dealt with before.
[00:09:10.160 --> 00:09:16.160]   And I think it introduces ethical considerations for you that maybe governments only previously
[00:09:16.160 --> 00:09:17.160]   dealt with.
[00:09:17.160 --> 00:09:18.160]   I see.
[00:09:18.160 --> 00:09:26.200]   So in your view, AI kind of allows single individuals to have sort of broader impact.
[00:09:26.200 --> 00:09:32.520]   And therefore, the tools that you actually make available to folks, there's more ethical
[00:09:32.520 --> 00:09:33.880]   issues within that.
[00:09:33.880 --> 00:09:34.880]   Yeah.
[00:09:34.880 --> 00:09:39.680]   A good way to think about this is, I think language models are interesting.
[00:09:39.680 --> 00:09:44.200]   Here's an ethical challenge that I find interesting with language models.
[00:09:44.200 --> 00:09:48.000]   You have a big language model that has a generative capability.
[00:09:48.000 --> 00:09:52.960]   You want to give that to basically everyone because it's sort of analogous to a new form
[00:09:52.960 --> 00:09:53.960]   of paintbrush.
[00:09:53.960 --> 00:09:54.960]   It's very general.
[00:09:54.960 --> 00:09:57.840]   We're going to do all kinds of stuff with it.
[00:09:57.840 --> 00:10:04.640]   Except this paintbrush reflects the implicit biases of the sort of data that it was trained
[00:10:04.640 --> 00:10:06.520]   on at massive scale.
[00:10:06.520 --> 00:10:09.840]   So okay, it's like a slightly racist paintbrush.
[00:10:09.840 --> 00:10:12.440]   The problem is now different to just having a paintbrush.
[00:10:12.440 --> 00:10:18.240]   You've got a paintbrush that has slight tendencies, and some of these tendencies seem fine to
[00:10:18.240 --> 00:10:23.920]   you, but some of the tendencies seem to reflect things that many people have a strong moral
[00:10:23.920 --> 00:10:27.360]   view of as being bad and in society.
[00:10:27.360 --> 00:10:28.360]   What do you do then?
[00:10:28.360 --> 00:10:32.680]   And I've actually spoken to lots and lots of artists about this, and most artists will
[00:10:32.680 --> 00:10:34.680]   just say, "Give me the paintbrush.
[00:10:34.680 --> 00:10:39.920]   I want the crazy funhouse mirror version of society so I can talk about it and make interesting
[00:10:39.920 --> 00:10:40.920]   things."
[00:10:40.920 --> 00:10:47.520]   That feels fine, but then I wonder about what happens if someone gets given this paintbrush
[00:10:47.520 --> 00:10:51.080]   and they just want to write text for a kind of economic purpose.
[00:10:51.080 --> 00:10:54.560]   They may not know much about the paintbrush they've been given, they may not know about
[00:10:54.560 --> 00:11:00.120]   its traits, and then suddenly they're unwittingly creating massive scaled-up versions of the
[00:11:00.120 --> 00:11:03.000]   biases inherent to that thing you gave them.
[00:11:03.000 --> 00:11:04.000]   That seems challenging.
[00:11:04.000 --> 00:11:09.120]   And where we as technology developers have a lot of choice, a sort of uncomfortable amount
[00:11:09.120 --> 00:11:12.440]   of choice, and a lot of problems which are not easy to fix.
[00:11:12.440 --> 00:11:14.440]   You can't fix this.
[00:11:14.440 --> 00:11:18.920]   You need to figure out how to talk about it and make people aware of it.
[00:11:18.920 --> 00:11:20.800]   Wow, that's a really clever analogy.
[00:11:20.800 --> 00:11:23.000]   I have not heard that one before.
[00:11:23.000 --> 00:11:24.000]   Yeah.
[00:11:24.000 --> 00:11:29.480]   I think it speaks to the weird scalability of a lot of this stuff.
[00:11:29.480 --> 00:11:34.920]   If we just have tools that let people scale themselves in various directions, and the
[00:11:34.920 --> 00:11:41.400]   directions are increasingly creative areas because we're building these scaled-up curve-fitting
[00:11:41.400 --> 00:11:47.160]   systems that can fit really weird curves, including in interesting semantic domains,
[00:11:47.160 --> 00:11:52.480]   then all the problems of curve-fitting now become weird problems of the production of
[00:11:52.480 --> 00:11:57.640]   art and thought, which feels different and challenging.
[00:11:57.640 --> 00:11:58.840]   I don't have great answers here.
[00:11:58.840 --> 00:12:05.360]   I have more like, "Oh dear, this is interesting," and then it feels different.
[00:12:05.360 --> 00:12:12.080]   But actually, it's interesting because you speak of the language model as, "Oh, just
[00:12:12.080 --> 00:12:14.120]   for example, what if you had a language model?"
[00:12:14.120 --> 00:12:17.680]   And OpenAI actually had this issue.
[00:12:17.680 --> 00:12:21.640]   I'm curious how you thought about it at the time and how you reflect on that now.
[00:12:21.640 --> 00:12:27.440]   I think at the time, so this is GPT-2, which is a language model that we announced and
[00:12:27.440 --> 00:12:31.640]   didn't initially release, but subsequently released in full.
[00:12:31.640 --> 00:12:38.280]   At the time, I think we made a kind of classic error, which is that if you're developing
[00:12:38.280 --> 00:12:44.000]   a technology, you see all of its potential very, very clearly, and you don't just see
[00:12:44.000 --> 00:12:45.680]   the technology you're holding in your hands.
[00:12:45.680 --> 00:12:49.960]   You see Gen 2, 3, 4, and 5 and the implications thereof.
[00:12:49.960 --> 00:12:56.540]   I think we treated some of our worries about the misuse of this technology.
[00:12:56.540 --> 00:13:00.600]   We were thinking about later versions of the technology than the one we were actually holding
[00:13:00.600 --> 00:13:05.640]   because what actually happened is we released it, we observed a huge amount of positive
[00:13:05.640 --> 00:13:10.920]   uses and really surprising ones like this game AI Dungeon, where a language model becomes
[00:13:10.920 --> 00:13:11.920]   a kind of dungeon master.
[00:13:11.920 --> 00:13:16.840]   It feels interesting and actually different, like a different form of game playing, something
[00:13:16.840 --> 00:13:17.840]   we wouldn't have expected.
[00:13:17.840 --> 00:13:20.840]   The misuses were relatively small.
[00:13:20.840 --> 00:13:25.640]   It's actually because it's really hard to make a misuse of a technology.
[00:13:25.640 --> 00:13:30.840]   It's probably as hard to make a misuse of a technology as it is to make a positive use.
[00:13:30.840 --> 00:13:33.640]   Luckily, most people want to do the positive uses.
[00:13:33.640 --> 00:13:38.040]   So your amount of people doing the misuses is a lot smaller.
[00:13:38.040 --> 00:13:42.720]   I think that means that the responsibility of technology developers is going to be more
[00:13:42.720 --> 00:13:48.360]   about maybe you're still going to trickle things out in stages, but you're ultimately
[00:13:48.360 --> 00:13:51.800]   going to release lots of stuff in some form.
[00:13:51.800 --> 00:13:57.400]   It's about thinking about how you can control some elements of the technology while making
[00:13:57.400 --> 00:13:59.120]   other parts accessible.
[00:13:59.120 --> 00:14:06.880]   Can you control how you'd expect a big generative system to be used while making it maximally
[00:14:06.880 --> 00:14:07.880]   accessible?
[00:14:07.880 --> 00:14:17.240]   Because you definitely don't want a big generative model that may have biased tendencies providing
[00:14:17.240 --> 00:14:23.880]   generations to people in, say, a mock interview process that happens before they speak to
[00:14:23.880 --> 00:14:25.440]   a human for an interview stage.
[00:14:25.440 --> 00:14:29.320]   Because that's the sort of usage that we can imagine and feels like the sort of thing you
[00:14:29.320 --> 00:14:30.320]   really want to avoid.
[00:14:30.320 --> 00:14:36.000]   But you can imagine ways in which you'd make this technology really, really, really broadly
[00:14:36.000 --> 00:14:40.640]   accessible while finding ways to carve out parts where you as a developer kind of say,
[00:14:40.640 --> 00:14:42.480]   "This probably isn't okay."
[00:14:42.480 --> 00:14:45.240]   So I think our thinking has become a lot more subtle.
[00:14:45.240 --> 00:14:49.560]   And I think we did anchor on the future more than the present.
[00:14:49.560 --> 00:14:52.800]   And that's been one of the main things that's changed.
[00:14:52.800 --> 00:14:53.800]   Interesting.
[00:14:53.800 --> 00:14:57.880]   So knowing what you know now, you wouldn't withhold the model?
[00:14:57.880 --> 00:15:02.360]   I think you'd still do staged release.
[00:15:02.360 --> 00:15:09.320]   But I think that you'd do more research earlier on characterizing the biases of the model
[00:15:09.320 --> 00:15:11.120]   and potential malicious uses.
[00:15:11.120 --> 00:15:13.920]   Because the thing that we did is we did some of this research.
[00:15:13.920 --> 00:15:18.960]   And then we did a lot more after some of the initial models have been released on characterizing
[00:15:18.960 --> 00:15:21.320]   subsequent models we are planning to release.
[00:15:21.320 --> 00:15:25.960]   What I think is now more helpful is if you have a load of that stuff front-loaded.
[00:15:25.960 --> 00:15:27.960]   So you're basically saying, "Here's the context.
[00:15:27.960 --> 00:15:32.520]   Here are the traits of this thing, which is going to slowly be released, and you should
[00:15:32.520 --> 00:15:33.520]   be aware of it."
[00:15:33.520 --> 00:15:36.880]   So yeah, I think we would have done stuff slightly differently.
[00:15:36.880 --> 00:15:39.280]   And I think that this is...
[00:15:39.280 --> 00:15:45.520]   What we're trying to do here is learn how to behave with these technologies.
[00:15:45.520 --> 00:15:50.600]   And some of that is about making yourself more culpable with this traditional for its
[00:15:50.600 --> 00:15:51.600]   outcomes.
[00:15:51.600 --> 00:15:55.720]   Because as a thinking exercise, it makes you think about different things to do.
[00:15:55.720 --> 00:16:01.720]   So I'm glad that part of the goal of GPT-2 is bring a problem that we actually don't
[00:16:01.720 --> 00:16:08.440]   get to get wrong in the future, earlier in time to a point where we can do different
[00:16:08.440 --> 00:16:09.440]   ways of release.
[00:16:09.440 --> 00:16:13.080]   And maybe some of them will be good, and some of them will be suboptimal, and learn from
[00:16:13.080 --> 00:16:14.080]   that.
[00:16:14.080 --> 00:16:17.640]   Because I think in five, six, seven years, these sorts of capabilities will need to be
[00:16:17.640 --> 00:16:22.400]   treated in a standardized way that we've thought about carefully.
[00:16:22.400 --> 00:16:25.480]   And getting to that requires lots of experiments now.
[00:16:25.480 --> 00:16:27.280]   But it's kind of interesting.
[00:16:27.280 --> 00:16:30.360]   I guess there's two kinds of problems.
[00:16:30.360 --> 00:16:36.240]   I think my understanding of the worry with GPT-2 is actually malicious uses, which more
[00:16:36.240 --> 00:16:38.560]   information probably wouldn't help with.
[00:16:38.560 --> 00:16:44.400]   But then there's also, I think, your idea of accidentally racist paintbrush.
[00:16:44.400 --> 00:16:48.120]   That sort of speaks to inadvertently bad uses.
[00:16:48.120 --> 00:16:55.040]   I mean, both seem like potential issues, but do you now view malicious uses as less of
[00:16:55.040 --> 00:16:56.040]   an issue?
[00:16:56.040 --> 00:17:00.400]   Because I really could imagine a very good language model having plenty of malicious
[00:17:00.400 --> 00:17:01.400]   uses.
[00:17:01.400 --> 00:17:05.800]   I suppose you could say, well, any interesting technology probably has malicious uses, so
[00:17:05.800 --> 00:17:08.000]   should we never release any kind of tool?
[00:17:08.000 --> 00:17:09.840]   How do you think about that?
[00:17:09.840 --> 00:17:10.840]   Yeah.
[00:17:10.840 --> 00:17:14.360]   Again, it's good that we're doing really easy questions on platforms.
[00:17:14.360 --> 00:17:16.920]   We're warming up for the easy stuff.
[00:17:16.920 --> 00:17:19.320]   Well, there's a couple of things.
[00:17:19.320 --> 00:17:24.840]   One of the things we did with GPT-2 was we released a detector system, which was a model
[00:17:24.840 --> 00:17:28.720]   trained to detect outputs of GPT-2 models.
[00:17:28.720 --> 00:17:34.320]   We also released a big data set of unsupervised generations from the model, so other people
[00:17:34.320 --> 00:17:36.640]   could build different detector systems.
[00:17:36.640 --> 00:17:41.760]   I think that a huge amount of dealing with misuse is just giving yourself awareness.
[00:17:41.760 --> 00:17:48.440]   You know, like, why are police forces around the world and security services able to actually
[00:17:48.440 --> 00:17:50.080]   deal with organized crime?
[00:17:50.080 --> 00:17:54.200]   Well, we can't make organized crime go away, because that's a socioeconomic phenomenon,
[00:17:54.200 --> 00:17:59.680]   but they can tool up on very specific ways to detect patterns of organized crime.
[00:17:59.680 --> 00:18:04.640]   I think it's similar here, where you need to release tools that can help others detect
[00:18:04.640 --> 00:18:08.720]   the outputs of the things you're releasing.
[00:18:08.720 --> 00:18:13.680]   So, avoiding malicious uses, I think it's actually kind of challenging.
[00:18:13.680 --> 00:18:19.200]   I think that it's a little unclear today how you completely rule that stuff out.
[00:18:19.200 --> 00:18:23.000]   I think it's generally challenging to do that with technologies.
[00:18:23.000 --> 00:18:27.400]   Some of how we've been approaching it is trying to make prototypes.
[00:18:27.400 --> 00:18:32.680]   The idea being, if we can make a prototype use case that's malicious and real, then we
[00:18:32.680 --> 00:18:35.000]   should sort of talk to affected people.
[00:18:35.000 --> 00:18:40.840]   The extent to which we would publicize that remains deeply unclear to me, because as you
[00:18:40.840 --> 00:18:46.400]   kind of sort of intuit, if you publicize malicious uses, it's like, "Look over here.
[00:18:46.400 --> 00:18:52.120]   Here's how you might misuse this thing we've released," which seems a little dangerous.
[00:18:52.120 --> 00:18:58.880]   I think that we're going to need new forms of control of technology in general at some
[00:18:58.880 --> 00:18:59.880]   point.
[00:18:59.880 --> 00:19:05.440]   I think that's this year's problem or next year's problem.
[00:19:05.440 --> 00:19:12.360]   In 2025, you're going to have these embarrassingly capable cognitive services, which can be made
[00:19:12.360 --> 00:19:14.800]   available to large numbers of people.
[00:19:14.800 --> 00:19:20.000]   I think cloud providers and governments and others are going to need to work together
[00:19:20.000 --> 00:19:25.240]   to really characterize what can be just generically available for everyone and what needs some
[00:19:25.240 --> 00:19:28.920]   level of care and attention paid to it.
[00:19:28.920 --> 00:19:36.040]   Getting to that's going to be incredibly unpleasant and difficult, but feels kind of inevitable.
[00:19:36.040 --> 00:19:43.240]   But I guess just to be concrete, if you created, say, a GPT-3 that was much more powerful,
[00:19:43.240 --> 00:19:46.680]   you think that you would probably release it along with a detector?
[00:19:46.680 --> 00:19:49.600]   Would it be the sort of compromise now?
[00:19:49.600 --> 00:19:53.560]   I think you'd think about different ways that you can release it, because some capabilities
[00:19:53.560 --> 00:19:54.560]   might be fine.
[00:19:54.560 --> 00:19:59.880]   Some might, you might want to have some sort of control, so you control the model, the
[00:19:59.880 --> 00:20:02.680]   people access services around it.
[00:20:02.680 --> 00:20:04.800]   That could be one way that you do it.
[00:20:04.800 --> 00:20:11.040]   Another way could be just releasing fine-tuned versions of models on specific data sets or
[00:20:11.040 --> 00:20:17.000]   specific areas, because if you fine-tune a model, it's kind of like neural silly putty,
[00:20:17.000 --> 00:20:21.760]   where you take this big blob of capability, you put it on a new data set, it takes on
[00:20:21.760 --> 00:20:25.880]   some of the traits of that data set, and in some sense, you've restricted it.
[00:20:25.880 --> 00:20:27.760]   So you can do things like that.
[00:20:27.760 --> 00:20:32.720]   I think the challenge for a lot of developers going forward is going to be in how to deal
[00:20:32.720 --> 00:20:36.440]   with the root artifacts themselves, like models themselves.
[00:20:36.440 --> 00:20:44.360]   Here's a thing I think about quite regularly is, it's not today, it's not next year, it's
[00:20:44.360 --> 00:20:51.720]   probably not even 2022, but definitely by 2025, we're going to have conditional video
[00:20:51.720 --> 00:20:52.720]   models.
[00:20:52.720 --> 00:20:57.600]   Someone in the AI community or some group of people are going to develop research that
[00:20:57.600 --> 00:21:04.480]   allows us to generate a model, generate a video that runs for some period of time, a
[00:21:04.480 --> 00:21:10.080]   few seconds, probably not like minutes, but they can guide it so that it includes specific
[00:21:10.080 --> 00:21:17.000]   people and they do specific things, and maybe you also get audio as well.
[00:21:17.000 --> 00:21:21.480]   That capability is obviously something, but it's like the much harder case of just a language
[00:21:21.480 --> 00:21:23.520]   model or just an image model.
[00:21:23.520 --> 00:21:31.040]   I think that that capability definitely gets quite a few controls applied to it and needs
[00:21:31.040 --> 00:21:37.240]   systems done for authentication of real content on the public internet as well.
[00:21:37.240 --> 00:21:39.240]   It provokes questions about that.
[00:21:39.240 --> 00:21:45.920]   I think we're heading into a weird era for all of this stuff.
[00:21:45.920 --> 00:21:50.760]   I think of the advances you get of releasing all of this stuff just publicly on the internet
[00:21:50.760 --> 00:21:56.840]   are pretty huge, but I also think that it's to some degree a dereliction of duty by the
[00:21:56.840 --> 00:22:03.520]   AI community to not think about the implications of where we are in three, four, five years,
[00:22:03.520 --> 00:22:08.440]   because I have quite a high confidence that we can't be in this state of affairs where
[00:22:08.440 --> 00:22:15.240]   the norm is to put everything online instantly, because I think we'll just develop things
[00:22:15.240 --> 00:22:16.240]   that are frankly too capable.
[00:22:16.240 --> 00:22:22.760]   By we, I mean AI researchers writ large, for you to be able to do that and say, "This is
[00:22:22.760 --> 00:22:23.760]   fine."
[00:22:23.760 --> 00:22:24.760]   I would ask you about this.
[00:22:24.760 --> 00:22:25.760]   What do you think about this sort of issue?
[00:22:25.760 --> 00:22:37.360]   What is the responsibility of technologists and how do we get to a more responsible place
[00:22:37.360 --> 00:22:38.360]   and is that even necessary?
[00:22:38.360 --> 00:22:41.840]   I bet you could ask me another question for that.
[00:22:41.840 --> 00:22:44.840]   I got to know.
[00:22:44.840 --> 00:22:46.120]   I don't know.
[00:22:46.120 --> 00:22:47.120]   It's funny.
[00:22:47.120 --> 00:22:52.120]   I feel like I really want to reserve the right to change my mind on some of this stuff.
[00:22:52.120 --> 00:22:53.120]   I feel like...
[00:22:53.120 --> 00:22:54.120]   So do I, to be clear.
[00:22:54.120 --> 00:23:00.120]   I didn't realize we were committing in this conversation.
[00:23:00.120 --> 00:23:09.360]   I think I'm kind of reluctant to say things publicly because it seems like actually the
[00:23:09.360 --> 00:23:16.480]   ethics really depend on the specifics of how the technology works and stuff.
[00:23:16.480 --> 00:23:21.400]   I think on GPT-2, just as an example, it seemed like...
[00:23:21.400 --> 00:23:26.080]   I thought opening up this decision was intriguing and different than what I would have done
[00:23:26.080 --> 00:23:29.100]   or what my instincts would have been.
[00:23:29.100 --> 00:23:35.000]   But it was kind of provocative to say, "Hey, we're not going to release this model."
[00:23:35.000 --> 00:23:40.440]   And I think the good thing about it, maybe it was it kind of got everyone talking and
[00:23:40.440 --> 00:23:41.600]   thinking about it.
[00:23:41.600 --> 00:23:47.320]   I guess also another thing that I don't really have a strong point of view on, but just seems
[00:23:47.320 --> 00:23:55.120]   like a little interesting, is it seems like at the moment every AI researcher is sort
[00:23:55.120 --> 00:23:59.640]   of asked to be their own kind of ethicist on this stuff.
[00:23:59.640 --> 00:24:07.040]   I see a lot of ethics documents coming out with even open source ML projects will sort
[00:24:07.040 --> 00:24:10.120]   of have their code of conduct.
[00:24:10.120 --> 00:24:15.400]   On one hand, it seems a little almost highfalutin to me.
[00:24:15.400 --> 00:24:22.560]   I feel like I have this instinct of, "Come on, should I put out a code of ethics with
[00:24:22.560 --> 00:24:24.280]   the toaster that I sell?"
[00:24:24.280 --> 00:24:29.920]   It seems a little unappealing about it, but I can actually also definitely articulate
[00:24:29.920 --> 00:24:30.920]   the other side of it.
[00:24:30.920 --> 00:24:37.280]   That if you think, I guess to me, it's less the power of an individual and more of just
[00:24:37.280 --> 00:24:43.920]   like sort of like if this technology can kind of compound and run amok, then maybe it's
[00:24:43.920 --> 00:24:47.360]   the case that people really should be thinking about it.
[00:24:47.360 --> 00:24:49.000]   But yeah, I just honestly don't know.
[00:24:49.000 --> 00:24:52.040]   And I don't even know, I guess I'm curious what you think about this because you're in
[00:24:52.040 --> 00:24:53.040]   this all the time.
[00:24:53.040 --> 00:25:00.760]   Do you think that AI researchers are in the best position to decide this stuff?
[00:25:00.760 --> 00:25:05.200]   I mean, if it really affects society as profoundly as you're saying, it seems like kind of everyone
[00:25:05.200 --> 00:25:09.080]   should get a say about how this stuff works, right?
[00:25:09.080 --> 00:25:10.080]   Yeah.
[00:25:10.080 --> 00:25:14.560]   So this is unfair.
[00:25:14.560 --> 00:25:18.960]   What's actually happening here is an unfair thing for AI researchers, which is that they
[00:25:18.960 --> 00:25:25.120]   are building powerful technologies, they're releasing them into a world that doesn't have
[00:25:25.120 --> 00:25:30.440]   any real notion of technology governance because it hasn't really been developed yet.
[00:25:30.440 --> 00:25:34.960]   And they're releasing them into systems that will use the technologies to do great amounts
[00:25:34.960 --> 00:25:37.680]   of good and maybe a small amount of harm.
[00:25:37.680 --> 00:25:42.280]   And so the challenge is like, "Well, shit, I didn't sign up for this.
[00:25:42.280 --> 00:25:43.920]   I wanted to do AI research.
[00:25:43.920 --> 00:25:49.240]   I didn't want to do like AI research plus societal ethics and geopolitics.
[00:25:49.240 --> 00:25:50.240]   That's also not my expertise."
[00:25:50.240 --> 00:25:53.360]   I think that's a very reasonable point.
[00:25:53.360 --> 00:26:00.200]   Unfortunately, there isn't like another crap team of people hiding behind some wall to
[00:26:00.200 --> 00:26:02.840]   entirely shoulder the burden of this.
[00:26:02.840 --> 00:26:08.880]   There are ethicists and social scientists and philosophers, members of the public, governments,
[00:26:08.880 --> 00:26:12.640]   all of them have thoughts about this and should be involved.
[00:26:12.640 --> 00:26:18.360]   But I think the way to view AI researchers is they're making stuff that's kind of important.
[00:26:18.360 --> 00:26:23.920]   They should view themselves as being analogous to engineers, like the people who build buildings
[00:26:23.920 --> 00:26:25.600]   and make sure bridges don't fall over.
[00:26:25.600 --> 00:26:26.600]   You have a notion of ethics.
[00:26:26.600 --> 00:26:31.560]   Chemists, you have a notion of ethics because chemists get trained how to make bombs.
[00:26:31.560 --> 00:26:36.240]   And so you kind of want your chemists to have a strong ethical compass so that most of them
[00:26:36.240 --> 00:26:41.600]   don't make explosives because until you have a really, really resilient and stable society,
[00:26:41.600 --> 00:26:45.440]   you don't want lots of people able to do this who have sort of no ethical grounding because
[00:26:45.440 --> 00:26:49.920]   they might do experiments that lead to literal blow ups.
[00:26:49.920 --> 00:26:53.000]   Or even people like lawyers who have codes of conduct and ethics.
[00:26:53.000 --> 00:26:58.440]   It's very strange to look at AI research and sort of more broadly computer science and
[00:26:58.440 --> 00:27:04.120]   see a relative lack of this when you see it in other disciplines that are as impactful
[00:27:04.120 --> 00:27:08.120]   or maybe even less impactful on our current world.
[00:27:08.120 --> 00:27:13.240]   I don't think any AI researcher is going to solve this on their own.
[00:27:13.240 --> 00:27:18.320]   But I think for the culture of culpability, of thinking that actually to some extent I
[00:27:18.320 --> 00:27:20.680]   am like a little responsible here.
[00:27:20.680 --> 00:27:21.680]   Not a lot.
[00:27:21.680 --> 00:27:26.240]   It's not my entire problem, but I have some responsibility is good because how you get
[00:27:26.240 --> 00:27:32.360]   systemic change is millions of people making very small decisions of their own lives.
[00:27:32.360 --> 00:27:36.200]   It's not like millions of people making huge of optional decisions because that doesn't
[00:27:36.200 --> 00:27:37.680]   happen at scale.
[00:27:37.680 --> 00:27:42.360]   But millions of people making like slight deltas is how you get massive change over
[00:27:42.360 --> 00:27:43.360]   time.
[00:27:43.360 --> 00:27:44.360]   And I think that's kind of what we need here.
[00:27:44.360 --> 00:27:50.560]   Hi, we'd love to take a moment to tell you guys about Weights and Biases.
[00:27:50.560 --> 00:27:56.080]   Weights and Biases is a tool that helps you track and visualize every detail of your machine
[00:27:56.080 --> 00:27:57.080]   learning models.
[00:27:57.080 --> 00:28:03.040]   We help you debug your machine learning models in real time, collaborate easily, and advance
[00:28:03.040 --> 00:28:06.120]   the state of the art in machine learning.
[00:28:06.120 --> 00:28:11.320]   You can integrate Weights and Biases into your models with just a few lines of code.
[00:28:11.320 --> 00:28:16.080]   With hyperparameter sweeps, you can find the best set of hyperparameters for your models
[00:28:16.080 --> 00:28:18.000]   automatically.
[00:28:18.000 --> 00:28:23.400]   You can also track and compare how many GPU resources your models are using.
[00:28:23.400 --> 00:28:30.080]   With one line of code, you can visualize model predictions in form of images, videos, audio,
[00:28:30.080 --> 00:28:35.840]   plotly charts, molecular data, segmentation maps, and 3D point clouds.
[00:28:35.840 --> 00:28:41.680]   You can save everything you need to reproduce your models days, weeks, or even months after
[00:28:41.680 --> 00:28:42.680]   training.
[00:28:42.680 --> 00:28:47.800]   Finally, with reports, you can make your models come alive.
[00:28:47.800 --> 00:28:52.800]   Reports are like blog posts in which your readers can interact with your model metrics
[00:28:52.800 --> 00:28:54.600]   and predictions.
[00:28:54.600 --> 00:29:00.800]   Reports serve as a centralized repository of metrics, predictions, hyperparameter stride,
[00:29:00.800 --> 00:29:02.560]   and accompanying notes.
[00:29:02.560 --> 00:29:08.240]   All of this together gives you a bird's eye view of your machine learning workflow.
[00:29:08.240 --> 00:29:13.720]   You can use reports to share your model insights, keep your team on the same page, and collaborate
[00:29:13.720 --> 00:29:14.720]   effectively remotely.
[00:29:14.720 --> 00:29:19.880]   I'll leave a link in the show notes below to help you get started.
[00:29:19.880 --> 00:29:23.200]   Now, let's get back to the episode.
[00:29:23.200 --> 00:29:27.760]   Well let me ask you another easy question.
[00:29:27.760 --> 00:29:32.280]   What do you think about military applications of AI?
[00:29:32.280 --> 00:29:40.320]   I think that, well, the military applications of AI aren't special in the sense that it's
[00:29:40.320 --> 00:29:44.320]   technology that's going to be used kind of generically in different domains.
[00:29:44.320 --> 00:29:47.600]   So it'll get used in military applications.
[00:29:47.600 --> 00:29:54.720]   I mostly don't like it because of sort of what I think of as the AK-47 problem.
[00:29:54.720 --> 00:30:03.120]   So the AK-47 was a technological innovation to make this type of rifle more repeatable,
[00:30:03.120 --> 00:30:08.360]   more maintainable, and easier to use by people who had much less knowledge of weaponry than
[00:30:08.360 --> 00:30:10.960]   many prior systems.
[00:30:10.960 --> 00:30:16.160]   You develop this system, it goes everywhere.
[00:30:16.160 --> 00:30:23.440]   It makes the act of taking life or carrying out war cheaper and more repeatable, massively
[00:30:23.440 --> 00:30:26.560]   cheaper and much more repeatable.
[00:30:26.560 --> 00:30:28.880]   And so we see a rise in conflict.
[00:30:28.880 --> 00:30:34.320]   And we also see that this artifact, this technical artifact, to some extent drives conflict.
[00:30:34.320 --> 00:30:39.240]   It doesn't create the conditions for conflict, but it gets injected into them and it worsens
[00:30:39.240 --> 00:30:41.400]   them because it's cheap and it works.
[00:30:41.400 --> 00:30:48.000]   And I think that AI, if applied sort of wrongly or rashly in a military context, does a lot
[00:30:48.000 --> 00:30:49.000]   of this.
[00:30:49.000 --> 00:30:54.120]   It makes certain things cheaper, certain things more repeatable, and seems really, really
[00:30:54.120 --> 00:30:55.120]   bad.
[00:30:55.120 --> 00:31:00.560]   I think AI for military awareness is much more of a kind of gray area.
[00:31:00.560 --> 00:31:08.040]   Like lots of some ways in which unsteady peace sort of holds in the world is by different
[00:31:08.040 --> 00:31:12.640]   sides who are at war with each other, having lots of awareness of each other, awareness
[00:31:12.640 --> 00:31:15.560]   of troop movements, distributions, what you're doing.
[00:31:15.560 --> 00:31:18.080]   And they use surveillance technologies to do this.
[00:31:18.080 --> 00:31:22.600]   And I think you can make a credible argument that the advances in computer vision that
[00:31:22.600 --> 00:31:29.920]   we're seeing that's being applied like massively widely may, if adopted at scale by lots of
[00:31:29.920 --> 00:31:35.240]   militaries at the same time, which is kind of what seems to be happening, may provide
[00:31:35.240 --> 00:31:40.280]   some diminishment on a certain type of conflict because it means there's generally more awareness.
[00:31:40.280 --> 00:31:47.440]   I think stuff like the moral question of lethal autonomous weapons is really, really challenging
[00:31:47.440 --> 00:31:52.040]   because we want it to be a moral question, but it's ultimately going to be an economic
[00:31:52.040 --> 00:31:53.040]   question.
[00:31:53.040 --> 00:31:57.520]   Like it's going to be a question that governments make decisions about on the motivation of
[00:31:57.520 --> 00:32:02.480]   like economic speed of decision and what it does to strategic advantage, which means it's
[00:32:02.480 --> 00:32:06.480]   really hard to reason about because neither you or I make these decisions and actually
[00:32:06.480 --> 00:32:12.040]   come at it with a radically different frame, probably of a strong intuitive push against
[00:32:12.040 --> 00:32:15.480]   from an existing, but that's not the frame that these people have.
[00:32:15.480 --> 00:32:16.480]   Right.
[00:32:16.480 --> 00:32:17.480]   Right.
[00:32:17.480 --> 00:32:18.480]   Let's do another easy question.
[00:32:18.480 --> 00:32:19.480]   What else you got?
[00:32:19.480 --> 00:32:20.480]   Okay.
[00:32:20.480 --> 00:32:32.440]   This is maybe a less loaded question, but I'm actually genuinely curious about this
[00:32:32.440 --> 00:32:40.040]   so, you recently put out this paper, I think it's called Towards Trustworthy AI Development.
[00:32:40.040 --> 00:32:45.680]   And I thought as someone who builds a system that does a lot of saving of experiments and
[00:32:45.680 --> 00:32:49.040]   models and things like that, I thought it was really intriguing that you picked as the
[00:32:49.040 --> 00:32:54.560]   subtitle "Mechanisms for Supporting Verifiable Claims".
[00:32:54.560 --> 00:33:00.960]   So it seems like you draw this incredibly bright, direct line between trustworthy AI
[00:33:00.960 --> 00:33:03.760]   development and supporting verifiable claims.
[00:33:03.760 --> 00:33:09.600]   And I was wondering if you could sort of tell me why that is so connected.
[00:33:09.600 --> 00:33:16.920]   Well, it's really easy for us to say things that have a moral or an ethical kind of value
[00:33:16.920 --> 00:33:24.440]   and in words committed organization to something like we value the safety of our systems and
[00:33:24.440 --> 00:33:29.280]   we value them not making biased decisions or what have you.
[00:33:29.280 --> 00:33:32.080]   But that's an aspiration.
[00:33:32.080 --> 00:33:37.440]   And it's very similar to a politician on the election campaign trail being like, "Well,
[00:33:37.440 --> 00:33:39.880]   if you elect me, I will do such and such for you.
[00:33:39.880 --> 00:33:43.280]   I'll give you all this money or I'll build this thing."
[00:33:43.280 --> 00:33:44.840]   But it's not very verifiable.
[00:33:44.840 --> 00:33:49.440]   Like you're sort of needing to believe the organization or believe the politician and
[00:33:49.440 --> 00:33:52.520]   they can't give much proof to you.
[00:33:52.520 --> 00:33:56.640]   Because AI is going to be really, really significant in society and it's going to play an increasingly
[00:33:56.640 --> 00:34:00.960]   large role, people are going to approach it with slightly more skepticism just as they
[00:34:00.960 --> 00:34:06.000]   do with anything else in their life that plays a large role and has effects on them.
[00:34:06.000 --> 00:34:11.400]   And they're going to want systems of recourse, systems of diagnosis, systems of sort of awareness
[00:34:11.400 --> 00:34:12.400]   about it.
[00:34:12.400 --> 00:34:16.880]   Now, today, for most of this, we just fall back on people.
[00:34:16.880 --> 00:34:22.320]   We fall back on the court system as a way to ensure stuff's verifiable.
[00:34:22.320 --> 00:34:27.640]   We have these mechanisms of the law that mean that if I, as a company, make a certain claim,
[00:34:27.640 --> 00:34:34.720]   especially one that has a fiduciary component, the validation of that claim comes from a
[00:34:34.720 --> 00:34:36.760]   load of stuff around my company.
[00:34:36.760 --> 00:34:42.000]   And the ability to verify it comes from action and also legal recourse if I'm not doing it.
[00:34:42.000 --> 00:34:43.480]   There's tons of stuff like that.
[00:34:43.480 --> 00:34:46.400]   But I guess, just before, I feel like you...
[00:34:46.400 --> 00:34:49.560]   Because some people will not have read the paper or listened to this.
[00:34:49.560 --> 00:34:53.640]   When you say supporting verifiable claims, what's an example of a claim that you might
[00:34:53.640 --> 00:34:57.240]   want to verify that would be relevant to trust for the AI development?
[00:34:57.240 --> 00:35:05.600]   A claim you might want to verify is that, say, our system is...
[00:35:05.600 --> 00:35:12.880]   We feel that we've identified many of the main biases in our system and have labeled
[00:35:12.880 --> 00:35:15.880]   it as such.
[00:35:15.880 --> 00:35:22.800]   But we want the world to validate that our system lacks bias in a critical area.
[00:35:22.800 --> 00:35:28.360]   So we're going to use a mechanism, a bias bounty, to get people to compete to try and
[00:35:28.360 --> 00:35:30.560]   find bias traits in our system.
[00:35:30.560 --> 00:35:33.360]   And so there you've got a thing, you're making a claim about it.
[00:35:33.360 --> 00:35:38.440]   I believe that it's relatively unbiased or I've taken steps to log the bias in it.
[00:35:38.440 --> 00:35:42.600]   But then you're introducing an additional thing, which is a transparent mechanism for
[00:35:42.600 --> 00:35:46.440]   other people to go and poke holes in your system and find biases in it.
[00:35:46.440 --> 00:35:49.880]   And that's going to make your claim more verifiable over time.
[00:35:49.880 --> 00:35:55.840]   And if it turns out that your system had some huge trait that you haven't spotted, well,
[00:35:55.840 --> 00:36:00.640]   at least the mechanism helps you identify it and then you're going to iterate from there.
[00:36:00.640 --> 00:36:06.440]   Similarly, we think about the creation of third-party auditing organizations.
[00:36:06.440 --> 00:36:08.360]   So you could have an additional step.
[00:36:08.360 --> 00:36:13.400]   You could have, I have a system making some claim about bias, putting a bias bounty out
[00:36:13.400 --> 00:36:16.600]   there so I have more people hitting my system.
[00:36:16.600 --> 00:36:22.520]   But if I'm being deployed in a critical area, and what I mean by critical is a system that
[00:36:22.520 --> 00:36:25.480]   makes decisions that affect someone's financial life.
[00:36:25.480 --> 00:36:31.200]   So any of these areas that policymakers really, really care about, then I can say, "Okay,
[00:36:31.200 --> 00:36:36.120]   my system will be audited by a third party when it gets used in these areas."
[00:36:36.120 --> 00:36:40.680]   And so now I'm really not asking you to believe me.
[00:36:40.680 --> 00:36:45.800]   I'm asking you to believe the results of my public bounty and the results of this third-party
[00:36:45.800 --> 00:36:46.800]   auditor.
[00:36:46.800 --> 00:36:52.200]   And I think when all of this stuff stacks on itself and gives us the ability to have
[00:36:52.200 --> 00:37:00.640]   trust in systems, other things might be, "I will make a claim about how I value privacy,
[00:37:00.640 --> 00:37:07.160]   but the mechanism by which I will be trading my models and aggregating data will be using
[00:37:07.160 --> 00:37:09.440]   sort of encrypted machine learning techniques."
[00:37:09.440 --> 00:37:13.320]   So there I've got this claim, but you can really verify it because I have an auditable
[00:37:13.320 --> 00:37:19.560]   system that shows you how I'm sort of preserving your privacy while manipulating your data.
[00:37:19.560 --> 00:37:24.000]   And so the idea of this report is basically produce a load of mechanisms that we and a
[00:37:24.000 --> 00:37:28.640]   bunch of other organizations and people think are quite good.
[00:37:28.640 --> 00:37:33.560]   And then the goal over the next year or two is to have organizations who are involved
[00:37:33.560 --> 00:37:38.200]   in the report and others who weren't, implement some of these mechanisms and try them out.
[00:37:38.200 --> 00:37:41.200]   We'll be trying to do this with at least a couple of them.
[00:37:41.200 --> 00:37:42.200]   Oh, cool.
[00:37:42.200 --> 00:37:44.360]   So I can join the red team too.
[00:37:44.360 --> 00:37:45.360]   Yeah, yeah.
[00:37:45.360 --> 00:37:49.640]   I'm really excited about red team.
[00:37:49.640 --> 00:37:54.120]   So obviously, we recommend a shared red team.
[00:37:54.120 --> 00:37:58.320]   That takes a little bit of unpacking because obviously, if you're two proprietary companies,
[00:37:58.320 --> 00:38:03.560]   your red teams can't share lots and lots of information about your proprietary products,
[00:38:03.560 --> 00:38:10.120]   but they can share the methods they use to like red team AI systems and they can standardize
[00:38:10.120 --> 00:38:12.680]   on some of those sort of best practices.
[00:38:12.680 --> 00:38:16.440]   That kind of thing feels really, really useful because eventually, you're going to want to
[00:38:16.440 --> 00:38:19.280]   make claims that you red team the system.
[00:38:19.280 --> 00:38:25.120]   And it's going to be easier to make a trustworthy claim if you use a kind of industry standard
[00:38:25.120 --> 00:38:29.080]   set of techniques that are well documented and many have done, but if you just sort of
[00:38:29.080 --> 00:38:31.080]   cowboy it and do it yourself.
[00:38:31.080 --> 00:38:32.320]   So yeah, please join the red team.
[00:38:32.320 --> 00:38:36.800]   We want lots of people on some shared red team infrastructure eventually.
[00:38:36.800 --> 00:38:40.640]   But the red team infrastructure is actually, it seems like the way you describe it, and
[00:38:40.640 --> 00:38:43.960]   I'm sure this comes from security, but I'm not super familiar with the field.
[00:38:43.960 --> 00:38:46.280]   It's like you have someone internal to your organization, right?
[00:38:46.280 --> 00:38:51.800]   Like you have an internal team that tries to break or tries to find problems with...
[00:38:51.800 --> 00:38:57.640]   You have that, and then you're seeking to find ways to have your internal team share
[00:38:57.640 --> 00:39:01.600]   insights with other people at other organizations.
[00:39:01.600 --> 00:39:05.800]   Now they can't say, "Here's the proprietary system I broke and what I did."
[00:39:05.800 --> 00:39:09.840]   But they can say, "When I like sit down and crack my knuckles and try and like red team
[00:39:09.840 --> 00:39:14.440]   an ML system, here are the approaches I use and here's what's effective."
[00:39:14.440 --> 00:39:20.120]   We not in red teaming, but we have actually done a little bit of this at OpenAI where
[00:39:20.120 --> 00:39:25.880]   in a GPT-2 research paper, we wrote about some of the ways we tried to probe the model
[00:39:25.880 --> 00:39:30.720]   for biases because we think that this is an area that's generally useful, especially useful
[00:39:30.720 --> 00:39:32.280]   to get standards on.
[00:39:32.280 --> 00:39:38.320]   And then since then, we have just been emailing our methodology to lots of people at other
[00:39:38.320 --> 00:39:39.320]   companies.
[00:39:39.320 --> 00:39:42.960]   These people can't tell us about the models for their testing for bias, but they can look
[00:39:42.960 --> 00:39:46.800]   at the probes we're suggesting and tell us if they seem sensible.
[00:39:46.800 --> 00:39:53.120]   And so that shows you how we're able to develop some shared knowledge without breaking proprietary
[00:39:53.120 --> 00:39:54.120]   stuff.
[00:39:54.120 --> 00:39:55.120]   Interesting.
[00:39:55.120 --> 00:40:01.760]   One thing I kept kind of thinking as I was reading your paper is, I use all kinds of
[00:40:01.760 --> 00:40:06.440]   technology that I don't think has made verifiable claims.
[00:40:06.440 --> 00:40:12.040]   I feel like I rely on all kinds of things to work.
[00:40:12.040 --> 00:40:16.520]   And maybe they're making claims, but I'm certainly not aware.
[00:40:16.520 --> 00:40:18.960]   I sort of assume that internet security works.
[00:40:18.960 --> 00:40:25.320]   I assume that I now have all these things plugged into my home network.
[00:40:25.320 --> 00:40:32.080]   But do you think that it sort of seemed like these might be just sort of best practices
[00:40:32.080 --> 00:40:37.240]   for developing any kind of technology or do you think there's something really AI-specific
[00:40:37.240 --> 00:40:38.240]   within it?
[00:40:38.240 --> 00:40:42.000]   And where would you even draw the line where you would sort of call something AI that sort
[00:40:42.000 --> 00:40:44.520]   of needs this kind of treatment?
[00:40:44.520 --> 00:40:47.440]   I think some of it comes down to the...
[00:40:47.440 --> 00:40:48.840]   So where do you draw the line?
[00:40:48.840 --> 00:40:55.000]   I think AI stuff is basically when you cross from a technology that can easily be sort
[00:40:55.000 --> 00:41:02.000]   of audited and analyzed and have the scope of its behavior defined to a technology where
[00:41:02.000 --> 00:41:07.560]   you can somewhat audit and analyze it and sort of list out where it'll do well, but
[00:41:07.560 --> 00:41:10.240]   you can't fully define its scope.
[00:41:10.240 --> 00:41:13.880]   And I think that a lot of like, just sort of once you train a neural net, you have this
[00:41:13.880 --> 00:41:19.120]   like big, like probabilistic system that will mostly do certain things, but it actually
[00:41:19.120 --> 00:41:24.920]   has a surface area that's inherently hard to characterize fully.
[00:41:24.920 --> 00:41:28.760]   It's very, very, very difficult to like fully list it out.
[00:41:28.760 --> 00:41:32.760]   And mostly it doesn't make a huge amount of sense to you because only a sort of subset
[00:41:32.760 --> 00:41:37.000]   of the area, the surface area of your system is actually going to be used at any one time.
[00:41:37.000 --> 00:41:45.680]   So it does have some kind of differences or, you know, bias bounties, right, is a kind
[00:41:45.680 --> 00:41:46.680]   of weird thing.
[00:41:46.680 --> 00:41:55.280]   It's sort of equivalent to saying, all right, before we elect this like mayor or before
[00:41:55.280 --> 00:42:00.200]   we appoint this person to an administrative position, we want a load of people to ask
[00:42:00.200 --> 00:42:05.280]   them a ton of different questions about quite abstract values that they may or may not have
[00:42:05.280 --> 00:42:10.000]   because we want to feel confident that they reflect the values that we'd like someone
[00:42:10.000 --> 00:42:11.000]   to have in that position.
[00:42:11.000 --> 00:42:12.000]   That feels different, actually.
[00:42:12.000 --> 00:42:17.120]   It feels a little different to like normal technologies.
[00:42:17.120 --> 00:42:21.640]   It would be absurd to expect we get to a world where everyone verifies every claim they make
[00:42:21.640 --> 00:42:24.080]   all the time because you have the time.
[00:42:24.080 --> 00:42:29.480]   You know, I mostly go through my life depending on my own beliefs, but other people are sticking
[00:42:29.480 --> 00:42:31.200]   to the rules of the game.
[00:42:31.200 --> 00:42:37.080]   So we all have some cases where we want to go in on something that's happening in our
[00:42:37.080 --> 00:42:40.040]   life and audit every single facet of this.
[00:42:40.040 --> 00:42:44.760]   And I think the way to think about why you need verifiable claims or ability to make
[00:42:44.760 --> 00:42:51.840]   them quite broadly is as governments consider how to sort of govern technology and how to
[00:42:51.840 --> 00:42:57.760]   let technology do the most good while minimizing the harm.
[00:42:57.760 --> 00:43:02.000]   It's probably going to come down to the ability to verify certain things in certain critical
[00:43:02.000 --> 00:43:03.000]   situations.
[00:43:03.000 --> 00:43:07.160]   So you're kind of building all of this stuff, not for the majority of your life, but for
[00:43:07.160 --> 00:43:10.080]   the really narrow edge cases where this has to happen.
[00:43:10.080 --> 00:43:15.680]   But necessarily, that means you need to build quite general tools for verification and then
[00:43:15.680 --> 00:43:18.040]   try to apply them in specific areas.
[00:43:18.040 --> 00:43:20.560]   It's interesting that, well, I don't know, it seems like there's been a lot of sort of
[00:43:20.560 --> 00:43:25.320]   complaining about AI research recently that a lot of the research claims, which are maybe
[00:43:25.320 --> 00:43:30.880]   not so loaded and not so applied and we don't interact with, are actually not really verifiable.
[00:43:30.880 --> 00:43:31.880]   Yeah.
[00:43:31.880 --> 00:43:36.160]   I mean, some of these things are just because there is a compute gap.
[00:43:36.160 --> 00:43:40.640]   There is a minority of organizations with a large amount of compute.
[00:43:40.640 --> 00:43:48.440]   There is a majority of organizations and a huge swath of academia, if not all of academia,
[00:43:48.440 --> 00:43:51.760]   that has very real computational limits.
[00:43:51.760 --> 00:43:56.760]   And this means that at a really high level, you can't really validate claims made by a
[00:43:56.760 --> 00:44:01.320]   subset of the industry because they're doing experiments at scales which you can't hope
[00:44:01.320 --> 00:44:02.320]   to meet.
[00:44:02.320 --> 00:44:08.880]   So some of this is about what are really general tools we can create to just resolve some of
[00:44:08.880 --> 00:44:11.480]   these kind of asymmetries of information.
[00:44:11.480 --> 00:44:18.240]   Because some issues of verifiability are less about your ability to verify a specific thing
[00:44:18.240 --> 00:44:22.160]   in that moment, it's more about having enough kind of cultural understanding of where the
[00:44:22.160 --> 00:44:26.080]   other person is coming from that you kind of understand what they're saying and the
[00:44:26.080 --> 00:44:31.560]   premise behind it and can trust them, which is less you demanding a certain type of verification
[00:44:31.560 --> 00:44:34.880]   but being like, "Okay, well, you're a complete alien to me.
[00:44:34.880 --> 00:44:38.920]   You come from another cultural context or another political ideology.
[00:44:38.920 --> 00:44:45.600]   However, we have this sort of strong shared understanding of this one thing that you're
[00:44:45.600 --> 00:44:47.640]   trying to get me to believe you about."
[00:44:47.640 --> 00:44:55.320]   And right now, if certain organizations wanted to motivate academia to do a certain type
[00:44:55.320 --> 00:45:01.520]   of research, it would depend on, "I come from this big compute premise land, and I'm asking
[00:45:01.520 --> 00:45:08.400]   you to hear me when I list out a concern that only really makes sense if you've done experimentation
[00:45:08.400 --> 00:45:11.240]   at my scale because that's calibrated by intuitions.
[00:45:11.240 --> 00:45:15.720]   So we need to find a way to give these people the ability to have the same conversation
[00:45:15.720 --> 00:45:19.200]   so that you can improve that stuff as well."
[00:45:19.200 --> 00:45:21.000]   So are you going to give them a ton of compute?
[00:45:21.000 --> 00:45:22.960]   What's your solution there?
[00:45:22.960 --> 00:45:32.480]   We basically specifically recommend that governments fund cloud computing, which is a bit different.
[00:45:32.480 --> 00:45:38.320]   It's a bit wonky, but one thing you need to bear in mind is that today, a lot of the way
[00:45:38.320 --> 00:45:44.120]   academic funding works centers usually on the notion of there being some bit of hardware
[00:45:44.120 --> 00:45:46.240]   or capital equipment that you're buying.
[00:45:46.240 --> 00:45:49.720]   And as we know, that stuff depreciates faster than cars.
[00:45:49.720 --> 00:45:54.120]   It's the worst thing to buy if you're a researcher at an academic institution.
[00:45:54.120 --> 00:46:00.560]   You'd be much better placed to buy a cloud computing credit or system that lets you access
[00:46:00.560 --> 00:46:02.960]   a variety of different clouds.
[00:46:02.960 --> 00:46:09.680]   We're generally, when we go and work with governments, pushing this idea that they should
[00:46:09.680 --> 00:46:15.560]   fund some kind of credit that backs onto a bunch of different cloud systems, because
[00:46:15.560 --> 00:46:20.400]   you don't want a government saying, "All right, all of America is going to run on Amazon's
[00:46:20.400 --> 00:46:21.400]   cloud."
[00:46:21.400 --> 00:46:23.520]   That's obviously a bad idea.
[00:46:23.520 --> 00:46:29.440]   But you can probably create a credit which backs onto the infrastructures of five or
[00:46:29.440 --> 00:46:33.840]   six large cloud entities and deal with the competitive concerns that way.
[00:46:33.840 --> 00:46:36.920]   And I think this is surprisingly tractable.
[00:46:36.920 --> 00:46:42.320]   Government policy ideas are relatively simple because they don't need to be any more complicated.
[00:46:42.320 --> 00:46:47.360]   And so we're kind of lobbying, for lack of a better word, governments to do this.
[00:46:47.360 --> 00:46:51.240]   I think the other thing to bear in mind is that lots of governments, because they've
[00:46:51.240 --> 00:46:58.680]   invested in supercomputers, really want to use supercomputers as their compute solution
[00:46:58.680 --> 00:46:59.680]   for academia.
[00:46:59.680 --> 00:47:02.240]   And that mostly doesn't work.
[00:47:02.240 --> 00:47:08.120]   You actually mostly need a dumber, simpler form of hardware for most forms of experimentation.
[00:47:08.120 --> 00:47:14.120]   So you're also saying to governments, "I know you spent all of this money on this supercomputer
[00:47:14.120 --> 00:47:15.120]   and it's wonderful.
[00:47:15.120 --> 00:47:17.520]   And yeah, it's great at simulating nuclear weapons.
[00:47:17.520 --> 00:47:18.520]   We love that.
[00:47:18.520 --> 00:47:21.080]   You don't need it for this.
[00:47:21.080 --> 00:47:23.880]   Stop trying to use it for this exclusively."
[00:47:23.880 --> 00:47:26.880]   So that's also where some of that comes from.
[00:47:26.880 --> 00:47:27.880]   Nice.
[00:47:27.880 --> 00:47:28.880]   I actually have not encountered that.
[00:47:28.880 --> 00:47:29.880]   That's an interesting...
[00:47:29.880 --> 00:47:37.960]   If you're like the US, you're like, "We've spent untold billions on having the winner
[00:47:37.960 --> 00:47:39.720]   of the top 500 list.
[00:47:39.720 --> 00:47:44.160]   And we're in some pitched geopolitical war with China.
[00:47:44.160 --> 00:47:46.200]   Of course we want to use this for AI."
[00:47:46.200 --> 00:47:54.120]   And you're like, "Yeah, dude, but some people just want an 8GPU server."
[00:47:54.120 --> 00:47:56.760]   Actually most people are fine with that.
[00:47:56.760 --> 00:48:03.080]   This thing is not easy to like multiplex and sample out to people compared to AWS or Microsoft
[00:48:03.080 --> 00:48:04.080]   or whatever.
[00:48:04.080 --> 00:48:05.080]   Interesting.
[00:48:05.080 --> 00:48:13.480]   Well, we're a little bit running out of time and I'm curious, we always end with two questions.
[00:48:13.480 --> 00:48:17.800]   I'm particularly interested in your point of view on this.
[00:48:17.800 --> 00:48:23.720]   The first one, you really view a lot of things going on in AI.
[00:48:23.720 --> 00:48:28.280]   From your vantage point at OpenAI and then also the newsletter that you put out.
[00:48:28.280 --> 00:48:35.960]   So what would you say is the topic that people don't pay enough attention to?
[00:48:35.960 --> 00:48:42.160]   The thing that just matters so much more than people compared to how much people look at
[00:48:42.160 --> 00:48:43.160]   it.
[00:48:43.160 --> 00:48:49.840]   I think the thing that no one looks at that really matters is advances in just a very
[00:48:49.840 --> 00:48:56.000]   niche part of computer vision, which is the problem of re-identification of an object
[00:48:56.000 --> 00:48:58.400]   or a person that you've seen previously.
[00:48:58.400 --> 00:49:08.080]   What I mean is that our ability to do pedestrian re-identification now is improving significantly.
[00:49:08.080 --> 00:49:11.840]   It's stacked on all of these ImageNet innovations.
[00:49:11.840 --> 00:49:17.040]   It's stacked on our ability to do rapid feature extraction from video feeds.
[00:49:17.040 --> 00:49:21.400]   It's stacked on a load of just interesting components innovations.
[00:49:21.400 --> 00:49:29.920]   It's creating this stream of technologies that will lead to really, really cheap surveillance
[00:49:29.920 --> 00:49:35.320]   that eventually is deployable on edge systems like drones or whatever by anyone.
[00:49:35.320 --> 00:49:41.080]   I think that we're massively underestimating the effects of that capability because it's
[00:49:41.080 --> 00:49:42.080]   not that interesting.
[00:49:42.080 --> 00:49:43.080]   It's not that advanced.
[00:49:43.080 --> 00:49:47.880]   It doesn't even require massively complex reinforcement learning or any of these things
[00:49:47.880 --> 00:49:49.960]   that researchers like to spend time on.
[00:49:49.960 --> 00:49:50.960]   It's just a basic component.
[00:49:50.960 --> 00:49:57.960]   But that is the component that supports surveillance states and authoritarianism.
[00:49:57.960 --> 00:50:06.280]   That is the component that can make it very easy for an otherwise liberal government to
[00:50:06.280 --> 00:50:13.360]   slip into a form of surveillance and control that no one would really want to have.
[00:50:13.360 --> 00:50:18.840]   I'm actually thinking about, "Can I write a survey or something about this?"
[00:50:18.840 --> 00:50:23.000]   Because it's not helpful for someone like OpenAI to warn of this.
[00:50:23.000 --> 00:50:24.640]   It's sort of a wrong message.
[00:50:24.640 --> 00:50:27.800]   It's maybe okay for me to write about it occasionally in my newsletters.
[00:50:27.800 --> 00:50:28.800]   I do.
[00:50:28.800 --> 00:50:32.280]   But I think about writing an essay like, "Has anyone noticed this?"
[00:50:32.280 --> 00:50:37.560]   Because I gather all of the scores, I look at all of the graphs and stuff.
[00:50:37.560 --> 00:50:38.560]   It's all going up.
[00:50:38.560 --> 00:50:39.560]   It's all going hockey stick.
[00:50:39.560 --> 00:50:40.560]   It's all getting cheap.
[00:50:40.560 --> 00:50:47.560]   It's not very cheerful, but I think it's important.
[00:50:47.560 --> 00:50:48.560]   Wow.
[00:50:48.560 --> 00:50:49.560]   Great answer as expected.
[00:50:49.560 --> 00:50:50.560]   All right.
[00:50:50.560 --> 00:50:53.160]   Here's my second question, which we always ask.
[00:50:53.160 --> 00:50:58.640]   Normally, we're talking to more industry practitioners, but maybe you can apply this to OpenAI.
[00:50:58.640 --> 00:51:02.640]   So when you look at the DML projects that you've witnessed, and OpenAI has actually
[00:51:02.640 --> 00:51:07.200]   had some really spectacular ones.
[00:51:07.200 --> 00:51:15.240]   What's the part of conception to complete that looks the hardest to you or maybe the
[00:51:15.240 --> 00:51:18.960]   most unexpectedly difficult piece of it?
[00:51:18.960 --> 00:51:25.440]   Sort of watching, solving Dota or being the most human at Dota.
[00:51:25.440 --> 00:51:28.800]   Where do things get stuck and why?
[00:51:28.800 --> 00:51:33.000]   Good question.
[00:51:33.000 --> 00:51:41.120]   I think that there are maybe two parts where projects get stuck or have interesting traits.
[00:51:41.120 --> 00:51:43.120]   One is just data.
[00:51:43.120 --> 00:51:46.720]   I used to really want data to not matter so much.
[00:51:46.720 --> 00:51:53.720]   And then you just look at it and realize that whether it's like Dota and how you ingest
[00:51:53.720 --> 00:51:59.400]   data from the game engine there or robotics and how you choose to do domain randomization
[00:51:59.400 --> 00:52:04.640]   and simulation or supervised learning where you're just figuring out what data sets I
[00:52:04.640 --> 00:52:09.200]   have and what mixing proportion do I give them during training and how many runs do
[00:52:09.200 --> 00:52:10.200]   I do.
[00:52:10.200 --> 00:52:11.680]   That just seems very hard.
[00:52:11.680 --> 00:52:13.160]   I think others have talked about this.
[00:52:13.160 --> 00:52:16.040]   It's not really a well-documented science.
[00:52:16.040 --> 00:52:19.720]   It's something that many people treat with intuition and just seems like an easy place
[00:52:19.720 --> 00:52:20.720]   to get stuck.
[00:52:20.720 --> 00:52:23.440]   And then the other is testing.
[00:52:23.440 --> 00:52:30.040]   Once I have a system, how well can I characterize it and what sort of tests can I use from the
[00:52:30.040 --> 00:52:36.400]   existing research literature and what tests do I need to build myself?
[00:52:36.400 --> 00:52:41.560]   We spend a lot of time figuring out new evaluations at OpenAI because for some systems you want
[00:52:41.560 --> 00:52:47.280]   to do a form of eval that doesn't yet exist to characterize performance in some domain.
[00:52:47.280 --> 00:52:53.080]   And figuring out how to test for a performance trait that may not even be present in a system
[00:52:53.080 --> 00:52:54.080]   is really hard.
[00:52:54.080 --> 00:52:55.080]   It's a really, really difficult question.
[00:52:55.080 --> 00:52:56.080]   So those would be the two areas.
[00:52:56.080 --> 00:53:04.080]   I can't help myself actually, as you were talking, I thought of one more question.
[00:53:04.080 --> 00:53:09.580]   I'm sorry to do this, but I feel like the people that I know or that I've watched closely
[00:53:09.580 --> 00:53:13.920]   at OpenAI have been actually spectacularly successful.
[00:53:13.920 --> 00:53:18.600]   They've been part of projects that have really seemed to me have succeeded, like the robot
[00:53:18.600 --> 00:53:22.920]   hand doing the Rubik's Cube and Dota.
[00:53:22.920 --> 00:53:28.080]   Are there a whole bunch of projects that we don't see that have just totally failed?
[00:53:28.080 --> 00:53:31.960]   I don't know if you remember Universe.
[00:53:31.960 --> 00:53:34.960]   That was sort of a failure.
[00:53:34.960 --> 00:53:39.520]   We tried to build a system which was kind of like OpenAI Gym, but the environments would
[00:53:39.520 --> 00:53:45.440]   be every Flash and HTML game that had been published on the internet.
[00:53:45.440 --> 00:53:50.880]   So that failed because of network asynchronicity.
[00:53:50.880 --> 00:53:55.680]   And so basically you ended up having, because we were sandboxing the things in the browser
[00:53:55.680 --> 00:53:59.680]   and you had a separate game engine that needed to go and talk over the network to them, our
[00:53:59.680 --> 00:54:05.120]   rail actually isn't really robust enough to that level of time jitter to do useful stuff.
[00:54:05.120 --> 00:54:07.920]   So that kind of didn't work.
[00:54:07.920 --> 00:54:11.680]   So we have some public failures, which I think is kind of helpful.
[00:54:11.680 --> 00:54:14.160]   Yeah, we have some kind of private ones.
[00:54:14.160 --> 00:54:19.480]   A lot of it is, you know, some people just spend a year or two on an idea, but it ends
[00:54:19.480 --> 00:54:21.160]   up not working out.
[00:54:21.160 --> 00:54:27.160]   Some people, and I won't name the projects that's public, but they came up with a simple
[00:54:27.160 --> 00:54:31.200]   thing that worked really well, and they spent six months trying to come up with what as
[00:54:31.200 --> 00:54:35.560]   a researcher they thought was the more disciplined or better approach to it.
[00:54:35.560 --> 00:54:38.560]   And the simple thing always worked and all of the other things they tried didn't.
[00:54:38.560 --> 00:54:41.560]   So they eventually published a system with like the simple thing.
[00:54:41.560 --> 00:54:48.560]   And they were like, yeah, it works, but I would much rather like my complex idea works.
[00:54:48.560 --> 00:54:55.560]   But we don't, like our big bets, like the hand or Dota or GPT, those have tended to
[00:54:55.560 --> 00:54:56.560]   go okay.
[00:54:56.560 --> 00:55:01.320]   But that's usually because they've come from a place of iteration.
[00:55:01.320 --> 00:55:08.560]   Like Dota came from prior work applying PBO and I think evolutionary algorithms to other
[00:55:08.560 --> 00:55:09.560]   systems.
[00:55:09.560 --> 00:55:13.040]   The hand came from prior work on just like block rotation.
[00:55:13.040 --> 00:55:16.560]   So once you can do block rotation, you can do a Rubik's.
[00:55:16.560 --> 00:55:22.320]   GPT came from prior work on scaling up language models, just a GPT one.
[00:55:22.320 --> 00:55:26.560]   So a lot of it's just happened sort of iteratively in the public domain.
[00:55:26.560 --> 00:55:33.560]   But yeah, we don't have an abnormal lack of failure, nor an abnormal amount of success.
[00:55:33.560 --> 00:55:36.560]   I think it's just like it's pretty in distribution.
[00:55:36.560 --> 00:55:37.560]   I hope.
[00:55:37.560 --> 00:55:38.560]   Awesome.
[00:55:38.560 --> 00:55:39.560]   Well, thanks.
[00:55:39.560 --> 00:55:40.560]   That was actually fun.
[00:55:40.560 --> 00:55:41.560]   Thanks very much.
[00:55:41.560 --> 00:55:42.560]   Yeah.
[00:55:42.560 --> 00:55:43.560]   Thanks.
[00:55:43.560 --> 00:55:44.560]   Thanks, Jack.
[00:55:44.560 --> 00:55:45.560]   All right.
[00:55:45.560 --> 00:55:46.560]   See you, man.
[00:55:46.560 --> 00:55:47.560]   All right.
[00:55:47.560 --> 00:55:47.560]   See you, man.
[00:55:48.560 --> 00:55:51.560]   Bye.
[00:55:51.560 --> 00:55:54.920]   [MUSIC PLAYING]
[00:55:54.920 --> 00:56:03.740]   [BLANK_AUDIO]

