
[00:00:00.000 --> 00:00:07.120]   It's really important to distinguish between the word as a sequence of characters as opposed
[00:00:07.120 --> 00:00:10.520]   to word in the sense of a pairing of form and meaning.
[00:00:10.520 --> 00:00:13.920]   Because what the language model is seeing is only the sequence of characters.
[00:00:13.920 --> 00:00:17.080]   And it's a bit easier to imagine what that's like if you think about a language you don't
[00:00:17.080 --> 00:00:18.080]   speak.
[00:00:18.080 --> 00:00:22.320]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:22.320 --> 00:00:24.440]   and I'm your host, Lucas Biewald.
[00:00:24.440 --> 00:00:28.480]   Today I'm talking to Emily Bender, who is a professor of linguistics at the University
[00:00:28.480 --> 00:00:34.160]   of Washington, who has a really wide range of interests in linguistics and NLP, from
[00:00:34.160 --> 00:00:41.960]   societal issues to multilingual variation to essentially philosophy of linguistics.
[00:00:41.960 --> 00:00:46.880]   And I'm especially excited to talk to her because she was actually my teacher for Linguistics
[00:00:46.880 --> 00:00:50.080]   One at Stanford University where I was an undergrad.
[00:00:50.080 --> 00:00:51.280]   And it was one of my favorite classes.
[00:00:51.280 --> 00:00:52.280]   I still remember it.
[00:00:52.280 --> 00:00:57.020]   I still remember a whole bunch of interesting facts that I learned, and it led to this lifelong
[00:00:57.020 --> 00:00:59.000]   interest in linguistics that I've really enjoyed.
[00:00:59.000 --> 00:01:04.480]   So could not be more excited to have a conversation with her.
[00:01:04.480 --> 00:01:08.480]   I thought it might make sense to start with the paper that you co-authored on the dangers
[00:01:08.480 --> 00:01:09.640]   of stochastic pairs.
[00:01:09.640 --> 00:01:11.040]   Can language models be too big?
[00:01:11.040 --> 00:01:16.720]   Which was notable even to me on Twitter for a lot of, I guess, controversy at Google,
[00:01:16.720 --> 00:01:20.360]   which I was hoping you could maybe start by describing, but then get into the meat of
[00:01:20.360 --> 00:01:22.840]   what the paper actually says.
[00:01:22.840 --> 00:01:23.840]   Yeah.
[00:01:23.840 --> 00:01:28.620]   So it's not in the IPA and hard to pronounce, but the title actually includes an emoji,
[00:01:28.620 --> 00:01:29.620]   right?
[00:01:29.620 --> 00:01:31.620]   The last character of the title is a parrot emoji.
[00:01:31.620 --> 00:01:37.260]   And we were doing that just kind of for fun because we liked the stochastic parrots metaphor.
[00:01:37.260 --> 00:01:41.380]   And there was a while before all this happened that we thought the thing about this paper
[00:01:41.380 --> 00:01:43.580]   would be it was the one with an emoji and a title.
[00:01:43.580 --> 00:01:46.380]   Like that was, little did we know.
[00:01:46.380 --> 00:01:52.540]   But the paper came about because of work that Dr. Timnit Gebru and Dr. Margaret Mitchell
[00:01:52.540 --> 00:01:59.220]   and their team were doing at Google, really trying to connect with the engineering teams
[00:01:59.220 --> 00:02:06.420]   to build in good practices to make the technology work better for more people and do less harm
[00:02:06.420 --> 00:02:07.420]   in the world.
[00:02:07.420 --> 00:02:09.700]   So that was sort of the role that they had there.
[00:02:09.700 --> 00:02:16.200]   And they noticed, especially Dr. Gebru, that there was this big push towards bigger and
[00:02:16.200 --> 00:02:17.540]   bigger language models, right?
[00:02:17.540 --> 00:02:22.300]   If the paper has this table, like just as the number of parameters and the size of the
[00:02:22.300 --> 00:02:26.460]   training data just explodes over the past couple of years, right?
[00:02:26.460 --> 00:02:30.260]   And so Dr. Gebru would actually direct message me on Twitter saying, "Hey, do you know of
[00:02:30.260 --> 00:02:35.060]   any papers that talk about the possible downsides to this?
[00:02:35.060 --> 00:02:36.940]   Any risks or have you written anything?"
[00:02:36.940 --> 00:02:41.180]   And I wrote back and I said, "No, I don't know of any such papers and I haven't written
[00:02:41.180 --> 00:02:46.540]   one, but off the top of my head, here's five or six things that we can be worried about."
[00:02:46.540 --> 00:02:48.220]   And about a day later I said, "You know what?
[00:02:48.220 --> 00:02:49.900]   That feels like a paper outline.
[00:02:49.900 --> 00:02:51.460]   So here's a paper outline.
[00:02:51.460 --> 00:02:53.620]   You want to write this together?"
[00:02:53.620 --> 00:02:59.380]   And so that was early September and the conference we decided to target was FACT, the Fairness,
[00:02:59.380 --> 00:03:03.980]   Accountability, and Transparency Conference, which took place finally in March, 2021.
[00:03:03.980 --> 00:03:07.460]   Submission deadline was October, I think, 8th of 2020.
[00:03:07.460 --> 00:03:12.820]   So in a month we put together this paper and that was possible because it actually wasn't
[00:03:12.820 --> 00:03:18.100]   just the two of us writing it or the four named authors finally, but in fact we had
[00:03:18.100 --> 00:03:19.420]   seven authors.
[00:03:19.420 --> 00:03:25.740]   So Dr. Gebru brought in Dr. Mitchell and it's really important to me to emphasize that they
[00:03:25.740 --> 00:03:28.820]   have doctorates, but I also know them well enough that I'm going to start full naming
[00:03:28.820 --> 00:03:30.460]   them now or first naming them actually.
[00:03:30.460 --> 00:03:35.100]   So Tim Neate brought in Meg and three other members of their team and I brought in my
[00:03:35.100 --> 00:03:38.180]   PhD student, Angelina McMillan Major.
[00:03:38.180 --> 00:03:43.660]   And between the seven of us, we sort of had enough different areas of expertise and literatures
[00:03:43.660 --> 00:03:47.100]   that we've read that we can pull together this survey paper.
[00:03:47.100 --> 00:03:50.780]   And so it came together and it was amazing.
[00:03:50.780 --> 00:03:55.140]   And also an interesting writing experience because we never had a Zoom meeting or anything
[00:03:55.140 --> 00:03:56.980]   where all of us spoke together.
[00:03:56.980 --> 00:04:00.540]   It was all done through remote collaboration in Overleaf.
[00:04:00.540 --> 00:04:05.220]   So not a super common way for research to get done, but it worked in this case.
[00:04:05.220 --> 00:04:08.140]   So the Google authors put it through what they call pub approve over there.
[00:04:08.140 --> 00:04:09.140]   It got approved.
[00:04:09.140 --> 00:04:13.660]   We submitted it to the conference and then put it away because none of us had actually
[00:04:13.660 --> 00:04:15.500]   anticipated working on that in the month of September.
[00:04:15.500 --> 00:04:17.060]   So it was like extra work for everybody.
[00:04:17.060 --> 00:04:20.380]   So we all turned back to the other stuff we need to be doing.
[00:04:20.380 --> 00:04:26.100]   And then in late November, out of nowhere, from my perspective, and I should say that
[00:04:26.100 --> 00:04:30.260]   in telling the story, I'm not at Google, I've not been funded by Google.
[00:04:30.260 --> 00:04:37.220]   And so I only have sort of secondhand understanding of what went on at Google plus what was out
[00:04:37.220 --> 00:04:38.700]   in the press eventually.
[00:04:38.700 --> 00:04:42.620]   But the Google co-authors were told to either retract their paper or take their names off
[00:04:42.620 --> 00:04:43.620]   of it.
[00:04:43.620 --> 00:04:45.140]   And they weren't told why.
[00:04:45.140 --> 00:04:49.420]   And they weren't offered a chance to sort of discuss what might need to be changed about
[00:04:49.420 --> 00:04:50.420]   the paper.
[00:04:50.420 --> 00:04:53.100]   It was just retract it or take your names off of it.
[00:04:53.100 --> 00:04:56.380]   And so we had this strange moment of, okay, what do we do with this paper?
[00:04:56.380 --> 00:04:59.660]   Because it seems kind of odd to put something out with just two authors that actually represents
[00:04:59.660 --> 00:05:02.740]   the work of seven people.
[00:05:02.740 --> 00:05:03.740]   What do we want to do here?
[00:05:03.740 --> 00:05:08.940]   And so my PhD student, Angie and I just returned to the Google co-authors and we said, we will
[00:05:08.940 --> 00:05:10.020]   follow your lead here.
[00:05:10.020 --> 00:05:12.660]   What do you want to have happen?
[00:05:12.660 --> 00:05:14.540]   And they said, no, we want this out in the world.
[00:05:14.540 --> 00:05:16.540]   So you two publish it.
[00:05:16.540 --> 00:05:17.940]   And that was the initial answer.
[00:05:17.940 --> 00:05:22.060]   And then Timmy, sort of on reflection, said, actually, this is not okay.
[00:05:22.060 --> 00:05:26.740]   This is not an okay way to treat a researcher who was hired to do this research, right?
[00:05:26.740 --> 00:05:30.860]   This was literally her job and the job of everyone on that team.
[00:05:30.860 --> 00:05:35.660]   And so she pushed back and the result of all that, you can go find in all the media coverage,
[00:05:35.660 --> 00:05:37.440]   is that she got fired.
[00:05:37.440 --> 00:05:39.200]   Google claims she resigned.
[00:05:39.200 --> 00:05:42.780]   Her team says she got resignated, which is a great neologism.
[00:05:42.780 --> 00:05:49.700]   And that went down fast enough that she was able to then put her name on the paper.
[00:05:49.700 --> 00:05:55.220]   And meanwhile, Meg started like working on documenting what had happened to Timneet.
[00:05:55.220 --> 00:05:59.020]   And the end result of that was that she was fired a few months later, but after the final
[00:05:59.020 --> 00:06:00.480]   version of the paper was done.
[00:06:00.480 --> 00:06:06.620]   So that's why the fourth author is Margaret Schmichel.
[00:06:06.620 --> 00:06:10.580]   So that's a really sad story for everybody involved.
[00:06:10.580 --> 00:06:15.380]   I mean, it's terrible mistreatment of Timneet and Meg and the other members of their team,
[00:06:15.380 --> 00:06:16.980]   those who are on our paper and those who weren't.
[00:06:16.980 --> 00:06:20.020]   It's become, I think, a really difficult environment to work in.
[00:06:20.020 --> 00:06:24.220]   It's sad for Google because they lost really wonderful expertise and a lot of goodwill
[00:06:24.220 --> 00:06:26.260]   in the research community.
[00:06:26.260 --> 00:06:35.380]   And it sheds a light on the sad state of affairs about the way corporate interests are influencing
[00:06:35.380 --> 00:06:38.100]   what's happening in research in our field right now.
[00:06:38.100 --> 00:06:43.300]   On the other hand, my co-authors and I still maintain we all really enjoyed the experience
[00:06:43.300 --> 00:06:47.420]   of working on this paper together and of weathering the stuff afterwards together.
[00:06:47.420 --> 00:06:53.280]   And one weird result is that this paper has gotten way more attention than it ordinarily
[00:06:53.280 --> 00:06:54.280]   would have.
[00:06:54.280 --> 00:06:55.280]   I mean, I think it's a good paper.
[00:06:55.280 --> 00:06:56.280]   It's a solid paper.
[00:06:56.280 --> 00:07:00.660]   And boy, did we put a lot of polish on it between the submission version and the camera
[00:07:00.660 --> 00:07:04.120]   ready because we knew it was going to be read by a lot of people.
[00:07:04.120 --> 00:07:10.540]   When I put up the camera ready as a preprint, I didn't put it on archive because those tend
[00:07:10.540 --> 00:07:13.440]   to get cited instead of the final published versions.
[00:07:13.440 --> 00:07:18.800]   So I just put it on my website and tweeted out a link with a bitly link to shorten it
[00:07:18.800 --> 00:07:21.420]   so that I could see how many times it was downloaded.
[00:07:21.420 --> 00:07:27.920]   It has been downloaded through that link alone over 10,000 times.
[00:07:27.920 --> 00:07:33.460]   And I know that there's other ways to get to it, which is way out of scale to anything
[00:07:33.460 --> 00:07:35.740]   that I've ever written otherwise.
[00:07:35.740 --> 00:07:41.600]   So that's been interesting as a researcher, but it's also, I think, fortunate because
[00:07:41.600 --> 00:07:43.920]   it has come to the attention of the public.
[00:07:43.920 --> 00:07:48.020]   And I think that this technology is widespread.
[00:07:48.020 --> 00:07:49.180]   It's being used.
[00:07:49.180 --> 00:07:51.820]   It's being used in lots of different ways.
[00:07:51.820 --> 00:07:56.660]   And so it's really valuable that the public at large has a chance to understand what's
[00:07:56.660 --> 00:07:57.660]   going on.
[00:07:57.660 --> 00:08:05.380]   And so through Google's gross misstep, I and my co-authors have been given the chance to
[00:08:05.380 --> 00:08:09.620]   help educate the public, which is something that I do feel fortunate about.
[00:08:09.620 --> 00:08:15.780]   I'd love to kind of get into what the paper talks about, but do you have any sense or
[00:08:15.780 --> 00:08:19.700]   has Google made any comments about what their objection was?
[00:08:19.700 --> 00:08:23.180]   Because I sort of had this feeling that it must be a really incendiary paper.
[00:08:23.180 --> 00:08:28.980]   And then in the prep for this interview, I actually read it and it felt pretty uncontroversial,
[00:08:28.980 --> 00:08:31.500]   I guess, was my feeling reading it.
[00:08:31.500 --> 00:08:34.900]   So I just wonder, I mean, maybe it's hard to know, but as they said, anything about
[00:08:34.900 --> 00:08:37.140]   what they didn't like about it?
[00:08:37.140 --> 00:08:39.020]   So there was, yes.
[00:08:39.020 --> 00:08:45.020]   In public comments, there's been things like, it doesn't cite relevant work that is trying
[00:08:45.020 --> 00:08:47.540]   to mitigate some of these issues.
[00:08:47.540 --> 00:08:52.160]   But at no point were we ever told which work we should have been citing.
[00:08:52.160 --> 00:08:55.160]   And we do in fact cite some work that is trying to mitigate these issues.
[00:08:55.160 --> 00:08:58.180]   So I don't know quite what that was about, but you're absolutely right.
[00:08:58.180 --> 00:09:02.900]   It was not, you know, we figured that we'd be ruffling some feathers with this paper
[00:09:02.900 --> 00:09:07.500]   because we were basically saying, hey, this thing that everyone's having so much fun chasing,
[00:09:07.500 --> 00:09:11.740]   maybe let's go a little bit slower and think about, you know, what kinds of downsides there
[00:09:11.740 --> 00:09:12.740]   are and how to do this safely.
[00:09:12.740 --> 00:09:16.460]   You know, there's going to be people who don't want to hear that, but we honestly thought
[00:09:16.460 --> 00:09:21.420]   it was going to be OpenAI who was upset because we, you know, GPT-3 is kind of the best known
[00:09:21.420 --> 00:09:25.860]   example of this and it was our running example too.
[00:09:25.860 --> 00:09:29.220]   So we thought we'd ruffle some feathers, did not realize we were going to be ruffling feathers
[00:09:29.220 --> 00:09:30.780]   inside Google.
[00:09:30.780 --> 00:09:33.220]   And it's basically a survey paper, right?
[00:09:33.220 --> 00:09:34.420]   We didn't run any experiments.
[00:09:34.420 --> 00:09:35.420]   We didn't do any analysis.
[00:09:35.420 --> 00:09:40.300]   What we did was we pulled together a bunch of different relevant perspectives on large
[00:09:40.300 --> 00:09:44.460]   language models and sort of brought them all together in one place.
[00:09:44.460 --> 00:09:54.180]   So it is surprising that the paper seems to have been part of the cause of Google, you
[00:09:54.180 --> 00:09:59.300]   know, basically blowing up this amazing asset that it had in terms of its ethical AI team.
[00:09:59.300 --> 00:10:00.860]   Interesting.
[00:10:00.860 --> 00:10:08.660]   And I guess one reading of your paper is, hey, you know, we should consider, you know,
[00:10:08.660 --> 00:10:12.180]   the downsides of large language models.
[00:10:12.180 --> 00:10:14.620]   I think maybe another person might read it.
[00:10:14.620 --> 00:10:18.340]   This might be an unfair reading, but maybe I could imagine someone having hurt feelings
[00:10:18.340 --> 00:10:22.860]   if they were working on large language models and they read your paper saying it's like
[00:10:22.860 --> 00:10:27.020]   an unethical thing to do to build large language models.
[00:10:27.020 --> 00:10:29.540]   Would that be like an overstatement of your claims?
[00:10:29.540 --> 00:10:35.340]   I don't have the paper in front of me, but I think maybe that could hurt feelings.
[00:10:35.340 --> 00:10:36.380]   I'm not sure.
[00:10:36.380 --> 00:10:42.580]   So I also do a lot of work in the space of societal impact of NLP in general, and that
[00:10:42.580 --> 00:10:46.380]   sometimes goes under the title of ethics in NLP.
[00:10:46.380 --> 00:10:52.820]   And I do see a lot of people reacting to that topic with hurt feelings.
[00:10:52.820 --> 00:10:57.300]   And I think it's connected with the way in which people identify with their work.
[00:10:57.300 --> 00:11:02.660]   And so if you say, hey, let's think about this technology we're building and how it
[00:11:02.660 --> 00:11:07.540]   behaves in the world and what we can do to make it be beneficial, and you use the term
[00:11:07.540 --> 00:11:14.460]   ethics to describe that, sometimes people want to read that as you're calling me unethical.
[00:11:14.460 --> 00:11:20.180]   And I think that that direction of the conversation is rarely actually valuable.
[00:11:20.180 --> 00:11:25.300]   And I do think that in general, people in this space want to be doing good things in
[00:11:25.300 --> 00:11:26.300]   the world.
[00:11:26.300 --> 00:11:30.260]   You know, certainly there are people who are working on technology with the goal of making
[00:11:30.260 --> 00:11:31.740]   a lot of money doing it.
[00:11:31.740 --> 00:11:39.220]   But I think there's this caricature of the tycoon or whoever who's just happy to crush
[00:11:39.220 --> 00:11:42.660]   all the little people to make as much money as possible.
[00:11:42.660 --> 00:11:44.900]   That's out there probably.
[00:11:44.900 --> 00:11:50.140]   But I think much more frequently, people are working within systems that give them certain
[00:11:50.140 --> 00:11:55.140]   commitments around maximizing value for shareholders and stuff like that, that make it harder to
[00:11:55.140 --> 00:11:58.180]   put on the brakes on some things that are making money right now for shareholders and
[00:11:58.180 --> 00:11:59.620]   take a bigger picture view.
[00:11:59.620 --> 00:12:03.460]   But it is much more valuable to talk about it in terms of what are those systems?
[00:12:03.460 --> 00:12:04.460]   What are the incentives?
[00:12:04.460 --> 00:12:08.860]   What can we as individuals do within those systems rather than think about people as
[00:12:08.860 --> 00:12:09.860]   ethical or unethical?
[00:12:09.860 --> 00:12:14.660]   I'm not sure that really speaks to your question, but hopefully it's somewhat helpful.
[00:12:14.660 --> 00:12:20.620]   No, I mean, I think you're saying that maybe your point is a little more nuanced than maybe
[00:12:20.620 --> 00:12:21.620]   some would take away.
[00:12:21.620 --> 00:12:27.340]   And I can see, I mean, I think, I mean, I guess, you know, I run a company and I love
[00:12:27.340 --> 00:12:33.620]   technology and I love building, I do recognize that lots of people get hurt.
[00:12:33.620 --> 00:12:39.700]   And I think it's great that people are pointing out issues and also kind of pumping the brakes
[00:12:39.700 --> 00:12:41.540]   and flagging the stuff.
[00:12:41.540 --> 00:12:46.660]   But I just, I could kind of see how someone might feel a little offended by it.
[00:12:46.660 --> 00:12:51.620]   I wasn't sure if I was kind of jumping to something or, like, I guess my question, well,
[00:12:51.620 --> 00:12:56.900]   the question that I kept thinking about with the whole paper in general as I was reading
[00:12:56.900 --> 00:13:00.500]   it is even sort of setting aside making money, right?
[00:13:00.500 --> 00:13:06.380]   Just talk about research and just the sort of excitement of building models that work,
[00:13:06.380 --> 00:13:08.620]   which I just, I feel that so deeply.
[00:13:08.620 --> 00:13:13.860]   Like GPT-3 for all its flaws is kind of amazing, what it does.
[00:13:13.860 --> 00:13:17.820]   I wouldn't have expected it to work so well.
[00:13:17.820 --> 00:13:25.940]   And so I guess, would you feel, would you argue that those kinds of directions of research
[00:13:25.940 --> 00:13:30.900]   should stop or what would you want an organization like OpenAI to do differently?
[00:13:30.900 --> 00:13:37.300]   Because I think it's a good example of a place that's kind of actually really showed that
[00:13:37.300 --> 00:13:42.880]   bigger models do kind of, you know, it's not obvious that like bigger models would perform
[00:13:42.880 --> 00:13:46.420]   tasks better at many extra orders of magnitude.
[00:13:46.420 --> 00:13:52.300]   Like, do you think, would you prefer that that research doesn't happen or happen differently
[00:13:52.300 --> 00:13:53.300]   somehow?
[00:13:53.300 --> 00:13:56.780]   So I think it's worth saying that OpenAI has actually put a lot of effort into thinking
[00:13:56.780 --> 00:14:00.540]   about what are the possible downsides and what could happen when this technology is
[00:14:00.540 --> 00:14:01.740]   released in the world.
[00:14:01.740 --> 00:14:04.500]   And that's important to note.
[00:14:04.500 --> 00:14:06.180]   And I'm glad that they're doing that.
[00:14:06.180 --> 00:14:12.580]   I think that what I would like to see more of is first of all, that kind of work, like
[00:14:12.580 --> 00:14:15.560]   what are the possible failure modes and how do they impact people?
[00:14:15.560 --> 00:14:19.660]   And then also when this is working as intended, you know, how can that impact people?
[00:14:19.660 --> 00:14:23.980]   And OpenAI has been doing some of that and I think that's great and they should do more,
[00:14:23.980 --> 00:14:30.140]   but also there's, you can look to other fields of engineering where, you know, before you
[00:14:30.140 --> 00:14:33.240]   take something and you put it into the world in a place where people are going to rely
[00:14:33.240 --> 00:14:36.940]   on it, there's all kinds of testing that has to be done and sort of understanding of what
[00:14:36.940 --> 00:14:40.500]   are the tolerances and, you know, what works and what doesn't, and what are the, what's
[00:14:40.500 --> 00:14:43.940]   the range of temperatures that this thing could be applicable in and what are the things
[00:14:43.940 --> 00:14:46.420]   you have to check for and certify and things like that.
[00:14:46.420 --> 00:14:51.180]   And we don't have very much of that yet going on in NLP.
[00:14:51.180 --> 00:14:55.320]   I can speak less to other areas of AI, but I honestly, I think there's similar issues
[00:14:55.320 --> 00:14:56.760]   elsewhere in AI.
[00:14:56.760 --> 00:15:02.360]   And so there's work actually that was done at Google by Meg Mitchell and Timnit Gebru
[00:15:02.360 --> 00:15:07.280]   and others on a framework called model cards, which was sort of steps in that direction
[00:15:07.280 --> 00:15:10.520]   of like, if you've built a model, what does somebody who's going to use this model need
[00:15:10.520 --> 00:15:11.980]   to know about it?
[00:15:11.980 --> 00:15:15.080]   And that's the kind of thing that I would like to see more of.
[00:15:15.080 --> 00:15:20.880]   And that is in contrast to just rampant AI hype, where people build something, it's cool,
[00:15:20.880 --> 00:15:22.260]   it's fun, it works well.
[00:15:22.260 --> 00:15:23.880]   And somehow that's not enough.
[00:15:23.880 --> 00:15:28.740]   And people have to say, you know, it's not enough that GPT-3 can produce coherent text.
[00:15:28.740 --> 00:15:31.940]   People have to say it's understanding language, which it absolutely isn't, as I'm sure we'll
[00:15:31.940 --> 00:15:32.940]   talk about later.
[00:15:32.940 --> 00:15:35.940]   Yeah, you have two good segues, but yeah.
[00:15:35.940 --> 00:15:36.940]   Yeah.
[00:15:36.940 --> 00:15:40.180]   It is all connected, right?
[00:15:40.180 --> 00:15:47.440]   So for some reason, the culture around AI is all about these, like trying to reach for
[00:15:47.440 --> 00:15:54.260]   these big claims rather than trying to build really well-scoped, reliable, sufficiently
[00:15:54.260 --> 00:15:57.580]   documented that they can be used safely and reliably systems.
[00:15:57.580 --> 00:16:00.820]   And so that's the direction that I would like to see more of is one thing.
[00:16:00.820 --> 00:16:05.620]   And then another thing, and we get into this in the paper, is that if the main pathway
[00:16:05.620 --> 00:16:12.020]   to success these days is just bigger and bigger and bigger, then you cut out lots of languages,
[00:16:12.020 --> 00:16:14.940]   communities, even within the languages that generally are well-supported, because they
[00:16:14.940 --> 00:16:17.100]   just can't amass that much data.
[00:16:17.100 --> 00:16:23.180]   And you also cut out smaller research groups, smaller companies that are not sitting on
[00:16:23.180 --> 00:16:29.460]   the kind of collections of data that, you know, Google is, or Facebook is, or Amazon
[00:16:29.460 --> 00:16:30.460]   is.
[00:16:30.460 --> 00:16:32.060]   Microsoft also does a bunch of big data work.
[00:16:32.060 --> 00:16:35.380]   They don't seem to have amassed data quite the same way as the other big ones.
[00:16:35.380 --> 00:16:41.380]   And that is unfortunate because it, I think, stifles creativity to a certain extent.
[00:16:41.380 --> 00:16:45.540]   If the whole community is rushing towards this one goal that only some can really effectively
[00:16:45.540 --> 00:16:51.180]   do, then we lose out on the other things that people might be trying instead.
[00:16:51.180 --> 00:16:56.860]   And I guess maybe a less obvious concern that you talk about in the paper is talking about
[00:16:56.860 --> 00:17:02.180]   how the models can encode bias in ways that are hard to notice.
[00:17:02.180 --> 00:17:07.300]   And I was wondering if you could, like, I guess when you talk about the harms that might
[00:17:07.300 --> 00:17:13.700]   happen from natural language models, do you have examples of things that are actually
[00:17:13.700 --> 00:17:18.500]   happening now, or is this more of a future-looking thing of, like, we're worried about as NLP
[00:17:18.500 --> 00:17:21.620]   becomes more pervasive, like, worrying about future harms?
[00:17:21.620 --> 00:17:22.620]   Yeah, no.
[00:17:22.620 --> 00:17:25.700]   So, I mean, absolutely happening now, and therefore easy to predict that it will keep
[00:17:25.700 --> 00:17:27.900]   happening in the future if we don't change.
[00:17:27.900 --> 00:17:32.260]   And here, the work of Safiya Noble with her book, Algorithms of Oppression, is a really
[00:17:32.260 --> 00:17:34.380]   important documentation of this.
[00:17:34.380 --> 00:17:41.420]   So, she looked into what are the ways in which identities which properly belong to the groups
[00:17:41.420 --> 00:17:45.780]   of people who have those identities are represented and reflected back to people in search.
[00:17:45.780 --> 00:17:51.300]   And in particular, her running example is the phrase "Black girls" and also "Black
[00:17:51.300 --> 00:17:52.300]   women."
[00:17:52.300 --> 00:17:56.780]   And, you know, these things have changed over time, and she's very careful to document when
[00:17:56.780 --> 00:17:59.740]   she's talking about particular examples what the date was.
[00:17:59.740 --> 00:18:06.260]   But early on, as she started this project, the phrase "Black girls" as a search keyword
[00:18:06.260 --> 00:18:08.620]   basically turned up pornography.
[00:18:08.620 --> 00:18:13.700]   And that, you might say, is, well, that's just in the data.
[00:18:13.700 --> 00:18:14.700]   Well, what data?
[00:18:14.700 --> 00:18:15.700]   Right?
[00:18:15.700 --> 00:18:16.700]   Where did that data come from?
[00:18:16.700 --> 00:18:25.100]   And if you get into the heart of her book, it's basically around that that's in the data
[00:18:25.100 --> 00:18:31.500]   because of the way in which the economy of the internet allows people to purchase and
[00:18:31.500 --> 00:18:33.940]   make money off of identity terms.
[00:18:33.940 --> 00:18:39.580]   And once these things were flagged, Google sort of like piecemeal making changes.
[00:18:39.580 --> 00:18:44.060]   So, you don't get pornography as the results for the search term "Black girls" anymore.
[00:18:44.060 --> 00:18:48.820]   But it's also possible to sort of poke at things and tell that it's very much sort of
[00:18:48.820 --> 00:18:53.380]   individual after the fact changes as opposed to anyone going through and systematically
[00:18:53.380 --> 00:19:00.700]   thinking about how to redesign the way that search engines and the sort of advertising
[00:19:00.700 --> 00:19:07.980]   driven ranking of search latches on to these incentives and then amplifies them.
[00:19:07.980 --> 00:19:13.780]   So one ongoing discussion in the AI community, you see it pop up on Twitter with great regularity,
[00:19:13.780 --> 00:19:19.900]   is, is the problem that the data is biased only or do the models also contribute?
[00:19:19.900 --> 00:19:22.420]   And the answer is absolutely models also contribute.
[00:19:22.420 --> 00:19:26.380]   And then there's this other layer to it of, well, that's just what's in the data.
[00:19:26.380 --> 00:19:32.300]   So one of the other really embarrassing examples for Google was there's a point at which Google
[00:19:32.300 --> 00:19:37.860]   image search turned up pictures of gorillas when you were searching for Black people.
[00:19:37.860 --> 00:19:43.340]   And I forget exactly the particular configuration of that, but embarrassing and awful and racist.
[00:19:43.340 --> 00:19:47.980]   And one reaction at the time was, well, that's just in the underlying data.
[00:19:47.980 --> 00:19:49.420]   And so, you know, not our fault.
[00:19:49.420 --> 00:19:53.940]   We're just showing what the world is saying, except that it's not true, right?
[00:19:53.940 --> 00:20:00.020]   Because the way the algorithms that do the ranking of search results and also the bidding
[00:20:00.020 --> 00:20:07.220]   for the AdWords is that is emphasizing particular incentives.
[00:20:07.220 --> 00:20:09.620]   So there is a certain thing in the underlying data.
[00:20:09.620 --> 00:20:12.220]   There's also the question of how did you collect that data?
[00:20:12.220 --> 00:20:13.220]   Where did it come from?
[00:20:13.220 --> 00:20:14.220]   What does it actually represent?
[00:20:14.220 --> 00:20:15.220]   It is not the world as it is.
[00:20:15.220 --> 00:20:17.620]   It is some particular collection of data.
[00:20:17.620 --> 00:20:19.860]   And then what is the optimization metric?
[00:20:19.860 --> 00:20:23.140]   What are all these modeling decisions that you've made and how does that interact with
[00:20:23.140 --> 00:20:26.540]   the various biases in the data and what's the incentive structure?
[00:20:26.540 --> 00:20:29.580]   So Sophia Noble's work is a great point to look.
[00:20:29.580 --> 00:20:38.260]   Latanya Sweeney documented, this is a 2013 paper, how if you put in at that point an
[00:20:38.260 --> 00:20:42.500]   African-American sounding name, one of the ads that would pop up suggested that that
[00:20:42.500 --> 00:20:44.300]   person had a criminal history.
[00:20:44.300 --> 00:20:47.940]   And if you put in a white sounding name, you tend to get just a more information about
[00:20:47.940 --> 00:20:48.940]   so-and-so.
[00:20:48.940 --> 00:20:50.140]   And that does real harm in the world.
[00:20:50.140 --> 00:20:54.340]   And it wasn't 100%, but it's significantly different between the two groups of names.
[00:20:54.340 --> 00:20:58.100]   It does real harm in the world because if you imagine someone is applying for a job
[00:20:58.100 --> 00:21:02.860]   or just making friends and someone does a Google search on them and here comes alongside
[00:21:02.860 --> 00:21:07.860]   this message suggesting they might be a criminal, that does harm.
[00:21:07.860 --> 00:21:09.340]   And then if I can give one more example.
[00:21:09.340 --> 00:21:11.780]   Please, yeah, these are great.
[00:21:11.780 --> 00:21:16.500]   Elia Robbins Spear did a really interesting work example around sentiment analysis and
[00:21:16.500 --> 00:21:17.500]   word embeddings.
[00:21:17.500 --> 00:21:18.500]   All right.
[00:21:18.500 --> 00:21:23.140]   So sentiment analysis is the task of taking some natural language text, and her example
[00:21:23.140 --> 00:21:30.340]   is English, and using it to calculate or predict the sentiment.
[00:21:30.340 --> 00:21:33.820]   Is this a text expressing positive feeling towards something, negative feeling towards
[00:21:33.820 --> 00:21:35.980]   something or not expressing feelings?
[00:21:35.980 --> 00:21:39.880]   And the particular data set she was working with, I think was Yelp restaurant reviews.
[00:21:39.880 --> 00:21:42.460]   So there it's take the text, predict the stars.
[00:21:42.460 --> 00:21:44.100]   Yeah, I've used that data set.
[00:21:44.100 --> 00:21:45.100]   Yeah, for sure.
[00:21:45.100 --> 00:21:46.100]   Right.
[00:21:46.100 --> 00:21:49.240]   And then as an external component, she's using word embeddings, which are representations
[00:21:49.240 --> 00:21:53.920]   of words into a vector space based on what other words they co-occur with.
[00:21:53.920 --> 00:21:58.440]   So some of the training data is in domain, the Yelp reviews, but then there's this component
[00:21:58.440 --> 00:22:01.920]   that's trained on general web garbage.
[00:22:01.920 --> 00:22:08.840]   And what she found using these sort of generic word embeddings was that the system systematically
[00:22:08.840 --> 00:22:12.160]   unpredicted the star ratings for Mexican restaurants.
[00:22:12.160 --> 00:22:13.560]   All right.
[00:22:13.560 --> 00:22:16.760]   And so she digs into it and looks into it and looks into why.
[00:22:16.760 --> 00:22:22.360]   And it turns out that because that general web garbage included the discourse about immigration
[00:22:22.360 --> 00:22:29.160]   into the US from and through Mexico, which has lots of really negative toxic opinions
[00:22:29.160 --> 00:22:35.880]   of Mexican people, the word embeddings picked up the word Mexican as akin to other negative
[00:22:35.880 --> 00:22:37.220]   sentiment words.
[00:22:37.220 --> 00:22:40.840]   And so if in your review of the restaurant, you called it a Mexican restaurant, according
[00:22:40.840 --> 00:22:43.160]   to the system, you have said something negative about it.
[00:22:43.160 --> 00:22:45.200]   So you can't possibly be giving it a five star review.
[00:22:45.200 --> 00:22:47.320]   Wow, that's a really interesting example.
[00:22:47.320 --> 00:22:53.080]   And I guess my next question is going to be, how do models play into this?
[00:22:53.080 --> 00:22:58.560]   I guess that's a good example of how not just the underlying data can have bias, but the
[00:22:58.560 --> 00:23:01.360]   model can literally have its own bias.
[00:23:01.360 --> 00:23:03.080]   Yeah.
[00:23:03.080 --> 00:23:10.360]   So the word embeddings picked up on co-occurrences between the word Mexican and lots of other
[00:23:10.360 --> 00:23:13.080]   things that also co-occurred with negative sentiment.
[00:23:13.080 --> 00:23:16.200]   And then that was used as a component in this other model.
[00:23:16.200 --> 00:23:20.960]   So yeah, there wasn't in the underlying Yelp reviews, any particular reason that the Mexican
[00:23:20.960 --> 00:23:23.880]   restaurants were rated lower.
[00:23:23.880 --> 00:23:29.080]   I don't know for sure if they were rated on average exactly the same, but it doesn't matter
[00:23:29.080 --> 00:23:33.800]   because the error was the system under predicting for any given restaurant.
[00:23:33.800 --> 00:23:36.440]   On average, it was missing in the low direction.
[00:23:36.440 --> 00:23:44.480]   So yeah, so that's a kind of bias that was picked up from an external data set.
[00:23:44.480 --> 00:23:50.120]   And we tend in NLP to use word embeddings as really handy detailed representations of
[00:23:50.120 --> 00:23:52.040]   word in quotes meaning, right?
[00:23:52.040 --> 00:23:55.440]   So word similarity, including semantic similarity.
[00:23:55.440 --> 00:24:02.320]   And if we don't pay attention to what meaning was picked up, what co-occurrence was picked
[00:24:02.320 --> 00:24:06.800]   up, then we can end up with stuff we really don't want in our systems.
[00:24:06.800 --> 00:24:11.840]   And I guess what would you recommend doing about that?
[00:24:11.840 --> 00:24:16.280]   Because they're really useful, word embeddings.
[00:24:16.280 --> 00:24:21.280]   And I'm sure in this case, it seems pretty simple of like, you're actually like it's
[00:24:21.280 --> 00:24:22.720]   hurting your performance, right?
[00:24:22.720 --> 00:24:27.040]   So it's not even like a model performance trade off here.
[00:24:27.040 --> 00:24:30.520]   So what could you possibly do?
[00:24:30.520 --> 00:24:34.360]   So there is a lot of work on so-called de-biasing of word embeddings.
[00:24:34.360 --> 00:24:38.920]   And if you look at Spear's work, she continues on to do some of that.
[00:24:38.920 --> 00:24:44.280]   And I think that part of it is work with more curated data sets.
[00:24:44.280 --> 00:24:52.360]   So the discourse around immigration from and through Mexico, even if you stick with only
[00:24:52.360 --> 00:24:57.280]   things like reputable news sources, you're still going to find that garbage, right?
[00:24:57.280 --> 00:25:03.280]   So that alone is not going to solve it, but it can be better, right?
[00:25:03.280 --> 00:25:09.120]   It's not possible to come up with a fully bias-free data set nor fully bias-free word
[00:25:09.120 --> 00:25:11.400]   embeddings, but you can do better.
[00:25:11.400 --> 00:25:15.800]   So one step is to sort of say, okay, how much better can we do with curated data?
[00:25:15.800 --> 00:25:18.480]   What about de-biasing techniques for the biases that we're aware of?
[00:25:18.480 --> 00:25:22.280]   Part of the problem with the de-biasing techniques is you have to know what you're looking for.
[00:25:22.280 --> 00:25:25.240]   And then on top of that, to think through failure modes.
[00:25:25.240 --> 00:25:29.960]   So in a particular use case, when you're building some technology, who are the stakeholders?
[00:25:29.960 --> 00:25:31.880]   Who's going to be impacted by it?
[00:25:31.880 --> 00:25:37.040]   If someone's restaurant rating is under-predicted for some reason, what does that mean in an
[00:25:37.040 --> 00:25:38.800]   actual use context?
[00:25:38.800 --> 00:25:41.120]   And what should we be testing for, right?
[00:25:41.120 --> 00:25:45.920]   To see if we have sufficiently de-biased for our use case for the stakeholders who are
[00:25:45.920 --> 00:25:48.800]   most likely to experience adverse impacts.
[00:25:48.800 --> 00:25:53.600]   I guess it does seem like it would be incredibly, I mean, it seems like it would actually be
[00:25:53.600 --> 00:25:58.440]   impossible to find a sort of unbiased data set of human language.
[00:25:58.440 --> 00:25:59.440]   Right.
[00:25:59.440 --> 00:26:00.440]   It doesn't exist.
[00:26:00.440 --> 00:26:05.760]   And I guess these are good segues into other papers that I want to talk about.
[00:26:05.760 --> 00:26:11.120]   So maybe we should just, in the interest of time, we should move on to the second paper
[00:26:11.120 --> 00:26:14.680]   that we want to talk about just to make sure we get to it, which is around, let me see
[00:26:14.680 --> 00:26:15.680]   if I can summarize this.
[00:26:16.160 --> 00:26:21.640]   This is basically sort of saying that language modeling only on kind of what you call form,
[00:26:21.640 --> 00:26:26.520]   which I think is just sort of like the words kind of coming through, this kind of GPT-3
[00:26:26.520 --> 00:26:32.240]   types of models that just sort of like look at these strings of words, can't have understanding,
[00:26:32.240 --> 00:26:33.960]   like true understanding.
[00:26:33.960 --> 00:26:37.560]   And I just thought one thing that was interesting is that you said you wrote the paper to sort
[00:26:37.560 --> 00:26:42.040]   of like end some kind of debate on Twitter that I was definitely not aware of.
[00:26:42.040 --> 00:26:45.680]   But I actually, I think I'm kind of coming into something with maybe more context than
[00:26:45.680 --> 00:26:46.680]   I knew.
[00:26:46.680 --> 00:26:50.880]   So maybe you can sort of summarize what the different possible positions are here and
[00:26:50.880 --> 00:26:53.400]   what you want to put to rest.
[00:26:53.400 --> 00:26:57.560]   So I kept finding myself getting into arguments on Twitter with people who were claiming that
[00:26:57.560 --> 00:27:00.360]   language models were understanding things.
[00:27:00.360 --> 00:27:01.560]   And I was like, no, they're not.
[00:27:01.560 --> 00:27:02.560]   They can't possibly be.
[00:27:02.560 --> 00:27:04.760]   And it's important to pin down what we mean by language models, right?
[00:27:04.760 --> 00:27:10.600]   So a language model is something like GPT-3 or BERT or otherwise, where it's training
[00:27:10.600 --> 00:27:17.800]   data is a whole bunch of text and the training task is predicting words in the text.
[00:27:17.800 --> 00:27:21.840]   So some of the times it's done sequentially, sometimes it's done with a masked language
[00:27:21.840 --> 00:27:27.560]   model objective where certain words are dropped out and the training objective is, okay, put
[00:27:27.560 --> 00:27:34.280]   those words back in and then do your model updating to gradient descent, et cetera.
[00:27:34.280 --> 00:27:39.000]   And for me as a linguist, I look at that and go, okay, useful technology, interesting,
[00:27:39.000 --> 00:27:43.040]   incredibly helpful in things like speech recognition and machine translation, where an important
[00:27:43.040 --> 00:27:46.680]   subtask is, okay, what's a likely string, right?
[00:27:46.680 --> 00:27:53.840]   So in a speech recognition setup, the acoustic model says, okay, here's a range of text strings
[00:27:53.840 --> 00:27:55.820]   that sound might've corresponded to.
[00:27:55.820 --> 00:28:01.480]   And then the language model comes in and says, okay, yeah, but it's important to rec a nice
[00:28:01.480 --> 00:28:03.480]   beach is a ridiculous thing to say.
[00:28:03.480 --> 00:28:05.920]   And it's important to recognize speech as a reasonable thing to say.
[00:28:05.920 --> 00:28:07.400]   So we're going to rank that one higher.
[00:28:07.400 --> 00:28:12.960]   So that's the kind of form-based tasks that they're initially meant for and good at.
[00:28:12.960 --> 00:28:17.880]   And then what's happened with the neural language modeling revolution in the past few years
[00:28:17.880 --> 00:28:23.480]   is that when you extract the word embeddings from language model, you have really finely
[00:28:23.480 --> 00:28:28.240]   fitted representation of word distribution, which is very useful.
[00:28:28.240 --> 00:28:32.520]   And some of them can even do where you get the word embeddings are contextual, right?
[00:28:32.520 --> 00:28:36.920]   So the information about the word and what it's likely to co-occur with isn't about that
[00:28:36.920 --> 00:28:39.640]   word across all the texts, but about that word in its current context.
[00:28:39.640 --> 00:28:43.920]   So super useful, but not the same thing as understanding language.
[00:28:43.920 --> 00:28:47.880]   And I kept getting into arguments with people who were not linguists who wanted to say,
[00:28:47.880 --> 00:28:49.560]   yeah, it is.
[00:28:49.560 --> 00:28:53.600]   So Alexander Kohler and I wrote this paper to just sort of say, okay, look, here's the
[00:28:53.600 --> 00:28:55.160]   argument, why not?
[00:28:55.160 --> 00:28:57.160]   With the hopes that that would put an end to it.
[00:28:57.160 --> 00:29:00.640]   And it didn't, like people still want to come argue with me about this.
[00:29:00.640 --> 00:29:05.400]   But the thing that is really hard to see and like sort of the value of linguistics in this
[00:29:05.400 --> 00:29:10.440]   place is that when we use language, we use it in, I'm sorry, I'm going to pull out a
[00:29:10.440 --> 00:29:14.800]   philosopher on you here, but Heidegger has this notion of thrownness.
[00:29:14.800 --> 00:29:18.920]   So you're in a state of thrownness when you are not aware of the tool you are using.
[00:29:18.920 --> 00:29:23.360]   And if you think about, you know, typing on a keyboard when it's going well, the keyboard
[00:29:23.360 --> 00:29:24.880]   disappears, right?
[00:29:24.880 --> 00:29:26.400]   And then you have a key that sticks.
[00:29:26.400 --> 00:29:29.480]   And then all of a sudden the keyboard is very, you know, there for you again.
[00:29:29.480 --> 00:29:30.920]   Well, language is the same way.
[00:29:30.920 --> 00:29:36.600]   When we are speaking a language that we are fluent in, it is not very visible to us until
[00:29:36.600 --> 00:29:37.960]   something makes us focus on it.
[00:29:37.960 --> 00:29:40.160]   And of course, linguistics is all about focusing on the language.
[00:29:40.160 --> 00:29:41.880]   So linguists are used to doing that.
[00:29:41.880 --> 00:29:49.020]   So when we talk about giving words to a language model, it's really important to distinguish
[00:29:49.020 --> 00:29:54.640]   between the word as a sequence of characters, as opposed to word in the sense of a pairing
[00:29:54.640 --> 00:29:59.520]   of form and meaning, because what the language model is seeing is only the sequence of characters.
[00:29:59.520 --> 00:30:02.720]   And it's a bit easier to imagine what that's like if you think about a language you don't
[00:30:02.720 --> 00:30:03.720]   speak.
[00:30:03.720 --> 00:30:04.720]   So what's a language you don't speak?
[00:30:04.720 --> 00:30:05.720]   Mandarin.
[00:30:05.720 --> 00:30:06.720]   Mandarin.
[00:30:06.720 --> 00:30:07.720]   Okay.
[00:30:07.720 --> 00:30:08.720]   You don't speak Mandarin.
[00:30:08.720 --> 00:30:09.720]   I assume you also therefore don't read Mandarin.
[00:30:09.720 --> 00:30:10.720]   Definitely don't.
[00:30:10.720 --> 00:30:11.720]   Well, okay.
[00:30:11.720 --> 00:30:12.720]   Yeah.
[00:30:12.720 --> 00:30:13.720]   Maybe recognize a couple of the characters.
[00:30:13.720 --> 00:30:16.120]   I mean, I read Japanese, so there's some overlap.
[00:30:16.120 --> 00:30:18.560]   Let's go a little bit further away.
[00:30:18.560 --> 00:30:19.880]   Do you read Cherokee?
[00:30:19.880 --> 00:30:21.240]   No, definitely not.
[00:30:21.240 --> 00:30:22.240]   Okay.
[00:30:22.240 --> 00:30:23.680]   So Cherokee has got this wonderful syllabary.
[00:30:23.680 --> 00:30:26.740]   It's a writing system where the characters represent syllables.
[00:30:26.740 --> 00:30:31.220]   If someone showed you a whole bunch of Cherokee text, that experience of looking at it would
[00:30:31.220 --> 00:30:37.220]   be a better model for what the computer is doing than you looking at English text, because
[00:30:37.220 --> 00:30:40.900]   you can't help but get the meaning part when you're looking at it, because English is a
[00:30:40.900 --> 00:30:42.380]   language you speak and read.
[00:30:42.380 --> 00:30:46.060]   And Mandarin is kind of in between there, because you would pick up a few of the hanzo
[00:30:46.060 --> 00:30:50.500]   that you recognize from Japanese kanji, and they can be quite the same.
[00:30:50.500 --> 00:30:53.220]   And I guess, I don't know.
[00:30:53.220 --> 00:30:57.940]   I don't want to argue with you, but I do want to sort of, I guess, advocate for...
[00:30:57.940 --> 00:30:58.940]   I don't know.
[00:30:58.940 --> 00:31:03.780]   I mean, so I haven't thought deeply about this topic.
[00:31:03.780 --> 00:31:11.540]   But I guess what I have seen in my life is these language models kind of working better
[00:31:11.540 --> 00:31:17.340]   and better than I could have imagined from the strategy that they employ, and sort of
[00:31:17.340 --> 00:31:21.460]   seeming like they're getting more and more subtle detail.
[00:31:21.460 --> 00:31:25.940]   And of course, when I was a kid, I learned about the Turing test, which seems like a
[00:31:25.940 --> 00:31:28.740]   pretty good test of understanding on its face, right?
[00:31:28.740 --> 00:31:30.780]   Which is, if sort of...
[00:31:30.780 --> 00:31:36.300]   I think the test is like if you have a conversation with something and you can't tell if it's
[00:31:36.300 --> 00:31:43.340]   a automated system or a human, then we can say that it has intelligence.
[00:31:43.340 --> 00:31:49.740]   And it sort of seems to me like these language models are on the verge of passing the Turing
[00:31:49.740 --> 00:31:51.900]   test.
[00:31:51.900 --> 00:31:58.880]   So I guess what would it take for you to feel like some automated technique actually has
[00:31:58.880 --> 00:32:02.860]   understanding of what it's consuming?
[00:32:02.860 --> 00:32:03.860]   Yeah.
[00:32:03.860 --> 00:32:08.020]   So I think the first thing I want to say about the Turing test is the reason it doesn't work,
[00:32:08.020 --> 00:32:12.460]   and I hate to disagree with a giant like Turing, because Turing's work was really important
[00:32:12.460 --> 00:32:13.460]   in foundation.
[00:32:13.460 --> 00:32:14.460]   But it was 100 years ago.
[00:32:14.460 --> 00:32:15.460]   It's possible that...
[00:32:15.460 --> 00:32:16.460]   70?
[00:32:16.460 --> 00:32:17.460]   I don't think it's 100 years ago.
[00:32:17.460 --> 00:32:18.460]   70.
[00:32:18.460 --> 00:32:19.460]   Fair, fair.
[00:32:19.460 --> 00:32:20.460]   70.
[00:32:20.460 --> 00:32:21.460]   Okay, 70.
[00:32:21.460 --> 00:32:22.460]   Sorry.
[00:32:22.460 --> 00:32:34.180]   As it turns out, people are too willing to make sense of language and too willing to
[00:32:34.180 --> 00:32:37.540]   build the context behind something that would make something make sense.
[00:32:37.540 --> 00:32:44.820]   And so we are not well positioned to actually be the testers in a Turing test.
[00:32:44.820 --> 00:32:46.740]   And so that's why that doesn't work.
[00:32:46.740 --> 00:32:52.260]   And so the language models, because they can come up with coherent seeming text, these
[00:32:52.260 --> 00:32:57.060]   are probable sequences given a little bit of noise in where you start, well, what would
[00:32:57.060 --> 00:33:01.900]   likely come next based on all that training data, then it comes out as something that
[00:33:01.900 --> 00:33:03.620]   we can make sense of.
[00:33:03.620 --> 00:33:08.540]   And then we are easily fooled into thinking that it actually meant to communicate that.
[00:33:08.540 --> 00:33:14.020]   So you're asking the question of what would show that a machine has understanding?
[00:33:14.020 --> 00:33:19.780]   And I think part of it is, well, let's talk about actually interfacing with the world
[00:33:19.780 --> 00:33:20.780]   in some way.
[00:33:20.780 --> 00:33:26.380]   And we certainly do have cases where machines in restricted domains for restricted ranges
[00:33:26.380 --> 00:33:28.500]   of things that they can do, do understand.
[00:33:28.500 --> 00:33:35.440]   So when you ask your local corporate spy bot to do something for you and it does the thing,
[00:33:35.440 --> 00:33:36.440]   it has understood.
[00:33:36.440 --> 00:33:37.440]   Wait, wait, wait.
[00:33:37.440 --> 00:33:38.440]   Sorry.
[00:33:38.440 --> 00:33:39.440]   What's a local corporate spy bot?
[00:33:39.440 --> 00:33:42.380]   Sorry, can you make something more concrete?
[00:33:42.380 --> 00:33:47.380]   I'm making a snarky remark about the privacy implications of things like Siri and Alexa
[00:33:47.380 --> 00:33:48.380]   and Google Home.
[00:33:48.380 --> 00:33:49.380]   Oh, I see.
[00:33:49.380 --> 00:33:50.380]   I see.
[00:33:50.380 --> 00:33:51.380]   Gotcha.
[00:33:51.380 --> 00:33:52.380]   Okay.
[00:33:52.380 --> 00:33:53.380]   Samsung Bixby's in the same space.
[00:33:53.380 --> 00:33:54.380]   Microsoft had Cortana.
[00:33:54.380 --> 00:33:55.380]   Right?
[00:33:55.380 --> 00:33:56.380]   Right, right, right.
[00:33:56.380 --> 00:33:57.380]   Gotcha.
[00:33:57.380 --> 00:33:58.380]   Yeah.
[00:33:58.380 --> 00:33:59.380]   Okay.
[00:33:59.380 --> 00:34:02.220]   So when you ask those things to set a timer or turn on the lights or dial a phone number
[00:34:02.220 --> 00:34:07.100]   or whatever, and it works, then yes, to a certain extent it has understood.
[00:34:07.100 --> 00:34:12.940]   And it has understood because its training setup was looking at not just language, but
[00:34:12.940 --> 00:34:17.140]   something external to language that needed to map to that.
[00:34:17.140 --> 00:34:19.500]   And so that's the kind of understanding.
[00:34:19.500 --> 00:34:21.620]   And the question is, how do you...
[00:34:21.620 --> 00:34:29.180]   So for somebody who was interested in doing that across some more general range of things,
[00:34:29.180 --> 00:34:30.180]   right?
[00:34:30.180 --> 00:34:35.580]   The question is, how do you set up tasks that require some kind of action in the world so
[00:34:35.580 --> 00:34:40.140]   that it can't be done just by bulldozing it with a language model saying, "Well, this
[00:34:40.140 --> 00:34:41.700]   is a likely thing to come next."
[00:34:41.700 --> 00:34:42.700]   Right?
[00:34:42.700 --> 00:34:47.300]   So you got to describe your octopus thought experiment because that was very evocative.
[00:34:47.300 --> 00:34:49.660]   And I have some questions.
[00:34:49.660 --> 00:34:50.660]   Okay.
[00:34:50.660 --> 00:34:58.020]   So the octopus thought experiment is about not just being able to understand, but learning
[00:34:58.020 --> 00:34:59.260]   to understand.
[00:34:59.260 --> 00:35:04.260]   And that's the difference between it and both the Turing test and Searle's thought experiment,
[00:35:04.260 --> 00:35:09.300]   where both of those basically say, imagine someone has set up the whole system, right?
[00:35:09.300 --> 00:35:12.900]   Then we could test it for intelligence or we can, as from a philosophical point of view,
[00:35:12.900 --> 00:35:13.900]   saying it's still not understanding.
[00:35:13.900 --> 00:35:17.620]   So those are just sort of, the system exists and we are thinking about it or testing it.
[00:35:17.620 --> 00:35:22.700]   And the octopus is this thing of saying, okay, if we had something that we assume, we posit
[00:35:22.700 --> 00:35:23.700]   that it is hyper-intelligent.
[00:35:23.700 --> 00:35:26.660]   And then that's part of why we picked the octopus.
[00:35:26.660 --> 00:35:30.960]   In fact, it was initially a dolphin, but we decided that octopuses are inherently more
[00:35:30.960 --> 00:35:37.380]   entertaining and also it was better because a dolphin's environment is a bit closer to
[00:35:37.380 --> 00:35:39.380]   a human's environment.
[00:35:39.380 --> 00:35:43.700]   So we wanted the octopus to be something that is posited to be super intelligent.
[00:35:43.700 --> 00:35:47.060]   And they are, I think, understood to be intelligent creatures that we said, like as smart as it
[00:35:47.060 --> 00:35:48.940]   needs to be, like, that's not the issue.
[00:35:48.940 --> 00:35:54.100]   So we are assuming intelligence, but then we are only giving it access to the form of
[00:35:54.100 --> 00:35:55.100]   language.
[00:35:55.100 --> 00:36:00.620]   So in our scenario, you have these two English speaking humans who end up stranded onto nearby
[00:36:00.620 --> 00:36:01.620]   islands.
[00:36:01.620 --> 00:36:06.340]   They're otherwise uninhabited, but they've had previous inhabitants who set up a telegraph,
[00:36:06.340 --> 00:36:07.780]   undersea telegraph cable.
[00:36:07.780 --> 00:36:11.360]   So these two humans can communicate with each other.
[00:36:11.360 --> 00:36:14.500]   We left it off stage how they discovered the telegraph or that the other ones on the other
[00:36:14.500 --> 00:36:16.500]   island, whatever, just that, assume it exists.
[00:36:16.500 --> 00:36:17.500]   It's a thought experiment.
[00:36:17.500 --> 00:36:18.500]   You can do things like that.
[00:36:18.500 --> 00:36:22.820]   You know, assume a spherical cow, except we don't need spherical cows.
[00:36:22.820 --> 00:36:28.300]   So telegraph cable and the humans are named A and B, and they're basically using English
[00:36:28.300 --> 00:36:31.340]   as encoded in Morse code to talk to each other.
[00:36:31.340 --> 00:36:36.140]   And this hyper-intelligent deep sea octopus that we called O comes along and taps into
[00:36:36.140 --> 00:36:37.780]   that cable.
[00:36:37.780 --> 00:36:42.200]   So the octopus can feel the pulses going through from Morse code.
[00:36:42.200 --> 00:36:47.580]   And the question is, what could the octopus actually potentially learn here?
[00:36:47.580 --> 00:36:51.700]   And because this is a hyper-intelligent octopus, it's got, you know, as much time as it wants,
[00:36:51.700 --> 00:36:53.300]   it's got as much memory as it wants.
[00:36:53.300 --> 00:36:58.420]   It is able to very closely model the patterns of, you know, what's likely to come next.
[00:36:58.420 --> 00:37:01.900]   So in our story, the octopus decides for some reason that it's lonely and it's going to
[00:37:01.900 --> 00:37:07.260]   cut the cable and pretend to be B while talking to A. And on reflection, it's like, poor B,
[00:37:07.260 --> 00:37:08.500]   just cut off from the world.
[00:37:08.500 --> 00:37:09.500]   Right?
[00:37:09.500 --> 00:37:13.060]   So maybe the octopus is also talking to B, pretending to be A, but we don't talk about
[00:37:13.060 --> 00:37:14.060]   that part.
[00:37:14.060 --> 00:37:21.980]   And so the question is, under what circumstances could the octopus continue to fool A that
[00:37:21.980 --> 00:37:23.460]   it's actually B?
[00:37:23.460 --> 00:37:27.640]   And we say this is in a sense, a weak version of the Turing test, because the way the Turing
[00:37:27.640 --> 00:37:32.460]   test was set up, A is given the task of deciding, am I talking to a human or not?
[00:37:32.460 --> 00:37:34.020]   And here there's subterfuge, right?
[00:37:34.020 --> 00:37:39.040]   The octopus is, its mere existence is unknown to A. Right?
[00:37:39.040 --> 00:37:43.540]   So, you know, if there's just sort of like chit-chat pleasantries, those things, you
[00:37:43.540 --> 00:37:48.420]   can just kind of follow a pattern and it's relatively inconsequential as long as what's
[00:37:48.420 --> 00:37:50.740]   coming out is internally coherent.
[00:37:50.740 --> 00:37:53.420]   And even if it's a little bit incoherent, well, maybe B is just being silly.
[00:37:53.420 --> 00:37:54.420]   Right?
[00:37:54.420 --> 00:37:55.420]   It doesn't matter so much.
[00:37:55.420 --> 00:37:58.540]   So we thought, okay, well, O could get away with that.
[00:37:58.540 --> 00:38:04.780]   But once you get more towards things where A actually really cares about communicating
[00:38:04.780 --> 00:38:09.420]   ideas to B and getting ideas back from B, it's going to get harder and harder for the
[00:38:09.420 --> 00:38:13.860]   octopus to maintain this semblance of good communication.
[00:38:13.860 --> 00:38:19.500]   So we go through this example where A builds a coconut catapult and the octopus is able
[00:38:19.500 --> 00:38:24.860]   to send back sort of like very cool invention, great job or something, even though A was
[00:38:24.860 --> 00:38:27.460]   asking for like, well, what happened when you built it?
[00:38:27.460 --> 00:38:31.380]   And but you know, the octopus has no experience of things like coconuts or rope or stuff like
[00:38:31.380 --> 00:38:32.380]   that.
[00:38:32.380 --> 00:38:38.220]   So it can't reason about those things in the world or even know that A is actually talking
[00:38:38.220 --> 00:38:39.220]   about them.
[00:38:39.220 --> 00:38:43.860]   All it can do is come back with, well, what's a likely form of a response in this context?
[00:38:43.860 --> 00:38:49.580]   And to the extent that O gets away with that, it's because A is willing to make sense of
[00:38:49.580 --> 00:38:50.580]   those utterances.
[00:38:50.580 --> 00:38:53.940]   O has no meaning in this scenario.
[00:38:53.940 --> 00:38:59.380]   And then finally we have a bear show up and start attacking A and A says to O or to B
[00:38:59.380 --> 00:39:02.420]   actually, help, I'm being attacked by a bear.
[00:39:02.420 --> 00:39:03.420]   All I have are these two sticks.
[00:39:03.420 --> 00:39:05.020]   What should I do?
[00:39:05.020 --> 00:39:07.260]   And you know, at that point O is utterly useless.
[00:39:07.260 --> 00:39:10.900]   And so we say, this is the point at which O would definitely fail the Turing test if
[00:39:10.900 --> 00:39:14.580]   A survived being eaten by the bear.
[00:39:14.580 --> 00:39:18.060]   But then we tried with GPT-2, like what would it say?
[00:39:18.060 --> 00:39:19.060]   The answers were hilarious.
[00:39:19.060 --> 00:39:24.620]   Like the words are in the right topic area enough that it comes back with something funny.
[00:39:24.620 --> 00:39:27.500]   And I encourage people to go look at the appendix to our paper where we put these, but it's
[00:39:27.500 --> 00:39:29.220]   never going to be helpful.
[00:39:29.220 --> 00:39:31.420]   And it's not actually expressing communicative intent.
[00:39:31.420 --> 00:39:36.060]   Well, I have to say like walking into that paper without knowing the context, I really
[00:39:36.060 --> 00:39:37.060]   enjoyed it.
[00:39:37.060 --> 00:39:44.100]   I think for me, I especially enjoyed it because the sort of concreteness of the thought experiment
[00:39:44.100 --> 00:39:49.420]   that was like evocative, but also, you know, kind of makes you like think like, huh, like
[00:39:49.420 --> 00:39:50.420]   what do I think about that?
[00:39:50.420 --> 00:39:55.100]   And I guess what I kept thinking was like, you know, for me, I feel like I've learned
[00:39:55.100 --> 00:39:57.780]   about a lot of things that I haven't like experienced.
[00:39:57.780 --> 00:40:01.540]   I mean, I was especially thinking about just kind of like learning math where there's kind
[00:40:01.540 --> 00:40:04.660]   of all these abstract topics.
[00:40:04.660 --> 00:40:10.580]   And I feel like in a way, I feel like I learned about math in some sense through form almost.
[00:40:10.580 --> 00:40:11.580]   It's all in my head, right?
[00:40:11.580 --> 00:40:13.580]   I'm kind of like learning things, like visualizing them.
[00:40:13.580 --> 00:40:19.500]   And I kind of wondered, like it seems possible to like learn to reason about things that
[00:40:19.500 --> 00:40:26.020]   you haven't seen or experienced just from like a stream of words, right?
[00:40:26.020 --> 00:40:31.620]   I even remember actually grading a blind student's papers that it was actually, it was really
[00:40:31.620 --> 00:40:36.020]   interesting, like, you know, how they, they walked through stuff in a math class and it
[00:40:36.020 --> 00:40:40.220]   seemed like they were visualizing things, even though, you know, they had never, you
[00:40:40.220 --> 00:40:41.700]   know, they were blind from birth.
[00:40:41.700 --> 00:40:46.940]   So I just wondering, like, I guess I'm not like totally convinced that the octopus couldn't
[00:40:46.940 --> 00:40:51.600]   somehow figure out what a catapult does if they kind of listen to all language.
[00:40:51.600 --> 00:40:57.200]   So if the octopus had actually had a chance to learn English, then yes, but it didn't
[00:40:57.200 --> 00:41:00.120]   because it never got that initial grounding.
[00:41:00.120 --> 00:41:04.720]   And we absolutely learn things through language that are outside of what we've directly experienced.
[00:41:04.720 --> 00:41:09.760]   You know, conversely, if you as a sighted person wanted to understand what it was like
[00:41:09.760 --> 00:41:14.040]   to live as a blind person, you could listen to or read what a blind person has to say
[00:41:14.040 --> 00:41:16.320]   about that and learn about it.
[00:41:16.320 --> 00:41:17.320]   Right.
[00:41:17.320 --> 00:41:21.400]   So that's definitely something that we can do, but we can do it because we have acquired
[00:41:21.400 --> 00:41:23.800]   linguists, acquired linguistic systems, right?
[00:41:23.800 --> 00:41:28.760]   So we, when we use language to communicate, we absolutely tell each other ideas and things
[00:41:28.760 --> 00:41:32.840]   that are outside of even our own experiences that we invent things and then transmit that
[00:41:32.840 --> 00:41:33.840]   to other people.
[00:41:33.840 --> 00:41:38.320]   But we do that based on this shared system that tells us, okay, here's the range of possible
[00:41:38.320 --> 00:41:39.320]   forms.
[00:41:39.320 --> 00:41:41.120]   These are the well-formed words and sentences.
[00:41:41.120 --> 00:41:43.080]   These are the sounds that we use in this language.
[00:41:43.080 --> 00:41:46.960]   These are the way the words are built up, the sentences are built up, and these are
[00:41:46.960 --> 00:41:49.560]   the standing meanings that they map to.
[00:41:49.560 --> 00:41:53.860]   And then we use those standing meanings to make guesses about communicative intent.
[00:41:53.860 --> 00:41:56.560]   And the problem for the octopus isn't that it's not smart.
[00:41:56.560 --> 00:41:58.700]   We said it's, you know, hyper-intelligent.
[00:41:58.700 --> 00:42:03.700]   It isn't that it couldn't, if it knew the language, understand those things.
[00:42:03.700 --> 00:42:07.840]   It's that its exposure to the language is not set up so that it can actually learn it
[00:42:07.840 --> 00:42:09.080]   as a linguistic system.
[00:42:09.080 --> 00:42:11.360]   All it can learn is distributional patterns.
[00:42:11.360 --> 00:42:16.960]   I guess what prevents the octopus from learning the language over time, like a human probably
[00:42:16.960 --> 00:42:17.960]   would.
[00:42:17.960 --> 00:42:18.960]   Okay.
[00:42:18.960 --> 00:42:23.600]   So it doesn't get to do, and in the paper we go into human language acquisition, for
[00:42:23.600 --> 00:42:26.520]   first language acquisition, it's all about joint attention, right?
[00:42:26.520 --> 00:42:32.640]   So when babies learn language, it starts from social connections to their caregivers and
[00:42:32.640 --> 00:42:36.220]   understanding that the caregivers are communicating something to them and then mapping the words
[00:42:36.220 --> 00:42:38.320]   onto those communicative intents.
[00:42:38.320 --> 00:42:43.180]   And the child language literature talks about the importance of joint attention, that kids
[00:42:43.180 --> 00:42:48.420]   learn words when their caregivers follow into their attention and attend to the same things
[00:42:48.420 --> 00:42:50.480]   and then provide those words.
[00:42:50.480 --> 00:42:53.420]   And so that experience, that mapping, the octopus doesn't get that.
[00:42:53.420 --> 00:42:56.000]   It's just getting the words going by.
[00:42:56.000 --> 00:43:05.200]   So do you think there's some algorithm possibly that could exist that could take a stream
[00:43:05.200 --> 00:43:10.620]   of words and understand them in that sense?
[00:43:10.620 --> 00:43:14.560]   So natural language understanding is a tremendously difficult problem because it relies not just
[00:43:14.560 --> 00:43:18.040]   on the linguistic system, but also on world knowledge and common sense reasoning, all
[00:43:18.040 --> 00:43:19.040]   those kinds of things.
[00:43:19.040 --> 00:43:23.440]   So you could certainly as way more certain than I actually am, but there's a big difference
[00:43:23.440 --> 00:43:29.760]   between saying, I'm going to build an algorithm that has understanding of linguistic structure,
[00:43:29.760 --> 00:43:33.240]   has understanding of linguistic meaning, has understanding of how those meanings map to
[00:43:33.240 --> 00:43:37.920]   a model of the world, and then use that to understand versus I'm going to build a system
[00:43:37.920 --> 00:43:41.960]   that only gets linguistic form and assume that it will get to understanding in some
[00:43:41.960 --> 00:43:42.960]   way.
[00:43:42.960 --> 00:43:47.480]   So yes, you could go much, much further with algorithms that have more in their input,
[00:43:47.480 --> 00:43:49.540]   in their training input than just form.
[00:43:49.540 --> 00:43:53.400]   So that's going to be things like visual grounding, it's going to be things like the ability to
[00:43:53.400 --> 00:43:56.240]   possibly query people for answers.
[00:43:56.240 --> 00:44:01.400]   It might be knowledge bases, it might be other sensors in some sort of embodied setup.
[00:44:01.400 --> 00:44:05.400]   So yeah, I'm not saying that natural language understanding is impossible and not something
[00:44:05.400 --> 00:44:06.400]   to work on.
[00:44:06.400 --> 00:44:08.880]   I'm saying that language modeling is not natural language understanding.
[00:44:08.880 --> 00:44:17.760]   But just so I'm clear, so just consuming language without kind of all this extra stuff, you're
[00:44:17.760 --> 00:44:25.840]   arguing that no algorithm could from just that really understand language.
[00:44:25.840 --> 00:44:27.760]   And by language, I mean form.
[00:44:27.760 --> 00:44:34.160]   So imagine that you are dropped into the Thai equivalent of the Library of Congress, and
[00:44:34.160 --> 00:44:38.960]   you have around you any book you could possibly want in Thai, but only in Thai.
[00:44:38.960 --> 00:44:42.560]   For some reason, this library doesn't have Thai, Chinese, Thai, French, Thai, English
[00:44:42.560 --> 00:44:46.360]   dictionaries, it's just Thai.
[00:44:46.360 --> 00:44:47.360]   Could you learn Thai?
[00:44:47.360 --> 00:44:48.360]   I think so.
[00:44:48.360 --> 00:44:52.680]   I mean, I guess what's hard is that I have a language already.
[00:44:52.680 --> 00:44:54.260]   But I feel like I...
[00:44:54.260 --> 00:44:55.260]   So what would you do?
[00:44:55.260 --> 00:44:58.320]   What would be your first step to learning Thai if you have just oodles and oodles of
[00:44:58.320 --> 00:45:00.120]   Thai books and that's it around you?
[00:45:00.120 --> 00:45:02.480]   What would I start to do?
[00:45:02.480 --> 00:45:03.480]   I mean, I would...
[00:45:03.480 --> 00:45:04.480]   I'm not sure.
[00:45:04.480 --> 00:45:07.720]   Do you think I couldn't learn Thai?
[00:45:07.720 --> 00:45:09.440]   So I'm curious about what you...
[00:45:09.440 --> 00:45:10.760]   So you as a person, could you learn Thai?
[00:45:10.760 --> 00:45:12.600]   Sure, you could go take a Thai language class.
[00:45:12.600 --> 00:45:16.360]   No, no, I mean, in this situation, just sort of dropped in with...
[00:45:16.360 --> 00:45:19.240]   I mean, people do learn...
[00:45:19.240 --> 00:45:23.440]   How do people learn hieroglyphics or something where there's no one around that still knows
[00:45:23.440 --> 00:45:24.440]   it?
[00:45:24.440 --> 00:45:25.440]   Do they need to find a Rosetta Stone or can they...
[00:45:25.440 --> 00:45:28.120]   So the Rosetta Stone is what unlocked the hieroglyphics.
[00:45:28.120 --> 00:45:33.520]   If you don't have something like that, then what you have to do is resort to hypotheses
[00:45:33.520 --> 00:45:39.480]   about distributions and say, "What do we know about the world in which these texts were
[00:45:39.480 --> 00:45:40.720]   written?
[00:45:40.720 --> 00:45:42.880]   What do we know about how languages work?
[00:45:42.880 --> 00:45:48.640]   And can we say, "Okay, well, given frequency analyses and the length of the words, this
[00:45:48.640 --> 00:45:52.440]   seems like a language that's got separate function words instead of lots of morphology.
[00:45:52.440 --> 00:45:55.640]   So that thing might be an article, that thing might be a copula verb."
[00:45:55.640 --> 00:45:57.920]   And you could do some analysis like that.
[00:45:57.920 --> 00:46:00.480]   It's not what language models are doing.
[00:46:00.480 --> 00:46:06.200]   And then to get from those sort of structural things into something about meaning, you have
[00:46:06.200 --> 00:46:07.800]   to make guesses about what's being described.
[00:46:07.800 --> 00:46:12.800]   You have to basically bring in some world knowledge and say, "How well does this fit?"
[00:46:12.800 --> 00:46:16.600]   So when I asked you that question about what would you do, I was thinking, well, possible
[00:46:16.600 --> 00:46:21.800]   answers are I would go find an illustrated encyclopedia that has pictures in it, but
[00:46:21.800 --> 00:46:23.280]   there's some visual grounding.
[00:46:23.280 --> 00:46:30.040]   Or I would go find a book from whose cover I could tell it was actually the Thai translation
[00:46:30.040 --> 00:46:32.880]   of Curious George.
[00:46:32.880 --> 00:46:33.880]   And then-
[00:46:33.880 --> 00:46:34.880]   These are great suggestions.
[00:46:34.960 --> 00:46:37.760]   But all of that is bringing in sort of extra-
[00:46:37.760 --> 00:46:38.760]   No, seriously.
[00:46:38.760 --> 00:46:39.760]   Yeah.
[00:46:39.760 --> 00:46:42.600]   And then once you have a foothold, you can build on it.
[00:46:42.600 --> 00:46:44.440]   And that's an interesting way to go.
[00:46:44.440 --> 00:46:48.440]   But if you just have form, it's not going to give you that information.
[00:46:48.440 --> 00:46:49.440]   Well, interesting.
[00:46:49.440 --> 00:46:50.440]   Thank you.
[00:46:50.440 --> 00:46:54.000]   This is really interesting.
[00:46:54.000 --> 00:46:57.160]   I guess my last question on this topic is, do you sort of predict that these language
[00:46:57.160 --> 00:47:03.300]   models will run into problems that we'll really experience and then we'll have to kind of
[00:47:03.300 --> 00:47:04.600]   change the approach?
[00:47:04.600 --> 00:47:11.600]   Or do you think that as our bar for applications of natural language goes up, they'll just
[00:47:11.600 --> 00:47:16.040]   sort of adapt and sort of find ways to incorporate external information, kind of like finding
[00:47:16.040 --> 00:47:20.640]   the Curious George translation?
[00:47:20.640 --> 00:47:23.880]   So I think that language models are going to remain useful.
[00:47:23.880 --> 00:47:28.520]   I mean, language models have been an important component of language technology since Shannon's
[00:47:28.520 --> 00:47:30.400]   work in the 1950s.
[00:47:30.400 --> 00:47:32.080]   This is longstanding.
[00:47:32.080 --> 00:47:35.840]   But I think that we are likely- it's so hard to predict the future.
[00:47:35.840 --> 00:47:43.280]   But my guess is that, or maybe what I would like to see, is that we get to a more stringent
[00:47:43.280 --> 00:47:48.320]   sense of what works and what's sort of an appropriate range of failure modes and what
[00:47:48.320 --> 00:47:50.180]   kind of fail-safes we need.
[00:47:50.180 --> 00:47:54.480]   And people are going to find that putting language models at the center of something
[00:47:54.480 --> 00:47:59.400]   where your application really requires you to have a commitment to accountability for
[00:47:59.400 --> 00:48:04.640]   the words that are uttered is going to be a very fragile way to go.
[00:48:04.640 --> 00:48:09.760]   And so my guess is that when we get to that point, we're going to decenter the language
[00:48:09.760 --> 00:48:14.440]   models and have them be something that is selecting among possible outputs again, or
[00:48:14.440 --> 00:48:20.480]   providing these word embeddings, but they are not a step towards general purpose language
[00:48:20.480 --> 00:48:23.040]   understanding the way they're hyped to be.
[00:48:23.040 --> 00:48:24.960]   Is that sort of one set of problems, right?
[00:48:24.960 --> 00:48:28.360]   If you have to have accountability for the words that are uttered, you do not want a
[00:48:28.360 --> 00:48:30.480]   stochastic parrot.
[00:48:30.480 --> 00:48:34.160]   You want something that will speak for you in a reliable way, not just make up what sounds
[00:48:34.160 --> 00:48:35.160]   good.
[00:48:35.160 --> 00:48:41.280]   And then the other thing is, if we take seriously these issues around bias and encoding and
[00:48:41.280 --> 00:48:46.400]   amplifying bias and training data, I think we're going to find that we want to work with
[00:48:46.400 --> 00:48:51.680]   algorithms that can make more of smaller data sets so that we can be better about curating
[00:48:51.680 --> 00:48:55.400]   and documenting and updating those data sets so that they stay current with what's going
[00:48:55.400 --> 00:49:00.040]   on rather than this path right now that relies on very large language models.
[00:49:00.040 --> 00:49:01.640]   So those are my guesses.
[00:49:01.640 --> 00:49:04.240]   There's also the environmental angle.
[00:49:04.240 --> 00:49:08.960]   Actually the energy uses angle is both environmental, but also about technology to a certain extent.
[00:49:08.960 --> 00:49:13.520]   So I think there are more and more people and there's Schwartz et al, Struble et al,
[00:49:13.520 --> 00:49:17.420]   Henderson et al, a bunch of work now sort of saying, "Hey, let's make sure we're also
[00:49:17.420 --> 00:49:21.200]   measuring the environmental impact as we do things or the carbon footprint so that we
[00:49:21.200 --> 00:49:25.040]   can direct effort to doing things in a more and more efficient way."
[00:49:25.040 --> 00:49:29.000]   So there's that angle, but there's also many situations where you don't have the whole
[00:49:29.000 --> 00:49:31.280]   cloud available.
[00:49:31.280 --> 00:49:35.560]   If you want to do computing on a mobile device, you're not going to be able to have an absolutely
[00:49:35.560 --> 00:49:37.300]   enormous language model in there.
[00:49:37.300 --> 00:49:41.720]   And so there's pressure to find leaner solutions.
[00:49:41.720 --> 00:49:46.920]   I think that that's a win-win sort of environmentally and then in terms of more flexibility with
[00:49:46.920 --> 00:49:47.920]   technology.
[00:49:47.920 --> 00:49:48.920]   Totally.
[00:49:48.920 --> 00:49:49.920]   Totally.
[00:49:49.920 --> 00:49:51.400]   And it's a good segue because you pointed out a bunch of this stuff in your paper about
[00:49:51.400 --> 00:49:54.720]   benchmarks, which I'd love to talk about a little bit.
[00:49:54.720 --> 00:49:59.120]   And maybe you could kind of summarize, I guess maybe start with what are benchmarks, probably
[00:49:59.120 --> 00:50:03.920]   most people know, but then kind of what are the possible pitfalls with them?
[00:50:03.920 --> 00:50:04.920]   Yeah.
[00:50:04.920 --> 00:50:08.560]   So I should say this is a paper called AI and the Everything in the Whole Wide World
[00:50:08.560 --> 00:50:12.640]   Benchmark that we presented at a workshop called Machine Learning Retrospectives at
[00:50:12.640 --> 00:50:14.960]   NIRPS last year.
[00:50:14.960 --> 00:50:20.600]   And it's joint work with Deb Raji and Alex Hanna and Emily Denton and Amanda Limpalata.
[00:50:20.600 --> 00:50:25.560]   And another collaboration where in this case, we actually do have meetings where we talk
[00:50:25.560 --> 00:50:30.560]   to each other, but of those people, the only one I've met in person so far is Amanda Lynn,
[00:50:30.560 --> 00:50:32.120]   who's a PhD student in my department.
[00:50:32.120 --> 00:50:35.400]   So pandemic life, right?
[00:50:35.400 --> 00:50:42.240]   But we got together because we were talking about the ways in which benchmarks are being
[00:50:42.240 --> 00:50:47.220]   sort of misused in the AI hype machine and in AI research that is sort of striving for
[00:50:47.220 --> 00:50:50.760]   generality and over claiming what the benchmark shows.
[00:50:50.760 --> 00:50:56.600]   So a benchmark is basically a standardized data set, typically with some gold standard
[00:50:56.600 --> 00:51:00.800]   labels, although you could also have benchmarks for things where the labels are inherent,
[00:51:00.800 --> 00:51:03.720]   like language modeling, right?
[00:51:03.720 --> 00:51:06.560]   What word actually came next is the gold standard label.
[00:51:06.560 --> 00:51:10.560]   And the idea is that you might have a standardized set of training data or possibly not, and
[00:51:10.560 --> 00:51:15.560]   then you've got the standardized test data and people can test different systems against
[00:51:15.560 --> 00:51:16.560]   this.
[00:51:16.560 --> 00:51:21.800]   So you're sort of saying, which system is more effective in this training regime or
[00:51:21.800 --> 00:51:25.080]   given this training data against that test data.
[00:51:25.080 --> 00:51:26.720]   So that's a benchmark.
[00:51:26.720 --> 00:51:31.000]   And let's say this, you asked me before if I could summarize the problems with benchmarks,
[00:51:31.000 --> 00:51:35.440]   and it's not so much benchmarks that have a problem, but the way that they're used.
[00:51:35.440 --> 00:51:39.480]   And I think this is an example of the map is not the territory.
[00:51:39.480 --> 00:51:42.660]   So people will tend to say, here's this benchmark about computer vision.
[00:51:42.660 --> 00:51:44.320]   So ImageNet is that, right?
[00:51:44.320 --> 00:51:49.520]   Or here's a benchmark about natural language understanding of English, and that's Glue
[00:51:49.520 --> 00:51:50.840]   and Super Glue.
[00:51:50.840 --> 00:51:55.700]   And people will say, I mean, I've actually seen this in like a PR thing that came out
[00:51:55.700 --> 00:52:02.000]   of Microsoft saying that computers understand English better than people now, right?
[00:52:02.000 --> 00:52:08.280]   Because this one setup scored higher than some humans on the Glue benchmark.
[00:52:08.280 --> 00:52:11.100]   And that's just a wild overclaim.
[00:52:11.100 --> 00:52:14.860]   And it's a misuse of what the benchmark is for.
[00:52:14.860 --> 00:52:16.920]   So what's the problem with the overclaims?
[00:52:16.920 --> 00:52:19.860]   Well, it kind of messes up the science, right?
[00:52:19.860 --> 00:52:25.280]   We're not doing science if we're not actually matching our conclusions to our experiments.
[00:52:25.280 --> 00:52:32.580]   And we live in a world of AI hype, which means that people are more likely to buy into and
[00:52:32.580 --> 00:52:37.860]   set up solutions that don't function as advertised because they live in a world where people
[00:52:37.860 --> 00:52:40.860]   are being told that Microsoft has built a system that understands English better than
[00:52:40.860 --> 00:52:41.900]   humans do.
[00:52:41.900 --> 00:52:47.860]   So of course you could also build an AI system that does whatever other implausible thing,
[00:52:47.860 --> 00:52:52.860]   like guesses someone's political affiliation by the way they smile or something, which
[00:52:52.860 --> 00:52:54.380]   makes no sense.
[00:52:54.380 --> 00:52:59.360]   But we live in a world where there's all these claims, overclaims about AI, and that makes
[00:52:59.360 --> 00:53:02.020]   these other ones also sound more plausible than they should.
[00:53:02.020 --> 00:53:05.180]   So those are the problems that I see.
[00:53:05.180 --> 00:53:07.780]   But benchmarking is important.
[00:53:07.780 --> 00:53:13.140]   There was, so in the history of computational linguistics, there was a while where when
[00:53:13.140 --> 00:53:17.820]   you wrote a paper for the ACL, the Association for Computational Linguistics, you would say,
[00:53:17.820 --> 00:53:23.460]   here's my system, here's how I built it, here's some sample inputs and outputs, done, right?
[00:53:23.460 --> 00:53:30.580]   And then the statistical machine learning sort of wave came through and brought with
[00:53:30.580 --> 00:53:35.020]   it the methodology of shared task evaluation challenges, which is sort of a historical
[00:53:35.020 --> 00:53:39.580]   version of benchmarking where NIST and other organizations would say, okay, we want to
[00:53:39.580 --> 00:53:43.820]   work on speech recognition and we want to actually get a sense of how these different
[00:53:43.820 --> 00:53:45.220]   systems compare to each other.
[00:53:45.220 --> 00:53:48.340]   So we're going to run a shared task evaluation challenge where everyone gets the same training
[00:53:48.340 --> 00:53:53.500]   data and we're going to have some held out test data that no one gets to see.
[00:53:53.500 --> 00:53:58.220]   And at a certain point, all the competitors submit their systems and we see what happened.
[00:53:58.220 --> 00:54:02.420]   And that's an improvement in the science compared to what was going on before.
[00:54:02.420 --> 00:54:06.140]   But that is not the whole story, right?
[00:54:06.140 --> 00:54:09.300]   If you want to understand how well a system is working, if you want to understand how
[00:54:09.300 --> 00:54:13.540]   to build the next system, you can't just test it on some standard thing.
[00:54:13.540 --> 00:54:17.820]   You also have to look at, well, what kinds of errors does it make?
[00:54:17.820 --> 00:54:22.580]   And how do the different systems compare, not just in their overall number, but in their
[00:54:22.580 --> 00:54:28.140]   failure modes and which inputs work for them and which ones don't and on and on like that,
[00:54:28.140 --> 00:54:32.020]   as opposed to, okay, I got the highest score, I'm done.
[00:54:32.020 --> 00:54:33.020]   Right, right.
[00:54:33.020 --> 00:54:34.020]   Well said.
[00:54:34.020 --> 00:54:38.860]   I guess I don't have much to add there.
[00:54:38.860 --> 00:54:42.540]   And I guess, could you say a little more about, I feel like this is a great paper in that
[00:54:42.540 --> 00:54:47.180]   you make these really concrete, sensible recommendations.
[00:54:47.180 --> 00:54:49.820]   You sort of suggest a few alternatives to benchmarks.
[00:54:49.820 --> 00:54:52.580]   Could you maybe run through those for anyone listening?
[00:54:52.580 --> 00:54:53.580]   Yeah, absolutely.
[00:54:53.580 --> 00:54:58.020]   So it's more compliments than alternatives to benchmarks, right?
[00:54:58.020 --> 00:55:02.660]   So in addition to benchmarks, this can be used sort of as a sanity check, right?
[00:55:02.660 --> 00:55:05.940]   Okay, did my system actually do better than a super naive baseline?
[00:55:05.940 --> 00:55:10.580]   Or I want to compare some systems head to head, let's use this benchmark.
[00:55:10.580 --> 00:55:14.780]   You might also use test suites, which are put together to sort of map out particular
[00:55:14.780 --> 00:55:20.420]   kinds of cases that you want to handle well, as opposed to just grabbing whatever happened
[00:55:20.420 --> 00:55:24.020]   to occur in your sample test data.
[00:55:24.020 --> 00:55:28.500]   You might do auditing, which is very much akin to test suites and sort of saying, so
[00:55:28.500 --> 00:55:34.800]   this is like Joy Blumwiney and Timnit Gebru and Deb Raji's work on auditing face recognition
[00:55:34.800 --> 00:55:40.420]   data sets where they sort of systematically created the set looking at two genders and
[00:55:40.420 --> 00:55:45.440]   a range of skin colors and sort of saying, okay, is its accuracy actually even across
[00:55:45.440 --> 00:55:46.820]   this set of people or no?
[00:55:46.820 --> 00:55:48.220]   And they found out no, right?
[00:55:48.220 --> 00:55:49.220]   So that's a...
[00:55:49.220 --> 00:55:51.500]   And how is that different than a benchmark?
[00:55:51.500 --> 00:55:53.680]   That kind of sounds like a benchmark, doesn't it?
[00:55:53.680 --> 00:55:58.580]   So it's not the way benchmarks are typically created, right?
[00:55:58.580 --> 00:56:02.140]   You could imagine someone creating a benchmark that is sort of systematically mapping out
[00:56:02.140 --> 00:56:03.900]   a space, but that's not the practice.
[00:56:03.900 --> 00:56:09.700]   The practice is we are going to go grab some data from somewhere and then hold out 10%
[00:56:09.700 --> 00:56:12.100]   of it to be the test.
[00:56:12.100 --> 00:56:16.380]   And the other 90% is training or 80% training, 10% dev, right?
[00:56:16.380 --> 00:56:22.940]   And the way benchmarks are typically put together is let's just grab a sample of data and see
[00:56:22.940 --> 00:56:28.300]   how well this thing works as opposed to let's create a testing regime through test suites
[00:56:28.300 --> 00:56:33.380]   or through this auditing process that can allow us to sort of find the contours of its
[00:56:33.380 --> 00:56:34.380]   failure mode.
[00:56:34.380 --> 00:56:39.580]   So not how well does it work on average, but okay, but how well does it work for this case
[00:56:39.580 --> 00:56:41.660]   and that case and that case?
[00:56:41.660 --> 00:56:47.420]   There's also adversarial testing, which is a few different things fall under adversarial
[00:56:47.420 --> 00:56:48.420]   testing.
[00:56:48.420 --> 00:56:52.060]   So sometimes people will create test sets by going and collecting all the examples that
[00:56:52.060 --> 00:56:59.300]   previous systems did poorly on to make a particularly hard test set, which is interesting in the
[00:56:59.300 --> 00:57:06.300]   sense that it can filter out the sort of freebies that are too easy, but also doesn't necessarily
[00:57:06.300 --> 00:57:12.060]   guide anything towards better performance for a particular use case, because it's just
[00:57:12.060 --> 00:57:15.940]   sort of like, well, we're selecting for what was hard for the previous model, not what's
[00:57:15.940 --> 00:57:21.300]   particularly important to get right or what's particularly likely to be frequent in our
[00:57:21.300 --> 00:57:22.300]   use case and so on.
[00:57:22.300 --> 00:57:24.580]   So that's one kind of adversarial testing.
[00:57:24.580 --> 00:57:27.500]   And then another one is what we did in the build it, break it shared tasks.
[00:57:27.500 --> 00:57:34.180]   So this was Alison Ettinger and Sudha Rao and Haldane and I in 2017 put together a shared
[00:57:34.180 --> 00:57:39.180]   task where we had system builders and then breaker teams.
[00:57:39.180 --> 00:57:42.020]   And the breaker team's goal was to find minimal pairs.
[00:57:42.020 --> 00:57:48.940]   So two examples that were minimally different to each other, but would work for which the
[00:57:48.940 --> 00:57:51.200]   systems would work for one, but not the other.
[00:57:51.200 --> 00:57:57.100]   And that would be a way of sort of mapping out what causes system failure.
[00:57:57.100 --> 00:58:00.620]   So you can look at that, you can look at error analysis.
[00:58:00.620 --> 00:58:03.760]   So take the test set from the benchmark or the dev set from the benchmark, and then go
[00:58:03.760 --> 00:58:07.700]   in and look and say, okay, what are the kinds of problems that are showing up here?
[00:58:07.700 --> 00:58:14.500]   A lot of systems that rely on language models tend to do really poorly with negation, which
[00:58:14.500 --> 00:58:18.760]   is one of these things that's very important to the meaning, but tends to be a short word
[00:58:18.760 --> 00:58:20.260]   or a sub word.
[00:58:20.260 --> 00:58:22.560]   And so it is easy to miss.
[00:58:22.560 --> 00:58:28.720]   So you can imagine, you know, speech recognition or machine translation, if you missed one
[00:58:28.720 --> 00:58:32.220]   word out of 20, it matters a lot what that word is, right?
[00:58:32.220 --> 00:58:37.180]   If you replace a with the, in many cases, that's not going to cause a lot of problems.
[00:58:37.180 --> 00:58:39.780]   But if you just skipped a not somewhere.
[00:58:39.780 --> 00:58:42.220]   Yeah, that makes sense.
[00:58:42.220 --> 00:58:43.220]   Yeah.
[00:58:43.220 --> 00:58:48.260]   So, you know, all of this is basically about looking at what it is we're trying to build,
[00:58:48.260 --> 00:58:55.340]   what it is we're testing on, how it fits into the motivating use cases, and then what works
[00:58:55.340 --> 00:58:59.140]   and what doesn't, and for what doesn't work, what are the implications?
[00:58:59.140 --> 00:59:02.260]   Like what happens in the real world if that failure happens?
[00:59:02.260 --> 00:59:04.280]   And also what are the likely causes?
[00:59:04.280 --> 00:59:07.000]   So what is tripping this up?
[00:59:07.000 --> 00:59:12.060]   And so all of that is what we would like to see instead of the leaderboardism, which is
[00:59:12.060 --> 00:59:15.820]   everyone just trying to climb to the top of the pile on the benchmark, which doesn't feel
[00:59:15.820 --> 00:59:21.620]   like it's really, I mean, people talking about the speed of progress in AI love to talk about
[00:59:21.620 --> 00:59:27.020]   how quickly that those leaderboard changes and how quickly the state of the art soda
[00:59:27.020 --> 00:59:29.760]   gets higher and higher on these various benchmarks.
[00:59:29.760 --> 00:59:35.220]   And I always think, yeah, but so like, what does that actually mean in terms of understanding
[00:59:35.220 --> 00:59:39.620]   the world better from a scientific point of view, or, you know, building technology that
[00:59:39.620 --> 00:59:44.180]   works better, not just in the average case, but also in the worst case.
[00:59:44.180 --> 00:59:50.060]   And so, yeah, it's interesting, you know, well, I had a couple, a couple of things came
[00:59:50.060 --> 00:59:51.780]   up for me reading that paper.
[00:59:51.780 --> 00:59:57.140]   I mean, I think I, when I started my career, I think it was just sort of on the tail end
[00:59:57.140 --> 01:00:03.020]   of ACL papers where they would just, it seemed like they would just cherry pick some examples,
[01:00:03.020 --> 01:00:04.500]   you know, where it worked and where it didn't.
[01:00:04.500 --> 01:00:05.500]   And it just seemed like ridiculous.
[01:00:05.500 --> 01:00:09.980]   Like I remember they had early benchmarks and people would have like lower accuracy
[01:00:09.980 --> 01:00:13.180]   than just sort of guessing the most common case or something, which, you know, you could
[01:00:13.180 --> 01:00:17.060]   argue that's better and people did, but that just seemed, seemed a little ridiculous to
[01:00:17.060 --> 01:00:18.060]   me.
[01:00:18.060 --> 01:00:21.020]   And I kept thinking, I remember there's the anecdote from your class about, I think it
[01:00:21.020 --> 01:00:26.980]   was Noam Chomsky, like saying that, oh, kids, you know, moms don't teach kids language,
[01:00:26.980 --> 01:00:29.020]   but they just actually, they do.
[01:00:29.020 --> 01:00:33.740]   And it's just like, no one bothered to check, you know, so it's kind of maddening.
[01:00:33.740 --> 01:00:39.220]   And I think I appreciated benchmarks from that, but then your recommendations are like
[01:00:39.220 --> 01:00:45.540]   not only reasonable, I think in companies, a lot of it is like, is like standard best
[01:00:45.540 --> 01:00:46.540]   practice.
[01:00:46.540 --> 01:00:52.340]   Like, I don't think you would just, you know, release like a new model without, you know,
[01:00:52.340 --> 01:00:55.700]   kind of trying it and getting a flavor for like where it works and where it doesn't,
[01:00:55.700 --> 01:00:58.540]   you know, you wouldn't just be like, oh, we, you know, we took 10% of the data, held it
[01:00:58.540 --> 01:01:04.260]   out, ship it, you know, but it does seem like, it does seem like that's actually one case
[01:01:04.260 --> 01:01:09.020]   where you see it more in companies than in sort of academic literature, probably because
[01:01:09.020 --> 01:01:12.580]   it's just easier to look at one number and be like, hey, we, we beat it.
[01:01:12.580 --> 01:01:13.580]   But clearly that's, that's flawed.
[01:01:13.580 --> 01:01:17.740]   So anyway, I thought that was a great, a great paper with really good suggestions that I
[01:01:17.740 --> 01:01:20.140]   think everyone should, should definitely follow.
[01:01:20.140 --> 01:01:25.220]   But I mean, I guess I also want to make sure we got to the last paper that we talked about,
[01:01:25.220 --> 01:01:27.820]   which is cool because I just want to make sure people know what is, what is the vendor
[01:01:27.820 --> 01:01:30.180]   rule and why is it important?
[01:01:30.180 --> 01:01:33.100]   So the vendor rule or the hashtag vendor rule.
[01:01:33.100 --> 01:01:36.780]   Yeah, it also isn't hashtag vendor rule as it seems like.
[01:01:36.780 --> 01:01:37.780]   Yeah, it's both.
[01:01:37.780 --> 01:01:42.500]   Say what it is first, and then I have some questions about best practice.
[01:01:42.500 --> 01:01:43.500]   Yeah.
[01:01:43.500 --> 01:01:47.420]   So, so, so it is itself a best practice, which says that you should always state the name
[01:01:47.420 --> 01:01:51.140]   of the language you're working on, even if it's just English.
[01:01:51.140 --> 01:01:54.060]   And this came about, this is a, this is a soapbox that I've been carrying around and
[01:01:54.060 --> 01:02:01.140]   periodically climbing up on since about 2009, where I saw a lot of that sort of pre neural
[01:02:01.140 --> 01:02:07.140]   statistical NLP work saying basically look ma, no linguistics and claiming that systems
[01:02:07.140 --> 01:02:11.420]   were language independent because there was no linguistic knowledge hard coded.
[01:02:11.420 --> 01:02:16.580]   And these supposedly language independent systems were mostly tested on English.
[01:02:16.580 --> 01:02:19.940]   And you also see a lot of work where people will publish, you know, a paper on machine
[01:02:19.940 --> 01:02:21.500]   reading or paper on sentiment analysis.
[01:02:21.500 --> 01:02:24.900]   And in fact, no, it's a paper on machine reading of English and sentiment analysis
[01:02:24.900 --> 01:02:26.500]   on English text.
[01:02:26.500 --> 01:02:33.180]   And flip side is if someone's working on Cherokee or Thai or Chinese or Italian, then that work
[01:02:33.180 --> 01:02:38.620]   gets, it's harder to get it accepted to the research conferences because it is deemed
[01:02:38.620 --> 01:02:42.740]   language specific where work on English is somehow general.
[01:02:42.740 --> 01:02:47.140]   And that's, that's a big problem for the science, big problem for getting to technology that
[01:02:47.140 --> 01:02:49.020]   actually works across languages.
[01:02:49.020 --> 01:02:53.760]   And so I've been sort of going around pestering people to actually test cross linguistically
[01:02:53.760 --> 01:02:56.580]   and to name the language they're working on.
[01:02:56.580 --> 01:03:03.700]   And in 2019, like three or four people, and this is, this is in that piece on the, on
[01:03:03.700 --> 01:03:08.980]   the gradient, I had their names listed, came sort of referred to this practice as the vendor
[01:03:08.980 --> 01:03:09.980]   rule.
[01:03:09.980 --> 01:03:12.140]   So I didn't name it, but once it was named, I ran with it.
[01:03:12.140 --> 01:03:16.620]   And part of it is, it's kind of a face threatening question to ask, right?
[01:03:16.620 --> 01:03:19.860]   If someone's written something about, you know, machine reading and I walk up and I
[01:03:19.860 --> 01:03:21.980]   say, well, what language, right?
[01:03:21.980 --> 01:03:24.180]   It's a stupid question to ask because it's obviously English.
[01:03:24.180 --> 01:03:26.340]   So it's face threatening to me.
[01:03:26.340 --> 01:03:28.580]   And it's also a little bit rude to them, right?
[01:03:28.580 --> 01:03:34.500]   To ask this question that says you should have said, and so I don't mind people blaming
[01:03:34.500 --> 01:03:35.500]   that on me.
[01:03:35.500 --> 01:03:40.060]   It's like, so the part of the reason I ran with the hashtag is if someone wants to go
[01:03:40.060 --> 01:03:43.940]   ask this question and they feel like it's sort of a silly question to ask, they can,
[01:03:43.940 --> 01:03:47.540]   they can pin it on me and I'm happy to lend my name to that.
[01:03:47.540 --> 01:03:48.540]   I see.
[01:03:48.540 --> 01:03:49.540]   Nice.
[01:03:49.540 --> 01:03:54.980]   And I guess this is a hard question, but I just, it's, you know, this is what comes to
[01:03:54.980 --> 01:03:55.980]   mind for me.
[01:03:55.980 --> 01:03:59.860]   It's like, wow, you know, English is so specific and probably has all these kinds of idiosyncrasies.
[01:03:59.860 --> 01:04:05.540]   Like, what do you, how do you think it'll be, might be different if it started in like
[01:04:05.540 --> 01:04:09.340]   Thai or Cherokee or something, or English just happened to, I mean, English must be
[01:04:09.340 --> 01:04:11.380]   unusual in all these ways, right?
[01:04:11.380 --> 01:04:15.860]   Like, like are there characteristics of English that are unusual and the world could have
[01:04:15.860 --> 01:04:16.860]   gone a different way?
[01:04:16.860 --> 01:04:17.860]   Yeah, absolutely.
[01:04:17.860 --> 01:04:20.780]   So, and actually in that paper, I list out a bunch of them.
[01:04:20.780 --> 01:04:23.540]   So one thing is English is a spoken language, not a signed language.
[01:04:23.540 --> 01:04:28.460]   So if we had, if we had started NLP with American sign language or another sign language, it
[01:04:28.460 --> 01:04:31.500]   would have been very different, I think.
[01:04:31.500 --> 01:04:32.500]   Right.
[01:04:32.500 --> 01:04:33.500]   Clearly.
[01:04:33.500 --> 01:04:34.500]   Yeah.
[01:04:34.500 --> 01:04:37.180]   So, you know, that's, that's, that's one big choice point.
[01:04:37.180 --> 01:04:42.540]   Another thing is that English has a very well-established and standardized writing system and many of
[01:04:42.540 --> 01:04:44.980]   the world's languages don't have a writing system at all.
[01:04:44.980 --> 01:04:49.420]   And many of them that do, don't have the degree of standardization that English does.
[01:04:49.420 --> 01:04:54.660]   So many languages will have a lot more code switching going on, on average than English
[01:04:54.660 --> 01:04:55.660]   does.
[01:04:55.660 --> 01:04:57.460]   Sorry, what is, what is code switching?
[01:04:57.460 --> 01:04:58.460]   Yeah.
[01:04:58.460 --> 01:05:01.500]   So code switching is when you use multiple languages in the same conversation, sometimes
[01:05:01.500 --> 01:05:03.220]   even in the same sentence.
[01:05:03.220 --> 01:05:08.980]   And that happens a lot in communities where there's a lot of bilingualism or multilingualism.
[01:05:08.980 --> 01:05:12.980]   So if you don't, if you and I, well, you know, you also speak Nihongo, right?
[01:05:12.980 --> 01:05:13.980]   You said so.
[01:05:13.980 --> 01:05:14.980]   Yep.
[01:05:14.980 --> 01:05:18.700]   You know, when you studied Kanji, you know, what was your favorite way to "benkyou" them?
[01:05:18.700 --> 01:05:23.180]   I am not a fluent code switcher, so that was really awkward and stupid sounding, but to
[01:05:23.180 --> 01:05:25.340]   illustrate the point, right?
[01:05:25.340 --> 01:05:31.020]   I remember actually when, yeah, I know, I've experienced that for sure.
[01:05:31.020 --> 01:05:32.020]   Yeah.
[01:05:32.020 --> 01:05:35.420]   So certainly English is involved in a lot of code switching, but there's also lots and
[01:05:35.420 --> 01:05:37.260]   lots of monolingual English data.
[01:05:37.260 --> 01:05:43.460]   And when you go into, you know, social media data for Indian languages, for example, enormous
[01:05:43.460 --> 01:05:45.780]   amounts of it are code switched with English.
[01:05:45.780 --> 01:05:50.780]   And so there's a whole range of interesting technical challenges that come up there.
[01:05:50.780 --> 01:05:57.860]   We live in a world where the first digital setups were sort of accommodated lower ASCII
[01:05:57.860 --> 01:05:59.660]   the most conveniently.
[01:05:59.660 --> 01:06:02.200]   English all fits in lower ASCII, right?
[01:06:02.200 --> 01:06:03.740]   English has relatively fixed word order.
[01:06:03.740 --> 01:06:08.300]   We have a relatively low, relatively simple morphology.
[01:06:08.300 --> 01:06:13.140]   So any given word that shows up is only going to show up in a few different forms.
[01:06:13.140 --> 01:06:17.060]   Compare that to Turkish where you can get like, I think millions of inflected forms
[01:06:17.060 --> 01:06:18.060]   the same route.
[01:06:18.060 --> 01:06:24.220]   And so that changes the way you handle data sparsity and what data sparsity looks like.
[01:06:24.220 --> 01:06:31.180]   So yeah, you know, English, our orthography is a mess, right?
[01:06:31.180 --> 01:06:38.120]   So you know, someone was just asking on Twitter, how come we do grapheme to phoneme prediction,
[01:06:38.120 --> 01:06:40.940]   but not phoneme to graphing prediction?
[01:06:40.940 --> 01:06:45.140]   So grapheme to phoneme is given a letter with the likely sound.
[01:06:45.140 --> 01:06:47.680]   And that's an important component of text to speech systems.
[01:06:47.680 --> 01:06:52.780]   When you hit an out of vocabulary word, phoneme to grapheme would be given a sound with the
[01:06:52.780 --> 01:06:54.260]   likely letter.
[01:06:54.260 --> 01:06:56.440]   And that's not a typical task.
[01:06:56.440 --> 01:07:01.300]   And I wonder to what extent that's true because of English's opaque and chaotic writing system.
[01:07:01.300 --> 01:07:04.260]   Sounds like an impossible task.
[01:07:04.260 --> 01:07:05.500]   Yeah, exactly.
[01:07:05.500 --> 01:07:09.900]   But if you were to look at, you know, so Japanese setting aside the Kanji, if you're just trying
[01:07:09.900 --> 01:07:14.140]   to transcribe Japanese in kana, that's way more straightforward.
[01:07:14.140 --> 01:07:19.900]   Spanish also has a very transparent and consistent grapheme to phoneme mapping in both directions.
[01:07:19.900 --> 01:07:24.220]   So down to things like that, the properties of a writing system for English.
[01:07:24.220 --> 01:07:30.220]   English likes to use white space between words and sentence final punctuation, right?
[01:07:30.220 --> 01:07:33.500]   These are things that we sort of just take as given, that it's easy to tokenize into
[01:07:33.500 --> 01:07:37.620]   sentences and words that just aren't going to be true in other languages.
[01:07:37.620 --> 01:07:38.620]   So I don't know.
[01:07:38.620 --> 01:07:39.860]   I couldn't tell you what NLP would look like.
[01:07:39.860 --> 01:07:42.660]   I can just sort of tell you sort of where the points of divergence might be.
[01:07:42.660 --> 01:07:43.660]   No, those are fun.
[01:07:43.660 --> 01:07:44.660]   Those are, yeah.
[01:07:44.660 --> 01:07:45.660]   I mean, definitely.
[01:07:45.660 --> 01:07:49.660]   I mean, I don't know.
[01:07:49.660 --> 01:07:53.940]   Those differences are so, so interesting.
[01:07:53.940 --> 01:07:57.180]   You voluntarily took a linguistics class, so I'm not surprised.
[01:07:57.180 --> 01:08:01.900]   Well, I think it's like, I mean, I just feel like linguistics is so cool.
[01:08:01.900 --> 01:08:06.180]   I mean, as an outsider, just because you, if you don't know it, then it's really eye
[01:08:06.180 --> 01:08:11.480]   opening to just to, yeah, because you swim in it to sort of see, oh, there's all these
[01:08:11.480 --> 01:08:13.580]   patterns that I never would have noticed.
[01:08:13.580 --> 01:08:16.720]   And I feel like especially, well, I don't know, like phonetics is like probably the
[01:08:16.720 --> 01:08:21.140]   most deep where you're just like, oh my God, those two sounds are different.
[01:08:21.140 --> 01:08:25.020]   I would not have, I would just never, never have noticed that.
[01:08:25.020 --> 01:08:28.060]   But then it's so clear to, it's so easy to do the thought experiment and realize you're
[01:08:28.060 --> 01:08:29.060]   wrong.
[01:08:29.060 --> 01:08:30.060]   It's just, I don't know.
[01:08:30.060 --> 01:08:31.060]   I love that stuff.
[01:08:31.060 --> 01:08:32.060]   And yeah, it's funny.
[01:08:32.060 --> 01:08:40.820]   I mean, I remember, I don't know, I feel like most of my early work was in parsing Japanese
[01:08:40.820 --> 01:08:41.820]   in different ways.
[01:08:41.820 --> 01:08:46.780]   And so I do remember, I do remember, I don't know, I guess it didn't seem like that was
[01:08:46.780 --> 01:08:52.740]   a impediment to publishing, but it was surprising that there was so little work on it for how
[01:08:52.740 --> 01:08:57.300]   necessary of a task it would be to deal with it.
[01:08:57.300 --> 01:09:01.460]   And then, and then in my first job, it was mostly processing Japanese language stuff.
[01:09:01.460 --> 01:09:07.380]   And there's, it was, it was striking how little research there was defined on the topic.
[01:09:07.380 --> 01:09:10.940]   Like I felt like there was just sort of more institutional knowledge inside of companies
[01:09:10.940 --> 01:09:13.660]   than, than literature on it.
[01:09:13.660 --> 01:09:17.820]   Because what happened in the research community as well, that kind of parsing problem is solved,
[01:09:17.820 --> 01:09:18.820]   right?
[01:09:18.820 --> 01:09:22.240]   Because people had made a certain progress on it for English and that was mistaken as
[01:09:22.240 --> 01:09:23.620]   the problem in general being solved.
[01:09:23.620 --> 01:09:24.620]   So what's new here?
[01:09:24.620 --> 01:09:25.940]   Well, this is for Japanese.
[01:09:25.940 --> 01:09:26.940]   That's new.
[01:09:26.940 --> 01:09:30.580]   This hasn't been done, but it's actually hard to get people to see that.
[01:09:30.580 --> 01:09:35.340]   And so my, my goal with what got called the Bender rule is to say, okay, let's keep English
[01:09:35.340 --> 01:09:38.700]   in its place and say, you know, when I've done this for English, I need to say that
[01:09:38.700 --> 01:09:44.220]   it's for English to hold room for the other work on other languages, which is also really
[01:09:44.220 --> 01:09:47.020]   important and, and novel and valuable.
[01:09:47.020 --> 01:09:51.980]   And we'll see it's you know, if we periodically go through different folks in the field, go
[01:09:51.980 --> 01:09:58.100]   through and count how many papers in an ACL conference actually work on different languages
[01:09:58.100 --> 01:10:01.500]   and actually say what language they work on and it's not changing as fast as I'd like,
[01:10:01.500 --> 01:10:03.220]   but there's some really good developments.
[01:10:03.220 --> 01:10:06.940]   So the universal dependencies project has produced tree banks for many, many languages.
[01:10:06.940 --> 01:10:11.820]   And that has spurred a whole bunch of, of very cross-linguistic work, which is exciting.
[01:10:11.820 --> 01:10:17.500]   And what do you think about, I mean, it, I mean, some of the most like evocative work
[01:10:17.500 --> 01:10:22.280]   feels like, you know, like building language models across like all the languages or like
[01:10:22.280 --> 01:10:26.660]   translation models that can kind of use pairs of languages in interesting ways where you
[01:10:26.660 --> 01:10:28.700]   have more data to help with ones with less data.
[01:10:28.700 --> 01:10:32.540]   I guess, do you, do you think that's like a fruitful direction or does that, do you
[01:10:32.540 --> 01:10:37.100]   think that sort of like encodes our biases somehow in the, in the way it works?
[01:10:37.100 --> 01:10:39.100]   So, I mean, it's, it's certainly interesting.
[01:10:39.100 --> 01:10:44.060]   And to the extent that we're relying on these massive data hungry things where languages
[01:10:44.060 --> 01:10:48.140]   just don't have that much data, seeing what we can do based on, you know, transfer from
[01:10:48.140 --> 01:10:50.900]   the bigger languages is an interesting and valuable way to go.
[01:10:50.900 --> 01:10:59.460]   I think the interesting questions to ask would be to what extent does this impose the conceptualization
[01:10:59.460 --> 01:11:03.980]   of the world encoded in English onto the results in these other languages?
[01:11:03.980 --> 01:11:05.900]   And you know, what follows from that?
[01:11:05.900 --> 01:11:09.980]   Like what, what are the risks and how does that compare to, well, but if we just do monolingual,
[01:11:09.980 --> 01:11:11.140]   we can only get this far.
[01:11:11.140 --> 01:11:12.380]   So we'll, we'll take those risks.
[01:11:12.380 --> 01:11:14.060]   We'll figure out how to mitigate them.
[01:11:14.060 --> 01:11:15.860]   That kind of work I think is important.
[01:11:15.860 --> 01:11:21.740]   And it's also really, really important to know that you are working with genuine data
[01:11:21.740 --> 01:11:23.020]   in the low resource languages.
[01:11:23.020 --> 01:11:26.860]   So there was this thing where it came out that I think it was Scots, the entire Scots
[01:11:26.860 --> 01:11:31.260]   Wikipedia was written by one person who doesn't speak Scots.
[01:11:31.260 --> 01:11:33.940]   And Wikipedia is this really important data source in NLP.
[01:11:33.940 --> 01:11:37.620]   So any NLP system that claims to be doing something for Scots just isn't.
[01:11:37.620 --> 01:11:44.060]   And a fantastic model in that regard is this research collective called Masakane, which
[01:11:44.060 --> 01:11:49.620]   is a continent spanning research initiative in Africa towards doing participatory research
[01:11:49.620 --> 01:11:51.940]   to create language resources for African languages.
[01:11:51.940 --> 01:11:56.180]   And they've done really interesting work on how to build up the community so that people
[01:11:56.180 --> 01:12:00.300]   can come contribute as, you know, as translators, not machine translation specialists, but people
[01:12:00.300 --> 01:12:02.380]   actually translating language.
[01:12:02.380 --> 01:12:07.100]   And there's a really cool paper that came out in, I think, findings of EMNLP last year
[01:12:07.100 --> 01:12:08.460]   describing the Masakane product.
[01:12:08.460 --> 01:12:09.920]   So project.
[01:12:09.920 --> 01:12:13.460]   So that kind of work of like, if you're going to work with low resource languages, being
[01:12:13.460 --> 01:12:18.260]   sure to connect with the community who would be the people using the technology, then you
[01:12:18.260 --> 01:12:20.260]   could find out, you know, okay, well, what are the concerns?
[01:12:20.260 --> 01:12:27.180]   To what extent do you want to, you know, bring in what we can do from using the larger resource
[01:12:27.180 --> 01:12:32.780]   languages versus would you rather stay monolingual and see where we can go and, you know, hear
[01:12:32.780 --> 01:12:35.180]   from the community and involve the community in the research.
[01:12:35.180 --> 01:12:37.340]   And I think Masakane is a great model of that.
[01:12:37.340 --> 01:12:38.340]   Cool.
[01:12:38.340 --> 01:12:39.820]   Well, that seems like a good place to end.
[01:12:39.820 --> 01:12:41.700]   We're way over time and you've been really generous.
[01:12:41.700 --> 01:12:45.500]   Thank you so much, but I really enjoyed talking to you.
[01:12:45.500 --> 01:12:46.500]   Likewise.
[01:12:46.500 --> 01:12:47.500]   Thank you.
[01:12:47.500 --> 01:12:48.500]   I can go on and on.
[01:12:48.500 --> 01:12:49.500]   So I appreciate the chance to do so.
[01:12:49.500 --> 01:12:53.900]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:12:53.900 --> 01:12:58.660]   to the show notes in the description where you can find links to all the papers that
[01:12:58.660 --> 01:13:02.900]   are mentioned, supplemental material, and a transcription that we work really hard to
[01:13:02.900 --> 01:13:03.900]   produce.

