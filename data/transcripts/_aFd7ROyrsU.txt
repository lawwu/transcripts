
[00:00:00.400 --> 00:00:06.800]   in recent weeks there has been a lot of turmoil surrounding ai technology it increasingly seems now
[00:00:06.800 --> 00:00:13.520]   like those grand promises of super intelligence or ai systems automating most of the economy
[00:00:13.520 --> 00:00:20.320]   these grand promises are probably not going to come true but what about the more practical promise
[00:00:20.320 --> 00:00:27.040]   the one that ai tools are going to make knowledge workers more productive especially if you do
[00:00:27.040 --> 00:00:33.760]   something like computer programming for which ai is well suited that's still true right well the
[00:00:33.760 --> 00:00:41.280]   answer turns out to be complicated a recent study about ai productivity yield a completely unexpected
[00:00:41.280 --> 00:00:48.240]   result it created a glitch in the matrix so to speak that leads to some deeper truths about this
[00:00:48.240 --> 00:00:54.160]   technology and its potential role in our work today i want to explore all of this if you're at all
[00:00:54.160 --> 00:01:00.080]   interested in how ai might impact your job in the near future this is an episode you won't want
[00:01:00.080 --> 00:01:13.360]   to miss as always i'm cal newport and this is deep questions
[00:01:13.360 --> 00:01:21.200]   today's episode a glitch in the ai matrix
[00:01:25.040 --> 00:01:29.680]   all right so i previewed there was a study that caught a lot of people off guard i want to load
[00:01:29.680 --> 00:01:36.000]   this on the screen here for those who are watching instead of just listening this study came out in
[00:01:36.000 --> 00:01:43.600]   july of 2025 it was produced by a non-profit called metr that's often pronounced meter it's a non-profit
[00:01:43.600 --> 00:01:48.960]   that does evaluation of ai and its capabilities they do high quality studies they don't have any particular
[00:01:48.960 --> 00:01:55.520]   bias ai companies often cite their results when they find them to be useful or impressive so this
[00:01:55.520 --> 00:02:00.720]   is sort of a neutral body that does these reports here's the title of their july report that caused a
[00:02:00.720 --> 00:02:10.640]   bit of a surprise measuring the impact of early 2025 ai on experienced open source developer productivity
[00:02:11.760 --> 00:02:18.720]   so what did they do i'm actually going to read from the methodology section because i think it's important
[00:02:18.720 --> 00:02:24.160]   to understand like exactly what they were testing here so let's read here together this is from the
[00:02:24.160 --> 00:02:30.800]   paper to directly measure the real world impact of ai tools on software development we recruited 16
[00:02:30.800 --> 00:02:35.840]   experienced developers from large open source repositories that they've contributed to for
[00:02:35.840 --> 00:02:42.000]   multiple years developers provide lists of real issues that would be valuable to the repository bug fixes
[00:02:42.000 --> 00:02:47.840]   features and refactors that would normally be part of their regular work then we randomly
[00:02:47.840 --> 00:02:56.560]   assign each issue to either allow or disallow use of ai while working on that particular issue when ai is
[00:02:56.560 --> 00:03:03.520]   allowed developers can use any tool they choose primarily cursor pro with clod 3.5 or 3.7 sonnet
[00:03:03.520 --> 00:03:08.400]   which were the frontier models at the time of the study when disallowed they work without generative ai
[00:03:08.400 --> 00:03:12.480]   assistance developers complete these tasks which average about two hours each while recording their
[00:03:12.480 --> 00:03:20.080]   screens then self-report total implementation time they needed all right so that's the setup it's it's a
[00:03:20.080 --> 00:03:26.160]   very elegant experiment here's developers working on the normal stuff they develop and issue by issue
[00:03:26.160 --> 00:03:31.840]   hey for this issue i want you to use ai for this issue you don't for this issue you don't use ai you do
[00:03:31.840 --> 00:03:36.880]   right so you get this nice randomized control of all the developers are both using ai and not using ai
[00:03:36.880 --> 00:03:44.640]   in a randomly selected manner the simple thing they did want to figure out is how long did it take to do
[00:03:44.640 --> 00:03:52.240]   these tasks when you were using ai versus not now here's where the conventional wisdom says computer
[00:03:52.240 --> 00:03:55.920]   programming is something that language models do well so of course it makes productivity go up
[00:03:55.920 --> 00:04:01.680]   i'm going to load up on the screen here the core chart of this whole paper so if you're looking
[00:04:01.680 --> 00:04:07.840]   at this what you're seeing right now is a bunch of green dots so what these are showing the the things
[00:04:07.840 --> 00:04:14.000]   you can see the data points you can see on my screen right now is predictions from various people
[00:04:14.000 --> 00:04:21.040]   about the increases of productivity that they thought ai would give them so if you'll notice
[00:04:21.920 --> 00:04:27.520]   there's a line in the middle here that's neutral so a data point on that line means ai makes no difference
[00:04:27.520 --> 00:04:33.680]   if a data point is below that line it means ai slows you down if it's above it it means ai speeds you up
[00:04:33.680 --> 00:04:38.720]   so the the first point is from economic experts they ask economic experts how much more productive
[00:04:38.720 --> 00:04:44.560]   will this make these programmers in this experiment and on average they said uh around the 40 speed up
[00:04:44.560 --> 00:04:49.680]   they ask machine learning experts so people who knew the technology they were similar yeah it should be
[00:04:49.680 --> 00:04:56.560]   about a 40 speed up they asked the developers themselves during the study like hey how much
[00:04:56.560 --> 00:05:01.120]   more productive do you think this is making you and they were like yeah like between 20 and 30 more
[00:05:01.120 --> 00:05:05.040]   productive and when they asked them after the study was over they were you know around the same place
[00:05:05.040 --> 00:05:10.800]   like a little bit over 20. what was actually measured i'll scroll up to show you that result
[00:05:10.800 --> 00:05:20.320]   the observed result is on average they were about 20 percent slower than the people not using ai so the ai
[00:05:20.320 --> 00:05:30.320]   tasks were slower than the tasks in which the programmers did not use ai this was an unexpected
[00:05:30.320 --> 00:05:34.560]   result they thought they would just be measuring how much more productive ai made these programmers
[00:05:34.560 --> 00:05:43.040]   they were unexpected to see uh that it makes them slower so this is a bit of a mystery and it's one
[00:05:43.040 --> 00:05:50.320]   we're going to look into because it gets to something core about ai and doing knowledge work why did
[00:05:50.320 --> 00:05:56.880]   programmers despite their predictions become less productive when they used ai all right so if we want
[00:05:56.880 --> 00:06:03.280]   to solve this particular productivity paradox i think what we need to do next is understand
[00:06:04.160 --> 00:06:09.680]   what type of work are these computer programmers doing we got to get specific here now i'm going
[00:06:09.680 --> 00:06:15.200]   to use a term that most of my audience is familiar with computer programming handling these tasks which
[00:06:15.200 --> 00:06:19.840]   are a combination i looked into more deeply but it's a combination of creating original code or fixing code
[00:06:19.840 --> 00:06:27.680]   they require what i call deep work deep work as you know is a task that require you to focus
[00:06:27.680 --> 00:06:32.640]   without distraction on something that's cognitively demanding so deep work is when you give your full attention to
[00:06:32.640 --> 00:06:39.200]   something that is demanding and you try to keep your attention on it as intensely as possible i sort of
[00:06:39.200 --> 00:06:43.600]   wrote the book and it's sort of i wrote the book on it i coined the term it's been 10 years jesse not hard to
[00:06:43.600 --> 00:06:49.920]   believe yeah in january it's going to be the 10 year anniversary of deep work and i guess i'm going to follow
[00:06:49.920 --> 00:06:53.600]   through with the promise i made my readers that if this book is still selling after 10 years
[00:06:54.160 --> 00:07:02.480]   a face tattoo boom like mike tyson like mike tyson deep work right across the forehead ryan holiday put
[00:07:02.480 --> 00:07:07.360]   his book title on his forearm he's going to one-up him right across the face that's what a real committed
[00:07:07.360 --> 00:07:11.760]   author does uh so anyways here's the type of work we're talking about when we talk about computer
[00:07:11.760 --> 00:07:16.400]   programs deep work right it's something that requires uh focus you're creating something complicated from
[00:07:16.400 --> 00:07:22.960]   scratch using your brain is kind of demanding and it benefits from being able to give it undistracted
[00:07:22.960 --> 00:07:27.600]   focus in the modern knowledge economy the core argument i make in my book on deep work is that
[00:07:27.600 --> 00:07:32.320]   this is the effort that creates most of the stuff that's actually valuable to the marketplace all the
[00:07:32.320 --> 00:07:36.640]   other things we do the emails the meetings the filling out the forms the submitting the things the
[00:07:36.640 --> 00:07:42.560]   tickets to it the making the powerpoint presentations that's basically all efforts that support deep work
[00:07:42.560 --> 00:07:46.960]   the stuff we have to do to keep the lights on and tell people about what we did but you can't run
[00:07:46.960 --> 00:07:50.880]   a profitable company just off of email meetings and powerpoints ultimately someone has to actually
[00:07:50.880 --> 00:07:56.480]   create something valuable using their brain and that requires deep work so in this study we had
[00:07:56.480 --> 00:08:02.640]   programmers doing deep work who used ai to help them and they thought that would make their deep work
[00:08:02.640 --> 00:08:09.520]   more productive but in reality it made things worse great so now we're narrowing in on the answer to
[00:08:09.520 --> 00:08:13.520]   this paradox so we know what these programmers are doing deep work and we know that ai wasn't helping
[00:08:13.520 --> 00:08:21.920]   them so let's narrow in this question even farther how specifically were these programmers integrating
[00:08:21.920 --> 00:08:29.520]   the ai tools they were using into their deep work efforts according to the study authors the
[00:08:29.520 --> 00:08:36.560]   programmers were working interactively with these tools so what they were doing was a lot of can you
[00:08:36.560 --> 00:08:41.920]   produce me this code i'll look at this code hey can you fix this about it or this is not working so
[00:08:41.920 --> 00:08:46.480]   kind of back and forth like that or here's some code you see any mistakes great then the the programmer
[00:08:46.480 --> 00:08:50.480]   would run it and be like it's still not quite working right can you try to fix it here so it's a
[00:08:50.480 --> 00:08:56.720]   very interactive loop they were going back and forth having code produced checking the code having the ai
[00:08:56.720 --> 00:09:01.760]   check the code looking at what it sent back so think of it as like a a back and forth interaction
[00:09:03.280 --> 00:09:11.120]   um here is how let me see here here is how the uh paper itself describes this let's see here okay here
[00:09:11.120 --> 00:09:17.760]   we go here's text from the actual paper about this collaboration when allowed to use ai developers spend a
[00:09:17.760 --> 00:09:23.680]   smaller proportion of their time actively coding and reading searching for information instead they spend
[00:09:23.680 --> 00:09:29.760]   time reviewing ai outputs prompting ai systems and waiting for ai generations interestingly they also spend a somewhat
[00:09:29.760 --> 00:09:33.280]   higher proportion of their time idle where their screen recording doesn't show any activity so
[00:09:33.280 --> 00:09:40.800]   the non-ai group when they're working on a task without a without ai is more time makes sense like
[00:09:40.800 --> 00:09:46.720]   actually writing code and the ai group is spending more time prompting asking for code asking for it to
[00:09:46.720 --> 00:09:51.120]   check code so it's more of this like interactive loop i'm going to give a name to this
[00:09:52.640 --> 00:09:57.280]   let's call this cybernetic collaboration because these programmers are collaborating on their deep
[00:09:57.280 --> 00:10:02.880]   work with a computer so it's cybernetic and they're collaborating let's call it cybernetic collaboration
[00:10:02.880 --> 00:10:08.320]   they're basically trying to split the cognitive effort of producing this code or fixing this code between
[00:10:08.320 --> 00:10:18.000]   them and this digital mind now intuitively cybernet collaboration should make you more productive why not
[00:10:18.000 --> 00:10:22.480]   right like you're you don't have to do as much deep work anymore because you're offloading some of it
[00:10:22.480 --> 00:10:27.120]   to a machine and machines are fast and machines are precise and hey this seems like you've just made
[00:10:27.120 --> 00:10:34.000]   things more productive but of course that didn't happen so now we've really narrowed in this paradox
[00:10:34.000 --> 00:10:38.800]   they're doing deep work they're collaborating interactively doing cybernetic collaboration with
[00:10:38.800 --> 00:10:43.440]   the ai while they're doing deep work and that is not returning more productivity it's still taking them
[00:10:43.440 --> 00:10:48.720]   longer to produce stuff than when they weren't using ai at all so let's narrow in even farther
[00:10:48.720 --> 00:10:56.880]   and let's step back now and ask the question what role does collaboration play in deep work is that
[00:10:56.880 --> 00:11:02.960]   something that you can do with multiple minds now this is actually an important question because a lot
[00:11:02.960 --> 00:11:08.640]   of people who had a glancing encounter with my book on deep work assumed that it had to be solitary
[00:11:09.440 --> 00:11:15.680]   but it doesn't actually i know a lot about collaborative deep work because this was one of
[00:11:15.680 --> 00:11:20.960]   the core skills i had mastered as a theoretical computer scientist like this is if anyone knows
[00:11:20.960 --> 00:11:26.640]   how to do collaborative deep work it's scientists that do math theory like me i learned how to do
[00:11:26.640 --> 00:11:32.080]   collaborative deep work first at mit when i was doing my doctoral work and then refined these skills as
[00:11:32.080 --> 00:11:37.760]   uh faculty computer science science faculty at georgetown i write in my book deep work about
[00:11:37.760 --> 00:11:46.320]   how to do it successfully collaboratively so yes deep work can be done collaboratively but how is it done
[00:11:46.320 --> 00:11:50.640]   successfully if you want to do deep work with someone else you want to collaborate in practice if
[00:11:50.640 --> 00:11:57.440]   you're hanging out with mathematicians how do we do collaborative deep work uh successfully well here's
[00:11:57.440 --> 00:12:04.240]   the thing you talk to any professional thinker who does this they'll say the same the reason why
[00:12:04.240 --> 00:12:10.640]   collaboration i guess i should say the way you make collaboration help with deep work is that you use
[00:12:10.640 --> 00:12:18.640]   the presence of other people to increase the intensity and duration of your focus so the underlying formula
[00:12:20.320 --> 00:12:26.480]   focus produces results deeper focus produces better results that formula reigns supreme so when i would
[00:12:26.480 --> 00:12:30.480]   sit down with my frequent collaborators to work on some sort of theorem or some sort of paper we're
[00:12:30.480 --> 00:12:35.600]   working on the whole game was trying to get even more intense focus here's how this works if i'm sitting
[00:12:35.600 --> 00:12:43.840]   at a whiteboard with two other mathematicians a couple things happen one i'm going to maintain my focus
[00:12:43.840 --> 00:12:48.480]   on this problem much longer than if i'm just alone looking at a notebook why because if i'm alone
[00:12:49.280 --> 00:12:54.480]   there's very little penalty for me allowing my attention to wander ah this is hard let me just
[00:12:54.480 --> 00:12:58.800]   like let my attention wander let me let some of the steam out of the metaphorical steam engine here
[00:12:58.800 --> 00:13:04.000]   let me go check something else nothing bad happens if i do it's just me trying to contain myself
[00:13:04.000 --> 00:13:10.000]   if i'm at a whiteboard with two other mathematicians however and i let my attention wander i lose the
[00:13:10.000 --> 00:13:13.840]   threat of thought and then what do i have to do a couple minutes later something that's kind of
[00:13:13.840 --> 00:13:18.800]   embarrassing i have to like hold on hold on go back i missed what you guys were just saying here
[00:13:19.520 --> 00:13:23.840]   because i let my mind wander everyone has to stop everyone has to go back so the social pressure
[00:13:23.840 --> 00:13:28.880]   of keeping up means that you keep your intensity longer the other thing that happens when you're
[00:13:28.880 --> 00:13:32.960]   working with other people is it pushes your intensity of concentration deeper right because
[00:13:32.960 --> 00:13:37.280]   you're sitting there trying to understand this someone has a breakthrough like okay hear me out and
[00:13:37.280 --> 00:13:41.760]   they go up to the whiteboard and they start working on some sort of like complicated simplification of
[00:13:41.760 --> 00:13:46.480]   an equation or some sort of graph construction and you're like man i gotta really lock in to follow
[00:13:46.480 --> 00:13:52.320]   this right someone is trying to download something complicated they just had an insight about to my
[00:13:52.320 --> 00:13:58.960]   brain and the only way to get there is to like really lock in and focus so it's a focus accelerator
[00:13:58.960 --> 00:14:04.640]   that is what you get when you're doing deep work i think it's off there now but jesse that whiteboard
[00:14:04.640 --> 00:14:08.960]   we had here in the hq for i used to bring like during the pandemic starting the pandemic used to
[00:14:08.960 --> 00:14:13.200]   bring my collaborators from hopkins and georgetown they'd come here and we'd work on the whiteboard
[00:14:13.200 --> 00:14:17.440]   yeah i don't know if it's still up there or not but like the last thing that there was so we would
[00:14:17.440 --> 00:14:20.800]   sit at that literal whiteboard and the last thing that was up there for a couple years we actually won
[00:14:20.800 --> 00:14:25.520]   an award for that paper so maybe i should have kept that it's a good paper but yeah that's what we do
[00:14:25.520 --> 00:14:29.360]   so uh there's other advantages of working with other people but this is like one of the big
[00:14:29.360 --> 00:14:34.960]   things you get out of deep work if you collaborate right is that it makes you focus harder and focus
[00:14:34.960 --> 00:14:39.920]   longer so again there's this underlying equation intensity of focus times time is how much you
[00:14:39.920 --> 00:14:44.800]   produce when you're doing cognitively demanding work that still reigns so collaboration all you're doing
[00:14:44.800 --> 00:14:50.320]   if you're doing it right on hard things is getting more focus you're squeezing more focus out of it
[00:14:50.320 --> 00:14:55.680]   i call that the whiteboard effect in my book go back to cybernetic collaboration that's not what
[00:14:55.680 --> 00:15:02.560]   these programmers are doing they are using collaboration with ai to reduce the amount of
[00:15:02.560 --> 00:15:08.160]   intensity of focus they have to experience to get those breaks you produce it i'll look at it
[00:15:08.160 --> 00:15:14.560]   and it's easier for me to try to debug what you did that was broken than it is for me to have to
[00:15:14.560 --> 00:15:19.840]   produce it from scratch it's easier for me to have these nice breaks while i'm waiting for the ai model to
[00:15:19.840 --> 00:15:24.560]   generate the code i'm going to look at which can take a little while we know this jesse because we get
[00:15:24.560 --> 00:15:29.760]   this question all the time now what should i do while waiting for the ai model to produce my code
[00:15:29.760 --> 00:15:35.520]   like we only put it on the show like once but we get this question a lot so that gives you a break so
[00:15:35.520 --> 00:15:40.800]   it makes the experience more pleasant there's actually the the authors of this say this somewhere
[00:15:40.800 --> 00:15:45.920]   in the study that it was a more pleasant experience for the programmers because you get all these breaks
[00:15:45.920 --> 00:15:50.320]   you don't have to be locked in when you're just looking at the bank blank coding page no word no piece
[00:15:50.320 --> 00:15:54.000]   of code is going to get there until you come up with it and write it it's really hard there's no real
[00:15:54.000 --> 00:15:58.000]   break until you tell yourself to take a break because you're trying to get all this code together
[00:15:58.000 --> 00:16:02.560]   compiling for small programs is fast too you don't get much of a break from it and so it's much more
[00:16:02.560 --> 00:16:07.600]   pleasant cybernet collaboration means much less uh much less intensity of focus much less duration
[00:16:07.600 --> 00:16:15.200]   of focus it takes less energy it feels nicer but that's why they're slower because intensity of focus
[00:16:15.200 --> 00:16:18.800]   is what tells you how fast you're going to go intensity of focus tells you how good the stuff
[00:16:18.800 --> 00:16:23.600]   you're going to produce is when it comes to deep work so the whiteboard effect says yeah come work
[00:16:23.600 --> 00:16:29.440]   with me so i can focus harder good stuff gets produced cybernet collaboration by contrast says i
[00:16:29.440 --> 00:16:34.000]   want a computer to offload some of the work so i get a lot more breaks but that means my brain
[00:16:34.880 --> 00:16:40.720]   is producing a lot less slower less quality stuff and that is why the time required to get the work
[00:16:40.720 --> 00:16:47.520]   done begins to go down now like look if the machine was actually able to take over all of the deep work
[00:16:47.520 --> 00:16:52.640]   that would be different if you could literally just say vibe code this program and then commit it
[00:16:52.640 --> 00:16:56.240]   that'd be great because now you're like i don't do any deep work at all but of course the machine
[00:16:56.800 --> 00:17:01.760]   the machine can't do that in cybernet collaboration they can't do that yet so now you're doing this
[00:17:01.760 --> 00:17:06.880]   back and forth dance where you ultimately still have to come up with the questions and edit the code and
[00:17:06.880 --> 00:17:10.800]   get everything to work but you're cutting down your intensity of focus so the whole operation inside
[00:17:10.800 --> 00:17:16.240]   your brain is going slower i want to bring up another quote here this is from an article from
[00:17:16.240 --> 00:17:22.400]   the atlantic that was written by roger karma and it talked about this study among other things the
[00:17:22.400 --> 00:17:28.320]   title of this article is just how bad would an ai bubble be and it really is about ai and productivity
[00:17:28.320 --> 00:17:33.840]   i'm quoted towards the end of this article it's a good article but here's what he heard from the meter
[00:17:33.840 --> 00:17:39.040]   developers about what was actually happening as they were doing this sort of cybernetic collaboration so
[00:17:39.040 --> 00:17:44.400]   let me quote here even the most advanced systems make small mistakes or slightly misunderstand directions
[00:17:44.400 --> 00:17:49.280]   requiring a human to carefully review their work and make changes where needed this appears to be what
[00:17:49.280 --> 00:17:53.440]   happened during the meter study developers ended up spending a lot of time checking and redoing the
[00:17:53.440 --> 00:17:58.640]   code that ai systems had produced often times often more time than it would have taken simply to write
[00:17:58.640 --> 00:18:04.720]   it themselves right so they were just in this loop of you riot i'll look at it you debug let me look at
[00:18:04.720 --> 00:18:11.040]   what you're doing but because they were avoiding the full intensity of focus that they're capable of
[00:18:11.040 --> 00:18:15.440]   because they avoided maintaining their focus without having context shifts or other distraction
[00:18:16.400 --> 00:18:21.840]   the work they were doing was just like not at their peak so now you you're doing all this work of
[00:18:21.840 --> 00:18:27.120]   cleaning stuff up and you're not operating at your peak this is the potential danger of cybernetic
[00:18:27.120 --> 00:18:36.640]   collaboration that when you downshift your mind let me downshift my focus intensity it just doesn't work as
[00:18:36.640 --> 00:18:42.800]   well it might feel nice but deep work doesn't really have a lot to do with nice all right so that's what
[00:18:42.800 --> 00:18:48.240]   seems to be going on there so some combination of this rhythm of going back and forth creates like a
[00:18:48.240 --> 00:18:53.040]   lot of work you might not have had to do before and more importantly it reduces the gear which your brain
[00:18:53.040 --> 00:18:57.360]   is operating so these programmers were just it was pleasant but they were producing you know stuff
[00:18:57.360 --> 00:19:02.080]   slower it was taking them longer to figure things out that's my one of my understandings from what is
[00:19:02.080 --> 00:19:07.200]   going on there uh but let's summarize this all the end of our deep dives we like to do some takeaways
[00:19:07.200 --> 00:19:11.920]   we've been experimenting with music our last takeaway theme music which i thought sounded like the
[00:19:11.920 --> 00:19:17.120]   beginning of a cable show about bass fishing a little aggressive so we're going to go a different
[00:19:17.120 --> 00:19:21.840]   way today for our takeaway theme music we're going to do something a little more you say jesse
[00:19:21.840 --> 00:19:26.800]   intellectual yeah calming all right so let's let's do our takeaways with a little bit of calming
[00:19:26.800 --> 00:19:37.920]   background music all right so what are our takeaways deep work rewards intensity of focus
[00:19:37.920 --> 00:19:42.320]   and if you add anything into your workflow that's going to reduce this intensity
[00:19:42.320 --> 00:19:48.320]   you'll probably get less productive this seems to be the trap that a lot of knowledge workers
[00:19:48.320 --> 00:19:54.880]   experimenting with ai right now are falling into focus is hard it doesn't feel pleasant
[00:19:54.880 --> 00:20:01.920]   it's tempting to try to make it go away but that doesn't make your work better now i'm absolutely
[00:20:01.920 --> 00:20:07.600]   convinced that there will be upcoming applications of ai that will help our productivity i think they
[00:20:07.600 --> 00:20:13.280]   will focus more on automating shallow tasks or speeding up things that don't require you to focus
[00:20:14.160 --> 00:20:20.960]   but cybernetic collaboration that is not the key to the ai future focus remains absolutely essential
[00:20:20.960 --> 00:20:26.560]   to doing deep work and for now it remains something that has to be hard to do
[00:20:26.560 --> 00:20:32.640]   i feel like we should slowly high five or something
[00:20:32.640 --> 00:20:40.640]   we did it so there we go there's a lot of other things going on in that study but i mean i think that's
[00:20:40.640 --> 00:20:45.520]   a lot of what's going on to it it's like look deep work is hard it's tempting to make deep work easier
[00:20:45.520 --> 00:20:48.400]   ai can help you do it but doesn't mean you're going to produce more work
[00:20:48.400 --> 00:20:54.640]   i love the takeaway portion of the deep dive segments now i'm working on it i think we're
[00:20:54.640 --> 00:20:57.920]   still working out the kinks but i like because while you're going through it i it's just so the
[00:20:57.920 --> 00:21:01.600]   audience knows i don't necessarily know what you're going to say and i was like well what do i do
[00:21:01.600 --> 00:21:06.720]   yeah so i mean i think it's good like sometimes it's advice and sometimes like here is the core
[00:21:06.720 --> 00:21:10.160]   message like here's the here's the thing i'm taking away from it so anyways that's a cool study
[00:21:10.160 --> 00:21:13.440]   man there's so much controversy around it like a lot of programmers are like well they don't know
[00:21:13.440 --> 00:21:17.520]   how to use the ai tools not true actually these are tools they got to choose which tools they use
[00:21:17.520 --> 00:21:22.000]   and these were developers that largely were already using these tools so you know there's always that
[00:21:22.000 --> 00:21:25.920]   argument of like well they're just using it wrong another argument it's a small study yeah it is small
[00:21:25.920 --> 00:21:30.000]   but it's a good signal it was a pretty clear signal it's pretty well designed
[00:21:30.000 --> 00:21:35.760]   another argument which i think is more important is some people will say yes that use of ai or you
[00:21:35.760 --> 00:21:40.320]   have it generating code from scratch is slower other uses where you're just automating like
[00:21:40.320 --> 00:21:44.800]   looking up information making that faster i believe that would make a programmer more productive so it
[00:21:44.800 --> 00:21:50.000]   does kind of uh that's not trying to get in the way of the deep work it's trying to speed up the things
[00:21:50.000 --> 00:21:54.240]   that's not deep work so you can spend more time doing deep work but basically i'm i'm sort of like a
[00:21:54.240 --> 00:22:00.800]   human a humanist intellectual chauvinist here the brain focusing hard is an incredibly powerful tool
[00:22:01.360 --> 00:22:05.360]   be wary of things that gets in the way of that i mean unless you really can just like outsource to
[00:22:05.360 --> 00:22:11.360]   work completely it might it might be some fool's gold all right we got good questions coming up got a
[00:22:11.360 --> 00:22:17.200]   good final segment this one will be interesting to jesse i did a little bit of original data journalism
[00:22:17.200 --> 00:22:22.640]   there's a claim out there that i'm looking into and the scene like is this claim true i actually looked
[00:22:22.640 --> 00:22:27.360]   up some data from something something it'll be kind of fun just investigate journalism but first
[00:22:28.160 --> 00:22:32.000]   before we get to the questions we have to do the thing that everyone is most interested in
[00:22:32.000 --> 00:22:38.640]   which is hearing from a sponsor jesse we have to talk about cozy earth as listeners know i'm a huge
[00:22:38.640 --> 00:22:43.280]   fan of their sheets they're the most comfortable sheets we own i think we have something like three
[00:22:43.280 --> 00:22:48.400]   different sets to rotate through we travel with them when we're away in our house rentals in the summer
[00:22:48.400 --> 00:22:52.240]   i will say this jesse if i was invited to stay in the lincoln bedroom
[00:22:53.200 --> 00:22:59.680]   at the white house i think my first question would be what type of sheets do you have second question
[00:22:59.680 --> 00:23:03.760]   would be why are you inviting me but first question would be what type of sheets you have
[00:23:03.760 --> 00:23:09.280]   but cozy earth here's what i'm excited about they've got some a new product one that i have been
[00:23:09.280 --> 00:23:14.320]   waiting for right so i'm a professor i'm a writer this means i'm not wearing a suit and going to an
[00:23:14.320 --> 00:23:18.720]   office every day and so i often wear pretty casual clothes and as the weather cools down i tend to
[00:23:18.720 --> 00:23:23.920]   wear sort of like athletic type pants you know i'm often on camera from the waist up it doesn't matter
[00:23:23.920 --> 00:23:29.200]   uh it doesn't matter what i'm wearing i mean i'm wearing bikini briefs right now because you know
[00:23:29.200 --> 00:23:36.480]   it's hot you got to get people to want cozy earth has a new product called bamboo joggers so that sort
[00:23:36.480 --> 00:23:42.880]   of like casual pant in their super comfortable style fabric that we already know and love you can
[00:23:42.880 --> 00:23:48.160]   wear their irresistibly soft jogger pants just about anywhere they're made from that viscose bamboo
[00:23:48.720 --> 00:23:54.640]   uh fabric this joggers are the softest staple wardrobe you're gonna one of the softest staples
[00:23:54.640 --> 00:23:58.240]   you're gonna have in your wardrobe whether you're staying in or stepping out and what about if you're
[00:23:58.240 --> 00:24:01.600]   going to like a slightly more formal affair so like you know me going to the lincoln bedroom to
[00:24:01.600 --> 00:24:07.520]   investigate their sheets well they also have you covered with their everyday pant which i have a
[00:24:07.520 --> 00:24:12.640]   pair of and i also love they they look snazzy but they're also like super comfortable with that fabric
[00:24:12.640 --> 00:24:17.040]   so cozy earth nailed comfort with their bamboo joggers and their everywhere pant that's next
[00:24:17.040 --> 00:24:23.040]   level everyone i know who is trying these things out says this is really comfortable so i'm a big cozy
[00:24:23.040 --> 00:24:29.600]   earth booster you should be too go to cozy earth.com deep and use my code deep for 40 off the best pants
[00:24:29.600 --> 00:24:34.640]   joggers shirts and everything and if you get a post purchase survey tell them you heard about cozy earth
[00:24:34.640 --> 00:24:41.920]   right here built for real life made to keep up with yours cozy earth that's cozy earth.com deep and used
[00:24:41.920 --> 00:24:50.080]   to code deep this show is also sponsored by better help we talk a lot about the different types of
[00:24:50.080 --> 00:24:55.280]   advice that might help you be more organized or make more progress on the things that might matter
[00:24:55.280 --> 00:25:00.320]   but you know what advice is probably the most important of all you need to cultivate a healthy
[00:25:00.320 --> 00:25:06.560]   relationship with your own mind everything else rests on that foundation if you want to improve
[00:25:06.560 --> 00:25:12.000]   this relationship therapy might be exactly what you need a professional who can help you understand
[00:25:12.000 --> 00:25:18.800]   your own mind and improve that relationship better help makes therapy easier with over 30 000 therapists
[00:25:18.800 --> 00:25:23.200]   better help is the world's largest online therapy platform they serve more than 5 million people
[00:25:23.200 --> 00:25:29.280]   globally it works they have an average client rating of 4.9 out of 5 stars for a live session based on
[00:25:29.280 --> 00:25:34.400]   over 1.7 million client reviews it's convenient too you can join a session with a therapist at the
[00:25:34.400 --> 00:25:40.560]   click of a button helping you fit therapy into your busy life plus you can switch therapists at any time
[00:25:40.560 --> 00:25:45.760]   as the largest online therapy provider in the world better help can provide access to mental health
[00:25:45.760 --> 00:25:52.320]   professionals with a diverse variety of expertise talk it out with better help our listeners can get
[00:25:52.320 --> 00:26:02.160]   10 percent off their first month at betterhelp.com if they go to betterhelp.com deep questions that's better help
[00:26:02.160 --> 00:26:11.680]   h-e-l-p dot com slash deep questions all right speaking of questions let's move on to our own
[00:26:11.680 --> 00:26:21.840]   all right first questions from sam i found that using chat gpt questions can act as project diaries i now keep
[00:26:21.840 --> 00:26:26.800]   multiple chats for projects i'm working on it enables a shutdown and a place marker for where i need to
[00:26:26.800 --> 00:26:35.680]   restart work can chat gpt chats act as single purpose notebooks for projects i mean sure a couple questions
[00:26:35.680 --> 00:26:42.640]   here what is a project note like this idea of like i need notebook to keep track of the progress on on
[00:26:42.640 --> 00:26:48.000]   my project was that something you're doing before i guess it's useful if you don't know where you left off
[00:26:48.000 --> 00:26:53.280]   and you want to have external notes on it but here's my concern my concern is that you're doing
[00:26:53.280 --> 00:26:57.840]   something like cybernetic collaboration with chat gpt we talked about this in the deep dive when you have
[00:26:57.840 --> 00:27:04.400]   this running list of notes and conversations with chat gpt i'm concerned what you're doing is transforming
[00:27:04.400 --> 00:27:09.360]   your work into something where you go back and forth with the chat bot you hand off more you know have
[00:27:09.360 --> 00:27:13.760]   them do it do some stuff and you look at what they did and you ask a question you go back and forth this is
[00:27:13.760 --> 00:27:18.160]   more pleasant this feels nicer it makes work seem better but just as the meter study found with
[00:27:18.160 --> 00:27:24.400]   programmers if it's reducing your peak intensity of focus or the sustained duration of your focus
[00:27:24.400 --> 00:27:28.560]   and you're doing anything that's non-trivial it also could be slowing you down and hurting the the
[00:27:28.560 --> 00:27:34.960]   the quality of your results the key with any type of effort that requires deep work is not reducing
[00:27:34.960 --> 00:27:40.080]   the difficulty of the deep work it's not reducing the intensity of focus it's setting things up so
[00:27:40.080 --> 00:27:45.440]   that you can reach high intensity of focus the focus mind is what produces value so that's my only issue
[00:27:45.440 --> 00:27:50.320]   i would be wary if you're trying to make work in this cybernet collaboration that might not be making
[00:27:50.320 --> 00:27:56.000]   you as productive as you think even if it feels nicer last week you talked about leaving a narrative for
[00:27:56.000 --> 00:28:01.760]   personal projects and originally i think this question was in relation to that but now that
[00:28:01.760 --> 00:28:05.360]   you talk about the cybernet collaboration it kind of makes sense but if you could touch on the narrative
[00:28:05.360 --> 00:28:10.480]   part too yeah so like understanding so what i meant by that is like understanding how to hit the ground
[00:28:10.480 --> 00:28:18.320]   running so you know if i'm writing something i will into writing sessions like okay i've gotten up to here
[00:28:18.320 --> 00:28:23.680]   and then i have a few notes i'm like okay here's what comes next so if i could like i'm missing this
[00:28:23.680 --> 00:28:27.840]   example but if i could get this key example you can we could finish this section and try to get to the
[00:28:27.840 --> 00:28:32.320]   end or something right so it's a little note i left for myself so that when i load up that writing project
[00:28:32.320 --> 00:28:37.600]   again i know what i'm doing i would do this a lot with mathematics papers as well right i would put notes
[00:28:37.600 --> 00:28:42.640]   right into the document where i was you know typing in my results or i would email these to my collaborators
[00:28:42.640 --> 00:28:47.280]   and be like okay well here's where i'm stuck but i think here are like three ways i was thinking might
[00:28:47.280 --> 00:28:51.440]   be useful for getting unstuck it's like a little narrative a little note so that when i get back
[00:28:51.440 --> 00:28:56.000]   to that work i'm like okay here here's where i was or here's where i was going to think so i do think
[00:28:56.000 --> 00:29:00.000]   that's useful but i usually just leave these notes right where i'm doing my work it's in the document
[00:29:00.000 --> 00:29:05.360]   where i'm writing it's in the in the document uh where i'm putting out my math equations and i think
[00:29:05.360 --> 00:29:10.160]   that's useful so you know where to start again and it's easier to get into it the thing that makes me more
[00:29:10.160 --> 00:29:15.920]   nervous is i this idea of this long ongoing interaction i i don't actually think that type
[00:29:15.920 --> 00:29:22.480]   of cybernet collaboration in most cases is a useful new form of knowledge work i think it's just escaping
[00:29:22.480 --> 00:29:27.600]   strain it's it's a it's a fancier version of like i'm looking at my phone a lot it sort of makes the
[00:29:27.600 --> 00:29:33.760]   work feel less less hard but hard is sometimes what you need all right who do we got next up is mark
[00:29:33.760 --> 00:29:39.600]   when using active recall to study a large volume of material how soon and how often should i revisit
[00:29:39.600 --> 00:29:44.000]   material i've already mastered right now i find that when i return to something a few weeks after
[00:29:44.000 --> 00:29:48.160]   learning it i've forgotten many of the details well i mean it depends what you're trying to do here
[00:29:48.160 --> 00:29:52.240]   like what are you learning and why are you trying to hold on to it if you learn something that you
[00:29:52.240 --> 00:29:57.680]   then use it will stay sort of active in your memory and it'll eventually will really dig in
[00:29:57.680 --> 00:30:01.920]   if you're memorizing some sort of information that you just basically don't come back to again
[00:30:01.920 --> 00:30:06.000]   you're going to need to retouch on that within a two-week window like if you let about two
[00:30:06.000 --> 00:30:09.040]   weeks or more go your mileage will vary depending on the information in your brain
[00:30:09.040 --> 00:30:14.480]   brain but if you let more than two weeks go without using information at all uh it's going
[00:30:14.480 --> 00:30:19.360]   to start to lose its location that is just like informal based on my experience as a as a student
[00:30:19.360 --> 00:30:24.000]   professor so if it's like this is arbitrary information i want to remember every week or so
[00:30:24.000 --> 00:30:28.560]   you want to keep going back and doing that active recall eventually if you do active recall enough it
[00:30:28.560 --> 00:30:33.680]   will uh lock in but again be sure that you need to if you're not using this stuff be sure you need to
[00:30:33.680 --> 00:30:40.640]   remember it like the best way to submit something in your memory is to use it to do it i i you know
[00:30:40.640 --> 00:30:46.000]   you teach the concept a couple times you really remember it you explain it to someone else because
[00:30:46.000 --> 00:30:50.320]   you're using it for a project you know you really remember it active recall of course simulates that
[00:30:50.320 --> 00:30:54.240]   by recalling the information from scratch you are firing up all the circuits you would use if you were
[00:30:54.240 --> 00:30:59.440]   actually applying the the technique so it's using your own brain's own memory apparatus but yeah i would
[00:30:59.440 --> 00:31:04.800]   find if i memorize something using active recall and just never touched it again within about two
[00:31:04.800 --> 00:31:10.480]   weeks i might start to forget it so i don't two weeks ish but what are you trying to remember and
[00:31:10.480 --> 00:31:15.600]   why and why aren't you using it so maybe it doesn't have to be all right who do we got next up is
[00:31:15.600 --> 00:31:21.040]   beth i've seen articles ranging from ai is terrible for the environment to studies suggesting
[00:31:21.040 --> 00:31:25.360]   its carbon emissions can actually be lower than traditional methods can you help put this in
[00:31:25.360 --> 00:31:31.200]   perspective for example by comparing the emissions from one person's chat pt gpt use to something more
[00:31:31.200 --> 00:31:35.760]   familiar like a plane trip and by clarifying whether ai's footprint is unique to platforms
[00:31:35.760 --> 00:31:41.360]   like chat gpt or similar to energy use and everyday tools like google search all right i mean i do have
[00:31:41.360 --> 00:31:45.520]   a hot take here um if you want like the actual answers i don't know i don't have the numbers in
[00:31:45.520 --> 00:31:51.680]   front of me but something like a google search is sufficiently uh significantly more energy efficient
[00:31:52.560 --> 00:31:58.800]   than like a chat gpt query i mean a chat gpt query you have to you have to have this frontier model
[00:31:58.800 --> 00:32:03.120]   loaded up in memory now this thing could have hundreds of billions if not a trillion parameters
[00:32:03.120 --> 00:32:07.520]   defining it right so you're not going to fit into the memory of a single gpu chip you're gonna have
[00:32:07.520 --> 00:32:12.480]   to shard this thing over like three four five maybe six gpus that are then going to have to be
[00:32:12.480 --> 00:32:18.320]   running all out just to generate tokens for your answers i compare this to a google search where they
[00:32:18.320 --> 00:32:24.080]   use commodity intel chips most of this stuff is cached they can be dynamic of like oh there's a
[00:32:24.080 --> 00:32:28.400]   little downtime on this chip hey can you like go look something up in a hyper efficient sort of cached
[00:32:28.400 --> 00:32:34.000]   you know search index and the the amount of computation they've got it's down you know minuscule
[00:32:34.000 --> 00:32:38.000]   probably the answer you're looking for is in some cdn server that's like 10 miles away
[00:32:38.000 --> 00:32:41.200]   so no it's a lot of computational power but here's my hot take
[00:32:42.320 --> 00:32:49.280]   i might i might get yelled at for this one jesse you ready for it yeah i think right now the focus
[00:32:49.280 --> 00:32:58.240]   on the environmental impact of ai is in part a way for people who are critical of technology or big tech
[00:32:58.240 --> 00:33:04.080]   to get in on like i don't like ai in a territory that they're much more comfortable with than the
[00:33:04.080 --> 00:33:10.320]   technology so if i'm just like i don't know like a typical like left-leaning critique of the tech industry
[00:33:10.320 --> 00:33:14.560]   and ai i'm super comfortable for talking environmental stuff like you don't care about
[00:33:14.560 --> 00:33:19.920]   the environment i do i saw the al gore documentary you don't know science the environment's important
[00:33:19.920 --> 00:33:24.960]   like i feel like i'm smart there when it's me talking to other people like that's where i'm
[00:33:24.960 --> 00:33:29.360]   smart like i know more about this than you you're you don't know science global warming's a real thing
[00:33:29.360 --> 00:33:36.320]   so that's like a very secure place of critique a less secure place of critique is you know the type of
[00:33:36.320 --> 00:33:40.000]   stuff i've been doing in my writing recently like coming in and be like here i don't think that
[00:33:40.000 --> 00:33:45.360]   reinforcement based learning post training refinements is giving you the sort of uh lost
[00:33:45.360 --> 00:33:49.440]   reductions that you would have expected on a more elongated power law curve like people getting into
[00:33:49.440 --> 00:33:54.080]   the technical details is very like uh it's hard and you're like i don't really know this details well
[00:33:54.080 --> 00:33:59.760]   but i i don't like this and i don't like the people involved and sam altman's kind of creepy
[00:33:59.760 --> 00:34:04.080]   and mark zuckerberg still talks like a robot whose emotion circuit board is shorted out we've covered this on
[00:34:04.080 --> 00:34:09.760]   the show and this is a territory that i'm much more comfortable in um just like also really leaning
[00:34:09.760 --> 00:34:16.240]   into like the specific ai safety concerns over the particular words that a particular chatbot will or
[00:34:16.240 --> 00:34:23.840]   will not say like well i'm comfortable there this is inappropriate speech but i think if we're really
[00:34:23.840 --> 00:34:28.640]   thinking about ai these are kind of these are kind of transient concerns because i don't think a future
[00:34:28.640 --> 00:34:34.320]   of these massive frontier models being queried for everything is a future that makes any economic sense
[00:34:34.320 --> 00:34:40.960]   anyways here's the right way to think about it if querying one of these models is bad for the environment
[00:34:40.960 --> 00:34:47.120]   that means the amount of computation it's using can't possibly be profitable
[00:34:47.120 --> 00:34:54.160]   so they can't be bad for the environment too long because it correlates also with expense and people
[00:34:54.160 --> 00:34:59.680]   aren't willing to pay for the massive computational expense i mean stuff it takes money to create heat
[00:34:59.680 --> 00:35:03.280]   because that requires electricity electricity costs money so like my thought about this is
[00:35:03.280 --> 00:35:10.400]   yes i would be concerned if we we had chat gpt used all the time but i just don't think it's profitable
[00:35:10.400 --> 00:35:16.240]   i think the future of ai is going to be it has to be systems that have much smaller models
[00:35:16.240 --> 00:35:22.160]   machine native models if possible meaning like this thing is running on my ipad not in the cloud somewhere
[00:35:22.160 --> 00:35:27.600]   on a bunch of gpus small specialized language models combined with other specialized components
[00:35:27.600 --> 00:35:32.880]   like policy networks for evaluating options future simulators for trying to understand what actions to
[00:35:32.880 --> 00:35:38.560]   take next control logic that's specific to the task at hand put together into a program that you can get
[00:35:38.560 --> 00:35:43.840]   your arms around that can run on existing hardware and does really good at being intelligent about a very
[00:35:43.840 --> 00:35:48.400]   specific thing that i'm convinced is the future of ai it doesn't have an environmental footprint that's
[00:35:48.400 --> 00:35:52.800]   different than you know other things that we're doing right now with our computer so there probably
[00:35:52.800 --> 00:35:58.560]   is it does use too much electricity but i just don't think these massive frontier models are going to
[00:35:58.560 --> 00:36:05.440]   it just doesn't make sense i cannot be querying a trillion parameter model that requires six h100 gpus
[00:36:05.440 --> 00:36:11.040]   to even produce a token for me for everything i'm doing in my life that would probably be an environmental
[00:36:11.040 --> 00:36:15.840]   catastrophe but we're not that's not sustainable i think frontier models like f1 cards it's a way of showing
[00:36:15.840 --> 00:36:19.760]   off your technology and proving that your company has the best technology but it's not the car we're
[00:36:19.760 --> 00:36:25.040]   buying on the ford lot you know a year from now that's gonna be a much simpler car but we'll buy
[00:36:25.040 --> 00:36:29.280]   it from the company who had the best f1 car because that convinced us so i don't know that's right so
[00:36:29.280 --> 00:36:35.200]   that's my hot take is of all the issues we have with ai the focus on environment right now it's
[00:36:35.200 --> 00:36:38.320]   we're not in some steady state yet i just think it makes people more comfortable because that's
[00:36:38.320 --> 00:36:43.840]   territory where they're familiar being the sort of higher level of the hierarchy and the argument
[00:36:43.840 --> 00:36:47.120]   and they're uncomfortable getting the tech because like let's be honest my people are nerds and are
[00:36:47.120 --> 00:36:52.240]   kind of uncomfortable to be around and we don't talk very normal and we know too much about algorithmic
[00:36:52.240 --> 00:36:55.520]   nonsense or whatever so i don't know that's what i think is going on so i'm not as worried about the
[00:36:55.520 --> 00:37:01.040]   environment because i don't think this industry can survive in a mode that is super bad for the
[00:37:01.040 --> 00:37:03.920]   environment it just costs too much money people aren't going to pay a thousand dollars a month
[00:37:04.880 --> 00:37:10.160]   to pay for all their queries for things i think smaller models can work did you know for example
[00:37:10.160 --> 00:37:17.040]   the model uh no one brown's model pluribus which was the first ai system to beat actual tournament
[00:37:17.040 --> 00:37:22.240]   players in texas hold on poker the original way they built it was with like very large neural networks
[00:37:22.240 --> 00:37:27.120]   and they used a supercomputer system a center at new pittsburgh to train it it was like okay and then
[00:37:27.120 --> 00:37:32.400]   they figured out like oh we could have a couple different networks and what matters is like the logic
[00:37:32.400 --> 00:37:37.520]   of how we connect them together and the logic of our ai and the system they built pluribus it can beat
[00:37:37.520 --> 00:37:42.080]   the professional players you can run on the laptop because it didn't have to it wasn't just like we
[00:37:42.080 --> 00:37:48.080]   have 100 billion parameter network that we're just like learn poker like no like we're we have a future
[00:37:48.080 --> 00:37:53.200]   simulator that simulates the possible mind states and cards of the other players then we have a smaller
[00:37:53.200 --> 00:37:57.520]   neural network train just to like understand different poker positions and say what would be good or bad
[00:37:57.520 --> 00:38:03.200]   and then we have some logic that we symbolic logic we hand programmed in about like okay let's calculate
[00:38:03.200 --> 00:38:08.000]   the value the expected value if we do this and this was the case here's what would happen let's give
[00:38:08.000 --> 00:38:12.720]   that a number what about this it's just like straightforward mix of neural network unsupervised
[00:38:12.720 --> 00:38:17.600]   learning with like symbolic old-fashioned hard-coded ai and the whole thing fits on a laptop and it can
[00:38:17.600 --> 00:38:25.040]   beat professional poker players so hopefully the future of ai i think is most most likely the environmental
[00:38:25.040 --> 00:38:30.720]   concerns are not going to be um i think substantially different than sort of the environmental footprint
[00:38:30.720 --> 00:38:40.080]   of the computing we have right now but we'll see all right what do we got next next up is david if you
[00:38:40.080 --> 00:38:45.360]   left academia would you spend your days as a writer and podcaster would you enjoy this or become antsy
[00:38:46.800 --> 00:38:53.040]   um yeah i would continue to write and i would continue to podcast i don't think i would become
[00:38:53.040 --> 00:39:00.240]   antsy so i've kind of two reactions to this no i want to become antsy uh writing and podcasting actually
[00:39:00.240 --> 00:39:05.440]   takes up a lot of time and it's not like i'm hurting for other things to do you know how many teams i'm
[00:39:05.440 --> 00:39:13.440]   coaching and my kids like schools and sports leagues right now let me guess three k six it's three i'm
[00:39:13.440 --> 00:39:18.080]   coaching three teams right now there's a lot of time doing though i have a lot of things to do that's
[00:39:18.080 --> 00:39:23.040]   fine but here's the here's the here's the bigger point though is writing and podcasting is like a
[00:39:23.040 --> 00:39:28.240]   lot of what i do as an academic right now so i i don't know how different it would be so you know for
[00:39:28.240 --> 00:39:33.200]   those who know my trajectory uh i've been a writer my entire adult life right i started writing in college
[00:39:33.200 --> 00:39:39.840]   i trained as a theoretical computer scientist focusing on the theory of distributed systems so i studied at
[00:39:39.840 --> 00:39:43.040]   nancy lynch's theory of distributed systems group at mit and did my postdoc
[00:39:43.040 --> 00:39:47.600]   under harry balakrishnan and his network and mobile systems group so i had a computer science
[00:39:47.600 --> 00:39:52.960]   specialty that was on the math behind distributed systems went to georgetown this is where like
[00:39:52.960 --> 00:39:57.840]   my nsf funding was all about this my papers were all about this my grad students we did a bunch of i
[00:39:57.840 --> 00:40:01.760]   was pretty good distributed theoretician published a bunch of papers around a bunch of steering committees
[00:40:01.760 --> 00:40:07.280]   somewhere around the time i was you know i got tenure then i got full professor around the time
[00:40:07.280 --> 00:40:14.160]   i was getting full professor georgetown my where i work they began making a real move for like we want
[00:40:14.160 --> 00:40:19.520]   to be one of the places grappling with technology and its impacts they call the field digital ethics like
[00:40:19.520 --> 00:40:24.160]   we want to be at the core of this because we're in washington dc we have a big ethics background here
[00:40:24.160 --> 00:40:29.360]   we were the university that like figured out bioethics in the 20th century the kennedy center for ethics
[00:40:29.360 --> 00:40:35.120]   we want to do the same thing for tech ethics we're in the we're here in dc we have all these policy
[00:40:35.120 --> 00:40:39.120]   centers we have one of the biggest tech law faculties that are law school like it makes a lot of sense
[00:40:39.120 --> 00:40:43.520]   and i was like this is kind of what i'm doing already with my writing i write about technology
[00:40:43.520 --> 00:40:49.040]   and how it impacts us i want it and so like i've really been focusing on that so i was one of the
[00:40:49.040 --> 00:40:54.400]   founding faculty members of georgetown center for digital ethics under inaugural director of our computer
[00:40:54.400 --> 00:40:59.760]   science ethics and society academic program it's the first major in the country to integrate computer
[00:40:59.760 --> 00:41:04.400]   science and ethics in a combined major there's places where there's majors where like you're
[00:41:04.400 --> 00:41:09.920]   a computer science major and you throw on a little bit of ethics first integrated one uh in the country
[00:41:09.920 --> 00:41:16.480]   it's like this is actually what i i'm doing largely right now as an academic is technology how it impacts
[00:41:16.480 --> 00:41:20.560]   us what to do about it and i do it in a lot of different forms and i think i think public outreach is
[00:41:20.560 --> 00:41:26.000]   important i think the podcast is very pragmatic but i reach a lot of people this way my writing for the
[00:41:26.000 --> 00:41:30.800]   new yorker now we're talking to a little bit more of a of like a rarefied cloud crowd this gets me in
[00:41:30.800 --> 00:41:36.320]   front of policy makers this gets me in front of like the senate and and on npr or whatever um but
[00:41:36.320 --> 00:41:40.560]   it's a way again to work on tech and its impact i do some academic papers on tech and its impact which
[00:41:40.560 --> 00:41:46.960]   is more for like an academic crowd and then my books fall somewhere in between so i don't really
[00:41:46.960 --> 00:41:52.720]   know it wouldn't my life wouldn't be that much different if i left academia to write and podcast writing
[00:41:52.720 --> 00:41:56.480]   podcasting is what i'm doing in academia the main difference would be there'd be less time around
[00:41:56.480 --> 00:42:01.840]   really smart people and less time around students both of which i like also teaching but teaching
[00:42:01.840 --> 00:42:05.520]   increasingly i can teach things that like helps me think about these thoughts anyway so you know
[00:42:05.520 --> 00:42:10.000]   no i want to be antsy and also i don't know how different it would be do you still do any math
[00:42:10.000 --> 00:42:16.480]   i haven't very recently i haven't yeah been a year or so so do you still teach undergrads like
[00:42:16.480 --> 00:42:21.600]   traditional computer science classes i do a mix of traditional computer science and stuff that's relevant for
[00:42:21.600 --> 00:42:27.200]   uh the computer science and ethics program yeah and i teach less than i did before too because i'm
[00:42:27.200 --> 00:42:33.920]   doing running these things or whatever so that's a good question though um i think we have a case study
[00:42:33.920 --> 00:42:40.160]   yeah before we do the case study if folks have calls then just go to the deep life dot com slash
[00:42:40.160 --> 00:42:45.280]   listen and submit some updated calls is that deep life or the deep life the deep life the deep life dot com
[00:42:45.280 --> 00:42:50.160]   slash listen there's a link yep and you can record the call right there from the browser yeah do that
[00:42:50.160 --> 00:42:56.080]   we have a lot of calls but they're getting kind of old right yeah you got a good shot a good pithy call
[00:42:56.080 --> 00:43:01.600]   on a topic i've been talking a lot recently you got a good shot now so i would go i would go record the
[00:43:01.600 --> 00:43:06.720]   calls uh we do have a case study however i love case studies this is kind of a long one but it's
[00:43:06.720 --> 00:43:11.680]   great case study on my theories of lifestyle centric planning i'm thinking so much about this because
[00:43:11.680 --> 00:43:17.120]   of my new books i'm happy to have this we have what theme music for this right yep do we i don't
[00:43:17.120 --> 00:43:22.000]   remember is it just to introduce the segment or do we play it the whole time i don't know the two
[00:43:22.000 --> 00:43:26.720]   episodes we did one but then last week you wanted to hear both all right play it now and see your mood
[00:43:26.720 --> 00:43:28.320]   at the end all right let's see what we let's hear it
[00:43:37.520 --> 00:43:43.200]   i just want to do a deep thoughts with jack handy all right so today's case study comes from vin
[00:43:43.200 --> 00:43:48.960]   i hold a bachelor's degree in interface design and in my 20s first worked as a web developer and later
[00:43:48.960 --> 00:43:54.000]   as a ux designer for several digital agencies i enjoyed my work but i felt stuck in big cities
[00:43:54.000 --> 00:44:02.240]   and kept dreaming about a life closer to nature so in my mid-20s i did something that you would surely
[00:44:02.240 --> 00:44:09.760]   advise against oh i threw my career into the trash and moved to norway to live and work on an organic
[00:44:09.760 --> 00:44:14.880]   farm i had a great time there and enjoyed every aspect of it however i didn't have a sustainable
[00:44:14.880 --> 00:44:20.640]   long-term plan i realized i needed and wanted to move back to my home country where my girlfriend now
[00:44:20.640 --> 00:44:25.760]   wife lived back home i decided that my next big move would be to find a job that allowed me to work in
[00:44:25.760 --> 00:44:31.440]   nature so i signed up for a nature guide program and actually managed to land a job at a as a guide
[00:44:31.440 --> 00:44:36.720]   at the very place where i had completed that course for the last six years i worked there running
[00:44:36.720 --> 00:44:44.400]   workshops and similar programs sounds like a happy ending right not quite life changed i became a father
[00:44:44.400 --> 00:44:49.440]   of two children and we moved closer to my parents so we could get support from the grandparents with family
[00:44:49.440 --> 00:44:54.480]   life this meant i was now living almost two hours away from my workplace constantly torn between family and
[00:44:54.480 --> 00:44:59.840]   professional life on top of that since most workshops take place on weekends i was away from my wife and
[00:44:59.840 --> 00:45:04.400]   our two little ones at the very time they needed me most it turned out that the dream of working in
[00:45:04.400 --> 00:45:12.320]   nature definitely had its flaws i'm going to stop there for a second let's take stock of where we are
[00:45:12.320 --> 00:45:18.160]   in the story so far we're seeing a common issue when people think about the deep life
[00:45:19.040 --> 00:45:27.200]   so a common issue that people have is that you fixate on a single change that you begin to believe
[00:45:27.200 --> 00:45:33.200]   will make everything better it's simpler to think about a single change we can sort of inhabit that
[00:45:33.200 --> 00:45:39.840]   change the idea of doing something radical itself makes us feel excited in anticipation and we we can
[00:45:39.840 --> 00:45:45.440]   sort of take the imagined feeling of like the best parts of that change and sort of expand that in our
[00:45:45.440 --> 00:45:50.800]   mind like my life is going to be better so savind did this with nature he's like i like nature so i'm
[00:45:50.800 --> 00:45:55.680]   just going to focus like a laser on if i could just be in nature all the time i would be really happy
[00:45:55.680 --> 00:46:00.640]   the problem as we often say when we talk about lifestyle centric planning is that your daily
[00:46:00.640 --> 00:46:06.000]   subjective mood is not the result of a single decision or change but on all of the relevant
[00:46:06.000 --> 00:46:11.600]   aspects of your life call this your lifestyle all the different things that are relevant during the day add up
[00:46:12.800 --> 00:46:18.000]   to give you your subjective experience of the day so you might like the part where i'm giving
[00:46:18.000 --> 00:46:21.520]   a workshop and it's in nature and that's nice but you have other parts of your life too like the fact
[00:46:21.520 --> 00:46:26.400]   that you had to drive two hours to get there and that you're kind of stressed out about what's going
[00:46:26.400 --> 00:46:32.320]   on with your your in your grand your parents and like the kids are you're not around enough to help
[00:46:32.320 --> 00:46:36.960]   and it's creating tension with your wife all of your houses maybe not it's nowhere near nature and
[00:46:36.960 --> 00:46:40.480]   you're actually spending most of your time in a car all of these other aspects of your lifestyle
[00:46:40.480 --> 00:46:44.880]   matter too and this is how you can make one radical change that you're excited about and yet end up
[00:46:44.880 --> 00:46:50.960]   less happy than you were before because by making this thing better you accidentally maybe made other
[00:46:50.960 --> 00:46:56.240]   things worse so lifestyle centric planning says you got to construct your ideal vision for an entire
[00:46:56.240 --> 00:47:01.120]   lifestyle all the parts of your life how would i reorient my life so that all of the parts were
[00:47:01.120 --> 00:47:04.960]   something that resonated and then how do i make progress towards that you're going to have you're going
[00:47:04.960 --> 00:47:10.080]   to enjoy your life more if you explicitly construct it so that all of the things that contribute to your
[00:47:10.080 --> 00:47:15.440]   enjoyment are are being pushed in ways that are better let's return this finn because he recognized
[00:47:15.440 --> 00:47:21.120]   that and let's see how he made some changes around this time i came across your books deep work so good
[00:47:21.120 --> 00:47:25.840]   they can't ignore you and slow productivity they gave me a framework to better understand my professional
[00:47:25.840 --> 00:47:31.680]   situation i had to take a hard look at myself and realize that for the last five to six years i hadn't
[00:47:31.680 --> 00:47:37.120]   built transferable career capital that could help me in a different role after some reflection i decided
[00:47:37.120 --> 00:47:42.160]   to revive my previously built career capital as a web developer however i hadn't been coding seriously
[00:47:42.160 --> 00:47:46.720]   for years and looking at job descriptions i realized how much had changed in the web development space
[00:47:46.720 --> 00:47:52.000]   and i had a lot of catching up to do all right so he's doing something here called evidence-based
[00:47:52.000 --> 00:47:58.000]   planning like so he makes this decision he's looking at his life and it's like actually i need a job
[00:47:58.000 --> 00:48:03.600]   that um it's going to whatever the criteria here but probably like more money more autonomy i don't
[00:48:03.600 --> 00:48:08.880]   have to commute so much um whatever web development is like i have career capital there let me start
[00:48:08.880 --> 00:48:12.720]   there but he does evidence-based planning he actually looked at real job listings like oh skill a
[00:48:12.720 --> 00:48:18.640]   b and c i don't have so i'm gonna i'm gonna summarize a little bit here but basically he uses like
[00:48:18.640 --> 00:48:23.440]   techniques from deep work and slow productivity as well as from scott young's excellent book ultra learning
[00:48:24.400 --> 00:48:30.240]   to begin a education process of learning the specific skills that his evidence-based planning
[00:48:30.240 --> 00:48:35.360]   said were important not what he wants to be true but like i need to know a b and c these employers
[00:48:35.360 --> 00:48:43.200]   want exactly those skills and how do i get there as effectively as possible i'll jump back to regular
[00:48:43.200 --> 00:48:48.080]   speed here thanks to your techniques and tools it took me about a year to get out of my difficult job
[00:48:48.080 --> 00:48:53.600]   situation and back into my old career the side effect i now earn more than twice as much as before
[00:48:53.600 --> 00:48:58.320]   my next move won't be as bold as the ones i made in the past i want to become a valuable asset for my
[00:48:58.320 --> 00:49:03.040]   company to get so good they can't ignore me and then use my career capital to reduce my working hours
[00:49:03.040 --> 00:49:09.040]   so i can have fridays off to spend in nature without having to work there so see what happened there
[00:49:09.040 --> 00:49:14.080]   is when he did lifestyle-centric planning he's like i like nature but by reorienting my life around my
[00:49:14.080 --> 00:49:18.560]   career is takes place in nature a lot of other things got worse and actually being in nature is
[00:49:18.560 --> 00:49:23.360]   not as great as you think when you're also like working and you realize like oh if i could take my
[00:49:23.360 --> 00:49:30.640]   programming expertise make it up to date make myself really valuable i can go back to programming
[00:49:30.640 --> 00:49:37.840]   get good enough that i can cut my hours still make more than my old salary only work three or four days
[00:49:37.840 --> 00:49:42.400]   a week and then spend time in nature very intentionally but now without the commute now being home with my
[00:49:42.400 --> 00:49:49.040]   family now things being more stable um man that's a better life so lifestyle-centric planning worked
[00:49:49.040 --> 00:49:53.600]   out better than just putting laying back like what's a radical thing i can do i'll move to norway i'll get
[00:49:53.600 --> 00:49:58.480]   a job as like a nature guide lifestyle-centric planning was not as exciting but it led them to a better place
[00:49:58.480 --> 00:50:05.920]   all right so we got coming up my investigative journalism beware i do a little bit of data reporting but
[00:50:05.920 --> 00:50:11.840]   first before we get there what you've really been waiting for to hear about another one of our sponsors
[00:50:11.840 --> 00:50:18.320]   so here's the thing if you run a small business you know there's nothing small about it every day there
[00:50:18.320 --> 00:50:24.800]   is a new decision to make and even the smallest decisions can feel massive jesse knows this i get
[00:50:24.800 --> 00:50:30.320]   upset whenever anyone refers to our business here as small i know how hard it is to run this just the
[00:50:30.320 --> 00:50:36.080]   other day for example the nice woman who runs the gift shop down the on the next block here in tacoma
[00:50:36.080 --> 00:50:41.440]   park asked me how life as a small business owner was going so naturally jesse and i rolled her car
[00:50:41.440 --> 00:50:45.360]   into a swamp that's where i can put up with that nonsense so here's what i'm trying to say it's hard
[00:50:45.360 --> 00:50:49.120]   to run a small business so when you can find decisions that are no-brainers you should take them
[00:50:49.120 --> 00:50:56.560]   and when it comes to selling thing using shopify is exactly one of those no brainers shopify's point
[00:50:56.560 --> 00:51:01.360]   of sale system is a unified command center for your retail business it brings together in-store and
[00:51:01.360 --> 00:51:07.360]   online operations across up to a thousand locations it has very impressive features like endless aisle
[00:51:07.360 --> 00:51:14.240]   ship to customer and buy online pickup buy online pickup in store and with shopify pos you can get
[00:51:14.240 --> 00:51:19.680]   personalized experiences to help shoppers keep coming back and they will come back based on
[00:51:19.680 --> 00:51:26.000]   report from ey businesses on shopify pos see real results like 22 better total cost of ownership
[00:51:26.000 --> 00:51:33.600]   and benefits equivalent to an 8.9 uplift in sales on average relevant to the market set survey so get
[00:51:33.600 --> 00:51:38.320]   all the big stuff for your small business right with shopify sign up for your one dollar per month
[00:51:38.320 --> 00:51:46.640]   trial and start selling today at shopify.com slash deep go to shopify.com slash deep shopify.com
[00:51:46.640 --> 00:51:54.560]   slash deep i also want to talk about our friends at my body tutor i've known adam gilbert my body tutors
[00:51:54.560 --> 00:52:01.200]   founder for many years and he is my go-to guy for fitness advice his company my body tutor is 100
[00:52:01.200 --> 00:52:07.440]   online coaching program that solves the biggest problem in health and fitness which is lack of consistency
[00:52:07.440 --> 00:52:12.000]   they do this by simplifying the process into practical sustainable behaviors and then giving
[00:52:12.000 --> 00:52:17.040]   you daily accountability and support what you need to actually keep taking action on your plan
[00:52:17.040 --> 00:52:21.520]   you have an app you check in every day like hey here's what i ate here's how i did my exercise
[00:52:21.520 --> 00:52:27.040]   and your coach checks it hey how'd it go if you have a question like you know i'm really struggling
[00:52:27.040 --> 00:52:31.440]   with x like great let's fix this right away knowing there's someone checking on what you do every
[00:52:31.440 --> 00:52:36.560]   day and is there to help you if anything's not working that's how you get consistency and getting
[00:52:36.560 --> 00:52:42.720]   healthier it's not the information it's the consistency and having this online coach it is
[00:52:42.720 --> 00:52:46.560]   a really really good way to get healthy all right so if you're like i want to get in better shape
[00:52:46.560 --> 00:52:54.080]   this is the way to do it go to my body tutor.com mention you heard about it on the deep questions
[00:52:54.080 --> 00:53:00.720]   podcast when you sign up and they will give you 50 off your first month that's my body tutor t-u-t-o-r.com
[00:53:00.720 --> 00:53:06.000]   and mention deep questions all right jesse let's move on to our final segment
[00:53:06.000 --> 00:53:12.800]   so i want to talk about an interesting article about technology and students
[00:53:12.800 --> 00:53:16.080]   and i want to look a little bit deeper at it and have a bigger point to make
[00:53:16.080 --> 00:53:21.440]   this article actually appeared recently in the washington post it came to my attention however
[00:53:21.440 --> 00:53:26.560]   when tyler cowen wrote about it on his excellent blog the marginal revolution i'll pull it up here
[00:53:26.560 --> 00:53:31.120]   on the screen the blog post because the post the washington post is behind a paywall but marginal
[00:53:31.120 --> 00:53:38.320]   revolution is not all right so here is tyler's setup just summarizing this article he says until
[00:53:38.320 --> 00:53:46.080]   recently a nearby radio telescope meant the local school could not use wi-fi and how did that go all
[00:53:46.080 --> 00:53:50.480]   right so this article that he's citing here is about green green bank west virginia have you heard
[00:53:50.480 --> 00:53:56.560]   about this place jesse no it comes up a lot in technology circles because it's rural west virginia
[00:53:56.560 --> 00:54:05.360]   and the world's largest steerable radio telescope is there oh really yeah and there's a town next to it
[00:54:05.360 --> 00:54:13.200]   and radio interference could mess with the telescope so there's no cell phones and there's no wi-fi
[00:54:14.000 --> 00:54:19.360]   so it's like a town that's kind of like off the grid because of this uh this telescope that's in
[00:54:19.360 --> 00:54:25.120]   the middle of the town so what this washington post article that tyler cowen is citing what it was about
[00:54:25.120 --> 00:54:30.560]   it was actually an op-ed it was written by someone who has a book he's working on about green bank what
[00:54:30.560 --> 00:54:37.200]   it was arguing is the fact that there is no wi-fi in the town has been hurting the students in the
[00:54:37.200 --> 00:54:41.360]   school there green bank has an elementary middle school combined school about 200 something students
[00:54:42.080 --> 00:54:48.800]   he's arguing without wi-fi they can't use like the modern chromebook online curriculums do online
[00:54:48.800 --> 00:54:56.960]   testing and this is hurting the students so let me read here this is a tyler cowen citing the washington
[00:54:56.960 --> 00:55:02.000]   post so what i'm about to read here is from the washington post article while the rest of the
[00:55:02.000 --> 00:55:09.360]   country rushed to bring tech in the classrooms green bank remained stuck in 1999 without wi-fi the school's 200
[00:55:09.360 --> 00:55:14.160]   students couldn't use chromebooks or digital textbooks or do research online teachers couldn't
[00:55:14.160 --> 00:55:20.160]   access individualized education programs online or use google docs for staff meetings even routine tasks
[00:55:20.160 --> 00:55:24.480]   such as state mandated standardized testing became challenging with students rotating through a small
[00:55:24.480 --> 00:55:30.400]   hardware computer lab where they took the exams some of the teachers say this has been a problem
[00:55:30.400 --> 00:55:36.000]   here's a quote here the ability to individualize learning with an ipad or a laptop that's basically
[00:55:36.000 --> 00:55:42.720]   impossible that was teacher darla huddle here's another teacher being quoted in the op-ed without
[00:55:42.720 --> 00:55:47.200]   the online component of our curriculum fully working it's really detrimental to our instruction
[00:55:48.080 --> 00:55:55.360]   that was sarah brown so this is the issue here they can't do this sort of modern ed tech stuff
[00:55:55.360 --> 00:56:03.040]   and the argument of this article is like that's held this school back and how do we know this well
[00:56:03.040 --> 00:56:09.600]   there isn't data on this in the article itself like specific data but um here's what the author says
[00:56:09.600 --> 00:56:13.600]   while these discussions dragged on so discussions about like is there some way we can get like low
[00:56:13.600 --> 00:56:17.760]   powered wi-fi or some other ways to get wi-fi in the school while these discussions dragged on
[00:56:17.760 --> 00:56:24.400]   students fell further behind in math and reading with greenbrink consistently posting the lowest test scores
[00:56:24.400 --> 00:56:32.720]   in the county all right this is an interesting argument here that without modern ed tech
[00:56:32.720 --> 00:56:37.840]   students are falling behind and that's it's like a natural experiment and look this school has the lowest
[00:56:37.840 --> 00:56:42.400]   scores in the county now this is like a relevant time to be arguing this because there's so much
[00:56:42.400 --> 00:56:47.440]   discussion about phones in schools and technology in schools and it's mainly negative like technology
[00:56:47.440 --> 00:56:52.640]   in schools is often negative but here's this this op-ed in the waapo that's being contrarian and tyler
[00:56:52.640 --> 00:56:57.120]   who also is has a real affinity for technology is like yeah come on let's be careful about it
[00:56:57.120 --> 00:57:04.480]   maybe some of this technology is really helping so i want to look into this uh you know i'm not an
[00:57:04.480 --> 00:57:10.320]   expert data journalist but like this is interesting and i want to look a little bit closer so what did i find
[00:57:11.360 --> 00:57:18.400]   well first is it true that green bank consistently post the lowest test scores in the county this is
[00:57:18.400 --> 00:57:22.560]   turns out to be called pocahontas county it's kind of interesting we're like in west virginia okay
[00:57:22.560 --> 00:57:27.440]   so it's pocahontas county west virginia yes the green bank elementary middle school
[00:57:27.440 --> 00:57:35.520]   has the lowest scores in the county but there's a problem here it's a very small county i mean it's
[00:57:35.520 --> 00:57:40.240]   a lot of geographical area but you know this is not montgomery county here in like washington dc
[00:57:40.240 --> 00:57:45.600]   the the county you know there's the green bank middle school and elementary school the county has
[00:57:45.600 --> 00:57:51.120]   one other middle school and a shared high school and then there's two smaller elementary schools that
[00:57:51.120 --> 00:57:55.600]   are in like another town over there much smaller one of which which has the county's like gifted and
[00:57:55.600 --> 00:58:01.040]   talented program there's only 70 kids there half of them are gifted and talented kids like we're we're not
[00:58:01.040 --> 00:58:08.960]   talking to big county it's a handful of schools of which like two of them of the six are uh in green
[00:58:08.960 --> 00:58:13.680]   bank one is shared and then there's like one other middle school and two other elementary schools they
[00:58:13.680 --> 00:58:18.640]   do so yes like the the elementary school in green bank is has lower scores than those two other elementary
[00:58:18.640 --> 00:58:25.440]   school the middle school has lower scores um than the other middle school all right but so it's it's a
[00:58:25.440 --> 00:58:31.840]   small county but that doesn't necessarily tell us much like maybe green bank you know for whatever
[00:58:31.840 --> 00:58:37.600]   reasons that part of this the county just is worse off and they just get worse scores i don't know what's
[00:58:37.600 --> 00:58:44.240]   going on there like do we know this is from not having wi-fi or not well what we really would need to
[00:58:44.240 --> 00:58:51.840]   test this is we need time series data so we need to see test performance over time right because until
[00:58:51.840 --> 00:58:58.640]   somewhat recently it wouldn't matter if you had wi-fi or not right so ipads were introduced in 2010
[00:58:58.640 --> 00:59:04.320]   we see the rise of chromebooks and classrooms picks up in the 2010s that's really where this thing this
[00:59:04.320 --> 00:59:11.840]   happens so before the 2010s there would be a lot more technological parity between this particular
[00:59:11.840 --> 00:59:16.240]   school and the other school in the county right because no one was using technology like that in school
[00:59:16.240 --> 00:59:23.040]   so what we really need is like a time series that shows somewhere in the 2010s the wi-fi
[00:59:23.040 --> 00:59:30.400]   free school begins to uh separate that they the impact on not having wi-fi makes them worse because
[00:59:30.400 --> 00:59:34.080]   they might have always been worse we want to see them get worse all right so i went looking for this
[00:59:34.080 --> 00:59:40.240]   data it's hard to find time series data for the particular schools in pocahontas county but because
[00:59:40.240 --> 00:59:47.040]   pocahontas county is so small there's two middle schools one of them is with wi-fi one of them's not
[00:59:47.040 --> 00:59:51.120]   there's three elementary schools one with one without and the other two add up basically to the
[00:59:51.120 --> 00:59:56.720]   size of the one that didn't have wi-fi it's such a small county what we can do is compare pocahontas
[00:59:56.720 --> 01:00:02.400]   county to other counties in west virginia because presumably if like half the students in your county
[01:00:02.400 --> 01:00:08.560]   didn't have wi-fi and that affected them that should bring down the whole county around the time
[01:00:08.560 --> 01:00:16.880]   that why chromebooks etc became big in school so there is data i can get data on uh any any ap you
[01:00:16.880 --> 01:00:22.160]   know these are state mandated test scores i can get data on this county by county so let's see what's
[01:00:22.160 --> 01:00:27.600]   going on here so let me load up some charts this is from education education recovery scorecard which
[01:00:27.600 --> 01:00:32.080]   aggregates a lot of this data it's really interested in like what happened after covet but they have data that
[01:00:32.080 --> 01:00:37.280]   goes back to as far as 2009 so it's perfect for us what i'm loading on the screen here for people who
[01:00:37.280 --> 01:00:42.880]   are watching instead of just listening is math performance grades three to eight because again
[01:00:42.880 --> 01:00:46.240]   going past middle school it's a shared high school so like we don't learn much from that
[01:00:46.240 --> 01:00:56.720]   from 2009 to 2024. all right so as we see here going from 2009 we see an increase there's a bit of a
[01:00:56.720 --> 01:01:01.760]   gap in the data but we see roughly actually math scores were getting better go pocahontas county math
[01:01:01.760 --> 01:01:09.680]   scores were getting better from 2009 until about 2017. in 2017 oh this is interesting the score
[01:01:09.680 --> 01:01:14.480]   started going down they went down went down then we get the pandemic where we don't have data but they
[01:01:14.480 --> 01:01:19.360]   were falling multiple years before the pandemic and then we see after the pandemic we begin to get a
[01:01:19.360 --> 01:01:24.560]   recovery which we've seen nationwide as things fell so low during the pandemic that there's a recovery
[01:01:24.560 --> 01:01:28.560]   right after so that's the chart we're seeing if we look at reading scores it's something kind of
[01:01:28.560 --> 01:01:36.480]   similar here the peak is around 2015 and then it begins to go down after that as well the timing
[01:01:36.480 --> 01:01:42.240]   here is compatible with the wi-fi hypothesis right that kind of makes sense chromebooks and all these
[01:01:42.240 --> 01:01:47.600]   other things that require internet begin spreading in the 2010s it might make sense that where would you
[01:01:47.600 --> 01:01:52.480]   first start seeing the impact of not having this technology like midway through the 2010s as these
[01:01:52.480 --> 01:01:57.520]   things were gaining traction that might be where you start seeing performance go down so this data
[01:01:57.520 --> 01:02:03.360]   from within pocahontas county itself is compatible roughly speaking is compatible with the washington
[01:02:03.360 --> 01:02:10.640]   post hypothesis of without wi-fi things started to get worse but if we want to do a controlled experiment
[01:02:10.640 --> 01:02:17.360]   we have to compare this the similar counties so what we need to do because pocahontas is half
[01:02:17.920 --> 01:02:22.160]   the non-wi-fi and we can't break that out so let's find other counties in west virginia
[01:02:22.160 --> 01:02:29.920]   that are similar in terms of size and demographics and socioeconomic status like very similar other
[01:02:29.920 --> 01:02:35.840]   counties nearby in west virginia where there were no radio telescopes and everyone is allowed to use
[01:02:35.840 --> 01:02:44.160]   wi-fi what we would expect to find if the wi-fi hypothesis was correct is that pocahontas county
[01:02:44.160 --> 01:02:50.880]   should have a much more notable drop in performance starting in the mid-2010s than these other counties
[01:02:50.880 --> 01:02:57.360]   that didn't have any wi-fi restrictions we have that data that's what's interesting so let me scroll here
[01:02:57.360 --> 01:03:04.000]   a little bit all right so this is we're going back to math performance here we have three stack charts
[01:03:04.000 --> 01:03:08.720]   so if you're if you're listening i'll explain to you what we see each of these charts has a downwards
[01:03:08.720 --> 01:03:14.240]   arrow and an upwards arrow the downwards arrow which is purple i'll put this up here on the
[01:03:14.240 --> 01:03:20.640]   screen for poca pocahontas county schools first that is the decline in math scores during that period
[01:03:20.640 --> 01:03:29.360]   up to 2019 where um let me get the exact dates here uh 2019 to 2022 where we had the steepest sort
[01:03:29.360 --> 01:03:35.440]   of losses we saw in those curves before and then the green arrow is the improvements that they've seen
[01:03:35.440 --> 01:03:40.400]   since the pandemic so this chart is showing here for pocahontas county schools it's just quantifying
[01:03:40.400 --> 01:03:48.880]   what we saw on that chart in that downhill period from 2019 through 2022 there was a uh 0.6 negative
[01:03:48.880 --> 01:03:54.960]   0.6 drop in their performance compared to i think this is like state average so yeah they fell and then
[01:03:54.960 --> 01:04:00.720]   we see the green there's like this 0.36 uh increase so don't worry about the numbers but there was an
[01:04:00.720 --> 01:04:05.360]   increase uh this is the recovery after the pandemic all right so i'm just quantifying what we saw on
[01:04:05.360 --> 01:04:11.600]   that chart here's what's interesting though they did the same thing for all of west virginia counties
[01:04:11.600 --> 01:04:21.680]   that's the next one and for counties that had similar uh socioeconomic demographic and size so
[01:04:21.680 --> 01:04:25.680]   similar population so if you're wondering this is nicholas county hampshire county barber county
[01:04:25.680 --> 01:04:31.840]   tucker county and pendleton county here's what's interesting the similar west virginia districts
[01:04:31.840 --> 01:04:35.520]   that were the same in terms of demographics but differed mainly and they didn't have the wi-fi
[01:04:35.520 --> 01:04:44.560]   restrictions they saw a bigger drop from 2019 to 2022 and they saw a smaller recovery after the pandemic
[01:04:44.560 --> 01:04:49.760]   so the similar counties that had wi-fi never had that taken away
[01:04:50.960 --> 01:04:57.360]   did worse than the county in which like half the students are in this wi-fi free zone and in fact
[01:04:57.360 --> 01:05:02.800]   if you look at the west virginia as a whole that also was worse than what pocahontas county did
[01:05:02.800 --> 01:05:11.040]   so look i don't have school by school data i hear this a lot i found a lot of informal reporting online
[01:05:11.040 --> 01:05:16.000]   that did blame poor performance in green bank by them not being able to use internet in the school so
[01:05:16.000 --> 01:05:20.560]   that might be true and these teachers seem to think it's a problem but if we look at the data without
[01:05:20.560 --> 01:05:27.440]   having time series data from specific the the handful of schools within pocahontas county we do
[01:05:27.440 --> 01:05:34.560]   not see here a data story that looks at that looks like the lack of internet once chromebooks became a
[01:05:34.560 --> 01:05:39.520]   thing and these online curriculums became a thing began to make green bank much worse it seems like
[01:05:39.520 --> 01:05:43.760]   that's just a bad school those schools are bad and they've been bad and i don't know why you'd have to
[01:05:43.760 --> 01:05:48.400]   know about like you know it's just like what this town is i don't know what particularly is bad about
[01:05:48.400 --> 01:05:52.240]   that town it could be little things by the way because these numbers are so small not to get too
[01:05:52.240 --> 01:05:58.160]   much into the data but i was looking at proficiency on math and reading test scores broken down by the
[01:05:58.160 --> 01:06:01.840]   individual school so i can get those numbers for the most recent times they've measured them
[01:06:01.840 --> 01:06:08.800]   and they're like you know 50 better for example in hillsborough elementary versus the green bank
[01:06:08.800 --> 01:06:14.640]   elementary but hillsborough elementary in pocahontas county is the gifted and talented program
[01:06:14.640 --> 01:06:21.840]   there's only 70 kids in that school 70 kids total so all it takes is like oh we have 30 of our gifted
[01:06:21.840 --> 01:06:27.600]   and talented kids are there you're going to get 50 better you know number of people who are math
[01:06:27.600 --> 01:06:32.320]   proficient right so these are small numbers so we have to be careful um so yeah there's not a lot of
[01:06:32.320 --> 01:06:37.360]   schools there and the green bank one doesn't do well but we don't have evidence because that it is
[01:06:37.360 --> 01:06:43.920]   because they couldn't use internet connected chromebooks again the only way for this to be possible
[01:06:43.920 --> 01:06:49.120]   would be somehow the the lack of chromebooks and internet connected tech
[01:06:49.120 --> 01:06:55.760]   really caused the green bank schools to fall really hard but for unrelated reasons coincidentally the other
[01:06:55.760 --> 01:07:00.800]   two or three schools in pocahontas county did unusually well during this period and they sort of
[01:07:00.800 --> 01:07:06.640]   offset the fall that the green bank was having and that's why that county even though other counties
[01:07:06.640 --> 01:07:10.800]   with the same demographics as pocahontas county fell farther that there was something special happening
[01:07:10.800 --> 01:07:15.040]   with the non-green bank schools of pocahontas county where they offset the green bank losses
[01:07:15.040 --> 01:07:18.480]   maybe that's true but we would have to hear a plausible reason why that's true and actually see
[01:07:18.480 --> 01:07:23.040]   school-by-school data so i don't know the author of that op-ed is writing a book on green bank and he
[01:07:23.040 --> 01:07:28.080]   might have really good data but knowing what we know now i think this is a nice cautionary tale
[01:07:28.080 --> 01:07:33.840]   and it's a good reminder for myself or anyone else who talks about technology trends when there's an
[01:07:33.840 --> 01:07:40.560]   answer that we like it's often easy to jump to it find any point of data that seems to imply that and
[01:07:40.560 --> 01:07:45.760]   expand it beyond what the data says i think that seems to be what was happening here there's a nice
[01:07:45.760 --> 01:07:52.720]   subtle leap from the schools without wi-fi are worse to the schools without wi-fi are worse
[01:07:52.720 --> 01:07:59.200]   because they don't have wi-fi it's an easy leap to make but the picture gets much more murkier once
[01:07:59.200 --> 01:08:03.120]   you pull even a little bit on the data story so anyways i guess this is just a back to school note
[01:08:03.120 --> 01:08:09.680]   of we all have to be careful and take with grains of salt claims that are made that sound intuitive and
[01:08:09.680 --> 01:08:16.640]   there's a little bit of data to support doesn't necessarily mean it's true like it's like when it
[01:08:16.640 --> 01:08:22.640]   comes to cell phones in schools i have read exhaustively not just the research but the
[01:08:22.640 --> 01:08:26.240]   debates that researchers are having about the research and the complaints and then how the
[01:08:26.240 --> 01:08:30.080]   complaints are answered i i you know there's an hour-long talk i give about the evolution of the
[01:08:30.080 --> 01:08:34.880]   research literature on harms from phones for kids i feel like i know that data very well and it makes me
[01:08:34.880 --> 01:08:40.480]   confident to say these are um often a problem the benefits aren't worth it you shouldn't have phones
[01:08:40.480 --> 01:08:46.080]   before high school if you're a kid right that's really different than like hey that school struggles
[01:08:46.080 --> 01:08:50.560]   and they don't have wi-fi internet's good so anyways there we go i'm not investigative journalist data
[01:08:50.560 --> 01:08:55.680]   and who knows maybe this this professor has the school by school data but i think the picture in
[01:08:55.680 --> 01:09:00.960]   west virginia pocahontas county is more complicated than if only i could synchronize my eureka math
[01:09:00.960 --> 01:09:06.800]   curriculum with like online resources our kids would suddenly be much better harder reality all
[01:09:06.800 --> 01:09:10.560]   right speaking of hard realities that's all the time we have for today so thank you for listening
[01:09:10.560 --> 01:09:16.560]   we'll be back next week with another episode and until then as always stay deep hey so if you like
[01:09:16.560 --> 01:09:21.840]   today's discussion about ai and productivity you should see uh check out my recent episode episode
[01:09:21.840 --> 01:09:28.640]   367 where i get into the whole past and future trajectory of ai it's called what if ai doesn't get
[01:09:28.640 --> 01:09:33.120]   much better than this that is a sort of must listen to understand where i'm coming from on ai so if
[01:09:33.120 --> 01:09:39.360]   you haven't heard that episode yet go check it out i'm cal newport and this is deep questions
[01:09:39.360 --> 01:09:44.400]   today's episode what if ai doesn't get much better than this

