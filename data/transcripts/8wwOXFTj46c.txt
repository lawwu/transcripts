
[00:00:00.000 --> 00:00:03.920]   So I wanted to take a little bit of time to talk about rates and biases.
[00:00:03.920 --> 00:00:08.200]   And I put together this page specifically for the use case of Kaggle.
[00:00:08.200 --> 00:00:10.840]   Cause I feel really passionately about Kaggle.
[00:00:10.840 --> 00:00:16.440]   And Isaac, I know you're also into Kaggling and Mani who's in the chat is
[00:00:16.440 --> 00:00:22.000]   also super into Kaggle competitions, but I wanted to use this as a jumping off
[00:00:22.000 --> 00:00:24.800]   point to talk about what we do at rates and biases.
[00:00:25.080 --> 00:00:31.240]   So our whole goal is to help you track your experiments so you don't have to
[00:00:31.240 --> 00:00:31.920]   repeat them.
[00:00:31.920 --> 00:00:37.480]   And you can be smart about finding the most optimal model faster.
[00:00:37.480 --> 00:00:42.880]   So one of the things that you can do with rates and biases is be able to iterate on
[00:00:42.880 --> 00:00:45.280]   your model architectures really quickly.
[00:00:45.280 --> 00:00:49.440]   And you're able to track any kind of model with rates and biases.
[00:00:49.440 --> 00:00:53.800]   So for example, here you can see you can use 1db log and you don't even have to use
[00:00:53.800 --> 00:00:57.640]   it, but then the context of a model, you can use it in the context of a for loop
[00:00:57.640 --> 00:01:04.320]   and anything that increases over a time step, you can log with rates and biases.
[00:01:04.320 --> 00:01:10.560]   We also have callbacks for extra boost, light GBM, scikit, and these are all one
[00:01:10.560 --> 00:01:11.600]   line callbacks.
[00:01:11.600 --> 00:01:15.200]   So you're able to track any model's performance using rates and biases.
[00:01:15.200 --> 00:01:20.440]   If you're working with Keras, you can just add a little callback and it'll log all
[00:01:20.440 --> 00:01:21.840]   of your metrics for you.
[00:01:22.560 --> 00:01:27.880]   We also allow you to do hyperparameter sweeps, which I will get to in just a
[00:01:27.880 --> 00:01:28.400]   second.
[00:01:28.400 --> 00:01:33.640]   Also, if you're working with PyTorch, we also support PyTorch.
[00:01:33.640 --> 00:01:35.000]   There's a link in here.
[00:01:35.000 --> 00:01:40.080]   I will drop all of these links into the chat so you can look at them.
[00:01:40.080 --> 00:01:44.120]   The second thing you can do is you can debug the performance of your model in
[00:01:44.120 --> 00:01:44.960]   real time.
[00:01:44.960 --> 00:01:50.160]   And you can actually log really cool things like metrics you saw before, but you can
[00:01:50.160 --> 00:01:51.920]   also log plot A charts.
[00:01:52.160 --> 00:01:53.120]   You can log images.
[00:01:53.120 --> 00:01:58.720]   So if you're training a semantic segmentation model, you can log the images,
[00:01:58.720 --> 00:01:59.920]   the predictions of your models.
[00:01:59.920 --> 00:02:02.720]   If you're doing reinforcement learning, you can log videos.
[00:02:02.720 --> 00:02:06.960]   If you're working with audio, we have support for that.
[00:02:06.960 --> 00:02:08.880]   3D objects is really cool.
[00:02:08.880 --> 00:02:12.120]   We've been working with a lot of self-driving car companies.
[00:02:12.120 --> 00:02:18.200]   So we built a little integration to allow you to visualize point clouds and be able
[00:02:18.200 --> 00:02:20.320]   to draw bounding boxes around them.
[00:02:20.560 --> 00:02:24.080]   So if you're working with any self-driving car data, you can use this.
[00:02:24.080 --> 00:02:29.080]   And again, all of this is just one line of code, 1db.log, and you pass in the thing
[00:02:29.080 --> 00:02:29.960]   that you want to log.
[00:02:29.960 --> 00:02:34.120]   You can log HTML and there's a bunch of other things.
[00:02:34.120 --> 00:02:37.920]   There's also something that we just launched.
[00:02:37.920 --> 00:02:41.760]   I'm going to try to find it really fast.
[00:02:41.760 --> 00:02:42.680]   I think it's this one.
[00:02:42.680 --> 00:02:47.800]   So my colleague Nick built this really sick logger for molecules.
[00:02:48.080 --> 00:02:54.440]   So if you give it any molecule data, we can actually show you the structure of these
[00:02:54.440 --> 00:02:54.960]   molecules.
[00:02:54.960 --> 00:02:58.760]   This is what Jonathan used in his talk.
[00:02:58.760 --> 00:03:01.280]   And I will link Jonathan's report.
[00:03:01.280 --> 00:03:04.560]   I'll drop it in the chat after this.
[00:03:04.560 --> 00:03:08.680]   So you can also do hyperparameter optimization.
[00:03:08.680 --> 00:03:10.520]   And I want to move through this really fast.
[00:03:10.520 --> 00:03:11.880]   I'll just show you two things.
[00:03:11.880 --> 00:03:17.560]   You can make parameter importance plots that show you which hyperparameters were the
[00:03:17.560 --> 00:03:18.200]   most important.
[00:03:18.200 --> 00:03:22.720]   So then you can hone in on a set of hyperparameters and a set of values that were
[00:03:22.720 --> 00:03:24.200]   the most important for your model.
[00:03:24.200 --> 00:03:29.400]   You can map all of these in this little parallel coordinates plot, which allow you
[00:03:29.400 --> 00:03:37.600]   to see what hyperparameter values are actually the most effective for the metric that
[00:03:37.600 --> 00:03:38.440]   you want.
[00:03:38.440 --> 00:03:43.920]   And then we also let you save models and reproduce them.
[00:03:44.160 --> 00:03:48.000]   And it's just one line of code you pass in your model, the file that your model
[00:03:48.000 --> 00:03:51.280]   waits, or a file with the entire model architecture.
[00:03:51.280 --> 00:03:54.800]   And then you can restore it with 1db.restore.
[00:03:54.800 --> 00:04:03.080]   And then finally, we just built this scikit model integration, which you can use to
[00:04:03.080 --> 00:04:08.160]   plot learning curves, ROC plots, precision recall curves, all of the most important
[00:04:08.160 --> 00:04:10.720]   plots that you would use every day.
[00:04:12.560 --> 00:04:15.360]   I also want to talk about reports really fast.
[00:04:15.360 --> 00:04:20.760]   So reports are, I think Jonathan's showed his report.
[00:04:20.760 --> 00:04:23.520]   Basically, these are like read-me's, but better.
[00:04:23.520 --> 00:04:29.560]   You can actually see your model performance and also the predictions that your model is
[00:04:29.560 --> 00:04:34.720]   making. So here you can see my colleague Stacy's amazing report on semantic
[00:04:34.720 --> 00:04:36.800]   segmentation for self-driving cars.
[00:04:36.800 --> 00:04:39.560]   And so the reports you can add markdown.
[00:04:40.240 --> 00:04:44.520]   And also, like I said, the model predictions and then all of these plots.
[00:04:44.520 --> 00:04:50.240]   And you can use these to stay on the same page with your team, or you can use this to
[00:04:50.240 --> 00:04:57.720]   be able to share the insights that you've had about your model with your
[00:04:57.720 --> 00:05:02.160]   stakeholders, with your boss, with your team, and you can be on the same page.
[00:05:02.160 --> 00:05:07.160]   We've been putting out some really cool reports that I would love for you guys to
[00:05:07.160 --> 00:05:09.800]   check out. We did one on debugging neural networks.
[00:05:10.520 --> 00:05:12.880]   This we did one on distributed training.
[00:05:12.880 --> 00:05:18.520]   And then if you're working, if you're trying to do something with COVID, we have a
[00:05:18.520 --> 00:05:24.600]   template for how you can approach COVID research and how you can run hyperparameter
[00:05:24.600 --> 00:05:29.760]   sweeps and visualize your model performance.
[00:05:29.760 --> 00:05:33.760]   Finally, I would like to talk about two more things really fast.
[00:05:33.760 --> 00:05:37.360]   One is we are looking for authors.
[00:05:37.360 --> 00:05:40.920]   So if you would like if you're working on any cool machine learning projects and you
[00:05:40.920 --> 00:05:46.920]   would like to write for us or you would like us to feature them, we would love to hear
[00:05:46.920 --> 00:05:50.640]   from you. I'll put my email address in the chat.
[00:05:50.640 --> 00:05:56.120]   And if you send me some writing examples, I would love to feature your work on our site.
[00:05:56.120 --> 00:06:02.000]   And then finally, we just launched our podcast and it's now on iTunes.
[00:06:02.680 --> 00:06:07.160]   So this is Lucas interviewing people who are doing really cool things with machine
[00:06:07.160 --> 00:06:08.120]   learning in production.
[00:06:08.120 --> 00:06:12.680]   So our first episode was with Brandon, who does machine learning at iRobot.
[00:06:12.680 --> 00:06:19.560]   And they go into how you would use machine learning to for robots that are in
[00:06:19.560 --> 00:06:25.800]   production. We also did one with Nicholas and we talked about how you do self-driving,
[00:06:25.800 --> 00:06:28.840]   how you put models into production for self-driving cars.

