
[00:00:00.000 --> 00:00:03.640]   Tesla AI Day presented the most amazing real-world AI
[00:00:03.640 --> 00:00:07.760]   and engineering effort I have ever seen in my life.
[00:00:07.760 --> 00:00:10.200]   I wrote this and I meant it.
[00:00:10.200 --> 00:00:12.060]   Why was it amazing to me?
[00:00:12.060 --> 00:00:15.160]   No, not primarily because of the Tesla bot.
[00:00:15.160 --> 00:00:18.540]   It was amazing because I believe the autonomous driving task
[00:00:18.540 --> 00:00:20.800]   and the general real-world robotics perception
[00:00:20.800 --> 00:00:23.040]   and planning task is a lot harder
[00:00:23.040 --> 00:00:24.760]   than people generally think.
[00:00:24.760 --> 00:00:27.680]   And I also believed the scale of effort
[00:00:27.680 --> 00:00:30.480]   in algorithm data, annotation simulation,
[00:00:30.480 --> 00:00:33.160]   inference compute and training compute required
[00:00:33.160 --> 00:00:36.400]   to solve these problems is something no one
[00:00:36.400 --> 00:00:38.520]   would be able to do in the near term.
[00:00:38.520 --> 00:00:42.160]   Yesterday was the first time I saw in one place
[00:00:42.160 --> 00:00:44.820]   just the kind and the scale of effort
[00:00:44.820 --> 00:00:47.180]   that has a chance to solve this,
[00:00:47.180 --> 00:00:48.840]   the autonomous driving problem
[00:00:48.840 --> 00:00:51.440]   and the general real-world robotics perception
[00:00:51.440 --> 00:00:52.760]   and planning problem.
[00:00:52.760 --> 00:00:56.340]   This includes the neural network architecture and pipeline,
[00:00:56.340 --> 00:00:58.840]   the autopilot compute hardware in the car,
[00:00:58.840 --> 00:01:01.080]   dojo compute hardware for training,
[00:01:01.080 --> 00:01:03.040]   the data and the annotation,
[00:01:03.040 --> 00:01:05.400]   the simulation for rare edge cases,
[00:01:05.400 --> 00:01:09.860]   and yes, the generalized application of all of the above
[00:01:09.860 --> 00:01:13.920]   beyond the car robot to the humanoid form.
[00:01:13.920 --> 00:01:15.920]   Let's go through the big innovations.
[00:01:15.920 --> 00:01:18.440]   The neural network.
[00:01:18.440 --> 00:01:20.360]   Each of these is a difficult
[00:01:20.360 --> 00:01:22.840]   and I would say brilliant design idea
[00:01:22.840 --> 00:01:25.240]   that is either a step or a leap forward
[00:01:25.240 --> 00:01:27.580]   from the state of the art in machine learning.
[00:01:27.580 --> 00:01:31.520]   First is to predict the vector space, not in image space.
[00:01:31.520 --> 00:01:33.740]   This alone is a big leap beyond
[00:01:33.740 --> 00:01:35.720]   what is usually done in computer vision
[00:01:35.720 --> 00:01:38.000]   that usually operates in the image space,
[00:01:38.000 --> 00:01:40.220]   in the two-dimensional image.
[00:01:40.220 --> 00:01:43.420]   The thing about reality is that it happens out there
[00:01:43.420 --> 00:01:44.980]   in the three-dimensional world
[00:01:44.980 --> 00:01:46.460]   and it doesn't make sense to be doing
[00:01:46.460 --> 00:01:49.020]   all the machine learning on the 2D projections
[00:01:49.020 --> 00:01:50.260]   onto images.
[00:01:50.260 --> 00:01:53.020]   Like many good ideas, this is an obvious one,
[00:01:53.020 --> 00:01:55.060]   but a very difficult one.
[00:01:55.060 --> 00:01:57.900]   Second is the fusion of camera sensor data
[00:01:57.900 --> 00:01:59.660]   before the detections.
[00:01:59.660 --> 00:02:02.420]   The detections performed by the different heads
[00:02:02.420 --> 00:02:04.700]   of the multitask neural network.
[00:02:04.700 --> 00:02:08.460]   For now, the fusion is at the multiscale feature level.
[00:02:08.460 --> 00:02:11.140]   Again, in retrospect, an obvious
[00:02:11.140 --> 00:02:13.120]   but a very difficult engineering step
[00:02:13.120 --> 00:02:16.300]   of doing the detection and the machine learning
[00:02:16.300 --> 00:02:19.340]   on all of the sensors combined
[00:02:19.340 --> 00:02:21.020]   as opposed to doing them individually
[00:02:21.020 --> 00:02:23.740]   and combining only the decisions.
[00:02:23.740 --> 00:02:25.860]   Third is using video context to model
[00:02:25.860 --> 00:02:28.120]   not just vector space, but time.
[00:02:28.120 --> 00:02:31.780]   At each frame, concatenating positional encodings,
[00:02:31.780 --> 00:02:34.540]   multicam features, and ego kinematics.
[00:02:34.540 --> 00:02:36.580]   Using a pretty cool spatial
[00:02:36.580 --> 00:02:39.020]   recurrent neural network architecture
[00:02:39.020 --> 00:02:41.180]   that forms a 2D grid around the car
[00:02:41.180 --> 00:02:43.940]   where each cell of the grid is a RNN,
[00:02:43.940 --> 00:02:45.400]   recurrent neural network.
[00:02:45.400 --> 00:02:47.020]   The other cool aspect of this
[00:02:47.020 --> 00:02:49.120]   is that you can then build a map
[00:02:49.120 --> 00:02:51.740]   in the space of RNN features.
[00:02:51.740 --> 00:02:54.540]   And then perhaps do planning in that space,
[00:02:54.540 --> 00:02:56.660]   which is a fascinating concept.
[00:02:56.660 --> 00:02:59.020]   Andrei Karpathy, I think also mentioned
[00:02:59.020 --> 00:03:00.580]   some future improvements,
[00:03:00.580 --> 00:03:03.260]   performing the fusion earlier and earlier
[00:03:03.260 --> 00:03:04.500]   in the neural network.
[00:03:04.500 --> 00:03:06.500]   So currently the fusion of space and time
[00:03:06.500 --> 00:03:08.140]   are late in the network.
[00:03:08.140 --> 00:03:10.540]   Moving the fusion earlier on
[00:03:10.540 --> 00:03:15.380]   takes us further toward full end-to-end driving
[00:03:15.380 --> 00:03:16.820]   with multiple modalities.
[00:03:16.820 --> 00:03:19.460]   Seamlessly fusing, integrating
[00:03:19.460 --> 00:03:21.740]   the multiple sources of sensory data.
[00:03:21.740 --> 00:03:24.060]   Finally, the place where there's currently,
[00:03:24.060 --> 00:03:25.940]   from my understanding, the least amount
[00:03:25.940 --> 00:03:29.780]   of utilization of neural networks is planning.
[00:03:29.780 --> 00:03:33.100]   So obviously optimal planning in action space
[00:03:33.100 --> 00:03:35.020]   is intractable, so that you have to come up
[00:03:35.020 --> 00:03:36.620]   with a bunch of heuristics.
[00:03:36.620 --> 00:03:38.900]   You can do those manually,
[00:03:38.900 --> 00:03:40.620]   or you could do those through learning.
[00:03:40.620 --> 00:03:41.900]   So the idea that was presented
[00:03:41.900 --> 00:03:44.660]   is to use neural networks as heuristics.
[00:03:44.660 --> 00:03:46.580]   In a similar way that neural networks
[00:03:46.580 --> 00:03:48.220]   were used as heuristics
[00:03:48.220 --> 00:03:49.780]   in the Monte Carlo tree search
[00:03:49.780 --> 00:03:51.620]   for mu zero and alpha zero
[00:03:51.620 --> 00:03:53.220]   to play different games,
[00:03:53.220 --> 00:03:54.980]   to play Go, to play chess.
[00:03:54.980 --> 00:03:56.660]   This allows you to significantly prune
[00:03:56.660 --> 00:03:59.420]   the search through action space
[00:03:59.420 --> 00:04:01.140]   for a plan that doesn't get stuck
[00:04:01.140 --> 00:04:02.980]   in the local optima and gets pretty close
[00:04:02.980 --> 00:04:04.340]   to the global optima.
[00:04:04.340 --> 00:04:06.700]   I really appreciated that the presentation
[00:04:06.700 --> 00:04:09.140]   didn't dumb anything down,
[00:04:09.140 --> 00:04:10.860]   but maybe in all the technical details,
[00:04:10.860 --> 00:04:12.540]   it was easy to miss just how much
[00:04:12.540 --> 00:04:14.980]   brilliant innovation that was here.
[00:04:14.980 --> 00:04:17.420]   The move to predicting in vector space
[00:04:17.420 --> 00:04:18.580]   is truly brilliant.
[00:04:18.580 --> 00:04:19.740]   Of course, you can only do that
[00:04:19.740 --> 00:04:20.700]   if you have the data
[00:04:20.700 --> 00:04:22.340]   and you have the annotation for it.
[00:04:22.340 --> 00:04:24.660]   But just to take that step
[00:04:24.660 --> 00:04:27.020]   is already taking a step outside the box
[00:04:27.020 --> 00:04:28.340]   of the way things are currently done
[00:04:28.340 --> 00:04:29.660]   in computer vision.
[00:04:29.660 --> 00:04:33.340]   Then fusing seamlessly across
[00:04:33.340 --> 00:04:35.620]   many camera sensors,
[00:04:35.620 --> 00:04:37.300]   incorporating timing to the whole thing
[00:04:37.300 --> 00:04:38.660]   in a way that's differentiable
[00:04:38.660 --> 00:04:40.740]   with these spatial RNNs.
[00:04:40.740 --> 00:04:42.500]   And then of course, using that beautiful
[00:04:42.500 --> 00:04:44.220]   mess of features,
[00:04:44.220 --> 00:04:47.060]   both on the individual image side
[00:04:47.060 --> 00:04:50.420]   and the RNN side to make plans
[00:04:50.420 --> 00:04:51.940]   using neural network architecture
[00:04:51.940 --> 00:04:53.380]   for as a heuristic.
[00:04:53.380 --> 00:04:55.700]   I mean, all of that is just brilliant.
[00:04:55.700 --> 00:04:57.820]   The other critical part of making all of this work
[00:04:57.820 --> 00:04:59.900]   is the data and the data annotation.
[00:04:59.900 --> 00:05:01.300]   First is the manual labeling.
[00:05:01.300 --> 00:05:03.100]   So to make the neural networks
[00:05:03.100 --> 00:05:04.860]   that predict in vector space work,
[00:05:04.860 --> 00:05:06.540]   you have to label in vector space.
[00:05:06.540 --> 00:05:08.020]   So you have to create in-house tools.
[00:05:08.020 --> 00:05:09.420]   And as it turns out,
[00:05:09.420 --> 00:05:12.100]   Tesla hired in-house team of annotators
[00:05:12.100 --> 00:05:14.700]   to use those tools to then perform
[00:05:14.700 --> 00:05:16.100]   the labeling of vector space
[00:05:16.100 --> 00:05:18.460]   and then project it out into the image space.
[00:05:18.460 --> 00:05:20.060]   First of all, that saves a lot of work.
[00:05:20.060 --> 00:05:21.020]   And second of all,
[00:05:21.020 --> 00:05:23.340]   that means you're directly performing
[00:05:23.340 --> 00:05:24.820]   the annotation in the space
[00:05:24.820 --> 00:05:26.380]   in which you're doing the prediction.
[00:05:26.380 --> 00:05:28.140]   Obviously, as was always the case,
[00:05:28.140 --> 00:05:30.300]   as is the case with self-supervised learning,
[00:05:30.300 --> 00:05:33.140]   auto labeling is the key to this whole thing.
[00:05:33.140 --> 00:05:34.980]   One of the interesting thing that was presented
[00:05:34.980 --> 00:05:37.100]   is the use of clips of data
[00:05:37.100 --> 00:05:40.500]   that includes video, IMU, GPS, odometry, and so on
[00:05:40.500 --> 00:05:43.660]   from multiple vehicles at the same location in time
[00:05:43.660 --> 00:05:46.500]   to generate labels of both the static world
[00:05:46.500 --> 00:05:49.980]   and the moving objects and their kinematics.
[00:05:49.980 --> 00:05:50.820]   That's really cool.
[00:05:50.820 --> 00:05:52.940]   You have these little clips,
[00:05:52.940 --> 00:05:55.860]   these buckets of data from different vehicles,
[00:05:55.860 --> 00:05:57.900]   and they're kind of annotating each other.
[00:05:57.900 --> 00:05:59.380]   You're registering them together
[00:05:59.380 --> 00:06:02.660]   to then combine a solid annotation
[00:06:02.660 --> 00:06:06.300]   of that particular part of road at that particular time.
[00:06:06.300 --> 00:06:08.500]   That's amazing because the more the fleet grows,
[00:06:08.500 --> 00:06:12.300]   the stronger that kind of auto labeling becomes.
[00:06:12.300 --> 00:06:14.700]   And the more edge cases you're able to catch that way.
[00:06:14.700 --> 00:06:15.820]   Speaking of edge cases,
[00:06:15.820 --> 00:06:19.040]   that's what Tesla is using simulation for,
[00:06:19.040 --> 00:06:20.580]   is to simulate rare edge cases
[00:06:20.580 --> 00:06:22.460]   that are not going to appear often in the data,
[00:06:22.460 --> 00:06:25.780]   even when that data set grows incredibly large.
[00:06:25.780 --> 00:06:27.880]   And also, they're using it for annotation
[00:06:27.880 --> 00:06:30.900]   of ultra complex scenes where accurate labeling
[00:06:30.900 --> 00:06:32.940]   of real world data is basically impossible,
[00:06:32.940 --> 00:06:35.860]   like a scene with like a hundred pedestrians,
[00:06:35.860 --> 00:06:38.180]   which I think is the example they used.
[00:06:38.180 --> 00:06:39.600]   So I honestly think the innovations
[00:06:39.600 --> 00:06:41.020]   on the neural network architecture
[00:06:41.020 --> 00:06:44.340]   and the data annotation is really just a big leap.
[00:06:44.340 --> 00:06:46.220]   Then there's the continued innovation
[00:06:46.220 --> 00:06:48.100]   on the autopilot computer side,
[00:06:48.100 --> 00:06:51.180]   the neural network compiler that optimizes latency,
[00:06:51.180 --> 00:06:52.380]   and so on.
[00:06:52.380 --> 00:06:56.460]   There's, I think I remember really nice testing
[00:06:56.460 --> 00:06:59.980]   and debugging tools for like variants
[00:06:59.980 --> 00:07:02.180]   of candidate trained neural networks
[00:07:02.180 --> 00:07:03.380]   to be deployed in the future,
[00:07:03.380 --> 00:07:05.760]   where you can compare different neural networks together.
[00:07:05.760 --> 00:07:07.860]   That's almost like developer tools
[00:07:07.860 --> 00:07:11.140]   for to be deployed neural networks.
[00:07:11.140 --> 00:07:13.900]   And it was mentioned that almost 10,000 GPUs
[00:07:13.900 --> 00:07:18.180]   are currently being used to continually retrain the network.
[00:07:18.180 --> 00:07:19.420]   I forget what the number was,
[00:07:19.420 --> 00:07:21.860]   but I think every week or every two weeks,
[00:07:21.860 --> 00:07:25.180]   the network is fully retrained end to end.
[00:07:25.180 --> 00:07:26.900]   The other really big innovation,
[00:07:26.900 --> 00:07:30.020]   but unlike the neural network and the data annotation,
[00:07:30.020 --> 00:07:32.660]   this is in the future, so to be deployed still,
[00:07:32.660 --> 00:07:34.020]   it's still under development,
[00:07:34.020 --> 00:07:37.940]   is the Dojo computer, which is used for training.
[00:07:37.940 --> 00:07:40.820]   So the autopilot computer is the computer on the car
[00:07:40.820 --> 00:07:41.940]   that is doing the inference,
[00:07:41.940 --> 00:07:43.740]   and Dojo computer is the thing
[00:07:43.740 --> 00:07:45.260]   that you would have in a data center
[00:07:45.260 --> 00:07:47.900]   that performs the training of the neural network.
[00:07:47.900 --> 00:07:51.300]   There's a, what they're calling a single training tile
[00:07:51.300 --> 00:07:53.820]   that is nine flops.
[00:07:53.820 --> 00:07:57.220]   It's made up of D1 chips that are built in house by Tesla.
[00:07:57.220 --> 00:07:59.440]   Each chip with super fast IO,
[00:07:59.440 --> 00:08:02.780]   each tile also with super fast IO.
[00:08:02.780 --> 00:08:04.060]   So you can basically connect
[00:08:04.060 --> 00:08:06.220]   an arbitrary number of these together,
[00:08:06.220 --> 00:08:08.620]   each with a power supply and cooling.
[00:08:08.620 --> 00:08:12.860]   And then I think they connected like a million nodes
[00:08:12.860 --> 00:08:14.740]   to have a compute center.
[00:08:14.740 --> 00:08:17.860]   I forget what the name is, but it's 1.1 exaflop.
[00:08:17.860 --> 00:08:19.620]   So combined with the fact
[00:08:19.620 --> 00:08:22.140]   that this can arbitrarily scale,
[00:08:22.140 --> 00:08:24.300]   I think this is basically contending
[00:08:24.300 --> 00:08:26.340]   to be the world's most powerful
[00:08:26.340 --> 00:08:28.180]   neural network training computer.
[00:08:28.180 --> 00:08:30.820]   Again, the entire picture that was presented
[00:08:30.820 --> 00:08:35.420]   on AI day is amazing because the, what would you call it?
[00:08:35.420 --> 00:08:38.940]   The Tesla AI machine can improve arbitrarily
[00:08:38.940 --> 00:08:40.940]   through the iterative data engine process
[00:08:40.940 --> 00:08:44.440]   of auto labeling plus manual labeling of edge cases.
[00:08:44.440 --> 00:08:46.340]   So like that labeling stage,
[00:08:46.340 --> 00:08:49.540]   plus the data collection, retraining, deploying.
[00:08:49.540 --> 00:08:52.540]   And then again, you go back to the data collection,
[00:08:52.540 --> 00:08:56.000]   the labeling, retraining, and deploying.
[00:08:56.000 --> 00:08:59.980]   And you can go through this loop as many times as you want
[00:08:59.980 --> 00:09:02.900]   to arbitrarily improve the performance of the network.
[00:09:02.900 --> 00:09:05.260]   I still think nobody knows how difficult
[00:09:05.260 --> 00:09:08.300]   the autonomous driving problem is,
[00:09:08.300 --> 00:09:11.780]   but I also think this loop does not have a ceiling.
[00:09:11.780 --> 00:09:14.560]   I still think there's a big place for driver sensing.
[00:09:14.560 --> 00:09:15.880]   I still think you have to solve
[00:09:15.880 --> 00:09:17.800]   the human robot interaction problem
[00:09:17.800 --> 00:09:19.860]   to make the experience more pleasant,
[00:09:19.860 --> 00:09:24.060]   but damn it, this loop of manual and auto labeling
[00:09:24.060 --> 00:09:25.860]   that leads to retraining, that leads to deployment,
[00:09:25.860 --> 00:09:28.020]   it goes back to the data collection
[00:09:28.020 --> 00:09:29.860]   and the auto labeling and the manual labeling
[00:09:29.860 --> 00:09:31.420]   is incredible.
[00:09:31.420 --> 00:09:33.860]   Second reason this whole effort is amazing
[00:09:33.860 --> 00:09:36.140]   is that Dojo can essentially become
[00:09:36.140 --> 00:09:38.900]   an AI training as a service,
[00:09:38.900 --> 00:09:41.860]   directly taken on AWS and Google Cloud.
[00:09:41.860 --> 00:09:44.300]   So there's no reason it needs to be utilized
[00:09:44.300 --> 00:09:47.060]   specifically for the autopilot computer.
[00:09:47.060 --> 00:09:48.720]   The simplicity of the way they described
[00:09:48.720 --> 00:09:50.780]   the deployment of PyTorch across these nodes,
[00:09:50.780 --> 00:09:52.620]   you can basically use it for any kind
[00:09:52.620 --> 00:09:53.940]   of machine learning problem,
[00:09:53.940 --> 00:09:55.940]   especially one that requires scale.
[00:09:55.940 --> 00:09:58.360]   Finally, the third reason all of this was amazing
[00:09:58.360 --> 00:10:00.020]   is that the neural network architecture
[00:10:00.020 --> 00:10:02.220]   and data engine pipeline is applicable
[00:10:02.220 --> 00:10:05.020]   to much more than just roads and driving.
[00:10:05.020 --> 00:10:07.620]   It can be used in the home, in the factory,
[00:10:07.620 --> 00:10:09.740]   and by robots of basically any form,
[00:10:09.740 --> 00:10:11.860]   as long as it has cameras and actuators,
[00:10:11.860 --> 00:10:15.040]   including, yes, the humanoid form.
[00:10:15.040 --> 00:10:17.180]   As someone who loves robotics,
[00:10:17.180 --> 00:10:19.980]   the presentation of a humanoid Tesla bot
[00:10:19.980 --> 00:10:21.980]   was truly exciting.
[00:10:21.980 --> 00:10:23.220]   Of course, for me personally,
[00:10:23.220 --> 00:10:26.900]   the lifelong dream has been to build the mind,
[00:10:26.900 --> 00:10:30.500]   the robot that becomes a friend and a companion to humans,
[00:10:30.500 --> 00:10:35.500]   not just a servant that performs boring and dangerous tasks.
[00:10:35.500 --> 00:10:38.120]   But to me, these two problems should,
[00:10:38.120 --> 00:10:41.100]   and I think will be solved in parallel.
[00:10:41.100 --> 00:10:43.200]   The Tesla bot, if successful,
[00:10:43.200 --> 00:10:45.040]   just might solve the latter problem
[00:10:45.040 --> 00:10:48.460]   of perception, movement, and object manipulation.
[00:10:48.460 --> 00:10:51.200]   And I hope to play a small part
[00:10:51.200 --> 00:10:55.400]   in solving the former problem of human-robot interaction,
[00:10:55.400 --> 00:10:57.440]   and yes, friendship.
[00:10:57.440 --> 00:11:01.320]   I'm not going to mention love when talking about robots.
[00:11:01.320 --> 00:11:03.480]   Either way, all of this, to me,
[00:11:03.480 --> 00:11:06.180]   paints a picture of an exciting future.
[00:11:06.180 --> 00:11:07.360]   Thanks for watching.
[00:11:07.360 --> 00:11:09.040]   Hope to see you next time.
[00:11:09.040 --> 00:11:11.620]   (upbeat music)
[00:11:11.620 --> 00:11:14.200]   (upbeat music)
[00:11:14.200 --> 00:11:16.780]   (upbeat music)
[00:11:16.780 --> 00:11:19.360]   (upbeat music)
[00:11:19.360 --> 00:11:21.940]   (upbeat music)
[00:11:21.940 --> 00:11:24.520]   (upbeat music)
[00:11:24.520 --> 00:11:34.520]   [BLANK_AUDIO]

