
[00:00:00.000 --> 00:00:13.840]   Oh, and if Kayla, if you could put the link for the live stream, that would be great.
[00:00:13.840 --> 00:00:15.840]   >> Will do.
[00:00:15.840 --> 00:00:17.840]   >> All right.
[00:00:17.840 --> 00:00:25.520]   Oh, and a quick note just for the presenters.
[00:00:25.520 --> 00:00:31.560]   So when you might not see that many folks in the Zoom, but there will be a good number
[00:00:31.560 --> 00:00:35.080]   of folks on YouTube and a lot of people watch it immediately afterwards.
[00:00:35.080 --> 00:00:39.760]   So even though there might be only 20, 30 people in the Zoom, it's a couple hundred
[00:00:39.760 --> 00:00:49.040]   people end up catching the stream and the posted videos.
[00:00:49.040 --> 00:01:01.760]   Okay.
[00:01:01.760 --> 00:01:06.800]   So we are live now.
[00:01:06.800 --> 00:01:13.720]   And let me welcome everybody who's coming in.
[00:01:13.720 --> 00:01:16.720]   Looks like -- just give me a second.
[00:01:16.720 --> 00:01:26.800]   I'm going to get a couple of -- looks like people are starting to come in.
[00:01:26.800 --> 00:01:27.800]   Great.
[00:01:27.800 --> 00:01:33.200]   There's two Richards showed up on my screen.
[00:01:33.200 --> 00:01:34.520]   That's exciting.
[00:01:34.520 --> 00:01:37.000]   That's double the trouble.
[00:01:37.000 --> 00:01:39.000]   All right.
[00:01:39.000 --> 00:01:42.360]   I am -- let's see.
[00:01:42.360 --> 00:01:46.640]   Actually, Kayla, I am not seeing any -- are you sure you set --
[00:01:46.640 --> 00:01:48.360]   >> I set the broadcast to Zoom.
[00:01:48.360 --> 00:01:51.200]   Sorry for the technical difficulties for anybody who's watching.
[00:01:51.200 --> 00:01:54.760]   Kayla, could you check to make sure the Zoom thing is open?
[00:01:54.760 --> 00:01:55.760]   >> Yes.
[00:01:55.760 --> 00:01:58.760]   You mean the YouTube?
[00:01:58.760 --> 00:02:04.000]   Or -- >> The Zoom webinar.
[00:02:04.000 --> 00:02:05.000]   >> Oh.
[00:02:05.000 --> 00:02:07.720]   >> Because there are not any attendees.
[00:02:07.720 --> 00:02:08.720]   >> Oh.
[00:02:08.720 --> 00:02:10.720]   I think maybe not.
[00:02:10.720 --> 00:02:14.000]   >> Ah, I just saw we're broadcasting now to all attendees.
[00:02:14.000 --> 00:02:15.000]   Okay.
[00:02:15.000 --> 00:02:16.000]   Great.
[00:02:16.000 --> 00:02:18.960]   So we're going to do the YouTube stream, but not the Zoom stream.
[00:02:18.960 --> 00:02:19.960]   >> There we go.
[00:02:19.960 --> 00:02:25.640]   >> Which would be a real shame not to have some of our top fans who show up in the Zoom
[00:02:25.640 --> 00:02:26.640]   able to see things.
[00:02:26.640 --> 00:02:28.320]   So now they're coming in.
[00:02:28.320 --> 00:02:29.320]   Great.
[00:02:29.320 --> 00:02:30.320]   Welcome, everybody.
[00:02:30.320 --> 00:02:37.880]   And apologies to anybody on the YouTube stream who had to sit through that fun tech debugging.
[00:02:37.880 --> 00:02:41.520]   Welcome to the Weights and Biases Deep Learning Salon, everyone.
[00:02:41.520 --> 00:02:46.840]   I am going to try something a little bit new this time.
[00:02:46.840 --> 00:02:47.840]   We have a poll.
[00:02:47.840 --> 00:02:52.440]   So, Kayla, there's a poll at the bottom of the screen if you could share that for the
[00:02:52.440 --> 00:02:53.640]   attendees.
[00:02:53.640 --> 00:02:57.720]   The poll this time is about attention.
[00:02:57.720 --> 00:03:01.800]   So the question is, is attention all you need?
[00:03:01.800 --> 00:03:05.240]   Yes, no, or what?
[00:03:05.240 --> 00:03:08.200]   Which are the three possible questions.
[00:03:08.200 --> 00:03:11.480]   The three possible answers to our provocative poll.
[00:03:11.480 --> 00:03:18.800]   So go ahead and answer that, and then we'll share the results when we are in between talks.
[00:03:18.800 --> 00:03:23.600]   So let me first introduce our first speaker.
[00:03:23.600 --> 00:03:31.320]   So our first speaker today is Richard Crabe, who is the founder and CEO of Numeri, which
[00:03:31.320 --> 00:03:37.360]   is the hardest data science tournament on the planet, and recently released a new way
[00:03:37.360 --> 00:03:42.840]   where you can get paid for your ability to generate high-quality signals useful for predicting
[00:03:42.840 --> 00:03:43.840]   stock markets.
[00:03:43.840 --> 00:03:50.280]   In both cases, you are paid in a novel crypto token based off of the Ethereum blockchain.
[00:03:50.280 --> 00:03:55.200]   And they've got a bunch of really great sort of corporate goals, visions, and mission that
[00:03:55.200 --> 00:03:57.520]   I'm excited to chat with Richard about.
[00:03:57.520 --> 00:04:01.480]   And he's also going to tell us a little bit about some of the details, you know, how to
[00:04:01.480 --> 00:04:05.840]   participate in this tournament and what you can do to make sure you do well.
[00:04:05.840 --> 00:04:08.720]   So go ahead and take it away, Richard.
[00:04:08.720 --> 00:04:09.720]   Welcome.
[00:04:09.720 --> 00:04:11.360]   Okay, well, yeah, thanks for having me.
[00:04:11.360 --> 00:04:18.880]   I wanted to first introduce Numeri and then share my screen and show you a few things.
[00:04:18.880 --> 00:04:24.720]   So the first thing about Numeri is that we're kind of an open hedge fund.
[00:04:24.720 --> 00:04:29.560]   We give away all of our data for free and allow anyone in the world to model the data
[00:04:29.560 --> 00:04:30.880]   that we give.
[00:04:30.880 --> 00:04:35.280]   So usually it's the last thing a hedge fund would ever do, but Numeri does it because
[00:04:35.280 --> 00:04:38.920]   we want to get the best possible talent looking at our data.
[00:04:38.920 --> 00:04:44.040]   So it's almost a little bit like a Kaggle competition, if you're familiar with that,
[00:04:44.040 --> 00:04:46.520]   except all the data is obfuscated.
[00:04:46.520 --> 00:04:49.160]   So you have no idea really what you're modeling.
[00:04:49.160 --> 00:04:53.680]   But for any good data scientist, you can still find the structure of the data without knowing
[00:04:53.680 --> 00:04:55.980]   what the features are.
[00:04:55.980 --> 00:05:03.560]   But I did want to share a little bit about what is unique about the stock market.
[00:05:03.560 --> 00:05:12.120]   And the most common problems that people have with overfitting in machine learning are kind
[00:05:12.120 --> 00:05:15.400]   of somehow way worse in finance.
[00:05:15.400 --> 00:05:19.820]   And this isn't very well understood by most people.
[00:05:19.820 --> 00:05:24.320]   And so I'll try to share some of the...
[00:05:24.320 --> 00:05:30.760]   So this is your classic back test.
[00:05:30.760 --> 00:05:38.440]   It has a kind of very good historical portion where you trained your model and then it just
[00:05:38.440 --> 00:05:41.840]   stops working whenever you trade it live.
[00:05:41.840 --> 00:05:44.240]   Your performance gets much worse.
[00:05:44.240 --> 00:05:47.680]   And this is the common pattern you see.
[00:05:47.680 --> 00:05:53.260]   And on the one hand, yeah, all machine learning problems have this.
[00:05:53.260 --> 00:06:00.560]   You can easily overfit a face detection algorithm and have your in-sample performance or even
[00:06:00.560 --> 00:06:05.320]   your holdout performance be worse than some other new set that you're showing later.
[00:06:05.320 --> 00:06:11.480]   But there is something special about finance where this is very common and perplexes a
[00:06:11.480 --> 00:06:13.800]   lot of people.
[00:06:13.800 --> 00:06:20.680]   So first, just to show you a little bit about numerized data to describe this problem.
[00:06:20.680 --> 00:06:22.480]   This is what it looks like.
[00:06:22.480 --> 00:06:23.480]   This is...
[00:06:23.480 --> 00:06:24.480]   You can get this on GitHub.
[00:06:24.480 --> 00:06:28.960]   It's just called our analysis and tips notebook.
[00:06:28.960 --> 00:06:32.040]   And it shows all the data that we have.
[00:06:32.040 --> 00:06:33.440]   This is what it looks like.
[00:06:33.440 --> 00:06:41.280]   You have all these features and we call them off the Dungeons and Dragons names.
[00:06:41.280 --> 00:06:43.880]   And you also have a target.
[00:06:43.880 --> 00:06:49.040]   And the target variable and the features are obfuscated to you, but you're trying to basically
[00:06:49.040 --> 00:06:51.320]   use these features to model this target.
[00:06:51.320 --> 00:06:54.640]   So it seems like a very simple supervised learning problem.
[00:06:54.640 --> 00:06:58.520]   And if you treat it that way, you will do quite badly.
[00:06:58.520 --> 00:07:00.080]   Because there's a few things that...
[00:07:00.080 --> 00:07:01.840]   I'm sorry.
[00:07:01.840 --> 00:07:07.880]   There's a few things that you're missing.
[00:07:07.880 --> 00:07:11.560]   So the first thing to notice is this column called era.
[00:07:11.560 --> 00:07:15.020]   Now the big problem with stock market data is time.
[00:07:15.020 --> 00:07:20.680]   And there's a lot of problems with the data modeling because all the data is in different
[00:07:20.680 --> 00:07:22.520]   times of time periods.
[00:07:22.520 --> 00:07:25.240]   And we call these eras on Numerai.
[00:07:25.240 --> 00:07:30.200]   And what happens is if people ignore the eras, they think their model is a lot better than
[00:07:30.200 --> 00:07:31.720]   it really is.
[00:07:31.720 --> 00:07:37.160]   And they end up making a model that doesn't generalize very well.
[00:07:37.160 --> 00:07:41.880]   And so you have to kind of take care that there is this time component.
[00:07:41.880 --> 00:07:43.960]   And there aren't that many eras.
[00:07:43.960 --> 00:07:48.640]   There are only about 100 or something eras.
[00:07:48.640 --> 00:07:54.640]   And so when you think about modeling this data, even though there's half a million rows,
[00:07:54.640 --> 00:07:58.120]   if there are very few eras, eras are the important thing.
[00:07:58.120 --> 00:07:59.720]   Eras are what's unique.
[00:07:59.720 --> 00:08:05.680]   Because in that time section, everything inside that time section, that era, is happening
[00:08:05.680 --> 00:08:07.120]   at the same time.
[00:08:07.120 --> 00:08:11.320]   So the data isn't independent by row.
[00:08:11.320 --> 00:08:13.600]   It's only independent by era.
[00:08:13.600 --> 00:08:23.160]   So this is over a decade of data, but you actually have very little number of independent
[00:08:23.160 --> 00:08:24.520]   observations.
[00:08:24.520 --> 00:08:25.960]   So that's one key thing.
[00:08:25.960 --> 00:08:29.680]   And then the other key thing is how the features interact with each other.
[00:08:29.680 --> 00:08:36.520]   So what happens with most models, going back to this, is during the learning phase, when
[00:08:36.520 --> 00:08:43.080]   you train on historical data, what happens is your model will pick up on the best risk
[00:08:43.080 --> 00:08:44.160]   for that period.
[00:08:44.160 --> 00:08:48.000]   Not the best alpha for that period, the best risk for that period.
[00:08:48.000 --> 00:08:54.940]   So if it was a period where momentum of common stock factor did well, or a period where value
[00:08:54.940 --> 00:09:00.860]   did well, these things will come out extremely strong in, say, the coefficient of a linear
[00:09:00.860 --> 00:09:02.880]   model that you made.
[00:09:02.880 --> 00:09:06.600]   And the trick in some ways with finance is kind of very strange.
[00:09:06.600 --> 00:09:11.960]   You have to model-- you have to create a model that sort of has some kind of coefficients
[00:09:11.960 --> 00:09:14.720]   on the features to predict something.
[00:09:14.720 --> 00:09:20.960]   But at the same time, you want the coefficients on the features to be as low as possible.
[00:09:20.960 --> 00:09:29.680]   So it's like a strange problem, because in some ways, you don't want to have risk exposures
[00:09:29.680 --> 00:09:36.080]   to the features, because they're really just adding risk, and they're not adding alpha.
[00:09:36.080 --> 00:09:45.720]   So what happens on Numeri, a lot of our users do something called feature neutralization.
[00:09:45.720 --> 00:09:51.400]   And feature neutralization is-- we have it in this example script that we give out.
[00:09:51.400 --> 00:09:57.600]   If you go to our website, you can download all this data for free and these example scripts.
[00:09:57.600 --> 00:10:04.700]   We actually include a little bit of code that does a linear projection.
[00:10:04.700 --> 00:10:09.400]   So what this is doing is saying, well, you've built a nonlinear model with a neural net,
[00:10:09.400 --> 00:10:15.520]   but I don't want to take on all these linear exposures to what are really just risks.
[00:10:15.520 --> 00:10:23.480]   And this does this linear projection, projects out these risks, and leaves what's left, the
[00:10:23.480 --> 00:10:30.000]   really high-quality alpha, the alpha that isn't just something that's in the features
[00:10:30.000 --> 00:10:31.600]   in a linear way.
[00:10:31.600 --> 00:10:37.400]   And what tends to happen if you can take off those features, then suddenly your estimates
[00:10:37.400 --> 00:10:38.400]   go down.
[00:10:38.400 --> 00:10:43.000]   You're like, well, jeez, I actually don't have as good a model as I think in sample,
[00:10:43.000 --> 00:10:49.800]   but it's actually a model that generalizes very well out of sample.
[00:10:49.800 --> 00:10:54.880]   And in most machine learning, typically the feature that you like the most is the feature
[00:10:54.880 --> 00:10:57.120]   you really want to keep.
[00:10:57.120 --> 00:11:02.480]   But in finance, the feature you like the most is the most dangerous risk that you're exposed
[00:11:02.480 --> 00:11:03.620]   to.
[00:11:03.620 --> 00:11:06.880]   And you have to reduce that risk.
[00:11:06.880 --> 00:11:12.300]   Otherwise, your model won't generalize and you will do extremely badly in live data.
[00:11:12.300 --> 00:11:18.040]   So that's kind of like a little bit of a secret about how to make your models generalize in
[00:11:18.040 --> 00:11:19.040]   finance.
[00:11:19.040 --> 00:11:21.560]   There's so much more to it.
[00:11:21.560 --> 00:11:27.240]   I hope you get to take a look at the data and see some of this for yourself.
[00:11:27.240 --> 00:11:28.240]   Thank you.
[00:11:28.240 --> 00:11:29.240]   All right.
[00:11:29.240 --> 00:11:40.840]   Well, yeah, thanks, Richard, for that quick look at what's up with finance data on Numeri.
[00:11:40.840 --> 00:11:45.200]   I think the first question that I have is actually, so there's something very unintuitive
[00:11:45.200 --> 00:11:50.760]   to me as somebody who comes from science into machine learning about the idea that you'd
[00:11:50.760 --> 00:11:55.160]   want to, that you'd want an unsparse model, right?
[00:11:55.160 --> 00:11:59.480]   That you'd want, it seems like this neutralizing feature exposure idea, you try and have as
[00:11:59.480 --> 00:12:03.960]   many small parameters across as many pieces as possible.
[00:12:03.960 --> 00:12:09.040]   And in science, you're often trying to find a sparse linear model rather than a diffuse
[00:12:09.040 --> 00:12:10.400]   nonlinear model.
[00:12:10.400 --> 00:12:14.920]   So can you comment on why you think there's that big gap between what works and is useful
[00:12:14.920 --> 00:12:17.680]   in science and what works and is useful in finance?
[00:12:17.680 --> 00:12:20.480]   Yeah, it's a really good question.
[00:12:20.480 --> 00:12:26.000]   I mean, the way to think about it is kind of like that the risks and the alphas are
[00:12:26.000 --> 00:12:28.280]   kind of dual.
[00:12:28.280 --> 00:12:36.300]   On the one hand, anything you find, any exposure you find that you think is an alpha, meaning
[00:12:36.300 --> 00:12:42.640]   something that has correlation, subsequent correlation with returns, it could just as
[00:12:42.640 --> 00:12:46.080]   easily kind of turn around.
[00:12:46.080 --> 00:12:50.720]   And so if you want to operate, it's almost like you have the assumption of, well, the
[00:12:50.720 --> 00:12:54.600]   market's actually efficient to every single feature that I have.
[00:12:54.600 --> 00:12:55.840]   Of course it is.
[00:12:55.840 --> 00:12:58.080]   Everybody's going to have learned from the past data.
[00:12:58.080 --> 00:13:06.240]   So what I actually will generalize is when I de-correlate myself from those basic factors
[00:13:06.240 --> 00:13:12.340]   and make a hedge fund that's quite differentiated from all the exposures that everyone else
[00:13:12.340 --> 00:13:13.340]   is taking on.
[00:13:13.340 --> 00:13:14.340]   I see.
[00:13:14.340 --> 00:13:20.080]   Yeah, because I think the motivation in science is to find a causal model of what's going
[00:13:20.080 --> 00:13:21.080]   on.
[00:13:21.080 --> 00:13:25.160]   That is, if I find something that's sparse, if I apply sort of like Occam's razor, then
[00:13:25.160 --> 00:13:31.760]   I'm finding something that's the real underlying model that causes the phenomena that I'm observing.
[00:13:31.760 --> 00:13:35.560]   But in finance, you don't necessarily want a causal model, right?
[00:13:35.560 --> 00:13:38.160]   You want a really good predictive model.
[00:13:38.160 --> 00:13:39.160]   Exactly.
[00:13:39.460 --> 00:13:45.500]   And what we see in our data is if you, in some ways, the less you know about how your
[00:13:45.500 --> 00:13:46.840]   model works, the better.
[00:13:46.840 --> 00:13:51.300]   It's like, it's somehow especially good for machine learning.
[00:13:51.300 --> 00:13:55.860]   A linear model, like I said, would only have just positive or negative coefficients on
[00:13:55.860 --> 00:13:57.540]   the features.
[00:13:57.540 --> 00:14:00.260]   And if you took those down to zero, you'd have nothing left.
[00:14:00.260 --> 00:14:05.560]   But a nonlinear model, there's actually something left and that's often the gold.
[00:14:05.560 --> 00:14:09.340]   And that's what we want people to focus on learning because that tends to generalize
[00:14:09.340 --> 00:14:10.820]   better.
[00:14:10.820 --> 00:14:12.660]   Interesting.
[00:14:12.660 --> 00:14:19.680]   And so then how do you see, so there's this idea of neutralizing feature exposure.
[00:14:19.680 --> 00:14:24.720]   How does that play into this new Numerai Signals tool that you're putting out or this new sort
[00:14:24.720 --> 00:14:26.020]   of kind of contest?
[00:14:26.020 --> 00:14:27.020]   Yeah.
[00:14:27.020 --> 00:14:32.420]   So Numerai Signals is a brand new thing we launched just about two weeks ago.
[00:14:32.420 --> 00:14:37.940]   And like I said, with Numerai, we give out all of our data and our data is kind of expensive.
[00:14:37.940 --> 00:14:42.740]   It's this high quality data you probably couldn't find, but it's all in this obfuscated way.
[00:14:42.740 --> 00:14:47.760]   But there are people out there who maybe have some of their own data already and they have
[00:14:47.760 --> 00:14:53.220]   cobbled together some Yahoo Finance data and combined it with some Bloomberg data and they've
[00:14:53.220 --> 00:14:57.120]   built their own signal out of that.
[00:14:57.120 --> 00:15:01.440]   And we want to say, well, if you have a signal like that, you should come to us too.
[00:15:01.440 --> 00:15:04.920]   We also want signals made on data that isn't ours.
[00:15:04.920 --> 00:15:06.200]   And that's what Numerai Signals is.
[00:15:06.200 --> 00:15:10.800]   You can come and submit your signal to us and get rewarded for it.
[00:15:10.800 --> 00:15:18.320]   But the twist is we are not really looking for a signal that is really correlated with
[00:15:18.320 --> 00:15:21.180]   return, really predictive of return.
[00:15:21.180 --> 00:15:27.100]   In some sense, because we already have a lot of signals that are predictive with return.
[00:15:27.100 --> 00:15:34.140]   So if you bring a model that we already have, we kind of think we should pay you zero.
[00:15:34.140 --> 00:15:40.600]   But if you bring a model that is uncorrelated from everything we have, i.e. neutral to everything
[00:15:40.600 --> 00:15:47.380]   we have, like a projection of everything we have, and also still has alpha even after
[00:15:47.380 --> 00:15:52.920]   we do that neutralization to your signal, then you really have something.
[00:15:52.920 --> 00:15:54.620]   And that's much more valuable.
[00:15:54.620 --> 00:15:56.860]   And we pay a lot for that.
[00:15:56.860 --> 00:15:58.700]   I see.
[00:15:58.700 --> 00:16:06.500]   So then, so how do you generate, I guess, so if I'm sitting here at home making my signal
[00:16:06.500 --> 00:16:13.460]   that I want to send to Numerai, what kinds of things can I do to ensure that what I think
[00:16:13.460 --> 00:16:16.100]   is valuable is also what you guys are going to think is valuable?
[00:16:16.100 --> 00:16:19.100]   How do I seek alpha, so to speak?
[00:16:19.100 --> 00:16:21.980]   Yeah, no, it is something of a black box.
[00:16:21.980 --> 00:16:25.900]   What we do say is anything basic probably won't work.
[00:16:25.900 --> 00:16:27.900]   So we have no surprise.
[00:16:27.900 --> 00:16:34.100]   We have the PE ratio of every single stock in the world, and we have it for 15, 18 years
[00:16:34.100 --> 00:16:35.380]   or so.
[00:16:35.380 --> 00:16:40.260]   So if you come with and you just start submitting the PE ratios of stocks, we will neutralize
[00:16:40.260 --> 00:16:42.060]   that by our PE.
[00:16:42.060 --> 00:16:45.400]   You'll have nothing left and you won't do well.
[00:16:45.400 --> 00:16:52.380]   But if you create a complicated model and it's on unusual data, then you're very likely
[00:16:52.380 --> 00:16:59.200]   to have a good portion of that be orthogonal to the models produced on Numerai and the
[00:16:59.200 --> 00:17:01.000]   data that we have.
[00:17:01.000 --> 00:17:06.360]   So basically, it's like, don't give us something you know kind of everyone has, and then you'll
[00:17:06.360 --> 00:17:07.360]   be good.
[00:17:07.360 --> 00:17:08.360]   I see.
[00:17:08.360 --> 00:17:14.340]   So, I mean, perhaps some of this stuff is proprietary information.
[00:17:14.340 --> 00:17:20.960]   But do you have any examples of signals that have been useful in this tool so far or like
[00:17:20.960 --> 00:17:23.360]   a prototypical example to share?
[00:17:23.360 --> 00:17:27.120]   Well, we have been surprised by some of them.
[00:17:27.120 --> 00:17:29.600]   We thought it was kind of...
[00:17:29.600 --> 00:17:33.400]   So Numerai, we say, is the hardest data science term in the world because you have to deal
[00:17:33.400 --> 00:17:34.520]   with all these problems.
[00:17:34.520 --> 00:17:39.160]   It's not just like downloading a Kaggle data set and building an XGBoost.
[00:17:39.160 --> 00:17:43.440]   There's a lot more to think about and it goes deep.
[00:17:43.440 --> 00:17:47.380]   That's why we have users who've been there for many years.
[00:17:47.380 --> 00:17:53.100]   But signals, we were very surprised to see that in the first few days of it, there were
[00:17:53.100 --> 00:18:01.980]   people uploading signals that were very orthogonal and had a lot more orthogonal than we could
[00:18:01.980 --> 00:18:07.140]   even make ourselves if we really tried.
[00:18:07.140 --> 00:18:13.720]   So it does seem like, and there's some people in Japan that are really strong.
[00:18:13.720 --> 00:18:20.260]   There are also some people from Numerai itself who've started building signals.
[00:18:20.260 --> 00:18:26.820]   So I think we're pretty surprised and who knows what data sets they're using to create
[00:18:26.820 --> 00:18:27.820]   these.
[00:18:27.820 --> 00:18:28.820]   We don't know.
[00:18:28.820 --> 00:18:31.660]   They never give us their model or their data.
[00:18:31.660 --> 00:18:34.380]   They're only giving us the output of their signal.
[00:18:34.380 --> 00:18:38.420]   So it's kind of cool in a way, we don't really know what they're doing to generate these
[00:18:38.420 --> 00:18:39.420]   things.
[00:18:39.420 --> 00:18:40.420]   I see.
[00:18:40.420 --> 00:18:41.420]   Interesting.
[00:18:41.420 --> 00:18:48.740]   So I guess one of the famous examples of the application of machine learning in finance
[00:18:48.740 --> 00:18:53.420]   was predicting crop yields months ahead of time using satellite data.
[00:18:53.420 --> 00:18:55.380]   I forget who the people were who did that.
[00:18:55.380 --> 00:18:58.660]   I don't know if you recall.
[00:18:58.660 --> 00:19:03.380]   So is that the kind of thing that generates the signals that you would be interested in
[00:19:03.380 --> 00:19:06.420]   or is it maybe a little bit more financial data?
[00:19:06.420 --> 00:19:12.420]   Yeah, I think that is always the kind of like, that's a bigger story than it's like real.
[00:19:12.420 --> 00:19:19.300]   The main alternative data that you want is going to apply to lots of stocks.
[00:19:19.300 --> 00:19:23.660]   And so if someone told me something about one stock, that's not a quant model.
[00:19:23.660 --> 00:19:24.860]   That's not a signal.
[00:19:24.860 --> 00:19:27.180]   That's just someone's stock tip.
[00:19:27.180 --> 00:19:29.500]   And Numerai Signals isn't for that.
[00:19:29.500 --> 00:19:32.380]   It's for like broad cross-sectional.
[00:19:32.380 --> 00:19:36.180]   You have some data that applies to 5,000 stocks.
[00:19:36.180 --> 00:19:42.020]   And it could be some NLP signal based off Twitter or it could be anything, but usually
[00:19:42.020 --> 00:19:45.060]   it's got to be really broad for it to be valuable.
[00:19:45.060 --> 00:19:48.580]   Because we never put the fund in one stock, right?
[00:19:48.580 --> 00:19:49.580]   So we need broad.
[00:19:49.580 --> 00:19:50.580]   Right, right.
[00:19:50.580 --> 00:19:51.580]   Certainly.
[00:19:51.580 --> 00:19:55.860]   That seems, I don't know that much about finance, but that sounds like a bad idea.
[00:19:55.860 --> 00:19:56.860]   Yeah.
[00:19:56.860 --> 00:20:02.220]   So we got a question in the Q&A and I actually encourage folks watching on YouTube and folks
[00:20:02.220 --> 00:20:08.820]   in Zoom to post in the Q&A or in the live chat to ask questions and I'll forward them
[00:20:08.820 --> 00:20:09.820]   to Richard.
[00:20:09.820 --> 00:20:16.500]   So the question is, how is this orthogonality assessed?
[00:20:16.500 --> 00:20:20.380]   You've given a couple of examples of ways to think about what this orthogonality means
[00:20:20.380 --> 00:20:22.460]   and sort of uniqueness or originality.
[00:20:22.460 --> 00:20:25.100]   What more can you say about that?
[00:20:25.100 --> 00:20:30.380]   Well that code I shared earlier was super high level, but the code is exactly what we
[00:20:30.380 --> 00:20:31.380]   use.
[00:20:31.380 --> 00:20:34.780]   So we do it with every single feature that we have.
[00:20:34.780 --> 00:20:39.860]   So there are about 310 features that we have and there are also what we call like nine
[00:20:39.860 --> 00:20:45.100]   risk factors like country risk or sector risk.
[00:20:45.100 --> 00:20:49.260]   Because if someone posts a model that it really only did well just because it had exposure
[00:20:49.260 --> 00:20:52.980]   to the tech industry and the tech industry happened to do well, that's again, not what
[00:20:52.980 --> 00:20:55.180]   we're looking for.
[00:20:55.180 --> 00:21:03.420]   So it is, and mathematically it's making this linear projection, it's regressing out all
[00:21:03.420 --> 00:21:10.220]   the linear exposures that you have to the things we have and what's remaining is what's
[00:21:10.220 --> 00:21:16.180]   valuable to us.
[00:21:16.180 --> 00:21:23.700]   So you mentioned, I guess, NLP signals maybe of behavior of users on social media.
[00:21:23.700 --> 00:21:26.020]   Those sound like useful features.
[00:21:26.020 --> 00:21:29.580]   Any other thoughts about what things might be orthogonal?
[00:21:29.580 --> 00:21:33.020]   Yeah, I think NLP is quite a big one.
[00:21:33.020 --> 00:21:36.460]   It's one I'm quite excited about.
[00:21:36.460 --> 00:21:41.940]   We actually did buy some news sentiment data, but they kind of mess it up.
[00:21:41.940 --> 00:21:47.300]   And if you look at it from, whenever we go talk to a new data vendor, they say, "Oh,
[00:21:47.300 --> 00:21:48.620]   we have all this data.
[00:21:48.620 --> 00:21:50.340]   We test it.
[00:21:50.340 --> 00:21:54.620]   It's got no original compared to what we already have."
[00:21:54.620 --> 00:22:00.340]   So I like that we're putting it out to the world and saying, anyone can kind of be a
[00:22:00.340 --> 00:22:02.700]   data vendor to us and provide anything.
[00:22:02.700 --> 00:22:05.340]   But NLP is the one I'm kind of the most excited about.
[00:22:05.340 --> 00:22:11.060]   And we have a user who's really experienced with NLP and has even written some numerite
[00:22:11.060 --> 00:22:15.740]   jokes with GPT-3 before to show us.
[00:22:15.740 --> 00:22:20.700]   And I think it's not really, I kind of, yeah, like I said, I'm quite skeptical of the alternative
[00:22:20.700 --> 00:22:21.700]   data craze.
[00:22:21.700 --> 00:22:26.640]   There's a lot you can do with kind of very good modeling on normal data.
[00:22:26.640 --> 00:22:32.820]   And there's a lot of text out there that companies have to produce about their funds or have
[00:22:32.820 --> 00:22:36.560]   to make statements about their companies.
[00:22:36.560 --> 00:22:42.500]   So mining that data, because it's already quite structured, I think there's quite a
[00:22:42.500 --> 00:22:46.460]   lot even there that we wouldn't have and that would do well.
[00:22:46.460 --> 00:22:55.580]   Yeah, I guess what I'm kind of aiming at is in your Numerize Signals blog post on Medium,
[00:22:55.580 --> 00:22:58.020]   you asked sort of, "Where is the next Ken Griffin?"
[00:22:58.020 --> 00:23:03.900]   And it seemed like one of the motivations you had was this idea that the power of crowds
[00:23:03.900 --> 00:23:09.540]   of people in garages with internet connections to find signals.
[00:23:09.540 --> 00:23:17.060]   So I guess, yeah, what do you think those next Ken Griffins, it seems like alternative
[00:23:17.060 --> 00:23:21.500]   data sources aren't the right thing, but what do you think those next Ken Griffins are and
[00:23:21.500 --> 00:23:24.220]   should be working on?
[00:23:24.220 --> 00:23:33.900]   Well, the other side of it is that Numerize Signals is just one week long predictions,
[00:23:33.900 --> 00:23:36.780]   whereas Numerize is one month long.
[00:23:36.780 --> 00:23:40.020]   And so there's actually a lot of things that work on a one week time horizon that would
[00:23:40.020 --> 00:23:41.540]   not work on a one month horizon.
[00:23:41.540 --> 00:23:47.740]   And a lot of that is actually technical data, which is data that's kind of built from the
[00:23:47.740 --> 00:23:49.340]   price series.
[00:23:49.340 --> 00:23:54.700]   And oftentimes you can have a very good technical model, but because it trades so often, you
[00:23:54.700 --> 00:23:57.040]   don't really make money off the costs.
[00:23:57.040 --> 00:24:00.780]   But technical features would be great.
[00:24:00.780 --> 00:24:04.640]   And we don't have lots of technical features at Numerize.
[00:24:04.640 --> 00:24:08.980]   And so I would say what's quite nice about that is anyone can make those.
[00:24:08.980 --> 00:24:14.060]   You just need the price and there's very easy access to get the price data and then you
[00:24:14.060 --> 00:24:15.880]   can make your own technical features.
[00:24:15.880 --> 00:24:18.820]   So I think a lot of models will be like that.
[00:24:18.820 --> 00:24:21.980]   Interesting.
[00:24:21.980 --> 00:24:28.100]   What do you think the next sort of shorter term than the master plan of building the
[00:24:28.100 --> 00:24:30.660]   last hedge fund?
[00:24:30.660 --> 00:24:35.120]   What are sort of the shorter term ways you want to extend either the core data science
[00:24:35.120 --> 00:24:37.360]   tournament or the signals product?
[00:24:37.360 --> 00:24:40.020]   Well, it's yeah, it's just two weeks old.
[00:24:40.020 --> 00:24:44.540]   It's doubling every week so far, but it's just two weeks.
[00:24:44.540 --> 00:24:47.440]   We think there will be a lot of staking there.
[00:24:47.440 --> 00:24:49.720]   The way Numerize works is you stake your models.
[00:24:49.720 --> 00:24:54.600]   Numerize has $5 million or so, maybe four and a half million dollars staked.
[00:24:54.600 --> 00:24:58.240]   Signals just started, but it has $24,000 staked.
[00:24:58.240 --> 00:25:03.520]   A year ago, all of Numerize put together was $20,000 stake.
[00:25:03.520 --> 00:25:05.200]   So it's really grown a lot.
[00:25:05.200 --> 00:25:11.240]   And the main focus is on those two things for now in the medium term.
[00:25:11.240 --> 00:25:18.560]   And the one thing we've also been building for a while and we haven't taken on capital
[00:25:18.560 --> 00:25:19.780]   to our fund.
[00:25:19.780 --> 00:25:23.940]   So at some point next year, we'll probably do that.
[00:25:23.940 --> 00:25:28.400]   But ultimately, the fund is really just for institutional investors and it's not really
[00:25:28.400 --> 00:25:31.740]   open to the public or our users.
[00:25:31.740 --> 00:25:37.780]   But I think the exciting thing is, you know, is the master plan to monopolize intelligence,
[00:25:37.780 --> 00:25:42.080]   number one, monopolize data, number two, which is what signals is about getting external
[00:25:42.080 --> 00:25:43.260]   data in.
[00:25:43.260 --> 00:25:46.720]   And the third thing is monopolize money.
[00:25:46.720 --> 00:25:52.320]   And so we're kind of going step by step.
[00:25:52.320 --> 00:25:55.840]   So there's this investor who you may have heard of.
[00:25:55.840 --> 00:26:00.240]   He lives in Nebraska, so you might not have heard of him, but Warren Buffett, who says
[00:26:00.240 --> 00:26:03.760]   that you should only invest in companies that have moats.
[00:26:03.760 --> 00:26:08.480]   So what do you think are, like you mentioned, you know, monopolize shows up in the master
[00:26:08.480 --> 00:26:10.200]   plan of Numerize a couple of times.
[00:26:10.200 --> 00:26:15.960]   So what do you think allows you to sort of build those moats to protect your monopolies
[00:26:15.960 --> 00:26:20.160]   in data and in intelligence?
[00:26:20.160 --> 00:26:26.000]   Well, I think you can make, you know, monopoly, it's something like it has negative connotations,
[00:26:26.000 --> 00:26:31.040]   but it's a kind of good word, I think, for what we're trying to say.
[00:26:31.040 --> 00:26:37.240]   We don't we want to be the best data science community and the most high paying and rewarding
[00:26:37.240 --> 00:26:38.920]   community.
[00:26:38.920 --> 00:26:42.800]   And we've already paid out over 40 million dollars to our data scientists.
[00:26:42.800 --> 00:26:45.800]   There are a number of millionaires from Numerize.
[00:26:45.800 --> 00:26:47.920]   And not many people know that.
[00:26:47.920 --> 00:26:50.920]   And I think the community is the whole thing.
[00:26:50.920 --> 00:26:53.640]   We don't trade our own model.
[00:26:53.640 --> 00:26:57.300]   If you decide to rip your data out of signals, we don't have it anymore.
[00:26:57.300 --> 00:27:00.680]   If you decide to pull your model out of Numerize, we don't have it anymore.
[00:27:00.680 --> 00:27:06.040]   So we're really relying on the fact that we can make these incentives that bring a community
[00:27:06.040 --> 00:27:11.560]   together and because they're staking and engaged and making a lot more than they could in other
[00:27:11.560 --> 00:27:12.760]   ways.
[00:27:12.760 --> 00:27:19.580]   It's going to get really big and there won't be a reason to quit.
[00:27:19.580 --> 00:27:20.580]   And I do like it.
[00:27:20.580 --> 00:27:24.460]   It's kind of like to a Bitcoin, like you can think of our users almost as being the miners
[00:27:24.460 --> 00:27:26.440]   of Bitcoin or something.
[00:27:26.440 --> 00:27:31.420]   And they're doing data mining on our data and then they're earning our cryptocurrency.
[00:27:31.420 --> 00:27:34.240]   And it's very sticky once you get a lot of miners.
[00:27:34.240 --> 00:27:36.760]   It's suddenly, well, this is the best place to be.
[00:27:36.760 --> 00:27:38.040]   Why would I move?
[00:27:38.040 --> 00:27:40.400]   And I think that's happening with us.
[00:27:40.400 --> 00:27:43.120]   And I think a lot of our users like it for that reason.
[00:27:43.120 --> 00:27:44.120]   I see.
[00:27:44.120 --> 00:27:50.160]   And in some ways, an even better analogy than to the Bitcoin miners would be to the Bitcoin
[00:27:50.160 --> 00:27:51.240]   traders, right?
[00:27:51.240 --> 00:27:56.320]   To the people who have been trading various forms of various coins and tokens.
[00:27:56.320 --> 00:28:01.520]   So do you see that as one of your competitors for mindshare and users?
[00:28:01.520 --> 00:28:07.960]   Or what would you say are some of your competitors, if there are any?
[00:28:07.960 --> 00:28:14.880]   Yeah, I do have a kind of a pet hatred of crypto traders, actually.
[00:28:14.880 --> 00:28:17.000]   They even call themselves DGens.
[00:28:17.000 --> 00:28:21.880]   But they're not very likable.
[00:28:21.880 --> 00:28:24.480]   Like it's not something they aim for, it seems.
[00:28:24.480 --> 00:28:25.480]   Yeah, yeah.
[00:28:25.480 --> 00:28:29.540]   And they're not super focused on the long term or the consequences of their actions.
[00:28:29.540 --> 00:28:31.960]   But that's fine.
[00:28:31.960 --> 00:28:40.080]   I think, you know, ultimately, the equity markets are a lot sort of better for the world
[00:28:40.080 --> 00:28:41.800]   than the crypto markets.
[00:28:41.800 --> 00:28:48.400]   And if you were an expert trader at Ponzi schemes, I don't think you should be proud of yourself.
[00:28:48.400 --> 00:28:54.360]   You know, no matter how good you are, you're doing the wrong thing.
[00:28:54.360 --> 00:28:57.680]   So yeah, but yeah, I think some of our users are very interested in crypto.
[00:28:57.680 --> 00:28:59.160]   Obviously, we have a cryptocurrency.
[00:28:59.160 --> 00:29:01.800]   So I don't hate crypto all through and through.
[00:29:01.800 --> 00:29:06.040]   I love the applications of crypto and the fact that you can use it to build communities
[00:29:06.040 --> 00:29:08.840]   and do things like staking.
[00:29:08.840 --> 00:29:14.300]   But yeah, we need to compete with the mindshare of the DeFi DGens.
[00:29:14.300 --> 00:29:18.440]   And some of the people, there's been some crypto projects that sort of say, you can
[00:29:18.440 --> 00:29:23.480]   just stake your cryptocurrency here and do nothing and earn 300% a year.
[00:29:23.480 --> 00:29:28.220]   And it's like, okay, well, how does that work long term?
[00:29:28.220 --> 00:29:30.880]   And everyone's like, we don't really care about the long term.
[00:29:30.880 --> 00:29:35.900]   So I think for Numeri, whenever people are using Numeri and staking on Numeri, it's much
[00:29:35.900 --> 00:29:37.800]   more about the long term.
[00:29:37.800 --> 00:29:42.740]   And it's much more about doing something like real.
[00:29:42.740 --> 00:29:47.900]   One last question from Michael in the Zoom Q&A.
[00:29:47.900 --> 00:29:54.240]   Do you see the quality of the meta model that is sort of generated by the data science tournament
[00:29:54.240 --> 00:29:55.900]   improve over time?
[00:29:55.900 --> 00:29:57.400]   Do you think there's a limit?
[00:29:57.400 --> 00:30:02.440]   And do you expect the same thing to happen with the output of the signals project?
[00:30:02.440 --> 00:30:06.080]   There is a very strange thing happening with the meta model.
[00:30:06.080 --> 00:30:09.520]   So meta model combines all the Numeri models together.
[00:30:09.520 --> 00:30:14.340]   And if you look at it over the last year, it's climbing.
[00:30:14.340 --> 00:30:17.760]   Not every week, some weeks it'll drop down, someone pulls their model out or something,
[00:30:17.760 --> 00:30:21.280]   but it's climbing kind of linearly.
[00:30:21.280 --> 00:30:24.600]   And you really should think there's like an asymptote to this, right?
[00:30:24.600 --> 00:30:26.160]   And we haven't been changing the data.
[00:30:26.160 --> 00:30:28.440]   It's not like it's going up because of the data.
[00:30:28.440 --> 00:30:34.280]   We've given out some tips and validation data and other things, but it's quite impressive
[00:30:34.280 --> 00:30:37.480]   to me how it's still going up.
[00:30:37.480 --> 00:30:44.480]   And so I really like that idea of, even if we did nothing, the community would make everything
[00:30:44.480 --> 00:30:48.200]   better without us releasing features.
[00:30:48.200 --> 00:30:49.960]   And that's the problem with most hedge funds.
[00:30:49.960 --> 00:30:54.240]   They find it hard to scale because they always have to be running out, buying new data, trying
[00:30:54.240 --> 00:30:56.560]   new things, and it's very chaotic.
[00:30:56.560 --> 00:31:02.360]   But Numeri, it's kind of even more chaotic, but the incentives keep it all aligned.
[00:31:02.360 --> 00:31:03.360]   Great.
[00:31:03.360 --> 00:31:06.640]   Well, that's good to hear.
[00:31:06.640 --> 00:31:11.960]   I'm excited to, you know, I think collaboration is a really important thing that both the
[00:31:11.960 --> 00:31:15.160]   machine learning community and the finance community could do better on.
[00:31:15.160 --> 00:31:20.800]   So I'm glad to hear from somebody who's putting so much into making that work well.
[00:31:20.800 --> 00:31:23.200]   Yeah, you're welcome.
[00:31:23.200 --> 00:31:24.200]   All right.
[00:31:24.200 --> 00:31:27.480]   Well, thanks for coming on.
[00:31:27.480 --> 00:31:30.000]   And feel free to stick around.
[00:31:30.000 --> 00:31:37.000]   You mentioned that NLP on data and alternative data was an important skill.
[00:31:37.000 --> 00:31:43.240]   And that's actually what we'll be hearing about next from our next speaker, Alexa Milosevic.
[00:31:43.240 --> 00:31:51.400]   Before we do that, I did want to do a quick promo here for the upcoming salon.
[00:31:51.400 --> 00:31:57.360]   So let me share my screen and we will talk about that.
[00:31:57.360 --> 00:32:04.320]   So the next salon is in two weeks.
[00:32:04.320 --> 00:32:08.480]   And so it's November 10th, same time, five o'clock Pacific.
[00:32:08.480 --> 00:32:11.220]   The two speakers, one will be Chirag Agarwal.
[00:32:11.220 --> 00:32:16.440]   So he'll be talking about variants of gradient, a new technique for detecting outliers and
[00:32:16.440 --> 00:32:18.600]   estimating example difficulty.
[00:32:18.600 --> 00:32:22.600]   We had the other author on just two weeks ago, Sarah Hooker, but she ended up having
[00:32:22.600 --> 00:32:26.840]   to talk about one of her other really exciting papers on the hardware lottery.
[00:32:26.840 --> 00:32:30.900]   So Chirag's coming along to talk about this variants of gradient paper.
[00:32:30.900 --> 00:32:32.800]   And then I'll be giving a talk.
[00:32:32.800 --> 00:32:35.920]   I also talked, I guess Sarah was maybe a month ago.
[00:32:35.920 --> 00:32:40.400]   I also talked two weeks ago, and I had so much fun, I think I'm going to do it again,
[00:32:40.400 --> 00:32:45.800]   about what you can learn by learning to multiply by one with machine learning, using that as
[00:32:45.800 --> 00:32:54.040]   a sort of demo or a basic question, a toy example, if you will.
[00:32:54.040 --> 00:32:57.660]   Then just so you know that we do these salons every two weeks, there's lots of great ones
[00:32:57.660 --> 00:32:58.660]   out there.
[00:32:58.660 --> 00:33:03.520]   I mentioned along with Sarah Hooker, we had Hannes Hopka from SAP Concur, you can see
[00:33:03.520 --> 00:33:04.520]   that.
[00:33:04.520 --> 00:33:09.400]   All these are recorded and put on our YouTube channel where you can review them.
[00:33:09.400 --> 00:33:13.240]   So that's Weights and Biases on youtube.com.
[00:33:13.240 --> 00:33:19.260]   And if you want to engage with the community that comes to these events that ask the questions,
[00:33:19.260 --> 00:33:25.240]   we gather together on a Slack forum, the Weights and Biases forum, bit.ly/slackforum.
[00:33:25.240 --> 00:33:29.600]   We have people on, Anthony Goldbloom, the CEO of Kaggle came and talked.
[00:33:29.600 --> 00:33:35.640]   Piero Molino, who's at Uber working on their AutoML solution, came and chatted with folks.
[00:33:35.640 --> 00:33:42.300]   Robert Nishihara, who is the CEO of AnyScale, is a lurker in our forum and occasionally
[00:33:42.300 --> 00:33:44.080]   emoji reacts to people's posts.
[00:33:44.080 --> 00:33:49.280]   So you've got a lot of interesting folks in the forum, and you could be one of them if
[00:33:49.280 --> 00:33:51.840]   you come through.
[00:33:51.840 --> 00:33:55.600]   So that's all I had to share about our community.
[00:33:55.600 --> 00:33:58.040]   One of those community members is here today.
[00:33:58.040 --> 00:34:04.040]   That is Alexa Milosevic, who is here to talk about a really cool tool for the automation
[00:34:04.040 --> 00:34:08.100]   of project management workflows with natural language processing.
[00:34:08.100 --> 00:34:13.520]   So Alexa's going to tell us a little bit about how they made this tool, this Jarvis tool,
[00:34:13.520 --> 00:34:15.920]   run blazing fast without hurting its accuracy.
[00:34:15.920 --> 00:34:19.560]   So I'm really excited to learn some of these tips.
[00:34:19.560 --> 00:34:25.400]   And actually, I hate to do this, but I do want to end the poll and share the results
[00:34:25.400 --> 00:34:26.860]   just before we do this.
[00:34:26.860 --> 00:34:31.160]   Speaking of natural language processing, the controversial question in NLP these days is,
[00:34:31.160 --> 00:34:33.680]   is attention all you need?
[00:34:33.680 --> 00:34:42.240]   So that is, is the attention mechanism, which is used to sort of learn a kind of short-term
[00:34:42.240 --> 00:34:47.400]   memory applicable to natural language, is this the only thing that you need?
[00:34:47.400 --> 00:34:49.080]   It's what powered GPT-3.
[00:34:49.080 --> 00:34:53.280]   I was chatting with one of the authors of GPT-3 this weekend, and we had a big fight
[00:34:53.280 --> 00:34:54.920]   about this.
[00:34:54.920 --> 00:34:59.040]   And so he believes attention probably is very close to all you need.
[00:34:59.040 --> 00:35:05.480]   It looks like from our results that the people who are familiar with this question on it
[00:35:05.480 --> 00:35:09.040]   by a five to one ratio think it is not.
[00:35:09.040 --> 00:35:12.820]   So thanks everybody for answering my provocative poll.
[00:35:12.820 --> 00:35:14.460]   And thanks, Alexa, for coming in.
[00:35:14.460 --> 00:35:16.360]   Go ahead and take it away.
[00:35:16.360 --> 00:35:18.200]   Yes, just a second.
[00:35:18.200 --> 00:35:19.200]   Let me share my screen.
[00:35:19.200 --> 00:35:20.360]   Okay, here it is.
[00:35:20.360 --> 00:35:22.280]   So hey, everyone.
[00:35:22.280 --> 00:35:26.520]   I'm Alexa and I am co-founder and CEO of Jarvis.
[00:35:26.520 --> 00:35:33.800]   And I will be talking to you about how we made our project management software run with
[00:35:33.800 --> 00:35:40.440]   NLP fast and how we actually don't even need UI that much anymore since we started using
[00:35:40.440 --> 00:35:42.560]   NLP this way.
[00:35:42.560 --> 00:35:46.960]   So first of all, we've all been witnessing some of the big milestones happening, especially
[00:35:46.960 --> 00:35:49.800]   in the last few years in NLP.
[00:35:49.800 --> 00:35:54.360]   And especially since BERT came out, there has been some increased interest in conversational
[00:35:54.360 --> 00:35:56.760]   AI.
[00:35:56.760 --> 00:36:00.420]   And with this increased interest came more NLP in production.
[00:36:00.420 --> 00:36:05.520]   So conversation, it slowly started turning from, is this even possible to how can we
[00:36:05.520 --> 00:36:09.340]   make it better and faster and actually run in production?
[00:36:09.340 --> 00:36:15.280]   So with Jarvis, we've encountered many problems and rated especially to inference speed.
[00:36:15.280 --> 00:36:20.160]   So we had to solve them fast just so we can get them to our users.
[00:36:20.160 --> 00:36:25.120]   And first I will talk a bit about what is Jarvis and why the speed is this important
[00:36:25.120 --> 00:36:26.280]   to us.
[00:36:26.280 --> 00:36:30.820]   So Jarvis automates repetitive parts of project management using natural language processing
[00:36:30.820 --> 00:36:35.940]   and makes getting information in and out of the software available with a single sentence.
[00:36:35.940 --> 00:36:40.000]   And since there is a gap between what employees want, and that is the focus on their work
[00:36:40.000 --> 00:36:46.040]   and what managers want to have as much information to draw conclusions from, this gap becomes
[00:36:46.040 --> 00:36:50.360]   even wider and wider with remote and asynchronous work.
[00:36:50.360 --> 00:36:55.220]   So as we can see on these graphs, what we do is try to take away all the communication
[00:36:55.220 --> 00:37:02.160]   between managers, managers, employees, employees themselves, and go through Jarvis, all of
[00:37:02.160 --> 00:37:09.220]   this communication and data processing so that we can just work remotely and asynchronous.
[00:37:09.220 --> 00:37:14.080]   And we do this with a system of smart notifications from which you can update work through either
[00:37:14.080 --> 00:37:20.320]   text or voice and receive status updates without even slightest needs to go to UI and fill
[00:37:20.320 --> 00:37:21.520]   out forms.
[00:37:21.520 --> 00:37:26.660]   So we analyze all this information and present it to managers in a way that they can find
[00:37:26.660 --> 00:37:29.080]   useful and draw conclusions from.
[00:37:29.080 --> 00:37:33.720]   And when you compare this to the current process, which requires you to manually search, enter,
[00:37:33.720 --> 00:37:39.480]   and submit everything, this requires and requires managers to either make sure everybody did
[00:37:39.480 --> 00:37:46.280]   this or they had to work with incomplete data, we can notice how much time can be saved.
[00:37:46.280 --> 00:37:53.780]   But to do this, we have to have our conversational AI actually work in real time.
[00:37:53.780 --> 00:37:57.720]   And for this, and not only real time, but of course, be accurate.
[00:37:57.720 --> 00:38:03.400]   And for this to be possible, we need both accuracy and speed to be going for us.
[00:38:03.400 --> 00:38:11.000]   So if I'm to be honest, our first version of Jarvis, we were first of all researchers
[00:38:11.000 --> 00:38:16.200]   and then I guess, gotten more into the development.
[00:38:16.200 --> 00:38:21.280]   So we started thinking like researchers and let's see what's the best accuracy we can
[00:38:21.280 --> 00:38:22.280]   get.
[00:38:22.280 --> 00:38:28.920]   So we started just badly when it comes to productionizing that model that we were making.
[00:38:28.920 --> 00:38:33.680]   And we bought into the hype and thought we could do everything with a large, do it all
[00:38:33.680 --> 00:38:39.480]   transformer with some like additional layers, like fine tuning and everything.
[00:38:39.480 --> 00:38:45.160]   And we've written all the codes sequentially, especially like the pre and post processing
[00:38:45.160 --> 00:38:46.160]   code.
[00:38:46.160 --> 00:38:49.540]   So we didn't really care about anything away from that model.
[00:38:49.540 --> 00:38:54.960]   So we were just being like, okay, we will build one big model with all the data that
[00:38:54.960 --> 00:38:57.600]   we have gathered crammed up into it.
[00:38:57.600 --> 00:39:01.280]   And this will give us great result because all of this data is connected and all of these
[00:39:01.280 --> 00:39:03.240]   tasks are connected.
[00:39:03.240 --> 00:39:09.280]   And we did get good accuracy, but our whole process from users sending the message to
[00:39:09.280 --> 00:39:12.920]   them receiving the response, it took around 10 seconds.
[00:39:12.920 --> 00:39:14.920]   And that's only for one conversation.
[00:39:14.920 --> 00:39:20.240]   If there are like concurrent conversations going on, this request takes even longer and
[00:39:20.240 --> 00:39:23.200]   performance just started rapidly going down.
[00:39:23.200 --> 00:39:27.880]   So we knew we had to improve this and that we couldn't do like full production with this
[00:39:27.880 --> 00:39:29.400]   type of model.
[00:39:29.400 --> 00:39:34.640]   And I mean, first thing that comes to our mind in this case is just, we were testing
[00:39:34.640 --> 00:39:36.400]   this out on CPU.
[00:39:36.400 --> 00:39:41.480]   Let's see how it works on GPU and see if this is a good enough option.
[00:39:41.480 --> 00:39:48.480]   And it did improve, but not nearly enough for this option to be viable because it will
[00:39:48.480 --> 00:39:53.280]   be a lot expensive since we would still have to have a lot of GPUs for a lot of concurrent
[00:39:53.280 --> 00:39:55.040]   conversations.
[00:39:55.040 --> 00:39:59.920]   And we also like try to code things a little differently, like factorizing the code wherever
[00:39:59.920 --> 00:40:05.320]   we can, less comprehensive generators, just trying to follow, I guess, good practices
[00:40:05.320 --> 00:40:08.760]   when it comes to running the Python code faster.
[00:40:08.760 --> 00:40:16.680]   But the thing is that we were using too much data science where we didn't have to, and
[00:40:16.680 --> 00:40:18.760]   especially machine learning.
[00:40:18.760 --> 00:40:23.160]   And we weren't specializing parts of our system enough.
[00:40:23.160 --> 00:40:28.280]   We wanted to do everything sequentially going to our big model that works so great.
[00:40:28.280 --> 00:40:32.200]   And I mean, everybody gets excited when they get this one big model that can do so many
[00:40:32.200 --> 00:40:37.900]   things, but we knew we couldn't get this to users and get them to use it this way.
[00:40:37.900 --> 00:40:44.480]   So we had to stay back and think how can we redesign the system to work better for us?
[00:40:44.480 --> 00:40:48.040]   And the little things that we didn't care about during the research really started to
[00:40:48.040 --> 00:40:52.340]   matter when it comes to using this model in production.
[00:40:52.340 --> 00:41:00.080]   So wherever it didn't occur to the accuracy, what we started doing is breaking down the
[00:41:00.080 --> 00:41:03.420]   models into atomic inference.
[00:41:03.420 --> 00:41:12.600]   So what I mean by that is we actually wanted one model to do its specific thing.
[00:41:12.600 --> 00:41:15.480]   And the lowest possible specific thing.
[00:41:15.480 --> 00:41:21.920]   If we can boil it down to simple classification, that's what we are just going with.
[00:41:21.920 --> 00:41:27.880]   And we a bit stepped away from deep learning for some of the models.
[00:41:27.880 --> 00:41:33.160]   Some of them, of course, still have to use it, especially those BERT-based models.
[00:41:33.160 --> 00:41:40.920]   And in some parts, we even totally stood away from machine learning data science.
[00:41:40.920 --> 00:41:44.280]   And some we just did like statistical models, let's say.
[00:41:44.280 --> 00:41:50.840]   And what it turns out is not that it only made it faster, but it also made it a bit
[00:41:50.840 --> 00:41:57.240]   more accurate when we use statistical models where we can.
[00:41:57.240 --> 00:42:00.760]   But I mean, of course, when there is context involved in everything, we can't.
[00:42:00.760 --> 00:42:07.400]   But for example, if we have to match closest name to the one that was misspelled, statistical
[00:42:07.400 --> 00:42:09.480]   models will still battle on that.
[00:42:09.480 --> 00:42:15.960]   And unless we want to do complete -- like for English names, we can do that.
[00:42:15.960 --> 00:42:21.120]   But for some names that aren't that common in data sets, we had to do some statistical
[00:42:21.120 --> 00:42:22.120]   models.
[00:42:22.120 --> 00:42:24.280]   And the second thing we did is just what I mentioned.
[00:42:24.280 --> 00:42:29.440]   So move away from deep learning when needed and when possible.
[00:42:29.440 --> 00:42:33.680]   We also started harnessing power of concurrency.
[00:42:33.680 --> 00:42:36.960]   And this is where a lot of time has to be spent.
[00:42:36.960 --> 00:42:41.280]   Just thinking which parts can actually be done concurrently.
[00:42:41.280 --> 00:42:45.480]   Because some parts are -- do have like some sort of relationship where you have to do
[00:42:45.480 --> 00:42:47.240]   them sequentially.
[00:42:47.240 --> 00:42:53.720]   But it's actually much more common that you can do concurrent things that just speed up
[00:42:53.720 --> 00:42:55.200]   things a lot.
[00:42:55.200 --> 00:43:01.000]   And in our case, we also moved away from Python to go to Go for non-machine learning stuff.
[00:43:01.000 --> 00:43:03.840]   Of course, for machine learning, we still kept Python.
[00:43:03.840 --> 00:43:10.000]   And this improves our speed incredibly because it harnessed innate concurrency that Go has.
[00:43:10.000 --> 00:43:15.200]   But if somebody is more comfortable with Python, there are also like Cython and some other
[00:43:15.200 --> 00:43:23.600]   solutions that speed up execution in Python without having to change the language.
[00:43:23.600 --> 00:43:28.900]   So with all this in place, we had already improved our model considerably.
[00:43:28.900 --> 00:43:33.640]   But as number of concurrent requests grew, we wanted to do something more just in case.
[00:43:33.640 --> 00:43:39.160]   So we wouldn't fall again into the same thing that we had before where we thought we were
[00:43:39.160 --> 00:43:40.160]   good enough.
[00:43:40.160 --> 00:43:44.640]   But then it turns out when the number of requests increases, we are not.
[00:43:44.640 --> 00:43:54.520]   So what we wanted to just check out, how can we actually build this inference to be better?
[00:43:54.520 --> 00:44:00.560]   So until now, we have building everything around the models themselves to be better.
[00:44:00.560 --> 00:44:02.480]   And like breaking them down and everything.
[00:44:02.480 --> 00:44:08.320]   But we wanted to see if we can also make those models work faster and infer everything that
[00:44:08.320 --> 00:44:10.400]   we've trained them faster.
[00:44:10.400 --> 00:44:16.360]   And some of the things that we tried next, first, we started using TVM on AWS.
[00:44:16.360 --> 00:44:21.880]   And this helped model inference speed, but we wanted to see if we can push it further.
[00:44:21.880 --> 00:44:27.260]   So we tried ONNX Runtime, which turned out to have like massive impact on us.
[00:44:27.260 --> 00:44:32.360]   And for those who might not be familiar with this, ONNX is a format representing ML models
[00:44:32.360 --> 00:44:37.800]   and the ONNX Runtime is a high performance engine for running these models.
[00:44:37.800 --> 00:44:42.420]   And it's better like our inference drastically.
[00:44:42.420 --> 00:44:49.340]   So much so that right now, for each of our models, it's around 10 milliseconds inference.
[00:44:49.340 --> 00:44:54.520]   And for some of them, it's in like single digits for some of the simpler models.
[00:44:54.520 --> 00:44:59.280]   And this makes the whole conversation look completely real time.
[00:44:59.280 --> 00:45:05.400]   Even if we have a lot of pre-processing and post-processing after that inference, it still
[00:45:05.400 --> 00:45:09.440]   helps a lot to make it like completely real time.
[00:45:09.440 --> 00:45:13.920]   And another thing that we would like to try soon as we start implementing, especially
[00:45:13.920 --> 00:45:20.320]   like document summarization, question answering, and some more complex feature is a TensorRT
[00:45:20.320 --> 00:45:23.920]   to see if this can speed us up even more.
[00:45:23.920 --> 00:45:29.520]   And if someone has good insight on how this works with transformers models, please do
[00:45:29.520 --> 00:45:35.960]   tell us so we would know if we should pursue it further.
[00:45:35.960 --> 00:45:38.080]   And yeah, thank you for attention.
[00:45:38.080 --> 00:45:42.680]   And if you would like to hear more about Jervis or chat about machine learning and conversational
[00:45:42.680 --> 00:45:49.000]   AI in general, you can reach out either by email or my Twitter, @borderstater here.
[00:45:49.000 --> 00:45:52.000]   And yeah, now I guess we're moving to QA.
[00:45:52.000 --> 00:45:54.120]   Yeah, great.
[00:45:54.120 --> 00:45:55.920]   Thanks a lot for the talk, Alexa.
[00:45:55.920 --> 00:46:02.320]   And yeah, I encourage folks who are in the Zoom or on the YouTube, in the YouTube chat
[00:46:02.320 --> 00:46:04.400]   to post their questions as we're going.
[00:46:04.400 --> 00:46:05.600]   I've got lots of questions.
[00:46:05.600 --> 00:46:07.000]   I'm very excited to ask them.
[00:46:07.000 --> 00:46:12.560]   So the first thing I'd like to ask is sort of towards the end there, more of a detail
[00:46:12.560 --> 00:46:14.120]   oriented question.
[00:46:14.120 --> 00:46:17.600]   But why did you choose the ONNX format?
[00:46:17.600 --> 00:46:21.680]   Like what made you look into that originally?
[00:46:21.680 --> 00:46:26.720]   And also what do you think is the secret behind why I was able to give you that big performance
[00:46:26.720 --> 00:46:27.720]   boost?
[00:46:27.720 --> 00:46:36.400]   Yeah, so it wasn't like the format itself, but like it's actually the ONNX runtime, which
[00:46:36.400 --> 00:46:44.760]   Microsoft developed and they especially updated it like a year ago, I think for BERT, which
[00:46:44.760 --> 00:46:46.000]   we are using underneath.
[00:46:46.000 --> 00:46:51.720]   So they even like, I mean, this was in research settings in like their lab settings, but they
[00:46:51.720 --> 00:46:57.800]   got like nearly 20 times improvement on both CPU and GPU.
[00:46:57.800 --> 00:46:59.080]   So that got us going.
[00:46:59.080 --> 00:47:04.640]   We didn't get, let's say that much, but we still got like five to 10 times X, which is
[00:47:04.640 --> 00:47:05.960]   pretty, pretty good.
[00:47:05.960 --> 00:47:08.760]   And we're right now able to even run it on CPU.
[00:47:08.760 --> 00:47:12.960]   Like we don't need any GPUs for the inference.
[00:47:12.960 --> 00:47:13.960]   I see.
[00:47:13.960 --> 00:47:20.000]   Yeah, because I guess, so I have come across the ONNX format only as just sort of this
[00:47:20.000 --> 00:47:24.120]   attempt to get everybody to play nicely with each other by sort of some of the people coming
[00:47:24.120 --> 00:47:25.200]   into the field later, right?
[00:47:25.200 --> 00:47:28.720]   You've got TensorFlow and PyTorch both have their solutions.
[00:47:28.720 --> 00:47:32.980]   And it seems like a lot of other people have been like, okay, can we agree on an open format?
[00:47:32.980 --> 00:47:37.360]   So it's actually, this is great news to me to hear that the open format can also bring
[00:47:37.360 --> 00:47:40.960]   you these performance benefits.
[00:47:40.960 --> 00:47:45.720]   Yeah, so I mean, Microsoft is the one pushing that format, especially.
[00:47:45.720 --> 00:47:49.880]   So that's probably why they created this runtime.
[00:47:49.880 --> 00:47:54.760]   But when we like researched a bit deeper, this runtime really, really helps.
[00:47:54.760 --> 00:47:55.760]   Great.
[00:47:55.760 --> 00:47:56.760]   That's cool.
[00:47:56.760 --> 00:48:04.080]   That's a nice detail about operating these things that I was not aware of.
[00:48:04.080 --> 00:48:08.160]   You mentioned being able to deploy just the CPU being important.
[00:48:08.160 --> 00:48:10.920]   Is that mostly just to reduce cloud costs?
[00:48:10.920 --> 00:48:15.840]   Or is this something that you can see maybe running on unspecialized hardware in phones
[00:48:15.840 --> 00:48:17.160]   or something like that?
[00:48:17.160 --> 00:48:21.960]   Yeah, so we right now are doing this on AWS.
[00:48:21.960 --> 00:48:25.800]   And we're just trying to reduce the cost as much as possible.
[00:48:25.800 --> 00:48:32.080]   And I mean, we didn't even need that last steps that I like put the like the last slide,
[00:48:32.080 --> 00:48:35.360]   we still have it like real time even without that.
[00:48:35.360 --> 00:48:40.360]   So when we implemented this, this made it like it gave us a lot of space to just go
[00:48:40.360 --> 00:48:44.240]   completely away from GPU for now at least.
[00:48:44.240 --> 00:48:49.600]   But yeah, at some point, it would be great to just be able to also do it on the phone
[00:48:49.600 --> 00:48:55.820]   directly, so that you don't have to actually have an internet connection to like, I mean,
[00:48:55.820 --> 00:48:59.920]   if you're on the airplane, and you can do you're just remember something so that you
[00:48:59.920 --> 00:49:04.160]   can do it right away, and then it gets uploaded when you get connected to the Wi Fi.
[00:49:04.160 --> 00:49:06.640]   So that is something that we are looking into.
[00:49:06.640 --> 00:49:07.640]   I see.
[00:49:07.640 --> 00:49:11.600]   Yeah, you also mentioned the sort of one of the pieces of the transition from your original
[00:49:11.600 --> 00:49:16.840]   application to the second app, like more performant one was you move from one like single almighty
[00:49:16.840 --> 00:49:19.160]   transformer model to many small models.
[00:49:19.160 --> 00:49:23.720]   So this is something that it seems to have been coming up like quite a bit in a lot of
[00:49:23.720 --> 00:49:28.120]   the conversations I've been having with folks on the salon, that people are moving over
[00:49:28.120 --> 00:49:32.800]   to these multi model model amalgamation type pipelines.
[00:49:32.800 --> 00:49:36.400]   So I was just wondering if you'd comment a little bit more about that process, what worked
[00:49:36.400 --> 00:49:41.280]   well, what didn't, what were the pain points and the tools that you use to just to switch
[00:49:41.280 --> 00:49:45.720]   over your your pipeline, because it's quite a different thing to manage a zoo of models
[00:49:45.720 --> 00:49:47.080]   than a single one.
[00:49:47.080 --> 00:49:48.080]   Yeah, yeah.
[00:49:48.080 --> 00:49:55.440]   So at first, let's say, we saw that a lot of people in the beginning, since we started
[00:49:55.440 --> 00:49:58.240]   working on this in the early 2019.
[00:49:58.240 --> 00:50:04.520]   So a lot of people at that time, were just taking it straight away, it was probably hype,
[00:50:04.520 --> 00:50:07.360]   but everybody was like, yeah, this is a silver bullet.
[00:50:07.360 --> 00:50:10.120]   So we were, we were like, kind of onto that hype.
[00:50:10.120 --> 00:50:17.040]   And then we saw that, I think Sonos acquired the company for like 50 million did use, especially
[00:50:17.040 --> 00:50:18.520]   like just one model.
[00:50:18.520 --> 00:50:21.840]   And like, they used one joint model for like a bunch of NLP tasks.
[00:50:21.840 --> 00:50:27.080]   And we were like, you know, if they can build it that good to be actually acquired, I mean,
[00:50:27.080 --> 00:50:28.500]   there must be something there.
[00:50:28.500 --> 00:50:35.600]   So we started just going straight to the joint model that does like a bunch of other tasks.
[00:50:35.600 --> 00:50:41.640]   But like, as we went going on about it, we noticed that a lot of those things can be
[00:50:41.640 --> 00:50:43.960]   done concurrently, like concurrently.
[00:50:43.960 --> 00:50:50.440]   So what instead of actually just going everything sequentially to that one model, we started
[00:50:50.440 --> 00:50:56.840]   building a lot of models that can be done in different like timestamps.
[00:50:56.840 --> 00:51:03.320]   And we did like, I guess we did have to work a bit more on the architecture, since we have
[00:51:03.320 --> 00:51:08.680]   to have like, we have gRPC communicating between those microservices and everything.
[00:51:08.680 --> 00:51:16.160]   So we did have to do a bit more architecture designing in it and just implementation details.
[00:51:16.160 --> 00:51:23.280]   But it helped us even with all the networking costs between those models, it still helped
[00:51:23.280 --> 00:51:26.080]   us a lot with the speed.
[00:51:26.080 --> 00:51:27.080]   I see.
[00:51:27.080 --> 00:51:31.920]   But that is, I guess, the biggest thing I see as somebody who's more of a like machine
[00:51:31.920 --> 00:51:36.400]   learning researcher than a machine learning engineer, or an ML ops person, that seems
[00:51:36.400 --> 00:51:40.720]   like a pretty big hurdle to come over this like ops problem of integrating that whole
[00:51:40.720 --> 00:51:42.380]   pipeline.
[00:51:42.380 --> 00:51:49.560]   So it seemed at first actually, but it's not like once you build a communication between
[00:51:49.560 --> 00:51:55.100]   two services, the rest become like incrementally simpler.
[00:51:55.100 --> 00:52:01.240]   So there's a lot of code that you can basically just copy out, that's like for the communication,
[00:52:01.240 --> 00:52:05.240]   and we completely removed states from most of these things.
[00:52:05.240 --> 00:52:10.000]   So we didn't actually have to save states at any point, it was just basically kicking
[00:52:10.000 --> 00:52:11.160]   the states around.
[00:52:11.160 --> 00:52:15.640]   So a bunch of codes was similar, just receiving the state, then packing it up for the next
[00:52:15.640 --> 00:52:16.640]   thing.
[00:52:16.640 --> 00:52:22.200]   So even though it seems like at first is a big hurdle to overcome, it's actually not.
[00:52:22.200 --> 00:52:28.360]   We did it in less than a month, just switching to that.
[00:52:28.360 --> 00:52:33.000]   So that's kind of related to another broader question that I wanted to ask.
[00:52:33.000 --> 00:52:37.640]   You mentioned that early on, you were effectively doing too much data science, too much machine
[00:52:37.640 --> 00:52:39.240]   learning, right?
[00:52:39.240 --> 00:52:44.000]   So how do we take some of these, and you eventually learned the lesson to kind of pare that back,
[00:52:44.000 --> 00:52:45.000]   right?
[00:52:45.000 --> 00:52:50.340]   So how do we take that lesson and the lesson of it's a good idea to have multiple smaller
[00:52:50.340 --> 00:52:54.600]   services communicating with each other, and sort of incorporate them maybe earlier in
[00:52:54.600 --> 00:52:55.600]   our workflow?
[00:52:55.600 --> 00:53:00.400]   Because when I talk to people, it seems like a lot of them have that same trajectory of
[00:53:00.400 --> 00:53:05.560]   pairing things away and adding more of a complicated MLOps structure.
[00:53:05.560 --> 00:53:08.560]   So what do you think we can do to get those earlier in our development cycle?
[00:53:08.560 --> 00:53:14.520]   Yeah, so I mean, for us, it was just, I guess, trying it out and seeing what works, what
[00:53:14.520 --> 00:53:19.520]   doesn't, and probably everything won't work the same for everybody.
[00:53:19.520 --> 00:53:25.880]   But just trying to think, what can you break down as for the multiple, which parts can
[00:53:25.880 --> 00:53:29.120]   you break down into simpler problems?
[00:53:29.120 --> 00:53:33.360]   You don't have to have a silver bullet, and you usually can't have a silver bullet for
[00:53:33.360 --> 00:53:34.360]   everything.
[00:53:34.360 --> 00:53:40.880]   So just trying to think what can you and how can you break down this problem into much
[00:53:40.880 --> 00:53:43.000]   more smaller problems.
[00:53:43.000 --> 00:53:48.320]   And then also, are those smaller problems actually best solved this way?
[00:53:48.320 --> 00:53:58.200]   So don't think, just try not to go with the feeling, let's say, but actually the reasonable
[00:53:58.200 --> 00:53:59.760]   explanation.
[00:53:59.760 --> 00:54:03.360]   So we were like, yeah, machine learning is totally going to kill it here.
[00:54:03.360 --> 00:54:09.400]   But it turns out that it doesn't, and statistical models are much better matching misspelled
[00:54:09.400 --> 00:54:14.960]   names than any deep learning, or even basic machine learning model.
[00:54:14.960 --> 00:54:15.960]   So just-
[00:54:15.960 --> 00:54:19.120]   So just a quick clarification, when you say statistical models, are you talking about
[00:54:19.120 --> 00:54:23.040]   things like, I don't know, like hidden Markov models for language?
[00:54:23.040 --> 00:54:29.000]   Or do you just mean like, yeah, like forming a probability distribution over tokens?
[00:54:29.000 --> 00:54:31.200]   Like, what are you talking about here?
[00:54:31.200 --> 00:54:36.940]   Yeah, like what we used was Gerard Winkler, or like Levenstein's distance.
[00:54:36.940 --> 00:54:43.840]   So something that's, yeah, that's not really, it's not even, I guess, a complete model.
[00:54:43.840 --> 00:54:52.000]   It's just computing what's the closest to something else in some sort of way.
[00:54:52.000 --> 00:54:53.000]   I see.
[00:54:53.000 --> 00:54:54.000]   Yeah.
[00:54:54.000 --> 00:54:57.200]   So often people say that ML can replace the need for hand-engineered features, right?
[00:54:57.200 --> 00:55:00.720]   So like the SIFT features or Laplacian pyramids for images.
[00:55:00.720 --> 00:55:03.200]   I know a little bit more about images than I do about NLP.
[00:55:03.200 --> 00:55:05.640]   So those are the examples that immediately come to mind.
[00:55:05.640 --> 00:55:10.200]   Like those have been effectively replaced in a lot of models by like the first couple
[00:55:10.200 --> 00:55:12.600]   layers of a conf net, essentially.
[00:55:12.600 --> 00:55:16.760]   But that doesn't mean that there aren't tons of places where actually doing something like
[00:55:16.760 --> 00:55:21.200]   that and pulling out those nice features can get you really good performance on one part
[00:55:21.200 --> 00:55:22.800]   of what your model needs to do.
[00:55:22.800 --> 00:55:24.280]   Yeah, yeah, exactly.
[00:55:24.280 --> 00:55:29.460]   And I actually started as a computer, like vision engineer as well.
[00:55:29.460 --> 00:55:36.880]   So when it comes to that part, even though we're like for hand gestures, we did use deep
[00:55:36.880 --> 00:55:46.920]   learning, but like for some kind of, to actually detect some things like movement or something
[00:55:46.920 --> 00:55:50.460]   like that, we used a bit simpler models.
[00:55:50.460 --> 00:55:54.760]   So I mean, it was like five years ago, so I'm not that like great remembering exact
[00:55:54.760 --> 00:56:00.520]   details, but I actually do have like a patent for a shelf during that time.
[00:56:00.520 --> 00:56:05.080]   But yeah, I'm much more into natural language processing right now.
[00:56:05.080 --> 00:56:09.320]   Yeah, so I guess I did want to get a little bit of your opinions on what's going on in
[00:56:09.320 --> 00:56:12.920]   natural processing these days, natural language processing these days.
[00:56:12.920 --> 00:56:19.480]   So the like the Zeitgeist now seems to be like transformers, transformers are it.
[00:56:19.480 --> 00:56:23.560]   We just need to get like the attention mechanism, we need to find a way to make that efficient.
[00:56:23.560 --> 00:56:30.120]   Then we need to scale that up, work with internet scale data, like, you know, GPT-3 is the move.
[00:56:30.120 --> 00:56:31.280]   Now we just need to make it smaller.
[00:56:31.280 --> 00:56:35.280]   Do you think this is a good direction for the NLP community to move in?
[00:56:35.280 --> 00:56:37.480]   Do you think this is the right one?
[00:56:37.480 --> 00:56:41.080]   So I think it depends on what actually you want NLP to do.
[00:56:41.080 --> 00:56:48.200]   So if you need NLP to just analyze the text, extract some information from it, and like
[00:56:48.200 --> 00:56:53.760]   just those, that sort of things that the transformer can be really great, but just like you said,
[00:56:53.760 --> 00:56:57.280]   like getting more data, making it more sophisticated and everything.
[00:56:57.280 --> 00:57:01.640]   But I don't think it's a solution to a broader problem of language.
[00:57:01.640 --> 00:57:08.760]   So I don't think that actually it can solve communication that isn't, let's say, even
[00:57:08.760 --> 00:57:10.560]   like you don't know what's going to happen.
[00:57:10.560 --> 00:57:17.520]   It's not really predefined, but it's not like an actual conversation going on since like
[00:57:17.520 --> 00:57:22.760]   it can just spew out anything that it read from the internet, which can be good, bad,
[00:57:22.760 --> 00:57:23.760]   terrible.
[00:57:23.760 --> 00:57:26.540]   So whatever is the most common connection with something else.
[00:57:26.540 --> 00:57:32.200]   So it's not like, it's not reason communication per se.
[00:57:32.200 --> 00:57:34.920]   I see.
[00:57:34.920 --> 00:57:38.560]   So do you think that that is something, some of that comes from the choice of using the
[00:57:38.560 --> 00:57:42.720]   cross entropy loss, this sort of like contrastive learning thing where you're trying to say,
[00:57:42.720 --> 00:57:46.960]   okay, like predict this next token by using all these negative examples of like, it's
[00:57:46.960 --> 00:57:51.160]   not, you know, masking things, all these kinds of approaches.
[00:57:51.160 --> 00:57:56.360]   So do you think, do you think that's an architectural limitation or do you think that's a problems
[00:57:56.360 --> 00:57:59.360]   specification limitation there?
[00:57:59.360 --> 00:58:05.400]   So for me, I think it's actually like the problem specification since it's, it's completely
[00:58:05.400 --> 00:58:15.700]   different to have some sort of real reasoning to get some real reasoning from that.
[00:58:15.700 --> 00:58:22.020]   What you can do is analyze a lot of things that humans already, like you can use tools
[00:58:22.020 --> 00:58:30.980]   that humans already use to pack it up and get something new there, but you can't actually,
[00:58:30.980 --> 00:58:33.860]   at least for me, can't build totally out of the box.
[00:58:33.860 --> 00:58:41.020]   Like it can be something that satisfies the closest possible thing you want to get to,
[00:58:41.020 --> 00:58:47.940]   but not nothing like, so I'm not sure like how to explain where I'm going at, but like
[00:58:47.940 --> 00:58:52.980]   in manufacturing, you see like those shapes that get really good results and everything,
[00:58:52.980 --> 00:58:57.220]   but they're just trying to satisfy a goal that was given to them, but they can like
[00:58:57.220 --> 00:59:02.260]   really think out of the box and be like, maybe we don't actually need this thing at all.
[00:59:02.260 --> 00:59:04.620]   Maybe we can build something completely different.
[00:59:04.620 --> 00:59:06.580]   Like that, that is lacking for me.
[00:59:06.580 --> 00:59:07.580]   I see.
[00:59:07.580 --> 00:59:08.580]   Yeah.
[00:59:08.580 --> 00:59:09.580]   So this, yeah.
[00:59:09.580 --> 00:59:12.660]   Related to the sort of problem of, you know, out of distribution, generalization, sort
[00:59:12.660 --> 00:59:17.560]   of rule extraction, things that people are finding computer vision models approaching
[00:59:17.560 --> 00:59:26.220]   like the problem of self-driving have also run into like that kind of difficulty or issue.
[00:59:26.220 --> 00:59:27.220]   Pretty much.
[00:59:27.220 --> 00:59:31.140]   I haven't like been researching that much self-driving per se.
[00:59:31.140 --> 00:59:35.100]   Like, I mean, I think self-driving can work great if a lot of self-driving cars are out
[00:59:35.100 --> 00:59:39.260]   there and they communicate between themselves and everything is great.
[00:59:39.260 --> 00:59:44.460]   But I haven't researched enough to talk about like real world scenario where there's like
[00:59:44.460 --> 00:59:51.080]   a bunch of random things going on, but in natural language and like in manufacturing,
[00:59:51.080 --> 00:59:57.540]   like I said, it can like build the best goal, like build the best box for this, but it can't
[00:59:57.540 --> 01:00:00.540]   say you may, might not need this box at all.
[01:00:00.540 --> 01:00:03.340]   Just put this box away and do something else.
[01:00:03.340 --> 01:00:05.940]   That's something that I don't think is still available.
[01:00:05.940 --> 01:00:06.940]   Yeah.
[01:00:06.940 --> 01:00:07.940]   That makes sense.
[01:00:07.940 --> 01:00:12.780]   At least, you know, for the, in the early days of computer chess, I don't know if this
[01:00:12.780 --> 01:00:18.620]   remains true, but some of the, even after computers beat humans, the best solution was
[01:00:18.620 --> 01:00:25.980]   a sort of like human computer hybrid, not in the Deus Ex, you know, or, you know, cyborg
[01:00:25.980 --> 01:00:31.180]   sense, but in the, like just two people, like a computer and a person working together.
[01:00:31.180 --> 01:00:34.580]   And so it seems like that's kind of the sort of thing that you're going with Jarvis in
[01:00:34.580 --> 01:00:42.740]   that there's a step of automation of information extraction and collection, but the whole process,
[01:00:42.740 --> 01:00:44.340]   the whole process is not being automated, right?
[01:00:44.340 --> 01:00:49.060]   The whole process of managing employees, of managing a project is not automated.
[01:00:49.060 --> 01:00:54.460]   It's still, it's an adjunct, it's a tool that aids somebody who is, you know, an individual
[01:00:54.460 --> 01:00:58.060]   human making those kinds of decisions and capable of that kind of reasoning.
[01:00:58.060 --> 01:00:59.340]   Yeah, exactly.
[01:00:59.340 --> 01:01:04.540]   It's about improving like performance of those people so they can focus on something more
[01:01:04.540 --> 01:01:10.220]   important and like, like some, some different example is just in medical field.
[01:01:10.220 --> 01:01:15.900]   There are some like medical images that machine learning models can just find some things
[01:01:15.900 --> 01:01:21.340]   that doctors can't, but then on the other way, there, there is something that only doctors
[01:01:21.340 --> 01:01:22.340]   can see.
[01:01:22.340 --> 01:01:26.820]   So it can, it can work like best if it's a hybrid thing.
[01:01:26.820 --> 01:01:27.820]   Yeah.
[01:01:27.820 --> 01:01:37.460]   Well, I think actually I have one more question for you, which is, so you're in, have you
[01:01:37.460 --> 01:01:42.820]   been using weights and biases as part of your natural language processing model building
[01:01:42.820 --> 01:01:44.420]   toolkit?
[01:01:44.420 --> 01:01:46.220]   And if so, like what have you found?
[01:01:46.220 --> 01:01:47.220]   What have you found useful?
[01:01:47.220 --> 01:01:48.220]   What have you been using?
[01:01:48.220 --> 01:01:49.220]   Yeah.
[01:01:49.220 --> 01:01:56.500]   So I have been using it and I started actually using it a while ago when we started using
[01:01:56.500 --> 01:02:02.240]   that one big model and it started like, and it started at first because it took a lot
[01:02:02.240 --> 01:02:07.660]   of time to train that big model and then you had to do like a bunch of different tuning
[01:02:07.660 --> 01:02:10.900]   parameters like either write code yourself.
[01:02:10.900 --> 01:02:16.440]   So back then I think it was like mostly just that thing that, that helped the most.
[01:02:16.440 --> 01:02:18.340]   So that's how I started using it.
[01:02:18.340 --> 01:02:21.700]   And I still find just sweeps.
[01:02:21.700 --> 01:02:25.980]   They're really, really best to think about it because it just goes through every single
[01:02:25.980 --> 01:02:31.340]   possibility and logs everything that's happening during that process.
[01:02:31.340 --> 01:02:36.860]   And especially like utilization of CPU, GPU and stuff like that was helpful for us because
[01:02:36.860 --> 01:02:44.760]   we can see if like during, like during that case, would we need like some sort of a change
[01:02:44.760 --> 01:02:45.760]   in, in the future?
[01:02:45.760 --> 01:02:46.760]   Great.
[01:02:46.760 --> 01:02:47.760]   Yeah.
[01:02:47.760 --> 01:02:50.620]   I'm glad to hear that the tool was useful.
[01:02:50.620 --> 01:02:56.440]   I think the artifacts toolkit is maybe newer than when you were working on that particular
[01:02:56.440 --> 01:02:57.440]   part of the project.
[01:02:57.440 --> 01:03:01.920]   So it's really helpful for orchestrating that model model pipeline where, or the end where
[01:03:01.920 --> 01:03:05.800]   you have things that are not just deep learning models that you want to, you know, combine
[01:03:05.800 --> 01:03:10.360]   together and, and communicate with one another.
[01:03:10.360 --> 01:03:16.280]   But yeah, well thanks so much Alexa for staying up late over there in Serbia to be able to
[01:03:16.280 --> 01:03:20.440]   come on and talk with folks and answer questions live.
[01:03:20.440 --> 01:03:23.100]   And it's a really cool and exciting tool.
[01:03:23.100 --> 01:03:28.480]   And I look forward to seeing you know, how you guys grow and and, and make great use
[01:03:28.480 --> 01:03:29.480]   of it.
[01:03:29.480 --> 01:03:30.480]   Yeah.
[01:03:30.480 --> 01:03:31.480]   Thanks so much for having me.
[01:03:31.480 --> 01:03:32.480]   This was really fun.
[01:03:32.480 --> 01:03:36.940]   And it was also fun to hear from Richard what he's doing and like, I mean, just that sort
[01:03:36.940 --> 01:03:43.080]   of kind of competition, but it's also like you earn, even if you don't like completely
[01:03:43.080 --> 01:03:46.320]   win it's, it's, it's really interesting thing to hear.
[01:03:46.320 --> 01:03:47.320]   Yeah.
[01:03:47.320 --> 01:03:48.680]   Well thanks for coming Alexa.
[01:03:48.680 --> 01:03:54.120]   And thanks to our audience on zoom and on YouTube for coming by in.
[01:03:54.120 --> 01:04:00.000]   I hope to see you all again in two weeks when I'll be giving a talk and Chirag Agarwal of
[01:04:00.000 --> 01:04:04.520]   Harvard will be here to talk about his variants of gradient paper.

