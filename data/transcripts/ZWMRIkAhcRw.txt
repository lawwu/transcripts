
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:08.200]   - Hi, I'm Carrie Phelps, Director of Product.
[00:00:08.200 --> 00:00:10.720]   At Weights & Biases, our mission is to build
[00:00:10.720 --> 00:00:13.080]   the best tools for machine learning.
[00:00:13.080 --> 00:00:15.880]   We're constantly getting new insights
[00:00:15.880 --> 00:00:19.680]   from the real world ML teams who are using WMB
[00:00:19.680 --> 00:00:22.360]   to get better models in production.
[00:00:22.360 --> 00:00:24.680]   We worked with experts at OpenAI
[00:00:24.680 --> 00:00:26.940]   and other industry leading teams
[00:00:26.940 --> 00:00:29.960]   to understand their key pain points.
[00:00:29.960 --> 00:00:33.820]   Every industry is starting to build machine learning teams
[00:00:33.820 --> 00:00:35.840]   in every vertical.
[00:00:35.840 --> 00:00:38.920]   This diagram is from a few years ago.
[00:00:38.920 --> 00:00:42.980]   The latest diagram of companies using ML in the real world
[00:00:42.980 --> 00:00:46.180]   is so large, it hardly fits on this slide.
[00:00:46.180 --> 00:00:50.180]   The industry is exploding,
[00:00:50.180 --> 00:00:53.960]   but standardized tools and workflows aren't keeping up.
[00:00:53.960 --> 00:00:58.700]   There's a tectonic shift between the software workflow
[00:00:58.700 --> 00:01:00.100]   and the ML workflow.
[00:01:00.100 --> 00:01:04.980]   There are tools for every stage of software development,
[00:01:04.980 --> 00:01:09.680]   but there are no clear standard tools for the ML workflow.
[00:01:09.680 --> 00:01:14.100]   We're missing a standardized industry process
[00:01:14.100 --> 00:01:17.980]   for moving models from training through production.
[00:01:17.980 --> 00:01:21.880]   What does a sample machine learning workflow look like?
[00:01:21.880 --> 00:01:25.400]   In the data curation stage,
[00:01:25.400 --> 00:01:27.480]   teams are starting with collecting
[00:01:27.480 --> 00:01:29.160]   and labeling raw data,
[00:01:29.160 --> 00:01:33.640]   then exploring and cleaning the data.
[00:01:33.640 --> 00:01:37.200]   Then they're preparing and transforming that data
[00:01:37.200 --> 00:01:39.280]   into trainable sets.
[00:01:39.280 --> 00:01:41.960]   There is critical metadata produced
[00:01:41.960 --> 00:01:44.760]   from the process of data curation.
[00:01:44.760 --> 00:01:47.520]   You want to be able to track the raw data,
[00:01:47.520 --> 00:01:49.640]   the pre-processing code.
[00:01:49.640 --> 00:01:53.180]   It's important to see when that pre-processing code changes
[00:01:53.180 --> 00:01:56.160]   because it has implications on the format
[00:01:56.160 --> 00:01:58.280]   and content of the datasets
[00:01:58.280 --> 00:02:00.720]   that then feed into the rest of the pipeline.
[00:02:00.720 --> 00:02:03.880]   You care about the training dataset
[00:02:03.880 --> 00:02:06.900]   that was produced by the dataset pre-processing step.
[00:02:06.900 --> 00:02:11.500]   When you split the data into training and evaluation,
[00:02:11.500 --> 00:02:15.080]   you want to capture the exact evaluation set
[00:02:15.080 --> 00:02:19.720]   so those examples aren't leaked to the model during training.
[00:02:19.720 --> 00:02:23.840]   The next stage after data curation is modeling.
[00:02:23.840 --> 00:02:27.720]   That starts with quick iterative experimentation.
[00:02:27.720 --> 00:02:29.680]   A machine learning practitioner
[00:02:29.680 --> 00:02:31.680]   is hacking around in a notebook,
[00:02:31.680 --> 00:02:34.240]   trying different architectures and hyperparameters
[00:02:34.240 --> 00:02:36.420]   and running lots of experiments.
[00:02:36.420 --> 00:02:41.100]   Then they're evaluating and optimizing those trained models,
[00:02:41.100 --> 00:02:45.720]   seeing how they perform on different held out test sets
[00:02:45.720 --> 00:02:48.920]   and running hyperparameter sweeps to optimize the model
[00:02:48.920 --> 00:02:52.640]   and squeeze out a next level of model performance.
[00:02:52.640 --> 00:02:55.820]   That process also produces data exhaust.
[00:02:55.820 --> 00:02:58.800]   This includes training metrics,
[00:02:58.800 --> 00:03:01.040]   which compares how models perform.
[00:03:01.040 --> 00:03:05.960]   Model checkpoints to capture each stage of training
[00:03:05.960 --> 00:03:10.200]   and associate the exact model weights with the metrics.
[00:03:10.200 --> 00:03:13.880]   Engineered features with custom transformations of raw data
[00:03:13.880 --> 00:03:15.920]   that might change as the team learns more
[00:03:15.920 --> 00:03:19.380]   about the problem space and iterates on features.
[00:03:19.380 --> 00:03:21.960]   The environment where the model was trained,
[00:03:21.960 --> 00:03:25.240]   including hardware, Python version,
[00:03:25.240 --> 00:03:27.240]   any installed requirements.
[00:03:27.240 --> 00:03:31.180]   Terminal logs are often lost to terminal history,
[00:03:31.180 --> 00:03:35.240]   but especially when debugging, it can be very useful.
[00:03:35.240 --> 00:03:37.800]   You want to be able to scroll back for a given model
[00:03:37.800 --> 00:03:41.280]   and view all the command line messages and warnings.
[00:03:41.280 --> 00:03:44.480]   System metrics can help identify opportunities
[00:03:44.480 --> 00:03:46.920]   for optimizing model training.
[00:03:46.920 --> 00:03:49.460]   For example, was this model running out of memory
[00:03:49.460 --> 00:03:50.660]   during training?
[00:03:50.660 --> 00:03:54.240]   Were there issues with the GPU or CPU?
[00:03:54.240 --> 00:03:56.620]   What about network requests?
[00:03:56.620 --> 00:03:59.120]   What were the bottlenecks from the hardware side
[00:03:59.120 --> 00:04:02.160]   that might have affected the model training process?
[00:04:02.160 --> 00:04:03.320]   Training code.
[00:04:03.320 --> 00:04:05.000]   When you edit a hyperparameter
[00:04:05.000 --> 00:04:06.940]   and quickly run a script again,
[00:04:06.940 --> 00:04:09.500]   that change isn't committed to Git.
[00:04:09.500 --> 00:04:13.300]   It can be really hard to get the exact state of the code
[00:04:13.300 --> 00:04:17.520]   if there's not a system picking up uncommitted code changes.
[00:04:17.520 --> 00:04:21.500]   It's critical to have the exact version of training code,
[00:04:21.500 --> 00:04:25.180]   including any uncommitted changes in a diff patch.
[00:04:25.180 --> 00:04:28.220]   Evaluation results help us understand
[00:04:28.220 --> 00:04:31.360]   how does the model perform on different test sets?
[00:04:31.360 --> 00:04:35.700]   Were there two models evaluated on the exact same test set?
[00:04:35.700 --> 00:04:36.920]   Sweeps.
[00:04:36.920 --> 00:04:39.480]   Did someone on the team run a Bayesian search
[00:04:39.480 --> 00:04:41.800]   to find the best hyperparameters?
[00:04:41.800 --> 00:04:43.620]   What combinations of hyperparameters
[00:04:43.620 --> 00:04:44.760]   have we already tried?
[00:04:45.940 --> 00:04:48.140]   Massive sweeps can be expensive,
[00:04:48.140 --> 00:04:50.660]   so it's critical to track what you've tried already
[00:04:50.660 --> 00:04:52.820]   so you're not wasting GPU time.
[00:04:52.820 --> 00:04:56.000]   And finally, we make it to the deployment stage
[00:04:56.000 --> 00:04:57.200]   with our trained model.
[00:04:57.200 --> 00:05:02.180]   First, the model is packaged up and productionized.
[00:05:02.180 --> 00:05:04.980]   That could be the same team of ML practitioners,
[00:05:04.980 --> 00:05:09.100]   or in many cases, that's a new team of ML engineers
[00:05:09.100 --> 00:05:12.520]   focused on reliability and performance in production.
[00:05:13.580 --> 00:05:16.660]   There's also serving and monitoring.
[00:05:16.660 --> 00:05:20.080]   Make sure that the model is performing well in production.
[00:05:20.080 --> 00:05:23.060]   Monitor statistics like time to inference
[00:05:23.060 --> 00:05:24.900]   and identify when the model
[00:05:24.900 --> 00:05:27.620]   is experiencing significant drift.
[00:05:27.620 --> 00:05:29.940]   The deployment stage produces its own set
[00:05:29.940 --> 00:05:33.140]   of key data to save and monitor.
[00:05:33.140 --> 00:05:35.100]   How is the model packaged up?
[00:05:35.100 --> 00:05:37.780]   What is the model seeing in production?
[00:05:37.780 --> 00:05:41.220]   How are real world inputs and predictions
[00:05:41.220 --> 00:05:44.780]   differing from the training and evaluation stages?
[00:05:44.780 --> 00:05:45.800]   Is there drift?
[00:05:45.800 --> 00:05:48.540]   Was the model trained on a dataset
[00:05:48.540 --> 00:05:50.460]   that has a different class distribution
[00:05:50.460 --> 00:05:52.340]   from the real world inputs?
[00:05:52.340 --> 00:05:54.420]   What are the performance metrics?
[00:05:54.420 --> 00:05:56.420]   And how are they changing?
[00:05:56.420 --> 00:05:58.680]   We want to see metrics over time
[00:05:58.680 --> 00:06:01.420]   to identify shifts in model performance.
[00:06:01.420 --> 00:06:04.740]   When is the right time to update a model in production?
[00:06:04.740 --> 00:06:08.500]   Alerts are critical for identifying model regressions
[00:06:08.500 --> 00:06:12.120]   and triggering processes to update the production model.
[00:06:12.120 --> 00:06:17.940]   This machine learning workflow is iterative and cyclical.
[00:06:17.940 --> 00:06:21.300]   Once it's set up, stable, and tracked,
[00:06:21.300 --> 00:06:24.640]   teams will run that process again and again.
[00:06:24.640 --> 00:06:28.640]   Getting the best model is a matter of iterating quickly,
[00:06:28.640 --> 00:06:31.500]   reacting to changes in the real world.
[00:06:31.500 --> 00:06:35.500]   We know that the untracked processes are brittle.
[00:06:35.500 --> 00:06:37.460]   Here's a screenshot from Stack Overflow
[00:06:37.460 --> 00:06:38.820]   from a few years ago,
[00:06:38.820 --> 00:06:41.120]   before everyone in software was using Git.
[00:06:41.120 --> 00:06:46.140]   This person is asking, "Why should I use version control?
[00:06:46.140 --> 00:06:48.060]   "Why not just duplicate the script
[00:06:48.060 --> 00:06:50.540]   "and name it version two, version three,
[00:06:50.540 --> 00:06:53.420]   "version four, final, final, final?"
[00:06:53.420 --> 00:06:55.700]   Before version control became a standard part
[00:06:55.700 --> 00:06:56.940]   of software development,
[00:06:56.940 --> 00:06:59.660]   it wasn't obvious how critical it would become.
[00:06:59.660 --> 00:07:04.180]   The machine learning workflow is in a similar place today.
[00:07:04.180 --> 00:07:07.140]   We have this whole host of critical metadata
[00:07:07.140 --> 00:07:12.140]   and context for data curation, modeling, and deployment.
[00:07:12.140 --> 00:07:14.700]   And today, I'm making the case
[00:07:14.700 --> 00:07:17.120]   that you need comprehensive tracking
[00:07:17.120 --> 00:07:20.540]   to have an effective machine learning pipeline.
[00:07:20.540 --> 00:07:22.740]   Here are a couple of case studies
[00:07:22.740 --> 00:07:25.700]   to highlight how real world WMB users
[00:07:25.700 --> 00:07:27.740]   are tracking their workflows.
[00:07:27.740 --> 00:07:30.100]   When we started working with OpenAI,
[00:07:30.100 --> 00:07:33.980]   they were working on a robotic hand to solve a Rubik's Cube.
[00:07:33.980 --> 00:07:37.900]   This is a hard problem with a lot of moving pieces.
[00:07:37.900 --> 00:07:40.300]   Even starting with a simpler problem
[00:07:40.300 --> 00:07:43.980]   of manipulating a solid cube without dropping it.
[00:07:43.980 --> 00:07:47.460]   It's really hard to train a model to do that effectively.
[00:07:47.460 --> 00:07:49.660]   It requires a lot of coordination.
[00:07:49.660 --> 00:07:52.860]   And they had many, many trials that failed,
[00:07:52.860 --> 00:07:55.000]   many experiments that they learned from.
[00:07:55.000 --> 00:07:59.180]   Multiple team members were training models in parallel.
[00:07:59.180 --> 00:08:03.700]   And before WMB, they had trouble communicating results.
[00:08:03.700 --> 00:08:04.900]   It was hard to capture
[00:08:04.900 --> 00:08:06.940]   and compare iterations of model training.
[00:08:06.940 --> 00:08:10.360]   What was this test accuracy?
[00:08:10.360 --> 00:08:12.640]   How long did this model take to train?
[00:08:12.640 --> 00:08:15.820]   What environment version was it using?
[00:08:15.820 --> 00:08:19.260]   Have you tried a lower learning rate?
[00:08:19.260 --> 00:08:21.220]   There were a flurry of Slack messages
[00:08:21.220 --> 00:08:24.100]   back and forth to communicate findings.
[00:08:24.100 --> 00:08:27.100]   And these details and context from modeling
[00:08:27.100 --> 00:08:30.100]   were hard to track without a dedicated system.
[00:08:30.100 --> 00:08:34.300]   OpenAI needed to make fast progress on production models.
[00:08:34.300 --> 00:08:36.900]   And they needed tools for tracking and iterating.
[00:08:36.900 --> 00:08:40.220]   Another one of our favorite customers to work with
[00:08:40.220 --> 00:08:42.860]   is Blue River, part of John Deere.
[00:08:42.860 --> 00:08:45.800]   Their goal is to use machine learning in agriculture,
[00:08:45.800 --> 00:08:48.700]   scaling up processes that are too slow
[00:08:48.700 --> 00:08:50.120]   and manual for a human.
[00:08:50.120 --> 00:08:53.300]   A little background on the problem space.
[00:08:53.300 --> 00:08:57.580]   Farms operating at a massive scale need to kill weeds fast.
[00:08:57.580 --> 00:08:59.540]   For example, when growing cotton,
[00:08:59.540 --> 00:09:02.340]   the weeds can compete for light and water
[00:09:02.340 --> 00:09:05.200]   and stunt the growth of the cotton plants.
[00:09:05.200 --> 00:09:08.780]   You can see this here in this comparison between the left,
[00:09:08.780 --> 00:09:12.340]   a field with weeds that are slowing the growth of the crop,
[00:09:12.340 --> 00:09:15.460]   and on the right, where weeds have been sprayed
[00:09:15.460 --> 00:09:17.020]   and the cotton is flourishing.
[00:09:17.020 --> 00:09:20.860]   When you're spraying a whole field of crops,
[00:09:20.860 --> 00:09:24.760]   this can produce a lot of chemicals that run off the fields.
[00:09:24.760 --> 00:09:27.240]   But what if you only spray the weeds?
[00:09:28.580 --> 00:09:31.020]   Well, that's a hard problem.
[00:09:31.020 --> 00:09:33.780]   Can you tell which plants on this slide are weeds?
[00:09:33.780 --> 00:09:39.440]   To me, the crop here in green and the weeds in red
[00:09:39.440 --> 00:09:41.140]   look almost the same.
[00:09:41.140 --> 00:09:44.500]   But Blue River is building a solution,
[00:09:44.500 --> 00:09:48.240]   an expert model that can spot weeds in fields.
[00:09:48.240 --> 00:09:51.060]   They drive tractors with GPUs on the back of them
[00:09:51.060 --> 00:09:54.100]   through fields and run models in real time
[00:09:54.100 --> 00:09:56.780]   to identify where the weeds are.
[00:09:56.780 --> 00:10:01.620]   Then, carefully, they only spray the weeds.
[00:10:01.620 --> 00:10:04.420]   This is an amazing application of machine learning
[00:10:04.420 --> 00:10:08.020]   and a very challenging workflow to manage end to end.
[00:10:08.020 --> 00:10:10.940]   They've got live data from different sources,
[00:10:10.940 --> 00:10:13.380]   real fields where their models are deployed.
[00:10:13.380 --> 00:10:16.800]   Data comes in from different times of day
[00:10:16.800 --> 00:10:19.120]   with different expert labelers,
[00:10:19.120 --> 00:10:21.200]   different crops and environments.
[00:10:21.200 --> 00:10:25.660]   They need to manage these constantly changing datasets.
[00:10:25.660 --> 00:10:28.640]   They need to track models trained in different conditions.
[00:10:28.640 --> 00:10:32.260]   And it's hard to stay on the same page
[00:10:32.260 --> 00:10:35.540]   when your team is operating in different fields.
[00:10:35.540 --> 00:10:38.540]   They need to track how changes to data upstream
[00:10:38.540 --> 00:10:40.860]   affects model performance downstream.
[00:10:40.860 --> 00:10:45.780]   OpenAI and Blue River are two examples
[00:10:45.780 --> 00:10:47.860]   of the hundreds of teams we work with
[00:10:47.860 --> 00:10:51.700]   across industries who have complex workflows.
[00:10:51.700 --> 00:10:56.340]   They need tools to not just track individual models,
[00:10:56.340 --> 00:11:00.500]   but capture the complete history of where a model came from.
[00:11:00.500 --> 00:11:03.620]   Looking across our customer teams,
[00:11:03.620 --> 00:11:06.700]   we've compiled a short list of best practices,
[00:11:06.700 --> 00:11:09.680]   common traits of an ideal ML workflow.
[00:11:09.680 --> 00:11:13.540]   One of the teams we work with at WMB
[00:11:13.540 --> 00:11:17.080]   is a group of ML practitioners at a national bank
[00:11:17.080 --> 00:11:19.920]   where they're deploying models to identify fraud.
[00:11:20.900 --> 00:11:23.740]   Models need to go through multiple stages of checks
[00:11:23.740 --> 00:11:26.220]   and audits by different teams
[00:11:26.220 --> 00:11:28.380]   before they make it to production.
[00:11:28.380 --> 00:11:33.060]   And this can be a complicated and multi-week process.
[00:11:33.060 --> 00:11:37.540]   Our key takeaway is that every step of the pipeline
[00:11:37.540 --> 00:11:39.180]   needs to be auditable
[00:11:39.180 --> 00:11:42.100]   by internal and regulatory stakeholders.
[00:11:42.100 --> 00:11:46.620]   And that audit trail needs to be captured automatically.
[00:11:46.620 --> 00:11:47.900]   A large team we work with
[00:11:47.900 --> 00:11:51.020]   has expanded to over a hundred models in production,
[00:11:51.020 --> 00:11:54.820]   and they're constantly onboarding new team members.
[00:11:54.820 --> 00:11:56.540]   To keep up with the demand,
[00:11:56.540 --> 00:11:59.300]   they need to be able to run and rerun
[00:11:59.300 --> 00:12:02.660]   every detail of the pipeline automatically.
[00:12:02.660 --> 00:12:04.520]   If someone leaves the team,
[00:12:04.520 --> 00:12:07.060]   the knowledge doesn't leave with them.
[00:12:07.060 --> 00:12:09.900]   Their model workflows are reproducible.
[00:12:09.900 --> 00:12:12.500]   So all the critical details are captured
[00:12:12.500 --> 00:12:14.360]   in a central system of record.
[00:12:15.660 --> 00:12:19.340]   One of our customers is running a popular generative model.
[00:12:19.340 --> 00:12:22.360]   Type in text and get a generated image.
[00:12:22.360 --> 00:12:25.900]   They're getting a massive number of requests,
[00:12:25.900 --> 00:12:28.800]   and they collect feedback on whether the users are satisfied
[00:12:28.800 --> 00:12:31.200]   with those generated images.
[00:12:31.200 --> 00:12:34.860]   They constantly need to retrain and tune the model
[00:12:34.860 --> 00:12:37.300]   to push a new version to production.
[00:12:37.300 --> 00:12:39.480]   Their workflow needs to be iterative
[00:12:39.480 --> 00:12:42.020]   so they can quickly update components
[00:12:42.020 --> 00:12:43.380]   and rerun the pipeline
[00:12:43.380 --> 00:12:46.020]   without much manual intervention or overhead.
[00:12:46.020 --> 00:12:49.980]   An autonomous vehicle company we work with
[00:12:49.980 --> 00:12:53.360]   has multiple teams working on different problems,
[00:12:53.360 --> 00:12:55.480]   like perception or path planning.
[00:12:55.480 --> 00:12:59.520]   It's dangerous to have these teams siloed.
[00:12:59.520 --> 00:13:01.640]   They all need to be on the same page
[00:13:01.640 --> 00:13:03.400]   about changes going into the car.
[00:13:03.400 --> 00:13:07.420]   Their workflow is necessarily collaborative,
[00:13:07.420 --> 00:13:10.200]   and it's critical that the tools they're using
[00:13:10.200 --> 00:13:13.400]   support sharing and communicating about results.
[00:13:13.400 --> 00:13:16.680]   One of our customer teams in healthcare
[00:13:16.680 --> 00:13:18.480]   is working with patient data,
[00:13:18.480 --> 00:13:21.040]   and they need to stay HIPAA compliant.
[00:13:21.040 --> 00:13:24.560]   Their ML workflow needs to be enterprise-grade,
[00:13:24.560 --> 00:13:26.660]   highly secure and stable,
[00:13:26.660 --> 00:13:30.880]   to protect their mission-critical, sensitive data.
[00:13:30.880 --> 00:13:33.160]   And finally, one of our customer teams
[00:13:33.160 --> 00:13:36.280]   is deploying models for Formula 1 racing.
[00:13:36.280 --> 00:13:38.500]   They need analysts on the ground
[00:13:38.500 --> 00:13:41.560]   to be able to run and analyze the results
[00:13:41.560 --> 00:13:43.640]   of models in real time.
[00:13:43.640 --> 00:13:47.360]   Their model workflow needs to be incredibly accessible
[00:13:47.360 --> 00:13:51.040]   with a visual, interactive, and friendly user experience.
[00:13:51.040 --> 00:13:53.680]   We're using these key principles
[00:13:53.680 --> 00:13:56.000]   of ML workflow best practices
[00:13:56.000 --> 00:13:57.800]   to inform the tools we build
[00:13:57.800 --> 00:14:00.960]   in our MLOps platform at WMB.
[00:14:00.960 --> 00:14:03.760]   This is going beyond experiment tracking.
[00:14:03.760 --> 00:14:05.240]   We're helping teams build
[00:14:05.240 --> 00:14:08.620]   an end-to-end standardized process,
[00:14:08.620 --> 00:14:12.780]   from fresh data all the way to fresh models in production.
[00:14:12.780 --> 00:14:16.580]   How do you scale up the process of iterating on models,
[00:14:16.580 --> 00:14:19.780]   centralize the knowledge of complex pipelines,
[00:14:19.780 --> 00:14:22.320]   and automate repeated stages?
[00:14:22.320 --> 00:14:26.380]   With our new tools for Model CI/CD,
[00:14:26.380 --> 00:14:29.840]   you can standardize and automate your workflows,
[00:14:29.840 --> 00:14:32.880]   and ensure that every model going to production
[00:14:32.880 --> 00:14:36.240]   has been rigorously tested and tracked.
[00:14:36.240 --> 00:14:38.780]   When did this model move into production?
[00:14:38.780 --> 00:14:40.540]   Who signed off?
[00:14:40.540 --> 00:14:42.220]   With the Model Registry,
[00:14:42.220 --> 00:14:46.660]   see a clear, auditable history of every change
[00:14:46.660 --> 00:14:49.580]   to models moving through workflow stages.
[00:14:49.580 --> 00:14:52.180]   See exactly who made a change,
[00:14:52.180 --> 00:14:54.540]   and when every change occurred.
[00:14:54.540 --> 00:14:57.920]   And easily click back to see all the details
[00:14:57.920 --> 00:14:59.220]   of each model version.
[00:15:00.460 --> 00:15:03.380]   What was the exact version of the dataset
[00:15:03.380 --> 00:15:05.460]   that this model was trained on?
[00:15:05.460 --> 00:15:08.120]   Are there two models that are being evaluated
[00:15:08.120 --> 00:15:10.160]   on the same exact test set?
[00:15:10.160 --> 00:15:13.360]   Add WMB to your pipeline once,
[00:15:13.360 --> 00:15:15.840]   and every time you run your training workflow,
[00:15:15.840 --> 00:15:19.320]   you'll be capturing a full, reproducible lineage.
[00:15:19.320 --> 00:15:22.320]   What happens when something goes wrong?
[00:15:22.320 --> 00:15:26.740]   Say you discover a bug in your data pre-processing code.
[00:15:26.740 --> 00:15:29.320]   How can you find all the datasets
[00:15:29.320 --> 00:15:31.600]   that were affected by that bug?
[00:15:31.600 --> 00:15:33.700]   How do you track down all the models
[00:15:33.700 --> 00:15:35.740]   that were trained using those datasets?
[00:15:35.740 --> 00:15:38.040]   With the Model Registry,
[00:15:38.040 --> 00:15:42.720]   it's easy to see all the steps upstream and downstream.
[00:15:42.720 --> 00:15:44.440]   Start from a central page
[00:15:44.440 --> 00:15:46.960]   that captures all the model versions
[00:15:46.960 --> 00:15:49.800]   you've iterated on for a given model task.
[00:15:49.800 --> 00:15:52.440]   Compare model versions,
[00:15:52.440 --> 00:15:56.600]   and easily get back to where a given model came from.
[00:15:56.600 --> 00:16:00.000]   Click through to see the run that produced a model,
[00:16:00.000 --> 00:16:04.040]   and the metrics and code associated with that model.
[00:16:04.040 --> 00:16:06.640]   Are there multiple evaluation steps?
[00:16:06.640 --> 00:16:10.280]   The reproducible lineage gives you a clear way
[00:16:10.280 --> 00:16:13.560]   to track dependencies, debug pipelines,
[00:16:13.560 --> 00:16:15.160]   and handle regressions.
[00:16:15.160 --> 00:16:18.280]   We know that the best way to get a new model to production
[00:16:18.280 --> 00:16:20.760]   is to have a repeatable process
[00:16:20.760 --> 00:16:22.960]   that allows for quick iteration.
[00:16:22.960 --> 00:16:26.920]   Use WMB to automatically trigger model retraining
[00:16:26.920 --> 00:16:28.680]   when new data is available.
[00:16:28.680 --> 00:16:32.960]   Set up a trigger in that same central dashboard
[00:16:32.960 --> 00:16:34.680]   where you're managing models.
[00:16:34.680 --> 00:16:36.800]   Trigger model evaluation jobs
[00:16:36.800 --> 00:16:40.140]   every time a new candidate model is registered.
[00:16:40.140 --> 00:16:42.520]   Ensure that all the models you're comparing
[00:16:42.520 --> 00:16:45.320]   were evaluated on the same test set,
[00:16:45.320 --> 00:16:48.800]   and that the comparison is really apples to apples.
[00:16:48.800 --> 00:16:51.720]   Reduce the manual work and cognitive overhead
[00:16:51.720 --> 00:16:54.640]   for iterating with automated tooling.
[00:16:54.640 --> 00:16:57.320]   Keeping team members on the same page is hard
[00:16:57.320 --> 00:17:00.640]   when there's lots of moving pieces in the ML workflow.
[00:17:00.640 --> 00:17:03.300]   With the collaborative dashboard for models,
[00:17:03.300 --> 00:17:06.200]   it's easy to see what model is in staging
[00:17:06.200 --> 00:17:08.320]   or in production right now.
[00:17:08.320 --> 00:17:11.700]   Answer questions like, how does my new candidate model
[00:17:11.700 --> 00:17:14.800]   compare to the one my colleague currently has in production?
[00:17:14.800 --> 00:17:16.820]   Without a lot of back and forth.
[00:17:16.820 --> 00:17:19.080]   We developed our tools for model evaluation
[00:17:19.080 --> 00:17:22.400]   and model management with the enterprise in mind.
[00:17:22.400 --> 00:17:24.080]   We support private hosting
[00:17:24.080 --> 00:17:26.720]   to meet rigorous security requirements.
[00:17:26.720 --> 00:17:28.940]   Customers with their own secure clusters
[00:17:28.940 --> 00:17:32.320]   use WMB to automatically trigger model retraining
[00:17:32.320 --> 00:17:34.000]   on their own compute,
[00:17:34.000 --> 00:17:38.480]   and use WMB for enterprise grade model CI/CD.
[00:17:38.480 --> 00:17:41.040]   Once models are rigorously evaluated,
[00:17:41.040 --> 00:17:44.820]   use WMB to package and deploy models to production.
[00:17:44.820 --> 00:17:47.480]   Our tools for deployment are customizable
[00:17:47.480 --> 00:17:49.560]   to suit the rigorous requirements
[00:17:49.560 --> 00:17:51.680]   of enterprise model serving.
[00:17:51.680 --> 00:17:53.400]   Run a customized job
[00:17:53.400 --> 00:17:55.800]   straight from the model management dashboard,
[00:17:55.800 --> 00:17:58.960]   or trigger the job automatically to run your own system.
[00:17:58.960 --> 00:18:04.320]   Customers are using WMB to standardize model deployment.
[00:18:04.320 --> 00:18:06.820]   So the system for keeping models up to date
[00:18:06.820 --> 00:18:09.140]   can scale as their team grows.
[00:18:09.140 --> 00:18:13.080]   And finally, WMB makes it accessible
[00:18:13.080 --> 00:18:15.540]   to manage and evaluate models
[00:18:15.540 --> 00:18:19.760]   with a single central home for all your production models.
[00:18:19.760 --> 00:18:21.840]   Your whole team can easily see
[00:18:21.840 --> 00:18:23.840]   the current state of production,
[00:18:23.840 --> 00:18:26.680]   upcoming candidates, and how they compare.
[00:18:26.680 --> 00:18:29.560]   Customize the stages that models move through,
[00:18:29.560 --> 00:18:31.480]   and track when a new candidate
[00:18:31.480 --> 00:18:33.940]   moves from staging to production.
[00:18:33.940 --> 00:18:36.640]   You can build a centralized model zoo
[00:18:36.640 --> 00:18:40.640]   to share and communicate about the best models
[00:18:40.640 --> 00:18:42.760]   available for different tasks.
[00:18:42.760 --> 00:18:46.100]   And with WMB, you can make it accessible
[00:18:46.100 --> 00:18:47.960]   for non-technical stakeholders
[00:18:47.960 --> 00:18:52.660]   to even trigger model retraining or evaluation themselves
[00:18:52.660 --> 00:18:55.660]   from that same unified dashboard.
[00:18:55.660 --> 00:18:57.940]   Our vision is an end-to-end
[00:18:57.940 --> 00:19:01.300]   developer-centric MLOps platform.
[00:19:01.300 --> 00:19:02.900]   We are building modular tools
[00:19:02.900 --> 00:19:05.580]   to solve pain points across the ML workflow.
[00:19:05.580 --> 00:19:09.240]   Use the WMB MLOps platform
[00:19:09.240 --> 00:19:12.980]   to version data and pipelines with artifacts,
[00:19:12.980 --> 00:19:16.160]   prep and visualize data with tables,
[00:19:16.160 --> 00:19:18.580]   track model training with experiments,
[00:19:18.580 --> 00:19:22.040]   optimize models with sweeps,
[00:19:22.040 --> 00:19:25.100]   test and evaluate with the model registry,
[00:19:25.100 --> 00:19:29.620]   and collaborate and share results with reports.
[00:19:29.620 --> 00:19:32.260]   The ML industry is moving so rapidly,
[00:19:32.260 --> 00:19:35.700]   teams need a comprehensive system for model management
[00:19:35.700 --> 00:19:37.600]   beyond experiment tracking.
[00:19:38.500 --> 00:19:41.220]   With our new tools for model evaluation,
[00:19:41.220 --> 00:19:43.540]   you can start tracking and automating
[00:19:43.540 --> 00:19:45.420]   the full model lifecycle.
[00:19:45.420 --> 00:19:48.220]   Capture data set versions
[00:19:48.220 --> 00:19:52.140]   and visualize example-level model predictions.
[00:19:52.140 --> 00:19:54.600]   Track models as they move to production
[00:19:54.600 --> 00:19:58.540]   with complete lineage and history.
[00:19:58.540 --> 00:20:01.900]   Coming this year, we'll be offering production monitoring
[00:20:01.900 --> 00:20:04.480]   for deploying models to close the loop
[00:20:04.480 --> 00:20:06.820]   from training to production.
[00:20:06.820 --> 00:20:11.040]   And finally, I'm excited to announce our new products,
[00:20:11.040 --> 00:20:15.000]   Models and Launch are now available for you to try.
[00:20:15.000 --> 00:20:18.740]   Try this quick guide to get started with Model CI/CD
[00:20:18.740 --> 00:20:22.740]   and help your team deploy better models faster.
[00:20:22.740 --> 00:20:24.940]   We're so excited to see what you'll build.
[00:20:24.940 --> 00:20:27.520]   (upbeat music)
[00:20:27.520 --> 00:20:30.100]   (upbeat music)
[00:20:30.100 --> 00:20:32.160]   you

