
[00:00:00.000 --> 00:00:03.360]   [MUSIC PLAYING]
[00:00:03.360 --> 00:00:08.000]   Welcome to this walkthrough of the Weights and Biases model
[00:00:08.000 --> 00:00:09.400]   registry.
[00:00:09.400 --> 00:00:12.480]   Part 1 of this video will cover model logging
[00:00:12.480 --> 00:00:16.160]   and how to register your best models in the registry.
[00:00:16.160 --> 00:00:18.440]   Part 2 will cover how to consume, document,
[00:00:18.440 --> 00:00:21.720]   and automate processes from the registry.
[00:00:21.720 --> 00:00:24.720]   But first, let's define a model registry,
[00:00:24.720 --> 00:00:27.320]   which is a repository of a team's trained models
[00:00:27.320 --> 00:00:30.440]   where ML practitioners publish candidates for further
[00:00:30.440 --> 00:00:33.000]   evaluation or production.
[00:00:33.000 --> 00:00:36.600]   Consumers, whether those be downstream teams or processes,
[00:00:36.600 --> 00:00:38.600]   can then retrieve those important models
[00:00:38.600 --> 00:00:40.760]   from a central place.
[00:00:40.760 --> 00:00:42.680]   Users can log their model checkpoints
[00:00:42.680 --> 00:00:46.200]   to Weights and Biases via artifacts.
[00:00:46.200 --> 00:00:49.040]   Artifacts enable users to log and version
[00:00:49.040 --> 00:00:51.600]   large serialized files.
[00:00:51.600 --> 00:00:55.840]   An artifact encapsulates all of the files and directories
[00:00:55.840 --> 00:01:00.680]   and consists of a user-defined name, a type,
[00:01:00.680 --> 00:01:04.600]   user-defined metadata in the form of keys and values,
[00:01:04.600 --> 00:01:09.640]   description, and the files and directories themselves.
[00:01:09.640 --> 00:01:12.440]   To create an artifact, all a user needs to do
[00:01:12.440 --> 00:01:16.160]   is create a run, create an artifact object,
[00:01:16.160 --> 00:01:18.800]   giving it a name, and in the case of models,
[00:01:18.800 --> 00:01:23.640]   a type of model, and then adding the paths
[00:01:23.640 --> 00:01:26.720]   to the directories and files to that artifact
[00:01:26.720 --> 00:01:30.480]   before finally logging it as part of a run.
[00:01:30.480 --> 00:01:33.440]   In this example, I'll talk through how
[00:01:33.440 --> 00:01:36.760]   to log your model checkpoint as part of a PyTorch training
[00:01:36.760 --> 00:01:38.440]   loop.
[00:01:38.440 --> 00:01:42.720]   First, we create a run with oneb.init,
[00:01:42.720 --> 00:01:47.160]   passing in our project, team entity, job type,
[00:01:47.160 --> 00:01:49.080]   and hyperparameter configurations.
[00:01:52.080 --> 00:01:53.760]   Throughout the training loop, we're
[00:01:53.760 --> 00:02:00.720]   logging training metrics along with evaluation metrics.
[00:02:00.720 --> 00:02:02.640]   And at the end of each epoch, we're
[00:02:02.640 --> 00:02:06.320]   serializing the model to ONIX.
[00:02:06.320 --> 00:02:10.280]   In this snippet of code, we're creating an artifact object,
[00:02:10.280 --> 00:02:15.160]   giving it a name, a type of model, and different metadata,
[00:02:15.160 --> 00:02:17.480]   such as the format.
[00:02:17.480 --> 00:02:20.200]   Finally, we're adding this model, the path to the model
[00:02:20.200 --> 00:02:26.160]   on its file to the artifact, and logging it as part of the run.
[00:02:26.160 --> 00:02:27.920]   When you log an artifact, you can also
[00:02:27.920 --> 00:02:32.480]   append aliases to indicate which versions are your best
[00:02:32.480 --> 00:02:35.040]   or latest during the course of training.
[00:02:35.040 --> 00:02:36.960]   In the Weights and Biases workspace,
[00:02:36.960 --> 00:02:41.360]   you can find your artifacts in the Artifacts tab.
[00:02:41.360 --> 00:02:43.960]   Here, we see all artifacts that have
[00:02:43.960 --> 00:02:48.840]   been logged across my project as the team has iterated
[00:02:48.840 --> 00:02:52.360]   and retrained the models.
[00:02:52.360 --> 00:02:55.280]   On the left here, I have this anomaly classifier,
[00:02:55.280 --> 00:02:57.920]   which has five checkpoints that have been logged
[00:02:57.920 --> 00:03:01.120]   during the course of training, with my best one indicated
[00:03:01.120 --> 00:03:02.760]   with this alias.
[00:03:02.760 --> 00:03:05.560]   With artifacts, I can understand all the metadata surrounding
[00:03:05.560 --> 00:03:09.840]   these files, such as when it was created, number of files,
[00:03:09.840 --> 00:03:13.280]   the size, and importantly, the exact experiment which
[00:03:13.280 --> 00:03:14.880]   generated this model checkpoint.
[00:03:18.360 --> 00:03:23.480]   I can also inspect the metadata, the files themselves,
[00:03:23.480 --> 00:03:26.840]   and look at a lineage view of all upstream and downstream
[00:03:26.840 --> 00:03:28.920]   runs and artifacts.
[00:03:28.920 --> 00:03:34.480]   Artifacts is a convenient way to store and manage
[00:03:34.480 --> 00:03:40.160]   all of your model checkpoints over time for a given project.
[00:03:40.160 --> 00:03:43.040]   Before getting to the model registry,
[00:03:43.040 --> 00:03:46.200]   I want to show additional integrations that allow
[00:03:46.200 --> 00:03:50.040]   you to log artifacts easily.
[00:03:50.040 --> 00:03:51.760]   If you're using PyTorch Lightning,
[00:03:51.760 --> 00:03:56.840]   you can use the model checkpoint callback and the WANDB logger
[00:03:56.840 --> 00:04:00.000]   to log model checkpoints automatically as artifacts
[00:04:00.000 --> 00:04:02.000]   and weights and biases.
[00:04:02.000 --> 00:04:05.000]   Simply indicate whether you want to log all checkpoints
[00:04:05.000 --> 00:04:08.240]   at a regular interval or just the last checkpoint
[00:04:08.240 --> 00:04:10.760]   at the end of training.
[00:04:10.760 --> 00:04:14.600]   Pass this WANDB logger into your trainer,
[00:04:14.600 --> 00:04:16.040]   and you should be good to go.
[00:04:16.040 --> 00:04:24.960]   Similarly, if you're using Hugging Face,
[00:04:24.960 --> 00:04:27.480]   all you have to do is set these environment variables
[00:04:27.480 --> 00:04:31.120]   to, again, indicate when and how to log your checkpoints,
[00:04:31.120 --> 00:04:35.400]   whether at the end of training or on a regular basis.
[00:04:35.400 --> 00:04:38.160]   Set the environment variables, indicate to your trainer
[00:04:38.160 --> 00:04:41.840]   that you're reporting to WANDB, and your checkpoints
[00:04:41.840 --> 00:04:43.480]   will get logged automatically.
[00:04:43.480 --> 00:04:46.240]   Finally, teams often already store their models
[00:04:46.240 --> 00:04:49.720]   in an object store, like S3 or GCS.
[00:04:49.720 --> 00:04:52.440]   You can log these assets to weights and biases
[00:04:52.440 --> 00:04:55.200]   as well as reference artifacts.
[00:04:55.200 --> 00:04:58.560]   In this case, weights and biases does not copy the checkpoints.
[00:04:58.560 --> 00:05:01.840]   We just manage a reference, along with metadata
[00:05:01.840 --> 00:05:05.720]   like the checksums, file sizes, and so forth.
[00:05:05.720 --> 00:05:08.760]   This enables you to understand exactly what experiment
[00:05:08.760 --> 00:05:14.760]   generated a given asset in some S3 or GCS bucket.
[00:05:14.760 --> 00:05:17.360]   In this example, we just create the artifact object
[00:05:17.360 --> 00:05:20.480]   with a name, a type of model, add our reference
[00:05:20.480 --> 00:05:26.240]   to the GCS or S3 bucket, and then log the artifact.
[00:05:26.240 --> 00:05:28.440]   Just make sure that if you're logging references
[00:05:28.440 --> 00:05:30.520]   or consuming these models, you need
[00:05:30.520 --> 00:05:34.600]   to have the right credentials to your appropriate cloud bucket.
[00:05:34.600 --> 00:05:36.240]   Back in the Artifacts tab, we can
[00:05:36.240 --> 00:05:38.400]   observe we have lots of models sitting here
[00:05:38.400 --> 00:05:40.480]   in our project.
[00:05:40.480 --> 00:05:41.800]   This can be hard to wrangle.
[00:05:41.800 --> 00:05:43.880]   It can be hard to identify what exactly
[00:05:43.880 --> 00:05:47.280]   is my most important model that I want to move on
[00:05:47.280 --> 00:05:50.080]   to production or evaluation.
[00:05:50.080 --> 00:05:52.800]   The Model Registry allows us to highlight or bookmark
[00:05:52.800 --> 00:05:57.120]   the best models that we have in a given project.
[00:05:57.120 --> 00:06:01.560]   All we have to do is select the artifact that we think is best
[00:06:01.560 --> 00:06:04.480]   and then click Link to Registry, where
[00:06:04.480 --> 00:06:06.520]   we can indicate what registered model
[00:06:06.520 --> 00:06:10.120]   we want to bookmark this specific version under.
[00:06:10.120 --> 00:06:15.320]   Registered models are designated ML tasks,
[00:06:15.320 --> 00:06:17.640]   which are documented and versioned over time
[00:06:17.640 --> 00:06:20.640]   in a central place where multiple consumers can
[00:06:20.640 --> 00:06:26.280]   come and understand how to use a given model.
[00:06:26.280 --> 00:06:28.000]   For this anomaly classifier, I don't
[00:06:28.000 --> 00:06:29.440]   have an existing registered model
[00:06:29.440 --> 00:06:32.200]   since this is the first instantiation of it.
[00:06:32.200 --> 00:06:34.120]   So let me create a new registered model.
[00:06:34.120 --> 00:06:37.480]   [MUSIC PLAYING]
[00:06:37.480 --> 00:06:44.640]   I can also paste a model card to better document
[00:06:44.640 --> 00:06:50.160]   the entire model, how to use it, its background, and so forth.
[00:06:50.160 --> 00:06:52.400]   I can also add a tag to better organize
[00:06:52.400 --> 00:06:54.880]   all of the registered models I have in my organization.
[00:06:54.880 --> 00:06:59.960]   Finally, I can add additional aliases
[00:06:59.960 --> 00:07:02.120]   to indicate the stage of this model,
[00:07:02.120 --> 00:07:04.960]   such as if it's in staging or this is a production model that
[00:07:04.960 --> 00:07:08.960]   needs to go and get deployed.
[00:07:08.960 --> 00:07:12.400]   Linking that to the registry, we can now
[00:07:12.400 --> 00:07:18.600]   see the model, its model card, and the bookmarked version
[00:07:18.600 --> 00:07:22.600]   from our granular project we saw earlier.
[00:07:22.600 --> 00:07:26.680]   The model registry is a team-scoped page
[00:07:26.680 --> 00:07:29.120]   that houses all of the important checkpoints
[00:07:29.120 --> 00:07:31.320]   across your different projects.
[00:07:31.320 --> 00:07:34.400]   So as teams iterate and generate hundreds, if not thousands,
[00:07:34.400 --> 00:07:36.720]   of checkpoints in their individual projects,
[00:07:36.720 --> 00:07:38.920]   they can bookmark the most important ones
[00:07:38.920 --> 00:07:42.880]   under Registered Models in the model registry.
[00:07:42.880 --> 00:07:44.580]   Here we have a bunch of different models
[00:07:44.580 --> 00:07:47.560]   for different use cases, like document extraction, image
[00:07:47.560 --> 00:07:51.200]   segmentation, and review summarization.
[00:07:51.200 --> 00:07:54.520]   Each one of these we can click into,
[00:07:54.520 --> 00:07:59.120]   understand the model card, and then
[00:07:59.120 --> 00:08:01.560]   click into the specific version that
[00:08:01.560 --> 00:08:04.640]   was bookmarked under this registered model.
[00:08:04.640 --> 00:08:07.320]   And that's part one of our Weights and Biases model
[00:08:07.320 --> 00:08:09.680]   registry walkthrough.
[00:08:09.680 --> 00:08:13.240]   In part two, I'll walk through how to effectively consume,
[00:08:13.240 --> 00:08:17.840]   document, and automate processes from the model registry.
[00:08:18.400 --> 00:08:21.440]   [MUSIC PLAYING]
[00:08:21.440 --> 00:08:24.020]   (upbeat music)

