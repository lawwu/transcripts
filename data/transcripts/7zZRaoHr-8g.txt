
[00:00:00.000 --> 00:00:06.280]   Welcome back everyone.
[00:00:06.280 --> 00:00:08.820]   This is the sixth and final screencast
[00:00:08.820 --> 00:00:10.680]   in our series on methods and metrics.
[00:00:10.680 --> 00:00:12.900]   We're going to talk about model evaluation.
[00:00:12.900 --> 00:00:15.300]   This is a high-level discussion that is directly
[00:00:15.300 --> 00:00:18.900]   oriented toward helping you with your final project work.
[00:00:18.900 --> 00:00:21.940]   Here's an overview. We're going to talk about baselines.
[00:00:21.940 --> 00:00:24.260]   What are they? Why are they important?
[00:00:24.260 --> 00:00:26.300]   We'll talk about the trials and
[00:00:26.300 --> 00:00:28.620]   tribulations of hyperparameter optimization
[00:00:28.620 --> 00:00:30.120]   and why it's important.
[00:00:30.120 --> 00:00:32.720]   We'll think about classifier comparison,
[00:00:32.720 --> 00:00:34.100]   a common mode to be in as
[00:00:34.100 --> 00:00:36.620]   you're evaluating systems and hypotheses.
[00:00:36.620 --> 00:00:38.520]   Then we'll talk about two things that are
[00:00:38.520 --> 00:00:40.960]   really particular to the deep learning era.
[00:00:40.960 --> 00:00:43.460]   How to assess models that don't converge in
[00:00:43.460 --> 00:00:46.460]   any strict sense and also the role of
[00:00:46.460 --> 00:00:48.660]   random parameter initialization in
[00:00:48.660 --> 00:00:51.800]   the performance of our biggest models.
[00:00:51.800 --> 00:00:54.180]   Let's start with baselines.
[00:00:54.180 --> 00:00:55.380]   We take this for granted,
[00:00:55.380 --> 00:00:58.500]   but this is actually pretty important conceptually.
[00:00:58.500 --> 00:01:01.520]   Here's a fundamental observation about baselines.
[00:01:01.520 --> 00:01:03.980]   Evaluation numbers in our field can
[00:01:03.980 --> 00:01:06.820]   never be understood properly in isolation.
[00:01:06.820 --> 00:01:09.740]   Suppose your system gets 0.95 F1,
[00:01:09.740 --> 00:01:11.520]   you feel overjoyed,
[00:01:11.520 --> 00:01:14.700]   but the first question reviewers will ask you is,
[00:01:14.700 --> 00:01:16.280]   is the task too easy?
[00:01:16.280 --> 00:01:19.600]   How do simple baselines do on the problem?
[00:01:19.600 --> 00:01:24.260]   Conversely, suppose your system gets 0.6 and you feel
[00:01:24.260 --> 00:01:26.500]   in despair because you
[00:01:26.500 --> 00:01:28.220]   feel like you haven't had a success here,
[00:01:28.220 --> 00:01:30.020]   but the next question should be,
[00:01:30.020 --> 00:01:31.180]   how do humans do?
[00:01:31.180 --> 00:01:33.780]   They're presumably a upper bound.
[00:01:33.780 --> 00:01:36.180]   If it's a hard task or a noisy task,
[00:01:36.180 --> 00:01:38.540]   human performance might be close to 0.61 and
[00:01:38.540 --> 00:01:41.380]   you might really have achieved something meaningful there.
[00:01:41.380 --> 00:01:43.520]   It's baseline models and in that case,
[00:01:43.520 --> 00:01:46.660]   Oracle models that are helping us to understand.
[00:01:46.660 --> 00:01:51.180]   Baselines are also crucial for strong experimental design.
[00:01:51.180 --> 00:01:54.940]   Defining your baseline should not be some afterthought,
[00:01:54.940 --> 00:01:56.640]   but rather central to how you
[00:01:56.640 --> 00:01:58.800]   define your overall hypotheses.
[00:01:58.800 --> 00:02:01.160]   Think about simple systems,
[00:02:01.160 --> 00:02:03.860]   think about ablations of your target system and
[00:02:03.860 --> 00:02:05.780]   incorporate those into your thinking
[00:02:05.780 --> 00:02:07.580]   about the comparisons that you'll make.
[00:02:07.580 --> 00:02:10.060]   Baselines are really just one aspect
[00:02:10.060 --> 00:02:12.820]   of the comparisons we want to offer.
[00:02:12.820 --> 00:02:16.640]   Baselines are essential for building a persuasive case.
[00:02:16.640 --> 00:02:18.660]   We saw that in my two examples there.
[00:02:18.660 --> 00:02:22.020]   To really understand and calibrate on what you achieved,
[00:02:22.020 --> 00:02:26.100]   we need some baselines to calibrate all of that stuff.
[00:02:26.100 --> 00:02:28.640]   They can also be used to illuminate
[00:02:28.640 --> 00:02:30.860]   specific aspects of the problem
[00:02:30.860 --> 00:02:33.720]   and specific virtues of your proposed system.
[00:02:33.720 --> 00:02:35.180]   That often falls under the heading
[00:02:35.180 --> 00:02:36.740]   of ablations of your system.
[00:02:36.740 --> 00:02:38.920]   Those are baselines that remove
[00:02:38.920 --> 00:02:41.220]   crucial features or components
[00:02:41.220 --> 00:02:43.420]   and test the model with the same protocol.
[00:02:43.420 --> 00:02:45.180]   Then the distance between
[00:02:45.180 --> 00:02:49.420]   your chosen model and the ablated model is a estimate of
[00:02:49.420 --> 00:02:50.920]   the importance of
[00:02:50.920 --> 00:02:54.580]   the ablated component to the overall system performance.
[00:02:54.580 --> 00:02:56.940]   Crucial aspect of arguing and
[00:02:56.940 --> 00:03:00.140]   supporting hypotheses and everything else.
[00:03:00.140 --> 00:03:05.340]   Random baselines are really useful for many purposes.
[00:03:05.340 --> 00:03:07.700]   First, they can provide a really true lower
[00:03:07.700 --> 00:03:10.460]   bound on how systems can do on your problem.
[00:03:10.460 --> 00:03:12.780]   Sometimes they are surprisingly robust,
[00:03:12.780 --> 00:03:14.780]   and so it's worth running these early.
[00:03:14.780 --> 00:03:18.340]   I think also they can help you fully debug your system.
[00:03:18.340 --> 00:03:20.300]   These are probably lightweight models that do
[00:03:20.300 --> 00:03:22.740]   relatively little processing and can make
[00:03:22.740 --> 00:03:24.660]   sure that everything is functioning
[00:03:24.660 --> 00:03:26.700]   and makes sense and all that other stuff.
[00:03:26.700 --> 00:03:28.820]   Scikit-learn again has you covered.
[00:03:28.820 --> 00:03:31.620]   They have dummy classifier and dummy regressor.
[00:03:31.620 --> 00:03:35.100]   They have different ways of acting as random models,
[00:03:35.100 --> 00:03:36.740]   and I think this is really useful to
[00:03:36.740 --> 00:03:39.860]   set up early in your process.
[00:03:39.860 --> 00:03:43.620]   You could also think about task specific baselines.
[00:03:43.620 --> 00:03:45.220]   This might require real thought
[00:03:45.220 --> 00:03:47.420]   and real study in the literature.
[00:03:47.420 --> 00:03:50.500]   Does your problem suggest a baseline that will reveal
[00:03:50.500 --> 00:03:53.500]   something about the problem or the way it's modeled?
[00:03:53.500 --> 00:03:57.100]   If so, you should have one of these task specific baselines.
[00:03:57.100 --> 00:03:59.860]   Here are two recent examples from NLU.
[00:03:59.860 --> 00:04:02.220]   The first one is natural language inference.
[00:04:02.220 --> 00:04:04.500]   People discovered that so-called
[00:04:04.500 --> 00:04:08.100]   hypothesis only baselines can be very strong.
[00:04:08.100 --> 00:04:09.900]   The reason this happens is that
[00:04:09.900 --> 00:04:12.100]   in the underlying crowdsourcing effort,
[00:04:12.100 --> 00:04:14.540]   crowd workers were given premise sentences
[00:04:14.540 --> 00:04:16.700]   and asked to construct three hypotheses,
[00:04:16.700 --> 00:04:18.420]   one for neutral, one for
[00:04:18.420 --> 00:04:20.780]   contradiction, and one for entailment.
[00:04:20.780 --> 00:04:22.860]   In that process of construction,
[00:04:22.860 --> 00:04:25.940]   they did some systematic things that convey
[00:04:25.940 --> 00:04:27.940]   information about the label
[00:04:27.940 --> 00:04:30.620]   inadvertently through the hypothesis.
[00:04:30.620 --> 00:04:33.540]   For example, many contradictions involve
[00:04:33.540 --> 00:04:36.420]   negation and many entailment pairs involve
[00:04:36.420 --> 00:04:39.780]   very general terms as part of the hypothesis.
[00:04:39.780 --> 00:04:41.620]   What that means is that the hypothesis
[00:04:41.620 --> 00:04:43.180]   actually carries information about
[00:04:43.180 --> 00:04:46.700]   the label and a hypothesis only baseline quantifies that.
[00:04:46.700 --> 00:04:48.580]   You simply fit a model without
[00:04:48.580 --> 00:04:51.540]   any premise information and see how you do.
[00:04:51.540 --> 00:04:54.540]   The finding of the literature is that very often,
[00:04:54.540 --> 00:04:56.700]   for our benchmarks, the hypothesis
[00:04:56.700 --> 00:04:59.140]   only baseline is way above chance.
[00:04:59.140 --> 00:05:00.340]   What that shows you is that
[00:05:00.340 --> 00:05:04.460]   the random baseline is not so informative anymore.
[00:05:04.460 --> 00:05:07.860]   There's a similar story for the story closed task.
[00:05:07.860 --> 00:05:09.700]   This is to distinguish between
[00:05:09.700 --> 00:05:12.500]   a coherent and incoherent ending for a story.
[00:05:12.500 --> 00:05:14.340]   Again, systems that look only at
[00:05:14.340 --> 00:05:16.340]   the ending often do really well.
[00:05:16.340 --> 00:05:17.660]   I think for the same reason,
[00:05:17.660 --> 00:05:20.580]   the coherent versus incoherent thing is often
[00:05:20.580 --> 00:05:23.900]   actually inferable just from the ending,
[00:05:23.900 --> 00:05:25.460]   neglecting the story.
[00:05:25.460 --> 00:05:28.580]   It's not that the task is broken here necessarily,
[00:05:28.580 --> 00:05:30.460]   but rather again, that you should think about
[00:05:30.460 --> 00:05:33.860]   this as a baseline to compare against and progress.
[00:05:33.860 --> 00:05:38.780]   It's really progress from this very specialized baseline.
[00:05:38.780 --> 00:05:42.860]   Next topic, hyperparameter optimization.
[00:05:42.860 --> 00:05:44.660]   This is discussed extensively in
[00:05:44.660 --> 00:05:47.460]   one of our background units on sentiment analysis.
[00:05:47.460 --> 00:05:49.580]   You might go there for a refresher.
[00:05:49.580 --> 00:05:52.300]   Here, I'll just briefly review the rationale.
[00:05:52.300 --> 00:05:55.860]   You want maybe to obtain the best version of your model,
[00:05:55.860 --> 00:05:57.700]   and that might mean exploring over
[00:05:57.700 --> 00:05:59.460]   different hyperparameters to find
[00:05:59.460 --> 00:06:01.660]   a optimal setting for it.
[00:06:01.660 --> 00:06:05.340]   Another motivation is about comparison between models.
[00:06:05.340 --> 00:06:07.220]   Suppose you do have a results table
[00:06:07.220 --> 00:06:09.580]   full of different systems you're comparing.
[00:06:09.580 --> 00:06:12.420]   It makes no sense to compare them against
[00:06:12.420 --> 00:06:14.500]   randomly chosen parameter settings
[00:06:14.500 --> 00:06:16.260]   because you really want to give
[00:06:16.260 --> 00:06:18.780]   every model the best chance to shine.
[00:06:18.780 --> 00:06:20.820]   Otherwise, there's an arbitrariness to
[00:06:20.820 --> 00:06:22.580]   the evaluation that might
[00:06:22.580 --> 00:06:24.980]   not translate into robust results.
[00:06:24.980 --> 00:06:27.780]   What you really do is give every system a chance by
[00:06:27.780 --> 00:06:31.060]   exploring a wide range of hyperparameters and
[00:06:31.060 --> 00:06:33.100]   reporting the optimal results
[00:06:33.100 --> 00:06:34.940]   according to that exploration.
[00:06:34.940 --> 00:06:37.300]   That's a fair comparison and it implies
[00:06:37.300 --> 00:06:40.300]   a lot of search over hyperparameters.
[00:06:40.300 --> 00:06:41.620]   You might want to understand
[00:06:41.620 --> 00:06:43.100]   the stability of your architecture.
[00:06:43.100 --> 00:06:44.420]   This is interestingly different.
[00:06:44.420 --> 00:06:45.580]   This is where you're not
[00:06:45.580 --> 00:06:47.500]   interested in the best parameters,
[00:06:47.500 --> 00:06:50.540]   but rather how stable system performance is under
[00:06:50.540 --> 00:06:53.820]   various choices people might make in order to get
[00:06:53.820 --> 00:06:57.020]   a sense for how robustly it will perform if people are
[00:06:57.020 --> 00:07:00.340]   say not attentive to these hyperparameters or set them
[00:07:00.340 --> 00:07:03.500]   incorrectly in a inadvertent accident
[00:07:03.500 --> 00:07:06.100]   or an adversarial setting.
[00:07:06.100 --> 00:07:09.980]   Crucial to all of this no matter what your goals,
[00:07:09.980 --> 00:07:12.100]   hyperparameter tuning must be done
[00:07:12.100 --> 00:07:14.340]   only on train and development data.
[00:07:14.340 --> 00:07:16.700]   You never do model selection of
[00:07:16.700 --> 00:07:19.100]   any kind based on the test data.
[00:07:19.100 --> 00:07:20.700]   This is a special case of
[00:07:20.700 --> 00:07:22.860]   the rule that I've been repeating throughout the course.
[00:07:22.860 --> 00:07:25.020]   This is really fundamental to how we think about
[00:07:25.020 --> 00:07:27.740]   testing and generalization and it applies with
[00:07:27.740 --> 00:07:29.900]   real force in the context of
[00:07:29.900 --> 00:07:33.020]   the kind of model selection we're doing here.
[00:07:33.020 --> 00:07:37.780]   Now, hyperparameter optimization has gotten really
[00:07:37.780 --> 00:07:39.460]   challenging in the era of
[00:07:39.460 --> 00:07:42.100]   long-running expensive training regimes.
[00:07:42.100 --> 00:07:44.260]   Let me give you a sense for what the problem is
[00:07:44.260 --> 00:07:46.020]   by way of an example.
[00:07:46.020 --> 00:07:48.620]   For each hyperparameter, you identify
[00:07:48.620 --> 00:07:51.860]   a large set of values for it in some range.
[00:07:51.860 --> 00:07:54.740]   Then you create a list of all the combinations
[00:07:54.740 --> 00:07:56.580]   of all the hyperparameters.
[00:07:56.580 --> 00:07:59.180]   This is the cross product of all the values
[00:07:59.180 --> 00:08:01.860]   for all the features you identified in step 1.
[00:08:01.860 --> 00:08:03.860]   What you can hear in that description is
[00:08:03.860 --> 00:08:07.180]   an exponential growth in the number of settings.
[00:08:07.180 --> 00:08:09.740]   For each setting, you cross-validate it
[00:08:09.740 --> 00:08:11.180]   on the available training data,
[00:08:11.180 --> 00:08:15.540]   which might imply 5, or 10, or 20 experiments.
[00:08:15.540 --> 00:08:18.780]   Then you choose the settings that did best in step 3,
[00:08:18.780 --> 00:08:21.820]   and you train on all the trained data using that setting,
[00:08:21.820 --> 00:08:24.220]   and then you evaluate that model on the test set.
[00:08:24.220 --> 00:08:26.580]   That is a pristine version
[00:08:26.580 --> 00:08:29.580]   of the protocol that we might be implementing.
[00:08:29.580 --> 00:08:31.420]   But here's the problem.
[00:08:31.420 --> 00:08:33.180]   Suppose parameter h1 has
[00:08:33.180 --> 00:08:36.060]   five values and parameter 2 has 10,
[00:08:36.060 --> 00:08:39.100]   then the total number of settings is now 50.
[00:08:39.100 --> 00:08:43.180]   Suppose we add 3, then it goes to 100.
[00:08:43.180 --> 00:08:44.620]   Now, suppose we're going to do
[00:08:44.620 --> 00:08:47.620]   five-fold cross-validation to select optimal parameters.
[00:08:47.620 --> 00:08:49.940]   Now we are at 500 runs.
[00:08:49.940 --> 00:08:54.060]   Very quickly, the number of experiments exploded.
[00:08:54.060 --> 00:08:56.300]   If each one of these runs takes a day,
[00:08:56.300 --> 00:08:58.620]   you're pretty much out of contention in terms of
[00:08:58.620 --> 00:09:01.660]   actually implementing this protocol completely.
[00:09:01.660 --> 00:09:03.820]   Something has to change.
[00:09:03.820 --> 00:09:06.540]   The above is untenable as
[00:09:06.540 --> 00:09:08.500]   a set of laws for the scientific community.
[00:09:08.500 --> 00:09:12.660]   We cannot insist on this level of hyperparameter optimization.
[00:09:12.660 --> 00:09:14.180]   If we adopted it,
[00:09:14.180 --> 00:09:17.020]   complex models trained on large datasets would end up
[00:09:17.020 --> 00:09:19.140]   disfavored and only the very
[00:09:19.140 --> 00:09:20.980]   wealthy would be able to participate.
[00:09:20.980 --> 00:09:22.180]   To give you a glimpse of this,
[00:09:22.180 --> 00:09:24.300]   here's a quote from a paper from a team at
[00:09:24.300 --> 00:09:27.060]   Google doing NLP for healthcare.
[00:09:27.060 --> 00:09:29.700]   The performance of all above neural networks were
[00:09:29.700 --> 00:09:32.700]   tuned automatically using Google Vizier with
[00:09:32.700 --> 00:09:36.700]   a total of over 200,000 GPU hours.
[00:09:36.700 --> 00:09:40.380]   That is a lot of money spent on a lot of compute.
[00:09:40.380 --> 00:09:43.740]   Obviously, we cannot insist on a similar level of
[00:09:43.740 --> 00:09:46.700]   investment for experiments say for this course,
[00:09:46.700 --> 00:09:49.420]   but frankly, for any contribution in the field,
[00:09:49.420 --> 00:09:51.420]   we have to have compromises.
[00:09:51.420 --> 00:09:53.620]   Here are some reasonable compromises.
[00:09:53.620 --> 00:09:55.740]   These are pragmatic steps you can take to
[00:09:55.740 --> 00:09:58.460]   alleviate this resource problem.
[00:09:58.460 --> 00:10:00.700]   I've given them in descending order of
[00:10:00.700 --> 00:10:04.260]   attractiveness and I find that as the days go by,
[00:10:04.260 --> 00:10:08.180]   we need to go lower and lower on this list.
[00:10:08.180 --> 00:10:10.500]   You could do random sampling and
[00:10:10.500 --> 00:10:13.900]   guided sampling to explore a large space on a fixed budget.
[00:10:13.900 --> 00:10:15.460]   This is nice because you have
[00:10:15.460 --> 00:10:17.820]   the cross product of all of the settings.
[00:10:17.820 --> 00:10:19.060]   That's too large.
[00:10:19.060 --> 00:10:21.480]   You simply randomly sample in the space,
[00:10:21.480 --> 00:10:23.700]   maybe with some guidance from a model,
[00:10:23.700 --> 00:10:25.980]   and you can then on a fixed budget of say,
[00:10:25.980 --> 00:10:27.860]   five or 10 or 100 runs,
[00:10:27.860 --> 00:10:30.980]   do a version of the full grid search.
[00:10:30.980 --> 00:10:33.980]   You could also search based on a few epochs of
[00:10:33.980 --> 00:10:36.940]   training. The expense comes from multiple epochs,
[00:10:36.940 --> 00:10:38.500]   maybe you do one or two,
[00:10:38.500 --> 00:10:40.280]   and then you pick the hyperparameters
[00:10:40.280 --> 00:10:42.340]   that were best at that point.
[00:10:42.340 --> 00:10:47.220]   If the learning curves are familiar and consistent,
[00:10:47.220 --> 00:10:50.540]   then this will be a pretty strong approach here.
[00:10:50.540 --> 00:10:53.780]   You could also search based on subsets of the data.
[00:10:53.780 --> 00:10:56.540]   This is fine, but it could be risky because we
[00:10:56.540 --> 00:11:00.180]   know some parameters are very dependent on dataset size.
[00:11:00.180 --> 00:11:02.300]   You're selecting based on small data
[00:11:02.300 --> 00:11:03.700]   and applying it to large data,
[00:11:03.700 --> 00:11:07.740]   even though you know that's probably a risky assumption.
[00:11:07.740 --> 00:11:10.500]   You could do heuristic search and define
[00:11:10.500 --> 00:11:12.980]   which hyperparameters matter less and then set
[00:11:12.980 --> 00:11:15.940]   them by hand and justify that in the paper.
[00:11:15.940 --> 00:11:17.540]   That's increasingly common.
[00:11:17.540 --> 00:11:18.980]   People describe things like we
[00:11:18.980 --> 00:11:21.420]   determined in our initial experiments
[00:11:21.420 --> 00:11:23.180]   that these hyperparameters had
[00:11:23.180 --> 00:11:25.940]   this optimal value or didn't matter that much,
[00:11:25.940 --> 00:11:29.020]   and so we chose these reasonable values.
[00:11:29.020 --> 00:11:32.260]   Then the actual search happens only over the ones that you
[00:11:32.260 --> 00:11:34.580]   can tell are important.
[00:11:34.580 --> 00:11:36.620]   We have to take your word for it that you've done
[00:11:36.620 --> 00:11:38.260]   the heuristic search responsibly,
[00:11:38.260 --> 00:11:40.780]   but this is obviously a really good way to
[00:11:40.780 --> 00:11:44.820]   balance exploration with constrained resources.
[00:11:44.820 --> 00:11:47.620]   You could find the optimal hyperparameters via
[00:11:47.620 --> 00:11:49.500]   a single split and use them for
[00:11:49.500 --> 00:11:51.500]   all subsequent splits and then justify
[00:11:51.500 --> 00:11:54.040]   that based on the fact that the splits are similar.
[00:11:54.040 --> 00:11:55.900]   That would automatically cut down
[00:11:55.900 --> 00:11:58.780]   substantially on the number of runs you need to do because you
[00:11:58.780 --> 00:12:03.020]   don't need to do so much cross-validation in this mode.
[00:12:03.020 --> 00:12:05.900]   Then finally, you could adopt others' choices.
[00:12:05.900 --> 00:12:08.060]   The skeptics will complain that these findings
[00:12:08.060 --> 00:12:09.980]   don't translate to new datasets,
[00:12:09.980 --> 00:12:11.780]   but it could be the only option.
[00:12:11.780 --> 00:12:14.180]   As I say, a few years ago,
[00:12:14.180 --> 00:12:15.840]   this was frowned upon,
[00:12:15.840 --> 00:12:18.720]   but now in the modern era with these massive models,
[00:12:18.720 --> 00:12:20.460]   it's basically the only option.
[00:12:20.460 --> 00:12:22.840]   I think increasingly, people are simply
[00:12:22.840 --> 00:12:25.260]   carrying forward other hyperparameters.
[00:12:25.260 --> 00:12:26.980]   It means less exploration.
[00:12:26.980 --> 00:12:29.860]   We might not be seeing the best versions of these models,
[00:12:29.860 --> 00:12:33.420]   but it might then again be the only option.
[00:12:33.420 --> 00:12:36.300]   In terms of tools for hyperparameter search,
[00:12:36.300 --> 00:12:38.660]   as always, Scikit is really rich with these things.
[00:12:38.660 --> 00:12:40.980]   They've got a lot of these toolings.
[00:12:40.980 --> 00:12:43.460]   In addition, Scikit Optimize is
[00:12:43.460 --> 00:12:46.140]   one level up in terms of sophistication.
[00:12:46.140 --> 00:12:48.980]   That's where you could do model-guided search through
[00:12:48.980 --> 00:12:52.380]   a hyperparameter grid in order to intelligently
[00:12:52.380 --> 00:12:54.460]   select good settings to
[00:12:54.460 --> 00:12:57.700]   lead to a good model on a fixed budget.
[00:12:57.700 --> 00:13:00.940]   Next topic, classifier comparison.
[00:13:00.940 --> 00:13:02.780]   This is a short one, but this can be important.
[00:13:02.780 --> 00:13:05.580]   Suppose you've assessed two classifier models.
[00:13:05.580 --> 00:13:09.000]   Their performance is probably different in some way.
[00:13:09.000 --> 00:13:11.180]   What can you do to establish whether
[00:13:11.180 --> 00:13:13.900]   these models are different in any meaningful sense?
[00:13:13.900 --> 00:13:15.220]   I think there are a few options.
[00:13:15.220 --> 00:13:17.660]   The first would be practical differences.
[00:13:17.660 --> 00:13:19.160]   If they obviously make
[00:13:19.160 --> 00:13:21.080]   a large number of different predictions,
[00:13:21.080 --> 00:13:24.140]   you might be able to quantify that difference in terms of
[00:13:24.140 --> 00:13:26.420]   some actual external outcome.
[00:13:26.420 --> 00:13:29.320]   That's a really good scenario to be in.
[00:13:29.320 --> 00:13:32.100]   You could also think about confidence intervals
[00:13:32.100 --> 00:13:34.940]   to further bolster the argument that you're making.
[00:13:34.940 --> 00:13:36.580]   This will give us a picture of how
[00:13:36.580 --> 00:13:39.860]   consistently different the two systems are.
[00:13:39.860 --> 00:13:41.680]   If they are consistently different,
[00:13:41.680 --> 00:13:43.360]   then you have a very clear argument
[00:13:43.360 --> 00:13:45.540]   in favor of one over the other.
[00:13:45.540 --> 00:13:49.460]   The Wilcoxon sign-rank test is a accepted method in
[00:13:49.460 --> 00:13:51.900]   the field for assessing classifiers
[00:13:51.900 --> 00:13:55.380]   using methodologies that are similar to standard t-tests.
[00:13:55.380 --> 00:13:57.180]   I guess the consensus is just that
[00:13:57.180 --> 00:13:59.620]   the assumptions behind the Wilcoxon test
[00:13:59.620 --> 00:14:02.940]   are somewhat more aligned with classifier comparison.
[00:14:02.940 --> 00:14:05.380]   To do that, as well as confidence intervals,
[00:14:05.380 --> 00:14:07.260]   you will have had to run your model on
[00:14:07.260 --> 00:14:10.860]   lots of different settings to get a long vector of
[00:14:10.860 --> 00:14:15.580]   10-20 scores to use as the basis for the stats testing.
[00:14:15.580 --> 00:14:17.260]   If that is too expensive,
[00:14:17.260 --> 00:14:19.820]   you could opt for a McNemars test.
[00:14:19.820 --> 00:14:22.300]   This is a comparison that you do over
[00:14:22.300 --> 00:14:25.900]   two single trained classifiers
[00:14:25.900 --> 00:14:27.980]   based on their confusion matrices.
[00:14:27.980 --> 00:14:29.540]   You only need one run.
[00:14:29.540 --> 00:14:32.440]   It will be unstable if the models are unstable,
[00:14:32.440 --> 00:14:36.460]   but it is a way of doing a stats test in the mode of
[00:14:36.460 --> 00:14:38.460]   the chi-squared test to give you
[00:14:38.460 --> 00:14:39.860]   some information about how
[00:14:39.860 --> 00:14:42.420]   two fixed artifacts compare to each other.
[00:14:42.420 --> 00:14:44.860]   Not as strong as the previous methods,
[00:14:44.860 --> 00:14:49.460]   but nonetheless useful and arguably better than nothing.
[00:14:50.340 --> 00:14:54.180]   A special topic for deep learning.
[00:14:54.180 --> 00:14:56.700]   How do you assess models without convergence?
[00:14:56.700 --> 00:14:58.300]   This never used to arise.
[00:14:58.300 --> 00:15:00.900]   Back in the days of all linear models,
[00:15:00.900 --> 00:15:02.740]   all these models would converge more or less
[00:15:02.740 --> 00:15:05.180]   instantly to epsilon loss,
[00:15:05.180 --> 00:15:07.180]   and then you could feel like that was how
[00:15:07.180 --> 00:15:09.660]   you'd move forward with assessing them.
[00:15:09.660 --> 00:15:11.980]   But now with neural networks,
[00:15:11.980 --> 00:15:14.460]   convergence has really taken center stage,
[00:15:14.460 --> 00:15:17.140]   and it's in a complicated way that it takes center stage.
[00:15:17.140 --> 00:15:21.040]   First, these models rarely converge to epsilon loss,
[00:15:21.040 --> 00:15:23.740]   and therefore it's non-issue whether or
[00:15:23.740 --> 00:15:25.900]   not that would be your stopping criterion.
[00:15:25.900 --> 00:15:27.700]   In addition, they might converge at
[00:15:27.700 --> 00:15:29.500]   different rates between runs,
[00:15:29.500 --> 00:15:32.540]   and their performance on the test set might not
[00:15:32.540 --> 00:15:36.620]   even be especially related to how small the loss got.
[00:15:36.620 --> 00:15:39.220]   You need to be thoughtful about
[00:15:39.220 --> 00:15:41.820]   exactly what your stopping criteria will be.
[00:15:41.820 --> 00:15:43.340]   Yes, sometimes a model with
[00:15:43.340 --> 00:15:45.540]   low final error turns out to be great,
[00:15:45.540 --> 00:15:47.220]   and sometimes it's worse than the one
[00:15:47.220 --> 00:15:48.700]   that finished with a higher error.
[00:15:48.700 --> 00:15:50.060]   This might have something to do with
[00:15:50.060 --> 00:15:52.380]   overfitting and regularization,
[00:15:52.380 --> 00:15:53.920]   but the bottom line here is,
[00:15:53.920 --> 00:15:56.120]   we don't know a priori what's going on.
[00:15:56.120 --> 00:15:59.100]   This is very experiment-driven.
[00:15:59.100 --> 00:16:03.060]   One thing to think about for stopping criteria in
[00:16:03.060 --> 00:16:06.700]   general is what we call incremental dev set testing.
[00:16:06.700 --> 00:16:09.640]   To address the uncertainty that I just reviewed,
[00:16:09.640 --> 00:16:12.140]   you regularly collect information about
[00:16:12.140 --> 00:16:15.760]   dev set performance as part of the training that you're doing.
[00:16:15.760 --> 00:16:18.260]   For example, at every 100th iteration,
[00:16:18.260 --> 00:16:20.860]   you could make predictions on the dev set and
[00:16:20.860 --> 00:16:24.140]   store the resulting vector of predictions.
[00:16:24.140 --> 00:16:26.780]   All the PyTorch models for this course have
[00:16:26.780 --> 00:16:28.960]   an early stopping parameter and a bunch of
[00:16:28.960 --> 00:16:31.900]   related parameters that will help you set this up in
[00:16:31.900 --> 00:16:34.960]   a way that will allow you to do this incremental testing,
[00:16:34.960 --> 00:16:37.580]   and with luck, get the best model.
[00:16:37.580 --> 00:16:39.980]   If you need a little bit of motivation for this,
[00:16:39.980 --> 00:16:42.320]   here are some plots from an actual model.
[00:16:42.320 --> 00:16:44.460]   You can see the loss going down very
[00:16:44.460 --> 00:16:47.700]   steadily across different iterations of training.
[00:16:47.700 --> 00:16:51.780]   But the performance on the dev set tells a very different story.
[00:16:51.780 --> 00:16:54.160]   You can see based on this performance that at
[00:16:54.160 --> 00:16:58.420]   a certain very early point in this process around iteration 10,
[00:16:58.420 --> 00:17:00.560]   our performance was actually getting worse
[00:17:00.560 --> 00:17:02.620]   even though the loss was going down.
[00:17:02.620 --> 00:17:06.340]   That just shows you that sometimes the steady loss curve is
[00:17:06.340 --> 00:17:09.220]   a picture of overfitting and not of
[00:17:09.220 --> 00:17:13.140]   your model actually getting better at the thing that you care about.
[00:17:13.140 --> 00:17:16.580]   Think carefully about your stopping criteria.
[00:17:16.580 --> 00:17:20.100]   In general though, I think we might want to take
[00:17:20.100 --> 00:17:24.380]   a more expansive view of how we do evaluation in this mode.
[00:17:24.380 --> 00:17:26.900]   Here what I'm pitching is that we look at
[00:17:26.900 --> 00:17:29.740]   the entire performance curve maybe with
[00:17:29.740 --> 00:17:33.180]   confidence intervals so we can make some confident distinctions.
[00:17:33.180 --> 00:17:36.420]   All these plots for different conditions across models we were
[00:17:36.420 --> 00:17:42.060]   comparing have epochs along the x-axis and F1 along the y-axis.
[00:17:42.060 --> 00:17:46.020]   If you step back, what I think you see is that our mittens model,
[00:17:46.020 --> 00:17:48.400]   the one that we were advocating for,
[00:17:48.400 --> 00:17:53.240]   is the best model on average but largely in early parts of training.
[00:17:53.240 --> 00:17:54.740]   If you train for long enough,
[00:17:54.740 --> 00:17:57.820]   a lot of the distinctions disappear.
[00:17:57.820 --> 00:18:00.500]   If you have a fixed budget of epochs,
[00:18:00.500 --> 00:18:01.760]   mittens is a good choice.
[00:18:01.760 --> 00:18:03.780]   If you don't care about the resources,
[00:18:03.780 --> 00:18:06.300]   it might not be so clear which one you should choose,
[00:18:06.300 --> 00:18:07.560]   maybe it doesn't matter.
[00:18:07.560 --> 00:18:10.960]   That's a nuanced lesson that I think is really powerful to
[00:18:10.960 --> 00:18:13.740]   teach and we can't really teach it if all we
[00:18:13.740 --> 00:18:16.380]   do is offer point estimates of model performance.
[00:18:16.380 --> 00:18:20.780]   We really need to see the full curve to see that level of nuance.
[00:18:20.780 --> 00:18:23.980]   I know that NLPers love their results tables,
[00:18:23.980 --> 00:18:25.660]   you should have results tables,
[00:18:25.660 --> 00:18:28.700]   but maybe you could supplement them with some figures that would
[00:18:28.700 --> 00:18:32.620]   give us a fuller picture of what was going on.
[00:18:32.620 --> 00:18:37.180]   Final topic, the role of random parameter initialization.
[00:18:37.180 --> 00:18:40.880]   Most deep learning models have parameters that are random at the start.
[00:18:40.880 --> 00:18:42.100]   Even if they're pre-trained,
[00:18:42.100 --> 00:18:45.500]   there are usually some random parameters in the mix there.
[00:18:45.500 --> 00:18:47.700]   This is clearly meaningful for
[00:18:47.700 --> 00:18:50.420]   the non-convex problems that we're posing.
[00:18:50.420 --> 00:18:52.560]   Simpler models can be impacted as well,
[00:18:52.560 --> 00:18:55.100]   but it's especially pressing in the deep learning era.
[00:18:55.100 --> 00:18:57.980]   Here is a relatively recent paper showing actually that
[00:18:57.980 --> 00:19:00.820]   different initializations for neural sequence models
[00:19:00.820 --> 00:19:05.280]   led to statistically significant differences in performance.
[00:19:05.280 --> 00:19:09.340]   A number of recent systems were actually indistinguishable in terms of
[00:19:09.340 --> 00:19:13.920]   their raw performance once we took this source of variation into account.
[00:19:13.920 --> 00:19:15.780]   There's a related issue here of
[00:19:15.780 --> 00:19:19.100]   catastrophic failure from unlucky initializations.
[00:19:19.100 --> 00:19:21.920]   Sometimes that happens, sometimes you see it,
[00:19:21.920 --> 00:19:23.880]   sometimes it's hard to notice.
[00:19:23.880 --> 00:19:28.300]   There's a question of how to report that as part of overall system performance.
[00:19:28.300 --> 00:19:30.540]   We need to be reflective about this.
[00:19:30.540 --> 00:19:32.940]   Maybe the bottom line here is just for
[00:19:32.940 --> 00:19:36.160]   the associated notebook for this unit, evaluation methods.
[00:19:36.160 --> 00:19:39.880]   I just showed you with the classic XOR problem,
[00:19:39.880 --> 00:19:41.220]   which has always been used to
[00:19:41.220 --> 00:19:44.200]   motivate the powerful models that we work with now,
[00:19:44.200 --> 00:19:46.580]   that you don't actually get success for
[00:19:46.580 --> 00:19:49.320]   a simple feed-forward network for that problem.
[00:19:49.320 --> 00:19:51.620]   Eight out of 10 times it succeeds,
[00:19:51.620 --> 00:19:54.680]   and two out of the 10 times it's a colossal failure.
[00:19:54.680 --> 00:19:59.060]   That is a glimpse of just how important initialization can be.
[00:19:59.060 --> 00:20:03.120]   Since we don't analytically understand why we're seeing this variation,
[00:20:03.120 --> 00:20:08.620]   the best response if you can afford it is a bunch more experiments.
[00:20:08.620 --> 00:20:13.460]   Let's wrap up. A lot of this in the back of my mind is
[00:20:13.460 --> 00:20:15.780]   oriented toward helping you with the protocols,
[00:20:15.780 --> 00:20:19.140]   which is a document associated with your final project,
[00:20:19.140 --> 00:20:21.800]   where you give us the nuts and bolts of the project,
[00:20:21.800 --> 00:20:25.180]   and try to identify any obstacles to success.
[00:20:25.180 --> 00:20:28.460]   All the lessons we've been teaching throughout this series are
[00:20:28.460 --> 00:20:30.840]   oriented toward helping you think critically about
[00:20:30.840 --> 00:20:36.020]   this protocol and ultimately set up a firm foundation for your project.
[00:20:36.020 --> 00:20:38.060]   With that out of the way, I thought I would look
[00:20:38.060 --> 00:20:41.340]   ahead to this moment and the future.
[00:20:41.340 --> 00:20:46.260]   I think this is an ideal moment for innovation in surprising new places.
[00:20:46.260 --> 00:20:50.500]   Architecture innovation, way overrated at this point.
[00:20:50.500 --> 00:20:51.900]   I mean, it's still important,
[00:20:51.900 --> 00:20:55.440]   but it is overrated relative to the amount of things people do.
[00:20:55.440 --> 00:20:58.760]   Metric innovation, way underrated.
[00:20:58.760 --> 00:21:02.880]   It's been a theme of these lectures that we need to think very carefully about
[00:21:02.880 --> 00:21:05.360]   our metrics because they are our guideposts
[00:21:05.360 --> 00:21:08.040]   toward whether we're succeeding or not.
[00:21:08.040 --> 00:21:10.420]   Relatedly, evaluations in general.
[00:21:10.420 --> 00:21:11.620]   We need innovation there.
[00:21:11.620 --> 00:21:14.840]   This is way underrated by the community at this point.
[00:21:14.840 --> 00:21:16.900]   Task innovation, underrated.
[00:21:16.900 --> 00:21:18.080]   We are seeing some things,
[00:21:18.080 --> 00:21:19.840]   so it's not as bad as two and three,
[00:21:19.840 --> 00:21:23.280]   but still we should all be participating in this area.
[00:21:23.280 --> 00:21:26.520]   Then finally, exhaustive hyperparameter search.
[00:21:26.520 --> 00:21:29.120]   You need to weigh this against other factors.
[00:21:29.120 --> 00:21:33.440]   There is more at play here than just that pristine scientific paradigm.
[00:21:33.440 --> 00:21:36.600]   We need to think about costs in every sense and
[00:21:36.600 --> 00:21:40.000]   how it relates to the innovations that we're likely to see.
[00:21:40.000 --> 00:21:41.980]   I'm pitching a pragmatic approach,
[00:21:41.980 --> 00:21:44.360]   but I'm also exhorting you to think
[00:21:44.360 --> 00:21:49.760]   expansively about how you might participate in pushing the field forward.
[00:21:49.760 --> 00:21:59.760]   [BLANK_AUDIO]

