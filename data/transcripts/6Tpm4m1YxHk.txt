
[00:00:00.000 --> 00:00:22.880]   Hey guys, thanks for being here. I'm Sahil. I am here with Hari. We're presenting on AI.
[00:00:22.880 --> 00:00:28.000]   Obviously, we're talking about trust, but let me give you a little background about us.
[00:00:28.700 --> 00:00:41.480]   Over the past 10 years, we have deployed AI in various industries from health monitoring to industrial IoT to network automation in telecom networks.
[00:00:41.480 --> 00:00:47.800]   And there's been one question that has been asked all along, all this time. Can we trust AI?
[00:00:47.800 --> 00:00:55.220]   Some of these systems are used in mission-critical applications, but the question really is,
[00:00:55.700 --> 00:01:01.000]   can we trust the inferences of these AI systems because they are impacting businesses,
[00:01:01.000 --> 00:01:04.920]   they're impacting business decisions, and the bottom line at the end of the day?
[00:01:04.920 --> 00:01:07.120]   So we're going to explore that topic today.
[00:01:07.120 --> 00:01:10.720]   With that, let me get us started.
[00:01:13.980 --> 00:01:18.060]   All right. So, you know, just like any presentation, we'll start with some stats.
[00:01:18.060 --> 00:01:24.280]   So, McKinsey is saying 78% of the companies are adopting AI.
[00:01:24.280 --> 00:01:28.860]   There's another research from Evi says 95% investing in AI.
[00:01:28.860 --> 00:01:30.220]   But here's the problem.
[00:01:30.220 --> 00:01:38.260]   Only 11% of the companies are focused on AI's governance, ensuring safe practices with AI.
[00:01:38.260 --> 00:01:47.480]   So that 67% gap is going to be a huge problem because it's not just about implementing the AI in the right way,
[00:01:47.480 --> 00:01:51.600]   it's also about understanding the impact of that AI in the long run.
[00:01:51.600 --> 00:01:54.780]   And let me quantify that with some examples.
[00:01:55.780 --> 00:01:59.140]   So you see a couple of examples here, telecom disruption.
[00:01:59.140 --> 00:02:04.520]   What really happened here was AI made some decision.
[00:02:04.520 --> 00:02:06.800]   Based on that, there was a network disruption.
[00:02:06.800 --> 00:02:14.600]   Now, if you look at AT&T's and Verizon's of the world, they're spending millions of dollars each minute the network is not working.
[00:02:15.600 --> 00:02:22.600]   Another example here is a gas sensor misinterpreted data that put human lives at risk.
[00:02:22.600 --> 00:02:33.460]   Third company lost millions of dollars because the supply chain company, AI, screwed up the SKUs and, you know, ended up in losses.
[00:02:33.900 --> 00:02:38.400]   So what we're trying to say here is that these are silent failures.
[00:02:38.400 --> 00:02:42.700]   You cannot quantify the impact of these failures ahead of time.
[00:02:42.700 --> 00:02:44.440]   You cannot see them coming ahead of time.
[00:02:44.440 --> 00:02:48.680]   But they are worth millions and billions of dollars over time.
[00:02:48.680 --> 00:02:54.760]   So this is extremely, extremely impactful as AI is getting adopted.
[00:02:57.400 --> 00:03:03.480]   So, taking a view of what trustworthy AI looks like, there are three main pillars.
[00:03:03.480 --> 00:03:05.000]   You talk about explainability.
[00:03:05.000 --> 00:03:12.540]   When you're talking about explainability, I think the most important thing is having a view of what's really under the hood.
[00:03:12.540 --> 00:03:15.400]   Otherwise, you're just flying blind.
[00:03:15.400 --> 00:03:20.260]   You have to understand why those inferences are being made on what basis.
[00:03:20.260 --> 00:03:22.140]   The other thing is traceability.
[00:03:22.140 --> 00:03:24.640]   It's like a flight recorder.
[00:03:25.040 --> 00:03:27.640]   It's capturing all the audit trails.
[00:03:27.640 --> 00:03:30.360]   It's ensuring that you can retrace the steps.
[00:03:30.360 --> 00:03:37.160]   And based on that, you can understand the particular situation, recreate the situation, and being able to solve it again.
[00:03:37.160 --> 00:03:39.700]   Guardrails are extremely important.
[00:03:39.700 --> 00:03:43.280]   They are to ensure that you don't end up in millions of dollars of losses.
[00:03:43.280 --> 00:03:47.400]   There is some threshold where it stops.
[00:03:47.400 --> 00:03:48.340]   The AI has got to stop.
[00:03:48.340 --> 00:03:53.060]   So, together, all of these build trust in a real system.
[00:03:53.220 --> 00:03:59.600]   More importantly, when you talk about real-world scenarios where you're implementing this, you're talking about scalability.
[00:03:59.600 --> 00:04:05.260]   These are the pillars to think about when you're scaling them in the real world.
[00:04:07.080 --> 00:04:09.200]   I'll have Hari talk about the pillars of trust.
[00:04:09.200 --> 00:04:10.560]   Hey, thanks, Sahil.
[00:04:10.560 --> 00:04:21.980]   So, every mission-critical system that we rely on today, be it aircrafts, be it energy grids, or be it even the simple banking financial systems, are built on principles of safety and understanding.
[00:04:21.980 --> 00:04:22.600]   Right?
[00:04:22.960 --> 00:04:27.880]   So, looking at the first pillar, our AI systems should be no different, so looking at the first pillar, right?
[00:04:27.880 --> 00:04:31.420]   First, AI has to show its work.
[00:04:31.420 --> 00:04:34.340]   Every important decision shouldn't be a mystery.
[00:04:35.180 --> 00:04:50.640]   It should come with a simple English explanation so that an end user, a decision maker, somebody who is auditing the system, is able to act on the information and not look for a data scientist to explain or translate what the system actually means.
[00:04:50.640 --> 00:04:51.820]   That's the first pillar.
[00:04:51.820 --> 00:04:54.260]   The second pillar, adaptive control.
[00:04:54.520 --> 00:04:55.560]   What do we mean by that?
[00:04:55.560 --> 00:04:57.600]   It's about building smart guardrains.
[00:04:57.600 --> 00:05:06.780]   If the AI system starts to wear off, makes a wrong decision, the system should be able to slow down, change its course, or at least call a human for help.
[00:05:06.780 --> 00:05:10.220]   Think of it as a lane assist for your AI.
[00:05:10.220 --> 00:05:13.400]   The third pillar is always have human in the loop.
[00:05:13.400 --> 00:05:14.980]   What do we mean by that?
[00:05:14.980 --> 00:05:24.260]   This is basically setting up the roles and the playbooks so that the right experts get pinged in the right time with the right information.
[00:05:24.260 --> 00:05:28.240]   without causing an overhead for both the system as well as the person, right?
[00:05:28.240 --> 00:05:33.020]   But all of these things are built on the bedrock foundation of traceability.
[00:05:33.020 --> 00:05:38.780]   Every data, every change is digitally signed and is trackable.
[00:05:38.780 --> 00:05:42.980]   Think of the concept of it like software bill of materials or even simple.
[00:05:42.980 --> 00:05:45.060]   Think of it like your FedEx package.
[00:05:45.060 --> 00:05:50.700]   From the time it leaves the warehouse till it reaches your doorstep, you can track every single step of it.
[00:05:51.180 --> 00:05:55.060]   So this was our, this is the three pillars of a trustworthy AI.
[00:05:55.060 --> 00:06:04.420]   But with these pillars in place, the larger question is, how do we actually weave them into the AI systems we are building and running today?
[00:06:04.420 --> 00:06:04.960]   Right?
[00:06:04.960 --> 00:06:06.040]   Let's look into that journey.
[00:06:10.380 --> 00:06:16.020]   So, like I said, how do we make these pillars reality in day-to-day AI operations?
[00:06:16.020 --> 00:06:24.940]   This is where we move beyond the standard MLOps and what we call it as X-Tops.
[00:06:24.940 --> 00:06:31.520]   Think of it as an MLOps, but with built-in conscience and a direct line of human oversight.
[00:06:32.320 --> 00:06:34.400]   This diagram isn't just a flow chart.
[00:06:34.400 --> 00:06:38.040]   It's the blueprint for the entire life cycle of AI.
[00:06:38.040 --> 00:06:41.680]   Let's begin with the verifiable traceability.
[00:06:41.680 --> 00:06:45.700]   Right from the data stage, know where your data comes from.
[00:06:45.700 --> 00:06:48.940]   Understand what are all the changes and how it is changing.
[00:06:48.940 --> 00:06:50.140]   No more guess words.
[00:06:50.980 --> 00:06:53.780]   When we train the models, we just don't train them for accuracy.
[00:06:53.780 --> 00:06:54.420]   Right?
[00:06:54.420 --> 00:06:57.020]   We are embedding actionable intelligibility.
[00:06:57.020 --> 00:07:04.980]   What does it mean is the model also learned to explain itself so that we can spot when its reason starts to drift.
[00:07:05.860 --> 00:07:10.320]   When we deploy, right, we put those adaptive cruise controls that we talk about.
[00:07:10.320 --> 00:07:13.000]   This is where the guardrails kicks in.
[00:07:13.000 --> 00:07:21.640]   Automatically adjusting to new situation, new data, and pausing to look at things if they drift.
[00:07:21.640 --> 00:07:25.980]   And when we deploy the model, this is where the human AI teaming comes in.
[00:07:25.980 --> 00:07:26.580]   Right?
[00:07:26.580 --> 00:07:35.460]   This is where the actual real-world feedback kicks in so that we could quickly improve the system and humans can step in when needed.
[00:07:35.460 --> 00:07:47.220]   X-Tops is about creating a system where every AI decision has a clear why, a when, and a who, and attached to it.
[00:07:47.220 --> 00:07:53.280]   It is about moving from just launching an AI system to launching an AI which we can truly trust.
[00:07:53.280 --> 00:07:58.940]   So, let's pause here.
[00:07:58.940 --> 00:07:59.440]   Right?
[00:07:59.440 --> 00:08:02.720]   Now, you all might be thinking, hey, we do MLOps day in and day out.
[00:08:03.140 --> 00:08:05.960]   Most of these modules that we spoke about is already there.
[00:08:05.960 --> 00:08:07.920]   So, what is unique, right?
[00:08:07.920 --> 00:08:10.520]   What is the big difference in doing this?
[00:08:10.520 --> 00:08:13.820]   The challenge is adopting an X-Tops is like a journey.
[00:08:13.820 --> 00:08:15.340]   It's not a flip of a switch.
[00:08:15.340 --> 00:08:25.060]   X-Tops is also taking about all those foundational pieces that we have and giving them a serious integrated upgrade, especially for trust.
[00:08:25.940 --> 00:08:29.320]   I'm not going to go through all of this, but let me touch upon a couple of things.
[00:08:29.320 --> 00:08:31.300]   Let's think guardrails and policies.
[00:08:31.300 --> 00:08:32.660]   We have IAM policies.
[00:08:32.660 --> 00:08:33.780]   We have security policies.
[00:08:33.780 --> 00:08:34.980]   MLOps providers, everything.
[00:08:34.980 --> 00:08:35.720]   Right?
[00:08:35.720 --> 00:08:44.760]   But X-Tops gives you dynamic AI-aware guardrails that you can actually understand the context and block a risky AI decision.
[00:08:45.620 --> 00:08:47.400]   Let's talk about monitoring in metrics.
[00:08:47.400 --> 00:08:49.440]   We do have standard MLOps metrics.
[00:08:49.440 --> 00:08:50.020]   Right?
[00:08:50.020 --> 00:08:57.440]   But X-Tops gives you dedicated trust-specific dashboards that both your leadership and the boards can understand.
[00:08:57.440 --> 00:08:59.040]   Human in the feedback.
[00:08:59.040 --> 00:09:04.140]   We do have human in the loop, but it is mostly ad hoc when it comes to MLOps.
[00:09:04.800 --> 00:09:07.040]   X-Tops, think of it as creating a fast lane.
[00:09:07.040 --> 00:09:12.880]   You click to fix workflows where human can look at some of these quick changes and go back and fix it.
[00:09:12.880 --> 00:09:13.320]   Right?
[00:09:13.320 --> 00:09:17.960]   The larger context is X-Tops is not reinventing the wheel.
[00:09:17.960 --> 00:09:18.540]   Right?
[00:09:18.540 --> 00:09:26.260]   It's about adding advanced safety and transparency features needed for the high-stakes enterprise of AI.
[00:09:27.540 --> 00:09:37.380]   And what is in for us, we spend less time firefighting, unpredictable AI behaviors, and spend more time actually building more innovative products.
[00:09:37.380 --> 00:09:48.440]   So, if you are serious about managing AI trust, you also need to measure what matters.
[00:09:48.440 --> 00:09:48.880]   Right?
[00:09:48.880 --> 00:09:54.780]   So, we talk about two metrics here, MTTRE and trust-adjusted risk and dollars.
[00:09:55.420 --> 00:09:59.640]   The MTTRE stands for Mean Time to Resolve Explainable Errors.
[00:09:59.640 --> 00:10:03.060]   Fancy name, but very simple idea.
[00:10:03.060 --> 00:10:12.700]   It's basically the time that takes for us to fix something unexpected when it happens to how quickly can we understand the why and response with the fix.
[00:10:12.700 --> 00:10:20.740]   The faster your MTTRE is, the team is more agile, less defects in the product, and quicker to solve the problems.
[00:10:20.740 --> 00:10:24.140]   Second, trust-adjusted risk and dollars.
[00:10:25.300 --> 00:10:31.220]   This idea is basically to put a price tag on what happens when the trust breaks.
[00:10:31.220 --> 00:10:32.080]   Right?
[00:10:32.080 --> 00:10:33.640]   What is actually the business cost?
[00:10:33.640 --> 00:10:34.520]   Is it fines?
[00:10:34.520 --> 00:10:35.820]   Is it lost customers?
[00:10:35.820 --> 00:10:37.360]   Is it damaged reputation?
[00:10:37.360 --> 00:10:38.220]   Right?
[00:10:38.220 --> 00:10:45.320]   And if your AI system keeps failing or remains a black box, this metric makes value of the trust.
[00:10:53.740 --> 00:10:55.120]   So, let me again pause here.
[00:10:55.120 --> 00:10:57.160]   We spoke about metrics.
[00:10:57.160 --> 00:10:59.960]   So, why obsess about all these metrics?
[00:10:59.960 --> 00:11:00.380]   Right?
[00:11:00.380 --> 00:11:02.740]   This is, we have enough of metrics in MLRs.
[00:11:02.740 --> 00:11:03.880]   We have enough of metrics.
[00:11:03.880 --> 00:11:04.580]   But why obsess?
[00:11:04.580 --> 00:11:06.100]   The challenge is this.
[00:11:06.100 --> 00:11:07.600]   Look at the first table.
[00:11:07.600 --> 00:11:08.220]   Right?
[00:11:08.220 --> 00:11:14.040]   On an average, an MTTRE takes several months in some of these cases to even find a resolution.
[00:11:14.040 --> 00:11:14.760]   Right?
[00:11:14.980 --> 00:11:16.480]   Now, imagine this.
[00:11:16.480 --> 00:11:20.320]   Imagine your AI is making a biased decision for months.
[00:11:20.320 --> 00:11:25.940]   The damage escalates quickly, and sometimes it also escalates exponentially.
[00:11:25.940 --> 00:11:28.080]   Now, look at the second table.
[00:11:28.080 --> 00:11:29.660]   It actually shows the fallout.
[00:11:29.660 --> 00:11:30.160]   Right?
[00:11:30.160 --> 00:11:31.900]   It is not just not one parameter.
[00:11:31.900 --> 00:11:35.280]   It starts with your direct fines.
[00:11:35.280 --> 00:11:43.740]   It starts with your engineering effort, regulatory scrutiny, and above all, the loss of trust and brand value of the products that we stand day in and day out for.
[00:11:43.740 --> 00:11:44.200]   Right?
[00:11:44.200 --> 00:11:45.700]   These are in small figures.
[00:11:45.700 --> 00:11:52.760]   A serious incident like a privacy bug or a bias in a credit card system could quickly escalate up to 700 millions of dollars.
[00:11:52.760 --> 00:11:53.560]   Right?
[00:11:53.560 --> 00:11:57.080]   So, this is why these metrics are not just about defense.
[00:11:57.080 --> 00:12:04.380]   These are about building resilient, reliable, and ultimately AI-powered products that the end users can trust.
[00:12:04.380 --> 00:12:08.920]   All said, and we are not talking out of thin air.
[00:12:08.920 --> 00:12:16.980]   So, Sahil is going to present a case study on a real incident and how we went about building this whole framework.
[00:12:16.980 --> 00:12:18.900]   All right.
[00:12:18.900 --> 00:12:19.420]   Perfect.
[00:12:19.420 --> 00:12:23.600]   So, that's the right stage for let's bring it all together.
[00:12:23.600 --> 00:12:26.460]   I'm going to talk about a company called GuardHat.
[00:12:26.460 --> 00:12:28.720]   This is a company that I used to work for.
[00:12:28.720 --> 00:12:31.840]   It's focused on worker safety.
[00:12:32.180 --> 00:12:41.760]   So, more specifically, it has an AI-driven platform that is geared towards solving worker safety problems in hazardous environments.
[00:12:41.760 --> 00:12:50.140]   So, what we're doing here is we built IoT devices, wearable devices, that would be worn by the workers.
[00:12:50.460 --> 00:12:57.880]   And then, at some point in time, these devices would get deployed, activated, and they will collect data.
[00:12:57.880 --> 00:13:03.660]   They'll collect health data as well as environmental data.
[00:13:03.660 --> 00:13:09.940]   And then, that data is sent to the backend system where the AI analyzes this in real time.
[00:13:10.520 --> 00:13:18.980]   And based on that, it is able to identify when an incident, predict when an incident is about to happen, and, you know, you can prevent that incident from happening.
[00:13:18.980 --> 00:13:22.260]   So, very mission-critical application.
[00:13:22.260 --> 00:13:30.660]   It was great because while we were saving lives in a way, there were enormous challenges.
[00:13:31.240 --> 00:13:35.660]   One of the inputs to the AI platform was the GPS.
[00:13:35.660 --> 00:13:42.360]   And as a result, 70% of the cases were false positives.
[00:13:42.360 --> 00:13:48.520]   And it's easier to say this now because it's after the fact, but back then, we didn't know that.
[00:13:48.520 --> 00:13:59.760]   And so, the behavior of the user was that at that point in time, the user stopped reacting to the alerts.
[00:13:59.880 --> 00:14:15.260]   They started ignoring the alerts, and that caused a huge safety risk, not just for the people, of course, their lives at stake, but even for the company, from the liability point of view, that workers were not reacting to alerts.
[00:14:15.260 --> 00:14:18.880]   So, we went back to the drawing board.
[00:14:18.880 --> 00:14:21.560]   We started identifying the issues.
[00:14:22.560 --> 00:14:30.400]   And, you know, if we were to do this without the X-DOPs framework, we would probably do an MTTR, mean time to resolution.
[00:14:30.400 --> 00:14:35.020]   If you look at it, you know, 70% of the time is spent in identifying the problem.
[00:14:35.020 --> 00:14:40.160]   Another 20% is spent in finding a solution, and then you deploy it.
[00:14:40.580 --> 00:14:46.260]   But I think the most critical part is that there is no system to identify the GPS drift.
[00:14:46.260 --> 00:14:47.460]   We wouldn't know about it.
[00:14:47.460 --> 00:14:53.760]   And then, because it's such a complicated model and code, that it's really hard to identify what's causing that problem.
[00:14:54.920 --> 00:15:04.940]   So, if you were to apply this model, day zero, you get an alert that was ignored during an incident.
[00:15:04.940 --> 00:15:09.660]   Day two, there is an attribution telemetry that will flag the anomaly.
[00:15:10.160 --> 00:15:18.400]   And day seven, you have a solution deployed, which fixes the GPS drift, or at least finds a reroute to the GPS drift.
[00:15:18.400 --> 00:15:27.420]   Now, having said that, to be real, this problem did not get solved in seven days.
[00:15:27.420 --> 00:15:29.060]   It took eight months for us.
[00:15:29.060 --> 00:15:33.660]   But this was a model problem that actually helped us to build this framework.
[00:15:33.660 --> 00:15:38.400]   And once we were able to build this framework, these kind of problems can be solved in seven days.
[00:15:38.400 --> 00:15:43.220]   We tested it across our enterprise and eventually became an enterprise standard.
[00:15:43.220 --> 00:15:45.400]   So, all this is great.
[00:15:45.400 --> 00:15:47.460]   You have the impact.
[00:15:47.460 --> 00:15:51.040]   You have, you can see the value in this.
[00:15:51.040 --> 00:15:52.940]   But here's the big question.
[00:15:52.940 --> 00:15:58.680]   What do, how do you convince the CIOs?
[00:15:58.680 --> 00:16:02.720]   How do they look at all of this and find value in this?
[00:16:02.720 --> 00:16:04.180]   What is the language that they talk?
[00:16:04.180 --> 00:16:06.140]   The answer is.
[00:16:08.140 --> 00:16:12.620]   So, you've got to convince the CIOs that this is saving money.
[00:16:12.620 --> 00:16:22.200]   And if you were to look at the left side of the slide, you'll see that the risk exposure that we're looking at is approximately $2.5 million per site per year.
[00:16:22.200 --> 00:16:33.160]   Now, some direct impact with this is that we were able to solve the, with this product, or this structure, we were able to solve the fines.
[00:16:33.520 --> 00:16:37.800]   And we saved $500,000 in fines every year per site.
[00:16:37.800 --> 00:16:49.240]   Beyond this, some of the indirect benefit was that this system, if it were to be working correctly, was supposed to prevent incidents, all of them.
[00:16:49.340 --> 00:16:53.480]   But it was preventing X percentage of incidences because it wasn't working correctly.
[00:16:53.480 --> 00:16:56.020]   But with this structure, it did work correctly.
[00:16:56.020 --> 00:17:02.060]   And then after that, you got the remaining value as well.
[00:17:02.060 --> 00:17:06.560]   So, I just want to wrap it up real quick.
[00:17:06.560 --> 00:17:12.620]   I think the outcome, you can see a lot of value there in terms of, you know, false alerts came down.
[00:17:12.620 --> 00:17:14.040]   Trust score went up.
[00:17:14.100 --> 00:17:17.080]   That means people started using those alerts.
[00:17:17.080 --> 00:17:19.880]   They were able to see value in those alerts.
[00:17:19.880 --> 00:17:33.660]   I think more important things were related to the telemetry itself, understanding why a particular inference was made, having the control where you can, if there's a GPS drift, you were able to switch.
[00:17:33.820 --> 00:17:36.480]   And then most important thing, human in the loop.
[00:17:36.480 --> 00:17:39.960]   So, if these things happen, someone is notified.
[00:17:39.960 --> 00:17:46.520]   We created a dashboard where someone is notified and someone is able to take action and retrain the model.
[00:17:46.520 --> 00:17:56.240]   So, with that, I mean, this slide is just a high-level overview of what we presented to you today.
[00:17:57.920 --> 00:17:59.280]   Thank you for being here.
[00:17:59.280 --> 00:18:01.860]   We'll just leave it at that.
[00:18:01.860 --> 00:18:08.100]   Thank you so much for the fantastic talk on the trust gap.
[00:18:08.100 --> 00:18:11.300]   I actually had a question for you because we have a minute or so.
[00:18:11.300 --> 00:18:20.760]   So, the phrasing that you had around, like, the trust-adjusted risk cost premium, how do you advise people to think about, like, reputational damage?
[00:18:20.760 --> 00:18:25.220]   Like, is this something that you have thought about measuring or investigating at all?
[00:18:25.220 --> 00:18:27.520]   You want to take it short?
[00:18:27.520 --> 00:18:32.460]   So, it's, like I was saying in the beginning, these are silent failures.
[00:18:32.460 --> 00:18:36.020]   You cannot, it's really hard to quantify the impact.
[00:18:36.020 --> 00:18:39.400]   And reputational damage is, again, right on top of that list.
[00:18:39.400 --> 00:18:43.540]   So, it's really hard to measure that, to be honest.
[00:18:43.540 --> 00:18:50.020]   But all you can do in this kind of a case is, you know, you can find, you know, people are creative.
[00:18:50.020 --> 00:18:52.520]   You can find ways to quantify some dollars to it.
[00:18:52.520 --> 00:18:54.940]   But, you know, it's really hard to predict.
[00:18:55.240 --> 00:18:57.780]   I mean, there's no short answer to it, let me just say that.
[00:18:57.780 --> 00:19:02.240]   I mean, there's no short answer to it, let me just say that.
[00:19:02.240 --> 00:19:02.780]   I mean, there's no short answer to it.

