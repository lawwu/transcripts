
[00:00:00.000 --> 00:00:09.520]   Welcome, everyone. Today, we're here to talk about the future of ML with one of the awesome
[00:00:09.520 --> 00:00:14.400]   companies who's building it, Weights & Biases. Thanks so much for being here with us today.
[00:00:14.400 --> 00:00:20.720]   So I'm Ronella from the community team at Repl.it, and we've got some special guests
[00:00:20.720 --> 00:00:27.440]   here with us today. So let's do some quick intros. We've got two awesome guests from
[00:00:27.440 --> 00:00:34.160]   Weights & Biases with us. We've got Morgan. Morgan's a growth ML engineer at Weights & Biases
[00:00:34.160 --> 00:00:38.800]   and has a background in natural language processing. Previously worked at Facebook
[00:00:38.800 --> 00:00:44.960]   on the safety team where he helps classify and flag potentially high severity content for removal.
[00:00:44.960 --> 00:00:51.520]   And then we've also got Lucas with us. Lucas is the CEO and co-founder of Weights & Biases,
[00:00:51.520 --> 00:00:59.440]   which is a developer-first ML ops platform. He also co-founded Figure8, formerly known as
[00:00:59.440 --> 00:01:06.000]   CrowdFlower, which is a pioneer in the ML data labeling space. And Figure8 was acquired by Appen
[00:01:06.000 --> 00:01:13.120]   in 2019. Lucas has dedicated his career to optimizing ML workflows, teaching ML practitioners,
[00:01:13.120 --> 00:01:17.840]   making machine learning more accessible to all, and occasionally tinkering with robots.
[00:01:17.840 --> 00:01:25.840]   So awesome to have you both here with us. And then we've also got Amjad, who is our CEO at Repl.it.
[00:01:25.840 --> 00:01:33.680]   Amjad is a Jordanian-American entrepreneur and engineer, for those of you who don't know him.
[00:01:33.680 --> 00:01:40.240]   He's our founder and CEO. And before Repl.it, Amjad worked at Facebook, and he oversaw the
[00:01:40.240 --> 00:01:45.600]   JavaScript infrastructure team, building some of the world's most popular open-source developer
[00:01:45.600 --> 00:01:51.280]   tools. And before that, he was a founding engineer at Codecademy. So welcome, everyone. We're so glad
[00:01:51.280 --> 00:01:57.040]   to have all of you here with us today. And thanks, everyone, in the chat for participating today.
[00:01:57.040 --> 00:02:06.560]   So we're going to get started with a really awesome example of Weights & Biases and Repl.it
[00:02:06.560 --> 00:02:11.680]   together at work. So Morgan is going to start us off with an awesome demo.
[00:02:13.280 --> 00:02:21.200]   Brilliant. Thanks, Ernella. So I'll start first by just giving a bit of context around what Weights
[00:02:21.200 --> 00:02:30.160]   & Biases does and the problems it's trying to solve. And so be it if you're a student or a
[00:02:30.160 --> 00:02:36.800]   researcher trying to publish an amazing machine learning paper, get a state-of-the-art result,
[00:02:36.800 --> 00:02:41.440]   speak at a famous conference, or if you're on the product side and in industry and you're trying to
[00:02:41.440 --> 00:02:51.120]   build apps that do all this amazing work, like ChatGPT does, there's a huge amount of work and
[00:02:51.120 --> 00:02:58.240]   experimentation that goes into either one of these. And the reality of building anything great is a
[00:02:58.240 --> 00:03:04.880]   little messy. And so Weights & Biases tries to address and corral some of that messiness.
[00:03:04.880 --> 00:03:12.240]   Specifically in terms of tracking how you're experimenting with your ML models, the results
[00:03:12.240 --> 00:03:17.520]   you're getting, but also the inputs you're putting in in terms of data, the potentially
[00:03:17.520 --> 00:03:21.680]   pre-trained models, and any hyperparameter settings that go into your model.
[00:03:21.680 --> 00:03:28.960]   And so how do people do this today? Some people are still maybe like log into the text files,
[00:03:28.960 --> 00:03:36.480]   they have their own custom solutions, which might be fine for you working alone on a small weekend
[00:03:36.480 --> 00:03:41.360]   project. It's a pretty manageable thing, but gets a little tricky when it comes to working with
[00:03:41.360 --> 00:03:46.320]   others. Or if you want to go back and revisit these results in six months time, some people
[00:03:46.320 --> 00:03:53.360]   might be using Google Sheets. And again, it has its place and works for smaller single player
[00:03:53.360 --> 00:04:08.800]   projects, but again, could be prone to forgetting to log crucial data or potentially just mislabeling
[00:04:08.800 --> 00:04:15.440]   data in your sheets. There are of course, like TensorBoard and solutions like this, where again,
[00:04:15.440 --> 00:04:22.640]   at the smaller scale for you working on smaller projects, tools like this work pretty well to
[00:04:22.640 --> 00:04:28.960]   keep track of your work. But the question is what happens when A, you want to revisit work after a
[00:04:28.960 --> 00:04:33.920]   long period of time, or B, you want to start to share this work and collaborate with others.
[00:04:33.920 --> 00:04:42.880]   Maybe it's a classmate beside you, maybe it's across your small ML team, maybe it's across
[00:04:42.880 --> 00:04:50.160]   a different machine learning team in your research lab or in your company. And so you quickly get
[00:04:50.160 --> 00:04:57.600]   these issues of people with their own custom login solutions and having trouble sharing their results
[00:04:57.600 --> 00:05:04.240]   between each other. We hit the classic example of people sharing their analysis that they've
[00:05:04.240 --> 00:05:09.600]   done in Jupyter Notebooks, but how do you know which notebook that analysis lived in? Those
[00:05:09.600 --> 00:05:16.080]   get shared around, like maybe an email, maybe you're doing a presentation to a principal
[00:05:16.080 --> 00:05:21.760]   investigator or senior management. And so you're going into PowerPoints, but again, where exactly
[00:05:21.760 --> 00:05:26.560]   did you leave that PowerPoint when you need to look back for it in three months time? Maybe you're
[00:05:26.560 --> 00:05:32.000]   dropping screenshots into Teams, Slack, we're in Discord, all this stays right. And so very quickly,
[00:05:32.000 --> 00:05:39.360]   it gets to be pretty unmanageable in terms of where you shared your analysis with your colleagues and
[00:05:39.360 --> 00:05:44.720]   where they can find it later. And so this is one of the things that Weights and Biases sets out to
[00:05:44.720 --> 00:05:52.800]   solve, taking these distributed, maybe fragile login solutions and bringing them into one system
[00:05:52.800 --> 00:05:57.760]   of records for all of your machine learning workloads. And so I'll give you a quick tour of
[00:05:57.760 --> 00:06:03.680]   what a Weights and Biases workspace looks like, and I'll show you how it works pretty seamlessly
[00:06:03.680 --> 00:06:09.280]   with Reblit, and especially with Reblit's new GPUs. And so this is an example of a Weights and
[00:06:09.280 --> 00:06:12.880]   Biases workspace where we've actually logged a hundred runs here. We're doing a lot of
[00:06:12.880 --> 00:06:19.280]   experimentation as is the case working in machine learning, trying to find the best results for our
[00:06:19.280 --> 00:06:24.240]   model. And you can see some of our metrics for the top metrics that we care about. I'll zoom
[00:06:24.240 --> 00:06:30.240]   a little bit there. So on the left-hand side, we have all of our runs here. And I won't go through
[00:06:30.240 --> 00:06:35.440]   this, but I want to emphasize tools like our parallel coordinates plot here, which lets you
[00:06:35.440 --> 00:06:41.280]   really see out of these hundred runs we've done, what are the most important hyperparameters or
[00:06:41.280 --> 00:06:47.520]   knobs that we can tweak in our training pipeline to get the most out of our model. And you can see
[00:06:47.520 --> 00:06:54.800]   some, for example, our learning rates in the center here work well in quite a narrow range,
[00:06:54.800 --> 00:07:02.560]   whereas others such as this early stopping rounds parameter here are a bit less sensitive to the
[00:07:02.560 --> 00:07:07.120]   value you set them. And so this can help you really squeeze the last bit out of your model,
[00:07:07.120 --> 00:07:14.080]   be it in terms of pushing your research to beat the state of the art or to get the very most
[00:07:14.080 --> 00:07:18.800]   out of your model when you're serving it to customers, be it in terms of latency or accuracy
[00:07:18.800 --> 00:07:25.840]   or whichever other metric you're interested in. And now weights and biases on a Repl.it. And so
[00:07:25.840 --> 00:07:35.520]   we have a Repl.it profile. It's replit.com/@wandb. And there, you'll see a few of our pinned
[00:07:36.880 --> 00:07:44.880]   repls. This quick start one is always a fun one to get started. And so here it's going to
[00:07:44.880 --> 00:07:50.240]   do some training. We can ignore this. It's just a small dependency issue. It's going to do some
[00:07:50.240 --> 00:07:57.920]   training of a small computer vision model on a popular dataset known as MNIST, which will then
[00:07:57.920 --> 00:08:03.840]   push the login results to weights and biases here. And so I've logged into weights and biases already,
[00:08:03.840 --> 00:08:10.320]   and you can see this already starting to kick off and start to add runs to my weights and biases
[00:08:10.320 --> 00:08:15.680]   project here. And we can go and take a look at some of those runs in this weights and biases project.
[00:08:15.680 --> 00:08:21.920]   I've logged a bunch of stuff already here. And so we have all of our training and validation
[00:08:21.920 --> 00:08:30.640]   metrics that we would like to see. But we've logged to tables here. And so tables is a tool
[00:08:30.640 --> 00:08:36.240]   that we have that lets you really get deeper into your data, be it in terms of before you start
[00:08:36.240 --> 00:08:41.840]   training or even understanding, in this case, how your model predictions look. So in this case,
[00:08:41.840 --> 00:08:46.880]   we have our predicted number here, which is a seven, and the target, which is also a seven.
[00:08:46.880 --> 00:08:52.560]   So again, pretty good guesses here. And so in tables, you can go a little bit further than just
[00:08:52.560 --> 00:08:58.480]   inspecting every sample, and you can group by some values here. Now you can see, for example,
[00:08:59.280 --> 00:09:06.480]   we've grouped by all of these zeros, and you can see each of those samples in your data set here.
[00:09:06.480 --> 00:09:12.400]   And this can be useful just to purely understand, to make sure that your data is labeled correctly,
[00:09:12.400 --> 00:09:19.120]   that you don't have sixes or eights when there should be zeros. And then in terms of evaluating
[00:09:19.120 --> 00:09:24.880]   your model, you can also look at the distribution of your predictions. And so this looks pretty good.
[00:09:25.680 --> 00:09:32.720]   Most of our predictions for zeros are indeed zeros, so looking pretty strong there.
[00:09:32.720 --> 00:09:42.560]   Now, I'm going to keep moving. Another REPL I wanted to show you today was our nano-GPT
[00:09:42.560 --> 00:09:51.200]   REPL. And so given all of the hype around LLMs at the moment, and this is a repo that
[00:09:51.200 --> 00:09:58.240]   Andrew Karpathy, who used to be the head of AI at Tesla and is a really well-known instructor
[00:09:58.240 --> 00:10:04.880]   or professor from Stanford and before that, this is a library he built, which is essentially the
[00:10:04.880 --> 00:10:13.600]   smallest library he can build to train a language model. And so this GPT model that he's training is
[00:10:13.600 --> 00:10:20.960]   very, very close to GPT-3 and all of the large language models you see in terms of architecture,
[00:10:20.960 --> 00:10:24.640]   those models, they have a few training tricks in them, but they're essentially just scaled-up
[00:10:24.640 --> 00:10:33.680]   versions of what Andrew has trained here today. And so I've actually fine-tuned one of his models
[00:10:33.680 --> 00:10:41.280]   here in REPL. We can stop that. And while that's stopping, this is one I've fine-tuned earlier. And
[00:10:41.280 --> 00:10:50.080]   so here we can see, again, our metrics for loss and validation loss. But the nice thing here in
[00:10:50.080 --> 00:10:56.400]   terms of the context of training language models is that we want to be able to visually inspect
[00:10:56.400 --> 00:11:02.000]   our results and understand, okay, the text this model is generating, is it any good?
[00:11:02.000 --> 00:11:07.760]   And so the goal of this model for context is to be able to generate text that sounds like
[00:11:07.760 --> 00:11:14.560]   Shakespeare. And so we train this for a thousand iterations and we can look at the outputs at the
[00:11:14.560 --> 00:11:19.680]   start of training, at the end of the training and see, okay, how is our model doing? And so this is
[00:11:19.680 --> 00:11:25.680]   at the start of the training, this is after 50 iterations and we were given this prompt to be or
[00:11:25.680 --> 00:11:31.280]   not to be, classic Shakespeare quotient, hoping it would lead on to something a little bit
[00:11:31.280 --> 00:11:38.320]   Shakespeare-like. But at this early stage, 50 iterations in, the text isn't particularly
[00:11:38.320 --> 00:11:43.920]   Shakespeare-like. It says, you need not wait the same time. The truth about our feelings,
[00:11:43.920 --> 00:11:47.760]   we often feel of feelings as a commodity, but think of feelings as a relationship.
[00:11:47.760 --> 00:11:52.960]   And so, because this model is also poor, it's also not the most eloquent text generator,
[00:11:52.960 --> 00:11:59.040]   but even the style of that language, it's not Shakespeare. So we can check out, okay,
[00:11:59.040 --> 00:12:04.640]   as we got closer to finishing training, we're up at 900 steps now, how is the text here?
[00:12:04.640 --> 00:12:11.840]   And if we take this one here, it starts again, to be or not to be, the night looks dark and the day
[00:12:11.840 --> 00:12:17.840]   of death is here tonight. Look at him or not. Juliet says, I'll go. Thomas says, Madam. And so
[00:12:17.840 --> 00:12:22.880]   you can clearly see, there's a quite a big difference there as the model has trained like
[00:12:22.880 --> 00:12:29.760]   another 850 steps. It's clearly sounding a lot more like Shakespeare, although still a little
[00:12:29.760 --> 00:12:37.840]   bit incoherent because of the small model size. But that's one other way that tables in Weights
[00:12:37.840 --> 00:12:42.880]   and Mises lets you kind of inspect things from the text generation side of things.
[00:12:42.880 --> 00:12:53.440]   You can go back here and check this out. And so here is our training or fine tuning. And so first,
[00:12:53.440 --> 00:13:00.800]   I just need to, in this case, we will log this to a private dashboard. And so it should see
[00:13:01.680 --> 00:13:08.800]   training kick off shortly. And so in this case, the model has been downloaded,
[00:13:08.800 --> 00:13:16.800]   pre-trained model has been downloaded and then gets loaded into Replica. And in the meantime,
[00:13:16.800 --> 00:13:24.000]   we can check that indeed, we do have a GPU on Replica. And so here you can see we have a Tesla
[00:13:24.000 --> 00:13:29.840]   K80. At the moment it's using 900 megabytes of memory, but that'll quickly fill up to most of
[00:13:29.840 --> 00:13:36.800]   the GPU once we start training. And so again, we're going to log to a private dashboard.
[00:13:36.800 --> 00:13:43.600]   And so training will shortly kick off. Right now it's running a validation loop there.
[00:13:43.600 --> 00:13:50.080]   While that is running, one more thing to show you in Weights and Mises. And so this is a report from
[00:13:50.080 --> 00:13:55.840]   the Pugging Face team and specifically the diffusers team who are working on a lot of the
[00:13:55.840 --> 00:14:02.720]   amazing stable diffusion examples and pipelines that are going around the internet at the moment.
[00:14:02.720 --> 00:14:10.080]   And so they used this Weights and Mises report to share, first internally, but then among the
[00:14:10.080 --> 00:14:16.480]   wider community, some training tips on the best way to train Dreamboots. And they gave a great
[00:14:16.480 --> 00:14:24.080]   log of all of their experiments, some of the settings they used, the prompt that they gave us,
[00:14:24.080 --> 00:14:28.080]   as well as the output images here. So again, just another really nice example,
[00:14:28.080 --> 00:14:34.800]   whether or not you're doing NLP or computer vision, or whether you're just more focused on the
[00:14:34.800 --> 00:14:40.560]   evaluation side and the prompt generation side of things, Weights and Mises works quite nicely there.
[00:14:40.560 --> 00:14:49.200]   And so here you can see, yeah, we are fine tuning a GPT model in Replica. Above here, you'll see
[00:14:49.200 --> 00:14:56.560]   the, this is the initial outputs. And again, it's not particularly Shakespeare-like, but as the model
[00:14:56.560 --> 00:15:02.880]   gets closer to iteration 1000, we'll have some good Shakespeare being generated on Replica.
[00:15:02.880 --> 00:15:08.960]   So that's about it for Weights and Mises on Replica. I mean, I could go all day about what
[00:15:08.960 --> 00:15:13.040]   else you can do, but I know I think everyone wants to get to the meat of this event with
[00:15:13.040 --> 00:15:16.800]   Amjad and Lucas. So I'll hand it back to Ornella.
[00:15:18.880 --> 00:15:24.800]   Awesome. And thank you so much, Morgan. That's awesome. And we added all of the links if you
[00:15:24.800 --> 00:15:33.040]   want to check them out after the show. They're in the chat. Thanks so much. So now we'll bring up
[00:15:33.040 --> 00:15:44.160]   Amjad and Lucas for the fireside chat. Thanks so much for being here again. We're going to
[00:15:44.160 --> 00:15:49.760]   go through some of the hottest questions in machine learning with both of you and really
[00:15:49.760 --> 00:15:57.040]   curious to hear your perspectives. So we've got some questions already submitted, and then we'll
[00:15:57.040 --> 00:16:01.920]   also have some time to answer questions from the audience. So if you have any, you know, throughout
[00:16:01.920 --> 00:16:09.760]   this conversation, please put them in the chat and we'll incorporate them into the chat. So we'd love
[00:16:09.760 --> 00:16:17.120]   to get started with the first question, which is what is the competitive moat for LLM companies like
[00:16:17.120 --> 00:16:24.480]   OpenAI, Anthropic, etc. Assuming LLM capabilities converge as all of the training data is public,
[00:16:24.480 --> 00:16:31.280]   won't it just be a race to the bottom in terms of pricing? Lucas, do you want to start us off with
[00:16:31.280 --> 00:16:35.760]   that one? Well, I'm curious what Amjad thinks here. I mean, I don't, I think that question
[00:16:35.760 --> 00:16:42.080]   kind of trivializes the effort that goes into saying these models. I mean, I don't think it's
[00:16:42.080 --> 00:16:48.240]   like an easy undertaking. And I think there's a lot of choices that go into that. And so I think,
[00:16:48.240 --> 00:16:55.280]   you know, I mean, just like our companies, you know, GPT certainly has quite a massive
[00:16:55.280 --> 00:17:01.920]   awareness that I think would not be trivial for someone to emulate, but they're going to have to
[00:17:01.920 --> 00:17:07.840]   keep, you know, making their product better than the competition, just like we do. And from my
[00:17:07.840 --> 00:17:15.040]   perspective, I mean, I think moats are kind of generally overrated. And I don't know, I've
[00:17:15.040 --> 00:17:19.440]   watched so many products that you think, you know, might not have a moat be sticky and successful.
[00:17:19.440 --> 00:17:30.320]   Yeah, I basically believe the same thing. I think that we're still very early. It's actually hard
[00:17:30.320 --> 00:17:35.840]   to tell how this space will evolve. I think some of these companies will end up with different
[00:17:35.840 --> 00:17:42.560]   trade-offs. You could already see that like Anthropic is maybe focused a little more on
[00:17:42.560 --> 00:17:51.440]   safety and personability, whereas OpenAI is focused more on creativity, maybe accuracy.
[00:17:51.440 --> 00:17:58.720]   There's a lot of different ways to kind of slice things. And I think different companies will end
[00:17:58.720 --> 00:18:09.440]   up with their own sort of niche. And the way to train these models, like I think now, like,
[00:18:09.440 --> 00:18:14.480]   you know, the pre-training part, maybe it's straightforward. There's a lot of data engineering
[00:18:14.480 --> 00:18:21.600]   in it, but there's now the reinforcement learning part, which is pretty novel and everyone's trying
[00:18:21.600 --> 00:18:29.440]   to figure it out. So I think overall, it's still pretty early to tell how things will shake out.
[00:18:29.440 --> 00:18:35.760]   As an open source guy, I hope that there's some version of this that's open source.
[00:18:35.760 --> 00:18:41.680]   And I think there will be, I mean, there's already a lot of activity in open source,
[00:18:41.680 --> 00:18:45.280]   but at the same time, I think these companies are going to be very profitable and big.
[00:18:48.880 --> 00:18:57.920]   Awesome. Let's move on to something more forward thinking in terms of the use cases.
[00:18:57.920 --> 00:19:01.760]   I'm wondering what you both think is the most exciting or impactful
[00:19:01.760 --> 00:19:06.480]   proposed use case of LLMs you've heard that hasn't been executed on yet.
[00:19:06.480 --> 00:19:15.680]   I guess from my perspective, you know, there's this temptation to define LLMs as just this
[00:19:15.680 --> 00:19:24.480]   magical technology that can do almost anything or operate at sort of a spectacular level of
[00:19:24.480 --> 00:19:30.000]   conversation. And so I think if that's really what you imagine LLMs could do,
[00:19:30.000 --> 00:19:36.160]   it's just incredible the number of use cases. I think one that I'm excited about
[00:19:36.160 --> 00:19:43.840]   myself is teaching humans. I mean, I feel like I consume a lot of educational YouTube
[00:19:44.800 --> 00:19:52.000]   content and I wonder, it does seem like you could get a lot of feedback on the quality of teaching.
[00:19:52.000 --> 00:19:58.320]   And even now, I think like for certain topics, I think Chachibitty is incredibly good at explaining
[00:19:58.320 --> 00:20:04.880]   them to me, but then there's a lot of kind of deeper topics that I'd love to learn more about
[00:20:04.880 --> 00:20:10.800]   and can't really find a great teacher. So I wonder if there's a way to make them teach humans better.
[00:20:10.800 --> 00:20:14.640]   That's just something I would enjoy as a consumer. It seems like it would be a great business if you
[00:20:14.640 --> 00:20:22.240]   could make it work. >> The most exciting thing that I feel like the least progress has been made
[00:20:22.240 --> 00:20:31.360]   on is what some people sometimes called action transformers or just like transformers or language
[00:20:31.360 --> 00:20:37.280]   models that can do things in the world. It doesn't mean that the language model needs to be trained
[00:20:37.280 --> 00:20:44.160]   to use these tools, although that would be useful, but sometimes just basic prompt engineering allow
[00:20:44.160 --> 00:20:51.520]   you to hook a browser, an interpreter, a calculator, a whatever into a language model.
[00:20:51.520 --> 00:20:58.800]   And yeah, at this point I would have expected even Chachibitty to have done some of that. Like
[00:20:58.800 --> 00:21:06.640]   you can easily make it do better math by adding like a Python interpreter into the prompt. So I'd
[00:21:06.640 --> 00:21:14.240]   be curious, maybe there are some failure modes that I'm not thinking about, but I think tool
[00:21:14.240 --> 00:21:23.040]   usage, if you think about humans, what made humans special certainly is language and communication,
[00:21:23.040 --> 00:21:30.480]   but even more than that is our ability to use tools and ability to use tools like what we're
[00:21:30.480 --> 00:21:36.240]   doing right now in order to communicate as well. And so augmenting language models with tools
[00:21:36.240 --> 00:21:42.080]   seems like an important step, but I haven't really seen a lot of progress on it.
[00:21:42.080 --> 00:21:46.800]   Did you see that demo video from Adept? It was so compelling. I immediately signed up for the
[00:21:46.800 --> 00:21:52.800]   beta, but if anyone from Adept is watching, let me end it off. It was so compelling, but you always
[00:21:52.800 --> 00:21:58.720]   worry with AI applications that how well does it really work in real world situations, but it was
[00:21:58.720 --> 00:22:04.400]   awesome demo. Yeah. One thing that's clear is that LLMs are amazing at demos.
[00:22:04.480 --> 00:22:11.440]   Like so a lot of the things that we ended up shipping at Repl.it, we actually demoed in 2020.
[00:22:11.440 --> 00:22:17.040]   And so it takes a long time for the technology to mature and make sure it's like production ready.
[00:22:17.040 --> 00:22:26.720]   That's really funny. So to talk about a specific technology,
[00:22:26.720 --> 00:22:32.320]   what is your favorite stable diffusion use case or example that you've seen?
[00:22:32.560 --> 00:22:40.880]   There was a Repl.it bounty. Repl.it bounties is a place to get software made if you don't know how
[00:22:40.880 --> 00:22:46.240]   to code yourself or don't have the time to, you can just like post a bounty to the community.
[00:22:46.240 --> 00:22:55.440]   And there was this bounty that asked to hook up a stable diffusion to a t-shirt. So you would go to
[00:22:55.440 --> 00:23:00.080]   a website, you would like put in some prompt, it will generate a picture. You would say, print it
[00:23:00.080 --> 00:23:06.160]   into a t-shirt and like the next day you'd get a t-shirt. And so that's like fascinating because
[00:23:06.160 --> 00:23:12.400]   it's like such a cyberpunk idea. Like the idea that I can make clothes with like language is
[00:23:12.400 --> 00:23:18.480]   pretty amazing. It's funny, I have like a personal list. This is not a good business, but this is a
[00:23:18.480 --> 00:23:23.280]   use case I've been enjoying. I have a three-year-old daughter and so she can't read. So I'm like,
[00:23:23.280 --> 00:23:27.920]   "Okay, I'm going to make a t-shirt." And so she can't read or write, but she really liked to make
[00:23:27.920 --> 00:23:33.200]   twans and go back to them. So we'll make like a plan for bedtime or like a plan for the day.
[00:23:33.200 --> 00:23:38.080]   And because she can't read or write, she can't necessarily remember them, but she wants a record.
[00:23:38.080 --> 00:23:44.320]   So we'll take all the things that we're going to do and we'll make little pictures of it and then
[00:23:44.320 --> 00:23:51.040]   print them out in a list and then we can refer back to them. And so Stable Diffusion is amazing
[00:23:51.040 --> 00:23:55.200]   for that because I can like draw all the crazy things that she wants to do.
[00:23:55.200 --> 00:24:03.040]   Wow, that's actually a profound idea. Like people who can't read or write are able to
[00:24:03.040 --> 00:24:08.000]   recall things or memorize things. That's very interesting.
[00:24:08.000 --> 00:24:13.360]   Yeah, it's funny because she's so unimpressed by the technology.
[00:24:19.280 --> 00:24:27.200]   That's so funny. We made one of my best friends a birthday card, which was actually a website on
[00:24:27.200 --> 00:24:35.520]   Repl.it. She's a poet and we fed some of her poetry and had it create images. And then we
[00:24:35.520 --> 00:24:40.320]   made this like collage of them on a website. Awesome. Share the link. I want to see this.
[00:24:40.320 --> 00:24:47.920]   That's so cool. We'll do. Awesome. Yeah, there's some really cool use cases. I'm excited to see
[00:24:47.920 --> 00:24:57.520]   what else gets spun up. Do you all see the hype around chat GPT, LLMs and text to image models
[00:24:57.520 --> 00:25:02.480]   diverting funding or resources from other potentially more impactful ML modeling,
[00:25:02.480 --> 00:25:08.320]   which is regular scale deep learning, traditional ML or time series forecasting?
[00:25:13.120 --> 00:25:17.840]   What do you think? I guess I don't. I mean, I feel like hype makes me nervous,
[00:25:17.840 --> 00:25:28.400]   but I don't think there's like a single pool of funding. I mean, it might be moving people's
[00:25:28.400 --> 00:25:32.720]   attention. Like, I don't know, like maybe grad students might be moving towards it,
[00:25:32.720 --> 00:25:35.920]   but I can't fault them. I mean, it's like new and exciting. It does seem like it impacts
[00:25:37.280 --> 00:25:42.800]   so many things I can understand. I do think, I mean, from my perspective, I do think there's a
[00:25:42.800 --> 00:25:51.920]   little bit of an influx of resumes that we get from people who used to work in crypto that makes
[00:25:51.920 --> 00:25:54.960]   me like a little bit nervous just because I feel like that whole community seems a little bit
[00:25:54.960 --> 00:25:59.600]   bonkers to me. I'm not like against crypto. I don't really have like a strong opinion, but I
[00:26:01.360 --> 00:26:09.040]   just, I would prefer to work in a hypeless world, but I guess that's not actually possible, sadly.
[00:26:09.040 --> 00:26:22.080]   I think hype serves a certain function. It's a way to accumulate like interest and capital and
[00:26:22.080 --> 00:26:31.360]   talent in one place. Sometimes that's like too much, like I think in the crypto 2021 hype cycle,
[00:26:31.360 --> 00:26:39.600]   that was like, you know, objectively like bad, just given that how many scams and things like
[00:26:39.600 --> 00:26:46.960]   that had got up and running. But you know, like the dot-com bubble, it was like warranted, you
[00:26:46.960 --> 00:26:54.160]   know, it was like there's this thing called the internet. Everyone was thinking rightly so that
[00:26:54.160 --> 00:26:59.120]   is the future. And by the way, a lot of the businesses that failed that were a joke, you
[00:26:59.120 --> 00:27:03.440]   know, eventually became reality like Webvan, Instacart, you know, things like that. And so
[00:27:03.440 --> 00:27:12.960]   I think just like it's like it's a part of human nature and just need to accept it and deal with
[00:27:12.960 --> 00:27:22.320]   it, potentially wield it to your benefit. But you know, ultimately, like I don't think of these
[00:27:22.320 --> 00:27:31.120]   things as zero sum. Like there's actually like, there's actually more capital in the world,
[00:27:31.120 --> 00:27:35.680]   or at least more capital in Silicon Valley than there are productive uses for that capital.
[00:27:37.760 --> 00:27:48.480]   And so hype maybe creates a way to be able to accumulate into one space, all that capital to
[00:27:48.480 --> 00:27:56.880]   put it to productive use or unproductive use. But it is what it is. And I think any field
[00:27:56.880 --> 00:28:06.320]   that feels that they're not getting enough capital should maybe think about their marketing
[00:28:06.320 --> 00:28:08.640]   and maybe build something like ChatGPD that goes viral.
[00:28:08.640 --> 00:28:15.680]   Yeah, it's funny. I always tell, you know, like younger researchers, like, you know,
[00:28:15.680 --> 00:28:21.760]   what's worse than hype is like no hype, right? Like, I mean, you know, I remember when I graduated
[00:28:21.760 --> 00:28:27.680]   and was looking for a job in AI, like, you know, basically like all you could do is kind of rank
[00:28:27.680 --> 00:28:32.480]   ads or, you know, go to Wall Street and try to pick stocks. And like the applications now are
[00:28:32.480 --> 00:28:36.720]   so much cooler. And I think the hype, you know, to some extent it's really justified. I mean,
[00:28:36.720 --> 00:28:44.000]   these products are like amazing. And like, I think the use cases are still untapped and it's important
[00:28:44.000 --> 00:28:50.880]   to sort of like separate from the hype and enjoy the breakthroughs that we're seeing right now.
[00:28:50.880 --> 00:28:54.400]   I mean, I just, I can't even believe, like that question about like, what's the most interesting
[00:28:54.400 --> 00:28:59.200]   use case of LLMs? It's like, where do I even begin? Like, it must be such a fun time to,
[00:28:59.200 --> 00:29:04.560]   to, you know, to be getting into AI with so many applications opening up.
[00:29:04.560 --> 00:29:12.640]   Yeah. And for what it's worth, I think it's sort of like more ML begets more ML. Like,
[00:29:12.640 --> 00:29:17.040]   you know, we, since we started working Ghostwriter, we started doing, you know,
[00:29:17.040 --> 00:29:22.800]   other like non-transformer ML to help us with building Ghostwriter itself. Yeah. It just came
[00:29:22.800 --> 00:29:27.680]   out recently, someone reversed a generic quote by, they found like a small linear regression model
[00:29:27.680 --> 00:29:35.680]   on the front end. And so I think these tools will just like compose in a really powerful way. So I
[00:29:35.680 --> 00:29:44.320]   think it just brings more attention to the field in general. Yeah. I think something that's so
[00:29:44.320 --> 00:29:51.040]   interesting about this boom that we're having right now too, is that open source is keeping
[00:29:51.040 --> 00:29:57.440]   pace with the large research universities and Googles of the world. So I'm wondering why you
[00:29:57.440 --> 00:30:04.880]   think that is? I, well, I think I know. I mean, I think that there's ML, unlike other software
[00:30:04.880 --> 00:30:11.600]   development, has this incredibly strong academic lineage where like open source is the default.
[00:30:11.600 --> 00:30:16.000]   And so I just think because like, you know, so much of the ML is coming directly from research
[00:30:16.000 --> 00:30:20.800]   and everybody in research is opening up their stuff. And, you know, until recently, like research
[00:30:20.800 --> 00:30:25.360]   had no funding to ever buy software for anything. Right. So, you know, research is always using all
[00:30:25.360 --> 00:30:29.920]   open source. And I think it's a fantastic thing. Like I think it makes everything,
[00:30:29.920 --> 00:30:35.120]   you know, available to, I mean, obviously the compute is expensive, but I think it is,
[00:30:35.120 --> 00:30:39.440]   you know, we shouldn't take for granted the fact that, you know, all the best in class
[00:30:39.440 --> 00:30:47.600]   tools are available to everyone in the world. Yeah. Yeah. I agree with that. I also think that
[00:30:47.600 --> 00:30:54.080]   this generation of models lend itself really nicely to open source.
[00:30:54.080 --> 00:31:02.080]   You know, they're, they're essentially stateless programs, right? So you give them input,
[00:31:02.080 --> 00:31:05.600]   you get an output. They're very portable. They can run on any machine.
[00:31:06.560 --> 00:31:14.160]   Especially diffusion models are very small. And so that really lends itself nicely to open source.
[00:31:14.160 --> 00:31:20.560]   I think it's been hard to replicate the really large models. I mean, some of them are doing
[00:31:20.560 --> 00:31:26.320]   well in benchmarks, but then when you try to use them, they're actually not very good. And I think
[00:31:26.320 --> 00:31:32.640]   there's a lot of things you're talking about. You want to name, name names? All of them. Like,
[00:31:33.280 --> 00:31:40.160]   yeah, like GLM, Bloom, all those, like, I mean, I respect everything that they've done, but they're
[00:31:40.160 --> 00:31:49.520]   really not that great. And then maybe they're still iterating on them. But like, I think maybe
[00:31:49.520 --> 00:31:53.520]   here is like, you know, what we talked about earlier, which is like a data advantage starts
[00:31:53.520 --> 00:31:59.280]   to show at the very large kind of number of parameters. That being said, like, I'm pretty
[00:31:59.280 --> 00:32:05.200]   sure they'll catch up pretty soon. Yeah. I also think there's a lot to make a model. I mean,
[00:32:05.200 --> 00:32:09.840]   you know, but like, like between like a model that like runs well on a benchmark versus does
[00:32:09.840 --> 00:32:14.400]   something actually useful for real human. That's not like a trivial undertaking.
[00:32:14.400 --> 00:32:19.440]   Yeah. And in some ways like benchmarks can actually be hurtful.
[00:32:19.440 --> 00:32:24.880]   Well, you know, it's funny. I agree, but you know, I remember before there were benchmarks,
[00:32:24.880 --> 00:32:28.640]   like, like nothing worse than benchmark focuses, no benchmark focuses.
[00:32:28.640 --> 00:32:32.160]   Just pure fluff marketing. Well, yeah. I mean, it used to be an
[00:32:32.160 --> 00:32:34.800]   environment where a lot of times people didn't even feel like they needed to,
[00:32:34.800 --> 00:32:39.200]   like, you know, in the 2000s, like it was like, you know, people wouldn't even,
[00:32:39.200 --> 00:32:44.000]   people would sometimes be, have ML that was worse than a baseline of just guessing
[00:32:44.000 --> 00:32:47.600]   the most common possible answer. And you could like publish a paper like that. It's absolutely
[00:32:47.600 --> 00:32:53.040]   ridiculous. Oh wow. And that's definitely awful. But yeah, I think maybe we've over-rotated
[00:32:54.080 --> 00:32:56.000]   to benchmark obsession. Yeah.
[00:32:56.000 --> 00:33:04.320]   So speaking of open source, I was wondering if Amjad could walk us through
[00:33:04.320 --> 00:33:09.440]   the approach to leveraging open source to build tools like Ghostwriter.
[00:33:09.440 --> 00:33:20.560]   Yeah, for sure. Ghostwriter, you know, when we started working on it, it wasn't really clear
[00:33:20.560 --> 00:33:26.800]   that there was going to be this like big open source movement that's going to help companies,
[00:33:26.800 --> 00:33:36.800]   startups like ours build, you know, co-pilot like experiences. And one of the main reasons we wanted
[00:33:36.800 --> 00:33:43.120]   to self-host it and not just like rely on an API is because we really want to control the latency
[00:33:43.120 --> 00:33:47.760]   and really want to control the quality. Want to be able to improve it over time,
[00:33:47.760 --> 00:33:55.280]   won't be able to collect data and feedback and iterate on it. And we felt like for some use cases,
[00:33:55.280 --> 00:34:00.400]   very large models or chat type use cases, it's fine to use sort of an API.
[00:34:00.400 --> 00:34:08.080]   But for something like this close to the user, where every keystroke we are presenting you with
[00:34:08.080 --> 00:34:15.120]   information, it was very important for us to own the experience end to end. So I think at first,
[00:34:15.120 --> 00:34:20.160]   it was just like a bunch of let's try everything out there. So we literally tried every model out
[00:34:20.160 --> 00:34:30.160]   there. And then we found Cogen from Salesforce. It was like, you know, when you're evaluating
[00:34:30.160 --> 00:34:35.920]   some technologies, they're never perfect for your use case, but you see a glimpse of like,
[00:34:35.920 --> 00:34:40.560]   okay, I can see this working. And then it was like two weeks of like manic work,
[00:34:41.680 --> 00:34:50.800]   try to make it faster. So we had like a big breakthrough, like an order to magnitude
[00:34:50.800 --> 00:34:55.760]   improvement in speed. And by the way, I think that got upstreamed as well. So now Cogen is
[00:34:55.760 --> 00:35:03.040]   pretty fast for everyone. And then, what's that? What was the break? I think it was a faster
[00:35:03.040 --> 00:35:09.120]   transformer and another optimization. I forgot which one, but both of them I think are now
[00:35:09.120 --> 00:35:15.680]   upstream. So the standard thing is pretty fast. I think when we first found it, it was like
[00:35:15.680 --> 00:35:31.360]   30 seconds per request. That always was like, yeah. And then there was a few other optimizations,
[00:35:31.360 --> 00:35:41.440]   for example, like streaming and caching and other ways kind of to run it more like efficiently.
[00:35:41.440 --> 00:35:48.400]   And it started feeling like something we could use. Eventually, we also did some fine tuning
[00:35:48.400 --> 00:35:53.360]   and figure out how to improve the model on the fly based on user feedback. Now we're training
[00:35:53.360 --> 00:35:57.040]   our model from scratch, but that's after we shipped, right? We shipped the product, people
[00:35:57.040 --> 00:36:03.840]   love it. And we're collecting a lot more data. We know what's missing. We know what features we
[00:36:03.840 --> 00:36:10.560]   want. And so we actually have our first model that we trained from scratch. We're using it
[00:36:10.560 --> 00:36:16.080]   internally. It needs a little bit more work, but we'll likely have something to announce over the
[00:36:16.080 --> 00:36:26.640]   next month or two. Really excited for that. What do you think have been the challenges of
[00:36:27.040 --> 00:36:32.960]   fine tuning and deploying LLMs for low latency while also keeping them performant?
[00:36:32.960 --> 00:36:38.080]   I mean, there's always a straight off between parameter size and
[00:36:38.080 --> 00:36:45.280]   latency. And it feels like pretty linear. I don't know if Lucas agrees with that, but
[00:36:45.280 --> 00:36:54.880]   just the bigger, the smarter. It's almost like IQ. That's pretty straightforward.
[00:36:55.440 --> 00:37:00.960]   So the one we deployed today is 2.7 billion parameters. And surprisingly,
[00:37:00.960 --> 00:37:06.080]   it's really the sweet spot where less than 2B is actually pretty dumb.
[00:37:06.080 --> 00:37:16.080]   And more than 3 billion, it becomes kind of slow, visibly slow. So we found the Goldilocks
[00:37:16.080 --> 00:37:25.360]   of language models, 2.7 billion parameters. In terms of fine tuning, it doesn't really have any
[00:37:25.360 --> 00:37:32.080]   bearing on latency. The tricky thing there is, again, what we talked about with the benchmarking
[00:37:32.080 --> 00:37:37.760]   is that you could do well on benchmark, then go try it in the real world. It's actually
[00:37:37.760 --> 00:37:45.120]   made the model performance worse. We had one case, for example, where it was doing better on
[00:37:45.120 --> 00:37:51.600]   Python after some fine tuning, but then it forgot how to write JSX, which is JavaScript's React
[00:37:51.600 --> 00:37:56.800]   syntax. And so we call testing by vibes. So in addition to the benchmark,
[00:37:56.800 --> 00:38:12.480]   that's one stage, that's not the whole thing. We kind of check the vibes of the model.
[00:38:13.680 --> 00:38:23.760]   And then if it passes the vibe check, it goes into an A/B test. So in A/B test, we check the
[00:38:23.760 --> 00:38:28.480]   acceptance rate. So typically our acceptance rate, I think, hovers around 25%
[00:38:28.480 --> 00:38:35.920]   of all suggestions. And if it enters outputs, then we're doing something well. If it's neutral,
[00:38:35.920 --> 00:38:42.320]   then maybe whatever we did wasn't useful. If it goes down, that's definitely bad.
[00:38:42.960 --> 00:38:48.400]   And so that's sort of the last test. We'd love to get more objective about it, but
[00:38:48.400 --> 00:38:53.440]   we haven't found a way, other than just building up more and more benchmark over time.
[00:38:53.440 --> 00:38:58.960]   The vibe-driven development.
[00:38:58.960 --> 00:39:10.240]   This is a question for the both of you. How do you maintain a rapid pace of shipping ML-powered
[00:39:10.240 --> 00:39:16.320]   products or MVPs with the field moving so fast? And how do you keep up with customer needs and
[00:39:16.320 --> 00:39:25.440]   all of that? I admire Repa. I'm curious for your answer to this one. I don't think you have a
[00:39:25.440 --> 00:39:36.160]   silver bullet. A lot of people, when they ask this question, they expect some kind of process.
[00:39:36.960 --> 00:39:44.640]   It's probably the other way around. It's probably less process to ship faster. So just hire great
[00:39:44.640 --> 00:39:52.880]   people and trust them to do great work and be okay with some mistakes. Because if you're going to
[00:39:52.880 --> 00:39:57.280]   have a higher throughput, you're also going to have higher rate of mistakes. So being able to
[00:39:57.280 --> 00:40:09.440]   correct for mistakes is going to be super important. I think it really matters that what
[00:40:09.440 --> 00:40:15.520]   you're building is fun. Because I think if it's fun and exciting, people want to do more things
[00:40:15.520 --> 00:40:25.520]   and release more things. And finally, we just encourage it via memes and traditions. We call
[00:40:25.520 --> 00:40:33.120]   it shipping season. It's one season where we start shipping a lot of stuff. We have t-shirts and all
[00:40:33.120 --> 00:40:38.400]   of that stuff. But at the same time, you want to balance it with some important behind-the-scenes
[00:40:38.400 --> 00:40:44.640]   work, like performance work. And sometimes we try to dress up performance work and
[00:40:44.640 --> 00:40:49.360]   sort of add glamorous work with some glamour in order to feel the same way about it.
[00:40:50.960 --> 00:40:55.280]   But if you're able to do both, I think you'll be unstoppable.
[00:40:55.280 --> 00:41:04.800]   Yeah, I think from our perspective at Wits & Bias, I think some of the things that I've learned
[00:41:04.800 --> 00:41:11.680]   is keeping a direct connection between the people building the product, the people using
[00:41:11.680 --> 00:41:18.080]   the product. Making sure there's tons of touch points. We invite all of our customers into our
[00:41:18.080 --> 00:41:25.760]   Slack channel just so it's really easy to have that conversation. We try to really encourage
[00:41:25.760 --> 00:41:32.160]   the engineers to engage with our customers. Because I think we don't have quite the gigantic
[00:41:32.160 --> 00:41:36.400]   community of Replica, but we do have tons and tons of users that we all admire and love and
[00:41:36.400 --> 00:41:43.200]   want to make happy. And so I think there's a temptation to put process and product people
[00:41:43.200 --> 00:41:47.120]   in between the users and the engineers to avoid interruption. And there's a lot of bits of that,
[00:41:47.120 --> 00:41:52.240]   but I think there's nothing like an engineer that ships a feature actually going out and telling the
[00:41:52.240 --> 00:41:59.200]   world, "Hey, I shipped this." And so we really try to keep that kind of connection with everyone in
[00:41:59.200 --> 00:42:02.960]   the company and our customers. And then I think there's another thing that I've really learned,
[00:42:02.960 --> 00:42:12.720]   which is fighting technical debt is so important for maintaining good velocity. So it feels slower
[00:42:13.600 --> 00:42:19.840]   at the time to keep upgrading your stack and keep changing the internal developer tools. But I think
[00:42:19.840 --> 00:42:24.160]   it's just so important to making sure that you're not slowing down over time.
[00:42:24.160 --> 00:42:35.360]   Awesome. So something that we shipped recently at Repl.it is bounties. And
[00:42:35.360 --> 00:42:42.400]   people are curious about how bounties fits in the overall vision for Repl.it.
[00:42:43.120 --> 00:42:49.280]   And when you think about all the advances that are coming out for developing software,
[00:42:49.280 --> 00:42:55.840]   are we going to get to a point where using GPT-4 to code apps with a prompt replaces bounties?
[00:42:55.840 --> 00:43:03.120]   I think bounties will be sufficiently AI-powered in the short term,
[00:43:03.120 --> 00:43:09.600]   like even probably before GPT-4 comes out. But our view at Repl.it from the start,
[00:43:09.600 --> 00:43:18.240]   I actually wrote this blog post in 2017 about how our view of AI computing in general is it
[00:43:18.240 --> 00:43:25.680]   augments people, does not really replace them. I don't really believe in this view that AI will
[00:43:25.680 --> 00:43:33.760]   replace people. I think there's some low-level work that will probably get replaced. And that's
[00:43:33.760 --> 00:43:44.800]   generally been not good for humanity, even if it comes at some sort of problems in the short term.
[00:43:44.800 --> 00:43:51.120]   And you'd want to have a way of dealing with these problems. Those are difficult discussions. But
[00:43:51.120 --> 00:43:58.160]   nonetheless, I think long-term technology just makes humans more productive, gives them more
[00:43:58.160 --> 00:44:06.480]   control over their time, and allows us to do more creative work. I think this generation of AI
[00:44:06.480 --> 00:44:14.000]   is no different. It is ultimately a tool. And I think part of its marketing has been that it is
[00:44:14.000 --> 00:44:21.600]   like an AGI or something like that. I don't think it's anywhere near there. I'm not sure
[00:44:21.600 --> 00:44:27.600]   what it takes to get there, but I still view it as a tool. LLMs are ultimately, in my opinion,
[00:44:27.600 --> 00:44:33.360]   completion engines. So you need human input or creativity in order for the LM to do something
[00:44:33.360 --> 00:44:42.960]   interesting. So with bounties, I think we supercharge both the buyer and the provider
[00:44:42.960 --> 00:44:50.080]   with AI. So we want to help the back buyer specify their problems using AI. I don't want to help the
[00:44:51.760 --> 00:45:00.000]   programmer maybe give them sort of a leg up or give them part of the solution using AI.
[00:45:00.000 --> 00:45:11.200]   Sorry about that. My internet dropped. Not good AI happening at Xfinity today.
[00:45:16.240 --> 00:45:26.080]   There's a question in the audience about asking if jobs will be filled with the role of chat
[00:45:26.080 --> 00:45:33.360]   GPT expert or AI user expert who can utilize AI tools efficiently. I think that's an interesting
[00:45:33.360 --> 00:45:38.800]   thing to think about. What do you all think?
[00:45:43.680 --> 00:45:52.240]   I guess the question is, is there a job to use the AI well? It certainly does seem like that.
[00:45:52.240 --> 00:45:56.640]   I think someone was talking about prompt engineers does sort of seem like that. It's interesting
[00:45:56.640 --> 00:46:07.760]   how asking a question well does seem like a critical task to using the GPT models well.
[00:46:08.320 --> 00:46:16.240]   It's a really interesting scale. It totally seems like the trends are in that direction.
[00:46:16.240 --> 00:46:22.960]   And I've been talking to a lot of our customers that are thinking about this and noticing.
[00:46:22.960 --> 00:46:28.720]   I don't know if you saw, there's a really hilarious result where if you ask the GPT a question,
[00:46:28.720 --> 00:46:34.960]   then you say, "Show your reasoning step by step after the question," it gets a lot more accurate.
[00:46:35.360 --> 00:46:40.240]   It's just absolutely fascinating. Probably good advice to humans as well.
[00:46:40.240 --> 00:46:46.720]   I think that stuff is really, really interesting. I don't think it's... I think it would be...
[00:46:46.720 --> 00:46:52.960]   To pontificate now about where this goes, you'll probably be wrong, but I agree that it does seem
[00:46:52.960 --> 00:46:57.280]   like there's a new kind of job being created that currently is called prompt engineer.
[00:46:59.280 --> 00:47:11.280]   Yeah. I mean, especially when you're trying to do something with LLMs that require context over time,
[00:47:11.280 --> 00:47:20.080]   prompt engineering becomes incredibly important. The context size of these models is fairly small
[00:47:20.080 --> 00:47:34.160]   right now. It's about 2000 tokens, which is maybe like four megabytes, which is similar to programming
[00:47:34.160 --> 00:47:43.120]   in Atari or something. It's fairly early days, requires similar to programming early video games.
[00:47:43.120 --> 00:47:48.640]   You had to do a lot of trickery to figure out how to fit things into memory. And so I think
[00:47:48.640 --> 00:47:57.280]   prompt engineering is pretty low level sort of thing right now. I see a lot of amazing engineering
[00:47:57.280 --> 00:48:04.240]   that goes into that. For example, there's this trick I learned about recently where
[00:48:04.240 --> 00:48:11.840]   let's say you have a large corpus of things that the LLM is supposed to be answering questions
[00:48:11.840 --> 00:48:20.160]   about and you can't put the entire thing into context. So what you do is you take the embeddings,
[00:48:20.160 --> 00:48:26.160]   which is the internal representation of a text of the question, and then you have an index of all
[00:48:26.160 --> 00:48:34.640]   the embeddings of the corpus or the data that you have. And then you do, you match those embeddings
[00:48:34.640 --> 00:48:44.480]   in order to figure out which context it include into the prompt. That's pretty amazing sort of
[00:48:44.480 --> 00:48:54.800]   algorithm. And yeah, I think it's going to be a specialty. Even if they solve the context length
[00:48:54.800 --> 00:49:00.400]   issue, you still got to have to be economical about it because you don't want to be slugging
[00:49:00.400 --> 00:49:07.600]   this data back and forth between the server and the client. And so in addition to sort of having
[00:49:07.600 --> 00:49:13.440]   these prompt tricks that Lucas talked about, there's also just like low level engineering of
[00:49:13.440 --> 00:49:24.160]   how much context to introduce for the LLM to read. >> We all have any thoughts about
[00:49:25.200 --> 00:49:33.200]   increased scrutiny from lawmakers on AI? So the US banned experts to China and the EU is
[00:49:33.200 --> 00:49:39.760]   considering legislation on the use of ML models if they're biased or harmful in some way. So
[00:49:39.760 --> 00:49:42.120]   wondering what your thoughts are on that? >>
[00:49:42.120 --> 00:49:53.920]   That's a complicated question. I think it's probably good that people are thinking about
[00:49:53.920 --> 00:49:59.520]   this. I do think that the impact of these models is quite big. And I think people don't
[00:49:59.520 --> 00:50:07.520]   really appreciate how if you put a vision model in a car that steers it, you've created a lethal
[00:50:07.520 --> 00:50:17.200]   weapon. And I think that this idea that only people in ML are allowed to think about the ethics
[00:50:17.200 --> 00:50:25.360]   in ML seems very bad to me. I think it's good for there to be a full conversation of everybody
[00:50:25.360 --> 00:50:33.440]   affected by ML, which is everybody on the planet. I do worry about the specifics of the legislation
[00:50:33.440 --> 00:50:40.560]   that we see. It's painful for me, I think, as a practitioner in the field, some of the
[00:50:42.160 --> 00:50:48.160]   things that come out of it. And some of the lawsuits to me seem misguided or unfair. But
[00:50:48.160 --> 00:50:53.840]   that's a really broad topic. I guess I would say overall, I think that everyone will be paying
[00:50:53.840 --> 00:50:58.720]   attention to this. I think it's probably good that governments are paying attention to this,
[00:50:58.720 --> 00:51:03.200]   but I'm sure that a lot of what they do is going to be frustrating for technologists like me.
[00:51:06.320 --> 00:51:15.600]   Yeah, I mean, there are more examples of technology of misguided
[00:51:15.600 --> 00:51:23.760]   legislation than something that's helpful. For example, the cookie thing, I don't think anyone
[00:51:23.760 --> 00:51:31.520]   thinks it's really helpful, including consumers. And there are better ways to handle it. I think
[00:51:31.520 --> 00:51:45.280]   the EU just has a history of creating surface level solutions. So, I mean, my hope is maybe
[00:51:45.280 --> 00:51:52.720]   legislators could use AI to regulate AI. Can you have a conversation with Chad Chibita about Chad
[00:51:52.720 --> 00:51:59.920]   Chibita? That could be interesting. Can we raise the level of understanding of these technologies,
[00:52:00.480 --> 00:52:05.440]   politicians and legislators using AI? That's something worth working on. If you're looking
[00:52:05.440 --> 00:52:13.840]   for something to work on, I think that's a pretty great idea. So, speaking of things to work on,
[00:52:13.840 --> 00:52:19.680]   there's a question from the audience. If you all have any products or ideas you're hoping to see
[00:52:19.680 --> 00:52:28.080]   built or explored in our upcoming hackathon? Yeah, I would really love more exploration of
[00:52:28.080 --> 00:52:35.280]   the idea of action models. And again, you can just fine tune a GPT-3 to do that, or you can just
[00:52:35.280 --> 00:52:41.760]   have a very elaborate prompt. But can you have a model that can control like the replit IDE,
[00:52:41.760 --> 00:52:46.960]   or can use Waze and MySys to train its own model, whatever? Like, can it do like interesting things,
[00:52:46.960 --> 00:52:52.400]   right? So, in the world using tools, I think that's like, that would be my prompt. It's like,
[00:52:52.960 --> 00:53:01.200]   make LLMs use tools. I think there's so many ways to approach this. I think it's probably the next
[00:53:01.200 --> 00:53:07.440]   sort of order of magnitude jump in usefulness is probably around this idea of tools.
[00:53:07.440 --> 00:53:12.960]   Yeah, I'll say one thing that I think is really coming that I think I would be exploring if I had
[00:53:12.960 --> 00:53:18.160]   more time is chat interfaces to everything. It's kind of like what you're saying, but instead of
[00:53:18.160 --> 00:53:23.680]   arbitrary tools, I think making chat interfaces to your own tools, I think they've been enraging
[00:53:23.680 --> 00:53:29.360]   for my whole lifetime. And so, you think a chat interface is garbage, but I actually think we
[00:53:29.360 --> 00:53:34.640]   might get to the point where I wonder if the best way to use any tool in the next few years could
[00:53:34.640 --> 00:53:39.280]   just be like just telling it in natural language what to do. I even wonder with weights and biases,
[00:53:39.280 --> 00:53:45.120]   it's like make me a report that tells me about this. It seems like we're maybe at the point
[00:53:45.120 --> 00:53:52.160]   where that could be actually valuable. We'll talk a little bit more about the
[00:53:52.160 --> 00:53:57.680]   hackathon in just a minute here, but wanted to close out by asking what products coming out of
[00:53:57.680 --> 00:54:05.600]   Repl.it and weights and biases in 2023 and what can you share about them and what are you excited
[00:54:05.600 --> 00:54:14.480]   about? Speaking of chat interfaces, we are releasing Ghostwriter chat and it's basically
[00:54:14.480 --> 00:54:21.440]   what Luke is just said, which is you can control the entire development experience at the end by
[00:54:21.440 --> 00:54:27.040]   just talking to the model. I would say that's pretty great. I'm so excited to see that.
[00:54:28.000 --> 00:54:35.440]   When is it coming out? I want to figure out. Probably early February.
[00:54:35.440 --> 00:54:45.840]   Hello?
[00:54:50.240 --> 00:54:54.080]   I think we lost Lucas to the internet AI gods.
[00:54:54.080 --> 00:55:00.720]   There he is. Sorry, they blew me off the Wi-Fi.
[00:55:00.720 --> 00:55:14.080]   It would have been a Repl.it livestream without something. Lucas, did you have an answer that
[00:55:14.080 --> 00:55:20.800]   you wanted to share? Yeah, we have a lot of stuff coming. I think a lot of it is around
[00:55:20.800 --> 00:55:30.960]   making stuff reliable in production. So, ways to monitor if your model, once it's being used,
[00:55:30.960 --> 00:55:36.640]   is working well. We think that's a really underserved use case we're excited about helping
[00:55:36.640 --> 00:55:46.480]   with. That sends you the ball. Yeah, I hope so. Thank you both so much for your time. This was
[00:55:46.480 --> 00:55:55.840]   extremely exciting to hear about. We're gonna go ahead and share how folks can apply what they've
[00:55:55.840 --> 00:56:01.200]   learned or tidbits they've gotten from this conversation in our upcoming Repl.it and
[00:56:01.200 --> 00:56:06.720]   Waits and Buyers Hackathon. Thanks so much for joining today. Thank you. Appreciate it.
[00:56:06.720 --> 00:56:19.600]   Cool. So, I wanted to share just quickly, I think some folks may have already seen it, but
[00:56:19.600 --> 00:56:27.440]   we are having a joint hackathon coming up starting on February 4th. And I wanted to
[00:56:27.440 --> 00:56:34.400]   share the information for how to apply for the hackathon. So, I'll paste the link in the chat
[00:56:34.400 --> 00:56:43.120]   to the application website. But basically, you'll register on this website and then you'll receive
[00:56:43.120 --> 00:56:47.600]   an email with all of the details. Something really, really awesome and special about this
[00:56:47.600 --> 00:56:53.840]   hackathon is that Waits and Buyers will be paying for GPUs for the participants. So,
[00:56:53.840 --> 00:57:00.400]   you'll have access to amazing power for your machine learning models on Repl.it.
[00:57:00.400 --> 00:57:06.480]   And I just wanted to go over the prizes really quickly. So, we've got a grand prize of 300,000
[00:57:06.480 --> 00:57:16.320]   cycles, which is incredible. And this is going to go to the best project of all of the projects from
[00:57:16.320 --> 00:57:22.880]   the hackathon. And then we've got specific awards for both best Waits and Buyers report as well as
[00:57:22.880 --> 00:57:28.320]   best Repl.it project. So, if you're not familiar with Waits and Buyers' reports, it's this really
[00:57:28.320 --> 00:57:34.720]   cool way of documenting a model and all that you've done with it. We'll share some resources
[00:57:34.720 --> 00:57:40.880]   about that as well. And then we've also got one for best Repl.it that's shared to the community.
[00:57:40.880 --> 00:57:46.480]   And then the last prize will be honorable mention. So, we're really excited for everyone
[00:57:46.480 --> 00:57:50.880]   to participate in this hackathon and looking forward to seeing you all on the wait list.
[00:57:51.760 --> 00:58:05.600]   Thanks so much for joining and we'll see you February 4th at our hackathon kickoff.

