
[00:00:00.000 --> 00:00:04.680]   [MUSIC PLAYING]
[00:00:04.680 --> 00:00:07.200]   Welcome to the community panel at the Fully Connected
[00:00:07.200 --> 00:00:09.840]   Conference presented by Weights & Biases.
[00:00:09.840 --> 00:00:13.360]   My name is Corey Straussman, and I'm a community manager.
[00:00:13.360 --> 00:00:16.280]   I'm excited to spotlight two members of our community,
[00:00:16.280 --> 00:00:19.960]   Andrada and Atharva, who will be presenting on today's topic,
[00:00:19.960 --> 00:00:21.880]   Built with WMB.
[00:00:21.880 --> 00:00:24.200]   As we continue to grow, you will see a lot
[00:00:24.200 --> 00:00:28.360]   of focus on the most successful part of WMB, you.
[00:00:28.360 --> 00:00:32.080]   We'd love to capture what you have utilized WMB to build
[00:00:32.080 --> 00:00:34.960]   and provide a platform for your ML practice.
[00:00:34.960 --> 00:00:37.520]   Today, I'm excited to introduce our panelists, who
[00:00:37.520 --> 00:00:40.800]   have accomplished so much with Weights & Biases.
[00:00:40.800 --> 00:00:44.400]   First, Andrada is a data scientist at Endeva,
[00:00:44.400 --> 00:00:48.520]   a WMB dev expert, Kaggle Notebooks Grandmaster,
[00:00:48.520 --> 00:00:52.240]   and ZBHP Global Data Science Ambassador.
[00:00:52.240 --> 00:00:53.600]   Andrada, welcome.
[00:00:53.600 --> 00:00:55.840]   Thank you, Corey, so much for the introduction.
[00:00:55.840 --> 00:00:59.400]   I'm super happy and grateful to be with you today.
[00:00:59.400 --> 00:01:03.240]   So before getting started, a little bit of a back story.
[00:01:03.240 --> 00:01:05.680]   So when I started doing data science,
[00:01:05.680 --> 00:01:08.400]   and I was starting to do experiments,
[00:01:08.400 --> 00:01:12.040]   I didn't even know at that time these are called experiments.
[00:01:12.040 --> 00:01:14.760]   And I remember I was doing, I think,
[00:01:14.760 --> 00:01:18.040]   the Melanoma competition on Kaggle a couple of years ago.
[00:01:18.040 --> 00:01:22.840]   And I was experimenting with efficient nets,
[00:01:22.840 --> 00:01:26.720]   and epoch sizes, and batch sizes.
[00:01:26.720 --> 00:01:28.680]   And it was super, super, super fun.
[00:01:28.680 --> 00:01:31.360]   And I was keeping all the outputs,
[00:01:31.360 --> 00:01:36.320]   the recording of all the outputs and the results in my notebook.
[00:01:36.320 --> 00:01:40.400]   And I had pages of pages of changes or little tweaks
[00:01:40.400 --> 00:01:41.440]   that I was doing.
[00:01:41.440 --> 00:01:42.800]   And I was writing them down.
[00:01:42.800 --> 00:01:47.600]   And of course, at some point, I kept track of some information
[00:01:47.600 --> 00:01:49.920]   or some information was lost in the process
[00:01:49.920 --> 00:01:51.240]   and was quite hectic.
[00:01:51.240 --> 00:01:52.880]   It was fun, but it was super hectic.
[00:01:52.880 --> 00:01:55.240]   And I didn't know, actually, there
[00:01:55.240 --> 00:02:03.320]   is a smart way to keep track of everything that I was doing
[00:02:03.320 --> 00:02:06.440]   instead of, I don't know, just writing on hard paper.
[00:02:06.440 --> 00:02:08.360]   And then I discovered weights and biases.
[00:02:08.360 --> 00:02:12.600]   And at this point, every time I'm doing anything,
[00:02:12.600 --> 00:02:15.680]   I am using weights and biases because it's extremely,
[00:02:15.680 --> 00:02:17.960]   extremely easy and seamless.
[00:02:17.960 --> 00:02:21.440]   And it goes right in the code.
[00:02:21.440 --> 00:02:24.080]   It's not very complicated, I swear to God.
[00:02:24.080 --> 00:02:26.000]   I'm sorry to say, but at the beginning,
[00:02:26.000 --> 00:02:28.360]   I really thought, oh, this is going to be so complicated.
[00:02:28.360 --> 00:02:31.120]   How do I implement this in my already existing code?
[00:02:31.120 --> 00:02:31.600]   It's not.
[00:02:31.600 --> 00:02:34.640]   It's just very, very simple lines of code.
[00:02:34.640 --> 00:02:36.880]   And in four or five lines of code,
[00:02:36.880 --> 00:02:38.560]   you are ready, set, and good to go.
[00:02:38.560 --> 00:02:40.240]   And you are tracking everything.
[00:02:40.240 --> 00:02:42.960]   And you are saving everything.
[00:02:42.960 --> 00:02:47.120]   All the artifacts, or your models,
[00:02:47.120 --> 00:02:49.480]   or the versions of your data sets
[00:02:49.480 --> 00:02:50.640]   that you are pre-processing.
[00:02:50.640 --> 00:02:52.640]   So it's super, super, super easy.
[00:02:52.640 --> 00:02:57.760]   And now my folders don't look so hectic anymore.
[00:02:57.760 --> 00:03:01.040]   So anyways, today, I am going to present to you
[00:03:01.040 --> 00:03:06.200]   a project I did on Kaggle a few months ago on RSNA Fracture
[00:03:06.200 --> 00:03:07.720]   Detection Competition.
[00:03:07.720 --> 00:03:10.480]   So very quickly, what we had to do,
[00:03:10.480 --> 00:03:16.520]   we had these JCon files and a metadata of those JCon files
[00:03:16.520 --> 00:03:20.520]   that contain CT scans of the cervical spine,
[00:03:20.520 --> 00:03:24.680]   or the part of the human that's the cervical spine.
[00:03:24.680 --> 00:03:29.000]   And we had to identify in which vertebrae,
[00:03:29.000 --> 00:03:34.480]   or if in any vertebrae, there was a fracture.
[00:03:34.480 --> 00:03:38.160]   There can be multiple fractures in one patient,
[00:03:38.160 --> 00:03:40.400]   or there can be no fractures in one patient.
[00:03:40.400 --> 00:03:43.240]   So this was the idea.
[00:03:43.240 --> 00:03:46.120]   So first, I did the exploratory data analysis
[00:03:46.120 --> 00:03:50.280]   and the pre-processing, which I really, really, truly love.
[00:03:50.280 --> 00:03:52.320]   This is the part that actually excites me the most.
[00:03:52.320 --> 00:03:55.800]   So here, very simply, you import weights and biases,
[00:03:55.800 --> 00:03:57.240]   and you can forget it.
[00:03:57.240 --> 00:04:02.880]   And then you log in the secret key.
[00:04:02.880 --> 00:04:05.080]   Usually, if you are in your own notebooks,
[00:04:05.080 --> 00:04:08.480]   you just have an MPI key that's in your profile,
[00:04:08.480 --> 00:04:09.480]   and you use it.
[00:04:09.480 --> 00:04:12.120]   But because this notebook is public,
[00:04:12.120 --> 00:04:14.640]   I am using Kaggle secrets.
[00:04:14.640 --> 00:04:17.080]   But it's just a line of code when you say, one,
[00:04:17.080 --> 00:04:20.960]   to be logged in, and then your unique API key.
[00:04:20.960 --> 00:04:23.680]   And because I've been using weights and biases for so long,
[00:04:23.680 --> 00:04:25.920]   and at some point, my code started
[00:04:25.920 --> 00:04:29.880]   to be repetitive throughout my notebooks in terms
[00:04:29.880 --> 00:04:31.560]   of what I wanted to do.
[00:04:31.560 --> 00:04:35.240]   Because for some example, I wanted to save plots,
[00:04:35.240 --> 00:04:40.080]   and saving multiple plots kind of got me repetitive code.
[00:04:40.080 --> 00:04:46.640]   Or save artifacts, I did these very quick and easy functions
[00:04:46.640 --> 00:04:48.600]   that I just call.
[00:04:48.600 --> 00:04:54.080]   And they save my artifacts for me, and here, my plots.
[00:04:54.080 --> 00:04:57.960]   I have here an argument that says line,
[00:04:57.960 --> 00:05:00.880]   if I want a line plot, a bias if I want a bar plot,
[00:05:00.880 --> 00:05:04.000]   and a scatter if I want scatter, and so on.
[00:05:04.000 --> 00:05:07.760]   OK, so first, I started looking at the tabular data.
[00:05:07.760 --> 00:05:09.440]   Here is my first experiment.
[00:05:09.440 --> 00:05:12.280]   This is super quick and easy.
[00:05:12.280 --> 00:05:14.880]   And this creates here--
[00:05:14.880 --> 00:05:16.040]   let's find out.
[00:05:16.040 --> 00:05:19.480]   So tabular explorer, I have here tabular explorer.
[00:05:19.480 --> 00:05:21.360]   Usually within my dashboard-- so this
[00:05:21.360 --> 00:05:23.720]   is my dashboard for this composition--
[00:05:23.720 --> 00:05:26.680]   I immediately, once I create the dashboard,
[00:05:26.680 --> 00:05:28.720]   and I start logging things into it,
[00:05:28.720 --> 00:05:31.480]   I come here and group by name.
[00:05:31.480 --> 00:05:33.520]   Because if you don't group by name,
[00:05:33.520 --> 00:05:37.440]   the experiments are going to come up as you run them.
[00:05:37.440 --> 00:05:40.480]   And as you can see, there are quite a lot of experiments
[00:05:40.480 --> 00:05:43.360]   for this particular project.
[00:05:43.360 --> 00:05:47.160]   So immediately, no question asked, I come here,
[00:05:47.160 --> 00:05:49.240]   and I put name, and I have everything here.
[00:05:49.240 --> 00:05:53.360]   And it's kind of grouped, so I don't have to, I don't know,
[00:05:53.360 --> 00:05:56.840]   mess around or search for a particular experiment
[00:05:56.840 --> 00:05:58.520]   that I'm looking at.
[00:05:58.520 --> 00:06:01.600]   OK, and then here, I am reading the data.
[00:06:01.600 --> 00:06:06.440]   I am looking at the CSV files.
[00:06:06.440 --> 00:06:09.160]   And here, I'm beginning the analysis.
[00:06:09.160 --> 00:06:11.640]   I'm doing, for example, some bar charts.
[00:06:11.640 --> 00:06:15.160]   So what I find extremely interesting and very, very
[00:06:15.160 --> 00:06:19.880]   useful is sometimes when you are having a big project--
[00:06:19.880 --> 00:06:23.000]   and here is not the case, but sometimes you
[00:06:23.000 --> 00:06:30.520]   have 5, 10, even tens of CSV files or just tabular data
[00:06:30.520 --> 00:06:32.360]   sets that you are looking at.
[00:06:32.360 --> 00:06:36.040]   And you are finding some very, very interesting information
[00:06:36.040 --> 00:06:38.600]   that you want to keep, like a graph, for example,
[00:06:38.600 --> 00:06:40.720]   that you want to look at in the future.
[00:06:40.720 --> 00:06:44.560]   In this case, I would highly recommend, like I am doing,
[00:06:44.560 --> 00:06:49.360]   is to log all the bar plots or the charts that
[00:06:49.360 --> 00:06:54.080]   are very, very interesting to you into the dashboard.
[00:06:54.080 --> 00:06:56.760]   Because in, I don't know, one month,
[00:06:56.760 --> 00:06:58.720]   when you come back to the project
[00:06:58.720 --> 00:07:01.800]   or maybe you want to look back in the past,
[00:07:01.800 --> 00:07:04.960]   it's going to be super, super easy instead of just looking
[00:07:04.960 --> 00:07:07.480]   between notebooks and notebooks to find that information.
[00:07:07.480 --> 00:07:10.680]   So this is why, for example, here, I
[00:07:10.680 --> 00:07:13.680]   have this graph where I see, OK, the patient.
[00:07:13.680 --> 00:07:15.920]   I can see the labels are 50/50.
[00:07:15.920 --> 00:07:19.800]   So I have 50% of my patients that had no fracture,
[00:07:19.800 --> 00:07:23.400]   and 50% of the patients had at least one fracture
[00:07:23.400 --> 00:07:25.160]   within their cervical spine.
[00:07:25.160 --> 00:07:27.520]   And here, I have also the information
[00:07:27.520 --> 00:07:31.720]   on which vertebrae within the seven vertebrae
[00:07:31.720 --> 00:07:34.560]   within the cervical spine had fractures.
[00:07:34.560 --> 00:07:36.760]   So most of the fractures, for example,
[00:07:36.760 --> 00:07:40.240]   were in the first vertebrae, whereas the least fractures
[00:07:40.240 --> 00:07:41.880]   were in the last vertebrae.
[00:07:41.880 --> 00:07:44.720]   And this is very, very useful information for you
[00:07:44.720 --> 00:07:48.080]   or for your teammates, because you can share this dashboard
[00:07:48.080 --> 00:07:50.160]   with your teammates.
[00:07:50.160 --> 00:07:54.560]   So save anything or any number that you
[00:07:54.560 --> 00:07:59.360]   might find useful or interesting or just useful in the future.
[00:07:59.360 --> 00:08:03.280]   OK, so here, I'm doing more exploration just
[00:08:03.280 --> 00:08:04.960]   to understand here.
[00:08:04.960 --> 00:08:09.040]   I'm also saving the images just to have an understanding
[00:08:09.040 --> 00:08:13.160]   within the dashboard, which is super, super, super easy.
[00:08:13.160 --> 00:08:16.680]   I just log using 1db.image.
[00:08:16.680 --> 00:08:19.960]   And then here, I have all my images within the notebook.
[00:08:19.960 --> 00:08:23.200]   But I also have my images within the dashboard,
[00:08:23.200 --> 00:08:24.560]   which are right here.
[00:08:24.560 --> 00:08:27.320]   And if I can look a little bit through this,
[00:08:27.320 --> 00:08:31.480]   you can see how they change as the CT scans evolve
[00:08:31.480 --> 00:08:35.160]   for this particular patient, which is very unique and weird
[00:08:35.160 --> 00:08:37.360]   number.
[00:08:37.360 --> 00:08:40.360]   OK, so I'm going through some example.
[00:08:40.360 --> 00:08:43.240]   Then I am looking at the DECOM metadata.
[00:08:43.240 --> 00:08:46.560]   If you might not know, within a DECOM file--
[00:08:46.560 --> 00:08:51.320]   this is just a format, how they store these medical images--
[00:08:51.320 --> 00:08:53.520]   they store some additional information,
[00:08:53.520 --> 00:08:59.960]   like the image position, how the certain camera
[00:08:59.960 --> 00:09:02.840]   or whatever is called in the medical field
[00:09:02.840 --> 00:09:06.360]   was the position of it when they took the scan.
[00:09:06.360 --> 00:09:08.560]   So a lot, a lot of information.
[00:09:08.560 --> 00:09:12.280]   So here, I'm saving all this metadata.
[00:09:12.280 --> 00:09:17.600]   I am logging this information within my dashboard.
[00:09:17.600 --> 00:09:21.040]   And then I am looking at it to see some distributions,
[00:09:21.040 --> 00:09:23.600]   saving them to weights and biases as I find them
[00:09:23.600 --> 00:09:26.120]   interesting or if I find them interesting,
[00:09:26.120 --> 00:09:27.360]   and so on and so forth.
[00:09:28.360 --> 00:09:31.920]   OK, now here, I'm looking at some bounding boxes
[00:09:31.920 --> 00:09:36.960]   because-- so it wasn't only the fact that the CT scans are
[00:09:36.960 --> 00:09:39.560]   or are they not within the image,
[00:09:39.560 --> 00:09:43.840]   but we also had the information of the placement
[00:09:43.840 --> 00:09:46.320]   where the structure was at.
[00:09:46.320 --> 00:09:50.040]   And I kind of wanted to look at that too.
[00:09:50.040 --> 00:09:53.040]   And I think that's about it.
[00:09:53.040 --> 00:09:55.280]   Here, I looked at the 3D scan.
[00:09:55.280 --> 00:09:58.960]   And then here, I'm saving my artifacts.
[00:09:58.960 --> 00:10:03.320]   First, the training data that I pre-processed
[00:10:03.320 --> 00:10:06.880]   from the competitors, from the competition.
[00:10:06.880 --> 00:10:11.680]   And then the second was the metadata from the decon file.
[00:10:11.680 --> 00:10:20.080]   OK, now the second notebook was just a very baseline model
[00:10:20.080 --> 00:10:25.960]   that I just trained and kind of tried to inference
[00:10:25.960 --> 00:10:33.200]   whether there was a fracture or not within the cervical spine.
[00:10:33.200 --> 00:10:38.000]   OK, so it's exactly the same procedure.
[00:10:38.000 --> 00:10:41.640]   We import weights, biases.
[00:10:41.640 --> 00:10:44.440]   Here are some other libraries that I used.
[00:10:44.440 --> 00:10:46.800]   Import my API key.
[00:10:46.800 --> 00:10:51.120]   Here again, I do this all the time.
[00:10:51.120 --> 00:10:53.280]   Even if I'm not using it, I'm doing it
[00:10:53.280 --> 00:10:54.760]   because you never know.
[00:10:54.760 --> 00:10:59.840]   My two trusted functions to plot and save my artifacts.
[00:10:59.840 --> 00:11:02.520]   And then I started looking at the evaluation metric
[00:11:02.520 --> 00:11:06.000]   because usually, you know, you have one target
[00:11:06.000 --> 00:11:07.640]   within a competition.
[00:11:07.640 --> 00:11:10.040]   In this case, there were seven--
[00:11:10.040 --> 00:11:11.640]   actually, eight targets.
[00:11:11.640 --> 00:11:16.680]   So one target flagging if the patient has a fracture
[00:11:16.680 --> 00:11:19.160]   and had any fracture.
[00:11:19.160 --> 00:11:21.960]   And then the other seven targets with zero
[00:11:21.960 --> 00:11:24.920]   if in that particular cervical area,
[00:11:24.920 --> 00:11:29.280]   there was a fracture and with one if it wasn't,
[00:11:29.280 --> 00:11:32.960]   and if one if it was a fracture.
[00:11:32.960 --> 00:11:34.600]   So there were eight targets in total,
[00:11:34.600 --> 00:11:35.720]   which was quite interesting.
[00:11:35.720 --> 00:11:38.160]   So the evaluation metric was a little bit interesting
[00:11:38.160 --> 00:11:39.440]   or I didn't even--
[00:11:39.440 --> 00:11:42.680]   I haven't seen something like this before.
[00:11:42.680 --> 00:11:45.360]   There was a custom loss function for this competition.
[00:11:45.360 --> 00:11:47.400]   Here is just the normal drill.
[00:11:47.400 --> 00:11:49.440]   I'm doing a data split.
[00:11:49.440 --> 00:11:51.960]   I'm creating the PyTorch data set
[00:11:51.960 --> 00:11:58.840]   to read in the metadata and the images.
[00:11:58.840 --> 00:12:03.160]   Here is the model, the loss, and the gradual warm-up.
[00:12:03.160 --> 00:12:06.920]   And then we're going into the training part.
[00:12:06.920 --> 00:12:10.480]   So here first is the training function,
[00:12:10.480 --> 00:12:15.320]   the training for one particular APOC, so just for one APOC.
[00:12:15.320 --> 00:12:20.720]   And just logging in to Weights and Biases,
[00:12:20.720 --> 00:12:24.080]   all the metrics that you want during training,
[00:12:24.080 --> 00:12:25.840]   it's super, super easy.
[00:12:25.840 --> 00:12:28.920]   So once you usually go through model.train,
[00:12:28.920 --> 00:12:31.800]   and then you look through all the batches
[00:12:31.800 --> 00:12:35.880]   and do all the forward and backwards propagation,
[00:12:35.880 --> 00:12:37.800]   you compute the losses.
[00:12:37.800 --> 00:12:40.680]   And then everything I did here was
[00:12:40.680 --> 00:12:43.840]   to log into Weights and Biases with 1db.log,
[00:12:43.840 --> 00:12:47.400]   the train loss, and then the competition loss.
[00:12:47.400 --> 00:12:48.640]   And then that's it.
[00:12:48.640 --> 00:12:50.920]   And then I add step equals APOC.
[00:12:50.920 --> 00:12:55.080]   So every APOC, one Weights and Biases
[00:12:55.080 --> 00:12:57.240]   logs this information for me.
[00:12:57.240 --> 00:13:01.400]   And then for the validation APOC, it's exactly the same.
[00:13:01.400 --> 00:13:04.640]   So at the end here, I log the validation loss,
[00:13:04.640 --> 00:13:06.880]   the validation competition loss.
[00:13:06.880 --> 00:13:09.800]   And then one more step, I added here
[00:13:09.800 --> 00:13:13.120]   the validation area under the curve,
[00:13:13.120 --> 00:13:16.120]   which was just another metric that I wanted
[00:13:16.120 --> 00:13:18.840]   to use in this competition.
[00:13:18.840 --> 00:13:22.320]   So you can log whatever and how many metrics
[00:13:22.320 --> 00:13:26.320]   you want for each APOC.
[00:13:26.320 --> 00:13:32.640]   OK, here is a very cutesy scheme, let's say,
[00:13:32.640 --> 00:13:37.040]   of how the full training pipeline works.
[00:13:37.040 --> 00:13:39.560]   And here with the spiky yellow is
[00:13:39.560 --> 00:13:41.360]   wherever I'm using Weights and Biases.
[00:13:41.360 --> 00:13:45.080]   So I'm using Weights and Biases only in three places.
[00:13:45.080 --> 00:13:47.040]   And then I run everything.
[00:13:47.040 --> 00:13:48.320]   And it's super easy.
[00:13:48.320 --> 00:13:51.840]   And it just keeps all the information
[00:13:51.840 --> 00:13:54.160]   that I want and that I need.
[00:13:54.160 --> 00:13:55.720]   So at the beginning, and then when
[00:13:55.720 --> 00:13:59.080]   I'm doing the training and the validation, and that's it.
[00:13:59.080 --> 00:14:01.200]   So at the beginning of the training,
[00:14:01.200 --> 00:14:04.720]   like the big main training function,
[00:14:04.720 --> 00:14:09.080]   I am creating my configuration for this particular training
[00:14:09.080 --> 00:14:10.040]   or an experiment.
[00:14:10.040 --> 00:14:13.080]   And say, I am creating my parameters here.
[00:14:13.080 --> 00:14:15.520]   You can put how many parameters you want here.
[00:14:15.520 --> 00:14:18.800]   I am explaining, OK, so my model is a dense net.
[00:14:18.800 --> 00:14:23.800]   Here I'm inputting all my APOCs, the splits, the k-fold
[00:14:23.800 --> 00:14:29.080]   validation that I used, the batch, the learning rates, how
[00:14:29.080 --> 00:14:34.000]   big or what was the size of the images, the stack size,
[00:14:34.000 --> 00:14:36.520]   and the data frame size.
[00:14:36.520 --> 00:14:41.200]   And then I update my parameters.
[00:14:41.200 --> 00:14:43.800]   So runConfig has my parameters.
[00:14:43.800 --> 00:14:46.000]   Then I create the experiment.
[00:14:46.000 --> 00:14:50.600]   And then I go through the steps that you would usually go.
[00:14:50.600 --> 00:14:54.280]   Here I track my model and the guidance of my model,
[00:14:54.280 --> 00:14:58.000]   1db.watch, with a frequency of 100.
[00:14:58.000 --> 00:15:00.640]   So you don't overclutter everything.
[00:15:00.640 --> 00:15:03.600]   And then I am training.
[00:15:03.600 --> 00:15:06.840]   Here are my training and my validation functions
[00:15:06.840 --> 00:15:09.360]   that I showed you previously.
[00:15:09.360 --> 00:15:10.960]   And then once everything is done,
[00:15:10.960 --> 00:15:13.920]   I just press digest to 1db.finish.
[00:15:13.920 --> 00:15:16.400]   And everything, everything, everything
[00:15:16.400 --> 00:15:20.520]   is stored within your dashboard for you,
[00:15:20.520 --> 00:15:22.240]   for your teammates to see.
[00:15:22.240 --> 00:15:26.760]   And it's extremely easy to also compare between experiments
[00:15:26.760 --> 00:15:30.440]   to see for how long they have run,
[00:15:30.440 --> 00:15:34.320]   also if they plateaued in some point, what was the point,
[00:15:34.320 --> 00:15:36.400]   and so on and so forth.
[00:15:36.400 --> 00:15:39.800]   So here I'm actually doing the training.
[00:15:39.800 --> 00:15:41.640]   It trains for a while.
[00:15:41.640 --> 00:15:45.200]   And then lastly, at the end, I'm saving to WordDefact.
[00:15:45.200 --> 00:15:50.720]   So first of all, it's the logs.
[00:15:50.720 --> 00:15:54.520]   So during the training, I also logged some information
[00:15:54.520 --> 00:15:58.800]   that I wanted into a txte file, which I saved.
[00:15:58.800 --> 00:16:01.200]   I actually can show you here.
[00:16:01.200 --> 00:16:03.960]   So within the artifacts, I have here
[00:16:03.960 --> 00:16:06.200]   logs, which has two versions.
[00:16:06.200 --> 00:16:08.440]   And in files, I have it here.
[00:16:08.440 --> 00:16:12.560]   So I can download it at any time if I want to.
[00:16:12.560 --> 00:16:14.320]   But that's the thing.
[00:16:14.320 --> 00:16:17.560]   You don't have to overclutter your device
[00:16:17.560 --> 00:16:20.800]   with a bunch of folders having dates.
[00:16:20.800 --> 00:16:23.920]   I don't know, 100 takes this for 100 experiments.
[00:16:23.920 --> 00:16:26.320]   You have just everything in your dashboard.
[00:16:26.320 --> 00:16:27.880]   And if you want to look at something,
[00:16:27.880 --> 00:16:30.560]   you can just download it.
[00:16:30.560 --> 00:16:34.440]   And then I have the actual model.
[00:16:34.440 --> 00:16:37.720]   So within my special function-- this is super easy.
[00:16:37.720 --> 00:16:39.120]   Anybody can do this.
[00:16:39.120 --> 00:16:42.360]   I have a data type for data set.
[00:16:42.360 --> 00:16:46.480]   If I have anything like a CSV or a parquet file or a txte file,
[00:16:46.480 --> 00:16:49.360]   and then for a model, if I have a trained model
[00:16:49.360 --> 00:16:52.160]   that I want to save.
[00:16:52.160 --> 00:16:55.040]   And here, for example, I have my model, again,
[00:16:55.040 --> 00:16:56.560]   with the version 1 and version 2.
[00:16:56.560 --> 00:16:58.480]   And then here is the file.
[00:16:58.480 --> 00:17:01.640]   OK, so yeah, I hope you liked it.
[00:17:01.640 --> 00:17:04.200]   And thank you so very much.
[00:17:04.200 --> 00:17:08.200]   And I hope you will try this in the future.
[00:17:08.200 --> 00:17:09.160]   Thank you.
[00:17:09.160 --> 00:17:09.960]   Thank you, Andrana.
[00:17:09.960 --> 00:17:13.000]   Thank you for this interesting use case.
[00:17:13.000 --> 00:17:16.520]   I'd like to introduce our next presenter, Atharva.
[00:17:16.520 --> 00:17:19.920]   Atharva is currently in the last semester of university.
[00:17:19.920 --> 00:17:21.960]   He's an intern at Walters Kluwer.
[00:17:21.960 --> 00:17:25.120]   And his work mostly involves document intelligence.
[00:17:25.120 --> 00:17:29.160]   Atharva has been an active WMB user for the past three years
[00:17:29.160 --> 00:17:31.440]   and is also one of our ambassadors.
[00:17:31.440 --> 00:17:33.880]   He's very active in Kaggle competitions,
[00:17:33.880 --> 00:17:37.600]   is a Kaggle competition expert, and is avidly involved
[00:17:37.600 --> 00:17:39.960]   in open-sourced contributions.
[00:17:39.960 --> 00:17:41.120]   Welcome, Atharva.
[00:17:41.120 --> 00:17:43.000]   Thank you, Pauri, for the nice introduction.
[00:17:43.000 --> 00:17:44.200]   Thank you for having me.
[00:17:44.200 --> 00:17:48.240]   So today, I'm going to talk about 3D segmentation
[00:17:48.240 --> 00:17:51.320]   with Moni and PyTorch with weights and biases
[00:17:51.320 --> 00:17:53.360]   integrated inside of it.
[00:17:53.360 --> 00:17:58.160]   The Moni library, as many of you might not know,
[00:17:58.160 --> 00:18:01.920]   that NVIDIA is the co-founder of the project Moni.
[00:18:01.920 --> 00:18:03.600]   And it is a PyTorch-based framework
[00:18:03.600 --> 00:18:06.600]   which you can use for your imaging data
[00:18:06.600 --> 00:18:10.400]   or any medical kind of problem statement that you may have.
[00:18:10.400 --> 00:18:13.160]   Moni has all of the things that you might
[00:18:13.160 --> 00:18:15.640]   need in any medical project.
[00:18:15.640 --> 00:18:17.280]   So it is completely open-source.
[00:18:17.280 --> 00:18:18.920]   Anyone can use it.
[00:18:18.920 --> 00:18:22.640]   And anyone can even see the code and contribute to it.
[00:18:22.640 --> 00:18:25.560]   So for this talk, I'm going to talk
[00:18:25.560 --> 00:18:28.200]   about segmentation of a spleen.
[00:18:28.200 --> 00:18:31.480]   So many of you might not know what a spleen is.
[00:18:31.480 --> 00:18:34.040]   Basically, a spleen is an organ which
[00:18:34.040 --> 00:18:36.320]   is located beside your stomach.
[00:18:36.320 --> 00:18:39.800]   And its main responsibility is to clean your blood cells
[00:18:39.800 --> 00:18:43.200]   or kill the unused blood cells, et cetera.
[00:18:43.200 --> 00:18:47.760]   So the data is from medicaldecathlon.com.
[00:18:47.760 --> 00:18:52.840]   There are many tasks inside of it, like lung segmentation,
[00:18:52.840 --> 00:18:54.400]   brain tumor segmentation.
[00:18:54.400 --> 00:18:56.320]   So I've just picked up the spleen.
[00:18:56.320 --> 00:19:00.640]   So here I'm just installing Moni, weights and biases,
[00:19:00.640 --> 00:19:03.680]   blood lip, all the necessary stuff.
[00:19:03.680 --> 00:19:06.040]   Then I'm downloading the data here.
[00:19:06.040 --> 00:19:08.520]   I'm downloading the spleen data set.
[00:19:08.520 --> 00:19:12.040]   And the spleen data set comes in 3D volumes.
[00:19:12.040 --> 00:19:16.400]   So this particular data set has 61 images inside of it,
[00:19:16.400 --> 00:19:18.880]   41 for training and 20 for testing.
[00:19:18.880 --> 00:19:21.480]   And the 3D images are arranged in a particular way
[00:19:21.480 --> 00:19:22.800]   that each image--
[00:19:22.800 --> 00:19:27.360]   so in each 3D image, we have multiple 2D images.
[00:19:27.360 --> 00:19:29.880]   So these are splices of that 3D image.
[00:19:29.880 --> 00:19:32.800]   And when we combine them in 3D mental,
[00:19:32.800 --> 00:19:35.080]   they form the complete 3D volume.
[00:19:35.080 --> 00:19:39.800]   And we can see the whole 3D structure of the image.
[00:19:39.800 --> 00:19:41.360]   So this is a segmentation task.
[00:19:41.360 --> 00:19:43.880]   And for any segmentation task, it
[00:19:43.880 --> 00:19:48.080]   is important to look at how the segmentations are drawn
[00:19:48.080 --> 00:19:50.640]   on the image, how they would look on the image
[00:19:50.640 --> 00:19:53.720]   actually, or the ground truths that you are dealing with.
[00:19:53.720 --> 00:19:57.000]   So I'm just defining a utility function
[00:19:57.000 --> 00:20:01.280]   to log the weights and biases, log the image on weights
[00:20:01.280 --> 00:20:05.440]   and biases dashboard, along with the mask data and the class
[00:20:05.440 --> 00:20:05.940]   label.
[00:20:05.940 --> 00:20:08.760]   So 0 is for background, and 1 is for the mask.
[00:20:08.760 --> 00:20:12.120]   We will see that shortly below.
[00:20:12.120 --> 00:20:18.160]   So I'm just logging a single image, some splices
[00:20:18.160 --> 00:20:19.880]   from a single images.
[00:20:19.880 --> 00:20:24.480]   And so I've just set 100 splices from a 3D volume.
[00:20:24.480 --> 00:20:31.200]   So we can see that this is index 0 is basically the slice 1,
[00:20:31.200 --> 00:20:32.480]   slice number 0.
[00:20:32.480 --> 00:20:37.440]   Then as you can go further, then slice 10, slice 15.
[00:20:37.440 --> 00:20:40.780]   So all the slices combine an image.
[00:20:40.780 --> 00:20:44.240]   And then we can see the segmentation mask here.
[00:20:44.240 --> 00:20:49.960]   So we can see that this red thing that is being marked here
[00:20:49.960 --> 00:20:54.200]   is the spline organ we are dealing with.
[00:20:54.200 --> 00:20:57.840]   And the 3D image starts from left.
[00:20:57.840 --> 00:21:00.680]   So let's say you are starting from the start.
[00:21:00.680 --> 00:21:03.680]   Then you are not seeing any spline organ here.
[00:21:03.680 --> 00:21:07.280]   But as you progress further, you might see the spline organ
[00:21:07.280 --> 00:21:11.560]   because it is in between.
[00:21:11.560 --> 00:21:13.560]   As you can see, these red markings are--
[00:21:13.560 --> 00:21:17.760]   so basically, the between part is being showed here.
[00:21:17.760 --> 00:21:24.240]   And then after that, we see that there is no segmentation mask.
[00:21:24.240 --> 00:21:27.160]   So we understand that the spline is basically
[00:21:27.160 --> 00:21:32.960]   located inside the 3D volume, in a part of the 3D volume.
[00:21:32.960 --> 00:21:36.440]   So yeah, moving further.
[00:21:36.440 --> 00:21:39.920]   Then I am defining the configuration
[00:21:39.920 --> 00:21:41.240]   in a dictionary format here.
[00:21:41.240 --> 00:21:43.680]   So Weights and Biases allow you to track
[00:21:43.680 --> 00:21:45.680]   everything you are doing.
[00:21:45.680 --> 00:21:48.800]   So you just have to format in a way
[00:21:48.800 --> 00:21:52.520]   that it can be later serialized into a dictionary format
[00:21:52.520 --> 00:21:57.360]   so that the Weights and Biases can accept that in the vaunt.init
[00:21:57.360 --> 00:21:58.320]   method.
[00:21:58.320 --> 00:22:01.720]   And as you can see, this is my cache rate, number of workers
[00:22:01.720 --> 00:22:04.680]   I am using, print_batch_size, validation_batch_size,
[00:22:04.680 --> 00:22:06.120]   learning_rate, et cetera.
[00:22:06.120 --> 00:22:09.120]   Everything is being tracked on Weights and Biases.
[00:22:09.120 --> 00:22:11.800]   And for this task, we are using dice metric,
[00:22:11.800 --> 00:22:14.880]   which is a common metric used for segmentation.
[00:22:14.880 --> 00:22:18.760]   And I will quickly run you through the training process,
[00:22:18.760 --> 00:22:23.080]   how you can log your losses, all the gradients, all
[00:22:23.080 --> 00:22:26.360]   the validation losses, accuracies, et cetera.
[00:22:26.360 --> 00:22:30.800]   So we start Weights and Biases run with vaunt.init method.
[00:22:30.800 --> 00:22:32.640]   Now you define the project.
[00:22:32.640 --> 00:22:35.000]   And vaunt.init accepts many arguments.
[00:22:35.000 --> 00:22:37.280]   You can look up on the documentation.
[00:22:37.280 --> 00:22:39.760]   So it accepts things like tags, which you
[00:22:39.760 --> 00:22:43.000]   can use for filtering the runs, then things like nodes,
[00:22:43.000 --> 00:22:45.320]   which you can keep a track of what
[00:22:45.320 --> 00:22:49.600]   you are doing at the time of running the experiment.
[00:22:49.600 --> 00:22:52.720]   Then I'm just using one.watch to log
[00:22:52.720 --> 00:22:53.880]   the gradients of the model.
[00:22:53.880 --> 00:22:55.520]   And I've set lock frequency to 100.
[00:22:55.520 --> 00:23:00.200]   So it will log after every 100 steps or 100 batches.
[00:23:00.200 --> 00:23:03.120]   Then this is a typical PyTorch training loop,
[00:23:03.120 --> 00:23:06.280]   which I won't explain in depth.
[00:23:06.280 --> 00:23:09.840]   So this is the main--
[00:23:09.840 --> 00:23:11.240]   all the losses are being tracked.
[00:23:11.240 --> 00:23:15.880]   So just with a single line, you can see what are the losses,
[00:23:15.880 --> 00:23:18.280]   what was the loss, how the loss progressed
[00:23:18.280 --> 00:23:20.840]   over the training period.
[00:23:20.840 --> 00:23:24.120]   And vaunt.log is a very powerful method.
[00:23:24.120 --> 00:23:29.360]   You can log anything, ranging it from scalar values or text.
[00:23:29.360 --> 00:23:31.040]   Even you can log 3D objects.
[00:23:31.040 --> 00:23:32.280]   You can log images.
[00:23:32.280 --> 00:23:36.160]   So anything you can think of can be logged using vaunt.log.
[00:23:36.160 --> 00:23:40.760]   Then I'm also logging training loss for each epoch.
[00:23:40.760 --> 00:23:43.400]   Here I was logging for per step.
[00:23:43.400 --> 00:23:45.200]   Here for each epoch.
[00:23:45.200 --> 00:23:48.280]   Then one of the main thing I log is learning rate,
[00:23:48.280 --> 00:23:52.160]   because if you are using a cosine scheduler with warmup,
[00:23:52.160 --> 00:23:58.400]   and if your learning rate is not behaving as per you said,
[00:23:58.400 --> 00:24:00.520]   for some reason, or some error input,
[00:24:00.520 --> 00:24:04.400]   you might never know, because it will not throw any error.
[00:24:04.400 --> 00:24:08.080]   So basically, you have to log the learning rate.
[00:24:08.080 --> 00:24:10.840]   It is a very important practice to log the learning rate,
[00:24:10.840 --> 00:24:14.560]   so that you know that there are no silent errors in your code,
[00:24:14.560 --> 00:24:17.080]   and all the learning rate situation
[00:24:17.080 --> 00:24:19.760]   is being taken care of.
[00:24:19.760 --> 00:24:23.400]   Then I'm also logging the dice metric here,
[00:24:23.400 --> 00:24:26.520]   which is the main thing we are interested in,
[00:24:26.520 --> 00:24:28.440]   the validation dice metric.
[00:24:28.440 --> 00:24:33.080]   Then also I'm saving the model, the best model after each epoch.
[00:24:33.080 --> 00:24:35.280]   So if the model improves after each epoch,
[00:24:35.280 --> 00:24:41.400]   I'm saving it as in a pytorch typical.ptx file.
[00:24:41.400 --> 00:24:44.360]   Then at the end of the run, I'm saving
[00:24:44.360 --> 00:24:48.760]   what was the best dice metric, what was the best metric epoch,
[00:24:48.760 --> 00:24:52.440]   where I got the best dice metric at the end of the run.
[00:24:52.440 --> 00:24:57.040]   So here you can see all the losses being plotted here.
[00:24:57.040 --> 00:25:01.240]   So I will quickly show you this in the dashboard itself.
[00:25:01.240 --> 00:25:04.800]   So these are all the runs that are run
[00:25:04.800 --> 00:25:06.800]   or being experimented with.
[00:25:06.800 --> 00:25:09.600]   And these are all the plots you can see.
[00:25:09.600 --> 00:25:13.080]   So this is brain loss, which was being logged after each step.
[00:25:13.080 --> 00:25:15.800]   This was the loss which was being logged after each epoch.
[00:25:15.800 --> 00:25:20.440]   And as I say that, sometimes I used a no learning rate decay.
[00:25:20.440 --> 00:25:22.400]   Sometimes I used a cosine decay.
[00:25:22.400 --> 00:25:25.560]   So you can clearly see how the logging, the learning rate
[00:25:25.560 --> 00:25:26.920]   was useful here.
[00:25:26.920 --> 00:25:29.520]   And we can see the dice metric.
[00:25:29.520 --> 00:25:33.440]   Also, it's very helpful if you plot a scalar chart, which
[00:25:33.440 --> 00:25:37.520]   will show what was the best experiment you ran.
[00:25:37.520 --> 00:25:41.360]   This is a 1B scalar chart, which shows that glorious donkey
[00:25:41.360 --> 00:25:47.600]   58 was my best experiment with a validation metric of 0.9527.
[00:25:47.600 --> 00:25:52.960]   And moving further to the gradient section,
[00:25:52.960 --> 00:25:55.080]   like logging the gradient section.
[00:25:55.080 --> 00:25:58.120]   So it is very important to know what
[00:25:58.120 --> 00:26:01.440]   the gradients of the model are across the training period.
[00:26:01.440 --> 00:26:03.640]   So with just a single line of command,
[00:26:03.640 --> 00:26:06.120]   you can log all the model gradient.
[00:26:06.120 --> 00:26:09.360]   And you can see the gradient of each layer.
[00:26:09.360 --> 00:26:13.400]   So how to interpret this graph is like,
[00:26:13.400 --> 00:26:16.760]   if first this is basically a histogram,
[00:26:16.760 --> 00:26:19.720]   and you might see for each layer,
[00:26:19.720 --> 00:26:23.680]   the histogram, that is the distribution of the histogram,
[00:26:23.680 --> 00:26:25.480]   should come towards 0.
[00:26:25.480 --> 00:26:29.800]   So as you can see in this gradient chart,
[00:26:29.800 --> 00:26:35.040]   at the later ending stage of the run or any experiment,
[00:26:35.040 --> 00:26:37.280]   the chart should be centered at 0,
[00:26:37.280 --> 00:26:41.640]   so that the model has reached a global optimum
[00:26:41.640 --> 00:26:46.320]   and there's no gradient update performing.
[00:26:46.320 --> 00:26:49.760]   And then also, you can see the system metrics,
[00:26:49.760 --> 00:26:52.920]   like GPU utilization, all the temperature.
[00:26:52.920 --> 00:26:56.840]   So it is very important that you use your GPU to the fullest
[00:26:56.840 --> 00:27:01.560]   and you use your hardware to the fullest
[00:27:01.560 --> 00:27:03.760]   so that there's no blockage.
[00:27:03.760 --> 00:27:08.400]   And the main-- another thing is versioning models.
[00:27:08.400 --> 00:27:12.200]   So the weights and biases also provide a way to--
[00:27:12.200 --> 00:27:14.600]   so we saw how to version your experiment
[00:27:14.600 --> 00:27:16.040]   or how to track the experiment.
[00:27:16.040 --> 00:27:18.880]   But weights and biases also provides versioning your models
[00:27:18.880 --> 00:27:20.120]   or data sets.
[00:27:20.120 --> 00:27:24.600]   So for each experiment, I was versioning the model
[00:27:24.600 --> 00:27:27.080]   and using the WANP.artifact command.
[00:27:27.080 --> 00:27:30.640]   So you can version your data sets or even models
[00:27:30.640 --> 00:27:31.960]   using this command.
[00:27:31.960 --> 00:27:36.080]   And I'll quickly show you the artifact.
[00:27:36.080 --> 00:27:39.440]   So as you can see, it creates for each run
[00:27:39.440 --> 00:27:42.120]   a version of the artifact or version of the model.
[00:27:42.120 --> 00:27:47.520]   And you can just download it using this small code snippet
[00:27:47.520 --> 00:27:50.520]   that weights and biases already provide.
[00:27:50.520 --> 00:27:56.000]   You can see useful information, like what was this model name.
[00:27:56.000 --> 00:27:58.240]   You can even set aliases to the model
[00:27:58.240 --> 00:28:02.760]   so that it becomes easy for you to filter out or download
[00:28:02.760 --> 00:28:03.640]   anything.
[00:28:03.640 --> 00:28:06.480]   Then you can see the metadata, all the configuration
[00:28:06.480 --> 00:28:09.920]   that was used to train this model, and all the files.
[00:28:09.920 --> 00:28:12.240]   So suppose in artifacts, you might have many files.
[00:28:12.240 --> 00:28:13.600]   Here, we have only one.
[00:28:13.600 --> 00:28:16.080]   But there can be many files in this.
[00:28:16.080 --> 00:28:19.240]   And we can even see the lineage, so from which
[00:28:19.240 --> 00:28:20.440]   run the artifact came.
[00:28:20.440 --> 00:28:24.280]   And then if we use this artifact in another run,
[00:28:24.280 --> 00:28:28.360]   you can also see this in this tab itself.
[00:28:28.360 --> 00:28:30.520]   Then moving to the--
[00:28:30.520 --> 00:28:33.880]   you can see the predictions in table format as well,
[00:28:33.880 --> 00:28:36.720]   all the configuration in the table format as well.
[00:28:36.720 --> 00:28:41.200]   So you can see all the things we have passed in the WANP.init
[00:28:41.200 --> 00:28:42.320]   in the config parameter.
[00:28:42.320 --> 00:28:44.540]   All of these things are being showed here.
[00:28:44.540 --> 00:28:46.640]   Then we can log.
[00:28:46.640 --> 00:28:50.720]   So one of the favorite features of Weights and Biases
[00:28:50.720 --> 00:28:52.800]   is tables for me.
[00:28:52.800 --> 00:28:55.360]   So it allows for error analysis.
[00:28:55.360 --> 00:28:58.120]   You can see where your model is doing wrong.
[00:28:58.120 --> 00:29:00.280]   So you can improve on that model.
[00:29:00.280 --> 00:29:04.640]   So now, Weights and Biases provide a WANP.table,
[00:29:04.640 --> 00:29:10.640]   which you can use to log arbitrary data in a table
[00:29:10.640 --> 00:29:11.160]   format.
[00:29:11.160 --> 00:29:13.280]   So you can group by certain things,
[00:29:13.280 --> 00:29:14.540]   or you can filter.
[00:29:14.540 --> 00:29:17.880]   You can do pretty much anything you can
[00:29:17.880 --> 00:29:19.880]   think of in a tabular format.
[00:29:19.880 --> 00:29:25.240]   So here, I'm just logging some sample files, sample
[00:29:25.240 --> 00:29:27.560]   predictions from my model.
[00:29:27.560 --> 00:29:33.800]   And we can see in this table that this is the file name,
[00:29:33.800 --> 00:29:39.080]   and this is the image that was used.
[00:29:39.080 --> 00:29:41.600]   And this was the ground truth mask, the ground truth
[00:29:41.600 --> 00:29:42.980]   segmentation mask.
[00:29:42.980 --> 00:29:44.400]   And this was the prediction.
[00:29:44.400 --> 00:29:47.760]   So we can see that almost ground truth prediction matches
[00:29:47.760 --> 00:29:48.560]   our ground truth.
[00:29:48.560 --> 00:29:53.320]   So our model is doing well in segmenting Splint WANOP.
[00:29:53.320 --> 00:29:56.720]   One more modification any user or anyone can do
[00:29:56.720 --> 00:30:01.920]   is also logging the loss for each of the images
[00:30:01.920 --> 00:30:03.040]   or each of the predictions.
[00:30:03.040 --> 00:30:05.440]   So you can sort by ascending or descending
[00:30:05.440 --> 00:30:09.400]   to know on which image the model has the maximum loss.
[00:30:09.400 --> 00:30:12.040]   So you can just filter out based on that
[00:30:12.040 --> 00:30:14.020]   to improve on the model.
[00:30:14.020 --> 00:30:18.420]   Then you can even compare two runs in a single table
[00:30:18.420 --> 00:30:21.760]   using the join feature in the tables itself.
[00:30:21.760 --> 00:30:26.500]   So for this image, we can see that this 0th run was kind
[00:30:26.500 --> 00:30:29.900]   on 65, and 1th index run is a leafy dog.
[00:30:29.900 --> 00:30:33.180]   You can compare both of the predictions
[00:30:33.180 --> 00:30:37.240]   from both of the runs using the weights and biases table.
[00:30:37.240 --> 00:30:40.860]   Then one of the pool charts that weights and biases provide
[00:30:40.860 --> 00:30:42.660]   is parallel coordinate chart.
[00:30:42.660 --> 00:30:46.280]   So I will quickly switch to another project
[00:30:46.280 --> 00:30:50.540]   because there's not much to see in this parallel coordinate
[00:30:50.540 --> 00:30:54.260]   chart as there were very much less parameters to tune.
[00:30:54.260 --> 00:30:57.900]   So recently, I did a project where
[00:30:57.900 --> 00:31:00.460]   I had to tune a lot of hyperparameters.
[00:31:00.460 --> 00:31:02.940]   And weights and biases provide their own hyperparameter
[00:31:02.940 --> 00:31:06.620]   tuning engine, which is called weights and biases sweeps.
[00:31:06.620 --> 00:31:11.180]   So you can use sweeps to automate your hyperparameter
[00:31:11.180 --> 00:31:11.980]   tuning.
[00:31:11.980 --> 00:31:16.700]   And so this is the dashboard that is being created.
[00:31:16.700 --> 00:31:19.980]   I'll quickly show how you can create such dashboards.
[00:31:19.980 --> 00:31:23.060]   So first, you have to define a sweep config.
[00:31:23.060 --> 00:31:24.780]   So sweep config is--
[00:31:24.780 --> 00:31:26.900]   you can define it in a dictionary format
[00:31:26.900 --> 00:31:30.660]   or even in YAML files if you are running through scripts.
[00:31:30.660 --> 00:31:33.340]   So you have to define the method that you
[00:31:33.340 --> 00:31:35.380]   are going to use for sweeps.
[00:31:35.380 --> 00:31:37.220]   So here, I am using random method.
[00:31:37.220 --> 00:31:39.180]   But weights and biases also provide
[00:31:39.180 --> 00:31:41.100]   grid and Bayesian search.
[00:31:41.100 --> 00:31:43.860]   Then all the parameters and all the distribution
[00:31:43.860 --> 00:31:47.380]   from where the values of the parameters
[00:31:47.380 --> 00:31:48.820]   should be sampled from.
[00:31:48.820 --> 00:31:53.300]   So if you have any non-changing parameter,
[00:31:53.300 --> 00:31:55.620]   you can just denote it by values.
[00:31:55.620 --> 00:31:58.220]   And for example, here, I'm saying
[00:31:58.220 --> 00:32:01.140]   batch size should come from a categorical distribution.
[00:32:01.140 --> 00:32:05.060]   And these are the values that the batch size should take.
[00:32:05.060 --> 00:32:08.900]   So similarly, you can look up more about the distributions.
[00:32:08.900 --> 00:32:11.420]   There are many distributions available on the documentation,
[00:32:11.420 --> 00:32:15.060]   like log uniform, end uniform, et cetera.
[00:32:15.060 --> 00:32:17.260]   Then we define the sweep config.
[00:32:17.260 --> 00:32:19.900]   And then we just define a single function
[00:32:19.900 --> 00:32:21.740]   where all the training will happen.
[00:32:21.740 --> 00:32:27.700]   And we just initialize the agent using 1b.sweep command.
[00:32:27.700 --> 00:32:30.540]   And it creates a sweep ID.
[00:32:30.540 --> 00:32:33.420]   It is a unique ID per sweep you perform.
[00:32:33.420 --> 00:32:37.060]   And then you can just automate the whole process
[00:32:37.060 --> 00:32:40.340]   of hyperparameter tuning by calling the 1b.agent command
[00:32:40.340 --> 00:32:42.940]   and giving it the sweep ID.
[00:32:42.940 --> 00:32:45.300]   The interesting thing about is, suppose
[00:32:45.300 --> 00:32:49.220]   if you are having four machines, and you want to perform--
[00:32:49.220 --> 00:32:54.220]   like, you want to perform the same sweep on all the four
[00:32:54.220 --> 00:32:55.100]   machines.
[00:32:55.100 --> 00:32:58.420]   So what you can do is take the same sweep ID
[00:32:58.420 --> 00:33:01.460]   and run the sweep on all four machines.
[00:33:01.460 --> 00:33:04.580]   And all the four machines' sweep configurations
[00:33:04.580 --> 00:33:07.980]   will be synced on the agent.
[00:33:07.980 --> 00:33:10.340]   Weights and Biases agent will sync
[00:33:10.340 --> 00:33:13.620]   all of the communications between all those four
[00:33:13.620 --> 00:33:14.120]   servers.
[00:33:14.120 --> 00:33:17.660]   So you can scale up this hyperparameter tuning
[00:33:17.660 --> 00:33:20.420]   to as many machines as you like.
[00:33:20.420 --> 00:33:23.980]   So that's the interesting thing of it.
[00:33:23.980 --> 00:33:30.820]   And so after running the sweep, you can see this dashboard.
[00:33:30.820 --> 00:33:34.500]   So for each sweep, you can have a single dashboard.
[00:33:34.500 --> 00:33:38.580]   And this chart shows how the runs were,
[00:33:38.580 --> 00:33:41.940]   how the accuracy was there on a particular timestamp.
[00:33:41.940 --> 00:33:44.780]   So we can see how the accuracy developed over the timestamp,
[00:33:44.780 --> 00:33:46.180]   for each timestamp.
[00:33:46.180 --> 00:33:48.620]   Then this is a parallel coordinates chart.
[00:33:48.620 --> 00:33:51.980]   So each line in this parallel coordinate chart
[00:33:51.980 --> 00:33:54.380]   denotes what was the configuration you saw.
[00:33:54.380 --> 00:33:58.140]   For example, for the highlighted one, it was batch size 24.
[00:33:58.140 --> 00:34:01.740]   Then learning rate of 0.0001 and so forth.
[00:34:01.740 --> 00:34:04.700]   And the final metric value, which are interested in.
[00:34:04.700 --> 00:34:07.540]   And this parameter important chart
[00:34:07.540 --> 00:34:09.140]   is basically a random forest model
[00:34:09.140 --> 00:34:12.700]   which weights and biases trains inside the browser
[00:34:12.700 --> 00:34:15.140]   to tell which parameters are having
[00:34:15.140 --> 00:34:17.500]   the most positive or negative impact
[00:34:17.500 --> 00:34:20.940]   on the metric of your choice.
[00:34:20.940 --> 00:34:23.660]   So you can see that here, the batch size
[00:34:23.660 --> 00:34:26.620]   is negatively correlated.
[00:34:26.620 --> 00:34:29.900]   So probably we should use lower batch sizes
[00:34:29.900 --> 00:34:31.700]   to get higher accuracy.
[00:34:31.700 --> 00:34:35.220]   And now the interesting thing is that you can even
[00:34:35.220 --> 00:34:37.220]   select a subset of runs to analyze.
[00:34:37.220 --> 00:34:41.620]   And this is updated dynamically on these only
[00:34:41.620 --> 00:34:42.980]   the selected runs.
[00:34:42.980 --> 00:34:45.660]   So that's a pretty good feature.
[00:34:45.660 --> 00:34:48.860]   As you can see that for this parameter important chart
[00:34:48.860 --> 00:34:50.980]   is only for these runs which are being
[00:34:50.980 --> 00:34:52.660]   selected inside this box.
[00:34:52.660 --> 00:34:55.500]   You can analyze whatever the range of runs
[00:34:55.500 --> 00:34:56.940]   you want to analyze.
[00:34:56.940 --> 00:35:00.780]   And so yeah, that's it from me.
[00:35:00.780 --> 00:35:02.620]   This concludes today's community panel
[00:35:02.620 --> 00:35:04.780]   focused on built with WMB.
[00:35:04.780 --> 00:35:07.300]   A big thank you to both Andrada and Atharva
[00:35:07.300 --> 00:35:08.700]   for presenting their use cases.
[00:35:08.700 --> 00:35:12.060]   [MUSIC PLAYING]
[00:35:12.060 --> 00:35:14.640]   (upbeat music)

