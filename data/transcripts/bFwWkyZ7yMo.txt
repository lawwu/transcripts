
[00:00:00.000 --> 00:00:04.400]   I think we're just at the beginning of actually understanding what a full multimodal model
[00:00:04.400 --> 00:00:08.680]   system, how exciting that might be to interact with.
[00:00:08.680 --> 00:00:12.480]   And it'll be quite different to I think what we're used to today with the chatbots.
[00:00:12.480 --> 00:00:17.640]   I think the next versions of this over in the next year, 18 months, you know, maybe
[00:00:17.640 --> 00:00:21.520]   we'll have some contextual understanding around the environment around you through a camera
[00:00:21.520 --> 00:00:23.920]   or whatever it is, a phone.
[00:00:23.920 --> 00:00:27.760]   You know, I could imagine that as the next awesome glasses or the next step.
[00:00:27.760 --> 00:00:33.840]   And then I think that we'll start becoming more fluid in understanding, oh, let's sample
[00:00:33.840 --> 00:00:34.840]   from a video.
[00:00:34.840 --> 00:00:38.280]   Let's use voice.
[00:00:38.280 --> 00:00:41.800]   Maybe even eventually things like touch and, you know, if you think about robotics and
[00:00:41.800 --> 00:00:45.280]   other things, you know, sensors, other types of sensors.
[00:00:45.280 --> 00:00:49.960]   As these systems and things like Gemini are becoming more multimodal and we start ingesting
[00:00:49.960 --> 00:00:56.520]   things like video and, you know, audio visual data as well as text data, and then, you know,
[00:00:56.520 --> 00:00:59.840]   the system starts correlating those things together.
[00:00:59.840 --> 00:01:04.600]   I think that is a form of proper grounding, actually.
[00:01:04.600 --> 00:01:09.400]   So I do think our systems are going to start to understand, you know, the physics of the
[00:01:09.400 --> 00:01:15.760]   real world better, where you're starting to learn about what your actions do in the world
[00:01:15.760 --> 00:01:21.880]   and how that affects the world itself, but also what next learning episode you're getting.
[00:01:21.880 --> 00:01:25.560]   So, you know, these RL agents we've always been working on and pioneered like AlphaZero
[00:01:25.560 --> 00:01:27.800]   and AlphaGo, they're active learners.
[00:01:27.800 --> 00:01:32.960]   What they decide to do next affects what the next learning piece of data or experience
[00:01:32.960 --> 00:01:34.120]   they're going to get.
[00:01:34.120 --> 00:01:35.720]   So there's this very interesting sort of feedback loop.
[00:01:35.720 --> 00:01:38.880]   And of course, if we ever want to be good at things like robotics, we're going to have
[00:01:38.880 --> 00:01:41.520]   to understand how to act in the real world.
[00:01:41.520 --> 00:01:45.000]   On the robotics subject, Ilya said when he was on the podcast that the reason OpenAI
[00:01:45.000 --> 00:01:48.320]   gave up on robotics was because they didn't have enough data in that domain, at least
[00:01:48.320 --> 00:01:50.600]   at the time they were pursuing it.
[00:01:50.600 --> 00:01:54.180]   I mean, you guys have put out different things like RoboTransformer and other things.
[00:01:54.180 --> 00:01:56.440]   Do you think that's still a bottleneck for robotics progress?
[00:01:56.440 --> 00:02:00.360]   So Ilya is right that that is more challenging because of the data problem.
[00:02:00.360 --> 00:02:05.100]   But it's also I think we're starting to see the beginnings of these large models being
[00:02:05.100 --> 00:02:10.440]   transferable to the robotics regime, learning in the general domain, language domain and
[00:02:10.440 --> 00:02:11.440]   other things.
[00:02:11.440 --> 00:02:15.760]   And then just treating tokens like Gato as any type of token, you know, the token could
[00:02:15.760 --> 00:02:19.520]   be an action, it could be a word, it could be part of an image, a pixel or whatever it
[00:02:19.520 --> 00:02:20.520]   is.
[00:02:20.520 --> 00:02:22.740]   And that's what I think true multimodality is.
[00:02:22.740 --> 00:02:27.440]   And to begin with, it's harder to train a system like that than a straightforward text
[00:02:27.440 --> 00:02:29.520]   language system.
[00:02:29.520 --> 00:02:34.040]   But actually, you know, going back to our earlier conversation of transfer learning,
[00:02:34.040 --> 00:02:40.480]   you start seeing that a true multimodal system, the other modalities benefit some different
[00:02:40.480 --> 00:02:41.480]   modalities.
[00:02:41.480 --> 00:02:45.120]   So you get better at language because you now understand a little bit about video.
[00:02:45.120 --> 00:02:51.360]   So I do think it's harder to get going, but actually ultimately, we'll have a more general,
[00:02:51.360 --> 00:02:52.960]   capable system like that.

