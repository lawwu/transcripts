
[00:00:00.000 --> 00:00:03.580]   I feel like these scaling laws have been very predictable, but then when you say
[00:00:03.580 --> 00:00:07.140]   like, well, you know, when, when is there going to be a commercial explosion in
[00:00:07.140 --> 00:00:10.560]   these models or what's the form it's going to be, or are the models going to
[00:00:10.560 --> 00:00:15.060]   do things instead of humans or pairing with humans, I feel like certainly my
[00:00:15.060 --> 00:00:19.420]   track record on predicting these things is terrible, but I also looking around,
[00:00:19.420 --> 00:00:21.500]   I don't really see anyone whose track record is great.
[00:00:21.500 --> 00:00:25.140]   I've been right about some things, but I've still, you know, with these
[00:00:25.140 --> 00:00:29.220]   theoretical pictures ahead, been wrong about most things, being right about 10%
[00:00:29.220 --> 00:00:33.500]   of the stuff is, you know, it sets you head and shoulders above, above, above,
[00:00:33.500 --> 00:00:34.580]   above many people.
[00:00:34.580 --> 00:00:38.980]   You know, if you look back to, I can't remember who it was kind of, you know,
[00:00:38.980 --> 00:00:43.020]   made these diagrams that are like, you know, here's, here's the village idiot.
[00:00:43.020 --> 00:00:44.140]   Here's Einstein.
[00:00:44.140 --> 00:00:45.880]   Here's the scale of intelligence, right?
[00:00:45.880 --> 00:00:50.420]   And the village idiot and Einstein are like very close to each other like that.
[00:00:50.420 --> 00:00:54.420]   Maybe that's still true in some abstract sense or something, but it's, it's not
[00:00:54.420 --> 00:00:58.500]   really what we're seeing, is it we're seeing like, that it seems like the human
[00:00:58.500 --> 00:01:05.180]   range is pretty broad and doesn't, we don't hit the human range in the same
[00:01:05.180 --> 00:01:08.020]   place or at the same time for different tasks, right?
[00:01:08.020 --> 00:01:11.820]   Like, you know, like write, write a sonnet, you know, in the style of
[00:01:11.820 --> 00:01:15.540]   Cormac McCarthy or something like, I don't know, I'm not very creative, so I
[00:01:15.540 --> 00:01:18.900]   couldn't do that, but like, you know, that's, that's a pretty high level human
[00:01:18.900 --> 00:01:19.340]   skill.
[00:01:19.340 --> 00:01:19.820]   Right.
[00:01:19.820 --> 00:01:23.420]   Um, and even the model is starting to get good at stuff of, you know, like
[00:01:23.420 --> 00:01:27.100]   constrained writing, you know, there's like write a, you know, write a page
[00:01:27.100 --> 00:01:29.900]   without using the letter E or something like write a page about X without using
[00:01:29.900 --> 00:01:30.500]   the letter E.
[00:01:30.500 --> 00:01:34.620]   Like, I think the models might be like superhuman or close to superhuman at
[00:01:34.620 --> 00:01:35.060]   that.
[00:01:35.060 --> 00:01:40.780]   Um, but when it comes to, you know, I, yeah, I don't know, prove relatively
[00:01:40.780 --> 00:01:45.620]   simple mathematical theorems, like they're, they're just starting to do the
[00:01:45.620 --> 00:01:46.460]   beginning of it.
[00:01:46.460 --> 00:01:49.500]   They make really dumb mistakes sometimes.
[00:01:49.500 --> 00:01:55.180]   And they, they really lack any kind of broad, like, you know, correcting your
[00:01:55.180 --> 00:01:57.620]   errors or doing some extended task.
[00:01:57.620 --> 00:02:02.900]   And so I don't know, it turns out that intelligence isn't, isn't a spectrum.
[00:02:02.900 --> 00:02:05.860]   There are a bunch of different areas of domain expertise.
[00:02:05.860 --> 00:02:09.300]   There are a bunch of different like kinds of skills, like memory is different.
[00:02:09.300 --> 00:02:11.700]   I mean, it's all, it's all formed in the blob.
[00:02:11.700 --> 00:02:13.740]   It's not, it's all formed in the blob.
[00:02:13.740 --> 00:02:17.700]   It's not complicated, but to the extent it even is on the spectrum, the spectrum
[00:02:17.700 --> 00:02:18.500]   is also wide.
[00:02:18.500 --> 00:02:21.780]   If you asked me 10 years ago, that's not what I would have expected at all.
[00:02:21.780 --> 00:02:24.500]   But, uh, I think that's very much the way it's turned out.
[00:02:24.540 --> 00:02:28.740]   One thing that's been surprising is like, I thought things might click into
[00:02:28.740 --> 00:02:30.180]   place a little more than they do.
[00:02:30.180 --> 00:02:34.340]   Like, you know, I thought like different cognitive abilities might all be
[00:02:34.340 --> 00:02:38.220]   connected and there was more of one secret behind them, but it's, it's like.
[00:02:38.220 --> 00:02:41.820]   The model just learns various things at different times, you know, and it can
[00:02:41.820 --> 00:02:45.580]   be like very good at coding, but like, you know, it can't, it can't quite, you
[00:02:45.580 --> 00:02:47.180]   know, prove the prime number theorem yet.
[00:02:47.180 --> 00:02:50.980]   And I don't, I mean, I guess it's a little bit the same for, for humans,
[00:02:50.980 --> 00:02:54.260]   although it's, it's weird, the juxtaposition of things that can do and
[00:02:54.260 --> 00:02:58.700]   not, I guess the main lesson is like having theories of intelligence
[00:02:58.700 --> 00:03:00.020]   or how intelligence works.
[00:03:00.020 --> 00:03:04.620]   Like a lot of these words just, just kind of like dissolve into a continuum, right?
[00:03:04.620 --> 00:03:08.900]   They, they just kind of like dematerialize, I think less in terms
[00:03:08.900 --> 00:03:11.620]   of intelligence and more in terms of what we see in front of us.
[00:03:11.620 --> 00:03:17.860]   If you told me in 2018, we'll have models in 2023, like Law 2 that can write
[00:03:17.860 --> 00:03:21.500]   theorems in the style of Shakespeare, whatever theory you want, you want, they
[00:03:21.500 --> 00:03:27.660]   can a standardized tests with open-ended questions, you know, just all kinds of
[00:03:27.660 --> 00:03:31.300]   really impressive things you would have said at that time, I would have said,
[00:03:31.300 --> 00:03:33.980]   Oh, you have AGI, you clearly have something that is a human level
[00:03:33.980 --> 00:03:37.740]   intelligence where these, while these things are impressive, it clearly seems
[00:03:37.740 --> 00:03:41.420]   we're not at human level, at least in the current generation and potentially
[00:03:41.420 --> 00:03:42.860]   for generations to come.
[00:03:42.860 --> 00:03:47.540]   What explains this discrepancy between super impressive performance in these
[00:03:47.540 --> 00:03:51.820]   benchmarks and in just like the things you could describe versus, yeah, generally.
[00:03:51.820 --> 00:03:54.300]   So that, that was one area where actually I was not prescient
[00:03:54.300 --> 00:03:55.380]   and I was surprised as well.
[00:03:55.380 --> 00:03:55.780]   Yeah.
[00:03:55.780 --> 00:04:00.780]   Um, so when I first looked at GPT-3 and, you know, more so the kind of things
[00:04:00.780 --> 00:04:06.140]   that we built in the early days at, at Anthropic, my, my general sense was I,
[00:04:06.140 --> 00:04:09.300]   you know, I looked at these and I'm like, it seems like they, they really
[00:04:09.300 --> 00:04:11.540]   grasped the essence of language.
[00:04:11.540 --> 00:04:13.700]   I'm not sure how much we need to scale them up.
[00:04:13.700 --> 00:04:18.300]   Like maybe we, maybe what's, what's more needed from here is like RL and all, and
[00:04:18.300 --> 00:04:22.020]   kind of, and kind of all the other stuff, like we might be kind of near the, you
[00:04:22.020 --> 00:04:26.580]   know, I thought in 2020, like we can scale this a bunch more, but I wonder if it's
[00:04:26.580 --> 00:04:30.100]   more efficient to scale it more or to start adding on these other objectives
[00:04:30.100 --> 00:04:35.740]   like, like RL, I thought maybe if you do as much RL as, you know, as, as you've
[00:04:35.740 --> 00:04:41.260]   done pre-training for a, for a, you know, 2020 style model that that's, that's the
[00:04:41.260 --> 00:04:45.020]   way to go and scaling it up, we'll keep working, but you know, is that, is that
[00:04:45.020 --> 00:04:46.140]   really the best path?
[00:04:46.140 --> 00:04:49.740]   And I, I think it, I don't know, it just keeps going.
[00:04:49.740 --> 00:04:54.220]   Like I thought it had understood a lot of the essence of language, but then, you
[00:04:54.220 --> 00:04:56.780]   know, there's, there's kind of, there's kind of further to go.
[00:04:56.780 --> 00:05:01.460]   The models are maybe two to three orders of magnitude smaller than the human brain.
[00:05:01.460 --> 00:05:05.060]   If you compare it to the number of synapses, while at the same time being
[00:05:05.060 --> 00:05:09.580]   trained on, you know, three to four more orders of magnitude of data, if you
[00:05:09.580 --> 00:05:15.060]   compare to, you know, number of words, human, a human sees as they're developing
[00:05:15.060 --> 00:05:19.500]   to age 18, we have to admit that that's a weird thing that doesn't match up.
[00:05:19.500 --> 00:05:22.380]   And, you know, it's one reason I'm a bit, you know, skeptical of
[00:05:22.380 --> 00:05:23.820]   kind of biological analogies.
[00:05:23.820 --> 00:05:28.020]   I thought in terms of them like five or six years ago, but now that we actually
[00:05:28.020 --> 00:05:31.700]   have these models in front of us as artifacts, it feels like almost all the
[00:05:31.700 --> 00:05:35.660]   evidence from that has been screened off by what we've seen and what we've seen
[00:05:35.660 --> 00:05:39.460]   are models that are much smaller than the human brain and yet, yet can do a lot of
[00:05:39.460 --> 00:05:43.460]   the things that humans can do and yet paradoxically require a lot more data.
[00:05:43.460 --> 00:05:47.740]   Um, so maybe we'll discover something that makes it all efficient, or maybe
[00:05:47.740 --> 00:05:51.940]   we'll understand why the discrepancy is present, but at the end of the day, I
[00:05:51.940 --> 00:05:53.100]   don't think it matters, right?
[00:05:53.100 --> 00:05:56.980]   If we keep scaling the way we are, I think what's more relevant at this point
[00:05:56.980 --> 00:06:00.660]   is just measuring the abilities of the model and seeing how far they are from
[00:06:00.660 --> 00:06:02.580]   humans and they don't seem terribly far to me.
[00:06:02.580 --> 00:06:05.620]   What do you make of the fact that these things have basically the entire
[00:06:05.620 --> 00:06:07.340]   corpus of human knowledge memorized.
[00:06:07.660 --> 00:06:11.260]   And as far as I'm aware, they haven't been able to make like a single new
[00:06:11.260 --> 00:06:13.220]   connection that has led to a discovery.
[00:06:13.220 --> 00:06:17.060]   Whereas if even a moderately intelligent person had this much stuff memorized,
[00:06:17.060 --> 00:06:19.740]   they'd notice, Oh, this thing causes this symptom.
[00:06:19.740 --> 00:06:21.420]   This other thing also causes the symptom.
[00:06:21.420 --> 00:06:23.260]   You know, there's a medical cure right here.
[00:06:23.260 --> 00:06:23.620]   Right.
[00:06:23.620 --> 00:06:25.580]   What, what, what, what shouldn't we be expecting that kind of stuff?
[00:06:25.580 --> 00:06:27.020]   I'm not, I'm not sure.
[00:06:27.020 --> 00:06:31.740]   I mean, I think, you know, I don't know these words, discovery, creativity.
[00:06:31.740 --> 00:06:35.220]   Like it's one of the lessons I've learned is that in, in, you know, in
[00:06:35.220 --> 00:06:39.300]   kind of the big blob of compute, often these, these ideas often end up being
[00:06:39.300 --> 00:06:43.580]   kind of fuzzy and elusive and hard to track down, but I think, I think there
[00:06:43.580 --> 00:06:45.780]   is something here, which is.
[00:06:45.780 --> 00:06:50.340]   I think the models do display a kind of ordinary creativity again, again, you
[00:06:50.340 --> 00:06:53.460]   know, the kind of like, you know, write a, write a sonnet, you know, in the
[00:06:53.460 --> 00:06:57.820]   style of Cormac McCarthy or Barbie or so, you know, like there is some
[00:06:57.820 --> 00:07:02.140]   creativity to that and I think they do draw, you know, new connections of the
[00:07:02.140 --> 00:07:03.940]   kind that an ordinary person would draw.
[00:07:04.180 --> 00:07:07.580]   I, I agree with you that there haven't been any kind of like, I don't know,
[00:07:07.580 --> 00:07:12.340]   like I would say like big scientific discoveries, I think that's a mix of
[00:07:12.340 --> 00:07:17.380]   like just the model skill level is not, is not high enough yet that I think is
[00:07:17.380 --> 00:07:19.100]   going to change with the, with the scaling.
[00:07:19.100 --> 00:07:22.580]   I do think there's an interesting point about, well, the models have an
[00:07:22.580 --> 00:07:26.700]   advantage, which is they know a lot more than us, you know, like should, should
[00:07:26.700 --> 00:07:27.980]   they have an advantage already?
[00:07:27.980 --> 00:07:31.300]   Even if, even if they, their skill level isn't, isn't, isn't quite high.
[00:07:31.300 --> 00:07:33.100]   Maybe that's kind of what you're getting at.
[00:07:33.580 --> 00:07:35.060]   I don't really have an answer to that.
[00:07:35.060 --> 00:07:39.140]   I mean, it seems certainly like memorization and facts and drawing
[00:07:39.140 --> 00:07:41.220]   connections is an area where the models are ahead.
[00:07:41.220 --> 00:07:46.300]   And I, I do think maybe you need those connections and you need
[00:07:46.300 --> 00:07:48.020]   a fairly high level of skill.
[00:07:48.020 --> 00:07:52.620]   I do think particularly in the area of biology for better and for worse, the
[00:07:52.620 --> 00:07:59.020]   complexity of biology is such that the current models know a lot of things
[00:07:59.020 --> 00:08:02.180]   right now, and that's what, that's what you need to make discoveries and draw.
[00:08:02.180 --> 00:08:05.100]   It's not like physics where you need to, you know, you need to think and
[00:08:05.100 --> 00:08:06.300]   come up with a formula in biology.
[00:08:06.300 --> 00:08:07.420]   You need to know a lot of things.
[00:08:07.420 --> 00:08:07.860]   Right.
[00:08:07.860 --> 00:08:11.660]   And so I do think the models know a lot of things and they have a skill level
[00:08:11.660 --> 00:08:13.500]   that's not quite high enough to put them together.
[00:08:13.500 --> 00:08:16.700]   And I think they are, they are just on the cusp of being able
[00:08:16.700 --> 00:08:17.660]   to put these things together.

