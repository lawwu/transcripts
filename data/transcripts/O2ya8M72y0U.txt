
[00:00:00.000 --> 00:00:11.960]   It's not that AI is uniquely dangerous in this respect or uniquely likely to lead to
[00:00:11.960 --> 00:00:16.960]   harmful unbridled competition, but it's more that we have already gone through the lessons
[00:00:16.960 --> 00:00:19.680]   of other technologies.
[00:00:19.680 --> 00:00:24.160]   With the case of cars, it took decades of gradually ratcheting up standards around fuel
[00:00:24.160 --> 00:00:28.400]   efficiency and seat belts and airbags and so forth.
[00:00:28.400 --> 00:00:31.520]   We're in the early stages of that process for AI.
[00:00:31.520 --> 00:00:35.360]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:35.360 --> 00:00:37.000]   models work in the real world.
[00:00:37.000 --> 00:00:39.320]   I'm your host, Lukas Biewald.
[00:00:39.320 --> 00:00:43.480]   Miles researches the societal impacts of artificial intelligence and how to make sure they go
[00:00:43.480 --> 00:00:44.480]   well.
[00:00:44.480 --> 00:00:48.320]   In 2018, he joined OpenAI as a research scientist on the policy team.
[00:00:48.320 --> 00:00:53.020]   Previously, he was a research fellow at the University of Oxford's Future for Humanity
[00:00:53.020 --> 00:00:58.160]   Institute and served as a member of Axon AI's Policy Technology Ethics Board.
[00:00:58.160 --> 00:01:00.200]   I'm super excited to talk to him.
[00:01:00.200 --> 00:01:04.560]   So I was looking at your background and it seemed like you had an interesting path into
[00:01:04.560 --> 00:01:05.560]   AI policy, right?
[00:01:05.560 --> 00:01:06.560]   Mm-hmm (affirmative).
[00:01:06.560 --> 00:01:10.240]   But were you originally working on energy and climate issues?
[00:01:10.240 --> 00:01:11.800]   Do I have that right?
[00:01:11.800 --> 00:01:12.800]   Yeah, exactly.
[00:01:12.800 --> 00:01:13.800]   Yeah.
[00:01:13.800 --> 00:01:18.440]   So in undergrad in DC and then for a little while after I graduated, I was working at
[00:01:18.440 --> 00:01:24.560]   the Department of Energy and other think tanks and stuff in the broad area of energy and
[00:01:24.560 --> 00:01:25.560]   climate policy.
[00:01:25.560 --> 00:01:30.640]   And I ended up wanting to go to grad school because I wanted to do more research and less
[00:01:30.640 --> 00:01:37.480]   administrative and support for boss support type stuff.
[00:01:37.480 --> 00:01:40.760]   I was like a special assistant was my role.
[00:01:40.760 --> 00:01:48.040]   And I learned a ton from being in government and working on something that was super hot
[00:01:48.040 --> 00:01:49.040]   at the time.
[00:01:49.040 --> 00:01:51.520]   Like energy and climate policy was a big, big deal.
[00:01:51.520 --> 00:01:55.480]   People are talking about it way more than AI or anything like that back in the day.
[00:01:55.480 --> 00:01:57.760]   And it was super enlightening.
[00:01:57.760 --> 00:02:02.840]   But I also eventually concluded that while energy was important, there wasn't as much
[00:02:02.840 --> 00:02:07.000]   sort of to be done in terms of research to move things forward.
[00:02:07.000 --> 00:02:13.400]   And AI was this sort of more fresh green field in terms of research and that it wasn't just
[00:02:13.400 --> 00:02:19.000]   a political problem of getting the right thing to happen, which I think is arguably the case
[00:02:19.000 --> 00:02:23.840]   with energy and climate change now where we have some good understanding of the policy
[00:02:23.840 --> 00:02:27.480]   issues, not everything, but we have a decent understanding of what needs to be done.
[00:02:27.480 --> 00:02:31.520]   And it's largely a matter of political will, whereas in AI, it's not even clear what we
[00:02:31.520 --> 00:02:36.240]   want and what should be done, let alone how to get the political will.
[00:02:36.240 --> 00:02:40.600]   So that felt like a more exciting opportunity for impact on the research side.
[00:02:40.600 --> 00:02:47.240]   I'm curious, having kind of thought deeply about both, I guess, which do you think is
[00:02:47.240 --> 00:02:52.000]   more of a threat to the future of humanity, climate change or AI?
[00:02:52.000 --> 00:02:53.560]   Which worries you more?
[00:02:53.560 --> 00:03:00.560]   Ooh, I definitely in the near term, I don't think AI is going to sort of end the human
[00:03:00.560 --> 00:03:03.160]   race or anything like that.
[00:03:03.160 --> 00:03:06.800]   It's still at a relatively early stage, but we do need to think about the long term risks
[00:03:06.800 --> 00:03:13.160]   of technologies as they're developed, whereas energy is something that's a clear and present
[00:03:13.160 --> 00:03:18.000]   danger right now, and I'm hopeful that AI will actually help with addressing it.
[00:03:18.000 --> 00:03:24.280]   But yeah, I think energy is sort of a, it's a sort of known long term time scale issue
[00:03:24.280 --> 00:03:28.760]   where we know that there's a bunch of things that need to change in order to get on a good
[00:03:28.760 --> 00:03:32.880]   trajectory, whereas AI is sort of, we know that we're not in sort of imminent danger
[00:03:32.880 --> 00:03:38.560]   of all jobs being automated or killer AI taking over the world or something, but we see these
[00:03:38.560 --> 00:03:42.360]   sort of trends and they're more uncertain in how quickly they'll develop.
[00:03:42.360 --> 00:03:49.120]   So I think we're actually, despite all the uncertainties with climate change and energy
[00:03:49.120 --> 00:03:53.200]   technology innovation where we don't really know what's going to happen in a few decades,
[00:03:53.200 --> 00:03:58.000]   we actually have more reasonable error bars for that than we do for AI, where it's like
[00:03:58.000 --> 00:04:01.880]   experts are all over the place in terms of thinking things will be solved in the near
[00:04:01.880 --> 00:04:03.720]   future or taking centuries.
[00:04:03.720 --> 00:04:09.600]   And I think it's actually harder to be very confident about AI than energy and climate
[00:04:09.600 --> 00:04:10.600]   change.
[00:04:10.600 --> 00:04:16.720]   Well, I guess, where do you kind of, like when you say sort of like media or near term,
[00:04:16.720 --> 00:04:22.560]   medium term, do you feel like there's like a serious possible issue with AI in the next
[00:04:22.560 --> 00:04:25.720]   say 50 years?
[00:04:25.720 --> 00:04:26.720]   It's really hard to say.
[00:04:26.720 --> 00:04:33.160]   I think we should try not to be too confident in where the trends are going to go.
[00:04:33.160 --> 00:04:37.440]   Like just in the past few years, there's been a lot of technological progress that people
[00:04:37.440 --> 00:04:38.440]   have found surprising.
[00:04:38.440 --> 00:04:42.760]   Like, you know, very few people were sort of expecting as much progress in things like
[00:04:42.760 --> 00:04:47.200]   machine translation and natural language generation as ended up happening.
[00:04:47.200 --> 00:04:51.640]   And similarly, things like image recognition have just like, you know, gone from like,
[00:04:51.640 --> 00:04:55.480]   okay, it's like in the human performance, you know, ballpark to like, okay, now it's
[00:04:55.480 --> 00:04:57.400]   superhuman, superhuman.
[00:04:57.400 --> 00:05:01.600]   In some metrics, it's not superhuman across the board, but you know, on sort of well-defined
[00:05:01.600 --> 00:05:04.880]   tasks, we've made a ton of progress.
[00:05:04.880 --> 00:05:08.600]   Will that lead to, you know, at some point systems that are harder to control or that
[00:05:08.600 --> 00:05:13.080]   might sort of, you know, cause unintended side effects and, you know, increasingly large
[00:05:13.080 --> 00:05:15.960]   sort of context where they're deployed?
[00:05:15.960 --> 00:05:17.120]   I think it's hard to say.
[00:05:17.120 --> 00:05:21.080]   I think we should think of this as something that, you know, where the stakes are going
[00:05:21.080 --> 00:05:26.680]   to rise, both as the capabilities increase and also as the sort of use cases grow.
[00:05:26.680 --> 00:05:32.760]   So even if AI sort of doesn't progress much in terms of capabilities, it will, it seems
[00:05:32.760 --> 00:05:36.920]   like there's economic pressure and other sorts of reasons why it's going to be deployed in
[00:05:36.920 --> 00:05:37.920]   more context.
[00:05:37.920 --> 00:05:41.760]   And as those sort of contexts get more dangerous, like the stakes are going to rise.
[00:05:41.760 --> 00:05:47.600]   So we're already seeing AI applied for things like, you know, predictive policing and face
[00:05:47.600 --> 00:05:52.560]   recognition and sort of things that are, you know, highly sensitive and also things like
[00:05:52.560 --> 00:05:56.940]   search engines and recommendation systems that are actually, you know, materially affecting
[00:05:56.940 --> 00:06:02.200]   what information people get and, you know, what products and services they use.
[00:06:02.200 --> 00:06:06.480]   It's definitely, you know, the sort of thing that, you know, the stakes are getting, are
[00:06:06.480 --> 00:06:11.440]   high and getting higher, but, you know, it's hard to say, you know, what the sort of end
[00:06:11.440 --> 00:06:12.760]   point of that evolution will be.
[00:06:12.760 --> 00:06:17.480]   And I think the key is that we, I think the key is to sort of think ahead and be at least
[00:06:17.480 --> 00:06:22.120]   a few years ahead in terms of what sorts of problems we're anticipating so that we're
[00:06:22.120 --> 00:06:23.120]   not caught off guard.
[00:06:23.120 --> 00:06:27.920]   So things like, you know, misuse of AI systems, I think we should not sort of wait until the
[00:06:27.920 --> 00:06:32.400]   risks have materialized, but instead, you know, think about what can we do at the design
[00:06:32.400 --> 00:06:34.920]   phase to make these problems less severe?
[00:06:34.920 --> 00:06:37.400]   Can we develop detection systems, those sorts of things?
[00:06:37.400 --> 00:06:41.360]   So I don't think there's any sort of like inevitable risk.
[00:06:41.360 --> 00:06:45.440]   I think it's more like, what is the, you know, how prepared are we relative to the capabilities
[00:06:45.440 --> 00:06:46.440]   that are coming.
[00:06:46.440 --> 00:06:53.320]   Do you think there's something, I feel like a lot of these things that I hear about sort
[00:06:53.320 --> 00:06:58.120]   of AI, it seems like you almost substitute the word like technology for AI and you'd
[00:06:58.120 --> 00:07:01.480]   kind of come to the same conclusions.
[00:07:01.480 --> 00:07:06.440]   Do you feel like there's something special about AI that like requires sort of more vigilance
[00:07:06.440 --> 00:07:09.680]   or different kinds of ethical concerns?
[00:07:09.680 --> 00:07:15.680]   Yeah, I think it starts to get a bit, you know, when there's more uncertainty and more
[00:07:15.680 --> 00:07:19.800]   sort of variation in the kinds of actions that systems take as opposed to just being,
[00:07:19.800 --> 00:07:24.280]   you know, sort of deterministic code that does the same thing over and over as opposed
[00:07:24.280 --> 00:07:28.440]   to dealing with uncertain inputs and, you know, potentially a shifting distribution
[00:07:28.440 --> 00:07:29.440]   of inputs.
[00:07:29.440 --> 00:07:34.560]   I think the risks can grow when you're sort of moving from software to AI, but definitely
[00:07:34.560 --> 00:07:39.320]   there are all sorts of systems in society that are very complex already, even before
[00:07:39.320 --> 00:07:44.800]   I started thinking, you know, the number of lines of code and, you know, sort of a passenger
[00:07:44.800 --> 00:07:48.400]   aircraft or, you know, it's the same and, you know, it's like super complex systems.
[00:07:48.400 --> 00:07:53.480]   That's not like AI is suddenly going to, you know, lead to this new complexity, but rather
[00:07:53.480 --> 00:07:59.600]   there are sort of elements of AI such as, you know, sort of processing of, you know,
[00:07:59.600 --> 00:08:05.320]   a wide range of inputs and potentially making decisions that, you know, previous software
[00:08:05.320 --> 00:08:11.680]   systems were not entrusted to do because we lacked, you know, sort of […] manipulation
[00:08:11.680 --> 00:08:12.680]   outputs.
[00:08:12.680 --> 00:08:16.040]   So, things like, you know, robots, for example, there are applications of AI that were just,
[00:08:16.040 --> 00:08:20.320]   you know, not possible, you know, if you don't have that technology.
[00:08:20.320 --> 00:08:23.520]   So, it's, you know, I wouldn't draw a clean line between them.
[00:08:23.520 --> 00:08:29.200]   I see, you know, AI as sort of like an extension of information technology and software and,
[00:08:29.200 --> 00:08:32.600]   you know, going back even further, things like electricity and, you know, it's unclear
[00:08:32.600 --> 00:08:37.240]   whether that like cognitive element, like the sort of sense which AI is about thinking
[00:08:37.240 --> 00:08:42.920]   as opposed to just, you know, following sort of rules blindly and, you know, is that actually
[00:08:42.920 --> 00:08:45.880]   a significant element that introduces, you know, a lot of risk.
[00:08:45.880 --> 00:08:52.600]   I don't know, you know, whether we should be sort of AI sort of exceptionalists, like
[00:08:52.600 --> 00:08:57.600]   treating AI like an exception or just treating it as part of this spectrum.
[00:08:57.600 --> 00:09:03.760]   I think there are pros and cons of both because you don't want to sort of ignore the growing
[00:09:03.760 --> 00:09:08.200]   capabilities in cases where AI can do things that previous technologies couldn't do.
[00:09:08.200 --> 00:09:11.440]   I think we're seeing lots of cases of, you know, just products that would never have
[00:09:11.440 --> 00:09:17.280]   existed prior to the current wave of AI, but we also don't want to sort of be hysterical
[00:09:17.280 --> 00:09:22.920]   and over-emphasize the novelty when there are, you know, lots of technologies that have
[00:09:22.920 --> 00:09:23.920]   big impacts.
[00:09:23.920 --> 00:09:30.720]   So, do you have like, I guess, do you have like an agenda of like some types of change
[00:09:30.720 --> 00:09:34.360]   that you want to like enact in the world at this point or are you still sort of trying
[00:09:34.360 --> 00:09:37.800]   to like figure out like what you're trying to get folks to do?
[00:09:37.800 --> 00:09:43.040]   Yeah, I'm still trying to figure out, you know, the overall sort of agenda, but I think
[00:09:43.040 --> 00:09:50.200]   some of the key components are sort of cooperation between AI developers, I think is super essential
[00:09:50.200 --> 00:09:54.880]   in order to figure out, you know, what the right practices should be and to hold one
[00:09:54.880 --> 00:09:55.880]   another accountable.
[00:09:55.880 --> 00:10:00.760]   And, you know, so for example, I did some work with Amanda Askell and Gillian Hadfield
[00:10:00.760 --> 00:10:05.760]   in a paper last year that talks about the need for cooperation in order to avoid a race
[00:10:05.760 --> 00:10:08.600]   to the bottom between AI developers.
[00:10:08.600 --> 00:10:09.600]   And so that's that.
[00:10:09.600 --> 00:10:13.360]   So, insofar as, you know, I have an agenda on that issue, it's avoiding, you know, races
[00:10:13.360 --> 00:10:14.360]   to the bottom and avoiding a scenario.
[00:10:14.360 --> 00:10:15.360]   Wait, what would that mean?
[00:10:15.360 --> 00:10:18.120]   What would a race to the bottom mean in this context?
[00:10:18.120 --> 00:10:24.360]   Yeah, so like driverless cars is perhaps the most clear cut case where there's, you know,
[00:10:24.360 --> 00:10:28.480]   there's an incentive to be safe, but there's also an incentive to get products to market.
[00:10:28.480 --> 00:10:34.440]   And I think there, and you know, in cases where systems are sort of deployed in a way
[00:10:34.440 --> 00:10:38.720]   that's premature in some respect, like not taking into account that there can be jaywalkers,
[00:10:38.720 --> 00:10:44.400]   for example, I think that's a case where, you know, there is a design element of the
[00:10:44.400 --> 00:10:47.680]   system that was not super thought through.
[00:10:47.680 --> 00:10:52.240]   And if you have, you know, people who are sort of overlooking these things and cutting
[00:10:52.240 --> 00:10:56.840]   corners in a rush to get products to market, I think that could cause harm for the whole
[00:10:56.840 --> 00:10:59.880]   ecosystem, not just causing, you know, individual crashes.
[00:10:59.880 --> 00:11:04.440]   But there are cases where sort of, you know, people following their own individual self-interest
[00:11:04.440 --> 00:11:09.800]   to sort of get things out there faster could lead them to, you know, cause harm and ultimately
[00:11:09.800 --> 00:11:12.000]   make the whole sector worse off.
[00:11:12.000 --> 00:11:16.480]   So that's why we need sort of regulations and other, as well as, you know, informal
[00:11:16.480 --> 00:11:20.880]   norms in the AI community that put things like safety and security and privacy and so
[00:11:20.880 --> 00:11:24.040]   forth first so that there's some guardrails, you know.
[00:11:24.040 --> 00:11:26.560]   So there's competition, but within certain guardrails.
[00:11:26.560 --> 00:11:31.480]   There's, you know, unbridled competition, I think could lead to sort of lowering of
[00:11:31.480 --> 00:11:36.360]   the standards and, you know, in an effort to sort of, you know, one up one another.
[00:11:36.360 --> 00:11:41.800]   Do you mean like unbridled competition in terms of like researchers or is that what
[00:11:41.800 --> 00:11:43.280]   you mean in this context or?
[00:11:43.280 --> 00:11:48.240]   Oh, so I specifically mean in terms of sort of products going to market.
[00:11:48.240 --> 00:11:52.800]   So if there's sort of no standards for say, you know, driverless car safety, unfortunately,
[00:11:52.800 --> 00:11:57.120]   I don't think there are any countries that have no standards, you know, that they're,
[00:11:57.120 --> 00:12:01.480]   you know, they're policymakers are aware of this need to impose some sort of guardrails.
[00:12:01.480 --> 00:12:07.000]   But you know, if there's a world in which the standards are insufficient and, you know,
[00:12:07.000 --> 00:12:10.840]   there's a need for and there's insufficient vigilance about, you know, how safety systems
[00:12:10.840 --> 00:12:16.920]   are, then there's not going to be sufficient pressure to make sure that the sector as a
[00:12:16.920 --> 00:12:19.760]   whole moves in the right direction.
[00:12:19.760 --> 00:12:25.040]   So it's sort of a tradeoff between individual interest and collective interest, as well
[00:12:25.040 --> 00:12:28.320]   as like short and long term, that I think could go awry.
[00:12:28.320 --> 00:12:32.400]   And that's why things like ethical principles that keep people thinking about the long term
[00:12:32.400 --> 00:12:36.840]   and sort of clarify what's expected of different actors, and also, you know, regulations that
[00:12:36.840 --> 00:12:41.920]   sort of impose some, you know, some floor on what the level of, you know, level of safety
[00:12:41.920 --> 00:12:46.160]   or security should be for given systems are necessary in order to prevent it just from
[00:12:46.160 --> 00:12:47.440]   being a wild west.
[00:12:47.440 --> 00:12:52.080]   So when I say unbridled competition, I just mean like, in the actual, you know, deployment
[00:12:52.080 --> 00:12:57.000]   side of, you know, there needs to be some sort of process for making sure that, you
[00:12:57.000 --> 00:13:00.440]   know, unsafe systems are deployed.
[00:13:00.440 --> 00:13:06.080]   And I guess like, is it so like, compare maybe like AI, like autonomous vehicles to just
[00:13:06.080 --> 00:13:08.240]   like car, the car industry, right?
[00:13:08.240 --> 00:13:12.400]   Like the car industry has like all kinds of like, safety regulations that make sense.
[00:13:12.400 --> 00:13:20.280]   And like, do you feel that, like AI needs some sort of different kind of cooperation
[00:13:20.280 --> 00:13:24.560]   than other industries maybe need?
[00:13:24.560 --> 00:13:25.560]   I'm not sure.
[00:13:25.560 --> 00:13:27.080]   I'm not sure it needs anything different.
[00:13:27.080 --> 00:13:29.880]   I think it just needs to catch up to where other sectors are.
[00:13:29.880 --> 00:13:35.160]   So you know, there's a whole history of like, nuclear power plants, you know, responsibly
[00:13:35.160 --> 00:13:38.880]   disclosing an incident in order for others to sort of learn from it.
[00:13:38.880 --> 00:13:43.120]   And similarly, there's a whole history of like, you know, airplane crash investigations
[00:13:43.120 --> 00:13:44.520]   and regulations and so forth.
[00:13:44.520 --> 00:13:50.000]   So it's not that driverless cars or AI generally is this sort of new scary thing.
[00:13:50.000 --> 00:13:55.040]   It's more that we need to apply the same sort of, you know, same sort of approach as we
[00:13:55.040 --> 00:13:56.520]   have done in other technologies.
[00:13:56.520 --> 00:14:00.160]   So going back to your question earlier about sort of AI versus other technologies, I think
[00:14:00.160 --> 00:14:02.160]   this is a case where I see them as very similar.
[00:14:02.160 --> 00:14:07.560]   It's not that, you know, AI is uniquely dangerous in this respect, or uniquely likely to lead
[00:14:07.560 --> 00:14:11.120]   to, you know, sort of harmful, unbridled competition.
[00:14:11.120 --> 00:14:15.000]   But it's, you know, it's more that we have already gone through the lessons of other
[00:14:15.000 --> 00:14:19.520]   technologies and, you know, with the case of cars, it took decades of like gradually
[00:14:19.520 --> 00:14:25.400]   ratcheting up standards around fuel efficiency and seatbelts and airbags and so forth.
[00:14:25.400 --> 00:14:28.840]   And we're in the early stages of that process for AI.
[00:14:28.840 --> 00:14:30.360]   Interesting.
[00:14:30.360 --> 00:14:35.720]   Is there like an industry that you think that's doing particularly well on this or like particularly
[00:14:35.720 --> 00:14:36.720]   badly?
[00:14:36.720 --> 00:14:37.720]   Good question.
[00:14:37.720 --> 00:14:41.320]   I think there, I think, you know, driverless cars is not like, you know, to be clear, is
[00:14:41.320 --> 00:14:43.080]   not like an ambiguously bad thing.
[00:14:43.080 --> 00:14:47.200]   I think there's a ton of great work on figuring out how to make, you know, driverless cars
[00:14:47.200 --> 00:14:48.200]   safe.
[00:14:48.200 --> 00:14:52.760]   And there is, you know, some sort of informal cooperation going on among those developers,
[00:14:52.760 --> 00:14:54.360]   though it's like kind of tricky.
[00:14:54.360 --> 00:14:58.680]   And, you know, due to things like antitrust laws, you know, that prevent, you know, too
[00:14:58.680 --> 00:15:02.960]   much coordination, it's not clear exactly, you know, how much is happening and how much
[00:15:02.960 --> 00:15:04.920]   can happen.
[00:15:04.920 --> 00:15:11.200]   Other technologies, so I think, you know, I think there's actually been like a ton of
[00:15:11.200 --> 00:15:17.200]   really good work on things like image recognition and trying to characterize, you know, robustness
[00:15:17.200 --> 00:15:18.200]   in that context.
[00:15:18.200 --> 00:15:23.560]   So, you know, with regard to things like interpretability, we're much further along in terms of, you
[00:15:23.560 --> 00:15:28.680]   know, interpreting, you know, image classifiers than we are, you know, with like language
[00:15:28.680 --> 00:15:29.680]   systems, for example.
[00:15:29.680 --> 00:15:34.040]   So just like, you know, classifiers generally, I would say, are a lot like the more mature
[00:15:34.040 --> 00:15:38.480]   end of the spectrum and that we have, you know, clear sort of metrics for performance,
[00:15:38.480 --> 00:15:42.400]   and there's a lot of open source models to compare things against.
[00:15:42.400 --> 00:15:48.360]   And there's a lot of, you know, sort of documentation of things like biases and, you know, potential,
[00:15:48.360 --> 00:15:49.960]   you know, issues with robustness.
[00:15:49.960 --> 00:15:54.440]   So I'd say that's not to say that like all classifiers are good or something like that,
[00:15:54.440 --> 00:15:57.600]   but I'd say that, you know, if you sort of look across AI, there are areas that are more
[00:15:57.600 --> 00:16:03.960]   and less mature in terms of, you know, the rigor of, you know, the rigor that people,
[00:16:03.960 --> 00:16:06.400]   you know, are applying.
[00:16:06.400 --> 00:16:10.800]   It does seem like, I guess it seems to me like AI does kind of come out of more than
[00:16:10.800 --> 00:16:15.160]   other industries, maybe kind of comes out of like a research background, or maybe there
[00:16:15.160 --> 00:16:19.720]   is like a little bit more of a culture of, you know, cooperation.
[00:16:19.720 --> 00:16:24.000]   Like it does seem sort of notable that so many models that people use in companies kind
[00:16:24.000 --> 00:16:28.680]   of come out of like open source projects or research papers.
[00:16:28.680 --> 00:16:34.040]   So it does seem like there's a fair amount of at least like cooperation on the technology
[00:16:34.040 --> 00:16:38.240]   side, like maybe more than you'd actually expect.
[00:16:38.240 --> 00:16:39.440]   And there is, yeah.
[00:16:39.440 --> 00:16:44.000]   And there's an interesting question of, you know, how long that will last and, you know,
[00:16:44.000 --> 00:16:47.320]   what are the like underlying drivers of that openness?
[00:16:47.320 --> 00:16:51.360]   One argument for, you know, why it might be more open than you might otherwise expect
[00:16:51.360 --> 00:16:57.120]   is just that individual AI researchers want to publish stuff, and therefore that puts
[00:16:57.120 --> 00:17:00.880]   pressure on the companies that hire those people to, you know, allow them to publish
[00:17:00.880 --> 00:17:01.880]   stuff.
[00:17:01.880 --> 00:17:05.640]   And, you know, I think that's like a very strong incentive, and the fact that, you know,
[00:17:05.640 --> 00:17:10.560]   AI researchers and engineers are sort of, you know, a scarce resource in the talent
[00:17:10.560 --> 00:17:15.840]   market, that gives them some influence over what the policies are at these companies.
[00:17:15.840 --> 00:17:19.920]   And there's also another argument for why, you know, it might be so open is that it can
[00:17:19.920 --> 00:17:24.080]   benefit the companies to be open by getting people to use their frameworks, and it can
[00:17:24.080 --> 00:17:26.720]   make it easier to hire people in the future.
[00:17:26.720 --> 00:17:30.520]   But I think, you know, I wouldn't be … I wouldn't sort of, you know, want us to rest
[00:17:30.520 --> 00:17:32.040]   on our laurels here.
[00:17:32.040 --> 00:17:37.600]   And that's why, for example, in a report that my colleagues and I put out recently, we talk
[00:17:37.600 --> 00:17:43.120]   about the need for more generously supporting academics with computing resources, because
[00:17:43.120 --> 00:17:48.640]   we don't necessarily want to just, you know, assume that, you know, sort of those in industry
[00:17:48.640 --> 00:17:52.800]   will continue to, you know, release products, and that, you know, forever there will be
[00:17:52.800 --> 00:17:58.000]   this sort of pressure from below to do these releases, because there could also be cases
[00:17:58.000 --> 00:18:03.200]   where suddenly there's a huge commercial potential that's identified, and, you know, they, you
[00:18:03.200 --> 00:18:04.720]   know, companies sort of close up.
[00:18:04.720 --> 00:18:08.880]   Or at an international level, there's sort of, you know, pressure due to competition
[00:18:08.880 --> 00:18:12.720]   with China to sort of, you know, damp down on publishing or something like that.
[00:18:12.720 --> 00:18:15.840]   There are all sorts of things that could happen longer term.
[00:18:15.840 --> 00:18:20.720]   So I would say, you know, I'm glad that there is so much openness and collaboration today.
[00:18:20.720 --> 00:18:24.160]   But I think, you know, we … given how much has changed in the last five years, I think
[00:18:24.160 --> 00:18:26.880]   we should be thinking about, you know, what are the policies to make sure that it, you
[00:18:26.880 --> 00:18:29.360]   know, continues in a good way.
[00:18:29.360 --> 00:18:33.600]   You're actually talking about, I think, a paper you just recently published, right?
[00:18:33.600 --> 00:18:35.520]   The Towards Trustworthy AI Development?
[00:18:35.520 --> 00:18:36.520]   Yeah, yeah.
[00:18:36.520 --> 00:18:38.720]   I had a couple … I had some questions about that.
[00:18:38.720 --> 00:18:39.720]   Maybe we could jump in there.
[00:18:39.720 --> 00:18:40.720]   Yeah, let's talk about it.
[00:18:40.720 --> 00:18:44.600]   Yeah, I thought it was like … and you were actually the first author of that paper.
[00:18:44.600 --> 00:18:46.640]   I imagine you did a fair amount of heavy lifting or …
[00:18:46.640 --> 00:18:49.640]   Yeah, I was one of five first authors.
[00:18:49.640 --> 00:18:54.040]   So it was like super collaborative, both like at the whole authors list and at the … at
[00:18:54.040 --> 00:18:55.040]   the sort of highest level.
[00:18:55.040 --> 00:18:56.040]   Got it.
[00:18:56.040 --> 00:18:57.040]   Cool.
[00:18:57.040 --> 00:19:02.280]   Well, I guess, you know, one thing I thought … it was kind of a provocative title.
[00:19:02.280 --> 00:19:07.280]   The Mechanism for Supporting Verifiable Claims, I thought was just kind of interesting.
[00:19:07.280 --> 00:19:09.240]   And I think I kind of agree with the importance of it.
[00:19:09.240 --> 00:19:12.000]   But maybe you could sort of say the thinking behind that.
[00:19:12.000 --> 00:19:18.120]   Yeah, so the basic idea in the report is that AI developers are sort of making all sorts
[00:19:18.120 --> 00:19:22.720]   of systems, and they want to sell them, or they want people to use them.
[00:19:22.720 --> 00:19:27.000]   And they sort of make various claims about these systems, like, "We did bias analysis,"
[00:19:27.000 --> 00:19:30.120]   or "They're robust in X, Y, and Z ways."
[00:19:30.120 --> 00:19:34.360]   And there's some claims like that that are relatively easy to verify.
[00:19:34.360 --> 00:19:38.760]   Like if you're in the case where it's like an open source system, you might be able,
[00:19:38.760 --> 00:19:45.560]   if you have technical expertise, to replicate the results, reproduce a certain error rate,
[00:19:45.560 --> 00:19:49.360]   and sort of verify that these AI developers are telling the truth.
[00:19:49.360 --> 00:19:52.000]   But there are other cases where it's not as obvious how to do that.
[00:19:52.000 --> 00:19:56.720]   For example, if it's a very compute-intensive system, it might be harder for academics to
[00:19:56.720 --> 00:19:58.440]   scrutinize it.
[00:19:58.440 --> 00:20:04.560]   Or if there are claims being made about privacy preservation, but it's sort of this new,
[00:20:04.560 --> 00:20:09.440]   fancy approach to privacy-preserving machine learning that there isn't a standard way of
[00:20:09.440 --> 00:20:15.000]   evaluating it, or it's a new library that hasn't been subject to a lot of scrutiny for
[00:20:15.000 --> 00:20:20.840]   bugs, those are cases where it's harder to verify the claims made by AI developers.
[00:20:20.840 --> 00:20:26.440]   So, what we did in this report is try and take a very broad, 30,000-foot view of, "Okay,
[00:20:26.440 --> 00:20:28.000]   what is the problem here?"
[00:20:28.000 --> 00:20:33.320]   And we broke it down into institutions, software, and hardware as the three elements of an AI
[00:20:33.320 --> 00:20:38.840]   app that can contribute to allowing one's claims to be verified.
[00:20:38.840 --> 00:20:42.680]   And then, in each of those cases, we zoom into what are the specific interventions that
[00:20:42.680 --> 00:20:43.680]   can be made.
[00:20:43.680 --> 00:20:47.000]   Like in software, for example, we talk about privacy-preserving machine learning and the
[00:20:47.000 --> 00:20:52.520]   need for more standardization and open-source libraries around there, so that it's sort
[00:20:52.520 --> 00:20:57.240]   of reducing skill requirement and increase the clarity of how different systems should
[00:20:57.240 --> 00:21:02.960]   be compared, and interpretability, audit trails for safety-critical systems, etc.
[00:21:02.960 --> 00:21:09.240]   Some of the things on the software side, as well as things at the level of hardware that
[00:21:09.240 --> 00:21:10.240]   could incrementally…
[00:21:10.240 --> 00:21:12.800]   It's not going to be solved overnight.
[00:21:12.800 --> 00:21:17.640]   We sort of don't want to over-claim what we're accomplishing here, but what we try to do
[00:21:17.640 --> 00:21:21.960]   is sort of survey what are the ways that people can show that they're actually being responsible,
[00:21:21.960 --> 00:21:24.320]   as opposed to just saying, "Hey, we're being responsible.
[00:21:24.320 --> 00:21:25.320]   We care about safety.
[00:21:25.320 --> 00:21:30.120]   How do you actually provide evidence of that, and are there ways in which there are barriers
[00:21:30.120 --> 00:21:33.120]   to providing that evidence that we can make progress?"
[00:21:33.120 --> 00:21:34.120]   Interesting.
[00:21:34.120 --> 00:21:35.120]   Yeah.
[00:21:35.120 --> 00:21:36.120]   I thought it was interesting.
[00:21:36.120 --> 00:21:42.520]   You actually sort of recommended the government gives cloud compute credits to academic institutions,
[00:21:42.520 --> 00:21:43.520]   right?
[00:21:43.520 --> 00:21:44.520]   It kind of struck me because…
[00:21:44.520 --> 00:21:45.520]   Or some other approach.
[00:21:45.520 --> 00:21:46.520]   Yeah.
[00:21:46.520 --> 00:21:47.520]   Oh, oh.
[00:21:47.520 --> 00:21:48.520]   I kind of thought…
[00:21:48.520 --> 00:21:58.080]   I actually remember kind of leaving academia to go into industry, and I guess one of the
[00:21:58.080 --> 00:22:04.920]   real impetuses for me was the fact that tech companies just had so much more data that
[00:22:04.920 --> 00:22:10.080]   kind of led to more interesting problems.
[00:22:10.080 --> 00:22:17.640]   I do feel like when I talk to my friends at Facebook or Google, they feel kind of more
[00:22:17.640 --> 00:22:23.360]   sophisticated in some specific ways, just having dealt with such enormous data sets
[00:22:23.360 --> 00:22:26.880]   in a way that I don't think typically gets published.
[00:22:26.880 --> 00:22:32.400]   I feel like OpenAI is kind of one of the few places that…
[00:22:32.400 --> 00:22:37.680]   You clearly do incredibly compute-intensive stuff, but I wonder if you actually really
[00:22:37.680 --> 00:22:44.760]   deal with the same kind of scale of data sets, and if that might just sort of…
[00:22:44.760 --> 00:22:49.200]   I feel like there might be the case that a couple of big companies are sort of getting
[00:22:49.200 --> 00:22:52.320]   a skill set that doesn't exist anywhere and doesn't really get published.
[00:22:52.320 --> 00:22:53.320]   I'm not sure.
[00:22:53.320 --> 00:22:54.320]   Yeah.
[00:22:54.320 --> 00:22:59.360]   I mean, there's definitely a sense in which some companies have this sort of infrastructure
[00:22:59.360 --> 00:23:03.720]   in place for generating huge amounts of labeled or unlabeled data, and that puts them in a
[00:23:03.720 --> 00:23:06.480]   strong position to sort of do work in that area.
[00:23:06.480 --> 00:23:14.120]   I think it's also possible to do sort of cutting-edge work with open-source data through
[00:23:14.120 --> 00:23:19.240]   sort of existing data sets, or scraping and sort of building your own data sets.
[00:23:19.240 --> 00:23:24.880]   So I wouldn't draw this sort of hard distinction, but yeah, I think there are lots of ways in
[00:23:24.880 --> 00:23:31.120]   which industry gives opportunities that aren't necessarily available elsewhere, and that's
[00:23:31.120 --> 00:23:34.720]   part of what's driven the academics into industry.
[00:23:34.720 --> 00:23:38.680]   And part of where we're coming from in this report is like, "Is that a good thing?
[00:23:38.680 --> 00:23:40.080]   Are there ways that you can balance it?"
[00:23:40.080 --> 00:23:44.200]   It's slightly easier to sort of say, "Okay, let's balance out the compute side of the
[00:23:44.200 --> 00:23:48.560]   equation than the data side of the equation," in part because a lot of the data is sort
[00:23:48.560 --> 00:23:54.200]   of private, and by nature, it's just sort of really hard to get that out of these companies
[00:23:54.200 --> 00:23:56.160]   in an ethical way.
[00:23:56.160 --> 00:24:01.760]   But I think we should also be thinking about data in terms of a differentiator between
[00:24:01.760 --> 00:24:03.480]   different sectors.
[00:24:03.480 --> 00:24:10.080]   I would also like to see governments, in addition to providing compute credits or other means
[00:24:10.080 --> 00:24:16.440]   of supporting academics, also be like building a data center or something like that.
[00:24:16.440 --> 00:24:20.960]   Also generating label data sets is another thing that governments can potentially do,
[00:24:20.960 --> 00:24:28.320]   because it's not clear that whatever is easiest to collect at Google or Facebook or Twitter
[00:24:28.320 --> 00:24:32.600]   or whatever is inherently the data that we need to solve all the problems in the world.
[00:24:32.600 --> 00:24:36.680]   In fact, those data sets by default have lots of biases.
[00:24:36.680 --> 00:24:41.520]   So, I think one potentially exciting area is government support for data sets that could
[00:24:41.520 --> 00:24:46.200]   be used by large numbers of people and that are specifically designed to be less biased
[00:24:46.200 --> 00:24:54.200]   and have different distributions of bias that are able to help a wide number of actors.
[00:24:54.200 --> 00:24:59.080]   And I think the fact that you can cheaply copy data is a strong argument for this being
[00:24:59.080 --> 00:25:00.560]   something that governments can do.
[00:25:00.560 --> 00:25:03.800]   It's like building a highway or something like that that benefits a large number of
[00:25:03.800 --> 00:25:04.800]   people.
[00:25:04.800 --> 00:25:09.240]   And yeah, you can try and exclude people from the highway and do tolls and stuff.
[00:25:09.240 --> 00:25:10.800]   But generally, it's public infrastructure.
[00:25:10.800 --> 00:25:15.680]   And similarly, producing data sets that can be widely used is another thing to consider.
[00:25:15.680 --> 00:25:16.680]   That's kind of a cool idea.
[00:25:16.680 --> 00:25:24.200]   I guess it does seem to me that data sets have pushed a lot of innovation in ML.
[00:25:24.200 --> 00:25:29.840]   I mean, I also remember when I was a grad student, just this frustration of it seemed
[00:25:29.840 --> 00:25:32.800]   like the tasks that we worked on were sort of based on the data sets that happened to
[00:25:32.800 --> 00:25:33.800]   be available.
[00:25:33.800 --> 00:25:38.640]   Although, I kind of feel like one of the issues that we had maybe was the...
[00:25:38.640 --> 00:25:43.640]   There were sort of collectives of people coming together to create data sets, but it would
[00:25:43.640 --> 00:25:49.080]   create a huge amount of kind of bureaucracy in that data collection process because no
[00:25:49.080 --> 00:25:51.320]   one person really owned it.
[00:25:51.320 --> 00:25:56.600]   And then I think it would end up being a much bigger undertaking than it necessarily needed
[00:25:56.600 --> 00:25:57.600]   to be.
[00:25:57.600 --> 00:26:02.200]   So I definitely see governments kind of having trouble just kind of making decisions like,
[00:26:02.200 --> 00:26:04.680]   "Okay, what are we actually going to collect here?"
[00:26:04.680 --> 00:26:05.680]   Yeah.
[00:26:05.680 --> 00:26:06.680]   Yeah.
[00:26:06.680 --> 00:26:08.760]   And it's not clear what to be done.
[00:26:08.760 --> 00:26:13.360]   You could also imagine, in addition to compute credits, giving academics data labeling credits
[00:26:13.360 --> 00:26:19.240]   that just allow them to sort of go to some third-party service and pay for some amount
[00:26:19.240 --> 00:26:20.240]   of data.
[00:26:20.240 --> 00:26:24.280]   I think there's probably a role for that in the same way that there's also a role for
[00:26:24.280 --> 00:26:28.440]   big public data sets that a bunch of people are using for some general class of problems.
[00:26:28.440 --> 00:26:33.680]   So I think you ideally want to reduce the barriers to entry on both for generating these
[00:26:33.680 --> 00:26:37.680]   big useful data sets, but also for making sure that people have access to more tailored
[00:26:37.680 --> 00:26:39.680]   data for their own needs.
[00:26:39.680 --> 00:26:40.680]   Hi.
[00:26:40.680 --> 00:26:45.040]   We'd love to take a moment to tell you guys about Weights and Biases.
[00:26:45.040 --> 00:26:50.560]   Weights and Biases is a tool that helps you track and visualize every detail of your machine
[00:26:50.560 --> 00:26:51.560]   learning models.
[00:26:51.560 --> 00:26:57.480]   We help you debug your machine learning models in real time, collaborate easily, and advance
[00:26:57.480 --> 00:27:00.560]   the state of the art in machine learning.
[00:27:00.560 --> 00:27:05.760]   You can integrate Weights and Biases into your models with just a few lines of code.
[00:27:05.760 --> 00:27:10.520]   With hyperparameter sweeps, you can find the best set of hyperparameters for your models
[00:27:10.520 --> 00:27:12.460]   automatically.
[00:27:12.460 --> 00:27:17.880]   You can also track and compare how many GPU resources your models are using.
[00:27:17.880 --> 00:27:24.560]   With one line of code, you can visualize model predictions in form of images, videos, audio,
[00:27:24.560 --> 00:27:30.300]   plotly charts, molecular data, segmentation maps, and 3D point clouds.
[00:27:30.300 --> 00:27:36.120]   You can save everything you need to reproduce your models days, weeks, or even months after
[00:27:36.120 --> 00:27:37.120]   training.
[00:27:37.120 --> 00:27:42.240]   Finally, with reports, you can make your models come alive.
[00:27:42.240 --> 00:27:47.280]   Reports are like blog posts in which your readers can interact with your model metrics
[00:27:47.280 --> 00:27:49.080]   and predictions.
[00:27:49.080 --> 00:27:55.280]   Reports serve as a centralized repository of metrics, predictions, hyperparameter strides,
[00:27:55.280 --> 00:27:56.280]   and accompanying notes.
[00:27:56.280 --> 00:28:02.700]   All of this together gives you a bird's eye view of your machine learning workflow.
[00:28:02.700 --> 00:28:08.200]   You can use reports to share your model insights, keep your team on the same page, and collaborate
[00:28:08.200 --> 00:28:09.200]   effectively remotely.
[00:28:09.200 --> 00:28:14.320]   I'll leave a link in the show notes below to help you get started.
[00:28:14.320 --> 00:28:17.240]   And now, let's get back to the episode.
[00:28:17.240 --> 00:28:23.200]   I'm curious, just a little bit jumping around, but what do you think about...
[00:28:23.200 --> 00:28:29.480]   It seems like OpenAI has actually been a real driver towards using more and more compute
[00:28:29.480 --> 00:28:32.480]   on these different challenges.
[00:28:32.480 --> 00:28:37.960]   Makes it hard to reproduce and makes it potentially some environmental impact.
[00:28:37.960 --> 00:28:40.240]   Do you have any thoughts on that?
[00:28:40.240 --> 00:28:41.240]   Yeah.
[00:28:41.240 --> 00:28:43.160]   I think it's a really good question.
[00:28:43.160 --> 00:28:49.280]   I think I've liked a lot of the publications on this topic, like the paper Green AI from
[00:28:49.280 --> 00:28:53.400]   some folks at the Allen Institute was a good indication of this topic, and lots of other
[00:28:53.400 --> 00:28:56.040]   people have been calling attention to it.
[00:28:56.040 --> 00:29:04.080]   I think, in general, my view is that all things being equal, we would like to not use more
[00:29:04.080 --> 00:29:06.600]   compute than is needed for solving a given problem.
[00:29:06.600 --> 00:29:11.880]   But there's some ways in which it's not as urgent or as bad of a problem as it might
[00:29:11.880 --> 00:29:17.760]   at first appear, such as the fact that the pre-training step of a large generative model,
[00:29:17.760 --> 00:29:23.360]   for example, only needs to be done once, and it can be fine-tuned relatively cheaply, or
[00:29:23.360 --> 00:29:27.800]   even used in a zero-shot fashion by millions or billions of people.
[00:29:27.800 --> 00:29:33.280]   I think it would be strange to look only at the training side and not also the inference
[00:29:33.280 --> 00:29:34.280]   side.
[00:29:34.280 --> 00:29:40.520]   I mean, your question wasn't about one or the other specifically, but I would just flag
[00:29:40.520 --> 00:29:47.920]   the difference between the two in that, say, Google using BERT for search result ranking,
[00:29:47.920 --> 00:29:51.920]   for example, presumably almost all of the compute cost there is on the inference side
[00:29:51.920 --> 00:29:56.680]   rather than the training side, just because it was originally a few hundred thousand dollars
[00:29:56.680 --> 00:30:01.920]   or something for the training, and now it's being used at large for billions of users.
[00:30:01.920 --> 00:30:05.920]   That's not to say that it's not an issue in terms of the environmental impacts, but you
[00:30:05.920 --> 00:30:11.160]   also have to look at the whole sort of game and think about inference, whereas I think
[00:30:11.160 --> 00:30:15.240]   a lot of the attention so far has been on things like big training runs.
[00:30:15.240 --> 00:30:17.600]   Yeah, I guess that's a good point.
[00:30:17.600 --> 00:30:19.320]   I wonder why that is.
[00:30:19.320 --> 00:30:25.400]   I actually sort of felt like when I-- I kind of looked into-- did my sort of back-of-the-envelope
[00:30:25.400 --> 00:30:30.880]   calculations on the whole thing, and it seemed to me like even if you took all the graphics
[00:30:30.880 --> 00:30:36.320]   cards that are made and ran them at all times, it wouldn't be near the environmental impact
[00:30:36.320 --> 00:30:39.080]   of just regular data centers.
[00:30:39.080 --> 00:30:45.360]   But I guess the trend line is certainly scary, right?
[00:30:45.360 --> 00:30:49.000]   Because it's like this exponential growth and volume of usage.
[00:30:49.000 --> 00:30:52.920]   And I guess maybe it feels like there's sort of a more natural barrier on the inference
[00:30:52.920 --> 00:30:53.920]   because it's like-- I don't know.
[00:30:53.920 --> 00:30:54.920]   Why does that feel like that?
[00:30:54.920 --> 00:30:56.920]   Maybe because companies are kind of doing it.
[00:30:56.920 --> 00:31:00.360]   They probably won't.
[00:31:00.360 --> 00:31:06.640]   I guess it sort of seems like there's some limits to the inference, whereas the training
[00:31:06.640 --> 00:31:07.640]   seems to be skyrocketing.
[00:31:07.640 --> 00:31:10.640]   At least that's my impression.
[00:31:10.640 --> 00:31:11.640]   Yeah.
[00:31:11.640 --> 00:31:12.640]   Yeah.
[00:31:12.640 --> 00:31:18.920]   And I think that's right in that we haven't really seen many big models being served in
[00:31:18.920 --> 00:31:19.920]   production.
[00:31:19.920 --> 00:31:26.160]   And generally, there's a lot of optimization pressure to keep things there, whereas in
[00:31:26.160 --> 00:31:30.720]   training and research, it's a bit more like everything goes and trying out things to see
[00:31:30.720 --> 00:31:31.720]   what works.
[00:31:31.720 --> 00:31:32.720]   Yeah.
[00:31:32.720 --> 00:31:34.600]   It'll be interesting to see how that evolves over time.
[00:31:34.600 --> 00:31:41.680]   Another thing I'd add is that I would also try not to pass all of AI in a sort of broad
[00:31:41.680 --> 00:31:45.960]   brush in that depending on the use case, you could actually be saving energy.
[00:31:45.960 --> 00:31:52.840]   I think like DeepMind and Google's use of DeepRL for controlling data center energy
[00:31:52.840 --> 00:31:56.760]   consumption, for example, is a case where you're actually able to reduce the net amount
[00:31:56.760 --> 00:32:00.120]   of energy used by applying some AI upfront.
[00:32:00.120 --> 00:32:03.040]   I guess one question I asked Jack and I thought his answer was very interesting.
[00:32:03.040 --> 00:32:06.720]   I'm kind of curious about yours.
[00:32:06.720 --> 00:32:11.880]   The whole thing around kind of not releasing the biggest GPT-2 model.
[00:32:11.880 --> 00:32:14.800]   Well, maybe I'll say what I thought about it.
[00:32:14.800 --> 00:32:16.040]   I didn't even tell Jack this.
[00:32:16.040 --> 00:32:17.040]   This is sort of my impression.
[00:32:17.040 --> 00:32:20.200]   I mean, I don't view myself as sort of the expert on this stuff.
[00:32:20.200 --> 00:32:25.920]   And I think at first I kind of felt like openness seems like really, really important to me
[00:32:25.920 --> 00:32:28.440]   as this like a core value.
[00:32:28.440 --> 00:32:32.520]   It sort of feels to me like if people are going to do stuff and call themselves like
[00:32:32.520 --> 00:32:39.400]   open AI, they really should be airing on the side of making their work public.
[00:32:39.400 --> 00:32:44.320]   But then I thought, well, it's kind of interesting that they've sort of chosen to think about
[00:32:44.320 --> 00:32:48.480]   the impact of releasing this and kind of making a controversial stance here.
[00:32:48.480 --> 00:32:51.160]   And also I thought, huh, like I wonder, and maybe they're right.
[00:32:51.160 --> 00:32:56.080]   It certainly seemed to me at the time like a really powerful language model could be
[00:32:56.080 --> 00:32:57.720]   used in bad ways.
[00:32:57.720 --> 00:33:02.640]   And so I think I didn't feel so sure of myself about what I thought.
[00:33:02.640 --> 00:33:07.520]   And then it sort of seemed like it didn't really matter because like other models came
[00:33:07.520 --> 00:33:10.600]   out like a month or two later.
[00:33:10.600 --> 00:33:16.480]   And like it almost seemed like maybe the most surprising thing is that there weren't more
[00:33:16.480 --> 00:33:21.680]   versions of such an impressive looking technology.
[00:33:21.680 --> 00:33:22.680]   I don't know why I started.
[00:33:22.680 --> 00:33:28.720]   I'm kind of curious from the inside, like how it felt for you and then if there are
[00:33:28.720 --> 00:33:30.360]   any lessons learned on it.
[00:33:30.360 --> 00:33:31.360]   Yeah.
[00:33:31.360 --> 00:33:35.600]   So I mean, from the inside, we also felt very unsure what to think.
[00:33:35.600 --> 00:33:39.960]   And we tried to sort of at each stage, like say clearly like, well, we don't know how
[00:33:39.960 --> 00:33:41.320]   dangerous this is.
[00:33:41.320 --> 00:33:45.560]   And yeah, we got this information and we're sort of, we were trying to like reduce the,
[00:33:45.560 --> 00:33:49.560]   you know, shrink the error bars over time in terms of, you know, both like beneficial
[00:33:49.560 --> 00:33:52.560]   uses and home electricity systems.
[00:33:52.560 --> 00:33:56.160]   That's not to say that we eventually, you know, converged on a total, you know, on an
[00:33:56.160 --> 00:33:59.520]   overall conclusion of like, okay, this is like definitely good for society.
[00:33:59.520 --> 00:34:04.400]   But, you know, we started with the default of openness and, you know, sort of these concerns
[00:34:04.400 --> 00:34:09.040]   arose in terms of people building, you know, proofs of concept of, you know, writing reviews
[00:34:09.040 --> 00:34:10.040]   for Amazon, for example.
[00:34:10.040 --> 00:34:11.800]   And like that seemed pretty scary.
[00:34:11.800 --> 00:34:15.360]   And, you know, writing sort of fake news stories seemed pretty scary.
[00:34:15.360 --> 00:34:19.840]   And, you know, ultimately what we, you know, sort of ended up doing was taking an incremental
[00:34:19.840 --> 00:34:24.200]   approach of sort of releasing, you know, progressively larger versions of the model.
[00:34:24.200 --> 00:34:28.080]   And obviously, you know, if we could sort of go back in time, we would sort of, you
[00:34:28.080 --> 00:34:32.040]   know, take all the insights that we have now and sort of feed that into an earlier stage
[00:34:32.040 --> 00:34:33.040]   in the process.
[00:34:33.040 --> 00:34:37.760]   But what we, what I think, what we ended up doing, I think was like a reasonable approach
[00:34:37.760 --> 00:34:42.280]   in the sense of like, you know, if there's a potential irreversible decision, like releasing
[00:34:42.280 --> 00:34:45.920]   a model, it makes sense to be a little bit cautious, you know, before you do it, if there
[00:34:45.920 --> 00:34:47.840]   are ways you can gather more information.
[00:34:47.840 --> 00:34:50.680]   I think there are ways to get, you can get some information by doing things like human
[00:34:50.680 --> 00:34:56.760]   studies and we, you know, worked with external research, you know, run to, you know, outputs
[00:34:56.760 --> 00:35:00.760]   by people and sort of, you know, statistically study, you know, the differences across the
[00:35:00.760 --> 00:35:04.640]   model sizes and that informed some of our decision making.
[00:35:04.640 --> 00:35:08.000]   But ultimately, it's really hard to sort of answer these questions because there's so
[00:35:08.000 --> 00:35:12.880]   many answers about, you know, economics of it and the motivations of bad actors.
[00:35:12.880 --> 00:35:17.520]   So I think it's an ongoing issue that, you know, can really fully resolve us.
[00:35:17.520 --> 00:35:23.480]   Do you feel like you, you really got new information that like informed decisions like along the
[00:35:23.480 --> 00:35:24.480]   way?
[00:35:24.480 --> 00:35:27.520]   Like what, what kind of information did you collect and what, what different information
[00:35:27.520 --> 00:35:29.680]   could you have gotten that would have made you make a different choice?
[00:35:29.680 --> 00:35:30.680]   Yeah.
[00:35:30.680 --> 00:35:35.240]   So just as a concrete example, we were like very unsure what those sort of scaling relationship
[00:35:35.240 --> 00:35:40.000]   is between, you know, sort of model size in this, you know, GPT-2, you know, regime of
[00:35:40.000 --> 00:35:43.920]   like 125 million to 1.5 billion parameters.
[00:35:43.920 --> 00:35:48.480]   We weren't sure what the, what the sort of relationship was between model size and convincingness
[00:35:48.480 --> 00:35:53.680]   or ability to be, you know, coherence and clarity and so forth.
[00:35:53.680 --> 00:35:59.480]   And we had a rough sense that there is this sort of, you know, smooth-ish relationship,
[00:35:59.480 --> 00:36:03.800]   you know, as you grow in model size, it takes fewer and fewer tries to get a given result.
[00:36:03.800 --> 00:36:08.760]   So, you know, less cherry picking is required, you know, for a given fixed level of sort
[00:36:08.760 --> 00:36:09.760]   of performance.
[00:36:09.760 --> 00:36:14.880]   And that seemed to be true, but we weren't sure, you know, you know, okay, but you know,
[00:36:14.880 --> 00:36:18.120]   we also weren't sure like, okay, for a given level of cherry picking, what's the level
[00:36:18.120 --> 00:36:19.720]   that you can achieve?
[00:36:19.720 --> 00:36:22.960]   And what we ultimately found was that there actually wasn't a huge difference between
[00:36:22.960 --> 00:36:25.160]   the two largest model sizes.
[00:36:25.160 --> 00:36:30.160]   And that was one of the factors that pushed us towards releasing the 1.5 billion model
[00:36:30.160 --> 00:36:33.720]   when otherwise, like if there had been more of a gap between the two, then that would
[00:36:33.720 --> 00:36:37.040]   have felt like there was more sort of risk in doing that release.
[00:36:37.040 --> 00:36:40.760]   And there were also other things happening, like other people releasing models, and we're
[00:36:40.760 --> 00:36:43.520]   able to do some sort of, you know, comparison between them.
[00:36:43.520 --> 00:36:47.320]   So, we were, you know, trying to, you know, absorb as much information and generate as
[00:36:47.320 --> 00:36:49.720]   much information as we can.
[00:36:49.720 --> 00:36:53.920]   But yeah, overall, you know, that's like probably one of the most clear cut cases where like
[00:36:53.920 --> 00:36:59.680]   the real, like diminishing marginal risk as you sort of increase in model size was a reason
[00:36:59.680 --> 00:37:05.120]   why we sort of, you know, felt like for scientific reproducibility reasons, other reasons, the
[00:37:05.120 --> 00:37:07.160]   benefits were sort of outweighing the cons there.
[00:37:07.160 --> 00:37:10.560]   Because there wasn't, you know, much increase in risk, but we also were seeing significantly
[00:37:10.560 --> 00:37:15.360]   improved performance across sort of standard NLP benchmark.
[00:37:15.360 --> 00:37:20.280]   So it seemed like it was a non-trivial increase in utility from a research perspective, and
[00:37:20.280 --> 00:37:23.600]   also would allow people to, you know, start grappling with, you know, some of the issues
[00:37:23.600 --> 00:37:29.040]   involved in training these large models, but also didn't seem to be a huge marginal increase.
[00:37:29.040 --> 00:37:33.800]   As someone who worked on natural language processing a while ago, I mean, I thought
[00:37:33.800 --> 00:37:39.200]   that in my view, the GPT-2 results were like incredibly impressive.
[00:37:39.200 --> 00:37:46.360]   And I think I thought at the time, this must have been about a year ago, that the applications
[00:37:46.360 --> 00:37:47.360]   would be enormous.
[00:37:47.360 --> 00:37:50.840]   And I think actually the applications have been kind of subtle.
[00:37:50.840 --> 00:37:54.320]   Like I've noticed, like, you know, I've noticed translation systems working a little better
[00:37:54.320 --> 00:37:55.880]   than they used to.
[00:37:55.880 --> 00:38:03.840]   And there was that like crazy adventure game that I played and, you know, it was kind of
[00:38:03.840 --> 00:38:04.840]   fun.
[00:38:04.840 --> 00:38:05.840]   Yeah.
[00:38:05.840 --> 00:38:11.600]   And I've seen, I've seen like people like suggesting plausible domain names, you know,
[00:38:11.600 --> 00:38:12.600]   for your business.
[00:38:12.600 --> 00:38:17.760]   And, you know, we see, I mean, we see on our website, you know, we see a lot of models
[00:38:17.760 --> 00:38:18.760]   come through.
[00:38:18.760 --> 00:38:22.400]   And so, you know, we do see people, you know, using the technology, but I don't think that
[00:38:22.400 --> 00:38:29.080]   like, you know, my mother has like noticed a difference in her world.
[00:38:29.080 --> 00:38:32.320]   Maybe that's not surprising in retrospect, but it's funny how it's like, it seems like
[00:38:32.320 --> 00:38:33.320]   there's this huge leap.
[00:38:33.320 --> 00:38:40.680]   And I feel like in the vision stuff, we maybe feel it a little more like, you know, I feel
[00:38:40.680 --> 00:38:45.920]   like, you know, face recognition is kind of, it feels like a little more, you know, ubiquitous
[00:38:45.920 --> 00:38:46.920]   to me and scary.
[00:38:46.920 --> 00:38:51.440]   And at least I noticed in my camera, like somehow finds, you know, people's faces and
[00:38:51.440 --> 00:38:53.480]   things like that, that definitely couldn't do a few years ago.
[00:38:53.480 --> 00:38:54.480]   I'm not sure.
[00:38:54.480 --> 00:38:55.480]   What do you think about this?
[00:38:55.480 --> 00:38:56.480]   Yeah.
[00:38:56.480 --> 00:39:01.600]   Well, I think it's, I think it's, I tend to, you know, sort of think in terms of, you know,
[00:39:01.600 --> 00:39:04.960]   general purpose technologies that, you know, could be misused or could be used for beneficial
[00:39:04.960 --> 00:39:05.960]   things.
[00:39:05.960 --> 00:39:09.280]   So I basically say the same thing, you know, that I was saying about the risks.
[00:39:09.280 --> 00:39:13.360]   So it's, you know, it's, we have some information about, you know, the fact that you can produce
[00:39:13.360 --> 00:39:17.280]   coherent text in some contexts, and that seems like it could be used for lots of, you know,
[00:39:17.280 --> 00:39:21.220]   commercial and creative applications, and also could be used for some malicious applications.
[00:39:21.220 --> 00:39:25.360]   But it, we might just not be at the level of performance for either of those domains,
[00:39:25.360 --> 00:39:30.000]   where it sort of is a straightforward replacement of, you know, what people were already doing.
[00:39:30.000 --> 00:39:34.600]   So I think we'll get there eventually, and that language models will continue to proliferate
[00:39:34.600 --> 00:39:38.720]   through a bunch of different applications, and including, you know, on the edge and in
[00:39:38.720 --> 00:39:41.480]   the cloud and all sorts of, you know, contexts.
[00:39:41.480 --> 00:39:44.120]   But I think, you know, there, you know, a few things need to happen.
[00:39:44.120 --> 00:39:50.260]   There needs to be more reliability and sort of higher, you know, higher, higher performance
[00:39:50.260 --> 00:39:54.680]   compared to humans on some tasks, where it's just not, not going to make sense to replace
[00:39:54.680 --> 00:39:59.320]   when it's, you know, to replace a human or augment a human if it hasn't yet reached that
[00:39:59.320 --> 00:40:00.680]   level of performance.
[00:40:00.680 --> 00:40:04.880]   And generally, I think we also need to figure out what the right workflows and, you know,
[00:40:04.880 --> 00:40:06.920]   ways of leveraging these systems look like.
[00:40:06.920 --> 00:40:10.560]   Because I don't think it's just, you know, replace a human with a language model.
[00:40:10.560 --> 00:40:14.400]   I think that's, you know, one of the sort of more, you know, naive, you know, sort of
[00:40:14.400 --> 00:40:17.920]   uses that you can do, but that depends on a very high level of performance and, you
[00:40:17.920 --> 00:40:22.080]   know, the right kind of use case where you don't need sort of online monitoring.
[00:40:22.080 --> 00:40:26.520]   But I think there's also other cases like sort of writing assistant, where the fact
[00:40:26.520 --> 00:40:31.960]   that it's not 100% reliable is not, you know, a game, it's not sort of a deal breaker.
[00:40:31.960 --> 00:40:35.600]   But if you're able to sort of get humans in the loop to sort of provide feedback on these
[00:40:35.600 --> 00:40:40.360]   systems and sort of choose among a few potential outputs, like both for, you know, beneficial
[00:40:40.360 --> 00:40:42.680]   and malicious purposes, I think that could be a game changer.
[00:40:42.680 --> 00:40:46.180]   So, I think we'll see further progress both on the technology and people finding better
[00:40:46.180 --> 00:40:47.180]   ways to use it.
[00:40:47.180 --> 00:40:48.180]   Interesting.
[00:40:48.180 --> 00:40:52.680]   Does OpenAI, like, do you sort of like continue to like push things forward?
[00:40:52.680 --> 00:40:55.680]   Are you sort of like, okay, we like made this model, like we're good?
[00:40:55.680 --> 00:40:58.480]   Like, how does that kind of think about that?
[00:40:58.480 --> 00:41:00.760]   Yeah, we're still continuing to push things forward.
[00:41:00.760 --> 00:41:06.400]   And, you know, both in terms of, you know, trying to understand these models better,
[00:41:06.400 --> 00:41:10.200]   like the ones we've already built, and also trying to push further in terms of improving
[00:41:10.200 --> 00:41:11.200]   performance.
[00:41:11.200 --> 00:41:16.120]   So, here's another question that I always have about OpenAI, if you don't mind.
[00:41:16.120 --> 00:41:17.120]   I'm kind of curious.
[00:41:17.120 --> 00:41:21.920]   So, like, you know, I think it's OpenAI has a, what's OpenAI's mission?
[00:41:21.920 --> 00:41:24.800]   Like ethical AI, something like that, right?
[00:41:24.800 --> 00:41:25.800]   Remind me.
[00:41:25.800 --> 00:41:26.800]   Yeah, yeah.
[00:41:26.800 --> 00:41:32.040]   The shorthand version is sort of build, you know, build artificial general intelligence
[00:41:32.040 --> 00:41:35.720]   and use it to, you know, help everyone.
[00:41:35.720 --> 00:41:41.720]   So I guess like, it seems like a funny mix of sort of policy and building.
[00:41:41.720 --> 00:41:46.640]   Like, do you think it's important that those two things are together?
[00:41:46.640 --> 00:41:55.360]   Like I would, I mean, I could imagine, like, should the people, like, I guess, how would
[00:41:55.360 --> 00:41:56.360]   you argue against this?
[00:41:56.360 --> 00:41:57.360]   Because I think I do think they should go together.
[00:41:57.360 --> 00:41:59.360]   I'm trying to think like, what's the other point of view?
[00:41:59.360 --> 00:42:05.000]   But it sort of seems like, well, like, does policy and building really necessarily need
[00:42:05.000 --> 00:42:06.000]   to be combined?
[00:42:06.000 --> 00:42:07.840]   Like, is it even combined in most cases?
[00:42:07.840 --> 00:42:13.200]   Like, you know, it sort of feels like the policy makers in general, like, aren't always
[00:42:13.200 --> 00:42:14.200]   engineers, right?
[00:42:14.200 --> 00:42:17.800]   Like, why, why have a thing that combines both?
[00:42:17.800 --> 00:42:18.920]   Yeah.
[00:42:18.920 --> 00:42:24.120]   So I think, you know, by default, a lot of people who are building powerful technologies
[00:42:24.120 --> 00:42:28.000]   are like de facto sort of policy makers and that they're setting, you know, the defaults
[00:42:28.000 --> 00:42:32.720]   and the, you know, how people think of things and they're sort of influencing what the early
[00:42:32.720 --> 00:42:34.040]   applications are and so forth.
[00:42:34.040 --> 00:42:35.920]   So I think you can't totally separate them.
[00:42:35.920 --> 00:42:39.680]   I think anyone who's sort of involved in technology, you know, should be thinking to some extent
[00:42:39.680 --> 00:42:41.680]   about, you know, their social impact.
[00:42:41.680 --> 00:42:44.720]   And, you know, there's also value to, like, division of labor.
[00:42:44.720 --> 00:42:48.360]   And that's why, you know, not every single person in the organization is on the policy
[00:42:48.360 --> 00:42:49.360]   team.
[00:42:49.360 --> 00:42:50.360]   We have a policy team and then we have a technology team.
[00:42:50.360 --> 00:42:51.360]   We have various different teams.
[00:42:51.360 --> 00:42:56.480]   So it makes sense to, like, have some specialization.
[00:42:56.480 --> 00:43:01.000]   But I think, you know, the reason why we think it's such a high priority is that we don't
[00:43:01.000 --> 00:43:06.200]   see sort of the impact of AI as reducible exclusively to the design of the technology.
[00:43:06.200 --> 00:43:11.720]   It's also about, you know, how is it, what sorts of policies are there to constrain the
[00:43:11.720 --> 00:43:18.200]   use of it, and are there ways of distributing the economic upsides of it, that it's not,
[00:43:18.200 --> 00:43:20.720]   you know, only benefiting a small number of people.
[00:43:20.720 --> 00:43:24.600]   So we think that, you know, it's not just a matter of building the technology, but also
[00:43:24.600 --> 00:43:28.520]   making sure that it happens at the right time and the right environment with the right infrastructure.
[00:43:28.520 --> 00:43:32.920]   And so that's why we invest a lot in sort of advocating for policy changes that we think
[00:43:32.920 --> 00:43:39.720]   will enable a more cooperative, more responsible AI ecosystem.
[00:43:39.720 --> 00:43:45.600]   Like among, like, I'm sure you have, like, a group of, like, AI ethicists or policy makers
[00:43:45.600 --> 00:43:47.400]   that you hang out with.
[00:43:47.400 --> 00:43:51.800]   I'm curious, like, in that, in those circles, like, you know, like, the things that I hear
[00:43:51.800 --> 00:43:56.360]   from people that are interested in AI, they seem like very, like, they don't seem very,
[00:43:56.360 --> 00:43:57.360]   like, controversial for me.
[00:43:57.360 --> 00:44:01.040]   It seems like people want, like, fairness, and they want, like, you know, openness and
[00:44:01.040 --> 00:44:02.040]   transparency.
[00:44:02.040 --> 00:44:08.240]   And these things all seem, like, really, like, you know, sensible, wonderful things to me.
[00:44:08.240 --> 00:44:10.920]   But I guess, like, where are there, like, disagreements?
[00:44:10.920 --> 00:44:15.160]   Like, are there, like, different, like, factions, you think, of people, like, thinking deeply
[00:44:15.160 --> 00:44:17.280]   about this topic?
[00:44:17.280 --> 00:44:18.280]   There definitely are.
[00:44:18.280 --> 00:44:19.280]   Yeah.
[00:44:19.280 --> 00:44:24.120]   So I'm not trying to say factions, because I try to, like, you know, build bridges and
[00:44:24.120 --> 00:44:28.400]   stuff like that, but there are definitely, like, differences in emphasis between people
[00:44:28.400 --> 00:44:33.240]   who are sort of focused on, you know, immediate ethical concerns around things like bias and
[00:44:33.240 --> 00:44:35.720]   surveillance, you know, sort of one end of the spectrum.
[00:44:35.720 --> 00:44:39.640]   You know, there are probably multiple spectra, but, you know, that's one end of one spectrum.
[00:44:39.640 --> 00:44:44.360]   And on the other end is people who think, you know, existential risk from, you know,
[00:44:44.360 --> 00:44:48.960]   AI systems that are too powerful to, you know, control, or unless you've sort of thought
[00:44:48.960 --> 00:44:52.360]   really hard about it in advance, then, you know, by default, things can go bad.
[00:44:52.360 --> 00:44:55.920]   That's the thing you should focus on and devote 100% of your attention to.
[00:44:55.920 --> 00:45:00.480]   So, I think, you know, personally, I find myself somewhere in the middle, and OpenAI
[00:45:00.480 --> 00:45:03.880]   as an organization finds itself somewhere in the middle, in that we are thinking about
[00:45:03.880 --> 00:45:08.920]   both bias and, you know, long-term safety, and thinking about present systems and AGI.
[00:45:08.920 --> 00:45:14.120]   And I think, you know, I think there's a lot of, you know, I don't think … I don't think
[00:45:14.120 --> 00:45:18.320]   that in terms of, you know, practices, ultimately, like, both sides are just trying to, like,
[00:45:18.320 --> 00:45:22.320]   figure out how to make sure this technology helps people and doesn't hurt people.
[00:45:22.320 --> 00:45:27.240]   And, in fact, often, you know, the conclusions of … in terms of the actual policies you
[00:45:27.240 --> 00:45:30.360]   recommend aren't that different, depending on whether you're focused on the immediate
[00:45:30.360 --> 00:45:31.360]   term or the long term.
[00:45:31.360 --> 00:45:35.880]   So, I see it more as sort of a spectrum, but there are definitely people with, you know,
[00:45:35.880 --> 00:45:36.880]   different emphasis.
[00:45:36.880 --> 00:45:37.880]   Is that, like, cop-out?
[00:45:37.880 --> 00:45:38.880]   Like, I don't know.
[00:45:38.880 --> 00:45:43.480]   Like, I feel like if the ethics is interesting, it must have, like … there must be, like,
[00:45:43.480 --> 00:45:44.480]   controversy, right?
[00:45:44.480 --> 00:45:49.280]   Like, have there been decisions, like, internal to open AI, or even just sort of, like, philosophical
[00:45:49.280 --> 00:45:54.560]   discussions where, like, people really take, like, strong, like, different stances?
[00:45:54.560 --> 00:45:58.400]   Like, you know, because I feel like when you put it in this general sense, it's like, who
[00:45:58.400 --> 00:46:03.120]   would argue that we shouldn't make AI safe, and who would argue that we shouldn't make
[00:46:03.120 --> 00:46:04.120]   AI biased?
[00:46:04.120 --> 00:46:08.960]   But I have a feeling, like, when you … when you zoom in on, like, what that really means,
[00:46:08.960 --> 00:46:10.960]   there must be points where they come into conflict.
[00:46:10.960 --> 00:46:11.960]   Yeah.
[00:46:11.960 --> 00:46:12.960]   Yeah.
[00:46:12.960 --> 00:46:17.640]   And to be clear, yeah, you were asking about factions, so I tried to give you a map of
[00:46:17.640 --> 00:46:18.640]   the factions.
[00:46:18.640 --> 00:46:19.640]   Oh, yeah, sure, sure, sure.
[00:46:19.640 --> 00:46:23.480]   But I think there's also, like, you know, within the factions, and, you know, there's
[00:46:23.480 --> 00:46:27.960]   also all sorts of debates, and, yeah, they're not, like, you know, consensus factions.
[00:46:27.960 --> 00:46:31.480]   So, yeah, like, let's just take, like, the short-term issues.
[00:46:31.480 --> 00:46:36.000]   So, I think among the people who are focused on, you know, short-term issues, I think there
[00:46:36.000 --> 00:46:40.560]   is sort of, you know, another sort of spectrum of people, you know, who are, you know, on
[00:46:40.560 --> 00:46:44.880]   the one end thinking, "Okay, like, we have to figure out how to do this right in, you
[00:46:44.880 --> 00:46:48.240]   know, sort of each case and figure out what the right norms are," and sort of, you know,
[00:46:48.240 --> 00:46:53.200]   they see it being, like, you know, both intrinsically important and symbolically important to get,
[00:46:53.200 --> 00:46:57.760]   you know, issues like bias and so forth sorted out as soon as possible, whereas there's,
[00:46:57.760 --> 00:47:01.440]   you know, and so that's sort of the, like, you know, the, like, hardcore, like, "Let's
[00:47:01.440 --> 00:47:04.680]   make sure that we're not causing harm," so, like, maybe the, like, Hippocratic oath
[00:47:04.680 --> 00:47:08.120]   end of the spectrum of, like, "First, do no harm," and, like, take stuff like bias
[00:47:08.120 --> 00:47:09.120]   super seriously.
[00:47:09.120 --> 00:47:12.960]   And then there's, you know, another, you know, there are also lots of people who are, you
[00:47:12.960 --> 00:47:17.360]   know, building systems that, you know, are sort of focused on getting products to market
[00:47:17.360 --> 00:47:22.200]   and they're focused on sort of releasing systems that can inform research, and in the case
[00:47:22.200 --> 00:47:26.880]   of GPT-2, we saw, like, some of the tension between these perspectives of, like, you know,
[00:47:26.880 --> 00:47:30.800]   there was potentially a tension between, you know, avoiding causing harm on the one hand
[00:47:30.800 --> 00:47:36.400]   and enabling people to understand our research and verify our claims and sort of build new
[00:47:36.400 --> 00:47:41.480]   systems and so you can maybe say, like, Hippocratic, you know, oath end of the spectrum and the,
[00:47:41.480 --> 00:47:44.000]   like, move fast and break things end of the spectrum.
[00:47:44.000 --> 00:47:48.040]   I think there's, like, an element of truth to both of them in that, like, you know, we
[00:47:48.040 --> 00:47:53.600]   shouldn't be harming people on the one hand and also, like, this technology needs to be
[00:47:53.600 --> 00:47:58.920]   iterated and there needs to be sort of publication and sort of, you know, models getting out
[00:47:58.920 --> 00:48:03.440]   there in the world in order for there to be sort of learning by doing and ultimately figuring
[00:48:03.440 --> 00:48:05.040]   out how to solve those problems.
[00:48:05.040 --> 00:48:11.760]   So, yeah, definitely, you know, you could say conflicts there and, you know, in the
[00:48:11.760 --> 00:48:15.760]   case of GPT-2, there were definitely a bunch of different perspectives internally at OpenAI
[00:48:15.760 --> 00:48:20.760]   and, you know, ultimately we tried to wrangle that into a consensus issue, but you can see
[00:48:20.760 --> 00:48:24.080]   the, like, ambiguity and the fact that we were, like, hedging a lot of our claims, like,
[00:48:24.080 --> 00:48:28.080]   you know, we're not sure how dangerous this is, you know, we're sort of taking, you know,
[00:48:28.080 --> 00:48:29.080]   one step at a time.
[00:48:29.080 --> 00:48:33.920]   That was because, you know, there were actual, you know, different, there were actual, like,
[00:48:33.920 --> 00:48:35.360]   competing values at stake.
[00:48:35.360 --> 00:48:38.280]   Like, there was, like, you know, there was, you know, I wouldn't say it's, like, totally
[00:48:38.280 --> 00:48:42.600]   zero-sum, but there was, like, some zero-sumness between sort of, like, avoiding potential
[00:48:42.600 --> 00:48:47.200]   causing harm if it turned out that GPT-2 is very dangerous and allowing people to verify
[00:48:47.200 --> 00:48:48.200]   that.
[00:48:48.200 --> 00:48:51.000]   Yeah, that totally makes sense.
[00:48:51.000 --> 00:48:54.040]   When you think about, I mean, I think you mentioned that you don't really have a good
[00:48:54.040 --> 00:49:00.560]   sense for, like, what all the policies would need to be to, like, unlike climate change,
[00:49:00.560 --> 00:49:04.200]   like, where it's maybe more clear what the sensible policies are.
[00:49:04.200 --> 00:49:11.000]   Are there some things that you, policies that you would enact if you had control or if you
[00:49:11.000 --> 00:49:14.920]   could, like, you know, recommend a few things to, say, like, the US government?
[00:49:14.920 --> 00:49:17.040]   Like, what would those things be?
[00:49:17.040 --> 00:49:23.240]   Yeah, so some of the stuff that we flagged in the Tor Trustworthy AI Development Report
[00:49:23.240 --> 00:49:25.160]   I would definitely consider here.
[00:49:25.160 --> 00:49:28.240]   So stuff like compute support for academia.
[00:49:28.240 --> 00:49:33.120]   I think generally, like, at a super high level, one policy change that I'd like to see is
[00:49:33.120 --> 00:49:39.840]   sort of more robust support of academic research and I, you know, including not just compute
[00:49:39.840 --> 00:49:44.240]   but also things like data and just sort of, you know, funding for academics so that they're
[00:49:44.240 --> 00:49:48.840]   not constantly, like, writing, you know, grant applications and, you know, sort of, I think
[00:49:48.840 --> 00:49:53.320]   there are lots of inefficiencies in the way that the American academic system works currently.
[00:49:53.320 --> 00:49:59.720]   So, like, more, you know, long-term funding of, you know, work in areas like security
[00:49:59.720 --> 00:50:05.760]   and privacy would be good, sort of more support for working on things like bias because I
[00:50:05.760 --> 00:50:09.880]   think what we're currently kind of in a regime where there's a little bit of funding here
[00:50:09.880 --> 00:50:14.080]   from, you know, Department of Defense, there's a little bit of funding here from National
[00:50:14.080 --> 00:50:18.240]   Science Foundation, and then there's, like, you know, everything that industry is doing.
[00:50:18.240 --> 00:50:23.680]   And what I'd like to see is a world in which, you know, an academic, you know, or, you know,
[00:50:23.680 --> 00:50:28.520]   postdoc or something like that doesn't see a big tradeoff between sort of working in
[00:50:28.520 --> 00:50:32.640]   industry versus academia in terms of the resources that they'll have and the amount of freedom
[00:50:32.640 --> 00:50:37.840]   that they'll have because I think we'll see both faster progress in AI and more creativity
[00:50:37.840 --> 00:50:42.600]   if people are able to think long-term and in different sectors and not be sort of constantly
[00:50:42.600 --> 00:50:44.600]   fighting over money.
[00:50:44.600 --> 00:50:49.960]   But also, there are sort of areas that, you know, by necessity need to be worked on over
[00:50:49.960 --> 00:50:50.960]   the long term.
[00:50:50.960 --> 00:50:54.440]   So, things like safety, for example, you would want that to be, like, you know, always an
[00:50:54.440 --> 00:50:57.480]   option for, you know, ambitious grad students to work on that.
[00:50:57.480 --> 00:50:58.880]   But it's actually not really the case today.
[00:50:58.880 --> 00:51:03.120]   A lot of the time today, like, you know, one could go to grad school and not find an easy
[00:51:03.120 --> 00:51:07.360]   way to work on AI safety because, you know, a lot of the grants available are at, you
[00:51:07.360 --> 00:51:10.520]   know, Department of Defense on X, Y, and Z or something like that.
[00:51:10.520 --> 00:51:15.240]   And, yeah, they do fund some safety stuff, but I would like to see, like, more balancing
[00:51:15.240 --> 00:51:21.200]   between the civilian and defense side and also sort of more, you know, more robust long-term
[00:51:21.200 --> 00:51:22.200]   funding.
[00:51:22.200 --> 00:51:23.200]   That makes sense.
[00:51:23.200 --> 00:51:24.200]   But what about regulation?
[00:51:24.200 --> 00:51:31.640]   Would you, like, push for, I mean, besides just, like, more kind of more funding, would
[00:51:31.640 --> 00:51:40.080]   you want our government to enact some, like, laws now around, like, putting guardrails
[00:51:40.080 --> 00:51:44.040]   around, like, AI research or deployments?
[00:51:44.040 --> 00:51:47.960]   Or would you want them to kind of wait and see until there's more information?
[00:51:47.960 --> 00:51:49.840]   Yeah, it's a good question.
[00:51:49.840 --> 00:51:55.280]   I think the answer is, like, yes, they should do some stuff now and other areas they should
[00:51:55.280 --> 00:51:56.280]   wait and see.
[00:51:56.280 --> 00:52:00.320]   So, I think in areas like, you know, driverless cars, for example, there's, like, already
[00:52:00.320 --> 00:52:05.600]   a sort of clear reason to act quickly and develop sort of sector-specific regulations.
[00:52:05.600 --> 00:52:10.120]   I think another area where I'd like to see more progress in developing, you know, clear
[00:52:10.120 --> 00:52:15.200]   guidance for, you know, sort of entrepreneurs and others is health applications of AI.
[00:52:15.200 --> 00:52:21.320]   So where there's, like, some effort to sort of, you know, figure out how AI systems should
[00:52:21.320 --> 00:52:25.200]   flow through the FDA, for example, and, you know, what should that review process look
[00:52:25.200 --> 00:52:30.960]   like, I think that's an area where it would be beneficial to see sort of more investment
[00:52:30.960 --> 00:52:35.200]   in capacity and expertise in the government so that they have a better, you know, ability
[00:52:35.200 --> 00:52:37.960]   to process these applications.
[00:52:37.960 --> 00:52:43.800]   And also, just, like, clearer structures for getting AI systems through a sort of regulatory
[00:52:43.800 --> 00:52:46.440]   process like that, I think, could be very valuable.
[00:52:46.440 --> 00:52:51.200]   I don't know exactly, you know, what the details of that should look like, but, you know, generally,
[00:52:51.200 --> 00:52:54.960]   that's something where, you know, we do not want people putting out, you know, sort of
[00:52:54.960 --> 00:52:59.240]   health applications that, you know, are causing harm, but we also don't want, you know, we
[00:52:59.240 --> 00:53:01.480]   don't want zero health applications of AI.
[00:53:01.480 --> 00:53:05.360]   So I think that's an area where, like, some guardrails in order to give people, like,
[00:53:05.360 --> 00:53:09.680]   long-term sort of clarity so it's not just, like, a black box of, oh, will my system be
[00:53:09.680 --> 00:53:14.960]   able to be deployed in, like, a year or two, but sort of long-term signals I think would
[00:53:14.960 --> 00:53:15.960]   be very valuable.
[00:53:15.960 --> 00:53:16.960]   Yeah, that makes sense.
[00:53:16.960 --> 00:53:17.960]   Cool.
[00:53:17.960 --> 00:53:20.960]   Well, we always end with two questions.
[00:53:20.960 --> 00:53:23.400]   So I'm curious how you're going to, what you'll say about it.
[00:53:23.400 --> 00:53:28.240]   So the first one is kind of broad, but kind of off the top of your head, when you think
[00:53:28.240 --> 00:53:34.680]   about all the sort of, like, topics in machine learning, what's the one that you think people
[00:53:34.680 --> 00:53:35.680]   don't talk about enough?
[00:53:35.680 --> 00:53:39.400]   Like, what's the, like, the sort of, like, underrated topic that you'd like to kind of
[00:53:39.400 --> 00:53:42.640]   push people to go maybe learn a little more about?
[00:53:42.640 --> 00:53:49.200]   I mean, I think one super interesting thing is, like, detection of language model outputs.
[00:53:49.200 --> 00:53:53.600]   And I mean, this is sort of partially, you know, me being biased from working on GPT-2
[00:53:53.600 --> 00:53:57.680]   a lot, but there's actually been, like, a ton of super interesting research on things
[00:53:57.680 --> 00:54:02.760]   like how the sampling strategy relates to detectability and how, like, model size relates
[00:54:02.760 --> 00:54:03.760]   to detectability.
[00:54:03.760 --> 00:54:08.840]   And one area that's, like, super uncertain is how fine-tuning relates to all of this.
[00:54:08.840 --> 00:54:13.520]   You can imagine a world in which, like, GPT-2 or other systems are used to generate, like,
[00:54:13.520 --> 00:54:14.520]   homework.
[00:54:14.520 --> 00:54:17.960]   And this is just, someone was giving me an example of this on Twitter just earlier today
[00:54:17.960 --> 00:54:18.960]   that I saw.
[00:54:18.960 --> 00:54:22.560]   And, you know, it's like people are using it to cheat or to generate, you know, phishing
[00:54:22.560 --> 00:54:24.000]   emails or something like that.
[00:54:24.000 --> 00:54:29.320]   I think it's a really interesting question, like, what the sort of limiting case of, like,
[00:54:29.320 --> 00:54:30.680]   detection of language models is.
[00:54:30.680 --> 00:54:32.320]   Like, will it just always be, like, on it?
[00:54:32.320 --> 00:54:33.720]   Like, who wins that arms race?
[00:54:33.720 --> 00:54:35.560]   Yeah, who wins that arms race?
[00:54:35.560 --> 00:54:39.480]   And also, like, are there steps that you can take to, you know, make it more winnable from
[00:54:39.480 --> 00:54:43.640]   the defender's perspective, like, you know, watermarking or something like that?
[00:54:43.640 --> 00:54:48.280]   Because I think it's, you know, hopefully it's not, like, a super urgent issue in that
[00:54:48.280 --> 00:54:52.480]   hopefully, you know, there aren't that many people actively trying to, you know, weaponize
[00:54:52.480 --> 00:54:53.480]   these or whatever.
[00:54:53.480 --> 00:54:59.960]   But I think there's been a ton of work by, you know, Google, Allen Institute for AI,
[00:54:59.960 --> 00:55:05.240]   University of Washington, OpenAI, and elsewhere on trying to, like, push the state of the
[00:55:05.240 --> 00:55:06.240]   art forward.
[00:55:06.240 --> 00:55:11.400]   But we still don't have a general theory of, you know, how all these things work together.
[00:55:11.400 --> 00:55:12.400]   Interesting.
[00:55:12.400 --> 00:55:13.400]   What is the state of the art?
[00:55:13.400 --> 00:55:16.440]   Like, can you generally detect these models?
[00:55:16.440 --> 00:55:25.440]   Yeah, so the state of the art is, like, around 95%-ish detection of sort of where it's sort
[00:55:25.440 --> 00:55:27.320]   of, you know, the...
[00:55:27.320 --> 00:55:32.240]   Actually, so we use a Roberta model, actually, in order to detect GPT-2.
[00:55:32.240 --> 00:55:38.000]   So when we released our sort of, you know, newest and best system, we're actually using
[00:55:38.000 --> 00:55:41.080]   a smaller model to detect the outputs from the larger model.
[00:55:41.080 --> 00:55:45.440]   So that's one of the things, is that, like, you know, it isn't one of the early findings
[00:55:45.440 --> 00:55:50.400]   from folks at the University of Washington and Allen Institute was that, you know, models
[00:55:50.400 --> 00:55:51.400]   can detect themselves.
[00:55:51.400 --> 00:55:54.360]   And that this was sort of an argument for releasing.
[00:55:54.360 --> 00:55:58.080]   We eventually found that, like, sidestepping and trying a different model architecture
[00:55:58.080 --> 00:56:02.800]   and then using it to detect the original one sometimes worked better.
[00:56:02.800 --> 00:56:07.160]   One of the things that we found is that it's, you know, easier to detect the smaller models,
[00:56:07.160 --> 00:56:10.640]   which is maybe not surprising, because they're just sort of worse in some respects, and they
[00:56:10.640 --> 00:56:13.680]   might be making more errors that are sort of catchable.
[00:56:13.680 --> 00:56:17.560]   But yeah, that's sort of, you know, what our initial findings have been.
[00:56:17.560 --> 00:56:21.800]   But there are other people who have found, you know, other really interesting things,
[00:56:21.800 --> 00:56:26.880]   like what are humans picking up on versus, you know, what sort of AI systems are detecting.
[00:56:26.880 --> 00:56:32.200]   Like AI systems can detect weird stuff like distribution of adverbs versus, you know,
[00:56:32.200 --> 00:56:33.200]   nouns or something.
[00:56:33.200 --> 00:56:34.400]   And like, oh, therefore it's fake.
[00:56:34.400 --> 00:56:36.280]   But humans are not looking for those kinds of things.
[00:56:36.280 --> 00:56:40.320]   They're looking for, you know, is it saying something that's like incoherent or is it
[00:56:40.320 --> 00:56:41.960]   repeating itself, things like that.
[00:56:41.960 --> 00:56:46.800]   So we, you know, that's another interesting finding is that humans and, you know, machines
[00:56:46.800 --> 00:56:50.920]   are complementary in terms of how they detect things.
[00:56:50.920 --> 00:56:51.920]   I guess that makes sense.
[00:56:51.920 --> 00:56:57.560]   I mean, maybe I'm overconfident or out of date, but I feel like I can still detect these
[00:56:57.560 --> 00:57:01.920]   models pretty reliably by noticing that they make no sense.
[00:57:01.920 --> 00:57:11.840]   Yeah, so I mean, there's a good sort of like game that you can play to detect like fake
[00:57:11.840 --> 00:57:13.400]   versus real Trump tweets.
[00:57:13.400 --> 00:57:18.720]   And there are a few other sort of like quizzes like that, that I think are worth trying.
[00:57:18.720 --> 00:57:23.440]   Sometimes harder than you might think, at least like in the context of fine tuning.
[00:57:23.440 --> 00:57:29.580]   I guess also the Twitter genre is really kind of pushing us to, yeah, I could see Twitter
[00:57:29.580 --> 00:57:35.400]   being a tough medium to detect human versus machine, but I feel like over like a few paragraphs,
[00:57:35.400 --> 00:57:37.720]   I think I could do it, but now I want to try.
[00:57:37.720 --> 00:57:42.120]   Yeah, the over a few paragraphs part is critical because, yeah, that's like one of the like
[00:57:42.120 --> 00:57:47.840]   robust findings that both for humans and machines, it's like easier to make more contact or,
[00:57:47.840 --> 00:57:50.000]   you know, a longer duration.
[00:57:50.000 --> 00:57:54.320]   So that's potentially like, it's actually kind of good in that respect that Twitter,
[00:57:54.320 --> 00:57:58.040]   you know, recently went from 140 to 280 characters.
[00:57:58.040 --> 00:58:01.560]   It's like a sort of sweet spot in terms of like predictability.
[00:58:01.560 --> 00:58:04.000]   It makes it a bit easier.
[00:58:04.000 --> 00:58:10.320]   Well maybe our popular culture is nudging us more towards the language of machines than
[00:58:10.320 --> 00:58:14.080]   the machines are learning human language.
[00:58:14.080 --> 00:58:16.080]   We'll see.
[00:58:16.080 --> 00:58:20.960]   Okay, last question.
[00:58:20.960 --> 00:58:26.120]   So this is kind of more, I guess it's like a more for practitioners, but I think actually
[00:58:26.120 --> 00:58:27.680]   I'd be curious your take on it.
[00:58:27.680 --> 00:58:34.520]   So when you look at things at OpenAIR or elsewhere and you look from like sort of the conception
[00:58:34.520 --> 00:58:40.840]   to like, you know, creation and deployment of like a complete model, I guess like where
[00:58:40.840 --> 00:58:43.440]   do you see the bottlenecks happening?
[00:58:43.440 --> 00:58:47.160]   Like what's sort of the hardest piece of kind of like getting something done and out the
[00:58:47.160 --> 00:58:48.160]   door?
[00:58:48.160 --> 00:58:49.160]   Good question.
[00:58:49.160 --> 00:58:57.040]   I mean, a lot of, it seems like a lot of it is, you know, sort of, you know, finding the
[00:58:57.040 --> 00:59:01.360]   right hyper parameters and, you know, the kind of stuff that y'all are, you know, and
[00:59:01.360 --> 00:59:06.720]   trying to help out with is sort of getting the right hyper parameters.
[00:59:06.720 --> 00:59:10.760]   Obviously like compute is a bottleneck, you know, if you don't have enough compute, but
[00:59:10.760 --> 00:59:16.160]   I'm sort of thinking in the context of OpenAIR where you have a decent amount of compute.
[00:59:16.160 --> 00:59:18.400]   Data is definitely always an uphill battle.
[00:59:18.400 --> 00:59:23.960]   Like it's, you know, even if you're, you know, have sort of good engineers who are, you know,
[00:59:23.960 --> 00:59:27.440]   good at gathering and cleaning data, there's always room for improvement.
[00:59:27.440 --> 00:59:31.480]   So I'd say, I'm not sure that I call it a bottleneck, but like something to like push
[00:59:31.480 --> 00:59:34.520]   on is, you know, quality of data.
[00:59:34.520 --> 00:59:35.520]   Yeah.
[00:59:35.520 --> 00:59:39.480]   And just like ML, you know, related to the hyper parameter thing, it's just like ML weirdness.
[00:59:39.480 --> 00:59:45.320]   So it's hard to debug ML systems and like weird silent failures, which kind of related,
[00:59:45.320 --> 00:59:48.640]   you know, and also be weird silent issues and data sets as well.
[00:59:48.640 --> 00:59:52.880]   But yeah, all of those lead to various weird dynamics.
[00:59:52.880 --> 00:59:58.760]   I mean, I do want to say for the record that I think, I hope that our product helps people
[00:59:58.760 --> 01:00:01.940]   more with actually the kind of collaboration that you're talking about than like specifically
[01:00:01.940 --> 01:00:03.600]   tuning the hyper parameters.
[01:00:03.600 --> 01:00:07.920]   So I think like both are, you know, important, but I'm really curious actually, because like
[01:00:07.920 --> 01:00:14.120]   you look at like, say that like we watched from the sidelines from far away OpenAIR trying
[01:00:14.120 --> 01:00:20.200]   to build like the robot hand and manipulate a Rubik's cube say, and just from like casually
[01:00:20.200 --> 01:00:24.680]   talking to folks on the team, it seemed like people felt like you were really close and
[01:00:24.680 --> 01:00:29.360]   then, you know, it took like two years, but then it actually got done, but over like a
[01:00:29.360 --> 01:00:30.360]   long period of time.
[01:00:30.360 --> 01:00:34.840]   And what happens in that like year or two?
[01:00:34.840 --> 01:00:38.760]   Like from your perspective, like what's like going on?
[01:00:38.760 --> 01:00:42.320]   Like it can't just be like tweaking hyper parameters, can it be?
[01:00:42.320 --> 01:00:43.320]   Yeah.
[01:00:43.320 --> 01:00:46.480]   I mean, I should emphasize that I'm not a technical person.
[01:00:46.480 --> 01:00:47.920]   So I think, yeah.
[01:00:47.920 --> 01:00:53.160]   And it's not, yeah, not just like, you know, sort of silly, just sort of tweaking the hyper
[01:00:53.160 --> 01:00:54.160]   parameters.
[01:00:54.160 --> 01:00:57.920]   In a way, I think you might have like a clearer perspective than someone who's right in there
[01:00:57.920 --> 01:01:00.360]   doing it, because you're kind of like watching it from the outside.
[01:01:00.360 --> 01:01:04.040]   Like it's not, yeah, I'm curious what you think.
[01:01:04.040 --> 01:01:05.040]   Yeah.
[01:01:05.040 --> 01:01:09.000]   I mean, in the case of robotics, like from my perspective, it felt like a fairly sort
[01:01:09.000 --> 01:01:12.520]   of smooth trajectory of like, yeah, every few months there'd be some kind of, you know,
[01:01:12.520 --> 01:01:14.200]   that seemed a bit more impressive.
[01:01:14.200 --> 01:01:18.200]   And it wasn't like, you know, it's sort of nothing happened for years.
[01:01:18.200 --> 01:01:21.720]   It was like, you know, maybe didn't solve the original problem for a while.
[01:01:21.720 --> 01:01:27.080]   But it seemed like there's always, you know, some area to push on.
[01:01:27.080 --> 01:01:32.760]   And yeah, I think you mentioned collaboration, I would say, yeah, just like knowing what
[01:01:32.760 --> 01:01:35.920]   sort of techniques to apply is like another part of it.
[01:01:35.920 --> 01:01:39.000]   So it's not, you know, the hyper parameters per se, but like, you know, knowing how to
[01:01:39.000 --> 01:01:43.760]   get, you know, transformers to work in, you know, in some new context, I think it's like,
[01:01:43.760 --> 01:01:44.760]   you know, non obvious.
[01:01:44.760 --> 01:01:49.280]   And, you know, the fact that there's not always sufficient information in papers to reproduce
[01:01:49.280 --> 01:01:53.560]   things sort of requires you to do a lot of, you know, sort of trial and error.
[01:01:53.560 --> 01:01:57.360]   And that's just sort of, you know, how ML research seems to work is like trying lots
[01:01:57.360 --> 01:02:03.120]   of stuff with hyper parameters and architecture and data, and so forth, and just take the
[01:02:03.120 --> 01:02:04.120]   time.
[01:02:04.120 --> 01:02:06.160]   So thank you so much for taking the time to talk.
[01:02:06.160 --> 01:02:10.720]   We'll put some of the papers you mentioned in the show notes.
[01:02:10.720 --> 01:02:11.720]   Yeah, that was really fun.
[01:02:11.720 --> 01:02:12.720]   I really appreciate it.
[01:02:12.720 --> 01:02:15.000]   It's been fun talking with you.
[01:02:15.000 --> 01:02:17.580]   (gentle music)
[01:02:17.580 --> 01:02:20.160]   (gentle music)
[01:02:20.160 --> 01:02:22.740]   (gentle music)
[01:02:22.740 --> 01:02:24.800]   you

