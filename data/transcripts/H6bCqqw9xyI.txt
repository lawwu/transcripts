
[00:00:00.000 --> 00:00:03.180]   In one of the early videos in this series on Langtrain,
[00:00:03.180 --> 00:00:05.760]   we talked about retrieval augmentation.
[00:00:05.760 --> 00:00:10.520]   And one of the most commonly asked questions
[00:00:10.520 --> 00:00:14.680]   from that is how can the large language model know
[00:00:14.680 --> 00:00:17.880]   when to actually search through the vector database?
[00:00:17.880 --> 00:00:20.400]   Because obviously if you're just chatting
[00:00:20.400 --> 00:00:21.400]   with the chat button,
[00:00:21.400 --> 00:00:24.000]   it doesn't need to refer to any external knowledge.
[00:00:24.000 --> 00:00:27.480]   At that point, there's no reason for the model
[00:00:27.480 --> 00:00:31.600]   to actually go to a vector database and retrieve information.
[00:00:31.600 --> 00:00:35.400]   So how can we make it an optional thing
[00:00:35.400 --> 00:00:39.200]   where we're not always querying our vector database?
[00:00:39.200 --> 00:00:41.520]   Well, I mean, there's kind of two options.
[00:00:41.520 --> 00:00:44.600]   The first option is you just actually stick with that
[00:00:44.600 --> 00:00:47.540]   and you just set like a similarity threshold
[00:00:47.540 --> 00:00:51.900]   where if a retrieved context is below that threshold,
[00:00:51.900 --> 00:00:54.760]   you just don't include it as the added information
[00:00:54.760 --> 00:00:58.960]   within your query to the large language model.
[00:00:58.960 --> 00:01:00.160]   And the second option,
[00:01:00.160 --> 00:01:02.360]   which is what we're going to talk about today
[00:01:02.360 --> 00:01:07.360]   is actually using a retrieval tool as part of a AI agent.
[00:01:07.360 --> 00:01:12.160]   So if you have been following along with this series,
[00:01:12.160 --> 00:01:15.080]   we're essentially going to take what we spoke about last,
[00:01:15.080 --> 00:01:18.080]   which is agents and what we spoke about earlier
[00:01:18.080 --> 00:01:20.400]   in the series, which is retrieval augmentation.
[00:01:20.400 --> 00:01:22.600]   And we're going to put them both together.
[00:01:22.600 --> 00:01:26.020]   So, I mean, let's jump straight into the code.
[00:01:26.020 --> 00:01:29.160]   So we have this notebook that you can follow along with.
[00:01:29.160 --> 00:01:30.720]   There'll be a link to this
[00:01:30.720 --> 00:01:33.360]   somewhere near the top of the video right now.
[00:01:33.360 --> 00:01:35.200]   And the first thing we need to do is obviously
[00:01:35.200 --> 00:01:37.160]   install any prerequisite libraries.
[00:01:37.160 --> 00:01:40.880]   So we have OpenAI, Pinecone, LangChain, TicToken,
[00:01:40.880 --> 00:01:43.000]   and the HungFace datasets.
[00:01:43.000 --> 00:01:44.840]   So, yeah, we run those.
[00:01:44.840 --> 00:01:46.960]   I've already run it, so I'm not going to run it again.
[00:01:46.960 --> 00:01:51.160]   First thing I'm going to do is actually load our dataset.
[00:01:51.160 --> 00:01:53.640]   Now, this is the dataset we're going to be using
[00:01:53.640 --> 00:01:57.040]   to create our knowledge base.
[00:01:57.040 --> 00:02:00.920]   It's a basically pre-processed dataset.
[00:02:00.920 --> 00:02:04.400]   We won't need to do any of the chunking
[00:02:04.400 --> 00:02:06.040]   or anything that we would usually do.
[00:02:06.040 --> 00:02:07.240]   And that's on purpose.
[00:02:07.240 --> 00:02:08.680]   I want this to be pretty simple
[00:02:08.680 --> 00:02:11.600]   and we can focus more on the agent side of things
[00:02:11.600 --> 00:02:13.760]   rather than the data prep side of things.
[00:02:13.760 --> 00:02:17.240]   So yeah, using the Sanford question answering dataset.
[00:02:17.240 --> 00:02:20.640]   And within this, so the reason that we don't need to do
[00:02:20.640 --> 00:02:23.400]   any of this data pre-processing that we usually do
[00:02:23.400 --> 00:02:25.840]   is if we just run this.
[00:02:25.840 --> 00:02:28.920]   So just conveying it into a pandas data frame
[00:02:28.920 --> 00:02:31.440]   is because we have these contexts
[00:02:31.440 --> 00:02:34.160]   and each context is roughly a paragraph
[00:02:34.160 --> 00:02:36.520]   or a little bit more of text.
[00:02:36.520 --> 00:02:38.560]   And that's what we're going to be indexing
[00:02:38.560 --> 00:02:40.000]   within our knowledge base.
[00:02:40.000 --> 00:02:43.240]   Typically, what you'll find is if you're, for example,
[00:02:43.240 --> 00:02:45.040]   working with PDFs and you want to store those
[00:02:45.040 --> 00:02:46.400]   in your knowledge base,
[00:02:46.400 --> 00:02:49.720]   you'll need to chunk that long piece of text
[00:02:49.720 --> 00:02:51.720]   into smaller chunks.
[00:02:51.720 --> 00:02:54.400]   This basically is already chunked.
[00:02:54.400 --> 00:02:56.640]   It just makes our life a little bit easier.
[00:02:56.640 --> 00:02:59.480]   But one thing we do need to do is actually deduplicate this
[00:02:59.480 --> 00:03:01.560]   because we have many of the same contexts
[00:03:01.560 --> 00:03:02.840]   over and over again.
[00:03:02.840 --> 00:03:05.080]   So we just do that here.
[00:03:05.080 --> 00:03:07.600]   So just drop duplicates on the subset.
[00:03:07.600 --> 00:03:09.680]   We're just going to keep the first of each one of those
[00:03:09.680 --> 00:03:11.280]   and do all that in place.
[00:03:11.280 --> 00:03:14.400]   And now you can see that the contexts are now different.
[00:03:14.400 --> 00:03:15.560]   Okay, cool.
[00:03:15.560 --> 00:03:19.480]   So I mean, that's the data prep side of things done.
[00:03:19.480 --> 00:03:21.560]   And what we're going to want to do now
[00:03:21.560 --> 00:03:24.320]   is initialize both the embedding model
[00:03:24.320 --> 00:03:26.080]   and our vector database.
[00:03:26.080 --> 00:03:28.000]   So embedding model first.
[00:03:28.000 --> 00:03:30.400]   So we're going to be using TextEmbeddingArda002
[00:03:30.400 --> 00:03:31.680]   from OpenAI.
[00:03:31.680 --> 00:03:34.560]   Again, you can use like any embedding model you want.
[00:03:34.560 --> 00:03:36.440]   It doesn't need to be OpenAI,
[00:03:36.440 --> 00:03:39.080]   doesn't need to be TextEmbeddingArda002.
[00:03:39.080 --> 00:03:41.640]   Okay, so I'm going to enter my API key there.
[00:03:41.640 --> 00:03:45.880]   So the API key can go from platform.openai.com.
[00:03:45.880 --> 00:03:48.600]   And then we need our Pinecone API key
[00:03:48.600 --> 00:03:49.880]   and Pinecone environment.
[00:03:49.880 --> 00:03:53.360]   So I'm going to go into my dashboard and grab those.
[00:03:53.360 --> 00:03:55.840]   So that is app.pinecone.io.
[00:03:55.840 --> 00:03:57.360]   You go to API keys,
[00:03:57.360 --> 00:04:00.120]   and what I want to do is just copy the key value
[00:04:00.120 --> 00:04:02.000]   and remember my environment here.
[00:04:02.000 --> 00:04:04.280]   So I've got us-west1-gcp.
[00:04:04.280 --> 00:04:05.920]   Okay, yours might vary.
[00:04:05.920 --> 00:04:08.520]   So make sure you actually check this
[00:04:08.520 --> 00:04:11.080]   for your environment variable.
[00:04:11.080 --> 00:04:12.760]   So now I'm going to run this.
[00:04:12.760 --> 00:04:16.040]   Enter my API key and then my environment.
[00:04:16.040 --> 00:04:20.000]   So us-west1-gcp.
[00:04:20.000 --> 00:04:24.400]   Cool, so this is going to first initialize that index.
[00:04:24.400 --> 00:04:26.200]   All right, so here.
[00:04:26.200 --> 00:04:30.000]   Sorry, this initializes the connection to Pinecone,
[00:04:30.000 --> 00:04:31.440]   not the index.
[00:04:31.440 --> 00:04:35.360]   And then if we don't have an existing index
[00:04:35.360 --> 00:04:37.640]   with this index name,
[00:04:37.640 --> 00:04:40.040]   then it initializes the index.
[00:04:40.040 --> 00:04:42.160]   Now for the metric, we're using dot product.
[00:04:42.160 --> 00:04:46.160]   That is specific to text embedding R002.
[00:04:46.160 --> 00:04:48.280]   A lot of models actually use cosine.
[00:04:48.280 --> 00:04:51.360]   So if you're not sure what to use,
[00:04:51.360 --> 00:04:54.160]   then I'd recommend you just use cosine
[00:04:54.160 --> 00:04:57.640]   and see how it works, if it works.
[00:04:57.640 --> 00:04:59.560]   And also dimensionality.
[00:04:59.560 --> 00:05:02.120]   So again, this is something specific to each model.
[00:05:02.120 --> 00:05:06.760]   For text embedding R002, it is 1536.
[00:05:06.760 --> 00:05:09.560]   Okay, cool, I will let that run.
[00:05:09.560 --> 00:05:11.120]   Okay, that's initialized.
[00:05:11.120 --> 00:05:12.480]   And then we connect to the index.
[00:05:12.480 --> 00:05:15.080]   So again, passing the same index name.
[00:05:15.080 --> 00:05:16.480]   I'm using gRPC index.
[00:05:16.480 --> 00:05:18.640]   You can also just use index.
[00:05:18.640 --> 00:05:21.120]   But gRPC index is just more reliable
[00:05:21.120 --> 00:05:22.280]   and a little bit faster.
[00:05:22.280 --> 00:05:24.240]   So I go with that.
[00:05:24.240 --> 00:05:26.160]   And then we're going to describe the index stats
[00:05:26.160 --> 00:05:28.320]   so we can see what's in there at the moment.
[00:05:28.320 --> 00:05:30.400]   And we should see that total vector count is zero
[00:05:30.400 --> 00:05:33.360]   'cause we haven't added anything in there yet.
[00:05:33.360 --> 00:05:35.480]   So, okay, that's great.
[00:05:35.480 --> 00:05:37.480]   And then we move on to indexing.
[00:05:37.480 --> 00:05:39.480]   So this is just where we're going to add
[00:05:39.480 --> 00:05:43.640]   all of these embeddings into Pinecone.
[00:05:43.640 --> 00:05:48.640]   Now, we do this directly with the Pinecone client
[00:05:48.640 --> 00:05:50.920]   and the gRPC index that we have here,
[00:05:50.920 --> 00:05:52.400]   rather than through LineChain
[00:05:52.400 --> 00:05:55.080]   because with LineChain, it's just slower.
[00:05:55.080 --> 00:05:58.840]   So I find this is just the better way of doing it.
[00:05:58.840 --> 00:06:01.120]   So we set our batch size to 100.
[00:06:01.120 --> 00:06:03.040]   That means we're going to just encode
[00:06:03.040 --> 00:06:06.040]   a hundred records or contexts at once.
[00:06:06.040 --> 00:06:09.920]   And we're going to add those to Pinecone
[00:06:09.920 --> 00:06:12.520]   in batches of a hundred at once as well.
[00:06:12.520 --> 00:06:14.480]   So then we just loop through our dataset.
[00:06:14.480 --> 00:06:18.080]   We get the batch, we get metadata.
[00:06:18.080 --> 00:06:19.760]   So metadata is just going to contain
[00:06:19.760 --> 00:06:22.640]   the title and the context.
[00:06:22.640 --> 00:06:26.440]   So if we come up here, title is this,
[00:06:26.440 --> 00:06:27.680]   and this is the context.
[00:06:27.680 --> 00:06:28.960]   Okay, looks good.
[00:06:28.960 --> 00:06:31.960]   And then, okay, where are we?
[00:06:31.960 --> 00:06:33.440]   Yeah, let's just run this actually.
[00:06:33.440 --> 00:06:35.840]   Okay, so we're creating our metadata.
[00:06:35.840 --> 00:06:39.480]   We get our context from the current batch,
[00:06:39.480 --> 00:06:43.680]   and then we embed those using text embedding R002.
[00:06:43.680 --> 00:06:48.520]   Okay, so these are like the chunks of text
[00:06:48.520 --> 00:06:49.360]   that we're passing in.
[00:06:49.360 --> 00:06:52.440]   We usually call them either context or documents
[00:06:52.440 --> 00:06:54.600]   or also passages as well.
[00:06:54.600 --> 00:06:56.200]   You can also call them that.
[00:06:56.200 --> 00:06:59.960]   They get referred to as any of those.
[00:06:59.960 --> 00:07:03.920]   Okay, and then what we do is we get our IDs.
[00:07:03.920 --> 00:07:07.200]   So the ID, again, it's just this here.
[00:07:07.200 --> 00:07:10.160]   So as unique ID for every item, that's important.
[00:07:10.160 --> 00:07:14.400]   Otherwise we're going to overwrite records within Pinecone.
[00:07:14.400 --> 00:07:16.120]   And then we just add everything to Pinecone.
[00:07:16.120 --> 00:07:20.080]   So we basically just take our IDs, okay?
[00:07:20.080 --> 00:07:23.640]   IDs, embeddings, and the metadata,
[00:07:23.640 --> 00:07:25.080]   and each of these is a list,
[00:07:25.080 --> 00:07:26.480]   and we zip those all together
[00:07:26.480 --> 00:07:29.240]   so that we get a list of tuples
[00:07:29.240 --> 00:07:32.320]   where each tuple contains a single record
[00:07:32.320 --> 00:07:36.000]   and that record's ID, embedding, and metadata.
[00:07:36.000 --> 00:07:40.440]   Okay, so I will fast forward to let this finish.
[00:07:40.440 --> 00:07:41.920]   Okay, so it's finished.
[00:07:41.920 --> 00:07:44.280]   And again, we can describe index stats,
[00:07:44.280 --> 00:07:47.400]   and we should see now that it has been populated
[00:07:47.400 --> 00:07:48.360]   with vectors, okay?
[00:07:48.360 --> 00:07:52.600]   So we have almost 19,000 vectors in there now, or records.
[00:07:52.600 --> 00:07:53.800]   Okay, cool.
[00:07:53.800 --> 00:07:57.120]   So up to here, we've been using the Pinecone client
[00:07:57.120 --> 00:07:57.960]   to do this.
[00:07:57.960 --> 00:07:59.640]   Again, like I said, it's just faster
[00:07:59.640 --> 00:08:03.080]   than using the implementation in LineChain at the moment,
[00:08:03.080 --> 00:08:05.360]   but now we're going to switch back to LineChain
[00:08:05.360 --> 00:08:09.120]   because we want to be able to use the conversational agent
[00:08:09.120 --> 00:08:13.360]   and all the other tooling that comes with LineChain.
[00:08:13.360 --> 00:08:16.640]   So what we're going to do is reinitialize our index,
[00:08:16.640 --> 00:08:20.040]   and we're going to use a normal index and not gRPC
[00:08:20.040 --> 00:08:23.360]   because that is what is implemented with LineChain.
[00:08:23.360 --> 00:08:24.840]   So we initialize that,
[00:08:24.840 --> 00:08:27.680]   and then we initialize a vector sort object,
[00:08:27.680 --> 00:08:32.080]   which is basically LineChain's version of the index
[00:08:32.080 --> 00:08:33.200]   that we create here.
[00:08:33.200 --> 00:08:36.440]   It just includes the embedding in there as well.
[00:08:36.440 --> 00:08:39.680]   And we'll also, so the text field, that's important.
[00:08:39.680 --> 00:08:42.720]   That's just the field within your metadata
[00:08:42.720 --> 00:08:46.040]   that contains the text for each record.
[00:08:46.040 --> 00:08:49.000]   So for us, that is text because we set it here.
[00:08:49.000 --> 00:08:52.600]   So let's run this.
[00:08:52.600 --> 00:08:57.360]   And like we did before in the previous retrieval video,
[00:08:57.360 --> 00:09:00.360]   we can test that this is working
[00:09:00.360 --> 00:09:04.000]   by using the similarity search method with our query here.
[00:09:04.000 --> 00:09:05.960]   So when was the College of Engineering
[00:09:05.960 --> 00:09:09.520]   in the University of Notre Dame established?
[00:09:09.520 --> 00:09:11.040]   And yeah, we pass that,
[00:09:11.040 --> 00:09:12.160]   and we say you want to return
[00:09:12.160 --> 00:09:15.880]   to top three most relevant documents,
[00:09:15.880 --> 00:09:18.080]   passages, context, whatever you want to call them.
[00:09:18.080 --> 00:09:21.960]   And we can see that we get, so this is a document here.
[00:09:21.960 --> 00:09:25.280]   So we have, I think that's probably relevant.
[00:09:25.280 --> 00:09:28.240]   This one is definitely relevant.
[00:09:28.240 --> 00:09:31.480]   And we have another document there as well.
[00:09:31.480 --> 00:09:33.720]   Okay, so we get those three results.
[00:09:33.720 --> 00:09:34.560]   Looks good.
[00:09:34.560 --> 00:09:38.800]   So let's now move on to the agent part of things.
[00:09:38.800 --> 00:09:42.840]   Okay, so our conversational agent needs our chat,
[00:09:42.840 --> 00:09:44.000]   large language model,
[00:09:44.000 --> 00:09:47.280]   conversational memory, and the retrieval QA chain.
[00:09:47.280 --> 00:09:51.520]   So we import each of those here, right?
[00:09:51.520 --> 00:09:53.680]   And let me explain what those actually are.
[00:09:53.680 --> 00:09:56.280]   So we have the chat LLM,
[00:09:56.280 --> 00:09:59.840]   that is basically is chat GPT, okay?
[00:09:59.840 --> 00:10:04.200]   So chat LLMs, they just receive the input
[00:10:04.200 --> 00:10:06.760]   in a different format to normal LLMs.
[00:10:06.760 --> 00:10:09.640]   That is more inducive to a chat
[00:10:09.640 --> 00:10:13.000]   like a stream of data or information.
[00:10:13.000 --> 00:10:15.480]   And then we have our conversational memory.
[00:10:15.480 --> 00:10:16.920]   So this is important.
[00:10:16.920 --> 00:10:18.080]   So we have our memory key.
[00:10:18.080 --> 00:10:19.440]   We're using chat history
[00:10:19.440 --> 00:10:24.440]   because that is what the memory is referred to
[00:10:24.440 --> 00:10:29.320]   in the, I think the conversational agent component.
[00:10:29.320 --> 00:10:31.360]   So whenever you're using conversational agent,
[00:10:31.360 --> 00:10:32.920]   you need to make sure you set memory key
[00:10:32.920 --> 00:10:34.760]   equal to chat history here.
[00:10:34.760 --> 00:10:38.240]   We're going to remember the previous five interactions
[00:10:38.240 --> 00:10:41.440]   and yeah, that's our conversational memory, okay?
[00:10:41.440 --> 00:10:45.680]   So after that, we set up our retrieval Q&A chain.
[00:10:45.680 --> 00:10:48.400]   So for that, we need our chat LLM.
[00:10:48.400 --> 00:10:50.280]   We set the chain type here to stuff,
[00:10:50.280 --> 00:10:54.320]   so that basically means when you are retrieving the,
[00:10:54.320 --> 00:10:57.200]   I think the three items from the vector store,
[00:10:57.200 --> 00:11:00.440]   we're going to just place them as is
[00:11:00.440 --> 00:11:03.120]   into the retrieval Q&A.
[00:11:03.120 --> 00:11:05.920]   So we're gonna kind of like stuff them all into the context
[00:11:05.920 --> 00:11:08.600]   rather than doing like any fancy summarization
[00:11:08.600 --> 00:11:10.080]   or anything like that, okay?
[00:11:10.080 --> 00:11:11.720]   And then we set our retriever
[00:11:11.720 --> 00:11:14.480]   and the retriever is our vector store,
[00:11:14.480 --> 00:11:16.680]   but as a retriever, okay?
[00:11:16.680 --> 00:11:19.920]   It was just a slightly different class or object.
[00:11:19.920 --> 00:11:21.680]   All right, so we run that.
[00:11:21.680 --> 00:11:24.360]   And then with those, we can generate our answer, okay?
[00:11:24.360 --> 00:11:26.920]   So we run and we're using the same query here.
[00:11:26.920 --> 00:11:28.680]   So you see that was a query.
[00:11:28.680 --> 00:11:30.280]   Let me come up here.
[00:11:30.280 --> 00:11:32.200]   When was the College of Engineering
[00:11:32.200 --> 00:11:34.480]   and University of Notre Dame established?
[00:11:34.480 --> 00:11:38.520]   We come down and the answer is the College of Engineering
[00:11:38.520 --> 00:11:41.960]   was established in 1920 at the University of Notre Dame.
[00:11:41.960 --> 00:11:43.400]   Okay, so cool.
[00:11:43.400 --> 00:11:46.360]   We get the answer and it is generated
[00:11:46.360 --> 00:11:51.360]   by our GPT 3.5 turbo model based on the context
[00:11:51.360 --> 00:11:54.760]   that we retrieved from our vector store, okay?
[00:11:54.760 --> 00:11:57.920]   So basically based on these three documents here.
[00:11:57.920 --> 00:12:02.920]   Cool, now that's good, but that isn't the whole thing yet.
[00:12:02.920 --> 00:12:06.160]   That's actually just a retrieval Q&A chain, okay?
[00:12:06.160 --> 00:12:08.360]   That isn't a conversational agent.
[00:12:08.360 --> 00:12:11.000]   To create our conversational agent,
[00:12:11.000 --> 00:12:15.080]   we actually need to convert our retrieval Q&A chain
[00:12:15.080 --> 00:12:19.000]   into a tool that the agent can use.
[00:12:19.000 --> 00:12:20.440]   So that's what we're doing here.
[00:12:20.440 --> 00:12:23.800]   We get a tools list, which is what we'll pass to our agent.
[00:12:23.800 --> 00:12:26.320]   And we can include multiple tools in there.
[00:12:26.320 --> 00:12:27.520]   That's why it's a list.
[00:12:27.520 --> 00:12:30.640]   But we're only actually using one tool in this case.
[00:12:30.640 --> 00:12:32.560]   So we define the name of that tool.
[00:12:32.560 --> 00:12:34.320]   We're gonna call it the knowledge base.
[00:12:34.320 --> 00:12:36.960]   We pass in the function that runs
[00:12:36.960 --> 00:12:39.800]   when the agent calls this chain,
[00:12:39.800 --> 00:12:42.800]   which is just Q&A run like we did here.
[00:12:42.800 --> 00:12:44.680]   And then we set a description.
[00:12:44.680 --> 00:12:46.400]   So this description is important
[00:12:46.400 --> 00:12:48.080]   because it is using this description
[00:12:48.080 --> 00:12:52.240]   that the conversational agent will decide
[00:12:52.240 --> 00:12:55.120]   which tool to use if you have multiple tools,
[00:12:55.120 --> 00:12:57.080]   or also just whether to use this tool.
[00:12:57.080 --> 00:12:58.400]   So we say use this tool
[00:12:58.400 --> 00:13:00.120]   when answering general knowledge queries
[00:13:00.120 --> 00:13:02.320]   to get more information about the topic.
[00:13:02.320 --> 00:13:05.200]   Okay, which I think is a pretty clear description
[00:13:05.200 --> 00:13:07.840]   as to when to use this tool, okay?
[00:13:07.840 --> 00:13:10.800]   And yeah, so from there, we initialize our agent.
[00:13:10.800 --> 00:13:12.520]   We're using this chat conversational
[00:13:12.520 --> 00:13:14.280]   react description agent.
[00:13:14.280 --> 00:13:16.520]   We pass in our tools, our LLM,
[00:13:16.520 --> 00:13:18.280]   but those just means we're going to get a load
[00:13:18.280 --> 00:13:20.880]   of printed output, which helps us just see
[00:13:20.880 --> 00:13:22.560]   what is actually going on.
[00:13:22.560 --> 00:13:25.720]   Max iterations defines a number of times
[00:13:25.720 --> 00:13:30.720]   the agent can use, basically go through a tool usage loop,
[00:13:30.720 --> 00:13:32.920]   which is, we're going to limit it to three.
[00:13:32.920 --> 00:13:36.040]   Otherwise it can, what can happen is it can keep going
[00:13:36.040 --> 00:13:38.280]   to tools over and over again and get stuck
[00:13:38.280 --> 00:13:40.960]   in an infinite loop, which we don't want.
[00:13:40.960 --> 00:13:45.520]   The model is going to decide when to stop generation.
[00:13:45.520 --> 00:13:48.080]   And we also need to include our conversational memory
[00:13:48.080 --> 00:13:50.640]   because this is a conversational agent.
[00:13:50.640 --> 00:13:54.680]   Okay, we run that and now our agent is ready to use.
[00:13:54.680 --> 00:13:57.800]   So we can, let's pass in that query that we used before.
[00:13:57.800 --> 00:14:01.760]   So the, this one was the, let me run it and we'll see.
[00:14:01.760 --> 00:14:04.680]   Okay, so this action input here is actually
[00:14:04.680 --> 00:14:09.400]   the generated question that the LLM is passing to our tool.
[00:14:09.400 --> 00:14:12.440]   So it might not be exactly what we put in
[00:14:12.440 --> 00:14:14.960]   or it might actually be the same.
[00:14:14.960 --> 00:14:15.800]   It depends.
[00:14:15.800 --> 00:14:19.720]   Basically, sometimes the agent will reformat this
[00:14:19.720 --> 00:14:22.400]   into a question that it thinks is going
[00:14:22.400 --> 00:14:24.760]   to get us better results.
[00:14:24.760 --> 00:14:27.440]   So our question is, when was the College of Engineering
[00:14:27.440 --> 00:14:30.200]   at the University of Notre Dame established?
[00:14:30.200 --> 00:14:32.760]   And the observation, because it refers
[00:14:32.760 --> 00:14:34.120]   to the knowledge base for this.
[00:14:34.120 --> 00:14:37.280]   So the observation is the College of Engineering
[00:14:37.280 --> 00:14:40.400]   at the University of Notre Dame was established in 1920.
[00:14:40.400 --> 00:14:41.240]   Okay.
[00:14:41.240 --> 00:14:43.880]   Then the agent is like, okay,
[00:14:43.880 --> 00:14:46.360]   I think I have enough information to answer this question.
[00:14:46.360 --> 00:14:48.400]   So it says final answer.
[00:14:48.400 --> 00:14:51.040]   And then the final answer it returns is this.
[00:14:51.040 --> 00:14:51.880]   Okay.
[00:14:51.880 --> 00:14:53.640]   Which is the same, same thing.
[00:14:53.640 --> 00:14:55.600]   Right, and then we can see that here.
[00:14:55.600 --> 00:14:57.560]   So that final output.
[00:14:57.560 --> 00:14:58.480]   Okay.
[00:14:58.480 --> 00:15:00.560]   Now, what if we ask it a,
[00:15:00.560 --> 00:15:02.400]   something that is not general knowledge.
[00:15:02.400 --> 00:15:04.560]   So what is two times seven?
[00:15:04.560 --> 00:15:07.600]   See what it will say.
[00:15:07.600 --> 00:15:08.440]   Okay.
[00:15:08.440 --> 00:15:11.160]   And you see, it doesn't decide to use a knowledge base here.
[00:15:11.160 --> 00:15:13.120]   It knows that it doesn't need to.
[00:15:13.120 --> 00:15:15.080]   So it just goes straight to final answer
[00:15:15.080 --> 00:15:18.200]   and it tells us it is 14.
[00:15:18.200 --> 00:15:19.480]   Okay.
[00:15:19.480 --> 00:15:22.400]   Now let's try some more.
[00:15:22.400 --> 00:15:24.800]   So I'm going to ask it to tell me some facts
[00:15:24.800 --> 00:15:26.840]   about the University of Notre Dame.
[00:15:26.840 --> 00:15:29.360]   So it knows to use a knowledge base
[00:15:29.360 --> 00:15:32.160]   and to pass in University of Notre Dame facts.
[00:15:32.160 --> 00:15:35.760]   So you can see here that it's not just passing in
[00:15:35.760 --> 00:15:37.200]   what I wrote here.
[00:15:37.200 --> 00:15:39.560]   It's actually passing in a generated version
[00:15:39.560 --> 00:15:42.600]   that it thinks will basically return better results.
[00:15:42.600 --> 00:15:43.440]   Okay.
[00:15:43.440 --> 00:15:45.520]   And what it got was,
[00:15:45.520 --> 00:15:49.680]   obviously it got some of the context that we saw before.
[00:15:49.680 --> 00:15:52.400]   And based on the information in those contexts,
[00:15:52.400 --> 00:15:55.960]   it's come up with all of this, all these facts, right?
[00:15:55.960 --> 00:15:56.800]   Which is quite a lot.
[00:15:56.800 --> 00:15:58.520]   So it's given us this bullet point list.
[00:15:58.520 --> 00:15:59.760]   And then the final answer.
[00:15:59.760 --> 00:16:02.240]   So based on this bullet point list,
[00:16:02.240 --> 00:16:04.520]   it's given us this like paragraph.
[00:16:04.520 --> 00:16:07.040]   So yeah, you can see.
[00:16:07.040 --> 00:16:08.440]   I haven't been through this.
[00:16:08.440 --> 00:16:10.280]   So I'm not sure how correct it is,
[00:16:10.280 --> 00:16:13.080]   but we can see that it is using the tool
[00:16:13.080 --> 00:16:16.200]   and it looks relatively accurate, I think.
[00:16:16.200 --> 00:16:18.200]   Okay.
[00:16:18.200 --> 00:16:19.280]   You can also see that here.
[00:16:19.280 --> 00:16:21.320]   So this output.
[00:16:21.320 --> 00:16:22.160]   Cool.
[00:16:22.160 --> 00:16:23.000]   Looks good.
[00:16:23.000 --> 00:16:25.000]   And what we can do is,
[00:16:25.000 --> 00:16:27.280]   because this is a conversational agent,
[00:16:27.280 --> 00:16:29.880]   we can actually ask it questions
[00:16:29.880 --> 00:16:33.520]   that are dependent on previous interactions.
[00:16:33.520 --> 00:16:37.160]   Like, can you summarize these facts in two short sentences?
[00:16:37.160 --> 00:16:38.880]   So we're not telling it which facts,
[00:16:38.880 --> 00:16:42.320]   we're just kind of referring to the previous interaction.
[00:16:42.320 --> 00:16:46.720]   So let's run that and see what it comes up with.
[00:16:46.720 --> 00:16:47.560]   Okay.
[00:16:47.560 --> 00:16:50.160]   I'm just gonna read it here 'cause it's a little bit easier.
[00:16:50.160 --> 00:16:51.000]   So yeah.
[00:16:51.000 --> 00:16:51.840]   We got our output here.
[00:16:51.840 --> 00:16:54.520]   The University of Notre Dame is a Catholic research
[00:16:54.520 --> 00:16:58.760]   university located in South Bend, Indiana, USA,
[00:16:58.760 --> 00:17:03.520]   is consistently ranked among the top 20 universities
[00:17:03.520 --> 00:17:07.960]   in the United States and as a major global university.
[00:17:07.960 --> 00:17:08.800]   Cool.
[00:17:08.800 --> 00:17:11.040]   So it managed to kind of summarize,
[00:17:11.040 --> 00:17:12.440]   obviously not good at everything.
[00:17:12.440 --> 00:17:14.920]   That would be pretty difficult,
[00:17:14.920 --> 00:17:17.840]   but I think it has a good summary there.
[00:17:17.840 --> 00:17:18.680]   Cool.
[00:17:18.680 --> 00:17:20.880]   So actually that's it for this video.
[00:17:20.880 --> 00:17:25.880]   So that is how we would implement a retrieval,
[00:17:25.880 --> 00:17:29.480]   augmented conversational agent in Liontrain.
[00:17:29.480 --> 00:17:33.080]   So really kind of taking the previous few videos
[00:17:33.080 --> 00:17:35.240]   and almost merging those all together.
[00:17:35.240 --> 00:17:37.920]   So, you know, we took the retrieval augmentation,
[00:17:37.920 --> 00:17:41.840]   we took an agent and we created a tool that, you know,
[00:17:41.840 --> 00:17:45.120]   could allow us to access our external knowledge base
[00:17:45.120 --> 00:17:47.840]   and implement that sort of long-term memory
[00:17:47.840 --> 00:17:50.040]   for our conversational agents.
[00:17:51.000 --> 00:17:54.200]   So this is a sort of pattern that we,
[00:17:54.200 --> 00:17:58.840]   I'm already seeing actually quite a lot in many use cases.
[00:17:58.840 --> 00:18:00.520]   So where we have this long-term memory,
[00:18:00.520 --> 00:18:02.200]   where we have agents
[00:18:02.200 --> 00:18:05.960]   and where we have conversational history.
[00:18:05.960 --> 00:18:08.160]   So I think, you know,
[00:18:08.160 --> 00:18:12.800]   especially if you're building tools or applications
[00:18:12.800 --> 00:18:15.000]   that use NLP, large language models,
[00:18:15.000 --> 00:18:16.920]   this is probably something that you're gonna come across
[00:18:16.920 --> 00:18:18.200]   if you haven't already.
[00:18:18.200 --> 00:18:20.160]   But anyway, that's it for this video.
[00:18:20.160 --> 00:18:23.880]   I hope all of this has been useful and interesting.
[00:18:23.880 --> 00:18:26.400]   So thank you very much for watching
[00:18:26.400 --> 00:18:28.520]   and I will see you again in the next one.
[00:18:28.520 --> 00:18:29.360]   Bye.
[00:18:29.520 --> 00:18:32.120]   (upbeat music)
[00:18:32.960 --> 00:18:35.360]   (soft music)
[00:18:35.640 --> 00:18:38.040]   (soft music)
[00:18:38.040 --> 00:18:40.440]   (soft music)
[00:18:40.440 --> 00:18:43.020]   (gentle music)
[00:18:43.020 --> 00:18:45.080]   you

