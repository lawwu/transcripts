
[00:00:00.000 --> 00:00:03.160]   We've never bowed down to government pressure
[00:00:03.160 --> 00:00:05.720]   anywhere in the world and we never will.
[00:00:05.720 --> 00:00:07.400]   We understand that we're hardcore
[00:00:07.400 --> 00:00:09.000]   and actually there is a bit of nuance
[00:00:09.000 --> 00:00:12.120]   about how different companies respond to this,
[00:00:12.120 --> 00:00:15.680]   but our response has always been just to say no.
[00:00:15.680 --> 00:00:18.320]   And if they threaten to block, well knock yourself out,
[00:00:18.320 --> 00:00:19.720]   you're gonna lose Wikipedia.
[00:00:19.720 --> 00:00:24.600]   - The following is a conversation with Jimmy Wales,
[00:00:24.600 --> 00:00:27.080]   co-founder of Wikipedia,
[00:00:27.080 --> 00:00:31.880]   one of, if not the most impactful websites ever,
[00:00:31.880 --> 00:00:34.320]   expanding the collective knowledge, intelligence,
[00:00:34.320 --> 00:00:37.280]   and wisdom of human civilization.
[00:00:37.280 --> 00:00:39.720]   This is the "Alex Friedman Podcast."
[00:00:39.720 --> 00:00:41.800]   To support it, please check out our sponsors
[00:00:41.800 --> 00:00:43.080]   in the description.
[00:00:43.080 --> 00:00:46.680]   And now, dear friends, here's Jimmy Wales.
[00:00:46.680 --> 00:00:49.200]   Let's start at the beginning.
[00:00:49.200 --> 00:00:51.840]   What is the origin story of Wikipedia?
[00:00:51.840 --> 00:00:53.560]   - The origin story of Wikipedia.
[00:00:53.560 --> 00:00:55.640]   Well, so I was watching the growth
[00:00:55.640 --> 00:01:00.040]   of the free software movement, open source software,
[00:01:00.040 --> 00:01:02.000]   and seeing programmers coming together
[00:01:02.000 --> 00:01:06.120]   to collaborate in new ways, sharing code,
[00:01:06.120 --> 00:01:08.320]   doing that under a free license,
[00:01:08.320 --> 00:01:09.960]   which is really interesting
[00:01:09.960 --> 00:01:13.280]   because it empowers an ability to work together
[00:01:13.280 --> 00:01:15.840]   that's really hard to do if the code is still proprietary
[00:01:15.840 --> 00:01:18.160]   because then if I chip in and help,
[00:01:18.160 --> 00:01:21.000]   we sort of have to figure out how I'm gonna be rewarded
[00:01:21.000 --> 00:01:21.840]   and what that is,
[00:01:21.840 --> 00:01:23.360]   the idea that everyone can copy it
[00:01:23.360 --> 00:01:26.000]   and it just is part of the commons
[00:01:26.000 --> 00:01:28.240]   really empowered a huge wave
[00:01:28.240 --> 00:01:32.400]   of creative software production.
[00:01:32.400 --> 00:01:34.280]   And I realized that that kind of collaboration
[00:01:34.280 --> 00:01:35.800]   could extend beyond just software
[00:01:35.800 --> 00:01:37.520]   to all kinds of cultural works.
[00:01:37.520 --> 00:01:41.040]   And the first thing that I thought of was an encyclopedia.
[00:01:41.040 --> 00:01:42.680]   I thought, oh, that seems obvious,
[00:01:42.680 --> 00:01:45.520]   that an encyclopedia, you can collaborate on it.
[00:01:45.520 --> 00:01:46.680]   There's a few reasons why.
[00:01:46.680 --> 00:01:49.280]   One, we all pretty much know
[00:01:49.280 --> 00:01:51.960]   what an encyclopedia entry on, say,
[00:01:51.960 --> 00:01:54.120]   the Eiffel Tower should be like.
[00:01:54.120 --> 00:01:57.000]   You know, you should see a picture, a few pictures, maybe,
[00:01:57.000 --> 00:02:01.040]   history, location, something about the architect,
[00:02:01.040 --> 00:02:02.360]   et cetera, et cetera.
[00:02:02.360 --> 00:02:03.960]   So we have a shared understanding
[00:02:03.960 --> 00:02:05.160]   of what it is we're trying to do,
[00:02:05.160 --> 00:02:06.320]   and then we can collaborate
[00:02:06.320 --> 00:02:07.520]   and different people can chip in
[00:02:07.520 --> 00:02:10.200]   and find sources and so on and so forth.
[00:02:10.200 --> 00:02:13.280]   So set up first, Newpedia,
[00:02:13.280 --> 00:02:17.080]   which was about two years before Wikipedia.
[00:02:17.080 --> 00:02:22.080]   And with Newpedia, we had this idea
[00:02:22.080 --> 00:02:24.520]   that in order to be respected,
[00:02:24.520 --> 00:02:26.280]   we had to be even more academic
[00:02:26.280 --> 00:02:29.040]   than a traditional encyclopedia,
[00:02:29.040 --> 00:02:31.160]   because a bunch of volunteers on the internet
[00:02:31.160 --> 00:02:32.920]   getting out of the right encyclopedia,
[00:02:32.920 --> 00:02:34.080]   you know, you could be made fun of
[00:02:34.080 --> 00:02:36.320]   if it's just every random person.
[00:02:36.320 --> 00:02:39.880]   So we had implemented this seven-stage review process
[00:02:39.880 --> 00:02:41.200]   to get anything published.
[00:02:41.200 --> 00:02:44.280]   And two things came of that.
[00:02:44.280 --> 00:02:47.560]   So one thing, one of the earliest entries
[00:02:47.560 --> 00:02:51.000]   that we published after this rigorous process,
[00:02:51.000 --> 00:02:52.600]   a few days later, we had to pull it,
[00:02:52.600 --> 00:02:54.200]   because as soon as it hit the web
[00:02:54.200 --> 00:02:57.200]   and the broader community took a look at it,
[00:02:57.200 --> 00:02:58.520]   people noticed plagiarism
[00:02:58.520 --> 00:03:00.960]   and realized that it wasn't actually that good,
[00:03:00.960 --> 00:03:03.640]   even though it had been reviewed by academics and so on.
[00:03:03.640 --> 00:03:04.480]   So we had to pull it.
[00:03:04.480 --> 00:03:05.400]   So it's like, oh, okay, well,
[00:03:05.400 --> 00:03:07.880]   so much for a seven-stage review process.
[00:03:07.880 --> 00:03:10.600]   But also I decided that I wanted to try,
[00:03:10.600 --> 00:03:12.680]   I was frustrated, why is this taking so long?
[00:03:12.680 --> 00:03:13.680]   Why is it so hard?
[00:03:14.480 --> 00:03:16.200]   So I thought, oh, okay.
[00:03:16.200 --> 00:03:20.680]   I saw that Robert Merton had won a Nobel Prize in economics
[00:03:20.680 --> 00:03:23.240]   for his work on option pricing theory.
[00:03:23.240 --> 00:03:25.280]   And when I was in academia, that's what I worked on,
[00:03:25.280 --> 00:03:27.720]   was option pricing theory, how to publish paper.
[00:03:27.720 --> 00:03:30.040]   So I'd worked through all of his academic papers
[00:03:30.040 --> 00:03:32.080]   and I knew his work quite well.
[00:03:32.080 --> 00:03:33.160]   I thought, oh, I'll just,
[00:03:33.160 --> 00:03:35.960]   I'll write a short biography of Merton.
[00:03:35.960 --> 00:03:39.240]   And when I started to do it, I'd been out of academia,
[00:03:39.240 --> 00:03:41.840]   hadn't been a grad student for a few years then.
[00:03:41.840 --> 00:03:43.960]   I felt this huge intimidation
[00:03:43.960 --> 00:03:46.520]   because they were gonna take my draft
[00:03:46.520 --> 00:03:49.280]   and send it to the most prestigious finance professors
[00:03:49.280 --> 00:03:53.080]   that we could find to give me feedback for revisions.
[00:03:53.080 --> 00:03:54.880]   And it felt like being back in grad school.
[00:03:54.880 --> 00:03:56.760]   It's like this really oppressive sort of like,
[00:03:56.760 --> 00:03:58.040]   you're gonna submit it for review
[00:03:58.040 --> 00:03:59.640]   and you're gonna get critiques.
[00:03:59.640 --> 00:04:00.960]   - A little bit of the bad part of grad school.
[00:04:00.960 --> 00:04:03.280]   - Yeah, yeah, the bad part of grad school, right?
[00:04:03.280 --> 00:04:05.280]   And so I was like, oh, this isn't intellectually fun.
[00:04:05.280 --> 00:04:07.520]   This is like the bad part of grad school.
[00:04:07.520 --> 00:04:09.480]   It's intimidating and there's a lot of
[00:04:11.360 --> 00:04:13.280]   potential embarrassment if I screw something up
[00:04:13.280 --> 00:04:14.480]   and so forth.
[00:04:14.480 --> 00:04:15.800]   And so that was when I realized,
[00:04:15.800 --> 00:04:17.280]   okay, look, this is never gonna work.
[00:04:17.280 --> 00:04:18.480]   This is not something that people
[00:04:18.480 --> 00:04:20.200]   are really gonna wanna do.
[00:04:20.200 --> 00:04:22.600]   So Jeremy Rosenfeld, one of my employees,
[00:04:22.600 --> 00:04:25.520]   had brought and showed me the wiki concept in December
[00:04:25.520 --> 00:04:29.160]   and then Larry Sanger brought in the same,
[00:04:29.160 --> 00:04:31.440]   said, "Hey, what about this wiki idea?"
[00:04:31.440 --> 00:04:36.440]   And so in January, we decided to launch Wikipedia,
[00:04:36.440 --> 00:04:37.480]   but we weren't sure.
[00:04:37.480 --> 00:04:40.440]   So the original project was called Newpedia.
[00:04:40.440 --> 00:04:41.800]   And even though it wasn't successful,
[00:04:41.800 --> 00:04:43.800]   we did have quite a group of academics
[00:04:43.800 --> 00:04:45.480]   and like really serious people.
[00:04:45.480 --> 00:04:48.680]   And we were concerned that, oh, maybe these academics
[00:04:48.680 --> 00:04:50.280]   are gonna really hate this idea
[00:04:50.280 --> 00:04:53.080]   and we shouldn't just convert the project immediately.
[00:04:53.080 --> 00:04:54.760]   We should launch this as a side project,
[00:04:54.760 --> 00:04:55.880]   the idea of, here's a wiki
[00:04:55.880 --> 00:04:58.240]   where we can start playing around.
[00:04:58.240 --> 00:05:00.800]   But actually we got more work done in two weeks
[00:05:00.800 --> 00:05:02.360]   than we had in almost two years
[00:05:02.360 --> 00:05:04.000]   because people were able to just jump on
[00:05:04.000 --> 00:05:05.480]   and start doing stuff.
[00:05:05.480 --> 00:05:07.760]   And it was actually a very exciting time.
[00:05:07.760 --> 00:05:08.840]   You know, you could, back then,
[00:05:08.840 --> 00:05:11.000]   you could be the first person who typed
[00:05:11.000 --> 00:05:13.280]   Africa is a continent and hit save,
[00:05:13.280 --> 00:05:15.360]   which isn't much of an encyclopedia entry,
[00:05:15.360 --> 00:05:18.160]   but it's true and it's a start and it's kind of fun.
[00:05:18.160 --> 00:05:20.120]   Like, you put your name down.
[00:05:20.120 --> 00:05:23.400]   Actually, a funny story was several years later,
[00:05:23.400 --> 00:05:26.320]   I just happened to be online and I saw when,
[00:05:26.320 --> 00:05:27.800]   I think his name is Robert Allman,
[00:05:27.800 --> 00:05:29.920]   won the Nobel Prize in economics.
[00:05:29.920 --> 00:05:32.400]   And we didn't have an entry on him at all,
[00:05:32.400 --> 00:05:34.200]   which was surprising, but it wasn't that surprising.
[00:05:34.200 --> 00:05:36.000]   This was still early days, you know?
[00:05:36.920 --> 00:05:39.560]   And so I got to be the first person to type,
[00:05:39.560 --> 00:05:42.000]   Robert Allman won Nobel Prize in economics and hit save,
[00:05:42.000 --> 00:05:44.440]   which again, wasn't a very good article.
[00:05:44.440 --> 00:05:45.560]   But then I came back two days later
[00:05:45.560 --> 00:05:47.760]   and people had improved it and so forth.
[00:05:47.760 --> 00:05:50.520]   So that second half of the experience
[00:05:50.520 --> 00:05:52.480]   where with Robert Merton, I never succeeded
[00:05:52.480 --> 00:05:53.680]   because it was just too intimidating.
[00:05:53.680 --> 00:05:55.840]   It was like, oh no, I was able to chip in and help.
[00:05:55.840 --> 00:05:57.040]   Other people jumped in.
[00:05:57.040 --> 00:05:58.560]   Everybody was interested in the topic
[00:05:58.560 --> 00:06:00.480]   because it's all in the news at the moment.
[00:06:00.480 --> 00:06:02.240]   And so it's just a completely different model,
[00:06:02.240 --> 00:06:03.520]   which worked much, much better.
[00:06:03.520 --> 00:06:06.000]   - What is it that made that so accessible, so fun,
[00:06:06.000 --> 00:06:09.320]   so natural to just add something?
[00:06:09.320 --> 00:06:10.720]   - Well, I think it's, you know,
[00:06:10.720 --> 00:06:12.000]   especially in the early days,
[00:06:12.000 --> 00:06:13.760]   and this, by the way, has gotten much harder
[00:06:13.760 --> 00:06:15.080]   because there are fewer topics
[00:06:15.080 --> 00:06:18.340]   that are just greenfield, you know, available.
[00:06:18.340 --> 00:06:22.480]   But you know, you could say, oh, well, you know,
[00:06:22.480 --> 00:06:26.560]   I know a little bit about this and I can get it started.
[00:06:26.560 --> 00:06:28.480]   But then it is fun to come back then
[00:06:28.480 --> 00:06:30.520]   and see other people have added and improved
[00:06:30.520 --> 00:06:31.480]   and so on and so forth.
[00:06:31.480 --> 00:06:34.160]   And that idea of collaborating, you know,
[00:06:34.160 --> 00:06:36.740]   where people can, much like open source software,
[00:06:36.740 --> 00:06:39.320]   you know, you put your code out
[00:06:39.320 --> 00:06:41.400]   and then people suggest revisions and they change it
[00:06:41.400 --> 00:06:45.880]   and it modifies and it grows beyond the original creator.
[00:06:45.880 --> 00:06:49.540]   It's just a kind of a fun, wonderful, quite geeky hobby,
[00:06:49.540 --> 00:06:51.440]   but people enjoy it.
[00:06:51.440 --> 00:06:53.680]   - How much debate was there over the interface,
[00:06:53.680 --> 00:06:55.920]   over the details of how to make that
[00:06:55.920 --> 00:06:57.200]   seamless and frictionless?
[00:06:57.200 --> 00:06:59.180]   - Yeah, I mean, not as much as there probably
[00:06:59.180 --> 00:07:01.040]   should have been, in a way.
[00:07:01.040 --> 00:07:04.400]   During that two years of the failure of Newpedia,
[00:07:04.400 --> 00:07:06.600]   where very little work got done,
[00:07:06.600 --> 00:07:08.600]   what was actually productive was
[00:07:08.600 --> 00:07:11.540]   there was a huge, long discussion, email discussion,
[00:07:11.540 --> 00:07:15.200]   very clever people talking about things like neutrality,
[00:07:15.200 --> 00:07:17.260]   talking about what is an encyclopedia,
[00:07:17.260 --> 00:07:19.440]   but also talking about more technical ideas, you know,
[00:07:19.440 --> 00:07:22.520]   things back then XML was kind of all the rage
[00:07:22.520 --> 00:07:25.200]   and thinking about, ah, could we, you know,
[00:07:25.200 --> 00:07:28.500]   shouldn't you have certain data
[00:07:28.500 --> 00:07:30.080]   that might be in multiple articles
[00:07:30.080 --> 00:07:31.320]   that gets updated automatically?
[00:07:31.320 --> 00:07:35.300]   So, for example, you know, the population of New York City,
[00:07:35.300 --> 00:07:37.920]   every 10 years there's a new official census,
[00:07:37.920 --> 00:07:41.240]   couldn't you just update that bit of data in one place
[00:07:41.240 --> 00:07:42.840]   and it would update across all languages?
[00:07:42.840 --> 00:07:45.240]   That is a reality today, but back then it was just like,
[00:07:45.240 --> 00:07:47.120]   hmm, how do we do that, how do we think about that?
[00:07:47.120 --> 00:07:49.320]   - So that is a reality today where it's,
[00:07:49.320 --> 00:07:52.160]   there's some universal variables, Wikidata.
[00:07:52.160 --> 00:07:56.200]   - Yeah, Wikidata, you can link, you know,
[00:07:56.200 --> 00:07:59.000]   from a Wikipedia entry, you can link to that piece of data
[00:07:59.000 --> 00:08:01.220]   in Wikidata, I mean, it's a pretty advanced thing,
[00:08:01.220 --> 00:08:03.000]   but there are advanced users who are doing that.
[00:08:03.000 --> 00:08:04.680]   And then when that gets updated,
[00:08:04.680 --> 00:08:06.760]   it updates in all the languages where you've done that.
[00:08:06.760 --> 00:08:07.920]   - I mean, that's really interesting.
[00:08:07.920 --> 00:08:10.000]   There was this chain of emails in the early days
[00:08:10.000 --> 00:08:13.220]   of discussing the details of what is,
[00:08:13.220 --> 00:08:14.920]   so there's the interface, there's the--
[00:08:14.920 --> 00:08:16.600]   - Yeah, so the interface, so an example,
[00:08:16.600 --> 00:08:18.760]   there was some software called UseModWiki,
[00:08:18.760 --> 00:08:22.560]   which we started with, it's quite amusing actually,
[00:08:22.560 --> 00:08:25.520]   because the main reason we launched with UseModWiki
[00:08:25.520 --> 00:08:28.160]   is that it was a single Perl script.
[00:08:28.160 --> 00:08:30.420]   So it was really easy for me to install it on the server
[00:08:30.420 --> 00:08:32.040]   and just get running.
[00:08:32.040 --> 00:08:35.400]   But it was, you know, some guy's hobby project,
[00:08:35.400 --> 00:08:37.760]   it was cool, but it was just a hobby project.
[00:08:37.760 --> 00:08:42.760]   And all the data was stored in flat text files.
[00:08:42.760 --> 00:08:45.440]   So there was no real database behind it.
[00:08:45.440 --> 00:08:49.320]   So to search the site, you basically used Grap,
[00:08:49.320 --> 00:08:52.000]   which is just like basic Unix utility
[00:08:52.000 --> 00:08:54.240]   to like look through all the files.
[00:08:54.240 --> 00:08:55.640]   So that clearly was never going to scale.
[00:08:55.640 --> 00:09:00.120]   But also, in the early days, it didn't have real logins.
[00:09:00.120 --> 00:09:03.160]   So you could set your username, but there were no passwords.
[00:09:03.160 --> 00:09:06.880]   So, you know, I might say Bob Smith,
[00:09:06.880 --> 00:09:08.080]   and then someone else comes along and says,
[00:09:08.080 --> 00:09:09.800]   oh, I'm Bob Smith, and they're both at it.
[00:09:09.800 --> 00:09:10.720]   Now that never really happened,
[00:09:10.720 --> 00:09:11.560]   we didn't have a problem with it,
[00:09:11.560 --> 00:09:13.160]   but it was kind of obvious, like you can't grow
[00:09:13.160 --> 00:09:15.280]   a big website where everybody can pretend to be everybody.
[00:09:15.280 --> 00:09:17.720]   That's not gonna be good for trust
[00:09:17.720 --> 00:09:19.440]   and reputation and so forth.
[00:09:19.440 --> 00:09:21.800]   So quickly, I had to write a little, you know,
[00:09:21.800 --> 00:09:24.480]   login, store people's passwords and things like that,
[00:09:24.480 --> 00:09:26.400]   so you can have unique identities.
[00:09:26.400 --> 00:09:28.400]   And then another example of something, you know,
[00:09:28.400 --> 00:09:30.040]   quite you would have never thought
[00:09:30.040 --> 00:09:31.080]   would have been a good idea,
[00:09:31.080 --> 00:09:32.400]   and it turned out to not be a problem,
[00:09:32.400 --> 00:09:35.980]   but to make a link in Wikipedia, in the early days,
[00:09:35.980 --> 00:09:39.720]   you would make a link to a page that may or may not exist
[00:09:39.720 --> 00:09:43.400]   by just using camel case, meaning it's like uppercase,
[00:09:43.400 --> 00:09:45.120]   lowercase, and you smash the words together.
[00:09:45.120 --> 00:09:50.000]   So maybe New York City, you might type N-E-W,
[00:09:50.000 --> 00:09:53.520]   no space, capital Y, York City,
[00:09:53.520 --> 00:09:54.360]   and that would make a link.
[00:09:54.360 --> 00:09:56.280]   But that was ugly, that was clearly not right.
[00:09:56.280 --> 00:09:58.120]   And so I was like, okay, well,
[00:09:58.120 --> 00:10:00.320]   that's just not gonna look nice.
[00:10:00.320 --> 00:10:01.680]   Let's just use square brackets,
[00:10:01.680 --> 00:10:04.200]   two square brackets makes a link.
[00:10:04.200 --> 00:10:05.560]   That may have been an option in the software,
[00:10:05.560 --> 00:10:07.000]   I'm not sure I thought up square brackets,
[00:10:07.000 --> 00:10:10.280]   but anyway, we just did that, which worked really well.
[00:10:10.280 --> 00:10:11.600]   It makes nice links, and you know,
[00:10:11.600 --> 00:10:13.720]   you can see in its red links or blue links,
[00:10:13.720 --> 00:10:15.680]   depending on if the page exists or not.
[00:10:15.680 --> 00:10:18.400]   But the thing that didn't occur to me even think about
[00:10:18.400 --> 00:10:21.240]   is that, for example, on the German language
[00:10:21.240 --> 00:10:24.200]   standard keyboard, there is no square bracket.
[00:10:24.200 --> 00:10:26.280]   So for German Wikipedia to succeed,
[00:10:26.280 --> 00:10:28.280]   people had to learn to do some alt codes
[00:10:28.280 --> 00:10:30.800]   to get the square bracket, or a lot of users
[00:10:30.800 --> 00:10:33.720]   cut and paste a square bracket when they could find one,
[00:10:33.720 --> 00:10:35.480]   they would just cut and paste one in.
[00:10:35.480 --> 00:10:37.520]   And yet, German Wikipedia has been a massive success,
[00:10:37.520 --> 00:10:40.120]   so somehow that didn't slow people down.
[00:10:40.120 --> 00:10:42.400]   - How is it that the German keyboards
[00:10:42.400 --> 00:10:43.560]   don't have a square bracket?
[00:10:43.560 --> 00:10:44.720]   How do you do programming?
[00:10:44.720 --> 00:10:48.880]   How do you live life to its fullest without square brackets?
[00:10:48.880 --> 00:10:50.880]   - Very good question, I'm not really sure.
[00:10:50.880 --> 00:10:53.240]   I mean, maybe it does now, because keyboard standards
[00:10:53.240 --> 00:10:56.840]   have drifted over time and becomes useful
[00:10:56.840 --> 00:10:57.680]   to have a certain character.
[00:10:57.680 --> 00:10:59.640]   I mean, it's the same thing, like there's not really
[00:10:59.640 --> 00:11:01.600]   a W character in Italian.
[00:11:01.600 --> 00:11:04.760]   And it wasn't on keyboards, or I think it is now,
[00:11:04.760 --> 00:11:08.040]   but in general, W is not a letter in Italian language,
[00:11:08.040 --> 00:11:10.600]   but it appears in enough international words
[00:11:10.600 --> 00:11:12.560]   that it's crept into Italian.
[00:11:12.560 --> 00:11:14.480]   - And all of these things are probably
[00:11:14.480 --> 00:11:17.160]   Wikipedia articles in themselves.
[00:11:17.160 --> 00:11:18.000]   - Oh yeah, oh yeah.
[00:11:18.000 --> 00:11:18.840]   - The discussion of square brackets in German.
[00:11:18.840 --> 00:11:20.200]   - The whole discussion, I'm sure.
[00:11:20.200 --> 00:11:23.280]   - On both the English and the German Wikipedia,
[00:11:23.280 --> 00:11:24.960]   and the difference between those two
[00:11:24.960 --> 00:11:28.000]   might be very interesting.
[00:11:28.000 --> 00:11:31.800]   So Wikidata's fascinating, but even the broader discussion
[00:11:31.800 --> 00:11:35.480]   of what is an encyclopedia, can you go to that
[00:11:35.480 --> 00:11:39.640]   sort of philosophical question of what is an encyclopedia?
[00:11:39.640 --> 00:11:40.480]   - What is an encyclopedia?
[00:11:40.480 --> 00:11:44.640]   So the way I would put it is an encyclopedia,
[00:11:44.640 --> 00:11:48.700]   what our goal is, is the sum of all human knowledge,
[00:11:48.700 --> 00:11:50.880]   but sum meaning summary.
[00:11:50.880 --> 00:11:53.740]   So, and this was an early debate,
[00:11:53.740 --> 00:11:57.160]   I mean, somebody started uploading the full text
[00:11:57.160 --> 00:11:59.160]   of Hamlet, for example, and we said,
[00:11:59.160 --> 00:12:00.880]   hmm, wait, hold on a second,
[00:12:00.880 --> 00:12:03.280]   that's not an encyclopedia article, but why not?
[00:12:03.280 --> 00:12:06.400]   So hence was born Wikisource,
[00:12:06.400 --> 00:12:08.760]   which is where you put original texts
[00:12:08.760 --> 00:12:11.200]   and things like that out of copyright text,
[00:12:11.200 --> 00:12:13.320]   because they said, no, an encyclopedia article
[00:12:13.320 --> 00:12:15.080]   about Hamlet, that's a perfectly valid thing,
[00:12:15.080 --> 00:12:16.360]   but the actual text of the play
[00:12:16.360 --> 00:12:18.520]   is not an encyclopedia article.
[00:12:18.520 --> 00:12:20.420]   So most of it's fairly obvious,
[00:12:20.420 --> 00:12:23.080]   but there are some interesting quirks and differences.
[00:12:23.080 --> 00:12:26.200]   So for example, as I understand it,
[00:12:26.200 --> 00:12:29.700]   in French language encyclopedias,
[00:12:29.700 --> 00:12:33.120]   traditionally, it would be quite common to have recipes,
[00:12:33.120 --> 00:12:34.840]   which in English language, that would be unusual.
[00:12:34.840 --> 00:12:38.940]   You wouldn't find a recipe for chocolate cake in Britannica.
[00:12:38.940 --> 00:12:41.980]   And so I actually don't know the current state,
[00:12:41.980 --> 00:12:44.620]   haven't even thought about that in many, many years now.
[00:12:44.620 --> 00:12:47.680]   - State of cake recipes in Wikipedia, in English Wikipedia?
[00:12:47.680 --> 00:12:49.560]   - I wouldn't say there's chocolate cake recipes.
[00:12:49.560 --> 00:12:52.220]   I mean, you might find a sample recipe somewhere.
[00:12:52.220 --> 00:12:54.160]   I'm not saying there are none, but in general, no.
[00:12:54.160 --> 00:12:55.440]   Like we wouldn't have recipes.
[00:12:55.440 --> 00:12:57.000]   - I told myself I would not get outraged
[00:12:57.000 --> 00:12:59.500]   in this conversation, but now I'm outraged.
[00:12:59.500 --> 00:13:00.380]   I'm deeply upset.
[00:13:00.380 --> 00:13:02.240]   - It's actually very complicated.
[00:13:02.240 --> 00:13:03.180]   I love to cook.
[00:13:03.180 --> 00:13:06.460]   I'm actually quite a good cook.
[00:13:06.460 --> 00:13:09.320]   What's interesting is it's very hard
[00:13:09.320 --> 00:13:12.000]   to have a neutral recipe because--
[00:13:12.000 --> 00:13:13.600]   - Like a canonical recipe for cake, chocolate cake.
[00:13:13.600 --> 00:13:17.000]   - A canonical recipe is kind of difficult to come by,
[00:13:17.000 --> 00:13:18.360]   because there's so many variants
[00:13:18.360 --> 00:13:20.800]   and it's all debatable and interesting.
[00:13:20.800 --> 00:13:23.200]   For something like chocolate cake, you could probably say,
[00:13:23.200 --> 00:13:24.640]   here's one of the earliest recipes,
[00:13:24.640 --> 00:13:26.440]   or here's one of the most common recipes.
[00:13:26.440 --> 00:13:31.440]   But for many, many things, the variants are as interesting
[00:13:31.440 --> 00:13:36.020]   as somebody said to me recently,
[00:13:36.020 --> 00:13:39.720]   10 Spaniards, 12 paella recipes.
[00:13:39.720 --> 00:13:44.240]   So these are all matters of open discussion.
[00:13:44.240 --> 00:13:49.240]   - Well, just to throw some numbers, as of May 27th, 2023,
[00:13:49.240 --> 00:13:54.760]   there are 6.66 million articles in the English Wikipedia
[00:13:54.760 --> 00:14:01.000]   containing over 4.3 billion words, including articles.
[00:14:01.000 --> 00:14:06.000]   The total number of pages is 58 million.
[00:14:06.000 --> 00:14:07.760]   Does that blow your mind?
[00:14:07.760 --> 00:14:09.120]   - I mean, yes, it does.
[00:14:09.120 --> 00:14:11.200]   I mean, it doesn't because I know those numbers
[00:14:11.200 --> 00:14:13.240]   and see them from time to time.
[00:14:13.240 --> 00:14:16.120]   But in another sense, a deeper sense, yeah, it does.
[00:14:16.120 --> 00:14:19.040]   I mean, it's really remarkable.
[00:14:19.040 --> 00:14:24.040]   I remember when English Wikipedia passed 100,000 articles
[00:14:24.040 --> 00:14:26.200]   and when German Wikipedia passed 100,000,
[00:14:26.200 --> 00:14:27.740]   'cause I happened to be in Germany
[00:14:27.740 --> 00:14:29.360]   with a bunch of Wikipedians that night.
[00:14:29.360 --> 00:14:32.640]   And then it seemed quite big.
[00:14:32.640 --> 00:14:34.020]   I mean, we knew at that time
[00:14:34.020 --> 00:14:36.800]   that it was nowhere near complete.
[00:14:36.800 --> 00:14:41.240]   I remember at Wikimania in Harvard
[00:14:41.240 --> 00:14:44.240]   when we did our annual conference there in Boston,
[00:14:44.240 --> 00:14:49.800]   someone who had come to the conference from Poland
[00:14:49.800 --> 00:14:54.800]   had brought along with him a small encyclopedia,
[00:14:54.800 --> 00:14:59.160]   a single volume encyclopedia of biographies.
[00:14:59.160 --> 00:15:02.000]   So short biographies, normally a paragraph or so
[00:15:02.000 --> 00:15:03.920]   about famous people in Poland.
[00:15:03.920 --> 00:15:07.240]   And there were some 22,000 entries.
[00:15:07.240 --> 00:15:10.040]   And he pointed out that even then, 2006,
[00:15:10.040 --> 00:15:12.040]   Wikipedia felt quite big.
[00:15:12.040 --> 00:15:13.520]   And he said, "In English Wikipedia,
[00:15:13.520 --> 00:15:15.960]   there's only a handful of these,
[00:15:15.960 --> 00:15:18.280]   less than 10%, I think he said."
[00:15:18.280 --> 00:15:21.040]   And so then you realize, yeah, actually,
[00:15:21.040 --> 00:15:26.040]   who was the mayor of Warsaw in 1873?
[00:15:26.040 --> 00:15:28.400]   Don't know, probably not in English Wikipedia,
[00:15:28.400 --> 00:15:30.140]   but it probably might be today.
[00:15:30.140 --> 00:15:33.120]   But there's so much out there.
[00:15:33.120 --> 00:15:34.400]   And of course, what we get into
[00:15:34.400 --> 00:15:36.960]   when we're talking about how many entries there are
[00:15:36.960 --> 00:15:39.940]   and how many could there be,
[00:15:39.940 --> 00:15:43.160]   is this very deep philosophical issue of notability,
[00:15:43.160 --> 00:15:45.600]   which is the question of, well,
[00:15:45.600 --> 00:15:47.880]   how do you draw the limit?
[00:15:47.880 --> 00:15:51.040]   How do you draw what is there?
[00:15:51.040 --> 00:15:54.280]   So sometimes people say, oh, there should be no limit,
[00:15:54.280 --> 00:15:56.000]   but I think that doesn't stand up to much scrutiny
[00:15:56.000 --> 00:15:57.400]   if you really pause and think about it.
[00:15:57.400 --> 00:16:01.800]   So I see in your hand there, you've got a BIC pen,
[00:16:01.800 --> 00:16:04.000]   pretty standard, everybody's seen
[00:16:04.000 --> 00:16:05.000]   billions of those in life.
[00:16:05.000 --> 00:16:05.840]   - Classic though.
[00:16:05.840 --> 00:16:08.080]   - It's a classic, clear BIC pen.
[00:16:08.080 --> 00:16:09.980]   So could we have an entry about that BIC pen?
[00:16:09.980 --> 00:16:13.100]   Well, I bet we do, that type of BIC pen,
[00:16:13.100 --> 00:16:14.700]   because it's classic, everybody knows it,
[00:16:14.700 --> 00:16:15.620]   and it's got a history.
[00:16:15.620 --> 00:16:17.940]   And actually, there's something interesting
[00:16:17.940 --> 00:16:19.500]   about the BIC company.
[00:16:19.500 --> 00:16:22.940]   They make pens, they also make kayaks,
[00:16:22.940 --> 00:16:23.940]   and there's something else there for them.
[00:16:23.940 --> 00:16:27.060]   So basically, they're sort of a definition
[00:16:27.060 --> 00:16:28.740]   by non-essentials company.
[00:16:28.740 --> 00:16:32.460]   Anything that's long and plastic, that's what they make.
[00:16:32.460 --> 00:16:33.300]   - Wow.
[00:16:33.300 --> 00:16:34.140]   (Lyle laughs)
[00:16:34.140 --> 00:16:36.420]   - So if you wanna find the common ground.
[00:16:36.420 --> 00:16:37.860]   - The platonic form of a BIC.
[00:16:37.860 --> 00:16:41.480]   - But could we have an article about that very BIC pen
[00:16:41.480 --> 00:16:42.760]   in your hand?
[00:16:42.760 --> 00:16:44.760]   So Lex Friedman's BIC pen as of this week.
[00:16:44.760 --> 00:16:45.600]   - Oh, the very, this instance.
[00:16:45.600 --> 00:16:47.760]   - The very specific instance.
[00:16:47.760 --> 00:16:50.540]   And the answer is no, there's not much known about it,
[00:16:50.540 --> 00:16:53.960]   I dare say, unless it's very special to you
[00:16:53.960 --> 00:16:55.720]   and your great-grandmother gave it to you or something,
[00:16:55.720 --> 00:16:56.920]   you probably know very little about it.
[00:16:56.920 --> 00:16:59.720]   It's a pen, it's just here in the office.
[00:16:59.720 --> 00:17:03.400]   So that's just to show there is a limit.
[00:17:03.400 --> 00:17:06.120]   I mean, in German Wikipedia, they used to talk about
[00:17:06.120 --> 00:17:10.840]   the rear nut of the wheel of Uli Fuchs' bicycle,
[00:17:10.840 --> 00:17:14.320]   Uli Fuchs, the well-known Wikipedian of the time,
[00:17:14.320 --> 00:17:15.960]   to sort of illustrate, like you can't have an article
[00:17:15.960 --> 00:17:17.140]   about literally everything.
[00:17:17.140 --> 00:17:18.900]   And so then it raises the question,
[00:17:18.900 --> 00:17:21.280]   what can you have an article about, what can't you?
[00:17:21.280 --> 00:17:24.060]   And that can vary depending on the subject matter.
[00:17:24.060 --> 00:17:29.520]   One of the areas where we try to be much more careful
[00:17:29.520 --> 00:17:31.520]   would be biographies.
[00:17:31.520 --> 00:17:34.720]   The reason is a biography of a living person,
[00:17:34.720 --> 00:17:37.160]   if you get it wrong, it can actually be quite hurtful,
[00:17:37.160 --> 00:17:38.000]   quite damaging.
[00:17:38.000 --> 00:17:41.200]   And so if someone is a private person
[00:17:41.200 --> 00:17:44.480]   and somebody tries to create a Wikipedia entry,
[00:17:44.480 --> 00:17:46.020]   there's no way to update it, there's not much known.
[00:17:46.020 --> 00:17:49.660]   So for example, an encyclopedia article about my mother,
[00:17:49.660 --> 00:17:52.280]   my mother, school teacher, later a pharmacist,
[00:17:52.280 --> 00:17:55.600]   wonderful woman, but never been in the news.
[00:17:55.600 --> 00:17:56.760]   I mean, other than me talking about
[00:17:56.760 --> 00:17:57.960]   why there shouldn't be a Wikipedia entry,
[00:17:57.960 --> 00:18:00.640]   that's probably made it in somewhere, standard example.
[00:18:00.640 --> 00:18:02.960]   But there's not enough known.
[00:18:02.960 --> 00:18:07.960]   And you could sort of imagine a database of genealogy,
[00:18:07.960 --> 00:18:09.760]   having date of birth, date of death,
[00:18:09.760 --> 00:18:12.980]   and certain elements like that of private people,
[00:18:12.980 --> 00:18:15.360]   but you couldn't really write a biography.
[00:18:15.360 --> 00:18:17.240]   One of the areas this comes up quite often
[00:18:17.240 --> 00:18:21.920]   is what we call BLP1A, we've got lots of acronyms.
[00:18:21.920 --> 00:18:23.440]   Biography of a living person
[00:18:23.440 --> 00:18:25.520]   who's notable for only one event.
[00:18:25.520 --> 00:18:28.140]   There's a real sort of danger zone.
[00:18:28.140 --> 00:18:31.640]   And the type of example would be a victim of a crime.
[00:18:31.640 --> 00:18:35.120]   So someone who's a victim of a famous serial killer,
[00:18:35.120 --> 00:18:37.320]   but about whom, like really not much is known.
[00:18:37.320 --> 00:18:38.320]   They weren't a public person,
[00:18:38.320 --> 00:18:39.920]   they're just a victim of a crime.
[00:18:39.920 --> 00:18:42.040]   We really shouldn't have an article about that person.
[00:18:42.040 --> 00:18:43.320]   They'll be mentioned, of course,
[00:18:43.320 --> 00:18:46.760]   and maybe the specific crime might have an article.
[00:18:46.760 --> 00:18:49.560]   But for that person, no, not really.
[00:18:49.560 --> 00:18:52.140]   That's not really something that makes any sense,
[00:18:52.140 --> 00:18:54.200]   because how can you write a biography
[00:18:54.200 --> 00:18:56.400]   about someone you don't know much about?
[00:18:56.400 --> 00:18:59.420]   And this is, it varies from field to field.
[00:18:59.420 --> 00:19:01.780]   So for example, for many academics,
[00:19:01.780 --> 00:19:05.160]   we will have an entry that we might not have
[00:19:05.160 --> 00:19:08.440]   in a different context, because for an academic,
[00:19:08.440 --> 00:19:11.760]   it's important to have sort of their career,
[00:19:11.760 --> 00:19:13.840]   what papers they've published, things like that.
[00:19:13.840 --> 00:19:15.600]   You may not know anything about their personal life,
[00:19:15.600 --> 00:19:18.040]   but that's actually not encyclopedically relevant
[00:19:18.040 --> 00:19:21.240]   in the same way that it is for a member of a royal family,
[00:19:21.240 --> 00:19:23.400]   where it's basically all about the family.
[00:19:23.400 --> 00:19:27.340]   So we're fairly nuanced about notability
[00:19:27.340 --> 00:19:28.800]   and where it comes in.
[00:19:28.800 --> 00:19:32.980]   And I've always thought that the term notability,
[00:19:32.980 --> 00:19:34.380]   I think, is a little problematic.
[00:19:34.380 --> 00:19:38.840]   I mean, we struggle about how to talk about it.
[00:19:38.840 --> 00:19:42.620]   The problem with notability is it can feel insulting
[00:19:42.620 --> 00:19:45.080]   to say, oh no, you're not noteworthy.
[00:19:45.080 --> 00:19:46.080]   Well, my mother's noteworthy.
[00:19:46.080 --> 00:19:48.080]   She's a really important person in my life, right?
[00:19:48.080 --> 00:19:49.220]   So that's not right.
[00:19:49.220 --> 00:19:51.100]   But it's more like verifiability.
[00:19:51.100 --> 00:19:53.300]   Is there a way to get information
[00:19:53.300 --> 00:19:56.120]   that actually makes an encyclopedia entry?
[00:19:56.120 --> 00:19:59.660]   - It so happens that there's a Wikipedia page about me,
[00:19:59.660 --> 00:20:01.660]   as I've learned recently.
[00:20:01.660 --> 00:20:06.040]   And the first thought I had when I saw that was,
[00:20:06.040 --> 00:20:10.320]   surely I am not notable enough.
[00:20:10.320 --> 00:20:12.860]   So I was very surprised and grateful
[00:20:12.860 --> 00:20:14.240]   that such a page could exist.
[00:20:14.240 --> 00:20:16.580]   And actually just allow me to say thank you
[00:20:16.580 --> 00:20:20.360]   to all the incredible people that are part of creating
[00:20:20.360 --> 00:20:22.820]   and maintaining Wikipedia.
[00:20:22.820 --> 00:20:24.620]   It's my favorite website on the internet,
[00:20:24.620 --> 00:20:27.040]   the collection of articles that Wikipedia has created.
[00:20:27.040 --> 00:20:28.140]   It's just incredible.
[00:20:28.140 --> 00:20:31.480]   We'll talk about the various details of that.
[00:20:31.480 --> 00:20:36.480]   But the love and care that goes into creating pages
[00:20:36.480 --> 00:20:40.420]   for individuals, for a big pen,
[00:20:40.420 --> 00:20:43.060]   for all this kind of stuff, is just really incredible.
[00:20:43.060 --> 00:20:46.300]   So I just felt the love when I saw that page.
[00:20:46.300 --> 00:20:48.820]   But I also felt, just 'cause I do this podcast,
[00:20:48.820 --> 00:20:51.600]   and I just through this podcast gotten to know
[00:20:51.600 --> 00:20:53.900]   a few individuals that are quite controversial.
[00:20:54.740 --> 00:20:58.060]   I've gotten to be on the receiving end of something quite,
[00:20:58.060 --> 00:21:02.440]   to me as a person who loves other human beings,
[00:21:02.440 --> 00:21:07.080]   I've gone to be at the receiving end of some kind of attacks
[00:21:07.080 --> 00:21:09.160]   through the Wikipedia form.
[00:21:09.160 --> 00:21:12.120]   Like you said, when you look at living individuals,
[00:21:12.120 --> 00:21:13.600]   it can be quite hurtful.
[00:21:13.600 --> 00:21:15.540]   The little details of information.
[00:21:15.540 --> 00:21:20.220]   And because I've become friends with Elon Musk,
[00:21:20.220 --> 00:21:22.600]   and I've interviewed him,
[00:21:22.600 --> 00:21:26.920]   but I've also interviewed people on the left, far left,
[00:21:26.920 --> 00:21:30.140]   people on the right, some people would say far right.
[00:21:30.140 --> 00:21:32.860]   And so now you take a step,
[00:21:32.860 --> 00:21:37.240]   you put your toe into the cold pool of politics,
[00:21:37.240 --> 00:21:39.680]   and the shark emerges from the depths
[00:21:39.680 --> 00:21:40.840]   and pulls you right in.
[00:21:40.840 --> 00:21:43.840]   - Boiling hot pool of politics.
[00:21:43.840 --> 00:21:45.280]   - I guess it's hot.
[00:21:45.280 --> 00:21:47.560]   And so I got to experience some of that.
[00:21:48.760 --> 00:21:53.760]   I think what you also realize is there has to be
[00:21:53.760 --> 00:21:58.960]   for Wikipedia kind of credible sources, verifiable sources.
[00:21:58.960 --> 00:22:02.400]   And there's a dance there because some of the sources
[00:22:02.400 --> 00:22:04.960]   are pieces of journalism.
[00:22:04.960 --> 00:22:06.560]   And of course, journalism operates
[00:22:06.560 --> 00:22:09.920]   under its own complicated incentives,
[00:22:09.920 --> 00:22:13.520]   such that people can write articles that are not factual,
[00:22:13.520 --> 00:22:16.600]   or are cherry picking all the flaws
[00:22:16.600 --> 00:22:18.640]   that you can have in a journalistic article.
[00:22:18.640 --> 00:22:22.440]   And those can be used as sources.
[00:22:22.440 --> 00:22:24.960]   It's like they dance hand in hand.
[00:22:24.960 --> 00:22:28.320]   And so for me, sadly enough,
[00:22:28.320 --> 00:22:31.240]   there was a really kind of concerted attack
[00:22:31.240 --> 00:22:33.400]   to say that I was never at MIT.
[00:22:33.400 --> 00:22:35.220]   I never did anything at MIT.
[00:22:35.220 --> 00:22:39.160]   Just to clarify, I am a research scientist at MIT.
[00:22:39.160 --> 00:22:41.640]   I have been there since 2015.
[00:22:41.640 --> 00:22:42.960]   I'm there today.
[00:22:42.960 --> 00:22:47.640]   I'm at a prestigious, amazing laboratory called LIDS.
[00:22:47.640 --> 00:22:49.600]   And I hope to be there for a long time.
[00:22:49.600 --> 00:22:51.440]   I work on AI, robotics, machine learning.
[00:22:51.440 --> 00:22:53.880]   There's a lot of incredible people there.
[00:22:53.880 --> 00:22:57.480]   And by the way, MIT has been very kind to defend me.
[00:22:57.480 --> 00:23:01.840]   Unlike Wikipedia says, it is not an unpaid position.
[00:23:01.840 --> 00:23:03.660]   There was no controversy.
[00:23:03.660 --> 00:23:08.660]   It was all very calm and happy and almost boring research
[00:23:08.660 --> 00:23:10.520]   that I've been doing there.
[00:23:10.520 --> 00:23:13.880]   And the other thing, because I am half Ukrainian,
[00:23:13.880 --> 00:23:16.800]   half Russian, and I've traveled to Ukraine,
[00:23:16.800 --> 00:23:18.720]   and I will travel to Ukraine again,
[00:23:18.720 --> 00:23:21.320]   and I will travel to Russia
[00:23:21.320 --> 00:23:23.400]   for some very difficult conversations.
[00:23:23.400 --> 00:23:25.200]   My heart's been broken by this war.
[00:23:25.200 --> 00:23:26.560]   I have family in both places.
[00:23:26.560 --> 00:23:28.600]   It's been a really difficult time.
[00:23:28.600 --> 00:23:32.400]   But the little battle about the biography there
[00:23:32.400 --> 00:23:36.380]   also starts becoming important for the first time for me.
[00:23:36.380 --> 00:23:39.280]   I also wanna clarify sort of personally,
[00:23:39.280 --> 00:23:42.960]   I use this opportunity of some inaccuracies there.
[00:23:42.960 --> 00:23:47.040]   My father was not born in Chkalovsk, Russia.
[00:23:47.040 --> 00:23:49.100]   He was born in Kiev, Ukraine.
[00:23:49.100 --> 00:23:53.820]   I was born in Chkalovsk, which is a town not in Russia.
[00:23:53.820 --> 00:23:55.640]   There is a town called that in Russia,
[00:23:55.640 --> 00:23:58.040]   but there's another town in Tajikistan,
[00:23:58.040 --> 00:24:00.760]   which is a former Republic of the Soviet Union.
[00:24:00.760 --> 00:24:05.280]   It is that town is now called B-U-S-T-O-N,
[00:24:05.280 --> 00:24:09.200]   Buston, which is funny 'cause we're now in Austin,
[00:24:09.200 --> 00:24:10.840]   and I also am in Boston.
[00:24:10.840 --> 00:24:12.480]   It seems like my whole life is surrounded
[00:24:12.480 --> 00:24:13.840]   by these kinds of towns.
[00:24:13.840 --> 00:24:15.560]   So I was born in Tajikistan,
[00:24:15.560 --> 00:24:17.480]   and the rest of the biography is interesting,
[00:24:17.480 --> 00:24:21.640]   but my family is very evenly distributed
[00:24:21.640 --> 00:24:23.640]   between their origins and where they grew up,
[00:24:23.640 --> 00:24:24.840]   between Ukraine and Russia,
[00:24:24.840 --> 00:24:27.400]   which adds a whole beautiful complexity
[00:24:27.400 --> 00:24:28.240]   to this whole thing.
[00:24:28.240 --> 00:24:30.920]   So I wanna just correct that.
[00:24:30.920 --> 00:24:34.640]   It's like the fascinating thing about Wikipedia
[00:24:34.640 --> 00:24:38.200]   is in some sense, those little details don't matter.
[00:24:38.200 --> 00:24:40.000]   But in another sense, what I felt
[00:24:40.000 --> 00:24:41.680]   when I saw a Wikipedia page about me
[00:24:41.680 --> 00:24:44.640]   or anybody I know, there's this beautiful
[00:24:44.640 --> 00:24:48.660]   kind of saving that this person existed,
[00:24:48.660 --> 00:24:52.320]   like a community that notices you.
[00:24:52.320 --> 00:24:54.800]   It says, "Huh," like a little,
[00:24:54.800 --> 00:24:57.000]   you see like a butterfly that floats,
[00:24:57.000 --> 00:24:58.440]   and you're like, "Huh."
[00:24:58.440 --> 00:25:00.520]   It's not just any butterfly, it's that one.
[00:25:00.520 --> 00:25:01.440]   I like that one.
[00:25:01.440 --> 00:25:02.720]   Or you see a puppy or something,
[00:25:02.720 --> 00:25:05.280]   or it's this big pen.
[00:25:05.280 --> 00:25:07.800]   This one, I remember this one, it has this scratch,
[00:25:07.800 --> 00:25:09.920]   and you get noticed in that way.
[00:25:09.920 --> 00:25:11.580]   I don't know, it's a beautiful thing
[00:25:11.580 --> 00:25:16.580]   and it's, I mean, maybe it's very silly of me and naive,
[00:25:16.580 --> 00:25:20.680]   but I feel like Wikipedia, in terms of individuals,
[00:25:20.680 --> 00:25:24.880]   is an opportunity to celebrate people,
[00:25:24.880 --> 00:25:26.760]   to celebrate ideas. - For sure, for sure.
[00:25:26.760 --> 00:25:29.580]   - And not a battleground of attacks,
[00:25:29.580 --> 00:25:32.000]   of the kind of stuff we might see on Twitter,
[00:25:32.000 --> 00:25:35.340]   like the mockery, the derision, this kind of stuff.
[00:25:35.340 --> 00:25:36.180]   - For sure.
[00:25:36.180 --> 00:25:37.800]   - And of course, you don't wanna cherry pick,
[00:25:37.800 --> 00:25:39.860]   all of us have flaws and so on,
[00:25:39.860 --> 00:25:44.800]   but it just feels like to highlight a controversy
[00:25:44.800 --> 00:25:47.640]   of some sort, one that doesn't at all represent
[00:25:47.640 --> 00:25:50.800]   the entirety of the human, in most cases, is sad.
[00:25:50.800 --> 00:25:51.720]   - Yeah, yeah, yeah.
[00:25:51.720 --> 00:25:55.200]   So there's a few things to unpack in all that.
[00:25:55.200 --> 00:25:58.240]   So first, one of the things I find really,
[00:25:58.240 --> 00:26:03.240]   always find very interesting is your status with MIT.
[00:26:03.240 --> 00:26:06.560]   Okay, that's upsetting and it's an argument
[00:26:06.560 --> 00:26:08.660]   and can be sorted out.
[00:26:08.660 --> 00:26:11.140]   But then what's interesting is you gave as much time
[00:26:11.140 --> 00:26:13.360]   to that, which is actually important and relevant
[00:26:13.360 --> 00:26:16.800]   to your career and so on, to also where your father
[00:26:16.800 --> 00:26:19.080]   was born, which most people would hardly notice,
[00:26:19.080 --> 00:26:21.040]   but is really meaningful to you.
[00:26:21.040 --> 00:26:22.760]   And I find that a lot when I talk to people
[00:26:22.760 --> 00:26:24.960]   who have a biography in Wikipedia,
[00:26:24.960 --> 00:26:28.520]   is they're often as annoyed by a tiny error
[00:26:28.520 --> 00:26:32.640]   that no one's gonna notice, like this town in Tajikistan
[00:26:32.640 --> 00:26:34.240]   has got a new name and so on.
[00:26:34.240 --> 00:26:35.840]   Nobody even knows what that means or whatever,
[00:26:35.840 --> 00:26:38.120]   but it can be super important.
[00:26:38.120 --> 00:26:42.600]   And so that's one of the reasons for biographies,
[00:26:42.600 --> 00:26:46.120]   we say like human dignity really matters.
[00:26:46.120 --> 00:26:49.520]   And so some of the things have to do with,
[00:26:49.520 --> 00:26:53.080]   and this is a common debate that goes on in Wikipedia,
[00:26:53.080 --> 00:26:55.480]   is what we call undue weight.
[00:26:55.480 --> 00:26:57.240]   So I'll give an example.
[00:26:57.240 --> 00:27:02.120]   There was a article I stumbled across many years ago
[00:27:02.120 --> 00:27:05.880]   about the mayor, or no, he wasn't a mayor,
[00:27:05.880 --> 00:27:08.640]   he was a city council member of,
[00:27:08.640 --> 00:27:10.360]   I think it was Peora, Illinois,
[00:27:10.360 --> 00:27:13.360]   but some small town in the Midwest.
[00:27:13.360 --> 00:27:15.860]   And the entry, he's been on the city council
[00:27:15.860 --> 00:27:17.960]   for 30 years or whatever, he's pretty,
[00:27:17.960 --> 00:27:19.440]   I mean, frankly, pretty boring guy
[00:27:19.440 --> 00:27:22.400]   and seems like a good local city politician.
[00:27:22.400 --> 00:27:24.040]   But in this very short biography,
[00:27:24.040 --> 00:27:26.680]   there was a whole paragraph, a long paragraph,
[00:27:26.680 --> 00:27:31.360]   about his son being arrested for DUI.
[00:27:31.360 --> 00:27:33.080]   And it was clearly undue weight.
[00:27:33.080 --> 00:27:34.840]   It's like, what has this got to do with this guy
[00:27:34.840 --> 00:27:36.160]   if it even deserves a mention?
[00:27:36.160 --> 00:27:40.440]   It wasn't even clear, had he done anything hypocritical?
[00:27:40.440 --> 00:27:42.840]   Had he done himself anything wrong?
[00:27:42.840 --> 00:27:45.440]   Even was his son, his son got a DUI, that's never great,
[00:27:45.440 --> 00:27:47.240]   but it happens to people and it doesn't seem
[00:27:47.240 --> 00:27:49.540]   like a massive scandal for your dad.
[00:27:49.540 --> 00:27:51.480]   So of course, I just took that out immediately.
[00:27:51.480 --> 00:27:53.480]   This is a long, long time ago.
[00:27:53.480 --> 00:27:56.600]   And that's the sort of thing where, you know,
[00:27:56.600 --> 00:27:59.560]   we have to really think about in a biography
[00:27:59.560 --> 00:28:01.880]   and about controversies to say,
[00:28:01.880 --> 00:28:03.200]   is this a real controversy?
[00:28:03.200 --> 00:28:07.440]   So in general, like one of the things we tend to say
[00:28:07.440 --> 00:28:11.360]   is like any section, so if there's a biography
[00:28:11.360 --> 00:28:14.000]   and there's a section called controversies,
[00:28:14.000 --> 00:28:16.040]   that's actually poor practice
[00:28:16.040 --> 00:28:18.880]   because it just invites people to say,
[00:28:18.880 --> 00:28:20.840]   oh, I wanna work on this entry.
[00:28:20.840 --> 00:28:21.920]   See, there's seven sections.
[00:28:21.920 --> 00:28:24.040]   Oh, this one's quite short, can I add something?
[00:28:24.040 --> 00:28:25.360]   Go out and find some more controversies.
[00:28:25.360 --> 00:28:26.480]   No, that's nonsense, right?
[00:28:26.480 --> 00:28:29.840]   And in general, putting it separate from everything else
[00:28:29.840 --> 00:28:30.920]   kind of makes it seem worse
[00:28:30.920 --> 00:28:33.080]   and also doesn't put it in the right context.
[00:28:33.080 --> 00:28:34.320]   Whereas if it's sort of a life flow
[00:28:34.320 --> 00:28:35.240]   and there is a controversy,
[00:28:35.240 --> 00:28:38.560]   there's always potential controversy for anyone,
[00:28:38.560 --> 00:28:41.560]   it should just be sort of worked into the overall article
[00:28:41.560 --> 00:28:43.120]   because then it doesn't become a temptation.
[00:28:43.120 --> 00:28:46.640]   You can contextualize appropriately and so forth.
[00:28:46.640 --> 00:28:47.860]   So that's, you know,
[00:28:47.860 --> 00:28:53.680]   that's part of the whole process.
[00:28:53.680 --> 00:28:56.880]   But I think for me, one of the most important things
[00:28:56.880 --> 00:28:59.920]   is what I call community health.
[00:28:59.920 --> 00:29:02.080]   So yeah, are we gonna get it wrong sometimes?
[00:29:02.080 --> 00:29:03.080]   Yeah, of course.
[00:29:03.080 --> 00:29:05.880]   We're humans and doing good quality,
[00:29:05.880 --> 00:29:08.800]   you know, sort of reference material is hard.
[00:29:08.800 --> 00:29:11.800]   The real question is how do people react,
[00:29:11.800 --> 00:29:15.640]   you know, to a criticism or a complaint or a concern?
[00:29:15.640 --> 00:29:20.640]   And if the reaction is defensiveness or combativeness back,
[00:29:20.640 --> 00:29:24.680]   or if someone's really sort of in there being aggressive
[00:29:24.680 --> 00:29:27.680]   and in the wrong, like, no, no, no, hold on,
[00:29:27.680 --> 00:29:29.240]   we've gotta do this the right way.
[00:29:29.240 --> 00:29:30.800]   You gotta say, okay, hold on, you know,
[00:29:30.800 --> 00:29:32.280]   are there good sources?
[00:29:32.280 --> 00:29:33.840]   Is this contextualized appropriately?
[00:29:33.840 --> 00:29:35.800]   Is it even important enough to mention?
[00:29:35.800 --> 00:29:37.920]   What does it mean?
[00:29:37.920 --> 00:29:43.000]   And sometimes one of the areas where I do think
[00:29:43.000 --> 00:29:45.600]   there is a very complicated flaw,
[00:29:45.600 --> 00:29:47.640]   and you've alluded to it a little bit,
[00:29:47.640 --> 00:29:51.600]   but it's like, we know the media is deeply flawed.
[00:29:51.600 --> 00:29:54.480]   We know that journalism can go wrong.
[00:29:54.480 --> 00:29:56.320]   And I would say, particularly in the last,
[00:29:56.320 --> 00:29:58.000]   whatever, 15 years,
[00:29:58.000 --> 00:30:01.360]   we've seen a real decimation of local media,
[00:30:01.360 --> 00:30:03.240]   local newspapers.
[00:30:03.240 --> 00:30:06.160]   We've seen a real rise in clickbait headlines
[00:30:06.160 --> 00:30:08.840]   and sort of eager focus on anything
[00:30:08.840 --> 00:30:10.080]   that might be controversial.
[00:30:10.080 --> 00:30:11.320]   We've always had that with us, of course.
[00:30:11.320 --> 00:30:13.640]   There's always been tabloid newspapers.
[00:30:13.640 --> 00:30:16.120]   But that makes it a little bit more challenging to say,
[00:30:16.120 --> 00:30:19.440]   okay, how do we sort things out
[00:30:19.440 --> 00:30:20.880]   when we have a pretty good sense
[00:30:20.880 --> 00:30:24.600]   that not every source is valid?
[00:30:24.600 --> 00:30:26.640]   So as an example,
[00:30:27.640 --> 00:30:31.000]   a few years ago, it's been quite a while now,
[00:30:31.000 --> 00:30:36.000]   we deprecated the Mail Online as a source.
[00:30:36.000 --> 00:30:40.640]   And the Mail Online, the digital arm of the Daily Mail,
[00:30:40.640 --> 00:30:41.680]   it's a tabloid.
[00:30:41.680 --> 00:30:46.680]   It's not completely, it's not fake news,
[00:30:46.680 --> 00:30:49.840]   but it does tend to run very hyped up stories.
[00:30:49.840 --> 00:30:51.920]   They really love to attack people
[00:30:51.920 --> 00:30:54.760]   and go on the attack for political reasons and so on.
[00:30:54.760 --> 00:30:55.840]   And it just isn't great.
[00:30:55.840 --> 00:30:57.960]   And so by saying deprecated,
[00:30:57.960 --> 00:30:59.320]   and I think some people say,
[00:30:59.320 --> 00:31:00.400]   oh, you banned the Daily Mail.
[00:31:00.400 --> 00:31:01.800]   No, we didn't ban it as a source.
[00:31:01.800 --> 00:31:04.800]   We just said, look, it's probably not a great source.
[00:31:04.800 --> 00:31:06.120]   You should probably look for a better source.
[00:31:06.120 --> 00:31:09.320]   So certainly, if the Daily Mail runs a headline saying,
[00:31:09.320 --> 00:31:13.800]   new cure for cancer, it's like,
[00:31:13.800 --> 00:31:16.600]   probably there's more serious sources
[00:31:16.600 --> 00:31:17.760]   than a tabloid newspaper.
[00:31:17.760 --> 00:31:21.360]   So in an article about lung cancer,
[00:31:21.360 --> 00:31:23.160]   you probably wouldn't cite the Daily Mail.
[00:31:23.160 --> 00:31:24.600]   That's kind of ridiculous.
[00:31:24.600 --> 00:31:27.160]   But also for celebrities and so forth,
[00:31:27.160 --> 00:31:28.080]   to sort of know, well,
[00:31:28.080 --> 00:31:30.280]   they do cover celebrity gossip a lot,
[00:31:30.280 --> 00:31:32.520]   but they also tend to have vendettas and so forth.
[00:31:32.520 --> 00:31:34.640]   And you really have to step back and go,
[00:31:34.640 --> 00:31:36.200]   is this really encyclopedic
[00:31:36.200 --> 00:31:38.840]   or is this just the Daily Mail going on a rant?
[00:31:38.840 --> 00:31:41.520]   - And some of that requires a great community health.
[00:31:41.520 --> 00:31:43.200]   - It requires massive community health.
[00:31:43.200 --> 00:31:45.880]   - Even for me, for stuff I've seen that's kind of,
[00:31:45.880 --> 00:31:47.960]   if actually iffy about people I know,
[00:31:47.960 --> 00:31:50.600]   things I know about myself,
[00:31:50.600 --> 00:31:55.600]   I still feel like a love for knowledge
[00:31:55.600 --> 00:31:58.800]   emanating from the article.
[00:31:58.800 --> 00:32:01.560]   I feel the community health.
[00:32:01.560 --> 00:32:05.600]   So I will take all slight inaccuracies.
[00:32:05.600 --> 00:32:09.920]   I love it because that means there's people,
[00:32:09.920 --> 00:32:13.360]   for the most part, I feel a respect and love
[00:32:13.360 --> 00:32:14.840]   in the search for knowledge.
[00:32:14.840 --> 00:32:17.920]   Like sometimes, 'cause I also love Stock Overflow,
[00:32:17.920 --> 00:32:20.120]   Stock Exchange for programming related things.
[00:32:20.120 --> 00:32:22.400]   And they can get a little cranky sometimes
[00:32:22.400 --> 00:32:23.920]   to a degree where it's like,
[00:32:23.920 --> 00:32:29.440]   it's not as, like you can feel the dynamics
[00:32:29.440 --> 00:32:32.040]   of the health of the particular community
[00:32:32.040 --> 00:32:33.680]   and sub-communities too,
[00:32:33.680 --> 00:32:37.800]   like a particular C# or Java or Python or whatever.
[00:32:37.800 --> 00:32:40.200]   There's little communities that emerge.
[00:32:40.200 --> 00:32:42.640]   You can feel the levels of toxicity
[00:32:42.640 --> 00:32:45.520]   'cause a little bit of strictness is good,
[00:32:45.520 --> 00:32:48.920]   but a little too much is bad because of the defensiveness.
[00:32:48.920 --> 00:32:51.120]   'Cause when somebody writes an answer
[00:32:51.120 --> 00:32:52.520]   and then somebody else kind of says,
[00:32:52.520 --> 00:32:54.280]   well, modify it and they get defensive
[00:32:54.280 --> 00:32:57.400]   and there's this tension that's not conducive
[00:32:57.400 --> 00:33:01.040]   to improving towards a more truthful depiction
[00:33:01.040 --> 00:33:02.480]   of that topic.
[00:33:02.480 --> 00:33:05.440]   - Yeah, a great example that I really loved
[00:33:05.440 --> 00:33:07.480]   this morning that I saw,
[00:33:07.480 --> 00:33:10.000]   someone left a note on my user talk page
[00:33:10.000 --> 00:33:12.000]   in English Wikipedia saying,
[00:33:12.000 --> 00:33:14.160]   it was quite a dramatic headline,
[00:33:14.160 --> 00:33:16.280]   racist hook on front page.
[00:33:16.280 --> 00:33:17.520]   So we have on the front page of Wikipedia,
[00:33:17.520 --> 00:33:19.880]   we have a little section called, did you know?
[00:33:19.880 --> 00:33:21.480]   And it's just little tidbits and facts,
[00:33:21.480 --> 00:33:22.640]   just things people find interesting.
[00:33:22.640 --> 00:33:25.160]   And there's a whole process for how things get there.
[00:33:25.160 --> 00:33:29.280]   And the one that somebody was raising a question about was,
[00:33:29.280 --> 00:33:34.280]   it was comparing a very well-known US football player, black.
[00:33:34.280 --> 00:33:39.440]   There was a quote from another famous sport person
[00:33:39.440 --> 00:33:43.800]   comparing him to a Lamborghini, clearly a compliment.
[00:33:43.800 --> 00:33:47.400]   And so somebody said, actually, here's a study,
[00:33:47.400 --> 00:33:48.600]   here's some interesting information
[00:33:48.600 --> 00:33:53.600]   about how black sports people are far more often compared
[00:33:53.600 --> 00:33:57.040]   to inanimate objects and given that kind of analogy.
[00:33:57.040 --> 00:34:00.680]   And I think it's demeaning to compare a person to a car,
[00:34:00.680 --> 00:34:01.600]   et cetera, et cetera.
[00:34:01.600 --> 00:34:05.480]   But they said, I'm not deleting it, I'm not removing it,
[00:34:05.480 --> 00:34:07.080]   I just wanna raise the question.
[00:34:07.080 --> 00:34:09.160]   And then there's this really interesting conversation
[00:34:09.160 --> 00:34:13.040]   that goes on where I think the general consensus was,
[00:34:13.040 --> 00:34:17.040]   you know what, this isn't like the alarming headline,
[00:34:17.040 --> 00:34:18.920]   racist thing on the front page of Wikipedia,
[00:34:18.920 --> 00:34:20.960]   that sounds, holy moly, that sounds bad.
[00:34:20.960 --> 00:34:22.320]   But it's sort of like, actually, yeah,
[00:34:22.320 --> 00:34:24.480]   this probably isn't the sort of analogy
[00:34:24.480 --> 00:34:26.040]   that we think is great.
[00:34:26.040 --> 00:34:27.480]   And so we should probably think about
[00:34:27.480 --> 00:34:30.120]   how to improve our language and not compare sports people
[00:34:30.120 --> 00:34:32.840]   to inanimate objects and particularly be aware
[00:34:32.840 --> 00:34:36.400]   of certain racial sensitivities that there might be
[00:34:36.400 --> 00:34:38.640]   around that sort of thing if there is a disparity
[00:34:38.640 --> 00:34:40.520]   in the media of how people are called.
[00:34:40.520 --> 00:34:41.960]   And I just thought, you know what,
[00:34:41.960 --> 00:34:43.360]   nothing for me to weigh in on here,
[00:34:43.360 --> 00:34:44.800]   this is a good conversation.
[00:34:44.800 --> 00:34:48.480]   Like, nobody's saying people should be banned
[00:34:48.480 --> 00:34:51.400]   if they refer to, what was his name,
[00:34:51.400 --> 00:34:53.040]   the fridge, Refrigerator Perry,
[00:34:53.040 --> 00:34:56.840]   the very famous comparison to an inanimate object
[00:34:56.840 --> 00:34:59.480]   of a Chicago Bears player many years ago.
[00:34:59.480 --> 00:35:01.640]   But they're just saying, hey, let's be careful about
[00:35:01.640 --> 00:35:03.920]   analogies that we just pick up from the media.
[00:35:03.920 --> 00:35:06.480]   I said, yeah, you know, that's good.
[00:35:06.480 --> 00:35:09.000]   - On the sort of deprecation of news sources,
[00:35:09.000 --> 00:35:11.320]   really interesting because I think what you're saying
[00:35:11.320 --> 00:35:14.680]   is ultimately you wanna make a article by article decision.
[00:35:15.160 --> 00:35:17.480]   Kind of use your own judgment.
[00:35:17.480 --> 00:35:19.600]   And it's such a subtle thing because
[00:35:19.600 --> 00:35:27.320]   there's just a lot of hit pieces written about
[00:35:27.320 --> 00:35:29.360]   individuals like myself, for example,
[00:35:29.360 --> 00:35:33.280]   that masquerade as kind of an objective,
[00:35:33.280 --> 00:35:35.440]   thorough exploration of a human being.
[00:35:35.440 --> 00:35:38.440]   It's fascinating to watch because controversy
[00:35:38.440 --> 00:35:41.160]   and hit pieces just get more clicks.
[00:35:41.160 --> 00:35:42.280]   - Oh yeah, for sure.
[00:35:42.280 --> 00:35:45.360]   I guess as a Wikipedia contributor,
[00:35:45.360 --> 00:35:49.960]   you start to deeply become aware of that
[00:35:49.960 --> 00:35:51.120]   and start to have a sense,
[00:35:51.120 --> 00:35:54.200]   like a radar of clickbait versus truth.
[00:35:54.200 --> 00:35:55.840]   Like to pick out the truth
[00:35:55.840 --> 00:35:58.160]   from the clickbaity type language.
[00:35:58.160 --> 00:36:00.640]   - Oh yeah, I mean, it's really important.
[00:36:00.640 --> 00:36:03.800]   And we talk a lot about weasel words.
[00:36:03.800 --> 00:36:09.920]   Actually, I'm sure we'll end up talking about AI
[00:36:09.920 --> 00:36:13.720]   and Chat GPT, but just to quickly mention in this area,
[00:36:13.720 --> 00:36:16.620]   I think one of the potentially powerful tools,
[00:36:16.620 --> 00:36:20.400]   because it is quite good at this,
[00:36:20.400 --> 00:36:23.720]   I've played around with and practiced it quite a lot,
[00:36:23.720 --> 00:36:28.720]   but Chat GPT-4 is really quite able to take a passage
[00:36:28.720 --> 00:36:33.960]   and point out potentially biased terms,
[00:36:33.960 --> 00:36:37.200]   to rewrite it to be more neutral.
[00:36:37.200 --> 00:36:41.800]   Now, it is a bit anodyne and it's a bit cliched.
[00:36:41.800 --> 00:36:44.440]   So sometimes it just takes the spirit out of something
[00:36:44.440 --> 00:36:45.500]   that's actually not bad.
[00:36:45.500 --> 00:36:48.520]   It's just like poetic language and you're like,
[00:36:48.520 --> 00:36:50.280]   well, okay, that's not actually helping.
[00:36:50.280 --> 00:36:51.760]   But in many cases,
[00:36:51.760 --> 00:36:53.120]   I think that sort of thing is quite interesting.
[00:36:53.120 --> 00:36:54.520]   And I'm also interested in,
[00:36:54.520 --> 00:37:02.360]   can you imagine where you feed in a Wikipedia entry
[00:37:02.360 --> 00:37:06.400]   and all the sources and you say,
[00:37:06.400 --> 00:37:08.040]   help me find anything in the article
[00:37:08.040 --> 00:37:12.520]   that is not accurately reflecting what's in the sources.
[00:37:12.520 --> 00:37:14.400]   And that doesn't have to be perfect.
[00:37:14.400 --> 00:37:17.680]   It only has to be good enough to be useful to community.
[00:37:17.680 --> 00:37:21.720]   So if it scans an article and all the sources and you say,
[00:37:21.720 --> 00:37:25.280]   oh, it came back with 10 suggestions
[00:37:25.280 --> 00:37:26.480]   and seven of them were decent
[00:37:26.480 --> 00:37:28.520]   and three of them it just didn't understand,
[00:37:28.520 --> 00:37:30.880]   well, actually that's probably worth my time to do.
[00:37:30.880 --> 00:37:35.880]   And it can help us really more quickly
[00:37:36.720 --> 00:37:40.720]   get good people to sort of review obscure entries
[00:37:40.720 --> 00:37:41.880]   and things like that.
[00:37:41.880 --> 00:37:44.040]   - So just as a small aside on that,
[00:37:44.040 --> 00:37:46.640]   and we'll probably talk about language models a little bit
[00:37:46.640 --> 00:37:50.160]   or a lot more, but one of the articles,
[00:37:50.160 --> 00:37:52.400]   one of the hit pieces about me,
[00:37:52.400 --> 00:37:55.320]   the journalist actually was very straightforward
[00:37:55.320 --> 00:37:58.120]   and honest about having used GPT
[00:37:58.120 --> 00:37:59.440]   to write part of the article.
[00:37:59.440 --> 00:38:00.280]   - Oh, interesting.
[00:38:00.280 --> 00:38:01.840]   - And then finding that it made an error
[00:38:01.840 --> 00:38:04.920]   and apologize for the error that GPT-4 generated,
[00:38:04.920 --> 00:38:07.880]   which has this kind of interesting loop,
[00:38:07.880 --> 00:38:12.000]   which the articles are used to write Wikipedia pages.
[00:38:12.000 --> 00:38:13.960]   GPT is trained on Wikipedia.
[00:38:13.960 --> 00:38:18.280]   And there's like this interesting loop
[00:38:18.280 --> 00:38:23.200]   where the weasel words and the nuances can get lost
[00:38:23.200 --> 00:38:27.880]   or can propagate even though they're not grounded in reality.
[00:38:27.880 --> 00:38:31.560]   Somehow in the generation of the language model,
[00:38:31.560 --> 00:38:35.280]   new truths can be created and kind of linger.
[00:38:35.280 --> 00:38:38.480]   - Yeah, there's a famous web comic
[00:38:38.480 --> 00:38:40.760]   that's titled "Cytogenesis,"
[00:38:40.760 --> 00:38:45.760]   which is about how something, an error's in Wikipedia,
[00:38:45.760 --> 00:38:47.280]   and there's no source for it,
[00:38:47.280 --> 00:38:50.840]   but then a lazy journalist reads it and writes the source.
[00:38:50.840 --> 00:38:53.040]   And then some helpful Wikipedian spots
[00:38:53.040 --> 00:38:54.520]   that it has no source, finds the source,
[00:38:54.520 --> 00:38:56.080]   and adds it to Wikipedia.
[00:38:56.080 --> 00:38:57.200]   And voila, magic.
[00:38:57.200 --> 00:38:58.440]   This happened to me once.
[00:38:59.720 --> 00:39:01.020]   Well, it nearly happened.
[00:39:01.020 --> 00:39:04.400]   There was this, I mean, it was really brief.
[00:39:04.400 --> 00:39:05.360]   I went back and researched it.
[00:39:05.360 --> 00:39:06.760]   I'm like, "This is really odd."
[00:39:06.760 --> 00:39:08.280]   So "Biography" magazine,
[00:39:08.280 --> 00:39:11.460]   which is a magazine published by the Biography TV channel,
[00:39:11.460 --> 00:39:15.440]   had a profile of me, and it said,
[00:39:15.440 --> 00:39:19.120]   "In his spare time," I'm not quoting exactly.
[00:39:19.120 --> 00:39:20.000]   I've spent many years, but,
[00:39:20.000 --> 00:39:22.960]   "In his spare time, he enjoys playing chess with friends."
[00:39:22.960 --> 00:39:24.400]   I thought, "Wow, that sounds great.
[00:39:24.400 --> 00:39:26.920]   Like, I would like to be that guy, but actually,
[00:39:26.920 --> 00:39:28.720]   I mean, I play chess with my kids sometimes,
[00:39:28.720 --> 00:39:31.360]   but no, it's not a hobby of mine."
[00:39:31.360 --> 00:39:36.360]   And I was like, "Where did they get that?"
[00:39:36.360 --> 00:39:37.720]   And I contacted the magazine.
[00:39:37.720 --> 00:39:38.560]   I said, "Where'd that come from?"
[00:39:38.560 --> 00:39:39.840]   They said, "Oh, it was in Wikipedia."
[00:39:39.840 --> 00:39:41.000]   I looked in the history.
[00:39:41.000 --> 00:39:42.400]   There had been vandalism of Wikipedia,
[00:39:42.400 --> 00:39:44.440]   which was not, you know, it's not damaging.
[00:39:44.440 --> 00:39:46.480]   It's just false.
[00:39:46.480 --> 00:39:48.880]   So, and it had already been removed.
[00:39:48.880 --> 00:39:49.960]   But then I thought, "Oh, gosh, well,
[00:39:49.960 --> 00:39:51.160]   I better mention this to people,
[00:39:51.160 --> 00:39:53.000]   because otherwise, somebody's gonna read that,
[00:39:53.000 --> 00:39:54.000]   and they're gonna add it to the entry,
[00:39:54.000 --> 00:39:56.200]   and it's gonna take on a life of its own."
[00:39:56.200 --> 00:39:58.000]   And then sometimes I wonder if it has,
[00:39:58.000 --> 00:40:00.560]   because I've been, I was invited a few years ago
[00:40:00.560 --> 00:40:02.280]   to do the ceremonial first move
[00:40:02.280 --> 00:40:03.680]   in the World Chess Championship,
[00:40:03.680 --> 00:40:05.160]   and I thought, "I wonder if they think
[00:40:05.160 --> 00:40:07.000]   I'm a really big chess enthusiast,"
[00:40:07.000 --> 00:40:09.720]   because they read this biography magazine article.
[00:40:09.720 --> 00:40:13.120]   So, but that problem,
[00:40:13.120 --> 00:40:14.640]   when we think about large language models
[00:40:14.640 --> 00:40:16.800]   and the ability to quickly generate
[00:40:16.800 --> 00:40:20.080]   very plausible but not true content,
[00:40:20.080 --> 00:40:21.880]   I think is something that there's gonna be
[00:40:21.880 --> 00:40:25.120]   a lot of shakeout, a lot of implications of that.
[00:40:25.120 --> 00:40:26.840]   - What would be hilarious is,
[00:40:26.840 --> 00:40:29.560]   because of the social pressure of Wikipedia
[00:40:29.560 --> 00:40:31.200]   and the momentum, you would actually start
[00:40:31.200 --> 00:40:32.840]   playing a lot more chess.
[00:40:32.840 --> 00:40:36.400]   Just to, not only the articles are written
[00:40:36.400 --> 00:40:40.360]   based on Wikipedia, but your own life trajectory changes
[00:40:40.360 --> 00:40:43.560]   because of Wikipedia, just to make it more convenient.
[00:40:43.560 --> 00:40:44.920]   - Yeah. - Aspire to.
[00:40:44.920 --> 00:40:47.160]   - Aspire to, yeah, aspirational.
[00:40:47.160 --> 00:40:50.600]   - What, if we could just talk about that
[00:40:50.600 --> 00:40:53.160]   before we jump back to some other
[00:40:53.160 --> 00:40:54.720]   interesting topics on Wikipedia.
[00:40:54.760 --> 00:40:58.200]   Let's talk about GPT-4 and large language models.
[00:40:58.200 --> 00:41:01.560]   So they are, in part, trained on Wikipedia content.
[00:41:01.560 --> 00:41:02.400]   - Yeah.
[00:41:02.400 --> 00:41:06.640]   - What are the pros and cons of these language models?
[00:41:06.640 --> 00:41:07.480]   What are your thoughts?
[00:41:07.480 --> 00:41:10.040]   - Yeah, so, I mean, there's a lot of stuff going on.
[00:41:10.040 --> 00:41:12.400]   Obviously, the technology's moved very quickly
[00:41:12.400 --> 00:41:15.200]   in the last six months and looks poised to do so
[00:41:15.200 --> 00:41:16.520]   for some time to come.
[00:41:16.520 --> 00:41:20.320]   So, first things first, I mean,
[00:41:20.320 --> 00:41:24.600]   part of our philosophy is the open licensing,
[00:41:24.600 --> 00:41:27.480]   the free licensing, the idea that, you know,
[00:41:27.480 --> 00:41:28.640]   this is what we're here for.
[00:41:28.640 --> 00:41:33.000]   We are a volunteer community and we write this encyclopedia.
[00:41:33.000 --> 00:41:37.080]   We give it to the world to do what you like with.
[00:41:37.080 --> 00:41:39.400]   You can modify it, redistribute it,
[00:41:39.400 --> 00:41:41.280]   redistribute modified versions,
[00:41:41.280 --> 00:41:44.320]   commercially, non-commercially, this is the licensing.
[00:41:44.320 --> 00:41:46.440]   So in that sense, of course, it's completely fine.
[00:41:46.440 --> 00:41:48.400]   Now, we do worry a bit about attribution
[00:41:48.400 --> 00:41:51.920]   because it is a Creative Commons
[00:41:51.920 --> 00:41:54.240]   attribution share-alike license.
[00:41:54.240 --> 00:41:56.080]   So, attribution is important,
[00:41:56.080 --> 00:41:57.680]   not just because of our licensing model
[00:41:57.680 --> 00:41:59.440]   and things like that, but it's just,
[00:41:59.440 --> 00:42:02.440]   proper attribution is just good intellectual practice.
[00:42:02.440 --> 00:42:07.120]   And that's a really hard, complicated question.
[00:42:07.120 --> 00:42:13.000]   You know, if I were to write something
[00:42:13.000 --> 00:42:16.440]   about my visit here, I might say in a blog post,
[00:42:16.440 --> 00:42:21.440]   you know, I was in Austin, which is a city in Texas.
[00:42:21.440 --> 00:42:23.840]   I'm not gonna put a source for Austin as a city in Texas.
[00:42:23.840 --> 00:42:24.920]   That's just general knowledge.
[00:42:24.920 --> 00:42:26.920]   I learned it somewhere, I can't tell you where.
[00:42:26.920 --> 00:42:30.440]   So you don't have to cite and reference every single thing.
[00:42:30.440 --> 00:42:32.320]   But you know, if I actually did research
[00:42:32.320 --> 00:42:33.720]   and I used something very heavily,
[00:42:33.720 --> 00:42:37.720]   it's just morally proper to give your sources.
[00:42:37.720 --> 00:42:39.520]   So we would like to see that.
[00:42:39.520 --> 00:42:43.760]   And obviously, you know, they call it grounding.
[00:42:43.760 --> 00:42:46.040]   So particularly people at Google are really keen
[00:42:46.040 --> 00:42:48.920]   on figuring out grounding.
[00:42:48.920 --> 00:42:49.760]   - Such a cool term.
[00:42:50.600 --> 00:42:53.840]   Any text that's generated, trying to ground it
[00:42:53.840 --> 00:42:58.120]   to the Wikipedia quality source.
[00:42:58.120 --> 00:42:59.960]   I mean, like the same kind of standard
[00:42:59.960 --> 00:43:02.600]   of what a source means that Wikipedia uses,
[00:43:02.600 --> 00:43:05.320]   the same kind of source will be generated.
[00:43:05.320 --> 00:43:06.160]   - The same kind of thing.
[00:43:06.160 --> 00:43:08.240]   And of course, one of the biggest flaws
[00:43:08.240 --> 00:43:12.840]   in Chattopadhyay right now is that it just literally
[00:43:12.840 --> 00:43:17.000]   will make things up just to be amiable, I think.
[00:43:17.000 --> 00:43:19.400]   It's programmed to be very helpful and amiable.
[00:43:19.400 --> 00:43:21.600]   It doesn't really know or care about the truth.
[00:43:21.600 --> 00:43:25.480]   - And get bullied into, it can kind of be convinced into.
[00:43:25.480 --> 00:43:26.960]   - Well, but like this morning,
[00:43:26.960 --> 00:43:29.320]   the story I was telling earlier
[00:43:29.320 --> 00:43:32.680]   about comparing a football player to a Lamborghini,
[00:43:32.680 --> 00:43:34.400]   and I thought, "Is that really racial?
[00:43:34.400 --> 00:43:35.480]   "I don't know."
[00:43:35.480 --> 00:43:36.640]   But I'm just, I'm mulling it over.
[00:43:36.640 --> 00:43:39.200]   And I thought, "I'm gonna go to ChattGBT."
[00:43:39.200 --> 00:43:41.140]   So I sent to ChattGBT4, I said,
[00:43:41.140 --> 00:43:44.480]   "You know, this happened in Wikipedia.
[00:43:44.480 --> 00:43:48.080]   "Can you think of examples where a white athlete
[00:43:48.080 --> 00:43:53.000]   "has been compared to a fast car inanimate object?"
[00:43:53.000 --> 00:43:55.560]   And it comes back with a very plausible essay
[00:43:55.560 --> 00:43:58.640]   where it tells why these analogies are common in sport.
[00:43:58.640 --> 00:43:59.800]   I said, "No, no, I really,
[00:43:59.800 --> 00:44:02.000]   "could you give me some specific examples?"
[00:44:02.000 --> 00:44:05.000]   So it gives me three specific examples, very plausible,
[00:44:05.000 --> 00:44:07.520]   correct names of athletes and contemporaries
[00:44:07.520 --> 00:44:09.160]   and all of that, could have been true.
[00:44:09.160 --> 00:44:12.000]   Googled every single quote and none of them existed.
[00:44:12.000 --> 00:44:14.120]   And so I'm like, "Well, that's really not good."
[00:44:14.120 --> 00:44:17.680]   Like, I wanted to explore a thought process
[00:44:17.680 --> 00:44:19.680]   I was in, I thought, first I thought,
[00:44:19.680 --> 00:44:20.560]   "How do I Google?"
[00:44:20.560 --> 00:44:21.960]   And it's like, "Well, it's kind of a hard thing to Google
[00:44:21.960 --> 00:44:24.120]   "'cause unless somebody's written about this specific topic,
[00:44:24.120 --> 00:44:26.660]   "it's, you know, it's a large language model,
[00:44:26.660 --> 00:44:28.020]   "it's processed all this data,
[00:44:28.020 --> 00:44:29.760]   "it can probably piece that together for me."
[00:44:29.760 --> 00:44:30.680]   But it just can't yet.
[00:44:30.680 --> 00:44:35.680]   So I think, I hope that ChattGBT5, six, seven,
[00:44:35.680 --> 00:44:39.280]   you know, three to five years,
[00:44:39.280 --> 00:44:44.280]   I'm hoping we'll see a much higher level of accuracy
[00:44:45.880 --> 00:44:48.240]   where when you ask a question like that,
[00:44:48.240 --> 00:44:51.640]   I think instead of being quite so eager to please
[00:44:51.640 --> 00:44:53.360]   by giving you a plausible sounding answer,
[00:44:53.360 --> 00:44:55.040]   it's just like, "Don't know."
[00:44:55.040 --> 00:45:00.040]   - Or maybe display the how much bullshit
[00:45:00.040 --> 00:45:02.200]   might be in this generated text.
[00:45:02.200 --> 00:45:05.440]   Like, "I really would like to make you happy right now,
[00:45:05.440 --> 00:45:07.480]   "but I'm really stretched thin with this generation."
[00:45:07.480 --> 00:45:10.320]   - Well, it's one of the things I've said for a long time.
[00:45:10.320 --> 00:45:13.200]   So in Wikipedia, one of the great things we do
[00:45:13.200 --> 00:45:14.880]   may not be great for our reputation
[00:45:14.880 --> 00:45:17.840]   except in a deeper sense for the longterm, I think it is.
[00:45:17.840 --> 00:45:20.440]   But you know, we'll all be unnoticed that says,
[00:45:20.440 --> 00:45:22.600]   "The neutrality of this section has been disputed,"
[00:45:22.600 --> 00:45:25.680]   or, "The following section doesn't cite any sources."
[00:45:25.680 --> 00:45:28.200]   And I always joke, you know,
[00:45:28.200 --> 00:45:30.680]   sometimes I wish the New York Times would run a banner
[00:45:30.680 --> 00:45:32.600]   saying, "The neutrality of this has been disputed."
[00:45:32.600 --> 00:45:33.480]   They could give us an,
[00:45:33.480 --> 00:45:34.720]   we had a big fight in the newsroom
[00:45:34.720 --> 00:45:36.840]   as to whether to run this or not,
[00:45:36.840 --> 00:45:38.520]   but we thought it's important enough to bring it to you.
[00:45:38.520 --> 00:45:40.680]   But just be aware that not all the journalists
[00:45:40.680 --> 00:45:42.280]   are on board with, "Oh, that's actually interesting,
[00:45:42.280 --> 00:45:43.280]   "and that's fine."
[00:45:43.280 --> 00:45:45.880]   I would trust them more for that level of transparency.
[00:45:45.880 --> 00:45:48.040]   So yeah, similarly, Chattopadhyay should say,
[00:45:48.040 --> 00:45:50.120]   "Yeah, 87% bullshit."
[00:45:50.120 --> 00:45:52.800]   - Well, the neutrality one is really interesting
[00:45:52.800 --> 00:45:56.600]   'cause that's basically a summary of the discussions
[00:45:56.600 --> 00:45:58.320]   that are going on underneath.
[00:45:58.320 --> 00:46:01.200]   It would be amazing if, I should be honest,
[00:46:01.200 --> 00:46:03.320]   I don't look at the top page often.
[00:46:03.320 --> 00:46:07.800]   It would be nice somehow if there was kind of a summary
[00:46:07.800 --> 00:46:09.960]   in this banner way of like,
[00:46:10.040 --> 00:46:14.000]   this, lots of wars have been fought on this here land
[00:46:14.000 --> 00:46:16.360]   for this here paragraph.
[00:46:16.360 --> 00:46:17.720]   - It's really interesting, yeah.
[00:46:17.720 --> 00:46:18.960]   I hadn't thought of that.
[00:46:18.960 --> 00:46:20.840]   Because one of the things I do spend a lot of time
[00:46:20.840 --> 00:46:23.080]   thinking about these days, and people have found it,
[00:46:23.080 --> 00:46:26.760]   we're moving slowly, but we are moving,
[00:46:26.760 --> 00:46:29.960]   thinking about, okay, these tools exist.
[00:46:29.960 --> 00:46:32.520]   Are there ways that this stuff can be useful
[00:46:32.520 --> 00:46:33.960]   to our community?
[00:46:33.960 --> 00:46:36.080]   Because a part of it is we do approach things
[00:46:36.080 --> 00:46:39.280]   in a non-commercial way, in a really deep sense.
[00:46:39.280 --> 00:46:41.920]   It's like, it's been great that Wikipedia
[00:46:41.920 --> 00:46:43.320]   has become very popular, but really,
[00:46:43.320 --> 00:46:47.080]   we're a community whose hobby is writing an encyclopedia.
[00:46:47.080 --> 00:46:48.920]   That's first, and if it's popular, great.
[00:46:48.920 --> 00:46:50.720]   If it's not, okay, we might have trouble paying
[00:46:50.720 --> 00:46:53.760]   for more servers, but it'll be fine.
[00:46:53.760 --> 00:46:57.080]   And so how do we help the community use these tools?
[00:46:57.080 --> 00:46:59.120]   What are the ways that these tools can support people?
[00:46:59.120 --> 00:47:01.480]   One example I never thought about,
[00:47:01.480 --> 00:47:04.360]   I'm gonna start playing with it, is feed in the article
[00:47:04.360 --> 00:47:06.280]   and feed in the talk page and say,
[00:47:06.280 --> 00:47:08.880]   "Can you suggest some warnings in the article
[00:47:08.880 --> 00:47:11.120]   "based on the conversations in the talk page?"
[00:47:11.120 --> 00:47:12.720]   I think it might be good at that.
[00:47:12.720 --> 00:47:15.680]   It might get it wrong sometimes, but again,
[00:47:15.680 --> 00:47:18.280]   if it's reasonably successful at doing that,
[00:47:18.280 --> 00:47:21.360]   and you can say, "Oh, actually, yeah, it does suggest
[00:47:21.360 --> 00:47:24.640]   "the neutrality of this has been disputed
[00:47:24.640 --> 00:47:27.880]   "on a section that has a seven-page discussion in the back,"
[00:47:27.880 --> 00:47:28.840]   that might be useful.
[00:47:28.840 --> 00:47:29.680]   I don't know.
[00:47:29.680 --> 00:47:30.800]   We're playing with it. - Yeah, I mean,
[00:47:30.800 --> 00:47:35.520]   some more color to the, not neutrality,
[00:47:35.520 --> 00:47:40.000]   but also the amount of emotion laid in
[00:47:40.000 --> 00:47:44.360]   in the exploration of this particular part of the topic.
[00:47:44.360 --> 00:47:49.080]   It might actually help you look at more controversial pages,
[00:47:49.080 --> 00:47:52.080]   like a page on war in Ukraine
[00:47:52.080 --> 00:47:54.340]   or a page on Israel and Palestine.
[00:47:54.340 --> 00:47:56.480]   There could be parts that everyone agrees on,
[00:47:56.480 --> 00:47:58.360]   and there's parts that are just like--
[00:47:58.360 --> 00:47:59.360]   - Tough. - Tough.
[00:47:59.360 --> 00:48:00.640]   - The hard parts, yeah.
[00:48:00.640 --> 00:48:03.240]   - It would be nice to, when looking at those beautiful,
[00:48:03.240 --> 00:48:06.400]   long articles, to know, like, all right,
[00:48:06.400 --> 00:48:09.160]   let me just take in some stuff where everybody agrees on.
[00:48:09.160 --> 00:48:11.520]   - I can give an example that I haven't looked at
[00:48:11.520 --> 00:48:13.520]   in a long time, but I was really pleased
[00:48:13.520 --> 00:48:15.480]   with what I saw at the time.
[00:48:15.480 --> 00:48:20.160]   So the discussion was that they're building something
[00:48:20.160 --> 00:48:24.300]   in Israel, and for their own political reasons,
[00:48:24.300 --> 00:48:27.520]   one side calls it a wall,
[00:48:27.520 --> 00:48:30.920]   hearkening back to Berlin Wall, apartheid.
[00:48:30.920 --> 00:48:32.920]   The other calls it a security fence.
[00:48:32.920 --> 00:48:34.920]   So we can understand quite quickly,
[00:48:34.920 --> 00:48:36.640]   if we give it a moment's thought, like, okay,
[00:48:36.640 --> 00:48:40.200]   I understand why people would have this grappling
[00:48:40.200 --> 00:48:43.460]   over the language, like, okay, you wanna highlight
[00:48:43.460 --> 00:48:45.020]   the negative aspects of this,
[00:48:45.020 --> 00:48:46.440]   and you wanna highlight the positive aspects,
[00:48:46.440 --> 00:48:48.760]   so you're gonna try and choose a different name.
[00:48:48.760 --> 00:48:52.600]   And so there was this really fantastic Wikipedia discussion
[00:48:52.600 --> 00:48:55.280]   on the talk page, how do we word that paragraph
[00:48:55.280 --> 00:48:57.360]   to talk about the different naming?
[00:48:57.360 --> 00:48:59.080]   It's called this by Israelis, it's called this
[00:48:59.080 --> 00:49:04.000]   by Palestinians, and how you explain that to people
[00:49:04.000 --> 00:49:05.520]   could be quite charged, right?
[00:49:05.520 --> 00:49:09.480]   You could easily explain, oh, there's this difference,
[00:49:09.480 --> 00:49:11.320]   and it's because this side's good and this side's bad,
[00:49:11.320 --> 00:49:12.320]   and that's why there's a difference.
[00:49:12.320 --> 00:49:15.660]   Or you could say, actually, let's just,
[00:49:15.660 --> 00:49:17.800]   let's try and really stay as neutral as we can
[00:49:17.800 --> 00:49:18.880]   and try to explain the reason.
[00:49:18.880 --> 00:49:22.920]   So you may come away from it with a concept.
[00:49:22.920 --> 00:49:26.400]   Oh, okay, I understand what this debate is about now.
[00:49:26.400 --> 00:49:31.400]   And just the term Israel-Palestine conflict
[00:49:31.400 --> 00:49:36.780]   is still the title of a page of Wikipedia,
[00:49:36.780 --> 00:49:41.780]   but the word conflict is something that is a charged word.
[00:49:41.780 --> 00:49:42.720]   - Of course, yeah.
[00:49:42.720 --> 00:49:47.520]   - Because from the Palestinian side or from certain sides,
[00:49:47.520 --> 00:49:50.180]   the word conflict doesn't accurately describe the situation
[00:49:50.180 --> 00:49:52.780]   because if you see it as a genocide,
[00:49:52.780 --> 00:49:54.800]   one-way genocide is not a conflict,
[00:49:54.800 --> 00:49:58.220]   because to people that discuss,
[00:49:58.220 --> 00:50:01.680]   that challenge the word conflict,
[00:50:01.680 --> 00:50:03.480]   they see conflict is when there's
[00:50:03.480 --> 00:50:06.000]   two equally powerful sides fighting.
[00:50:06.000 --> 00:50:07.760]   - Yeah, yeah, no, it's hard.
[00:50:07.760 --> 00:50:11.720]   And in a number of cases, so this actually speaks
[00:50:11.720 --> 00:50:14.580]   to a slightly broader phenomenon,
[00:50:14.580 --> 00:50:16.540]   which is there are a number of cases
[00:50:16.540 --> 00:50:20.280]   where there is no one word that can get consensus.
[00:50:20.280 --> 00:50:24.500]   And in the body of an article, that's usually okay,
[00:50:24.500 --> 00:50:26.600]   because we can explain the whole thing.
[00:50:26.600 --> 00:50:28.040]   You can come away with an understanding
[00:50:28.040 --> 00:50:30.520]   of why each side wants to use a certain word,
[00:50:30.520 --> 00:50:33.860]   but there are some aspects, like the page has to have a title.
[00:50:33.860 --> 00:50:35.360]   So, there's that.
[00:50:35.360 --> 00:50:40.160]   Same thing with certain things like photos.
[00:50:40.160 --> 00:50:43.200]   It's like, well, there's different photos, which one's best?
[00:50:43.200 --> 00:50:44.560]   A lot of different views on that,
[00:50:44.560 --> 00:50:47.000]   but at the end of the day, you need the lead photo,
[00:50:47.000 --> 00:50:49.100]   'cause there's one slot for a lead photo.
[00:50:49.100 --> 00:50:51.300]   Categories is another one.
[00:50:52.320 --> 00:50:56.540]   So, at one point, I have no idea if it's in there today,
[00:50:56.540 --> 00:51:01.540]   but I don't think so, I was listed in,
[00:51:01.540 --> 00:51:04.940]   American entrepreneurs, fine, American atheists.
[00:51:04.940 --> 00:51:08.580]   And I said, hmm, that doesn't feel right to me.
[00:51:08.580 --> 00:51:10.420]   Just personally, it's true.
[00:51:10.420 --> 00:51:14.980]   I mean, I wouldn't disagree with the objective fact of it,
[00:51:14.980 --> 00:51:17.020]   but when you click the category and you see
[00:51:17.020 --> 00:51:20.700]   a lot of people who are, you might say,
[00:51:20.700 --> 00:51:23.760]   American atheist activist, 'cause that's their big issue.
[00:51:23.760 --> 00:51:27.540]   So, Madeline Murray O'Hare, or various famous people,
[00:51:27.540 --> 00:51:29.480]   Richard Dawkins, who make it a big part
[00:51:29.480 --> 00:51:31.920]   of their public argument and persona,
[00:51:31.920 --> 00:51:32.760]   but that's not true of me.
[00:51:32.760 --> 00:51:34.480]   It's just like my private personal belief.
[00:51:34.480 --> 00:51:37.040]   It doesn't really, it's not something I campaign about.
[00:51:37.040 --> 00:51:39.040]   So, it felt weird to put me in the category,
[00:51:39.040 --> 00:51:41.760]   but what category would you put?
[00:51:41.760 --> 00:51:42.960]   And do you need that guy?
[00:51:42.960 --> 00:51:45.960]   In this case, I argued, it doesn't need that guy.
[00:51:45.960 --> 00:51:48.240]   Like, that's not, I don't speak about it publicly,
[00:51:48.240 --> 00:51:50.480]   except incidentally from time to time.
[00:51:50.480 --> 00:51:52.500]   I don't campaign about it, so it's weird to put me
[00:51:52.500 --> 00:51:54.260]   with this group of people.
[00:51:54.260 --> 00:51:55.660]   And that argument carried the day.
[00:51:55.660 --> 00:51:57.300]   I hope not just because it was me,
[00:51:57.300 --> 00:52:00.300]   but categories can be like that,
[00:52:00.300 --> 00:52:03.580]   where you're either in the category or you're not.
[00:52:03.580 --> 00:52:06.220]   And sometimes it's a lot more complicated than that.
[00:52:06.220 --> 00:52:09.300]   And is it, again, we go back to, is it undue weight?
[00:52:09.300 --> 00:52:15.140]   If someone who is now prominent in public life
[00:52:15.140 --> 00:52:18.340]   and generally considered to be a good person
[00:52:20.340 --> 00:52:25.340]   was convicted of something, let's say DUI,
[00:52:25.340 --> 00:52:27.720]   when they were young, we normally,
[00:52:27.720 --> 00:52:30.040]   in normal sort of discourse, we don't think,
[00:52:30.040 --> 00:52:31.280]   oh, this person should be in the category
[00:52:31.280 --> 00:52:32.960]   of American criminals.
[00:52:32.960 --> 00:52:35.600]   Because you think, oh, a criminal, technically speaking,
[00:52:35.600 --> 00:52:39.000]   it's against the law to drive under the influence of alcohol
[00:52:39.000 --> 00:52:42.520]   and you were arrested and you spent a month in prison
[00:52:42.520 --> 00:52:45.280]   or whatever, but it's odd to say that's a criminal.
[00:52:45.280 --> 00:52:48.680]   So, just as an example in this area is,
[00:52:50.080 --> 00:52:51.500]   Mark Wahlberg, Marky Mark,
[00:52:51.500 --> 00:52:52.340]   that's what I always think of him as,
[00:52:52.340 --> 00:52:54.860]   'cause that was his first sort of famous name,
[00:52:54.860 --> 00:52:57.500]   who I wouldn't think should be listed
[00:52:57.500 --> 00:53:00.300]   as in the category American criminal,
[00:53:00.300 --> 00:53:03.780]   even though he was convicted of quite a bad crime
[00:53:03.780 --> 00:53:04.740]   when he was a young person,
[00:53:04.740 --> 00:53:07.060]   but we don't think of him as a criminal.
[00:53:07.060 --> 00:53:08.420]   Should the entry talk about that?
[00:53:08.420 --> 00:53:11.820]   Yeah, that's actually an important part of his life story,
[00:53:11.820 --> 00:53:13.100]   that he had a very rough youth
[00:53:13.100 --> 00:53:15.820]   and he could have gone down a really dark path
[00:53:15.820 --> 00:53:18.260]   and he turned his life around, that's actually interesting.
[00:53:18.260 --> 00:53:20.740]   So, categories are tricky.
[00:53:20.740 --> 00:53:22.140]   - Especially with people,
[00:53:22.140 --> 00:53:27.000]   because we like to assign labels to people
[00:53:27.000 --> 00:53:30.160]   and to ideas somehow, and those labels stick.
[00:53:30.160 --> 00:53:31.200]   - Yeah. - And there's certain words
[00:53:31.200 --> 00:53:34.260]   that have a lot of power, like criminal,
[00:53:34.260 --> 00:53:36.900]   like political, left, right, center,
[00:53:36.900 --> 00:53:41.460]   anarchist, objectivist,
[00:53:41.460 --> 00:53:44.280]   what other philosophies are there?
[00:53:44.280 --> 00:53:47.160]   Marxist, communist, social democrat,
[00:53:47.160 --> 00:53:50.480]   democratic socialist, socialist.
[00:53:50.480 --> 00:53:52.600]   And if you add that as a category,
[00:53:52.600 --> 00:53:55.880]   all of a sudden it's like, oh boy, you're that guy now.
[00:53:55.880 --> 00:53:56.720]   - Yeah. - And I don't know
[00:53:56.720 --> 00:53:58.080]   if you wanna be that guy. - Well, there's definitely
[00:53:58.080 --> 00:54:01.160]   some really charged ones, like alt-right,
[00:54:01.160 --> 00:54:04.240]   I think is quite a complicated and tough label.
[00:54:04.240 --> 00:54:06.880]   I mean, it's not a completely meaningless label,
[00:54:06.880 --> 00:54:08.600]   but boy, I think you really have to pause
[00:54:08.600 --> 00:54:11.360]   before you actually put that label on someone,
[00:54:11.360 --> 00:54:14.200]   partly because now you're putting them in a group of people,
[00:54:14.200 --> 00:54:18.080]   some of whom you wouldn't wanna be grouped with.
[00:54:18.080 --> 00:54:19.960]   So it's, yeah.
[00:54:19.960 --> 00:54:22.460]   - Let's go into some, you mentioned the hot water
[00:54:22.460 --> 00:54:25.860]   of the pool that we're both tipping a toe in.
[00:54:25.860 --> 00:54:29.320]   Do you think Wikipedia has a left-leaning political bias,
[00:54:29.320 --> 00:54:31.440]   which is something it is sometimes accused of?
[00:54:31.440 --> 00:54:34.220]   - Yeah, so I don't think so, not broadly.
[00:54:34.220 --> 00:54:40.480]   And I think you can always point to specific entries
[00:54:40.480 --> 00:54:42.300]   and talk about specific biases,
[00:54:42.300 --> 00:54:44.840]   but that's part of the process of Wikipedia.
[00:54:44.840 --> 00:54:48.440]   Anyone can come and challenge and to go on about that.
[00:54:48.440 --> 00:54:52.240]   But I see fairly often on Twitter
[00:54:52.240 --> 00:54:57.600]   quite extreme accusations of bias.
[00:54:57.600 --> 00:55:00.840]   And I think, actually, I just, I don't see it.
[00:55:00.840 --> 00:55:01.660]   I don't buy that.
[00:55:01.660 --> 00:55:03.920]   And if you ask people for an example,
[00:55:03.920 --> 00:55:05.220]   they normally struggle,
[00:55:05.220 --> 00:55:09.120]   and depending on who they are and what it's about.
[00:55:10.040 --> 00:55:13.460]   So it's certainly true that some people
[00:55:13.460 --> 00:55:16.020]   who have quite fringe viewpoints,
[00:55:16.020 --> 00:55:22.340]   and who knows, the full rush of history in 500 years,
[00:55:22.340 --> 00:55:24.920]   they might be considered to be path-breaking geniuses,
[00:55:24.920 --> 00:55:28.320]   but at the moment, quite fringe views,
[00:55:28.320 --> 00:55:30.500]   and they're just unhappy that Wikipedia doesn't report
[00:55:30.500 --> 00:55:33.060]   on their fringe views as being mainstream.
[00:55:33.060 --> 00:55:36.040]   And that, by the way, goes across all kinds of fields.
[00:55:36.040 --> 00:55:38.820]   I mean, I was once accosted on the street
[00:55:39.600 --> 00:55:42.800]   outside the TED conference in Vancouver
[00:55:42.800 --> 00:55:46.160]   by a guy who's a homeopath, who was very upset
[00:55:46.160 --> 00:55:48.640]   that Wikipedia's entry on homeopathy
[00:55:48.640 --> 00:55:52.000]   basically says it's pseudoscience.
[00:55:52.000 --> 00:55:53.240]   And he felt that was biased.
[00:55:53.240 --> 00:55:54.920]   And I said, "Well, I can't really help you
[00:55:54.920 --> 00:55:58.560]   "because we cite good quality sources
[00:55:58.560 --> 00:56:00.720]   "to talk about the scientific status,
[00:56:00.720 --> 00:56:02.700]   "and it's not very good."
[00:56:02.700 --> 00:56:04.160]   So it depends.
[00:56:04.160 --> 00:56:06.240]   And I think it's something
[00:56:06.240 --> 00:56:08.920]   that we should always be vigilant about.
[00:56:08.920 --> 00:56:13.040]   But it's, in general, I think we're pretty good.
[00:56:13.040 --> 00:56:14.680]   And I think any time you go
[00:56:14.680 --> 00:56:19.680]   to any serious political controversy,
[00:56:19.680 --> 00:56:22.120]   we should have a pretty balanced perspective
[00:56:22.120 --> 00:56:26.280]   on who's saying what, what the views are, and so forth.
[00:56:26.280 --> 00:56:30.000]   I would actually argue that the areas
[00:56:30.000 --> 00:56:32.880]   where we are more likely to have bias
[00:56:32.880 --> 00:56:34.560]   that persists for a long period of time
[00:56:34.560 --> 00:56:37.280]   are actually fairly obscure things,
[00:56:37.280 --> 00:56:39.600]   or maybe fairly non-political things.
[00:56:39.600 --> 00:56:41.560]   So I always give, it's kind of a humorous example,
[00:56:41.560 --> 00:56:43.580]   but it's meaningful.
[00:56:43.580 --> 00:56:49.220]   If you read our entries about Japanese anime,
[00:56:49.220 --> 00:56:52.720]   they tend to be very, very positive and very favorable
[00:56:52.720 --> 00:56:54.880]   because almost no one knows about Japanese anime
[00:56:54.880 --> 00:56:56.320]   except for fans.
[00:56:56.320 --> 00:56:58.520]   And so the people who come and spend their days
[00:56:58.520 --> 00:57:01.200]   writing Japanese anime articles, they love it.
[00:57:01.200 --> 00:57:03.920]   They kind of have an inherent love for the whole area.
[00:57:03.920 --> 00:57:05.840]   Now, they'll, of course, being human beings,
[00:57:05.840 --> 00:57:07.520]   they'll have their internal debates and disputes
[00:57:07.520 --> 00:57:09.520]   about what's better or not.
[00:57:09.520 --> 00:57:11.600]   But in general, they're quite positive
[00:57:11.600 --> 00:57:13.240]   because nobody actually cares.
[00:57:13.240 --> 00:57:16.280]   On anything that people are quite passionate about,
[00:57:16.280 --> 00:57:20.240]   then hopefully there's quite a lot of interesting stuff.
[00:57:20.240 --> 00:57:22.720]   So I'll give an example, a contemporary example,
[00:57:22.720 --> 00:57:24.520]   where I think we've done a good job
[00:57:24.520 --> 00:57:27.600]   as of my most recent sort of look at it.
[00:57:27.600 --> 00:57:31.960]   And that is the question about the efficacy of masks
[00:57:31.960 --> 00:57:34.040]   during the COVID pandemic.
[00:57:34.040 --> 00:57:38.640]   And that's an area where I would say the public authorities
[00:57:38.640 --> 00:57:41.480]   really kind of jerked us all around a bit.
[00:57:41.480 --> 00:57:42.600]   In the very first days, they said,
[00:57:42.600 --> 00:57:44.940]   "Whatever you do, don't rush on and buy masks."
[00:57:44.940 --> 00:57:50.400]   And their concern was shortages in hospitals.
[00:57:50.400 --> 00:57:52.000]   Okay, fair enough.
[00:57:52.000 --> 00:57:52.840]   Later, it's like,
[00:57:52.840 --> 00:57:55.280]   now everybody's gotta wear a mask everywhere.
[00:57:55.280 --> 00:57:56.640]   It really works really well.
[00:57:56.640 --> 00:58:01.040]   Then now I think it's the evidence is mixed.
[00:58:01.040 --> 00:58:03.120]   Masks seem to help, in my personal view,
[00:58:03.120 --> 00:58:04.320]   masks seem to help.
[00:58:04.320 --> 00:58:06.280]   They're no huge burden.
[00:58:06.280 --> 00:58:08.600]   You might as well wear a mask in any environment
[00:58:08.600 --> 00:58:11.700]   where you're with a giant crowd of people and so forth.
[00:58:11.700 --> 00:58:15.240]   But it's very politicized, that one.
[00:58:15.240 --> 00:58:19.840]   It's very politicized where certainly in the US,
[00:58:19.840 --> 00:58:22.520]   you know, much more so, I mean, I live in the UK,
[00:58:22.520 --> 00:58:25.600]   I live in London, I've never seen kind of on the streets
[00:58:25.600 --> 00:58:29.000]   sort of the kind of thing that there's a lot of reports
[00:58:29.000 --> 00:58:31.600]   of people actively angry because someone else
[00:58:31.600 --> 00:58:35.320]   is wearing a mask, that sort of thing in public.
[00:58:35.320 --> 00:58:38.960]   And so because it became very politicized,
[00:58:38.960 --> 00:58:41.720]   then clearly if Wikipedia,
[00:58:41.720 --> 00:58:43.160]   no, so anyway, if you go to Wikipedia
[00:58:43.160 --> 00:58:44.240]   and you research this topic,
[00:58:44.240 --> 00:58:46.520]   I think you'll find more or less what I've just said.
[00:58:46.520 --> 00:58:49.160]   Actually, after it's all, you know,
[00:58:49.160 --> 00:58:51.920]   to this point in history, it's mixed evidence.
[00:58:51.920 --> 00:58:53.080]   Like masks seem to help,
[00:58:53.080 --> 00:58:54.960]   but maybe not as much as some of the authorities said,
[00:58:54.960 --> 00:58:56.880]   and here we are.
[00:58:56.880 --> 00:58:58.840]   And that's kind of an example where I think,
[00:58:58.840 --> 00:59:00.000]   "Okay, we've done a good job,
[00:59:00.000 --> 00:59:02.720]   "but I suspect there are people on both sides
[00:59:02.720 --> 00:59:04.320]   "of that very emotional debate
[00:59:04.320 --> 00:59:06.240]   "who think this is ridiculous."
[00:59:06.240 --> 00:59:08.520]   Hopefully we've got quality sources,
[00:59:08.520 --> 00:59:10.680]   so then hopefully those people who read this can say,
[00:59:10.680 --> 00:59:13.500]   "Oh, actually, you know, it is complicated."
[00:59:13.500 --> 00:59:16.000]   If you can get to the point of saying,
[00:59:16.000 --> 00:59:19.900]   "Okay, I have my view, but I understand other views,
[00:59:19.900 --> 00:59:22.240]   "and I do think it's a complicated question,"
[00:59:22.240 --> 00:59:24.840]   great, now we're a little bit more mature as a society.
[00:59:24.840 --> 00:59:26.000]   - Well, that one is an interesting one
[00:59:26.000 --> 00:59:29.200]   because I feel like I hope that that article
[00:59:29.200 --> 00:59:31.600]   also contains the meta conversation
[00:59:31.600 --> 00:59:34.360]   about the politicization of that topic.
[00:59:34.360 --> 00:59:35.800]   To me, it's almost more interesting
[00:59:35.800 --> 00:59:37.880]   than whether masks work or not,
[00:59:37.880 --> 00:59:39.000]   at least at this point,
[00:59:39.000 --> 00:59:43.320]   is like why it became, masks became a symbol
[00:59:43.320 --> 00:59:46.000]   of the oppression of a centralized government,
[00:59:46.000 --> 00:59:47.020]   if you wear them.
[00:59:47.020 --> 00:59:52.560]   You're a sheep that follows the mass control,
[00:59:52.560 --> 00:59:56.160]   the mass hysteria of an authoritarian regime,
[00:59:56.160 --> 00:59:57.960]   and if you don't wear a mask,
[00:59:57.960 --> 01:00:01.920]   then you are a science denier, anti-vaxxer,
[01:00:01.920 --> 01:00:05.460]   a alt-right, probably a Nazi.
[01:00:05.460 --> 01:00:11.600]   - Exactly, and that whole politicization of society
[01:00:11.600 --> 01:00:13.720]   is just so damaging,
[01:00:13.720 --> 01:00:18.920]   and I don't know, in the broader world,
[01:00:18.920 --> 01:00:20.080]   how do we start to fix that?
[01:00:20.080 --> 01:00:21.800]   That's a really hard question.
[01:00:21.800 --> 01:00:23.320]   - Well, at every moment,
[01:00:23.320 --> 01:00:26.080]   'cause you mentioned mainstream and fringe,
[01:00:26.080 --> 01:00:27.600]   there seems to be a tension here,
[01:00:27.600 --> 01:00:31.000]   and I wonder what your philosophy is on it
[01:00:31.000 --> 01:00:33.960]   because there's mainstream ideas and there's fringe ideas.
[01:00:33.960 --> 01:00:37.960]   You look at lab leak theory for this virus.
[01:00:37.960 --> 01:00:40.880]   There could be other things we can discuss
[01:00:40.880 --> 01:00:45.120]   where there's a mainstream narrative
[01:00:45.120 --> 01:00:50.120]   where if you just look at the percent of the population
[01:00:50.440 --> 01:00:53.560]   or the population with platforms, what they say,
[01:00:53.560 --> 01:00:58.560]   and then what is a small percentage in opposition to that,
[01:00:58.560 --> 01:01:01.840]   and what is Wikipedia's responsibility
[01:01:01.840 --> 01:01:04.160]   to accurately represent both the mainstream
[01:01:04.160 --> 01:01:05.320]   and the fringe, do you think?
[01:01:05.320 --> 01:01:08.240]   - Well, I mean, I think we have to try to do our best
[01:01:08.240 --> 01:01:12.760]   to recognize both, but also to appropriately contextualize,
[01:01:12.760 --> 01:01:14.720]   and so this can be quite hard,
[01:01:14.720 --> 01:01:16.120]   particularly when emotions are high.
[01:01:16.120 --> 01:01:18.120]   That's just a fact about human beings.
[01:01:19.720 --> 01:01:21.120]   I'll give a simpler example
[01:01:21.120 --> 01:01:23.240]   because there's not a lot of emotion around it.
[01:01:23.240 --> 01:01:27.240]   Our entry on the moon doesn't say,
[01:01:27.240 --> 01:01:30.160]   some say the moon's made of rocks, some say cheese.
[01:01:30.160 --> 01:01:31.000]   Who knows?
[01:01:31.000 --> 01:01:35.280]   That kind of false neutrality is not what we wanna get to.
[01:01:35.280 --> 01:01:37.880]   That doesn't make any sense, but that one's easy.
[01:01:37.880 --> 01:01:38.940]   We all understand.
[01:01:38.940 --> 01:01:41.880]   I think there is a Wikipedia entry
[01:01:41.880 --> 01:01:44.120]   called something like the moon is made of cheese
[01:01:44.120 --> 01:01:47.240]   where it talks about this is a common sort of joke
[01:01:47.240 --> 01:01:49.640]   or thing that children say
[01:01:49.640 --> 01:01:52.400]   or that people tell to children or whatever.
[01:01:52.400 --> 01:01:53.440]   It's just a thing.
[01:01:53.440 --> 01:01:55.600]   Everybody's heard moon's made of cheese,
[01:01:55.600 --> 01:02:01.680]   but nobody thinks, wow, Wikipedia's so one-sided,
[01:02:01.680 --> 01:02:04.040]   it doesn't even acknowledge the cheese theory.
[01:02:04.040 --> 01:02:08.240]   I'd say the same thing about flat Earth, again.
[01:02:08.240 --> 01:02:09.800]   - That's exactly what I'm looking up right now.
[01:02:09.800 --> 01:02:11.200]   - Very little controversy.
[01:02:12.920 --> 01:02:17.200]   We will have an entry about flat Earth theorizing,
[01:02:17.200 --> 01:02:18.400]   flat Earth people.
[01:02:18.400 --> 01:02:22.200]   My personal view is most of the people
[01:02:22.200 --> 01:02:24.560]   who claim to be flat Earthers are just having a laugh,
[01:02:24.560 --> 01:02:26.680]   trolling, and more power to them.
[01:02:26.680 --> 01:02:31.440]   Have some fun, but let's not be ridiculous.
[01:02:31.440 --> 01:02:33.000]   - Of course, for most of human history,
[01:02:33.000 --> 01:02:34.440]   people believe that the Earth is flat,
[01:02:34.440 --> 01:02:36.160]   so the article I'm looking at
[01:02:36.160 --> 01:02:38.160]   is actually kind of focusing on this history.
[01:02:38.160 --> 01:02:40.480]   Flat Earth is an archaic and scientifically disproven
[01:02:40.480 --> 01:02:43.280]   conception of the Earth's shape as a plane or disc.
[01:02:43.280 --> 01:02:47.720]   Many ancient cultures subscribe to flat Earth cosmography
[01:02:47.720 --> 01:02:48.720]   with pretty cool pictures
[01:02:48.720 --> 01:02:50.800]   of what a flat Earth would look like.
[01:02:50.800 --> 01:02:52.120]   With dragon, is that a dragon?
[01:02:52.120 --> 01:02:54.640]   No, angels on the edge.
[01:02:54.640 --> 01:02:56.000]   There's a lot of controversy about that.
[01:02:56.000 --> 01:02:56.840]   What is on the edge?
[01:02:56.840 --> 01:02:57.660]   Is it the wall?
[01:02:57.660 --> 01:02:58.500]   Is it angels?
[01:02:58.500 --> 01:02:59.440]   Is it dragons?
[01:02:59.440 --> 01:03:00.280]   Is there a dome?
[01:03:00.280 --> 01:03:04.800]   - And how can you fly from South Africa to Perth?
[01:03:04.800 --> 01:03:07.280]   Because on a flat Earth view,
[01:03:07.280 --> 01:03:09.480]   that's really too far for any plane to make it.
[01:03:09.480 --> 01:03:11.800]   - But I wanna know-- - It's all spread out.
[01:03:11.800 --> 01:03:14.400]   - What I wanna know is what's on the other side, Jimmy?
[01:03:14.400 --> 01:03:16.200]   What's on the other side?
[01:03:16.200 --> 01:03:18.000]   That's what all of us want to know.
[01:03:18.000 --> 01:03:23.120]   So there's some, I presume there's probably a small section
[01:03:23.120 --> 01:03:25.960]   about the conspiracy theory of flat Earth,
[01:03:25.960 --> 01:03:28.460]   'cause I think there's a sizable percent of the population
[01:03:28.460 --> 01:03:31.960]   who at least will say they believe in a flat Earth.
[01:03:31.960 --> 01:03:36.600]   I think it is a movement that just says
[01:03:36.600 --> 01:03:38.760]   that the mainstream narrative,
[01:03:38.760 --> 01:03:40.680]   to have distrust and skepticism
[01:03:40.680 --> 01:03:41.820]   about the mainstream narrative,
[01:03:41.820 --> 01:03:43.200]   which to a very small degree
[01:03:43.200 --> 01:03:45.040]   is probably a very productive thing to do
[01:03:45.040 --> 01:03:47.180]   as part of the scientific process,
[01:03:47.180 --> 01:03:49.720]   but you can get a little silly and ridiculous with it.
[01:03:49.720 --> 01:03:53.280]   - Yeah, I mean, yeah, it's exactly right.
[01:03:53.280 --> 01:03:58.280]   And so I think I find on many, many cases,
[01:03:58.280 --> 01:04:00.380]   and of course I, like anybody else,
[01:04:00.380 --> 01:04:03.080]   might quibble about this or that in any Wikipedia article,
[01:04:03.080 --> 01:04:05.420]   but in general, I think there is a pretty good
[01:04:05.860 --> 01:04:10.460]   sort of willingness and indeed eagerness to say,
[01:04:10.460 --> 01:04:12.940]   oh, let's fairly represent
[01:04:12.940 --> 01:04:15.820]   all of the meaningfully important sides.
[01:04:15.820 --> 01:04:18.480]   So there's still a lot to unpack in that, right?
[01:04:18.480 --> 01:04:20.620]   So meaningfully important.
[01:04:20.620 --> 01:04:25.620]   So people who are raising questions
[01:04:25.620 --> 01:04:30.540]   about the efficacy of masks,
[01:04:30.540 --> 01:04:32.700]   okay, that's actually a reasonable thing
[01:04:32.700 --> 01:04:33.660]   to have a discussion about,
[01:04:33.660 --> 01:04:34.980]   and hopefully we should treat that
[01:04:34.980 --> 01:04:38.180]   as a fair conversation to have
[01:04:38.180 --> 01:04:40.340]   and actually address which authorities have said what
[01:04:40.340 --> 01:04:41.500]   and so on and so forth.
[01:04:41.500 --> 01:04:44.540]   And then there are other cases
[01:04:44.540 --> 01:04:48.460]   where it's not meaningful opposition.
[01:04:48.460 --> 01:04:49.740]   Like you just wouldn't say,
[01:04:49.740 --> 01:04:54.700]   I mean, I doubt if the main article, "Moon,"
[01:04:54.700 --> 01:04:59.220]   it may mention, geez, probably not even,
[01:04:59.220 --> 01:05:00.860]   because it's not credible
[01:05:00.860 --> 01:05:03.340]   and it's not even meant to be serious by anyone,
[01:05:03.340 --> 01:05:05.940]   or the article on the Earth
[01:05:05.940 --> 01:05:07.960]   certainly won't have a paragraph that says,
[01:05:07.960 --> 01:05:09.780]   well, most scientists think it's round,
[01:05:09.780 --> 01:05:11.900]   but certain people think flat.
[01:05:11.900 --> 01:05:14.120]   Like that's just a silly thing to put in that article.
[01:05:14.120 --> 01:05:16.420]   You would wanna sort of address,
[01:05:16.420 --> 01:05:18.460]   that's an interesting cultural phenomenon.
[01:05:18.460 --> 01:05:19.820]   You wanna put it somewhere.
[01:05:19.820 --> 01:05:26.100]   So this goes into all kinds of things about politics.
[01:05:26.100 --> 01:05:29.380]   You wanna be really careful, really thoughtful
[01:05:29.380 --> 01:05:34.380]   about not getting caught up in the anger of our times
[01:05:34.380 --> 01:05:38.020]   and really recognize.
[01:05:38.020 --> 01:05:39.760]   You know, I always thought,
[01:05:39.760 --> 01:05:44.340]   I remember being really kind of proud of the US
[01:05:44.340 --> 01:05:48.220]   at the time when McCain was running against Obama,
[01:05:48.220 --> 01:05:49.940]   because I thought, oh, I've got plenty of disagreements
[01:05:49.940 --> 01:05:51.100]   with both of them,
[01:05:51.100 --> 01:05:53.340]   but they both seem like thoughtful and interesting people
[01:05:53.340 --> 01:05:54.820]   who I would have different disagreements with,
[01:05:54.820 --> 01:05:57.540]   but I always felt like, yeah, that's good.
[01:05:57.540 --> 01:05:58.580]   Now we can have a debate.
[01:05:58.580 --> 01:05:59.900]   Now we can have an interesting debate,
[01:05:59.900 --> 01:06:02.540]   and it isn't just sort of people slamming each other,
[01:06:02.540 --> 01:06:04.040]   personal attacks and so forth.
[01:06:04.040 --> 01:06:09.180]   - And you're saying Wikipedia also represented that?
[01:06:09.180 --> 01:06:12.140]   - I hope so, yeah, and I think so, in the main.
[01:06:12.140 --> 01:06:14.820]   Obviously, you can always find a debate
[01:06:14.820 --> 01:06:18.020]   that went horribly wrong, 'cause there's humans involved.
[01:06:18.020 --> 01:06:19.780]   - But speaking of those humans,
[01:06:19.780 --> 01:06:24.500]   I would venture to guess, I don't know the data,
[01:06:24.500 --> 01:06:27.780]   maybe you can let me know,
[01:06:27.780 --> 01:06:30.540]   but the personal political leaning
[01:06:30.540 --> 01:06:34.340]   of the group of people who edit Wikipedia
[01:06:34.340 --> 01:06:37.820]   probably leans left, I would guess.
[01:06:37.820 --> 01:06:39.340]   So to me, the question there is,
[01:06:39.340 --> 01:06:41.340]   I mean, the same is true for Silicon Valley.
[01:06:41.340 --> 01:06:44.180]   The task for Silicon Valley is to create platforms
[01:06:44.180 --> 01:06:45.860]   that are not politically biased,
[01:06:45.860 --> 01:06:50.460]   even though there is a bias for the engineers who create it.
[01:06:50.460 --> 01:06:53.860]   And I think, I believe it's possible to do that.
[01:06:53.860 --> 01:06:55.540]   You know, there's kind of conspiracy theories
[01:06:55.540 --> 01:06:57.140]   that it somehow is impossible,
[01:06:57.140 --> 01:06:58.500]   and there's this whole conspiracy
[01:06:58.500 --> 01:07:00.420]   where the left is controlling it and so on.
[01:07:00.420 --> 01:07:03.140]   I think engineers, for the most part,
[01:07:03.140 --> 01:07:05.940]   want to create platforms that are open and unbiased,
[01:07:05.940 --> 01:07:08.460]   that create all kinds of perspective,
[01:07:08.460 --> 01:07:10.220]   'cause that's super exciting
[01:07:10.220 --> 01:07:12.220]   to have all kinds of perspectives battle it out.
[01:07:12.220 --> 01:07:16.860]   But still, is there a degree
[01:07:16.860 --> 01:07:19.100]   to which the personal political bias
[01:07:19.100 --> 01:07:20.980]   of the editors might seep in,
[01:07:20.980 --> 01:07:22.660]   in silly ways and in big ways?
[01:07:22.660 --> 01:07:25.660]   Silly ways could be, I think,
[01:07:25.660 --> 01:07:27.580]   hopefully I'm correct in saying this,
[01:07:27.580 --> 01:07:32.540]   but the right will call it the Democrat Party,
[01:07:32.540 --> 01:07:34.940]   and the left will call it the Democratic Party.
[01:07:34.940 --> 01:07:39.380]   Like subtle, it always hits my ear weird.
[01:07:39.380 --> 01:07:41.620]   Like, are we children here?
[01:07:41.620 --> 01:07:43.820]   That we're literally taking words
[01:07:43.820 --> 01:07:45.660]   and just jabbing at each other.
[01:07:45.660 --> 01:07:49.260]   Like, I could capitalize a thing in a certain way,
[01:07:49.260 --> 01:07:52.820]   or I can just take a word and mess with them.
[01:07:52.820 --> 01:07:54.980]   That's a small way of how you use words.
[01:07:54.980 --> 01:07:58.060]   But you can also have a bigger way
[01:07:58.060 --> 01:08:02.220]   about beliefs, about various perspectives
[01:08:02.220 --> 01:08:05.260]   on political events, on Hunter Biden's laptop,
[01:08:05.260 --> 01:08:07.100]   on how big of a story that is or not,
[01:08:07.100 --> 01:08:09.940]   how big the censorship of that story is or not.
[01:08:09.940 --> 01:08:12.700]   And then there's these camps that take very strong points,
[01:08:12.700 --> 01:08:15.820]   and they construct big narratives around that.
[01:08:15.820 --> 01:08:18.060]   I mean, it's very sizable percent of the population
[01:08:18.060 --> 01:08:21.140]   believes the two narratives that compete with each other.
[01:08:21.140 --> 01:08:24.540]   - Yeah, I mean, it's really interesting.
[01:08:25.100 --> 01:08:29.180]   And it feels, it's hard to judge, you know,
[01:08:29.180 --> 01:08:32.700]   the sweep of history within your own lifetime.
[01:08:32.700 --> 01:08:35.540]   But it feels like it's gotten much worse,
[01:08:35.540 --> 01:08:38.700]   that this idea of two parallel universes
[01:08:38.700 --> 01:08:41.820]   where people can agree on certain basic facts
[01:08:41.820 --> 01:08:45.260]   feels worse than it used to be.
[01:08:45.260 --> 01:08:47.660]   And I'm not sure if that's true,
[01:08:47.660 --> 01:08:48.700]   or if it just feels that way,
[01:08:48.700 --> 01:08:51.700]   but I also, I'm not sure what the causes are.
[01:08:51.700 --> 01:08:56.700]   I think I would lay a lot of the blame in recent years
[01:08:56.700 --> 01:09:01.420]   on social media algorithms,
[01:09:01.420 --> 01:09:05.140]   which reward clickbait headlines,
[01:09:05.140 --> 01:09:09.220]   which reward tweets that go viral,
[01:09:09.220 --> 01:09:12.060]   and they go viral because they're cute and clever.
[01:09:12.060 --> 01:09:16.220]   I mean, my most successful tweet ever,
[01:09:16.220 --> 01:09:18.300]   by a fairly wide margin,
[01:09:19.260 --> 01:09:22.460]   some reporter tweeted at Elon Musk,
[01:09:22.460 --> 01:09:26.060]   'cause he was complaining about Wikipedia or something,
[01:09:26.060 --> 01:09:27.340]   "You should buy Wikipedia."
[01:09:27.340 --> 01:09:29.940]   And I just wrote, "Not for sale."
[01:09:29.940 --> 01:09:33.700]   And, you know, 90 zillion retweets,
[01:09:33.700 --> 01:09:36.740]   and people liked it, and it was all very good.
[01:09:36.740 --> 01:09:38.420]   But I'm like, you know what?
[01:09:38.420 --> 01:09:39.980]   It's a cute line, right?
[01:09:39.980 --> 01:09:41.940]   And it's a good mic drop and all that.
[01:09:41.940 --> 01:09:43.660]   And I was pleased with myself.
[01:09:43.660 --> 01:09:45.420]   Like, it's not really discourse, right?
[01:09:45.420 --> 01:09:49.620]   It's not really sort of what I like to do,
[01:09:49.620 --> 01:09:51.420]   but it's what social media really rewards,
[01:09:51.420 --> 01:09:55.300]   which is kind of lets you and him have a fight, right?
[01:09:55.300 --> 01:09:56.140]   And that's more interesting.
[01:09:56.140 --> 01:09:57.140]   I mean, it's funny, because at the time,
[01:09:57.140 --> 01:10:00.220]   I was texting with Elon, who was very pleasant to me,
[01:10:00.220 --> 01:10:01.940]   and all of that.
[01:10:01.940 --> 01:10:03.420]   - He might've been a little bit shitty.
[01:10:03.420 --> 01:10:05.020]   The reporter might've been a little bit shitty,
[01:10:05.020 --> 01:10:06.220]   but you fed into the shitty
[01:10:06.220 --> 01:10:09.340]   with a snarky, funny response, "Not for sale."
[01:10:09.340 --> 01:10:12.220]   And like, where do you, like, what?
[01:10:12.220 --> 01:10:13.700]   So that's a funny little exchange,
[01:10:13.700 --> 01:10:15.500]   and you could probably, after that, laugh it off,
[01:10:15.500 --> 01:10:16.780]   and it's fun.
[01:10:16.780 --> 01:10:20.500]   But like, that kind of mechanism that rewards the snark
[01:10:20.500 --> 01:10:22.780]   can go into viciousness.
[01:10:22.780 --> 01:10:23.620]   - Yeah, yeah.
[01:10:23.620 --> 01:10:26.380]   Well, and we certainly see it online.
[01:10:26.380 --> 01:10:30.940]   You know, like, a series of tweets, you know,
[01:10:30.940 --> 01:10:35.060]   sort of a tweet thread of 15 tweets
[01:10:35.060 --> 01:10:38.460]   that assesses the quality of the evidence for masks,
[01:10:38.460 --> 01:10:40.740]   pros and cons, and sort of where this,
[01:10:40.740 --> 01:10:43.260]   that's not gonna go viral, you know?
[01:10:43.260 --> 01:10:48.100]   But, you know, a smackdown for a famous politician
[01:10:48.100 --> 01:10:49.740]   who was famously in favor of masks,
[01:10:49.740 --> 01:10:52.380]   who also went to a dinner and didn't wear a mask,
[01:10:52.380 --> 01:10:53.540]   that's gonna go viral.
[01:10:53.540 --> 01:10:57.740]   And, you know, that's partly human nature.
[01:10:57.740 --> 01:11:00.300]   You know, people love to call out hypocrisy and all of that,
[01:11:00.300 --> 01:11:04.420]   but it's partly what these systems elevate automatically.
[01:11:04.420 --> 01:11:08.460]   I talk about this with respect to Facebook, for example.
[01:11:08.460 --> 01:11:10.760]   So I think Facebook has done a pretty good job,
[01:11:11.680 --> 01:11:13.800]   although it's taken longer than it should in some cases.
[01:11:13.800 --> 01:11:17.960]   But, you know, if you have a very large following
[01:11:17.960 --> 01:11:19.760]   and you're really spouting hatred
[01:11:19.760 --> 01:11:23.040]   or misinformation, disinformation,
[01:11:23.040 --> 01:11:24.400]   they've kicked people off.
[01:11:24.400 --> 01:11:27.420]   They've done, you know, some reasonable things there.
[01:11:27.420 --> 01:11:31.560]   But actually the deeper issue is of this,
[01:11:31.560 --> 01:11:33.840]   the anger we're talking about,
[01:11:33.840 --> 01:11:36.960]   of the contentiousness of everything.
[01:11:36.960 --> 01:11:41.960]   I make a family example with two great stereotypes.
[01:11:41.960 --> 01:11:46.840]   So one, the crackpot racist uncle,
[01:11:46.840 --> 01:11:49.080]   and one, the sweet grandma.
[01:11:49.080 --> 01:11:50.920]   And I always wanna point out,
[01:11:50.920 --> 01:11:53.380]   all of my uncles in my family were wonderful people,
[01:11:53.380 --> 01:11:55.200]   so I didn't have a crackpot racist uncle,
[01:11:55.200 --> 01:11:57.140]   but everybody knows the stereotype.
[01:11:57.140 --> 01:12:00.520]   Well, so grandma, she just posts like sweet comments
[01:12:00.520 --> 01:12:02.760]   on the kids' pictures and congratulates people
[01:12:02.760 --> 01:12:04.680]   on their wedding anniversary.
[01:12:04.680 --> 01:12:07.680]   And crackpot uncle's posting his nonsense.
[01:12:07.680 --> 01:12:10.360]   And normally, it's sort of at Christmas dinner,
[01:12:10.360 --> 01:12:11.440]   everybody rolls their eyes,
[01:12:11.440 --> 01:12:12.520]   oh yeah, Uncle Frank's here,
[01:12:12.520 --> 01:12:14.400]   he's probably gonna say some racist comment
[01:12:14.400 --> 01:12:16.000]   and we're gonna tell him to shut up
[01:12:16.000 --> 01:12:17.960]   or, you know, maybe let's not invite him this year,
[01:12:17.960 --> 01:12:20.880]   you know, normal human drama.
[01:12:20.880 --> 01:12:22.600]   He's got his three mates down at the pub
[01:12:22.600 --> 01:12:25.060]   who listen to him and all of that.
[01:12:25.060 --> 01:12:29.440]   But now, grandma's got, you know, 54 followers on Facebook,
[01:12:29.440 --> 01:12:30.740]   which is the intimate family,
[01:12:30.740 --> 01:12:33.360]   and racist uncle has 714.
[01:12:33.360 --> 01:12:35.360]   He's not a massive influence or whatever,
[01:12:35.360 --> 01:12:36.180]   but how did that happen?
[01:12:36.180 --> 01:12:38.560]   It's because the algorithm notices,
[01:12:38.560 --> 01:12:41.240]   oh, when she posts, nothing happens,
[01:12:41.240 --> 01:12:43.640]   he posts and then everybody jumps in to go,
[01:12:43.640 --> 01:12:45.240]   gosh, shut up, Uncle Frank, you know,
[01:12:45.240 --> 01:12:47.000]   like that's outrageous.
[01:12:47.000 --> 01:12:48.420]   And it's like, oh, there's engagement,
[01:12:48.420 --> 01:12:50.120]   there's page views, there's ads, right?
[01:12:50.120 --> 01:12:52.320]   And those algorithms,
[01:12:52.320 --> 01:12:54.100]   I think they're working to improve that,
[01:12:54.100 --> 01:12:56.000]   but it's really hard for them.
[01:12:56.000 --> 01:12:59.200]   It's hard to improve that if that actually is working.
[01:12:59.200 --> 01:13:02.540]   If the people who are saying things that get engagement,
[01:13:03.500 --> 01:13:06.020]   if it's not too awful, but it's just, you know,
[01:13:06.020 --> 01:13:07.900]   like maybe it's not a racist uncle,
[01:13:07.900 --> 01:13:09.340]   but maybe it's an uncle who posts a lot
[01:13:09.340 --> 01:13:12.120]   about what an idiot Biden is, right?
[01:13:12.120 --> 01:13:13.860]   Which isn't necessarily an offensive
[01:13:13.860 --> 01:13:16.740]   or blockable or banable thing, and it shouldn't be.
[01:13:16.740 --> 01:13:19.100]   But if that's the discourse that gets elevated
[01:13:19.100 --> 01:13:21.080]   because it gets a rise out of people,
[01:13:21.080 --> 01:13:23.100]   then suddenly in a society, it's like,
[01:13:23.100 --> 01:13:25.540]   oh, this is, we get more of what we reward.
[01:13:25.540 --> 01:13:28.700]   So I think that's a piece of what's gone on.
[01:13:28.700 --> 01:13:32.500]   - Well, if we could just take that tangent,
[01:13:32.500 --> 01:13:36.900]   I'm having a conversation with Mark Zuckerberg a second time.
[01:13:36.900 --> 01:13:38.340]   Is there something you can comment on
[01:13:38.340 --> 01:13:39.660]   how to decrease toxicity
[01:13:39.660 --> 01:13:41.700]   on that particular platform, Facebook?
[01:13:41.700 --> 01:13:44.720]   You also have worked on creating a social network
[01:13:44.720 --> 01:13:46.580]   that is less toxic yourself.
[01:13:46.580 --> 01:13:49.760]   So can we just talk about the different ideas
[01:13:49.760 --> 01:13:52.860]   that these already big social networks can do
[01:13:52.860 --> 01:13:54.660]   and what you have been trying to do?
[01:13:54.660 --> 01:13:58.500]   - So a piece of it is, it's hard.
[01:13:58.500 --> 01:14:01.900]   So I don't, the problem with making a recommendation
[01:14:01.900 --> 01:14:04.500]   to Facebook is that I actually believe
[01:14:04.500 --> 01:14:07.460]   their business model makes it really hard for them.
[01:14:07.460 --> 01:14:11.300]   And I'm not anti-capitalism, I'm not, you know, great.
[01:14:11.300 --> 01:14:13.080]   Somebody's got business, they're making money.
[01:14:13.080 --> 01:14:15.800]   That's not where I come from.
[01:14:15.800 --> 01:14:19.620]   But certain business models mean you are gonna prioritize
[01:14:19.620 --> 01:14:22.660]   things that maybe aren't that long-term healthful.
[01:14:22.660 --> 01:14:24.620]   And so that's a big piece of it.
[01:14:24.620 --> 01:14:26.920]   So certainly for Facebook, you could say,
[01:14:27.280 --> 01:14:31.440]   you know, with vast resources,
[01:14:31.440 --> 01:14:35.040]   start to prioritize content that's higher quality,
[01:14:35.040 --> 01:14:36.880]   that's healing, that's kind.
[01:14:36.880 --> 01:14:39.680]   Try not to prioritize content
[01:14:39.680 --> 01:14:42.160]   that seems to be just getting a rise out of people.
[01:14:42.160 --> 01:14:45.040]   Now those are vague human descriptions, right?
[01:14:45.040 --> 01:14:47.720]   But I do believe good machine learning algorithms,
[01:14:47.720 --> 01:14:50.120]   you can optimize in slightly different ways.
[01:14:50.120 --> 01:14:52.760]   But to do that, you may have to say,
[01:14:52.760 --> 01:14:57.240]   actually, we're not necessarily gonna increase page views
[01:14:57.240 --> 01:14:59.160]   to the maximum extent right now.
[01:14:59.160 --> 01:15:01.320]   And I've said this to people at Facebook.
[01:15:01.320 --> 01:15:06.320]   It's like, you know, if your actions are, you know,
[01:15:06.320 --> 01:15:10.840]   convincing people that you're breaking Western civilization,
[01:15:10.840 --> 01:15:13.200]   that's really bad for business in the long run.
[01:15:13.200 --> 01:15:17.420]   Certainly these days, I'll say,
[01:15:17.420 --> 01:15:20.280]   Twitter is the thing that's on people's minds
[01:15:20.280 --> 01:15:22.720]   as being more upsetting at the moment.
[01:15:22.720 --> 01:15:23.960]   But I think it's true.
[01:15:23.960 --> 01:15:28.720]   And so one of the things that's really interesting
[01:15:28.720 --> 01:15:32.440]   about Facebook compared to a lot of companies
[01:15:32.440 --> 01:15:36.160]   is that Mark has a pretty unprecedented amount of power.
[01:15:36.160 --> 01:15:38.440]   His ability to name members of the board,
[01:15:38.440 --> 01:15:42.700]   his control of the company is pretty hard to break.
[01:15:42.700 --> 01:15:46.220]   Even if financial results aren't as good as they could be,
[01:15:46.220 --> 01:15:49.060]   because he's taken a step back from
[01:15:49.060 --> 01:15:50.960]   the perfect optimization to say,
[01:15:50.960 --> 01:15:54.240]   actually, for the long-term health in the next 50 years
[01:15:54.240 --> 01:15:57.520]   of this organization, we need to rein in some of the things
[01:15:57.520 --> 01:15:59.080]   that are working for us and making money
[01:15:59.080 --> 01:16:02.380]   because they're actually giving us a bad reputation.
[01:16:02.380 --> 01:16:05.000]   So one of the recommendations I would say is,
[01:16:05.000 --> 01:16:07.040]   and this is not to do with the algorithms and all that,
[01:16:07.040 --> 01:16:09.120]   but you know, how about just a moratorium
[01:16:09.120 --> 01:16:11.320]   on all political advertising?
[01:16:11.320 --> 01:16:13.880]   I don't think it's their most profitable segment,
[01:16:13.880 --> 01:16:16.620]   but it's given rise to a lot of deep, hard questions
[01:16:16.620 --> 01:16:21.620]   about dark money, about ads that are run
[01:16:21.620 --> 01:16:25.500]   by questionable people that push false narratives,
[01:16:25.500 --> 01:16:28.700]   or the classic kind of thing is you run,
[01:16:28.700 --> 01:16:33.820]   I saw a study about Brexit in the UK
[01:16:33.820 --> 01:16:36.420]   where people were talking about there were ads run
[01:16:36.420 --> 01:16:43.300]   to animal rights activists saying,
[01:16:43.300 --> 01:16:45.100]   finally, when we're out from under Europe,
[01:16:45.100 --> 01:16:48.260]   the UK can pass proper animal rights legislation.
[01:16:48.260 --> 01:16:51.580]   We're not constrained by the European process.
[01:16:51.580 --> 01:16:55.540]   Similarly, for people who are advocates of fox hunting
[01:16:55.540 --> 01:16:57.160]   to say, finally, when we're out of Europe,
[01:16:57.160 --> 01:16:58.900]   we can re-implement.
[01:16:58.900 --> 01:17:01.100]   So you're telling people what they wanna hear.
[01:17:01.100 --> 01:17:04.280]   And in some cases, it's really hard
[01:17:04.280 --> 01:17:06.780]   for journalists to see that.
[01:17:06.780 --> 01:17:09.880]   So it used to be that for political advertising,
[01:17:09.880 --> 01:17:12.620]   you really needed to find some kind of mainstream narrative,
[01:17:12.620 --> 01:17:14.480]   and this is still true to an extent.
[01:17:14.480 --> 01:17:18.460]   Mainstream narrative that 60% of people can say,
[01:17:18.460 --> 01:17:19.440]   oh, I can buy into that,
[01:17:19.440 --> 01:17:20.900]   which meant it pushed you to the center.
[01:17:20.900 --> 01:17:24.480]   It pushed you to sort of try and find some nuanced balance.
[01:17:24.480 --> 01:17:26.820]   But if your main method of recruiting people
[01:17:26.820 --> 01:17:31.060]   is a tiny little one-on-one conversation with them
[01:17:31.060 --> 01:17:34.900]   because you're able to target using targeted advertising,
[01:17:34.900 --> 01:17:37.100]   suddenly you don't need consistent.
[01:17:37.100 --> 01:17:42.100]   You just need a really good targeting operation,
[01:17:42.100 --> 01:17:44.680]   really good Cambridge analytic style
[01:17:44.680 --> 01:17:47.480]   machine learning algorithm data to convince people.
[01:17:47.480 --> 01:17:49.000]   And that just feels really problematic.
[01:17:49.000 --> 01:17:50.680]   So I mean, until they can think about
[01:17:50.680 --> 01:17:52.000]   how to solve that problem, I would just say,
[01:17:52.000 --> 01:17:54.520]   you know what, it's gonna cost us X amount,
[01:17:54.520 --> 01:17:57.880]   but it's gonna be worth it to kind of say,
[01:17:57.880 --> 01:17:58.960]   you know what, we actually think
[01:17:58.960 --> 01:18:02.920]   our political advertising policy hasn't really helped
[01:18:02.920 --> 01:18:05.800]   contribute to discourse and dialogue
[01:18:05.800 --> 01:18:10.000]   and finding reasoned middle ground and compromise solutions.
[01:18:10.000 --> 01:18:11.960]   So let's just not do that for a while
[01:18:11.960 --> 01:18:12.880]   until we figure that out.
[01:18:12.880 --> 01:18:14.660]   So that's maybe a piece of advice.
[01:18:14.660 --> 01:18:17.500]   - And coupled with, as you were saying,
[01:18:17.500 --> 01:18:21.800]   recommender systems for the newsfeed and other contexts
[01:18:21.800 --> 01:18:24.060]   that don't always optimize engagement,
[01:18:24.060 --> 01:18:28.300]   but optimize the long-term mental wellbeing
[01:18:28.300 --> 01:18:30.500]   and balance and growth of a human being.
[01:18:30.500 --> 01:18:31.340]   - Yeah.
[01:18:31.340 --> 01:18:32.580]   - That's a very difficult problem.
[01:18:32.580 --> 01:18:34.780]   - It's a difficult problem, yeah.
[01:18:34.780 --> 01:18:39.780]   And you know, so with WT Social, WikiTree and Social,
[01:18:40.440 --> 01:18:43.460]   we're launching in a few months time,
[01:18:43.460 --> 01:18:46.340]   a completely new system, new domain name,
[01:18:46.340 --> 01:18:47.780]   new lots of things.
[01:18:47.780 --> 01:18:51.280]   But the idea is to say, let's focus on trust.
[01:18:51.280 --> 01:18:54.900]   People can rate each other as trustworthy,
[01:18:54.900 --> 01:18:56.100]   rate content as trustworthy.
[01:18:56.100 --> 01:18:57.200]   You have to start from somewhere.
[01:18:57.200 --> 01:19:00.180]   So we'll start with a core base of our tiny community
[01:19:00.180 --> 01:19:03.180]   who I think are sensible, thoughtful people.
[01:19:03.180 --> 01:19:05.020]   We wanna recruit more, but to say, you know what,
[01:19:05.020 --> 01:19:07.780]   actually let's have that as a pretty strong element
[01:19:07.780 --> 01:19:10.920]   to say, let's not optimize based on
[01:19:10.920 --> 01:19:13.520]   what gets the most page views in this session.
[01:19:13.520 --> 01:19:18.520]   Let's optimize on what sort of the feedback from people is.
[01:19:18.520 --> 01:19:21.760]   This is meaningfully enhancing my life.
[01:19:21.760 --> 01:19:22.960]   And so part of that is,
[01:19:22.960 --> 01:19:25.280]   and it's probably not a good business model,
[01:19:25.280 --> 01:19:26.360]   but part of that is say, okay,
[01:19:26.360 --> 01:19:28.480]   we're not gonna pursue an advertising business model,
[01:19:28.480 --> 01:19:33.280]   but a membership model where you can,
[01:19:33.280 --> 01:19:34.280]   you don't have to be a member,
[01:19:34.280 --> 01:19:36.120]   but you can pay to be a member.
[01:19:36.120 --> 01:19:37.600]   You maybe get some benefit from that,
[01:19:37.600 --> 01:19:41.860]   but in general to say, actually the problem with,
[01:19:41.860 --> 01:19:44.620]   and actually the division I would say is,
[01:19:44.620 --> 01:19:46.480]   and the analogy I would give is,
[01:19:46.480 --> 01:19:52.360]   broadcast television funded by advertising
[01:19:52.360 --> 01:19:57.360]   gives you a different result than paying for HBO,
[01:19:57.360 --> 01:20:01.380]   paying for Netflix, paying for whatever.
[01:20:01.380 --> 01:20:04.580]   And the reason is, you know, if you think about it,
[01:20:04.580 --> 01:20:08.980]   what is your incentive as a TV producer,
[01:20:08.980 --> 01:20:13.980]   you're gonna make a comedy for ABC network in the US,
[01:20:13.980 --> 01:20:16.020]   you basically say, I want something that
[01:20:16.020 --> 01:20:18.020]   almost everybody will like and listen to.
[01:20:18.020 --> 01:20:20.140]   So it tends to be a little blander,
[01:20:20.140 --> 01:20:22.780]   you know, family friendly, whatever.
[01:20:22.780 --> 01:20:25.060]   Whereas if you say, oh, actually,
[01:20:25.060 --> 01:20:28.260]   I'm gonna use the HBO example and an old example.
[01:20:28.260 --> 01:20:29.260]   You say, you know what?
[01:20:29.260 --> 01:20:31.260]   Sopranos isn't for everybody.
[01:20:31.260 --> 01:20:32.940]   Sex and the City isn't for everybody.
[01:20:32.940 --> 01:20:34.900]   But between the two shows,
[01:20:34.900 --> 01:20:36.100]   we've got something for everybody
[01:20:36.100 --> 01:20:37.700]   that they're willing to pay for.
[01:20:37.700 --> 01:20:40.740]   So you can get edgier, higher quality in my view,
[01:20:40.740 --> 01:20:42.260]   content rather than saying,
[01:20:42.260 --> 01:20:44.500]   it's gotta not offend anybody in the world.
[01:20:44.500 --> 01:20:46.860]   It's gotta be for everybody, which is really hard.
[01:20:46.860 --> 01:20:49.860]   So same thing, you know, here in a social network,
[01:20:49.860 --> 01:20:51.560]   if your business model is advertising,
[01:20:51.560 --> 01:20:53.580]   it's gonna drive you in one direction.
[01:20:53.580 --> 01:20:55.220]   If your business model is membership,
[01:20:55.220 --> 01:20:56.780]   I think it drives you in a different direction.
[01:20:56.780 --> 01:21:00.860]   I actually, and I've said this to Elon about Twitter Blue,
[01:21:00.860 --> 01:21:02.840]   which I think wasn't rolled out well.
[01:21:02.840 --> 01:21:04.660]   And so forth, but it's like, hmm,
[01:21:04.660 --> 01:21:06.980]   the piece of that that I like is to say,
[01:21:06.980 --> 01:21:10.020]   look, actually, if there's a model
[01:21:10.020 --> 01:21:12.260]   where your revenue is coming from people
[01:21:12.260 --> 01:21:14.420]   who are willing to pay for the service,
[01:21:14.420 --> 01:21:16.140]   even if it's only part of your revenue,
[01:21:16.140 --> 01:21:17.860]   if it's a substantial part,
[01:21:17.860 --> 01:21:21.100]   that does change your broader incentives to say,
[01:21:21.100 --> 01:21:23.600]   actually, are people gonna be willing to pay for something
[01:21:23.600 --> 01:21:25.900]   that's actually just toxicity in their lives?
[01:21:25.900 --> 01:21:28.860]   Now, I'm not sure it's been rolled out well.
[01:21:28.860 --> 01:21:31.160]   I'm not sure how it's going.
[01:21:31.160 --> 01:21:35.340]   And maybe I'm wrong about that as a plausible business model.
[01:21:35.340 --> 01:21:38.100]   But I do think it's interesting to think about
[01:21:38.100 --> 01:21:43.100]   just in broad terms, business model drives outcomes
[01:21:43.100 --> 01:21:44.740]   in sometimes surprising ways,
[01:21:44.740 --> 01:21:46.620]   unless you really pause to think about it.
[01:21:46.620 --> 01:21:49.780]   - So if we can just link on Twitter and Elon,
[01:21:49.780 --> 01:21:52.700]   before I would love to talk to you
[01:21:52.700 --> 01:21:54.940]   about the underlying business model, Wikipedia,
[01:21:54.940 --> 01:21:57.820]   which is this brilliant, bold move at the very beginning.
[01:21:57.820 --> 01:22:00.740]   But since you mentioned Twitter, what do you think works?
[01:22:00.740 --> 01:22:03.060]   What do you think is broken about Twitter?
[01:22:03.060 --> 01:22:05.380]   - Oof, I mean, it's a long conversation,
[01:22:05.380 --> 01:22:09.660]   but to start with, one of the things that I always say is,
[01:22:09.660 --> 01:22:11.080]   it's a really hard problem.
[01:22:11.080 --> 01:22:12.780]   So I can see that right up front.
[01:22:12.780 --> 01:22:16.220]   I said this about the old ownership of Twitter
[01:22:16.220 --> 01:22:18.620]   and the new ownership of Twitter,
[01:22:18.620 --> 01:22:20.820]   because unlike Wikipedia,
[01:22:20.820 --> 01:22:23.620]   and this is true actually for all social media,
[01:22:23.620 --> 01:22:26.180]   there's a box and the box basically says,
[01:22:26.180 --> 01:22:27.000]   what do you think?
[01:22:27.000 --> 01:22:27.980]   What's on your mind?
[01:22:27.980 --> 01:22:30.180]   You can write whatever the hell you want, right?
[01:22:30.180 --> 01:22:32.620]   This is true, by the way, even for YouTube.
[01:22:32.620 --> 01:22:33.980]   I mean, the box is to upload a video,
[01:22:33.980 --> 01:22:36.420]   but again, it's just like an open-ended invitation
[01:22:36.420 --> 01:22:38.020]   to express yourself.
[01:22:38.020 --> 01:22:40.700]   And what makes that hard is some people have really toxic,
[01:22:40.700 --> 01:22:43.140]   really bad, you know, some people are very aggressive.
[01:22:43.140 --> 01:22:45.620]   They're actually stalking, they're actually, you know,
[01:22:45.620 --> 01:22:49.860]   abusive, and suddenly you deal with a lot of problems.
[01:22:49.860 --> 01:22:52.220]   Whereas at Wikipedia, there is no box that says,
[01:22:52.220 --> 01:22:53.740]   what's on your mind?
[01:22:53.740 --> 01:22:55.700]   There's a box that says,
[01:22:55.700 --> 01:22:59.340]   this is an entry about the moon.
[01:22:59.340 --> 01:23:00.980]   Please be neutral, please cite your facts.
[01:23:00.980 --> 01:23:02.060]   Then there's a talk page,
[01:23:02.060 --> 01:23:06.260]   which is not coming rant about Donald Trump.
[01:23:06.260 --> 01:23:08.220]   If you go on the talk page of the Donald Trump entry
[01:23:08.220 --> 01:23:10.260]   and you just start ranting about Donald Trump,
[01:23:10.260 --> 01:23:11.260]   people would say, what are you doing?
[01:23:11.260 --> 01:23:12.140]   Like, stop doing that.
[01:23:12.140 --> 01:23:13.860]   Like, we're not here to discuss,
[01:23:13.860 --> 01:23:15.740]   like there's a whole world of the internet out there
[01:23:15.740 --> 01:23:17.220]   for you to go and rant about Donald Trump.
[01:23:17.220 --> 01:23:18.940]   - It's just not fun to do on Wikipedia.
[01:23:18.940 --> 01:23:20.660]   Somehow it's fun on Twitter.
[01:23:20.660 --> 01:23:23.940]   - Well, also on Wikipedia, people are gonna say, stop.
[01:23:23.940 --> 01:23:26.660]   And actually, are you here to tell us,
[01:23:26.660 --> 01:23:28.140]   like, how can we improve the article?
[01:23:28.140 --> 01:23:29.460]   Or are you just here to rant about Trump?
[01:23:29.460 --> 01:23:31.140]   'Cause that's not actually interesting.
[01:23:31.140 --> 01:23:33.340]   So because the goal is different.
[01:23:33.340 --> 01:23:36.020]   So that's just admitting and saying upfront,
[01:23:36.020 --> 01:23:37.740]   this is a hard problem.
[01:23:37.740 --> 01:23:42.260]   Certainly, I'm writing a book on trust.
[01:23:42.260 --> 01:23:46.100]   So the idea is, in the last 20 years,
[01:23:46.100 --> 01:23:51.100]   we've lost trust in all kinds of institutions and politics.
[01:23:51.100 --> 01:23:54.380]   The Edelman Trust Barometer Survey
[01:23:54.380 --> 01:23:56.500]   has been done for a long time.
[01:23:56.500 --> 01:23:58.700]   And trust in politicians, trust in journalism,
[01:23:58.700 --> 01:24:00.660]   it's declined substantially.
[01:24:00.660 --> 01:24:03.140]   And I think in many cases, deservedly.
[01:24:03.140 --> 01:24:05.500]   So how do we restore trust?
[01:24:05.500 --> 01:24:07.460]   And how do we think about that?
[01:24:07.460 --> 01:24:12.460]   - And does that also include trust in the idea of truth?
[01:24:12.460 --> 01:24:14.980]   - Trust in the idea of truth.
[01:24:14.980 --> 01:24:17.300]   Even the concept of facts and truth
[01:24:17.300 --> 01:24:18.540]   is really, really important.
[01:24:18.540 --> 01:24:23.060]   And the idea of uncomfortable truths is really important.
[01:24:23.060 --> 01:24:28.060]   Now, so when we look at Twitter, right?
[01:24:28.060 --> 01:24:30.580]   And we can see, okay, this is really hard.
[01:24:30.580 --> 01:24:35.780]   So here's my story about Twitter.
[01:24:35.780 --> 01:24:37.240]   It's a two-part story.
[01:24:37.240 --> 01:24:41.700]   And it's all pre-Elon Musk ownership.
[01:24:41.700 --> 01:24:43.260]   So many years back,
[01:24:43.260 --> 01:24:47.700]   somebody accused me of horrible crimes on Twitter.
[01:24:47.700 --> 01:24:51.580]   And I, like anybody would, I was like,
[01:24:51.580 --> 01:24:52.420]   I'm in the public eye.
[01:24:52.420 --> 01:24:53.700]   People say bad things.
[01:24:53.700 --> 01:24:55.700]   I don't really, you know, I brush it off, whatever.
[01:24:55.700 --> 01:24:57.260]   But I'm like, this is actually really bad.
[01:24:57.260 --> 01:25:01.860]   Like, accusing me of pedophilia, like, that's just not okay.
[01:25:01.860 --> 01:25:03.300]   So I thought, I'm gonna report this.
[01:25:03.300 --> 01:25:05.780]   So I click report, and I report the tweet.
[01:25:05.780 --> 01:25:08.740]   And there's five others, and I go through the process.
[01:25:08.740 --> 01:25:11.500]   And then I get an email that says, you know, whatever,
[01:25:11.500 --> 01:25:13.740]   a couple hours later, saying, thank you for your report.
[01:25:13.740 --> 01:25:14.700]   We're looking into this.
[01:25:14.700 --> 01:25:16.100]   Great, okay, good.
[01:25:16.100 --> 01:25:18.980]   Then several hours further, I get an email back saying,
[01:25:18.980 --> 01:25:20.020]   sorry, we don't see anything here
[01:25:20.020 --> 01:25:22.140]   to violate our terms of use.
[01:25:22.140 --> 01:25:23.020]   And I'm like, okay.
[01:25:23.020 --> 01:25:25.700]   So I email Jack, and I say, Jack, come on,
[01:25:25.700 --> 01:25:26.780]   like, this is ridiculous.
[01:25:26.780 --> 01:25:31.100]   And he emails back roughly saying, yeah, sorry, Jimmy.
[01:25:31.100 --> 01:25:33.260]   Don't worry, we'll sort this out.
[01:25:33.260 --> 01:25:36.100]   And I just thought to myself, you know what?
[01:25:36.100 --> 01:25:37.380]   That's not the point, right?
[01:25:37.380 --> 01:25:38.700]   I'm Jimmy Wales.
[01:25:38.700 --> 01:25:39.660]   I know Jack Dorsey.
[01:25:39.660 --> 01:25:40.900]   I can email Jack Dorsey.
[01:25:40.900 --> 01:25:43.460]   He'll listen to me 'cause he's got an email from me,
[01:25:43.460 --> 01:25:44.460]   and sorts it out for me.
[01:25:44.460 --> 01:25:48.140]   What about the teenager who's being bullied
[01:25:48.140 --> 01:25:50.660]   and is getting abuse, right,
[01:25:50.660 --> 01:25:52.660]   and getting accusations that aren't true?
[01:25:52.660 --> 01:25:55.500]   Are they getting the same kind of like really poor result
[01:25:55.500 --> 01:25:56.820]   in that case?
[01:25:56.820 --> 01:26:00.980]   So fast forward a few years, same thing happens.
[01:26:00.980 --> 01:26:05.620]   The exact quote I'll use, please help me.
[01:26:05.620 --> 01:26:08.740]   I'm only 10 years old, and Jimmy Wales raped me last week.
[01:26:08.740 --> 01:26:09.780]   So I come on, fuck off.
[01:26:09.780 --> 01:26:10.660]   Like, that's ridiculous.
[01:26:10.660 --> 01:26:12.660]   So I report, I'm like, this time I'm reporting,
[01:26:12.660 --> 01:26:15.020]   but I'm thinking, well, we'll see what happens.
[01:26:15.020 --> 01:26:19.860]   This one gets even worse because then I get a same result,
[01:26:19.860 --> 01:26:21.980]   email back saying, sorry, we don't see any problems.
[01:26:21.980 --> 01:26:24.220]   So I raise it with other members of the board who I know,
[01:26:24.220 --> 01:26:26.900]   and Jack, and like, this is really ridiculous.
[01:26:26.900 --> 01:26:29.020]   Like, this is outrageous.
[01:26:29.020 --> 01:26:32.100]   And some of the board members, friends of mine,
[01:26:32.100 --> 01:26:34.660]   sympathetic, and so good for them,
[01:26:34.660 --> 01:26:36.940]   but I actually got an email back then
[01:26:36.940 --> 01:26:41.380]   from the general counsel, head of trust and safety saying,
[01:26:41.380 --> 01:26:42.700]   actually, there's nothing in this tweet
[01:26:42.700 --> 01:26:44.100]   that violates our terms of service.
[01:26:44.100 --> 01:26:48.100]   We don't regard, and gave reference to the Me Too movement.
[01:26:48.100 --> 01:26:50.060]   If we didn't allow accusations,
[01:26:50.060 --> 01:26:52.340]   the Me Too movement, it's an important thing.
[01:26:52.340 --> 01:26:53.660]   And I was like, you know what?
[01:26:53.660 --> 01:26:56.620]   Actually, if someone says I'm 10 years old
[01:26:56.620 --> 01:26:57.940]   and someone raped me last week,
[01:26:57.940 --> 01:26:59.340]   I think the advice should be,
[01:26:59.340 --> 01:27:01.180]   here's the phone number of the police.
[01:27:01.180 --> 01:27:02.660]   Like, you need to get the police involved.
[01:27:02.660 --> 01:27:05.140]   Twitter's not the place for that accusation.
[01:27:05.140 --> 01:27:08.500]   So even back then, by the way, they did delete those tweets,
[01:27:08.500 --> 01:27:11.860]   but I mean, the rationale they gave is spammy behavior.
[01:27:11.860 --> 01:27:13.780]   So completely separate from abusing me,
[01:27:13.780 --> 01:27:16.220]   it was just like, oh, well, they were retweeting too often.
[01:27:16.220 --> 01:27:17.660]   Okay, whatever.
[01:27:17.660 --> 01:27:19.500]   So like, that's just broken.
[01:27:19.500 --> 01:27:21.940]   Like, that's a system that it's not working
[01:27:21.940 --> 01:27:23.500]   for people in the public eye.
[01:27:23.500 --> 01:27:26.820]   I'm sure it's not working for private people who get abuse.
[01:27:26.820 --> 01:27:28.620]   Really horrible abuse can happen.
[01:27:28.620 --> 01:27:31.380]   So how is that today?
[01:27:31.380 --> 01:27:34.380]   Well, it hasn't happened to me since Elon took over,
[01:27:34.380 --> 01:27:35.820]   but I don't see why it couldn't.
[01:27:35.820 --> 01:27:38.620]   And I suspect now if I send a report and email someone,
[01:27:38.620 --> 01:27:40.820]   there's no one there to email me back
[01:27:40.820 --> 01:27:43.980]   'cause he's gotten rid of a lot of the trust and safety staff.
[01:27:43.980 --> 01:27:46.820]   So I suspect that problem is still really hard.
[01:27:46.820 --> 01:27:49.940]   - Just content moderation at huge scales.
[01:27:49.940 --> 01:27:52.540]   - At huge scales is really something.
[01:27:52.540 --> 01:27:54.380]   And I don't know the full answer to this.
[01:27:54.380 --> 01:27:56.860]   I mean, a piece of it could be,
[01:27:56.860 --> 01:28:02.940]   to say actually making specific allegations of crimes,
[01:28:02.940 --> 01:28:06.180]   this isn't the place to do that.
[01:28:06.180 --> 01:28:07.540]   We've got a huge database.
[01:28:07.540 --> 01:28:09.620]   If you've got an accusation of crime,
[01:28:09.620 --> 01:28:12.460]   here's who you should call, the police, the FBI,
[01:28:12.460 --> 01:28:13.580]   whatever it is.
[01:28:13.580 --> 01:28:15.340]   It's not to be done in public.
[01:28:15.340 --> 01:28:17.660]   And then you do face really complicated questions
[01:28:17.660 --> 01:28:20.740]   about Me Too movement and people coming forward in public
[01:28:20.740 --> 01:28:21.580]   and all of that.
[01:28:21.580 --> 01:28:23.060]   But again, it's like,
[01:28:23.060 --> 01:28:24.980]   probably you should talk to a journalist, right?
[01:28:24.980 --> 01:28:27.940]   Probably there are better avenues than just tweeting
[01:28:27.940 --> 01:28:31.020]   from an account that was created 10 days ago,
[01:28:31.020 --> 01:28:33.180]   obviously set up to abuse someone.
[01:28:33.180 --> 01:28:36.260]   So I think they could do a lot better,
[01:28:36.260 --> 01:28:38.140]   but I also admit it's a hard problem.
[01:28:38.140 --> 01:28:41.100]   - And there's also ways to indirectly or more humorously
[01:28:41.100 --> 01:28:44.380]   or a more mocking way to make the same kinds of accusations.
[01:28:44.380 --> 01:28:46.300]   In fact, the accusations you mentioned,
[01:28:46.300 --> 01:28:48.300]   if I were to guess, don't go that viral
[01:28:48.300 --> 01:28:50.780]   'cause they're not funny enough or cutting enough.
[01:28:50.780 --> 01:28:55.540]   But if you make it witty and cutting and meme it somehow,
[01:28:55.540 --> 01:28:58.820]   sometimes actually indirectly making the accusation
[01:28:58.820 --> 01:29:00.420]   versus directly making the accusation,
[01:29:00.420 --> 01:29:03.300]   that can go viral and that can destroy reputations.
[01:29:03.300 --> 01:29:05.500]   And you get to watch yourself,
[01:29:05.500 --> 01:29:09.740]   just all kinds of narratives take hold.
[01:29:09.740 --> 01:29:13.140]   - No, I mean, I remember another case that didn't bother me
[01:29:13.140 --> 01:29:16.260]   'cause it wasn't of that nature,
[01:29:16.260 --> 01:29:17.860]   but somebody was saying,
[01:29:17.860 --> 01:29:22.820]   "I'm sure you're making millions off of Wikipedia."
[01:29:22.820 --> 01:29:25.340]   I'm like, "No, actually, I don't even work there.
[01:29:25.340 --> 01:29:26.800]   "I have no salary."
[01:29:26.800 --> 01:29:29.340]   And they're like, "You're lying.
[01:29:29.340 --> 01:29:31.140]   "I'm gonna check your 990 form,"
[01:29:31.140 --> 01:29:35.220]   which is the US form for tax reporting for charities.
[01:29:35.220 --> 01:29:37.700]   I was like, "Yeah, I'm not gonna, here's the link.
[01:29:37.700 --> 01:29:40.400]   "Go read it and you'll see I'm listed as a board member
[01:29:40.400 --> 01:29:41.900]   "and my salary is listed as zero."
[01:29:41.900 --> 01:29:45.820]   So, you know, things like that, it's like,
[01:29:45.820 --> 01:29:49.180]   "Okay, that one, that feels like you're wrong,
[01:29:49.180 --> 01:29:50.320]   "but I can take that
[01:29:50.320 --> 01:29:52.580]   "and we can have that debate quite quickly."
[01:29:52.580 --> 01:29:54.840]   And again, it didn't go viral because it was kind of silly.
[01:29:54.840 --> 01:29:58.260]   And if anything would have gone viral, it was me responding.
[01:29:58.260 --> 01:29:59.140]   But that's one where it's like,
[01:29:59.140 --> 01:30:00.340]   "Actually, I'm happy to respond
[01:30:00.340 --> 01:30:03.400]   "because a lot of people don't know that I don't work there
[01:30:03.400 --> 01:30:06.300]   "and that I don't make millions and I'm not a billionaire."
[01:30:06.300 --> 01:30:07.340]   Well, they must know that
[01:30:07.340 --> 01:30:10.420]   'cause it's in most news media about me.
[01:30:10.420 --> 01:30:13.700]   But the other one I didn't respond to publicly
[01:30:13.700 --> 01:30:16.420]   because it's like Barbra Streisand effect.
[01:30:16.420 --> 01:30:18.780]   You know, it's like sometimes calling attention
[01:30:18.780 --> 01:30:19.700]   to someone who's abusing you
[01:30:19.700 --> 01:30:23.940]   who basically has no followers and so on is just a waste.
[01:30:23.940 --> 01:30:25.100]   - And everything you're describing now
[01:30:25.100 --> 01:30:28.060]   is just something that all of us have to kind of learn
[01:30:28.060 --> 01:30:29.460]   'cause everybody's in the public eye.
[01:30:29.460 --> 01:30:32.060]   I think when you have just two followers
[01:30:32.060 --> 01:30:33.620]   and you get bullied by one of the followers,
[01:30:33.620 --> 01:30:35.860]   it hurts just as much as when you have a large number.
[01:30:35.860 --> 01:30:38.420]   So it's not, your situation, I think,
[01:30:38.420 --> 01:30:41.060]   is echoed in the situations of millions of other,
[01:30:41.060 --> 01:30:43.000]   especially teenagers and kids and so on.
[01:30:43.000 --> 01:30:46.000]   - Yeah, I mean, it's actually an example.
[01:30:46.000 --> 01:30:52.740]   So we don't generally use my picture
[01:30:52.740 --> 01:30:56.140]   in the banners anymore on Wikipedia, but we did.
[01:30:56.140 --> 01:30:58.000]   And then we did an experiment one year
[01:30:58.000 --> 01:30:59.980]   where we tried other people's pictures,
[01:30:59.980 --> 01:31:01.860]   so one of our developers.
[01:31:01.860 --> 01:31:05.820]   And, you know, one guy, lovely, very sweet guy,
[01:31:05.820 --> 01:31:09.460]   and he doesn't look like your immediate thought
[01:31:09.460 --> 01:31:12.060]   of a nerdy Silicon Valley developer.
[01:31:12.060 --> 01:31:15.060]   He looks like a heavy metal dude 'cause he's cool.
[01:31:15.060 --> 01:31:18.500]   And so suddenly here he is with long hair and tattoos
[01:31:18.500 --> 01:31:20.660]   and there's his sort of say,
[01:31:20.660 --> 01:31:22.140]   here's what your money goes for,
[01:31:22.140 --> 01:31:24.700]   here's my letter asking for support.
[01:31:24.700 --> 01:31:27.100]   And he got massive abuse from Wikipedia,
[01:31:27.100 --> 01:31:30.420]   like calling him creepy and, you know, like really massive.
[01:31:30.420 --> 01:31:33.300]   And this was being shown to 80 million people a day.
[01:31:33.300 --> 01:31:35.140]   His picture, not the abuse, right?
[01:31:35.140 --> 01:31:37.420]   The abuse was elsewhere on the internet.
[01:31:37.420 --> 01:31:39.140]   And he was bothered by it.
[01:31:39.140 --> 01:31:40.940]   And I thought, you know what, there is a difference.
[01:31:40.940 --> 01:31:43.340]   I actually am in the public eye.
[01:31:43.340 --> 01:31:45.740]   I get huge benefits from being in the public eye.
[01:31:45.740 --> 01:31:47.220]   I go around and make public speeches.
[01:31:47.220 --> 01:31:48.820]   If any random thing I think of,
[01:31:48.820 --> 01:31:51.460]   I can write and get it published in the New York Times
[01:31:51.460 --> 01:31:53.540]   and, you know, have this interesting life.
[01:31:53.540 --> 01:31:54.860]   He's not a public figure.
[01:31:54.860 --> 01:31:58.460]   And so actually he wasn't mad at us.
[01:31:58.460 --> 01:32:00.180]   He wasn't mad, you know, it was just like,
[01:32:00.180 --> 01:32:03.100]   yeah, actually suddenly being thrust in the public eye
[01:32:03.100 --> 01:32:05.340]   and you get suddenly lots of abuse,
[01:32:05.340 --> 01:32:07.980]   which normally, you know, if you're a teenager
[01:32:07.980 --> 01:32:09.620]   and somebody in your class is abusing you,
[01:32:09.620 --> 01:32:11.200]   it's not gonna go viral.
[01:32:11.200 --> 01:32:12.780]   So you're only gonna, it's gonna be hurtful
[01:32:12.780 --> 01:32:16.320]   because it's local and it's your classmates or whatever.
[01:32:16.320 --> 01:32:19.700]   But when sort of ordinary people go viral
[01:32:19.700 --> 01:32:24.020]   in some abusive way, it's really, really quite tragic.
[01:32:24.020 --> 01:32:27.220]   - I don't know, even at a small scale, it feels viral.
[01:32:27.220 --> 01:32:28.900]   When five people-- - I suppose you're right, yeah.
[01:32:28.900 --> 01:32:31.180]   - Five people at your school and there's a rumor
[01:32:31.180 --> 01:32:33.620]   and there's this feeling like you're surrounded
[01:32:33.620 --> 01:32:36.640]   and nobody, and the feeling of loneliness, I think,
[01:32:36.640 --> 01:32:39.860]   which you're speaking to when you don't have a plat,
[01:32:39.860 --> 01:32:42.500]   when you at least feel like you don't have a platform
[01:32:42.500 --> 01:32:43.900]   to defend yourself.
[01:32:43.900 --> 01:32:46.140]   And then this powerlessness that I think
[01:32:46.140 --> 01:32:49.340]   a lot of teenagers definitely feel and a lot of people.
[01:32:49.340 --> 01:32:50.260]   - I think you're right, yeah.
[01:32:50.260 --> 01:32:53.740]   - And that, I think even when just like two people
[01:32:53.740 --> 01:32:55.980]   make up stuff about you or lie about you
[01:32:55.980 --> 01:32:58.340]   or say mean things about you or bully you,
[01:32:58.340 --> 01:33:00.420]   that can feel like a crowd.
[01:33:00.420 --> 01:33:03.180]   - Yeah, yeah, no, it's true.
[01:33:03.180 --> 01:33:05.020]   - I mean, whatever that is in our genetics,
[01:33:05.020 --> 01:33:07.700]   in our biology, in the way our brain works,
[01:33:07.700 --> 01:33:09.620]   it just can be a terrifying experience.
[01:33:09.620 --> 01:33:14.580]   And somehow to correct that, I mean, I think,
[01:33:14.580 --> 01:33:16.180]   because everybody feels the pain of that,
[01:33:16.180 --> 01:33:17.380]   everybody suffers the pain of that,
[01:33:17.380 --> 01:33:20.500]   I think we'll be forced to fix that as a society
[01:33:20.500 --> 01:33:21.900]   to figure out a way around that.
[01:33:21.900 --> 01:33:23.260]   - I think it's really hard to fix
[01:33:23.260 --> 01:33:27.460]   because I don't think that problem isn't necessarily new.
[01:33:28.780 --> 01:33:33.780]   Someone in high school who writes graffiti
[01:33:33.780 --> 01:33:39.180]   that says Becky is a slut and spreads a rumor
[01:33:39.180 --> 01:33:41.420]   about what Becky did last weekend,
[01:33:41.420 --> 01:33:43.500]   that's always been damaging, it's always been hurtful.
[01:33:43.500 --> 01:33:45.020]   And that's really hard.
[01:33:45.020 --> 01:33:47.660]   - Those kinds of attacks are as old as time itself.
[01:33:47.660 --> 01:33:49.060]   They precede the internet.
[01:33:49.060 --> 01:33:50.540]   Now, what do you think about this technology
[01:33:50.540 --> 01:33:53.460]   that feels Wikipedia-like,
[01:33:53.460 --> 01:33:55.740]   which is community notes on Twitter?
[01:33:55.740 --> 01:33:57.500]   Do you like it?
[01:33:57.500 --> 01:33:58.940]   - Yeah. - Pros and cons?
[01:33:58.940 --> 01:34:00.060]   Do you think it's scalable?
[01:34:00.060 --> 01:34:00.900]   - I do like it.
[01:34:00.900 --> 01:34:03.900]   I don't know enough about specifically how it's implemented
[01:34:03.900 --> 01:34:06.820]   to really have a very deep view,
[01:34:06.820 --> 01:34:08.260]   but I do think it's quite,
[01:34:08.260 --> 01:34:11.700]   the uses I've seen of it I've found quite good.
[01:34:11.700 --> 01:34:16.060]   And in some cases, changed my mind.
[01:34:16.060 --> 01:34:18.780]   It's like I see something,
[01:34:18.780 --> 01:34:21.140]   and of course, the sort of human tendency
[01:34:21.140 --> 01:34:26.140]   is to retweet something that you,
[01:34:26.540 --> 01:34:29.980]   hope is true or that you are afraid is true.
[01:34:29.980 --> 01:34:34.740]   Or it's like that kind of quick mental action.
[01:34:34.740 --> 01:34:37.420]   And then I saw something that I liked and agreed with,
[01:34:37.420 --> 01:34:39.780]   and then a community note under it that made me think,
[01:34:39.780 --> 01:34:42.740]   oh, actually, this is a more nuanced issue.
[01:34:42.740 --> 01:34:44.180]   So I like that.
[01:34:44.180 --> 01:34:46.060]   I think that's really important.
[01:34:46.060 --> 01:34:47.580]   Now, how is it specifically implemented?
[01:34:47.580 --> 01:34:48.420]   Is it scalable?
[01:34:48.420 --> 01:34:49.740]   I don't really know how they've done it,
[01:34:49.740 --> 01:34:51.740]   so I can't really comment on that.
[01:34:51.740 --> 01:34:54.340]   But in general, I do think it's,
[01:34:55.740 --> 01:34:59.780]   when your only mechanisms on Twitter,
[01:34:59.780 --> 01:35:01.100]   and you're a big Twitter user,
[01:35:01.100 --> 01:35:02.180]   we know the platform,
[01:35:02.180 --> 01:35:05.180]   and you've got plenty of followers and all of that.
[01:35:05.180 --> 01:35:08.260]   The only mechanisms are retweeting,
[01:35:08.260 --> 01:35:11.940]   replying, blocking.
[01:35:11.940 --> 01:35:15.460]   It's a pretty limited scope,
[01:35:15.460 --> 01:35:17.180]   and it's kind of good if there's a way
[01:35:17.180 --> 01:35:21.300]   to elevate a specific thoughtful response.
[01:35:21.300 --> 01:35:23.100]   And it kind of goes to, again,
[01:35:23.100 --> 01:35:27.060]   does the algorithm just pick the retweet or the,
[01:35:27.060 --> 01:35:28.060]   I mean, retweeting,
[01:35:28.060 --> 01:35:30.660]   it's not even the algorithm that makes it viral.
[01:35:30.660 --> 01:35:35.620]   Like, if Paolo Coelho, very famous author,
[01:35:35.620 --> 01:35:36.980]   I think he's got like, I don't know,
[01:35:36.980 --> 01:35:37.820]   I haven't looked lately.
[01:35:37.820 --> 01:35:39.100]   He used to have 8 million Twitter followers.
[01:35:39.100 --> 01:35:41.540]   I think I looked, he's got 16 million now or whatever.
[01:35:41.540 --> 01:35:42.540]   Well, if he retweets something,
[01:35:42.540 --> 01:35:44.460]   it's gonna get seen a lot.
[01:35:44.460 --> 01:35:46.140]   Or Elon Musk, if he retweets something,
[01:35:46.140 --> 01:35:47.220]   it's gonna get seen a lot.
[01:35:47.220 --> 01:35:48.220]   That's not an algorithm,
[01:35:48.220 --> 01:35:50.460]   that's just the way the platform works.
[01:35:50.460 --> 01:35:53.900]   So it is kind of nice if you have something else,
[01:35:53.900 --> 01:35:55.300]   and how that something else is designed,
[01:35:55.300 --> 01:35:57.980]   that's obviously a complicated question.
[01:35:57.980 --> 01:35:59.460]   - Well, there's an interesting thing
[01:35:59.460 --> 01:36:01.220]   that I think Twitter is doing,
[01:36:01.220 --> 01:36:03.300]   but I know Facebook is doing for sure,
[01:36:03.300 --> 01:36:06.460]   which is really interesting.
[01:36:06.460 --> 01:36:08.540]   So you have, what are the signals
[01:36:08.540 --> 01:36:10.540]   that a human can provide at scale?
[01:36:10.540 --> 01:36:13.460]   Like, in Twitter, it's retweet.
[01:36:13.460 --> 01:36:14.300]   - Yep.
[01:36:14.300 --> 01:36:16.420]   - In Facebook, I think you can share.
[01:36:16.420 --> 01:36:18.020]   I forget, but there's basic interactions.
[01:36:18.020 --> 01:36:19.500]   You can have comment and so on.
[01:36:19.500 --> 01:36:20.340]   - Yeah.
[01:36:20.340 --> 01:36:23.060]   - But also in Facebook, and YouTube has this too,
[01:36:23.060 --> 01:36:26.380]   is would you like to see more of this,
[01:36:26.380 --> 01:36:28.740]   or would you like to see less of this?
[01:36:28.740 --> 01:36:30.180]   They post that sometimes.
[01:36:30.180 --> 01:36:33.260]   And the thing that the neural net
[01:36:33.260 --> 01:36:35.100]   that's learning from that has to figure out
[01:36:35.100 --> 01:36:37.700]   is the intent behind you saying,
[01:36:37.700 --> 01:36:39.180]   I wanna see less of this.
[01:36:39.180 --> 01:36:42.100]   Did you see too much of this content already?
[01:36:42.100 --> 01:36:45.380]   You like it, but you don't wanna see so much of it.
[01:36:45.380 --> 01:36:47.620]   You already figured it out, great.
[01:36:47.620 --> 01:36:50.540]   Or does this content not make you feel good?
[01:36:50.540 --> 01:36:51.940]   There's so many interpretations
[01:36:51.940 --> 01:36:53.380]   to how I'd like to see less of this.
[01:36:53.380 --> 01:36:55.940]   But if you get that kind of signal,
[01:36:55.940 --> 01:37:00.940]   this actually can create a really powerfully curated
[01:37:00.940 --> 01:37:05.420]   list of content that is fed to you every day.
[01:37:05.420 --> 01:37:08.620]   That doesn't create an echo chamber or a silo.
[01:37:08.620 --> 01:37:12.860]   It actually just makes you feel good in the good way,
[01:37:12.860 --> 01:37:14.520]   which is like it challenges you,
[01:37:14.520 --> 01:37:16.300]   but it doesn't exhaust you
[01:37:16.300 --> 01:37:20.180]   and make you kind of this weird animal.
[01:37:20.180 --> 01:37:21.660]   - I've been saying for a long time,
[01:37:21.660 --> 01:37:23.340]   if I went on Facebook one morning
[01:37:23.340 --> 01:37:26.680]   and they said, oh, we're testing a new option,
[01:37:26.680 --> 01:37:29.180]   rather than showing you things
[01:37:29.180 --> 01:37:31.020]   we think you're going to like,
[01:37:31.020 --> 01:37:32.080]   we wanna show you some things
[01:37:32.080 --> 01:37:34.500]   that we think you will disagree with,
[01:37:34.500 --> 01:37:38.780]   but which we have some signals that suggest it's of quality.
[01:37:38.780 --> 01:37:40.140]   Like, now that sounds interesting.
[01:37:40.140 --> 01:37:41.260]   - Yeah, that sounds really interesting.
[01:37:41.260 --> 01:37:43.340]   - I wanna see something where,
[01:37:43.340 --> 01:37:45.300]   you know, like, oh, I don't agree with.
[01:37:45.300 --> 01:37:48.940]   So Larry Lessig is a good friend of mine,
[01:37:48.940 --> 01:37:50.060]   founder of Creative Commons,
[01:37:50.060 --> 01:37:51.260]   and he's moved on to doing stuff
[01:37:51.260 --> 01:37:53.340]   about corruption in politics and so on.
[01:37:53.340 --> 01:37:55.380]   And I don't always agree with Larry,
[01:37:55.380 --> 01:37:57.700]   but I always grapple with Larry
[01:37:57.700 --> 01:37:59.900]   because he's so interesting and he's so thoughtful
[01:37:59.900 --> 01:38:01.920]   that even when we don't agree,
[01:38:01.920 --> 01:38:04.780]   I'm like, actually, I wanna hear him out, right?
[01:38:04.780 --> 01:38:06.660]   Because I'm gonna learn from it.
[01:38:06.660 --> 01:38:08.140]   And that doesn't mean I always come around
[01:38:08.140 --> 01:38:08.960]   to agreeing with him,
[01:38:08.960 --> 01:38:10.340]   but I'm gonna understand a perspective of it.
[01:38:10.340 --> 01:38:12.620]   And that's really a great feeling.
[01:38:12.620 --> 01:38:14.420]   - Yeah, there's this interesting thing on social media
[01:38:14.420 --> 01:38:17.780]   where people kind of accuse others of saying,
[01:38:17.780 --> 01:38:19.180]   well, you don't wanna hear opinions
[01:38:19.180 --> 01:38:21.500]   that you disagree with or ideas you disagree with.
[01:38:21.500 --> 01:38:24.940]   I think this is something that's thrown at me all the time.
[01:38:24.940 --> 01:38:27.020]   The reality is there's literally
[01:38:27.020 --> 01:38:29.700]   almost nothing I enjoy more.
[01:38:29.700 --> 01:38:31.140]   - It's a odd thing to accuse you of
[01:38:31.140 --> 01:38:34.180]   'cause you have quite a wide range of long conversations
[01:38:34.180 --> 01:38:35.820]   with a very diverse bunch of people.
[01:38:35.820 --> 01:38:37.480]   - But there is a very,
[01:38:37.480 --> 01:38:41.780]   there is like a very harsh drop-off
[01:38:41.780 --> 01:38:44.460]   because what I like is high quality disagreement
[01:38:44.460 --> 01:38:45.900]   that really makes me think.
[01:38:45.900 --> 01:38:47.780]   And at a certain point, there's a threshold,
[01:38:47.780 --> 01:38:49.020]   it's a kind of a gray area
[01:38:49.020 --> 01:38:50.900]   when the quality of the disagreement,
[01:38:50.900 --> 01:38:52.340]   it just sounds like mocking
[01:38:52.340 --> 01:38:54.480]   and you're not really interested
[01:38:54.480 --> 01:38:56.460]   in a deep understanding of the topic
[01:38:56.460 --> 01:38:57.940]   or you yourself don't seem to carry
[01:38:57.940 --> 01:38:59.300]   deep understanding of the topic.
[01:38:59.300 --> 01:39:03.660]   Like there's something called Intelligence Squared Debates.
[01:39:03.660 --> 01:39:05.860]   The main one is the British version.
[01:39:05.860 --> 01:39:08.400]   With the British accent, everything always sounds better.
[01:39:08.400 --> 01:39:11.800]   And the Brits seem to argue more intensely,
[01:39:11.800 --> 01:39:15.400]   like they're invigorated, they're energized by the debate.
[01:39:15.400 --> 01:39:17.880]   Those people, I often disagree
[01:39:17.880 --> 01:39:20.820]   with basically everybody involved and it's so fun.
[01:39:20.820 --> 01:39:23.280]   I learned something, that's high quality.
[01:39:23.280 --> 01:39:24.440]   If we could do that,
[01:39:24.440 --> 01:39:27.580]   if there's some way for me to click a button that says,
[01:39:27.580 --> 01:39:31.960]   filter out lower quality just today.
[01:39:31.960 --> 01:39:34.720]   Sometimes show it to me 'cause I wanna be able to,
[01:39:34.720 --> 01:39:38.240]   but today, I'm just not in the mood for the mockery.
[01:39:38.240 --> 01:39:42.000]   Just high quality stuff, even flatter.
[01:39:42.000 --> 01:39:45.280]   I wanna get high quality arguments for the flat Earth.
[01:39:45.280 --> 01:39:46.800]   It would make me feel good
[01:39:46.800 --> 01:39:49.520]   because I would see, oh, that's really interesting.
[01:39:49.520 --> 01:39:52.720]   Like I never really thought in my mind
[01:39:52.720 --> 01:39:55.800]   to challenge the mainstream narrative
[01:39:55.800 --> 01:40:00.000]   of general relativity, right?
[01:40:00.000 --> 01:40:01.920]   Of a perception of physics.
[01:40:01.920 --> 01:40:03.120]   Maybe all of reality,
[01:40:03.120 --> 01:40:06.380]   maybe all of space time is an illusion.
[01:40:06.380 --> 01:40:07.480]   That's really interesting.
[01:40:07.480 --> 01:40:10.120]   I never really thought about, let me consider that fully.
[01:40:10.120 --> 01:40:11.040]   Okay, what's the evidence?
[01:40:11.040 --> 01:40:12.280]   How do you test that?
[01:40:12.280 --> 01:40:14.160]   What are the alternatives?
[01:40:14.160 --> 01:40:18.200]   How would you be able to have such consistent perception
[01:40:18.200 --> 01:40:21.440]   of a physical reality if all of it is an illusion?
[01:40:21.440 --> 01:40:23.080]   All of us seem to share the same kind
[01:40:23.080 --> 01:40:24.520]   of perception of reality.
[01:40:24.520 --> 01:40:27.060]   That's the kind of stuff I love,
[01:40:27.060 --> 01:40:28.760]   but not like the mockery of it.
[01:40:28.760 --> 01:40:34.760]   It seems that social media can kind of inspire.
[01:40:34.760 --> 01:40:38.680]   - Yeah, I talk sometimes about how people assume
[01:40:38.680 --> 01:40:41.560]   that like the big debates in Wikipedia
[01:40:41.560 --> 01:40:46.560]   or the sort of arguments are between the party of the left
[01:40:46.560 --> 01:40:47.880]   and the party of the right.
[01:40:47.880 --> 01:40:49.760]   And I always say, no, it's actually the party
[01:40:49.760 --> 01:40:52.600]   of the kind and thoughtful and the party of the jerks
[01:40:52.600 --> 01:40:53.960]   is really it.
[01:40:53.960 --> 01:40:56.120]   I mean, left and right, like, yeah,
[01:40:56.120 --> 01:40:57.840]   bring me somebody I disagree with politically
[01:40:57.840 --> 01:40:59.840]   as long as they're thoughtful, kind,
[01:40:59.840 --> 01:41:01.840]   we're gonna have a real discussion.
[01:41:01.840 --> 01:41:06.840]   I give an example of our article on abortion.
[01:41:06.840 --> 01:41:12.120]   So, if you can bring together a kind
[01:41:12.120 --> 01:41:13.640]   and thoughtful Catholic priest
[01:41:13.640 --> 01:41:16.680]   and a kind and thoughtful planned paranoid activist,
[01:41:16.680 --> 01:41:19.840]   and they're gonna work together on the article on abortion,
[01:41:19.840 --> 01:41:22.840]   that can be a really great thing.
[01:41:22.840 --> 01:41:24.040]   If they're both kind and thoughtful,
[01:41:24.040 --> 01:41:25.560]   like that's the important part.
[01:41:25.560 --> 01:41:27.420]   They're never gonna agree on the topic,
[01:41:27.420 --> 01:41:28.760]   but they will understand, okay,
[01:41:28.760 --> 01:41:31.400]   like Wikipedia is not gonna take a side,
[01:41:31.400 --> 01:41:33.680]   but Wikipedia is gonna explain what the debate is about.
[01:41:33.680 --> 01:41:36.800]   And we're gonna try to characterize it fairly.
[01:41:36.800 --> 01:41:39.520]   And it turns out like you're kind and thoughtful people,
[01:41:39.520 --> 01:41:41.280]   even if they're quite ideological,
[01:41:41.280 --> 01:41:42.920]   like a Catholic priest is generally gonna be
[01:41:42.920 --> 01:41:45.720]   quite ideological on the subject of abortion,
[01:41:45.720 --> 01:41:49.640]   but they can grapple with ideas and they can discuss,
[01:41:49.640 --> 01:41:51.520]   and they may feel very proud of the entry
[01:41:51.520 --> 01:41:52.680]   at the end of the day,
[01:41:52.680 --> 01:41:55.260]   not because they suppress the other side's views,
[01:41:55.260 --> 01:41:58.140]   but because they think the case has been stated very well,
[01:41:58.140 --> 01:42:00.320]   that other people can come to understand it.
[01:42:00.320 --> 01:42:01.760]   And if you're highly ideological,
[01:42:01.760 --> 01:42:04.160]   you assume, I think naturally,
[01:42:04.160 --> 01:42:06.420]   if people understood as much about this as I do,
[01:42:06.420 --> 01:42:07.560]   they'll probably agree with me.
[01:42:07.560 --> 01:42:10.640]   You may be wrong about that, but that's often the case.
[01:42:10.640 --> 01:42:12.240]   So that's where, you know,
[01:42:12.240 --> 01:42:14.740]   that's what I think we need to encourage more of
[01:42:14.740 --> 01:42:17.840]   in society generally is grappling with ideas
[01:42:17.840 --> 01:42:21.920]   in a really, you know, thoughtful way.
[01:42:21.920 --> 01:42:25.880]   - So is it possible if the majority of volunteers,
[01:42:25.880 --> 01:42:29.980]   editors of Wikipedia really dislike Donald Trump?
[01:42:29.980 --> 01:42:31.460]   - Mm.
[01:42:31.460 --> 01:42:34.440]   - Are they still able to write an article
[01:42:34.440 --> 01:42:38.520]   that empathizes with the perspective of,
[01:42:38.520 --> 01:42:39.700]   for a time at least,
[01:42:39.700 --> 01:42:41.920]   a very large percentage of the United States
[01:42:41.920 --> 01:42:43.880]   that were supporters of Donald Trump,
[01:42:43.880 --> 01:42:47.280]   and to have a full, broad representation
[01:42:47.280 --> 01:42:50.440]   of him as a human being, him as a political leader,
[01:42:50.440 --> 01:42:53.320]   him as a set of policies promised
[01:42:53.320 --> 01:42:55.620]   and implemented, all that kind of stuff?
[01:42:55.620 --> 01:42:57.120]   - Yeah, I think so.
[01:42:57.120 --> 01:43:00.280]   And I think if you read the article, it's pretty good.
[01:43:00.280 --> 01:43:05.760]   And I think a piece of that is within our community,
[01:43:05.760 --> 01:43:11.120]   if people have the self-awareness to understand,
[01:43:11.120 --> 01:43:13.980]   so I personally wouldn't go
[01:43:13.980 --> 01:43:15.600]   and edit the entry on Donald Trump.
[01:43:15.600 --> 01:43:17.600]   I get emotional about it, and I'm like,
[01:43:17.600 --> 01:43:19.240]   I'm not good at this.
[01:43:19.240 --> 01:43:21.720]   And if I tried to do it, I would fail.
[01:43:21.720 --> 01:43:23.580]   I wouldn't be a good Wikipedian.
[01:43:23.580 --> 01:43:25.180]   So it's better if I just step back
[01:43:25.180 --> 01:43:27.480]   and let people who are more dispassionate
[01:43:27.480 --> 01:43:29.600]   on this topic edit it.
[01:43:29.600 --> 01:43:30.920]   Whereas there are other topics
[01:43:30.920 --> 01:43:33.900]   that are incredibly emotional to some people,
[01:43:33.900 --> 01:43:36.300]   where I can actually do quite well.
[01:43:36.300 --> 01:43:38.020]   Like, I'm gonna be okay.
[01:43:38.020 --> 01:43:42.560]   Maybe, we were discussing earlier, the efficacy of masks.
[01:43:42.560 --> 01:43:44.440]   I'm like, oh, I think that's an interesting problem,
[01:43:44.440 --> 01:43:45.800]   and I don't know the answer,
[01:43:45.800 --> 01:43:48.400]   but I can help kind of catalog what's the best evidence,
[01:43:48.400 --> 01:43:50.680]   and so on, and I'm not gonna get upset.
[01:43:50.680 --> 01:43:51.920]   I'm not gonna get angry.
[01:43:51.920 --> 01:43:54.300]   I'm able to be a good Wikipedian.
[01:43:54.300 --> 01:43:55.600]   So I think that's important.
[01:43:55.600 --> 01:44:00.600]   And I do think, though, in a related framework,
[01:44:00.600 --> 01:44:07.140]   that the composition of the community is really important.
[01:44:07.140 --> 01:44:11.200]   Not because Wikipedia is or should be a battleground,
[01:44:11.200 --> 01:44:13.040]   but because blind spots.
[01:44:13.040 --> 01:44:15.480]   Like, maybe I don't even realize what's biased,
[01:44:15.480 --> 01:44:18.240]   if I'm particularly of a certain point of view,
[01:44:18.240 --> 01:44:20.080]   and I've never thought much about it.
[01:44:20.080 --> 01:44:23.080]   So one of the things we focus on a lot,
[01:44:23.080 --> 01:44:26.640]   the Wikipedia volunteers are,
[01:44:26.640 --> 01:44:27.720]   we don't know the exact number,
[01:44:27.720 --> 01:44:31.320]   but let's say 80% plus male.
[01:44:31.320 --> 01:44:33.000]   And they're of a certain demographic.
[01:44:33.000 --> 01:44:35.960]   They tend to be college-educated,
[01:44:35.960 --> 01:44:39.900]   heavier on tech geeks than not, et cetera, et cetera.
[01:44:39.900 --> 01:44:42.160]   So there is a demographic to the community,
[01:44:42.160 --> 01:44:43.220]   and that's pretty much global.
[01:44:43.220 --> 01:44:44.800]   I mean, somebody said to me once,
[01:44:44.800 --> 01:44:47.280]   "Why is it only white men who edit Wikipedia?"
[01:44:47.280 --> 01:44:49.080]   And I said, "You've obviously not met
[01:44:49.080 --> 01:44:51.440]   "the Japanese Wikipedia community."
[01:44:51.440 --> 01:44:54.000]   It's kind of a joke, because the broader principle
[01:44:54.000 --> 01:44:56.280]   still stands, who edits Japanese Wikipedia?
[01:44:56.280 --> 01:44:59.520]   A bunch of geeky men, right?
[01:44:59.520 --> 01:45:00.760]   And women as well.
[01:45:00.760 --> 01:45:01.960]   So we do have women in the community,
[01:45:01.960 --> 01:45:03.240]   and that's very important.
[01:45:03.240 --> 01:45:04.720]   But we do think, okay, you know what?
[01:45:04.720 --> 01:45:06.800]   That does lead to some problems.
[01:45:06.800 --> 01:45:10.120]   It leads to some content issues,
[01:45:10.120 --> 01:45:13.280]   simply because people write more about what they know
[01:45:13.280 --> 01:45:14.840]   and what they're interested in.
[01:45:14.840 --> 01:45:18.720]   They'll tend to be dismissive of things as being unimportant,
[01:45:18.720 --> 01:45:20.680]   if it's not something that they personally
[01:45:20.680 --> 01:45:21.720]   have an interest in.
[01:45:21.720 --> 01:45:27.680]   I like the example, as a parent, I would say,
[01:45:27.680 --> 01:45:29.760]   our entries on early childhood development
[01:45:29.760 --> 01:45:31.640]   probably aren't as good as they should be,
[01:45:31.640 --> 01:45:33.680]   because a lot of the Wikipedia volunteers,
[01:45:33.680 --> 01:45:35.800]   actually, we're getting older, the Wikipedians,
[01:45:35.800 --> 01:45:38.360]   so the demographic has changed a bit.
[01:45:38.360 --> 01:45:42.320]   But it's like, if you've got a bunch of 25-year-old
[01:45:42.320 --> 01:45:46.800]   tech geek dudes who don't have kids,
[01:45:46.800 --> 01:45:48.200]   they're just not gonna be interested
[01:45:48.200 --> 01:45:49.680]   in early childhood development.
[01:45:49.680 --> 01:45:50.800]   And if they tried to write about it,
[01:45:50.800 --> 01:45:51.880]   they probably wouldn't do a good job,
[01:45:51.880 --> 01:45:53.320]   'cause they don't know anything about it.
[01:45:53.320 --> 01:45:58.320]   And somebody did a look at our entries on novelists
[01:45:58.320 --> 01:46:00.600]   who've won a major literary prize.
[01:46:00.600 --> 01:46:03.520]   And they looked at the male novelist versus the female.
[01:46:03.520 --> 01:46:07.200]   And the male novelist had longer and higher quality entries.
[01:46:07.200 --> 01:46:08.040]   And why is that?
[01:46:08.040 --> 01:46:11.200]   Well, it's not because, 'cause I know hundreds
[01:46:11.200 --> 01:46:13.720]   of Wikipedians, it's not because these are a bunch
[01:46:13.720 --> 01:46:17.600]   of biased, sexist men who are like,
[01:46:17.600 --> 01:46:20.160]   books by women are not important.
[01:46:20.160 --> 01:46:25.160]   It's like, no, actually, there is a gender
[01:46:25.160 --> 01:46:27.240]   kind of breakdown of readership.
[01:46:27.240 --> 01:46:31.280]   There are books, like "Hard Science Fiction"
[01:46:31.280 --> 01:46:32.120]   is a classic example.
[01:46:32.120 --> 01:46:35.680]   "Hard Science Fiction," mostly read by men.
[01:46:35.680 --> 01:46:39.200]   Other types of novels, more read by women.
[01:46:39.200 --> 01:46:41.000]   And if we don't have women in the community,
[01:46:41.000 --> 01:46:44.880]   then these award-winning, clearly important novelists
[01:46:44.880 --> 01:46:45.960]   may have less coverage.
[01:46:45.960 --> 01:46:48.440]   And not because anybody consciously thinks,
[01:46:48.440 --> 01:46:51.680]   oh, we don't like what, a book by Maya Angelou,
[01:46:51.680 --> 01:46:55.080]   like who cares, she's a poet, that's not interesting.
[01:46:55.080 --> 01:46:57.320]   No, but just because, well, people write what they know,
[01:46:57.320 --> 01:46:58.680]   they write what they're interested in.
[01:46:58.680 --> 01:47:00.620]   So we do think diversity in the community
[01:47:00.620 --> 01:47:01.760]   is really important.
[01:47:01.760 --> 01:47:05.080]   And that's one area where I do think it's really clear.
[01:47:05.080 --> 01:47:07.600]   But I can also say, you know what, actually,
[01:47:07.600 --> 01:47:10.720]   that also applies in the political sphere.
[01:47:10.720 --> 01:47:13.800]   Like, to say, actually, we do want kind
[01:47:13.800 --> 01:47:16.560]   and thoughtful Catholic priests,
[01:47:16.560 --> 01:47:18.040]   kind and thoughtful conservatives,
[01:47:18.040 --> 01:47:19.920]   kind and thoughtful libertarians,
[01:47:19.920 --> 01:47:23.460]   kind and thoughtful Marxists to come in.
[01:47:23.460 --> 01:47:25.440]   But the key is the kind and thoughtful piece.
[01:47:25.440 --> 01:47:28.360]   So when people sometimes come to Wikipedia,
[01:47:28.360 --> 01:47:33.000]   outraged by some dramatic thing that's happened on Twitter,
[01:47:33.000 --> 01:47:35.000]   they come to Wikipedia with a chip on their shoulder,
[01:47:35.000 --> 01:47:38.760]   ready to do battle, and it just doesn't work out very well.
[01:47:38.760 --> 01:47:41.320]   - And there's tribes in general where,
[01:47:41.320 --> 01:47:45.360]   I think there's a responsibility on the larger group
[01:47:45.360 --> 01:47:48.840]   to be even kinder and more welcoming to the smaller group.
[01:47:48.840 --> 01:47:50.840]   - Yeah, we think that's really important.
[01:47:50.840 --> 01:47:53.640]   And so, you know, oftentimes people come in,
[01:47:53.640 --> 01:47:55.840]   and you know, there's a lot,
[01:47:55.840 --> 01:47:57.560]   when I talk about community health,
[01:47:57.560 --> 01:48:00.880]   one of the aspects of that that we do think about a lot,
[01:48:00.880 --> 01:48:05.440]   that I think about a lot, is not about politics.
[01:48:05.440 --> 01:48:08.880]   It's just like, how are we treating newcomers
[01:48:08.880 --> 01:48:10.440]   to the community?
[01:48:10.440 --> 01:48:13.480]   And so I can tell you what our ideals are,
[01:48:13.480 --> 01:48:17.160]   what our philosophy is, but do we live up to that?
[01:48:17.160 --> 01:48:19.320]   So, you know, the ideal is you come to Wikipedia,
[01:48:19.320 --> 01:48:21.680]   you know, we have rules,
[01:48:21.680 --> 01:48:24.760]   like one of our fundamental rules is ignore all rules,
[01:48:24.760 --> 01:48:26.440]   which is partly written that way
[01:48:26.440 --> 01:48:29.080]   because it kind of piques people's attention,
[01:48:29.080 --> 01:48:32.080]   like, what the hell kind of rule is that, you know?
[01:48:32.080 --> 01:48:35.680]   But basically says, look, don't get nervous and depressed
[01:48:35.680 --> 01:48:37.480]   about a bunch of, you know,
[01:48:37.480 --> 01:48:39.680]   what's the formatting of your footnote, right?
[01:48:39.680 --> 01:48:41.840]   So you shouldn't come to Wikipedia,
[01:48:41.840 --> 01:48:43.840]   add a link and then get banned or yelled at
[01:48:43.840 --> 01:48:45.560]   because it's not the right format.
[01:48:45.560 --> 01:48:48.920]   Instead, somebody should go, oh, hey,
[01:48:48.920 --> 01:48:50.360]   yeah, thanks for helping,
[01:48:50.360 --> 01:48:55.000]   but, you know, here's the link to how to format,
[01:48:55.000 --> 01:48:56.320]   you know, if you want to keep going,
[01:48:56.320 --> 01:48:59.480]   you might want to learn how to format a footnote.
[01:48:59.480 --> 01:49:01.600]   And to be friendly and to be open and to say,
[01:49:01.600 --> 01:49:02.960]   oh, right, oh, you're new,
[01:49:02.960 --> 01:49:06.280]   and you clearly don't know everything about Wikipedia.
[01:49:06.280 --> 01:49:08.440]   And, you know, sometimes in any community,
[01:49:08.440 --> 01:49:09.280]   that can be quite hard.
[01:49:09.280 --> 01:49:12.840]   So people come in and they've got a great big idea
[01:49:12.840 --> 01:49:14.760]   and they're going to propose this to the Wikipedia community
[01:49:14.760 --> 01:49:16.360]   and they have no idea.
[01:49:16.360 --> 01:49:18.000]   That's basically a perennial discussion
[01:49:18.000 --> 01:49:20.880]   we've had 7,000 times before.
[01:49:20.880 --> 01:49:23.560]   And so then ideally you would say to the person,
[01:49:23.560 --> 01:49:25.320]   oh, yeah, great, thanks.
[01:49:25.320 --> 01:49:27.840]   Like a lot of people have, and here's where we got to,
[01:49:27.840 --> 01:49:30.720]   and here's the nuanced conversation we've had about that
[01:49:30.720 --> 01:49:32.960]   in the past that I think you'll find interesting.
[01:49:32.960 --> 01:49:34.920]   And sometimes people are just like, oh God, another one,
[01:49:34.920 --> 01:49:37.520]   you know, who's come in with this idea, which doesn't work.
[01:49:37.520 --> 01:49:39.040]   And they don't understand why.
[01:49:39.040 --> 01:49:40.720]   - You can lose patience, but you shouldn't.
[01:49:40.720 --> 01:49:42.640]   - And that's kind of human, you know.
[01:49:42.640 --> 01:49:45.640]   But I think it just does require really thinking,
[01:49:45.640 --> 01:49:50.640]   you know, in a self-aware manner of like,
[01:49:50.640 --> 01:49:52.880]   oh, I was once a newbie.
[01:49:52.880 --> 01:49:54.240]   Actually, we have a great,
[01:49:54.240 --> 01:49:58.200]   I just did an interview with Emily Temple Woods,
[01:49:58.200 --> 01:50:00.280]   who she was Wikipedia of the Year.
[01:50:00.280 --> 01:50:03.160]   She's just like a great, well-known Wikipedian.
[01:50:03.160 --> 01:50:04.640]   And I interviewed her for my book
[01:50:04.640 --> 01:50:06.840]   and she told me something I never knew.
[01:50:06.840 --> 01:50:07.960]   Apparently it's not secret.
[01:50:07.960 --> 01:50:09.360]   Like, she didn't reveal it to me,
[01:50:09.360 --> 01:50:13.560]   but is that when she started at Wikipedia, she was a vandal.
[01:50:13.560 --> 01:50:15.800]   She came in and vandalized Wikipedia.
[01:50:15.800 --> 01:50:17.560]   And then basically what happened was
[01:50:17.560 --> 01:50:21.320]   she'd done some sort of vandalized a couple of articles
[01:50:21.320 --> 01:50:23.840]   and then somebody popped up on her talk page and said,
[01:50:23.840 --> 01:50:25.240]   "Hey, like, why are you doing this?
[01:50:25.240 --> 01:50:27.480]   "Like, we're trying to make an encyclopedia here."
[01:50:27.480 --> 01:50:29.160]   And this wasn't very kind.
[01:50:29.160 --> 01:50:31.000]   And she felt so bad.
[01:50:31.000 --> 01:50:33.560]   She's like, "Oh, right, I didn't really think of it that way."
[01:50:33.560 --> 01:50:36.560]   She just was coming in as, she was like 13 years old,
[01:50:36.560 --> 01:50:39.680]   combative and, you know, like having fun and trolling a bit.
[01:50:39.680 --> 01:50:42.360]   And then she's like, "Oh, actually, oh, I see your point."
[01:50:42.360 --> 01:50:43.600]   And became a great Wikipedian.
[01:50:43.600 --> 01:50:45.600]   So that's the ideal really,
[01:50:45.600 --> 01:50:48.720]   is that you don't just go, troll, block, fuck off.
[01:50:48.720 --> 01:50:51.320]   You go, "Hey, you know, like, what gives?"
[01:50:51.320 --> 01:50:56.040]   Which is, I think the way we tend to treat things
[01:50:56.040 --> 01:50:58.800]   in real life, you know, if you've got somebody
[01:50:58.800 --> 01:51:01.840]   who's doing something obnoxious in your friend group,
[01:51:01.840 --> 01:51:05.320]   you probably go, "Hey, like, really,
[01:51:05.320 --> 01:51:06.920]   "I don't know if you've noticed,
[01:51:06.920 --> 01:51:09.520]   "but I think this person is actually quite hurt
[01:51:09.520 --> 01:51:11.600]   "that you keep making that joke about them."
[01:51:11.600 --> 01:51:13.800]   And then they usually go, "Oh, you know what?
[01:51:13.800 --> 01:51:15.800]   "I didn't, I thought that was okay, I didn't."
[01:51:15.800 --> 01:51:17.000]   And then they stop.
[01:51:17.000 --> 01:51:19.200]   Or they keep it up and then everybody goes,
[01:51:19.200 --> 01:51:20.600]   "Well, you're the asshole."
[01:51:20.600 --> 01:51:23.680]   - Well, yeah, I mean, that's just an example
[01:51:23.680 --> 01:51:25.600]   that gives me faith in humanity,
[01:51:25.600 --> 01:51:30.600]   that we're all capable and wanting to be kind to each other.
[01:51:30.600 --> 01:51:33.920]   And in general, the fact that there's a small group
[01:51:33.920 --> 01:51:38.400]   of volunteers that are able to contribute so much
[01:51:38.400 --> 01:51:40.320]   to the organization, the collection,
[01:51:40.320 --> 01:51:45.640]   the discussion of all of human knowledge,
[01:51:45.640 --> 01:51:47.680]   it's so, it makes me so grateful
[01:51:47.680 --> 01:51:50.320]   to be part of this whole human project.
[01:51:50.320 --> 01:51:52.480]   That's one of the reasons I love Wikipedia,
[01:51:52.480 --> 01:51:54.360]   is it gives me faith in humanity.
[01:51:54.360 --> 01:51:59.360]   - No, I once was at Wikimania, our annual conference,
[01:51:59.360 --> 01:52:01.760]   and people come from all around the world,
[01:52:01.760 --> 01:52:04.520]   like really active volunteers.
[01:52:04.520 --> 01:52:06.720]   I was at the dinner, we were in Egypt,
[01:52:06.720 --> 01:52:08.360]   at Wikimania in Alexandria,
[01:52:08.360 --> 01:52:11.040]   at the sort of closing dinner or whatever.
[01:52:11.040 --> 01:52:12.720]   And a friend of mine came and sat at the table,
[01:52:12.720 --> 01:52:16.000]   and she's sort of been in the movement more broadly,
[01:52:16.000 --> 01:52:17.800]   Creative Commons, she's not really a Wikipedian,
[01:52:17.800 --> 01:52:18.640]   she'd come to the conference
[01:52:18.640 --> 01:52:21.280]   'cause she's into Creative Commons and all that.
[01:52:21.280 --> 01:52:22.840]   So we have dinner and it just turned out,
[01:52:22.840 --> 01:52:25.280]   I sat down at the table with most of the members
[01:52:25.280 --> 01:52:27.600]   of the English Language Arbitration Committee.
[01:52:27.600 --> 01:52:31.320]   And they're a bunch of very sweet, geeky Wikipedians.
[01:52:31.320 --> 01:52:33.720]   And as we left the table, I said to her,
[01:52:33.720 --> 01:52:36.960]   "It's really like, I still find this
[01:52:36.960 --> 01:52:38.960]   "kind of sense of amazement.
[01:52:38.960 --> 01:52:41.340]   "Like we just had dinner with some of the most powerful
[01:52:41.340 --> 01:52:43.760]   "people in English language media."
[01:52:43.760 --> 01:52:44.880]   'Cause they're the people who are like
[01:52:44.880 --> 01:52:48.000]   the final court of appeal in English Wikipedia.
[01:52:48.000 --> 01:52:50.880]   And thank goodness they're not media moguls, right?
[01:52:50.880 --> 01:52:52.480]   They're just a bunch of geeks,
[01:52:52.480 --> 01:52:54.920]   who are just like well-liked in the community
[01:52:54.920 --> 01:52:56.200]   'cause they're kind and they're thoughtful
[01:52:56.200 --> 01:52:59.160]   and they really sort of think about things.
[01:52:59.160 --> 01:53:01.840]   I was like, "This is great, love Wikipedia."
[01:53:01.840 --> 01:53:06.840]   - It's like to the degree that geeks run the best aspect
[01:53:06.840 --> 01:53:10.840]   of human civilization brings me joy in all aspects.
[01:53:10.840 --> 01:53:14.120]   And this is true in programming, like Linux,
[01:53:14.120 --> 01:53:18.040]   like programmers, like people that kind of specialize
[01:53:18.040 --> 01:53:21.980]   in a thing and they don't really get caught up
[01:53:21.980 --> 01:53:25.680]   into the mess of the bickering of society.
[01:53:25.680 --> 01:53:27.120]   They just kind of do their thing
[01:53:27.120 --> 01:53:29.720]   and they value the craftsmanship of it,
[01:53:29.720 --> 01:53:30.560]   the competence of it.
[01:53:30.560 --> 01:53:33.480]   - Well, if you've never heard of this or looked into it,
[01:53:33.480 --> 01:53:34.720]   you'll enjoy it.
[01:53:34.720 --> 01:53:36.760]   I read something recently that I didn't even know about,
[01:53:36.760 --> 01:53:41.760]   but like the fundamental time zones
[01:53:41.760 --> 01:53:44.440]   and they change from time to time.
[01:53:44.440 --> 01:53:46.700]   Sometimes a country will pass daylight savings
[01:53:46.700 --> 01:53:49.080]   or move it by a week, whatever.
[01:53:49.080 --> 01:53:54.080]   There's a file that's done all sort of UNIX-based computers
[01:53:54.080 --> 01:53:56.880]   and basically all computers end up using this file.
[01:53:56.880 --> 01:53:59.800]   It's the official time zone file, but why is it official?
[01:53:59.800 --> 01:54:01.640]   It's just this one guy.
[01:54:01.640 --> 01:54:04.600]   It's like this guy and a group, a community around him.
[01:54:04.600 --> 01:54:07.680]   And basically something weird happened
[01:54:07.680 --> 01:54:11.040]   and it broke something because he was on vacation.
[01:54:11.040 --> 01:54:13.520]   And I'm just like, isn't that wild, right?
[01:54:13.520 --> 01:54:15.320]   That you would think, I mean, first of all,
[01:54:15.320 --> 01:54:16.560]   most people never even think about like,
[01:54:16.560 --> 01:54:18.940]   how do computers know about time zones?
[01:54:18.940 --> 01:54:22.040]   Well, they know 'cause they just use this file,
[01:54:22.040 --> 01:54:23.320]   which tells all the time zones
[01:54:23.320 --> 01:54:25.760]   and which dates they change and all of that.
[01:54:25.760 --> 01:54:28.240]   But there's this one guy and he doesn't get paid for it.
[01:54:28.240 --> 01:54:31.760]   It's just, he's like, with all the billions of people
[01:54:31.760 --> 01:54:33.840]   on the planet, he sort of put his hand up and goes,
[01:54:33.840 --> 01:54:36.080]   yo, I'll take care of the time zones.
[01:54:36.080 --> 01:54:38.920]   - And there's a lot, a lot, a lot of programmers
[01:54:38.920 --> 01:54:43.520]   listening to this right now with PTSD about time zones.
[01:54:43.520 --> 01:54:46.680]   And then there, I mean, there's on top of this one guy,
[01:54:46.680 --> 01:54:48.480]   there's other libraries,
[01:54:48.480 --> 01:54:49.840]   the different programming languages
[01:54:49.840 --> 01:54:51.460]   that help manage the time zones for you,
[01:54:51.460 --> 01:54:54.040]   but still there's just within those,
[01:54:55.280 --> 01:54:58.080]   it's amazing just the packages, the libraries,
[01:54:58.080 --> 01:55:02.000]   how few people build them out of their own love
[01:55:02.000 --> 01:55:05.160]   for building, for creating, for community and all of that.
[01:55:05.160 --> 01:55:08.080]   It's, I almost like don't want to interfere
[01:55:08.080 --> 01:55:10.480]   with the natural habitat of the geek, right?
[01:55:10.480 --> 01:55:11.960]   Like when you spot 'em in the wild,
[01:55:11.960 --> 01:55:14.160]   you just wanna be like, whoa, careful.
[01:55:14.160 --> 01:55:17.000]   That thing, that thing needs to be treasured.
[01:55:17.000 --> 01:55:20.720]   - I met a guy many years ago, lovely, really sweet guy.
[01:55:20.720 --> 01:55:25.440]   And he was running a bot on English Wikipedia
[01:55:25.440 --> 01:55:27.640]   that I thought, wow, that's actually super clever.
[01:55:27.640 --> 01:55:31.280]   And what he had done is, his bot was like spell checking,
[01:55:31.280 --> 01:55:33.280]   but rather than simple spell checking,
[01:55:33.280 --> 01:55:35.720]   what he had done is create a database
[01:55:35.720 --> 01:55:39.240]   of words that are commonly mistaken for other words.
[01:55:39.240 --> 01:55:43.040]   They're spelled wrong, so I can't even give an example.
[01:55:43.040 --> 01:55:47.000]   And so the word is, people often spell it wrong,
[01:55:47.000 --> 01:55:49.520]   but no spell checker catches it
[01:55:49.520 --> 01:55:52.200]   because it is another word.
[01:55:52.200 --> 01:55:54.840]   And so what he did is, he wrote a bot
[01:55:54.840 --> 01:55:58.400]   that looks for these words and then checks the sentence
[01:55:58.400 --> 01:56:00.480]   around it for certain keywords.
[01:56:00.480 --> 01:56:04.920]   So in some context, this isn't correct,
[01:56:04.920 --> 01:56:09.920]   but buoy and boy, people sometimes type B-O-Y
[01:56:09.920 --> 01:56:11.760]   when they mean B-O-U-Y.
[01:56:11.760 --> 01:56:14.560]   So if he sees the word boy, B-O-Y in an article,
[01:56:14.560 --> 01:56:15.960]   he would look in the context and see,
[01:56:15.960 --> 01:56:17.520]   is this a nautical reference?
[01:56:17.520 --> 01:56:19.840]   And if it was, he didn't auto correct,
[01:56:19.840 --> 01:56:21.960]   he just would flag it up to himself to go,
[01:56:21.960 --> 01:56:23.120]   oh, check this one out.
[01:56:23.120 --> 01:56:24.440]   And that's not a great example,
[01:56:24.440 --> 01:56:26.640]   but he had thousands of examples.
[01:56:26.640 --> 01:56:28.200]   I was like, that's amazing.
[01:56:28.200 --> 01:56:30.240]   Like I would have never thought to do that.
[01:56:30.240 --> 01:56:31.640]   And I'm glad that somebody did.
[01:56:31.640 --> 01:56:34.800]   And that's also part of the openness of the system.
[01:56:34.800 --> 01:56:37.200]   And also I think being a charity,
[01:56:37.200 --> 01:56:40.320]   being this idea of like, actually,
[01:56:40.320 --> 01:56:44.040]   this is a gift to the world that makes someone go,
[01:56:44.040 --> 01:56:46.080]   oh, oh, well, I'll put my hand up.
[01:56:46.080 --> 01:56:48.880]   Like I see a little piece of things that I can make better
[01:56:48.880 --> 01:56:49.880]   'cause I'm a good programmer
[01:56:49.880 --> 01:56:51.480]   and I can write this script to do this thing
[01:56:51.480 --> 01:56:52.600]   and I'll find it fun.
[01:56:52.600 --> 01:56:54.600]   Amazing.
[01:56:54.600 --> 01:56:58.520]   - Well, I gotta ask about this big, bold decision
[01:56:58.520 --> 01:57:00.400]   at the very beginning to not do advertisements
[01:57:00.400 --> 01:57:02.880]   on the website and just in general,
[01:57:02.880 --> 01:57:05.000]   the philosophy of the business model, Wikipedia,
[01:57:05.000 --> 01:57:06.440]   what went behind that?
[01:57:06.440 --> 01:57:09.920]   - Yeah, so I think most people know this,
[01:57:09.920 --> 01:57:11.080]   but we're a charity.
[01:57:11.080 --> 01:57:15.800]   So in the US, you know, registered as a charity.
[01:57:15.800 --> 01:57:19.120]   And we don't have any ads on the site.
[01:57:19.120 --> 01:57:24.040]   And the vast majority of the money is from donations,
[01:57:24.040 --> 01:57:26.680]   but the vast majority from small donors.
[01:57:26.680 --> 01:57:29.200]   So people giving 25 bucks or whatever.
[01:57:29.200 --> 01:57:31.000]   - If you're listening to this, go donate.
[01:57:31.000 --> 01:57:32.960]   - Go donate. - Donate now.
[01:57:32.960 --> 01:57:34.480]   I've donated so many times.
[01:57:34.480 --> 01:57:36.680]   - And we have, you know, millions of donors every year,
[01:57:36.680 --> 01:57:38.640]   but it's like a small percentage of people.
[01:57:38.640 --> 01:57:39.880]   I would say in the early days,
[01:57:39.880 --> 01:57:42.880]   a big part of it was aesthetic almost
[01:57:42.880 --> 01:57:43.960]   as much as anything else.
[01:57:43.960 --> 01:57:46.080]   It was just like, I just think,
[01:57:46.080 --> 01:57:48.360]   I don't really want ads in Wikipedia.
[01:57:48.360 --> 01:57:50.200]   Like, I just think it would be,
[01:57:50.200 --> 01:57:52.080]   there's a lot of reasons why it might not be good.
[01:57:52.080 --> 01:57:57.080]   And even back then, I didn't think as much as I have since
[01:57:57.080 --> 01:58:01.880]   about a business model can tend to drive you
[01:58:01.880 --> 01:58:03.320]   in a certain place.
[01:58:03.320 --> 01:58:06.720]   And really thinking that through in advance
[01:58:06.720 --> 01:58:08.880]   is really important because you might say,
[01:58:08.880 --> 01:58:13.360]   yeah, we're really, really keen on community control
[01:58:13.360 --> 01:58:16.400]   and neutrality, but if we had an advertising-based
[01:58:16.400 --> 01:58:19.880]   business model, probably that would begin to erode.
[01:58:19.880 --> 01:58:22.120]   Even if I believe in it very strongly,
[01:58:22.120 --> 01:58:24.320]   organizations tend to follow the money
[01:58:24.320 --> 01:58:25.720]   in the DNA in the long run.
[01:58:25.720 --> 01:58:29.960]   And so things like, I mean, it's easy to think about
[01:58:29.960 --> 01:58:31.120]   some of the immediate problems.
[01:58:31.120 --> 01:58:36.120]   So like if you go to read about, I don't know,
[01:58:36.120 --> 01:58:42.480]   Nissan car company.
[01:58:42.480 --> 01:58:44.960]   And if you saw an ad for the new Nissan
[01:58:44.960 --> 01:58:46.880]   at the top of the page, you might be like,
[01:58:46.880 --> 01:58:47.880]   did they pay for this?
[01:58:47.880 --> 01:58:51.040]   Or like, do the advertisers have influence over the content?
[01:58:51.040 --> 01:58:52.280]   Because you kind of wonder about that
[01:58:52.280 --> 01:58:53.680]   for all kinds of media.
[01:58:53.680 --> 01:58:55.480]   - And that undermines trust.
[01:58:55.480 --> 01:58:57.000]   - Undermines trust, right?
[01:58:57.000 --> 01:59:01.600]   But also things like, we don't have clickbait headlines
[01:59:01.600 --> 01:59:02.440]   in Wikipedia.
[01:59:02.440 --> 01:59:05.040]   You've never seen Wikipedia entries
[01:59:05.040 --> 01:59:07.720]   with all this kind of listicles,
[01:59:07.720 --> 01:59:11.320]   sort of the 10 funniest cat pictures,
[01:59:11.320 --> 01:59:13.120]   number seven make you cry.
[01:59:13.120 --> 01:59:14.880]   None of that kind of stuff, 'cause there's no incentive,
[01:59:14.880 --> 01:59:16.480]   no reason to do that.
[01:59:16.480 --> 01:59:21.440]   Also, there's no reason to have an algorithm to say,
[01:59:21.440 --> 01:59:25.040]   actually, we're gonna use our algorithm to drive you
[01:59:25.040 --> 01:59:26.560]   to stay on the website longer.
[01:59:26.560 --> 01:59:29.160]   We're gonna use the algorithm to drive you to,
[01:59:29.160 --> 01:59:32.680]   it's like, oh, you're reading about Queen Victoria.
[01:59:32.680 --> 01:59:34.120]   There's nothing to sell you when you're reading
[01:59:34.120 --> 01:59:34.960]   about Queen Victoria.
[01:59:34.960 --> 01:59:36.160]   Let's move you on to Las Vegas,
[01:59:36.160 --> 01:59:38.440]   'cause actually the ad revenue around hotels
[01:59:38.440 --> 01:59:40.040]   in Las Vegas is quite good.
[01:59:40.040 --> 01:59:42.320]   So we don't have that sort of,
[01:59:42.320 --> 01:59:44.320]   there's no incentive for the organization to go,
[01:59:44.320 --> 01:59:46.480]   oh, let's move people around to things
[01:59:46.480 --> 01:59:47.960]   that have better ad revenue.
[01:59:47.960 --> 01:59:50.040]   Instead, it's just like, oh, well,
[01:59:50.040 --> 01:59:51.880]   what's most interesting to the community,
[01:59:51.880 --> 01:59:53.040]   just to make those links.
[01:59:53.040 --> 01:59:58.040]   So that decision just seemed obvious to me,
[01:59:58.040 --> 02:00:03.120]   but as I say, it was less of a business decision
[02:00:03.120 --> 02:00:04.600]   and more of an aesthetic.
[02:00:04.600 --> 02:00:06.560]   It's like, oh, this is how I,
[02:00:06.560 --> 02:00:08.320]   I like Wikipedia, it doesn't have ads.
[02:00:08.320 --> 02:00:11.160]   Don't really want, in these early days,
[02:00:11.160 --> 02:00:14.720]   like a lot of the ads, that was well before the era
[02:00:14.720 --> 02:00:16.920]   of really quality ad targeting and all that.
[02:00:16.920 --> 02:00:18.280]   So you get a lot of--
[02:00:18.280 --> 02:00:19.120]   - Banners.
[02:00:19.120 --> 02:00:20.680]   - Banners, punch the monkey ads
[02:00:20.680 --> 02:00:22.560]   and all that kind of nonsense.
[02:00:22.560 --> 02:00:27.560]   And so, but there was no guarantee.
[02:00:27.560 --> 02:00:30.160]   There was no, it was not really clear
[02:00:30.160 --> 02:00:33.240]   how could we fund this?
[02:00:33.240 --> 02:00:34.600]   Like it was pretty cheap,
[02:00:34.600 --> 02:00:37.040]   is it still is quite cheap compared to,
[02:00:37.960 --> 02:00:42.280]   most, we don't have 100,000 employees and all of that,
[02:00:42.280 --> 02:00:45.800]   but would we be able to raise money through donations?
[02:00:45.800 --> 02:00:50.200]   And so I remember the first time that we did,
[02:00:50.200 --> 02:00:54.040]   like really did a donation campaign
[02:00:54.040 --> 02:00:59.040]   was on a Christmas day in 2003, I think it was.
[02:00:59.040 --> 02:01:04.720]   There was, we had three servers,
[02:01:04.720 --> 02:01:06.520]   database servers and two front end servers,
[02:01:06.520 --> 02:01:07.840]   and they were all the same size
[02:01:07.840 --> 02:01:10.960]   or whatever, and two of them crashed.
[02:01:10.960 --> 02:01:12.800]   They broke, like, I don't even know,
[02:01:12.800 --> 02:01:14.440]   remember now, like the hard drive,
[02:01:14.440 --> 02:01:16.880]   that was like, it's Christmas day.
[02:01:16.880 --> 02:01:18.480]   So I scrambled on Christmas day
[02:01:18.480 --> 02:01:21.200]   to sort of go onto the database server,
[02:01:21.200 --> 02:01:22.400]   which fortunately survived,
[02:01:22.400 --> 02:01:24.800]   and have it become a front end server as well.
[02:01:24.800 --> 02:01:26.800]   And then the site was really slow
[02:01:26.800 --> 02:01:28.200]   and it wasn't working very well.
[02:01:28.200 --> 02:01:29.360]   And I was like, okay, it's time,
[02:01:29.360 --> 02:01:31.400]   we need to do a fundraiser.
[02:01:31.400 --> 02:01:36.400]   And so I was hoping to raise $20,000 in a month's time,
[02:01:37.160 --> 02:01:41.400]   but we raised nearly 30,000 within two, three weeks time.
[02:01:41.400 --> 02:01:43.160]   So that was the first proof point of like,
[02:01:43.160 --> 02:01:46.280]   oh, like we put a banner up and people will donate.
[02:01:46.280 --> 02:01:47.800]   Like we just explained we need the money
[02:01:47.800 --> 02:01:49.320]   and people are like, already,
[02:01:49.320 --> 02:01:50.520]   we were very small back then,
[02:01:50.520 --> 02:01:51.800]   and people were like, oh yeah,
[02:01:51.800 --> 02:01:54.200]   like, I love this, I wanna contribute.
[02:01:54.200 --> 02:01:55.120]   Then over the years,
[02:01:55.120 --> 02:01:57.600]   we've become more sophisticated
[02:01:57.600 --> 02:01:59.440]   about the fundraising campaigns,
[02:01:59.440 --> 02:02:03.080]   and we've tested a lot of different messaging and so forth.
[02:02:03.080 --> 02:02:05.280]   What we used to think,
[02:02:05.280 --> 02:02:08.320]   I remember one year we really went heavy with,
[02:02:08.320 --> 02:02:10.440]   we have great ambitions to,
[02:02:10.440 --> 02:02:14.200]   the idea of Wikipedia is a free encyclopedia
[02:02:14.200 --> 02:02:16.560]   for every single person on the planet.
[02:02:16.560 --> 02:02:20.760]   So what about the languages of Sub-Saharan Africa?
[02:02:20.760 --> 02:02:22.600]   So I thought, okay, we're trying to raise money,
[02:02:22.600 --> 02:02:24.280]   we need to talk about that,
[02:02:24.280 --> 02:02:26.720]   'cause it's really important and near and dear to my heart.
[02:02:26.720 --> 02:02:28.560]   And just instinctively,
[02:02:28.560 --> 02:02:30.600]   knowing nothing about charity fundraising,
[02:02:30.600 --> 02:02:31.680]   you see it all around, it's like,
[02:02:31.680 --> 02:02:33.920]   oh, charities always mention
[02:02:34.400 --> 02:02:35.800]   the poor people they're helping,
[02:02:35.800 --> 02:02:38.880]   so let's talk about that, didn't really work as well.
[02:02:38.880 --> 02:02:42.520]   The pitch that, this is very vague and very sort of broad,
[02:02:42.520 --> 02:02:45.760]   but the pitch that works better than any other in general
[02:02:45.760 --> 02:02:49.480]   is a fairness pitch of,
[02:02:49.480 --> 02:02:52.280]   you use it all the time, you should probably chip in.
[02:02:52.280 --> 02:02:54.240]   And most people are like, yeah, you know what?
[02:02:54.240 --> 02:02:56.000]   My life would suck without Wikipedia,
[02:02:56.000 --> 02:02:59.360]   I use it constantly, and whatever, I should chip in.
[02:02:59.360 --> 02:03:01.360]   It just seems like the right thing to do.
[02:03:02.400 --> 02:03:04.520]   And there's many variants on that, obviously.
[02:03:04.520 --> 02:03:06.840]   And that's really, it works,
[02:03:06.840 --> 02:03:09.600]   and people are like, oh yeah, Wikipedia, I love Wikipedia,
[02:03:09.600 --> 02:03:11.520]   and I shouldn't.
[02:03:11.520 --> 02:03:13.440]   And so sometimes people say,
[02:03:13.440 --> 02:03:17.600]   why are you always begging for money on the website?
[02:03:17.600 --> 02:03:20.400]   And it's not that often, it's not that much,
[02:03:20.400 --> 02:03:22.280]   but it does happen.
[02:03:22.280 --> 02:03:24.320]   They're like, why don't you just get Google
[02:03:24.320 --> 02:03:29.320]   and Facebook and Microsoft, why don't they pay for it?
[02:03:29.520 --> 02:03:33.960]   And I'm like, I don't think that's really the right answer.
[02:03:33.960 --> 02:03:35.480]   - Influence starts to creep in.
[02:03:35.480 --> 02:03:36.840]   - Influence starts to creep in,
[02:03:36.840 --> 02:03:38.720]   and questions start to creep in.
[02:03:38.720 --> 02:03:42.360]   Like the best funding for Wikipedia is the small donors.
[02:03:42.360 --> 02:03:43.920]   We also have major donors, right?
[02:03:43.920 --> 02:03:46.400]   We have high net worth people who donate.
[02:03:46.400 --> 02:03:49.320]   But we always are very careful about that sort of thing,
[02:03:49.320 --> 02:03:52.640]   to say, wow, that's really great and really important,
[02:03:52.640 --> 02:03:56.200]   but we can't let that become influence,
[02:03:56.200 --> 02:03:59.320]   because that would just be really quite, yeah.
[02:03:59.800 --> 02:04:01.000]   Not good for Wikipedia.
[02:04:01.000 --> 02:04:03.720]   - I would love to know how many times I've visited Wikipedia
[02:04:03.720 --> 02:04:05.640]   and how much time I've spent on it,
[02:04:05.640 --> 02:04:07.640]   because I have a general sense
[02:04:07.640 --> 02:04:10.280]   that it's the most useful site I've ever used,
[02:04:10.280 --> 02:04:12.080]   competing maybe with Google search,
[02:04:12.080 --> 02:04:15.640]   which ultimately lands on Wikipedia.
[02:04:15.640 --> 02:04:16.600]   - Yeah, yeah, yeah.
[02:04:16.600 --> 02:04:19.520]   - But if I were just reminded of,
[02:04:19.520 --> 02:04:22.160]   hey, remember all those times your life was made better
[02:04:22.160 --> 02:04:23.400]   because of this site?
[02:04:23.400 --> 02:04:25.840]   I think I would be much more like, yeah.
[02:04:25.840 --> 02:04:29.960]   Why did I waste money on site XYZ,
[02:04:29.960 --> 02:04:33.080]   when I could be like, I should be giving a lot here?
[02:04:33.080 --> 02:04:36.040]   - Well, you know, the Guardian newspaper
[02:04:36.040 --> 02:04:38.600]   has a similar model, which is, they have ads,
[02:04:38.600 --> 02:04:40.600]   but they also, there's no paywall,
[02:04:40.600 --> 02:04:42.840]   but they just encourage people to donate.
[02:04:42.840 --> 02:04:44.120]   And they do that.
[02:04:44.120 --> 02:04:46.660]   Like I've sometimes seen a banner saying,
[02:04:46.660 --> 02:04:51.880]   oh, this is your 134th article you've read this year.
[02:04:51.880 --> 02:04:52.960]   Would you like to donate?
[02:04:52.960 --> 02:04:54.600]   And I think that's, I think it's effective.
[02:04:54.600 --> 02:04:56.080]   I mean, they're testing.
[02:04:56.080 --> 02:04:58.240]   But also I wonder, for some people,
[02:04:58.240 --> 02:05:01.360]   if they just don't feel like guilty and then think,
[02:05:01.360 --> 02:05:03.280]   well, I shouldn't bother them so much.
[02:05:03.280 --> 02:05:04.360]   I don't know.
[02:05:04.360 --> 02:05:05.200]   It's a good question.
[02:05:05.200 --> 02:05:06.360]   I don't know the answer.
[02:05:06.360 --> 02:05:08.000]   - I guess that's the thing I could also turn on,
[02:05:08.000 --> 02:05:09.360]   'cause that would make me happy.
[02:05:09.360 --> 02:05:11.720]   I feel like legitimately there's some sites,
[02:05:11.720 --> 02:05:14.960]   and this speaks to our social media discussion,
[02:05:14.960 --> 02:05:19.720]   Wikipedia unquestionably makes me feel better about myself
[02:05:19.720 --> 02:05:20.840]   if I spend time on it.
[02:05:20.840 --> 02:05:22.900]   Like there's some websites where I'm like,
[02:05:22.900 --> 02:05:27.240]   if I spend time on Twitter, sometimes I'm like, I regret.
[02:05:27.240 --> 02:05:29.080]   There's, I think Elon talks about this,
[02:05:29.080 --> 02:05:31.240]   minimize the number of regretted minutes.
[02:05:31.240 --> 02:05:32.080]   - Yeah.
[02:05:32.080 --> 02:05:36.520]   - My number of regretted minutes on Wikipedia is like zero.
[02:05:36.520 --> 02:05:38.600]   Like I don't remember a time.
[02:05:38.600 --> 02:05:41.620]   I've just discovered this,
[02:05:41.620 --> 02:05:44.720]   I started following on Instagram, a page,
[02:05:44.720 --> 02:05:46.720]   depth of Wikipedia.
[02:05:46.720 --> 02:05:47.560]   - Oh yeah.
[02:05:47.560 --> 02:05:49.320]   - There's like crazy Wikipedia pages.
[02:05:49.320 --> 02:05:51.160]   There's no Wikipedia page that--
[02:05:51.160 --> 02:05:54.460]   - Yeah, I gave her a media contributor of the year award
[02:05:54.460 --> 02:05:55.820]   this year, 'cause she's so great.
[02:05:55.820 --> 02:05:56.660]   - Yeah, she's amazing.
[02:05:56.660 --> 02:05:58.420]   - Depth of Wikipedia is so fun.
[02:05:58.420 --> 02:06:02.180]   - So I, yeah, so that's the kind of interesting point
[02:06:02.180 --> 02:06:05.660]   that I don't even know if there's a competitor.
[02:06:05.660 --> 02:06:08.420]   There may be this sort of programming stack overflow
[02:06:08.420 --> 02:06:10.260]   type of websites, but everything else,
[02:06:10.260 --> 02:06:11.660]   there's always a trade off.
[02:06:11.660 --> 02:06:14.460]   It's probably because of the ad driven model,
[02:06:14.460 --> 02:06:17.780]   because there's an incentive to pull you into clickbait,
[02:06:17.780 --> 02:06:19.420]   and Wikipedia has no clickbait.
[02:06:19.420 --> 02:06:21.560]   It's all about the quality of the knowledge
[02:06:21.560 --> 02:06:22.400]   and the wisdom and so on.
[02:06:22.400 --> 02:06:23.340]   - Yeah, that's right.
[02:06:23.340 --> 02:06:25.980]   And I also like stack over, although I wonder,
[02:06:25.980 --> 02:06:27.580]   I wonder what you think of this.
[02:06:27.580 --> 02:06:31.700]   So I only program for fun as a hobby,
[02:06:31.700 --> 02:06:34.100]   and I don't have enough time to do it, but I do.
[02:06:34.100 --> 02:06:37.360]   And I'm not very good at it, so therefore I end up
[02:06:37.360 --> 02:06:39.140]   on stack overflow quite a lot,
[02:06:39.140 --> 02:06:41.100]   trying to figure out what's gone wrong.
[02:06:41.100 --> 02:06:44.220]   And I have really transitioned to using
[02:06:44.220 --> 02:06:47.540]   ChatterBeeTee much more for that,
[02:06:47.540 --> 02:06:51.280]   because I can often find the answer clearly explained,
[02:06:51.280 --> 02:06:54.740]   and it just, it works better than sifting through threads.
[02:06:54.740 --> 02:06:56.060]   And I kind of feel bad about that,
[02:06:56.060 --> 02:06:58.500]   because I do love stack overflow and their community.
[02:06:58.500 --> 02:07:00.380]   I mean, I'm assuming, I haven't read anything
[02:07:00.380 --> 02:07:01.220]   in the news about it.
[02:07:01.220 --> 02:07:03.620]   I'm assuming they are keenly aware of this,
[02:07:03.620 --> 02:07:07.040]   and they're thinking about how can we sort of use
[02:07:07.040 --> 02:07:10.300]   this chunk of knowledge that we've got here
[02:07:10.300 --> 02:07:12.100]   and provide a new type of interface
[02:07:12.100 --> 02:07:14.480]   where you can query it with a question
[02:07:14.480 --> 02:07:18.280]   and actually get an answer that's based on the answers
[02:07:18.280 --> 02:07:20.040]   that we've had, I don't know.
[02:07:20.040 --> 02:07:22.960]   - And I think stack overflow currently
[02:07:22.960 --> 02:07:26.080]   has policies against using GPT.
[02:07:26.080 --> 02:07:28.120]   Like there's a contentious kind of tension.
[02:07:28.120 --> 02:07:28.960]   - Of course, yeah, yeah, yeah.
[02:07:28.960 --> 02:07:31.280]   - But they're trying to figure that out.
[02:07:31.280 --> 02:07:33.520]   - And so we are similar in that regard.
[02:07:33.520 --> 02:07:35.820]   Like obviously all the things we've talked about,
[02:07:35.820 --> 02:07:37.160]   like ChatterBeeTee makes stuff up,
[02:07:37.160 --> 02:07:38.720]   and it makes up references.
[02:07:38.720 --> 02:07:41.160]   So our community has already put into place
[02:07:41.160 --> 02:07:43.720]   some policies about it, but roughly speaking,
[02:07:43.720 --> 02:07:45.840]   there's always more nuance, but roughly speaking,
[02:07:45.840 --> 02:07:49.380]   it's sort of like you, the human, are responsible
[02:07:49.380 --> 02:07:51.280]   for what you put into Wikipedia.
[02:07:51.280 --> 02:07:54.920]   So if you use ChatterBeeTee, you better check it.
[02:07:54.920 --> 02:07:56.400]   'Cause there's a lot of great use cases of,
[02:07:56.400 --> 02:08:00.200]   you know, like, oh, well, I'm not a native speaker
[02:08:00.200 --> 02:08:02.960]   of German, but I kind of am pretty good.
[02:08:02.960 --> 02:08:03.800]   I'm not talking about myself,
[02:08:03.800 --> 02:08:05.640]   a hypothetical me that's pretty good.
[02:08:05.640 --> 02:08:10.240]   And I kind of just wanna run my edit through ChatterBeeTee
[02:08:10.240 --> 02:08:13.640]   in German to go make sure my grammar's okay.
[02:08:13.640 --> 02:08:14.680]   That's actually cool.
[02:08:14.680 --> 02:08:20.440]   - Does it make you sad that people might use,
[02:08:20.440 --> 02:08:24.060]   increasingly use, ChatGPT for something
[02:08:24.060 --> 02:08:25.640]   where they would previously use Wikipedia?
[02:08:25.640 --> 02:08:29.640]   So basically use it to answer basic questions
[02:08:29.640 --> 02:08:33.820]   about the Eiffel Tower, and where the answer
[02:08:33.820 --> 02:08:36.880]   really comes at the source of it from Wikipedia,
[02:08:36.880 --> 02:08:38.720]   but they're using this as an interface.
[02:08:38.720 --> 02:08:40.600]   - Yeah, no, no, that's completely fine.
[02:08:40.600 --> 02:08:43.520]   I mean, part of it is our ethos has always been,
[02:08:43.520 --> 02:08:45.520]   here's our gift to the world, make something.
[02:08:45.520 --> 02:08:49.140]   So if the knowledge is more accessible to people,
[02:08:49.140 --> 02:08:52.720]   even if they're not coming through us, that's fine.
[02:08:52.720 --> 02:08:55.640]   Now, obviously we do have certain business model concerns,
[02:08:55.640 --> 02:08:57.640]   right, like if, and we've talked,
[02:08:57.640 --> 02:08:59.340]   where we've had more conversation about this,
[02:08:59.340 --> 02:09:02.880]   this whole GPT thing is new, things like,
[02:09:02.880 --> 02:09:07.880]   if you ask Alexa, you know, what is the Eiffel Tower,
[02:09:08.440 --> 02:09:11.000]   and she reads you the first two sentences from Wikipedia
[02:09:11.000 --> 02:09:13.160]   and doesn't say it's from Wikipedia,
[02:09:13.160 --> 02:09:15.460]   and they've recently started citing Wikipedia,
[02:09:15.460 --> 02:09:17.440]   then we worry like, oh, if people don't know
[02:09:17.440 --> 02:09:19.200]   they're getting the knowledge from us,
[02:09:19.200 --> 02:09:21.040]   are they gonna donate money, or are they just saying,
[02:09:21.040 --> 02:09:23.400]   oh, what's Wikipedia for, I can just ask Alexa.
[02:09:23.400 --> 02:09:25.520]   It's like, well, Alexa only knows anything
[02:09:25.520 --> 02:09:26.960]   'cause she read Wikipedia.
[02:09:26.960 --> 02:09:29.320]   So we do think about that, but it doesn't bother me
[02:09:29.320 --> 02:09:31.520]   in the sense of like, oh, I want people
[02:09:31.520 --> 02:09:33.680]   to always come to Wikipedia first.
[02:09:33.680 --> 02:09:36.640]   But we're also, you know, had a great demo,
[02:09:36.640 --> 02:09:38.960]   like literally just hacked together over a weekend
[02:09:38.960 --> 02:09:41.100]   by our head of machine learning,
[02:09:41.100 --> 02:09:44.040]   where he did this little thing to say,
[02:09:44.040 --> 02:09:47.080]   you could ask any question,
[02:09:47.080 --> 02:09:48.520]   and he was just knocking it together,
[02:09:48.520 --> 02:09:53.520]   so he used the OpenAI's API just to make a demo,
[02:09:53.520 --> 02:09:58.360]   ask a question, why do ducks fly south for winter?
[02:09:58.360 --> 02:09:59.620]   Just the kind of thing you think,
[02:09:59.620 --> 02:10:03.080]   oh, I might just Google for that,
[02:10:03.080 --> 02:10:05.800]   or I might start looking in Wikipedia, I don't know.
[02:10:05.800 --> 02:10:08.240]   And so what he does, he asks Chachapiti,
[02:10:08.240 --> 02:10:10.880]   what are some Wikipedia entries that might answer this?
[02:10:10.880 --> 02:10:13.600]   Then he grabbed those Wikipedia entries,
[02:10:13.600 --> 02:10:15.200]   said, here's some Wikipedia entries,
[02:10:15.200 --> 02:10:18.440]   answer this question based only on the information in this.
[02:10:18.440 --> 02:10:19.760]   And he had pretty good results,
[02:10:19.760 --> 02:10:21.840]   and it kind of prevented them making stuff up.
[02:10:21.840 --> 02:10:23.880]   It's just he hacked together over the weekend,
[02:10:23.880 --> 02:10:27.240]   but what it made me think about was,
[02:10:27.240 --> 02:10:30.720]   oh, okay, so now we've got this huge body of knowledge
[02:10:30.720 --> 02:10:34.920]   that in many cases, you're like, oh, I'm really,
[02:10:34.920 --> 02:10:37.000]   I wanna know about Queen Victoria,
[02:10:37.000 --> 02:10:38.520]   I'm just gonna go read the Wikipedia entry,
[02:10:38.520 --> 02:10:43.520]   and it's gonna take me through her life and so forth.
[02:10:43.520 --> 02:10:45.640]   But other times you've got a specific question,
[02:10:45.640 --> 02:10:48.280]   and maybe we could have a better search experience
[02:10:48.280 --> 02:10:50.280]   where you can come to Wikipedia,
[02:10:50.280 --> 02:10:51.760]   ask your specific question,
[02:10:51.760 --> 02:10:54.200]   get your specific answer that's from Wikipedia,
[02:10:54.200 --> 02:10:57.800]   including links to the articles you might wanna read next.
[02:10:57.800 --> 02:10:59.040]   And that's just a step forward,
[02:10:59.040 --> 02:11:01.560]   like that's just using a new type of technology
[02:11:01.560 --> 02:11:05.880]   to make the extraction of information from this body of text
[02:11:05.880 --> 02:11:08.760]   into my brain faster and easier.
[02:11:08.760 --> 02:11:10.720]   So I think that's kind of cool.
[02:11:10.720 --> 02:11:15.140]   - I would love to see a Chad GPT grounding
[02:11:15.140 --> 02:11:17.440]   into websites like Wikipedia,
[02:11:17.440 --> 02:11:19.880]   and the other comparable website to me
[02:11:19.880 --> 02:11:21.920]   will be like Wolfram Alpha
[02:11:21.920 --> 02:11:24.480]   for more mathematical knowledge, that kind of stuff.
[02:11:24.480 --> 02:11:27.320]   So grounding, like taking you to a page
[02:11:27.320 --> 02:11:30.160]   that is really crafted, as opposed to,
[02:11:30.160 --> 02:11:32.320]   like the moment you start actually taking you
[02:11:32.320 --> 02:11:36.640]   to like journalist websites, like news websites,
[02:11:36.640 --> 02:11:38.080]   starts getting a little iffy.
[02:11:38.080 --> 02:11:39.240]   - Yeah, yeah, yeah. - It's getting a little--
[02:11:39.240 --> 02:11:40.520]   - Yeah, yeah, yeah.
[02:11:40.520 --> 02:11:42.600]   - 'Cause they have, you're now in a land
[02:11:42.600 --> 02:11:44.240]   that has a wrong incentive.
[02:11:44.240 --> 02:11:45.720]   - Right, yeah. - You pulled in--
[02:11:45.720 --> 02:11:48.080]   - And you need somebody to have filtered through that
[02:11:48.080 --> 02:11:50.640]   and sort of tried to knock off the rough edges, yeah.
[02:11:50.640 --> 02:11:53.480]   No, it's very, I think that's exactly right.
[02:11:53.480 --> 02:11:58.480]   And I think, you know, I think that kind of grounding
[02:12:00.120 --> 02:12:02.480]   is, I think they're working really hard on it.
[02:12:02.480 --> 02:12:03.640]   I think that's really important.
[02:12:03.640 --> 02:12:06.800]   And that actually, when I, so if you ask me to step back
[02:12:06.800 --> 02:12:09.680]   and be like very business-like about our business model
[02:12:09.680 --> 02:12:11.560]   and where's it gonna go for us,
[02:12:11.560 --> 02:12:13.760]   and are we gonna lose half our donations
[02:12:13.760 --> 02:12:15.400]   'cause everybody's just gonna stop coming to Wikipedia
[02:12:15.400 --> 02:12:18.800]   and go to Chad GPT, I think grounding will help a lot
[02:12:18.800 --> 02:12:22.160]   because frankly, most questions people have,
[02:12:22.160 --> 02:12:23.840]   if they provide proper links,
[02:12:23.840 --> 02:12:24.840]   we're gonna be at the top of that
[02:12:24.840 --> 02:12:26.320]   just like we are in Google.
[02:12:26.320 --> 02:12:28.080]   So we're still gonna get tons of recognition
[02:12:28.080 --> 02:12:30.240]   and tons of traffic just from,
[02:12:30.240 --> 02:12:35.000]   even if it's just the moral properness of saying,
[02:12:35.000 --> 02:12:36.800]   here's my source.
[02:12:36.800 --> 02:12:39.160]   So I think we're gonna be all right in that.
[02:12:39.160 --> 02:12:40.640]   - Yeah, and the close partnership of,
[02:12:40.640 --> 02:12:43.760]   if the model is fine-tuned, is constantly retrained,
[02:12:43.760 --> 02:12:45.640]   then Wikipedia is one of the primary places
[02:12:45.640 --> 02:12:48.600]   where if you want to change what the model knows,
[02:12:48.600 --> 02:12:52.000]   one of the things you should do is contribute to Wikipedia
[02:12:52.000 --> 02:12:54.120]   or clarify Wikipedia. - Yeah, yeah, yeah.
[02:12:54.120 --> 02:12:57.240]   - Or elaborate, expand, all that kind of stuff.
[02:12:57.240 --> 02:12:58.880]   - You mentioned all of us have controversies,
[02:12:58.880 --> 02:12:59.880]   I have to ask.
[02:12:59.880 --> 02:13:02.320]   Do you find the controversy
[02:13:02.320 --> 02:13:04.080]   of whether you are the sole founder
[02:13:04.080 --> 02:13:07.960]   or the co-founder of Wikipedia ironic,
[02:13:07.960 --> 02:13:10.540]   absurd, interesting, important?
[02:13:10.540 --> 02:13:13.080]   What are your comments?
[02:13:13.080 --> 02:13:16.560]   - I would say unimportant, not that interesting.
[02:13:16.560 --> 02:13:19.840]   I mean, one of the things that people
[02:13:19.840 --> 02:13:21.520]   are sometimes surprised to hear me say
[02:13:21.520 --> 02:13:25.120]   is I actually think Larry Sanger doesn't get enough credit
[02:13:25.120 --> 02:13:27.120]   for his early work in Wikipedia
[02:13:27.120 --> 02:13:28.280]   even though I think co-founder's
[02:13:28.280 --> 02:13:30.240]   not the right title for that.
[02:13:30.240 --> 02:13:33.480]   So, you know, like he had a lot of impact
[02:13:33.480 --> 02:13:35.120]   and a lot of great work,
[02:13:35.120 --> 02:13:37.080]   and I disagree with him about a lot of things since
[02:13:37.080 --> 02:13:38.600]   and all that, and that's fine.
[02:13:38.600 --> 02:13:41.200]   So yeah, no, to me, that's like,
[02:13:41.200 --> 02:13:44.640]   it's one of these things that the media love
[02:13:44.640 --> 02:13:46.720]   a falling out story,
[02:13:46.720 --> 02:13:48.440]   so they wanna make a big deal out of it,
[02:13:48.440 --> 02:13:51.040]   and I'm just like, yeah, no.
[02:13:51.040 --> 02:13:53.520]   - So there's a lot of interesting engineering contributions
[02:13:53.520 --> 02:13:54.920]   in the early days, like you were saying,
[02:13:54.920 --> 02:13:57.000]   there's debates about how to structure it,
[02:13:57.000 --> 02:13:59.360]   what the heck is this thing that we're doing,
[02:13:59.360 --> 02:14:02.200]   and there's important people that contributed to that.
[02:14:02.200 --> 02:14:03.440]   - Yeah, definitely.
[02:14:03.440 --> 02:14:06.440]   - So he also, you said you had some disagreements,
[02:14:06.440 --> 02:14:09.880]   Larry Sanger said that nobody should trust Wikipedia,
[02:14:09.880 --> 02:14:11.600]   and that Wikipedia seems to assume
[02:14:11.600 --> 02:14:13.800]   that there's only one legitimate defensible version
[02:14:13.800 --> 02:14:16.480]   of the truth on any controversial question.
[02:14:16.480 --> 02:14:18.800]   That's not how Wikipedia used to be.
[02:14:18.800 --> 02:14:21.160]   I presume you disagree with that analysis.
[02:14:21.160 --> 02:14:22.800]   - I mean, just straight up, I disagree.
[02:14:22.800 --> 02:14:24.840]   Like, go and read any Wikipedia entry
[02:14:24.840 --> 02:14:26.000]   on a controversial topic,
[02:14:26.000 --> 02:14:28.920]   and what you'll see is a really diligent effort
[02:14:28.920 --> 02:14:30.720]   to explain all the relevant sides,
[02:14:30.720 --> 02:14:32.280]   so yeah, just disagree.
[02:14:32.280 --> 02:14:33.920]   - So on controversial questions,
[02:14:33.920 --> 02:14:36.240]   you think perspectives are generally represented?
[02:14:36.240 --> 02:14:39.240]   I mean, it has to do with the kind of the tension
[02:14:39.240 --> 02:14:42.400]   between the mainstream and the non-mainstream
[02:14:42.400 --> 02:14:43.520]   that we were talking about.
[02:14:43.520 --> 02:14:45.240]   - Yeah, no, I mean, for sure.
[02:14:45.240 --> 02:14:51.120]   Like, to take this area of discussion seriously
[02:14:51.120 --> 02:14:54.120]   is to say, yeah, you know what,
[02:14:54.120 --> 02:14:55.800]   actually, that is a big part
[02:14:55.800 --> 02:14:58.800]   of what Wikipedians spend their time grappling with,
[02:14:58.800 --> 02:15:03.640]   is to say, you know, how do we figure out
[02:15:03.640 --> 02:15:09.980]   whether a less popular view is pseudoscience?
[02:15:09.980 --> 02:15:13.560]   Is it just a less popular view
[02:15:13.560 --> 02:15:16.080]   that's gaining acceptance in the mainstream?
[02:15:16.080 --> 02:15:19.640]   Is it fringe versus crackpot, et cetera, et cetera?
[02:15:19.640 --> 02:15:22.220]   And that debate is what you've gotta do.
[02:15:22.220 --> 02:15:24.160]   There's no choice about having that debate,
[02:15:24.160 --> 02:15:27.800]   of grappling with something, and I think we do,
[02:15:27.800 --> 02:15:28.960]   and I think that's really important,
[02:15:28.960 --> 02:15:33.520]   and I think if anybody said to the Wikipedia community,
[02:15:33.520 --> 02:15:35.040]   gee, you should stop, you know,
[02:15:35.040 --> 02:15:39.280]   sort of covering minority viewpoints on this issue,
[02:15:39.280 --> 02:15:40.400]   I think they would say,
[02:15:40.400 --> 02:15:42.360]   I don't even understand why you would say that.
[02:15:42.360 --> 02:15:45.400]   Like, we have to sort of grapple with minority viewpoints
[02:15:45.400 --> 02:15:47.540]   in science and politics and so on,
[02:15:49.580 --> 02:15:52.780]   and this is one of the reasons why, you know,
[02:15:52.780 --> 02:15:56.340]   there is no magic simple answer to all these things.
[02:15:56.340 --> 02:16:00.860]   It's really contextual.
[02:16:00.860 --> 02:16:01.780]   It's case by case.
[02:16:01.780 --> 02:16:03.460]   It's like, you know, you've gotta really say,
[02:16:03.460 --> 02:16:05.260]   okay, what is the context here?
[02:16:05.260 --> 02:16:06.100]   How do you do it?
[02:16:06.100 --> 02:16:08.300]   And you've always gotta be open to correction
[02:16:08.300 --> 02:16:10.860]   and to change and to sort of challenge
[02:16:10.860 --> 02:16:13.100]   and always be sort of serious about that.
[02:16:13.100 --> 02:16:15.140]   - I think what happens, again, with social media
[02:16:15.140 --> 02:16:19.080]   is when there is that grappling process in Wikipedia
[02:16:19.080 --> 02:16:21.680]   and a decision is made to remove a paragraph
[02:16:21.680 --> 02:16:24.320]   or to remove a thing or to say a thing,
[02:16:24.320 --> 02:16:27.640]   you're gonna notice the one direction
[02:16:27.640 --> 02:16:30.840]   of the oscillation of the grappling and not the correction,
[02:16:30.840 --> 02:16:32.200]   and you've gotta highlight that and say,
[02:16:32.200 --> 02:16:36.520]   how come this person, I don't know,
[02:16:36.520 --> 02:16:38.760]   maybe legitimacy of elections,
[02:16:38.760 --> 02:16:40.160]   that's the thing that comes up.
[02:16:40.160 --> 02:16:42.640]   Donald Trump, maybe previous stuff.
[02:16:42.640 --> 02:16:44.120]   - I can give a really good example,
[02:16:44.120 --> 02:16:47.720]   which is there was this sort of dust up
[02:16:47.720 --> 02:16:52.720]   about the definition of recession in Wikipedia.
[02:16:52.720 --> 02:16:55.200]   So the accusation was,
[02:16:55.200 --> 02:16:58.240]   and the accusation was often quite ridiculous and extreme,
[02:16:58.240 --> 02:17:01.440]   which is under pressure from the Biden administration,
[02:17:01.440 --> 02:17:04.160]   Wikipedia changed the definition of recession
[02:17:04.160 --> 02:17:05.720]   to make Biden look good.
[02:17:05.720 --> 02:17:07.360]   Or we did it not under pressure,
[02:17:07.360 --> 02:17:11.400]   but because we're a bunch of lunatic leftists and so on.
[02:17:11.400 --> 02:17:13.400]   And then, you know, when I see something like that
[02:17:13.400 --> 02:17:15.560]   in the press, I'm like, oh dear, what's happened here?
[02:17:15.560 --> 02:17:16.460]   How did we do that?
[02:17:16.460 --> 02:17:19.480]   'Cause I always just accept things for five seconds first.
[02:17:19.480 --> 02:17:20.880]   And then I go and I look and I'm like, you know what?
[02:17:20.880 --> 02:17:23.880]   That's literally completely not what happened.
[02:17:23.880 --> 02:17:26.880]   What happened was one editor
[02:17:26.880 --> 02:17:28.680]   thought the article needed restructuring.
[02:17:28.680 --> 02:17:29.620]   So the article has always said,
[02:17:29.620 --> 02:17:33.040]   so the traditional kind of loose definition of recession
[02:17:33.040 --> 02:17:35.900]   is two quarters of negative growth.
[02:17:35.900 --> 02:17:38.600]   But there's always been, within economics,
[02:17:38.600 --> 02:17:41.160]   within important agencies in different countries
[02:17:41.160 --> 02:17:43.280]   around the world, a lot of nuance around that.
[02:17:43.280 --> 02:17:45.880]   And there's other like factors that go into it and so forth.
[02:17:45.880 --> 02:17:49.040]   And it's just an interesting, complicated topic.
[02:17:49.040 --> 02:17:51.120]   And so the article has always had
[02:17:51.120 --> 02:17:53.120]   the definition of two quarters.
[02:17:53.120 --> 02:17:54.600]   And the only thing that really changed
[02:17:54.600 --> 02:17:56.960]   was moving that from the lead,
[02:17:56.960 --> 02:17:59.720]   from the top paragraph to further down.
[02:17:59.720 --> 02:18:02.400]   And then news stories appeared saying,
[02:18:02.400 --> 02:18:04.840]   Wikipedia has changed the definition of recession.
[02:18:04.840 --> 02:18:07.040]   And then we got a huge rush of trolls coming in.
[02:18:07.040 --> 02:18:08.840]   So the article was temporarily protected,
[02:18:08.840 --> 02:18:10.720]   I think only semi-protected,
[02:18:10.720 --> 02:18:13.120]   and people were told go to the talk page to discuss.
[02:18:13.120 --> 02:18:15.400]   - So it was a dust up that was,
[02:18:15.400 --> 02:18:16.720]   when you look at it as a Wikipedia,
[02:18:16.720 --> 02:18:17.560]   and you're like, oh,
[02:18:17.560 --> 02:18:21.520]   like this is a really routine kind of editorial debate.
[02:18:21.520 --> 02:18:23.280]   Another example, which unfortunately,
[02:18:23.280 --> 02:18:25.880]   our friend Elon fell for, I would say,
[02:18:25.880 --> 02:18:28.160]   is the Twitter files.
[02:18:28.160 --> 02:18:30.760]   So there was an article called "The Twitter Files,"
[02:18:30.760 --> 02:18:32.520]   which is about these files that were released
[02:18:32.520 --> 02:18:34.440]   once Elon took control of Twitter
[02:18:34.440 --> 02:18:36.560]   and he released internal documents.
[02:18:36.560 --> 02:18:41.560]   And what happened was somebody nominated it for deletion.
[02:18:42.200 --> 02:18:44.400]   But even the nomination said,
[02:18:44.400 --> 02:18:46.040]   this is actually,
[02:18:46.040 --> 02:18:49.440]   this is mainly about the Hunter Biden laptop controversy.
[02:18:49.440 --> 02:18:52.040]   Shouldn't this information be there instead?
[02:18:52.040 --> 02:18:53.440]   So anyone can, like,
[02:18:53.440 --> 02:18:55.880]   it takes exactly one human being anywhere on the planet
[02:18:55.880 --> 02:18:57.800]   to propose something for deletion.
[02:18:57.800 --> 02:19:00.200]   And that triggers a process where people discuss it,
[02:19:00.200 --> 02:19:01.880]   which within a few hours,
[02:19:01.880 --> 02:19:03.840]   it was what we call snowball closed,
[02:19:03.840 --> 02:19:06.120]   i.e. this doesn't have a snowball's chance
[02:19:06.120 --> 02:19:08.240]   in hell of passing.
[02:19:08.240 --> 02:19:11.200]   So an admin goes, yeah, wrong.
[02:19:11.200 --> 02:19:12.880]   And closed the debate and that was it.
[02:19:12.880 --> 02:19:15.600]   That was the whole thing that happened.
[02:19:15.600 --> 02:19:18.400]   And so nobody proposed suppressing the information.
[02:19:18.400 --> 02:19:19.920]   Nobody proposed it wasn't important.
[02:19:19.920 --> 02:19:23.960]   It was just like editorially boring internal questions.
[02:19:23.960 --> 02:19:26.680]   And, you know, so sometimes people read stuff like that
[02:19:26.680 --> 02:19:28.280]   and they're like, oh, you see?
[02:19:28.280 --> 02:19:29.200]   Look at these leftists.
[02:19:29.200 --> 02:19:31.120]   They're trying to suppress the truth again.
[02:19:31.120 --> 02:19:33.000]   It's like, well, slow down a second and come and look.
[02:19:33.000 --> 02:19:35.000]   Like, literally, it's not what happened.
[02:19:35.000 --> 02:19:39.640]   - Yeah, so I think the right is more sensitive to censorship.
[02:19:40.880 --> 02:19:44.960]   And so they will more likely highlight,
[02:19:44.960 --> 02:19:49.720]   there's more virality to highlighting something
[02:19:49.720 --> 02:19:52.240]   that looks like censorship in any walks of life.
[02:19:52.240 --> 02:19:54.640]   And this moving a paragraph from one place to another
[02:19:54.640 --> 02:19:57.520]   or removing it and so on as part of the regular grappling
[02:19:57.520 --> 02:20:00.680]   of Wikipedia can make a hell of a good article
[02:20:00.680 --> 02:20:01.520]   or YouTube video.
[02:20:01.520 --> 02:20:02.360]   - Yeah, yeah, yeah.
[02:20:02.360 --> 02:20:05.600]   No, it sounds really enticing and intriguing
[02:20:05.600 --> 02:20:08.040]   and surprising to most people because they're like,
[02:20:08.040 --> 02:20:10.080]   oh, no, I'm reading Wikipedia.
[02:20:10.080 --> 02:20:12.560]   It doesn't seem like a crackpot leftist website.
[02:20:12.560 --> 02:20:16.880]   It seems pretty kind of dull, really, in its own geeky way.
[02:20:16.880 --> 02:20:17.720]   - Well, that's how I--
[02:20:17.720 --> 02:20:18.680]   - So that makes a good story.
[02:20:18.680 --> 02:20:20.240]   It's like, oh, am I being misled
[02:20:20.240 --> 02:20:24.760]   because there's a shadowy cabal of Jimmy Wales?
[02:20:24.760 --> 02:20:27.120]   - You know, I generally, I read political stuff.
[02:20:27.120 --> 02:20:29.840]   I mentioned to you that I'm traveling
[02:20:29.840 --> 02:20:33.760]   to have some very difficult conversation
[02:20:33.760 --> 02:20:36.560]   with high profile figures, both in the war in Ukraine
[02:20:36.560 --> 02:20:39.000]   and in Israel and Palestine.
[02:20:39.000 --> 02:20:43.080]   And I read the Wikipedia articles around that.
[02:20:43.080 --> 02:20:46.400]   And I also read books on the conflict
[02:20:46.400 --> 02:20:48.480]   and the history of the different regions.
[02:20:48.480 --> 02:20:51.680]   And I find the Wikipedia articles to be very balanced
[02:20:51.680 --> 02:20:53.960]   and there's many perspectives being represented.
[02:20:53.960 --> 02:20:55.180]   But then I ask myself, well,
[02:20:55.180 --> 02:20:58.400]   am I one of them leftist crackpots?
[02:20:58.400 --> 02:20:59.640]   They can't see the truth.
[02:20:59.640 --> 02:21:02.080]   I mean, it's something I ask myself all the time.
[02:21:02.080 --> 02:21:03.960]   Forget the leftists, just crackpot.
[02:21:03.960 --> 02:21:08.960]   Am I just being a sheep in accepting it?
[02:21:08.960 --> 02:21:10.720]   - I think that's an important question to always ask,
[02:21:10.720 --> 02:21:12.120]   but not too much.
[02:21:12.120 --> 02:21:13.440]   - Yeah, no, I agree completely.
[02:21:13.440 --> 02:21:15.080]   - A little bit, but not too much.
[02:21:15.080 --> 02:21:17.400]   - No, I think we always have to challenge ourselves
[02:21:17.400 --> 02:21:20.640]   of like, what do I potentially have wrong?
[02:21:20.640 --> 02:21:23.040]   - Well, you mentioned pressure from government.
[02:21:23.040 --> 02:21:28.680]   You've criticized Twitter for allowing,
[02:21:28.680 --> 02:21:33.100]   giving in to Turkey's government censorship.
[02:21:33.100 --> 02:21:36.340]   There's also conspiracy theories or accusations
[02:21:36.340 --> 02:21:41.040]   of Wikipedia being open to pressure
[02:21:41.040 --> 02:21:43.080]   from government or government organizations,
[02:21:43.080 --> 02:21:45.560]   FBI and all this kind of stuff.
[02:21:45.560 --> 02:21:48.640]   What is the philosophy about pressure
[02:21:48.640 --> 02:21:50.320]   from government and censorship?
[02:21:50.320 --> 02:21:52.920]   - So we're super hardcore on this.
[02:21:52.920 --> 02:21:56.080]   We've never bowed down to government pressure
[02:21:56.080 --> 02:21:58.080]   anywhere in the world and we never will.
[02:21:58.080 --> 02:22:02.080]   And we understand that we're hardcore.
[02:22:02.080 --> 02:22:03.680]   And actually there is a bit of nuance
[02:22:03.680 --> 02:22:06.800]   about how different companies respond to this,
[02:22:06.800 --> 02:22:10.320]   but our response has always been just to say no.
[02:22:10.320 --> 02:22:13.000]   And if they threaten to block, well, knock yourself out,
[02:22:13.000 --> 02:22:14.400]   you're gonna lose Wikipedia.
[02:22:14.400 --> 02:22:17.160]   And that's been very successful for us as a strategy
[02:22:17.160 --> 02:22:22.120]   because governments know they can't just casually threaten
[02:22:22.120 --> 02:22:24.280]   to block Wikipedia or block us for two days
[02:22:24.280 --> 02:22:25.600]   and we're gonna cave in immediately
[02:22:25.600 --> 02:22:27.680]   to get back into the market.
[02:22:27.680 --> 02:22:29.440]   And that's what a lot of companies have done.
[02:22:29.440 --> 02:22:31.400]   And I don't think that's good.
[02:22:31.400 --> 02:22:34.280]   We can go one level deeper and say,
[02:22:34.280 --> 02:22:35.720]   I'm actually quite sympathetic.
[02:22:35.720 --> 02:22:39.320]   If you have staff members in a certain country
[02:22:39.320 --> 02:22:41.320]   and they are at physical risk,
[02:22:41.320 --> 02:22:43.180]   you've gotta put that into your equation.
[02:22:43.180 --> 02:22:44.180]   So I understand that.
[02:22:44.180 --> 02:22:47.760]   Like if Elon said, actually I've got 100 staff members
[02:22:47.760 --> 02:22:50.200]   on the ground in such and such a country,
[02:22:50.200 --> 02:22:53.440]   and if we don't comply, somebody's gonna get arrested
[02:22:53.440 --> 02:22:54.940]   and it could be quite serious.
[02:22:54.940 --> 02:22:57.120]   Okay, that's a tough one, right?
[02:22:57.120 --> 02:22:58.900]   That's actually really hard.
[02:23:00.920 --> 02:23:04.400]   But yeah, no, and then the FBI one, no, no.
[02:23:04.400 --> 02:23:06.640]   We, like the criticism I saw,
[02:23:06.640 --> 02:23:09.240]   I kind of prepared for this 'cause I saw people responding
[02:23:09.240 --> 02:23:12.340]   to your requests for questions and I was like,
[02:23:12.340 --> 02:23:14.580]   somebody's like, oh, well, don't you think it was really bad
[02:23:14.580 --> 02:23:15.920]   that you da, da, da, da, da?
[02:23:15.920 --> 02:23:17.800]   And I said, I actually reached out to staff saying,
[02:23:17.800 --> 02:23:20.240]   can you just make sure I've got my facts right?
[02:23:20.240 --> 02:23:25.240]   And the answer is we received zero requests of any kind
[02:23:25.240 --> 02:23:28.400]   from the FBI or any of the other government agencies
[02:23:28.400 --> 02:23:31.280]   for any changes to content in Wikipedia.
[02:23:31.280 --> 02:23:33.320]   And had we received those requests
[02:23:33.320 --> 02:23:35.040]   at the level of the Wikimedia Foundation,
[02:23:35.040 --> 02:23:37.320]   we would have said, it's not our,
[02:23:37.320 --> 02:23:38.360]   like we can't do anything
[02:23:38.360 --> 02:23:40.800]   because Wikipedia is written by the community.
[02:23:40.800 --> 02:23:42.840]   And so the Wikimedia Foundation
[02:23:42.840 --> 02:23:45.520]   can't change the content of Wikipedia without causing,
[02:23:45.520 --> 02:23:47.360]   I mean, God, that would be a massive controversy
[02:23:47.360 --> 02:23:48.560]   you can't even imagine.
[02:23:48.560 --> 02:23:51.720]   What we did do, and this is what I've done,
[02:23:51.720 --> 02:23:55.600]   I've been to China and met with the Minister of Propaganda.
[02:23:56.440 --> 02:24:00.000]   We've had discussions with governments all around the world,
[02:24:00.000 --> 02:24:02.480]   not because we wanna do their bidding,
[02:24:02.480 --> 02:24:04.040]   but because we don't wanna do their bidding,
[02:24:04.040 --> 02:24:05.400]   but we also don't wanna be blocked.
[02:24:05.400 --> 02:24:07.400]   And we think actually having these conversations
[02:24:07.400 --> 02:24:08.240]   are really important.
[02:24:08.240 --> 02:24:10.240]   Now, there's no threat of being blocked in the US,
[02:24:10.240 --> 02:24:11.340]   like that's just never gonna happen.
[02:24:11.340 --> 02:24:12.840]   There is the First Amendment.
[02:24:12.840 --> 02:24:15.680]   But in other countries around the world, it's like, okay,
[02:24:15.680 --> 02:24:16.760]   what are you upset about?
[02:24:16.760 --> 02:24:19.400]   Let's have the conversation, like let's understand,
[02:24:19.400 --> 02:24:21.320]   and let's have a dialogue about it
[02:24:21.320 --> 02:24:24.120]   so that you can understand where we come from
[02:24:24.120 --> 02:24:26.440]   and what we're doing and why.
[02:24:26.440 --> 02:24:30.560]   And then sometimes it's like, gee,
[02:24:30.560 --> 02:24:34.280]   if somebody complains that something's bad in Wikipedia,
[02:24:34.280 --> 02:24:36.800]   whoever they are, don't care who they are.
[02:24:36.800 --> 02:24:40.620]   Could be you, could be the government, could be the Pope,
[02:24:40.620 --> 02:24:41.840]   I don't care who they are.
[02:24:41.840 --> 02:24:43.920]   It's like, oh, okay, well, our responsibility as Wikipedia
[02:24:43.920 --> 02:24:46.240]   is to go, oh, hold on, let's check, right?
[02:24:46.240 --> 02:24:47.120]   Is that right or wrong?
[02:24:47.120 --> 02:24:49.460]   Is there something that we've got wrong in Wikipedia?
[02:24:49.460 --> 02:24:50.920]   Not because you're threatening to block us,
[02:24:50.920 --> 02:24:53.760]   but because we want Wikipedia to be correct.
[02:24:53.760 --> 02:24:55.920]   So we do have these dialogues with people.
[02:24:55.920 --> 02:25:00.920]   And a big part of what was going on with,
[02:25:00.920 --> 02:25:06.280]   you might call it pressure on social media companies
[02:25:06.280 --> 02:25:09.280]   or dialogue with, depending on, as we talked earlier,
[02:25:09.280 --> 02:25:12.180]   grapple with the language, depending on what your view is.
[02:25:12.180 --> 02:25:17.200]   In our case, it was really just about, oh, okay, right,
[02:25:17.200 --> 02:25:21.040]   they wanna have a dialogue about COVID information,
[02:25:21.040 --> 02:25:22.720]   misinformation.
[02:25:22.720 --> 02:25:25.400]   We're this enormous source of information
[02:25:25.400 --> 02:25:27.600]   which the world depends on.
[02:25:27.600 --> 02:25:29.440]   We're gonna have that conversation, right?
[02:25:29.440 --> 02:25:32.280]   We're happy to say, here's, if they say,
[02:25:32.280 --> 02:25:36.360]   how do you know that Wikipedia is not gonna be pushing
[02:25:36.360 --> 02:25:38.740]   some crazy anti-vax narrative?
[02:25:38.740 --> 02:25:42.320]   First, I mean, I think it's somewhat inappropriate
[02:25:42.320 --> 02:25:45.340]   for a government to be asking pointed questions
[02:25:45.340 --> 02:25:48.340]   in a way that implies possible penalties.
[02:25:48.340 --> 02:25:51.200]   I'm not sure that ever happened because we would just go,
[02:25:51.200 --> 02:25:56.200]   I don't know, the Chinese blocked us and so it goes, right?
[02:25:56.200 --> 02:25:58.680]   We're not gonna cave in to any kind of government pressure,
[02:25:58.680 --> 02:26:03.480]   but whatever the appropriateness of what they were doing,
[02:26:03.480 --> 02:26:06.320]   I think there is a role for government in just saying,
[02:26:06.320 --> 02:26:09.040]   let's understand the information ecosystem.
[02:26:09.040 --> 02:26:11.400]   Let's think about the problem of misinformation,
[02:26:11.400 --> 02:26:13.240]   disinformation in society,
[02:26:13.240 --> 02:26:15.540]   particularly around election security,
[02:26:15.540 --> 02:26:17.920]   all these kinds of things.
[02:26:17.920 --> 02:26:21.720]   So, I think it would be irresponsible of us
[02:26:21.720 --> 02:26:25.560]   to get a call from a government agency and say,
[02:26:25.560 --> 02:26:28.800]   yeah, why don't you just fuck off, you're the government.
[02:26:28.800 --> 02:26:30.360]   But it would also be irresponsible to go,
[02:26:30.360 --> 02:26:32.540]   oh dear, government agent's not happy,
[02:26:32.540 --> 02:26:35.560]   let's fix Wikipedia so the FBI loves us.
[02:26:35.560 --> 02:26:37.680]   - When you say you wanna have discussions
[02:26:37.680 --> 02:26:40.520]   with the Chinese government or with organizations
[02:26:40.520 --> 02:26:44.040]   like CDC and WHO, it's to thoroughly understand
[02:26:44.040 --> 02:26:46.000]   what the mainstream narrative is
[02:26:46.000 --> 02:26:48.080]   so that it can be properly represented
[02:26:48.080 --> 02:26:50.760]   but not drive what the articles are.
[02:26:50.760 --> 02:26:53.520]   - Well, it's actually important to say,
[02:26:53.520 --> 02:26:57.040]   like whatever the Wikimedia Foundation thinks
[02:26:57.040 --> 02:27:00.040]   has no impact on what's in Wikipedia.
[02:27:00.040 --> 02:27:03.360]   So, it's more about saying to them,
[02:27:03.360 --> 02:27:06.400]   right, we understand you're the World Health Organization
[02:27:06.400 --> 02:27:08.640]   or you're whoever and part of your job
[02:27:08.640 --> 02:27:13.120]   is to sort of public health, it's about communications.
[02:27:13.120 --> 02:27:14.600]   You wanna understand the world.
[02:27:14.600 --> 02:27:16.120]   So, it's more about, oh, well,
[02:27:16.120 --> 02:27:17.960]   let's explain how Wikipedia works.
[02:27:17.960 --> 02:27:19.840]   - So, it's more about explaining how Wikipedia works
[02:27:19.840 --> 02:27:22.320]   and like, hey, it's the volunteers.
[02:27:22.320 --> 02:27:23.560]   - Yeah, yeah, exactly.
[02:27:23.560 --> 02:27:28.560]   - It's a battle of ideas and here's how the sources are used.
[02:27:28.560 --> 02:27:30.320]   - Yeah, exactly.
[02:27:30.320 --> 02:27:31.160]   - What are legitimate sources
[02:27:31.160 --> 02:27:32.480]   and what are not legitimate sources.
[02:27:32.480 --> 02:27:33.320]   - Yeah, exactly.
[02:27:33.320 --> 02:27:35.000]   - I mean, I suppose there's some battle
[02:27:35.000 --> 02:27:36.920]   about what is a legitimate source.
[02:27:36.920 --> 02:27:39.680]   There could be statements made that CDC,
[02:27:39.680 --> 02:27:44.680]   I mean, like there's a government organizations
[02:27:44.680 --> 02:27:48.640]   in general have sold themselves to be the place
[02:27:48.640 --> 02:27:50.840]   where you go for expertise.
[02:27:50.840 --> 02:27:55.160]   And some of that has been to a small degree
[02:27:55.160 --> 02:27:57.680]   raised in question over the response to the pandemic.
[02:27:57.680 --> 02:28:00.360]   - Well, I think in many cases,
[02:28:00.360 --> 02:28:03.920]   and this goes back to my topic of trust.
[02:28:03.920 --> 02:28:08.560]   So, there were definitely cases of public officials,
[02:28:08.560 --> 02:28:12.400]   public organizations, where I felt like
[02:28:12.400 --> 02:28:14.400]   they lost the trust of the public
[02:28:14.400 --> 02:28:16.920]   because they didn't trust the public.
[02:28:16.920 --> 02:28:19.960]   And so, the idea is like, we really need people
[02:28:19.960 --> 02:28:21.920]   to take this seriously and take actions.
[02:28:21.920 --> 02:28:26.600]   Therefore, we're gonna put out some overblown claims
[02:28:26.600 --> 02:28:29.400]   because it's gonna scare people into behaving correctly.
[02:28:29.400 --> 02:28:32.600]   You know what, that might work for a little while,
[02:28:32.600 --> 02:28:34.120]   but it doesn't work in the long run
[02:28:34.120 --> 02:28:37.080]   because suddenly people go from a default stance
[02:28:37.080 --> 02:28:40.400]   of like the Center for Disease Control,
[02:28:40.400 --> 02:28:43.480]   very well-respected scientific organization,
[02:28:43.480 --> 02:28:47.240]   sort of, I don't know, they've got a vault in Atlanta
[02:28:47.240 --> 02:28:49.680]   with the last vial of smallpox or whatever it is
[02:28:49.680 --> 02:28:52.440]   that people think about them, and to go,
[02:28:52.440 --> 02:28:53.600]   oh, right, these are scientists
[02:28:53.600 --> 02:28:55.520]   we should actually take seriously and listen to,
[02:28:55.520 --> 02:28:57.640]   and they're not politicized.
[02:28:57.640 --> 02:29:01.240]   And it's like, okay, and if you put out statements,
[02:29:01.240 --> 02:29:02.360]   I don't know if the CDC did,
[02:29:02.360 --> 02:29:04.680]   but health organization, whoever,
[02:29:04.680 --> 02:29:08.760]   that are provably false, and also provably,
[02:29:08.760 --> 02:29:09.920]   you kind of knew they were false,
[02:29:09.920 --> 02:29:11.120]   but you did it to scare people
[02:29:11.120 --> 02:29:13.280]   'cause you wanted them to do the right thing.
[02:29:13.280 --> 02:29:14.640]   It's like, no, you know what,
[02:29:14.640 --> 02:29:16.320]   that's not gonna work in the long run.
[02:29:16.320 --> 02:29:17.560]   Like, you're gonna lose people,
[02:29:17.560 --> 02:29:19.400]   and now you've got a bigger problem,
[02:29:19.400 --> 02:29:21.320]   which is a lack of trust in science,
[02:29:21.320 --> 02:29:24.640]   a lack of trust in authorities, who are, you know,
[02:29:24.640 --> 02:29:27.360]   by and large, they're like quite boring
[02:29:27.360 --> 02:29:28.920]   government bureaucrat scientists
[02:29:28.920 --> 02:29:31.280]   who just are trying to help the world.
[02:29:31.280 --> 02:29:33.720]   - Well, I've been criticized.
[02:29:34.560 --> 02:29:36.400]   And I've been torn on this.
[02:29:36.400 --> 02:29:39.960]   I've been criticized for criticizing Anthony Fauci too hard.
[02:29:39.960 --> 02:29:42.560]   The degree to which I criticized him
[02:29:42.560 --> 02:29:44.840]   is because he's a leader,
[02:29:44.840 --> 02:29:48.720]   and I'm just observing the effect
[02:29:48.720 --> 02:29:53.080]   in the loss of trust in institutions like the NIH,
[02:29:53.080 --> 02:29:54.240]   that where I personally know
[02:29:54.240 --> 02:29:55.840]   there's a lot of incredible scientists
[02:29:55.840 --> 02:29:57.400]   doing incredible work.
[02:29:57.400 --> 02:29:58.760]   And I have to blame the leaders
[02:29:58.760 --> 02:30:00.440]   for the effects on the distrust
[02:30:00.440 --> 02:30:02.920]   in the scientific work that they're doing
[02:30:02.920 --> 02:30:07.920]   because of what I perceive as basic human flaws
[02:30:07.920 --> 02:30:12.320]   of communication, of arrogance, of ego, of politics,
[02:30:12.320 --> 02:30:13.240]   all those kinds of things.
[02:30:13.240 --> 02:30:16.120]   Now, you could say you're being too harsh, possible,
[02:30:16.120 --> 02:30:18.400]   but I think that's the whole point of free speech
[02:30:18.400 --> 02:30:21.560]   is you can criticize the people who lead.
[02:30:21.560 --> 02:30:24.240]   Leaders, unfortunately or fortunately,
[02:30:24.240 --> 02:30:28.240]   are responsible for the effects on society.
[02:30:28.240 --> 02:30:29.880]   To me, Anthony Fauci,
[02:30:29.880 --> 02:30:33.080]   or whoever in the scientific position around the pandemic,
[02:30:33.080 --> 02:30:37.480]   had an opportunity to have a FDR moment
[02:30:37.480 --> 02:30:39.240]   or to get everybody together
[02:30:39.240 --> 02:30:42.200]   and inspire about the power of science
[02:30:42.200 --> 02:30:44.280]   to rapidly develop a vaccine
[02:30:44.280 --> 02:30:46.960]   that saves us from this pandemic and future pandemic
[02:30:46.960 --> 02:30:49.880]   that can threaten the wellbeing of human civilization.
[02:30:49.880 --> 02:30:53.000]   This was epic and awesome and sexy.
[02:30:53.000 --> 02:30:56.000]   And to me, when I talk to people about science,
[02:30:56.000 --> 02:30:57.200]   it's anything but sexy
[02:30:57.200 --> 02:30:59.840]   in terms of the virology and biology development
[02:30:59.840 --> 02:31:03.440]   because it's been politicized, it's icky,
[02:31:03.440 --> 02:31:04.880]   and people just don't wanna,
[02:31:04.880 --> 02:31:06.560]   don't talk to me about the vaccine.
[02:31:06.560 --> 02:31:07.720]   I understand, I understand.
[02:31:07.720 --> 02:31:08.880]   I got vaccinated.
[02:31:08.880 --> 02:31:10.720]   There's just, let's switch topics.
[02:31:10.720 --> 02:31:12.400]   - Right, yeah, yeah, yeah. - Quick.
[02:31:12.400 --> 02:31:13.240]   - Yeah.
[02:31:13.240 --> 02:31:14.920]   Well, it's interesting 'cause as I said,
[02:31:14.920 --> 02:31:17.280]   I live in the UK and I think it's,
[02:31:17.280 --> 02:31:20.040]   all these things are a little less politicized there.
[02:31:20.040 --> 02:31:24.880]   And I haven't paid close enough attention to Fauci
[02:31:24.880 --> 02:31:27.120]   to have a really strong view.
[02:31:27.120 --> 02:31:29.000]   I'm sure I would disagree with some things.
[02:31:29.000 --> 02:31:30.160]   I definitely, you know,
[02:31:30.160 --> 02:31:34.280]   I remember hearing at the beginning of the pandemic
[02:31:34.280 --> 02:31:38.280]   as I'm unwrapping my Amazon package with the masks I bought
[02:31:38.280 --> 02:31:39.480]   'cause I heard there's a pandemic
[02:31:39.480 --> 02:31:42.720]   and I just was like, I want some N95 mask, please.
[02:31:42.720 --> 02:31:45.920]   And they were saying, don't buy masks.
[02:31:45.920 --> 02:31:47.400]   And the motivation was
[02:31:47.400 --> 02:31:48.680]   because they didn't want there to be shortages
[02:31:48.680 --> 02:31:50.720]   in hospitals, fine.
[02:31:50.720 --> 02:31:53.840]   But they were also statements of masks won't,
[02:31:53.840 --> 02:31:55.160]   they're not effective and they won't help you.
[02:31:55.160 --> 02:31:58.200]   And then the complete about face to,
[02:31:58.200 --> 02:32:00.440]   you're ridiculous if you're not wearing them.
[02:32:00.440 --> 02:32:01.680]   You know, it's just like, no,
[02:32:01.680 --> 02:32:06.200]   like that about face just lost people from day one.
[02:32:06.200 --> 02:32:09.160]   - The distrust in the intelligence of the public
[02:32:09.160 --> 02:32:11.480]   to deal with nuance, to deal with the uncertainty.
[02:32:11.480 --> 02:32:13.600]   - Yeah, this is exactly what, you know,
[02:32:13.600 --> 02:32:17.560]   I think this is where the Wikipedia neutral point of view
[02:32:17.560 --> 02:32:20.520]   is and should be an ideal.
[02:32:20.520 --> 02:32:22.880]   And obviously every article and everything we could,
[02:32:22.880 --> 02:32:25.480]   you know me now and you know how I am about these things,
[02:32:25.480 --> 02:32:27.920]   but like ideally is to say, look,
[02:32:27.920 --> 02:32:29.800]   we're happy to show you all the perspectives.
[02:32:29.800 --> 02:32:32.000]   This is Planned Parenthood's view
[02:32:32.000 --> 02:32:33.880]   and this is Catholic Church view.
[02:32:33.880 --> 02:32:35.040]   And we're gonna explain that
[02:32:35.040 --> 02:32:36.240]   and we're gonna try to be thoughtful
[02:32:36.240 --> 02:32:40.840]   and put in the best arguments from all sides
[02:32:40.840 --> 02:32:41.760]   'cause I trust you.
[02:32:41.760 --> 02:32:44.880]   Like you read that and you're gonna be more educated
[02:32:44.880 --> 02:32:46.560]   and you're gonna begin to make a decision.
[02:32:46.560 --> 02:32:49.080]   I mean, I can just talk in the UK,
[02:32:49.080 --> 02:32:51.960]   the government, da, da, da, da.
[02:32:51.960 --> 02:32:53.680]   When we found out in the UK
[02:32:53.680 --> 02:32:56.440]   that very high level government officials
[02:32:56.440 --> 02:33:00.160]   were not following the rules they had put on everyone else,
[02:33:00.160 --> 02:33:03.160]   I moved from, I had just become a UK citizen
[02:33:03.160 --> 02:33:05.360]   just a little while before the pandemic.
[02:33:05.360 --> 02:33:06.880]   And you know, it's kind of emotional.
[02:33:06.880 --> 02:33:08.400]   Like you get a passport in a new country
[02:33:08.400 --> 02:33:09.280]   and you feel quite good.
[02:33:09.280 --> 02:33:10.960]   And I did my oath to the queen
[02:33:10.960 --> 02:33:13.120]   and then they dragged the poor old lady out
[02:33:13.120 --> 02:33:14.280]   to tell us all to be good.
[02:33:14.280 --> 02:33:15.800]   And I was like, we're British
[02:33:15.800 --> 02:33:17.160]   and we're gonna do the right things.
[02:33:17.160 --> 02:33:20.200]   And you know, it's gonna be tough, but we're gonna, you know.
[02:33:20.200 --> 02:33:22.640]   So you have that kind of Dunkirk spirit moment
[02:33:22.640 --> 02:33:25.960]   and you're like following the rules to a T
[02:33:25.960 --> 02:33:26.920]   and then suddenly it's like,
[02:33:26.920 --> 02:33:28.720]   well, they're not following the rules.
[02:33:28.720 --> 02:33:31.880]   And so suddenly I shifted personally from,
[02:33:31.880 --> 02:33:32.760]   I'm gonna follow the rules
[02:33:32.760 --> 02:33:35.400]   even if I don't completely agree with them,
[02:33:35.400 --> 02:33:36.600]   I'll still follow 'cause I think
[02:33:36.600 --> 02:33:38.720]   we've got all chipping together to like, you know what?
[02:33:38.720 --> 02:33:40.920]   I'm gonna make wise and thoughtful decisions
[02:33:40.920 --> 02:33:43.000]   for myself and my family.
[02:33:43.000 --> 02:33:45.160]   And that generally is gonna mean following the rules,
[02:33:45.160 --> 02:33:48.520]   but it's basically, you know, when they're,
[02:33:48.520 --> 02:33:50.360]   you know, at certain moments in time,
[02:33:50.360 --> 02:33:52.600]   like you're not allowed to be in an outside space
[02:33:52.600 --> 02:33:54.440]   unless you're exercising.
[02:33:54.440 --> 02:33:58.240]   I'm like, I think I can sit in a park and read a book.
[02:33:58.240 --> 02:33:59.600]   Like, it's gonna be fine.
[02:33:59.600 --> 02:34:01.280]   Like that's irrational rule,
[02:34:01.280 --> 02:34:03.640]   which I would have been following just personally
[02:34:03.640 --> 02:34:06.320]   of like, I'm just gonna do the right thing, yeah.
[02:34:06.320 --> 02:34:09.080]   - And the loss of trust, I think at scale
[02:34:09.080 --> 02:34:10.760]   was probably harmful to science.
[02:34:10.760 --> 02:34:13.720]   And to me, the scientific method
[02:34:13.720 --> 02:34:15.360]   and the scientific communities
[02:34:15.360 --> 02:34:19.080]   is one of the biggest hopes, at least to me,
[02:34:19.080 --> 02:34:22.560]   for the survival and the thriving of human civilization.
[02:34:22.560 --> 02:34:23.400]   - Absolutely.
[02:34:23.400 --> 02:34:25.920]   And I, you know, I think you see
[02:34:25.920 --> 02:34:28.160]   some of the ramifications of this.
[02:34:28.160 --> 02:34:31.640]   There's always been like pretty anti-science,
[02:34:31.640 --> 02:34:33.720]   anti-vax people, okay?
[02:34:33.720 --> 02:34:34.600]   That's always been a thing,
[02:34:34.600 --> 02:34:36.160]   but I feel like it's bigger now
[02:34:36.160 --> 02:34:41.120]   simply because of that lowering of trust.
[02:34:41.120 --> 02:34:43.880]   So a lot of people, yeah, maybe it's like you say,
[02:34:43.880 --> 02:34:45.680]   a lot of people are like, yeah, I got vaccinated
[02:34:45.680 --> 02:34:47.000]   and I really don't wanna talk about this
[02:34:47.000 --> 02:34:49.320]   because it's so toxic, you know?
[02:34:49.320 --> 02:34:52.240]   And that's unfortunate 'cause I think people should say,
[02:34:52.240 --> 02:34:53.920]   what an amazing thing.
[02:34:53.920 --> 02:34:58.880]   And you know, there's also a whole range of discourse
[02:34:58.880 --> 02:35:03.880]   around if this were a disease that were primarily,
[02:35:03.880 --> 02:35:07.560]   that was primarily killing babies,
[02:35:07.560 --> 02:35:09.080]   I think people's emotions about it
[02:35:09.080 --> 02:35:11.920]   would have been very different, right or wrong,
[02:35:11.920 --> 02:35:13.320]   than the fact that when you really looked at
[02:35:13.320 --> 02:35:18.320]   the sort of death rate of getting COVID,
[02:35:18.320 --> 02:35:21.760]   wow, it's really dramatically different.
[02:35:21.760 --> 02:35:26.760]   If you're late in life, this was really dangerous.
[02:35:26.760 --> 02:35:31.400]   And if you're 23 years old, yeah, well, it's not great.
[02:35:31.400 --> 02:35:33.360]   Like, and long COVID's a thing and all of that.
[02:35:33.360 --> 02:35:36.920]   But, and I think some of the public communications,
[02:35:36.920 --> 02:35:41.320]   again, were failing to properly contextualize it.
[02:35:41.320 --> 02:35:43.760]   Not all of it, you know, it's a complicated matter,
[02:35:43.760 --> 02:35:45.440]   but yeah.
[02:35:45.440 --> 02:35:48.920]   - Let me read you a Reddit comment that received two likes.
[02:35:48.920 --> 02:35:49.760]   - Oh.
[02:35:49.760 --> 02:35:50.920]   (Lex laughing)
[02:35:50.920 --> 02:35:52.320]   Two whole people liked it.
[02:35:52.320 --> 02:35:53.720]   - Yeah, two people liked it.
[02:35:53.720 --> 02:35:57.320]   And I don't know, maybe you can comment on
[02:35:57.320 --> 02:35:58.280]   whether there's truth to it,
[02:35:58.280 --> 02:35:59.720]   but I just found it interesting
[02:35:59.720 --> 02:36:01.040]   because I've been doing a lot of research
[02:36:01.040 --> 02:36:02.440]   on World War II recently.
[02:36:02.440 --> 02:36:04.160]   So this is about Hitler.
[02:36:04.160 --> 02:36:05.000]   - Oh, okay.
[02:36:05.000 --> 02:36:07.400]   - Here's, it's a long statement.
[02:36:07.400 --> 02:36:09.360]   I was there when a big push was made
[02:36:09.360 --> 02:36:11.280]   to fight bias at Wikipedia.
[02:36:11.280 --> 02:36:13.560]   Our target became getting the Hitler article
[02:36:13.560 --> 02:36:15.680]   to be Wiki's featured article.
[02:36:15.680 --> 02:36:18.440]   The idea was that the voting body only wanted articles
[02:36:18.440 --> 02:36:20.720]   that were good PR and especially articles
[02:36:20.720 --> 02:36:23.000]   about socially liberal topics.
[02:36:23.000 --> 02:36:26.880]   So the Hitler article had to be two to three times better
[02:36:26.880 --> 02:36:29.680]   and more academically researched to beat the competition.
[02:36:29.680 --> 02:36:31.440]   This bias seems to hold today.
[02:36:31.440 --> 02:36:34.560]   For example, the current list of political featured articles
[02:36:34.560 --> 02:36:37.200]   at a glance seems to have only two books,
[02:36:37.200 --> 02:36:40.680]   one on anarchism and one on Karl Marx.
[02:36:40.680 --> 02:36:43.280]   Surely we're not going to say
[02:36:43.280 --> 02:36:45.120]   there have only ever been two articles
[02:36:45.120 --> 02:36:48.760]   about political non-biography books worth being featured,
[02:36:48.760 --> 02:36:51.760]   especially compared to 200 plus video games.
[02:36:51.760 --> 02:36:54.960]   That's the only topics with good books
[02:36:54.960 --> 02:36:56.520]   are socialism and anarchy.
[02:36:56.520 --> 02:36:59.880]   Do you have any interesting comments on this kind of,
[02:36:59.880 --> 02:37:02.920]   so featured, how the featured are selected,
[02:37:02.920 --> 02:37:07.920]   maybe Hitler because he's a special figure, you know.
[02:37:07.920 --> 02:37:09.800]   - I love that, I love that.
[02:37:09.800 --> 02:37:13.840]   No, I love the comparison to how many video games have been.
[02:37:13.840 --> 02:37:17.000]   And that definitely speaks to my earlier as like,
[02:37:17.000 --> 02:37:19.800]   if you've got a lot of young geeky men
[02:37:19.800 --> 02:37:21.720]   who really like video games,
[02:37:21.720 --> 02:37:24.760]   that doesn't necessarily get you the right place
[02:37:24.760 --> 02:37:25.860]   in every respect.
[02:37:25.860 --> 02:37:32.120]   Certainly, yeah, so here's a funny story.
[02:37:32.120 --> 02:37:37.960]   I woke up one morning to a bunch of journalists in Germany
[02:37:37.960 --> 02:37:39.720]   trying to get in touch with me
[02:37:39.720 --> 02:37:42.200]   because German language Wikipedia chose to have
[02:37:42.200 --> 02:37:46.520]   as the featured article of the day, swastika.
[02:37:46.520 --> 02:37:48.440]   And people were going crazy about it.
[02:37:48.440 --> 02:37:51.880]   And some people were saying it's illegal,
[02:37:51.880 --> 02:37:53.820]   has German Wikipedia been taken over
[02:37:53.820 --> 02:37:56.680]   by Nazi sympathizers and so on.
[02:37:56.680 --> 02:37:58.800]   And it turned out it's not illegal,
[02:37:58.800 --> 02:38:00.840]   like discussing the swastika,
[02:38:00.840 --> 02:38:04.400]   using the swastika as a political campaign
[02:38:04.400 --> 02:38:07.160]   and using it in certain ways is illegal in Germany
[02:38:07.160 --> 02:38:08.640]   in a way that it wouldn't be in the US
[02:38:08.640 --> 02:38:10.520]   because of First Amendment.
[02:38:10.520 --> 02:38:11.800]   But in this case, it was like,
[02:38:11.800 --> 02:38:15.000]   actually, part of the point is the swastika symbol
[02:38:15.000 --> 02:38:17.080]   is from other cultures as well.
[02:38:17.080 --> 02:38:18.920]   And they just thought it was interesting.
[02:38:18.920 --> 02:38:20.800]   And I did joke to the community, I'm like,
[02:38:20.800 --> 02:38:23.200]   please don't put the swastika on the front page
[02:38:23.200 --> 02:38:25.360]   without warning me 'cause I'm gonna get a lot of,
[02:38:25.360 --> 02:38:27.040]   now it wouldn't be me, it's the foundation.
[02:38:27.040 --> 02:38:29.520]   I'm not that much on the front lines.
[02:38:29.520 --> 02:38:32.040]   And so I would say that to put Hitler
[02:38:32.040 --> 02:38:34.040]   on the front page of Wikipedia,
[02:38:34.040 --> 02:38:36.320]   it is a special topic and you would wanna say,
[02:38:36.320 --> 02:38:39.440]   yeah, let's be really careful that it's really, really good
[02:38:39.440 --> 02:38:42.200]   before we do that because if we put it on the front page
[02:38:42.200 --> 02:38:45.680]   and it's not good enough, that could be a problem.
[02:38:45.680 --> 02:38:49.800]   There's no inherent reason, like clearly World War II
[02:38:49.800 --> 02:38:53.920]   is a very popular topic in Wikipedia.
[02:38:53.920 --> 02:38:56.000]   It's like, they're on the History Channel.
[02:38:56.000 --> 02:38:58.360]   Like people, it's a fascinating period of history
[02:38:58.360 --> 02:39:00.400]   that people are very interested in.
[02:39:00.400 --> 02:39:03.520]   And then on the other piece, like anarchism
[02:39:03.520 --> 02:39:08.280]   and Karl Marx, yeah, I mean, that's interesting.
[02:39:08.280 --> 02:39:12.600]   I'm surprised to hear that not more political books
[02:39:12.600 --> 02:39:15.120]   or topics have made it to the front page.
[02:39:15.120 --> 02:39:16.840]   - Now we're taking this Reddit comment.
[02:39:16.840 --> 02:39:19.400]   - I mean, as if it's completely,
[02:39:19.400 --> 02:39:21.640]   but I'm trusting, so I think that's probably is right.
[02:39:21.640 --> 02:39:23.280]   They probably did have the list up.
[02:39:23.280 --> 02:39:26.800]   No, I think that piece, the piece about how many
[02:39:26.800 --> 02:39:29.640]   of those featured articles have been video games
[02:39:29.640 --> 02:39:32.520]   and if it's disproportionate, I think we should,
[02:39:32.520 --> 02:39:34.840]   the community should go, actually what's gone,
[02:39:34.840 --> 02:39:37.040]   like that doesn't seem quite right.
[02:39:37.040 --> 02:39:41.240]   You know, I mean, you can imagine
[02:39:41.240 --> 02:39:45.760]   that because you're looking for an article
[02:39:45.760 --> 02:39:47.840]   to be on the front page of Wikipedia,
[02:39:47.840 --> 02:39:51.680]   you wanna have a bit of diversity in it.
[02:39:51.680 --> 02:39:54.680]   You want it to be not always something
[02:39:54.680 --> 02:39:56.200]   that's really popular that week.
[02:39:56.200 --> 02:39:57.840]   So like, I don't know, the last couple of weeks,
[02:39:57.840 --> 02:40:00.080]   maybe Succession, a big finale of Succession
[02:40:00.080 --> 02:40:02.000]   might lead you to think, oh, let's put Succession
[02:40:02.000 --> 02:40:04.040]   on the front page, that's gonna be popular.
[02:40:04.040 --> 02:40:06.400]   In other cases, you kinda want,
[02:40:06.400 --> 02:40:08.440]   pick something super obscure and quirky
[02:40:08.440 --> 02:40:11.200]   because people also find that interesting and fun.
[02:40:11.200 --> 02:40:12.720]   So yeah, don't know, but you don't want it
[02:40:12.720 --> 02:40:15.400]   to be video games most of the time.
[02:40:15.400 --> 02:40:16.640]   That sounds quite bad.
[02:40:16.640 --> 02:40:22.480]   - Well, let me ask you just for,
[02:40:22.480 --> 02:40:25.400]   as somebody who's seen the whole thing,
[02:40:25.400 --> 02:40:28.280]   the development of the millions of articles,
[02:40:30.120 --> 02:40:33.480]   big impossible question, what's your favorite article?
[02:40:33.480 --> 02:40:34.920]   - My favorite article?
[02:40:34.920 --> 02:40:38.040]   Well, I've got an amusing answer,
[02:40:38.040 --> 02:40:40.960]   which is possibly also true.
[02:40:40.960 --> 02:40:44.440]   There's an article in Wikipedia
[02:40:44.440 --> 02:40:47.200]   called "Inherently Funny Words."
[02:40:47.200 --> 02:40:49.320]   And one of the reasons I love it is
[02:40:49.320 --> 02:40:54.280]   when it was created early in the history of Wikipedia,
[02:40:54.280 --> 02:40:56.160]   it kinda became like a dumping ground.
[02:40:56.160 --> 02:40:57.920]   People would just come by and write in any word
[02:40:57.920 --> 02:40:59.920]   that they thought sounded funny.
[02:40:59.920 --> 02:41:01.640]   And then it was nominated for deletion
[02:41:01.640 --> 02:41:03.720]   'cause somebody's like, this is just a dumping ground,
[02:41:03.720 --> 02:41:06.320]   like people are putting all kinds of nonsense in.
[02:41:06.320 --> 02:41:07.640]   And in that deletion debate,
[02:41:07.640 --> 02:41:10.200]   somebody came forward and said, essentially,
[02:41:10.200 --> 02:41:14.040]   wait a second, hold on, this is actually a legitimate concept
[02:41:14.040 --> 02:41:16.040]   in the theory of humor and comedy.
[02:41:16.040 --> 02:41:17.840]   And a lot of famous comedians and humorists
[02:41:17.840 --> 02:41:19.720]   have written about it.
[02:41:19.720 --> 02:41:23.640]   And it's actually a legitimate topic.
[02:41:23.640 --> 02:41:26.320]   So then they went through and they meticulously referenced
[02:41:26.320 --> 02:41:27.560]   every word that was in there
[02:41:27.560 --> 02:41:29.480]   and threw out a bunch that weren't.
[02:41:29.480 --> 02:41:30.960]   And so it becomes this really interesting.
[02:41:30.960 --> 02:41:32.760]   Now, my biggest disappointment,
[02:41:32.760 --> 02:41:35.400]   and it's the right decision to make
[02:41:35.400 --> 02:41:38.160]   because there was no source,
[02:41:38.160 --> 02:41:41.920]   but it was a picture of a cow,
[02:41:41.920 --> 02:41:46.360]   but there was a rope around its head
[02:41:46.360 --> 02:41:50.200]   tying on some horns onto the cow.
[02:41:50.200 --> 02:41:52.800]   So it was kind of a funny looking picture.
[02:41:52.800 --> 02:41:55.500]   It looked like a bull with horns,
[02:41:55.500 --> 02:41:58.680]   but it's just like a normal milk cow.
[02:41:58.680 --> 02:42:00.560]   And below it, the caption said,
[02:42:00.560 --> 02:42:04.320]   according to some, cow is an inherently funny word,
[02:42:04.320 --> 02:42:05.840]   which is just hilarious to me,
[02:42:05.840 --> 02:42:07.520]   partly because the according to some
[02:42:07.520 --> 02:42:09.320]   sounds a lot like Wikipedia,
[02:42:09.320 --> 02:42:10.160]   but there was no source.
[02:42:10.160 --> 02:42:12.640]   So it went away and I feel very sad about that.
[02:42:12.640 --> 02:42:14.640]   But I've always liked that.
[02:42:14.640 --> 02:42:16.840]   And actually, the reason depths of Wikipedia
[02:42:16.840 --> 02:42:21.480]   amuses me so greatly is because it does highlight
[02:42:21.480 --> 02:42:24.200]   really interesting, obscure stuff.
[02:42:24.200 --> 02:42:27.000]   And you're like, wow, I can't believe somebody wrote
[02:42:27.000 --> 02:42:28.480]   about that in Wikipedia.
[02:42:28.480 --> 02:42:29.320]   It's quite amusing.
[02:42:29.320 --> 02:42:32.200]   And sometimes there's a bit of wry humor in Wikipedia.
[02:42:32.200 --> 02:42:33.640]   There's always a struggle.
[02:42:33.640 --> 02:42:36.400]   You're not trying to be funny,
[02:42:36.400 --> 02:42:38.680]   but occasionally a little inside humor
[02:42:38.680 --> 02:42:40.480]   can be quite healthy.
[02:42:40.480 --> 02:42:43.800]   - Apparently words with the letter K are funny.
[02:42:43.800 --> 02:42:46.800]   There's a lot of really well researched stuff on this page.
[02:42:46.800 --> 02:42:47.640]   - Yeah.
[02:42:47.640 --> 02:42:49.480]   - It's actually exciting.
[02:42:49.480 --> 02:42:51.680]   And I should mention for depths of Wikipedia,
[02:42:51.680 --> 02:42:56.560]   it's run by Annie Rowerta.
[02:42:56.560 --> 02:42:57.600]   - That's right, Annie.
[02:42:57.600 --> 02:43:01.400]   - And let me just read off some of the pages.
[02:43:01.400 --> 02:43:04.720]   Octopolis and Octolantis.
[02:43:04.720 --> 02:43:05.640]   - Oh yeah, that was.
[02:43:05.640 --> 02:43:08.200]   - Are two separate non-human underwater settlements
[02:43:08.200 --> 02:43:12.320]   built by the gloomy octopuses in Jervis Bay, East Australia.
[02:43:12.320 --> 02:43:15.800]   The first settlement named Octopolis by biologists
[02:43:15.800 --> 02:43:17.680]   was found in 2009.
[02:43:17.680 --> 02:43:19.360]   The individual structures in Octopolis
[02:43:19.360 --> 02:43:23.240]   consist of burrows around a piece of human detritus
[02:43:23.240 --> 02:43:25.320]   believed to be scrap metal.
[02:43:25.320 --> 02:43:26.760]   And it goes on in this way.
[02:43:26.760 --> 02:43:33.640]   Satiric misspelling, least concerned species.
[02:43:33.640 --> 02:43:35.160]   Humans were formally assessed
[02:43:35.160 --> 02:43:40.160]   as a species of least concern in 2008.
[02:43:40.160 --> 02:43:43.480]   I think Hitchhiker's Guide to the Galaxy
[02:43:43.480 --> 02:43:45.680]   would slightly disagree.
[02:43:45.680 --> 02:43:46.800]   And last one, let me just say.
[02:43:46.800 --> 02:43:48.680]   Friendship paradox is the phenomena
[02:43:48.680 --> 02:43:53.120]   first observed by the sociologist Scott Feld in 1991
[02:43:53.120 --> 02:43:55.480]   that on average an individual's friends
[02:43:55.480 --> 02:43:58.120]   have more friends than that individual.
[02:43:58.120 --> 02:43:58.960]   - Oh, that's really interesting.
[02:43:58.960 --> 02:43:59.800]   - That's very lonely.
[02:43:59.800 --> 02:44:01.560]   - Isn't that, that's the kind of thing
[02:44:01.560 --> 02:44:04.840]   that makes you wanna, like it sounds implausible at first
[02:44:04.840 --> 02:44:06.960]   'cause shouldn't everybody have on average
[02:44:06.960 --> 02:44:09.360]   about the same number of friends as all their friends?
[02:44:09.360 --> 02:44:11.320]   So you really wanna dig into the math of that
[02:44:11.320 --> 02:44:13.640]   and really think, oh, why would that be true?
[02:44:13.640 --> 02:44:17.160]   - And it's one way to feel more lonely
[02:44:17.160 --> 02:44:19.880]   in a mathematically rigorous way.
[02:44:19.880 --> 02:44:22.600]   Somebody also on Reddit asks,
[02:44:22.600 --> 02:44:25.480]   I would love to hear some war stories from behind the scenes.
[02:44:25.480 --> 02:44:26.960]   Is there something that we haven't mentioned
[02:44:26.960 --> 02:44:28.200]   that was particularly difficult
[02:44:28.200 --> 02:44:32.120]   in this entire journey you're on with Wikipedia?
[02:44:32.120 --> 02:44:33.960]   - I mean, it's hard to say.
[02:44:33.960 --> 02:44:38.160]   I mean, so part of what I always say about myself
[02:44:38.160 --> 02:44:41.080]   is that I'm a pathological optimist.
[02:44:41.080 --> 02:44:42.960]   So I always think everything is fine.
[02:44:42.960 --> 02:44:46.600]   And so things that other people might find a struggle,
[02:44:46.600 --> 02:44:48.560]   I'm just like, oh, well, this is the thing we're doing today.
[02:44:48.560 --> 02:44:50.100]   So that's kind of about me.
[02:44:50.100 --> 02:44:52.720]   And it's actually, I'm aware of this about myself.
[02:44:52.720 --> 02:44:55.520]   So I do like to have a few pessimistic people around me
[02:44:55.520 --> 02:44:57.740]   to keep me a bit on balance.
[02:44:57.740 --> 02:45:01.920]   Yeah, I mean, I would say some of the hard things,
[02:45:01.920 --> 02:45:03.360]   I mean, there were hard moments
[02:45:03.360 --> 02:45:06.240]   like when two out of three servers crashed on Christmas day
[02:45:06.240 --> 02:45:09.400]   and then we needed to do a fundraiser
[02:45:09.400 --> 02:45:12.040]   and no idea what was gonna happen.
[02:45:12.040 --> 02:45:18.120]   I would say as well, like in that early period of time,
[02:45:19.360 --> 02:45:23.800]   the growth of the website and the traffic to the website
[02:45:23.800 --> 02:45:25.080]   was phenomenally great.
[02:45:25.080 --> 02:45:26.360]   The growth of the community,
[02:45:26.360 --> 02:45:29.240]   and in fact, the healthy growth of the community was fine.
[02:45:29.240 --> 02:45:31.500]   And then the Wikimedia Foundation,
[02:45:31.500 --> 02:45:34.340]   the nonprofit I set up to own and operate Wikipedia,
[02:45:34.340 --> 02:45:37.800]   as a small organization, it had a lot of growing pains.
[02:45:37.800 --> 02:45:44.020]   And that was the piece that's just like many companies
[02:45:44.020 --> 02:45:46.740]   or many organizations that are in a fast growth.
[02:45:46.740 --> 02:45:48.960]   It's like, you've hired the wrong people
[02:45:48.960 --> 02:45:50.600]   or there's this conflict that's arisen
[02:45:50.600 --> 02:45:53.480]   and nobody's got experience to do this and all that.
[02:45:53.480 --> 02:45:55.640]   So no specific stories to tell,
[02:45:55.640 --> 02:45:58.320]   but I would say growing the organization
[02:45:58.320 --> 02:46:00.480]   was harder than growing the community
[02:46:00.480 --> 02:46:02.760]   and growing the website, which is interesting.
[02:46:02.760 --> 02:46:05.400]   - Well, yeah, it's kind of miraculous and inspiring
[02:46:05.400 --> 02:46:08.680]   that a community can emerge and be stable
[02:46:08.680 --> 02:46:13.680]   and that has so much kind of productive, positive output.
[02:46:13.680 --> 02:46:16.920]   Kind of makes you think.
[02:46:16.920 --> 02:46:18.600]   I mean, I don't, it's one of those things
[02:46:18.600 --> 02:46:20.200]   you don't wanna analyze too much
[02:46:20.200 --> 02:46:24.520]   'cause you don't wanna mess with a beautiful thing,
[02:46:24.520 --> 02:46:26.360]   but it gives me faith in communities.
[02:46:26.360 --> 02:46:27.200]   - Yeah, yeah.
[02:46:27.200 --> 02:46:29.560]   - That they can spring up in other domains as well.
[02:46:29.560 --> 02:46:30.880]   - Yeah, I think that's exactly right.
[02:46:30.880 --> 02:46:35.600]   And at Fandom, my for-profit wiki company,
[02:46:35.600 --> 02:46:39.280]   where it's like all these communities about pop culture,
[02:46:39.280 --> 02:46:43.880]   mainly sort of entertainment, gaming, and so on,
[02:46:43.880 --> 02:46:46.240]   there's a lot of small communities.
[02:46:46.240 --> 02:46:50.240]   And so I went last year to our Community Connect Conference
[02:46:50.240 --> 02:46:51.960]   and just met some of these people.
[02:46:51.960 --> 02:46:55.600]   And like, here's one of the leaders of the Star Wars wiki,
[02:46:55.600 --> 02:46:58.360]   which is called Wookieepedia, which I think is great.
[02:46:58.360 --> 02:47:01.440]   And he's telling me about his community and all that.
[02:47:01.440 --> 02:47:03.800]   And I'm like, oh, right, yeah, I love this.
[02:47:03.800 --> 02:47:07.480]   So it's not the same purpose as Wikipedia
[02:47:07.480 --> 02:47:11.040]   of a neutral, high-quality encyclopedia,
[02:47:11.040 --> 02:47:13.280]   but a lot of the same values are there of like,
[02:47:13.280 --> 02:47:15.520]   oh, people should be nice to each other.
[02:47:15.520 --> 02:47:17.560]   It's like, when people get upset, it's like,
[02:47:17.560 --> 02:47:19.600]   just remember, we're working on a Star Wars wiki together.
[02:47:19.600 --> 02:47:22.200]   Like, there's no reason to get too outraged.
[02:47:22.200 --> 02:47:26.080]   And just kind people, just like geeky people with a hobby.
[02:47:26.080 --> 02:47:32.240]   - Where do you see Wikipedia in 10 years, 100 years,
[02:47:32.240 --> 02:47:34.120]   and 1,000 years?
[02:47:34.120 --> 02:47:35.840]   - Dun, dun, dun.
[02:47:35.840 --> 02:47:40.840]   Right, so 10 years, I would say pretty much the same.
[02:47:41.840 --> 02:47:46.160]   Like, we're not gonna become TikTok, you know,
[02:47:46.160 --> 02:47:50.200]   with entertainment, scroll by video,
[02:47:50.200 --> 02:47:54.480]   humor and blah, blah, blah, an encyclopedia.
[02:47:54.480 --> 02:47:57.120]   I think in 10 years, we probably will have
[02:47:57.120 --> 02:48:02.320]   a lot more AI-supporting tools, like I've talked about.
[02:48:02.320 --> 02:48:05.080]   And probably your search experience will be,
[02:48:05.080 --> 02:48:06.920]   you can ask a question and get the answer
[02:48:06.920 --> 02:48:09.400]   rather than, you know, from our body of work.
[02:48:09.400 --> 02:48:11.720]   - So search and discovery, a little bit improved.
[02:48:11.720 --> 02:48:14.000]   - Yeah, yeah, all that.
[02:48:14.000 --> 02:48:16.000]   I always say one of the things that people,
[02:48:16.000 --> 02:48:18.000]   most people won't notice,
[02:48:18.000 --> 02:48:20.760]   because already they don't notice it,
[02:48:20.760 --> 02:48:23.200]   is the growth of Wikipedia
[02:48:23.200 --> 02:48:25.520]   in the languages of the developing world.
[02:48:25.520 --> 02:48:29.320]   So you probably don't speak Swahili,
[02:48:29.320 --> 02:48:30.960]   so you're probably not checking out
[02:48:30.960 --> 02:48:33.160]   that Swahili Wikipedia is doing very well.
[02:48:33.160 --> 02:48:36.200]   And it is doing very well.
[02:48:36.200 --> 02:48:37.400]   And I think that kind of growth
[02:48:37.400 --> 02:48:40.480]   is actually super important and super interesting.
[02:48:40.480 --> 02:48:41.800]   But most people won't notice that.
[02:48:41.800 --> 02:48:44.040]   - If we can just link on that, if we could.
[02:48:44.040 --> 02:48:47.880]   Do you think, there's so much incredible translation work
[02:48:47.880 --> 02:48:50.240]   is being done with AI, with language models.
[02:48:50.240 --> 02:48:55.240]   Do you think that can accelerate Wikipedia?
[02:48:55.240 --> 02:48:56.800]   So you start with the basic draft
[02:48:56.800 --> 02:48:59.560]   of the translation of articles and then build on top of that.
[02:48:59.560 --> 02:49:03.840]   - So what I used to say is,
[02:49:03.840 --> 02:49:05.800]   like machine translation for many years
[02:49:05.800 --> 02:49:07.360]   wasn't much use to the community,
[02:49:07.360 --> 02:49:09.040]   'cause it just wasn't good enough.
[02:49:09.040 --> 02:49:13.280]   As it's gotten better, it's tended to be a lot better
[02:49:13.280 --> 02:49:17.120]   in what we might call economically important languages.
[02:49:17.120 --> 02:49:19.080]   That's because the corpus that they train on
[02:49:19.080 --> 02:49:20.120]   and all of that.
[02:49:20.120 --> 02:49:24.040]   So to translate from English to Spanish,
[02:49:24.040 --> 02:49:26.280]   if you've tried Google Translate recently,
[02:49:26.280 --> 02:49:28.400]   Spanish to English is what I would do.
[02:49:28.400 --> 02:49:29.280]   It's pretty good.
[02:49:29.280 --> 02:49:30.280]   Like it's actually not bad.
[02:49:30.280 --> 02:49:31.480]   It used to be half a joke,
[02:49:31.480 --> 02:49:32.760]   and then for a while it was kind of like,
[02:49:32.760 --> 02:49:33.960]   well, you can get the gist of something.
[02:49:33.960 --> 02:49:36.480]   And now it's like, actually it's pretty good.
[02:49:36.480 --> 02:49:38.280]   However, we've got a huge Spanish community
[02:49:38.280 --> 02:49:40.080]   who write in native Spanish.
[02:49:40.080 --> 02:49:42.320]   So they're able to use it and they find it useful,
[02:49:42.320 --> 02:49:44.000]   but they're writing.
[02:49:44.000 --> 02:49:47.400]   But if you tried to do English to Zulu,
[02:49:47.400 --> 02:49:50.720]   where there's not that much investment,
[02:49:50.720 --> 02:49:53.080]   like there's loads of reasons to invest in English to Spanish
[02:49:53.080 --> 02:49:55.840]   'cause they're both huge economically important languages,
[02:49:55.840 --> 02:49:56.680]   Zulu not so much.
[02:49:56.680 --> 02:50:00.200]   So for those smaller languages, it was just still terrible.
[02:50:00.200 --> 02:50:03.360]   My understanding is it's improved dramatically
[02:50:03.360 --> 02:50:05.880]   and also because the new methods of training
[02:50:05.880 --> 02:50:10.880]   don't necessarily involve identical corpuses
[02:50:10.880 --> 02:50:13.160]   to try to match things up,
[02:50:13.160 --> 02:50:16.760]   but rather reading and understanding with tokens
[02:50:16.760 --> 02:50:18.400]   and large language models,
[02:50:18.400 --> 02:50:19.640]   and then reading and understanding,
[02:50:19.640 --> 02:50:22.040]   and then you get a much richer.
[02:50:22.040 --> 02:50:23.480]   Anyway, apparently it's quite improved.
[02:50:23.480 --> 02:50:26.760]   So I think that now it is quite possible
[02:50:26.760 --> 02:50:29.600]   that these smaller language communities are gonna say,
[02:50:29.600 --> 02:50:32.840]   oh, well, finally I can put something in English
[02:50:32.840 --> 02:50:36.480]   and I can get out Zulu that I feel comfortable sharing
[02:50:36.480 --> 02:50:38.880]   with my community because it's actually good enough,
[02:50:38.880 --> 02:50:40.680]   or I can edit it a bit here and there.
[02:50:40.680 --> 02:50:41.720]   So I think that's huge.
[02:50:41.720 --> 02:50:43.280]   So I do think that's gonna happen a lot.
[02:50:43.280 --> 02:50:44.960]   And that's gonna accelerate, again,
[02:50:44.960 --> 02:50:47.120]   what will remain to most people an invisible trend,
[02:50:47.120 --> 02:50:50.080]   but that's the growth in all these other languages.
[02:50:50.080 --> 02:50:52.080]   So then move on to 100 years.
[02:50:52.080 --> 02:50:54.880]   - Oh, it's starting to get scary.
[02:50:54.880 --> 02:50:58.360]   - Well, the only thing I say about 100 years is like,
[02:50:58.360 --> 02:51:02.960]   we've built the Wikimedia Foundation,
[02:51:02.960 --> 02:51:06.640]   and we run it in a quite cautious
[02:51:06.640 --> 02:51:10.080]   and financially conservative and careful way.
[02:51:10.080 --> 02:51:12.920]   So every year we build our reserves,
[02:51:12.920 --> 02:51:15.440]   every year we put aside a little bit more money.
[02:51:15.440 --> 02:51:17.200]   We also have the endowment fund,
[02:51:17.200 --> 02:51:18.920]   which we just passed 100 million.
[02:51:18.920 --> 02:51:22.640]   That's a completely separate fund with a separate board
[02:51:22.640 --> 02:51:24.800]   so that it's not just like a big fat bank account
[02:51:24.800 --> 02:51:28.040]   for some future profligate CEO to blow through.
[02:51:28.040 --> 02:51:30.640]   The foundation will have to get the approval
[02:51:30.640 --> 02:51:34.200]   of a second order board to be able to access that money.
[02:51:34.200 --> 02:51:36.240]   And that board can make other grants
[02:51:36.240 --> 02:51:38.480]   through the community and things like that.
[02:51:38.480 --> 02:51:41.920]   So the point of all that is I hope and believe
[02:51:41.920 --> 02:51:46.840]   that we're building in a financially stable way,
[02:51:46.840 --> 02:51:49.560]   that we can weather various storms along the way
[02:51:49.560 --> 02:51:54.560]   so that hopefully we're not taking the kind of risks.
[02:51:54.560 --> 02:51:57.280]   And by the way, we're not taking too few risks either.
[02:51:57.280 --> 02:51:58.280]   That's always hard.
[02:51:58.280 --> 02:52:01.040]   I think the Wikimedia Foundation and Wikipedia
[02:52:01.040 --> 02:52:02.680]   will exist in 100 years,
[02:52:02.680 --> 02:52:05.960]   if anybody exists in 100 years, we'll be there.
[02:52:05.960 --> 02:52:08.000]   - Do you think the internet just looks
[02:52:08.000 --> 02:52:11.000]   unpredictably different than just the web?
[02:52:11.000 --> 02:52:11.920]   - I do, I do.
[02:52:11.920 --> 02:52:14.560]   I mean, I think right now,
[02:52:14.560 --> 02:52:19.200]   this sort of enormous step forward we've seen
[02:52:19.200 --> 02:52:20.800]   has become public in the last year
[02:52:20.800 --> 02:52:22.320]   of the large language models,
[02:52:22.320 --> 02:52:26.000]   really is something else, right?
[02:52:26.000 --> 02:52:26.840]   It's really interesting.
[02:52:26.840 --> 02:52:28.600]   You and I have both talked today
[02:52:28.600 --> 02:52:30.800]   about the flaws and the limitations,
[02:52:30.800 --> 02:52:33.440]   but still it's, as someone who's been around technology
[02:52:33.440 --> 02:52:36.320]   for a long time, it's sort of that feeling
[02:52:36.320 --> 02:52:38.680]   of the first time I saw a web browser,
[02:52:38.680 --> 02:52:40.640]   the first time I saw the iPhone,
[02:52:40.640 --> 02:52:42.920]   like the first time the internet was like really usable
[02:52:42.920 --> 02:52:44.240]   on a phone and it's like, wow,
[02:52:44.240 --> 02:52:46.960]   that's a step change difference.
[02:52:46.960 --> 02:52:48.600]   There's a few other, you know.
[02:52:48.600 --> 02:52:49.920]   - Maybe a Google search.
[02:52:49.920 --> 02:52:50.760]   - Google search was actually one.
[02:52:50.760 --> 02:52:51.760]   - I remember the first search.
[02:52:51.760 --> 02:52:54.160]   - Because I remember AltaVista was kind of cool for a while,
[02:52:54.160 --> 02:52:55.600]   then it just got more and more useless
[02:52:55.600 --> 02:52:57.240]   'cause the algorithm wasn't good.
[02:52:57.240 --> 02:52:58.320]   And it's like, oh, Google search,
[02:52:58.320 --> 02:52:59.800]   now the internet works again.
[02:52:59.800 --> 02:53:03.960]   And so large language model, it feels like that to me,
[02:53:03.960 --> 02:53:06.400]   like, oh wow, this is something new
[02:53:06.400 --> 02:53:08.480]   and really pretty remarkable.
[02:53:08.480 --> 02:53:09.520]   And it's gonna have some downsides,
[02:53:09.520 --> 02:53:14.520]   like the negative use case.
[02:53:14.520 --> 02:53:16.080]   People in the area who are experts,
[02:53:16.080 --> 02:53:17.680]   they're giving a lot of warnings
[02:53:17.680 --> 02:53:19.760]   and I don't know enough to, I'm not that worried,
[02:53:19.760 --> 02:53:22.240]   but I'm a pathological optimist.
[02:53:22.240 --> 02:53:25.200]   But I do see some like really low hanging fruit,
[02:53:25.200 --> 02:53:26.440]   bad things that can happen.
[02:53:26.440 --> 02:53:31.440]   So my example is, how about some highly customized spam
[02:53:31.440 --> 02:53:38.560]   where the email that you receive isn't just like
[02:53:38.560 --> 02:53:40.840]   misspelled words and like trying to get through filters,
[02:53:40.840 --> 02:53:42.840]   but actually is a targeted email to you
[02:53:42.840 --> 02:53:44.760]   that knows something about you
[02:53:44.760 --> 02:53:46.840]   by reading your LinkedIn profile
[02:53:46.840 --> 02:53:48.320]   and writes a plausible email
[02:53:48.320 --> 02:53:49.560]   that will get through the filters.
[02:53:49.640 --> 02:53:52.880]   And it's like suddenly, oh, that's a new problem,
[02:53:52.880 --> 02:53:55.040]   that's gonna be interesting.
[02:53:55.040 --> 02:53:58.440]   - Is there a, just on the Wikipedia editing side,
[02:53:58.440 --> 02:54:00.320]   does it make the job of the volunteer,
[02:54:00.320 --> 02:54:03.720]   of the editor more difficult if in a world
[02:54:03.720 --> 02:54:07.000]   where larger and larger percentage of the internet
[02:54:07.000 --> 02:54:08.520]   is written by an LLM?
[02:54:08.520 --> 02:54:11.840]   - So one of my predictions, and we'll see,
[02:54:11.840 --> 02:54:14.480]   ask me again in five years how this panned out,
[02:54:14.480 --> 02:54:19.280]   is that in a way this will strengthen
[02:54:19.280 --> 02:54:24.080]   the value and importance of some traditional brands.
[02:54:24.080 --> 02:54:29.080]   So if I see a news story and it's from
[02:54:29.080 --> 02:54:33.200]   the Wall Street Journal, from the New York Times,
[02:54:33.200 --> 02:54:36.680]   from Fox News, I know what I'm getting
[02:54:36.680 --> 02:54:40.040]   and I trust it to whatever extent I might have,
[02:54:40.040 --> 02:54:42.960]   you know, trust or distrust in any of those.
[02:54:42.960 --> 02:54:46.760]   And if I see a brand new website that looks plausible,
[02:54:46.760 --> 02:54:48.040]   but I've never heard of it,
[02:54:48.040 --> 02:54:50.360]   and it could be machine-generated content
[02:54:50.360 --> 02:54:53.000]   that may be full of errors, I think I'll be more cautious.
[02:54:53.000 --> 02:54:54.200]   I think I'm more interested.
[02:54:54.200 --> 02:54:56.320]   And we can also talk about this around
[02:54:56.320 --> 02:54:58.640]   photographic evidence.
[02:54:58.640 --> 02:55:00.240]   So obviously there will be scandals
[02:55:00.240 --> 02:55:04.120]   where major media organizations get fooled by a fake photo.
[02:55:04.120 --> 02:55:08.560]   However, if I see a photo of, the recent one
[02:55:08.560 --> 02:55:12.280]   was the Pope wearing an expensive puffer jacket,
[02:55:12.280 --> 02:55:14.380]   I'm gonna go, yeah, that's amazing
[02:55:14.380 --> 02:55:16.400]   that a fake like that could be generated,
[02:55:16.400 --> 02:55:18.280]   but my immediate thought is not,
[02:55:18.280 --> 02:55:21.320]   oh, so the Pope's dipping into the money, eh?
[02:55:21.320 --> 02:55:23.080]   Probably 'cause this particular Pope
[02:55:23.080 --> 02:55:25.180]   doesn't seem like he'd be the type.
[02:55:25.180 --> 02:55:27.680]   - My favorite is extensive pictures
[02:55:27.680 --> 02:55:29.560]   of Joe Biden and Donald Trump
[02:55:29.560 --> 02:55:31.000]   hanging out and having fun together.
[02:55:31.000 --> 02:55:33.160]   - Yeah, brilliant.
[02:55:33.160 --> 02:55:37.920]   So I think people will care about the provenance of a photo.
[02:55:37.920 --> 02:55:40.360]   And if you show me a photo and you say,
[02:55:40.360 --> 02:55:45.360]   yeah, this photo is from Fox News,
[02:55:45.520 --> 02:55:47.880]   even though I don't necessarily think that's the highest,
[02:55:47.880 --> 02:55:50.080]   but I'm like, well, it's a news organization
[02:55:50.080 --> 02:55:51.120]   and they're gonna have journalists in it,
[02:55:51.120 --> 02:55:54.520]   they're gonna make sure the photo is what it purports to be,
[02:55:54.520 --> 02:55:56.720]   that's very different from a photo
[02:55:56.720 --> 02:55:58.040]   randomly circulating on Twitter.
[02:55:58.040 --> 02:56:00.240]   Whereas I would say 15 years ago,
[02:56:00.240 --> 02:56:02.400]   a photo randomly circulating on Twitter,
[02:56:02.400 --> 02:56:06.000]   in most cases, the worst you could do,
[02:56:06.000 --> 02:56:10.000]   and this did happen, is misrepresent the battlefield.
[02:56:10.000 --> 02:56:13.280]   So like, oh, here's a bunch of injured children.
[02:56:13.280 --> 02:56:14.360]   Look what Israel's done,
[02:56:14.360 --> 02:56:15.680]   but actually it wasn't Israel,
[02:56:15.680 --> 02:56:17.920]   it was another case 10 years ago.
[02:56:17.920 --> 02:56:20.960]   That has happened, that has always been around.
[02:56:20.960 --> 02:56:25.160]   But now we can have much more specifically constructed,
[02:56:25.160 --> 02:56:26.720]   plausible looking photos,
[02:56:26.720 --> 02:56:28.360]   that if I just see them circulating on Twitter,
[02:56:28.360 --> 02:56:30.680]   I'm gonna go, just don't know, not sure.
[02:56:30.680 --> 02:56:32.760]   Like I can make that in five minutes.
[02:56:32.760 --> 02:56:35.240]   - Well, I also hope that it's kind of like
[02:56:35.240 --> 02:56:36.360]   what you're writing about in your book,
[02:56:36.360 --> 02:56:38.760]   that we could also have citizen journalists
[02:56:38.760 --> 02:56:43.760]   that have a stable, verifiable trust that builds up.
[02:56:43.840 --> 02:56:45.880]   So it doesn't have to be New York Times,
[02:56:45.880 --> 02:56:49.000]   this organization, that it could be an organization of one,
[02:56:49.000 --> 02:56:50.920]   as long as it's stable and carries through time
[02:56:50.920 --> 02:56:52.240]   and it builds up or builds up.
[02:56:52.240 --> 02:56:56.040]   - No, I agree, but the one thing I've said in the past,
[02:56:56.040 --> 02:56:57.760]   and this depends on who that person is
[02:56:57.760 --> 02:56:59.280]   and what they're doing, but it's like,
[02:56:59.280 --> 02:57:02.320]   I think my credibility, my general credibility in the world
[02:57:02.320 --> 02:57:05.400]   should be the equal of a New York Times reporter.
[02:57:05.400 --> 02:57:08.720]   So if something happens and I witness it
[02:57:08.720 --> 02:57:10.440]   and I write about it, people are gonna go,
[02:57:10.440 --> 02:57:12.360]   well, Jimmy Wales said it.
[02:57:12.360 --> 02:57:15.200]   That's just like if a New York Times reporter said it,
[02:57:15.200 --> 02:57:17.980]   I'm gonna tend to think he didn't just make it up.
[02:57:17.980 --> 02:57:20.280]   Truth is, nothing interesting ever happens around me.
[02:57:20.280 --> 02:57:23.040]   I don't go to war zones, I don't go to big press conferences,
[02:57:23.040 --> 02:57:27.960]   I don't interview Putin and Zelensky, right?
[02:57:27.960 --> 02:57:30.400]   So just to an extent, yes,
[02:57:30.400 --> 02:57:32.100]   whereas I do think for other people,
[02:57:32.100 --> 02:57:36.360]   those traditional models of credibility
[02:57:36.360 --> 02:57:39.280]   are really, really important.
[02:57:39.280 --> 02:57:41.600]   And then there is this sort of citizen journalism,
[02:57:41.600 --> 02:57:44.880]   I don't know if you think of what you do as journalism,
[02:57:44.880 --> 02:57:46.840]   I kind of think it is, but you do interviews,
[02:57:46.840 --> 02:57:48.920]   you do long form interviews.
[02:57:48.920 --> 02:57:51.800]   And I think people, like if you come and you say,
[02:57:51.800 --> 02:57:55.160]   right, here's my tape, but you wouldn't hand out a tape,
[02:57:55.160 --> 02:57:57.320]   like I just gestured you as if I'm handing you a cassette
[02:57:57.320 --> 02:58:00.040]   tape, but if you put it into your podcast,
[02:58:00.040 --> 02:58:03.120]   here's my interview with Zelensky,
[02:58:03.120 --> 02:58:06.840]   and people aren't gonna go, yeah, how do we know?
[02:58:06.840 --> 02:58:08.160]   That could be a deep fake.
[02:58:08.160 --> 02:58:09.320]   Like you could have faked that,
[02:58:09.320 --> 02:58:11.320]   'cause people are like, well, no,
[02:58:11.320 --> 02:58:12.920]   like you're a well-known podcaster
[02:58:12.920 --> 02:58:14.540]   and you do interview interesting people,
[02:58:14.540 --> 02:58:17.120]   and yeah, like you wouldn't think that.
[02:58:17.120 --> 02:58:19.080]   So that your brand becomes really important,
[02:58:19.080 --> 02:58:21.760]   whereas if suddenly, and I've seen this already,
[02:58:21.760 --> 02:58:26.720]   I've seen sort of video with subtitles in English,
[02:58:26.720 --> 02:58:29.600]   and apparently the Ukrainian was the same,
[02:58:29.600 --> 02:58:32.840]   and Zelensky saying something really outrageous.
[02:58:32.840 --> 02:58:34.200]   And I'm like, yeah, I don't believe that.
[02:58:34.200 --> 02:58:37.080]   Like, I don't think he said that in a meeting with,
[02:58:37.080 --> 02:58:38.840]   you know, whatever.
[02:58:38.840 --> 02:58:42.280]   I think that's Russian propaganda, or probably just trolls.
[02:58:42.280 --> 02:58:44.600]   - Yeah, and then building platforms and mechanisms
[02:58:44.600 --> 02:58:46.640]   of how that trust can be verified.
[02:58:46.640 --> 02:58:47.800]   Whether, you know, if something appears
[02:58:47.800 --> 02:58:50.000]   on a Wikipedia page, that means something.
[02:58:50.000 --> 02:58:52.780]   If something appears on, like say,
[02:58:52.780 --> 02:58:54.360]   my Twitter account, that means something.
[02:58:54.360 --> 02:58:57.960]   That means I, this particular human, have signed off on it.
[02:58:57.960 --> 02:58:58.800]   - Yeah, exactly.
[02:58:58.800 --> 02:59:02.660]   - And then the trust you have in this particular human
[02:59:02.660 --> 02:59:04.720]   transfers to the piece of content.
[02:59:04.720 --> 02:59:07.800]   And then each, hopefully there's millions of people
[02:59:07.800 --> 02:59:10.120]   with different metrics of trust.
[02:59:10.120 --> 02:59:12.440]   And then you could see that there's a certain kind of bias
[02:59:12.440 --> 02:59:14.400]   in the set of conversations you're having.
[02:59:14.400 --> 02:59:16.600]   So maybe, okay, I trust this person to have this kind
[02:59:16.600 --> 02:59:18.160]   of bias, and I'll go to this other person
[02:59:18.160 --> 02:59:20.360]   with this other kind of bias, and I can integrate them
[02:59:20.360 --> 02:59:22.520]   in this kind of way, just like you said with Fox News
[02:59:22.520 --> 02:59:23.360]   and whatever else.
[02:59:23.360 --> 02:59:24.760]   - Yeah, and the Boston Journal, New York Times,
[02:59:24.760 --> 02:59:27.220]   like they've all got their, like where they sit.
[02:59:27.220 --> 02:59:29.480]   Yeah.
[02:59:29.480 --> 02:59:34.400]   - So you have built, I would say,
[02:59:34.400 --> 02:59:37.960]   one of, if not the most impactful website
[02:59:37.960 --> 02:59:39.920]   in the history of human civilization.
[02:59:39.920 --> 02:59:44.240]   So let me ask for you to give advice to young people
[02:59:44.240 --> 02:59:46.120]   how to have impact in this world.
[02:59:46.120 --> 02:59:48.940]   High schoolers, college students, wanting to have
[02:59:48.940 --> 02:59:50.360]   a big positive impact in the world.
[02:59:50.360 --> 02:59:51.240]   - Yeah, great.
[02:59:51.240 --> 02:59:54.040]   If you wanna be successful, do something you're really
[02:59:54.040 --> 02:59:56.960]   passionate about, rather than some kind of cold calculation
[02:59:56.960 --> 02:59:58.680]   of what can make you the most money.
[02:59:58.680 --> 03:00:00.760]   'Cause if you go and try to do something,
[03:00:00.760 --> 03:00:02.080]   and you're like, I'm not that interested,
[03:00:02.080 --> 03:00:03.680]   but I'm gonna make a lot of money doing it,
[03:00:03.680 --> 03:00:05.560]   you're probably not gonna be that good at it.
[03:00:05.560 --> 03:00:07.760]   And so that is a big piece of it.
[03:00:07.760 --> 03:00:14.440]   I also like, so for startups, I give this advice.
[03:00:14.440 --> 03:00:17.280]   So young, and this is a career startup,
[03:00:17.280 --> 03:00:19.920]   any kind of young person just starting out,
[03:00:19.920 --> 03:00:24.840]   is like, be persistent, right?
[03:00:24.840 --> 03:00:27.320]   There'll be moments when it's not working out,
[03:00:27.320 --> 03:00:29.360]   and you can't just give up too easily.
[03:00:29.360 --> 03:00:31.080]   You've gotta persist through some hard times.
[03:00:31.080 --> 03:00:33.080]   Maybe two servers crash on a Sunday,
[03:00:33.080 --> 03:00:35.320]   and you've gotta sort of scramble to figure it out,
[03:00:35.320 --> 03:00:36.580]   but persist through that.
[03:00:36.580 --> 03:00:41.200]   And then also, be prepared to pivot.
[03:00:41.200 --> 03:00:43.120]   That's a newer word, new for me,
[03:00:43.120 --> 03:00:48.120]   but when I pivoted from Newpedia to Wikipedia,
[03:00:48.120 --> 03:00:49.360]   it's like, this isn't working,
[03:00:49.360 --> 03:00:51.180]   I've gotta completely change.
[03:00:51.180 --> 03:00:53.360]   So be willing to completely change direction
[03:00:53.360 --> 03:00:54.440]   when something's not working.
[03:00:54.440 --> 03:00:58.440]   Now the problem with these two wonderful pieces of advice
[03:00:58.440 --> 03:01:01.200]   is which situation am I in today, right?
[03:01:01.200 --> 03:01:03.560]   Is this a moment when I need to just power through
[03:01:03.560 --> 03:01:06.520]   and persist because I'm gonna find a way to make this work,
[03:01:06.520 --> 03:01:07.960]   or is this a moment where I need to go,
[03:01:07.960 --> 03:01:09.440]   actually, this is totally not working,
[03:01:09.440 --> 03:01:11.560]   and I need to change direction?
[03:01:11.560 --> 03:01:13.880]   But also, I think for me, that always gives me a framework
[03:01:13.880 --> 03:01:17.900]   of like, okay, here's a problem.
[03:01:17.900 --> 03:01:19.600]   Do we need to change direction,
[03:01:19.600 --> 03:01:21.280]   or do we need to kind of power through it?
[03:01:21.280 --> 03:01:24.800]   And just knowing those are the choices,
[03:01:24.800 --> 03:01:26.560]   and they're not always the only choices,
[03:01:26.560 --> 03:01:29.560]   but those are choices, I think can be helpful to say,
[03:01:29.560 --> 03:01:34.560]   okay, am I checking in out,
[03:01:34.560 --> 03:01:36.040]   'cause I'm having a little bump,
[03:01:36.040 --> 03:01:37.000]   and I'm feeling it emotional,
[03:01:37.000 --> 03:01:38.520]   and I'm just gonna give up too soon?
[03:01:38.520 --> 03:01:40.720]   Okay, ask yourself that question.
[03:01:40.720 --> 03:01:42.920]   And also, it's like, am I being pigheaded
[03:01:42.920 --> 03:01:45.240]   and trying to do something that actually doesn't make sense?
[03:01:45.240 --> 03:01:46.360]   Okay, ask yourself that question too,
[03:01:46.360 --> 03:01:48.560]   even though they're contradictory questions.
[03:01:48.560 --> 03:01:51.600]   Sometimes it'll be one, sometimes it'll be the other,
[03:01:51.600 --> 03:01:53.520]   and you gotta really think it through.
[03:01:53.520 --> 03:01:55.440]   - I think persisting with the business model
[03:01:55.520 --> 03:01:59.960]   behind Wikipedia is such an inspiring story,
[03:01:59.960 --> 03:02:03.120]   because we live in a capitalist world.
[03:02:03.120 --> 03:02:07.040]   We live in a scary world, I think,
[03:02:07.040 --> 03:02:08.460]   for an internet business.
[03:02:08.460 --> 03:02:12.440]   And so to do things differently
[03:02:12.440 --> 03:02:14.200]   than a lot of websites are doing,
[03:02:14.200 --> 03:02:18.120]   like Wikipedia has lived through the successive explosion
[03:02:18.120 --> 03:02:20.880]   of many websites that are basically ad-driven.
[03:02:20.880 --> 03:02:22.640]   Google is ad-driven.
[03:02:23.760 --> 03:02:26.840]   Facebook, Twitter, all of these websites are ad-driven.
[03:02:26.840 --> 03:02:31.840]   And to see them succeed, become these incredibly rich,
[03:02:31.840 --> 03:02:37.040]   powerful companies that if I could just have that money,
[03:02:37.040 --> 03:02:39.240]   you would think as somebody running Wikipedia,
[03:02:39.240 --> 03:02:41.120]   I could do so much positive stuff, right?
[03:02:41.120 --> 03:02:43.780]   And so to persist through that is,
[03:02:43.780 --> 03:02:46.980]   I think is, from my perspective now,
[03:02:46.980 --> 03:02:50.400]   Monday night quarterback or whatever,
[03:02:50.400 --> 03:02:53.440]   is the right decision.
[03:02:53.440 --> 03:02:55.640]   - But boy, is that a tough decision.
[03:02:55.640 --> 03:02:58.400]   - It seemed easy at the time, so.
[03:02:58.400 --> 03:02:59.920]   - And then you just kinda stay with it, stick with it.
[03:02:59.920 --> 03:03:01.800]   - Yeah, you just stay with it, it's working.
[03:03:01.800 --> 03:03:03.400]   - So on that one, you chose persistent.
[03:03:03.400 --> 03:03:07.360]   - Yeah, well, yeah.
[03:03:07.360 --> 03:03:11.400]   I mean, I always like to give an example of MySpace,
[03:03:11.400 --> 03:03:13.360]   'cause I just think it's an amusing story.
[03:03:13.360 --> 03:03:17.640]   So MySpace was poised, I would say, to be Facebook, right?
[03:03:17.640 --> 03:03:21.000]   It was huge, it was viral, it was lots of things.
[03:03:21.000 --> 03:03:23.400]   Kind of foreshadowed a bit of maybe even TikTok,
[03:03:23.400 --> 03:03:26.400]   because it was like a lot of entertainment content, casual.
[03:03:26.400 --> 03:03:30.920]   And then Rupert Murdoch bought it,
[03:03:30.920 --> 03:03:33.520]   and it collapsed within a few years.
[03:03:33.520 --> 03:03:36.280]   And part of that, I think, was because they were really,
[03:03:36.280 --> 03:03:39.280]   really heavy on ads and less heavy
[03:03:39.280 --> 03:03:40.800]   on the customer experience.
[03:03:40.800 --> 03:03:43.120]   So I remember to accept a friend request
[03:03:43.120 --> 03:03:45.880]   was like three clicks where you saw three ads.
[03:03:45.880 --> 03:03:47.880]   And on Facebook, you accept the friend request,
[03:03:47.880 --> 03:03:49.000]   you didn't even leave the page,
[03:03:49.000 --> 03:03:51.400]   it just, like, that just accepted.
[03:03:51.400 --> 03:03:53.680]   But what is interesting, so I used to give this example
[03:03:53.680 --> 03:03:56.840]   of like, yeah, well, Rupert Murdoch really screwed that one
[03:03:56.840 --> 03:03:58.640]   up, in a sense, maybe he did, but somebody said,
[03:03:58.640 --> 03:04:01.040]   you know what, actually, he bought it for,
[03:04:01.040 --> 03:04:02.040]   and I don't remember the numbers,
[03:04:02.040 --> 03:04:03.040]   he bought it for 800 million,
[03:04:03.040 --> 03:04:06.920]   and it was very profitable through its decline.
[03:04:06.920 --> 03:04:09.000]   He actually made his money back and more.
[03:04:09.000 --> 03:04:11.160]   So it wasn't, like, from a financial point of view,
[03:04:11.160 --> 03:04:12.680]   it was a bad investment in the sense of,
[03:04:12.680 --> 03:04:14.400]   you could have been Facebook,
[03:04:14.400 --> 03:04:17.000]   but on sort of more mundane metrics, it's like,
[03:04:17.000 --> 03:04:18.120]   actually it worked out okay for him.
[03:04:18.120 --> 03:04:20.120]   It all matters how you define success.
[03:04:20.120 --> 03:04:23.880]   - It does, and that is also advice to young people.
[03:04:23.880 --> 03:04:27.280]   One of the things I would say,
[03:04:27.280 --> 03:04:31.600]   like, when we have our mental models of success,
[03:04:31.600 --> 03:04:33.640]   as an entrepreneur, for example,
[03:04:33.640 --> 03:04:37.560]   and your examples in your mind are Bill Gates,
[03:04:37.560 --> 03:04:42.560]   Mark Zuckerberg, so people who, at a very young age,
[03:04:42.560 --> 03:04:45.480]   had one really great idea that just went straight
[03:04:45.480 --> 03:04:47.000]   to the moon and became one of the richest people
[03:04:47.000 --> 03:04:49.600]   in the world, that is really unusual,
[03:04:49.600 --> 03:04:52.040]   like, really, really rare.
[03:04:52.040 --> 03:04:53.520]   And for most entrepreneurs,
[03:04:53.520 --> 03:04:55.560]   that is not the life path you're gonna take.
[03:04:55.560 --> 03:04:57.640]   You're gonna fail, you're gonna reboot,
[03:04:57.640 --> 03:04:59.400]   you're gonna learn from what you failed at,
[03:04:59.400 --> 03:05:01.320]   you're gonna try something different.
[03:05:01.320 --> 03:05:02.760]   And that is really important,
[03:05:02.760 --> 03:05:05.880]   because if your standard of success is,
[03:05:05.880 --> 03:05:09.280]   well, I feel sad because I'm not as rich as Elon Musk,
[03:05:09.280 --> 03:05:13.000]   it's like, well, so should almost everyone,
[03:05:13.000 --> 03:05:14.760]   possibly everyone, except Elon Musk
[03:05:14.760 --> 03:05:16.040]   is not as rich as Elon Musk.
[03:05:16.040 --> 03:05:18.760]   And so that, like, realistically,
[03:05:18.760 --> 03:05:20.720]   you can set a standard of success,
[03:05:20.720 --> 03:05:24.160]   even in a really narrow sense, which I don't recommend,
[03:05:24.160 --> 03:05:27.640]   of thinking about your financial success.
[03:05:27.640 --> 03:05:30.040]   It's like, if you measure your financial success
[03:05:30.040 --> 03:05:35.040]   by thinking about billionaires, like, that's heavy,
[03:05:35.040 --> 03:05:37.280]   like, that's probably not good.
[03:05:37.280 --> 03:05:38.540]   I don't recommend it.
[03:05:38.540 --> 03:05:42.800]   Whereas, like, I personally, like, for me,
[03:05:42.800 --> 03:05:44.240]   when people, when journalists say,
[03:05:44.240 --> 03:05:45.760]   oh, how does it feel to not be a billionaire,
[03:05:45.760 --> 03:05:48.480]   I usually say, I don't know, how's it feel to you?
[03:05:48.480 --> 03:05:49.480]   'Cause they're not.
[03:05:49.480 --> 03:05:53.920]   But also, I'm like, I live in London.
[03:05:53.920 --> 03:05:57.000]   The number of bankers that no one's ever heard of
[03:05:57.000 --> 03:05:59.640]   who live in London, who make far more money than I ever will,
[03:05:59.640 --> 03:06:01.480]   is quite a large number.
[03:06:01.480 --> 03:06:05.000]   And I wouldn't trade my life for theirs at all, right?
[03:06:05.000 --> 03:06:06.760]   Because mine is so interesting.
[03:06:06.760 --> 03:06:11.120]   Like, oh, right, Jimmy, we need you to go
[03:06:11.120 --> 03:06:13.840]   and meet the Chinese propaganda minister.
[03:06:13.840 --> 03:06:15.600]   Oh, okay, that's super interesting.
[03:06:15.600 --> 03:06:18.600]   Like, yeah, Jimmy, you know, like, here's the situation.
[03:06:18.600 --> 03:06:20.320]   Like, you can go to this country,
[03:06:20.320 --> 03:06:24.040]   and while you're there, the president has asked to see you.
[03:06:24.040 --> 03:06:26.120]   It's like, God, that's super interesting.
[03:06:26.120 --> 03:06:28.600]   Jimmy, you're going to this place,
[03:06:28.600 --> 03:06:30.600]   and there's a local Wikipedian who said,
[03:06:30.600 --> 03:06:32.840]   do you wanna stay with me and my family?
[03:06:32.840 --> 03:06:35.000]   And I'm like, yeah, like, that's really cool.
[03:06:35.000 --> 03:06:36.120]   Like, I would like to do that.
[03:06:36.120 --> 03:06:38.240]   That's really interesting.
[03:06:38.240 --> 03:06:39.560]   I don't do that all the time, but I've done it,
[03:06:39.560 --> 03:06:40.400]   and it's great.
[03:06:40.400 --> 03:06:43.360]   So, like, for me, that's like arranging your life
[03:06:43.360 --> 03:06:48.360]   so that you have interesting experiences is just great.
[03:06:48.360 --> 03:06:51.560]   - Well, this is more to the question
[03:06:51.560 --> 03:06:54.120]   of what Wikipedia looks like in 1,000 years.
[03:06:54.120 --> 03:06:56.440]   What do you think is the meaning of this whole thing?
[03:06:56.440 --> 03:06:58.660]   Why are we here, human civilization?
[03:06:58.660 --> 03:07:00.280]   What's the meaning of life?
[03:07:00.280 --> 03:07:05.280]   - I don't think there is an external answer to that question.
[03:07:05.280 --> 03:07:08.560]   - And I should mention that there's a very good
[03:07:08.560 --> 03:07:10.920]   Wikipedia page on the different philosophies
[03:07:10.920 --> 03:07:11.760]   of the meaning of life.
[03:07:11.760 --> 03:07:12.580]   - Oh, interesting.
[03:07:12.580 --> 03:07:14.320]   - So, I'll have to read that and see what I think.
[03:07:14.320 --> 03:07:16.680]   Hopefully, it's neutral and gives a wide frame.
[03:07:16.680 --> 03:07:18.440]   - Oh, it's a really good reference
[03:07:18.440 --> 03:07:21.360]   to a lot of different philosophies about meaning.
[03:07:21.360 --> 03:07:23.840]   The 20th century philosophy in general,
[03:07:23.840 --> 03:07:26.440]   from Nietzsche to the existentialist,
[03:07:26.440 --> 03:07:29.360]   to all, some of the Brewer,
[03:07:29.360 --> 03:07:30.920]   all of them have an idea of meaning.
[03:07:30.920 --> 03:07:33.400]   They really struggle systematically, rigorously,
[03:07:33.400 --> 03:07:34.840]   and that's what the page,
[03:07:34.840 --> 03:07:37.080]   and obviously, a shout out to "The Hitchhiker's Guide"
[03:07:37.080 --> 03:07:37.920]   and all that kind of stuff.
[03:07:37.920 --> 03:07:38.960]   - Yeah, yeah, yeah.
[03:07:38.960 --> 03:07:41.800]   No, I think there's no external answer to that.
[03:07:41.800 --> 03:07:43.000]   I think it's internal.
[03:07:43.000 --> 03:07:48.000]   I think we decide what meaning we will have in our lives
[03:07:48.000 --> 03:07:50.300]   and what we're gonna do with ourselves.
[03:07:50.300 --> 03:07:53.920]   And so, when I think, you know,
[03:07:53.920 --> 03:07:57.320]   if we're talking about thousand years, millions of years,
[03:07:57.320 --> 03:08:01.860]   Uri Milner wrote a book.
[03:08:01.860 --> 03:08:03.920]   He's a big internet investor guy.
[03:08:03.920 --> 03:08:07.560]   He wrote a book advocating quite strongly
[03:08:08.680 --> 03:08:12.760]   for humans exploring the universe
[03:08:12.760 --> 03:08:14.360]   and getting off the planet.
[03:08:14.360 --> 03:08:16.600]   And he funds projects to like,
[03:08:16.600 --> 03:08:19.200]   send like using lasers to send little cameras
[03:08:19.200 --> 03:08:20.660]   and interesting stuff.
[03:08:20.660 --> 03:08:24.160]   And he talks a lot in the book about meaning.
[03:08:24.160 --> 03:08:27.560]   It's like his view is that the purpose of the human species
[03:08:27.560 --> 03:08:31.360]   is to broadly survive and get off the planet.
[03:08:31.360 --> 03:08:33.600]   Well, I don't agree with everything he has to say
[03:08:33.600 --> 03:08:35.440]   'cause I think that's not a meaning
[03:08:35.440 --> 03:08:38.000]   that can motivate most people in their own lives.
[03:08:38.000 --> 03:08:40.080]   It's like, okay, great.
[03:08:40.080 --> 03:08:42.080]   You know, like the distances of space
[03:08:42.080 --> 03:08:43.320]   are absolutely enormous.
[03:08:43.320 --> 03:08:45.640]   So, I don't know what,
[03:08:45.640 --> 03:08:48.160]   should we build generation ships to start flying places?
[03:08:48.160 --> 03:08:49.400]   Well, I can't do that.
[03:08:49.400 --> 03:08:51.720]   And I'm not, even if I could, even if I'm Elon Musk
[03:08:51.720 --> 03:08:53.880]   and I could devote all my wealth to build,
[03:08:53.880 --> 03:08:55.700]   I'll be dead on the ship on the way.
[03:08:55.700 --> 03:08:57.760]   So, is that really meaning?
[03:08:57.760 --> 03:09:00.800]   But I think it's really interesting to think about.
[03:09:00.800 --> 03:09:02.480]   And reading his little book,
[03:09:02.480 --> 03:09:04.480]   it's quite a short little book, reading his book,
[03:09:04.480 --> 03:09:06.880]   it made me, it did make me think about,
[03:09:06.880 --> 03:09:07.880]   wow, like this is big.
[03:09:07.880 --> 03:09:09.120]   Like, this is not what you think about
[03:09:09.120 --> 03:09:10.440]   in your day-to-day life.
[03:09:10.440 --> 03:09:13.920]   It's like, where is the human species going to be
[03:09:13.920 --> 03:09:15.680]   in 10 million years?
[03:09:15.680 --> 03:09:19.560]   And it does make you sort of turn back to Earth and say,
[03:09:19.560 --> 03:09:23.240]   gee, let's not destroy the planet.
[03:09:23.240 --> 03:09:26.360]   Like, we're stuck here for at least a while.
[03:09:26.360 --> 03:09:31.140]   And therefore, we should really think about sustainability.
[03:09:32.600 --> 03:09:37.000]   And I mean, one million year sustainability.
[03:09:37.000 --> 03:09:38.240]   And we don't have all the answers.
[03:09:38.240 --> 03:09:40.040]   We have nothing close to the answers.
[03:09:40.040 --> 03:09:43.320]   I'm actually excited about AI in this regard,
[03:09:43.320 --> 03:09:44.660]   while also bracketing.
[03:09:44.660 --> 03:09:45.960]   Yeah, I understand there's also risks
[03:09:45.960 --> 03:09:48.360]   and people are terrified of AI.
[03:09:48.360 --> 03:09:50.320]   But I actually think it is quite interesting,
[03:09:50.320 --> 03:09:54.200]   this moment in time that we may have in the next 50 years
[03:09:54.200 --> 03:10:00.080]   to really, really solve some really long-term human problems,
[03:10:00.080 --> 03:10:02.120]   for example, in health.
[03:10:02.120 --> 03:10:06.640]   Like, the progress that's being made in cancer treatment,
[03:10:06.640 --> 03:10:08.960]   because we are able to, at scale,
[03:10:08.960 --> 03:10:15.200]   model molecules and genetics and things like this,
[03:10:15.200 --> 03:10:16.960]   it gets huge, it's really exciting.
[03:10:16.960 --> 03:10:22.640]   So if we can hang on for a little while,
[03:10:22.640 --> 03:10:27.280]   and certain problems that seem completely intractable today,
[03:10:27.280 --> 03:10:30.200]   like climate change, may end up being actually not that hard.
[03:10:30.200 --> 03:10:33.360]   And we just might be able to alleviate
[03:10:33.360 --> 03:10:35.760]   the full diversity of human suffering.
[03:10:35.760 --> 03:10:36.900]   - For sure, yeah.
[03:10:36.900 --> 03:10:41.480]   - And in so doing, help increase the chance
[03:10:41.480 --> 03:10:44.360]   that we can propagate the flame of human consciousness
[03:10:44.360 --> 03:10:46.640]   out towards the stars.
[03:10:46.640 --> 03:10:51.240]   And I think another important one, if we fail to do that,
[03:10:51.240 --> 03:10:54.400]   for me, is propagating and maintaining
[03:10:54.400 --> 03:10:58.760]   the full diversity and richness and complexity
[03:10:58.760 --> 03:11:02.240]   and expansiveness of human knowledge.
[03:11:02.240 --> 03:11:03.960]   So if we destroy ourselves,
[03:11:03.960 --> 03:11:07.440]   it would make me feel a little bit okay.
[03:11:07.440 --> 03:11:08.520]   - Yeah, you just--
[03:11:08.520 --> 03:11:09.360]   - If the human knowledge--
[03:11:09.360 --> 03:11:12.520]   - Just triggered me to say something really interesting,
[03:11:12.520 --> 03:11:16.860]   which is, when we talked earlier about translating
[03:11:16.860 --> 03:11:18.920]   and using machines to translate,
[03:11:18.920 --> 03:11:21.480]   we mostly talked about small languages
[03:11:21.480 --> 03:11:22.760]   and translating into English.
[03:11:22.760 --> 03:11:24.320]   But I always like to tell this story
[03:11:24.320 --> 03:11:26.840]   of something inconsequential, really.
[03:11:26.840 --> 03:11:30.920]   But there's, I was in Norway, in Bergen, Norway,
[03:11:30.920 --> 03:11:32.880]   where every year they've got this annual festival
[03:11:32.880 --> 03:11:37.600]   called Buekorp, which is young groups drumming,
[03:11:37.600 --> 03:11:39.080]   and they have a drumming competition.
[03:11:39.080 --> 03:11:40.640]   It's the 17 sectors of the city,
[03:11:40.640 --> 03:11:42.280]   and they've been doing it for a couple hundred years
[03:11:42.280 --> 03:11:43.560]   or whatever.
[03:11:43.560 --> 03:11:48.040]   They wrote about it in the three languages of Norway.
[03:11:48.040 --> 03:11:50.720]   And then from there, it was translated into English,
[03:11:50.720 --> 03:11:53.040]   into German, et cetera, et cetera.
[03:11:53.040 --> 03:11:55.320]   And so what I love about that story
[03:11:55.320 --> 03:11:57.040]   is what it reminds me is,
[03:11:57.040 --> 03:12:00.800]   this machine translation goes both ways.
[03:12:00.800 --> 03:12:02.800]   And when you talk about the richness
[03:12:02.800 --> 03:12:04.900]   and broadness of human culture,
[03:12:04.900 --> 03:12:08.320]   we're already seeing some really great pieces of this.
[03:12:08.320 --> 03:12:12.120]   So, like Korean soap operas, really popular,
[03:12:12.120 --> 03:12:13.880]   not with me, but with people.
[03:12:13.880 --> 03:12:17.280]   And the ability to, you know,
[03:12:17.280 --> 03:12:20.480]   imagine taking a very famous, very popular,
[03:12:20.480 --> 03:12:23.840]   very well-known Korean drama,
[03:12:23.840 --> 03:12:26.240]   and now, I mean, and I literally mean now,
[03:12:26.240 --> 03:12:29.080]   we're just about there technologically,
[03:12:29.080 --> 03:12:33.400]   where we use a machine to re-dub it in English
[03:12:33.400 --> 03:12:37.040]   in an automated way, including digitally editing the faces
[03:12:37.040 --> 03:12:38.920]   so it doesn't look dubbed.
[03:12:38.920 --> 03:12:43.640]   And so suddenly you say, oh, wow, like here's a piece of,
[03:12:43.640 --> 03:12:46.500]   you know, it's the Korean equivalent of,
[03:12:46.500 --> 03:12:48.160]   maybe it's "Friends" as a comedy,
[03:12:48.160 --> 03:12:50.760]   or maybe it's "Succession" just to be very contemporary.
[03:12:50.760 --> 03:12:52.680]   It's something that really impacted a lot of people,
[03:12:52.680 --> 03:12:53.520]   and they really loved it.
[03:12:53.520 --> 03:12:56.120]   And we have literally no idea what it's about.
[03:12:56.120 --> 03:13:00.760]   And suddenly it's like, wow, you know, like music,
[03:13:00.760 --> 03:13:03.680]   street music from wherever in the world
[03:13:03.680 --> 03:13:08.200]   can suddenly become accessible to us all in new ways.
[03:13:08.200 --> 03:13:09.680]   It's so cool.
[03:13:09.680 --> 03:13:11.520]   - It's really exciting to get access
[03:13:11.520 --> 03:13:14.680]   to the richness of culture in China,
[03:13:14.680 --> 03:13:18.960]   in the many different subcultures of Africa, South America.
[03:13:18.960 --> 03:13:21.320]   - One of my unsuccessful arguments
[03:13:21.320 --> 03:13:25.680]   with the Chinese government is by blocking Wikipedia,
[03:13:25.680 --> 03:13:28.480]   right, you aren't just stopping people in China
[03:13:28.480 --> 03:13:31.040]   from reading Chinese Wikipedia
[03:13:31.040 --> 03:13:32.440]   and other language versions of Wikipedia,
[03:13:32.440 --> 03:13:34.880]   you're also preventing the Chinese people
[03:13:34.880 --> 03:13:36.880]   from telling their story.
[03:13:36.880 --> 03:13:40.560]   So is there a small festival in a small town in China
[03:13:40.560 --> 03:13:42.360]   like Buekor?
[03:13:42.360 --> 03:13:44.020]   I don't know, but by the way,
[03:13:44.020 --> 03:13:45.880]   the people who live in that village,
[03:13:45.880 --> 03:13:48.360]   that small town of 50,000,
[03:13:48.360 --> 03:13:49.720]   they can't put that in Wikipedia
[03:13:49.720 --> 03:13:51.480]   and get it translated into other places.
[03:13:51.480 --> 03:13:54.280]   They can't share their culture and their knowledge.
[03:13:54.280 --> 03:13:55.400]   And I think for China,
[03:13:55.400 --> 03:13:57.600]   this should be a somewhat influential argument
[03:13:57.600 --> 03:14:00.600]   because China does feel misunderstood in the world.
[03:14:00.600 --> 03:14:02.040]   It's like, okay, well, there's one way,
[03:14:02.040 --> 03:14:06.040]   if you wanna help people understand, put it in Wikipedia.
[03:14:06.040 --> 03:14:08.360]   That's what people go to when they wanna understand.
[03:14:08.360 --> 03:14:13.260]   - And give the amazing, incredible people of China a voice.
[03:14:13.260 --> 03:14:14.700]   - Exactly.
[03:14:14.700 --> 03:14:16.040]   - Jimmy, thank you so much.
[03:14:16.040 --> 03:14:17.920]   I'm such a huge fan of everything you've done.
[03:14:17.920 --> 03:14:18.760]   - Oh, thank you.
[03:14:18.760 --> 03:14:19.800]   - Yeah, it's really great.
[03:14:19.800 --> 03:14:23.040]   - I'm deeply, deeply, deeply, deeply grateful for Wikipedia.
[03:14:23.040 --> 03:14:24.800]   I love it, it brings me joy.
[03:14:24.800 --> 03:14:27.640]   I donate all the time, you should donate too.
[03:14:27.640 --> 03:14:29.120]   It's a huge honor to finally talk with you.
[03:14:29.120 --> 03:14:30.500]   It's just amazing.
[03:14:30.500 --> 03:14:31.520]   Thank you so much for today.
[03:14:31.520 --> 03:14:32.680]   - Thanks for having me.
[03:14:32.680 --> 03:14:35.080]   - Thanks for listening to this conversation
[03:14:35.080 --> 03:14:36.280]   with Jimmy Wales.
[03:14:36.280 --> 03:14:37.400]   To support this podcast,
[03:14:37.400 --> 03:14:40.440]   please check out our sponsors in the description.
[03:14:40.440 --> 03:14:43.000]   And now, let me leave you with some words
[03:14:43.000 --> 03:14:46.320]   from the world historian Daniel Borsten.
[03:14:47.520 --> 03:14:50.360]   The greatest enemy of knowledge is not ignorance.
[03:14:50.360 --> 03:14:53.160]   It is the illusion of knowledge.
[03:14:53.160 --> 03:14:57.720]   Thank you for listening, and hope to see you next time.
[03:14:57.720 --> 03:15:00.300]   (upbeat music)
[03:15:00.300 --> 03:15:02.880]   (upbeat music)
[03:15:02.880 --> 03:15:12.880]   [BLANK_AUDIO]

