
[00:00:00.000 --> 00:00:04.720]   A book that Trenton recommended, The Symbolic Species, has this really interesting argument.
[00:00:04.720 --> 00:00:10.800]   When we just think of language as this contingent and maybe sub-optimal way to represent ideas,
[00:00:10.800 --> 00:00:15.360]   maybe one of the reasons that LLMs have succeeded is because language has evolved
[00:00:15.360 --> 00:00:21.440]   for tens of thousands of years to be this sort of cast in which young minds can develop.
[00:00:21.440 --> 00:00:26.320]   Certainly when you talk to multimodal or computer vision researchers versus when you talk to
[00:00:26.320 --> 00:00:31.120]   language model researchers, people who work in other modalities have to put enormous amounts
[00:00:31.120 --> 00:00:35.600]   of thought into exactly what the right representation space for the images is.
[00:00:35.600 --> 00:00:38.080]   Understanding the right level of representation there, really hard.
[00:00:38.080 --> 00:00:40.960]   In language, people are just like, "Well, I guess you just predict the next token."
[00:00:40.960 --> 00:00:43.220]   (laughing)

