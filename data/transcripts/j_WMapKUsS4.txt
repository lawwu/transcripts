
[00:00:00.000 --> 00:00:01.600]   Do you literally use GPT-3?
[00:00:01.600 --> 00:00:02.240]   We do.
[00:00:02.240 --> 00:00:08.640]   And in fact, we just published a paper about it, we won the best paper
[00:00:08.640 --> 00:00:10.760]   award at one of the works of ACL.
[00:00:10.760 --> 00:00:17.320]   In that particular case, we were using GPT-3 for generating training
[00:00:17.320 --> 00:00:19.360]   data for language summarization.
[00:00:19.360 --> 00:00:23.640]   So that's an interesting approach.
[00:00:23.640 --> 00:00:28.200]   I think one that I know several people are following in different domains, but
[00:00:28.200 --> 00:00:35.200]   instead of using GPT-3 directly at inference time to use it as a way to
[00:00:35.200 --> 00:00:40.440]   enhance and generate high volumes of training data with different priming
[00:00:40.440 --> 00:00:44.120]   mechanisms, it's a very interesting approach.
[00:00:44.120 --> 00:00:49.000]   And one that we showed in our publication that it's actually better than just
[00:00:49.000 --> 00:00:52.800]   having a lot of humans generating training data.
[00:00:52.800 --> 00:00:54.440]   So that's an interesting...
[00:00:54.440 --> 00:00:55.920]   Can you tell me more about how this works?
[00:00:55.920 --> 00:01:00.840]   How do you exactly generate the data and what's the summarization task?
[00:01:00.840 --> 00:01:10.520]   Yeah, it is a summarization task and summarization of medical conversations
[00:01:10.520 --> 00:01:16.520]   is pretty hard because you need to generate the data, but also you need to
[00:01:16.520 --> 00:01:19.440]   generate data that is...
[00:01:19.440 --> 00:01:24.280]   Sorry, you need to have the original data, but then generate summaries and you
[00:01:24.280 --> 00:01:29.480]   need to generate summaries and examples of summaries which are mostly correct,
[00:01:29.480 --> 00:01:34.320]   but some that might be incorrect to sort of also make decisions on where
[00:01:34.320 --> 00:01:35.400]   you're training the model.
[00:01:35.400 --> 00:01:40.080]   It has to learn what is a good medical summarization and what's
[00:01:40.080 --> 00:01:41.720]   a bad medical summarization.
[00:01:41.720 --> 00:01:50.240]   So in the case of this project, what we did is prime GPT-3 with a number of
[00:01:50.240 --> 00:01:58.480]   examples of both positive and negative summaries to conversations, and then
[00:01:58.480 --> 00:02:03.440]   have it generate thousands of different training examples that we use to
[00:02:03.440 --> 00:02:06.400]   train our own offline model.
[00:02:06.400 --> 00:02:14.000]   And interestingly, the availability of more data, but also more nuanced
[00:02:14.000 --> 00:02:19.560]   variabilities that GPT-3 was generating itself was made that the final model
[00:02:19.560 --> 00:02:23.440]   that we were training was better than anything that we could have trained
[00:02:23.440 --> 00:02:26.720]   with our own data and our own human labelers.
[00:02:26.720 --> 00:02:31.360]   If you're enjoying Gradient Descent, I'd really love for you to check out
[00:02:31.360 --> 00:02:35.480]   Fully Connected, which is an inclusive machine learning community that we're
[00:02:35.480 --> 00:02:40.000]   building to let everyone know about all the stuff going on in ML and all the
[00:02:40.000 --> 00:02:41.280]   new research coming out.
[00:02:41.280 --> 00:02:46.520]   If you go to wmb.ai/fc, you can see all the different stuff that we do,
[00:02:46.520 --> 00:02:50.560]   including Gradient Descent, but also salons where we talk about new research
[00:02:50.560 --> 00:02:54.960]   and folks share insights, AMAs where you can directly connect with members of
[00:02:54.960 --> 00:02:59.520]   our community, and a Slack channel where you can get answers to everything from
[00:02:59.520 --> 00:03:05.360]   very basic questions about ML to bug reports on weights and biases to how to
[00:03:05.360 --> 00:03:06.400]   hire an ML team.
[00:03:06.400 --> 00:03:08.040]   We're looking forward to meeting you.

