
[00:00:00.000 --> 00:00:06.500]   This is a fun book to read because then you just mentioned in there what the original sources to read to you
[00:00:06.500 --> 00:00:12.220]   You know, it's like the it's like the herald bloom of economics, right? You just like it's a book written for smart people
[00:00:12.220 --> 00:00:16.580]   Okay, so let's just jump into it. Okay book we're talking about is
[00:00:16.580 --> 00:00:22.520]   Goat who is the greatest economist of all time and why does it matter? All right, let's start with Keynes
[00:00:22.520 --> 00:00:24.880]   So in the section in Keynes
[00:00:25.720 --> 00:00:32.340]   You quote him. I think talking about Robert Marshall. He says Alfred Marshall. Oh, sorry, Alfred Marshall. He says
[00:00:32.340 --> 00:00:37.800]   The master economist must possess a rare combination of gifts. He must be a mathematician
[00:00:37.800 --> 00:00:40.640]   historian statesman philosopher
[00:00:40.640 --> 00:00:43.160]   No part of man's nature
[00:00:43.160 --> 00:00:50.400]   Man's nature or his institutions must lie entirely outside his regard and you say well Keynes is obviously talking about himself
[00:00:50.600 --> 00:00:55.860]   Because he was all those things and he was arguably the only person who was all those things at the time
[00:00:55.860 --> 00:00:59.040]   He must have known that. Okay. Well, you know what? I'm gonna ask now wait
[00:00:59.040 --> 00:01:04.880]   So, what should we make of Tyler Cowen citing Keynes
[00:01:04.880 --> 00:01:10.540]   Using this quote a quote that also applies to Tyler Cowen. I don't think it applies to me
[00:01:10.540 --> 00:01:13.640]   What's the exact list again? Am I a statesman?
[00:01:13.640 --> 00:01:19.160]   Did I surely all at the Treaty of Versailles or something comparable? I don't know. We're in Washington
[00:01:19.160 --> 00:01:21.160]   I'm sure you talked to all the people who matter quite a bit
[00:01:21.160 --> 00:01:24.880]   Well, I guess I'm more of a statesman than most economists
[00:01:24.880 --> 00:01:30.320]   But I don't come close to Keynes in the breadth of his high-level achievement in each of those areas
[00:01:30.320 --> 00:01:37.840]   Hmm. Okay. Let's talk about them those achievements. So chapter 12 general theory of interest employment and money
[00:01:37.840 --> 00:01:39.400]   Here's a quote
[00:01:39.400 --> 00:01:44.840]   It is probable that the actual average result of investments even during periods of progress and prosperity
[00:01:44.840 --> 00:01:47.520]   Have disappointed the hopes which promoted them
[00:01:48.080 --> 00:01:55.720]   if human nature felt no temptation to take a chance no satisfaction profit apart in constructing a factory a railway a mine or a farm
[00:01:55.720 --> 00:01:59.860]   There might not be much investment merely as a result of cold calculation
[00:01:59.860 --> 00:02:06.040]   Now it's a fascinating idea that investment is irrational or most investment throughout history has been irrational
[00:02:06.040 --> 00:02:12.200]   But when we think today about the fact that active investing exists, you know for winter's curse like reasons
[00:02:12.200 --> 00:02:15.480]   VCS probably make on average less returns than the market
[00:02:16.400 --> 00:02:19.180]   There's a whole bunch of different examples you can go through right M&A
[00:02:19.180 --> 00:02:25.440]   Usually doesn't achieve this energy as it expects throughout history has more most investment been selfishly irrational
[00:02:25.440 --> 00:02:29.280]   Well, Adam Smith was the first one. I know to have made this point that
[00:02:29.280 --> 00:02:33.120]   Projectors, I think he called them are overly optimistic
[00:02:33.120 --> 00:02:39.840]   So people who do startups are overly optimistic people who have well-entrenched VC franchises
[00:02:39.840 --> 00:02:44.680]   Make a lot of money and there's some kind of bifurcation in the distribution, right?
[00:02:44.680 --> 00:02:49.160]   Then there's a lot of others who are just playing at it and maybe hoping to break even
[00:02:49.160 --> 00:02:51.760]   so
[00:02:51.760 --> 00:02:58.200]   the rate of return on private investment if you include small businesses, it's highly skewed and
[00:02:58.200 --> 00:03:04.240]   Just a few percent of the people doing this make anything at all. So there's a lot to what Cain said
[00:03:04.240 --> 00:03:08.560]   I don't think he described it adequately in terms of a probability distribution
[00:03:08.560 --> 00:03:13.120]   But then again, he probably didn't have the data, but I wouldn't reject it out of hand. Hmm
[00:03:13.280 --> 00:03:15.280]   another example here is
[00:03:15.280 --> 00:03:22.760]   This is something your colleague Alex. I brought talks about a lot. Is that innovators don't internalize most of the gains to give to society?
[00:03:22.760 --> 00:03:27.360]   So here's another example where you know, the entrepreneur compared to one of his first employees
[00:03:27.360 --> 00:03:30.120]   Is he that much better off for taking the extra risk and working that much harder?
[00:03:30.120 --> 00:03:33.000]   What does this tell us about?
[00:03:33.000 --> 00:03:38.200]   Is it is a marvelous insight that you know, we're actually more risk-seeking than it's selfishly
[00:03:39.480 --> 00:03:44.760]   Selfishly good for us. That was Reuven Brenner's claim in some of his books on risk again
[00:03:44.760 --> 00:03:48.040]   I think you have to distinguish between different parts of the distribution
[00:03:48.040 --> 00:03:53.080]   So it seems there's a very large number of people who foolishly start small businesses
[00:03:53.080 --> 00:03:59.560]   Maybe they overly value autonomy when they ought to just get a job with a relatively stable company
[00:03:59.560 --> 00:04:02.360]   So they're part of the thesis is correct
[00:04:02.360 --> 00:04:07.980]   And I doubt if there's really big social returns to whatever those people do even if they could make a go of it
[00:04:08.800 --> 00:04:13.600]   but there's another part of the distribution people who are actually innovating or have
[00:04:13.600 --> 00:04:20.340]   Realistic prospects of doing so where I do think those social returns are very high now that 2% figure
[00:04:20.340 --> 00:04:25.080]   That's cited a lot. I don't think it's really based in much real
[00:04:25.080 --> 00:04:28.400]   It's maybe not a crazy seat-of-the-pants estimate
[00:04:28.400 --> 00:04:34.020]   But people think like oh, we know it's 2% and we really don't so you look at Picasso, right? He
[00:04:34.020 --> 00:04:37.920]   Helped generate cubism with Brock and some other artists
[00:04:38.520 --> 00:04:44.380]   How good is our estimate of Picasso's income compared to the spinoffs from Picasso?
[00:04:44.380 --> 00:04:50.360]   We just don't really know right? We don't know. It's 2% It could be 1% It could be 6%
[00:04:50.360 --> 00:04:53.040]   How different do you think it is an art versus?
[00:04:53.040 --> 00:04:58.760]   I don't know entrepreneurship versus different kinds of entrepreneurship. There's different industries there as well, right?
[00:04:58.760 --> 00:05:05.400]   I'm not sure. It's that different. So say if some people start blogging a lot of people copy them, right?
[00:05:05.400 --> 00:05:09.120]   Well, some people start painting in a particular style a lot of people copy them
[00:05:09.120 --> 00:05:12.440]   I'm not saying the numbers are the same, but they don't sound like
[00:05:12.440 --> 00:05:17.800]   Issues that in principle are so different into 2% overestimate or underestimate it might be wrong
[00:05:17.800 --> 00:05:23.320]   But in which way is it wrong? My seat-of-the-pants estimate would be 2 to 5% So I think it's pretty close
[00:05:23.320 --> 00:05:27.640]   But again, that's not based on anything firm. Here's another quote from Keynes
[00:05:27.640 --> 00:05:35.180]   Investment based on genuine long-term expectation is so difficult as to be scarcely practicable
[00:05:35.420 --> 00:05:42.140]   He who attempts it must surely lead much more laborious days and run greater risk than he who tries to guess better than the crowd
[00:05:42.140 --> 00:05:44.340]   How the crowd will behave?
[00:05:44.340 --> 00:05:52.220]   So one way to look at this is like, oh, he just doesn't understand efficient market hypothesis. It's like before random walks or something, but
[00:05:52.220 --> 00:05:55.300]   there's things you can see in the market today where
[00:05:55.300 --> 00:06:01.700]   You know are the prospects for future dividends so much higher after Koba than they were immediately after the crash
[00:06:03.660 --> 00:06:07.820]   How much a market behavior can be explained by these sorts of claims from Keynes
[00:06:07.820 --> 00:06:11.240]   I think Keynes had the view that for his time
[00:06:11.240 --> 00:06:17.700]   you could be a short-run speculator and in fact beat the markets and he believed that he did so and
[00:06:17.700 --> 00:06:23.100]   At least he did for some periods of his life. That may have been luck or maybe he did have special insight
[00:06:23.100 --> 00:06:30.460]   It probably wasn't true in general that we don't really know did efficient markets hold during, you know, Britain at that time
[00:06:30.500 --> 00:06:34.180]   Maybe there just were profit opportunities for smarter than average people
[00:06:34.180 --> 00:06:41.260]   So that's a view I'm inclined not to believe it but again, I don't think it's absurd
[00:06:41.260 --> 00:06:49.380]   Keynes is saying for people who want money. There's this bias toward the short term. You can get your profits and get out and
[00:06:49.380 --> 00:06:54.580]   That's damaging long-term investment, which in fact he wanted to socialize
[00:06:54.980 --> 00:07:00.620]   So he's being led to a very bad place by the argument. But again, we shouldn't dismiss it out of hand
[00:07:00.620 --> 00:07:03.800]   Why is it not easy to retrospectively study?
[00:07:03.800 --> 00:07:08.620]   What how efficient markets were back then in the same way we can study it now in terms of like oh you look at
[00:07:08.620 --> 00:07:16.620]   The price earnings ratios and then what were the dividends afterwards over the coming decades for those companies based on their stock price or something?
[00:07:16.620 --> 00:07:21.060]   I don't know how many publicly traded firms there were in Britain at that time
[00:07:21.060 --> 00:07:26.180]   I don't know how good the data are things like bid ask spread at what price you actually
[00:07:26.180 --> 00:07:30.460]   Executed trades can really matter for testing efficient markets hypothesis
[00:07:30.460 --> 00:07:32.820]   so
[00:07:32.820 --> 00:07:38.480]   Probably we can't tell even though there must be share price data of some sort at what frequency
[00:07:38.480 --> 00:07:41.100]   Well, is it once a day? Is it once a week?
[00:07:41.100 --> 00:07:46.940]   We don't have the sort of data we have now where you can just test anything you want. Hmm
[00:07:47.180 --> 00:07:51.660]   He also made a interesting point is like not only is it not profitable, but even if you succeed
[00:07:51.660 --> 00:07:56.360]   Society will look at the contrarian in a very negative light
[00:07:56.360 --> 00:07:59.860]   You will be like doubly punished for being a contrarian
[00:07:59.860 --> 00:08:01.660]   But that doesn't seem to be the case, right?
[00:08:01.660 --> 00:08:05.540]   Like you have somebody like Warren Buffett or Charlie Munger people who do beat the market are actually pretty revered
[00:08:05.540 --> 00:08:10.460]   They're not punished in public opinion and they pursued mostly long-term strategies, right?
[00:08:10.460 --> 00:08:16.860]   But again trying to make sense of Keynes if you think about long-term investing and I don't think he meant Buffett style investing
[00:08:16.860 --> 00:08:23.480]   I think he meant building factories trying to figure out what people would want to buy 25 years from that point in time
[00:08:23.480 --> 00:08:30.220]   That probably was much harder than today. So you had way less access to data
[00:08:30.220 --> 00:08:32.620]   your ability to
[00:08:32.620 --> 00:08:35.980]   build an international supply chain was much weaker a
[00:08:35.980 --> 00:08:42.620]   Geopolitical turmoil at various points in time was much higher. So again, it's not a crazy view
[00:08:42.620 --> 00:08:45.540]   I think there's a lot in Keynes. It's very much of his time
[00:08:46.180 --> 00:08:51.700]   That he presents out of a kind of overconfidence as being general and it's not general. It may not even be true
[00:08:51.700 --> 00:08:57.020]   But there were some reasons why you could believe it. Mm-hmm another quote from Keynes
[00:08:57.020 --> 00:09:01.380]   I guess I won't read the whole quote in full but basically says over time as
[00:09:01.380 --> 00:09:07.620]   Investments good or markets get more mature more and more of equities are held basically by passive investors
[00:09:07.620 --> 00:09:11.180]   People who don't have direct hand in the involvement of the enterprise
[00:09:11.660 --> 00:09:15.020]   And you know the the share of the market that's passive investment now is much bigger
[00:09:15.020 --> 00:09:20.860]   Should we be worried about this as long as at the margin people can do things? I'm not very worried about it
[00:09:20.860 --> 00:09:25.380]   So there's two different kinds of worries. One is that no one monitors the value of companies
[00:09:25.380 --> 00:09:30.340]   It seems to me those incentives aren't weaker. There's more research than ever before
[00:09:30.340 --> 00:09:33.860]   There's maybe a problem not enough companies are publicly held
[00:09:33.860 --> 00:09:39.020]   But you can always if you know something the rest of the market doesn't buy or sell short and do better
[00:09:39.460 --> 00:09:47.460]   The other worry is those passive investors have economies of scale and they'll end up colluding with each other and you'll have say like
[00:09:47.460 --> 00:09:53.820]   Three to five mutual funds private equity firms owning a big chunk of the market portfolio and in essence
[00:09:53.820 --> 00:09:59.060]   Directly or indirectly they'll tell those firms not to compete. It's a weird form of collusion
[00:09:59.060 --> 00:10:05.580]   They don't issue explicit instructions like say the same few mutual funds own coke and Pepsi
[00:10:05.980 --> 00:10:13.620]   Should coke and Pepsi compete or should they collude? Well, they might just pick lazier managers who in some way give you implicit collusion
[00:10:13.620 --> 00:10:16.140]   hmm, maybe this is another example of the
[00:10:16.140 --> 00:10:22.140]   Innovators being unable to internalize their games as an active investors who are providing this information in the market, you know
[00:10:22.140 --> 00:10:25.900]   They don't make out that much better than the passive investors, but they're actually providing a valuable service
[00:10:25.900 --> 00:10:31.620]   But this the benefits are diffused throughout society. I think overconfidence helps us on that front
[00:10:31.620 --> 00:10:34.980]   So there's quote-unquote too much trading from a private point of view
[00:10:34.980 --> 00:10:36.820]   But from a social point of view
[00:10:36.820 --> 00:10:41.900]   Maybe you can only have too much trading or too little trading and you might rather have too much trading
[00:10:41.900 --> 00:10:45.300]   Explain that why why is there why can it only be too much or too little?
[00:10:45.300 --> 00:10:50.120]   Well, let's say the relevant choice variable is investor temperament
[00:10:50.120 --> 00:10:56.580]   So yes, you'd prefer it if everyone had the temperament just to do what was socially optimal, but if temperament is some
[00:10:57.900 --> 00:11:03.580]   this inclination in you and you can just be overconfident or not confident enough and
[00:11:03.580 --> 00:11:08.620]   Overconfidence gives you too much trading that might be the best we can do again
[00:11:08.620 --> 00:11:15.180]   Fine-tuning would be best of all but I've never seen humans where you could just fine-tune all their emotions to the point where they ought
[00:11:15.180 --> 00:11:21.460]   To be yeah, okay, so we can ask the question. How far above optimal are we or if we are above optimal?
[00:11:21.460 --> 00:11:25.340]   In the chapter Kane says that over time as markets get more mature
[00:11:25.340 --> 00:11:30.980]   They become more speculative and the example he gives us a New York market seems more speculative to him than the London market at a time
[00:11:30.980 --> 00:11:33.980]   But today finances 8% of GDP
[00:11:33.980 --> 00:11:40.000]   Is that what we should expect it to be to have to efficiently allocate capital?
[00:11:40.000 --> 00:11:43.460]   Is there some reason we can just look at that number and flash you say that that's too big
[00:11:43.460 --> 00:11:48.760]   I think the relevant number for the financial sector is what percentage it is of wealth not GDP
[00:11:49.620 --> 00:11:55.340]   So you're managing wealth and the financial sector has been a pretty constant 2% of wealth
[00:11:55.340 --> 00:12:00.300]   For a few decades in the United States with bumps obviously 2008 matters
[00:12:00.300 --> 00:12:04.400]   But it's more or less 2% and that makes it sound a lot less sinister
[00:12:04.400 --> 00:12:08.260]   It's not actually like growing at the expense of something and eating up the economy
[00:12:08.260 --> 00:12:10.180]   so
[00:12:10.180 --> 00:12:15.740]   you you would prefer it's less than 2% right but 2% does not sound outrageously high to me and
[00:12:16.620 --> 00:12:23.600]   If the ratio of wealth to GDP grows over time, which it tends to do when you have durable capital and no major wars
[00:12:23.600 --> 00:12:29.500]   The financial sector will grow relative to GDP. But again, that's not sinister. Think of it in terms of wealth
[00:12:29.500 --> 00:12:30.300]   I see
[00:12:30.300 --> 00:12:35.500]   So one way to think about it is like the the management cost as a fraction of the assets under management or something
[00:12:35.500 --> 00:12:37.500]   And that's right. Yeah
[00:12:37.500 --> 00:12:38.900]   Yeah, okay interesting
[00:12:38.900 --> 00:12:43.180]   Um, I want to go back to the the risk aversion thing again because I don't know how to think about this
[00:12:43.180 --> 00:12:49.460]   So, you know his whole thing is these animal spirits. They guide us to make all these bets and engage in all this activity
[00:12:49.460 --> 00:12:52.300]   in some senses he's saying like
[00:12:52.300 --> 00:12:58.820]   Not only are we not risk-neutral, but we're more risk-seeking than is than is rational. Whereas
[00:12:58.820 --> 00:13:02.540]   The the way you'd conventionally think about it is that humans are risk averse, right?
[00:13:02.540 --> 00:13:06.460]   They they have they prefer to take less risk than is rational in some sense
[00:13:06.460 --> 00:13:12.300]   How do we score this? Well here Milton Friedman another goat contender comes into the picture
[00:13:12.300 --> 00:13:18.900]   So his famous piece with savage makes the point that risk aversion is essentially context dependent
[00:13:18.900 --> 00:13:22.700]   So he was a behavioral economist before we knew of such things
[00:13:22.700 --> 00:13:29.040]   so the same people typically will buy insurance and gamble gambling you can interpret quite broadly and
[00:13:29.040 --> 00:13:34.060]   That's the right way to think about it. So just flat-out risk aversion or risk-loving behavior
[00:13:34.060 --> 00:13:38.100]   It doesn't really exist. Almost everyone is context dependent
[00:13:38.100 --> 00:13:44.300]   Now why you choose the context you do maybe it's some kind of exercise in mood management
[00:13:44.300 --> 00:13:48.780]   So you ensure your house so you can sleep well at night you fire insurance
[00:13:48.780 --> 00:13:55.820]   But then you get a little bored and to stimulate yourself you're betting on these NBA games and yes, that's foolish
[00:13:55.820 --> 00:14:01.180]   but it keeps you busy and it helps you follow analytics and you read about the games online and
[00:14:01.180 --> 00:14:07.460]   Maybe that's efficient mood management and that's the way to think about risk behavior. I don't bet by the way
[00:14:07.460 --> 00:14:13.380]   I mean you could say I bet with my career, but I don't bet on things. Well, what is the way in what?
[00:14:13.380 --> 00:14:14.580]   What's your version of the lottery ticket?
[00:14:14.580 --> 00:14:20.060]   what is the thing where you just for the entertainment value or the distraction value you you take more risk than
[00:14:20.060 --> 00:14:22.580]   Then would seem rational
[00:14:22.580 --> 00:14:29.500]   Well like writing the book goat, which is not with any known publisher. It's just online. It's free
[00:14:29.500 --> 00:14:31.940]   It's published within GPT for
[00:14:31.940 --> 00:14:34.820]   Took me quite a while to write the book
[00:14:35.860 --> 00:14:41.060]   I'm not sure. There's a huge downside, but it's risky in the sense of it's not what anyone else was doing
[00:14:41.060 --> 00:14:42.340]   So that was a kind of risk
[00:14:42.340 --> 00:14:48.540]   I invested a lot of my writing time in something weird and I've done things like that pretty frequently
[00:14:48.540 --> 00:14:55.180]   So that keeps me you could say excited or like starting MRU, you know the online education videos and economics
[00:14:55.180 --> 00:14:59.900]   No pecuniary return to me at all indirectly. It costs me a lot of money
[00:14:59.900 --> 00:15:05.660]   That's a sort of risk. I feel it's paid off for me in a big way
[00:15:05.940 --> 00:15:10.660]   But on one hand you can say well Tyler, what do you actually have from that? And the answer is nothing
[00:15:10.660 --> 00:15:13.940]   Yeah, well, this actually raises the question
[00:15:13.940 --> 00:15:17.860]   I was gonna ask about these go contenders in general and how you're judging them where
[00:15:17.860 --> 00:15:20.580]   You're looking at their work as a whole
[00:15:20.580 --> 00:15:25.620]   Given that I don't know some of these risk payoff that these intellectuals take some of them don't pay off
[00:15:25.620 --> 00:15:30.940]   Should we just be looking at their top contributions and just disregard everything else for Hayek?
[00:15:30.940 --> 00:15:34.320]   I think one of the points you have against him is that his top three articles are amazing
[00:15:34.320 --> 00:15:36.320]   But after that, there's a drop-off
[00:15:36.320 --> 00:15:40.500]   The the top risk you take are the only ones that matter. Why are we looking at the other stuff?
[00:15:40.500 --> 00:15:46.400]   I don't think they're the only ones that matter but I'll weight them pretty heavily but your failures do reflect
[00:15:46.400 --> 00:15:49.220]   Usually and how you think or what you know about the world
[00:15:49.220 --> 00:15:51.780]   so Hayek's failures
[00:15:51.780 --> 00:15:55.380]   For instance his inability to come up with the normative standard in
[00:15:55.380 --> 00:15:57.620]   Constitution of Liberty
[00:15:57.620 --> 00:16:00.460]   It shows in some ways. He just wasn't rigorous enough
[00:16:00.660 --> 00:16:06.920]   he was content with the kind of Germanic put a lot of complex ideas out there and hope they're profound and
[00:16:06.920 --> 00:16:11.480]   You see that even in his best work now that is profound
[00:16:11.480 --> 00:16:18.240]   But it's not as if the failures and the best work for all these people are unrelated and same with Keynes like Keynes
[00:16:18.240 --> 00:16:20.320]   More or less changed his mind every year
[00:16:20.320 --> 00:16:28.620]   That's a strength, but it's also a weakness and by considering Keynes is really good works and bad works like his defenses of tariffs
[00:16:29.280 --> 00:16:31.460]   You see that and the best work
[00:16:31.460 --> 00:16:35.000]   He also moved on from in some way if you read how to pay for the war in
[00:16:35.000 --> 00:16:39.680]   1940 if you didn't know better you would think it's someone criticizing the general theory
[00:16:39.680 --> 00:16:43.040]   How?
[00:16:43.040 --> 00:16:48.260]   Does quantity have a quality of all of its own when you think of great intellectuals where many of these people have like volumes and
[00:16:48.260 --> 00:16:49.920]   volumes of work
[00:16:49.920 --> 00:16:55.360]   Was that necessary for them to get the greatest sets or is the rest of it just a distraction from the things that really stand?
[00:16:55.360 --> 00:17:01.880]   That's a time for the best people. It's necessary. So John Stuart Mill wrote an enormous amount most of its quite interesting
[00:17:01.880 --> 00:17:05.760]   But his ability to see things from multiple perspectives
[00:17:05.760 --> 00:17:12.440]   I think was in part stemming from the fact that he wrote a lot about many different topics like French history ancient Greece
[00:17:12.440 --> 00:17:15.080]   He had real depth and breadth
[00:17:15.080 --> 00:17:22.080]   Mm-hmm. If Keynes is alive today, what are the odds that he's in a polycule in Berkeley writing the best written less wrong posts?
[00:17:22.080 --> 00:17:24.080]   You've ever seen
[00:17:24.280 --> 00:17:28.400]   I'm not sure what the counterfactual means. So Keynes is so British
[00:17:28.400 --> 00:17:37.120]   Maybe he's an effective altruist at Cambridge and I you know, given how he seems to have run a sex life
[00:17:37.120 --> 00:17:40.160]   I don't think he needed a polycule like a polycule
[00:17:40.160 --> 00:17:45.320]   It's almost a Williamsonian device to economize on transactions costs
[00:17:45.320 --> 00:17:50.200]   But Keynes according to his own notes seems to have done things on a very casual basis
[00:17:50.200 --> 00:17:54.480]   He had a spreadsheet right of a spreadsheet and it from context it appears
[00:17:54.480 --> 00:18:00.280]   He met these people very casually right and didn't need to be embedded in where the five people who share regularly
[00:18:00.280 --> 00:18:05.400]   so that's not a hypothetical we saw we think we saw what he did and
[00:18:05.400 --> 00:18:07.480]   I
[00:18:07.480 --> 00:18:11.520]   Think he'd be at Cambridge, right? That's where he was. Why should he not today be at Cambridge?
[00:18:11.520 --> 00:18:14.080]   How did a person?
[00:18:14.080 --> 00:18:19.560]   How did a gay intellectual get that amount of influence in the Britain of that time when you think of somebody like Alan Turing?
[00:18:19.680 --> 00:18:26.080]   You know helps Britain win World War two and is castrated because of you know, one illicit encounter that is caught
[00:18:26.080 --> 00:18:32.960]   Was it just not public? How did he how did he get away with it? Basically? I don't think it was a secret about Keynes
[00:18:32.960 --> 00:18:39.560]   He had interacted with enough people that I think it was broadly known. He was politically very powerful
[00:18:39.560 --> 00:18:46.160]   He was astute as someone managing his career. He was one of the most effective people you could say of all time
[00:18:46.160 --> 00:18:51.960]   Not just amongst economists, and I've never seen evidence that Keynes was in any kind of danger
[00:18:51.960 --> 00:18:57.560]   Turing also may have intersected with national security concerns in a different way
[00:18:57.560 --> 00:19:03.240]   I'm not sure. We know the fall Alan Turing story and why it went as badly as it did
[00:19:03.240 --> 00:19:08.360]   But there was in the past very selectively and I do mean very selectively
[00:19:08.360 --> 00:19:15.760]   More tolerance of deviance than people today sometimes realize. Oh interesting since it's benefited from that
[00:19:15.760 --> 00:19:20.920]   But again, I would stress the word selectively the same or what determines who is selected for this tolerance
[00:19:20.920 --> 00:19:24.040]   I don't feel I understand that very well, but there's plenty
[00:19:24.040 --> 00:19:29.280]   Say in Europe and Britain of the early 20th century
[00:19:29.280 --> 00:19:34.600]   We're quote-unquote outrageous things were done and it's hard to find evidence that people were punished for it
[00:19:34.600 --> 00:19:40.160]   Now what accounts for the difference between them and the people who were punished? I would like to see a very good book on that
[00:19:40.160 --> 00:19:42.360]   hmm
[00:19:42.360 --> 00:19:46.280]   Yeah, I guess it's similar to our time right we have certain taboos and you can get away with
[00:19:46.280 --> 00:19:49.120]   Yeah, they say whatever on Twitter and other
[00:19:49.120 --> 00:19:54.560]   Actually, how have you got no I feel like you've never been in at least as far as I know
[00:19:54.560 --> 00:19:59.120]   I haven't heard you being in the part of any single controversy, but you have some opinions out there
[00:19:59.120 --> 00:20:03.320]   I feel people have been very nice to me. Yeah, would you what you do? How did you become the Keynes of our time?
[00:20:03.320 --> 00:20:06.040]   We're comparing
[00:20:10.240 --> 00:20:15.600]   I think just being good-natured. Yeah helps and helping a lot of people helps and
[00:20:15.600 --> 00:20:20.080]   Turing I'm a huge fan of wrote a paper on him with Michelle Dawson
[00:20:20.080 --> 00:20:24.440]   But it's not obvious that he was a very good diplomat and it seems
[00:20:24.440 --> 00:20:29.520]   He very likely was a pretty terrible diplomat and that might be feeding into this difference
[00:20:29.520 --> 00:20:32.800]   How do you think about the long-term value of?
[00:20:32.800 --> 00:20:39.240]   and the long-term impact of intellectuals you disagree with so do you think over the course of history basically the
[00:20:39.640 --> 00:20:45.280]   Improvements they make to the discourse and the additional things they give us a chance to think about that washes out their object level
[00:20:45.280 --> 00:20:48.240]   the things that were object level wrong about or
[00:20:48.240 --> 00:20:53.940]   Well, it's worked that way so far, right? So we've had economic growth obviously with interruptions
[00:20:53.940 --> 00:21:01.200]   But so much is fed into the stream and you have to be pretty happy with today's world compared with say 1880
[00:21:01.200 --> 00:21:05.640]   The future may or may not bring us the same but if the future brings us
[00:21:06.080 --> 00:21:11.000]   Continuing economic growth, but I'm gonna say exactly that. I'll be happy they fed into the stream
[00:21:11.000 --> 00:21:13.520]   They may have been wrong, but things really worked out
[00:21:13.520 --> 00:21:16.040]   but if the future brings us
[00:21:16.040 --> 00:21:20.320]   in a shrinking population asymptotically approaching a very low level and
[00:21:20.320 --> 00:21:27.840]   Greater poverty and more war then you've got to wonder well who is responsible for that, right? Who would be responsible for that? Oh
[00:21:27.840 --> 00:21:36.000]   We don't know but I think secular thinkers will fall in relative status if that's the outcome and that's most
[00:21:36.440 --> 00:21:43.700]   Prominent intellectuals today myself included. Yeah, who would rise in status as a result? Well, there's a number of people
[00:21:43.700 --> 00:21:47.000]   complaining strenuously about fertility declines
[00:21:47.000 --> 00:21:50.400]   If there's more war
[00:21:50.400 --> 00:21:57.400]   Probably the Hawks will rise in status whether or not they should an alternative scenario is that the pacifists rise in status
[00:21:57.400 --> 00:22:03.400]   But I basically never see the pacifists rising in status for any more than brief moments like after the Vietnam War
[00:22:03.400 --> 00:22:05.400]   Maybe they did after World War one
[00:22:06.200 --> 00:22:10.980]   Yes, but again that didn't last because World War two swept all that away, right?
[00:22:10.980 --> 00:22:14.920]   so the pacifists seem to lose long-term status no matter what and
[00:22:14.920 --> 00:22:22.080]   That means the Hawks would gain in status and those worried about fertility and whatever technology
[00:22:22.080 --> 00:22:27.000]   Drives the new wars if that is what happens. Let's say it's drones. It's possible, right?
[00:22:27.000 --> 00:22:29.520]   people who warned against drones
[00:22:29.520 --> 00:22:35.080]   Which is not currently that big a thing there are quite a few such people, but there's no one out there known
[00:22:36.080 --> 00:22:42.520]   For worrying about drones the way say Eliezer is worried about known for worrying about AI now drones in a way are AI
[00:22:42.520 --> 00:22:49.320]   But it's different. Yeah, although Nat Friedman's Jordan Armstrong other people have you know talked about we're not that far away from don't sickness
[00:22:49.320 --> 00:22:51.320]   I guess you have
[00:22:51.320 --> 00:23:02.200]   Many ways. Yeah, but those people could end up as much more important than they are now. Yeah. Okay. Let's talk about Hayek. Sure
[00:23:02.880 --> 00:23:05.080]   So I before I get into his actual views
[00:23:05.080 --> 00:23:10.400]   I think his career is a tremendous white pill in the sense that he writes the road to serfdom in 1944
[00:23:10.400 --> 00:23:15.120]   when Nazi Germany and Soviet Union are both, you know prominent players and
[00:23:15.120 --> 00:23:19.080]   Honestly, like the world the way things shaked out
[00:23:19.080 --> 00:23:25.160]   I mean, he would be pretty pleased that like a lot of the biggest collectivism's of the day have been
[00:23:25.160 --> 00:23:28.960]   Wiped out so it is a tremendous white bill. You can have a career like that
[00:23:28.960 --> 00:23:32.120]   he was
[00:23:32.760 --> 00:23:37.760]   Not as right as he thought at the time, but he ended up being too grumpy in his later years
[00:23:37.760 --> 00:23:38.160]   Oh, really?
[00:23:38.160 --> 00:23:43.040]   He thought well collectivism is still going to engulf the world and I think he became a grumpy old man
[00:23:43.040 --> 00:23:49.600]   And then maybe it's one thing to be a grumpy old man in 2024, but to be a grumpy old man in the 80s
[00:23:49.600 --> 00:23:58.680]   Didn't seem justified. What was the cause? Well, like what specifically did he see that he he thought there were atavistic instincts in the human
[00:23:58.680 --> 00:24:01.800]   Spirit which were biologically built in that led us to be
[00:24:01.800 --> 00:24:08.840]   Collectivists right and to envious and not appreciative of how impersonal orders worked and that this would cause the West
[00:24:08.840 --> 00:24:13.440]   To turn into something quite crummy. Hmm. I wouldn't say he's been proven wrong
[00:24:13.440 --> 00:24:16.960]   but a lot of the West has had a pretty good run since then and
[00:24:16.960 --> 00:24:24.660]   There's not major evidence that he's correct the bad events we've seen like some war coming back
[00:24:26.920 --> 00:24:30.160]   Something weird happening in our politics. I'm not sure how to describe it
[00:24:30.160 --> 00:24:35.720]   I'm not sure they fit the Hayek model a sort of simply the accretion of more socialism
[00:24:35.720 --> 00:24:41.380]   But in terms of the basic psychological urges towards enemy and resentment
[00:24:41.380 --> 00:24:48.000]   There's a rise of wokeness provide evidence for his view, but now wokeness I would say is peaked and is falling. That's a big debate
[00:24:48.000 --> 00:24:54.840]   Mm-hmm. I don't see wokeness as our biggest problem. I see excessive bureaucracy sclerotic institutions
[00:24:55.240 --> 00:25:01.680]   Bureaucracy. Yeah as bigger problems. They're not unrelated to openness to be clear, but I think they're more fundamental and harder to fix
[00:25:01.680 --> 00:25:08.600]   Let's talk about hikes hikes arguments. So obviously he has a famous argument about decentralization
[00:25:08.600 --> 00:25:13.320]   But when we look at companies like Amazon uber these other big tech companies
[00:25:13.320 --> 00:25:18.800]   They actually do a pretty good job of central planning, right? There's like a sea of logistics and drivers and
[00:25:18.800 --> 00:25:22.120]   Trade-offs that they had a square
[00:25:22.120 --> 00:25:24.400]   Do they provide evidence that central planning can work?
[00:25:25.080 --> 00:25:27.680]   Well, I'm not a Kosian so Kosan is famous
[00:25:27.680 --> 00:25:32.880]   1937 article said the firm is planning and he contrasted that to the market, right?
[00:25:32.880 --> 00:25:39.600]   I think the firm is the market the firm is always making contracts in the market is subject to market checks and balances to me
[00:25:39.600 --> 00:25:40.880]   It's not
[00:25:40.880 --> 00:25:44.280]   You know an island of central planning and the broader froth of the market
[00:25:44.280 --> 00:25:49.240]   So I'm just not cozy and so people are cozy and this is an embarrassing question for them
[00:25:49.240 --> 00:25:54.080]   But I'll just say Amazon being great is the market working and they're not centrally planning
[00:25:54.400 --> 00:26:00.000]   Even the Soviet Union it was very bad, but it didn't end up being central planning. It started off that way for a few years
[00:26:00.000 --> 00:26:06.840]   So I think people misinterpret large business firms in many ways on both the left and the right
[00:26:06.840 --> 00:26:09.520]   Wait, but under this argument
[00:26:09.520 --> 00:26:16.720]   it still adds to the credence of the people who argue that basically we need the government to control because if it is the case that
[00:26:16.720 --> 00:26:18.240]   Soviet Union is still
[00:26:18.240 --> 00:26:20.800]   Not central planning people would say well that but yeah
[00:26:20.800 --> 00:26:26.000]   But that's kind of what I want in terms of there's still kind of checks in terms of import exports of the market test
[00:26:26.000 --> 00:26:28.000]   It's still applied to the government in that sense
[00:26:28.000 --> 00:26:32.000]   What's wrong with that argument that basically to read the government as I get that kind of firm?
[00:26:32.000 --> 00:26:40.220]   I'm not sure. I've followed your question. I would say this I view the later Soviet Union as being highly decentralized
[00:26:40.220 --> 00:26:47.040]   Managers optimizing their own rents and setting prices too low to take in bribes a la Paul Craig Roberts
[00:26:47.040 --> 00:26:49.280]   what he wrote in the 80s and
[00:26:49.600 --> 00:26:56.120]   That's a very bad decentralized system and it was sort of backed up by something highly central communist party in the USSR
[00:26:56.120 --> 00:27:03.720]   But it's not like the early attempts at true central planning in the Soviet Union in the 20s right after the revolution
[00:27:03.720 --> 00:27:08.640]   Which did totally fail and were abandoned pretty quickly even by Lenin. Mm-hmm
[00:27:08.640 --> 00:27:15.280]   Would you kind of 50s period in the Soviet Union as more centrally planned then or more decentralized by that point decentralized?
[00:27:15.280 --> 00:27:17.280]   You have central plans for a number of things
[00:27:18.120 --> 00:27:25.960]   Obviously weaponry steel production you have targets, but even that tends to collapse into decentralized action just with bad incentives
[00:27:25.960 --> 00:27:29.760]   Mm-hmm. So your explanation for why did the Soviet Union have high growth in the 50s?
[00:27:29.760 --> 00:27:33.700]   Is it more catch-up? Is it more that they weren't communist at the time?
[00:27:33.700 --> 00:27:37.920]   How would you explain a lot of the Soviet high growth was rebuilding after the war?
[00:27:37.920 --> 00:27:44.800]   Which the central planning can do relatively well, right you see government rebuilding cities say in Germany that works pretty well
[00:27:45.200 --> 00:27:49.480]   But most of all this is even before World War two just urbanization
[00:27:49.480 --> 00:27:54.200]   It shouldn't be underrated today given that we've observed China
[00:27:54.200 --> 00:27:58.560]   But so much of Chinese growth was driven by urbanization so much of Soviet growth
[00:27:58.560 --> 00:28:04.520]   You take someone working on a farm producing almost nothing put them in the city even under a bad system
[00:28:04.520 --> 00:28:10.160]   They're going to be a lot more productive. Hmm, and that drove so much of Soviet growth before after the war
[00:28:10.600 --> 00:28:14.800]   But that at some point more or less ends right as it has well
[00:28:14.800 --> 00:28:19.560]   It hasn't quite ended with China, but it certainly slowed down and people don't pay enough attention to that
[00:28:19.560 --> 00:28:26.300]   I don't know why it now seems pretty obvious. Mm-hmm, but going back to the the point about firms, so I guess I
[00:28:26.300 --> 00:28:29.800]   The point I was trying to make is I don't understand why the argument you make that
[00:28:29.800 --> 00:28:35.480]   Well, these firms are still within the market in the sense that they have to pass these market tests
[00:28:35.840 --> 00:28:43.320]   Why that couldn't also apply to government-directed production because then people are times it does right government runs a bunch of enterprises
[00:28:43.320 --> 00:28:50.520]   They may have monopoly positions, but many are open to the market in Singapore government hospitals compete with private hospitals
[00:28:50.520 --> 00:28:56.520]   Government hospitals seem to be fine. I know they get some means of support. Yeah, but they're not all terrible
[00:28:56.520 --> 00:29:02.020]   But I guess as a general principle you'd be against more government-directed production, right?
[00:29:02.800 --> 00:29:05.880]   Well depends on the context. So if it's say the military
[00:29:05.880 --> 00:29:12.600]   Probably we ought to be building a lot more of some particular things and it will be done through Boeing Lockheed and so on
[00:29:12.600 --> 00:29:18.320]   But the government's directing it paying for it in some way quote-unquote planning it and we need to do that
[00:29:18.320 --> 00:29:20.800]   We've at times done that well in the past
[00:29:20.800 --> 00:29:22.840]   so
[00:29:22.840 --> 00:29:27.560]   People overrate the distinction between government and market I think especially libertarians
[00:29:27.880 --> 00:29:33.400]   But that said there's an awful lot of government bureaucracy. That's terrible doesn't have a big market check
[00:29:33.400 --> 00:29:39.820]   But very often governments act through markets and have to contract or hire consultants or hire outside parties
[00:29:39.820 --> 00:29:41.820]   And it's more like a market than you think
[00:29:41.820 --> 00:29:45.600]   Hmm. I want to ask it about another part of Hayek
[00:29:45.600 --> 00:29:50.820]   So he has an argument about how it's really hard to aggregate information towards a central planner
[00:29:51.040 --> 00:29:57.960]   But then more recently there have been results in computer science that just finding the general equilibrium as computationally intractable
[00:29:57.960 --> 00:29:59.640]   and
[00:29:59.640 --> 00:30:00.760]   Which raises the question?
[00:30:00.760 --> 00:30:06.200]   Well, the market is somehow solving this problem right separate from the problem getting the information make it use making use of information
[00:30:06.200 --> 00:30:08.360]   to allocate scarce resources
[00:30:08.360 --> 00:30:12.360]   How is that computationally a process that's possible?
[00:30:12.360 --> 00:30:19.160]   You know, I'm sure you're aware like you did linear optimization non-convex constraints have how does the market solve this problem?
[00:30:19.880 --> 00:30:22.800]   Well, the markets not solving for a general equilibrium
[00:30:22.800 --> 00:30:25.880]   It's just solving for something that gets us into the next day
[00:30:25.880 --> 00:30:31.000]   And that's a big part of the triumph just living to fight another day wealth not going down
[00:30:31.000 --> 00:30:34.920]   Not everyone quitting and if you can do that things will get better
[00:30:34.920 --> 00:30:39.560]   And that's what we're pretty good at doing is just building a sustainable structure
[00:30:39.560 --> 00:30:43.840]   And a lot of it isn't sustainable like the fools who start these new small businesses
[00:30:43.840 --> 00:30:47.900]   But they do pretty quickly disappear and that's part of the market as well
[00:30:48.560 --> 00:30:52.960]   So if you view the whole thing in terms of computing a general equilibrium
[00:30:52.960 --> 00:30:58.160]   I think one of Hayek's great insights is that's just the wrong way to think about the whole problem. So lack of
[00:30:58.160 --> 00:31:01.960]   Computational ability to do that
[00:31:01.960 --> 00:31:08.320]   Doesn't worry me for either the market or planning because to the extent planning does work. It doesn't work by succeeding at that
[00:31:08.320 --> 00:31:15.080]   Like Singaporean public hospitals don't work because they solve some computational problem
[00:31:15.080 --> 00:31:20.960]   They seem to work as the people running them care about doing a good job and enough of the workers go along with that
[00:31:20.960 --> 00:31:25.640]   Yeah, so related to that. I think in the meaning of competition. He makes the point that
[00:31:25.640 --> 00:31:28.520]   that the most interesting part of
[00:31:28.520 --> 00:31:33.560]   markets is when they go from one equilibrium to another because that's where they're trying to figure out what to produce and how to
[00:31:33.560 --> 00:31:38.880]   produce it better and so on and not the equilibriums himself and it seemed related to the Peter Thiel point in zero to one that
[00:31:38.880 --> 00:31:44.800]   Monopoly is when you have interesting things happen because when there's this competitive equilibrium, there's no profits to
[00:31:45.480 --> 00:31:49.640]   Invest in R&D or to do cool new things. Do those seem like related points in my reading?
[00:31:49.640 --> 00:31:55.920]   Absolutely, and I excess a competition as a discovery process or procedure makes that point very explicitly
[00:31:55.920 --> 00:32:01.000]   Yeah, and that's one of his handful of greatest essays one of the greatest essays in all of economics. Mm-hmm
[00:32:01.000 --> 00:32:07.000]   Is there a contradiction in Hayek in the sense that the decentralization he's calling for?
[00:32:07.000 --> 00:32:13.040]   Results in the results and have a specialist having to use the very scientism and statistical aggregates
[00:32:13.040 --> 00:32:16.880]   Of course that I underrate scientism scientism is great
[00:32:16.880 --> 00:32:22.920]   It can be abused but we all rely on scientism if you have an mRNA vaccine in your arm
[00:32:22.920 --> 00:32:25.000]   Well, how do you feel about scientism and so on?
[00:32:25.000 --> 00:32:25.920]   Hmm
[00:32:25.920 --> 00:32:27.480]   How much should we worry about?
[00:32:27.480 --> 00:32:33.920]   This opening up the whole system to fragilities if there's like no one mind that understands large parts of how everything fits together
[00:32:33.920 --> 00:32:40.240]   People talk about this in the context of if there's a war in China and the the producers didn't think about that possibility when they put
[00:32:40.360 --> 00:32:46.800]   Valuable manufacturing in Taiwan and stuff like that. No one mind understanding things is inevitable under all systems
[00:32:46.800 --> 00:32:49.680]   This gets into some of the alignment debates, right?
[00:32:49.680 --> 00:32:56.080]   If you had one mind that understood everything or could control everything you have to worry a great deal about the corrupt ability of that mind
[00:32:56.080 --> 00:33:03.360]   So legibility transparency are not per se good you want enough of them in the right places, but you need some kind of balance
[00:33:03.360 --> 00:33:06.440]   so I think
[00:33:06.920 --> 00:33:10.040]   Supply chains are no longer an under analyzed problem
[00:33:10.040 --> 00:33:15.800]   But until kovat they were and they're a big deal and the Hayek Ian argument doesn't always work
[00:33:15.800 --> 00:33:22.240]   Because the signal you have is of the current price and that's not telling you how high are the inframarginal
[00:33:22.240 --> 00:33:28.960]   Values if you get say cut off from being able to buy vaccines from India because you're you know at the bottom of the queue
[00:33:28.960 --> 00:33:31.320]   So that was a problem
[00:33:31.320 --> 00:33:37.120]   It was the market failing because the price doesn't tell you inframarginal values and when you move from some
[00:33:37.120 --> 00:33:42.640]   Ability to buy the output to zero those inframarginal values really matter. Mm-hmm
[00:33:42.640 --> 00:33:49.600]   What would Hayek make of AI agents and as they get more powerful you have some market between the AI agents?
[00:33:49.600 --> 00:33:53.240]   There's some sort of decentralized order as a result
[00:33:53.240 --> 00:33:54.920]   What insights would you have about that?
[00:33:54.920 --> 00:33:59.840]   well a lot of Hayek Ian's wrote about these issues including at George Mason in the
[00:33:59.840 --> 00:34:07.360]   1980s and I think some of those people even talked to Hayek about this and my recollection which is imperfect is that he found all
[00:34:07.360 --> 00:34:14.900]   This very interesting and in the spirit of his work and Don Lavoie was leading this research paragram. He died prematurely of cancer
[00:34:14.900 --> 00:34:19.240]   Bill Tala was also involved and some of this has been written up
[00:34:19.880 --> 00:34:25.520]   And it is very Hayek Ian and George Mason George Mason actually was a pioneer in this area. Mm-hmm
[00:34:25.520 --> 00:34:30.760]   Well, what do you make of AI agents the market between them and the sort of infrastructure in order that would there you need to?
[00:34:30.760 --> 00:34:33.440]   Facilitate that they're gonna replicate markets on their own
[00:34:33.440 --> 00:34:39.460]   This has been my prediction and I think they're gonna evolve their own currencies. Maybe at first they'll use Bitcoin
[00:34:39.460 --> 00:34:45.640]   But they'll be an entire property rights will be based at least at first on what we now call NFTs
[00:34:45.640 --> 00:34:49.120]   I'm not sure that will end up being the right name for them
[00:34:49.400 --> 00:34:55.480]   But if you want property rights in a so-called imaginary world, that's where you would start with Bitcoin and NFTs
[00:34:55.480 --> 00:34:58.880]   So, I don't know what percent of GDP this will be at first
[00:34:58.880 --> 00:34:59.960]   It will be quite small
[00:34:59.960 --> 00:35:05.960]   But it will grow over time and it's going to show Hayek to have been right that had these decentralized systems evolve
[00:35:05.960 --> 00:35:09.200]   Do you anticipate that it'll be a sort of a completely different?
[00:35:09.200 --> 00:35:14.840]   Spear and there's like the AI agents economy and there's a human economy and they obviously have links between them
[00:35:14.840 --> 00:35:18.680]   But it's not intermixed like they're not on the same social media or the same
[00:35:19.440 --> 00:35:22.720]   Task rabbit or whatever. It's like it's a very separate infrastructure. That's needed to
[00:35:22.720 --> 00:35:25.600]   For the AI is to talk to themselves or just talk to humans
[00:35:25.600 --> 00:35:31.660]   I don't see why we wouldn't force segregation and you might have some segregated outlets like maybe X Twitter
[00:35:31.660 --> 00:35:36.000]   Well, we'll keep off the bots. Let's say it can even manage to do that
[00:35:36.000 --> 00:35:39.320]   But if I want to hire, you know a proofreader
[00:35:39.320 --> 00:35:47.080]   I'm gonna deal with the AI sector and pay them in Bitcoin and I'll just say to my like personal AI assistant
[00:35:47.080 --> 00:35:52.600]   Hey go out and hire an AI and pay them with whatever and then just not think about it anymore and it will happen
[00:35:52.600 --> 00:35:56.200]   maybe because there's like much higher transaction costs with dealing with humans and
[00:35:56.200 --> 00:36:01.560]   Interacting with the human world. Whereas they can just send a bunch of vectors to each other. It's much faster for them to just
[00:36:01.560 --> 00:36:09.600]   Just like a separate dedicated infrastructure for that and but transactions costs for dealing with humans will fall because you'll deal with their quote-unquote
[00:36:09.600 --> 00:36:10.920]   Assistance, right?
[00:36:10.920 --> 00:36:18.080]   So you'll only deal with the difficult human when you need to and people who are very effective will segregate their tasks in a way
[00:36:18.080 --> 00:36:25.160]   that reflects their comparative advantage and people who are not effective will be very poor at that and that will lead to some kind of
[00:36:25.160 --> 00:36:27.360]   bifurcation of personal productivity
[00:36:27.360 --> 00:36:29.000]   hmm
[00:36:29.000 --> 00:36:33.720]   Like how well do you will you know what to delegate to your AI? I'll predict you'll be very good at it
[00:36:33.720 --> 00:36:35.720]   You may not have figured it out yet. Mm-hmm
[00:36:36.280 --> 00:36:42.900]   But say you're like a a+ on it and other people are D. That's a big comparative advantage for you. Mm-hmm
[00:36:42.900 --> 00:36:44.560]   so
[00:36:44.560 --> 00:36:50.160]   We're talking I guess about like GPT 5 level models. What do you think in your mind about like, okay, this is GPT 5
[00:36:50.160 --> 00:36:52.640]   What happens with GPT 6 GPT 7? Do you see it?
[00:36:52.640 --> 00:36:58.080]   Just do you still think in the frame of having a bunch of RAs or how does it seem like a different sort of thing?
[00:36:58.080 --> 00:37:01.440]   At some point. I'm not sure what those numbers going up
[00:37:01.960 --> 00:37:09.840]   Mean or what a GPT 7 would look like or how much smarter it could get I think people make too many assumptions there
[00:37:09.840 --> 00:37:16.320]   It could be the real advantages are integrating it into workflows by things that are not better GPT's at all
[00:37:16.320 --> 00:37:19.480]   And once you get to GPT say 5.5
[00:37:19.480 --> 00:37:26.840]   I'm not sure you can just turn up the dial on smarts and have it like integrate general relativity and quantum mechanics
[00:37:26.840 --> 00:37:31.840]   Why not? I don't think that's how intelligence works and this is a Hayek Ian point and some of these problems
[00:37:31.840 --> 00:37:38.680]   There just may be no answer. Like maybe the universe isn't that legible and if it's not that legible the GPT 11
[00:37:38.680 --> 00:37:42.840]   Doesn't really make sense as a you know, a creature or whatever
[00:37:42.840 --> 00:37:48.160]   Mm-hmm. Isn't there a hacking argument to be made that listen, you can have billions of copies of these things
[00:37:48.160 --> 00:37:53.000]   imagine the sort of decentralized order that could result the amount of decentralized tacit knowledge that
[00:37:53.160 --> 00:37:57.920]   billions of copies talking to each other could have that in and of itself is an argument to be made about
[00:37:57.920 --> 00:38:03.280]   The whole thing as a as a emergent order will be much more powerful than we're anticipating
[00:38:03.280 --> 00:38:08.000]   Well, I think it will be highly productive what tacit knowledge means with a eyes
[00:38:08.000 --> 00:38:13.080]   I don't think we understand yet. Is it by definition all non tacit?
[00:38:13.080 --> 00:38:18.680]   What is the fact that how GPT for works is not legible to us or even its creators so much?
[00:38:19.040 --> 00:38:23.600]   Does that mean it's possessing of tacit knowledge or is it not knowledge?
[00:38:23.600 --> 00:38:27.000]   So, you know, none of those categories are well thought out in my opinion
[00:38:27.000 --> 00:38:32.360]   So we need to restructure our whole discourse about tacit knowledge in some new different way
[00:38:32.360 --> 00:38:38.020]   But I agree these networks are they eyes even before like GPT 11?
[00:38:38.020 --> 00:38:39.480]   They're gonna be super productive
[00:38:39.480 --> 00:38:43.840]   But they're still gonna face bottlenecks, right and I don't know how good they'll be
[00:38:43.840 --> 00:38:49.800]   It's a overcoming the behavioral bottlenecks of actual human beings the bottlenecks of the law and regulation
[00:38:49.800 --> 00:38:53.880]   Mmm, and we're gonna have more regulation as we have more eyes, right? Yeah
[00:38:53.880 --> 00:38:55.680]   Well when you say they'll be uncertainties
[00:38:55.680 --> 00:38:59.660]   I think you made this argument when you were responding to Alex Epstein on fossil future where he said
[00:38:59.660 --> 00:39:05.240]   Uncertainties also extend out into the domain where there's a bad outcome or much bigger outcome than you're anticipating. That's right
[00:39:05.240 --> 00:39:07.240]   So can we apply the same?
[00:39:07.240 --> 00:39:10.960]   Argument to AI like the fact that there is uncertainty is also a reason for worry
[00:39:11.720 --> 00:39:17.240]   Well, it's always reason for worry, but there's uncertainty about a lot of things and AI will help us with those other uncertainties
[00:39:17.240 --> 00:39:23.000]   So on net do you think more intelligence is likely to be good or bad including against X risk?
[00:39:23.000 --> 00:39:25.000]   And I think it's more likely to be good
[00:39:25.000 --> 00:39:30.400]   So if it were the only risk I'd be more worried about it than if there's a whole multitude of risks
[00:39:30.400 --> 00:39:32.720]   But clearly there's a whole multitude of risks
[00:39:32.720 --> 00:39:35.140]   But since people grew up in pretty stable times
[00:39:35.140 --> 00:39:41.360]   They tend not to see that in emotionally vivid terms and then this one monster comes along and they're all terrified
[00:39:41.360 --> 00:39:44.200]   What would Hayek think of prediction markets?
[00:39:44.200 --> 00:39:47.800]   Well, there were prediction markets in Hayek's time
[00:39:47.800 --> 00:39:54.560]   I don't know that he wrote about them, but I strongly suspect he would see them as markets that through prices communicate information
[00:39:54.560 --> 00:39:57.320]   But even at the like they're on the time of the Civil War
[00:39:57.320 --> 00:40:01.440]   There were so-called bucket shops in the US in New York where you would bet on things
[00:40:01.440 --> 00:40:08.680]   They were betting markets with cash settlement probably never called prediction markets, but they were exactly that later on
[00:40:08.680 --> 00:40:15.600]   They were banned, but it's an old-standing thing. Hmm. There were betting markets on lives in a 17th century, Britain
[00:40:15.600 --> 00:40:22.260]   Different attempts to outlaw them which I think basically ended up succeeding but under the table. I'm sure it still went on to some extent
[00:40:22.260 --> 00:40:22.760]   Yeah
[00:40:22.760 --> 00:40:29.160]   the reason is just think about this is because his whole argument about the price system is that you can have a single dial that
[00:40:29.160 --> 00:40:32.960]   Aggregates so much information, but it's precisely for this and that'd be for that reason
[00:40:32.960 --> 00:40:36.300]   It's so useful to somebody who's trying to decide, you know based on that information
[00:40:36.300 --> 00:40:42.320]   But it's precisely for this reason that it's so aggregated that it's hard to learn about any one particular input to that dial
[00:40:42.320 --> 00:40:46.080]   But I would stress it's not a single dial and whether Hayek thought it was a single dial
[00:40:46.080 --> 00:40:52.680]   I think you can argue that either way. So people in markets. They also observe quantities. They observe reaction speeds
[00:40:52.680 --> 00:40:58.000]   There's a lot of dimensions to prices other than just oh this newspaper cost $4
[00:40:58.000 --> 00:41:06.000]   The terms on which it's advertised so markets work so well because people are solving this complex multi-dimensional problem
[00:41:06.000 --> 00:41:12.800]   and the price really is not a sufficient statistic the way it is an Arahonda brew and I think at times Hayek understood that and
[00:41:12.800 --> 00:41:17.560]   At other times he writes as if he doesn't understand it, but it's an important point. Mm-hmm
[00:41:17.560 --> 00:41:23.760]   Somewhat related question. What does it tell us about the difficulty of preserving good institutions good people?
[00:41:23.760 --> 00:41:29.920]   That the median age of a corporation is 18 years and they don't get better over time right decade after decade
[00:41:29.920 --> 00:41:37.400]   What corporation the visual corporations that continue improving in that way? Well, I think some firms keep improving for a long time
[00:41:37.400 --> 00:41:40.780]   So there are Japanese firms that date back to the 17th century
[00:41:40.780 --> 00:41:45.840]   Right, they must be better today or even in 1970 than they were way back when?
[00:41:45.840 --> 00:41:48.560]   like the leading four or five Danish firms
[00:41:48.560 --> 00:41:51.280]   none of them are
[00:41:51.280 --> 00:41:58.360]   Younger than the 1920s. So Maersk, you know the firm that came up with those epic the pharmaceutical firm
[00:41:58.960 --> 00:42:03.020]   They must be much better than they were back then. So right they have to be
[00:42:03.020 --> 00:42:04.600]   so
[00:42:04.600 --> 00:42:08.640]   How that is possible to me is a puzzle, but I think in plenty of cases. It's true
[00:42:08.640 --> 00:42:11.320]   it can really say that the
[00:42:11.320 --> 00:42:15.360]   The best firms in the world aren't ones that have been improving over time
[00:42:15.360 --> 00:42:17.960]   it
[00:42:17.960 --> 00:42:20.040]   You know, if you look at the biggest companies in our market cap
[00:42:20.040 --> 00:42:24.480]   It's not like this is what it you what it takes to get there as hundreds of years of continual refinement
[00:42:25.040 --> 00:42:28.360]   What does that tell us about the world or just hundreds of years?
[00:42:28.360 --> 00:42:32.840]   But again, don't be overly biased by the u.s. Experience and the tech sector. Mm-hmm
[00:42:32.840 --> 00:42:38.240]   There's around the world plenty of firms that at least seem to get better as they get older
[00:42:38.240 --> 00:42:42.200]   Certainly their market cap goes up. Some of that might just be a population effect
[00:42:42.200 --> 00:42:46.360]   Maybe their productivity per some unit is in some ways going down
[00:42:46.360 --> 00:42:53.400]   But that's a very common case and why the u.s. Is such an outlier is an interesting question, right?
[00:42:53.400 --> 00:42:58.640]   Israel clearly is an outlier in a sense. They only have pretty young firms, right?
[00:42:58.640 --> 00:43:03.120]   And they've done very well in terms of growth. Can it be explained by the fact that in these other countries?
[00:43:03.120 --> 00:43:06.680]   It's actually just harder to start a new company. Not necessarily. The older companies are actually getting better
[00:43:06.680 --> 00:43:11.920]   Possibly but it does seem the older companies are often getting better right like in Denmark
[00:43:11.920 --> 00:43:15.160]   So, you know take China's
[00:43:15.160 --> 00:43:18.920]   Pretty much entirely new firms because of communism
[00:43:19.800 --> 00:43:22.640]   Japan in particular seems to have a lot of very old firms
[00:43:22.640 --> 00:43:27.240]   I don't know if they're getting better, but I don't think you can write that off as a possibility. Mm-hmm
[00:43:27.240 --> 00:43:32.600]   This is Hayek in competition as a discovery process and it seems like he predicted NIMBY ism
[00:43:32.600 --> 00:43:34.600]   So he says in a democratic society
[00:43:34.600 --> 00:43:36.760]   It would be completely impossible
[00:43:36.760 --> 00:43:43.240]   Using commands that could not be regarded as just to bring about those changes that are undoubtedly necessary
[00:43:43.240 --> 00:43:47.600]   But the necessity of which could not be strictly demonstrated in a particular case
[00:43:48.600 --> 00:43:51.880]   So it seems like he's kind of talking about what we today called NIMBY ism
[00:43:51.880 --> 00:43:58.460]   There's plenty of be ism in earlier times. You look at the 19th century debates over restructuring Paris
[00:43:58.460 --> 00:44:06.080]   Houseman and putting in the broader boulevards and the like that met with very strong opposition. It's a kind of miracle that it happened. Yeah
[00:44:06.080 --> 00:44:10.000]   Is this is this thing that's inherent to the democratic system?
[00:44:10.000 --> 00:44:13.800]   I would recently I interviewed Dominic Cummings and obviously planning is a big issue in the UK
[00:44:13.800 --> 00:44:19.240]   It seems like every country has every democratic country has this kind of a problem and most autocratic countries have it too
[00:44:19.240 --> 00:44:25.220]   Now China is an exception. They will probably slide into some kind of NIMBY ism, even if they stay autocratic
[00:44:25.220 --> 00:44:32.720]   Just people resist change interest groups always matter public opinion a la David Hume always matters and
[00:44:32.720 --> 00:44:40.040]   It's easy to not do anything on a given day right and that just keeps on sliding into the future. Mm-hmm
[00:44:40.440 --> 00:44:44.160]   So but nothing I guess India has had a lot of NIMBY ism
[00:44:44.160 --> 00:44:48.780]   It's fallen away greatly under Modi and especially what the state governments have done
[00:44:48.780 --> 00:44:54.960]   But it's can be very hard to build things in India still. Mm-hmm. Oh, it is a democracy
[00:44:54.960 --> 00:45:00.360]   I guess the China example, we'll see what happens there. That's right, but it would be very surprising
[00:45:00.360 --> 00:45:05.880]   Because the Chinese government is highly responsive to public opinion on most but not all issues
[00:45:06.400 --> 00:45:12.360]   So, why wouldn't they become more NIMBY, especially with the shrinking population there? They're way over bill, right?
[00:45:12.360 --> 00:45:18.640]   So the pressure to build will be weak and in cases where they ought to build I would think quite soon they won't mm-hmm
[00:45:18.640 --> 00:45:21.360]   how much of economics is a study of
[00:45:21.360 --> 00:45:23.200]   the
[00:45:23.200 --> 00:45:29.200]   The systems that human beings use to allocate scarce resources and how much is is just something you'd expect it to be true of aliens
[00:45:29.200 --> 00:45:30.400]   AI's
[00:45:30.400 --> 00:45:33.700]   It's interesting when you read the history of economic thought how often they make
[00:45:35.080 --> 00:45:40.540]   They make mention of human nature specifically like Keynes is talking about the people have higher discount rates, right?
[00:45:40.540 --> 00:45:50.400]   Yeah, but what are your thoughts here my former colleague Gordon Tulloch wrote a very interesting book on the economics of ant societies and
[00:45:50.400 --> 00:45:52.240]   animal societies and
[00:45:52.240 --> 00:45:58.560]   Very often they obey human like principles or more accurately humans obey non-human animal like principles
[00:45:58.560 --> 00:46:04.680]   So I suspect it's fairly universal and depends less on quote-unquote human nature
[00:46:04.720 --> 00:46:10.120]   Then we sometimes like to suggest maybe that is a bit of a knock on some behavioral economics
[00:46:10.120 --> 00:46:14.860]   The logic of the system Armin Elshian wrote on this Gary Becker wrote on this
[00:46:14.860 --> 00:46:21.040]   there's some debates on this in the early 1960s and that the automatic principles of profit and loss and
[00:46:21.040 --> 00:46:28.720]   Selection at a firm firm wide level really matters and it's responsible for a lot of economics being true
[00:46:28.720 --> 00:46:31.920]   I think that's correct. Actually that that raises an interesting question of
[00:46:32.760 --> 00:46:40.200]   Within firms they the sort of input they're getting from the outside world. The ground truth data is profit loss bankruptcy
[00:46:40.200 --> 00:46:41.760]   It's like a very
[00:46:41.760 --> 00:46:47.520]   Very condensed information and from this data make the determination of who to fire who to hire who to promote what projects to pursue
[00:46:47.520 --> 00:46:54.120]   How do you make sense of how firms disaggregate this this very condensed information
[00:46:54.120 --> 00:46:59.680]   I would like to see a very good estimate of how much of productivity gains is just from selection and
[00:47:00.000 --> 00:47:03.800]   how much is from well smart humans figure out better ways of doing things and
[00:47:03.800 --> 00:47:10.400]   there's some related pieces on this in the international trade literature, so when you have freer trade a
[00:47:10.400 --> 00:47:16.920]   shockingly high percentage of the productivity gains come from your worst firms being bankrupted by the free trade and
[00:47:16.920 --> 00:47:20.160]   Alex Tabarrok has some mr. Posts on this
[00:47:20.160 --> 00:47:22.000]   I don't recall the exact numbers
[00:47:22.000 --> 00:47:27.520]   but it was higher than almost anyone thought and that to me suggests the Elshian Becker mechanisms of
[00:47:28.000 --> 00:47:30.000]   evolution at the level of the firm
[00:47:30.000 --> 00:47:36.080]   Enterprise or even sector. They're just a lot more important than human ingenuity. And that's a pretty Hayek Ian point
[00:47:36.080 --> 00:47:40.740]   Hayek presumably read those pieces in the 60s. I don't think he ever commented on them
[00:47:40.740 --> 00:47:42.840]   interesting
[00:47:42.840 --> 00:47:44.600]   Let's talk about
[00:47:44.600 --> 00:47:46.600]   mill
[00:47:46.600 --> 00:47:51.560]   So his arguments about
[00:47:51.560 --> 00:47:53.680]   the law force against
[00:47:53.680 --> 00:47:57.360]   woman and how basically throughout history that the state of
[00:47:57.360 --> 00:48:03.200]   woman in in his society is not natural or the wisdom of the ages but just the result of the fact that men are
[00:48:03.200 --> 00:48:08.600]   Stronger and have codified that can we apply that argument in today's society against children and the way we treat them?
[00:48:08.600 --> 00:48:13.920]   Yes, I think we should treat children much better. We've made quite a few steps in that direction
[00:48:13.920 --> 00:48:17.760]   It's interesting to think of Mills argument as it relates to Hayek
[00:48:18.320 --> 00:48:23.240]   So mill is arguing you can see more than just the local information. So keep in mind when mill wrote
[00:48:23.240 --> 00:48:28.520]   Every society that he knew of at least treated women very poorly oppressed women
[00:48:28.520 --> 00:48:35.720]   Women because they were physically weaker or at a big disadvantage if you think there's some matrilineal exceptions mill didn't know about them
[00:48:35.720 --> 00:48:37.720]   so it appeared universal and
[00:48:37.720 --> 00:48:42.040]   Mills chief argument is to say you're making a big mistake if you
[00:48:42.640 --> 00:48:49.560]   overly aggregate information from this one observation that behind it is a lot of structure and a lot of the structure is
[00:48:49.560 --> 00:48:55.440]   Contingent and that if I mill unpack the contingency for you, you will see behind the signals
[00:48:55.440 --> 00:48:58.840]   So mill is much more irrationalist than Hayek
[00:48:58.840 --> 00:49:02.880]   It's one reason why Hayek hated mill but clearly on the issue of women
[00:49:02.880 --> 00:49:07.520]   Mill was completely correct that women can do much better will do much better
[00:49:07.520 --> 00:49:12.600]   It's not clear what the end of this process will be it will just continue for a long time
[00:49:13.600 --> 00:49:16.160]   women achieving in excellent ways and
[00:49:16.160 --> 00:49:22.360]   It's mills greatest work. I think it's one of the greatest pieces of social social science and it is anti Hayek Ian
[00:49:22.360 --> 00:49:25.240]   It's anti small-c conservatism
[00:49:25.240 --> 00:49:32.240]   his other book on Liberty is very high akin though right in the sense that the free speech is needed because information is is
[00:49:32.240 --> 00:49:35.240]   Contained in many different people's minds. That's right
[00:49:35.240 --> 00:49:40.920]   And I think mill integrated sort of you could call it Hayek and anti Hayek better than Hayek ever did
[00:49:40.920 --> 00:49:46.480]   That's why I think mill is the greater thinker of the two. Mm-hmm, but but on the topic of children
[00:49:46.480 --> 00:49:52.520]   What would mill say specifically I guess you could have talked about it if you wanted to but I don't know if he was in
[00:49:52.520 --> 00:49:57.160]   Today's world we send him to school. They're there for eight hours a day. Most of the time is probably wasted
[00:49:57.160 --> 00:50:01.440]   And we just like use a lot of coercion on them that we don't need to how would we think?
[00:50:01.440 --> 00:50:03.040]   How would he think about this issue?
[00:50:03.040 --> 00:50:07.800]   There's mills own upbringing which was quite strict and by current standards oppressive
[00:50:07.880 --> 00:50:11.120]   But apparently extremely effective in making mill smart
[00:50:11.120 --> 00:50:17.800]   So I think mill very much thought that kids should be induced to learn the classics
[00:50:17.800 --> 00:50:25.000]   But he also stressed they needed free play of the imagination in a way that he drew from German and also British
[00:50:25.000 --> 00:50:28.480]   Romanticism and he wanted some kind of synthesis of the two
[00:50:28.480 --> 00:50:34.080]   But by current standards mill I think still would be seen as a meanie toward kids
[00:50:34.080 --> 00:50:40.400]   but he was progressive by the standards of his own day do you buy the arguments about aristocratic tutoring for people like mill and
[00:50:40.400 --> 00:50:42.040]   There's many other cases like this
[00:50:42.040 --> 00:50:47.280]   But you know since they were kids they were taught in the top by one-on-one tutors and that's that explains part of their
[00:50:47.280 --> 00:50:54.320]   Greatness, I believe in one-on-one tutors, but I don't know how much of those examples is selection, right?
[00:50:54.320 --> 00:50:56.520]   So I'm not sure how important it is
[00:50:56.520 --> 00:51:01.960]   Hmm, but just as a matter of fact if I were a wealthy person and just had a new kid
[00:51:01.960 --> 00:51:07.600]   I would absolutely invest in one-on-one tutors. Hmm. You talk in the book about how mill is very concerned about the
[00:51:07.600 --> 00:51:10.260]   quality and the character development of the population
[00:51:10.260 --> 00:51:17.280]   But when we think about the fact that somebody like him was elected to the parliament at the time the greatest thinker who's alive is
[00:51:17.280 --> 00:51:19.280]   elected to
[00:51:19.280 --> 00:51:24.560]   To government and it's hard to imagine that could be true with today's world
[00:51:24.560 --> 00:51:28.040]   Does he have a does he have a point with the regard to the quality of the population?
[00:51:28.960 --> 00:51:32.600]   Well mill as with women, he thought a lot of improvement was possible
[00:51:32.600 --> 00:51:38.000]   Yeah, and we shouldn't overly generalize from seeing all the dunces around us. Yeah, so to speak
[00:51:38.000 --> 00:51:41.080]   Maybe the the book is still out on that one
[00:51:41.080 --> 00:51:46.120]   But it's an encouraging belief and I think it's more right than wrong
[00:51:46.120 --> 00:51:51.600]   There's been a lot of moral progress since mills time right and everything, but certainly how people treat children
[00:51:51.600 --> 00:51:56.000]   and or their their why how men treat their wives and
[00:51:58.480 --> 00:52:00.720]   Even when you see negative reversals
[00:52:00.720 --> 00:52:08.000]   Steven Pinker so far seems to be right on that one, but you do see places, you know, like Iran how women were treated
[00:52:08.000 --> 00:52:11.680]   Seems to have been much better in the 1970s than it is today
[00:52:11.680 --> 00:52:15.300]   So there are definitely reversals but on the specific reversal of
[00:52:15.300 --> 00:52:21.640]   Somebody of mills quality probably wouldn't get elected to Congress in the US or a Parliament of the UK is how?
[00:52:22.560 --> 00:52:30.680]   Advice may get through the cracks due to all the local statesmen who wisely advise their representatives in the house, right? So I
[00:52:30.680 --> 00:52:36.920]   Don't know how much that process is better or worse compared to say the 1960s
[00:52:36.920 --> 00:52:41.240]   I know plenty of smart people who think it's worse. I'm not convinced. That's true. Mm-hmm
[00:52:41.240 --> 00:52:43.880]   Let's talk about Smith
[00:52:43.880 --> 00:52:45.880]   Adam Smith. Yeah
[00:52:45.880 --> 00:52:49.140]   Okay
[00:52:49.140 --> 00:52:53.120]   One of the things I find really remarkable about him is he publishes in
[00:52:53.120 --> 00:52:58.900]   1776 the Wealth of Nations and basically around that time Gibbon publishes the decline and fall of the Roman Empire. Yep
[00:52:58.900 --> 00:53:04.740]   So he publishes the decline of all the Roman Empire and one of his lines in there is if you were asked to
[00:53:04.740 --> 00:53:09.860]   State a period of time and man's condition was what is his best? It was drew, you know during the reign of Commodus to Domitian
[00:53:09.860 --> 00:53:12.780]   And that's like 2,000 years before that, right?
[00:53:12.780 --> 00:53:18.480]   So there's basically been at least it's like plausible to somebody really smart that there's basically been no growth since before 2,000 years
[00:53:19.020 --> 00:53:23.180]   And in that context to be making the case or markets and mechanization and division of labor
[00:53:23.180 --> 00:53:27.700]   I think it's like even more impressive when you when you put it in the context that he has like basically been seeing on you
[00:53:27.700 --> 00:53:32.260]   Know 0.5 percent or less growth strongly agree. And this is in a way Smith being like mill
[00:53:32.260 --> 00:53:39.140]   yeah, Smith's is seeing the local information a very small growth and the world barely being better than the Roman Empire and
[00:53:39.140 --> 00:53:44.140]   Inferring from that with increasing returns division of labor. How much is possible?
[00:53:44.140 --> 00:53:48.380]   So Smith is a bit more of a rationalist than Hayek makes him out to be right
[00:53:49.060 --> 00:53:53.020]   now, I wonder if we use the same sort of
[00:53:53.020 --> 00:53:57.020]   Extrapolated thinking that Smith uses of we haven't seen that much growth yet
[00:53:57.020 --> 00:54:00.660]   But if you apply these sorts of principles, this is what you would expect to see
[00:54:00.660 --> 00:54:05.540]   What would he make of the potential AI economy where we see 2% growth a year now?
[00:54:05.540 --> 00:54:08.660]   But you have you know billions of potential more agents or something
[00:54:08.660 --> 00:54:13.900]   It would even say well actually you might have 10% growth because of this like this is a you would need more economic principles
[00:54:13.900 --> 00:54:18.420]   Explain this or that just adding that to our list of existing principles would imply big gains
[00:54:18.420 --> 00:54:22.020]   It's hard to say what Smith would predict for AI
[00:54:22.020 --> 00:54:27.220]   My suspicion is that the notion of 10% growth was simply not conceivable to him
[00:54:27.220 --> 00:54:33.800]   So he wouldn't have predicted it because he never saw anything like it that to him 3% growth would be a bit like
[00:54:33.800 --> 00:54:38.300]   10% growth it would just shock him and and bowl him over
[00:54:39.540 --> 00:54:44.620]   But Smith does also emphasize different human bottlenecks and constraints of the law
[00:54:44.620 --> 00:54:51.380]   So it's quite possible Smith would see those bottlenecks as mattering and checking AI growth and its speed
[00:54:51.380 --> 00:54:56.260]   Hmm, but as a principle given the change we saw pre
[00:54:56.260 --> 00:54:59.340]   pre-industrial revolution and after 1870
[00:54:59.340 --> 00:55:06.580]   Does that does to you it seem plausible that you could go from the current regime to a regime where you have 10% growth for?
[00:55:06.580 --> 00:55:08.060]   decades on end
[00:55:08.060 --> 00:55:09.980]   That does not seem plausible to me
[00:55:09.980 --> 00:55:15.640]   But I would stress the point that high rates of growth decades on end the numbers cease to have meaning
[00:55:15.640 --> 00:55:22.340]   Because the numbers make the most sense when the economy is broadly similar like oh everyone eats apples and each year
[00:55:22.340 --> 00:55:29.300]   There's 10% more apples than a roughly constant price as the basket changes the numbers become meaningless. It's not to deny
[00:55:29.300 --> 00:55:31.300]   There's a lot of growth
[00:55:31.500 --> 00:55:39.060]   But you can think about it better by discarding the number and presumably AI will change the composition of various bundles quite a bit over time
[00:55:39.060 --> 00:55:43.460]   So when you hear these estimates about what the GDP per capita was in the Roman Empire
[00:55:43.460 --> 00:55:46.620]   Do you just disregard that and think in terms of qualitative changes from that time?
[00:55:46.620 --> 00:55:49.460]   Depends what they're being compared to
[00:55:49.460 --> 00:55:56.740]   So there's pieces in economic history that are looking at say 17th 18th century Europe comparing it to the Roman Empire
[00:55:56.740 --> 00:56:00.740]   Most of GDP is agriculture, which is pretty comparable, right?
[00:56:01.300 --> 00:56:05.460]   Especially in Europe. It's not wheat versus corn. It's wheat and wheat and
[00:56:05.460 --> 00:56:09.140]   I've seen estimates that all say by 1730
[00:56:09.140 --> 00:56:14.940]   Some parts of Western Europe are clearly better off than the Roman Empire at its peak but like within range
[00:56:14.940 --> 00:56:19.300]   That those are the best estimates. I know and I trust those they're not perfect
[00:56:19.300 --> 00:56:23.100]   But I don't think there's an index number problem so much. Mm-hmm
[00:56:23.100 --> 00:56:29.820]   And so when people say we're 50% richer than an average Roman at the peak of the Empire you
[00:56:30.380 --> 00:56:32.860]   This this kind of thinking doesn't make sense to you
[00:56:32.860 --> 00:56:36.500]   It doesn't make sense to me and a simple way to show that let's say you could buy
[00:56:36.500 --> 00:56:44.740]   From the Sears robot catalog of today. Yeah, or from 1905 and you have $50,000 to spend which catalog
[00:56:44.740 --> 00:56:46.740]   Would you rather buy from?
[00:56:46.740 --> 00:56:49.060]   You have to think about it, right?
[00:56:49.060 --> 00:56:55.180]   Now if you just look at changes in the CPI, it should be obvious you would prefer the catalog from 1905
[00:56:55.420 --> 00:57:01.780]   Everything's so much cheaper that white shirt costs almost nothing right at the same time. You don't want that stuff
[00:57:01.780 --> 00:57:03.780]   It's not mostly part of the modern bundle
[00:57:03.780 --> 00:57:10.580]   So even if you ended up preferring the earlier catalog the fact that you have to think about it reflects the ambiguities. Mm-hmm
[00:57:10.580 --> 00:57:15.480]   When you read the contemporaries of Smith other economists were writing at the time
[00:57:15.480 --> 00:57:18.780]   where his arguments just
[00:57:18.780 --> 00:57:24.780]   Clearly given the evidence at the time much better than everybody around or was it just that expose to is clearly, right?
[00:57:24.780 --> 00:57:28.420]   But given the arguments at the time it could have gotten any one of different ways
[00:57:28.420 --> 00:57:33.700]   Well, there aren't that many economists at the time of Smith. So it depends what you're counting
[00:57:33.700 --> 00:57:39.620]   I mean the two fellow Scots you could compare Smith to or Sir James Stewart who published a major work
[00:57:39.620 --> 00:57:44.500]   I think in 1767 on some matters Stewart was ahead of Smith
[00:57:44.500 --> 00:57:46.820]   Not most clearly Smith was far greater
[00:57:46.820 --> 00:57:53.060]   But Stewart was no slouch and the other point of comparison is David Hume Smith's best friend, of course
[00:57:53.700 --> 00:57:58.780]   Per page you could argue Hume was better than Smith. Certainly on monetary theory Hume was better than Smith
[00:57:58.780 --> 00:58:02.060]   Now, he's not a goat contender. He just didn't do enough
[00:58:02.060 --> 00:58:08.540]   But I wouldn't say Smith was ahead of Hume. He had more and more important insights
[00:58:08.540 --> 00:58:15.180]   But Hume was pretty impressive. Now if you're talking about oh like the 18th century German camera lists
[00:58:15.180 --> 00:58:22.060]   Well, they were bad mercantilists, but there's people say writing in Sweden in the 1760s
[00:58:23.020 --> 00:58:29.820]   Analyzing exchange rates who had better understandings of exchange rates than Smith ever did. So it's not that he just dominated everyone
[00:58:29.820 --> 00:58:31.700]   Mm-hmm
[00:58:31.700 --> 00:58:37.300]   Let me offer some other potential nominees that were not in the book for goat and I want your opinions of them
[00:58:37.300 --> 00:58:44.500]   Henry George in terms of explaining how land is fundamentally different from labor and capital when we're thinking about the economy
[00:58:44.500 --> 00:58:51.660]   Well first, I'm not sure land is that fundamentally different from labor and capital a lot of the value of land comes from improvements
[00:58:51.660 --> 00:58:56.820]   And what's an improvement can be quite subtle. It doesn't just have to be, you know, putting a plow to the land
[00:58:56.820 --> 00:59:03.700]   so I would put George in the top 25 very important thinker, but he's a bit of a
[00:59:03.700 --> 00:59:09.740]   Not a one-note Johnny his book on protectionism is still one of the best books on free trade
[00:59:09.740 --> 00:59:14.420]   But he's circumscribed in a way say Smith and Mill were not. Mm-hmm
[00:59:16.100 --> 00:59:21.540]   Today's he would it does a status rise we see your rents in big cities status is way
[00:59:21.540 --> 00:59:25.380]   Yeah, this reason right of yimby-nimby, and I think that's correct. He was undervalued
[00:59:25.380 --> 00:59:31.360]   He's worth reading very carefully a few years ago. We're recording here at Mercatus. We had a
[00:59:31.360 --> 00:59:38.040]   Like 12 person two-day session with Peter Thiel just on reading Henry George. It's all we did and
[00:59:38.040 --> 00:59:43.140]   People came away very impressed. I think mm-hmm. I live for people are interested
[00:59:43.140 --> 00:59:49.100]   They might enjoy the episode I did with Lars Doucette who oh, I don't know about this. He's a Georgist. Oh, yeah, he's uh,
[00:59:49.100 --> 00:59:52.140]   He's he's a really smart guy. He wrote
[00:59:52.140 --> 00:59:57.020]   Basically, he wrote a book review of Henry George that one Scott Alexander's book review contest
[00:59:57.020 --> 01:00:01.420]   Oh, I know what this and then he's turned it into a whole book of his own, which is actually really good
[01:00:01.420 --> 01:00:08.820]   And I think there's something truly humane in George when you read him that can be a bit infectious. That's positive. Mm-hmm
[01:00:09.860 --> 01:00:15.300]   And it's something say there was some insane turnout for his funeral, right? Like he was very popular at the time, right?
[01:00:15.300 --> 01:00:17.420]   Oh, yeah. Yeah, and that was deserved. Yeah
[01:00:17.420 --> 01:00:19.540]   I guess you already answered this question
[01:00:19.540 --> 01:00:25.100]   But Ronald Coase in terms of helping us think about firms and you know property rights and transaction costs
[01:00:25.100 --> 01:00:30.660]   Well, even though I think the 1937 piece is wrong. It did create one of the most important genres
[01:00:30.660 --> 01:00:34.620]   He gets a lot of credit for that. He gets a lot of credit for the Coase theorem
[01:00:34.740 --> 01:00:40.160]   The FCC property rights piece is superb. The lighthouse piece is very good again
[01:00:40.160 --> 01:00:44.540]   He's in the top 25 but in terms of you'd like his quantity its own quality
[01:00:44.540 --> 01:00:48.260]   It's just not quite enough. There's no macro
[01:00:48.260 --> 01:00:53.280]   But of course you rate him very very highly. How about your former advisor Thomas Schelling?
[01:00:53.280 --> 01:01:00.680]   He is a top-tier Nobel Laureate, but I don't think he's a serious contender for greatest economist of all time
[01:01:00.820 --> 01:01:07.080]   He gets the most credit for making game theory intuitive empirical and workable and that's worth a lot
[01:01:07.080 --> 01:01:15.200]   Economics of self-command he was a pioneer but in a way that's just going back to the Greeks and Smith
[01:01:15.200 --> 01:01:20.300]   He's not a serious contender for goat, but a top-tier Nobel Laureate for sure
[01:01:20.300 --> 01:01:26.860]   You have he have a fun quote in in the book on arrow where you say his work was a Nobel Prize winning important
[01:01:26.860 --> 01:01:28.860]   But not important important
[01:01:28.940 --> 01:01:32.560]   Well, some parts of it were important important like how to price securities
[01:01:32.560 --> 01:01:37.780]   So I think I underrated arrow a bit in the book if you ask like what regrets do I have about the book?
[01:01:37.780 --> 01:01:44.300]   I say very very nice things about arrow, but I think I should have pushed him even more. What would error say my prediction markets?
[01:01:44.300 --> 01:01:49.740]   Well, he was really the pioneer of theoretically understanding how they work. Yeah, so
[01:01:49.740 --> 01:01:57.520]   He was around until quite recently, I'm sure he had things to say about prediction markets probably positive. Hmm
[01:01:58.700 --> 01:02:00.020]   I
[01:02:00.020 --> 01:02:05.380]   Knew so what one of the points you make in the book is economics at the time was really a way of carrying forward big
[01:02:05.380 --> 01:02:10.740]   Ideas about the world what discipline today is where that happens? Well internet writing
[01:02:10.740 --> 01:02:15.180]   It's not a discipline, but it's a sphere and plenty of it happens more than ever before
[01:02:15.180 --> 01:02:20.680]   But it's segregated from what counts as original theorizing in the academic sense of that word
[01:02:20.680 --> 01:02:23.820]   Is that a good or bad segregation? I'm not sure
[01:02:24.900 --> 01:02:31.720]   But it's really a very sharp radical break from how things had been and it's why I don't think there'll be a new goat contender
[01:02:31.720 --> 01:02:37.760]   Probably not ever or if there is it will be something AI related. Hmm. Yeah, that's that sounds about right to me
[01:02:37.760 --> 01:02:44.380]   But within the context of internet writing, obviously, there's many disciplines their economics being a prominent one. Yeah
[01:02:44.380 --> 01:02:46.740]   When you split it up
[01:02:46.740 --> 01:02:52.940]   Is there a discipline in terms of I don't people writing in terms of computer science concepts or people writing in terms of economic concepts?
[01:02:52.940 --> 01:02:58.600]   Who is today? Yes, the discipline cease to matter that really good internet writing is multidisciplinary
[01:02:58.600 --> 01:03:06.280]   When I meet someone like a Scott Aronson who's doing like computer science AI type internet writing on his blog
[01:03:06.280 --> 01:03:12.780]   I have way more in common with him than with a typical research economist say at Boston University
[01:03:12.780 --> 01:03:18.580]   And it's not because I know enough about computer science like I may or may not know a certain amount
[01:03:18.580 --> 01:03:24.820]   But it's because our two enterprises are so similar or Scott Alexander. He writes about mental illness also
[01:03:24.820 --> 01:03:30.380]   That just feels so similar and we really have to rethink what the disciplines are
[01:03:30.380 --> 01:03:35.300]   It may be that method of writing is the key differentiator for this particular sphere
[01:03:35.300 --> 01:03:40.500]   Yeah, it's got everything Scott Aronson was my professor in college. Oh, yeah. Yeah
[01:03:40.500 --> 01:03:44.880]   That that's where I decided I'm not gonna go to grad school because you just see
[01:03:44.880 --> 01:03:47.900]   Because you see like two standard deviations above you easily
[01:03:48.580 --> 01:03:50.580]   You know, you might as well choose a different game
[01:03:50.580 --> 01:03:56.700]   But his method of thinking and writing is infectious like that is Scott Alexander. Yeah of the rest of us. Yeah
[01:03:56.700 --> 01:04:02.580]   So I think in the book you say you were raised as much by
[01:04:02.580 --> 01:04:09.460]   Economic thought or the history of economic thought as you are by your graduate training more much more not it's not even close
[01:04:09.460 --> 01:04:14.300]   Today people would say I was talking to Basil Hopper and who's a young economist
[01:04:14.300 --> 01:04:18.240]   He said he was raised on Marginal Revolution on it in the same way that you were raised on the history of economic talk
[01:04:18.240 --> 01:04:20.340]   Thought how do you?
[01:04:20.340 --> 01:04:26.940]   Does this seem like a good trade? Are you happy that people today are raised on Scott Alexander and Marginal Revolution at the margin?
[01:04:26.940 --> 01:04:29.420]   I would like to see more people raised on Marginal Revolution
[01:04:29.420 --> 01:04:31.380]   I don't just mean that in a selfish way
[01:04:31.380 --> 01:04:38.060]   Yeah, the internet writing mode of thinking I would like to see more economists and research scientists raised on it
[01:04:38.060 --> 01:04:44.000]   But the number may be higher than we think like if I hadn't run emergent ventures, I wouldn't know about Basil per se
[01:04:44.280 --> 01:04:46.280]   maybe would not have met him and
[01:04:46.280 --> 01:04:52.340]   It's infectious. So it might always be a minority, but it will be the people most likely to have new ideas
[01:04:52.340 --> 01:04:57.560]   and it's a very powerful new mode of thought which I'll call internet way of writing and thinking and
[01:04:57.560 --> 01:05:03.040]   It's not sufficiently recognized as something like a new field or discipline, but that's what it is. Mm-hmm
[01:05:03.040 --> 01:05:08.160]   I wonder if you're doing enough of that when it comes to AI where I think you have really interesting thoughts about GPT 5 level
[01:05:08.160 --> 01:05:12.160]   Stuff but it's not somebody with your sort of polymathic understanding or different fields
[01:05:12.680 --> 01:05:15.040]   If you just like extrapolate out these trends
[01:05:15.040 --> 01:05:19.240]   I feel like you might have a lot of interesting thoughts about when you think just in terms of what might be possible with
[01:05:19.240 --> 01:05:21.160]   Something much further down the line
[01:05:21.160 --> 01:05:27.060]   well, I have a whole book with AI predictions averages over and I have about 30 Bloomberg columns and
[01:05:27.060 --> 01:05:32.480]   Probably 30 or 40 Marginal Revolution posts. I can just say I'll do more
[01:05:32.480 --> 01:05:39.220]   But the idea at which ideas arrive at me is the binding constraint. I'm not holding them back
[01:05:40.520 --> 01:05:44.160]   Speaking of basil. He had an interesting question. Should
[01:05:44.160 --> 01:05:51.560]   Should should society or government subsidize savings so that we're in effect having
[01:05:51.560 --> 01:05:58.720]   It leads to basically a zero social discount rate. So it's people on average. I probably have you know
[01:05:58.720 --> 01:06:03.340]   They're they're prioritizing their own lives. They have discount rates based on their own lives
[01:06:03.960 --> 01:06:09.280]   If we're long-term, I should the government's upset be subsidizing savings. I'll come close to saying yes first
[01:06:09.280 --> 01:06:13.760]   We tax savings right now, so we should stop taxing savings. Absolutely
[01:06:13.760 --> 01:06:18.640]   I think it's hard to come up with workable ways of subsidizing savings
[01:06:18.640 --> 01:06:24.640]   That don't give rich people a lot of free stuff in a way that's politically unacceptable and also unfair
[01:06:24.640 --> 01:06:28.360]   So I'm not sure we have a good way of subsidizing savings
[01:06:28.360 --> 01:06:32.320]   But in principle, I would be for it if we could do it in the proper targeted manner
[01:06:32.600 --> 01:06:37.480]   Hmm, although you had a good argument against this in stubborn attachments, right that over the long term
[01:06:37.480 --> 01:06:41.680]   if the if economic growth is high enough, then the savings of the rich will just be
[01:06:41.680 --> 01:06:45.080]   Dissipated to everybody below
[01:06:45.080 --> 01:06:50.860]   Well, I'm not sure to whom it's dissipated. It does get dissipated the great fortunes of the past are mostly gone
[01:06:50.860 --> 01:06:56.680]   But they may not go to people below and the idea of writing into a tax system
[01:06:57.080 --> 01:07:04.320]   Subsidies on that scale in essence subsidies to wealth not not GDP, but wealth is say six to eight times GDP
[01:07:04.320 --> 01:07:08.800]   I just think the practical problems are quite significant. It's not an idea. I'm pushing
[01:07:08.800 --> 01:07:11.960]   But there may at margins be ways you can do it
[01:07:11.960 --> 01:07:16.240]   That say only benefit people who are poor ways
[01:07:16.240 --> 01:07:22.200]   You can improve like the workings of local credit unions through better either regulation deregulation
[01:07:22.200 --> 01:07:27.360]   That are a kind of de facto subsidy without having to subsidize all of the saved wealth
[01:07:27.360 --> 01:07:30.800]   There's got to be a lot of ways you can do that and we should look for that more
[01:07:30.800 --> 01:07:37.360]   Relatedly, I think a couple years ago Paul Schleming had an interesting paper that if you look from 1311 to now
[01:07:37.360 --> 01:07:45.400]   Interest rates have been declining that there's been hundreds of years of interest rate declines. Well, what is the big-picture explanation of this trend?
[01:07:45.400 --> 01:07:51.200]   I'm not sure we have one. You may know Cowen's third law all propositions about real interest rates are wrong
[01:07:51.400 --> 01:07:55.360]   But simply lower risk better information higher buffers of wealth
[01:07:55.360 --> 01:07:57.880]   Would be what you'd call the intuitive
[01:07:57.880 --> 01:08:06.360]   Economistic explanations, there's probably something to them. But how much of that trend do they actually explain is like a percent of the variance
[01:08:06.360 --> 01:08:08.580]   I don't know. Mm-hmm. Let's talk about anarchy
[01:08:08.580 --> 01:08:13.440]   You feel papers about this. I hadn't read the last time we talked and they're really interesting
[01:08:13.440 --> 01:08:15.520]   so
[01:08:15.520 --> 01:08:18.080]   Maybe you can restate your arguments as you answer this question
[01:08:18.080 --> 01:08:23.240]   But how much of your arguments about how network industries lead to these cartel like dynamics?
[01:08:23.240 --> 01:08:30.520]   How much of that can help explain what happened to social media web 2.0? I don't view that as such a cartel
[01:08:30.520 --> 01:08:34.880]   I think there's a cartel at one level which is small but significant
[01:08:34.880 --> 01:08:40.580]   This is maybe more true three four years ago than today with Elon owning Twitter and other changes
[01:08:40.580 --> 01:08:43.240]   But if someone got kicked off social media platforms
[01:08:43.760 --> 01:08:49.000]   Three four years ago, they would tend to get kicked off all or most of them. It wasn't like a
[01:08:49.000 --> 01:08:50.920]   consciously
[01:08:50.920 --> 01:08:56.440]   Collusive decision, but it's a bit like oh, I know the guy who runs that platform and he's pretty smart
[01:08:56.440 --> 01:08:58.520]   and if he's worried I should be worried and
[01:08:58.520 --> 01:09:05.800]   That was very bad. I don't think it was otherwise such a collusive equilibrium. Maybe some dimensions on hiring
[01:09:05.800 --> 01:09:13.120]   Social you know people software engineers there was some collusion not enough bidding
[01:09:13.640 --> 01:09:16.500]   But it was mostly competing for attention
[01:09:16.500 --> 01:09:23.080]   So I think the real risk protection agencies aside of network based collusion is through banking systems
[01:09:23.080 --> 01:09:27.840]   Where you have clearing houses and payments networks and to be part of it
[01:09:27.840 --> 01:09:33.720]   The clearing house in the absence of legal constraint can indeed help everyone collude
[01:09:33.720 --> 01:09:37.120]   And if you don't go along with the collusion, you're kicked out of the payment system
[01:09:37.120 --> 01:09:39.640]   That strikes me as a real issue
[01:09:40.240 --> 01:09:46.480]   Do your arguments against anarchy do they apply it all to web 3.0 crypto like stuff?
[01:09:46.480 --> 01:09:54.240]   Do I think it will evolve into collusion, I don't see why it would I'm open to hearing the argument that it could though
[01:09:54.240 --> 01:09:56.240]   What would that argument go like?
[01:09:56.240 --> 01:09:59.200]   Well, I guess we did see with crypto that you have
[01:09:59.200 --> 01:10:05.880]   In order to just have workable settlement you needed these centralized institutions and
[01:10:06.320 --> 01:10:10.160]   from there you can get kicked off those and the government is involved with those and you can
[01:10:10.160 --> 01:10:14.880]   You can maybe abstract the government away and say that they will need to collude in some sense in order to facilitate
[01:10:14.880 --> 01:10:18.560]   Transactions and the exchanges have ended up quite centralized, right? Yeah
[01:10:18.560 --> 01:10:23.420]   And that's an example of clearing houses and exchanges writing the vulnerable node
[01:10:23.420 --> 01:10:26.960]   But I don't know how much web 3.0 is ever going to rely on that
[01:10:26.960 --> 01:10:34.160]   It seems you can create new crypto assets more or less. It will there's the focality of getting them started
[01:10:35.160 --> 01:10:38.880]   But if there's a real problem with the pre-existing crypto assets
[01:10:38.880 --> 01:10:45.960]   I would think you could overcome that so I would expect something more like a to and fro waves of centralization decentralization and
[01:10:45.960 --> 01:10:53.540]   Natural checks embedded in the system. That's my intuition at least does your argument against anarchy prove too much in the sense that globally?
[01:10:53.540 --> 01:11:00.640]   Different nations have anarchic relations with each other and they can't they can't enforce course and monopoly on each other
[01:11:00.640 --> 01:11:05.600]   But they can't coordinate to punish bad actors and the way you would want protection agencies to do right like we can sanction North
[01:11:05.600 --> 01:11:09.240]   Korea together or something. I think that's a very good point and very good question
[01:11:09.240 --> 01:11:15.600]   But I would rephrase my argument you could say it's my argument against anarchy and it is an argument against anarchy
[01:11:15.600 --> 01:11:19.560]   But it's also an argument that says anarchy is everywhere. So within government
[01:11:19.560 --> 01:11:24.480]   Well, the feds the state governments all the different layers of federalism. There's a kind of anarchy
[01:11:24.480 --> 01:11:29.760]   There's not quite a final layer of adjudication the way you might think we pretend there is
[01:11:29.880 --> 01:11:32.820]   I'm not sure how strong it is internationally, of course
[01:11:32.820 --> 01:11:40.720]   You know how much gets enforced by hegemon how much is spontaneous order even the different parts of the federal government
[01:11:40.720 --> 01:11:43.840]   They're in a kind of anarchy with respect to each other
[01:11:43.840 --> 01:11:49.840]   So you need a fair degree of collusion for things to work and you ought to accept that
[01:11:49.840 --> 01:11:53.280]   But maybe in a Straussian way where you don't trumpet it too loudly
[01:11:54.600 --> 01:12:02.320]   but the point that anarchy itself will evolve enough collusion to enable it to persist if it persists at all is
[01:12:02.320 --> 01:12:06.520]   My central point my point is like well anarchy isn't that different?
[01:12:06.520 --> 01:12:13.440]   Now given we've put a lot of social political capital into our current institutions. I
[01:12:13.440 --> 01:12:20.880]   Don't see why you would press the anarchy button, but if I'm North Korea and I can press the anarchy button for North Korea, I
[01:12:21.600 --> 01:12:24.020]   Get that it might just evolve into Haiti
[01:12:24.020 --> 01:12:30.200]   But I probably would press the anarchy button for North Korea. If at least someone would come in and control the loose nukes
[01:12:30.200 --> 01:12:36.120]   Yeah, this is related to one of those classic arguments against the anarchy that under anarchy anything is allowed. So the government is allowed
[01:12:36.120 --> 01:12:41.320]   Therefore we're in a state of anarchy in some sense in a funny way that arguments, correct
[01:12:41.320 --> 01:12:48.340]   We have revolved something like government and Haiti has done this but in very bad ways where it's right gangs and killings
[01:12:48.720 --> 01:12:55.320]   It doesn't have to be that bad. There's medieval Iceland medieval Ireland. They had various forms of anarchy
[01:12:55.320 --> 01:12:59.040]   Clearly limited in their destructiveness by low population
[01:12:59.040 --> 01:13:01.400]   ineffective weapons
[01:13:01.400 --> 01:13:03.360]   But they had a kind of stability
[01:13:03.360 --> 01:13:07.680]   You can't just dismiss them and you can debate how governmental were they?
[01:13:07.680 --> 01:13:12.680]   but the ambiguity of those debates is part of the point that every system has a lot of anarchy and
[01:13:12.680 --> 01:13:15.640]   Anarchies have a fair degree of collusion if they survive
[01:13:15.640 --> 01:13:20.880]   oh, actually, so I want to go back to much earlier in the conversation where you're saying listen over
[01:13:20.880 --> 01:13:26.580]   It seems like intelligence is is a net good. So just that being your heuristic you should
[01:13:26.580 --> 01:13:32.400]   Call forth the AI. Well, not uncritically you need more argument, but just as a starting point
[01:13:32.400 --> 01:13:36.960]   Yeah, it's like if more intelligence isn't gonna help you you have some really big problems
[01:13:36.960 --> 01:13:42.600]   Anyway, but I don't know if you still have the view that we're we have like an 800 year timeline for human civilization
[01:13:42.600 --> 01:13:44.600]   but that that sort of timeline implies that
[01:13:45.280 --> 01:13:49.240]   Intelligence actually is gonna be the because well the thing the reason we have an 800 year timeline
[01:13:49.240 --> 01:13:51.920]   Presumably is like some product of intelligence, right?
[01:13:51.920 --> 01:13:59.400]   My worry is that energy becomes too cheap and people at very low cost can destroy things rather easily
[01:13:59.400 --> 01:14:05.760]   So say if a nuclear if destroying a city with a nuclear weapon cost $50,000
[01:14:05.760 --> 01:14:13.880]   What would the world look like I'm just not sure it might be more stable than we think but I'm greatly worried and I could
[01:14:13.880 --> 01:14:18.040]   Readily imagine it falling apart. Yeah, but I guess the bigger point I'm making is that
[01:14:18.040 --> 01:14:22.160]   in this case the reason the nuke got so cheap was
[01:14:22.160 --> 01:14:26.600]   Because of intelligence now that doesn't mean we should stop intelligence
[01:14:26.600 --> 01:14:32.760]   But it just that in if if that's like the end result of intelligence over hundreds of years. That doesn't seem like intelligence is always
[01:14:32.760 --> 01:14:36.520]   And that good. Well, we're doing better than the other great apes
[01:14:36.520 --> 01:14:41.680]   I would say even though we face these really big risks and in the meantime, we did incredible things
[01:14:41.680 --> 01:14:47.280]   so that's a gamble I would take but I believe we should view it more self-consciously as a sort of gamble and
[01:14:47.280 --> 01:14:49.080]   It's too late to turn back
[01:14:49.080 --> 01:14:56.320]   the fundamental choice was one of decentralization and that may have happened hundreds of millions or billions of years ago and
[01:14:56.320 --> 01:14:58.680]   Once you opt for decentralization
[01:14:58.680 --> 01:15:04.280]   Intelligence is going to have advantages and you're not going to be able to turn the clock back on it
[01:15:04.280 --> 01:15:09.040]   So you're walking this tightrope and by goodness, you'd better do a good job. I
[01:15:09.640 --> 01:15:16.160]   Mean we should frame our broader history more like that and it has implications for how you think about x-risk again
[01:15:16.160 --> 01:15:18.780]   I think if the x-ray x-risk people a bit of them
[01:15:18.780 --> 01:15:23.980]   It's like well, I've been living in Berkeley a long time and it's it's really not that different
[01:15:23.980 --> 01:15:29.960]   My life's a bit better and we can't risk all of this, but that's not how you should view broader history
[01:15:29.960 --> 01:15:32.920]   I feel like you're an expert person you have a yeah
[01:15:32.920 --> 01:15:36.880]   I mean even they don't think we're like a hundred percent guaranteed to go out by 800 years or something
[01:15:36.880 --> 01:15:41.860]   They're guaranteed at all. It's up to us. I just think the risk not that everyone dies
[01:15:41.860 --> 01:15:48.960]   I think that's quite low but that we retreat to some kind of pretty chaotic form of like medieval Balkans existence
[01:15:48.960 --> 01:15:57.200]   With a much lower population that seems to me quite a high risk with or without AI. It's probably the default setting. Mm-hmm
[01:15:57.200 --> 01:16:00.960]   Given that you think that's a default setting
[01:16:00.960 --> 01:16:06.640]   Why is that not a big part of your when you're thinking about how new technologies are coming about?
[01:16:07.160 --> 01:16:12.440]   why why not consciously think in terms of is this getting us to the outcome where we avoid this sort of
[01:16:12.440 --> 01:16:16.960]   Free industrial state that would result from the the $50,000 nukes
[01:16:16.960 --> 01:16:23.820]   Well, if you think the risk is cheap energy more than AI per se admittedly AI could speed the path to cheap energy
[01:16:23.820 --> 01:16:26.360]   It seems very hard to control
[01:16:26.360 --> 01:16:34.720]   The strategy that's worked best so far is to have relatively benevolent nations become hegemons and established dominance
[01:16:35.560 --> 01:16:38.920]   So it does influence me. I want the US UK
[01:16:38.920 --> 01:16:43.860]   Some other subset of nations to establish dominance and AI it may not work forever
[01:16:43.860 --> 01:16:49.640]   But in a decentralized world, it sure beats the alternative. So a lot of the AI types
[01:16:49.640 --> 01:16:55.320]   They're too rationalist and they don't start with the premise that we chose a decentralized world a very very long time ago
[01:16:55.320 --> 01:16:57.320]   even way before humans and
[01:16:57.320 --> 01:17:03.520]   I think you made interesting point when you were talking about Keynes in the book where he said one of his faults was that he
[01:17:03.720 --> 01:17:05.800]   Assumed that people like him would always be in charge
[01:17:05.800 --> 01:17:06.360]   That's right
[01:17:06.360 --> 01:17:11.320]   and I do see that also in the the alignment discourse like alignment is you know if it's just handing over the government and just
[01:17:11.320 --> 01:17:15.200]   Assuming the government does what you'd expect it to do and I worry about this from my own point of view
[01:17:15.200 --> 01:17:20.800]   So even if you think u.s. Is pretty benevolent today, which is a highly contested and mixed proposition
[01:17:20.800 --> 01:17:23.360]   And I'm an American citizen pretty patriotic
[01:17:23.360 --> 01:17:29.880]   but I'm fully aware of the long history of my government in killing and slaving doing other terrible things to people and
[01:17:30.400 --> 01:17:35.320]   Then you have to rethink that over a long period of time. It may be the worst
[01:17:35.320 --> 01:17:39.960]   Time period that affects the final outcome even if the average is pretty good
[01:17:39.960 --> 01:17:44.200]   And then if power corrupts and if government even indirectly
[01:17:44.200 --> 01:17:49.960]   Controls AI system so US government could become worse because it's a leader in AI, right?
[01:17:49.960 --> 01:17:54.600]   but again, I've got to still take that over China or Russia or
[01:17:54.600 --> 01:17:57.600]   wherever else it might be I
[01:17:57.960 --> 01:18:02.720]   Just don't really understand when people talk about national security
[01:18:02.720 --> 01:18:08.120]   I've never seen the AI doomers say anything that made sense and I recall those early days
[01:18:08.120 --> 01:18:14.880]   Remember China issued that edict where they said we're only gonna put a a eyes that are safe and they can't criticize the CCP
[01:18:14.880 --> 01:18:18.580]   How many super smart people and I mean super smart likes V?
[01:18:18.580 --> 01:18:23.520]   Just jump on that and say see China's not going to compete with us. We can shut AI down
[01:18:23.760 --> 01:18:31.120]   They just seem to have zero understanding of some properties of decentralized worlds or a leisure's tweet. Was it from yesterday?
[01:18:31.120 --> 01:18:32.800]   I
[01:18:32.800 --> 01:18:34.040]   Didn't think it was a joke
[01:18:34.040 --> 01:18:38.520]   Oh that there's a problem that AI can read all the legal code and threaten us with all these penalties
[01:18:38.520 --> 01:18:41.820]   It's like he has no idea how screwed up the legal system is
[01:18:41.820 --> 01:18:47.280]   Mmm. Yeah, it would just mean courtroom waits of like 70 or 700 years
[01:18:47.280 --> 01:18:51.780]   It's not it wouldn't become a thing people are afraid of it would be a social problem in some way
[01:18:52.240 --> 01:18:56.040]   What's your sense of how the government reacts when the labs are doing?
[01:18:56.040 --> 01:19:01.080]   regardless of how they should react how they will react and when the letter doing like I don't know tell me ten billion dollar training
[01:19:01.080 --> 01:19:06.640]   Runs and if if under the premise that you know, these are powerful models not human level per se
[01:19:06.640 --> 01:19:10.120]   But just they can do all kinds of crazy stuff. How do you think the government's gonna?
[01:19:10.120 --> 01:19:14.600]   Are they gonna nationalize the labs or how do you stay in Washington? What's your sense?
[01:19:14.600 --> 01:19:20.000]   I think our national security people are amongst the smartest people in our government. Mm-hmm
[01:19:20.280 --> 01:19:25.640]   They're mostly well intentioned in a good way. They're paying careful attention to many things
[01:19:25.640 --> 01:19:34.580]   But what will be the political will to do what they don't control and my guess is until there's sort of an SPF like incident
[01:19:34.580 --> 01:19:36.400]   Which might even not be significant
[01:19:36.400 --> 01:19:43.360]   But a headlines incident which SPF was even if it doesn't affect the future evolution of crypto, which I guess is my view. It won't
[01:19:44.640 --> 01:19:50.480]   Until there's that we won't do much of anything and then we'll have an SPF like incident and will overreact
[01:19:50.480 --> 01:19:56.840]   That seems a very common pattern in American history and the fact that it's AI the stakes might be higher or whatever. I
[01:19:56.840 --> 01:20:00.400]   Doubt if it will change the recurrence of that pattern
[01:20:00.400 --> 01:20:04.800]   How would Robert Nozick think about different AI utopias?
[01:20:04.800 --> 01:20:09.240]   Well, I think he did think about different AI utopias, right?
[01:20:09.240 --> 01:20:12.000]   so I
[01:20:12.000 --> 01:20:14.680]   Believe he whether he wrote or talked about it
[01:20:14.680 --> 01:20:21.520]   But the notion of humans much smarter than they are or the notion of aliens coming down who are like in some way morally
[01:20:21.520 --> 01:20:27.920]   Intellectually way beyond us. He did write about that and he was worried about how they would treat us. So he was sensitive
[01:20:27.920 --> 01:20:32.240]   To what you would call AI risk viewed a bit more broadly very early on
[01:20:32.240 --> 01:20:35.120]   Hmm. What was this? What was his take?
[01:20:35.600 --> 01:20:42.980]   Well Nozick is not a thinker of of takes he was a thinker of speculations and multiple possibilities. I liked about him
[01:20:42.980 --> 01:20:45.760]   He was worried about it this I know
[01:20:45.760 --> 01:20:48.560]   And I talked to him about it
[01:20:48.560 --> 01:20:50.940]   But I couldn't boil it down to a simple take
[01:20:50.940 --> 01:20:55.000]   They made him a vegetarian I should add wait that made him
[01:20:55.000 --> 01:20:58.440]   Oh because we want to be treating the the entities that are to us as a
[01:20:58.440 --> 01:21:04.800]   Aliens from outer space might treat us. We are like that to animals may not be a perfect analogy
[01:21:04.800 --> 01:21:11.060]   But it's still an interesting point and therefore we should be vegetarians. That was his argument at least he felt he should be
[01:21:11.060 --> 01:21:17.760]   I wonder if we should honor past generations more or at least like respect their wishes more for if we think of the the alignment problem
[01:21:17.760 --> 01:21:21.520]   It's a similar to how we react to our previous generations
[01:21:21.520 --> 01:21:26.200]   Do you know if we do we want the eyes to treat us as we treat people thousands of years ago?
[01:21:26.200 --> 01:21:28.560]   Yeah, it's a good question. And
[01:21:28.560 --> 01:21:33.480]   I've never met anyone who's consistent with how they view wishes of the dead. Yeah
[01:21:33.480 --> 01:21:37.880]   I don't think there is a consistent philosophically grounded point of view on that one
[01:21:37.880 --> 01:21:43.360]   Hmm, I guess the the sort of Thomas Paine view you don't regard them at all. Is that not self-consistent?
[01:21:43.360 --> 01:21:47.080]   It's consistent, but I've never met anyone who actually lives according to it. Oh
[01:21:47.080 --> 01:21:49.600]   And what since are they contradicting themselves?
[01:21:49.600 --> 01:21:56.320]   Well say, you know, they're their spouse were to die and the spouse gave them instructions. Sure. They would put weight on those instructions
[01:21:56.320 --> 01:22:01.220]   Somewhere out there. There's probably someone who wouldn't but I've never met such a person
[01:22:01.360 --> 01:22:06.680]   Hmm, and how about the Burke view that you take them very seriously. Why is that not self-consistent the birth view?
[01:22:06.680 --> 01:22:08.680]   What do you mean Burke Berkeley? Oh
[01:22:08.680 --> 01:22:15.280]   Well, it's time inconsistent to take those preferences seriously and Burke himself understood that he was a very deep thinker
[01:22:15.280 --> 01:22:22.280]   So well, you take them seriously now, but as time passes other ancestors come along. They have somewhat different views
[01:22:22.280 --> 01:22:25.440]   You have to keep on changing course what you should do now
[01:22:25.880 --> 01:22:33.220]   Should it be what the ancestors behind us want or your best estimate of what the 30 or 40 years of ancestors to come?
[01:22:33.220 --> 01:22:40.560]   Will want once they have become ancestors, so it's time inconsistent. It is not again. There's not going to be a strictly philosophical
[01:22:40.560 --> 01:22:49.920]   Resolution there will be practical attempts to find something sustainable and that which survives will be that which we do and then we'll
[01:22:49.920 --> 01:22:54.200]   Somewhat rationalize it exposed. Yeah. Yeah, there's an interesting book about
[01:22:55.400 --> 01:23:01.000]   The ancient ancient Greeks. What was it called? I forgot the name but it's talks about the hearts that they have for the
[01:23:01.000 --> 01:23:04.120]   For their families where the the dead become gods
[01:23:04.120 --> 01:23:07.240]   But then over time if you keep this heart going for hundreds of years
[01:23:07.240 --> 01:23:11.120]   It's like thousands of ancestors that led you didn't remember their names, right? Who are you praying to?
[01:23:11.120 --> 01:23:14.760]   And then it's like the arrow impossibility theorem for all the gods
[01:23:14.760 --> 01:23:19.440]   Yeah
[01:23:19.440 --> 01:23:24.200]   Okay, we were talking before we started recording about Argentina and the reforms are trying there
[01:23:24.920 --> 01:23:28.880]   And they're trying to dollarize because the dollar is more stable than their currency
[01:23:28.880 --> 01:23:31.480]   But this raises the question of why is a dollar so stable?
[01:23:31.480 --> 01:23:38.040]   So we're also democracy right but like the dollar that seems pretty well managed. What is the larger explanation of why?
[01:23:38.040 --> 01:23:41.620]   Monetary policies seems well managed in the u.s
[01:23:41.620 --> 01:23:48.440]   Well u.s. Voters hate inflation mostly for good reasons and we have enough wealth that we can pay our bills
[01:23:48.440 --> 01:23:53.480]   Without having to inflate very much and 2% has been stable now for quite a while
[01:23:53.960 --> 01:23:59.720]   Now it's an interesting question, which I cannot answer and I have looked into this and I have asked smart people from Argentina
[01:23:59.720 --> 01:24:04.760]   Why does Argentina in particular have recurring waves of hyperinflation?
[01:24:04.760 --> 01:24:09.120]   Is there something about the structure of their interest groups that?
[01:24:09.120 --> 01:24:13.760]   Inevitably recurringly leads them to demand too much I suppose
[01:24:13.760 --> 01:24:17.320]   But there's plenty of poor badly run countries that don't have hyperinflation
[01:24:18.160 --> 01:24:23.800]   African countries historically have not had high rates of hyperinflation haven't had high rates of inflation
[01:24:23.800 --> 01:24:25.960]   Why is that?
[01:24:25.960 --> 01:24:32.760]   Well, maybe they don't capture enough through senior age for some reason currency holdings aren't large enough. There's some kind of financial repression
[01:24:32.760 --> 01:24:40.640]   I don't know, but it's very hard to explain why some of these countries but not others go crazy with the printing press. Mm-hmm
[01:24:40.640 --> 01:24:45.320]   And this is maybe a broader question about different institutions in the government
[01:24:45.960 --> 01:24:50.680]   Where I don't understand enough to evaluate they're like object level decisions
[01:24:50.680 --> 01:24:54.320]   But if you look at the Supreme Court or the Federal Reserve or something just from a distance
[01:24:54.320 --> 01:24:59.320]   It seems like they're really well-run competent organizations with a highly technocratic, you know
[01:24:59.320 --> 01:25:04.240]   Nonpartisan people running them. They're not nonpartisan, but they're still well run. Yeah, and
[01:25:04.240 --> 01:25:09.600]   What's the theory of why these issues in particular are so much better run?
[01:25:09.600 --> 01:25:13.760]   Is it just that they are divorce that we're one step back from direct elections?
[01:25:13.760 --> 01:25:18.960]   Is it that they have traditions of knowledge within them? How do we think about this? I think both of those
[01:25:18.960 --> 01:25:24.600]   I don't think the elections point is sufficient because there's plenty of unelected bodies. They're totally corrupt
[01:25:24.600 --> 01:25:26.600]   Yeah around the world. Most of them are perhaps
[01:25:26.600 --> 01:25:35.540]   Some sense of American civic virtue that gets communicated and then the incentives are such say you're on the Fed for a while
[01:25:35.540 --> 01:25:41.560]   What you can do afterward can be rewarding, but you want a reputation for having done a good job
[01:25:42.080 --> 01:25:49.120]   So your sense of morality and your private self-interest coincide and that's pretty strong and we're still in that loop
[01:25:49.120 --> 01:25:52.160]   I don't really see signs of that loop breaking. Mmm
[01:25:52.160 --> 01:25:57.240]   It was also striking to me how many times I'll read an interesting article or paper and the person who wrote it
[01:25:57.240 --> 01:25:59.320]   It's like the former head of the Federal Reserve in New York or something
[01:25:59.320 --> 01:26:03.760]   It just seems like a that's a strong vindication of these institutions that the standards are very high
[01:26:03.760 --> 01:26:10.440]   And if you speak with any of those people like we've been on Fed boards ask them questions. They're super smart super involved curious
[01:26:11.520 --> 01:26:14.240]   Really for the most part do want the best thing for their country
[01:26:14.240 --> 01:26:19.100]   Going back to these economists you at the end you talked about how you're kind of
[01:26:19.100 --> 01:26:23.720]   Disappointed in this turn that economics has taken maybe I just I'm not surprised right?
[01:26:23.720 --> 01:26:30.240]   It's division of labor Adam Smith who said it would make people a bit feeble-minded and incurious was completely correct
[01:26:30.240 --> 01:26:35.880]   Wait, Adam Smith said what would make people a division of labor? I see right? Yeah, not stupid
[01:26:36.240 --> 01:26:41.960]   You know current economic researchers probably have never been smarter, but they're way less broad and less curious
[01:26:41.960 --> 01:26:49.920]   Patrick Olson, but put it an interesting way where he said in the past maybe thinkers were more interested in
[01:26:49.920 --> 01:26:55.680]   Delving into the biggest questions, but if they they couldn't do it rigorously in a tractable way
[01:26:55.680 --> 01:27:00.320]   They would just they would made the trade-off in favor of the big question and today we make the opposite trade-off
[01:27:00.320 --> 01:27:02.080]   I does that seem like a fair comparison?
[01:27:02.080 --> 01:27:06.400]   I think that's correct. And I would add that saying the time of Smith. There was nothing you could do rigorously
[01:27:06.400 --> 01:27:08.260]   So there was no other option
[01:27:08.260 --> 01:27:15.200]   Well, oh, I'm gonna specialize and you know memorizing all the grain prices and run some great econometrics on that and that'll be rigorous
[01:27:15.200 --> 01:27:17.920]   It's really William Stanley Jevons
[01:27:17.920 --> 01:27:23.540]   Who to the Anglo world introduced this notion? There's something else you can do that's rigorous
[01:27:23.540 --> 01:27:30.660]   It was not yet rigorous, but he opened the door and showed people the alternative. Mm-hmm of the Jevons paradox
[01:27:31.500 --> 01:27:32.220]   I
[01:27:32.220 --> 01:27:36.100]   Would say his work in statistics originally on the value of money, right?
[01:27:36.100 --> 01:27:40.820]   but his statistical work on call also had some rigor so you're not wrong to cite them and
[01:27:40.820 --> 01:27:47.900]   Jevons just showed that rigorous statistical work and economics could be the same thing and
[01:27:47.900 --> 01:27:53.700]   That was his greater innovation than just marginalism. So he's an underrated figure
[01:27:53.700 --> 01:27:59.180]   Maybe he should be in the book in a way, but it had some unfortunate secondary consequences
[01:27:59.220 --> 01:28:05.740]   Too many people crowd into specialization crowd is a funny word to use because they're each sitting in their separate nodes
[01:28:05.740 --> 01:28:10.220]   But it's a kind of crowding. Is there some sort of Hayek in a solution here?
[01:28:10.220 --> 01:28:16.700]   Where in markets the effect of having this sort of decentralized process is the the sum is greater than the parts
[01:28:16.700 --> 01:28:22.780]   Whereas in academic disciplines the sum is just a bunch of different statistical aggregates
[01:28:22.780 --> 01:28:25.700]   there's no grand theory that comes together as a result of
[01:28:26.820 --> 01:28:29.900]   All this micro work. Is there some high-tech in solution here?
[01:28:29.900 --> 01:28:36.260]   Well, yes, you and I are the Hayek Ian solution that a specialist proliferate we can be quote-unquote
[01:28:36.260 --> 01:28:42.080]   parasitic on them and take what they do and turn it into interesting larger bundles that they haven't dreamt of and
[01:28:42.080 --> 01:28:47.020]   Make some kind of living doing that and we're much smaller in number
[01:28:47.020 --> 01:28:52.500]   But I'm not sure how numerous we should be and there's a bunch of us, right? Hmm. You're just ever category Tyler
[01:28:52.500 --> 01:28:55.260]   I'm running a podcast here
[01:28:55.260 --> 01:28:59.900]   I'm running a podcast and we're not in a separate category. We're exactly in the same category is my point
[01:28:59.900 --> 01:29:04.820]   Uh, and what do you see as a future of this sort of the kind of sort of thinking you do?
[01:29:04.820 --> 01:29:09.400]   Are you do you see yourself as the last of the literary economist or is there a future of this kind of?
[01:29:09.400 --> 01:29:13.180]   Is it just gonna be the slate-star codexes? Are they gonna take care of it or?
[01:29:13.180 --> 01:29:18.300]   This sort of lineage of thinking. Well, the next me won't be like me
[01:29:18.300 --> 01:29:23.140]   Yeah, in that sense, I'm the last but I don't think it will disappear. It will take new forms
[01:29:23.540 --> 01:29:29.620]   It may have a lot more to do with AI and I don't think it's going to go away. This is to demand for it
[01:29:29.620 --> 01:29:37.100]   There's a real demand for our products. We have a lot of readers listeners people interested whatever and there'll be ways to monetize that
[01:29:37.100 --> 01:29:40.300]   the challenge might be
[01:29:40.300 --> 01:29:46.340]   Competing against AI and it doesn't have to be that AI does it better than you or I do though it might
[01:29:46.340 --> 01:29:52.460]   But simply the people prefer to read what the AI is generate for 10 or 20 years and it's harder to get an audience
[01:29:52.820 --> 01:29:56.500]   Because playing with the AI's is a lot of fun. So that will be a real challenge
[01:29:56.500 --> 01:30:00.500]   I think some of us will be up to it. You'll be faced with it more than I will be
[01:30:00.500 --> 01:30:03.960]   But it's gonna change a lot. Yeah, okay
[01:30:03.960 --> 01:30:09.660]   One of the final things I want to do is I want to go into political philosophy a little bit. Okay, and
[01:30:09.660 --> 01:30:22.260]   Okay, so I want to ask you about sort of certain potential weaknesses of the democratic capitalist model that we live in and in
[01:30:22.260 --> 01:30:26.720]   terms of both in terms of whether you think they're object level law wrong, sorry object of a right and
[01:30:26.720 --> 01:30:29.380]   second how
[01:30:29.380 --> 01:30:31.740]   regardless of how right they are how how
[01:30:31.740 --> 01:30:37.340]   persuasive and how powerful a force they will be against our you know, our system of
[01:30:37.340 --> 01:30:40.220]   Government and functioning. Okay
[01:30:40.220 --> 01:30:42.900]   Okay, so there's a libertarian critique that basically
[01:30:42.900 --> 01:30:49.040]   Democracy is sort of a random walk with a drift towards socialism and there's also a ratchet effect where government programs don't go away
[01:30:49.040 --> 01:30:51.380]   And so it just ends up towards socialism at the end
[01:30:51.940 --> 01:30:55.660]   It ends up with having government that is too large. Yeah, but I
[01:30:55.660 --> 01:30:58.900]   Don't see the evidence that it's road to serfdom
[01:30:58.900 --> 01:31:03.500]   France Sweden have had pretty big governments way too large in my opinion
[01:31:03.500 --> 01:31:07.420]   But they haven't threatened to turn autocratic or totalitarian
[01:31:07.420 --> 01:31:12.940]   Certainly not and you've seen reforms in many of those countries Sweden moved away from
[01:31:12.940 --> 01:31:17.260]   Government approaching 70% of GDP and now it's quite manageable
[01:31:17.840 --> 01:31:25.340]   Government there should be smaller yet. I don't think the trend is that negative. It's more of a problem with regulation and the administrative state
[01:31:25.340 --> 01:31:29.680]   But we've shown an ability to create new sectors like big parts of tech
[01:31:29.680 --> 01:31:35.320]   They're not unregulated laws apply to them, but they're way less regulated and it's a kind of race
[01:31:35.320 --> 01:31:40.240]   That race doesn't look too bad to me at the moment like we could lose it
[01:31:40.240 --> 01:31:47.200]   But so far so good. So it's the critique should be taken seriously, but it's yet to be validated. Hmm
[01:31:47.680 --> 01:31:51.920]   How about the egalitarian critique from the left that you can't have?
[01:31:51.920 --> 01:31:59.200]   The inequality the market creates with the political and moral equality that the you know, humans deserve and demand they just say that
[01:31:59.200 --> 01:32:03.960]   US has high degree of income inequality. Yeah
[01:32:03.960 --> 01:32:08.200]   So does Brazil a much less well-functioning society?
[01:32:08.200 --> 01:32:14.320]   Brazil continues on average it will probably grow one or two percent. That's not a great record
[01:32:14.320 --> 01:32:16.600]   But Brazil has to go up in a puff of smoke
[01:32:16.600 --> 01:32:21.960]   I don't see it. Hmm and how about the Nietzschean critique and I in the
[01:32:21.960 --> 01:32:24.720]   The end of history Fukuyama says this is more powerful
[01:32:24.720 --> 01:32:28.380]   He this is the one he's more worried about than the more more so than the leftist critique and over time
[01:32:28.380 --> 01:32:33.960]   Basically what you end up with is the last man and you can't defend the civilization, you know the story. It's a lot of words
[01:32:33.960 --> 01:32:41.380]   Short the market I've asked Fukuyama this he's not this is a long time ago, but he wasn't short the market then
[01:32:42.400 --> 01:32:48.160]   Again, it's it's a real issue. It seems to me the problems of today for the most part are
[01:32:48.160 --> 01:32:51.440]   More manageable than the problems of any previous era
[01:32:51.440 --> 01:32:54.160]   We still you know might all go poof
[01:32:54.160 --> 01:32:58.400]   Return to medieval Balkan style existence in a millennia or whatever
[01:32:58.400 --> 01:33:05.320]   But it's a fight and we're totally in the fight and we have a lot of resources and talent. So like let's do it
[01:33:05.320 --> 01:33:11.920]   Okay, I don't see why that particular worry. It's a lot of words. Mm-hmm
[01:33:11.920 --> 01:33:17.360]   And I like to get very concrete like even if you're not short the market if that were the main relevant worry
[01:33:17.360 --> 01:33:21.920]   Where would that show up in asset prices as it got worse? It's a very concrete question
[01:33:21.920 --> 01:33:27.640]   I think it's very useful to ask and when people don't have a clear answer. I get worried. Where does your prediction that?
[01:33:27.640 --> 01:33:32.840]   Hundreds of years down the line will have the $50,000 nukes. Where does that show up in the asset prices?
[01:33:32.840 --> 01:33:37.200]   I think at some point VIX, you know an index of volatility will go up
[01:33:38.600 --> 01:33:43.080]   Probably not soon nuclear proliferation has not gone crazy, which is wonderful
[01:33:43.080 --> 01:33:47.920]   But I think at some point it's hard to imagine it not getting out of control
[01:33:47.920 --> 01:33:52.880]   Last I read VIX is surprisingly low and stable. That's right
[01:33:52.880 --> 01:33:56.440]   I think 24 is on on the path to be a pretty good year
[01:33:56.440 --> 01:34:01.840]   Yeah, or do you think the market is just wrong in terms of thinking about both geopolitical risk from you know, Israel or no
[01:34:01.840 --> 01:34:08.400]   I don't think the markets wrong at all. I think that war will converge. I'm not saying the humanitarian outcome is a good one
[01:34:08.480 --> 01:34:13.640]   But in terms of the global economy, I think markets are thinking rationally about it though
[01:34:13.640 --> 01:34:15.640]   The rational forecast of course is often wrong
[01:34:15.640 --> 01:34:21.680]   What's your sense on the scaling stuff when you look at the arguments in the terms of what's coming? How do you react to that?
[01:34:21.680 --> 01:34:27.680]   Well your piece on that was great. I don't feel I have the expertise to judge that as a technical matter
[01:34:27.680 --> 01:34:33.680]   It does seem to me intuitively it would be weird on the technical side if scaling just stopped working
[01:34:34.000 --> 01:34:41.520]   But on the knowledge side, I think people underestimate possible barriers and what I have in mind is quite a bit of reality
[01:34:41.520 --> 01:34:45.120]   the universe might in some very fundamental way simply not be legible and
[01:34:45.120 --> 01:34:50.840]   That there's no easy and fruitful way to just quote-unquote apply more intelligence to the problem
[01:34:50.840 --> 01:34:54.620]   Like oh you want to integrate general relativity and quantum mechanics?
[01:34:54.620 --> 01:35:00.520]   It may just be we've hit the frontier and there's not a final layer of oh, here's how it fits together
[01:35:00.760 --> 01:35:06.120]   so there's no way to train in AI or other thing to make it smarter to solve that and
[01:35:06.120 --> 01:35:11.880]   Maybe a lot of the world is like that and that to me people are not taking seriously enough
[01:35:11.880 --> 01:35:17.280]   So I'm not sure what the net returns will be to bigger and better and smarter AI. Mm-hmm
[01:35:17.280 --> 01:35:23.160]   That seems possible for you know, P versus NP type of reasons. It's just like harder to make for further discoveries
[01:35:23.160 --> 01:35:28.120]   But I feel like we have pretty good estimates in terms of like the declining
[01:35:29.160 --> 01:35:34.840]   researcher productivity because of low-hanging fruit being gone in this sort of sense of we're reaching the frontier and
[01:35:34.840 --> 01:35:40.200]   Whatever percent it is a year if you can just keep the AI population growing faster than that
[01:35:40.200 --> 01:35:41.440]   If you just want to be crude about it
[01:35:41.440 --> 01:35:46.800]   that seems enough to if not get to the ultimate physical synthesis at least much farther than
[01:35:46.800 --> 01:35:52.280]   Where human civilization would get in the same span of time that seems very plausible. I think we'll get further
[01:35:52.280 --> 01:35:59.840]   I expect big productivity gains as a side note. I'm less convinced by the declining researcher productivity argument than I used to be
[01:35:59.840 --> 01:36:06.560]   So the best way to measure productivity for an economist is wages and wages of researchers haven't gone down
[01:36:06.560 --> 01:36:11.800]   Period in fact, they've gone up now. They may not be producing new ideas
[01:36:11.800 --> 01:36:17.600]   You might be paying them to be functionaries or to manage PR or does this manage other researchers?
[01:36:19.280 --> 01:36:22.760]   But I think that's a worry that we have a lot more researchers
[01:36:22.760 --> 01:36:31.360]   With generally rising researcher wages and that hasn't boosted productivity growth China, India, South Korea brought into the world economy
[01:36:31.360 --> 01:36:34.120]   scientific talent
[01:36:34.120 --> 01:36:40.680]   It's better than if we hadn't done it, but it hasn't in absolute terms boosted productivity growth and maybe that's a worrisome sign
[01:36:40.680 --> 01:36:44.320]   on on the metric of researcher wages
[01:36:44.320 --> 01:36:52.080]   it seems like it could just be a fact that even the even though the less marginally useful improvements are worth the extra cost in
[01:36:52.080 --> 01:36:53.200]   terms of if you think of a company
[01:36:53.200 --> 01:36:57.000]   Like Google is probably paying as engineers a lot more than I was paying in the early days
[01:36:57.000 --> 01:36:59.240]   Even though they're doing less now because you know
[01:36:59.240 --> 01:37:03.680]   I have changing a pixel in the the new Google page is gonna affect billions of users
[01:37:03.680 --> 01:37:10.160]   The similar thing could be happening economy, right that might hold for Google researchers, but take people in pharma biomedicine
[01:37:10.160 --> 01:37:13.240]   There's a lot of private sector
[01:37:13.480 --> 01:37:20.240]   Financed research or indirectly financed by buying up smaller companies and it only makes sense if you get something out of it
[01:37:20.240 --> 01:37:25.080]   That really works like a good vaccine or good medication ozempic super profitable
[01:37:25.080 --> 01:37:29.520]   So wages for biomedical researchers in general haven't gone down
[01:37:29.520 --> 01:37:32.720]   Now finally, it's paying off
[01:37:32.720 --> 01:37:39.840]   But I'm not sure AI will be as revolutionary as the other AI optimists believe I do think it will raise
[01:37:40.440 --> 01:37:44.240]   Productivity growth in ways which are visible. Mm-hmm to what extent?
[01:37:44.240 --> 01:37:49.240]   In the conventional growth story you think in terms of population size, right?
[01:37:49.240 --> 01:37:51.520]   And then so you just increase the population size
[01:37:51.520 --> 01:37:55.480]   You get much more research at the other end to what extent does it make sense to think about well
[01:37:55.480 --> 01:38:00.640]   if you have these billions of AI copies that we can think of that in terms of as a proxy of how much progress they
[01:38:00.640 --> 01:38:07.440]   Could produce is that a not a sensible way to think about that at some point having billions of copies probably won't matter
[01:38:07.640 --> 01:38:15.320]   it will matter much more how good is the best thing we have and how well integrated is it into our other systems which
[01:38:15.320 --> 01:38:17.320]   have bottlenecks of their own and
[01:38:17.320 --> 01:38:18.600]   the
[01:38:18.600 --> 01:38:21.400]   Principles governing the growth of that are much harder to discern
[01:38:21.400 --> 01:38:24.680]   It's probably a much slower growth than just juicing up
[01:38:24.680 --> 01:38:27.880]   Oh, we've got a lot of these things and they're trained on more and more GPUs
[01:38:27.880 --> 01:38:34.280]   Mm-hmm, but precisely because the top seems to matter so much is why we might expect bigger gains, right?
[01:38:34.280 --> 01:38:37.800]   So if you think about Jews in the 20th century a you know
[01:38:37.800 --> 01:38:40.440]   2% of population or less than that and 20% of the Nobel Prizes
[01:38:40.440 --> 01:38:45.640]   it does seem like you can have a much bigger impact than if you're on the very tail if you just have
[01:38:45.640 --> 01:38:48.160]   Just a few hundred John von Neumann copies
[01:38:48.160 --> 01:38:54.160]   Maybe that's a good analogy that the impact of AI will be like in the 20th century the impact of Jews, right?
[01:38:54.160 --> 01:38:57.480]   Excellent, right? Yeah, but it's not
[01:38:57.480 --> 01:39:00.320]   Extraordinary. It's not a science fiction novel
[01:39:00.320 --> 01:39:03.840]   It is I mean if you read the early 20th century stuff as you have, you know
[01:39:03.840 --> 01:39:05.840]   It's like a slow takeoff right there of like, you know
[01:39:05.840 --> 01:39:11.640]   Go from b2 rockets to the moon and a couple of decades. It's okay. It's kind of crazy pace of change
[01:39:11.640 --> 01:39:15.180]   Yeah, that's what I think it will be like again great stagnation is over
[01:39:15.180 --> 01:39:18.800]   We'll go back to those earlier rates of change transform a lot of the world
[01:39:18.800 --> 01:39:21.760]   Mostly a big positive a lot of chaos
[01:39:21.760 --> 01:39:25.700]   Disrupted institutions along the way that's my prediction
[01:39:25.700 --> 01:39:31.400]   But no one writes a science fiction novel about the 20th century. It feels a bit ordinary still
[01:39:32.040 --> 01:39:38.440]   Mmm, yeah, even though it wasn't I forget the name of the philosopher you asked us to but the the feminist philosopher you asked the question
[01:39:38.440 --> 01:39:43.760]   I mean, yes. Yes, you asked the question. What would have to be different for you to be a social conservative, right?
[01:39:43.760 --> 01:39:48.200]   What would have to be different for you to not be a doer per se but just one of these people who like?
[01:39:48.200 --> 01:39:52.200]   This is the main thing to be thinking about during this period of history or something like that
[01:39:52.200 --> 01:39:54.720]   Well, I think it is one of the main things we should be thinking about
[01:39:54.720 --> 01:39:57.240]   But I would say if I thought
[01:39:57.640 --> 01:40:01.720]   International cooperation were very possible. I would at least
[01:40:01.720 --> 01:40:04.560]   Possibly have very different views than I do now
[01:40:04.560 --> 01:40:12.480]   Or if I thought no other country could make progress on AI those seem unlikely to me, but they're not logically impossible. So
[01:40:12.480 --> 01:40:21.200]   The fundamental premise where I differ from a lot of the doomers is my understanding of a decentralized world and its principles being primary
[01:40:21.200 --> 01:40:27.600]   Their understanding is some kind of comparison like here's the little people and here's the big monster and the big monster gets
[01:40:27.600 --> 01:40:32.520]   Bigger and even if the big monster does a lot of good things, it's just getting bigger and here the little people
[01:40:32.520 --> 01:40:39.560]   That's a possible framework. But if you start with decentralization and competition and well, how are we going to manage this?
[01:40:39.560 --> 01:40:44.440]   In some ways my perspective might be more pessimistic
[01:40:44.440 --> 01:40:50.200]   But you you don't just think you can wake up in the morning and like legislate safety
[01:40:52.840 --> 01:40:59.600]   Mm-hmm. You look at the history of relative safety having come from hegemons and you hope your hegemon stays good enough
[01:40:59.600 --> 01:41:05.320]   Which is a deeply fraught proposition. I recognize that. Mm-hmm. What's the next book?
[01:41:05.320 --> 01:41:12.560]   I'm already writing it part of it is on Jevons, but the title is the marginal revolution
[01:41:12.560 --> 01:41:18.960]   But not about the blog about the actual marginal. Yeah, but it's maybe a monograph like 40,000 words
[01:41:19.320 --> 01:41:22.680]   Mm-hmm, but I don't think book length should matter anymore. I
[01:41:22.680 --> 01:41:29.040]   Want to be more radical on that. I think 40,000 words is perfect because it actually fit in context. So when you do that you
[01:41:29.040 --> 01:41:39.320]   Know context may be bigger by then. Yeah, but I want to have it in GPT in some way. Okay, whatever is replaced it. Mm-hmm
[01:41:39.320 --> 01:41:45.280]   Okay. Those are all the questions. I had Tyler. This is a lot of fun and keep up the great work and
[01:41:46.200 --> 01:41:51.020]   Delighted you're at it. Thank you. Thank you. Yeah, thanks for coming on the podcast the third time now, so a lot of fun
[01:41:51.020 --> 01:41:53.400]   Okay. Bye everyone
[01:41:53.400 --> 01:41:57.760]   Hey everybody, I hope you enjoyed that episode as
[01:41:57.760 --> 01:42:01.760]   Always the most helpful thing you can do is to share the podcast
[01:42:01.760 --> 01:42:06.860]   Send it to people you think might enjoy it put it in Twitter your group chats, etc. Just splits the world
[01:42:06.860 --> 01:42:10.640]   Appreciate your listening. I'll see you next time Cheers
[01:42:10.640 --> 01:42:12.640]   (Music)
[01:42:12.640 --> 01:42:14.640]   (Music)
[01:42:14.640 --> 01:42:16.640]   (Music)
[01:42:16.640 --> 01:42:18.640]   (Music)
[01:42:18.640 --> 01:42:20.640]   (Music)

