
[00:00:00.000 --> 00:00:02.740]   The following is a conversation with Jan Lekun,
[00:00:02.740 --> 00:00:04.540]   his second time on the podcast.
[00:00:04.540 --> 00:00:09.180]   He is the chief AI scientist at Meta, formerly Facebook,
[00:00:09.180 --> 00:00:13.060]   professor at NYU, Turing Award winner,
[00:00:13.060 --> 00:00:15.620]   one of the seminal figures in the history
[00:00:15.620 --> 00:00:18.500]   of machine learning and artificial intelligence,
[00:00:18.500 --> 00:00:21.980]   and someone who is brilliant and opinionated
[00:00:21.980 --> 00:00:23.460]   in the best kind of way,
[00:00:23.460 --> 00:00:26.000]   and so is always fun to talk to.
[00:00:26.000 --> 00:00:28.000]   This is the Lex Friedman Podcast.
[00:00:28.000 --> 00:00:29.980]   To support it, please check out our sponsors
[00:00:29.980 --> 00:00:33.500]   in the description, and now, here's my conversation
[00:00:33.500 --> 00:00:35.040]   with Jan Lekun.
[00:00:35.040 --> 00:00:37.540]   You co-wrote the article,
[00:00:37.540 --> 00:00:40.900]   "Self-supervised learning, the dark matter of intelligence."
[00:00:40.900 --> 00:00:43.720]   Great title, by the way, with Ishan Misra.
[00:00:43.720 --> 00:00:46.640]   So let me ask, what is self-supervised learning,
[00:00:46.640 --> 00:00:49.940]   and why is it the dark matter of intelligence?
[00:00:49.940 --> 00:00:51.780]   - I'll start by the dark matter part.
[00:00:51.780 --> 00:00:55.700]   There is obviously a kind of learning
[00:00:55.700 --> 00:00:59.860]   that humans and animals are doing
[00:00:59.860 --> 00:01:02.820]   that we currently are not reproducing properly
[00:01:02.820 --> 00:01:04.660]   with machines or with AI, right?
[00:01:04.660 --> 00:01:07.460]   So the most popular approaches to machine learning today
[00:01:07.460 --> 00:01:09.660]   are, or paradigms, I should say,
[00:01:09.660 --> 00:01:12.700]   are supervised learning and reinforcement learning.
[00:01:12.700 --> 00:01:15.120]   And they are extremely inefficient.
[00:01:15.120 --> 00:01:17.620]   Supervised learning requires many samples
[00:01:17.620 --> 00:01:19.740]   for learning anything,
[00:01:19.740 --> 00:01:21.820]   and reinforcement learning requires
[00:01:21.820 --> 00:01:24.900]   a ridiculously large number of trial and errors
[00:01:24.900 --> 00:01:27.300]   for a system to learn anything.
[00:01:29.340 --> 00:01:32.060]   And that's why we don't have self-driving cars.
[00:01:32.060 --> 00:01:32.980]   (Lex laughing)
[00:01:32.980 --> 00:01:34.780]   - That was a big leap from one to the other.
[00:01:34.780 --> 00:01:38.780]   Okay, so that, to solve difficult problems,
[00:01:38.780 --> 00:01:42.340]   you have to have a lot of human annotation
[00:01:42.340 --> 00:01:44.080]   for supervised learning to work,
[00:01:44.080 --> 00:01:45.500]   and to solve those difficult problems
[00:01:45.500 --> 00:01:47.920]   with reinforcement learning, you have to have
[00:01:47.920 --> 00:01:50.220]   some way to maybe simulate that problem
[00:01:50.220 --> 00:01:52.700]   such that you can do that large-scale kind of learning
[00:01:52.700 --> 00:01:54.420]   that reinforcement learning requires.
[00:01:54.420 --> 00:01:57.180]   - Right, so how is it that, you know,
[00:01:57.180 --> 00:01:59.020]   most teenagers can learn to drive a car
[00:01:59.020 --> 00:02:02.300]   in about 20 hours of practice,
[00:02:02.300 --> 00:02:07.300]   whereas even with millions of hours of simulated practice,
[00:02:07.300 --> 00:02:09.220]   a self-driving car can't actually learn
[00:02:09.220 --> 00:02:10.700]   to drive itself properly.
[00:02:10.700 --> 00:02:13.900]   And so obviously we're missing something, right?
[00:02:13.900 --> 00:02:16.420]   And it's quite obvious for a lot of people that,
[00:02:16.420 --> 00:02:19.520]   you know, the immediate response you get from many people
[00:02:19.520 --> 00:02:22.840]   is, well, you know, humans use their background knowledge
[00:02:22.840 --> 00:02:25.820]   to learn faster, and they're right.
[00:02:25.820 --> 00:02:28.260]   Now, how was that background knowledge acquired?
[00:02:28.260 --> 00:02:30.040]   And that's the big question.
[00:02:30.040 --> 00:02:32.380]   So now you have to ask, you know,
[00:02:32.380 --> 00:02:35.100]   how do babies in their first few months of life
[00:02:35.100 --> 00:02:37.100]   learn how the world works?
[00:02:37.100 --> 00:02:38.220]   Mostly by observation,
[00:02:38.220 --> 00:02:40.220]   because they can hardly act in the world.
[00:02:40.220 --> 00:02:42.500]   And they learn an enormous amount
[00:02:42.500 --> 00:02:43.820]   of background knowledge about the world.
[00:02:43.820 --> 00:02:47.940]   That may be the basis of what we call common sense.
[00:02:47.940 --> 00:02:51.220]   This type of learning, it's not learning a task,
[00:02:51.220 --> 00:02:53.620]   it's not being reinforced for anything,
[00:02:53.620 --> 00:02:57.240]   it's just observing the world and figuring out how it works.
[00:02:57.240 --> 00:03:01.140]   Building world models, learning world models.
[00:03:01.140 --> 00:03:02.060]   How do we do this?
[00:03:02.060 --> 00:03:04.500]   And how do we reproduce this in machines?
[00:03:04.500 --> 00:03:07.620]   So self-supervised learning is, you know,
[00:03:07.620 --> 00:03:10.220]   one instance or one attempt
[00:03:10.220 --> 00:03:13.020]   at trying to reproduce this kind of learning.
[00:03:13.020 --> 00:03:16.300]   - Okay, so you're looking at just observation,
[00:03:16.300 --> 00:03:18.620]   so not even the interacting part of a child.
[00:03:18.620 --> 00:03:21.540]   It's just sitting there watching mom and dad walk around,
[00:03:21.540 --> 00:03:23.420]   pick up stuff, all of that.
[00:03:23.420 --> 00:03:25.500]   - That's what you mean by background knowledge.
[00:03:25.500 --> 00:03:27.500]   - Perhaps not even watching mom and dad,
[00:03:27.500 --> 00:03:29.980]   just, you know, watching the world go by.
[00:03:29.980 --> 00:03:31.900]   - Just having eyes open or having eyes closed,
[00:03:31.900 --> 00:03:34.460]   or the very act of opening and closing eyes
[00:03:34.460 --> 00:03:36.260]   that the world appears and disappears,
[00:03:36.260 --> 00:03:37.820]   all that basic information.
[00:03:37.820 --> 00:03:43.100]   And you're saying in order to learn to drive,
[00:03:43.100 --> 00:03:45.820]   like the reason humans are able to learn to drive quickly,
[00:03:45.820 --> 00:03:47.340]   some faster than others,
[00:03:47.340 --> 00:03:48.660]   is because of the background knowledge
[00:03:48.660 --> 00:03:51.740]   they were able to watch cars operate in the world
[00:03:51.740 --> 00:03:53.580]   in the many years leading up to it,
[00:03:53.580 --> 00:03:55.740]   the physics of basic objects, all that kind of stuff.
[00:03:55.740 --> 00:03:56.580]   - That's right.
[00:03:56.580 --> 00:03:57.420]   I mean, the basic physics of objects,
[00:03:57.420 --> 00:03:59.540]   you don't even know, you don't even need to know,
[00:03:59.540 --> 00:04:00.820]   you know, how a car works, right?
[00:04:00.820 --> 00:04:02.460]   Because that you can learn fairly quickly.
[00:04:02.460 --> 00:04:03.780]   I mean, the example I use very often
[00:04:03.780 --> 00:04:05.700]   is you're driving next to a cliff,
[00:04:05.700 --> 00:04:08.100]   and you know in advance,
[00:04:08.100 --> 00:04:11.820]   because of your understanding of intuitive physics,
[00:04:11.820 --> 00:04:13.740]   that if you turn the wheel to the right,
[00:04:13.740 --> 00:04:15.020]   the car will veer to the right,
[00:04:15.020 --> 00:04:17.580]   will run off the cliff, fall off the cliff,
[00:04:17.580 --> 00:04:20.420]   and nothing good will come out of this, right?
[00:04:20.420 --> 00:04:22.740]   But if you are a sort of, you know,
[00:04:22.740 --> 00:04:25.100]   tabula rasa reinforcement learning system
[00:04:25.100 --> 00:04:27.060]   that doesn't have a model of the world,
[00:04:27.060 --> 00:04:30.500]   you have to repeat falling off this cliff
[00:04:30.500 --> 00:04:32.780]   thousands of times before you figure out it's a bad idea.
[00:04:32.780 --> 00:04:34.580]   And then a few more thousand times
[00:04:34.580 --> 00:04:36.940]   before you figure out how to not do it.
[00:04:36.940 --> 00:04:38.460]   And then a few more million times
[00:04:38.460 --> 00:04:39.780]   before you figure out how to not do it
[00:04:39.780 --> 00:04:42.500]   in every situation you ever encounter.
[00:04:42.500 --> 00:04:45.820]   - So self-supervised learning still has to have
[00:04:45.820 --> 00:04:50.100]   some source of truth being told to it by somebody.
[00:04:50.100 --> 00:04:54.540]   - So you have to figure out a way without human assistance
[00:04:54.540 --> 00:04:56.580]   or without significant amount of human assistance
[00:04:56.580 --> 00:04:59.100]   to get that truth from the world.
[00:04:59.100 --> 00:05:03.980]   So the mystery there is how much signal is there,
[00:05:03.980 --> 00:05:06.260]   how much truth is there that the world gives you,
[00:05:06.260 --> 00:05:08.180]   whether it's the human world,
[00:05:08.180 --> 00:05:10.020]   like you watch YouTube or something like that,
[00:05:10.020 --> 00:05:12.980]   or it's the more natural world.
[00:05:12.980 --> 00:05:14.900]   So how much signal is there?
[00:05:14.900 --> 00:05:16.300]   - So here's the trick.
[00:05:16.300 --> 00:05:20.580]   There is way more signal in sort of a self-supervised setting
[00:05:20.580 --> 00:05:22.500]   than there is in either a supervised
[00:05:22.500 --> 00:05:24.540]   or reinforcement setting.
[00:05:24.540 --> 00:05:28.340]   And this is going to my analogy of the cake.
[00:05:28.340 --> 00:05:32.340]   Le cake as someone has called it,
[00:05:32.340 --> 00:05:36.020]   where when you try to figure out how much information
[00:05:36.020 --> 00:05:37.820]   you ask the machine to predict
[00:05:37.820 --> 00:05:40.980]   and how much feedback you give the machine at every trial,
[00:05:40.980 --> 00:05:41.820]   in reinforcement learning,
[00:05:41.820 --> 00:05:43.300]   you give the machine a single scaler,
[00:05:43.300 --> 00:05:45.340]   you tell the machine you did good, you did bad,
[00:05:45.340 --> 00:05:49.580]   and you only tell this to the machine once in a while.
[00:05:49.580 --> 00:05:51.380]   When I say you, it could be the universe
[00:05:51.380 --> 00:05:52.780]   telling the machine, right?
[00:05:52.780 --> 00:05:55.780]   But it's just one scaler.
[00:05:55.780 --> 00:05:57.100]   And so as a consequence,
[00:05:57.100 --> 00:05:59.540]   you cannot possibly learn something very complicated
[00:05:59.540 --> 00:06:01.060]   without many, many, many trials
[00:06:01.060 --> 00:06:04.700]   where you get many, many feedbacks of this type.
[00:06:04.700 --> 00:06:08.860]   Supervised learning, you give a few bits to the machine
[00:06:08.860 --> 00:06:10.180]   at every sample.
[00:06:10.180 --> 00:06:14.300]   Let's say you're training a system on, you know,
[00:06:14.300 --> 00:06:16.300]   recognizing images on ImageNet.
[00:06:16.300 --> 00:06:17.660]   There is 1000 categories,
[00:06:17.660 --> 00:06:20.900]   that's a little less than 10 bits of information per sample.
[00:06:20.900 --> 00:06:24.620]   But self-supervised learning here is a setting.
[00:06:24.620 --> 00:06:26.340]   Ideally, we don't know how to do this yet,
[00:06:26.340 --> 00:06:31.340]   but ideally you would show a machine a segment of video
[00:06:31.340 --> 00:06:34.140]   and then stop the video and ask the machine to predict
[00:06:34.140 --> 00:06:35.540]   what's going to happen next.
[00:06:35.540 --> 00:06:38.660]   And so you let the machine predict,
[00:06:38.660 --> 00:06:41.380]   and then you let time go by
[00:06:41.380 --> 00:06:44.260]   and show the machine what actually happened.
[00:06:44.260 --> 00:06:46.300]   And hope the machine will, you know,
[00:06:46.300 --> 00:06:49.340]   learn to do a better job at predicting next time around.
[00:06:49.340 --> 00:06:51.500]   There's a huge amount of information you give the machine
[00:06:51.500 --> 00:06:56.500]   because it's an entire video clip of, you know,
[00:06:56.500 --> 00:06:59.180]   the future after the video clip you fed it
[00:06:59.180 --> 00:07:00.220]   in the first place.
[00:07:00.220 --> 00:07:02.820]   - So both for language and for vision,
[00:07:02.820 --> 00:07:06.860]   there's a subtle, seemingly trivial construction,
[00:07:06.860 --> 00:07:08.460]   but maybe that's representative
[00:07:08.460 --> 00:07:10.580]   of what is required to create intelligence,
[00:07:10.580 --> 00:07:12.820]   which is filling the gap.
[00:07:13.700 --> 00:07:14.700]   - Filling the gaps.
[00:07:14.700 --> 00:07:17.780]   - Sounds dumb, but can you,
[00:07:17.780 --> 00:07:22.060]   it is possible you could solve all of intelligence
[00:07:22.060 --> 00:07:25.260]   in this way, just for both language,
[00:07:25.260 --> 00:07:28.780]   just give a sentence and continue it,
[00:07:28.780 --> 00:07:31.140]   or give a sentence and there's a gap in it,
[00:07:31.140 --> 00:07:33.500]   some words blanked out,
[00:07:33.500 --> 00:07:35.700]   and you fill in what words go there.
[00:07:35.700 --> 00:07:39.180]   For vision, you give a sequence of images
[00:07:39.180 --> 00:07:40.940]   and predict what's gonna happen next,
[00:07:40.940 --> 00:07:43.020]   or you fill in what happened in between.
[00:07:43.860 --> 00:07:47.020]   Do you think it's possible that formulation alone,
[00:07:47.020 --> 00:07:50.980]   as a signal for self-supervised learning,
[00:07:50.980 --> 00:07:53.620]   can solve intelligence for vision and language?
[00:07:53.620 --> 00:07:56.300]   - I think that's our best shot at the moment.
[00:07:56.300 --> 00:07:59.820]   So whether this will take us all the way to,
[00:07:59.820 --> 00:08:01.780]   you know, human level intelligence or something,
[00:08:01.780 --> 00:08:04.860]   or just cat level intelligence is not clear,
[00:08:04.860 --> 00:08:07.340]   but among all the possible approaches
[00:08:07.340 --> 00:08:09.500]   that people have proposed, I think it's our best shot.
[00:08:09.500 --> 00:08:14.500]   So I think this idea of an intelligence system
[00:08:14.500 --> 00:08:18.860]   filling in the blanks, either predicting the future,
[00:08:18.860 --> 00:08:22.180]   inferring the past, filling in missing information,
[00:08:22.180 --> 00:08:26.660]   I'm currently filling the blank of what is behind your head
[00:08:26.660 --> 00:08:30.580]   and what your head looks like from the back,
[00:08:30.580 --> 00:08:33.740]   because I have basic knowledge about how humans are made.
[00:08:33.740 --> 00:08:35.660]   And I don't know if you're gonna,
[00:08:35.660 --> 00:08:37.260]   what are you gonna say, at which point you're gonna speak,
[00:08:37.260 --> 00:08:38.980]   whether you're gonna move your head this way or that way,
[00:08:38.980 --> 00:08:40.260]   which way you're gonna look.
[00:08:40.260 --> 00:08:42.100]   But I know you're not gonna just dematerialize
[00:08:42.100 --> 00:08:44.940]   and reappear three meters down the hall,
[00:08:44.940 --> 00:08:49.540]   because I know what's possible and what's impossible,
[00:08:49.540 --> 00:08:50.900]   according to intuitive physics.
[00:08:50.900 --> 00:08:53.260]   - So you have a model of what's possible and what's impossible
[00:08:53.260 --> 00:08:55.100]   and then you'd be very surprised if it happens,
[00:08:55.100 --> 00:08:57.860]   and then you'll have to reconstruct your model.
[00:08:57.860 --> 00:08:59.620]   - Right, so that's the model of the world.
[00:08:59.620 --> 00:09:02.260]   It's what tells you what fills in the blanks.
[00:09:02.260 --> 00:09:04.460]   So given your partial information
[00:09:04.460 --> 00:09:07.220]   about the state of the world, given by your perception,
[00:09:08.060 --> 00:09:11.340]   your model of the world fills in the missing information.
[00:09:11.340 --> 00:09:13.740]   And that includes predicting the future,
[00:09:13.740 --> 00:09:15.220]   retrodicting the past,
[00:09:15.220 --> 00:09:18.380]   filling in things you don't immediately perceive.
[00:09:18.380 --> 00:09:22.260]   - And that doesn't have to be purely generic vision
[00:09:22.260 --> 00:09:24.300]   or visual information or generic language.
[00:09:24.300 --> 00:09:25.820]   You can go to specifics,
[00:09:25.820 --> 00:09:30.260]   like predicting what control decision you make
[00:09:30.260 --> 00:09:31.580]   when you're driving in a lane.
[00:09:31.580 --> 00:09:35.580]   You have a sequence of images from a vehicle,
[00:09:35.580 --> 00:09:38.380]   and then you have information,
[00:09:38.380 --> 00:09:41.780]   if you record it on video, where the car ended up going.
[00:09:41.780 --> 00:09:45.500]   So you can go back in time and predict where the car went
[00:09:45.500 --> 00:09:46.660]   based on the visual information.
[00:09:46.660 --> 00:09:49.420]   That's very specific, domain-specific.
[00:09:49.420 --> 00:09:51.460]   - Right, but the question is whether we can come up
[00:09:51.460 --> 00:09:56.460]   with sort of a generic method for training machines
[00:09:56.460 --> 00:09:59.820]   to do this kind of prediction or filling in the blanks.
[00:09:59.820 --> 00:10:03.220]   So right now, this type of approach
[00:10:03.220 --> 00:10:05.540]   has been unbelievably successful
[00:10:05.540 --> 00:10:08.140]   in the context of natural language processing.
[00:10:08.140 --> 00:10:09.660]   Every model in natural language processing
[00:10:09.660 --> 00:10:12.220]   is pre-trained in self-supervised manner
[00:10:12.220 --> 00:10:13.660]   to fill in the blanks.
[00:10:13.660 --> 00:10:16.380]   You show it a sequence of words, you remove 10% of them,
[00:10:16.380 --> 00:10:17.900]   and then you train some gigantic neural net
[00:10:17.900 --> 00:10:20.260]   to predict the words that are missing.
[00:10:20.260 --> 00:10:22.660]   And once you've pre-trained that network,
[00:10:22.660 --> 00:10:26.540]   you can use the internal representation learned by it
[00:10:26.540 --> 00:10:30.380]   as input to something that you trained, supervised,
[00:10:30.380 --> 00:10:32.140]   or whatever.
[00:10:32.140 --> 00:10:33.300]   That's been incredibly successful.
[00:10:33.300 --> 00:10:37.500]   Not so successful in images, although it's making progress.
[00:10:37.500 --> 00:10:42.500]   And it's based on sort of manual data augmentation.
[00:10:42.500 --> 00:10:43.460]   We can go into this later.
[00:10:43.460 --> 00:10:47.140]   But what has not been successful yet is training from video.
[00:10:47.140 --> 00:10:50.180]   So getting a machine to learn to represent the visual world,
[00:10:50.180 --> 00:10:52.700]   for example, by just watching video.
[00:10:52.700 --> 00:10:54.740]   Nobody has really succeeded in doing this.
[00:10:54.740 --> 00:10:57.460]   - Okay, well, let's kind of give a high-level overview.
[00:10:57.460 --> 00:11:02.340]   What's the difference in kind and in difficulty
[00:11:02.340 --> 00:11:03.900]   between vision and language?
[00:11:03.900 --> 00:11:08.820]   So you said people haven't been able to really kind of crack
[00:11:08.820 --> 00:11:10.420]   the problem of vision open
[00:11:10.420 --> 00:11:11.900]   in terms of self-supervised learning,
[00:11:11.900 --> 00:11:13.740]   but that may not be necessarily
[00:11:13.740 --> 00:11:15.820]   because it's fundamentally more difficult.
[00:11:15.820 --> 00:11:18.660]   Maybe like when we're talking about achieving,
[00:11:18.660 --> 00:11:22.260]   like passing the Turing test in the full spirit
[00:11:22.260 --> 00:11:24.860]   of the Turing test in language might be harder than vision.
[00:11:24.860 --> 00:11:26.380]   That's not obvious.
[00:11:26.380 --> 00:11:29.380]   So in your view, which is harder,
[00:11:29.380 --> 00:11:31.660]   or perhaps are they just the same problem?
[00:11:31.660 --> 00:11:34.820]   The farther we get to solving each,
[00:11:34.820 --> 00:11:36.700]   the more we realize it's all the same thing.
[00:11:36.700 --> 00:11:37.620]   It's all the same cake.
[00:11:37.620 --> 00:11:40.180]   - I think what I'm looking for are methods
[00:11:40.180 --> 00:11:43.580]   that make them look essentially like the same cake,
[00:11:43.580 --> 00:11:44.740]   but currently they're not.
[00:11:44.740 --> 00:11:48.460]   And the main issue with learning world models
[00:11:48.460 --> 00:11:50.860]   or learning predictive models is that
[00:11:50.860 --> 00:11:55.860]   the prediction is never a single thing,
[00:11:55.860 --> 00:11:59.220]   because the world is not entirely predictable.
[00:11:59.220 --> 00:12:00.700]   It may be deterministic or stochastic.
[00:12:00.700 --> 00:12:02.940]   We can get into the philosophical discussion about it,
[00:12:02.940 --> 00:12:05.260]   but even if it's deterministic,
[00:12:05.260 --> 00:12:07.420]   it's not entirely predictable.
[00:12:07.420 --> 00:12:11.740]   And so if I play a short video clip
[00:12:11.740 --> 00:12:14.140]   and then I ask you to predict what's going to happen next,
[00:12:14.140 --> 00:12:16.340]   there's many, many plausible continuations
[00:12:16.340 --> 00:12:18.300]   for that video clip.
[00:12:18.300 --> 00:12:20.540]   And the number of continuation grows
[00:12:20.540 --> 00:12:23.900]   with the interval of time that you're asking the system
[00:12:23.900 --> 00:12:25.700]   to make a prediction for.
[00:12:26.460 --> 00:12:29.860]   And so one big question with self-supervised learning
[00:12:29.860 --> 00:12:32.300]   is how you represent this uncertainty,
[00:12:32.300 --> 00:12:35.180]   how you represent multiple discrete outcomes,
[00:12:35.180 --> 00:12:37.060]   how you represent a sort of continuum
[00:12:37.060 --> 00:12:40.380]   of possible outcomes, et cetera.
[00:12:40.380 --> 00:12:45.180]   And if you are a sort of a classical machine learning person,
[00:12:45.180 --> 00:12:47.580]   you say, "Oh, you just represent a distribution."
[00:12:47.580 --> 00:12:52.540]   And that we know how to do when we're predicting words,
[00:12:52.540 --> 00:12:53.660]   missing words in the text,
[00:12:53.660 --> 00:12:56.820]   because you can have a neural net give a score
[00:12:56.820 --> 00:12:58.580]   for every word in a dictionary.
[00:12:58.580 --> 00:13:02.420]   It's a big list of numbers, maybe 100,000 or so.
[00:13:02.420 --> 00:13:05.220]   And you can turn them into a probability distribution
[00:13:05.220 --> 00:13:07.620]   that tells you when I say a sentence,
[00:13:07.620 --> 00:13:12.300]   the cat is chasing the blank in the kitchen.
[00:13:12.300 --> 00:13:15.820]   There are only a few words that make sense there.
[00:13:15.820 --> 00:13:18.340]   It could be a mouse or it could be a lizard spot
[00:13:18.340 --> 00:13:19.540]   or something like that.
[00:13:21.540 --> 00:13:25.820]   And if I say the blank is chasing the blank in the savanna,
[00:13:25.820 --> 00:13:27.820]   you also have a bunch of plausible options
[00:13:27.820 --> 00:13:29.180]   for those two words, right?
[00:13:29.180 --> 00:13:33.620]   Because you have kind of underlying reality
[00:13:33.620 --> 00:13:36.260]   that you can refer to to sort of fill in those blanks.
[00:13:36.260 --> 00:13:42.020]   So you cannot say for sure in the savanna
[00:13:42.020 --> 00:13:44.460]   if it's a lion or a cheetah or whatever,
[00:13:44.460 --> 00:13:49.460]   you cannot know if it's a zebra or a gnu or whatever,
[00:13:49.460 --> 00:13:50.860]   wildebeest, the same thing.
[00:13:50.860 --> 00:13:51.700]   - Yeah.
[00:13:51.700 --> 00:13:56.820]   - But you can represent the uncertainty
[00:13:56.820 --> 00:13:58.460]   by just a long list of numbers.
[00:13:58.460 --> 00:14:01.780]   Now, if I do the same thing with video
[00:14:01.780 --> 00:14:04.300]   and I ask you to predict a video clip,
[00:14:04.300 --> 00:14:07.380]   it's not a discrete set of potential frames.
[00:14:07.380 --> 00:14:09.980]   You have to have some way of representing
[00:14:09.980 --> 00:14:13.540]   a sort of infinite number of plausible continuations
[00:14:13.540 --> 00:14:17.460]   of multiple frames in a high dimensional continuous space.
[00:14:17.460 --> 00:14:20.540]   And we just have no idea how to do this properly.
[00:14:20.540 --> 00:14:22.860]   - Finite high dimensional.
[00:14:22.860 --> 00:14:23.700]   So like you--
[00:14:23.700 --> 00:14:25.300]   - It's finite high dimensional, yes.
[00:14:25.300 --> 00:14:26.220]   - Just like the words,
[00:14:26.220 --> 00:14:31.220]   they try to get it down to a small finite set
[00:14:31.220 --> 00:14:34.220]   of like under a million, something like that.
[00:14:34.220 --> 00:14:35.060]   - Something like that.
[00:14:35.060 --> 00:14:36.020]   - I mean, it's kind of ridiculous
[00:14:36.020 --> 00:14:39.020]   that we're doing a distribution
[00:14:39.020 --> 00:14:42.900]   of every single possible word for language and it works.
[00:14:42.900 --> 00:14:45.300]   It feels like that's a really dumb way to do it.
[00:14:45.300 --> 00:14:49.720]   Like there seems to be like there should be
[00:14:49.720 --> 00:14:52.900]   some more compressed representation
[00:14:52.900 --> 00:14:55.020]   of the distribution of the words.
[00:14:55.020 --> 00:14:56.140]   - You're right about that.
[00:14:56.140 --> 00:14:56.980]   - And so--
[00:14:56.980 --> 00:14:57.800]   - I agree.
[00:14:57.800 --> 00:14:58.900]   - Do you have any interesting ideas
[00:14:58.900 --> 00:15:01.860]   about how to represent all of reality in a compressed way
[00:15:01.860 --> 00:15:03.780]   such that you can form a distribution over it?
[00:15:03.780 --> 00:15:06.180]   - That's one of the big questions, how do you do that?
[00:15:06.180 --> 00:15:07.980]   But I mean, what's kind of,
[00:15:07.980 --> 00:15:12.140]   another thing that really is stupid about,
[00:15:12.140 --> 00:15:13.060]   I shouldn't say stupid,
[00:15:13.060 --> 00:15:15.540]   but like simplistic about current approaches
[00:15:15.540 --> 00:15:19.340]   to self-supervised learning in NLP and text
[00:15:19.340 --> 00:15:21.880]   is that not only do you represent
[00:15:21.880 --> 00:15:23.780]   a giant distribution of words,
[00:15:23.780 --> 00:15:25.640]   but for multiple words that are missing,
[00:15:25.640 --> 00:15:27.660]   those distributions are essentially independent
[00:15:27.660 --> 00:15:28.500]   of each other.
[00:15:28.500 --> 00:15:33.020]   And you don't pay too much of a price for this.
[00:15:33.020 --> 00:15:36.720]   So you can't, so the system,
[00:15:36.720 --> 00:15:38.900]   in the sentence that I gave earlier,
[00:15:38.900 --> 00:15:43.620]   if it gives a certain probability for a lion and a cheetah,
[00:15:43.620 --> 00:15:48.420]   and then a certain probability for gazelle, wildebeest,
[00:15:48.420 --> 00:15:52.980]   and zebra, those two probabilities
[00:15:52.980 --> 00:15:54.780]   are independent of each other.
[00:15:54.780 --> 00:15:58.020]   And it's not the case that those things are independent.
[00:15:58.020 --> 00:16:01.440]   Lions actually attack like bigger animals than cheetahs.
[00:16:01.440 --> 00:16:05.940]   So, there's a huge independence hypothesis in this process,
[00:16:05.940 --> 00:16:07.780]   which is not actually true.
[00:16:07.780 --> 00:16:10.860]   The reason for this is that we don't know how to represent
[00:16:10.860 --> 00:16:15.580]   properly distributions over combinatorial sequences
[00:16:15.580 --> 00:16:17.340]   of symbols, essentially,
[00:16:17.340 --> 00:16:18.980]   because the number grows exponentially
[00:16:18.980 --> 00:16:21.300]   with the length of the symbols.
[00:16:21.300 --> 00:16:22.740]   And so we have to use tricks for this,
[00:16:22.740 --> 00:16:26.380]   but those techniques can get around,
[00:16:26.380 --> 00:16:27.780]   like don't even deal with it.
[00:16:27.780 --> 00:16:30.420]   So the big question is,
[00:16:30.420 --> 00:16:33.380]   would there be some sort of abstract
[00:16:33.380 --> 00:16:37.420]   latent representation of text that would say that,
[00:16:37.420 --> 00:16:40.660]   when I switch lion for gazelle,
[00:16:40.660 --> 00:16:45.480]   lion for cheetah, I also have to switch zebra for gazelle.
[00:16:45.480 --> 00:16:48.720]   - Yeah, so this independence assumption,
[00:16:48.720 --> 00:16:51.140]   let me throw some criticism at you that I often hear
[00:16:51.140 --> 00:16:52.940]   and see how you respond.
[00:16:52.940 --> 00:16:56.020]   So this kind of filling in the blanks is just statistics.
[00:16:56.020 --> 00:16:58.000]   You're not learning anything,
[00:16:58.000 --> 00:17:01.580]   like the deep underlying concepts.
[00:17:01.580 --> 00:17:05.660]   You're just mimicking stuff from the past.
[00:17:05.660 --> 00:17:07.540]   You're not learning anything new
[00:17:07.540 --> 00:17:10.800]   such that you can use it to generalize about the world.
[00:17:10.800 --> 00:17:14.100]   Or, okay, let me just say the crude version,
[00:17:14.100 --> 00:17:16.200]   which is it's just statistics.
[00:17:16.200 --> 00:17:17.860]   It's not intelligence.
[00:17:17.860 --> 00:17:19.620]   What do you have to say to that?
[00:17:19.620 --> 00:17:20.880]   What do you usually say to that
[00:17:20.880 --> 00:17:22.640]   if you kind of hear this kind of thing?
[00:17:22.640 --> 00:17:23.940]   - I don't get into those discussions
[00:17:23.940 --> 00:17:26.740]   because they're kind of pointless.
[00:17:26.740 --> 00:17:28.740]   So first of all, it's quite possible
[00:17:28.740 --> 00:17:30.460]   that intelligence is just statistics.
[00:17:30.460 --> 00:17:32.540]   It's just statistics of a particular kind.
[00:17:32.540 --> 00:17:35.580]   - But this is the philosophical question.
[00:17:35.580 --> 00:17:40.260]   Is it possible that intelligence is just statistics?
[00:17:40.260 --> 00:17:41.580]   - Yeah.
[00:17:41.580 --> 00:17:43.500]   But what kind of statistics?
[00:17:43.500 --> 00:17:46.180]   So if you are asking the question,
[00:17:46.180 --> 00:17:50.620]   are the models of the world that we learn,
[00:17:50.620 --> 00:17:52.300]   do they have some notion of causality?
[00:17:52.300 --> 00:17:53.380]   Yes.
[00:17:53.380 --> 00:17:56.220]   So if the criticism comes from people who say,
[00:17:56.220 --> 00:17:59.420]   current machine learning system don't care about causality,
[00:17:59.420 --> 00:18:03.100]   which by the way is wrong, I agree with them.
[00:18:03.100 --> 00:18:06.560]   Your model of the world should have your actions
[00:18:06.560 --> 00:18:09.100]   as one of the inputs,
[00:18:09.100 --> 00:18:11.420]   and that will drive you to learn causal models of the world
[00:18:11.420 --> 00:18:15.060]   where you know what intervention in the world
[00:18:15.060 --> 00:18:16.700]   will cause what result,
[00:18:16.700 --> 00:18:19.420]   or you can do this by observation of other agents
[00:18:19.420 --> 00:18:21.920]   acting in the world and observing the effect,
[00:18:21.920 --> 00:18:24.220]   other humans, for example.
[00:18:24.220 --> 00:18:28.380]   So I think at some level of description,
[00:18:28.380 --> 00:18:30.200]   intelligence is just statistics,
[00:18:30.200 --> 00:18:35.180]   but that doesn't mean you won't have models
[00:18:35.180 --> 00:18:40.060]   that have deep mechanistic explanation for what goes on.
[00:18:40.060 --> 00:18:41.740]   The question is, how do you learn them?
[00:18:41.740 --> 00:18:44.420]   That's the question I'm interested in.
[00:18:44.420 --> 00:18:49.340]   Because a lot of people who actually voice their criticism
[00:18:49.340 --> 00:18:50.980]   say that those mechanistic model
[00:18:50.980 --> 00:18:52.660]   have to come from someplace else.
[00:18:52.660 --> 00:18:54.060]   They have to come from human designers,
[00:18:54.060 --> 00:18:56.180]   they have to come from I don't know what.
[00:18:56.180 --> 00:18:57.880]   And obviously we learn them.
[00:18:57.880 --> 00:19:01.800]   Or if we don't learn them as an individual,
[00:19:01.800 --> 00:19:04.920]   nature learn them for us using evolution.
[00:19:04.920 --> 00:19:07.180]   So regardless of what you think,
[00:19:07.180 --> 00:19:09.420]   those processes have been learned somehow.
[00:19:10.260 --> 00:19:12.940]   - So if you look at the human brain,
[00:19:12.940 --> 00:19:14.660]   just like when we humans introspect
[00:19:14.660 --> 00:19:16.340]   about how the brain works,
[00:19:16.340 --> 00:19:20.260]   it seems like when we think about what is intelligence,
[00:19:20.260 --> 00:19:22.460]   we think about the high level stuff,
[00:19:22.460 --> 00:19:23.960]   like the models we've constructed,
[00:19:23.960 --> 00:19:25.580]   concepts like cognitive science,
[00:19:25.580 --> 00:19:28.700]   like concepts of memory and reasoning module,
[00:19:28.700 --> 00:19:31.660]   almost like these high level modules.
[00:19:31.660 --> 00:19:34.400]   Is this serve as a good analogy?
[00:19:35.380 --> 00:19:40.380]   Like are we ignoring the dark matter,
[00:19:40.380 --> 00:19:43.580]   the basic low level mechanisms,
[00:19:43.580 --> 00:19:45.820]   just like we ignore the way the operating system works,
[00:19:45.820 --> 00:19:49.660]   we're just using the high level software.
[00:19:49.660 --> 00:19:52.740]   We're ignoring that at the low level,
[00:19:52.740 --> 00:19:56.460]   the neural network might be doing something like statistics.
[00:19:56.460 --> 00:19:59.140]   Like me, sorry to use this word
[00:19:59.140 --> 00:20:00.580]   probably incorrectly and crudely,
[00:20:00.580 --> 00:20:03.340]   but doing this kind of fill in the gap kind of learning
[00:20:03.340 --> 00:20:05.740]   and just kind of updating the model constantly
[00:20:05.740 --> 00:20:09.260]   in order to be able to support the raw sensory information,
[00:20:09.260 --> 00:20:11.380]   to predict it and then adjust to the prediction
[00:20:11.380 --> 00:20:12.420]   when it's wrong.
[00:20:12.420 --> 00:20:15.860]   But like when we look at our brain at the high level,
[00:20:15.860 --> 00:20:18.340]   it feels like we're playing chess,
[00:20:18.340 --> 00:20:22.260]   like we're playing with high level concepts
[00:20:22.260 --> 00:20:23.700]   and we're stitching them together
[00:20:23.700 --> 00:20:26.040]   and we're putting them into long-term memory.
[00:20:26.040 --> 00:20:28.300]   But really what's going underneath
[00:20:28.300 --> 00:20:30.200]   is something we're not able to introspect,
[00:20:30.200 --> 00:20:34.460]   which is this kind of simple, large neural network
[00:20:34.460 --> 00:20:36.020]   that's just filling in the gaps.
[00:20:36.020 --> 00:20:38.260]   - Right, well, okay, so there's a lot of questions
[00:20:38.260 --> 00:20:39.780]   and a lot of answers there.
[00:20:39.780 --> 00:20:40.620]   Okay, so first of all,
[00:20:40.620 --> 00:20:42.700]   there's a whole school of thought in neuroscience,
[00:20:42.700 --> 00:20:45.260]   computational neuroscience in particular,
[00:20:45.260 --> 00:20:47.800]   that likes the idea of predictive coding,
[00:20:47.800 --> 00:20:50.820]   which is really related to the idea I was talking about
[00:20:50.820 --> 00:20:52.060]   in self-supervised learning.
[00:20:52.060 --> 00:20:53.580]   So everything is about prediction,
[00:20:53.580 --> 00:20:56.360]   the essence of intelligence is the ability to predict
[00:20:56.360 --> 00:20:58.860]   and everything the brain does is trying to predict
[00:20:59.940 --> 00:21:02.140]   everything from everything else.
[00:21:02.140 --> 00:21:04.780]   Okay, and that's really sort of the underlying principle,
[00:21:04.780 --> 00:21:07.820]   if you want, that self-supervised learning
[00:21:07.820 --> 00:21:10.660]   is trying to kind of reproduce this idea of prediction
[00:21:10.660 --> 00:21:13.060]   as kind of an essential mechanism
[00:21:13.060 --> 00:21:16.320]   of task-independent learning, if you want.
[00:21:16.320 --> 00:21:19.340]   The next step is what kind of intelligence
[00:21:19.340 --> 00:21:21.140]   are you interested in reproducing?
[00:21:21.140 --> 00:21:25.300]   And of course, we all think about trying to reproduce
[00:21:25.300 --> 00:21:28.340]   high-level cognitive processes in humans,
[00:21:28.340 --> 00:21:30.420]   but with machines, we're not even at the level
[00:21:30.420 --> 00:21:35.420]   of even reproducing the learning processes in a cat brain.
[00:21:35.420 --> 00:21:39.020]   The most intelligent or intelligent systems
[00:21:39.020 --> 00:21:41.960]   don't have as much common sense as a house cat.
[00:21:41.960 --> 00:21:45.180]   So how is it that cats learn?
[00:21:45.180 --> 00:21:47.900]   And cats don't do a whole lot of reasoning.
[00:21:47.900 --> 00:21:49.580]   They certainly have causal models.
[00:21:49.580 --> 00:21:53.620]   They certainly have, because many cats can figure out
[00:21:53.620 --> 00:21:56.580]   how they can act on the world to get what they want.
[00:21:56.660 --> 00:22:01.660]   They certainly have a fantastic model of intuitive physics,
[00:22:01.660 --> 00:22:04.620]   certainly the dynamics of their own bodies,
[00:22:04.620 --> 00:22:06.940]   but also of praise and things like that, right?
[00:22:06.940 --> 00:22:09.940]   So they're pretty smart.
[00:22:09.940 --> 00:22:12.460]   They only do this with about 800 million neurons.
[00:22:12.460 --> 00:22:15.980]   We are not anywhere close
[00:22:15.980 --> 00:22:17.980]   to reproducing this kind of thing.
[00:22:17.980 --> 00:22:21.340]   So to some extent, I could say,
[00:22:21.340 --> 00:22:24.980]   let's not even worry about the high-level cognition
[00:22:26.340 --> 00:22:28.660]   and long-term planning and reasoning that humans can do
[00:22:28.660 --> 00:22:30.100]   until we figure out,
[00:22:30.100 --> 00:22:32.500]   can we even reproduce what cats are doing?
[00:22:32.500 --> 00:22:37.000]   Now, that said, this ability to learn world models,
[00:22:37.000 --> 00:22:40.140]   I think is the key to the possibility
[00:22:40.140 --> 00:22:43.160]   of learning machines that can also reason.
[00:22:43.160 --> 00:22:44.340]   So whenever I give a talk, I say,
[00:22:44.340 --> 00:22:47.300]   there are three main challenges in machine learning.
[00:22:47.300 --> 00:22:49.940]   The first one is getting machines to learn
[00:22:49.940 --> 00:22:51.820]   to represent the world,
[00:22:51.820 --> 00:22:53.920]   and I'm proposing self-supervised learning.
[00:22:54.840 --> 00:22:58.000]   The second is getting machines to reason
[00:22:58.000 --> 00:22:59.240]   in ways that are compatible
[00:22:59.240 --> 00:23:01.600]   with essentially gradient-based learning,
[00:23:01.600 --> 00:23:04.280]   because this is what deep learning is all about, really.
[00:23:04.280 --> 00:23:07.640]   And the third one is something we have no idea how to solve,
[00:23:07.640 --> 00:23:09.480]   at least I have no idea how to solve,
[00:23:09.480 --> 00:23:14.360]   is can we get machines to learn hierarchical representations
[00:23:14.360 --> 00:23:15.980]   of action plans?
[00:23:15.980 --> 00:23:18.780]   We know how to train them
[00:23:18.780 --> 00:23:21.280]   to learn hierarchical representations of perception,
[00:23:22.240 --> 00:23:23.680]   with convolutional nets and things like that,
[00:23:23.680 --> 00:23:26.080]   and transformers, but what about action plans?
[00:23:26.080 --> 00:23:28.320]   Can we get them to spontaneously learn
[00:23:28.320 --> 00:23:30.560]   good hierarchical representations of actions?
[00:23:30.560 --> 00:23:32.440]   - Also gradient-based.
[00:23:32.440 --> 00:23:35.920]   - Yeah, all of that needs to be somewhat differentiable
[00:23:35.920 --> 00:23:38.760]   so that you can apply sort of gradient-based learning,
[00:23:38.760 --> 00:23:40.920]   which is really what deep learning is about.
[00:23:40.920 --> 00:23:46.760]   - So it's background, knowledge, ability to reason
[00:23:46.760 --> 00:23:50.520]   in a way that's differentiable,
[00:23:50.520 --> 00:23:53.840]   that is somehow connected, deeply integrated
[00:23:53.840 --> 00:23:55.480]   with that background knowledge,
[00:23:55.480 --> 00:23:57.640]   or builds on top of that background knowledge,
[00:23:57.640 --> 00:23:59.120]   and then given that background knowledge,
[00:23:59.120 --> 00:24:02.400]   be able to make hierarchical plans in the world.
[00:24:02.400 --> 00:24:05.480]   - So if you take classical optimal control,
[00:24:05.480 --> 00:24:07.000]   there's something in classical optimal control
[00:24:07.000 --> 00:24:10.520]   called model predictive control,
[00:24:10.520 --> 00:24:13.840]   and it's been around since the early '60s.
[00:24:13.840 --> 00:24:16.840]   NASA uses that to compute trajectories of rockets.
[00:24:16.840 --> 00:24:20.600]   And the basic idea is that you have a predictive model
[00:24:20.600 --> 00:24:21.840]   of the rocket, let's say,
[00:24:21.840 --> 00:24:25.440]   or whatever system you intend to control,
[00:24:25.440 --> 00:24:28.360]   which, given the state of the system at time t,
[00:24:28.360 --> 00:24:31.640]   and given an action that you're taking on the system,
[00:24:31.640 --> 00:24:33.520]   so for a rocket to be thrust,
[00:24:33.520 --> 00:24:35.600]   and all the controls you can have,
[00:24:35.600 --> 00:24:37.960]   it gives you the state of the system
[00:24:37.960 --> 00:24:39.560]   at time t plus delta t, right?
[00:24:39.560 --> 00:24:42.360]   So basically a differential equation, something like that.
[00:24:44.240 --> 00:24:45.920]   And if you have this model,
[00:24:45.920 --> 00:24:47.840]   and you have this model in the form
[00:24:47.840 --> 00:24:49.360]   of some sort of neural net,
[00:24:49.360 --> 00:24:51.600]   or some sort of set of formula
[00:24:51.600 --> 00:24:53.600]   that you can back propagate gradient through,
[00:24:53.600 --> 00:24:55.880]   you can do what's called model predictive control,
[00:24:55.880 --> 00:24:58.280]   or gradient-based model predictive control.
[00:24:58.280 --> 00:25:03.280]   So you have, you can unroll that model in time,
[00:25:03.280 --> 00:25:09.560]   you feed it a hypothesized sequence of actions,
[00:25:09.560 --> 00:25:13.560]   and then you have some objective function
[00:25:13.560 --> 00:25:16.040]   that measures how well, at the end of the trajectory,
[00:25:16.040 --> 00:25:18.880]   the system has succeeded or matched what you want it to do.
[00:25:18.880 --> 00:25:21.200]   Is it a robot harm?
[00:25:21.200 --> 00:25:23.400]   Have you grasped the object you want to grasp?
[00:25:23.400 --> 00:25:26.320]   If it's a rocket, are you at the right place
[00:25:26.320 --> 00:25:27.600]   near the space station?
[00:25:27.600 --> 00:25:29.080]   Things like that.
[00:25:29.080 --> 00:25:30.960]   And by back propagation through time,
[00:25:30.960 --> 00:25:32.800]   and again, this was invented in the 1960s
[00:25:32.800 --> 00:25:35.120]   by optimal control theorists,
[00:25:35.120 --> 00:25:39.080]   you can figure out what is the optimal sequence of actions
[00:25:39.080 --> 00:25:44.040]   that will get my system to the best final state.
[00:25:44.040 --> 00:25:47.560]   So that's a form of reasoning.
[00:25:47.560 --> 00:25:48.640]   It's basically planning,
[00:25:48.640 --> 00:25:51.120]   and a lot of planning systems in robotics
[00:25:51.120 --> 00:25:52.840]   are actually based on this.
[00:25:52.840 --> 00:25:56.160]   And you can think of this as a form of reasoning.
[00:25:56.160 --> 00:26:00.800]   So to take the example of the teenager driving a car again,
[00:26:00.800 --> 00:26:02.880]   you have a pretty good dynamical model of the car,
[00:26:02.880 --> 00:26:04.200]   it doesn't need to be very accurate,
[00:26:04.200 --> 00:26:06.760]   but you know, again, that if you turn the wheel
[00:26:06.760 --> 00:26:08.080]   to the right and there is a cliff,
[00:26:08.080 --> 00:26:09.400]   you're gonna run off the cliff, right?
[00:26:09.400 --> 00:26:10.840]   You don't need to have a very accurate model
[00:26:10.840 --> 00:26:12.080]   to predict that.
[00:26:12.080 --> 00:26:13.640]   And you can run this in your mind
[00:26:13.640 --> 00:26:16.040]   and decide not to do it for that reason,
[00:26:16.040 --> 00:26:17.440]   because you can predict in advance
[00:26:17.440 --> 00:26:18.560]   that the result is gonna be bad.
[00:26:18.560 --> 00:26:21.000]   So you can sort of imagine different scenarios
[00:26:21.000 --> 00:26:25.200]   and then employ or take the first step in the scenario
[00:26:25.200 --> 00:26:26.360]   that is most favorable,
[00:26:26.360 --> 00:26:27.800]   and then repeat the process of planning.
[00:26:27.800 --> 00:26:30.560]   That's called receding horizon model predictive control.
[00:26:30.560 --> 00:26:33.000]   So even all those things have names,
[00:26:33.000 --> 00:26:34.640]   going back decades.
[00:26:35.800 --> 00:26:38.960]   And so if you're not, you know,
[00:26:38.960 --> 00:26:40.040]   in classical optimal control,
[00:26:40.040 --> 00:26:42.440]   the model of the world is not generally learned.
[00:26:42.440 --> 00:26:44.840]   There's, you know, sometimes a few parameters
[00:26:44.840 --> 00:26:45.680]   you have to identify,
[00:26:45.680 --> 00:26:47.080]   that's called systems identification.
[00:26:47.080 --> 00:26:52.000]   But generally the model is mostly deterministic
[00:26:52.000 --> 00:26:53.280]   and mostly built by hand.
[00:26:53.280 --> 00:26:55.640]   So the big question of AI,
[00:26:55.640 --> 00:26:58.720]   I think the big challenge of AI for the next decade
[00:26:58.720 --> 00:27:01.080]   is how do we get machines to run predictive models
[00:27:01.080 --> 00:27:03.680]   of the world that deal with uncertainty
[00:27:03.680 --> 00:27:05.800]   and deal with the real world in all this complexity.
[00:27:05.800 --> 00:27:08.120]   So it's not just the trajectory of a rocket,
[00:27:08.120 --> 00:27:10.200]   which you can reduce to first principles.
[00:27:10.200 --> 00:27:13.000]   It's not even just the trajectory of a robot arm,
[00:27:13.000 --> 00:27:14.880]   which again, you can model by, you know,
[00:27:14.880 --> 00:27:17.160]   careful mathematics, but it's everything else,
[00:27:17.160 --> 00:27:18.840]   everything we observe in the world, you know,
[00:27:18.840 --> 00:27:22.960]   people behavior, you know, physical systems
[00:27:22.960 --> 00:27:27.960]   that involve collective phenomena like water or, you know,
[00:27:27.960 --> 00:27:31.880]   trees and, you know, branches in a tree or something,
[00:27:31.880 --> 00:27:35.040]   or like complex things that, you know,
[00:27:35.040 --> 00:27:38.520]   humans have no trouble developing abstract representations
[00:27:38.520 --> 00:27:39.840]   and predictive model for,
[00:27:39.840 --> 00:27:41.600]   but we still don't know how to do with machines.
[00:27:41.600 --> 00:27:43.880]   - Where do you put in these three,
[00:27:43.880 --> 00:27:46.160]   maybe in the planning stages,
[00:27:46.160 --> 00:27:50.640]   the game theoretic nature of this world,
[00:27:50.640 --> 00:27:52.960]   where your actions not only respond
[00:27:52.960 --> 00:27:55.520]   to the dynamic nature of the world, the environment,
[00:27:55.520 --> 00:27:57.520]   but also affect it.
[00:27:57.520 --> 00:27:59.840]   So if there's other humans involved,
[00:27:59.840 --> 00:28:02.240]   is this point number four,
[00:28:02.240 --> 00:28:03.400]   or is it somehow integrated
[00:28:03.400 --> 00:28:06.640]   into the hierarchical representation of action in your view?
[00:28:06.640 --> 00:28:07.480]   - I think it's integrated.
[00:28:07.480 --> 00:28:11.360]   It's just that now your model of the world has to deal with,
[00:28:11.360 --> 00:28:13.080]   you know, it just makes it more complicated, right?
[00:28:13.080 --> 00:28:15.600]   The fact that humans are complicated
[00:28:15.600 --> 00:28:17.240]   and not easily predictable,
[00:28:17.240 --> 00:28:19.880]   that makes your model of the world much more complicated,
[00:28:19.880 --> 00:28:21.360]   that much more complicated.
[00:28:21.360 --> 00:28:23.080]   - Well, there's a chess, I mean,
[00:28:23.080 --> 00:28:25.280]   I suppose chess is an analogy.
[00:28:25.280 --> 00:28:28.120]   So Monte Carlo tree search.
[00:28:28.120 --> 00:28:32.040]   I mean, there's a, I go, you go, I go, you go.
[00:28:32.040 --> 00:28:34.960]   Like Andrej Karpathy recently gave a talk
[00:28:34.960 --> 00:28:36.960]   at MIT about car doors.
[00:28:36.960 --> 00:28:39.280]   I think there's some machine learning too,
[00:28:39.280 --> 00:28:40.920]   but mostly car doors.
[00:28:40.920 --> 00:28:43.360]   And there's a dynamic nature to the car,
[00:28:43.360 --> 00:28:45.720]   like the person opening the door checking.
[00:28:45.720 --> 00:28:46.880]   I mean, he wasn't talking about that.
[00:28:46.880 --> 00:28:48.440]   He was talking about the perception problem
[00:28:48.440 --> 00:28:50.920]   of what the ontology of what defines a car door,
[00:28:50.920 --> 00:28:52.920]   this big philosophical question.
[00:28:52.920 --> 00:28:54.080]   But to me, it was interesting,
[00:28:54.080 --> 00:28:55.720]   'cause like it's obvious
[00:28:55.720 --> 00:28:57.320]   that the person opening the car doors,
[00:28:57.320 --> 00:28:59.600]   they're trying to get out, like here in New York,
[00:28:59.600 --> 00:29:01.440]   trying to get out of the car.
[00:29:01.440 --> 00:29:03.640]   You slowing down is going to signal something.
[00:29:03.640 --> 00:29:05.440]   You speeding up is gonna signal something.
[00:29:05.440 --> 00:29:06.480]   That's a dance.
[00:29:06.480 --> 00:29:10.200]   It's a asynchronous chess game.
[00:29:10.200 --> 00:29:11.040]   I don't know.
[00:29:11.040 --> 00:29:16.960]   So it feels like it's not just,
[00:29:16.960 --> 00:29:18.800]   I mean, I guess you can integrate all of them
[00:29:18.800 --> 00:29:20.360]   to one giant model,
[00:29:20.360 --> 00:29:24.360]   like the entirety of these little interactions.
[00:29:24.360 --> 00:29:25.760]   'Cause it's not as complicated as chess.
[00:29:25.760 --> 00:29:27.160]   It's just like a little dance.
[00:29:27.160 --> 00:29:28.800]   We do like a little dance together
[00:29:28.800 --> 00:29:30.000]   and then we figure it out.
[00:29:30.000 --> 00:29:32.520]   - Well, in some ways it's way more complicated than chess
[00:29:32.520 --> 00:29:35.000]   because it's continuous,
[00:29:35.000 --> 00:29:37.280]   it's uncertain in a continuous manner.
[00:29:37.280 --> 00:29:39.840]   - It doesn't feel more complicated.
[00:29:39.840 --> 00:29:41.080]   - But it doesn't feel more complicated
[00:29:41.080 --> 00:29:43.680]   because that's what we've evolved to solve.
[00:29:43.680 --> 00:29:45.480]   This is the kind of problem we've evolved to solve.
[00:29:45.480 --> 00:29:46.400]   And so we're good at it
[00:29:46.400 --> 00:29:49.280]   because nature has made us good at it.
[00:29:49.280 --> 00:29:52.360]   Nature has not made us good at chess.
[00:29:52.360 --> 00:29:54.200]   We completely suck at chess.
[00:29:55.720 --> 00:29:58.000]   In fact, that's why we designed it as a game,
[00:29:58.000 --> 00:29:59.040]   is to be challenging.
[00:29:59.040 --> 00:30:02.600]   And if there is something that recent progress
[00:30:02.600 --> 00:30:05.600]   in chess and Go has made us realize
[00:30:05.600 --> 00:30:07.920]   is that humans are really terrible at those things,
[00:30:07.920 --> 00:30:08.920]   like really bad.
[00:30:08.920 --> 00:30:11.520]   There was a story before AlphaGo
[00:30:11.520 --> 00:30:15.200]   that the best Go players thought
[00:30:15.200 --> 00:30:16.720]   there were maybe two or three stones
[00:30:16.720 --> 00:30:19.640]   behind an ideal player that they would call God.
[00:30:19.640 --> 00:30:23.680]   In fact, no, there are like nine or 10 stones behind,
[00:30:23.680 --> 00:30:25.360]   I mean, which is bad.
[00:30:25.360 --> 00:30:27.400]   So we're not good at,
[00:30:27.400 --> 00:30:30.360]   and it's because we have limited working memory.
[00:30:30.360 --> 00:30:32.960]   We're not very good at doing this tree exploration
[00:30:32.960 --> 00:30:36.760]   that computers are much better at doing than we are,
[00:30:36.760 --> 00:30:37.960]   but we are much better
[00:30:37.960 --> 00:30:40.600]   at learning differentiable models of the world.
[00:30:40.600 --> 00:30:43.840]   I mean, I say differentiable in a kind of,
[00:30:43.840 --> 00:30:46.040]   I should say not differentiable in the sense
[00:30:46.040 --> 00:30:47.480]   that we run backprop through it,
[00:30:47.480 --> 00:30:50.520]   but in the sense that our brain has some mechanism
[00:30:50.520 --> 00:30:54.080]   for estimating gradients of some kind.
[00:30:54.080 --> 00:30:56.520]   And that's what makes us efficient.
[00:30:56.520 --> 00:31:01.520]   So if you have an agent that consists of a model
[00:31:01.520 --> 00:31:04.360]   of the world, which in the human brain
[00:31:04.360 --> 00:31:06.800]   is basically the entire front half of your brain,
[00:31:06.800 --> 00:31:13.320]   an objective function, which in humans
[00:31:13.320 --> 00:31:14.400]   is a combination of two things.
[00:31:14.400 --> 00:31:17.640]   There is your sort of intrinsic motivation module,
[00:31:17.640 --> 00:31:19.120]   which is in the basal ganglia,
[00:31:19.120 --> 00:31:20.080]   the base of your brain.
[00:31:20.080 --> 00:31:22.480]   That's the thing that measures pain and hunger
[00:31:22.480 --> 00:31:26.800]   and things like that, like immediate feelings and emotions.
[00:31:26.800 --> 00:31:30.720]   And then there is the equivalent
[00:31:30.720 --> 00:31:32.560]   of what people in reinforcement learning call a critic,
[00:31:32.560 --> 00:31:36.040]   which is a sort of module that predicts ahead
[00:31:36.040 --> 00:31:41.040]   what the outcome of a situation will be.
[00:31:41.040 --> 00:31:43.800]   And so it's not a cost function,
[00:31:43.800 --> 00:31:45.400]   but it's sort of not an objective function,
[00:31:45.400 --> 00:31:49.000]   but it's sort of a trained predictor
[00:31:49.000 --> 00:31:50.960]   of the ultimate objective function.
[00:31:50.960 --> 00:31:52.560]   And that also is differentiable.
[00:31:52.560 --> 00:31:54.640]   And so if all of this is differentiable,
[00:31:54.640 --> 00:31:59.640]   your cost function, your critic, your world model,
[00:31:59.640 --> 00:32:03.080]   then you can use gradient-based type methods
[00:32:03.080 --> 00:32:05.840]   to do planning, to do reasoning, to do learning,
[00:32:05.840 --> 00:32:08.160]   to do all the things that we'd like
[00:32:08.160 --> 00:32:11.840]   an intelligent agent to do.
[00:32:11.840 --> 00:32:14.200]   - And a gradient-based learning,
[00:32:14.200 --> 00:32:15.360]   like what's your intuition?
[00:32:15.360 --> 00:32:18.400]   That's probably at the core of what can solve intelligence.
[00:32:18.400 --> 00:32:23.400]   So you don't need like a logic-based reasoning in your view.
[00:32:23.400 --> 00:32:27.280]   - I don't know how to make logic-based reasoning
[00:32:27.280 --> 00:32:29.760]   compatible with efficient learning.
[00:32:29.760 --> 00:32:31.000]   - Yeah.
[00:32:31.000 --> 00:32:32.320]   - Okay, I mean, there is a big question,
[00:32:32.320 --> 00:32:33.880]   perhaps a philosophical question.
[00:32:33.880 --> 00:32:35.200]   I mean, it's not that philosophical,
[00:32:35.200 --> 00:32:38.080]   but that we can ask is that, you know,
[00:32:38.080 --> 00:32:40.360]   all the learning algorithms we know
[00:32:40.360 --> 00:32:43.280]   from engineering and computer science
[00:32:43.280 --> 00:32:46.520]   proceed by optimizing some objective function.
[00:32:46.520 --> 00:32:47.400]   - Yeah. - Right?
[00:32:48.320 --> 00:32:49.920]   So one question we may ask is,
[00:32:49.920 --> 00:32:54.720]   does learning in the brain minimize an objective function?
[00:32:54.720 --> 00:32:57.320]   I mean, it could be a composite
[00:32:57.320 --> 00:32:58.480]   of multiple objective functions,
[00:32:58.480 --> 00:33:00.320]   but it's still an objective function.
[00:33:00.320 --> 00:33:04.640]   Second, if it does optimize an objective function,
[00:33:04.640 --> 00:33:09.640]   does it do it by some sort of gradient estimation?
[00:33:09.640 --> 00:33:10.880]   You know, it doesn't need to be backprop,
[00:33:10.880 --> 00:33:14.840]   but some way of estimating the gradient in an efficient manner
[00:33:14.840 --> 00:33:17.000]   whose complexity is on the same order of magnitude
[00:33:17.000 --> 00:33:20.760]   as actually running the inference.
[00:33:20.760 --> 00:33:24.920]   'Cause you can't afford to do things like, you know,
[00:33:24.920 --> 00:33:26.520]   perturbing a weight in your brain
[00:33:26.520 --> 00:33:28.040]   to figure out what the effect is,
[00:33:28.040 --> 00:33:29.640]   and then sort of, you know,
[00:33:29.640 --> 00:33:32.640]   you can't do sort of estimating gradient by perturbation.
[00:33:32.640 --> 00:33:35.400]   It's, to me, it seems very implausible
[00:33:35.400 --> 00:33:39.200]   that the brain uses some sort of, you know,
[00:33:39.200 --> 00:33:42.960]   zeroth-order black box gradient-free optimization
[00:33:42.960 --> 00:33:45.160]   because it's so much less efficient
[00:33:45.160 --> 00:33:46.280]   than gradient optimization.
[00:33:46.280 --> 00:33:49.240]   So it has to have a way of estimating gradient.
[00:33:49.240 --> 00:33:52.760]   - Is it possible that some kind of logic-based reasoning
[00:33:52.760 --> 00:33:56.320]   emerges in pockets as a useful, like you said,
[00:33:56.320 --> 00:33:58.080]   if the brain is an objective function,
[00:33:58.080 --> 00:34:01.280]   maybe it's a mechanism for creating objective functions.
[00:34:01.280 --> 00:34:06.280]   It's a mechanism for creating knowledge bases, for example,
[00:34:06.280 --> 00:34:08.360]   that can then be queried.
[00:34:08.360 --> 00:34:10.240]   Like, maybe it's like an efficient representation
[00:34:10.240 --> 00:34:12.640]   of knowledge that's learned in a gradient-based way
[00:34:12.640 --> 00:34:13.760]   or something like that.
[00:34:13.760 --> 00:34:15.920]   - Well, so I think there is a lot of different types
[00:34:15.920 --> 00:34:17.280]   of intelligence.
[00:34:17.280 --> 00:34:19.600]   So first of all, I think the type of logical reasoning
[00:34:19.600 --> 00:34:23.040]   that we think about, that we are, you know,
[00:34:23.040 --> 00:34:26.000]   maybe stemming from, you know, sort of classical AI
[00:34:26.000 --> 00:34:27.640]   of the 1970s and '80s,
[00:34:27.640 --> 00:34:32.920]   I think humans use that relatively rarely
[00:34:32.920 --> 00:34:34.640]   and are not particularly good at it.
[00:34:34.640 --> 00:34:37.480]   - But we judge each other based on our ability
[00:34:37.480 --> 00:34:40.520]   to solve those rare problems.
[00:34:40.520 --> 00:34:41.600]   It's called an IQ test.
[00:34:41.600 --> 00:34:42.600]   - I don't think so.
[00:34:42.600 --> 00:34:45.160]   Like, I'm not very good at chess.
[00:34:45.160 --> 00:34:48.480]   - Yes, I'm judging you this whole time because,
[00:34:48.480 --> 00:34:49.720]   well, we actually-
[00:34:49.720 --> 00:34:51.760]   - With your, you know, heritage,
[00:34:51.760 --> 00:34:53.440]   I'm sure you're good at chess.
[00:34:53.440 --> 00:34:55.000]   - No, stereotypes.
[00:34:55.000 --> 00:34:56.640]   Not all stereotypes are true.
[00:34:56.640 --> 00:34:58.960]   - Well, I'm terrible at chess.
[00:34:58.960 --> 00:35:03.960]   So, you know, but I think perhaps another type
[00:35:03.960 --> 00:35:07.560]   of intelligence that I have is this, you know,
[00:35:07.560 --> 00:35:11.000]   ability of sort of building models to the world from,
[00:35:11.000 --> 00:35:15.640]   you know, reasoning, obviously, but also data.
[00:35:15.640 --> 00:35:18.600]   And those models generally are more kind of analogical,
[00:35:18.600 --> 00:35:19.440]   right?
[00:35:19.440 --> 00:35:23.880]   So it's reasoning by simulation and by analogy,
[00:35:23.880 --> 00:35:26.840]   where you use one model to apply to a new situation,
[00:35:26.840 --> 00:35:28.400]   even though you've never seen that situation,
[00:35:28.400 --> 00:35:31.560]   you can sort of connect it to a situation
[00:35:31.560 --> 00:35:33.440]   you've encountered before.
[00:35:33.440 --> 00:35:36.360]   And your reasoning is more, you know,
[00:35:36.360 --> 00:35:38.360]   akin to some sort of internal simulation.
[00:35:38.360 --> 00:35:41.080]   So you're kind of simulating what's happening
[00:35:41.080 --> 00:35:42.160]   when you're building, I don't know,
[00:35:42.160 --> 00:35:44.000]   a box out of wood or something, right?
[00:35:44.000 --> 00:35:46.360]   You kind of imagine in advance,
[00:35:46.360 --> 00:35:47.760]   like what would be the result of, you know,
[00:35:47.760 --> 00:35:49.560]   cutting the wood in this particular way?
[00:35:49.560 --> 00:35:52.800]   Are you going to use, you know, screws and nails or whatever?
[00:35:52.800 --> 00:35:54.080]   When you are interacting with someone,
[00:35:54.080 --> 00:35:55.720]   you also have a model of that person
[00:35:55.720 --> 00:35:59.480]   and sort of interact with that person, you know,
[00:35:59.480 --> 00:36:03.560]   having this model in mind to kind of tell the person
[00:36:03.560 --> 00:36:05.200]   what you think is useful to them.
[00:36:05.240 --> 00:36:10.160]   So I think this ability to construct models of the world
[00:36:10.160 --> 00:36:13.840]   is basically the essence of intelligence.
[00:36:13.840 --> 00:36:18.200]   And the ability to use it then to plan actions
[00:36:18.200 --> 00:36:23.040]   that will fulfill a particular criterion,
[00:36:23.040 --> 00:36:25.440]   of course, is necessary as well.
[00:36:25.440 --> 00:36:27.720]   - So I'm going to ask you a series of impossible questions
[00:36:27.720 --> 00:36:30.160]   as we keep asking, as I've been doing.
[00:36:30.160 --> 00:36:33.400]   So if that's the fundamental sort of dark matter
[00:36:33.400 --> 00:36:36.560]   of intelligence, this ability to form a background model,
[00:36:36.560 --> 00:36:41.440]   what's your intuition about how much knowledge is required?
[00:36:41.440 --> 00:36:43.120]   You know, I think dark matter,
[00:36:43.120 --> 00:36:46.000]   you can put a percentage on it
[00:36:46.000 --> 00:36:50.040]   of the composition of the universe
[00:36:50.040 --> 00:36:51.440]   and how much of it is dark matter,
[00:36:51.440 --> 00:36:52.640]   how much of it is dark energy.
[00:36:52.640 --> 00:36:57.640]   How much information do you think is required
[00:36:57.640 --> 00:36:59.920]   to be a house cat?
[00:36:59.920 --> 00:37:02.160]   So you have to be able to, when you see a box,
[00:37:02.160 --> 00:37:06.240]   go in it, when you see a human compute the most evil action,
[00:37:06.240 --> 00:37:09.600]   if there's a thing that's near an edge, you knock it off.
[00:37:09.600 --> 00:37:12.720]   All of that, plus the extra stuff you mentioned,
[00:37:12.720 --> 00:37:15.720]   which is a great self-awareness of the physics
[00:37:15.720 --> 00:37:18.740]   of your own body and the world.
[00:37:18.740 --> 00:37:21.600]   How much knowledge is required, do you think, to solve it?
[00:37:21.600 --> 00:37:25.600]   I don't even know how to measure an answer to that question.
[00:37:25.600 --> 00:37:26.680]   - I'm not sure how to measure it,
[00:37:26.680 --> 00:37:31.140]   but whatever it is, it fits in about 800,000 neurons.
[00:37:32.100 --> 00:37:33.900]   800 million neurons, sorry.
[00:37:33.900 --> 00:37:35.380]   - The representation does.
[00:37:35.380 --> 00:37:38.660]   - Everything, all knowledge, everything, right?
[00:37:38.660 --> 00:37:41.460]   It's less than a billion.
[00:37:41.460 --> 00:37:44.380]   A dog is two billion, but a cat is less than one billion.
[00:37:44.380 --> 00:37:48.100]   And so multiply that by a thousand
[00:37:48.100 --> 00:37:50.260]   and you get the number of synapses.
[00:37:50.260 --> 00:37:52.740]   And I think almost all of it is learned
[00:37:52.740 --> 00:37:55.900]   through a sort of self-supervised learning,
[00:37:55.900 --> 00:37:58.460]   although I think a tiny sliver
[00:37:58.460 --> 00:37:59.860]   is learned through reinforcement learning,
[00:37:59.860 --> 00:38:02.180]   and certainly very little through
[00:38:02.180 --> 00:38:03.300]   classical supervised learning,
[00:38:03.300 --> 00:38:05.180]   although it's not even clear how supervised learning
[00:38:05.180 --> 00:38:08.100]   actually works in the biological world.
[00:38:08.100 --> 00:38:12.860]   So I think almost all of it is self-supervised learning,
[00:38:12.860 --> 00:38:17.860]   but it's driven by the sort of ingrained objective functions
[00:38:17.860 --> 00:38:21.380]   that a cat or a human have at the base of their brain,
[00:38:21.380 --> 00:38:24.880]   which kind of drives their behavior.
[00:38:24.880 --> 00:38:28.560]   So nature tells us you're hungry.
[00:38:29.480 --> 00:38:31.900]   It doesn't tell us how to feed ourselves.
[00:38:31.900 --> 00:38:33.500]   That's something that the rest of our brain
[00:38:33.500 --> 00:38:34.820]   has to figure out, right?
[00:38:34.820 --> 00:38:37.940]   - What's interesting is there might be more
[00:38:37.940 --> 00:38:39.660]   like deeper objective functions
[00:38:39.660 --> 00:38:41.300]   that are allowing the whole thing.
[00:38:41.300 --> 00:38:44.500]   So hunger may be some kind of,
[00:38:44.500 --> 00:38:46.140]   now you go to like neurobiology,
[00:38:46.140 --> 00:38:47.500]   it might be just the brain
[00:38:47.500 --> 00:38:52.460]   trying to maintain homeostasis.
[00:38:52.460 --> 00:38:57.460]   So hunger is just one of the human perceivable symptoms
[00:38:58.020 --> 00:39:01.440]   of the brain being unhappy with the way things are currently.
[00:39:01.440 --> 00:39:03.240]   It could be just like one really dumb
[00:39:03.240 --> 00:39:04.920]   objective function at the core.
[00:39:04.920 --> 00:39:08.440]   - But that's how behavior is driven.
[00:39:08.440 --> 00:39:11.240]   The fact that the basal ganglia
[00:39:11.240 --> 00:39:14.800]   drive us to do things that are different
[00:39:14.800 --> 00:39:18.160]   from say an orangutan or certainly a cat
[00:39:18.160 --> 00:39:20.040]   is what makes human nature
[00:39:20.040 --> 00:39:23.240]   versus orangutan nature versus cat nature.
[00:39:23.240 --> 00:39:24.420]   So for example,
[00:39:25.680 --> 00:39:28.540]   our basal ganglia drives us to
[00:39:28.540 --> 00:39:32.220]   seek the company of other humans.
[00:39:32.220 --> 00:39:34.540]   And that's because nature has figured out
[00:39:34.540 --> 00:39:36.140]   that we need to be social animals
[00:39:36.140 --> 00:39:37.540]   for our species to survive.
[00:39:37.540 --> 00:39:40.320]   And it's true of many primates.
[00:39:40.320 --> 00:39:42.620]   It's not true of orangutans.
[00:39:42.620 --> 00:39:44.920]   Orangutans are solitary animals.
[00:39:44.920 --> 00:39:46.900]   They don't seek the company of others.
[00:39:46.900 --> 00:39:48.140]   In fact, they avoid them.
[00:39:48.140 --> 00:39:51.060]   In fact, they scream at them when they come too close
[00:39:51.060 --> 00:39:52.740]   because they're territorial.
[00:39:52.740 --> 00:39:54.940]   'Cause for their survival,
[00:39:55.880 --> 00:39:58.280]   evolution has figured out that's the best thing.
[00:39:58.280 --> 00:40:00.040]   I mean, they're occasionally social, of course,
[00:40:00.040 --> 00:40:03.520]   for reproduction and stuff like that.
[00:40:03.520 --> 00:40:05.920]   But they're mostly solitary.
[00:40:05.920 --> 00:40:09.720]   So all of those behaviors are not part of intelligence.
[00:40:09.720 --> 00:40:11.040]   People say, "Oh, you're never gonna have
[00:40:11.040 --> 00:40:13.960]   "intelligent machines because human intelligence is social."
[00:40:13.960 --> 00:40:16.840]   But then you look at orangutans, you look at octopus.
[00:40:16.840 --> 00:40:18.800]   Octopus never know their parents.
[00:40:18.800 --> 00:40:20.480]   They barely interact with any other.
[00:40:20.480 --> 00:40:22.200]   And they get to be really smart
[00:40:22.200 --> 00:40:24.920]   in less than a year, in like half a year.
[00:40:24.920 --> 00:40:27.640]   In a year, they're adults.
[00:40:27.640 --> 00:40:28.800]   In two years, they're dead.
[00:40:28.800 --> 00:40:33.600]   So there are things that we think, as humans,
[00:40:33.600 --> 00:40:35.760]   are intimately linked with intelligence,
[00:40:35.760 --> 00:40:38.840]   like social interaction, like language.
[00:40:38.840 --> 00:40:43.520]   I think we give way too much importance to language
[00:40:43.520 --> 00:40:46.760]   as a substrate of intelligence as humans
[00:40:46.760 --> 00:40:49.840]   because we think our reasoning is so linked with language.
[00:40:49.840 --> 00:40:53.480]   - So to solve the house cat intelligence problem,
[00:40:53.480 --> 00:40:55.480]   you think you could do it on a desert island?
[00:40:55.480 --> 00:40:56.760]   You could have-- - Pretty much.
[00:40:56.760 --> 00:40:58.760]   - You could just have a cat sitting there
[00:40:58.760 --> 00:41:03.160]   looking at the waves, at the ocean waves,
[00:41:03.160 --> 00:41:05.720]   and figure a lot of it out.
[00:41:05.720 --> 00:41:08.840]   - It needs to have sort of the right set of drives
[00:41:08.840 --> 00:41:12.540]   to kind of get it to do the thing
[00:41:12.540 --> 00:41:14.320]   and learn the appropriate things, right?
[00:41:14.320 --> 00:41:19.320]   But like, for example, baby humans are driven
[00:41:19.440 --> 00:41:21.920]   to learn to stand up and walk.
[00:41:21.920 --> 00:41:26.000]   That's kind of, this desire is hardwired.
[00:41:26.000 --> 00:41:28.520]   How to do it precisely is not, that's learned.
[00:41:28.520 --> 00:41:30.440]   But the desire to-- - To walk?
[00:41:30.440 --> 00:41:35.440]   - Move around and stand up, that's sort of hardwired.
[00:41:35.440 --> 00:41:38.040]   But it's very simple to hardwire this kind of stuff.
[00:41:38.040 --> 00:41:42.760]   - Oh, like the desire to, well, that's interesting.
[00:41:42.760 --> 00:41:44.400]   You're hardwired to want to walk.
[00:41:45.600 --> 00:41:50.440]   That's not a, there's gotta be a deeper need for walking.
[00:41:50.440 --> 00:41:53.120]   I think it was probably socially imposed by society
[00:41:53.120 --> 00:41:55.560]   that you need to walk, all the other bipedal--
[00:41:55.560 --> 00:41:58.280]   - No, like a lot of simple animals that, you know,
[00:41:58.280 --> 00:42:01.040]   would probably walk without ever watching
[00:42:01.040 --> 00:42:03.880]   any other members of the species.
[00:42:03.880 --> 00:42:06.820]   - It seems like a scary thing to have to do
[00:42:06.820 --> 00:42:09.280]   'cause you suck at bipedal walking at first.
[00:42:09.280 --> 00:42:13.380]   It seems crawling is much safer, much more,
[00:42:13.380 --> 00:42:15.120]   like, why are you in a hurry?
[00:42:15.720 --> 00:42:17.560]   - Well, because you have this thing
[00:42:17.560 --> 00:42:19.320]   that drives you to do it, you know,
[00:42:19.320 --> 00:42:25.080]   which is sort of part of the sort of human development.
[00:42:25.080 --> 00:42:26.720]   - Is that understood, actually, what--
[00:42:26.720 --> 00:42:28.280]   - Not entirely, no.
[00:42:28.280 --> 00:42:29.760]   - What's the reason to get on two feet?
[00:42:29.760 --> 00:42:30.680]   It's really hard.
[00:42:30.680 --> 00:42:32.800]   Like, most animals don't get on two feet.
[00:42:32.800 --> 00:42:34.000]   Why is that? - Well, they get on four feet.
[00:42:34.000 --> 00:42:35.800]   You know, many mammals get on four feet.
[00:42:35.800 --> 00:42:36.800]   - Yeah, they do get-- - Very quickly.
[00:42:36.800 --> 00:42:38.520]   Some of them, extremely quickly.
[00:42:38.520 --> 00:42:41.400]   - But I don't, you know, like, from the last time
[00:42:41.400 --> 00:42:43.640]   I've interacted with a table, that's much more stable
[00:42:43.640 --> 00:42:44.980]   than a thing on two legs.
[00:42:44.980 --> 00:42:46.440]   It's just a really hard problem.
[00:42:46.440 --> 00:42:49.640]   - Yeah, I mean, birds have figured it out with two feet.
[00:42:49.640 --> 00:42:52.000]   - Well, technically, we can go into ontology.
[00:42:52.000 --> 00:42:53.160]   They have four.
[00:42:53.160 --> 00:42:54.480]   I guess they have two feet.
[00:42:54.480 --> 00:42:56.400]   - They have two feet. - Chickens.
[00:42:56.400 --> 00:42:58.840]   - You know, dinosaurs have two feet, many of them.
[00:42:58.840 --> 00:42:59.680]   - Allegedly.
[00:42:59.680 --> 00:43:04.320]   I'm just now learning that T-Rex was eating grass,
[00:43:04.320 --> 00:43:05.400]   not other animals.
[00:43:05.400 --> 00:43:08.040]   T-Rex might have been a friendly pet.
[00:43:08.040 --> 00:43:11.560]   What do you think about, I don't know if you looked at
[00:43:12.440 --> 00:43:14.620]   the test for general intelligence
[00:43:14.620 --> 00:43:16.380]   that Francois Chollet put together.
[00:43:16.380 --> 00:43:18.100]   I don't know if you got a chance to look at
[00:43:18.100 --> 00:43:19.660]   that kind of thing.
[00:43:19.660 --> 00:43:21.820]   What's your intuition about how to solve
[00:43:21.820 --> 00:43:23.740]   like an IQ type of test?
[00:43:23.740 --> 00:43:24.580]   - I don't know.
[00:43:24.580 --> 00:43:26.140]   I think it's so outside of my radar screen
[00:43:26.140 --> 00:43:30.700]   that it's not really relevant, I think, in the short term.
[00:43:30.700 --> 00:43:33.920]   - Well, I guess one way to ask, another way,
[00:43:33.920 --> 00:43:37.280]   perhaps more closer to what, to your work,
[00:43:37.280 --> 00:43:40.260]   is like, how do you solve MNIST
[00:43:40.260 --> 00:43:42.720]   with very little example data?
[00:43:42.720 --> 00:43:44.840]   - That's right, and that's the answer to this probably,
[00:43:44.840 --> 00:43:45.840]   is self-supervised learning.
[00:43:45.840 --> 00:43:48.240]   Just learn to represent images, and then learning
[00:43:48.240 --> 00:43:51.800]   to recognize handwritten digits on top of this
[00:43:51.800 --> 00:43:53.640]   will only require a few samples.
[00:43:53.640 --> 00:43:55.480]   And we observe this in humans, right?
[00:43:55.480 --> 00:43:58.680]   You show a young child a picture book
[00:43:58.680 --> 00:44:01.960]   with a couple of pictures of an elephant, and that's it.
[00:44:01.960 --> 00:44:03.880]   The child knows what an elephant is.
[00:44:03.880 --> 00:44:06.720]   And we see this today with practical systems
[00:44:06.720 --> 00:44:09.520]   that we train image recognition systems
[00:44:09.520 --> 00:44:13.660]   with enormous amounts of images,
[00:44:13.660 --> 00:44:15.740]   either completely self-supervised
[00:44:15.740 --> 00:44:16.960]   or very weakly supervised.
[00:44:16.960 --> 00:44:20.840]   For example, you can train a neural net
[00:44:20.840 --> 00:44:24.120]   to predict whatever hashtag people type on Instagram, right?
[00:44:24.120 --> 00:44:25.740]   Then you can do this with billions of images
[00:44:25.740 --> 00:44:28.520]   'cause there's billions per day that are showing up.
[00:44:28.520 --> 00:44:30.640]   So the amount of training data there
[00:44:30.640 --> 00:44:32.300]   is essentially unlimited.
[00:44:32.300 --> 00:44:35.020]   And then you take the output representation,
[00:44:35.020 --> 00:44:37.320]   you know, a couple layers down from the output
[00:44:37.320 --> 00:44:39.400]   of what the system learned,
[00:44:39.400 --> 00:44:42.000]   and feed this as input to a classifier
[00:44:42.000 --> 00:44:43.760]   for any object in the world that you want,
[00:44:43.760 --> 00:44:45.000]   and it works pretty well.
[00:44:45.000 --> 00:44:47.600]   So that's transfer learning, okay?
[00:44:47.600 --> 00:44:50.140]   Or weakly supervised transfer learning.
[00:44:50.140 --> 00:44:53.480]   People are making very, very fast progress
[00:44:53.480 --> 00:44:55.280]   using self-supervised learning
[00:44:55.280 --> 00:44:57.380]   for this kind of scenario as well.
[00:44:57.380 --> 00:45:00.760]   And, you know, my guess is that
[00:45:00.760 --> 00:45:02.520]   that's gonna be the future.
[00:45:02.520 --> 00:45:03.640]   - For self-supervised learning,
[00:45:03.640 --> 00:45:06.800]   how much cleaning do you think is needed
[00:45:06.800 --> 00:45:11.760]   for filtering malicious signal,
[00:45:11.760 --> 00:45:13.000]   or what's a better term?
[00:45:13.000 --> 00:45:15.720]   But like a lot of people use hashtags on Instagram
[00:45:15.720 --> 00:45:20.040]   to get like good SEO
[00:45:20.040 --> 00:45:23.080]   that doesn't fully represent the contents of the image.
[00:45:23.080 --> 00:45:24.480]   Like they'll put a picture of a cat
[00:45:24.480 --> 00:45:28.040]   and hashtag it with like science, awesome, fun.
[00:45:28.040 --> 00:45:29.720]   I don't know, all kinds.
[00:45:29.720 --> 00:45:31.200]   Why would you put science?
[00:45:31.200 --> 00:45:33.080]   That's not very good SEO.
[00:45:33.080 --> 00:45:34.960]   - The way my colleagues who worked on this project
[00:45:34.960 --> 00:45:38.680]   at Facebook now Meta, Meta.AI,
[00:45:38.680 --> 00:45:41.560]   a few years ago dealt with this is that
[00:45:41.560 --> 00:45:43.760]   they only selected something like 17,000 tags
[00:45:43.760 --> 00:45:48.080]   that correspond to kind of physical things or situations.
[00:45:48.080 --> 00:45:50.320]   Like, you know, that has some visual content.
[00:45:50.320 --> 00:45:55.800]   So, you know, you wouldn't have like #tbt
[00:45:55.800 --> 00:45:57.120]   or anything like that.
[00:45:57.120 --> 00:46:00.800]   - Oh, so they keep a very select set of hashtags
[00:46:00.800 --> 00:46:01.800]   is what you're saying? - Yeah.
[00:46:01.800 --> 00:46:02.640]   - Okay.
[00:46:02.960 --> 00:46:06.040]   - It's still on the order of, you know, 10 to 20,000.
[00:46:06.040 --> 00:46:07.920]   So it's fairly large.
[00:46:07.920 --> 00:46:09.040]   - Okay.
[00:46:09.040 --> 00:46:11.240]   Can you tell me about data augmentation?
[00:46:11.240 --> 00:46:13.240]   What the heck is data augmentation?
[00:46:13.240 --> 00:46:18.240]   And how is it used, maybe contrast of learning for video?
[00:46:18.240 --> 00:46:20.840]   What are some cool ideas here?
[00:46:20.840 --> 00:46:22.080]   - Right, so data augmentation,
[00:46:22.080 --> 00:46:23.800]   I mean, first data augmentation, you know,
[00:46:23.800 --> 00:46:26.120]   is the idea of artificially increasing the size
[00:46:26.120 --> 00:46:29.360]   of your training set by distorting the images
[00:46:29.360 --> 00:46:31.000]   that you have in ways that don't change
[00:46:31.000 --> 00:46:32.320]   the nature of the image, right?
[00:46:32.320 --> 00:46:33.960]   So you take, you do MNIST,
[00:46:33.960 --> 00:46:35.440]   you can do data augmentation on MNIST.
[00:46:35.440 --> 00:46:37.320]   And people have done this since the 1990s, right?
[00:46:37.320 --> 00:46:40.840]   You take an MNIST digit and you shift it a little bit,
[00:46:40.840 --> 00:46:44.800]   or you change the size or rotate it, skew it,
[00:46:44.800 --> 00:46:46.960]   you know, et cetera.
[00:46:46.960 --> 00:46:48.240]   - Add noise.
[00:46:48.240 --> 00:46:49.480]   - Add noise, et cetera.
[00:46:49.480 --> 00:46:50.800]   And it works better.
[00:46:50.800 --> 00:46:53.440]   If you train a supervised classifier with augmented data,
[00:46:53.440 --> 00:46:55.560]   you're gonna get better results.
[00:46:55.560 --> 00:46:58.600]   Now it's become really interesting
[00:46:58.600 --> 00:47:00.400]   over the last couple of years,
[00:47:00.400 --> 00:47:04.160]   because a lot of self-supervised learning techniques
[00:47:04.160 --> 00:47:08.000]   to pre-train vision systems are based on data augmentation.
[00:47:08.000 --> 00:47:12.000]   And the basic techniques is originally inspired
[00:47:12.000 --> 00:47:15.840]   by techniques that I worked on in the early '90s
[00:47:15.840 --> 00:47:17.720]   and Geoff Hinton worked on also in the early '90s.
[00:47:17.720 --> 00:47:20.040]   They were sort of parallel work.
[00:47:20.040 --> 00:47:21.600]   I used to call this Siamese networks.
[00:47:21.600 --> 00:47:24.960]   So basically you take two identical copies
[00:47:24.960 --> 00:47:27.720]   of the same network, they share the same weights,
[00:47:27.720 --> 00:47:31.760]   and you show two different views of the same object.
[00:47:31.760 --> 00:47:33.920]   Either those two different views may have been obtained
[00:47:33.920 --> 00:47:36.480]   by data augmentation, or maybe it's two different views
[00:47:36.480 --> 00:47:39.320]   of the same scene from a camera that you moved
[00:47:39.320 --> 00:47:41.360]   or at different times or something like that, right?
[00:47:41.360 --> 00:47:44.480]   Or two pictures of the same person, things like that.
[00:47:44.480 --> 00:47:46.440]   And then you train this neural net,
[00:47:46.440 --> 00:47:48.400]   those two identical copies of this neural net
[00:47:48.400 --> 00:47:51.480]   to produce an output representation, a vector,
[00:47:51.480 --> 00:47:53.920]   in such a way that the representation
[00:47:53.960 --> 00:47:58.920]   for those two images are as close to each other as possible,
[00:47:58.920 --> 00:48:00.840]   as identical to each other as possible, right?
[00:48:00.840 --> 00:48:04.640]   Because you want the system to basically learn a function
[00:48:04.640 --> 00:48:07.160]   that will be invariant, that will not change,
[00:48:07.160 --> 00:48:10.840]   whose output will not change when you transform those inputs
[00:48:10.840 --> 00:48:12.880]   in those particular ways, right?
[00:48:12.880 --> 00:48:15.680]   So that's easy to do.
[00:48:15.680 --> 00:48:17.720]   What's complicated is how do you make sure
[00:48:17.720 --> 00:48:19.520]   that when you show two images that are different,
[00:48:19.520 --> 00:48:21.960]   the system will produce different things?
[00:48:21.960 --> 00:48:26.200]   Because if you don't have a specific provision for this,
[00:48:26.200 --> 00:48:28.240]   the system will just ignore the input.
[00:48:28.240 --> 00:48:30.360]   When you train it, it will end up ignoring the input
[00:48:30.360 --> 00:48:31.720]   and just produce a constant vector
[00:48:31.720 --> 00:48:33.640]   that is the same for every input, right?
[00:48:33.640 --> 00:48:35.200]   That's called a collapse.
[00:48:35.200 --> 00:48:36.720]   Now, how do you avoid collapse?
[00:48:36.720 --> 00:48:37.760]   So there's two ideas.
[00:48:37.760 --> 00:48:41.560]   One idea that I proposed in the early '90s
[00:48:41.560 --> 00:48:43.080]   with my colleagues at Bell Labs,
[00:48:43.080 --> 00:48:45.360]   Jane Bromley and a couple other people,
[00:48:45.360 --> 00:48:48.280]   which we now call contrastive learning,
[00:48:48.280 --> 00:48:50.040]   which is to have negative examples, right?
[00:48:50.040 --> 00:48:53.200]   So you have pairs of images that you know are different
[00:48:53.200 --> 00:48:57.520]   and you show them to the network and those two copies,
[00:48:57.520 --> 00:48:59.560]   and then you push the two output vectors
[00:48:59.560 --> 00:49:01.120]   away from each other.
[00:49:01.120 --> 00:49:02.240]   And it will eventually guarantee
[00:49:02.240 --> 00:49:04.920]   that things that are semantically similar
[00:49:04.920 --> 00:49:06.520]   produce similar representations
[00:49:06.520 --> 00:49:07.360]   and things that are different
[00:49:07.360 --> 00:49:09.040]   produce different representations.
[00:49:09.040 --> 00:49:11.480]   We actually came up with this idea
[00:49:11.480 --> 00:49:14.520]   for a project of doing signature verification.
[00:49:14.520 --> 00:49:17.880]   So we would collect signatures
[00:49:17.880 --> 00:49:20.200]   from multiple signatures on the same person
[00:49:20.200 --> 00:49:21.440]   and then train a neural net
[00:49:21.440 --> 00:49:23.320]   to produce the same representation.
[00:49:23.320 --> 00:49:28.320]   And then force the system to produce different representation
[00:49:28.320 --> 00:49:29.880]   from different signatures.
[00:49:29.880 --> 00:49:33.000]   This was actually, the problem was proposed
[00:49:33.000 --> 00:49:36.720]   by people from what was a subsidiary of AT&T at the time
[00:49:36.720 --> 00:49:38.280]   called NCR.
[00:49:38.280 --> 00:49:41.040]   And they were interested in storing a representation
[00:49:41.040 --> 00:49:43.520]   of the signature on the 80 bytes
[00:49:43.520 --> 00:49:46.680]   of the magnetic strip of a credit card.
[00:49:46.680 --> 00:49:48.800]   So we came up with this idea of having a neural net
[00:49:48.800 --> 00:49:52.320]   with 80 outputs that we quantized on bytes
[00:49:52.320 --> 00:49:53.880]   so that we could encode the-
[00:49:53.880 --> 00:49:55.480]   - And that encoding was then used to compare
[00:49:55.480 --> 00:49:57.120]   whether the signature matches or not.
[00:49:57.120 --> 00:49:57.960]   - That's right.
[00:49:57.960 --> 00:50:00.680]   So then you would sign, it would run through the neural net
[00:50:00.680 --> 00:50:02.440]   and then you would compare the output vector
[00:50:02.440 --> 00:50:03.280]   to whatever is stored on your card.
[00:50:03.280 --> 00:50:04.680]   - Did it actually work?
[00:50:04.680 --> 00:50:06.880]   - It worked, but they ended up not using it.
[00:50:06.880 --> 00:50:10.160]   Because nobody cares actually.
[00:50:10.160 --> 00:50:13.840]   I mean, the American financial payment system
[00:50:13.840 --> 00:50:17.600]   is incredibly lax in that respect compared to Europe.
[00:50:17.600 --> 00:50:19.000]   - Oh, with the signatures?
[00:50:19.000 --> 00:50:20.560]   What's the purpose of signatures anyway?
[00:50:20.560 --> 00:50:21.400]   This is very-
[00:50:21.400 --> 00:50:23.320]   - Nobody looks at them, nobody cares.
[00:50:23.320 --> 00:50:24.480]   - It's, yeah.
[00:50:24.480 --> 00:50:25.320]   - Yeah, no.
[00:50:25.320 --> 00:50:27.840]   So that's contrastive learning, right?
[00:50:27.840 --> 00:50:29.480]   So you need positive and negative pairs.
[00:50:29.480 --> 00:50:31.760]   And the problem with that is that,
[00:50:31.760 --> 00:50:34.760]   even though I had the original paper on this,
[00:50:34.760 --> 00:50:36.800]   I'm actually not very positive about it
[00:50:36.800 --> 00:50:38.680]   because it doesn't work in high dimension.
[00:50:38.680 --> 00:50:41.040]   If your representation is high dimensional,
[00:50:41.040 --> 00:50:44.280]   there's just too many ways for two things to be different.
[00:50:44.280 --> 00:50:46.320]   And so you would need lots and lots and lots
[00:50:46.320 --> 00:50:48.240]   of negative pairs.
[00:50:48.240 --> 00:50:50.800]   So there is a particular implementation of this,
[00:50:50.800 --> 00:50:51.920]   which is relatively recent
[00:50:51.920 --> 00:50:54.840]   from actually the Google Toronto group,
[00:50:54.840 --> 00:50:58.800]   where Geoff Hinton is the senior member there.
[00:50:58.800 --> 00:51:01.360]   And it's called SimClear, S-I-M-C-L-A-R.
[00:51:01.360 --> 00:51:03.720]   And it, basically a particular way
[00:51:03.720 --> 00:51:06.760]   of implementing this idea of contrastive learning,
[00:51:06.760 --> 00:51:08.600]   the particular objective function.
[00:51:08.640 --> 00:51:13.200]   Now, what I'm much more enthusiastic about these days
[00:51:13.200 --> 00:51:14.640]   is non-contrastive methods.
[00:51:14.640 --> 00:51:16.600]   So other ways to guarantee that
[00:51:16.600 --> 00:51:20.680]   the representations would be different
[00:51:20.680 --> 00:51:23.240]   for different inputs.
[00:51:23.240 --> 00:51:26.160]   And it's actually based on an idea
[00:51:26.160 --> 00:51:29.520]   that Geoff Hinton proposed in the early '90s
[00:51:29.520 --> 00:51:31.960]   with his student at the time, Sue Becker.
[00:51:31.960 --> 00:51:32.800]   And it's based on the idea
[00:51:32.800 --> 00:51:34.280]   of maximizing the mutual information
[00:51:34.280 --> 00:51:36.160]   between the outputs of the two systems.
[00:51:36.160 --> 00:51:37.440]   You only show positive pairs,
[00:51:37.440 --> 00:51:38.800]   you only show pairs of images
[00:51:38.800 --> 00:51:41.640]   that you know are somewhat similar.
[00:51:41.640 --> 00:51:44.160]   And you train the two networks to be informative,
[00:51:44.160 --> 00:51:49.720]   but also to be as informative of each other as possible.
[00:51:49.720 --> 00:51:52.240]   So basically one representation has to be predictable
[00:51:52.240 --> 00:51:53.880]   from the other, essentially.
[00:51:53.880 --> 00:51:57.160]   And he proposed that idea,
[00:51:57.160 --> 00:52:00.200]   had a couple of papers in the early '90s,
[00:52:00.200 --> 00:52:03.080]   and then nothing was done about it for decades.
[00:52:03.080 --> 00:52:04.800]   And I kind of revived this idea
[00:52:04.800 --> 00:52:07.000]   together with my postdocs at FAIR,
[00:52:07.000 --> 00:52:09.640]   particularly a postdoc called Stefan Duny,
[00:52:09.640 --> 00:52:12.520]   who's now a junior professor in Finland
[00:52:12.520 --> 00:52:13.880]   at the University of Aalto.
[00:52:13.880 --> 00:52:19.280]   We came up with something that we called Barlow Twins,
[00:52:19.280 --> 00:52:20.640]   and it's a particular way
[00:52:20.640 --> 00:52:25.480]   of maximizing the information content of a vector
[00:52:25.480 --> 00:52:28.080]   using some hypotheses.
[00:52:28.080 --> 00:52:32.040]   And we have kind of another version of it
[00:52:32.040 --> 00:52:34.520]   that's more recent now called VICREG, V-I-C-R-E-G,
[00:52:34.520 --> 00:52:37.880]   that means variance, invariance, covariance, regularization.
[00:52:37.880 --> 00:52:39.960]   And it's the thing I'm the most excited about
[00:52:39.960 --> 00:52:41.720]   in machine learning in the last 15 years.
[00:52:41.720 --> 00:52:44.360]   I mean, I'm really, really excited about this.
[00:52:44.360 --> 00:52:47.920]   - What kind of data augmentation is useful for that,
[00:52:47.920 --> 00:52:50.240]   not contrast to the learning method?
[00:52:50.240 --> 00:52:52.640]   Are we talking about, does that not matter that much?
[00:52:52.640 --> 00:52:55.920]   Or it seems like a very important part of the step.
[00:52:55.920 --> 00:52:56.760]   - Yeah.
[00:52:56.760 --> 00:52:58.160]   - How you generate the images that are similar,
[00:52:58.160 --> 00:52:59.600]   but sufficiently different.
[00:52:59.600 --> 00:53:00.440]   - Yeah, that's right.
[00:53:00.440 --> 00:53:02.400]   It's an important step, and it's also an annoying step,
[00:53:02.400 --> 00:53:03.760]   because you need to have that knowledge
[00:53:03.760 --> 00:53:06.800]   of what data augmentation you can do
[00:53:06.800 --> 00:53:10.400]   that do not change the nature of the object.
[00:53:10.400 --> 00:53:13.160]   And so the standard scenario,
[00:53:13.160 --> 00:53:15.400]   which a lot of people working in this area are using,
[00:53:15.400 --> 00:53:19.640]   is you use the type of distortion.
[00:53:19.640 --> 00:53:22.040]   So basically you do a geometric distortion.
[00:53:22.040 --> 00:53:24.240]   So one basically just shifts the image a little bit,
[00:53:24.240 --> 00:53:25.240]   it's called cropping.
[00:53:25.240 --> 00:53:27.760]   Another one kind of changes the scale a little bit.
[00:53:27.760 --> 00:53:29.160]   Another one kind of rotates it.
[00:53:29.160 --> 00:53:30.720]   Another one changes the colors.
[00:53:30.720 --> 00:53:32.960]   You can do a shift in color balance
[00:53:32.960 --> 00:53:34.120]   or something like that.
[00:53:34.120 --> 00:53:35.840]   Saturation.
[00:53:35.840 --> 00:53:37.160]   Another one sort of blurs it.
[00:53:37.160 --> 00:53:38.160]   Another one adds noise.
[00:53:38.160 --> 00:53:41.160]   So you have like a catalog of kind of standard things,
[00:53:41.160 --> 00:53:44.040]   and people try to use the same ones for different algorithms
[00:53:44.040 --> 00:53:45.960]   so that they can compare.
[00:53:45.960 --> 00:53:48.240]   But some algorithms, some self-supervised algorithm
[00:53:48.240 --> 00:53:50.680]   actually can deal with much bigger,
[00:53:50.680 --> 00:53:53.560]   like more aggressive data augmentation, and some don't.
[00:53:53.560 --> 00:53:56.400]   So that kind of makes the whole thing difficult.
[00:53:56.400 --> 00:53:58.800]   But that's the kind of distortions we're talking about.
[00:53:58.800 --> 00:54:03.560]   And so you train with those distortions,
[00:54:03.560 --> 00:54:07.320]   and then you chop off the last layer,
[00:54:07.320 --> 00:54:11.120]   a couple layers of the network,
[00:54:11.120 --> 00:54:13.560]   and you use the representation as input to a classifier.
[00:54:13.560 --> 00:54:17.640]   You train the classifier on ImageNet, let's say,
[00:54:17.640 --> 00:54:20.520]   or whatever, and measure the performance.
[00:54:20.520 --> 00:54:23.120]   And interestingly enough,
[00:54:23.120 --> 00:54:24.400]   the methods that are really good
[00:54:24.400 --> 00:54:26.840]   at eliminating the information that is irrelevant,
[00:54:26.840 --> 00:54:29.200]   which is the distortions between those images,
[00:54:29.200 --> 00:54:32.400]   do a good job at eliminating it.
[00:54:32.400 --> 00:54:34.080]   And as a consequence,
[00:54:34.080 --> 00:54:37.200]   you cannot use the representations in those systems
[00:54:37.200 --> 00:54:39.880]   for things like object detection and localization,
[00:54:39.880 --> 00:54:41.520]   because that information is gone.
[00:54:41.520 --> 00:54:44.720]   So the type of data augmentation you need to do
[00:54:44.720 --> 00:54:48.640]   depends on the task you want eventually the system to solve.
[00:54:48.640 --> 00:54:50.680]   And the type of data augmentation,
[00:54:50.680 --> 00:54:52.560]   standard data augmentation that we use today,
[00:54:52.560 --> 00:54:54.680]   are only appropriate for object recognition
[00:54:54.680 --> 00:54:56.000]   or image classification.
[00:54:56.000 --> 00:54:57.760]   They're not appropriate for things like--
[00:54:57.760 --> 00:55:00.800]   - Can you help me out understand why localization is--
[00:55:00.800 --> 00:55:03.760]   So you're saying it's just not good at the negative,
[00:55:03.760 --> 00:55:05.440]   like at classifying the negative,
[00:55:05.440 --> 00:55:07.920]   so that's why it can't be used for the localization?
[00:55:07.920 --> 00:55:10.360]   - No, it's just that you train the system,
[00:55:10.360 --> 00:55:12.360]   you give it an image,
[00:55:12.360 --> 00:55:14.960]   and then you give it the same image shifted and scaled,
[00:55:14.960 --> 00:55:17.400]   and you tell it that's the same image.
[00:55:17.400 --> 00:55:19.160]   So the system basically is trained
[00:55:19.160 --> 00:55:22.040]   to eliminate the information about position and size.
[00:55:22.040 --> 00:55:24.760]   So now you want to use that--
[00:55:24.760 --> 00:55:26.200]   - Oh, like literally--
[00:55:26.200 --> 00:55:27.760]   - Where an object is and what size it is.
[00:55:27.760 --> 00:55:30.800]   - Like a bounding box, like to be able to actually, okay.
[00:55:30.800 --> 00:55:34.160]   It can still find the object in the image,
[00:55:34.160 --> 00:55:35.960]   it's just not very good at finding
[00:55:35.960 --> 00:55:39.000]   the exact boundaries of that object, interesting.
[00:55:39.000 --> 00:55:41.120]   Interesting, which, you know,
[00:55:41.120 --> 00:55:43.480]   that's an interesting sort of philosophical question.
[00:55:43.480 --> 00:55:47.040]   How important is object localization anyway?
[00:55:47.040 --> 00:55:51.240]   We're like obsessed by measuring like image segmentation,
[00:55:51.240 --> 00:55:53.080]   obsessed by measuring perfectly
[00:55:53.080 --> 00:55:54.700]   knowing the boundaries of objects,
[00:55:54.700 --> 00:55:59.700]   when arguably that's not that essential
[00:55:59.700 --> 00:56:03.800]   to understanding what are the contents of the scene.
[00:56:03.800 --> 00:56:05.880]   - On the other hand, I think evolutionarily,
[00:56:05.880 --> 00:56:08.200]   the first vision systems in animals
[00:56:08.200 --> 00:56:10.040]   were basically all about localization,
[00:56:10.040 --> 00:56:12.480]   very little about recognition.
[00:56:12.480 --> 00:56:15.320]   And in the human brain, you have two separate pathways
[00:56:15.320 --> 00:56:20.320]   for recognizing the nature of a scene or an object,
[00:56:20.320 --> 00:56:22.320]   and localizing objects.
[00:56:22.320 --> 00:56:25.200]   So you use the first pathway called the ventral pathway
[00:56:25.200 --> 00:56:28.160]   for telling what you're looking at.
[00:56:28.160 --> 00:56:30.580]   The other pathway, the dorsal pathway,
[00:56:30.580 --> 00:56:34.140]   is used for navigation, for grasping, for everything else.
[00:56:34.140 --> 00:56:36.900]   And basically a lot of the things you need for survival
[00:56:36.900 --> 00:56:39.740]   are localization and detection.
[00:56:39.740 --> 00:56:45.060]   - Is similarity learning or contrastive learning,
[00:56:45.060 --> 00:56:46.540]   are these non-contrastive methods
[00:56:46.540 --> 00:56:48.860]   the same as understanding something?
[00:56:48.860 --> 00:56:50.680]   Just because you know a distorted cat
[00:56:50.680 --> 00:56:52.620]   is the same as a non-distorted cat,
[00:56:52.620 --> 00:56:56.740]   does that mean you understand what it means to be a cat?
[00:56:56.740 --> 00:56:57.580]   - To some extent.
[00:56:57.580 --> 00:57:00.100]   I mean, it's a superficial understanding, obviously.
[00:57:00.100 --> 00:57:02.380]   - But what is the ceiling of this method, do you think?
[00:57:02.380 --> 00:57:05.140]   Is this just one trick on the path
[00:57:05.140 --> 00:57:07.300]   to doing self-supervised learning,
[00:57:07.300 --> 00:57:10.020]   or can we go really, really far?
[00:57:10.020 --> 00:57:11.260]   - I think we can go really far.
[00:57:11.260 --> 00:57:16.260]   So if we figure out how to use techniques of that type,
[00:57:16.260 --> 00:57:19.460]   perhaps very different, but of the same nature,
[00:57:19.460 --> 00:57:22.460]   to train a system from video,
[00:57:22.460 --> 00:57:24.260]   to do video prediction, essentially,
[00:57:24.260 --> 00:57:29.080]   I think we'll have a path towards,
[00:57:29.080 --> 00:57:31.340]   I wouldn't say unlimited,
[00:57:31.340 --> 00:57:36.340]   but a path towards some level of physical common sense
[00:57:36.340 --> 00:57:38.100]   in machines.
[00:57:38.100 --> 00:57:43.100]   And I also think that that ability to learn
[00:57:43.100 --> 00:57:47.720]   how the world works from a high-throughput channel,
[00:57:47.720 --> 00:57:51.900]   like vision, is a necessary step
[00:57:51.900 --> 00:57:55.540]   towards real artificial intelligence.
[00:57:55.540 --> 00:57:58.100]   In other words, I believe in grounded intelligence.
[00:57:58.100 --> 00:57:59.920]   I don't think we can train a machine
[00:57:59.920 --> 00:58:02.180]   to be intelligent purely from text,
[00:58:02.180 --> 00:58:04.420]   because I think the amount of information
[00:58:04.420 --> 00:58:06.220]   about the world that's contained in text
[00:58:06.220 --> 00:58:09.960]   is tiny compared to what we need to know.
[00:58:09.960 --> 00:58:15.300]   So for example, and people have attempted to do this
[00:58:15.300 --> 00:58:17.460]   for 30 years, right, the PSYCH project,
[00:58:17.460 --> 00:58:18.420]   and things like that, right,
[00:58:18.420 --> 00:58:20.620]   of basically kind of writing down all the facts
[00:58:20.620 --> 00:58:24.100]   that are known and hoping that some sort of common sense
[00:58:24.100 --> 00:58:27.180]   will emerge, I think it's basically hopeless.
[00:58:27.180 --> 00:58:28.300]   But let me take an example.
[00:58:28.300 --> 00:58:31.300]   You take an object, I describe a situation to you.
[00:58:31.300 --> 00:58:33.540]   I take an object, I put it on the table,
[00:58:33.540 --> 00:58:34.940]   and I push the table.
[00:58:34.940 --> 00:58:37.220]   It's completely obvious to you that the object
[00:58:37.220 --> 00:58:39.220]   will be pushed with the table, right,
[00:58:39.220 --> 00:58:40.580]   because it's sitting on it.
[00:58:40.580 --> 00:58:43.420]   There's no text in the world, I believe,
[00:58:43.420 --> 00:58:45.060]   that explains this.
[00:58:45.060 --> 00:58:48.380]   And so if you train a machine as powerful as it could be,
[00:58:48.380 --> 00:58:53.380]   you know, your GPT 5000, or whatever it is,
[00:58:53.380 --> 00:58:55.640]   it's never gonna learn about this.
[00:58:55.640 --> 00:59:01.020]   That information is just not present in any text.
[00:59:01.020 --> 00:59:03.260]   - Well, the question, like with the PSYCH project,
[00:59:03.260 --> 00:59:08.020]   the dream, I think, is to have like 10 million,
[00:59:08.020 --> 00:59:13.020]   say, facts like that, that give you a head start,
[00:59:13.260 --> 00:59:15.460]   like a parent guiding you.
[00:59:15.460 --> 00:59:17.580]   Now, we humans don't need a parent to tell us
[00:59:17.580 --> 00:59:19.500]   that the table will move, sorry,
[00:59:19.500 --> 00:59:21.700]   the smartphone will move with the table.
[00:59:21.700 --> 00:59:25.900]   But we get a lot of guidance in other ways,
[00:59:25.900 --> 00:59:28.420]   so it's possible that we can give it a quick shortcut.
[00:59:28.420 --> 00:59:29.460]   - What about a cat?
[00:59:29.460 --> 00:59:31.060]   A cat knows that.
[00:59:31.060 --> 00:59:33.380]   - No, but they evolved, so--
[00:59:33.380 --> 00:59:34.660]   - No, they learn like us.
[00:59:34.660 --> 00:59:37.340]   - Sorry, the physics of stuff?
[00:59:37.340 --> 00:59:38.740]   - Yeah.
[00:59:38.740 --> 00:59:41.620]   - Well, yeah, so you're saying it's,
[00:59:42.500 --> 00:59:45.100]   so you're putting a lot of intelligence
[00:59:45.100 --> 00:59:47.140]   onto the nurture side, not the nature.
[00:59:47.140 --> 00:59:47.980]   - Yes.
[00:59:47.980 --> 00:59:50.020]   - 'Cause we seem to have, you know,
[00:59:50.020 --> 00:59:52.540]   there's a very inefficient, arguably,
[00:59:52.540 --> 00:59:55.540]   process of evolution that got us from bacteria
[00:59:55.540 --> 00:59:56.960]   to who we are today.
[00:59:56.960 --> 00:59:59.780]   Started at the bottom, now we're here.
[00:59:59.780 --> 01:00:04.260]   So the question is how, okay,
[01:00:04.260 --> 01:00:05.980]   the question is how fundamental is that,
[01:00:05.980 --> 01:00:08.380]   the nature of the whole hardware?
[01:00:08.380 --> 01:00:11.660]   And then, is there any way to shortcut it
[01:00:11.660 --> 01:00:12.500]   if it's fundamental?
[01:00:12.500 --> 01:00:14.280]   If it's not, if it's most of the intelligence,
[01:00:14.280 --> 01:00:15.900]   most of the cool stuff we've been talking about
[01:00:15.900 --> 01:00:18.780]   is mostly nurture, mostly trained,
[01:00:18.780 --> 01:00:20.660]   we figure it out by observing the world,
[01:00:20.660 --> 01:00:24.780]   we can form that big, beautiful, sexy background model
[01:00:24.780 --> 01:00:27.240]   that you're talking about just by sitting there.
[01:00:27.240 --> 01:00:32.600]   Then, okay, then you need to, then like maybe,
[01:00:32.600 --> 01:00:37.820]   it is all supervised learning all the way down.
[01:00:37.820 --> 01:00:38.980]   It's all supervised learning, so.
[01:00:38.980 --> 01:00:42.180]   Whatever it is that makes human intelligence
[01:00:42.180 --> 01:00:44.100]   different from other animals,
[01:00:44.100 --> 01:00:46.340]   which a lot of people think is language
[01:00:46.340 --> 01:00:48.740]   and logical reasoning and this kind of stuff,
[01:00:48.740 --> 01:00:49.900]   it cannot be that complicated
[01:00:49.900 --> 01:00:52.860]   because it only popped up in the last million years.
[01:00:52.860 --> 01:00:53.700]   - Yeah.
[01:00:53.700 --> 01:00:59.660]   - And it only involves less than 1% of a genome,
[01:00:59.660 --> 01:01:01.220]   which is the difference between human genome
[01:01:01.220 --> 01:01:03.420]   and chimps or whatever.
[01:01:03.420 --> 01:01:06.900]   So it can't be that complicated.
[01:01:06.900 --> 01:01:08.020]   It can't be that fundamental.
[01:01:08.020 --> 01:01:10.860]   I mean, most of the complicated stuff
[01:01:10.860 --> 01:01:12.460]   already exists in cats and dogs
[01:01:12.460 --> 01:01:15.800]   and certainly primates, non-human primates.
[01:01:15.800 --> 01:01:18.620]   - Yeah, that little thing with humans
[01:01:18.620 --> 01:01:22.420]   might be just something about social interaction
[01:01:22.420 --> 01:01:23.940]   and ability to maintain ideas
[01:01:23.940 --> 01:01:28.100]   across a collective of people.
[01:01:28.100 --> 01:01:30.800]   It sounds very dramatic and very impressive,
[01:01:30.800 --> 01:01:33.340]   but it probably isn't, mechanistically speaking.
[01:01:33.340 --> 01:01:34.660]   - It is, but we're not there yet.
[01:01:34.660 --> 01:01:37.300]   Like, we have, I mean, this is numbers.
[01:01:37.300 --> 01:01:42.300]   - Number 634 in the list of problems we have to solve.
[01:01:42.300 --> 01:01:43.380]   (laughs)
[01:01:43.380 --> 01:01:46.860]   - So basic physics of the world is number one.
[01:01:46.860 --> 01:01:51.580]   What do you, just a quick tangent on data augmentation.
[01:01:51.580 --> 01:01:56.580]   So a lot of it is hard-coded versus learned.
[01:01:56.580 --> 01:02:00.940]   Do you have any intuition that maybe
[01:02:00.940 --> 01:02:03.580]   there could be some weird data augmentation,
[01:02:03.580 --> 01:02:06.180]   like generative type of data augmentation,
[01:02:06.180 --> 01:02:07.660]   like doing something weird to images,
[01:02:07.660 --> 01:02:12.660]   which then improves the similarity learning process?
[01:02:12.660 --> 01:02:16.260]   So not just kind of dumb, simple distortions,
[01:02:16.260 --> 01:02:18.100]   but by you shaking your head,
[01:02:18.100 --> 01:02:20.900]   just saying that even simple distortions are enough.
[01:02:20.900 --> 01:02:22.780]   - I think, no, I think data augmentation
[01:02:22.780 --> 01:02:25.080]   is a temporary, necessary evil.
[01:02:25.080 --> 01:02:28.880]   So what people are working on now is two things.
[01:02:28.880 --> 01:02:32.220]   One is the type of self-supervised learning,
[01:02:32.220 --> 01:02:35.460]   like trying to translate the type of self-supervised learning
[01:02:35.460 --> 01:02:38.660]   people use in language, translating these two images,
[01:02:38.660 --> 01:02:41.820]   which is basically a denoising autoencoder method, right?
[01:02:41.820 --> 01:02:46.820]   So you take an image, you block, you mask some parts of it,
[01:02:46.820 --> 01:02:49.500]   and then you train some giant neural net
[01:02:49.500 --> 01:02:52.660]   to reconstruct the parts that are missing.
[01:02:52.660 --> 01:02:56.220]   And until very recently,
[01:02:56.220 --> 01:02:59.140]   there was no working methods for that.
[01:02:59.140 --> 01:03:01.620]   All the autoencoder type methods for images
[01:03:01.620 --> 01:03:03.740]   weren't producing very good representation,
[01:03:03.740 --> 01:03:06.620]   but there's a paper now coming out of the FAIR group
[01:03:06.620 --> 01:03:08.980]   at MNLO Park that actually works very well.
[01:03:08.980 --> 01:03:12.140]   So that doesn't require data augmentation,
[01:03:12.140 --> 01:03:14.460]   that requires only masking.
[01:03:14.460 --> 01:03:15.300]   Okay.
[01:03:15.300 --> 01:03:17.180]   - Only masking for images.
[01:03:17.180 --> 01:03:19.060]   Okay.
[01:03:19.060 --> 01:03:20.300]   - Right, so you mask part of the image
[01:03:20.300 --> 01:03:23.060]   and you train a system, which, you know,
[01:03:23.060 --> 01:03:26.620]   in this case is a transformer because you can,
[01:03:26.620 --> 01:03:28.380]   the transformer represents the image
[01:03:28.380 --> 01:03:30.860]   as non-overlapping patches,
[01:03:30.860 --> 01:03:33.260]   so it's easy to mask patches and things like that.
[01:03:33.260 --> 01:03:35.620]   - Okay, then my question transfers to that problem,
[01:03:35.620 --> 01:03:38.740]   the masking, like why should the mask be a square
[01:03:38.740 --> 01:03:40.060]   or a rectangle?
[01:03:40.060 --> 01:03:41.580]   - So it doesn't matter, like, you know,
[01:03:41.580 --> 01:03:44.300]   I think we're gonna come up probably in the future
[01:03:44.300 --> 01:03:48.540]   with sort of, you know, ways to mask that are, you know,
[01:03:48.540 --> 01:03:51.140]   kind of random, essentially.
[01:03:51.140 --> 01:03:52.860]   I mean, they are random already, but-
[01:03:52.860 --> 01:03:55.820]   - No, no, but like something that's challenging,
[01:03:55.820 --> 01:03:59.380]   like optimally challenging.
[01:03:59.380 --> 01:04:02.460]   So like, I mean, maybe it's a metaphor that doesn't apply,
[01:04:02.460 --> 01:04:06.420]   but you're, it seems like there's a data augmentation
[01:04:06.420 --> 01:04:09.860]   or masking, there's an interactive element with it.
[01:04:09.860 --> 01:04:11.980]   Like, you're almost like playing with an image.
[01:04:11.980 --> 01:04:12.820]   - Yeah.
[01:04:12.820 --> 01:04:14.180]   - And like, it's like the way we play
[01:04:14.180 --> 01:04:15.660]   with an image in our minds.
[01:04:15.660 --> 01:04:16.700]   - No, but it's like dropout.
[01:04:16.700 --> 01:04:18.300]   It's like Boson machine training.
[01:04:18.300 --> 01:04:23.180]   You know, every time you see a percept,
[01:04:23.180 --> 01:04:26.820]   you also, you can perturb it in some way.
[01:04:26.820 --> 01:04:31.500]   And then the principle of the training procedure
[01:04:31.500 --> 01:04:33.580]   is to minimize the difference of the output
[01:04:33.580 --> 01:04:36.900]   or the representation between the clean version
[01:04:36.900 --> 01:04:40.260]   and the corrupted version, essentially, right?
[01:04:40.260 --> 01:04:42.020]   And you can do this in real time, right?
[01:04:42.020 --> 01:04:44.220]   So, you know, Boson machine work like this, right?
[01:04:44.220 --> 01:04:47.420]   You show a percept, you tell the machine
[01:04:47.420 --> 01:04:49.820]   that's a good combination of activities
[01:04:49.820 --> 01:04:50.900]   or your input neurons.
[01:04:50.900 --> 01:04:57.020]   And then you either let them go their merry way
[01:04:57.020 --> 01:05:00.060]   without clamping them to values,
[01:05:00.060 --> 01:05:02.380]   or you only do this with a subset.
[01:05:02.380 --> 01:05:04.620]   And what you're doing is you're training the system
[01:05:04.620 --> 01:05:07.980]   so that the stable state of the entire network
[01:05:07.980 --> 01:05:10.660]   is the same regardless of whether it sees the entire input
[01:05:10.660 --> 01:05:12.460]   or whether it sees only part of it.
[01:05:12.460 --> 01:05:15.380]   You know, denoising autoencoder method
[01:05:15.380 --> 01:05:16.940]   is basically the same thing, right?
[01:05:16.940 --> 01:05:19.540]   You're training a system to reproduce the input,
[01:05:19.540 --> 01:05:21.820]   the complete input and filling the blanks,
[01:05:21.820 --> 01:05:24.060]   regardless of which parts are missing.
[01:05:24.060 --> 01:05:26.220]   And that's really the underlying principle.
[01:05:26.220 --> 01:05:28.260]   And you could imagine sort of even in the brain,
[01:05:28.260 --> 01:05:30.700]   some sort of neural principle where, you know,
[01:05:30.700 --> 01:05:32.780]   neurons kind of oscillate, right?
[01:05:32.780 --> 01:05:35.460]   So they take their activity and then temporarily
[01:05:35.460 --> 01:05:37.980]   they kind of shut off to, you know,
[01:05:37.980 --> 01:05:42.100]   force the rest of the system to basically reconstruct
[01:05:42.100 --> 01:05:44.780]   the input without their help, you know?
[01:05:44.780 --> 01:05:49.020]   And I mean, you could imagine, you know,
[01:05:49.020 --> 01:05:51.060]   more or less biologically possible processes.
[01:05:51.060 --> 01:05:51.900]   - Something like that.
[01:05:51.900 --> 01:05:54.940]   And I guess with this denoising autoencoder
[01:05:54.940 --> 01:05:58.700]   and masking and data augmentation,
[01:05:58.700 --> 01:06:01.140]   you don't have to worry about being super efficient.
[01:06:01.140 --> 01:06:03.980]   You can just do as much as you want
[01:06:03.980 --> 01:06:06.180]   and get better over time.
[01:06:06.180 --> 01:06:08.780]   'Cause I was thinking like you might wanna be clever
[01:06:08.780 --> 01:06:12.020]   about the way you do all of these procedures, you know,
[01:06:12.020 --> 01:06:16.740]   but that's only, it's somehow costly to do every iteration,
[01:06:16.740 --> 01:06:17.940]   but it's not really.
[01:06:17.940 --> 01:06:20.300]   - Not really, maybe.
[01:06:20.300 --> 01:06:21.500]   And then there is, you know,
[01:06:21.500 --> 01:06:24.180]   data augmentation without explicit data augmentation.
[01:06:24.180 --> 01:06:25.580]   Is data augmentation by weighting,
[01:06:25.580 --> 01:06:28.100]   which is, you know, the sort of video prediction.
[01:06:28.100 --> 01:06:31.500]   You're observing a video clip,
[01:06:31.500 --> 01:06:35.940]   observing the, you know, the continuation of that video clip
[01:06:35.940 --> 01:06:38.060]   and you try to learn a representation
[01:06:38.060 --> 01:06:40.260]   using the joint embedding architectures
[01:06:40.260 --> 01:06:43.300]   in such a way that the representation of the future clip
[01:06:43.300 --> 01:06:45.660]   is easily predictable from the representation
[01:06:45.660 --> 01:06:47.380]   of the observed clip.
[01:06:47.380 --> 01:06:51.860]   - Do you think YouTube has enough raw data
[01:06:52.740 --> 01:06:56.420]   from which to learn how to be a cat?
[01:06:56.420 --> 01:06:57.780]   - I think so.
[01:06:57.780 --> 01:07:01.220]   - So the amount of data is not the constraint.
[01:07:01.220 --> 01:07:04.140]   - No, it would require some selection, I think.
[01:07:04.140 --> 01:07:05.420]   - Some selection?
[01:07:05.420 --> 01:07:07.060]   - Some selection of, you know,
[01:07:07.060 --> 01:07:08.460]   maybe the right type of data.
[01:07:08.460 --> 01:07:11.100]   - Don't go down the rabbit hole of just cat videos.
[01:07:11.100 --> 01:07:14.580]   I might, you might need to watch some lectures or something.
[01:07:14.580 --> 01:07:15.700]   - No.
[01:07:15.700 --> 01:07:17.500]   - How meta would that be?
[01:07:17.500 --> 01:07:21.380]   If it like watches lectures about intelligence
[01:07:21.380 --> 01:07:24.300]   and then learns, watches your lectures on NYU
[01:07:24.300 --> 01:07:26.220]   and learns from that how to be intelligent.
[01:07:26.220 --> 01:07:27.860]   - I don't think there'd be enough.
[01:07:27.860 --> 01:07:33.220]   - What's your, do you find multimodal learning interesting?
[01:07:33.220 --> 01:07:35.060]   We've been talking about visual language,
[01:07:35.060 --> 01:07:36.460]   like combining those together,
[01:07:36.460 --> 01:07:38.140]   maybe audio, all those kinds of things.
[01:07:38.140 --> 01:07:40.380]   - There's a lot of things that I find interesting
[01:07:40.380 --> 01:07:43.260]   in the short term, but are not addressing
[01:07:43.260 --> 01:07:45.220]   the important problem that I think are really
[01:07:45.220 --> 01:07:46.660]   kind of the big challenges.
[01:07:46.660 --> 01:07:48.940]   So I think, you know, things like multitask learning,
[01:07:48.940 --> 01:07:53.940]   continual learning, you know, adversarial issues.
[01:07:53.940 --> 01:07:57.020]   I mean, those have, you know, great practical interests
[01:07:57.020 --> 01:08:00.300]   in the relatively short term possibly,
[01:08:00.300 --> 01:08:01.460]   but I don't think they're fundamental, you know,
[01:08:01.460 --> 01:08:04.380]   active learning, even to some extent reinforcement learning.
[01:08:04.380 --> 01:08:07.940]   I think those things will become either obsolete
[01:08:07.940 --> 01:08:12.940]   or useless or easy once we figured out
[01:08:12.940 --> 01:08:15.900]   how to do self-supervised representation learning
[01:08:15.900 --> 01:08:19.300]   or learning predictive world models.
[01:08:19.300 --> 01:08:21.540]   And so I think that's what, you know,
[01:08:21.540 --> 01:08:24.420]   the entire community should be focusing on.
[01:08:24.420 --> 01:08:25.460]   At least people who are interested
[01:08:25.460 --> 01:08:27.220]   in sort of fundamental questions or, you know,
[01:08:27.220 --> 01:08:29.540]   really kind of pushing the envelope of AI
[01:08:29.540 --> 01:08:31.460]   towards the next stage.
[01:08:31.460 --> 01:08:33.340]   But of course there's like a huge amount of, you know,
[01:08:33.340 --> 01:08:35.860]   very interesting work to do in sort of practical questions
[01:08:35.860 --> 01:08:38.020]   that have, you know, short term impact.
[01:08:38.020 --> 01:08:41.300]   - Well, you know, it's difficult to talk about
[01:08:41.300 --> 01:08:44.260]   the temporal scale because all of human civilization
[01:08:44.260 --> 01:08:48.580]   will eventually be destroyed because the sun will die out.
[01:08:48.580 --> 01:08:50.300]   And even if Elon Musk is successful
[01:08:50.300 --> 01:08:54.620]   in multi-planetary colonization across the galaxy,
[01:08:54.620 --> 01:08:56.620]   eventually the entirety of it
[01:08:56.620 --> 01:08:58.980]   will just become giant black holes.
[01:08:58.980 --> 01:09:00.820]   And that's gonna keep the universe.
[01:09:00.820 --> 01:09:02.140]   - That's gonna take a while, though.
[01:09:02.140 --> 01:09:04.860]   - So, but what I'm saying is then that logic
[01:09:04.860 --> 01:09:07.420]   can be used to say it's all meaningless.
[01:09:07.420 --> 01:09:10.940]   I'm saying all that to say that multitask learning
[01:09:11.900 --> 01:09:16.220]   might be, you're calling it practical or pragmatic
[01:09:16.220 --> 01:09:18.340]   or whatever, that might be the thing
[01:09:18.340 --> 01:09:21.140]   that achieves something very akin to intelligence
[01:09:21.140 --> 01:09:26.940]   while we're trying to solve the more general problem
[01:09:26.940 --> 01:09:29.460]   of self-supervised learning of background knowledge.
[01:09:29.460 --> 01:09:30.660]   So the reason I bring that up,
[01:09:30.660 --> 01:09:33.080]   maybe one way to ask that question.
[01:09:33.080 --> 01:09:34.740]   I've been very impressed by what
[01:09:34.740 --> 01:09:36.460]   Tesla Autopilot team is doing.
[01:09:36.460 --> 01:09:38.340]   I don't know if you've gotten a chance to glance
[01:09:38.340 --> 01:09:42.140]   at this particular one example of multitask learning
[01:09:42.140 --> 01:09:45.000]   where they're literally taking the problem,
[01:09:45.000 --> 01:09:48.940]   like, I don't know, Charles Darwin studying animals.
[01:09:48.940 --> 01:09:52.100]   They're studying the problem of driving and asking,
[01:09:52.100 --> 01:09:55.020]   okay, what are all the things you have to perceive?
[01:09:55.020 --> 01:09:57.860]   And the way they're solving it is, one,
[01:09:57.860 --> 01:10:00.420]   there's an ontology where you're bringing that to the table.
[01:10:00.420 --> 01:10:02.300]   So you're formulating a bunch of different tasks.
[01:10:02.300 --> 01:10:04.260]   It's like over 100 tasks or something like that
[01:10:04.260 --> 01:10:06.060]   that they're involved in driving.
[01:10:06.060 --> 01:10:07.740]   And then they're deploying it
[01:10:07.740 --> 01:10:10.580]   and then getting data back from people that run into trouble
[01:10:10.580 --> 01:10:12.700]   and they're trying to figure out, do we add tasks?
[01:10:12.700 --> 01:10:15.900]   Do we, like, we focus on each individual task separately.
[01:10:15.900 --> 01:10:17.140]   - Sure. - In fact, half,
[01:10:17.140 --> 01:10:20.020]   so I would say, I'll classify Andrej Karpathy's talk
[01:10:20.020 --> 01:10:20.860]   in two ways.
[01:10:20.860 --> 01:10:23.140]   So one was about doors and the other one
[01:10:23.140 --> 01:10:24.740]   about how much ImageNet sucks.
[01:10:24.740 --> 01:10:28.600]   He kept going back and forth on those two topics,
[01:10:28.600 --> 01:10:30.060]   which ImageNet sucks,
[01:10:30.060 --> 01:10:33.060]   meaning you can't just use a single benchmark.
[01:10:33.060 --> 01:10:36.060]   There's so, like, you have to have, like,
[01:10:36.060 --> 01:10:38.460]   a giant suite of benchmarks to understand
[01:10:38.460 --> 01:10:40.020]   how well your system actually works.
[01:10:40.020 --> 01:10:40.860]   - Oh, I agree with him.
[01:10:40.860 --> 01:10:42.980]   I mean, he's a very sensible guy.
[01:10:42.980 --> 01:10:47.620]   Now, okay, it's very clear that if you're faced
[01:10:47.620 --> 01:10:50.500]   with an engineering problem that you need to solve
[01:10:50.500 --> 01:10:51.940]   in a relatively short time,
[01:10:51.940 --> 01:10:55.900]   particularly if you have Elon Musk breathing down your neck,
[01:10:55.900 --> 01:10:57.380]   you're going to have to take shortcuts, right?
[01:10:57.380 --> 01:11:02.380]   You might think about the fact that the right thing to do
[01:11:02.380 --> 01:11:04.540]   and the long-term solution involves, you know,
[01:11:04.540 --> 01:11:06.580]   some fancy self-supervisioning,
[01:11:06.580 --> 01:11:10.260]   but you have Elon Musk breathing down your neck,
[01:11:10.260 --> 01:11:13.620]   and this involves human lives,
[01:11:13.620 --> 01:11:17.380]   and so you have to basically just do
[01:11:17.380 --> 01:11:22.380]   the systematic engineering and fine-tuning and refinements
[01:11:22.380 --> 01:11:26.380]   and trial and error and all that stuff.
[01:11:26.380 --> 01:11:27.460]   There's nothing wrong with that.
[01:11:27.460 --> 01:11:28.620]   That's called engineering.
[01:11:28.620 --> 01:11:32.100]   That's called putting technology out
[01:11:34.420 --> 01:11:38.620]   in the world, and you have to kind of ironclad it
[01:11:38.620 --> 01:11:40.460]   before you do this, you know,
[01:11:40.460 --> 01:11:46.260]   so much for, you know, grand ideas and principles.
[01:11:46.260 --> 01:11:50.740]   But, you know, I'm placing myself sort of, you know,
[01:11:50.740 --> 01:11:54.500]   some, you know, upstream of this,
[01:11:54.500 --> 01:11:55.780]   quite a bit upstream of this.
[01:11:55.780 --> 01:11:58.260]   - Your Plato, think about platonic forms.
[01:11:58.260 --> 01:11:59.900]   You're- - It's not platonic,
[01:11:59.900 --> 01:12:03.100]   because eventually I want that stuff to get used,
[01:12:03.100 --> 01:12:06.900]   but it's okay if it takes five or 10 years
[01:12:06.900 --> 01:12:09.300]   for the community to realize this is the right thing to do.
[01:12:09.300 --> 01:12:11.260]   I've done this before.
[01:12:11.260 --> 01:12:13.220]   It's been the case before that, you know,
[01:12:13.220 --> 01:12:14.420]   I've made that case.
[01:12:14.420 --> 01:12:17.740]   I mean, if you look back in the mid-2000s, for example,
[01:12:17.740 --> 01:12:18.980]   and you ask yourself the question,
[01:12:18.980 --> 01:12:22.060]   okay, I want to recognize cars or faces or whatever,
[01:12:22.060 --> 01:12:25.580]   you know, I can use convolutional nets,
[01:12:25.580 --> 01:12:28.380]   so I can use sort of more conventional
[01:12:28.380 --> 01:12:29.900]   kind of computer vision techniques, you know,
[01:12:29.900 --> 01:12:32.580]   using interest point detectors or SIFT,
[01:12:32.580 --> 01:12:34.300]   dense SIFT features and, you know,
[01:12:34.300 --> 01:12:35.740]   sticking an SVM on top.
[01:12:35.740 --> 01:12:37.820]   At that time, the datasets were so small
[01:12:37.820 --> 01:12:41.940]   that those methods that use more hand engineering
[01:12:41.940 --> 01:12:43.580]   worked better than conv nets.
[01:12:43.580 --> 01:12:45.540]   There was just not enough data for conv nets,
[01:12:45.540 --> 01:12:47.860]   and conv nets were a little slow
[01:12:47.860 --> 01:12:50.820]   with the kind of hardware that was available at the time.
[01:12:50.820 --> 01:12:55.580]   And there was a sea change when, basically when, you know,
[01:12:55.580 --> 01:12:58.580]   datasets became bigger and GPUs became available.
[01:12:58.580 --> 01:13:02.900]   That's what, you know, two of the main factors
[01:13:02.900 --> 01:13:05.900]   that basically made people change their mind.
[01:13:05.900 --> 01:13:11.820]   And you can look at the history of,
[01:13:11.820 --> 01:13:15.500]   like all sub branches of AI or pattern recognition,
[01:13:15.500 --> 01:13:19.740]   and there's a similar trajectory followed by techniques
[01:13:19.740 --> 01:13:22.220]   where people start by, you know,
[01:13:22.220 --> 01:13:23.780]   engineering the hell out of it.
[01:13:25.180 --> 01:13:29.180]   You know, be it optical character recognition,
[01:13:29.180 --> 01:13:31.740]   speech recognition, computer vision,
[01:13:31.740 --> 01:13:34.260]   like image recognition in general,
[01:13:34.260 --> 01:13:35.980]   natural language understanding, like, you know,
[01:13:35.980 --> 01:13:37.980]   translation, things like that, right?
[01:13:37.980 --> 01:13:39.980]   You start to engineer the hell out of it.
[01:13:39.980 --> 01:13:42.700]   You start to acquire all the knowledge,
[01:13:42.700 --> 01:13:44.780]   the prior knowledge you know about image formation,
[01:13:44.780 --> 01:13:46.620]   about, you know, the shape of characters,
[01:13:46.620 --> 01:13:49.580]   about, you know, morphological operations,
[01:13:49.580 --> 01:13:52.420]   about like feature extraction, Fourier transforms,
[01:13:52.420 --> 01:13:54.500]   you know, Wernicke moments, you know, whatever, right?
[01:13:54.500 --> 01:13:56.300]   People have come up with thousands of ways
[01:13:56.300 --> 01:13:58.620]   of representing images so that they could be
[01:13:58.620 --> 01:14:01.620]   easily classified afterwards.
[01:14:01.620 --> 01:14:03.020]   Same for speech recognition, right?
[01:14:03.020 --> 01:14:05.020]   There is, you know, it took decades for people
[01:14:05.020 --> 01:14:07.940]   to figure out a good front end to pre-process
[01:14:07.940 --> 01:14:10.540]   a speech signal so that, you know,
[01:14:10.540 --> 01:14:13.420]   the information about what is being said is preserved,
[01:14:13.420 --> 01:14:15.940]   but most of the information about the identity
[01:14:15.940 --> 01:14:17.060]   of the speaker is gone.
[01:14:17.060 --> 01:14:21.940]   You know, Kestrel coefficients or whatever, right?
[01:14:21.940 --> 01:14:23.860]   And same for text, right?
[01:14:24.540 --> 01:14:27.460]   You do name entity recognition and you parse
[01:14:27.460 --> 01:14:32.460]   and you do tagging of the parts of speech.
[01:14:32.460 --> 01:14:35.580]   And, you know, you do this sort of tree representation
[01:14:35.580 --> 01:14:37.500]   of clauses and all that stuff, right,
[01:14:37.500 --> 01:14:39.180]   before you can do anything.
[01:14:39.180 --> 01:14:44.620]   So that's how it starts, right?
[01:14:44.620 --> 01:14:46.300]   Just engineer the hell out of it.
[01:14:46.300 --> 01:14:49.020]   And then you start having data
[01:14:49.020 --> 01:14:51.260]   and maybe you have more powerful computers,
[01:14:51.260 --> 01:14:53.460]   maybe you know something about statistical learning.
[01:14:53.460 --> 01:14:54.660]   So you start using machine learning
[01:14:54.660 --> 01:14:56.740]   and it's usually a small sliver on top of your
[01:14:56.740 --> 01:14:58.300]   kind of handcrafted system where, you know,
[01:14:58.300 --> 01:15:00.580]   you extract features by hand.
[01:15:00.580 --> 01:15:02.580]   Okay, and now, you know, nowadays,
[01:15:02.580 --> 01:15:04.100]   the standard way of doing this is that
[01:15:04.100 --> 01:15:05.380]   you train the entire thing end to end
[01:15:05.380 --> 01:15:07.740]   with a deep learning system and it learns its own features
[01:15:07.740 --> 01:15:11.940]   and, you know, speech recognition systems nowadays,
[01:15:11.940 --> 01:15:13.900]   OCR systems, are completely end to end.
[01:15:13.900 --> 01:15:16.380]   It's, you know, it's some giant neural net
[01:15:16.380 --> 01:15:19.940]   that takes raw waveforms and produces a sequence
[01:15:19.940 --> 01:15:21.380]   of characters coming out.
[01:15:21.380 --> 01:15:23.020]   And it's just a huge neural net, right?
[01:15:23.020 --> 01:15:24.940]   There's no, in a Markov model,
[01:15:24.940 --> 01:15:27.380]   there's no language model that is explicit
[01:15:27.380 --> 01:15:29.540]   other than, you know, something that's ingrained
[01:15:29.540 --> 01:15:31.900]   in the sort of neural language model, if you want.
[01:15:31.900 --> 01:15:34.340]   Same for translation, same for all kinds of stuff.
[01:15:34.340 --> 01:15:37.380]   So you see this continuous evolution
[01:15:37.380 --> 01:15:41.340]   from, you know, less and less handcrafting
[01:15:41.340 --> 01:15:42.700]   and more and more learning.
[01:15:42.700 --> 01:15:49.260]   And I think, I mean, it's true in biology as well.
[01:15:49.260 --> 01:15:52.860]   - So, I mean, we might disagree about this.
[01:15:52.860 --> 01:15:54.020]   Maybe not.
[01:15:54.020 --> 01:15:56.860]   In this one little piece at the end,
[01:15:56.860 --> 01:15:58.340]   you mentioned active learning.
[01:15:58.340 --> 01:16:01.460]   It feels like active learning,
[01:16:01.460 --> 01:16:04.700]   which is the selection of data and also the interactivity,
[01:16:04.700 --> 01:16:06.780]   needs to be part of this giant neural network.
[01:16:06.780 --> 01:16:08.340]   You cannot just be an observer
[01:16:08.340 --> 01:16:09.700]   to do self-supervised learning.
[01:16:09.700 --> 01:16:12.180]   You have to, well, I don't,
[01:16:12.180 --> 01:16:14.540]   self-supervised learning is just a word,
[01:16:14.540 --> 01:16:16.740]   but I would, whatever this giant stack
[01:16:16.740 --> 01:16:19.620]   of a neural network that's automatically learning,
[01:16:19.620 --> 01:16:24.620]   it feels, my intuition is that you have to have a system,
[01:16:24.620 --> 01:16:30.180]   whether it's a physical robot or a digital robot
[01:16:30.180 --> 01:16:32.300]   that's interacting with the world
[01:16:32.300 --> 01:16:35.900]   and doing so in a flawed way and improving over time
[01:16:35.900 --> 01:16:41.820]   in order to form the self-supervised learning well.
[01:16:41.820 --> 01:16:44.940]   You can't just give it a giant sea of data.
[01:16:44.940 --> 01:16:47.060]   - Okay, I agree and I disagree.
[01:16:47.060 --> 01:16:52.060]   I agree in the sense that I think, I agree in two ways.
[01:16:52.060 --> 01:16:55.100]   The first way I agree is that if you want,
[01:16:55.100 --> 01:16:57.420]   and you certainly need a causal model of the world
[01:16:57.420 --> 01:17:00.460]   that allows you to predict the consequences of your actions,
[01:17:00.460 --> 01:17:02.740]   to train that model, you need to take actions.
[01:17:02.740 --> 01:17:06.100]   You need to be able to act in a world and see the effect
[01:17:06.100 --> 01:17:08.420]   for you to learn causal models of the world.
[01:17:08.420 --> 01:17:11.500]   - So, that's not obvious because you can observe others.
[01:17:11.500 --> 01:17:12.340]   - You can observe others.
[01:17:12.340 --> 01:17:14.660]   - And you can infer that they're similar to you
[01:17:14.660 --> 01:17:15.900]   and then you can learn from that.
[01:17:15.900 --> 01:17:18.340]   - Yeah, but then you have to kind of hardware that part,
[01:17:18.340 --> 01:17:19.820]   right, and then you don't mirror neurons
[01:17:19.820 --> 01:17:20.660]   and all that stuff, right?
[01:17:20.660 --> 01:17:23.220]   So, and it's not clear to me
[01:17:23.220 --> 01:17:24.380]   how you would do this in a machine.
[01:17:24.380 --> 01:17:29.380]   So, I think the action part would be necessary
[01:17:29.380 --> 01:17:32.580]   for having causal models of the world.
[01:17:32.580 --> 01:17:36.660]   The second reason it may be necessary
[01:17:36.660 --> 01:17:40.580]   or at least more efficient is that active learning
[01:17:40.580 --> 01:17:44.900]   basically goes for the jiggler of what you don't know, right?
[01:17:44.900 --> 01:17:49.900]   There's obvious areas of uncertainty about your world
[01:17:49.900 --> 01:17:52.980]   and about how the world behaves.
[01:17:52.980 --> 01:17:56.220]   And you can resolve this uncertainty
[01:17:56.220 --> 01:18:00.300]   by systematic exploration of that part that you don't know.
[01:18:00.300 --> 01:18:01.700]   And if you know that you don't know,
[01:18:01.700 --> 01:18:03.020]   then it makes you curious.
[01:18:03.020 --> 01:18:05.580]   You kind of look into situations that...
[01:18:05.580 --> 01:18:09.260]   And across the animal world,
[01:18:09.260 --> 01:18:13.740]   different species are different levels of curiosity, right?
[01:18:13.740 --> 01:18:15.100]   Depending on how they're built, right?
[01:18:15.100 --> 01:18:18.740]   So, cats and rats are incredibly curious,
[01:18:18.740 --> 01:18:20.620]   dogs not so much, I mean, less.
[01:18:20.620 --> 01:18:22.100]   - Yeah, so it could be useful
[01:18:22.100 --> 01:18:23.900]   to have that kind of curiosity.
[01:18:23.900 --> 01:18:24.740]   - So, it'd be useful,
[01:18:24.740 --> 01:18:26.980]   but curiosity just makes the process faster.
[01:18:26.980 --> 01:18:28.780]   It doesn't make the process exist.
[01:18:28.780 --> 01:18:34.580]   So, what process, what learning process is it
[01:18:34.580 --> 01:18:38.660]   that active learning makes more efficient?
[01:18:38.660 --> 01:18:40.380]   And I'm asking that first question.
[01:18:43.380 --> 01:18:44.780]   We haven't answered that question yet.
[01:18:44.780 --> 01:18:48.100]   So, I worry about active learning once this question is...
[01:18:48.100 --> 01:18:50.820]   - So, it's the more fundamental question to ask.
[01:18:50.820 --> 01:18:54.580]   And if active learning or interaction
[01:18:54.580 --> 01:18:57.100]   increases the efficiency of the learning,
[01:18:57.100 --> 01:19:00.260]   see, sometimes it becomes very different
[01:19:00.260 --> 01:19:04.820]   if the increase is several orders of magnitude, right?
[01:19:04.820 --> 01:19:05.660]   - That's true.
[01:19:05.660 --> 01:19:08.100]   - But fundamentally, it's still the same thing
[01:19:08.100 --> 01:19:11.180]   in building up the intuition about how to,
[01:19:11.180 --> 01:19:12.420]   in a self-supervised way,
[01:19:12.420 --> 01:19:13.820]   to construct background models,
[01:19:13.820 --> 01:19:18.640]   efficient or inefficient, is the core problem.
[01:19:18.640 --> 01:19:20.820]   What do you think about Yoshua Bengio's
[01:19:20.820 --> 01:19:22.900]   talking about consciousness
[01:19:22.900 --> 01:19:24.540]   and all of these kinds of concepts?
[01:19:24.540 --> 01:19:29.540]   - Okay, I don't know what consciousness is, but...
[01:19:29.540 --> 01:19:31.980]   - It's a good opener.
[01:19:31.980 --> 01:19:33.580]   - And to some extent, a lot of the things
[01:19:33.580 --> 01:19:35.980]   that are said about consciousness remind me
[01:19:35.980 --> 01:19:38.740]   of the questions people were asking themselves
[01:19:38.740 --> 01:19:41.340]   in the 18th century or 17th century
[01:19:41.340 --> 01:19:44.100]   when they discovered that, you know,
[01:19:44.100 --> 01:19:46.300]   how the eye works and the fact that the image
[01:19:46.300 --> 01:19:49.900]   at the back of the eye was upside down, right?
[01:19:49.900 --> 01:19:51.180]   Because you have a lens.
[01:19:51.180 --> 01:19:53.420]   And so, on your retina, the image that forms
[01:19:53.420 --> 01:19:55.500]   is an image of the world, but it's upside down.
[01:19:55.500 --> 01:19:58.200]   How is it that you see right side up?
[01:19:58.200 --> 01:20:00.460]   And, you know, with what we know today in science,
[01:20:00.460 --> 01:20:03.860]   you know, we realize this question doesn't make any sense
[01:20:03.860 --> 01:20:06.340]   or is kind of ridiculous in some way, right?
[01:20:06.340 --> 01:20:08.180]   So, I think a lot of what is said about consciousness
[01:20:08.180 --> 01:20:09.020]   is of that nature.
[01:20:09.020 --> 01:20:10.980]   Now, that said, there's a lot of really smart people
[01:20:10.980 --> 01:20:13.820]   that, for whom I have a lot of respect,
[01:20:13.820 --> 01:20:15.060]   who are talking about this topic,
[01:20:15.060 --> 01:20:17.380]   people like David Chalmers, who is a colleague of mine
[01:20:17.380 --> 01:20:18.220]   at NYU.
[01:20:18.220 --> 01:20:23.760]   I have kind of an unorthodox, folk,
[01:20:23.760 --> 01:20:29.220]   speculative hypothesis about consciousness.
[01:20:29.220 --> 01:20:32.020]   So, we're talking about the study of a world model.
[01:20:32.020 --> 01:20:35.540]   And I think, you know, our entire prefrontal cortex
[01:20:35.540 --> 01:20:39.320]   basically is the engine for our world model.
[01:20:40.820 --> 01:20:44.580]   But when we are attending at a particular situation,
[01:20:44.580 --> 01:20:46.060]   we're focused on that situation.
[01:20:46.060 --> 01:20:48.580]   We basically cannot attend to anything else.
[01:20:48.580 --> 01:20:53.580]   And that seems to suggest that we basically have only one
[01:20:53.580 --> 01:20:58.400]   world model engine in our prefrontal cortex.
[01:20:58.400 --> 01:21:02.580]   That engine is configurable to the situation at hand.
[01:21:02.580 --> 01:21:04.620]   So, we are building a box out of wood,
[01:21:04.620 --> 01:21:08.340]   or we are, you know, driving down the highway,
[01:21:08.340 --> 01:21:09.260]   playing chess.
[01:21:09.340 --> 01:21:12.860]   We basically have a single model of the world
[01:21:12.860 --> 01:21:15.380]   that we're configuring to the situation at hand,
[01:21:15.380 --> 01:21:18.080]   which is why we can only attend to one task at a time.
[01:21:18.080 --> 01:21:21.700]   Now, if there is a task that we do repeatedly,
[01:21:21.700 --> 01:21:25.980]   it goes from the sort of deliberate reasoning
[01:21:25.980 --> 01:21:27.460]   using model of the world and prediction,
[01:21:27.460 --> 01:21:29.340]   and perhaps something like model predictive control,
[01:21:29.340 --> 01:21:31.420]   which I was talking about earlier,
[01:21:31.420 --> 01:21:33.380]   to something that is more subconscious
[01:21:33.380 --> 01:21:34.420]   that becomes automatic.
[01:21:34.420 --> 01:21:36.340]   So, I don't know if you've ever played against
[01:21:36.340 --> 01:21:37.960]   a chess grandmaster.
[01:21:38.980 --> 01:21:42.980]   You know, I get wiped out in 10 plies, right?
[01:21:42.980 --> 01:21:45.980]   And, you know, I have to think about my move
[01:21:45.980 --> 01:21:48.680]   for, you know, like 15 minutes.
[01:21:48.680 --> 01:21:52.300]   And the person in front of me, the grandmaster,
[01:21:52.300 --> 01:21:55.200]   you know, would just like react within seconds, right?
[01:21:55.200 --> 01:21:58.580]   You know, he doesn't need to think about it.
[01:21:58.580 --> 01:21:59.980]   That's become part of the subconscious,
[01:21:59.980 --> 01:22:02.620]   because, you know, it's basically just pattern recognition
[01:22:02.620 --> 01:22:03.460]   at this point.
[01:22:03.460 --> 01:22:07.660]   Same, you know, the first few hours you drive a car,
[01:22:07.660 --> 01:22:08.660]   you're really attentive.
[01:22:08.660 --> 01:22:09.660]   You can't do anything else.
[01:22:09.660 --> 01:22:13.180]   And then after 20, 30 hours of practice, 50 hours,
[01:22:13.180 --> 01:22:14.100]   you know, it's subconscious.
[01:22:14.100 --> 01:22:15.420]   You can talk to the person next to you,
[01:22:15.420 --> 01:22:17.060]   you know, things like that, right?
[01:22:17.060 --> 01:22:18.980]   Unless the situation becomes unpredictable,
[01:22:18.980 --> 01:22:21.020]   and then you have to stop talking.
[01:22:21.020 --> 01:22:23.780]   So, that suggests you only have one model in your head.
[01:22:23.780 --> 01:22:27.820]   And it might suggest the idea that consciousness
[01:22:27.820 --> 01:22:29.700]   basically is the module that configures
[01:22:29.700 --> 01:22:31.700]   this world model of yours.
[01:22:31.700 --> 01:22:35.260]   You know, you need to have some sort of executive
[01:22:35.260 --> 01:22:38.300]   kind of overseer that configures your world model
[01:22:38.300 --> 01:22:40.540]   for the situation at hand.
[01:22:40.540 --> 01:22:43.780]   And that leads to kind of the really curious concept
[01:22:43.780 --> 01:22:46.860]   that consciousness is not a consequence of the power
[01:22:46.860 --> 01:22:49.940]   of our minds, but of the limitation of our brains.
[01:22:49.940 --> 01:22:52.020]   But because we have only one world model,
[01:22:52.020 --> 01:22:53.660]   we have to be conscious.
[01:22:53.660 --> 01:22:57.620]   If we had as many world models as there are situations
[01:22:57.620 --> 01:23:00.740]   we encounter, then we could do all of them simultaneously,
[01:23:00.740 --> 01:23:02.940]   and we wouldn't need this sort of executive control
[01:23:02.940 --> 01:23:04.500]   that we call consciousness.
[01:23:04.500 --> 01:23:05.340]   - Yeah, interesting.
[01:23:05.340 --> 01:23:08.940]   And somehow maybe that executive controller,
[01:23:08.940 --> 01:23:10.980]   I mean, the hard problem of consciousness,
[01:23:10.980 --> 01:23:12.860]   there's some kind of chemicals in biology
[01:23:12.860 --> 01:23:15.940]   that's creating a feeling, like it feels
[01:23:15.940 --> 01:23:17.740]   to experience some of these things.
[01:23:17.740 --> 01:23:22.460]   That's kind of like the hard question is,
[01:23:22.460 --> 01:23:24.880]   what the heck is that, and why is that useful?
[01:23:24.880 --> 01:23:26.180]   Maybe the more pragmatic question,
[01:23:26.180 --> 01:23:29.940]   why is it useful to feel like this is really you
[01:23:29.940 --> 01:23:33.360]   experiencing this versus just like information
[01:23:33.360 --> 01:23:34.360]   being processed?
[01:23:35.360 --> 01:23:39.040]   - It could be just a very nice side effect
[01:23:39.040 --> 01:23:43.640]   of the way we evolved that it's just very useful
[01:23:43.640 --> 01:23:48.640]   to feel a sense of ownership to the decisions you make,
[01:23:48.640 --> 01:23:51.760]   to the perceptions you make, to the model
[01:23:51.760 --> 01:23:53.200]   you're trying to maintain.
[01:23:53.200 --> 01:23:56.280]   Like you own this thing, and it's the only one you got,
[01:23:56.280 --> 01:23:58.440]   and if you lose it, it's gonna really suck.
[01:23:58.440 --> 01:24:00.640]   And so you should really send the brain
[01:24:00.640 --> 01:24:02.300]   some signals about it.
[01:24:03.720 --> 01:24:06.840]   - What ideas do you believe might be true
[01:24:06.840 --> 01:24:10.080]   that most or at least many people disagree with you with,
[01:24:10.080 --> 01:24:13.760]   let's say in the space of machine learning?
[01:24:13.760 --> 01:24:14.920]   - Well, it depends who you talk about.
[01:24:14.920 --> 01:24:19.920]   But I think, so certainly there is a bunch of people
[01:24:19.920 --> 01:24:22.000]   who are nativist, right, who think that a lot
[01:24:22.000 --> 01:24:24.080]   of the basic things about the world are kind of hardwired
[01:24:24.080 --> 01:24:25.320]   in our minds.
[01:24:25.320 --> 01:24:28.880]   Things like the world is three-dimensional, for example.
[01:24:28.880 --> 01:24:30.400]   Is that hardwired?
[01:24:30.400 --> 01:24:33.080]   Things like object permanence, is it something
[01:24:33.080 --> 01:24:37.520]   that we learn before the age of three months or so,
[01:24:37.520 --> 01:24:39.360]   or are we born with it?
[01:24:39.360 --> 01:24:42.640]   And there are wide disagreements among
[01:24:42.640 --> 01:24:46.560]   the cognitive scientists for this.
[01:24:46.560 --> 01:24:49.040]   I think those things are actually very simple to learn.
[01:24:49.040 --> 01:24:54.240]   Is it the case that the oriented edge detectors in V1
[01:24:54.240 --> 01:24:56.160]   are learned, or are they hardwired?
[01:24:56.160 --> 01:24:57.280]   I think they are learned.
[01:24:57.280 --> 01:24:58.560]   They might be learned before birth,
[01:24:58.560 --> 01:25:00.600]   because it's really easy to generate signals
[01:25:00.600 --> 01:25:03.000]   from the retina that actually will train edge detectors.
[01:25:03.000 --> 01:25:06.760]   So, and again, those are things that can be learned
[01:25:06.760 --> 01:25:09.560]   within minutes of opening your eyes, right?
[01:25:09.560 --> 01:25:14.040]   I mean, since the 1990s, we have algorithms
[01:25:14.040 --> 01:25:15.440]   that can learn oriented edge detectors
[01:25:15.440 --> 01:25:17.840]   completely unsupervised with the equivalent
[01:25:17.840 --> 01:25:19.080]   of a few minutes of real time.
[01:25:19.080 --> 01:25:21.540]   So those things have to be learned.
[01:25:21.540 --> 01:25:26.160]   There's also those MIT experiments where you kind of plug
[01:25:26.160 --> 01:25:30.000]   the optical nerve on the auditory cortex of a baby ferret,
[01:25:30.000 --> 01:25:31.280]   right, and that auditory cortex
[01:25:31.280 --> 01:25:33.400]   becomes a visual cortex, essentially.
[01:25:33.400 --> 01:25:37.980]   So, you know, clearly there's learning taking place there.
[01:25:37.980 --> 01:25:40.680]   So, you know, I think a lot of what people think
[01:25:40.680 --> 01:25:43.160]   are so basic that they need to be hardwired,
[01:25:43.160 --> 01:25:44.440]   I think a lot of those things are learned
[01:25:44.440 --> 01:25:46.240]   because they are easy to learn.
[01:25:46.240 --> 01:25:49.960]   - So you put a lot of value in the power of learning.
[01:25:49.960 --> 01:25:53.340]   What kind of things do you suspect might not be learned?
[01:25:53.340 --> 01:25:56.040]   Is there something that could not be learned?
[01:25:56.040 --> 01:25:59.760]   - So your intrinsic drives are not learned.
[01:25:59.760 --> 01:26:03.440]   There are the things that, you know, make humans human
[01:26:03.440 --> 01:26:07.400]   or make, you know, cats different from dogs, right?
[01:26:07.400 --> 01:26:10.000]   It's the basic drives that are kind of hardwired
[01:26:10.000 --> 01:26:11.920]   in our basal ganglia.
[01:26:11.920 --> 01:26:14.000]   I mean, there are people who are working
[01:26:14.000 --> 01:26:15.040]   on this kind of stuff.
[01:26:15.040 --> 01:26:16.320]   It's called intrinsic motivation
[01:26:16.320 --> 01:26:18.160]   in the context of reinforcement learning.
[01:26:18.160 --> 01:26:20.040]   So these are objective functions
[01:26:20.040 --> 01:26:23.040]   where the reward doesn't come from the external world.
[01:26:23.040 --> 01:26:24.600]   It's computed by your own brain.
[01:26:24.600 --> 01:26:28.120]   Your own brain computes whether you're happy or not, right?
[01:26:28.120 --> 01:26:32.500]   It measures your degree of comfort or incomfort.
[01:26:32.500 --> 01:26:36.080]   And because it's your brain computing this,
[01:26:36.080 --> 01:26:37.760]   presumably it knows also how to estimate
[01:26:37.760 --> 01:26:38.760]   gradients of this, right?
[01:26:38.760 --> 01:26:43.760]   So it's easier to learn when your objective is intrinsic.
[01:26:43.760 --> 01:26:48.760]   So that has to be hardwired.
[01:26:48.760 --> 01:26:53.420]   The critic that makes long-term prediction of the outcome,
[01:26:53.420 --> 01:26:56.720]   which is the eventual result of this, that's learned.
[01:26:56.720 --> 01:26:59.020]   And perception is learned,
[01:26:59.020 --> 01:27:01.220]   and your model of the world is learned.
[01:27:01.220 --> 01:27:04.200]   But let me take an example of, you know, why the critic,
[01:27:04.200 --> 01:27:06.800]   I mean, an example of how the critic may be learned, right?
[01:27:06.800 --> 01:27:11.200]   If I come to you, you know, I reach across the table
[01:27:11.200 --> 01:27:13.320]   and I pinch your arm, right?
[01:27:13.320 --> 01:27:15.040]   Complete surprise for you.
[01:27:15.040 --> 01:27:15.880]   You would not have expected this from me.
[01:27:15.880 --> 01:27:18.040]   - I was expecting that the whole time, but yes, right.
[01:27:18.040 --> 01:27:20.360]   Let's say for the sake of the story, yes.
[01:27:21.720 --> 01:27:24.940]   - Okay, your basal ganglia is gonna light up
[01:27:24.940 --> 01:27:26.780]   'cause it's gonna hurt, right?
[01:27:26.780 --> 01:27:31.080]   And now your model of the world includes the fact that
[01:27:31.080 --> 01:27:33.160]   I may pinch you if I approach my-
[01:27:33.160 --> 01:27:36.180]   - Don't trust humans.
[01:27:36.180 --> 01:27:37.820]   - Right, my hand to your arm.
[01:27:37.820 --> 01:27:39.960]   So if I try again, you're gonna recoil.
[01:27:39.960 --> 01:27:44.020]   And that's your critic, your predictive,
[01:27:44.020 --> 01:27:47.800]   you know, your predictor of your ultimate pain system
[01:27:50.460 --> 01:27:52.320]   that predicts that something bad is gonna happen
[01:27:52.320 --> 01:27:53.760]   and you recoil to avoid it.
[01:27:53.760 --> 01:27:55.160]   - So even that can be learned.
[01:27:55.160 --> 01:27:56.600]   - That is learned, definitely.
[01:27:56.600 --> 01:27:59.320]   This is what allows you also to, you know,
[01:27:59.320 --> 01:28:00.600]   define sub goals, right?
[01:28:00.600 --> 01:28:04.440]   So the fact that, you know, you're a school child,
[01:28:04.440 --> 01:28:07.000]   you wake up in the morning and you go to school and,
[01:28:07.000 --> 01:28:11.640]   you know, it's not because you necessarily like waking up
[01:28:11.640 --> 01:28:12.720]   early and going to school,
[01:28:12.720 --> 01:28:14.640]   but you know that there is a long-term objective
[01:28:14.640 --> 01:28:15.840]   you're trying to optimize.
[01:28:15.840 --> 01:28:18.120]   - So Ernest Becker, I'm not sure if you're familiar
[01:28:18.120 --> 01:28:20.060]   with him, the philosopher, he wrote the book
[01:28:20.060 --> 01:28:22.180]   "Denial of Death" and his idea is that
[01:28:22.180 --> 01:28:24.460]   one of the core motivations of human beings
[01:28:24.460 --> 01:28:27.260]   are terror of death, are fear of death.
[01:28:27.260 --> 01:28:28.900]   That's what makes us unique from cats.
[01:28:28.900 --> 01:28:30.540]   Cats are just surviving.
[01:28:30.540 --> 01:28:35.540]   They do not have a deep, like a cognizance,
[01:28:35.540 --> 01:28:41.700]   introspection that over the horizon is the end.
[01:28:41.700 --> 01:28:43.020]   And he says that, I mean,
[01:28:43.020 --> 01:28:44.380]   there's a terror management theory
[01:28:44.380 --> 01:28:47.500]   that just all these psychological experiments that show,
[01:28:47.500 --> 01:28:52.500]   basically this idea that all of human civilization,
[01:28:52.500 --> 01:28:56.820]   everything we create is kind of trying to forget
[01:28:56.820 --> 01:29:00.620]   if even for a brief moment that we're going to die.
[01:29:00.620 --> 01:29:03.720]   When do you think humans understand
[01:29:03.720 --> 01:29:04.860]   that they're going to die?
[01:29:04.860 --> 01:29:07.460]   Is it learned early on also?
[01:29:07.460 --> 01:29:09.060]   Like?
[01:29:09.060 --> 01:29:12.500]   - I don't know at what point, I mean, it's a question,
[01:29:12.500 --> 01:29:14.620]   like, you know, at what point do you realize that,
[01:29:14.620 --> 01:29:16.460]   you know, what death really is?
[01:29:16.460 --> 01:29:18.220]   And I think most people don't actually realize
[01:29:18.220 --> 01:29:19.260]   what death is, right?
[01:29:19.260 --> 01:29:20.980]   I mean, most people believe that you go to heaven
[01:29:20.980 --> 01:29:21.900]   or something, right?
[01:29:21.900 --> 01:29:25.600]   - So to push back on that, what Ernest Becker says
[01:29:25.600 --> 01:29:29.340]   and Sheldon Solomon, all of those folks,
[01:29:29.340 --> 01:29:31.660]   and I find those ideas a little bit compelling
[01:29:31.660 --> 01:29:34.140]   is that there is moments in life, early in life,
[01:29:34.140 --> 01:29:36.540]   a lot of this fun happens early in life
[01:29:36.540 --> 01:29:41.540]   when you are, when you do deeply experience the terror
[01:29:41.540 --> 01:29:45.340]   of this realization and all the things you think about,
[01:29:45.340 --> 01:29:47.220]   about religion, all those kinds of things
[01:29:47.220 --> 01:29:49.620]   that we kind of think about more like teenage years
[01:29:49.620 --> 01:29:52.140]   and later, we're talking about way earlier.
[01:29:52.140 --> 01:29:53.220]   - No, it was like seven or eight years,
[01:29:53.220 --> 01:29:54.060]   something like that, yeah.
[01:29:54.060 --> 01:29:58.820]   - You realize, holy crap, this is,
[01:29:58.820 --> 01:30:00.700]   like the mystery, the terror, like,
[01:30:00.700 --> 01:30:03.240]   it's almost like you're a little prey,
[01:30:03.240 --> 01:30:05.380]   a little baby deer sitting in the darkness
[01:30:05.380 --> 01:30:08.060]   of the jungle, the woods, looking all around you,
[01:30:08.060 --> 01:30:09.580]   the darkness full of terror.
[01:30:09.580 --> 01:30:12.180]   I mean, that realization says, okay,
[01:30:12.180 --> 01:30:14.500]   I'm going to go back in the comfort of my mind
[01:30:14.500 --> 01:30:16.820]   where there is a deep meaning,
[01:30:16.820 --> 01:30:20.420]   where there is a, maybe like, pretend I'm immortal
[01:30:20.420 --> 01:30:25.060]   in however way, however kind of idea I can construct
[01:30:25.060 --> 01:30:27.180]   to help me understand that I'm immortal.
[01:30:27.180 --> 01:30:28.660]   Religion helps with that.
[01:30:28.660 --> 01:30:31.460]   You can delude yourself in all kinds of ways,
[01:30:31.460 --> 01:30:34.220]   like lose yourself in the busyness of each day,
[01:30:34.220 --> 01:30:36.380]   have little goals in mind, all those kinds of things
[01:30:36.380 --> 01:30:38.220]   to think that it's gonna go on forever,
[01:30:38.220 --> 01:30:40.780]   and you kind of know you're gonna die, yeah,
[01:30:40.780 --> 01:30:43.820]   and it's gonna be sad, but you don't really understand
[01:30:43.820 --> 01:30:45.140]   that you're going to die.
[01:30:45.140 --> 01:30:46.460]   And so that's their idea.
[01:30:46.460 --> 01:30:49.940]   And I find that compelling because it does seem
[01:30:49.940 --> 01:30:52.820]   to be a core unique aspect of human nature
[01:30:52.820 --> 01:30:55.180]   that we're able to think that we're going,
[01:30:55.180 --> 01:30:59.540]   we're able to really understand that this life is finite.
[01:30:59.540 --> 01:31:00.620]   That seems important.
[01:31:00.620 --> 01:31:02.280]   - There's a bunch of different things there.
[01:31:02.280 --> 01:31:03.660]   So first of all, I don't think there is
[01:31:03.660 --> 01:31:07.520]   a qualitative difference between us and cats in the term.
[01:31:07.520 --> 01:31:09.240]   I think the difference is that we just have
[01:31:09.240 --> 01:31:14.240]   a better long-term ability to predict in the long term,
[01:31:14.240 --> 01:31:17.380]   and so we have a better understanding of how the world works,
[01:31:17.380 --> 01:31:20.180]   so we have a better understanding of finiteness of life
[01:31:20.180 --> 01:31:21.020]   and things like that.
[01:31:21.020 --> 01:31:23.520]   - So we have a better planning engine than cats?
[01:31:23.520 --> 01:31:24.440]   - Yeah.
[01:31:24.440 --> 01:31:25.280]   - Okay.
[01:31:25.280 --> 01:31:28.840]   - But what's the motivation for planning that far?
[01:31:28.840 --> 01:31:30.160]   - Well, I think it's just a side effect
[01:31:30.160 --> 01:31:32.320]   of the fact that we have just a better planning engine
[01:31:32.320 --> 01:31:34.760]   because it makes us, as I said,
[01:31:34.760 --> 01:31:37.400]   the essence of intelligence is the ability to predict.
[01:31:37.400 --> 01:31:41.200]   And so because we're smarter, as a side effect,
[01:31:41.200 --> 01:31:43.480]   we also have this ability to kind of make predictions
[01:31:43.480 --> 01:31:47.560]   about our own future existence or lack thereof.
[01:31:47.560 --> 01:31:48.480]   - Okay.
[01:31:48.480 --> 01:31:50.520]   - You say religion helps with that.
[01:31:50.520 --> 01:31:52.960]   I think religion hurts, actually.
[01:31:52.960 --> 01:31:55.600]   It makes people worry about what's gonna happen
[01:31:55.600 --> 01:31:57.480]   after their death, et cetera.
[01:31:57.480 --> 01:32:01.160]   If you believe that you just don't exist after death,
[01:32:01.160 --> 01:32:02.920]   it solves completely the problem, at least.
[01:32:02.920 --> 01:32:04.960]   - You're saying if you don't believe in God,
[01:32:04.960 --> 01:32:07.200]   you don't worry about what happens after death?
[01:32:07.200 --> 01:32:08.240]   - Yeah.
[01:32:08.240 --> 01:32:09.080]   - I don't know.
[01:32:09.080 --> 01:32:11.880]   - You only worry about this life
[01:32:11.880 --> 01:32:14.240]   because that's the only one you have.
[01:32:14.240 --> 01:32:16.120]   - I think it's, well, I don't know.
[01:32:16.120 --> 01:32:17.760]   If I were to say what Ernest Becker says,
[01:32:17.760 --> 01:32:22.160]   and I would say I agree with him more than not,
[01:32:22.160 --> 01:32:26.160]   is you do deeply worry.
[01:32:26.160 --> 01:32:27.880]   If you believe there's no God,
[01:32:27.880 --> 01:32:31.760]   there's still a deep worry of the mystery of it all.
[01:32:31.760 --> 01:32:35.680]   Like, how does that make any sense that it just ends?
[01:32:35.680 --> 01:32:39.720]   I don't think we can truly understand that this right,
[01:32:39.720 --> 01:32:43.000]   I mean, so much of our life, the consciousness, the ego,
[01:32:43.000 --> 01:32:46.120]   is invested in this being.
[01:32:46.120 --> 01:32:51.560]   - Science keeps bringing humanity down from its pedestal.
[01:32:51.560 --> 01:32:54.720]   And that's just another example of it.
[01:32:54.720 --> 01:32:57.840]   - That's wonderful, but for us individual humans,
[01:32:57.840 --> 01:33:00.280]   we don't like to be brought down from a pedestal.
[01:33:00.280 --> 01:33:01.720]   You're saying like-- - I'm fine with it.
[01:33:01.720 --> 01:33:04.140]   - But see, you're fine with it because, well,
[01:33:04.140 --> 01:33:06.360]   so what Ernest Becker would say is you're fine with it
[01:33:06.360 --> 01:33:08.560]   because that's just a more peaceful existence for you,
[01:33:08.560 --> 01:33:09.560]   but you're not really fine.
[01:33:09.560 --> 01:33:12.000]   You're hiding from, in fact, some of the people
[01:33:12.000 --> 01:33:17.000]   that experience the deepest trauma earlier in life,
[01:33:17.000 --> 01:33:19.600]   they often, before they seek extensive therapy,
[01:33:19.600 --> 01:33:21.080]   will say that I'm fine.
[01:33:21.080 --> 01:33:23.480]   It's like when you talk to people who are truly angry,
[01:33:23.480 --> 01:33:25.440]   how are you doing, I'm fine.
[01:33:25.440 --> 01:33:27.800]   The question is what's going on.
[01:33:27.800 --> 01:33:29.200]   - Now, I had a near-death experience.
[01:33:29.200 --> 01:33:33.640]   I had a very bad motorbike accident when I was 17.
[01:33:33.640 --> 01:33:36.940]   So, but that didn't have any impact
[01:33:36.940 --> 01:33:40.460]   on my reflection on that topic.
[01:33:40.460 --> 01:33:43.120]   - So I'm basically just playing a bit of a devil's advocate,
[01:33:43.120 --> 01:33:46.820]   pushing back on wondering is it truly possible
[01:33:46.820 --> 01:33:47.660]   to accept death?
[01:33:47.660 --> 01:33:49.700]   And the flip side that's more interesting, I think,
[01:33:49.700 --> 01:33:54.700]   for AI and robotics is how important is it to have this
[01:33:54.700 --> 01:33:57.160]   as one of the suite of motivations,
[01:33:57.160 --> 01:34:02.160]   is to not just avoid falling off the roof or something
[01:34:02.160 --> 01:34:07.160]   like that, but ponder the end of the ride.
[01:34:07.160 --> 01:34:14.820]   If you listen to the Stoics, it's a great motivator.
[01:34:14.820 --> 01:34:16.900]   It adds a sense of urgency.
[01:34:16.900 --> 01:34:21.440]   So maybe to truly fear death or be cognizant of it
[01:34:21.440 --> 01:34:25.520]   might give a deeper meaning and urgency
[01:34:25.520 --> 01:34:28.300]   to the moment, to live fully.
[01:34:30.560 --> 01:34:32.220]   - Maybe I don't disagree with that.
[01:34:32.220 --> 01:34:34.960]   I mean, I think what motivates me here is,
[01:34:34.960 --> 01:34:38.980]   you know, knowing more about human nature.
[01:34:38.980 --> 01:34:41.760]   I mean, I think human nature and human intelligence
[01:34:41.760 --> 01:34:42.600]   is a big mystery.
[01:34:42.600 --> 01:34:46.580]   It's a scientific mystery, in addition to, you know,
[01:34:46.580 --> 01:34:48.580]   philosophical and et cetera,
[01:34:48.580 --> 01:34:50.860]   but, you know, I'm a true believer in science.
[01:34:50.860 --> 01:34:55.860]   So, and I do have kind of a belief that for complex systems
[01:34:57.540 --> 01:35:02.540]   like the brain and the mind, the way to understand it
[01:35:02.540 --> 01:35:05.360]   is to try to reproduce it with, you know,
[01:35:05.360 --> 01:35:07.660]   artifacts that you build, because you know
[01:35:07.660 --> 01:35:10.000]   what's essential to it when you try to build it.
[01:35:10.000 --> 01:35:12.420]   You know, the same way I've used this analogy before
[01:35:12.420 --> 01:35:15.820]   with you, I believe, the same way we only started
[01:35:15.820 --> 01:35:18.640]   to understand aerodynamics when we started
[01:35:18.640 --> 01:35:20.440]   building airplanes, and that helped us understand
[01:35:20.440 --> 01:35:21.340]   how birds fly.
[01:35:21.340 --> 01:35:25.480]   So I think there's kind of a similar process here
[01:35:25.480 --> 01:35:29.660]   where we don't have a theory, a full theory of intelligence,
[01:35:29.660 --> 01:35:31.760]   but building, you know, intelligent artifacts
[01:35:31.760 --> 01:35:34.640]   will help us perhaps develop some, you know,
[01:35:34.640 --> 01:35:37.800]   underlying theory that encompasses not just
[01:35:37.800 --> 01:35:41.920]   artificial implements, but also human
[01:35:41.920 --> 01:35:43.840]   and biological intelligence in general.
[01:35:43.840 --> 01:35:46.080]   - So you're an interesting person to ask this question
[01:35:46.080 --> 01:35:49.400]   about sort of all kinds of different other
[01:35:49.400 --> 01:35:53.100]   intelligent entities or intelligences.
[01:35:53.100 --> 01:35:56.300]   What are your thoughts about kind of like the Turing
[01:35:56.300 --> 01:35:58.020]   or the Chinese room question?
[01:35:58.020 --> 01:36:04.100]   If we create an AI system that exhibits a lot of properties
[01:36:04.100 --> 01:36:06.400]   of intelligence and consciousness,
[01:36:06.400 --> 01:36:10.220]   how comfortable are you thinking of that entity
[01:36:10.220 --> 01:36:12.340]   as intelligent or conscious?
[01:36:12.340 --> 01:36:15.560]   So you're trying to build now systems that have intelligence
[01:36:15.560 --> 01:36:17.420]   and there's metrics about their performance,
[01:36:17.420 --> 01:36:21.280]   but that metric is external.
[01:36:22.540 --> 01:36:23.380]   - Okay.
[01:36:23.380 --> 01:36:26.420]   - So how are you, are you okay calling a thing intelligent
[01:36:26.420 --> 01:36:29.020]   or are you going to be like most humans
[01:36:29.020 --> 01:36:32.700]   and be once again unhappy to be brought down
[01:36:32.700 --> 01:36:34.900]   from a pedestal of consciousness/intelligence?
[01:36:34.900 --> 01:36:39.500]   - No, I'll be very happy to understand
[01:36:39.500 --> 01:36:45.500]   more about human nature, human mind, and human intelligence
[01:36:45.500 --> 01:36:47.200]   through the construction of machines
[01:36:47.200 --> 01:36:50.560]   that have similar abilities.
[01:36:50.560 --> 01:36:54.480]   And if a consequence of this is to bring down humanity
[01:36:54.480 --> 01:36:58.000]   one notch down from its already low pedestal,
[01:36:58.000 --> 01:36:59.100]   I'm just fine with it.
[01:36:59.100 --> 01:37:01.300]   That's just the reality of life.
[01:37:01.300 --> 01:37:02.440]   So I'm fine with that.
[01:37:02.440 --> 01:37:04.980]   Now, you were asking me about things that,
[01:37:04.980 --> 01:37:07.900]   opinions I have that a lot of people may disagree with.
[01:37:07.900 --> 01:37:12.740]   I think if we think about the design
[01:37:12.740 --> 01:37:14.220]   of an autonomous intelligence system,
[01:37:14.220 --> 01:37:18.660]   so assuming that we are somewhat successful at some level
[01:37:18.660 --> 01:37:20.420]   of getting machines to learn models of the world,
[01:37:20.420 --> 01:37:22.580]   predictive models of the world,
[01:37:22.580 --> 01:37:25.820]   we build intrinsic motivation objective functions
[01:37:25.820 --> 01:37:28.300]   to drive the behavior of that system.
[01:37:28.300 --> 01:37:30.060]   The system also has perception modules
[01:37:30.060 --> 01:37:32.780]   that allows it to estimate the state of the world
[01:37:32.780 --> 01:37:35.460]   and then have some way of figuring out a sequence of actions
[01:37:35.460 --> 01:37:38.000]   that, you know, to optimize a particular objective.
[01:37:38.000 --> 01:37:42.700]   If it has a critic of the type that I was describing before,
[01:37:42.700 --> 01:37:44.580]   the thing that makes you recoil your arm
[01:37:44.580 --> 01:37:46.340]   the second time I try to pinch you,
[01:37:48.580 --> 01:37:51.660]   intelligent autonomous machine will have emotions.
[01:37:51.660 --> 01:37:54.020]   I think emotions are an integral part
[01:37:54.020 --> 01:37:56.380]   of autonomous intelligence.
[01:37:56.380 --> 01:37:58.980]   If you have an intelligent system
[01:37:58.980 --> 01:38:03.120]   that is driven by intrinsic motivation, by objectives,
[01:38:03.120 --> 01:38:07.640]   if it has a critic that allows it to predict in advance
[01:38:07.640 --> 01:38:10.060]   whether the outcome of a situation
[01:38:10.060 --> 01:38:12.220]   is going to be good or bad, it's going to have emotions.
[01:38:12.220 --> 01:38:13.460]   It's going to have fear.
[01:38:13.460 --> 01:38:14.300]   - Yes.
[01:38:14.300 --> 01:38:18.140]   - When it predicts that the outcome is going to be bad
[01:38:18.140 --> 01:38:20.700]   and something to avoid, it's going to have elation
[01:38:20.700 --> 01:38:22.620]   when it predicts it's going to be good.
[01:38:22.620 --> 01:38:28.180]   If it has drives to relate with humans,
[01:38:28.180 --> 01:38:30.620]   you know, in some ways, the way humans have,
[01:38:30.620 --> 01:38:34.460]   you know, it's going to be social, right?
[01:38:34.460 --> 01:38:37.380]   And so it's going to have emotions about attachment
[01:38:37.380 --> 01:38:38.620]   and things of that type.
[01:38:38.620 --> 01:38:43.620]   So I think, you know, the sort of sci-fi thing
[01:38:43.620 --> 01:38:46.900]   where, you know, you see commander data
[01:38:46.900 --> 01:38:50.100]   like having an emotion chip that you can turn off, right?
[01:38:50.100 --> 01:38:51.700]   I think that's ridiculous.
[01:38:51.700 --> 01:38:53.380]   - So, I mean, here's the difficult
[01:38:53.380 --> 01:38:57.820]   philosophical social question.
[01:38:57.820 --> 01:39:00.040]   Do you think there will be a time,
[01:39:00.040 --> 01:39:03.120]   like a civil rights movement for robots where,
[01:39:03.120 --> 01:39:06.460]   okay, forget the movement, but a discussion,
[01:39:06.460 --> 01:39:11.460]   like the Supreme Court, that particular kinds of robots,
[01:39:12.900 --> 01:39:14.860]   you know, particular kinds of systems
[01:39:14.860 --> 01:39:18.300]   deserve the same rights as humans
[01:39:18.300 --> 01:39:21.640]   because they can suffer just as humans can,
[01:39:21.640 --> 01:39:24.740]   all those kinds of things?
[01:39:24.740 --> 01:39:27.340]   - Well, perhaps, perhaps not.
[01:39:27.340 --> 01:39:29.580]   Like imagine that humans were,
[01:39:29.580 --> 01:39:33.740]   that you could, you know, die and be restored.
[01:39:33.740 --> 01:39:35.500]   Like, you know, you could be sort of, you know,
[01:39:35.500 --> 01:39:37.540]   be 3D reprinted and, you know,
[01:39:37.540 --> 01:39:40.740]   your brain could be reconstructed in its finest details.
[01:39:40.740 --> 01:39:43.140]   Our ideas of rights will change in that case.
[01:39:43.140 --> 01:39:44.540]   If you can always just,
[01:39:44.540 --> 01:39:48.220]   there's always a backup, you could always restore.
[01:39:48.220 --> 01:39:50.260]   Maybe like the importance of murder
[01:39:50.260 --> 01:39:51.980]   will go down one notch.
[01:39:51.980 --> 01:39:52.820]   - That's right.
[01:39:52.820 --> 01:39:56.140]   But also the, your, you know,
[01:39:56.140 --> 01:39:59.620]   desire to do dangerous things like, you know,
[01:39:59.620 --> 01:40:01.940]   doing skydiving or, you know,
[01:40:01.940 --> 01:40:06.100]   or, you know, race car driving, you know,
[01:40:06.100 --> 01:40:07.500]   car racing or that kind of stuff, you know,
[01:40:07.500 --> 01:40:09.420]   would probably increase.
[01:40:09.420 --> 01:40:11.100]   Or, you know, airplane aerobatics
[01:40:11.100 --> 01:40:11.940]   or that kind of stuff, right?
[01:40:11.940 --> 01:40:14.140]   It would be fine to do a lot of those things
[01:40:14.140 --> 01:40:17.460]   or explore, you know, dangerous areas and things like that.
[01:40:17.460 --> 01:40:19.180]   It would kind of change your relationship.
[01:40:19.180 --> 01:40:22.380]   So now it's very likely that robots would be like that
[01:40:22.380 --> 01:40:27.060]   because, you know, they'll be based on perhaps technology
[01:40:27.060 --> 01:40:30.140]   that is somewhat similar to today's technology.
[01:40:30.140 --> 01:40:32.260]   And you can always have a backup.
[01:40:32.260 --> 01:40:34.300]   - So it's possible.
[01:40:34.300 --> 01:40:35.700]   I don't know if you like video games,
[01:40:35.700 --> 01:40:39.340]   but there's a game called Diablo and-
[01:40:39.340 --> 01:40:41.860]   - Oh, my sons are huge fans of this.
[01:40:41.860 --> 01:40:42.700]   - Yes.
[01:40:42.700 --> 01:40:47.060]   - And in fact, they made a game that's inspired by it.
[01:40:47.060 --> 01:40:47.900]   - Awesome.
[01:40:47.900 --> 01:40:49.260]   Like built a game?
[01:40:49.260 --> 01:40:52.380]   - My three sons have a game design studio between them.
[01:40:52.380 --> 01:40:53.220]   Yeah. - That's awesome.
[01:40:53.220 --> 01:40:54.060]   - They came out with a game.
[01:40:54.060 --> 01:40:55.540]   - Like it just came out last year?
[01:40:55.540 --> 01:40:57.300]   - No, this was last year, early last year,
[01:40:57.300 --> 01:40:58.140]   about a year ago.
[01:40:58.140 --> 01:40:58.980]   - That's awesome.
[01:40:58.980 --> 01:41:01.980]   But so in Diablo, there's something called hardcore mode,
[01:41:01.980 --> 01:41:05.420]   which if you die, there's no, you're gone.
[01:41:05.420 --> 01:41:06.260]   - Right.
[01:41:06.260 --> 01:41:07.100]   - That's it.
[01:41:07.220 --> 01:41:09.660]   So it's possible with AI systems,
[01:41:09.660 --> 01:41:13.220]   for them to be able to operate successfully
[01:41:13.220 --> 01:41:15.540]   and for us to treat them in a certain way,
[01:41:15.540 --> 01:41:18.380]   'cause they have to be integrated in human society,
[01:41:18.380 --> 01:41:22.020]   they have to be able to die, no copies allowed.
[01:41:22.020 --> 01:41:23.860]   In fact, copying is illegal.
[01:41:23.860 --> 01:41:25.260]   It's possible with humans as well,
[01:41:25.260 --> 01:41:28.580]   like cloning will be illegal, even when it's possible.
[01:41:28.580 --> 01:41:29.940]   - But cloning is not copying, right?
[01:41:29.940 --> 01:41:33.100]   I mean, you don't reproduce the mind of the person
[01:41:33.100 --> 01:41:33.940]   and experience.
[01:41:33.940 --> 01:41:34.780]   - Right.
[01:41:34.780 --> 01:41:36.420]   - It's just a delayed twin, so.
[01:41:36.420 --> 01:41:39.060]   - But then it's, but we were talking about with computers
[01:41:39.060 --> 01:41:40.500]   that you will be able to copy.
[01:41:40.500 --> 01:41:41.340]   - Right.
[01:41:41.340 --> 01:41:42.660]   - You will be able to perfectly save,
[01:41:42.660 --> 01:41:46.660]   pickle the mind state.
[01:41:46.660 --> 01:41:49.660]   And it's possible that that will be illegal
[01:41:49.660 --> 01:41:52.320]   because that goes against,
[01:41:52.320 --> 01:41:55.980]   that will destroy the motivation of the system.
[01:41:55.980 --> 01:41:59.100]   - Okay, so let's say you have a domestic robot.
[01:41:59.100 --> 01:41:59.940]   - Yes.
[01:41:59.940 --> 01:42:01.380]   - Okay, sometime in the future.
[01:42:01.380 --> 01:42:02.460]   - Yes.
[01:42:02.460 --> 01:42:04.940]   - And the domestic robot, you know,
[01:42:04.940 --> 01:42:07.140]   comes to you kind of somewhat pre-trained,
[01:42:07.140 --> 01:42:08.340]   you know, it can do a bunch of things.
[01:42:08.340 --> 01:42:09.180]   - Yes.
[01:42:09.180 --> 01:42:10.580]   - But it has a particular personality
[01:42:10.580 --> 01:42:12.300]   that makes it slightly different from the other robots
[01:42:12.300 --> 01:42:14.220]   because that makes them more interesting.
[01:42:14.220 --> 01:42:15.920]   And then because it's, you know,
[01:42:15.920 --> 01:42:18.060]   it's lived with you for five years,
[01:42:18.060 --> 01:42:21.900]   you've grown some attachment to it and vice versa.
[01:42:21.900 --> 01:42:24.380]   And it's learned a lot about you.
[01:42:24.380 --> 01:42:25.900]   Or maybe it's not a household robot.
[01:42:25.900 --> 01:42:29.380]   Maybe it's a virtual assistant that lives in your,
[01:42:29.380 --> 01:42:32.580]   you know, augmented reality glasses or whatever, right?
[01:42:32.580 --> 01:42:35.020]   You know, the horror movie type thing, right?
[01:42:35.020 --> 01:42:39.620]   And that system, to some extent,
[01:42:39.620 --> 01:42:43.900]   the intelligence in that system is a bit like your child
[01:42:43.900 --> 01:42:47.100]   or maybe your PhD student in the sense that
[01:42:47.100 --> 01:42:49.780]   there's a lot of you in that machine now, right?
[01:42:49.780 --> 01:42:50.620]   - Yeah.
[01:42:50.620 --> 01:42:53.500]   - And so if it were a living thing,
[01:42:53.500 --> 01:42:56.560]   you would do this for free if you want, right?
[01:42:56.560 --> 01:42:58.380]   If it's your child, your child can, you know,
[01:42:58.380 --> 01:43:01.580]   then live his or her own life.
[01:43:01.580 --> 01:43:04.020]   And, you know, the fact that they learn stuff from you
[01:43:04.020 --> 01:43:06.020]   doesn't mean that you have any ownership of it, right?
[01:43:06.020 --> 01:43:06.860]   - Yeah.
[01:43:06.860 --> 01:43:09.380]   - But if it's a robot that you've trained,
[01:43:09.380 --> 01:43:13.980]   perhaps you have some intellectual property claim about-
[01:43:13.980 --> 01:43:15.140]   - Intellectual property?
[01:43:15.140 --> 01:43:18.160]   Oh, I thought you meant like a permanence value
[01:43:18.160 --> 01:43:20.140]   in the sense that's part of you is in-
[01:43:20.140 --> 01:43:21.700]   - Well, there is permanence value, right?
[01:43:21.700 --> 01:43:24.660]   So you would lose a lot if that robot were to be destroyed
[01:43:24.660 --> 01:43:26.460]   and you had no backup, you would lose a lot.
[01:43:26.460 --> 01:43:28.100]   You would lose a lot of investment, you know,
[01:43:28.100 --> 01:43:31.860]   kind of like a person dying, you know,
[01:43:31.860 --> 01:43:35.620]   that a friend of yours dying or a coworker
[01:43:35.620 --> 01:43:36.820]   or something like that.
[01:43:36.820 --> 01:43:42.340]   - But also you have like intellectual property rights
[01:43:42.340 --> 01:43:45.940]   in the sense that that system is fine-tuned
[01:43:45.940 --> 01:43:47.340]   to your particular existence.
[01:43:47.340 --> 01:43:49.860]   So that's now a very unique instantiation
[01:43:49.860 --> 01:43:51.980]   of that original background model,
[01:43:51.980 --> 01:43:54.260]   whatever it was that arrived.
[01:43:54.260 --> 01:43:55.660]   - And then there are issues of privacy, right?
[01:43:55.660 --> 01:43:59.700]   Because now imagine that that robot has its own
[01:43:59.700 --> 01:44:02.820]   kind of volition and decides to work for someone else
[01:44:02.820 --> 01:44:05.980]   or kind of, you know, thinks life with you
[01:44:05.980 --> 01:44:07.820]   is sort of untenable or whatever.
[01:44:07.820 --> 01:44:08.660]   - Right.
[01:44:08.660 --> 01:44:12.740]   - Now, all the things that that system learned from you,
[01:44:12.740 --> 01:44:16.820]   you know, can you like, you know,
[01:44:16.820 --> 01:44:18.100]   delete all the personal information
[01:44:18.100 --> 01:44:19.620]   that that system knows about you?
[01:44:19.620 --> 01:44:20.580]   - Yeah.
[01:44:20.580 --> 01:44:22.180]   - I mean, that would be kind of an ethical question.
[01:44:22.180 --> 01:44:26.500]   Like, you know, can you erase the mind of a intelligent
[01:44:26.500 --> 01:44:29.820]   robot to protect your privacy?
[01:44:29.820 --> 01:44:30.660]   - Yeah.
[01:44:30.660 --> 01:44:31.540]   - You can't do this with humans.
[01:44:31.540 --> 01:44:32.620]   You can ask them to shut up,
[01:44:32.620 --> 01:44:35.620]   but that you don't have complete power over them.
[01:44:35.620 --> 01:44:36.780]   - Can't erase humans.
[01:44:36.780 --> 01:44:39.020]   Yeah, it's the problem with the relationships, you know,
[01:44:39.020 --> 01:44:42.660]   that you break up, you can't erase the other human.
[01:44:42.660 --> 01:44:44.940]   With robots, I think it will have to be the same thing
[01:44:44.940 --> 01:44:49.940]   with robots, that risk, that there has to be some risk
[01:44:50.300 --> 01:44:55.100]   to our interactions to truly experience them deeply,
[01:44:55.100 --> 01:44:56.140]   it feels like.
[01:44:56.140 --> 01:44:59.620]   So you have to be able to lose your robot friend
[01:44:59.620 --> 01:45:01.660]   and that robot friend to go tweeting
[01:45:01.660 --> 01:45:03.700]   about how much of an asshole you are.
[01:45:03.700 --> 01:45:06.140]   - But then are you allowed to, you know,
[01:45:06.140 --> 01:45:08.620]   murder the robot to protect your private information?
[01:45:08.620 --> 01:45:09.460]   - Yeah, probably not.
[01:45:09.460 --> 01:45:10.300]   - If the robot decides to leave?
[01:45:10.300 --> 01:45:14.540]   - I have this intuition that for robots with certain,
[01:45:14.540 --> 01:45:16.820]   like, it's almost like a regulation.
[01:45:16.820 --> 01:45:19.220]   If you declare your robot to be,
[01:45:19.220 --> 01:45:20.980]   let's call it sentient or something like that,
[01:45:20.980 --> 01:45:24.180]   like this robot is designed for human interaction,
[01:45:24.180 --> 01:45:26.020]   then you're not allowed to murder these robots.
[01:45:26.020 --> 01:45:28.180]   It's the same as murdering other humans.
[01:45:28.180 --> 01:45:30.300]   - Well, but what about you do a backup of the robot
[01:45:30.300 --> 01:45:32.580]   that you preserve on a hard drive
[01:45:32.580 --> 01:45:33.860]   with the equivalent in the future?
[01:45:33.860 --> 01:45:34.700]   - That might be illegal.
[01:45:34.700 --> 01:45:38.020]   It's like piracy is illegal.
[01:45:38.020 --> 01:45:39.740]   - No, but it's your own robot, right?
[01:45:39.740 --> 01:45:41.620]   - But you can't, you don't--
[01:45:41.620 --> 01:45:44.980]   - But then you can wipe out his brain,
[01:45:44.980 --> 01:45:47.380]   so this robot doesn't know anything about you anymore,
[01:45:47.380 --> 01:45:50.380]   but you still have, technically it's still in existence
[01:45:50.380 --> 01:45:51.660]   because you backed it up.
[01:45:51.660 --> 01:45:53.500]   - And then there'll be these great speeches
[01:45:53.500 --> 01:45:55.420]   at the Supreme Court by saying,
[01:45:55.420 --> 01:45:57.780]   oh, sure, you can erase the mind of the robot
[01:45:57.780 --> 01:46:00.020]   just like you can erase the mind of a human.
[01:46:00.020 --> 01:46:01.060]   We both can suffer.
[01:46:01.060 --> 01:46:02.180]   There'll be some epic, like,
[01:46:02.180 --> 01:46:05.620]   Obama-type character with a speech that we,
[01:46:05.620 --> 01:46:07.940]   like, the robots and the humans are the same.
[01:46:07.940 --> 01:46:11.340]   We can both suffer, we can both hope,
[01:46:11.340 --> 01:46:14.820]   we can both, all of those kinds of things,
[01:46:14.820 --> 01:46:17.220]   raise families, all that kind of stuff.
[01:46:17.220 --> 01:46:20.100]   It's interesting for these, just like you said,
[01:46:20.100 --> 01:46:24.180]   emotion seems to be a fascinatingly powerful aspect
[01:46:24.180 --> 01:46:27.340]   of human-to-human interaction, human-robot interaction,
[01:46:27.340 --> 01:46:30.460]   and if they're able to exhibit emotions,
[01:46:30.460 --> 01:46:33.540]   at the end of the day, that's probably going to
[01:46:33.540 --> 01:46:37.100]   have us deeply consider human rights,
[01:46:37.100 --> 01:46:38.460]   like what we value in humans,
[01:46:38.460 --> 01:46:40.300]   what we value in other animals.
[01:46:40.300 --> 01:46:42.140]   That's why robots and AI is great.
[01:46:42.140 --> 01:46:44.260]   It makes us ask really good questions.
[01:46:44.260 --> 01:46:45.460]   - The hard questions, yeah.
[01:46:46.100 --> 01:46:49.580]   You asked about the Chinese room-type argument.
[01:46:49.580 --> 01:46:51.500]   Is it real, if it looks real?
[01:46:51.500 --> 01:46:54.300]   I think the Chinese room argument is a ridiculous one.
[01:46:54.300 --> 01:46:57.820]   - So for people who don't know, Chinese room is,
[01:46:57.820 --> 01:47:00.740]   I don't even know how to formulate it well,
[01:47:00.740 --> 01:47:04.620]   but basically, you can mimic the behavior
[01:47:04.620 --> 01:47:06.780]   of an intelligent system by just following
[01:47:06.780 --> 01:47:10.180]   a giant algorithm codebook that tells you
[01:47:10.180 --> 01:47:12.880]   exactly how to respond in exactly each case,
[01:47:12.880 --> 01:47:14.700]   but is that really intelligent?
[01:47:14.700 --> 01:47:16.580]   It's like a giant lookup table.
[01:47:16.580 --> 01:47:18.580]   When this person says this, you answer this.
[01:47:18.580 --> 01:47:21.020]   When this person says this, you answer this.
[01:47:21.020 --> 01:47:24.300]   And if you understand how that works,
[01:47:24.300 --> 01:47:27.340]   you have this giant, nearly infinite lookup table.
[01:47:27.340 --> 01:47:28.620]   Is that really intelligence?
[01:47:28.620 --> 01:47:31.300]   'Cause intelligence seems to be a mechanism
[01:47:31.300 --> 01:47:33.420]   that's much more interesting and complex
[01:47:33.420 --> 01:47:34.620]   than this lookup table.
[01:47:34.620 --> 01:47:35.460]   - I don't think so.
[01:47:35.460 --> 01:47:38.940]   So the real question comes down to,
[01:47:38.940 --> 01:47:43.940]   do you think you can mechanize intelligence in some way,
[01:47:44.340 --> 01:47:47.580]   even if that involves learning?
[01:47:47.580 --> 01:47:49.300]   And the answer is, of course, yes.
[01:47:49.300 --> 01:47:50.740]   There's no question.
[01:47:50.740 --> 01:47:52.140]   There's a second question then,
[01:47:52.140 --> 01:47:56.540]   which is, assuming you can reproduce intelligence
[01:47:56.540 --> 01:47:59.380]   in sort of different hardware than biological hardware,
[01:47:59.380 --> 01:48:00.620]   you know, like computers,
[01:48:00.620 --> 01:48:07.420]   can you match human intelligence in all the domains
[01:48:07.420 --> 01:48:11.860]   in which humans are intelligent?
[01:48:11.860 --> 01:48:13.940]   Is it possible, right?
[01:48:13.940 --> 01:48:17.060]   So that's the hypothesis of strong AI.
[01:48:17.060 --> 01:48:20.700]   The answer to this, in my opinion, is an unqualified yes.
[01:48:20.700 --> 01:48:22.620]   This will happen at some point.
[01:48:22.620 --> 01:48:25.300]   There's no question that machines, at some point,
[01:48:25.300 --> 01:48:26.580]   will become more intelligent than humans
[01:48:26.580 --> 01:48:28.580]   in all domains where humans are intelligent.
[01:48:28.580 --> 01:48:30.180]   This is not for tomorrow.
[01:48:30.180 --> 01:48:32.180]   It's gonna take a long time,
[01:48:32.180 --> 01:48:37.180]   regardless of what Elon and others have claimed or believed.
[01:48:37.180 --> 01:48:42.060]   This is a lot harder than many of those guys think it is.
[01:48:43.420 --> 01:48:45.780]   And many of those guys who thought it was simpler than that
[01:48:45.780 --> 01:48:47.460]   years, you know, five years ago,
[01:48:47.460 --> 01:48:49.900]   now think it's hard because it's been five years
[01:48:49.900 --> 01:48:53.420]   and they realize it's gonna take a lot longer.
[01:48:53.420 --> 01:48:56.180]   That includes a bunch of people at DeepMind, for example.
[01:48:56.180 --> 01:48:57.020]   - Oh, interesting.
[01:48:57.020 --> 01:48:59.340]   I haven't actually touched base with the DeepMind folks,
[01:48:59.340 --> 01:49:03.300]   but some of it, Elon or Demis Hassabis,
[01:49:03.300 --> 01:49:05.820]   I mean, sometimes in your role,
[01:49:05.820 --> 01:49:08.780]   you have to kind of create deadlines
[01:49:08.780 --> 01:49:10.740]   that are nearer than farther away
[01:49:10.740 --> 01:49:12.820]   to kind of create an urgency.
[01:49:12.820 --> 01:49:15.180]   'Cause you have to believe the impossible is possible
[01:49:15.180 --> 01:49:16.180]   in order to accomplish it.
[01:49:16.180 --> 01:49:18.540]   And there's, of course, a flip side to that coin,
[01:49:18.540 --> 01:49:21.260]   but it's a weird, you can't be too cynical
[01:49:21.260 --> 01:49:22.420]   if you wanna get something done.
[01:49:22.420 --> 01:49:24.300]   - Absolutely, I agree with that.
[01:49:24.300 --> 01:49:26.900]   But I mean, you have to inspire people, right,
[01:49:26.900 --> 01:49:28.740]   to work on sort of ambitious things.
[01:49:28.740 --> 01:49:35.580]   So, you know, it's certainly a lot harder than we believe,
[01:49:35.580 --> 01:49:38.180]   but there's no question in my mind that this will happen.
[01:49:38.180 --> 01:49:40.260]   And now, you know, people are kind of worried about
[01:49:40.260 --> 01:49:42.460]   what does that mean for humans?
[01:49:42.460 --> 01:49:45.100]   They are gonna be brought down from their pedestal,
[01:49:45.100 --> 01:49:47.940]   you know, a bunch of notches with that.
[01:49:47.940 --> 01:49:51.700]   And, you know, is that gonna be good or bad?
[01:49:51.700 --> 01:49:53.460]   I mean, it's just gonna give more power, right?
[01:49:53.460 --> 01:49:56.180]   It's an amplifier for human intelligence, really.
[01:49:56.180 --> 01:49:59.700]   - So speaking of doing cool, ambitious things,
[01:49:59.700 --> 01:50:02.900]   FAIR, the Facebook AI Research Group,
[01:50:02.900 --> 01:50:05.500]   has recently celebrated its eighth birthday,
[01:50:05.500 --> 01:50:08.620]   or maybe you can correct me on that.
[01:50:08.620 --> 01:50:11.580]   Looking back, what has been the successes,
[01:50:11.580 --> 01:50:13.340]   the failures, the lessons learned
[01:50:13.340 --> 01:50:14.420]   from the eight years of FAIR?
[01:50:14.420 --> 01:50:16.540]   And maybe you can also give context of
[01:50:16.540 --> 01:50:21.260]   where does the newly minted meta AI fit into,
[01:50:21.260 --> 01:50:22.580]   how does it relate to FAIR?
[01:50:22.580 --> 01:50:23.740]   - Right, so let me tell you a little bit
[01:50:23.740 --> 01:50:25.500]   about the organization of all this.
[01:50:25.500 --> 01:50:30.020]   Yeah, FAIR was created almost exactly eight years ago.
[01:50:30.020 --> 01:50:31.220]   It wasn't called FAIR yet.
[01:50:31.220 --> 01:50:33.540]   It took that name a few months later.
[01:50:33.540 --> 01:50:37.740]   And at the time I joined Facebook,
[01:50:37.740 --> 01:50:39.460]   there was a group called the AI Group
[01:50:39.460 --> 01:50:43.500]   that had about 12 engineers and a few scientists,
[01:50:43.500 --> 01:50:45.420]   like 10 engineers and two scientists
[01:50:45.420 --> 01:50:47.020]   or something like that.
[01:50:47.020 --> 01:50:49.900]   I ran it for three and a half years as a director,
[01:50:49.900 --> 01:50:52.300]   hired the first few scientists
[01:50:52.300 --> 01:50:55.300]   and kind of set up the culture and organized it,
[01:50:55.300 --> 01:50:57.820]   explained to the Facebook leadership
[01:50:57.820 --> 01:51:00.140]   what fundamental research was about
[01:51:00.140 --> 01:51:03.580]   and how it can work within industry
[01:51:03.580 --> 01:51:05.700]   and how it needs to be open and everything.
[01:51:07.180 --> 01:51:12.180]   And I think it's been an unqualified success
[01:51:12.180 --> 01:51:16.460]   in the sense that FAIR has simultaneously produced
[01:51:16.460 --> 01:51:20.900]   top-level research and advanced the science
[01:51:20.900 --> 01:51:22.500]   and the technology, provided tools,
[01:51:22.500 --> 01:51:24.980]   open source tools like PyTorch and many others.
[01:51:24.980 --> 01:51:29.820]   But at the same time has had a direct
[01:51:29.820 --> 01:51:34.620]   or mostly indirect impact on Facebook at the time,
[01:51:34.620 --> 01:51:37.900]   now meta, in the sense that a lot of systems
[01:51:37.900 --> 01:51:44.340]   that meta is built around now are based on research projects
[01:51:44.340 --> 01:51:48.300]   that started at FAIR.
[01:51:48.300 --> 01:51:50.020]   And so if you were to take out deep learning
[01:51:50.020 --> 01:51:55.020]   out of Facebook services now and meta more generally,
[01:51:55.020 --> 01:51:57.660]   I mean, the company would literally crumble.
[01:51:57.660 --> 01:52:01.380]   I mean, it's completely built around AI these days
[01:52:01.380 --> 01:52:03.900]   and it's really essential to the operations.
[01:52:03.900 --> 01:52:06.540]   So what happened after three and a half years
[01:52:06.540 --> 01:52:10.140]   is that I changed role, I became chief scientist.
[01:52:10.140 --> 01:52:14.780]   So I'm not doing day-to-day management of FAIR anymore.
[01:52:14.780 --> 01:52:17.020]   I'm more of a kind of, you know,
[01:52:17.020 --> 01:52:18.780]   think about strategy and things like that.
[01:52:18.780 --> 01:52:21.380]   And I carry my, I conduct my own research.
[01:52:21.380 --> 01:52:23.220]   I have, you know, my own kind of research group
[01:52:23.220 --> 01:52:25.220]   working on self-supervised learning and things like this,
[01:52:25.220 --> 01:52:28.140]   which I didn't have time to do when I was director.
[01:52:28.140 --> 01:52:33.140]   So now FAIR is run by Jol Pinot and Antoine Borde.
[01:52:33.820 --> 01:52:36.300]   Together, because FAIR is kind of split in two now,
[01:52:36.300 --> 01:52:37.820]   there's something called FAIR Labs,
[01:52:37.820 --> 01:52:40.900]   which is sort of bottom-up, science-driven research
[01:52:40.900 --> 01:52:43.420]   and FAIR Excel, which is slightly more organized
[01:52:43.420 --> 01:52:47.660]   for bigger projects that require a little more kind of focus
[01:52:47.660 --> 01:52:49.740]   and more engineering support and things like that.
[01:52:49.740 --> 01:52:52.860]   So Jol needs FAIR Lab and Antoine Borde needs FAIR Excel.
[01:52:52.860 --> 01:52:54.540]   - Where are they located?
[01:52:54.540 --> 01:52:56.620]   - It's delocalized all over.
[01:52:56.620 --> 01:53:02.500]   So there's no question that the leadership of the company
[01:53:02.500 --> 01:53:06.540]   believes that this was a very worthwhile investment.
[01:53:06.540 --> 01:53:11.540]   And what that means is that it's there for the long run.
[01:53:11.540 --> 01:53:16.780]   Right, so there is, if you want to talk in these terms,
[01:53:16.780 --> 01:53:19.540]   which I don't like, there's a business model, if you want,
[01:53:19.540 --> 01:53:23.660]   where FAIR, despite being a very fundamental research lab,
[01:53:23.660 --> 01:53:25.900]   brings a lot of value to the company,
[01:53:25.900 --> 01:53:27.860]   mostly indirectly through other groups.
[01:53:27.860 --> 01:53:31.540]   Now, what happened three and a half years ago
[01:53:31.540 --> 01:53:34.620]   when I stepped down was also the creation of Facebook AI,
[01:53:34.620 --> 01:53:37.660]   which was basically a larger organization
[01:53:37.660 --> 01:53:41.700]   that covers FAIR, so FAIR is included in it,
[01:53:41.700 --> 01:53:46.260]   but also has other organizations that are focused
[01:53:46.260 --> 01:53:51.220]   on applied research or advanced development of AI technology
[01:53:51.220 --> 01:53:54.660]   that is more focused on the products of the company.
[01:53:54.660 --> 01:53:56.660]   - So less emphasis on fundamental research.
[01:53:56.660 --> 01:53:58.220]   - Less fundamental, but it's still research.
[01:53:58.220 --> 01:53:59.740]   I mean, there's a lot of papers coming out
[01:53:59.740 --> 01:54:03.940]   of those organizations and people are awesome
[01:54:03.940 --> 01:54:06.380]   and wonderful to interact with,
[01:54:06.380 --> 01:54:11.380]   but it serves as kind of a way to kind of scale up,
[01:54:11.380 --> 01:54:15.700]   if you want, sort of AI technology,
[01:54:15.700 --> 01:54:19.380]   which may be very experimental and sort of lab prototypes
[01:54:19.380 --> 01:54:20.580]   into things that are usable.
[01:54:20.580 --> 01:54:23.020]   - So FAIR is a subset of meta AI.
[01:54:23.020 --> 01:54:25.140]   Is FAIR become like KFC?
[01:54:25.140 --> 01:54:29.420]   It'll just keep the F, nobody cares what the F stands for?
[01:54:29.420 --> 01:54:34.420]   - We'll know soon enough, probably by the end of 2021.
[01:54:34.420 --> 01:54:38.420]   - I guess it's not a giant change, Mare, FAIR.
[01:54:38.420 --> 01:54:39.540]   - Well, Mare doesn't sound too good,
[01:54:39.540 --> 01:54:43.540]   but the brand people are kind of deciding on this
[01:54:43.540 --> 01:54:45.860]   and they've been hesitating for a while now
[01:54:45.860 --> 01:54:48.500]   and they tell us they're gonna come up with an answer
[01:54:48.500 --> 01:54:50.460]   as to whether FAIR is gonna change name
[01:54:50.460 --> 01:54:53.180]   or whether we're gonna change just the meaning of the F.
[01:54:53.180 --> 01:54:54.180]   - Oh, that's a good call.
[01:54:54.180 --> 01:54:56.140]   I would keep FAIR and change the meaning of the F.
[01:54:56.140 --> 01:54:57.620]   - That would be my preference.
[01:54:58.340 --> 01:55:00.980]   - I would turn the F into fundamental.
[01:55:00.980 --> 01:55:02.260]   - Oh, that's good. - Fundamental AI research.
[01:55:02.260 --> 01:55:03.100]   - Oh, that's really good, yeah, yeah.
[01:55:03.100 --> 01:55:04.260]   - Within meta AI.
[01:55:04.260 --> 01:55:06.700]   So this would be sort of meta FAIR,
[01:55:06.700 --> 01:55:08.340]   but people would call it FAIR, right?
[01:55:08.340 --> 01:55:09.340]   - Yeah, exactly.
[01:55:09.340 --> 01:55:10.180]   I like it.
[01:55:10.180 --> 01:55:15.180]   - And now meta AI is part of the reality lab.
[01:55:15.180 --> 01:55:21.180]   So, you know, meta now, the new Facebook, right,
[01:55:21.180 --> 01:55:23.940]   it's called meta and it's kind of divided
[01:55:23.940 --> 01:55:28.660]   into, you know, Facebook, Instagram, WhatsApp,
[01:55:28.660 --> 01:55:32.900]   and reality lab.
[01:55:32.900 --> 01:55:35.700]   And reality lab is about, you know, AR, VR,
[01:55:35.700 --> 01:55:39.180]   you know, telepresence, communication,
[01:55:39.180 --> 01:55:40.460]   technology and stuff like that.
[01:55:40.460 --> 01:55:44.140]   It's kind of the, you can think of it as the sort of,
[01:55:44.140 --> 01:55:47.820]   a combination of sort of new products
[01:55:47.820 --> 01:55:51.900]   and technology part of meta.
[01:55:51.900 --> 01:55:54.180]   - Is that where the touch sensing for robots,
[01:55:54.180 --> 01:55:56.020]   I saw that you were posting about, that's-
[01:55:56.020 --> 01:55:58.180]   - But touch sensing for robot is part of FAIR, actually.
[01:55:58.180 --> 01:56:00.500]   That's a FAIR product. - Oh, it is, okay, cool.
[01:56:00.500 --> 01:56:02.980]   - This is also the, no, but there is the other way,
[01:56:02.980 --> 01:56:05.980]   the haptic glove, right? - Yes.
[01:56:05.980 --> 01:56:07.860]   - That has like- - That's more reality lab.
[01:56:07.860 --> 01:56:10.700]   - That's reality lab research.
[01:56:10.700 --> 01:56:11.700]   - Reality lab research.
[01:56:11.700 --> 01:56:14.340]   But by the way, the touch sensors is super interesting.
[01:56:14.340 --> 01:56:16.060]   Like integrating that modality
[01:56:16.060 --> 01:56:20.060]   into the whole sensing suite is very interesting.
[01:56:20.060 --> 01:56:23.620]   So what do you think about the metaverse?
[01:56:23.620 --> 01:56:27.740]   What do you think about this whole kind of expansion
[01:56:27.740 --> 01:56:30.820]   of the view of the role of Facebook and meta in the world?
[01:56:30.820 --> 01:56:32.420]   - Well, metaverse really should be thought of
[01:56:32.420 --> 01:56:35.260]   as the next step in the internet, right?
[01:56:35.260 --> 01:56:40.260]   Sort of trying to kind of, you know,
[01:56:40.260 --> 01:56:44.060]   make the experience more compelling of, you know,
[01:56:44.060 --> 01:56:47.900]   being connected either with other people or with content.
[01:56:49.420 --> 01:56:54.260]   And, you know, we are evolved and trained to evolve in,
[01:56:54.260 --> 01:56:57.260]   you know, 3D environments where, you know,
[01:56:57.260 --> 01:57:00.980]   we can see other people, we can talk to them when near them,
[01:57:00.980 --> 01:57:04.060]   or, you know, and other people are far away can't hear us,
[01:57:04.060 --> 01:57:04.980]   you know, things like that, right?
[01:57:04.980 --> 01:57:08.580]   So there's a lot of social conventions that exist
[01:57:08.580 --> 01:57:10.740]   in the real world that we can try to transpose.
[01:57:10.740 --> 01:57:13.220]   Now, what is gonna be eventually the,
[01:57:13.220 --> 01:57:16.180]   how compelling is it gonna be?
[01:57:16.180 --> 01:57:18.700]   Like, you know, is it gonna be the case
[01:57:18.700 --> 01:57:21.260]   that people are gonna be willing to do this
[01:57:21.260 --> 01:57:22.660]   if they have to wear, you know,
[01:57:22.660 --> 01:57:24.580]   a huge pair of goggles all day?
[01:57:24.580 --> 01:57:25.500]   Maybe not.
[01:57:25.500 --> 01:57:27.460]   - But then again, if the experience
[01:57:27.460 --> 01:57:30.300]   is sufficiently compelling, maybe so.
[01:57:30.300 --> 01:57:32.140]   - Or if the device that you have to wear
[01:57:32.140 --> 01:57:34.300]   is just basically a pair of glasses, you know,
[01:57:34.300 --> 01:57:36.780]   and technology makes sufficient progress for that.
[01:57:36.780 --> 01:57:41.540]   You know, AR is a much easier concept to grasp
[01:57:41.540 --> 01:57:43.180]   that you're gonna have, you know,
[01:57:43.180 --> 01:57:46.580]   augmented reality glasses that basically contain
[01:57:46.580 --> 01:57:48.620]   some sort of, you know, virtual assistant
[01:57:48.620 --> 01:57:50.260]   that can help you in your daily lives.
[01:57:50.260 --> 01:57:51.900]   - But at the same time with the AR,
[01:57:51.900 --> 01:57:53.460]   you have to contend with reality.
[01:57:53.460 --> 01:57:55.860]   With VR, you can completely detach yourself from reality,
[01:57:55.860 --> 01:57:57.180]   so it gives you freedom.
[01:57:57.180 --> 01:58:00.340]   It might be easier to design worlds in VR.
[01:58:00.340 --> 01:58:02.300]   - Yeah, but you can imagine, you know,
[01:58:02.300 --> 01:58:03.540]   the metaverse being-
[01:58:03.540 --> 01:58:05.580]   - A mix.
[01:58:05.580 --> 01:58:06.500]   - A mix, right.
[01:58:06.500 --> 01:58:09.300]   Or like you can have objects that exist in the metaverse
[01:58:09.300 --> 01:58:11.180]   that, you know, pop up on top of the real world
[01:58:11.180 --> 01:58:14.380]   or only exist in virtual reality.
[01:58:14.380 --> 01:58:17.060]   - Okay, let me ask the hard question.
[01:58:17.060 --> 01:58:18.300]   - Oh, because all of this was easy.
[01:58:18.300 --> 01:58:19.380]   - This was easy.
[01:58:19.380 --> 01:58:24.260]   The Facebook, now Meta, the social network
[01:58:24.260 --> 01:58:28.260]   has been painted by the media as net negative for society,
[01:58:28.260 --> 01:58:30.820]   even destructive and evil at times.
[01:58:30.820 --> 01:58:34.060]   You've pushed back against this, defending Facebook.
[01:58:34.060 --> 01:58:36.540]   Can you explain your defense?
[01:58:36.540 --> 01:58:38.620]   - Yeah, so the description,
[01:58:38.620 --> 01:58:42.580]   the company that is being described in some media
[01:58:42.580 --> 01:58:47.340]   is not the company we know when we work inside.
[01:58:47.340 --> 01:58:51.260]   And, you know, it could be claimed that
[01:58:51.260 --> 01:58:52.860]   a lot of employees are uninformed
[01:58:52.860 --> 01:58:54.580]   about what really goes on in a company,
[01:58:54.580 --> 01:58:56.540]   but, you know, I'm a vice president.
[01:58:56.540 --> 01:58:58.660]   I mean, I have a pretty good vision of what goes on.
[01:58:58.660 --> 01:59:00.180]   You know, I don't know everything, obviously.
[01:59:00.180 --> 01:59:01.860]   I'm not involved in everything,
[01:59:01.860 --> 01:59:04.580]   but certainly not in decision about like, you know,
[01:59:04.580 --> 01:59:06.100]   content moderation or anything like this,
[01:59:06.100 --> 01:59:10.140]   but I have some decent vision of what goes on.
[01:59:10.140 --> 01:59:13.660]   And this evil that is being described, I just don't see it.
[01:59:13.660 --> 01:59:18.180]   And then, you know, I think there is an easy story to buy,
[01:59:18.180 --> 01:59:21.740]   which is that, you know, all the bad things in the world,
[01:59:21.740 --> 01:59:25.140]   and, you know, the reason your friend believe crazy stuff,
[01:59:25.140 --> 01:59:28.740]   you know, there's an easy scapegoat, right,
[01:59:28.740 --> 01:59:34.460]   in social media in general, Facebook in particular.
[01:59:34.460 --> 01:59:35.460]   We have to look at the data.
[01:59:35.460 --> 01:59:40.060]   Like, is it the case that Facebook, for example,
[01:59:40.060 --> 01:59:41.660]   polarizes people politically?
[01:59:42.700 --> 01:59:45.220]   Are there academic studies that show this?
[01:59:45.220 --> 01:59:48.980]   Is it the case that, you know, teenagers
[01:59:48.980 --> 01:59:52.140]   think of themselves less if they use Instagram more?
[01:59:52.140 --> 01:59:55.340]   Is it the case that, you know,
[01:59:55.340 --> 01:59:59.140]   people get more riled up against, you know,
[01:59:59.140 --> 02:00:02.700]   opposite sides in a debate or political opinion
[02:00:02.700 --> 02:00:05.700]   if they are more on Facebook or if they are less?
[02:00:05.700 --> 02:00:10.700]   And study after study show that none of this is true.
[02:00:10.700 --> 02:00:12.420]   This is independent studies by academic.
[02:00:12.420 --> 02:00:15.900]   They're not funded by Facebook or Meta, you know,
[02:00:15.900 --> 02:00:18.300]   studied by Stanford, by some of my colleagues at NYU,
[02:00:18.300 --> 02:00:21.220]   actually, with whom I have no connection.
[02:00:21.220 --> 02:00:25.020]   You know, there's a study recently, they paid people,
[02:00:25.020 --> 02:00:29.940]   I think it was in the former Yugoslavia.
[02:00:29.940 --> 02:00:31.820]   I'm not exactly sure in what part,
[02:00:31.820 --> 02:00:34.380]   but they paid people to not use Facebook for a while
[02:00:34.380 --> 02:00:39.380]   in the period before the anniversary of the Srebrenica
[02:00:39.940 --> 02:00:41.140]   massacres, right?
[02:00:41.140 --> 02:00:43.180]   So, you know, people get riled up, like, should, you know,
[02:00:43.180 --> 02:00:45.460]   should we have a celebration?
[02:00:45.460 --> 02:00:48.700]   I mean, a memorial kind of celebration for it or not.
[02:00:48.700 --> 02:00:51.420]   So they paid a bunch of people to not use Facebook
[02:00:51.420 --> 02:00:52.580]   for a few weeks.
[02:00:52.580 --> 02:00:57.580]   And it turns out that those people ended up being
[02:00:57.580 --> 02:01:00.460]   more polarized than they were at the beginning.
[02:01:00.460 --> 02:01:01.620]   And the people who were more on Facebook
[02:01:01.620 --> 02:01:02.620]   were less polarized.
[02:01:02.620 --> 02:01:06.620]   There's a study, you know, from Stanford of,
[02:01:07.620 --> 02:01:11.020]   economists at Stanford that try to identify the causes
[02:01:11.020 --> 02:01:14.460]   of increasing polarization in the US.
[02:01:14.460 --> 02:01:17.460]   And it's been going on for 40 years before, you know,
[02:01:17.460 --> 02:01:19.100]   Mark Zuckerberg was born.
[02:01:19.100 --> 02:01:19.940]   - Yeah.
[02:01:19.940 --> 02:01:20.780]   - Continuously.
[02:01:20.780 --> 02:01:24.220]   And so if there is a cause,
[02:01:24.220 --> 02:01:26.100]   it's not Facebook or social media.
[02:01:26.100 --> 02:01:28.100]   So you could say if social media just accelerated,
[02:01:28.100 --> 02:01:31.740]   but no, I mean, it's basically a continuous evolution
[02:01:31.740 --> 02:01:34.300]   by some measure of polarization in the US.
[02:01:34.300 --> 02:01:36.300]   And then you compare this with other countries
[02:01:36.300 --> 02:01:40.100]   like the West half of Germany,
[02:01:40.100 --> 02:01:43.260]   because you can go 40 years in the East side
[02:01:43.260 --> 02:01:46.020]   or Denmark or other countries.
[02:01:46.020 --> 02:01:47.980]   And they use Facebook just as much.
[02:01:47.980 --> 02:01:49.140]   And they're not getting more polarized,
[02:01:49.140 --> 02:01:50.540]   they're getting less polarized.
[02:01:50.540 --> 02:01:52.980]   So if you want to look for, you know,
[02:01:52.980 --> 02:01:54.700]   a causal relationship there,
[02:01:54.700 --> 02:01:58.420]   you can find a scapegoat, but you can't find a cause.
[02:01:58.420 --> 02:02:00.260]   Now, if you want to fix the problem,
[02:02:00.260 --> 02:02:01.620]   you have to find the right cause.
[02:02:01.620 --> 02:02:04.900]   And what riles me up is that people now are,
[02:02:04.900 --> 02:02:08.300]   people now are accusing Facebook of bad deeds
[02:02:08.300 --> 02:02:09.300]   that are done by others.
[02:02:09.300 --> 02:02:12.380]   And those others are, we're not doing anything about them.
[02:02:12.380 --> 02:02:14.460]   And by the way, those others include
[02:02:14.460 --> 02:02:15.660]   the owner of the Wall Street Journal
[02:02:15.660 --> 02:02:17.700]   in which all of those papers were published.
[02:02:17.700 --> 02:02:20.060]   - So I should mention that I'm talking to Shrepp,
[02:02:20.060 --> 02:02:23.460]   Mike Shrepp on this podcast and also Mark Zuckerberg,
[02:02:23.460 --> 02:02:26.340]   and probably these are conversations you can have with them.
[02:02:26.340 --> 02:02:27.620]   'Cause it's very interesting to me,
[02:02:27.620 --> 02:02:31.900]   even if Facebook has some measurable negative effect,
[02:02:31.900 --> 02:02:33.780]   you can't just consider that in isolation.
[02:02:33.780 --> 02:02:35.900]   You have to consider about all the positive ways
[02:02:35.900 --> 02:02:36.780]   that it connects us.
[02:02:36.780 --> 02:02:38.100]   So like every technology--
[02:02:38.100 --> 02:02:39.620]   - It connects people, it's a question.
[02:02:39.620 --> 02:02:43.860]   - You can't just say like, there's an increase in division.
[02:02:43.860 --> 02:02:46.060]   Yes, probably Google search engine
[02:02:46.060 --> 02:02:47.860]   has created increase in division,
[02:02:47.860 --> 02:02:49.860]   but you have to consider about how much information
[02:02:49.860 --> 02:02:51.100]   are brought to the world.
[02:02:51.100 --> 02:02:53.660]   Like I'm sure Wikipedia created more division
[02:02:53.660 --> 02:02:55.300]   if you just look at the division,
[02:02:55.300 --> 02:02:57.660]   but you have to look at the full context of the world
[02:02:57.660 --> 02:02:59.100]   and did it make a better world?
[02:02:59.100 --> 02:03:01.580]   - I mean, the printing press has created more division.
[02:03:01.580 --> 02:03:03.020]   - Exactly.
[02:03:03.020 --> 02:03:06.860]   - So when the printing press was invented,
[02:03:06.860 --> 02:03:09.300]   the first books that were printed
[02:03:09.300 --> 02:03:10.780]   were things like the Bible,
[02:03:10.780 --> 02:03:13.780]   and that allowed people to read the Bible by themselves,
[02:03:13.780 --> 02:03:17.380]   not get the message uniquely from priests in Europe.
[02:03:17.380 --> 02:03:20.340]   And that created the Protestant movement
[02:03:20.340 --> 02:03:23.660]   and 200 years of religious persecution and wars.
[02:03:23.660 --> 02:03:26.180]   So that's a bad side effect of the printing press.
[02:03:26.180 --> 02:03:28.500]   Social networks aren't being nearly as bad
[02:03:28.500 --> 02:03:29.340]   as the printing press,
[02:03:29.340 --> 02:03:31.940]   but nobody would say the printing press was a bad idea.
[02:03:32.900 --> 02:03:35.100]   - Yeah, a lot of it is perception
[02:03:35.100 --> 02:03:38.420]   and there's a lot of different incentives operating here.
[02:03:38.420 --> 02:03:40.020]   Maybe a quick comment,
[02:03:40.020 --> 02:03:42.660]   since you're one of the top leaders at Facebook
[02:03:42.660 --> 02:03:46.740]   and at Meta, sorry, that's in the tech space,
[02:03:46.740 --> 02:03:48.500]   I'm sure Facebook involves
[02:03:48.500 --> 02:03:52.020]   a lot of incredible technological challenges
[02:03:52.020 --> 02:03:52.900]   that need to be solved.
[02:03:52.900 --> 02:03:54.980]   A lot of it probably is on the computer infrastructure,
[02:03:54.980 --> 02:03:58.900]   the hardware, I mean, it's just a huge amount.
[02:03:58.900 --> 02:04:00.380]   Maybe can you give me context
[02:04:00.380 --> 02:04:04.380]   about how much of Schrepp's life is AI
[02:04:04.380 --> 02:04:06.220]   and how much of it is low-level compute?
[02:04:06.220 --> 02:04:09.580]   How much of it is flying all around doing business stuff
[02:04:09.580 --> 02:04:12.020]   in the same way Zuckerberg, Mark Zuckerberg?
[02:04:12.020 --> 02:04:13.740]   - They really focus on AI.
[02:04:13.740 --> 02:04:18.740]   I mean, certainly in the run-up of the creation affair
[02:04:18.740 --> 02:04:24.060]   and for at least a year after that, if not more,
[02:04:24.060 --> 02:04:26.700]   Mark was very, very much focused on AI
[02:04:26.700 --> 02:04:29.700]   and was spending quite a lot of effort on it.
[02:04:29.700 --> 02:04:30.780]   And that's his style.
[02:04:30.780 --> 02:04:32.060]   When he gets interested in something,
[02:04:32.060 --> 02:04:34.100]   he reads everything about it.
[02:04:34.100 --> 02:04:36.900]   He read some of my papers, for example, before I joined.
[02:04:36.900 --> 02:04:41.860]   And so he learned a lot about it.
[02:04:41.860 --> 02:04:43.740]   - He said he liked notes.
[02:04:43.740 --> 02:04:44.580]   - Right.
[02:04:44.580 --> 02:04:51.100]   And Schrepp was really into it also.
[02:04:51.100 --> 02:04:52.780]   I mean, Schrepp is really kind of,
[02:04:52.780 --> 02:04:57.940]   has something I've tried to preserve also
[02:04:57.940 --> 02:05:00.180]   despite my not so young age,
[02:05:00.180 --> 02:05:03.180]   which is a sense of wonder about science and technology.
[02:05:03.180 --> 02:05:05.260]   And he certainly has that.
[02:05:05.260 --> 02:05:07.420]   He's also a wonderful person.
[02:05:07.420 --> 02:05:10.380]   I mean, in terms of like as a manager,
[02:05:10.380 --> 02:05:12.140]   like dealing with people and everything,
[02:05:12.140 --> 02:05:13.220]   Mark also actually.
[02:05:13.220 --> 02:05:18.020]   So, I mean, they're very like, you know, very human people.
[02:05:18.020 --> 02:05:20.340]   In the case of Mark, it's shockingly human,
[02:05:20.340 --> 02:05:23.180]   you know, given his trajectory.
[02:05:23.180 --> 02:05:27.060]   I mean, the personality of him
[02:05:27.060 --> 02:05:29.580]   that he's painting in the press is just completely wrong.
[02:05:29.580 --> 02:05:30.420]   - Yeah.
[02:05:30.420 --> 02:05:31.940]   But you have to know how to play the press.
[02:05:31.940 --> 02:05:36.180]   So that's, I put some of that responsibility on him too.
[02:05:36.180 --> 02:05:41.180]   You have to, it's like, you know, like the director,
[02:05:41.180 --> 02:05:44.300]   the conductor of an orchestra,
[02:05:44.300 --> 02:05:46.940]   you have to play the press and the public
[02:05:46.940 --> 02:05:47.980]   in a certain kind of way
[02:05:47.980 --> 02:05:49.700]   where you convey your true self to them.
[02:05:49.700 --> 02:05:51.100]   If there's a depth of kindness to it.
[02:05:51.100 --> 02:05:51.940]   - It's hard.
[02:05:51.940 --> 02:05:53.700]   And it's probably not the best at it.
[02:05:53.700 --> 02:05:54.620]   So, yeah.
[02:05:56.420 --> 02:05:57.700]   You have to learn.
[02:05:57.700 --> 02:06:00.420]   And it's sad to see, I'll talk to him about it,
[02:06:00.420 --> 02:06:04.020]   but Shrepp is slowly stepping down.
[02:06:04.020 --> 02:06:07.500]   It's always sad to see folks sort of be there
[02:06:07.500 --> 02:06:11.220]   for a long time and slowly, I guess time is sad.
[02:06:11.220 --> 02:06:14.780]   - I think he's done the thing he set out to do.
[02:06:14.780 --> 02:06:17.540]   And, you know, he's got, you know,
[02:06:17.540 --> 02:06:21.420]   family priorities and stuff like that.
[02:06:21.460 --> 02:06:26.460]   And I understand, you know, after 13 years or something.
[02:06:26.460 --> 02:06:28.900]   - It's been a good run.
[02:06:28.900 --> 02:06:32.140]   - Which in Silicon Valley is basically a lifetime.
[02:06:32.140 --> 02:06:32.980]   - Yeah.
[02:06:32.980 --> 02:06:34.980]   - You know, cause you know, it's dog years.
[02:06:34.980 --> 02:06:37.620]   - So in Europe, the conference just wrapped up.
[02:06:37.620 --> 02:06:40.580]   Let me just go back to something else.
[02:06:40.580 --> 02:06:42.500]   You posted that the paper you coauthored
[02:06:42.500 --> 02:06:44.460]   was rejected from Europe.
[02:06:44.460 --> 02:06:47.140]   As you said, proudly in quotes, rejected.
[02:06:47.140 --> 02:06:48.940]   - I'm a joke.
[02:06:48.940 --> 02:06:49.780]   - Yeah, I know.
[02:06:51.300 --> 02:06:53.940]   Can you describe this paper and like,
[02:06:53.940 --> 02:06:55.700]   what was the idea in it?
[02:06:55.700 --> 02:06:58.460]   And also maybe this is a good opportunity
[02:06:58.460 --> 02:07:00.580]   to ask what are the pros and cons,
[02:07:00.580 --> 02:07:03.620]   what works and what doesn't about the review process?
[02:07:03.620 --> 02:07:04.980]   - Yeah, let me talk about the paper first.
[02:07:04.980 --> 02:07:08.260]   I'll talk about the review process afterwards.
[02:07:08.260 --> 02:07:10.700]   The paper is called VICREG.
[02:07:10.700 --> 02:07:12.540]   So this is, I mentioned that before,
[02:07:12.540 --> 02:07:14.900]   variance, invariance, covariance, regularization.
[02:07:14.900 --> 02:07:18.260]   And it's a technique, a non-contrastive learning technique
[02:07:18.260 --> 02:07:21.300]   for what I call joint embedding architecture.
[02:07:21.300 --> 02:07:23.340]   So Siamese nets are an example
[02:07:23.340 --> 02:07:24.860]   of joint embedding architecture.
[02:07:24.860 --> 02:07:26.580]   So joint embedding architecture is,
[02:07:26.580 --> 02:07:30.620]   let me back up a little bit, right?
[02:07:30.620 --> 02:07:33.300]   So if you want to do self-supervised learning,
[02:07:33.300 --> 02:07:35.140]   you can do it by prediction.
[02:07:35.140 --> 02:07:38.580]   So let's say you want to train a system to predict video,
[02:07:38.580 --> 02:07:39.420]   right?
[02:07:39.420 --> 02:07:40.260]   You show it a video clip
[02:07:40.260 --> 02:07:43.580]   and you train the system to predict the next,
[02:07:43.580 --> 02:07:45.060]   the continuation of that video clip.
[02:07:45.060 --> 02:07:47.820]   Now, because you need to handle uncertainty,
[02:07:47.820 --> 02:07:48.980]   because there are many, you know,
[02:07:48.980 --> 02:07:51.580]   many continuations that are plausible,
[02:07:51.580 --> 02:07:54.020]   you need to have, you need to handle this in some way.
[02:07:54.020 --> 02:07:56.660]   You need to have a way for the system
[02:07:56.660 --> 02:08:00.620]   to be able to produce multiple predictions.
[02:08:00.620 --> 02:08:03.500]   And the way, the only way I know to do this
[02:08:03.500 --> 02:08:05.420]   is through what's called a latent variable.
[02:08:05.420 --> 02:08:08.780]   So you have some sort of hidden vector
[02:08:08.780 --> 02:08:11.180]   of a variable that you can vary over a set
[02:08:11.180 --> 02:08:12.700]   or draw from a distribution.
[02:08:12.700 --> 02:08:14.500]   And as you vary this vector over a set,
[02:08:14.500 --> 02:08:15.620]   the output, the prediction,
[02:08:15.620 --> 02:08:18.460]   varies over a set of plausible predictions.
[02:08:18.460 --> 02:08:19.460]   Okay, so that's called,
[02:08:19.460 --> 02:08:23.220]   I call this a generative latent variable model.
[02:08:23.220 --> 02:08:24.940]   - Got it.
[02:08:24.940 --> 02:08:27.020]   - Okay, now there is an alternative to this,
[02:08:27.020 --> 02:08:28.660]   to handle uncertainty.
[02:08:28.660 --> 02:08:31.140]   And instead of directly predicting
[02:08:31.140 --> 02:08:34.820]   the next frames of the clip,
[02:08:34.820 --> 02:08:39.620]   you also run those through another neural net.
[02:08:39.620 --> 02:08:42.460]   So you now have two neural nets,
[02:08:42.460 --> 02:08:45.780]   one that looks at the, you know,
[02:08:45.780 --> 02:08:48.660]   the initial segment of the video clip,
[02:08:48.660 --> 02:08:51.220]   and another one that looks at the continuation
[02:08:51.220 --> 02:08:52.380]   during training, right?
[02:08:52.380 --> 02:08:56.260]   And what you're trying to do is learn a representation
[02:08:56.260 --> 02:08:59.020]   of those two video clips
[02:08:59.020 --> 02:09:00.740]   that is maximally informative
[02:09:00.740 --> 02:09:03.420]   about the video clips themselves,
[02:09:03.420 --> 02:09:07.140]   but is such that you can predict the representation
[02:09:07.140 --> 02:09:08.540]   of the second video clip
[02:09:08.540 --> 02:09:11.340]   from the representation of the first one, easily.
[02:09:11.340 --> 02:09:13.580]   Okay, and you can sort of formalize this
[02:09:13.580 --> 02:09:15.300]   in terms of maximizing mutual information
[02:09:15.300 --> 02:09:18.100]   and some stuff like that, but it doesn't matter.
[02:09:18.100 --> 02:09:23.100]   What you want is informative representations
[02:09:23.100 --> 02:09:27.460]   of the two video clips that are mutually predictable.
[02:09:27.460 --> 02:09:30.860]   What that means is that there's a lot of details
[02:09:30.860 --> 02:09:33.140]   in the second video clips that are irrelevant.
[02:09:33.140 --> 02:09:38.780]   You know, let's say a video clip consists in,
[02:09:38.780 --> 02:09:42.020]   you know, a camera panning the scene.
[02:09:42.020 --> 02:09:43.340]   There's going to be a piece of that room
[02:09:43.340 --> 02:09:44.740]   that is going to be revealed,
[02:09:44.740 --> 02:09:46.180]   and I can somewhat predict
[02:09:46.180 --> 02:09:48.060]   what that room is going to look like,
[02:09:48.060 --> 02:09:50.220]   but I may not be able to predict the details
[02:09:50.220 --> 02:09:52.300]   of the texture of the ground
[02:09:52.300 --> 02:09:54.500]   and where the tiles are ending and stuff like that, right?
[02:09:54.500 --> 02:09:56.340]   So those are irrelevant details
[02:09:56.340 --> 02:09:59.620]   that perhaps my representation will eliminate.
[02:09:59.620 --> 02:10:03.660]   And so what I need is to train this second neural net
[02:10:03.660 --> 02:10:08.660]   in such a way that whenever the continuation
[02:10:09.020 --> 02:10:12.220]   video clip varies over all the plausible continuations,
[02:10:12.220 --> 02:10:15.620]   the representation doesn't change.
[02:10:15.620 --> 02:10:16.460]   - Got it.
[02:10:16.460 --> 02:10:18.100]   So it's the, yeah, yeah, got it.
[02:10:18.100 --> 02:10:20.860]   Over the space of representations,
[02:10:20.860 --> 02:10:21.860]   doing the same kind of thing
[02:10:21.860 --> 02:10:24.300]   as you're doing with similarity learning.
[02:10:24.300 --> 02:10:25.700]   - Right.
[02:10:25.700 --> 02:10:28.060]   So these are two ways to handle
[02:10:28.060 --> 02:10:29.580]   multimodality in a prediction, right?
[02:10:29.580 --> 02:10:32.260]   In the first way, you parametrize the prediction
[02:10:32.260 --> 02:10:33.460]   with a latent variable,
[02:10:33.460 --> 02:10:35.780]   but you predict pixels essentially, right?
[02:10:35.780 --> 02:10:38.380]   In the second one, you don't predict pixels,
[02:10:38.380 --> 02:10:40.660]   you predict an abstract representation of pixels
[02:10:40.660 --> 02:10:43.460]   and you guarantee that this abstract representation
[02:10:43.460 --> 02:10:46.140]   has as much information as possible about the input,
[02:10:46.140 --> 02:10:47.020]   but sort of, you know,
[02:10:47.020 --> 02:10:49.700]   drops all the stuff that you really can't predict,
[02:10:49.700 --> 02:10:50.540]   essentially.
[02:10:50.540 --> 02:10:53.860]   I used to be a big fan of the first approach.
[02:10:53.860 --> 02:10:55.580]   And in fact, in this paper with Ishan Mishra,
[02:10:55.580 --> 02:10:58.340]   this blog post, the dark matter intelligence,
[02:10:58.340 --> 02:10:59.740]   I was kind of advocating for this.
[02:10:59.740 --> 02:11:01.540]   And in the last year and a half,
[02:11:01.540 --> 02:11:02.780]   I've completely changed my mind.
[02:11:02.780 --> 02:11:04.580]   I'm now a big fan of the second one.
[02:11:05.540 --> 02:11:10.020]   And it's because of a small collection of algorithms
[02:11:10.020 --> 02:11:13.220]   that have been proposed over the last year and a half
[02:11:13.220 --> 02:11:17.820]   or so, two years to do this, including V-Craig,
[02:11:17.820 --> 02:11:19.620]   its predecessor called Barlow-Twins,
[02:11:19.620 --> 02:11:23.140]   which I mentioned, a method from our friends
[02:11:23.140 --> 02:11:24.540]   at DeepMind called BYOL.
[02:11:24.540 --> 02:11:29.580]   And there's a bunch of others now that kind of work similarly.
[02:11:29.580 --> 02:11:32.580]   So they're all based on this idea of joint embedding.
[02:11:32.580 --> 02:11:34.660]   Some of them have an explicit criterion
[02:11:34.660 --> 02:11:36.620]   that is an approximation of mutual information.
[02:11:36.620 --> 02:11:39.420]   Some others at BYOL work, but we don't really know why.
[02:11:39.420 --> 02:11:41.220]   And there's been like lots of theoretical papers
[02:11:41.220 --> 02:11:42.340]   about why BYOL works.
[02:11:42.340 --> 02:11:43.940]   No, it's not that because we take it out
[02:11:43.940 --> 02:11:46.020]   and it still works and blah, blah, blah.
[02:11:46.020 --> 02:11:47.820]   I mean, so there's like a big debate,
[02:11:47.820 --> 02:11:51.540]   but the important point is that we now have a collection
[02:11:51.540 --> 02:11:53.700]   of non-contrastive joint embedding methods,
[02:11:53.700 --> 02:11:56.380]   which I think is the best thing since sliced bread.
[02:11:56.380 --> 02:11:58.300]   So I'm super excited about this
[02:11:58.300 --> 02:12:02.020]   because I think it's our best shot for techniques
[02:12:02.020 --> 02:12:06.340]   that would allow us to kind of build predictive world models
[02:12:06.340 --> 02:12:07.460]   and at the same time,
[02:12:07.460 --> 02:12:09.900]   learn hierarchical representations of the world
[02:12:09.900 --> 02:12:11.860]   where what matters about the world is preserved
[02:12:11.860 --> 02:12:14.420]   and what is irrelevant is eliminated.
[02:12:14.420 --> 02:12:17.020]   - And by the way, the representations of before and after
[02:12:17.020 --> 02:12:20.540]   is in the space in a sequence of images,
[02:12:20.540 --> 02:12:22.300]   or is it for single images?
[02:12:22.300 --> 02:12:24.660]   - It would be either for a single image, for a sequence.
[02:12:24.660 --> 02:12:25.660]   It doesn't have to be images.
[02:12:25.660 --> 02:12:26.700]   This could be applied to text.
[02:12:26.700 --> 02:12:28.540]   This could be applied to just about any signal.
[02:12:28.540 --> 02:12:32.940]   I'm looking for methods that are generally applicable
[02:12:32.940 --> 02:12:36.180]   that are not specific to one particular modality.
[02:12:36.180 --> 02:12:37.620]   It could be audio or whatever.
[02:12:37.620 --> 02:12:38.460]   - Got it.
[02:12:38.460 --> 02:12:39.660]   So what's the story behind this paper?
[02:12:39.660 --> 02:12:43.460]   This paper is what, is describing one such method?
[02:12:43.460 --> 02:12:44.500]   - This is this Vick-Reich method.
[02:12:44.500 --> 02:12:45.700]   So this is co-authored,
[02:12:45.700 --> 02:12:49.260]   the first author is a student called Adrien Barne,
[02:12:49.260 --> 02:12:52.700]   who is a resident PhD student at Fer Paris,
[02:12:52.700 --> 02:12:55.820]   who is co-advised by me and Jean Ponce,
[02:12:55.820 --> 02:12:58.180]   who's a professor at Ecole Normale Suprieure,
[02:12:58.180 --> 02:13:00.660]   also a research director at INRIA.
[02:13:00.660 --> 02:13:03.580]   So this is a wonderful program in France
[02:13:03.580 --> 02:13:06.620]   where PhD students can basically do their PhD in industry.
[02:13:06.620 --> 02:13:08.940]   And that's kind of what's happening here.
[02:13:08.940 --> 02:13:15.420]   And this paper is a follow-up on this Barlow-Twin paper
[02:13:15.420 --> 02:13:18.340]   by my former postdoc, now Stphane Denis,
[02:13:18.340 --> 02:13:21.500]   with Li Jing and Joris Bontart
[02:13:21.500 --> 02:13:24.700]   and a bunch of other people from Fer.
[02:13:24.700 --> 02:13:27.780]   And one of the main criticism from reviewers
[02:13:27.780 --> 02:13:31.340]   is that Vick-Reich is not different enough from Barlow-Twins.
[02:13:31.340 --> 02:13:36.340]   But my impression is that it's Barlow-Twins
[02:13:36.340 --> 02:13:39.860]   with a few bugs fixed essentially.
[02:13:39.860 --> 02:13:43.140]   And in the end, this is what people will use.
[02:13:43.140 --> 02:13:44.500]   - Right.
[02:13:44.500 --> 02:13:48.980]   - But I'm used to stuff that I submit being rejected for a while.
[02:13:48.980 --> 02:13:51.300]   - So it might be rejected and actually exceptionally well cited
[02:13:51.300 --> 02:13:52.140]   'cause people use it.
[02:13:52.140 --> 02:13:54.340]   - Well, it's already cited like a bunch of times.
[02:13:54.340 --> 02:13:57.580]   - So, I mean, the question is then to the deeper question
[02:13:57.580 --> 02:14:00.220]   about peer review and conferences.
[02:14:00.220 --> 02:14:02.580]   I mean, computer science as a field is kind of unique
[02:14:02.580 --> 02:14:04.940]   that the conference is highly prized.
[02:14:04.940 --> 02:14:05.780]   That's one.
[02:14:05.780 --> 02:14:06.620]   - Right.
[02:14:06.620 --> 02:14:08.860]   - And it's interesting because the peer review process
[02:14:08.860 --> 02:14:11.020]   there is similar, I suppose, to journals,
[02:14:11.020 --> 02:14:13.600]   but it's accelerated significantly.
[02:14:13.600 --> 02:14:16.500]   Well, not significantly, but it goes fast.
[02:14:16.500 --> 02:14:19.740]   And it's a nice way to get stuff out quickly,
[02:14:19.740 --> 02:14:20.740]   to peer review it quickly,
[02:14:20.740 --> 02:14:22.580]   go to present it quickly to the community.
[02:14:22.580 --> 02:14:25.100]   So not quickly, but quicker.
[02:14:25.100 --> 02:14:25.940]   - Yeah.
[02:14:25.940 --> 02:14:27.780]   - But nevertheless, it has many of the same flaws
[02:14:27.780 --> 02:14:30.180]   of peer review 'cause it's a limited number
[02:14:30.180 --> 02:14:31.460]   of people look at it.
[02:14:31.460 --> 02:14:32.740]   There's bias and the following,
[02:14:32.740 --> 02:14:35.520]   like that if you wanna do new ideas,
[02:14:35.520 --> 02:14:37.020]   you're gonna get pushed back.
[02:14:37.020 --> 02:14:42.060]   There's self-interested people that kind of can infer
[02:14:42.060 --> 02:14:46.700]   who submitted it and kind of be cranky about it,
[02:14:46.700 --> 02:14:47.700]   all that kind of stuff.
[02:14:47.700 --> 02:14:50.980]   - Yeah, I mean, there's a lot of social phenomenon there.
[02:14:50.980 --> 02:14:53.180]   There's one social phenomenon, which is that
[02:14:53.180 --> 02:14:56.760]   because the field has been growing exponentially,
[02:14:56.760 --> 02:14:58.540]   the vast majority of people in the field
[02:14:58.540 --> 02:14:59.980]   are extremely junior.
[02:14:59.980 --> 02:15:00.820]   - Yeah.
[02:15:00.820 --> 02:15:03.220]   - So as a consequence, and that's just a consequence
[02:15:03.220 --> 02:15:04.860]   of the field growing, right?
[02:15:04.860 --> 02:15:07.860]   So as the number of, as the size of the field
[02:15:07.860 --> 02:15:10.100]   kind of starts saturating, you will have less
[02:15:10.100 --> 02:15:15.100]   of that problem of reviewers being very inexperienced.
[02:15:15.100 --> 02:15:20.160]   A consequence of this is that young reviewers,
[02:15:20.160 --> 02:15:22.860]   I mean, there's a phenomenon which is that
[02:15:22.860 --> 02:15:24.620]   reviewers try to make their life easy
[02:15:24.620 --> 02:15:27.460]   and to make their life easy when reviewing a paper
[02:15:27.460 --> 02:15:29.700]   is very simple, you just have to find a flaw in the paper.
[02:15:29.700 --> 02:15:30.540]   Right?
[02:15:30.540 --> 02:15:34.500]   So basically they see their task as finding flaws in papers
[02:15:34.500 --> 02:15:36.740]   and most papers have flaws, even the good ones.
[02:15:36.740 --> 02:15:38.140]   - Yeah.
[02:15:38.140 --> 02:15:41.500]   - And so it's easy to do that.
[02:15:41.500 --> 02:15:46.420]   Your job is easier as a reviewer if you just focus on this.
[02:15:46.420 --> 02:15:50.840]   But what's important is like, is there a new idea
[02:15:50.840 --> 02:15:54.120]   in that paper that is likely to influence?
[02:15:54.120 --> 02:15:56.240]   It doesn't matter if the experiments are not that great,
[02:15:56.240 --> 02:16:00.680]   if the protocol is, you know, so-so, you know,
[02:16:00.680 --> 02:16:05.040]   things like that, as long as there is a worthy idea in it
[02:16:05.040 --> 02:16:08.080]   that will influence the way people think about the problem,
[02:16:08.080 --> 02:16:11.160]   even if they make it better, you know, eventually,
[02:16:11.160 --> 02:16:15.460]   I think that's really what makes a paper useful.
[02:16:15.460 --> 02:16:19.480]   And so this combination of social phenomena
[02:16:19.480 --> 02:16:24.120]   creates a disease that has plagued, you know,
[02:16:24.120 --> 02:16:26.640]   other fields in the past, like speech recognition,
[02:16:26.640 --> 02:16:28.520]   where basically, you know, people chase numbers
[02:16:28.520 --> 02:16:33.520]   on benchmarks and it's much easier to get a paper accepted
[02:16:33.520 --> 02:16:37.000]   if it brings an incremental improvement
[02:16:37.000 --> 02:16:42.000]   on a sort of mainstream, well-accepted method or problem.
[02:16:43.800 --> 02:16:46.020]   And those are, to me, boring papers.
[02:16:46.020 --> 02:16:47.860]   I mean, they're not useless, right?
[02:16:47.860 --> 02:16:52.340]   Because industry, you know, strives on those kind of progress
[02:16:52.340 --> 02:16:54.020]   but they're not the one that I'm interested in
[02:16:54.020 --> 02:16:55.620]   in terms of like new concepts and new ideas.
[02:16:55.620 --> 02:16:59.260]   So papers that are really trying to strike
[02:16:59.260 --> 02:17:02.560]   kind of new advances generally don't make it.
[02:17:02.560 --> 02:17:04.200]   Now, thankfully, we have Archive.
[02:17:04.200 --> 02:17:05.260]   - Archive, exactly.
[02:17:05.260 --> 02:17:08.820]   And then there's open review type of situations where you,
[02:17:08.820 --> 02:17:11.620]   and then, I mean, Twitter is a kind of open review.
[02:17:11.620 --> 02:17:13.840]   I'm a huge believer that review should be done
[02:17:13.840 --> 02:17:15.680]   by thousands of people, not two people.
[02:17:15.680 --> 02:17:16.700]   - I agree.
[02:17:16.700 --> 02:17:19.540]   - And so Archive, like do you see a future
[02:17:19.540 --> 02:17:21.200]   where a lot of really strong papers,
[02:17:21.200 --> 02:17:23.620]   it's already the present, but a growing future
[02:17:23.620 --> 02:17:25.320]   where it'll just be Archive
[02:17:25.320 --> 02:17:31.260]   and you're presenting an ongoing, continuous conference
[02:17:31.260 --> 02:17:35.540]   called Twitter/the Internet/Archive Sanity.
[02:17:35.540 --> 02:17:38.000]   Andre just released a new version.
[02:17:38.000 --> 02:17:40.880]   So just not, you know, not being so elitist
[02:17:40.880 --> 02:17:43.420]   about this particular gating.
[02:17:43.420 --> 02:17:44.940]   - It's not a question of being elitist or not.
[02:17:44.940 --> 02:17:49.940]   It's a question of being basically recommendation
[02:17:49.940 --> 02:17:53.340]   and seal of approvals for people who don't see themselves
[02:17:53.340 --> 02:17:55.900]   as having the ability to do so by themselves, right?
[02:17:55.900 --> 02:17:57.300]   And so it saves time, right?
[02:17:57.300 --> 02:17:59.980]   If you rely on other people's opinion
[02:17:59.980 --> 02:18:03.700]   and you trust those people or those groups
[02:18:03.700 --> 02:18:07.300]   to evaluate a paper for you,
[02:18:07.300 --> 02:18:09.920]   that saves you time.
[02:18:09.920 --> 02:18:13.340]   'Cause you don't have to like scrutinize the paper as much.
[02:18:13.340 --> 02:18:15.140]   It is brought to your attention.
[02:18:15.140 --> 02:18:15.980]   I mean, there's the whole idea
[02:18:15.980 --> 02:18:18.700]   of sort of collective recommender system, right?
[02:18:18.700 --> 02:18:21.180]   So I actually thought about this a lot,
[02:18:21.180 --> 02:18:24.140]   you know, about 10, 15 years ago,
[02:18:24.140 --> 02:18:27.020]   'cause there were discussions at NIPS
[02:18:27.020 --> 02:18:31.220]   and we're about to create iClear with Yoshua Bengio.
[02:18:31.220 --> 02:18:34.820]   And so I wrote a document kind of describing
[02:18:34.820 --> 02:18:38.020]   a reviewing system, which basically was,
[02:18:38.020 --> 02:18:39.660]   you post your paper on some repository,
[02:18:39.660 --> 02:18:42.540]   let's say archive or now could be open review.
[02:18:42.540 --> 02:18:46.200]   And then you can form a reviewing entity,
[02:18:46.200 --> 02:18:48.120]   which is equivalent to a reviewing board,
[02:18:48.120 --> 02:18:52.320]   you know, of a journal or a program committee
[02:18:52.320 --> 02:18:53.940]   of a conference.
[02:18:53.940 --> 02:18:55.540]   You have to list the members
[02:18:55.540 --> 02:18:59.400]   and then that group, reviewing entity,
[02:18:59.400 --> 02:19:02.560]   can choose to review a particular paper,
[02:19:02.560 --> 02:19:03.680]   spontaneously or not.
[02:19:03.680 --> 02:19:05.580]   There is no exclusive relationship anymore
[02:19:05.580 --> 02:19:09.160]   between a paper and a venue or reviewing entity.
[02:19:09.160 --> 02:19:11.160]   Any reviewing entity can review any paper
[02:19:11.160 --> 02:19:14.080]   or may choose not to.
[02:19:14.080 --> 02:19:16.640]   And then, you know, give an evaluation.
[02:19:16.640 --> 02:19:17.920]   It's not published, not published,
[02:19:17.920 --> 02:19:20.320]   it's just an evaluation and a comment,
[02:19:20.320 --> 02:19:23.660]   which would be public, signed by the reviewing entity.
[02:19:23.660 --> 02:19:25.880]   And if it's signed by a reviewing entity,
[02:19:25.880 --> 02:19:27.760]   you know, it's one of the members of reviewing entity.
[02:19:27.760 --> 02:19:30.680]   So if the reviewing entity is, you know,
[02:19:30.680 --> 02:19:33.700]   Lex Friedman's, you know, preferred papers, right?
[02:19:33.700 --> 02:19:35.620]   You know, it's Lex Friedman writing the review.
[02:19:35.620 --> 02:19:36.460]   - Yes.
[02:19:36.700 --> 02:19:40.920]   So for me, that's a beautiful system, I think.
[02:19:40.920 --> 02:19:42.900]   But what's in addition to that,
[02:19:42.900 --> 02:19:45.800]   it feels like there should be a reputation system
[02:19:45.800 --> 02:19:47.480]   for the reviewers.
[02:19:47.480 --> 02:19:49.020]   - For the reviewing entities,
[02:19:49.020 --> 02:19:50.260]   not the reviewers individually.
[02:19:50.260 --> 02:19:51.700]   - The reviewing entities, sure.
[02:19:51.700 --> 02:19:53.900]   But even within that, the reviewers too,
[02:19:53.900 --> 02:19:57.140]   because there's another thing here.
[02:19:57.140 --> 02:19:59.340]   It's not just the reputation,
[02:19:59.340 --> 02:20:02.780]   it's an incentive for an individual person to do great.
[02:20:02.780 --> 02:20:05.060]   Right now, in the academic setting,
[02:20:05.060 --> 02:20:07.900]   the incentive is kind of internal,
[02:20:07.900 --> 02:20:09.240]   just wanting to do a good job.
[02:20:09.240 --> 02:20:11.260]   But honestly, that's not a strong enough incentive
[02:20:11.260 --> 02:20:13.700]   to do a really good job in reading a paper,
[02:20:13.700 --> 02:20:16.380]   in finding the beautiful amidst the mistakes and the flaws
[02:20:16.380 --> 02:20:17.780]   and all that kind of stuff.
[02:20:17.780 --> 02:20:19.220]   Like, if you're the person
[02:20:19.220 --> 02:20:22.420]   that first discovered a powerful paper,
[02:20:22.420 --> 02:20:25.100]   and you get to be proud of that discovery,
[02:20:25.100 --> 02:20:27.740]   then that gives a huge incentive to you.
[02:20:27.740 --> 02:20:29.300]   - That's a big part of my proposal, actually,
[02:20:29.300 --> 02:20:31.260]   where I describe that as, you know,
[02:20:31.700 --> 02:20:33.700]   if your evaluation of papers
[02:20:33.700 --> 02:20:37.500]   is predictive of future success,
[02:20:37.500 --> 02:20:40.900]   then your reputation should go up as a reviewing entity.
[02:20:40.900 --> 02:20:43.740]   So, yeah, exactly.
[02:20:43.740 --> 02:20:46.260]   I mean, I even had a master's student
[02:20:46.260 --> 02:20:49.540]   who was a master's student in library science
[02:20:49.540 --> 02:20:50.380]   and computer science,
[02:20:50.380 --> 02:20:52.460]   actually kind of work out exactly
[02:20:52.460 --> 02:20:55.100]   how that should work with formulas and everything.
[02:20:55.100 --> 02:20:56.780]   - So, in terms of implementation,
[02:20:56.780 --> 02:20:58.580]   do you think that's something that's doable?
[02:20:58.580 --> 02:21:00.740]   - I mean, I've been sort of talking about this
[02:21:00.740 --> 02:21:02.940]   to sort of various people like, you know,
[02:21:02.940 --> 02:21:05.900]   Andrew McCallum, who started Open Review.
[02:21:05.900 --> 02:21:07.780]   And the reason why we picked Open Review
[02:21:07.780 --> 02:21:09.060]   for iClear initially,
[02:21:09.060 --> 02:21:11.380]   even though it was very early for them,
[02:21:11.380 --> 02:21:14.260]   is because my hope was that iClear,
[02:21:14.260 --> 02:21:16.700]   it was eventually going to kind of
[02:21:16.700 --> 02:21:18.580]   inaugurate this type of system.
[02:21:18.580 --> 02:21:22.220]   So iClear kept the idea of open reviews.
[02:21:22.220 --> 02:21:23.820]   So where the reviews are, you know,
[02:21:23.820 --> 02:21:27.300]   published with a paper, which I think is very useful.
[02:21:27.300 --> 02:21:29.740]   But in many ways, that's kind of reverted
[02:21:29.740 --> 02:21:33.260]   to kind of more of a conventional type conferences
[02:21:33.260 --> 02:21:34.100]   for everything else.
[02:21:34.100 --> 02:21:37.780]   And that, I mean, I don't run iClear,
[02:21:37.780 --> 02:21:41.180]   I'm just the president of the foundation,
[02:21:41.180 --> 02:21:44.100]   but, you know, people who run it
[02:21:44.100 --> 02:21:45.620]   should make decisions about how to run it.
[02:21:45.620 --> 02:21:47.340]   And I'm not going to tell them
[02:21:47.340 --> 02:21:48.500]   because they are volunteers
[02:21:48.500 --> 02:21:50.300]   and I'm really thankful that they do that.
[02:21:50.300 --> 02:21:53.820]   So, but I'm saddened by the fact that
[02:21:53.820 --> 02:21:57.060]   we're not being innovative enough.
[02:21:57.060 --> 02:21:57.900]   - Yeah, me too.
[02:21:57.900 --> 02:21:59.660]   I hope that changes.
[02:21:59.660 --> 02:22:02.060]   Yeah, 'cause the communication of science broadly,
[02:22:02.060 --> 02:22:04.420]   but the communication of computer science ideas
[02:22:04.420 --> 02:22:08.300]   is how you make those ideas have impact, I think.
[02:22:08.300 --> 02:22:11.420]   - Yeah, and I think, you know, a lot of this is
[02:22:11.420 --> 02:22:16.220]   because people have in their mind kind of an objective,
[02:22:16.220 --> 02:22:19.100]   which is, you know, fairness for authors
[02:22:19.100 --> 02:22:22.260]   and the ability to count points basically
[02:22:22.260 --> 02:22:24.860]   and give credits accurately.
[02:22:24.860 --> 02:22:28.860]   But that comes at the expense of the progress of science.
[02:22:28.860 --> 02:22:29.700]   So to some extent,
[02:22:29.700 --> 02:22:32.140]   we're slowing down the progress of science.
[02:22:32.140 --> 02:22:34.420]   - And are we actually achieving fairness?
[02:22:34.420 --> 02:22:36.460]   - And we're not achieving fairness, you know,
[02:22:36.460 --> 02:22:38.060]   we still have biases, you know,
[02:22:38.060 --> 02:22:39.780]   we're doing, you know, a double-blind review,
[02:22:39.780 --> 02:22:44.340]   but, you know, the biases are still there.
[02:22:44.340 --> 02:22:46.700]   There are different kinds of biases.
[02:22:46.700 --> 02:22:49.340]   - You write that the phenomenon of emergence,
[02:22:49.340 --> 02:22:51.660]   collective behavior exhibited by a large collection
[02:22:51.660 --> 02:22:54.220]   of simple elements in interaction
[02:22:54.220 --> 02:22:55.700]   is one of the things that got you
[02:22:55.700 --> 02:22:57.740]   into neural nets in the first place.
[02:22:57.740 --> 02:22:59.060]   I love cellular automata,
[02:22:59.060 --> 02:23:01.940]   I love simple interacting elements
[02:23:01.940 --> 02:23:04.020]   and the things that emerge from them.
[02:23:04.020 --> 02:23:07.260]   Do you think we understand how complex systems
[02:23:07.260 --> 02:23:09.580]   can emerge from such simple components
[02:23:09.580 --> 02:23:11.020]   that interact simply?
[02:23:11.020 --> 02:23:12.260]   - No, we don't.
[02:23:12.260 --> 02:23:13.100]   It's a big mystery.
[02:23:13.100 --> 02:23:14.460]   Also, it's a mystery for physicists,
[02:23:14.460 --> 02:23:16.020]   it's a mystery for biologists.
[02:23:16.020 --> 02:23:22.060]   You know, how is it that the universe around us
[02:23:22.060 --> 02:23:25.140]   seems to be increasing in complexity and not decreasing?
[02:23:25.140 --> 02:23:29.620]   I mean, that is a kind of curious property of physics
[02:23:29.620 --> 02:23:32.340]   that despite the second law of thermodynamics,
[02:23:32.340 --> 02:23:35.940]   we seem to be, you know, evolution and learning
[02:23:35.940 --> 02:23:39.620]   and et cetera seems to be kind of at least locally
[02:23:39.620 --> 02:23:43.980]   to increase complexity, not decrease it.
[02:23:43.980 --> 02:23:46.500]   So perhaps the ultimate purpose of the universe
[02:23:46.500 --> 02:23:49.060]   is to just get more complex.
[02:23:49.060 --> 02:23:54.060]   - Have these, I mean, small pockets of beautiful complexity.
[02:23:55.060 --> 02:23:57.100]   Does that, does cellular automata,
[02:23:57.100 --> 02:23:59.660]   do these kinds of emergence of complex systems
[02:23:59.660 --> 02:24:04.100]   give you some intuition or guide your understanding
[02:24:04.100 --> 02:24:06.660]   of machine learning systems and neural networks and so on?
[02:24:06.660 --> 02:24:09.420]   Or are these for you right now, disparate concepts?
[02:24:09.420 --> 02:24:10.860]   - Well, it got me into it.
[02:24:10.860 --> 02:24:15.580]   You know, I discovered the existence of the perceptron
[02:24:15.580 --> 02:24:18.540]   when I was a college student, you know,
[02:24:18.540 --> 02:24:20.940]   by reading a book on, it was a debate between Chomsky
[02:24:20.940 --> 02:24:24.180]   and Piaget and Seymour Papert from MIT
[02:24:24.180 --> 02:24:26.620]   was kind of singing the praise of the perceptron
[02:24:26.620 --> 02:24:27.460]   in that book.
[02:24:27.460 --> 02:24:29.740]   And I, the first time I heard about the learning machine,
[02:24:29.740 --> 02:24:31.340]   right, so I started digging the literature
[02:24:31.340 --> 02:24:33.540]   and I found those books,
[02:24:33.540 --> 02:24:36.020]   which were basically transcription of, you know,
[02:24:36.020 --> 02:24:39.860]   workshops or conferences from the 50s and 60s
[02:24:39.860 --> 02:24:42.140]   about self-organizing systems.
[02:24:42.140 --> 02:24:44.540]   So there were, there was a series of conferences
[02:24:44.540 --> 02:24:48.140]   on self-organizing systems and these books on this.
[02:24:48.140 --> 02:24:50.180]   Some of them are, you can actually get them
[02:24:50.180 --> 02:24:53.220]   at the internet archive, you know, the digital version.
[02:24:53.220 --> 02:24:58.260]   And there are like fascinating articles in there by,
[02:24:58.260 --> 02:25:00.340]   there's a guy whose name has been largely forgotten,
[02:25:00.340 --> 02:25:01.740]   Heinz von Foerster.
[02:25:01.740 --> 02:25:06.180]   So it was a German physicist who immigrated to the US
[02:25:06.180 --> 02:25:11.180]   and worked on self-organizing systems in the 50s.
[02:25:11.180 --> 02:25:12.860]   And in the 60s, he created,
[02:25:12.860 --> 02:25:14.420]   at University of Illinois Urbana-Champaign,
[02:25:14.420 --> 02:25:18.900]   he created the biological computer laboratory, BCL,
[02:25:18.900 --> 02:25:21.580]   which was, you know, all about neural nets.
[02:25:21.580 --> 02:25:23.340]   Unfortunately, that was kind of towards the end
[02:25:23.340 --> 02:25:24.820]   of the popularity of neural nets.
[02:25:24.820 --> 02:25:27.660]   So that lab never kind of strived very much,
[02:25:27.660 --> 02:25:30.260]   but he wrote a bunch of papers about self-organization
[02:25:30.260 --> 02:25:33.420]   and about the mystery of self-organization.
[02:25:33.420 --> 02:25:35.620]   An example he has is, you take,
[02:25:35.620 --> 02:25:37.980]   imagine you are in space, there's no gravity,
[02:25:37.980 --> 02:25:42.100]   you have a big box with magnets in it, okay?
[02:25:42.100 --> 02:25:43.820]   You know, kind of rectangular magnets
[02:25:43.820 --> 02:25:46.820]   with North Pole on one end, South Pole on the other end.
[02:25:46.820 --> 02:25:48.980]   You shake the box gently and the magnets
[02:25:48.980 --> 02:25:50.100]   will kind of stick to themselves
[02:25:50.100 --> 02:25:53.660]   and probably form a complex structure, you know,
[02:25:53.660 --> 02:25:55.420]   spontaneously, you know,
[02:25:55.420 --> 02:25:57.020]   that could be an example of self-organization.
[02:25:57.020 --> 02:25:58.340]   But, you know, you have lots of examples,
[02:25:58.340 --> 02:26:01.180]   neural nets are an example of self-organization too,
[02:26:01.180 --> 02:26:02.980]   you know, in many respect.
[02:26:02.980 --> 02:26:05.900]   And it's a bit of a mystery, you know,
[02:26:05.900 --> 02:26:09.420]   how, like what is possible with this, you know,
[02:26:09.420 --> 02:26:11.940]   pattern formation in physical systems,
[02:26:11.940 --> 02:26:14.700]   in chaotic system and things like that, you know,
[02:26:14.700 --> 02:26:16.860]   the emergence of life, you know, things like that.
[02:26:16.860 --> 02:26:19.540]   So, you know, how does that happen?
[02:26:19.540 --> 02:26:22.540]   So it's a big puzzle for physicists as well.
[02:26:22.540 --> 02:26:24.660]   - It feels like understanding this,
[02:26:24.660 --> 02:26:29.660]   the mathematics of emergence in some constrained situations
[02:26:29.660 --> 02:26:32.060]   might help us create intelligence.
[02:26:32.060 --> 02:26:35.980]   Like help us add a little spice to the systems
[02:26:35.980 --> 02:26:39.500]   because you seem to be able to,
[02:26:39.500 --> 02:26:41.900]   in complex systems with emergence,
[02:26:41.900 --> 02:26:44.620]   to be able to get a lot from little.
[02:26:44.620 --> 02:26:47.020]   And so that seems like a shortcut
[02:26:47.020 --> 02:26:49.700]   to get big leaps in performance.
[02:26:49.700 --> 02:26:51.100]   But-
[02:26:51.100 --> 02:26:53.660]   - But there's a missing theoretical concept
[02:26:53.660 --> 02:26:55.020]   that we don't have.
[02:26:55.020 --> 02:26:55.860]   - Yeah.
[02:26:55.860 --> 02:26:58.420]   - And it's something also I've been fascinated by
[02:26:58.420 --> 02:27:00.700]   since my undergrad days.
[02:27:00.700 --> 02:27:03.900]   And it's how you measure complexity, right?
[02:27:03.900 --> 02:27:06.940]   So we don't actually have good ways of measuring,
[02:27:06.940 --> 02:27:09.860]   or at least we don't have good ways of interpreting
[02:27:09.860 --> 02:27:11.940]   the measures that we have at our disposal.
[02:27:11.940 --> 02:27:14.500]   Like how do you measure the complexity of something, right?
[02:27:14.500 --> 02:27:15.660]   So there's all those things, you know,
[02:27:15.660 --> 02:27:18.540]   like, you know, Kolmogorov, Chaitin, Solomonov complexity
[02:27:18.540 --> 02:27:20.940]   of, you know, the length of the shortest program
[02:27:20.940 --> 02:27:22.460]   that would generate a bit string
[02:27:22.460 --> 02:27:25.220]   can be thought of as the complexity of that bit string.
[02:27:25.220 --> 02:27:26.060]   - Mm-hmm.
[02:27:26.060 --> 02:27:28.180]   - I've been fascinated by that concept.
[02:27:28.180 --> 02:27:32.380]   The problem with that is that that complexity
[02:27:32.380 --> 02:27:34.980]   is defined up to a constant, which can be very large.
[02:27:34.980 --> 02:27:36.740]   - Right.
[02:27:36.740 --> 02:27:38.860]   - There are similar concepts that are derived from,
[02:27:38.860 --> 02:27:43.340]   you know, Bayesian probability theory,
[02:27:43.340 --> 02:27:45.580]   where, you know, the complexity of something
[02:27:45.580 --> 02:27:49.460]   is the negative log of its probability, essentially, right?
[02:27:49.460 --> 02:27:52.260]   And you have a complete equivalence between the two things.
[02:27:52.260 --> 02:27:53.180]   And there you would think, you know,
[02:27:53.180 --> 02:27:54.420]   the probability is something
[02:27:54.420 --> 02:27:56.220]   that's well-defined mathematically,
[02:27:56.220 --> 02:27:58.220]   which means complexity is well-defined.
[02:27:58.220 --> 02:27:59.060]   But it's not true.
[02:27:59.060 --> 02:28:02.660]   You need to have a model of the distribution.
[02:28:02.660 --> 02:28:03.780]   You may need to have a prior
[02:28:03.780 --> 02:28:05.100]   if you're doing Bayesian inference.
[02:28:05.100 --> 02:28:06.580]   And the prior plays the same role
[02:28:06.580 --> 02:28:07.940]   as the choice of the computer
[02:28:07.940 --> 02:28:10.460]   with which you measure Kolmogorov complexity.
[02:28:10.460 --> 02:28:12.980]   And so every measure of complexity we have
[02:28:12.980 --> 02:28:14.500]   has some arbitrariness in it.
[02:28:14.500 --> 02:28:17.740]   You know, an additive constant,
[02:28:17.740 --> 02:28:20.500]   which can be arbitrarily large.
[02:28:20.500 --> 02:28:24.260]   And so, you know, how can we come up with a good theory
[02:28:24.260 --> 02:28:25.580]   of how things become more complex
[02:28:25.580 --> 02:28:26.900]   if we don't have a good measure of complexity?
[02:28:26.900 --> 02:28:28.580]   - Yeah, which we need for,
[02:28:28.580 --> 02:28:31.500]   there's one way that people study this
[02:28:31.500 --> 02:28:32.980]   in the space of biology,
[02:28:32.980 --> 02:28:34.540]   the people that study the origin of life
[02:28:34.540 --> 02:28:37.820]   or try to recreate life in the laboratory.
[02:28:37.820 --> 02:28:39.860]   And the more interesting one is the alien one,
[02:28:39.860 --> 02:28:42.020]   is when we go to other planets,
[02:28:42.020 --> 02:28:44.700]   how would we recognize this life?
[02:28:44.700 --> 02:28:47.500]   'Cause, you know, complexity, we associate complexity,
[02:28:47.500 --> 02:28:49.820]   maybe some level of mobility with life.
[02:28:49.820 --> 02:28:55.700]   You know, we have to be able to like have concrete algorithms
[02:28:55.700 --> 02:29:00.780]   for like measuring the level of complexity we see
[02:29:00.780 --> 02:29:03.340]   in order to know the difference between life and non-life.
[02:29:03.340 --> 02:29:04.620]   - And the problem is that complexity
[02:29:04.620 --> 02:29:06.060]   is in the eye of the beholder.
[02:29:06.060 --> 02:29:08.060]   So let me give you an example.
[02:29:08.100 --> 02:29:13.100]   If I give you an image of the MNIST digits, right?
[02:29:13.100 --> 02:29:16.020]   And I flip through MNIST digits,
[02:29:16.020 --> 02:29:18.700]   there is some, obviously some structure to it
[02:29:18.700 --> 02:29:21.060]   because local structure, you know,
[02:29:21.060 --> 02:29:22.780]   neighboring pixels are correlated
[02:29:22.780 --> 02:29:26.140]   across the entire dataset.
[02:29:26.140 --> 02:29:30.980]   I imagine that I apply a random permutation
[02:29:30.980 --> 02:29:34.580]   to all the pixels, a fixed random permutation.
[02:29:34.580 --> 02:29:37.980]   I show you those images, they will look, you know,
[02:29:37.980 --> 02:29:40.420]   really disorganized to you, more complex.
[02:29:40.420 --> 02:29:43.500]   In fact, they're not more complex in absolute terms,
[02:29:43.500 --> 02:29:46.100]   they're exactly the same as originally, right?
[02:29:46.100 --> 02:29:47.860]   And if you knew what the permutation was, you know,
[02:29:47.860 --> 02:29:50.020]   you could undo the permutation.
[02:29:50.020 --> 02:29:52.900]   Now, imagine I give you special glasses
[02:29:52.900 --> 02:29:54.700]   that undo that permutation.
[02:29:54.700 --> 02:29:57.620]   Now, all of a sudden, what looked complicated becomes simple.
[02:29:57.620 --> 02:29:58.460]   - Right.
[02:29:58.460 --> 02:30:00.900]   - So if you have two, if you have, you know,
[02:30:00.900 --> 02:30:03.820]   humans on one end and then another race of aliens
[02:30:03.820 --> 02:30:05.980]   that sees the universe with permutation glasses.
[02:30:05.980 --> 02:30:07.460]   - Yeah, with the permutation glasses.
[02:30:07.460 --> 02:30:08.740]   (Lex laughing)
[02:30:08.740 --> 02:30:11.380]   - What we perceive as simple to them is hardly complicated,
[02:30:11.380 --> 02:30:12.340]   it's probably heat.
[02:30:12.340 --> 02:30:13.540]   - Yeah, heat, yeah.
[02:30:13.540 --> 02:30:15.900]   - Okay, and what they perceive as simple to us
[02:30:15.900 --> 02:30:19.060]   is random fluctuation, it's heat.
[02:30:19.060 --> 02:30:20.460]   - Yeah.
[02:30:20.460 --> 02:30:21.300]   - So-
[02:30:21.300 --> 02:30:22.780]   - Truly in the eye of the beholder,
[02:30:22.780 --> 02:30:24.940]   depends what kind of glasses you're wearing.
[02:30:24.940 --> 02:30:25.780]   - Right.
[02:30:25.780 --> 02:30:26.860]   - Depends what kind of algorithm you're running
[02:30:26.860 --> 02:30:28.380]   in your perception system.
[02:30:28.380 --> 02:30:31.140]   - So I don't think we'll have a theory of intelligence,
[02:30:31.140 --> 02:30:34.380]   self-organization, evolution, things like this
[02:30:34.380 --> 02:30:38.540]   until we have a good handle on a notion of complexity,
[02:30:38.540 --> 02:30:40.860]   which we know is in the eye of the beholder.
[02:30:40.860 --> 02:30:44.420]   - Yeah, it's sad to think that we might not be able
[02:30:44.420 --> 02:30:47.620]   to detect or interact with alien species
[02:30:47.620 --> 02:30:50.340]   because we're wearing different glasses.
[02:30:50.340 --> 02:30:51.500]   - Because the notion of locality
[02:30:51.500 --> 02:30:52.460]   might be different from ours.
[02:30:52.460 --> 02:30:53.300]   - Yeah, exactly.
[02:30:53.300 --> 02:30:55.260]   - This actually connects with fascinating questions
[02:30:55.260 --> 02:30:58.140]   in physics at the moment, like modern physics,
[02:30:58.140 --> 02:31:00.300]   quantum physics, like, you know, questions about,
[02:31:00.300 --> 02:31:02.580]   like, you know, can we recover the information
[02:31:02.580 --> 02:31:04.620]   that's lost in a black hole and things like this, right?
[02:31:04.620 --> 02:31:07.980]   And that relies on notions of complexity,
[02:31:07.980 --> 02:31:11.700]   which, you know, I find this fascinating.
[02:31:11.700 --> 02:31:13.420]   - Can you describe your personal quest
[02:31:13.420 --> 02:31:18.420]   to build an expressive electronic wind instrument, EWI?
[02:31:18.420 --> 02:31:20.660]   What is it?
[02:31:20.660 --> 02:31:24.060]   What does it take to build it?
[02:31:24.060 --> 02:31:25.140]   - Well, I'm a tinkerer.
[02:31:25.140 --> 02:31:26.820]   I like building things.
[02:31:26.820 --> 02:31:29.020]   I like building things with combinations of electronics
[02:31:29.020 --> 02:31:31.120]   and, you know, mechanical stuff.
[02:31:32.460 --> 02:31:34.140]   You know, I have a bunch of different hobbies,
[02:31:34.140 --> 02:31:38.020]   but, you know, probably my first one was little,
[02:31:38.020 --> 02:31:39.820]   was building model airplanes and stuff like that.
[02:31:39.820 --> 02:31:41.900]   And I still do that to some extent,
[02:31:41.900 --> 02:31:42.740]   but also electronics.
[02:31:42.740 --> 02:31:45.180]   I taught myself electronics before I studied it.
[02:31:45.180 --> 02:31:48.140]   And the reason I taught myself electronics
[02:31:48.140 --> 02:31:49.620]   is because of music.
[02:31:49.620 --> 02:31:53.180]   My cousin was an aspiring electronic musician
[02:31:53.180 --> 02:31:55.020]   and he had an analog synthesizer.
[02:31:55.020 --> 02:31:58.020]   And I was, you know, basically modifying it for him
[02:31:58.020 --> 02:32:00.260]   and building sequencers and stuff like that, right, for him.
[02:32:00.260 --> 02:32:02.620]   I was in high school when I was doing this.
[02:32:02.620 --> 02:32:06.060]   - How's the interest in like progressive rock, like '80s?
[02:32:06.060 --> 02:32:07.980]   Like what's the greatest band of all time,
[02:32:07.980 --> 02:32:09.500]   according to Yann LeCun?
[02:32:09.500 --> 02:32:11.100]   - Oh, man, there's too many of them.
[02:32:11.100 --> 02:32:16.100]   But, you know, it's a combination of, you know,
[02:32:16.100 --> 02:32:19.820]   Mahavishnu Orchestra, Weather Report,
[02:32:19.820 --> 02:32:22.780]   Yes, Genesis, you know,
[02:32:22.780 --> 02:32:23.980]   - Yes, Genesis.
[02:32:23.980 --> 02:32:28.100]   - Pre-Peter Gabriel, Gentle Giant, you know,
[02:32:28.100 --> 02:32:29.100]   things like that.
[02:32:29.100 --> 02:32:29.940]   - Great.
[02:32:29.940 --> 02:32:32.300]   Okay, so this love of electronics
[02:32:32.300 --> 02:32:34.260]   and this love of music combined together.
[02:32:34.260 --> 02:32:36.340]   - Right, so I was actually trained to play
[02:32:36.340 --> 02:32:39.500]   Baroque and Renaissance music.
[02:32:39.500 --> 02:32:43.300]   And I played in an orchestra when I was in high school
[02:32:43.300 --> 02:32:45.780]   and first year of college.
[02:32:45.780 --> 02:32:48.060]   And I played the recorder, cram horn,
[02:32:48.060 --> 02:32:50.220]   a little bit of oboe, you know, things like that.
[02:32:50.220 --> 02:32:52.540]   So I'm a wind instrument player.
[02:32:52.540 --> 02:32:54.100]   But I always wanted to play improvised music,
[02:32:54.100 --> 02:32:56.340]   even though I don't know anything about it.
[02:32:56.340 --> 02:32:58.780]   And the only way I figured, you know,
[02:32:58.780 --> 02:33:01.060]   short of like learning to play a saxophone
[02:33:01.060 --> 02:33:03.540]   was to play electronic wind instruments.
[02:33:03.540 --> 02:33:04.540]   So they behave, you know,
[02:33:04.540 --> 02:33:06.380]   the fingering is similar to a saxophone,
[02:33:06.380 --> 02:33:09.060]   but, you know, you have wide variety of sound
[02:33:09.060 --> 02:33:11.020]   because you control the synthesizer with it.
[02:33:11.020 --> 02:33:13.100]   So I had a bunch of those, you know,
[02:33:13.100 --> 02:33:18.100]   going back to the late 80s from either Yamaha or Akai.
[02:33:18.100 --> 02:33:22.500]   They're both kind of the main manufacturers of those.
[02:33:22.500 --> 02:33:23.660]   So they were classically, you know,
[02:33:23.660 --> 02:33:25.700]   going back several decades.
[02:33:25.700 --> 02:33:27.660]   But I've never been completely satisfied with them
[02:33:27.660 --> 02:33:29.260]   because of lack of expressivity.
[02:33:29.260 --> 02:33:32.460]   And, you know, those things, you know,
[02:33:32.460 --> 02:33:33.420]   are somewhat expressive.
[02:33:33.420 --> 02:33:34.780]   I mean, they measure the breath pressure,
[02:33:34.780 --> 02:33:36.540]   they measure the lip pressure,
[02:33:36.540 --> 02:33:39.820]   and, you know, you have various parameters.
[02:33:39.820 --> 02:33:41.500]   You can vary it with fingers,
[02:33:41.500 --> 02:33:44.820]   but they're not really as expressive
[02:33:44.820 --> 02:33:47.100]   as an acoustic instrument, right?
[02:33:47.100 --> 02:33:49.420]   You hear John Coltrane play two notes
[02:33:49.420 --> 02:33:50.820]   and you know it's John Coltrane,
[02:33:50.820 --> 02:33:53.060]   you know, it's got a unique sound.
[02:33:53.060 --> 02:33:54.340]   Or Miles Davis, right?
[02:33:54.340 --> 02:33:57.540]   You can hear it's Miles Davis playing the trumpet
[02:33:57.540 --> 02:34:02.540]   because the sound reflects their, you know,
[02:34:02.540 --> 02:34:04.780]   physiognomy, basically.
[02:34:04.780 --> 02:34:09.700]   The shape of the vocal track kind of shapes the sound.
[02:34:09.700 --> 02:34:12.860]   So how do you do this with an electronic instrument?
[02:34:12.860 --> 02:34:16.140]   And I was, many years ago I met a guy called David Wessel.
[02:34:16.140 --> 02:34:18.780]   He was a professor at Berkeley
[02:34:18.780 --> 02:34:21.940]   and created the Center for like, you know,
[02:34:21.940 --> 02:34:23.500]   music technology there.
[02:34:23.500 --> 02:34:26.140]   And he was interested in that question.
[02:34:26.140 --> 02:34:28.620]   And so I kept kind of thinking about this for many years.
[02:34:28.620 --> 02:34:31.540]   And finally, because of COVID, you know, I was at home.
[02:34:31.540 --> 02:34:32.580]   I was in my workshop.
[02:34:32.580 --> 02:34:36.020]   My workshop serves also as my kind of Zoom room
[02:34:36.020 --> 02:34:37.340]   and home office.
[02:34:37.340 --> 02:34:38.780]   - This is in New Jersey?
[02:34:38.780 --> 02:34:39.620]   - In New Jersey.
[02:34:39.620 --> 02:34:43.580]   And I started really being serious about, you know,
[02:34:43.580 --> 02:34:45.780]   building my own EWI instrument.
[02:34:45.780 --> 02:34:48.140]   - What else is going on in that New Jersey workshop?
[02:34:48.140 --> 02:34:50.860]   Is there some crazy stuff you've built?
[02:34:50.860 --> 02:34:55.180]   Like just, or like left on the workshop floor, left behind?
[02:34:55.180 --> 02:34:57.580]   - A lot of crazy stuff is, you know,
[02:34:57.580 --> 02:35:01.660]   electronics built with microcontrollers of various kinds
[02:35:01.660 --> 02:35:04.860]   and, you know, weird flying contraptions.
[02:35:04.860 --> 02:35:08.700]   - So you still love flying?
[02:35:08.700 --> 02:35:09.860]   - It's a family disease.
[02:35:09.860 --> 02:35:12.620]   My dad got me into it when I was a kid
[02:35:12.620 --> 02:35:16.820]   and he was building model airplanes when he was a kid.
[02:35:16.820 --> 02:35:19.780]   And he was a mechanical engineer.
[02:35:19.780 --> 02:35:21.140]   He taught himself electronics also.
[02:35:21.140 --> 02:35:24.060]   So he built his early radio control systems
[02:35:24.060 --> 02:35:26.420]   in the late sixties, early seventies.
[02:35:26.420 --> 02:35:29.780]   And so that's what got me into, I mean,
[02:35:29.780 --> 02:35:31.060]   he got me into kind of, you know,
[02:35:31.060 --> 02:35:33.020]   engineering and science and technology.
[02:35:33.020 --> 02:35:36.100]   - Do you also have an interest in appreciation of flight
[02:35:36.100 --> 02:35:39.220]   in other forms, like with drones, quadropters, or do you,
[02:35:39.220 --> 02:35:42.700]   is it model airplane, the thing that's-
[02:35:42.700 --> 02:35:45.180]   - You know, before drones were, you know,
[02:35:45.180 --> 02:35:49.180]   kind of a consumer product, you know,
[02:35:49.180 --> 02:35:50.220]   I built my own, you know,
[02:35:50.220 --> 02:35:51.940]   with also building a microcontroller
[02:35:51.940 --> 02:35:56.220]   with a gyroscopes and accelerometers for stabilization,
[02:35:56.220 --> 02:35:57.700]   writing the firmware for it, you know.
[02:35:57.700 --> 02:35:59.140]   And then when it became kind of a standard thing
[02:35:59.140 --> 02:36:00.300]   you could buy, it was boring, you know,
[02:36:00.300 --> 02:36:02.460]   I stopped doing it, it was not fun anymore.
[02:36:02.460 --> 02:36:06.260]   - Yeah, you were doing it before it was cool.
[02:36:06.260 --> 02:36:07.100]   - Yeah.
[02:36:07.100 --> 02:36:10.020]   - What advice would you give to a young person today
[02:36:10.020 --> 02:36:13.780]   in high school and college that dreams of doing
[02:36:13.780 --> 02:36:15.940]   something big like Yann LeCun,
[02:36:15.940 --> 02:36:18.940]   like let's talk in the space of intelligence,
[02:36:18.940 --> 02:36:20.940]   dreams of having a chance to solve
[02:36:20.940 --> 02:36:23.980]   some fundamental problem in space of intelligence,
[02:36:23.980 --> 02:36:26.180]   both for their career and just in life,
[02:36:26.180 --> 02:36:30.700]   being somebody who was a part of creating something special?
[02:36:30.700 --> 02:36:35.420]   - So try to get interested by big questions,
[02:36:35.420 --> 02:36:38.660]   things like, you know, what is intelligence?
[02:36:38.660 --> 02:36:40.420]   What is the universe made of?
[02:36:40.420 --> 02:36:41.660]   What's life all about?
[02:36:41.660 --> 02:36:42.500]   Things like that.
[02:36:42.500 --> 02:36:49.060]   Like even like crazy big questions, like what's time?
[02:36:49.060 --> 02:36:50.620]   Like nobody knows what time is.
[02:36:51.460 --> 02:36:56.460]   And then learn basic things, like basic methods,
[02:36:56.460 --> 02:37:03.260]   either from math, from physics or from engineering.
[02:37:03.260 --> 02:37:05.620]   Things that have a long shelf life.
[02:37:05.620 --> 02:37:08.740]   Like if you have a choice between like, you know,
[02:37:08.740 --> 02:37:11.700]   learning, you know, mobile programming on iPhone
[02:37:11.700 --> 02:37:14.860]   or quantum mechanics, take quantum mechanics.
[02:37:14.860 --> 02:37:18.500]   Because you're gonna learn things
[02:37:18.500 --> 02:37:20.420]   that you have no idea exist.
[02:37:20.420 --> 02:37:25.340]   And you may not, you may never be a quantum physicist,
[02:37:25.340 --> 02:37:26.780]   but you'll learn about path integrals
[02:37:26.780 --> 02:37:29.140]   and path integrals are used everywhere.
[02:37:29.140 --> 02:37:31.100]   It's the same formula that you use for, you know,
[02:37:31.100 --> 02:37:33.300]   Bayesian integration and stuff like that.
[02:37:33.300 --> 02:37:38.100]   - So the ideas, the little ideas within quantum mechanics,
[02:37:38.100 --> 02:37:41.460]   within some of these kind of more solidified fields
[02:37:41.460 --> 02:37:42.660]   will have a longer shelf life.
[02:37:42.660 --> 02:37:46.940]   They'll somehow use indirectly in your work.
[02:37:46.940 --> 02:37:48.100]   - Learn classical mechanics,
[02:37:48.100 --> 02:37:50.420]   like you learn about Lagrangians, for example.
[02:37:50.420 --> 02:37:55.140]   Which is like a hugely useful concept, you know,
[02:37:55.140 --> 02:37:57.300]   for all kinds of different things.
[02:37:57.300 --> 02:38:01.660]   Learn statistical physics, because all the math
[02:38:01.660 --> 02:38:04.420]   that comes out of, you know, for machine learning,
[02:38:04.420 --> 02:38:07.260]   basically comes out of what's figured out
[02:38:07.260 --> 02:38:09.220]   by statistical physicists in the, you know,
[02:38:09.220 --> 02:38:10.940]   late 19th, early 20th century, right?
[02:38:10.940 --> 02:38:14.260]   So, and for some of them actually more recently,
[02:38:14.260 --> 02:38:16.100]   for by people like Giorgio Parisi,
[02:38:16.100 --> 02:38:19.060]   who just got the Nobel prize for the replica method,
[02:38:19.060 --> 02:38:23.180]   among other things, it's used for a lot of different things.
[02:38:23.180 --> 02:38:25.580]   You know, variational inference,
[02:38:25.580 --> 02:38:27.620]   that math comes from statistical physics.
[02:38:27.620 --> 02:38:33.580]   So, a lot of those kind of, you know, basic courses,
[02:38:33.580 --> 02:38:36.220]   you know, if you do electrical engineering,
[02:38:36.220 --> 02:38:37.620]   you take signal processing,
[02:38:37.620 --> 02:38:39.860]   you'll learn about Fourier transforms.
[02:38:39.860 --> 02:38:42.700]   Again, something super useful is at the basis
[02:38:42.700 --> 02:38:44.900]   of things like graph neural nets,
[02:38:44.900 --> 02:38:49.380]   which is an entirely new sub area of, you know,
[02:38:49.380 --> 02:38:50.660]   AI machine learning, deep learning,
[02:38:50.660 --> 02:38:52.140]   which I think is super promising
[02:38:52.140 --> 02:38:54.340]   for all kinds of applications.
[02:38:54.340 --> 02:38:55.220]   Something very promising,
[02:38:55.220 --> 02:38:56.660]   if you're more interested in applications,
[02:38:56.660 --> 02:38:58.820]   is the applications of AI machine learning
[02:38:58.820 --> 02:39:00.420]   and deep learning to science.
[02:39:00.420 --> 02:39:05.540]   Or to science that can help solve big problems in the world.
[02:39:05.540 --> 02:39:09.220]   I have colleagues at Meta, at FAIR,
[02:39:09.220 --> 02:39:11.220]   who started this project called Open Catalyst,
[02:39:11.220 --> 02:39:14.540]   and it's an open project collaborative.
[02:39:14.540 --> 02:39:16.620]   And the idea is to use deep learning
[02:39:16.620 --> 02:39:21.620]   to help design new chemical compounds or materials
[02:39:21.620 --> 02:39:23.740]   that would facilitate the separation
[02:39:23.740 --> 02:39:25.780]   of hydrogen from oxygen.
[02:39:25.780 --> 02:39:29.020]   If you can efficiently separate oxygen from hydrogen
[02:39:29.020 --> 02:39:33.500]   with electricity, you solve climate change.
[02:39:33.500 --> 02:39:34.420]   It's as simple as that.
[02:39:34.420 --> 02:39:37.580]   Because you cover, you know,
[02:39:37.580 --> 02:39:39.740]   some random desert with solar panels,
[02:39:39.740 --> 02:39:43.420]   and you have them work all day, produce hydrogen,
[02:39:43.420 --> 02:39:45.380]   and then you shoot the hydrogen wherever it's needed.
[02:39:45.380 --> 02:39:46.820]   You don't need anything else.
[02:39:46.820 --> 02:39:53.420]   You know, you have controllable power
[02:39:53.420 --> 02:39:55.620]   that's, you know, can be transported anywhere.
[02:39:55.620 --> 02:39:59.700]   So if we have a large-scale, efficient
[02:39:59.700 --> 02:40:04.180]   energy storage technology, like producing hydrogen,
[02:40:04.180 --> 02:40:06.620]   we solve climate change.
[02:40:06.620 --> 02:40:08.500]   Here's another way to solve climate change,
[02:40:08.500 --> 02:40:10.420]   is figuring out how to make fusion work.
[02:40:10.420 --> 02:40:11.460]   Now, the problem with fusion
[02:40:11.460 --> 02:40:13.580]   is that you make a super-hot plasma,
[02:40:13.580 --> 02:40:16.220]   and the plasma is unstable, and you can't control it.
[02:40:16.220 --> 02:40:17.940]   Maybe with deep learning, you can find controllers
[02:40:17.940 --> 02:40:19.100]   that will stabilize plasma
[02:40:19.100 --> 02:40:21.620]   and make, you know, practical fusion reactors.
[02:40:21.620 --> 02:40:23.060]   I mean, that's very speculative,
[02:40:23.060 --> 02:40:24.460]   but, you know, it's worth trying,
[02:40:24.460 --> 02:40:28.260]   because, you know, the payoff is huge.
[02:40:28.260 --> 02:40:29.900]   There's a group at Google working on this,
[02:40:29.900 --> 02:40:31.140]   led by John Platt.
[02:40:31.140 --> 02:40:33.900]   - So, control, convert as many problems
[02:40:33.900 --> 02:40:36.780]   in science and physics and biology and chemistry
[02:40:36.780 --> 02:40:39.740]   into a learnable problem,
[02:40:39.740 --> 02:40:41.540]   and see if a machine can learn it.
[02:40:41.540 --> 02:40:43.900]   - Right, I mean, there's properties of, you know,
[02:40:43.900 --> 02:40:46.300]   complex materials that we don't understand
[02:40:46.300 --> 02:40:48.540]   from first principle, for example, right?
[02:40:48.540 --> 02:40:53.060]   So, you know, if we could design new, you know,
[02:40:53.060 --> 02:40:56.420]   new materials, we could make more efficient batteries.
[02:40:56.420 --> 02:40:58.780]   You know, we could make maybe faster electronics.
[02:40:58.780 --> 02:41:01.900]   We could, I mean, there's a lot of things we can imagine
[02:41:01.900 --> 02:41:04.500]   doing, or, you know, lighter materials
[02:41:04.500 --> 02:41:06.420]   for cars or airplanes and things like that.
[02:41:06.420 --> 02:41:07.620]   Maybe better fuel cells.
[02:41:07.620 --> 02:41:09.500]   I mean, there's all kinds of stuff we can imagine.
[02:41:09.500 --> 02:41:12.300]   If we had good fuel cells, hydrogen fuel cells,
[02:41:12.300 --> 02:41:13.620]   we could use them to power airplanes,
[02:41:13.620 --> 02:41:17.220]   and, you know, transportation wouldn't be, or cars,
[02:41:17.220 --> 02:41:20.300]   we wouldn't have emission problem,
[02:41:20.300 --> 02:41:24.580]   CO2 emission problems for air transportation anymore.
[02:41:24.580 --> 02:41:26.500]   So, there's a lot of those things,
[02:41:26.500 --> 02:41:29.180]   I think, where AI, you know, can be used.
[02:41:29.180 --> 02:41:32.420]   And this is not even talking about all the sort of
[02:41:32.420 --> 02:41:35.660]   medicine, biology, and everything like that, right?
[02:41:35.660 --> 02:41:38.100]   You know, like protein folding, you know,
[02:41:38.100 --> 02:41:40.540]   figuring out, like, how can you design your proteins
[02:41:40.540 --> 02:41:42.820]   that it sticks to another protein at a particular site,
[02:41:42.820 --> 02:41:45.180]   because that's how you design drugs in the end.
[02:41:45.180 --> 02:41:47.580]   So, you know, deep learning would be useful,
[02:41:47.580 --> 02:41:49.260]   all of this, and those are kind of, you know,
[02:41:49.260 --> 02:41:51.100]   would be sort of enormous progress
[02:41:51.100 --> 02:41:53.380]   if we could use it for that.
[02:41:53.380 --> 02:41:54.300]   Here's an example.
[02:41:54.300 --> 02:41:58.260]   If you take, this is like from recent material physics,
[02:41:58.260 --> 02:42:02.180]   you take a monoatomic layer of graphene, right?
[02:42:02.180 --> 02:42:04.900]   So, it's just carbon on an hexagonal mesh,
[02:42:04.900 --> 02:42:09.140]   and you make this single atom thick.
[02:42:09.140 --> 02:42:10.340]   You put another one on top,
[02:42:10.340 --> 02:42:13.100]   you twist them by some magic number of degrees,
[02:42:13.100 --> 02:42:16.780]   three degrees or something, it becomes superconductor.
[02:42:16.780 --> 02:42:18.100]   Nobody has any idea why.
[02:42:18.100 --> 02:42:20.820]   (both laughing)
[02:42:20.820 --> 02:42:22.460]   - I wanna know how that was discovered,
[02:42:22.460 --> 02:42:23.900]   but that's the kind of thing that machine learning
[02:42:23.900 --> 02:42:25.820]   can actually discover, these kinds of things.
[02:42:25.820 --> 02:42:28.980]   - Maybe not, but there is a hint, perhaps,
[02:42:28.980 --> 02:42:31.740]   that with machine learning, we would train a system
[02:42:31.740 --> 02:42:34.860]   to basically be a phenomenological model
[02:42:34.860 --> 02:42:37.220]   of some complex emergent phenomenon,
[02:42:37.220 --> 02:42:40.380]   which superconductivity is one of those,
[02:42:40.380 --> 02:42:45.340]   where this collective phenomenon is too difficult
[02:42:45.340 --> 02:42:46.900]   to describe from first principles
[02:42:46.900 --> 02:42:51.900]   with the usual sort of reductionist type method.
[02:42:51.900 --> 02:42:54.940]   But we could have deep learning systems
[02:42:54.940 --> 02:42:57.660]   that predict the properties of a system
[02:42:57.660 --> 02:42:59.180]   from a description of it,
[02:42:59.180 --> 02:43:02.660]   after being trained with sufficiently many samples.
[02:43:02.660 --> 02:43:06.660]   This guy, Pascal Fouad, at EPFL,
[02:43:06.660 --> 02:43:08.100]   he has a startup company
[02:43:08.100 --> 02:43:13.420]   where he basically trained a convolutional net,
[02:43:13.420 --> 02:43:17.980]   essentially, to predict the aerodynamic properties of solids.
[02:43:17.980 --> 02:43:19.620]   And you can generate as much data as you want
[02:43:19.620 --> 02:43:21.900]   by just running computational free dynamics, right?
[02:43:21.900 --> 02:43:26.900]   So, you give a wing airfoil or something,
[02:43:28.260 --> 02:43:29.780]   a shape of some kind,
[02:43:29.780 --> 02:43:31.380]   and you run computational free dynamics,
[02:43:31.380 --> 02:43:36.380]   you get, as a result, the drag and lift
[02:43:36.380 --> 02:43:37.460]   and all that stuff, right?
[02:43:37.460 --> 02:43:40.060]   And you can generate lots of data,
[02:43:40.060 --> 02:43:41.780]   train a neural net to make those predictions,
[02:43:41.780 --> 02:43:44.100]   and now what you have is a differentiable model
[02:43:44.100 --> 02:43:46.940]   of, let's say, drag and lift,
[02:43:46.940 --> 02:43:48.660]   as a function of the shape of that solid.
[02:43:48.660 --> 02:43:49.900]   And so you can do background and design,
[02:43:49.900 --> 02:43:51.460]   you can optimize the shape,
[02:43:51.460 --> 02:43:53.220]   so you get the properties you want.
[02:43:53.220 --> 02:43:56.020]   - Yeah, that's incredible.
[02:43:56.020 --> 02:43:56.860]   That's incredible.
[02:43:56.860 --> 02:43:58.260]   And on top of all that,
[02:43:58.260 --> 02:44:01.420]   probably you should read a little bit of literature
[02:44:01.420 --> 02:44:06.420]   and a little bit of history for inspiration and for wisdom,
[02:44:06.420 --> 02:44:08.780]   'cause after all, all of these technologies
[02:44:08.780 --> 02:44:10.260]   will have to work in the human world.
[02:44:10.260 --> 02:44:11.100]   - Yes.
[02:44:11.100 --> 02:44:12.620]   - And the human world is complicated.
[02:44:12.620 --> 02:44:14.100]   - It is, certainly.
[02:44:14.100 --> 02:44:18.380]   - Jan, this is an amazing conversation.
[02:44:18.380 --> 02:44:20.380]   I'm really honored that you would talk with me today.
[02:44:20.380 --> 02:44:21.820]   Thank you for all the amazing work
[02:44:21.820 --> 02:44:23.780]   you're doing at FAIR, at META,
[02:44:23.780 --> 02:44:26.220]   and thank you for being so passionate
[02:44:26.220 --> 02:44:28.780]   after all these years about everything that's going on.
[02:44:28.780 --> 02:44:31.620]   You're a beacon of hope for the machine learning community.
[02:44:31.620 --> 02:44:32.700]   And thank you so much
[02:44:32.700 --> 02:44:34.460]   for spending your valuable time with me today.
[02:44:34.460 --> 02:44:35.300]   That was awesome.
[02:44:35.300 --> 02:44:36.300]   - Thanks for having me on.
[02:44:36.300 --> 02:44:37.780]   That was a pleasure.
[02:44:37.780 --> 02:44:41.420]   - Thanks for listening to this conversation with Jan Lekun.
[02:44:41.420 --> 02:44:42.780]   To support this podcast,
[02:44:42.780 --> 02:44:45.740]   please check out our sponsors in the description.
[02:44:45.740 --> 02:44:47.820]   And now, let me leave you with some words
[02:44:47.820 --> 02:44:49.580]   from Isaac Asimov.
[02:44:49.580 --> 02:44:53.700]   "Your assumptions are your windows on the world.
[02:44:53.700 --> 02:44:55.940]   "Scrub them off every once in a while,
[02:44:55.940 --> 02:44:57.860]   "or the light won't come in."
[02:44:57.860 --> 02:45:00.060]   Thank you for listening,
[02:45:00.060 --> 02:45:02.060]   and hope to see you next time.
[02:45:02.060 --> 02:45:04.660]   (upbeat music)
[02:45:04.660 --> 02:45:07.260]   (upbeat music)
[02:45:07.260 --> 02:45:17.260]   [BLANK_AUDIO]

