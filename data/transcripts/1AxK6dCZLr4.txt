
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:12.580]   - Hello, my name is Diego Casabuena
[00:00:12.580 --> 00:00:15.460]   and I am an ML practitioner at Spotify.
[00:00:15.460 --> 00:00:17.200]   Today I'm very excited to be here
[00:00:17.200 --> 00:00:20.240]   and to present to you how Spotify is pushing boundaries
[00:00:20.240 --> 00:00:22.040]   with weights and biases.
[00:00:22.040 --> 00:00:25.440]   Our story begins with a company called Pods.
[00:00:25.440 --> 00:00:28.040]   At Pods, we built the world's first
[00:00:28.040 --> 00:00:30.840]   60 second podcast preview feed.
[00:00:30.840 --> 00:00:33.600]   And to do this, we had to put together
[00:00:33.600 --> 00:00:35.800]   a very complex pipeline.
[00:00:35.800 --> 00:00:39.560]   To build this technology, we listened to RSS feeds,
[00:00:39.560 --> 00:00:43.660]   the internet protocol where podcast content is published to.
[00:00:43.660 --> 00:00:47.000]   We had to download the audios, transcribe them,
[00:00:47.000 --> 00:00:48.800]   and pass them through a complex series
[00:00:48.800 --> 00:00:52.040]   of machine learning models that would output clips.
[00:00:52.040 --> 00:00:54.200]   And those clips would then be recommended
[00:00:54.200 --> 00:00:57.040]   and served to users in this clean app interface
[00:00:57.040 --> 00:00:58.880]   that you see on the right.
[00:00:58.880 --> 00:01:01.240]   Now, this is the work of a relatively small team
[00:01:01.240 --> 00:01:02.920]   of engineers and entrepreneurs
[00:01:02.920 --> 00:01:05.300]   from New York to Silicon Valley.
[00:01:05.300 --> 00:01:08.180]   And this team met in all sorts of ways.
[00:01:08.180 --> 00:01:10.320]   Some people met the conventional ways
[00:01:10.320 --> 00:01:14.020]   of getting their companies acquired into Yahoo
[00:01:14.020 --> 00:01:17.500]   and working on previous companies together in the past.
[00:01:17.500 --> 00:01:21.120]   Others like me joined from Fighting Filipino Cali
[00:01:21.120 --> 00:01:24.480]   in martial arts, which is kind of cool.
[00:01:24.480 --> 00:01:26.660]   And actually, the team was never together
[00:01:26.660 --> 00:01:28.840]   in the same room since most of this happened
[00:01:28.840 --> 00:01:31.320]   during the COVID-19 pandemic.
[00:01:31.320 --> 00:01:34.880]   Our founding team, Doug Imbrucce, Seyo Jummu,
[00:01:34.880 --> 00:01:38.640]   Rasmus Swickson, and Greg Bape did a fantastic job
[00:01:38.640 --> 00:01:40.340]   and a big shout out to them.
[00:01:40.340 --> 00:01:43.400]   I was fortunate enough to become the first employee
[00:01:43.400 --> 00:01:45.400]   at this company and to take on the task
[00:01:45.400 --> 00:01:49.240]   of generating machine learned clip previews.
[00:01:49.240 --> 00:01:52.040]   As of 2020, we were one of very few companies
[00:01:52.040 --> 00:01:56.280]   applying transformer technology into consumer applications.
[00:01:56.280 --> 00:01:58.280]   In particular, we're trying to build
[00:01:58.280 --> 00:01:59.860]   these podcast highlights.
[00:01:59.860 --> 00:02:02.560]   The podcast highlights are just clips
[00:02:02.560 --> 00:02:05.800]   around one minute long that try to embody the episode
[00:02:05.800 --> 00:02:07.600]   but also be highly engaging.
[00:02:07.600 --> 00:02:11.240]   For copyright reasons, we couldn't collage multiple pieces
[00:02:11.240 --> 00:02:12.280]   of a podcast together.
[00:02:12.280 --> 00:02:14.340]   So it's not exactly like a movie trailer,
[00:02:14.340 --> 00:02:16.960]   but it is made to the same effect.
[00:02:16.960 --> 00:02:18.700]   And there was no easy task.
[00:02:18.700 --> 00:02:21.480]   However, it came out of a product implication,
[00:02:21.480 --> 00:02:24.660]   which is we were displaying transcription on screen,
[00:02:24.660 --> 00:02:26.840]   which meant that we had transcriptions.
[00:02:26.840 --> 00:02:30.340]   And with transcriptions, we started using transformer models.
[00:02:30.340 --> 00:02:33.160]   We basically took a state-of-the-art technology
[00:02:33.160 --> 00:02:35.120]   and applied it to a different domain.
[00:02:35.120 --> 00:02:40.720]   Now, this company was acquired in 2021 by Spotify
[00:02:40.720 --> 00:02:43.540]   in order to accelerate audio discovery
[00:02:43.540 --> 00:02:45.320]   and podcast consumption.
[00:02:45.320 --> 00:02:47.800]   And here we are making podcast clips
[00:02:47.800 --> 00:02:49.980]   for the Spotify catalog.
[00:02:49.980 --> 00:02:51.720]   So when we arrived at Spotify,
[00:02:51.720 --> 00:02:53.480]   we were placed inside of a cohort
[00:02:53.480 --> 00:02:56.760]   called Listening Experiences, or LEX for short.
[00:02:56.760 --> 00:02:58.880]   LEX is a group of over a hundred people
[00:02:58.880 --> 00:03:01.120]   working inside of personalization
[00:03:01.120 --> 00:03:03.460]   to bring you the best new experiences
[00:03:03.460 --> 00:03:05.320]   across all major content categories,
[00:03:05.320 --> 00:03:08.960]   including music, podcasts, and audio books.
[00:03:08.960 --> 00:03:11.680]   So you may have interacted with some of our products,
[00:03:11.680 --> 00:03:16.040]   including blends, enhance, mixes, such as daily mixes,
[00:03:16.040 --> 00:03:18.920]   and genre mixes, as well as auto-mix.
[00:03:18.920 --> 00:03:23.060]   So where does weights and biases come into play?
[00:03:23.060 --> 00:03:25.440]   Now, this becomes more of a personal story.
[00:03:25.440 --> 00:03:30.480]   I first interacted with weights and biases back in 2018
[00:03:30.480 --> 00:03:32.400]   while working at a research group
[00:03:32.400 --> 00:03:34.680]   inside of NYU Center of Data Science
[00:03:34.680 --> 00:03:36.520]   under Professor Rob Fergus.
[00:03:36.520 --> 00:03:40.000]   More specifically, we were researching the pre-training
[00:03:40.000 --> 00:03:43.640]   and fine-tuning of transformer models.
[00:03:43.640 --> 00:03:45.520]   Chats GPT didn't exist,
[00:03:45.520 --> 00:03:48.160]   and neither did all the higher versions of GPT.
[00:03:48.160 --> 00:03:52.520]   It was only GPT-1 and a model called Birth by Google.
[00:03:52.520 --> 00:03:55.280]   And back then, this was a hot topic too.
[00:03:55.280 --> 00:03:56.800]   We were trying to pre-train
[00:03:56.800 --> 00:03:58.860]   and fine-tune many variants of these models
[00:03:58.860 --> 00:04:01.920]   using one of those old school SLURM systems,
[00:04:01.920 --> 00:04:04.780]   where you publish your jobs,
[00:04:04.780 --> 00:04:06.600]   and you wait for it to be done,
[00:04:06.600 --> 00:04:10.660]   and you just SSH into machines to see how things progress.
[00:04:10.660 --> 00:04:13.680]   Essentially, weights and biases was a game changer
[00:04:13.680 --> 00:04:15.320]   because we were training hundreds of variants
[00:04:15.320 --> 00:04:16.900]   of these things, and we wanted to know
[00:04:16.900 --> 00:04:19.080]   that things were okay when we launched the jobs,
[00:04:19.080 --> 00:04:22.120]   and that everything was tracked with no data loss,
[00:04:22.120 --> 00:04:24.360]   and we were able to compare things afterwards,
[00:04:24.360 --> 00:04:26.440]   and that was pretty spectacular.
[00:04:26.440 --> 00:04:29.800]   And so I'm proud to have carried weights and biases with me
[00:04:29.800 --> 00:04:32.120]   from academic research through startups,
[00:04:32.120 --> 00:04:34.280]   and now into big companies such as Spotify.
[00:04:34.280 --> 00:04:38.440]   At startups, I continued using weights and biases.
[00:04:38.440 --> 00:04:41.560]   I worked on a couple, including one which I co-founded,
[00:04:41.560 --> 00:04:44.580]   and I built modules for facial recognition
[00:04:44.580 --> 00:04:46.960]   and also collaborative recommendation.
[00:04:46.960 --> 00:04:50.400]   Weights and biases, again, kept all my runs together.
[00:04:50.400 --> 00:04:52.080]   And then I landed at pods,
[00:04:52.080 --> 00:04:56.960]   where we trained a ton of models, including transformers,
[00:04:56.960 --> 00:04:58.800]   including recommender systems,
[00:04:58.800 --> 00:05:00.620]   including graph neural networks.
[00:05:00.620 --> 00:05:05.080]   The transformer model was still a novelty,
[00:05:05.080 --> 00:05:07.200]   and our challenges at the time
[00:05:07.200 --> 00:05:12.200]   were to take the base recipes, those pre-trained models,
[00:05:12.200 --> 00:05:15.640]   and fine tune them on tasks that were important to us
[00:05:15.640 --> 00:05:17.700]   and not important to academia.
[00:05:17.700 --> 00:05:19.700]   Because, again, at the time,
[00:05:19.700 --> 00:05:22.560]   there was a lot of publication around academic tasks
[00:05:22.560 --> 00:05:25.000]   and data sets, but not necessarily the things
[00:05:25.000 --> 00:05:26.380]   that we needed to do.
[00:05:26.380 --> 00:05:28.920]   So we had to go out and collect our own data,
[00:05:28.920 --> 00:05:31.100]   figure out whether the data was any good,
[00:05:31.100 --> 00:05:34.360]   fine tune models, and ensure that these fine tunings
[00:05:34.360 --> 00:05:37.420]   would actually transfer the learning from the base models.
[00:05:37.420 --> 00:05:40.240]   And in order to do that, we had to collect a lot of data,
[00:05:40.240 --> 00:05:42.080]   and we had to train a lot of models.
[00:05:42.080 --> 00:05:44.660]   And the only way for us to keep distractible
[00:05:44.660 --> 00:05:47.560]   in such a small team was to use weights and biases
[00:05:47.560 --> 00:05:51.100]   to keep our run comparisons and our analysis in one place.
[00:05:51.100 --> 00:05:52.780]   So then we got into Spotify,
[00:05:52.780 --> 00:05:54.460]   and we procured the technology,
[00:05:54.460 --> 00:05:57.740]   continued using weights and biases for training runs.
[00:05:57.740 --> 00:05:59.880]   We've additionally started using weights and biases
[00:05:59.880 --> 00:06:02.400]   to profile inference when we have new models,
[00:06:02.400 --> 00:06:06.380]   because that saves us time by checking the system metrics
[00:06:06.380 --> 00:06:07.600]   when new models are running
[00:06:07.600 --> 00:06:10.660]   and ensuring that they're ready for production.
[00:06:10.660 --> 00:06:12.380]   Now, Spotify has great tools of its own,
[00:06:12.380 --> 00:06:15.320]   such as Backstage, where you can monitor teams,
[00:06:15.320 --> 00:06:19.860]   projects, workflows, data endpoints, A/B tests, and more.
[00:06:19.860 --> 00:06:22.900]   So you should check that out too for production purposes.
[00:06:22.900 --> 00:06:25.920]   For development purposes, it is my personal opinion
[00:06:25.920 --> 00:06:28.300]   that weights and biases is the best tool
[00:06:28.300 --> 00:06:30.100]   to keep all your runs together.
[00:06:30.100 --> 00:06:32.180]   And today, weights and biases at Spotify
[00:06:32.180 --> 00:06:35.140]   brings value to multiple teams at listening experiences
[00:06:35.140 --> 00:06:37.700]   and the brother Spotify community.
[00:06:37.700 --> 00:06:39.100]   So let's bring it back to the beginning.
[00:06:39.100 --> 00:06:40.580]   How are we pushing the boundaries?
[00:06:40.580 --> 00:06:42.980]   Well, quite literally, we are bringing the start
[00:06:42.980 --> 00:06:46.120]   and then offsets of audio closer together.
[00:06:46.120 --> 00:06:47.920]   I have two case studies for this.
[00:06:47.920 --> 00:06:50.080]   There is pods, which I already discussed,
[00:06:50.080 --> 00:06:51.840]   where we're using machine learning
[00:06:51.840 --> 00:06:54.220]   to extract features from podcast content,
[00:06:54.220 --> 00:06:56.220]   increasing our podcast understanding,
[00:06:56.220 --> 00:06:59.120]   and ultimately generating clips of many sorts
[00:06:59.120 --> 00:07:02.480]   to show to users as previous for podcasts.
[00:07:02.480 --> 00:07:04.640]   So the process is quite involved,
[00:07:04.640 --> 00:07:07.120]   and there's a lot of models in use.
[00:07:07.120 --> 00:07:10.200]   There's a lot of practitioners working on different pieces,
[00:07:10.200 --> 00:07:11.800]   and a way to bring them all together
[00:07:11.800 --> 00:07:15.440]   under the same site is to use weights and biases.
[00:07:15.440 --> 00:07:19.020]   Now, there's a second example, cue points.
[00:07:19.020 --> 00:07:20.420]   Have you ever heard of AutoMix?
[00:07:20.420 --> 00:07:24.760]   It is a great feature that basically
[00:07:24.760 --> 00:07:27.240]   blends music tracks together,
[00:07:27.240 --> 00:07:29.760]   like a DJ would do it for you.
[00:07:29.760 --> 00:07:32.040]   And we use ML for this, actually.
[00:07:32.040 --> 00:07:35.520]   And in this case, we push boundaries together
[00:07:35.520 --> 00:07:38.640]   by taking a little bit from the start of his track
[00:07:38.640 --> 00:07:40.480]   and a little bit from the end of the track
[00:07:40.480 --> 00:07:42.000]   and feeding in and out from there,
[00:07:42.000 --> 00:07:44.540]   creating a cool effect that sounds
[00:07:44.540 --> 00:07:47.000]   like you're listening to a DJ.
[00:07:47.000 --> 00:07:50.320]   For this, we had to try many combinations of features
[00:07:50.320 --> 00:07:51.880]   and many types of models.
[00:07:51.880 --> 00:07:55.360]   And in order, again, to select the best model,
[00:07:55.360 --> 00:07:56.920]   we had to use weights and biases.
[00:07:56.920 --> 00:08:00.360]   And in particular, a cool feature that we used here
[00:08:00.360 --> 00:08:03.640]   was we needed to visualize some of this information.
[00:08:03.640 --> 00:08:07.380]   So we uploaded it as assets into weights and biases
[00:08:07.380 --> 00:08:09.240]   for our evaluation sets,
[00:08:09.240 --> 00:08:12.280]   and we were able to just listen to the clips
[00:08:12.280 --> 00:08:15.200]   and we were able to listen to the tracks.
[00:08:15.200 --> 00:08:17.160]   And it was very successful.
[00:08:17.160 --> 00:08:20.640]   We could then export it and send it for human evaluation.
[00:08:20.640 --> 00:08:22.840]   So that was a great lift.
[00:08:22.840 --> 00:08:24.320]   So there's many other use cases
[00:08:24.320 --> 00:08:27.080]   within Spotify for weights and biases.
[00:08:27.080 --> 00:08:30.520]   And there's many teams that started testing out
[00:08:30.520 --> 00:08:34.440]   the technology or the service for their own products.
[00:08:34.440 --> 00:08:39.280]   And that includes the teams behind Enhance, Daily Mixes,
[00:08:39.280 --> 00:08:41.100]   and also Tech Research.
[00:08:41.100 --> 00:08:43.580]   And in Tech Research, we have very smart individuals
[00:08:43.580 --> 00:08:45.740]   working on the state of the art
[00:08:45.740 --> 00:08:49.640]   and bringing us new research such as speech diarization.
[00:08:49.640 --> 00:08:53.240]   So some of us are using weights and biases.
[00:08:53.240 --> 00:08:54.960]   Some of us are testing it.
[00:08:54.960 --> 00:08:56.020]   What do we like about it?
[00:08:56.020 --> 00:08:59.320]   We've aggregated the feedback into three main categories.
[00:08:59.320 --> 00:09:02.180]   We believe that weights and biases is versatile.
[00:09:02.180 --> 00:09:05.000]   We believe that it helps us aggregate data really well
[00:09:05.000 --> 00:09:08.260]   and that it increases our presentation skills.
[00:09:08.260 --> 00:09:11.920]   The tool is versatile, meaning you can take the same code
[00:09:11.920 --> 00:09:14.520]   and run it across multiple frameworks,
[00:09:14.520 --> 00:09:17.440]   infrastructure setups, and model types.
[00:09:17.440 --> 00:09:20.720]   And so it is very portable and easy to use
[00:09:20.720 --> 00:09:24.240]   and easy to learn for different ML engineers.
[00:09:24.240 --> 00:09:27.520]   Now, the aggregation is super important
[00:09:27.520 --> 00:09:31.040]   because when you have a lot of people using the tool
[00:09:31.040 --> 00:09:33.420]   and training all these sorts of different models
[00:09:33.420 --> 00:09:36.000]   across projects and teams,
[00:09:36.000 --> 00:09:38.860]   weights and biases brings it all under one roof
[00:09:38.860 --> 00:09:41.420]   so that you can share all the information
[00:09:41.420 --> 00:09:43.920]   of what one team learns with another.
[00:09:43.920 --> 00:09:45.620]   And that is very important to us.
[00:09:45.620 --> 00:09:50.660]   Additionally, all the tools to sort and filter the data
[00:09:50.660 --> 00:09:55.180]   and compare metrics and generate insights
[00:09:55.180 --> 00:09:57.500]   is really valuable because it saves a lot
[00:09:57.500 --> 00:09:59.220]   of engineering time that you can just do it
[00:09:59.220 --> 00:10:03.700]   on the web interface with just a few clicks.
[00:10:03.700 --> 00:10:07.060]   The filtering, sorting, and other tools
[00:10:07.060 --> 00:10:10.440]   that exist on the platform to help us filter our data,
[00:10:10.440 --> 00:10:13.260]   figure out our best runs, and which models we should push
[00:10:13.260 --> 00:10:16.540]   for human evaluation or to production are great
[00:10:16.540 --> 00:10:19.180]   because it saves a lot of engineering time.
[00:10:19.180 --> 00:10:20.720]   And more recently, we've been trying out
[00:10:20.720 --> 00:10:23.780]   the hyperparameter tuning, and that is great
[00:10:23.780 --> 00:10:27.500]   because that way we can send out batches of training runs
[00:10:27.500 --> 00:10:30.260]   and see summarized results afterwards
[00:10:30.260 --> 00:10:32.140]   with no additional effort.
[00:10:32.140 --> 00:10:35.000]   And finally, there's presentation.
[00:10:35.000 --> 00:10:40.000]   So for presentation, there's a number of visualizations
[00:10:40.000 --> 00:10:44.260]   that are actually important to us as we train our models.
[00:10:44.260 --> 00:10:46.280]   Some models have specifics,
[00:10:46.280 --> 00:10:49.300]   like if you use classical machine learning
[00:10:49.300 --> 00:10:52.820]   and you're training a forest or a tree-based model,
[00:10:52.820 --> 00:10:56.520]   like a random forest, there's actually specific tools
[00:10:56.520 --> 00:10:59.780]   such as feature importance plots that one can use
[00:10:59.780 --> 00:11:01.940]   to decide whether the features that you're using
[00:11:01.940 --> 00:11:04.180]   are actually important or not.
[00:11:04.180 --> 00:11:06.980]   Weights and Biases has one line of plots
[00:11:06.980 --> 00:11:10.540]   that you can basically instantiate with one line of code,
[00:11:10.540 --> 00:11:12.600]   and that just shows on your dashboard
[00:11:12.600 --> 00:11:14.280]   every time you train the same model.
[00:11:14.280 --> 00:11:15.820]   And then when you train multiple models,
[00:11:15.820 --> 00:11:18.940]   it compares them such that you know
[00:11:18.940 --> 00:11:21.100]   how the feature importance is, for example,
[00:11:21.100 --> 00:11:23.740]   changed across different runs.
[00:11:23.740 --> 00:11:25.280]   And so that is pretty cool because, again,
[00:11:25.280 --> 00:11:28.460]   you get a lot for free for a lot of code.
[00:11:28.460 --> 00:11:31.340]   And so that is important to us.
[00:11:31.340 --> 00:11:35.060]   Also, the fact that once you have all the runs logged,
[00:11:35.060 --> 00:11:38.540]   and once you have all these charts,
[00:11:38.540 --> 00:11:42.820]   you don't have to export them or save them somewhere
[00:11:42.820 --> 00:11:45.020]   or bringing assets from outside.
[00:11:45.020 --> 00:11:46.700]   All your information is in one place,
[00:11:46.700 --> 00:11:49.700]   and so you can literally just drag and drop it
[00:11:49.700 --> 00:11:54.340]   into these reports where you then add some text
[00:11:54.340 --> 00:11:56.600]   and send them to your leadership.
[00:11:56.600 --> 00:11:58.720]   You're to share it for your team.
[00:11:58.720 --> 00:12:02.840]   It's great for presentations and, again,
[00:12:02.840 --> 00:12:04.020]   for information share.
[00:12:04.020 --> 00:12:06.080]   So we also really like that feature.
[00:12:06.080 --> 00:12:08.240]   And then finally, I would like to compliment
[00:12:08.240 --> 00:12:13.000]   the technical support, which has been incredibly helpful
[00:12:13.000 --> 00:12:17.400]   to us, very prompt and very responsive.
[00:12:17.400 --> 00:12:20.440]   And they've solved many of the issues that we've had
[00:12:21.960 --> 00:12:25.940]   as we were integrating Weights and Biases into Spotify.
[00:12:25.940 --> 00:12:28.660]   And they have even acted on some of the feedback
[00:12:28.660 --> 00:12:31.500]   that we have given them to improve the app.
[00:12:31.500 --> 00:12:33.420]   So we really appreciate that.
[00:12:33.420 --> 00:12:38.220]   Now that's the story of how we found Weights and Biases,
[00:12:38.220 --> 00:12:40.940]   how it came to Spotify, how people are using it,
[00:12:40.940 --> 00:12:42.500]   why they like it.
[00:12:42.500 --> 00:12:44.540]   And thank you so much for listening.
[00:12:44.540 --> 00:12:47.120]   (upbeat music)
[00:12:47.120 --> 00:12:49.700]   (gentle music)
[00:12:49.700 --> 00:12:54.460]   [Music]

