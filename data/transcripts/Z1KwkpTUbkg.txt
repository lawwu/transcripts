
[00:00:00.000 --> 00:00:09.500]   The following is a conversation with Jeff Hawkins, a neuroscientist seeking to understand the structure, function, and origin of intelligence in the human brain.
[00:00:09.500 --> 00:00:28.500]   He previously wrote a seminal book on the subject titled "On Intelligence" and recently a new book called "A Thousand Brains" which presents a new theory of intelligence that Richard Dawkins, for example, has been raving about, calling the book "brilliant and exhilarating."
[00:00:29.100 --> 00:00:34.200]   I can't read those two words and not think of him saying it in his British accent.
[00:00:34.200 --> 00:00:41.800]   Quick mention of our sponsors, Codecademy, BioOptimizers, ExpressVPN, Asleep, and Blinkist.
[00:00:41.800 --> 00:00:44.700]   Check them out in the description to support this podcast.
[00:00:44.700 --> 00:00:58.400]   As a side note, let me say that one small but powerful idea that Jeff Hawkins mentions in his new book is that if human civilization were to destroy itself, all of knowledge, all our creations will go with us.
[00:00:59.100 --> 00:01:16.200]   He proposes that we should think about how to save that knowledge in a way that long outlives us, whether that's on Earth, in orbit around Earth, or in deep space, and then to send messages that advertise this backup of human knowledge to other intelligent alien civilizations.
[00:01:16.200 --> 00:01:25.300]   The main message of this advertisement is not that we are here, but that we were once here.
[00:01:26.300 --> 00:01:46.500]   This little difference somehow was deeply humbling to me, that we may, with some non-zero likelihood, destroy ourselves, and that an alien civilization thousands or millions of years from now may come across this knowledge store, and they would only with some low probability even notice it, not to mention be able to interpret it.
[00:01:46.500 --> 00:01:52.600]   And the deeper question here for me is what information in all of human knowledge is even essential?
[00:01:52.600 --> 00:01:55.500]   Does Wikipedia capture it or not at all?
[00:01:55.900 --> 00:02:03.100]   This thought experiment forces me to wonder what are the things we've accomplished and are hoping to still accomplish that will outlive us?
[00:02:03.100 --> 00:02:08.300]   Is it things like complex buildings, bridges, cars, rockets?
[00:02:08.300 --> 00:02:11.700]   Is it ideas like science, physics, and mathematics?
[00:02:11.700 --> 00:02:13.800]   Is it music and art?
[00:02:13.800 --> 00:02:19.100]   Is it computers, computational systems, or even artificial intelligence systems?
[00:02:19.100 --> 00:02:24.100]   I personally can't imagine that aliens wouldn't already have all of these things.
[00:02:24.600 --> 00:02:27.500]   In fact, much more and much better.
[00:02:27.500 --> 00:02:38.100]   To me, the only unique thing we may have is consciousness itself, and the actual subjective experience of suffering, of happiness, of hatred, of love.
[00:02:38.100 --> 00:02:49.100]   If we can record these experiences in the highest resolution directly from the human brain, such that aliens will be able to replay them, that is what we should store and send as a message.
[00:02:49.100 --> 00:02:53.600]   Not Wikipedia, but the extremes of conscious experiences.
[00:02:54.000 --> 00:02:57.300]   The most important of which, of course, is love.
[00:02:57.300 --> 00:03:03.300]   This is the Lex Friedman Podcast, and here is my conversation with Jeff Hawkins.
[00:03:03.300 --> 00:03:07.100]   We previously talked over two years ago.
[00:03:07.100 --> 00:03:11.900]   Do you think there's still neurons in your brain that remember that conversation?
[00:03:11.900 --> 00:03:14.300]   That remember me and got excited?
[00:03:14.300 --> 00:03:18.000]   Like there's a Lex neuron in your brain that just like finally has a purpose?
[00:03:18.300 --> 00:03:24.900]   I do remember our conversation, or I have some memories of it, and I formed additional memories of you in the meantime.
[00:03:24.900 --> 00:03:29.500]   I wouldn't say there's a neuron or a neurons in my brain that know you.
[00:03:29.500 --> 00:03:37.200]   There are synapses in my brain that have formed that reflect my knowledge of you and the model I have of you and the world.
[00:03:37.200 --> 00:03:42.900]   And whether they're the exact same synapses were formed two years ago, it's hard to say because these things come and go all the time.
[00:03:42.900 --> 00:03:49.100]   But we know from one thing to know about brains is that when you think of things, you often erase the memory and rewrite it again.
[00:03:49.100 --> 00:03:53.900]   So, yes, but I have a memory of you and I have that's instantiated in synapses.
[00:03:53.900 --> 00:03:55.500]   There's a simpler way to think about it.
[00:03:55.500 --> 00:04:02.000]   Like so you have we have a model of the world in your head, and that model is continually being updated.
[00:04:02.000 --> 00:04:03.800]   I updated this morning.
[00:04:03.800 --> 00:04:05.600]   You offered me this water.
[00:04:05.600 --> 00:04:06.700]   You said it was from the refrigerator.
[00:04:06.700 --> 00:04:08.400]   I remember these things.
[00:04:08.400 --> 00:04:14.400]   And so the model includes where we live, the places we know, the words, the objects in the world.
[00:04:14.400 --> 00:04:17.200]   It's just a monstrous model and it's constantly being updated.
[00:04:17.200 --> 00:04:19.000]   And people are just part of that model.
[00:04:19.000 --> 00:04:23.900]   So are animals, so are other physical objects, so are events we've done.
[00:04:23.900 --> 00:04:29.900]   So it's no special in my mind special place for the memories of humans.
[00:04:29.900 --> 00:04:37.600]   I mean, obviously, I know a lot about my wife and friends and so on.
[00:04:37.600 --> 00:04:41.300]   But it's not like a special place for humans are over here.
[00:04:41.300 --> 00:04:44.800]   But we model everything and we model other people's behaviors, too.
[00:04:44.800 --> 00:04:51.100]   So if I said yours is a copy of your mind in my mind, it's just because I know how humans I've learned how humans behave.
[00:04:51.100 --> 00:04:54.000]   And I learned some things about you.
[00:04:54.000 --> 00:04:56.100]   And that's part of my world model.
[00:04:56.100 --> 00:05:01.900]   Well, I just also mean the collective intelligence of the human species.
[00:05:02.400 --> 00:05:08.300]   I wonder if there is something fundamental to the brain that enables that.
[00:05:08.300 --> 00:05:11.000]   So modeling other humans with their ideas.
[00:05:11.000 --> 00:05:14.100]   You're actually jumping into a lot of big topics.
[00:05:14.100 --> 00:05:17.500]   Like collective intelligence is a separate topic that a lot of people like to talk about.
[00:05:17.500 --> 00:05:18.300]   We can talk about that.
[00:05:18.300 --> 00:05:22.000]   But so that's interesting.
[00:05:22.000 --> 00:05:23.500]   Like, you know, we're not just individuals.
[00:05:23.500 --> 00:05:25.200]   We live in society and so on.
[00:05:25.200 --> 00:05:29.800]   But from our research point of view, and so again, let's just talk.
[00:05:29.800 --> 00:05:31.000]   We study the neocortex.
[00:05:31.000 --> 00:05:32.500]   It's a sheet of neural tissue.
[00:05:32.500 --> 00:05:34.400]   It's about 75% of your brain.
[00:05:34.400 --> 00:05:37.700]   It runs on this very repetitive algorithm.
[00:05:37.700 --> 00:05:39.300]   It's a very repetitive circuit.
[00:05:39.300 --> 00:05:45.600]   And so you can apply that algorithm to lots of different problems, but it's all underneath.
[00:05:45.600 --> 00:05:46.200]   It's the same thing.
[00:05:46.200 --> 00:05:47.400]   We're just building this model.
[00:05:47.400 --> 00:05:56.100]   So from our point of view, we wouldn't look for these special circuits someplace buried in your brain that might be related to other, you know, understanding other humans.
[00:05:56.100 --> 00:05:59.100]   It's more like, you know, how do we build a model of anything?
[00:05:59.100 --> 00:06:00.700]   How do we understand anything in the world?
[00:06:00.700 --> 00:06:03.300]   And humans are just another part of the things we understand.
[00:06:03.300 --> 00:06:11.200]   So there's nothing to the brain that knows the emergent phenomenon of collective intelligence.
[00:06:11.200 --> 00:06:12.800]   Well, I certainly know about that.
[00:06:12.800 --> 00:06:13.800]   I've heard the terms.
[00:06:13.800 --> 00:06:14.300]   I've read.
[00:06:14.300 --> 00:06:16.900]   No, but that's an idea.
[00:06:16.900 --> 00:06:22.600]   Well, I think we have language, which is sort of built into our brains, and that's a key part of collective intelligence.
[00:06:22.600 --> 00:06:28.400]   So there are some, you know, prior assumptions about the world we're going to live in when we're born.
[00:06:28.400 --> 00:06:30.100]   We're not just a blank slate.
[00:06:30.100 --> 00:06:35.700]   And so, you know, did we evolve to take advantage of those situations?
[00:06:35.700 --> 00:06:36.000]   Yes.
[00:06:36.000 --> 00:06:38.500]   But again, we study only part of the brain, the neocortex.
[00:06:38.500 --> 00:06:55.200]   There's other parts of the brain are very much involved in societal interactions and human emotions and how we interact and even societal issues about, you know, how we interact with other people, when we support them, when we're reading, things like that.
[00:06:55.800 --> 00:07:01.900]   I mean, certainly the brain is a great place where to study intelligence.
[00:07:01.900 --> 00:07:06.900]   I wonder if it's the fundamental atom of intelligence.
[00:07:06.900 --> 00:07:15.400]   Well, I would say it's absolutely an essential component, even if you believe in collective intelligence as, hey, that's where it's all happening.
[00:07:15.400 --> 00:07:17.700]   That's what we need to study, which I don't believe that, by the way.
[00:07:17.700 --> 00:07:19.900]   I think it's really important, but I don't think that is the thing.
[00:07:20.700 --> 00:07:26.600]   But even if you do believe that, then you have to understand how the brain works in doing that.
[00:07:26.600 --> 00:07:34.400]   It's more like we are intelligent individuals and together we are much more magnified, our intelligence.
[00:07:34.400 --> 00:07:36.200]   We can do things which we couldn't do individually.
[00:07:36.200 --> 00:07:42.500]   But even as individuals, we're pretty damn smart and we can model things and understand the world and interact with it.
[00:07:42.500 --> 00:07:47.100]   So to me, if you're going to start someplace, you need to start with the brain.
[00:07:47.100 --> 00:07:49.900]   Then you could say, well, how do brains interact with each other?
[00:07:50.100 --> 00:07:51.800]   And what is the nature of language?
[00:07:51.800 --> 00:07:55.600]   And how do we share models that I've learned something about the world?
[00:07:55.600 --> 00:07:56.500]   How do I share it with you?
[00:07:56.500 --> 00:08:00.000]   Which is really what sort of communal intelligence is.
[00:08:00.000 --> 00:08:01.500]   I know something, you know something.
[00:08:01.500 --> 00:08:03.900]   We've had different experiences in the world.
[00:08:03.900 --> 00:08:05.500]   I've learned something about brains.
[00:08:05.500 --> 00:08:06.700]   Maybe I can impart that to you.
[00:08:06.700 --> 00:08:10.200]   You've learned something about physics and you can impart that to me.
[00:08:10.200 --> 00:08:18.300]   But it all comes down to even just the epistemological question of, well, what is knowledge and how do you represent it in the brain?
[00:08:18.600 --> 00:08:19.000]   Right.
[00:08:19.000 --> 00:08:21.800]   And it's not, that's where it's going to reside, right?
[00:08:21.800 --> 00:08:22.900]   Or in our writings.
[00:08:22.900 --> 00:08:29.100]   It's obvious that human collaboration, human interaction is how we build societies.
[00:08:29.100 --> 00:08:29.500]   Yeah.
[00:08:29.500 --> 00:08:40.400]   But some of the things you talk about and work on, some of those elements of what makes up an intelligent entity is there with a single person.
[00:08:40.400 --> 00:08:41.100]   Oh, absolutely.
[00:08:41.100 --> 00:08:47.500]   I mean, we can't deny that the brain is the core element here in, at least I think it's obvious.
[00:08:47.800 --> 00:08:50.600]   The brain is the core element in all theories of intelligence.
[00:08:50.600 --> 00:08:52.700]   It's where knowledge is represented.
[00:08:52.700 --> 00:08:54.300]   It's where knowledge is created.
[00:08:54.300 --> 00:08:58.500]   We interact, we share, we build upon each other's work.
[00:08:58.500 --> 00:09:01.300]   But without a brain, you'd have nothing.
[00:09:01.300 --> 00:09:03.600]   You know, there would be no intelligence without brains.
[00:09:03.600 --> 00:09:07.000]   And so, so that's where we start.
[00:09:07.000 --> 00:09:10.100]   I got into this field because I just was curious as to who I am.
[00:09:10.100 --> 00:09:12.300]   You know, how, you know, how do I think?
[00:09:12.300 --> 00:09:14.500]   What's going on in my head when I'm, when I'm thinking?
[00:09:14.500 --> 00:09:15.900]   What does it mean to know something?
[00:09:16.500 --> 00:09:22.600]   You know, I can ask what it means for me to know something independent of how I learned it from you or from someone else or from society.
[00:09:22.600 --> 00:09:25.900]   What does it mean for me to know that I have a model of you in my head?
[00:09:25.900 --> 00:09:30.200]   What does it mean to know I know what this microphone does and how it works physically, even though I can't see it right now?
[00:09:30.200 --> 00:09:32.100]   How do I know that?
[00:09:32.100 --> 00:09:33.400]   What does it mean?
[00:09:33.400 --> 00:09:37.400]   How the neurons do that at the fundamental level of neurons and synapses and so on?
[00:09:37.400 --> 00:09:39.900]   Those are really fascinating questions.
[00:09:39.900 --> 00:09:44.500]   And I'm happy to, just happy to understand those if I could.
[00:09:44.600 --> 00:09:54.900]   So in your, in your new book, you talk about our brain, our mind as being made up of many brains.
[00:09:54.900 --> 00:10:00.300]   So the book is called A Thousand Brains, A Thousand Brain Theory of Intelligence.
[00:10:00.300 --> 00:10:01.900]   What is the key idea of this book?
[00:10:01.900 --> 00:10:08.000]   The book has three sections and it has sort of maybe three big ideas.
[00:10:08.000 --> 00:10:12.800]   So the first section is all about what we've learned about the neocortex and that's the thousand brains theory.
[00:10:13.200 --> 00:10:17.700]   Just to complete the picture of the second section is all about AI and the third section is about the future of humanity.
[00:10:17.700 --> 00:10:33.200]   So the thousand brains theory, the big idea there, if you've, if I had to summarize into one big idea, is that we think of the brain, the neocortex is learning this model of the world.
[00:10:33.200 --> 00:10:38.900]   But what we learned is actually there's tens of thousands of independent modeling systems going on.
[00:10:39.100 --> 00:10:45.000]   And so each, what we call a column in the cortex is about 150,000 of them is a complete modeling system.
[00:10:45.000 --> 00:10:48.900]   So it's a collective intelligence in your head in some sense.
[00:10:48.900 --> 00:10:55.500]   So the thousand brains theory is about where do I have knowledge about, you know, this coffee cup or where's the model of this cell phone?
[00:10:55.500 --> 00:10:56.900]   It's not in one place.
[00:10:56.900 --> 00:11:01.100]   It's in thousands of separate models that are complementary and they communicate with each other through voting.
[00:11:01.100 --> 00:11:06.200]   So this idea that we have, we feel like we're one person, you know, that's our experience.
[00:11:06.200 --> 00:11:07.100]   We can explain that.
[00:11:07.200 --> 00:11:16.700]   But reality, there's lots of these, like, it's almost like little brains, like, but they're, they're sophisticated modeling systems, about 150,000 of them in each human brain.
[00:11:16.700 --> 00:11:24.300]   And that's a total different way of thinking about how the neocortex is structured than we or anyone else thought of even just five years ago.
[00:11:24.300 --> 00:11:31.700]   So you mentioned you started this journey just looking in the mirror, trying to understand who you are.
[00:11:31.700 --> 00:11:35.500]   So if you have many brains, who are you then?
[00:11:35.900 --> 00:11:38.500]   So it's interesting, we have a singular perception, right?
[00:11:38.500 --> 00:11:40.100]   You know, we think, oh, I'm just here.
[00:11:40.100 --> 00:11:40.800]   I'm looking at you.
[00:11:40.800 --> 00:11:48.000]   But it's, it's composed of all these things, like there's sounds and there's, and there's this vision and there's touch and all kinds of inputs.
[00:11:48.000 --> 00:11:52.800]   Yet we have the singular perception and what the thousand brain series says, we have these models that are visual models.
[00:11:52.800 --> 00:11:57.000]   We have a lot of models, auditory models, models, I'll talk to models and so on, but they vote.
[00:11:57.000 --> 00:12:00.100]   And so they send in the cortex.
[00:12:00.100 --> 00:12:05.800]   You can think about these columns as bad, like little grains of rice, 150,000 stacked next to each other.
[00:12:06.500 --> 00:12:12.000]   And each one is its own little modeling system, but they have these long range connections that go between them.
[00:12:12.000 --> 00:12:15.800]   And we call those voting connections or voting neurons.
[00:12:15.800 --> 00:12:22.100]   And so the different columns try to reach a consensus, like what am I looking at?
[00:12:22.100 --> 00:12:25.700]   Okay, you know, each one has some ambiguity, but they come to a consensus.
[00:12:25.700 --> 00:12:27.000]   Oh, there's a water bottle I'm looking at.
[00:12:27.000 --> 00:12:31.700]   We are only consciously able to perceive the voting.
[00:12:31.700 --> 00:12:35.100]   We're not able to perceive anything that goes under the hood.
[00:12:35.600 --> 00:12:38.500]   So the voting is what we're aware of.
[00:12:38.500 --> 00:12:40.700]   - The results of the voting. - Yeah, the voting.
[00:12:40.700 --> 00:12:42.500]   Well, you can imagine it this way.
[00:12:42.500 --> 00:12:44.400]   We were just talking about eye movements a moment ago.
[00:12:44.400 --> 00:12:47.600]   So as I'm looking at something, my eyes are moving about three times a second.
[00:12:47.600 --> 00:12:51.600]   And with each movement, a completely new input is coming into the brain.
[00:12:51.600 --> 00:12:52.500]   It's not repetitive.
[00:12:52.500 --> 00:12:53.500]   It's not shifting it around.
[00:12:53.500 --> 00:12:54.400]   It's completely new.
[00:12:54.400 --> 00:12:55.800]   I'm totally unaware of it.
[00:12:55.800 --> 00:12:57.200]   I can't perceive it.
[00:12:57.200 --> 00:13:00.800]   But yet, if I looked at the neurons in your brain, they're going on and off, on and off, on and off, on and off.
[00:13:00.800 --> 00:13:02.700]   But the voting neurons are not.
[00:13:03.300 --> 00:13:07.600]   The voting neurons are saying, you know, we all agree, even though I'm looking at different parts of this, this is a water bottle right now.
[00:13:07.600 --> 00:13:09.600]   And that's not changing.
[00:13:09.600 --> 00:13:12.800]   And it's in some position and pose relative to me.
[00:13:12.800 --> 00:13:16.700]   So I have this perception of the water bottle about two feet away from me at a certain pose to me.
[00:13:16.700 --> 00:13:18.800]   That is not changing.
[00:13:18.800 --> 00:13:20.100]   That's the only part I'm aware of.
[00:13:20.100 --> 00:13:24.800]   I can't be aware of the fact that the inputs from the eyes are moving and changing and all this other stuff.
[00:13:24.800 --> 00:13:29.500]   So these long-range connections are the part we can be conscious of.
[00:13:29.800 --> 00:13:34.500]   The individual activity in each column doesn't go anywhere else.
[00:13:34.500 --> 00:13:36.100]   It doesn't get shared anywhere else.
[00:13:36.100 --> 00:13:43.700]   There's no way to extract it and talk about it or extract it and even remember it to say, oh, yes, I can recall that.
[00:13:43.700 --> 00:13:54.600]   But these long-range connections are the things that are accessible to language and to our, you know, it's like the hippocampus, our memories, you know, our short-term memory systems and so on.
[00:13:55.000 --> 00:14:00.900]   So we're not aware of 95% or maybe it's even 98% of what's going on in your brain.
[00:14:00.900 --> 00:14:09.500]   We're only aware of this sort of stable, somewhat stable voting outcome of all these things that are going on underneath the hood.
[00:14:09.500 --> 00:14:17.600]   So what would you say is the basic element in the thousand brains theory of intelligence of intelligence?
[00:14:17.600 --> 00:14:21.300]   Like what's the atom of intelligence when you think about it?
[00:14:21.300 --> 00:14:24.600]   Is it the individual brains and then what is a brain?
[00:14:24.600 --> 00:14:30.100]   Well, let's, can we just talk about what intelligence is first and then we can talk about what the elements are.
[00:14:30.100 --> 00:14:41.600]   So in my book, intelligence is the ability to learn a model of the world, to build internal to your head a model that represents the structure of everything.
[00:14:41.600 --> 00:14:49.400]   You know, to know that this is a table and that's a coffee cup and this is a gooseneck lamp and all this, to know these things, I have to have a model in my head.
[00:14:49.400 --> 00:14:51.000]   I just don't look at them and go, what is that?
[00:14:51.500 --> 00:14:56.000]   I already have internal representations of these things in my head and I had to learn them.
[00:14:56.000 --> 00:14:57.500]   I wasn't born to any of that knowledge.
[00:14:57.500 --> 00:15:00.400]   You were, you know, we have some lights in the room here.
[00:15:00.400 --> 00:15:02.900]   I, you know, that's not part of my evolutionary heritage, right?
[00:15:02.900 --> 00:15:03.900]   It's not in my genes.
[00:15:03.900 --> 00:15:11.900]   So we have this incredible model and the model includes not only what things look like and feel like, but where they are relative to each other and how they behave.
[00:15:11.900 --> 00:15:21.400]   I've never picked up this water bottle before, but I know that if I took my hand on that blue thing and I turn it, it'll probably make a funny little sound as a little plastic things detach and then it'll rotate and it'll rotate.
[00:15:21.400 --> 00:15:22.400]   Certain way it'll come off.
[00:15:22.400 --> 00:15:23.200]   How do I know that?
[00:15:23.200 --> 00:15:24.500]   Because I have this model in my head.
[00:15:24.500 --> 00:15:32.000]   So the essence of intelligence is our ability to learn a model and the more sophisticated our model is, the smarter we are.
[00:15:32.000 --> 00:15:43.300]   Not that there is a single intelligence because you can know about, you know, a lot about things that I don't know and I know about things you don't know and we can both be very smart, but we both learned a model of the world through interacting with it.
[00:15:43.300 --> 00:15:45.300]   So that is the essence of intelligence.
[00:15:45.300 --> 00:15:49.600]   Then we can ask ourselves, what are the mechanisms in the brain that allow us to do that?
[00:15:49.600 --> 00:15:51.100]   And what are the mechanisms of learning?
[00:15:51.100 --> 00:15:54.600]   Not just the neural mechanisms, but what are the general process for how we learn a model?
[00:15:54.600 --> 00:15:56.300]   So that was a big insight for us.
[00:15:56.300 --> 00:16:00.600]   It's like, what are the actual things that, how do you learn this stuff?
[00:16:00.600 --> 00:16:02.200]   It turns out you have to learn it through movement.
[00:16:02.200 --> 00:16:05.200]   You can't learn it just by, that's how we learn.
[00:16:05.200 --> 00:16:05.900]   We learn through movement.
[00:16:05.900 --> 00:16:12.100]   We learn, so you build up this model by observing things and touching them and moving them and walking around the world and so on.
[00:16:12.100 --> 00:16:14.100]   So either you move or the thing moves.
[00:16:14.100 --> 00:16:14.900]   Somehow.
[00:16:14.900 --> 00:16:15.500]   Yeah.
[00:16:15.900 --> 00:16:18.600]   You obviously can learn things just by reading a book, something like that.
[00:16:18.600 --> 00:16:21.100]   But think about if I were to say, oh, here's a new house.
[00:16:21.100 --> 00:16:23.300]   I want you to learn, you know, what do you do?
[00:16:23.300 --> 00:16:23.900]   You have to walk.
[00:16:23.900 --> 00:16:25.600]   You have to walk from room to room.
[00:16:25.600 --> 00:16:29.300]   You have to open the doors, look around, see what's on the left, what's on the right.
[00:16:29.300 --> 00:16:31.600]   As you do this, you're building a model in your head.
[00:16:31.600 --> 00:16:33.500]   It's just, that's what you're doing.
[00:16:33.500 --> 00:16:36.100]   You can't just sit there and say, I'm going to grok the house.
[00:16:36.100 --> 00:16:39.800]   No, you know, or you could, you don't even want to just sit there and read some description of it.
[00:16:39.800 --> 00:16:40.300]   Right?
[00:16:40.300 --> 00:16:40.800]   Yeah.
[00:16:40.800 --> 00:16:42.200]   You literally physically interact.
[00:16:42.200 --> 00:16:43.300]   And the same with like a smartphone.
[00:16:43.300 --> 00:16:46.200]   If I'm going to learn a new app, I touch it and I move things around.
[00:16:46.200 --> 00:16:48.700]   I see what happens when I do things with it.
[00:16:48.700 --> 00:16:50.700]   So that's the basic way we learn in the world.
[00:16:50.700 --> 00:16:56.200]   And by the way, when you say model, you mean something that can be used for prediction in the future.
[00:16:56.200 --> 00:17:00.500]   It's used for prediction and for behavior and planning.
[00:17:00.500 --> 00:17:01.500]   Right.
[00:17:01.500 --> 00:17:04.600]   And does a pretty good job at doing so.
[00:17:04.600 --> 00:17:05.200]   Yeah.
[00:17:05.200 --> 00:17:06.700]   Here's the way to think about a model.
[00:17:06.700 --> 00:17:08.200]   A lot of people get hung up on this.
[00:17:08.200 --> 00:17:13.000]   So you can imagine an architect making a model of a house.
[00:17:13.200 --> 00:17:13.700]   Right.
[00:17:13.700 --> 00:17:15.100]   So there's a physical model.
[00:17:15.100 --> 00:17:15.900]   It's small.
[00:17:15.900 --> 00:17:17.100]   And why do they do that?
[00:17:17.100 --> 00:17:20.300]   Well, we do that because you can imagine what it would look like from different angles.
[00:17:20.300 --> 00:17:20.800]   Okay.
[00:17:20.800 --> 00:17:21.500]   Look from here.
[00:17:21.500 --> 00:17:21.800]   Look there.
[00:17:21.800 --> 00:17:27.400]   And you can also say, well, how far to get from the garage to the swimming pool or something like that.
[00:17:27.400 --> 00:17:27.600]   Right.
[00:17:27.600 --> 00:17:28.900]   You can imagine looking at this.
[00:17:28.900 --> 00:17:30.500]   And so what would you view from this location?
[00:17:30.500 --> 00:17:35.200]   So we build these physical models to let you imagine the future and imagine behaviors.
[00:17:35.200 --> 00:17:38.500]   Now we can take that same model and put it in a computer.
[00:17:38.500 --> 00:17:42.200]   So we now today, they'll build models of houses and a computer.
[00:17:42.200 --> 00:17:48.400]   And they do that using a set of, we'll come back to this term in a moment, reference frames.
[00:17:48.400 --> 00:17:53.100]   But basically you assign a reference frame for the house and you assign different things for the house in different locations.
[00:17:53.100 --> 00:17:57.400]   And then the computer can generate an image and say, okay, this is what it looks like from this direction.
[00:17:57.400 --> 00:18:00.400]   The brain is doing something remarkably similar to this.
[00:18:00.400 --> 00:18:01.300]   Surprising.
[00:18:01.300 --> 00:18:03.400]   It's using reference frames.
[00:18:03.400 --> 00:18:04.100]   It's building these.
[00:18:04.100 --> 00:18:08.100]   It's similar to a model on a computer, which has the same benefits of building a physical model.
[00:18:08.100 --> 00:18:11.900]   It allows me to say, what would this thing look like if it was in this orientation?
[00:18:12.100 --> 00:18:14.100]   What would likely happen if I push this button?
[00:18:14.100 --> 00:18:15.400]   I've never pushed this button before.
[00:18:15.400 --> 00:18:17.500]   Or how would I accomplish something?
[00:18:17.500 --> 00:18:21.800]   I want to convey a new idea I've learned.
[00:18:21.800 --> 00:18:22.900]   How would I do that?
[00:18:22.900 --> 00:18:25.200]   I can imagine in my head, well, I could talk about it.
[00:18:25.200 --> 00:18:26.600]   I could write a book.
[00:18:26.600 --> 00:18:28.600]   I could do some podcasts.
[00:18:28.600 --> 00:18:35.700]   I could, you know, maybe tell my neighbor, you know, and I can imagine the outcomes of all these things before I do any of them.
[00:18:35.700 --> 00:18:37.400]   That's what the model lets you do.
[00:18:37.400 --> 00:18:41.500]   It lets us plan the future and imagine the consequences of our actions.
[00:18:41.800 --> 00:18:44.000]   Prediction, you asked about prediction.
[00:18:44.000 --> 00:18:46.600]   Prediction is not the goal of the model.
[00:18:46.600 --> 00:18:49.200]   Prediction is an inherent property of it.
[00:18:49.200 --> 00:18:51.700]   And it's how the model corrects itself.
[00:18:51.700 --> 00:18:55.000]   So prediction is fundamental to intelligence.
[00:18:55.000 --> 00:18:57.200]   It's fundamental to building a model.
[00:18:57.200 --> 00:18:58.700]   And the model is intelligent.
[00:18:58.700 --> 00:19:01.500]   And let me go back and be very precise about this.
[00:19:01.500 --> 00:19:03.200]   Prediction, you can think of prediction two ways.
[00:19:03.200 --> 00:19:05.300]   One is like, hey, what would happen if I did this?
[00:19:05.300 --> 00:19:06.400]   That's a type of prediction.
[00:19:06.400 --> 00:19:08.700]   That's a key part of intelligence.
[00:19:08.700 --> 00:19:13.200]   But even prediction is like, oh, what's this water bottle going to feel like when I pick it up?
[00:19:13.200 --> 00:19:15.800]   You know, and that doesn't seem very intelligent.
[00:19:15.800 --> 00:19:22.800]   But the way to think, one way to think about prediction is it's a way for us to learn where our model is wrong.
[00:19:22.800 --> 00:19:27.300]   So if I picked up this water bottle and it felt hot, I'd be very surprised.
[00:19:27.300 --> 00:19:30.300]   Or if I picked it up, it was very light, I'd be surprised.
[00:19:30.300 --> 00:19:35.400]   Or if I turned this top and it didn't, I had to turn it the other way, I'd be surprised.
[00:19:35.800 --> 00:19:39.100]   And so all those might have a prediction like, okay, I'm going to do it, I'll drink some water.
[00:19:39.100 --> 00:19:40.400]   I'm going to go, okay, I do this.
[00:19:40.400 --> 00:19:41.100]   There it is.
[00:19:41.100 --> 00:19:42.000]   I feel opening, right?
[00:19:42.000 --> 00:19:43.300]   What if I had to turn it the other way?
[00:19:43.300 --> 00:19:44.800]   Or what if it's split in two?
[00:19:44.800 --> 00:19:47.400]   Then I say, oh my gosh, I misunderstood this.
[00:19:47.400 --> 00:19:48.800]   I didn't have the right model of this thing.
[00:19:48.800 --> 00:19:50.100]   My attention would be drawn to it.
[00:19:50.100 --> 00:19:52.200]   I'll be looking at it going, well, how the hell did that happen?
[00:19:52.200 --> 00:19:53.700]   You know, why did it open up that way?
[00:19:53.700 --> 00:19:56.300]   And I would update my model by doing it.
[00:19:56.300 --> 00:19:59.500]   Just by looking at it and playing around with it, I'd update it and say, this is a new type of water bottle.
[00:19:59.900 --> 00:20:09.400]   But you're talking about sort of complicated things like a water bottle, but this also applies for just basic vision, just like seeing things.
[00:20:09.400 --> 00:20:14.400]   It's almost like a precondition of just perceiving the world is predicting.
[00:20:14.400 --> 00:20:14.700]   Mm hmm.
[00:20:14.700 --> 00:20:21.100]   It's just everything that you see is first passed through your prediction.
[00:20:21.100 --> 00:20:23.100]   Everything you see and feel.
[00:20:23.100 --> 00:20:26.600]   In fact, this is the insight I had back in the late 80s.
[00:20:26.600 --> 00:20:28.700]   No, excuse me, early 80s.
[00:20:29.000 --> 00:20:38.800]   And I know that people have reached the same idea, is that every sensory input you get, not just vision, but touch and hearing, you have an expectation about it.
[00:20:38.800 --> 00:20:41.300]   And a prediction.
[00:20:41.300 --> 00:20:43.000]   Sometimes you can predict very accurately.
[00:20:43.000 --> 00:20:43.900]   Sometimes you can't.
[00:20:43.900 --> 00:20:46.600]   I can't predict what next word is going to come out of your mouth.
[00:20:46.600 --> 00:20:49.000]   But as you start talking, I'll get better and better predictions.
[00:20:49.000 --> 00:20:51.600]   And if you talk about some topics, I'd be very surprised.
[00:20:51.600 --> 00:20:57.300]   So I have this sort of background prediction that's going on all the time for all of my senses.
[00:20:58.200 --> 00:21:02.500]   Again, the way I think about that is this is how we learn.
[00:21:02.500 --> 00:21:04.500]   It's more about how we learn.
[00:21:04.500 --> 00:21:06.700]   It's a test of our understanding.
[00:21:06.700 --> 00:21:07.900]   Our predictions are a test.
[00:21:07.900 --> 00:21:09.900]   Is this really a water bottle?
[00:21:09.900 --> 00:21:13.500]   If it is, I shouldn't see, you know, a little finger sticking out the side.
[00:21:13.500 --> 00:21:16.100]   And if I saw a little finger sticking out, I was like, what the hell's going on?
[00:21:16.100 --> 00:21:17.900]   That's not normal.
[00:21:17.900 --> 00:21:24.600]   I mean, that's fascinating that, let me linger on this for a second.
[00:21:25.200 --> 00:21:33.100]   It really honestly feels that prediction is fundamental to everything, to the way our mind operates, to intelligence.
[00:21:33.100 --> 00:21:39.700]   So like, it's just a different way to see intelligence, which is like everything starts at prediction.
[00:21:39.700 --> 00:21:41.600]   And prediction requires a model.
[00:21:41.600 --> 00:21:45.000]   You can't predict something unless you have a model of it.
[00:21:45.000 --> 00:21:45.500]   Right.
[00:21:45.500 --> 00:21:47.600]   But the action is prediction.
[00:21:47.600 --> 00:21:50.300]   So like the thing the model does is prediction.
[00:21:50.300 --> 00:21:58.700]   But it also, yeah, but you can then extend it to things like, what would happen if I took this today?
[00:21:58.700 --> 00:21:59.700]   I went and did this.
[00:21:59.700 --> 00:22:00.600]   What would be like that?
[00:22:00.600 --> 00:22:04.900]   Or how you can extend prediction to like, oh, I want to get a promotion at work.
[00:22:04.900 --> 00:22:07.200]   What action should I take?
[00:22:07.200 --> 00:22:09.400]   And you can say, if I did this, I predict what might happen.
[00:22:09.400 --> 00:22:11.600]   If I spoke to someone, I predict what might happen.
[00:22:11.600 --> 00:22:13.500]   So it's not just low-level predictions.
[00:22:13.500 --> 00:22:15.100]   Yeah, it's all predictions.
[00:22:15.100 --> 00:22:16.600]   It's like this black box.
[00:22:16.600 --> 00:22:19.100]   You can ask basically any question, low-level or high-level.
[00:22:19.100 --> 00:22:20.800]   So we started off with that observation.
[00:22:20.800 --> 00:22:23.800]   It's all, it's just this nonstop prediction.
[00:22:23.800 --> 00:22:24.900]   And I write about this in the book.
[00:22:24.900 --> 00:22:29.000]   And then we asked, how do neurons actually make predictions physically?
[00:22:29.000 --> 00:22:31.100]   Like, what does the neuron do when it makes a prediction?
[00:22:31.100 --> 00:22:34.700]   And, or the neural tissue does when it makes a prediction.
[00:22:34.700 --> 00:22:38.800]   And then we asked, what are the mechanisms by how we build a model that allows you to make prediction?
[00:22:38.800 --> 00:22:45.400]   So we started with prediction as sort of the fundamental research agenda, if in some sense.
[00:22:45.500 --> 00:22:48.800]   Like, and say, well, we understand how the brain makes predictions.
[00:22:48.800 --> 00:22:51.400]   We'll understand how it builds these models and how it learns.
[00:22:51.400 --> 00:22:52.900]   And that's the core of intelligence.
[00:22:52.900 --> 00:22:57.600]   So it was like, it was the key that got us in the door to say, that is our research agenda.
[00:22:57.600 --> 00:22:58.900]   Understand predictions.
[00:22:58.900 --> 00:23:05.300]   So in this whole process, where does intelligence originate, would you say?
[00:23:05.300 --> 00:23:15.400]   So if we look at things that are much less intelligence than humans, and you start to build up a human through the process of evolution,
[00:23:16.100 --> 00:23:25.800]   where's this magic thing that has a prediction model or a model that's able to predict that starts to look a lot more like intelligence?
[00:23:25.800 --> 00:23:32.700]   Is there a place where, Richard Dawkins wrote an introduction to your book, an excellent introduction.
[00:23:32.700 --> 00:23:35.700]   I mean, it puts a lot of things into context.
[00:23:35.700 --> 00:23:40.800]   It's funny just looking at parallels for your book and Darwin's Origin of Species.
[00:23:40.800 --> 00:23:44.600]   So Darwin wrote about the origin of species.
[00:23:44.600 --> 00:23:48.800]   So what is the origin of intelligence?
[00:23:48.800 --> 00:23:49.000]   Yeah.
[00:23:49.000 --> 00:23:52.100]   Well, we have a theory about it, and it's just that, it's a theory.
[00:23:52.100 --> 00:23:53.700]   Theory goes as follows.
[00:23:53.700 --> 00:23:58.700]   As soon as living things started to move, they're not just floating in the sea.
[00:23:58.700 --> 00:24:01.800]   They're not just a plant, you know, grounded someplace.
[00:24:01.800 --> 00:24:08.000]   As soon as they started to move, there was an advantage to moving intelligently, to moving in certain ways.
[00:24:08.300 --> 00:24:16.900]   And there's some very simple things you can do, you know, bacteria or single-cell organisms can move towards a source of gradient of food or something like that.
[00:24:16.900 --> 00:24:25.300]   But an animal that might know where it is and know where it's been and how to get back to that place, or an animal that might say, "Oh, there was a source of food someplace.
[00:24:25.300 --> 00:24:26.200]   How do I get to it?"
[00:24:26.200 --> 00:24:27.700]   Or, "There was a danger.
[00:24:27.700 --> 00:24:28.300]   How do I get to it?"
[00:24:28.300 --> 00:24:29.100]   Or, "There was a mate.
[00:24:29.100 --> 00:24:30.000]   How do I get to them?"
[00:24:30.000 --> 00:24:33.500]   There was a big evolution advantage to that.
[00:24:33.500 --> 00:24:40.300]   So early on, there was a pressure to start understanding your environment, like, "Where am I and where have I been?
[00:24:40.300 --> 00:24:42.400]   And what happened in those different places?"
[00:24:42.400 --> 00:24:47.400]   So we still have this neural mechanism in our brains.
[00:24:47.400 --> 00:24:50.300]   It's in the mammals.
[00:24:50.300 --> 00:24:53.200]   It's in the hippocampus and entorhinal cortex.
[00:24:53.200 --> 00:24:54.500]   These are older parts of the brain.
[00:24:54.500 --> 00:24:57.300]   And these are very well studied.
[00:24:57.300 --> 00:25:00.700]   We build a map of our environment.
[00:25:00.700 --> 00:25:06.900]   So these neurons in these parts of the brain know where I am in this room and where the door was and things like that.
[00:25:06.900 --> 00:25:09.400]   - So a lot of other mammals have this kind of capability.
[00:25:09.400 --> 00:25:10.500]   - All mammals have this.
[00:25:10.500 --> 00:25:10.900]   Right?
[00:25:10.900 --> 00:25:21.000]   And almost any animal that knows where it is and can get around must have some mapping system, must have some way of saying, "I've learned a map of my environment.
[00:25:21.000 --> 00:25:23.000]   I have hummingbirds in my backyard."
[00:25:23.000 --> 00:25:25.300]   And they go to the same places all the time.
[00:25:25.300 --> 00:25:27.300]   They must know where they are.
[00:25:27.300 --> 00:25:28.700]   They just know where they are when they're flying.
[00:25:28.700 --> 00:25:30.000]   They're not just randomly flying around.
[00:25:30.000 --> 00:25:32.100]   They know particular flowers they come back to.
[00:25:32.100 --> 00:25:34.400]   So we all have this.
[00:25:34.400 --> 00:25:40.500]   And it turns out it's very tricky to get neurons to do this, to build a map of an environment.
[00:25:40.500 --> 00:25:51.900]   And so we now know there's these famous studies that's still very active about place cells and grid cells and these other types of cells in the older parts of the brain and how they build these maps of the world.
[00:25:51.900 --> 00:25:53.000]   And it's really clever.
[00:25:53.000 --> 00:25:57.200]   It's obviously been under a lot of evolutionary pressure over a long period of time to get good at this.
[00:25:57.200 --> 00:25:59.500]   So animals now know where they are.
[00:26:00.100 --> 00:26:11.900]   What we think has happened, and there's a lot of evidence to suggest this, is that that mechanism we learned to map like a space was repackaged.
[00:26:11.900 --> 00:26:16.600]   The same type of neurons was repackaged into a more compact form.
[00:26:16.600 --> 00:26:19.600]   And that became the cortical column.
[00:26:19.600 --> 00:26:23.700]   And it was in some sense genericized, if that's a word.
[00:26:23.700 --> 00:26:33.800]   It was turned into a very specific thing about learning maps of environments to learning maps of anything, learning a model of anything, not just your space, but coffee cups and so on.
[00:26:33.800 --> 00:26:42.600]   And it got sort of repackaged into a more compact version, a more universal version, and then replicated.
[00:26:42.600 --> 00:26:50.800]   So the reason we're so flexible is we have a very generic version of this mapping algorithm and we have 150,000 copies of it.
[00:26:51.200 --> 00:26:53.700]   Sounds a lot like the progress of deep learning.
[00:26:53.700 --> 00:26:55.200]   How so?
[00:26:55.200 --> 00:27:07.500]   So take neural networks that seem to work well for a specific task, compress them and multiply it by a lot.
[00:27:07.500 --> 00:27:09.200]   And then you just stack them on top of it.
[00:27:09.200 --> 00:27:13.300]   It's like the story of Transformers in Natural Language Processing.
[00:27:13.300 --> 00:27:20.000]   But in deep learning networks, they end up, you're replicating an element, but you still need the entire network to do anything.
[00:27:20.000 --> 00:27:20.300]   Right.
[00:27:20.700 --> 00:27:25.500]   Here, what's going on, each individual element is a complete learning system.
[00:27:25.500 --> 00:27:29.100]   This is why I can take a human brain, cut it in half, and it still works.
[00:27:29.100 --> 00:27:31.700]   It's pretty amazing.
[00:27:31.700 --> 00:27:33.100]   It's fundamentally distributed.
[00:27:33.100 --> 00:27:35.500]   It's fundamentally distributed, complete modeling systems.
[00:27:35.500 --> 00:27:39.100]   So, but that's our story we like to tell.
[00:27:39.100 --> 00:27:43.100]   I would guess it's likely largely right.
[00:27:43.100 --> 00:27:48.100]   But there's a lot of evidence supporting that story, this evolutionary story.
[00:27:50.000 --> 00:27:55.300]   The thing which brought me to this idea is that the human brain got big very quickly.
[00:27:55.300 --> 00:28:04.500]   So that led to the proposal a long time ago that, well, there's this common element, just instead of creating new things, it just replicated something.
[00:28:04.500 --> 00:28:06.600]   We also are extremely flexible.
[00:28:06.600 --> 00:28:10.000]   We can learn things that we had no history about.
[00:28:10.000 --> 00:28:11.000]   Right.
[00:28:11.000 --> 00:28:15.100]   And so that tells us that the learning algorithm is very generic.
[00:28:15.100 --> 00:28:20.200]   It's very kind of universal because it doesn't assume any prior knowledge about what it's learning.
[00:28:20.200 --> 00:28:26.100]   And so you combine those things together and you say, OK, well, how did that come about?
[00:28:26.100 --> 00:28:27.900]   Where did that universal algorithm come from?
[00:28:27.900 --> 00:28:29.800]   It had to come from something that wasn't universal.
[00:28:29.800 --> 00:28:31.300]   It came from something that was more specific.
[00:28:31.300 --> 00:28:37.600]   And so anyway, this led to our hypothesis that you would find grid cells and place cell equivalents in the neocortex.
[00:28:37.600 --> 00:28:43.300]   And when we first published our first papers on this theory, we didn't know of evidence for that.
[00:28:43.500 --> 00:28:45.300]   It turns out there was some, but we didn't know about it.
[00:28:45.300 --> 00:28:50.500]   And since then, so then we became aware of evidence for grid cells and for parts of the neocortex.
[00:28:50.500 --> 00:28:53.000]   And then now there's been new evidence coming out.
[00:28:53.000 --> 00:28:56.500]   There's some interesting papers that came out just January of this year.
[00:28:56.500 --> 00:29:06.600]   So one of our predictions was if this evolutionary hypothesis is correct, we would see grid cell place cell equivalent cells that work like them through every column in the neocortex.
[00:29:06.600 --> 00:29:07.600]   And that's starting to be seen.
[00:29:07.600 --> 00:29:12.100]   What does it mean that, why is it important that they're present?
[00:29:12.100 --> 00:29:15.900]   Because it tells us, well, we're asking about the evolutionary origin of intelligence, right?
[00:29:15.900 --> 00:29:24.000]   So our theory is that these columns in the cortex are working on the same principles, they're modeling systems.
[00:29:24.000 --> 00:29:26.700]   And it's hard to imagine how neurons do this.
[00:29:26.700 --> 00:29:31.600]   And so we said, hey, it's really hard to imagine how neurons could learn these models of things.
[00:29:31.600 --> 00:29:33.300]   We can talk about the details of that if you want.
[00:29:33.300 --> 00:29:39.800]   But there's this other part of the brain, we know that learns models of environments.
[00:29:40.600 --> 00:29:45.000]   So could that mechanism to learn to model this room be used to learn to model the water bottle?
[00:29:45.000 --> 00:29:47.000]   Is it the same mechanism?
[00:29:47.000 --> 00:29:53.700]   So we said it's much more likely the brain's using the same mechanism, which case it would have these equivalent cell types.
[00:29:53.700 --> 00:30:03.200]   So it's basically the whole theory is built on the idea that these columns have reference frames and we're learning these models and these grid cells create these reference frames.
[00:30:03.600 --> 00:30:16.500]   So it's basically the major, in some sense, the major predictive part of this theory is that we will find these equivalent mechanisms in each column in the inner cortex, which tells us that that's what they're doing.
[00:30:16.500 --> 00:30:19.200]   They're learning these sensory motor models of the world.
[00:30:19.200 --> 00:30:22.600]   So just we're pretty confident that would happen.
[00:30:22.600 --> 00:30:23.700]   But now we're seeing the evidence.
[00:30:23.700 --> 00:30:28.600]   So the evolutionary process nature does a lot of copy and paste and see what happens.
[00:30:28.600 --> 00:30:29.000]   Yeah.
[00:30:29.000 --> 00:30:29.600]   Yeah.
[00:30:29.600 --> 00:30:30.400]   There's no direction to it.
[00:30:30.500 --> 00:30:37.100]   But it just found out like, hey, if I took this, these elements and made more of them, what happens?
[00:30:37.100 --> 00:30:39.500]   And let's hook them up to the eyes and let's hook them up to ears.
[00:30:39.500 --> 00:30:43.100]   And that seems to work pretty well for us.
[00:30:43.100 --> 00:30:48.400]   Again, just to take a quick step back to our conversation of collective intelligence.
[00:30:48.400 --> 00:30:54.600]   Do you sometimes see that as just another copy and paste aspect?
[00:30:54.600 --> 00:31:00.100]   Is copying and pasting these brains in humans and making a lot of them?
[00:31:00.500 --> 00:31:05.900]   And then creating social structures that then almost operate as a single brain?
[00:31:05.900 --> 00:31:08.500]   I wouldn't have said it, but you said it sounded pretty good.
[00:31:08.500 --> 00:31:15.300]   So to you, the brain is like a, is its own thing.
[00:31:15.300 --> 00:31:18.800]   I mean, our goal is to understand how the neural cortex works.
[00:31:18.800 --> 00:31:24.300]   We can argue how essential that is to understanding the human brain, because it's not the entire human brain.
[00:31:24.300 --> 00:31:28.500]   You can argue how essential that is to understanding human intelligence.
[00:31:28.700 --> 00:31:34.500]   You can argue how essential this is to communal intelligence.
[00:31:34.500 --> 00:31:38.700]   Our goal was to understand the neural cortex.
[00:31:38.700 --> 00:31:38.900]   Yeah.
[00:31:38.900 --> 00:31:44.700]   So what is the neural cortex and where does it fit in the various aspects of what the brain does?
[00:31:44.700 --> 00:31:46.200]   Like, how important is it to you?
[00:31:46.200 --> 00:31:54.500]   Well, obviously, again, I mentioned again in the beginning, it's about 70 to 75% of the volume of a human brain.
[00:31:54.700 --> 00:31:59.900]   So it dominates our brain in terms of size, not in terms of number of neurons, but in terms of size.
[00:31:59.900 --> 00:32:02.500]   Size isn't everything, Jeff.
[00:32:02.500 --> 00:32:06.300]   I know, but it's nothing, it's not that.
[00:32:06.300 --> 00:32:11.300]   We know that all high level vision, hearing and touch happens in the neural cortex.
[00:32:11.300 --> 00:32:20.800]   We know that all language occurs and is understood in the neural cortex, whether that's spoken language, written language, sign language, language of mathematics, language of physics, music.
[00:32:22.000 --> 00:32:25.300]   We know that all high level planning and thinking occurs in the neural cortex.
[00:32:25.300 --> 00:32:32.500]   If I were to say, you know, what part of your brain designed a computer and understands programming and creates music, it's all the neural cortex.
[00:32:32.500 --> 00:32:36.400]   So then that's an undeniable fact.
[00:32:36.400 --> 00:32:40.400]   But then there's other parts of our brain are important too, right?
[00:32:40.400 --> 00:32:43.600]   Our emotional states, our body regulating our body.
[00:32:44.600 --> 00:32:52.000]   So the way I like to look at it is, you know, can you understand the neural cortex without the rest of the brain?
[00:32:52.000 --> 00:32:54.900]   And some people say you can't, and I think absolutely you can.
[00:32:54.900 --> 00:32:58.400]   It's not that they're not interacting, but you can understand it.
[00:32:58.400 --> 00:33:01.800]   Can you understand the neural cortex without understanding the emotions of fear?
[00:33:01.800 --> 00:33:02.500]   Yes, you can.
[00:33:02.500 --> 00:33:03.900]   You can understand how the system works.
[00:33:03.900 --> 00:33:05.200]   It's just a modeling system.
[00:33:05.200 --> 00:33:12.500]   I make the analogy in the book that it's like a map of the world, and how that map is used depends on who's using it.
[00:33:12.800 --> 00:33:20.000]   So how our map of our world in our neural cortex, how we manifest as a human, depends on the rest of our brain.
[00:33:20.000 --> 00:33:21.200]   What are our motivations?
[00:33:21.200 --> 00:33:22.300]   You know, what are my desires?
[00:33:22.300 --> 00:33:23.800]   Am I a nice guy or not a nice guy?
[00:33:23.800 --> 00:33:26.500]   Am I a cheater or not a cheater?
[00:33:26.500 --> 00:33:31.200]   You know, how important different things are in my life.
[00:33:31.200 --> 00:33:36.500]   But the neural cortex can be understood on its own.
[00:33:36.500 --> 00:33:41.500]   And I say that as a neuroscientist, I know there's all these interactions.
[00:33:41.500 --> 00:33:44.600]   And I want to say I don't know them, and we don't think about them.
[00:33:44.600 --> 00:33:48.000]   But from a layperson's point of view, you can say it's a modeling system.
[00:33:48.000 --> 00:33:54.200]   I don't generally think too much about the communal aspect of intelligence, which you brought up a number of times already.
[00:33:54.200 --> 00:33:56.800]   So that's not really been my concern.
[00:33:56.800 --> 00:33:58.500]   - I just wonder if there's a continuum.
[00:33:58.500 --> 00:34:07.600]   From the origin of the universe, like, this pockets of complexities that form living organisms.
[00:34:07.700 --> 00:34:12.900]   I wonder if we're just, if you look at humans, we feel like we're at the top.
[00:34:12.900 --> 00:34:24.800]   I wonder if there's like just, where everybody probably, every living type pocket of complexity is probably thinks they're the, pardon the French, they're the shit.
[00:34:24.800 --> 00:34:26.600]   They're at the top of the pyramid.
[00:34:26.600 --> 00:34:28.000]   - Well, if they're thinking.
[00:34:28.000 --> 00:34:30.600]   - Well, then what is thinking?
[00:34:30.600 --> 00:34:40.400]   - In a sense, the whole point is in their sense of the world, their sense is that they're at the top of it.
[00:34:40.400 --> 00:34:41.900]   Like, what does a turtle think?
[00:34:41.900 --> 00:34:50.300]   - But you're bringing up, the problems of complexity and complexity theory are, it's a huge, interesting problem in science.
[00:34:50.300 --> 00:34:57.900]   And I think we've made surprisingly little progress in understanding complex systems in general.
[00:34:59.200 --> 00:35:02.500]   And so, the Santa Fe Institute was founded to study this.
[00:35:02.500 --> 00:35:04.900]   And even the scientists there will say, it's really hard.
[00:35:04.900 --> 00:35:10.300]   We haven't really been able to figure out exactly, that science isn't really congealed yet.
[00:35:10.300 --> 00:35:13.200]   We're still trying to figure out the basic elements of that science.
[00:35:13.200 --> 00:35:16.600]   What, where does complexity come from?
[00:35:16.600 --> 00:35:18.000]   And what is it and how you define it?
[00:35:18.000 --> 00:35:26.100]   Whether it's DNA creating bodies or phenotypes or it's individuals creating societies or ants and markets and so on.
[00:35:26.100 --> 00:35:27.900]   It's a very complex thing.
[00:35:28.100 --> 00:35:30.600]   I'm not a complexity theorist person, right?
[00:35:30.600 --> 00:35:35.400]   I think you need to ask, well, the brain itself is a complex system.
[00:35:35.400 --> 00:35:37.400]   So, can we understand that?
[00:35:37.400 --> 00:35:40.400]   I think we've made a lot of progress understanding how the brain works.
[00:35:40.400 --> 00:35:46.000]   So, but I haven't brought it out to like, oh, well, where are we on the complexity spectrum?
[00:35:46.000 --> 00:35:49.700]   It's like, it's a great question.
[00:35:49.700 --> 00:35:53.900]   - I prefer for that answer to be, we're not special.
[00:35:54.700 --> 00:35:58.800]   It seems like if we're honest, most likely we're not special.
[00:35:58.800 --> 00:36:03.300]   So, if there is a spectrum, we're probably not in some kind of significant place in that spectrum.
[00:36:03.300 --> 00:36:05.400]   - I think there's one thing we could say that we are special.
[00:36:05.400 --> 00:36:07.500]   And again, only here on Earth.
[00:36:07.500 --> 00:36:09.100]   I'm not saying I'm bad.
[00:36:09.100 --> 00:36:21.200]   Is that if we think about knowledge, what we know, we clearly, human brains have, are the only brains that have a certain types of knowledge.
[00:36:21.200 --> 00:36:27.800]   We're the only brains on this Earth to understand what the Earth is, how old it is, that the universe is a picture as a whole.
[00:36:27.800 --> 00:36:32.800]   We're the only organisms to understand DNA and the origins of, you know, of species.
[00:36:32.800 --> 00:36:37.300]   No other species on this planet has that knowledge.
[00:36:37.300 --> 00:36:45.900]   So, if we think about, I like to think about, you know, one of the endeavors of humanity is to understand the universe as much as we can.
[00:36:45.900 --> 00:36:50.800]   I think our species is further along in that, undeniably.
[00:36:51.800 --> 00:36:54.900]   Whether our theories are right or wrong, we can debate, but at least we have theories.
[00:36:54.900 --> 00:37:00.100]   You know, we know that what the sun is and how its fusion is and how, what black holes are.
[00:37:00.100 --> 00:37:04.900]   And, you know, we know general theory of relativity and no other animal has any of this knowledge.
[00:37:04.900 --> 00:37:07.400]   So, in that sense that we're special.
[00:37:07.400 --> 00:37:12.600]   Are we special in terms of the hierarchy of complexity in the universe?
[00:37:12.600 --> 00:37:13.200]   Probably not.
[00:37:13.200 --> 00:37:18.100]   - Can we look at a neuron?
[00:37:19.800 --> 00:37:22.500]   You say that prediction happens in the neuron.
[00:37:22.500 --> 00:37:23.400]   What does that mean?
[00:37:23.400 --> 00:37:27.200]   So, the neuron traditionally is seen as the basic element of the brain.
[00:37:27.200 --> 00:37:31.600]   - So, I mentioned this earlier, that prediction was our research agenda.
[00:37:31.600 --> 00:37:32.200]   - Yeah.
[00:37:32.200 --> 00:37:35.600]   - We said, okay, how does the brain make a prediction?
[00:37:35.600 --> 00:37:42.600]   Like, I'm about to grab this water bottle and my brain is predicting what I'm gonna feel on all my parts of my fingers.
[00:37:42.600 --> 00:37:45.200]   If I felt something really odd on any part here, I'd notice it.
[00:37:45.200 --> 00:37:48.000]   So, my brain is predicting what it's gonna feel as I grab this thing.
[00:37:49.000 --> 00:37:52.300]   So, what is that, how does that manifest itself in neural tissue, right?
[00:37:52.300 --> 00:38:01.400]   We got brains made of neurons and there's chemicals and there's neurons and there's spikes and the connect, you know, where is the prediction going on?
[00:38:01.400 --> 00:38:08.800]   And one argument could be that, well, when I'm predicting something, a neuron must be firing in advance.
[00:38:08.800 --> 00:38:13.000]   It's like, okay, this neuron represents what you're gonna feel and it's firing, it's sending a spike.
[00:38:13.000 --> 00:38:15.600]   And certainly that happens to some extent.
[00:38:15.600 --> 00:38:18.000]   But our predictions are so ubiquitous.
[00:38:18.900 --> 00:38:21.200]   They were making so many of them, which we're totally unaware of.
[00:38:21.200 --> 00:38:23.600]   Just the vast majority of them, you have no idea that you're doing this.
[00:38:23.600 --> 00:38:28.700]   That it wasn't really, we were trying to figure out how could this be?
[00:38:28.700 --> 00:38:31.200]   Where are these happening, right?
[00:38:31.200 --> 00:38:45.600]   And I won't walk you through the whole story unless you insist upon it, but we came to the realization that most of your predictions are occurring inside individual neurons, especially the most common are in the pyramidal cells.
[00:38:46.500 --> 00:38:48.700]   And there's a property of neurons.
[00:38:48.700 --> 00:38:55.800]   Everyone knows, or most people know, that a neuron is a cell and it has this spike called an action potential and it sends information.
[00:38:55.800 --> 00:38:59.400]   But we now know that there's these spikes internal to the neuron.
[00:38:59.400 --> 00:39:00.900]   They're called dendritic spikes.
[00:39:00.900 --> 00:39:05.000]   They travel along the branches of the neuron and they don't leave the neuron.
[00:39:05.000 --> 00:39:06.100]   They're just internal only.
[00:39:06.100 --> 00:39:10.900]   There's far more dendritic spikes than there are action potentials, far more.
[00:39:10.900 --> 00:39:12.300]   They're happening all the time.
[00:39:12.800 --> 00:39:19.200]   And what we came to understand that those dendritic spikes, the ones that are occurring, are actually a form of prediction.
[00:39:19.200 --> 00:39:24.800]   They're telling the neuron, the neuron is saying, "I expect that I might become active shortly."
[00:39:24.800 --> 00:39:32.100]   And that internal, so the internal spike is a way of saying, "You might be generating external spikes soon.
[00:39:32.100 --> 00:39:34.300]   I've predicted you're going to become active."
[00:39:34.300 --> 00:39:41.700]   And we wrote a paper in 2016 which explained how this manifests itself in neural tissue.
[00:39:42.300 --> 00:39:44.600]   And how it is that this all works together.
[00:39:44.600 --> 00:39:48.400]   But the vast majority, we think there's a lot of evidence supporting it.
[00:39:48.400 --> 00:39:52.100]   So that's where we think that most of these predictions are internal.
[00:39:52.100 --> 00:39:55.500]   That's why you can't be, they're internal to the neuron, you can't perceive them.
[00:39:55.500 --> 00:40:07.800]   - From understanding the prediction mechanism of a single neuron, do you think there's deep insights to be gained about the prediction capabilities of the mini brains and then the bigger brain and then the brain?
[00:40:07.800 --> 00:40:08.500]   - Oh yeah, yeah, yeah.
[00:40:08.500 --> 00:40:11.700]   So having a prediction side of individual neuron is not that useful.
[00:40:12.400 --> 00:40:13.300]   You know, so what?
[00:40:13.300 --> 00:40:23.500]   The way it manifests itself in neural tissue is that when a neuron, a neuron emits these spikes, a very singular type of event.
[00:40:23.500 --> 00:40:31.800]   If a neuron is predicting that it's going to be active, it emits its spike very, a little bit sooner, just a few milliseconds sooner than it would have otherwise.
[00:40:31.800 --> 00:40:36.500]   It's like, I give the analogy in the book, it's like a sprinter on a starting blocks in a race.
[00:40:37.100 --> 00:40:41.200]   And if someone says, get ready, set, you get up and you're ready to go.
[00:40:41.200 --> 00:40:43.900]   And then when you're ready to start, you get a little bit earlier start.
[00:40:43.900 --> 00:40:48.000]   So that it's that, that ready set is like the prediction and the neurons like ready to go quicker.
[00:40:48.000 --> 00:40:57.900]   And what happens is when you have a whole bunch of neurons together and they're all getting these inputs, the ones that are in the predictive state, the ones that are anticipating to become active.
[00:40:57.900 --> 00:41:01.600]   If they do become active, they happen sooner, they disable everything else.
[00:41:01.600 --> 00:41:03.600]   And it leads to different representations in the brain.
[00:41:03.600 --> 00:41:08.100]   So you have to, it's not isolated just to the neuron.
[00:41:08.100 --> 00:41:11.400]   The prediction occurs with the neuron, but the network behavior changes.
[00:41:11.400 --> 00:41:16.300]   So what happens under different predictions, different inputs have different representations.
[00:41:16.300 --> 00:41:21.700]   So how I, what I predict is going to be different under different contexts.
[00:41:21.700 --> 00:41:24.600]   You know, what my input will be is different under different contexts.
[00:41:24.600 --> 00:41:27.700]   So this is, this is a key little theory, how this works.
[00:41:27.700 --> 00:41:34.100]   So the theory of the thousand brains, if you were to count the number of brains, how would you do it?
[00:41:34.100 --> 00:41:42.100]   The thousand brain theory says that basically every cortical column in your neocortex is a complete modeling system.
[00:41:42.100 --> 00:41:46.100]   And that when I ask, where do I have a model of something like a coffee cup?
[00:41:46.100 --> 00:41:47.500]   It's not in one of those models.
[00:41:47.500 --> 00:41:48.900]   It's in thousands of those models.
[00:41:48.900 --> 00:41:50.900]   There's thousands of models of coffee cups.
[00:41:50.900 --> 00:41:52.800]   That's what the thousand brains theory says.
[00:41:52.800 --> 00:41:53.800]   And there's a voting mechanism.
[00:41:53.800 --> 00:41:59.300]   And there's a voting mechanism, which you lead, which you're, which is the thing you're, which you're conscious of, which leads to your singular perception.
[00:41:59.300 --> 00:42:02.600]   That's why you perceive something.
[00:42:02.600 --> 00:42:04.600]   So that's the thousand brains theory.
[00:42:04.600 --> 00:42:10.000]   The details, how we got to that theory are complicated.
[00:42:10.000 --> 00:42:12.200]   It wasn't, we just thought of it one day.
[00:42:12.200 --> 00:42:16.100]   And one of those details, we had to ask, how does a model make predictions?
[00:42:16.100 --> 00:42:18.300]   And we talked about just these predictive neurons.
[00:42:18.300 --> 00:42:19.900]   That's part of this theory.
[00:42:19.900 --> 00:42:21.500]   It's like saying, oh, it's a detail.
[00:42:21.500 --> 00:42:23.000]   But it was like a crack in the door.
[00:42:23.000 --> 00:42:25.400]   It's like, how are we going to figure out how these neurons built through this?
[00:42:25.400 --> 00:42:26.600]   You know, what is going on here?
[00:42:26.600 --> 00:42:30.700]   So we just looked at prediction as like, well, we know that's ubiquitous.
[00:42:30.700 --> 00:42:33.000]   We know that every part of the cortex is making predictions.
[00:42:33.000 --> 00:42:36.900]   Therefore, whatever the predictive system is, it's going to be everywhere.
[00:42:36.900 --> 00:42:39.500]   We know there's a gazillion predictions happening at once.
[00:42:39.500 --> 00:42:45.900]   So this is where we can start teasing apart, you know, ask questions about, you know, how could neurons be making these predictions?
[00:42:45.900 --> 00:42:50.500]   And that sort of built up to now what we have, this thousand brains theory, which is complex.
[00:42:50.500 --> 00:42:52.300]   You know, it's just, I can state it simply.
[00:42:52.600 --> 00:42:53.900]   But we just didn't think of it.
[00:42:53.900 --> 00:42:56.000]   We had to get there step by step.
[00:42:56.000 --> 00:42:59.000]   Very, it took years to get there.
[00:42:59.000 --> 00:43:02.900]   And where does reference frames fit in?
[00:43:02.900 --> 00:43:04.500]   So, yeah.
[00:43:04.500 --> 00:43:05.100]   Okay.
[00:43:05.100 --> 00:43:10.900]   So again, a reference frame, I mentioned earlier about the, you know, a model of a house.
[00:43:10.900 --> 00:43:14.500]   And I said, if you're going to build a model of a house in a computer, they have a reference frame.
[00:43:14.500 --> 00:43:18.700]   And you can think of reference frame like Cartesian coordinates, like X, Y, and Z axes.
[00:43:18.700 --> 00:43:21.300]   So I could say, oh, I'm going to design a house.
[00:43:21.500 --> 00:43:27.200]   I can say, well, the front door is at this location, X, Y, Z, and the roof is at this location, X, Y, Z, and so on.
[00:43:27.200 --> 00:43:28.600]   That's a type of reference frame.
[00:43:28.600 --> 00:43:37.500]   So it turns out for you to make a prediction, and I walk you through the thought experiment in the book where I was predicting what my finger was going to feel when I touch the coffee cup.
[00:43:37.500 --> 00:43:39.800]   It was a ceramic coffee cup, but this one will do.
[00:43:39.800 --> 00:43:50.500]   And what I realized is that to make a prediction, what my finger is going to feel like, it's going to feel different than this, which would feel different if I touch the hole or this thing on the bottom.
[00:43:51.200 --> 00:43:57.400]   To make that prediction, the cortex needs to know where the finger is, the tip of the finger, relative to the coffee cup.
[00:43:57.400 --> 00:44:00.100]   And exactly relative to the coffee cup.
[00:44:00.100 --> 00:44:03.200]   And to do that, I have to have a reference frame for the coffee cup.
[00:44:03.200 --> 00:44:07.100]   It has to have a way of representing the location of my finger to the coffee cup.
[00:44:07.100 --> 00:44:11.500]   And then we realized, of course, every part of your skin has to have a reference frame relative to the things it touches.
[00:44:11.500 --> 00:44:13.000]   And then we did the same thing with vision.
[00:44:13.000 --> 00:44:22.000]   But so the idea that a reference frame is necessary to make a prediction when you're touching something or when you're seeing something and you're moving your eyes, you're moving your fingers.
[00:44:22.000 --> 00:44:24.600]   It's just a requirement to know what to predict.
[00:44:24.600 --> 00:44:27.400]   If I have a structure, I'm going to make a prediction.
[00:44:27.400 --> 00:44:30.200]   I have to know where it is I'm looking or touching it.
[00:44:30.200 --> 00:44:34.200]   So then we said, well, how do neurons make reference frames?
[00:44:34.200 --> 00:44:35.200]   It's not obvious.
[00:44:35.200 --> 00:44:38.000]   You know, X, Y, Z coordinates don't exist in the brain.
[00:44:38.000 --> 00:44:39.600]   It's just not the way it works.
[00:44:40.000 --> 00:44:50.500]   So that's when we looked at the older part of the brain, the hippocampus and the adrenal cortex, where we knew that in that part of the brain, there's a reference frame for a room or reference frame for environment.
[00:44:50.500 --> 00:44:53.500]   Remember I talked earlier about how you could make a map of this room.
[00:44:53.500 --> 00:44:58.800]   So we said, oh, they are implementing reference frames there.
[00:44:58.800 --> 00:45:02.700]   So we knew that reference frames needed to exist in every quarter of a column.
[00:45:02.700 --> 00:45:06.300]   And so that was a deductive thing.
[00:45:06.300 --> 00:45:08.000]   We just deduced it has to exist.
[00:45:08.000 --> 00:45:18.400]   So you take the old mammalian ability to know where you are in a particular space and you start applying that to higher and higher levels.
[00:45:18.400 --> 00:45:21.100]   Yeah, you first you apply it to like where your finger is.
[00:45:21.100 --> 00:45:22.300]   So here's what I think about it.
[00:45:22.300 --> 00:45:25.000]   The old part of the brain says, where's my body in this room?
[00:45:25.000 --> 00:45:25.700]   Yeah.
[00:45:25.700 --> 00:45:30.400]   The new part of the brain says, where's my finger relative to this this object?
[00:45:30.400 --> 00:45:30.800]   Yeah.
[00:45:30.800 --> 00:45:35.400]   Where is a section of my retina relative to this object?
[00:45:35.400 --> 00:45:35.700]   Like where?
[00:45:35.700 --> 00:45:37.400]   Where is I'm looking at one little corner?
[00:45:37.400 --> 00:45:39.300]   Where is that relative to this patch of my retina?
[00:45:39.300 --> 00:45:39.600]   Yeah.
[00:45:39.600 --> 00:45:48.200]   And then we take the same thing and apply it to concepts, mathematics, physics, you know, humanity, whatever you want to think.
[00:45:48.200 --> 00:45:50.300]   Eventually you're pondering your own mortality.
[00:45:50.300 --> 00:45:51.200]   Well, whatever.
[00:45:51.200 --> 00:45:58.100]   But the point is, when we think about the world, when we have knowledge about the world, how is that knowledge organized, Lex?
[00:45:58.100 --> 00:45:59.500]   Where do you, where is it in your head?
[00:45:59.500 --> 00:46:01.500]   The answer is it's in reference frames.
[00:46:01.900 --> 00:46:13.800]   So the way I learned the structure of this water bottle, where the features are relative to each other, when I think about history or democracy or mathematics, the same basic underlying structures happening.
[00:46:13.800 --> 00:46:17.000]   There's reference frames for where the knowledge that you're assigning things to.
[00:46:17.000 --> 00:46:21.100]   So in the book, I go through examples like mathematics and language and politics.
[00:46:21.100 --> 00:46:24.500]   But the evidence is very clear in the neuroscience.
[00:46:24.500 --> 00:46:29.800]   The same mechanism that we use to model this coffee cup, we're going to use to model high level thoughts.
[00:46:30.300 --> 00:46:33.800]   You're, you're, you're the demise of the humanity, whatever you want to think about.
[00:46:33.800 --> 00:46:47.500]   It's interesting to think about how different are the representations of those higher dimensional concepts, higher level concepts, how different the representation there is in terms of reference frames versus spatial.
[00:46:47.500 --> 00:46:53.000]   But interesting thing, it's, it's, it's a different application, but it's the exact same mechanism.
[00:46:53.900 --> 00:47:03.800]   But isn't there some aspect to a higher level concepts that they seem to be hierarchical, like they just seem to integrate a lot of information into that.
[00:47:03.800 --> 00:47:06.100]   So is our physical objects.
[00:47:06.100 --> 00:47:08.400]   So take this water bottle.
[00:47:08.400 --> 00:47:15.400]   I'm not particular to this brand, but this is a Fiji water bottle and it has a logo on it.
[00:47:15.400 --> 00:47:22.000]   I use this example in my book, our, our company's coffee cup has a logo on it, but this object is hierarchical.
[00:47:22.500 --> 00:47:27.700]   It is, it's got like a cylinder and a cap, but then it has this logo on it and the logo has a word.
[00:47:27.700 --> 00:47:30.000]   The word has letters, the letters are different features.
[00:47:30.000 --> 00:47:33.600]   And so I don't have to remember, I don't have to think about this.
[00:47:33.600 --> 00:47:36.000]   So I said, oh, there's a Fiji logo on this water bottle.
[00:47:36.000 --> 00:47:38.600]   I don't have to go through and say, oh, what is the Fiji logo?
[00:47:38.600 --> 00:47:44.700]   It's the F and I and a J and I, and there's a hibiscus flower and, and, oh, it has the pest, you know, the stamen on it.
[00:47:44.700 --> 00:47:45.400]   I don't have to do that.
[00:47:45.400 --> 00:47:48.500]   I just incorporate all of that in some sort of hierarchical representation.
[00:47:48.500 --> 00:47:52.300]   I say, you know, put this logo on this water bottle.
[00:47:52.400 --> 00:47:52.800]   Yeah.
[00:47:52.800 --> 00:47:57.500]   And, and, and then the logo has a word and the word has letters, all hierarchical.
[00:47:57.500 --> 00:47:58.700]   It's all that stuff is big.
[00:47:58.700 --> 00:48:01.200]   It's amazing that the brain instantly just does all that.
[00:48:01.200 --> 00:48:01.500]   Yeah.
[00:48:01.500 --> 00:48:08.100]   The idea that there's, there's water, it's liquid and the idea that you can drink it when you're thirsty.
[00:48:08.100 --> 00:48:09.600]   The idea that there's brands.
[00:48:09.600 --> 00:48:10.000]   Yeah.
[00:48:10.000 --> 00:48:16.700]   And then there's like all of that information is instantly like built into the whole thing once you proceed.
[00:48:16.700 --> 00:48:19.300]   So I wanted to get back to your point about hierarchical representation.
[00:48:19.300 --> 00:48:21.100]   The world itself is hierarchical.
[00:48:21.100 --> 00:48:22.100]   Right.
[00:48:22.200 --> 00:48:23.900]   And I can take this microphone in front of me.
[00:48:23.900 --> 00:48:25.800]   I know inside there's going to be some electronics.
[00:48:25.800 --> 00:48:29.300]   I know there's going to be some wires and I know there's going to be a little diaphragm that moves back and forth.
[00:48:29.300 --> 00:48:31.700]   I don't see that, but I know it.
[00:48:31.700 --> 00:48:34.700]   So everything in the world is hierarchical.
[00:48:34.700 --> 00:48:37.200]   You just go into a room, it's composed of other components.
[00:48:37.200 --> 00:48:42.500]   The kitchen has a refrigerator, you know, the refrigerator has a door, the door has a hinge, the hinge has screws and pins.
[00:48:42.500 --> 00:48:42.600]   Yeah.
[00:48:42.600 --> 00:48:51.100]   I mean, so anyway, the, the, the modeling system that exists in every cortical column learns the hierarchical structure of objects.
[00:48:51.600 --> 00:48:54.400]   So it's a very sophisticated modeling system in this grain of rice.
[00:48:54.400 --> 00:48:57.600]   It's hard to imagine, but this grain of rice can do really sophisticated things.
[00:48:57.600 --> 00:48:59.000]   It's got a hundred thousand neurons in it.
[00:48:59.000 --> 00:49:01.500]   It's very sophisticated.
[00:49:01.500 --> 00:49:09.500]   So that same mechanism that can model a water bottle or coffee cup can model conceptual objects as well.
[00:49:09.500 --> 00:49:20.100]   It's, it's, that's the beauty of this discovery that this guy, Vernon Mouncastle made many, many years ago, which is that there's, there's a single cortical algorithm underlying everything we're doing.
[00:49:21.200 --> 00:49:26.800]   So common sense concepts and higher level concepts are all represented in the same way.
[00:49:26.800 --> 00:49:28.300]   They're set in the same mechanisms.
[00:49:28.300 --> 00:49:28.600]   Yeah.
[00:49:28.600 --> 00:49:30.700]   It's a little bit like computers, right?
[00:49:30.700 --> 00:49:32.700]   All computers are universal Turing machines.
[00:49:32.700 --> 00:49:39.000]   Even the little teeny one that's, you know, my toaster and the big one that's running some cloud server someplace.
[00:49:39.000 --> 00:49:41.500]   They're all running on the same principle.
[00:49:41.500 --> 00:49:42.600]   They can apply different things.
[00:49:42.600 --> 00:49:45.300]   So the brain is all built on the same principle.
[00:49:45.300 --> 00:49:50.800]   It's all about learning these models, structured models using movement and reference frames.
[00:49:51.300 --> 00:49:55.200]   And it can be applied to something as simple as a water bottle and a coffee cup.
[00:49:55.200 --> 00:49:55.700]   And it can be right.
[00:49:55.700 --> 00:49:57.700]   Just thinking like, what's the future of humanity?
[00:49:57.700 --> 00:50:02.000]   And you know, why do you have a hedgehog on your, on your desk?
[00:50:02.000 --> 00:50:02.500]   I don't know.
[00:50:02.500 --> 00:50:03.600]   Nobody knows.
[00:50:03.600 --> 00:50:06.300]   Well, I think it's a hedgehog.
[00:50:06.300 --> 00:50:07.200]   That's right.
[00:50:07.200 --> 00:50:08.900]   It's a hedgehog in the fog.
[00:50:08.900 --> 00:50:10.600]   It's a Russian reference.
[00:50:10.600 --> 00:50:18.600]   Does it give you any inclination or hope about how difficult it is to engineer common sense reasoning?
[00:50:19.200 --> 00:50:21.900]   So how complicated is this whole process?
[00:50:21.900 --> 00:50:30.500]   So looking at the brain, is this a marvel of engineering or is it pretty dumb stuff stack on top of each other over and over again?
[00:50:30.500 --> 00:50:33.100]   It can be both.
[00:50:33.100 --> 00:50:34.400]   Can it be both?
[00:50:34.400 --> 00:50:34.900]   Right?
[00:50:34.900 --> 00:50:45.000]   I don't know if it can be both because if it's an incredible engineering job, that means it's very, so evolution did a lot of work.
[00:50:45.000 --> 00:50:46.700]   It, yeah.
[00:50:46.700 --> 00:50:48.400]   But then, but then it just copied that.
[00:50:49.400 --> 00:50:49.700]   Right?
[00:50:49.700 --> 00:50:57.800]   So as I said earlier, the figuring out how to model something like a space is really hard and evolution had to go through a lot of trick.
[00:50:57.800 --> 00:51:01.800]   And these, these, these cells I was talking about, these grid cells and place cells, they're really complicated.
[00:51:01.800 --> 00:51:02.900]   This is not simple stuff.
[00:51:02.900 --> 00:51:07.300]   This neural tissue works on these really unexpected, weird mechanisms.
[00:51:07.300 --> 00:51:09.700]   But it did it.
[00:51:09.700 --> 00:51:10.500]   It figured it out.
[00:51:10.500 --> 00:51:13.300]   But now you could just make lots of copies of it.
[00:51:13.300 --> 00:51:15.400]   But then finding, yeah.
[00:51:15.400 --> 00:51:20.600]   So it's a very interesting idea that's a lot of copies of a basic mini brain.
[00:51:20.600 --> 00:51:28.900]   But the question is how difficult it is to find that mini brain that you can copy and paste effectively.
[00:51:28.900 --> 00:51:33.100]   Well, today we know enough to build this.
[00:51:33.100 --> 00:51:36.700]   I'm sitting here with, you know, I know the steps we have to go.
[00:51:36.700 --> 00:51:40.600]   There's still some engineering problems to solve, but we know enough.
[00:51:40.600 --> 00:51:43.600]   And this is not like, oh, this is an interesting idea.
[00:51:43.600 --> 00:51:45.500]   We have to go think about it for another few decades.
[00:51:45.500 --> 00:51:47.600]   No, we actually understand it pretty well details.
[00:51:47.600 --> 00:51:51.200]   So not all the details, but most of them.
[00:51:51.200 --> 00:51:54.900]   So it's complicated, but it is an engineering problem.
[00:51:54.900 --> 00:51:57.400]   So in my company, we are working on that.
[00:51:57.400 --> 00:52:00.300]   We are basically laid out a roadmap, how we do this.
[00:52:00.300 --> 00:52:02.900]   It's not going to take decades.
[00:52:02.900 --> 00:52:04.200]   It's a matter of a few years.
[00:52:04.200 --> 00:52:08.200]   Optimistically, but I think that's possible.
[00:52:08.200 --> 00:52:11.200]   It's, you know, complex things.
[00:52:11.200 --> 00:52:12.500]   If you understand them, you can build them.
[00:52:12.800 --> 00:52:17.200]   So in which domain do you think it's best to build them?
[00:52:17.200 --> 00:52:25.000]   Are we talking about robotics, like entities that operate in the physical world that are able to interact with that world?
[00:52:25.000 --> 00:52:27.900]   Are we talking about entities that operate in the digital world?
[00:52:27.900 --> 00:52:37.900]   Are we talking about something more like, more specific, like is done in the machine learning community where you look at natural language or computer vision?
[00:52:37.900 --> 00:52:40.900]   Where do you think is easiest to...
[00:52:41.200 --> 00:52:44.100]   It's the first two more than the third one, I would say.
[00:52:44.100 --> 00:52:49.300]   Again, let's just use computers as an analogy.
[00:52:49.300 --> 00:52:58.500]   The pioneers in computing, people like John Ben-Norman, Alan Turing, they created this thing, you know, we now call the universal Turing machine, which is a computer, right?
[00:52:58.500 --> 00:53:01.000]   Did they know how it was going to be applied?
[00:53:01.000 --> 00:53:02.100]   Where it was going to be used?
[00:53:02.100 --> 00:53:04.000]   You know, could they envision any of the future?
[00:53:04.000 --> 00:53:04.300]   No.
[00:53:04.400 --> 00:53:12.700]   They just said, this is like a really interesting computational idea about algorithms and how you can implement them in a machine.
[00:53:12.700 --> 00:53:16.600]   And we're doing something similar to that today.
[00:53:16.600 --> 00:53:23.900]   Like we are building this sort of universal learning principle that can be applied to many, many different things.
[00:53:23.900 --> 00:53:27.700]   But the robotics piece of that, the interactive...
[00:53:27.700 --> 00:53:28.100]   Okay, all right.
[00:53:28.100 --> 00:53:29.500]   Let's be specific.
[00:53:29.500 --> 00:53:33.200]   You can think of this cortical column as what we call a sensory motor learning system.
[00:53:33.200 --> 00:53:36.100]   It has the idea that there's a sensor and then it's moving.
[00:53:36.100 --> 00:53:38.200]   That sensor can be physical.
[00:53:38.200 --> 00:53:41.000]   It could be like my finger and it's moving in the world.
[00:53:41.000 --> 00:53:43.200]   It could be like my eye and it's physically moving.
[00:53:43.200 --> 00:53:44.700]   It can also be virtual.
[00:53:44.700 --> 00:53:56.200]   So it could be, an example would be, I could have a system that lives in the internet that actually samples information on the internet and moves by following links.
[00:53:56.200 --> 00:53:58.200]   That's a sensory motor system.
[00:53:58.200 --> 00:54:03.800]   So something that echoes the process of a finger moving along a cortical column.
[00:54:03.800 --> 00:54:05.400]   In a very, very loose sense.
[00:54:05.400 --> 00:54:12.800]   It's like, again, learning is inherently about discovering the structure of the world and discover the structure of the world, you have to move through the world.
[00:54:12.800 --> 00:54:17.400]   Even if it's a virtual world, even if it's a conceptual world, you have to move through it.
[00:54:17.400 --> 00:54:20.900]   It doesn't exist in one, it has some structure to it.
[00:54:20.900 --> 00:54:26.000]   So here's a couple of predictions that getting what you're talking about.
[00:54:27.100 --> 00:54:29.800]   In humans, the same algorithm does robotics.
[00:54:29.800 --> 00:54:32.000]   It moves my arms, my eyes, my body.
[00:54:32.000 --> 00:54:32.300]   Right?
[00:54:32.300 --> 00:54:38.200]   And so in the future, to me, robotics and AI will merge.
[00:54:38.200 --> 00:54:48.400]   They're not going to be separate fields because the algorithms for really controlling robots are going to be the same algorithms we have in our brain, these sensory motor algorithms.
[00:54:48.400 --> 00:54:50.500]   Today we're not there, but I think that's going to happen.
[00:54:50.500 --> 00:54:56.300]   But not all AI systems will be robotics.
[00:54:57.300 --> 00:55:00.100]   You can have systems that have very different types of embodiments.
[00:55:00.100 --> 00:55:02.900]   Some will have physical movements, some will not have physical movements.
[00:55:02.900 --> 00:55:05.900]   It's a very generic learning system.
[00:55:05.900 --> 00:55:15.600]   Again, it's like computers, the Turing machine, it doesn't say how it's supposed to be implemented, doesn't tell you how big it is, doesn't tell you what you can apply it to, but it's a computational principle.
[00:55:15.600 --> 00:55:19.800]   Cortical column equivalent is a computational principle about learning.
[00:55:19.800 --> 00:55:23.000]   It's about how you learn and it can be applied to a gazillion things.
[00:55:23.200 --> 00:55:34.000]   This is what I think this is, I think this impact of AI is going to be as large, if not larger, than computing has been in the last century by far, because it's getting at a fundamental thing.
[00:55:34.000 --> 00:55:35.900]   It's not a vision system or a learning system.
[00:55:35.900 --> 00:55:38.500]   It's a, it's not a vision system or a hearing system.
[00:55:38.500 --> 00:55:39.800]   It is a learning system.
[00:55:39.800 --> 00:55:44.300]   It's a fundamental principle, how you learn to structure in the world, how you can gain knowledge and be intelligent.
[00:55:44.300 --> 00:55:47.200]   And that's what the thousand brain says was going on.
[00:55:47.200 --> 00:55:50.200]   And we have a particular implementation in our head, but it doesn't have to be like that at all.
[00:55:51.000 --> 00:55:53.600]   Do you think there's going to be some kind of impact?
[00:55:53.600 --> 00:55:56.100]   Okay, let me ask it another way.
[00:55:56.100 --> 00:56:03.600]   What do increasingly intelligent AI systems do with us humans in the following way?
[00:56:03.600 --> 00:56:07.300]   Like how hard is the human in the loop problem?
[00:56:07.300 --> 00:56:15.300]   How hard is it to interact the finger on the coffee cup equivalent of having a conversation with a human being?
[00:56:15.300 --> 00:56:19.400]   So how hard is it to fit into our little human world?
[00:56:20.300 --> 00:56:22.900]   I don't, I think it's a lot of engineering problems.
[00:56:22.900 --> 00:56:25.000]   I don't think it's a fundamental problem.
[00:56:25.000 --> 00:56:26.700]   I could ask you the same question.
[00:56:26.700 --> 00:56:29.000]   How hard is it for computers to fit into a human world?
[00:56:29.000 --> 00:56:29.800]   Right.
[00:56:29.800 --> 00:56:31.600]   That, I mean, that's essentially what I'm asking.
[00:56:31.600 --> 00:56:36.800]   Like how much are we elitist?
[00:56:36.800 --> 00:56:41.500]   Are we as humans, like we tried to keep out systems?
[00:56:41.500 --> 00:56:42.200]   I don't know.
[00:56:42.200 --> 00:56:43.300]   I, I, I'm not sure.
[00:56:43.300 --> 00:56:46.300]   I think I'm not sure that's the right question.
[00:56:46.300 --> 00:56:48.800]   Let's, let's look at computers as an analogy.
[00:56:49.200 --> 00:56:51.300]   Computers are a million times faster than us.
[00:56:51.300 --> 00:56:52.600]   They do things we can't understand.
[00:56:52.600 --> 00:56:55.500]   Most people have no idea what's going on when they use computers, right?
[00:56:55.500 --> 00:56:58.100]   How do we integrate them in our society?
[00:56:58.100 --> 00:57:01.700]   Well, they're, we don't think of them as their own entity.
[00:57:01.700 --> 00:57:03.200]   They're not living things.
[00:57:03.200 --> 00:57:05.700]   We don't afford them rights.
[00:57:05.700 --> 00:57:09.000]   We, we rely on them.
[00:57:09.000 --> 00:57:14.900]   Our survival as a 7 billion people or something like that is relying on computers now.
[00:57:16.100 --> 00:57:22.500]   Don't you think that's a fundamental problem that we see them as something we can't, we don't give rights to?
[00:57:22.500 --> 00:57:23.300]   Computers?
[00:57:23.300 --> 00:57:24.500]   So yeah, computers.
[00:57:24.500 --> 00:57:39.700]   So robots, computers, intelligent systems, it feels like for them to operate successfully, they would need to have a lot of the elements that we would start having to think about, like, should this entity have rights?
[00:57:39.700 --> 00:57:40.800]   I don't think so.
[00:57:40.800 --> 00:57:44.200]   I think it's tempting to think that way.
[00:57:44.500 --> 00:57:47.800]   First of all, I don't think anyone, hardly anyone thinks that's for computers today.
[00:57:47.800 --> 00:57:49.800]   No one says, oh, this thing needs a right.
[00:57:49.800 --> 00:57:51.000]   I shouldn't be able to turn it off.
[00:57:51.000 --> 00:57:56.400]   Or, you know, if I throw it in the trash can, you know, and hit it with a sledgehammer, I might form a criminal act.
[00:57:56.400 --> 00:57:57.500]   No, no one thinks that.
[00:57:57.500 --> 00:58:02.700]   And now we think about intelligent machines, which is where you're going.
[00:58:02.700 --> 00:58:07.900]   And, and all of a sudden, like, well, now we can't do that.
[00:58:07.900 --> 00:58:12.200]   I think the basic problem we have here is that people think intelligent machines will be like us.
[00:58:12.600 --> 00:58:15.900]   They're going to have the same emotions as we do, the same feelings as we do.
[00:58:15.900 --> 00:58:22.200]   What if I can build an intelligent machine that absolutely could care less about whether it was on or off or destroyed or not?
[00:58:22.200 --> 00:58:23.000]   It just doesn't care.
[00:58:23.000 --> 00:58:23.800]   It's just like a map.
[00:58:23.800 --> 00:58:25.200]   It's just a modeling system.
[00:58:25.200 --> 00:58:27.200]   It has no desires to live.
[00:58:27.200 --> 00:58:28.200]   Nothing.
[00:58:28.200 --> 00:58:36.900]   Is it possible to create a system that can model the world deeply and not care about whether it lives or dies?
[00:58:36.900 --> 00:58:37.400]   Absolutely.
[00:58:37.400 --> 00:58:38.500]   No question about it.
[00:58:38.500 --> 00:58:41.100]   To me, that's not 100% obvious.
[00:58:41.100 --> 00:58:42.100]   It's obvious to me.
[00:58:42.100 --> 00:58:44.100]   So we can debate it if you want.
[00:58:44.100 --> 00:58:44.600]   Yeah.
[00:58:44.600 --> 00:58:47.700]   Where does your, where does your desire to live come from?
[00:58:47.700 --> 00:58:51.400]   It's an old evolutionary design.
[00:58:51.400 --> 00:58:54.000]   I mean, we could argue, does it really matter if we live or not?
[00:58:54.000 --> 00:58:55.800]   Objectively, no.
[00:58:55.800 --> 00:58:56.900]   Right?
[00:58:56.900 --> 00:58:58.200]   We're all going to die eventually.
[00:58:58.200 --> 00:59:02.800]   But evolution makes us want to live.
[00:59:02.800 --> 00:59:05.100]   Evolution makes us want to fight to live.
[00:59:05.100 --> 00:59:12.000]   Evolutionists want to care and love one another and to care for our children and our, and our relatives and our family and, and so on.
[00:59:13.000 --> 00:59:14.200]   And those are all good things.
[00:59:14.200 --> 00:59:19.500]   But they come about not because we're smart, because we're animals that grew up.
[00:59:19.500 --> 00:59:22.700]   You know, the hummingbird in my backyard cares about its offspring.
[00:59:22.700 --> 00:59:27.800]   You know, the, every living thing in some sense cares about, you know, surviving.
[00:59:27.800 --> 00:59:32.100]   But when we talk about creating intelligent machines, we're not creating life.
[00:59:32.100 --> 00:59:34.200]   We're not creating evolving creatures.
[00:59:34.200 --> 00:59:36.000]   We're not creating living things.
[00:59:36.000 --> 00:59:39.900]   We're just creating a machine that can learn really sophisticated stuff.
[00:59:39.900 --> 00:59:48.200]   And that machine, it may even be able to talk to us, but it doesn't, it's not going to have a desire to live unless somehow we put it into that system.
[00:59:48.200 --> 00:59:50.100]   Well, there's learning, right?
[00:59:50.100 --> 00:59:53.200]   The thing is, but you don't learn to want to live.
[00:59:53.200 --> 00:59:54.400]   It's built into you.
[00:59:54.400 --> 00:59:57.900]   It's, well, so people like Ernest Becker argue.
[00:59:57.900 --> 00:59:59.100]   So, okay.
[00:59:59.100 --> 01:00:02.100]   There's the fact of finiteness of life.
[01:00:02.100 --> 01:00:06.300]   The way we think about it is something we learned.
[01:00:06.300 --> 01:00:07.900]   Perhaps.
[01:00:07.900 --> 01:00:08.800]   So, okay.
[01:00:08.900 --> 01:00:09.200]   Yeah.
[01:00:09.200 --> 01:00:15.300]   And some people decide they don't want to live and some people decide, you know, you can, but the desire to live is built in DNA.
[01:00:15.300 --> 01:00:15.700]   Right.
[01:00:15.700 --> 01:00:22.500]   But I think what I'm trying to get to is in order to accomplish goals, it's useful to have the urgency of mortality.
[01:00:22.500 --> 01:00:26.000]   So what the Stoics talked about is meditating in your mortality.
[01:00:26.000 --> 01:00:26.300]   Yeah.
[01:00:26.300 --> 01:00:38.800]   It might be a very useful thing to do to die and have the urgency of death and to realize that to conceive yourself as an entity that operates in the
[01:00:38.800 --> 01:00:50.200]   world that eventually will no longer be a part of this world and actually conceive of yourself as a conscious entity might be very useful for you to be a system that makes sense of the world.
[01:00:50.200 --> 01:00:51.800]   Otherwise, you might get lazy.
[01:00:51.800 --> 01:00:53.200]   Well, okay.
[01:00:53.200 --> 01:00:55.300]   We're going to build these machines, right?
[01:00:55.300 --> 01:00:57.700]   So we're talking about building AI.
[01:00:57.700 --> 01:00:58.100]   What?
[01:00:58.100 --> 01:01:07.100]   But we're building the equivalent of the cortical columns, the neocortex.
[01:01:07.200 --> 01:01:11.300]   And the question is, where do they arrive at?
[01:01:11.300 --> 01:01:14.600]   Because we're not hard coding everything in where.
[01:01:14.600 --> 01:01:21.300]   Well, in terms of if you build the neocortex equivalent, it will not have any of these desires or emotional states.
[01:01:21.300 --> 01:01:29.500]   Now, you can argue that that neocortex won't be useful unless I give it some agency, unless I give it some desire, unless I give it some motivation.
[01:01:29.500 --> 01:01:31.000]   Otherwise, you'll be lazy, do nothing.
[01:01:31.000 --> 01:01:31.700]   Right.
[01:01:31.700 --> 01:01:32.600]   You could argue that.
[01:01:32.600 --> 01:01:35.800]   But on its own, it's not going to do those things.
[01:01:36.100 --> 01:01:40.600]   It's just not going to sit there and say, "I understand the world, therefore I care to live."
[01:01:40.600 --> 01:01:41.800]   No, it's not going to do that.
[01:01:41.800 --> 01:01:43.100]   It's just going to say, "I understand the world."
[01:01:43.100 --> 01:01:44.400]   Why is that obvious to you?
[01:01:44.400 --> 01:01:46.600]   Why don't you think it's...
[01:01:46.600 --> 01:01:47.900]   Okay, let me ask it this way.
[01:01:47.900 --> 01:02:05.300]   Do you think it's possible it will at least assign to itself agency and perceive itself in this world as being a conscious entity as a useful way to operate in the world
[01:02:05.300 --> 01:02:07.000]   and to make sense of the world?
[01:02:07.000 --> 01:02:15.600]   I think intelligent machine can be conscious, but that does not, again, imply any of these desires and goals that you're worried about.
[01:02:15.600 --> 01:02:20.300]   We can talk about what it means for a machine to be conscious.
[01:02:20.300 --> 01:02:23.100]   And by the way, not worry about, but get excited about.
[01:02:23.100 --> 01:02:25.200]   It's not necessarily that we should worry about it.
[01:02:25.200 --> 01:02:29.300]   So I think there's a legitimate problem, or not problem, a question to ask.
[01:02:29.300 --> 01:02:31.900]   If you build this modeling system, what's it going to model?
[01:02:31.900 --> 01:02:32.700]   Yes.
[01:02:32.700 --> 01:02:33.000]   Right?
[01:02:33.000 --> 01:02:34.600]   What's its desire?
[01:02:34.900 --> 01:02:36.000]   What's its goal?
[01:02:36.000 --> 01:02:37.100]   What are we applying it to?
[01:02:37.100 --> 01:02:37.800]   Right?
[01:02:37.800 --> 01:02:39.300]   So that's an interesting question.
[01:02:39.300 --> 01:02:43.600]   One thing, and it depends on the application.
[01:02:43.600 --> 01:02:46.500]   It's not something that's inherent to the modeling system.
[01:02:46.500 --> 01:02:49.200]   It's something we apply to the modeling system in a particular way.
[01:02:49.200 --> 01:02:57.800]   So if I wanted to make a really smart car, it would have to know about driving in cars and what's important in driving in cars.
[01:02:57.800 --> 01:02:59.900]   It's not going to figure that out on its own.
[01:02:59.900 --> 01:03:03.800]   It's not going to sit there and say, "You know, I've understood the world and I've decided..."
[01:03:03.800 --> 01:03:04.900]   No, no, no, no.
[01:03:04.900 --> 01:03:05.700]   We're going to have to tell it.
[01:03:05.700 --> 01:03:06.600]   We're going to have to say like...
[01:03:06.600 --> 01:03:08.900]   So imagine I make this car really smart.
[01:03:08.900 --> 01:03:10.700]   It learns about your driving habits.
[01:03:10.700 --> 01:03:11.900]   It learns about the world.
[01:03:11.900 --> 01:03:16.400]   And it's just, you know, is it one day going to wake up and say, "You know what?
[01:03:16.400 --> 01:03:19.000]   I'm tired of driving and doing what you want.
[01:03:19.000 --> 01:03:21.500]   I think I have better ideas about how to spend my time."
[01:03:21.500 --> 01:03:22.600]   Well, okay.
[01:03:22.600 --> 01:03:23.900]   No, it's not going to do that.
[01:03:23.900 --> 01:03:32.600]   Well, part of me is playing a little bit of devil's advocate, but part of me is also trying to think through this because I've studied cars quite a bit.
[01:03:32.600 --> 01:03:35.000]   And I've studied pedestrians and cyclists quite a bit.
[01:03:35.000 --> 01:03:45.400]   And there's part of me that thinks that there needs to be more intelligence than we realize in order to drive successfully.
[01:03:45.400 --> 01:03:55.000]   That game theory of human interaction seems to require some deep understanding of human nature.
[01:03:55.600 --> 01:04:02.200]   That... Okay, when a pedestrian crosses the street, there's some sense...
[01:04:02.200 --> 01:04:06.900]   They look at a car, usually, and then they look away.
[01:04:06.900 --> 01:04:12.500]   There's some sense in which they say, "I believe that you're not going to murder me.
[01:04:12.500 --> 01:04:14.300]   You don't have the guts to murder me."
[01:04:14.300 --> 01:04:24.800]   This is the little dance of pedestrian car interaction is saying, "I'm going to look away and I'm going to put my life in your hands because I think you're human.
[01:04:24.800 --> 01:04:25.800]   You're not going to kill me."
[01:04:25.800 --> 01:04:34.200]   And then the car, in order to successfully operate in Manhattan streets, has to say, "No, no, no, I am going to kill you."
[01:04:34.200 --> 01:04:35.200]   Like a little bit.
[01:04:35.200 --> 01:04:38.700]   There's a little bit of this weird inkling of mutual murder.
[01:04:38.700 --> 01:04:39.300]   Yeah, yeah, yeah.
[01:04:39.300 --> 01:04:40.300]   And that's the dance.
[01:04:40.300 --> 01:04:42.300]   And we somehow successfully operate through that.
[01:04:42.300 --> 01:04:43.600]   So, do you think you were born with that?
[01:04:43.600 --> 01:04:45.300]   Or did you learn that social interaction?
[01:04:47.400 --> 01:04:57.400]   I think it might have a lot of the same elements that you're talking about, which is we're leveraging things we were born with and applying them in the context that...
[01:04:57.400 --> 01:05:02.400]   All right, I would have said that that kind of interaction is learned.
[01:05:02.400 --> 01:05:05.800]   Because, you know, people in different cultures have different interactions like that.
[01:05:05.800 --> 01:05:09.600]   If you cross the street in different cities and different parts of the world, they have different ways of interacting.
[01:05:09.600 --> 01:05:10.900]   I would say it's learned.
[01:05:10.900 --> 01:05:12.900]   And I would say an intelligence system can learn that, too.
[01:05:13.700 --> 01:05:15.300]   But that does not lead...
[01:05:15.300 --> 01:05:19.800]   And the intelligence system can understand humans.
[01:05:19.800 --> 01:05:25.800]   It could understand that, you know, just like I can study an animal and learn something about that animal.
[01:05:25.800 --> 01:05:28.800]   You know, I could study apes and learn something about their culture and so on.
[01:05:28.800 --> 01:05:30.700]   I don't have to be an ape to know that.
[01:05:30.700 --> 01:05:34.100]   I may not be completely, but I can understand something.
[01:05:34.100 --> 01:05:35.900]   So, intelligence machine can model that.
[01:05:35.900 --> 01:05:36.900]   That's just part of the world.
[01:05:36.900 --> 01:05:38.200]   It's just part of the interactions.
[01:05:38.600 --> 01:05:50.200]   The question we're trying to get at, will the intelligence machine have its own personal agency that's beyond, you know, what we assign to it or its own personal, you know, goals or will it evolve and create these things?
[01:05:50.200 --> 01:05:55.400]   My confidence comes from understanding the mechanisms I'm talking about creating.
[01:05:55.400 --> 01:05:57.800]   This is not hand-wavy stuff.
[01:05:57.800 --> 01:05:59.100]   It's down to the details.
[01:05:59.100 --> 01:06:00.300]   I'm going to build it.
[01:06:00.300 --> 01:06:01.900]   And I know what it's going to look like.
[01:06:01.900 --> 01:06:03.000]   And I know what's it going to behave.
[01:06:03.000 --> 01:06:05.700]   I know what the kind of things it could do and the kind of things it can't do.
[01:06:06.000 --> 01:06:11.600]   Just like when I build a computer, I know it's not going to on its own decide to put another register inside of it.
[01:06:11.600 --> 01:06:12.400]   It can't do that.
[01:06:12.400 --> 01:06:13.600]   No way.
[01:06:13.600 --> 01:06:16.200]   No matter what your software does, it can't add a register to the computer.
[01:06:16.200 --> 01:06:26.300]   So, in this way, when we build AI systems, we have to make choices about how we embed them.
[01:06:26.300 --> 01:06:27.700]   So, I talk about this in the book.
[01:06:27.700 --> 01:06:31.500]   I said, you know, intelligence system is not just the neocortex equivalent.
[01:06:31.800 --> 01:06:36.800]   You have to have that, but it has to have some kind of embodiment, physical or virtual.
[01:06:36.800 --> 01:06:38.300]   It has to have some sort of goals.
[01:06:38.300 --> 01:06:42.200]   It has to have some sort of ideas about dangers, about things it shouldn't do.
[01:06:42.200 --> 01:06:46.200]   Like, you know, like we build in safeguards into systems.
[01:06:46.200 --> 01:06:47.800]   We have them in our bodies.
[01:06:47.800 --> 01:06:49.600]   We have put them in the cars, right?
[01:06:49.600 --> 01:06:56.000]   My car follows my directions until the day it sees I'm about to hit something and it ignores my directions and puts the brakes on.
[01:06:56.000 --> 01:06:58.100]   So, we can build those things in.
[01:06:58.300 --> 01:07:01.900]   So, that's a very interesting problem, how to build those in.
[01:07:01.900 --> 01:07:12.800]   I think my differing opinion about the risks of AI for most people is that people assume that somehow those things will just appear automatically and it'll evolve.
[01:07:12.800 --> 01:07:17.300]   And intelligence itself begets that stuff or requires it.
[01:07:17.300 --> 01:07:18.300]   But it's not.
[01:07:18.300 --> 01:07:20.700]   Intelligence of the neocortex equivalent doesn't require this.
[01:07:20.700 --> 01:07:23.300]   The neocortex equivalent just says, I'm a learning system.
[01:07:23.300 --> 01:07:24.900]   Tell me what you want me to learn.
[01:07:24.900 --> 01:07:27.100]   And I'll tell you, ask me questions.
[01:07:27.100 --> 01:07:28.000]   I'll tell you the answers.
[01:07:28.500 --> 01:07:31.500]   But, and that, again, it's again like a map.
[01:07:31.500 --> 01:07:37.100]   It doesn't, a map has no intent about things, but you can use it to solve problems.
[01:07:37.100 --> 01:07:37.700]   Okay.
[01:07:37.700 --> 01:07:45.700]   So, the building, engineering the neocortex in itself is just creating an intelligent prediction system.
[01:07:45.700 --> 01:07:46.700]   Modeling system.
[01:07:46.700 --> 01:07:48.300]   Sorry, a modeling system.
[01:07:48.300 --> 01:07:48.600]   Yeah.
[01:07:48.600 --> 01:07:56.500]   You can use it to then make predictions and then, but you can also put it inside a thing that's actually acting in this world.
[01:07:56.500 --> 01:07:58.600]   You have to put it inside something.
[01:07:58.600 --> 01:08:01.300]   It's, again, think of the map analogy, right?
[01:08:01.300 --> 01:08:02.800]   A map on its own doesn't do anything.
[01:08:02.800 --> 01:08:03.200]   Right.
[01:08:03.200 --> 01:08:04.700]   It's just inert.
[01:08:04.700 --> 01:08:06.500]   It's just, you can learn, but it's just inert.
[01:08:06.500 --> 01:08:09.300]   So, we have to embed it somehow in something to do something.
[01:08:09.300 --> 01:08:11.600]   So, so what's your intuition here?
[01:08:11.600 --> 01:08:21.100]   You had, you had a conversation with Sam Harris recently that was sort of, you've had a bit of a disagreement and you're sticking on this point.
[01:08:21.300 --> 01:08:29.800]   You know, Elon Musk, Stuart Russell kind of have us worry existential threats of AI.
[01:08:29.800 --> 01:08:31.400]   What's your intuition?
[01:08:31.400 --> 01:08:41.000]   Why, if we engineer an increasingly intelligent neocortex type of system in the computer, why that shouldn't be a thing that we worry about?
[01:08:41.000 --> 01:08:45.000]   It was interesting, you used the word intuition and Sam Harris used the word intuition too.
[01:08:45.000 --> 01:08:50.000]   And, and when he used that intuition, that word, I immediately stopped and said, oh, that's the cause of the problem.
[01:08:50.600 --> 01:08:51.900]   He's using intuition.
[01:08:51.900 --> 01:08:53.400]   I'm not speaking about my intuition.
[01:08:53.400 --> 01:08:53.800]   Yes.
[01:08:53.800 --> 01:09:02.200]   I'm speaking about something I understand, something I'm going to build, something I am building, something I understand completely, or at least well enough to know what it's all, I'm guessing.
[01:09:02.200 --> 01:09:03.700]   I know what this thing's going to do.
[01:09:03.700 --> 01:09:13.800]   And I think most people who are worried, they have trouble separating out, they don't have, they don't have the knowledge or the understanding about like what is intelligence?
[01:09:13.800 --> 01:09:15.100]   How's it manifest in the brain?
[01:09:15.100 --> 01:09:17.300]   How's it separate from these other functions in the brain?
[01:09:17.300 --> 01:09:20.400]   And so, they imagine it's going to be human-like or animal-like.
[01:09:20.700 --> 01:09:26.400]   It's going to have, it's going to have the same sort of drives and emotions we have, but there's no reason for that.
[01:09:26.400 --> 01:09:29.100]   That's just because there's, there's an unknown.
[01:09:29.100 --> 01:09:32.200]   If you, if the unknown is like, oh my God, you know, I don't know what this is going to do.
[01:09:32.200 --> 01:09:32.900]   We have to be careful.
[01:09:32.900 --> 01:09:34.400]   It could be like us, but really smarter.
[01:09:34.400 --> 01:09:36.600]   I'm saying, no, it won't be like us.
[01:09:36.600 --> 01:09:38.800]   It'll be really smart, but it won't be like us at all.
[01:09:38.800 --> 01:09:45.800]   And, um, and, but I, I'm coming from that, not because I just guessing, I'm not intuitive using intuition.
[01:09:45.800 --> 01:09:47.800]   I'm basing it on like, okay, I understand this thing works.
[01:09:47.800 --> 01:09:48.500]   This is what it does.
[01:09:48.500 --> 01:09:49.400]   Let me explain it to you.
[01:09:50.100 --> 01:09:50.500]   Okay.
[01:09:50.500 --> 01:09:52.400]   But, uh, to push back.
[01:09:52.400 --> 01:10:03.400]   So I also disagree with the, the intuitions that Sam has, but, but I also disagree with what you just said, which, you know, what's a good, uh, analogy.
[01:10:03.400 --> 01:10:12.100]   So if you look at the Twitter algorithm in the early days, just recommender systems, you can understand how recommender systems work.
[01:10:13.000 --> 01:10:21.800]   What you can't understand in the early days is when you apply that recommender system at scale to thousands of millions of people, how that can change societies.
[01:10:21.800 --> 01:10:40.500]   So the question is, yes, you're just saying, this is how an engineer in your cortex works, but the, like when you have a very useful, uh, tick tock type of service that goes viral when you're in your cortex goes viral and then millions of people start using it.
[01:10:40.500 --> 01:10:41.600]   Can that destroy the world?
[01:10:41.700 --> 01:10:42.000]   No.
[01:10:42.000 --> 01:10:43.900]   Uh, well, first of all, this is back.
[01:10:43.900 --> 01:10:46.900]   One thing I want to say is that, um, AI is a dangerous technology.
[01:10:46.900 --> 01:10:49.700]   I don't, I'm not denying that all technology is dangerous.
[01:10:49.700 --> 01:10:51.400]   Well, and AI maybe particularly so.
[01:10:51.400 --> 01:10:51.600]   Yeah.
[01:10:51.600 --> 01:10:52.200]   Okay.
[01:10:52.200 --> 01:10:54.500]   So, um, am I worried about it?
[01:10:54.500 --> 01:10:55.600]   Yeah, I'm totally worried about it.
[01:10:55.600 --> 01:11:00.900]   The thing where the narrow component we're talking about now is the existential risk of AI.
[01:11:00.900 --> 01:11:01.700]   Right.
[01:11:01.700 --> 01:11:05.800]   So I want to make that distinction because I think AI can be applied poorly.
[01:11:05.800 --> 01:11:10.900]   It can be applied in ways that, you know, people are going to understand the consequences of it.
[01:11:11.300 --> 01:11:20.500]   Um, these are all potentially very bad things, but they're not the AI system creating this existential risk on its own.
[01:11:20.500 --> 01:11:23.100]   And that's the only place I disagree with other people.
[01:11:23.100 --> 01:11:23.600]   Right.
[01:11:23.600 --> 01:11:29.300]   So, so I, I think the existential risk thing is, um, humans are really damn good at surviving.
[01:11:29.300 --> 01:11:34.300]   So to kill off the human race, it'd be very, very difficult.
[01:11:34.300 --> 01:11:36.500]   Yes, but you can even, I'll go further.
[01:11:36.500 --> 01:11:41.200]   I don't think AI systems are ever going to try to, I don't think AI systems are ever going to like, say,
[01:11:41.600 --> 01:11:42.700]   I'm going to ignore you.
[01:11:42.700 --> 01:11:44.200]   I'm going to do what I think is best.
[01:11:44.200 --> 01:11:49.900]   Um, I don't think that's going to happen, at least not in the way I'm talking about it.
[01:11:49.900 --> 01:11:54.700]   So the Twitter recommendation outrun this interesting example.
[01:11:54.700 --> 01:11:58.000]   Let's, let's use computers as an analogy again.
[01:11:58.000 --> 01:11:58.700]   Right.
[01:11:58.700 --> 01:12:00.300]   I build a computer.
[01:12:00.300 --> 01:12:01.800]   It's a universal computing machine.
[01:12:01.800 --> 01:12:03.700]   I can't predict what people are going to use it for.
[01:12:03.700 --> 01:12:05.200]   They can build all kinds of things.
[01:12:05.200 --> 01:12:07.600]   They can, they can even create computer viruses.
[01:12:07.600 --> 01:12:09.200]   It's, you know, all kinds of stuff.
[01:12:10.000 --> 01:12:13.600]   So there's some unknown about its utility and about where it's going to go.
[01:12:13.600 --> 01:12:20.100]   But on the other hand, I pointed out that once I build a computer, it's not going to fundamentally change how it computes.
[01:12:20.100 --> 01:12:23.700]   It's like, I use the example of a register, which is a part, internal part of a computer.
[01:12:23.700 --> 01:12:27.900]   Um, you know, I say it can't just sit there, cause computers don't evolve.
[01:12:27.900 --> 01:12:29.700]   They don't replicate, they don't evolve.
[01:12:29.700 --> 01:12:34.800]   They don't, you know, the physical manifestation of the computer itself is not going to, there's certain things it can't do.
[01:12:34.800 --> 01:12:35.700]   Right.
[01:12:35.700 --> 01:12:38.700]   So we can break into things like things that are possible to happen.
[01:12:38.700 --> 01:12:41.600]   We can't predict and things that are just impossible to happen.
[01:12:41.600 --> 01:12:46.000]   Unless we go out of our way to make them happen, they're not going to happen unless somebody makes them happen.
[01:12:46.000 --> 01:12:46.500]   Yeah.
[01:12:46.500 --> 01:12:48.100]   So there's, there's a bunch of things to say.
[01:12:48.100 --> 01:12:51.500]   One is the physical aspect, which you're absolutely right.
[01:12:51.500 --> 01:12:57.600]   We have to build a thing for it to operate in the physical world and you can just stop building them.
[01:12:57.600 --> 01:13:05.300]   Uh, you know, the moment they're not doing the thing you want them to do, or just change the design or change the design.
[01:13:05.500 --> 01:13:11.900]   The question is, I mean, there's a, it's possible in the physical world, this is probably longer term is you automate the building.
[01:13:11.900 --> 01:13:14.900]   It makes, it makes a lot of sense to automate the building.
[01:13:14.900 --> 01:13:21.100]   There's a lot of factories that are doing more and more and more automation to go from raw resources to the final product.
[01:13:21.100 --> 01:13:32.900]   It's possible to imagine that it's obviously much more efficient to keep, to create a factory that's creating robots that do something, uh, you know, do something extremely useful for society.
[01:13:32.900 --> 01:13:34.600]   It could be a personal assistance.
[01:13:34.600 --> 01:13:41.500]   It could be, uh, it could be, it could be your toaster, but a toaster that's much has deeper knowledge of your culinary preferences.
[01:13:41.500 --> 01:13:41.700]   Yeah.
[01:13:41.700 --> 01:13:44.100]   And that could, uh,
[01:13:44.100 --> 01:13:45.800]   Well, I think now you've hit on the right thing.
[01:13:45.800 --> 01:13:48.800]   The real thing we need to be worried about Lex is self-replication.
[01:13:48.800 --> 01:13:49.700]   Right.
[01:13:49.700 --> 01:13:50.700]   That is the thing that we're
[01:13:50.700 --> 01:13:51.700]   In the physical world.
[01:13:51.700 --> 01:13:52.300]   Yeah.
[01:13:52.300 --> 01:13:53.600]   Or even the virtual world.
[01:13:53.600 --> 01:13:57.300]   Self-replication because self-replication is dangerous.
[01:13:57.300 --> 01:14:01.700]   It's probably more likely to be killed by a virus, you know, or a human hand veneered virus.
[01:14:02.200 --> 01:14:11.200]   Anybody can create a, you know, there's the technology is getting so almost anybody, but not anybody, but a lot of people could create a human engineered virus that could wipe out humanity.
[01:14:11.200 --> 01:14:13.000]   That is really dangerous.
[01:14:13.000 --> 01:14:14.200]   No intelligence required.
[01:14:14.200 --> 01:14:15.900]   Just self-replication.
[01:14:15.900 --> 01:14:20.100]   So, um, so we need to be careful about that.
[01:14:20.100 --> 01:14:25.700]   So when I think about, you know, AI, I'm not thinking about robots, building robots.
[01:14:25.700 --> 01:14:26.500]   Don't do that.
[01:14:26.500 --> 01:14:28.400]   Don't build a, you know, just
[01:14:28.600 --> 01:14:31.000]   Well, that's because you're interested in creating intelligence.
[01:14:31.000 --> 01:14:36.700]   It seems like self-replication is a good way to make a lot of money.
[01:14:36.700 --> 01:14:37.500]   Well, fine.
[01:14:37.500 --> 01:14:42.000]   But so is, you know, maybe editing viruses is a good way to, I don't know.
[01:14:42.000 --> 01:14:53.900]   The point is, if as a society, when we want to look at existential risks, the existential risks we face that we can control almost all of all of around self-replication.
[01:14:53.900 --> 01:14:54.300]   Yes.
[01:14:54.800 --> 01:15:01.700]   The question is, I don't see a good way to make a lot of money by engineering viruses and deploying them on the world.
[01:15:01.700 --> 01:15:04.500]   There could be, there could be applications that are useful.
[01:15:04.500 --> 01:15:05.600]   Let's separate out.
[01:15:05.600 --> 01:15:06.700]   Let's separate out.
[01:15:06.700 --> 01:15:07.900]   I mean, you don't need to.
[01:15:07.900 --> 01:15:11.600]   You only need some, you know, terrorists who wants to do it because it doesn't take a lot of money to make viruses.
[01:15:11.600 --> 01:15:16.000]   Um, let's just separate out what's risky and what's not risky.
[01:15:16.000 --> 01:15:19.600]   I'm arguing that the intelligence side of this equation is not risky.
[01:15:19.600 --> 01:15:20.100]   It's not risky.
[01:15:20.100 --> 01:15:20.900]   It's not risky at all.
[01:15:20.900 --> 01:15:23.600]   It's the self-replication side of the equation that's risky.
[01:15:24.000 --> 01:15:26.000]   And I'm not dismissing that.
[01:15:26.000 --> 01:15:27.200]   I'm scared as hell.
[01:15:27.200 --> 01:15:29.900]   It's like the paperclip maximizer thing.
[01:15:29.900 --> 01:15:30.300]   Yeah.
[01:15:30.300 --> 01:15:34.700]   That those are often like talked about in the same conversation.
[01:15:34.700 --> 01:15:37.400]   Um, I think you're right.
[01:15:37.400 --> 01:15:47.300]   Like creating ultra intelligent, super intelligent systems is not necessarily coupled with a self-replicating, arbitrarily self-replicating systems.
[01:15:47.300 --> 01:15:47.600]   Yeah.
[01:15:47.600 --> 01:15:49.800]   And you don't get evolution unless you're self-replicating.
[01:15:49.800 --> 01:15:50.500]   Yeah.
[01:15:50.500 --> 01:15:55.700]   And so I think that's just this argument that people have trouble separating those two out.
[01:15:55.700 --> 01:15:58.600]   They just think, oh, yeah, intelligence looks like us.
[01:15:58.600 --> 01:16:01.000]   And look how, look at the damage we've done to this planet.
[01:16:01.000 --> 01:16:03.600]   Like how we've, you know, destroyed all these other species.
[01:16:03.600 --> 01:16:03.800]   Yeah.
[01:16:03.800 --> 01:16:04.800]   Well, we replicate.
[01:16:04.800 --> 01:16:07.300]   We're 8 billion of us or 7 billion of us now.
[01:16:07.300 --> 01:16:08.700]   So, um.
[01:16:08.700 --> 01:16:18.700]   I think the idea is that the more intelligent we're able to build systems, the more tempting it becomes from a capitalist perspective of creating products.
[01:16:18.700 --> 01:16:22.400]   The more tempting it becomes to create self-reproducing systems.
[01:16:22.400 --> 01:16:22.900]   All right.
[01:16:22.900 --> 01:16:24.400]   So let's say that's true.
[01:16:24.400 --> 01:16:26.800]   So does that mean we don't build intelligent systems?
[01:16:26.800 --> 01:16:30.000]   No, that means we regulate, we, we understand the risks.
[01:16:30.000 --> 01:16:31.900]   Uh, we regulate them.
[01:16:31.900 --> 01:16:32.300]   Yeah.
[01:16:32.300 --> 01:16:39.800]   Uh, you know, look, there's a lot of things we could do as a society, which have some sort of financial benefit to someone, which could do a lot of harm.
[01:16:39.800 --> 01:16:42.500]   And we have to learn how to regulate those things.
[01:16:42.500 --> 01:16:44.200]   We have to learn how to deal with those things.
[01:16:44.200 --> 01:16:45.300]   I will argue this.
[01:16:45.300 --> 01:16:46.200]   I would say the opposite.
[01:16:46.200 --> 01:16:54.100]   Like, I would say having intelligent machines at our disposal will actually help us in the end more because it'll help us understand these risks better.
[01:16:54.100 --> 01:16:55.400]   It'll help us mitigate these risks better.
[01:16:55.400 --> 01:16:58.900]   There might be ways of saying, oh, well, how do we solve climate change problems?
[01:16:58.900 --> 01:17:00.700]   You know, how do we do this or how do we do that?
[01:17:00.700 --> 01:17:08.400]   Um, that just like computers are dangerous in the hands of the wrong people, but they've been so great for so many other things.
[01:17:08.400 --> 01:17:09.500]   We live with those dangers.
[01:17:09.500 --> 01:17:12.200]   And I think we have to do the same with intelligent machines.
[01:17:12.500 --> 01:17:22.900]   We just, but we have to be constantly vigilant about this idea of a bad actors doing bad things with them and B, um, don't ever, ever create a self-replicating system.
[01:17:22.900 --> 01:17:29.000]   Um, uh, and, and by the way, I don't even know if you could create a self-replicating system that uses a factory.
[01:17:29.000 --> 01:17:30.000]   That's really dangerous.
[01:17:30.000 --> 01:17:33.700]   You know, nature's way of self-replicating is so amazing.
[01:17:33.700 --> 01:17:36.500]   Um, you know, it doesn't require anything.
[01:17:36.500 --> 01:17:39.800]   It just, you know, the thing and resources and it goes right.
[01:17:39.800 --> 01:17:40.300]   Yeah.
[01:17:40.300 --> 01:17:50.400]   Um, if I said to you, you know what we have to build, uh, our goal is to build a factory that can make, that builds new factories and it has to end to end supply chain.
[01:17:50.400 --> 01:17:54.800]   It has to, it has to mine the resources, get the energy.
[01:17:54.800 --> 01:17:58.000]   I mean, that's really hard.
[01:17:58.000 --> 01:18:00.700]   It's, you know, no one's doing that in the next, you know, a hundred years.
[01:18:00.700 --> 01:18:07.300]   I've been extremely impressed by the efforts of Elon Musk and Tesla to try to do exactly that.
[01:18:07.300 --> 01:18:09.800]   Not, not from raw resource.
[01:18:10.000 --> 01:18:17.200]   Well, he actually, I think states the goal is to go from raw resource to the, uh, the final car in one factory.
[01:18:17.200 --> 01:18:17.500]   Yeah.
[01:18:17.500 --> 01:18:18.800]   And that's, that's the main goal.
[01:18:18.800 --> 01:18:22.600]   Of course it's not currently possible, but they're taking huge leaps.
[01:18:22.600 --> 01:18:23.900]   Well, he's not the only one to do that.
[01:18:23.900 --> 01:18:24.200]   It is.
[01:18:24.200 --> 01:18:27.200]   This has been a goal for many, uh, industries for a long, long time.
[01:18:27.200 --> 01:18:28.800]   Um, it's difficult to do.
[01:18:28.800 --> 01:18:36.200]   Well, a lot of people, what they do is instead they have like a million suppliers and then they like, there's everybody's man.
[01:18:36.200 --> 01:18:40.300]   They all co locate them and they, and they tie the systems together.
[01:18:40.300 --> 01:18:42.300]   It's a fundamentally distributed system.
[01:18:42.300 --> 01:18:48.300]   I think that's, that also is not getting at the issue I was just talking about, um, which is self replication.
[01:18:48.300 --> 01:18:56.500]   It's, um, I mean, self replication means there's no entity involved other than the entity that's replicating.
[01:18:56.500 --> 01:18:58.100]   Um, right.
[01:18:58.100 --> 01:19:02.200]   And so if there are humans in this, in the loop, that's not really self replicating, right?
[01:19:02.200 --> 01:19:12.500]   It's unless somehow we're duped into doing it, but it's also, I don't necessarily agree with you.
[01:19:12.500 --> 01:19:15.800]   Cause you, you've kind of mentioned that AI will not say no to us.
[01:19:15.800 --> 01:19:17.800]   I just think they will.
[01:19:17.800 --> 01:19:18.500]   Uh, yeah.
[01:19:18.500 --> 01:19:18.800]   Yeah.
[01:19:18.800 --> 01:19:23.300]   So like, um, I think it's a useful feature to build in.
[01:19:23.300 --> 01:19:29.200]   I'm just trying to like, uh, put myself in the mind of engineers to sometimes say no.
[01:19:29.200 --> 01:19:31.300]   You know, if you, yeah.
[01:19:31.300 --> 01:19:33.800]   Well, I mean, I gave the example earlier, right?
[01:19:33.800 --> 01:19:35.800]   I gave the example of my car, right?
[01:19:35.800 --> 01:19:43.700]   My car turns the wheel and, and applies the accelerator and the brake, as I say, until it decides there's something dangerous.
[01:19:43.700 --> 01:19:44.100]   Yes.
[01:19:44.100 --> 01:19:45.200]   And then it doesn't do that.
[01:19:45.200 --> 01:19:45.700]   Yeah.
[01:19:45.700 --> 01:19:49.900]   Now that was something it didn't decide to do.
[01:19:49.900 --> 01:19:52.000]   It's something we programmed into the car.
[01:19:52.000 --> 01:19:53.700]   Uh, and so good.
[01:19:53.700 --> 01:19:54.600]   It was a good idea.
[01:19:54.600 --> 01:19:55.000]   Right?
[01:19:56.100 --> 01:20:02.100]   The question again, isn't like, well, if we create an intelligent system, will it ever ignore our commands?
[01:20:02.100 --> 01:20:02.700]   Of course it will.
[01:20:02.700 --> 01:20:10.400]   And sometimes is it going to do it because it came up, came up with its own goals that serve its purposes and it doesn't care about our purposes?
[01:20:10.400 --> 01:20:11.800]   No, I don't think that's going to happen.
[01:20:11.800 --> 01:20:12.900]   Okay.
[01:20:12.900 --> 01:20:18.100]   So let me ask you about these, uh, super intelligent cortical systems that we engineer and us humans.
[01:20:20.100 --> 01:20:27.100]   Do you think, uh, with these entities operating out there in the world, what, what is the future most promising future look like?
[01:20:27.100 --> 01:20:32.800]   Is it us merging with them or is it us?
[01:20:32.800 --> 01:20:37.800]   Like, how do we keep us humans around when you have increasingly intelligent beings?
[01:20:37.800 --> 01:20:42.100]   Is it, uh, one of the dreams is to upload our minds in the digital space.
[01:20:42.100 --> 01:20:48.800]   So can we just give our minds to these systems so they can operate on them?
[01:20:48.900 --> 01:20:51.800]   Is there some kind of more interesting merger or is there more, more?
[01:20:51.800 --> 01:20:55.700]   So in the third part of my book, I talked about all these scenarios and let me just walk through them.
[01:20:55.700 --> 01:20:56.000]   Sure.
[01:20:56.000 --> 01:20:59.800]   Um, the uploading the mind one.
[01:20:59.800 --> 01:21:00.300]   Yes.
[01:21:00.300 --> 01:21:03.300]   Extremely, really difficult to do.
[01:21:03.300 --> 01:21:06.700]   Like, like we have no idea how to do this even remotely right now.
[01:21:06.700 --> 01:21:12.700]   Um, so it would be a very long way away, but I make the argument you wouldn't like the result.
[01:21:12.700 --> 01:21:16.200]   Um, and you wouldn't be pleased with the result.
[01:21:16.200 --> 01:21:17.900]   It's really not what you think it's going to be.
[01:21:18.300 --> 01:21:21.400]   Um, imagine I could upload your brain into a, into a computer right now.
[01:21:21.400 --> 01:21:23.400]   And now the computer's sitting there going, Hey, I'm over here.
[01:21:23.400 --> 01:21:23.800]   Great.
[01:21:23.800 --> 01:21:25.500]   Get rid of that old bio person.
[01:21:25.500 --> 01:21:26.100]   I don't need them.
[01:21:26.100 --> 01:21:27.100]   You're still sitting here.
[01:21:27.100 --> 01:21:27.800]   Yeah.
[01:21:27.800 --> 01:21:28.500]   What are you going to do?
[01:21:28.500 --> 01:21:30.200]   You just know, no, that's not me.
[01:21:30.200 --> 01:21:30.600]   I'm here.
[01:21:30.600 --> 01:21:30.900]   Right.
[01:21:30.900 --> 01:21:31.100]   Yeah.
[01:21:31.100 --> 01:21:40.900]   Are you going to feel satisfied that then you, but people imagine, look, I'm on my deathbed and I'm about to, you know, expire and I push the button and I'm uploaded, but think about it a little differently.
[01:21:41.400 --> 01:21:50.700]   And, and so I don't think it's going to be a thing because people, by the time we're able to do this, if ever, because you have to replicate the entire body, not just the brain.
[01:21:50.700 --> 01:21:52.900]   It's, it's really, it's, I walked through the issues.
[01:21:52.900 --> 01:21:54.400]   It's really substantial.
[01:21:54.400 --> 01:21:55.200]   Um,
[01:21:55.200 --> 01:21:57.800]   do you have a sense of what makes us, us?
[01:21:57.800 --> 01:22:04.500]   Is there, if, is there a shortcut to it can only save a certain part that makes us truly ours?
[01:22:04.500 --> 01:22:06.800]   No, but I think that machine would feel like it's you too.
[01:22:06.800 --> 01:22:07.600]   Right.
[01:22:07.600 --> 01:22:08.100]   Right.
[01:22:08.100 --> 01:22:11.000]   If two people just like I have a child, I have a, I have a child, right?
[01:22:11.000 --> 01:22:11.600]   I have two daughters.
[01:22:11.600 --> 01:22:12.000]   Mm-hmm.
[01:22:12.000 --> 01:22:13.400]   They're independent people.
[01:22:13.400 --> 01:22:14.100]   I created them.
[01:22:14.100 --> 01:22:14.900]   Well, partly.
[01:22:14.900 --> 01:22:15.300]   Yeah.
[01:22:15.300 --> 01:22:23.100]   And, um, uh, I don't just because they're somewhat like me, I don't feel on them and they don't feel like on me.
[01:22:23.100 --> 01:22:24.800]   So if you split apart, you have two people.
[01:22:24.800 --> 01:22:28.300]   So we can come back to what, what makes, what consciousness do you want?
[01:22:28.300 --> 01:22:31.300]   We can talk about that, but we don't have a remote consciousness.
[01:22:31.300 --> 01:22:33.100]   I'm not sitting there going, oh, I'm conscious of that.
[01:22:33.100 --> 01:22:34.900]   You know, I'm in that system over there.
[01:22:34.900 --> 01:22:37.700]   So let's say we, let's stay on our topic.
[01:22:37.700 --> 01:22:37.900]   Okay.
[01:22:37.900 --> 01:22:39.700]   So one was uploading a brain.
[01:22:39.700 --> 01:22:40.000]   Yeah.
[01:22:40.700 --> 01:22:45.300]   Ain't gonna happen in a hundred years, maybe a thousand, but I don't think people are going to want to do it.
[01:22:45.300 --> 01:22:50.400]   The merging your mind with, uh, you know, the neural link thing, right?
[01:22:50.400 --> 01:22:53.500]   Like again, really, really difficult.
[01:22:53.500 --> 01:22:56.900]   It's, it's one thing to make progress, to control a prosthetic arm.
[01:22:56.900 --> 01:23:02.300]   It's another to have like a billion or several billion, you know, things and understanding what those signals mean.
[01:23:02.300 --> 01:23:07.100]   Like it's the one thing that like, okay, I can learn to think some patterns to make something happen.
[01:23:07.500 --> 01:23:15.200]   It's quite another thing to have a system, a computer, which actually knows exactly what cells it's talking to and how it's talking to them and interacting in a way like that.
[01:23:15.200 --> 01:23:16.700]   Very, very difficult.
[01:23:16.700 --> 01:23:18.500]   We're not getting anywhere closer to that.
[01:23:18.500 --> 01:23:19.800]   Um, interesting.
[01:23:19.800 --> 01:23:22.300]   Can I, can I, uh, can I ask a question here?
[01:23:22.300 --> 01:23:36.900]   What, so for me, what makes that merger very difficult practically in the next 10, 20, 50 years is like literally the biology side of it, which is like, it's just hard to do that kind of surgery in a safe way.
[01:23:36.900 --> 01:23:37.400]   Yeah.
[01:23:37.400 --> 01:23:44.900]   But your intuition is even the machine learning part of it, where the machine has to learn what the heck it's talking to.
[01:23:44.900 --> 01:23:45.700]   That's even hard.
[01:23:45.700 --> 01:23:46.600]   I think it's even harder.
[01:23:46.600 --> 01:23:51.100]   And it's not, it's, it's easy to do when you're talking about hundreds of signals.
[01:23:51.100 --> 01:23:55.400]   It's, it's a totally different thing to say, talking about billions of signals.
[01:23:55.400 --> 01:23:58.900]   So you don't think it's the raw, it's a machine learning problem.
[01:23:58.900 --> 01:23:59.900]   You don't think it could be learned?
[01:23:59.900 --> 01:24:00.100]   No.
[01:24:00.100 --> 01:24:03.400]   Well, I'm just saying, no, I think you'd have to have detailed knowledge.
[01:24:03.400 --> 01:24:06.600]   You'd have to know exactly what the types of neurons you're connecting to.
[01:24:07.300 --> 01:24:10.100]   I mean, in the brain, there's these, there are neurons that do all different types of things.
[01:24:10.100 --> 01:24:10.900]   It's not like a neural network.
[01:24:10.900 --> 01:24:13.300]   It's a very complex organism system up here.
[01:24:13.300 --> 01:24:15.300]   We talked about the grid cells and the place cells.
[01:24:15.300 --> 01:24:22.800]   You know, you have to know what kind of cells you're talking to and what they're doing and how their timing works and all, all this stuff, which you can't, today there's no way of doing that.
[01:24:22.800 --> 01:24:23.000]   Right.
[01:24:23.000 --> 01:24:31.600]   But I think it's, I think it's a, I think the problem, you're right that the biological aspect of it, like who wants to have a surgery and have this stuff, you know, inserted in your brain, that's a problem.
[01:24:31.600 --> 01:24:33.500]   But let's assume we solve that problem.
[01:24:34.100 --> 01:24:37.000]   I think the, the information coding aspect is much worse.
[01:24:37.000 --> 01:24:38.400]   I think that's much worse.
[01:24:38.400 --> 01:24:39.800]   It's not like what they're doing today.
[01:24:39.800 --> 01:24:40.200]   Today.
[01:24:40.200 --> 01:24:51.800]   It's simple machine learning stuff because you're doing simple things, but if you want to merge your brain, like I'm thinking on the internet, I'm merge my brain with the machine and we're both doing, that's a totally different issue.
[01:24:51.800 --> 01:24:52.600]   That's interesting.
[01:24:52.600 --> 01:24:54.400]   I tend to think if the, okay.
[01:24:54.400 --> 01:24:54.600]   Yeah.
[01:24:54.600 --> 01:25:02.100]   If you have a super clean signal from a bunch of neurons at the start, you don't know what those neurons are.
[01:25:02.700 --> 01:25:07.600]   I think that's much easier than the getting of the clean signal.
[01:25:07.600 --> 01:25:12.500]   I think if you think about today's machine learning, that's what you would conclude.
[01:25:12.500 --> 01:25:12.800]   Right.
[01:25:12.800 --> 01:25:16.300]   I'm thinking about what's going on in the brain and I don't reach that conclusion.
[01:25:16.300 --> 01:25:17.100]   So we'll have to see.
[01:25:17.100 --> 01:25:17.700]   Sure.
[01:25:17.700 --> 01:25:22.100]   But I don't think even, even then, I think there's kind of a sad future.
[01:25:22.100 --> 01:25:26.900]   Like, you know, do I, do I have to like plug my brain into a computer?
[01:25:26.900 --> 01:25:28.300]   I'm still a biological organism.
[01:25:28.300 --> 01:25:29.600]   I assume I'm still going to die.
[01:25:29.600 --> 01:25:31.400]   So what, what have I achieved?
[01:25:31.400 --> 01:25:32.000]   Right.
[01:25:32.000 --> 01:25:33.500]   You know, what have I achieved today?
[01:25:33.500 --> 01:25:40.200]   Some sort of, oh, I disagree that we don't know what those are, but it seems like there could be a lot of different applications.
[01:25:40.200 --> 01:25:47.200]   It's like virtual reality is to expand your brain's capability to, to, to like to read Wikipedia.
[01:25:47.200 --> 01:25:48.300]   Yeah, but, but fine.
[01:25:48.300 --> 01:25:50.100]   But, but you're still a biological organism.
[01:25:50.100 --> 01:25:50.400]   Yes.
[01:25:50.400 --> 01:25:50.600]   Yes.
[01:25:50.600 --> 01:25:52.200]   You know, you're still, you're still mortal.
[01:25:52.200 --> 01:25:52.900]   You're still, all right.
[01:25:52.900 --> 01:25:54.700]   So, so what are you accomplishing?
[01:25:54.700 --> 01:25:57.500]   You're making your life in this short period of time better, right?
[01:25:57.500 --> 01:26:01.100]   Just like having the internet made our life better.
[01:26:01.100 --> 01:26:01.300]   Yeah.
[01:26:01.300 --> 01:26:01.500]   Yeah.
[01:26:01.500 --> 01:26:01.900]   Okay.
[01:26:02.100 --> 01:26:07.900]   So I think that's of, of, if I think about all the possible gains we can have here, that's a marginal one.
[01:26:07.900 --> 01:26:12.100]   It's an individual, Hey, I'm better, you know, I'm smarter.
[01:26:12.100 --> 01:26:15.400]   Um, but you'll find I'm not against it.
[01:26:15.400 --> 01:26:16.700]   I just don't think it's earth changing.
[01:26:16.700 --> 01:26:20.200]   I, but, so this is the true of the internet.
[01:26:20.200 --> 01:26:24.800]   When each of us individuals are smarter, we get a chance to then share our smartness.
[01:26:24.800 --> 01:26:31.800]   We get smarter and smarter together as like, as a collective, this is kind of like this ant colony of, why don't I just create an intelligent machine that doesn't have any of this?
[01:26:31.800 --> 01:26:33.000]   Biological nonsense.
[01:26:33.000 --> 01:26:34.800]   That has all the same.
[01:26:34.800 --> 01:26:38.000]   It's it's everything except don't burden it with my brain.
[01:26:38.000 --> 01:26:39.200]   Yeah.
[01:26:39.200 --> 01:26:39.600]   Right.
[01:26:39.600 --> 01:26:40.500]   It has a brain.
[01:26:40.500 --> 01:26:41.200]   It is smart.
[01:26:41.200 --> 01:26:43.300]   It's like my child, but it's much, much smarter than me.
[01:26:43.300 --> 01:26:55.800]   So I have a choice between doing some implant, doing some hybrid, weird, you know, biological thing that bleeding and all these problems and limited by my brain or creating a system, which is super smart that I can talk to.
[01:26:55.800 --> 01:26:56.400]   Yeah.
[01:26:56.400 --> 01:26:58.100]   Um, that helps me understand the world.
[01:26:58.100 --> 01:27:00.700]   They can read the internet, you know, read Wikipedia and talk to me.
[01:27:00.700 --> 01:27:08.500]   I guess my, uh, the open questions there are what does the manifestation of super intelligence look like?
[01:27:08.500 --> 01:27:13.300]   So like, what are we going to, you talked about, why do I want to merge with AI?
[01:27:13.300 --> 01:27:16.100]   Like what, what's the actual marginal benefit here?
[01:27:16.100 --> 01:27:24.200]   If I, if we have a super intelligent system, how will it make our life better?
[01:27:24.200 --> 01:27:28.100]   So let's, let's, that's a great question, but let's break it down to little pieces.
[01:27:28.100 --> 01:27:28.500]   All right.
[01:27:28.800 --> 01:27:32.100]   On the one hand, it can make our life better in lots of simple ways.
[01:27:32.100 --> 01:27:35.500]   You mentioned like a care robot or something that helps me do things.
[01:27:35.500 --> 01:27:35.900]   It cooks.
[01:27:35.900 --> 01:27:36.700]   I don't know what it does.
[01:27:36.700 --> 01:27:37.000]   Right.
[01:27:37.000 --> 01:27:39.500]   Little things like that, we can have better, smarter cars.
[01:27:39.500 --> 01:27:45.300]   We can have, you know, better agents that aid helping us in our work environment and things like that.
[01:27:45.300 --> 01:27:48.500]   To me, that's like the easy stuff, the simple stuff in the beginning.
[01:27:48.500 --> 01:27:56.400]   Um, um, and so in the same way that computers made our lives better in ways, many, many ways, I will have those kinds of things.
[01:27:57.700 --> 01:28:04.900]   To me, the really exciting thing about AI is the sort of its transcendent, transcendent quality in terms of humanity.
[01:28:04.900 --> 01:28:07.000]   We're still biological organisms.
[01:28:07.000 --> 01:28:08.300]   We're still stuck here on earth.
[01:28:08.300 --> 01:28:10.500]   It's going to be hard for us to live anywhere else.
[01:28:10.500 --> 01:28:14.200]   Uh, I don't think you and I are going to want to live on Mars anytime soon.
[01:28:14.200 --> 01:28:21.100]   Um, and, um, and we're flawed, you know, we may end up destroying ourselves.
[01:28:21.100 --> 01:28:23.100]   It's totally possible.
[01:28:23.100 --> 01:28:26.400]   Uh, we, if not completely, we could destroy our civilizations.
[01:28:26.400 --> 01:28:33.600]   You know, it's just face the fact that we have issues here, but we can create intelligent machines that can help us in various ways.
[01:28:33.600 --> 01:28:37.100]   For example, one example I gave, and that sounds a little sci-fi, but I believe this.
[01:28:37.100 --> 01:28:43.600]   If we really wanted to live on Mars, we'd have to have intelligent systems that go there and build the habitat for us.
[01:28:43.600 --> 01:28:44.400]   Not humans.
[01:28:44.400 --> 01:28:45.800]   Humans are never going to do this.
[01:28:45.800 --> 01:28:46.700]   It's just too hard.
[01:28:46.700 --> 01:28:54.000]   Um, but could we have a thousand or 10,000, you know, engineering workers up there doing this stuff, building things, terraforming Mars?
[01:28:54.000 --> 01:28:54.300]   Sure.
[01:28:54.300 --> 01:28:55.300]   Maybe we can move to Mars.
[01:28:55.700 --> 01:29:00.500]   But then if we want to, if we want to go around the universe, should I send my children around the universe?
[01:29:00.500 --> 01:29:08.400]   Or should I send some intelligent machine, which is like a child that represents me and understands our needs here on earth that could travel through space?
[01:29:08.400 --> 01:29:15.600]   Um, so it's sort of, in some sense, intelligence allows us to transcend our, the limitations of our biology.
[01:29:15.600 --> 01:29:20.000]   Uh, and don't think of it as a negative thing.
[01:29:20.000 --> 01:29:25.800]   It's in some sense, my children transcend my, my biology too, because they, they live beyond me.
[01:29:25.800 --> 01:29:26.300]   Yeah.
[01:29:26.300 --> 01:29:31.400]   Um, and we impart, they represent me and they also have their own knowledge and I can impart knowledge to them.
[01:29:31.400 --> 01:29:34.500]   So intelligent machines will be like that too, but not limited like us.
[01:29:34.500 --> 01:29:43.800]   But the question is, um, there's so many ways that transcendence can happen and the merger with AI and humans is one of those ways.
[01:29:43.800 --> 01:29:52.200]   So you said intelligent, basically beings or systems propagating throughout the universe, representing us humans.
[01:29:52.200 --> 01:29:58.600]   They represent us humans in the sense they represent our knowledge and our history, not us individually.
[01:29:58.600 --> 01:30:00.700]   Right.
[01:30:00.700 --> 01:30:01.000]   Right.
[01:30:01.000 --> 01:30:09.100]   But I mean, the question is, is it just the database with, uh, with the really damn good, uh, model?
[01:30:09.100 --> 01:30:11.400]   No, no, they're conscious, conscious, just like us.
[01:30:11.400 --> 01:30:12.200]   Okay.
[01:30:12.200 --> 01:30:13.400]   But just different.
[01:30:13.500 --> 01:30:14.100]   They're different.
[01:30:14.100 --> 01:30:15.900]   Uh, just like my children are different.
[01:30:15.900 --> 01:30:17.500]   They're like me, but they're different.
[01:30:17.500 --> 01:30:19.400]   Um, these are more different.
[01:30:19.400 --> 01:30:26.800]   I guess maybe I've already, I kind of, I take a very broad view of our life here on, on earth.
[01:30:26.800 --> 01:30:29.000]   I say, you know, why are we living here?
[01:30:29.000 --> 01:30:29.600]   Are we just living?
[01:30:29.600 --> 01:30:32.000]   Cause we live, is it, are we surviving?
[01:30:32.000 --> 01:30:33.100]   Cause we can survive.
[01:30:33.100 --> 01:30:36.300]   Are we fighting just cause we want to just keep going?
[01:30:36.300 --> 01:30:37.100]   What's the point of it?
[01:30:37.100 --> 01:30:37.400]   Yeah.
[01:30:37.400 --> 01:30:38.000]   Right.
[01:30:38.500 --> 01:30:56.600]   So to me, the point, if I asked myself, what's the point of life is what's transcends that ephemeral sort of biological experience is to me, this is my answer is the acquisition of knowledge to understand more about the universe, uh, and to explore.
[01:30:56.600 --> 01:30:58.800]   And that's partly to learn more.
[01:30:58.800 --> 01:30:59.200]   Right.
[01:31:00.000 --> 01:31:03.800]   Um, I don't view it as a terrible thing.
[01:31:03.800 --> 01:31:12.000]   If the ultimate outcome of humanity is we create systems that are intelligent, that are offspring, but they're not like us at all.
[01:31:12.000 --> 01:31:18.900]   And we stay, we stay here and live on earth as long as we can, which won't be forever, but as long as we can.
[01:31:18.900 --> 01:31:22.400]   And, but that would be a great thing to do.
[01:31:22.400 --> 01:31:25.300]   It's not a, it's not like a negative thing.
[01:31:25.500 --> 01:31:38.700]   Well, would you be okay then if, um, the human species vanishes, but our knowledge is preserved and keeps being expanded by intelligent systems?
[01:31:38.700 --> 01:31:42.300]   I want our knowledge to be preserved and expanded.
[01:31:42.300 --> 01:31:43.000]   Yeah.
[01:31:43.000 --> 01:31:45.100]   Am I okay with humans dying?
[01:31:45.100 --> 01:31:46.600]   No, I don't want that to happen.
[01:31:46.600 --> 01:31:53.400]   But if it, if it does happen, what if we were sitting here and this is the, we were the last two people on earth and we're saying Lex, we blew it.
[01:31:53.400 --> 01:31:54.400]   It's all over.
[01:31:54.400 --> 01:31:54.600]   Right.
[01:31:54.600 --> 01:31:54.900]   Yeah.
[01:31:55.500 --> 01:32:03.800]   Wouldn't I feel better if I knew that our knowledge was preserved and that we had agents that knew about that, that were trans, you know, that left earth.
[01:32:03.800 --> 01:32:05.100]   I wouldn't want that.
[01:32:05.100 --> 01:32:06.700]   It's better than not having that.
[01:32:06.700 --> 01:32:11.400]   You know, I make the analogy of like, you know, the dinosaurs, the poor dinosaurs, they live for, you know, tens of millions of years.
[01:32:11.400 --> 01:32:12.500]   They've raised their kids.
[01:32:12.500 --> 01:32:14.700]   They, you know, they, they fought to survive.
[01:32:14.700 --> 01:32:15.400]   They were hungry.
[01:32:15.400 --> 01:32:19.000]   They, they, you know, they did everything we do and then they're all gone.
[01:32:19.000 --> 01:32:19.600]   Yeah.
[01:32:19.600 --> 01:32:26.400]   Like, you know, and, and if we didn't discover their bones, nobody would ever know that they ever existed.
[01:32:26.400 --> 01:32:26.700]   Right.
[01:32:26.700 --> 01:32:28.000]   We want to be like that.
[01:32:28.000 --> 01:32:28.800]   I don't want to be like that.
[01:32:28.800 --> 01:32:30.800]   There's a sad aspect to it.
[01:32:30.800 --> 01:32:34.100]   And it's kind of, it's jarring to think about that.
[01:32:34.100 --> 01:32:40.100]   It's possible that a human like intelligence civilization has previously existed on earth.
[01:32:40.100 --> 01:32:49.000]   The reason I say this is like, it is jarring to think that we would not, if they went extinct, we wouldn't be able to find evidence of them.
[01:32:49.000 --> 01:32:50.500]   After a sufficient amount of time.
[01:32:50.500 --> 01:33:04.200]   After a sufficient amount of time, of course, there's like, look, basically humans, like if we destroy ourselves now, human civilization destroy ourselves now, after a sufficient amount of time, we would not be, we'd find evidence of the dinosaurs.
[01:33:04.200 --> 01:33:06.600]   We would not find evidence of us humans.
[01:33:06.600 --> 01:33:06.800]   Yeah.
[01:33:06.800 --> 01:33:08.400]   That's kind of an odd thing to think about.
[01:33:08.400 --> 01:33:18.000]   Although I'm not sure if we have enough knowledge about species going back for billions of years, that we could, we could, we might be able to eliminate that possibility.
[01:33:18.000 --> 01:33:19.500]   But it's an interesting question.
[01:33:19.500 --> 01:33:26.300]   Of course, this is a similar question to, you know, there were lots of intelligence species throughout our galaxy that have all disappeared.
[01:33:26.300 --> 01:33:27.100]   Yeah.
[01:33:27.100 --> 01:33:36.100]   That's super sad that they're exactly that there may have been much more intelligent alien civilizations in our galaxy.
[01:33:36.100 --> 01:33:37.200]   There are no longer there.
[01:33:37.200 --> 01:33:37.500]   Yeah.
[01:33:38.400 --> 01:33:52.600]   You actually talked about this, that humans might destroy ourselves and how we might preserve our knowledge and advertise that knowledge to other...
[01:33:52.600 --> 01:33:54.900]   Advertise is a funny word to use.
[01:33:54.900 --> 01:33:56.900]   From a PR perspective.
[01:33:56.900 --> 01:33:58.200]   There's no financial gain in this.
[01:33:58.200 --> 01:34:04.100]   You know, like make it like from a tourism perspective, make it interesting.
[01:34:04.100 --> 01:34:06.900]   Can you describe how you think about this problem?
[01:34:06.900 --> 01:34:09.600]   I broke it down into two parts, actually three parts.
[01:34:09.600 --> 01:34:18.600]   One is, you know, there's a lot of things we know that what if we ended, what if our civilization collapsed?
[01:34:18.600 --> 01:34:20.200]   I'm not talking tomorrow.
[01:34:20.200 --> 01:34:22.000]   We could be a thousand years from now, Lexi.
[01:34:22.000 --> 01:34:22.800]   We don't really know.
[01:34:22.800 --> 01:34:26.000]   But historically, it would be likely at some point.
[01:34:26.000 --> 01:34:27.300]   Time flies when you're having fun.
[01:34:27.300 --> 01:34:27.500]   Yeah.
[01:34:27.500 --> 01:34:29.200]   That's a good way to put it.
[01:34:29.200 --> 01:34:34.400]   You know, could we, and then intelligent life evolved again on this planet.
[01:34:34.400 --> 01:34:37.500]   Wouldn't they want to know a lot about us and what we knew?
[01:34:37.500 --> 01:34:39.400]   Wouldn't they be able to ask us questions?
[01:34:39.400 --> 01:34:42.700]   So one very simple thing I said, how would we archive what we know?
[01:34:42.700 --> 01:34:44.100]   That was a very simple idea.
[01:34:44.100 --> 01:34:46.100]   I said, you know what, that wouldn't be that hard.
[01:34:46.100 --> 01:34:51.600]   Put a few satellites, you know, going around the sun and we'd upload Wikipedia every day and that kind of thing.
[01:34:51.600 --> 01:34:57.000]   So, you know, if we end up killing ourselves, well, it's up there and the next intelligence piece will find it and learn something.
[01:34:57.000 --> 01:34:57.900]   They would like that.
[01:34:57.900 --> 01:34:58.800]   They would appreciate that.
[01:34:58.800 --> 01:35:02.100]   So that's one thing.
[01:35:02.100 --> 01:35:08.600]   The next thing I said, well, what if, you know, how outside of our solar system, we have the SETI program.
[01:35:08.600 --> 01:35:11.100]   We're looking for these intelligent signals from everybody.
[01:35:11.100 --> 01:35:24.500]   And if you do a little bit of math, which I did in the book, and you say, well, what if intelligent species only live for 10,000 years before, you know, technologically intelligent species, like ones are really able to do the stuff we're just starting to be able to do.
[01:35:24.500 --> 01:35:29.200]   Well, the chances are we wouldn't be able to see any of them because they would have all been disappeared by now.
[01:35:29.200 --> 01:35:32.100]   They've lived for 10,000 years and now they're gone.
[01:35:32.100 --> 01:35:41.200]   And so we're not going to find these signals being sent from these people because, but I said, what kind of signal could you create that would last a million years or a billion years?
[01:35:41.200 --> 01:35:44.900]   That someone would say, damn it, someone smart lived there.
[01:35:44.900 --> 01:35:45.800]   We know that.
[01:35:45.800 --> 01:35:48.500]   That would be a life changing event for us to figure that out.
[01:35:48.500 --> 01:35:50.600]   Well, what we're looking for today in the SETI program isn't that.
[01:35:50.600 --> 01:35:53.900]   We're looking for very coded signals in some sense.
[01:35:53.900 --> 01:35:57.600]   And so I asked myself, what would be a different type of signal one could create?
[01:35:57.600 --> 01:35:59.300]   I've always thought about this throughout my life.
[01:35:59.300 --> 01:36:10.400]   In the book, I gave one possible suggestion, which was we now detect planets going around other stars.
[01:36:10.400 --> 01:36:14.800]   And we do that by seeing this slight dimming of the light as the planets move in front of them.
[01:36:14.800 --> 01:36:20.100]   That's how we detect planets elsewhere in our galaxy.
[01:36:20.100 --> 01:36:30.600]   What if we created something like that, that just rotated around the sun and it blocked out a little bit of light in a particular pattern that someone said, hey, that's not a planet.
[01:36:30.600 --> 01:36:33.200]   That is a sign that someone was once there.
[01:36:33.200 --> 01:36:37.900]   You can say, what if it's beating up pi, you know, 3 point whatever.
[01:36:37.900 --> 01:36:38.600]   So I did.
[01:36:38.600 --> 01:36:40.100]   From a distance you can.
[01:36:40.100 --> 01:36:45.300]   From a distance, broadly broadcast, takes no continual activation on our part.
[01:36:45.300 --> 01:36:46.100]   This is the key, right?
[01:36:46.100 --> 01:36:49.100]   No one has to be sitting there running a computer and supplying it with power.
[01:36:49.100 --> 01:36:50.400]   It just goes on.
[01:36:50.400 --> 01:36:52.700]   So we go, it's continuous.
[01:36:52.700 --> 01:36:57.300]   And I argued that part of the SETI program should be looking for signals like that.
[01:36:57.300 --> 01:37:01.200]   And to look for signals like that, you ought to figure out how would we create a signal?
[01:37:01.200 --> 01:37:13.000]   Like, what would we create that would be like that, that would persist for millions of years, that would be broadcast broadly, that you could see from a distance, that was unequivocal, came from an intelligent species.
[01:37:13.000 --> 01:37:16.600]   And so I gave that one example because they don't know what I know of, actually.
[01:37:16.600 --> 01:37:27.800]   And then finally, right, if ultimately our solar system will die at some point in time, you know, how do we go beyond that?
[01:37:27.800 --> 01:37:36.200]   And I think it's possible, if at all possible, we'll have to create intelligent machines that travel throughout the solar system or throughout the galaxy.
[01:37:36.200 --> 01:37:37.800]   And I don't think that's going to be humans.
[01:37:37.800 --> 01:37:40.200]   I don't think it's going to be biological organisms.
[01:37:40.200 --> 01:37:41.900]   So these are just things to think about, you know.
[01:37:41.900 --> 01:37:44.600]   Like, what's the, you know, I don't want to be like the dinosaur.
[01:37:44.600 --> 01:37:46.200]   I don't want to just live and, okay, that was it.
[01:37:46.200 --> 01:37:47.700]   We're done, you know.
[01:37:47.700 --> 01:38:03.700]   Well, there is a kind of presumption that we're going to live forever, which I think it is a bit sad to imagine that the message we send as you talk about is that we were once here instead of we are here.
[01:38:03.700 --> 01:38:07.400]   Well, it could be we are still here.
[01:38:07.400 --> 01:38:11.400]   But it's more of a it's more of an insurance policy in case we're not here, you know.
[01:38:11.400 --> 01:38:16.200]   Well, I don't know, but there is something I think about.
[01:38:16.200 --> 01:38:26.700]   We as humans don't often think about this, but it's like whenever I record a video, I've done this a couple of times in my life.
[01:38:26.700 --> 01:38:30.400]   I've recorded a video for my future self, just for personal, just for fun.
[01:38:30.400 --> 01:38:40.100]   And it's always just fascinating to think about that, preserving yourself for future civilizations.
[01:38:40.100 --> 01:38:42.800]   For me, it was preserving myself for a future me.
[01:38:42.800 --> 01:38:47.400]   But that's a little that's a little fun example of archival.
[01:38:47.400 --> 01:38:54.500]   Well, these podcasts are preserving you and I in a way for future, hopefully well after we're gone.
[01:38:54.500 --> 01:38:59.900]   But you don't often we're sitting here talking about this.
[01:38:59.900 --> 01:39:10.200]   You are not thinking about the fact that you and I are going to die and there'll be like 10 years after somebody watching this and we're still alive.
[01:39:10.200 --> 01:39:12.500]   You know, in some sense I do.
[01:39:12.500 --> 01:39:22.800]   I'm here because I want to talk about ideas and these ideas transcend me and they transcend this time and on our planet.
[01:39:22.800 --> 01:39:28.100]   We're talking here about ideas that could be around a thousand years from now or a million years from now.
[01:39:28.100 --> 01:39:34.100]   When I wrote my book, I had an audience in mind and one of the clearest audiences was-
[01:39:34.100 --> 01:39:34.800]   Aliens.
[01:39:34.800 --> 01:39:38.000]   No, were people reading this a hundred years from now.
[01:39:38.000 --> 01:39:38.300]   Yes.
[01:39:38.300 --> 01:39:42.800]   I said to myself, how do I make this book relevant to someone reading this a hundred years from now?
[01:39:42.800 --> 01:39:45.700]   What would they want to know that we were thinking back then?
[01:39:45.700 --> 01:39:49.700]   What would make it like that was an interesting, it's still an interesting book.
[01:39:49.700 --> 01:39:53.900]   I'm not sure I could achieve that, but that was how I thought about it.
[01:39:53.900 --> 01:39:59.000]   Because these ideas, especially in the third part of the book, the ones we were just talking about, these crazy,
[01:39:59.000 --> 01:40:06.800]   what sounds like crazy ideas about storing our knowledge and merging our brains with computers and sending our machines out into space.
[01:40:06.800 --> 01:40:09.800]   It's not going to happen in my lifetime.
[01:40:09.800 --> 01:40:12.100]   And they may not even happen in the next hundred years.
[01:40:12.100 --> 01:40:13.700]   They may not happen for a thousand years.
[01:40:13.700 --> 01:40:15.400]   Who knows?
[01:40:15.400 --> 01:40:27.200]   But we have the unique opportunity right now, we, you, me, and other people like this, to sort of at least propose the agenda that might impact the future like that.
[01:40:27.200 --> 01:40:39.100]   It's a fascinating way to think, both writing or creating, try to create ideas, try to create things that hold up in time.
[01:40:39.100 --> 01:40:39.700]   Yeah.
[01:40:39.700 --> 01:40:42.700]   You know, understanding how the brain works, we're going to figure that out once.
[01:40:42.700 --> 01:40:43.200]   That's it.
[01:40:43.200 --> 01:40:44.700]   It's going to be figured out once.
[01:40:44.700 --> 01:40:46.700]   And after that, that's the answer.
[01:40:46.700 --> 01:40:49.700]   And people will study that thousands of years from now.
[01:40:49.700 --> 01:40:53.700]   We still venerate Newton and Einstein.
[01:40:53.700 --> 01:41:01.500]   And because ideas are exciting even well into the future.
[01:41:01.500 --> 01:41:07.600]   Well, the interesting thing is big ideas, even if they're wrong, are still useful.
[01:41:07.600 --> 01:41:09.600]   Like--
[01:41:09.600 --> 01:41:11.600]   Yeah, especially if they're not completely wrong.
[01:41:11.600 --> 01:41:12.700]   Like, right, right, right.
[01:41:12.700 --> 01:41:16.700]   Newton's laws are not wrong, they're just Einstein's are better.
[01:41:16.700 --> 01:41:18.700]   So, um--
[01:41:18.700 --> 01:41:22.200]   So, yeah, I mean, but we're talking with Newton and Einstein, we're talking about physics.
[01:41:22.200 --> 01:41:33.700]   I wonder if we'll ever achieve that kind of clarity about understanding like complex systems and this particular manifestation of complex systems, which is the human brain.
[01:41:33.700 --> 01:41:36.200]   Oh, I'm totally optimistic we can do that.
[01:41:36.200 --> 01:41:38.200]   I mean, we're making progress at it.
[01:41:38.200 --> 01:41:41.200]   I don't see any reason why we can't completely.
[01:41:41.200 --> 01:41:48.200]   I mean, completely understand in the sense, you know, we don't really completely understand what all the molecules in this water bottle are doing.
[01:41:48.200 --> 01:41:51.200]   But, you know, we have laws that sort of capture it pretty good.
[01:41:51.200 --> 01:41:54.200]   And so we'll have that kind of understanding.
[01:41:54.200 --> 01:41:58.200]   I mean, it's not like you're going to know what every neuron in your brain is doing.
[01:41:58.200 --> 01:42:02.200]   But enough to, first of all, to build it.
[01:42:02.200 --> 01:42:09.200]   And second of all, to do, you know, do what physics does, which is like have concrete experiments where we can validate.
[01:42:09.200 --> 01:42:12.200]   I mean, this is happening right now.
[01:42:12.200 --> 01:42:15.200]   It's not some future thing.
[01:42:15.200 --> 01:42:17.200]   You know, I'm very optimistic about it.
[01:42:17.200 --> 01:42:19.200]   I know about our work and what we're doing.
[01:42:19.200 --> 01:42:21.200]   We'll have to prove it to people.
[01:42:21.200 --> 01:42:27.200]   But I consider myself a rational person.
[01:42:27.200 --> 01:42:31.200]   And, you know, until fairly recently, I wouldn't have said that.
[01:42:31.200 --> 01:42:35.200]   But right now, where I'm sitting right now, I'm saying, you know, this is going to happen.
[01:42:35.200 --> 01:42:38.200]   There's no big obstacles to it.
[01:42:38.200 --> 01:42:41.200]   We finally have a framework for understanding what's going on in the cortex.
[01:42:41.200 --> 01:42:44.200]   And that's liberating.
[01:42:44.200 --> 01:42:47.200]   It's like, oh, it's happening.
[01:42:47.200 --> 01:42:49.200]   So I can't see why we wouldn't be able to understand it.
[01:42:49.200 --> 01:42:51.200]   I just can't.
[01:42:51.200 --> 01:42:52.200]   Okay.
[01:42:52.200 --> 01:42:56.200]   So, I mean, on that topic, let me ask you to play devil's advocate.
[01:42:56.200 --> 01:43:05.200]   Is it possible for you to imagine, look 100 years from now, and looking at your book,
[01:43:05.200 --> 01:43:08.200]   in which ways might your ideas be wrong?
[01:43:08.200 --> 01:43:13.200]   Oh, I worry about this all the time.
[01:43:13.200 --> 01:43:14.200]   It's still useful.
[01:43:14.200 --> 01:43:16.200]   Yeah.
[01:43:16.200 --> 01:43:18.200]   Yeah.
[01:43:18.200 --> 01:43:26.200]   I think there's, you know, I can best relate it to, like, things I'm worried about right now.
[01:43:26.200 --> 01:43:28.200]   So we talk about this voting idea, right?
[01:43:28.200 --> 01:43:29.200]   It's happening.
[01:43:29.200 --> 01:43:30.200]   There's no question it's happening.
[01:43:30.200 --> 01:43:39.200]   But it could be far more, there's enough things I don't know about it that it might be working
[01:43:39.200 --> 01:43:41.200]   in ways differently than I'm thinking about.
[01:43:41.200 --> 01:43:42.200]   What's voting?
[01:43:42.200 --> 01:43:43.200]   Who's voting?
[01:43:43.200 --> 01:43:44.200]   You know, where are representations?
[01:43:44.200 --> 01:43:47.200]   I talked about, like, you have a thousand models of a coffee cup like that.
[01:43:47.200 --> 01:43:53.200]   That could turn out to be wrong because it may be there are a thousand models that are
[01:43:53.200 --> 01:43:57.200]   sub-models, but not really a single model of the coffee cup.
[01:43:57.200 --> 01:44:02.200]   I mean, there's things, these are all sort of on the edges, things that I present as,
[01:44:02.200 --> 01:44:03.200]   like, oh, it's so simple and clean.
[01:44:03.200 --> 01:44:04.200]   Well, it's not that.
[01:44:04.200 --> 01:44:06.200]   It's always going to be more complex.
[01:44:06.200 --> 01:44:13.200]   And there's parts of the theory which I don't understand the complexity well.
[01:44:13.200 --> 01:44:19.200]   So I think the idea that the brain is a distributed modeling system is not controversial at all,
[01:44:19.200 --> 01:44:20.200]   right?
[01:44:20.200 --> 01:44:22.200]   That's well understood by many people.
[01:44:22.200 --> 01:44:27.200]   The question then is, are each cortical column an independent modeling system?
[01:44:27.200 --> 01:44:30.200]   I could be wrong about that.
[01:44:30.200 --> 01:44:32.200]   I don't think so, but I worry about it.
[01:44:32.200 --> 01:44:38.200]   My intuition, not even thinking why you could be wrong, is the same intuition I have about
[01:44:38.200 --> 01:44:46.200]   any sort of physicist, like, strength theory, that we as humans desire for a clean explanation.
[01:44:46.200 --> 01:44:54.200]   And a hundred years from now, intelligent systems might look back at us and laugh at
[01:44:54.200 --> 01:44:59.200]   how we try to get rid of the whole mess by having simple explanation, when the reality
[01:44:59.200 --> 01:45:01.200]   is it's way messier.
[01:45:01.200 --> 01:45:04.200]   And in fact, it's impossible to understand.
[01:45:04.200 --> 01:45:05.200]   You can only build it.
[01:45:05.200 --> 01:45:08.200]   It's like this idea of complex systems and cellular automata.
[01:45:08.200 --> 01:45:10.200]   You can only launch the thing.
[01:45:10.200 --> 01:45:11.200]   You cannot understand it.
[01:45:11.200 --> 01:45:17.200]   Yeah, I think that the history of science suggests that's not likely to occur.
[01:45:17.200 --> 01:45:22.200]   The history of science suggests that as a theorist, and we're theorists, you look for
[01:45:22.200 --> 01:45:24.200]   simple explanations, right?
[01:45:24.200 --> 01:45:29.200]   Fully knowing that whatever simple explanation you're going to come up with is not going
[01:45:29.200 --> 01:45:30.200]   to be completely correct.
[01:45:30.200 --> 01:45:31.200]   I mean, it can't be.
[01:45:31.200 --> 01:45:33.200]   I mean, it's just more complexity.
[01:45:33.200 --> 01:45:36.200]   But that's the role of theorists play.
[01:45:36.200 --> 01:45:42.200]   They give you a framework on which you now can talk about a problem and figure out, okay,
[01:45:42.200 --> 01:45:44.200]   now we can start digging in more details.
[01:45:44.200 --> 01:45:48.200]   The best frameworks stick around while the details change.
[01:45:48.200 --> 01:45:55.200]   Again, the classic example is Newton and Einstein, right?
[01:45:55.200 --> 01:45:57.200]   Newton's theories are still used.
[01:45:57.200 --> 01:45:58.200]   They're still valuable.
[01:45:58.200 --> 01:45:59.200]   They're still practical.
[01:45:59.200 --> 01:46:01.200]   They're not wrong.
[01:46:01.200 --> 01:46:02.200]   Just they've been refined.
[01:46:02.200 --> 01:46:03.200]   Yeah, but that's in physics.
[01:46:03.200 --> 01:46:05.200]   It's not obvious, by the way.
[01:46:05.200 --> 01:46:10.200]   It's not obvious for physics either that the universe should be such that it's amenable
[01:46:10.200 --> 01:46:11.200]   to these simple-
[01:46:11.200 --> 01:46:16.200]   I know, but it's so far it appears to be, as far as we can tell.
[01:46:16.200 --> 01:46:19.200]   Yeah, I mean, but as far as we could tell.
[01:46:19.200 --> 01:46:24.200]   But it's also an open question whether the brain is amenable to such clean theories.
[01:46:24.200 --> 01:46:27.200]   That's not the brain, but intelligence.
[01:46:27.200 --> 01:46:28.200]   Well, I don't know.
[01:46:28.200 --> 01:46:29.200]   I would take intelligence out of it.
[01:46:29.200 --> 01:46:39.200]   I would just say, well, okay, the evidence we have suggests that the human brain is,
[01:46:39.200 --> 01:46:44.200]   A, at the one time, extremely messy and complex, but there's some parts that are very regular
[01:46:44.200 --> 01:46:45.200]   and structured.
[01:46:45.200 --> 01:46:46.200]   That's why we started the neocortex.
[01:46:46.200 --> 01:46:51.200]   It's extremely regular in its structure, and unbelievably so.
[01:46:51.200 --> 01:46:56.200]   And then I mentioned earlier, the other thing is its universal abilities.
[01:46:56.200 --> 01:46:59.200]   It is so flexible to learn so many things.
[01:46:59.200 --> 01:47:01.200]   We haven't figured out what it can't learn yet.
[01:47:01.200 --> 01:47:03.200]   We don't know, but we haven't figured out yet.
[01:47:03.200 --> 01:47:06.200]   But it learns things that it never was evolved to learn.
[01:47:06.200 --> 01:47:08.200]   So those give us hope.
[01:47:08.200 --> 01:47:13.200]   That's why I went into this field, because I said, you know, this regular structure,
[01:47:13.200 --> 01:47:15.200]   it's doing this amazing number of things.
[01:47:15.200 --> 01:47:20.200]   There's got to be some underlying principles that are common, and other scientists have
[01:47:20.200 --> 01:47:23.200]   come up with the same conclusions.
[01:47:23.200 --> 01:47:24.200]   It's promising.
[01:47:24.200 --> 01:47:25.200]   It's promising.
[01:47:25.200 --> 01:47:32.200]   And whether the theories play out exactly this way or not, that is the role that theorists
[01:47:32.200 --> 01:47:33.200]   play.
[01:47:33.200 --> 01:47:38.200]   And so far, it's worked out well, even though maybe we don't understand all the laws of
[01:47:38.200 --> 01:47:39.200]   physics.
[01:47:39.200 --> 01:47:41.200]   But so far, it's been pretty damn useful.
[01:47:41.200 --> 01:47:45.200]   The ones we have, our theories are pretty useful.
[01:47:45.200 --> 01:47:51.200]   You mentioned that we should not necessarily be, at least to the degree that we are, worried
[01:47:51.200 --> 01:47:59.200]   about the existential risks of artificial intelligence relative to human risks from
[01:47:59.200 --> 01:48:02.200]   human nature being an existential risk.
[01:48:02.200 --> 01:48:06.200]   What aspect of human nature worries you the most in terms of the survival of the human
[01:48:06.200 --> 01:48:09.200]   species?
[01:48:09.200 --> 01:48:13.200]   I'm disappointed in humanity, in humans.
[01:48:13.200 --> 01:48:14.200]   I mean, all of us.
[01:48:14.200 --> 01:48:17.200]   I'm one, so I'm disappointed in myself, too.
[01:48:17.200 --> 01:48:20.200]   It's kind of a sad state.
[01:48:20.200 --> 01:48:22.200]   There's two things that disappoint me.
[01:48:22.200 --> 01:48:29.200]   One is how it's difficult for us to separate our rational component of ourselves from our
[01:48:29.200 --> 01:48:36.200]   evolutionary heritage, which is not always pretty.
[01:48:36.200 --> 01:48:41.200]   Rape is an evolutionary good strategy for reproduction.
[01:48:41.200 --> 01:48:44.200]   Murder can be at times, too.
[01:48:44.200 --> 01:48:49.200]   Making other people miserable at times is a good strategy for reproduction.
[01:48:49.200 --> 01:48:54.200]   And so now that we know that, and yet we have this sort of ... You and I can have this very
[01:48:54.200 --> 01:48:59.200]   rational discussion talking about intelligence and brains and life and so on.
[01:48:59.200 --> 01:49:01.200]   It seems like it's so hard.
[01:49:01.200 --> 01:49:06.200]   It's just a big transition to get humans, all humans, to make the transition from being
[01:49:06.200 --> 01:49:10.200]   like, "Let's pay no attention to all that ugly stuff over here.
[01:49:10.200 --> 01:49:12.200]   Let's just focus on the interesting."
[01:49:12.200 --> 01:49:16.200]   What's unique about humanity is our knowledge and our intellect.
[01:49:16.200 --> 01:49:19.200]   But the fact that we're striving is in itself amazing.
[01:49:19.200 --> 01:49:26.200]   The fact that we're able to overcome that part, and it seems like we are more and more
[01:49:26.200 --> 01:49:28.200]   becoming successful at overcoming that part.
[01:49:28.200 --> 01:49:30.200]   That is the optimistic view, and I agree with you.
[01:49:30.200 --> 01:49:32.200]   But I worry about it.
[01:49:32.200 --> 01:49:34.200]   I'm not saying I'm worrying about it.
[01:49:34.200 --> 01:49:35.200]   I think maybe that was your question.
[01:49:35.200 --> 01:49:37.200]   I still worry about it.
[01:49:37.200 --> 01:49:41.200]   It could end tomorrow because some terrorists could get nuclear bombs and blow us all up.
[01:49:41.200 --> 01:49:43.200]   Who knows, right?
[01:49:43.200 --> 01:49:47.200]   The other thing I'm disappointed is, and I understand it.
[01:49:47.200 --> 01:49:48.200]   I guess you can't really be disappointed.
[01:49:48.200 --> 01:49:52.200]   It's just a fact is that we're so prone to false beliefs.
[01:49:52.200 --> 01:49:55.200]   We have a model in our head.
[01:49:55.200 --> 01:50:01.200]   The things we can interact with directly, physical objects, people, that model's pretty good.
[01:50:01.200 --> 01:50:03.200]   We can test it all the time.
[01:50:03.200 --> 01:50:06.200]   I touch something, I look at it, I talk to you, see if my model's correct.
[01:50:06.200 --> 01:50:10.200]   But so much of what we know is stuff I can't directly interact with.
[01:50:10.200 --> 01:50:12.200]   I only know because someone told me about it.
[01:50:12.200 --> 01:50:19.200]   So we're inherently prone to having false beliefs because if I'm told something,
[01:50:19.200 --> 01:50:21.200]   how am I going to know it's right or wrong?
[01:50:21.200 --> 01:50:26.200]   Then we have the scientific process which says we are inherently flawed.
[01:50:26.200 --> 01:50:34.200]   So the only way we can get closer to the truth is by looking for contrary evidence.
[01:50:34.200 --> 01:50:41.200]   Yeah, like this conspiracy theory, this theory that scientists keep telling me about
[01:50:41.200 --> 01:50:43.200]   that the Earth is round.
[01:50:43.200 --> 01:50:47.200]   As far as I can tell, when I look out, it looks pretty flat.
[01:50:47.200 --> 01:50:48.200]   Yeah.
[01:50:48.200 --> 01:50:49.200]   So yeah, there is a tension.
[01:50:49.200 --> 01:50:57.200]   But it's also, I tend to believe that we haven't figured out most of this thing.
[01:50:57.200 --> 01:51:01.200]   Most of nature around us is a mystery.
[01:51:01.200 --> 01:51:04.200]   But does that worry you?
[01:51:04.200 --> 01:51:06.200]   I mean, it's like, oh, that's like a pleasure.
[01:51:06.200 --> 01:51:07.200]   More to figure out, right?
[01:51:07.200 --> 01:51:08.200]   Yeah, that's exciting.
[01:51:08.200 --> 01:51:13.200]   But I'm saying like there's going to be a lot of "wrong ideas."
[01:51:13.200 --> 01:51:18.200]   I mean, I've been thinking a lot about engineering systems like social networks
[01:51:18.200 --> 01:51:22.200]   and so on, and I've been worried about censorship and thinking through all that
[01:51:22.200 --> 01:51:25.200]   kind of stuff because there's a lot of wrong ideas.
[01:51:25.200 --> 01:51:27.200]   There's a lot of dangerous ideas.
[01:51:27.200 --> 01:51:34.200]   But then I also read history and see when you censor ideas that are wrong.
[01:51:34.200 --> 01:51:40.200]   Now, this could be small-scale censorship, like a young grad student who comes up,
[01:51:40.200 --> 01:51:43.200]   who raises their hand and says some crazy idea.
[01:51:43.200 --> 01:51:47.200]   A form of censorship could be, I shouldn't use the word censorship, but-
[01:51:47.200 --> 01:51:49.200]   I don't know what you mean.
[01:51:49.200 --> 01:51:53.200]   De-incentivize them from, no, no, no, no, this is the way it's been done.
[01:51:53.200 --> 01:51:55.200]   Yeah, you're a foolish kid, don't think.
[01:51:55.200 --> 01:51:56.200]   Yeah, you're foolish.
[01:51:56.200 --> 01:52:04.200]   So in some sense, those wrong ideas most of the time end up being wrong,
[01:52:04.200 --> 01:52:05.200]   but sometimes end up being-
[01:52:05.200 --> 01:52:06.200]   I agree with you.
[01:52:06.200 --> 01:52:09.200]   So I don't like the word censorship.
[01:52:09.200 --> 01:52:16.200]   At the very end of the book, I ended up with a sort of a plea or a recommended
[01:52:16.200 --> 01:52:17.200]   force of action.
[01:52:17.200 --> 01:52:23.200]   And the best way I could, I know how to deal with this issue that you bring up,
[01:52:23.200 --> 01:52:28.200]   is if everybody understood as part of your upbringing in life,
[01:52:28.200 --> 01:52:32.200]   something about how your brain works, that it builds a model of the world,
[01:52:32.200 --> 01:52:35.200]   how it works, how basic it builds that model of the world,
[01:52:35.200 --> 01:52:39.200]   and that the model is not the real world, it's just a model.
[01:52:39.200 --> 01:52:42.200]   And it's never going to reflect the entire world, and it can be wrong,
[01:52:42.200 --> 01:52:43.200]   and it's easy to be wrong.
[01:52:43.200 --> 01:52:48.200]   And here's all the ways you can get a wrong model in your head, right?
[01:52:48.200 --> 01:52:52.200]   It's not to prescribe what's right or wrong, just understand that process.
[01:52:52.200 --> 01:52:55.200]   If we all understood the processes, and I get together and you say,
[01:52:55.200 --> 01:52:57.200]   "I disagree with you, Jeff," and I say, "Lex, I disagree with you,"
[01:52:57.200 --> 01:53:02.200]   that at least we understand that we're both trying to model something.
[01:53:02.200 --> 01:53:05.200]   We both have different information which leads to our different models,
[01:53:05.200 --> 01:53:08.200]   and therefore I shouldn't hold it against you, and you shouldn't hold it against me.
[01:53:08.200 --> 01:53:12.200]   And we can at least agree that, well, what can we look for in its common ground
[01:53:12.200 --> 01:53:18.200]   to test our beliefs, as opposed to so much as we raise our kids on dogma,
[01:53:18.200 --> 01:53:24.200]   which is this is a fact, and this is a fact, and these people are bad.
[01:53:24.200 --> 01:53:31.200]   If everyone knew just to be skeptical of every belief, and why,
[01:53:31.200 --> 01:53:35.200]   and how their brains do that, I think we might have a better world.
[01:53:35.200 --> 01:53:40.200]   Do you think the human mind is able to comprehend reality?
[01:53:40.200 --> 01:53:45.200]   So you talk about this creating models that are better and better.
[01:53:45.200 --> 01:53:49.200]   How close do you think we get to reality?
[01:53:49.200 --> 01:53:54.200]   So the wildest ideas is like Donald Hoffman saying we're very far away from reality.
[01:53:54.200 --> 01:53:56.200]   Do you think we're getting close to reality?
[01:53:56.200 --> 01:54:00.200]   Well, I guess it depends on what you define reality.
[01:54:00.200 --> 01:54:04.200]   We have a model of the world that's very useful.
[01:54:04.200 --> 01:54:06.200]   For basic goals of survival.
[01:54:06.200 --> 01:54:11.200]   Well, for our survival and our pleasure, whatever, right?
[01:54:11.200 --> 01:54:13.200]   So that's useful.
[01:54:13.200 --> 01:54:14.200]   I mean, it's really useful.
[01:54:14.200 --> 01:54:15.200]   Oh, we can build planes.
[01:54:15.200 --> 01:54:16.200]   We can build computers.
[01:54:16.200 --> 01:54:18.200]   We can do these things, right?
[01:54:18.200 --> 01:54:23.200]   I don't think, I don't know the answer to that question.
[01:54:23.200 --> 01:54:26.200]   I think that's part of the question we're trying to figure out, right?
[01:54:26.200 --> 01:54:29.200]   Obviously, if you end up with a theory of everything,
[01:54:29.200 --> 01:54:32.200]   that really is a theory of everything, and all of a sudden,
[01:54:32.200 --> 01:54:35.200]   everything comes into play, and there's no room for something else,
[01:54:35.200 --> 01:54:37.200]   then you might feel like we have a good model of the world.
[01:54:37.200 --> 01:54:40.200]   Yeah, but if we have a theory of everything, and somehow, first of all,
[01:54:40.200 --> 01:54:43.200]   you'll never be able to really conclusively say it's a theory of everything,
[01:54:43.200 --> 01:54:47.200]   but say somehow we are very damn sure it's a theory of everything.
[01:54:47.200 --> 01:54:49.200]   We understand what happened at the Big Bang,
[01:54:49.200 --> 01:54:52.200]   and how just the entirety of the physical process.
[01:54:52.200 --> 01:54:59.200]   I'm still not sure that gives us an understanding of the next many layers
[01:54:59.200 --> 01:55:02.200]   of the hierarchy of abstractions that form.
[01:55:02.200 --> 01:55:05.200]   Well, also, what if string theory turns out to be true?
[01:55:05.200 --> 01:55:08.200]   And then you say, well, we have no reality, no modeling,
[01:55:08.200 --> 01:55:12.200]   what's going on in those other dimensions that are wrapped into it on each other?
[01:55:12.200 --> 01:55:15.200]   Or the multiverse.
[01:55:15.200 --> 01:55:19.200]   I honestly don't know how, for us, for human interaction,
[01:55:19.200 --> 01:55:22.200]   for ideas of intelligence, how it helps us to understand
[01:55:22.200 --> 01:55:28.200]   that we're made up of vibrating strings that are like 10 to the whatever times
[01:55:28.200 --> 01:55:31.200]   smaller than us.
[01:55:31.200 --> 01:55:34.200]   You could probably build better weapons and better rockets,
[01:55:34.200 --> 01:55:37.200]   but you're not going to be able to understand intelligence.
[01:55:37.200 --> 01:55:38.200]   Maybe better computers.
[01:55:38.200 --> 01:55:39.200]   No, you won't be able to.
[01:55:39.200 --> 01:55:41.200]   I think it's just more purely knowledge.
[01:55:41.200 --> 01:55:46.200]   It might lead to a better understanding of the beginning of the universe.
[01:55:46.200 --> 01:55:50.200]   It might lead to a better understanding of, I don't know.
[01:55:50.200 --> 01:55:54.200]   I guess I think the acquisition of knowledge has always been one
[01:55:54.200 --> 01:55:59.200]   where you pursue it for its own pleasure,
[01:55:59.200 --> 01:56:03.200]   and you don't always know what is going to make a difference.
[01:56:03.200 --> 01:56:04.200]   Yeah.
[01:56:04.200 --> 01:56:07.200]   You're pleasantly surprised by the weird things you find.
[01:56:07.200 --> 01:56:11.200]   Do you think for the neocortex in general,
[01:56:11.200 --> 01:56:16.200]   do you think there's a lot of innovation to be done on the machine side?
[01:56:16.200 --> 01:56:19.200]   You use the computer as a metaphor quite a bit.
[01:56:19.200 --> 01:56:23.200]   Is there different types of computer that would help us build intelligence?
[01:56:23.200 --> 01:56:25.200]   I mean, what are the physical manifestations of intelligent machines?
[01:56:25.200 --> 01:56:26.200]   Yeah.
[01:56:26.200 --> 01:56:30.200]   Oh, no, it's going to be totally crazy.
[01:56:30.200 --> 01:56:32.200]   We have no idea how this is going to look out yet.
[01:56:32.200 --> 01:56:34.200]   You can already see this.
[01:56:34.200 --> 01:56:38.200]   Today, of course, we model these things on traditional computers,
[01:56:38.200 --> 01:56:44.200]   and now GPUs are really popular with neural networks and so on.
[01:56:44.200 --> 01:56:49.200]   But there are companies coming up with fundamentally new physical substrates
[01:56:49.200 --> 01:56:51.200]   that are just really cool.
[01:56:51.200 --> 01:56:53.200]   I don't know if they're going to work or not,
[01:56:53.200 --> 01:56:57.200]   but I think there'll be decades of innovation here.
[01:56:57.200 --> 01:56:58.200]   Yeah.
[01:56:58.200 --> 01:56:59.200]   Totally.
[01:56:59.200 --> 01:57:03.200]   Do you think the final thing will be messy like our biology is messy?
[01:57:03.200 --> 01:57:08.200]   Or do you think it's the old bird versus airplane question?
[01:57:08.200 --> 01:57:16.200]   Or do you think we could just build airplanes that fly way better than birds
[01:57:16.200 --> 01:57:22.200]   in the same way we could build electrical neocortex?
[01:57:22.200 --> 01:57:23.200]   Yeah.
[01:57:23.200 --> 01:57:25.200]   Can I riff on the bird thing a bit?
[01:57:25.200 --> 01:57:27.200]   Because I think it's interesting.
[01:57:27.200 --> 01:57:29.200]   People really misunderstand this.
[01:57:29.200 --> 01:57:35.200]   The Wright brothers, the problem they were trying to solve was controlled flight,
[01:57:35.200 --> 01:57:38.200]   how to turn an airplane, not how to propel an airplane.
[01:57:38.200 --> 01:57:39.200]   They weren't worried about that.
[01:57:39.200 --> 01:57:40.200]   Interesting. Yeah.
[01:57:40.200 --> 01:57:44.200]   At that time, there was already wing shapes, which they had from studying birds.
[01:57:44.200 --> 01:57:46.200]   There was already gliders that carried people.
[01:57:46.200 --> 01:57:49.200]   The problem was if you put a rudder on the back of a glider and you turn it,
[01:57:49.200 --> 01:57:51.200]   the plane falls out of the sky.
[01:57:51.200 --> 01:57:54.200]   So the problem was how do you control flight?
[01:57:54.200 --> 01:57:58.200]   And they studied birds, and they actually had birds in captivity.
[01:57:58.200 --> 01:57:59.200]   They watched birds in wind tunnels.
[01:57:59.200 --> 01:58:03.200]   They observed them in the wild, and they discovered the secret was the birds
[01:58:03.200 --> 01:58:05.200]   twist their wings when they turn.
[01:58:05.200 --> 01:58:07.200]   And so that's what they did on the Wright brothers' flyer.
[01:58:07.200 --> 01:58:09.200]   They had these sticks, and you would twist the wing,
[01:58:09.200 --> 01:58:12.200]   and that was their innovation, not their propeller.
[01:58:12.200 --> 01:58:15.200]   And today, airplanes still twist their wings.
[01:58:15.200 --> 01:58:16.200]   We don't twist the entire wing.
[01:58:16.200 --> 01:58:20.200]   We just twist the tail end of it, the flaps, which is the same thing.
[01:58:20.200 --> 01:58:24.200]   So today's airplanes fly on the same principles as birds, which is observed by--
[01:58:24.200 --> 01:58:26.200]   so everyone get that analogy wrong.
[01:58:26.200 --> 01:58:28.200]   But let's step back from that.
[01:58:28.200 --> 01:58:34.200]   Once you understand the principles of flight, you can choose how to implement them.
[01:58:34.200 --> 01:58:39.200]   No one's going to use bones and feathers and muscles, but they do have wings,
[01:58:39.200 --> 01:58:40.200]   and we don't flap them.
[01:58:40.200 --> 01:58:41.200]   We have propellers.
[01:58:41.200 --> 01:58:46.200]   So when we have the principles of computation that goes on to modeling the world
[01:58:46.200 --> 01:58:50.200]   in a brain, we understand those principles very clearly.
[01:58:50.200 --> 01:58:53.200]   We have choices on how to implement them, and some of them will be biological-like
[01:58:53.200 --> 01:58:56.200]   and some won't.
[01:58:56.200 --> 01:58:59.200]   But I do think there's going to be a huge amount of innovation here.
[01:58:59.200 --> 01:59:01.200]   Just think about the innovation went into the computers.
[01:59:01.200 --> 01:59:04.200]   They had to invent the transistor.
[01:59:04.200 --> 01:59:06.200]   They had to invent the silicon chip.
[01:59:06.200 --> 01:59:09.200]   They had to invent software.
[01:59:09.200 --> 01:59:12.200]   I mean, there's millions of things they had to do, memory systems.
[01:59:12.200 --> 01:59:14.200]   It's going to be similar.
[01:59:14.200 --> 01:59:19.200]   Well, it's interesting that the deep learning--the effectiveness of deep learning
[01:59:19.200 --> 01:59:23.200]   for specific tasks is driving a lot of innovation in the hardware,
[01:59:23.200 --> 01:59:29.200]   which may have effects for actually allowing us to discover intelligence systems
[01:59:29.200 --> 01:59:32.200]   that operate very differently, or at least much bigger than deep learning.
[01:59:32.200 --> 01:59:33.200]   Yeah, interesting.
[01:59:33.200 --> 01:59:39.200]   So ultimately, it's good to have an application that's making our life better now
[01:59:39.200 --> 01:59:43.200]   because the capitalist process, if you can make money,
[01:59:43.200 --> 01:59:44.200]   that works.
[01:59:44.200 --> 01:59:48.200]   I mean, the other way--Neil deGrasse Tyson writes about this--
[01:59:48.200 --> 01:59:53.200]   is the other way we fund science, of course, is through military conquests.
[01:59:53.200 --> 01:59:56.200]   Here's an interesting thing we're doing on this regard.
[01:59:56.200 --> 01:59:59.200]   So we used to have a series of these biological principles,
[01:59:59.200 --> 02:00:02.200]   and we can see how to build these intelligent machines,
[02:00:02.200 --> 02:00:07.200]   but we've decided to apply some of these principles to today's machine learning techniques.
[02:00:07.200 --> 02:00:09.200]   So we didn't talk about this principle.
[02:00:09.200 --> 02:00:12.200]   One is sparsity in the brain.
[02:00:12.200 --> 02:00:14.200]   Most of the neurons are inactive at any point in time.
[02:00:14.200 --> 02:00:18.200]   It's sparse, and the connectivity is sparse, and that's different than deep learning networks.
[02:00:18.200 --> 02:00:23.200]   So we've already shown that we can speed up existing deep learning networks
[02:00:23.200 --> 02:00:28.200]   anywhere from 10 to a factor of 100--I mean, literally 100--
[02:00:28.200 --> 02:00:30.200]   and make them more robust at the same time.
[02:00:30.200 --> 02:00:34.200]   So this is commercially very, very valuable.
[02:00:34.200 --> 02:00:41.200]   And so if we can prove this actually in the largest systems that are commercially applied today,
[02:00:41.200 --> 02:00:44.200]   what's the big commercial desire to do this?
[02:00:44.200 --> 02:00:50.200]   Well, sparsity is something that doesn't run really well on existing hardware.
[02:00:50.200 --> 02:00:56.200]   It doesn't really run really well on GPUs and on CPUs.
[02:00:56.200 --> 02:01:00.200]   And so that would be a way of sort of bringing more brain principles
[02:01:00.200 --> 02:01:04.200]   into the existing system on a commercially valuable basis.
[02:01:04.200 --> 02:01:10.200]   Another thing we can think we can do is we're going to use these dendrites models of--
[02:01:10.200 --> 02:01:13.200]   we talked earlier about the prediction occurring inside a neuron.
[02:01:13.200 --> 02:01:17.200]   That basic property can be applied to existing neural networks
[02:01:17.200 --> 02:01:21.200]   and allow them to learn continuously, which is something they don't do today.
[02:01:21.200 --> 02:01:22.200]   And so they--
[02:01:22.200 --> 02:01:24.200]   The dendritic spikes that you were talking about.
[02:01:24.200 --> 02:01:27.200]   Yeah, well, we wouldn't model them as spikes, but the idea that you have--
[02:01:27.200 --> 02:01:30.200]   that today's neural networks have something called a point neuron,
[02:01:30.200 --> 02:01:32.200]   which is a very simple model of a neuron.
[02:01:32.200 --> 02:01:36.200]   And by adding dendrites to them at just one more level of complexity
[02:01:36.200 --> 02:01:41.200]   that's in biological systems, you can solve problems in continuous learning
[02:01:41.200 --> 02:01:43.200]   and rapid learning.
[02:01:43.200 --> 02:01:47.200]   So we're trying to take-- we're trying to bring the existing field--
[02:01:47.200 --> 02:01:48.200]   we'll see if we can do it.
[02:01:48.200 --> 02:01:53.200]   We're trying to bring the existing field of machine learning commercially along with us.
[02:01:53.200 --> 02:01:57.200]   You brought up this idea of keeping-- paying for it commercially along with us
[02:01:57.200 --> 02:02:00.200]   as we move towards the ultimate goal of a true AI system.
[02:02:00.200 --> 02:02:04.200]   Even small innovations on neural networks are really, really exciting.
[02:02:04.200 --> 02:02:09.200]   It seems like such a trivial model of the brain
[02:02:09.200 --> 02:02:13.200]   and applying different insights that just even, like you said,
[02:02:13.200 --> 02:02:19.200]   continuous learning or making it more asynchronous
[02:02:19.200 --> 02:02:24.200]   or maybe making more dynamic or like incentivizing--
[02:02:24.200 --> 02:02:27.200]   Or more robust, even just more robust.
[02:02:27.200 --> 02:02:33.200]   And making it somehow much better-- incentivizing sparsity somehow.
[02:02:33.200 --> 02:02:34.200]   Yeah.
[02:02:34.200 --> 02:02:36.200]   Well, if you can make things 100 times faster,
[02:02:36.200 --> 02:02:38.200]   then there's plenty of incentive there.
[02:02:38.200 --> 02:02:39.200]   That's true.
[02:02:39.200 --> 02:02:43.200]   People are spending millions of dollars just training some of these networks now,
[02:02:43.200 --> 02:02:46.200]   these transforming networks.
[02:02:46.200 --> 02:02:48.200]   Let me ask you the big question.
[02:02:48.200 --> 02:02:53.200]   How, for young people listening to this today in high school and college,
[02:02:53.200 --> 02:02:58.200]   what advice would you give them in terms of which career path to take
[02:02:58.200 --> 02:03:03.200]   and maybe just about life in general?
[02:03:03.200 --> 02:03:08.200]   Well, in my case, I didn't start life with any kind of goals.
[02:03:08.200 --> 02:03:11.200]   When I was going to college, it was like, "Oh, what did I study?"
[02:03:11.200 --> 02:03:15.200]   "Well, maybe I'll do some electrical engineering stuff."
[02:03:15.200 --> 02:03:17.200]   It wasn't like-- Today you see some of these young kids are so motivated,
[02:03:17.200 --> 02:03:19.200]   they're going to change the world.
[02:03:19.200 --> 02:03:22.200]   I was like, "Eh, whatever."
[02:03:22.200 --> 02:03:26.200]   But then I did fall in love with something, besides my wife.
[02:03:26.200 --> 02:03:29.200]   I fell in love with this, like, "Oh, my God, it would be so cool
[02:03:29.200 --> 02:03:31.200]   to understand how the brain works."
[02:03:31.200 --> 02:03:34.200]   Then I said to myself, "That's the most important thing I could work on."
[02:03:34.200 --> 02:03:37.200]   I can't imagine anything more important because if we understand how brains work,
[02:03:37.200 --> 02:03:39.200]   we could build telecom machines and they could figure out
[02:03:39.200 --> 02:03:42.200]   all the other big questions of the world.
[02:03:42.200 --> 02:03:44.200]   Then I said, "I want to understand how I work."
[02:03:44.200 --> 02:03:49.200]   I fell in love with this idea and I became passionate about it.
[02:03:49.200 --> 02:03:53.200]   This is a trope. People say this, but it's true.
[02:03:53.200 --> 02:03:56.200]   Because I was passionate about it, I was able to put up
[02:03:56.200 --> 02:04:00.200]   with almost so much crap.
[02:04:00.200 --> 02:04:03.200]   I was that person who said, "You can't do this."
[02:04:03.200 --> 02:04:05.200]   I was a graduate student at Berkeley when they said,
[02:04:05.200 --> 02:04:08.200]   "You can't study this problem. No one can solve this,"
[02:04:08.200 --> 02:04:10.200]   or, "You can't get funded for it."
[02:04:10.200 --> 02:04:12.200]   Then I went in to do mobile computing and it was like,
[02:04:12.200 --> 02:04:17.200]   people say, "You can't do that. You can't build a cell phone."
[02:04:17.200 --> 02:04:20.200]   But all along, I kept being motivated because I wanted to work on this problem.
[02:04:20.200 --> 02:04:22.200]   I said, "I want to understand how the brain works."
[02:04:22.200 --> 02:04:24.200]   I got myself, I got one lifetime.
[02:04:24.200 --> 02:04:27.200]   I'm going to figure it out, do the best I can.
[02:04:27.200 --> 02:04:31.200]   By having that, because it's really, as you pointed out, Lex,
[02:04:31.200 --> 02:04:34.200]   it's really hard to do these things.
[02:04:34.200 --> 02:04:38.200]   There's so many downers along the way, so many obstacles that get in your way.
[02:04:38.200 --> 02:04:42.200]   I'm sitting here happy all the time, but trust me, it's not always like that.
[02:04:42.200 --> 02:04:47.200]   I guess the happiness, the passion is a prerequisite for surviving the whole thing.
[02:04:47.200 --> 02:04:51.200]   Yeah, I think so. I think that's right.
[02:04:51.200 --> 02:04:54.200]   I don't want to sit to someone and say, "You need to find a passion and do it."
[02:04:54.200 --> 02:04:59.200]   No, maybe you don't, but if you do find something you're passionate about,
[02:04:59.200 --> 02:05:04.200]   then you can follow it as far as your passion will let you put up with it.
[02:05:04.200 --> 02:05:09.200]   Do you remember how you found it, how the spark happened?
[02:05:09.200 --> 02:05:11.200]   Why specifically for me?
[02:05:11.200 --> 02:05:13.200]   Yeah, because you said, "It's such an interesting,"
[02:05:13.200 --> 02:05:18.200]   so almost like later in life, by later, I mean not when you were five,
[02:05:18.200 --> 02:05:22.200]   you didn't really know, and then all of a sudden you fell in love with it.
[02:05:22.200 --> 02:05:25.200]   Yeah, there was two separate events that compounded one another.
[02:05:25.200 --> 02:05:30.200]   One, when I was probably a teenager, I might have been 17 or 18,
[02:05:30.200 --> 02:05:34.200]   I made a list of the most interesting problems I could think of.
[02:05:34.200 --> 02:05:36.200]   First was, why does the universe exist?
[02:05:36.200 --> 02:05:39.200]   It seems like not existing is more likely.
[02:05:39.200 --> 02:05:42.200]   The second one was, well, given it exists, why does it behave the way it does?
[02:05:42.200 --> 02:05:45.200]   It's laws of physics. Why is it equal to mc squared, not mc cubed?
[02:05:45.200 --> 02:05:47.200]   That's an interesting question. I don't know.
[02:05:47.200 --> 02:05:50.200]   The third one was, what's the origin of life?
[02:05:50.200 --> 02:05:53.200]   The fourth one was, what's intelligence?
[02:05:53.200 --> 02:05:56.200]   I stopped there. I said, "Well, that's probably the most interesting one."
[02:05:56.200 --> 02:05:59.200]   I put that aside as a teenager.
[02:05:59.200 --> 02:06:04.200]   But then when I was 22, and I was reading the--
[02:06:04.200 --> 02:06:08.200]   No, excuse me, it was 1979.
[02:06:08.200 --> 02:06:11.200]   I was reading-- At that time I was 22.
[02:06:11.200 --> 02:06:14.200]   I was reading the September issue of Scientific American,
[02:06:14.200 --> 02:06:16.200]   which is all about the brain.
[02:06:16.200 --> 02:06:22.200]   And then the final essay was by Francis Crick, of DNA fame.
[02:06:22.200 --> 02:06:25.200]   And he had taken his interest to studying the brain now.
[02:06:25.200 --> 02:06:28.200]   And he said, "You know, there's something wrong here."
[02:06:28.200 --> 02:06:32.200]   He says, "We got all this data, all this fact."
[02:06:32.200 --> 02:06:35.200]   This is 1979. "All these facts about the brain.
[02:06:35.200 --> 02:06:37.200]   Tons and tons of facts about the brain.
[02:06:37.200 --> 02:06:40.200]   Do we need more facts, or do we just need to think about a way
[02:06:40.200 --> 02:06:42.200]   of rearranging the facts we have?
[02:06:42.200 --> 02:06:45.200]   Maybe we're just not thinking about the problem correctly."
[02:06:45.200 --> 02:06:50.200]   So he says, "This shouldn't be like this."
[02:06:50.200 --> 02:06:53.200]   So I read that and I said, "Wow."
[02:06:53.200 --> 02:06:57.200]   I said, "I don't have to become an experimental neuroscientist.
[02:06:57.200 --> 02:07:02.200]   I could just look at all those facts and become a theoretician
[02:07:02.200 --> 02:07:04.200]   and try to figure it out."
[02:07:04.200 --> 02:07:07.200]   And I said, "That I felt like was something I would be good at."
[02:07:07.200 --> 02:07:09.200]   I said, "I wouldn't be a good experimentalist.
[02:07:09.200 --> 02:07:11.200]   I don't have the patience for it.
[02:07:11.200 --> 02:07:14.200]   But I'm a good thinker, and I love puzzles.
[02:07:14.200 --> 02:07:16.200]   It's like the biggest puzzle in the world.
[02:07:16.200 --> 02:07:18.200]   It's the biggest puzzle of all time.
[02:07:18.200 --> 02:07:20.200]   I got all the puzzle pieces in front of me.
[02:07:20.200 --> 02:07:22.200]   Damn, that was exciting."
[02:07:22.200 --> 02:07:25.200]   And there's something, obviously, you can't convert into words
[02:07:25.200 --> 02:07:28.200]   that just kind of sparked this passion.
[02:07:28.200 --> 02:07:32.200]   I mean, I have that a few times in my life, just something--
[02:07:32.200 --> 02:07:33.200]   Yeah.
[02:07:33.200 --> 02:07:36.200]   --just like you--it grabs you.
[02:07:36.200 --> 02:07:38.200]   Yeah. I thought it was something that was both important
[02:07:38.200 --> 02:07:40.200]   and that I could make a contribution to.
[02:07:40.200 --> 02:07:41.200]   Yeah.
[02:07:41.200 --> 02:07:43.200]   And so all of a sudden, I felt like, "Oh, it gave me purpose in life."
[02:07:43.200 --> 02:07:44.200]   Yeah.
[02:07:44.200 --> 02:07:47.200]   I honestly don't think it has to be as big as one of those four questions.
[02:07:47.200 --> 02:07:48.200]   No, no, but--
[02:07:48.200 --> 02:07:51.200]   I think you can find those things in the smallest.
[02:07:51.200 --> 02:07:52.200]   Oh, absolutely.
[02:07:52.200 --> 02:07:56.200]   I'm with--David Foster Wallace said, "The key to life is to be unboreable."
[02:07:56.200 --> 02:08:02.200]   I think it's very possible to find that intensity of joy in the smallest thing.
[02:08:02.200 --> 02:08:03.200]   Absolutely.
[02:08:03.200 --> 02:08:04.200]   I'm just--you asked me my story.
[02:08:04.200 --> 02:08:05.200]   Yeah, yeah.
[02:08:05.200 --> 02:08:07.200]   No, but I'm actually speaking to the audience.
[02:08:07.200 --> 02:08:09.200]   It doesn't have to be those four.
[02:08:09.200 --> 02:08:12.200]   You happen to get excited by one of the bigger questions of--
[02:08:12.200 --> 02:08:13.200]   Yeah.
[02:08:13.200 --> 02:08:17.200]   --in the universe, but even the smallest things.
[02:08:17.200 --> 02:08:18.200]   No, absolutely.
[02:08:18.200 --> 02:08:19.200]   I'm watching the Olympics now.
[02:08:19.200 --> 02:08:20.200]   Oh, yeah.
[02:08:20.200 --> 02:08:24.200]   Just giving yourself life--giving your life over to the study
[02:08:24.200 --> 02:08:27.200]   and the mastery of a particular sport is fascinating.
[02:08:27.200 --> 02:08:28.200]   It is.
[02:08:28.200 --> 02:08:33.200]   And if it sparks joy and passion, you're able to, in the case of the Olympics,
[02:08:33.200 --> 02:08:36.200]   basically suffer for a couple of decades to achieve perfection.
[02:08:36.200 --> 02:08:38.200]   I mean, you can find joy and passion just being a parent.
[02:08:38.200 --> 02:08:39.200]   I mean, it's--
[02:08:39.200 --> 02:08:40.200]   Yeah.
[02:08:40.200 --> 02:08:41.200]   The parenting one is funny.
[02:08:41.200 --> 02:08:46.200]   So I was--not always, but for a long time, wanted kids and get married and stuff,
[02:08:46.200 --> 02:08:51.200]   and especially it has to do with the fact that I've seen a lot of people
[02:08:51.200 --> 02:08:57.200]   that I respect get a whole other level of joy from kids.
[02:08:57.200 --> 02:09:05.200]   And at first, it's like you're thinking is, "Well, I don't have enough time in the day," right?
[02:09:05.200 --> 02:09:06.200]   If I have this passion to solve--
[02:09:06.200 --> 02:09:07.200]   Which is true.
[02:09:07.200 --> 02:09:08.200]   --the problem--
[02:09:08.200 --> 02:09:09.200]   Which is true.
[02:09:09.200 --> 02:09:14.200]   --but if I want to solve intelligence, how is this kid situation going to help me?
[02:09:14.200 --> 02:09:22.200]   But then you realize that, like you said, the things that sparks joy,
[02:09:22.200 --> 02:09:26.200]   and it's very possible that kids can provide even a greater or deeper
[02:09:26.200 --> 02:09:32.200]   or more meaningful joy than those bigger questions when they enrich each other.
[02:09:32.200 --> 02:09:34.200]   And that seemed like--obviously, when I was younger,
[02:09:34.200 --> 02:09:38.200]   it was probably a counterintuitive notion because there's only so many hours in the day.
[02:09:38.200 --> 02:09:44.200]   But then life is finite, and you have to pick the things that give you joy.
[02:09:44.200 --> 02:09:48.200]   Yeah. But you also--I understand you can be patient, too.
[02:09:48.200 --> 02:09:52.200]   I mean, it's finite, but we do have whatever, 50 years or something.
[02:09:52.200 --> 02:09:53.200]   It's also long, yeah.
[02:09:53.200 --> 02:09:58.200]   So in my case, I had to give up on my dream of the neuroscience
[02:09:58.200 --> 02:10:01.200]   because I was a graduate student at Berkeley, and they told me I couldn't do this,
[02:10:01.200 --> 02:10:04.200]   and I couldn't get funded.
[02:10:04.200 --> 02:10:09.200]   And so I went back in the computing industry for a number of years.
[02:10:09.200 --> 02:10:11.200]   I thought it would be four, but it turned out to be more.
[02:10:11.200 --> 02:10:15.200]   But I said, "I'll come back. I'm definitely going to come back.
[02:10:15.200 --> 02:10:18.200]   I know I'm going to do this computer stuff for a while, but I'm definitely coming back.
[02:10:18.200 --> 02:10:21.200]   Everyone knows that." And it's the same as raising kids.
[02:10:21.200 --> 02:10:23.200]   Well, yeah, you have to spend a lot of time with your kids.
[02:10:23.200 --> 02:10:25.200]   It's fun, enjoyable.
[02:10:25.200 --> 02:10:28.200]   But that doesn't mean you have to give up on other dreams.
[02:10:28.200 --> 02:10:32.200]   It just means that you may have to wait a week or two to work on that next idea.
[02:10:32.200 --> 02:10:39.200]   You talk about the darker side of me, disappointing sides of human nature
[02:10:39.200 --> 02:10:43.200]   that we're hoping to overcome so that we don't destroy ourselves.
[02:10:43.200 --> 02:10:49.200]   I tend to put a lot of value in the broad general concept of love,
[02:10:49.200 --> 02:10:58.200]   of the human capacity of compassion towards each other, of just kindness,
[02:10:58.200 --> 02:11:01.200]   whatever that longing of just the human-to-human connection.
[02:11:01.200 --> 02:11:04.200]   It connects back to our initial discussion.
[02:11:04.200 --> 02:11:07.200]   I tend to see a lot of value in this collective intelligence aspect.
[02:11:07.200 --> 02:11:12.200]   I think some of the magic of human civilization happens when there's...
[02:11:12.200 --> 02:11:15.200]   A party is not as fun when you're alone.
[02:11:15.200 --> 02:11:17.200]   I totally agree with you on these issues.
[02:11:17.200 --> 02:11:22.200]   Do you think, from a neocortex perspective,
[02:11:22.200 --> 02:11:25.200]   what role does love play in the human condition?
[02:11:25.200 --> 02:11:26.200]   Well, those are two separate things.
[02:11:26.200 --> 02:11:29.200]   From a neocortex point of view, I don't think it doesn't impact
[02:11:29.200 --> 02:11:32.200]   our thinking about the neocortex.
[02:11:32.200 --> 02:11:36.200]   From a human condition point of view, I think it's core.
[02:11:36.200 --> 02:11:44.200]   I mean, we get so much pleasure out of loving people and helping people.
[02:11:44.200 --> 02:11:47.200]   So, I'll rack that up to old brain stuff,
[02:11:47.200 --> 02:11:52.200]   and maybe we can throw it under the bus of evolution if you want.
[02:11:52.200 --> 02:11:54.200]   That's fine.
[02:11:54.200 --> 02:11:58.200]   It doesn't impact how we think about how we model the world.
[02:11:58.200 --> 02:12:01.200]   But from a humanity point of view, I think it's essential.
[02:12:01.200 --> 02:12:03.200]   I tend to give it to the new brain.
[02:12:03.200 --> 02:12:07.200]   Also, I tend to think that some aspects of that need to be engineered
[02:12:07.200 --> 02:12:16.200]   into AI systems, both in their ability to have compassion for other humans
[02:12:16.200 --> 02:12:22.200]   and their ability to maximize love in the world between humans.
[02:12:22.200 --> 02:12:25.200]   I'm more thinking about social networks.
[02:12:25.200 --> 02:12:29.200]   Whenever there's a deep integration between AI systems and humans,
[02:12:29.200 --> 02:12:32.200]   specific applications where it's AI and humans,
[02:12:32.200 --> 02:12:40.200]   I think that's something that's often not talked about in terms of metrics
[02:12:40.200 --> 02:12:46.200]   over which you try to maximize, like which metric to maximize in a system.
[02:12:46.200 --> 02:12:54.200]   It seems like one of the most powerful things in societies is the capacity
[02:12:54.200 --> 02:12:55.200]   to--
[02:12:55.200 --> 02:12:56.200]   It's a fascinating idea.
[02:12:56.200 --> 02:12:58.200]   I think it's a great way of thinking about it.
[02:12:58.200 --> 02:13:03.200]   I have been thinking more of these fundamental mechanisms in the brain
[02:13:03.200 --> 02:13:07.200]   as opposed to the social interaction between humans and AI systems
[02:13:07.200 --> 02:13:09.200]   in the future.
[02:13:09.200 --> 02:13:12.200]   I think if you think about that, you're absolutely right.
[02:13:12.200 --> 02:13:14.200]   But that's a complex system.
[02:13:14.200 --> 02:13:16.200]   I can have intelligence systems that don't have that component,
[02:13:16.200 --> 02:13:18.200]   but they're not interacting with people.
[02:13:18.200 --> 02:13:21.200]   They're just running something or building something.
[02:13:21.200 --> 02:13:23.200]   I don't know.
[02:13:23.200 --> 02:13:26.200]   But if you think about interacting with humans, yeah.
[02:13:26.200 --> 02:13:28.200]   But it has to be engineered in there.
[02:13:28.200 --> 02:13:30.200]   I don't think it's going to appear on its own.
[02:13:30.200 --> 02:13:32.200]   That's a good question.
[02:13:32.200 --> 02:13:35.200]   Yeah, well, we'll look at it.
[02:13:35.200 --> 02:13:41.200]   In terms of from a reinforcement learning perspective,
[02:13:41.200 --> 02:13:47.200]   whether the darker sides of human nature or the better angels of our nature
[02:13:47.200 --> 02:13:50.200]   win out statistically speaking, I don't know.
[02:13:50.200 --> 02:13:54.200]   I tend to be optimistic and hope that love wins out in the end.
[02:13:54.200 --> 02:14:01.200]   You've done a lot of incredible stuff, and your book is driving towards
[02:14:01.200 --> 02:14:06.200]   this fourth question that you started with on the nature of intelligence.
[02:14:06.200 --> 02:14:13.200]   What do you hope your legacy for people reading 100 years from now--
[02:14:13.200 --> 02:14:15.200]   How do you hope they remember your work?
[02:14:15.200 --> 02:14:17.200]   How do you hope they remember this book?
[02:14:17.200 --> 02:14:22.200]   Well, I think as an entrepreneur or a scientist or any human
[02:14:22.200 --> 02:14:27.200]   who's trying to accomplish some things, I have a view that really
[02:14:27.200 --> 02:14:31.200]   all you can do is accelerate the inevitable.
[02:14:31.200 --> 02:14:36.200]   It's like if we didn't study the brain, someone else would study the brain.
[02:14:36.200 --> 02:14:39.200]   If Elon Musk didn't make electric cars, someone else would do it eventually.
[02:14:39.200 --> 02:14:42.200]   If Thomas Edison didn't invent a light bulb,
[02:14:42.200 --> 02:14:44.200]   we wouldn't be using candles today.
[02:14:44.200 --> 02:14:49.200]   What you can do as an individual is you can accelerate something
[02:14:49.200 --> 02:14:52.200]   that's beneficial and make it happen sooner than whatever.
[02:14:52.200 --> 02:14:55.200]   That's really it. That's all you can do.
[02:14:55.200 --> 02:15:00.200]   You can't create a new reality that it wasn't going to happen.
[02:15:00.200 --> 02:15:04.200]   From that perspective, I would hope that our work--
[02:15:04.200 --> 02:15:07.200]   not just me, but our work in general--
[02:15:07.200 --> 02:15:10.200]   people would look back and say, "Hey, they really helped make
[02:15:10.200 --> 02:15:13.200]   this better future happen sooner."
[02:15:13.200 --> 02:15:17.200]   "They helped us understand the nature of false beliefs
[02:15:17.200 --> 02:15:19.200]   sooner than they made Ryan Ruddup."
[02:15:19.200 --> 02:15:21.200]   "Now we're so happy that we have these intelligent machines
[02:15:21.200 --> 02:15:24.200]   doing these things, helping us, that maybe that solved
[02:15:24.200 --> 02:15:28.200]   the climate change problem and they made it happen sooner."
[02:15:28.200 --> 02:15:30.200]   I think that's the best I would hope for.
[02:15:30.200 --> 02:15:33.200]   Some would say, "Those guys just moved the needle forward
[02:15:33.200 --> 02:15:36.200]   a little bit in time."
[02:15:36.200 --> 02:15:42.200]   Well, it feels like the progress of human civilization is not--
[02:15:42.200 --> 02:15:44.200]   there's a lot of trajectories.
[02:15:44.200 --> 02:15:50.200]   If you have individuals that accelerate towards one direction,
[02:15:50.200 --> 02:15:52.200]   that helps steer human civilization.
[02:15:52.200 --> 02:15:58.200]   I think in a long stretch of time, all trajectories will be traveled,
[02:15:58.200 --> 02:16:01.200]   but I think it's nice for this particular civilization on Earth
[02:16:01.200 --> 02:16:03.200]   to travel down one that's not--
[02:16:03.200 --> 02:16:06.200]   I think you're right. We have to take the whole period of
[02:16:06.200 --> 02:16:08.200]   World War II, Nazism, or something like that.
[02:16:08.200 --> 02:16:11.200]   Well, that was a bad sidestep. We've been over there for a while.
[02:16:11.200 --> 02:16:15.200]   But there is the optimistic view about life that ultimately
[02:16:15.200 --> 02:16:18.200]   it does converge in a positive way.
[02:16:18.200 --> 02:16:24.200]   It progresses ultimately, even if we have years of darkness.
[02:16:24.200 --> 02:16:26.200]   Yeah, so I think you could perhaps--
[02:16:26.200 --> 02:16:28.200]   that's accelerating the positive.
[02:16:28.200 --> 02:16:33.200]   It could also mean eliminating some bad missteps along the way too.
[02:16:33.200 --> 02:16:36.200]   But I'm an optimist in that way.
[02:16:36.200 --> 02:16:39.200]   Despite we're talking about the end of civilization,
[02:16:39.200 --> 02:16:42.200]   I think we're going to live for a long time. I hope we are.
[02:16:42.200 --> 02:16:44.200]   I think our society in the future is going to be better.
[02:16:44.200 --> 02:16:45.200]   We're going to have less discord.
[02:16:45.200 --> 02:16:47.200]   We're going to have less people killing each other.
[02:16:47.200 --> 02:16:50.200]   We'll live in some sort of way that's compatible
[02:16:50.200 --> 02:16:53.200]   with the carrying capacity of the Earth.
[02:16:53.200 --> 02:16:56.200]   I'm optimistic these things will happen,
[02:16:56.200 --> 02:16:58.200]   and all we can do is try to get there sooner.
[02:16:58.200 --> 02:17:00.200]   And at the very least, if we do destroy ourselves,
[02:17:00.200 --> 02:17:02.200]   we'll have a few satellites orbiting--
[02:17:02.200 --> 02:17:04.200]   Hopefully, yeah.
[02:17:04.200 --> 02:17:08.200]   --that will tell alien civilization that we were once here.
[02:17:08.200 --> 02:17:12.200]   Or maybe our future inhabitants of Earth.
[02:17:12.200 --> 02:17:15.200]   Imagine the planet of the apes in here.
[02:17:15.200 --> 02:17:18.200]   We kill ourselves a million years from now or a billion years from now.
[02:17:18.200 --> 02:17:19.200]   There's another species on the planet.
[02:17:19.200 --> 02:17:21.200]   There's curious creatures who were once here.
[02:17:21.200 --> 02:17:22.200]   Yeah.
[02:17:22.200 --> 02:17:24.200]   Jeff, thank you so much for your work,
[02:17:24.200 --> 02:17:27.200]   and thank you so much for talking to me once again.
[02:17:27.200 --> 02:17:29.200]   Well, actually, it's great. I love what you do.
[02:17:29.200 --> 02:17:30.200]   I love your podcast.
[02:17:30.200 --> 02:17:32.200]   You have the most interesting people, me aside.
[02:17:32.200 --> 02:17:34.200]   [laughter]
[02:17:34.200 --> 02:17:38.200]   So it's a real service, I think, you do for--
[02:17:38.200 --> 02:17:40.200]   in a very broader sense for humanity, I think.
[02:17:40.200 --> 02:17:41.200]   Thanks, Jeff.
[02:17:41.200 --> 02:17:43.200]   All right. It's a pleasure.
[02:17:43.200 --> 02:17:45.200]   Thanks for listening to this conversation with Jeff Hawkins,
[02:17:45.200 --> 02:17:50.200]   and thank you to Codecademy, BioOptimizers, ExpressVPN,
[02:17:50.200 --> 02:17:53.200]   Asleep, and Blinkist.
[02:17:53.200 --> 02:17:56.200]   Check them out in the description to support this podcast.
[02:17:56.200 --> 02:18:01.200]   And now let me leave you with some words from Albert Camus.
[02:18:01.200 --> 02:18:05.200]   "An intellectual is someone whose mind watches itself."
[02:18:05.200 --> 02:18:08.200]   I like this because I'm happy to be both halves,
[02:18:08.200 --> 02:18:11.200]   the watcher and the watched.
[02:18:11.200 --> 02:18:13.200]   Can they be brought together?
[02:18:13.200 --> 02:18:17.200]   This is a practical question we must try to answer.
[02:18:17.200 --> 02:18:20.200]   Thank you for listening, and hope to see you next time.
[02:18:20.200 --> 02:18:23.200]   [no audio]
[02:18:23.200 --> 02:18:26.200]   [no audio]
[02:18:26.200 --> 02:18:29.780]   (Session concluded at 4pm)

