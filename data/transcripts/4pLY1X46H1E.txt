
[00:00:00.000 --> 00:00:04.060]   This is Sax's big day. There he is. Oh, there's Tucker. Oh, look
[00:00:04.060 --> 00:00:07.560]   at the smile on Sax's face. This is the greatest day in the
[00:00:07.560 --> 00:00:26.900]   history of the All-in Park. Look at how happy Sax is. Oh, Sax! By the way, I'm not ashamed of that. You're not, no, I'm honored. Oh, whoa, Sax! How threatened do you feel right now? This is the highest
[00:00:26.900 --> 00:00:41.480]   rated hosting in cable history. This is the world's true greatest moderator. Exactly. No doubt. No doubt. Sax, where is this in relation to your marriage and the birth of your children? Don't ask. It's right up there. Up there. He's like, what children? It is for me.
[00:00:41.480 --> 00:00:48.020]   Rain Man David Sax.
[00:00:48.020 --> 00:01:11.360]   All right, everybody, we've got an amazing guest for you today here on the All-in Podcast. Sax's dream has come true. Tucker Carlson is with us today. You know, Tucker, he was the number one TV host for much of
[00:01:11.360 --> 00:01:28.560]   the past decade, including last year, when shockingly, he was fired from Fox News on April 24. Reason for the firing. It's never been pinned down, but maybe we'll get into it today. And we're going to find out what is motivating a post Fox News Tucker, who has
[00:01:28.600 --> 00:01:48.440]   obviously launched a show on X, the platform formerly known as Twitter. He's done 42 episodes and counting. He's had everybody from Donald Trump, Andrew Tate, Dave Portnoy, and the newly elected president of Argentina on the program. So welcome to the All-in
[00:01:48.440 --> 00:01:50.640]   Podcast, Tucker Carlson.
[00:01:50.640 --> 00:01:54.960]   Thank you for having me. It is it is a legit honor to be here.
[00:01:55.760 --> 00:02:19.520]   Two part question to kick us off here. First, have you figured out why you were fired from Fox? And let's get into that a bit. And second, given that you were the number one host for for much of the past decade, and I think probably in the top five highest paid of all time, what's motivating you now? What's the mission here as an independent journalist? Take those two questions in whichever order you like.
[00:02:19.640 --> 00:02:47.880]   I don't know why I was fired. I mean, it kind of is an Agatha Christie story. There are like so many suspects, you know what I mean? But I don't know. I was never told I can only speculate. There were a lot of different things going on. I had a lot of opinions that were unpopular, you know, with people who might have influenced my show getting canceled. So I really don't know. I will say, you know, right after it happened, people said, well, how can they fire the top guy? And because that's what it is.
[00:02:47.880 --> 00:03:13.640]   I'm certainly not the first high rated host to get fired. It's not only about ratings. There are a lot of different factors. It's a big company you all have worked for and run big companies. And, you know, it's there's a lot of complicated stuff going on. And it's never exactly clear, you know, why things happen the way they do. But I was not shocked by it. I was shocked by it in the short term sense. I didn't expect to have my show canceled that morning. But I was not shocked at all.
[00:03:14.640 --> 00:03:41.200]   When I thought about it for a minute, I'd expected that, you know, you can't kind of give the finger to everybody and persist in a in a corporate job. So I no hard feelings. And I and I in fact, I said that on the call when I received the news. It's not my company. And I never felt like I had a right to be on the air. I was I was working at the pleasure of the family that runs the company who treated me very well. And, and, and they wanted me off. And so I was off.
[00:03:41.360 --> 00:03:59.760]   Did you ever have moments where somebody taps you on the shoulder and says, advertiser XYZ is getting uncomfortable, or we're trying to land this new advertiser, and right, they want you to shape things in one way? Did you ever feel that pressure? Is that or is that just a thing that is kind of like a boogeyman that doesn't actually exist?
[00:03:59.920 --> 00:04:20.560]   Oh, it will it not only exists, it defines news coverage, especially on pharma. You know, because pharma is the biggest advertiser in television, as I know, you know, and so for sure. I mean, if you know, Pfizer sponsoring your show, you're not going to question the facts. I mean, it's kind of that simple. So absolutely. And of course, that's why they're the biggest advertiser. So they can shape news coverage. I mean, that's, that's the point.
[00:04:20.960 --> 00:04:49.840]   But I personally never had a single person say to me, don't say this. That I recall, I haven't thought about it too much. But that certainly I was there 14 years. And I didn't have that experience regularly, or at all really that I can remember. And, and I think, you know, my producers may have been told that, but it didn't ever get to me because I was always really clear, which is, I always said out loud to the supervisors there, you know, I work for your company, I don't own this network.
[00:04:50.240 --> 00:05:14.080]   All I can control is what I say. If you don't like what I say, don't have me on TV. But as long as I'm on TV, I'm going to say what I think is true. And in a million cases, I said only part of what I think, not because of my employer, but just because you shouldn't actually say everything you think. I mean, I have some crackpot views too. Or I have resentments that I didn't want to work out on the air. I mean, I did, you know, you're straining yourself and you want to, as you do in your personal life.
[00:05:14.080 --> 00:05:30.960]   But on no question of principle, did I ever pull back because I just, I wouldn't do that. And again, I was just super clear. If you don't like what I'm saying, take me off the air, but I'm not going to, you know, toe a line. And because I was so clear about that, I just think they didn't think it was worth having some kind of dispute with me.
[00:05:30.960 --> 00:05:48.880]   And to their great credit, for the time that I was there, and I said this many times in public, like I took positions on the Ukraine war, on the COVID vaccine, on the COVID lockdowns, among other issues that I think, you know, I've been vindicated on pretty conclusively on the origins of COVID.
[00:05:48.880 --> 00:06:04.720]   And all of those are super unpopular. On January 6th, which was so hated at the company where I worked that people, a number of people, including on-air people, four that I can think of, resigned in protest over my, over me suggesting that actually it was more complicated than it looked.
[00:06:04.720 --> 00:06:14.960]   And there were a bunch of federal agents in the crowd. How can you say that? Are you claiming a false flag? Well, no, not, wouldn't use that phrase, but like, this is something weird going on here. Well, I've been vindicated on that.
[00:06:14.960 --> 00:06:30.080]   It sounds like I'm bragging. I'm not. I'm just stating factually that I said things that were truly hated by a lot of the people who worked there and they let me keep saying them. So it's kind of hard to complain really, at this point, right? Again, it's not my company.
[00:06:30.080 --> 00:06:58.120]   Just from a business standpoint, I think it's weird for a company to fire their top performer and to do so without giving any notes. I mean, if any of us had a superstar executive or a superstar engineer, like a hundred x engineer, working at one of our companies, and like day in and day out, they were, you know, hitting every milestone and crushing it. Like if you had a problem with them, you would give them a note, you would just like try to say, Hey, can we just like it?
[00:06:58.120 --> 00:07:19.400]   So I just think from like a business standpoint, it's so weird. It just seems like self-destructive. And I think it was, I mean, their ratings really cratered in the wake of making this change. Maybe they've come back a little bit, but I don't think it's ever been the same. I just think it's a crazy way to operate a business. So yeah, it's their right. I mean, they can do whatever they want, but I don't understand it as a way of doing business.
[00:07:19.400 --> 00:07:42.120]   Well, I don't understand it as a way of living either. I mean, you know, everybody in the course of life, whether as a parent or an employer, or just a friend has to deliver uncomfortable news or disagree with someone that you deal with. And you have a moral obligation to explain the disagreement. You can't just, you know, levy the penalty and leave it at that. You have to explain why you're doing that. And I think that's, it's incumbent on us morally to do that.
[00:07:42.280 --> 00:08:10.920]   I wasn't that mad about it actually, because I know the rules of that particular business, which are really harsh. And I've been in it, you know, my whole life. And so I've seen a lot of people as talented or more talented than I meet bad ends. And, you know, for reasons that I thought were not justified. And I know them all really well. So you work in a business like that, you know what it is. You know, the black car is going to show up at 3am and tow to the Lubyanka. And that's just what it is. You know what I mean?
[00:08:10.920 --> 00:08:12.440]   You can't kind of whine about it, you know?
[00:08:12.440 --> 00:08:42.360]   How much of it was the, you know, this is a family owned business. And the patriarch, obviously, pioneered opinion based, you know, journalism slash entertainment commentary. And the younger ones maybe were on the other side of the political aisle, and maybe were not as I don't know, cutthroat, or maybe didn't share the same philosophy of their dad, what was your relationship like with the new generation with Rupert, et cetera?
[00:08:42.360 --> 00:08:44.360]   And how much did that play into it? Do you think?
[00:08:44.360 --> 00:09:13.360]   Well, my relationship with the father and son who are directly involved in that company was, from my perspective, very strong. And I will say this about the Murdochs. They're very polite. I mean, they're really kind of very Anglo, almost elaborately polite in a way that I'm not mocking, I'm complimenting. And they're not confrontational, they're not nasty in the way that they deal with people directly. And I prefer that as sort of a way of communicating with people.
[00:09:13.360 --> 00:09:33.360]   So I got along with them very, very well. I always liked them. And they were very nice to me, elaborately nice to me, and always gave me assurances of my right to say what I thought was true. And so again, I can only speculate. I will say, though, and you see this with Trump, especially, I don't think I'm anywhere near as divisive as Trump. Obviously, I'm not as powerful as Trump. I'm not the figure Trump is.
[00:09:33.360 --> 00:10:00.360]   But one thing that maybe Trump and I have in common is we're really disliked by a certain set of people, affluent people, highly educated people, people who work at NGOs, government, finance, really kind of hate a certain brand of politics. So it's not being conservative. You can be conservative in the sort of, I work at Cato or Heritage, or we need to get back to free trade or whatever, that kind of thing, the Reaganite foreign policy. That's all fine.
[00:10:00.360 --> 00:10:14.360]   But if you start asking questions like, well, why doesn't our country act in its own interest? There's something about that that's uniquely offensive to them, to that whole class of people. Now, I could not have more contempt or loathing for those people, having grown up among them. I know how repulsive they are.
[00:10:14.360 --> 00:10:36.360]   So their hatred of me, I wear as a badge of honor. It actually makes me happy. But it's hard to take. I would say, I mean, again, I'm just speculating in my specific case, but I know more broadly, like it's very hard to have lunch at the Four Seasons in Jackson during the winter because there's some private equity wife who's going to scream at you on your way to the men's room because that world hates you.
[00:10:36.360 --> 00:10:49.360]   And so if you live in that world and you're employing someone like me, you know, you hear about it. I guess that's my point. You hear about it like, what? That guy's a Nazi. What's the deal with that?
[00:10:49.360 --> 00:11:05.360]   I want to actually pull this thread. I would love your perspective on the state of American society, just less on the political spectrum of Republican versus Democrat. But just just observe for us, Tucker, what do you see in American society? Where are we as a society? What has happened? What is happening?
[00:11:05.360 --> 00:11:27.360]   Well, this isn't an eight hour podcast. I could actually give you my very lengthy theories and views on that. But I will just say one thing that I've been thinking about a lot recently. I just had my college roommate staying at my house. And, you know, we're, of course, the same age, known each other our whole adult lives. He's been very successful and he lives in an enclave of very, very successful people.
[00:11:27.360 --> 00:11:38.360]   And and so we're familiar with this culture. And we were talking about American and he's from an immigrant family. So he's got a kind of broader perspective, I would argue, a broader perspective on like America. He's 54, as I am.
[00:11:38.360 --> 00:11:52.360]   And we were talking about how obviously this is not a democracy. It's not even a sort of decent facsimile of a democracy. It's to call it a democracy is like ridiculous, actually. But it's even worse than that.
[00:11:52.360 --> 00:12:21.360]   Our politics and not just our politics, but our public conversation reflects the very specific and parochial concerns of a tiny, tiny group of people, which is middle aged, affluent women who tend to be very angry and tend to mostly with their husbands, but probably for other reasons, too, and exercise this like wildly disproportionate power over what we can talk about and think about and the rules that the rest of us live by.
[00:12:21.360 --> 00:12:39.360]   It's just kind of amazing. And he happens to live in Jackson, Wyoming. So and I go there to ski and to fish and I have for a very long time. And I always say to him, I can't go anymore because I yelled at at lunch over my elk chili or in the lift line or whatever, or at the West Side Market.
[00:12:39.360 --> 00:12:54.360]   Literally a relative of mine yelled at me while buying bananas in the West Side Market. She lives there. And I'm thinking, what is it about this group of people that hates me so much when again, I know them really well. I'm related in some cases to them.
[00:12:54.360 --> 00:13:07.360]   So and I'm not quite sure, but I just I see our politics and our concerns, which if you take three steps back are like insanely picky. You like trans black lives matter.
[00:13:07.360 --> 00:13:21.360]   Well, I never said they didn't. But like if that's your main public policy objective to celebrate trans black lives in a country of 360 million people, it's got a lot of big problems. You are not seeing the whole picture.
[00:13:21.360 --> 00:13:37.360]   What is that? And what it is, again, is the disproportionate influence of a class of people and their neuroses. I wouldn't even say policy concerns, but like there are things they're worried about and their weird personal tics and like the result of years of therapy and SSRIs on their brains.
[00:13:37.360 --> 00:13:48.360]   Like that kind of controls our whole conversation. And my friend was saying, because he's really smart, he's like, yeah, but the good news is this can't last because it's just too stupid.
[00:13:48.360 --> 00:13:57.360]   And at some point very soon, the country is going to revert to the place that all countries begin, which is in a conversation about things that matter, like who comprises the population.
[00:13:57.360 --> 00:14:16.360]   Do we have enough water? Where are we on energy exactly? How are we going to manage these complex relationships with other countries? Like the things that, you know, the stuff of government, the stuff of resources, of course, resources, but like just the basic questions that should dominate the consciousness of any country and should dominate our public conversation.
[00:14:16.360 --> 00:14:24.360]   It's like our public obsessions are getting increasingly irrelevant, actually. Increasingly, it's like crazy.
[00:14:24.360 --> 00:14:25.360]   As our problems get bigger.
[00:14:25.360 --> 00:14:26.360]   Yeah, and the problem gets bigger.
[00:14:26.360 --> 00:14:29.360]   The conversation becomes more inane as the problems get bigger.
[00:14:29.360 --> 00:14:31.360]   Thank you. Exactly. That's exactly right.
[00:14:31.360 --> 00:14:57.360]   So when you look at that, that cohorts disproportionate impact, and then you translate it, for example, last night, there was, I guess, like a almost riot protest when folks were trying to light the Christmas tree in Rockefeller Center, I guess, and there was like a huge pro Hamas, I think, somebody check me out if this is wrong, protest and then people were pushing back on the cops and all this stuff. And all these folks are trying to do was just like the Christmas tree.
[00:14:57.360 --> 00:15:01.360]   Can you connect the dots? How does that cohort get to?
[00:15:01.360 --> 00:15:19.360]   No, but it's actually a branch from the same tree, Christmas tree in this case, because that whole conversation, well, I think it's interesting and I think it's geopolitically significant and I've been to those countries and I know people there, so I'm interested in it. And there's a lot to care about and be interested in.
[00:15:19.360 --> 00:15:40.360]   But the displacement of all of our public passions onto a conflict in a foreign country, however important that conflict may be, really kind of tells you a lot. It's like, in other words, yeah, I care what's happening between Israel and Hamas. I have views on it, which are probably pretty mainstream views, whatever. I don't have any very interesting views on it.
[00:15:40.360 --> 00:15:55.360]   But it's a little weird for your entire country, thousands of miles away, to be so preoccupied with that conflict that they miss big history changing events happening like in their own country. And I think that's, again, a species of the same problem.
[00:15:55.360 --> 00:16:10.360]   We are unable to, the problems that confront us are so big that we can't deal with them. And actually, my wife and I had this conversation the other night. I was for years a magazine writer and I have to file every Friday. I work for Bill Kristol at the Weekly Standard. And obviously, I regret that. But it was interesting at the time.
[00:16:10.360 --> 00:16:25.360]   And I have to file the story every week. And I'd stay up all night because I'm very lazy and I put stuff off. And I'd stay up every Thursday night, all night. And I had a call. It's going to the printing press in Pennsylvania. This was back when it was in print. And you've got to file it.
[00:16:25.360 --> 00:16:43.360]   And right around 8.30 a.m. on Friday morning, I would have this overwhelming urge to rearrange the books on my shelves in my library by title or by subject. I'd have all these weird kind of like librarian fantasies about rearranging books, which on a normal day, like, why would you do that? Better things to do.
[00:16:43.360 --> 00:17:06.360]   But as my problems mounted and I couldn't write the lead I wanted, I would transfer all my anxiety onto something I felt I could control or that was of lower stakes, less consequence. And I think that's kind of what we're seeing a little bit. We care more about foreign wars and trans lives as the obvious things that hold our society together start to fall apart.
[00:17:06.360 --> 00:17:11.360]   How do you even deal with that? I don't know if I'm being very articulate, but I think that's what's happening.
[00:17:11.360 --> 00:17:15.360]   It makes actually perfect sense. What do you put on the top of that list of things we should be focused on, Tucker?
[00:17:15.360 --> 00:17:18.360]   Oh, I mean, from my perspective, by far the most important...
[00:17:18.360 --> 00:17:22.360]   Yeah, if you rank one, two, three, this is what America needs to put its energy on.
[00:17:22.360 --> 00:17:36.360]   It's not even close. National cohesion, and by which I mean something specific. What does the majority of the country have in common with one another? Because look, the arc of the last century's American history is super, super interesting.
[00:17:36.360 --> 00:17:50.360]   So you have this massive influx of immigration, the Ellis Island generation, late 19th, early 20th century. And it's both good and bad. We only remember the good.
[00:17:50.360 --> 00:17:56.360]   But there was a lot of social volatility, like a lot. Like the mayor of Chicago got shot in his house. There was bombings on Wall Street.
[00:17:56.360 --> 00:18:02.360]   Like the whole, the Wobblies, the anarchists, like the foot soldiers that were immigrants, working class European immigrants.
[00:18:02.360 --> 00:18:15.360]   And part of the problem was there was a lot of immigrants and I mean, Sacco and Vanzetti, you know, who shot the clerk in, was it Brockton, Mass? Anyway, it was in Massachusetts, Boston.
[00:18:15.360 --> 00:18:26.360]   They'd been in the country for just a few years and they immediately got sucked into radical politics. Well, why was that? Well, because they weren't kind of bought in or rooted in or hadn't been fully assimilated into American society.
[00:18:26.360 --> 00:18:35.360]   So then you have the First World War and we basically shut down immigration and we have this period of settling where like all Americans, let's think through our civic religion, what ties us together.
[00:18:35.360 --> 00:18:42.360]   And then that leads into October of 29 and you do have this national crisis that lasts for more than a decade and we didn't blow up.
[00:18:42.360 --> 00:18:55.360]   And we had a successful, you know, the CCC. We like had these big programs, which I'll say this is a conservative, kind of worked in keeping people fed and focused, gave them purpose, kept the country from collapsing during the Great Depression.
[00:18:55.360 --> 00:19:05.360]   If that happened now, when there is no broad agreement on what it means to be an American, no agreement at all on what we all have in common, I don't think we can withstand it, actually. I really don't.
[00:19:05.360 --> 00:19:12.360]   And I'm not even convinced it matters as much what that civic religion is as it does that we have one.
[00:19:12.360 --> 00:19:23.360]   If we don't have anything that ties us together, when that day comes, and you know what I mean, when the economic crisis comes, because it's coming, like what's that going to look like?
[00:19:23.360 --> 00:19:28.360]   It's going to be very scary. And so that's what I'm most worried about by far.
[00:19:28.360 --> 00:19:36.360]   How do you define national cohesion? I guess maybe then like what are the few elements that you think America needs to agree on as a majority?
[00:19:36.360 --> 00:19:40.360]   Wake me up from a deep sleep and ask me what it means to be an American.
[00:19:40.360 --> 00:19:47.360]   And then do the same to 1000 other people and we'll just do a survey on it. And do the majority of those people give the same answer.
[00:19:47.360 --> 00:19:54.360]   And if they don't, like so this is, I mean, I have so many theories, I'm going to control myself.
[00:19:54.360 --> 00:20:00.360]   But I would just say one other thing, which is we overestimate people's ability to metabolize change.
[00:20:00.360 --> 00:20:07.360]   I'm not like a strict Darwinist or anything, but I do sort of believe that over time people adapt to their environment.
[00:20:07.360 --> 00:20:14.360]   And that in us is, most of our abilities are inborn. Like that's just true. Sorry. I have dogs. I know that.
[00:20:14.360 --> 00:20:23.360]   And people just haven't, you know, the world didn't change that much from, you know, 460 or whatever, the sacking of Rome until the Renaissance.
[00:20:23.360 --> 00:20:29.360]   It just didn't. And then the Industrial Revolution to now has been like such massive change that is driving people insane.
[00:20:29.360 --> 00:20:33.360]   People can't handle relentless change. And technologists love relentless change.
[00:20:33.360 --> 00:20:38.360]   And I like a lot of technologists, a couple of really good friends of mine, but they're anomalies.
[00:20:38.360 --> 00:20:43.360]   A lot of them are literally autistic. I'm not attacking them. I'm praising them, but they're different from most people.
[00:20:43.360 --> 00:20:47.360]   -Sachs is on the program right now, Tucker. -Right. You know, like they can handle it.
[00:20:47.360 --> 00:20:52.360]   They can like, they want a world that's totally different from today's world tomorrow. They want that.
[00:20:52.360 --> 00:20:55.360]   -They thrive in that flux. -They do.
[00:20:55.360 --> 00:20:59.360]   -Right. -But they don't understand how rare they are.
[00:20:59.360 --> 00:21:08.360]   And if you impose that on a society and you don't ever have a period where you pause and let things settle, you will blow up that society.
[00:21:08.360 --> 00:21:14.360]   And the Chinese, who I don't admire for many things at all, know this. And I admire that about them.
[00:21:14.360 --> 00:21:19.360]   They think that change for its own sake is dangerous. Now, they think it's dangerous to their power, and they're absolutely right.
[00:21:19.360 --> 00:21:23.360]   But it's also dangerous just to like the idea of a society. What is a society?
[00:21:23.360 --> 00:21:28.360]   And so, anyway, I could go on and on and on. But that's my main concern.
[00:21:28.360 --> 00:21:31.360]   Let me propose a theory and see if you react to it.
[00:21:31.360 --> 00:21:38.360]   I think that there's a big orientation in society where it's constantly about trying to shift the power dynamics.
[00:21:38.360 --> 00:21:42.360]   That everyone always feels like they don't have something that someone else has.
[00:21:42.360 --> 00:21:50.360]   And that other person that has it, for some reason, is advantaged in a way that to them always feels unfair.
[00:21:50.360 --> 00:21:53.360]   -And everyone feels this way. Everyone. -That's right.
[00:21:53.360 --> 00:21:57.360]   You're at the top of the food chain. You feel like someone else has something you don't have.
[00:21:57.360 --> 00:22:01.360]   And it's unfair, and the system is set up that way, no matter where you sit.
[00:22:01.360 --> 00:22:07.360]   And as a result of that... And by the way, this goes back... I get waxed super philosophical on this stuff.
[00:22:07.360 --> 00:22:16.360]   But Buddhism's always had this point that the desire that humans have is the one thing that causes all of societal suffering, behavior, everything.
[00:22:16.360 --> 00:22:20.360]   But it all comes down to this. It's that you see that someone else has something, you desire it,
[00:22:20.360 --> 00:22:27.360]   and then you want to change the dynamic of how the system, how society is organized to fix that.
[00:22:27.360 --> 00:22:34.360]   The thing about technology, if you think back, the catapult gave an advantage to an army that allowed them to win in a war.
[00:22:34.360 --> 00:22:36.360]   So did the rifle, so did the nuclear bomb.
[00:22:36.360 --> 00:22:42.360]   That ultimately technology was the enabling force that allowed a rapid shift in power dynamics.
[00:22:42.360 --> 00:22:48.360]   And all technology ultimately creates leverage for the creator of the technology initially, and then it diffuses to everyone.
[00:22:48.360 --> 00:22:56.360]   But that is perhaps why... This is a theory I have, which is that there's generally pessimism towards technology,
[00:22:56.360 --> 00:23:04.360]   because it creates unfair advantages that shift the power dynamic too quickly and too advantageously.
[00:23:04.360 --> 00:23:08.360]   And it's also why I think you don't hear a lot of support for technology from most of society,
[00:23:08.360 --> 00:23:11.360]   because technology creators are such a small percentage of society.
[00:23:11.360 --> 00:23:15.360]   So I don't know if that resonates with your point, but for me it's always been like...
[00:23:15.360 --> 00:23:20.360]   Well, I think that's really smart, and I think it's indisputably true. Absolutely.
[00:23:20.360 --> 00:23:27.360]   I will say you're making an argument, in addition to many other things, against tribalisms,
[00:23:27.360 --> 00:23:35.360]   because tribalism accentuates these preexisting impulses in people, which again, like all impulses, are just inborn.
[00:23:35.360 --> 00:23:39.360]   Human nature is immutable, so let's just start there. It cannot be changed.
[00:23:39.360 --> 00:23:43.360]   AI will not change human nature. It will change everything but human nature.
[00:23:43.360 --> 00:23:52.360]   And envy and the fact that prosperity is a relative measure above starvation...
[00:23:52.360 --> 00:23:53.360]   Totally.
[00:23:53.360 --> 00:24:02.360]   So it's like there's a famous Russian proverb like, "I've got a cow, my neighbor has a cow, he gets a cow, now he has two cows, I kill his cow."
[00:24:02.360 --> 00:24:04.360]   Right. Totally.
[00:24:04.360 --> 00:24:15.360]   But tribalism, especially in a country like ours, which as noted doesn't have a working majority of people with obvious connection to each other,
[00:24:15.360 --> 00:24:22.360]   it really, really exacerbates that a lot, because it makes it more obvious.
[00:24:22.360 --> 00:24:27.360]   So all of a sudden it's not just like, "Hey, the smart kids are richer, they have two cows."
[00:24:27.360 --> 00:24:31.360]   It's like, "The Jews are richer, or the Indians are richer, or the whites are richer," or whatever.
[00:24:31.360 --> 00:24:41.360]   It's like, just name the group. And once you go down that road, that leads to violence, and mass violence, actually.
[00:24:41.360 --> 00:24:48.360]   A lot of people listening right now, Tucker, are probably a little bit confused hearing you talk about cohesion of the country,
[00:24:48.360 --> 00:24:56.360]   when most people would look at the tribalism as almost most manifested in cable news.
[00:24:56.360 --> 00:25:02.360]   You and Rachel Maddow every night were number one and number two in the ratings, taking either side of the tribe.
[00:25:02.360 --> 00:25:08.360]   So is this something that's evolved, and how did your time in cable news inform you to this?
[00:25:08.360 --> 00:25:14.360]   Because most people would say, "Wait, isn't Tucker and Rachel, aren't those the two tribe leaders?"
[00:25:14.360 --> 00:25:18.360]   I think there are dumb people who think that. But no, here's the difference.
[00:25:18.360 --> 00:25:20.360]   My views...
[00:25:20.360 --> 00:25:22.360]   Wait, are you talking about me?
[00:25:22.360 --> 00:25:26.360]   I'm not including you in that. I'm not including you. No, no, no, no.
[00:25:26.360 --> 00:25:28.360]   If the two fits.
[00:25:28.360 --> 00:25:31.360]   I'm asking you half of the audience to be clear here.
[00:25:31.360 --> 00:25:33.360]   I have a second thing to say. I'm sorry.
[00:25:33.360 --> 00:25:38.360]   It's all good. It's all good, Tucker. Why should you be any different than the other panelists?
[00:25:38.360 --> 00:25:46.360]   No, I love, and not only do I love, I don't always love, but I certainly think that reason disagreement is essential.
[00:25:46.360 --> 00:25:49.360]   It's the alternative to violence, by the way. That's what that is.
[00:25:49.360 --> 00:25:52.360]   Debate and politics are the alternative to violence. You get two choices.
[00:25:52.360 --> 00:25:56.360]   We're going to do it by arguing about it, or we're going to do it by force.
[00:25:56.360 --> 00:26:03.360]   So I absolutely think it's essential to debate things, because if we don't, then we just have to shoot each other, and I'm against that.
[00:26:03.360 --> 00:26:12.360]   But that's very different from tribalism, because tribalism is based on things that you can't change.
[00:26:12.360 --> 00:26:18.360]   So, for example, in 2002, I hosted a show on CNN, and it was the run-up to the war in Iraq.
[00:26:18.360 --> 00:26:24.360]   I was uncomfortable with the idea that we would be invading Iraq to respond to an attack on America that had nothing to do with Iraq.
[00:26:24.360 --> 00:26:32.360]   I did think that didn't make sense, but I was sold on the idea finally by someone, and I foolishly parroted the Bush administration's line on that.
[00:26:32.360 --> 00:26:35.360]   So I was a pro-war person for a time.
[00:26:35.360 --> 00:26:38.360]   Then I went to Iraq, and I changed my views completely.
[00:26:38.360 --> 00:26:45.360]   So I switched sides completely, and I wound up on the other side, anti-war in Iraq, and that took about a week.
[00:26:45.360 --> 00:26:48.360]   I was born a white man.
[00:26:48.360 --> 00:26:49.360]   You were born whatever you are.
[00:26:49.360 --> 00:26:50.360]   Everyone's born whatever they are.
[00:26:50.360 --> 00:26:51.360]   That can't change.
[00:26:51.360 --> 00:26:59.360]   So if we stoke division along lines that can't be changed, then we're really at a cul-de-sac.
[00:26:59.360 --> 00:27:01.360]   Our differences can never be resolved.
[00:27:01.360 --> 00:27:02.360]   You will always be what you are.
[00:27:02.360 --> 00:27:05.360]   You can't change it, and the same for me.
[00:27:05.360 --> 00:27:12.360]   And so those are the things that wind up becoming generational conflicts, civil wars, Rwanda.
[00:27:12.360 --> 00:27:17.360]   And so I've always been against that my entire life, and my entire time on Fox I argued against that.
[00:27:17.360 --> 00:27:30.360]   I think affirmative action is completely not only immoral because it's an insult to the idea of America, which is like the people who do the best get the most, but it is also a recipe over time for violence.
[00:27:30.360 --> 00:27:31.360]   And I made that case.
[00:27:31.360 --> 00:27:37.360]   I've made that case every single day since David and I had lunch 30 years ago.
[00:27:37.360 --> 00:27:38.360]   I've never not thought that.
[00:27:38.360 --> 00:27:51.360]   I've always had the Dr. Seuss view of race relations, which is we should de-emphasize race and elevate merit, achievement, hard work, character, the things that we can control.
[00:27:51.360 --> 00:27:56.360]   But the other side has gone – and by the way, liberals used to agree with me.
[00:27:56.360 --> 00:28:01.360]   That used to be a liberal position within my lifetime, and then all of a sudden it became a Nazi position.
[00:28:01.360 --> 00:28:14.360]   And that's just a manipulation of language, but the truth is on the race stuff, which is what matters over time, if you convince people to hate others on the basis of their race, you've committed a massive sin and you've done a lot to wreck our country.
[00:28:14.360 --> 00:28:15.360]   That's coming from the left.
[00:28:15.360 --> 00:28:16.360]   I'm sorry.
[00:28:16.360 --> 00:28:17.360]   That's just true.
[00:28:17.360 --> 00:28:29.360]   Just to connect these ideas, I think that is a major source of our division is one of the reasons why people have a hard time saying what it means to be an American is because there are dueling visions of what it means to be an American.
[00:28:29.360 --> 00:28:33.360]   The left has been trying to change or rewrite American history.
[00:28:33.360 --> 00:28:40.360]   So, for example, the key year in American history, the founding of the nation was not 1776 with the Declaration of Independence.
[00:28:40.360 --> 00:28:42.360]   It was 1619.
[00:28:42.360 --> 00:28:50.360]   This is the base of the whole 1619 project of the New York Times is that the founding of the nation began with the importation of slavery to the New World.
[00:28:50.360 --> 00:28:52.360]   That was the key year.
[00:28:52.360 --> 00:29:00.360]   And so I think there's been an effort for a long time on the part of the left to rewrite what it means to be an American and to rewrite American history.
[00:29:00.360 --> 00:29:02.360]   It's actually in a weird way.
[00:29:02.360 --> 00:29:10.360]   It's the opposite of what Deng Xiaoping did in China, where Deng basically he flipped the doctrine over there from communism to capitalism.
[00:29:10.360 --> 00:29:12.360]   He didn't outright declare it.
[00:29:12.360 --> 00:29:18.360]   He said that it doesn't matter whether a cat is white or black as long as it catches mice.
[00:29:18.360 --> 00:29:19.360]   And they didn't change the name.
[00:29:19.360 --> 00:29:21.360]   They didn't change the party.
[00:29:21.360 --> 00:29:22.360]   It's still the Chinese Communist Party.
[00:29:22.360 --> 00:29:24.360]   They didn't change the flag.
[00:29:24.360 --> 00:29:27.360]   But he did a hot swap of the back end to capitalism.
[00:29:27.360 --> 00:29:31.360]   And they changed what the country was about in that.
[00:29:31.360 --> 00:29:33.360]   It worked out great for them.
[00:29:33.360 --> 00:29:37.360]   I think in a similar way in the United States, they haven't changed the flag.
[00:29:37.360 --> 00:29:38.360]   They haven't changed the name.
[00:29:38.360 --> 00:29:43.360]   But there is an attempt to hot swap the back end of what it means to be an American.
[00:29:43.360 --> 00:29:45.360]   And I think this is the root of the conflict.
[00:29:45.360 --> 00:29:47.360]   I mean I agree with that completely.
[00:29:47.360 --> 00:29:57.360]   And I don't think we've thought through what it means to change out of many one to out of one many.
[00:29:57.360 --> 00:30:04.360]   I think that's – we've set ourselves up for something really, really scary, again for violent conflict.
[00:30:04.360 --> 00:30:07.360]   Because I don't see how that resolves itself.
[00:30:07.360 --> 00:30:13.360]   I mean why is people understand this in a diverse society, pluralist society, whatever you call it,
[00:30:13.360 --> 00:30:17.360]   a society where you've got a lot of different people, a lot of different backgrounds and shades and religions and all this stuff.
[00:30:17.360 --> 00:30:25.360]   You need to de-emphasize the things that divide us inherently and emphasize the things that unite us.
[00:30:25.360 --> 00:30:26.360]   That's not hard.
[00:30:26.360 --> 00:30:31.360]   And it's also so obvious that if you're not doing it, my first question is why are you not doing that?
[00:30:31.360 --> 00:30:33.360]   I mean why are you not?
[00:30:33.360 --> 00:30:39.360]   And that kind of explains the true loathing that I have for the people in charge on both sides.
[00:30:39.360 --> 00:30:47.360]   Because like if you did that in your household with your children, your kids would be in rehab or jail or dead.
[00:30:47.360 --> 00:30:48.360]   It's that obvious.
[00:30:48.360 --> 00:30:50.360]   Like you would never do that to your kids.
[00:30:50.360 --> 00:30:52.360]   So why would you do that to your population?
[00:30:52.360 --> 00:30:53.360]   Seriously.
[00:30:53.360 --> 00:30:54.360]   Freeberg, you had a question?
[00:30:54.360 --> 00:30:56.360]   Well, where does responsibility lie there, Tucker?
[00:30:56.360 --> 00:30:58.360]   What's the mechanism for doing that?
[00:30:58.360 --> 00:31:04.360]   We often, and I find in conversations, everyone says the government should.
[00:31:04.360 --> 00:31:17.360]   And I often question why the government should anything in my life, in my social settings, in how I live, how I do business.
[00:31:17.360 --> 00:31:26.360]   Does the government, the federal government in the United States, have a role and responsibility to do what you're suggesting we need to do in the United States?
[00:31:26.360 --> 00:31:35.360]   Or is it the media, or is it social leaders, or is it business leaders, or is it local governments?
[00:31:35.360 --> 00:31:42.360]   Who is responsible ultimately for creating the social cohesion necessary for the U.S. to enter a new era of prosperity?
[00:31:42.360 --> 00:31:46.360]   And why is that the right group to be responsible?
[00:31:46.360 --> 00:31:51.360]   Well, look, I would say, well, it's a great question.
[00:31:51.360 --> 00:31:53.360]   And I would say a new era of prosperity.
[00:31:53.360 --> 00:31:55.360]   I don't think that's really the goal.
[00:31:55.360 --> 00:31:56.360]   We have prosperity.
[00:31:56.360 --> 00:31:59.360]   And I think it's a real question as to whether people want prosperity.
[00:31:59.360 --> 00:32:00.360]   Social cohesion.
[00:32:00.360 --> 00:32:01.360]   Right.
[00:32:01.360 --> 00:32:05.360]   Do people want, I mean, I think there's actually a lot of evidence that human beings sort of hate prosperity, actually.
[00:32:05.360 --> 00:32:08.360]   We're not designed to be prosperous.
[00:32:08.360 --> 00:32:13.360]   And the second we are, we invent climate crises to make us less prosperous.
[00:32:13.360 --> 00:32:14.360]   I mean, that's really what that is, right?
[00:32:14.360 --> 00:32:22.360]   But anyway, leaving that aside, it's incumbent on all of us, anyone with authority, anyone with a voice, anyone with money.
[00:32:22.360 --> 00:32:24.360]   And it's really kind of as simple.
[00:32:24.360 --> 00:32:28.360]   And I do think, you know, government is kind of the last to fall in line.
[00:32:28.360 --> 00:32:31.360]   It's really as simple as making things socially unacceptable.
[00:32:31.360 --> 00:32:34.360]   We certainly see. I grew up in a world where people smoked on airplanes.
[00:32:34.360 --> 00:32:41.360]   OK, now that was banned by the FAA, but it was also made totally socially unacceptable.
[00:32:41.360 --> 00:32:43.360]   You couldn't light a cigarette in someone's kitchen.
[00:32:43.360 --> 00:32:49.360]   You just wouldn't think to do that in the same way that you wouldn't think to give the middle finger to an old lady, which is not actually illegal.
[00:32:49.360 --> 00:32:55.360]   But nobody would do that because it's so appalling in the minds of everybody and everyone's happy to say, what the hell are you doing?
[00:32:55.360 --> 00:33:04.360]   And I feel the same way about the race stuff. It's like, oh, you know, race this race that someone should say, whoa, whoa, whoa, whoa, whoa. Stop that.
[00:33:04.360 --> 00:33:07.360]   There's part of what you're saying, which I think is.
[00:33:07.360 --> 00:33:30.360]   Which I really agree with, which is this idea that we have gone as the human race, OK, from having to make major adaptations over the arc of our evolution as a species to now what you're kind of saying is we're now dealing with all these minor adaptations and it kind of breaks the entire circuitry of the brain, which is like before we had to fight to evade the animals to feed ourselves.
[00:33:30.360 --> 00:33:37.360]   To build the machine to all of a sudden go from an agrarian to an industrial. These were huge adaptations.
[00:33:37.360 --> 00:33:41.360]   And now we have all of this almost surplus and excess.
[00:33:41.360 --> 00:33:55.360]   And it's a little bit of a head scratcher for a lot of people because now all the adaptations that are left are very minor in shape because all of these other things that would other occupy your time, sustenance, survival, resiliency are taken off the table.
[00:33:55.360 --> 00:34:04.360]   I get that the other part of what you say, which is very Rene Girard, which is like, hey, there's all this copying of people and desire.
[00:34:04.360 --> 00:34:09.360]   There's going to be violence if we don't figure out how to.
[00:34:09.360 --> 00:34:12.360]   Decharge it.
[00:34:12.360 --> 00:34:17.360]   What do you think we need to do in order to do that discharge?
[00:34:17.360 --> 00:34:22.360]   How do you get these people to stop focusing on the small order bits?
[00:34:22.360 --> 00:34:29.360]   And how do we reorganize people to focus on the big order bits so that we minimize this risk of violence?
[00:34:29.360 --> 00:34:35.360]   We minimize the one group of immutable traits fighting another group of immutable traits.
[00:34:35.360 --> 00:34:36.360]   How does that happen?
[00:34:36.360 --> 00:34:42.360]   I'm pretty pessimistic about a country this big.
[00:34:42.360 --> 00:34:43.360]   We say the country.
[00:34:43.360 --> 00:34:48.360]   I mean, when I talk about the country, I'm talking about people I know or grew up with or people who speak my language.
[00:34:48.360 --> 00:34:54.360]   I mean, the country is so big that something can happen in New Mexico, something big can happen in New Mexico or San Francisco for that matter.
[00:34:54.360 --> 00:34:55.360]   And people don't even know.
[00:34:55.360 --> 00:35:04.360]   But in general, I would say it's hard to see change in course before we're forced to.
[00:35:04.360 --> 00:35:10.360]   And so I do think you think basically there has to be a moment of.
[00:35:10.360 --> 00:35:17.360]   Something that's so egregious that causes a national reconciliation of some kind, essentially, that says, hold on a second.
[00:35:17.360 --> 00:35:21.360]   You want to know what I really think, which is like kind of crackpot, but I know that it's right.
[00:35:21.360 --> 00:35:24.360]   Yes, I do think the problem is, is prosperity.
[00:35:24.360 --> 00:35:32.360]   And I've noticed this as a middle aged man, as I've gotten older and I know people who've been successful, in some cases, very successful.
[00:35:32.360 --> 00:35:37.360]   And I've noticed that when they succeed, when they get everything they want, they destroy themselves.
[00:35:37.360 --> 00:35:39.360]   I've noticed this again and again and again.
[00:35:39.360 --> 00:35:41.360]   It's you are the dog who caught the car.
[00:35:41.360 --> 00:35:44.360]   And it's and actually it's more than just having idle time.
[00:35:44.360 --> 00:35:46.360]   There's something there's a metaphysical quality.
[00:35:46.360 --> 00:35:51.360]   There's there's there are factors here that I don't understand that are deeper, actually.
[00:35:51.360 --> 00:35:52.360]   But I just notice it.
[00:35:52.360 --> 00:36:00.360]   There's something about affluence that over time convinces people to kill themselves.
[00:36:00.360 --> 00:36:07.360]   And you see it like in a literal sense in the euthanasia numbers out of Canada and out of Western Europe.
[00:36:07.360 --> 00:36:11.360]   You see in a literal sense when you look at the incidence of diabetes as correlated to GDP.
[00:36:11.360 --> 00:36:12.360]   That's right. That's exactly right.
[00:36:12.360 --> 00:36:13.360]   It's exactly right.
[00:36:13.360 --> 00:36:25.360]   You look at these emerging economies and as GDP cranks, the first thing that happens is heart disease cranks up and diabetes cranks up because to your point, Tucker, they start to enjoy the prosperity of their earned life.
[00:36:25.360 --> 00:36:26.360]   But they're not enjoying it.
[00:36:26.360 --> 00:36:27.360]   That's the thing.
[00:36:27.360 --> 00:36:31.360]   The other thing you see is they stop having children, which is a form of suicide.
[00:36:31.360 --> 00:36:32.360]   You're not reproducing.
[00:36:32.360 --> 00:36:40.360]   You know, I mean, there's no other culture who has left a written record in history that didn't see having children is like the primary.
[00:36:40.360 --> 00:36:44.360]   First of all, the primary source of wealth and not because they worked on your farm, but because they exist.
[00:36:44.360 --> 00:36:46.360]   And so do your genes.
[00:36:46.360 --> 00:36:51.360]   So if you start to see every it's not just a matter of culture.
[00:36:51.360 --> 00:36:53.360]   It happens in non-Christian Japan.
[00:36:53.360 --> 00:36:55.360]   It happens in post-Christian Spain.
[00:36:55.360 --> 00:37:04.360]   It happens in South Korea with a point seven replacement, you know, reproduction rate point seven.
[00:37:04.360 --> 00:37:06.360]   So what is that?
[00:37:06.360 --> 00:37:08.360]   And again, it's the same thing.
[00:37:08.360 --> 00:37:10.360]   It's that affluence kills you.
[00:37:10.360 --> 00:37:13.360]   And if you don't believe it, fast for three days.
[00:37:13.360 --> 00:37:17.360]   Fasting, by the way, is a feature not just of, you know, Christian history.
[00:37:17.360 --> 00:37:25.360]   Every religion and every culture that I'm aware of has acknowledged fasting as an important religious ritual.
[00:37:25.360 --> 00:37:26.360]   Why is that?
[00:37:26.360 --> 00:37:29.360]   Why would foregoing food be so important?
[00:37:29.360 --> 00:37:34.360]   Try it for three days and experience, you know, you're quivering like a tuning fork.
[00:37:34.360 --> 00:37:36.360]   Your sensitivity just explodes.
[00:37:36.360 --> 00:37:42.360]   There's something about eating too much, literally satisfying the most basic need of all, which is for calories.
[00:37:42.360 --> 00:37:44.360]   There's something about that that kills you.
[00:37:44.360 --> 00:37:47.360]   And there's something about foregoing it that enlightens you.
[00:37:47.360 --> 00:37:48.360]   It's like super interesting.
[00:37:48.360 --> 00:37:50.360]   So I look, there's so much I don't know.
[00:37:50.360 --> 00:37:52.360]   I'm the last person you want to listen to.
[00:37:52.360 --> 00:37:54.360]   These are deeper waters that I'm qualified to wade in.
[00:37:54.360 --> 00:37:59.360]   But my senses tell me very, very strongly that the core problem is having too much.
[00:37:59.360 --> 00:38:01.360]   And I'm not calling for taking things away.
[00:38:01.360 --> 00:38:04.360]   I think the global warming bullshit is bullshit.
[00:38:04.360 --> 00:38:07.360]   OK, I mean, obviously it is climate crisis.
[00:38:07.360 --> 00:38:10.360]   It's propaganda, but it's coming out of it's also sincere to some extent.
[00:38:10.360 --> 00:38:13.360]   It's not just a power grab by Davos.
[00:38:13.360 --> 00:38:14.360]   It's more than that.
[00:38:14.360 --> 00:38:19.360]   It's people's gut level sense that we need to have less because this is killing us.
[00:38:19.360 --> 00:38:20.360]   And that is real.
[00:38:20.360 --> 00:38:24.360]   Freiburg is, Tucker said global warming is bullshit.
[00:38:24.360 --> 00:38:26.360]   I'm just going to pull up an image for him.
[00:38:26.360 --> 00:38:28.360]   I just want his reaction to it.
[00:38:28.360 --> 00:38:29.360]   Freiburg?
[00:38:29.360 --> 00:38:30.360]   You just said that to the Sultan of Science.
[00:38:30.360 --> 00:38:31.360]   Let's see.
[00:38:31.360 --> 00:38:42.360]   No, no, but Tucker has like, no, no, I think it's like just on that topic, you know, data shows temperatures are getting warmer off of a baseline of 500 years and now things get warmer.
[00:38:42.360 --> 00:38:43.360]   Oh, no, no, I'm so sorry.
[00:38:43.360 --> 00:38:45.360]   Can I just make a small correction?
[00:38:45.360 --> 00:38:47.360]   No, I'm not saying it's not getting warmer.
[00:38:47.360 --> 00:38:48.360]   It seems to be getting warmer.
[00:38:48.360 --> 00:39:00.360]   I'm really saying this whole elaborate theory that humans are causing this rise in temperatures and we know how to reverse it through, say, shutting down, you know, the use of hydrocarbons.
[00:39:00.360 --> 00:39:07.360]   Yeah, I mean, and even if that were true, what we're doing to affect those changes shows how fraudulent the whole thing is.
[00:39:07.360 --> 00:39:08.360]   I mean, you know, the whole arguments.
[00:39:08.360 --> 00:39:10.360]   But I'm not actually calling into question that it's changing.
[00:39:10.360 --> 00:39:11.360]   Temperatures always change.
[00:39:11.360 --> 00:39:14.360]   I live in Maine, which was shaped by the glaciers 10,000 years ago.
[00:39:14.360 --> 00:39:16.360]   So, like, yeah, we've had some climate change.
[00:39:16.360 --> 00:39:20.360]   Do you think that human civilization is not threatened by the changing climate?
[00:39:20.360 --> 00:39:22.360]   I just, this is a quick little point, but.
[00:39:22.360 --> 00:39:24.360]   Do I think, well, sure.
[00:39:24.360 --> 00:39:27.360]   I mean, it's threatened in some ways, probably help in other ways.
[00:39:27.360 --> 00:39:28.360]   I personally hate hot weather.
[00:39:28.360 --> 00:39:31.360]   So, you know, I don't care for it.
[00:39:31.360 --> 00:39:39.360]   I think there's quite a bit of evidence that previous, that actually temperature changes can be much more radical than we think.
[00:39:39.360 --> 00:39:41.360]   And I'll just give you one example that no one's ever explained.
[00:39:41.360 --> 00:39:52.360]   But you're aware that there are all these woolly mammoths found in Kastan Ice in Siberia and the Soviet, you know, early Soviet 1920s expedition to map all of Russia found these things.
[00:39:52.360 --> 00:39:55.360]   And at least one stranded expedition ate them.
[00:39:55.360 --> 00:40:02.360]   OK, they ate the woolly mammoth because it had been frozen so thoroughly that after thousands of years, the meat was still edible.
[00:40:02.360 --> 00:40:05.360]   Yeah. Well, the question is, like, how did that happen?
[00:40:05.360 --> 00:40:09.360]   And I think it was the guy who ran Birdseye Frozen Food, who was an expert on freezing meat.
[00:40:09.360 --> 00:40:12.360]   It's like, wait a second, that weighs X thousand pounds.
[00:40:12.360 --> 00:40:15.360]   It would have to be frozen extremely quickly.
[00:40:15.360 --> 00:40:17.360]   And I know this from hunting, too.
[00:40:17.360 --> 00:40:21.360]   You have to freeze meat very fast to keep it from spoiling.
[00:40:21.360 --> 00:40:22.360]   From the bacteria.
[00:40:22.360 --> 00:40:23.360]   How did that happen?
[00:40:23.360 --> 00:40:26.360]   Well, no, no, for real, though, because it decomposes very, a big animal.
[00:40:26.360 --> 00:40:35.360]   If you shoot, say, a moose or a big deer or a mule deer, you have to open it up immediately or the meat will spoil because the bigger it is, the more heat it generates.
[00:40:35.360 --> 00:40:45.360]   Right. We know that it's like, OK, so how exactly did hundreds or thousands of large animals get frozen so quickly that the meat is still edible thousands of years later?
[00:40:45.360 --> 00:40:47.360]   That's an honest question that no one's ever answered.
[00:40:47.360 --> 00:40:54.360]   And the only potential or even plausible answer is temperature changed so fast it flash froze them.
[00:40:54.360 --> 00:40:56.360]   Well, how did that happen? Like what?
[00:40:56.360 --> 00:40:58.360]   No, these are sincere questions.
[00:40:58.360 --> 00:41:01.360]   Interesting. I'll look into it. I'll get back to you. I've not heard about this.
[00:41:01.360 --> 00:41:03.360]   No, no, but like, what is that?
[00:41:03.360 --> 00:41:05.360]   Flash frozen food. That's interesting.
[00:41:05.360 --> 00:41:07.360]   This reminds me of that Marlon Brando film.
[00:41:07.360 --> 00:41:11.360]   You remember where he like goes to an island and eats the like super rare food?
[00:41:11.360 --> 00:41:13.360]   It's like a Komodo dragon or something.
[00:41:13.360 --> 00:41:16.360]   The graduates, Matthew Broderick's in it. So good.
[00:41:16.360 --> 00:41:21.360]   It's like the expedition to Siberia to eat the flash frozen woolly mammoths and make a good sequel.
[00:41:21.360 --> 00:41:23.360]   But it's interesting. I'm going to look into it.
[00:41:23.360 --> 00:41:25.360]   I don't know the answer to how and why the woolly mammoths froze so fast.
[00:41:25.360 --> 00:41:31.360]   But the point is, the climate, here's the point, that we have physical evidence that the climate on Earth,
[00:41:31.360 --> 00:41:36.360]   within, you know, inside our atmosphere has changed dramatically, so dramatically that it would, you know,
[00:41:36.360 --> 00:41:44.360]   kill every person who was affected by it like a number of times that we know of over the course of the history of the Earth.
[00:41:44.360 --> 00:41:50.360]   So like, I mean, I think there could, you know, we could be in for actual climate change that killed everybody.
[00:41:50.360 --> 00:41:55.360]   That's entirely possible. But the idea that it's the fault of the United States because of, you know,
[00:41:55.360 --> 00:41:59.360]   you drive an F-150 is like so absurd. It's like hard to believe anyone takes that seriously.
[00:41:59.360 --> 00:42:01.360]   There's like no evidence of that, actually. Sorry.
[00:42:01.360 --> 00:42:05.360]   Yeah, I'm generally just an optimist about solutions anyway.
[00:42:05.360 --> 00:42:08.360]   But, you know, I think human ingenuity has always prevailed.
[00:42:08.360 --> 00:42:15.360]   So all this pessimism and catastrophizing, I do disagree with respect to like,
[00:42:15.360 --> 00:42:20.360]   there is science that indicates that there are elements that were kind of affecting things in an adverse way,
[00:42:20.360 --> 00:42:25.360]   a challenging way. But I'm not concerned about the challenges just because of where human technology sits
[00:42:25.360 --> 00:42:31.360]   and the things we can do. So I'm, I have a slightly different point of view, but I hear you on it.
[00:42:31.360 --> 00:42:33.360]   I don't want to debate the point. Sorry, Chamath, go ahead.
[00:42:33.360 --> 00:42:35.360]   Yeah, that's a whole nother thing.
[00:42:35.360 --> 00:42:39.360]   I just want to up-level this back to something because I think if I had to summarize what you're saying is,
[00:42:39.360 --> 00:42:45.360]   you're saying, Chamath, our society is too prosperous. And as a result, that prosperity creates
[00:42:45.360 --> 00:42:51.360]   small fringe issues that dominate, which then creates an inability for us to be cohesive.
[00:42:51.360 --> 00:42:56.360]   So then if we have something that allows us to be forced to be more resilient,
[00:42:56.360 --> 00:43:01.360]   we will actually wake up from this fever and say, hold on, guys, what are the big issues?
[00:43:01.360 --> 00:43:05.360]   Let's really figure out what matters. Let's get organized and let's go after those.
[00:43:05.360 --> 00:43:09.360]   And that both is a healing of the country, but it's almost like our insurance policy.
[00:43:09.360 --> 00:43:14.360]   Is that kind of a, is that a fair summary? If I had to put what you're saying in a box or is that?
[00:43:14.360 --> 00:43:21.360]   Well, I would, I would hate ever to seem like I'm looking forward to catastrophe because I'm not.
[00:43:21.360 --> 00:43:23.360]   And I hope it doesn't unfold that way.
[00:43:23.360 --> 00:43:25.360]   No, no, no, I don't think you're saying that.
[00:43:25.360 --> 00:43:29.360]   No, I mean, I used to drink too much. And I know a lot of people used to drink too much and, you know,
[00:43:29.360 --> 00:43:33.360]   use drugs or whatever. And some of them didn't get better until they went to jail, you know,
[00:43:33.360 --> 00:43:35.360]   which is what people who drink too much tend to do over time.
[00:43:35.360 --> 00:43:40.360]   That never happened to me. And I'm so grateful for it. Like, you don't need to, you know, get to the bottom.
[00:43:40.360 --> 00:43:41.360]   You don't need to get there. Right.
[00:43:41.360 --> 00:43:46.360]   You don't. I don't think that you do. I'm just, I'm just really worried that we're not even having a conversation about this.
[00:43:46.360 --> 00:43:50.360]   And, but I, again, would point to what I said a second ago, but there's something metaphysical here.
[00:43:50.360 --> 00:43:55.360]   There's something deeper than just, you know, the, the search for advantage,
[00:43:55.360 --> 00:44:00.360]   which is a big motivator among people, but there's something more than that going on.
[00:44:00.360 --> 00:44:05.360]   Why are we intentionally wrecking the society? And we are.
[00:44:05.360 --> 00:44:10.360]   And I, and I hear people say, well, it's because some people are getting paid.
[00:44:10.360 --> 00:44:13.360]   Yeah, it's not really an adequate answer. It's more than that. There's something going on.
[00:44:13.360 --> 00:44:19.360]   Like instinctively people want to, some people want to tear it all down.
[00:44:19.360 --> 00:44:22.360]   And I don't think it's necessarily so they can rebuild it to their own advantage.
[00:44:22.360 --> 00:44:26.360]   I think like destruction is the point. And I just watch carefully.
[00:44:26.360 --> 00:44:31.360]   And I don't really know what's going on here, but something much deeper than what we're acknowledging is going on here.
[00:44:31.360 --> 00:44:35.360]   Well, I think it comes back to this point about, I can't get what these other people have.
[00:44:35.360 --> 00:44:39.360]   And there's now an insurmountable barrier that I cannot ever get there.
[00:44:39.360 --> 00:44:45.360]   Yeah. I mean, that doesn't explain why, but hold on, that doesn't explain like why, for example,
[00:44:45.360 --> 00:44:50.360]   Dustin Moskowitz is supporting revolutionary politics and chase a Boudin.
[00:44:50.360 --> 00:44:53.360]   I think it's a good example, Sachs. I think it does.
[00:44:53.360 --> 00:44:57.360]   When you have massive abundance to Schmott's point about the idle mind,
[00:44:57.360 --> 00:45:02.360]   you know, Dustin Moskowitz has achieved more than any human could achieve billions of dollars.
[00:45:02.360 --> 00:45:07.360]   And now he's got this surplus. And then whatever, you know, to Tucker's point,
[00:45:07.360 --> 00:45:14.360]   whatever he's dealing with personally now becomes this, you know, canvas under which he's going to deploy that wealth in an outsourced way.
[00:45:14.360 --> 00:45:16.360]   I think it actually does explain it.
[00:45:16.360 --> 00:45:22.360]   It fits more with like luxury beliefs than with the idea that like scarcity or envy is somehow driving it.
[00:45:22.360 --> 00:45:26.360]   Yeah, but yes, so those are luxury beliefs, and he's got a huge bankroll.
[00:45:26.360 --> 00:45:30.360]   Therefore, it has an outsized impact as sort of Tucker's been saying.
[00:45:30.360 --> 00:45:37.360]   I think the luxury point is once you reach a certain point, your attention can then shift to morality.
[00:45:37.360 --> 00:45:41.360]   When you're starving in a street, and you need to feed your children,
[00:45:41.360 --> 00:45:49.360]   you're not focusing your whole day on the morality and better treatment of others in the world and extending, you know, morality to the rest of the world.
[00:45:49.360 --> 00:45:53.360]   But when you have that luxury, you have to focus your attention in that sense.
[00:45:53.360 --> 00:46:00.360]   I do think that there's this large element that the term equality gets recast into every aspect of society,
[00:46:00.360 --> 00:46:04.360]   which ultimately leads to this general point of view of Marxism,
[00:46:04.360 --> 00:46:08.360]   which is everyone has to have an equal outcome versus an equal opportunity.
[00:46:08.360 --> 00:46:11.360]   And that's where the prosperity takes this.
[00:46:11.360 --> 00:46:17.360]   But I think I agree with a lot of what you're saying, but I think it's maybe more profound than that.
[00:46:17.360 --> 00:46:22.360]   I mean, you're absolutely right that only societies that have reached a certain point can afford to think about certain things.
[00:46:22.360 --> 00:46:27.360]   You don't see, you know, hunter-gatherer societies don't debate effective altruism because they're looking for roots.
[00:46:27.360 --> 00:46:28.360]   So I agree with that.
[00:46:28.360 --> 00:46:38.360]   However, if you look at the behavior of some of the richest people in our society, and I can't speak to Dustin Moskowitz, but I have this, you know, I know people like that.
[00:46:38.360 --> 00:46:42.360]   What they're funding are not exactly luxury beliefs.
[00:46:42.360 --> 00:46:44.360]   It's they're like funding destruction, actually.
[00:46:44.360 --> 00:46:52.360]   And so like previous generations of liberal rich people funded things like public libraries, summer camps for poor kids.
[00:46:52.360 --> 00:46:55.360]   Remember the New York Times used to have these fundraisers, like Fresh Air Fund or whatever.
[00:46:55.360 --> 00:46:57.360]   And people made fun of it. I always kind of like that.
[00:46:57.360 --> 00:47:01.360]   Like you have extra money and like, you know, help the poor kids.
[00:47:01.360 --> 00:47:04.360]   You know, whether it works or not, we can debate, but like I get it.
[00:47:04.360 --> 00:47:09.360]   But Dustin Moskowitz, if you're Dustin Moskowitz or any, I don't mean to pick on him, but any like really rich person,
[00:47:09.360 --> 00:47:14.360]   and you're at Baker's Bay or some discovery property that, you know, you and your billionaire friends go to,
[00:47:14.360 --> 00:47:17.360]   you might occur to you like, why not build one of these for like all the poor kids?
[00:47:17.360 --> 00:47:21.360]   Only poor black kids get to go to this resort for a week.
[00:47:21.360 --> 00:47:24.360]   And that might be, it might work, it might not work.
[00:47:24.360 --> 00:47:29.360]   Like all of American social, the history of American social reform is kind of species of that.
[00:47:29.360 --> 00:47:31.360]   Like we're going to help poor people by doing this or that.
[00:47:31.360 --> 00:47:32.360]   That's not what they're doing.
[00:47:32.360 --> 00:47:34.360]   It's like, hey, poor people, here are some more crack pipes.
[00:47:34.360 --> 00:47:37.360]   And like, it's immoral to criticize your drug addiction.
[00:47:37.360 --> 00:47:42.360]   And what we need are more black people selling weed or it's cool to set fires.
[00:47:42.360 --> 00:47:44.360]   And we can't criticize you for burning down Wendy's.
[00:47:44.360 --> 00:47:45.360]   Like what?
[00:47:45.360 --> 00:47:48.360]   That is not the same thing.
[00:47:48.360 --> 00:47:50.360]   That is super dark.
[00:47:50.360 --> 00:47:53.360]   There's no even possibility of uplift or advancement.
[00:47:53.360 --> 00:47:54.360]   It's just the opposite.
[00:47:54.360 --> 00:47:55.360]   They know it.
[00:47:55.360 --> 00:47:56.360]   These are smart people.
[00:47:56.360 --> 00:47:58.360]   So like, what are we looking at?
[00:47:58.360 --> 00:48:03.360]   - Well, Tucker, he's in Dustin's mind, I'm sure he believes he's doing the right thing.
[00:48:03.360 --> 00:48:04.360]   - Yeah, yeah, yeah.
[00:48:04.360 --> 00:48:05.360]   - So he's just kind of like.
[00:48:05.360 --> 00:48:06.360]   - He thinks he's funding social justice.
[00:48:06.360 --> 00:48:09.360]   I think like Soros would be an even better example, right?
[00:48:09.360 --> 00:48:14.360]   Where like, it's hard to argue that what he's funding is not extremely destructive.
[00:48:14.360 --> 00:48:21.360]   So the revolutionary politics are not somehow coming from below the way you would expect a revolution to normally happen.
[00:48:21.360 --> 00:48:25.360]   They're being imposed from above by some of the biggest winners in our society.
[00:48:25.360 --> 00:48:26.360]   And that is weird.
[00:48:26.360 --> 00:48:27.360]   That is a contradiction.
[00:48:27.360 --> 00:48:28.360]   - They're not being false.
[00:48:28.360 --> 00:48:29.360]   - That is not what you expect.
[00:48:29.360 --> 00:48:30.360]   - It is a bizarre outcome.
[00:48:30.360 --> 00:48:31.360]   Let's.
[00:48:31.360 --> 00:48:33.360]   - It's exactly what you'd expect.
[00:48:33.360 --> 00:48:35.360]   And it's exactly what happened in the Bolshevik revolution.
[00:48:35.360 --> 00:48:44.360]   I mean, half of the Romanov household, you know, Nicholas II, who was murdered with his wife and children in the basement, as you know, by the Bolsheviks.
[00:48:44.360 --> 00:48:47.360]   His extended household supported the Bolsheviks.
[00:48:47.360 --> 00:48:53.360]   Rich people, who are a small minority, by the way, there are a million different revolutionary groups at the end of the First World War in Russia.
[00:48:53.360 --> 00:48:55.360]   Clearly, there was going to be a change.
[00:48:55.360 --> 00:48:56.360]   Clearly, the monarchy was doomed.
[00:48:56.360 --> 00:48:57.360]   And there were a lot of different options.
[00:48:57.360 --> 00:49:02.360]   And the most radical, crazy, nihilistic, atheistic option was the Bolsheviks.
[00:49:02.360 --> 00:49:04.360]   And they had the support of the rich people.
[00:49:04.360 --> 00:49:06.360]   And like, including the Czar's family.
[00:49:06.360 --> 00:49:07.360]   So, like, what is that?
[00:49:07.360 --> 00:49:08.360]   It's the same thing.
[00:49:08.360 --> 00:49:09.360]   Always the same thing.
[00:49:09.360 --> 00:49:14.360]   It's the people with the most have the strongest desire to kill themselves and their society.
[00:49:14.360 --> 00:49:15.360]   That's just true.
[00:49:15.360 --> 00:49:16.360]   - It's an interesting theory.
[00:49:16.360 --> 00:49:18.360]   I mean, there are some people who are very thoughtful, right?
[00:49:18.360 --> 00:49:22.360]   You have Bill Gates or Jeff Bezos, who are thinking about philanthropy in a very thoughtful way.
[00:49:22.360 --> 00:49:23.360]   Now, they can make mistakes, of course.
[00:49:23.360 --> 00:49:25.360]   But they're thinking, hey, where's the most suffering?
[00:49:25.360 --> 00:49:33.360]   How do I work backwards from mosquito tents to, you know, vaccines or whatever, you know, we perceive is, what they perceive is the most suffering thing.
[00:49:33.360 --> 00:49:35.360]   They can relieve in the world.
[00:49:35.360 --> 00:49:36.360]   So, it does seem like it can go either way.
[00:49:36.360 --> 00:49:43.360]   - Tucker, do you think it's because most people that get very wealthy very fast feel like it was unfair?
[00:49:43.360 --> 00:49:47.360]   And they feel guilty and then they have to, like, destroy the system that got them there?
[00:49:47.360 --> 00:49:49.360]   - I think there's a lot.
[00:49:49.360 --> 00:49:54.360]   I think that's such a deep point, though, because and you, I know that, you know, you live among people.
[00:49:54.360 --> 00:49:56.360]   I mean, you're familiar with this world.
[00:49:56.360 --> 00:49:59.360]   So, and only someone who is could say that because you're absolutely right.
[00:49:59.360 --> 00:50:00.360]   Rich people are not all the same.
[00:50:00.360 --> 00:50:06.360]   If you meet someone who made his money incrementally over time, he's much less likely to have the desire.
[00:50:06.360 --> 00:50:08.360]   You know, you may not agree with him on everything.
[00:50:08.360 --> 00:50:09.360]   He may be a good person, a bad person.
[00:50:09.360 --> 00:50:11.360]   But he doesn't want to tear it down.
[00:50:11.360 --> 00:50:14.360]   And he doesn't feel especially guilty for what he has.
[00:50:14.360 --> 00:50:16.360]   And I have found just a non-political point.
[00:50:16.360 --> 00:50:22.360]   But as I get older, I want to spend less time with people who hate themselves because they're not capable of loving other people.
[00:50:22.360 --> 00:50:27.360]   Like, that's not a good place to start, actually, as a person, to hate yourself or feel super guilty.
[00:50:27.360 --> 00:50:28.360]   It really isn't.
[00:50:28.360 --> 00:50:31.360]   You don't end up helping others when you feel that way.
[00:50:31.360 --> 00:50:34.360]   But when you find people who inherited a lot of money, we never talk about that.
[00:50:34.360 --> 00:50:36.360]   It's a huge problem in this country.
[00:50:36.360 --> 00:50:38.360]   I'm not arguing for the death tax at all.
[00:50:38.360 --> 00:50:44.360]   But I'm just saying as an observation, there are an awful, there's a huge class of people with massive inherited wealth.
[00:50:44.360 --> 00:50:46.360]   And they're almost all horrible.
[00:50:46.360 --> 00:50:47.360]   Not all of them.
[00:50:47.360 --> 00:50:50.360]   I mean, I know, you know, very well, I know some of them and they're fine.
[00:50:50.360 --> 00:50:51.360]   Some of them are great.
[00:50:51.360 --> 00:50:52.360]   But as a class, they're awful.
[00:50:52.360 --> 00:50:54.360]   And they don't help at all.
[00:50:54.360 --> 00:50:56.360]   And they're also super annoying and dumb.
[00:50:56.360 --> 00:51:00.360]   And I think it's the wealth, it's just true and everyone knows it's true.
[00:51:00.360 --> 00:51:04.360]   We're about to do that for tens of millions of Americans, right?
[00:51:04.360 --> 00:51:09.360]   You know, we're about to go the largest generation of wealth transfer beyond imagination.
[00:51:09.360 --> 00:51:11.360]   We're talking trillions and trillions and trillions.
[00:51:11.360 --> 00:51:18.360]   So, that problem that you just encapsulated, we're going to cascade across tens of millions of Americans.
[00:51:18.360 --> 00:51:20.360]   It's also happened in Japan.
[00:51:20.360 --> 00:51:23.360]   And in Japan, they have a negative growth rate as well.
[00:51:23.360 --> 00:51:26.360]   Let's pivot to a topic that we can talk.
[00:51:26.360 --> 00:51:28.360]   Wait, can I ask you, have you been to Japan recently?
[00:51:28.360 --> 00:51:30.360]   Yeah, I was there last year.
[00:51:30.360 --> 00:51:31.360]   Okay, so me too.
[00:51:31.360 --> 00:51:38.360]   So, you know, you always read like all the libertarian economists in the Wall Street Journal.
[00:51:38.360 --> 00:51:40.360]   They're always like, "Oh, Japan's a disaster, negative growth."
[00:51:40.360 --> 00:51:42.360]   And then you go to Japan.
[00:51:42.360 --> 00:51:44.360]   Oh, it's the best place you could ever visit.
[00:51:44.360 --> 00:51:46.360]   It's my favorite country, hands down.
[00:51:46.360 --> 00:51:47.360]   Did you go to Tokyo?
[00:51:47.360 --> 00:51:48.360]   Did you go skiing in Osaka?
[00:51:48.360 --> 00:51:49.360]   Where'd you go?
[00:51:49.360 --> 00:51:52.360]   I went to Kyoto in Tokyo and I'll be back to ski on the North Island because it's like, why wouldn't you?
[00:51:52.360 --> 00:51:53.360]   It's like the perfect place, right?
[00:51:53.360 --> 00:51:54.360]   Have you ever skied in Seko?
[00:51:54.360 --> 00:51:56.360]   I never have, but I'm very psyched.
[00:51:56.360 --> 00:51:57.360]   It's incredible.
[00:51:57.360 --> 00:51:58.360]   I went last year.
[00:51:58.360 --> 00:52:01.360]   It is like 25 out of 30 days in January and February.
[00:52:01.360 --> 00:52:04.360]   It dumps powder and the powder is like dust.
[00:52:04.360 --> 00:52:07.360]   It's like makes Utah powder look like heavy.
[00:52:07.360 --> 00:52:08.360]   It is insane.
[00:52:08.360 --> 00:52:11.360]   I'm going back if you want to go in February.
[00:52:11.360 --> 00:52:14.360]   I've got a video to take home.
[00:52:14.360 --> 00:52:15.360]   I'm talking to Tucker right now.
[00:52:15.360 --> 00:52:17.360]   The two of you could share a room.
[00:52:17.360 --> 00:52:19.360]   You guys could have like one.
[00:52:19.360 --> 00:52:23.360]   This is going to make Sax lose his mind if Tucker and I go to the set.
[00:52:23.360 --> 00:52:25.360]   Can I just ask you a question?
[00:52:25.360 --> 00:52:33.360]   If Japan is the butt of every joke that economists tell or the focus of the concern of economy,
[00:52:33.360 --> 00:52:36.360]   lots of chin tugging and fretting about Japan's negative growth economy, and then you go to
[00:52:36.360 --> 00:52:40.360]   Japan and there's literally not one piece of litter and four-year-olds ride the subway
[00:52:40.360 --> 00:52:41.360]   on a company.
[00:52:41.360 --> 00:52:44.360]   Highest function society you could imagine, but yet birth rate is going down.
[00:52:44.360 --> 00:52:45.360]   That's the only thing that's strange.
[00:52:45.360 --> 00:52:46.360]   The birth rate is a real thing.
[00:52:46.360 --> 00:52:47.360]   I totally agree.
[00:52:47.360 --> 00:52:50.360]   That's probably the product of getting atomic bombs dropped on you.
[00:52:50.360 --> 00:52:55.360]   Maybe the fact also that the highest testosterone males flew their planes into US aircraft carriers
[00:52:55.360 --> 00:52:56.360]   or whatever.
[00:52:56.360 --> 00:52:57.360]   There's probably a lot of reason.
[00:52:57.360 --> 00:52:58.360]   Interesting theory.
[00:52:58.360 --> 00:53:00.360]   That's a real thing, by the way.
[00:53:00.360 --> 00:53:03.360]   You kill all the high team males and your society changes.
[00:53:03.360 --> 00:53:06.360]   I don't think that we're using the right measurements.
[00:53:06.360 --> 00:53:10.360]   I do think that we're taking economists a little more seriously than they deserve because
[00:53:10.360 --> 00:53:14.360]   they're not describing what Japan actually is, which is freaking awesome and way more
[00:53:14.360 --> 00:53:18.360]   functional than our society, despite the massive disparity in growth rates.
[00:53:18.360 --> 00:53:23.360]   The thing you'll find is every person in Japan takes their job that they're doing as incredibly
[00:53:23.360 --> 00:53:25.360]   important and has massive pride in it.
[00:53:25.360 --> 00:53:28.360]   That's why when you go there as an American, you're like, "Wait, wasn't this what America
[00:53:28.360 --> 00:53:29.360]   was about?"
[00:53:29.360 --> 00:53:31.360]   It's really shocking as Americans.
[00:53:31.360 --> 00:53:33.360]   We're like, "This is what I want society to be.
[00:53:33.360 --> 00:53:37.360]   I want the person who works in the subway or the stock market or the newspaper to take
[00:53:37.360 --> 00:53:40.360]   their job deadly serious and put their best effort in."
[00:53:40.360 --> 00:53:42.360]   Why should I care about growth rate?
[00:53:42.360 --> 00:53:43.360]   Hold on, though.
[00:53:43.360 --> 00:53:48.360]   Why should I care about economic growth, at least as measured in the conventional sense?
[00:53:48.360 --> 00:53:50.360]   The only reason is if you have a lot of debt.
[00:53:50.360 --> 00:53:51.360]   That's it.
[00:53:51.360 --> 00:53:52.360]   Yes.
[00:53:52.360 --> 00:53:53.360]   Right.
[00:53:53.360 --> 00:53:56.360]   If you didn't have all the debt, you shouldn't care about economic growth rate with respect
[00:53:56.360 --> 00:53:57.360]   to the measure of prosperity.
[00:53:57.360 --> 00:53:59.360]   The strange thing, though, is I've had—
[00:53:59.360 --> 00:54:05.360]   Also, if the pie isn't getting bigger and people aren't doing better, then that does
[00:54:05.360 --> 00:54:07.360]   sow the seeds for a revolution.
[00:54:07.360 --> 00:54:13.360]   I mean, all this divisiveness will start to explode if people don't feel like their circumstances
[00:54:13.360 --> 00:54:14.360]   are getting better off.
[00:54:14.360 --> 00:54:19.360]   This is where, I mean, in this one limited way, I guess I'd be in favor of rapid change,
[00:54:19.360 --> 00:54:21.360]   which is the technological area.
[00:54:21.360 --> 00:54:23.360]   I mean, I'm against revolutionary politics.
[00:54:23.360 --> 00:54:26.360]   I'm against that kind of revolutionary change because it almost never works out.
[00:54:26.360 --> 00:54:33.360]   But I do think that revolutionary change in the narrow category of technology is ultimately
[00:54:33.360 --> 00:54:34.360]   good for us.
[00:54:34.360 --> 00:54:35.360]   I know it creates challenges.
[00:54:35.360 --> 00:54:36.360]   I know it creates disruption.
[00:54:36.360 --> 00:54:39.360]   People do lose their jobs and have to find new ones.
[00:54:39.360 --> 00:54:44.360]   But it is the basis for American prosperity and the basis for our economy being productive
[00:54:44.360 --> 00:54:47.360]   and America having a powerful military and all those things.
[00:54:47.360 --> 00:54:53.360]   So, I don't know if this is a difference between us, Tucker, but I do think that in this area
[00:54:53.360 --> 00:54:56.360]   of technological change, I'm not sort of a dispositional conservative.
[00:54:56.360 --> 00:54:58.360]   I mean, this is kind of like Peter Thiel, right?
[00:54:58.360 --> 00:55:00.360]   I don't want us to slow down.
[00:55:00.360 --> 00:55:02.360]   Actually, I want us to be successful.
[00:55:02.360 --> 00:55:08.360]   And it feels like, actually, it's the people on the left who are generally in this camp
[00:55:08.360 --> 00:55:17.360]   of wanting to slow us down, admire us in regulations, and make it harder to make progress technologically
[00:55:17.360 --> 00:55:19.360]   because it's something they don't control.
[00:55:19.360 --> 00:55:20.360]   I agree with that.
[00:55:20.360 --> 00:55:22.360]   I guess the net result is what I care about.
[00:55:22.360 --> 00:55:28.360]   I think if you talk to old people, which now that I'm getting older and I've talked to
[00:55:28.360 --> 00:55:31.360]   older relatives and stuff, like, what bothers you?
[00:55:31.360 --> 00:55:35.360]   And I used to think it was taking a leak six times a night, but it's really not.
[00:55:35.360 --> 00:55:39.360]   The thing that bothers old people I've spoken to anyway is the change.
[00:55:39.360 --> 00:55:41.360]   It's like they just don't recognize it.
[00:55:41.360 --> 00:55:43.360]   And that's really hard for people to deal with.
[00:55:43.360 --> 00:55:48.360]   And so, all I'm saying, I'm certainly not calling for a halt to technological progress.
[00:55:48.360 --> 00:55:49.360]   That would be terrible.
[00:55:49.360 --> 00:55:56.360]   All I'm saying is, in tandem with those advancements should be the concern about people's ability
[00:55:56.360 --> 00:55:58.360]   to digest massive change.
[00:55:58.360 --> 00:56:00.360]   And let's keep some things the same.
[00:56:00.360 --> 00:56:02.360]   You just can't change everything.
[00:56:02.360 --> 00:56:03.360]   It's bad.
[00:56:03.360 --> 00:56:04.360]   It's super bad.
[00:56:04.360 --> 00:56:07.360]   So, maybe we get AI, but let's keep Halloween.
[00:56:07.360 --> 00:56:13.360]   As you're arguing for tradition and things that bring people together, this country was
[00:56:13.360 --> 00:56:20.360]   built off of immigrants, obviously, and this is one of the most polarizing issues, Tucker,
[00:56:20.360 --> 00:56:25.360]   in each election cycle and in the country at large right now.
[00:56:25.360 --> 00:56:33.360]   What is your vision for how American immigration should work here in the 21st century?
[00:56:33.360 --> 00:56:35.360]   Ideally, or right now?
[00:56:35.360 --> 00:56:37.360]   Right now, and then ideally.
[00:56:37.360 --> 00:56:40.360]   Let's start with what would you do right now, short term, and then ideally.
[00:56:40.360 --> 00:56:44.360]   Well, actually, if you don't mind, I'll quickly invert it and say, of course, the goal is
[00:56:44.360 --> 00:56:48.360]   just a rational immigration policy whose purpose is to help your country.
[00:56:48.360 --> 00:56:49.360]   What would that look like?
[00:56:49.360 --> 00:56:53.360]   Well, I can see an argument, for example, if you needed, I don't know, there was a time
[00:56:53.360 --> 00:56:58.360]   for the guy who was just saying, my college roommate, my best friend, came to this country
[00:56:58.360 --> 00:57:02.360]   from India because both his parents were physicians and there was a lack of physicians with
[00:57:02.360 --> 00:57:03.360]   deindustrialization.
[00:57:03.360 --> 00:57:08.360]   Everyone was moving out of the small towns, and so we expedited the visas of foreign physicians.
[00:57:08.360 --> 00:57:09.360]   Most of them were Indians.
[00:57:09.360 --> 00:57:10.360]   I think most were Indians.
[00:57:10.360 --> 00:57:12.360]   His parents actually came from Africa, but whatever.
[00:57:12.360 --> 00:57:16.360]   The point is, they're Indian doctors and they came here and they settled in a little town
[00:57:16.360 --> 00:57:20.360]   in Massachusetts, a rural town, dying mill town, and it was great.
[00:57:20.360 --> 00:57:21.360]   It was great for them.
[00:57:21.360 --> 00:57:23.360]   It was great for the town.
[00:57:23.360 --> 00:57:25.360]   The guy became my best friend.
[00:57:25.360 --> 00:57:27.360]   I mean, like, that's just, that's what you want, right?
[00:57:27.360 --> 00:57:32.360]   We need this and we're going to, in a very smart, intentional way, try to get it on the
[00:57:32.360 --> 00:57:33.360]   world market.
[00:57:33.360 --> 00:57:34.360]   And we can, we will.
[00:57:34.360 --> 00:57:36.360]   And I'm totally for that.
[00:57:36.360 --> 00:57:41.360]   What we're doing now is throwing open the doors to anyone who wants to come here from
[00:57:41.360 --> 00:57:45.360]   the poorest countries in the world at a scale that we can't possibly digest.
[00:57:45.360 --> 00:57:48.360]   So, how many people are here illegally working off the books?
[00:57:48.360 --> 00:57:50.360]   Some estimate 60, 70 million.
[00:57:50.360 --> 00:57:53.360]   Those are real estimates, by the way, not crackpot estimates.
[00:57:53.360 --> 00:57:54.360]   But the truth is, we don't know.
[00:57:54.360 --> 00:57:55.360]   We have no idea.
[00:57:55.360 --> 00:57:57.360]   So, we've completely lost control.
[00:57:57.360 --> 00:57:59.360]   And we don't know who these people are.
[00:57:59.360 --> 00:58:02.360]   I honestly think most of them are here for a better life.
[00:58:02.360 --> 00:58:03.360]   I believe that.
[00:58:03.360 --> 00:58:04.360]   I think most of them are probably good people.
[00:58:04.360 --> 00:58:07.360]   I think most of them agree with my politics, actually, for whatever it's worth.
[00:58:07.360 --> 00:58:11.360]   Definitely much more than the unhappy private equity wives do, for sure.
[00:58:11.360 --> 00:58:14.360]   The average guy from Nigeria is, like, on my side.
[00:58:14.360 --> 00:58:16.360]   Every Salvadoran agrees with me.
[00:58:16.360 --> 00:58:17.360]   So, I like that.
[00:58:17.360 --> 00:58:20.360]   But I also think it's too much, actually.
[00:58:20.360 --> 00:58:25.360]   At exactly the moment when native-born Americans' birth rates are tanking, over 100,000 people
[00:58:25.360 --> 00:58:33.360]   die of fentanyl ODs, and we're pushing euthanasia on the population, which we are, you're basically
[00:58:33.360 --> 00:58:36.360]   saying, "I've given up on the people who live here, whatever their color or background,
[00:58:36.360 --> 00:58:39.360]   and we're just going to import new people and replace them with new people."
[00:58:39.360 --> 00:58:40.360]   That's literally what's happening.
[00:58:40.360 --> 00:58:44.360]   And when you say that out loud, they freak out and call you some sort of crazed bigot
[00:58:44.360 --> 00:58:47.360]   for saying the word "replacement," but that's what it is.
[00:58:47.360 --> 00:58:50.360]   And it's happening in Western Europe, in Ireland particularly.
[00:58:50.360 --> 00:58:52.360]   That's what those riots were about.
[00:58:52.360 --> 00:58:54.360]   Not that I'm endorsing the riots, but that's what that was.
[00:58:54.360 --> 00:58:57.360]   And that's, like, insane for a government to do to its own people.
[00:58:57.360 --> 00:58:59.360]   It's just, it's totally insane.
[00:58:59.360 --> 00:59:06.360]   And more than anything, it is an expression of loathing for the people who live there.
[00:59:06.360 --> 00:59:07.360]   It's a dilution of their political power.
[00:59:07.360 --> 00:59:09.360]   It's a dilution of their economic power.
[00:59:09.360 --> 00:59:14.360]   It's the total destruction of the basic services that they paid for, like schools, hospitals,
[00:59:14.360 --> 00:59:15.360]   roads.
[00:59:15.360 --> 00:59:16.360]   Like, those are gone.
[00:59:16.360 --> 00:59:20.360]   So, the last thing I'll say is, people say, "Well, immigrants are coming here looking
[00:59:20.360 --> 00:59:21.360]   for a better life."
[00:59:21.360 --> 00:59:24.360]   And I believe that, having talked to a lot of immigrants and grown up around them in
[00:59:24.360 --> 00:59:25.360]   Southern California.
[00:59:25.360 --> 00:59:26.360]   I totally believe that.
[00:59:26.360 --> 00:59:31.360]   But the question for government is, "Am I making the citizens' life better?"
[00:59:31.360 --> 00:59:33.360]   It's like no one even thinks that.
[00:59:33.360 --> 00:59:38.360]   So, is importing people making the life—so, when my friend's parents came, both physicians
[00:59:38.360 --> 00:59:42.360]   to the little town, little mill town in New England, his parents made the town better
[00:59:42.360 --> 00:59:43.360]   for the people who lived there.
[00:59:43.360 --> 00:59:44.360]   They were the doctors.
[00:59:44.360 --> 00:59:47.360]   And they were super successful, actually, and great people.
[00:59:47.360 --> 00:59:51.360]   >>Corey: Tucker, can you give us a rundown of the current political landscape?
[00:59:51.360 --> 00:59:57.360]   Just tell us, I'm just really curious what you think about RFK, Biden, Trump, Vivian.
[00:59:57.360 --> 00:59:59.360]   Maybe like 10, 15 seconds on each.
[00:59:59.360 --> 01:00:01.360]   >>Tucker: It's so hard.
[01:00:01.360 --> 01:00:07.360]   I mean, the thing that jumps out today, my view changes.
[01:00:07.360 --> 01:00:10.360]   I don't think Biden can be the nominee.
[01:00:10.360 --> 01:00:15.360]   His only point was to stop Bernie Sanders, basically.
[01:00:15.360 --> 01:00:19.360]   And he's outlived his usefulness, so I'm pretty sure he'll be replaced.
[01:00:19.360 --> 01:00:24.360]   But as on the Republican side and the independents, it's so hard to know.
[01:00:24.360 --> 01:00:30.360]   I'm just mesmerized by the love for Nikki Haley, who's like the most—and I'm sorry.
[01:00:30.360 --> 01:00:36.360]   For example, I saw Jamie Dimon, who I really like and I know and I have always admired.
[01:00:36.360 --> 01:00:42.360]   But when Jamie Dimon starts free-balling on Nikki Haley and saying nonsensical things about Nikki Haley,
[01:00:42.360 --> 01:00:44.360]   I'm like, I want to call him and say, "You're humiliating yourself."
[01:00:44.360 --> 01:00:47.360]   First of all, you're way out of your depth. You have no idea what you're talking about.
[01:00:47.360 --> 01:00:50.360]   But for another, you're just betraying how completely out of touch you are.
[01:00:50.360 --> 01:00:55.360]   And I think Jamie got—and again, I say this with admiration for Jamie Dimon, which has long held.
[01:00:55.360 --> 01:01:02.360]   But if you get to a place where you think that Nikki Haley has a shot of getting elected in a free and fair election,
[01:01:02.360 --> 01:01:04.360]   you have no idea what's going on in your own country.
[01:01:04.360 --> 01:01:07.360]   It's just embarrassing to say that. Super embarrassing.
[01:01:07.360 --> 01:01:10.360]   Explain to the audience. Unpack that. Why is Nikki Haley not here?
[01:01:10.360 --> 01:01:15.360]   It really—it's not personal. I actually don't care for Nikki Haley as a person, but that's immaterial.
[01:01:15.360 --> 01:01:21.360]   Nikki Haley's program, what she stands for, what she believes, may be moral or immoral. God will judge.
[01:01:21.360 --> 01:01:25.360]   It's not popular with the public. And it's super easy to know that.
[01:01:25.360 --> 01:01:27.360]   Her stance on war, specifically, yeah.
[01:01:27.360 --> 01:01:31.360]   Her stance on war and on economics. And we know that from looking at the Gallup poll,
[01:01:31.360 --> 01:01:34.360]   which is a rolling survey of people's attitudes on things.
[01:01:34.360 --> 01:01:40.360]   And in an actual democracy, you would see the leading candidates, or people who hope to become the leading candidates,
[01:01:40.360 --> 01:01:48.360]   articulate concerns and lay out views that reflected the concerns of the population who are going to vote them into office.
[01:01:48.360 --> 01:01:50.360]   Who does that the most right now?
[01:01:50.360 --> 01:01:51.360]   Well, Trump.
[01:01:51.360 --> 01:01:55.360]   Is it fair to say Nikki Haley is basically an unreconstructed Bush Republican?
[01:01:55.360 --> 01:01:56.360]   Yes, yes.
[01:01:56.360 --> 01:01:59.360]   I mean, her views are basically the same views of George W. Bush.
[01:01:59.360 --> 01:02:01.360]   She's once again in all sorts of new wars.
[01:02:01.360 --> 01:02:06.360]   And I don't even think she regrets the forever wars in the Middle East that were so disastrous.
[01:02:06.360 --> 01:02:11.360]   I haven't heard one word of criticism from her or remorse that she supported all those things.
[01:02:11.360 --> 01:02:15.360]   And on the economic policies, it's kind of this like corporatism.
[01:02:15.360 --> 01:02:20.360]   It's like the whole Trump rebellion never even happened in the Republican Party.
[01:02:20.360 --> 01:02:22.360]   It's just like right back to the past.
[01:02:22.360 --> 01:02:30.360]   And the fact that the establishment sort of coalescing around her kind of tells me that as much as I didn't want to believe this,
[01:02:30.360 --> 01:02:35.360]   I think that Trump is still the indispensable figure in the Republican Party.
[01:02:35.360 --> 01:02:39.360]   Because if you take him away, they're going to revert right back to where they were.
[01:02:39.360 --> 01:02:44.360]   They're going to go right back to the factory settings of Republicans, which is Bush Republicanism.
[01:02:44.360 --> 01:02:46.360]   Tucker, can you just finish the rest of them?
[01:02:46.360 --> 01:02:48.360]   RFK, Vivek, Trump.
[01:02:48.360 --> 01:02:55.360]   Well, I mean, I know Vivek well personally, and it shows that I'm out of touch, too.
[01:02:55.360 --> 01:02:59.360]   I mean, you know, I'm 54, so I don't have my finger on every pulse.
[01:02:59.360 --> 01:03:01.360]   But so I like Vivek, actually.
[01:03:01.360 --> 01:03:06.360]   And but I see these surveys where he's not doing well and people don't like his personality or whatever.
[01:03:06.360 --> 01:03:09.360]   He's a little overbearing.
[01:03:09.360 --> 01:03:10.360]   That never bothers me at all.
[01:03:10.360 --> 01:03:12.360]   I'm just not put off by that at all.
[01:03:12.360 --> 01:03:15.360]   And I'm a little bit confused by why he's not doing better.
[01:03:15.360 --> 01:03:18.360]   Again, that's a reflection on I've just made fun of Jamie Dimon.
[01:03:18.360 --> 01:03:21.360]   Here I am doing it myself. Like, why isn't Vivek doing better?
[01:03:21.360 --> 01:03:24.360]   I'm sure there are reasons. I just don't. I think his program is solid.
[01:03:24.360 --> 01:03:26.360]   I think he's reasonable. I think he's smart.
[01:03:26.360 --> 01:03:30.360]   I don't think he's some creepy agenda.
[01:03:30.360 --> 01:03:33.360]   So but he's not he's not doing well. That's just a fact.
[01:03:33.360 --> 01:03:38.360]   Bobby Kennedy, I know quite well and and think a lot of him.
[01:03:38.360 --> 01:03:43.360]   And he's a very he's I think he's a decent man and a principled person.
[01:03:43.360 --> 01:03:50.360]   I think he's smart. You know, I'm I don't have quite as much confidence in maybe some of the people around him.
[01:03:50.360 --> 01:03:54.360]   I'm not I feel like maybe there are other agendas that he's not aware of.
[01:03:54.360 --> 01:03:58.360]   You know, I don't know. I can't assess. But yeah.
[01:03:58.360 --> 01:04:01.360]   So that's what I think. President Trump.
[01:04:01.360 --> 01:04:04.360]   Well, I mean, you know, The New York Times had a piece.
[01:04:04.360 --> 01:04:09.360]   I think it was Jonathan Swan was smart and had a piece telling you what you already knew.
[01:04:09.360 --> 01:04:17.360]   But proving it with numbers that Trump became the nominee in August of last year, 2022,
[01:04:17.360 --> 01:04:20.360]   when the FBI went through his wife's underwear drawer in his house.
[01:04:20.360 --> 01:04:25.360]   Like that was so insane that even if you're like, oh, I can't deal with more Trump and he didn't actually do anything.
[01:04:25.360 --> 01:04:32.360]   Put Jared in charge. You know, like there are lots of frustrations you could have about Trump if you supported Trump and you could be disappointed.
[01:04:32.360 --> 01:04:37.360]   But the second the FBI raids his house on a documents charge and anyone from Washington,
[01:04:37.360 --> 01:04:42.360]   as I am, can tell you that's like insane, like every that's like so common.
[01:04:42.360 --> 01:04:47.360]   It's not if they're charging him for that. That's a joke. Where are the felonies he supposedly committed?
[01:04:47.360 --> 01:04:51.360]   I was led to believe he like murdered people and buried them in the Meadowlands.
[01:04:51.360 --> 01:04:56.360]   Do you know what I mean? Like that's the best they got when they did that.
[01:04:56.360 --> 01:05:00.360]   I know for a fact in this piece showed it, but I knew it already.
[01:05:00.360 --> 01:05:04.360]   A lot of people are like, you know, I have mixed feelings about Trump. I don't want to deal with more Trump drama.
[01:05:04.360 --> 01:05:08.360]   But if this is allowed to happen, our system won't continue.
[01:05:08.360 --> 01:05:13.360]   That's so outrageous that I mean, let's just stop lying about it. That's a political prosecution.
[01:05:13.360 --> 01:05:19.360]   You can't have that of the presidential front runner. You can't have that. We already did it with Nixon.
[01:05:19.360 --> 01:05:23.360]   You can't do that again. And so I think that's the key to his surge. I really do think that.
[01:05:23.360 --> 01:05:27.360]   Who's the Democratic nominee if you think Biden's not going to be? Oh, it's Gavin.
[01:05:27.360 --> 01:05:33.360]   Gavin. It's Gavin. For sure. Well, of course, because he's by far the most evil person in the Democratic Party.
[01:05:33.360 --> 01:05:37.360]   And in the end, they just sort of rise to the peak.
[01:05:37.360 --> 01:05:48.360]   Oil and water, man. Yeah. Tucker, just just comment on Gavin vetoing the bill that said that you could kind of remove the parents,
[01:05:48.360 --> 01:05:53.360]   remove the child from the parents if the parents don't affirm the transgender rights of the child.
[01:05:53.360 --> 01:06:02.360]   I think there was a lot of folks who sit in your camp typically who were shocked and surprised and said, wow, I didn't know Gavin actually, you know, had a span of opinions.
[01:06:02.360 --> 01:06:07.360]   And this is interesting to see. I mean, what's what like there's no one I know who thought that.
[01:06:07.360 --> 01:06:14.360]   And everyone I know who watched that thought the same thing they thought when he met with Xi, which is, oh, he's he's going to be the nominee.
[01:06:14.360 --> 01:06:18.360]   Yeah, of course. That's what that was. I mean, I know I know Gavin Newsom.
[01:06:18.360 --> 01:06:23.360]   And, you know, I think a lot about Gavin Newsom, many different things about Gavin Newsom.
[01:06:23.360 --> 01:06:28.360]   But one thing I know for a fact about Gavin Newsom is he has the capacity to be delighted.
[01:06:28.360 --> 01:06:33.360]   Lie detector test. Gavin Newsom will say anything he needs to say.
[01:06:33.360 --> 01:06:36.360]   Not like Biden's not like this, actually. Whatever Biden's fault. He's not like this.
[01:06:36.360 --> 01:06:42.360]   Like Biden, you know, he has like guilt. If he's lying to you, he gets twitchy.
[01:06:42.360 --> 01:06:49.360]   Gavin Newsom's palms don't sweat. His respiration doesn't increase. His body temperature doesn't change.
[01:06:49.360 --> 01:06:54.360]   Nothing changes in Gavin Newsom when he lies to your face. And there are not that many people like that, actually.
[01:06:54.360 --> 01:07:05.360]   That's a rare quality, like to lock down the state, to keep people's kids from getting an education and to arrest people for surfing and then go have dinner at the French Laundry.
[01:07:05.360 --> 01:07:09.360]   Like most people couldn't do that. They just feel like you're saying he's a sociopath.
[01:07:09.360 --> 01:07:16.360]   Just he can lie and not care. I'm not a psychiatrist, but I so I don't know that I don't really know the category and I'm not going to diagnose him.
[01:07:16.360 --> 01:07:22.360]   But I'll just say in 50 years of being around a lot of people, I've met very few who can behave that way.
[01:07:22.360 --> 01:07:27.360]   Very, very few. It's a very unusual quality. And of course, it's probably useful in politics.
[01:07:27.360 --> 01:07:32.360]   Is he electable? Is he electable? Yeah, exactly. Are the American people going to see that the way that you see it or.
[01:07:32.360 --> 01:07:38.360]   Well, as you know, the system in California does not include elections. I mean, it has nothing to do with what the people think.
[01:07:38.360 --> 01:07:41.360]   It's a it's a machine state. It's the most corrupt out of 50.
[01:07:41.360 --> 01:07:47.360]   Kamala Harris was like despised by most Californians and she was a sitting U.S. senator.
[01:07:47.360 --> 01:07:53.360]   Diane, poor Dianne Feinstein, my neighbor in Washington. I don't think she was a horrible person, just for the record.
[01:07:53.360 --> 01:07:58.360]   But she was non-compass menace and like she could have lived well beyond her death as a U.S. senator.
[01:07:58.360 --> 01:08:04.360]   So it's like it's not a democratic state, small d democratic state. It's not run on the basis of what the population wants.
[01:08:04.360 --> 01:08:12.360]   It's a fixed game in California. And so it does make me very uncomfortable that someone from that political culture,
[01:08:12.360 --> 01:08:21.360]   which is an utterly corrupt political culture and authoritarian political culture, could like enter a presidential race because like clearly,
[01:08:21.360 --> 01:08:25.360]   what are you running on if you're Gavin Newsom as a native Californian?
[01:08:25.360 --> 01:08:33.360]   You know, I know what the state was like in 1985 because I live there and it's completely degraded from that from that time.
[01:08:33.360 --> 01:08:38.360]   And like, how did that happen? Well, part of the big reason, the big reason is the political leadership of the state.
[01:08:38.360 --> 01:08:43.360]   You've got nothing to run on. What are you running? Have you driven through L.A. recently? Like, seriously.
[01:08:43.360 --> 01:08:52.360]   So the fact that he would get in the race suggests, you know, they think that they can win without the consent of voters.
[01:08:52.360 --> 01:08:57.360]   And that freaks me out. Well, he'll have the media working on overdrive for him. Right.
[01:08:57.360 --> 01:09:01.360]   I mean, they will turn him somehow into like John F. Kennedy or something.
[01:09:01.360 --> 01:09:07.360]   I mean, they will paper over all of his flaws and they'll you know, they'll basically write puff pieces.
[01:09:07.360 --> 01:09:13.360]   It'll be like nonstop and then they'll be attacking Trump. So the media will put him over the top, I think, if he.
[01:09:13.360 --> 01:09:17.360]   Well, assuming that we have the same media that we had in 2020.
[01:09:17.360 --> 01:09:21.360]   That's true. But I mean, that's why you just got to pray every night for Elon's health.
[01:09:21.360 --> 01:09:28.360]   And I mean it, too. I mean, it's the only platform at scale in the world that's pretty, you know, there's censorship on it.
[01:09:28.360 --> 01:09:36.360]   But there's not mass censorship, actually. There isn't. And that's the only platform of its kind at scale.
[01:09:36.360 --> 01:09:37.360]   It's the only one.
[01:09:37.360 --> 01:09:47.360]   So let's talk about that, actually. So, I mean, we've talked in this conversation about how our public discourse is inane and self-destructive and divisive.
[01:09:47.360 --> 01:09:52.360]   I would add another word to that, which is controlled, you know, controlled.
[01:09:52.360 --> 01:09:56.360]   A good example of this, I think, was just on the Ukraine war.
[01:09:56.360 --> 01:10:10.360]   David Arkamia, who is Zelensky's parliamentary leader, who was the lead negotiator for the Ukrainians at Istanbul in the first month of the war, when there was a deal on the table, he just testified, basically said in an interview,
[01:10:10.360 --> 01:10:15.360]   "There was a deal on the table to shut the war down. We just would have had to agree to make Ukraine neutral."
[01:10:15.360 --> 01:10:22.360]   And, of course, the Biden administration told them not to. This war is and has always been about NATO expansion.
[01:10:22.360 --> 01:10:33.360]   And yet the party line from the media, even as all of these proof points stack up, I mean, we're now on like the 10th person, first-hand witness, to say that this is what this war is all about.
[01:10:33.360 --> 01:10:38.360]   You still can't get the media to honestly report this. This is one example.
[01:10:38.360 --> 01:10:49.360]   Okay. And I believe that this is one of the reasons why you were fired, Tucker, is because you were literally the only person on mainstream network news saying what the war is really about.
[01:10:49.360 --> 01:10:56.360]   So this is like one really big example is that we cannot get any truth on an issue as big as the Ukraine war.
[01:10:56.360 --> 01:11:01.360]   So I guess my question for you is, how does control like that happen?
[01:11:01.360 --> 01:11:09.360]   I don't really understand it myself. We live in what's supposed to be a big democracy. There's supposed to be a lot of media channels.
[01:11:09.360 --> 01:11:21.360]   But yet they all enforce a certain narrative on pretty much every issue. But even, you know, again, I'm picking on, I think, one of the biggest issues right now, which is this war we've got going on.
[01:11:21.360 --> 01:11:24.360]   I mean, how does that happen? I don't understand it. How do they maintain that control?
[01:11:24.360 --> 01:11:32.360]   This is one of my personal concerns about technology and about progress of all kinds, which is you don't actually know where it's going. You don't.
[01:11:32.360 --> 01:11:44.360]   I mean, your best forecaster or mine have very often been wrong. And, you know, of course, the promise of the Internet was diversity and access to information from a lot of different sources, unfiltered information.
[01:11:44.360 --> 01:11:57.360]   You can talk to people in foreign countries for free. You know, every American will have an encyclopedia at his fingertips and people are gonna be much more informed and no one will be able to control it because it's free and open.
[01:11:57.360 --> 01:12:08.360]   And the effect has been, you know, the opposite. There's less, I would argue, there's been less freedom in information than there was 30 years ago.
[01:12:08.360 --> 01:12:13.360]   How did that happen? I mean, that's, it's a very, I mean, you guys are much better situated to answer that question. Someone should think about that, I think.
[01:12:13.360 --> 01:12:23.360]   But the bottom line is there are just not that many pipelines, actually. You know, in television, there were three big news channels, cable, you know, broadcast kind of receded in importance.
[01:12:23.360 --> 01:12:29.360]   It's mostly about prostate health now. And so there are three big channels, two of them on one side, one was on the other.
[01:12:29.360 --> 01:12:40.360]   And then there were the social media giants, but there are not that many of them. And they were all locked down and they were all riddled with intel, in some cases, actual salaried intel officers.
[01:12:40.360 --> 01:12:47.360]   Matt Taibbi's amazing reporting has shown this, not just American either, from foreign countries. The whole thing was an op. It was insane.
[01:12:47.360 --> 01:12:58.360]   And, you know, you could, I'm not going to beat up on Fox News, but there was a kind of a fairly narrow band of acceptable views allowed on that channel. Is that control? Yes, it is.
[01:12:58.360 --> 01:13:05.360]   And so there really was no remaining place with scale where someone with a dissenting view could give it voice.
[01:13:05.360 --> 01:13:22.360]   And that's just crazy. It's the opposite of what we were promised. But whatever, not to whine about it. But the existence of X, where anyone around the world or in most countries anyway can get for free a whole range of opinions that aren't controlled, that changes everything.
[01:13:22.360 --> 01:13:32.360]   Like the primacy of control of information in a war cannot be overstated. Like that's, you can debate whether it's more important than ammunition, but it's right up there, you know.
[01:13:32.360 --> 01:13:41.360]   And so I think this election, if that platform stays free for the next 12 months, you know, I think we have a shot at at least of a real election.
[01:13:41.360 --> 01:13:44.360]   But if it does, and I think they're going to do whatever they can to shut it down.
[01:13:44.360 --> 01:13:51.360]   We also have web-based shows, podcasts, and that even predated a little bit, you know, taking over X.
[01:13:51.360 --> 01:13:59.360]   There is a self-correcting mechanism here. If you feel too controlled, like maybe perhaps you felt or you felt pressure at Fox.
[01:13:59.360 --> 01:14:10.360]   I don't speak for you if you did. Then all of a sudden, Joe Rogan, All In Podcast, whatever, podcasts all emerge and now your show on X, and I'm sure it's going to be on other platforms as well.
[01:14:10.360 --> 01:14:22.360]   Maybe you can speak to what you think the impact of a post-Fox Tucker Carlson show will be, and how will you be able to sort of shape the show differently, if at all?
[01:14:22.360 --> 01:14:29.360]   And what's the mission here? That was my first question we never really got to, which is post-Fox, post-money, post-fame.
[01:14:29.360 --> 01:14:33.360]   What is Tucker Carlson's mission statement going forward? What is your goal here?
[01:14:33.360 --> 01:14:43.360]   Just the same as before, and I should just be extra clear. I wish I could tell harder stories about Fox, you know, forcing me to take some line or other, but they really didn't.
[01:14:43.360 --> 01:14:58.360]   But, you know, they didn't want the show anymore, so that kind of tells you. But anyway, the point is, my intent in hosting that show is the same as my intent in hosting the show we're about to launch on X, which is, you know, just do your best to say what you think is true.
[01:14:58.360 --> 01:15:06.360]   Bring perspectives and information that you don't think are widely covered to a larger audience, and, you know, try and stay firm in that. Admit when you're wrong.
[01:15:06.360 --> 01:15:16.360]   Like, just be honest. It's actually not—it's, you know, I got in this business because I hadn't graduated from college, and my father's like, "It's a pretty pure meritocracy in journalism. You should do it."
[01:15:16.360 --> 01:15:21.360]   And it's also pretty simple. No real skills required. Just be literate, which I was.
[01:15:21.360 --> 01:15:22.360]   And is the show going to evolve a little bit?
[01:15:22.360 --> 01:15:23.360]   That's it.
[01:15:23.360 --> 01:15:26.360]   Because you've been doing this experiment. You've done about 40 episodes or so.
[01:15:26.360 --> 01:15:37.360]   Here's the way that it already has evolved. So I got laid off in April, and I, you know, I like to fish and bird hunt, so I did a lot of that. And then I was like, "Ugh, I need to do something."
[01:15:37.360 --> 01:15:43.360]   So I've been stuck in a studio for all these, you know, many, many years. I couldn't really go anywhere.
[01:15:43.360 --> 01:15:50.360]   And I went—I took seven foreign trips in four months. I just went to all these places where I knew people. I was interested in what was going on.
[01:15:50.360 --> 01:15:59.360]   And I just found it amazing. Two things I found amazing. One, the view of America, the vantage you get from a foreign country on what's happening in your own is completely different.
[01:15:59.360 --> 01:16:09.360]   There's also a lot—the whole world is reshuffling, in my view, after the Ukraine war. February 2022 really did change a lot, particularly with respect to America's place in the world.
[01:16:09.360 --> 01:16:16.360]   That's worth covering. And the second thing I learned once I started putting videos up on X was that it's international.
[01:16:16.360 --> 01:16:24.360]   I mean, I was just shocked by that. I mean, because think about it. I mean, I'm working for a U.S. news channel. It's one of three U.S. news channels I've worked for.
[01:16:24.360 --> 01:16:31.360]   CNN was international, but I was on CNN U.S. So I'm just used to thinking about America being viewed by Americans, having a conversation about this country.
[01:16:31.360 --> 01:16:40.360]   I had no experience at all of a big international audience, and that platform has that. And so I was amazed by that.
[01:16:40.360 --> 01:16:48.360]   And so we're going to continue to cover the rest of the world. I'm going over to Dubai in February, interviewing a bunch of heads of state.
[01:16:48.360 --> 01:16:50.360]   It's incredible. I've been spending time in this wild—
[01:16:50.360 --> 01:16:56.360]   It's just so interesting. There's so much going on. It's like crazy. People—and this is not a political point.
[01:16:56.360 --> 01:17:03.360]   This is like a human point that bothers me almost more than anything politically, which is the death of curiosity.
[01:17:03.360 --> 01:17:11.360]   It's like people are not curious. Like, what the hell? I thought the technological revolution would set off like explosions in the brain of every person.
[01:17:11.360 --> 01:17:17.360]   Like, what is that? I want to learn more. And it had exactly the opposite. It's like, lull me to sleep with TikTok. Don't tell me more. I don't want to know.
[01:17:17.360 --> 01:17:20.360]   Just double down on what you already know, what you already believe over and over and over and over again.
[01:17:20.360 --> 01:17:27.360]   Yeah, what is that? I'm like—I know—I'm less certain in my beliefs. I know that I know less than any time in my life.
[01:17:27.360 --> 01:17:32.360]   And I'm much more interested in many more things than I've ever been. I think that's normal.
[01:17:32.360 --> 01:17:36.360]   And I think there's something like in the water or something that's making people not care.
[01:17:36.360 --> 01:17:41.360]   The UFO thing. Like, what? Whatever your views of that—like, well, what is that? Shut up.
[01:17:41.360 --> 01:17:46.360]   And that's normal people don't want to hear it. Why? I don't know. Whatever. I don't want to preach.
[01:17:46.360 --> 01:17:49.360]   I mean, post-COVID, we still don't have an accounting of what happened.
[01:17:49.360 --> 01:17:54.360]   Tucker, what do you think about the clip yesterday of Elon?
[01:17:54.360 --> 01:17:58.360]   Let's play it for 45 seconds and then get your response. Tucker Carlson.
[01:17:58.360 --> 01:18:05.360]   Apology tour, if you will. This had been said online. There was all of the criticism. There was advertisers leaving.
[01:18:05.360 --> 01:18:06.360]   We talked to Bob Iger today.
[01:18:06.360 --> 01:18:07.360]   I hope they stop.
[01:18:07.360 --> 01:18:08.360]   You hope—
[01:18:08.360 --> 01:18:10.360]   Don't advertise.
[01:18:10.360 --> 01:18:11.360]   You don't want them to advertise?
[01:18:11.360 --> 01:18:12.360]   No.
[01:18:12.360 --> 01:18:13.360]   What do you mean?
[01:18:13.360 --> 01:18:21.360]   If somebody's going to try to blackmail me with advertising, blackmail me with money, go fuck yourself.
[01:18:21.360 --> 01:18:25.360]   But—
[01:18:25.360 --> 01:18:35.360]   Go fuck yourself. Is that clear? I hope it is. Hey, Bob, if you're in the audience.
[01:18:35.360 --> 01:18:37.360]   Well, let me ask you then—
[01:18:37.360 --> 01:18:39.360]   That's how I feel.
[01:18:39.360 --> 01:18:45.360]   All right, Tucker, your response to the "Good for you." The old G-F-Y from Elon to Bob.
[01:18:45.360 --> 01:18:51.360]   I mean, I've interviewed people every week for over 30 years, and so I know what it is to interview—
[01:18:51.360 --> 01:18:54.360]   I just dropped my Perrier. Excuse me.
[01:18:54.360 --> 01:18:58.360]   And I know I have a lot of experience interviewing people, and I've interviewed Elon, you know.
[01:18:58.360 --> 01:19:03.360]   And I don't understand how the New York Times character, fussy little guy from the New York Times, how could you not laugh?
[01:19:03.360 --> 01:19:06.360]   Like, what? He just told Bob Iger to fuck himself.
[01:19:06.360 --> 01:19:08.360]   He's a terrible moderator.
[01:19:08.360 --> 01:19:14.360]   Don't get me started. You and I are in sync. Andrew R. Sorkin is amongst the weakest of moderators and interviewers.
[01:19:14.360 --> 01:19:18.360]   He's just—what a fussy little douche. Like, that's the moment where he just, like—
[01:19:18.360 --> 01:19:20.360]   I agree. Huge douche.
[01:19:20.360 --> 01:19:26.360]   You were rocking your total, like—are you joking? Elon Musk just told Bob Iger to fuck himself?
[01:19:26.360 --> 01:19:28.360]   Yes. My response, he's a robot.
[01:19:28.360 --> 01:19:30.360]   I'd be texting my wife. It's unbelievable. I know.
[01:19:30.360 --> 01:19:36.360]   No, of course I love it. I love it. I am in the Iger thing.
[01:19:36.360 --> 01:19:42.360]   You know, I don't hate Bob Iger or anything, but, like, I keep hearing from people, you know, mutual people I know who know Bob Iger very well
[01:19:42.360 --> 01:19:44.360]   that he's, like, very serious about running for president.
[01:19:44.360 --> 01:19:48.360]   It's like, if you really think, if you're Bob Iger, you can run for president.
[01:19:48.360 --> 01:19:51.360]   You have anything to offer people or they want you to be president, like, you're pretty out of touch.
[01:19:51.360 --> 01:19:59.360]   So I like that. But, you know, big picture, I'm really—and I'm not just saying this because it, you know,
[01:19:59.360 --> 01:20:02.360]   aligns with my interests of some kind. I mean it.
[01:20:02.360 --> 01:20:07.360]   I'm worried about the pressure that's going to be brought to bear on that platform, on this platform, on X,
[01:20:07.360 --> 01:20:12.360]   because it's the only one, the only big one, you know, huge one, international one, tens of millions of people,
[01:20:12.360 --> 01:20:18.360]   hundreds of millions of people, like, they have to—they, meaning the people who would like to maintain the status quo,
[01:20:18.360 --> 01:20:20.360]   kind of have to shut it down.
[01:20:20.360 --> 01:20:30.360]   And I am just so hoping that, you know, I can help in any way and that all decent people, whatever their views,
[01:20:30.360 --> 01:20:37.360]   add their voice to a chorus that says, "No, you can't shut down the one big free speech platform in the world.
[01:20:37.360 --> 01:20:39.360]   You can't do that because then it's just dictatorship.
[01:20:39.360 --> 01:20:42.360]   You're not free if you don't have free information and you can't say what you really think."
[01:20:42.360 --> 01:20:46.360]   You were subject to a lot of these advertising attacks.
[01:20:46.360 --> 01:20:50.360]   They went after Fox and your show specifically, and it worked.
[01:20:50.360 --> 01:20:53.360]   They got people to not advertise on your show.
[01:20:53.360 --> 01:20:54.360]   Oh, yeah.
[01:20:54.360 --> 01:20:58.360]   So these advertising boycotts do work, Media Matters, whatever, you know, whoever's doing them.
[01:20:58.360 --> 01:21:05.360]   So I'm curious what you see the business model being for your show and then how you hope to be resistant to it.
[01:21:05.360 --> 01:21:10.360]   Are you going to just go straight up subscription and hope that half your audience or 10 percent of your audience pays
[01:21:10.360 --> 01:21:12.360]   and you put half out for free, half out for sub?
[01:21:12.360 --> 01:21:13.360]   What are you thinking?
[01:21:13.360 --> 01:21:16.360]   You've got to have a subscription component to it.
[01:21:16.360 --> 01:21:24.360]   And, of course, we're selling ads against our content, and we'll be doing that on X and on TuckerCarlson.com.
[01:21:24.360 --> 01:21:28.360]   We'll be hosting the subscription part of it.
[01:21:28.360 --> 01:21:30.360]   Of course, we're selling ads.
[01:21:30.360 --> 01:21:32.360]   We'll have network ads, too.
[01:21:32.360 --> 01:21:35.360]   I mean, there are lots of different things you can do.
[01:21:35.360 --> 01:21:40.360]   But in the end, you have to have some subscription component if you're going to do it at scale.
[01:21:40.360 --> 01:21:43.360]   I mean, we brought our whole staff, almost our whole staff from Fox with us.
[01:21:43.360 --> 01:21:45.360]   So we've got a bunch of people.
[01:21:45.360 --> 01:21:50.360]   We've got bigger ambitions than have been on display so far.
[01:21:50.360 --> 01:21:55.360]   And, you know, you've got to have more information.
[01:21:55.360 --> 01:22:00.360]   And I would just say this about just having been in this one business my whole life.
[01:22:00.360 --> 01:22:03.360]   The range of stories is the problem.
[01:22:03.360 --> 01:22:06.360]   It's not that the stories are all totally dishonest.
[01:22:06.360 --> 01:22:08.360]   Some are dishonest, but some are not.
[01:22:08.360 --> 01:22:14.360]   They're technically true, but they're taken from such a small pot of stories that it's like it's crazy.
[01:22:14.360 --> 01:22:16.360]   There's all this stuff going on.
[01:22:16.360 --> 01:22:23.360]   I mean, I was completely obsessed and remain obsessed with the industrial sabotage of the Nord Stream pipeline.
[01:22:23.360 --> 01:22:27.360]   It's like that's a major historical event.
[01:22:27.360 --> 01:22:29.360]   Like you just ended the EU.
[01:22:29.360 --> 01:22:33.360]   You just hobbled the economic engine of Europe, which is Germany.
[01:22:33.360 --> 01:22:35.360]   And like who did that?
[01:22:35.360 --> 01:22:37.360]   And there were like very few stories on that.
[01:22:37.360 --> 01:22:39.360]   That's like a very big deal.
[01:22:39.360 --> 01:22:42.360]   And again, back to the curiosity thing, but it's more than that.
[01:22:42.360 --> 01:22:46.360]   It's like the people sort of know where not to go.
[01:22:46.360 --> 01:22:50.360]   And I just feel like that's first of all that's soul death.
[01:22:50.360 --> 01:22:54.360]   You're not a free man if you're that constrained in your thinking.
[01:22:54.360 --> 01:22:57.360]   If you're letting somebody else tell you what you're allowed to think.
[01:22:57.360 --> 01:23:00.360]   How can your wife even sleep with you at that point?
[01:23:00.360 --> 01:23:03.360]   No one can respect you if you live like that, A.
[01:23:03.360 --> 01:23:05.360]   B, you can't have a democracy.
[01:23:05.360 --> 01:23:07.360]   You can't have a free country under those circumstances.
[01:23:07.360 --> 01:23:09.360]   So I don't think I'm overstating it.
[01:23:09.360 --> 01:23:14.360]   And so we're going to try to, I hope, be one of many different similar efforts
[01:23:14.360 --> 01:23:18.360]   to just add to the sum total of information and analysis of that information.
[01:23:18.360 --> 01:23:21.360]   Well, the end of democracy and sex would be kind of rough, I think.
[01:23:21.360 --> 01:23:22.360]   Yeah, go ahead, Shubham.
[01:23:22.360 --> 01:23:23.360]   Yeah.
[01:23:23.360 --> 01:23:27.360]   I have a final question, and I'm not trying to be a conspiracy theorist.
[01:23:27.360 --> 01:23:33.360]   But if one of the presidential candidates called you and said,
[01:23:33.360 --> 01:23:36.360]   Tucker, be my VP.
[01:23:36.360 --> 01:23:38.360]   And replace Kamala Harris?
[01:23:38.360 --> 01:23:40.360]   I mean, I don't think I could do that.
[01:23:40.360 --> 01:23:41.360]   She's a historic first.
[01:23:41.360 --> 01:23:45.360]   Would you consider it if Donald Trump gave you that call?
[01:23:45.360 --> 01:23:47.360]   Well, of course I would consider it.
[01:23:47.360 --> 01:23:50.360]   I mean, you have no idea how open-minded I am.
[01:23:50.360 --> 01:23:53.360]   I don't consider anything, actually.
[01:23:53.360 --> 01:23:58.360]   But I mean, the truth is I kind of don't respect people who do stuff like that.
[01:23:58.360 --> 01:24:01.360]   I really believe I've got a lot of theories which I will not inflict on you,
[01:24:01.360 --> 01:24:04.360]   but which I do inflict on my own four children.
[01:24:04.360 --> 01:24:08.360]   And my main theory of life is that you should do what you were designed to do.
[01:24:08.360 --> 01:24:10.360]   I don't believe this whole you can be whatever you want to be thing.
[01:24:10.360 --> 01:24:12.360]   I think it's an absurd lie.
[01:24:12.360 --> 01:24:14.360]   I think we're made for certain things.
[01:24:14.360 --> 01:24:15.360]   We have certain aptitudes.
[01:24:15.360 --> 01:24:16.360]   They're inborn.
[01:24:16.360 --> 01:24:20.360]   We can hone them, and we need to through practice, repetition.
[01:24:20.360 --> 01:24:26.360]   But it's very--I think it's a sign of hubris, which is always the death,
[01:24:26.360 --> 01:24:30.360]   particularly of men, hubris, thinking you have more power than you do.
[01:24:30.360 --> 01:24:33.360]   When midlife, you're like, well, actually, what I really want to do is direct.
[01:24:33.360 --> 01:24:35.360]   It's like you just won best actress, honey.
[01:24:35.360 --> 01:24:37.360]   Go back to acting.
[01:24:37.360 --> 01:24:39.360]   And I've never been involved in politics.
[01:24:39.360 --> 01:24:42.360]   I've never--I haven't even voted in all elections.
[01:24:42.360 --> 01:24:43.360]   I'm serious.
[01:24:43.360 --> 01:24:51.360]   So the idea that I'm going to at 54 run for national office is a little--you know what I mean?
[01:24:51.360 --> 01:24:53.360]   I don't take myself quite that seriously.
[01:24:53.360 --> 01:24:55.360]   I mean I can't imagine doing something like that.
[01:24:55.360 --> 01:24:56.360]   I'm just being honest.
[01:24:56.360 --> 01:25:06.360]   >>Corey: I think this question is that if we're--at least the Republicans will now prove that we've lived in a 12-year era of a non-traditional candidate.
[01:25:06.360 --> 01:25:12.360]   That was essentially a media personality that was able to then curate a plurality of support, right?
[01:25:12.360 --> 01:25:13.360]   >>Joe: That's right.
[01:25:13.360 --> 01:25:15.360]   >>Corey: And there's going to be something that comes after him.
[01:25:15.360 --> 01:25:22.360]   And so I'm just trying to get a sense of if it's not you, it's probably, to be very honest, somebody like you, right?
[01:25:22.360 --> 01:25:23.360]   >>Joe: Yes.
[01:25:23.360 --> 01:25:25.360]   >>Corey: Just talk to us about that for just a second.
[01:25:25.360 --> 01:25:33.360]   >>Joe: I completely agree with you and I love your characterization of Trump as someone who's from a media background because that's closer to--I'm not diminishing.
[01:25:33.360 --> 01:25:34.360]   I'm from media backgrounds.
[01:25:34.360 --> 01:25:35.360]   I'm not attacking it.
[01:25:35.360 --> 01:25:41.360]   But that's a lot closer to the truth than most characterizations, casino magnet, developer.
[01:25:41.360 --> 01:25:42.360]   He's a media guy.
[01:25:42.360 --> 01:25:44.360]   Yeah, he had lots of businesses.
[01:25:44.360 --> 01:25:45.360]   But he was a media guy.
[01:25:45.360 --> 01:25:47.360]   He had the top show on NBC.
[01:25:47.360 --> 01:25:50.360]   And so I think you're absolutely right.
[01:25:50.360 --> 01:25:55.360]   My concern is that--and I have pure contempt for the professional political class.
[01:25:55.360 --> 01:25:56.360]   I've written a book about it.
[01:25:56.360 --> 01:25:59.360]   I've expressed it daily for a number of years now and it's real.
[01:25:59.360 --> 01:26:11.360]   However, I think that there--these are complex systems and it's better to have someone who understands the systems in an ideal world administer the systems because he's more effective at doing so.
[01:26:11.360 --> 01:26:30.360]   And I also think that once you decide that like, hey, let's just go crazy and you couple that with true social disorder, like you get to a place where you can't buy anything at CVS because it's chained up because shoplifting has been legalized as it has been in California.
[01:26:30.360 --> 01:26:37.360]   What you're going to get is fascism because people can't live in that--they can't live with chaos.
[01:26:37.360 --> 01:26:41.360]   Like that's the one thing they can't deal with and I've covered a couple of wars and that was my main conclusion.
[01:26:41.360 --> 01:26:43.360]   The main problem with war is not that people get killed.
[01:26:43.360 --> 01:26:49.360]   It's that people have to live with total uncertainty and craziness and that's incompatible with what people want.
[01:26:49.360 --> 01:26:51.360]   Like that's the worst thing.
[01:26:51.360 --> 01:26:52.360]   You know, we're all going to die.
[01:26:52.360 --> 01:26:54.360]   Dying is not the worst thing.
[01:26:54.360 --> 01:26:56.360]   The worst thing is living in chaos.
[01:26:56.360 --> 01:26:58.360]   And we're starting to live in chaos.
[01:26:58.360 --> 01:27:01.360]   And so the return to order is what scares me.
[01:27:01.360 --> 01:27:03.360]   I think it would be very easy and I do think Gavin Newsom is a fascist.
[01:27:03.360 --> 01:27:10.360]   I think he's the kind of person who would have no problem, no hesitation about using the DOJ to imprison his political opponents.
[01:27:10.360 --> 01:27:14.360]   Now Biden is imprisoning his political opponents but at least they're lying about it.
[01:27:14.360 --> 01:27:20.360]   Gavin Newsom is the kind of person who would be like, "Well, yeah, you're a threat to the general order and you're going to jail."
[01:27:20.360 --> 01:27:26.360]   And I think because we're in a moment of chaos right now, people kind of want that actually.
[01:27:26.360 --> 01:27:34.360]   I think one of the purposes of degrading and confusing our society is to make way for authoritarianism even more than we have now.
[01:27:34.360 --> 01:27:37.360]   So that kind of freaks me out actually.
[01:27:37.360 --> 01:27:50.360]   I personally, what I would really like is a kind of colorless, you know, boring, non-charismatic like Gerald Ford, Mike Pence without their views.
[01:27:50.360 --> 01:27:52.360]   Both of those men were bad men in my opinion.
[01:27:52.360 --> 01:28:01.360]   But someone like that who could govern without making it about himself and restore the country to a sense of rules-based order.
[01:28:01.360 --> 01:28:05.360]   That's what I really want but I'm probably going to be denied that.
[01:28:05.360 --> 01:28:12.360]   Tucker, I feel like one of the other consequences of the way things have gone is that the solution has always been to throw more money at the problem.
[01:28:12.360 --> 01:28:15.360]   And we've got this kind of out of control fiscal condition.
[01:28:15.360 --> 01:28:22.360]   What's your point of view on the fiscal condition of the US as a priority, of the federal government that is as a priority?
[01:28:22.360 --> 01:28:28.360]   And, you know, do we need to shrink government, shrink overall discretionary spending?
[01:28:28.360 --> 01:28:29.360]   Do we need to cut these entitlements?
[01:28:29.360 --> 01:28:30.360]   Do we need to do it all?
[01:28:30.360 --> 01:28:37.360]   And how do we get there given that everyone gets elected by telling people, "I'm going to give you more stuff."
[01:28:37.360 --> 01:28:42.360]   And then this just kind of cascades for decades until eventually bad shit happens.
[01:28:42.360 --> 01:28:44.360]   Well, who's going to buy our debt?
[01:28:44.360 --> 01:28:46.360]   I mean, that's like, it's scary.
[01:28:46.360 --> 01:28:48.360]   It's so scary.
[01:28:48.360 --> 01:28:51.360]   You know, this is, of course, the problem with democracy.
[01:28:51.360 --> 01:28:56.360]   I mean, I think since, you know, from the Roman Republic until 1776, like how many democracies were there?
[01:28:56.360 --> 01:28:57.360]   Exactly.
[01:28:57.360 --> 01:28:59.360]   Let me do the math on that.
[01:28:59.360 --> 01:29:02.360]   Approximately zero in that range.
[01:29:02.360 --> 01:29:07.360]   And this was the critique, of course, in Europe of democracy at the time.
[01:29:07.360 --> 01:29:14.360]   Not that it gave too much freedom to the average person, but that it would result in tyranny.
[01:29:14.360 --> 01:29:23.360]   And when the majority discovered it could steal the goods of whomever it wanted legally, legally, you would wind up with dictatorship.
[01:29:23.360 --> 01:29:27.360]   I mean, this is like a very well-tried path, but I mean, I hope it doesn't get to that.
[01:29:27.360 --> 01:29:29.360]   I'm not making an argument against democracy.
[01:29:29.360 --> 01:29:34.360]   I'm just saying it's a little bit harder to perpetuate than we thought that it was.
[01:29:34.360 --> 01:29:38.360]   And this is kind of the fear that people have had for hundreds of years.
[01:29:38.360 --> 01:29:42.360]   The fiscal condition is the manifestation of that structural problem.
[01:29:42.360 --> 01:29:45.360]   Yes, yes, that's exactly right.
[01:29:45.360 --> 01:29:50.360]   Let's end on this Elon clip from yesterday, just about virtue.
[01:29:50.360 --> 01:29:52.360]   I'd like to get your reaction to this, Tucker.
[01:29:52.360 --> 01:29:58.360]   Tesla has done more to help the environment than all other companies combined.
[01:29:58.360 --> 01:30:06.360]   Would it be fair to say that, therefore, as a leader of the company, I've done more for the environment than any single human on Earth?
[01:30:06.360 --> 01:30:08.360]   How do you feel about that?
[01:30:08.360 --> 01:30:11.360]   How do I feel about that?
[01:30:11.360 --> 01:30:16.360]   Yeah, no, I'm asking you personally how you feel about that, because we were talking about power and influence and...
[01:30:16.360 --> 01:30:21.360]   I'm saying what I care about is the reality of goodness, not the perception of it.
[01:30:21.360 --> 01:30:27.360]   And what I see all over the place is people who care about looking good while doing evil.
[01:30:27.360 --> 01:30:28.360]   Fuck them.
[01:30:28.360 --> 01:30:31.360]   Your thoughts on virtue signaling versus reality?
[01:30:31.360 --> 01:30:41.360]   It was like the hand of God massaging my central nervous system, like every erogenous zone was just heightened to awareness.
[01:30:41.360 --> 01:30:43.360]   I couldn't agree more with...
[01:30:43.360 --> 01:30:45.360]   That's a full release.
[01:30:45.360 --> 01:30:47.360]   I'm so dirty, I'm sorry.
[01:30:47.360 --> 01:30:52.360]   No, but I so vehemently agree, and I don't even like electric cars.
[01:30:52.360 --> 01:30:53.360]   I don't think they help the environment.
[01:30:53.360 --> 01:30:55.360]   I didn't even agree with that before.
[01:30:55.360 --> 01:31:02.360]   I just love his point, which is the point, which is what actually matters is what you do.
[01:31:02.360 --> 01:31:03.360]   It's not what you think.
[01:31:03.360 --> 01:31:05.360]   What matters is helping other people.
[01:31:05.360 --> 01:31:11.360]   And I also think that the fact that Andrew Ross Sorkin has a television job shows this is not a meritocracy.
[01:31:11.360 --> 01:31:13.360]   Like, I don't know how he got that.
[01:31:13.360 --> 01:31:17.360]   But his response to that was like, he didn't even...
[01:31:17.360 --> 01:31:18.360]   He can't even...
[01:31:18.360 --> 01:31:24.360]   I'll just kind of say this is like so inside baseball, but it's literally what I do for a living, have done all my life.
[01:31:24.360 --> 01:31:25.360]   That's what we do, Tucker.
[01:31:25.360 --> 01:31:26.360]   You and I do this.
[01:31:26.360 --> 01:31:29.360]   The key to interviewing people, yes, is to listen to them.
[01:31:29.360 --> 01:31:31.360]   Larry King, I used to fill in for Larry King at CNN.
[01:31:31.360 --> 01:31:33.360]   He was number one, 9pm, all this stuff.
[01:31:33.360 --> 01:31:36.360]   The first time I filled in for Larry King, there was no research on the guests.
[01:31:36.360 --> 01:31:38.360]   I had two guests, no research.
[01:31:38.360 --> 01:31:41.360]   And I said to the producer, Larry was like in Cabo with wife number seven.
[01:31:41.360 --> 01:31:43.360]   And I said, where's the research?
[01:31:43.360 --> 01:31:44.360]   Oh, Larry doesn't do research.
[01:31:44.360 --> 01:31:46.360]   So I do the show and fine.
[01:31:46.360 --> 01:31:48.360]   And I called him after, I was like, dude, you do no research?
[01:31:48.360 --> 01:31:52.360]   He goes, and he was number one, most dominant figure in cable news by far.
[01:31:52.360 --> 01:31:55.360]   And he goes, no, I just listen.
[01:31:55.360 --> 01:31:57.360]   And when someone says something weird, I pause.
[01:31:57.360 --> 01:31:59.360]   And I said, wait, wait a second, wait a second, wait a second.
[01:31:59.360 --> 01:32:01.360]   You killed someone in 1962?
[01:32:01.360 --> 01:32:02.360]   Why'd you kill him?
[01:32:02.360 --> 01:32:03.360]   And follow up.
[01:32:03.360 --> 01:32:06.360]   You're present in the interview.
[01:32:06.360 --> 01:32:07.360]   Yes.
[01:32:07.360 --> 01:32:09.360]   And then you follow up based on what this guest said.
[01:32:09.360 --> 01:32:10.360]   Notice how Jason's interrupting you right now.
[01:32:10.360 --> 01:32:11.360]   Sorry, go ahead, Tucker.
[01:32:11.360 --> 01:32:12.360]   No, I love it.
[01:32:12.360 --> 01:32:13.360]   I love it.
[01:32:13.360 --> 01:32:14.360]   I'm listening to what Tucker said and reflecting back to what he said.
[01:32:14.360 --> 01:32:15.360]   Because what?
[01:32:15.360 --> 01:32:16.360]   Because game realizes game.
[01:32:16.360 --> 01:32:17.360]   That's right.
[01:32:17.360 --> 01:32:18.360]   It's a co-equate.
[01:32:18.360 --> 01:32:21.360]   J-Cal's treating this as validation.
[01:32:21.360 --> 01:32:22.360]   Absolutely.
[01:32:22.360 --> 01:32:25.360]   There's actually a story to this, Tucker, that you don't need to know about.
[01:32:25.360 --> 01:32:28.360]   I can tell it's tantalizing.
[01:32:28.360 --> 01:32:29.360]   It is.
[01:32:29.360 --> 01:32:30.360]   Oh, it is.
[01:32:30.360 --> 01:32:31.360]   It is.
[01:32:31.360 --> 01:32:32.360]   Trust me.
[01:32:32.360 --> 01:32:33.360]   Well, let's end here.
[01:32:33.360 --> 01:32:37.360]   You know, we have a lot of fans of not only, there's about 18% crossover between All In
[01:32:37.360 --> 01:32:38.360]   and your work, Tucker.
[01:32:38.360 --> 01:32:43.360]   And they send me images of you out there and about in the world.
[01:32:43.360 --> 01:32:46.360]   And I just got this one in my DMs.
[01:32:46.360 --> 01:32:48.360]   I don't know what's going on.
[01:32:48.360 --> 01:32:49.360]   What's going on here?
[01:32:49.360 --> 01:32:50.360]   Is that a party movie?
[01:32:50.360 --> 01:32:51.360]   Is that a party movie?
[01:32:51.360 --> 01:32:52.360]   I'm sure where you guys are.
[01:32:52.360 --> 01:32:53.360]   This is the bromance is complete.
[01:32:53.360 --> 01:32:55.360]   We got to run out of these yet?
[01:32:55.360 --> 01:32:56.360]   That is so.
[01:32:56.360 --> 01:32:57.360]   You know what?
[01:32:57.360 --> 01:32:58.360]   I'm not.
[01:32:58.360 --> 01:32:59.360]   That's kind of hunky.
[01:32:59.360 --> 01:33:00.360]   I'm just being honest.
[01:33:00.360 --> 01:33:01.360]   Yeah.
[01:33:01.360 --> 01:33:02.360]   Yeah.
[01:33:02.360 --> 01:33:03.360]   It's a lot going on here.
[01:33:03.360 --> 01:33:04.360]   Well, listen, we got to wrap.
[01:33:04.360 --> 01:33:05.360]   It's like a Vineyard Vine t-shirt.
[01:33:05.360 --> 01:33:07.360]   The bromance is now complete.
[01:33:07.360 --> 01:33:09.360]   Tucker, we'd love to have you back on a regular basis.
[01:33:09.360 --> 01:33:10.360]   Actually, I think you.
[01:33:10.360 --> 01:33:11.360]   That was so fun.
[01:33:11.360 --> 01:33:12.360]   That was super fun.
[01:33:12.360 --> 01:33:13.360]   That was great.
[01:33:13.360 --> 01:33:14.360]   And I'm sorry for talking too much.
[01:33:14.360 --> 01:33:17.360]   You guys spun me up into a frenzy, but I appreciate it.
[01:33:17.360 --> 01:33:18.360]   No, no.
[01:33:18.360 --> 01:33:19.360]   Great guests.
[01:33:19.360 --> 01:33:20.360]   I thought you were number one in media.
[01:33:20.360 --> 01:33:21.360]   I was so fun.
[01:33:21.360 --> 01:33:22.360]   I really enjoyed it.
[01:33:22.360 --> 01:33:23.360]   Thanks for having me.
[01:33:23.360 --> 01:33:24.360]   Thank you.
[01:33:24.360 --> 01:33:25.360]   Really, really good.
[01:33:25.360 --> 01:33:26.360]   Thank you very much, Tucker.
[01:33:26.360 --> 01:33:27.360]   Thank you.
[01:33:27.360 --> 01:33:28.360]   Thanks, guys.
[01:33:28.360 --> 01:33:29.360]   That was amazing.
[01:33:29.360 --> 01:33:30.360]   Good luck with the new launch.
[01:33:30.360 --> 01:33:32.360]   Everybody go to Tucker Farlson dot com.
[01:33:32.360 --> 01:33:35.360]   And when the subs come out on day one, give them a sub.
[01:33:35.360 --> 01:33:40.360]   Okay, so Tucker sub didn't come out exactly how I wanted.
[01:33:40.360 --> 01:33:43.360]   You can stop.
[01:33:43.360 --> 01:33:46.360]   Are you going to be a sub for Tucker's tax?
[01:33:46.360 --> 01:33:47.360]   Are you something?
[01:33:47.360 --> 01:33:50.360]   I'm going to Dom for Tucker.
[01:33:50.360 --> 01:33:51.360]   All right.
[01:33:51.360 --> 01:33:52.360]   Thanks, Doc.
[01:33:52.360 --> 01:33:53.360]   See you guys.
[01:33:53.360 --> 01:33:54.360]   What do you guys think?
[01:33:54.360 --> 01:33:55.360]   That was fun.
[01:33:55.360 --> 01:33:56.360]   That was great.
[01:33:56.360 --> 01:33:57.360]   Yeah.
[01:33:57.360 --> 01:33:58.360]   Let's show you.
[01:33:58.360 --> 01:33:59.360]   I'm a sucker for Tucker.
[01:33:59.360 --> 01:34:00.360]   I'll be honest.
[01:34:00.360 --> 01:34:01.360]   Like, can we get him on group chat?
[01:34:01.360 --> 01:34:02.360]   I mean, can you imagine?
[01:34:02.360 --> 01:34:03.360]   I can sleep.
[01:34:03.360 --> 01:34:04.360]   That's a fun guy.
[01:34:04.360 --> 01:34:05.360]   Great guy.
[01:34:05.360 --> 01:34:06.360]   He's a great entertainer.
[01:34:06.360 --> 01:34:07.360]   And you know what?
[01:34:07.360 --> 01:34:11.360]   He's into I know he's right wing conservative, but he's actually, I think, a first principle
[01:34:11.360 --> 01:34:13.360]   thinker who thinks for himself.
[01:34:13.360 --> 01:34:14.360]   We didn't get into January six.
[01:34:14.360 --> 01:34:15.360]   We ran out of time.
[01:34:15.360 --> 01:34:18.960]   But, you know, he was, you know, not happy about that.
[01:34:18.960 --> 01:34:20.520]   He wasn't happy about election denial.
[01:34:20.520 --> 01:34:21.960]   I think he's intellectually rigorous.
[01:34:21.960 --> 01:34:22.960]   Wow.
[01:34:22.960 --> 01:34:23.960]   J.K.
[01:34:23.960 --> 01:34:24.960]   What do you think?
[01:34:24.960 --> 01:34:25.960]   I really liked it.
[01:34:25.960 --> 01:34:29.840]   I like talking to people that have opinions that forced me to, like, actually rethink
[01:34:29.840 --> 01:34:31.480]   about how I think.
[01:34:31.480 --> 01:34:38.480]   And it was, I think, the most impactful thing that he said to me, which touches upon my
[01:34:38.480 --> 01:34:45.440]   own life, was just how one feels when you have a little bit too much too early versus
[01:34:45.440 --> 01:34:49.040]   grinding slowly and compounding success over many years.
[01:34:49.040 --> 01:34:53.960]   I think it does create in moments an element of self sabotage.
[01:34:53.960 --> 01:34:55.240]   I've lived it in my own life.
[01:34:55.240 --> 01:34:57.360]   So that totally resonated.
[01:34:57.360 --> 01:35:03.900]   This idea that there's just so much abundance that causes people to not really fight over
[01:35:03.900 --> 01:35:06.720]   the big issues and then fight over the fringe issues.
[01:35:06.720 --> 01:35:09.720]   I do think that there's an element of huge truth in that.
[01:35:09.720 --> 01:35:11.280]   It was really, really good.
[01:35:11.280 --> 01:35:16.360]   I think like that this is probably one of the very few ones that we've done that I would
[01:35:16.360 --> 01:35:17.360]   listen to over.
[01:35:17.360 --> 01:35:18.360]   Totally.
[01:35:18.360 --> 01:35:20.640]   I'd say that was like one of my favorite episodes or probably my favorite episode.
[01:35:20.640 --> 01:35:21.640]   Definitely a top five.
[01:35:21.640 --> 01:35:22.640]   Just as a listener.
[01:35:22.640 --> 01:35:24.640]   I just enjoyed listening the whole fucking time.
[01:35:24.640 --> 01:35:25.640]   This is the thing.
[01:35:25.640 --> 01:35:28.000]   I just wanted to, I could hear him talk for hours probably.
[01:35:28.000 --> 01:35:29.000]   I know.
[01:35:29.000 --> 01:35:30.000]   I didn't want to talk.
[01:35:30.000 --> 01:35:31.000]   I didn't have anything to say.
[01:35:31.000 --> 01:35:32.000]   I just wanted to chill and hang out.
[01:35:32.000 --> 01:35:33.000]   Like listen to.
[01:35:33.000 --> 01:35:34.000]   It was great.
[01:35:34.000 --> 01:35:35.000]   Yeah, me too.
[01:35:35.000 --> 01:35:36.000]   Have you done Tucker's show, Chamath, yet?
[01:35:36.000 --> 01:35:38.760]   I did do the show in December, which I couldn't do.
[01:35:38.760 --> 01:35:40.560]   And at some point in the spring, I will do the show.
[01:35:40.560 --> 01:35:43.160]   But then I was like, can you come on our show?
[01:35:43.160 --> 01:35:44.160]   And he was like, yeah.
[01:35:44.160 --> 01:35:45.160]   There you go.
[01:35:45.160 --> 01:35:46.160]   It's amazing.
[01:35:46.160 --> 01:35:47.160]   It's amazing.
[01:35:47.160 --> 01:35:48.160]   Yeah, that was amazing.
[01:35:48.160 --> 01:35:49.160]   Sax, didn't you like go on his show?
[01:35:49.160 --> 01:35:51.920]   No, I've been on Tucker's show a couple of times when it was on Fox.
[01:35:51.920 --> 01:35:56.600]   And one appearance was about Chasey Boudin, you know, the DA that we got tossed out.
[01:35:56.600 --> 01:35:59.600]   And then I went on a couple other times.
[01:35:59.600 --> 01:36:01.560]   I think one was about the economy.
[01:36:01.560 --> 01:36:04.440]   I was on a show a couple of times for, you know, short segments.
[01:36:04.440 --> 01:36:07.960]   But anyway, I thought what was interesting about this conversation was that it was much
[01:36:07.960 --> 01:36:11.040]   more philosophical than I was expecting.
[01:36:11.040 --> 01:36:14.560]   We touched on a few like policy issues.
[01:36:14.560 --> 01:36:20.840]   But we spent most of the time talking about his deeper clinical diagnosis of American
[01:36:20.840 --> 01:36:24.800]   culture and American discourse.
[01:36:24.800 --> 01:36:29.480]   And I would say that one of the things that's maybe unique about his take, and I mean, definitely
[01:36:29.480 --> 01:36:32.920]   different than mine, is it's much more psychological than
[01:36:32.920 --> 01:36:35.640]   metaphysical, even metaphysical and psychological.
[01:36:35.640 --> 01:36:40.760]   I mean, like, I don't really ask too many questions about why people believe what they
[01:36:40.760 --> 01:36:41.760]   believe.
[01:36:41.760 --> 01:36:45.000]   I just sort of take it as a given and then discuss whether they're right or wrong, whereas
[01:36:45.000 --> 01:36:51.440]   he actually sort of psychoanalyzes why people have the views that they have.
[01:36:51.440 --> 01:36:53.440]   And you know what I mean?
[01:36:53.440 --> 01:36:54.840]   It's actually kind of interesting.
[01:36:54.840 --> 01:36:57.480]   That whole methodology for whatever it's worth resonates with me.
[01:36:57.480 --> 01:37:01.920]   I mean, I think it's very, it's the reason early on, I really gravitated to Rene Girard,
[01:37:01.920 --> 01:37:05.600]   because I thought it demonstrated a lot of how people behave to me in a language that
[01:37:05.600 --> 01:37:06.600]   I could understand.
[01:37:06.600 --> 01:37:11.200]   So when Tucker describes problems in this context, to me, it's very powerful, just because
[01:37:11.200 --> 01:37:17.480]   that is how I kind of frame things as I think humans are driven by psychological incentives.
[01:37:17.480 --> 01:37:23.560]   And I think that there is something very worth exploring here, which is these Western cultures
[01:37:23.560 --> 01:37:27.080]   get so prosperous, that the big things we don't fight over.
[01:37:27.080 --> 01:37:30.320]   So then we fight over the little things, or we have to invent things to fight over.
[01:37:30.320 --> 01:37:33.600]   And the virtue signaling he kind of brings up in his own personal experience, which is,
[01:37:33.600 --> 01:37:38.160]   hey, I go to Jackson Hole, I'm getting accosted, you know, in line at the lift, people's wives
[01:37:38.160 --> 01:37:41.400]   are upset at me, the husbands are upset at me, the hedge fund people, these are all people
[01:37:41.400 --> 01:37:45.000]   of incredible wealth, incredible privilege, who have extra cycles.
[01:37:45.000 --> 01:37:50.440]   And instead of having a debate over the issues, in good faith, full contact debate, like we
[01:37:50.440 --> 01:37:53.200]   do here, they just want to vilify people.
[01:37:53.200 --> 01:37:56.960]   And you know, I tried to listen to him and say, Hey, what does he get right here?
[01:37:56.960 --> 01:37:58.880]   Now, I obviously think he's wrong about climate change.
[01:37:58.880 --> 01:38:00.360]   I thought that was bonkers.
[01:38:00.360 --> 01:38:04.920]   But I do think he understands human nature pretty well from doing 30 years of interviews.
[01:38:04.920 --> 01:38:09.240]   Yeah, it's definitely worth exploring Jason, this idea that the more successful people
[01:38:09.240 --> 01:38:12.120]   get, the more self loathing there is.
[01:38:12.120 --> 01:38:15.160]   And then it manifests in some, you know, really destructive ways.
[01:38:15.160 --> 01:38:18.920]   I think especially if you don't have a foundation, once you get successful enough, there's very
[01:38:18.920 --> 01:38:19.920]   little progress.
[01:38:19.920 --> 01:38:22.400]   I think a lot of human happiness comes from progress.
[01:38:22.400 --> 01:38:23.400]   Progress exactly.
[01:38:23.400 --> 01:38:24.400]   Right?
[01:38:24.400 --> 01:38:25.400]   Where's the purpose?
[01:38:25.400 --> 01:38:28.680]   Well, they're just there's just no higher order bits.
[01:38:28.680 --> 01:38:31.000]   You buy the house, that's a huge accomplishment.
[01:38:31.000 --> 01:38:34.720]   But then when you buy your second house or third house, I think the diminishing returns
[01:38:34.720 --> 01:38:35.720]   are severe.
[01:38:35.720 --> 01:38:36.720]   Yeah.
[01:38:36.720 --> 01:38:37.960]   And then and then these things are just baggage.
[01:38:37.960 --> 01:38:38.960]   They're like albatrosses.
[01:38:38.960 --> 01:38:41.920]   Yeah, and then you're spending all this time dealing with headaches.
[01:38:41.920 --> 01:38:44.120]   You know, what's your progress?
[01:38:44.120 --> 01:38:45.120]   What's your problem?
[01:38:45.120 --> 01:38:49.280]   What's the purpose of my I mean, sacks when you bought your sixth house, I mean,
[01:38:49.280 --> 01:38:54.200]   I'm just busy reorganizing the US political establishment.
[01:38:54.200 --> 01:38:56.480]   So I think he's making progress in his own way.
[01:38:56.480 --> 01:38:57.480]   Saks is making progress.
[01:38:57.480 --> 01:38:58.480]   Yeah.
[01:38:58.480 --> 01:38:59.480]   Yeah.
[01:38:59.480 --> 01:39:04.680]   I you know, he's where does he sit on the political spectrum today, Saks because you
[01:39:04.680 --> 01:39:09.760]   and he are both Yeah, because you're both kind of outcasts in the Republican Party now,
[01:39:09.760 --> 01:39:10.760]   right?
[01:39:10.760 --> 01:39:16.600]   Like I would describe Tucker as probably the most influential populist in you know, on
[01:39:16.600 --> 01:39:18.160]   the on the right.
[01:39:18.160 --> 01:39:19.760]   So there's a sort of populism.
[01:39:19.760 --> 01:39:25.880]   Well, there's a populism of the left that I guess was Bernie Sanders before he told,
[01:39:25.880 --> 01:39:30.240]   you know, maybe like, five, eight years ago, whatever, describe us, describe for the audience,
[01:39:30.240 --> 01:39:34.240]   your perception of populism, you know, on the left and the right, what they share and
[01:39:34.240 --> 01:39:38.760]   what they don't share kind of thing, because this does seem to be the emerging party, I
[01:39:38.760 --> 01:39:42.800]   think we would all agree, is that people are sick of these extremes, and they want something
[01:39:42.800 --> 01:39:43.800]   new.
[01:39:43.800 --> 01:39:46.800]   And the something new seems to be populism.
[01:39:46.800 --> 01:39:52.280]   Well, one way to think about populism is just democracy.
[01:39:52.280 --> 01:39:56.160]   Populism is the word that the elite gives to democracy they don't like.
[01:39:56.160 --> 01:40:00.960]   So for example, vast majority of the country wants our borders sealed.
[01:40:00.960 --> 01:40:02.920]   For some reason, the elites don't want that.
[01:40:02.920 --> 01:40:04.840]   So that's labeled populism.
[01:40:04.840 --> 01:40:09.000]   I think the vast majority of the country regrets the forever wars in the Middle East and doesn't
[01:40:09.000 --> 01:40:13.800]   want us getting involved in more wars, the sort of hyper interventionism.
[01:40:13.800 --> 01:40:17.240]   I think that's like a major part of the platform.
[01:40:17.240 --> 01:40:21.600]   And then of course, I think the third big area of reevaluation was around our free trade
[01:40:21.600 --> 01:40:22.600]   policies.
[01:40:22.600 --> 01:40:27.880]   You know, they really ended up hollowing out America's industrial capability and exported
[01:40:27.880 --> 01:40:32.680]   a lot of manufacturing jobs, globalization to China, globalization.
[01:40:32.680 --> 01:40:35.560]   So yeah, I think on the right.
[01:40:35.560 --> 01:40:42.080]   Big picture, I would say that populism is a nationalist reaction to the hyper globalization
[01:40:42.080 --> 01:40:45.200]   that happened that was encouraged by the elites over the past few decades.
[01:40:45.200 --> 01:40:49.560]   And he hit on that he said, you know, at some point, are the politicians going to match
[01:40:49.560 --> 01:40:56.600]   what the people want, and they call this getting to Denmark, high functioning governments in
[01:40:56.600 --> 01:41:03.480]   the Nordics, you know, tend to reflect what the candidates reflect what the people want,
[01:41:03.480 --> 01:41:04.480]   right.
[01:41:04.480 --> 01:41:06.640]   And we seem to have this kind of broken here.
[01:41:06.640 --> 01:41:10.600]   We have one other story people have wanted us to comment on when we had our week off.
[01:41:10.600 --> 01:41:11.960]   So I thought we'd get into the open AI thing.
[01:41:11.960 --> 01:41:17.360]   I just wanted to point out, I don't mean to make this like a love triangle sacks, but
[01:41:17.360 --> 01:41:18.920]   Tucker and I did go skiing last year.
[01:41:18.920 --> 01:41:22.960]   We tried to keep it but now that it's out, you know, here it is.
[01:41:22.960 --> 01:41:26.480]   I'm sorry about this, but we were actually in the second at the same time.
[01:41:26.480 --> 01:41:28.480]   So it's gonna come out at some point.
[01:41:28.480 --> 01:41:30.200]   So we might as well talk about it.
[01:41:30.200 --> 01:41:33.680]   Look how old and nasty those ski clothes are.
[01:41:33.680 --> 01:41:34.680]   Oh, yeah.
[01:41:34.680 --> 01:41:38.680]   Well, you know what, we're both very woke and we don't want we're virtue signaling that
[01:41:38.680 --> 01:41:39.920]   we don't we want to recycle.
[01:41:39.920 --> 01:41:43.120]   So we bought all this stuff on eBay.
[01:41:43.120 --> 01:41:47.480]   I thought that was Phil Helmuth on the left.
[01:41:47.480 --> 01:41:49.200]   That's me before I was a big face.
[01:41:49.200 --> 01:41:51.800]   All right, let's do the open AI thing.
[01:41:51.800 --> 01:41:54.120]   If you guys have time, we can we go over this real quick.
[01:41:54.120 --> 01:41:58.640]   I don't want to we can I just wonder if the story stale but I mean, I think one of the
[01:41:58.640 --> 01:42:04.000]   things we can do here is just sort of now that the three acts are complete, we can actually
[01:42:04.000 --> 01:42:09.160]   talk about the epilogue just to for people who have no no, man, I think the epilogue
[01:42:09.160 --> 01:42:10.160]   is about to drop.
[01:42:10.160 --> 01:42:12.880]   But yeah, well, that's what that's what we'll get at.
[01:42:12.880 --> 01:42:14.920]   I think we're still in the second act here.
[01:42:14.920 --> 01:42:16.560]   I think this is over by a long shot.
[01:42:16.560 --> 01:42:17.560]   All right.
[01:42:17.560 --> 01:42:20.960]   So let me see why I'm gonna hold on that is I'm gonna architect and I'm gonna hand it
[01:42:20.960 --> 01:42:21.960]   to you tomorrow.
[01:42:21.960 --> 01:42:25.600]   Act one, Sam's fired act to chaos for a week we were off.
[01:42:25.600 --> 01:42:29.520]   You know what were all the reasons Act three, obviously, just yesterday, Sam's back.
[01:42:29.520 --> 01:42:32.040]   I think he is out Microsoft's an observer on the board.
[01:42:32.040 --> 01:42:36.740]   So I guess the epilogue trim off is what we want to know is what happened?
[01:42:36.740 --> 01:42:37.800]   Why did this all happen?
[01:42:37.800 --> 01:42:39.480]   Was there a major breakthrough?
[01:42:39.480 --> 01:42:45.840]   Was it Sam doing deals with Masayoshi son or in the Middle East to make you know, AI
[01:42:45.840 --> 01:42:46.840]   ships?
[01:42:46.840 --> 01:42:50.000]   What do you if we if the epilogue does drop?
[01:42:50.000 --> 01:42:53.660]   Or if like you're saying this is the second act in the third acts going to begin whichever
[01:42:53.660 --> 01:42:55.520]   metaphor you want to use?
[01:42:55.520 --> 01:42:56.520]   What will it say?
[01:42:56.520 --> 01:42:57.520]   Jim?
[01:42:57.520 --> 01:43:02.680]   I think we can all agree on what happened, which is that the employees realize that in
[01:43:02.680 --> 01:43:08.640]   the absence of leadership, business leadership, that the enterprise value of that company
[01:43:08.640 --> 01:43:10.680]   was going to disintegrate.
[01:43:10.680 --> 01:43:17.320]   And what would have been imperiled was an $86 billion valuation secondary.
[01:43:17.320 --> 01:43:22.380]   So I think the employees did what was in their best interest.
[01:43:22.380 --> 01:43:28.680]   And it makes the obvious and logical sense, which is, we need to circle the wagons and
[01:43:28.680 --> 01:43:32.600]   get the business leadership of this company back into this place.
[01:43:32.600 --> 01:43:36.600]   So that the value of the enterprise is sustained.
[01:43:36.600 --> 01:43:37.740]   They did that.
[01:43:37.740 --> 01:43:42.800]   So I think this valuation is going to hold I think the secondary is going to happen.
[01:43:42.800 --> 01:43:49.920]   So I think like from that perspective, whatever was supposed to happen there happen.
[01:43:49.920 --> 01:43:55.020]   So what are the interesting threads that are left over?
[01:43:55.020 --> 01:44:01.000]   We don't yet have a full accounting of what precipitated all of the all of the decision
[01:44:01.000 --> 01:44:03.080]   making at the board.
[01:44:03.080 --> 01:44:10.200]   Number one, number two is the person who seemed to be the principal inventor is now no longer
[01:44:10.200 --> 01:44:11.200]   on the board.
[01:44:11.200 --> 01:44:16.740]   And I think based on Sam's blog post, could probably no longer be at the company.
[01:44:16.740 --> 01:44:19.880]   That seems like an important thing.
[01:44:19.880 --> 01:44:24.200]   And then the third thing which I found out was Brett Taylor, who I work with at Facebook,
[01:44:24.200 --> 01:44:32.880]   who I've known for a long time, very sober, reasonable guy puts out his own addendum to
[01:44:32.880 --> 01:44:38.400]   the blog post that basically says his chairmanship of this board is purely transitory.
[01:44:38.400 --> 01:44:42.920]   What do you take from that?
[01:44:42.920 --> 01:44:46.520]   My takeaway is whatever's happening is still TBD.
[01:44:46.520 --> 01:44:50.600]   There is enough question marks that look, I told my friends on the board without saying
[01:44:50.600 --> 01:44:55.920]   who it was, whatever you guys do, the most important thing that you need to do is you
[01:44:55.920 --> 01:44:59.800]   need to retain great counsel and make sure that there and I said this publicly, make
[01:44:59.800 --> 01:45:06.880]   sure that there's phenomenal DNO insurance and make sure that there aren't any issues
[01:45:06.880 --> 01:45:10.760]   where you could be held personally liable for whatever happens in the future.
[01:45:10.760 --> 01:45:13.160]   You're saying of the valve, the corporate valve.
[01:45:13.160 --> 01:45:17.160]   Now that's a generic thing that I think all corporate directors should do.
[01:45:17.160 --> 01:45:23.600]   I think it's even more important here because you have a non standard governance structure
[01:45:23.600 --> 01:45:29.200]   that could change and it could change because the people involved wanted to change, but
[01:45:29.200 --> 01:45:32.960]   it could also could change because the government says, Hey, hold on a second.
[01:45:32.960 --> 01:45:34.680]   This should never have been like this in the first place.
[01:45:34.680 --> 01:45:37.000]   I'll give you an example of that.
[01:45:37.000 --> 01:45:41.480]   You know, at Facebook there was a moment where we divested all of our IP to a subsidiary
[01:45:41.480 --> 01:45:47.320]   in Ireland and for tax reasons and there were two signatories to that deal.
[01:45:47.320 --> 01:45:53.040]   It was me and Zuck and six or seven years later, this is long after I left Facebook,
[01:45:53.040 --> 01:45:56.320]   the IRS says, hold on a second, you misvalued these assets.
[01:45:56.320 --> 01:45:58.680]   We're owed X billions of dollars in taxes.
[01:45:58.680 --> 01:46:02.200]   It was a huge, long drawn out thing.
[01:46:02.200 --> 01:46:08.000]   So at a minimum, if there's great value created here, there will be consequences with respect
[01:46:08.000 --> 01:46:10.440]   to tax and who paid what.
[01:46:10.440 --> 01:46:15.080]   And if you have the shielding of a nonprofit entity, there's theoretically a lot of taxes
[01:46:15.080 --> 01:46:16.080]   that could have been paid that not.
[01:46:16.080 --> 01:46:21.320]   So all of this to me says a lot of really open, curious threads, which means it's not
[01:46:21.320 --> 01:46:22.800]   the epilogue yet.
[01:46:22.800 --> 01:46:25.440]   We're probably somewhere at the end of act two.
[01:46:25.440 --> 01:46:31.480]   Friedberg, my question to you is there's been speculation that there was a superintelligence
[01:46:31.480 --> 01:46:35.200]   AGI breakthrough possibly, and that's what spooked everybody.
[01:46:35.200 --> 01:46:40.640]   And maybe the claim that Sam wasn't forthcoming with the board was that he didn't tell them
[01:46:40.640 --> 01:46:41.640]   about it.
[01:46:41.640 --> 01:46:46.160]   The second thing was there's been some 4chan posts, again, this is pure speculation, that
[01:46:46.160 --> 01:46:52.520]   maybe the AI they're working on could break in some way encryption and that would have
[01:46:52.520 --> 01:46:56.560]   been caused a really chaotic global moment.
[01:46:56.560 --> 01:47:01.400]   What are your thoughts on those three potential items or more?
[01:47:01.400 --> 01:47:04.200]   Are any of those possibilities in your mind, Dave?
[01:47:04.200 --> 01:47:05.200]   Friedberg?
[01:47:05.200 --> 01:47:06.200]   No idea.
[01:47:06.200 --> 01:47:07.200]   But I can take that.
[01:47:07.200 --> 01:47:08.200]   Yeah, go ahead.
[01:47:08.200 --> 01:47:09.200]   So it's actually take some.
[01:47:09.200 --> 01:47:13.360]   So I think the best theory on what actually happened here, what precipitated all of this
[01:47:13.360 --> 01:47:17.120]   was broken in a piece by Reuters.
[01:47:17.120 --> 01:47:22.160]   And what it said was that there was a letter to the board that was written in the few days
[01:47:22.160 --> 01:47:28.000]   before Sam's firing by the board in which they were raising concerns over the development
[01:47:28.000 --> 01:47:35.680]   of Q*, which was a new breakthrough at OpenAI that allows these language models to do math.
[01:47:35.680 --> 01:47:38.880]   And previously, LLMs just weren't very good at math.
[01:47:38.880 --> 01:47:41.880]   It would sort of predict the next word, but that wasn't actually based on mathematical
[01:47:41.880 --> 01:47:42.880]   reasoning.
[01:47:42.880 --> 01:47:46.120]   Q* actually allows the AI to do math.
[01:47:46.120 --> 01:47:49.120]   It understands mathematical reasoning.
[01:47:49.120 --> 01:47:53.400]   Currently the capability is only at a grade school level right now, but it is highly accurate
[01:47:53.400 --> 01:47:56.760]   and it can be scaled up with more computing power.
[01:47:56.760 --> 01:48:01.440]   And I don't think it was coincidental that Sam was supposedly in the Middle East seeking
[01:48:01.440 --> 01:48:04.680]   to raise billions of dollars to create a new chip company, basically a new specialized
[01:48:04.680 --> 01:48:08.400]   chip or ASIC, to perhaps run these types of models.
[01:48:08.400 --> 01:48:10.760]   So they were going to scale up this capability.
[01:48:10.760 --> 01:48:16.100]   And what mathematical reasoning allows you to do is unlock a whole new problem set.
[01:48:16.100 --> 01:48:22.640]   So for example, in chemistry, in physics, in computer science, in cryptography, in encryption,
[01:48:22.640 --> 01:48:29.200]   all these things, mathematical reasoning underpins all of these disciplines.
[01:48:29.200 --> 01:48:35.940]   And so it does represent a major new piece towards AGI.
[01:48:35.940 --> 01:48:42.080]   And so I think the best theory about what happened is that the board, but I'd say specifically
[01:48:42.080 --> 01:48:50.480]   Amelia, had a panic or moment of panic or concern, freak out or whatever, concern, whatever,
[01:48:50.480 --> 01:48:51.840]   about this.
[01:48:51.840 --> 01:48:57.200]   And it combined with probably underlying concerns that the board had about Sam's other activity
[01:48:57.200 --> 01:48:59.680]   because he's got his fingers in a lot of pies here.
[01:48:59.680 --> 01:49:06.200]   So it hits two of the three potential reasons, you know, putting aside any personal behavior,
[01:49:06.200 --> 01:49:08.460]   it seems like there's no personal behavior here.
[01:49:08.460 --> 01:49:09.460]   So it's those two, right?
[01:49:09.460 --> 01:49:10.460]   Right.
[01:49:10.460 --> 01:49:13.860]   And so they did the program, but they apparently didn't really think it through at all.
[01:49:13.860 --> 01:49:16.420]   And they didn't really think these are not, it's not a professional board.
[01:49:16.420 --> 01:49:17.420]   Yeah, it's constructed.
[01:49:17.420 --> 01:49:18.420]   Well, I don't know.
[01:49:18.420 --> 01:49:22.740]   I think Adam, Adam is very professional, but the other two were considered, but he only
[01:49:22.740 --> 01:49:27.340]   has a time for all just and then the two others who were, you know, from nonprofits are just
[01:49:27.340 --> 01:49:28.340]   not very familiar.
[01:49:28.340 --> 01:49:31.980]   And so what happened is they took this drastic action, but didn't explain it.
[01:49:31.980 --> 01:49:36.920]   And with each passing day, it became more glaring that they would not explain it.
[01:49:36.920 --> 01:49:43.300]   So the pressure sort of built on the company to explain itself and provide a good justification
[01:49:43.300 --> 01:49:44.300]   for this.
[01:49:44.300 --> 01:49:49.900]   And then meanwhile, I think Sam just ran a textbook counter coup operation here.
[01:49:49.900 --> 01:49:55.920]   I mean, they got hearts, emojis, there was that aspect of it, but disguise behind the
[01:49:55.920 --> 01:50:00.480]   velvet glove of all these hearts and saying, I love you and all this stuff was the iron
[01:50:00.480 --> 01:50:01.480]   fist.
[01:50:01.480 --> 01:50:08.280]   And what they realized was they got over 700 of the 770 open AI employees to sign a petition.
[01:50:08.280 --> 01:50:12.440]   They were going with Sam and Sam went to Microsoft and set up shop.
[01:50:12.440 --> 01:50:16.680]   So the threat to the board was, I'm going to take the whole company with me and set
[01:50:16.680 --> 01:50:18.720]   up shop over at Microsoft.
[01:50:18.720 --> 01:50:20.280]   And that was sort of the compelling threat.
[01:50:20.280 --> 01:50:23.520]   Basically, all the employees threatened to quit and go with Sam.
[01:50:23.520 --> 01:50:26.700]   And so the board was under immense pressure.
[01:50:26.700 --> 01:50:31.620]   And then at the same time, you've got the sense that Ilya was under a lot of personal
[01:50:31.620 --> 01:50:34.860]   pressure from his friends, from people he knew at the company.
[01:50:34.860 --> 01:50:38.560]   It's not probably irrelevant that there's about to be a huge cash out.
[01:50:38.560 --> 01:50:42.680]   There's about to be a big secondary at a $86 billion valuation.
[01:50:42.680 --> 01:50:46.000]   So all these early employees were about to make a lot of money.
[01:50:46.000 --> 01:50:50.340]   In any event, for all of these reasons, I don't think Ilya is motivated by money.
[01:50:50.340 --> 01:50:53.300]   I think he's motivated by wanting to keep open AI intact.
[01:50:53.300 --> 01:50:57.380]   I think that he must have had a moment where he realized, wait a second, what we've done
[01:50:57.380 --> 01:51:01.080]   here in throwing out Sam is destroying the company.
[01:51:01.080 --> 01:51:02.800]   And then he recanted.
[01:51:02.800 --> 01:51:05.060]   He basically apologized and signed that petition.
[01:51:05.060 --> 01:51:08.860]   And at that moment, it just became a fait accompli that Sam was going to get his job
[01:51:08.860 --> 01:51:09.860]   back.
[01:51:09.860 --> 01:51:11.300]   And so that's basically what happened.
[01:51:11.300 --> 01:51:17.780]   Now in terms of the epilogue, where I disagree slightly with Chamath is that although they
[01:51:17.780 --> 01:51:20.900]   still have to form this board, they're going to form a nine person board and they only
[01:51:20.900 --> 01:51:23.220]   have three members so far.
[01:51:23.220 --> 01:51:28.580]   I think the conclusion here is it's sort of a foregone conclusion, which is the board
[01:51:28.580 --> 01:51:30.100]   can never fire Sam again.
[01:51:30.100 --> 01:51:33.020]   I mean, they're not going to go through that again.
[01:51:33.020 --> 01:51:34.820]   Therefore he has total control.
[01:51:34.820 --> 01:51:35.820]   It would take a lot.
[01:51:35.820 --> 01:51:36.820]   It would take a lot.
[01:51:36.820 --> 01:51:42.020]   So I think that Sam has won and he's going to consolidate his control over the company.
[01:51:42.020 --> 01:51:43.740]   By the way, I thought he already had control.
[01:51:43.740 --> 01:51:44.740]   I thought that.
[01:51:44.740 --> 01:51:45.740]   Yeah, you outlined that in the previous episode.
[01:51:45.740 --> 01:51:47.980]   We all thought that he had control.
[01:51:47.980 --> 01:51:48.980]   Apparently he did not.
[01:51:48.980 --> 01:51:51.180]   Do you think he makes it and spins it out and make it a for profit?
[01:51:51.180 --> 01:51:55.500]   Do you think it just makes it a for profit and then says, Hey, we'll give X amount of
[01:51:55.500 --> 01:51:58.500]   the equity or whatever to this nonprofit, but we're going to, we're just going to flip
[01:51:58.500 --> 01:52:01.460]   this thing and separate it out and clean up the original sin.
[01:52:01.460 --> 01:52:03.780]   I think apparently there's like tax problems with doing that.
[01:52:03.780 --> 01:52:09.780]   But I, but I think that what happens is look, they already have this for profit LLC entity,
[01:52:09.780 --> 01:52:12.860]   which is where all the investors have have basically put their money into and they've
[01:52:12.860 --> 01:52:18.500]   gotten shares or membership interests or some sort of profit interest, synthetic shares,
[01:52:18.500 --> 01:52:19.500]   phantom shares.
[01:52:19.500 --> 01:52:24.380]   And then the vast majority of that is owned by this foundation, the nonprofit foundation
[01:52:24.380 --> 01:52:26.860]   that I always saw was under Sam's control, but apparently it wasn't.
[01:52:26.860 --> 01:52:31.380]   I think it now will be, I think that I just think that what's going to happen in the next
[01:52:31.380 --> 01:52:35.860]   few months is that Sam will consolidate his control because he's proven that he has a
[01:52:35.860 --> 01:52:40.260]   total loyalty of the troops and they're behind him and there's no choice.
[01:52:40.260 --> 01:52:42.460]   So why won't he get everything he wants?
[01:52:42.460 --> 01:52:48.060]   Yeah, all of the, all of the smoke I think leads to the fire that you point out, which
[01:52:48.060 --> 01:52:50.540]   is there was some great advance.
[01:52:50.540 --> 01:52:55.340]   There was some deals going on and those things, it matches what the board said.
[01:52:55.340 --> 01:52:58.820]   We didn't feel like we were, he was being forthcoming with us and it could be on two
[01:52:58.820 --> 01:52:59.820]   issues like that.
[01:52:59.820 --> 01:53:00.820]   It just totally makes sense.
[01:53:00.820 --> 01:53:03.660]   It locks the puzzle pieces in place.
[01:53:03.660 --> 01:53:09.220]   Dave, let's assume that they have now not only done language models, predicting the
[01:53:09.220 --> 01:53:13.180]   next word and, and, you know, understanding that, but there is some reasoning going on
[01:53:13.180 --> 01:53:17.940]   here and they understand math and it understands how to do the next math problem.
[01:53:17.940 --> 01:53:19.980]   I don't know if we put that under reinforcement learning.
[01:53:19.980 --> 01:53:23.380]   It's obviously very different than language models.
[01:53:23.380 --> 01:53:27.800]   Explain what you think if that is what's happening here with the Q project, what that could mean
[01:53:27.800 --> 01:53:30.220]   on a scientific basis.
[01:53:30.220 --> 01:53:31.220]   I don't know enough.
[01:53:31.220 --> 01:53:32.220]   I'm sorry.
[01:53:32.220 --> 01:53:35.780]   I mean, this is why we love you because you're honest when you don't know.
[01:53:35.780 --> 01:53:41.740]   Chamath, anything here as we wrap in terms of watching this whole brouhaha?
[01:53:41.740 --> 01:53:44.140]   I think it's probably not the end.
[01:53:44.140 --> 01:53:45.140]   Yeah.
[01:53:45.140 --> 01:53:46.140]   Yeah.
[01:53:46.140 --> 01:53:47.140]   More drama.
[01:53:47.140 --> 01:53:48.620]   I think there probably will be more drama.
[01:53:48.620 --> 01:53:49.620]   Yeah.
[01:53:49.620 --> 01:53:51.380]   I mean, it's a lot of drama.
[01:53:51.380 --> 01:53:55.900]   Did you see the article this morning on where they started saying all the houses that Sam
[01:53:55.900 --> 01:53:58.060]   bought, he bought all these houses.
[01:53:58.060 --> 01:54:01.340]   And so I think there's a lot of investigators digging around now trying to figure out all
[01:54:01.340 --> 01:54:05.380]   the backstory because the board wasn't so forthcoming with what happened and why they
[01:54:05.380 --> 01:54:07.020]   made this decision.
[01:54:07.020 --> 01:54:10.300]   There's a lot of people digging around trying to figure out more about Sam than may have
[01:54:10.300 --> 01:54:12.700]   been, you know, looked into in the past.
[01:54:12.700 --> 01:54:17.980]   So this will reveal all sorts of new threads that will start to become part of the narrative.
[01:54:17.980 --> 01:54:19.180]   It's unfortunate.
[01:54:19.180 --> 01:54:24.100]   I think that the technology, the progress is what really matters here.
[01:54:24.100 --> 01:54:26.460]   Not all the kind of personal people stuff.
[01:54:26.460 --> 01:54:27.460]   It's weird.
[01:54:27.460 --> 01:54:30.860]   I also the other comment I'll make I thought one of the biggest takeaways for me on the
[01:54:30.860 --> 01:54:36.420]   whole drama last week was that the employees basically got their way.
[01:54:36.420 --> 01:54:40.020]   Employees got together, voted and said this is what we want.
[01:54:40.020 --> 01:54:41.300]   And the board did what they wanted.
[01:54:41.300 --> 01:54:46.180]   And it really, I think sets another precedent much like I think Elon said a really big precedent
[01:54:46.180 --> 01:54:49.980]   in Silicon Valley when he came in and slashed heads at Twitter.
[01:54:49.980 --> 01:54:54.060]   The precedent that it triggered a lot of other executives to start to think, well, maybe
[01:54:54.060 --> 01:54:55.060]   that's possible.
[01:54:55.060 --> 01:54:58.740]   And I should think about doing, you know, more cost savings and so on.
[01:54:58.740 --> 01:55:03.140]   This is another interesting precedent where an entire employee base gets together and
[01:55:03.140 --> 01:55:08.860]   says we want x and the board acquiesced and said, here you go, you can have x.
[01:55:08.860 --> 01:55:12.180]   Does that mean that other startups and other companies are going to start to see employee
[01:55:12.180 --> 01:55:16.620]   groups band together saying we want x in a more vocal public way?
[01:55:16.620 --> 01:55:17.620]   Possibly.
[01:55:17.620 --> 01:55:20.540]   Well, I mean, freeberg, this is not a new tactic, right?
[01:55:20.540 --> 01:55:24.300]   But look, if the employees are willing to sign a petition on mass, and you get over
[01:55:24.300 --> 01:55:30.260]   90% of them supporting something, and then you also threatened to all set up shop somewhere
[01:55:30.260 --> 01:55:33.940]   else, I mean, boards usually respond to that pressure.
[01:55:33.940 --> 01:55:34.940]   Yeah.
[01:55:34.940 --> 01:55:38.900]   But look, I think that it's very clear that Sam has the support of the troops.
[01:55:38.900 --> 01:55:39.900]   I'm not questioning that.
[01:55:39.900 --> 01:55:44.540]   But I think it would be a mistake to just see this as some sort of spontaneous groundswell.
[01:55:44.540 --> 01:55:47.500]   I think there was clearly like some organization to it.
[01:55:47.500 --> 01:55:52.340]   Like I said, I think he ran a textbook operation there.
[01:55:52.340 --> 01:55:57.660]   And wasn't it Paul Graham who said something like if Sam were dropped on an island of cannibals,
[01:55:57.660 --> 01:56:02.340]   what kind of a Lord of the Flies situation, he'd be the one to come out on top.
[01:56:02.340 --> 01:56:04.460]   He's like really, he's in his element.
[01:56:04.460 --> 01:56:08.180]   Don't just assume, look, this is all covered up by hearts and I love you.
[01:56:08.180 --> 01:56:12.780]   I mean, I've never read a corporate announcement with the word loving it more times.
[01:56:12.780 --> 01:56:15.100]   This was not about loving hearts.
[01:56:15.100 --> 01:56:19.260]   You know, there's some ruthless corporate infighting here.
[01:56:19.260 --> 01:56:20.260]   Yeah.
[01:56:20.260 --> 01:56:21.260]   And he came out on top.
[01:56:21.260 --> 01:56:22.700]   But look, the board was totally incompetent.
[01:56:22.700 --> 01:56:28.940]   I mean, listen, if if a board is going to take a drastic action of firing the founder
[01:56:28.940 --> 01:56:34.420]   CEO when everything is going great, I mean, because everything's been going great at open
[01:56:34.420 --> 01:56:37.980]   AI, it's incumbent on them to explain their actions and there needs to be
[01:56:37.980 --> 01:56:38.980]   Why didn't they just do that?
[01:56:38.980 --> 01:56:39.980]   Why didn't they do that?
[01:56:39.980 --> 01:56:40.980]   Why didn't they do that?
[01:56:40.980 --> 01:56:41.980]   Did they?
[01:56:41.980 --> 01:56:42.980]   There needs to be a
[01:56:42.980 --> 01:56:43.980]   Why didn't they do that?
[01:56:43.980 --> 01:56:45.900]   Even as a CYA or for their reputation?
[01:56:45.900 --> 01:56:47.340]   I'm saying it was totally incompetent.
[01:56:47.340 --> 01:56:49.060]   Yeah, they should have dropped it.
[01:56:49.060 --> 01:56:50.060]   Yeah.
[01:56:50.060 --> 01:56:54.060]   Well, no, what I'm saying is, either there's a smoking gun or there's not, I don't think
[01:56:54.060 --> 01:56:57.860]   you should take an action like this ever, unless there's a smoking gun.
[01:56:57.860 --> 01:57:00.900]   And if there is a smoking gun, you need to communicate that.
[01:57:00.900 --> 01:57:01.900]   Right.
[01:57:01.900 --> 01:57:04.380]   And I think with each passing day that they didn't communicate it, people came to the
[01:57:04.380 --> 01:57:06.900]   conclusion, there is no smoking gun.
[01:57:06.900 --> 01:57:11.740]   And so my guess is there is no smoking gun, and they were overly hasty about taking this
[01:57:11.740 --> 01:57:12.740]   action.
[01:57:12.740 --> 01:57:14.860]   Yeah, this could have been a very simple discussion.
[01:57:14.860 --> 01:57:20.220]   Hey, Sam, can we work on you telling us in advance when you're going and doing dealmaking
[01:57:20.220 --> 01:57:22.900]   so we can be in alignment on it?
[01:57:22.900 --> 01:57:26.660]   Like that seemed like there was something that could have been done that wasn't firing
[01:57:26.660 --> 01:57:28.340]   him in this explosive way.
[01:57:28.340 --> 01:57:32.500]   Yeah, I mean, like, you could, you could put somebody on a pip, you could do all kinds
[01:57:32.500 --> 01:57:34.740]   of things.
[01:57:34.740 --> 01:57:40.300]   So taking the step of Yeah, letting somebody go in that way, with that orchestration.
[01:57:40.300 --> 01:57:47.660]   Again, I just I just think that this stuff is too juicy, and too interesting for the
[01:57:47.660 --> 01:57:49.140]   details to not come out.
[01:57:49.140 --> 01:57:50.540]   Oh, it will come out.
[01:57:50.540 --> 01:57:52.700]   Yeah, the fact that it hasn't come out yet is crazy.
[01:57:52.700 --> 01:57:54.940]   I would have suspected it would have come out in the first 72 hours.
[01:57:54.940 --> 01:57:58.620]   So you think there is maybe not a smoking gun, but there's something?
[01:57:58.620 --> 01:58:03.940]   No, no, no, I think I think it's what what we said, which is that the economically rational
[01:58:03.940 --> 01:58:09.420]   decision for all the employees, and for Sam and Greg, was to do what they did, because
[01:58:09.420 --> 01:58:13.700]   it allows them to get an $86 billion valuation and a big secondary done, right.
[01:58:13.700 --> 01:58:15.780]   So that makes complete rational sense.
[01:58:15.780 --> 01:58:20.500]   That is the rational decision that should have happened.
[01:58:20.500 --> 01:58:28.580]   And so that did, but it doesn't stop whatever underlying chaos was happening, that caused
[01:58:28.580 --> 01:58:33.540]   this decision to exist in the first place to not come out.
[01:58:33.540 --> 01:58:37.620]   I just think that the incentives for that decision to come out now, for example, from
[01:58:37.620 --> 01:58:40.380]   the two departing board members is quite high.
[01:58:40.380 --> 01:58:43.220]   How do you get this information from the incoming board members?
[01:58:43.220 --> 01:58:45.540]   How did they not say anything?
[01:58:45.540 --> 01:58:49.180]   All the new six people will have that join the board will have to get read into this
[01:58:49.180 --> 01:58:50.180]   thing, right.
[01:58:50.180 --> 01:58:54.980]   So you're just multiplying the number of people that knows whatever it is.
[01:58:54.980 --> 01:58:59.020]   And it's either going to be David to your point path a which is the board didn't know
[01:58:59.020 --> 01:59:03.500]   what they're doing in the act of hastily or, you know, the self preservation of here's
[01:59:03.500 --> 01:59:07.180]   what we knew, but it is just going to come out and leak after leak after leak.
[01:59:07.180 --> 01:59:12.540]   And then to your point, the pulling the sweater of all of the other deals that may have been
[01:59:12.540 --> 01:59:15.940]   happening on the side or whatever, all of this stuff now comes out because it's just
[01:59:15.940 --> 01:59:18.260]   too salacious for too many people.
[01:59:18.260 --> 01:59:21.420]   The numbers are too big, everything just looks too juicy.
[01:59:21.420 --> 01:59:26.740]   Now you have this like hidden technology that could theoretically ruin the world.
[01:59:26.740 --> 01:59:30.580]   Everybody will be leaking to everybody.
[01:59:30.580 --> 01:59:34.940]   That's the that's the only guarantee here, which is why I really think that you have
[01:59:34.940 --> 01:59:41.540]   to commend the leadership team for trying to become very militaristic about all of this.
[01:59:41.540 --> 01:59:46.380]   Everybody had the same tweets, they said the same things they use the same heart emojis.
[01:59:46.380 --> 01:59:50.020]   I mean, it was extremely well managed.
[01:59:50.020 --> 01:59:51.020]   It's mantras.
[01:59:51.020 --> 01:59:53.860]   It's amazing what a secondary will do to the troops.
[01:59:53.860 --> 01:59:59.140]   And there's a line meant of the mantra, the mantra was that open AI is nothing without
[01:59:59.140 --> 02:00:01.140]   its employees or something like that.
[02:00:01.140 --> 02:00:02.140]   Right?
[02:00:02.140 --> 02:00:03.140]   It's nothing without our team or something.
[02:00:03.140 --> 02:00:08.500]   And that was that was basically something that on the surface appeared to be a positive
[02:00:08.500 --> 02:00:11.260]   affirmation of camaraderie.
[02:00:11.260 --> 02:00:14.900]   But like I said, iron fist beneath a velvet glove.
[02:00:14.900 --> 02:00:15.900]   It's a threat.
[02:00:15.900 --> 02:00:16.900]   It's a threat.
[02:00:16.900 --> 02:00:17.900]   We can set up shop.
[02:00:17.900 --> 02:00:22.740]   It's also the formula in in an Excel spreadsheet that says they are nothing without the team.
[02:00:22.740 --> 02:00:25.060]   The enterprise value equals zero.
[02:00:25.060 --> 02:00:28.740]   Your company is a zero without us and we can go set up shop somewhere else.
[02:00:28.740 --> 02:00:30.780]   So it was perfect.
[02:00:30.780 --> 02:00:32.620]   Velvet glove, iron fist type stuff.
[02:00:32.620 --> 02:00:34.700]   Yeah, well done, Sam.
[02:00:34.700 --> 02:00:37.020]   Well, Sam will be on the pod in the coming weeks.
[02:00:37.020 --> 02:00:38.300]   I think I've been texting with them.
[02:00:38.300 --> 02:00:42.940]   So I think if Sam, I think what Elon said yesterday was also really interesting, which
[02:00:42.940 --> 02:00:50.920]   is he described Ilya as an extremely moral person who thinks about these things subtly.
[02:00:50.920 --> 02:00:56.260]   And so, again, and I've known Adam for a long time, I think Adam is tremendous.
[02:00:56.260 --> 02:00:57.700]   He is core Adam D'Angelo.
[02:00:57.700 --> 02:00:59.380]   Yeah, he was the CTO at Facebook.
[02:00:59.380 --> 02:01:02.460]   We worked together in the trenches in war for years.
[02:01:02.460 --> 02:01:05.380]   He is just the best of the best.
[02:01:05.380 --> 02:01:06.380]   Totally.
[02:01:06.380 --> 02:01:07.380]   He's very smart.
[02:01:07.380 --> 02:01:09.820]   You know, very, very knowledgeable.
[02:01:09.820 --> 02:01:12.780]   This is not an irrational, emotional person.
[02:01:12.780 --> 02:01:25.380]   My guess is that they had good reasons to want to act based on the Ilya's philosophy
[02:01:25.380 --> 02:01:26.720]   around AI safety.
[02:01:26.720 --> 02:01:31.300]   You may not agree with that philosophy, but their mistake was if you're going to do something
[02:01:31.300 --> 02:01:32.820]   like that, you have to be able to defend it.
[02:01:32.820 --> 02:01:34.300]   You have to be able to communicate it.
[02:01:34.300 --> 02:01:40.060]   And I mean, if your concern here was around AI safety, write a manifesto, you know, explain
[02:01:40.060 --> 02:01:44.340]   the values that you're invoking and supporting.
[02:01:44.340 --> 02:01:49.260]   I think that's the takeaway for all of us, for all of us on boards that at some point
[02:01:49.260 --> 02:01:54.140]   have to make these decisions because we're all faced with them and we've made them.
[02:01:54.140 --> 02:01:59.780]   The lesson that I learned is we're at a point where it is so important that you give employees
[02:01:59.780 --> 02:02:03.380]   the transparency to re underwrite why they should stay.
[02:02:03.380 --> 02:02:08.260]   So when you make a decision like this in any company going forward, my reaction will be
[02:02:08.260 --> 02:02:12.040]   open the kimono and lay out the case bear.
[02:02:12.040 --> 02:02:16.460]   So if you have to make a CEO transition, this is exactly why we did it.
[02:02:16.460 --> 02:02:19.020]   This was the precipitating events that caused it.
[02:02:19.020 --> 02:02:21.740]   Here's the evidence and here's what we're going to do about it.
[02:02:21.740 --> 02:02:26.180]   And I think that that's probably a good takeaway for all boards to learn, which is that level
[02:02:26.180 --> 02:02:30.660]   of transparency is going to be needed in the future so that folks don't fill in the blanks
[02:02:30.660 --> 02:02:32.780]   with their own conspiracy theories.
[02:02:32.780 --> 02:02:37.060]   Also there were apparently two companies or two organizations running in parallel here
[02:02:37.060 --> 02:02:41.380]   to build on your points to mop which is Ilya and the nonprofit.
[02:02:41.380 --> 02:02:45.220]   This might have been the right decision for that organization, but the right decision
[02:02:45.220 --> 02:02:50.620]   for the employees who are incented by the secondary that was about to be shipping the
[02:02:50.620 --> 02:02:55.380]   employees apparently billions of dollars, that for profit company, this would be the
[02:02:55.380 --> 02:02:56.620]   wrong decision.
[02:02:56.620 --> 02:03:02.060]   The for profit company should go fast and it should be releasing where Jason and Jason
[02:03:02.060 --> 02:03:08.100]   like the from what was reported, there's like this obligation and it's hard to understand
[02:03:08.100 --> 02:03:14.700]   what it means of the board to determine when AGI is reached and then as a result, essentially
[02:03:14.700 --> 02:03:17.780]   hit the kill switch on the commercial business.
[02:03:17.780 --> 02:03:24.300]   And if I were a board member dealing with that, I would want a gazillion trillion dollars
[02:03:24.300 --> 02:03:27.040]   of insurance to cover me.
[02:03:27.040 --> 02:03:33.060]   And the reason is that when that's litigated, not if, when that's litigated, it is that
[02:03:33.060 --> 02:03:42.380]   board that will be at the center of dealing with that financial responsibility and liability.
[02:03:42.380 --> 02:03:45.300]   This is the other master stroke I think of what Sam did.
[02:03:45.300 --> 02:03:50.460]   He's not even on the board so that liability is no longer his.
[02:03:50.460 --> 02:03:56.740]   So he's got complete control of the business and the actual none of the responsibility
[02:03:56.740 --> 02:04:00.140]   and the actual fiscal responsibility he doesn't have to bear.
[02:04:00.140 --> 02:04:02.660]   Can I speak to the kill switch for a second?
[02:04:02.660 --> 02:04:04.820]   So I think it's a really interesting point.
[02:04:04.820 --> 02:04:07.380]   So by the way, Jason, there is a for profit entity here.
[02:04:07.380 --> 02:04:09.860]   The for profit entity is this new LLC that was created.
[02:04:09.860 --> 02:04:14.700]   The nonprofit entity is above it at the governance layer and it kind of owns the LLC.
[02:04:14.700 --> 02:04:15.700]   Right.
[02:04:15.700 --> 02:04:21.620]   So again, investors and employees get compensated out of the LLC and then this nonprofit foundation
[02:04:21.620 --> 02:04:23.820]   was supposed to exercise a really complicated org chart.
[02:04:23.820 --> 02:04:24.820]   There you go.
[02:04:24.820 --> 02:04:28.140]   Whenever an org chart has more than like two arrows, it's crazy.
[02:04:28.140 --> 02:04:29.140]   You're fucked.
[02:04:29.140 --> 02:04:30.140]   Right.
[02:04:30.140 --> 02:04:31.140]   How many arrows?
[02:04:31.140 --> 02:04:33.380]   1, 2, 3, 4, 5, 6, 7, 8 arrows.
[02:04:33.380 --> 02:04:34.380]   That's right.
[02:04:34.380 --> 02:04:35.380]   It's too many.
[02:04:35.380 --> 02:04:41.900]   But my point is that this complexity was justified by this idea of the kill switch that if the
[02:04:41.900 --> 02:04:45.960]   AI gets out of control, the AGI gets out of control, we're going to have this board of
[02:04:45.960 --> 02:04:50.020]   super wise people who are not motivated by a profit incentive, right?
[02:04:50.020 --> 02:04:53.020]   Because that we can't trust the profit motive.
[02:04:53.020 --> 02:04:54.020]   Right.
[02:04:54.020 --> 02:05:00.820]   And so we're going to have this board of wise elders who are going to make this super intelligent
[02:05:00.820 --> 02:05:02.420]   decision.
[02:05:02.420 --> 02:05:05.740]   And if this whole episode shows anything, it shows this structure completely failed.
[02:05:05.740 --> 02:05:09.740]   I mean, the board ended up acting in a completely incompetent way.
[02:05:09.740 --> 02:05:14.860]   Either they had good cause to do what they did and didn't explain it, which was incompetent,
[02:05:14.860 --> 02:05:17.100]   or they had no cause at all, which was incompetent.
[02:05:17.100 --> 02:05:22.060]   Either way, you cannot say that this board acted with a high degree of competence, no
[02:05:22.060 --> 02:05:24.500]   matter how competent any of the individuals are.
[02:05:24.500 --> 02:05:28.140]   I'm just saying that as a board dynamic, it completely failed.
[02:05:28.140 --> 02:05:33.460]   So they did not invent some higher form of governance as they originally claimed was
[02:05:33.460 --> 02:05:34.460]   necessary.
[02:05:34.460 --> 02:05:35.460]   It's a Franken structure.
[02:05:35.460 --> 02:05:36.460]   It's a Franken structure.
[02:05:36.460 --> 02:05:37.460]   So it didn't work.
[02:05:37.460 --> 02:05:43.300]   And it's all, I mean, it's, I think, important lesson in human motivations, which is just
[02:05:43.300 --> 02:05:48.460]   because you take out the profit motive does not mean that human beings all of a sudden
[02:05:48.460 --> 02:05:49.460]   become noble.
[02:05:49.460 --> 02:05:54.780]   They just pursue other agendas, basically political agendas or whatever.
[02:05:54.780 --> 02:06:02.340]   And so this idea that we're going to solve the AGI problem or alignment issue by creating
[02:06:02.340 --> 02:06:06.940]   nonprofit structures, I think that this episode proves that's not going to work.
[02:06:06.940 --> 02:06:08.500]   It's going to look elsewhere.
[02:06:08.500 --> 02:06:13.100]   And you and I talked about this on Twitter spaces and x spaces, which was, you know,
[02:06:13.100 --> 02:06:18.220]   people give VCs and, you know, investors a hard time about, you know, I don't know their
[02:06:18.220 --> 02:06:20.380]   existence and how they operate in the world.
[02:06:20.380 --> 02:06:21.380]   Okay, fair enough.
[02:06:21.380 --> 02:06:27.060]   I'm sure there's valid criticisms, but the share price and employees participating and
[02:06:27.060 --> 02:06:33.100]   the share price going up and secondaries occurring on a regular basis is the most perfect structure
[02:06:33.100 --> 02:06:37.180]   that has been created by humans to date for running an organization, I think we would
[02:06:37.180 --> 02:06:38.880]   all agree, and it's super imperfect.
[02:06:38.880 --> 02:06:40.580]   And there's weird things that happen.
[02:06:40.580 --> 02:06:43.980]   But if you want everybody to grow in the right direction, giving them some shares, and then
[02:06:43.980 --> 02:06:47.860]   everybody watching the share price go up into the right is pretty phenomenal.
[02:06:47.860 --> 02:06:50.080]   This board did not have one VC on it.
[02:06:50.080 --> 02:06:51.080]   It was not
[02:06:51.080 --> 02:06:55.460]   and all the VCs in the company were they were the biggest, you know, they were the most
[02:06:55.460 --> 02:06:59.940]   aggressive tweeters saying, you know, WTF, like, what are you doing?
[02:06:59.940 --> 02:07:02.460]   Because they could see their investment going up in smoke.
[02:07:02.460 --> 02:07:06.740]   Yes, they had a lot of money with the share price, but no control.
[02:07:06.740 --> 02:07:10.100]   So as much as you don't like the profit motive, it does create alignment.
[02:07:10.100 --> 02:07:12.200]   And that makes people predictable.
[02:07:12.200 --> 02:07:13.200]   And that's a good thing.
[02:07:13.200 --> 02:07:20.920]   And like Adam Smith said, in the 1700s, the reason we can trust that the butcher and the
[02:07:20.920 --> 02:07:25.520]   baker will serve us our dinner is because they're going to make a profit.
[02:07:25.520 --> 02:07:26.520]   They're aligned with us.
[02:07:26.520 --> 02:07:31.100]   And, you know, we don't look to charity, we look to their self interest.
[02:07:31.100 --> 02:07:35.820]   And like you said, for all the shit that VCs take, that if you properly align people, it
[02:07:35.820 --> 02:07:41.600]   can create, I think, it creates a great enterprise, it creates a potential great, great enterprise
[02:07:41.600 --> 02:07:43.180]   for great outcomes, great outcomes.
[02:07:43.180 --> 02:07:44.180]   Absolutely.
[02:07:44.180 --> 02:07:45.180]   All right.
[02:07:45.180 --> 02:07:50.220]   This has been another extraordinary, all in episode probably top five.
[02:07:50.220 --> 02:07:53.820]   Thanks so much for Tucker Carlson for coming on the program.
[02:07:53.820 --> 02:07:58.780]   And for the Sultan of Science, David Friedberg, the rain man.
[02:07:58.780 --> 02:07:59.780]   Yeah, definitely.
[02:07:59.780 --> 02:08:03.140]   David Sachs and the dictator himself.
[02:08:03.140 --> 02:08:05.380]   I'm Paulie Hoppettia.
[02:08:05.380 --> 02:08:06.780]   I'm the world's greatest moderator.
[02:08:06.780 --> 02:08:08.180]   We'll see you next time.
[02:08:08.180 --> 02:08:19.620]   And it said We open sourced it to the fans and they've
[02:08:19.620 --> 02:08:30.060]   just gone crazy with it I'm going all in
[02:08:30.060 --> 02:08:41.180]   I'm going all in I'm going all in
[02:08:41.180 --> 02:08:54.740]   I'm going all in I'm going all in
[02:08:54.740 --> 02:09:04.860]   I'm going all in I'm going all in
[02:09:04.860 --> 02:09:05.860]   I love you.
[02:09:05.860 --> 02:09:06.860]   I love you.

