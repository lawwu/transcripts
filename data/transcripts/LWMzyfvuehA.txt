
[00:00:00.000 --> 00:00:07.280]   Hi, everyone.
[00:00:07.280 --> 00:00:09.280]   Welcome to CS224N.
[00:00:09.280 --> 00:00:12.880]   We're about two minutes in, so let's get started.
[00:00:12.880 --> 00:00:17.240]   So today, we've got what I think is quite an exciting lecture topic.
[00:00:17.240 --> 00:00:21.760]   We're going to talk about self-attention and transformers.
[00:00:21.760 --> 00:00:25.320]   So these are some ideas that are sort of the foundation
[00:00:25.320 --> 00:00:29.400]   of most of the modern advances in natural language processing.
[00:00:29.400 --> 00:00:34.640]   And actually, AI systems in a broad range of fields.
[00:00:34.640 --> 00:00:37.600]   So it's a very, very fun topic.
[00:00:37.600 --> 00:00:39.480]   Before we get into that--
[00:00:39.480 --> 00:00:49.240]   OK, before we get into that, we're going to have a couple of reminders.
[00:00:49.240 --> 00:00:51.120]   So there are brand new lecture notes.
[00:00:51.120 --> 00:00:51.620]   Woo!
[00:00:51.620 --> 00:00:53.560]   [CHEERING]
[00:00:53.560 --> 00:00:54.640]   Nice, thank you.
[00:00:54.640 --> 00:00:57.140]   Yeah.
[00:00:57.140 --> 00:00:59.280]   I'm very excited about them.
[00:00:59.280 --> 00:01:02.460]   They go into-- they pretty much follow along
[00:01:02.460 --> 00:01:07.700]   with what I'll be talking about today, but go into considerably more detail.
[00:01:07.700 --> 00:01:11.780]   Assignment four is due a week from today.
[00:01:11.780 --> 00:01:15.380]   Yeah, so the issues with Azure continue.
[00:01:15.380 --> 00:01:16.540]   Thankfully-- woo!
[00:01:16.540 --> 00:01:25.100]   Thankfully, our TAs especially has tested that this works on Colab,
[00:01:25.100 --> 00:01:29.040]   and the amount of training is such that a Colab session will
[00:01:29.040 --> 00:01:33.340]   allow you to train your machine translation system.
[00:01:33.340 --> 00:01:35.180]   So if you don't have a GPU, use Colab.
[00:01:35.180 --> 00:01:37.100]   We're continuing to work on getting access
[00:01:37.100 --> 00:01:41.900]   to more GPUs for assignment five in the final project.
[00:01:41.900 --> 00:01:44.660]   We'll continue to update you as we're able to.
[00:01:44.660 --> 00:01:49.100]   But the usual systems this year are no longer
[00:01:49.100 --> 00:01:52.140]   holding because companies are changing their minds about things.
[00:01:52.140 --> 00:01:57.540]   OK, so our final project proposal, you have a proposal
[00:01:57.540 --> 00:02:00.460]   of what you want to work on for your final project.
[00:02:00.460 --> 00:02:04.400]   We will give you feedback on whether we think it's a feasible idea
[00:02:04.400 --> 00:02:05.260]   or how to change it.
[00:02:05.260 --> 00:02:07.980]   So this is very important because we want you to work on something
[00:02:07.980 --> 00:02:11.160]   that we think has a good chance of success for the rest of the quarter.
[00:02:11.160 --> 00:02:12.460]   That's going to be out tonight.
[00:02:12.460 --> 00:02:15.660]   We'll have an ad announcement when it is out.
[00:02:15.660 --> 00:02:19.020]   And we want to get you feedback on that pretty quickly
[00:02:19.020 --> 00:02:22.140]   because you'll be working on this after assignment five is done.
[00:02:22.140 --> 00:02:26.340]   Really, the major core component of the course after that
[00:02:26.340 --> 00:02:29.380]   is the final project.
[00:02:29.380 --> 00:02:32.860]   OK, any questions?
[00:02:32.860 --> 00:02:33.360]   Cool.
[00:02:33.360 --> 00:02:33.860]   OK.
[00:02:33.860 --> 00:02:41.900]   So let's take a look back into what we've done so far in this course
[00:02:41.900 --> 00:02:47.120]   and see what we were doing in natural language processing.
[00:02:47.120 --> 00:02:48.080]   What was our strategy?
[00:02:48.080 --> 00:02:50.080]   If you had a natural language processing problem
[00:02:50.080 --> 00:02:53.060]   and you wanted to take your best effort attempt at it
[00:02:53.060 --> 00:02:56.040]   without doing anything too fancy, you would have said, OK,
[00:02:56.040 --> 00:03:01.180]   I'm going to have a bidirectional LSTM instead of a simple RNN.
[00:03:01.180 --> 00:03:04.220]   I'm going to use an LSTM to encode my sentences,
[00:03:04.220 --> 00:03:06.140]   I get bidirectional context.
[00:03:06.140 --> 00:03:09.300]   And if I have an output that I'm trying to generate,
[00:03:09.300 --> 00:03:14.100]   I'll have a unidirectional LSTM that I was going to generate one by one.
[00:03:14.100 --> 00:03:17.140]   So you have a translation or a parse or whatever.
[00:03:17.140 --> 00:03:20.500]   And so maybe I've encoded in a bidirectional LSTM the source sentence
[00:03:20.500 --> 00:03:24.260]   and I'm sort of one by one decoding out the target
[00:03:24.260 --> 00:03:26.480]   with my unidirectional LSTM.
[00:03:26.480 --> 00:03:30.680]   And then also, I was going to use something like attention
[00:03:30.680 --> 00:03:34.800]   to give flexible access to memory if I felt
[00:03:34.800 --> 00:03:37.080]   like I needed to do this sort of look back and see
[00:03:37.080 --> 00:03:39.040]   where I want to translate from.
[00:03:39.040 --> 00:03:41.960]   And this was just working exceptionally well.
[00:03:41.960 --> 00:03:46.200]   And we motivated attention through wanting to do machine translation.
[00:03:46.200 --> 00:03:48.140]   And you have this bottleneck where you don't
[00:03:48.140 --> 00:03:52.840]   want to have to encode the whole source sentence in a single vector.
[00:03:52.840 --> 00:03:55.000]   And in this lecture, we have the same goal.
[00:03:55.000 --> 00:03:57.380]   So we're going to be looking at a lot of the same problems
[00:03:57.380 --> 00:03:58.520]   that we did previously.
[00:03:58.520 --> 00:04:00.560]   But we're going to use different building blocks.
[00:04:00.560 --> 00:04:07.480]   We're going to say, if 2014 to 2017-ish I was using recurrence
[00:04:07.480 --> 00:04:10.400]   through lots of trial and error, years later,
[00:04:10.400 --> 00:04:12.720]   it had these brand new building blocks that we
[00:04:12.720 --> 00:04:17.160]   can plug in, direct replacement for LSTMs.
[00:04:17.160 --> 00:04:22.120]   And they're going to allow for just a huge range of much more successful
[00:04:22.120 --> 00:04:23.320]   applications.
[00:04:23.320 --> 00:04:28.680]   And so what are the issues with the recurrent neural networks
[00:04:28.680 --> 00:04:29.680]   we used to use?
[00:04:29.680 --> 00:04:32.720]   And what are the new systems that we're going to use from this point moving
[00:04:32.720 --> 00:04:35.160]   forward?
[00:04:35.160 --> 00:04:38.880]   So one of the issues with a recurrent neural network
[00:04:38.880 --> 00:04:41.680]   is what we're going to call linear interaction distance.
[00:04:41.680 --> 00:04:47.200]   So as we know, RNNs are unrolled left to right or right to left,
[00:04:47.200 --> 00:04:49.840]   depending on the language and the direction.
[00:04:49.840 --> 00:04:53.080]   But it encodes the notion of linear locality, which is useful.
[00:04:53.080 --> 00:04:55.600]   Because if two words occur right next to each other,
[00:04:55.600 --> 00:04:57.320]   sometimes they're actually quite related.
[00:04:57.320 --> 00:04:58.640]   So tasty pizza.
[00:04:58.640 --> 00:04:59.680]   They're nearby.
[00:04:59.680 --> 00:05:04.360]   And in the recurrent neural network, you encode tasty.
[00:05:04.360 --> 00:05:08.720]   And then you walk one step, and you encode pizza.
[00:05:08.720 --> 00:05:12.600]   So nearby words do often affect each other's meanings.
[00:05:12.600 --> 00:05:17.200]   But you have this problem where very long distance dependencies
[00:05:17.200 --> 00:05:18.960]   can take a very long time to interact.
[00:05:18.960 --> 00:05:21.400]   So if I have the sentence, the chef--
[00:05:21.400 --> 00:05:22.600]   so those are nearby.
[00:05:22.600 --> 00:05:25.120]   Those interact with each other.
[00:05:25.120 --> 00:05:28.680]   And then who, and then a bunch of stuff.
[00:05:28.680 --> 00:05:32.520]   Like the chef who went to the stores and picked up the ingredients
[00:05:32.520 --> 00:05:35.320]   and loves garlic.
[00:05:35.320 --> 00:05:37.160]   And then was.
[00:05:37.160 --> 00:05:40.440]   Like I actually have an RNN step, this sort
[00:05:40.440 --> 00:05:43.000]   of application of the recurrent weight matrix
[00:05:43.000 --> 00:05:47.320]   and some element-wise nonlinearities once, twice, three times.
[00:05:47.320 --> 00:05:52.520]   As many times as there is potentially the length of the sequence between chef
[00:05:52.520 --> 00:05:53.760]   and was.
[00:05:53.760 --> 00:05:54.960]   And it's the chef who was.
[00:05:54.960 --> 00:05:56.840]   So this is a long distance dependency.
[00:05:56.840 --> 00:06:01.120]   Should feel kind of related to the stuff that we did in dependency syntax.
[00:06:01.120 --> 00:06:06.440]   But it's quite difficult to learn potentially
[00:06:06.440 --> 00:06:09.080]   that these words should be related.
[00:06:09.080 --> 00:06:19.440]   So if you have a lot of steps between words,
[00:06:19.440 --> 00:06:22.400]   it can be difficult to learn the dependencies between them.
[00:06:22.400 --> 00:06:24.240]   We talked about all these gradient problems.
[00:06:24.240 --> 00:06:29.680]   LSTMs do a lot better at modeling the gradients across long distances
[00:06:29.680 --> 00:06:31.280]   than simple recurrent neural networks.
[00:06:31.280 --> 00:06:33.960]   But it's not perfect.
[00:06:33.960 --> 00:06:36.680]   And we already know that this linear order
[00:06:36.680 --> 00:06:40.520]   isn't sort of the right way to think about sentences.
[00:06:40.520 --> 00:06:46.920]   So if I wanted to learn that it's the chef who was,
[00:06:46.920 --> 00:06:51.720]   then I might have a hard time doing it because the gradients have
[00:06:51.720 --> 00:06:53.160]   to propagate from was to chef.
[00:06:53.160 --> 00:06:56.440]   And really, I'd like more direct connection
[00:06:56.440 --> 00:06:59.080]   between words that might be related in the sentence.
[00:06:59.080 --> 00:07:04.000]   Or in a document even, if these are going to get much longer.
[00:07:04.000 --> 00:07:06.160]   So this is this linear interaction distance problem.
[00:07:06.160 --> 00:07:08.400]   We would like words that might be related
[00:07:08.400 --> 00:07:11.000]   to be able to interact with each other in the neural networks
[00:07:11.000 --> 00:07:19.800]   computation graph more easily than being linearly far away
[00:07:19.800 --> 00:07:23.000]   so that we can learn these long distance dependencies better.
[00:07:23.000 --> 00:07:25.560]   And there's a related problem too that again comes back
[00:07:25.560 --> 00:07:28.640]   to the recurrent neural networks dependence on the index.
[00:07:28.640 --> 00:07:32.880]   On the index into the sequence, often called a dependence on time.
[00:07:32.880 --> 00:07:36.800]   So in a recurrent neural network, the forward and backward passes
[00:07:36.800 --> 00:07:39.520]   have O of sequence length many.
[00:07:39.520 --> 00:07:41.600]   So that means just roughly sequence, in this case,
[00:07:41.600 --> 00:07:45.000]   just sequence length many unparallelizable operations.
[00:07:45.000 --> 00:07:47.240]   So we know GPUs are great.
[00:07:47.240 --> 00:07:50.520]   They can do a lot of operations at once,
[00:07:50.520 --> 00:07:53.840]   as long as there's no dependency between the operations in terms
[00:07:53.840 --> 00:07:54.360]   of time.
[00:07:54.360 --> 00:07:57.680]   You have to compute one and then compute the other.
[00:07:57.680 --> 00:07:59.800]   But in a recurrent neural network, you
[00:07:59.800 --> 00:08:03.800]   can't actually compute the RNN hidden state for time step 5
[00:08:03.800 --> 00:08:06.920]   before you compute the RNN hidden state for time step 4
[00:08:06.920 --> 00:08:08.560]   or time step 3.
[00:08:08.560 --> 00:08:11.320]   And so you get this graph that looks very similar,
[00:08:11.320 --> 00:08:13.200]   where if I want to compute this hidden state,
[00:08:13.200 --> 00:08:16.160]   so I've got some word, I have zero operations
[00:08:16.160 --> 00:08:18.600]   I need to do before I can compute this state.
[00:08:18.600 --> 00:08:22.560]   I have one operation I can do before I can compute this state.
[00:08:22.560 --> 00:08:25.280]   And as my sequence length grows, I've got--
[00:08:25.280 --> 00:08:27.040]   OK, here I've got three operations
[00:08:27.040 --> 00:08:28.880]   I need to do before I can compute
[00:08:28.880 --> 00:08:32.080]   the state with the number 3, because I need to compute this
[00:08:32.080 --> 00:08:33.880]   and this and that.
[00:08:33.880 --> 00:08:37.000]   So there's three unparallelizable operations
[00:08:37.000 --> 00:08:39.640]   that I'm glomming all the matrix multiplies and stuff
[00:08:39.640 --> 00:08:40.880]   into a single one.
[00:08:40.880 --> 00:08:42.480]   So 1, 2, 3.
[00:08:42.480 --> 00:08:45.320]   And of course, this grows with the sequence length as well.
[00:08:45.320 --> 00:08:48.720]   So down over here, as the sequence length grows,
[00:08:48.720 --> 00:08:50.520]   I can't parallelize--
[00:08:50.520 --> 00:08:53.600]   I can't just have a big GPU just kachanka
[00:08:53.600 --> 00:08:56.840]   with the matrix multiply to compute this state,
[00:08:56.840 --> 00:08:59.600]   because I need to compute all the previous states beforehand.
[00:08:59.600 --> 00:09:03.520]   OK, any questions about that?
[00:09:03.520 --> 00:09:06.040]   So these are these two related problems,
[00:09:06.040 --> 00:09:07.960]   both with the dependence on time.
[00:09:07.960 --> 00:09:08.800]   Yeah.
[00:09:08.800 --> 00:09:11.360]   Yeah, so I have a question on the linear interaction issues.
[00:09:11.360 --> 00:09:13.880]   I thought that was the whole point of the attention network,
[00:09:13.880 --> 00:09:17.960]   and then how maybe you want, during the training,
[00:09:17.960 --> 00:09:21.080]   of the actual cells that depend more on each other.
[00:09:21.080 --> 00:09:22.840]   Can't we do something like the attention
[00:09:22.840 --> 00:09:26.200]   and then work our way around that?
[00:09:26.200 --> 00:09:28.760]   So the question is, with the linear interaction distance,
[00:09:28.760 --> 00:09:30.440]   wasn't this the point of attention
[00:09:30.440 --> 00:09:31.720]   that gets around that?
[00:09:31.720 --> 00:09:33.880]   Can't we use something with attention to help,
[00:09:33.880 --> 00:09:35.040]   or does that just help?
[00:09:35.040 --> 00:09:37.480]   So it won't solve the parallelizability problem.
[00:09:37.480 --> 00:09:39.840]   And in fact, everything we do in the rest of this lecture
[00:09:39.840 --> 00:09:41.160]   will be attention-based.
[00:09:41.160 --> 00:09:42.620]   But we'll get rid of the recurrence
[00:09:42.620 --> 00:09:44.360]   and just do attention, more or less.
[00:09:44.360 --> 00:09:48.720]   So well, yeah, it's a great intuition.
[00:09:48.720 --> 00:09:49.920]   Any other questions?
[00:09:49.920 --> 00:09:54.480]   OK, cool.
[00:09:54.480 --> 00:09:57.920]   So if not recurrence, what about attention?
[00:09:57.920 --> 00:10:00.040]   See, I'm just a slide back.
[00:10:00.040 --> 00:10:04.440]   And so we're going to get deep into attention today.
[00:10:04.440 --> 00:10:06.520]   But just for the second, attention
[00:10:06.520 --> 00:10:08.160]   treats each word's representation
[00:10:08.160 --> 00:10:11.480]   as a query to access and incorporate information
[00:10:11.480 --> 00:10:12.840]   from a set of values.
[00:10:12.840 --> 00:10:14.880]   So previously, we were in a decoder.
[00:10:14.880 --> 00:10:17.360]   We were decoding out a translation of a sentence.
[00:10:17.360 --> 00:10:19.280]   And we attended to the encoder so
[00:10:19.280 --> 00:10:21.520]   that we didn't have to store the entire representation
[00:10:21.520 --> 00:10:24.020]   of the source sentence into a single vector.
[00:10:24.020 --> 00:10:26.120]   And here, today, we'll think about attention
[00:10:26.120 --> 00:10:27.520]   within a single sentence.
[00:10:27.520 --> 00:10:29.840]   So I've got this sentence written out here
[00:10:29.840 --> 00:10:32.680]   with a word 1 through word t, in this case.
[00:10:32.680 --> 00:10:35.960]   And right on these integers in the boxes,
[00:10:35.960 --> 00:10:38.840]   I'm writing out the number of unparallelizable operations
[00:10:38.840 --> 00:10:41.880]   that you need to do before you can compute these.
[00:10:41.880 --> 00:10:43.600]   So for each word, you can independently
[00:10:43.600 --> 00:10:46.920]   compute its embedding without doing anything else previously,
[00:10:46.920 --> 00:10:50.320]   because the embedding just depends on the word identity.
[00:10:50.320 --> 00:10:53.460]   And then with attention, if I wanted
[00:10:53.460 --> 00:10:55.580]   to build an attention representation of this word
[00:10:55.580 --> 00:10:57.780]   by looking at all the other words in the sequence,
[00:10:57.780 --> 00:10:59.740]   that's one big operation.
[00:10:59.740 --> 00:11:02.460]   And I can do them in parallel for all the words.
[00:11:02.460 --> 00:11:04.460]   So the attention for this word, I
[00:11:04.460 --> 00:11:06.100]   can do for the attention for this word.
[00:11:06.100 --> 00:11:09.100]   I don't need to walk left to right like I did for an RNN.
[00:11:09.100 --> 00:11:10.940]   Again, we'll get much deeper into this.
[00:11:10.940 --> 00:11:13.940]   But you should have the intuition
[00:11:13.940 --> 00:11:16.220]   that it solves the linear interaction
[00:11:16.220 --> 00:11:18.860]   problem and the non-parallelizability problem.
[00:11:18.860 --> 00:11:22.120]   Because now, no matter how far away words are from each other,
[00:11:22.120 --> 00:11:23.980]   I am potentially interacting.
[00:11:23.980 --> 00:11:27.600]   I might just attend to you, even if you're very, very far away,
[00:11:27.600 --> 00:11:29.840]   sort of independent of how far away you are.
[00:11:29.840 --> 00:11:33.120]   And I also don't need to sort of walk along the sequence
[00:11:33.120 --> 00:11:34.320]   linearly long.
[00:11:34.320 --> 00:11:36.840]   So I'm treating the whole sequence at once.
[00:11:36.840 --> 00:11:38.320]   All right.
[00:11:38.320 --> 00:11:40.520]   So the intuition is that attention
[00:11:40.520 --> 00:11:42.280]   allows you to look very far away at once.
[00:11:42.280 --> 00:11:44.440]   And it doesn't have this dependence on the sequence
[00:11:44.440 --> 00:11:47.120]   index that keeps us from parallelizing operations.
[00:11:47.120 --> 00:11:48.620]   And so now, the rest of the lecture
[00:11:48.620 --> 00:11:51.780]   will talk in great depth about attention.
[00:11:51.780 --> 00:11:55.220]   So maybe let's just move on.
[00:11:55.220 --> 00:11:56.300]   OK.
[00:11:56.300 --> 00:12:00.180]   So let's think more deeply about attention.
[00:12:00.180 --> 00:12:02.660]   One thing that you might think of with attention
[00:12:02.660 --> 00:12:05.540]   is that it's sort of performing kind of a fuzzy lookup
[00:12:05.540 --> 00:12:07.180]   in a key value store.
[00:12:07.180 --> 00:12:09.540]   So you have a bunch of keys, a bunch of values,
[00:12:09.540 --> 00:12:12.140]   and it's going to help you sort of access that.
[00:12:12.140 --> 00:12:14.300]   So in an actual lookup table, just
[00:12:14.300 --> 00:12:18.220]   like a dictionary in Python, for example, very simple.
[00:12:18.220 --> 00:12:22.100]   You have a table of keys that each key maps to a value.
[00:12:22.100 --> 00:12:23.500]   And then you give it a query.
[00:12:23.500 --> 00:12:26.300]   And the query matches one of the keys.
[00:12:26.300 --> 00:12:27.940]   And then you return the value.
[00:12:27.940 --> 00:12:31.420]   So I've got a bunch of keys here.
[00:12:31.420 --> 00:12:33.220]   And my query matches the key.
[00:12:33.220 --> 00:12:34.660]   So I return the value.
[00:12:34.660 --> 00:12:36.940]   Simple, fair, easy.
[00:12:36.940 --> 00:12:37.740]   OK.
[00:12:37.740 --> 00:12:39.500]   Good.
[00:12:39.500 --> 00:12:44.060]   And in attention, so just like we saw before,
[00:12:44.060 --> 00:12:46.660]   the query matches all keys softly.
[00:12:46.660 --> 00:12:48.940]   There's no exact match.
[00:12:48.940 --> 00:12:50.780]   You sort of compute some sort of similarity
[00:12:50.780 --> 00:12:52.660]   between the key and all of the--
[00:12:52.660 --> 00:12:54.620]   sorry, the query and all of the keys.
[00:12:54.620 --> 00:12:56.260]   And then you sort of weight the results.
[00:12:56.260 --> 00:12:57.780]   So you've got a query again.
[00:12:57.780 --> 00:13:00.020]   You've got a bunch of keys.
[00:13:00.020 --> 00:13:04.500]   The query, to different extents, is similar to each of the keys.
[00:13:04.500 --> 00:13:08.460]   And you will sort of measure that similarity between 0 and 1
[00:13:08.460 --> 00:13:10.140]   through a softmax.
[00:13:10.140 --> 00:13:12.380]   And then you get the values out.
[00:13:12.380 --> 00:13:15.660]   So you average them via the weights of the similarity
[00:13:15.660 --> 00:13:18.780]   between the key and the query and the keys.
[00:13:18.780 --> 00:13:20.580]   You do a weighted sum with those weights.
[00:13:20.580 --> 00:13:21.660]   And you get an output.
[00:13:21.660 --> 00:13:24.780]   So it really is quite a bit like a lookup table,
[00:13:24.780 --> 00:13:29.940]   but in this sort of soft vector space, mushy sort of sense.
[00:13:29.940 --> 00:13:32.140]   So I'm really doing some kind of accessing
[00:13:32.140 --> 00:13:35.820]   into this information that's stored in the key value store.
[00:13:35.820 --> 00:13:38.220]   But I'm sort of softly looking at all of the results.
[00:13:41.220 --> 00:13:42.300]   OK, any questions there?
[00:13:42.300 --> 00:13:46.940]   Cool.
[00:13:46.940 --> 00:13:48.620]   So what might this look like?
[00:13:48.620 --> 00:13:50.980]   So if I was trying to represent this sentence,
[00:13:50.980 --> 00:13:54.260]   I went to Stanford CS224n and learned.
[00:13:54.260 --> 00:13:56.260]   So I'm trying to build a representation of learned.
[00:13:56.260 --> 00:14:01.580]   I have a key for each word.
[00:14:01.580 --> 00:14:04.500]   So this is this self-attention thing that we'll get into.
[00:14:04.500 --> 00:14:06.740]   I have a key for each word, a value for each word.
[00:14:06.740 --> 00:14:08.380]   I've got the query for learned.
[00:14:08.380 --> 00:14:11.620]   And I've got these sort of tealish bars up top,
[00:14:11.620 --> 00:14:13.620]   which sort of might say how much you're
[00:14:13.620 --> 00:14:15.500]   going to try to access each of the word.
[00:14:15.500 --> 00:14:18.300]   Like, oh, maybe 224n is not that important.
[00:14:18.300 --> 00:14:20.500]   CS, maybe that determines what I learned.
[00:14:20.500 --> 00:14:22.700]   You know, Stanford.
[00:14:22.700 --> 00:14:25.180]   And then learned, maybe that's important to representing
[00:14:25.180 --> 00:14:25.900]   itself.
[00:14:25.900 --> 00:14:28.100]   So you sort of look across at the whole sentence
[00:14:28.100 --> 00:14:31.020]   and build up this sort of soft accessing of information
[00:14:31.020 --> 00:14:35.860]   across the sentence in order to represent learned in context.
[00:14:35.860 --> 00:14:38.860]   So this is just a toy diagram.
[00:14:38.860 --> 00:14:40.460]   So let's get into the math.
[00:14:40.460 --> 00:14:43.540]   So we're going to look at a sequence of words.
[00:14:43.540 --> 00:14:46.860]   So that's w1 to n, a sequence of words in a vocabulary.
[00:14:46.860 --> 00:14:49.140]   So this is like, you know, Zuko made his uncle tea.
[00:14:49.140 --> 00:14:50.340]   That's a good sequence.
[00:14:50.340 --> 00:14:52.580]   And for each word, we're going to embed it
[00:14:52.580 --> 00:14:54.620]   with this embedding matrix, just like we've
[00:14:54.620 --> 00:14:56.180]   been doing in this class.
[00:14:56.180 --> 00:14:57.780]   So I have this embedding matrix that
[00:14:57.780 --> 00:15:02.460]   goes from the vocabulary size to the dimensionality d.
[00:15:02.460 --> 00:15:04.660]   So each word has a non-contextual,
[00:15:04.660 --> 00:15:07.500]   only dependent on itself, word embedding.
[00:15:07.500 --> 00:15:11.340]   And now I'm going to transform each word with one of three
[00:15:11.340 --> 00:15:12.500]   different weight matrices.
[00:15:12.500 --> 00:15:16.820]   So this is often called key query value self-attention.
[00:15:16.820 --> 00:15:19.980]   So I have a matrix Q, which is an rd to d.
[00:15:19.980 --> 00:15:23.420]   So this maps xi, which is a vector of dimensionality d,
[00:15:23.420 --> 00:15:25.780]   to another vector of dimensionality d.
[00:15:25.780 --> 00:15:28.500]   And that's going to be a query vector.
[00:15:28.500 --> 00:15:31.260]   So it takes an xi and it sort of rotates it,
[00:15:31.260 --> 00:15:33.940]   shuffles it around, stretches it, squishes it.
[00:15:33.940 --> 00:15:35.060]   Makes it different.
[00:15:35.060 --> 00:15:35.940]   And now it's a query.
[00:15:35.940 --> 00:15:38.300]   And now for a different learnable parameter, k--
[00:15:38.300 --> 00:15:39.500]   so that's another matrix.
[00:15:39.500 --> 00:15:41.940]   I'm going to come up with my keys.
[00:15:41.940 --> 00:15:45.220]   And with a different learnable parameter, v,
[00:15:45.220 --> 00:15:47.100]   I'm going to come up with my values.
[00:15:47.100 --> 00:15:49.640]   So I'm taking each of the non-contextual word embeddings,
[00:15:49.640 --> 00:15:53.300]   each of these xi's, and I'm transforming each of them
[00:15:53.300 --> 00:15:57.140]   to come up with my query for that word, my key for that word,
[00:15:57.140 --> 00:16:00.220]   and my value for that word.
[00:16:00.220 --> 00:16:03.700]   So every word is doing each of these roles.
[00:16:03.700 --> 00:16:06.660]   Next, I'm going to compute all pairs of similarities
[00:16:06.660 --> 00:16:08.220]   between the keys and queries.
[00:16:08.220 --> 00:16:10.500]   So in the toy example we saw, I was
[00:16:10.500 --> 00:16:13.260]   computing the similarity between a single query for the word
[00:16:13.260 --> 00:16:17.380]   learned and all of the keys for the entire sentence.
[00:16:17.380 --> 00:16:20.380]   In this context, I'm computing all pairs of similarities
[00:16:20.380 --> 00:16:24.300]   between all keys and all values because I want to represent
[00:16:24.300 --> 00:16:25.140]   all of these sums.
[00:16:25.140 --> 00:16:27.620]   So I've got this sort of dot--
[00:16:27.620 --> 00:16:29.780]   I'm just going to take the dot product between these two
[00:16:29.780 --> 00:16:30.420]   vectors.
[00:16:30.420 --> 00:16:31.820]   So I've got qi.
[00:16:31.820 --> 00:16:34.140]   So this is saying the query for word i
[00:16:34.140 --> 00:16:36.100]   dotted with the key for word j.
[00:16:36.100 --> 00:16:40.980]   And I get this score, which is a real value.
[00:16:40.980 --> 00:16:42.820]   Might be very large negative, might be zero,
[00:16:42.820 --> 00:16:44.660]   might be very large and positive.
[00:16:44.660 --> 00:16:46.580]   And so that's like, how much should I
[00:16:46.580 --> 00:16:50.140]   look at j in this lookup table?
[00:16:50.140 --> 00:16:51.340]   And then I do the softmax.
[00:16:51.340 --> 00:16:52.580]   So I softmax.
[00:16:52.580 --> 00:16:55.140]   So I say that the actual weight that I'm
[00:16:55.140 --> 00:16:58.660]   going to look at j from i is softmax of this
[00:16:58.660 --> 00:17:00.900]   over all of the possible indices.
[00:17:00.900 --> 00:17:03.780]   So it's like the affinity between i and j
[00:17:03.780 --> 00:17:06.020]   normalized by the affinity between i
[00:17:06.020 --> 00:17:08.220]   and all of the possible j prime in the sequence.
[00:17:08.220 --> 00:17:13.940]   And then my output is just the weighted sum of values.
[00:17:13.940 --> 00:17:16.060]   So I've got this output for word i.
[00:17:16.060 --> 00:17:18.420]   So maybe i is like 1 for Zuko.
[00:17:18.420 --> 00:17:22.140]   And I'm representing it as the sum of these weights
[00:17:22.140 --> 00:17:23.140]   for all j.
[00:17:23.140 --> 00:17:26.180]   So Zuko and maid and his and uncle and t.
[00:17:26.180 --> 00:17:30.140]   And the value vector for that word j.
[00:17:30.140 --> 00:17:34.940]   I'm looking from i to j as much as alpha ij.
[00:17:34.940 --> 00:17:37.380]   What's the dimension of Wi?
[00:17:37.380 --> 00:17:39.380]   [INAUDIBLE]
[00:17:39.380 --> 00:17:44.900]   Oh, Wi, you can either think of it as a symbol in vocab v.
[00:17:44.900 --> 00:17:47.660]   So that's like, you could think of it as a one-hot vector.
[00:17:47.660 --> 00:17:51.220]   And yeah, in this case, we are, I guess, thinking of it as--
[00:17:51.220 --> 00:17:54.380]   so one-hot vector in dimensionality size of vocab.
[00:17:54.380 --> 00:17:59.660]   So in the matrix E, you see that it's r d by bars around v.
[00:17:59.660 --> 00:18:01.700]   That's size of the vocabulary.
[00:18:01.700 --> 00:18:05.100]   So when I do E multiplied by Wi, that's
[00:18:05.100 --> 00:18:10.540]   taking E, which is d by v, multiplying it by w, which is v,
[00:18:10.540 --> 00:18:13.100]   and returning a vector that's dimensionality d.
[00:18:13.100 --> 00:18:16.940]   So w in that first line, like w1n,
[00:18:16.940 --> 00:18:20.700]   that's a matrix where it has maybe
[00:18:20.700 --> 00:18:23.500]   like a column for every word in that sentence.
[00:18:23.500 --> 00:18:25.820]   And each column is a length v.
[00:18:25.820 --> 00:18:28.460]   Yeah, usually, I guess we think of it as having a--
[00:18:28.460 --> 00:18:31.780]   I mean, if I'm putting the sequence length index first,
[00:18:31.780 --> 00:18:33.980]   you might think of it as having a row for each word.
[00:18:33.980 --> 00:18:37.020]   But similarly, yeah, it's n, which is the sequence length.
[00:18:37.020 --> 00:18:39.020]   And then the second dimension would be v,
[00:18:39.020 --> 00:18:40.740]   which is the vocabulary size.
[00:18:40.740 --> 00:18:42.740]   And then that gets mapped to this thing, which
[00:18:42.740 --> 00:18:46.380]   is sequence length by d.
[00:18:46.380 --> 00:18:49.460]   Why do we learn two different matrices, q and k,
[00:18:49.460 --> 00:18:51.420]   when q transpose--
[00:18:51.420 --> 00:18:56.100]   qi transpose kj is really just one matrix in the middle?
[00:18:56.100 --> 00:18:57.100]   That's a great question.
[00:18:57.100 --> 00:18:59.500]   It ends up being because this will end up
[00:18:59.500 --> 00:19:02.060]   being a low-rank approximation to that matrix.
[00:19:02.060 --> 00:19:05.500]   So it is for computational efficiency reasons.
[00:19:05.500 --> 00:19:07.660]   Although it also, I think, feels kind
[00:19:07.660 --> 00:19:09.940]   of nice in the presentation.
[00:19:09.940 --> 00:19:11.300]   But yeah, what we'll end up doing
[00:19:11.300 --> 00:19:14.860]   is having a very low-rank approximation to qk transpose.
[00:19:14.860 --> 00:19:17.380]   And so you actually do do it like this.
[00:19:17.380 --> 00:19:19.780]   It's a good question.
[00:19:19.780 --> 00:19:26.140]   Is vii, so the query with any specific?
[00:19:26.140 --> 00:19:27.620]   Sorry, could you repeat that for me?
[00:19:27.620 --> 00:19:32.620]   This eii, so the query of the word dotted with the key
[00:19:32.620 --> 00:19:34.980]   by itself, does it look like an identity,
[00:19:34.980 --> 00:19:37.420]   or does it look like anything in particular?
[00:19:37.420 --> 00:19:38.340]   That's a good question.
[00:19:38.340 --> 00:19:40.500]   OK, let me remember to repeat questions.
[00:19:40.500 --> 00:19:44.660]   So does eii, for j equal to i, so looking at itself,
[00:19:44.660 --> 00:19:46.080]   look like anything in particular?
[00:19:46.080 --> 00:19:47.660]   Does it look like the identity?
[00:19:47.660 --> 00:19:48.820]   Is that the question?
[00:19:48.820 --> 00:19:53.020]   OK, so right, it's unclear, actually.
[00:19:53.020 --> 00:19:54.940]   This question of should you look at yourself
[00:19:54.940 --> 00:19:57.020]   for representing yourself, well, it's
[00:19:57.020 --> 00:20:00.780]   going to be encoded by the matrices q and k.
[00:20:00.780 --> 00:20:02.940]   If I didn't have q and k in there,
[00:20:02.940 --> 00:20:04.940]   if those were the identity matrices,
[00:20:04.940 --> 00:20:07.460]   if q is identity, k is identity, then this
[00:20:07.460 --> 00:20:09.860]   would be sort of dot product with yourself, which is going
[00:20:09.860 --> 00:20:12.180]   to be high on average, like you're pointing
[00:20:12.180 --> 00:20:13.860]   in the same direction as yourself.
[00:20:13.860 --> 00:20:18.460]   But it could be that qxi and kxi might
[00:20:18.460 --> 00:20:21.060]   be sort of arbitrarily different from each other,
[00:20:21.060 --> 00:20:24.140]   because q could be the identity, and k
[00:20:24.140 --> 00:20:27.060]   could map you to the negative of yourself,
[00:20:27.060 --> 00:20:29.060]   for example, so that you don't look at yourself.
[00:20:29.060 --> 00:20:30.940]   So this is all learned in practice.
[00:20:30.940 --> 00:20:32.380]   So you end up--
[00:20:32.380 --> 00:20:35.860]   it can sort of decide by learning
[00:20:35.860 --> 00:20:38.180]   whether you should be looking at yourself or not.
[00:20:38.180 --> 00:20:39.580]   And that's some of the flexibility
[00:20:39.580 --> 00:20:42.820]   that parametrizing it as q and k gives you
[00:20:42.820 --> 00:20:46.180]   that wouldn't be there if I just used xis everywhere
[00:20:46.180 --> 00:20:49.820]   in this equation.
[00:20:49.820 --> 00:20:51.860]   I'm going to try to move on, I'm afraid,
[00:20:51.860 --> 00:20:53.340]   because there's a lot to get on.
[00:20:53.340 --> 00:20:55.540]   But we'll keep talking about self-attention.
[00:20:55.540 --> 00:20:57.660]   And so as more questions come up,
[00:20:57.660 --> 00:21:01.460]   I can also potentially return back.
[00:21:01.460 --> 00:21:05.140]   OK, so this is our basic building block.
[00:21:05.140 --> 00:21:06.860]   But there are a bunch of barriers
[00:21:06.860 --> 00:21:10.260]   to using it as a replacement for LSTMs.
[00:21:10.260 --> 00:21:13.020]   And so what we're going to do for this portion of the lecture
[00:21:13.020 --> 00:21:14.740]   is talk about the minimal components
[00:21:14.740 --> 00:21:18.180]   that we need in order to use self-attention as sort
[00:21:18.180 --> 00:21:21.540]   of this very fundamental building block.
[00:21:21.540 --> 00:21:25.260]   So we can't use it as it stands as I've presented it,
[00:21:25.260 --> 00:21:26.740]   because there are a couple of things
[00:21:26.740 --> 00:21:29.340]   that we need to sort of solve or fix.
[00:21:29.340 --> 00:21:32.180]   One of them is that there's no notion of sequence order
[00:21:32.180 --> 00:21:33.500]   in self-attention.
[00:21:33.500 --> 00:21:37.580]   So what does this mean?
[00:21:37.580 --> 00:21:40.140]   If I have a sentence like--
[00:21:40.140 --> 00:21:42.380]   I'm going to move over here to the whiteboard briefly,
[00:21:42.380 --> 00:21:46.820]   and hopefully I'll write quite large.
[00:21:46.820 --> 00:21:55.700]   If I have a sentence like, Zuko made his uncle.
[00:21:55.700 --> 00:22:05.780]   And let's say, his uncle made Zuko.
[00:22:05.780 --> 00:22:08.700]   If I were to embed each of these words
[00:22:08.700 --> 00:22:10.700]   using its embedding matrix, the embedding matrix
[00:22:10.700 --> 00:22:14.980]   isn't dependent on the index of the word.
[00:22:14.980 --> 00:22:18.060]   So this is the word index 1, 2, 3, 4,
[00:22:18.060 --> 00:22:21.900]   versus now his is over here, and uncle.
[00:22:21.900 --> 00:22:23.900]   And so when I compute the self-attention--
[00:22:23.900 --> 00:22:26.240]   and there's a lot more on this in the lecture notes that
[00:22:26.240 --> 00:22:29.860]   goes through a full example--
[00:22:29.860 --> 00:22:32.260]   the actual self-attention operation
[00:22:32.260 --> 00:22:34.380]   will give you exactly the same representations
[00:22:34.380 --> 00:22:38.060]   for this sequence, Zuko made his uncle, as for this sequence,
[00:22:38.060 --> 00:22:40.020]   his uncle made Zuko.
[00:22:40.020 --> 00:22:41.680]   And that's bad, because they're sentences
[00:22:41.680 --> 00:22:43.900]   that mean different things.
[00:22:43.900 --> 00:22:46.820]   And so it's this idea that self-attention
[00:22:46.820 --> 00:22:48.580]   is an operation on sets.
[00:22:48.580 --> 00:22:50.780]   You have a set of vectors that you're
[00:22:50.780 --> 00:22:52.660]   going to perform self-attention on,
[00:22:52.660 --> 00:22:55.740]   and nowhere does the exact position of the words
[00:22:55.740 --> 00:22:59.140]   come into play directly.
[00:22:59.140 --> 00:23:02.540]   So we're going to encode the position of words
[00:23:02.540 --> 00:23:06.100]   through the keys, queries, and values that we have.
[00:23:06.100 --> 00:23:10.060]   So consider now representing each sequence index--
[00:23:10.060 --> 00:23:12.100]   our sequences are going from 1 to n--
[00:23:12.100 --> 00:23:13.380]   as a vector.
[00:23:13.380 --> 00:23:17.220]   So don't worry so far about how it's being made,
[00:23:17.220 --> 00:23:20.300]   but you can imagine representing the number 1,
[00:23:20.300 --> 00:23:23.540]   the position 1, the position 2, the position 3,
[00:23:23.540 --> 00:23:25.580]   as a vector in the dimensionality d,
[00:23:25.580 --> 00:23:29.260]   just like we're representing our keys, queries, and values.
[00:23:29.260 --> 00:23:33.060]   And so these are position vectors.
[00:23:33.060 --> 00:23:37.940]   If you were to want to incorporate the information
[00:23:37.940 --> 00:23:42.380]   represented by these positions into our self-attention,
[00:23:42.380 --> 00:23:45.140]   you could just add these vectors, these p i
[00:23:45.140 --> 00:23:48.020]   vectors, to the inputs.
[00:23:48.020 --> 00:23:53.340]   So if I have this xi embedding of a word, which
[00:23:53.340 --> 00:23:56.060]   is the word at position i, but really just represents,
[00:23:56.060 --> 00:23:59.100]   oh, the word zuko is here, now I can say, oh, it's the word
[00:23:59.100 --> 00:24:03.860]   zuko, and it's at position 5, because this vector represents
[00:24:03.860 --> 00:24:04.660]   position 5.
[00:24:04.660 --> 00:24:11.260]   So how do we do this?
[00:24:11.260 --> 00:24:12.860]   And we might only have to do this once.
[00:24:12.860 --> 00:24:16.420]   So we can do it once at the very input to the network,
[00:24:16.420 --> 00:24:18.260]   and then that is sufficient.
[00:24:18.260 --> 00:24:19.840]   We don't have to do it at every layer,
[00:24:19.840 --> 00:24:23.660]   because it knows from the input.
[00:24:23.660 --> 00:24:26.060]   So one way in which people have done this
[00:24:26.060 --> 00:24:29.500]   is look at these sinusoidal position representations.
[00:24:29.500 --> 00:24:32.100]   So this looks a little bit like this, where you have--
[00:24:32.100 --> 00:24:35.980]   so this is a vector p i, which is in dimensionality d.
[00:24:35.980 --> 00:24:40.500]   And each one of the dimensions, you take the value i,
[00:24:40.500 --> 00:24:45.740]   you modify it by some constant, and you pass it
[00:24:45.740 --> 00:24:47.780]   to the sine or cosine function, and you
[00:24:47.780 --> 00:24:49.900]   get these sort of values that vary
[00:24:49.900 --> 00:24:53.260]   according to the period, differing periods depending
[00:24:53.260 --> 00:24:54.340]   on the dimensionality's d.
[00:24:54.340 --> 00:24:57.500]   So I've got this sort of a representation of a matrix,
[00:24:57.500 --> 00:24:59.580]   where d is the vertical dimension,
[00:24:59.580 --> 00:25:01.500]   and then n is the horizontal.
[00:25:01.500 --> 00:25:05.580]   And you can see that there's sort of like, oh, as I walk
[00:25:05.580 --> 00:25:08.300]   along, you see the period of the sine function going up and down,
[00:25:08.300 --> 00:25:11.220]   and each of the dimensions d has a different period.
[00:25:11.220 --> 00:25:13.620]   And so together, you can represent a bunch of different
[00:25:13.620 --> 00:25:15.140]   sort of position indices.
[00:25:15.140 --> 00:25:20.020]   And it gives this intuition that, oh, maybe sort
[00:25:20.020 --> 00:25:22.780]   of the absolute position of a word isn't as important.
[00:25:22.780 --> 00:25:24.220]   You've got the sort of periodicity
[00:25:24.220 --> 00:25:26.220]   of the sines and cosines.
[00:25:26.220 --> 00:25:29.500]   And maybe that allows you to extrapolate to longer sequences.
[00:25:29.500 --> 00:25:32.140]   But in practice, that doesn't work.
[00:25:32.140 --> 00:25:34.220]   But this is sort of like an early notion
[00:25:34.220 --> 00:25:37.140]   that is still sometimes used for how to represent position
[00:25:37.140 --> 00:25:40.700]   in transformers and self-attention networks
[00:25:40.700 --> 00:25:43.260]   in general.
[00:25:43.260 --> 00:25:45.180]   So that's one idea.
[00:25:45.180 --> 00:25:48.580]   You might think it's a little bit complicated,
[00:25:48.580 --> 00:25:50.300]   a little bit unintuitive.
[00:25:50.300 --> 00:25:54.380]   Here's something that feels a little bit more deep learning.
[00:25:54.380 --> 00:25:57.460]   So we're just going to say, oh, I've
[00:25:57.460 --> 00:25:59.860]   got a maximum sequence length of n.
[00:25:59.860 --> 00:26:01.940]   And I'm just going to learn a matrix that's
[00:26:01.940 --> 00:26:03.700]   dimensionality d by n.
[00:26:03.700 --> 00:26:05.500]   And that's going to represent my positions.
[00:26:05.500 --> 00:26:07.780]   And I'm going to learn it as a parameter, just like I
[00:26:07.780 --> 00:26:09.100]   learn every other parameter.
[00:26:09.100 --> 00:26:10.020]   And what do they mean?
[00:26:10.020 --> 00:26:10.860]   Oh, I have no idea.
[00:26:10.860 --> 00:26:13.100]   But it represents position.
[00:26:13.100 --> 00:26:19.420]   So you just sort of add this matrix to the xi's,
[00:26:19.420 --> 00:26:22.140]   your input embeddings.
[00:26:22.140 --> 00:26:24.180]   And it learns to fit to data.
[00:26:24.180 --> 00:26:26.300]   So whatever representation of position
[00:26:26.300 --> 00:26:30.300]   that's linear, sort of index-based that you want,
[00:26:30.300 --> 00:26:31.460]   you can learn.
[00:26:31.460 --> 00:26:33.900]   And the cons are that, well, you definitely now
[00:26:33.900 --> 00:26:37.980]   can't represent anything that's longer than n words long, right?
[00:26:37.980 --> 00:26:41.660]   No sequence longer than n you can handle because, well,
[00:26:41.660 --> 00:26:44.420]   you only learned a matrix of this many positions.
[00:26:44.420 --> 00:26:47.660]   And so in practice, you'll get a model error
[00:26:47.660 --> 00:26:50.500]   if you pass a self-attention model, something longer
[00:26:50.500 --> 00:26:51.620]   than length n.
[00:26:51.620 --> 00:26:56.620]   It will just sort of crash and say, I can't do this.
[00:26:56.620 --> 00:26:59.660]   And so this is sort of what most systems nowadays use.
[00:26:59.660 --> 00:27:02.220]   There are more flexible representations of position,
[00:27:02.220 --> 00:27:04.940]   including a couple in the lecture notes.
[00:27:04.940 --> 00:27:08.020]   You might want to look at the relative linear position,
[00:27:08.020 --> 00:27:09.900]   or words before or after each other,
[00:27:09.900 --> 00:27:11.500]   but not their absolute position.
[00:27:11.500 --> 00:27:13.300]   There's also some sort of representations
[00:27:13.300 --> 00:27:16.660]   that harken back to our dependency syntax.
[00:27:16.660 --> 00:27:19.100]   Because, oh, maybe words that are close in the dependency
[00:27:19.100 --> 00:27:21.620]   parse tree should be the things that are sort of close
[00:27:21.620 --> 00:27:25.060]   in the self-attention operation.
[00:27:25.060 --> 00:27:28.340]   OK, questions?
[00:27:28.340 --> 00:27:32.420]   In practice, do we typically just make n large enough
[00:27:32.420 --> 00:27:36.500]   that we don't run into the issue of having something
[00:27:36.500 --> 00:27:39.060]   that can be input longer than n?
[00:27:39.060 --> 00:27:41.900]   So the question is, in practice, do we just make n long enough
[00:27:41.900 --> 00:27:44.100]   that we don't run into the problem where we're going
[00:27:44.100 --> 00:27:46.660]   to look at a text longer than n?
[00:27:46.660 --> 00:27:49.420]   No, in practice, it's actually quite a problem.
[00:27:49.420 --> 00:27:52.540]   Even today, even in the largest, biggest language models,
[00:27:52.540 --> 00:27:58.020]   and can I fit this prompt into chat GPT or whatever?
[00:27:58.020 --> 00:27:59.820]   It's a thing that you might see on Twitter.
[00:27:59.820 --> 00:28:01.980]   These continue to be issues.
[00:28:01.980 --> 00:28:04.980]   And part of it is because the self-attention operation--
[00:28:04.980 --> 00:28:06.900]   and we'll get into this later in the lecture--
[00:28:06.900 --> 00:28:10.060]   it's quadratic complexity in the sequence length.
[00:28:10.060 --> 00:28:14.420]   So you're going to spend n squared memory budget in order
[00:28:14.420 --> 00:28:15.740]   to make sequence lengths longer.
[00:28:15.740 --> 00:28:21.420]   So in practice, this might be on a large model, say, 4,000 or so.
[00:28:21.420 --> 00:28:24.620]   n is 4,000, so you can fit 4,000 words, which feels like a lot,
[00:28:24.620 --> 00:28:26.220]   but it's not going to fit a novel.
[00:28:26.220 --> 00:28:29.740]   It's not going to fit a Wikipedia page.
[00:28:29.740 --> 00:28:33.740]   And there are models that do longer sequences, for sure.
[00:28:33.740 --> 00:28:35.280]   And again, we'll talk a bit about it,
[00:28:35.280 --> 00:28:36.700]   but no, this actually is an issue.
[00:28:36.700 --> 00:28:43.140]   How do you know that the p you learned
[00:28:43.140 --> 00:28:47.580]   is the position, which is not any other?
[00:28:47.580 --> 00:28:48.220]   I don't.
[00:28:48.220 --> 00:28:49.100]   It's yours.
[00:28:49.100 --> 00:28:49.700]   Yeah.
[00:28:49.700 --> 00:28:51.580]   So how do you know that the p that you've learned,
[00:28:51.580 --> 00:28:53.960]   this matrix that you've learned, is representing position
[00:28:53.960 --> 00:28:55.540]   as opposed to anything else?
[00:28:55.540 --> 00:28:58.700]   And the reason is the only thing it correlates is position.
[00:28:58.700 --> 00:29:02.180]   So when I see these vectors, I'm adding this p matrix
[00:29:02.180 --> 00:29:05.460]   to my x matrix, the word embeddings.
[00:29:05.460 --> 00:29:06.720]   I'm adding them together.
[00:29:06.720 --> 00:29:08.420]   And the words that show up at each index
[00:29:08.420 --> 00:29:10.900]   will vary depending on what word actually
[00:29:10.900 --> 00:29:12.380]   showed up there in the example.
[00:29:12.380 --> 00:29:13.820]   But the p matrix never differs.
[00:29:13.820 --> 00:29:16.300]   It's always exactly the same at every index.
[00:29:16.300 --> 00:29:18.300]   And so it's the only thing in the data
[00:29:18.300 --> 00:29:19.260]   that it correlates with.
[00:29:19.260 --> 00:29:21.260]   So you're learning it implicitly.
[00:29:21.260 --> 00:29:24.540]   This vector at index 1 is always at index 1 for every example,
[00:29:24.540 --> 00:29:26.300]   for every gradient update.
[00:29:26.300 --> 00:29:31.900]   And nothing else co-occurs like that.
[00:29:31.900 --> 00:29:32.380]   Yeah.
[00:29:32.380 --> 00:29:33.820]   So what do you end up learning?
[00:29:33.820 --> 00:29:34.340]   I don't know.
[00:29:34.340 --> 00:29:34.900]   It's unclear.
[00:29:34.900 --> 00:29:37.820]   But it definitely allows you to know, oh, this word
[00:29:37.820 --> 00:29:39.020]   is with this index.
[00:29:39.020 --> 00:29:41.700]   Yeah.
[00:29:41.700 --> 00:29:42.200]   OK.
[00:29:42.200 --> 00:29:42.700]   Yeah.
[00:29:42.700 --> 00:29:47.100]   Just quickly, when you say quadratic constant in space,
[00:29:47.100 --> 00:29:49.580]   is a sequence right now defined as a sequence?
[00:29:49.580 --> 00:29:51.180]   Is there a sequence of words?
[00:29:51.180 --> 00:29:57.300]   Or I'm trying to figure out what unit is using it.
[00:29:57.300 --> 00:29:57.800]   OK.
[00:29:57.800 --> 00:30:00.300]   So the question is, when this is quadratic in the sequence,
[00:30:00.300 --> 00:30:01.420]   is that a sequence of words?
[00:30:01.420 --> 00:30:01.920]   Yeah.
[00:30:01.920 --> 00:30:03.700]   Think of it as a sequence of words.
[00:30:03.700 --> 00:30:05.120]   Sometimes there'll be pieces that
[00:30:05.120 --> 00:30:06.620]   are smaller than words, which we'll
[00:30:06.620 --> 00:30:08.180]   go into in the next lecture.
[00:30:08.180 --> 00:30:08.700]   But yeah.
[00:30:08.700 --> 00:30:10.220]   Think of this as a sequence of words,
[00:30:10.220 --> 00:30:12.100]   but not necessarily just for a sentence,
[00:30:12.100 --> 00:30:15.380]   maybe for an entire paragraph, or an entire document,
[00:30:15.380 --> 00:30:16.980]   or something like that.
[00:30:16.980 --> 00:30:17.480]   OK.
[00:30:17.480 --> 00:30:19.820]   But the attention is where it is.
[00:30:19.820 --> 00:30:23.700]   Yeah, the attention is based words to words.
[00:30:23.700 --> 00:30:24.260]   OK.
[00:30:24.260 --> 00:30:25.060]   Cool.
[00:30:25.060 --> 00:30:27.100]   I'm going to move on.
[00:30:27.100 --> 00:30:28.700]   OK.
[00:30:28.700 --> 00:30:29.200]   Right.
[00:30:29.200 --> 00:30:30.540]   So we have another problem.
[00:30:30.540 --> 00:30:34.060]   Another is that, based on the presentation of self-attention
[00:30:34.060 --> 00:30:36.900]   that we've done, there's really no nonlinearities
[00:30:36.900 --> 00:30:38.940]   for deep learning magic.
[00:30:38.940 --> 00:30:43.420]   We're just computing weighted averages of stuff.
[00:30:43.420 --> 00:30:47.820]   So if I apply self-attention, and then apply self-attention
[00:30:47.820 --> 00:30:50.660]   again, and then again, and again, and again,
[00:30:50.660 --> 00:30:52.820]   you should look at the next lecture notes
[00:30:52.820 --> 00:30:53.540]   if you're interested in this.
[00:30:53.540 --> 00:30:54.580]   It's actually quite cool.
[00:30:54.580 --> 00:30:56.280]   But what you end up doing is you're just
[00:30:56.280 --> 00:30:58.240]   re-averaging value vectors together.
[00:30:58.240 --> 00:31:00.940]   So you're computing averages of value vectors,
[00:31:00.940 --> 00:31:03.700]   and it ends up looking like one big self-attention.
[00:31:03.700 --> 00:31:05.400]   But there's an easy fix to this if you
[00:31:05.400 --> 00:31:07.980]   want the traditional deep learning magic.
[00:31:07.980 --> 00:31:10.180]   And you can just add a feed-forward network
[00:31:10.180 --> 00:31:11.940]   to post-process each output vector.
[00:31:11.940 --> 00:31:13.500]   So I've got a word here.
[00:31:13.500 --> 00:31:15.460]   That's the output of self-attention.
[00:31:15.460 --> 00:31:17.460]   And I'm going to pass it through--
[00:31:17.460 --> 00:31:20.380]   in this case, I'm calling it a multilayer perceptron MLP.
[00:31:20.380 --> 00:31:23.820]   So this is a vector in Rd that's going to be--
[00:31:23.820 --> 00:31:26.420]   and it's taking in as input a vector in Rd.
[00:31:26.420 --> 00:31:30.020]   And you do the usual multilayer perceptron thing,
[00:31:30.020 --> 00:31:32.460]   where you have the output, and you multiply it by a matrix,
[00:31:32.460 --> 00:31:36.020]   pass it through a nonlinearity, multiply it by another matrix.
[00:31:36.020 --> 00:31:38.300]   And so what this looks like in self-attention
[00:31:38.300 --> 00:31:40.580]   is that I've got this sentence, the chef who--
[00:31:40.580 --> 00:31:42.140]   da, da, da, da, da-- food.
[00:31:42.140 --> 00:31:44.040]   And I've got my embeddings for it.
[00:31:44.040 --> 00:31:46.780]   I pass it through this whole big self-attention block, which
[00:31:46.780 --> 00:31:49.140]   looks at the whole sequence and incorporates
[00:31:49.140 --> 00:31:50.540]   context and all that.
[00:31:50.540 --> 00:31:52.660]   And then I pass each one individually
[00:31:52.660 --> 00:31:55.300]   through a feed-forward layer.
[00:31:55.300 --> 00:31:58.420]   So this embedding, that's the output of the self-attention
[00:31:58.420 --> 00:32:01.060]   for the word "the," is passed independently
[00:32:01.060 --> 00:32:03.540]   through a multilayer perceptron here.
[00:32:03.540 --> 00:32:09.540]   And you can think of it as combining together or processing
[00:32:09.540 --> 00:32:10.620]   the result of attention.
[00:32:11.600 --> 00:32:14.360]   So there's a number of reasons why we do this.
[00:32:14.360 --> 00:32:16.020]   One of them also is that you can actually
[00:32:16.020 --> 00:32:20.120]   stack a ton of computation into these feed-forward networks
[00:32:20.120 --> 00:32:22.600]   very, very efficiently, very parallelizable,
[00:32:22.600 --> 00:32:23.880]   very good for GPUs.
[00:32:23.880 --> 00:32:25.600]   But this is what's done in practice.
[00:32:25.600 --> 00:32:27.280]   So you do self-attention, and then you
[00:32:27.280 --> 00:32:31.320]   can pass it through this position-wise feed-forward
[00:32:31.320 --> 00:32:31.760]   layer.
[00:32:31.760 --> 00:32:34.000]   Every word is processed independently
[00:32:34.000 --> 00:32:40.360]   by this feed-forward network to process the result.
[00:32:40.360 --> 00:32:43.740]   So that's adding our classical deep learning nonlinearities
[00:32:43.740 --> 00:32:45.980]   for self-attention.
[00:32:45.980 --> 00:32:49.260]   And that's an easy fix for this no nonlinearities
[00:32:49.260 --> 00:32:50.920]   problem in self-attention.
[00:32:50.920 --> 00:32:52.860]   And then we have a last issue before we
[00:32:52.860 --> 00:32:56.300]   have our final minimal self-attention building block
[00:32:56.300 --> 00:32:59.740]   with which we can replace RNNs.
[00:32:59.740 --> 00:33:02.100]   And that's that-- well, when I've
[00:33:02.100 --> 00:33:04.940]   been writing out all of these examples of self-attention,
[00:33:04.940 --> 00:33:07.700]   you can look at the entire sequence.
[00:33:07.700 --> 00:33:12.400]   And in practice, for some tasks, such as machine translation
[00:33:12.400 --> 00:33:13.940]   or language modeling, whenever you
[00:33:13.940 --> 00:33:16.700]   want to define a probability distribution over a sequence,
[00:33:16.700 --> 00:33:18.820]   you can't cheat and look at the future.
[00:33:18.820 --> 00:33:24.860]   So at every time step, I could define the set
[00:33:24.860 --> 00:33:29.260]   of keys and queries and values to only include past words.
[00:33:29.260 --> 00:33:31.140]   But this is inefficient.
[00:33:31.140 --> 00:33:31.780]   Bear with me.
[00:33:31.780 --> 00:33:34.660]   It's inefficient because you can't parallelize it so well.
[00:33:34.660 --> 00:33:37.940]   So instead, we compute the entire n by n matrix,
[00:33:37.940 --> 00:33:41.220]   just like I showed in the slide discussing self-attention.
[00:33:41.220 --> 00:33:42.940]   And then I mask out words in the future.
[00:33:42.940 --> 00:33:45.500]   So for this score, eij--
[00:33:45.500 --> 00:33:49.900]   and I computed eij for all n by n pairs of words--
[00:33:49.900 --> 00:33:54.820]   is equal to whatever it was before if the word that you're
[00:33:54.820 --> 00:33:59.380]   looking at, index j, is an index that is less than or equal to
[00:33:59.380 --> 00:34:01.740]   where you are, index i.
[00:34:01.740 --> 00:34:04.820]   And it's equal to negative infinity-ish otherwise,
[00:34:04.820 --> 00:34:06.220]   if it's in the future.
[00:34:06.220 --> 00:34:08.640]   And when you softmax the eij, negative infinity
[00:34:08.640 --> 00:34:11.220]   gets mapped to 0.
[00:34:11.220 --> 00:34:13.980]   So now my attention is weighted 0.
[00:34:13.980 --> 00:34:16.500]   My weighted average is 0 on the future.
[00:34:16.500 --> 00:34:18.740]   So I can't look at it.
[00:34:18.740 --> 00:34:20.020]   What does this look like?
[00:34:20.020 --> 00:34:24.140]   So in order to encode these words, the chef who--
[00:34:24.140 --> 00:34:27.940]   maybe the start symbol there--
[00:34:27.940 --> 00:34:29.940]   I can look at these words.
[00:34:29.940 --> 00:34:31.620]   That's all pairs of words.
[00:34:31.620 --> 00:34:32.700]   And then I just gray out--
[00:34:32.700 --> 00:34:35.860]   I negative infinity out the words I can't look at.
[00:34:35.860 --> 00:34:37.300]   So when encoding the start symbol,
[00:34:37.300 --> 00:34:39.260]   I can just look at the start symbol.
[00:34:39.260 --> 00:34:43.220]   When encoding the, I can look at the start symbol and the.
[00:34:43.220 --> 00:34:46.220]   When encoding chef, I can look at start the chef.
[00:34:46.220 --> 00:34:48.960]   But I can't look at who.
[00:34:48.960 --> 00:34:53.780]   And so with this representation of chef that is only looking
[00:34:53.780 --> 00:34:57.940]   at start the chef, I can define a probability distribution
[00:34:57.940 --> 00:35:01.100]   using this vector that allows me to predict who
[00:35:01.100 --> 00:35:03.180]   without having cheated by already looking ahead
[00:35:03.180 --> 00:35:05.660]   and seeing that, well, who is the next word.
[00:35:05.660 --> 00:35:11.900]   Questions?
[00:35:11.900 --> 00:35:15.020]   So it says for using it in decoders.
[00:35:15.020 --> 00:35:17.020]   Do we do this for both the encoding layer
[00:35:17.020 --> 00:35:18.140]   and the decoding layer?
[00:35:18.140 --> 00:35:19.700]   Or for the encoding layer, are we
[00:35:19.700 --> 00:35:21.700]   allowing ourselves to look for--
[00:35:21.700 --> 00:35:23.580]   The question is, it says here that we're
[00:35:23.580 --> 00:35:24.540]   using this in a decoder.
[00:35:24.540 --> 00:35:26.700]   Do we also use it in the encoder?
[00:35:26.700 --> 00:35:31.060]   So this is the distinction between a bidirectional LSTM
[00:35:31.060 --> 00:35:33.140]   and a unidirectional LSTM.
[00:35:33.140 --> 00:35:37.100]   So wherever you don't need this constraint,
[00:35:37.100 --> 00:35:38.180]   you probably don't use it.
[00:35:38.180 --> 00:35:40.180]   So if you're using an encoder on the source
[00:35:40.180 --> 00:35:42.460]   sentence of your machine translation problem,
[00:35:42.460 --> 00:35:44.380]   you probably don't do this masking
[00:35:44.380 --> 00:35:46.220]   because it's probably good to let everything
[00:35:46.220 --> 00:35:47.140]   look at each other.
[00:35:47.140 --> 00:35:48.800]   And then whenever you do need to use it
[00:35:48.800 --> 00:35:51.620]   because you have this autoregressive probability
[00:35:51.620 --> 00:35:54.980]   of word one, probability of two given one, three given two
[00:35:54.980 --> 00:35:56.340]   and one, then you would use this.
[00:35:56.340 --> 00:35:58.740]   So traditionally, yes, in decoders, you will use it.
[00:35:58.740 --> 00:36:02.260]   In encoders, you will not.
[00:36:02.260 --> 00:36:04.380]   Yes.
[00:36:04.380 --> 00:36:07.500]   My question is a little bit philosophical.
[00:36:07.500 --> 00:36:10.780]   How humans actually generate sentences
[00:36:10.780 --> 00:36:14.980]   by having some notion of the probability of future words
[00:36:14.980 --> 00:36:19.020]   before they say the words that--
[00:36:19.020 --> 00:36:24.620]   or before they choose the words that they are currently
[00:36:24.620 --> 00:36:26.940]   speaking or writing, generating?
[00:36:26.940 --> 00:36:27.520]   Good question.
[00:36:27.520 --> 00:36:30.460]   So the question is, isn't looking ahead a little bit
[00:36:30.460 --> 00:36:32.900]   and predicting or getting an idea of the words
[00:36:32.900 --> 00:36:35.460]   that you might say in the future sort of how humans generate
[00:36:35.460 --> 00:36:38.660]   language instead of the strict constraint of not
[00:36:38.660 --> 00:36:39.940]   seeing it into the future?
[00:36:39.940 --> 00:36:40.740]   Is that what you're--
[00:36:40.740 --> 00:36:41.580]   OK.
[00:36:41.580 --> 00:36:43.180]   So right.
[00:36:43.180 --> 00:36:46.420]   Trying to plan ahead to see what I should do
[00:36:46.420 --> 00:36:48.820]   is definitely an interesting idea.
[00:36:48.820 --> 00:36:51.180]   But when I am training the network,
[00:36:51.180 --> 00:36:55.460]   I can't-- if I'm teaching it to try to predict the next word,
[00:36:55.460 --> 00:36:57.100]   and if I give it the answer, it's
[00:36:57.100 --> 00:36:59.820]   not going to learn anything useful.
[00:36:59.820 --> 00:37:01.680]   So in practice, when I'm generating text,
[00:37:01.680 --> 00:37:03.980]   maybe it would be a good idea to make some guesses
[00:37:03.980 --> 00:37:07.700]   far into the future or have a high-level plan or something.
[00:37:07.700 --> 00:37:11.180]   But in training the network, I can't encode that intuition
[00:37:11.180 --> 00:37:13.580]   about how humans build--
[00:37:13.580 --> 00:37:15.260]   like, generate sequences of language
[00:37:15.260 --> 00:37:17.020]   by just giving it the answer of the future
[00:37:17.020 --> 00:37:19.820]   directly, at least, because then it's just too easy.
[00:37:19.820 --> 00:37:21.900]   There's nothing to learn.
[00:37:21.900 --> 00:37:22.380]   Yeah.
[00:37:22.380 --> 00:37:24.460]   But there might be interesting ideas about maybe giving
[00:37:24.460 --> 00:37:26.540]   the network a hint as to what kind of thing
[00:37:26.540 --> 00:37:28.220]   could come next, for example.
[00:37:28.220 --> 00:37:29.660]   But that's out of scope for this.
[00:37:29.660 --> 00:37:31.180]   Yeah.
[00:37:31.180 --> 00:37:32.220]   Yeah, question over here.
[00:37:32.220 --> 00:37:35.820]   So I understand why we want to mask the future for stuff
[00:37:35.820 --> 00:37:37.460]   like language models, but how does it
[00:37:37.460 --> 00:37:39.260]   apply to machine translation?
[00:37:39.260 --> 00:37:40.500]   Like, why would we use it there?
[00:37:40.500 --> 00:37:41.000]   Yeah.
[00:37:41.000 --> 00:37:43.500]   So in machine translation--
[00:37:43.500 --> 00:37:46.020]   I'm going to come over to this board
[00:37:46.020 --> 00:37:49.380]   and hopefully get a better marker.
[00:37:49.380 --> 00:37:50.020]   Nice.
[00:37:50.020 --> 00:37:54.980]   In machine translation, I have a sentence like,
[00:37:54.980 --> 00:37:59.500]   "I like pizza."
[00:37:59.500 --> 00:38:04.820]   And I want to be able to translate it--
[00:38:04.820 --> 00:38:08.820]   "Je me pizza."
[00:38:08.820 --> 00:38:09.940]   Nice.
[00:38:09.940 --> 00:38:14.980]   And so when I'm looking at "I like pizza,"
[00:38:14.980 --> 00:38:16.380]   I get this as the input.
[00:38:16.380 --> 00:38:22.820]   And so I want self-attention without masking,
[00:38:22.820 --> 00:38:26.660]   because I want "I" to look at "like" and "I" to look at
[00:38:26.660 --> 00:38:29.100]   "pizza" and "like" to look at "pizza."
[00:38:29.100 --> 00:38:31.100]   And then when I'm generating this,
[00:38:31.100 --> 00:38:35.180]   if my tokens are like "Je m la pizza,"
[00:38:35.180 --> 00:38:37.740]   I want to, in encoding this word,
[00:38:37.740 --> 00:38:40.600]   I want to be able to look only at myself.
[00:38:40.600 --> 00:38:43.100]   And we'll talk about encoder-decoder architectures
[00:38:43.100 --> 00:38:46.120]   in this later in the lecture.
[00:38:46.120 --> 00:38:48.620]   But I want to be able to look at myself, none of the future,
[00:38:48.620 --> 00:38:50.020]   and all of this.
[00:38:50.020 --> 00:38:52.780]   And so what I'm talking about right now in this masking case
[00:38:52.780 --> 00:38:59.380]   is masking out with negative infinity all of these words.
[00:38:59.380 --> 00:39:02.540]   So that attention score from "Je" to everything else
[00:39:02.540 --> 00:39:05.820]   should be negative infinity.
[00:39:05.820 --> 00:39:06.300]   Yeah.
[00:39:06.300 --> 00:39:07.780]   Does that answer your question?
[00:39:07.780 --> 00:39:09.380]   Great.
[00:39:09.380 --> 00:39:11.380]   OK, let's move ahead.
[00:39:11.380 --> 00:39:16.500]   OK, so that was our last big building block
[00:39:16.500 --> 00:39:17.660]   issue with self-attention.
[00:39:17.660 --> 00:39:19.220]   So this is what I would call--
[00:39:19.220 --> 00:39:22.220]   and this is my personal opinion-- a minimal self-attention
[00:39:22.220 --> 00:39:22.900]   building block.
[00:39:22.900 --> 00:39:25.620]   You have self-attention, the basis of the method.
[00:39:25.620 --> 00:39:29.260]   So that's sort of here in the red.
[00:39:29.260 --> 00:39:31.980]   And maybe we had the inputs to the sequence here.
[00:39:31.980 --> 00:39:34.620]   And then you embed it with that embedding matrix E.
[00:39:34.620 --> 00:39:36.780]   And then you add position embeddings.
[00:39:36.780 --> 00:39:38.580]   And then these three arrows represent
[00:39:38.580 --> 00:39:42.980]   using the key, the value, and the query that's
[00:39:42.980 --> 00:39:44.140]   sort of stylized there.
[00:39:44.140 --> 00:39:47.300]   This is often how you see these diagrams.
[00:39:47.300 --> 00:39:50.420]   And so you pass it to self-attention
[00:39:50.420 --> 00:39:53.100]   with the position representation.
[00:39:53.100 --> 00:39:54.980]   So that specifies the sequence order,
[00:39:54.980 --> 00:39:57.460]   because otherwise you'd have no idea what order the words
[00:39:57.460 --> 00:39:59.260]   showed up in.
[00:39:59.260 --> 00:40:01.940]   You have the nonlinearities in sort of the TLFeedForward
[00:40:01.940 --> 00:40:05.820]   network there to sort of provide that sort of squashing
[00:40:05.820 --> 00:40:08.900]   and sort of deep learning expressivity.
[00:40:08.900 --> 00:40:10.700]   And then you have masking in order
[00:40:10.700 --> 00:40:13.500]   to have parallelizable operations that
[00:40:13.500 --> 00:40:15.460]   don't look at the future.
[00:40:15.460 --> 00:40:18.100]   So this is sort of our minimal architecture.
[00:40:18.100 --> 00:40:20.180]   And then up at the top above here,
[00:40:20.180 --> 00:40:22.380]   so you have this thing-- maybe you repeat this sort of
[00:40:22.380 --> 00:40:24.460]   self-attention and feedforward many times.
[00:40:24.460 --> 00:40:27.380]   So self-attention, feedforward, self-attention, feedforward,
[00:40:27.380 --> 00:40:28.940]   self-attention, feedforward.
[00:40:28.940 --> 00:40:31.140]   That's what I'm calling this block.
[00:40:31.140 --> 00:40:33.540]   And then maybe at the end of it, you predict something.
[00:40:33.540 --> 00:40:33.860]   I don't know.
[00:40:33.860 --> 00:40:35.340]   We haven't really talked about that.
[00:40:35.340 --> 00:40:36.940]   But you have these representations.
[00:40:36.940 --> 00:40:38.500]   And then you predict the next word,
[00:40:38.500 --> 00:40:40.740]   or you predict the sentiment, or you predict whatever.
[00:40:40.740 --> 00:40:44.640]   So this is like a self-attention architecture.
[00:40:44.640 --> 00:40:46.760]   OK, we're going to move on to the transformer next.
[00:40:46.760 --> 00:40:48.260]   So if there are any questions-- yeah?
[00:40:48.260 --> 00:40:52.180]   [INAUDIBLE]
[00:40:52.180 --> 00:40:53.380]   Other way around.
[00:40:53.380 --> 00:40:56.140]   We will use masking for decoders,
[00:40:56.140 --> 00:41:00.420]   where I want to decode out a sequence where I have
[00:41:00.420 --> 00:41:02.380]   an informational constraint, where
[00:41:02.380 --> 00:41:05.140]   to represent this word properly, I cannot have
[00:41:05.140 --> 00:41:06.460]   the information of the future.
[00:41:06.460 --> 00:41:08.580]   [INAUDIBLE]
[00:41:08.580 --> 00:41:09.080]   Yeah, OK.
[00:41:09.080 --> 00:41:16.220]   OK, great.
[00:41:16.220 --> 00:41:17.860]   So now let's talk about the transformer.
[00:41:17.860 --> 00:41:20.660]   So what I've pitched to you is what
[00:41:20.660 --> 00:41:24.740]   I call a minimal self-attention architecture.
[00:41:24.740 --> 00:41:28.980]   And I quite like pitching it that way.
[00:41:28.980 --> 00:41:30.820]   But really, no one uses the architecture
[00:41:30.820 --> 00:41:34.100]   that was just up on the slide, the previous slide.
[00:41:34.100 --> 00:41:36.060]   It doesn't work quite as well as it could.
[00:41:36.060 --> 00:41:38.660]   And there's a bunch of important details
[00:41:38.660 --> 00:41:41.700]   that we'll talk about now that goes into the transformer.
[00:41:41.700 --> 00:41:46.220]   What I would hope, though, to have you take away from that
[00:41:46.220 --> 00:41:48.020]   is that the transformer architecture,
[00:41:48.020 --> 00:41:51.420]   as I'll present it now, is not necessarily
[00:41:51.420 --> 00:41:54.580]   the end point of our search for better and better ways
[00:41:54.580 --> 00:41:57.940]   of representing language, even though it's now ubiquitous
[00:41:57.940 --> 00:42:00.060]   and has been for a couple of years.
[00:42:00.060 --> 00:42:01.500]   So think about these sort of ideas
[00:42:01.500 --> 00:42:05.020]   of the problems of using self-attention
[00:42:05.020 --> 00:42:08.940]   and maybe ways of fixing some of the issues with transformers.
[00:42:08.940 --> 00:42:13.500]   OK, so a transformer decoder is how we'll build systems
[00:42:13.500 --> 00:42:14.460]   like language models.
[00:42:14.460 --> 00:42:15.740]   And so we've discussed this.
[00:42:15.740 --> 00:42:18.940]   It's like our decoder with our self-attention-only sort
[00:42:18.940 --> 00:42:20.300]   of minimal architecture.
[00:42:20.300 --> 00:42:21.940]   It's got a couple of extra components,
[00:42:21.940 --> 00:42:23.400]   some of which I've grayed out here,
[00:42:23.400 --> 00:42:25.220]   that we'll go over one by one.
[00:42:25.220 --> 00:42:28.900]   The first that's actually different
[00:42:28.900 --> 00:42:31.660]   is that we'll replace our self-attention
[00:42:31.660 --> 00:42:35.820]   with masking with masked multi-head self-attention.
[00:42:35.820 --> 00:42:36.940]   This ends up being crucial.
[00:42:36.940 --> 00:42:39.820]   It's probably the most important distinction
[00:42:39.820 --> 00:42:42.360]   between the transformer and this sort of minimal architecture
[00:42:42.360 --> 00:42:43.820]   that I've presented.
[00:42:43.820 --> 00:42:46.740]   So let's come back to our toy example of attention,
[00:42:46.740 --> 00:42:49.060]   where we've been trying to represent the word learned
[00:42:49.060 --> 00:42:52.660]   in the context of the sequence, I went to Stanford CS224N
[00:42:52.660 --> 00:42:54.740]   and learned.
[00:42:54.740 --> 00:42:57.580]   And I was sort of giving these teal bars to say,
[00:42:57.580 --> 00:43:01.020]   oh, maybe intuitively you look at various things
[00:43:01.020 --> 00:43:04.340]   to build up your representation of learned.
[00:43:04.340 --> 00:43:06.540]   But really, there are varying ways
[00:43:06.540 --> 00:43:09.660]   in which I want to look back at the sequence
[00:43:09.660 --> 00:43:13.660]   to see varying sort of aspects of information
[00:43:13.660 --> 00:43:16.300]   that I want to incorporate into my representation.
[00:43:16.300 --> 00:43:19.340]   So maybe in this way, I sort of want
[00:43:19.340 --> 00:43:25.900]   to look at Stanford CS224N, because, oh, it's like entities.
[00:43:25.900 --> 00:43:28.300]   You learn different stuff at Stanford CS224N
[00:43:28.300 --> 00:43:31.940]   than you do at other courses or other universities or whatever.
[00:43:31.940 --> 00:43:35.180]   And so maybe I want to look here for this reason.
[00:43:35.180 --> 00:43:37.700]   And maybe in another sense, I actually
[00:43:37.700 --> 00:43:39.380]   want to look at the word learned.
[00:43:39.380 --> 00:43:43.380]   And I want to look at I. I went and learned.
[00:43:43.380 --> 00:43:46.300]   And I want to see maybe syntactically relevant words.
[00:43:46.300 --> 00:43:47.940]   It's very different reasons for which
[00:43:47.940 --> 00:43:49.640]   I might want to look at different things
[00:43:49.640 --> 00:43:50.900]   in the sequence.
[00:43:50.900 --> 00:43:52.740]   And so trying to average it all out
[00:43:52.740 --> 00:43:55.020]   with a single operation of self-attention
[00:43:55.020 --> 00:43:58.340]   ends up being maybe somewhat too difficult in a way
[00:43:58.340 --> 00:44:00.180]   that will make precise in assignment 5.
[00:44:00.180 --> 00:44:03.940]   Nice, we'll do a little bit more math.
[00:44:03.940 --> 00:44:08.140]   OK, so any questions about this intuition?
[00:44:11.580 --> 00:44:14.340]   [INAUDIBLE]
[00:44:14.340 --> 00:44:17.140]   Yeah, so it should be an application of attention
[00:44:17.140 --> 00:44:19.140]   just as I've presented it.
[00:44:19.140 --> 00:44:22.300]   So one independent define the keys, define the queries,
[00:44:22.300 --> 00:44:23.020]   define the values.
[00:44:23.020 --> 00:44:24.940]   I'll define it more precisely here.
[00:44:24.940 --> 00:44:27.460]   But think of it as I do attention once,
[00:44:27.460 --> 00:44:31.620]   and then I do it again with different parameters,
[00:44:31.620 --> 00:44:33.740]   being able to look at different things, et cetera.
[00:44:33.740 --> 00:44:36.220]   [INAUDIBLE]
[00:44:36.220 --> 00:44:38.660]   How do we ensure that they look at different things?
[00:44:38.660 --> 00:44:40.060]   We do not-- OK, so the question is,
[00:44:40.060 --> 00:44:41.980]   if we have two separate sets of weights trying to learn,
[00:44:41.980 --> 00:44:44.700]   say, to do this and to do that, how do we ensure that they
[00:44:44.700 --> 00:44:45.940]   learn different things?
[00:44:45.940 --> 00:44:49.100]   We do not ensure that they learn different things.
[00:44:49.100 --> 00:44:52.780]   And in practice, they do, although not perfectly.
[00:44:52.780 --> 00:44:55.660]   So it ends up being the case that you have some redundancy,
[00:44:55.660 --> 00:44:57.420]   and you can cut out some of these.
[00:44:57.420 --> 00:44:59.140]   But that's out of scope for this.
[00:44:59.140 --> 00:45:02.300]   But we hope, just like we hope that different dimensions
[00:45:02.300 --> 00:45:04.500]   in our feedforward layers will learn different things
[00:45:04.500 --> 00:45:06.740]   because of lack of symmetry and whatever,
[00:45:06.740 --> 00:45:09.380]   that we hope that the heads will start to specialize.
[00:45:09.380 --> 00:45:11.460]   And that will mean they'll specialize even more.
[00:45:11.460 --> 00:45:14.380]   And yeah.
[00:45:14.380 --> 00:45:16.340]   OK.
[00:45:16.340 --> 00:45:18.620]   All right, so in order to discuss multi-head self
[00:45:18.620 --> 00:45:22.100]   attention well, we really need to talk about the matrices,
[00:45:22.100 --> 00:45:25.220]   how we're going to implement this in GPUs efficiently.
[00:45:25.220 --> 00:45:27.780]   We're going to talk about the sequence-stacked form
[00:45:27.780 --> 00:45:29.260]   of attention.
[00:45:29.260 --> 00:45:31.660]   So we've been talking about each word sort of individually
[00:45:31.660 --> 00:45:35.140]   as a vector in dimensionality D. But really, we're
[00:45:35.140 --> 00:45:38.900]   going to be working on these as big matrices that are stacked.
[00:45:38.900 --> 00:45:42.340]   So I take all of my word embeddings, x1 to xn,
[00:45:42.340 --> 00:45:43.900]   and I stack them together.
[00:45:43.900 --> 00:45:49.860]   And now I have a big matrix that is in dimensionality Rn by D.
[00:45:49.860 --> 00:45:55.180]   OK, and now with my matrices K, Q, and V,
[00:45:55.180 --> 00:45:58.220]   I can just multiply them on this side of x.
[00:45:58.220 --> 00:46:04.300]   So x is Rn by D. K is Rd by D. So n by D times d by D
[00:46:04.300 --> 00:46:07.060]   gives you n by D again.
[00:46:07.060 --> 00:46:10.460]   So I can just compute a big matrix multiply
[00:46:10.460 --> 00:46:13.340]   on my whole sequence to multiply each one of the words
[00:46:13.340 --> 00:46:17.100]   of my key query and value matrices very efficiently.
[00:46:17.100 --> 00:46:18.860]   So this is sort of this vectorization idea.
[00:46:18.860 --> 00:46:20.780]   I don't want to for loop over the sequence.
[00:46:20.780 --> 00:46:23.540]   I represent the sequence as a big matrix,
[00:46:23.540 --> 00:46:27.580]   and I just do one big matrix multiply.
[00:46:27.580 --> 00:46:29.440]   Then the output is defined as this sort
[00:46:29.440 --> 00:46:31.660]   of inscrutable bit of math, which
[00:46:31.660 --> 00:46:35.260]   I'm going to go over visually.
[00:46:35.260 --> 00:46:37.820]   So first, we're going to take the key query dot
[00:46:37.820 --> 00:46:39.460]   products in one matrix.
[00:46:39.460 --> 00:46:47.100]   So we've got xq, which is Rn by D.
[00:46:47.100 --> 00:46:50.660]   And I've got xk transpose, which is Rd by n.
[00:46:50.660 --> 00:46:53.140]   So n by D, d by n.
[00:46:53.140 --> 00:46:55.400]   This is computing all of the eij's,
[00:46:55.400 --> 00:46:58.100]   these scores for self-attention.
[00:46:58.100 --> 00:47:00.620]   So this is all pairs of attention scores
[00:47:00.620 --> 00:47:03.060]   computed in one big matrix multiply.
[00:47:03.580 --> 00:47:04.580]   OK?
[00:47:04.580 --> 00:47:06.180]   So this is this big matrix here.
[00:47:06.180 --> 00:47:09.620]   Next, I use the softmax.
[00:47:09.620 --> 00:47:13.860]   So I softmax this over the second dimension,
[00:47:13.860 --> 00:47:15.980]   the second n dimension.
[00:47:15.980 --> 00:47:19.060]   And I get my sort of normalized scores,
[00:47:19.060 --> 00:47:20.700]   and then I multiply with xv.
[00:47:20.700 --> 00:47:26.180]   So this is an n by n matrix multiplied by an n by D matrix.
[00:47:26.180 --> 00:47:26.900]   And what do I get?
[00:47:26.900 --> 00:47:29.540]   Well, this is just doing the weighted average.
[00:47:29.540 --> 00:47:32.340]   So this is one big weighted average.
[00:47:32.340 --> 00:47:34.220]   Big weighted average contribution
[00:47:34.220 --> 00:47:37.380]   on the whole matrix, giving me my whole self-attention output
[00:47:37.380 --> 00:47:41.620]   in Rn by D. So I've just restated identically
[00:47:41.620 --> 00:47:43.980]   the self-attention operations, but computed
[00:47:43.980 --> 00:47:47.220]   in terms of matrices so that you could do this efficiently
[00:47:47.220 --> 00:47:47.860]   on a GPU.
[00:47:47.860 --> 00:47:52.060]   OK.
[00:47:52.060 --> 00:47:54.180]   So multi-headed attention.
[00:47:54.180 --> 00:47:55.500]   This is going to give us--
[00:47:55.500 --> 00:47:57.460]   and it's going to be important to compute this
[00:47:57.460 --> 00:47:59.580]   in terms of the matrices, which we'll see.
[00:47:59.580 --> 00:48:01.000]   This is going to give us the ability
[00:48:01.000 --> 00:48:04.220]   to look in multiple places at once for different reasons.
[00:48:04.220 --> 00:48:09.060]   So for self-attention looks where this dot product here
[00:48:09.060 --> 00:48:10.580]   is high.
[00:48:10.580 --> 00:48:15.380]   This xi, the Q matrix, the key matrix.
[00:48:15.380 --> 00:48:18.020]   But maybe we want to look in different places
[00:48:18.020 --> 00:48:19.300]   for different reasons.
[00:48:19.300 --> 00:48:24.540]   So we actually define multiple query, key, and value matrices.
[00:48:24.540 --> 00:48:26.740]   So I'm going to have a bunch of heads.
[00:48:26.740 --> 00:48:30.260]   I'm going to have h self-attention heads.
[00:48:30.260 --> 00:48:33.060]   And for each head, I'm going to define an independent query,
[00:48:33.060 --> 00:48:34.860]   key, and value matrix.
[00:48:34.860 --> 00:48:37.220]   And I'm going to say that its shape is
[00:48:37.220 --> 00:48:39.340]   going to map from the model dimensionality
[00:48:39.340 --> 00:48:41.300]   to the model dimensionality over h.
[00:48:41.300 --> 00:48:43.260]   So each one of these is doing projection down
[00:48:43.260 --> 00:48:45.340]   to a lower dimensional space.
[00:48:45.340 --> 00:48:47.700]   This is going to be for computational efficiency.
[00:48:47.700 --> 00:48:51.260]   And I'll just apply self-attention
[00:48:51.260 --> 00:48:53.300]   independently for each output.
[00:48:53.300 --> 00:48:56.100]   So this equation here is identical to the one
[00:48:56.100 --> 00:48:58.540]   we saw for single-headed self-attention,
[00:48:58.540 --> 00:49:02.860]   except I've got these sort of l indices everywhere.
[00:49:02.860 --> 00:49:04.540]   So I've got this lower dimensional thing.
[00:49:04.540 --> 00:49:06.540]   I'm mapping to a lower dimensional space.
[00:49:06.540 --> 00:49:09.100]   And then I do have my lower dimensional value vector
[00:49:09.100 --> 00:49:09.700]   there.
[00:49:09.700 --> 00:49:11.900]   So my output is an rd by h.
[00:49:11.900 --> 00:49:14.860]   But really, you're doing exactly the same kind of operation.
[00:49:14.860 --> 00:49:17.620]   I'm just doing it h different times.
[00:49:17.620 --> 00:49:19.700]   And then you combine the outputs.
[00:49:19.700 --> 00:49:22.140]   So I've done sort of look in different places
[00:49:22.140 --> 00:49:24.900]   with the different key, query, and value matrices.
[00:49:24.900 --> 00:49:28.340]   And then I get each of their outputs.
[00:49:28.340 --> 00:49:31.020]   And then I concatenate them together.
[00:49:31.020 --> 00:49:33.540]   So each one is dimensionality d by h.
[00:49:33.540 --> 00:49:36.140]   And I concatenate them together and then sort of mix them
[00:49:36.140 --> 00:49:39.820]   together with the final linear transformation.
[00:49:39.820 --> 00:49:43.040]   And so each head gets to look at different things
[00:49:43.040 --> 00:49:45.420]   and construct their value vectors differently.
[00:49:45.420 --> 00:49:49.500]   And then I sort of combine the result all together at once.
[00:49:49.500 --> 00:49:51.660]   Let's go through this visually, because it's
[00:49:51.660 --> 00:49:52.580]   at least helpful for me.
[00:49:55.820 --> 00:49:58.540]   It's actually not more costly to do this, really,
[00:49:58.540 --> 00:50:01.060]   than it is to compute a single head of self-attention.
[00:50:01.060 --> 00:50:02.520]   And we'll see through the pictures.
[00:50:02.520 --> 00:50:07.940]   So in single-headed self-attention,
[00:50:07.940 --> 00:50:09.460]   we computed xq.
[00:50:09.460 --> 00:50:11.100]   And in multi-headed self-attention,
[00:50:11.100 --> 00:50:13.860]   we'll also compute xq the same way.
[00:50:13.860 --> 00:50:16.260]   So xq is rn by d.
[00:50:16.260 --> 00:50:24.500]   And then we can reshape it into rn, that's sequence length,
[00:50:24.500 --> 00:50:29.420]   times the number of heads, times the model dimensionality
[00:50:29.420 --> 00:50:30.500]   over the number of heads.
[00:50:30.500 --> 00:50:32.580]   So I've just reshaped it to say, now I've
[00:50:32.580 --> 00:50:35.540]   got a big three-axis tensor.
[00:50:35.540 --> 00:50:37.820]   The first axis is the sequence length.
[00:50:37.820 --> 00:50:39.420]   The second one is the number of heads.
[00:50:39.420 --> 00:50:42.020]   The third is this reduced model dimensionality.
[00:50:42.020 --> 00:50:43.780]   And that costs nothing.
[00:50:43.780 --> 00:50:45.940]   And do the same thing for x and v.
[00:50:45.940 --> 00:50:48.460]   And then I transpose so that I've got the head
[00:50:48.460 --> 00:50:50.740]   axis as the first axis.
[00:50:50.740 --> 00:50:53.460]   And now I can compute all my other operations
[00:50:53.460 --> 00:50:58.020]   with the head axis, kind of like a batch.
[00:50:58.020 --> 00:51:01.780]   So what does this look like in practice?
[00:51:01.780 --> 00:51:05.100]   Instead of having one big xq matrix that's
[00:51:05.100 --> 00:51:08.180]   model dimensionality d, I've got, in this case,
[00:51:08.180 --> 00:51:12.780]   three xq matrices of model dimensionality d by 3, d by 3,
[00:51:12.780 --> 00:51:13.860]   d by 3.
[00:51:13.860 --> 00:51:16.500]   Same thing with the key matrix here.
[00:51:16.500 --> 00:51:18.540]   So everything looks almost identical.
[00:51:18.540 --> 00:51:21.000]   It's just the reshaping of the tensors.
[00:51:21.000 --> 00:51:23.340]   And now, at the output of this, I've
[00:51:23.340 --> 00:51:26.860]   got three sets of attention scores
[00:51:26.860 --> 00:51:29.060]   just by doing this reshape.
[00:51:29.060 --> 00:51:33.420]   And the cost is that, well, each of my attention heads
[00:51:33.420 --> 00:51:35.900]   has only a d by h vector to work with instead
[00:51:35.900 --> 00:51:38.020]   of a d-dimensional vector to work with.
[00:51:38.020 --> 00:51:38.860]   So I get the output.
[00:51:38.860 --> 00:51:43.340]   I get these three sets of pairs of scores.
[00:51:43.340 --> 00:51:45.660]   I compute the softmax independently
[00:51:45.660 --> 00:51:47.100]   for each of the three.
[00:51:47.100 --> 00:51:50.680]   And then I have three value matrices there as well,
[00:51:50.680 --> 00:51:52.660]   each of them lower dimensional.
[00:51:52.660 --> 00:51:56.700]   And then finally, I get my three different output vectors.
[00:51:56.700 --> 00:51:58.460]   And I have a final linear transformation
[00:51:58.460 --> 00:52:01.020]   to mush them together.
[00:52:01.020 --> 00:52:02.660]   And I get an output.
[00:52:02.660 --> 00:52:04.620]   And in summary, what this allows you to do
[00:52:04.620 --> 00:52:08.220]   is exactly what I gave in the toy example, which
[00:52:08.220 --> 00:52:10.240]   was I can have each of these heads
[00:52:10.240 --> 00:52:12.820]   look at different parts of a sequence for different reasons.
[00:52:12.820 --> 00:52:21.020]   So this is at a given block, right?
[00:52:21.020 --> 00:52:23.780]   All of these attention heads are for a given transformer block.
[00:52:23.780 --> 00:52:26.980]   A next block could also have three attention heads.
[00:52:26.980 --> 00:52:30.340]   The question is, are all of these for a given block?
[00:52:30.340 --> 00:52:31.800]   And we'll talk about a block again.
[00:52:31.800 --> 00:52:35.060]   But this block was this sort of pair of self-attention
[00:52:35.060 --> 00:52:36.300]   and feed-forward network.
[00:52:36.300 --> 00:52:37.920]   So you do self-attention, feed-forward.
[00:52:37.920 --> 00:52:38.780]   That's one block.
[00:52:38.780 --> 00:52:40.120]   Another block is another self-attention,
[00:52:40.120 --> 00:52:41.340]   another feed-forward.
[00:52:41.340 --> 00:52:43.260]   And the question is, are the parameters shared
[00:52:43.260 --> 00:52:44.900]   between the blocks or not?
[00:52:44.900 --> 00:52:46.180]   Generally, they are not shared.
[00:52:46.180 --> 00:52:48.660]   You'll have independent parameters at every block,
[00:52:48.660 --> 00:52:52.700]   although there are some exceptions.
[00:52:52.700 --> 00:52:55.380]   Voting on that, is it typically the case
[00:52:55.380 --> 00:52:58.820]   that you have the same number of heads at each block?
[00:52:58.820 --> 00:53:01.380]   Or do you vary the number of heads across blocks?
[00:53:01.380 --> 00:53:04.020]   You have this-- you definitely could vary it.
[00:53:04.020 --> 00:53:05.540]   People haven't found reason to vary--
[00:53:05.540 --> 00:53:07.960]   so the question is, do you have different numbers of heads
[00:53:07.960 --> 00:53:09.340]   across the different blocks?
[00:53:09.340 --> 00:53:12.780]   Or do you have the same number of heads across all blocks?
[00:53:12.780 --> 00:53:14.540]   The simplest thing is to just have
[00:53:14.540 --> 00:53:16.940]   it be the same everywhere, which is what people have done.
[00:53:16.940 --> 00:53:19.100]   I haven't yet found a good reason to vary it,
[00:53:19.100 --> 00:53:21.860]   but it could be interesting.
[00:53:21.860 --> 00:53:25.540]   It's definitely the case that after training these networks,
[00:53:25.540 --> 00:53:27.900]   you can actually just totally zero out,
[00:53:27.900 --> 00:53:30.900]   remove some of the attention heads.
[00:53:30.900 --> 00:53:35.700]   And I'd be curious to know if you could remove more or less,
[00:53:35.700 --> 00:53:39.380]   depending on the layer index, which might then say,
[00:53:39.380 --> 00:53:40.580]   oh, we should just have fewer.
[00:53:40.580 --> 00:53:42.420]   But again, it's not actually more expensive
[00:53:42.420 --> 00:53:43.700]   to have a bunch.
[00:53:43.700 --> 00:53:46.740]   So people tend to instead set the number of heads
[00:53:46.740 --> 00:53:51.180]   to be roughly so that you have a reasonable number of dimensions
[00:53:51.180 --> 00:53:55.460]   per head, given the total model dimensionality d that you want.
[00:53:55.460 --> 00:54:00.260]   So for example, I might want at least 64 dimensions per head,
[00:54:00.260 --> 00:54:04.340]   which if d is 128, that tells me how many heads
[00:54:04.340 --> 00:54:06.020]   I'm going to have, roughly.
[00:54:06.020 --> 00:54:07.860]   So people tend to scale the number of heads
[00:54:07.860 --> 00:54:09.620]   up with the model dimensionality.
[00:54:09.620 --> 00:54:15.820]   Yeah, with that xq, by slicing it into different columns,
[00:54:15.820 --> 00:54:19.020]   you're reducing the rank of the final matrix, right?
[00:54:19.020 --> 00:54:19.820]   Yeah.
[00:54:19.820 --> 00:54:23.180]   But that doesn't really have any effect on the results.
[00:54:23.180 --> 00:54:29.300]   So the question is, by having these reduced xq and xk
[00:54:29.300 --> 00:54:32.940]   matrices, this is a very low rank approximation.
[00:54:32.940 --> 00:54:35.340]   This little sliver and this little sliver
[00:54:35.340 --> 00:54:38.020]   defining this whole big matrix, it's very low rank.
[00:54:38.020 --> 00:54:39.780]   Is that not bad?
[00:54:39.780 --> 00:54:40.940]   In practice, no.
[00:54:40.940 --> 00:54:42.700]   I mean, again, it's the reason why
[00:54:42.700 --> 00:54:45.820]   we limit the number of heads depending on the model
[00:54:45.820 --> 00:54:49.620]   dimensionality, because you want intuitively at least
[00:54:49.620 --> 00:54:51.220]   some number of dimensions.
[00:54:51.220 --> 00:54:55.820]   So 64 is sometimes done, 128, something like that.
[00:54:55.820 --> 00:54:58.300]   But if you're not giving each head too much to do,
[00:54:58.300 --> 00:55:01.020]   and it's got sort of a simple job, you've got a lot of heads,
[00:55:01.020 --> 00:55:04.260]   it ends up sort of being OK.
[00:55:04.260 --> 00:55:05.980]   All we really know is that empirically,
[00:55:05.980 --> 00:55:09.300]   it's way better to have more heads than one.
[00:55:12.420 --> 00:55:14.140]   Yes.
[00:55:14.140 --> 00:55:16.300]   I'm wondering, have there been studies
[00:55:16.300 --> 00:55:22.140]   to see if information in one of the sets of the attention
[00:55:22.140 --> 00:55:25.100]   scores, like information that one of them
[00:55:25.100 --> 00:55:29.540]   learns is consistent and related to each other,
[00:55:29.540 --> 00:55:32.380]   or how are they related?
[00:55:32.380 --> 00:55:34.580]   So the question is, have there been studies to see
[00:55:34.580 --> 00:55:37.140]   if there's consistent information encoded
[00:55:37.140 --> 00:55:38.780]   by the attention heads?
[00:55:38.780 --> 00:55:40.740]   And yes.
[00:55:40.740 --> 00:55:42.580]   Actually, there's been quite a lot of study
[00:55:42.580 --> 00:55:44.980]   and interpretability and analysis of these models
[00:55:44.980 --> 00:55:48.420]   to try to figure out what roles, what sort of mechanistic roles
[00:55:48.420 --> 00:55:50.180]   each of these heads takes on.
[00:55:50.180 --> 00:55:52.740]   And there's quite a bit of exciting results
[00:55:52.740 --> 00:55:55.140]   there around some attention heads
[00:55:55.140 --> 00:55:59.820]   learning to pick out the syntactic dependencies,
[00:55:59.820 --> 00:56:03.780]   or maybe doing a global averaging of context.
[00:56:03.780 --> 00:56:05.420]   The question is quite nuanced, though,
[00:56:05.420 --> 00:56:07.780]   because in a deep network, it's unclear--
[00:56:07.780 --> 00:56:09.620]   and we should talk about this more offline--
[00:56:09.620 --> 00:56:12.580]   it's unclear if you look at a word 10 layers deep
[00:56:12.580 --> 00:56:14.900]   in a network what you're really looking at,
[00:56:14.900 --> 00:56:17.060]   because it's already incorporated context
[00:56:17.060 --> 00:56:20.140]   from everyone else, and it's a little bit unclear.
[00:56:20.140 --> 00:56:21.300]   Active area of research.
[00:56:21.300 --> 00:56:25.100]   But I think I should move on now to keep
[00:56:25.100 --> 00:56:26.780]   discussing transformers.
[00:56:26.780 --> 00:56:30.700]   But yeah, if you want to talk more about it, I'm happy to.
[00:56:30.700 --> 00:56:31.420]   OK.
[00:56:31.420 --> 00:56:34.580]   So another sort of hack that I'm going to toss in here--
[00:56:34.580 --> 00:56:36.300]   I mean, maybe they wouldn't call it hack,
[00:56:36.300 --> 00:56:39.620]   but it's a nice little method to improve things.
[00:56:39.620 --> 00:56:42.060]   It's called scaled dot product attention.
[00:56:42.060 --> 00:56:45.500]   So one of the issues with this sort of key query value
[00:56:45.500 --> 00:56:47.940]   self-attention is that when the model dimensionality becomes
[00:56:47.940 --> 00:56:51.700]   large, the dot products between vectors, even random vectors,
[00:56:51.700 --> 00:56:55.060]   tend to become large.
[00:56:55.060 --> 00:56:58.060]   And when that happens, the inputs to the softmax function
[00:56:58.060 --> 00:57:01.380]   can be very large, making the gradient small.
[00:57:01.380 --> 00:57:03.300]   So intuitively, if you have two random vectors
[00:57:03.300 --> 00:57:06.540]   and model dimensionality d, and you just dot product them
[00:57:06.540 --> 00:57:09.140]   together, as d grows, their dot product
[00:57:09.140 --> 00:57:11.540]   grows in expectation to be very large.
[00:57:11.540 --> 00:57:13.940]   And so you sort of want to start out
[00:57:13.940 --> 00:57:16.900]   with everyone's attention being very uniform, very flat,
[00:57:16.900 --> 00:57:18.780]   sort of look everywhere.
[00:57:18.780 --> 00:57:20.620]   But if some dot products are very large,
[00:57:20.620 --> 00:57:23.660]   then learning will be inhibited.
[00:57:23.660 --> 00:57:26.300]   And so what you end up doing is you just sort of--
[00:57:26.300 --> 00:57:29.700]   for each of your heads, you just sort of divide all the scores
[00:57:29.700 --> 00:57:31.380]   by this constant that's determined
[00:57:31.380 --> 00:57:33.060]   by the model dimensionality.
[00:57:33.060 --> 00:57:35.660]   So as the vectors grow very large,
[00:57:35.660 --> 00:57:40.500]   their dot products don't, at least at initialization time.
[00:57:40.500 --> 00:57:45.020]   So this is sort of like a nice little important,
[00:57:45.020 --> 00:57:46.020]   but maybe not--
[00:57:46.020 --> 00:57:52.260]   yeah, it's important to know.
[00:57:52.260 --> 00:57:55.500]   And so that's called scaled dot product attention.
[00:57:55.500 --> 00:57:58.340]   From here on out, we'll just assume that we do this.
[00:57:58.340 --> 00:57:59.540]   It's quite easy to implement.
[00:57:59.540 --> 00:58:05.060]   You just do a little division in all of your computations.
[00:58:05.060 --> 00:58:07.260]   OK, so now in the transformer decoder,
[00:58:07.260 --> 00:58:08.780]   we've got a couple of other things
[00:58:08.780 --> 00:58:12.660]   that I have unfaded out here.
[00:58:12.660 --> 00:58:15.220]   We have two big optimization tricks, or optimization
[00:58:15.220 --> 00:58:17.060]   methods, I should say, really, because these
[00:58:17.060 --> 00:58:20.100]   are quite important, that end up being very important.
[00:58:20.100 --> 00:58:22.940]   We've got residual connections and layer normalization.
[00:58:22.940 --> 00:58:26.980]   And in transformer diagrams that you see sort of around the web,
[00:58:26.980 --> 00:58:32.380]   they're often written together as this add and norm box.
[00:58:32.380 --> 00:58:34.540]   And in practice, in the transformer decoder,
[00:58:34.540 --> 00:58:38.620]   I'm going to apply mask multi-head attention
[00:58:38.620 --> 00:58:41.340]   and then do this sort of optimization add a norm.
[00:58:41.340 --> 00:58:43.340]   Then I'll do a feed forward application
[00:58:43.340 --> 00:58:44.660]   and then add a norm.
[00:58:44.660 --> 00:58:47.660]   So this is quite important.
[00:58:47.660 --> 00:58:51.820]   So let's go over these two individual components.
[00:58:51.820 --> 00:58:53.300]   The first is residual connections.
[00:58:53.300 --> 00:58:55.460]   I mean, I think we've talked about residual connections
[00:58:55.460 --> 00:58:56.100]   before, right?
[00:58:56.100 --> 00:58:58.140]   So it's worth doing it again.
[00:58:58.140 --> 00:59:01.820]   But it's really a good trick to help models train better.
[00:59:01.820 --> 00:59:04.660]   So just to recap, we're going to take--
[00:59:04.660 --> 00:59:08.220]   instead of having this sort of-- you have a layer, layer i
[00:59:08.220 --> 00:59:10.540]   minus 1, and you pass it through a thing.
[00:59:10.540 --> 00:59:11.620]   Maybe it's self-attention.
[00:59:11.620 --> 00:59:13.060]   Maybe it's a feed forward network.
[00:59:13.060 --> 00:59:16.380]   Now you've got layer i.
[00:59:16.380 --> 00:59:23.060]   I'm going to add the result of layer i to its input here.
[00:59:23.060 --> 00:59:25.320]   So now I'm saying I'm just going to compute the layer,
[00:59:25.320 --> 00:59:27.740]   and I'm going to add in the input to the layer
[00:59:27.740 --> 00:59:30.780]   so that I only have to learn the residual
[00:59:30.780 --> 00:59:32.020]   from the previous layer.
[00:59:32.020 --> 00:59:33.720]   So I've got this sort of connection here.
[00:59:33.720 --> 00:59:34.860]   It's often written as this.
[00:59:34.860 --> 00:59:38.940]   It's sort of like, boop, connection.
[00:59:38.940 --> 00:59:39.860]   It goes around.
[00:59:39.860 --> 00:59:41.740]   And you should think that the gradient is just
[00:59:41.740 --> 00:59:43.820]   really great through the residual connection.
[00:59:43.820 --> 00:59:47.700]   Like, ah, if I've got vanishing or exploding gradient--
[00:59:47.700 --> 00:59:49.500]   vanishing gradients through this layer,
[00:59:49.500 --> 00:59:51.740]   well, I can at least learn everything behind it
[00:59:51.740 --> 00:59:54.160]   because I've got this residual connection where
[00:59:54.160 --> 00:59:58.060]   the gradient is 1 because it's the identity.
[00:59:58.060 --> 00:59:59.180]   This is really nice.
[00:59:59.180 --> 01:00:01.460]   And it also maybe is like a--
[01:00:01.460 --> 01:00:03.980]   at least at initialization, everything
[01:00:03.980 --> 01:00:06.660]   looks a little bit like the identity function now, right?
[01:00:06.660 --> 01:00:08.920]   Because if the contribution of the layer
[01:00:08.920 --> 01:00:11.680]   is somewhat small because all of your weights are small,
[01:00:11.680 --> 01:00:13.980]   and I have the addition from the input,
[01:00:13.980 --> 01:00:15.620]   maybe the whole thing looks a little bit
[01:00:15.620 --> 01:00:18.380]   like the identity, which might be a good sort of place
[01:00:18.380 --> 01:00:20.340]   to start.
[01:00:20.340 --> 01:00:22.100]   And there are really nice visualizations.
[01:00:22.100 --> 01:00:24.800]   I just love this visualization.
[01:00:24.800 --> 01:00:26.420]   So this is your lost landscape.
[01:00:26.420 --> 01:00:28.000]   So you're gradient descent, and you're
[01:00:28.000 --> 01:00:30.860]   trying to traverse the mountains of the lost landscape.
[01:00:30.860 --> 01:00:32.580]   This is like the parameter space.
[01:00:32.580 --> 01:00:34.620]   And down is better in your loss function.
[01:00:34.620 --> 01:00:35.500]   And it's really hard.
[01:00:35.500 --> 01:00:38.060]   So you get stuck in some local optima,
[01:00:38.060 --> 01:00:41.180]   and you can't sort of find your way to get out.
[01:00:41.180 --> 01:00:43.140]   And then this is with residual connections.
[01:00:43.140 --> 01:00:44.300]   I mean, come on.
[01:00:44.300 --> 01:00:47.060]   You just sort of walk down.
[01:00:47.060 --> 01:00:48.860]   I mean, that's not actually, I guess,
[01:00:48.860 --> 01:00:50.380]   really how it works all the time.
[01:00:50.380 --> 01:00:52.140]   But I really love this.
[01:00:52.140 --> 01:00:52.640]   It's great.
[01:00:52.640 --> 01:00:58.540]   OK.
[01:00:58.540 --> 01:01:00.260]   So yeah, we've seen residual connections.
[01:01:00.260 --> 01:01:02.860]   We should move on to layer normalization.
[01:01:02.860 --> 01:01:06.500]   So layer norm is another thing to help your model train
[01:01:06.500 --> 01:01:08.380]   faster.
[01:01:08.380 --> 01:01:14.760]   And the intuitions around layer normalization
[01:01:14.760 --> 01:01:17.040]   and sort of the empiricism of it working very well
[01:01:17.040 --> 01:01:21.020]   maybe aren't perfectly, let's say, connected.
[01:01:21.020 --> 01:01:25.700]   But you should imagine, I suppose,
[01:01:25.700 --> 01:01:29.860]   that we want to say this variation within each layer.
[01:01:29.860 --> 01:01:31.180]   Things can get very big.
[01:01:31.180 --> 01:01:33.140]   Things can get very small.
[01:01:33.140 --> 01:01:36.700]   That's not actually informative because of variations
[01:01:36.700 --> 01:01:39.940]   between maybe the gradients.
[01:01:39.940 --> 01:01:43.860]   Or I've got sort of weird things going on in my layers
[01:01:43.860 --> 01:01:45.140]   that I can't totally control.
[01:01:45.140 --> 01:01:47.740]   I haven't been able to sort of make everything behave sort
[01:01:47.740 --> 01:01:50.460]   of nicely where everything stays roughly the same norm.
[01:01:50.460 --> 01:01:51.660]   Maybe some things explode.
[01:01:51.660 --> 01:01:54.660]   Maybe some things shrink.
[01:01:54.660 --> 01:01:59.580]   And I want to cut down on sort of uninformative variation
[01:01:59.580 --> 01:02:00.940]   between layers.
[01:02:00.940 --> 01:02:03.740]   So I'm going to let x and rd be an individual word
[01:02:03.740 --> 01:02:05.380]   vector in the model.
[01:02:05.380 --> 01:02:09.100]   So this is like I have a single index, one vector.
[01:02:09.100 --> 01:02:12.660]   And what I'm going to try to do is just normalize it.
[01:02:12.660 --> 01:02:15.540]   Normalize it in the sense of it's got a bunch of variation.
[01:02:15.540 --> 01:02:17.700]   And I'm going to cut out on everything.
[01:02:17.700 --> 01:02:20.340]   I'm going to normalize it to unit mean and standard
[01:02:20.340 --> 01:02:21.020]   deviation.
[01:02:21.020 --> 01:02:26.700]   So I'm going to estimate the mean here across--
[01:02:26.700 --> 01:02:30.180]   so for all of the dimensions in the vector,
[01:02:30.180 --> 01:02:32.660]   so j equals 1 to the model dimensionality,
[01:02:32.660 --> 01:02:33.900]   I'm going to sum up the value.
[01:02:33.900 --> 01:02:35.860]   So I've got this one big word vector.
[01:02:35.860 --> 01:02:37.540]   And I sum up all the values.
[01:02:37.540 --> 01:02:40.220]   Division by d here, that's the mean.
[01:02:40.220 --> 01:02:43.540]   I'm going to have my estimate of the standard deviation.
[01:02:43.540 --> 01:02:45.020]   Again, these should say estimates.
[01:02:45.020 --> 01:02:47.300]   This is my simple estimate of the standard deviation
[01:02:47.300 --> 01:02:50.500]   or the values within this one vector.
[01:02:50.500 --> 01:02:53.700]   And I'm just going to--
[01:02:53.700 --> 01:02:58.060]   and then possibly, I guess I can have learned parameters
[01:02:58.060 --> 01:03:02.780]   to try to scale back out in terms of multiplicatively
[01:03:02.780 --> 01:03:04.500]   and additively here.
[01:03:04.500 --> 01:03:05.540]   That's optional.
[01:03:05.540 --> 01:03:08.380]   We're going to compute this standardization.
[01:03:08.380 --> 01:03:11.340]   I'm going to take my vector x, subtract out the mean,
[01:03:11.340 --> 01:03:12.780]   divide by the standard deviation,
[01:03:12.780 --> 01:03:14.820]   plus this epsilon constant.
[01:03:14.820 --> 01:03:16.300]   If there's not a lot of variation,
[01:03:16.300 --> 01:03:17.900]   I don't want things to explode.
[01:03:17.900 --> 01:03:21.700]   So I'm going to have this epsilon there that's close to 0.
[01:03:21.700 --> 01:03:25.500]   So this part here, x minus mu over square root sigma
[01:03:25.500 --> 01:03:28.540]   plus epsilon, is saying take all the variation
[01:03:28.540 --> 01:03:32.600]   and normalize it to unit mean and standard deviation.
[01:03:32.600 --> 01:03:37.080]   And then maybe I want to scale it, stretch it back out,
[01:03:37.080 --> 01:03:40.860]   and then maybe add an offset beta that I've learned.
[01:03:40.860 --> 01:03:43.300]   Although in practice, actually, this part-- and discuss this
[01:03:43.300 --> 01:03:44.580]   in the lecture notes--
[01:03:44.580 --> 01:03:47.940]   in practice, this part maybe isn't actually that important.
[01:03:47.940 --> 01:03:51.220]   But so layer normalization, yeah, you're sort of--
[01:03:51.220 --> 01:03:54.000]   you can think of this as when I get the output of layer
[01:03:54.000 --> 01:03:55.940]   normalization, it's going to be--
[01:03:55.940 --> 01:03:58.940]   sort of look nice and look similar to the next layer
[01:03:58.940 --> 01:04:00.940]   independent of what's gone on because it's
[01:04:00.940 --> 01:04:02.780]   going to be unit mean and standard deviation.
[01:04:02.780 --> 01:04:04.660]   So maybe that makes for a better thing
[01:04:04.660 --> 01:04:06.260]   to learn off of for the next layer.
[01:04:06.260 --> 01:04:12.340]   OK, any questions for residual or layer norm?
[01:04:12.340 --> 01:04:13.220]   Yes.
[01:04:13.220 --> 01:04:17.340]   What would it mean to subtract the scalar mu from the vector x?
[01:04:17.340 --> 01:04:18.780]   Yeah, it's a good question.
[01:04:18.780 --> 01:04:21.580]   When I subtract the scalar mu from the vector x,
[01:04:21.580 --> 01:04:27.980]   I broadcast mu to dimensionality d and remove mu from all d.
[01:04:27.980 --> 01:04:29.300]   Yeah, good point.
[01:04:29.300 --> 01:04:29.900]   Thank you.
[01:04:29.900 --> 01:04:30.580]   That was unclear.
[01:04:30.580 --> 01:04:31.080]   Yeah.
[01:04:31.080 --> 01:04:37.420]   In the fourth bullet, maybe I'm confused.
[01:04:37.420 --> 01:04:38.300]   Is it divided?
[01:04:38.300 --> 01:04:42.500]   Should it be divided by d or from mean?
[01:04:42.500 --> 01:04:43.620]   Sorry, can you repeat that?
[01:04:43.620 --> 01:04:47.220]   In the fourth bullet point when you're calculating the mean,
[01:04:47.220 --> 01:04:49.660]   is it divided by d or is it--
[01:04:49.660 --> 01:04:51.180]   or maybe I'm just confused.
[01:04:51.180 --> 01:04:52.340]   I think it is divided by d.
[01:04:52.340 --> 01:04:52.820]   Yeah.
[01:04:52.820 --> 01:04:55.180]   Oh.
[01:04:55.180 --> 01:04:57.300]   These are-- so this is the average deviation
[01:04:57.300 --> 01:05:00.260]   from the mean of all of the-- yeah.
[01:05:00.260 --> 01:05:00.760]   Yes.
[01:05:00.760 --> 01:05:04.700]   So if you have five words in a sentence by their norm,
[01:05:04.700 --> 01:05:09.460]   do you normalize based on the statistics of these five words
[01:05:09.460 --> 01:05:11.700]   or do you want one word by one?
[01:05:11.700 --> 01:05:14.500]   So the question is, if I have five words in the sequence,
[01:05:14.500 --> 01:05:18.820]   do I normalize by aggregating the statistics to estimate mu
[01:05:18.820 --> 01:05:21.060]   and sigma across all the five words,
[01:05:21.060 --> 01:05:24.140]   share their statistics, or do it independently for each word?
[01:05:24.140 --> 01:05:25.700]   This is a great question, which I
[01:05:25.700 --> 01:05:28.260]   think in all the papers that discuss transformers
[01:05:28.260 --> 01:05:30.140]   is under specified.
[01:05:30.140 --> 01:05:33.060]   You do not share across the five words, which
[01:05:33.060 --> 01:05:35.500]   is somewhat confusing to me.
[01:05:35.500 --> 01:05:39.180]   So each of the five words is done completely independently.
[01:05:39.180 --> 01:05:41.380]   You could have shared across the five words
[01:05:41.380 --> 01:05:43.300]   and said that my estimate of the statistics
[01:05:43.300 --> 01:05:49.740]   are just based on all five, but you do not.
[01:05:49.740 --> 01:05:51.380]   I can't pretend I understand totally why.
[01:05:51.380 --> 01:05:51.880]   [INAUDIBLE]
[01:05:51.880 --> 01:05:54.360]   [INAUDIBLE]
[01:05:54.360 --> 01:06:01.400]   For example, per batch or per output of the same position?
[01:06:01.400 --> 01:06:02.840]   So similar question.
[01:06:02.840 --> 01:06:06.760]   The question is, if you have a batch of sequences,
[01:06:06.760 --> 01:06:10.040]   so just like we were doing batch-based training,
[01:06:10.040 --> 01:06:11.840]   do you for a single word--
[01:06:11.840 --> 01:06:13.760]   now, we don't share across the sequence index
[01:06:13.760 --> 01:06:16.680]   for sharing the statistics, but do you share across the batch?
[01:06:16.680 --> 01:06:17.680]   And the answer is no.
[01:06:17.680 --> 01:06:19.480]   You also do not share across the batch.
[01:06:19.480 --> 01:06:22.160]   In fact, layer normalization was sort of
[01:06:22.160 --> 01:06:25.360]   invented as a replacement for batch normalization, which
[01:06:25.360 --> 01:06:26.320]   did just that.
[01:06:26.320 --> 01:06:27.900]   And the issue with batch normalization
[01:06:27.900 --> 01:06:30.960]   is that now your forward pass sort of depends in a way
[01:06:30.960 --> 01:06:34.080]   that you don't like on examples that should be not
[01:06:34.080 --> 01:06:35.400]   related to your example.
[01:06:35.400 --> 01:06:37.800]   And so, yeah, you don't share statistics across the batch.
[01:06:37.800 --> 01:06:41.640]   OK.
[01:06:41.640 --> 01:06:44.240]   Cool.
[01:06:44.240 --> 01:06:48.600]   OK, so now we have our full transformer decoder,
[01:06:48.600 --> 01:06:50.520]   and we have our blocks.
[01:06:50.520 --> 01:06:52.960]   So in this sort of slightly grayed out thing here
[01:06:52.960 --> 01:06:58.640]   that says repeat for a number of decoder blocks,
[01:06:58.640 --> 01:07:00.520]   each block consists of--
[01:07:00.520 --> 01:07:02.400]   I pass it through self-attention,
[01:07:02.400 --> 01:07:04.720]   and then my add and norm.
[01:07:04.720 --> 01:07:06.480]   So I've got this residual connection here
[01:07:06.480 --> 01:07:08.400]   that goes around, add.
[01:07:08.400 --> 01:07:10.780]   I've got the layer normalization there, and then
[01:07:10.780 --> 01:07:15.240]   a feed-forward layer, and then another add and norm.
[01:07:15.240 --> 01:07:18.040]   And so that sort of set of four operations,
[01:07:18.040 --> 01:07:21.720]   I apply for some number of times, number of blocks.
[01:07:21.720 --> 01:07:24.000]   So that whole thing is called a single block.
[01:07:24.000 --> 01:07:24.960]   And that's it.
[01:07:24.960 --> 01:07:29.040]   That's the transformer decoder as it is.
[01:07:29.040 --> 01:07:34.040]   Cool, so that's a whole architecture right there.
[01:07:34.040 --> 01:07:36.800]   We've solved things like needing to represent position.
[01:07:36.800 --> 01:07:39.960]   We've solved things like not being
[01:07:39.960 --> 01:07:41.760]   able to look into the future.
[01:07:41.760 --> 01:07:44.120]   We've solved a lot of different optimization problems.
[01:07:44.120 --> 01:07:44.680]   You had a question?
[01:07:44.680 --> 01:07:45.180]   Yes.
[01:07:45.180 --> 01:07:48.080]   [INAUDIBLE]
[01:07:48.080 --> 01:07:49.680]   Yes.
[01:07:49.680 --> 01:07:52.800]   Yes, masked multi-head attention, yeah.
[01:07:52.800 --> 01:07:55.880]   With the dot product scaling with the square root
[01:07:55.880 --> 01:07:57.120]   d over h as well, yeah.
[01:07:57.120 --> 01:08:05.880]   So the question is, how do these models
[01:08:05.880 --> 01:08:08.560]   handle variable length inputs?
[01:08:08.560 --> 01:08:13.680]   Yeah, so if you have--
[01:08:13.680 --> 01:08:18.180]   so the input to the GPU forward pass
[01:08:18.180 --> 01:08:20.820]   is going to be a constant length.
[01:08:20.820 --> 01:08:24.740]   So you're going to maybe pad to a constant length.
[01:08:24.740 --> 01:08:28.580]   And in order to not look at the future, the stuff that's
[01:08:28.580 --> 01:08:32.720]   happening in the future, you can mask out the pad tokens,
[01:08:32.720 --> 01:08:35.480]   just like the masking that we showed for not looking
[01:08:35.480 --> 01:08:36.520]   at the future in general.
[01:08:36.520 --> 01:08:40.380]   You can just say, set all of the attention weights to 0,
[01:08:40.380 --> 01:08:42.300]   or the scores to negative infinity
[01:08:42.300 --> 01:08:43.520]   for all of the pad tokens.
[01:08:43.520 --> 01:08:47.660]   [INAUDIBLE]
[01:08:47.660 --> 01:08:48.280]   Yeah, exactly.
[01:08:48.280 --> 01:08:52.200]   So you can set everything to this maximum length.
[01:08:52.200 --> 01:08:53.820]   Now, in practice-- so the question was,
[01:08:53.820 --> 01:08:55.740]   do you set this length that you have everything
[01:08:55.740 --> 01:08:56.980]   be that maximum length?
[01:08:56.980 --> 01:09:00.780]   I mean, yes, often, although you can save computation
[01:09:00.780 --> 01:09:03.420]   by setting it to something smaller.
[01:09:03.420 --> 01:09:06.140]   And everything-- the math all still works out.
[01:09:06.140 --> 01:09:08.700]   You just have to code it properly so it can handle--
[01:09:08.700 --> 01:09:10.220]   you set everything instead of to n.
[01:09:10.220 --> 01:09:12.260]   You set it all to 5 if everything
[01:09:12.260 --> 01:09:15.260]   is shorter than length 5, and you save a lot of computation.
[01:09:15.260 --> 01:09:19.340]   All of the self-attention operations just work.
[01:09:19.340 --> 01:09:21.980]   So yeah.
[01:09:21.980 --> 01:09:25.340]   How many layers are in the feedforward normally?
[01:09:25.340 --> 01:09:27.500]   There's one hidden layer in the feedforward usually.
[01:09:27.500 --> 01:09:28.060]   Oh, just one?
[01:09:28.060 --> 01:09:28.980]   Yeah.
[01:09:28.980 --> 01:09:30.060]   OK, I should move on.
[01:09:30.060 --> 01:09:32.460]   We've got a couple more things and not very much time.
[01:09:32.460 --> 01:09:33.820]   OK.
[01:09:33.820 --> 01:09:35.620]   But I'll be here after the class as well.
[01:09:35.620 --> 01:09:38.420]   So in the encoder-- so the transformer encoder
[01:09:38.420 --> 01:09:39.540]   is almost identical.
[01:09:39.540 --> 01:09:41.900]   But again, we want bidirectional context.
[01:09:41.900 --> 01:09:44.500]   And so we just don't do the masking.
[01:09:44.500 --> 01:09:46.740]   So I've got in my multi-head attention here,
[01:09:46.740 --> 01:09:48.620]   I've got no masking.
[01:09:48.620 --> 01:09:53.380]   And so it's that easy to make the model bidirectional.
[01:09:53.380 --> 01:09:54.100]   So that's easy.
[01:09:54.100 --> 01:09:55.800]   So that's called the transformer encoder.
[01:09:55.800 --> 01:09:58.100]   It's almost identical but no masking.
[01:09:58.100 --> 01:10:01.900]   And then finally, we've got the transformer encoder decoder,
[01:10:01.900 --> 01:10:04.060]   which is actually how the transformer was originally
[01:10:04.060 --> 01:10:07.900]   presented in this paper, "Attention is All You Need."
[01:10:07.900 --> 01:10:10.700]   And this is when we want to have a bidirectional network.
[01:10:10.700 --> 01:10:11.500]   Here's the encoder.
[01:10:11.500 --> 01:10:13.420]   It takes in, say, my source sentence
[01:10:13.420 --> 01:10:15.060]   for machine translation.
[01:10:15.060 --> 01:10:17.580]   Its multi-headed attention is not masked.
[01:10:17.580 --> 01:10:22.140]   And I have a decoder to decode out my sentence.
[01:10:22.140 --> 01:10:24.740]   Now, but you'll see that this is slightly more complicated.
[01:10:24.740 --> 01:10:27.500]   I have my masked multi-head self-attention,
[01:10:27.500 --> 01:10:29.940]   just like I had before in my decoder.
[01:10:29.940 --> 01:10:32.980]   But now I have an extra operation,
[01:10:32.980 --> 01:10:35.220]   which is called cross-attention, where
[01:10:35.220 --> 01:10:41.460]   I am going to use my decoder vectors as my queries.
[01:10:41.460 --> 01:10:45.860]   Then I'll take the output of the encoder as my keys and values.
[01:10:45.860 --> 01:10:48.100]   So now for every word in the decoder,
[01:10:48.100 --> 01:10:50.580]   I'm looking at all the possible words
[01:10:50.580 --> 01:10:53.460]   in the output of all of the blocks of the encoder.
[01:10:53.460 --> 01:10:54.460]   Yes?
[01:10:54.460 --> 01:10:54.940]   [INAUDIBLE]
[01:10:54.940 --> 01:11:04.340]   How do we get a key and value separated from the output?
[01:11:04.340 --> 01:11:07.900]   Because didn't we collapse those into the single output?
[01:11:07.900 --> 01:11:10.700]   So we-- well, how-- sorry.
[01:11:10.700 --> 01:11:12.780]   How will we get the keys and values out?
[01:11:12.780 --> 01:11:15.020]   Like, how do we-- because when we have the output,
[01:11:15.020 --> 01:11:19.220]   didn't we collapse the keys and values into a single output?
[01:11:19.220 --> 01:11:20.100]   So the output--
[01:11:20.100 --> 01:11:21.020]   [INAUDIBLE]
[01:11:21.020 --> 01:11:22.440]   Yeah, the question is, how do you
[01:11:22.440 --> 01:11:24.220]   get the keys and values and queries out
[01:11:24.220 --> 01:11:25.700]   of this single collapsed output?
[01:11:25.700 --> 01:11:27.900]   Now, remember, the output for each word
[01:11:27.900 --> 01:11:30.620]   is just this weighted average of the value vectors
[01:11:30.620 --> 01:11:33.020]   for the previous words.
[01:11:33.020 --> 01:11:35.780]   And then from that output for the next layer,
[01:11:35.780 --> 01:11:38.700]   we apply a new key, query, and value transformation
[01:11:38.700 --> 01:11:42.420]   to each of them for the next layer of self-attention.
[01:11:42.420 --> 01:11:43.780]   So it's not actually that you're--
[01:11:43.780 --> 01:11:44.280]   [INAUDIBLE]
[01:11:44.280 --> 01:11:50.780]   Yeah, you apply the key matrix, the query matrix,
[01:11:50.780 --> 01:11:52.580]   to the output of whatever came before it.
[01:11:52.580 --> 01:11:53.820]   Yeah.
[01:11:53.820 --> 01:11:56.260]   And so just in a little bit of math,
[01:11:56.260 --> 01:11:59.820]   we have these vectors, h1 through hn,
[01:11:59.820 --> 01:12:03.420]   I'm going to call them the output of the encoder.
[01:12:03.420 --> 01:12:08.020]   And then I've got vectors that are the output of the decoder.
[01:12:08.020 --> 01:12:09.500]   So I've got these z's I'm calling
[01:12:09.500 --> 01:12:10.860]   the output of the decoder.
[01:12:10.860 --> 01:12:15.340]   And then I simply define my keys and my values
[01:12:15.340 --> 01:12:19.460]   from the encoder vectors, these h's.
[01:12:19.460 --> 01:12:22.860]   So I take the h's, I apply a key matrix and a value matrix,
[01:12:22.860 --> 01:12:26.540]   and then I define the queries from my decoder.
[01:12:26.540 --> 01:12:29.060]   So my queries here-- so this is why two of the arrows
[01:12:29.060 --> 01:12:31.300]   come from the encoder, and one of the arrows
[01:12:31.300 --> 01:12:32.540]   comes from the decoder.
[01:12:32.540 --> 01:12:36.140]   I've got my z's here, my queries, my keys and values
[01:12:36.140 --> 01:12:37.260]   from the encoder.
[01:12:37.260 --> 01:12:44.320]   So that is it.
[01:12:44.320 --> 01:12:45.540]   I've got a couple of minutes.
[01:12:45.540 --> 01:12:48.340]   I want to discuss some of the results of transformers,
[01:12:48.340 --> 01:12:49.920]   and I'm happy to answer more questions
[01:12:49.920 --> 01:12:53.060]   about transformers after class.
[01:12:53.060 --> 01:12:56.340]   So really, the original results of transformers,
[01:12:56.340 --> 01:12:58.580]   they had this big pitch for, oh, look,
[01:12:58.580 --> 01:13:02.340]   you can do way more computation because of parallelization.
[01:13:02.340 --> 01:13:04.780]   They got great results in machine translation.
[01:13:04.780 --> 01:13:13.060]   So you had-- you had transformers doing quite well,
[01:13:13.060 --> 01:13:16.980]   although not astoundingly better than existing machine
[01:13:16.980 --> 01:13:20.020]   translation systems.
[01:13:20.020 --> 01:13:22.260]   But they were significantly more efficient to train.
[01:13:22.260 --> 01:13:25.380]   Because you don't have this parallelization problem,
[01:13:25.380 --> 01:13:27.620]   you could compute on much more data much faster,
[01:13:27.620 --> 01:13:32.100]   and you could make use of faster GPUs much more.
[01:13:32.100 --> 01:13:35.060]   After that, there were things like document generation,
[01:13:35.060 --> 01:13:37.940]   where you had the old standard of sequence-to-sequence models
[01:13:37.940 --> 01:13:39.060]   to the LSTMs.
[01:13:39.060 --> 01:13:42.420]   And eventually, everything became transformers
[01:13:42.420 --> 01:13:45.140]   all the way down.
[01:13:45.140 --> 01:13:47.340]   Transformers also enabled this revolution
[01:13:47.340 --> 01:13:50.420]   into pre-training, which we'll go over in next year,
[01:13:50.420 --> 01:13:52.060]   next class.
[01:13:52.060 --> 01:13:54.660]   And the efficiency, the parallelizability
[01:13:54.660 --> 01:13:58.340]   allows you to compute on tons and tons of data.
[01:13:58.340 --> 01:14:02.580]   And so after a certain point, on standard large benchmarks,
[01:14:02.580 --> 01:14:04.740]   everything became transformer-based.
[01:14:04.740 --> 01:14:07.540]   This ability to make use of lots and lots of data,
[01:14:07.540 --> 01:14:10.380]   lots and lots of compute, just put transformers head
[01:14:10.380 --> 01:14:13.420]   and shoulders above LSTMs in, let's say,
[01:14:13.420 --> 01:14:19.900]   almost every modern advancement in natural language processing.
[01:14:19.900 --> 01:14:24.620]   There are many drawbacks and variants to transformers.
[01:14:24.620 --> 01:14:25.820]   The clearest one that people have
[01:14:25.820 --> 01:14:28.420]   tried to work on quite a bit is this quadratic compute
[01:14:28.420 --> 01:14:29.260]   problem.
[01:14:29.260 --> 01:14:31.780]   So this all pairs of interactions
[01:14:31.780 --> 01:14:34.420]   means that our total computation for each block
[01:14:34.420 --> 01:14:36.260]   grows quadratically with the sequence length.
[01:14:36.260 --> 01:14:39.780]   And in a student's question, we heard that, well,
[01:14:39.780 --> 01:14:41.580]   as the sequence length becomes long,
[01:14:41.580 --> 01:14:44.220]   if I want to process a whole Wikipedia article,
[01:14:44.220 --> 01:14:48.100]   a whole novel, that becomes quite unfeasible.
[01:14:48.100 --> 01:14:50.780]   And actually, that's a step backwards in some sense,
[01:14:50.780 --> 01:14:52.740]   because for recurrent neural networks,
[01:14:52.740 --> 01:14:55.540]   it only grew linearly with the sequence length.
[01:14:55.540 --> 01:14:57.380]   Other things people have tried to work on
[01:14:57.380 --> 01:14:59.860]   are better position representations,
[01:14:59.860 --> 01:15:02.060]   because the absolute index of a word
[01:15:02.060 --> 01:15:05.260]   is not really the best way maybe to represent
[01:15:05.260 --> 01:15:07.940]   its position in a sequence.
[01:15:07.940 --> 01:15:10.300]   And just to give you an intuition of quadratic sequence
[01:15:10.300 --> 01:15:13.620]   length, remember that we had this big matrix multiply here
[01:15:13.620 --> 01:15:16.860]   that resulted in this matrix of n by n.
[01:15:16.860 --> 01:15:20.380]   And computing this is a big cost.
[01:15:20.380 --> 01:15:22.420]   It costs a lot of memory.
[01:15:22.420 --> 01:15:23.700]   And so there's been work--
[01:15:23.700 --> 01:15:24.220]   oh, yeah.
[01:15:24.220 --> 01:15:26.260]   And so if you think of the model dimensionality
[01:15:26.260 --> 01:15:29.340]   as like 1,000, although today it gets much larger,
[01:15:29.340 --> 01:15:32.460]   then for a short sequence of n is roughly 30,
[01:15:32.460 --> 01:15:37.540]   maybe if you're computing n squared times d, 30 isn't so
[01:15:37.540 --> 01:15:38.740]   bad.
[01:15:38.740 --> 01:15:42.020]   But if you had something like 50,000,
[01:15:42.020 --> 01:15:46.540]   then n squared becomes huge and sort of totally infeasible.
[01:15:46.540 --> 01:15:49.180]   So people have tried to sort of map things down
[01:15:49.180 --> 01:15:51.060]   to a lower dimensional space to get rid
[01:15:51.060 --> 01:15:54.540]   of the sort of quadratic computation.
[01:15:54.540 --> 01:15:56.820]   But in practice, I mean, as people
[01:15:56.820 --> 01:15:59.700]   have gone to things like GPT-3, Chat-GPT,
[01:15:59.700 --> 01:16:03.260]   most of the computation doesn't show up in the self-attention.
[01:16:03.260 --> 01:16:05.500]   So people are wondering sort of is it even
[01:16:05.500 --> 01:16:08.900]   necessary to get rid of the self-attention operations
[01:16:08.900 --> 01:16:10.300]   quadratic constraint?
[01:16:10.300 --> 01:16:12.900]   It's an open form of research whether this
[01:16:12.900 --> 01:16:14.780]   is sort of necessary.
[01:16:14.780 --> 01:16:17.460]   And then finally, there have been a ton of modifications
[01:16:17.460 --> 01:16:21.940]   to the transformer over the last five, four-ish years.
[01:16:21.940 --> 01:16:25.100]   And it turns out that the original transformer
[01:16:25.100 --> 01:16:27.820]   plus maybe a couple of modifications
[01:16:27.820 --> 01:16:31.140]   is pretty much the best thing there is still.
[01:16:31.140 --> 01:16:32.580]   There have been a couple of things
[01:16:32.580 --> 01:16:33.940]   that end up being important.
[01:16:33.940 --> 01:16:37.380]   Changing out the nonlinearities in the feedforward network
[01:16:37.380 --> 01:16:38.740]   ends up being important.
[01:16:38.740 --> 01:16:43.140]   But it's had lasting power so far.
[01:16:43.140 --> 01:16:46.340]   But I think it's ripe for people to come through and think
[01:16:46.340 --> 01:16:49.220]   about how to sort of improve it in various ways.
[01:16:49.220 --> 01:16:52.020]   So pre-training is on Tuesday.
[01:16:52.020 --> 01:16:53.260]   Good luck on assignment four.
[01:16:53.260 --> 01:16:57.060]   And then we'll have the project proposal documents out tonight
[01:16:57.060 --> 01:16:59.140]   for you to talk about.
[01:16:59.140 --> 01:17:09.140]   [BLANK_AUDIO]

