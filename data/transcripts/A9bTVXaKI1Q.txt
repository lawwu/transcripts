
[00:00:00.000 --> 00:00:07.920]   Translation is in this sort of space of so-called AI complete problems. So solving it would
[00:00:07.920 --> 00:00:15.760]   be equivalent to the advent of strong AI, if you will. Because for any particular translation
[00:00:15.760 --> 00:00:19.440]   problem, world knowledge is required to solve the problem.
[00:00:19.440 --> 00:00:23.840]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:23.840 --> 00:00:26.080]   and I'm your host, Lukas Biewald.
[00:00:26.080 --> 00:00:31.920]   Spence Green is a machine translation researcher and also the CEO of a startup called Lilt,
[00:00:31.920 --> 00:00:37.840]   which is a leading language translation services company. He has been using TensorFlow since the
[00:00:37.840 --> 00:00:42.640]   very beginning and has been putting deep learning models into production for longer than almost any
[00:00:42.640 --> 00:00:48.320]   of us. I'm super excited to talk to him today. I think the best place to start here is you're
[00:00:48.320 --> 00:00:53.760]   the CEO of Lilt and you built Lilt. Maybe you could just give us a description of what Lilt is
[00:00:54.400 --> 00:00:57.360]   and what it does. Well, I think it's important to
[00:00:57.360 --> 00:01:05.680]   sort of say where the company came from and the problem that it solves. And then I can kind of
[00:01:05.680 --> 00:01:10.000]   explain what it does. I think what it does follows from that. Perfect. Yeah, that's great.
[00:01:10.000 --> 00:01:15.280]   Where it started, at least for me personally, in my mid-20s, I decided I wanted to learn a language.
[00:01:15.280 --> 00:01:20.400]   And so I moved to the Middle East for about two and a half years. And while I was there,
[00:01:20.400 --> 00:01:26.320]   two important things happened. The first was I learned that... So I was learning Arabic and I had
[00:01:26.320 --> 00:01:31.840]   a friend and I was talking to him one night and he said... He was like the building watchman in
[00:01:31.840 --> 00:01:35.920]   my building. I was talking to him and I was like, "What did you do in Egypt where he was from?" And
[00:01:35.920 --> 00:01:39.120]   he said, "I was an accountant." And I said, "Oh, really? Why weren't you an accountant here?" And
[00:01:39.120 --> 00:01:43.760]   he said, "Because I don't speak English." I was like, "Okay. Well, we're in an Arabic-speaking
[00:01:43.760 --> 00:01:49.200]   country and you can't get a job as an accountant?" And it's because you just can't get that...
[00:01:50.160 --> 00:01:53.760]   People make a certain amount of money if they speak English. If they don't, they make less.
[00:01:53.760 --> 00:01:58.960]   And I'd never really encountered that before. And six months or so after that conversation,
[00:01:58.960 --> 00:02:05.360]   Google Translate came out and I got really excited about that. And so I left my job,
[00:02:05.360 --> 00:02:11.840]   went to grad school and started working on MT. And then a couple of years later, I was at Google
[00:02:11.840 --> 00:02:17.680]   working on Translate where I met John, my now co-founder, and Franzach, who started the group
[00:02:17.680 --> 00:02:21.600]   at Google and did all the really early pioneering work in statistical MT.
[00:02:21.600 --> 00:02:28.080]   And we were originally talking about books a lot and why books don't get translated. And we found
[00:02:28.080 --> 00:02:33.200]   that Google's localization team that did all of their language-related work for the products
[00:02:33.200 --> 00:02:40.640]   didn't use Google Translate. And this was kind of amazing to me. Why would this be? And the reason
[00:02:40.640 --> 00:02:46.160]   is because in any sort of business setting or non-consumer setting, you need a quality guarantee.
[00:02:46.160 --> 00:02:49.760]   And so an MT system can give you, like any machine learning system, it can give you a prediction,
[00:02:49.760 --> 00:02:54.480]   but it can't really give you a grounded certificate of correctness about whether it's right. And
[00:02:54.480 --> 00:02:59.600]   that's what businesses want or book publishers or whatever. So we started building these human in
[00:02:59.600 --> 00:03:04.240]   the loop systems where you need the human for the certificate of correctness, but the crux of
[00:03:04.240 --> 00:03:09.600]   the problem is to make that intervention as efficient as you can. I guess my biggest
[00:03:09.600 --> 00:03:14.240]   question that I was thinking about that I've always wanted to ask you is sort of like,
[00:03:14.240 --> 00:03:20.000]   how different is the problem of translating something properly versus sort of setting up
[00:03:20.000 --> 00:03:25.040]   a kind of human in the loop system with a human translator to translate well?
[00:03:25.040 --> 00:03:28.720]   Is it almost the same problem or is it quite different?
[00:03:28.720 --> 00:03:36.160]   By translating it properly, what do you mean? I guess I mean, so Google Translate is just
[00:03:36.160 --> 00:03:42.880]   trying to give me the best possible translation. I sort of assume that what you're doing is helping
[00:03:42.880 --> 00:03:48.000]   a translator be successful translating something, presumably by kind of guessing
[00:03:48.000 --> 00:03:53.200]   likely translations. Yeah, right. So it's a good question. So the question is the mode of
[00:03:53.200 --> 00:03:59.520]   interaction with the machine and the way that machine translation systems have been used
[00:03:59.520 --> 00:04:05.280]   really since the early fifties was when this line of research started. It's funny,
[00:04:05.280 --> 00:04:11.760]   machine translation was like this really old machine learning task. And originally people
[00:04:11.760 --> 00:04:15.520]   thought the digital computers that were developed during the second world war for
[00:04:15.520 --> 00:04:21.120]   bomb making and for cryptography, the initial idea was, well, Russian is just
[00:04:21.120 --> 00:04:28.240]   English encrypted and Cyrillic. And so we can just decrypt Russian. And so the initial systems that
[00:04:28.240 --> 00:04:33.200]   were built in the fifties weren't very good. And so the naive idea was, well, let's just take the
[00:04:33.200 --> 00:04:40.560]   machine output and pay somebody to fix it. And this sort of linear editing workflow is
[00:04:41.280 --> 00:04:47.520]   what our work in grad school was about, was going beyond that in some way, like a richer mode of
[00:04:47.520 --> 00:04:52.960]   interaction. And what we came up with was effectively a predictive typing interface.
[00:04:52.960 --> 00:04:56.400]   Or there are two problems that we really wanted to solve. One was when you're doing translation,
[00:04:56.400 --> 00:05:00.560]   the system makes the same mistake over and over again. Documents tend to be pretty repetitive.
[00:05:00.560 --> 00:05:03.920]   It's an annoying user experience and it's inefficient when the system just makes the
[00:05:03.920 --> 00:05:08.640]   wrong prediction over and over again. So the solution to that is to have a system that does
[00:05:08.640 --> 00:05:13.760]   online learning, which was part of the work. And the other was, well, how can you interact with
[00:05:13.760 --> 00:05:21.760]   a text string beyond just using your cursor and fixing parts of it? And that is doing predictive
[00:05:21.760 --> 00:05:25.440]   typing. So if you put those two together, you want to do online learning and you want to do
[00:05:25.440 --> 00:05:30.160]   predictive typing, it's kind of a fundamentally different system architecture than the type of
[00:05:30.160 --> 00:05:36.720]   system you've built for Google Translate system architecture. Although it seems fairly close.
[00:05:38.080 --> 00:05:42.720]   The predictive typing, I would think you have a language model and a translation model. Is it
[00:05:42.720 --> 00:05:47.040]   the same? Or at least that's how MT systems used to work, or at least in my memory, right?
[00:05:47.040 --> 00:05:54.240]   That's the way that the statistical systems used to work. And really it came down to doing inference
[00:05:54.240 --> 00:05:59.840]   really rapidly. Well, yeah, it came down to doing inference really rapidly and doing inference with
[00:05:59.840 --> 00:06:05.920]   a prefix. So instead of just decoding a sentence with a null prefix, you send it part of what the
[00:06:05.920 --> 00:06:10.080]   translator did. The old systems, we actually had a paper on this a couple of years ago,
[00:06:10.080 --> 00:06:14.480]   how to do inference with a prefix was a algorithmic problem that you had to solve.
[00:06:14.480 --> 00:06:18.720]   The new neural systems just do greedy beam search. So it's actually pretty straightforward
[00:06:18.720 --> 00:06:22.720]   to do that these days. And is that what you're using?
[00:06:22.720 --> 00:06:28.960]   Yeah. I mean, like everything in NLP these days, it's a transformer architecture and a pretty
[00:06:28.960 --> 00:06:34.560]   vanilla one too. What our team really focuses on is domain adaptation, rapid and efficient domain
[00:06:34.560 --> 00:06:39.280]   adaptation. So we do personalized models either at the user level or at the workflow level for
[00:06:39.280 --> 00:06:43.600]   all of our customers. All right. And workflow means like a set of documents. So you're sort
[00:06:43.600 --> 00:06:47.600]   of like learning a specialized model as the transition happens?
[00:06:47.600 --> 00:06:53.520]   I think the way to think about it is more kind of like from your early days, which is anywhere that
[00:06:53.520 --> 00:06:58.720]   you have an annotation standard, you would have a personalized model. So if you think about in a
[00:06:58.720 --> 00:07:05.360]   business, like a marketing workflow has a writing standard that may be different than a legal
[00:07:05.360 --> 00:07:09.600]   workflow. And so you would have different models for each one of those workflows.
[00:07:09.600 --> 00:07:16.400]   I see. And so you're actually training then thousands of more models.
[00:07:16.400 --> 00:07:23.440]   Yes, that's correct. And that also has... So that's right. So there are bunches of different
[00:07:23.440 --> 00:07:29.440]   models being trained continuously in production all the time right now. And the way that you can
[00:07:29.440 --> 00:07:35.120]   think about what the translator does, and I think what's really interesting about this task is in
[00:07:35.120 --> 00:07:39.280]   most machine learning settings, like data annotation for supervised learning is some
[00:07:39.280 --> 00:07:44.080]   operating costs. You have to pay people to go off and do it. It's an artificial task.
[00:07:44.080 --> 00:07:48.480]   Translation, you can think about them. They're just doing data labeling. They're reading English
[00:07:48.480 --> 00:07:52.560]   sentences and typing French sentences. As soon as they finish that, you just train on it.
[00:07:52.560 --> 00:07:57.040]   Right. And do the models get noticeably better over time?
[00:07:57.040 --> 00:07:57.540]   Yes.
[00:07:57.540 --> 00:08:04.480]   That's super cool. So I guess I'm curious about the technical details of just making this work,
[00:08:04.480 --> 00:08:10.320]   but before getting into that, I'm curious, you started in 2014. Is that right?
[00:08:10.320 --> 00:08:13.600]   Early 2015, we started the company.
[00:08:13.600 --> 00:08:19.120]   2015. So you've seen such an arc in terms of... I mean, I feel like machine translation has had
[00:08:19.120 --> 00:08:26.800]   such big changes, at least from my perspective. Has that been hard to adapt to? Has that
[00:08:26.800 --> 00:08:31.840]   kind of helped you? Have you had to learn new skills to take advantage of it?
[00:08:31.840 --> 00:08:41.520]   Yeah. We started the company in late 2014, and the system that we had, which we had built at Stanford
[00:08:41.520 --> 00:08:47.840]   over the course of about 10 years, was competitive with Google Translate. And so then in December
[00:08:47.840 --> 00:08:52.640]   2014, the first sort of neural MT paper was published. I mean, people worked on neural MT
[00:08:52.640 --> 00:08:56.480]   in the '90s, but it didn't work. And so they sort of got it to work again. There were two papers
[00:08:56.480 --> 00:09:03.920]   published, one in December 2014, the other one in January of 2015. And it was pretty promising,
[00:09:03.920 --> 00:09:08.240]   but nowhere near production ready. And then I think the thing that was really quite shocking
[00:09:08.240 --> 00:09:13.600]   was how quickly Google got that into a production scale system, which happened in the late summer
[00:09:13.600 --> 00:09:20.480]   of 2016. And so at that point, our system was as competitive as anyone. And then suddenly,
[00:09:20.480 --> 00:09:29.200]   there was this huge leap in translation quality. And we were sort of graduating, all three of us,
[00:09:29.200 --> 00:09:36.400]   John and I, and a third guy, right at this crossover point. So we didn't really have any
[00:09:36.400 --> 00:09:42.800]   empirical experience with these neural machine translation systems. So we had to build a neural
[00:09:42.800 --> 00:09:50.480]   MT system from scratch over the course of about six months. And we went from... The Stanford system
[00:09:50.480 --> 00:09:56.160]   was about 120,000 lines of code that had been developed over a decade, going to a system that
[00:09:56.160 --> 00:10:02.400]   I think was about 5,700, 6,000 lines of code. That's amazing.
[00:10:02.400 --> 00:10:07.280]   I mean, it's really quite shocking. I mean, a bunch of that is like pushing a lot of the
[00:10:09.520 --> 00:10:13.600]   functionality down into the framework, which everything in the Stanford system was custom
[00:10:13.600 --> 00:10:20.800]   built. I guess in 2016, what framework are you using? Is this CAFE or is it even before that?
[00:10:20.800 --> 00:10:24.480]   No, we wrote it in TensorFlow from the beginning.
[00:10:24.480 --> 00:10:26.160]   Oh, wow. Wow. Cool.
[00:10:26.160 --> 00:10:33.200]   It was, I guess, an okay technology bet. I think there's some push to move to PyTorch,
[00:10:33.200 --> 00:10:36.320]   but we've got a pretty significant investment in TensorFlow at this point.
[00:10:36.320 --> 00:10:40.640]   Yeah, I would think so. And were you sure that it was going to work? I mean,
[00:10:40.640 --> 00:10:46.640]   this seems like a really painful experiment for a startup to do mid-flight.
[00:10:46.640 --> 00:10:53.200]   It was terrible. Yeah. I mean, you just kind of had to do it. The results were so compelling.
[00:10:53.200 --> 00:11:01.040]   And I think that MT really is probably, of all the tasks within NLP, that deep learning is really
[00:11:01.040 --> 00:11:05.360]   revolutionized. I think you're really pointing to make the case that MT is probably the
[00:11:05.360 --> 00:11:12.240]   most significant example. The recent language modeling work, of course, is really impressive,
[00:11:12.240 --> 00:11:17.920]   but MT just went from being kind of funny to being meaningfully good.
[00:11:17.920 --> 00:11:24.400]   And I guess, how did you find enough parallel corporate to make this work?
[00:11:26.480 --> 00:11:32.000]   Well, there's quite a bit of public domain data. So, for example, the UN has to publish all of its
[00:11:32.000 --> 00:11:37.600]   proceedings in its member languages. There are news organizations like the AP that publish in
[00:11:37.600 --> 00:11:42.560]   different languages. There are open source projects, that GNOME project, for example,
[00:11:42.560 --> 00:11:46.880]   that publishes all their strings in a bunch of different languages. So, you can train on all
[00:11:46.880 --> 00:11:50.800]   that. And then you've got web crawl too, which is where most of the training data comes from.
[00:11:51.360 --> 00:11:56.640]   I see. I see. It's funny. I remember working on MT briefly at Stanford and feeling like it was
[00:11:56.640 --> 00:12:00.000]   really unfair that Google had so much more access to data.
[00:12:00.000 --> 00:12:01.520]   It does help to have a search engine.
[00:12:01.520 --> 00:12:08.320]   And I don't know if you came up... I mean, I guess if you're mostly doing web crawl,
[00:12:08.320 --> 00:12:13.440]   then that makes sense. But I remember just all kinds of weird artifacts from... I think we were
[00:12:13.440 --> 00:12:17.600]   training on sort of the EU data that was in all those languages. And it was just such kind of
[00:12:17.600 --> 00:12:23.120]   bias towards political meanings of nouns. It just seemed ludicrous sometimes.
[00:12:23.120 --> 00:12:28.720]   So, I think that's, in an enterprise setting, that's the real value of domain adaptation.
[00:12:28.720 --> 00:12:36.960]   And the second thing that I think is interesting is the legacy approach to enterprise translation
[00:12:36.960 --> 00:12:43.680]   within the enterprise is to just build a database of all your past translation. And if you translated
[00:12:43.680 --> 00:12:47.520]   something before, you just look it up in the corpus and retrieve it. Otherwise, you send it
[00:12:47.520 --> 00:12:53.680]   off to a vendor. So, big companies that have been doing translation for decades have this big corpus
[00:12:53.680 --> 00:13:02.720]   that they've built up. And so, we train on that too. And that sort of customer-specific training
[00:13:02.720 --> 00:13:08.400]   is where you get the real improvement versus just a big general domain system.
[00:13:09.600 --> 00:13:19.280]   I guess at the end of the day, do you measure your results in how fast you can get a translation
[00:13:19.280 --> 00:13:25.360]   done? Is that kind of your core metric? And I guess if so, how does that change with the quality of
[00:13:25.360 --> 00:13:29.600]   the translation? Do you kind of get diminishing returns? Or as it gets close to perfect,
[00:13:29.600 --> 00:13:38.160]   can someone just cruise through a translation? Well, I think that there are... You guys should
[00:13:38.160 --> 00:13:46.400]   say a few sentences about how a customer would work with us. So, an example of one of our customers is
[00:13:46.400 --> 00:13:51.280]   Intel. And if you go to intel.com, in the top right corner, there's a drop down and you can
[00:13:51.280 --> 00:13:59.440]   change the site into 16 different languages. And that's all of our work. And so, if you start
[00:13:59.440 --> 00:14:03.760]   looking that way, you'll see translation all around you. You'll see it on websites. You'll
[00:14:03.760 --> 00:14:09.520]   see it in mobile apps. You'll see it when you get on the airplane and get 10 language options for
[00:14:09.520 --> 00:14:15.280]   the in-flight entertainment system. That's where this can be used. And right now, it's a problem
[00:14:15.280 --> 00:14:20.880]   that you can solve with people. You can hire people to solve it. The problem is the number of...
[00:14:20.880 --> 00:14:25.920]   The amount of information that's being produced far exceeds the number of people that are being
[00:14:25.920 --> 00:14:31.520]   produced in the world right now. And so, you can't just solve it with just throwing bodies at it.
[00:14:31.520 --> 00:14:37.600]   And so, that's why you need some automation. So, an example like that Intel website,
[00:14:37.600 --> 00:14:44.240]   from their side, what they just see is us delivering words. And the only real metrics
[00:14:44.240 --> 00:14:49.040]   that matter are how quickly that gets done and the quality level that it gets done at.
[00:14:49.040 --> 00:14:55.200]   And they don't really care whether it's machines or lemmings or whatever is doing the translation
[00:14:55.200 --> 00:15:03.920]   work. On our side, the whole name of the game is using automation to reduce the production cost
[00:15:03.920 --> 00:15:11.920]   and the production cost per word. And so, when you produce a word to give to an enterprise, there's a
[00:15:11.920 --> 00:15:16.720]   translation cost and a QA cost and a workflow routing cost, and there's a software hosting
[00:15:16.720 --> 00:15:21.040]   cost. There's a bunch of different cost buckets, and it's just minimizing that.
[00:15:21.760 --> 00:15:25.840]   But I would think that... Am I wrong that the majority of the cost would be the human that's
[00:15:25.840 --> 00:15:28.960]   doing the translation? That's exactly right. So,
[00:15:28.960 --> 00:15:34.400]   then the metrics that we care about internally have to do with making that part more efficient,
[00:15:34.400 --> 00:15:40.160]   but that's not something that... It translates into business value and then it reduces the cost
[00:15:40.160 --> 00:15:45.600]   of what we provide to customers and it makes it faster. But those metrics are not the same metrics
[00:15:45.600 --> 00:15:50.800]   that our customers care about. Are there cases where you worry about
[00:15:50.800 --> 00:15:58.800]   with a self-driving car where someone... It's so good that they stop watching and then the car
[00:15:58.800 --> 00:16:04.240]   crashes. Does your translation ever get so good that you worry that an annotator might just start
[00:16:04.240 --> 00:16:11.760]   accepting every prediction and quality might suffer? Yeah, this is a good question. I think
[00:16:11.760 --> 00:16:18.000]   it's more of a risk, and this bears out empirically in the linear post-editing workflow that I
[00:16:18.000 --> 00:16:22.480]   mentioned where I just give you some machine output for some random machine and ask you to
[00:16:22.480 --> 00:16:28.240]   correct it. It's kind of a passive task and cognitively it's not very engaging.
[00:16:28.240 --> 00:16:35.040]   And so, people tend to just gloss through that and make mistakes. Whereas in the predictive
[00:16:35.040 --> 00:16:41.440]   typing, it's like an active engaged task. And so, if they're basically cheating there,
[00:16:41.440 --> 00:16:49.040]   then it comes down to performance management on our part of, "Whoa, this person did 2000 words in
[00:16:49.040 --> 00:16:54.880]   10 seconds. That doesn't seem right." So, you can kind of monitor that.
[00:16:54.880 --> 00:17:01.040]   And how do customers think about the quality? Is it sort of like an intuitive
[00:17:01.040 --> 00:17:03.760]   feel for it or are they spot checking it? How does that work?
[00:17:06.320 --> 00:17:11.680]   I think it's again in the same realm of an annotation standard like your world where
[00:17:11.680 --> 00:17:18.240]   we work with the customer to define what we call a text specification, which is what are the text
[00:17:18.240 --> 00:17:22.400]   requirements within each language? And that usually follows from marketing guidelines.
[00:17:22.400 --> 00:17:28.720]   They have their brand and style and copy editing guidelines. And then how does that manifest in
[00:17:28.720 --> 00:17:35.920]   Chinese and Japanese and German and French? And so, then we have a QA process where we have raters
[00:17:35.920 --> 00:17:42.320]   go in and rate the sentences according to that framework. And then that's what we deliver back
[00:17:42.320 --> 00:17:48.240]   to them. Oh, so you don't just deliver the result, you deliver an estimate of the quality
[00:17:48.240 --> 00:17:54.800]   based on raters. Oh, I see. That's cool. They must appreciate that. Or is that industry standards
[00:17:54.800 --> 00:17:56.560]   to do that? No.
[00:17:56.560 --> 00:18:04.320]   Oh, cool. There are some vendors that they'll implement a scorecard and they'll give you the
[00:18:04.320 --> 00:18:09.040]   scorecard back with the deliverable, but we just try to keep it. We just count the number of
[00:18:09.040 --> 00:18:15.120]   sentences where there's some annotation error and then we fix those, but it gives you some sense for
[00:18:15.120 --> 00:18:22.640]   what the overall error rate is. Got it. So, I guess I'm sure you've seen,
[00:18:22.640 --> 00:18:29.760]   I think people have pointed out that in translation, there can be ethical issues. I think people
[00:18:29.760 --> 00:18:36.320]   noticed that Google was in languages where the pronouns aren't gender specific, making a key for
[00:18:36.320 --> 00:18:42.080]   traditionally male occupations. Is that something that you think about or incorporate into
[00:18:42.080 --> 00:18:47.120]   your models at all? Well, I mean,
[00:18:47.120 --> 00:18:55.760]   I think the nice... So, I'll just give you an example. Part of my work in grad school was on
[00:18:55.760 --> 00:19:04.000]   Arabic. And when you work with Arabic corpora, there's almost all male pronouns because it's
[00:19:04.000 --> 00:19:09.520]   coming from Newswire and most of the people who are active politically in the Arab world are male.
[00:19:09.520 --> 00:19:17.120]   So, that's the representation in the data. And so, systems will tend to predict masculine and
[00:19:17.120 --> 00:19:21.600]   pronouns for lots of different things. But then the human in the loop model,
[00:19:22.480 --> 00:19:28.080]   you have people who are there correcting that and they can use the suggestion or not. And
[00:19:28.080 --> 00:19:34.480]   by that annotation, you'll get a different statistical trend that the system will start
[00:19:34.480 --> 00:19:36.080]   to learn. I see.
[00:19:36.080 --> 00:19:42.240]   So, it's sort of self-correcting. Cool. I guess I really am interested to know about
[00:19:42.240 --> 00:19:46.880]   the technical details of your system as much as you can share. I mean, you were a super early user
[00:19:46.880 --> 00:19:51.920]   of TensorFlow and you have all these models running in production. I mean, can you, at a
[00:19:51.920 --> 00:19:57.440]   high level, just sort of tell me how the system works and how it's evolved? Do you use TensorFlow
[00:19:57.440 --> 00:20:03.040]   serving to serve these up? Or how do you even run all these models in production at once?
[00:20:03.040 --> 00:20:07.040]   Yeah. It's an interesting... So,
[00:20:07.040 --> 00:20:20.800]   I think that maybe the most interesting part of it is how do you... The interesting cloud problem
[00:20:20.800 --> 00:20:27.280]   to solve, of which there are several, but I think the big ones are you have a budget. If you're
[00:20:27.280 --> 00:20:33.840]   implementing predictive typing, you have a budget of about 200 milliseconds before the suggestions
[00:20:33.840 --> 00:20:40.800]   feel sluggish. And so, that means that the speed of light starts to become a problem. And so,
[00:20:40.800 --> 00:20:46.320]   you have to have a multi-region setup because our community of people who are working are all over
[00:20:46.320 --> 00:20:51.120]   the world. You usually hire translators within their linguistic community that are fluent in
[00:20:51.120 --> 00:20:55.760]   that native language. So, we have people all over the world. So, the first thing is it has to be a
[00:20:55.760 --> 00:21:03.120]   multi-region system. The second is it's doing online learning. So, you have to coordinate
[00:21:03.120 --> 00:21:11.200]   model updates across regions. And the third thing that I think is interesting is to make inference
[00:21:11.200 --> 00:21:17.040]   fast. Commonly, in a big large-scale system like Google Translate, you'll batch a bunch of requests,
[00:21:17.040 --> 00:21:22.320]   put it on custom hardware, run it, and then return it. But if you're switching in personalized models
[00:21:22.320 --> 00:21:27.840]   to the decoder basically on every request, then you have to run on the CPU and you have to have
[00:21:27.840 --> 00:21:34.160]   a multi-level cache to be pulling these models up and off of cold storage and loading them onto the
[00:21:34.160 --> 00:21:40.160]   machine. So, that's been a lot of the engineering is to make it fast worldwide and to make the
[00:21:40.160 --> 00:21:43.000]   learning synchronized worldwide. Lukas:
[00:21:43.000 --> 00:21:53.120]   And I guess you mentioned that there's some notion of switching to PyTorch. What would
[00:21:53.120 --> 00:21:54.480]   push that at all? Nick:
[00:21:54.480 --> 00:22:01.680]   This is where my expertise, my empirical limitations run into a line. I think at ease of...
[00:22:01.680 --> 00:22:07.760]   The two things that I've heard from our team are you can prototype faster in PyTorch than in
[00:22:07.760 --> 00:22:13.760]   TensorFlow. And then there have been some backwards compatibility issues from TensorFlow
[00:22:13.760 --> 00:22:19.360]   1 to TensorFlow 2. There tend to be more breaking changes. And so, we've got our system running in
[00:22:19.360 --> 00:22:26.640]   some TensorFlow 2 compatibility mode with some frozen graphs from before. And so, that's been
[00:22:26.640 --> 00:22:29.440]   a little bit of a problem. Lukas:
[00:22:29.440 --> 00:22:37.200]   I think one just sort of notable thing from our perspective has been this sort of rapid ascendance
[00:22:37.200 --> 00:22:41.520]   of hugging face. Has that been relevant to you at all? Do you use it anywhere?
[00:22:41.520 --> 00:22:44.080]   Nick: We don't. I think when...
[00:22:44.080 --> 00:22:53.600]   It's funny. When that paper, when the Transformer paper came out, I went to grad school. Ashish
[00:22:53.600 --> 00:22:58.800]   was... Vaswani was a contemporary at grad school. And then Jakub Buzkurait has been a great friend
[00:22:58.800 --> 00:23:03.600]   of our company. And so, we kind of called Jakub the next day and we're like, "Let's talk about
[00:23:03.600 --> 00:23:10.400]   this." And so, we talked it through and we started working on it. And it was a really tricky model to
[00:23:10.400 --> 00:23:15.280]   get working correctly. And it took some time. So, we started... I think that paper came on
[00:23:15.280 --> 00:23:20.560]   a Tuesday of Memory Served. And I think Joran started working on the implementation on Wednesday
[00:23:20.560 --> 00:23:28.240]   morning, something like that. And it was December or January before we had a working model.
[00:23:28.880 --> 00:23:34.160]   And I think their Tensor2Tensor release helped a lot. There's some of the Black Magic is in there
[00:23:34.160 --> 00:23:40.800]   that helped. So, this was mid-2017. But it's tricky to get working right in production. So,
[00:23:40.800 --> 00:23:47.760]   I think having a library that people can use more broadly that may not have the same internal
[00:23:47.760 --> 00:23:51.680]   resources to get these systems working, it's really, really, really valuable.
[00:23:51.680 --> 00:24:02.160]   Totally. Totally. Do you think that given your... Do your latency and throughput
[00:24:02.160 --> 00:24:07.440]   requirements mean that your models are different at all than what a Google Translate might use?
[00:24:07.440 --> 00:24:17.120]   Yes. If you're running on custom hardware, you can, of course, afford to run higher dimensional
[00:24:17.120 --> 00:24:22.240]   and more expressive models. So, we have to do quite a bit of work with knowledge distillation
[00:24:22.240 --> 00:24:28.400]   to try to compress the models so that inference is fast on the CPU. It's also been really helpful,
[00:24:28.400 --> 00:24:32.160]   Intel is one of our investors. And so, their technical teams have helped us with some
[00:24:32.160 --> 00:24:37.840]   optimizations to make it run faster on the CPU. And that's been really valuable.
[00:24:37.840 --> 00:24:42.560]   That's cool. Do you use different models at all for different language pairs?
[00:24:42.560 --> 00:24:49.120]   It's kind of... Yeah. The short answer is yes. There's a general domain model for every language
[00:24:49.120 --> 00:24:54.560]   pair that the domain adaptation starts from. And it basically just forks off of that. And then,
[00:24:54.560 --> 00:25:03.040]   the model fork starts learning. And so, we change the general domain models much less frequently.
[00:25:03.040 --> 00:25:09.600]   And so, we just actually yesterday released new models for English to Japanese and Japanese to
[00:25:09.600 --> 00:25:14.880]   English. And one of the researchers has been working on a much deeper encoder. So, I think
[00:25:14.880 --> 00:25:18.800]   the one that came out yesterday has a 12-layer encoder, whereas historically, we've been running
[00:25:18.800 --> 00:25:24.640]   a four-layer encoder or something like that. So now, over the next little bit, we'll be moving
[00:25:24.640 --> 00:25:29.600]   more of our general domain models to some of the current state of the model architecture.
[00:25:29.600 --> 00:25:33.760]   And your general domain models, though, those are different for each language pair, right?
[00:25:33.760 --> 00:25:38.160]   Or is there sort of one? Yes. Oh, that's an important point.
[00:25:38.160 --> 00:25:43.840]   So, I think one of the most exciting papers in the last couple of years was training multi-source,
[00:25:43.840 --> 00:25:49.680]   multi-target models. And so, Google had a paper, it was last year or the year before, where they
[00:25:49.680 --> 00:25:55.040]   just piled all the corpora together and trained this huge neural network. And this is really hard
[00:25:55.040 --> 00:26:03.520]   to think about coming from the statistical MT days, because it's just crazy to do in a statistical MT
[00:26:03.520 --> 00:26:10.400]   system. But we use some groups of languages. So, we'll group similar languages, especially if
[00:26:10.400 --> 00:26:14.720]   they're low-resource languages and we don't have much data. And then you'll have a system that's
[00:26:14.720 --> 00:26:18.640]   for five different languages or so. But there's something about that that's
[00:26:18.640 --> 00:26:24.240]   so appealing. I mean, I'm way out of date, so I never saw that working when I was in grad school,
[00:26:24.240 --> 00:26:28.400]   but I love the idea of it. Yeah. It's a really attractive idea.
[00:26:28.400 --> 00:26:31.680]   And it sounds like it's actually kind of working. It does work. Yeah.
[00:26:31.680 --> 00:26:37.760]   So, I guess, I don't know how much you feel comfortable expounding on this topic, but I'm
[00:26:37.760 --> 00:26:47.120]   really curious, do you have a feeling on how far MT goes? Do you think that human level MT is
[00:26:47.120 --> 00:26:54.640]   realistic? It's funny when you talk about companies wanting quality guarantees. I mean,
[00:26:54.640 --> 00:27:00.080]   I would think just having used a lot of Google Translate in my life, quality guarantees seem
[00:27:00.080 --> 00:27:03.680]   like it would be useful, but also it just seems like the quality of Google Translate just isn't
[00:27:03.680 --> 00:27:09.040]   good enough that I would want to put that on my website generally. Do you expect that that
[00:27:09.040 --> 00:27:16.720]   is likely to change? Yeah. I guess I can offer some sort of
[00:27:16.720 --> 00:27:19.840]   assorted comments on thinking about that. Please. Thank you.
[00:27:19.840 --> 00:27:27.120]   In no particular order, because I think there are both technical and social issues to do with that.
[00:27:28.240 --> 00:27:33.680]   And I think there's some philosophical issues. So, let's start with the philosophical issue.
[00:27:33.680 --> 00:27:41.120]   Translation is in this sort of space of so-called AI complete problems. So, solving it would be
[00:27:41.120 --> 00:27:48.480]   equivalent to the advent of strong AI, if you will, because for any particular translation problem,
[00:27:48.480 --> 00:27:54.480]   world knowledge is required to solve the problem. And there are inputs that are not in the string
[00:27:54.480 --> 00:27:57.760]   that are required to produce a translation in another language.
[00:27:57.760 --> 00:28:04.000]   Although, sorry to cut you off, but I guess it feels like based on what I've seen lately from
[00:28:04.000 --> 00:28:09.040]   Google Translate, that it feels like less AI complete than I would have thought.
[00:28:09.040 --> 00:28:13.680]   Yes. So, that's the next comment that I'll make, which is that philosophical statement
[00:28:13.680 --> 00:28:19.600]   doesn't mean that within business settings, you should not be using it. And I'll give you an
[00:28:19.600 --> 00:28:26.160]   example. So, one space we've been looking at recently is crypto. Well, four months ago,
[00:28:26.160 --> 00:28:31.200]   nobody knew what a non-fungible token is. So, how do you translate that into Swahili and Korean?
[00:28:31.200 --> 00:28:34.240]   Well, an empty system is not going to give you the answer to that question because language is
[00:28:34.240 --> 00:28:39.200]   productive. People are making new words all the time. And machines are not making up new words
[00:28:39.200 --> 00:28:44.560]   all the time. People are. And so, philosophically, you've got to have training data for the system to
[00:28:44.560 --> 00:28:49.600]   be able to produce a result. People do not need training data to do that. But then I think
[00:28:49.600 --> 00:28:54.720]   increasingly, there are a lot of business settings where it's good enough to solve the problem.
[00:28:54.720 --> 00:29:02.160]   So, if you go for years, you can go to Airbnb and look at a flat and click translate with Google,
[00:29:02.160 --> 00:29:08.160]   and it'll give you a translation. It may not be perfect, but it's certainly enough to convince
[00:29:08.160 --> 00:29:13.280]   you you want to buy this, rent this flat. And I think there will be more and more cases where
[00:29:13.280 --> 00:29:19.360]   fully automatic machine translation solves the business problem at hand. I think that's
[00:29:19.360 --> 00:29:24.000]   absolutely true. And then I think there's a third part of it, which is sort of social and
[00:29:24.000 --> 00:29:34.400]   organizational, which is how soon, VP of marketing, are you willing to let raw machine translation go
[00:29:34.400 --> 00:29:42.480]   on your landing page with no oversight? And one way to think about that is how soon are you, Lucas,
[00:29:42.480 --> 00:29:47.440]   ready for a machine to respond to all of your email? All of my own email?
[00:29:47.440 --> 00:29:52.160]   Yeah. Well, I have to say- Some of it, probably sure. But
[00:29:52.160 --> 00:29:56.480]   others, parts of it, a little bit dangerous. I mean, this might be kind of an off the wall
[00:29:56.480 --> 00:30:02.640]   question, but I have noticed my... I think I have a slightly more polite writing style because of
[00:30:02.640 --> 00:30:08.560]   Google's predictive text algorithm. I kind of wonder if you're slightly shaping the translations
[00:30:08.560 --> 00:30:13.360]   with your predictions, even if the translator is kind of getting involved and sort of making it
[00:30:13.360 --> 00:30:20.880]   match. Oh, yes. This is called priming. So, it's a common feature of psychological research. And so,
[00:30:20.880 --> 00:30:24.400]   one of the things that we showed in grad school was when you show somebody a suggestion, they
[00:30:24.400 --> 00:30:29.440]   tend to generate a final answer that's closer to the suggestion than if they start from scratch.
[00:30:29.440 --> 00:30:34.800]   So, I guess there's some... I mean, I guess maybe it's better that I write slightly more politely.
[00:30:34.800 --> 00:30:37.520]   I don't know. Maybe there's some good that you could do with that.
[00:30:37.520 --> 00:30:41.600]   It's pulling your writing down to mean behavior. So, a mean level of performance. So,
[00:30:41.600 --> 00:30:44.400]   I'm not sure if that's great or not. Pulling down or pulling up. I don't know.
[00:30:44.400 --> 00:30:48.720]   Yeah. Or maybe it's pulling you up to a mean level of performance, right?
[00:30:48.720 --> 00:30:54.400]   Do you think that the translators kind of learn to use your system as well? Do you see
[00:30:54.400 --> 00:30:57.840]   productivity going up for an individual that's doing this?
[00:30:57.840 --> 00:31:04.320]   Yeah. We have an HCI team and this is one of the main things that they're working on right now,
[00:31:04.320 --> 00:31:13.920]   which is... I think... Yeah, I think... I remember right when we started the company,
[00:31:13.920 --> 00:31:19.920]   one of my co-advisors, Jeff Hare, who started Trifacta, I was telling him... This was really
[00:31:19.920 --> 00:31:24.240]   early on and I was showing him some of the stuff we were building and we want to optimize this and
[00:31:24.240 --> 00:31:28.400]   we want to do that. And he said, "Let me stop you right there. In the early days of a company,
[00:31:28.400 --> 00:31:34.720]   you're just trying to make things less horrible than they are." And you're going to be in that
[00:31:34.720 --> 00:31:41.040]   phase for a long time before you get to the optimization phase. So, I think for a lot of
[00:31:41.040 --> 00:31:47.680]   the last number of years, it was catching up on neural MT, making the system faster, multi-region,
[00:31:47.680 --> 00:31:52.240]   making the system more responsive in the browser. And there was just a lot of
[00:31:52.240 --> 00:31:58.720]   unbreaking work that was going on. And now, we've got some pretty convincing results that
[00:31:58.720 --> 00:32:05.040]   the highest... The thing that we really ought to focus on is how people use the system.
[00:32:07.040 --> 00:32:12.560]   The greatest predictive variable of performance is just the individual's identity. And so,
[00:32:12.560 --> 00:32:17.280]   when we look at how people use it, there's really high variance in the degree to which they utilize
[00:32:17.280 --> 00:32:22.960]   the suggestions, how they use the different input devices on the keyboard, how they navigate and
[00:32:22.960 --> 00:32:27.280]   work through a document. So, the team's spending quite a bit of time on user training right now,
[00:32:27.280 --> 00:32:31.280]   actually. So, user training, not modifying the interface, but actually training people to use?
[00:32:31.280 --> 00:32:35.760]   User training, yeah. Interesting. Have you ever considered doing multiple
[00:32:36.320 --> 00:32:43.600]   suggestions? Is that possibly better? Yeah. One of the reasons that this predictive
[00:32:43.600 --> 00:32:51.280]   approach to MT didn't work really well is because the interfaces that were built up until our work,
[00:32:51.280 --> 00:32:56.960]   they use a dropdown box. And it turns out when you put stuff on the screen, people read it,
[00:32:56.960 --> 00:33:03.280]   which slows them down. So, what you want to do is show them the one best prediction that's the
[00:33:03.280 --> 00:33:08.560]   very best prediction you can show them. I see. Interesting. I bet that's especially
[00:33:08.560 --> 00:33:11.680]   true when you're confident in your predictions. Yeah.
[00:33:11.680 --> 00:33:17.280]   Cool. Is there many other surprises in terms of your interfacing with humans? I feel like my last
[00:33:17.280 --> 00:33:21.920]   company was a labeling company. I just had all these interesting ways that the interaction between
[00:33:21.920 --> 00:33:28.800]   humans and machines surprised me. Has the way that you engage with the human changed at all over the
[00:33:28.800 --> 00:33:34.480]   years that you're running this besides training? Maybe one of the biggest things that we learned
[00:33:34.480 --> 00:33:42.080]   is that historically within translation, in this translation world, I mentioned this MT work goes
[00:33:42.080 --> 00:33:47.280]   back to the '50s. So, in professional translation is a, I don't know, predates agriculture or
[00:33:47.280 --> 00:33:51.120]   something. That's really an old profession, right? Sure.
[00:33:51.120 --> 00:33:57.840]   So, these people have been engaged with AI systems for 50 years. And for most of that period of time,
[00:33:57.840 --> 00:34:07.040]   the systems are really bad. So, there's a lot of bias against these systems. And people,
[00:34:07.040 --> 00:34:12.480]   especially those who use them for a while when they weren't really good, they were reluctant to
[00:34:12.480 --> 00:34:17.280]   try them. I think more broadly now people are using them because MT is a lot better. But
[00:34:17.280 --> 00:34:23.440]   we found that resistance to change was really significant. And the way to get around that was to
[00:34:24.560 --> 00:34:29.120]   align incentives better with the business model, which, okay, what do people actually want more
[00:34:29.120 --> 00:34:35.360]   than they want to not embrace machine learning? Well, they want to get paid. They want to be
[00:34:35.360 --> 00:34:39.360]   recognized for their work. They want to be appreciated. They want to have a good work
[00:34:39.360 --> 00:34:46.160]   environment and work with good people. And so, I think we found that focusing on those things,
[00:34:46.160 --> 00:34:52.800]   when you do those right, then people are really open to, let me try this automation.
[00:34:53.520 --> 00:34:59.680]   Let me... I'm okay with the fact that you're changing the interface every week and all that
[00:34:59.680 --> 00:35:09.280]   stuff. Yeah. That makes sense. Yeah. Is there a feedback loop with the ratings? I think that
[00:35:09.280 --> 00:35:13.040]   might be an important thing too, if you're then rating the quality of the translation.
[00:35:13.040 --> 00:35:17.600]   Yes. So, this is... We have, I believe, we just submitted a paper to EMNLP. Hopefully,
[00:35:17.600 --> 00:35:22.160]   it'll get in. And we've been working on a bilingual grammatical error correction.
[00:35:22.160 --> 00:35:28.000]   So, what the reviewers do, you can think of as another review step. So, we took an English input,
[00:35:28.000 --> 00:35:33.440]   we generated some French, maybe there's some bugs in the French, and we give that to another person
[00:35:33.440 --> 00:35:37.840]   who then is going to find and fix those bugs, or maybe they make some stylistic changes,
[00:35:37.840 --> 00:35:42.800]   or who knows what they do. So, that just becomes another prediction problem with two inputs,
[00:35:42.800 --> 00:35:49.600]   the English and the un-corrected, unverified French, or whatever you want to call it.
[00:35:49.600 --> 00:35:55.200]   And they're going to predict the verified French. And so, you can use a sequence prediction
[00:35:55.200 --> 00:36:00.560]   architecture model for that, or you can use sequence modeling for that. And so,
[00:36:00.560 --> 00:36:06.080]   the team has been working on that for about the past year and a half, and they've got it working
[00:36:06.080 --> 00:36:11.920]   now. And we announced that last fall, and we'll have it in production, I think, sometime in the
[00:36:11.920 --> 00:36:16.880]   second half of the year. Wow. That's so cool. And I guess in production, what would that mean?
[00:36:16.880 --> 00:36:22.000]   Like once you finish editing a document, it sort of goes through and makes suggestions?
[00:36:22.000 --> 00:36:29.680]   Yeah. It's a fancy grammar checker, only it's a grammar checker that's data-oriented instead of
[00:36:29.680 --> 00:36:36.960]   based on rules. And it can learn simple phenomenon like spelling mistakes, but it can also learn
[00:36:36.960 --> 00:36:41.760]   stylistic edits. Well, it sounds like it's also incorporating the source language too, right?
[00:36:41.760 --> 00:36:47.120]   Yeah. So, that's how it's different than a Grammarly or the grammar checker that you have
[00:36:47.120 --> 00:36:53.440]   in Google Docs or whatever. And that instead of... You only have one language to look at,
[00:36:53.440 --> 00:36:58.720]   the string that you're generating is constrained by this other source language input. So, you can't
[00:36:58.720 --> 00:37:04.320]   just generate anything. You've got this sort of very strict constraint, which is the source
[00:37:04.320 --> 00:37:10.960]   language. And do you plan to do a separate one for every single document stream or workstream
[00:37:10.960 --> 00:37:12.800]   that you have? Yes.
[00:37:12.800 --> 00:37:16.320]   You can use the same infrastructure for that that you use for the translation.
[00:37:16.320 --> 00:37:21.040]   That's so cool. Well, cool. So, we always end with two questions that I want to give you a little
[00:37:21.040 --> 00:37:25.440]   time to chew on these. I guess one is kind of open-ended, but I'd be interested in your
[00:37:25.440 --> 00:37:31.360]   thoughts in MT specifically is, what's an underrated aspect of machine learning or
[00:37:31.360 --> 00:37:37.120]   machine translation that you think people should pay more attention to or that you'd be thinking
[00:37:37.120 --> 00:37:43.200]   about if you weren't working on Lilt? Maybe it's around the question that you
[00:37:43.200 --> 00:37:49.200]   posed earlier, which is this sort of human parody question with translation, which there was a paper,
[00:37:49.200 --> 00:37:53.600]   I don't know, two years ago, Microsoft had a paper saying, "Human parody has been achieved."
[00:37:53.600 --> 00:37:58.400]   And then two weeks ago, Google published a paper on archive saying, "Human parody has not been
[00:37:58.400 --> 00:38:09.440]   achieved." And I think that in our application, there's a lot to translation quality, which is
[00:38:09.440 --> 00:38:14.960]   the particular message that you're trying to deliver to an audience, which a lot has to do
[00:38:14.960 --> 00:38:21.360]   with how the audience feels. And certainly in my time in grad school, I was really focused on just
[00:38:22.240 --> 00:38:28.720]   generating the output that matches the reference. So the blue score goes up and I can write a paper.
[00:38:28.720 --> 00:38:34.400]   And I think there's a lot of interesting work to think about broader pragmatic context of the
[00:38:34.400 --> 00:38:39.600]   language that's generated and is it appropriate for the context that you're in and for the domain.
[00:38:39.600 --> 00:38:45.200]   And that's really hard to evaluate, but it's really worth thinking about whether it's in
[00:38:45.200 --> 00:38:50.160]   natural language generation or machine translation or whatever else.
[00:38:50.160 --> 00:38:54.880]   So I think maybe thinking about that a little bit harder, I would spend some time on it.
[00:38:54.880 --> 00:39:00.720]   Yeah, the blue score is funny because it seems like such a sad metric for translation. It makes
[00:39:00.720 --> 00:39:07.680]   sense that it works, but it just seems so ludicrously simple. At some point, I feel like
[00:39:07.680 --> 00:39:11.040]   it must lose meaning as the best possible metric, right?
[00:39:11.040 --> 00:39:16.480]   Well, people studied it a lot. And I think the conclusion was that it's the least bad
[00:39:16.480 --> 00:39:22.160]   thing that we've come up with. And over two decades of study, it continued to be the least,
[00:39:22.160 --> 00:39:27.600]   nobody could come up with anything that was as convenient and correlated better with human
[00:39:27.600 --> 00:39:33.840]   judgment. So maybe it's a testament to a simple idea that people are still using 20 years later.
[00:39:33.840 --> 00:39:39.280]   I guess simple metrics are better than complicated metrics. There might be a lesson for business.
[00:39:39.280 --> 00:39:40.800]   There might be a lesson there too. Yeah.
[00:39:40.800 --> 00:39:44.640]   And I guess the final question, we always ask, what's the biggest challenge of
[00:39:44.640 --> 00:39:48.160]   machine learning in the real world? But I'd like to tailor it to you a bit of just like,
[00:39:48.160 --> 00:39:53.520]   what's been the hardest part of getting these language models to work in production? You
[00:39:53.520 --> 00:39:57.760]   touched on it a bit, but I'd love to hear, especially any part that might be surprising to
[00:39:57.760 --> 00:40:03.200]   yourself as an academic before starting the company. Where have the challenges been?
[00:40:03.200 --> 00:40:09.120]   Yeah. If I think back to when we started the company, the research prototype that we had,
[00:40:10.240 --> 00:40:16.720]   you could translate, you had to specialize it to one document. So if you're going to translate a
[00:40:16.720 --> 00:40:22.480]   document, you had to compile this part of it and then load it into a production system. And you
[00:40:22.480 --> 00:40:26.320]   could send it the document and it would translate it. And if you send it anything else, it basically
[00:40:26.320 --> 00:40:32.400]   wouldn't work. And I remember when we raised money for the company, I told the investors,
[00:40:32.400 --> 00:40:35.680]   I was like, yeah, we're going to take this prototype and have a production product in
[00:40:35.680 --> 00:40:40.400]   like six weeks or something. And what actually happened is it took us nine months and the
[00:40:40.400 --> 00:40:45.440]   problems we had to solve turned into an ACL paper. And this is, you should not do this.
[00:40:45.440 --> 00:40:53.200]   This is very bad. And I think I really underestimated how far it is from
[00:40:53.200 --> 00:41:01.040]   kind of a research prototype. It's actually a pretty effective system to an MVP for something
[00:41:01.040 --> 00:41:06.000]   like what we do, which is taking any document from any company and generating a reasonable output
[00:41:06.000 --> 00:41:09.360]   and doing that with the learning turned on and the inference and all that stuff.
[00:41:09.360 --> 00:41:16.880]   Getting to a large scale production system, which is probably not surprising to anybody who's worked
[00:41:16.880 --> 00:41:23.120]   in these production scale MT systems, but the amount of detailed large scale engineering work
[00:41:23.120 --> 00:41:29.200]   that has to go into that was surprising to us, I think, even having worked on Google Translate.
[00:41:29.840 --> 00:41:34.320]   Well, can you give an example? What was something you ran into? Because it does seem like
[00:41:34.320 --> 00:41:38.800]   that shouldn't take nine months. What went wrong?
[00:41:38.800 --> 00:41:48.480]   Well, in those days in that original system, you had to be able to load the entire bytext
[00:41:48.480 --> 00:41:54.880]   into memory. So the systems stored words as atomic strings, and you had to have all the
[00:41:54.880 --> 00:41:58.480]   strings in memory to be able to generate a translation. So we did a lot of work on
[00:41:58.480 --> 00:42:02.480]   what's called a compact translation model, where you can load the entire bytext into a running
[00:42:02.480 --> 00:42:07.440]   production node, and the lookups happen fast enough that you can generate an output.
[00:42:07.440 --> 00:42:14.320]   I think in the neural setting, what's been really challenging is you can't do batching.
[00:42:14.320 --> 00:42:22.080]   You can't just put it on a GPU or a TPU because the latency constraint that you have. So that's
[00:42:22.080 --> 00:42:28.000]   meant a lot of work on CPU inference, on the way the production infrastructure swaps personalized
[00:42:28.000 --> 00:42:35.920]   models onto and off of the production nodes. It seems conceptually really simple, but when you
[00:42:35.920 --> 00:42:40.240]   actually get down into it, you're like, "Wow, we've been at this for two months and we're still
[00:42:40.240 --> 00:42:44.560]   not quite there yet. What's happening?" That's been our experience, I think.
[00:42:44.560 --> 00:42:48.640]   Interesting. I guess at the time, there was probably a lot less stuff to help you.
[00:42:48.640 --> 00:42:55.360]   Yeah, there was no Kubernetes. There was none of that type of infrastructure.
[00:42:56.160 --> 00:42:59.920]   Awesome. Well, thanks so much. This is really fun. And thanks for sharing so much about how
[00:42:59.920 --> 00:43:03.920]   your company operates. Yeah, it's always good to chat with you.
[00:43:03.920 --> 00:43:09.520]   If you're enjoying Gradient Descent, I'd really love for you to check out Fully Connected,
[00:43:09.520 --> 00:43:14.960]   which is an inclusive machine learning community that we're building to let everyone know about
[00:43:14.960 --> 00:43:22.000]   all the stuff going on in ML and all the new research coming out. If you go to wmv.ai/fc,
[00:43:22.000 --> 00:43:25.680]   you can see all the different stuff that we do, including Gradient Descent, but also
[00:43:25.680 --> 00:43:31.120]   salons where we talk about new research and folks share insights, AMAs where you can directly
[00:43:31.120 --> 00:43:36.080]   connect with members of our community, and a Slack channel where you can get answers to
[00:43:36.080 --> 00:43:41.440]   everything from very basic questions about ML to bug reports on weights and biases to
[00:43:41.440 --> 00:43:45.600]   how to hire an ML team. We're looking forward to meeting you.

