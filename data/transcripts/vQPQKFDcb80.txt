
[00:00:00.000 --> 00:00:04.600]   - Hey, well, good evening, good afternoon, everyone.
[00:00:04.600 --> 00:00:06.200]   I think there are so many different people,
[00:00:06.200 --> 00:00:07.440]   so many different places.
[00:00:07.440 --> 00:00:10.520]   So hello to every one of you.
[00:00:10.520 --> 00:00:13.120]   It's very exciting to speak about
[00:00:13.120 --> 00:00:16.120]   and talk about privacy preserving machine learning
[00:00:16.120 --> 00:00:20.440]   in general and more specifically, deep learning.
[00:00:20.440 --> 00:00:23.960]   Today, I'll be talking about membership inference at tax
[00:00:23.960 --> 00:00:26.320]   as just one simple example of
[00:00:28.440 --> 00:00:32.120]   how surprisingly easy it is to attack
[00:00:32.120 --> 00:00:34.200]   even a well-trained deep learning model
[00:00:34.200 --> 00:00:37.120]   that most of those that are shared nowadays
[00:00:37.120 --> 00:00:39.640]   and even very critical,
[00:00:39.640 --> 00:00:43.800]   some of them deployment and production systems.
[00:00:43.800 --> 00:00:45.960]   Now, the reason for this,
[00:00:45.960 --> 00:00:48.520]   and I believe that so many other people also,
[00:00:48.520 --> 00:00:50.360]   I don't wanna speak for myself something big,
[00:00:50.360 --> 00:00:53.820]   but people have predicted that 2020 will be the year
[00:00:53.820 --> 00:00:56.240]   of privacy preserving machine learning.
[00:00:56.240 --> 00:01:00.960]   And this was even long before COVID-19 happened.
[00:01:00.960 --> 00:01:02.920]   And we saw that early this year
[00:01:02.920 --> 00:01:06.560]   when this pandemic hit the globe,
[00:01:06.560 --> 00:01:08.240]   that technology tried to jump in
[00:01:08.240 --> 00:01:09.760]   and tried to find solutions.
[00:01:09.760 --> 00:01:12.120]   One of the specific solutions that has been
[00:01:12.120 --> 00:01:17.400]   showing some promising results is the tracking
[00:01:17.400 --> 00:01:19.560]   of people who might get infected
[00:01:19.560 --> 00:01:22.880]   and the interactions between those people.
[00:01:22.880 --> 00:01:26.320]   However, it was not easy to implement at all.
[00:01:26.320 --> 00:01:28.600]   And it was not easy to be adopted
[00:01:28.600 --> 00:01:30.040]   in so many different countries
[00:01:30.040 --> 00:01:33.320]   and so many even to publish them on the stores
[00:01:33.320 --> 00:01:36.480]   because of the privacy preserving issues.
[00:01:36.480 --> 00:01:39.520]   We want to have something that is really good,
[00:01:39.520 --> 00:01:41.640]   but at the same time, we also want to make sure
[00:01:41.640 --> 00:01:45.600]   that we are not violating the privacy of people
[00:01:45.600 --> 00:01:48.040]   and the privacy of users.
[00:01:48.040 --> 00:01:50.840]   And privacy and security is not a new topic,
[00:01:50.840 --> 00:01:55.480]   but it has a whole new, different perspective
[00:01:55.480 --> 00:01:59.440]   and aspects when it comes to artificial intelligence
[00:01:59.440 --> 00:02:00.600]   and machine learning.
[00:02:00.600 --> 00:02:03.680]   And this is because by definition,
[00:02:03.680 --> 00:02:06.520]   the development, before speaking about privacy,
[00:02:06.520 --> 00:02:08.960]   the development of a machine learning model
[00:02:08.960 --> 00:02:12.320]   or an AI or deep learning algorithm
[00:02:12.320 --> 00:02:15.960]   is very different from conventional software engineering.
[00:02:15.960 --> 00:02:18.920]   First of all, some of the differences include that
[00:02:19.920 --> 00:02:24.240]   when we are developing a software product, a program,
[00:02:24.240 --> 00:02:26.360]   we have a very specific goal.
[00:02:26.360 --> 00:02:29.640]   And the goal there is to meet a set of
[00:02:29.640 --> 00:02:32.240]   well-defined functional requirements.
[00:02:32.240 --> 00:02:33.880]   Functional requirement means if I click
[00:02:33.880 --> 00:02:36.800]   on a specific button, something will happen, right?
[00:02:36.800 --> 00:02:40.040]   And we also have some non-functional requirements
[00:02:40.040 --> 00:02:43.040]   that includes the security and the quality.
[00:02:43.040 --> 00:02:46.280]   And because traditional software engineering
[00:02:46.280 --> 00:02:49.840]   is written by humans, the quality of this software
[00:02:49.840 --> 00:02:53.720]   heavily rely on the quality of the source code itself,
[00:02:53.720 --> 00:02:56.960]   which ultimately means the experience
[00:02:56.960 --> 00:03:00.200]   of the software developer have written that source code.
[00:03:00.200 --> 00:03:06.200]   And therefore, once you have a good,
[00:03:06.200 --> 00:03:11.240]   well-written, well-tested software system shipped,
[00:03:11.240 --> 00:03:16.560]   that software system will continue to operate as expected
[00:03:16.560 --> 00:03:18.680]   unless you change something with it.
[00:03:18.680 --> 00:03:21.160]   But this story is completely different
[00:03:21.160 --> 00:03:22.520]   when it comes to machine learning.
[00:03:22.520 --> 00:03:27.160]   For one, the goal here is to optimize a specific metric.
[00:03:27.160 --> 00:03:29.880]   That's often the accuracy you want to,
[00:03:29.880 --> 00:03:35.200]   maximize your accuracy, you want to minimize your loss
[00:03:35.200 --> 00:03:37.160]   of some type of predictive model.
[00:03:37.160 --> 00:03:41.000]   And therefore, the quality of this produced model
[00:03:41.000 --> 00:03:44.440]   is not only affected by the quality of the source code,
[00:03:44.440 --> 00:03:47.080]   rather it's heavily affected by what?
[00:03:47.080 --> 00:03:50.360]   By the data we are using to generate such models.
[00:03:50.360 --> 00:03:53.920]   Because by definition, machine learning is defined
[00:03:53.920 --> 00:03:57.040]   as one of the best definitions.
[00:03:57.040 --> 00:03:59.320]   Machine learning is defined as programs
[00:03:59.320 --> 00:04:01.780]   that generalize from data.
[00:04:01.780 --> 00:04:03.280]   So if we look from very big picture,
[00:04:03.280 --> 00:04:05.640]   what we are trying to do is to solve a problem
[00:04:05.640 --> 00:04:09.700]   using some automated approach without explicitly programming,
[00:04:09.700 --> 00:04:12.660]   without explicitly having to write code
[00:04:12.660 --> 00:04:15.520]   for that solution or that program.
[00:04:15.520 --> 00:04:18.000]   So machine learning are programs that generalize from data
[00:04:18.000 --> 00:04:20.600]   and therefore they heavily rely on the data
[00:04:20.600 --> 00:04:24.920]   and all those hyper parameters that take place
[00:04:24.920 --> 00:04:27.040]   in generating the specific model.
[00:04:27.040 --> 00:04:29.860]   At the end of the day, deep learning is still
[00:04:29.860 --> 00:04:34.240]   an iterative search problem that goes through hundreds
[00:04:34.240 --> 00:04:36.600]   to sometimes thousands of iterations
[00:04:36.600 --> 00:04:39.720]   before generating a deep learning model.
[00:04:39.720 --> 00:04:43.680]   Not only it is an iterative and it's time consuming
[00:04:43.680 --> 00:04:46.160]   and it takes so many different cycles
[00:04:46.160 --> 00:04:51.160]   and training iterations before you reach at a good model,
[00:04:51.160 --> 00:04:56.240]   but in reality, there are also different people involved
[00:04:56.240 --> 00:04:57.960]   in creating a deep learning model
[00:04:57.960 --> 00:04:59.280]   or machine learning model.
[00:04:59.280 --> 00:05:03.040]   So you have data scientists that use a wide range of tools
[00:05:03.040 --> 00:05:06.840]   to collect the data and we just seen in the previous talk
[00:05:06.840 --> 00:05:10.180]   that data collection and cleaning is one of the important,
[00:05:10.180 --> 00:05:13.380]   this is true across all machine learning projects.
[00:05:13.380 --> 00:05:16.120]   So there are different people involved here,
[00:05:16.120 --> 00:05:17.800]   different tools involved here.
[00:05:17.800 --> 00:05:22.560]   So what we see here is a typical development lifecycle
[00:05:22.560 --> 00:05:24.660]   of a machine learning model.
[00:05:24.660 --> 00:05:27.560]   So you usually have an idea in mind, you want to build it,
[00:05:27.560 --> 00:05:29.560]   you need to have an access to a dataset.
[00:05:29.560 --> 00:05:32.060]   Usually as Boris said,
[00:05:32.060 --> 00:05:33.680]   you want to have some reference model.
[00:05:33.680 --> 00:05:37.200]   You don't wanna create models from scratch, right?
[00:05:37.200 --> 00:05:39.280]   So we start, we create a model.
[00:05:39.280 --> 00:05:42.040]   Usually I'm a PyTorch fan.
[00:05:42.040 --> 00:05:43.780]   So whether you are a PyTorch fan
[00:05:43.780 --> 00:05:46.560]   or a TensorFlow or a Keras,
[00:05:46.560 --> 00:05:48.300]   you start by building a model, you train it,
[00:05:48.300 --> 00:05:51.780]   you evaluate it and that's where we really go through
[00:05:51.780 --> 00:05:54.140]   so many different numbers of iterations
[00:05:54.140 --> 00:05:56.940]   until we reach a very good result.
[00:05:56.940 --> 00:06:00.860]   So that's usually done by a software data engineer
[00:06:00.860 --> 00:06:02.900]   or a machine learning engineer.
[00:06:02.900 --> 00:06:06.560]   Once a machine learning model is created,
[00:06:06.560 --> 00:06:08.220]   the task is not done.
[00:06:08.220 --> 00:06:11.980]   You either need to deploy it or expose it through some API
[00:06:11.980 --> 00:06:14.100]   and even when you do that,
[00:06:14.100 --> 00:06:16.500]   you still need to keep monitoring that model
[00:06:16.500 --> 00:06:18.340]   to see if there are new instances
[00:06:18.340 --> 00:06:22.140]   that your model fails to detect or to do its task on.
[00:06:22.140 --> 00:06:23.800]   So you retrain that model.
[00:06:23.800 --> 00:06:27.500]   So the moral of this long story here
[00:06:27.500 --> 00:06:29.380]   is that there are different applications,
[00:06:29.380 --> 00:06:31.980]   different people, different experiences.
[00:06:31.980 --> 00:06:34.220]   We usually even experience with different,
[00:06:34.220 --> 00:06:39.020]   with the, sorry, with different libraries and platforms
[00:06:39.020 --> 00:06:41.780]   to generate some good model.
[00:06:41.780 --> 00:06:45.740]   And that makes the models really be exposed
[00:06:45.740 --> 00:06:47.380]   to different types of attacks
[00:06:47.380 --> 00:06:51.920]   than conventional software services or software systems.
[00:06:51.920 --> 00:06:56.420]   Some of the privacy pinpoints that might
[00:06:56.420 --> 00:06:58.900]   hinder the overall
[00:06:58.900 --> 00:07:04.000]   application of the model
[00:07:04.000 --> 00:07:07.660]   actually happen in places where we have,
[00:07:07.660 --> 00:07:09.900]   where we can interact with the model.
[00:07:09.900 --> 00:07:12.860]   And therefore the attacks on a deep learning model
[00:07:12.860 --> 00:07:15.660]   could actually be categorized based on
[00:07:15.660 --> 00:07:19.180]   how much access do you have to that model?
[00:07:19.180 --> 00:07:21.020]   So in a very big general picture,
[00:07:21.020 --> 00:07:23.500]   the attacks on deep learning models could be,
[00:07:23.500 --> 00:07:26.820]   they are categorized in two general classes,
[00:07:26.820 --> 00:07:28.660]   a white box attack, a black box attack.
[00:07:28.660 --> 00:07:32.740]   The white box attacks means you usually are an insider
[00:07:32.740 --> 00:07:34.940]   or you have the access to the model.
[00:07:34.940 --> 00:07:36.300]   You know the architecture of the model,
[00:07:36.300 --> 00:07:38.540]   you know the parameters of the model.
[00:07:38.540 --> 00:07:41.380]   A black box attack is usually,
[00:07:41.380 --> 00:07:42.760]   you have no access to the model,
[00:07:42.760 --> 00:07:44.580]   you can only query the model.
[00:07:44.580 --> 00:07:47.740]   So run inferences, you submit a data record to the model,
[00:07:47.740 --> 00:07:50.020]   you receive the output of that model.
[00:07:50.020 --> 00:07:55.020]   Something like a model that is exposed on Google AutoML,
[00:07:55.020 --> 00:07:57.900]   where it's just an API, you submit an input,
[00:07:57.900 --> 00:07:58.960]   you receive an output.
[00:07:58.960 --> 00:08:03.620]   And therefore based on the level of interaction or access
[00:08:03.620 --> 00:08:05.580]   that you have to a specific model,
[00:08:05.580 --> 00:08:07.920]   privacy could be hindered or could be,
[00:08:07.920 --> 00:08:12.900]   it could be violated in different places
[00:08:12.900 --> 00:08:14.240]   and in different ways.
[00:08:14.240 --> 00:08:16.540]   So the first way of course is that
[00:08:16.540 --> 00:08:18.020]   when we train a deep learning model,
[00:08:18.020 --> 00:08:20.960]   we need to make sure that the data we are using
[00:08:20.960 --> 00:08:25.260]   to train that model is kept in good hands,
[00:08:25.260 --> 00:08:29.060]   it's kept private, and that people who are participating
[00:08:29.060 --> 00:08:31.840]   in this study or this statistical analysis
[00:08:31.840 --> 00:08:34.100]   or this deep learning model,
[00:08:34.100 --> 00:08:36.300]   their information is not being violated
[00:08:36.300 --> 00:08:38.700]   and it's not being used for some other
[00:08:38.700 --> 00:08:41.120]   malicious activities or tasks.
[00:08:41.120 --> 00:08:44.120]   Some of the simple, not simple,
[00:08:44.120 --> 00:08:47.460]   but some of the common attacks to do here on the datasets,
[00:08:47.460 --> 00:08:50.700]   specifically those that are available publicly
[00:08:50.700 --> 00:08:55.700]   is to modify such data, maybe change the labels
[00:08:55.700 --> 00:08:58.580]   and make the true labels become a wrong label,
[00:08:58.580 --> 00:09:03.580]   add some noises or inject some wrongly classified datasets
[00:09:03.780 --> 00:09:04.820]   and things like that.
[00:09:04.820 --> 00:09:08.200]   Also if you are a hospital,
[00:09:08.200 --> 00:09:11.300]   if you are a hospital and you have a dataset
[00:09:11.300 --> 00:09:13.740]   and then somebody comes from academia or industry
[00:09:13.740 --> 00:09:15.620]   and they say, hey, we have this great idea,
[00:09:15.620 --> 00:09:18.100]   we need to have access to your dataset.
[00:09:18.100 --> 00:09:20.980]   Hospitals cannot just share their datasets,
[00:09:20.980 --> 00:09:23.820]   there are regularizations and policies
[00:09:23.820 --> 00:09:25.820]   such as HIPAA in the United States
[00:09:25.820 --> 00:09:28.300]   and some other European unions and Middle East,
[00:09:28.300 --> 00:09:31.620]   there are regulations that prevent sharing
[00:09:31.620 --> 00:09:34.060]   of very sensitive data of people.
[00:09:34.060 --> 00:09:37.900]   And therefore, privacy could happen to the training dataset.
[00:09:37.900 --> 00:09:41.740]   Sometimes we have access to, there exists lots of data,
[00:09:41.740 --> 00:09:43.540]   but we don't have access to it.
[00:09:43.540 --> 00:09:46.420]   Other places where we can also attack
[00:09:46.420 --> 00:09:48.260]   the overall development lifecycle,
[00:09:48.260 --> 00:09:52.300]   if we are an insider, this is,
[00:09:52.300 --> 00:09:55.540]   I hope that people who are developing
[00:09:55.540 --> 00:10:00.020]   machine learning models, they are good enough
[00:10:00.020 --> 00:10:03.020]   that they are not going to poison the models
[00:10:03.020 --> 00:10:04.820]   that they are creating for their industry
[00:10:04.820 --> 00:10:09.020]   or for their academic or research usage
[00:10:09.020 --> 00:10:11.780]   or future usage or applications.
[00:10:11.780 --> 00:10:13.820]   But people can actually, malicious people,
[00:10:13.820 --> 00:10:17.260]   malicious users can actually build some backdoors
[00:10:17.260 --> 00:10:21.300]   into their models that make it easier to facilitate
[00:10:21.300 --> 00:10:24.040]   attacking such models in the future.
[00:10:24.040 --> 00:10:27.260]   And this is also correct and true
[00:10:27.260 --> 00:10:29.620]   in the case of federated learning.
[00:10:29.620 --> 00:10:32.260]   And if you're not aware of federated learning,
[00:10:32.260 --> 00:10:37.020]   simply speaking, it's an approach that was created
[00:10:37.020 --> 00:10:41.660]   a couple of years ago, and basically federated learning,
[00:10:41.660 --> 00:10:42.820]   the idea is very simple.
[00:10:42.820 --> 00:10:45.780]   So if we have a hospital and you have an algorithm provider
[00:10:45.780 --> 00:10:48.220]   or somebody who wants to create a deep learning model
[00:10:48.220 --> 00:10:50.580]   using data that resides on the hospital side,
[00:10:50.580 --> 00:10:55.940]   traditionally, we would require the hospital
[00:10:55.940 --> 00:10:58.180]   to upload their data to the server
[00:10:58.180 --> 00:11:00.940]   or to the person who wants to build this algorithm.
[00:11:00.940 --> 00:11:02.240]   So you send the data to them,
[00:11:02.240 --> 00:11:04.020]   and then they build a deep learning model
[00:11:04.020 --> 00:11:05.700]   on top of this data.
[00:11:05.700 --> 00:11:08.580]   But to preserve the privacy of this data,
[00:11:08.580 --> 00:11:11.020]   and especially in the case of hospitals,
[00:11:11.020 --> 00:11:12.860]   we cannot simply share this data,
[00:11:12.860 --> 00:11:15.300]   and therefore, instead of sending the data,
[00:11:15.300 --> 00:11:16.660]   we send the model.
[00:11:16.660 --> 00:11:20.940]   But nowadays, federated learning is mostly used
[00:11:20.940 --> 00:11:24.940]   on our mobile phones, on Apple Watches,
[00:11:24.940 --> 00:11:27.700]   and user devices.
[00:11:27.700 --> 00:11:29.660]   So the process here, or the idea here,
[00:11:29.660 --> 00:11:32.380]   is that when you go every night to your bed,
[00:11:32.380 --> 00:11:37.380]   you put your Apple Watch or iPhone or Samsung device
[00:11:37.380 --> 00:11:40.940]   through a charger, it is charging, it is idle,
[00:11:40.940 --> 00:11:44.180]   it is connected to the internet.
[00:11:44.180 --> 00:11:46.460]   Google at that point, or Apple,
[00:11:46.460 --> 00:11:49.740]   is training a small deep learning model
[00:11:49.740 --> 00:11:52.660]   to predict what you are going to type next.
[00:11:52.660 --> 00:11:55.580]   So auto-correction, Siri, Alexa,
[00:11:55.580 --> 00:11:59.100]   and all of those devices, wearable devices as well,
[00:11:59.100 --> 00:12:00.820]   that we are using all around the house,
[00:12:00.820 --> 00:12:04.080]   they are training every night on our machines,
[00:12:04.080 --> 00:12:06.500]   so that we don't have to share our private data
[00:12:06.500 --> 00:12:08.260]   with the providers, with the servers,
[00:12:08.260 --> 00:12:09.700]   to train these models.
[00:12:09.700 --> 00:12:12.580]   So they train small models on our devices,
[00:12:12.580 --> 00:12:14.740]   and those small models are then sent to the server
[00:12:14.740 --> 00:12:16.740]   where they all get aggregated.
[00:12:16.740 --> 00:12:19.260]   And then a smarter version will be released,
[00:12:19.260 --> 00:12:21.340]   of that model will be released in the next update,
[00:12:21.340 --> 00:12:23.980]   or the next version, and so on and so forth.
[00:12:23.980 --> 00:12:27.460]   So if you are participating in a federated learning,
[00:12:27.460 --> 00:12:31.260]   you can also generate some backdoors in your models,
[00:12:31.260 --> 00:12:32.660]   or you can poison your models,
[00:12:32.660 --> 00:12:35.340]   or you can do some tricks that will allow you
[00:12:35.340 --> 00:12:38.960]   to watch, to attack, and to expose or retrieve
[00:12:38.960 --> 00:12:41.460]   sensitive information from the global models
[00:12:41.460 --> 00:12:43.900]   that are being built on the server side.
[00:12:43.900 --> 00:12:46.720]   The other type of attacks that,
[00:12:46.720 --> 00:12:49.220]   one of the most common attacks actually nowadays
[00:12:49.220 --> 00:12:52.380]   is an attack that happens on a model
[00:12:52.380 --> 00:12:54.940]   that has already been trained and deployed
[00:12:54.940 --> 00:12:57.620]   in a production system, or it's exposed
[00:12:57.620 --> 00:13:01.940]   using some REST API or just an API online.
[00:13:01.940 --> 00:13:04.900]   Some of those attacks are adversarial attacks,
[00:13:04.900 --> 00:13:06.500]   and I believe one of the talks today
[00:13:06.500 --> 00:13:10.460]   will be talking about generating adversarial texts,
[00:13:10.460 --> 00:13:11.940]   I believe, something like that.
[00:13:11.940 --> 00:13:16.380]   The main idea here is that in an adversarial attack,
[00:13:16.380 --> 00:13:21.380]   you create an input to a model that will fool the model,
[00:13:21.500 --> 00:13:24.780]   will make the model produce a wrong result.
[00:13:24.780 --> 00:13:26.180]   I have read a couple of weeks,
[00:13:26.180 --> 00:13:29.200]   or maybe one month ago actually,
[00:13:29.200 --> 00:13:34.200]   about researchers from McAfee, the security company,
[00:13:34.200 --> 00:13:39.580]   they fooled a Tesla car to see the 35 miles per hour
[00:13:39.580 --> 00:13:44.020]   speed limit as an 85 miles per hour speed limit.
[00:13:44.020 --> 00:13:48.620]   So basically, they do not have access to the model,
[00:13:48.620 --> 00:13:50.100]   and to the computer vision model
[00:13:50.100 --> 00:13:52.540]   that looks at the speed limits in the Tesla car,
[00:13:52.540 --> 00:13:57.540]   but they were able to handcraft a speed limit
[00:13:57.540 --> 00:14:01.660]   that for us humans, it looks like 35 miles per hour,
[00:14:01.660 --> 00:14:02.980]   but for the deep learning model,
[00:14:02.980 --> 00:14:05.780]   it looks like an 85 miles per hour.
[00:14:05.780 --> 00:14:10.780]   And actually, if you see my web browser here,
[00:14:10.780 --> 00:14:12.220]   Ian Goodfellow,
[00:14:16.620 --> 00:14:20.060]   Ian Goodfellow, the head of AI at Google,
[00:14:20.060 --> 00:14:23.620]   he's one of the first people who created this,
[00:14:23.620 --> 00:14:25.840]   the idea of adversarial attacks.
[00:14:25.840 --> 00:14:29.360]   And this is a very famous example
[00:14:29.360 --> 00:14:33.580]   where we have a state of the art deep learning model
[00:14:33.580 --> 00:14:35.460]   that's able to classify this.
[00:14:35.460 --> 00:14:37.560]   I think this is not a very good picture,
[00:14:37.560 --> 00:14:42.420]   but it's able to classify this panda right here as a panda,
[00:14:42.420 --> 00:14:46.100]   with a confidence level of about 60%, not bad at all.
[00:14:46.100 --> 00:14:50.980]   Using this noise, we are able to handicraft this noise
[00:14:50.980 --> 00:14:53.180]   by having a white access to the model.
[00:14:53.180 --> 00:14:55.500]   What basically happening here is that we are trying
[00:14:55.500 --> 00:14:58.780]   to find the loss function with respect to the input,
[00:14:58.780 --> 00:15:01.460]   and then finding the sign the gradient of that,
[00:15:01.460 --> 00:15:04.340]   and that will generalize for us some noise.
[00:15:04.340 --> 00:15:06.980]   We want to find a noise that if we add it
[00:15:06.980 --> 00:15:08.860]   to this specific picture,
[00:15:08.860 --> 00:15:12.660]   the output will still be for us humans,
[00:15:12.660 --> 00:15:14.860]   an image of a panda.
[00:15:14.860 --> 00:15:17.740]   But this noise, because it's added at the pixel level,
[00:15:17.740 --> 00:15:20.540]   now the same state of the art model
[00:15:20.540 --> 00:15:23.660]   is classifying this input as a gibbon
[00:15:23.660 --> 00:15:27.880]   with almost 100% confidence to score.
[00:15:27.880 --> 00:15:31.560]   So this is where adversarial attacks are happening.
[00:15:31.560 --> 00:15:36.100]   Some of the interesting ones by the researchers,
[00:15:36.100 --> 00:15:41.100]   they tried to fool the YOLO deep learning models.
[00:15:41.100 --> 00:15:43.380]   So YOLO is one of the models that are capable
[00:15:43.380 --> 00:15:48.380]   to detect a human and label it as a person we see here.
[00:15:48.380 --> 00:15:52.420]   This person was able to generate this noise
[00:15:52.420 --> 00:15:55.300]   that will now fool this deep learning model
[00:15:55.300 --> 00:15:58.660]   to not recognizing this person as a person.
[00:15:58.660 --> 00:16:01.740]   And I have read that some smart entrepreneurs
[00:16:01.740 --> 00:16:05.420]   in Hong Kong and South Korea, I believe recently,
[00:16:05.420 --> 00:16:08.780]   they created fashion or clothes and face masks
[00:16:08.780 --> 00:16:12.100]   that have such type of noisy images
[00:16:12.100 --> 00:16:14.960]   to fool the facial recognition devices.
[00:16:14.960 --> 00:16:18.740]   So even very well trained models
[00:16:18.740 --> 00:16:21.300]   are vulnerable to such attacks.
[00:16:21.300 --> 00:16:25.060]   But what I'm going to explain today
[00:16:25.060 --> 00:16:27.460]   is about inference attacks,
[00:16:27.460 --> 00:16:30.940]   and specifically membership inference attack.
[00:16:30.940 --> 00:16:33.820]   So what is a membership inference attack?
[00:16:33.820 --> 00:16:37.340]   Basically, this membership inference attack means
[00:16:37.340 --> 00:16:41.780]   giving access to a well trained
[00:16:41.780 --> 00:16:45.500]   and a deployed deep learning model somewhere on the internet,
[00:16:45.500 --> 00:16:47.900]   we have a black box access to this model
[00:16:47.900 --> 00:16:52.900]   in such a way that we can only push an input to this model
[00:16:52.900 --> 00:16:56.060]   and receive the output of this model.
[00:16:56.060 --> 00:16:58.080]   And in this example,
[00:16:58.080 --> 00:17:00.380]   the output is a probability distribution.
[00:17:00.380 --> 00:17:04.620]   So perhaps a classification of this specific data sample.
[00:17:06.140 --> 00:17:10.060]   The attack is to take this input
[00:17:10.060 --> 00:17:13.640]   along with the output of this model that we aim to attack,
[00:17:13.640 --> 00:17:15.380]   by the way, the model that we aim to attack
[00:17:15.380 --> 00:17:18.640]   will be called a target network or the target model.
[00:17:18.640 --> 00:17:23.220]   So we take the input image or whatever data type it is,
[00:17:23.220 --> 00:17:26.180]   along with the output of the target model.
[00:17:26.180 --> 00:17:27.980]   And we should be able to tell
[00:17:27.980 --> 00:17:31.760]   whether this specific input was used
[00:17:31.760 --> 00:17:34.340]   and training this model or not,
[00:17:34.340 --> 00:17:36.020]   hence the name membership.
[00:17:36.020 --> 00:17:38.880]   So you want to know if this input sample
[00:17:38.880 --> 00:17:42.460]   was a member of the training dataset of this model.
[00:17:42.460 --> 00:17:44.740]   Now, this seems very trivial example,
[00:17:44.740 --> 00:17:47.620]   and some people will say, why do I even care?
[00:17:47.620 --> 00:17:49.860]   But this represents a HIPAA violation
[00:17:49.860 --> 00:17:52.220]   of the model that we are trying to attack
[00:17:52.220 --> 00:17:56.160]   is a model that was trained on electronic health records
[00:17:56.160 --> 00:17:58.400]   or patients information at some hospital.
[00:17:58.400 --> 00:18:02.620]   So that by itself means that this model
[00:18:02.620 --> 00:18:05.900]   is already breaking policies and breaking the rules.
[00:18:05.900 --> 00:18:09.020]   And this also means that our deep learning models
[00:18:09.020 --> 00:18:11.240]   are leaking lots of information.
[00:18:11.240 --> 00:18:14.380]   So a simple question here,
[00:18:14.380 --> 00:18:17.380]   how are we going to perform this attack?
[00:18:17.380 --> 00:18:20.000]   How are we going to check if a deep learning model
[00:18:20.000 --> 00:18:22.800]   is leaking or not leaking?
[00:18:22.800 --> 00:18:25.580]   If a simple sample belongs to
[00:18:25.580 --> 00:18:27.980]   or doesn't belong to a specific dataset,
[00:18:27.980 --> 00:18:30.340]   notice that my answer is a yes or no.
[00:18:30.340 --> 00:18:32.940]   So if you're really into deep learning,
[00:18:32.940 --> 00:18:35.700]   you will say, okay, I'm going to build a classifier
[00:18:35.700 --> 00:18:37.660]   that's going to take these two inputs
[00:18:37.660 --> 00:18:40.300]   and somehow tell me whether these two inputs belong to
[00:18:40.300 --> 00:18:44.220]   or do not belong to the training data sample.
[00:18:44.220 --> 00:18:45.940]   So this is correct.
[00:18:45.940 --> 00:18:49.940]   This is how we are going to attack target models
[00:18:49.940 --> 00:18:53.000]   to test their information leakage
[00:18:53.000 --> 00:18:56.440]   by turning deep learning against itself.
[00:18:56.440 --> 00:18:58.820]   So in other words, simply speaking,
[00:18:58.820 --> 00:19:02.140]   we are going to build a binary classifier
[00:19:02.140 --> 00:19:04.520]   that will take an input distribution,
[00:19:05.420 --> 00:19:09.940]   the probability distribution of some output vector
[00:19:09.940 --> 00:19:12.900]   from a model, push it in our attack model.
[00:19:12.900 --> 00:19:15.340]   And this attack model will give us either a zero or one.
[00:19:15.340 --> 00:19:17.900]   Zero means the specific data item
[00:19:17.900 --> 00:19:20.500]   did not belong to the training dataset.
[00:19:20.500 --> 00:19:22.220]   One means this specific data item
[00:19:22.220 --> 00:19:24.380]   was actually used in training this model.
[00:19:24.380 --> 00:19:27.680]   And that means this model is leaking information.
[00:19:27.680 --> 00:19:30.980]   So why will this even work?
[00:19:31.860 --> 00:19:36.860]   This works and this is actually was first shed light on this
[00:19:36.860 --> 00:19:42.780]   by a professor Riza Shukri.
[00:19:42.780 --> 00:19:46.380]   He and his team conducted experiments
[00:19:46.380 --> 00:19:48.660]   that showed that deep learning models,
[00:19:48.660 --> 00:19:53.660]   machine learning models actually behave differently
[00:19:53.660 --> 00:19:57.900]   on data items that they have seen during the training
[00:19:57.900 --> 00:20:00.260]   versus data items or input samples
[00:20:00.260 --> 00:20:02.560]   they have never seen before.
[00:20:02.560 --> 00:20:03.420]   And this makes sense.
[00:20:03.420 --> 00:20:06.140]   If you actually, when you overfit your model,
[00:20:06.140 --> 00:20:10.500]   and I bet most of us have run into the issue of overfitting,
[00:20:10.500 --> 00:20:12.900]   you see that your model starts to really learn
[00:20:12.900 --> 00:20:17.180]   the input data very well, the training data very well,
[00:20:17.180 --> 00:20:19.940]   but it fails to generalize to a new data
[00:20:19.940 --> 00:20:22.080]   or to the testing dataset.
[00:20:22.080 --> 00:20:24.860]   And therefore we take this characteristic
[00:20:24.860 --> 00:20:28.500]   of machine learning models, and we use it to attack them
[00:20:28.500 --> 00:20:30.620]   because based on this characteristic,
[00:20:30.620 --> 00:20:32.900]   we can tell if a machine learning model
[00:20:32.900 --> 00:20:34.580]   is leaking information or not.
[00:20:34.580 --> 00:20:38.540]   Simply speaking, if this probability of distribution
[00:20:38.540 --> 00:20:42.900]   consisted of about 90% probability being
[00:20:42.900 --> 00:20:45.020]   that this sample belongs to this class,
[00:20:45.020 --> 00:20:48.300]   and much less samples, much less distribution
[00:20:48.300 --> 00:20:51.340]   in the other two classes, then we can tell
[00:20:51.340 --> 00:20:53.780]   and be confident to some level that yes,
[00:20:53.780 --> 00:20:57.180]   this data item belongs to that model,
[00:20:57.180 --> 00:21:00.380]   unless this model has done some really very good
[00:21:00.380 --> 00:21:03.240]   regularization and generalization techniques.
[00:21:03.240 --> 00:21:09.380]   So the way we are going to attack this target model,
[00:21:09.380 --> 00:21:11.500]   we are going to consider it as a black box.
[00:21:11.500 --> 00:21:13.460]   So we don't have access to the architecture,
[00:21:13.460 --> 00:21:16.580]   we don't know the architecture of the model.
[00:21:16.580 --> 00:21:18.240]   Sometimes we don't even know whether
[00:21:18.240 --> 00:21:19.740]   it's a deep learning model or not.
[00:21:19.740 --> 00:21:21.780]   Doesn't matter at all actually.
[00:21:21.780 --> 00:21:25.500]   Experiments have shown that to be able to attack
[00:21:25.500 --> 00:21:28.180]   such a model, we only need to train an attack network.
[00:21:28.180 --> 00:21:30.460]   This attack network will need a data set.
[00:21:30.460 --> 00:21:33.620]   The data set is the outputs or the probability vectors
[00:21:33.620 --> 00:21:36.340]   or probability distributions of the inputs
[00:21:36.340 --> 00:21:38.780]   that were fed into our target model.
[00:21:38.780 --> 00:21:41.460]   And since the most difficult scenario,
[00:21:41.460 --> 00:21:43.580]   this attack model is going to be a black box,
[00:21:43.580 --> 00:21:47.260]   what we are going to do is we are going to imitate
[00:21:47.260 --> 00:21:52.260]   or shadow the performance or another performance,
[00:21:52.260 --> 00:21:54.740]   the task of this model.
[00:21:54.740 --> 00:21:56.340]   So the first step we are going to do
[00:21:56.340 --> 00:22:01.080]   is that we are going to build a new algorithms
[00:22:01.080 --> 00:22:03.060]   that will shadow or mimic,
[00:22:03.060 --> 00:22:05.860]   try to imitate the behavior of this network.
[00:22:05.860 --> 00:22:08.260]   And then we will have full access to those networks
[00:22:08.260 --> 00:22:10.500]   and we'll push lots of information into them,
[00:22:10.500 --> 00:22:14.900]   receive lots of outputs and then train our binary classifier.
[00:22:14.900 --> 00:22:18.900]   So this is a more overview of the approach.
[00:22:18.900 --> 00:22:22.160]   We don't have access to the model
[00:22:22.160 --> 00:22:23.980]   that we are trying to attack.
[00:22:23.980 --> 00:22:28.020]   Perhaps that model was built by Google AutoML
[00:22:28.020 --> 00:22:30.060]   by someone else.
[00:22:30.060 --> 00:22:34.180]   So we can only run inferences or queries on that model.
[00:22:34.180 --> 00:22:36.780]   So the first thing we can do is to mimic
[00:22:36.780 --> 00:22:38.660]   the behavior of that attack.
[00:22:38.660 --> 00:22:43.660]   So we know that model classifies whether a specific X-ray
[00:22:43.660 --> 00:22:47.000]   shows some specific disease or not.
[00:22:47.000 --> 00:22:50.540]   So the first step will be to collect or to find a data set
[00:22:50.540 --> 00:22:54.060]   that's very similar to our task at hand.
[00:22:54.060 --> 00:22:57.020]   Then what we are going to do is that take that data set
[00:22:57.020 --> 00:23:00.340]   and divide it into different, into subsets,
[00:23:00.340 --> 00:23:04.580]   because we are going to use some of the data sets
[00:23:04.580 --> 00:23:07.860]   to train the shadow network that we have.
[00:23:07.860 --> 00:23:10.300]   And those will be labeled as N.
[00:23:10.300 --> 00:23:14.460]   So those instances, those data records
[00:23:14.460 --> 00:23:17.520]   that we use to train that were actually used
[00:23:17.520 --> 00:23:18.900]   in training the shadow model.
[00:23:18.900 --> 00:23:22.700]   And then we are going to use another parts of this data set
[00:23:22.700 --> 00:23:25.460]   only for the inference purposes.
[00:23:25.460 --> 00:23:28.540]   So these data set will be used to run inferences
[00:23:28.540 --> 00:23:31.160]   of the shadow model that we are going to train,
[00:23:31.160 --> 00:23:33.860]   but they were never used in training the model.
[00:23:33.860 --> 00:23:35.500]   And then at the end of the day,
[00:23:35.500 --> 00:23:38.220]   we are going to have what a data set
[00:23:38.220 --> 00:23:44.260]   of probability distributions of some data records
[00:23:44.260 --> 00:23:47.660]   that were used in training the shadow model
[00:23:47.660 --> 00:23:50.900]   and a data set of probability distributions
[00:23:50.900 --> 00:23:55.340]   of data records that were not used in training the shadow.
[00:23:55.340 --> 00:23:58.760]   So we have N and out along with probability distributions,
[00:23:58.760 --> 00:24:02.420]   we are going to use them into training the attack network.
[00:24:02.420 --> 00:24:05.940]   And now the attack network will study the behavior
[00:24:05.940 --> 00:24:08.620]   of the shadow model to be able to distinguish
[00:24:08.620 --> 00:24:12.860]   the behavior of the shadow model on the sample data.
[00:24:12.860 --> 00:24:14.680]   Even though this is a simple example,
[00:24:14.680 --> 00:24:17.500]   but if you look at the probability distributions here,
[00:24:17.500 --> 00:24:19.300]   between data items that belong
[00:24:19.300 --> 00:24:21.580]   and do not belong to the training set,
[00:24:21.580 --> 00:24:23.740]   you can probably get an intuitive,
[00:24:23.740 --> 00:24:28.740]   very high level idea about membership inference attacks.
[00:24:28.740 --> 00:24:32.700]   Usually to make this work better in reality,
[00:24:32.700 --> 00:24:35.700]   we are not going to use only one neural network.
[00:24:35.700 --> 00:24:38.060]   The more shadow models you build,
[00:24:38.060 --> 00:24:43.060]   the more you are to build an algorithm
[00:24:43.060 --> 00:24:45.080]   that is very similar to the algorithm
[00:24:45.080 --> 00:24:46.980]   that we are trying to attack.
[00:24:46.980 --> 00:24:48.980]   Because remember, we don't know the architecture
[00:24:48.980 --> 00:24:50.580]   of the algorithm that we are trying to attack.
[00:24:50.580 --> 00:24:52.700]   We don't even know what type of algorithm is it.
[00:24:52.700 --> 00:24:55.340]   Is it some conventional machine learning
[00:24:55.340 --> 00:24:56.540]   or a neural network?
[00:24:56.540 --> 00:24:59.740]   And therefore, the more shadow models you build,
[00:24:59.740 --> 00:25:04.660]   you are, the closer you are to the architecture,
[00:25:04.660 --> 00:25:07.780]   to the type of algorithm that you are attacking.
[00:25:07.780 --> 00:25:11.860]   In reality, I've also seen that a very smart idea here
[00:25:11.860 --> 00:25:16.620]   is to also build an attack model for every single class.
[00:25:17.620 --> 00:25:20.500]   So for every single class for the true label.
[00:25:20.500 --> 00:25:23.340]   So if the true label here was this first label,
[00:25:23.340 --> 00:25:25.620]   so we are going to build an attack network
[00:25:25.620 --> 00:25:29.020]   that will be able to attack the target model
[00:25:29.020 --> 00:25:31.220]   based on the true class output,
[00:25:31.220 --> 00:25:33.660]   because the probability distribution will be different
[00:25:33.660 --> 00:25:35.300]   among the classes as well.
[00:25:35.300 --> 00:25:36.820]   And this is generally true.
[00:25:36.820 --> 00:25:39.380]   We've seen that deep learning models
[00:25:39.380 --> 00:25:42.580]   tend to perform better on some classes
[00:25:42.580 --> 00:25:44.620]   and the same data set than other classes
[00:25:44.620 --> 00:25:45.960]   because of the data distribution,
[00:25:45.960 --> 00:25:47.820]   because of the imbalance problems
[00:25:47.820 --> 00:25:50.960]   and so many other issues and concerns.
[00:25:50.960 --> 00:25:53.900]   And therefore, building the attack network,
[00:25:53.900 --> 00:25:55.740]   you could also build an attack network
[00:25:55.740 --> 00:25:57.780]   for every single class in the shadow model
[00:25:57.780 --> 00:25:59.680]   and that will give you better results.
[00:25:59.680 --> 00:26:02.440]   Just for reference purposes,
[00:26:02.440 --> 00:26:04.460]   I have a Google collab notebook here.
[00:26:04.460 --> 00:26:05.860]   I can take you through the code.
[00:26:05.860 --> 00:26:10.860]   Nothing really crazy, very straightforward.
[00:26:10.860 --> 00:26:14.020]   But if you don't want to build everything from scratch,
[00:26:14.020 --> 00:26:17.720]   I urge you to look at the ML Privacy Meter tool.
[00:26:17.720 --> 00:26:22.720]   So this was also built by Dr. Shukri, Riza and others.
[00:26:22.720 --> 00:26:26.920]   They are the ones that actually first written a paper
[00:26:26.920 --> 00:26:28.520]   about membership inference attacks
[00:26:28.520 --> 00:26:31.080]   and came up with the idea of shadow models.
[00:26:31.080 --> 00:26:34.300]   So this tool will allow you to just plug in
[00:26:34.300 --> 00:26:36.480]   your trained model with the data set
[00:26:36.480 --> 00:26:39.480]   and they will return a meter
[00:26:39.480 --> 00:26:42.280]   and very beautiful visualizations
[00:26:42.280 --> 00:26:46.100]   about how vulnerable your model
[00:26:46.100 --> 00:26:47.820]   about membership inference attacks
[00:26:47.820 --> 00:26:52.820]   and which of the instances are most sensitive.
[00:26:52.820 --> 00:26:59.940]   So to implement here, again, PyTorch fan here.
[00:26:59.940 --> 00:27:03.220]   So nothing crazy.
[00:27:03.220 --> 00:27:07.640]   We are loading Cypher 10 data set
[00:27:07.640 --> 00:27:09.480]   and splitting the data set.
[00:27:09.480 --> 00:27:11.160]   Probably the only different thing here
[00:27:11.160 --> 00:27:15.180]   from normal training cycle
[00:27:15.180 --> 00:27:17.380]   is the splitting of the data set.
[00:27:17.380 --> 00:27:20.740]   I'm splitting the 50,000, was it 50,000 Cypher 10?
[00:27:20.740 --> 00:27:25.480]   Yes, 50,000 training, splitting it into four different sets.
[00:27:25.480 --> 00:27:30.820]   Each one of these sets has 12,500 images
[00:27:30.820 --> 00:27:33.420]   and I'm using data sampler here,
[00:27:33.420 --> 00:27:36.820]   random data sampler from subset random sampler
[00:27:36.820 --> 00:27:41.100]   to give me those 12,500 images from each set.
[00:27:41.100 --> 00:27:44.800]   Why I have four, each two of them
[00:27:44.800 --> 00:27:47.560]   will be used to train a model,
[00:27:47.560 --> 00:27:49.300]   whether it's two shadow models
[00:27:49.300 --> 00:27:52.180]   or whether it's one model that you are trying to attack
[00:27:52.180 --> 00:27:54.880]   and building a shadow model, it's up to you.
[00:27:54.880 --> 00:27:58.280]   And each one of these models will need two sets.
[00:27:58.280 --> 00:28:01.700]   One of them is the data sets that were used
[00:28:01.700 --> 00:28:03.000]   and training the model.
[00:28:03.000 --> 00:28:05.820]   The other one is the data set
[00:28:05.820 --> 00:28:08.260]   that was not used and training the model.
[00:28:08.260 --> 00:28:12.360]   And those two will later consist the labels
[00:28:12.360 --> 00:28:14.360]   for your binary classifier.
[00:28:14.360 --> 00:28:15.480]   So that's it.
[00:28:15.480 --> 00:28:20.560]   Training a regular neural network,
[00:28:20.560 --> 00:28:23.660]   a function for evaluating a neural network,
[00:28:23.660 --> 00:28:25.840]   reusing some of the code here
[00:28:25.840 --> 00:28:28.540]   to build a ResNet neural networks.
[00:28:28.540 --> 00:28:31.440]   I know the problem at hand very easy,
[00:28:31.440 --> 00:28:35.080]   but just trying to use ResNets pre-trained models.
[00:28:36.040 --> 00:28:39.900]   Just for historical reasons,
[00:28:39.900 --> 00:28:41.260]   one of them is called Alice,
[00:28:41.260 --> 00:28:42.920]   the other one is called Bob.
[00:28:42.920 --> 00:28:45.020]   Now I'm thinking about it, it's a little bit funny,
[00:28:45.020 --> 00:28:47.340]   but perhaps I should call the target model
[00:28:47.340 --> 00:28:49.680]   and the shadow model, make it more clear.
[00:28:49.680 --> 00:28:51.500]   But one of them is ResNet 50,
[00:28:51.500 --> 00:28:54.980]   the other one ResNet 100, I think 110.
[00:28:54.980 --> 00:28:57.500]   And now we're building the attack model.
[00:28:57.500 --> 00:28:58.860]   Nothing crazy here.
[00:28:58.860 --> 00:29:03.580]   Straightforward binary classifier,
[00:29:04.580 --> 00:29:07.600]   fully connected layers that we'll use
[00:29:07.600 --> 00:29:10.260]   to create the attack network.
[00:29:10.260 --> 00:29:13.480]   More just validation source code.
[00:29:13.480 --> 00:29:18.180]   And our attack is able to achieve almost 80% accuracy.
[00:29:18.180 --> 00:29:24.520]   So this will also be true for most of the
[00:29:24.520 --> 00:29:29.000]   well-known architectures,
[00:29:29.000 --> 00:29:31.760]   most of the well-known data sets
[00:29:33.220 --> 00:29:34.880]   that we have publicly available,
[00:29:34.880 --> 00:29:37.880]   the MS datasets, the ImageNet and so on and so forth.
[00:29:37.880 --> 00:29:42.880]   But this also transfers to well-structured applications
[00:29:42.880 --> 00:29:45.440]   such as medical applications.
[00:29:45.440 --> 00:29:47.160]   And we were able to prove in the lab
[00:29:47.160 --> 00:29:52.160]   that even some of the well-trained medical models,
[00:29:52.160 --> 00:29:57.160]   they are leaking just crazy amounts of data.
[00:29:57.160 --> 00:29:59.240]   If you look at model inversion techniques,
[00:29:59.240 --> 00:30:00.920]   and not only are able to tell
[00:30:00.920 --> 00:30:04.760]   whether a specific data item existed in the dataset or not,
[00:30:04.760 --> 00:30:08.020]   but you're also able to retrieve some data
[00:30:08.020 --> 00:30:10.300]   from that data item.
[00:30:10.300 --> 00:30:13.100]   And model inversion is actually very simple,
[00:30:13.100 --> 00:30:15.300]   I think I can talk about it in two minutes.
[00:30:15.300 --> 00:30:18.460]   Basically, you can give the outputs of one model
[00:30:18.460 --> 00:30:19.820]   and to create an attack model
[00:30:19.820 --> 00:30:23.620]   that does something like an autoencoder.
[00:30:23.620 --> 00:30:27.140]   So the bottleneck layer is the output of the attack model,
[00:30:27.140 --> 00:30:28.140]   the target model,
[00:30:28.140 --> 00:30:29.860]   and then you bring a dataset
[00:30:29.860 --> 00:30:32.900]   whose data distribution very similar to the target model
[00:30:32.900 --> 00:30:34.720]   that you're trying to attack.
[00:30:34.720 --> 00:30:37.380]   And you just use the autoencoder architecture
[00:30:37.380 --> 00:30:42.380]   to reconstruct those inputs
[00:30:42.380 --> 00:30:46.220]   from the output of the distribution.
[00:30:46.220 --> 00:30:48.860]   So this is simply the membership inference attacks.
[00:30:48.860 --> 00:30:51.660]   And this is one of the first steps
[00:30:51.660 --> 00:30:56.500]   to show how people have really been bracing
[00:30:56.500 --> 00:30:59.200]   to breaking the state of the art of deep learning models
[00:30:59.200 --> 00:31:02.540]   while ignoring very important issues
[00:31:02.540 --> 00:31:04.540]   and concerns around them.
[00:31:04.540 --> 00:31:05.900]   And one of the previous,
[00:31:05.900 --> 00:31:10.780]   actually before working on this current task,
[00:31:10.780 --> 00:31:15.780]   I was working on the lifecycle development
[00:31:15.780 --> 00:31:17.460]   and actually weights and biases
[00:31:17.460 --> 00:31:19.740]   does something really amazing.
[00:31:19.740 --> 00:31:22.340]   And this actually reminds me about
[00:31:22.340 --> 00:31:24.540]   the gold rush in California,
[00:31:24.540 --> 00:31:26.660]   I think 1840, something like this.
[00:31:26.660 --> 00:31:29.740]   People heard that there's a gold in California,
[00:31:29.740 --> 00:31:32.080]   everybody started rushing to mine for gold
[00:31:32.080 --> 00:31:34.120]   to be able to find gold, right?
[00:31:34.120 --> 00:31:36.080]   But the true entrepreneurs are those
[00:31:36.080 --> 00:31:38.800]   who were able to tackle the important things.
[00:31:38.800 --> 00:31:42.420]   People who, some people made some money from gold,
[00:31:42.420 --> 00:31:45.040]   but the people who made most money at that time
[00:31:45.040 --> 00:31:48.700]   are those who bought lands or rented lands,
[00:31:48.700 --> 00:31:52.400]   those who made the shovels, the boots,
[00:31:52.420 --> 00:31:56.980]   and the jeans for people who are mining for gold.
[00:31:56.980 --> 00:31:59.220]   So something similar happening nowadays,
[00:31:59.220 --> 00:32:02.660]   everybody since 2010s until now,
[00:32:02.660 --> 00:32:05.100]   everybody has been rushing towards
[00:32:05.100 --> 00:32:06.580]   creating the best architecture.
[00:32:06.580 --> 00:32:10.100]   And after that, the best models,
[00:32:10.100 --> 00:32:11.400]   breaking the state of the art,
[00:32:11.400 --> 00:32:15.620]   but the issues of the model management and privacy now
[00:32:15.620 --> 00:32:19.740]   have becoming to the surface of our research
[00:32:19.740 --> 00:32:21.760]   and our application nowadays.
[00:32:21.760 --> 00:32:25.900]   So even Tesla cars have been fooled
[00:32:25.900 --> 00:32:29.340]   and their models have been fooled into
[00:32:29.340 --> 00:32:32.220]   and tricked into producing wrong results.
[00:32:32.220 --> 00:32:35.180]   Very well trained models have done the same thing as well.
[00:32:35.180 --> 00:32:38.700]   And lots of the machine learning and deep learning models
[00:32:38.700 --> 00:32:43.700]   that are deployed in very reputable systems
[00:32:43.700 --> 00:32:50.100]   are actually leaking some amount about users
[00:32:50.100 --> 00:32:53.780]   and the data that are included in training such models.
[00:32:53.780 --> 00:32:56.680]   And therefore this area is really going to be
[00:32:56.680 --> 00:32:58.740]   a very hot topic this year.
[00:32:58.740 --> 00:33:00.060]   And the current pandemic,
[00:33:00.060 --> 00:33:03.800]   even though everything around it has been terrible,
[00:33:03.800 --> 00:33:05.900]   but it's actually accelerated this area
[00:33:05.900 --> 00:33:07.340]   because when technology again,
[00:33:07.340 --> 00:33:10.220]   tried to solve this problems by tracking people
[00:33:10.220 --> 00:33:12.940]   who have the virus and their interactions with other people,
[00:33:12.940 --> 00:33:14.940]   they were struck and hit by the wall
[00:33:14.940 --> 00:33:16.620]   of privacy preserving issues.
[00:33:16.620 --> 00:33:19.620]   So now we really need to solve and focus on that
[00:33:19.620 --> 00:33:20.820]   and such issues.
[00:33:20.820 --> 00:33:26.780]   So what are some of the approaches to prevent
[00:33:26.780 --> 00:33:31.420]   or to hinder or to slow down privacy leakage,
[00:33:31.420 --> 00:33:35.240]   specifically in a membership inference attack?
[00:33:35.240 --> 00:33:37.780]   And totally speaking,
[00:33:37.780 --> 00:33:40.780]   what we are studying in a membership inference attack
[00:33:40.780 --> 00:33:44.020]   is the relationship between the input image
[00:33:44.020 --> 00:33:46.340]   and the output of the target model.
[00:33:46.340 --> 00:33:48.100]   So the output of the target model
[00:33:48.100 --> 00:33:51.380]   is most likely a vector of probabilities.
[00:33:51.380 --> 00:33:56.420]   So if we limit those probabilities to only the best case,
[00:33:56.420 --> 00:33:58.220]   only to the top probability,
[00:33:58.220 --> 00:34:02.180]   then we are somehow reducing
[00:34:02.180 --> 00:34:04.760]   or slowing down membership inference attacks.
[00:34:04.760 --> 00:34:08.900]   So very trivial solution could be really effective,
[00:34:08.900 --> 00:34:13.140]   but a very dedicated adversaries
[00:34:13.140 --> 00:34:15.880]   or people who are able to manage
[00:34:15.880 --> 00:34:18.180]   well-designed membership inference attacks
[00:34:18.180 --> 00:34:22.100]   will still be able to leak information
[00:34:22.100 --> 00:34:23.780]   and know whether a specific item
[00:34:23.780 --> 00:34:25.780]   was used in training the model
[00:34:25.780 --> 00:34:27.900]   only by the top probability.
[00:34:27.900 --> 00:34:32.580]   Another solution is to coarsen the predictions.
[00:34:32.580 --> 00:34:37.580]   So just coarsen the predictions means,
[00:34:42.120 --> 00:34:45.900]   like I forgot what's the correct English word there.
[00:34:45.900 --> 00:34:48.740]   Sometimes it just goes into your mind.
[00:34:48.740 --> 00:34:53.100]   But if we have a probability that's a 0.095,
[00:34:53.100 --> 00:34:55.700]   then we say it's 0.096, for example.
[00:34:55.700 --> 00:35:00.060]   So we are coarsening the predictions of the vector
[00:35:00.060 --> 00:35:03.320]   and we're not adding noise per say here,
[00:35:03.320 --> 00:35:05.580]   but we are improving the results a little bit
[00:35:05.580 --> 00:35:09.900]   so that we are trying to give less information
[00:35:09.900 --> 00:35:12.100]   about the probability distribution.
[00:35:12.100 --> 00:35:13.880]   Of course, again, by the definition
[00:35:13.880 --> 00:35:15.700]   of membership inference attacks,
[00:35:15.700 --> 00:35:18.140]   overfit is one of the main reasons,
[00:35:18.140 --> 00:35:19.900]   but it is not the only reason.
[00:35:19.900 --> 00:35:22.860]   And therefore reducing overfitting
[00:35:22.860 --> 00:35:24.960]   or generalizing your model,
[00:35:24.960 --> 00:35:26.660]   the better you generalize it,
[00:35:26.660 --> 00:35:28.960]   the less likely it is to be infected
[00:35:28.960 --> 00:35:30.720]   by a membership inference attack.
[00:35:30.720 --> 00:35:36.100]   But again, regularization is not an easy topic
[00:35:36.100 --> 00:35:39.920]   and having a well-generalized model
[00:35:39.920 --> 00:35:42.660]   sometimes means dropping in its accuracy
[00:35:42.660 --> 00:35:45.940]   and that's not preferable for industry.
[00:35:45.940 --> 00:35:49.840]   Even 0.5% increase or decrease in accuracy
[00:35:49.840 --> 00:35:53.240]   could mean lots and lots of money for an industry
[00:35:53.240 --> 00:35:55.900]   and therefore they might sacrifice,
[00:35:55.900 --> 00:35:58.560]   unfortunately, with some user privacy
[00:35:58.560 --> 00:36:03.560]   to keep that money flowing or things like that.
[00:36:03.560 --> 00:36:07.020]   One of the potential solutions
[00:36:07.020 --> 00:36:10.640]   that has been also coming to the surface lately
[00:36:10.640 --> 00:36:12.720]   is the differential privacy.
[00:36:12.720 --> 00:36:15.920]   So quickly going over it,
[00:36:15.920 --> 00:36:18.040]   what is differential privacy?
[00:36:18.040 --> 00:36:20.580]   Basically one of the main contributors in this field,
[00:36:20.580 --> 00:36:24.020]   if you want to read more about it is Cynthia Dwork.
[00:36:24.020 --> 00:36:29.020]   I believe she was a researcher at Google.
[00:36:29.020 --> 00:36:32.160]   Basically differential privacy is a guarantee
[00:36:32.160 --> 00:36:35.320]   or a protocol that tells you,
[00:36:35.320 --> 00:36:38.800]   if you run your statistical query on this dataset,
[00:36:38.800 --> 00:36:43.800]   then we guarantee this much
[00:36:43.800 --> 00:36:50.200]   that the privacy of people in this dataset will be preserved.
[00:36:50.200 --> 00:36:51.900]   In other words,
[00:36:51.900 --> 00:36:58.040]   privacy preserving, differential privacy
[00:36:58.040 --> 00:37:00.560]   answers the following question
[00:37:00.560 --> 00:37:03.640]   or actually asks the following question.
[00:37:03.640 --> 00:37:06.180]   If I have a dataset and let's assume
[00:37:06.180 --> 00:37:09.360]   that this is some feature that we,
[00:37:09.360 --> 00:37:13.800]   about people or participants in this dataset or database
[00:37:13.800 --> 00:37:16.160]   that are private, so this could be the gender,
[00:37:16.160 --> 00:37:18.920]   this could be some type of,
[00:37:18.920 --> 00:37:20.720]   whether you have a disease or not.
[00:37:20.720 --> 00:37:22.960]   Anyways, this is some private information
[00:37:22.960 --> 00:37:26.160]   that we want to make sure that if anybody runs a query
[00:37:26.160 --> 00:37:31.160]   on this database will not affect the privacy
[00:37:32.760 --> 00:37:35.000]   of the users who are participating in this dataset.
[00:37:35.000 --> 00:37:37.680]   So differential privacy says,
[00:37:37.680 --> 00:37:40.160]   if you run your query on this dataset
[00:37:40.160 --> 00:37:43.720]   and then you remove one of the participants
[00:37:43.720 --> 00:37:46.760]   from this dataset and use the same query again
[00:37:46.760 --> 00:37:51.300]   on the same dataset without a specific participant,
[00:37:51.300 --> 00:37:54.800]   will the query, the result of the query be the same
[00:37:54.800 --> 00:37:56.560]   between both queries?
[00:37:56.560 --> 00:37:58.920]   If it is the same and exactly the same,
[00:37:58.920 --> 00:38:02.760]   then you have a perfect privacy.
[00:38:02.760 --> 00:38:06.000]   If it is not, this means that the absence
[00:38:06.000 --> 00:38:08.640]   or existence of specific data point
[00:38:08.640 --> 00:38:13.160]   or specific patient in this dataset is affecting the query,
[00:38:13.160 --> 00:38:16.280]   which means that the query is exposing information
[00:38:16.280 --> 00:38:17.880]   about this dataset.
[00:38:17.880 --> 00:38:19.680]   And therefore the question here becomes,
[00:38:19.680 --> 00:38:21.480]   or the research problem becomes,
[00:38:21.480 --> 00:38:25.300]   can we construct a query that does not change
[00:38:25.300 --> 00:38:27.060]   or maybe change but very little,
[00:38:27.060 --> 00:38:29.960]   no matter what we remove from the database?
[00:38:29.960 --> 00:38:33.680]   And to give you a very simple example here,
[00:38:33.680 --> 00:38:38.320]   the sum query on a database is not differentially private.
[00:38:38.320 --> 00:38:40.760]   The sum query, so basically if you want to add,
[00:38:40.760 --> 00:38:44.100]   if you want to know the sum of all the data points
[00:38:44.100 --> 00:38:46.600]   on your dataset, and this is because if you add
[00:38:46.600 --> 00:38:48.400]   this dataset, the query is the sum.
[00:38:48.400 --> 00:38:51.600]   If you added it, the result is one, two, three, four.
[00:38:51.600 --> 00:38:53.360]   If you remove one of these participants,
[00:38:53.360 --> 00:38:55.140]   let's say the last participant,
[00:38:55.140 --> 00:38:56.840]   now the query will be three.
[00:38:56.840 --> 00:38:59.200]   And simply by subtracting these two queries,
[00:38:59.200 --> 00:39:02.260]   we will know that the value of the data point
[00:39:02.260 --> 00:39:05.000]   that was removed is one, and therefore we are leaking
[00:39:05.000 --> 00:39:08.800]   lots of information about this, lots of information,
[00:39:08.800 --> 00:39:11.040]   or we're leaking information about specific people
[00:39:11.040 --> 00:39:15.560]   in the dataset, and/or specific data records or data items.
[00:39:15.560 --> 00:39:19.520]   So what we do in differential privacy
[00:39:19.520 --> 00:39:23.560]   is we create parallel datasets or databases,
[00:39:23.560 --> 00:39:26.100]   and each one of them, we take one specific item.
[00:39:26.100 --> 00:39:27.860]   We run the query across all of them,
[00:39:27.860 --> 00:39:29.960]   and then we find the maximum difference
[00:39:29.960 --> 00:39:32.560]   between the query on the original database
[00:39:32.560 --> 00:39:34.840]   and any one of the parallel databases,
[00:39:34.840 --> 00:39:37.120]   and that difference, the maximum difference
[00:39:37.120 --> 00:39:40.800]   in the query results is called sensitivity.
[00:39:40.800 --> 00:39:43.600]   And therefore the sensitivity of the sum query here
[00:39:43.600 --> 00:39:46.400]   is one, which is a little bit high.
[00:39:46.400 --> 00:39:51.000]   So you want to reduce that sensitivity as much as possible,
[00:39:51.000 --> 00:39:52.880]   and when you reduce it very much,
[00:39:52.880 --> 00:39:56.260]   that the query on any one of these parallel databases
[00:39:56.260 --> 00:39:59.180]   equals to the query on the original database is the same,
[00:39:59.180 --> 00:40:04.860]   then you have perfect privacy-preserving query,
[00:40:04.860 --> 00:40:07.980]   which perhaps sometimes might not be useful
[00:40:07.980 --> 00:40:09.780]   if you're really not learning anything
[00:40:09.780 --> 00:40:12.860]   when you're removing or adding or replacing different people,
[00:40:12.860 --> 00:40:16.200]   and some other times might be just impractical, right?
[00:40:16.200 --> 00:40:20.260]   How does differential privacy work in a real life?
[00:40:20.260 --> 00:40:23.940]   Differential privacy focuses on adding noise to the data,
[00:40:23.940 --> 00:40:26.680]   and we add noise to the data in two different ways.
[00:40:26.680 --> 00:40:30.660]   First way is called the global differential privacy.
[00:40:30.660 --> 00:40:33.760]   You run your query on the data on a plain database
[00:40:33.760 --> 00:40:36.960]   that has data records and a plain text.
[00:40:36.960 --> 00:40:38.920]   In other words, they are not encrypted.
[00:40:38.920 --> 00:40:43.160]   - Gabe, you have five more minutes left, I'm sorry.
[00:40:43.160 --> 00:40:45.360]   - Okay, cool. (laughs)
[00:40:45.360 --> 00:40:48.240]   So in a global differential privacy,
[00:40:48.240 --> 00:40:51.320]   you run your query on an encrypted data,
[00:40:51.320 --> 00:40:53.640]   and then you add noise to your query.
[00:40:53.640 --> 00:41:01.420]   The accuracy results becomes really good,
[00:41:01.420 --> 00:41:03.660]   but you have to trust the database curator.
[00:41:03.660 --> 00:41:07.300]   Different way to adding noise
[00:41:07.300 --> 00:41:09.320]   is called local differential privacy,
[00:41:09.320 --> 00:41:10.520]   and local differential privacy,
[00:41:10.520 --> 00:41:15.120]   we add noise to every data sample by itself,
[00:41:15.120 --> 00:41:17.240]   and one of the very famous ways here
[00:41:17.240 --> 00:41:19.920]   is called randomized response,
[00:41:19.920 --> 00:41:21.600]   and the idea here is really fun,
[00:41:21.600 --> 00:41:23.600]   and I'm going to end with it.
[00:41:23.600 --> 00:41:26.360]   The idea about it here is that we as humans,
[00:41:26.360 --> 00:41:29.120]   or as people who are participating in a query,
[00:41:29.120 --> 00:41:30.840]   or a statistical analysis,
[00:41:30.840 --> 00:41:34.160]   or volunteering with our health records
[00:41:34.160 --> 00:41:36.440]   to create a deep learning model,
[00:41:36.440 --> 00:41:40.080]   we don't necessarily need to answer the truth.
[00:41:40.080 --> 00:41:44.480]   And this concept comes from some sociology studies,
[00:41:46.000 --> 00:41:48.500]   randomized response, the idea is simple.
[00:41:48.500 --> 00:41:50.520]   So assume the government wants to know
[00:41:50.520 --> 00:41:54.720]   what is the percentage of people that use illegal drugs.
[00:41:54.720 --> 00:41:57.600]   Of course, if the government cannot interview
[00:41:57.600 --> 00:42:00.840]   every single person and ask them, do you smoke or not,
[00:42:00.840 --> 00:42:02.760]   we are never going to answer,
[00:42:02.760 --> 00:42:05.040]   never going to give the true answer, right?
[00:42:05.040 --> 00:42:07.280]   However, even if the government says,
[00:42:07.280 --> 00:42:08.720]   I'm not really interested in knowing
[00:42:08.720 --> 00:42:11.900]   whether you specifically smoke or not,
[00:42:11.900 --> 00:42:15.520]   we're still not going to answer the truth response.
[00:42:15.520 --> 00:42:18.200]   So the idea here is that the government in this case
[00:42:18.200 --> 00:42:21.240]   is going to give every participant a coin,
[00:42:21.240 --> 00:42:22.520]   and tell the participant,
[00:42:22.520 --> 00:42:25.120]   go ahead and flip this coin twice.
[00:42:25.120 --> 00:42:29.260]   If the first flip is heads, then you must answer the truth.
[00:42:29.260 --> 00:42:32.600]   If the first flip is tails,
[00:42:32.600 --> 00:42:35.200]   then you answer based on the second flip.
[00:42:35.200 --> 00:42:38.400]   And this way, even if a person says, yes, I smoke,
[00:42:38.400 --> 00:42:40.440]   the government does not know whether they said yes,
[00:42:40.440 --> 00:42:42.560]   they smoke because they flipped the first time heads
[00:42:42.560 --> 00:42:44.720]   and they said the true answer,
[00:42:44.720 --> 00:42:46.680]   or whether they flipped it twice
[00:42:46.680 --> 00:42:50.240]   and they answered just randomly.
[00:42:50.240 --> 00:42:52.680]   However, at the end of the day, using statistics,
[00:42:52.680 --> 00:42:55.320]   we will be able to know the percentage of people
[00:42:55.320 --> 00:42:57.440]   that actually smoke.
[00:42:57.440 --> 00:43:01.160]   So this is how differential privacy can help us
[00:43:01.160 --> 00:43:03.720]   in so many different approaches.
[00:43:03.720 --> 00:43:05.080]   So thank you very much.
[00:43:05.080 --> 00:43:06.860]   Sorry for going over time.
[00:43:06.860 --> 00:43:08.720]   I'm not sure, maybe I'm not.
[00:43:08.720 --> 00:43:10.080]   But if you have any questions,
[00:43:10.080 --> 00:43:12.520]   please feel free to ask them.
[00:43:12.520 --> 00:43:15.680]   I'm on Twitter, you can always ask me any questions there
[00:43:15.680 --> 00:43:16.500]   if you want.
[00:43:16.500 --> 00:43:18.320]   - Thanks.
[00:43:18.320 --> 00:43:19.840]   People love to talk.
[00:43:19.840 --> 00:43:21.240]   There are a lot of questions.
[00:43:21.240 --> 00:43:23.200]   I just wanted to make sure we have enough time left
[00:43:23.200 --> 00:43:24.620]   for everyone, I'm sorry.
[00:43:24.620 --> 00:43:29.960]   So Joseph asks, what kind of security protection techniques
[00:43:29.960 --> 00:43:34.780]   are used in the banking industry for ML models?
[00:43:34.780 --> 00:43:38.160]   - Banking is very similar to health domains
[00:43:38.160 --> 00:43:41.300]   because the privacy is a very important aspect there.
[00:43:42.200 --> 00:43:44.400]   Differential privacy is one of the approaches,
[00:43:44.400 --> 00:43:47.820]   but nowadays there have been more sophisticated approaches.
[00:43:47.820 --> 00:43:51.320]   For example, at the startup, I'm working at Triple Blind,
[00:43:51.320 --> 00:43:54.340]   we are able to train on data without seeing the data.
[00:43:54.340 --> 00:43:59.920]   So some flavors of homomorphic encryption
[00:43:59.920 --> 00:44:03.120]   will allow you to train on encrypted data
[00:44:03.120 --> 00:44:04.140]   without seeing the data.
[00:44:04.140 --> 00:44:06.320]   So this is one of the approaches.
[00:44:06.320 --> 00:44:09.660]   Multiparty computations or secure multiparty computation
[00:44:09.660 --> 00:44:11.720]   is another possible way that will allow you
[00:44:11.720 --> 00:44:13.520]   to train on data without seeing it.
[00:44:13.520 --> 00:44:17.000]   - Thanks.
[00:44:17.000 --> 00:44:19.680]   And then Michael asks, can you use this method
[00:44:19.680 --> 00:44:23.760]   only if you have access to the training data
[00:44:23.760 --> 00:44:26.160]   used to train the target model?
[00:44:26.160 --> 00:44:29.440]   - No, but then it's just going to be
[00:44:29.440 --> 00:44:30.680]   a little bit more difficult.
[00:44:30.680 --> 00:44:31.760]   There are some type of attacks
[00:44:31.760 --> 00:44:34.140]   that are called exploratory attacks.
[00:44:34.140 --> 00:44:36.120]   So you attack the target model to know
[00:44:36.120 --> 00:44:38.360]   the type of data distribution,
[00:44:38.360 --> 00:44:40.640]   where did their data come from?
[00:44:40.640 --> 00:44:43.880]   I just assumed here that I know the data set
[00:44:43.880 --> 00:44:46.200]   just for simplicity and explanation purposes,
[00:44:46.200 --> 00:44:49.860]   but in reality, we might not know what type of data,
[00:44:49.860 --> 00:44:51.740]   not even the classes of the data.
[00:44:51.740 --> 00:44:56.160]   At that time, you need to do more hacking around the model,
[00:44:56.160 --> 00:44:59.280]   documentation reading, somehow exploratory attacks
[00:44:59.280 --> 00:45:01.240]   to figure out the data distribution,
[00:45:01.240 --> 00:45:02.920]   and then try to find something.
[00:45:02.920 --> 00:45:05.680]   If something does not exist, try to use GANs, right,
[00:45:05.680 --> 00:45:08.240]   to generate your own (laughs)
[00:45:08.240 --> 00:45:10.560]   - Working around, cool.
[00:45:10.560 --> 00:45:13.440]   - I think just to step in,
[00:45:13.440 --> 00:45:15.080]   I think a little bit of the confusion there
[00:45:15.080 --> 00:45:16.440]   was a few people didn't realize
[00:45:16.440 --> 00:45:19.980]   that you just need similar data to what was trained on,
[00:45:19.980 --> 00:45:23.080]   not the exact, it's not like I need a copy of CIFAR-10
[00:45:23.080 --> 00:45:23.920]   and then I can figure out
[00:45:23.920 --> 00:45:26.480]   they used this example from CIFAR-10.
[00:45:26.480 --> 00:45:28.400]   That's just a simple example.
[00:45:28.400 --> 00:45:30.420]   I just need to have something equivalent.
[00:45:30.420 --> 00:45:31.880]   And that's, I think, a really cool,
[00:45:31.880 --> 00:45:33.880]   I had never seen this kind of attack before,
[00:45:33.880 --> 00:45:35.240]   and I think it's really cool
[00:45:35.240 --> 00:45:36.840]   that you're able to be so successful.
[00:45:36.840 --> 00:45:39.000]   - Yeah, thanks for clarifying that point.
[00:45:39.000 --> 00:45:40.880]   Of course, we don't have access to the same data,
[00:45:40.880 --> 00:45:42.880]   otherwise there's no point of the attack.
[00:45:42.880 --> 00:45:48.560]   But yeah, we have access to some similar data, yes.
[00:45:48.560 --> 00:45:51.180]   But I just use CIFAR-10 there for simplicity.
[00:45:51.180 --> 00:45:55.280]   - And then Joseph also asked,
[00:45:55.280 --> 00:46:00.280]   is this method of member attack networks
[00:46:00.280 --> 00:46:04.160]   used in all state-of-the-art models, or just like?
[00:46:04.160 --> 00:46:08.040]   - Well, of course, models that are well generalized,
[00:46:08.040 --> 00:46:11.680]   it's much more difficult, but they are not unhackable.
[00:46:11.680 --> 00:46:13.600]   - Nice.
[00:46:13.600 --> 00:46:16.960]   So Kayla asked, actually,
[00:46:16.960 --> 00:46:19.120]   this question is coming from YouTube.
[00:46:19.120 --> 00:46:21.480]   First of all, they said, "Thanks for doing this.
[00:46:21.480 --> 00:46:24.160]   "We need more efforts to make machine learning accessible
[00:46:24.160 --> 00:46:27.560]   "and privacy-preserving, particularly."
[00:46:27.560 --> 00:46:31.080]   And then he asked, "Any efforts to implement these concepts
[00:46:31.080 --> 00:46:35.000]   "with distributed ledger technology, like blockchain?"
[00:46:35.000 --> 00:46:37.040]   - I did not get the question.
[00:46:37.040 --> 00:46:39.280]   Any efforts for?
[00:46:39.280 --> 00:46:41.480]   - Any efforts to implement these concepts
[00:46:41.480 --> 00:46:46.480]   using a blockchain in a decentralized fashion, basically?
[00:46:46.480 --> 00:46:49.160]   - Oh, I'm not really familiar with blockchain,
[00:46:49.160 --> 00:46:51.620]   but I know some of my colleagues have been borrowing
[00:46:51.620 --> 00:46:53.360]   some of the approaches there
[00:46:53.360 --> 00:46:55.880]   to be implemented in federated learning.
[00:46:55.880 --> 00:46:58.160]   So something similar is happening,
[00:46:58.160 --> 00:47:02.920]   but I cannot really answer on that.
[00:47:02.920 --> 00:47:04.640]   - Something similar is happening in blockchain.
[00:47:04.640 --> 00:47:05.800]   - Yep, exactly.
[00:47:05.800 --> 00:47:09.200]   Somebody is building something crazy out there, but yeah.
[00:47:09.200 --> 00:47:11.640]   - Whether it works or not remains to be seen.
[00:47:11.640 --> 00:47:16.600]   So someone asked, "What is the ground truth
[00:47:16.600 --> 00:47:18.720]   "for the ATT&CK network?"
[00:47:18.720 --> 00:47:20.360]   And this was from Vijay.
[00:47:20.360 --> 00:47:24.400]   - Ground truth of the ATT&CK network means?
[00:47:24.400 --> 00:47:27.680]   - Could you elaborate on that question, Vijay?
[00:47:27.680 --> 00:47:29.360]   There's another one that I'll go to.
[00:47:29.360 --> 00:47:30.600]   - Where can I see the questions?
[00:47:30.600 --> 00:47:31.760]   Is that in the Q&A?
[00:47:31.760 --> 00:47:33.320]   - You can see them in the chat.
[00:47:33.320 --> 00:47:36.320]   (keyboard clacking)
[00:47:36.320 --> 00:47:43.520]   - And then the last question that I see here is,
[00:47:43.520 --> 00:47:47.000]   "Do you know how to implement capsule networks?
[00:47:47.000 --> 00:47:48.560]   "If so, could you talk about them?"
[00:47:48.560 --> 00:47:50.880]   - I think that's not really the topic.
[00:47:50.880 --> 00:47:54.280]   That's more of a different topic.
[00:47:54.280 --> 00:47:59.280]   But I think probably the new TensorFlow documentation
[00:47:59.280 --> 00:48:01.600]   has some examples about that.
[00:48:02.600 --> 00:48:05.760]   - So the other question that we didn't quite get clarity
[00:48:05.760 --> 00:48:07.440]   around was essentially just,
[00:48:07.440 --> 00:48:10.800]   "What is the ground truth for ATT&CK networks?"
[00:48:10.800 --> 00:48:15.800]   So is there like a right answer, I mean,
[00:48:15.800 --> 00:48:18.080]   for what the ATT&CK networks are aiming for?
[00:48:18.080 --> 00:48:22.280]   - The ground truth of the ATT&CK,
[00:48:22.280 --> 00:48:26.160]   we are creating our own dataset,
[00:48:26.160 --> 00:48:28.160]   and that's our ground truth.
[00:48:28.160 --> 00:48:29.720]   We're using part of the dataset
[00:48:29.720 --> 00:48:32.520]   and training the shadow model,
[00:48:32.520 --> 00:48:36.120]   and we keep another part of the training dataset
[00:48:36.120 --> 00:48:38.520]   for inference purposes only,
[00:48:38.520 --> 00:48:40.000]   and that's our ground truth.
[00:48:40.000 --> 00:48:42.640]   - Gotcha.
[00:48:42.640 --> 00:48:43.840]   Cool.
[00:48:43.840 --> 00:48:45.720]   I think those are all the questions.
[00:48:45.720 --> 00:48:48.040]   Also Vijay, if you had follow-ups,
[00:48:48.040 --> 00:48:51.240]   you can ask Gaurav in our stack maybe.
[00:48:51.240 --> 00:48:52.920]   I don't know if you're on there yet,
[00:48:52.920 --> 00:48:54.760]   but I'm gonna make sure that you do get on there
[00:48:54.760 --> 00:48:55.600]   after this.
[00:48:55.600 --> 00:48:59.600]   - Sure, yeah, just a quick question.
[00:48:59.600 --> 00:49:03.360]   Google my name or just Twitter.
[00:49:03.360 --> 00:49:05.040]   How do we say Twitter me?
[00:49:05.040 --> 00:49:06.360]   Find me on Twitter.
[00:49:06.360 --> 00:49:07.200]   - Tweet him.
[00:49:07.200 --> 00:49:08.440]   That's good.
[00:49:08.440 --> 00:49:09.880]   Thank you so much for coming.
[00:49:09.880 --> 00:49:10.720]   - Yeah, of course.

