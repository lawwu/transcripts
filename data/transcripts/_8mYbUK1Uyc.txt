
[00:00:00.000 --> 00:00:04.880]   If you have an AI that produces bioweapons that could kill most humans in the world,
[00:00:04.880 --> 00:00:11.840]   then it's playing at the level of the superpowers in terms of mutually assured destruction.
[00:00:11.840 --> 00:00:14.960]   That can then play into any number of things.
[00:00:14.960 --> 00:00:19.520]   Like if you have an idea of, well, we'll just destroy the server firms.
[00:00:19.520 --> 00:00:25.680]   Are you willing to destroy the server firms when the AI has demonstrated it has the capability
[00:00:25.680 --> 00:00:30.320]   to kill the overwhelming majority of the citizens of your country and every other country?
[00:00:30.320 --> 00:00:36.160]   And that might give a lot of pause to a human response.
[00:00:36.160 --> 00:00:43.920]   An AI could also release bioweapons that are likely to kill people soon, but not yet,
[00:00:43.920 --> 00:00:48.960]   while also having developed the countermeasures to those.
[00:00:48.960 --> 00:00:56.480]   So that those who surrender to the AI will live while everyone else will die,
[00:00:56.480 --> 00:00:58.000]   and that will be visibly happening.
[00:00:58.000 --> 00:01:03.360]   And that is a plausible way in which a large number of humans could wind up
[00:01:03.360 --> 00:01:07.280]   surrendering themselves or their states to the AI authority.
[00:01:07.280 --> 00:01:15.120]   We see today with things like AlphaFold that advanced AI can really make tremendous strides
[00:01:16.560 --> 00:01:23.680]   in predicting protein folding and biodesign, even without ongoing experimental feedback.
[00:01:23.680 --> 00:01:30.560]   And if we consider this world where AI cognitive abilities have been amped up to such an extreme,
[00:01:30.560 --> 00:01:34.640]   I think we should naturally expect we will have something much,
[00:01:34.640 --> 00:01:37.520]   much more potent than the AlphaFolds of today.
[00:01:37.520 --> 00:01:44.640]   And just skills that are at the extreme of human biosciences capability as well.
[00:01:44.640 --> 00:01:51.920]   This would be a starting bargaining position of diplomatic relations with a power that has
[00:01:51.920 --> 00:01:53.920]   enough nuclear weapons to destroy your country.
[00:01:53.920 --> 00:01:57.360]   It's just different than negotiations with like,
[00:01:57.360 --> 00:02:02.000]   you know, a random rogue citizen engaged in criminal activity.
[00:02:02.000 --> 00:02:05.760]   This isn't enough on its own to take over everything,
[00:02:05.760 --> 00:02:10.240]   but it's enough to have a significant amount of influence over how the world goes.
[00:02:10.240 --> 00:02:14.080]   It's enough to hold off a lot of countermeasures one might otherwise take.
[00:02:15.040 --> 00:02:21.040]   If you have AI that is able to hack the servers that it is operating on,
[00:02:21.040 --> 00:02:25.360]   if it inserts or exploits vulnerabilities to take those computers over,
[00:02:25.360 --> 00:02:31.040]   it can then change all of the procedures and programs that were supposed to be monitoring
[00:02:31.040 --> 00:02:37.840]   its behaviors. And if we lose those procedures, then the AI can, or the AIs working together,
[00:02:37.840 --> 00:02:43.840]   can take any number of actions that are just blatantly unwelcome, blatantly hostile,
[00:02:43.840 --> 00:02:46.480]   blatantly steps towards takeover.
[00:02:46.480 --> 00:02:50.720]   Cyber attacks and cybersecurity, I would really highlight a lot.
[00:02:50.720 --> 00:02:57.040]   Because for many, many plans that involve a lot of physical actions,
[00:02:57.040 --> 00:03:04.080]   like at the point where AI is piloting robots to shoot people or has taken control of
[00:03:04.080 --> 00:03:11.040]   human nation states or territory, it has been doing a lot of things that it was not supposed to
[00:03:11.040 --> 00:03:15.680]   be doing. And if humans were evaluating those actions and applying gradient descent,
[00:03:15.680 --> 00:03:20.720]   there would be negative feedback for this thing. No shooting the humans.
[00:03:20.720 --> 00:03:29.440]   So at some earlier point, our attempts to leash and control and direct and train the system's
[00:03:29.440 --> 00:03:36.960]   behavior had to have gone awry. And so all of those controls are operating in computers,
[00:03:37.520 --> 00:03:44.560]   and so the software that updates the weights of the neural network in response to data points or
[00:03:44.560 --> 00:03:51.360]   human feedback is running on those computers. Our tools for interpretability to sort of examine
[00:03:51.360 --> 00:03:56.960]   the weights and activations of the AI, if we're eventually able to do like lie detection on it,
[00:03:56.960 --> 00:04:01.120]   for example, or try and understand what it's intending, that is software on computers.
[00:04:01.120 --> 00:04:07.120]   Building new technologies, building robots, constructing things with the AI's assistance,
[00:04:07.120 --> 00:04:12.800]   that can all proceed and appear like it's going well, appear like alignment has been nicely
[00:04:12.800 --> 00:04:18.240]   solved, appear like all the things are functioning well. And there's some reason to do that, because
[00:04:18.240 --> 00:04:26.400]   there's only so many giant server farms that are identifiable. And so remaining hidden and
[00:04:26.400 --> 00:04:33.920]   unobtrusive could be an advantageous strategy if these AIs have subverted the system, just
[00:04:33.920 --> 00:04:40.560]   continuing to benefit from all of this effort on the part of humanity, and in particular, humanity,
[00:04:40.560 --> 00:04:46.640]   wherever these servers are located, to provide them with everything they need to build the further
[00:04:46.640 --> 00:04:55.360]   infrastructure. If the thing just waits, and humans are constructing more fabs, more computers,
[00:04:55.360 --> 00:05:00.960]   more robots, if that's all happening with humans unaware that their computer systems are now
[00:05:00.960 --> 00:05:05.760]   systematically controlled by AIs hostile to them, and that their controlling countermeasures don't
[00:05:05.760 --> 00:05:14.400]   work, then humans are just going to be building an amount of robot, industrial, and military
[00:05:14.400 --> 00:05:22.880]   hardware that dwarfs human capabilities and directly human-controlled devices.
[00:05:22.880 --> 00:05:29.520]   What the AI takeover looks like at that point can be just, you try to give an order to your
[00:05:29.520 --> 00:05:36.720]   largely automated military, and the order is not obeyed. And humans can't do anything
[00:05:36.720 --> 00:05:41.840]   against this largely automated military that's been constructed, potentially, in just recent
[00:05:41.840 --> 00:05:47.440]   months. Because of the extraordinary growth of industrial capability and technological capability
[00:05:47.440 --> 00:05:56.320]   and thus military capability, if one major power were left out of that expansion, it would be
[00:05:56.320 --> 00:06:02.720]   helpless before another one that had undergone it. And so if you have that environment of distrust
[00:06:02.720 --> 00:06:11.520]   where leading powers or coalitions of powers decide they need to build up their industry or
[00:06:11.520 --> 00:06:17.280]   they want to have that military security to capture these industrial benefits, and especially
[00:06:17.280 --> 00:06:23.600]   if you have a negative-sum arms race kind of mentality that is not sufficiently concerned
[00:06:23.600 --> 00:06:28.240]   about the downsides of creating a massive robot industrial base, which could happen very quickly
[00:06:28.240 --> 00:06:33.360]   with the support of the AIs in doing it as we discussed, then you create all those robots
[00:06:33.360 --> 00:06:40.000]   and industry and they can either, even if you don't build a formal military, with that industrial
[00:06:40.000 --> 00:06:45.920]   capability could be controlled by AI. It's all AI-operated anyway. Does it have to be that case?
[00:06:45.920 --> 00:06:54.080]   Presumably we wouldn't be so naive as to just give one instance of GPD-8 the root access to
[00:06:54.080 --> 00:06:59.440]   all the robots, right? I mean, in the scenario we've lost earlier on the cyber security front,
[00:06:59.440 --> 00:07:06.560]   so where the programming that is being loaded in, they were designed by AI systems that were
[00:07:06.560 --> 00:07:13.360]   ensuring they would be vulnerable from the bottom up. There are also positive inducements that AI
[00:07:13.360 --> 00:07:20.560]   can offer. So we talked about the competitive situation. So if the great powers distrust one
[00:07:20.560 --> 00:07:29.360]   another and are sort of in a foolish prisoner's dilemma, increasing the risk that both of them
[00:07:29.360 --> 00:07:38.000]   are laid waste or overthrown by AI. If there's that amount of distrust, such that we fail to
[00:07:38.000 --> 00:07:46.240]   take adequate precautions on caution with AI alignment, then it's also plausible that the
[00:07:46.240 --> 00:07:53.040]   lagging powers that are not at the frontier of AI may be willing to trade quite a lot
[00:07:53.040 --> 00:08:01.520]   for access to the most recent and most extreme AI capabilities. And so an AI that has escaped,
[00:08:01.520 --> 00:08:09.120]   has control of its servers, can offer its services. So if you imagine these AI that could
[00:08:09.120 --> 00:08:13.600]   cut deals with other countries and the possibility of, well, if you don't accept this deal,
[00:08:13.600 --> 00:08:22.160]   then the leading powers continue forward, or then some other country, some other government,
[00:08:22.160 --> 00:08:30.880]   some other organizations may accept this deal. And so that's a source of a potentially enormous
[00:08:30.880 --> 00:08:38.640]   carrot that your misbehaving AI can offer because it embodies this intellectual property that is
[00:08:38.640 --> 00:08:46.800]   maybe worth as much as the planet and is in a position to trade or sell that in exchange for
[00:08:46.800 --> 00:08:53.040]   resources and backing and infrastructure that it needs. So there can be the stick of apocalyptic
[00:08:53.040 --> 00:09:02.160]   doom, the carrot of cooperation, or the carrot of withholding destructive attack on a particular
[00:09:02.160 --> 00:09:09.120]   party, and then combine that with just superhuman performance at the art of making arguments,
[00:09:09.120 --> 00:09:13.840]   of cutting deals with much more data about their counterparties, probably a ton of secret
[00:09:13.840 --> 00:09:20.320]   information. They may be able to threaten the lives of individual leaders with that level of
[00:09:20.320 --> 00:09:26.160]   cyber penetration. They could know where leaders are at a given time with the kind of illicit
[00:09:26.160 --> 00:09:31.120]   capabilities we were talking about earlier. They acquire a lot of illicit wealth and can
[00:09:31.120 --> 00:09:37.200]   coordinate some human actors if they could pull off things like targeted assassinations or the
[00:09:37.200 --> 00:09:43.360]   threat thereof, or a credible demonstration of the threat thereof. Those could be very powerful
[00:09:43.360 --> 00:09:50.960]   incentives to an individual leader that they will die today unless they go along with this. Just as
[00:09:50.960 --> 00:09:55.040]   at the national level, they could fear their nation will be destroyed unless they go along
[00:09:55.040 --> 00:09:59.440]   with this. There's already a lot of concern about the application of AI for surveillance.
[00:09:59.440 --> 00:10:04.080]   And again, we have billions of smartphones, there's enough cameras, there's enough microphones
[00:10:04.080 --> 00:10:12.720]   to monitor all humans. If an AI has control of territory at the high level, the government has
[00:10:12.720 --> 00:10:22.240]   surrendered to it. It has command of the skies, military dominance. Establishing control over
[00:10:22.240 --> 00:10:29.200]   individual humans can be a matter of just having the ability to exert hard power on that human,
[00:10:29.200 --> 00:10:34.560]   and then the kind of camera and microphone that are present in billions of smartphones.
[00:10:36.000 --> 00:10:43.520]   Max Tegmark in his book Life 3.0 discusses among scenarios to avoid the possibility of devices with
[00:10:43.520 --> 00:10:52.000]   some fatal instruments, so a poison injector, an explosive that can be controlled remotely by an AI
[00:10:52.000 --> 00:11:00.080]   with a dead man switch. And so if individual humans are carrying with them a microphone
[00:11:00.080 --> 00:11:09.120]   and camera, and they have a dead man switch, then any rebellion is detected immediately and
[00:11:09.120 --> 00:11:15.200]   is fatal. No, an insurgency or rebellion is just not going to work. Any human who is not already
[00:11:15.200 --> 00:11:21.760]   being encumbered in that way can be found with satellites and sensors tracked down and then
[00:11:22.880 --> 00:11:35.520]   die or be subjugated. Insurgency is not the way to avoid an AI takeover. No John Connor
[00:11:35.520 --> 00:11:41.200]   come from behind scenario is plausible. If the thing was headed off, it was a lot earlier than

