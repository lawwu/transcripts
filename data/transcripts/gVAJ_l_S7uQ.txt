
[00:00:00.000 --> 00:00:06.240]   Today I want to talk about something a little bit different and instead of focusing on a
[00:00:06.240 --> 00:00:14.080]   specific topic I want to actually talk about how I would learn machine learning and in particular
[00:00:14.080 --> 00:00:22.880]   NLP if I was starting from scratch. I've tried to keep this almost like guide as simple as possible
[00:00:23.840 --> 00:00:33.680]   because I think a lot of learning in terms of machine learning, data science, NLP, whatever
[00:00:33.680 --> 00:00:44.240]   else really needs to come from your own interests and so I think a very strict order of things to do
[00:00:44.240 --> 00:00:52.160]   can almost ruin that learning experience and so it's very important to just try and
[00:00:52.160 --> 00:00:58.560]   kind of figure things out as you go but at the same time I do think it's important to
[00:00:58.560 --> 00:01:09.440]   be aware of maybe a few particular courses or resources or books that have been very helpful
[00:01:09.440 --> 00:01:16.320]   for others in the past and therefore could also help you. So I've broken it down into
[00:01:17.120 --> 00:01:24.880]   kind of four parts so there's the machine learning 101, applied ML, the origin of what I would say
[00:01:24.880 --> 00:01:34.960]   modern NLP and also just modern NLP and then after that I've also added in another bit so
[00:01:34.960 --> 00:01:41.360]   this is obviously very specific and we'll just mention this at the end but we won't
[00:01:41.360 --> 00:01:49.680]   really go into too much depth on it. So each of these parts here act as almost like a stage in
[00:01:49.680 --> 00:01:58.240]   your journey of learning and of course at the start you actually need to learn about machine
[00:01:58.240 --> 00:02:09.600]   learning and this is almost not even the start. So yes you would start here but you also need to learn
[00:02:09.600 --> 00:02:17.200]   Python for example. So in reality it might actually look something like this. So you would
[00:02:17.200 --> 00:02:24.320]   start with ML 101 and at the same time in parallel you might want to be learning Python. Now the
[00:02:24.320 --> 00:02:30.240]   reason I say in parallel rather than just starting with Python is more to do with whether you're
[00:02:30.240 --> 00:02:38.480]   interested in that or not because for me I started Python with the specific interest of
[00:02:38.480 --> 00:02:45.040]   like algorithmic trading and then very quickly that moved into sort of more machine learning.
[00:02:45.040 --> 00:02:52.400]   But learning Python for the sake of Python I wasn't necessarily that interested in although
[00:02:52.400 --> 00:02:57.840]   I will say that once I got started I very quickly became very obsessed with just coding random
[00:02:57.840 --> 00:03:07.120]   things. So in my personal opinion and that's not necessarily going to be right for everyone,
[00:03:07.120 --> 00:03:14.800]   I would kind of learn those two things in parallel because you have to learn the Python side for the
[00:03:14.800 --> 00:03:20.720]   machine learning side but they both complement each other and having both of them in parallel
[00:03:20.720 --> 00:03:27.040]   rather than just doing Python will probably keep you more interested than just focusing on one at
[00:03:27.040 --> 00:03:37.680]   a time. So in terms of resources for just Python I use Codecademy more than anything else. Now when
[00:03:37.680 --> 00:03:46.240]   I used it it was I think cheaper than it is now. So something to be aware of but there are so many
[00:03:46.240 --> 00:03:52.400]   good resources out there for learning Python. You probably aren't going to struggle to find anything.
[00:03:52.400 --> 00:04:00.000]   The other thing I would say is also try projects. So just building projects will help your coding
[00:04:00.000 --> 00:04:07.440]   skills and every other component on here so much because it shows you where you're missing
[00:04:07.440 --> 00:04:14.880]   knowledge and also helps you learn how to apply what you have learned. And at the same time I
[00:04:14.880 --> 00:04:19.840]   think it's very motivating to build something. At least for me I find it incredibly motivating to
[00:04:19.840 --> 00:04:25.120]   just build something cool that I find interesting. So that's another thing that I would definitely
[00:04:25.120 --> 00:04:32.240]   recommend on the Python side. So yeah that's a prerequisite almost or something you do in
[00:04:32.240 --> 00:04:39.040]   parallel to what I have here. So machine learning 101 is really kind of an introduction to machine
[00:04:39.040 --> 00:04:45.680]   learning and where you would learn about key concepts. So you have neural networks up here,
[00:04:45.680 --> 00:04:51.520]   loss optimization, activation functions, and there is a lot more than just that in there.
[00:04:51.520 --> 00:05:00.720]   So with all of these resources you can mostly get them for free. So neural networks from scratch is
[00:05:00.720 --> 00:05:06.560]   very good by Sendex. And Sendex in general is also very good especially when you're starting Python.
[00:05:06.560 --> 00:05:12.000]   That's probably another thing to add to the Python prerequisites. You can also learn from
[00:05:12.000 --> 00:05:17.600]   Sendex. That's one of the places I learned as well. So neural networks from scratch is
[00:05:17.600 --> 00:05:25.040]   in my opinion a really good resource and it's one of those things that wasn't around when I was
[00:05:25.040 --> 00:05:33.600]   learning this stuff but I really wish it had been. And so much so that I bought it anyway and it is
[00:05:33.600 --> 00:05:41.360]   genuinely incredible. Now you don't need to buy the book as well. The book is great and I like
[00:05:41.360 --> 00:05:48.320]   having something physical but you can also do all this stuff online as well. Or not all of it. I
[00:05:48.320 --> 00:05:55.200]   think the first parts of the book has already been covered on Sendex's YouTube channel. So
[00:05:55.200 --> 00:06:02.880]   where are we? Where do we find that? I think it's just YouTube Sendex. Yeah so just Sendex.
[00:06:02.880 --> 00:06:08.240]   And you can find on here he has here we are it's literally right at the top neural networks from
[00:06:08.240 --> 00:06:12.480]   scratch intro neural code. And this is really nice as well because you have a lot of visuals
[00:06:12.480 --> 00:06:18.720]   in here and obviously Sendex is talking through it which is really nice. But of course also the
[00:06:18.720 --> 00:06:26.400]   book is is really incredible. And the reason I like it is because it's going through the code
[00:06:26.400 --> 00:06:32.240]   of everything making everything very visual but breaking everything down. Because what you'll find
[00:06:32.240 --> 00:06:38.880]   later on in machine learning that everything is very abstract but it's so important to understand
[00:06:38.880 --> 00:06:48.160]   the core concepts behind all of these things. So we can see here like he's really breaking down
[00:06:48.160 --> 00:06:55.360]   everything. So this looks horrible but this is a really nice way to actually just understand what
[00:06:55.360 --> 00:07:02.640]   is going on in a neural network. And then later on with like actually coding and keeping things
[00:07:02.640 --> 00:07:08.880]   really simple like inputs weights. And the issue is that in most cases most tutorials they go
[00:07:08.880 --> 00:07:19.040]   too abstract too quickly. Whereas in this book and on the video tutorials Sendex doesn't go
[00:07:20.240 --> 00:07:26.160]   too abstract too quickly. He builds it up from nothing which is really nice especially if you're
[00:07:26.160 --> 00:07:32.640]   just starting out. So that's one in my opinion very good resource for when you're starting out.
[00:07:32.640 --> 00:07:40.080]   Another one that it seems almost like everyone in machine learning or data science has done this
[00:07:40.080 --> 00:07:48.000]   course at some point. And it is I will be honest it is outdated but not so much in it's not too
[00:07:48.000 --> 00:07:53.920]   outdated it's still very valuable. So the things that you learn on this course very much focus on
[00:07:53.920 --> 00:08:00.960]   the concepts behind much of the older stuff in machine learning and particularly the things that
[00:08:00.960 --> 00:08:08.240]   make machine learning what it is today. Now one of the main reasons I say this is outdated is that it
[00:08:08.240 --> 00:08:15.440]   uses a programming language called Octave which is almost like an open source MATLAB if any of you
[00:08:15.440 --> 00:08:24.640]   know what MATLAB is. So in reality it is actually very similar to Python syntax so it's not so much
[00:08:24.640 --> 00:08:31.600]   of a bad thing that you use Octave because it translates over very easily. And one thing that
[00:08:31.600 --> 00:08:35.760]   is great about this course is that you can actually enroll for free. I don't know if it's
[00:08:35.760 --> 00:08:42.560]   free the entire way through as it's been ages since I did this course but either way you can
[00:08:42.560 --> 00:08:48.880]   audit almost everything on Coursera for free it just means you don't get a certificate at the end
[00:08:48.880 --> 00:08:55.440]   but that's completely fine. And if you really wanted a certificate at the end of it you can
[00:08:55.440 --> 00:09:00.880]   obviously pay for it. But this is an incredible course and you can see some of the things that
[00:09:00.880 --> 00:09:06.000]   go through linear algebra this is so important just a little bit of understanding of the maths
[00:09:06.000 --> 00:09:16.240]   behind all this to start with logistic regression, regularization, they go through so many important
[00:09:16.240 --> 00:09:22.960]   concepts that honestly like absolutely crucial for later on. So this is you know without a doubt
[00:09:22.960 --> 00:09:30.480]   one of the best courses when you're getting started I think it's really really good.
[00:09:30.480 --> 00:09:35.200]   And then the final one that I wanted to talk about is a 100-page machine learning book.
[00:09:35.200 --> 00:09:41.600]   Again obviously this is a book I do think you can get it online as far as I know.
[00:09:41.600 --> 00:09:50.000]   So I'd have a look at that it at one point you could for sure there's a pdf but this is really
[00:09:50.000 --> 00:09:55.760]   nice because it's it's 100 pages well I think I think maybe there's a few more than 100.
[00:09:55.760 --> 00:10:06.480]   Yeah okay so it goes kind of over 100 pages but it because it's such such a small number of pages
[00:10:06.480 --> 00:10:14.800]   everything in here is is very important so it's kind of stripped down everything you need just
[00:10:14.800 --> 00:10:21.760]   so you have all the important things. So we can go ahead maybe find something interesting here.
[00:10:21.760 --> 00:10:30.720]   Let's see what whatever this is here that I made a note of a long time ago.
[00:10:30.720 --> 00:10:38.880]   Okay so I'm looking at like just classification first regression this is like just a few paragraphs
[00:10:38.880 --> 00:10:42.000]   explaining the difference between these two incredibly important things.
[00:10:42.000 --> 00:10:50.560]   There we have parameters versus high parameters it's just so much important stuff and you have
[00:10:50.560 --> 00:10:57.920]   charts and visuals which I personally really like to have visuals in everything as much as possible.
[00:10:57.920 --> 00:11:08.160]   So this is another really really cool book that I would 100% recommend it's also code as well
[00:11:08.160 --> 00:11:17.520]   is it python yeah so that's really nice in my opinion and once you have gone past that
[00:11:17.520 --> 00:11:24.480]   you kind of want to start looking at applied machine learning. So this is the only thing
[00:11:24.480 --> 00:11:29.360]   on this list I haven't personally gone through although I have watched a few videos from this
[00:11:29.360 --> 00:11:35.840]   and the reason I recommend it is because Daniel Burke is absolutely incredible at taking
[00:11:35.840 --> 00:11:41.520]   machine learning and stripping down everything you don't need to know and just keeping what you do
[00:11:41.520 --> 00:11:50.960]   need to know and teaching you in an incredibly entertaining way. So this boot camp course as
[00:11:50.960 --> 00:11:57.040]   well you can I think possibly also find it on Udemy it has an incredibly high rating every
[00:11:57.040 --> 00:12:01.280]   course that Daniel Burke has done is incredibly high rating people are super happy with it
[00:12:01.280 --> 00:12:10.640]   and it's really friendly to beginners so that's why I recommend this even though I haven't been
[00:12:10.640 --> 00:12:17.040]   all the way through it myself but either way I'm very confident that this is a very good resource
[00:12:17.040 --> 00:12:23.280]   for those of you that are interested now but of course it's up to you and another thing that I
[00:12:23.280 --> 00:12:28.800]   would say here is okay I think it's very important to learn this sort of thing
[00:12:28.800 --> 00:12:35.120]   but at the same time if you're if you're really just kind of impatient and I mean it's probably
[00:12:35.120 --> 00:12:40.880]   best you do it but if you are super impatient fine you can probably skip it and move on to the
[00:12:40.880 --> 00:12:49.280]   modern NLP stuff because a lot of the code in modern NLP is actually quite simple so you can
[00:12:49.280 --> 00:12:55.200]   maybe drop that bit just as long as you know you're aware that there's going to be a lot of
[00:12:55.200 --> 00:13:00.640]   things that come up where you're going to be quite confused like pandas and numpy if you haven't seen
[00:13:00.640 --> 00:13:05.600]   them before but nonetheless you will learn a bit of numpy in these early ones especially neural
[00:13:05.600 --> 00:13:12.480]   networks from scratch so it's kind of possible to skip it if you if you're just impatient
[00:13:12.480 --> 00:13:17.120]   which is fine I'm impatient all the time as well I just want to learn new things
[00:13:17.120 --> 00:13:24.800]   so then the next thing is if you're really going for NLP I think it's really good to learn
[00:13:25.520 --> 00:13:33.040]   what is current and present in NLP but at the same time to really understand any of that it's
[00:13:33.040 --> 00:13:40.240]   very important to understand where it came from so I would definitely recommend
[00:13:40.240 --> 00:13:52.240]   Sanford CS224N it's incredible on YouTube that will take you through so many like incredibly
[00:13:52.240 --> 00:13:59.680]   useful things in NLP that are taught by some of the like best people in the world
[00:13:59.680 --> 00:14:08.480]   and it's very relevant it's incredibly useful the only thing is at least when I went through this
[00:14:08.480 --> 00:14:13.280]   course which to be fair was a few years ago they didn't really have anything on the most recent
[00:14:13.280 --> 00:14:20.960]   stuff in NLP that might have changed now and that is why I have kept it over here because
[00:14:21.520 --> 00:14:28.080]   if they do have anything on the more recent stuff in NLP I think it's very little so that's why I
[00:14:28.080 --> 00:14:34.480]   kept it in sort of the more the orange origin stuff because it mainly seems to focus on things
[00:14:34.480 --> 00:14:40.640]   that you see here like word vectors recurrent neural networks function gradient and maybe
[00:14:40.640 --> 00:14:45.120]   they go into attention near the end although I can't say for sure there and the next thing that
[00:14:45.120 --> 00:14:50.160]   I would recommend and that I also did so I did this course and I found this to be very good
[00:14:50.800 --> 00:14:56.240]   over on Coursera again I do believe if you look in like enrollment options down here
[00:14:56.240 --> 00:15:05.280]   or come down here you can audit the course can I take the course for free you can audit the course
[00:15:05.280 --> 00:15:10.880]   for free see now I'm not sure how you do that it's somewhere on here they keep it kind of hidden
[00:15:10.880 --> 00:15:18.400]   but you can audit it for free so that's incredibly useful and this is a really good course it's
[00:15:18.400 --> 00:15:26.240]   actually a specialization so it's multiple courses in one again they don't really go into the more
[00:15:26.240 --> 00:15:31.680]   modern stuff although I do know that they I believe actually paired up with Hugging Face
[00:15:31.680 --> 00:15:42.160]   which is like modern NLP and did introduce some of that at some point so yeah sort of attention
[00:15:42.160 --> 00:15:51.680]   models yeah you know you know brought in t5 invert so and reformer models as well so that's that's
[00:15:51.680 --> 00:15:57.120]   pretty cool so even at the end of this you will get to learn some of the more recent NLP stuff
[00:15:57.120 --> 00:16:06.240]   so then the final bit is modern NLP so in here there are a few things so how transformers work
[00:16:06.240 --> 00:16:12.320]   so this is an article I wrote and that of course there'll be links to this that you can access for
[00:16:12.320 --> 00:16:21.840]   free and in this I just wanted to explain where transformers came from in terms of going from
[00:16:21.840 --> 00:16:26.800]   recurrent neural networks through to adding attention to those recurrent neural networks
[00:16:26.800 --> 00:16:34.240]   encoded decoder models and how that tension works and I mean obviously I'm biased but I think this
[00:16:34.240 --> 00:16:42.720]   is a relatively good summary of what a transformer model is and transform models are really the
[00:16:42.720 --> 00:16:50.160]   foundation of modern NLP they're incredibly important so I think that's quite good just
[00:16:50.160 --> 00:16:54.480]   to read through that and try and understand what is actually going on and then there's
[00:16:54.480 --> 00:17:01.920]   two courses here so there's my course here you can always find discount on this but I don't
[00:17:01.920 --> 00:17:06.960]   necessarily think you need to even pay because there's a Hugging Face course and the Hugging
[00:17:06.960 --> 00:17:12.320]   Face course will cover everything I cover in my course anyway and more the only thing is that
[00:17:12.320 --> 00:17:17.280]   maybe my course is maybe a little more applied and maybe a little more concise in some parts
[00:17:17.280 --> 00:17:22.480]   and as well a lot of the stuff I cover in my course I also cover on YouTube for free
[00:17:22.480 --> 00:17:29.760]   so you just have a bit more of a structure I suppose so the Hugging Face course is like I said
[00:17:29.760 --> 00:17:37.280]   incredible you have a nice little summary of the course here which I can't can I zoom in that's
[00:17:37.280 --> 00:17:42.960]   probably the closest I'm going to get so you have an introduction diving in and more advanced stuff
[00:17:42.960 --> 00:17:47.680]   and you go through everything so you're using transformers Hugging Face they have multiple
[00:17:47.680 --> 00:17:54.880]   libraries for NLP transformers library datasets library tokenizers library they're all very good
[00:17:54.880 --> 00:18:02.960]   I would definitely recommend this as that sort of starting point or as the almost the foundation
[00:18:02.960 --> 00:18:12.800]   of where you learn your modern NLP techniques so that's the modern NLP stuff and yeah as I
[00:18:12.800 --> 00:18:17.440]   have written here I think at this point you're definitely in a good position to do some pretty
[00:18:17.440 --> 00:18:22.880]   interesting projects so at this point I would expect you've probably done some projects anyway
[00:18:23.520 --> 00:18:28.560]   but if you haven't then for sure at this point you really should do some projects
[00:18:28.560 --> 00:18:35.760]   so these are just projects that I've done in the past and actually showed you how to do
[00:18:35.760 --> 00:18:42.000]   on YouTube so there's building transform models from scratch I think this is really useful to
[00:18:42.000 --> 00:18:50.320]   understand how all of this works and there's a few things in there so this is like a building an
[00:18:50.320 --> 00:18:58.080]   Italian BERT model from from scratch and now I do have other series as well on that sort of thing
[00:18:58.080 --> 00:19:04.320]   and there's also another series I did here which again is the same it's like a Q&A app
[00:19:04.320 --> 00:19:13.680]   when I used to have that awful GoPro camera not very nice but anyway they are at least useful
[00:19:13.680 --> 00:19:19.600]   I hope as a guide or as inspiration to what sort of projects you can do so I mean that's really
[00:19:19.600 --> 00:19:24.960]   you know when it comes to NLP that's probably as much as you want to go but I will mention
[00:19:24.960 --> 00:19:31.840]   of course a lot of what I do is going to sort of semantic search and information retrieval so if
[00:19:31.840 --> 00:19:37.760]   that's the sort of thing that you're interested in then of course there are other resources that
[00:19:37.760 --> 00:19:44.240]   you can go through for that and these are all free if you want them to be free so the only one that
[00:19:44.240 --> 00:19:49.680]   isn't sometimes free is this bug here which we'll talk about in a minute but there are these two
[00:19:49.680 --> 00:19:56.800]   here which are just series that I've written and if you go on here you can see there's quite a line
[00:19:56.800 --> 00:20:03.920]   here and it's all free so it's quite I think it's it's a good deal so you come down and you can see
[00:20:03.920 --> 00:20:11.040]   we have all these different chapters that really go through what we would call sentence transformers
[00:20:11.680 --> 00:20:18.480]   semantic search and for me I think this is a really interesting sort of subdomain of NLP
[00:20:18.480 --> 00:20:24.400]   and then here we're looking at vector search another important part of of information
[00:20:24.400 --> 00:20:30.880]   retrieval and I of course again be able to find a free link to this in the in the description
[00:20:30.880 --> 00:20:35.680]   you see it's a few parts here we're just introducing similarity search
[00:20:35.680 --> 00:20:43.280]   face and a few other things so again free course I think it's a good deal
[00:20:43.280 --> 00:20:49.440]   and then this is the final one so mining of massive data sets is a really cool book
[00:20:49.440 --> 00:20:55.760]   it's quite big as well it's this really interesting picture on the front here
[00:20:55.760 --> 00:21:02.960]   and this is this is definitely this is more like when you start getting into your
[00:21:02.960 --> 00:21:07.280]   theory side of things and it's not directly applicable to a lot of NLP I will be honest
[00:21:07.280 --> 00:21:16.400]   but there is so many interesting things it's almost like the it's like the theory of everything
[00:21:16.400 --> 00:21:22.560]   that built up information retrieval and some obviously more advanced stuff as well
[00:21:22.560 --> 00:21:29.040]   but this is really when you're getting into trying to understand everything on
[00:21:29.040 --> 00:21:38.000]   a almost deeper level I would say though at the same time a lot of it you might not even find
[00:21:38.000 --> 00:21:44.640]   directly applicable to vector search as I said it's more everything behind vector search
[00:21:44.640 --> 00:21:51.440]   it can be incredibly useful and some of this stuff is is really interesting and definitely
[00:21:51.440 --> 00:21:56.720]   useful when you're actually implementing these sort of things but that's another one that's
[00:21:56.720 --> 00:22:05.600]   really good but like I said you can also get that online for free which is really nice so if you
[00:22:05.600 --> 00:22:14.960]   if you go over to here mmds.org you can come here and where is it I think there is a link
[00:22:14.960 --> 00:22:19.440]   over yeah so you have all the links here I'm sure there's a way to find a whole booking one
[00:22:20.640 --> 00:22:28.480]   oh yeah yeah in this as well so you can also do that it's pretty useful so yeah I mean that
[00:22:28.480 --> 00:22:37.200]   that's kind of what I would recommend you do if you're looking to learn NLP machine learning
[00:22:37.200 --> 00:22:42.480]   all this sort of stuff that's the route I would go a lot of those things obviously I went through
[00:22:42.480 --> 00:22:50.960]   myself and for me it has worked out well so far so that's a good indication that at least some of
[00:22:50.960 --> 00:22:57.440]   these resources will hopefully be useful to you as well and you know I get this question asked a
[00:22:57.440 --> 00:23:02.880]   lot you know how do I learn these things and hopefully this video will be some sort of guide
[00:23:02.880 --> 00:23:09.600]   for at least some of that but I'm also super interested in what resources you recommend
[00:23:09.600 --> 00:23:15.280]   so yeah definitely let me know if there's anything you think is missing and yeah thank
[00:23:15.280 --> 00:23:21.280]   you very much for watching I hope it's been useful and I will see you in the next one bye

