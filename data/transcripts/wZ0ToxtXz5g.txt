
[00:00:00.000 --> 00:00:07.040]   So suppose that it's the case that in a year, a multimodal model can solve ARC, let's say
[00:00:07.040 --> 00:00:11.960]   get 80% or whatever the average human would get, then AGI?
[00:00:11.960 --> 00:00:12.960]   Quite possibly, yes.
[00:00:12.960 --> 00:00:19.600]   I think if you start, so honestly, what I would like to see is an LLM type model solving
[00:00:19.600 --> 00:00:28.580]   ARC at like 80%, but only trained on information that is not explicitly trying to anticipate
[00:00:28.580 --> 00:00:30.660]   what's going to be in the ARC test set.
[00:00:30.660 --> 00:00:34.720]   But isn't the whole point of ARC that you can't sort of, it's a new chart of type of
[00:00:34.720 --> 00:00:35.720]   intelligence test every single time?
[00:00:35.720 --> 00:00:36.720]   Yes, that is the point.
[00:00:36.720 --> 00:00:41.280]   So if ARC were a perfect, flawless benchmark, it would be impossible to anticipate what's
[00:00:41.280 --> 00:00:42.280]   in the test set.
[00:00:42.280 --> 00:00:47.240]   And, you know, ARC was released more than four years ago, and so far it's been resistant
[00:00:47.240 --> 00:00:48.340]   to memorization.
[00:00:48.340 --> 00:00:53.920]   So I think it has, to some extent, passed the test of time, but I don't think it's perfect.
[00:00:53.920 --> 00:01:00.840]   I think if you try to make by hand hundreds of thousands of ARC tasks, and then you try
[00:01:00.840 --> 00:01:07.100]   to multiply them by programmatically generating variations, and then you end up with maybe
[00:01:07.100 --> 00:01:12.760]   hundreds of millions of tasks, just by brute forcing the task space, there will be enough
[00:01:12.760 --> 00:01:16.120]   overlap between what you're trained on and what's in the test set that you can actually
[00:01:16.120 --> 00:01:17.320]   score very highly.
[00:01:17.320 --> 00:01:20.620]   So, you know, with enough scale, you can always cheat.
[00:01:20.620 --> 00:01:24.020]   If you can do this for every single thing that supposedly requires intelligence, then
[00:01:24.020 --> 00:01:25.020]   what good is intelligence?
[00:01:25.020 --> 00:01:26.940]   Apparently, you can just brute force intelligence.
[00:01:26.940 --> 00:01:32.580]   If the world, if your life were a static distribution, then sure, you could just brute force the
[00:01:32.580 --> 00:01:34.580]   space of possible behaviors.
[00:01:34.580 --> 00:01:40.980]   Like, you know, the way we think about intelligence, there are several metaphors I like to use,
[00:01:40.980 --> 00:01:47.020]   but one of them is you can think of intelligence as a pathfinding algorithm in future situation
[00:01:47.020 --> 00:01:48.020]   space.
[00:01:48.020 --> 00:01:51.580]   So like, I don't know if you're familiar with game development, like RTS game development,
[00:01:51.580 --> 00:01:54.680]   but you have a map, right?
[00:01:54.680 --> 00:01:59.860]   And you have, it's like a 2D map, and you have partial information about it.
[00:01:59.860 --> 00:02:04.820]   Like there is some fog of war on your map, there are areas that you haven't explored
[00:02:04.820 --> 00:02:06.140]   yet, you know nothing about them.
[00:02:06.140 --> 00:02:10.000]   And then there are areas that you've explored, but you only know how they were like in the
[00:02:10.000 --> 00:02:11.000]   past.
[00:02:11.000 --> 00:02:12.700]   You don't know how they are like today.
[00:02:12.700 --> 00:02:20.940]   And now instead of thinking about a 2D map, think about the space of possible future situations
[00:02:20.940 --> 00:02:24.140]   that you might encounter and how they are connected to each other.
[00:02:24.140 --> 00:02:25.460]   Intelligence is a pathfinding algorithm.
[00:02:25.460 --> 00:02:30.460]   So once you set a goal, it will tell you how to get there optimally.
[00:02:30.460 --> 00:02:36.980]   But of course, it's constrained by the information you have.
[00:02:36.980 --> 00:02:40.260]   It cannot pathfind in an area that you know nothing about.
[00:02:40.260 --> 00:02:43.820]   It cannot also anticipate changes.
[00:02:43.820 --> 00:02:52.380]   And the thing is, if you had complete information about the map, then you could solve the pathfinding
[00:02:52.380 --> 00:02:58.980]   problem by simply memorizing every possible path, every mapping from point A to point
[00:02:58.980 --> 00:03:03.500]   B. You could solve the problem with pure memory.
[00:03:03.500 --> 00:03:07.980]   But the reason you cannot do that in real life is because you don't actually know what's
[00:03:07.980 --> 00:03:10.020]   going to happen in the future.
[00:03:10.020 --> 00:03:11.020]   Life is ever-changing.
[00:03:11.020 --> 00:03:13.900]   I feel like you're using words like memorization, which we would never use for human children.
[00:03:13.900 --> 00:03:18.620]   If your kid learns to do algebra and then now learns to do calculus, you wouldn't say
[00:03:18.620 --> 00:03:19.620]   they've memorized calculus.
[00:03:19.620 --> 00:03:24.620]   If they can just solve any arbitrary algebraic problem, you wouldn't say they've memorized
[00:03:24.620 --> 00:03:25.620]   algebra.
[00:03:25.620 --> 00:03:26.620]   You'd say they've learned algebra.
[00:03:26.620 --> 00:03:29.300]   Humans are never really doing pure memorization or pure reasoning.
[00:03:29.300 --> 00:03:32.260]   But that's only because you're semantically labeling when the human does a skill, it's
[00:03:32.260 --> 00:03:35.160]   a memorization, when the exact same skill is done by the LLM, as you can measure by
[00:03:35.160 --> 00:03:36.160]   these benchmarks.
[00:03:36.160 --> 00:03:37.900]   And you can just plug in any sort of math problem.
[00:03:37.900 --> 00:03:41.380]   Most humans are doing the exact same as the LLM is doing, which is just, for instance,
[00:03:41.380 --> 00:03:46.380]   I know if you learn to add numbers, you're memorizing an algorithm, you're memorizing
[00:03:46.380 --> 00:03:48.660]   a program and then you can reapply it.
[00:03:48.660 --> 00:03:52.740]   You are not synthesizing on the fly the addition program.
[00:03:52.740 --> 00:03:55.660]   So obviously at some point, some human had to figure out how to do addition.
[00:03:55.660 --> 00:04:00.740]   But the way a kid learns it is not that they sort of figure out from the absence of set
[00:04:00.740 --> 00:04:01.740]   theory how to do addition.
[00:04:01.740 --> 00:04:04.380]   I think what you learn in school is mostly memorization.
[00:04:04.380 --> 00:04:05.380]   Right.
[00:04:05.380 --> 00:04:10.740]   So my claim is that, listen, these models are vastly under-parameterized relative to
[00:04:10.740 --> 00:04:14.300]   how many flops or how many parameters you have in the human brain.
[00:04:14.300 --> 00:04:18.860]   And so, yeah, they're not going to be like coming up with new theorems like the smartest
[00:04:18.860 --> 00:04:19.900]   humans can.
[00:04:19.900 --> 00:04:22.420]   But most humans can't do that either.
[00:04:22.420 --> 00:04:25.940]   What most humans do, it sounds like it's similar to what you are calling memorization, which
[00:04:25.940 --> 00:04:32.220]   is memorizing skills or memorizing, you know, techniques that you've learned.
[00:04:32.220 --> 00:04:35.820]   And so it sounds like it's compatible in your, tell me if this is wrong.
[00:04:35.820 --> 00:04:40.400]   Is it compatible in your world if like all the remote workers are gone, but they're doing
[00:04:40.400 --> 00:04:43.180]   skills which we can potentially make synthetic data of?
[00:04:43.180 --> 00:04:47.240]   So we record everybody's screen and every single remote worker's screen.
[00:04:47.240 --> 00:04:49.860]   We sort of understand the skills they're performing there.
[00:04:49.860 --> 00:04:51.780]   And now we've trained a model that can do all this.
[00:04:51.780 --> 00:04:53.420]   All the remote workers are unemployed.
[00:04:53.420 --> 00:04:57.680]   We're generating trillions of dollars of economic activity from AI remote workers.
[00:04:57.680 --> 00:05:00.100]   In that world, are we still in the memorization regime?
[00:05:00.100 --> 00:05:01.100]   So sure.
[00:05:01.100 --> 00:05:04.880]   With memorization, you can automate almost anything.
[00:05:04.880 --> 00:05:08.620]   As long as it's a static distribution, as long as you don't have to deal with change.

