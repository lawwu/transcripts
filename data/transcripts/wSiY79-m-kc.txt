
[00:00:00.000 --> 00:00:05.000]   So next up we have our guest lecturer for the day, who's Peter Abbeel.
[00:00:05.000 --> 00:00:11.000]   Peter is a professor at Berkeley where he was my PhD advisor until very recently.
[00:00:11.000 --> 00:00:12.000]   One week ago.
[00:00:12.000 --> 00:00:15.000]   Approximately one week ago, yep.
[00:00:15.000 --> 00:00:21.000]   And so Peter was one of the pioneers in just applying machine learning and robotics in general.
[00:00:21.000 --> 00:00:28.000]   And in particular, recently he's been focused on deep reinforcement learning, generative
[00:00:28.000 --> 00:00:33.000]   models, meta-learning, so like kind of many of the really exciting areas of machine learning
[00:00:33.000 --> 00:00:35.000]   that many of you have probably heard of.
[00:00:35.000 --> 00:00:40.000]   In addition to academic work, Peter has founded a couple of startups.
[00:00:40.000 --> 00:00:44.000]   He started Gradescope with Sergey, who presented a couple of weeks ago.
[00:00:44.000 --> 00:00:51.000]   And more recently he started Covariant, which is applying AI to real-world problems in robotics.
[00:00:51.000 --> 00:00:53.000]   So very excited to have Peter here.
[00:00:53.000 --> 00:00:54.000]   Welcome.
[00:00:54.000 --> 00:00:55.000]   Thank you.
[00:00:55.000 --> 00:00:58.000]   (audience applauding)
[00:00:58.000 --> 00:01:03.000]   - So in this session what we're gonna do is we're gonna talk about research directions.
[00:01:03.000 --> 00:01:07.000]   And so maybe the first thing you might wonder is why even talk about research directions?
[00:01:07.000 --> 00:01:10.000]   Maybe you just wanna build a system that works.
[00:01:10.000 --> 00:01:16.000]   But it turns out in AI, the kind of time between a new research result coming out and it actually
[00:01:16.000 --> 00:01:19.000]   being put to use is extremely short.
[00:01:19.000 --> 00:01:23.000]   And very often, kind of the best systems today use a lot of results that didn't exist two
[00:01:23.000 --> 00:01:24.000]   years ago.
[00:01:24.000 --> 00:01:28.000]   And so keeping up with the latest research actually really matters to build the best
[00:01:28.000 --> 00:01:29.000]   systems today.
[00:01:29.000 --> 00:01:31.000]   It's pretty much unavoidable.
[00:01:31.000 --> 00:01:34.000]   (dog barking)
[00:01:34.000 --> 00:01:36.000]   New visitor.
[00:01:36.000 --> 00:01:39.000]   (laughing)
[00:01:39.000 --> 00:01:43.000]   So one of the places you can learn a lot about research is archive.
[00:01:43.000 --> 00:01:45.000]   Maybe we'll do a quick raise of hands.
[00:01:45.000 --> 00:01:47.000]   Who has read an archive paper in the last week?
[00:01:47.000 --> 00:01:49.000]   Everybody within the last week.
[00:01:49.000 --> 00:01:50.000]   Wow, that's amazing.
[00:01:50.000 --> 00:01:51.000]   Okay.
[00:01:51.000 --> 00:01:54.000]   Who has read all our papers that came out the last week?
[00:01:54.000 --> 00:01:55.000]   (laughing)
[00:01:55.000 --> 00:01:58.000]   So nobody, I suspect, I hope.
[00:01:58.000 --> 00:02:00.000]   It's impossible.
[00:02:00.000 --> 00:02:04.000]   Here's a graph of the number of papers coming in archive starting in 2012.
[00:02:04.000 --> 00:02:08.000]   This one runs till 2017, was shooting up at the end.
[00:02:08.000 --> 00:02:09.000]   Curious where it's now.
[00:02:09.000 --> 00:02:14.000]   2012, per month there was about 200 papers coming out on artificial intelligence.
[00:02:14.000 --> 00:02:19.000]   And then by 2017 it was going up to 2,000 papers per month, which is just not something
[00:02:19.000 --> 00:02:23.000]   you can very easily keep up with.
[00:02:23.000 --> 00:02:27.000]   One thing I want to do here is give you some ideas of what are some of the directions
[00:02:27.000 --> 00:02:30.000]   that I think are very exciting, that'll make a lot of changes in the near future
[00:02:30.000 --> 00:02:32.000]   in how we solve problems.
[00:02:32.000 --> 00:02:36.000]   There's a list of directions I think are pretty important, and we'll see a lot of change,
[00:02:36.000 --> 00:02:39.000]   but we only have an hour and a half.
[00:02:39.000 --> 00:02:44.000]   So what I'm gonna talk about is the first column here, but that doesn't mean the other ones
[00:02:44.000 --> 00:02:47.000]   aren't gonna be very important too, it's just we only have so much time.
[00:02:47.000 --> 00:02:51.000]   But I just want to mention that now bias in machine learning is really important,
[00:02:51.000 --> 00:02:55.000]   and really what it often means is that if your data set is not properly collected,
[00:02:55.000 --> 00:02:59.000]   it'll reflect biases in your data collection process into how the system
[00:02:59.000 --> 00:03:03.000]   that might make really large scale decisions, where those biases can be a lot more harmful
[00:03:03.000 --> 00:03:07.000]   than if it was just a single person having a bias.
[00:03:07.000 --> 00:03:11.000]   Long horizon reasoning is something that I would say pretty much no systems do today.
[00:03:11.000 --> 00:03:15.000]   Pretty much all machine learning systems deployed just make an instantaneous decision
[00:03:15.000 --> 00:03:19.000]   or might think one second ahead, but none of them do really long term reasoning.
[00:03:19.000 --> 00:03:22.000]   But I think more progress will be made there in the near future.
[00:03:22.000 --> 00:03:25.000]   Safe learning is a pretty important one.
[00:03:25.000 --> 00:03:30.000]   As you actively collect data or as a physical system, learn in an environment,
[00:03:30.000 --> 00:03:34.000]   let's say a self-driving car, you want to try something new you never tried before
[00:03:34.000 --> 00:03:37.000]   and see how it works, that's usually unsafe.
[00:03:37.000 --> 00:03:40.000]   And so the question becomes how do you learn new things?
[00:03:40.000 --> 00:03:44.000]   You get novelty without the dangers of being unsafe or breaking things
[00:03:44.000 --> 00:03:46.000]   that are valuable.
[00:03:46.000 --> 00:03:48.000]   Value alignment is much further out.
[00:03:48.000 --> 00:03:51.000]   This you're probably not gonna use anytime soon in any of your projects,
[00:03:51.000 --> 00:03:56.000]   but that's the notion that if we actually build really good AI that's really good
[00:03:56.000 --> 00:04:00.000]   at optimizing objectives we're given, we say okay, I don't know, maximize money
[00:04:00.000 --> 00:04:03.000]   in my bank account or something and it might do it really well,
[00:04:03.000 --> 00:04:05.000]   but it might not have sold your house.
[00:04:05.000 --> 00:04:07.000]   And you're like well, that's not what I was asking for
[00:04:07.000 --> 00:04:11.000]   and value alignment is exactly about that, about making sure that the way
[00:04:11.000 --> 00:04:15.000]   we express things, what we ask for, if it gets fully optimized for,
[00:04:15.000 --> 00:04:19.000]   that it actually leads to a good result because it interprets what we say
[00:04:19.000 --> 00:04:21.000]   rather than taking it too literally.
[00:04:21.000 --> 00:04:24.000]   And then planning and learning are things that people consider
[00:04:24.000 --> 00:04:26.000]   very disparate these days still.
[00:04:26.000 --> 00:04:30.000]   Planning is A* search that finds a path from, I don't know, San Francisco
[00:04:30.000 --> 00:04:32.000]   to Palo Alto or something.
[00:04:32.000 --> 00:04:35.000]   And then learning is this pattern recognition thing, but in the future
[00:04:35.000 --> 00:04:39.000]   they need to be brought together to build much more powerful systems.
[00:04:39.000 --> 00:04:42.000]   Those are the ones I'm not gonna talk about this time, maybe next time,
[00:04:42.000 --> 00:04:45.000]   who knows, I'm gonna focus on the first column here.
[00:04:45.000 --> 00:04:49.000]   Aside from sampling research directions, what I wanna do,
[00:04:49.000 --> 00:04:52.000]   I wanna give a bit of a summary after sampling through them
[00:04:52.000 --> 00:04:56.000]   of the overall research themes that come back and back and back across them
[00:04:56.000 --> 00:04:59.000]   and also give some pointers on how to keep up with these thousands
[00:04:59.000 --> 00:05:01.000]   of archive papers that come out every month.
[00:05:01.000 --> 00:05:06.000]   So a first really important direction that I think we'll see a lot
[00:05:06.000 --> 00:05:10.000]   of deployments in the near future is few-shot learning.
[00:05:10.000 --> 00:05:13.000]   And few-shot here is in contrast to learning from, let's say,
[00:05:13.000 --> 00:05:16.000]   tens or hundreds of thousands of examples that you annotate.
[00:05:16.000 --> 00:05:19.000]   How do you learn from just a small number of examples?
[00:05:19.000 --> 00:05:22.000]   As indeed, supervised learning works really well when you have
[00:05:22.000 --> 00:05:25.000]   massive amounts of label data, but think about people.
[00:05:25.000 --> 00:05:28.000]   I'm looking around the room here, I'm very confident that it was a time
[00:05:28.000 --> 00:05:31.000]   in all of your lives where this thing does not exist
[00:05:31.000 --> 00:05:33.000]   and that thing also did not exist.
[00:05:33.000 --> 00:05:36.000]   And then you saw it for the first time.
[00:05:36.000 --> 00:05:39.000]   And you just needed to see one example
[00:05:39.000 --> 00:05:41.000]   and then show a new one to you.
[00:05:41.000 --> 00:05:43.000]   You know, yeah, that's a hoverboard again.
[00:05:43.000 --> 00:05:44.000]   That was easy for you.
[00:05:44.000 --> 00:05:47.000]   This one, it's a one-wheel plus, another one-wheel plus,
[00:05:47.000 --> 00:05:49.000]   another hoverboard.
[00:05:49.000 --> 00:05:53.000]   And so those things are very easy for people to learn from one example,
[00:05:53.000 --> 00:05:56.000]   whereas pretty much all kind of common wisdom in machine learning
[00:05:56.000 --> 00:05:59.000]   is you get lots of data and finally something will work.
[00:05:59.000 --> 00:06:01.000]   So why can we do this?
[00:06:01.000 --> 00:06:04.000]   How come we can actually do something that seems so contrary
[00:06:04.000 --> 00:06:07.000]   to what our deep nets are able to do?
[00:06:07.000 --> 00:06:11.000]   Well, one way to think of it is that we have a very strong prior notion
[00:06:11.000 --> 00:06:13.000]   of what categories are.
[00:06:13.000 --> 00:06:15.000]   We have seen many other things in the past,
[00:06:15.000 --> 00:06:18.000]   maybe dogs, maybe cats, cars, and so forth.
[00:06:18.000 --> 00:06:20.000]   They've been categorized.
[00:06:20.000 --> 00:06:24.000]   And what we've learned from that is maybe not just what a cat is
[00:06:24.000 --> 00:06:27.000]   or what a dog is, but the notion of what delineates a category.
[00:06:27.000 --> 00:06:30.000]   And then when we see one example of a new category,
[00:06:30.000 --> 00:06:33.000]   we might be able to reapply that notion of how we delineate a category
[00:06:33.000 --> 00:06:38.000]   around what we just saw and recognize something from just one example.
[00:06:38.000 --> 00:06:40.000]   So how do we equip a machine with such a prior?
[00:06:40.000 --> 00:06:41.000]   Good question.
[00:06:41.000 --> 00:06:44.000]   Maybe we need to show it lots of categories also.
[00:06:44.000 --> 00:06:46.000]   And maybe from seeing lots of categories,
[00:06:46.000 --> 00:06:48.000]   it can start seeing what it means to be a category
[00:06:48.000 --> 00:06:50.000]   and then learn from one example.
[00:06:50.000 --> 00:06:53.000]   So I'll give you one direction within fused-shot learning
[00:06:53.000 --> 00:06:55.000]   that I think is particularly promising,
[00:06:55.000 --> 00:06:58.000]   but there are definitely other directions too.
[00:06:58.000 --> 00:07:00.000]   This one's called model-agnostic meta-learning.
[00:07:00.000 --> 00:07:02.000]   And the starting observation here
[00:07:02.000 --> 00:07:03.000]   is standard computer vision practice.
[00:07:03.000 --> 00:07:05.000]   In standard computer vision, what do people do?
[00:07:05.000 --> 00:07:08.000]   Well, they say, let's train on ImageNet,
[00:07:08.000 --> 00:07:11.000]   and then let's fine-tune on the thing we actually care about.
[00:07:11.000 --> 00:07:13.000]   And that's nice, 'cause that way we might need less labels
[00:07:13.000 --> 00:07:14.000]   on the task we care about,
[00:07:14.000 --> 00:07:16.000]   because we already trained on other tasks,
[00:07:16.000 --> 00:07:18.000]   and the neural net has learned to extract features
[00:07:18.000 --> 00:07:21.000]   that are reusable across many tasks.
[00:07:21.000 --> 00:07:23.000]   It actually works really well.
[00:07:23.000 --> 00:07:24.000]   And I would say when people think about
[00:07:24.000 --> 00:07:26.000]   deep learning breakthroughs,
[00:07:26.000 --> 00:07:28.000]   sometimes there's two big things people don't think about.
[00:07:28.000 --> 00:07:32.000]   2012, ImageNet, better results than ever before,
[00:07:32.000 --> 00:07:34.000]   but also right thereafter, hey, it turns out
[00:07:34.000 --> 00:07:36.000]   that you can reuse those neural nets
[00:07:36.000 --> 00:07:38.000]   and fine-tune them for other things.
[00:07:38.000 --> 00:07:39.000]   So you learn something meaningful
[00:07:39.000 --> 00:07:41.000]   that's reusable inside them.
[00:07:41.000 --> 00:07:43.000]   You might have some questions, like,
[00:07:43.000 --> 00:07:45.000]   how does this generalize to behavior learning?
[00:07:45.000 --> 00:07:47.000]   Let's say you don't just wanna do image recognition,
[00:07:47.000 --> 00:07:48.000]   do something with behaviors.
[00:07:48.000 --> 00:07:51.000]   Or even just a more fundamental question.
[00:07:51.000 --> 00:07:53.000]   This is kind of training on one thing
[00:07:53.000 --> 00:07:55.000]   and hoping it'll work on something else
[00:07:55.000 --> 00:07:57.000]   is kind of hoping that you're gonna be lucky.
[00:07:57.000 --> 00:07:59.000]   But can you force yourself to be lucky
[00:07:59.000 --> 00:08:02.000]   and force it to work on the new thing that comes later?
[00:08:02.000 --> 00:08:05.000]   And that's the idea I wanna describe here.
[00:08:05.000 --> 00:08:08.000]   So model-agnostic meta-learning says,
[00:08:08.000 --> 00:08:12.000]   let's do end-to-end learning of a parameter vector theta,
[00:08:12.000 --> 00:08:14.000]   that's the parameters of a big neural network,
[00:08:14.000 --> 00:08:18.000]   that makes the neural network then a good initialization
[00:08:18.000 --> 00:08:21.000]   to start learning something new afterwards.
[00:08:23.000 --> 00:08:25.000]   What does that mean to learn something new?
[00:08:25.000 --> 00:08:26.000]   This fine-tuning thing is,
[00:08:26.000 --> 00:08:28.000]   well, you have some parameter vector theta
[00:08:28.000 --> 00:08:29.000]   and you hope that your network
[00:08:29.000 --> 00:08:31.000]   is pretty ready to learn something new.
[00:08:31.000 --> 00:08:33.000]   Well, you get some new training data.
[00:08:33.000 --> 00:08:36.000]   You take a gradient of the training data loss
[00:08:36.000 --> 00:08:38.000]   and update your parameter vector
[00:08:38.000 --> 00:08:41.000]   and you get a fine-tuned parameter vector for the new task.
[00:08:41.000 --> 00:08:44.000]   And if your pre-training is good,
[00:08:44.000 --> 00:08:45.000]   what that means is that
[00:08:45.000 --> 00:08:48.000]   that is defined as the fine-tuning working well.
[00:08:48.000 --> 00:08:50.000]   So if theta prime is good for the new task,
[00:08:50.000 --> 00:08:52.000]   the pre-training was successful.
[00:08:52.000 --> 00:08:55.000]   So we can ask the question, can we force this to be true?
[00:08:55.000 --> 00:08:57.000]   Can we just ask for a theta such that
[00:08:57.000 --> 00:09:00.000]   when we fine-tune, it's gonna give good results?
[00:09:00.000 --> 00:09:03.000]   Well, here's one way to do this.
[00:09:03.000 --> 00:09:06.000]   You say, I'm gonna have a set of tasks.
[00:09:06.000 --> 00:09:08.000]   So typically, I have a set of training examples.
[00:09:08.000 --> 00:09:11.000]   Now I'm gonna have a set of tasks indexed by i.
[00:09:11.000 --> 00:09:14.000]   And each task will have its own training examples.
[00:09:14.000 --> 00:09:17.000]   Maybe one task is about recognizing cats versus dogs,
[00:09:17.000 --> 00:09:21.000]   another one about maybe bicycles versus cars, and so forth.
[00:09:21.000 --> 00:09:23.000]   So each task i has its own training examples.
[00:09:23.000 --> 00:09:27.000]   And we want to find is a parameter vector theta
[00:09:27.000 --> 00:09:30.000]   that is such that if we randomly sample a task i,
[00:09:30.000 --> 00:09:31.000]   and then get to do a gradient update
[00:09:31.000 --> 00:09:33.000]   on the training data for that task,
[00:09:33.000 --> 00:09:35.000]   the resulting parameter vector,
[00:09:35.000 --> 00:09:37.000]   the fine-tuned parameter vector,
[00:09:37.000 --> 00:09:40.000]   is then good, does really well
[00:09:40.000 --> 00:09:43.000]   on the validation data for task i.
[00:09:43.000 --> 00:09:45.000]   If that's the case, then this theta was ready
[00:09:45.000 --> 00:09:47.000]   for fine-tuning, and especially so
[00:09:47.000 --> 00:09:50.000]   if that's the case for all tasks i.
[00:09:50.000 --> 00:09:52.000]   But this itself, you can actually see
[00:09:52.000 --> 00:09:53.000]   as an optimization problem.
[00:09:53.000 --> 00:09:56.000]   You can try to optimize to find a theta
[00:09:56.000 --> 00:09:59.000]   that is good at this across a wide range of tasks.
[00:09:59.000 --> 00:10:03.000]   With automatic differentiation frameworks
[00:10:03.000 --> 00:10:05.000]   like TensorFlow or PyTorch,
[00:10:05.000 --> 00:10:08.000]   you can literally formulate your problem that way,
[00:10:08.000 --> 00:10:09.000]   and it'll take care of taking derivatives
[00:10:09.000 --> 00:10:11.000]   through that entire process.
[00:10:11.000 --> 00:10:12.000]   It's pretty complex, actually,
[00:10:12.000 --> 00:10:14.000]   'cause you're expecting it to be good
[00:10:14.000 --> 00:10:17.000]   after a gradient update, so you're gonna have gradients
[00:10:17.000 --> 00:10:18.000]   of gradients happening on the inside,
[00:10:18.000 --> 00:10:20.000]   but you don't have to worry about it.
[00:10:20.000 --> 00:10:23.000]   Automatic differentiation will take care of that for you.
[00:10:23.000 --> 00:10:25.000]   Another way to look at this pictorially
[00:10:25.000 --> 00:10:26.000]   is that you're trying to find,
[00:10:26.000 --> 00:10:28.000]   in this high-dimensional space,
[00:10:28.000 --> 00:10:30.000]   a parameter vector theta that is such
[00:10:30.000 --> 00:10:33.000]   that with a small gradient update,
[00:10:33.000 --> 00:10:35.000]   a gradient update on just a small amount of data,
[00:10:35.000 --> 00:10:38.000]   you can jump onto a good solution for a new task.
[00:10:38.000 --> 00:10:44.000]   Okay, so let's look at an example challenge.
[00:10:44.000 --> 00:10:47.000]   A standard challenge for few-shot learning
[00:10:47.000 --> 00:10:51.000]   is Omniglot and Mini ImageNet.
[00:10:51.000 --> 00:10:54.000]   Those are the two standard tasks.
[00:10:54.000 --> 00:10:56.000]   What happens in there is you take ImageNet,
[00:10:56.000 --> 00:10:58.000]   which has many, many categories,
[00:10:58.000 --> 00:11:03.000]   and you define tasks by taking a subset of categories
[00:11:03.000 --> 00:11:05.000]   and only taking the data for that subset of categories,
[00:11:05.000 --> 00:11:06.000]   and that would be one task.
[00:11:06.000 --> 00:11:08.000]   And you take another subset of categories
[00:11:08.000 --> 00:11:10.000]   to find another task, and so forth.
[00:11:10.000 --> 00:11:13.000]   And then what you get is the first task
[00:11:13.000 --> 00:11:15.000]   shown on the first row has some kind of bird,
[00:11:15.000 --> 00:11:19.000]   some kind of mushroom, dog, maybe microphone,
[00:11:19.000 --> 00:11:22.000]   and then piano.
[00:11:22.000 --> 00:11:26.000]   And then you'd hope that after seeing those five examples,
[00:11:26.000 --> 00:11:28.000]   one example maybe of each class here,
[00:11:28.000 --> 00:11:30.000]   that then when faced with a test set
[00:11:30.000 --> 00:11:34.000]   within that subtask i, you do well and know
[00:11:34.000 --> 00:11:37.000]   that the first one's a dog and the second one is a piano.
[00:11:37.000 --> 00:11:41.000]   That's what happens at meta-training time.
[00:11:41.000 --> 00:11:42.000]   Then at meta-testing time,
[00:11:42.000 --> 00:11:44.000]   you hope that when new categories come about
[00:11:44.000 --> 00:11:46.000]   and there's a different type of dog,
[00:11:46.000 --> 00:11:49.000]   other new categories that you have never seen before,
[00:11:49.000 --> 00:11:52.000]   that you can just see one example and also adapt quickly.
[00:11:52.000 --> 00:11:54.000]   And the reason you might expect this works
[00:11:54.000 --> 00:11:55.000]   is just like in supervised learning.
[00:11:55.000 --> 00:11:57.000]   In supervised learning, you have trainings data,
[00:11:57.000 --> 00:11:59.000]   validation data to make sure you don't overfit,
[00:11:59.000 --> 00:12:00.000]   and then test data.
[00:12:00.000 --> 00:12:03.000]   And you do well on the test data if you do this well
[00:12:03.000 --> 00:12:05.000]   because the test data comes from the same distribution
[00:12:05.000 --> 00:12:07.000]   in some sense as the training data.
[00:12:07.000 --> 00:12:09.000]   Same thing should be true here.
[00:12:09.000 --> 00:12:10.000]   If you have task distributions,
[00:12:10.000 --> 00:12:13.000]   then if your distribution over task
[00:12:13.000 --> 00:12:15.000]   is such that at testing time, meta-testing time,
[00:12:15.000 --> 00:12:17.000]   the distribution over task is similar
[00:12:17.000 --> 00:12:19.000]   as it was at training time,
[00:12:19.000 --> 00:12:22.000]   then you might expect this to work pretty well.
[00:12:22.000 --> 00:12:24.000]   How well does this work?
[00:12:24.000 --> 00:12:29.000]   Omniglot is a data set where there's 1200 training classes,
[00:12:29.000 --> 00:12:31.000]   I mean there's 1200 different handwritten characters
[00:12:31.000 --> 00:12:33.000]   that you get to meta-train on.
[00:12:33.000 --> 00:12:35.000]   And there's 423 test classes,
[00:12:35.000 --> 00:12:38.000]   so new handwritten characters you've never seen before.
[00:12:38.000 --> 00:12:39.000]   And the hope is because you learn to recognize
[00:12:39.000 --> 00:12:42.000]   the original set of handwritten characters really well,
[00:12:42.000 --> 00:12:45.000]   you can quickly fine tune onto new ones.
[00:12:45.000 --> 00:12:48.000]   So it shows here is one shot means that you get one example
[00:12:48.000 --> 00:12:50.000]   of each class, five way means that there is five classes
[00:12:50.000 --> 00:12:53.000]   in your task, so five different types of characters.
[00:12:53.000 --> 00:12:55.000]   And it's shown that this approach I just described,
[00:12:55.000 --> 00:12:57.000]   which is in green, with just one example of each,
[00:12:57.000 --> 00:13:02.000]   five way classification has only 1.1% error rate.
[00:13:02.000 --> 00:13:07.000]   So one example, five way classification, 1.1% error rate,
[00:13:07.000 --> 00:13:10.000]   which obviously it means it's reusing a lot of things
[00:13:10.000 --> 00:13:12.000]   it's learned in the past to quickly fine tune
[00:13:12.000 --> 00:13:13.000]   on this new problem.
[00:13:13.000 --> 00:13:16.000]   Five shot means you get five examples of each,
[00:13:16.000 --> 00:13:20.000]   and then it brings it down to something 0.2% or even lower.
[00:13:20.000 --> 00:13:23.000]   20 way is harder because there's 20 characters
[00:13:23.000 --> 00:13:25.000]   to distinguish, so the baseline rate,
[00:13:25.000 --> 00:13:28.000]   if you had something naive, would be a much higher error rate.
[00:13:28.000 --> 00:13:30.000]   You see it's still there, it learns very, very quickly
[00:13:30.000 --> 00:13:32.000]   from a small number of examples to classify
[00:13:32.000 --> 00:13:35.000]   handwritten characters it's never seen before.
[00:13:35.000 --> 00:13:39.000]   Then ImageNet, which is a harder task
[00:13:39.000 --> 00:13:41.000]   than handwritten character recognition,
[00:13:41.000 --> 00:13:43.000]   we see similarly that even from a very small number
[00:13:43.000 --> 00:13:45.000]   of examples, five way classification,
[00:13:45.000 --> 00:13:48.000]   one example of each class is able to adapt
[00:13:48.000 --> 00:13:51.000]   and get an error rate of about 50%,
[00:13:51.000 --> 00:13:52.000]   which is much better than chance,
[00:13:52.000 --> 00:13:54.000]   obviously not human level error rate,
[00:13:54.000 --> 00:13:55.000]   but it's quite good.
[00:13:55.000 --> 00:13:57.000]   And if you get five examples of each class,
[00:13:57.000 --> 00:13:59.000]   bring it down to about 32% error rate.
[00:13:59.000 --> 00:14:01.000]   So you can learn pretty good classifiers
[00:14:01.000 --> 00:14:04.000]   from a very small number of examples.
[00:14:04.000 --> 00:14:08.000]   If ahead of time, you had a very large number of examples
[00:14:08.000 --> 00:14:11.000]   on related problems that you could pre-train on
[00:14:11.000 --> 00:14:13.000]   to then fine tune from later.
[00:14:13.000 --> 00:14:16.000]   If you wanna learn more about this,
[00:14:16.000 --> 00:14:18.000]   there's quite a few papers on meta-learning
[00:14:18.000 --> 00:14:21.000]   for classification, which is what I just described.
[00:14:21.000 --> 00:14:24.000]   There's a really cool demo that I actually
[00:14:24.000 --> 00:14:26.000]   wanna show live here, let's see if I can
[00:14:26.000 --> 00:14:29.000]   pull up a browser window easily.
[00:14:29.000 --> 00:14:34.000]   A demo by John Schulman and Alex Nichols from OpenAI.
[00:14:37.000 --> 00:14:38.000]   Let's see.
[00:14:38.000 --> 00:14:54.000]   Reptile named somewhat after mammal, related approach.
[00:14:54.000 --> 00:14:57.000]   So this demo is amazing.
[00:14:57.000 --> 00:14:59.000]   I'm gonna clear this out.
[00:14:59.000 --> 00:15:02.000]   Erase this.
[00:15:02.000 --> 00:15:04.000]   So this has been trained on the Omniglot data,
[00:15:04.000 --> 00:15:06.000]   the handwritten character set.
[00:15:06.000 --> 00:15:08.000]   And so the hope is that if you now define
[00:15:08.000 --> 00:15:10.000]   new handwritten characters or drawings,
[00:15:10.000 --> 00:15:12.000]   it can learn from just one example
[00:15:12.000 --> 00:15:14.000]   what the category is that you're drawing.
[00:15:14.000 --> 00:15:16.000]   So let's try this.
[00:15:16.000 --> 00:15:21.000]   So I'm gonna draw a few things here, let's see.
[00:15:21.000 --> 00:15:26.000]   Maybe I'll draw a house, some kind of house.
[00:15:26.000 --> 00:15:28.000]   Let's see.
[00:15:28.000 --> 00:15:33.000]   Okay, there's even a door.
[00:15:35.000 --> 00:15:38.000]   Okay, nobody's gonna hire me for my drawing skills.
[00:15:38.000 --> 00:15:39.000]   (audience laughing)
[00:15:39.000 --> 00:15:42.000]   Maybe we'll draw a car.
[00:15:42.000 --> 00:15:45.000]   Remember, it's only seen handwritten characters before,
[00:15:45.000 --> 00:15:48.000]   so these are completely new things.
[00:15:48.000 --> 00:15:50.000]   It's never seen before.
[00:15:50.000 --> 00:15:56.000]   Complete those wheels.
[00:15:56.000 --> 00:15:59.000]   And maybe a tree.
[00:16:02.000 --> 00:16:05.000]   Very straight tree, well, not so straight.
[00:16:05.000 --> 00:16:08.000]   Okay.
[00:16:08.000 --> 00:16:12.000]   All right, so we have three categories here.
[00:16:12.000 --> 00:16:15.000]   What shall we draw on the right,
[00:16:15.000 --> 00:16:16.000]   and see if it can recognize it
[00:16:16.000 --> 00:16:18.000]   from just one example of each category?
[00:16:18.000 --> 00:16:21.000]   You want the car?
[00:16:21.000 --> 00:16:23.000]   Okay, sure.
[00:16:23.000 --> 00:16:24.000]   (audience laughing)
[00:16:24.000 --> 00:16:26.000]   I'll try car again.
[00:16:26.000 --> 00:16:27.000]   Let's see.
[00:16:27.000 --> 00:16:29.000]   And I'm gonna draw it a little differently.
[00:16:29.000 --> 00:16:31.000]   I mean, not just because I can't repeat it,
[00:16:31.000 --> 00:16:35.000]   but I'm gonna, on purpose, not try to draw
[00:16:35.000 --> 00:16:36.000]   the exact same car, 'cause we want to see
[00:16:36.000 --> 00:16:39.000]   if it can generalize to what it means to be a car.
[00:16:39.000 --> 00:16:42.000]   So now, either way, a car will have wheels,
[00:16:42.000 --> 00:16:45.000]   so maybe I'll already start to draw the wheels.
[00:16:45.000 --> 00:16:48.000]   Ah, it thinks it's pretty likely a car.
[00:16:48.000 --> 00:16:52.000]   It's actually very common, it's a car.
[00:16:52.000 --> 00:16:54.000]   I can only ruin this from here.
[00:16:54.000 --> 00:16:56.000]   (audience laughing)
[00:16:56.000 --> 00:16:57.000]   Let's see.
[00:17:00.000 --> 00:17:03.000]   Okay, it's becoming less confident as I'm proceeding.
[00:17:03.000 --> 00:17:06.000]   (audience laughing)
[00:17:06.000 --> 00:17:08.000]   Maybe I'm just gonna hatchback car.
[00:17:08.000 --> 00:17:09.000]   Let's see.
[00:17:09.000 --> 00:17:12.000]   (audience laughing)
[00:17:12.000 --> 00:17:14.000]   Oh, yeah, clearly.
[00:17:14.000 --> 00:17:17.000]   But clearly, it's still pretty confident.
[00:17:17.000 --> 00:17:20.000]   It's a car, more likely than any of the other things,
[00:17:20.000 --> 00:17:23.000]   54% from just one example of each category.
[00:17:23.000 --> 00:17:24.000]   And this is a live demo,
[00:17:24.000 --> 00:17:26.000]   it's not pre-programmed in any way.
[00:17:26.000 --> 00:17:27.000]   You can go try this yourself.
[00:17:27.000 --> 00:17:29.000]   You Google reptile OpenAI,
[00:17:29.000 --> 00:17:31.000]   you'll find this demo, and it works.
[00:17:31.000 --> 00:17:34.000]   Well, every live demo I've done so far
[00:17:34.000 --> 00:17:37.000]   has actually worked that it finds the right category.
[00:17:37.000 --> 00:17:41.000]   We actually use this at Gradescope, something very similar,
[00:17:41.000 --> 00:17:43.000]   where we build AI software
[00:17:43.000 --> 00:17:45.000]   to automatically grade homework and exams.
[00:17:45.000 --> 00:17:48.000]   Maybe Sergey mentioned something about it.
[00:17:48.000 --> 00:17:50.000]   Not this specifically, but essentially,
[00:17:50.000 --> 00:17:54.000]   well, you start grading is the idea,
[00:17:54.000 --> 00:17:56.000]   and there's maybe 500 students
[00:17:56.000 --> 00:17:57.000]   who gave answers to a question.
[00:17:57.000 --> 00:17:59.000]   And once you grade a few of them,
[00:17:59.000 --> 00:18:01.000]   you've essentially provided
[00:18:01.000 --> 00:18:03.000]   a small number of labeled examples,
[00:18:03.000 --> 00:18:05.000]   and it can generalize to the other ones
[00:18:05.000 --> 00:18:07.000]   that you haven't looked at yet,
[00:18:07.000 --> 00:18:09.000]   at roughly 90% success rate,
[00:18:09.000 --> 00:18:10.000]   and then you can decide whether you wanna keep going
[00:18:10.000 --> 00:18:12.000]   or whether maybe you just let the students
[00:18:12.000 --> 00:18:14.000]   submit regrade requests at the end,
[00:18:14.000 --> 00:18:17.000]   and then you fix the ones that maybe weren't perfect.
[00:18:17.000 --> 00:18:19.000]   And so you can save an enormous amount of time.
[00:18:19.000 --> 00:18:22.000]   And this is a perfect kind of few-shot learning setting,
[00:18:22.000 --> 00:18:23.000]   not just from a scientific point of view,
[00:18:23.000 --> 00:18:25.000]   but the notion that you'll design new questions,
[00:18:25.000 --> 00:18:27.000]   and these questions will be related
[00:18:27.000 --> 00:18:29.000]   to older questions, of course,
[00:18:29.000 --> 00:18:30.000]   'cause it's on the same material,
[00:18:30.000 --> 00:18:32.000]   and so now you should be able to reuse
[00:18:32.000 --> 00:18:33.000]   what you trained in the past
[00:18:33.000 --> 00:18:35.000]   to quickly learn to grade something new.
[00:18:35.000 --> 00:18:39.000]   People don't just look at meta-learning for classification,
[00:18:39.000 --> 00:18:41.000]   they also look at it for optimization,
[00:18:41.000 --> 00:18:43.000]   where you might say,
[00:18:43.000 --> 00:18:44.000]   well, gradient descent is human-designed.
[00:18:44.000 --> 00:18:46.000]   Can we find something that's actually better
[00:18:46.000 --> 00:18:47.000]   than gradient descent?
[00:18:47.000 --> 00:18:48.000]   I think pretty hard to do,
[00:18:48.000 --> 00:18:50.000]   but if you have a narrow class of problems,
[00:18:50.000 --> 00:18:52.000]   often you can still find something that's learned
[00:18:52.000 --> 00:18:54.000]   that's more effective than gradient descent.
[00:18:55.000 --> 00:18:57.000]   You can also do this for generative models,
[00:18:57.000 --> 00:18:58.000]   we'll talk more about those later,
[00:18:58.000 --> 00:19:01.000]   but the notion that you can classify something is one way,
[00:19:01.000 --> 00:19:02.000]   another thing you could say,
[00:19:02.000 --> 00:19:04.000]   if I've learned to generate a lot of handwritten characters,
[00:19:04.000 --> 00:19:06.000]   and I'm now shown a new one,
[00:19:06.000 --> 00:19:10.000]   can I then generate a nice range of that character
[00:19:10.000 --> 00:19:12.000]   that I've been shown one of?
[00:19:12.000 --> 00:19:16.000]   Okay, now I wanna switch gears
[00:19:16.000 --> 00:19:17.000]   to a different kind of learning.
[00:19:17.000 --> 00:19:18.000]   Few-shot learning is about learning
[00:19:18.000 --> 00:19:19.000]   from a small set of examples,
[00:19:19.000 --> 00:19:21.000]   but often still in the classification setting,
[00:19:21.000 --> 00:19:23.000]   I wanna switch to reinforcement learning.
[00:19:23.000 --> 00:19:28.000]   So here, who here has heard of reinforcement learning before?
[00:19:28.000 --> 00:19:29.000]   Almost everyone.
[00:19:29.000 --> 00:19:30.000]   Who here has implemented
[00:19:30.000 --> 00:19:32.000]   something reinforcement learning before?
[00:19:32.000 --> 00:19:34.000]   Huh, still a pretty good number.
[00:19:34.000 --> 00:19:37.000]   Okay, what did you implement?
[00:19:37.000 --> 00:19:39.000]   - It was, like, I had a markup,
[00:19:39.000 --> 00:19:41.000]   it was not deep reinforcement learning,
[00:19:41.000 --> 00:19:43.000]   but I had like a markup problem,
[00:19:43.000 --> 00:19:45.000]   I have an economics PhD student, so.
[00:19:45.000 --> 00:19:48.000]   - So you implement value iteration, policy gradients.
[00:19:48.000 --> 00:19:51.000]   - Q-learning. - Q-learning, okay.
[00:19:51.000 --> 00:19:54.000]   How about you, I think you also raised your hand.
[00:19:54.000 --> 00:19:59.000]   - Yeah, I did imitation based, and then EDPG.
[00:19:59.000 --> 00:20:04.000]   - EDPG, okay, great, and then some other hands there.
[00:20:04.000 --> 00:20:06.000]   Anybody else there?
[00:20:06.000 --> 00:20:13.000]   Cart-Pol, and which algorithm did he implement for it?
[00:20:13.000 --> 00:20:16.000]   - This is a long time ago.
[00:20:16.000 --> 00:20:18.000]   I'm trying to remember.
[00:20:19.000 --> 00:20:23.000]   I also did like, ranges and time difference.
[00:20:23.000 --> 00:20:28.000]   - Oh, well, okay.
[00:20:28.000 --> 00:20:31.000]   So we have about five people here,
[00:20:31.000 --> 00:20:33.000]   I think, who implemented RL before,
[00:20:33.000 --> 00:20:34.000]   which is pretty good.
[00:20:34.000 --> 00:20:36.000]   It turns out, reinforcement learning
[00:20:36.000 --> 00:20:38.000]   does not have that many applications in the real world
[00:20:38.000 --> 00:20:40.000]   just yet compared to supervised learning.
[00:20:40.000 --> 00:20:41.000]   Like, if you look around,
[00:20:41.000 --> 00:20:43.000]   the amount of use of reinforcement learning
[00:20:43.000 --> 00:20:44.000]   tends to be much more limited.
[00:20:44.000 --> 00:20:47.000]   There are places, simple reinforcement learning problems,
[00:20:47.000 --> 00:20:50.000]   like bandit problems, which are used, of course,
[00:20:50.000 --> 00:20:54.000]   extensively for advertising and for optimizing news feeds.
[00:20:54.000 --> 00:20:56.000]   In robotics, reinforcement learning can play a big role
[00:20:56.000 --> 00:20:57.000]   'cause it can fine tune performance
[00:20:57.000 --> 00:20:59.000]   on top of what you might have already learned in the past
[00:20:59.000 --> 00:21:03.000]   from maybe demonstrations or something you hard-coded.
[00:21:03.000 --> 00:21:05.000]   But in general, I would say it's kind of on the cusp
[00:21:05.000 --> 00:21:07.000]   of making its way into applications,
[00:21:07.000 --> 00:21:09.000]   but not so widespread yet.
[00:21:09.000 --> 00:21:12.000]   But this might be the time you can go make that difference.
[00:21:12.000 --> 00:21:14.000]   So in reinforcement learning,
[00:21:14.000 --> 00:21:16.000]   the agent interacts with the world.
[00:21:16.000 --> 00:21:18.000]   The agent, we can think of as essentially the brain
[00:21:18.000 --> 00:21:20.000]   or the software, might then make the robot
[00:21:20.000 --> 00:21:22.000]   and environment change state,
[00:21:22.000 --> 00:21:24.000]   and then there's a new state,
[00:21:24.000 --> 00:21:25.000]   and a new action gets taken,
[00:21:25.000 --> 00:21:27.000]   and this loop just repeats over and over and over,
[00:21:27.000 --> 00:21:31.000]   and the purpose of the agent is to maximize reward.
[00:21:31.000 --> 00:21:33.000]   And so the way you interact with that agent,
[00:21:33.000 --> 00:21:35.000]   assuming that reinforcement learning algorithm
[00:21:35.000 --> 00:21:38.000]   already exists, is by saying, this is my reward function.
[00:21:38.000 --> 00:21:39.000]   And I'd say, okay, I don't know.
[00:21:39.000 --> 00:21:43.000]   Maybe your reward function is to get a positive reward
[00:21:43.000 --> 00:21:45.000]   when you reach a destination as a self-driving car,
[00:21:45.000 --> 00:21:47.000]   and a very negative reward for accidents,
[00:21:47.000 --> 00:21:48.000]   and maybe a slightly negative reward
[00:21:48.000 --> 00:21:50.000]   as long as you're not at the destination yet.
[00:21:50.000 --> 00:21:52.000]   'Cause it might encourage getting to the destination
[00:21:52.000 --> 00:21:53.000]   sooner rather than later,
[00:21:53.000 --> 00:21:56.000]   but also encourage you to be cautious.
[00:21:56.000 --> 00:21:58.000]   Compared to supervised learning,
[00:21:58.000 --> 00:22:00.000]   there are actually a lot of new challenges
[00:22:00.000 --> 00:22:01.000]   in reinforcement learning.
[00:22:01.000 --> 00:22:02.000]   So one of them is credit assignment.
[00:22:02.000 --> 00:22:03.000]   What does that mean?
[00:22:03.000 --> 00:22:04.000]   Let's say you have a cooking robot,
[00:22:04.000 --> 00:22:06.000]   and you ask it to cook a meal,
[00:22:06.000 --> 00:22:08.000]   and now half an hour later, it comes back with a meal,
[00:22:08.000 --> 00:22:09.000]   and it gives it to you, and you're like,
[00:22:09.000 --> 00:22:11.000]   hmm, four-star meal.
[00:22:11.000 --> 00:22:13.000]   So okay, now the cooking robot knows
[00:22:13.000 --> 00:22:14.000]   that that was a four-star meal,
[00:22:14.000 --> 00:22:16.000]   but what did it do right or wrong in that past half hour?
[00:22:16.000 --> 00:22:17.000]   It doesn't really know.
[00:22:17.000 --> 00:22:19.000]   Then it might go cook again.
[00:22:19.000 --> 00:22:22.000]   It comes back, and now you say, three-star meal.
[00:22:22.000 --> 00:22:25.000]   And you might say, well, that means something it did
[00:22:25.000 --> 00:22:28.000]   was better the first time and worse the second time,
[00:22:28.000 --> 00:22:29.000]   but there might be a mix of things.
[00:22:29.000 --> 00:22:30.000]   There might be some things that were better
[00:22:30.000 --> 00:22:33.000]   the second time, even though overall it was worse.
[00:22:33.000 --> 00:22:35.000]   And so that's the credit assignment problem.
[00:22:35.000 --> 00:22:36.000]   Then your system going in and say,
[00:22:36.000 --> 00:22:38.000]   well, what did I do those two times,
[00:22:38.000 --> 00:22:39.000]   and what might have been,
[00:22:39.000 --> 00:22:42.000]   what contributed to it being better versus worse?
[00:22:42.000 --> 00:22:44.000]   From two trajectories, actually pretty much impossible.
[00:22:44.000 --> 00:22:47.000]   You often need thousands, tens of thousands of trials
[00:22:47.000 --> 00:22:49.000]   before it can tease apart the benefits
[00:22:49.000 --> 00:22:51.000]   that you get from things versus downsides.
[00:22:51.000 --> 00:22:54.000]   Another big challenge is stability.
[00:22:54.000 --> 00:22:55.000]   If you have a reinforced learning system,
[00:22:55.000 --> 00:22:58.000]   it just tries things, and if there's a feedback loop
[00:22:58.000 --> 00:23:01.000]   like you have here, you could destabilize the system,
[00:23:01.000 --> 00:23:04.000]   and it might crash, and then you cannot continue
[00:23:04.000 --> 00:23:06.000]   to learn maybe, and you need to fix things
[00:23:06.000 --> 00:23:08.000]   before things continue.
[00:23:08.000 --> 00:23:10.000]   Exploration's not a big one.
[00:23:10.000 --> 00:23:12.000]   The whole point in reinforced learning
[00:23:12.000 --> 00:23:13.000]   is that you become better than what you've been
[00:23:13.000 --> 00:23:16.000]   hard-coded to do, and so to become better
[00:23:16.000 --> 00:23:17.000]   than what you've been hard-coded to do,
[00:23:17.000 --> 00:23:20.000]   you need to somehow try things you never tried before.
[00:23:20.000 --> 00:23:22.000]   And of course, that again, interacts
[00:23:22.000 --> 00:23:24.000]   in difficult ways with stability and safety.
[00:23:24.000 --> 00:23:26.000]   Despite this challenge, actually,
[00:23:26.000 --> 00:23:27.000]   there's been a lot of progress.
[00:23:27.000 --> 00:23:31.000]   A big breakthrough in 2013 was Vlad and me
[00:23:31.000 --> 00:23:32.000]   and collaborators at DeepMind showing that
[00:23:32.000 --> 00:23:35.000]   a deep neural network can learn to play
[00:23:35.000 --> 00:23:36.000]   a wide range of Atari games.
[00:23:36.000 --> 00:23:38.000]   From its own trial and error,
[00:23:38.000 --> 00:23:40.000]   repeatedly trying to play the game
[00:23:40.000 --> 00:23:41.000]   and just learn to maximize score.
[00:23:41.000 --> 00:23:43.000]   A lot of other progress on this since,
[00:23:43.000 --> 00:23:46.000]   a lot of it out of DeepMind, OpenAI, and Berkeley,
[00:23:46.000 --> 00:23:48.000]   showing that you can do even much better
[00:23:48.000 --> 00:23:50.000]   than they did in 2013, but that was kind of
[00:23:50.000 --> 00:23:52.000]   the first, I would say, the first moment
[00:23:52.000 --> 00:23:55.000]   that people saw evidence that a lot is possible,
[00:23:55.000 --> 00:23:58.000]   that people didn't think before would be possible.
[00:23:58.000 --> 00:24:01.000]   Much like in computer vision in 2012,
[00:24:01.000 --> 00:24:02.000]   the ImageNet breakthrough.
[00:24:02.000 --> 00:24:05.000]   And of course, from there, DeepMind specifically
[00:24:05.000 --> 00:24:09.000]   took it to the game of Go, a traditionally seen
[00:24:09.000 --> 00:24:11.000]   as a very big challenge for computer science.
[00:24:11.000 --> 00:24:16.000]   In fact, in 2012, when surveys went out
[00:24:16.000 --> 00:24:18.000]   asking how long will it take before
[00:24:18.000 --> 00:24:20.000]   there's a human level Go player,
[00:24:20.000 --> 00:24:23.000]   the median probably landed around 20 to 30 years.
[00:24:23.000 --> 00:24:27.000]   But it actually only took from 2012 to 2015,
[00:24:27.000 --> 00:24:28.000]   so it only took three years.
[00:24:28.000 --> 00:24:29.000]   So a lot of people thought it was gonna
[00:24:29.000 --> 00:24:32.000]   take a very long time, and it happened three years later.
[00:24:32.000 --> 00:24:34.000]   You might wonder why the mismatch.
[00:24:34.000 --> 00:24:37.000]   Well, for a game like Go, you can think of it in two ways.
[00:24:37.000 --> 00:24:39.000]   You can say, okay, how can I play this game?
[00:24:39.000 --> 00:24:41.000]   Well, I make a move, y'all make a move,
[00:24:41.000 --> 00:24:43.000]   you make a move, I make a move, you make a move.
[00:24:43.000 --> 00:24:46.000]   It branches many, many ways, and the many futures
[00:24:46.000 --> 00:24:49.000]   that are possible, there's so many of them,
[00:24:49.000 --> 00:24:50.000]   you cannot enumerate them.
[00:24:50.000 --> 00:24:53.000]   Unlike a game like Tic-Tac-Toe and a three-by-three grid,
[00:24:53.000 --> 00:24:55.000]   you can think through all the possible ways
[00:24:55.000 --> 00:24:57.000]   of playing the game, and you can solve it
[00:24:57.000 --> 00:24:58.000]   in an explicit way.
[00:24:58.000 --> 00:25:00.000]   In Go, nobody has solved it explicitly.
[00:25:00.000 --> 00:25:02.000]   Nobody has provably shown that, let's say,
[00:25:02.000 --> 00:25:04.000]   white is guaranteed to win if they play correctly,
[00:25:04.000 --> 00:25:06.000]   or black is guaranteed to win.
[00:25:06.000 --> 00:25:08.000]   But that's how people were thinking about it back then.
[00:25:08.000 --> 00:25:10.000]   Another way to think of it is that it's just an image
[00:25:10.000 --> 00:25:14.000]   with white pixels, black pixels, and empty pixels,
[00:25:14.000 --> 00:25:16.000]   and if you can build a pattern recognizer
[00:25:16.000 --> 00:25:19.000]   that can predict whether your move is more likely to win
[00:25:19.000 --> 00:25:21.000]   than another move available to you,
[00:25:21.000 --> 00:25:23.000]   then that can guide you in what moves to make,
[00:25:23.000 --> 00:25:25.000]   and you can actually play really, really well.
[00:25:25.000 --> 00:25:26.000]   And that's effectively what happened,
[00:25:26.000 --> 00:25:28.000]   turning into a pattern recognition problem
[00:25:28.000 --> 00:25:30.000]   rather than a massive search problem.
[00:25:30.000 --> 00:25:32.000]   Admittedly, the pattern recognition is supervised
[00:25:32.000 --> 00:25:34.000]   by some shallow searches and so forth,
[00:25:34.000 --> 00:25:37.000]   so it's a mix of both, but that's really the big change,
[00:25:37.000 --> 00:25:39.000]   that you don't need to search that far.
[00:25:39.000 --> 00:25:41.000]   Pattern recognition can do a lot.
[00:25:41.000 --> 00:25:45.000]   OpenAI, around the same time as DeepMind showed Go,
[00:25:45.000 --> 00:25:47.000]   showed that it's possible to learn to play
[00:25:47.000 --> 00:25:51.000]   the game of Dota 2, which is a pretty popular video game
[00:25:51.000 --> 00:25:53.000]   where you play five on five, but initial results
[00:25:53.000 --> 00:25:55.000]   were one on one at the time.
[00:25:55.000 --> 00:25:58.000]   Recently, also five on five, where computer players
[00:25:58.000 --> 00:26:01.000]   can now do as well as human players.
[00:26:01.000 --> 00:26:04.000]   At Berkeley, our focus was really on robotics at the time,
[00:26:04.000 --> 00:26:06.000]   and so what we're watching here
[00:26:06.000 --> 00:26:08.000]   is actual learning in action, not just the result,
[00:26:08.000 --> 00:26:10.000]   but the learning happening, and the neural network
[00:26:10.000 --> 00:26:12.000]   is being updated each iteration,
[00:26:12.000 --> 00:26:14.000]   become better and better and better
[00:26:14.000 --> 00:26:16.000]   at controlling this robot.
[00:26:16.000 --> 00:26:18.000]   And so in about 2,000 updates to the network,
[00:26:18.000 --> 00:26:20.000]   it's able to run.
[00:26:20.000 --> 00:26:23.000]   And the beauty here is that the way you then
[00:26:23.000 --> 00:26:25.000]   wanna program a new robot to run,
[00:26:25.000 --> 00:26:27.000]   instead of thinking about Newtonian physics
[00:26:27.000 --> 00:26:29.000]   and what's different about this new,
[00:26:29.000 --> 00:26:32.000]   let's say maybe four-legged robot you're gonna get next,
[00:26:32.000 --> 00:26:34.000]   you just run the same code, and it can also learn
[00:26:34.000 --> 00:26:36.000]   to control a four-legged robot.
[00:26:36.000 --> 00:26:39.000]   In fact, the exact same code can learn to control,
[00:26:39.000 --> 00:26:42.000]   learn to play the video games, the Atari games,
[00:26:42.000 --> 00:26:44.000]   just as well.
[00:26:44.000 --> 00:26:46.000]   And so what you get here is something very, very general
[00:26:46.000 --> 00:26:48.000]   that learns from its own trial and error,
[00:26:48.000 --> 00:26:50.000]   and whatever problem you put it in front of,
[00:26:50.000 --> 00:26:54.000]   over time, it figures out how to solve that problem.
[00:26:56.000 --> 00:26:59.000]   Here we have four-legged locomotion,
[00:26:59.000 --> 00:27:01.000]   and traditionally, people would work on this
[00:27:01.000 --> 00:27:05.000]   by very carefully thinking about contact forces,
[00:27:05.000 --> 00:27:07.000]   center of mass relative to the support triangle
[00:27:07.000 --> 00:27:09.000]   of the three feet on the ground,
[00:27:09.000 --> 00:27:11.000]   static stability, dynamic stability,
[00:27:11.000 --> 00:27:13.000]   a lot of considerations, and actually have
[00:27:13.000 --> 00:27:15.000]   a very hard time getting this to work,
[00:27:15.000 --> 00:27:19.000]   but with learning, it can just automatically figure it out.
[00:27:19.000 --> 00:27:20.000]   Here's another problem, is how can you get
[00:27:20.000 --> 00:27:22.000]   your robot to get up?
[00:27:22.000 --> 00:27:25.000]   Here, the reward function is distance of,
[00:27:25.000 --> 00:27:27.000]   based on distance of the head to standing head height.
[00:27:27.000 --> 00:27:30.000]   The closer the head is to standing head height,
[00:27:30.000 --> 00:27:33.000]   the higher the reward, and it figures out standing up.
[00:27:33.000 --> 00:27:36.000]   What we have here is a very wide range of behavior,
[00:27:36.000 --> 00:27:38.000]   so this is the humanoid character, again,
[00:27:38.000 --> 00:27:40.000]   that's been trained to do pretty much
[00:27:40.000 --> 00:27:43.000]   any behavior a person could do, yes?
[00:27:43.000 --> 00:27:47.000]   (student speaks off-microphone)
[00:27:47.000 --> 00:27:49.000]   Yeah, it's a good question.
[00:27:49.000 --> 00:27:53.000]   So for one skill, I would say that if you were
[00:27:53.000 --> 00:27:57.000]   to run it on a physical system, in real time,
[00:27:57.000 --> 00:28:01.000]   and one physical system, it would take about two weeks,
[00:28:01.000 --> 00:28:03.000]   with the approaches I showed here.
[00:28:03.000 --> 00:28:05.000]   I think we can do much better in the near future,
[00:28:05.000 --> 00:28:07.000]   but with the approach I showed here,
[00:28:07.000 --> 00:28:08.000]   it would take about two weeks.
[00:28:08.000 --> 00:28:10.000]   In simulation, you can run faster than real time,
[00:28:10.000 --> 00:28:12.000]   you can run many simulations in parallel,
[00:28:12.000 --> 00:28:14.000]   you can do it in a couple hours.
[00:28:14.000 --> 00:28:20.000]   Doesn't need to be humanoid characters,
[00:28:20.000 --> 00:28:24.000]   this is a simulated lion, and its muscles are controlled
[00:28:24.000 --> 00:28:27.000]   by a neural network that's been trained beforehand
[00:28:27.000 --> 00:28:31.000]   to make it run, and withstanding perturbation forces
[00:28:31.000 --> 00:28:33.000]   shown with the big red arrow.
[00:28:33.000 --> 00:28:35.000]   You can also do it with real robots.
[00:28:35.000 --> 00:28:37.000]   This is Brett, the Berkeley robot
[00:28:37.000 --> 00:28:39.000]   for the elimination of tedious tasks.
[00:28:39.000 --> 00:28:43.000]   Brett lives on the seventh floor of the AI lab at Berkeley,
[00:28:43.000 --> 00:28:46.000]   and here he's practicing Lego block stacking.
[00:28:46.000 --> 00:28:48.000]   This is a slightly more confined skill,
[00:28:48.000 --> 00:28:50.000]   so it actually takes just one hour for it to learn
[00:28:50.000 --> 00:28:51.000]   to stack a Lego block.
[00:28:51.000 --> 00:28:55.000]   And with these results, actually what happened next
[00:28:55.000 --> 00:28:58.000]   is that NASA contacted us and wanted to see
[00:28:58.000 --> 00:29:00.000]   if we could reinforce some learning,
[00:29:00.000 --> 00:29:03.000]   could enable to learn to control this robot.
[00:29:03.000 --> 00:29:04.000]   So you might look at this robot and say,
[00:29:04.000 --> 00:29:06.000]   why would you build a robot this way?
[00:29:06.000 --> 00:29:09.000]   It's, why not just build a Mars rover with wheels,
[00:29:09.000 --> 00:29:10.000]   like we're used to?
[00:29:10.000 --> 00:29:13.000]   Here's the reasoning.
[00:29:13.000 --> 00:29:17.000]   They like this format because if you extend the cables,
[00:29:18.000 --> 00:29:20.000]   you can bundle it together, it's a bit like a tent,
[00:29:20.000 --> 00:29:23.000]   effectively, the tent fits in a small bag,
[00:29:23.000 --> 00:29:26.000]   but then when you expand it, it's a big robot.
[00:29:26.000 --> 00:29:28.000]   Because it's pretty big when you expand it,
[00:29:28.000 --> 00:29:30.000]   then it can actually be seen, be thought of as having
[00:29:30.000 --> 00:29:33.000]   a big wheel in every possible direction at any given time,
[00:29:33.000 --> 00:29:35.000]   'cause it's the shape of a sphere.
[00:29:35.000 --> 00:29:37.000]   Only problem then is how do you make it move?
[00:29:37.000 --> 00:29:38.000]   And traditionally what you would do
[00:29:38.000 --> 00:29:39.000]   is you would think very carefully about,
[00:29:39.000 --> 00:29:41.000]   how do I shift the center of mass?
[00:29:41.000 --> 00:29:43.000]   Which cable should I make shorter or longer
[00:29:43.000 --> 00:29:44.000]   to make the center of mass shift,
[00:29:44.000 --> 00:29:46.000]   and then it might tumble and repeat, repeat.
[00:29:46.000 --> 00:29:48.000]   They tried it for a long time,
[00:29:48.000 --> 00:29:49.000]   but actually never really worked.
[00:29:49.000 --> 00:29:54.000]   Then with reinforcement learning, you just let it run for,
[00:29:54.000 --> 00:29:56.000]   I forget exactly how long this experiment took to run,
[00:29:56.000 --> 00:29:58.000]   but probably a couple days.
[00:29:58.000 --> 00:30:00.000]   We actually fully trained this in simulation,
[00:30:00.000 --> 00:30:03.000]   and then the controller worked on the real robot,
[00:30:03.000 --> 00:30:06.000]   even though it was fully trained in simulation.
[00:30:06.000 --> 00:30:09.000]   And this is actually in Over Mountain View at NASA Ames.
[00:30:09.000 --> 00:30:12.000]   So a lot of good successes there,
[00:30:12.000 --> 00:30:14.000]   and so I would say reinforcement learning
[00:30:14.000 --> 00:30:16.000]   is kind of becoming ready to be applied
[00:30:16.000 --> 00:30:18.000]   if you're willing to collect a lot of data,
[00:30:18.000 --> 00:30:20.000]   which often is a little tricky, by the way,
[00:30:20.000 --> 00:30:22.000]   because while you collect data, you might break things.
[00:30:22.000 --> 00:30:24.000]   So in real life, systems collecting data
[00:30:24.000 --> 00:30:25.000]   can be a little tricky.
[00:30:25.000 --> 00:30:29.000]   So master's achieved on all of these tasks
[00:30:29.000 --> 00:30:31.000]   that I showed to you, but another question you could ask,
[00:30:31.000 --> 00:30:33.000]   how smart is this system really?
[00:30:33.000 --> 00:30:36.000]   When you think about people, and you say,
[00:30:36.000 --> 00:30:38.000]   well, maybe somebody is like, I don't know,
[00:30:38.000 --> 00:30:39.000]   really good at something,
[00:30:39.000 --> 00:30:41.000]   does that mean they're very smart?
[00:30:41.000 --> 00:30:43.000]   Not necessarily, like you would probably then wonder,
[00:30:43.000 --> 00:30:45.000]   how long have they been learning that?
[00:30:45.000 --> 00:30:47.000]   If they learned it overnight, they must be really smart,
[00:30:47.000 --> 00:30:49.000]   'cause that's a complex thing to acquire,
[00:30:49.000 --> 00:30:51.000]   but if they've been doing it for 30 years,
[00:30:51.000 --> 00:30:53.000]   nothing but that one thing, and they're really good at it,
[00:30:53.000 --> 00:30:55.000]   well, maybe that doesn't mean necessarily
[00:30:55.000 --> 00:30:56.000]   that they're that smart.
[00:30:56.000 --> 00:30:58.000]   And same for these reinforced learning systems.
[00:30:58.000 --> 00:31:01.000]   Question could be, how good is the learning,
[00:31:01.000 --> 00:31:02.000]   rather than the final result?
[00:31:02.000 --> 00:31:05.000]   If you look at the Atari games,
[00:31:05.000 --> 00:31:08.000]   these plots essentially summarize how long it takes
[00:31:08.000 --> 00:31:10.000]   a human to learn to play a new game
[00:31:10.000 --> 00:31:15.000]   versus a Double DQN, which is at the time,
[00:31:15.000 --> 00:31:17.000]   a couple years ago, the most sample efficient algorithm,
[00:31:17.000 --> 00:31:19.000]   and currently there are not that many that are,
[00:31:19.000 --> 00:31:22.000]   I mean, it's not very different from what we have today.
[00:31:22.000 --> 00:31:25.000]   Double DQN takes 115 hours to become as good as a human
[00:31:25.000 --> 00:31:27.000]   can become in 15 minutes.
[00:31:27.000 --> 00:31:31.000]   So many orders of magnitude different in learning speed.
[00:31:31.000 --> 00:31:34.000]   So question then becomes, how do we bridge this gap?
[00:31:34.000 --> 00:31:37.000]   It's gonna be a similar thinking to what we had before.
[00:31:37.000 --> 00:31:39.000]   What we did for classification,
[00:31:39.000 --> 00:31:42.000]   went from classification to few-shot classification.
[00:31:42.000 --> 00:31:45.000]   We'll think about whether we can do something similar here.
[00:31:45.000 --> 00:31:48.000]   If you look at methods like TRPO, DQN, A3C, and so forth,
[00:31:48.000 --> 00:31:50.000]   they're all fully general methods,
[00:31:50.000 --> 00:31:52.000]   which means no matter what problem you face them with,
[00:31:52.000 --> 00:31:55.000]   they'll be able to solve it, which is nice,
[00:31:55.000 --> 00:31:57.000]   but they also don't have any prior information
[00:31:57.000 --> 00:31:58.000]   what the problem might be,
[00:31:58.000 --> 00:31:59.000]   and so they're not really attuned
[00:31:59.000 --> 00:32:01.000]   to solving a problem more quickly
[00:32:01.000 --> 00:32:03.000]   by having any prior understanding
[00:32:03.000 --> 00:32:05.000]   of what problems it could encounter.
[00:32:05.000 --> 00:32:07.000]   And in the real world, the environments that we encounter
[00:32:07.000 --> 00:32:09.000]   are a very tiny subset of the environments
[00:32:09.000 --> 00:32:11.000]   we can define mathematically.
[00:32:11.000 --> 00:32:13.000]   For example, every real world environment
[00:32:13.000 --> 00:32:16.000]   will satisfy our world's, our universe's physics,
[00:32:16.000 --> 00:32:19.000]   but in principle, you could define other physics laws,
[00:32:19.000 --> 00:32:21.000]   and that would be a new environment in simulation,
[00:32:21.000 --> 00:32:24.000]   but it just, you wouldn't encounter it in the real world.
[00:32:24.000 --> 00:32:26.000]   So the question is, can we develop faster algorithms
[00:32:26.000 --> 00:32:29.000]   that take advantage of this prior knowledge somehow?
[00:32:29.000 --> 00:32:32.000]   One way you might wanna do it
[00:32:32.000 --> 00:32:34.000]   is you might just wanna build it into it,
[00:32:34.000 --> 00:32:35.000]   but that's actually pretty hard.
[00:32:35.000 --> 00:32:36.000]   So what we're gonna look at is,
[00:32:36.000 --> 00:32:38.000]   can we somehow have this agent learn ahead of time
[00:32:38.000 --> 00:32:40.000]   some things so that they can more quickly
[00:32:40.000 --> 00:32:42.000]   learn something new in the future?
[00:32:42.000 --> 00:32:44.000]   So what is an agent anyway?
[00:32:44.000 --> 00:32:46.000]   Inside an agent, there's usually a learning algorithm
[00:32:46.000 --> 00:32:48.000]   and then a policy or a Q function
[00:32:48.000 --> 00:32:50.000]   that's being updated by the learning algorithm.
[00:32:50.000 --> 00:32:53.000]   The policy or Q function does the interaction
[00:32:53.000 --> 00:32:54.000]   with the environment, and the learning algorithm
[00:32:54.000 --> 00:32:57.000]   observes that, and from the sequence of interactions,
[00:32:57.000 --> 00:32:59.000]   updates the policy.
[00:32:59.000 --> 00:33:01.000]   And so in environment A, policy gets updated
[00:33:01.000 --> 00:33:02.000]   to be specialized to environment A.
[00:33:02.000 --> 00:33:05.000]   Environment B gets specialized to environment B,
[00:33:05.000 --> 00:33:05.840]   and so forth.
[00:33:05.840 --> 00:33:09.600]   Traditional RL research is really about designing
[00:33:09.600 --> 00:33:13.000]   that RL algorithm box, and the hope is that somehow
[00:33:13.000 --> 00:33:14.200]   you come up with a really good algorithm
[00:33:14.200 --> 00:33:16.000]   that allows you to learn very quickly.
[00:33:16.000 --> 00:33:19.000]   But despite many, many decades of research on this,
[00:33:19.000 --> 00:33:21.000]   none of these algorithms work nearly as well as
[00:33:21.000 --> 00:33:25.000]   compared to humans how fast they pick up a new skill.
[00:33:25.000 --> 00:33:27.000]   So how about an alternative?
[00:33:27.000 --> 00:33:28.000]   How about instead of just learning the policy,
[00:33:28.000 --> 00:33:30.000]   we also learn the algorithm?
[00:33:30.000 --> 00:33:31.000]   Maybe if you learn the algorithm,
[00:33:31.000 --> 00:33:33.000]   we can learn something much better
[00:33:33.000 --> 00:33:36.000]   than we've been able to design so far by hand.
[00:33:36.000 --> 00:33:38.000]   So what would that mean?
[00:33:38.000 --> 00:33:40.000]   There would be meta-training environments,
[00:33:40.000 --> 00:33:43.000]   and the algorithm gets to interact with these environments,
[00:33:43.000 --> 00:33:44.000]   and from those interactions, it's supposed to extract
[00:33:44.000 --> 00:33:48.000]   something that then becomes a new type of agent
[00:33:48.000 --> 00:33:49.000]   that could learn a lot more quickly
[00:33:49.000 --> 00:33:52.000]   than a traditional RL agent.
[00:33:52.000 --> 00:33:54.000]   In environment F, it's never seen before,
[00:33:54.000 --> 00:33:57.000]   it should learn quickly, or G, and so forth.
[00:33:57.000 --> 00:34:00.000]   So how can we formalize this?
[00:34:00.000 --> 00:34:02.000]   Here's one way to formalize this.
[00:34:02.000 --> 00:34:04.000]   What we want is an agent parameterized
[00:34:04.000 --> 00:34:07.000]   by a parameter vector theta that is such that
[00:34:07.000 --> 00:34:09.000]   if dropped in a random environment M,
[00:34:09.000 --> 00:34:13.000]   and getting capital K episodes in that environment M,
[00:34:13.000 --> 00:34:15.000]   it should collect high reward.
[00:34:15.000 --> 00:34:16.000]   If we keep capital K small,
[00:34:16.000 --> 00:34:19.000]   in this drawing here, capital K equals two,
[00:34:19.000 --> 00:34:20.000]   that means that we expect an agent
[00:34:20.000 --> 00:34:22.000]   when dropped in a new environment,
[00:34:22.000 --> 00:34:23.000]   and only having two episodes,
[00:34:23.000 --> 00:34:25.000]   to already collect high reward.
[00:34:25.000 --> 00:34:29.000]   If it can do that, it means it learns real, really quickly.
[00:34:29.000 --> 00:34:31.000]   And how do we find this agent?
[00:34:31.000 --> 00:34:33.000]   Well, we can actually set this up
[00:34:33.000 --> 00:34:35.000]   as an optimization problem again.
[00:34:35.000 --> 00:34:38.000]   We have an objective, which is to find this agent
[00:34:38.000 --> 00:34:40.000]   that collects high reward
[00:34:40.000 --> 00:34:42.000]   when only having a small number of episodes
[00:34:42.000 --> 00:34:45.000]   in a randomly chosen environment.
[00:34:45.000 --> 00:34:47.000]   And then the hope would be,
[00:34:47.000 --> 00:34:49.000]   just like in supervised learning,
[00:34:49.000 --> 00:34:52.000]   that if you do really well on your training examples
[00:34:52.000 --> 00:34:53.000]   in supervised learning,
[00:34:53.000 --> 00:34:56.000]   in training environments in reinforcement learning here,
[00:34:56.000 --> 00:34:58.000]   that you'd also do well on new environments
[00:34:58.000 --> 00:35:00.000]   you've never seen before at meta-testing time.
[00:35:00.000 --> 00:35:01.000]   Question?
[00:35:01.000 --> 00:35:04.000]   - So this would depend on how close
[00:35:04.000 --> 00:35:06.000]   the new environment is to the older environment.
[00:35:06.000 --> 00:35:07.000]   - Exactly, absolutely.
[00:35:07.000 --> 00:35:10.000]   - How do you figure that part out?
[00:35:10.000 --> 00:35:11.000]   - Yeah, so--
[00:35:11.000 --> 00:35:12.000]   - Like these are the environments
[00:35:12.000 --> 00:35:13.000]   I should be training on.
[00:35:13.000 --> 00:35:14.000]   - Yeah, so that's exactly, I would say,
[00:35:14.000 --> 00:35:16.000]   where a big part of the frontier,
[00:35:16.000 --> 00:35:18.000]   and where the frontier needs to move much further.
[00:35:18.000 --> 00:35:20.000]   So I'll give you some examples where this works,
[00:35:20.000 --> 00:35:23.000]   and you'll see that training and test environments
[00:35:23.000 --> 00:35:24.000]   are fairly related,
[00:35:24.000 --> 00:35:29.000]   which means that it doesn't generalize as widely yet
[00:35:29.000 --> 00:35:31.000]   as humans can generalize,
[00:35:31.000 --> 00:35:33.000]   but it shows some evidence that this is possible
[00:35:33.000 --> 00:35:35.000]   if there's a reasonable relationship
[00:35:35.000 --> 00:35:38.000]   between training environments and testing environments.
[00:35:38.000 --> 00:35:42.000]   So what could we make the agent?
[00:35:42.000 --> 00:35:45.000]   Maybe we could make it a recurrent neural network.
[00:35:45.000 --> 00:35:46.000]   Why is that meaningful?
[00:35:46.000 --> 00:35:47.000]   A recurrent neural network can encode
[00:35:47.000 --> 00:35:49.000]   any computer program effectively,
[00:35:49.000 --> 00:35:51.000]   and so if we think that there exists
[00:35:51.000 --> 00:35:53.000]   a good reinforcement learning agent out there,
[00:35:53.000 --> 00:35:55.000]   some kind of system that learns fast,
[00:35:56.000 --> 00:35:59.000]   then allowing it to be a recurrent neural network
[00:35:59.000 --> 00:36:01.000]   in principle should allow it to find that program
[00:36:01.000 --> 00:36:03.000]   that can do this for us.
[00:36:03.000 --> 00:36:07.000]   The weights in the RNN would correspond
[00:36:07.000 --> 00:36:10.000]   to the algorithm that essentially does the learning
[00:36:10.000 --> 00:36:11.000]   from seeing experiences,
[00:36:11.000 --> 00:36:14.000]   as well as the prior of what it might expect
[00:36:14.000 --> 00:36:16.000]   to be a good policy,
[00:36:16.000 --> 00:36:18.000]   or what this environment might be like.
[00:36:18.000 --> 00:36:20.000]   And then the activations in the recurrent neural net,
[00:36:20.000 --> 00:36:23.000]   they will change as you act in a new environment.
[00:36:23.000 --> 00:36:26.000]   These activations correspond to having fine tuned
[00:36:26.000 --> 00:36:27.000]   to the current environment,
[00:36:27.000 --> 00:36:30.000]   and so they encode the policy that you have currently.
[00:36:30.000 --> 00:36:32.000]   The objective itself can be trained
[00:36:32.000 --> 00:36:33.000]   with reinforcement learning.
[00:36:33.000 --> 00:36:34.000]   So you can actually learn
[00:36:34.000 --> 00:36:36.000]   a fast reinforcement learning agent
[00:36:36.000 --> 00:36:39.000]   by running a classical human designed
[00:36:39.000 --> 00:36:41.000]   slow reinforcement learning algorithm.
[00:36:41.000 --> 00:36:43.000]   Because if you look at this objective,
[00:36:43.000 --> 00:36:45.000]   it's like reinforcement learning.
[00:36:45.000 --> 00:36:49.000]   It's just that when you start off
[00:36:49.000 --> 00:36:50.000]   in reinforcement learning,
[00:36:50.000 --> 00:36:52.000]   you tend to always start off in the same environment,
[00:36:52.000 --> 00:36:54.000]   possibly a random state,
[00:36:54.000 --> 00:36:55.000]   but the same environment in here,
[00:36:55.000 --> 00:36:57.000]   you just start off in a random environment,
[00:36:57.000 --> 00:36:59.000]   but otherwise, it's the same kind of objective.
[00:36:59.000 --> 00:37:04.000]   One place we tested this is in multi-armed bandit problems.
[00:37:04.000 --> 00:37:05.000]   So multi-armed bandit is a problem
[00:37:05.000 --> 00:37:07.000]   where you have many bandits to choose from.
[00:37:07.000 --> 00:37:09.000]   Each bandit has a different probability of payoff,
[00:37:09.000 --> 00:37:12.000]   but you don't know the probabilities ahead of time.
[00:37:12.000 --> 00:37:13.000]   And so you're supposed to explore,
[00:37:13.000 --> 00:37:15.000]   see which ones have high probability of payoff,
[00:37:15.000 --> 00:37:16.000]   and then exploit,
[00:37:16.000 --> 00:37:18.000]   zone in the ones that pays off more frequently.
[00:37:18.000 --> 00:37:21.000]   So this is a canonical reinforcement learning problem,
[00:37:21.000 --> 00:37:22.000]   but in some sense,
[00:37:22.000 --> 00:37:23.000]   the simplest one that exists out there.
[00:37:23.000 --> 00:37:25.000]   And it turns out that humans have designed
[00:37:25.000 --> 00:37:29.000]   asymptotically optimal algorithms for this.
[00:37:29.000 --> 00:37:32.000]   So you can rely on something like Gittins indices
[00:37:32.000 --> 00:37:35.000]   to optimally explore this bandit space
[00:37:35.000 --> 00:37:39.000]   and on expectation maximize reward.
[00:37:39.000 --> 00:37:40.000]   So the question then becomes,
[00:37:40.000 --> 00:37:42.000]   can we have a reinforcement learning agent
[00:37:42.000 --> 00:37:44.000]   that has learned to reinforcement learn,
[00:37:44.000 --> 00:37:46.000]   such that it does as well as these
[00:37:46.000 --> 00:37:48.000]   asymptotically optimal algorithms?
[00:37:48.000 --> 00:37:49.000]   Well, compared to many of them,
[00:37:49.000 --> 00:37:52.000]   Gittins did the best among the provably optimal ones
[00:37:52.000 --> 00:37:53.000]   designed by humans.
[00:37:53.000 --> 00:37:55.000]   And we see that learning to reinforce learning
[00:37:55.000 --> 00:37:57.000]   does about as well as Gittins.
[00:37:57.000 --> 00:37:59.000]   So it means that we can do as well
[00:37:59.000 --> 00:38:00.000]   as the best human design ones,
[00:38:00.000 --> 00:38:02.000]   which we know are asymptotically optimal
[00:38:02.000 --> 00:38:04.000]   by just learning the agent,
[00:38:04.000 --> 00:38:06.000]   by having it face many, many bandit problems.
[00:38:06.000 --> 00:38:08.000]   And over time, the recurrent neural network
[00:38:08.000 --> 00:38:10.000]   becomes really good at facing a new bandit problem
[00:38:10.000 --> 00:38:12.000]   it's never seen before.
[00:38:12.000 --> 00:38:14.000]   Here's a robotic setting,
[00:38:14.000 --> 00:38:17.000]   still a narrow task distribution, to your point.
[00:38:17.000 --> 00:38:19.000]   This is the half cheetah
[00:38:19.000 --> 00:38:21.000]   classical reinforcement learning domain.
[00:38:21.000 --> 00:38:25.000]   The cheetah is gonna always be the same.
[00:38:25.000 --> 00:38:27.000]   The only thing that's gonna change
[00:38:27.000 --> 00:38:29.000]   is the task description,
[00:38:29.000 --> 00:38:31.000]   which is the speed at which it's supposed to run.
[00:38:31.000 --> 00:38:34.000]   And so a new environment means a new target speed,
[00:38:34.000 --> 00:38:37.000]   which will experience as it runs at a certain speed,
[00:38:37.000 --> 00:38:38.000]   it'll see how much reward it gets.
[00:38:38.000 --> 00:38:40.000]   Whereas a different speed gets different reward.
[00:38:40.000 --> 00:38:41.000]   And you'd hope that from that,
[00:38:41.000 --> 00:38:43.000]   it can figure out what the right speed is
[00:38:43.000 --> 00:38:46.000]   and fine tune onto that right speed quickly.
[00:38:46.000 --> 00:38:48.000]   So here's this in action.
[00:38:48.000 --> 00:38:51.000]   This is the first episode in this environment
[00:38:51.000 --> 00:38:53.000]   with this specific reward function.
[00:38:53.000 --> 00:38:55.000]   And we see that from the first episode,
[00:38:55.000 --> 00:38:58.000]   it already starts achieving high reward.
[00:38:58.000 --> 00:38:59.000]   Now to be fair,
[00:38:59.000 --> 00:39:01.000]   we had a lot of meta training ahead of time
[00:39:01.000 --> 00:39:03.000]   where it was trained to be ready
[00:39:03.000 --> 00:39:05.000]   to be in an environment that's exactly like this.
[00:39:05.000 --> 00:39:07.000]   It just doesn't know ahead of time
[00:39:07.000 --> 00:39:09.000]   what the reward function is going to be.
[00:39:09.000 --> 00:39:11.000]   Same thing for AntRobot.
[00:39:11.000 --> 00:39:13.000]   Oh, didn't play.
[00:39:13.000 --> 00:39:15.000]   Didn't play.
[00:39:15.000 --> 00:39:27.000]   Huh.
[00:39:27.000 --> 00:39:36.000]   I don't have full control over my mouse cursor.
[00:39:36.000 --> 00:39:38.000]   Not sure why.
[00:39:39.000 --> 00:39:44.000]   Okay, we'll skip the end.
[00:39:44.000 --> 00:39:46.000]   We'll go to the next one.
[00:39:46.000 --> 00:39:48.000]   Maze navigation.
[00:39:48.000 --> 00:39:50.000]   This is a wider range of environments.
[00:39:50.000 --> 00:39:51.000]   So it's a very classical robotics problem.
[00:39:51.000 --> 00:39:53.000]   When you're dropped in a new environment,
[00:39:53.000 --> 00:39:56.000]   can you find your way to a target if you don't have a map?
[00:39:56.000 --> 00:39:58.000]   And you might not even know where the target is,
[00:39:58.000 --> 00:40:01.000]   but you'll recognize it in this case by being very red.
[00:40:01.000 --> 00:40:02.000]   So the map is shown on the right,
[00:40:02.000 --> 00:40:05.000]   but it's not available to the system.
[00:40:05.000 --> 00:40:08.000]   Just for our purposes to see what's going on.
[00:40:08.000 --> 00:40:10.000]   The system just gets a first person camera view
[00:40:10.000 --> 00:40:12.000]   and can steer two degrees to the left,
[00:40:12.000 --> 00:40:15.000]   two degrees to the right, or keep going straight.
[00:40:15.000 --> 00:40:17.000]   If you randomly move in this environment,
[00:40:17.000 --> 00:40:19.000]   this is the kind of motion you get.
[00:40:19.000 --> 00:40:22.000]   It's not very exploratory and not very good in general.
[00:40:22.000 --> 00:40:26.000]   Then after the agent has trained
[00:40:26.000 --> 00:40:29.000]   in many, many other mazes in the past,
[00:40:29.000 --> 00:40:32.000]   and every time it got two episodes in each maze,
[00:40:32.000 --> 00:40:34.000]   and then a new maze, new maze, new maze,
[00:40:34.000 --> 00:40:37.000]   it's now dropped in yet another new one.
[00:40:37.000 --> 00:40:39.000]   Let's see what happens.
[00:40:39.000 --> 00:40:40.000]   We actually get what we hoped for.
[00:40:40.000 --> 00:40:45.000]   It's running down, always looking for a target,
[00:40:45.000 --> 00:40:47.000]   sees the target, runs to it,
[00:40:47.000 --> 00:40:51.000]   and then quickly runs to the target again.
[00:40:51.000 --> 00:40:52.000]   So remember enough about that environment
[00:40:52.000 --> 00:40:53.000]   to go to the target again.
[00:40:53.000 --> 00:40:56.000]   So sometimes you build an internal map of the space
[00:40:56.000 --> 00:40:58.000]   to go visit that same spot again.
[00:40:58.000 --> 00:41:00.000]   How well does it work?
[00:41:00.000 --> 00:41:01.000]   Here are learning curves,
[00:41:01.000 --> 00:41:02.000]   and these are interesting learning curves.
[00:41:02.000 --> 00:41:05.000]   They're not the curves that you're used to.
[00:41:05.000 --> 00:41:08.000]   So horizontal axis is still iterations.
[00:41:08.000 --> 00:41:09.000]   When you see something like this, you'd probably say,
[00:41:09.000 --> 00:41:11.000]   oh, must be some training, some test curves,
[00:41:11.000 --> 00:41:12.000]   and training is doing really well,
[00:41:12.000 --> 00:41:14.000]   and test is not doing as well.
[00:41:14.000 --> 00:41:15.000]   But actually in meta-training,
[00:41:15.000 --> 00:41:17.000]   where we generate our environments,
[00:41:17.000 --> 00:41:19.000]   every time when we're training,
[00:41:19.000 --> 00:41:21.000]   we generate a new environment.
[00:41:21.000 --> 00:41:23.000]   So train and test are now different,
[00:41:23.000 --> 00:41:24.000]   'cause while we're training,
[00:41:24.000 --> 00:41:26.000]   we're only seeing an environment again and again
[00:41:26.000 --> 00:41:28.000]   that we've never seen before, never any repeats.
[00:41:28.000 --> 00:41:30.000]   So these are all training curves,
[00:41:30.000 --> 00:41:32.000]   but with different random seeds.
[00:41:32.000 --> 00:41:34.000]   And the training curves, which are also test curves,
[00:41:34.000 --> 00:41:36.000]   because there's always a new environment,
[00:41:36.000 --> 00:41:38.000]   show that sometimes it doesn't work so well.
[00:41:38.000 --> 00:41:40.000]   You might wonder what's going on.
[00:41:40.000 --> 00:41:42.000]   How could it not work well?
[00:41:42.000 --> 00:41:44.000]   Well, one way it could not work well is that,
[00:41:44.000 --> 00:41:47.000]   let's say it finally finds the target in the current maze,
[00:41:47.000 --> 00:41:49.000]   and then it memorizes, okay, I now know,
[00:41:49.000 --> 00:41:51.000]   you go straight, then right, then left, then right,
[00:41:51.000 --> 00:41:53.000]   then somewhere straight, and that's where the target is.
[00:41:53.000 --> 00:41:55.000]   It gets dropped in a new maze, it doesn't work anymore.
[00:41:55.000 --> 00:41:57.000]   That would be overfitting.
[00:41:57.000 --> 00:41:59.000]   And that is easy to avoid, actually.
[00:41:59.000 --> 00:42:01.000]   You just make smaller updates, regularize,
[00:42:01.000 --> 00:42:03.000]   you can avoid that kind of overfitting.
[00:42:03.000 --> 00:42:05.000]   What we're seeing here is actually underfitting.
[00:42:05.000 --> 00:42:07.000]   It's where it just doesn't get enough signal
[00:42:07.000 --> 00:42:09.000]   when it just randomly explores a maze,
[00:42:09.000 --> 00:42:12.000]   but it takes so long before it finally finds a target
[00:42:12.000 --> 00:42:14.000]   that it cannot do the credit assignment anymore.
[00:42:14.000 --> 00:42:16.000]   It doesn't get enough signal to understand
[00:42:16.000 --> 00:42:18.000]   what it should learn from this experience,
[00:42:18.000 --> 00:42:20.000]   and it just ends up not learning.
[00:42:20.000 --> 00:42:24.000]   Part of it is the architecture we used.
[00:42:24.000 --> 00:42:25.000]   We use a recurrent neural network,
[00:42:25.000 --> 00:42:27.000]   and recurrent neural networks are known,
[00:42:27.000 --> 00:42:29.000]   if the horizon's very, very long,
[00:42:29.000 --> 00:42:31.000]   then it's hard to propagate all that signal back.
[00:42:31.000 --> 00:42:33.000]   Usually, you do some kind of, you stop the gradient,
[00:42:33.000 --> 00:42:36.000]   you clip it maybe at 20 steps, and 20 steps,
[00:42:36.000 --> 00:42:39.000]   well, if it takes 1,000 steps to find a target,
[00:42:39.000 --> 00:42:41.000]   if you clip it at 20 steps, you're not learning.
[00:42:41.000 --> 00:42:43.000]   You're not seeing what you're supposed to see.
[00:42:43.000 --> 00:42:45.000]   And if you don't clip at 20 steps,
[00:42:45.000 --> 00:42:46.000]   well, then your gradients might explode or vanish,
[00:42:46.000 --> 00:42:48.000]   you don't learn either.
[00:42:48.000 --> 00:42:51.000]   We actually looked at a different architecture called Snail.
[00:42:51.000 --> 00:42:53.000]   This is now two years ago.
[00:42:53.000 --> 00:42:56.000]   It's an architecture that combines ideas
[00:42:56.000 --> 00:42:59.000]   from WaveNets, which is dilated temporal convolutions.
[00:42:59.000 --> 00:43:01.000]   So instead of having hidden units,
[00:43:01.000 --> 00:43:03.000]   we're gonna have dilated temporal convolution,
[00:43:03.000 --> 00:43:06.000]   but also attention is all you need
[00:43:06.000 --> 00:43:09.000]   to be able to see in detail what's there in the past.
[00:43:09.000 --> 00:43:11.000]   Because if you just use a WaveNet,
[00:43:11.000 --> 00:43:12.000]   you would lose a lot of detail,
[00:43:12.000 --> 00:43:14.000]   and you would not remember enough about the past.
[00:43:14.000 --> 00:43:17.000]   So you have both the tension and dilated convolutions.
[00:43:17.000 --> 00:43:19.000]   With that, we can actually solve the bandits
[00:43:19.000 --> 00:43:21.000]   better than with the LSTMs,
[00:43:21.000 --> 00:43:23.000]   and we can also solve bigger mazes
[00:43:24.000 --> 00:43:26.000]   than we can solve with the LSTM version.
[00:43:26.000 --> 00:43:29.000]   So the large maze, it has much shorter times,
[00:43:29.000 --> 00:43:31.000]   which is better.
[00:43:31.000 --> 00:43:33.000]   And here's an example of a much larger maze
[00:43:33.000 --> 00:43:34.000]   that's solvable this way.
[00:43:34.000 --> 00:43:36.000]   This is an example of where we essentially run
[00:43:36.000 --> 00:43:38.000]   the same algorithm at a high level,
[00:43:38.000 --> 00:43:41.000]   all we change is the architecture from LSTMs to,
[00:43:41.000 --> 00:43:45.000]   in this case, attention plus dilated convolutions,
[00:43:45.000 --> 00:43:49.000]   and it allows the signal to propagate and learn.
[00:43:49.000 --> 00:43:51.000]   This is at meta testing time.
[00:43:51.000 --> 00:43:53.000]   You might say, why is it taking so long
[00:43:53.000 --> 00:43:54.000]   before it actually finds a target?
[00:43:54.000 --> 00:43:56.000]   Well, it doesn't know where the target is.
[00:43:56.000 --> 00:43:58.000]   And so we picked specifically an example here
[00:43:58.000 --> 00:44:00.000]   where it's unlucky in all its choices,
[00:44:00.000 --> 00:44:02.000]   so you can see that it is smart enough
[00:44:02.000 --> 00:44:05.000]   to not revisit places it's been to before.
[00:44:05.000 --> 00:44:07.000]   So it remembers enough, 'cause it doesn't have the map,
[00:44:07.000 --> 00:44:09.000]   but it remembers enough in that past experience
[00:44:09.000 --> 00:44:10.000]   in the first person vision
[00:44:10.000 --> 00:44:12.000]   to not revisit places it's been before.
[00:44:12.000 --> 00:44:16.000]   There's a lot of work in meta learning for RL also,
[00:44:16.000 --> 00:44:18.000]   not just the work I've shown here,
[00:44:18.000 --> 00:44:19.000]   but probably the maze navigation
[00:44:20.000 --> 00:44:24.000]   is still some of the most impressive results out there.
[00:44:24.000 --> 00:44:29.000]   And one thing I wanna highlight
[00:44:29.000 --> 00:44:31.000]   is the model-agnostic meta learning that I showed earlier
[00:44:31.000 --> 00:44:34.000]   can actually also be applied to reinforcement learning.
[00:44:34.000 --> 00:44:35.000]   Here I show a different approach,
[00:44:35.000 --> 00:44:38.000]   but there is some connection you can make there too.
[00:44:38.000 --> 00:44:40.000]   I would say the big challenge with it
[00:44:40.000 --> 00:44:43.000]   is coming to your question is the environments
[00:44:43.000 --> 00:44:44.000]   in which we test.
[00:44:44.000 --> 00:44:45.000]   So people are starting to design,
[00:44:45.000 --> 00:44:48.000]   we've had VisDoom, DeepMindLab, Majoka environments,
[00:44:48.000 --> 00:44:50.000]   but people are starting to design new contests
[00:44:50.000 --> 00:44:53.000]   around, let's say, video games with multiple levels.
[00:44:53.000 --> 00:44:56.000]   You can get to meta train on levels one through 20,
[00:44:56.000 --> 00:44:58.000]   but then meta test on levels maybe 21 through 30.
[00:44:58.000 --> 00:45:00.000]   And can you adapt quickly?
[00:45:00.000 --> 00:45:02.000]   And so opening out how to contest around it.
[00:45:02.000 --> 00:45:04.000]   The results have not shown
[00:45:04.000 --> 00:45:07.000]   kind of meta learning breakthroughs yet.
[00:45:07.000 --> 00:45:09.000]   Maybe it's too hard, maybe it's too easy.
[00:45:09.000 --> 00:45:12.000]   Kind of standard fine tuning worked just fine
[00:45:12.000 --> 00:45:14.000]   in the retro contest that was held.
[00:45:14.000 --> 00:45:16.000]   But that's really one of the big challenges,
[00:45:16.000 --> 00:45:18.000]   how to design a range of environments
[00:45:18.000 --> 00:45:20.000]   such that you can meta train and generalize
[00:45:20.000 --> 00:45:22.000]   beyond what we've seen so far.
[00:45:22.000 --> 00:45:26.000]   Another way you can get a lot of signal action
[00:45:26.000 --> 00:45:29.000]   instead of reinforcement learning is imitation learning,
[00:45:29.000 --> 00:45:31.000]   where you get examples to learn from.
[00:45:31.000 --> 00:45:35.000]   It's actually seen a lot of success in robotics in the past.
[00:45:35.000 --> 00:45:38.000]   And typically in imitation learning,
[00:45:38.000 --> 00:45:40.000]   you get many demonstrations of something.
[00:45:40.000 --> 00:45:43.000]   And then from that you extract a policy
[00:45:43.000 --> 00:45:45.000]   that can hopefully do well at your task.
[00:45:45.000 --> 00:45:47.000]   Maybe first you wanna assemble a chair,
[00:45:47.000 --> 00:45:49.000]   give a lot of demonstrations, learn to assemble a chair.
[00:45:49.000 --> 00:45:50.000]   Then you wanna assemble a table,
[00:45:50.000 --> 00:45:53.000]   a lot of demonstrations, learn to assemble a table.
[00:45:53.000 --> 00:45:55.000]   But that's exactly what we don't like
[00:45:55.000 --> 00:45:57.000]   about supervised learning, same thing here.
[00:45:57.000 --> 00:45:58.000]   You need to give a lot of examples
[00:45:58.000 --> 00:45:59.000]   every time you want something new.
[00:45:59.000 --> 00:46:00.000]   Can we avoid this?
[00:46:00.000 --> 00:46:02.000]   Can we instead have a system like this?
[00:46:02.000 --> 00:46:05.000]   You see a bunch of demonstrations of different tasks.
[00:46:05.000 --> 00:46:07.000]   You learn the essence of what it means
[00:46:07.000 --> 00:46:09.000]   to be a demonstration of a task.
[00:46:09.000 --> 00:46:12.000]   And then in the future, you learn from one demonstration
[00:46:12.000 --> 00:46:14.000]   how to solve a new problem.
[00:46:15.000 --> 00:46:19.000]   And this could be demonstration of task F or G and so forth.
[00:46:19.000 --> 00:46:25.000]   All right, here's one way to formulate this.
[00:46:25.000 --> 00:46:29.000]   If we wanna be able to learn from one demonstration,
[00:46:29.000 --> 00:46:32.000]   we can formulate it as at training time,
[00:46:32.000 --> 00:46:35.000]   we will have two demos of each task.
[00:46:35.000 --> 00:46:38.000]   And the first demo is the one we're gonna consider
[00:46:38.000 --> 00:46:39.000]   that we'll have available in the future
[00:46:39.000 --> 00:46:41.000]   when we only have one demo.
[00:46:41.000 --> 00:46:43.000]   And so that's a whole video,
[00:46:43.000 --> 00:46:44.000]   let's say that we'll get to look at.
[00:46:44.000 --> 00:46:47.000]   And the second demo we'll use for training,
[00:46:47.000 --> 00:46:51.000]   we'll say, okay, in each frame in the second demo,
[00:46:51.000 --> 00:46:54.000]   can we predict what action we should take
[00:46:54.000 --> 00:46:59.000]   in that situation by indexing into the other demo?
[00:46:59.000 --> 00:47:00.000]   And then we have a loss here,
[00:47:00.000 --> 00:47:02.000]   whether we predict the correct action or not.
[00:47:02.000 --> 00:47:05.000]   So we can train that in a supervised way.
[00:47:05.000 --> 00:47:07.000]   And if you do really well at this,
[00:47:07.000 --> 00:47:08.000]   and then there's a new task,
[00:47:08.000 --> 00:47:10.000]   all we need to do is use that same neural network
[00:47:10.000 --> 00:47:11.000]   to index into the demo.
[00:47:11.000 --> 00:47:14.000]   And for the current situation, predict the action to take.
[00:47:14.000 --> 00:47:16.000]   I should test it is, this was at OpenAI at the time
[00:47:16.000 --> 00:47:18.000]   for block stacking.
[00:47:18.000 --> 00:47:20.000]   So here we have a wide range of tasks,
[00:47:20.000 --> 00:47:22.000]   a different task means a different combination of blocks
[00:47:22.000 --> 00:47:24.000]   that you're supposed to achieve.
[00:47:24.000 --> 00:47:26.000]   It's supposed to learn also that it doesn't matter
[00:47:26.000 --> 00:47:28.000]   what the coordinates are of the blocks,
[00:47:28.000 --> 00:47:30.000]   what ultimately only matters from the demonstration.
[00:47:30.000 --> 00:47:32.000]   When you see two demonstrations,
[00:47:32.000 --> 00:47:34.000]   you'll see that don't put blocks in the same place.
[00:47:34.000 --> 00:47:37.000]   But in both of them, let's say A will go on to B
[00:47:37.000 --> 00:47:39.000]   and C will go on to D.
[00:47:39.000 --> 00:47:40.000]   And it'll learn that that's the thing
[00:47:40.000 --> 00:47:42.000]   it has to pay attention to.
[00:47:42.000 --> 00:47:46.000]   And we see on the left here is a video,
[00:47:46.000 --> 00:47:48.000]   the one demonstration it gets.
[00:47:48.000 --> 00:47:50.000]   On the right is the execution based on that one demo.
[00:47:50.000 --> 00:47:52.000]   A is of course been trained on many pairs
[00:47:52.000 --> 00:47:54.000]   of demonstrations in the past.
[00:47:54.000 --> 00:47:57.000]   What we see here is that it's paying attention
[00:47:57.000 --> 00:47:59.000]   to different time slices in this video.
[00:47:59.000 --> 00:48:01.000]   We're watching it as a linear video,
[00:48:01.000 --> 00:48:02.000]   but the neural net gets to see the entire video
[00:48:02.000 --> 00:48:05.000]   at the same time, chooses what to pay attention to
[00:48:05.000 --> 00:48:07.000]   based on what it's seeing over there.
[00:48:07.000 --> 00:48:09.000]   And then over here you can see which blocks
[00:48:09.000 --> 00:48:11.000]   it's paying attention to based on what it's seeing
[00:48:11.000 --> 00:48:14.000]   in this video over here, what blocks this robot
[00:48:14.000 --> 00:48:16.000]   is paying attention to in the demonstration.
[00:48:16.000 --> 00:48:20.000]   And we see it actually successfully builds
[00:48:20.000 --> 00:48:24.000]   the same configuration as was demonstrated on the left,
[00:48:24.000 --> 00:48:26.000]   but the coordinates are completely different
[00:48:26.000 --> 00:48:28.000]   than what they were on the left.
[00:48:28.000 --> 00:48:31.000]   So this is one way to formulate it.
[00:48:31.000 --> 00:48:33.000]   Now I wanna share with you a different idea,
[00:48:33.000 --> 00:48:36.000]   which is how we can do this with MAML.
[00:48:36.000 --> 00:48:38.000]   So we're gonna solve the same problem.
[00:48:38.000 --> 00:48:39.000]   We wanna learn from one demonstration,
[00:48:39.000 --> 00:48:41.000]   but with a completely different approach.
[00:48:41.000 --> 00:48:44.000]   And I would say this is kind of somewhat typical
[00:48:44.000 --> 00:48:46.000]   of research in that there's often problems
[00:48:46.000 --> 00:48:48.000]   where people have completely different approaches
[00:48:48.000 --> 00:48:50.000]   and they both kind of work, and it's not clear yet,
[00:48:50.000 --> 00:48:53.000]   five years from now, what people are gonna use.
[00:48:53.000 --> 00:48:55.000]   Often probably a combination of both ideas,
[00:48:55.000 --> 00:48:57.000]   but who knows?
[00:48:57.000 --> 00:49:01.000]   So we saw MAML for supervised learning.
[00:49:01.000 --> 00:49:03.000]   Learning from demonstrations is like supervised learning.
[00:49:03.000 --> 00:49:05.000]   You have an input, an image you currently see,
[00:49:05.000 --> 00:49:07.000]   and then an action you take.
[00:49:07.000 --> 00:49:10.000]   And so that's a regression or classification problem.
[00:49:10.000 --> 00:49:11.000]   So you can do the same thing.
[00:49:11.000 --> 00:49:14.000]   You say I'm gonna train on a wide range of tasks
[00:49:14.000 --> 00:49:18.000]   where now my training task, if I have two demos,
[00:49:18.000 --> 00:49:20.000]   one demo is the training task,
[00:49:20.000 --> 00:49:22.000]   the other demo is the validation task.
[00:49:22.000 --> 00:49:23.000]   And I wanna be able to do one gradient update
[00:49:23.000 --> 00:49:26.000]   based on all the training demonstration data,
[00:49:26.000 --> 00:49:29.000]   that one demo, such that the resulting fine-tuned
[00:49:29.000 --> 00:49:31.000]   parameter vector performs well in predicting
[00:49:31.000 --> 00:49:33.000]   the actions in the second demo.
[00:49:33.000 --> 00:49:36.000]   If you can do that, then you adapt it quickly
[00:49:36.000 --> 00:49:38.000]   and you can expect this to do well.
[00:49:38.000 --> 00:49:41.000]   So here's object placing from pixels.
[00:49:41.000 --> 00:49:45.000]   So we have an input demo shown on the left.
[00:49:45.000 --> 00:49:47.000]   Somebody is teleoperating Brett
[00:49:47.000 --> 00:49:50.000]   to put the apple into the blue bowl.
[00:49:50.000 --> 00:49:55.000]   The robot at meta-training time did not have a blue bowl,
[00:49:55.000 --> 00:49:56.000]   did not have a red apple.
[00:49:56.000 --> 00:49:57.000]   This is the option on top,
[00:49:57.000 --> 00:50:00.000]   but these are now test objects, test targets.
[00:50:00.000 --> 00:50:02.000]   It sees that one demonstration,
[00:50:02.000 --> 00:50:05.000]   it does a gradient update based on data,
[00:50:05.000 --> 00:50:07.000]   that's image to action.
[00:50:07.000 --> 00:50:10.000]   And here it is now asked to do the same task
[00:50:10.000 --> 00:50:18.000]   from the one demonstration and it succeeds.
[00:50:18.000 --> 00:50:19.000]   You see it learned the concept
[00:50:19.000 --> 00:50:21.000]   that you need to pay attention to the bowl
[00:50:21.000 --> 00:50:22.000]   that the thing was dropped into,
[00:50:22.000 --> 00:50:24.000]   not the coordinates of where things were dropped.
[00:50:24.000 --> 00:50:27.000]   And it correctly recognizes that same object in the image.
[00:50:27.000 --> 00:50:29.000]   It's never seen that object before,
[00:50:29.000 --> 00:50:32.000]   but it finds it and steers the robot onto it
[00:50:32.000 --> 00:50:34.000]   to drop the apple.
[00:50:34.000 --> 00:50:36.000]   Now, question you could ask is,
[00:50:36.000 --> 00:50:37.000]   can we learn from just video?
[00:50:37.000 --> 00:50:41.000]   Usually, teleoperating a robot is pretty hard to do
[00:50:41.000 --> 00:50:42.000]   and it's very tedious.
[00:50:42.000 --> 00:50:44.000]   So what if we can just take a video
[00:50:44.000 --> 00:50:45.000]   of a person doing the job
[00:50:45.000 --> 00:50:49.000]   and just learn from that directly?
[00:50:49.000 --> 00:50:50.000]   And what I wanna illustrate here
[00:50:50.000 --> 00:50:52.000]   is a kind of very clever idea
[00:50:52.000 --> 00:50:53.000]   that is much more general
[00:50:53.000 --> 00:50:57.000]   than it is specific to learning from demonstrations.
[00:50:57.000 --> 00:50:58.000]   If you look at this notion,
[00:50:58.000 --> 00:51:01.000]   we have a validation loss and a training loss.
[00:51:01.000 --> 00:51:06.000]   There's no reason they need to be the same loss.
[00:51:06.000 --> 00:51:08.000]   Let me make this more concrete.
[00:51:08.000 --> 00:51:10.000]   When we're testing at test time,
[00:51:10.000 --> 00:51:13.000]   we want to just have a human,
[00:51:13.000 --> 00:51:16.000]   having a video of a human doing a task.
[00:51:16.000 --> 00:51:18.000]   So the gradient update on that train,
[00:51:18.000 --> 00:51:21.000]   we want that to be on a video,
[00:51:21.000 --> 00:51:24.000]   not on a teleop demo.
[00:51:24.000 --> 00:51:26.000]   But when we're training the robot ahead of time,
[00:51:26.000 --> 00:51:28.000]   maybe we're still happy to teleop
[00:51:28.000 --> 00:51:30.000]   and the validation loss can be based on teleop.
[00:51:30.000 --> 00:51:32.000]   And so that's what we're gonna set up here.
[00:51:32.000 --> 00:51:33.000]   Validation loss based on teleop,
[00:51:33.000 --> 00:51:36.000]   this ensures that we grounded into the overall loss
[00:51:36.000 --> 00:51:39.000]   being about, do you take the right actions?
[00:51:39.000 --> 00:51:41.000]   But then the training loss on the inside
[00:51:41.000 --> 00:51:45.000]   is based on predicting the next video frame.
[00:51:45.000 --> 00:51:46.000]   So what does this look like?
[00:51:46.000 --> 00:51:51.000]   Pictorially, we'll have a neural network with two heads.
[00:51:51.000 --> 00:51:56.000]   One head is predicting the action.
[00:51:56.000 --> 00:51:58.000]   The other head is predicting the next frame
[00:51:58.000 --> 00:52:00.000]   you'll see in the video.
[00:52:00.000 --> 00:52:05.000]   And the hope is that if you get one demo of just a video,
[00:52:05.000 --> 00:52:08.000]   if you do training along this path
[00:52:08.000 --> 00:52:10.000]   to predict the next frame better
[00:52:10.000 --> 00:52:12.000]   from that one demonstration,
[00:52:12.000 --> 00:52:15.000]   then the backpropagation happening here
[00:52:15.000 --> 00:52:18.000]   and resulting also updates happening here
[00:52:18.000 --> 00:52:20.000]   will be such that if then you forward propagate this way,
[00:52:20.000 --> 00:52:25.000]   you'll get the right actions on the second demo
[00:52:25.000 --> 00:52:27.000]   that was a teleop demo.
[00:52:27.000 --> 00:52:28.000]   This is a very general idea
[00:52:28.000 --> 00:52:30.000]   that you can essentially end to end train
[00:52:30.000 --> 00:52:32.000]   where you have one type of data
[00:52:32.000 --> 00:52:34.000]   that's easier to get maybe in the future.
[00:52:34.000 --> 00:52:36.000]   Another type of data that's harder to get,
[00:52:36.000 --> 00:52:37.000]   you can get some of it
[00:52:37.000 --> 00:52:39.000]   when you're initially training your system.
[00:52:39.000 --> 00:52:41.000]   You can set up a network with two heads
[00:52:41.000 --> 00:52:43.000]   and you can actually train it specifically
[00:52:43.000 --> 00:52:46.000]   that when you collect that easier to collect type of data,
[00:52:46.000 --> 00:52:49.000]   that it should result in also becoming good
[00:52:49.000 --> 00:52:51.000]   on this other output.
[00:52:51.000 --> 00:52:54.000]   And so it's different from just having an auxiliary loss.
[00:52:54.000 --> 00:52:55.000]   An auxiliary loss,
[00:52:55.000 --> 00:52:57.000]   you just kind of train two losses at the same time.
[00:52:57.000 --> 00:52:59.000]   But this, the objective is set up
[00:52:59.000 --> 00:53:01.000]   that training along this path
[00:53:01.000 --> 00:53:03.000]   is supposed to help that other path.
[00:53:03.000 --> 00:53:04.000]   - Specifically. - Specifically, yes.
[00:53:04.000 --> 00:53:07.000]   - So how's this different from say transfer learning?
[00:53:07.000 --> 00:53:10.000]   - So this is a type of transfer.
[00:53:10.000 --> 00:53:12.000]   So transfer is about when you learn on one problem,
[00:53:12.000 --> 00:53:14.000]   how do you learn then quickly on another problem?
[00:53:14.000 --> 00:53:17.000]   This is essentially end to end transfer learning
[00:53:17.000 --> 00:53:18.000]   where instead of just saying,
[00:53:18.000 --> 00:53:20.000]   I wanna, I'm training on one thing
[00:53:20.000 --> 00:53:21.000]   and I hope it'll transfer well
[00:53:21.000 --> 00:53:24.000]   by choosing a clever architecture or something like that.
[00:53:24.000 --> 00:53:25.000]   Here it's saying,
[00:53:25.000 --> 00:53:27.000]   well, I still need to choose a clever architecture.
[00:53:27.000 --> 00:53:29.000]   It's also setting up an objective
[00:53:29.000 --> 00:53:31.000]   that says it has to transfer.
[00:53:31.000 --> 00:53:34.000]   So at training time, you're forcing it to transfer
[00:53:34.000 --> 00:53:37.000]   where you learn along the bottom path
[00:53:37.000 --> 00:53:40.000]   should transfer to doing well on the top path.
[00:53:40.000 --> 00:53:41.000]   And you're exactly right.
[00:53:41.000 --> 00:53:42.000]   This is in the context of imitation
[00:53:42.000 --> 00:53:44.000]   and we can imagine doing this in very other contexts.
[00:53:44.000 --> 00:53:45.000]   Like I don't know what it would be,
[00:53:45.000 --> 00:53:49.000]   but maybe it would be something along the lines of,
[00:53:49.000 --> 00:53:50.000]   I don't know, making this up on the spot,
[00:53:50.000 --> 00:53:52.000]   but maybe you like to annotate animals
[00:53:52.000 --> 00:53:55.000]   where their head is and their legs and so forth.
[00:53:55.000 --> 00:53:59.000]   But maybe at annotation time later,
[00:53:59.000 --> 00:54:01.000]   all you wanna do is annotate the head.
[00:54:01.000 --> 00:54:03.000]   You don't wanna worry about anything else,
[00:54:03.000 --> 00:54:04.000]   but you want it to be the case
[00:54:04.000 --> 00:54:07.000]   that if you get one example of annotated head on an animal
[00:54:07.000 --> 00:54:10.000]   that the back propagation leads to on the other path here
[00:54:10.000 --> 00:54:12.000]   that it also automatically starts finding the legs
[00:54:12.000 --> 00:54:14.000]   and the tail because it's seen for this animal,
[00:54:14.000 --> 00:54:15.000]   this is where the head is.
[00:54:15.000 --> 00:54:16.000]   So then usually the body is like this
[00:54:16.000 --> 00:54:19.000]   and maybe I can also predict where head and tail are
[00:54:19.000 --> 00:54:21.000]   along the other path,
[00:54:21.000 --> 00:54:23.000]   where body and tail are on the other path.
[00:54:23.000 --> 00:54:27.000]   Little extra thing we need to do actually,
[00:54:27.000 --> 00:54:29.000]   if you try to do it this way,
[00:54:29.000 --> 00:54:32.000]   predicting pixel values is actually not a great loss.
[00:54:32.000 --> 00:54:35.000]   Because imagine your robot is trying to do something
[00:54:35.000 --> 00:54:38.000]   and it's paying attention to this video of a person
[00:54:38.000 --> 00:54:40.000]   and the person is doing something here with their hands
[00:54:40.000 --> 00:54:41.000]   and so much else happening
[00:54:41.000 --> 00:54:43.000]   and predicting all the details of the pixels
[00:54:43.000 --> 00:54:45.000]   makes you pay attention to a lot of things in the image
[00:54:45.000 --> 00:54:47.000]   that don't matter for the task.
[00:54:47.000 --> 00:54:50.000]   And so what you can do instead of trying to predict pixels,
[00:54:50.000 --> 00:54:52.000]   you can say we should have a different loss function.
[00:54:52.000 --> 00:54:54.000]   It should pay attention to things that matter,
[00:54:54.000 --> 00:54:56.000]   the placement of the objects and so forth.
[00:54:56.000 --> 00:54:57.000]   Maybe you can design it by hand,
[00:54:57.000 --> 00:54:58.000]   but you actually didn't wanna do that.
[00:54:58.000 --> 00:55:00.000]   We actually learned that loss function.
[00:55:00.000 --> 00:55:03.000]   So we specifically said we're gonna learn a loss function
[00:55:03.000 --> 00:55:06.000]   that is such that if you use the learn loss function
[00:55:06.000 --> 00:55:08.000]   and then do a gradient update
[00:55:08.000 --> 00:55:09.000]   against the learn loss function,
[00:55:09.000 --> 00:55:11.000]   then it does well on the other path.
[00:55:11.000 --> 00:55:14.000]   This is a lot like generative adversarial networks
[00:55:14.000 --> 00:55:16.000]   where you have a generator and a discriminator.
[00:55:16.000 --> 00:55:19.000]   You can think of this as a discriminator here,
[00:55:19.000 --> 00:55:21.000]   whether in some sense the image generated here,
[00:55:21.000 --> 00:55:24.000]   the discriminator is happy with it or not.
[00:55:24.000 --> 00:55:27.000]   And then you back propagate through that
[00:55:27.000 --> 00:55:29.000]   to get good performance on the top.
[00:55:29.000 --> 00:55:31.000]   Here's what happens.
[00:55:31.000 --> 00:55:34.000]   Now the robot just watches the human do it.
[00:55:34.000 --> 00:55:36.000]   And then after that,
[00:55:36.000 --> 00:55:38.000]   human puts things in a different place
[00:55:38.000 --> 00:55:41.000]   and it actually does the right thing.
[00:55:41.000 --> 00:55:50.000]   - What is the ratio of the amount of data
[00:55:50.000 --> 00:55:54.000]   that you have to do for teleop training
[00:55:54.000 --> 00:55:56.000]   versus human video training?
[00:55:56.000 --> 00:55:58.000]   - So during teleop training,
[00:55:58.000 --> 00:55:59.000]   the meta training,
[00:55:59.000 --> 00:56:04.000]   we take one teleop demonstration and one non-teleop.
[00:56:04.000 --> 00:56:05.000]   So one video.
[00:56:05.000 --> 00:56:07.000]   But you could probably mix that up.
[00:56:07.000 --> 00:56:09.000]   You could probably say, okay,
[00:56:09.000 --> 00:56:13.000]   instead of having one video for one teleop,
[00:56:13.000 --> 00:56:16.000]   you could probably record 100 videos for one teleop
[00:56:16.000 --> 00:56:17.000]   'cause you can say, well,
[00:56:17.000 --> 00:56:19.000]   here's 100 ways of doing that same task.
[00:56:19.000 --> 00:56:20.000]   And that might give you more signal,
[00:56:20.000 --> 00:56:22.000]   more invariance to learn from.
[00:56:22.000 --> 00:56:24.000]   It might be a more efficient data collection scheme.
[00:56:24.000 --> 00:56:26.000]   So in practice, that'd probably be pretty clever to do.
[00:56:26.000 --> 00:56:29.000]   I don't think we actually did that in this paper.
[00:56:29.000 --> 00:56:34.000]   So next idea I wanna talk about
[00:56:34.000 --> 00:56:39.000]   is actually Josh's PhD thesis work in some sense,
[00:56:39.000 --> 00:56:40.000]   domain randomization.
[00:56:40.000 --> 00:56:42.000]   And I think this is,
[00:56:42.000 --> 00:56:45.000]   so actually two weeks ago,
[00:56:45.000 --> 00:56:48.000]   there was a session in just north of San Francisco
[00:56:48.000 --> 00:56:51.000]   and this was organized by Fenod Kozlov
[00:56:51.000 --> 00:56:52.000]   from Kozlov Ventures
[00:56:52.000 --> 00:56:55.000]   and he brought in Bill Gates and a couple of people,
[00:56:55.000 --> 00:56:57.000]   like six AI experts,
[00:56:57.000 --> 00:57:00.000]   and went around the table and the question was,
[00:57:00.000 --> 00:57:03.000]   what do you think is one of the most surprising results
[00:57:03.000 --> 00:57:06.000]   that has happened in the last year or two
[00:57:06.000 --> 00:57:07.000]   or something like that?
[00:57:07.000 --> 00:57:10.000]   And this is the one that I chose to highlight
[00:57:10.000 --> 00:57:11.000]   in that discussion
[00:57:11.000 --> 00:57:13.000]   'cause I think it's really surprising
[00:57:13.000 --> 00:57:15.000]   how well this has worked.
[00:57:15.000 --> 00:57:19.000]   So why would you wanna work in simulation?
[00:57:19.000 --> 00:57:20.000]   Actually, there's so many reasons
[00:57:20.000 --> 00:57:21.000]   you'd wanna work in simulation.
[00:57:21.000 --> 00:57:23.000]   Everybody loves working in simulation.
[00:57:23.000 --> 00:57:26.000]   It's less expensive, faster, more scalable,
[00:57:26.000 --> 00:57:28.000]   less dangerous, easier to label,
[00:57:28.000 --> 00:57:30.000]   but often what you learn in simulation
[00:57:30.000 --> 00:57:31.000]   does not work in the real world
[00:57:31.000 --> 00:57:32.000]   and that's kind of the catch.
[00:57:32.000 --> 00:57:34.000]   So you get cool demos in simulation
[00:57:34.000 --> 00:57:36.000]   but no real working system.
[00:57:36.000 --> 00:57:37.000]   You might say, well,
[00:57:37.000 --> 00:57:39.000]   why not just build a realistic simulator?
[00:57:39.000 --> 00:57:41.000]   That's actually really hard to do.
[00:57:41.000 --> 00:57:42.000]   Of course, in self-driving space,
[00:57:42.000 --> 00:57:45.000]   people are trying, it's a massive market and so forth,
[00:57:45.000 --> 00:57:46.000]   but even then,
[00:57:46.000 --> 00:57:48.000]   there's still no realistic driving simulator out there
[00:57:48.000 --> 00:57:51.000]   despite people working on it for such a long time.
[00:57:51.000 --> 00:57:53.000]   Often also, the more realistic you make your simulator,
[00:57:53.000 --> 00:57:55.000]   the more you're gonna start paying the same price
[00:57:55.000 --> 00:57:57.000]   as you're working in the real world.
[00:57:57.000 --> 00:57:59.000]   You might all of a sudden have very expensive compute
[00:58:00.000 --> 00:58:03.000]   that you need to pay for rather than a cheap simulator
[00:58:03.000 --> 00:58:06.000]   that is much cheaper than real world experiments.
[00:58:06.000 --> 00:58:08.000]   Another idea that's seen actually
[00:58:08.000 --> 00:58:09.000]   a good amount of traction
[00:58:09.000 --> 00:58:10.000]   and is very interesting is this notion
[00:58:10.000 --> 00:58:13.000]   of domain confusion or domain adaptation.
[00:58:13.000 --> 00:58:14.000]   And this idea,
[00:58:14.000 --> 00:58:16.000]   the idea is that you have your neural network
[00:58:16.000 --> 00:58:19.000]   and you feed in simulated image and real image.
[00:58:19.000 --> 00:58:21.000]   You feed it into the same network
[00:58:21.000 --> 00:58:25.000]   and after some processing at some point in the network,
[00:58:25.000 --> 00:58:28.000]   you have a hidden layer and from that hidden layer,
[00:58:28.000 --> 00:58:30.000]   you will use a classifier
[00:58:30.000 --> 00:58:31.000]   and the classifier is supposed to say
[00:58:31.000 --> 00:58:35.000]   which one of these two is simulated versus real.
[00:58:35.000 --> 00:58:37.000]   And if that classifier is not able to succeed,
[00:58:37.000 --> 00:58:39.000]   then everything that comes after
[00:58:39.000 --> 00:58:40.000]   will generalize between sim and real
[00:58:40.000 --> 00:58:43.000]   'cause no information has been left in there
[00:58:43.000 --> 00:58:44.000]   about sim versus real.
[00:58:44.000 --> 00:58:47.000]   It's actually a very interesting approach
[00:58:47.000 --> 00:58:49.000]   and I think we'll see much more of that in the future,
[00:58:49.000 --> 00:58:51.000]   but one of the challenges so far
[00:58:51.000 --> 00:58:54.000]   has been that that is a training that's very competitive
[00:58:54.000 --> 00:58:57.000]   'cause that classifier is trying to erase information
[00:58:57.000 --> 00:58:58.000]   whereas the other classifier
[00:58:58.000 --> 00:59:00.000]   is trying to extract information.
[00:59:00.000 --> 00:59:03.000]   You get this battle of keeping information, erasing it.
[00:59:03.000 --> 00:59:06.000]   Here's the domain randomization idea.
[00:59:06.000 --> 00:59:07.000]   It's a very simple idea
[00:59:07.000 --> 00:59:10.000]   and you probably would not expect it would work
[00:59:10.000 --> 00:59:12.000]   until you actually see that it works.
[00:59:12.000 --> 00:59:15.000]   Here are simulated images
[00:59:15.000 --> 00:59:18.000]   and then here is a real world image
[00:59:18.000 --> 00:59:19.000]   and the task we looked at
[00:59:19.000 --> 00:59:24.000]   and we is really Josh, 99.9% of the work
[00:59:24.000 --> 00:59:28.000]   is can we detect where, let's say,
[00:59:28.000 --> 00:59:31.000]   the hexagonal block is in the image?
[00:59:31.000 --> 00:59:33.000]   And then maybe a robot can go pick it up.
[00:59:33.000 --> 00:59:37.000]   Okay, then we'll have simulated images.
[00:59:37.000 --> 00:59:39.000]   We'll train on the simulated images,
[00:59:39.000 --> 00:59:41.000]   but we'll introduce so much variation,
[00:59:41.000 --> 00:59:43.000]   so many versions of simulation
[00:59:43.000 --> 00:59:45.000]   that the hope is that when it sees
[00:59:45.000 --> 00:59:46.000]   then yet another simulator,
[00:59:46.000 --> 00:59:47.000]   it also does well 'cause it's seen
[00:59:47.000 --> 00:59:49.000]   so many variations already
[00:59:49.000 --> 00:59:50.000]   and then when that yet another simulator
[00:59:50.000 --> 00:59:52.000]   happens to be real world, it still works.
[00:59:53.000 --> 00:59:55.000]   It's a little crazy 'cause none of these simulators
[00:59:55.000 --> 00:59:57.000]   look like the real world,
[00:59:57.000 --> 00:59:59.000]   but the hope is that if you randomize enough,
[00:59:59.000 --> 01:00:04.000]   lighting, occlusions, viewpoints, textures,
[01:00:04.000 --> 01:00:06.000]   color schemes and so forth,
[01:00:06.000 --> 01:00:08.000]   if you randomize that enough
[01:00:08.000 --> 01:00:11.000]   that the only thing left for the network to learn
[01:00:11.000 --> 01:00:12.000]   is the thing that you care about,
[01:00:12.000 --> 01:00:14.000]   which is finding the hexagonal shape
[01:00:14.000 --> 01:00:16.000]   'cause there's no other signal left.
[01:00:16.000 --> 01:00:17.000]   You've randomized everything else.
[01:00:17.000 --> 01:00:19.000]   Only thing it can learn about is the hexagonal shape
[01:00:19.000 --> 01:00:21.000]   and then when it goes through a real world image,
[01:00:21.000 --> 01:00:25.000]   it'll see that hexagonal shape and find it.
[01:00:25.000 --> 01:00:29.000]   Some prior work that happened a little before we worked
[01:00:29.000 --> 01:00:32.000]   on this, Sergei Levin and Fredrik Sadeghi
[01:00:32.000 --> 01:00:34.000]   looked at this for quadcopter flight.
[01:00:34.000 --> 01:00:35.000]   What they did is very interesting.
[01:00:35.000 --> 01:00:37.000]   They pre-trained a network on ImageNet.
[01:00:37.000 --> 01:00:40.000]   Just for, so the network's pre-trained on ImageNet.
[01:00:40.000 --> 01:00:44.000]   Then they trained it in these kinds of simulations
[01:00:44.000 --> 01:00:46.000]   for a quadcopter to navigate down hallways,
[01:00:46.000 --> 01:00:48.000]   the most open space.
[01:00:49.000 --> 01:00:51.000]   And it actually then was tested in the real world
[01:00:51.000 --> 01:00:53.000]   and it was able to navigate hallways
[01:00:53.000 --> 01:00:55.000]   and buildings in Berkeley.
[01:00:55.000 --> 01:00:56.000]   And so we had a generalization there
[01:00:56.000 --> 01:00:57.000]   from this kind of simulation,
[01:00:57.000 --> 01:01:00.000]   it's more realistic than the ones we'd like to work with
[01:01:00.000 --> 01:01:02.000]   and still pre-trained on ImageNet
[01:01:02.000 --> 01:01:04.000]   so it has some real world data in it.
[01:01:04.000 --> 01:01:05.000]   It was very surprising.
[01:01:05.000 --> 01:01:09.000]   We wanted to see can you have much less realistic simulation
[01:01:09.000 --> 01:01:11.000]   because you can then simulate much faster
[01:01:11.000 --> 01:01:15.000]   and can you do it without pre-training on ImageNet?
[01:01:15.000 --> 01:01:18.000]   We see here is the results.
[01:01:18.000 --> 01:01:20.000]   Horizontal axis, number of training examples,
[01:01:20.000 --> 01:01:21.000]   logarithmic scale.
[01:01:21.000 --> 01:01:25.000]   Vertical axis is average error in the real world.
[01:01:25.000 --> 01:01:26.000]   So it's measured in the real world
[01:01:26.000 --> 01:01:28.000]   even though all the training is in simulation.
[01:01:28.000 --> 01:01:30.000]   We see that you can localize the block
[01:01:30.000 --> 01:01:32.000]   with 1.5 centimeter accuracy.
[01:01:32.000 --> 01:01:33.000]   For monocular vision,
[01:01:33.000 --> 01:01:35.000]   that is essentially the best you can hope for.
[01:01:35.000 --> 01:01:37.000]   If you look at robotics research in the past,
[01:01:37.000 --> 01:01:39.000]   when you do really, really good calibration
[01:01:39.000 --> 01:01:42.000]   of a single camera view onto a scene
[01:01:42.000 --> 01:01:43.000]   to localize an object,
[01:01:43.000 --> 01:01:45.000]   1.5 centimeters is really, really good.
[01:01:45.000 --> 01:01:47.000]   It's able to learn that just in simulation.
[01:01:48.000 --> 01:01:51.000]   Then we investigated, okay, what might matter the most?
[01:01:51.000 --> 01:01:53.000]   And the thing that mattered the most
[01:01:53.000 --> 01:01:54.000]   was the randomization of the textures.
[01:01:54.000 --> 01:01:57.000]   So randomizing the textures was really important.
[01:01:57.000 --> 01:02:01.000]   This is the number of unique textures used by the renderer
[01:02:01.000 --> 01:02:03.000]   and if you don't use many unique textures,
[01:02:03.000 --> 01:02:04.000]   only 10 unique textures,
[01:02:04.000 --> 01:02:07.000]   you actually get a pretty high error, like 15 centimeters.
[01:02:07.000 --> 01:02:10.000]   Once you have 10,000 different textures used,
[01:02:10.000 --> 01:02:13.000]   you get down to about 1.5 centimeter error.
[01:02:13.000 --> 01:02:16.000]   Do you need to pre-train on ImageNet?
[01:02:16.000 --> 01:02:17.000]   It helps.
[01:02:17.000 --> 01:02:19.000]   Blue Curve is with pre-training on ImageNet.
[01:02:19.000 --> 01:02:21.000]   You get there faster, but not that much faster.
[01:02:21.000 --> 01:02:23.000]   After about 8,000 examples,
[01:02:23.000 --> 01:02:25.000]   you do just as well without the pre-training.
[01:02:25.000 --> 01:02:27.000]   And again, this is measured on real world,
[01:02:27.000 --> 01:02:28.000]   training only in simulation.
[01:02:28.000 --> 01:02:31.000]   Then the next thing,
[01:02:31.000 --> 01:02:32.000]   it's kind of funny I'm presenting this
[01:02:32.000 --> 01:02:35.000]   'cause it's all your work, but (laughs)
[01:02:35.000 --> 01:02:37.000]   next thing we did,
[01:02:37.000 --> 01:02:40.000]   we meaning, again, Josh, really,
[01:02:40.000 --> 01:02:44.000]   is looking at can we make this then grasp objects?
[01:02:45.000 --> 01:02:46.000]   So how would we do that?
[01:02:46.000 --> 01:02:48.000]   Well, you train in simulation,
[01:02:48.000 --> 01:02:50.000]   but how many simulated objects are there?
[01:02:50.000 --> 01:02:52.000]   A few thousand in ShapeNet.
[01:02:52.000 --> 01:02:53.000]   If you just train on those,
[01:02:53.000 --> 01:02:55.000]   it's not gonna generalize well to the real world.
[01:02:55.000 --> 01:02:56.000]   It's not enough.
[01:02:56.000 --> 01:03:00.000]   So the clever idea then was to slice those objects,
[01:03:00.000 --> 01:03:02.000]   so any object could become three objects
[01:03:02.000 --> 01:03:06.000]   and then recombine these pieces in random ways.
[01:03:06.000 --> 01:03:07.000]   That way, you generate random objects
[01:03:07.000 --> 01:03:09.000]   that look nothing like real world objects,
[01:03:09.000 --> 01:03:12.000]   but locally, they are structured like real world objects.
[01:03:12.000 --> 01:03:14.000]   And it turns out if you train on that,
[01:03:14.000 --> 01:03:16.000]   you can generalize to real world.
[01:03:16.000 --> 01:03:20.000]   I'll skip the table thing here and I'll show the video.
[01:03:20.000 --> 01:03:22.000]   So what we're watching here
[01:03:22.000 --> 01:03:26.000]   is a robot that has not seen any real world data ever
[01:03:26.000 --> 01:03:29.000]   and has learned to pick up objects.
[01:03:29.000 --> 01:03:32.000]   And so here,
[01:03:32.000 --> 01:03:35.000]   seeing new object screwdriver,
[01:03:35.000 --> 01:03:36.000]   never seen that before,
[01:03:36.000 --> 01:03:37.000]   picks it up, no problem.
[01:03:37.000 --> 01:03:41.000]   Did this approach fully solve picking up objects?
[01:03:41.000 --> 01:03:42.000]   Well, it did not.
[01:03:42.000 --> 01:03:45.000]   I mean, the success rate with this approach is about 80%.
[01:03:45.000 --> 01:03:46.000]   That's a really good success rate
[01:03:46.000 --> 01:03:48.000]   compared to other approaches.
[01:03:48.000 --> 01:03:53.000]   And we never had access to real world data for this.
[01:03:53.000 --> 01:03:58.000]   - [Student] So is this RL with--
[01:03:58.000 --> 01:03:59.000]   - There's no RL in here.
[01:03:59.000 --> 01:04:03.000]   This is essentially grasp point prediction.
[01:04:03.000 --> 01:04:06.000]   So in simulation, it learns to predict
[01:04:06.000 --> 01:04:09.000]   where it should place the gripper on the object
[01:04:09.000 --> 01:04:12.000]   and then it uses geometric calculations to go there
[01:04:12.000 --> 01:04:14.000]   and then executes it.
[01:04:14.000 --> 01:04:17.000]   - [Student] What's, like, I guess,
[01:04:17.000 --> 01:04:20.000]   yeah, how do you know if you succeed?
[01:04:20.000 --> 01:04:22.000]   - So in simulation, you know you succeeded
[01:04:22.000 --> 01:04:23.000]   'cause you can simulate whether
[01:04:23.000 --> 01:04:24.000]   you pick up successfully or not.
[01:04:24.000 --> 01:04:26.000]   And here, we just need to run some tests
[01:04:26.000 --> 01:04:28.000]   to validate how well it works in the real world.
[01:04:28.000 --> 01:04:30.000]   We're not training anymore.
[01:04:30.000 --> 01:04:32.000]   There are things like the Google Arm farm
[01:04:32.000 --> 01:04:33.000]   where you actually,
[01:04:33.000 --> 01:04:35.000]   you just check after you pick something,
[01:04:35.000 --> 01:04:38.000]   is your gripper fully closed or not?
[01:04:38.000 --> 01:04:39.000]   Or does it look different
[01:04:39.000 --> 01:04:40.000]   from a gripper with nothing in it?
[01:04:40.000 --> 01:04:42.000]   And so that's a way to keep training in the real world,
[01:04:42.000 --> 01:04:44.000]   but that's not what we did here.
[01:04:44.000 --> 01:04:50.000]   And actually, you can take this idea
[01:04:50.000 --> 01:04:52.000]   of domain randomization a lot further.
[01:04:52.000 --> 01:04:53.000]   You can say I'm gonna,
[01:04:53.000 --> 01:04:55.000]   instead of what I showed to you so far
[01:04:55.000 --> 01:04:57.000]   was about randomizing the simulator
[01:04:57.000 --> 01:05:00.000]   so that you learn something that works in every simulator.
[01:05:00.000 --> 01:05:03.000]   But it turns out that sometimes
[01:05:03.000 --> 01:05:05.000]   the discrepancy between real world
[01:05:05.000 --> 01:05:07.000]   and your simulators is too big.
[01:05:07.000 --> 01:05:10.000]   And then if you wanna build a set of simulators
[01:05:10.000 --> 01:05:12.000]   that might be covering enough space
[01:05:12.000 --> 01:05:14.000]   so that it'll also work in the real world,
[01:05:14.000 --> 01:05:17.000]   they're so different that you can't find anything
[01:05:17.000 --> 01:05:19.000]   that works in all those simulators.
[01:05:19.000 --> 01:05:21.000]   You can actually change the way you do things.
[01:05:21.000 --> 01:05:23.000]   Instead of trying to learn this robust approach
[01:05:23.000 --> 01:05:24.000]   that works in all simulators,
[01:05:24.000 --> 01:05:26.000]   you can train a recurrent neural network
[01:05:26.000 --> 01:05:28.000]   that does few-shot reinforcement learning.
[01:05:28.000 --> 01:05:30.000]   So there will be reinforcement learning here.
[01:05:30.000 --> 01:05:32.000]   That's few-shot reinforcement learning
[01:05:32.000 --> 01:05:35.000]   and adapts to the new simulator on the spot.
[01:05:35.000 --> 01:05:38.000]   And so you get trained across many, many simulators,
[01:05:38.000 --> 01:05:41.000]   a neural network that is able to adapt very quickly
[01:05:41.000 --> 01:05:43.000]   to the new simulator it's currently in.
[01:05:43.000 --> 01:05:45.000]   You need to have many, many simulators
[01:05:45.000 --> 01:05:47.000]   and then you put it in the real world
[01:05:47.000 --> 01:05:50.000]   and let it run in the real world.
[01:05:50.000 --> 01:05:52.000]   And so this is a robotic hand
[01:05:52.000 --> 01:05:53.000]   controlled by a neural network
[01:05:53.000 --> 01:05:56.000]   that's been trained only in simulation
[01:05:56.000 --> 01:05:59.000]   but can control the real hand
[01:05:59.000 --> 01:06:01.000]   to get the block into the matching configuration
[01:06:01.000 --> 01:06:03.000]   shown in the bottom right.
[01:06:04.000 --> 01:06:08.000]   And so it's really amazing that,
[01:06:08.000 --> 01:06:10.000]   and I think that's really the very surprising aspect here
[01:06:10.000 --> 01:06:11.000]   about domain randomization,
[01:06:11.000 --> 01:06:15.000]   that it is possible to have relatively simple simulations,
[01:06:15.000 --> 01:06:18.000]   either visual or mechanical simulations,
[01:06:18.000 --> 01:06:20.000]   but if you randomize enough,
[01:06:20.000 --> 01:06:21.000]   and there's some cleverness
[01:06:21.000 --> 01:06:23.000]   that has to go into how you randomize.
[01:06:23.000 --> 01:06:24.000]   So you randomize enough
[01:06:24.000 --> 01:06:26.000]   over things that you should be randomizing over
[01:06:26.000 --> 01:06:29.000]   but retain the essence that you can learn something
[01:06:29.000 --> 01:06:31.000]   that can actually work in the real world.
[01:06:31.000 --> 01:06:32.000]   Yes?
[01:06:32.000 --> 01:06:35.000]   - [Student] So how do you define enough?
[01:06:35.000 --> 01:06:36.000]   - How do you define enough?
[01:06:36.000 --> 01:06:37.000]   Yeah, that's a hard question.
[01:06:37.000 --> 01:06:40.000]   For some things, I mean,
[01:06:40.000 --> 01:06:43.000]   so this project here, in hand manipulation,
[01:06:43.000 --> 01:06:47.000]   we had a project at OpenAI
[01:06:47.000 --> 01:06:48.000]   that essentially looked at the same method
[01:06:48.000 --> 01:06:51.000]   for pushing objects on a table.
[01:06:51.000 --> 01:06:52.000]   And for that project,
[01:06:52.000 --> 01:06:54.000]   it took three month internship
[01:06:54.000 --> 01:06:56.000]   of an exceptionally strong intern
[01:06:56.000 --> 01:06:58.000]   to get enough domain randomization in place
[01:06:58.000 --> 01:07:00.000]   in the simulator that the real robot
[01:07:00.000 --> 01:07:03.000]   could reliably push objects on the table around.
[01:07:03.000 --> 01:07:05.000]   But from that, it took another year
[01:07:05.000 --> 01:07:07.000]   before, I don't know,
[01:07:07.000 --> 01:07:09.000]   10 person OpenAI robotics team
[01:07:09.000 --> 01:07:12.000]   did enough to make it work on the hand,
[01:07:12.000 --> 01:07:13.000]   which is much, much more complex.
[01:07:13.000 --> 01:07:16.000]   And so there's a lot of thought that has to go into
[01:07:16.000 --> 01:07:18.000]   how do you randomize, what do you randomize?
[01:07:18.000 --> 01:07:20.000]   There's other aspects, of course, in robotics.
[01:07:20.000 --> 01:07:21.000]   You need to track the block.
[01:07:21.000 --> 01:07:24.000]   And so this is,
[01:07:24.000 --> 01:07:28.000]   assuming that you have full information of where,
[01:07:28.000 --> 01:07:29.000]   you might assume you have full information
[01:07:29.000 --> 01:07:30.000]   of where the block is, state information,
[01:07:30.000 --> 01:07:32.000]   that you need a system to track it.
[01:07:32.000 --> 01:07:34.000]   And so there's many aspects in this problem
[01:07:34.000 --> 01:07:36.000]   beyond just the manipulation aspect.
[01:07:36.000 --> 01:07:38.000]   But it can take a while
[01:07:38.000 --> 01:07:39.000]   before you figure out how to do it.
[01:07:39.000 --> 01:07:41.000]   One of the things I don't have in here,
[01:07:41.000 --> 01:07:42.000]   but we are doing now,
[01:07:42.000 --> 01:07:43.000]   and it's starting to work really well,
[01:07:43.000 --> 01:07:44.000]   is to also learn that.
[01:07:44.000 --> 01:07:45.000]   So we have a new approach
[01:07:45.000 --> 01:07:47.000]   to model-based reinforcement learning
[01:07:47.000 --> 01:07:50.000]   that effectively learns the domain randomization
[01:07:50.000 --> 01:07:51.000]   on the fly.
[01:07:51.000 --> 01:07:53.000]   And so the robot just collects data in the real world,
[01:07:53.000 --> 01:07:54.000]   learns instead of one simulator,
[01:07:54.000 --> 01:07:56.000]   learns a set of simulators,
[01:07:56.000 --> 01:07:59.000]   and then meta-learns against that set of simulators
[01:07:59.000 --> 01:08:01.000]   to learn something that will work well in the real world.
[01:08:01.000 --> 01:08:03.000]   And that can learn to stack Lego blocks now
[01:08:03.000 --> 01:08:06.000]   in 10 minutes of interaction with the world,
[01:08:06.000 --> 01:08:07.000]   rather than an hour,
[01:08:07.000 --> 01:08:09.000]   and probably even more quickly soon.
[01:08:09.000 --> 01:08:18.000]   So completely switching gears here,
[01:08:18.000 --> 01:08:19.000]   architecture search.
[01:08:19.000 --> 01:08:20.000]   I think if you've,
[01:08:20.000 --> 01:08:22.000]   I mean, you've all been training neural nets,
[01:08:22.000 --> 01:08:24.000]   so you know that one of the big choices
[01:08:24.000 --> 01:08:26.000]   you have to make is to choose
[01:08:26.000 --> 01:08:27.000]   the architecture of the neural net,
[01:08:27.000 --> 01:08:30.000]   and it has a very big impact on how well it works.
[01:08:30.000 --> 01:08:32.000]   Current practice tends to be,
[01:08:32.000 --> 01:08:34.000]   a machine learning solution consists of data
[01:08:34.000 --> 01:08:37.000]   plus compute plus machine learning expertise.
[01:08:37.000 --> 01:08:38.000]   Question you can ask,
[01:08:38.000 --> 01:08:40.000]   can we turn that into a solution is data
[01:08:40.000 --> 01:08:43.000]   plus, let's say, 100 times more compute.
[01:08:43.000 --> 01:08:44.000]   And 100 times more compute
[01:08:44.000 --> 01:08:46.000]   allows your computer to cycle through ideas,
[01:08:46.000 --> 01:08:48.000]   let's say, of architectures,
[01:08:48.000 --> 01:08:50.000]   rather than you doing it yourself.
[01:08:50.000 --> 01:08:51.000]   It might not be as efficient yet
[01:08:51.000 --> 01:08:53.000]   as humans can think through architectures,
[01:08:53.000 --> 01:08:56.000]   but that, so that's why maybe 100 times more compute
[01:08:56.000 --> 01:08:58.000]   is enough to get there.
[01:08:58.000 --> 01:08:59.000]   Google started looking at this,
[01:08:59.000 --> 01:09:01.000]   they have pretty much infinite compute.
[01:09:01.000 --> 01:09:03.000]   The starting observation was,
[01:09:03.000 --> 01:09:06.000]   if you look at the top one accuracy on ImageNet,
[01:09:06.000 --> 01:09:11.000]   AlexNet, then GoogleNet, ResNet, VGG,
[01:09:11.000 --> 01:09:14.000]   more ResNet, Inception, and so forth,
[01:09:14.000 --> 01:09:16.000]   and it became better and better and better,
[01:09:16.000 --> 01:09:18.000]   can we automate this process?
[01:09:18.000 --> 01:09:19.000]   If you look at it,
[01:09:19.000 --> 01:09:21.000]   what do these networks have?
[01:09:21.000 --> 01:09:24.000]   Well, they usually have some kind of convolutional layers,
[01:09:24.000 --> 01:09:25.000]   but you have to choose how many,
[01:09:25.000 --> 01:09:28.000]   you have to choose whether you want skip connections or not,
[01:09:28.000 --> 01:09:30.000]   you have to choose the filter sizes,
[01:09:30.000 --> 01:09:34.000]   how many filters, do you have batch norm or not batch norm,
[01:09:34.000 --> 01:09:35.000]   all those things.
[01:09:35.000 --> 01:09:36.000]   And so, but then the question is,
[01:09:36.000 --> 01:09:40.000]   maybe you can just have a recurrent neural network,
[01:09:40.000 --> 01:09:42.000]   which people have used to generate text,
[01:09:42.000 --> 01:09:44.000]   generate the textual description
[01:09:44.000 --> 01:09:45.000]   of what the architecture is like.
[01:09:45.000 --> 01:09:49.000]   So you might say, okay, I'm gonna output filter width five,
[01:09:49.000 --> 01:09:51.000]   filter height three, number of filters 24,
[01:09:51.000 --> 01:09:53.000]   and then keep going and build up
[01:09:53.000 --> 01:09:55.000]   an entire neural net architecture that way.
[01:09:55.000 --> 01:10:00.000]   Your recurrent neural network outputs a architecture,
[01:10:00.000 --> 01:10:02.000]   then you build that architecture,
[01:10:02.000 --> 01:10:04.000]   you train your network on,
[01:10:04.000 --> 01:10:05.000]   let's say ImageNet or something,
[01:10:05.000 --> 01:10:07.000]   you see how well it does,
[01:10:07.000 --> 01:10:09.000]   and now you know how good that architecture is.
[01:10:09.000 --> 01:10:10.000]   Then you might say, well,
[01:10:10.000 --> 01:10:11.000]   that recurrent neural network
[01:10:11.000 --> 01:10:13.000]   that generated the architecture made a bad choice,
[01:10:13.000 --> 01:10:15.000]   let's make a parameter update to it,
[01:10:15.000 --> 01:10:16.000]   and hopefully it'll generate
[01:10:16.000 --> 01:10:17.000]   a better architecture choice next.
[01:10:17.000 --> 01:10:20.000]   That's actually a reinforcement learning problem.
[01:10:20.000 --> 01:10:21.000]   'Cause making decisions over time,
[01:10:21.000 --> 01:10:24.000]   and so to formulate that as a reinforcement learning problem,
[01:10:24.000 --> 01:10:26.000]   to train that recurrent neural network
[01:10:26.000 --> 01:10:29.000]   to output strings describing neural net architectures,
[01:10:29.000 --> 01:10:31.000]   then here's the controller RNN.
[01:10:31.000 --> 01:10:35.000]   So it might choose number of filters,
[01:10:35.000 --> 01:10:38.000]   filter height, filter width, stride height, stride width,
[01:10:38.000 --> 01:10:41.000]   number of filters for the next layer, and so forth.
[01:10:41.000 --> 01:10:43.000]   And at some point it says I've had enough,
[01:10:43.000 --> 01:10:45.000]   and this is the full architecture.
[01:10:45.000 --> 01:10:47.000]   You can train this with reinforce.
[01:10:47.000 --> 01:10:49.000]   I don't wanna get into a lot of specifics here,
[01:10:49.000 --> 01:10:50.000]   but the bottom line is that you can actually use
[01:10:50.000 --> 01:10:53.000]   standard reinforcement learning to train this.
[01:10:53.000 --> 01:10:55.000]   'Cause that'd be very expensive, yeah.
[01:10:55.000 --> 01:10:57.000]   - Wouldn't this be much more
[01:10:57.000 --> 01:10:59.000]   for when you do overtraining in some sense?
[01:10:59.000 --> 01:11:00.000]   - So that's a good question.
[01:11:00.000 --> 01:11:02.000]   I'll answer that in a moment.
[01:11:02.000 --> 01:11:05.000]   It's very compute intense,
[01:11:05.000 --> 01:11:08.000]   so Google does massive distributed training for this.
[01:11:08.000 --> 01:11:11.000]   And after they did it,
[01:11:11.000 --> 01:11:16.000]   they found an architecture that they trained on CIFAR,
[01:11:16.000 --> 01:11:18.000]   which is faster to train on,
[01:11:18.000 --> 01:11:21.000]   and then evaluated on ImageNet.
[01:11:21.000 --> 01:11:23.000]   So they trained purely on CIFAR
[01:11:23.000 --> 01:11:25.000]   and found that the architecture they found
[01:11:25.000 --> 01:11:27.000]   is reusable on ImageNet
[01:11:27.000 --> 01:11:30.000]   in a way that generalizes quite well.
[01:11:30.000 --> 01:11:33.000]   And so better than DenseNet, which was the prior best,
[01:11:33.000 --> 01:11:37.000]   and was a human designed architecture for ImageNet.
[01:11:37.000 --> 01:11:39.000]   So that was a very interesting kind of generalization.
[01:11:39.000 --> 01:11:42.000]   They did something similar for LSTMs,
[01:11:42.000 --> 01:11:45.000]   or Recurrent Neural Nets.
[01:11:45.000 --> 01:11:48.000]   Can you find a better LSTM cell than the human design one?
[01:11:48.000 --> 01:11:50.000]   That didn't really work out that well
[01:11:50.000 --> 01:11:51.000]   to find something that's better,
[01:11:51.000 --> 01:11:52.000]   and people are still using LSTMs.
[01:11:52.000 --> 01:11:55.000]   But the ImageNet one, people have started to use
[01:11:55.000 --> 01:11:57.000]   the kind of essentially the ResNet block
[01:11:57.000 --> 01:11:59.000]   that is found by architecture search
[01:11:59.000 --> 01:12:00.000]   is often used rather than
[01:12:00.000 --> 01:12:02.000]   the original human designed ResNet block.
[01:12:02.000 --> 01:12:06.000]   Here are some numbers on this
[01:12:06.000 --> 01:12:09.000]   showing that you can actually also do this
[01:12:09.000 --> 01:12:12.000]   if you wanna improve the amount of compute,
[01:12:12.000 --> 01:12:15.000]   meaning you wanna find a neural network architecture
[01:12:15.000 --> 01:12:18.000]   that does almost as well as the best one,
[01:12:18.000 --> 01:12:19.000]   but uses less compute,
[01:12:19.000 --> 01:12:22.000]   so it uses less power than maybe the best one out there.
[01:12:22.000 --> 01:12:27.000]   So the key idea here is that architectures are strings.
[01:12:27.000 --> 01:12:28.000]   You can have an RNN generate the string,
[01:12:28.000 --> 01:12:29.000]   and then evaluate,
[01:12:29.000 --> 01:12:32.000]   and you can use reinforcement learning to search over.
[01:12:32.000 --> 01:12:34.000]   You can also search over activation functions.
[01:12:35.000 --> 01:12:37.000]   That original result actually took
[01:12:37.000 --> 01:12:39.000]   a really large amount of compute.
[01:12:39.000 --> 01:12:40.000]   Since then, Google had a new result.
[01:12:40.000 --> 01:12:42.000]   What they did there is essentially they said,
[01:12:42.000 --> 01:12:44.000]   we're gonna have a master architecture
[01:12:44.000 --> 01:12:46.000]   that has everything in it.
[01:12:46.000 --> 01:12:48.000]   And then the recurrent neural net just selects out things
[01:12:48.000 --> 01:12:51.000]   that it doesn't use for the current training run,
[01:12:51.000 --> 01:12:54.000]   but it keeps the parameters for everything.
[01:12:54.000 --> 01:12:57.000]   And so when it then trains another network,
[01:12:57.000 --> 01:12:58.000]   you can have the parameters already,
[01:12:58.000 --> 01:12:59.000]   most of them ready to go.
[01:12:59.000 --> 01:13:02.000]   It just has a different set of channels is used
[01:13:02.000 --> 01:13:04.000]   than in the previous training run maybe.
[01:13:04.000 --> 01:13:06.000]   And that speeds it up drastically.
[01:13:06.000 --> 01:13:09.000]   Another thing you could say,
[01:13:09.000 --> 01:13:11.000]   well, all this, I mean, architecture is one thing.
[01:13:11.000 --> 01:13:12.000]   The other tricks you're usually busy with
[01:13:12.000 --> 01:13:14.000]   is data augmentation.
[01:13:14.000 --> 01:13:15.000]   Whether you're doing domain randomization
[01:13:15.000 --> 01:13:17.000]   or any kind of regular training,
[01:13:17.000 --> 01:13:18.000]   you wanna maybe shift your image,
[01:13:18.000 --> 01:13:20.000]   or turn it a little bit,
[01:13:20.000 --> 01:13:21.000]   or make it darker, brighter,
[01:13:21.000 --> 01:13:24.000]   squeeze things, and so forth, re-crop it.
[01:13:24.000 --> 01:13:27.000]   And so a lot of machine learning research is on the model,
[01:13:27.000 --> 01:13:29.000]   but the data processing really matters too.
[01:13:29.000 --> 01:13:31.000]   And so people enlarge the data sets.
[01:13:31.000 --> 01:13:32.000]   You have one CAD image,
[01:13:32.000 --> 01:13:35.000]   you make it into seven CAD images instead of one.
[01:13:35.000 --> 01:13:38.000]   Google, again, because again,
[01:13:38.000 --> 01:13:41.000]   it took massive amount of compute to do this,
[01:13:41.000 --> 01:13:46.000]   show that it's possible to learn data augmentation schemes
[01:13:46.000 --> 01:13:49.000]   that are more optimal than human design ones.
[01:13:49.000 --> 01:13:52.000]   And so we'd have essentially an image coming in,
[01:13:52.000 --> 01:13:54.000]   and then there would be a probability of applying,
[01:13:54.000 --> 01:13:55.000]   and we'd learn that,
[01:13:55.000 --> 01:13:57.000]   the probability, optimal probability of applying
[01:13:57.000 --> 01:13:59.000]   and equalize, or rotate, or solarize,
[01:13:59.000 --> 01:14:01.000]   and equalize, and so forth.
[01:14:01.000 --> 01:14:03.000]   And so from that, it would get,
[01:14:03.000 --> 01:14:05.000]   as it feeds into the network for training,
[01:14:05.000 --> 01:14:07.000]   data augmentation would happen on the fly
[01:14:07.000 --> 01:14:09.000]   based on the scheme it learned.
[01:14:09.000 --> 01:14:11.000]   It worked quite well
[01:14:11.000 --> 01:14:15.000]   compared to human design augmentation schemes.
[01:14:15.000 --> 01:14:17.000]   And so we have here, for example,
[01:14:17.000 --> 01:14:20.000]   wired ResNet on CIFAR-100,
[01:14:20.000 --> 01:14:21.000]   no data augmentation, 1880,
[01:14:21.000 --> 01:14:24.000]   standard data augmentation, 1841 error rate,
[01:14:24.000 --> 01:14:26.000]   auto-augment, 1709.
[01:14:26.000 --> 01:14:29.000]   It's actually better than what humans were doing before.
[01:14:30.000 --> 01:14:33.000]   It actually is getting very close to human error,
[01:14:33.000 --> 01:14:36.000]   human level error rates on some of these data sets,
[01:14:36.000 --> 01:14:38.000]   in this case, ImageNet.
[01:14:38.000 --> 01:14:41.000]   Here are some references for that.
[01:14:41.000 --> 01:14:44.000]   Now, you might say, I don't have Google Cloud Compute,
[01:14:44.000 --> 01:14:46.000]   or maybe you have some compute,
[01:14:46.000 --> 01:14:48.000]   but not the same scale that they have.
[01:14:48.000 --> 01:14:50.000]   I mean, recently was something like,
[01:14:50.000 --> 01:14:53.000]   one machine learning paper can sometimes take more
[01:14:53.000 --> 01:14:56.000]   CO2 emissions than flying a private jet
[01:14:56.000 --> 01:14:57.000]   across the world or something
[01:14:57.000 --> 01:14:59.000]   for one of those massive compute papers.
[01:14:59.000 --> 01:15:02.000]   So you might ask yourself some questions about,
[01:15:02.000 --> 01:15:04.000]   should we always run these experiments?
[01:15:04.000 --> 01:15:07.000]   But for this particular one,
[01:15:07.000 --> 01:15:09.000]   we just published a paper
[01:15:09.000 --> 01:15:11.000]   that's gonna be presented at ICML
[01:15:11.000 --> 01:15:15.000]   that does it with 1,000 times less compute
[01:15:15.000 --> 01:15:16.000]   and makes it completely practical
[01:15:16.000 --> 01:15:19.000]   with any computer that most people have access to.
[01:15:19.000 --> 01:15:21.000]   And so the idea here,
[01:15:21.000 --> 01:15:25.000]   so we're comparing here with Google's auto-augment,
[01:15:25.000 --> 01:15:28.000]   and what we did is called population-based augmentation.
[01:15:28.000 --> 01:15:30.000]   Actually, invented by Peter Chen,
[01:15:30.000 --> 01:15:32.000]   who two of you know really well.
[01:15:32.000 --> 01:15:34.000]   He came up with this idea.
[01:15:34.000 --> 01:15:37.000]   Population-based augmentation does about as well
[01:15:37.000 --> 01:15:40.000]   as auto-augment and has 1,000x less compute needed.
[01:15:40.000 --> 01:15:41.000]   What's different about it?
[01:15:41.000 --> 01:15:44.000]   How come we only need 1,000x less compute
[01:15:44.000 --> 01:15:45.000]   to do this augmentation?
[01:15:45.000 --> 01:15:48.000]   The traditional auto-augment tries to come up
[01:15:48.000 --> 01:15:51.000]   with a fixed policy
[01:15:51.000 --> 01:15:55.000]   that you can apply to your data coming through.
[01:15:55.000 --> 01:15:57.000]   And so you have a fixed policy,
[01:15:57.000 --> 01:15:58.000]   you see how well it works,
[01:15:58.000 --> 01:15:59.000]   you update your policy,
[01:15:59.000 --> 01:16:01.000]   see how well it works, and keep repeating.
[01:16:01.000 --> 01:16:04.000]   So that takes a really long time to find the right policy.
[01:16:04.000 --> 01:16:06.000]   In population-based training,
[01:16:06.000 --> 01:16:08.000]   what you have is you have multiple policies
[01:16:08.000 --> 01:16:11.000]   for augmentation at the same time.
[01:16:11.000 --> 01:16:12.000]   You can think of them as hyperparameters,
[01:16:12.000 --> 01:16:14.000]   like you have multiple hyperparameter runs
[01:16:14.000 --> 01:16:15.000]   happening in parallel.
[01:16:15.000 --> 01:16:19.000]   The hyperparameters happen to be augmentation schemes,
[01:16:19.000 --> 01:16:21.000]   but many runs in parallel.
[01:16:21.000 --> 01:16:22.000]   You look at your population
[01:16:22.000 --> 01:16:23.000]   after a certain amount of training,
[01:16:23.000 --> 01:16:26.000]   and you see which population is doing better on test data,
[01:16:27.000 --> 01:16:30.000]   you keep those around, you get rid of the others,
[01:16:30.000 --> 01:16:33.000]   you replicate the good ones, and repeat.
[01:16:33.000 --> 01:16:35.000]   And so it's like a hyperparameter search,
[01:16:35.000 --> 01:16:36.000]   population-based hyperparameter search.
[01:16:36.000 --> 01:16:39.000]   What we end up with is not just something at the end,
[01:16:39.000 --> 01:16:41.000]   we only do one pass through, actually,
[01:16:41.000 --> 01:16:45.000]   we do one run, we end up with just a scheme over time
[01:16:45.000 --> 01:16:47.000]   of what is the right data augmentation to use
[01:16:47.000 --> 01:16:49.000]   in the first run of training,
[01:16:49.000 --> 01:16:50.000]   next run, next run, next run.
[01:16:50.000 --> 01:16:52.000]   And so we have a time-varying scheme,
[01:16:52.000 --> 01:16:54.000]   which is more powerful than a fixed scheme,
[01:16:54.000 --> 01:16:56.000]   and we only need to do one run through everything
[01:16:56.000 --> 01:16:58.000]   instead of doing many, many runs,
[01:16:58.000 --> 01:17:01.000]   and that's why we get the 1000x savings in compute.
[01:17:01.000 --> 01:17:07.000]   Another direction that I wanna quickly highlight here
[01:17:07.000 --> 01:17:08.000]   is unsupervised learning.
[01:17:08.000 --> 01:17:11.000]   Unsupervised learning, kind of the main idea here
[01:17:11.000 --> 01:17:12.000]   is that supervised learning,
[01:17:12.000 --> 01:17:14.000]   oh, all this data annotation is expensive,
[01:17:14.000 --> 01:17:18.000]   it takes a lot of time, a lot of money
[01:17:18.000 --> 01:17:19.000]   to get things annotated,
[01:17:19.000 --> 01:17:20.000]   can we just use unlabeled data instead?
[01:17:20.000 --> 01:17:22.000]   'Cause like babies, they just look around in the world,
[01:17:22.000 --> 01:17:24.000]   they get no annotation, and then you give one annotation,
[01:17:24.000 --> 01:17:25.000]   they know what to do.
[01:17:25.000 --> 01:17:29.000]   So, okay, can we learn a network that embeds the data
[01:17:29.000 --> 01:17:31.000]   in some way is one of the main ideas,
[01:17:31.000 --> 01:17:33.000]   so you take your data, an image gets embedded,
[01:17:33.000 --> 01:17:35.000]   and that embedding is somehow better
[01:17:35.000 --> 01:17:38.000]   to then train something on than the original pixels.
[01:17:38.000 --> 01:17:40.000]   If that's the case, you can learn more quickly
[01:17:40.000 --> 01:17:42.000]   thanks to learning and embedding.
[01:17:42.000 --> 01:17:45.000]   Or sometimes you train a network,
[01:17:45.000 --> 01:17:46.000]   and it's not about the embedding,
[01:17:46.000 --> 01:17:49.000]   it's about the network itself that you use during training,
[01:17:49.000 --> 01:17:51.000]   already encoding a lot of features in it,
[01:17:51.000 --> 01:17:53.000]   and you'll be able to fine tune that network
[01:17:53.000 --> 01:17:55.000]   to do something with.
[01:17:55.000 --> 01:17:57.000]   So the main family of models here
[01:17:57.000 --> 01:17:58.000]   are variational autoencoders,
[01:17:58.000 --> 01:18:01.000]   they go from image to some code,
[01:18:01.000 --> 01:18:03.000]   so it's an embedding model.
[01:18:03.000 --> 01:18:04.000]   Generative adversarial networks,
[01:18:04.000 --> 01:18:07.000]   they can also go from image to code.
[01:18:07.000 --> 01:18:10.000]   Then there are exact likelihood models
[01:18:10.000 --> 01:18:12.000]   like autoregressive, pixel CNN and so forth,
[01:18:12.000 --> 01:18:16.000]   and flow models, which are another way
[01:18:16.000 --> 01:18:18.000]   to essentially generate data,
[01:18:18.000 --> 01:18:20.000]   and the hope is that the latent vector you start from
[01:18:20.000 --> 01:18:23.000]   is like good representation of the data,
[01:18:23.000 --> 01:18:25.000]   better than the pixels you end up with.
[01:18:25.000 --> 01:18:26.000]   And then there's puzzles,
[01:18:26.000 --> 01:18:28.000]   and actually these have worked the best recently,
[01:18:28.000 --> 01:18:31.000]   I'll highlight some results in a bit.
[01:18:31.000 --> 01:18:32.000]   So there are generative models,
[01:18:32.000 --> 01:18:37.000]   think about why, if you wanna understand an image,
[01:18:37.000 --> 01:18:38.000]   why just take the image and try to process it,
[01:18:38.000 --> 01:18:41.000]   why not just see if we can actually generate images?
[01:18:41.000 --> 01:18:42.000]   If we can generate them,
[01:18:42.000 --> 01:18:43.000]   that means we have a deeper understanding of them
[01:18:43.000 --> 01:18:45.000]   than if all we can do is process them
[01:18:45.000 --> 01:18:47.000]   and turn them into something else.
[01:18:47.000 --> 01:18:50.000]   So let's say we have a model,
[01:18:50.000 --> 01:18:53.000]   a network that we set up to go from a code to an image,
[01:18:53.000 --> 01:18:54.000]   very easy to set up,
[01:18:54.000 --> 01:18:56.000]   it's just like a model that becomes bigger, bigger, bigger
[01:18:56.000 --> 01:18:59.000]   in terms of what you have in your square
[01:18:59.000 --> 01:19:01.000]   that you're generating.
[01:19:01.000 --> 01:19:03.000]   This is the images you get,
[01:19:03.000 --> 01:19:04.000]   100 different random codes,
[01:19:04.000 --> 01:19:06.000]   grid of 10 by 10, random codes,
[01:19:06.000 --> 01:19:08.000]   different random codes use same network,
[01:19:08.000 --> 01:19:10.000]   100 different images, so that's good,
[01:19:10.000 --> 01:19:11.000]   it knows how to generate different images,
[01:19:11.000 --> 01:19:13.000]   but none of them are interesting.
[01:19:13.000 --> 01:19:15.000]   You start training it, here's what happens,
[01:19:15.000 --> 01:19:17.000]   the codes remain the same,
[01:19:17.000 --> 01:19:19.000]   it's one network, whenever you have a different code,
[01:19:19.000 --> 01:19:20.000]   you get a different image.
[01:19:20.000 --> 01:19:23.000]   This is when you train on CIFAR images,
[01:19:23.000 --> 01:19:26.000]   train on face images, this is what you get.
[01:19:26.000 --> 01:19:30.000]   You train on rope, you can get images like this,
[01:19:30.000 --> 01:19:31.000]   where you learn how to interpolate
[01:19:31.000 --> 01:19:35.000]   between different phases in a rope manipulation session.
[01:19:35.000 --> 01:19:38.000]   You train on faces, ooh, you train on faces.
[01:19:38.000 --> 01:19:43.000]   This is what you get, so you can learn to generate faces,
[01:19:43.000 --> 01:19:54.000]   new random code means new face,
[01:19:54.000 --> 01:19:58.000]   and in fact, you can actually play with the variables
[01:19:58.000 --> 01:20:00.000]   in the latent code and find variables
[01:20:00.000 --> 01:20:02.000]   that correspond to specific things they might care about,
[01:20:02.000 --> 01:20:04.000]   make somebody smile versus not smile,
[01:20:05.000 --> 01:20:07.000]   change their age, change their eyes,
[01:20:07.000 --> 01:20:09.000]   change their hair and so forth.
[01:20:09.000 --> 01:20:15.000]   Wait, we saw this one, and actually,
[01:20:15.000 --> 01:20:16.000]   there's a website that you might have seen,
[01:20:16.000 --> 01:20:19.000]   thispersondoesnotexist.com, and that website,
[01:20:19.000 --> 01:20:21.000]   whenever you refresh, you get a new person,
[01:20:21.000 --> 01:20:23.000]   but a person that doesn't exist,
[01:20:23.000 --> 01:20:24.000]   so this person does not exist,
[01:20:24.000 --> 01:20:26.000]   but this image was generated by a neural network
[01:20:26.000 --> 01:20:27.000]   and it looks like a real person.
[01:20:27.000 --> 01:20:29.000]   Here's another one, and if you go refresh again,
[01:20:29.000 --> 01:20:31.000]   you'll get yet another one.
[01:20:31.000 --> 01:20:33.000]   You don't have to go from a random code to an image,
[01:20:33.000 --> 01:20:35.000]   you can go from one image to another one.
[01:20:35.000 --> 01:20:38.000]   So this is a horse to zebra,
[01:20:38.000 --> 01:20:40.000]   and then here's one of my favorite results
[01:20:40.000 --> 01:20:42.000]   from the last year, is let's say you don't know
[01:20:42.000 --> 01:20:45.000]   how to dance, but you want a good dance video of yourself,
[01:20:45.000 --> 01:20:48.000]   well, if a neural network can turn a horse into a zebra,
[01:20:48.000 --> 01:20:51.000]   why should it not be able to turn a professional dancer
[01:20:51.000 --> 01:20:53.000]   into you dancing?
[01:20:53.000 --> 01:20:54.000]   That's exactly what Tinghui Zhao
[01:20:54.000 --> 01:20:57.000]   and Caroline Chen did at Berkeley.
[01:20:57.000 --> 01:20:59.000]   So you use the same idea,
[01:21:02.000 --> 01:21:05.000]   and turn a video of a professional dancer into themselves.
[01:21:05.000 --> 01:21:07.000]   So all they need to do is this simple motion,
[01:21:07.000 --> 01:21:08.000]   and that's enough
[01:21:08.000 --> 01:21:17.000]   to then turn a professional dance video into you dancing.
[01:21:17.000 --> 01:21:22.000]   It's pretty amazing that this is possible.
[01:21:22.000 --> 01:21:25.000]   Also has some downsides,
[01:21:25.000 --> 01:21:27.000]   you might not trust videos as much anymore
[01:21:27.000 --> 01:21:28.000]   as you used to,
[01:21:28.000 --> 01:21:31.000]   but it's pretty amazing if you want dance videos
[01:21:31.000 --> 01:21:32.000]   of yourself.
[01:21:32.000 --> 01:21:34.000]   It doesn't need to be this kind of dancing,
[01:21:34.000 --> 01:21:38.000]   maybe some of you are more into ballet dancing,
[01:21:38.000 --> 01:21:41.000]   if you want that, it's no problem either.
[01:21:41.000 --> 01:21:44.000]   Just get a video of a ballet dancer somewhere on YouTube,
[01:21:44.000 --> 01:21:47.000]   and you're ready to go.
[01:21:47.000 --> 01:21:50.000]   (audience laughing)
[01:21:50.000 --> 01:21:58.000]   I'm gonna skip over to Photoshop One.
[01:21:58.000 --> 01:22:00.000]   You can also use for text generation,
[01:22:00.000 --> 01:22:02.000]   and once you do text generation,
[01:22:02.000 --> 01:22:04.000]   you can also do text reading.
[01:22:04.000 --> 01:22:05.000]   So you can give it new text,
[01:22:05.000 --> 01:22:07.000]   and you can figure out whether it's positive or negative.
[01:22:07.000 --> 01:22:09.000]   So this one here,
[01:22:09.000 --> 01:22:15.000]   has been trained on a lot of text,
[01:22:15.000 --> 01:22:17.000]   and now reads it character by character,
[01:22:17.000 --> 01:22:19.000]   and realizes it's quite positive so far.
[01:22:19.000 --> 01:22:20.000]   It's apparently a great book,
[01:22:20.000 --> 01:22:23.000]   but then the screenplay was not so good,
[01:22:23.000 --> 01:22:24.000]   the movie was actually pretty bad,
[01:22:24.000 --> 01:22:28.000]   and it turns into red as it reads along in this paragraph.
[01:22:29.000 --> 01:22:30.000]   The thing I wanna show here is that,
[01:22:30.000 --> 01:22:34.000]   I think unsurprised learning is actually starting to arrive,
[01:22:34.000 --> 01:22:36.000]   and not just with the cool demos I just showed,
[01:22:36.000 --> 01:22:37.000]   I feel like the past few years,
[01:22:37.000 --> 01:22:40.000]   unsurprised learning has been a lot about cool demos,
[01:22:40.000 --> 01:22:43.000]   but the original motivation was to go from
[01:22:43.000 --> 01:22:45.000]   needing lots of labeled data,
[01:22:45.000 --> 01:22:47.000]   to leading a small amount of labeled data,
[01:22:47.000 --> 01:22:50.000]   and just a lot of unlabeled data will be enough.
[01:22:50.000 --> 01:22:53.000]   Here's a graph, this is a paper that came out,
[01:22:53.000 --> 01:22:55.000]   I believe five days ago.
[01:22:55.000 --> 01:22:58.000]   Aaron VandenOord is the main person behind it.
[01:22:59.000 --> 01:23:01.000]   This is contrast predictive coding.
[01:23:01.000 --> 01:23:04.000]   I'll show you an image of what that actually is in a moment.
[01:23:04.000 --> 01:23:05.000]   But let's look at this graph,
[01:23:05.000 --> 01:23:08.000]   number of labeled images per class.
[01:23:08.000 --> 01:23:12.000]   This is ImageNet top five accuracy.
[01:23:12.000 --> 01:23:17.000]   The best supervised ResNet, shown in red,
[01:23:17.000 --> 01:23:20.000]   and then semi-supervised, shown in blue.
[01:23:20.000 --> 01:23:21.000]   And what you see here is that,
[01:23:21.000 --> 01:23:23.000]   by having additional unlabeled data,
[01:23:23.000 --> 01:23:25.000]   you can very significantly outperform
[01:23:25.000 --> 01:23:27.000]   purely labeled data training.
[01:23:28.000 --> 01:23:30.000]   Of course, once you have infinite labeled data,
[01:23:30.000 --> 01:23:32.000]   it's not gonna be a difference anymore,
[01:23:32.000 --> 01:23:35.000]   but especially when you need a low data regime,
[01:23:35.000 --> 01:23:37.000]   it's a very big difference.
[01:23:37.000 --> 01:23:39.000]   Let me show you a comparison of the different methods
[01:23:39.000 --> 01:23:41.000]   that have been most successful at this.
[01:23:41.000 --> 01:23:46.000]   They're all kind of little kind of tricks in some sense.
[01:23:46.000 --> 01:23:49.000]   The first one, I'll skip the first two.
[01:23:49.000 --> 01:23:52.000]   Relative position is one where you train a neural network,
[01:23:52.000 --> 01:23:54.000]   you give it two patches from an image,
[01:23:54.000 --> 01:23:57.000]   and you ask it, where are these two patches relative to each other?
[01:23:57.000 --> 01:24:02.000]   You might say, oh, well, this one is maybe 20 pixels north of that one,
[01:24:02.000 --> 01:24:06.000]   and maybe 40 pixels to the right or something.
[01:24:06.000 --> 01:24:08.000]   You train neural network to do that well,
[01:24:08.000 --> 01:24:12.000]   and then after you train the network to do that,
[01:24:12.000 --> 01:24:14.000]   all you get to do is to train a linear classifier
[01:24:14.000 --> 01:24:16.000]   on the features from that network.
[01:24:16.000 --> 01:24:19.000]   And so the hope is that if you define a really good task,
[01:24:19.000 --> 01:24:22.000]   that what you learn in a task you don't need to annotate for
[01:24:22.000 --> 01:24:25.000]   is a way of turning an image into a set of features
[01:24:25.000 --> 01:24:27.000]   where a linear classifier is enough,
[01:24:27.000 --> 01:24:29.000]   which means it learned everything there is to learn
[01:24:29.000 --> 01:24:31.000]   pretty much about those images.
[01:24:31.000 --> 01:24:36.000]   And so linear classifier gets top one accuracy of 36% on ImageNet.
[01:24:36.000 --> 01:24:40.000]   Then colorization is where you take an image,
[01:24:40.000 --> 01:24:42.000]   you turn RGB into grayscale,
[01:24:42.000 --> 01:24:45.000]   and you train a network to turn it back into RGB.
[01:24:45.000 --> 01:24:48.000]   You then use that network's final layer
[01:24:48.000 --> 01:24:51.000]   and train only a linear classifier on top,
[01:24:51.000 --> 01:24:53.000]   on label data now,
[01:24:53.000 --> 01:24:56.000]   and the linear classifier is enough on that kind of features
[01:24:56.000 --> 01:24:59.000]   to get 39.6%.
[01:24:59.000 --> 01:25:03.000]   Then the original, there's a combo of these three previous,
[01:25:03.000 --> 01:25:07.000]   four previous ones, which achieved 48.7%.
[01:25:07.000 --> 01:25:09.000]   If you train fully supervised,
[01:25:09.000 --> 01:25:11.000]   which means you're not restricted to a linear classifier,
[01:25:11.000 --> 01:25:12.000]   get a nonlinear classifier,
[01:25:12.000 --> 01:25:16.000]   the best performing network out there is 78%, 78%.
[01:25:16.000 --> 01:25:21.000]   So people were at 48 two years ago.
[01:25:21.000 --> 01:25:26.000]   Then one year ago, CPC came in at 55.4.
[01:25:26.000 --> 01:25:29.000]   Oh no, rotation revenue came in at 55.4.
[01:25:29.000 --> 01:25:31.000]   Then the new CPC came out at 61.
[01:25:31.000 --> 01:25:36.000]   So it's getting closer very quickly
[01:25:36.000 --> 01:25:38.000]   in terms of what you can do with just a linear classifier
[01:25:38.000 --> 01:25:40.000]   on top of unsupervised learning,
[01:25:40.000 --> 01:25:42.000]   which is really amazing.
[01:25:42.000 --> 01:25:43.000]   What does CPC do?
[01:25:43.000 --> 01:25:44.000]   The one that works the best,
[01:25:44.000 --> 01:25:46.000]   essentially it takes your image
[01:25:46.000 --> 01:25:49.000]   and it turns it into a bunch of patches with overlap.
[01:25:49.000 --> 01:25:51.000]   And then it'll say,
[01:25:51.000 --> 01:25:54.000]   given let's say the top row of patches,
[01:25:54.000 --> 01:25:58.000]   can you predict a patch at the bottom right?
[01:25:58.000 --> 01:25:59.000]   But if you do it naively,
[01:25:59.000 --> 01:26:01.000]   you try to predict pixels, it doesn't work.
[01:26:01.000 --> 01:26:04.000]   The contrastive part,
[01:26:04.000 --> 01:26:05.000]   what it refers to is that
[01:26:05.000 --> 01:26:07.000]   instead of trying to predict pixels,
[01:26:07.000 --> 01:26:11.000]   what you're given is you're given a thousand patches
[01:26:11.000 --> 01:26:12.000]   and you're supposed to pick
[01:26:12.000 --> 01:26:14.000]   which one of those 1,000 patches
[01:26:14.000 --> 01:26:16.000]   is the one that actually sits in that spot.
[01:26:16.000 --> 01:26:18.000]   And if you train your network
[01:26:18.000 --> 01:26:20.000]   to be really good at correctly picking
[01:26:20.000 --> 01:26:22.000]   which one of the 1,000 is the correct one,
[01:26:22.000 --> 01:26:25.000]   then it turns out it learns a set of features
[01:26:25.000 --> 01:26:28.000]   that is almost as good as features,
[01:26:28.000 --> 01:26:29.000]   not there just yet,
[01:26:29.000 --> 01:26:31.000]   but getting closer and closer to features
[01:26:31.000 --> 01:26:33.000]   that you get from just pure supervised learning
[01:26:33.000 --> 01:26:34.000]   on ImageNet.
[01:26:34.000 --> 01:26:36.000]   And so that's really, really interesting.
[01:26:36.000 --> 01:26:38.000]   Contrastive loss, of course,
[01:26:38.000 --> 01:26:40.000]   has many things to choose.
[01:26:40.000 --> 01:26:42.000]   The other 999, where do you pick them?
[01:26:42.000 --> 01:26:43.000]   Do you pick them from the same image?
[01:26:43.000 --> 01:26:45.000]   Do you pick them from a different image?
[01:26:45.000 --> 01:26:47.000]   And so forth, there's computational aspects.
[01:26:47.000 --> 01:26:49.000]   You feed it as in batch through a GPU.
[01:26:49.000 --> 01:26:52.000]   So typically, you pick them from the same mini batch
[01:26:52.000 --> 01:26:53.000]   that you feed through,
[01:26:53.000 --> 01:26:55.000]   otherwise, the memory aspects are too complicated.
[01:26:55.000 --> 01:26:58.000]   But still, a lot of choices to be made,
[01:26:58.000 --> 01:26:59.000]   but the paper from last week
[01:26:59.000 --> 01:27:02.000]   shows that it gets extremely close.
[01:27:02.000 --> 01:27:03.000]   I mean, again, 61%,
[01:27:03.000 --> 01:27:08.000]   compared to high 30s it was just two years ago
[01:27:08.000 --> 01:27:11.000]   to now 61% on top one ImageNet accuracy
[01:27:11.000 --> 01:27:14.000]   with a linear classifier on top of these features.
[01:27:14.000 --> 01:27:17.000]   So instead of going through lifelong learning,
[01:27:17.000 --> 01:27:20.000]   I think we have a couple minutes left, right Josh?
[01:27:20.000 --> 01:27:21.000]   Instead of--
[01:27:21.000 --> 01:27:22.000]   - Yeah, five minutes.
[01:27:22.000 --> 01:27:23.000]   - Five minutes left.
[01:27:23.000 --> 01:27:24.000]   Instead of going through lifelong learning,
[01:27:24.000 --> 01:27:29.000]   what I'm gonna do is skip to,
[01:27:29.000 --> 01:27:34.000]   instead of covering another topic,
[01:27:35.000 --> 01:27:39.000]   I wanna cover
[01:27:39.000 --> 01:27:48.000]   a couple of other things here.
[01:27:48.000 --> 01:27:49.000]   So let's see.
[01:27:49.000 --> 01:27:54.000]   I think if we think about key enablers for AI progress,
[01:27:54.000 --> 01:27:56.000]   I think many people would agree,
[01:27:56.000 --> 01:27:58.000]   it's data, lots of data,
[01:27:58.000 --> 01:28:01.000]   lots of compute to process the data,
[01:28:01.000 --> 01:28:03.000]   and you need to think about how to do that.
[01:28:03.000 --> 01:28:05.000]   So some human ingenuity is needed.
[01:28:05.000 --> 01:28:08.000]   When you think about which problems to work on
[01:28:08.000 --> 01:28:10.000]   from a research point of view,
[01:28:10.000 --> 01:28:12.000]   you can often think about your problems
[01:28:12.000 --> 01:28:14.000]   as following one of these two categories.
[01:28:14.000 --> 01:28:15.000]   So one category is a category
[01:28:15.000 --> 01:28:17.000]   where there's a lot of ingenuity required
[01:28:17.000 --> 01:28:20.000]   and maybe smaller amount of data and compute,
[01:28:20.000 --> 01:28:22.000]   or a lot of data and compute
[01:28:22.000 --> 01:28:24.000]   and less ingenuity required maybe.
[01:28:24.000 --> 01:28:28.000]   And you might say, well, where should I be?
[01:28:28.000 --> 01:28:29.000]   If you think about which problem to work on next,
[01:28:29.000 --> 01:28:30.000]   and you might say, well,
[01:28:30.000 --> 01:28:32.000]   maybe it depends on how smart I am.
[01:28:32.000 --> 01:28:34.000]   I'm very smart, maybe I wanna be on the left
[01:28:34.000 --> 01:28:37.000]   'cause then it leverages my smartness more,
[01:28:37.000 --> 01:28:38.000]   and I'm less smart, I wanna be on the right
[01:28:38.000 --> 01:28:40.000]   because it's less needed.
[01:28:40.000 --> 01:28:42.000]   But I'm gonna argue that actually
[01:28:42.000 --> 01:28:44.000]   you always wanna be on the right.
[01:28:44.000 --> 01:28:46.000]   And the reason you always wanna be on the right
[01:28:46.000 --> 01:28:47.000]   is that's where the low-hanging fruit is.
[01:28:47.000 --> 01:28:49.000]   That's where the new opportunities are.
[01:28:49.000 --> 01:28:50.000]   If you're on the left,
[01:28:50.000 --> 01:28:53.000]   things that are dominated by just human ingenuity,
[01:28:53.000 --> 01:28:55.000]   essentially, it's like working in mathematics.
[01:28:55.000 --> 01:28:58.000]   And mathematics is a field that progresses, sure,
[01:28:58.000 --> 01:29:00.000]   but it requires like extremely rare,
[01:29:00.000 --> 01:29:03.000]   exceptional insights to get to the next step.
[01:29:03.000 --> 01:29:06.000]   And a lot of low-hanging fruit was actually figured out
[01:29:06.000 --> 01:29:08.000]   in the 1500s when people invented calculus and so forth.
[01:29:08.000 --> 01:29:10.000]   I mean, deep learning essentially relies
[01:29:10.000 --> 01:29:12.000]   on the math of mostly of the 1500s,
[01:29:12.000 --> 01:29:14.000]   much less so of the math that happened since.
[01:29:14.000 --> 01:29:17.000]   And so if you're over here, that's great,
[01:29:17.000 --> 01:29:18.000]   but it's actually really hard.
[01:29:18.000 --> 01:29:20.000]   You're competing with everybody from the past,
[01:29:20.000 --> 01:29:23.000]   including people from last year, but the year before,
[01:29:23.000 --> 01:29:25.000]   but all the way back to Newton and so forth
[01:29:25.000 --> 01:29:26.000]   to do something new.
[01:29:26.000 --> 01:29:27.000]   Whereas if you're on the right, you might say,
[01:29:27.000 --> 01:29:29.000]   well, last year, nobody had this data.
[01:29:29.000 --> 01:29:32.000]   Last year, nobody had to compute to process this data.
[01:29:32.000 --> 01:29:33.000]   So actually, I could try something
[01:29:33.000 --> 01:29:35.000]   nobody could have tried last year.
[01:29:35.000 --> 01:29:37.000]   So there might be some low-hanging fruit
[01:29:37.000 --> 01:29:38.000]   that I can find relatively quickly,
[01:29:38.000 --> 01:29:40.000]   and that will give a cool new result
[01:29:40.000 --> 01:29:41.000]   that wasn't possible before.
[01:29:41.000 --> 01:29:43.000]   It still requires ingenuity.
[01:29:43.000 --> 01:29:44.000]   I'm not saying it requires no ingenuity,
[01:29:44.000 --> 01:29:46.000]   but I think it's really good to think about,
[01:29:46.000 --> 01:29:48.000]   am I really working on something
[01:29:48.000 --> 01:29:51.000]   that people could not have worked on a year or two ago?
[01:29:51.000 --> 01:29:53.000]   'Cause the answer is yes to that.
[01:29:53.000 --> 01:29:57.000]   You have a much higher chance of doing something original
[01:29:57.000 --> 01:29:58.000]   than if you work on something where if you think about it,
[01:29:58.000 --> 01:30:01.000]   yeah, five years ago, people could have worked on it.
[01:30:01.000 --> 01:30:03.000]   Then you have a lot of competition already happening
[01:30:03.000 --> 01:30:05.000]   in the past that did not succeed,
[01:30:05.000 --> 01:30:07.000]   so maybe the problem you're trying to solve
[01:30:07.000 --> 01:30:08.000]   is even too hard to solve.
[01:30:08.000 --> 01:30:12.000]   So learning to learn is one example
[01:30:12.000 --> 01:30:13.000]   where the amount of compute required
[01:30:13.000 --> 01:30:16.000]   is pretty high compared to what people had in the past.
[01:30:16.000 --> 01:30:18.000]   That might make you ask the question,
[01:30:18.000 --> 01:30:20.000]   how much compute do, for example,
[01:30:20.000 --> 01:30:21.000]   PhD students at Berkeley use?
[01:30:21.000 --> 01:30:26.000]   I would say people often use about four to eight GPUs
[01:30:26.000 --> 01:30:31.000]   for local development, and maybe $2,000 to $5,000 a month
[01:30:31.000 --> 01:30:33.000]   in cloud compute per person.
[01:30:33.000 --> 01:30:37.000]   So it's not zero, but it's also not that crazy.
[01:30:37.000 --> 01:30:40.000]   If you look at the statistics of, okay,
[01:30:40.000 --> 01:30:42.000]   who published papers at NeurIPS, ICML, and so forth,
[01:30:42.000 --> 01:30:46.000]   if you look at the top publishing places,
[01:30:46.000 --> 01:30:48.000]   Google's always first 'cause they have, I mean,
[01:30:48.000 --> 01:30:50.000]   10 or 100 times more people doing machine learning research
[01:30:50.000 --> 01:30:52.000]   than anybody else, but then second
[01:30:52.000 --> 01:30:54.000]   is pretty much always Berkeley,
[01:30:54.000 --> 01:30:56.000]   and this is the amount of compute that's needed
[01:30:56.000 --> 01:30:58.000]   to get those papers out.
[01:30:58.000 --> 01:31:01.000]   I'm hoping to trend towards 10 to 20 GPUs per student
[01:31:01.000 --> 01:31:05.000]   and 10 to 20K of cloud compute per month.
[01:31:05.000 --> 01:31:09.000]   If anybody wants to donate, please talk to me.
[01:31:09.000 --> 01:31:11.000]   I think that's where it will have to be
[01:31:11.000 --> 01:31:14.000]   a couple years from now to be competitive,
[01:31:14.000 --> 01:31:17.000]   but I think we're good where we are now,
[01:31:17.000 --> 01:31:19.000]   but we need to trend towards that.
[01:31:19.000 --> 01:31:22.000]   And the last topic I wanna cover
[01:31:22.000 --> 01:31:24.000]   is how to keep up with all of this.
[01:31:24.000 --> 01:31:27.000]   The first thing you might ask is how to read a paper,
[01:31:27.000 --> 01:31:28.000]   and I don't wanna step through that,
[01:31:28.000 --> 01:31:29.000]   but I have some guidelines here
[01:31:29.000 --> 01:31:32.000]   that I can share the slides with Josh and Lucas
[01:31:32.000 --> 01:31:34.000]   so you can look at the slides later
[01:31:34.000 --> 01:31:36.000]   on how to read a paper, and it's definitely
[01:31:36.000 --> 01:31:38.000]   not reading linearly from start to finish
[01:31:38.000 --> 01:31:41.000]   'cause usually you waste a lot of time when you do that.
[01:31:41.000 --> 01:31:43.000]   It's really important that you quickly
[01:31:43.000 --> 01:31:45.000]   go to the back of the paper
[01:31:45.000 --> 01:31:47.000]   to see if there's actually any real results in there
[01:31:47.000 --> 01:31:49.000]   and that you have a good understanding of,
[01:31:49.000 --> 01:31:51.000]   essentially, you need to have a really good understanding
[01:31:51.000 --> 01:31:52.000]   of what's good.
[01:31:52.000 --> 01:31:54.000]   I showed you the table for CPC and so forth.
[01:31:54.000 --> 01:31:55.000]   You need to understand.
[01:31:55.000 --> 01:31:56.000]   Somebody comes up with a paper and says,
[01:31:56.000 --> 01:31:57.000]   I did semi-supervised learning,
[01:31:57.000 --> 01:32:00.000]   and I got major breakthrough, it's very cool,
[01:32:00.000 --> 01:32:02.000]   and with just linear classifier,
[01:32:02.000 --> 01:32:05.000]   I can get top 1% of 25%.
[01:32:05.000 --> 01:32:07.000]   It's great, and then you need to know, no,
[01:32:07.000 --> 01:32:09.000]   we're already at 61% today.
[01:32:09.000 --> 01:32:11.000]   25% is not great.
[01:32:11.000 --> 01:32:13.000]   This paper's probably not that relevant for me.
[01:32:13.000 --> 01:32:14.000]   It might be some good ideas,
[01:32:14.000 --> 01:32:16.000]   but definitely the results are not that strong,
[01:32:16.000 --> 01:32:20.000]   so the ideas did not lead to strong results yet.
[01:32:20.000 --> 01:32:22.000]   And so being able to skip forward
[01:32:22.000 --> 01:32:23.000]   and understanding whether the results
[01:32:23.000 --> 01:32:24.000]   are actually meaningful or not
[01:32:24.000 --> 01:32:27.000]   is probably the most important skill to acquire
[01:32:27.000 --> 01:32:30.000]   because then you can filter out many, many papers.
[01:32:30.000 --> 01:32:32.000]   But there's a lot more detail
[01:32:32.000 --> 01:32:35.000]   that you might wanna think about when reading papers
[01:32:35.000 --> 01:32:37.000]   and really quick, really rejecting papers
[01:32:37.000 --> 01:32:39.000]   that you should not be reading.
[01:32:39.000 --> 01:32:41.000]   What papers to read?
[01:32:41.000 --> 01:32:43.000]   Just read all the papers is one way.
[01:32:43.000 --> 01:32:44.000]   It's really hard.
[01:32:44.000 --> 01:32:46.000]   We talked about that at the beginning.
[01:32:46.000 --> 01:32:48.000]   It's not really practical.
[01:32:48.000 --> 01:32:51.000]   So how do you find which papers to read?
[01:32:51.000 --> 01:32:52.000]   Well, here are a few pointers.
[01:32:52.000 --> 01:32:55.000]   Import AI is a newsletter by Jack Clark from OpenAI,
[01:32:55.000 --> 01:32:58.000]   and it highlights a couple of papers each week.
[01:32:58.000 --> 01:32:59.000]   And it already summarizes them,
[01:32:59.000 --> 01:33:02.000]   and you know that from the summary,
[01:33:02.000 --> 01:33:03.000]   this is actually a relevant result.
[01:33:03.000 --> 01:33:05.000]   I might wanna read the details behind this.
[01:33:05.000 --> 01:33:07.000]   Something called Archive Sanity,
[01:33:07.000 --> 01:33:10.000]   which is a collaborative filtering system on paper reading.
[01:33:10.000 --> 01:33:11.000]   If you're part of that,
[01:33:11.000 --> 01:33:13.000]   it can keep track of what papers you like, don't like,
[01:33:13.000 --> 01:33:15.000]   what papers people like you,
[01:33:15.000 --> 01:33:17.000]   meaning in terms of other papers they've read
[01:33:17.000 --> 01:33:19.000]   and that you have also read,
[01:33:19.000 --> 01:33:20.000]   what you might wanna read
[01:33:20.000 --> 01:33:22.000]   based on what they recently liked.
[01:33:22.000 --> 01:33:24.000]   So it's a good way to see suggestions
[01:33:24.000 --> 01:33:26.000]   of what you should be reading maybe next.
[01:33:26.000 --> 01:33:28.000]   Twitter has a lot of suggestions of people saying,
[01:33:28.000 --> 01:33:31.000]   oh, this paper was great, you should check it out.
[01:33:31.000 --> 01:33:32.000]   And there are also some groups
[01:33:32.000 --> 01:33:36.000]   where there's discussions online on Facebook and on Reddit.
[01:33:36.000 --> 01:33:39.000]   The last thing I highly recommend
[01:33:39.000 --> 01:33:40.000]   is actually that you form a reading group.
[01:33:40.000 --> 01:33:42.000]   If you're gonna be reading papers,
[01:33:42.000 --> 01:33:44.000]   even if just one other person,
[01:33:44.000 --> 01:33:46.000]   you can cut in half the number of papers you need to read.
[01:33:46.000 --> 01:33:49.000]   And so if you're gonna start reading papers,
[01:33:49.000 --> 01:33:50.000]   I would very highly recommend you find
[01:33:50.000 --> 01:33:53.000]   at least one other person who wants to read them with you
[01:33:53.000 --> 01:33:56.000]   and you together just, you split the work
[01:33:56.000 --> 01:33:57.000]   and you just present it to each other
[01:33:57.000 --> 01:34:00.000]   and you can maybe get presented in five minutes
[01:34:00.000 --> 01:34:01.000]   what this paper is about.
[01:34:01.000 --> 01:34:03.000]   Moreover, they filtered out 100 other papers
[01:34:03.000 --> 01:34:05.000]   that they decided were not worth reading,
[01:34:05.000 --> 01:34:07.000]   that you didn't have to bother filtering out.
[01:34:07.000 --> 01:34:09.000]   And so this is probably one of the most important things
[01:34:09.000 --> 01:34:11.000]   if you wanna keep up.
[01:34:11.000 --> 01:34:12.000]   Thank you.
[01:34:13.000 --> 01:34:16.000]   (audience applauding)
[01:34:16.000 --> 01:34:19.360]   [APPLAUSE]

