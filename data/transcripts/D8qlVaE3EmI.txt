
[00:00:00.000 --> 00:00:06.480]   what's an underrated aspect of machine learning or machine translation that you think people should
[00:00:06.480 --> 00:00:10.720]   pay more attention to or that you'd be thinking about if you weren't working on Lilt?
[00:00:10.720 --> 00:00:18.000]   Maybe it's around the question that you posed earlier, which is this human parody question
[00:00:18.000 --> 00:00:23.360]   with translation, which there was a paper, I don't know, two years ago, Microsoft had a paper saying
[00:00:23.360 --> 00:00:29.120]   human parody has been achieved. And then two weeks ago, Google published a paper on archive saying
[00:00:29.120 --> 00:00:36.800]   human parody has not been achieved. And I think that in our application,
[00:00:36.800 --> 00:00:44.240]   there's a lot to translation quality, which is like the particular message that you're trying
[00:00:44.240 --> 00:00:48.080]   to deliver to an audience, which a lot has to do like how the audience feels.
[00:00:48.080 --> 00:00:55.520]   And certainly in my time in grad school, I was really focused on just like generating the output
[00:00:55.520 --> 00:01:01.520]   that matches the reference. So the blue score goes up and I can write a paper. And I think
[00:01:01.520 --> 00:01:06.800]   there's a lot of interesting work to think about broader pragmatic context of the language that's
[00:01:06.800 --> 00:01:12.080]   generated and is it appropriate for the context that you're in and for the domain. And that's
[00:01:12.080 --> 00:01:16.960]   really hard to evaluate, but it's really worth thinking about whether it's in
[00:01:16.960 --> 00:01:23.040]   natural language generation or machine translation or whatever else. So I think
[00:01:23.920 --> 00:01:26.640]   maybe thinking about that a little bit harder, I would spend some time on it.
[00:01:26.640 --> 00:01:30.160]   Yeah. The blue score is funny because it seems like such a sad
[00:01:30.160 --> 00:01:36.480]   metric for translation. It makes sense that it works, but it just seems so ludicrously simple.
[00:01:36.480 --> 00:01:42.800]   I mean, at some point, I feel like it must sort of lose meaning as the best possible metric, right?
[00:01:42.800 --> 00:01:48.640]   Well, people studied it a lot. And I think the conclusion was that it's the least bad thing that
[00:01:48.640 --> 00:01:54.560]   we've come up with. And over two decades of study, it continued to be the least... Nobody could come
[00:01:54.560 --> 00:02:00.320]   up with anything that was as convenient and correlated better with human judgment. So
[00:02:00.320 --> 00:02:05.600]   maybe it's a testament to a simple idea that people are still using 20 years later.
[00:02:05.600 --> 00:02:11.040]   I guess simple metrics are better than complicated metrics. There might be a lesson for business.
[00:02:11.040 --> 00:02:14.880]   There might be a lesson there too. Yeah. If you're enjoying Gradient Descent,
[00:02:15.440 --> 00:02:20.480]   I'd really love for you to check out Fully Connected, which is an inclusive machine learning
[00:02:20.480 --> 00:02:25.920]   community that we're building to let everyone know about all the stuff going on in ML and all the new
[00:02:25.920 --> 00:02:32.240]   research coming out. If you go to wmb.ai/fc, you can see all the different stuff that we do,
[00:02:32.240 --> 00:02:37.760]   including Gradient Descent, but also salons where we talk about new research and folks share insights,
[00:02:37.760 --> 00:02:43.200]   AMAs where you can directly connect with members of our community, and a Slack channel where you
[00:02:43.200 --> 00:02:49.040]   can get answers to everything from very basic questions about ML to bug reports on weights
[00:02:49.040 --> 00:02:54.080]   and biases to how to hire an ML team. We're looking forward to meeting you.

