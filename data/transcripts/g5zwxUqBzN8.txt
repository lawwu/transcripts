
[00:00:00.000 --> 00:00:06.120]   Welcome back everyone.
[00:00:06.120 --> 00:00:08.040]   This is part three in our series on
[00:00:08.040 --> 00:00:10.440]   advanced behavioral testing for NLU.
[00:00:10.440 --> 00:00:13.040]   Our focus is the principle of compositionality.
[00:00:13.040 --> 00:00:15.260]   This is a principle that is important to me as
[00:00:15.260 --> 00:00:18.480]   a linguistic semanticist and it's also arguably
[00:00:18.480 --> 00:00:21.480]   a prerequisite for understanding the goals of
[00:00:21.480 --> 00:00:23.120]   COGs and re-COGs which are
[00:00:23.120 --> 00:00:26.260]   compositional generalization benchmarks.
[00:00:26.260 --> 00:00:28.800]   Let's start with an informal statement of
[00:00:28.800 --> 00:00:30.540]   the compositionality principle.
[00:00:30.540 --> 00:00:33.880]   It says, "The meaning of a phrase is a function of
[00:00:33.880 --> 00:00:36.720]   the meanings of its immediate syntactic constituents
[00:00:36.720 --> 00:00:38.800]   and the way they are combined."
[00:00:38.800 --> 00:00:42.000]   That's the principle. Let's unpack it by way of an example.
[00:00:42.000 --> 00:00:44.800]   I have a simple syntactic structure here,
[00:00:44.800 --> 00:00:48.040]   a full sentence, "Every student admired the idea."
[00:00:48.040 --> 00:00:50.960]   The compositionality principle says that the meaning of
[00:00:50.960 --> 00:00:53.880]   this S node for sentence here is fully
[00:00:53.880 --> 00:00:56.880]   determined by the meaning of its two constituent parts,
[00:00:56.880 --> 00:00:59.040]   NP and VP.
[00:00:59.040 --> 00:01:01.760]   You can see that this implies a recursive process.
[00:01:01.760 --> 00:01:03.080]   What is the meaning for the NP?
[00:01:03.080 --> 00:01:05.780]   Well, that is fully determined by the meanings of
[00:01:05.780 --> 00:01:10.480]   this debt for determiner node and this N for noun node.
[00:01:10.480 --> 00:01:13.240]   The meanings of those are easy to see.
[00:01:13.240 --> 00:01:15.080]   Those are fully determined by their parts,
[00:01:15.080 --> 00:01:16.880]   which there's just one part for each,
[00:01:16.880 --> 00:01:18.480]   and those are lexical items.
[00:01:18.480 --> 00:01:21.340]   That's where this recursive process grounds out.
[00:01:21.340 --> 00:01:24.340]   The intuition is that you just have to learn
[00:01:24.340 --> 00:01:27.400]   all the lexical items of the languages that you speak.
[00:01:27.400 --> 00:01:28.940]   Having done that and having
[00:01:28.940 --> 00:01:30.920]   figured out how they combine with each other,
[00:01:30.920 --> 00:01:33.800]   you have a recursive process that allows you to combine
[00:01:33.800 --> 00:01:35.200]   things in new ways and
[00:01:35.200 --> 00:01:37.860]   understand novel combinations of these elements.
[00:01:37.860 --> 00:01:39.620]   The compositionality is saying that you have
[00:01:39.620 --> 00:01:41.940]   guarantee there because the meaning of
[00:01:41.940 --> 00:01:43.980]   the whole will be a function of
[00:01:43.980 --> 00:01:46.420]   the meaning of the parts and how they are combined.
[00:01:46.420 --> 00:01:49.600]   We could also think about this in a bottom-up fashion.
[00:01:49.600 --> 00:01:51.480]   We start with those lexical items,
[00:01:51.480 --> 00:01:53.280]   their meanings are stipulated or
[00:01:53.280 --> 00:01:55.640]   learned and memorized.
[00:01:55.640 --> 00:01:57.600]   Then those meanings in turn
[00:01:57.600 --> 00:02:00.980]   determine the meanings of these parent nodes here,
[00:02:00.980 --> 00:02:03.240]   which in turn determine the meanings of
[00:02:03.240 --> 00:02:06.320]   the complex nodes above them and so forth until we have
[00:02:06.320 --> 00:02:10.640]   derived bottom-up a meaning for the entire sentence.
[00:02:10.640 --> 00:02:13.320]   Why do linguists tend
[00:02:13.320 --> 00:02:15.560]   to adhere to the compositionality principle?
[00:02:15.560 --> 00:02:18.200]   Well, this can be a little bit hard to reconstruct,
[00:02:18.200 --> 00:02:21.600]   but I would say that the usual motivations are as follows.
[00:02:21.600 --> 00:02:23.920]   First, we might just hope that
[00:02:23.920 --> 00:02:26.480]   as semantics is trying to study language,
[00:02:26.480 --> 00:02:29.600]   we would model all the meaningful units of the language,
[00:02:29.600 --> 00:02:31.480]   and that would imply that we have gone all the way
[00:02:31.480 --> 00:02:33.960]   down to even the most incidental looking
[00:02:33.960 --> 00:02:36.400]   lexical items and given them meanings in
[00:02:36.400 --> 00:02:38.800]   isolation like good lexicographers
[00:02:38.800 --> 00:02:40.460]   might feel they should do.
[00:02:40.460 --> 00:02:43.260]   In practice, I should point out that that means there's
[00:02:43.260 --> 00:02:45.960]   a lot of abstraction around linguistic semantics
[00:02:45.960 --> 00:02:47.800]   because it is just hard,
[00:02:47.800 --> 00:02:49.640]   perhaps impossible, to give
[00:02:49.640 --> 00:02:51.520]   a meaning for a word like every in
[00:02:51.520 --> 00:02:53.920]   isolation from the things that it combines with.
[00:02:53.920 --> 00:02:56.080]   What happens in practice actually
[00:02:56.080 --> 00:02:58.560]   is that the meanings assigned are functions.
[00:02:58.560 --> 00:03:00.360]   What we're saying here is that every is
[00:03:00.360 --> 00:03:02.160]   a functional meaning that when
[00:03:02.160 --> 00:03:04.440]   combined with the meaning for student,
[00:03:04.440 --> 00:03:06.840]   delivers another function that when
[00:03:06.840 --> 00:03:09.000]   combined with the meaning of this verb phrase,
[00:03:09.000 --> 00:03:11.720]   finally gives us a meaning for this S node up here,
[00:03:11.720 --> 00:03:14.780]   and it's something like universal quantification where,
[00:03:14.780 --> 00:03:16.680]   in this case, if something is a student,
[00:03:16.680 --> 00:03:19.320]   then it has the property of admiring the idea.
[00:03:19.320 --> 00:03:21.600]   That would be the fundamental claim of the sentence,
[00:03:21.600 --> 00:03:24.720]   and you can see there that that claim was driven by
[00:03:24.720 --> 00:03:26.100]   every down there in
[00:03:26.100 --> 00:03:28.960]   this determiner position inside the subject.
[00:03:28.960 --> 00:03:30.600]   A great deal of abstraction,
[00:03:30.600 --> 00:03:32.160]   but that is a technique for
[00:03:32.160 --> 00:03:34.220]   giving meanings to all the meaningful units,
[00:03:34.220 --> 00:03:35.880]   which should be a consequence
[00:03:35.880 --> 00:03:38.220]   of adhering to compositionality.
[00:03:38.220 --> 00:03:40.900]   You often hear linguists talk about
[00:03:40.900 --> 00:03:42.720]   the supposed infinite capacity
[00:03:42.720 --> 00:03:44.900]   that humans have for dealing with language.
[00:03:44.900 --> 00:03:47.520]   I grant that there is some sense in which this is true
[00:03:47.520 --> 00:03:50.360]   because there seems to be no principle bound on
[00:03:50.360 --> 00:03:52.000]   the complexity or length of
[00:03:52.000 --> 00:03:53.640]   the sentences that we can understand
[00:03:53.640 --> 00:03:55.440]   in a abstract way.
[00:03:55.440 --> 00:03:58.020]   But this needs to be heavily qualified.
[00:03:58.020 --> 00:04:01.040]   I'm sad to report that we are all finite beings,
[00:04:01.040 --> 00:04:03.720]   and therefore there is only a finite capacity in all of
[00:04:03.720 --> 00:04:08.280]   us to make and understand language.
[00:04:08.280 --> 00:04:09.920]   This is a little bit overblown,
[00:04:09.920 --> 00:04:11.040]   but I see what this is getting,
[00:04:11.040 --> 00:04:12.680]   and I think the fundamental intuition is
[00:04:12.680 --> 00:04:14.780]   something more like creativity.
[00:04:14.780 --> 00:04:17.000]   We have an impressive ability to be
[00:04:17.000 --> 00:04:18.400]   creative with language.
[00:04:18.400 --> 00:04:21.000]   By and large, the sentences that you
[00:04:21.000 --> 00:04:23.040]   produce today and the sentences that you
[00:04:23.040 --> 00:04:25.200]   interpreted today had never been
[00:04:25.200 --> 00:04:27.540]   encountered before in all of human history.
[00:04:27.540 --> 00:04:29.320]   Most sentences are like that,
[00:04:29.320 --> 00:04:31.940]   and yet nonetheless, we are able to instantly and
[00:04:31.940 --> 00:04:34.240]   effortlessly produce these sentences
[00:04:34.240 --> 00:04:35.880]   and understand what they mean.
[00:04:35.880 --> 00:04:39.680]   That does imply that there is some capacity in
[00:04:39.680 --> 00:04:42.800]   us for making use of a finite resource,
[00:04:42.800 --> 00:04:44.040]   say the lexical items,
[00:04:44.040 --> 00:04:46.680]   combining them in new ways in order to be
[00:04:46.680 --> 00:04:48.160]   creative with language and
[00:04:48.160 --> 00:04:51.760]   compositionality could be seen as an explanation for that.
[00:04:51.760 --> 00:04:54.080]   There's also a related idea from
[00:04:54.080 --> 00:04:56.040]   cognitive science called systematicity,
[00:04:56.040 --> 00:04:58.940]   which I think is a slightly more general notion than
[00:04:58.940 --> 00:05:03.280]   compositionality and may be a more correct characterization.
[00:05:03.280 --> 00:05:05.760]   Let's dive into that a little bit under
[00:05:05.760 --> 00:05:09.200]   the heading of compositionality or systematicity.
[00:05:09.200 --> 00:05:11.400]   The systematicity idea traces,
[00:05:11.400 --> 00:05:13.840]   as far as I know, to Fodor and Pilashin.
[00:05:13.840 --> 00:05:16.300]   They say, "What we mean when we say
[00:05:16.300 --> 00:05:18.940]   that linguistic capacities are systematic,
[00:05:18.940 --> 00:05:21.680]   is that the ability to produce or understand
[00:05:21.680 --> 00:05:24.700]   some sentences is intrinsically connected to
[00:05:24.700 --> 00:05:28.360]   the ability to produce or understand certain other ones."
[00:05:28.360 --> 00:05:31.000]   The idea is that if you understand the sentence,
[00:05:31.000 --> 00:05:32.620]   Sandy loves the puppy,
[00:05:32.620 --> 00:05:34.920]   then just by that very fact,
[00:05:34.920 --> 00:05:37.600]   you also understand the puppy loves Sandy.
[00:05:37.600 --> 00:05:39.520]   If you recognize that there is
[00:05:39.520 --> 00:05:41.440]   a certain distributional affinity
[00:05:41.440 --> 00:05:43.520]   between the turtle and the puppy,
[00:05:43.520 --> 00:05:45.680]   you can also instantly and
[00:05:45.680 --> 00:05:48.280]   effortlessly understand the turtle loves the puppy,
[00:05:48.280 --> 00:05:49.800]   the puppy loves the turtle,
[00:05:49.800 --> 00:05:52.240]   the turtle loves Sandy, and so forth and so on.
[00:05:52.240 --> 00:05:55.760]   You get this instant explosion in the number of things that
[00:05:55.760 --> 00:05:58.680]   you know in some sense as a consequence of
[00:05:58.680 --> 00:06:02.320]   your own understanding of language being so systematic.
[00:06:02.320 --> 00:06:05.240]   I do think that compositionality could be
[00:06:05.240 --> 00:06:07.600]   a particular way of explaining what we
[00:06:07.600 --> 00:06:09.360]   observe about the systematicity
[00:06:09.360 --> 00:06:11.360]   of the human capacity for language.
[00:06:11.360 --> 00:06:14.200]   But I think systematicity is arguably more general.
[00:06:14.200 --> 00:06:16.440]   You can see that it's given a distributional
[00:06:16.440 --> 00:06:19.340]   characterization here that might allow for things that are
[00:06:19.340 --> 00:06:22.140]   not strictly compositional but nonetheless,
[00:06:22.140 --> 00:06:24.680]   importantly, systematic.
[00:06:24.680 --> 00:06:28.460]   Systematicity is a powerful idea for thinking about
[00:06:28.460 --> 00:06:29.920]   the intuition behind many of
[00:06:29.920 --> 00:06:31.420]   the behavioral tests that we run,
[00:06:31.420 --> 00:06:35.380]   especially the hypothesis-driven challenge tests that we run.
[00:06:35.380 --> 00:06:38.960]   Because very often when we express concerns about systems,
[00:06:38.960 --> 00:06:40.880]   they are concerns that are grounded in
[00:06:40.880 --> 00:06:43.080]   a certain lack of systematicity.
[00:06:43.080 --> 00:06:45.200]   Here's a brief example to illustrate this.
[00:06:45.200 --> 00:06:48.440]   This is from a real sentiment classification model
[00:06:48.440 --> 00:06:50.940]   that I developed that I thought was pretty good,
[00:06:50.940 --> 00:06:54.160]   and I started posing little challenge problems to it.
[00:06:54.160 --> 00:06:58.040]   I was initially very encouraged by these examples.
[00:06:58.040 --> 00:07:01.800]   The bakery sells a mean apple pie is
[00:07:01.800 --> 00:07:05.400]   generally a positive claim about this bakery's pies,
[00:07:05.400 --> 00:07:08.640]   and it involves this very unusual sense of mean,
[00:07:08.640 --> 00:07:10.440]   which essentially means good.
[00:07:10.440 --> 00:07:12.920]   A mean apple pie is typically a good one.
[00:07:12.920 --> 00:07:15.440]   I was encouraged that the gold label
[00:07:15.440 --> 00:07:17.920]   and the predicted label aligned here.
[00:07:17.920 --> 00:07:21.200]   Similarly, for they sell a mean apple pie,
[00:07:21.200 --> 00:07:23.160]   I was happy to see this alignment,
[00:07:23.160 --> 00:07:26.180]   and I started to think that my model truly understand
[00:07:26.180 --> 00:07:30.140]   this very specialized sense of the adjective mean.
[00:07:30.140 --> 00:07:33.160]   But that fell apart with the next two examples.
[00:07:33.160 --> 00:07:34.880]   She sells a mean apple pie,
[00:07:34.880 --> 00:07:36.360]   he sells a mean apple pie,
[00:07:36.360 --> 00:07:38.340]   both of those were predicted negative,
[00:07:38.340 --> 00:07:41.120]   whereas the gold label is of course still positive.
[00:07:41.120 --> 00:07:42.720]   The errors are worrisome,
[00:07:42.720 --> 00:07:44.400]   but the deeper thing that I was worried
[00:07:44.400 --> 00:07:46.340]   about is the lack of systematicity,
[00:07:46.340 --> 00:07:48.000]   because as a human,
[00:07:48.000 --> 00:07:51.260]   I have no expectation that changing the subject from
[00:07:51.260 --> 00:07:54.280]   a plural pronoun to a singular one or using
[00:07:54.280 --> 00:07:56.560]   a pronoun as opposed to a full noun phrase like
[00:07:56.560 --> 00:07:58.800]   the bakery would have any effect on
[00:07:58.800 --> 00:08:01.900]   the interpretation of the adjective mean in these cases.
[00:08:01.900 --> 00:08:04.320]   Yet nonetheless, the model's predictions
[00:08:04.320 --> 00:08:08.720]   changed and that manifests for me as a lack of systematicity.
[00:08:08.720 --> 00:08:11.380]   That's a guiding intuition behind many of
[00:08:11.380 --> 00:08:12.880]   the adversarial or challenge
[00:08:12.880 --> 00:08:14.360]   datasets that people have posed.
[00:08:14.360 --> 00:08:17.400]   They have a hypothesis grounded in the systematicity of
[00:08:17.400 --> 00:08:20.160]   language and they observe departures from that in
[00:08:20.160 --> 00:08:24.560]   their models and they begin to worry about those models.
[00:08:24.560 --> 00:08:26.720]   It's interesting to reflect on
[00:08:26.720 --> 00:08:28.240]   the compositionality principle in
[00:08:28.240 --> 00:08:31.200]   the context of the history of AI models.
[00:08:31.200 --> 00:08:33.860]   In the earliest eras of AI like
[00:08:33.860 --> 00:08:36.560]   the Sherdlue system or the chat 80 system
[00:08:36.560 --> 00:08:38.140]   that we saw on the first day,
[00:08:38.140 --> 00:08:41.960]   we got compositionality by design because those were
[00:08:41.960 --> 00:08:44.640]   implemented grammars, symbolic grammars that
[00:08:44.640 --> 00:08:47.520]   themselves adhere to the compositionality principle.
[00:08:47.520 --> 00:08:49.720]   We didn't wonder about whether
[00:08:49.720 --> 00:08:51.600]   these NLU models were compositional
[00:08:51.600 --> 00:08:55.040]   because we presupposed that they would be.
[00:08:55.040 --> 00:08:58.120]   Parts of that actually did carry forward
[00:08:58.120 --> 00:09:00.800]   into the more modern machine learning era.
[00:09:00.800 --> 00:09:03.440]   For example, many semantic parsing systems,
[00:09:03.440 --> 00:09:06.360]   like this one depicted from Percy Leung's work,
[00:09:06.360 --> 00:09:08.620]   were also compositional in the sense that
[00:09:08.620 --> 00:09:11.240]   underlyingly there was a compositional grammar
[00:09:11.240 --> 00:09:13.140]   and the task was to learn weights
[00:09:13.140 --> 00:09:14.740]   on the rules of that grammar.
[00:09:14.740 --> 00:09:17.420]   Arguably, the resulting artifacts were
[00:09:17.420 --> 00:09:19.500]   compositional with some stochasticity
[00:09:19.500 --> 00:09:23.080]   associated with them being probabilistic models.
[00:09:23.080 --> 00:09:26.180]   Even in the more modern deep learning era,
[00:09:26.180 --> 00:09:29.540]   we again saw systems that were arguably compositional.
[00:09:29.540 --> 00:09:31.140]   This is from the paper that launched
[00:09:31.140 --> 00:09:32.640]   the Stanford Sentiment Treebank.
[00:09:32.640 --> 00:09:36.940]   It's a recursive tree-structured neural network.
[00:09:36.940 --> 00:09:40.120]   It abides by the compositionality principle in
[00:09:40.120 --> 00:09:41.900]   the sense that all the nodes depicted
[00:09:41.900 --> 00:09:44.180]   in these structures denote vectors.
[00:09:44.180 --> 00:09:46.560]   There was a complicated deep learning function that
[00:09:46.560 --> 00:09:48.480]   combined those vectors to derive
[00:09:48.480 --> 00:09:50.280]   the meaning for their parent nodes.
[00:09:50.280 --> 00:09:52.200]   It did that recursively until we
[00:09:52.200 --> 00:09:54.600]   got a meaning for the entire sentence.
[00:09:54.600 --> 00:09:56.800]   It's not symbolic in the way of
[00:09:56.800 --> 00:09:58.400]   these older systems and in
[00:09:58.400 --> 00:10:00.720]   the way of much work in linguistic semantics,
[00:10:00.720 --> 00:10:03.660]   but it is arguably a compositional system,
[00:10:03.660 --> 00:10:05.580]   an intriguing property of
[00:10:05.580 --> 00:10:08.240]   those deep learning artifacts in fact.
[00:10:08.240 --> 00:10:10.640]   But we have, it seems,
[00:10:10.640 --> 00:10:12.720]   moved away from that perspective.
[00:10:12.720 --> 00:10:15.100]   Now we're confronted all the time with
[00:10:15.100 --> 00:10:17.800]   these huge typically transformer-based models
[00:10:17.800 --> 00:10:20.360]   where everything is connected to everything else.
[00:10:20.360 --> 00:10:22.180]   It is certainly clear that we have
[00:10:22.180 --> 00:10:24.520]   no guarantees of compositionality
[00:10:24.520 --> 00:10:27.360]   or systematicity built into these networks.
[00:10:27.360 --> 00:10:29.180]   In fact, in the earliest days,
[00:10:29.180 --> 00:10:30.860]   people often worried that
[00:10:30.860 --> 00:10:32.840]   even though they were performing well,
[00:10:32.840 --> 00:10:35.800]   they were learning non-systematic solutions and that
[00:10:35.800 --> 00:10:39.120]   motivated a lot of challenge testing for them.
[00:10:39.120 --> 00:10:40.840]   The question now is,
[00:10:40.840 --> 00:10:43.520]   can we pose behavioral tests that will truly
[00:10:43.520 --> 00:10:45.840]   assess whether models like this,
[00:10:45.840 --> 00:10:48.000]   which are hard to understand analytically,
[00:10:48.000 --> 00:10:50.960]   have found systematic solutions
[00:10:50.960 --> 00:10:53.000]   to the language problems that we pose?
[00:10:53.000 --> 00:10:55.360]   If the answer is no, we should worry.
[00:10:55.360 --> 00:10:56.880]   If the answer is yes,
[00:10:56.880 --> 00:10:59.020]   it's an amazing discovery about why
[00:10:59.020 --> 00:11:01.360]   these models perform so well and also
[00:11:01.360 --> 00:11:04.200]   often an amazing discovery about the power of
[00:11:04.200 --> 00:11:06.320]   data-driven learning alone to
[00:11:06.320 --> 00:11:10.120]   deliver systematic solutions to language problems.
[00:11:10.120 --> 00:11:11.840]   It's an open question but
[00:11:11.840 --> 00:11:14.080]   a tremendously exciting set of
[00:11:14.080 --> 00:11:16.200]   questions to be exploring now as
[00:11:16.200 --> 00:11:18.480]   our models are getting so good even at
[00:11:18.480 --> 00:11:22.080]   the hard behavioral tasks that we pose for them.
[00:11:22.080 --> 00:11:32.080]   [BLANK_AUDIO]

