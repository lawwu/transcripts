
[00:00:00.000 --> 00:00:03.000]   [MUSIC PLAYING]
[00:00:03.000 --> 00:00:05.520]   So welcome, everyone, to my webinar.
[00:00:05.520 --> 00:00:09.200]   And today, we're going to talk about the Sable Baselines 3
[00:00:09.200 --> 00:00:11.720]   integration with Weights and Biases.
[00:00:11.720 --> 00:00:14.080]   And by the end of the webinar, you'll
[00:00:14.080 --> 00:00:17.160]   learn how to use Sable Baselines 3 to conduct
[00:00:17.160 --> 00:00:18.800]   reinforcement learning experiments
[00:00:18.800 --> 00:00:21.720]   and how to use Weights and Biases to keep track
[00:00:21.720 --> 00:00:25.360]   of the metrics and the videos of the agents playing the game.
[00:00:25.360 --> 00:00:29.000]   So here is the agenda for the talk today.
[00:00:29.000 --> 00:00:34.120]   So we're going to talk about what Weights and Biases is.
[00:00:34.120 --> 00:00:37.360]   We're going to talk about what Sable Baselines is.
[00:00:37.360 --> 00:00:40.360]   And I'll talk about the recent integration we made.
[00:00:40.360 --> 00:00:44.120]   Then we'll go into a live demo with a Colab notebook.
[00:00:44.120 --> 00:00:47.720]   And lastly, I'll also showcase how
[00:00:47.720 --> 00:00:49.280]   you can use Weights and Biases UI
[00:00:49.280 --> 00:00:51.440]   to do a lot of experiment analysis
[00:00:51.440 --> 00:00:52.480]   in a really easy fashion.
[00:00:55.760 --> 00:00:59.440]   So before talking about what Weights and Biases is,
[00:00:59.440 --> 00:01:01.400]   what Sable Baselines 3 is, I want
[00:01:01.400 --> 00:01:03.920]   to give you a little context of doing research
[00:01:03.920 --> 00:01:05.800]   in reinforcement learning.
[00:01:05.800 --> 00:01:11.440]   So overall, as a PhD student, at the start of my PhD program,
[00:01:11.440 --> 00:01:16.000]   I was really trying to find ways of how I can do reinforcement
[00:01:16.000 --> 00:01:16.840]   learning research.
[00:01:16.840 --> 00:01:20.480]   And it was very quick before I realized managing RL experiments
[00:01:20.480 --> 00:01:21.480]   can be a pain.
[00:01:21.480 --> 00:01:24.360]   There's a lot of problems on the reproducibility side.
[00:01:24.360 --> 00:01:26.800]   If you don't record your hyperparameters,
[00:01:26.800 --> 00:01:30.520]   your commands that was used to produce the experiment,
[00:01:30.520 --> 00:01:33.440]   if you don't record the software dependency,
[00:01:33.440 --> 00:01:36.240]   then it's very possible that after just a few months,
[00:01:36.240 --> 00:01:38.640]   you wouldn't be able to reproduce your experiment
[00:01:38.640 --> 00:01:39.560]   anymore.
[00:01:39.560 --> 00:01:43.400]   There's also a lot of issue with analysis and visualization.
[00:01:43.400 --> 00:01:47.560]   How you set up, how you save the plots and data
[00:01:47.560 --> 00:01:50.120]   can be problematic sometimes, and how
[00:01:50.120 --> 00:01:52.520]   you view them aggregately.
[00:01:52.520 --> 00:01:56.360]   Those are all little problems, but in aggregate,
[00:01:56.360 --> 00:01:59.800]   they become somewhat of pain for research.
[00:01:59.800 --> 00:02:02.120]   And there's also the reliability concern
[00:02:02.120 --> 00:02:05.360]   with doing reinforcement learning research.
[00:02:05.360 --> 00:02:07.880]   There are thousands of reinforcement learning
[00:02:07.880 --> 00:02:10.960]   implementations out there, but we
[00:02:10.960 --> 00:02:14.880]   know that RL algorithms can be quite sensitive to implementation
[00:02:14.880 --> 00:02:16.440]   details.
[00:02:16.440 --> 00:02:19.920]   So today, I'll talk about how Weights and Biases will
[00:02:19.920 --> 00:02:23.000]   help address the reproducibility challenge, the challenge
[00:02:23.000 --> 00:02:26.320]   with analysis and visualization, and how stable baselines will
[00:02:26.320 --> 00:02:27.920]   address the reliability.
[00:02:27.920 --> 00:02:34.440]   So let's talk about Weights and Biases in a nutshell.
[00:02:34.440 --> 00:02:40.920]   So I will actually share a video from my colleague, Charles,
[00:02:40.920 --> 00:02:43.280]   because he has done a really great video just
[00:02:43.280 --> 00:02:45.120]   to showcase all of the features.
[00:02:45.120 --> 00:02:47.040]   So let me go ahead and share the video.
[00:02:47.040 --> 00:02:55.360]   [VIDEO PLAYBACK]
[00:02:55.360 --> 00:02:57.360]   - Weights and Biases is the developer stack
[00:02:57.360 --> 00:02:59.000]   for machine learning practitioners.
[00:02:59.000 --> 00:03:00.920]   Use these lightweight, interoperable tools
[00:03:00.920 --> 00:03:03.520]   for the entire lifecycle of your machine learning projects--
[00:03:03.520 --> 00:03:06.120]   experiment tracking, hyperparameter optimization,
[00:03:06.120 --> 00:03:09.120]   data set and model versioning, even sharing results.
[00:03:09.120 --> 00:03:12.560]   WB is trusted by over 70,000 machine learning practitioners,
[00:03:12.560 --> 00:03:15.400]   delivering better medicine, safer self-driving cars,
[00:03:15.400 --> 00:03:17.800]   more sustainable farming, technologies that build
[00:03:17.800 --> 00:03:19.320]   a better collective future.
[00:03:19.320 --> 00:03:21.120]   You can easily capture all the information
[00:03:21.120 --> 00:03:22.960]   you need to make your models reproducible.
[00:03:22.960 --> 00:03:24.640]   Your future self will thank you.
[00:03:24.640 --> 00:03:26.840]   Start with tracking metrics for a single experiment,
[00:03:26.840 --> 00:03:28.300]   then compare hundreds of experiments,
[00:03:28.300 --> 00:03:29.800]   visualize thousands of predictions,
[00:03:29.800 --> 00:03:31.760]   and share the insights you've gained.
[00:03:31.760 --> 00:03:33.800]   Take your existing ML code, regardless
[00:03:33.800 --> 00:03:36.680]   of what framework you're using, add a lightweight WNB
[00:03:36.680 --> 00:03:39.920]   integration, and quickly get live metrics, terminal logs,
[00:03:39.920 --> 00:03:43.080]   and system stats streamed to the centralized dashboard.
[00:03:43.080 --> 00:03:46.240]   WNB also organizes the results of entire projects.
[00:03:46.240 --> 00:03:48.640]   Even if you and your team tried thousands of experiments,
[00:03:48.640 --> 00:03:51.320]   you can still painlessly access the results in a single place,
[00:03:51.320 --> 00:03:54.200]   from any device, at any time, and query and filter results
[00:03:54.200 --> 00:03:55.840]   to spot issues and trends.
[00:03:55.840 --> 00:03:57.760]   Track your progress towards the optimal model,
[00:03:57.760 --> 00:03:59.640]   take notes on your results across experiments,
[00:03:59.640 --> 00:04:01.560]   and share findings with collaborators.
[00:04:01.560 --> 00:04:03.640]   Capture the valuable research efforts and insights
[00:04:03.640 --> 00:04:06.840]   that are often lost over time, or when a team member leaves.
[00:04:06.840 --> 00:04:08.920]   Everything is built to empower collaboration.
[00:04:08.920 --> 00:04:10.260]   Just imagine what your team could
[00:04:10.260 --> 00:04:12.320]   do with a shared tool for data set versioning,
[00:04:12.320 --> 00:04:14.960]   model management, experiment tracking, and hyperparameter
[00:04:14.960 --> 00:04:15.880]   schemes.
[00:04:15.880 --> 00:04:17.960]   That's a quick summary of Weights and Biases,
[00:04:17.960 --> 00:04:19.520]   the flexible, lightweight developer
[00:04:19.520 --> 00:04:21.560]   stack for machine learning practitioners.
[00:04:21.560 --> 00:04:24.400]   Get started now, and see live results in just five minutes.
[00:04:24.400 --> 00:04:34.060]   Cool.
[00:04:34.060 --> 00:04:38.400]   So that's basically a Weights and Biases in a nutshell.
[00:04:38.400 --> 00:04:41.240]   Let me go ahead and share my screen again.
[00:04:41.240 --> 00:04:50.600]   And then you can always go to the Weights and Biases website.
[00:04:50.600 --> 00:04:53.400]   I think we do-- our marketing team does a great job
[00:04:53.400 --> 00:04:57.400]   in listing all of the features, and you
[00:04:57.400 --> 00:04:59.480]   can see how you can integrate Weights and Biases
[00:04:59.480 --> 00:05:00.640]   into your code.
[00:05:00.640 --> 00:05:03.200]   There are some code samples.
[00:05:03.200 --> 00:05:05.080]   So I highly encourage you to check out
[00:05:05.080 --> 00:05:06.760]   our website and documentation.
[00:05:06.760 --> 00:05:12.320]   And the website is wmb.ai/site.
[00:05:12.320 --> 00:05:17.040]   With that said, we'll talk about what Stable Baselines is.
[00:05:17.040 --> 00:05:24.120]   And Antonian and Anssi will give you an introduction
[00:05:24.120 --> 00:05:26.800]   on what Stable Baseline is.
[00:05:26.800 --> 00:05:29.040]   Hand it over to you.
[00:05:29.040 --> 00:05:29.540]   Thank you.
[00:05:29.540 --> 00:05:33.000]   So well, as mentioned, the idea between Stable Baseline
[00:05:33.000 --> 00:05:37.720]   is you have many, many different RL libraries out there.
[00:05:37.720 --> 00:05:42.040]   And we focus on mainly providing reliable implementation
[00:05:42.040 --> 00:05:46.520]   of RL algorithm while still being user-friendly.
[00:05:46.520 --> 00:05:49.440]   And we focus on the very specific case,
[00:05:49.440 --> 00:05:53.320]   which is model-free single-agent reinforcement learning.
[00:05:53.320 --> 00:05:57.960]   And the last thing we focus on, and that makes us,
[00:05:57.960 --> 00:06:00.400]   I would say, different from other libraries,
[00:06:00.400 --> 00:06:03.920]   is we favor readability and simplicity over modularity.
[00:06:03.920 --> 00:06:10.400]   So we don't have many modular components.
[00:06:10.400 --> 00:06:15.960]   We mostly have implementation that are not self-contained,
[00:06:15.960 --> 00:06:18.000]   but almost self-contained.
[00:06:18.000 --> 00:06:23.640]   And then, so for the user-friendly part,
[00:06:23.640 --> 00:06:28.720]   this is how easy it is to use in very few lines of code.
[00:06:28.720 --> 00:06:31.280]   This is for creating a soft actor-critic agent
[00:06:31.280 --> 00:06:35.640]   on the pendulum task.
[00:06:35.640 --> 00:06:39.920]   And so you mostly need to define--
[00:06:39.920 --> 00:06:42.440]   it's just follow a scikit-learn syntax.
[00:06:42.440 --> 00:06:48.720]   You just define your model class and then call learn on it,
[00:06:48.720 --> 00:06:50.400]   so to train the agent.
[00:06:50.400 --> 00:06:54.560]   And afterward, after saving and loading with this simple API,
[00:06:54.560 --> 00:06:59.080]   you can query your agent with the predict method.
[00:06:59.080 --> 00:07:01.520]   And this is most of what you need
[00:07:01.520 --> 00:07:07.400]   to know about the API for most of the problem.
[00:07:07.400 --> 00:07:09.280]   And for the rest, I will leave it to Ansi.
[00:07:09.280 --> 00:07:13.640]   Right, thanks, Antonin.
[00:07:13.640 --> 00:07:17.360]   So just to continue on what Antonin was telling,
[00:07:17.360 --> 00:07:20.080]   the other highlights of the stable baselines
[00:07:20.080 --> 00:07:24.000]   three library is that we have almost full test coverage,
[00:07:24.000 --> 00:07:24.520]   almost.
[00:07:24.520 --> 00:07:26.280]   There's some nitpicks that are not tested,
[00:07:26.280 --> 00:07:27.880]   but those are small things.
[00:07:27.880 --> 00:07:29.520]   We have a super active community,
[00:07:29.520 --> 00:07:32.560]   so we have issues almost daily we can handle,
[00:07:32.560 --> 00:07:34.880]   and bug reports, people reporting bugs and so on,
[00:07:34.880 --> 00:07:36.240]   and also doing pull requests.
[00:07:36.240 --> 00:07:38.680]   So that's very handy.
[00:07:38.680 --> 00:07:41.920]   We also like to advertise our documentation, which
[00:07:41.920 --> 00:07:43.880]   covers almost everything.
[00:07:43.880 --> 00:07:47.040]   And we always update it when there is some bigger questions.
[00:07:47.040 --> 00:07:50.760]   And finally, we also have this comprehensive--
[00:07:50.760 --> 00:07:52.520]   we have callbacks you can modify,
[00:07:52.520 --> 00:07:54.160]   but we also have tensor port logging,
[00:07:54.160 --> 00:07:57.080]   but that's going to be touched on by Costa soon.
[00:07:57.080 --> 00:08:02.040]   And finally, we have this training framework included
[00:08:02.040 --> 00:08:03.720]   you can use to run your experiments,
[00:08:03.720 --> 00:08:08.040]   store your runs, et cetera, but also to replicate the results.
[00:08:08.040 --> 00:08:10.760]   So we have gone to great lengths to ensure
[00:08:10.760 --> 00:08:12.560]   that our implementations are correct.
[00:08:12.560 --> 00:08:15.560]   And to that end, we have this ZUR library
[00:08:15.560 --> 00:08:17.400]   you can use to replicate the runs,
[00:08:17.400 --> 00:08:19.440]   and store the hyperparameters, and so on.
[00:08:19.440 --> 00:08:22.680]   But all of this, more on that from Costa.
[00:08:22.680 --> 00:08:24.880]   So now I think it goes back to Costa now.
[00:08:24.880 --> 00:08:30.480]   Thank you for the excellent introduction
[00:08:30.480 --> 00:08:31.880]   on the Stable Beta 7.3.
[00:08:31.880 --> 00:08:34.000]   I'm always a big fan of the library.
[00:08:34.000 --> 00:08:37.200]   I think all of the features are really well tested and well
[00:08:37.200 --> 00:08:38.080]   developed.
[00:08:38.080 --> 00:08:46.120]   And with that, I am ready to introduce to you--
[00:08:46.120 --> 00:08:49.120]   to tell you more about how Ways and Biases could address
[00:08:49.120 --> 00:08:50.480]   the three challenges--
[00:08:50.480 --> 00:08:53.600]   to address the reproducibility challenge, and analysis
[00:08:53.600 --> 00:08:57.160]   challenge, and how Stable Beta Science will address
[00:08:57.160 --> 00:08:59.680]   the reliability challenge.
[00:08:59.680 --> 00:09:05.280]   So continuing on the reproducibility challenge,
[00:09:05.280 --> 00:09:08.680]   so there's a lot of open source reinforcement learning papers
[00:09:08.680 --> 00:09:11.640]   or libraries that when I see their instructions for building
[00:09:11.640 --> 00:09:13.920]   the library or reproducing a paper,
[00:09:13.920 --> 00:09:16.400]   I see instructions like this.
[00:09:16.400 --> 00:09:19.280]   It's like pseudo pip install gem.
[00:09:19.280 --> 00:09:22.080]   And basically, they don't really pin dependencies.
[00:09:22.080 --> 00:09:26.680]   And sometimes that's pretty problematic.
[00:09:26.680 --> 00:09:30.920]   Because if TensorFlow, if they modify an API or something,
[00:09:30.920 --> 00:09:33.400]   then your code will break.
[00:09:33.400 --> 00:09:37.320]   And it also sometimes is confusing
[00:09:37.320 --> 00:09:40.080]   as to how people are recording their hyperparameters.
[00:09:40.080 --> 00:09:44.000]   I've seen some repository where they put hyperparameters
[00:09:44.000 --> 00:09:46.400]   as the name of the folders.
[00:09:46.400 --> 00:09:50.360]   All of these-- and saving the data in CSV files--
[00:09:50.360 --> 00:09:54.480]   all of these methods are fairly homebrew.
[00:09:54.480 --> 00:09:56.560]   That definitely works.
[00:09:56.560 --> 00:10:00.360]   But what we at Ways and Biases is trying to do
[00:10:00.360 --> 00:10:03.440]   is to make this whole process much more streamlined
[00:10:03.440 --> 00:10:05.560]   so that you can log everything.
[00:10:05.560 --> 00:10:07.680]   You can log the software dependencies,
[00:10:07.680 --> 00:10:09.440]   hyperparameters, commands to help
[00:10:09.440 --> 00:10:12.200]   you reproduce your experiments a little easier.
[00:10:12.200 --> 00:10:16.000]   So that's what we're trying to do, to say goodbye to this.
[00:10:16.000 --> 00:10:21.480]   Instead, we have-- for each run that you run with your scripts,
[00:10:21.480 --> 00:10:24.680]   we record your experiments in a centralized dashboard
[00:10:24.680 --> 00:10:27.840]   that you can see all of the information that are related.
[00:10:27.840 --> 00:10:30.680]   So for example, we record the hyperparameters.
[00:10:30.680 --> 00:10:32.760]   And you can see all of the configuration
[00:10:32.760 --> 00:10:34.920]   for a particular algorithm.
[00:10:34.920 --> 00:10:37.320]   We also log the reproduced commands.
[00:10:37.320 --> 00:10:40.000]   In particular, you can see this was a command
[00:10:40.000 --> 00:10:43.800]   that, in the past, I have used to run a reinforcement learning
[00:10:43.800 --> 00:10:44.840]   experiment.
[00:10:44.840 --> 00:10:47.760]   So usually, how I would reproduce this experiment
[00:10:47.760 --> 00:10:50.840]   is simply say, I want to clone this repository,
[00:10:50.840 --> 00:10:54.120]   check out this, and just copy and paste this command.
[00:10:54.120 --> 00:10:58.760]   And I would be able to reproduce this experiment.
[00:10:58.760 --> 00:11:02.000]   And we also automatically ping the software dependencies
[00:11:02.000 --> 00:11:02.520]   as well.
[00:11:02.520 --> 00:11:05.240]   So over here, as you can see, we log the requirements.txt.
[00:11:08.480 --> 00:11:11.440]   And another challenge is analysis and visualization.
[00:11:11.440 --> 00:11:13.560]   This is something that I personally
[00:11:13.560 --> 00:11:15.520]   have felt a lot of pain in.
[00:11:15.520 --> 00:11:18.480]   So whenever I go see a reinforcement learning paper,
[00:11:18.480 --> 00:11:21.760]   I always see some episodic return curves
[00:11:21.760 --> 00:11:22.920]   that look like this.
[00:11:22.920 --> 00:11:24.640]   They're really pretty.
[00:11:24.640 --> 00:11:26.320]   All the curves are smooth.
[00:11:26.320 --> 00:11:30.400]   But when I try to replicate similar plots,
[00:11:30.400 --> 00:11:33.320]   I realize it's actually quite a bit of trouble.
[00:11:33.320 --> 00:11:38.240]   I usually end up writing 50 to 100 lines of matplotlib code.
[00:11:38.240 --> 00:11:41.160]   And spend hours to debug it.
[00:11:41.160 --> 00:11:45.680]   It was overall just not a very smooth experience.
[00:11:45.680 --> 00:11:48.840]   And you have to spend a lot of time and effort into it.
[00:11:48.840 --> 00:11:58.040]   And also, to help debug your experiments,
[00:11:58.040 --> 00:12:00.160]   in a lot of situations, you also really
[00:12:00.160 --> 00:12:03.200]   want to see how well your agent is actually learning.
[00:12:03.200 --> 00:12:06.160]   So this episodic return is a great way
[00:12:06.160 --> 00:12:09.120]   to understand how good your agent is performing.
[00:12:09.120 --> 00:12:11.400]   But it's just a metric.
[00:12:11.400 --> 00:12:13.240]   It's not the ultimate measure.
[00:12:13.240 --> 00:12:15.680]   To me, the ultimate measure is to visualize
[00:12:15.680 --> 00:12:19.400]   what the agent is doing to actually let the agent play
[00:12:19.400 --> 00:12:21.320]   the game or perform the task.
[00:12:21.320 --> 00:12:26.320]   Over here, you see a video of this agent playing Hachita.
[00:12:26.320 --> 00:12:29.840]   And this agent is actually trained with PPO.
[00:12:29.840 --> 00:12:34.320]   And this kind of explains why PPO in this episodic return
[00:12:34.320 --> 00:12:37.600]   graph is having less than 2,000 return.
[00:12:37.600 --> 00:12:41.840]   It's just because it was stuck in this suboptimal behavior.
[00:12:41.840 --> 00:12:47.360]   So if, by chance, that PPO was exploring the other ways
[00:12:47.360 --> 00:12:51.960]   to walk, maybe we can see PPO match the performance of TD3.
[00:12:51.960 --> 00:12:56.880]   So recording everything as much as we can is very important.
[00:12:56.880 --> 00:12:58.880]   And we also would really like a way
[00:12:58.880 --> 00:13:02.960]   to analyze things very easily without writing a lot of code,
[00:13:02.960 --> 00:13:05.960]   worrying about how my data is going to be viewed.
[00:13:05.960 --> 00:13:10.040]   And also, when we share our results
[00:13:10.040 --> 00:13:12.480]   or write experiments notes-- in the past,
[00:13:12.480 --> 00:13:16.080]   I tried to use Google Docs to write experiments notes.
[00:13:16.080 --> 00:13:18.840]   And that wasn't exactly--
[00:13:18.840 --> 00:13:20.720]   sometimes that was a little bit of trouble,
[00:13:20.720 --> 00:13:24.680]   just because if I see a number over at this Google Doc,
[00:13:24.680 --> 00:13:25.640]   I don't know where--
[00:13:25.640 --> 00:13:28.320]   if it's after three months, I wouldn't know
[00:13:28.320 --> 00:13:30.800]   where this number comes from.
[00:13:30.800 --> 00:13:33.600]   And those are all the things that, in Weights and Biases,
[00:13:33.600 --> 00:13:35.640]   we try to address.
[00:13:35.640 --> 00:13:41.320]   So instead of this, in Weights and Biases,
[00:13:41.320 --> 00:13:45.720]   you can basically take the data and just
[00:13:45.720 --> 00:13:47.360]   generate those plots on a fly.
[00:13:47.360 --> 00:13:49.800]   And that's the advantage of having UI.
[00:13:49.800 --> 00:13:53.360]   In a sense, we have these panels that basically work
[00:13:53.360 --> 00:13:57.320]   as an interactive Matplotlib utility that you can just
[00:13:57.320 --> 00:13:59.000]   plot these algorithms.
[00:13:59.000 --> 00:14:02.240]   And over here, I also log the videos as well.
[00:14:02.240 --> 00:14:04.000]   So overall, it's just really helpful.
[00:14:04.000 --> 00:14:08.600]   And the reason why we're able to create these charts on the fly
[00:14:08.600 --> 00:14:12.640]   is because I can simply do--
[00:14:12.640 --> 00:14:14.520]   I conduct a lot of experiments.
[00:14:14.520 --> 00:14:17.920]   And I can use the filtering and the group tools
[00:14:17.920 --> 00:14:24.120]   to filter-- to get to obtain the experiments
[00:14:24.120 --> 00:14:25.880]   that I really care about.
[00:14:25.880 --> 00:14:27.880]   And group is a very powerful feature,
[00:14:27.880 --> 00:14:33.240]   because if you group the experiments by the algorithm,
[00:14:33.240 --> 00:14:37.120]   then it's going to create these shaded area that
[00:14:37.120 --> 00:14:38.920]   are known as error bars.
[00:14:38.920 --> 00:14:40.600]   So that is a very common practice
[00:14:40.600 --> 00:14:43.120]   in reinforcement learning papers.
[00:14:43.120 --> 00:14:45.360]   So with Weights and Biases, this is really easy to do.
[00:14:45.360 --> 00:14:47.760]   With a Matplotlib, you usually end up
[00:14:47.760 --> 00:14:49.560]   writing a lot of code for it.
[00:14:49.560 --> 00:14:53.560]   And we also have a feature called the Weights and Biases
[00:14:53.560 --> 00:14:54.200]   report.
[00:14:54.200 --> 00:14:57.400]   That is, you can basically write the things
[00:14:57.400 --> 00:14:59.040]   that you can in the Google Doc.
[00:14:59.040 --> 00:15:00.800]   You can write in Weights and Biases report,
[00:15:00.800 --> 00:15:03.080]   except all of the numbers in a report,
[00:15:03.080 --> 00:15:06.360]   you can directly trace through the original experiment.
[00:15:06.360 --> 00:15:08.400]   So that really helps with the reproducibility,
[00:15:08.400 --> 00:15:12.400]   because if I see a number, I can trace back to a particular run.
[00:15:12.400 --> 00:15:15.080]   And if I see the run, if you recall,
[00:15:15.080 --> 00:15:17.680]   I have all those reproducibility information.
[00:15:17.680 --> 00:15:20.080]   And then I can reproduce the results.
[00:15:20.080 --> 00:15:22.280]   So overall, Weights and Biases just
[00:15:22.280 --> 00:15:24.080]   provide a lot of tools to address
[00:15:24.080 --> 00:15:28.760]   the analysis and visualization and reproducibility.
[00:15:28.760 --> 00:15:33.840]   And as far as reliability is concerned,
[00:15:33.840 --> 00:15:35.600]   our reinforcement learning algorithms
[00:15:35.600 --> 00:15:39.400]   is really sometimes very sensitive to implementation
[00:15:39.400 --> 00:15:39.920]   details.
[00:15:39.920 --> 00:15:42.240]   In fact, there is a number of paper
[00:15:42.240 --> 00:15:45.600]   on this topic talking about how implementation details could
[00:15:45.600 --> 00:15:47.880]   have an impact on the algorithm's performance.
[00:15:47.880 --> 00:15:51.760]   And that's where Sable Baselines 3 really shines,
[00:15:51.760 --> 00:15:54.560]   is because all of the code goes through a rigorous review
[00:15:54.560 --> 00:16:00.080]   process, and all of the code follows really high quality
[00:16:00.080 --> 00:16:04.200]   standards, so that you have a peace of mind
[00:16:04.200 --> 00:16:09.840]   that this is, in a sense, the most correct implementation.
[00:16:09.840 --> 00:16:12.720]   And now we're ready to talk about the recent integration
[00:16:12.720 --> 00:16:16.160]   that we made with Weights and Biases and Sable Baselines 3.
[00:16:16.160 --> 00:16:21.120]   In particular, in our WMB package,
[00:16:21.120 --> 00:16:25.000]   there is this WMB callback that can be used
[00:16:25.000 --> 00:16:26.800]   with the Sable Baselines code.
[00:16:26.800 --> 00:16:28.880]   And the way you would set it up is
[00:16:28.880 --> 00:16:31.440]   to initialize the Weights and Biases run,
[00:16:31.440 --> 00:16:34.160]   you create a model from Sable Baseline 3,
[00:16:34.160 --> 00:16:39.560]   and then you have this little callback that allows you to say,
[00:16:39.560 --> 00:16:43.800]   I want to save the gradient of the model every 100 steps,
[00:16:43.800 --> 00:16:47.440]   and I can save my model to a particular folder
[00:16:47.440 --> 00:16:49.320]   so that after the run has finished,
[00:16:49.320 --> 00:16:51.000]   Weights and Biases will automatically
[00:16:51.000 --> 00:16:52.840]   upload the model to the cloud.
[00:16:52.840 --> 00:16:57.240]   So those are just some highlights.
[00:16:57.240 --> 00:16:57.920]   Oh, right.
[00:16:57.920 --> 00:17:00.000]   Also, Weights and Biases callback
[00:17:00.000 --> 00:17:03.760]   is going to record all of the hyperparameters that
[00:17:03.760 --> 00:17:05.960]   is associated with a model.
[00:17:05.960 --> 00:17:09.680]   So the general theme, again, is to try
[00:17:09.680 --> 00:17:13.640]   to record everything, as much things as we can,
[00:17:13.640 --> 00:17:17.280]   and in the dashboard so that we can later query
[00:17:17.280 --> 00:17:20.280]   those results very easily.
[00:17:20.280 --> 00:17:25.040]   And we also support recording videos of the agents playing
[00:17:25.040 --> 00:17:25.920]   the game as well.
[00:17:25.920 --> 00:17:30.600]   And the code actually becomes a little bit more verbose.
[00:17:30.600 --> 00:17:35.800]   But basically, you can use this vectorized video recorder
[00:17:35.800 --> 00:17:38.600]   to record videos, and our integration
[00:17:38.600 --> 00:17:41.000]   will automatically upload the videos
[00:17:41.000 --> 00:17:44.040]   that is produced by the Jam environment to the cloud.
[00:17:46.800 --> 00:17:50.160]   So now is a demo time.
[00:17:50.160 --> 00:17:53.480]   So we're going to open a Colab notebook just
[00:17:53.480 --> 00:17:58.560]   to see this from end to end, how you can create a--
[00:17:58.560 --> 00:18:01.400]   how you can do a carpool experiments
[00:18:01.400 --> 00:18:04.800]   with stable baselines and having all of the metrics and videos
[00:18:04.800 --> 00:18:06.600]   being logged to the cloud.
[00:18:06.600 --> 00:18:11.160]   So if you go to wmb.me/sb3, that's
[00:18:11.160 --> 00:18:14.280]   where you can play with the notebook.
[00:18:14.280 --> 00:18:16.280]   So I'm going to go ahead and do that.
[00:18:16.280 --> 00:18:25.600]   And let me zoom in a little bit.
[00:18:25.600 --> 00:18:31.280]   So I am going to connect to the notebook.
[00:18:31.280 --> 00:18:34.280]   Feel free to open a link and try to follow along.
[00:18:34.280 --> 00:18:38.160]   I think this will be a really fun practice.
[00:18:38.160 --> 00:18:41.480]   So over here, there are just some preliminary introductions.
[00:18:41.480 --> 00:18:43.360]   Eventually, we want to put this notebook
[00:18:43.360 --> 00:18:45.920]   as part of the stable baselines documentation
[00:18:45.920 --> 00:18:47.720]   as well so that it's just a little easier
[00:18:47.720 --> 00:18:49.560]   for the adaptation.
[00:18:49.560 --> 00:18:52.920]   And then we're going to set up the environment
[00:18:52.920 --> 00:18:56.720]   by installing some dependencies.
[00:18:56.720 --> 00:19:00.760]   So I'm just going to click Run Anyway.
[00:19:00.760 --> 00:19:03.480]   And something you'll notice is that I'm installing
[00:19:03.480 --> 00:19:05.240]   the master branch of WMB.
[00:19:05.240 --> 00:19:09.160]   This is because I have since made some improvement
[00:19:09.160 --> 00:19:13.080]   over the stable baselines integration in our WMB package
[00:19:13.080 --> 00:19:17.680]   but the changes is not live yet.
[00:19:17.680 --> 00:19:21.080]   But we're pushing a release either today or tomorrow.
[00:19:21.080 --> 00:19:24.600]   So by tomorrow, you should be able to replace this
[00:19:24.600 --> 00:19:27.800]   with just WMB.
[00:19:27.800 --> 00:19:30.000]   So what this part of the code is setting up
[00:19:30.000 --> 00:19:33.280]   is installing WMB stable baselines 3
[00:19:33.280 --> 00:19:37.080]   and set up a virtual display so that we can record the videos
[00:19:37.080 --> 00:19:40.160]   and upload to the cloud.
[00:19:40.160 --> 00:19:43.960]   The next step is to basically run everything we have.
[00:19:43.960 --> 00:19:47.040]   I tried to make the code as simple as possible.
[00:19:47.040 --> 00:19:49.520]   So after this, there's not much left.
[00:19:49.520 --> 00:19:53.800]   So if you just run this cell, that
[00:19:53.800 --> 00:19:56.360]   will basically create a Weights and Biases run
[00:19:56.360 --> 00:19:59.680]   with all of the metrics.
[00:19:59.680 --> 00:20:01.560]   So this is warming up.
[00:20:06.720 --> 00:20:10.560]   And it's going to tell you to log in to Weights and Biases.
[00:20:10.560 --> 00:20:18.280]   So you can click on this URL to copy your API keys.
[00:20:18.280 --> 00:20:22.800]   So if you would like to follow along with this Colab Notebook,
[00:20:22.800 --> 00:20:25.480]   please register accounts.
[00:20:25.480 --> 00:20:28.160]   And then you can go to WMB.ai authorized
[00:20:28.160 --> 00:20:30.560]   and you'll be able to copy this API key.
[00:20:30.560 --> 00:20:38.760]   And then you can paste it here and it will log your account in
[00:20:38.760 --> 00:20:41.480]   so that all of your experiments will
[00:20:41.480 --> 00:20:46.160]   be tracked into your account.
[00:20:46.160 --> 00:20:49.120]   So I'll just hit Enter.
[00:20:49.120 --> 00:20:53.880]   But after this experiment is finished--
[00:20:53.880 --> 00:20:57.440]   oh, actually, while this experiment is running,
[00:20:57.440 --> 00:21:00.120]   feel free to continue the process
[00:21:00.120 --> 00:21:01.800]   if you want to follow along.
[00:21:01.800 --> 00:21:05.480]   Just register an account, paste the API key,
[00:21:05.480 --> 00:21:08.920]   and you'll be able to produce this experiment.
[00:21:08.920 --> 00:21:12.720]   And over here, this cell creates this run page
[00:21:12.720 --> 00:21:14.160]   that I can just click on.
[00:21:14.160 --> 00:21:22.920]   And it's going to have some basic stuff.
[00:21:22.920 --> 00:21:24.880]   And then as this experiment finishes,
[00:21:24.880 --> 00:21:26.080]   we're going to see more stuff.
[00:21:27.080 --> 00:21:31.680]   And we're only training the agents for roughly one
[00:21:31.680 --> 00:21:35.000]   to two minutes for 25,000 steps.
[00:21:35.000 --> 00:21:36.920]   So this should finish pretty soon.
[00:21:36.920 --> 00:21:51.960]   And a couple of details on the SAPL Baselines
[00:21:51.960 --> 00:21:52.760]   integration.
[00:21:52.760 --> 00:21:58.000]   And a couple of details on the SAPL Baselines integration
[00:21:58.000 --> 00:22:02.160]   as far as how it was implemented under the hood.
[00:22:02.160 --> 00:22:03.720]   Oh, there's some display.
[00:22:03.720 --> 00:22:10.480]   So basically, what's happening under the hood
[00:22:10.480 --> 00:22:14.480]   is SAPL Baselines 3 have TensorBoard integration.
[00:22:14.480 --> 00:22:20.160]   So if you toggle this TensorBoard log equals runs,
[00:22:20.160 --> 00:22:22.200]   it will automatically save the metrics
[00:22:22.200 --> 00:22:26.000]   with your experiments to the runs folder.
[00:22:26.000 --> 00:22:28.680]   And in Weights and Biases, we have this argument
[00:22:28.680 --> 00:22:30.640]   called syncTensorBoard equal to true.
[00:22:30.640 --> 00:22:32.560]   So it's a really straightforward integration
[00:22:32.560 --> 00:22:35.600]   that as soon as you turn on this argument,
[00:22:35.600 --> 00:22:40.760]   then all of your metrics is synced to the cloud.
[00:22:40.760 --> 00:22:44.160]   So now if we go back, this should finish.
[00:22:44.160 --> 00:22:46.680]   Yep, the program has ended successfully.
[00:22:46.680 --> 00:22:51.240]   It's just uploading the final metrics to the cloud.
[00:22:51.240 --> 00:22:54.440]   And now if we go back here, we refresh the page,
[00:22:54.440 --> 00:22:56.120]   we should see everything ready.
[00:22:56.120 --> 00:22:58.480]   So the first time you see this dashboard,
[00:22:58.480 --> 00:23:01.200]   it's going to have quite a few components.
[00:23:01.200 --> 00:23:03.640]   But I encourage you to play around with it.
[00:23:03.640 --> 00:23:07.480]   And in particular, the way I like to do it is to move this--
[00:23:07.480 --> 00:23:08.360]   oh, also, sorry.
[00:23:08.360 --> 00:23:11.440]   Let me zoom it in a little bit.
[00:23:11.440 --> 00:23:17.080]   The usual way I like to do it is to click on this section
[00:23:17.080 --> 00:23:19.680]   over here and sort of move it at top.
[00:23:19.680 --> 00:23:23.240]   And then I usually move the rollouts related stuff
[00:23:23.240 --> 00:23:24.200]   to the top as well.
[00:23:24.200 --> 00:23:29.920]   So there is a little bit of a customization, so to speak.
[00:23:29.920 --> 00:23:38.280]   And then this global types, a global step
[00:23:38.280 --> 00:23:39.800]   is not really that helpful.
[00:23:39.800 --> 00:23:41.760]   So I'll also remove it.
[00:23:41.760 --> 00:23:45.320]   And then I also move the video over here.
[00:23:45.320 --> 00:23:48.880]   So that's-- oh, there's also the FPS information
[00:23:48.880 --> 00:23:51.560]   that sometimes can be quite handy to help you measure
[00:23:51.560 --> 00:23:53.120]   the performance as well.
[00:23:53.120 --> 00:23:55.440]   But this is basically it.
[00:23:55.440 --> 00:23:58.800]   Over here, you can see the mean episodic return.
[00:23:58.800 --> 00:24:02.520]   And over here, you can see the mean episodic length
[00:24:02.520 --> 00:24:03.800]   with stable baselines.
[00:24:03.800 --> 00:24:08.760]   And over here is the video of the agents playing the game.
[00:24:08.760 --> 00:24:12.240]   And the reason why this dashboard is really handy
[00:24:12.240 --> 00:24:14.320]   because you can click on this button
[00:24:14.320 --> 00:24:18.760]   and see what the agent was doing at different stages
[00:24:18.760 --> 00:24:19.760]   of the training.
[00:24:19.760 --> 00:24:22.440]   We see that before--
[00:24:22.440 --> 00:24:24.560]   almost before any training at all,
[00:24:24.560 --> 00:24:29.200]   the card poll will fail almost immediately.
[00:24:29.200 --> 00:24:33.040]   Whereas if we move it to the middle,
[00:24:33.040 --> 00:24:35.160]   it will stay alive a little bit longer.
[00:24:35.160 --> 00:24:40.120]   But then it probably will fail, will fall over.
[00:24:40.120 --> 00:24:44.200]   And whereas in the end, it would almost balance a card poll
[00:24:44.200 --> 00:24:45.520]   almost perfectly.
[00:24:45.520 --> 00:24:51.080]   So for me, having this video panel is just a game changer
[00:24:51.080 --> 00:24:55.520]   because it really allows me to view all of my experiments
[00:24:55.520 --> 00:24:56.920]   in a very easy fashion.
[00:24:56.920 --> 00:24:59.960]   And I can see exactly what the agent is doing,
[00:24:59.960 --> 00:25:02.680]   which is really helpful for debugging purposes.
[00:25:02.680 --> 00:25:04.080]   And of course, over here, you can
[00:25:04.080 --> 00:25:08.400]   see all of the training losses, all of the metrics.
[00:25:08.400 --> 00:25:12.000]   You can also toggle this optional gradient logging
[00:25:12.000 --> 00:25:14.400]   that unfortunately is probably not
[00:25:14.400 --> 00:25:19.760]   very helpful to reinforcement learning models,
[00:25:19.760 --> 00:25:22.560]   but it's just so cool to see.
[00:25:22.560 --> 00:25:26.600]   And over here, you can play with a couple of buttons over here.
[00:25:26.600 --> 00:25:31.280]   You can tune with this global smoothing.
[00:25:31.280 --> 00:25:36.320]   And you can also tune the smoothing individually.
[00:25:36.320 --> 00:25:39.840]   Another thing is that by default,
[00:25:39.840 --> 00:25:43.760]   Weights and Biases has this x-axis being the step.
[00:25:43.760 --> 00:25:46.200]   But using the TensorBoard integration,
[00:25:46.200 --> 00:25:47.800]   the TensorBoard integration by default
[00:25:47.800 --> 00:25:52.320]   uses a global step as the x-axis.
[00:25:52.320 --> 00:25:59.160]   So here is when you actually see the 25k total number of time
[00:25:59.160 --> 00:26:01.000]   steps for the training.
[00:26:01.000 --> 00:26:03.040]   So this is just a training panel.
[00:26:03.040 --> 00:26:06.360]   And over here, you can see the system panels as well.
[00:26:06.360 --> 00:26:08.200]   So this is also really handy.
[00:26:08.200 --> 00:26:10.040]   It will show you all of the system metrics,
[00:26:10.040 --> 00:26:14.160]   such as CPU utilization, memory utilization, and so on.
[00:26:14.160 --> 00:26:18.040]   It's handy because sometimes you can see exactly
[00:26:18.040 --> 00:26:20.040]   how your memory blows up.
[00:26:20.040 --> 00:26:23.320]   So if you're training your agents using DQN
[00:26:23.320 --> 00:26:26.680]   with the 1 million samples replay buffer,
[00:26:26.680 --> 00:26:30.600]   you'll see your system memory just gradually go up.
[00:26:30.600 --> 00:26:33.000]   And then if your memory is limited,
[00:26:33.000 --> 00:26:36.520]   it will sort of blow up and throw an error to you.
[00:26:36.520 --> 00:26:38.440]   But this chart over here will tell you
[00:26:38.440 --> 00:26:40.320]   exactly how it was blowing up.
[00:26:40.320 --> 00:26:42.800]   Another really helpful debugging utility.
[00:26:42.800 --> 00:26:46.240]   And also, this GPU utilization is very helpful as well
[00:26:46.240 --> 00:26:49.640]   because it tells you exactly how well your algorithm is
[00:26:49.640 --> 00:26:50.480]   using GPU.
[00:26:50.480 --> 00:26:53.160]   And unfortunately, sort of the modern--
[00:26:53.160 --> 00:26:56.120]   most of the reinforcement learning algorithms today
[00:26:56.120 --> 00:26:58.920]   have a lot of components that's running on a CPU part.
[00:26:58.920 --> 00:27:02.200]   And you need to do constant transfer between the GPU--
[00:27:02.200 --> 00:27:04.960]   transferring data between the CPU and GPU devices.
[00:27:04.960 --> 00:27:10.120]   And as a result, the GPU is not really utilizing it that much.
[00:27:10.120 --> 00:27:12.840]   So for example, over here, you can see the GPU utilization
[00:27:12.840 --> 00:27:14.400]   is around 8.
[00:27:14.400 --> 00:27:17.760]   GPU, at least in my experience, usually really comes handy
[00:27:17.760 --> 00:27:19.880]   when you have experiments that uses
[00:27:19.880 --> 00:27:21.840]   convolutional neural networks.
[00:27:21.840 --> 00:27:26.360]   That's when you see the GPU utilization really goes up.
[00:27:26.360 --> 00:27:29.840]   You can see the GPU temperature and the power usage.
[00:27:29.840 --> 00:27:32.240]   All of those things are quite interesting
[00:27:32.240 --> 00:27:34.200]   if you want to check them out.
[00:27:34.200 --> 00:27:37.240]   We also keep the original TensorBoard files.
[00:27:37.240 --> 00:27:40.720]   And we keep the logs, the center output and center
[00:27:40.720 --> 00:27:42.560]   error of your algorithm.
[00:27:42.560 --> 00:27:46.000]   So if you want to log something using a print statement,
[00:27:46.000 --> 00:27:48.040]   you can purportedly do that as well.
[00:27:48.040 --> 00:27:50.520]   And over here, we have the Files tab
[00:27:50.520 --> 00:27:54.200]   that contains a lot of useful information as well.
[00:27:54.200 --> 00:27:56.560]   The first is this requirements subtext.
[00:27:56.560 --> 00:28:00.200]   By default, the Ways and Biases integration
[00:28:00.200 --> 00:28:04.360]   will automatically log all of your dependencies.
[00:28:04.360 --> 00:28:09.360]   Secondly, we also log this model.zip.
[00:28:09.360 --> 00:28:15.320]   If you-- oh.
[00:28:15.320 --> 00:28:16.920]   We'll also log that.
[00:28:16.920 --> 00:28:22.960]   If you specify this part of the code, if you say model safe
[00:28:22.960 --> 00:28:27.840]   path, so if you specify this to a particular folder,
[00:28:27.840 --> 00:28:31.000]   at the end of your training, the WMB callback
[00:28:31.000 --> 00:28:35.520]   will automatically upload your model to the cloud.
[00:28:35.520 --> 00:28:39.000]   So again, really helpful.
[00:28:39.000 --> 00:28:41.280]   And you can also see the videos over here
[00:28:41.280 --> 00:28:42.880]   if you want to download them as well.
[00:28:42.880 --> 00:28:50.280]   So I think this might be a good point for a pause
[00:28:50.280 --> 00:28:52.680]   to see if there's any questions.
[00:28:57.320 --> 00:29:03.920]   We did have one, which was already answered by Antonin.
[00:29:03.920 --> 00:29:07.680]   But I guess it might be relevant to other people too.
[00:29:07.680 --> 00:29:12.320]   So Antonin, if you could just read the answered question.
[00:29:12.320 --> 00:29:16.520]   So the question was about, does it
[00:29:16.520 --> 00:29:19.320]   work for recording video with different simulator
[00:29:19.320 --> 00:29:23.720]   during training, such as Gazebo and PackBuildit?
[00:29:23.720 --> 00:29:27.160]   So the answer to that is, all of the recording
[00:29:27.160 --> 00:29:31.400]   rely on your implementation of the gym environment,
[00:29:31.400 --> 00:29:34.000]   and especially on the render method.
[00:29:34.000 --> 00:29:38.360]   So if you render method return an image when
[00:29:38.360 --> 00:29:41.840]   called with the RGB array argument, then it will work.
[00:29:41.840 --> 00:29:44.800]   Otherwise, you will need to define a camera
[00:29:44.800 --> 00:29:50.040]   object in PackBuildit, and probably the same for Gazebo.
[00:29:50.040 --> 00:29:50.560]   Perfect.
[00:29:50.560 --> 00:29:51.960]   Thank you.
[00:29:51.960 --> 00:29:56.680]   And then we had another one from Chong.
[00:29:56.680 --> 00:29:59.280]   It came in a few minutes ago.
[00:29:59.280 --> 00:30:04.240]   And it says, I'm curious if the parameter sweep support object
[00:30:04.240 --> 00:30:07.320]   function and class instead of string and number.
[00:30:07.320 --> 00:30:17.280]   So I'm not sure if I understand the question.
[00:30:17.280 --> 00:30:22.080]   If the parameters sweep support, are you
[00:30:22.080 --> 00:30:26.960]   talking about if we can log function and classes
[00:30:26.960 --> 00:30:30.400]   in the config section of the run?
[00:30:30.400 --> 00:30:35.920]   So the short answer is no.
[00:30:35.920 --> 00:30:39.640]   I think you can only record strings.
[00:30:39.640 --> 00:30:43.200]   So by the way, thanks for mentioning that.
[00:30:43.200 --> 00:30:48.040]   Over here, we log every parameters
[00:30:48.040 --> 00:30:50.200]   that are related to the model class.
[00:30:50.200 --> 00:30:53.560]   So over here, you can see the action space.
[00:30:53.560 --> 00:30:56.680]   You can see the device that was on,
[00:30:56.680 --> 00:30:58.720]   the hyperparameters of the algorithm,
[00:30:58.720 --> 00:31:02.080]   such as number of epochs, a number of steps.
[00:31:02.080 --> 00:31:06.560]   But I don't believe you can record functions and classes,
[00:31:06.560 --> 00:31:11.160]   because you would have to pickle them, which you can perfectly
[00:31:11.160 --> 00:31:18.360]   do if you pickle them and then use WMB.save.
[00:31:18.360 --> 00:31:23.920]   It's going to upload those pickle files into this run.
[00:31:23.920 --> 00:31:25.680]   And then you can maybe have a logic
[00:31:25.680 --> 00:31:27.880]   to load them later if you want.
[00:31:27.880 --> 00:31:33.040]   Yeah, so I'll circle back to this
[00:31:33.040 --> 00:31:36.920]   if you have follow-up questions.
[00:31:36.920 --> 00:31:44.000]   So following up on the video recording, for the most part,
[00:31:44.000 --> 00:31:45.480]   the answer is yes.
[00:31:45.480 --> 00:31:48.360]   If you have the arcade environments,
[00:31:48.360 --> 00:31:51.600]   such as the Atari environments, then
[00:31:51.600 --> 00:31:54.680]   it's automatically going to record the videos for you,
[00:31:54.680 --> 00:31:57.120]   versus if you have Pi Bullets, again,
[00:31:57.120 --> 00:31:59.320]   it's also going to record the videos for you.
[00:31:59.320 --> 00:32:05.760]   So before diving into this, I think
[00:32:05.760 --> 00:32:10.200]   we're good with the question and answering.
[00:32:10.200 --> 00:32:14.600]   And then the next step for me, I think,
[00:32:14.600 --> 00:32:19.760]   is to show you some of the ways you can manage experiments
[00:32:19.760 --> 00:32:23.080]   and really do a lot of analysis with the experiments.
[00:32:23.080 --> 00:32:26.920]   And in a sense, that's really where some biases shine,
[00:32:26.920 --> 00:32:30.520]   because I've seen a lot of software,
[00:32:30.520 --> 00:32:32.240]   they really record the experiments well.
[00:32:32.240 --> 00:32:34.560]   They record all of the hyperparameters, the metrics
[00:32:34.560 --> 00:32:36.000]   really well.
[00:32:36.000 --> 00:32:38.400]   But when you analyze them, it becomes a little bit
[00:32:38.400 --> 00:32:40.360]   inflexible.
[00:32:40.360 --> 00:32:43.160]   So I'm going to try to do that, show you that.
[00:32:43.160 --> 00:32:46.960]   So let me clear everything.
[00:32:46.960 --> 00:32:49.200]   So I'm going to set this--
[00:32:49.200 --> 00:32:50.920]   I'm going to clear my workspace.
[00:32:50.920 --> 00:32:53.920]   So this is, if you go to your project,
[00:32:53.920 --> 00:32:55.840]   this is essentially what you're going to see.
[00:32:55.840 --> 00:32:57.560]   You're going to see a lot of runs.
[00:32:57.560 --> 00:33:01.400]   But maybe this is not immediately enlightening
[00:33:01.400 --> 00:33:02.560]   to you.
[00:33:02.560 --> 00:33:04.880]   That's where these filters and groups
[00:33:04.880 --> 00:33:06.920]   features really come in handy.
[00:33:06.920 --> 00:33:11.160]   So in particular, if I say, I want to group things
[00:33:11.160 --> 00:33:13.880]   by the algorithm--
[00:33:13.880 --> 00:33:19.440]   actually, sorry, I want to group things by the environment name.
[00:33:19.440 --> 00:33:23.200]   So immediately, I see there are three environments,
[00:33:23.200 --> 00:33:27.200]   the carpool, which is basically what we demoed.
[00:33:27.200 --> 00:33:31.240]   There's this ant bulletin, and there's breakouts.
[00:33:31.240 --> 00:33:36.440]   And there was these runs that are not really
[00:33:36.440 --> 00:33:38.800]   going to be helpful for us, but we don't want to see it.
[00:33:38.800 --> 00:33:43.960]   So we can use a filter to say, I want my end thing
[00:33:43.960 --> 00:33:47.600]   to not equal to null.
[00:33:47.600 --> 00:33:49.520]   So it just kind of eradicated that.
[00:33:49.520 --> 00:33:51.880]   And then if we see things on the right,
[00:33:51.880 --> 00:33:55.440]   it's going to show me a bunch of videos and rollouts.
[00:33:55.440 --> 00:33:59.040]   But these are going to--
[00:33:59.040 --> 00:34:02.840]   it's going to combine every game, the data from all three
[00:34:02.840 --> 00:34:03.320]   games.
[00:34:03.320 --> 00:34:06.360]   So what I can do is to toggle this I button
[00:34:06.360 --> 00:34:09.400]   so that I can see all of the runs that
[00:34:09.400 --> 00:34:12.880]   are specific to breakouts.
[00:34:12.880 --> 00:34:17.280]   So over here, I see four runs.
[00:34:17.280 --> 00:34:22.080]   Sorry, I also need to add a filter to say, I'll go--
[00:34:22.080 --> 00:34:23.000]   no, I could run.
[00:34:23.000 --> 00:34:26.240]   This is just to purge the experiments that I'm not
[00:34:26.240 --> 00:34:28.240]   really interested in.
[00:34:28.240 --> 00:34:29.880]   So if you start a new run like this,
[00:34:29.880 --> 00:34:32.600]   you shouldn't have to do it.
[00:34:32.600 --> 00:34:36.440]   Yeah, so this really showed me all of the interesting pieces
[00:34:36.440 --> 00:34:37.360]   I would like to see.
[00:34:37.360 --> 00:34:40.320]   And then I can say, using this slider,
[00:34:40.320 --> 00:34:44.760]   I can see that, oh, in the beginning of the game--
[00:34:44.760 --> 00:34:47.400]   this is where I zoom in a little bit.
[00:34:47.400 --> 00:34:48.800]   The agent isn't performing well.
[00:34:48.800 --> 00:34:52.760]   You see eight Atari games playing on parallel.
[00:34:52.760 --> 00:34:57.360]   That's because there are eight parallel environments that
[00:34:57.360 --> 00:34:58.640]   was used to train the agent.
[00:34:58.640 --> 00:35:00.320]   And as you can see in the beginning,
[00:35:00.320 --> 00:35:01.960]   they don't perform very well.
[00:35:01.960 --> 00:35:03.560]   But then you can use the slider again
[00:35:03.560 --> 00:35:05.880]   to see how they perform in the end.
[00:35:05.880 --> 00:35:09.440]   And I think our agent performs very well.
[00:35:09.440 --> 00:35:11.800]   And it might have, in this instance,
[00:35:11.800 --> 00:35:13.720]   broke the Atari simulator.
[00:35:13.720 --> 00:35:16.600]   I have never seen this before.
[00:35:16.600 --> 00:35:19.440]   But I think this is like a waiting screen
[00:35:19.440 --> 00:35:22.200]   after talking to Anzi.
[00:35:22.200 --> 00:35:26.160]   But super interesting stuff.
[00:35:26.160 --> 00:35:28.720]   And similarly, I can see the experiments
[00:35:28.720 --> 00:35:33.160]   with the high bullet environment.
[00:35:33.160 --> 00:35:35.560]   And over here, I see a bunch of experiments.
[00:35:35.560 --> 00:35:38.280]   But this is actually--
[00:35:38.280 --> 00:35:46.880]   so let me remove a few things real quick just to make
[00:35:46.880 --> 00:35:48.520]   seeing things a little easier.
[00:35:48.520 --> 00:35:51.640]   And you can definitely make this whole process--
[00:35:51.640 --> 00:35:54.280]   so in the dashboard, rerecord your settings.
[00:35:54.280 --> 00:35:57.640]   So once you do this again, all of the settings will be saved.
[00:35:57.640 --> 00:36:01.400]   And in my case, my personal workspace,
[00:36:01.400 --> 00:36:04.680]   I don't need to rearrange the dashboard every time.
[00:36:04.680 --> 00:36:08.240]   It's just the first time I need to do it.
[00:36:08.240 --> 00:36:10.000]   So there are actually two algorithms.
[00:36:10.000 --> 00:36:15.080]   There are TD3-- there are four algorithms
[00:36:15.080 --> 00:36:18.600]   that I run under the Antz bullet and environments.
[00:36:18.600 --> 00:36:21.080]   Two of them is from PPL.
[00:36:21.080 --> 00:36:22.960]   Two of them is from TD3.
[00:36:22.960 --> 00:36:25.600]   And the way I can see it is to say,
[00:36:25.600 --> 00:36:28.280]   I want to also group things by algorithm.
[00:36:28.280 --> 00:36:33.760]   So if I click on this again, I can
[00:36:33.760 --> 00:36:38.960]   see there are two runs from TD3 and two runs from PPL.
[00:36:38.960 --> 00:36:42.920]   And this is where things get very interesting.
[00:36:42.920 --> 00:36:47.360]   So if I, again, visualize things using the global step,
[00:36:47.360 --> 00:36:52.080]   I can see that both TD3 algorithm and PPL algorithm
[00:36:52.080 --> 00:36:56.840]   are trained agents for 1 million time steps.
[00:36:56.840 --> 00:37:01.600]   But you see for TD3, which is this green curve,
[00:37:01.600 --> 00:37:04.080]   the arrow bar is much larger.
[00:37:04.080 --> 00:37:05.560]   And why is that?
[00:37:05.560 --> 00:37:08.240]   So if I further group things again,
[00:37:08.240 --> 00:37:13.760]   if I say I want to group things by name,
[00:37:13.760 --> 00:37:16.200]   then it really gives me a lot of insight.
[00:37:16.200 --> 00:37:21.040]   That is because the first TD3 run, which is the green run,
[00:37:21.040 --> 00:37:22.920]   performs really well.
[00:37:22.920 --> 00:37:27.960]   It achieves about 3,000 episodic return,
[00:37:27.960 --> 00:37:35.040]   whereas the second TD3 run, which is this dandy cherry,
[00:37:35.040 --> 00:37:36.520]   performs much worse.
[00:37:36.520 --> 00:37:40.320]   That's why if I group things by name,
[00:37:40.320 --> 00:37:43.120]   I see the algorithm's performance in aggregates.
[00:37:43.120 --> 00:37:44.360]   I see the arrow bars.
[00:37:44.360 --> 00:37:48.160]   And then I know that TD3's performance, at least
[00:37:48.160 --> 00:37:51.400]   in this limited settings, its performance
[00:37:51.400 --> 00:37:54.000]   is a little bit unstable.
[00:37:54.000 --> 00:37:57.960]   And I can smooth things however I like it.
[00:37:57.960 --> 00:38:04.680]   And then you can find a source code for doing this.
[00:38:04.680 --> 00:38:07.120]   So eventually, I'll make these code
[00:38:07.120 --> 00:38:09.840]   into the stable base science integration,
[00:38:09.840 --> 00:38:13.280]   stable base science documentation,
[00:38:13.280 --> 00:38:15.560]   or in the biases documentation.
[00:38:15.560 --> 00:38:18.120]   But it's really just simple code.
[00:38:18.120 --> 00:38:24.920]   I have this code produce these Atari experiments, where
[00:38:24.920 --> 00:38:28.480]   I basically import PPL, set up the hyperparameters that
[00:38:28.480 --> 00:38:32.000]   was specific to the Atari environments,
[00:38:32.000 --> 00:38:37.800]   and then call the model.learn using the WMB callback.
[00:38:37.800 --> 00:38:41.680]   All of a sudden, it gives me the video recording and then
[00:38:41.680 --> 00:38:43.520]   the episodic return curves.
[00:38:43.520 --> 00:38:47.360]   And similar thing with the PyBulletMs.
[00:38:47.360 --> 00:38:48.480]   It's really straightforward.
[00:38:48.480 --> 00:38:54.680]   So let me post this into the chat real quick,
[00:38:54.680 --> 00:38:56.600]   in case you want to check it out.
[00:38:56.600 --> 00:39:10.360]   So that is basically for the experiment analysis part.
[00:39:10.360 --> 00:39:13.600]   Just trying to showcase you some basic tools,
[00:39:13.600 --> 00:39:15.680]   such as the filtering and the group tools,
[00:39:15.680 --> 00:39:19.120]   that's really helpful for doing experiment analysis.
[00:39:19.120 --> 00:39:22.160]   And of course, this report allows you
[00:39:22.160 --> 00:39:23.920]   to add a lot of things to it.
[00:39:23.920 --> 00:39:26.840]   So if you want to write some notes about your experiments,
[00:39:26.840 --> 00:39:29.440]   you perfectly can.
[00:39:29.440 --> 00:39:33.640]   So if I want to add something over here,
[00:39:33.640 --> 00:39:40.800]   I would just say this is PyBulletMs.
[00:39:40.800 --> 00:39:45.560]   And then I can add a rich set of medias over here.
[00:39:45.560 --> 00:39:50.520]   So it's really flexible to help you edit a lot of stuff.
[00:39:50.520 --> 00:39:53.680]   And in the past, for my personal research,
[00:39:53.680 --> 00:39:56.920]   I use Weights and Biases extensively.
[00:39:56.920 --> 00:39:59.200]   I almost built my entire--
[00:39:59.200 --> 00:40:01.240]   I built my research infrastructure
[00:40:01.240 --> 00:40:02.800]   using Weights and Biases.
[00:40:02.800 --> 00:40:07.120]   And so with my research with real-time strategy games,
[00:40:07.120 --> 00:40:10.920]   you can see this report that almost looks like a paper,
[00:40:10.920 --> 00:40:15.520]   that you can find all of the videos of agents playing
[00:40:15.520 --> 00:40:18.040]   a game.
[00:40:18.040 --> 00:40:22.120]   Overall, just offers a really rich visualization.
[00:40:22.120 --> 00:40:27.160]   And I can play around with it, zoom in a little bit.
[00:40:27.160 --> 00:40:29.200]   So this goes back to what I was saying.
[00:40:29.200 --> 00:40:32.280]   If I see a number over here, I know how to reproduce it.
[00:40:32.280 --> 00:40:38.400]   Because if I see that these numbers over here,
[00:40:38.400 --> 00:40:41.160]   for example, if I see this purple run is performing
[00:40:41.160 --> 00:40:43.560]   really well, then all I need to do
[00:40:43.560 --> 00:40:48.360]   is to find the corresponding experiments,
[00:40:48.360 --> 00:40:52.080]   and then simply open it.
[00:40:52.080 --> 00:40:55.840]   And I can see, oh, the command that was used to produce it,
[00:40:55.840 --> 00:40:59.960]   the hyperparameters, and then the requirements.txt as well.
[00:40:59.960 --> 00:41:02.600]   So all of these are readily available to me.
[00:41:02.600 --> 00:41:08.800]   So I'm going to pause another second.
[00:41:08.800 --> 00:41:13.720]   So does anyone have any questions?
[00:41:13.720 --> 00:41:14.640]   Let me see.
[00:41:14.640 --> 00:41:28.680]   There was another one that Antoine already took in the chat.
[00:41:28.680 --> 00:41:31.000]   And the question was, is it possible to have
[00:41:31.000 --> 00:41:33.160]   two or more cameras per pie bullet
[00:41:33.160 --> 00:41:37.160]   and for visualizations, for example,
[00:41:37.160 --> 00:41:41.320]   agent's first person view and another camera hovering
[00:41:41.320 --> 00:41:42.400]   on top of the scene?
[00:41:42.400 --> 00:41:46.720]   Yeah, that's a good question.
[00:41:46.720 --> 00:41:52.160]   I think Antoine has already provided a great answer.
[00:41:52.160 --> 00:41:56.880]   So if anything, I'm going to try to support that,
[00:41:56.880 --> 00:42:00.360]   is if you implement your render function in a way
[00:42:00.360 --> 00:42:05.840]   that you concatenate the images of both angles in pie bullet,
[00:42:05.840 --> 00:42:08.720]   then essentially what's going to happen
[00:42:08.720 --> 00:42:16.160]   is you're going to have videos that look like this, where
[00:42:16.160 --> 00:42:19.480]   this image is going to be the first angle of your pie bullet
[00:42:19.480 --> 00:42:22.040]   F, and this is going to be the second angle.
[00:42:22.040 --> 00:42:24.680]   So as long as you concatenate those images,
[00:42:24.680 --> 00:42:25.840]   you can perfectly do so.
[00:42:25.840 --> 00:42:33.600]   Any further questions?
[00:42:33.600 --> 00:42:36.800]   Cool.
[00:42:36.800 --> 00:42:51.840]   So for the most part, this is everything I have.
[00:42:51.840 --> 00:42:55.920]   I hope it's useful to you.
[00:42:55.920 --> 00:42:59.480]   I hope it will improve your workflow.
[00:42:59.480 --> 00:43:03.640]   So I think at this point, it's just really chatting.
[00:43:03.640 --> 00:43:05.760]   Oh, yeah.
[00:43:05.760 --> 00:43:07.680]   Forgot I have this one more thing.
[00:43:07.680 --> 00:43:14.920]   So I was going to try to demo how you can visualize
[00:43:14.920 --> 00:43:18.440]   the parameters, sort of another use case for the grouping
[00:43:18.440 --> 00:43:18.960]   feature.
[00:43:18.960 --> 00:43:24.000]   So if I run my experiment with different number of steps,
[00:43:24.000 --> 00:43:28.800]   how do I visualize their impact on the performance
[00:43:28.800 --> 00:43:29.680]   of the agents?
[00:43:29.680 --> 00:43:32.480]   And the answer is really just I group things
[00:43:32.480 --> 00:43:35.920]   by the environment name, and I group things by the end steps.
[00:43:35.920 --> 00:43:41.760]   Then I see under number of times end steps, 512.
[00:43:41.760 --> 00:43:43.520]   Then we're going to see two runs.
[00:43:43.520 --> 00:43:49.240]   So you probably want to run at minimum two runs
[00:43:49.240 --> 00:43:53.320]   so that you can see some error bars so that your experiments
[00:43:53.320 --> 00:43:56.400]   works in general, not just that you got the lucky seed.
[00:43:56.400 --> 00:44:02.160]   And we can go into these to check out the source code.
[00:44:02.160 --> 00:44:04.640]   And again, it's just pretty straightforward.
[00:44:04.640 --> 00:44:08.560]   I just modify this end steps to be 512.
[00:44:08.560 --> 00:44:12.640]   And in separate runs, I just modify them to 256, et cetera.
[00:44:12.960 --> 00:44:18.960]   So all of these will be, in my opinion,
[00:44:18.960 --> 00:44:22.160]   really helps you to analyze the experiments
[00:44:22.160 --> 00:44:25.360]   and just a really great front end tool
[00:44:25.360 --> 00:44:30.720]   to help you extract insights of your experiments very easily.
[00:44:30.720 --> 00:44:37.720]   And of course, another thing that I forgot to mention
[00:44:37.720 --> 00:44:42.160]   is we log all of the models to the run.
[00:44:42.160 --> 00:44:45.840]   So you can either download those models
[00:44:45.840 --> 00:44:49.440]   and manually load it to your code if you like.
[00:44:49.440 --> 00:44:54.440]   But we also provide APIs for you to basically pull
[00:44:54.440 --> 00:44:55.560]   every data you want.
[00:44:55.560 --> 00:44:59.600]   So you can use our API to download every data is
[00:44:59.600 --> 00:45:02.560]   under the charts.
[00:45:02.560 --> 00:45:05.480]   But you can also use the API to automatically locate
[00:45:05.480 --> 00:45:09.040]   this model.zip and just load it to your code.
[00:45:09.040 --> 00:45:13.800]   So it's really flexible and all of your data is always yours.
[00:45:13.800 --> 00:45:15.040]   We just kind of store them.
[00:45:15.040 --> 00:45:21.560]   So yeah, that's basically it.
[00:45:21.560 --> 00:45:26.000]   Feel free to join our Slack channel, follow us on Twitter,
[00:45:26.000 --> 00:45:30.440]   or follow our YouTube channel if you like.
[00:45:30.440 --> 00:45:32.960]   And this is basically--
[00:45:32.960 --> 00:45:34.000]   any questions?
[00:45:34.000 --> 00:45:36.360]   I'll keep the slides so that you can see the links.
[00:45:36.360 --> 00:45:39.800]   [AUDIO OUT]
[00:45:39.800 --> 00:45:58.480]   Thank you so much for coming to the webinar.
[00:45:58.480 --> 00:46:02.280]   And thank you so much for the kind words.
[00:46:03.280 --> 00:46:06.760]   [AUDIO OUT]
[00:46:06.760 --> 00:46:17.400]   Yeah, so let me see.
[00:46:17.400 --> 00:46:20.880]   How much time do we have left?
[00:46:20.880 --> 00:46:23.960]   So I think we have a few minutes left.
[00:46:23.960 --> 00:46:27.160]   And it's sort of open for questions, of course.
[00:46:27.160 --> 00:46:30.600]   But probably we could go for a chat.
[00:46:30.600 --> 00:46:34.560]   If anyone has any questions for our guests,
[00:46:34.560 --> 00:46:37.280]   also feel free to post in a chat.
[00:46:37.280 --> 00:46:44.680]   Antonio and Anzi are very busy.
[00:46:44.680 --> 00:46:47.160]   So this might be a good chance if you have any questions
[00:46:47.160 --> 00:46:50.120]   about stable baselines.
[00:46:50.120 --> 00:46:52.440]   I personally have one.
[00:46:52.440 --> 00:46:57.160]   I was wondering about roadmap.
[00:46:57.160 --> 00:47:02.200]   I know, Antonio, you're working on using
[00:47:02.200 --> 00:47:06.240]   DQN with vectorized environments.
[00:47:06.240 --> 00:47:13.520]   What are your recent thoughts on it and progress?
[00:47:13.520 --> 00:47:16.680]   Yes, so the question is about using multiple environments
[00:47:16.680 --> 00:47:22.440]   to train our policy algorithm like DQN or Softact to Critic.
[00:47:22.440 --> 00:47:24.840]   So I've got a minimal working version,
[00:47:24.840 --> 00:47:28.600]   I would say, that is already online in a separate branch.
[00:47:28.600 --> 00:47:31.920]   And I'm, in fact, currently using it for a European project.
[00:47:31.920 --> 00:47:40.240]   So it works quite well and a lot to collect more data much
[00:47:40.240 --> 00:47:41.720]   faster.
[00:47:41.720 --> 00:47:43.640]   But it needs some tuning compared
[00:47:43.640 --> 00:47:47.680]   to classic hyperparameters.
[00:47:47.680 --> 00:47:49.520]   That's mostly the main issue.
[00:47:49.520 --> 00:47:51.240]   And the other issue is that you will also
[00:47:51.240 --> 00:47:53.560]   lose some sample efficiency.
[00:47:53.560 --> 00:47:54.560]   You will explore more.
[00:47:54.560 --> 00:47:56.360]   You will collect a sample faster.
[00:47:56.360 --> 00:47:58.920]   But you will also lose some sample efficiency.
[00:47:58.920 --> 00:48:05.960]   And currently, the version is only tested for the use case
[00:48:05.960 --> 00:48:06.520]   I'm doing.
[00:48:06.520 --> 00:48:09.480]   So mostly Softact to Critic and TQC,
[00:48:09.480 --> 00:48:14.160]   which is truncated quantum critics in the contrib folder,
[00:48:14.160 --> 00:48:15.880]   contrib repo.
[00:48:15.880 --> 00:48:19.200]   And I know also it doesn't work with our recent feature, which
[00:48:19.200 --> 00:48:22.120]   is supporting any type of observation,
[00:48:22.120 --> 00:48:23.800]   especially the dictionary observation.
[00:48:23.800 --> 00:48:25.320]   This doesn't work yet.
[00:48:25.320 --> 00:48:29.160]   But I plan to work on making that feature proper
[00:48:29.160 --> 00:48:31.640]   in September because before I would be on holidays
[00:48:31.640 --> 00:48:35.080]   and I won't have any time for that.
[00:48:35.080 --> 00:48:37.960]   So that's mostly it.
[00:48:37.960 --> 00:48:40.440]   So it's a nice feature.
[00:48:40.440 --> 00:48:41.320]   It works well.
[00:48:41.320 --> 00:48:45.800]   But it needs some polishing and it needs some tuning.
[00:48:45.800 --> 00:48:49.200]   Yeah, I'd love to see eventually a DKM works
[00:48:49.200 --> 00:48:52.960]   with a vectorized environment just because PPO works
[00:48:52.960 --> 00:48:54.360]   with a vectorized environment.
[00:48:54.360 --> 00:49:00.400]   And it's throughput and it's relatively fast.
[00:49:00.400 --> 00:49:03.240]   And it's still easy to debug because your code
[00:49:03.240 --> 00:49:04.920]   is mostly synchronous.
[00:49:04.920 --> 00:49:07.160]   Whereas I think in the DKM world,
[00:49:07.160 --> 00:49:11.800]   you have Apex DQM that although runs much faster,
[00:49:11.800 --> 00:49:16.440]   I feel the asynchronous structure, at least to me,
[00:49:16.440 --> 00:49:18.080]   it's harder to grasp.
[00:49:18.080 --> 00:49:20.080]   Yes.
[00:49:20.080 --> 00:49:25.400]   Yeah, and the thing is this was a design choice
[00:49:25.400 --> 00:49:27.640]   that I made a while ago that everything
[00:49:27.640 --> 00:49:30.040]   was a vectorized environment because I had that, in fact,
[00:49:30.040 --> 00:49:33.320]   in mind to allow to have a synchronous environment
[00:49:33.320 --> 00:49:36.800]   at the end for any algorithms.
[00:49:36.800 --> 00:49:41.240]   But first, we wanted to have the baseline working and ready
[00:49:41.240 --> 00:49:46.200]   and not make things too complicated already.
[00:49:46.200 --> 00:49:47.040]   Yeah, makes sense.
[00:49:47.040 --> 00:49:48.080]   Makes sense.
[00:49:48.080 --> 00:49:49.080]   Looking forward to it.
[00:49:49.080 --> 00:50:00.920]   Sort of on the related notes, have you two seen
[00:50:00.920 --> 00:50:05.280]   the recent implementation work from Dominic?
[00:50:05.280 --> 00:50:08.640]   It's the reproduction of the DQM work, reproduction
[00:50:08.640 --> 00:50:09.760]   of the rainbow DQM.
[00:50:09.760 --> 00:50:14.520]   Yeah, I thought it was kind of related
[00:50:14.520 --> 00:50:17.320]   and super cool at the same time.
[00:50:17.320 --> 00:50:21.120]   So for those of you who haven't seen it,
[00:50:21.120 --> 00:50:25.000]   this is a work that I personally felt really excited about.
[00:50:25.000 --> 00:50:26.800]   Let me see if I can find it real quick.
[00:50:26.800 --> 00:50:32.240]   Yeah, I'll post it in the chat.
[00:50:32.240 --> 00:50:35.640]   So this is a work from Dominic.
[00:50:35.640 --> 00:50:39.960]   And if you read his paper, he basically
[00:50:39.960 --> 00:50:45.080]   was able to train agents using rainbow,
[00:50:45.080 --> 00:50:50.640]   using a fraction of the original rainbow's paper's cost
[00:50:50.640 --> 00:50:54.120]   while still maintaining a relatively good performance.
[00:50:54.120 --> 00:50:57.200]   So for me, that was really exciting.
[00:50:57.200 --> 00:51:01.720]   This is kind of the final result that they have.
[00:51:01.720 --> 00:51:05.440]   So you can see that Dominic's implementation only
[00:51:05.440 --> 00:51:10.480]   runs for 10 million steps, yet it really performs fairly
[00:51:10.480 --> 00:51:11.680]   strongly as well.
[00:51:11.680 --> 00:51:17.440]   Whereas other repositories usually run for longer,
[00:51:17.440 --> 00:51:20.280]   although they also perform really well as well.
[00:51:20.280 --> 00:51:26.360]   Let me post this in the chat as well.
[00:51:26.360 --> 00:51:29.400]   There's another question, which is, in fact, quite
[00:51:29.400 --> 00:51:31.360]   interesting to answer.
[00:51:31.360 --> 00:51:34.880]   The question is, is SB3 tightly connected to my research
[00:51:34.880 --> 00:51:39.640]   position, or is it, for the most part, a personal project?
[00:51:39.640 --> 00:51:43.920]   So it started when we started the first version.
[00:51:43.920 --> 00:51:46.640]   It was quite connected to what I was doing,
[00:51:46.640 --> 00:51:50.880]   but then it became a personal project
[00:51:50.880 --> 00:51:55.560]   as it was done in another lab.
[00:51:55.560 --> 00:51:58.840]   And then for the stable Baseline 3,
[00:51:58.840 --> 00:52:02.040]   I got the chance that my research institute
[00:52:02.040 --> 00:52:04.800]   allowed me also to work on that during my work hours.
[00:52:04.800 --> 00:52:10.000]   So now it's also connected to my research position
[00:52:10.000 --> 00:52:15.600]   because I use it for all the research I'm doing.
[00:52:15.600 --> 00:52:19.120]   But at the same time, if I was only
[00:52:19.120 --> 00:52:21.320]   working on it during my working hours,
[00:52:21.320 --> 00:52:24.720]   I couldn't do everything or it would still miss some time.
[00:52:24.720 --> 00:52:29.640]   So I'm also considering it as a personal project
[00:52:29.640 --> 00:52:33.800]   and working on it at other times.
[00:52:33.800 --> 00:52:37.560]   So it's mostly connected to my research position,
[00:52:37.560 --> 00:52:40.360]   but it's also part of a personal project.
[00:52:40.360 --> 00:52:45.160]   And for NC, I will let him answer.
[00:52:45.160 --> 00:52:47.320]   Yeah, for me, it's half and half.
[00:52:47.320 --> 00:52:48.720]   So I use it for my research.
[00:52:48.720 --> 00:52:51.320]   But lately, it has mostly been a personal project.
[00:52:51.320 --> 00:52:53.520]   I like to help people around answering questions.
[00:52:53.520 --> 00:52:54.840]   So I'll phone and doesn't have to.
[00:52:54.840 --> 00:53:03.000]   Yeah, my hat's really off to you two
[00:53:03.000 --> 00:53:06.640]   because I really enjoy using stable baselines, especially
[00:53:06.640 --> 00:53:10.760]   some of the utility tools for my personal research.
[00:53:10.760 --> 00:53:12.640]   It's like I don't need to maintain them.
[00:53:12.640 --> 00:53:14.880]   I just kind of import stable baselines
[00:53:14.880 --> 00:53:18.120]   and just use the common tools such as the vectorized
[00:53:18.120 --> 00:53:20.600]   environments, the Atari wrappers.
[00:53:20.600 --> 00:53:25.960]   It's just been a joy to work with.
[00:53:25.960 --> 00:53:29.760]   Yeah, and the check environment is also quite used by people
[00:53:29.760 --> 00:53:32.520]   because we had so much issues that at the end,
[00:53:32.520 --> 00:53:35.280]   we decided to write directly a code
[00:53:35.280 --> 00:53:40.120]   to check people's custom environments to avoid answering
[00:53:40.120 --> 00:53:43.920]   the same question again and again.
[00:53:43.920 --> 00:53:46.440]   Yeah, the vectorized environments, I don't know.
[00:53:46.440 --> 00:53:49.560]   I feel like probably OpenAI should have included
[00:53:49.560 --> 00:53:53.480]   in the OpenAI gem repository a long time ago.
[00:53:53.480 --> 00:53:57.320]   There is an implementation, but I wouldn't rely on it.
[00:54:00.040 --> 00:54:02.720]   Yeah, and it has different APIs as well.
[00:54:02.720 --> 00:54:03.400]   So I don't know.
[00:54:03.400 --> 00:54:04.840]   Sometimes it's tricky.
[00:54:04.840 --> 00:54:13.760]   How many PRs do you guys review every day?
[00:54:13.760 --> 00:54:21.120]   So pull requests, not that much.
[00:54:21.120 --> 00:54:23.240]   What we usually have is sometimes
[00:54:23.240 --> 00:54:24.680]   either very small pull requests.
[00:54:24.680 --> 00:54:28.600]   We got almost one issue per day at least,
[00:54:28.600 --> 00:54:31.640]   or 50 issue per month, more or less.
[00:54:31.640 --> 00:54:34.240]   We got more issues than pull requests.
[00:54:34.240 --> 00:54:35.680]   And for pull requests, it depends
[00:54:35.680 --> 00:54:38.360]   because sometimes we just get big pull
[00:54:38.360 --> 00:54:40.680]   requests that last three months.
[00:54:40.680 --> 00:54:42.960]   And this already take quite some time
[00:54:42.960 --> 00:54:47.120]   to ensure the quality, like the Dict observation
[00:54:47.120 --> 00:54:47.880]   implementation.
[00:54:47.880 --> 00:54:51.680]   It took three to four months to fully--
[00:54:51.680 --> 00:54:53.720]   between the first working version
[00:54:53.720 --> 00:54:58.680]   to the completely completed version,
[00:54:58.680 --> 00:55:00.320]   yeah, it took us some time.
[00:55:00.320 --> 00:55:03.000]   So it depends, but it's mostly issues.
[00:55:03.000 --> 00:55:06.560]   Or big pull requests.
[00:55:06.560 --> 00:55:09.800]   Nice.
[00:55:09.800 --> 00:55:12.080]   I guess we can wrap up here.
[00:55:12.080 --> 00:55:17.080]   I want to say thanks again to Costa for running the webinar.
[00:55:17.080 --> 00:55:20.400]   Thanks to all the great feedback in the chat,
[00:55:20.400 --> 00:55:25.760]   and special thanks to our guests today for taking the time
[00:55:25.760 --> 00:55:27.960]   to talk about your work.
[00:55:27.960 --> 00:55:29.960]   The recording and all the materials
[00:55:29.960 --> 00:55:31.960]   will be shared in a follow-up email,
[00:55:31.960 --> 00:55:36.440]   so be sure to check your inbox in the coming days.
[00:55:36.440 --> 00:55:39.720]   And hope to see you around in the WNB community.
[00:55:39.720 --> 00:55:40.640]   Take care, everyone.
[00:55:40.640 --> 00:55:41.140]   Bye.
[00:55:41.140 --> 00:55:44.480]   [MUSIC PLAYING]
[00:55:44.480 --> 00:55:47.560]   (chill modern music)
[00:55:47.560 --> 00:55:50.640]   (chill modern music)
[00:55:50.640 --> 00:55:53.140]   (chill music)

