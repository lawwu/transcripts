
[00:00:00.000 --> 00:00:01.400]   I'm gonna take...
[00:00:01.400 --> 00:00:10.360]   Cool.
[00:00:10.360 --> 00:00:13.800]   Hey everybody, and welcome back to week 12
[00:00:13.800 --> 00:00:15.260]   of Fast Book Reading Group.
[00:00:15.260 --> 00:00:19.400]   So I think looking back, it's actually almost three months,
[00:00:19.400 --> 00:00:22.120]   and it's so great to see that we've been at this
[00:00:22.120 --> 00:00:25.640]   for three months now, or 12 weeks.
[00:00:25.640 --> 00:00:27.320]   It's almost three months, but 12 weeks,
[00:00:27.320 --> 00:00:29.160]   and we've been doing this week after week,
[00:00:29.160 --> 00:00:30.520]   and we've been coming back.
[00:00:30.520 --> 00:00:33.300]   So I thought before maybe getting started today,
[00:00:33.300 --> 00:00:36.000]   it might just as well be helpful to just have a look
[00:00:36.000 --> 00:00:37.740]   at what all we've covered.
[00:00:37.740 --> 00:00:41.240]   So if I go to Fast Book, this is the repository,
[00:00:41.240 --> 00:00:44.680]   and you already know we have the playlist.
[00:00:44.680 --> 00:00:48.840]   If I go Weights and Biases here,
[00:00:48.840 --> 00:00:53.160]   I go to the Weights and Biases channel,
[00:00:53.160 --> 00:00:55.080]   and you can see the videos.
[00:00:55.080 --> 00:00:56.600]   So we already have the playlist.
[00:00:56.600 --> 00:00:58.000]   If I go to that playlist,
[00:00:58.840 --> 00:01:03.400]   which is this one,
[00:01:03.400 --> 00:01:05.440]   I'm sorry, everything in a playlist here.
[00:01:05.440 --> 00:01:07.480]   And I thought it might just as well be helpful
[00:01:07.480 --> 00:01:09.600]   to see like how much ground we've covered.
[00:01:09.600 --> 00:01:12.360]   So we kind of, I was just going through,
[00:01:12.360 --> 00:01:15.720]   'cause I was drafting a tweet today just about Fast Book,
[00:01:15.720 --> 00:01:17.840]   and I was having a look at like what all,
[00:01:17.840 --> 00:01:20.400]   what's all the things that we've done,
[00:01:20.400 --> 00:01:21.820]   and having a look at,
[00:01:21.820 --> 00:01:24.880]   and like just starting from our journey.
[00:01:24.880 --> 00:01:27.920]   And I remember with Jeremy,
[00:01:27.920 --> 00:01:29.160]   before we started intro,
[00:01:29.160 --> 00:01:31.760]   and before we started even the first chapter,
[00:01:31.760 --> 00:01:33.080]   I remember making a call,
[00:01:33.080 --> 00:01:35.200]   and I remember as a group,
[00:01:35.200 --> 00:01:36.760]   we decided that we're gonna stick to this,
[00:01:36.760 --> 00:01:39.640]   and we're gonna end Fast Book together.
[00:01:39.640 --> 00:01:41.840]   And we've covered really quite a bit.
[00:01:41.840 --> 00:01:43.080]   Like we started with how to,
[00:01:43.080 --> 00:01:44.700]   this was the chapter where we learned
[00:01:44.700 --> 00:01:47.560]   how to put things in production for the first time.
[00:01:47.560 --> 00:01:49.320]   We skipped over ethics for over time,
[00:01:49.320 --> 00:01:51.120]   but we looked at MNIST basics
[00:01:51.120 --> 00:01:53.240]   where we learned how to classify,
[00:01:53.240 --> 00:01:57.640]   where we learned how to classify basically digits,
[00:01:57.640 --> 00:01:58.940]   or handwritten digits.
[00:01:58.940 --> 00:02:01.560]   And in pet breeds is where things started
[00:02:01.560 --> 00:02:02.840]   getting a little more interesting,
[00:02:02.840 --> 00:02:05.660]   where we started doing a bit of an actual problem.
[00:02:05.660 --> 00:02:08.420]   That's a more real world related problem.
[00:02:08.420 --> 00:02:11.160]   And in multi, so this was all classification,
[00:02:11.160 --> 00:02:13.820]   and we were just starting up basics with classification.
[00:02:13.820 --> 00:02:15.840]   And then when we got to multicat,
[00:02:15.840 --> 00:02:18.060]   the next thing we started looking at in multicat
[00:02:18.060 --> 00:02:21.080]   was how we can do multicategory classifications
[00:02:21.080 --> 00:02:23.160]   when an image has multiple labels.
[00:02:23.160 --> 00:02:24.440]   So what to do with that?
[00:02:24.440 --> 00:02:27.560]   And we saw that FastAI kind of,
[00:02:27.560 --> 00:02:28.920]   it's really easy to solve.
[00:02:28.920 --> 00:02:30.800]   It's really easy to work with FastAI.
[00:02:30.800 --> 00:02:32.520]   And like with multicat,
[00:02:32.520 --> 00:02:34.160]   there's not a lot that needs to change.
[00:02:34.160 --> 00:02:36.680]   All we had to do was just let FastAI know
[00:02:36.680 --> 00:02:38.800]   that we're working on this multicat problem.
[00:02:38.800 --> 00:02:40.240]   And we looked at the loss functions,
[00:02:40.240 --> 00:02:42.640]   how the loss functions changed.
[00:02:42.640 --> 00:02:47.040]   And we found this idea of like using sigmoid
[00:02:47.040 --> 00:02:48.440]   instead of softmax.
[00:02:48.440 --> 00:02:52.120]   So those were the main changes when we got to multicat.
[00:02:52.120 --> 00:02:53.680]   And in sizing in TDA,
[00:02:53.680 --> 00:02:57.080]   we actually started looking at the data augmentations.
[00:02:57.080 --> 00:03:00.800]   We started looking at what test time augmentation stands for.
[00:03:00.800 --> 00:03:02.720]   And finally, we got to Colab.
[00:03:02.720 --> 00:03:03.880]   And when we got to Colab,
[00:03:03.880 --> 00:03:06.920]   this was about movie recommendation systems.
[00:03:06.920 --> 00:03:08.840]   So we learned about,
[00:03:08.840 --> 00:03:10.640]   basically we learned about movie embeddings,
[00:03:10.640 --> 00:03:12.460]   or we learned about user embeddings,
[00:03:12.460 --> 00:03:14.640]   and we learned how these various
[00:03:14.640 --> 00:03:16.000]   recommendation models are built.
[00:03:16.000 --> 00:03:18.040]   So it's the concepts that we learned
[00:03:18.040 --> 00:03:21.480]   were not just for movie or movie recommendation,
[00:03:21.480 --> 00:03:24.720]   these were concepts that could be used in product matching
[00:03:24.720 --> 00:03:27.240]   or in basically getting a good understanding
[00:03:27.240 --> 00:03:29.160]   of the customer user base.
[00:03:29.160 --> 00:03:31.520]   And now I'm so excited that we're at this
[00:03:31.520 --> 00:03:32.840]   chapter convolutions.
[00:03:32.840 --> 00:03:36.400]   So we skipped over Tableau and we skipped over NLP.
[00:03:36.400 --> 00:03:38.280]   As I've mentioned, Tableau is something that
[00:03:38.280 --> 00:03:39.640]   Zach Miller is going to come
[00:03:39.640 --> 00:03:42.040]   and he's going to touch on Tableau.
[00:03:42.040 --> 00:03:45.120]   And for NLP, it's still in the works,
[00:03:45.120 --> 00:03:48.560]   but I will share plans about what we have for NLP.
[00:03:48.560 --> 00:03:51.080]   So that's something that will come in a few weeks.
[00:03:51.080 --> 00:03:52.320]   So what we're going to do now is
[00:03:52.320 --> 00:03:55.320]   we're going to look at convolutions today
[00:03:55.320 --> 00:03:58.560]   and convolutions will form the basics of,
[00:03:58.560 --> 00:04:00.520]   basically every convolutional neural network
[00:04:00.520 --> 00:04:02.240]   has convolutions in it, right?
[00:04:02.240 --> 00:04:05.120]   So what we're going to cover today
[00:04:05.120 --> 00:04:09.640]   is we're going to look at what convolutions are really,
[00:04:09.640 --> 00:04:12.280]   as in what does a 1D convolution mean?
[00:04:12.280 --> 00:04:14.640]   What does it mean when we say 2D convolution?
[00:04:14.640 --> 00:04:16.560]   What does it mean when we say 3D convolution?
[00:04:16.560 --> 00:04:18.840]   Or like, what's the exact mathematics
[00:04:18.840 --> 00:04:21.600]   or what's the exact operations that are going,
[00:04:21.600 --> 00:04:23.560]   every time that we say,
[00:04:23.560 --> 00:04:26.120]   oh, we're doing a convolution operation.
[00:04:26.120 --> 00:04:29.560]   And then that will form the basics for like ResNet.
[00:04:29.560 --> 00:04:34.000]   And something I do want to highlight right now is that,
[00:04:34.000 --> 00:04:38.480]   so let me go to the forums
[00:04:38.480 --> 00:04:40.360]   and something I do want to highlight.
[00:04:40.360 --> 00:04:43.120]   So we, as I mentioned, we've got the forums
[00:04:43.120 --> 00:04:45.200]   and if you go to this paper, reading group forums,
[00:04:45.200 --> 00:04:48.200]   let me paste that link in the chat as well.
[00:04:49.200 --> 00:04:52.200]   (mouse clicking)
[00:04:52.200 --> 00:04:54.480]   I'm just pasting that link, so there it is.
[00:04:54.480 --> 00:04:58.640]   So one thing that you will see is happening on the side
[00:04:58.640 --> 00:05:00.760]   is that we're starting with paper reading groups
[00:05:00.760 --> 00:05:01.920]   where we're starting with
[00:05:01.920 --> 00:05:03.320]   beginner friendly paper reading groups.
[00:05:03.320 --> 00:05:05.240]   So we're starting with ResNet, DenseNet,
[00:05:05.240 --> 00:05:07.400]   Squeeze and Excitation, and then EfficientNets.
[00:05:07.400 --> 00:05:09.760]   So those are going to be very beginner friendly
[00:05:09.760 --> 00:05:12.000]   paper reading groups that we're going to start looking at.
[00:05:12.000 --> 00:05:16.040]   And you see that FastBook also has ResNet in it.
[00:05:16.040 --> 00:05:18.000]   So I think this would be a really good point
[00:05:18.000 --> 00:05:19.440]   if you really want to start.
[00:05:19.440 --> 00:05:23.760]   The reason why we're doing this is so people from FastBook
[00:05:23.760 --> 00:05:27.120]   can transition easily into paper reading groups.
[00:05:27.120 --> 00:05:32.120]   And like the, it does require a good sense of like
[00:05:32.120 --> 00:05:34.120]   being able to read the papers
[00:05:34.120 --> 00:05:36.080]   and being able to understand the papers
[00:05:36.080 --> 00:05:38.200]   to be a really good deep learning practitioner.
[00:05:38.200 --> 00:05:41.080]   So what we want to do at Weights & Biases is we,
[00:05:41.080 --> 00:05:42.840]   as I mentioned, right in the first week
[00:05:42.840 --> 00:05:44.400]   is that we want to support you
[00:05:44.400 --> 00:05:46.280]   become good deep learning practitioners.
[00:05:46.280 --> 00:05:48.680]   And this is going to be the point
[00:05:48.680 --> 00:05:49.960]   that could be a transition point
[00:05:49.960 --> 00:05:52.920]   where we learn the basics about convolutions
[00:05:52.920 --> 00:05:55.320]   and then we go to ResNets and we could shift
[00:05:55.320 --> 00:05:58.960]   to having weekly paper reading groups
[00:05:58.960 --> 00:06:00.640]   or bi-weekly paper reading groups.
[00:06:00.640 --> 00:06:02.440]   So then after that point,
[00:06:02.440 --> 00:06:05.560]   the platform for vision would be set
[00:06:05.560 --> 00:06:07.080]   and then you can go back to like
[00:06:07.080 --> 00:06:08.760]   having more Kaggle competitions
[00:06:08.760 --> 00:06:10.480]   or participating in more Kaggle competitions.
[00:06:10.480 --> 00:06:13.040]   So that's how kind of my journey started as well.
[00:06:13.040 --> 00:06:15.000]   And I'm trying to just mimic that
[00:06:15.000 --> 00:06:17.520]   for what worked, for everything that worked for me.
[00:06:17.520 --> 00:06:19.280]   And I'm just trying to mimic that for this group
[00:06:19.280 --> 00:06:20.600]   and it's worked so far.
[00:06:20.600 --> 00:06:24.120]   So that's this idea that I did want to share
[00:06:24.120 --> 00:06:26.720]   about beginner friendly paper reading groups.
[00:06:26.720 --> 00:06:29.840]   And if I go to the Fastbook reading group
[00:06:29.840 --> 00:06:31.840]   and I get to week 12 discussion thread.
[00:06:31.840 --> 00:06:36.840]   So as is usual, if you go to 1db.me/fastbook12
[00:06:36.840 --> 00:06:40.400]   which is the link that I'm going to paste that
[00:06:40.400 --> 00:06:41.440]   in the chat as well.
[00:06:41.440 --> 00:06:44.440]   So this is the link where we're going to have
[00:06:44.440 --> 00:06:46.000]   all our discussions today.
[00:06:46.000 --> 00:06:49.440]   So if I go to 1db.me/fastbook12,
[00:06:49.440 --> 00:06:51.800]   oh, I think I sent the chat to Angelica,
[00:06:51.800 --> 00:06:52.760]   but she's shared it.
[00:06:52.760 --> 00:06:57.080]   So if you get to 1db.me/fastbook12,
[00:06:57.080 --> 00:06:59.560]   this will bring us to this discussion thread
[00:06:59.560 --> 00:07:00.760]   and this is where we're going to have
[00:07:00.760 --> 00:07:02.080]   all our discussions today.
[00:07:02.080 --> 00:07:06.160]   I usually highlight the blog posts
[00:07:06.160 --> 00:07:10.200]   and through PowerPoint presentations.
[00:07:10.200 --> 00:07:12.000]   I thought maybe let's try something differently.
[00:07:12.000 --> 00:07:14.240]   Angelica has this really nice,
[00:07:14.240 --> 00:07:16.080]   Angelica curated this really nice list.
[00:07:16.080 --> 00:07:18.440]   And I'm so glad to see that many of you
[00:07:18.440 --> 00:07:20.560]   are still coming back even week 12
[00:07:20.560 --> 00:07:23.040]   and you're still writing these wonderful,
[00:07:23.040 --> 00:07:25.680]   wonderful blog posts, which is really great.
[00:07:25.680 --> 00:07:29.480]   So Vinayak wrote about this anime recommender system
[00:07:29.480 --> 00:07:30.680]   with Fast.ai too.
[00:07:30.680 --> 00:07:33.920]   And Ravi Mishra then wrote two blog posts
[00:07:33.920 --> 00:07:35.280]   about part one and part two
[00:07:35.280 --> 00:07:36.920]   about building a movie recommender.
[00:07:36.920 --> 00:07:38.000]   So he's got this Netflix,
[00:07:38.000 --> 00:07:39.960]   which is just everything that we covered
[00:07:39.960 --> 00:07:43.480]   in our last sessions about movie lens.
[00:07:43.480 --> 00:07:45.680]   And then there's this other one,
[00:07:45.680 --> 00:07:47.560]   which is part two that does a recap
[00:07:47.560 --> 00:07:49.560]   and goes more into the detail of like,
[00:07:49.560 --> 00:07:51.120]   what exactly are user embeddings
[00:07:51.120 --> 00:07:54.560]   and like what are biases and everything that we covered.
[00:07:54.560 --> 00:07:56.680]   And we've also got the blog posts from,
[00:07:56.680 --> 00:08:01.120]   we've also got the blog posts from Ravi Chandra,
[00:08:01.120 --> 00:08:02.680]   which is about, again,
[00:08:02.680 --> 00:08:04.680]   he's been coming back week after week.
[00:08:04.680 --> 00:08:06.160]   So you can see how this Fast book
[00:08:06.160 --> 00:08:09.560]   has lots of these wonderful different blog posts.
[00:08:09.560 --> 00:08:13.080]   So that's the resources.
[00:08:13.080 --> 00:08:16.200]   And then one thing I was really excited to see as well
[00:08:16.200 --> 00:08:21.200]   in the forums was in terms of projects and resources,
[00:08:21.200 --> 00:08:24.480]   I saw there's this create a movie recommender,
[00:08:24.480 --> 00:08:27.960]   which is a discussion started by Ravi Mishra.
[00:08:27.960 --> 00:08:30.480]   So I was really excited to see this discussion.
[00:08:30.480 --> 00:08:33.080]   And in this one, Ravi's mentioned
[00:08:33.080 --> 00:08:35.440]   that he's been working on building,
[00:08:35.440 --> 00:08:36.720]   like he's written this blog post
[00:08:36.720 --> 00:08:38.640]   about movie recommender systems.
[00:08:38.640 --> 00:08:43.640]   And the idea is to then use what we learned in chapter two.
[00:08:43.640 --> 00:08:44.880]   So remember in chapter two,
[00:08:44.880 --> 00:08:46.520]   we learned how to put things in production
[00:08:46.520 --> 00:08:49.280]   and we built like a system
[00:08:49.280 --> 00:08:51.040]   that could classify basically images
[00:08:51.040 --> 00:08:53.840]   and it could predict what the image labels are.
[00:08:53.840 --> 00:08:56.320]   Ravi wants to do that for movie recommender systems.
[00:08:56.320 --> 00:08:58.880]   So he wants to build basically an API
[00:08:58.880 --> 00:09:00.840]   and deploy it in the cloud.
[00:09:00.840 --> 00:09:03.200]   And so I think this is a really, really interesting project.
[00:09:03.200 --> 00:09:04.680]   And if you want to be involved,
[00:09:04.680 --> 00:09:06.160]   please come and join this thread.
[00:09:06.160 --> 00:09:09.320]   So let me just post that in the chat as well.
[00:09:09.320 --> 00:09:18.840]   Okay, so with that being said and the background provided,
[00:09:18.840 --> 00:09:23.840]   I think we are now ready to get started with convolutions.
[00:09:23.840 --> 00:09:29.080]   So as part of convolutions, as I've already mentioned,
[00:09:29.080 --> 00:09:31.000]   what we're going to do is we're going to look
[00:09:31.000 --> 00:09:34.840]   at what the exact operations are for convolutions.
[00:09:34.840 --> 00:09:39.840]   So we kind of, I kind of touched upon how convolutions are
[00:09:39.840 --> 00:09:43.480]   things that can be used to like detect edges
[00:09:43.480 --> 00:09:44.800]   or they can be used to,
[00:09:44.800 --> 00:09:47.760]   they can be used to detect horizontal edges
[00:09:47.760 --> 00:09:50.040]   or they can be used to detect vertical edges
[00:09:50.040 --> 00:09:52.440]   or they can be, there's like these convolution filters
[00:09:52.440 --> 00:09:55.520]   that can blur an image or it can sharpen an image.
[00:09:55.520 --> 00:09:57.200]   And there's like all of these different things
[00:09:57.200 --> 00:09:59.440]   that convolution filters can do.
[00:09:59.440 --> 00:10:03.840]   What I didn't touch upon is like how that magic happens.
[00:10:03.840 --> 00:10:07.840]   So it's my, so today I'm going to actually start doing that.
[00:10:07.840 --> 00:10:11.600]   So one thing when we were looking at convolutions last time,
[00:10:11.600 --> 00:10:16.600]   we saw that if you have an image like this
[00:10:16.600 --> 00:10:24.400]   and remember if this is three, right?
[00:10:24.400 --> 00:10:29.400]   So let me just go back to that image that we have here.
[00:10:29.400 --> 00:10:34.400]   So see how this is just a snapshot of that digit three.
[00:10:34.400 --> 00:10:38.120]   So in this one, you'll see like everywhere
[00:10:38.120 --> 00:10:41.540]   where the number is three, like these edges,
[00:10:41.540 --> 00:10:46.780]   every point in this image, that has a darker pixel value.
[00:10:46.780 --> 00:10:51.960]   So all pixel values, if it's a bit image,
[00:10:51.960 --> 00:10:55.420]   all pixel values are between zero and 255.
[00:10:55.420 --> 00:10:58.720]   And as we can see in this kind of data frame,
[00:10:58.720 --> 00:11:02.840]   so in this case, the 255 represents black.
[00:11:02.840 --> 00:11:05.080]   It could be that in different places,
[00:11:05.080 --> 00:11:07.720]   this could mean different things, but for our sake,
[00:11:07.720 --> 00:11:09.040]   when we're trying to look at this chapter,
[00:11:09.040 --> 00:11:11.520]   let's just say 255 represents black.
[00:11:11.520 --> 00:11:16.440]   So all of these pixels that are the digit three themselves,
[00:11:16.440 --> 00:11:17.680]   they're going to be higher value.
[00:11:17.680 --> 00:11:21.040]   So these are going to be of values of around 200
[00:11:21.040 --> 00:11:22.760]   to 255 range, right?
[00:11:22.760 --> 00:11:25.600]   And then everywhere around, everywhere around surrounding,
[00:11:25.600 --> 00:11:26.640]   that's just the background.
[00:11:26.640 --> 00:11:30.320]   So like all of this background that's around,
[00:11:30.320 --> 00:11:32.320]   'cause it's white or it's brighter,
[00:11:32.320 --> 00:11:37.320]   it's going to have a value of between zero.
[00:11:37.320 --> 00:11:41.160]   It's basically going to be closer to zero.
[00:11:41.160 --> 00:11:43.840]   So this is going to be approximately zero
[00:11:43.840 --> 00:11:45.680]   or between say zero and 50.
[00:11:45.680 --> 00:11:47.480]   So it's gonna have much lower values
[00:11:47.480 --> 00:11:49.200]   'cause it's much brighter.
[00:11:49.200 --> 00:11:54.200]   And what we're doing now is when we have a convolution
[00:11:54.280 --> 00:11:58.120]   filter, so what we do next is if we want to detect,
[00:11:58.120 --> 00:12:01.280]   say the edges of this image,
[00:12:01.280 --> 00:12:05.080]   then what you have is you basically have a convolution
[00:12:05.080 --> 00:12:08.400]   filter that looks something like this.
[00:12:08.400 --> 00:12:12.840]   So you have a convolution filter.
[00:12:12.840 --> 00:12:15.040]   So if you want to detect, say top edges,
[00:12:15.040 --> 00:12:17.880]   you go minus one, minus minus one, zero, zero, zero,
[00:12:17.880 --> 00:12:19.000]   one, one, one.
[00:12:19.000 --> 00:12:23.000]   Now what happens is that this convolution filter,
[00:12:23.000 --> 00:12:24.400]   'cause it's a three by three,
[00:12:24.400 --> 00:12:25.840]   will go all over the image.
[00:12:25.840 --> 00:12:28.320]   It's going to go like this, right?
[00:12:28.320 --> 00:12:29.160]   Something like that.
[00:12:29.160 --> 00:12:30.360]   So it's just going to go in the 2D plane,
[00:12:30.360 --> 00:12:32.240]   this convolution filter.
[00:12:32.240 --> 00:12:35.640]   And then when this convolution filter gets to this point
[00:12:35.640 --> 00:12:41.760]   over here, then you start to get like some values
[00:12:41.760 --> 00:12:44.280]   'cause over here, everything is zero, right?
[00:12:44.280 --> 00:12:45.840]   When the convolution filter is here,
[00:12:45.840 --> 00:12:48.840]   minus one, minus one, minus one, and one, one, one.
[00:12:48.840 --> 00:12:52.360]   If you multiply this with the background pixels,
[00:12:52.360 --> 00:12:55.000]   so in a convolution, remember all that was happening,
[00:12:55.000 --> 00:12:56.920]   and this is something we covered last time,
[00:12:56.920 --> 00:13:00.400]   is what you do is if you have an image slice,
[00:13:00.400 --> 00:13:02.080]   which could be this image slice,
[00:13:02.080 --> 00:13:04.240]   and it will have some values like zero.
[00:13:04.240 --> 00:13:07.960]   This would have like values like zero,
[00:13:07.960 --> 00:13:11.520]   one, two, three, four, five, six, seven, some values.
[00:13:11.520 --> 00:13:14.320]   All that happens is like you multiply these things
[00:13:14.320 --> 00:13:15.560]   element by element, right?
[00:13:15.560 --> 00:13:18.280]   That's all that's happening in convolution.
[00:13:18.280 --> 00:13:21.680]   And then what you see is that when this convolution filter
[00:13:21.680 --> 00:13:24.080]   reaches this point, which is just the background,
[00:13:24.080 --> 00:13:27.040]   it's going to have a zero value, correct?
[00:13:27.040 --> 00:13:28.760]   But when it reaches this point,
[00:13:28.760 --> 00:13:31.600]   when it reaches the top edge,
[00:13:31.600 --> 00:13:34.760]   so if it's, say it reaches this point,
[00:13:34.760 --> 00:13:36.080]   what's gonna happen is like,
[00:13:36.080 --> 00:13:39.200]   'cause this filter at the top is minus one,
[00:13:39.200 --> 00:13:41.080]   and the background values are zero,
[00:13:41.080 --> 00:13:44.840]   so this row is going to give me, say, an output of zero.
[00:13:44.840 --> 00:13:47.160]   Then this middle row is also giving me an output of zero
[00:13:47.160 --> 00:13:50.480]   'cause the convolution filter is zero anyway.
[00:13:50.480 --> 00:13:52.440]   And then when it reaches the top edge,
[00:13:52.440 --> 00:13:55.480]   this row is going to give me some positive value.
[00:13:55.480 --> 00:13:58.080]   So the value could be, say, 150.
[00:13:58.080 --> 00:13:59.840]   So what's gonna happen is like,
[00:13:59.840 --> 00:14:02.240]   every time you take this convolution filter
[00:14:02.240 --> 00:14:03.720]   throughout your image,
[00:14:03.720 --> 00:14:07.360]   every time you hit a point where it's going from
[00:14:07.360 --> 00:14:10.120]   white background to dark values,
[00:14:10.120 --> 00:14:12.320]   or basically like smaller pixel values
[00:14:12.320 --> 00:14:13.720]   to dark pixel values,
[00:14:13.720 --> 00:14:15.160]   that's every time where this thing
[00:14:15.160 --> 00:14:16.960]   will give you a positive number.
[00:14:16.960 --> 00:14:19.760]   So everywhere else, like everywhere else in the image
[00:14:19.760 --> 00:14:23.160]   where there's this yellow highlighter,
[00:14:23.160 --> 00:14:24.480]   everywhere else in this image,
[00:14:24.480 --> 00:14:27.400]   this convolution filter is going to give me
[00:14:27.400 --> 00:14:29.960]   a value of zero, right?
[00:14:29.960 --> 00:14:33.360]   So everywhere, it's gonna give me a value of zero.
[00:14:33.360 --> 00:14:35.840]   But at any point where it reaches a point,
[00:14:35.840 --> 00:14:37.400]   like, for example, over here,
[00:14:37.400 --> 00:14:39.720]   any time when it reaches this,
[00:14:39.720 --> 00:14:41.440]   'cause the values are minus one, minus one,
[00:14:41.440 --> 00:14:43.840]   and here are one, one, one,
[00:14:43.840 --> 00:14:46.560]   at this point, it's gonna give me a large positive value,
[00:14:46.560 --> 00:14:48.560]   so maybe like 120.
[00:14:48.560 --> 00:14:51.680]   So what happens is when this filter goes through this image,
[00:14:51.680 --> 00:14:54.720]   'cause it's gonna give me large values at only points
[00:14:54.720 --> 00:14:57.560]   which are top edges,
[00:14:57.560 --> 00:14:59.080]   or which are points that are transitioning
[00:14:59.080 --> 00:15:01.560]   from white background to dark background,
[00:15:01.560 --> 00:15:03.880]   then this is exactly where
[00:15:03.880 --> 00:15:05.520]   it's going to detect the top edge.
[00:15:05.520 --> 00:15:07.640]   So this is how a top edge detector,
[00:15:07.640 --> 00:15:10.640]   basically, this is how manually
[00:15:10.640 --> 00:15:12.200]   these features could be assigned.
[00:15:12.200 --> 00:15:15.200]   So when you say you wanna do something like say,
[00:15:15.200 --> 00:15:18.440]   do basically a vertical edge,
[00:15:18.440 --> 00:15:21.040]   the values of the filter could be changed
[00:15:21.040 --> 00:15:23.240]   to like minus one, minus one, minus one,
[00:15:23.240 --> 00:15:25.320]   zero, zero, zero, one, one, one.
[00:15:25.320 --> 00:15:26.720]   And then what's gonna happen
[00:15:26.720 --> 00:15:28.880]   is when this filter goes through the image,
[00:15:28.880 --> 00:15:30.600]   it's just going to find points
[00:15:30.600 --> 00:15:33.920]   that go from white regions to dark regions,
[00:15:33.920 --> 00:15:35.680]   but in the vertical plane, right?
[00:15:35.680 --> 00:15:37.800]   So it's going minus one, minus one, minus one,
[00:15:37.800 --> 00:15:39.600]   and one, one, one.
[00:15:39.600 --> 00:15:42.160]   So you see, it's going from like white regions
[00:15:42.160 --> 00:15:44.120]   in the background on the left,
[00:15:44.120 --> 00:15:47.400]   and then dark regions of the digit on the right.
[00:15:47.400 --> 00:15:50.160]   So it's actually found vertical edge,
[00:15:50.160 --> 00:15:53.280]   which is the left vertical edge, basically.
[00:15:53.280 --> 00:15:54.880]   So I just wanted to give that background
[00:15:54.880 --> 00:15:58.400]   before I started looking into this,
[00:15:58.400 --> 00:16:01.200]   started showing you like how the top detector works
[00:16:01.200 --> 00:16:03.360]   or how this edge detector works.
[00:16:03.360 --> 00:16:05.520]   And like these are, in this case,
[00:16:05.520 --> 00:16:07.720]   we are manually, like when we're doing this,
[00:16:07.720 --> 00:16:11.000]   in this case, we are manually designing the filters.
[00:16:11.000 --> 00:16:12.840]   So like this, if the filter value
[00:16:12.840 --> 00:16:14.400]   is minus one, minus one, minus one,
[00:16:14.400 --> 00:16:15.960]   and then zero, zero, zero over here,
[00:16:15.960 --> 00:16:17.640]   and then one, one, one on the right,
[00:16:17.640 --> 00:16:20.200]   then that's a vertical detector.
[00:16:20.200 --> 00:16:23.560]   But in a real case, when you say,
[00:16:23.560 --> 00:16:26.240]   in the deep learning model,
[00:16:26.240 --> 00:16:27.680]   when you define your convolution,
[00:16:27.680 --> 00:16:29.800]   or when you say, oh, I need my first layer
[00:16:29.800 --> 00:16:32.240]   as a convolution layer with X amount of filters
[00:16:32.240 --> 00:16:34.160]   or X amount of layers,
[00:16:34.160 --> 00:16:37.080]   then that's when the model is learning these filters.
[00:16:37.080 --> 00:16:38.320]   But we'll get to that very soon.
[00:16:38.320 --> 00:16:40.360]   So don't worry about that now.
[00:16:40.360 --> 00:16:43.000]   I'll just go in and check if there's,
[00:16:43.000 --> 00:16:45.600]   are there any questions on,
[00:16:45.600 --> 00:16:51.360]   oh, thanks for posting this Vinayak.
[00:16:51.360 --> 00:16:53.160]   I missed a Korean's blog post.
[00:16:53.160 --> 00:16:55.560]   So Korean's also written about Week 10,
[00:16:55.560 --> 00:16:57.480]   Embeddings and Recommender Systems.
[00:16:57.480 --> 00:16:59.040]   Thanks very much for pointing this out.
[00:16:59.040 --> 00:17:03.040]   I'll add that to this comment here.
[00:17:03.040 --> 00:17:10.000]   Thanks Naresh for pointing this tutorial.
[00:17:10.000 --> 00:17:11.640]   So are there any questions about,
[00:17:11.640 --> 00:17:14.440]   I'm assuming then there's no questions about like
[00:17:14.440 --> 00:17:17.880]   how the convolution kernel is then able to find edges
[00:17:17.880 --> 00:17:19.920]   or like top edge, left edge, vertical edge,
[00:17:19.920 --> 00:17:21.640]   or like just things like this.
[00:17:21.640 --> 00:17:25.120]   So then if you want to do something like blurring,
[00:17:25.120 --> 00:17:28.880]   or basically then you just change the values of this filter
[00:17:28.880 --> 00:17:31.720]   and then it is able to do different things, right?
[00:17:31.720 --> 00:17:34.320]   So if I make something like, if I make,
[00:17:34.320 --> 00:17:37.400]   and I'm just saying this as an example,
[00:17:37.400 --> 00:17:42.400]   if I make something like one at the bottom
[00:17:42.400 --> 00:17:47.080]   and then maybe like minus 0.2, 0.2, 0.2 around,
[00:17:47.080 --> 00:17:48.240]   then what's that gonna do?
[00:17:48.240 --> 00:17:51.960]   It's just basically gonna have a value of the pixel
[00:17:51.960 --> 00:17:54.280]   in the center and then everything around it
[00:17:54.280 --> 00:17:55.760]   is going to be a low pixel value,
[00:17:55.760 --> 00:17:57.600]   which means it's going to be blurred, right?
[00:17:57.600 --> 00:17:59.240]   So if you pass this to the image,
[00:17:59.240 --> 00:18:01.440]   then you're basically gonna blur the image.
[00:18:01.440 --> 00:18:04.720]   So these are just examples that I wanted to show you.
[00:18:04.720 --> 00:18:07.880]   So when you go back and you read like this section
[00:18:07.880 --> 00:18:10.760]   of the notebook or this section of the fast book,
[00:18:10.760 --> 00:18:13.960]   you will see basically everything that I've explained,
[00:18:13.960 --> 00:18:16.120]   but just being done in PyTorch.
[00:18:16.120 --> 00:18:19.560]   So let me show some code for that.
[00:18:19.560 --> 00:18:23.040]   So the top edge, if you have a look,
[00:18:23.040 --> 00:18:26.800]   this is how we define a top edge detector,
[00:18:26.800 --> 00:18:29.600]   minus one at the top, 0, 0, 0 in the middle,
[00:18:29.600 --> 00:18:30.640]   and then 1, 1, 1.
[00:18:30.640 --> 00:18:32.040]   And as I've mentioned, it goes,
[00:18:32.040 --> 00:18:35.400]   it will find points where it is going from white background
[00:18:35.400 --> 00:18:37.040]   to dark values.
[00:18:37.040 --> 00:18:41.120]   So some points like basically in this row,
[00:18:41.120 --> 00:18:43.920]   like in this row five, it could find these three points.
[00:18:43.920 --> 00:18:48.040]   If you have a convolution filter that has these three,
[00:18:48.040 --> 00:18:50.240]   basically if you have a convolution filter,
[00:18:59.800 --> 00:19:01.800]   So if you have a convolution filter,
[00:19:01.800 --> 00:19:04.580]   say somewhere here.
[00:19:04.580 --> 00:19:07.080]   So that's when it will give you a positive value.
[00:19:07.080 --> 00:19:12.680]   So then this is just like showing this thing in PyTorch,
[00:19:12.680 --> 00:19:14.280]   it's like let's grab a subset.
[00:19:14.280 --> 00:19:17.780]   So we grab four, five, six, and six, seven, eight.
[00:19:17.780 --> 00:19:20.080]   And basically these are the values that you have.
[00:19:20.080 --> 00:19:23.720]   So row four, five, six, and column six, seven, eight
[00:19:23.720 --> 00:19:24.640]   are these ones.
[00:19:24.640 --> 00:19:27.500]   So zero, zero, zero, one, four, two, one, five, five,
[00:19:27.500 --> 00:19:29.880]   two, four, six, and then two, five, four.
[00:19:29.880 --> 00:19:31.480]   So then you get these values for this.
[00:19:31.480 --> 00:19:34.800]   It just grabs like a small part of the main image,
[00:19:34.800 --> 00:19:37.040]   and then you just do your multiplication
[00:19:37.040 --> 00:19:38.400]   and you do the sum.
[00:19:38.400 --> 00:19:40.680]   So see how it's giving a large positive value
[00:19:40.680 --> 00:19:42.300]   every time it hits a top edge?
[00:19:42.300 --> 00:19:43.740]   That's exactly the idea.
[00:19:43.740 --> 00:19:46.120]   But anywhere else in the image
[00:19:46.120 --> 00:19:48.000]   will give you a small value.
[00:19:48.000 --> 00:19:51.680]   So I guess I will skip over this part
[00:19:51.680 --> 00:19:53.240]   'cause I think it's really not,
[00:19:53.240 --> 00:19:55.080]   like what I've explained,
[00:19:55.080 --> 00:19:58.740]   this is just the exact same explanation in the book.
[00:19:58.740 --> 00:20:01.260]   So when you go back and you read this section of the book,
[00:20:01.260 --> 00:20:03.660]   you will see how top edges can be detected
[00:20:03.660 --> 00:20:06.420]   and basically everything that I've said.
[00:20:06.420 --> 00:20:08.940]   So then the next thing I do wanna highlight
[00:20:08.940 --> 00:20:11.100]   is like mapping a convolution kernel.
[00:20:11.100 --> 00:20:13.120]   So what exactly happens is,
[00:20:13.120 --> 00:20:18.460]   and this is something I did touch on briefly in the past
[00:20:18.460 --> 00:20:21.740]   a lot many times, but what you have,
[00:20:21.740 --> 00:20:24.940]   so what happens is let's say if I have my input image
[00:20:24.940 --> 00:20:26.780]   that looks like this, right?
[00:20:26.780 --> 00:20:29.580]   And my kernel that looks like this.
[00:20:29.580 --> 00:20:31.020]   So as it says over here,
[00:20:31.020 --> 00:20:33.820]   your kernel is going to go from top left
[00:20:33.820 --> 00:20:35.660]   and then to the top right and so on.
[00:20:35.660 --> 00:20:37.140]   So the kernel, so this kernel
[00:20:37.140 --> 00:20:38.980]   is going to start somewhere here
[00:20:38.980 --> 00:20:41.260]   and then it's going to move one row to the right.
[00:20:41.260 --> 00:20:43.220]   So it's gonna start in the next row,
[00:20:43.220 --> 00:20:44.780]   then it's gonna move one row to the right,
[00:20:44.780 --> 00:20:47.500]   then it's gonna start somewhere like this and so on.
[00:20:47.500 --> 00:20:49.540]   So this kernel is then traveling
[00:20:49.540 --> 00:20:52.060]   or basically covering this whole image.
[00:20:52.060 --> 00:20:54.420]   So this is what this thing is doing
[00:20:54.420 --> 00:20:56.460]   is like mapping a convolution kernel.
[00:20:56.460 --> 00:21:00.460]   And then in this case, all that FastAI is doing
[00:21:00.460 --> 00:21:03.260]   is like it's finding the grid of the coordinates
[00:21:03.260 --> 00:21:05.200]   in a nested list comprehension.
[00:21:05.200 --> 00:21:09.140]   So all it is saying is like, okay, let's start with,
[00:21:09.140 --> 00:21:11.420]   'cause this kernel is gonna go like all the way,
[00:21:11.420 --> 00:21:12.660]   you could find the coordinates
[00:21:12.660 --> 00:21:14.560]   at which the center of the kernel lies,
[00:21:14.560 --> 00:21:16.680]   like these could be the coordinates,
[00:21:16.680 --> 00:21:18.340]   you could define them manually.
[00:21:18.340 --> 00:21:19.860]   Of course, all of this,
[00:21:19.860 --> 00:21:21.980]   you never have to do in your own again,
[00:21:21.980 --> 00:21:25.720]   like PyTorch can, this part of the book
[00:21:25.720 --> 00:21:27.820]   is just showing you things manually
[00:21:27.820 --> 00:21:29.920]   and like just showing you like you could do this
[00:21:29.920 --> 00:21:31.600]   convolution kernel mapping on your own.
[00:21:31.600 --> 00:21:34.460]   You could map the, basically you could do this map
[00:21:34.460 --> 00:21:36.680]   map the kernel to the image on your own.
[00:21:36.680 --> 00:21:38.180]   And all they're doing is like defining
[00:21:38.180 --> 00:21:39.400]   these set of grid points.
[00:21:39.400 --> 00:21:41.480]   So like you start with top left 1 1,
[00:21:41.480 --> 00:21:42.640]   which is this pointer.
[00:21:42.640 --> 00:21:46.560]   So X, Y, basically, if let's say these are the one, two,
[00:21:46.560 --> 00:21:48.040]   three, four, one, two, three, four.
[00:21:48.040 --> 00:21:51.460]   So you start with 1, 1, 1, 2, 1, 3, 1, 4
[00:21:51.460 --> 00:21:54.380]   and then so on 2, 1, 2, 2, 2, 3, 2, 4, or so on.
[00:21:54.380 --> 00:21:57.000]   So basically you just define these grid points.
[00:21:57.000 --> 00:21:58.300]   And then these are the grid points
[00:21:58.300 --> 00:22:01.760]   at which you do the convolution and you get some value.
[00:22:01.760 --> 00:22:03.800]   So you can show the output image.
[00:22:03.800 --> 00:22:08.120]   And as you can see, if I map my top edge detector
[00:22:08.120 --> 00:22:10.360]   to my whole image, all it has done
[00:22:10.360 --> 00:22:12.080]   is that it has detected the top edges.
[00:22:12.080 --> 00:22:15.440]   So see how it is dark at the top edges.
[00:22:15.440 --> 00:22:18.120]   It's just detected these, it's done its thing.
[00:22:18.120 --> 00:22:21.120]   And you could also have like a left vertical detector.
[00:22:21.120 --> 00:22:25.400]   So you can see how it's dark every time on your left edges.
[00:22:25.400 --> 00:22:28.880]   So these are the points here and that's pretty much it.
[00:22:28.880 --> 00:22:31.380]   And then you'll see, as I've said,
[00:22:31.380 --> 00:22:35.840]   like the convolution kernel is basically going over the,
[00:22:35.840 --> 00:22:38.860]   or being mapped over the whole image.
[00:22:38.860 --> 00:22:41.880]   So in this example, there's this paper,
[00:22:41.880 --> 00:22:43.560]   a guide to convolution arithmetic,
[00:22:43.560 --> 00:22:46.360]   which is actually a really good paper
[00:22:46.360 --> 00:22:47.680]   that I would recommend reading.
[00:22:47.680 --> 00:22:50.640]   So if you have time and you want to understand
[00:22:50.640 --> 00:22:53.080]   convolutions and you want to understand like,
[00:22:53.080 --> 00:22:54.460]   and you want to just get in the practice
[00:22:54.460 --> 00:22:56.120]   of like just reading papers,
[00:22:56.120 --> 00:22:58.920]   this is a good resource to get started.
[00:22:58.920 --> 00:23:01.280]   So in this case, they just show like various mappings
[00:23:01.280 --> 00:23:03.840]   of various strides and various convolutions.
[00:23:03.840 --> 00:23:05.680]   And you see that in this case,
[00:23:05.680 --> 00:23:10.680]   if my image is represented by this light blue at the back,
[00:23:10.680 --> 00:23:13.560]   my convolution kernel, which is three by three
[00:23:13.560 --> 00:23:16.280]   is represented by this dark blue at the top.
[00:23:16.280 --> 00:23:18.240]   So what's going to happen is like,
[00:23:18.240 --> 00:23:23.240]   let me just paste this in here,
[00:23:23.240 --> 00:23:26.480]   so I can explain how the output comes.
[00:23:26.480 --> 00:23:28.760]   So you have your image at the back.
[00:23:28.760 --> 00:23:31.520]   So let's say my image values are,
[00:23:31.520 --> 00:23:37.080]   say 10, this one is 20.
[00:23:37.080 --> 00:23:40.080]   So I'm just, these are not the kernel values, right?
[00:23:40.080 --> 00:23:43.440]   The thing at the top, this thing at the top,
[00:23:43.440 --> 00:23:45.800]   oh, that's wrong, that's not a good color for this.
[00:23:45.800 --> 00:23:49.360]   This thing at the top, like this bit is my kernel.
[00:23:49.360 --> 00:23:52.000]   So this is my kernel that looks something like this.
[00:23:52.000 --> 00:23:53.760]   That's my kernel.
[00:23:53.760 --> 00:23:56.880]   At the bottom, this whole thing at the bottom,
[00:23:56.880 --> 00:23:58.280]   that's the image, right?
[00:23:58.280 --> 00:24:00.800]   So the image could have some values,
[00:24:00.800 --> 00:24:04.600]   but then what I'm doing is my kernel is multiplied
[00:24:04.600 --> 00:24:07.980]   by like this three cross three map, every element.
[00:24:07.980 --> 00:24:11.160]   Let me try this.
[00:24:11.160 --> 00:24:14.720]   So every element like this element, this element,
[00:24:14.720 --> 00:24:17.680]   this element, this, this, and the nine elements,
[00:24:17.680 --> 00:24:20.000]   basically they multiply with each other
[00:24:20.000 --> 00:24:21.400]   and you take the sum.
[00:24:21.400 --> 00:24:23.800]   So you multiply whatever the value of the kernel is,
[00:24:23.800 --> 00:24:26.840]   and then whatever the value of the image is at this point,
[00:24:26.840 --> 00:24:28.960]   same thing for this point, same thing for this point,
[00:24:28.960 --> 00:24:31.320]   same thing for all points on the three cross three grid,
[00:24:31.320 --> 00:24:32.440]   and you take the sum.
[00:24:32.440 --> 00:24:34.960]   So this green thing represents your output
[00:24:34.960 --> 00:24:38.840]   of applying the convolution to this three by three point.
[00:24:38.840 --> 00:24:39.760]   Same thing over here,
[00:24:39.760 --> 00:24:42.760]   now you can apply a convolution kernel to the top right,
[00:24:42.760 --> 00:24:45.240]   then you apply the convolution kernel to the bottom left,
[00:24:45.240 --> 00:24:46.800]   and you apply to the convolution kernel
[00:24:46.800 --> 00:24:47.840]   to the bottom right.
[00:24:47.840 --> 00:24:52.840]   So see how, what happens is like you had an input
[00:24:52.840 --> 00:24:57.280]   of four by four, when you apply the three by three kernel
[00:24:57.280 --> 00:25:01.280]   on top, your output size became smaller.
[00:25:01.280 --> 00:25:04.760]   So this is something I want you to remember
[00:25:04.760 --> 00:25:07.360]   when we go next and we start looking at padding
[00:25:07.360 --> 00:25:09.680]   or all that stuff.
[00:25:09.680 --> 00:25:12.880]   What I want you to remember or see from this image
[00:25:12.880 --> 00:25:16.240]   is like you have some input image,
[00:25:16.240 --> 00:25:19.040]   then you have a kernel size of three by three,
[00:25:19.040 --> 00:25:24.040]   and then the output is actually a smaller size feature map
[00:25:24.040 --> 00:25:26.040]   than the input itself.
[00:25:26.040 --> 00:25:27.880]   So the output is two by two,
[00:25:27.880 --> 00:25:29.760]   whereas the input was four by four.
[00:25:29.760 --> 00:25:39.560]   So that's just the basics of like applying convolutions.
[00:25:39.560 --> 00:25:43.720]   And then this section is just applying those convolutions
[00:25:43.720 --> 00:25:44.680]   in PyTorch.
[00:25:44.680 --> 00:25:48.240]   So you can see the good thing about PyTorch
[00:25:48.240 --> 00:25:51.560]   is like you don't have to apply convolutions one by one.
[00:25:51.560 --> 00:25:54.240]   The one thing it can do is like convolution,
[00:25:54.240 --> 00:25:57.520]   basically your PyTorch accepts the convolution kernels
[00:25:57.520 --> 00:26:01.160]   in this particular, I will explain exactly
[00:26:01.160 --> 00:26:03.200]   what each of these things mean,
[00:26:03.200 --> 00:26:04.520]   and your input is in this thing.
[00:26:04.520 --> 00:26:06.760]   So what PyTorch can actually do
[00:26:06.760 --> 00:26:09.200]   is it can apply multiple convolutions.
[00:26:09.200 --> 00:26:11.120]   So like it can have these convolution operations
[00:26:11.120 --> 00:26:12.600]   on multiple images.
[00:26:12.600 --> 00:26:15.920]   So that's why PyTorch is really fast.
[00:26:15.920 --> 00:26:18.600]   And the next thing is that PyTorch can do these operations
[00:26:18.600 --> 00:26:19.840]   on the GPU.
[00:26:19.840 --> 00:26:23.200]   So like that's the reason why when we run GPU
[00:26:23.200 --> 00:26:25.520]   or when we're using PyTorch, it's really, really fast
[00:26:25.520 --> 00:26:28.640]   'cause it's doing all of these things parallelly, right?
[00:26:28.640 --> 00:26:33.640]   So what that means is instead of like in this example,
[00:26:33.640 --> 00:26:37.040]   I've just applied one kernel to one image,
[00:26:37.040 --> 00:26:42.040]   what PyTorch can do is you can have multiple
[00:26:42.040 --> 00:26:44.400]   or so on multiple images.
[00:26:44.400 --> 00:26:48.200]   And then this kernel is being applied to this position,
[00:26:48.200 --> 00:26:50.600]   this position, this position,
[00:26:50.600 --> 00:26:52.400]   basically all of these three,
[00:26:52.400 --> 00:26:54.680]   multiple images then are being applied.
[00:26:54.680 --> 00:26:57.800]   And all of this operation is then happening parallelly
[00:26:57.800 --> 00:26:58.960]   in PyTorch.
[00:26:58.960 --> 00:27:03.760]   So in this section, all this part is like you stack your,
[00:27:03.760 --> 00:27:05.000]   you just basically define.
[00:27:05.000 --> 00:27:07.440]   So it's just showing like you can have multiple kernels
[00:27:07.440 --> 00:27:11.000]   that can be applied to multiple images.
[00:27:11.000 --> 00:27:14.560]   So all they're saying is let's define some edge kernels.
[00:27:14.560 --> 00:27:15.800]   So your left edge,
[00:27:15.800 --> 00:27:18.160]   we already know what the left edge looks like.
[00:27:18.160 --> 00:27:22.400]   If I say left edge,
[00:27:22.400 --> 00:27:25.360]   so you can see this was my left edge kernel, right?
[00:27:25.360 --> 00:27:29.880]   My top edge, remember top edge was minus ones at the top
[00:27:29.880 --> 00:27:31.480]   and ones at the bottom.
[00:27:31.480 --> 00:27:34.240]   And then you define two more, which is like diagonal edge.
[00:27:34.240 --> 00:27:38.400]   So you can see how this one is minus one at the diagonal
[00:27:38.400 --> 00:27:40.560]   and then one at the center diagonal.
[00:27:40.560 --> 00:27:42.960]   So it's gonna actually find diagonals like this
[00:27:42.960 --> 00:27:45.240]   that look at a slant 45 degree angle.
[00:27:45.240 --> 00:27:47.280]   And then similar for the diagonal edge too,
[00:27:47.280 --> 00:27:50.400]   which is gonna find diagonals like that, right?
[00:27:50.400 --> 00:27:52.200]   So those are the shapes.
[00:27:52.200 --> 00:27:55.680]   So again, diagonal, one is finding diagonals like that.
[00:27:55.680 --> 00:27:58.160]   The other one is finding diagonals like that.
[00:27:58.160 --> 00:28:00.920]   So all you can do is you can define your edge kernels.
[00:28:00.920 --> 00:28:04.880]   You basically stack all of these kernels in your torch.
[00:28:04.880 --> 00:28:07.640]   And you can see now my edge kernels
[00:28:07.640 --> 00:28:09.520]   of shape four by three by three,
[00:28:09.520 --> 00:28:11.640]   which means I have four kernels
[00:28:11.640 --> 00:28:15.600]   of where each kernel has a size of three by three, correct?
[00:28:15.600 --> 00:28:19.920]   So all you can do is then you define your data block,
[00:28:19.920 --> 00:28:21.440]   which is just, this is just MNIST.
[00:28:21.440 --> 00:28:22.960]   This is something we've done in the past.
[00:28:22.960 --> 00:28:25.600]   You just define your MNIST data block
[00:28:25.600 --> 00:28:27.400]   and you define your data loaders.
[00:28:27.400 --> 00:28:29.560]   You get the first batch, which looks like this,
[00:28:29.560 --> 00:28:32.000]   64 by one by 28 by 28.
[00:28:32.000 --> 00:28:33.480]   What does this mean?
[00:28:33.480 --> 00:28:38.480]   This means you have 64 images of single channel
[00:28:38.480 --> 00:28:42.920]   where each image is of size 28 by 28, okay?
[00:28:42.920 --> 00:28:45.200]   So the shapes of this have been explained here.
[00:28:45.200 --> 00:28:48.800]   Mini-batch, so PyTorch, whenever you check shape,
[00:28:48.800 --> 00:28:51.640]   PyTorch shows number of channels first
[00:28:51.640 --> 00:28:53.920]   and then it shows the height and width, okay?
[00:28:53.920 --> 00:28:56.720]   Same thing for weight, for the filters.
[00:28:56.720 --> 00:28:59.560]   You have the channels, in channels,
[00:28:59.560 --> 00:29:01.680]   height and width of your kernels.
[00:29:01.680 --> 00:29:04.440]   And then for the mini, or basically your batch shape
[00:29:04.440 --> 00:29:05.480]   becomes like mini-batch,
[00:29:05.480 --> 00:29:07.960]   which is the number of items in your batch,
[00:29:07.960 --> 00:29:10.760]   the number of channels in your particular image
[00:29:10.760 --> 00:29:12.480]   of one item of the batch,
[00:29:12.480 --> 00:29:15.560]   and then the height and width of your image.
[00:29:15.560 --> 00:29:17.600]   So how do I read this when I'm looking at this?
[00:29:17.600 --> 00:29:21.840]   Like my first batch is of shape 64 by one by 28 by 28.
[00:29:21.840 --> 00:29:24.920]   I read this like I have 64 images.
[00:29:24.920 --> 00:29:26.720]   Each image is one channel
[00:29:26.720 --> 00:29:30.440]   and the each image height and width is 28 by 28, okay?
[00:29:30.440 --> 00:29:33.360]   So all I do, I move them across to the CPU.
[00:29:33.360 --> 00:29:36.200]   So let me run these paths.
[00:29:36.200 --> 00:29:38.480]   All I do is I move them across to the CPU.
[00:29:38.480 --> 00:29:44.680]   One thing you will see is like the weight filter
[00:29:44.680 --> 00:29:49.520]   accepts like my kernel to be like a 40 input.
[00:29:49.520 --> 00:29:51.640]   So for now, 'cause we're doing things manually,
[00:29:51.640 --> 00:29:53.240]   all we have to do is we have to add
[00:29:53.240 --> 00:29:54.720]   that dimensions ourselves.
[00:29:54.720 --> 00:29:59.720]   So remember, our kernel's like four by three by three.
[00:29:59.720 --> 00:30:05.600]   So all we have to do is like we add this dimension one,
[00:30:05.600 --> 00:30:09.680]   which just signifies that our input image has one channel.
[00:30:09.680 --> 00:30:11.920]   Okay, so all we do is over here.
[00:30:11.920 --> 00:30:15.200]   So how do I read this?
[00:30:15.200 --> 00:30:16.800]   It means I have four channels,
[00:30:16.800 --> 00:30:18.960]   I have, sorry, I have four kernels.
[00:30:18.960 --> 00:30:21.720]   Each kernel is of shape three by three.
[00:30:21.720 --> 00:30:24.840]   So these kernels are like top edge.
[00:30:24.840 --> 00:30:27.600]   So this is one of those kernels.
[00:30:27.600 --> 00:30:30.320]   So you have four of these kernels, which are three by three.
[00:30:30.320 --> 00:30:32.640]   And then this one, it just represents
[00:30:32.640 --> 00:30:35.640]   that my input has one channel, okay?
[00:30:35.640 --> 00:30:39.200]   So all I do, I do that.
[00:30:39.200 --> 00:30:42.440]   And then now I'm ready to apply the convolution operation.
[00:30:42.440 --> 00:30:44.240]   Okay, so I take my batch.
[00:30:44.240 --> 00:30:46.120]   So this is when I was saying like,
[00:30:46.120 --> 00:30:48.920]   PyTorch can apply multiple kernels to multiple images
[00:30:48.920 --> 00:30:50.560]   and like all of this happens parallelly
[00:30:50.560 --> 00:30:52.680]   and it's not something we have to worry about
[00:30:52.680 --> 00:30:53.640]   how that works.
[00:30:53.640 --> 00:30:55.240]   So we just say, oh, PyTorch,
[00:30:55.240 --> 00:30:57.320]   please use the con2d operation.
[00:30:57.320 --> 00:31:01.120]   So f.con2d accepts two things.
[00:31:01.120 --> 00:31:03.960]   It accepts your input, which is your input image
[00:31:03.960 --> 00:31:05.800]   or your input feature map.
[00:31:05.800 --> 00:31:08.520]   And it also accepts your kernel weights.
[00:31:08.520 --> 00:31:12.120]   So in this case, my edge kernels are this.
[00:31:12.120 --> 00:31:14.760]   So remember, this is what my edge kernels are.
[00:31:14.760 --> 00:31:17.520]   This one is the vertical left detector.
[00:31:17.520 --> 00:31:19.680]   This one is the top edge detector.
[00:31:19.680 --> 00:31:21.640]   This one is the diagonal detector.
[00:31:21.640 --> 00:31:23.720]   And this one is the second diagonal detector.
[00:31:23.720 --> 00:31:26.200]   So what PyTorch is gonna do,
[00:31:26.200 --> 00:31:28.320]   it's going to apply these four kernels
[00:31:28.320 --> 00:31:30.520]   to all images in my batch.
[00:31:30.520 --> 00:31:35.080]   And then you get an output of 64 by four by 26 by 26.
[00:31:35.080 --> 00:31:40.000]   So it's really important to see what just happened.
[00:31:40.000 --> 00:31:44.800]   So remember, my input shape was xb.shape.
[00:31:44.800 --> 00:31:48.000]   And my output shape is now this batch features.shape.
[00:31:49.000 --> 00:31:51.600]   Can you see the difference?
[00:31:51.600 --> 00:31:55.480]   So before applying the convolution operation,
[00:31:55.480 --> 00:31:59.280]   we had 64 images of 28 by 28,
[00:31:59.280 --> 00:32:02.880]   and we had one channel images.
[00:32:02.880 --> 00:32:07.480]   Now we have 64 images of 26 by 26,
[00:32:07.480 --> 00:32:09.440]   and we have four channel images.
[00:32:09.440 --> 00:32:13.600]   So I guess at this point,
[00:32:13.600 --> 00:32:15.680]   it's really important in your head
[00:32:15.680 --> 00:32:18.960]   to understand what exactly happened.
[00:32:18.960 --> 00:32:21.840]   And something when I was doing the fast AI course,
[00:32:21.840 --> 00:32:25.120]   something Jeremy kept on saying and really stressed on
[00:32:25.120 --> 00:32:28.640]   was that it's really important to see the input shapes,
[00:32:28.640 --> 00:32:30.960]   and it's really important to see the output shapes,
[00:32:30.960 --> 00:32:33.240]   and try and map the two.
[00:32:33.240 --> 00:32:35.560]   So try and map like what exactly happened.
[00:32:35.560 --> 00:32:38.080]   So let's see what exactly happened.
[00:32:38.080 --> 00:32:41.920]   Or like, why do we have four channels as outputs?
[00:32:41.920 --> 00:32:44.840]   And why is like this 26 by 26, right?
[00:32:45.840 --> 00:32:49.320]   I'll just go check if there's any questions.
[00:32:49.320 --> 00:32:51.960]   Actually, I would appreciate, do one thing.
[00:32:51.960 --> 00:32:53.480]   I'm just gonna take a minute.
[00:32:53.480 --> 00:32:56.920]   I think this will be a really helpful exercise.
[00:32:56.920 --> 00:33:01.920]   Let's make this very much more interactive.
[00:33:01.920 --> 00:33:06.000]   So take a minute to think like,
[00:33:06.000 --> 00:33:08.440]   I've already given you all the details.
[00:33:08.440 --> 00:33:09.680]   I've already explained everything
[00:33:09.680 --> 00:33:11.560]   that's happening in the convolutions.
[00:33:11.560 --> 00:33:13.440]   But take a minute to first,
[00:33:13.440 --> 00:33:18.440]   let's brainstorm why was my input 28 by 28?
[00:33:18.440 --> 00:33:20.720]   And when I applied a three by three kernel
[00:33:20.720 --> 00:33:24.680]   to this 28 by 28 input image, my output is 26 by 26.
[00:33:24.680 --> 00:33:26.080]   So why do you think that happened?
[00:33:26.080 --> 00:33:27.920]   So I guess just post over here
[00:33:27.920 --> 00:33:30.440]   and see like if you have an example.
[00:33:30.440 --> 00:33:37.440]   So come have a post and I'm just gonna,
[00:33:37.440 --> 00:33:39.000]   let's just make this more interactive.
[00:33:39.000 --> 00:33:40.720]   I think this is the best way forward.
[00:33:40.720 --> 00:33:42.600]   So while you're posting your answers,
[00:33:43.600 --> 00:33:45.560]   I also want you to think,
[00:33:45.560 --> 00:33:49.840]   why do you think our input image had one channel,
[00:33:49.840 --> 00:33:53.000]   whereas our output feature map has four channels?
[00:33:53.000 --> 00:33:54.560]   So what changed?
[00:33:54.560 --> 00:33:55.960]   So these are the two things.
[00:33:55.960 --> 00:33:59.880]   And meanwhile, I'll start drawing some,
[00:33:59.880 --> 00:34:03.880]   I'll start posting the answer here.
[00:34:03.880 --> 00:34:06.880]   (keyboard clacking)
[00:34:06.880 --> 00:34:21.320]   Okay, Vinayak saying, if we have an RGB image,
[00:34:21.320 --> 00:34:22.840]   we don't have an RGB image.
[00:34:22.840 --> 00:34:28.680]   That is incorrect.
[00:34:28.680 --> 00:34:29.640]   I think Vinayak you're jumping ahead.
[00:34:29.640 --> 00:34:31.360]   We don't have an RGB image.
[00:34:31.360 --> 00:34:34.360]   (keyboard clacking)
[00:34:34.360 --> 00:34:37.360]   (keyboard clacking)
[00:34:37.360 --> 00:34:40.360]   (keyboard clacking)
[00:34:40.360 --> 00:34:43.360]   (keyboard clacking)
[00:34:43.360 --> 00:34:46.360]   (keyboard clacking)
[00:34:46.360 --> 00:34:49.360]   (keyboard clacking)
[00:34:49.360 --> 00:34:55.360]   (keyboard clacking)
[00:34:55.360 --> 00:35:05.360]   (keyboard clacking)
[00:35:05.360 --> 00:35:10.360]   (keyboard clacking)
[00:35:11.360 --> 00:35:14.360]   (keyboard clacking)
[00:35:14.360 --> 00:35:20.360]   (keyboard clacking)
[00:35:20.360 --> 00:35:26.360]   (keyboard clacking)
[00:35:26.360 --> 00:35:33.360]   (keyboard clacking)
[00:35:38.360 --> 00:35:41.360]   (keyboard clacking)
[00:35:41.360 --> 00:35:49.280]   On this tensor, tot size 64, one by 28 by 28,
[00:35:49.280 --> 00:35:51.240]   if we apply four filters,
[00:35:51.240 --> 00:35:53.880]   it would produce four tot size,
[00:35:53.880 --> 00:35:59.600]   64 by one by 26 for each filter.
[00:35:59.600 --> 00:36:01.880]   Will be stacked and make a tot size,
[00:36:01.880 --> 00:36:05.720]   this size is reduced from 28 to 26
[00:36:05.720 --> 00:36:07.160]   because of applying the filter.
[00:36:07.160 --> 00:36:08.240]   - That's perfect.
[00:36:08.240 --> 00:36:13.160]   But let's just quickly wait for Kevin and Vinayak to finish.
[00:36:13.160 --> 00:36:15.440]   I really wanna see what they're looking at as well.
[00:36:15.440 --> 00:36:18.000]   So Kevin says, I believe the reason is
[00:36:18.000 --> 00:36:20.240]   because our kernels are three by three size.
[00:36:20.240 --> 00:36:22.440]   So when we compute each location,
[00:36:22.440 --> 00:36:24.800]   we lose two pixels in each direction
[00:36:24.800 --> 00:36:28.080]   because it won't be able to strive past 28.
[00:36:28.080 --> 00:36:29.000]   That's correct.
[00:36:29.000 --> 00:36:31.040]   I'm showing it on nine wide.
[00:36:31.040 --> 00:36:32.680]   So if you have one, two, three, one, two.
[00:36:32.680 --> 00:36:33.520]   Yeah, that's correct.
[00:36:33.520 --> 00:36:35.440]   That's a really good explanation.
[00:36:35.440 --> 00:36:37.520]   So that's why we lose the two pixels.
[00:36:37.520 --> 00:36:41.000]   That's correct.
[00:36:41.000 --> 00:36:44.920]   I'm so happy to see each one of you got it.
[00:36:44.920 --> 00:36:48.240]   So I guess then what I'm explaining,
[00:36:48.240 --> 00:36:50.920]   this is, I guess, also good feedback
[00:36:50.920 --> 00:36:53.200]   that what I'm explaining is making sense.
[00:36:53.200 --> 00:36:56.440]   So I will just still try and summarize.
[00:36:56.440 --> 00:36:58.000]   I know all of you have got it,
[00:36:58.000 --> 00:37:02.920]   but remember in our input batch, we had 64 images, right?
[00:37:02.920 --> 00:37:05.480]   And each image looks something like this.
[00:37:05.480 --> 00:37:06.920]   So you have 28 by 28,
[00:37:06.920 --> 00:37:09.960]   and then your 64th image is also 28 by 28.
[00:37:09.960 --> 00:37:12.400]   So you have 28, so on.
[00:37:12.400 --> 00:37:14.520]   So I'm not gonna draw the whole 28 lines,
[00:37:14.520 --> 00:37:17.440]   but that's just how the image looks like.
[00:37:17.440 --> 00:37:21.160]   And then what happens is you're telling PyTorch
[00:37:21.160 --> 00:37:23.200]   that I have four kernels.
[00:37:23.200 --> 00:37:25.840]   So one, two, three, and four,
[00:37:25.840 --> 00:37:27.840]   and they're all single channel.
[00:37:27.840 --> 00:37:29.280]   So all you're saying is,
[00:37:29.280 --> 00:37:32.240]   let's take the example of one image, right?
[00:37:32.240 --> 00:37:36.080]   What's gonna happen is like when you apply this one kernel
[00:37:36.080 --> 00:37:41.080]   to this image, your output is going to be of shape 26 by 26.
[00:37:41.080 --> 00:37:43.360]   Why?
[00:37:43.360 --> 00:37:48.360]   Well, this is just exactly as I explained over here.
[00:37:48.360 --> 00:37:52.120]   So when the kernel of three by three was being applied,
[00:37:52.120 --> 00:37:56.200]   it first got applied here, then in this position,
[00:37:56.200 --> 00:37:58.880]   then in this position, and then finally here.
[00:37:58.880 --> 00:38:01.280]   So when you had your output of four by four,
[00:38:01.280 --> 00:38:02.680]   your output is two by two.
[00:38:02.680 --> 00:38:04.840]   So you're losing two, basically,
[00:38:04.840 --> 00:38:07.440]   'cause you can't really have,
[00:38:07.440 --> 00:38:10.120]   you can't really draw, like,
[00:38:10.120 --> 00:38:11.920]   it can't really go out of bounds.
[00:38:11.920 --> 00:38:15.200]   So that's why there's only like two by two,
[00:38:15.200 --> 00:38:17.200]   or there's only like four positions
[00:38:17.200 --> 00:38:18.920]   where this kernel can be applied.
[00:38:18.920 --> 00:38:21.160]   The four positions are this one, this one,
[00:38:21.160 --> 00:38:22.200]   this one, and this one.
[00:38:22.200 --> 00:38:24.920]   Those are the four dots where this kernel can be applied.
[00:38:24.920 --> 00:38:26.880]   So that's why output is two by two.
[00:38:26.880 --> 00:38:29.680]   Similarly, when your output is 28 by 28,
[00:38:29.680 --> 00:38:32.240]   there's only 26 positions, like this one, this one,
[00:38:32.240 --> 00:38:35.240]   so on over here, and then like this.
[00:38:35.240 --> 00:38:38.440]   So you only have 26 positions
[00:38:38.440 --> 00:38:40.440]   where this kernel can be applied.
[00:38:40.440 --> 00:38:43.600]   So that's why your output is 26 by 26.
[00:38:43.600 --> 00:38:47.200]   So we sold the first idea of like,
[00:38:47.200 --> 00:38:50.360]   why the output is of shape 26 by 26.
[00:38:50.360 --> 00:38:51.920]   So now the next thing is,
[00:38:51.920 --> 00:38:56.320]   even though my input had like one channel,
[00:38:56.320 --> 00:38:58.360]   so even though my input had one channel,
[00:38:58.360 --> 00:39:02.400]   then why does the output have like four by 26 by 26, right?
[00:39:02.400 --> 00:39:07.600]   That's because four of these kernels get applied.
[00:39:07.600 --> 00:39:08.680]   So this one as well.
[00:39:08.680 --> 00:39:11.160]   So what you have is you don't have one output
[00:39:11.160 --> 00:39:13.080]   of 26 by 26 per image,
[00:39:13.080 --> 00:39:17.360]   but you actually have four outputs of 26 by 26.
[00:39:17.360 --> 00:39:19.560]   So when you stack them together,
[00:39:19.560 --> 00:39:22.880]   your output is going to look like four by 26 by 26.
[00:39:22.880 --> 00:39:26.160]   I hope that makes sense.
[00:39:26.160 --> 00:39:31.160]   So that's why when we're going here,
[00:39:31.160 --> 00:39:33.800]   that's why for each of the 64 images,
[00:39:33.800 --> 00:39:35.880]   when you had an input of like 28 by 28,
[00:39:35.880 --> 00:39:37.120]   you get for each of those,
[00:39:37.120 --> 00:39:39.600]   you get an output of four by 26 by 26.
[00:39:39.600 --> 00:39:44.600]   So this is really exactly what's going on
[00:39:44.600 --> 00:39:50.200]   when you apply convolution.
[00:39:50.200 --> 00:39:53.280]   Now in deep learning, or when you say,
[00:39:53.280 --> 00:39:54.880]   you define your ResNet.
[00:39:54.880 --> 00:39:58.880]   So if you go from torchvision.models,
[00:39:58.880 --> 00:40:02.880]   import ResNet, let's just say 18.
[00:40:02.880 --> 00:40:05.920]   When you define a model like this,
[00:40:05.920 --> 00:40:08.480]   which is a ResNet 18 or like a much bigger models,
[00:40:08.480 --> 00:40:12.000]   which has convolution 2D operation right at the beginning,
[00:40:12.000 --> 00:40:13.600]   and it's like saying stride and padding,
[00:40:13.600 --> 00:40:15.000]   don't worry about that for now.
[00:40:15.000 --> 00:40:17.120]   It has a different kernel size,
[00:40:17.120 --> 00:40:21.120]   but it's saying, let's try and understand what this is.
[00:40:21.120 --> 00:40:24.720]   So the convolution, this operation is saying
[00:40:24.720 --> 00:40:26.720]   each of my input images,
[00:40:26.720 --> 00:40:28.640]   don't worry about anything else, okay?
[00:40:28.640 --> 00:40:33.280]   So we're just looking at the conv1 operation of a ResNet.
[00:40:33.280 --> 00:40:34.840]   So what this is saying,
[00:40:34.840 --> 00:40:37.360]   my input images have three channels.
[00:40:37.360 --> 00:40:39.600]   I haven't shown you how that looks like,
[00:40:39.600 --> 00:40:41.040]   but let's just say,
[00:40:41.040 --> 00:40:44.520]   let's just follow me along as I'm saying this.
[00:40:44.520 --> 00:40:47.160]   My input images have three channels.
[00:40:47.160 --> 00:40:49.960]   I want my kernel size to be seven by seven.
[00:40:49.960 --> 00:40:51.960]   So in this, sorry, not over here,
[00:40:51.960 --> 00:40:53.840]   in this example, we have three by three,
[00:40:53.840 --> 00:40:55.840]   but all this is saying is like,
[00:40:55.840 --> 00:40:58.280]   I want my kernel size to be seven by seven,
[00:40:58.280 --> 00:41:02.680]   and I want 64 such kernels for each image.
[00:41:02.680 --> 00:41:04.640]   Make sense?
[00:41:04.640 --> 00:41:08.960]   So then for each image, you're going to get 64 outputs.
[00:41:08.960 --> 00:41:12.480]   That's why your output, when you do something like this,
[00:41:12.480 --> 00:41:14.880]   your output is going to have 64 channels.
[00:41:14.880 --> 00:41:17.160]   I hope that helps.
[00:41:17.160 --> 00:41:23.160]   Another way to do this would be my input has shape that,
[00:41:24.000 --> 00:41:27.960]   let's just create a conv2d operation.
[00:41:27.960 --> 00:41:30.360]   I say my input channels is one,
[00:41:30.360 --> 00:41:32.880]   my output channels is 128,
[00:41:32.880 --> 00:41:36.320]   and my kernel size is say three by three.
[00:41:36.320 --> 00:41:38.560]   So I'm just creating some conv,
[00:41:38.560 --> 00:41:41.360]   and then I could do conv xp,
[00:41:41.360 --> 00:41:43.280]   and I could say shape, that should work.
[00:41:43.280 --> 00:41:44.320]   Cool.
[00:41:44.320 --> 00:41:45.320]   So now you can see like,
[00:41:45.320 --> 00:41:46.920]   now you can play around with this.
[00:41:46.920 --> 00:41:49.040]   All I'm saying is, hey, PyDodge,
[00:41:49.040 --> 00:41:51.200]   please apply this convolution operation.
[00:41:51.200 --> 00:41:53.000]   My inputs have one channel,
[00:41:53.000 --> 00:41:55.600]   'cause they're all one by 28 by 28, right?
[00:41:55.600 --> 00:42:00.600]   So if I just check the shape of one element of my batch,
[00:42:00.600 --> 00:42:03.400]   it has one by 28 by 28.
[00:42:03.400 --> 00:42:05.160]   My inputs have one channel,
[00:42:05.160 --> 00:42:07.680]   and I want my kernel size to be three by three,
[00:42:07.680 --> 00:42:10.280]   which is this exact thing.
[00:42:10.280 --> 00:42:11.600]   And all I'm saying is,
[00:42:11.600 --> 00:42:15.520]   instead of like having four kernels that we define,
[00:42:15.520 --> 00:42:18.800]   like edge detector, all of these edge detectors,
[00:42:18.800 --> 00:42:22.480]   PyDodge, please define 128 such kernels for me.
[00:42:23.320 --> 00:42:25.760]   And you do the learning.
[00:42:25.760 --> 00:42:28.200]   So I don't know how you define these,
[00:42:28.200 --> 00:42:31.080]   like what the values of these kernels are.
[00:42:31.080 --> 00:42:32.200]   You do the learning,
[00:42:32.200 --> 00:42:34.680]   and this is how a ResNet will learn
[00:42:34.680 --> 00:42:37.040]   how to apply convolution to the image.
[00:42:37.040 --> 00:42:38.400]   So I just want to, I guess,
[00:42:38.400 --> 00:42:41.240]   I kind of explained a lot more than,
[00:42:41.240 --> 00:42:45.400]   and spent a lot more time in this section
[00:42:45.400 --> 00:42:47.040]   than actually in the book,
[00:42:47.040 --> 00:42:49.160]   but I think it will be helpful.
[00:42:49.160 --> 00:42:51.360]   So that's where I'll stop.
[00:42:51.360 --> 00:42:53.760]   So now when you apply the convolution,
[00:42:53.760 --> 00:42:58.160]   coming back, 'cause we were looking at the edge kernels,
[00:42:58.160 --> 00:43:02.680]   if you just plot the output of like the top edge
[00:43:02.680 --> 00:43:03.720]   or the left edge detector,
[00:43:03.720 --> 00:43:06.320]   you can see like your image looks something like this.
[00:43:06.320 --> 00:43:16.000]   I had a misunderstanding for the longest time
[00:43:16.000 --> 00:43:18.440]   that the number of filters
[00:43:18.440 --> 00:43:19.960]   had something to do with the sizes,
[00:43:19.960 --> 00:43:22.680]   but the size is actually determined by the kernel size
[00:43:22.680 --> 00:43:24.080]   and not the number of filters you use.
[00:43:24.080 --> 00:43:25.920]   That is absolutely correct, yes.
[00:43:25.920 --> 00:43:28.920]   The number of filters,
[00:43:28.920 --> 00:43:32.000]   all it defines is the number of output channels.
[00:43:32.000 --> 00:43:33.680]   So if you have 128 filters,
[00:43:33.680 --> 00:43:35.200]   which I had in the example before,
[00:43:35.200 --> 00:43:36.760]   you have 128 output channels,
[00:43:36.760 --> 00:43:39.240]   'cause each kernel is then,
[00:43:39.240 --> 00:43:40.680]   basically you have 128 kernels
[00:43:40.680 --> 00:43:43.000]   that are going to be applied to the input image.
[00:43:43.000 --> 00:43:46.680]   Let's not jump to the RGB.
[00:43:46.680 --> 00:43:50.960]   So this is something we're gonna cover just over here.
[00:43:50.960 --> 00:43:52.200]   We're gonna cover the RGB image.
[00:43:52.200 --> 00:43:53.880]   I don't remember the exact section,
[00:43:53.880 --> 00:43:55.040]   but we're gonna cover RGB.
[00:43:55.040 --> 00:43:57.840]   So I'm not even gonna look at that question.
[00:43:57.840 --> 00:43:59.760]   Okay, give me one sec.
[00:43:59.760 --> 00:44:08.720]   So now we're next to stripes and padding.
[00:44:08.720 --> 00:44:12.560]   Oh, so now we're next to stripes and padding.
[00:44:12.560 --> 00:44:13.720]   So what does it mean?
[00:44:13.720 --> 00:44:15.680]   So remember when I just created,
[00:44:15.680 --> 00:44:18.680]   like I showed you ResNet-18,
[00:44:18.680 --> 00:44:27.440]   and I showed you like the conv1 operation of this model,
[00:44:27.440 --> 00:44:32.840]   looks like it has something called stride and padding in it.
[00:44:32.840 --> 00:44:35.120]   So what does that mean?
[00:44:35.120 --> 00:44:38.000]   What does this stride,
[00:44:38.000 --> 00:44:39.960]   what does it mean to say,
[00:44:39.960 --> 00:44:42.760]   hey, convolution, please have a stride of two by two
[00:44:42.760 --> 00:44:44.840]   and please have a padding of three by three.
[00:44:44.840 --> 00:44:46.880]   So what does that really mean?
[00:44:46.880 --> 00:44:48.240]   Let's have a look.
[00:44:48.240 --> 00:44:52.400]   So then one thing then is happening over here is
[00:44:52.400 --> 00:45:01.880]   my input is say one, two, three, four,
[00:45:01.880 --> 00:45:07.480]   one, two, three, and four.
[00:45:07.480 --> 00:45:12.480]   So let's say my input is four by four.
[00:45:12.760 --> 00:45:13.800]   Right?
[00:45:13.800 --> 00:45:18.800]   And we were gonna apply a kernel of three by three.
[00:45:18.800 --> 00:45:24.360]   So if we apply a kernel of three by three,
[00:45:24.360 --> 00:45:27.640]   we could only apply it to position like that one,
[00:45:27.640 --> 00:45:30.480]   'cause then that's where the kernel kind of be applied.
[00:45:30.480 --> 00:45:31.960]   We could apply it to this position,
[00:45:31.960 --> 00:45:33.240]   we could apply it to this position,
[00:45:33.240 --> 00:45:35.000]   and we could apply it to this position.
[00:45:35.000 --> 00:45:37.640]   So what's gonna happen is like you get the output,
[00:45:37.640 --> 00:45:39.400]   which is two by two, correct?
[00:45:40.280 --> 00:45:44.880]   Now, every time this happens or every time the,
[00:45:44.880 --> 00:45:48.440]   'cause deep learning model would have like multiple,
[00:45:48.440 --> 00:45:50.480]   multiple convolution operations.
[00:45:50.480 --> 00:45:54.560]   So if your input is say something like 28 by 28,
[00:45:54.560 --> 00:45:57.360]   if you're passing that down to a 50-layer network
[00:45:57.360 --> 00:45:59.400]   and you keep losing two dimensions,
[00:45:59.400 --> 00:46:03.280]   or like basically your spatial dimensions are lost by two,
[00:46:03.280 --> 00:46:06.040]   so it becomes 26 by 26 or so on,
[00:46:06.040 --> 00:46:09.080]   there's gonna be a point where you're gonna be one by one,
[00:46:09.080 --> 00:46:11.720]   and then you can't apply a convolution kernel anymore,
[00:46:11.720 --> 00:46:12.560]   right?
[00:46:12.560 --> 00:46:13.640]   'Cause then you just have one point.
[00:46:13.640 --> 00:46:14.920]   How are you gonna apply a convolution,
[00:46:14.920 --> 00:46:17.880]   a three by three convolution to a one by one point?
[00:46:17.880 --> 00:46:22.000]   So the idea is then there's something called padding.
[00:46:22.000 --> 00:46:23.520]   So what is padding?
[00:46:23.520 --> 00:46:28.520]   It just means you just introduce another layer
[00:46:28.520 --> 00:46:31.840]   of zeros around it, okay?
[00:46:31.840 --> 00:46:34.520]   So this is how that looks like.
[00:46:36.360 --> 00:46:41.360]   So now instead of your image being four by four,
[00:46:41.360 --> 00:46:42.400]   it becomes five by five.
[00:46:42.400 --> 00:46:47.120]   So these are all, there's like different ways of padding.
[00:46:47.120 --> 00:46:49.280]   I'm just showing you zero padding right now.
[00:46:49.280 --> 00:46:53.000]   So these are all zeros, right?
[00:46:53.000 --> 00:46:55.720]   Sorry, not these ones, just the gray ones outside.
[00:46:55.720 --> 00:47:00.480]   I'm so zero and it should be like that.
[00:47:00.480 --> 00:47:03.040]   So the outsides are all zeros, right?
[00:47:03.040 --> 00:47:05.440]   And now what's gonna happen is like,
[00:47:05.440 --> 00:47:07.480]   you get these extra positions
[00:47:07.480 --> 00:47:13.840]   where the convolution kernel could also be applied, correct?
[00:47:13.840 --> 00:47:19.680]   'Cause now at this position, you still have these options,
[00:47:19.680 --> 00:47:24.560]   even though like this part of the image is all just zeros
[00:47:24.560 --> 00:47:27.920]   and the actual information, image information
[00:47:27.920 --> 00:47:32.000]   is only in my highlighted yellow part
[00:47:32.000 --> 00:47:36.760]   and the highlighted gray part is all zeros.
[00:47:36.760 --> 00:47:38.200]   That's completely okay
[00:47:38.200 --> 00:47:39.840]   'cause it's only happening in this section.
[00:47:39.840 --> 00:47:43.360]   Like sure, you're mixing the image information with zeros,
[00:47:43.360 --> 00:47:45.800]   but at least what you're gonna have is like,
[00:47:45.800 --> 00:47:49.040]   you're gonna have an output of shape four by four,
[00:47:49.040 --> 00:47:55.120]   of shape four by four, instead of like the same,
[00:47:55.120 --> 00:47:56.720]   you're gonna have the output size,
[00:47:56.720 --> 00:47:58.880]   which is the same as the input size.
[00:47:58.880 --> 00:48:01.240]   So that is what padding one can do.
[00:48:01.240 --> 00:48:03.400]   But if you have like a padding two,
[00:48:03.400 --> 00:48:06.680]   so you have, say you introduce another padding
[00:48:06.680 --> 00:48:07.960]   and you have more zeros,
[00:48:07.960 --> 00:48:10.240]   then your output could be five by five,
[00:48:10.240 --> 00:48:12.800]   even though your input is four by four.
[00:48:12.800 --> 00:48:15.000]   So this is what is called padding.
[00:48:15.000 --> 00:48:19.000]   And one of the best places to learn more about padding
[00:48:19.000 --> 00:48:22.760]   is just search the FastAI docs, FastAI padding.
[00:48:22.760 --> 00:48:27.800]   I just wanna see, I think there was like this idea
[00:48:27.800 --> 00:48:31.040]   of reflective padding and everything in the FastAI docs.
[00:48:31.040 --> 00:48:33.320]   So I just wanna see, oh, there it is.
[00:48:33.320 --> 00:48:36.880]   So see how, just have a look at this part
[00:48:36.880 --> 00:48:38.520]   of this part of the image.
[00:48:38.520 --> 00:48:43.480]   What you can do, there's like, instead of just adding,
[00:48:43.480 --> 00:48:46.400]   and I'm not just showing you what padding is,
[00:48:46.400 --> 00:48:48.400]   which is what's mentioned in the book,
[00:48:48.400 --> 00:48:49.840]   I'm also trying to explain like
[00:48:49.840 --> 00:48:51.400]   there's different types of padding.
[00:48:51.400 --> 00:48:54.280]   So instead of like just, what you could do is,
[00:48:54.280 --> 00:48:56.320]   instead of just like having these zeros,
[00:48:56.320 --> 00:48:59.480]   you could have like this element at these three places,
[00:48:59.480 --> 00:49:01.840]   then this point at these three places and so on.
[00:49:01.840 --> 00:49:04.520]   So then that kind of padding is called border padding.
[00:49:04.520 --> 00:49:06.920]   So you could have this one here, this one here.
[00:49:06.920 --> 00:49:10.080]   So it's like that kind of padding is called border padding.
[00:49:10.080 --> 00:49:11.080]   In reflection padding,
[00:49:11.080 --> 00:49:14.480]   all you do is like you start reflecting this section.
[00:49:14.480 --> 00:49:17.880]   So this one comes at these three places,
[00:49:17.880 --> 00:49:20.640]   then this point is reflected to come at this point and so on.
[00:49:20.640 --> 00:49:22.440]   So this is just reflection.
[00:49:22.440 --> 00:49:25.920]   So you can see like how the images look differently
[00:49:25.920 --> 00:49:27.840]   when you pad them differently.
[00:49:28.840 --> 00:49:31.720]   In most practical situations,
[00:49:31.720 --> 00:49:33.680]   reflection padding is the way to go
[00:49:33.680 --> 00:49:35.920]   and it kind of works a lot better.
[00:49:35.920 --> 00:49:38.840]   So I'm just sharing that experience with you guys.
[00:49:38.840 --> 00:49:40.600]   So that is padding.
[00:49:40.600 --> 00:49:42.720]   Okay, so that's the idea of padding.
[00:49:42.720 --> 00:49:45.320]   The next thing is, what is stride?
[00:49:45.320 --> 00:49:52.280]   So let me draw one, two, three, four,
[00:49:54.840 --> 00:49:58.440]   one, two, three, four.
[00:49:58.440 --> 00:50:01.880]   So now you apply a convolution here, right?
[00:50:01.880 --> 00:50:03.520]   This is my first position.
[00:50:03.520 --> 00:50:05.560]   Then the second position that you would have applied
[00:50:05.560 --> 00:50:07.640]   the convolution, three by three convolution.
[00:50:07.640 --> 00:50:12.240]   So let's say I have my input of five by five.
[00:50:12.240 --> 00:50:16.440]   I apply my convolution at this first position.
[00:50:16.440 --> 00:50:19.560]   Then ideally what you do is you go from this to your right,
[00:50:19.560 --> 00:50:21.120]   so you apply the convolution.
[00:50:21.120 --> 00:50:23.080]   So see, this was point one comma one,
[00:50:23.080 --> 00:50:24.960]   which is where the convolution is applied.
[00:50:24.960 --> 00:50:26.480]   Then you would apply the convolution kernel
[00:50:26.480 --> 00:50:27.840]   at point one comma two.
[00:50:27.840 --> 00:50:32.280]   But if you define and say a stride of two,
[00:50:32.280 --> 00:50:35.000]   then you don't apply it at one comma two,
[00:50:35.000 --> 00:50:37.600]   rather you apply it at one comma three,
[00:50:37.600 --> 00:50:39.760]   which is this point, okay?
[00:50:39.760 --> 00:50:41.640]   So then your next convolution kernel
[00:50:41.640 --> 00:50:43.040]   gets applied at this point.
[00:50:43.040 --> 00:50:45.440]   And then instead of applying it two comma one,
[00:50:45.440 --> 00:50:48.400]   you don't apply the convolution kernel here.
[00:50:50.080 --> 00:50:53.200]   You apply at this point, three comma one,
[00:50:53.200 --> 00:50:55.240]   and similarly, three comma three.
[00:50:55.240 --> 00:50:58.600]   So the four points where you apply the convolution kernel,
[00:50:58.600 --> 00:51:01.440]   then it becomes one comma one, one comma three,
[00:51:01.440 --> 00:51:03.160]   three comma one, three comma three.
[00:51:03.160 --> 00:51:05.160]   That is what it means to have a padding,
[00:51:05.160 --> 00:51:07.080]   or sorry, a stride of two by two,
[00:51:07.080 --> 00:51:10.040]   which means like you're taking bigger strides.
[00:51:10.040 --> 00:51:11.920]   So you know when we're walking,
[00:51:11.920 --> 00:51:14.840]   we just walk like one, let's say that's the normal stride.
[00:51:14.840 --> 00:51:16.520]   But when you start to take bigger stride,
[00:51:16.520 --> 00:51:18.440]   that means you're taking bigger steps.
[00:51:18.440 --> 00:51:21.000]   So our convolution kernel is now taking bigger steps,
[00:51:21.000 --> 00:51:22.840]   it's taking bigger strides.
[00:51:22.840 --> 00:51:26.160]   So this is this idea of like having strides and padding.
[00:51:26.160 --> 00:51:28.560]   So while padding on one end makes sure,
[00:51:28.560 --> 00:51:30.880]   like it adds extra dimensions
[00:51:30.880 --> 00:51:33.040]   and it makes sure like our input is,
[00:51:33.040 --> 00:51:36.440]   the output is the same size as the input,
[00:51:36.440 --> 00:51:38.680]   strides on the other hand,
[00:51:38.680 --> 00:51:40.080]   make sure like if you have,
[00:51:40.080 --> 00:51:42.480]   let's say you have an input image size of,
[00:51:42.480 --> 00:51:45.240]   let's say you have a massive, massive image,
[00:51:45.240 --> 00:51:48.480]   two, one, six, zero by two, one, six, zero.
[00:51:48.480 --> 00:51:51.720]   It's gonna take ages to keep applying convolutions to this
[00:51:51.720 --> 00:51:55.160]   and bring it down to say like 64 by 64, things like that.
[00:51:55.160 --> 00:51:57.640]   So what you do is like when you have a really big
[00:51:57.640 --> 00:51:59.120]   input image, you have a padding,
[00:51:59.120 --> 00:52:02.040]   you add a stride of like stride two and all that stuff.
[00:52:02.040 --> 00:52:06.520]   So I just wanted to explain what padding and stride are.
[00:52:06.520 --> 00:52:13.000]   Wow, I took a lot of time explaining this few sections.
[00:52:13.000 --> 00:52:15.040]   Anyway, let me just go and have a look
[00:52:15.040 --> 00:52:18.920]   if there's questions, which there are,
[00:52:18.920 --> 00:52:21.040]   which is pretty good to see.
[00:52:21.040 --> 00:52:23.000]   Can kernel filters,
[00:52:23.000 --> 00:52:25.400]   can kernels of filters be more than four in number?
[00:52:25.400 --> 00:52:26.680]   Yes, that is correct.
[00:52:26.680 --> 00:52:27.520]   They can be.
[00:52:27.520 --> 00:52:31.400]   Vinayak, I'm not gonna read this
[00:52:31.400 --> 00:52:33.520]   'cause I'm gonna come back to RGB.
[00:52:33.520 --> 00:52:37.360]   I'm gonna come back to RGB in a sec.
[00:52:37.360 --> 00:52:42.160]   Maybe a bit more could be something like this.
[00:52:42.160 --> 00:52:44.520]   Oh yeah, thanks Anush for pointing this out
[00:52:44.520 --> 00:52:45.600]   that this is really helpful.
[00:52:45.600 --> 00:52:46.680]   Like you have clean channels.
[00:52:46.680 --> 00:52:50.120]   That I'm just generally very, I was being lazy.
[00:52:50.120 --> 00:52:53.320]   So I guess, thanks for writing it like this.
[00:52:53.320 --> 00:52:56.960]   Can anyone tell me,
[00:52:56.960 --> 00:53:01.960]   is there any forums for solving Colab notebook errors?
[00:53:01.960 --> 00:53:05.480]   Fast.ai forums, or even just post them to us
[00:53:05.480 --> 00:53:08.480]   if you try forums.fast.ai.
[00:53:08.480 --> 00:53:11.480]   And then if you search the kind of error that you have,
[00:53:11.480 --> 00:53:14.560]   like say Colab error, I don't know.
[00:53:14.560 --> 00:53:15.720]   If you search some things,
[00:53:15.720 --> 00:53:18.000]   you'll see like there's all these errors
[00:53:18.000 --> 00:53:20.600]   that are being discussed, or you could do them here.
[00:53:20.600 --> 00:53:25.760]   When would be a scenario
[00:53:25.760 --> 00:53:28.360]   when you would wanna increase the stride
[00:53:28.360 --> 00:53:30.600]   just to reduce the image size faster?
[00:53:30.600 --> 00:53:31.440]   Yes, that is correct.
[00:53:31.440 --> 00:53:36.200]   So see ResNet, I keep going back to the ResNet example.
[00:53:36.200 --> 00:53:40.880]   So ResNet18.conf1, right?
[00:53:40.880 --> 00:53:41.840]   That was my first.
[00:53:41.840 --> 00:53:43.800]   So in this case, you have a stride too.
[00:53:43.800 --> 00:53:46.000]   So you have a bigger stride,
[00:53:46.000 --> 00:53:48.440]   but you also have a padding 3x3.
[00:53:48.440 --> 00:53:50.200]   Let's see ResNet34.
[00:53:50.200 --> 00:53:52.480]   Is that the same case over there?
[00:53:52.480 --> 00:53:54.720]   What about, but the kernel size is 7x7,
[00:53:54.720 --> 00:53:55.800]   so that makes sense.
[00:53:55.800 --> 00:53:59.600]   So when you have a, basically,
[00:53:59.600 --> 00:54:02.480]   I keep referring the ResNet, which is gonna be next,
[00:54:02.480 --> 00:54:03.640]   but it's just the architecture
[00:54:03.640 --> 00:54:06.240]   that has a bunch of convolutions.
[00:54:06.240 --> 00:54:09.840]   And mostly what I've seen
[00:54:09.840 --> 00:54:11.520]   from reading all of these different papers
[00:54:11.520 --> 00:54:14.880]   is that you have bigger strides in the earlier layers,
[00:54:14.880 --> 00:54:16.960]   and you have smaller strides in the later layers.
[00:54:16.960 --> 00:54:19.160]   So you have bigger strides in the earlier layers
[00:54:19.160 --> 00:54:21.800]   is because when your input image size is massive,
[00:54:21.800 --> 00:54:24.000]   you just stride and you have like big strides.
[00:54:24.000 --> 00:54:26.680]   So you keep having all of these like massive strides,
[00:54:26.680 --> 00:54:29.160]   so you cover more ground in your earlier layers,
[00:54:29.160 --> 00:54:30.600]   'cause it's gonna take a lot of time
[00:54:30.600 --> 00:54:33.960]   if you go at an input image size of 2160
[00:54:33.960 --> 00:54:36.080]   or like 1600 by 1600.
[00:54:36.080 --> 00:54:37.600]   I'm just bringing up examples.
[00:54:37.600 --> 00:54:39.600]   But you have very large image size
[00:54:39.600 --> 00:54:41.320]   and that's when generally you'll see
[00:54:41.320 --> 00:54:43.840]   like the earlier layers have a bigger stride.
[00:54:43.840 --> 00:54:50.880]   You can post over here.
[00:54:50.880 --> 00:54:53.120]   Oh, thanks, Gurujesh, for pointing that out.
[00:54:53.120 --> 00:54:56.200]   I guess it would be kind of,
[00:54:56.200 --> 00:54:58.200]   it would kind of be stride versus,
[00:54:58.200 --> 00:54:59.400]   sorry, oh, you cut it off.
[00:54:59.400 --> 00:55:01.040]   Okay, I'm just gonna skip then.
[00:55:01.040 --> 00:55:02.640]   I'm gonna keep moving on.
[00:55:02.640 --> 00:55:06.240]   All right.
[00:55:07.440 --> 00:55:09.360]   This part is really important
[00:55:09.360 --> 00:55:11.880]   and I quite like,
[00:55:11.880 --> 00:55:14.800]   and I was quite amazed by it as well.
[00:55:14.800 --> 00:55:15.640]   So I guess,
[00:55:15.640 --> 00:55:18.480]   let me explain it to you.
[00:55:18.480 --> 00:55:25.800]   It should be self-explanatory,
[00:55:25.800 --> 00:55:27.400]   but let me try.
[00:55:27.400 --> 00:55:30.840]   So let's say you have an input image
[00:55:30.840 --> 00:55:34.160]   of capital A, B, C, D, E, F, blah, blah, blah.
[00:55:35.560 --> 00:55:38.160]   And you have your kernel represented
[00:55:38.160 --> 00:55:40.800]   by alpha, beta, gamma, and delta.
[00:55:40.800 --> 00:55:42.840]   So all this is saying is,
[00:55:42.840 --> 00:55:47.280]   my input image,
[00:55:47.280 --> 00:55:48.840]   instead of like, you know,
[00:55:48.840 --> 00:55:53.720]   just having symbols to represent pixel values.
[00:55:53.720 --> 00:55:55.360]   So this is just like variables.
[00:55:55.360 --> 00:55:57.800]   And my kernels are like this,
[00:55:57.800 --> 00:56:00.600]   alpha, beta, gamma, delta.
[00:56:03.280 --> 00:56:04.800]   What's gonna happen in,
[00:56:04.800 --> 00:56:06.960]   when we apply this convolution kernel.
[00:56:06.960 --> 00:56:08.040]   So you start,
[00:56:08.040 --> 00:56:12.520]   you first apply this at this position,
[00:56:12.520 --> 00:56:14.160]   then over here,
[00:56:14.160 --> 00:56:15.440]   then like this,
[00:56:15.440 --> 00:56:16.920]   and then at that position, right?
[00:56:16.920 --> 00:56:19.840]   So top left, top right, bottom left, bottom right.
[00:56:19.840 --> 00:56:22.760]   So this is like, this is how it looks like.
[00:56:22.760 --> 00:56:24.360]   When you apply this to top left,
[00:56:24.360 --> 00:56:26.520]   this gets applied here
[00:56:26.520 --> 00:56:31.280]   and the output that it yields is say maybe capital P.
[00:56:31.280 --> 00:56:33.840]   When you apply this kernel to the top right,
[00:56:33.840 --> 00:56:36.720]   the output that maybe yields is capital Q.
[00:56:36.720 --> 00:56:38.360]   When you apply this to the bottom left,
[00:56:38.360 --> 00:56:40.360]   the output that yields is maybe R.
[00:56:40.360 --> 00:56:41.760]   And same thing at the bottom right,
[00:56:41.760 --> 00:56:44.000]   the output that yields is S.
[00:56:44.000 --> 00:56:47.440]   Now, instead of just looking at this
[00:56:47.440 --> 00:56:50.200]   from like this operation
[00:56:50.200 --> 00:56:52.280]   as a special convolution operation,
[00:56:52.280 --> 00:56:56.120]   we could also look at this as a standard machine learning,
[00:56:56.120 --> 00:56:59.880]   as a standard matrix multiplication operation.
[00:56:59.880 --> 00:57:01.560]   So let's see how that looks like.
[00:57:01.560 --> 00:57:04.480]   Oh, sorry.
[00:57:04.480 --> 00:57:08.640]   Then basically this is just like saying,
[00:57:08.640 --> 00:57:12.840]   oh, P is equal to alpha times A plus beta times B
[00:57:12.840 --> 00:57:16.160]   plus gamma times D plus delta times E, right?
[00:57:16.160 --> 00:57:18.920]   Q is alpha times B,
[00:57:18.920 --> 00:57:20.440]   beta times C,
[00:57:20.440 --> 00:57:24.080]   gamma times E and delta times F and so on, right?
[00:57:24.080 --> 00:57:24.920]   That's okay.
[00:57:24.920 --> 00:57:26.920]   So P is this value, Q is this value.
[00:57:26.920 --> 00:57:30.560]   All you can do is like,
[00:57:30.560 --> 00:57:35.040]   you can represent these equations basically in the,
[00:57:35.040 --> 00:57:37.640]   like P is equal to this, Q is equal to this
[00:57:37.640 --> 00:57:41.760]   as a form of matrix multiplication like this.
[00:57:41.760 --> 00:57:45.240]   So if you have your input image flattened as A, B, C, D, E,
[00:57:45.240 --> 00:57:47.600]   F and so on,
[00:57:47.600 --> 00:57:49.840]   and you apply your first rule,
[00:57:49.840 --> 00:57:51.720]   'cause remember what was P like?
[00:57:51.720 --> 00:57:56.720]   P was alpha A, beta B, gamma D and delta E.
[00:57:57.680 --> 00:58:01.600]   So alpha A, beta B, gamma D, delta E.
[00:58:01.600 --> 00:58:03.600]   So what you can do is like at those positions,
[00:58:03.600 --> 00:58:06.320]   you can have those values, everywhere else can be zero.
[00:58:06.320 --> 00:58:08.680]   Same for Q, you could have it like this,
[00:58:08.680 --> 00:58:12.440]   same for R, same for S.
[00:58:12.440 --> 00:58:15.200]   So this gives you your PQRS.
[00:58:15.200 --> 00:58:17.560]   All this is saying,
[00:58:17.560 --> 00:58:20.320]   or like all this section of the book is saying is like,
[00:58:20.320 --> 00:58:23.400]   it is convolutions are just,
[00:58:23.400 --> 00:58:25.360]   this is just like,
[00:58:25.360 --> 00:58:28.320]   you can just look at convolutions as matrix multiplication,
[00:58:28.320 --> 00:58:30.960]   where there's like these bunch of zeros everywhere else.
[00:58:30.960 --> 00:58:34.280]   So the zeros actually are the untrainable things,
[00:58:34.280 --> 00:58:36.160]   like these are the things that you learn.
[00:58:36.160 --> 00:58:37.520]   So when I was saying like,
[00:58:37.520 --> 00:58:39.640]   you actually just learn these values,
[00:58:39.640 --> 00:58:41.600]   alpha, beta, gamma and delta.
[00:58:41.600 --> 00:58:42.760]   So remember until now,
[00:58:42.760 --> 00:58:44.760]   all we've looked at is kernels that are,
[00:58:44.760 --> 00:58:47.640]   that we define with values minus one, one and so on.
[00:58:47.640 --> 00:58:52.520]   In actual deep learning, when you're training your,
[00:58:52.520 --> 00:58:54.040]   when I keep saying actual deep learning,
[00:58:54.040 --> 00:58:56.320]   all I mean to say is like practical,
[00:58:56.320 --> 00:58:59.280]   when you implementation, when I say actual things,
[00:58:59.280 --> 00:59:01.680]   I just mean like the implementation of it.
[00:59:01.680 --> 00:59:04.480]   So when you're implementing things in PyTorch,
[00:59:04.480 --> 00:59:06.560]   all it is doing is like, you can look at this,
[00:59:06.560 --> 00:59:08.360]   it's just gonna train these,
[00:59:08.360 --> 00:59:11.120]   it's just gonna train the model to learn these four values,
[00:59:11.120 --> 00:59:12.720]   alpha, beta, gamma, delta.
[00:59:12.720 --> 00:59:14.040]   And then you can just look at that
[00:59:14.040 --> 00:59:16.560]   in terms of like a matrix multiplication operation,
[00:59:16.560 --> 00:59:19.160]   where everywhere else is zero, which is untrainable.
[00:59:19.160 --> 00:59:21.000]   So that's the point of this.
[00:59:21.000 --> 00:59:23.880]   That's just like a good point to see is this like,
[00:59:23.880 --> 00:59:27.440]   a different viewpoint of looking at things.
[00:59:27.440 --> 00:59:32.400]   Okay, before we move on to the first convolutional network.
[00:59:32.400 --> 00:59:34.400]   So actually I'm gonna go,
[00:59:34.400 --> 00:59:36.160]   today we're gonna wrap up 1.2
[00:59:36.160 --> 00:59:38.040]   and we're gonna go all about 1.3.
[00:59:38.040 --> 00:59:41.160]   So we're gonna cover until color images.
[00:59:41.160 --> 00:59:45.520]   From 1.4, so we'll be left with 1.4 for next section
[00:59:45.520 --> 00:59:47.520]   and that's gonna be fine.
[00:59:47.520 --> 00:59:49.360]   So we're gonna cover 1.4 next week
[00:59:49.360 --> 00:59:50.840]   and then straight after that,
[00:59:50.840 --> 00:59:53.120]   we're going to go into the first nets.
[00:59:53.120 --> 00:59:55.800]   So today then I'm just gonna cover the 1.3.
[00:59:55.800 --> 00:59:56.640]   So let me see,
[00:59:56.640 --> 01:00:00.400]   'cause I wanna cover things into more detail.
[01:00:00.400 --> 01:00:02.960]   So let me see if there's any questions.
[01:00:02.960 --> 01:00:03.800]   I'll figure it out.
[01:00:03.800 --> 01:00:06.760]   Kernel size doesn't really reduce image size
[01:00:06.760 --> 01:00:09.440]   nearly as much as strike does, only around the edges
[01:00:09.440 --> 01:00:12.080]   and kernel size would require
[01:00:12.080 --> 01:00:14.360]   memory intensive processes as well.
[01:00:14.360 --> 01:00:15.880]   Yes, that is correct.
[01:00:15.880 --> 01:00:18.080]   I'm so glad you're summarizing things
[01:00:18.080 --> 01:00:19.520]   better than I am Kevin.
[01:00:19.520 --> 01:00:22.200]   So that's really helpful to see.
[01:00:23.200 --> 01:00:24.800]   As I just said,
[01:00:24.800 --> 01:00:27.360]   I think of it as covering more of the image
[01:00:27.360 --> 01:00:28.200]   in a lot of stride.
[01:00:28.200 --> 01:00:31.240]   Thank you for agreeing with me.
[01:00:31.240 --> 01:00:38.720]   One of the formula I could click,
[01:00:38.720 --> 01:00:39.560]   the output size.
[01:00:39.560 --> 01:00:40.800]   Oh yes, that is correct.
[01:00:40.800 --> 01:00:45.800]   There's like, there's this Stanford CS231N.
[01:00:45.800 --> 01:00:51.040]   So I guess that's what you're referring to.
[01:00:51.040 --> 01:00:54.960]   So there's this introduction to convolutions, I guess.
[01:00:54.960 --> 01:00:57.400]   Sorry, let me just do a quick Google.
[01:00:57.400 --> 01:01:01.200]   Introduction to convolutions CS231N.
[01:01:01.200 --> 01:01:06.600]   I guess it's this one.
[01:01:06.600 --> 01:01:13.080]   So this is another course run by Stanford and Andrew Ng.
[01:01:13.080 --> 01:01:15.920]   And over here, there's like,
[01:01:15.920 --> 01:01:18.000]   everything that I've explained to you so far,
[01:01:18.000 --> 01:01:20.240]   you can like have formulas of like,
[01:01:20.240 --> 01:01:22.360]   oh, if the padding is gonna be two,
[01:01:22.360 --> 01:01:24.560]   if the stride is gonna be three,
[01:01:24.560 --> 01:01:27.080]   then what's the output size gonna be and all of that.
[01:01:27.080 --> 01:01:29.640]   I never look at the formulas,
[01:01:29.640 --> 01:01:31.280]   but some people like to,
[01:01:31.280 --> 01:01:32.840]   so it could be very helpful.
[01:01:32.840 --> 01:01:36.080]   I'm not saying like one approach is better than the other,
[01:01:36.080 --> 01:01:37.440]   but I think this is the place.
[01:01:37.440 --> 01:01:41.200]   So I just wanna also show everybody
[01:01:41.200 --> 01:01:42.080]   who's doing fast book,
[01:01:42.080 --> 01:01:43.840]   like there's these other resources as well
[01:01:43.840 --> 01:01:45.360]   that are really, really nice.
[01:01:45.360 --> 01:01:48.040]   So let me just post it in the discussion thread.
[01:01:49.040 --> 01:01:51.960]   (mouse clicking)
[01:01:51.960 --> 01:01:54.200]   All right, I guess it's that one.
[01:01:54.200 --> 01:02:00.720]   Cool, so now let's just go and create our first CNN.
[01:02:00.720 --> 01:02:03.160]   Somebody was typing,
[01:02:03.160 --> 01:02:07.000]   I'm just interested in if that's a question.
[01:02:07.000 --> 01:02:08.640]   Anyway, okay, I'll move on.
[01:02:14.480 --> 01:02:17.760]   So remember, all of this we're doing,
[01:02:17.760 --> 01:02:20.840]   we've just understood how kernels get applied,
[01:02:20.840 --> 01:02:22.600]   what the shape of the kernels are,
[01:02:22.600 --> 01:02:24.440]   how does the shape of the input change
[01:02:24.440 --> 01:02:25.400]   when the shape of,
[01:02:25.400 --> 01:02:27.080]   depending on the stride and the padding
[01:02:27.080 --> 01:02:29.240]   and the kernel size.
[01:02:29.240 --> 01:02:31.760]   But the main idea of like,
[01:02:31.760 --> 01:02:33.640]   this is all helpful to know,
[01:02:33.640 --> 01:02:34.920]   but actually what we want to do
[01:02:34.920 --> 01:02:36.240]   is we want to create a CNN
[01:02:36.240 --> 01:02:39.120]   and we want to learn the convolutional network, right?
[01:02:39.120 --> 01:02:40.800]   To do some tasks.
[01:02:40.800 --> 01:02:43.640]   So we're coming back to that problem in this section.
[01:02:44.600 --> 01:02:46.840]   Remember our task that we were trying to do
[01:02:46.840 --> 01:02:50.280]   was basically trying to classify input images
[01:02:50.280 --> 01:02:52.480]   and the input images were DLs.
[01:02:52.480 --> 01:02:56.680]   We have DLs defined?
[01:02:56.680 --> 01:02:57.520]   Yeah, we did.
[01:02:57.520 --> 01:03:02.280]   And the input images were these.
[01:03:02.280 --> 01:03:04.080]   You had threes and sevens.
[01:03:04.080 --> 01:03:05.520]   So all we're trying to do right now
[01:03:05.520 --> 01:03:07.800]   is we're gonna classify threes from sevens.
[01:03:07.800 --> 01:03:10.200]   So remember from MNIST chapter,
[01:03:10.200 --> 01:03:13.400]   we defined like a simple neural net like this.
[01:03:13.400 --> 01:03:15.920]   In this case, let's replace that
[01:03:15.920 --> 01:03:17.720]   and let's define a simple neural net
[01:03:17.720 --> 01:03:19.600]   using convolution operations.
[01:03:19.600 --> 01:03:22.480]   So the first operation is a convolution operation
[01:03:22.480 --> 01:03:25.160]   that takes a one channel input.
[01:03:25.160 --> 01:03:26.800]   So now you know how to read this.
[01:03:26.800 --> 01:03:29.280]   Excuse me.
[01:03:29.280 --> 01:03:31.440]   So it takes one channel inputs.
[01:03:31.440 --> 01:03:37.600]   You have 30 kernels or basically 30 kernels for each
[01:03:37.600 --> 01:03:39.720]   of those, each of the,
[01:03:39.720 --> 01:03:42.680]   you have 30 kernels for each image.
[01:03:42.680 --> 01:03:44.880]   Your kernel size is three by three
[01:03:44.880 --> 01:03:46.160]   and your padding is one.
[01:03:46.160 --> 01:03:47.560]   So what does that look like?
[01:03:47.560 --> 01:03:53.920]   Your input image, oh, sorry.
[01:03:53.920 --> 01:03:57.920]   Your input image is single channel
[01:03:57.920 --> 01:03:59.880]   and I think it's 28 by 28.
[01:03:59.880 --> 01:04:03.760]   So we know that from the input and you have 30 kernels.
[01:04:03.760 --> 01:04:08.760]   So these are 30 of them, 30 kernels of size three by three
[01:04:09.720 --> 01:04:14.240]   padding around this is one and stride is one.
[01:04:14.240 --> 01:04:15.080]   So that's fine.
[01:04:15.080 --> 01:04:17.080]   So padding around this is one.
[01:04:17.080 --> 01:04:20.200]   So the output is also going to be 28 by 28
[01:04:20.200 --> 01:04:22.240]   but 30 by 28 by 28.
[01:04:22.240 --> 01:04:26.360]   So let me explain what I'm trying to say.
[01:04:26.360 --> 01:04:29.400]   So if you just take this,
[01:04:29.400 --> 01:04:32.560]   one equals this.
[01:04:36.120 --> 01:04:40.000]   So my input shape is 64 by one by 28 by 28
[01:04:40.000 --> 01:04:42.440]   which is just representing this one image.
[01:04:42.440 --> 01:04:43.400]   So if I go
[01:04:43.400 --> 01:04:46.200]   con
[01:04:46.200 --> 01:04:48.480]   XP
[01:04:48.480 --> 01:04:51.000]   dot shape.
[01:04:51.000 --> 01:04:56.440]   As you can see, now my output is 64 by 30 by 28 by 28.
[01:04:56.440 --> 01:04:59.040]   But then see what this CNN does.
[01:04:59.040 --> 01:05:02.840]   It first goes from one channel to 30 channels
[01:05:02.840 --> 01:05:05.480]   and then it's going back from 30 channels to one channel.
[01:05:05.480 --> 01:05:07.880]   So how would you read this convolution operation?
[01:05:07.880 --> 01:05:11.600]   You would say, okay, remember now my output.
[01:05:11.600 --> 01:05:13.640]   So the output from this is
[01:05:13.640 --> 01:05:18.520]   the output from the first convolution operation channel
[01:05:18.520 --> 01:05:21.320]   looks something like this, right?
[01:05:21.320 --> 01:05:24.240]   So you have still 28 by 28
[01:05:24.240 --> 01:05:26.960]   but you have 30 such channels now.
[01:05:26.960 --> 01:05:28.400]   So this is the output.
[01:05:28.400 --> 01:05:32.160]   And then to this, we apply the second convolution operation
[01:05:32.160 --> 01:05:35.040]   which says for the second convolution operation
[01:05:35.040 --> 01:05:39.120]   we have a kernel size of input channels 30.
[01:05:39.120 --> 01:05:42.880]   So remember, this is my first con.
[01:05:42.880 --> 01:05:46.560]   This is my second con.
[01:05:46.560 --> 01:05:50.640]   This is my out
[01:05:50.640 --> 01:05:54.520]   from the first con.
[01:05:54.520 --> 01:05:57.120]   This is the out one.
[01:05:57.120 --> 01:05:57.960]   Okay.
[01:05:57.960 --> 01:06:00.840]   So then the second convolution is saying,
[01:06:01.680 --> 01:06:06.200]   hey, PyTorch, please apply a convolution of 30 input channels
[01:06:06.200 --> 01:06:08.400]   and then have just one such filter.
[01:06:08.400 --> 01:06:10.160]   So what's that gonna do?
[01:06:10.160 --> 01:06:13.400]   When you apply this to this,
[01:06:13.400 --> 01:06:16.280]   you're going to get a single output, right?
[01:06:16.280 --> 01:06:19.200]   'Cause it's gonna apply these 30 channels, 30 channels
[01:06:19.200 --> 01:06:20.480]   and then it's gonna apply,
[01:06:20.480 --> 01:06:24.560]   and then it's gonna apply, sorry, the kernel size is three.
[01:06:24.560 --> 01:06:26.000]   So you get output off.
[01:06:26.000 --> 01:06:31.560]   You just get the output of like this, 28 by 28.
[01:06:31.560 --> 01:06:33.800]   So for each position, when this gets applied,
[01:06:33.800 --> 01:06:35.640]   'cause this is three by three,
[01:06:35.640 --> 01:06:38.400]   so you have like being applied three by three,
[01:06:38.400 --> 01:06:42.240]   then to this point, three by three, so on, so on, so on,
[01:06:42.240 --> 01:06:44.040]   you get the output of 28 by 28.
[01:06:44.040 --> 01:06:47.840]   Correct?
[01:06:47.840 --> 01:06:49.440]   That's what this things look like.
[01:06:49.440 --> 01:06:50.280]   Oh, look at that.
[01:06:50.280 --> 01:06:52.720]   And exactly, that's it.
[01:06:52.720 --> 01:06:57.200]   I swear when I was going through this notebook,
[01:06:57.200 --> 01:07:00.080]   I didn't think of the output shapes in such detail
[01:07:00.080 --> 01:07:03.160]   and I never thought that I would explain this chapter
[01:07:03.160 --> 01:07:04.520]   by like drawing these things
[01:07:04.520 --> 01:07:07.880]   or like looking at the input shapes and output shapes.
[01:07:07.880 --> 01:07:11.560]   But now that this whole operation of like this broken CNN
[01:07:11.560 --> 01:07:13.560]   can now be represented by this image.
[01:07:13.560 --> 01:07:15.600]   And I hope it makes sense
[01:07:15.600 --> 01:07:19.600]   as to why the output is 64 by one by 28 by 28.
[01:07:19.600 --> 01:07:21.840]   So...
[01:07:21.840 --> 01:07:28.720]   Okay, so I hope that this makes sense
[01:07:28.720 --> 01:07:31.440]   that the output is 64 by one by 28 by 28.
[01:07:31.440 --> 01:07:33.160]   In case it's unclear, let me know.
[01:07:33.160 --> 01:07:36.320]   So let's leave it at that.
[01:07:36.320 --> 01:07:38.640]   So what you can do is, this is just refactoring,
[01:07:38.640 --> 01:07:40.920]   like you can just define a conf operation
[01:07:40.920 --> 01:07:42.720]   that takes in some number of input filters
[01:07:42.720 --> 01:07:44.320]   and says number of output filters.
[01:07:44.320 --> 01:07:46.640]   So you don't have to like define convolution like this.
[01:07:46.640 --> 01:07:48.400]   You could just say conf,
[01:07:48.400 --> 01:07:51.320]   you could say something like conf number input filters is 30,
[01:07:51.320 --> 01:07:54.120]   output filters is 30.
[01:07:54.120 --> 01:07:56.680]   So that would define this.
[01:07:56.680 --> 01:07:58.600]   Sorry, this is doing forward, don't worry about it.
[01:07:58.600 --> 01:08:00.640]   But all I'm trying to say is like,
[01:08:00.640 --> 01:08:05.640]   all this is doing is just creating like this,
[01:08:05.640 --> 01:08:07.840]   is first it's creating a convolution
[01:08:07.840 --> 01:08:10.400]   and then it's just returning my result.
[01:08:10.400 --> 01:08:13.960]   So, sorry, actually what it is doing is like,
[01:08:13.960 --> 01:08:18.320]   it's creating my convolution, adding value on top.
[01:08:18.320 --> 01:08:19.560]   Fair enough.
[01:08:19.560 --> 01:08:24.560]   So then why did it say, what if I go one by three?
[01:08:24.560 --> 01:08:25.880]   Oh yeah, that's fine.
[01:08:25.880 --> 01:08:27.720]   I hadn't defined it properly.
[01:08:28.240 --> 01:08:32.880]   So I could just, this is just the convenience of like,
[01:08:32.880 --> 01:08:35.560]   instead of writing conf to the end value,
[01:08:35.560 --> 01:08:38.320]   you could define these two things in this function.
[01:08:38.320 --> 01:08:41.880]   So if I say my input channel is one, output channel is 30,
[01:08:41.880 --> 01:08:43.560]   it just defines this sequential.
[01:08:43.560 --> 01:08:45.400]   So this is just a PyTorch convenience.
[01:08:45.400 --> 01:08:48.600]   So now I could define my simple CNN.
[01:08:48.600 --> 01:08:51.120]   Sequential just means like you apply operations
[01:08:51.120 --> 01:08:53.000]   one by one by one.
[01:08:53.000 --> 01:08:56.080]   So this con is just going from like single input channel
[01:08:56.080 --> 01:08:59.360]   to four, 'cause you have four kernels that get applied.
[01:08:59.360 --> 01:09:01.000]   Then it's going from four to eight,
[01:09:01.000 --> 01:09:03.800]   it's going from eight to 16 and so on.
[01:09:03.800 --> 01:09:06.120]   And remember in each of these places,
[01:09:06.120 --> 01:09:08.440]   you have a padding of one.
[01:09:08.440 --> 01:09:11.640]   So that's completely fine.
[01:09:11.640 --> 01:09:13.600]   So then you start and this just shows like
[01:09:13.600 --> 01:09:15.520]   what the output shapes are going to be.
[01:09:15.520 --> 01:09:17.480]   I'm not going to go into like the details
[01:09:17.480 --> 01:09:19.880]   of why the output shape is 14 by 14.
[01:09:19.880 --> 01:09:21.800]   I leave this as an exercise.
[01:09:21.800 --> 01:09:22.960]   So please come back.
[01:09:22.960 --> 01:09:24.960]   This is something you should do is like,
[01:09:25.960 --> 01:09:28.280]   oh, can you please explain how we go from 30 filters?
[01:09:28.280 --> 01:09:30.040]   I will do Kevin, just give me a one sec
[01:09:30.040 --> 01:09:32.000]   and I will go back to that.
[01:09:32.000 --> 01:09:35.440]   But one thing I do want to say is like as exercise,
[01:09:35.440 --> 01:09:39.000]   please come back to this forum and do something like this.
[01:09:39.000 --> 01:09:40.560]   For each of these operations,
[01:09:40.560 --> 01:09:43.360]   now I've told you how to like look at the output shapes
[01:09:43.360 --> 01:09:44.600]   and the input shapes.
[01:09:44.600 --> 01:09:46.200]   Please for each of these things,
[01:09:46.200 --> 01:09:51.000]   draw a drawing like this or like even on a piece of paper
[01:09:51.000 --> 01:09:54.800]   and then look why the output size is 14 by 14
[01:09:54.800 --> 01:09:57.040]   or look at it why the output size is seven by seven
[01:09:57.040 --> 01:09:58.560]   or four by four and so on.
[01:09:58.560 --> 01:10:02.960]   So we can define, Kevin, I'm going to come back to that
[01:10:02.960 --> 01:10:05.560]   just in a second, let me just wrap this up.
[01:10:05.560 --> 01:10:08.720]   So I'm just define, I define my simple CNN like this,
[01:10:08.720 --> 01:10:11.480]   which is just like the sequential of convolution operations
[01:10:11.480 --> 01:10:13.320]   followed by a value.
[01:10:13.320 --> 01:10:17.400]   So when I apply that to my input, I get this output 64 by two
[01:10:17.400 --> 01:10:19.760]   'cause remember the output is saying output channels
[01:10:19.760 --> 01:10:21.440]   of two by two.
[01:10:21.440 --> 01:10:24.600]   Finally, we can use the simple CNN in our learner.
[01:10:24.600 --> 01:10:27.480]   So this is just like the first lecture,
[01:10:27.480 --> 01:10:29.920]   you just have your learner, you have your data loader,
[01:10:29.920 --> 01:10:32.800]   you pass this in and you say, show me the learned on
[01:10:32.800 --> 01:10:35.520]   summary, so this is what my model looks like.
[01:10:35.520 --> 01:10:37.080]   And then you can say fit.
[01:10:37.080 --> 01:10:43.160]   And you see the accuracy is 0.99.
[01:10:43.160 --> 01:10:50.040]   So all we've done in this part is we've actually just
[01:10:50.040 --> 01:10:54.360]   created a convolution neural network from scratch.
[01:10:54.360 --> 01:10:56.240]   So what is a convolution neural network?
[01:10:56.240 --> 01:10:59.520]   It's just a network or a neural network that has convolution
[01:10:59.520 --> 01:11:01.520]   followed by some non-linearity.
[01:11:01.520 --> 01:11:04.320]   So see, in this case, we could classify,
[01:11:04.320 --> 01:11:06.640]   this is a big moment, like don't,
[01:11:06.640 --> 01:11:09.600]   I think I want to highlight how big of a moment this is.
[01:11:09.600 --> 01:11:13.000]   For everybody, each one of us,
[01:11:13.000 --> 01:11:17.160]   when we started our Fastbook journeys from the scratch,
[01:11:17.160 --> 01:11:21.080]   we just use things like ResNet,
[01:11:21.080 --> 01:11:23.440]   or we use things that were part of the learner.
[01:11:23.440 --> 01:11:26.080]   We never built a model architecture from scratch.
[01:11:26.080 --> 01:11:28.400]   So in this case, what we've done,
[01:11:28.400 --> 01:11:30.880]   even though it's a very simple, simple CNN, right?
[01:11:30.880 --> 01:11:32.520]   It's baby steps.
[01:11:32.520 --> 01:11:36.360]   But what we have done is we created our very first
[01:11:36.360 --> 01:11:39.480]   convolution neural network to do classification,
[01:11:39.480 --> 01:11:42.680]   and it can classify threes from,
[01:11:42.680 --> 01:11:44.960]   handwritten digit threes from sevens
[01:11:44.960 --> 01:11:47.440]   with a 99.11 accuracy.
[01:11:47.440 --> 01:11:50.640]   So I say well done to everybody who's followed me till here.
[01:11:50.640 --> 01:11:53.240]   So I think that's a really, really good step.
[01:11:53.240 --> 01:11:56.280]   And this is something to be proud of
[01:11:56.280 --> 01:11:59.240]   when we finish our chapter today.
[01:11:59.240 --> 01:12:02.040]   So now I'm going to go back to this point of like,
[01:12:02.040 --> 01:12:04.840]   can you explain how we go from 30 filters to one filter?
[01:12:04.840 --> 01:12:15.640]   So we're going back up.
[01:12:15.640 --> 01:12:17.800]   So remember we created like this small CNN,
[01:12:17.800 --> 01:12:20.800]   which looked like this, and I passed in my input.
[01:12:21.880 --> 01:12:24.160]   The first convolution operation
[01:12:24.160 --> 01:12:27.880]   had 30 kernels of one channel.
[01:12:27.880 --> 01:12:31.720]   So you have your input looks like this,
[01:12:31.720 --> 01:12:34.560]   one by 28 by 28, right?
[01:12:34.560 --> 01:12:35.800]   This is my input.
[01:12:35.800 --> 01:12:37.400]   And you have 30 kernels.
[01:12:37.400 --> 01:12:40.720]   So you have 30 kernels of three by three size.
[01:12:40.720 --> 01:12:45.480]   What we're doing is we're applying these 30 kernels,
[01:12:45.480 --> 01:12:48.520]   so your output becomes 30 by 28 by 28,
[01:12:48.520 --> 01:12:50.800]   'cause you had a padding of one, correct?
[01:12:50.800 --> 01:12:54.760]   So this is what my output looks like, 28 by 28,
[01:12:54.760 --> 01:12:56.000]   but 30 such things.
[01:12:56.000 --> 01:12:56.840]   So each of these,
[01:12:56.840 --> 01:13:00.080]   so it becomes like a 30 channel block, right?
[01:13:00.080 --> 01:13:05.960]   Now, my kernel in this second part was,
[01:13:05.960 --> 01:13:10.680]   it said the number of input channels is 30.
[01:13:10.680 --> 01:13:14.560]   I want one such kernel.
[01:13:14.560 --> 01:13:17.240]   So even though PyTorch calls this like out channels,
[01:13:17.240 --> 01:13:20.400]   I kind of look at it as the number of kernels.
[01:13:20.400 --> 01:13:23.080]   And the kernel size is three.
[01:13:23.080 --> 01:13:27.400]   So this is, let me make this smaller.
[01:13:27.400 --> 01:13:33.720]   So it's like this, right?
[01:13:33.720 --> 01:13:37.440]   You have 30 channels, three by three, correct?
[01:13:37.440 --> 01:13:40.400]   And then this kernel will get applied
[01:13:40.400 --> 01:13:43.360]   to the similar like a convolution operation here.
[01:13:43.360 --> 01:13:45.600]   So when this is being applied,
[01:13:45.600 --> 01:13:49.080]   it will start at a position over here like this,
[01:13:49.080 --> 01:13:51.800]   then it will move on to this, then so on like this.
[01:13:51.800 --> 01:13:54.120]   But remember now, instead of like my input
[01:13:54.120 --> 01:13:56.520]   being one channel, this is 30 channels.
[01:13:56.520 --> 01:14:00.480]   So the first time this kernel gets applied,
[01:14:00.480 --> 01:14:03.200]   let's say it's at the top left position,
[01:14:03.200 --> 01:14:05.000]   it's going to look like this, right?
[01:14:05.000 --> 01:14:08.880]   So it's going to get applied to the input image
[01:14:08.880 --> 01:14:12.200]   of 30 channels, and then this is 30 by three by three
[01:14:12.200 --> 01:14:13.040]   as well.
[01:14:13.040 --> 01:14:14.400]   So when you apply this whole thing,
[01:14:14.400 --> 01:14:16.680]   just don't worry about the depth.
[01:14:16.680 --> 01:14:17.800]   When you apply this whole thing,
[01:14:17.800 --> 01:14:20.440]   you're going to get output, which is one number.
[01:14:20.440 --> 01:14:24.800]   Does that make sense, Kevin?
[01:14:24.800 --> 01:14:26.600]   You're going to get an output of one number
[01:14:26.600 --> 01:14:28.640]   because the kernel has 30 channels.
[01:14:28.640 --> 01:14:31.600]   It's applying and doing the sum
[01:14:31.600 --> 01:14:34.720]   of like all those 30 channels all at once.
[01:14:34.720 --> 01:14:36.440]   So you get one number output.
[01:14:36.440 --> 01:14:38.080]   And because this kernel gets applied
[01:14:38.080 --> 01:14:40.760]   at like all these 28 by 28 positions,
[01:14:40.760 --> 01:14:45.760]   your output is a single channel, 28 by 28.
[01:14:45.760 --> 01:14:48.760]   (keyboard clicking)
[01:14:48.760 --> 01:14:51.280]   Could you...
[01:14:51.280 --> 01:14:54.000]   (keyboard clicking)
[01:14:54.000 --> 01:14:56.320]   Oh, how do we determine if it's a three or seven?
[01:14:56.320 --> 01:15:01.080]   Oh, in this case, this was like a broken CNN.
[01:15:01.080 --> 01:15:03.240]   This is, we're not using this for training
[01:15:03.240 --> 01:15:04.520]   for threes and sevens.
[01:15:04.520 --> 01:15:08.240]   What we're using for training is this simple CNN, right?
[01:15:08.240 --> 01:15:10.840]   So this was just a small CNN that we created.
[01:15:10.840 --> 01:15:14.240]   I just want to, this was just to like show
[01:15:14.240 --> 01:15:15.880]   like what exactly is going on
[01:15:15.880 --> 01:15:18.040]   and just to show input shapes and output shapes
[01:15:18.040 --> 01:15:20.560]   if you apply this to your input.
[01:15:20.560 --> 01:15:23.200]   But this is not the model that we use for,
[01:15:23.200 --> 01:15:25.200]   this is not the model that we use for learning.
[01:15:25.200 --> 01:15:30.200]   For the exact same reason, like the output is one filter.
[01:15:30.200 --> 01:15:33.160]   Like how are we classifying the threes and sevens?
[01:15:33.160 --> 01:15:34.280]   We are not.
[01:15:34.280 --> 01:15:38.960]   So we use this simple CNN that has an output of two.
[01:15:38.960 --> 01:15:41.000]   Okay, so then these becomes your probabilities.
[01:15:41.000 --> 01:15:45.400]   So just saying we're using simple CNN.
[01:15:45.400 --> 01:15:49.440]   All right, I think I will stop here
[01:15:49.440 --> 01:15:51.920]   even though I wanted to finish to 1.3.
[01:15:51.920 --> 01:15:55.920]   I will stop here.
[01:15:55.920 --> 01:16:01.360]   And when you go back, I think this week,
[01:16:01.360 --> 01:16:03.120]   'cause it's already time.
[01:16:03.120 --> 01:16:06.760]   When you go back this week, then do this, do the following.
[01:16:06.760 --> 01:16:08.600]   And you will really feel like
[01:16:09.880 --> 01:16:11.160]   you've learned a lot from this,
[01:16:11.160 --> 01:16:13.360]   from the exercise that I'm about to tell you.
[01:16:13.360 --> 01:16:14.280]   So do this.
[01:16:14.280 --> 01:16:17.400]   When you go back, just have a look at this simple CNN.
[01:16:17.400 --> 01:16:22.800]   Like I'm drawing these images for, to explain like
[01:16:22.800 --> 01:16:25.000]   when I had an input of 28 by 28,
[01:16:25.000 --> 01:16:28.280]   then why does the output have one filter and so on?
[01:16:28.280 --> 01:16:32.160]   Like I draw the output of using this broken CNN.
[01:16:32.160 --> 01:16:34.520]   You do the same thing for this simple CNN.
[01:16:34.520 --> 01:16:39.080]   So go back and figure out why the output is 64 by two.
[01:16:39.080 --> 01:16:44.040]   Okay, I want somebody to post in the forums
[01:16:44.040 --> 01:16:48.800]   and just say like why the output is 64 by two, okay?
[01:16:48.800 --> 01:16:54.840]   And we will get back to the RGB.
[01:16:54.840 --> 01:16:58.520]   I'm so sorry, Vinayak, I kept saying we're also,
[01:16:58.520 --> 01:16:59.360]   oh, that's correct.
[01:16:59.360 --> 01:17:00.280]   Yes, you've got it though.
[01:17:00.280 --> 01:17:02.440]   I won't explain it now, but you've got it.
[01:17:02.440 --> 01:17:04.080]   That's a good understanding.
[01:17:04.080 --> 01:17:05.400]   I'm sorry, Vinayak, I kept saying
[01:17:05.400 --> 01:17:07.400]   we're gonna look at RGB later, later, later,
[01:17:07.400 --> 01:17:11.760]   but we're out of time and we're gonna look at RGB next week.
[01:17:11.760 --> 01:17:14.840]   So thanks everybody for joining me
[01:17:14.840 --> 01:17:16.840]   and I will see you guys next week.
[01:17:16.840 --> 01:17:19.560]   If you have any questions, actually, please post them here
[01:17:19.560 --> 01:17:21.440]   and I will answer them.
[01:17:21.440 --> 01:17:24.000]   But thanks for now and I'll see you guys next week.
[01:17:24.000 --> 01:17:27.580]   (computer mouse clicking)
[01:17:27.580 --> 01:17:31.160]   (computer mouse clicking)
[01:17:31.160 --> 01:17:34.660]   (computer mouse clicking)
[01:17:34.660 --> 01:17:38.160]   (computer mouse clicking)
[01:17:38.160 --> 01:17:40.160]   Thank you.

