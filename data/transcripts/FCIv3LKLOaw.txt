
[00:00:00.000 --> 00:00:04.400]   - Yeah, sure, yeah thanks for inviting me, Noah and Sean.
[00:00:04.400 --> 00:00:06.220]   It's always a pleasure.
[00:00:06.220 --> 00:00:11.560]   And yeah, I guess my background is studied neural networks,
[00:00:11.560 --> 00:00:13.560]   sort of how to optimize them,
[00:00:13.560 --> 00:00:16.080]   how to prove they converge in grad school,
[00:00:16.080 --> 00:00:19.120]   then worked at, that was at Berkeley,
[00:00:19.120 --> 00:00:21.360]   joined Weights and Biases,
[00:00:21.360 --> 00:00:24.920]   the experiment management and MLOps startup,
[00:00:24.920 --> 00:00:29.760]   series A to series C, did education for them,
[00:00:29.760 --> 00:00:32.960]   then did a full stack deep learning online course
[00:00:32.960 --> 00:00:37.240]   about how to deploy models in the pre-foundation model
[00:00:37.240 --> 00:00:39.640]   or liminal foundation model era,
[00:00:39.640 --> 00:00:43.720]   and then now work for Modal, infrastructure company
[00:00:43.720 --> 00:00:47.520]   that helps people run data-intensive workloads
[00:00:47.520 --> 00:00:49.200]   like ML inference.
[00:00:49.200 --> 00:00:54.080]   So, yeah, so then, oh yeah,
[00:00:54.080 --> 00:00:58.200]   so FSDL fans in the chat, like Sean,
[00:00:58.200 --> 00:01:00.460]   yeah, we'd love, maybe we'll be able to do something
[00:01:00.460 --> 00:01:02.560]   under that banner again sometime.
[00:01:02.560 --> 00:01:06.920]   But yeah, so wanted to talk today
[00:01:06.920 --> 00:01:10.400]   about running and fine-tuning open-source language models.
[00:01:10.400 --> 00:01:12.360]   Why would you do it?
[00:01:12.360 --> 00:01:15.720]   The answer is not always with both of these things.
[00:01:15.720 --> 00:01:17.840]   And then like some things about how,
[00:01:17.840 --> 00:01:19.240]   some like high-level things.
[00:01:19.240 --> 00:01:23.000]   This course, my understanding is oriented
[00:01:23.000 --> 00:01:24.800]   at software engineers who wanna learn more
[00:01:24.800 --> 00:01:27.080]   about like running AI models
[00:01:27.080 --> 00:01:29.480]   and building systems around them.
[00:01:29.480 --> 00:01:30.960]   So that's kind of the background
[00:01:30.960 --> 00:01:32.800]   that I've assumed in a lot of these.
[00:01:32.800 --> 00:01:37.160]   And then, yeah, to actually kick us off,
[00:01:37.160 --> 00:01:38.560]   before we go through the slides,
[00:01:38.560 --> 00:01:40.960]   I'm actually gonna do a quick demo.
[00:01:40.960 --> 00:01:44.480]   This is something that I got set up just yesterday,
[00:01:44.480 --> 00:01:48.840]   but like since it is, you know, in the news,
[00:01:48.840 --> 00:01:51.640]   like quite literally,
[00:01:53.440 --> 00:01:55.080]   let's run a local model
[00:01:55.080 --> 00:01:58.320]   or rather run our own inference on a model.
[00:01:58.320 --> 00:02:02.760]   Let's run this DeepSeq R1 model
[00:02:02.760 --> 00:02:05.000]   that people keep talking about.
[00:02:05.000 --> 00:02:09.400]   So this is coming from the modal, our examples repo.
[00:02:09.400 --> 00:02:11.280]   So you can try this code out yourself.
[00:02:11.280 --> 00:02:13.920]   All you need is like a single Python file
[00:02:13.920 --> 00:02:17.600]   and a modal account, and you're ready to go.
[00:02:17.600 --> 00:02:19.360]   I'm gonna kick it off here.
[00:02:19.360 --> 00:02:22.720]   Oops, you need a virtual environment with Python in it.
[00:02:22.720 --> 00:02:24.360]   That's one thing you need, I suppose.
[00:02:24.360 --> 00:02:26.320]   I forgot to mention that.
[00:02:26.320 --> 00:02:29.600]   Okay, so let's run this guy here in the terminal.
[00:02:29.600 --> 00:02:30.440]   It's my VS code.
[00:02:30.440 --> 00:02:31.400]   The code's up there.
[00:02:31.400 --> 00:02:35.040]   You know, as is often the case,
[00:02:35.040 --> 00:02:37.000]   the code is not supremely interesting.
[00:02:37.000 --> 00:02:42.000]   I'm pulling in, I'm running it with Llama CPP here.
[00:02:42.000 --> 00:02:46.880]   Llama CPP has very, very low precision quants.
[00:02:46.880 --> 00:02:49.760]   So there's a ternary quant of DeepSeq R1.
[00:02:49.760 --> 00:02:52.440]   That means all the values are either minus one, zero,
[00:02:52.440 --> 00:02:54.680]   or one in the weights.
[00:02:54.680 --> 00:02:57.320]   And that's enough to squeeze it down to fit
[00:02:57.320 --> 00:03:01.000]   on a single machine with multiple GPUs.
[00:03:01.000 --> 00:03:03.640]   In this case, four L40S GPUs.
[00:03:03.640 --> 00:03:06.920]   So that's why I'm running with Llama CPP here.
[00:03:06.920 --> 00:03:11.080]   So let's see, spinning up right now.
[00:03:11.080 --> 00:03:12.280]   Oh man, we're out of it.
[00:03:12.280 --> 00:03:14.800]   Four XL40Ss on modal, so we might have to wait
[00:03:14.800 --> 00:03:19.800]   as many as 15 or 30 seconds for that to spin up.
[00:03:19.800 --> 00:03:21.280]   While we're waiting for that,
[00:03:21.280 --> 00:03:23.640]   let me show you just a little bit
[00:03:23.640 --> 00:03:25.800]   about what's going on here.
[00:03:25.800 --> 00:03:28.680]   We're running Llama CPP here.
[00:03:28.680 --> 00:03:35.840]   Running these things is an exercise in configuration.
[00:03:35.840 --> 00:03:38.240]   So if you've ever administered a database,
[00:03:38.240 --> 00:03:40.960]   you'll be familiar with this sort of thing.
[00:03:40.960 --> 00:03:44.880]   Or if you've run compilation for a serious large project,
[00:03:44.880 --> 00:03:50.840]   you got your mysterious flags with mysterious arguments
[00:03:50.840 --> 00:03:54.320]   that have meaningful impact on the performance.
[00:03:54.320 --> 00:03:56.600]   So controlling, in this case, the KV cache
[00:03:56.600 --> 00:03:59.600]   and setting the quantization precision of that,
[00:03:59.600 --> 00:04:02.640]   along with some other things for Llama CPP.
[00:04:02.640 --> 00:04:04.800]   Okay, so we had about a minute queue for GPUs.
[00:04:04.800 --> 00:04:08.880]   That's actually, that's like a P95 probably
[00:04:08.880 --> 00:04:10.960]   for XL40Ss on modal.
[00:04:10.960 --> 00:04:14.520]   So sometimes you roll the dice and you get a natural one.
[00:04:14.520 --> 00:04:18.280]   But, so it took us about 60 seconds maybe to spin this up
[00:04:18.280 --> 00:04:21.280]   and get a hold of four XL40S GPUs.
[00:04:21.280 --> 00:04:23.480]   If this happens to you, DM me
[00:04:23.480 --> 00:04:26.720]   and I'll go smack our GPU cluster with a hammer
[00:04:26.720 --> 00:04:29.920]   and try and make it go faster.
[00:04:29.920 --> 00:04:31.960]   All right, so this is loading up,
[00:04:31.960 --> 00:04:36.960]   loading up all the stuff you need to do Llama to run.
[00:04:37.680 --> 00:04:41.000]   DeepSeq R1, this is the model loader.
[00:04:41.000 --> 00:04:43.280]   Actually, it turns out it's about 100 something gigabytes
[00:04:43.280 --> 00:04:45.440]   once you've quantized it down this far.
[00:04:45.440 --> 00:04:47.960]   These are all different layers here.
[00:04:47.960 --> 00:04:50.080]   Nothing too interesting in the model
[00:04:50.080 --> 00:04:51.360]   of like architecture itself.
[00:04:51.360 --> 00:04:53.920]   It's really the like data and the inference tech
[00:04:53.920 --> 00:04:57.800]   that DeepSeq built that's really the interesting part.
[00:04:57.800 --> 00:05:00.120]   So skipping past all this extra stuff.
[00:05:00.120 --> 00:05:02.720]   So now we're at the point where we're like loading the model.
[00:05:02.720 --> 00:05:04.840]   This one, so you're running,
[00:05:04.840 --> 00:05:07.360]   you wanna run your own model, great, okay.
[00:05:07.360 --> 00:05:10.040]   If you have a GPU to have it on all the time,
[00:05:10.040 --> 00:05:12.160]   then you gotta have, you know,
[00:05:12.160 --> 00:05:14.920]   why do we have four GPUs, why not just one?
[00:05:14.920 --> 00:05:16.560]   We gotta have 100 gigabytes of space
[00:05:16.560 --> 00:05:18.960]   to hold all the weights for this thing in.
[00:05:18.960 --> 00:05:21.640]   That's 100 something gigabytes of RAM.
[00:05:21.640 --> 00:05:23.880]   That, you know, problem with RAM is like,
[00:05:23.880 --> 00:05:26.960]   you can't share RAM and when you unplug it,
[00:05:26.960 --> 00:05:28.480]   the data goes out of it.
[00:05:28.480 --> 00:05:30.860]   So this is actually one of the major cost sources
[00:05:30.860 --> 00:05:32.960]   for running your own model.
[00:05:32.960 --> 00:05:35.360]   It's like, you gotta have, if you want,
[00:05:35.360 --> 00:05:37.040]   like, if you wanna avoid this latency
[00:05:37.040 --> 00:05:40.040]   that we're looking at here for of like, you know,
[00:05:40.040 --> 00:05:45.040]   what's it about, you know, minute, 90 seconds to spin up.
[00:05:45.040 --> 00:05:49.520]   That's like separate from any like modal overhead.
[00:05:49.520 --> 00:05:53.120]   This is just the raw moving bytes around setting up RAM.
[00:05:53.120 --> 00:05:56.640]   If you wanna avoid that, you gotta have stuff hot in RAM
[00:05:56.640 --> 00:05:57.800]   and RAM is not free.
[00:05:57.800 --> 00:06:01.000]   That's, you either, you know,
[00:06:01.000 --> 00:06:03.860]   you gotta pay to keep something warm
[00:06:03.860 --> 00:06:05.240]   on a serverless provider like modal
[00:06:05.240 --> 00:06:08.320]   or you gotta have an instance running in the cloud.
[00:06:08.320 --> 00:06:10.200]   But that's all, that's been done.
[00:06:10.200 --> 00:06:11.760]   We're now doing prompt processing.
[00:06:11.760 --> 00:06:14.560]   This is the prompt, Unsloth, by the way,
[00:06:14.560 --> 00:06:16.920]   is the team that did the quantization here
[00:06:16.920 --> 00:06:19.640]   for DeepSeek R1 down to three bits.
[00:06:19.640 --> 00:06:22.880]   And they, their demo prompt
[00:06:22.880 --> 00:06:25.840]   is what I've just like copied directly here.
[00:06:25.840 --> 00:06:29.320]   It's, oh yeah, the prints mess up a little bit
[00:06:29.320 --> 00:06:30.580]   sometimes here.
[00:06:32.520 --> 00:06:34.520]   But there should be at the top,
[00:06:34.520 --> 00:06:37.280]   the beginning of the prompt is something like,
[00:06:37.280 --> 00:06:39.980]   please write the game Flappy Bird in Python.
[00:06:39.980 --> 00:06:44.800]   So that's the prompt along with some instructions.
[00:06:44.800 --> 00:06:46.840]   That prompt has gone into DeepSeek
[00:06:46.840 --> 00:06:48.800]   and is now being processed.
[00:06:48.800 --> 00:06:52.240]   Okay, prompt processing is done.
[00:06:52.240 --> 00:06:56.880]   And now the beloved think token has been emitted
[00:06:56.880 --> 00:06:59.240]   and the model has begun to think about it,
[00:06:59.240 --> 00:07:01.120]   what it wants to do.
[00:07:01.120 --> 00:07:04.960]   So this deployment is not super well optimized.
[00:07:04.960 --> 00:07:06.960]   There's a substantial amount of host overhead,
[00:07:06.960 --> 00:07:09.600]   which means the GPU is not actually working all the time,
[00:07:09.600 --> 00:07:11.620]   even as we're generating these tokens.
[00:07:11.620 --> 00:07:16.620]   That's probably either a Llama CPP needs like a PR
[00:07:16.620 --> 00:07:19.800]   or I missed some compiler flag or something.
[00:07:19.800 --> 00:07:21.780]   The CPU usage is also kind of low.
[00:07:21.780 --> 00:07:23.800]   So I'm suspicious that maybe I messed something up
[00:07:23.800 --> 00:07:25.240]   in the compile.
[00:07:25.240 --> 00:07:27.760]   So it's 10 tokens per second right now.
[00:07:27.760 --> 00:07:28.780]   There's line buffering.
[00:07:28.780 --> 00:07:30.480]   So you aren't seeing the tokens live.
[00:07:30.480 --> 00:07:33.080]   You see them once a line is emitted.
[00:07:33.080 --> 00:07:37.120]   But yeah, but runs about 10 tokens per second
[00:07:37.120 --> 00:07:41.960]   on these L40s GPUs and could probably be boosted up
[00:07:41.960 --> 00:07:44.800]   to about 50 tokens per second
[00:07:44.800 --> 00:07:47.200]   by removing some of this host overhead.
[00:07:47.200 --> 00:07:50.320]   And then from there probably optimize kernels
[00:07:50.320 --> 00:07:51.520]   for this architecture.
[00:07:51.520 --> 00:07:53.760]   Some other things would maybe double it again.
[00:07:53.760 --> 00:07:55.920]   Oh, finished thinking pretty quickly that time.
[00:07:55.920 --> 00:07:58.160]   Interesting thing with these models is like,
[00:07:58.160 --> 00:08:01.480]   they think for very variable amounts of time
[00:08:01.480 --> 00:08:04.720]   controlled by how hard they think the problem is.
[00:08:04.720 --> 00:08:07.360]   And so sometimes it finishes thinking pretty quickly
[00:08:07.360 --> 00:08:08.180]   like here.
[00:08:08.180 --> 00:08:10.740]   Sometimes it thinks for like 20 minutes.
[00:08:10.740 --> 00:08:12.920]   So, you know, go make a coffee.
[00:08:12.920 --> 00:08:15.520]   I don't know, go compile something else
[00:08:15.520 --> 00:08:17.500]   while it's writing your answer.
[00:08:17.500 --> 00:08:23.000]   And yeah, I think the quality of the output here
[00:08:23.000 --> 00:08:24.440]   is reasonably good.
[00:08:24.440 --> 00:08:27.920]   One thing the Unsloft people call out
[00:08:27.920 --> 00:08:29.760]   and I've noticed in a couple of generations
[00:08:29.760 --> 00:08:31.600]   is these super low bit quants
[00:08:31.600 --> 00:08:35.120]   sometimes throw in a random junk token.
[00:08:35.120 --> 00:08:38.760]   So this case here, I bet that dense there is like,
[00:08:38.760 --> 00:08:40.400]   that might not be defined.
[00:08:40.400 --> 00:08:42.480]   Yeah, it doesn't look like that's defined.
[00:08:42.480 --> 00:08:44.520]   Rough, that's probably supposed to be a zero.
[00:08:44.520 --> 00:08:46.080]   There's some inference config stuff
[00:08:46.080 --> 00:08:48.120]   that I haven't played with that can reduce that
[00:08:48.120 --> 00:08:49.600]   and improve the quality,
[00:08:49.600 --> 00:08:52.280]   but that comes with the quantization territory.
[00:08:52.280 --> 00:08:54.800]   Yeah, so there it goes.
[00:08:54.800 --> 00:08:56.320]   It thought about it for a while,
[00:08:56.320 --> 00:08:57.960]   did some backtracking,
[00:08:57.960 --> 00:09:01.000]   and then wrote some code for Flappy Bird.
[00:09:01.000 --> 00:09:03.400]   So there's running a model
[00:09:03.400 --> 00:09:05.160]   and some of the stuff I talked about along the way,
[00:09:05.160 --> 00:09:06.960]   some of the main concerns you're gonna have
[00:09:06.960 --> 00:09:09.020]   running your own model.
[00:09:09.020 --> 00:09:13.000]   Yeah, any questions before I dive into the slides?
[00:09:13.000 --> 00:09:15.880]   >> Really quickly, so we haven't gotten
[00:09:15.880 --> 00:09:17.360]   super into the internals.
[00:09:17.360 --> 00:09:19.800]   Could you go over what quantization is?
[00:09:19.800 --> 00:09:21.360]   Like why do you need to do that
[00:09:21.360 --> 00:09:24.080]   and what that process generally looks like?
[00:09:24.080 --> 00:09:25.720]   >> Yeah, sure, I'll talk more about this,
[00:09:25.720 --> 00:09:28.640]   but the basic answer is the model is trained
[00:09:28.640 --> 00:09:31.320]   as a bunch of floating-point numbers.
[00:09:31.320 --> 00:09:34.320]   For DeepSeq R1, they were eight-bit floating-point numbers.
[00:09:34.320 --> 00:09:35.560]   That's crazy small.
[00:09:35.560 --> 00:09:39.980]   They worked hard to reduce the overhead during training.
[00:09:39.980 --> 00:09:41.480]   More typical is 16-bit
[00:09:41.480 --> 00:09:44.260]   or even 32-bit floating-point numbers.
[00:09:44.260 --> 00:09:48.800]   Default in Python is often 64-bit floating-point numbers,
[00:09:48.800 --> 00:09:53.800]   but that's way too much for neural networks most of the time.
[00:09:55.400 --> 00:09:57.200]   They don't need that level of precision.
[00:09:57.200 --> 00:09:59.200]   So you can save a lot on memory,
[00:09:59.200 --> 00:10:01.920]   which saves a ton on inference speed,
[00:10:01.920 --> 00:10:06.920]   especially in these low user count settings
[00:10:06.920 --> 00:10:09.640]   and heavy decode settings,
[00:10:09.640 --> 00:10:12.240]   like a reasoning model that produces a ton of tokens
[00:10:12.240 --> 00:10:14.080]   in response to a single input.
[00:10:14.080 --> 00:10:17.360]   Yeah, that helps a lot to decrease
[00:10:17.360 --> 00:10:19.520]   the memory footprint of the weights,
[00:10:19.520 --> 00:10:23.280]   even if that's all you do with your quantization.
[00:10:24.680 --> 00:10:27.000]   But yeah, so there we go.
[00:10:27.000 --> 00:10:28.320]   It actually finished.
[00:10:28.320 --> 00:10:31.640]   I noticed a couple more typos in there.
[00:10:31.640 --> 00:10:32.760]   That's probably, yeah,
[00:10:32.760 --> 00:10:34.400]   should tune the inference config on there.
[00:10:34.400 --> 00:10:37.480]   There's like a top-P, min-P sampling thing
[00:10:37.480 --> 00:10:40.140]   that you can do that I haven't dialed in yet.
[00:10:40.140 --> 00:10:42.880]   But yeah.
[00:10:42.880 --> 00:10:49.380]   Yeah, anybody, any other questions before we dive in?
[00:10:53.640 --> 00:10:56.480]   Yeah, feel free to interrupt me as we're going.
[00:10:56.480 --> 00:10:58.520]   Let's make this a little bit more
[00:10:58.520 --> 00:11:00.440]   on the interactive side, ideally.
[00:11:00.440 --> 00:11:01.960]   Got a lot of slides, a lot to cover,
[00:11:01.960 --> 00:11:06.400]   but what we cover depends on
[00:11:06.400 --> 00:11:08.160]   what people are most interested in.
[00:11:08.160 --> 00:11:14.320]   Okay, so I just ran an open-source language model,
[00:11:14.320 --> 00:11:15.480]   DeepSeq R1.
[00:11:15.480 --> 00:11:17.080]   Let's talk about just in general,
[00:11:17.080 --> 00:11:18.080]   what does that take?
[00:11:18.080 --> 00:11:20.200]   Define some of the things that I talked about there,
[00:11:20.200 --> 00:11:22.760]   like memory, bandwidth constraints,
[00:11:22.760 --> 00:11:26.120]   and quantization, and all this other stuff.
[00:11:26.120 --> 00:11:28.520]   We'll also talk a bit about fine-tuning models,
[00:11:28.520 --> 00:11:30.120]   customizing them to your use case
[00:11:30.120 --> 00:11:32.760]   by doing actual machine learning and training.
[00:11:32.760 --> 00:11:35.120]   Before doing that,
[00:11:35.120 --> 00:11:37.480]   I do want to talk about the why of this here,
[00:11:37.480 --> 00:11:39.080]   like just 'cause something is,
[00:11:39.080 --> 00:11:41.800]   even if something's quick, easy, and free,
[00:11:41.800 --> 00:11:43.360]   it doesn't mean it's a good idea.
[00:11:43.360 --> 00:11:45.340]   And running and fine-tuning your own models
[00:11:45.340 --> 00:11:47.640]   is none of those things.
[00:11:47.640 --> 00:11:50.400]   So you want to make sure you have a good idea
[00:11:50.400 --> 00:11:52.440]   why you want to do this.
[00:11:52.440 --> 00:11:56.540]   Why not just use a managed service behind an API?
[00:11:56.540 --> 00:11:59.200]   One of the primary reasons to do it
[00:11:59.200 --> 00:12:01.440]   is if you don't need Frontier capabilities,
[00:12:01.440 --> 00:12:04.140]   so if you don't need to run DeepSeq R1
[00:12:04.140 --> 00:12:07.680]   to get reasoning traces for your customer support JapPot,
[00:12:07.680 --> 00:12:10.640]   that just needs to ask them to turn the thing off
[00:12:10.640 --> 00:12:11.940]   and turn it back on again.
[00:12:11.940 --> 00:12:14.600]   That level of LLM inference,
[00:12:14.600 --> 00:12:16.760]   the software is pretty commodity.
[00:12:16.760 --> 00:12:20.000]   The hardware to run it's getting easier and cheaper.
[00:12:20.000 --> 00:12:25.000]   And so you can frequently run that relatively inexpensively.
[00:12:25.000 --> 00:12:28.720]   And so you don't need a proprietary model,
[00:12:28.720 --> 00:12:32.220]   and you can often, the complexity of serving is lower.
[00:12:32.220 --> 00:12:35.480]   Just like a call-out on that DeepSeq R1 demo,
[00:12:35.480 --> 00:12:39.240]   there's probably an order of magnitude and a half
[00:12:39.240 --> 00:12:41.520]   of improvement that could be done to that.
[00:12:41.520 --> 00:12:44.320]   So a 30x improvement is probably low-hanging fruit,
[00:12:44.320 --> 00:12:45.780]   like a week of engineering.
[00:12:45.780 --> 00:12:51.720]   But right now, running that on Modal is $300 a megatoken.
[00:12:51.720 --> 00:12:57.340]   And just having DeepSeq run it for you is $3 a megatoken.
[00:12:57.340 --> 00:13:02.340]   So that's a pretty big difference,
[00:13:02.340 --> 00:13:07.600]   even assuming we can get a 30x cost reduction running
[00:13:07.600 --> 00:13:12.560]   just by doing more than a day's engineering
[00:13:12.560 --> 00:13:14.080]   to get it running.
[00:13:14.080 --> 00:13:17.320]   So that's a reason people sometimes think
[00:13:17.320 --> 00:13:19.040]   running their own LLM inference makes sense
[00:13:19.040 --> 00:13:20.120]   is to save money.
[00:13:20.120 --> 00:13:21.200]   And that intuition, I think,
[00:13:21.200 --> 00:13:24.680]   comes from getting fleeced by cloud providers
[00:13:24.680 --> 00:13:27.080]   who will charge you an arm and a leg
[00:13:27.080 --> 00:13:30.920]   to just stand up commodity Redis on commodity hardware.
[00:13:30.920 --> 00:13:33.420]   But right now, that's not the case.
[00:13:33.420 --> 00:13:37.520]   So the main reason I think that people bring up
[00:13:37.520 --> 00:13:40.120]   is to manage security and improve data governance.
[00:13:40.120 --> 00:13:42.560]   You want to make sure to run this thing yourself.
[00:13:44.000 --> 00:13:45.340]   The more control you want,
[00:13:45.340 --> 00:13:47.960]   the more complex this problem is going to be,
[00:13:47.960 --> 00:13:52.960]   the more eventually it ends up getting your own GPUs
[00:13:52.960 --> 00:13:54.880]   and putting them in a cage,
[00:13:54.880 --> 00:13:59.020]   which is probably six months or a year of engineering work,
[00:13:59.020 --> 00:14:02.400]   and then a lot of ongoing maintenance.
[00:14:02.400 --> 00:14:04.700]   But at the very least, running it with a cloud provider,
[00:14:04.700 --> 00:14:08.360]   whether that's Modal or raw dogging AWS,
[00:14:08.360 --> 00:14:11.660]   can improve your security and data governance posture.
[00:14:11.660 --> 00:14:14.580]   Not everybody wants to send data
[00:14:14.580 --> 00:14:17.980]   to untrustworthy nation states
[00:14:17.980 --> 00:14:19.820]   like the United States or China.
[00:14:19.820 --> 00:14:23.340]   Then gaining control over inference
[00:14:23.340 --> 00:14:25.980]   is maybe the one that I would say is most important.
[00:14:25.980 --> 00:14:31.060]   It's like, and most general, it's like API providers,
[00:14:31.060 --> 00:14:33.120]   there's only so much they can do.
[00:14:33.120 --> 00:14:35.420]   If they're proprietary, they got to hide stuff from you,
[00:14:35.420 --> 00:14:38.180]   whether that's reasoning chains, in OpenAI's case,
[00:14:38.180 --> 00:14:41.180]   or like log probs, also in OpenAI's case,
[00:14:41.180 --> 00:14:43.500]   or just like the increased customization
[00:14:43.500 --> 00:14:46.140]   decreases their ability to amortize work,
[00:14:46.140 --> 00:14:48.480]   to spread it across multiple customers,
[00:14:48.480 --> 00:14:50.780]   which is the way that they get things cheaper
[00:14:50.780 --> 00:14:53.380]   than you can run it yourself, sort of economies of scale.
[00:14:53.380 --> 00:14:55.640]   And so the more flexible,
[00:14:55.640 --> 00:14:57.460]   the more different your deployment is,
[00:14:57.460 --> 00:15:00.400]   the harder it is for them to do that,
[00:15:00.400 --> 00:15:06.700]   to run this variety of workloads economically.
[00:15:07.500 --> 00:15:10.980]   I think over time, all of these things
[00:15:10.980 --> 00:15:13.300]   are going to lean more in the favor
[00:15:13.300 --> 00:15:15.060]   of running your own LLM inference.
[00:15:15.060 --> 00:15:16.980]   Like frontier capabilities will go off
[00:15:16.980 --> 00:15:19.100]   in the direction of artificial super intelligence
[00:15:19.100 --> 00:15:23.140]   or whatever, but the baseline capabilities
[00:15:23.140 --> 00:15:25.100]   that anybody can just download off of Hugging Face
[00:15:25.100 --> 00:15:27.700]   will just keep on getting better.
[00:15:27.700 --> 00:15:31.020]   So we just saw reasoning, a one-level capabilities.
[00:15:31.020 --> 00:15:33.380]   Six months ago, I told people,
[00:15:33.380 --> 00:15:34.760]   "You got to go to OpenAI for that.
[00:15:34.760 --> 00:15:36.260]   "Now you can run it yourself."
[00:15:37.140 --> 00:15:38.340]   But I think the most important one
[00:15:38.340 --> 00:15:41.900]   that's going to tilt in the direction
[00:15:41.900 --> 00:15:44.180]   of running your own inference as the field matures
[00:15:44.180 --> 00:15:45.700]   is gaining control over inference.
[00:15:45.700 --> 00:15:48.180]   Like things are just going to get way more flexible.
[00:15:48.180 --> 00:15:49.980]   People are going to discover all kinds of crazy things
[00:15:49.980 --> 00:15:52.580]   you can do with like hacking the internals,
[00:15:52.580 --> 00:15:54.460]   with log probabilities.
[00:15:54.460 --> 00:15:56.340]   People will rediscover what everybody was doing
[00:15:56.340 --> 00:16:00.660]   in 2022 and 2023, when people still had access
[00:16:00.660 --> 00:16:03.220]   to the models internals,
[00:16:03.220 --> 00:16:05.780]   and discover that it makes their lives better.
[00:16:05.780 --> 00:16:07.620]   And you'll want to run your own inference
[00:16:07.620 --> 00:16:09.980]   for that, to control that.
[00:16:09.980 --> 00:16:12.480]   See a question, Juliette.
[00:16:12.480 --> 00:16:13.500]   - Yeah, Charles.
[00:16:13.500 --> 00:16:15.340]   So before we carry on, and I'm not sure
[00:16:15.340 --> 00:16:17.420]   if you're going to speak more about this as we go forward,
[00:16:17.420 --> 00:16:18.700]   but could you speak a bit about
[00:16:18.700 --> 00:16:20.860]   how inference is currently working,
[00:16:20.860 --> 00:16:23.720]   just to make it a bit more concrete in my mind?
[00:16:23.720 --> 00:16:27.100]   - When you say how inference is currently working,
[00:16:27.100 --> 00:16:28.900]   do you mean like how people normally,
[00:16:28.900 --> 00:16:30.700]   the alternative to running your own?
[00:16:30.700 --> 00:16:35.540]   - Well, you're saying that, and I'm not familiar,
[00:16:35.540 --> 00:16:37.380]   I'm not so familiar with the word inference itself.
[00:16:37.380 --> 00:16:39.620]   Like, could you share a bit about how current models
[00:16:39.620 --> 00:16:42.140]   are using inference and like how it works today,
[00:16:42.140 --> 00:16:44.020]   so that then I understand how to better like tweak it
[00:16:44.020 --> 00:16:45.360]   and what it's like?
[00:16:45.360 --> 00:16:46.200]   - Got it.
[00:16:46.200 --> 00:16:47.020]   Yeah, sure.
[00:16:47.020 --> 00:16:48.780]   Sorry, that's a bit of jargon.
[00:16:48.780 --> 00:16:51.140]   Inference just means running the model, right?
[00:16:51.140 --> 00:16:52.420]   Like putting something into the model
[00:16:52.420 --> 00:16:54.500]   and something coming out of it.
[00:16:54.500 --> 00:16:57.460]   Goes back to the like probabilistic backing of these models.
[00:16:57.460 --> 00:17:00.020]   Like you do it, you're like predicting
[00:17:00.020 --> 00:17:02.140]   what the future tokens are going to be.
[00:17:02.140 --> 00:17:05.620]   And that's like inference, like logical inference.
[00:17:05.620 --> 00:17:08.980]   But yeah, that's where the term comes from.
[00:17:08.980 --> 00:17:11.140]   But yeah.
[00:17:11.140 --> 00:17:13.420]   Cool.
[00:17:13.420 --> 00:17:17.460]   So yeah, so it's like, this is like replacing OpenAI's API
[00:17:17.460 --> 00:17:19.900]   or Anthropx API or OpenRouter
[00:17:19.900 --> 00:17:21.580]   with a service you host yourself,
[00:17:21.580 --> 00:17:24.140]   is what we're talking about here.
[00:17:24.140 --> 00:17:28.300]   Cool, yeah, definitely if I'm like,
[00:17:28.300 --> 00:17:30.380]   especially since, you know, I usually speak
[00:17:30.380 --> 00:17:32.700]   to more of an ML engineering audience.
[00:17:32.700 --> 00:17:36.180]   So like, if I just like forget that I haven't defined a term,
[00:17:36.180 --> 00:17:38.520]   please do interrupt me and ask me about it.
[00:17:38.520 --> 00:17:42.180]   Spent some time on this one already,
[00:17:42.180 --> 00:17:43.980]   so I won't go into more detail on this.
[00:17:43.980 --> 00:17:46.880]   But I would just say like, it's not that uncommon
[00:17:46.880 --> 00:17:49.380]   that proprietary software leads open software
[00:17:49.380 --> 00:17:51.940]   and raw capabilities like Oracle SQL
[00:17:51.940 --> 00:17:56.100]   and Microsoft SQL Server and like OSX and Windows
[00:17:56.100 --> 00:17:58.020]   have a bunch of things that like,
[00:17:58.020 --> 00:18:00.420]   beat their open source equivalents
[00:18:00.420 --> 00:18:03.580]   and have for a long time, like query optimizers in particular
[00:18:03.580 --> 00:18:05.940]   in the case of databases.
[00:18:05.940 --> 00:18:07.620]   So like, it's maybe not so surprising
[00:18:07.620 --> 00:18:10.940]   that that's the case in AI.
[00:18:10.940 --> 00:18:14.100]   But the like, the places in general,
[00:18:14.100 --> 00:18:17.340]   these things have co-existed in other domains.
[00:18:17.340 --> 00:18:20.300]   And then open software has been preferred in cases
[00:18:20.300 --> 00:18:22.260]   where it's more important to be able to hack on
[00:18:22.260 --> 00:18:25.940]   and integrate deeply with a technology.
[00:18:25.940 --> 00:18:30.820]   And so, you know, we're likely to see some mixture stably.
[00:18:30.820 --> 00:18:35.140]   And I, you know, I initially said this
[00:18:35.140 --> 00:18:38.240]   at one of SWIX's events, the AI Engineer Summit,
[00:18:38.240 --> 00:18:43.940]   year and a half ago now, and this has remained true.
[00:18:43.940 --> 00:18:46.580]   So that's at least 18 months of prediction stability,
[00:18:46.580 --> 00:18:49.960]   which is best you can maybe hope for these days.
[00:18:49.960 --> 00:18:53.300]   Yeah, so saving money.
[00:18:53.300 --> 00:18:55.180]   A lot of people want to run their own models
[00:18:55.180 --> 00:18:56.820]   to save money.
[00:18:56.820 --> 00:18:59.300]   Right now, inference is priced like a commodity.
[00:18:59.300 --> 00:19:01.980]   People find it relatively easy to change models.
[00:19:01.980 --> 00:19:05.060]   Little prompt tuning, keep a couple of prompts around,
[00:19:05.060 --> 00:19:07.700]   ask a language model to rewrite your prompts for you.
[00:19:07.700 --> 00:19:11.000]   Like, yeah, this among other factors has led
[00:19:11.000 --> 00:19:14.300]   to this LM inference being priced like a commodity
[00:19:14.300 --> 00:19:16.240]   rather than like a service.
[00:19:16.240 --> 00:19:20.820]   And so it's actually like quite difficult
[00:19:20.820 --> 00:19:22.580]   to run it more cheaply yourself.
[00:19:23.580 --> 00:19:24.980]   And so there's a couple of things
[00:19:24.980 --> 00:19:25.940]   that might swing in your favor.
[00:19:25.940 --> 00:19:27.580]   If you have idle GPUs,
[00:19:27.580 --> 00:19:29.820]   like maybe you have an ML team internally,
[00:19:29.820 --> 00:19:32.100]   and they like, when they're not doing training runs,
[00:19:32.100 --> 00:19:33.820]   they have GPUs just sitting there.
[00:19:33.820 --> 00:19:37.420]   You might just mine cryptocurrency with them instead,
[00:19:37.420 --> 00:19:41.220]   you know, like faster time to ROI.
[00:19:41.220 --> 00:19:44.160]   But like, you know, that at least if you have them,
[00:19:44.160 --> 00:19:47.060]   that like, you're just paying electricity.
[00:19:47.060 --> 00:19:49.560]   So that makes it a little bit easier.
[00:19:49.560 --> 00:19:51.540]   But electricity costs are actually quite high
[00:19:51.540 --> 00:19:55.460]   for these things, you know, kilowatt per accelerator
[00:19:55.460 --> 00:19:56.480]   for the big ones.
[00:19:56.480 --> 00:20:01.180]   The, like, taking a really big generic model,
[00:20:01.180 --> 00:20:02.860]   one of these like foundation models,
[00:20:02.860 --> 00:20:06.300]   like OpenAI's O1 model or Claude,
[00:20:06.300 --> 00:20:10.220]   and distilling it for just the problems that you care about
[00:20:10.220 --> 00:20:13.300]   into something like smaller and easier to run,
[00:20:13.300 --> 00:20:15.720]   that's a way that you can like save money.
[00:20:15.720 --> 00:20:19.340]   And we'll talk a bit about that if we get to fine tuning,
[00:20:19.340 --> 00:20:22.020]   if we spend time on that in fine tuning.
[00:20:22.020 --> 00:20:24.740]   But, you know, that can help a lot.
[00:20:24.740 --> 00:20:29.580]   If your traffic is super high and dependable,
[00:20:29.580 --> 00:20:32.020]   and you can just like allocate some GPUs to it,
[00:20:32.020 --> 00:20:33.660]   and like, you know, run it, you know,
[00:20:33.660 --> 00:20:37.660]   just get a block of EC2 instances with GPUs on them,
[00:20:37.660 --> 00:20:40.540]   hold them there, send traffic to it.
[00:20:40.540 --> 00:20:43.760]   It's flat, you're utilizing all the GPUs all the time.
[00:20:43.760 --> 00:20:45.580]   You could probably start to like compete
[00:20:45.580 --> 00:20:48.940]   with the model providers there on price.
[00:20:49.540 --> 00:20:52.160]   And then finally, it's like, if it's like once a week,
[00:20:52.160 --> 00:20:56.020]   you need to like process like every support conversation
[00:20:56.020 --> 00:20:58.500]   that you had and add annotations to it
[00:20:58.500 --> 00:21:00.020]   and generate a report.
[00:21:00.020 --> 00:21:01.300]   So it's like once a week,
[00:21:01.300 --> 00:21:06.300]   you need like 10 mega tokens per minute throughput.
[00:21:06.300 --> 00:21:10.060]   And then like rest of the time you don't,
[00:21:10.060 --> 00:21:12.300]   then like the proprietary model providers
[00:21:12.300 --> 00:21:14.220]   are gonna push you onto their enterprise tier
[00:21:14.220 --> 00:21:16.700]   for those big rate limits.
[00:21:16.700 --> 00:21:19.620]   But you can actually like,
[00:21:19.620 --> 00:21:23.060]   and so that's gonna push up the cost of using a provider.
[00:21:23.060 --> 00:21:28.980]   But then it's also easier to run super like big batches.
[00:21:28.980 --> 00:21:31.820]   Like it's actually kind of like easier
[00:21:31.820 --> 00:21:34.900]   to run these things economically at scale
[00:21:34.900 --> 00:21:35.900]   than it is at small scale.
[00:21:35.900 --> 00:21:38.260]   Somewhat counterintuitively maybe for a software engineer
[00:21:38.260 --> 00:21:41.540]   who's used to running like databases and web servers.
[00:21:41.540 --> 00:21:44.500]   Just like the nature of GPUs is that it's easier to use them
[00:21:44.500 --> 00:21:46.380]   the more work you have for them.
[00:21:46.380 --> 00:21:49.860]   And so that makes, you know,
[00:21:49.860 --> 00:21:54.160]   these like batch and less latency sensitive workloads,
[00:21:54.160 --> 00:21:56.620]   like more amenable to running yourself
[00:21:56.620 --> 00:21:58.700]   if you can get ahold of serverless GPUs
[00:21:58.700 --> 00:22:00.780]   through a platform like Modal,
[00:22:00.780 --> 00:22:04.740]   Replicate, Google Cloud Run, something like that.
[00:22:04.740 --> 00:22:10.260]   Okay, so that's everything on like why you would do this,
[00:22:10.260 --> 00:22:13.420]   why you would run your own OpenAI API
[00:22:13.420 --> 00:22:15.500]   or Anthropic API replacement.
[00:22:15.500 --> 00:22:18.720]   Any questions before we move on?
[00:22:18.720 --> 00:22:22.220]   I saw the chat had some activity, maybe check that out.
[00:22:22.220 --> 00:22:23.380]   Anybody wanna speak up?
[00:22:23.380 --> 00:22:30.860]   - No, I think we're just sort of adding color
[00:22:30.860 --> 00:22:33.420]   to different stuff.
[00:22:33.420 --> 00:22:35.900]   - Got it, thanks for grabbing the chat.
[00:22:35.900 --> 00:22:37.700]   Okay, so let's start.
[00:22:37.700 --> 00:22:40.300]   Like I've already mentioned hardware and GPUs a lot,
[00:22:40.300 --> 00:22:43.140]   so let's talk about that a little bit more.
[00:22:43.140 --> 00:22:46.500]   Talk a little bit about like picking a model,
[00:22:46.500 --> 00:22:49.140]   then deep dive on like serving inference,
[00:22:49.140 --> 00:22:51.660]   a little bit on the tooling for it.
[00:22:51.660 --> 00:22:54.560]   Then like fine tuning,
[00:22:54.560 --> 00:22:56.460]   like how do you customize these models
[00:22:56.460 --> 00:23:00.620]   and then close out with thinking about observability
[00:23:00.620 --> 00:23:02.060]   and continual improvement.
[00:23:02.060 --> 00:23:07.380]   Okay, and yeah, link for the slides there.
[00:23:07.380 --> 00:23:10.380]   Of course, you'll be able to get it after the session.
[00:23:10.380 --> 00:23:13.500]   Okay, so picking hardware is pretty easy.
[00:23:13.500 --> 00:23:15.900]   Just use NVIDIA GPUs, don't have to go any further.
[00:23:15.900 --> 00:23:18.140]   No, let me go into a little bit more detail
[00:23:18.140 --> 00:23:19.540]   about why that's the case.
[00:23:19.540 --> 00:23:22.460]   So Juliet wanted like a little bit more color and detail
[00:23:22.460 --> 00:23:24.660]   on what does LLM inference mean.
[00:23:24.660 --> 00:23:26.420]   So what LLM inference means
[00:23:26.420 --> 00:23:28.660]   is you need to take the like parameters of the model,
[00:23:28.660 --> 00:23:32.700]   the weights, this like giant pile of floating point numbers.
[00:23:32.700 --> 00:23:35.620]   Those are gonna be sitting in some storage.
[00:23:35.620 --> 00:23:39.100]   You need to bring them into the place where compute happens.
[00:23:39.100 --> 00:23:41.260]   So like even if they're sitting in memory,
[00:23:41.260 --> 00:23:42.580]   like compute doesn't happen in memory.
[00:23:42.580 --> 00:23:46.100]   Compute happens like on chip inside of like registers.
[00:23:46.100 --> 00:23:48.020]   So you gotta move all of that in.
[00:23:48.020 --> 00:23:50.580]   And the fun fact is like you actually need
[00:23:50.580 --> 00:23:53.700]   like pretty much every single weight needs to go in.
[00:23:53.700 --> 00:23:55.740]   So like for most models,
[00:23:55.740 --> 00:23:58.980]   you can just look at how many gigabytes is that model file.
[00:23:58.980 --> 00:24:00.260]   And that tells you how many bytes
[00:24:00.260 --> 00:24:04.180]   they're gonna need to move in to get computed on.
[00:24:04.180 --> 00:24:06.500]   So like you're running an 8B model
[00:24:06.500 --> 00:24:10.580]   in one byte quantization, that's 8 billion weights.
[00:24:10.580 --> 00:24:13.580]   One byte per weight, that's eight gigabytes.
[00:24:13.580 --> 00:24:15.580]   So you need to move eight gigabytes
[00:24:15.580 --> 00:24:17.780]   out of wherever they're stored
[00:24:17.780 --> 00:24:20.220]   and into the place where compute happens.
[00:24:20.220 --> 00:24:23.780]   And then like that happens,
[00:24:23.780 --> 00:24:27.100]   like you're pushing tokens and activations
[00:24:27.100 --> 00:24:30.340]   through those weights to get out the next token.
[00:24:30.340 --> 00:24:34.140]   On your first iteration, you're sending in the whole prompt.
[00:24:34.140 --> 00:24:35.780]   And so you're sending in a whole prompt
[00:24:35.780 --> 00:24:37.180]   and generating an output token.
[00:24:37.180 --> 00:24:39.060]   So is guava a fruit?
[00:24:39.060 --> 00:24:40.540]   Yes.
[00:24:40.540 --> 00:24:43.300]   In the process of like pushing something through the weights,
[00:24:43.300 --> 00:24:45.540]   you can kind of rough estimation
[00:24:45.540 --> 00:24:49.060]   is that you want to do one,
[00:24:49.060 --> 00:24:53.140]   you wanna do two floating point operations per weight.
[00:24:53.140 --> 00:24:55.200]   So that's like, you want to multiply the weight
[00:24:55.200 --> 00:24:57.420]   with some number and then you're gonna add it
[00:24:57.420 --> 00:24:59.180]   to some other number.
[00:24:59.180 --> 00:25:01.580]   So that's two operations per weight.
[00:25:01.580 --> 00:25:03.100]   This is very napkin math.
[00:25:03.100 --> 00:25:06.420]   But again, nobody should have to write
[00:25:06.420 --> 00:25:08.140]   this very small number of wizards
[00:25:08.140 --> 00:25:10.260]   to write the actual code here.
[00:25:10.260 --> 00:25:13.600]   The core thing is being able to reason as an engineer
[00:25:13.600 --> 00:25:16.540]   about what the system's requirements are
[00:25:16.540 --> 00:25:20.260]   and how to, kind of like with a database,
[00:25:20.260 --> 00:25:22.460]   you don't have to be able to write a B-tree from scratch
[00:25:22.460 --> 00:25:25.300]   on a chalkboard unless you're interviewing at Google.
[00:25:25.300 --> 00:25:27.780]   But you should know how indices work
[00:25:27.780 --> 00:25:29.980]   so that you can like think about queries
[00:25:29.980 --> 00:25:32.220]   and structure tables in a smart way.
[00:25:32.220 --> 00:25:34.420]   And so similarly here,
[00:25:34.420 --> 00:25:36.620]   I'm just trying to give you the like intuition you need
[00:25:36.620 --> 00:25:39.100]   for understanding this workload.
[00:25:39.100 --> 00:25:42.020]   So for this, we have four tokens.
[00:25:42.020 --> 00:25:46.300]   We've got like one output.
[00:25:46.300 --> 00:25:47.860]   Yeah, we got four tokens coming in.
[00:25:47.860 --> 00:25:50.540]   We've got 8 billion parameters.
[00:25:50.540 --> 00:25:52.160]   So eight times two times four,
[00:25:52.160 --> 00:25:55.860]   that's 64 billion floating point operations.
[00:25:55.860 --> 00:25:58.660]   And then that gets us one token.
[00:25:58.660 --> 00:26:00.380]   Then we got to repeat this every time
[00:26:00.380 --> 00:26:01.860]   we want to generate another token.
[00:26:01.860 --> 00:26:03.380]   So we're going to move the weights.
[00:26:03.380 --> 00:26:05.780]   Like they have to, they go into where they get muted,
[00:26:05.780 --> 00:26:06.860]   then back out.
[00:26:06.860 --> 00:26:09.620]   Because we're talking about like registers and caches here.
[00:26:09.620 --> 00:26:11.980]   If you think of your like low level hardware stuff,
[00:26:11.980 --> 00:26:13.380]   registers and caches.
[00:26:13.380 --> 00:26:14.500]   So they can't hold the whole weight.
[00:26:14.500 --> 00:26:16.260]   So they got to go in and out the whole time.
[00:26:16.260 --> 00:26:17.860]   Again, if you're a database person,
[00:26:17.860 --> 00:26:20.200]   you should think of like running a sequential scan
[00:26:20.200 --> 00:26:23.180]   on your database over and over and over again
[00:26:23.180 --> 00:26:25.660]   on like a billion row database.
[00:26:25.660 --> 00:26:29.320]   So it's wild that we can even run it as fast as we do.
[00:26:29.320 --> 00:26:32.020]   But this is the workload.
[00:26:32.020 --> 00:26:35.140]   The hard part about it is the scale.
[00:26:35.140 --> 00:26:36.780]   The easy part about it is that this
[00:26:36.780 --> 00:26:41.180]   is like relatively simple control flow at the core.
[00:26:41.180 --> 00:26:45.340]   So that makes it amenable to acceleration with GPUs.
[00:26:45.340 --> 00:26:49.300]   GPUs have a bunch of, like if you look at the chip itself,
[00:26:49.300 --> 00:26:50.860]   this is the chip area.
[00:26:50.860 --> 00:26:55.620]   CPUs spend most of their space on like control flow logic.
[00:26:55.620 --> 00:26:59.140]   And then caches that hide how smart the CPU is being
[00:26:59.140 --> 00:27:01.700]   about like control flow and switching work.
[00:27:01.700 --> 00:27:04.140]   And then like relatively less is actually
[00:27:04.140 --> 00:27:06.900]   given over to the part that does like calculations, which
[00:27:06.900 --> 00:27:08.780]   is here in green.
[00:27:08.780 --> 00:27:12.260]   GPUs, on the other hand, are just all calculation.
[00:27:12.260 --> 00:27:15.500]   And they have relatively simple control flow
[00:27:15.500 --> 00:27:20.500]   and like relatively less cache memory.
[00:27:20.500 --> 00:27:23.940]   And that-- because it doesn't need to hold 100 programs
[00:27:23.940 --> 00:27:25.780]   at once or whatever.
[00:27:25.780 --> 00:27:31.900]   And so that means you can really rip through a workload
[00:27:31.900 --> 00:27:35.020]   like this one that has like relatively simple stuff, where
[00:27:35.020 --> 00:27:36.700]   most of what you want to do is just
[00:27:36.700 --> 00:27:41.260]   like zoom through doing simple math on a bunch of numbers.
[00:27:41.260 --> 00:27:44.060]   So that's why GPUs are designed for this,
[00:27:44.060 --> 00:27:46.220]   because it works well for graphics, which also looks
[00:27:46.220 --> 00:27:49.380]   like ripping through a bunch of math.
[00:27:49.380 --> 00:27:51.940]   Basically the same math on a bunch of different inputs,
[00:27:51.940 --> 00:27:53.820]   this graphics workload.
[00:27:53.820 --> 00:27:55.620]   But they've like tilted now even further
[00:27:55.620 --> 00:27:57.180]   in the direction of being specialized
[00:27:57.180 --> 00:28:06.540]   for running language models and big neural networks.
[00:28:06.540 --> 00:28:10.620]   The TLDR here is like the GPU is 100 duck-sized horses,
[00:28:10.620 --> 00:28:14.940]   a bunch of tiny cores doing like very simple stuff.
[00:28:14.940 --> 00:28:18.940]   And that wins out over the one horse-sized duck
[00:28:18.940 --> 00:28:22.500]   that is the CPU that you're used to programming and working
[00:28:22.500 --> 00:28:25.180]   with.
[00:28:25.180 --> 00:28:26.620]   There's like one other piece here,
[00:28:26.620 --> 00:28:28.780]   which is like if you're looking at a top-tier GPU,
[00:28:28.780 --> 00:28:31.740]   one of the things that makes the top-tier ones really good,
[00:28:31.740 --> 00:28:35.220]   like an H100, is that they have soldered the RAM
[00:28:35.220 --> 00:28:39.980]   onto the chip, which is not something you normally do.
[00:28:39.980 --> 00:28:43.980]   But it gives you much faster communication, lower latency,
[00:28:43.980 --> 00:28:46.460]   higher throughput, which is really important.
[00:28:46.460 --> 00:28:48.140]   The memory is still slower than the math,
[00:28:48.140 --> 00:28:50.100]   which is really important if you start
[00:28:50.100 --> 00:28:52.420]   to think about optimizing these things.
[00:28:52.420 --> 00:28:55.740]   But we don't have to go that deep.
[00:28:55.740 --> 00:29:01.060]   So the TLDR here is that it's like NVIDIA-inferenced GPUs
[00:29:01.060 --> 00:29:03.380]   from one or two generations back are what you probably
[00:29:03.380 --> 00:29:04.860]   want to run with.
[00:29:04.860 --> 00:29:06.740]   The primary constrained resource is
[00:29:06.740 --> 00:29:08.900]   how much space there is in this memory
[00:29:08.900 --> 00:29:11.100]   to hold all those weights.
[00:29:11.100 --> 00:29:12.140]   Well, it's the weights.
[00:29:12.140 --> 00:29:13.640]   And then later you're going to start
[00:29:13.640 --> 00:29:16.380]   adding things like past sequences you've run on
[00:29:16.380 --> 00:29:17.220]   in a cache.
[00:29:17.220 --> 00:29:19.900]   And then there's never enough RAM.
[00:29:19.900 --> 00:29:22.380]   And so when you're looking at buying GPUs yourself
[00:29:22.380 --> 00:29:25.380]   for rent or which ones to rent from the cloud,
[00:29:25.380 --> 00:29:28.340]   look for the ones with more VRAM.
[00:29:28.340 --> 00:29:31.940]   And then this is a primary reason
[00:29:31.940 --> 00:29:33.940]   to want to make your model weights smaller,
[00:29:33.940 --> 00:29:36.260]   to go from high-precision floating-point numbers
[00:29:36.260 --> 00:29:38.100]   to low-precision floating-point numbers,
[00:29:38.100 --> 00:29:41.740]   or even more exotic things, because they
[00:29:41.740 --> 00:29:43.420]   save space in that memory.
[00:29:43.420 --> 00:29:46.980]   And they make it easier to move the things in and out
[00:29:46.980 --> 00:29:49.620]   of memory and into where the compute happens.
[00:29:49.620 --> 00:29:52.580]   So the thing you want is a recent but not bleeding-edge
[00:29:52.580 --> 00:29:55.460]   GPU unless you enjoy pain.
[00:29:55.460 --> 00:29:58.620]   So most recent GPUs from NVIDIA are the Blackwell architecture.
[00:29:58.620 --> 00:30:01.780]   That's the 5,000 series of GeForce GPUs,
[00:30:01.780 --> 00:30:08.660]   your local neighborhood GPU, and then the Blackwell B200s
[00:30:08.660 --> 00:30:11.940]   and similar data center GPUs.
[00:30:11.940 --> 00:30:13.860]   Generally, you're going to find that you
[00:30:13.860 --> 00:30:16.420]   don't get the full speedup that you'd like because people
[00:30:16.420 --> 00:30:19.420]   don't compile for that architecture always
[00:30:19.420 --> 00:30:20.780]   and yada, yada.
[00:30:20.780 --> 00:30:23.260]   And then things are randomly broken.
[00:30:23.260 --> 00:30:26.100]   And then they're really hard to get a hold of and expensive.
[00:30:26.100 --> 00:30:28.300]   So the sweet spot is one generation
[00:30:28.300 --> 00:30:30.980]   behind whatever OpenAI and Meta are training on.
[00:30:30.980 --> 00:30:32.980]   So now that's Hopper GPUs.
[00:30:32.980 --> 00:30:37.820]   H200s were free on Amazon, at least on EC2,
[00:30:37.820 --> 00:30:40.140]   for a bit there a couple weeks ago.
[00:30:40.140 --> 00:30:45.540]   And then loveless GPUs like the L40s that I ran my demo on,
[00:30:45.540 --> 00:30:47.220]   those are pretty nice.
[00:30:47.220 --> 00:30:50.740]   Loveless is the more-- or sorry, L40s
[00:30:50.740 --> 00:30:53.540]   is the more inference-oriented data center GPU.
[00:30:53.540 --> 00:30:55.780]   So data center GPU means like ones
[00:30:55.780 --> 00:30:58.180]   you're going to find in the public clouds.
[00:30:58.180 --> 00:31:02.460]   NVIDIA doesn't really let people put your friendly local GPU,
[00:31:02.460 --> 00:31:05.540]   the same one you can buy locally and put in your own machine.
[00:31:05.540 --> 00:31:07.380]   They don't really let them run in the clouds
[00:31:07.380 --> 00:31:09.940]   unless NVIDIA is on the cap table.
[00:31:09.940 --> 00:31:13.940]   So that doesn't work for AWS and GCP.
[00:31:13.940 --> 00:31:16.280]   So that's a data center GPU.
[00:31:16.280 --> 00:31:18.580]   And then an inference data center GPU
[00:31:18.580 --> 00:31:22.620]   is one that's less focused on connecting a whole shitload
[00:31:22.620 --> 00:31:25.500]   of GPUs together, like 10,000 or 100,000,
[00:31:25.500 --> 00:31:30.620]   with a super fast custom network InfiniBand.
[00:31:30.620 --> 00:31:38.700]   And instead, they're more focused
[00:31:38.700 --> 00:31:43.860]   on just having one reasonably sized effective individual GPU.
[00:31:43.860 --> 00:31:46.420]   So the L40s are getting pretty mature.
[00:31:46.420 --> 00:31:47.860]   So I might recommend those.
[00:31:47.860 --> 00:31:52.700]   For a while, the H100, which is really more of a training GPU,
[00:31:52.700 --> 00:31:53.860]   was kind of the better one.
[00:31:53.860 --> 00:31:58.240]   I think, yeah, just because the L40s was relatively mature.
[00:31:58.240 --> 00:32:01.020]   If your model's small, if you're running a small model,
[00:32:01.020 --> 00:32:07.540]   like a modern BERT or one of the 3 billion or 1 billion models,
[00:32:07.540 --> 00:32:10.780]   you can get away with running it even a generation further back.
[00:32:10.780 --> 00:32:12.740]   And that's really nice, very stable.
[00:32:12.740 --> 00:32:16.900]   The Ampere A10 is a really real workhorse GPU,
[00:32:16.900 --> 00:32:18.660]   easy to get ahold of.
[00:32:18.660 --> 00:32:22.740]   You can transparently scale up to thousands of those on modal
[00:32:22.740 --> 00:32:24.820]   when it comes time.
[00:32:24.820 --> 00:32:27.500]   So that's pretty nice.
[00:32:27.500 --> 00:32:33.860]   Just a quick-- since part NVIDIA is in the news these days,
[00:32:33.860 --> 00:32:35.820]   like why NVIDIA?
[00:32:35.820 --> 00:32:40.700]   AMD and Intel GPUs are still butt catching up on performance.
[00:32:40.700 --> 00:32:42.940]   So nominally, you look at the sticker
[00:32:42.940 --> 00:32:46.540]   on the side that says Flops, and the AMD GPUs look good.
[00:32:46.540 --> 00:32:49.420]   And Intel Gaudi looks pretty good.
[00:32:49.420 --> 00:32:50.780]   The software stack is way behind.
[00:32:50.780 --> 00:32:54.860]   There's a great post from Dylan Patel and others
[00:32:54.860 --> 00:32:57.820]   that's semi-analysis, just ripping on the AMD software
[00:32:57.820 --> 00:32:58.320]   stack.
[00:32:58.320 --> 00:33:00.020]   George Hopps has done the same thing.
[00:33:00.020 --> 00:33:01.780]   It's just pain.
[00:33:01.780 --> 00:33:02.980]   That's a bet the company move.
[00:33:02.980 --> 00:33:06.020]   It's like, we can maybe either write the software ourselves
[00:33:06.020 --> 00:33:08.180]   or spend so much money on AMD chips
[00:33:08.180 --> 00:33:11.100]   that AMD will fix this for us.
[00:33:11.100 --> 00:33:12.780]   That's not really like, oh, I want
[00:33:12.780 --> 00:33:16.220]   to stand up a service kind of thing,
[00:33:16.220 --> 00:33:18.020]   stick with the well-trodden paths.
[00:33:18.020 --> 00:33:19.500]   There's non-GPU alternatives.
[00:33:19.500 --> 00:33:21.700]   There are other accelerators that are designed,
[00:33:21.700 --> 00:33:25.220]   unlike CPUs, for super high throughput and low memory
[00:33:25.220 --> 00:33:27.260]   bandwidth.
[00:33:27.260 --> 00:33:29.660]   TPU is the most mature one, the Tensor Processing
[00:33:29.660 --> 00:33:30.740]   Unit from Google.
[00:33:30.740 --> 00:33:32.340]   Unfortunately, it's very from Google
[00:33:32.340 --> 00:33:34.140]   in that they only run in Google Cloud.
[00:33:34.140 --> 00:33:36.720]   And the software stack is pretty decent for them, actually,
[00:33:36.720 --> 00:33:42.220]   like Jax, which can be used as a back end for PyTorch.
[00:33:42.220 --> 00:33:46.540]   But like many things in Google, the internal software for it
[00:33:46.540 --> 00:33:49.300]   is way better than anything you'll ever use.
[00:33:49.300 --> 00:33:51.900]   And you're second in line behind their internal engineers
[00:33:51.900 --> 00:33:54.020]   for any bug fixes.
[00:33:54.020 --> 00:33:57.140]   So caveat emptor there.
[00:33:57.140 --> 00:33:59.060]   The Grok and Cerebrus accelerators
[00:33:59.060 --> 00:34:02.180]   are still a little bit too bleeding edge.
[00:34:02.180 --> 00:34:04.500]   At that point, you're kind of not running your own LM
[00:34:04.500 --> 00:34:05.220]   inference anymore.
[00:34:05.220 --> 00:34:08.000]   You're having somebody else run it as a service for you
[00:34:08.000 --> 00:34:09.520]   on chips that they run.
[00:34:09.520 --> 00:34:10.920]   It's kind of the way it works.
[00:34:10.920 --> 00:34:15.360]   It's unclear if they could do it-- what's
[00:34:15.360 --> 00:34:18.720]   the word I'm looking for-- cost-effectively as well.
[00:34:18.720 --> 00:34:21.960]   Those chips are very expensive to run.
[00:34:21.960 --> 00:34:24.080]   I would say any of the other accelerators you see
[00:34:24.080 --> 00:34:26.680]   aren't super worth considering.
[00:34:26.680 --> 00:34:28.320]   But in general, long term, I would
[00:34:28.320 --> 00:34:29.720]   expect this to change a lot.
[00:34:29.720 --> 00:34:34.520]   NVIDIA has a very thick stack of water-cooled network cards
[00:34:34.520 --> 00:34:36.820]   that can do a little bit of math for you.
[00:34:36.820 --> 00:34:38.340]   That's crazy shit, and it's going
[00:34:38.340 --> 00:34:40.460]   to take a long time for anybody to catch up there.
[00:34:40.460 --> 00:34:44.300]   But inference is actually pretty easy
[00:34:44.300 --> 00:34:46.620]   to match their performance on.
[00:34:46.620 --> 00:34:48.860]   So I expect a lot of innovation in this space,
[00:34:48.860 --> 00:34:52.460]   and VCs are spending accordingly.
[00:34:52.460 --> 00:34:56.140]   Last thing I'll say is the startup that I work on,
[00:34:56.140 --> 00:34:58.420]   Modal, it makes getting GPUs really easy.
[00:34:58.420 --> 00:35:01.060]   So a lot of it-- this is high-performance computing
[00:35:01.060 --> 00:35:01.900]   hardware.
[00:35:01.900 --> 00:35:04.220]   It's normally a huge pain to get.
[00:35:04.220 --> 00:35:06.180]   If you've run a Kubernetes cluster,
[00:35:06.180 --> 00:35:09.860]   you know that heterogeneous compute makes you cry.
[00:35:09.860 --> 00:35:12.240]   There's a reason they call it taints.
[00:35:12.240 --> 00:35:14.580]   So Modal makes getting GPUs super easy,
[00:35:14.580 --> 00:35:18.020]   just like add Python decorators, get stuff to run on GPUs.
[00:35:18.020 --> 00:35:24.500]   This is real code that our CEO ran to test our H100 scaling,
[00:35:24.500 --> 00:35:27.660]   just like let me just run 100,000 times,
[00:35:27.660 --> 00:35:30.860]   time [AUDIO OUT] sleep one on an H100.
[00:35:30.860 --> 00:35:34.500]   And this is all the code that you need to run that.
[00:35:34.500 --> 00:35:38.660]   In our enterprise tier, this would scale up to 500 H100s
[00:35:38.660 --> 00:35:43.860]   or more, pretty transparently.
[00:35:43.860 --> 00:35:46.460]   So when you need it, we've got it.
[00:35:46.460 --> 00:35:50.420]   OK, so that's everything I want to say on hardware.
[00:35:50.420 --> 00:35:52.740]   Any questions about that stuff before I
[00:35:52.740 --> 00:35:57.140]   dive into talking about the zoo of models?
[00:36:00.460 --> 00:36:03.820]   No, I think we're pretty good.
[00:36:03.820 --> 00:36:07.180]   I like the commentary on TPUs.
[00:36:07.180 --> 00:36:08.340]   Yeah.
[00:36:08.340 --> 00:36:10.340]   It'd be cool if they sold them.
[00:36:10.340 --> 00:36:11.140]   That would be great.
[00:36:11.140 --> 00:36:12.380]   I'd have one in my house.
[00:36:12.380 --> 00:36:14.380]   But yeah.
[00:36:14.380 --> 00:36:16.180]   So was that--
[00:36:16.180 --> 00:36:17.940]   They're eating all the ones they can make.
[00:36:17.940 --> 00:36:20.140]   So it's almost like a competitive advantage.
[00:36:20.140 --> 00:36:22.180]   Make more, you know?
[00:36:22.180 --> 00:36:25.740]   How hard could it be to build a semiconductor foundry?
[00:36:25.740 --> 00:36:28.460]   I thought, why do you have a money printer
[00:36:28.460 --> 00:36:30.300]   if you aren't going to use the money for good stuff?
[00:36:30.300 --> 00:36:33.260]   Anyway, I'm sure they have great reasons for this.
[00:36:33.260 --> 00:36:34.740]   But yeah.
[00:36:34.740 --> 00:36:35.780]   Oh, yes.
[00:36:35.780 --> 00:36:38.420]   Anyway, I won't go on any more tangents there.
[00:36:38.420 --> 00:36:43.100]   But DM me on Twitter if you want to talk more about this.
[00:36:43.100 --> 00:36:45.020]   Yeah, and also, oh, yeah, I wrote a guide
[00:36:45.020 --> 00:36:49.540]   to using GPUs, modal.com/gpuglossary,
[00:36:49.540 --> 00:36:51.300]   GPU hyphen glossary.
[00:36:51.300 --> 00:36:53.660]   So if you're interested in this stuff, check it out.
[00:36:53.660 --> 00:36:55.940]   It's kind of intended to give you
[00:36:55.940 --> 00:37:00.900]   the intuition for this hardware and a little bit of debugging
[00:37:00.900 --> 00:37:05.140]   on the software stack because most people didn't encounter
[00:37:05.140 --> 00:37:07.380]   anything like this in their computer science
[00:37:07.380 --> 00:37:11.860]   education, their boot camp, or their working experience so far.
[00:37:11.860 --> 00:37:12.740]   So yeah.
[00:37:12.740 --> 00:37:13.420]   All right.
[00:37:13.420 --> 00:37:15.340]   So I could talk for hours about that.
[00:37:15.340 --> 00:37:17.220]   But let's talk about model selection.
[00:37:17.220 --> 00:37:21.420]   So what is the actual model we're going to run?
[00:37:21.420 --> 00:37:25.740]   My one piece of advice that I've contractually obligated,
[00:37:25.740 --> 00:37:28.780]   before you start thinking about, oh, what model am I going to run?
[00:37:28.780 --> 00:37:30.380]   How do I--
[00:37:30.380 --> 00:37:34.260]   I want to do a good job on this task.
[00:37:34.260 --> 00:37:36.380]   Make sure you've defined the task well
[00:37:36.380 --> 00:37:40.960]   and you have evals, an ability to evaluate whether the--
[00:37:40.960 --> 00:37:42.500]   you swap out a model for another one.
[00:37:42.500 --> 00:37:43.500]   Is it better or not?
[00:37:43.500 --> 00:37:44.980]   You can start with vibe checks.
[00:37:44.980 --> 00:37:47.780]   You just run one prompt that you like
[00:37:47.780 --> 00:37:51.340]   that helps you get good smell for a model.
[00:37:51.340 --> 00:37:53.180]   But that's going to--
[00:37:53.180 --> 00:37:55.300]   that works for a very short period of time.
[00:37:55.300 --> 00:37:58.780]   10 inputs, 50 inputs, how long does it
[00:37:58.780 --> 00:38:01.820]   take you to write that out with ground truth answers?
[00:38:01.820 --> 00:38:07.500]   If it takes you an hour, put on your jams and do it.
[00:38:07.500 --> 00:38:09.820]   That's the length of Brat.
[00:38:09.820 --> 00:38:14.380]   Just listen to Brat and write out 10 or 50 evals.
[00:38:14.380 --> 00:38:16.900]   Just because it's kind of like test-driven development,
[00:38:16.900 --> 00:38:19.820]   where everybody says write the tests and then
[00:38:19.820 --> 00:38:20.860]   write the software.
[00:38:20.860 --> 00:38:23.620]   But in this case, with test-driven development,
[00:38:23.620 --> 00:38:25.220]   one reason people don't do it is because they
[00:38:25.220 --> 00:38:27.620]   can mentally run tests really well.
[00:38:27.620 --> 00:38:29.540]   I know what a test--
[00:38:29.540 --> 00:38:31.820]   I know all the different ways this code could misbehave.
[00:38:31.820 --> 00:38:33.580]   I don't have to write it out as a test.
[00:38:33.580 --> 00:38:35.460]   And if you're good, that's correct.
[00:38:35.460 --> 00:38:38.380]   If you're bad at software, like me,
[00:38:38.380 --> 00:38:39.940]   then you need the test to help you.
[00:38:39.940 --> 00:38:41.780]   But in this case, nobody is good at predicting
[00:38:41.780 --> 00:38:43.660]   the behavior of these models.
[00:38:43.660 --> 00:38:45.980]   And so evals are really critical,
[00:38:45.980 --> 00:38:49.900]   being able to check is this actually improving things
[00:38:49.900 --> 00:38:50.500]   or not.
[00:38:50.500 --> 00:38:53.540]   So do this, even just 10 things in a notebook.
[00:38:53.540 --> 00:38:56.020]   Don't go and buy an eval framework to do this.
[00:38:56.020 --> 00:39:00.300]   Just find a way to run models in the terminal in a notebook
[00:39:00.300 --> 00:39:05.460]   that helps you make these decisions like an engineer,
[00:39:05.460 --> 00:39:08.700]   not like a scientist like me.
[00:39:08.700 --> 00:39:11.780]   OK, so model options here are still, I would say,
[00:39:11.780 --> 00:39:13.180]   limited but growing.
[00:39:13.180 --> 00:39:15.140]   I might drop the limited sometime soon,
[00:39:15.140 --> 00:39:17.780]   because it's starting to feel like we have options.
[00:39:17.780 --> 00:39:20.980]   Meta's Llama model series is pretty well-regarded
[00:39:20.980 --> 00:39:23.460]   and has the very strong backing of Meta.
[00:39:23.460 --> 00:39:26.200]   So if I'm an engineer thinking about which open source
[00:39:26.200 --> 00:39:28.700]   software am I going to build on, I actually think about that
[00:39:28.700 --> 00:39:34.060]   a lot more so than raw capabilities a lot of the time.
[00:39:34.060 --> 00:39:37.260]   And the key thing here is there's a pretty big community
[00:39:37.260 --> 00:39:40.500]   building on Llama, making their software work really well
[00:39:40.500 --> 00:39:42.820]   with Llama, doing things with Llama
[00:39:42.820 --> 00:39:44.660]   that you would otherwise have to do yourself.
[00:39:44.660 --> 00:39:47.620]   So Neural Magic, major contributor
[00:39:47.620 --> 00:39:51.660]   to an inference framework called VLLM,
[00:39:51.660 --> 00:39:53.300]   they quantize models for you.
[00:39:53.300 --> 00:39:57.100]   So they squish them down so they're a lot smaller.
[00:39:57.100 --> 00:39:58.740]   Now you don't have to do that yourself.
[00:39:58.740 --> 00:40:00.020]   That's very nice.
[00:40:00.020 --> 00:40:03.540]   Noose Research does a lot of fine-tuning of models
[00:40:03.540 --> 00:40:07.820]   to remove their chat GPT slop behavior.
[00:40:07.820 --> 00:40:09.020]   So it's nice to have that.
[00:40:09.020 --> 00:40:12.860]   And RCAI will mush together five different Llamas
[00:40:12.860 --> 00:40:17.340]   to make one Penta Llama that weirdly works better
[00:40:17.340 --> 00:40:19.380]   than any of the five inputs.
[00:40:19.380 --> 00:40:21.460]   And then you don't have to do any of that yourself.
[00:40:21.460 --> 00:40:22.180]   Very nice.
[00:40:22.180 --> 00:40:23.760]   And then because it's backed by Meta,
[00:40:23.760 --> 00:40:26.300]   you can expect there will be continued investment in it.
[00:40:26.300 --> 00:40:29.020]   Meta's been great about open source in other places,
[00:40:29.020 --> 00:40:31.100]   like, I don't know, React.
[00:40:31.100 --> 00:40:32.740]   So maybe that's a bad one to pick
[00:40:32.740 --> 00:40:34.060]   because of the licensing thing.
[00:40:34.060 --> 00:40:35.340]   But they learned their lesson.
[00:40:35.340 --> 00:40:39.220]   So you can build on Meta comfortably.
[00:40:39.220 --> 00:40:41.820]   DeepSeek model series is on the rise.
[00:40:41.820 --> 00:40:46.140]   Not the first model series out of China
[00:40:46.140 --> 00:40:50.660]   to catch people's attention, the other one being Quen.
[00:40:50.660 --> 00:40:53.060]   There's slightly less tooling and integration
[00:40:53.060 --> 00:40:54.500]   than the Llama model series.
[00:40:54.500 --> 00:40:55.880]   But an important thing to note is
[00:40:55.880 --> 00:40:58.940]   that it is released under the MIT license.
[00:40:58.940 --> 00:41:00.380]   So the model weights are released
[00:41:00.380 --> 00:41:03.300]   under a normal open source license
[00:41:03.300 --> 00:41:07.220]   that the open source initiative would put their stamp on.
[00:41:07.220 --> 00:41:12.220]   But the Llama model is under a proprietary license that
[00:41:12.220 --> 00:41:15.340]   says, for example, if you're Amazon or Google,
[00:41:15.340 --> 00:41:17.540]   you can't use this.
[00:41:17.540 --> 00:41:19.060]   Not literally, but effectively.
[00:41:19.860 --> 00:41:22.580]   And a couple other things that make it less open,
[00:41:22.580 --> 00:41:26.260]   slightly less open, might make your lawyers nervous.
[00:41:26.260 --> 00:41:29.340]   So maybe DeepSeek will just push Llama
[00:41:29.340 --> 00:41:33.500]   to go MIT, inshallah that will happen with Llama 4.
[00:41:33.500 --> 00:41:37.220]   There are others to pay attention to.
[00:41:37.220 --> 00:41:39.940]   You might see a shitty model come out of a model training
[00:41:39.940 --> 00:41:43.660]   team, or sorry, you might see a non-state-of-the-art model come
[00:41:43.660 --> 00:41:45.060]   out of a model training team.
[00:41:45.060 --> 00:41:47.180]   But that doesn't mean that the team is bad.
[00:41:47.180 --> 00:41:51.220]   It's just that it takes a long time to get really good.
[00:41:51.220 --> 00:41:53.340]   So ones to watch are the Allen Institute's
[00:41:53.340 --> 00:41:56.060]   been putting out some good models with the Olmo series
[00:41:56.060 --> 00:41:57.620]   and the Molmo model.
[00:41:57.620 --> 00:42:00.940]   Microsoft's been doing their small language models with Phi.
[00:42:00.940 --> 00:42:04.380]   Mistral's has been quiet for a bit,
[00:42:04.380 --> 00:42:05.820]   but they keep putting out models.
[00:42:05.820 --> 00:42:06.700]   And Quen.
[00:42:06.700 --> 00:42:10.140]   Maybe in the future, the enterprise cloud homies
[00:42:10.140 --> 00:42:12.620]   Snowflake and Databricks will put out
[00:42:12.620 --> 00:42:13.860]   really compelling models.
[00:42:13.860 --> 00:42:17.500]   Mostly, Arctic and DBRX are fun for research reasons
[00:42:17.500 --> 00:42:20.620]   rather than raw capabilities.
[00:42:20.620 --> 00:42:23.860]   But yeah, that's kind of a small number of options.
[00:42:23.860 --> 00:42:27.180]   A little bit more like databases in the late '90s, early 2000s
[00:42:27.180 --> 00:42:31.340]   than databases today, where everybody and their mother
[00:42:31.340 --> 00:42:39.460]   has their own data fusion analytic database.
[00:42:39.460 --> 00:42:43.940]   But yeah, a little bit about quantization.
[00:42:43.940 --> 00:42:45.900]   So I've mentioned this a lot.
[00:42:45.900 --> 00:42:50.860]   So by default, floats are 32 or 64 bits, like integers are.
[00:42:50.860 --> 00:42:52.860]   Neural networks do not need this.
[00:42:52.860 --> 00:42:54.900]   Digital computers that you're used to programming
[00:42:54.900 --> 00:42:55.740]   are very precise.
[00:42:55.740 --> 00:42:58.260]   They go back to this--
[00:42:58.260 --> 00:43:02.980]   pardon me-- the Z2 by Konrad Zuse.
[00:43:02.980 --> 00:43:06.300]   He made this basically a clock that was a computer.
[00:43:06.300 --> 00:43:08.060]   Physical plates were being pushed around.
[00:43:08.060 --> 00:43:12.540]   And I think this is an AND gate or an XOR gate.
[00:43:12.540 --> 00:43:16.140]   So it only moves if one of the two plates on one side
[00:43:16.140 --> 00:43:17.500]   moves forward.
[00:43:17.500 --> 00:43:19.140]   So it's very physical clockwork.
[00:43:19.140 --> 00:43:22.820]   That's the lineage of digital computers.
[00:43:22.820 --> 00:43:24.460]   At the same time, in the '40s, people
[00:43:24.460 --> 00:43:25.860]   were working on analog computers.
[00:43:25.860 --> 00:43:28.460]   So on the right is a numerical integrator.
[00:43:28.460 --> 00:43:30.660]   That's on the other side of World War II.
[00:43:30.660 --> 00:43:33.460]   I think this is artillery trajectory calculations.
[00:43:33.460 --> 00:43:34.460]   You see there's a ball.
[00:43:34.460 --> 00:43:35.660]   And that ball rolls around.
[00:43:35.660 --> 00:43:37.020]   And you would calculate the speed
[00:43:37.020 --> 00:43:39.460]   that the ball is rolling around by changing the gears.
[00:43:39.460 --> 00:43:41.100]   Neural networks are way more like that.
[00:43:41.100 --> 00:43:44.460]   They're more like-- they're imprecise
[00:43:44.460 --> 00:43:47.220]   because they are the raw physical world
[00:43:47.220 --> 00:43:50.220]   without the intervention of a clock system
[00:43:50.220 --> 00:43:53.200]   to abstract it away and make it all ones and zeros
[00:43:53.200 --> 00:43:55.300]   and specific time steps.
[00:43:55.300 --> 00:43:58.380]   Neural networks are way more like these analog computers.
[00:43:58.380 --> 00:44:00.860]   And so how precise do you need to be
[00:44:00.860 --> 00:44:03.180]   when you're measuring a number that's
[00:44:03.180 --> 00:44:05.660]   coming out of an analog system?
[00:44:05.660 --> 00:44:08.700]   It's never going to be exactly the same with an analog system
[00:44:08.700 --> 00:44:09.900]   anyway.
[00:44:09.900 --> 00:44:14.780]   So why not decrease the precision?
[00:44:14.780 --> 00:44:18.740]   Whereas you change one bit in a digital computer,
[00:44:18.740 --> 00:44:24.220]   and it's like throwing a stick into a clock.
[00:44:24.220 --> 00:44:28.860]   The whole thing explodes and stops running.
[00:44:28.860 --> 00:44:31.140]   So this is the reason why you can aggressively
[00:44:31.140 --> 00:44:34.020]   quantize neural networks in a way
[00:44:34.020 --> 00:44:37.620]   that you can't do with lossily compressing, I don't know,
[00:44:37.620 --> 00:44:38.780]   Postgres.
[00:44:38.780 --> 00:44:43.420]   If you quantized every byte in Postgres down to 4 bits,
[00:44:43.420 --> 00:44:46.140]   you would just get garbage.
[00:44:46.140 --> 00:44:50.700]   So this quantization is really key for performance.
[00:44:50.700 --> 00:44:54.820]   The safe choice, you'll see, is 16 bits.
[00:44:54.820 --> 00:44:59.500]   FP16 or BF Brain Float 16.
[00:44:59.500 --> 00:45:01.900]   Weight quantization only, that means just make
[00:45:01.900 --> 00:45:06.540]   the model itself smaller, makes it smaller in memory.
[00:45:06.540 --> 00:45:09.980]   And then that whole thing about moving it in and out of compute
[00:45:09.980 --> 00:45:12.100]   is easier because it's smaller.
[00:45:12.100 --> 00:45:12.720]   That's great.
[00:45:12.720 --> 00:45:15.020]   And then that doesn't actually quantize the math.
[00:45:15.020 --> 00:45:20.700]   The actual math that happens still happens at 16-bit, 32-bit.
[00:45:20.700 --> 00:45:24.820]   To do activation quantization requires more recent GPUs,
[00:45:24.820 --> 00:45:27.380]   sometimes requires special compilation flags.
[00:45:27.380 --> 00:45:30.020]   Not always is the operation that you want to speed up.
[00:45:30.020 --> 00:45:32.780]   Does it already have a kernel written for you
[00:45:32.780 --> 00:45:37.660]   by TreeDAO or some other wizard to make the GPU go at full speed?
[00:45:37.660 --> 00:45:38.540]   So that's harder.
[00:45:38.540 --> 00:45:40.100]   It doesn't always work.
[00:45:40.100 --> 00:45:42.500]   VLM has great docs on this.
[00:45:42.500 --> 00:45:44.540]   And there's some papers as well.
[00:45:44.540 --> 00:45:49.180]   Give me FP16 or give me death, question mark, is a good paper.
[00:45:49.180 --> 00:45:51.820]   Because the answer is you don't need death.
[00:45:51.820 --> 00:45:52.580]   Don't be dramatic.
[00:45:52.580 --> 00:45:54.620]   You can use the quants.
[00:45:54.620 --> 00:45:57.820]   Evals help you decide whether the quantization is hurting.
[00:45:57.820 --> 00:46:03.020]   So I was running DeepSeq R1 in ternary, actually.
[00:46:03.020 --> 00:46:05.740]   So 1, 0, minus 1 in that demo.
[00:46:05.740 --> 00:46:07.540]   That's extreme quantization.
[00:46:07.540 --> 00:46:09.900]   There's no way the full model performance or anything
[00:46:09.900 --> 00:46:11.900]   close to it is retained.
[00:46:11.900 --> 00:46:13.660]   You need evals to determine whether you've
[00:46:13.660 --> 00:46:16.660]   lost the thing that made you pick the model in the first place.
[00:46:16.660 --> 00:46:20.460]   So make sure you have a way to check this.
[00:46:20.460 --> 00:46:24.220]   And benchmarks, don't trust benchmarks.
[00:46:24.220 --> 00:46:25.780]   People's benchmarks are wrong.
[00:46:25.780 --> 00:46:29.700]   They're different from your workload.
[00:46:29.700 --> 00:46:31.700]   You've got to run this stuff yourself.
[00:46:31.700 --> 00:46:34.100]   So curate your own internal benchmarks
[00:46:34.100 --> 00:46:38.220]   to help you scale up your own taste in models and intuition.
[00:46:38.220 --> 00:46:42.980]   I have more slides on fine tuning in a bit.
[00:46:42.980 --> 00:46:46.220]   But people who want to run their own models
[00:46:46.220 --> 00:46:49.220]   often have this DIY hacker spirit.
[00:46:49.220 --> 00:46:50.720]   And they're like, why should I just
[00:46:50.720 --> 00:46:52.380]   use the weights everybody else is using?
[00:46:52.380 --> 00:46:54.020]   I want to fine tune these things.
[00:46:54.020 --> 00:46:54.900]   This is really hard.
[00:46:54.900 --> 00:46:56.740]   I'll talk more about why it's hard in a bit.
[00:46:56.740 --> 00:46:58.460]   But try to get as far as you can just
[00:46:58.460 --> 00:47:02.500]   with prompting and really control flow around models.
[00:47:02.500 --> 00:47:05.500]   I don't know, DeepSeek R1 writes Python code.
[00:47:05.500 --> 00:47:07.260]   The Python code is wrong.
[00:47:07.260 --> 00:47:10.900]   Take the code, run it, take the error message, pipe it back in.
[00:47:10.900 --> 00:47:15.300]   So writing things around models, instead of fine tuning it
[00:47:15.300 --> 00:47:17.060]   to write better Python code, that's
[00:47:17.060 --> 00:47:19.300]   what all the model providers are doing.
[00:47:19.300 --> 00:47:22.540]   You're hard to compete with them on a lot of this stuff.
[00:47:22.540 --> 00:47:26.420]   So managing prompts and managing control flow around models
[00:47:26.420 --> 00:47:29.220]   is way easier as a software engineer
[00:47:29.220 --> 00:47:35.340]   and has way better ROI per effort ROI.
[00:47:35.340 --> 00:47:39.220]   So definitely start with just prompting, retrieval, et cetera.
[00:47:39.220 --> 00:47:46.140]   Yeah, I want to make sure to talk about the inference
[00:47:46.140 --> 00:47:49.700]   frameworks and what Suri inference looks like.
[00:47:49.700 --> 00:47:52.180]   Running LLMs inference economically
[00:47:52.180 --> 00:47:55.860]   requires a ton of thought and effort on optimization.
[00:47:55.860 --> 00:48:00.860]   This is not something you can sit down and write yourself,
[00:48:00.860 --> 00:48:05.460]   even if you're a code force's top 1%.
[00:48:05.460 --> 00:48:06.660]   There's a lot to write.
[00:48:06.660 --> 00:48:10.620]   A fast matrix multiplication is-- yeah,
[00:48:10.620 --> 00:48:12.900]   the standards are very high.
[00:48:12.900 --> 00:48:16.380]   So the current core of the stack that's most popular
[00:48:16.380 --> 00:48:18.700]   is PyTorch and CUDA.
[00:48:18.700 --> 00:48:24.180]   So PyTorch is a combo of a Python steering library
[00:48:24.180 --> 00:48:29.940]   and then a C++ internal library and libraries
[00:48:29.940 --> 00:48:35.220]   for doing all the hard shit, including CUDA C++,
[00:48:35.220 --> 00:48:38.860]   AKA C++ that runs on GPUs.
[00:48:38.860 --> 00:48:40.460]   That's where all the work gets done.
[00:48:40.460 --> 00:48:42.420]   Python is not usually the bottleneck.
[00:48:42.420 --> 00:48:45.980]   Don't get excited and rewrite that part in Rust.
[00:48:45.980 --> 00:48:48.620]   You're going to find out that that didn't help you that much.
[00:48:48.620 --> 00:48:52.420]   There's some features that make it easier to write Torch
[00:48:52.420 --> 00:48:53.740]   and still get good performance.
[00:48:53.740 --> 00:48:56.700]   So Torch added a compiler a couple of years
[00:48:56.700 --> 00:48:59.220]   ago now in version 2.
[00:48:59.220 --> 00:49:02.500]   But compilers are young until they're 40.
[00:49:02.500 --> 00:49:05.140]   But it's very promising and can get you
[00:49:05.140 --> 00:49:08.100]   most of the speed up of writing a bunch of custom stuff.
[00:49:08.100 --> 00:49:11.060]   But even besides writing custom GPU code,
[00:49:11.060 --> 00:49:12.420]   there's a bunch of things you need
[00:49:12.420 --> 00:49:15.740]   to build on top of raw matmuls, like the stuff that showed up
[00:49:15.740 --> 00:49:20.500]   in my napkin math diagram to serve inference fast.
[00:49:20.500 --> 00:49:21.780]   There's a bunch of caching.
[00:49:21.780 --> 00:49:23.400]   You don't want to roll your own cache.
[00:49:23.400 --> 00:49:26.620]   Rolling your own cache is a recipe for pain.
[00:49:26.620 --> 00:49:29.660]   There's continuous batching is this smart stuff
[00:49:29.660 --> 00:49:32.180]   for rearranging requests as they're on the way.
[00:49:32.180 --> 00:49:36.700]   Speculative decoding is a way to improve your throughput
[00:49:36.700 --> 00:49:39.780]   and has a ton of gotchas.
[00:49:39.780 --> 00:49:42.460]   So you don't want to build all this just for yourself.
[00:49:42.460 --> 00:49:45.220]   This is a clear case for a framework,
[00:49:45.220 --> 00:49:47.500]   just like database management systems.
[00:49:47.500 --> 00:49:51.420]   This is a don't roll your own case rather than a don't
[00:49:51.420 --> 00:49:54.260]   overcomplicate shit with a tool case,
[00:49:54.260 --> 00:49:59.380]   like the classic the two genders in engineering.
[00:49:59.380 --> 00:50:04.300]   So I would strongly recommend the VLM inference server
[00:50:04.300 --> 00:50:06.660]   on a number of grounds.
[00:50:06.660 --> 00:50:11.380]   So like Postgres, VLM started as a Berkeley academic project.
[00:50:11.380 --> 00:50:13.780]   They introduced this thing called paged attention,
[00:50:13.780 --> 00:50:19.740]   paged KV caching, and then kind of ran with it from there.
[00:50:19.740 --> 00:50:22.140]   There's performance numbers, and we can talk about them,
[00:50:22.140 --> 00:50:25.780]   but they're pretty prominent.
[00:50:25.780 --> 00:50:27.780]   People are gunning to beat them on workloads.
[00:50:27.780 --> 00:50:29.540]   And also, don't trust anybody's benchmarks.
[00:50:29.540 --> 00:50:32.020]   You have to run it to decide whether you agree.
[00:50:32.020 --> 00:50:34.220]   Anyway, that doesn't apply just for models.
[00:50:34.220 --> 00:50:36.060]   It also applies for performance.
[00:50:36.060 --> 00:50:39.020]   They really won Mindshare as the inference server,
[00:50:39.020 --> 00:50:43.020]   and so they've attracted a ton of external contributions.
[00:50:43.020 --> 00:50:45.460]   So now, Neural Magic was a startup,
[00:50:45.460 --> 00:50:47.260]   got acquired by Red Hat, a.k.a.
[00:50:47.260 --> 00:50:53.140]   IBM, basically exclusively to support their work on VLM.
[00:50:53.140 --> 00:50:56.620]   And so they got tons of contributions
[00:50:56.620 --> 00:51:01.060]   from any scale, IBM, bunch of people contributing stuff.
[00:51:01.060 --> 00:51:03.500]   And that's really important for open source success.
[00:51:03.500 --> 00:51:05.100]   Open source software succeeds when
[00:51:05.100 --> 00:51:07.740]   it creates this locus for cooperation
[00:51:07.740 --> 00:51:11.820]   between otherwise competing private organizations,
[00:51:11.820 --> 00:51:14.700]   whether they're nonprofit or for profit or whatever.
[00:51:14.700 --> 00:51:16.940]   And VLM has done that.
[00:51:16.940 --> 00:51:19.700]   So it's kind of hard to dislodge a project
[00:51:19.700 --> 00:51:22.700]   like that once it's held that crown for a while.
[00:51:22.700 --> 00:51:25.780]   It's not undislodgable yet, so it's not quite like Postgres,
[00:51:25.780 --> 00:51:27.820]   where you can be like, just use Postgres,
[00:51:27.820 --> 00:51:30.980]   and feel pretty like that's been around for 30 years,
[00:51:30.980 --> 00:51:33.780]   and this is more like 30 months less.
[00:51:33.780 --> 00:51:36.140]   But yeah, also pretty easy to use,
[00:51:36.140 --> 00:51:38.980]   like PIP installable once you have your GPU drivers.
[00:51:38.980 --> 00:51:41.660]   They make an OpenAI compatible API layer,
[00:51:41.660 --> 00:51:47.100]   which NVIDIA has refused to do with TensorRT, LLM, and Triton.
[00:51:47.100 --> 00:51:52.620]   So it's got a bunch of nice features and good performance.
[00:51:52.620 --> 00:51:54.380]   The main alternative, I would suggest,
[00:51:54.380 --> 00:52:00.060]   is NVIDIA's offering the ONNX, TensorRT, TensorRT, LLM,
[00:52:00.060 --> 00:52:01.580]   Triton kind of stack.
[00:52:01.580 --> 00:52:03.820]   There's this NVIDIA stack.
[00:52:03.820 --> 00:52:05.380]   Legally, it's open source, because you
[00:52:05.380 --> 00:52:06.340]   can read the source code.
[00:52:06.340 --> 00:52:09.780]   And it's under, I forget, either Apache or MI2 license.
[00:52:09.780 --> 00:52:12.540]   But if you look at the source code history,
[00:52:12.540 --> 00:52:16.380]   you'll see that it updates in the form of one 10,000 line
[00:52:16.380 --> 00:52:21.060]   commit with 5,000 deletions every week or two that
[00:52:21.060 --> 00:52:23.380]   says fixes.
[00:52:23.380 --> 00:52:26.380]   So pretty hard to maintain a fork.
[00:52:26.380 --> 00:52:29.580]   Pretty hard to-- you don't get input on the roadmap.
[00:52:29.580 --> 00:52:31.460]   VLM, on the other hand, classic.
[00:52:31.460 --> 00:52:34.540]   True open governance and open source.
[00:52:34.540 --> 00:52:37.780]   You can actually participate.
[00:52:37.780 --> 00:52:39.620]   Show up to the biweekly meetings.
[00:52:39.620 --> 00:52:41.460]   It's fun.
[00:52:41.460 --> 00:52:43.780]   Yeah, good performance, but maybe not top.
[00:52:43.780 --> 00:52:45.340]   What's up, Twix?
[00:52:45.340 --> 00:52:47.340]   >>SGLang?
[00:52:47.340 --> 00:52:50.020]   >>Yeah, SGLang, there's some cool stuff.
[00:52:50.020 --> 00:52:53.020]   They have this nice interface for prompt programming
[00:52:53.020 --> 00:52:55.060]   that's kind of cool.
[00:52:55.060 --> 00:52:58.820]   And sometimes they beat VLM on performance.
[00:52:58.820 --> 00:53:00.740]   But yeah, with open source projects,
[00:53:00.740 --> 00:53:03.340]   you win when you can draw the most contribution.
[00:53:03.340 --> 00:53:07.620]   So I feel like even if SGLang is winning over VLM
[00:53:07.620 --> 00:53:10.540]   in certain places currently, I doubt that that will persist.
[00:53:10.540 --> 00:53:11.140]   But we'll see.
[00:53:11.140 --> 00:53:13.700]   SGLang is another good one to look at.
[00:53:13.700 --> 00:53:14.740]   >>Yeah, OK.
[00:53:14.740 --> 00:53:16.860]   My impression was that they're both from Berkeley,
[00:53:16.860 --> 00:53:20.780]   and I thought basically SGLang is kind of the new generation
[00:53:20.780 --> 00:53:23.700]   of-- it's an anointed successor.
[00:53:23.700 --> 00:53:25.180]   >>Yeah, we'll see.
[00:53:25.180 --> 00:53:26.060]   We'll see.
[00:53:26.060 --> 00:53:28.180]   I don't think they've attracted the same degree
[00:53:28.180 --> 00:53:30.700]   of external contribution, which is important.
[00:53:30.700 --> 00:53:32.460]   >>They try to do it.
[00:53:32.460 --> 00:53:33.020]   OK, cool.
[00:53:33.020 --> 00:53:33.540]   >>Yeah.
[00:53:33.540 --> 00:53:35.580]   But yeah, good call out.
[00:53:35.580 --> 00:53:37.780]   That part of the slide's a little bit older,
[00:53:37.780 --> 00:53:42.700]   so I should maybe bump SGLang up to its own part.
[00:53:42.700 --> 00:53:45.260]   If you're going to be running your own inference,
[00:53:45.260 --> 00:53:47.140]   this is a high-performance computing workload.
[00:53:47.140 --> 00:53:48.540]   It's an expensive workload.
[00:53:48.540 --> 00:53:49.500]   Performance matters.
[00:53:49.500 --> 00:53:52.680]   Engineering effort can do 100x speedups
[00:53:52.680 --> 00:53:56.780]   and can take you from hundreds of dollars a megatoken
[00:53:56.780 --> 00:53:59.740]   to dollars or tens of dollars a megatoken.
[00:53:59.740 --> 00:54:04.340]   So you will need to debug performance and optimize it.
[00:54:04.340 --> 00:54:07.380]   And the only tool for doing that is profiling.
[00:54:07.380 --> 00:54:10.540]   So you're going to want to-- even
[00:54:10.540 --> 00:54:12.340]   if you aren't writing your own stuff,
[00:54:12.340 --> 00:54:14.420]   like if you're just using VLM, if you
[00:54:14.420 --> 00:54:17.100]   want to figure out what all these flags do
[00:54:17.100 --> 00:54:19.300]   and which ones you should use on your workload,
[00:54:19.300 --> 00:54:20.880]   you're going to want to profile stuff.
[00:54:20.880 --> 00:54:23.780]   There's built-in profiler support in VLM
[00:54:23.780 --> 00:54:25.660]   to try and make it easy.
[00:54:25.660 --> 00:54:28.620]   So PyTorch has a tracer and profiler.
[00:54:28.620 --> 00:54:30.740]   That's kind of like what VLM integrates with.
[00:54:30.740 --> 00:54:33.980]   There's also NVIDIA Insight, both for creating and viewing
[00:54:33.980 --> 00:54:35.260]   traces.
[00:54:35.260 --> 00:54:39.820]   That's their slightly more boomery corporate performance
[00:54:39.820 --> 00:54:40.340]   debugger.
[00:54:40.340 --> 00:54:43.140]   It's got a lot of nice features, though, can't lie.
[00:54:43.140 --> 00:54:49.340]   But yeah, it's the same basic tracing and profiling stuff,
[00:54:49.340 --> 00:54:52.380]   except there's work on the CPU and on the GPU,
[00:54:52.380 --> 00:54:54.060]   so that makes it a little bit harder.
[00:54:54.060 --> 00:54:55.660]   I would also just generally recommend,
[00:54:55.660 --> 00:55:00.100]   if you're thinking about this a lot, running a tracer
[00:55:00.100 --> 00:55:04.260]   and just looking at the trace a couple of times for PyTorch,
[00:55:04.260 --> 00:55:08.100]   VLM, whatever, just because you learn a ton from looking
[00:55:08.100 --> 00:55:11.100]   at a trace, a trace of an execution,
[00:55:11.100 --> 00:55:14.420]   all the function calls, all the stacks that
[00:55:14.420 --> 00:55:17.820]   resulted in your program running.
[00:55:17.820 --> 00:55:20.460]   No better way to learn about a program.
[00:55:20.460 --> 00:55:22.140]   I prefer it to reading the source code.
[00:55:22.140 --> 00:55:23.900]   That's where I start, and then I go back to the source code
[00:55:23.900 --> 00:55:25.780]   to figure out what things are doing.
[00:55:25.780 --> 00:55:28.660]   It's way easier than trying to build up
[00:55:28.660 --> 00:55:32.740]   a mental model of a programming model and concurrency
[00:55:32.740 --> 00:55:35.420]   implications, et cetera, just from reading source code.
[00:55:35.420 --> 00:55:36.900]   It's unnatural.
[00:55:36.900 --> 00:55:41.980]   Humans were meant to observe processes in evolution, not
[00:55:41.980 --> 00:55:42.860]   as programs.
[00:55:42.860 --> 00:55:46.820]   But yeah, so some recommendations for tools
[00:55:46.820 --> 00:55:47.320]   there.
[00:55:47.320 --> 00:55:50.540]   We also have some demos for how to run this stuff on Modal
[00:55:50.540 --> 00:55:53.620]   if you want to try that out.
[00:55:53.620 --> 00:55:58.700]   As a first pass for GPU optimization for, OK,
[00:55:58.700 --> 00:56:01.260]   is this making good use of the GPU?
[00:56:01.260 --> 00:56:05.620]   Very first pass is this number, GPU utilization.
[00:56:05.620 --> 00:56:08.060]   What fraction of time is anything
[00:56:08.060 --> 00:56:10.460]   running on the GPU at all?
[00:56:10.460 --> 00:56:12.100]   So that catches-- I don't know.
[00:56:12.100 --> 00:56:13.900]   If you looked at my DeepSeek R1, you
[00:56:13.900 --> 00:56:17.220]   would see that this utilization number is really low, like 20%.
[00:56:17.220 --> 00:56:19.740]   That means the host is getting in the way a lot
[00:56:19.740 --> 00:56:22.620]   and stuff isn't running on the GPU a ton.
[00:56:22.620 --> 00:56:26.100]   This is not like model maximum flops utilization or model
[00:56:26.100 --> 00:56:26.920]   flops utilization.
[00:56:26.920 --> 00:56:28.980]   This is not like what fraction of the number
[00:56:28.980 --> 00:56:31.420]   NVIDIA quoted you for flops that you're getting.
[00:56:31.420 --> 00:56:32.700]   This is way far away from that.
[00:56:32.700 --> 00:56:35.220]   This is just like-- this is a smoke check.
[00:56:35.220 --> 00:56:37.620]   Is the GPU running what fraction of the time?
[00:56:37.620 --> 00:56:39.980]   You would like for this to be 100%.
[00:56:39.980 --> 00:56:47.380]   Like, this is-- yeah, that's an attainable goal, 95% to 99%.
[00:56:47.380 --> 00:56:49.660]   Unlike CPU utilization, that's not a problem.
[00:56:49.660 --> 00:56:51.420]   That's a goal.
[00:56:51.420 --> 00:56:54.020]   So GPU utilization here is like a first check.
[00:56:54.020 --> 00:56:56.700]   Problem is, just because work is running on a GPU
[00:56:56.700 --> 00:56:59.300]   doesn't mean progress is being made
[00:56:59.300 --> 00:57:01.060]   or that that work is efficient.
[00:57:01.060 --> 00:57:04.660]   So the two other things to check are power utilization
[00:57:04.660 --> 00:57:05.700]   and temperature.
[00:57:05.700 --> 00:57:09.720]   Fundamentally, GPUs are limited by how much power
[00:57:09.720 --> 00:57:12.020]   they can draw to run their calculations
[00:57:12.020 --> 00:57:14.740]   and how much heat that generates that they
[00:57:14.740 --> 00:57:17.780]   need to get out of the system in order
[00:57:17.780 --> 00:57:20.020]   to keep running without melting.
[00:57:20.020 --> 00:57:26.620]   So you want to see power utilization 80% to 100%.
[00:57:26.620 --> 00:57:29.980]   And you want to see GPU temperatures running high 60
[00:57:29.980 --> 00:57:35.340]   Celsius for the data center GPUs, maybe low 70s,
[00:57:35.340 --> 00:57:37.500]   but pretty close to their thermal design power,
[00:57:37.500 --> 00:57:41.700]   maybe 5 to 10 degrees off of the power at which NVIDIA says,
[00:57:41.700 --> 00:57:43.300]   whoa, warranty's off.
[00:57:46.460 --> 00:57:50.260]   That means you're most likely making
[00:57:50.260 --> 00:57:54.380]   really good use of the GPU, whereas this GPU utilization,
[00:57:54.380 --> 00:57:56.700]   100% that we have here on the left,
[00:57:56.700 --> 00:57:58.580]   is actually a deadlocked system.
[00:57:58.580 --> 00:58:01.140]   It's like two GPUs are both expecting the other
[00:58:01.140 --> 00:58:03.860]   to send a message, like two polite people trying
[00:58:03.860 --> 00:58:05.340]   to go through a door.
[00:58:05.340 --> 00:58:07.540]   And so they're both executing something
[00:58:07.540 --> 00:58:10.500]   because they're both being like, waiting for that message, dog.
[00:58:10.500 --> 00:58:11.980]   But they aren't making any progress.
[00:58:11.980 --> 00:58:13.420]   And the system is hung.
[00:58:13.420 --> 00:58:15.660]   But it has 100% GPU utilization.
[00:58:15.660 --> 00:58:19.220]   So you won't see that that often if you're
[00:58:19.220 --> 00:58:22.180]   running an inference framework.
[00:58:22.180 --> 00:58:26.260]   But it is something to watch out for and why, on Modal,
[00:58:26.260 --> 00:58:29.060]   I learned Rust in order to be able to add these
[00:58:29.060 --> 00:58:30.780]   to our dashboard.
[00:58:30.780 --> 00:58:32.380]   I think it's that important to show it
[00:58:32.380 --> 00:58:36.700]   to people, the power and the temperature.
[00:58:36.700 --> 00:58:37.340]   Cool.
[00:58:37.340 --> 00:58:37.820]   All right.
[00:58:37.820 --> 00:58:39.420]   So I do want to talk about fine tuning
[00:58:39.420 --> 00:58:41.860]   since it was in the title, conscious of time.
[00:58:41.860 --> 00:58:43.500]   So I'm going to rip through this.
[00:58:43.500 --> 00:58:45.400]   And then if we have more time, we
[00:58:45.400 --> 00:58:47.700]   can dive deep via questions.
[00:58:47.700 --> 00:58:50.540]   Sound good, Sean, Noah?
[00:58:50.540 --> 00:58:51.060]   Thumbs up?
[00:58:51.060 --> 00:58:51.580]   All right.
[00:58:51.580 --> 00:58:52.860]   Yeah, that's great.
[00:58:52.860 --> 00:58:54.140]   All right, yeah, fine tuning.
[00:58:54.140 --> 00:58:56.740]   So fine tuning means taking the weights of the model
[00:58:56.740 --> 00:59:00.460]   and using data to customize them, not via rag,
[00:59:00.460 --> 00:59:03.860]   but by actually changing those numbers.
[00:59:03.860 --> 00:59:05.580]   So when does it make sense to do that
[00:59:05.580 --> 00:59:07.300]   and make your own custom model?
[00:59:07.300 --> 00:59:10.620]   If you can take the capabilities that an API has
[00:59:10.620 --> 00:59:12.620]   and distill them into a smaller model--
[00:59:12.620 --> 00:59:14.540]   so train a smaller model to mimic
[00:59:14.540 --> 00:59:17.260]   the behavior of a big model, then you can--
[00:59:17.260 --> 00:59:20.420]   frequently, you don't need all the things like GPT.
[00:59:20.420 --> 00:59:24.020]   The big models know the name of every arrondissement in France
[00:59:24.020 --> 00:59:27.300]   and things about 15th century sculpt--
[00:59:27.300 --> 00:59:29.900]   or esotericism that you probably don't
[00:59:29.900 --> 00:59:31.780]   need in a support chatbot.
[00:59:31.780 --> 00:59:34.940]   So a smaller model with less weights,
[00:59:34.940 --> 00:59:40.100]   less room to store knowledge, could probably
[00:59:40.100 --> 00:59:42.500]   serve your purposes.
[00:59:42.500 --> 00:59:45.820]   I think of this a bit like a Python to Rust rewrite.
[00:59:45.820 --> 00:59:48.020]   You start off when you aren't sure what you need.
[00:59:48.020 --> 00:59:50.020]   You write in Python because it's easy to change,
[00:59:50.020 --> 00:59:52.100]   just like changing a prompt is easy,
[00:59:52.100 --> 00:59:55.260]   and switching between proprietary model providers
[00:59:55.260 --> 00:59:57.460]   is easy, upgrades are easy.
[00:59:57.460 --> 00:59:59.660]   But then once you really understand what you're doing,
[00:59:59.660 --> 01:00:03.900]   you rewrite it in Rust to get better performance.
[01:00:03.900 --> 01:00:05.540]   And then that Rust rewrite is going
[01:00:05.540 --> 01:00:08.500]   to be more maintenance work and harder to update, yada, yada,
[01:00:08.500 --> 01:00:12.300]   but it's going to be 100x cheaper or something.
[01:00:12.300 --> 01:00:14.020]   And so both the good and the bad things
[01:00:14.020 --> 01:00:15.420]   about that kind of rewrite-- it's
[01:00:15.420 --> 01:00:17.780]   a very similar engineering decision in terms
[01:00:17.780 --> 01:00:24.860]   of technical debt, feature velocity, cost of engineers,
[01:00:24.860 --> 01:00:26.300]   all this stuff.
[01:00:26.300 --> 01:00:28.060]   There's a nice product called OpenPipe
[01:00:28.060 --> 01:00:32.380]   that will help you steal capabilities as a service.
[01:00:32.380 --> 01:00:35.740]   So maybe check them out.
[01:00:35.740 --> 01:00:37.740]   If you want tighter control of style,
[01:00:37.740 --> 01:00:39.340]   like you want it to always respond
[01:00:39.340 --> 01:00:42.260]   in the voice of a pirate and never break k-fabe,
[01:00:42.260 --> 01:00:44.220]   fine tuning is pretty good at that.
[01:00:44.220 --> 01:00:46.900]   Relatively small amounts of data can do that.
[01:00:46.900 --> 01:00:49.060]   It's pretty bad at adding knowledge.
[01:00:49.060 --> 01:00:51.700]   That's usually better to do search or retrieval, which
[01:00:51.700 --> 01:00:55.340]   is what people call RAG, like get the knowledge from somewhere
[01:00:55.340 --> 01:00:56.780]   and stuff it in the prompt.
[01:00:56.780 --> 01:00:59.900]   Prompts can get pretty big these days.
[01:00:59.900 --> 01:01:02.020]   So your search doesn't have to be
[01:01:02.020 --> 01:01:04.500]   as good as it needed to be a year and a half ago.
[01:01:04.500 --> 01:01:06.220]   You can get vaguely the right information
[01:01:06.220 --> 01:01:08.180]   and put it in the prompt.
[01:01:08.180 --> 01:01:12.820]   The holy grail would be for you to define a reward function
[01:01:12.820 --> 01:01:14.780]   of what does it mean for this model to do well.
[01:01:14.780 --> 01:01:19.860]   Maybe that's customer retention, NPS, whatever.
[01:01:19.860 --> 01:01:23.340]   And then you could do ML directly on those rewards
[01:01:23.340 --> 01:01:26.020]   to optimize the model for that.
[01:01:26.020 --> 01:01:27.580]   That's the holy grail.
[01:01:27.580 --> 01:01:32.380]   Then you could just sit back and monitor that RL system.
[01:01:32.380 --> 01:01:36.820]   And then you would magically make that reward number go up.
[01:01:36.820 --> 01:01:37.860]   Could be stock price.
[01:01:37.860 --> 01:01:38.860]   That would be nice.
[01:01:38.860 --> 01:01:41.700]   The problem is there's a large gap between the things you
[01:01:41.700 --> 01:01:44.540]   want to improve, and the things that you can actually measure,
[01:01:44.540 --> 01:01:47.060]   and the things that you can provide to a model,
[01:01:47.060 --> 01:01:48.820]   measure quickly enough, et cetera.
[01:01:48.820 --> 01:01:51.260]   And also the rewards need to be unhackable.
[01:01:51.260 --> 01:01:55.860]   They need to be exactly what you want to maximize.
[01:01:55.860 --> 01:01:58.660]   When you do ML, ML is like paperclip maximization.
[01:01:58.660 --> 01:02:00.660]   It's like, you told me to make this number go up.
[01:02:00.660 --> 01:02:02.160]   I'm going to make this number go up.
[01:02:02.160 --> 01:02:05.340]   Imagine the brooms from "The Sorcerer's Apprentice."
[01:02:05.340 --> 01:02:07.100]   So if your rewards aren't something
[01:02:07.100 --> 01:02:10.180]   that's extremely logically correct,
[01:02:10.180 --> 01:02:12.020]   does this code compile?
[01:02:12.020 --> 01:02:14.780]   And does it run faster?
[01:02:14.780 --> 01:02:15.700]   They're hackable.
[01:02:15.700 --> 01:02:18.020]   So there's this famous example from OpenAI
[01:02:18.020 --> 01:02:20.620]   where they trained a model to drive a boat in this boat
[01:02:20.620 --> 01:02:21.620]   racing game.
[01:02:21.620 --> 01:02:23.140]   And it was trying to maximize points.
[01:02:23.140 --> 01:02:24.720]   And what it learned was, actually, you
[01:02:24.720 --> 01:02:28.740]   don't want to win the race and do
[01:02:28.740 --> 01:02:30.660]   what the game is supposed to do, which
[01:02:30.660 --> 01:02:33.900]   is collect these little pips and finish a race.
[01:02:33.900 --> 01:02:36.180]   If you want to score max, what you actually want to do
[01:02:36.180 --> 01:02:38.620]   is find this tiny little corner and slam against the wall
[01:02:38.620 --> 01:02:42.620]   repeatedly, picking up this bonus item that respawns,
[01:02:42.620 --> 01:02:44.820]   and just slamming against the wall over and over again
[01:02:44.820 --> 01:02:50.900]   and pick up the bonus item when it spawns.
[01:02:50.900 --> 01:02:53.740]   Very inhuman.
[01:02:53.740 --> 01:02:56.220]   More like a speed runner playing a video game
[01:02:56.220 --> 01:02:58.380]   than a normal human.
[01:02:58.380 --> 01:03:01.180]   So imagine this, but with your customer support.
[01:03:01.180 --> 01:03:04.500]   Great way to get customers to give a 10 on an NPS
[01:03:04.500 --> 01:03:07.540]   is to hack their machine and say,
[01:03:07.540 --> 01:03:11.020]   your machine is locked down until you put a 10 on our NPS.
[01:03:11.020 --> 01:03:13.660]   So be careful when using that approach.
[01:03:13.660 --> 01:03:15.580]   But that is the direction we're going.
[01:03:15.580 --> 01:03:17.820]   And it's RL for things like reasoning models
[01:03:17.820 --> 01:03:20.700]   gets better and more mainstreamed.
[01:03:20.700 --> 01:03:23.700]   It's kind of the long-term direction we're going.
[01:03:23.700 --> 01:03:26.980]   But that's not where we are today.
[01:03:26.980 --> 01:03:29.460]   Where we are today is really more like stealing capabilities
[01:03:29.460 --> 01:03:34.580]   from public APIs and distilling them.
[01:03:34.580 --> 01:03:38.380]   So the main reason fine-tuning can save costs,
[01:03:38.380 --> 01:03:42.220]   can improve performance, why shouldn't you do it?
[01:03:42.220 --> 01:03:43.980]   Fine-tuning is machine learning.
[01:03:43.980 --> 01:03:47.220]   Running inference is mostly normal software engineering
[01:03:47.220 --> 01:03:50.900]   with some fun spicy bits-- GPUs, floating point numbers.
[01:03:50.900 --> 01:03:53.580]   But machine learning is a whole different beast.
[01:03:53.580 --> 01:03:56.180]   Machine learning engineering has a lot
[01:03:56.180 --> 01:03:59.540]   in common with hardware and with scientific research.
[01:03:59.540 --> 01:04:01.060]   And it's just fucking hard.
[01:04:01.060 --> 01:04:04.780]   You've got non-determinism of the normal variety.
[01:04:04.780 --> 01:04:06.740]   On top of that, there's epistemic uncertainty.
[01:04:06.740 --> 01:04:08.100]   We don't understand these models.
[01:04:08.100 --> 01:04:10.900]   We don't understand the optimization process.
[01:04:10.900 --> 01:04:12.580]   There's all the floating point nonsense,
[01:04:12.580 --> 01:04:15.060]   which is much worse in machine learning than elsewhere.
[01:04:15.060 --> 01:04:17.340]   You've got to maintain a bunch of data pipelines.
[01:04:17.340 --> 01:04:19.460]   No one's favorite form of software engineering.
[01:04:19.460 --> 01:04:21.580]   This is a high-performance computing workload.
[01:04:21.580 --> 01:04:24.860]   Terra or Exaflop scale, if not more.
[01:04:24.860 --> 01:04:27.180]   Like, yeah, high-performance computing sucks.
[01:04:27.180 --> 01:04:29.680]   There's a reason why only the Department of Energy does it.
[01:04:29.680 --> 01:04:34.740]   And now a few people training models.
[01:04:34.740 --> 01:04:36.660]   There's a bunch of bad software out there.
[01:04:36.660 --> 01:04:38.500]   Like, the software in ML is frankly bad.
[01:04:38.500 --> 01:04:42.460]   It's written by people like me with scientific background.
[01:04:42.460 --> 01:04:44.380]   You have to deal-- things are inferential.
[01:04:44.380 --> 01:04:46.820]   You have to deal with statistical inference.
[01:04:46.820 --> 01:04:48.460]   Yeah, there's data involved.
[01:04:48.460 --> 01:04:50.300]   And now data is getting stored in a form
[01:04:50.300 --> 01:04:51.420]   that no one understands.
[01:04:51.420 --> 01:04:53.020]   Like, user data went in.
[01:04:53.020 --> 01:04:55.260]   And somebody can maybe pull a "New York Times" article
[01:04:55.260 --> 01:04:56.900]   directly out of your model weights.
[01:04:56.900 --> 01:04:59.500]   This scares lawyers.
[01:04:59.500 --> 01:05:03.140]   And so that is tricky and probably
[01:05:03.140 --> 01:05:06.740]   is going to require some Supreme Court rulings and so on
[01:05:06.740 --> 01:05:08.860]   to really figure out.
[01:05:08.860 --> 01:05:11.420]   Yeah, and when Mercury is a retrograde, your GPUs run slower.
[01:05:11.420 --> 01:05:11.780]   I'm sorry.
[01:05:11.780 --> 01:05:12.700]   That's just how it is.
[01:05:12.700 --> 01:05:14.260]   It's just, like, the point is there's
[01:05:14.260 --> 01:05:17.060]   a lot of complexity that's very hard to get an engineering
[01:05:17.060 --> 01:05:18.460]   grip on.
[01:05:18.460 --> 01:05:20.580]   So if you can solve it in literally any other way,
[01:05:20.580 --> 01:05:21.240]   try that first.
[01:05:21.240 --> 01:05:22.180]   Be creative.
[01:05:22.180 --> 01:05:24.620]   Think of ways you can solve this problem without fine tuning.
[01:05:24.620 --> 01:05:26.020]   What information can you bring in?
[01:05:26.020 --> 01:05:29.540]   What program control flow can you put around a model?
[01:05:29.540 --> 01:05:32.980]   Like, distillation is the easiest ML problem
[01:05:32.980 --> 01:05:36.700]   because you're using an ML model to mimic an ML model.
[01:05:36.700 --> 01:05:39.140]   And you can write down the math for that.
[01:05:39.140 --> 01:05:39.860]   It's perfect.
[01:05:39.860 --> 01:05:41.140]   It's very easy.
[01:05:41.140 --> 01:05:43.740]   Like, there's a notion of a data-generating process.
[01:05:43.740 --> 01:05:46.340]   In the real world, that's like the climate of the planet
[01:05:46.340 --> 01:05:47.380]   Earth.
[01:05:47.380 --> 01:05:50.340]   But in distillation, it's like an API call.
[01:05:50.340 --> 01:05:51.220]   Much easier.
[01:05:51.220 --> 01:05:53.780]   So if you have never fine-tuned before,
[01:05:53.780 --> 01:05:56.540]   definitely start with stealing capabilities
[01:05:56.540 --> 01:05:58.340]   from OpenAI, a.k.a.
[01:05:58.340 --> 01:06:01.900]   distillation, rather than anything else.
[01:06:01.900 --> 01:06:04.500]   To do this, you're going to need even more high-performance
[01:06:04.500 --> 01:06:04.780]   hardware.
[01:06:04.780 --> 01:06:06.860]   I focused on running models at the beginning.
[01:06:06.860 --> 01:06:09.620]   Fine-tuning blows out your memory budget,
[01:06:09.620 --> 01:06:11.580]   even with these parameter-efficient methods
[01:06:11.580 --> 01:06:13.700]   that are out there.
[01:06:13.700 --> 01:06:15.580]   Like, kind of what happens during training
[01:06:15.580 --> 01:06:18.420]   is you run a program forwards, and then you flip it around
[01:06:18.420 --> 01:06:19.740]   and run it backwards.
[01:06:19.740 --> 01:06:22.620]   So that puts a lot of extra pressure on memory.
[01:06:22.620 --> 01:06:25.420]   Then you also, during training, you want lots of examples
[01:06:25.420 --> 01:06:29.140]   so the model doesn't learn too much from one specific example.
[01:06:29.140 --> 01:06:32.260]   And you also want large batches to make better use
[01:06:32.260 --> 01:06:36.260]   of the big compute and to make better use of all
[01:06:36.260 --> 01:06:38.380]   those floating-point units.
[01:06:38.380 --> 01:06:40.900]   So that puts pressure on memory.
[01:06:40.900 --> 01:06:42.560]   And then optimization just, in general,
[01:06:42.560 --> 01:06:46.300]   requires some extra tensors that are the size of or larger
[01:06:46.300 --> 01:06:47.500]   than the model parameters.
[01:06:47.500 --> 01:06:49.820]   Sorry, some arrays, some extra arrays of floating-point
[01:06:49.820 --> 01:06:53.940]   numbers that are at least the size of the model parameters
[01:06:53.940 --> 01:06:54.860]   themselves.
[01:06:54.860 --> 01:06:57.300]   So you've got gradients and optimizer states.
[01:06:57.300 --> 01:07:01.700]   These are basically like 2 to 10 extra copies of the model
[01:07:01.700 --> 01:07:03.660]   weights are going to be floating around.
[01:07:03.660 --> 01:07:05.300]   There's ways to shard it, but you
[01:07:05.300 --> 01:07:07.300]   can't get around the fact that a lot of this stuff
[01:07:07.300 --> 01:07:09.540]   just needs to be stored.
[01:07:09.540 --> 01:07:15.020]   So you're going to need eight 80-gigabyte GPUs, or 32
[01:07:15.020 --> 01:07:17.020]   of them, connected in a network.
[01:07:17.020 --> 01:07:20.340]   And yeah, the software for that is pretty hard,
[01:07:20.340 --> 01:07:22.220]   or pretty rough.
[01:07:22.220 --> 01:07:25.060]   I already talked about how hard machine learning is.
[01:07:25.060 --> 01:07:27.900]   It's like there are software engineering practices that
[01:07:27.900 --> 01:07:30.980]   can prevent it from being made harder.
[01:07:30.980 --> 01:07:35.820]   I worked on experiment tracking software, weights and biases.
[01:07:35.820 --> 01:07:37.900]   That said, I worked on it for a reason.
[01:07:37.900 --> 01:07:40.300]   It's like when I was training models, the thing I wanted
[01:07:40.300 --> 01:07:45.140]   was being able to store voluminous quantities of data
[01:07:45.140 --> 01:07:46.380]   that come out of my run.
[01:07:46.380 --> 01:07:51.740]   Tons of metrics, gradients, inputs, outputs, loss values.
[01:07:51.740 --> 01:07:53.340]   There's just a bunch of stuff that you
[01:07:53.340 --> 01:07:56.940]   want to keep track of on top of very fast-changing code
[01:07:56.940 --> 01:07:58.580]   and configuration.
[01:07:58.580 --> 01:08:01.380]   And so you want a place to store that.
[01:08:01.380 --> 01:08:03.100]   The software is hard to debug.
[01:08:03.100 --> 01:08:04.480]   You don't know where the bugs are.
[01:08:04.480 --> 01:08:07.020]   So you want to store very raw information
[01:08:07.020 --> 01:08:10.140]   from which you can calculate the thing that reveals your bug.
[01:08:10.140 --> 01:08:13.020]   This is actually, I would say, like Honeycomb,
[01:08:13.020 --> 01:08:15.140]   their approach to observability is very similar.
[01:08:15.140 --> 01:08:17.580]   This is like observability for training runs.
[01:08:17.580 --> 01:08:19.940]   Observability is like recording enough about your system
[01:08:19.940 --> 01:08:23.980]   that you can debug it from your logs without having to SSH in.
[01:08:23.980 --> 01:08:25.580]   Same thing with model training.
[01:08:25.580 --> 01:08:27.740]   So yeah, weights and biases, hosted version,
[01:08:27.740 --> 01:08:29.980]   Neptune's hosted version, MLflow.
[01:08:29.980 --> 01:08:32.580]   You can run yourself.
[01:08:32.580 --> 01:08:33.820]   Yeah.
[01:08:33.820 --> 01:08:34.320]   You--
[01:08:34.320 --> 01:08:35.620]   [INTERPOSING VOICES]
[01:08:35.620 --> 01:08:36.420]   Hm?
[01:08:36.420 --> 01:08:38.100]   TensorBoard?
[01:08:38.100 --> 01:08:41.740]   Yeah, so TensorBoard, you have to run TensorBoard yourself.
[01:08:41.740 --> 01:08:43.320]   There's no real hosted service for it.
[01:08:43.320 --> 01:08:45.500]   I think they shut down TensorBoard.dev.
[01:08:45.500 --> 01:08:47.500]   So even if you're willing to make it public,
[01:08:47.500 --> 01:08:51.140]   you can't even use TensorBoard.dev anymore.
[01:08:51.140 --> 01:08:53.300]   Yeah, that's my most sad kill by Google,
[01:08:53.300 --> 01:08:56.260]   because it hits me personally, or maybe happiest,
[01:08:56.260 --> 01:08:58.860]   because I'm a shareholder in weights and biases.
[01:08:58.860 --> 01:09:04.300]   But yeah, so yeah, TensorBoard is really good
[01:09:04.300 --> 01:09:06.300]   at a small number of experiments.
[01:09:06.300 --> 01:09:08.740]   It's bad at collaboration and bad at large numbers
[01:09:08.740 --> 01:09:10.300]   of experiments.
[01:09:10.300 --> 01:09:13.460]   Other experiment tracking workflows that have gotten more--
[01:09:13.460 --> 01:09:15.340]   or experiment tracking solutions that
[01:09:15.340 --> 01:09:18.180]   have gotten more love, like the venture-backed ones
[01:09:18.180 --> 01:09:23.580]   or the open source ones, are better for that.
[01:09:23.580 --> 01:09:26.460]   So you can-- I would say a lot of software engineers
[01:09:26.460 --> 01:09:28.460]   come into the ML engineers' habitat
[01:09:28.460 --> 01:09:33.780]   and are pretty disgusted to discover the state of affairs.
[01:09:33.780 --> 01:09:35.740]   So you definitely do, in general,
[01:09:35.740 --> 01:09:37.540]   as a software engineer entering this field,
[01:09:37.540 --> 01:09:38.740]   you will be disgusted.
[01:09:38.740 --> 01:09:42.180]   And you should push people to up their SWE standards.
[01:09:42.180 --> 01:09:44.020]   But there's actually a lot of benefit
[01:09:44.020 --> 01:09:48.220]   to fast-moving code in ML engineering.
[01:09:48.220 --> 01:09:50.180]   It is researchy in that way.
[01:09:50.180 --> 01:09:53.660]   So you do want fast iteration.
[01:09:53.660 --> 01:09:55.380]   A lot of software engineering practices
[01:09:55.380 --> 01:09:57.740]   are oriented to a slower cycle of iteration
[01:09:57.740 --> 01:09:59.980]   and less interactive iteration.
[01:09:59.980 --> 01:10:02.700]   So the detente that I've found works
[01:10:02.700 --> 01:10:08.140]   is build internal libraries in normal code files,
[01:10:08.140 --> 01:10:10.820]   but then use them via Jupyter Notebooks
[01:10:10.820 --> 01:10:15.660]   so that you can poke prod, run ad hoc workflows, et cetera.
[01:10:15.660 --> 01:10:17.900]   And then as soon as something in a Jupyter Notebook
[01:10:17.900 --> 01:10:20.540]   starts to become regularly useful,
[01:10:20.540 --> 01:10:23.780]   pull that out into your utils.py, at the very least,
[01:10:23.780 --> 01:10:26.740]   if not an internal library.
[01:10:26.740 --> 01:10:30.220]   So yeah, Noah mentioned at the beginning--
[01:10:30.220 --> 01:10:33.220]   or I forget, maybe it was just me.
[01:10:33.220 --> 01:10:36.740]   Anyway, full-stack deep learning course I taught in 2022
[01:10:36.740 --> 01:10:40.540]   still has the basics of how to run ML engineering.
[01:10:40.540 --> 01:10:42.380]   The main thing that's changed is that we're
[01:10:42.380 --> 01:10:43.960]   talking about fine-tuning here.
[01:10:43.960 --> 01:10:46.420]   And back then, we were talking about training from scratch,
[01:10:46.420 --> 01:10:49.180]   because the foundation model era was only beginning.
[01:10:49.180 --> 01:10:52.100]   But the basic stuff in there, like the YouTube videos,
[01:10:52.100 --> 01:10:55.020]   the lecture-level stuff, is all still, I would say,
[01:10:55.020 --> 01:10:56.860]   pretty much solid gold.
[01:10:56.860 --> 01:10:58.420]   And then the code's rotted a bit,
[01:10:58.420 --> 01:11:01.740]   but it's at least vibes-level helpful.
[01:11:01.740 --> 01:11:06.220]   OK, actually, the observability stuff
[01:11:06.220 --> 01:11:08.100]   is less interesting and relevant.
[01:11:08.100 --> 01:11:12.140]   The main point is the eventual goal with any ML feature
[01:11:12.140 --> 01:11:18.100]   is to build a virtuous cycle, a data flywheel, a data engine,
[01:11:18.100 --> 01:11:21.140]   something that allows you to capture user data, annotate it,
[01:11:21.140 --> 01:11:23.780]   collect it into evals, and improve the underlying system.
[01:11:23.780 --> 01:11:26.780]   This is like-- if you're running your own LM inference,
[01:11:26.780 --> 01:11:28.320]   one of the ways you're going to make
[01:11:28.320 --> 01:11:31.060]   this thing truly better than what you could get elsewhere
[01:11:31.060 --> 01:11:37.340]   is building your own custom semi-self-improving system,
[01:11:37.340 --> 01:11:40.100]   or at least continually-improving system,
[01:11:40.100 --> 01:11:42.900]   based off of user data.
[01:11:42.900 --> 01:11:45.900]   There's some specialized tooling for collecting this stuff up,
[01:11:45.900 --> 01:11:48.260]   whether it's offline style with something
[01:11:48.260 --> 01:11:50.300]   like Weights and Biases Weave.
[01:11:50.300 --> 01:11:54.460]   You can see Sean's recent conversation with Sean
[01:11:54.460 --> 01:11:56.860]   from Weights and Biases on how he used Weave,
[01:11:56.860 --> 01:12:02.020]   among other tools, to win at Sweebench.
[01:12:02.020 --> 01:12:05.780]   >>Then Thomas came on Thursday and went over Weave.
[01:12:05.780 --> 01:12:06.940]   >>Oh, nice.
[01:12:06.940 --> 01:12:12.220]   OK, yeah, that's pure product on Weave, plus Sean--
[01:12:12.220 --> 01:12:14.420]   oh, wait, in this class or somewhere else?
[01:12:14.420 --> 01:12:15.540]   Oh, in this class, awesome.
[01:12:15.540 --> 01:12:17.020]   >>Yeah, Thomas came in on Thursday
[01:12:17.020 --> 01:12:20.760]   and did an hour and a half and change on Weave.
[01:12:20.760 --> 01:12:21.780]   >>Nice, yeah.
[01:12:21.780 --> 01:12:25.300]   So I would say Weave is really good for this offline evals,
[01:12:25.300 --> 01:12:29.180]   which is collect up a data set, kind of run code on it.
[01:12:29.180 --> 01:12:31.260]   The code and the data set co-evolve.
[01:12:31.260 --> 01:12:34.620]   And this is very much how an ML engineer approaches
[01:12:34.620 --> 01:12:37.900]   evaluation, coming from academic benchmarking,
[01:12:37.900 --> 01:12:39.100]   really, originally.
[01:12:39.100 --> 01:12:41.300]   And then there's a different style of evals.
[01:12:41.300 --> 01:12:43.980]   I don't know if you're going to have anybody from Lang Chain
[01:12:43.980 --> 01:12:45.940]   or LOM Index or one of these other people
[01:12:45.940 --> 01:12:50.420]   who are also building these observability tooling.
[01:12:50.420 --> 01:12:52.300]   There's this product engineer style,
[01:12:52.300 --> 01:12:54.100]   which is just collect up information
[01:12:54.100 --> 01:12:56.340]   and then let anybody write to it.
[01:12:56.340 --> 01:12:58.220]   Anybody can come in and annotate a trace
[01:12:58.220 --> 01:13:01.540]   and be like, this one is wrong.
[01:13:01.540 --> 01:13:04.940]   Lang Smith is very open-ended, the tool from Lang Chain,
[01:13:04.940 --> 01:13:08.820]   as are a lot of the other observability tooling--
[01:13:08.820 --> 01:13:12.060]   or sorry, these more online eval-oriented things.
[01:13:12.060 --> 01:13:16.940]   It's about raw stuff from production.
[01:13:16.940 --> 01:13:20.820]   And it's about a living database of all the information
[01:13:20.820 --> 01:13:23.940]   you've learned about your users, your problem,
[01:13:23.940 --> 01:13:25.740]   the behavior of models.
[01:13:25.740 --> 01:13:29.540]   And so it's this very dynamic, active artifact,
[01:13:29.540 --> 01:13:32.740]   which has its place.
[01:13:32.740 --> 01:13:35.820]   I think the more you need input from people who are not
[01:13:35.820 --> 01:13:38.420]   you to evaluate models-- like, for example,
[01:13:38.420 --> 01:13:41.900]   it's producing medical traces, and you are not a doctor.
[01:13:41.900 --> 01:13:44.940]   As opposed to producing code, and you are a programmer,
[01:13:44.940 --> 01:13:47.900]   then being able to bring in more people is more helpful.
[01:13:47.900 --> 01:13:52.420]   And so there's utility to these more online-style things.
[01:13:52.420 --> 01:13:54.460]   You can also actually build this stuff yourself.
[01:13:54.460 --> 01:13:57.180]   One thing I will say is these people
[01:13:57.180 --> 01:13:59.780]   don't know that much more about running these models than you
[01:13:59.780 --> 01:14:01.540]   do and getting them to perform well.
[01:14:01.540 --> 01:14:03.700]   And the workflows are not really set down for this.
[01:14:03.700 --> 01:14:05.580]   So with experiment management, that's
[01:14:05.580 --> 01:14:06.620]   been pretty figured out.
[01:14:06.620 --> 01:14:08.540]   It's an older thing.
[01:14:08.540 --> 01:14:11.420]   And so there's lots of-- the tooling
[01:14:11.420 --> 01:14:15.020]   has good ideas baked into it and will teach you to be better.
[01:14:15.020 --> 01:14:18.080]   These tools are in the design partner phase,
[01:14:18.080 --> 01:14:20.420]   a.k.a. the provide free engineering and design
[01:14:20.420 --> 01:14:26.580]   work for somebody you're also paying for a service phase.
[01:14:26.580 --> 01:14:30.660]   So if you have a good internal data engineering
[01:14:30.660 --> 01:14:34.980]   team that is good at, say, an open telemetry integration,
[01:14:34.980 --> 01:14:39.060]   would love to set up a little ClickHouse instance
[01:14:39.060 --> 01:14:40.420]   or something.
[01:14:40.420 --> 01:14:44.940]   And that's exciting to you, the prospect
[01:14:44.940 --> 01:14:46.600]   of putting something like that together,
[01:14:46.600 --> 01:14:47.740]   you or somebody on your team.
[01:14:47.740 --> 01:14:49.740]   You can build your own with something like this.
[01:14:49.740 --> 01:14:54.700]   And then the front end people can hack on the experience.
[01:14:54.700 --> 01:14:57.780]   So Brian Bischoff at Hex is big on this,
[01:14:57.780 --> 01:15:00.820]   because Hex has both really incredible internal data
[01:15:00.820 --> 01:15:03.580]   engineering and they're a data notebook product.
[01:15:03.580 --> 01:15:05.420]   So they can actually dog food their product
[01:15:05.420 --> 01:15:08.320]   to do their evaluation of their product.
[01:15:08.320 --> 01:15:09.740]   So not everybody's in the situation
[01:15:09.740 --> 01:15:14.300]   to be able to do that, but it's like a bigger fraction
[01:15:14.300 --> 01:15:16.020]   than it is with some of the other stuff
[01:15:16.020 --> 01:15:17.820]   that we've talked about.
[01:15:17.820 --> 01:15:24.020]   More tilted in the build direction than the buy.
[01:15:24.020 --> 01:15:25.100]   OK, so that's everything.
[01:15:25.100 --> 01:15:27.660]   I'll do my quick pitch here.
[01:15:27.660 --> 01:15:29.340]   I mentioned at the beginning, if you
[01:15:29.340 --> 01:15:31.820]   want to run code on GPUs in the cloud,
[01:15:31.820 --> 01:15:37.060]   Modal is the infrastructure provider that I'm working on.
[01:15:37.060 --> 01:15:43.300]   That-- I joined this company because I
[01:15:43.300 --> 01:15:44.460]   thought their shit was great.
[01:15:44.460 --> 01:15:47.020]   I was talking about how much I liked it on social media,
[01:15:47.020 --> 01:15:49.060]   and they're like, what if we paid you to do this?
[01:15:49.060 --> 01:15:50.940]   And I was like, no.
[01:15:50.940 --> 01:15:52.140]   I love this so much.
[01:15:52.140 --> 01:15:54.180]   Please don't pay me to do it, because then people
[01:15:54.180 --> 01:15:56.020]   won't trust me when I tell them it's coded.
[01:15:56.020 --> 01:15:57.580]   But eventually I gave in.
[01:15:57.580 --> 01:16:01.460]   Now I work at Modal, and they pay me to say this.
[01:16:01.460 --> 01:16:05.100]   The same thing I was saying before, which is Modal is great.
[01:16:05.100 --> 01:16:08.020]   It's like, you pay for only the hardware you use.
[01:16:08.020 --> 01:16:12.660]   Important when the hardware is so expensive.
[01:16:12.660 --> 01:16:15.020]   They built the whole--
[01:16:15.020 --> 01:16:18.740]   all the infrastructure is built from the ground up in Rust,
[01:16:18.740 --> 01:16:22.460]   BTW, to design for data-intensive workloads.
[01:16:22.460 --> 01:16:24.820]   There's a great podcast with our co-founder,
[01:16:24.820 --> 01:16:30.100]   with Sean, that completely separate from learning
[01:16:30.100 --> 01:16:31.020]   about Modal.
[01:16:31.020 --> 01:16:36.300]   It's just like, gain 10 IQ points, or 10 levels
[01:16:36.300 --> 01:16:39.580]   in computer infrastructure from hearing the story,
[01:16:39.580 --> 01:16:41.940]   learning about the software that was built,
[01:16:41.940 --> 01:16:43.380]   and how they sped it up.
[01:16:43.380 --> 01:16:46.020]   It's also a great data council talk on it.
[01:16:46.020 --> 01:16:48.500]   Just designed to run stuff fast.
[01:16:48.500 --> 01:16:52.860]   And then, unlike other serverless GPU narrow sense
[01:16:52.860 --> 01:16:57.020]   providers, Modal has code sandboxes, web endpoints,
[01:16:57.020 --> 01:17:00.140]   makes it easy to stand up a user interface around your stuff.
[01:17:00.140 --> 01:17:03.340]   So that's why I ended up going all in on Modal.
[01:17:03.340 --> 01:17:05.900]   It was like, wow, not only does this run my models,
[01:17:05.900 --> 01:17:10.660]   but I learned how to properly use fast API from Modal's
[01:17:10.660 --> 01:17:12.660]   integration with it.
[01:17:12.660 --> 01:17:16.740]   And yeah, that's just the tip of the iceberg
[01:17:16.740 --> 01:17:19.300]   on the additional things that it provides.
[01:17:19.300 --> 01:17:23.260]   So that can be for running your fine-tuning jobs,
[01:17:23.260 --> 01:17:26.240]   if you've decided you want to distill models yourself.
[01:17:26.240 --> 01:17:27.820]   It can be just running the inference,
[01:17:27.820 --> 01:17:31.100]   to be able to scale up and down, and handle changing inference
[01:17:31.100 --> 01:17:32.820]   load, and make sure you're filling up
[01:17:32.820 --> 01:17:35.780]   all the GPUs that you're using.
[01:17:35.780 --> 01:17:38.500]   And it can be for doing your evaluations,
[01:17:38.500 --> 01:17:41.660]   running these things online or offline,
[01:17:41.660 --> 01:17:46.100]   creating data to help you observe your system
[01:17:46.100 --> 01:17:46.980]   and make it better.
[01:17:46.980 --> 01:17:52.820]   So it's like full service, serverless cloud
[01:17:52.820 --> 01:17:59.260]   infrastructure that doesn't require a PhD in Kubernetes.
[01:17:59.260 --> 01:17:59.960]   Great.
[01:17:59.960 --> 01:18:02.780]   All right, that's all I got.
[01:18:02.780 --> 01:18:03.940]   Any questions?
[01:18:03.940 --> 01:18:08.820]   That was sick.
[01:18:08.820 --> 01:18:09.420]   Thanks so much.
[01:18:09.420 --> 01:18:11.980]   We love Modal in this house.
[01:18:11.980 --> 01:18:13.580]   I was in the process of rewriting it,
[01:18:13.580 --> 01:18:16.780]   so everyone that got the-- and also everyone,
[01:18:16.780 --> 01:18:19.680]   Charles is the person that we talked to to get the Modal
[01:18:19.680 --> 01:18:20.900]   credits for the course.
[01:18:20.900 --> 01:18:23.820]   So everyone, a big, big thank you to Charles for that.
[01:18:23.820 --> 01:18:25.980]   But the entire course, every single--
[01:18:25.980 --> 01:18:29.220]   this cohort builds three projects, all of which
[01:18:29.220 --> 01:18:31.740]   are built off of FastAPI that lives in Modal.
[01:18:31.740 --> 01:18:34.300]   So we love Modal here.
[01:18:34.300 --> 01:18:35.220]   It's great.
[01:18:35.220 --> 01:18:39.900]   Yeah, if you ever run into any bugs,
[01:18:39.900 --> 01:18:42.460]   definitely slide into our Slack.
[01:18:42.460 --> 01:18:46.780]   There's a decent chance you'll get co-founder support
[01:18:46.780 --> 01:18:49.220]   if you slide into the Slack.
[01:18:49.220 --> 01:18:53.940]   And yeah, hopefully you've been pointed to the examples page,
[01:18:53.940 --> 01:18:57.280]   modal.com/docs/examples.
[01:18:57.280 --> 01:19:04.300]   I slave to ensure that those things run end-to-end.
[01:19:04.300 --> 01:19:08.980]   They're continuously monitored and run stochastically
[01:19:08.980 --> 01:19:11.380]   at times during the day to ensure that.
[01:19:11.380 --> 01:19:14.500]   So if you run into-- they should run.
[01:19:14.500 --> 01:19:15.900]   They should help you get started.
[01:19:15.900 --> 01:19:17.460]   They're designed to be something you
[01:19:17.460 --> 01:19:19.860]   can build production-grade services off
[01:19:19.860 --> 01:19:22.100]   of as much as possible.
[01:19:22.100 --> 01:19:25.260]   And so yeah, if you want any help with those,
[01:19:25.260 --> 01:19:26.860]   slide into the Slack.
[01:19:26.860 --> 01:19:27.580]   Tag me.
[01:19:27.580 --> 01:19:31.620]   Feel free to tag me on stuff related
[01:19:31.620 --> 01:19:36.060]   to the course or otherwise.
[01:19:36.060 --> 01:19:37.060]   I love the examples.
[01:19:37.060 --> 01:19:39.180]   I should talk to you sometime, how you set all of that up.
[01:19:39.180 --> 01:19:40.380]   Because I was very impressed.
[01:19:40.380 --> 01:19:43.140]   I ran through the comfy UI workflow a couple of days ago.
[01:19:43.140 --> 01:19:44.940]   And I was able to tweak a few things.
[01:19:44.940 --> 01:19:46.300]   I pulled down the code example.
[01:19:46.300 --> 01:19:47.980]   I got a few different things running.
[01:19:47.980 --> 01:19:49.180]   I was like, holy shit.
[01:19:49.180 --> 01:19:51.220]   I just pulled down an example from the internet
[01:19:51.220 --> 01:19:53.500]   and just ran the command that it said to run.
[01:19:53.500 --> 01:19:54.260]   And then it ran.
[01:19:54.260 --> 01:19:55.680]   And I was like, that never happens.
[01:19:55.680 --> 01:19:58.540]   There's always some other thing I have to do.
[01:19:58.540 --> 01:20:01.060]   I was very impressed.
[01:20:01.060 --> 01:20:05.180]   Yeah, part of it is that as an infrastructure product,
[01:20:05.180 --> 01:20:07.660]   the thing that kills being able to run code
[01:20:07.660 --> 01:20:10.100]   is the differences between infrastructure and like,
[01:20:10.100 --> 01:20:16.900]   oh, well, that will only run if you set this LD flags thing
[01:20:16.900 --> 01:20:19.540]   or have this installed.
[01:20:19.540 --> 01:20:20.620]   It works on my machine.
[01:20:20.620 --> 01:20:22.240]   See, the thing about the modal examples
[01:20:22.240 --> 01:20:23.700]   is they all work on my machine.
[01:20:23.700 --> 01:20:26.940]   And my machine is modal, which you can also run them on.
[01:20:26.940 --> 01:20:29.580]   So that does make it a lot easier.
[01:20:29.580 --> 01:20:31.160]   I think that's generally true for being
[01:20:31.160 --> 01:20:35.260]   able to share things that run on modal within your team,
[01:20:35.260 --> 01:20:39.180]   making it easier to do that.
[01:20:39.180 --> 01:20:42.740]   But then separately, like, yeah, the trick--
[01:20:42.740 --> 01:20:44.660]   and this is actually like an engineering trick
[01:20:44.660 --> 01:20:46.660]   that is surprised it took me this long to learn.
[01:20:46.660 --> 01:20:49.220]   It's like there's tests and there's monitoring.
[01:20:49.220 --> 01:20:51.020]   And there are a lot of things that
[01:20:51.020 --> 01:20:52.580]   are really hard to write as tests.
[01:20:52.580 --> 01:20:54.620]   Slow down your iteration speed.
[01:20:54.620 --> 01:20:57.820]   Like, yeah, require a bunch of disgusting mocking
[01:20:57.820 --> 01:21:00.140]   that breaks as often as the actual code does.
[01:21:00.140 --> 01:21:02.700]   Or you could monitor production and fix
[01:21:02.700 --> 01:21:05.500]   issues that arise there, a.k.a. do both.
[01:21:05.500 --> 01:21:10.420]   So yeah, that's an important trick for the modal examples,
[01:21:10.420 --> 01:21:14.020]   but also for all the things you would maybe run using--
[01:21:14.020 --> 01:21:16.460]   as part of running your own language model inference
[01:21:16.460 --> 01:21:18.020]   or running your own AI-powered app.
[01:21:18.020 --> 01:21:20.060]   It's like, monitor the shit out of this thing.
[01:21:20.060 --> 01:21:23.960]   >>Awesome.
[01:21:23.960 --> 01:21:24.460]   Cool.
[01:21:24.460 --> 01:21:27.740]   Well, before we let Charles go, does anybody have any questions?
[01:21:27.740 --> 01:21:30.180]   I know I'm sure given everyone's background here,
[01:21:30.180 --> 01:21:32.180]   there's a lot of-- everyone's brain
[01:21:32.180 --> 01:21:35.140]   feels very full with all of the hardware architecture
[01:21:35.140 --> 01:21:37.420]   that you just learned and terminology.
[01:21:37.420 --> 01:21:40.060]   But just want to open it up for anyone.
[01:21:40.060 --> 01:21:47.260]   >>I think-- I'll kill time while people ask questions.
[01:21:47.260 --> 01:21:51.300]   But I think that it's always intimidating for people
[01:21:51.300 --> 01:21:56.540]   sort of running their own models and fine-tuning them.
[01:21:56.540 --> 01:21:59.820]   I'm just like, what's a really good first exercise
[01:21:59.820 --> 01:22:02.940]   that you could-- probably you have some tutorials on modal
[01:22:02.940 --> 01:22:05.900]   that you would recommend people just go through.
[01:22:05.900 --> 01:22:07.140]   >>Yeah, running your own model.
[01:22:07.140 --> 01:22:10.300]   I would actually say, if you don't
[01:22:10.300 --> 01:22:17.900]   have a MacBook M2 or later with at least 32 gigabytes of RAM,
[01:22:17.900 --> 01:22:19.220]   go ahead and buy one of those.
[01:22:19.220 --> 01:22:21.860]   Get your company to buy it for you.
[01:22:21.860 --> 01:22:25.900]   So that turns out to be actually a really incredible machine
[01:22:25.900 --> 01:22:27.100]   for running local inference.
[01:22:27.100 --> 01:22:28.820]   Has to do with the memory bandwidth stuff
[01:22:28.820 --> 01:22:31.580]   that we talked about, like moving the bytes in and out
[01:22:31.580 --> 01:22:33.300]   really fast.
[01:22:33.300 --> 01:22:35.980]   And so that-- I would actually say
[01:22:35.980 --> 01:22:38.460]   like that was the first thing I did back when you
[01:22:38.460 --> 01:22:41.420]   had to torrent llama weights.
[01:22:41.420 --> 01:22:45.780]   That running it locally-- and there's good tools out there
[01:22:45.780 --> 01:22:48.140]   for this, Ollama.
[01:22:48.140 --> 01:22:51.420]   You can also use the same thing you would run on a cloud server
[01:22:51.420 --> 01:22:53.900]   like VLLM.
[01:22:53.900 --> 01:22:57.020]   That is-- that's probably the easiest way
[01:22:57.020 --> 01:22:59.420]   to get started with running some of your own inference.
[01:22:59.420 --> 01:23:03.140]   And then the cost is amortized more effectively.
[01:23:03.140 --> 01:23:05.740]   And you can use it for other stuff, the computer
[01:23:05.740 --> 01:23:07.620]   that you're using for this.
[01:23:07.620 --> 01:23:11.460]   So that's actually probably my-- it's bad modal marketing
[01:23:11.460 --> 01:23:12.020]   to say that.
[01:23:12.020 --> 01:23:15.100]   But I would say people like to be able to poke and prod.
[01:23:15.100 --> 01:23:17.920]   If you don't already know modal, I know modal well enough
[01:23:17.920 --> 01:23:19.960]   that now it's not any harder for me
[01:23:19.960 --> 01:23:23.260]   to use modal to try these things than to run it on my MacBook.
[01:23:23.260 --> 01:23:24.300]   But it takes some time.
[01:23:24.660 --> 01:23:28.820]   And everybody knows how to use a command line as part
[01:23:28.820 --> 01:23:31.220]   of becoming a software engineer.
[01:23:31.220 --> 01:23:33.740]   So yeah.
[01:23:33.740 --> 01:23:35.540]   So that's my primary recommendation.
[01:23:35.540 --> 01:23:40.220]   For fine-tuning, I would say distilling a model
[01:23:40.220 --> 01:23:43.420]   is the easiest thing to do.
[01:23:43.420 --> 01:23:45.900]   Besides, I guess our demo for fine-tuning,
[01:23:45.900 --> 01:23:47.300]   which I didn't have time to show,
[01:23:47.300 --> 01:23:50.140]   it's like fine-tuning something on somebody's Slack messages
[01:23:50.140 --> 01:23:51.660]   so that it talks like them.
[01:23:51.660 --> 01:23:56.900]   And that's easy, fun, the stakes are low,
[01:23:56.900 --> 01:24:01.340]   and it teaches you some things about the software
[01:24:01.340 --> 01:24:03.580]   and about fine-tuning problems.
[01:24:03.580 --> 01:24:06.940]   But then to really understand what
[01:24:06.940 --> 01:24:10.260]   it means to fine-tune in pursuit of a specific objective,
[01:24:10.260 --> 01:24:12.260]   it's like distillation of a large model.
[01:24:12.260 --> 01:24:18.460]   Yeah, totally.
[01:24:18.460 --> 01:24:21.500]   I did insert a little comment about what distillation means.
[01:24:21.500 --> 01:24:23.980]   Because apparently, a lot of people
[01:24:23.980 --> 01:24:30.180]   kind of view training on output of GPT-4 as distillation.
[01:24:30.180 --> 01:24:33.580]   But the purist would be like, you
[01:24:33.580 --> 01:24:35.820]   have to train on the logits.
[01:24:35.820 --> 01:24:37.620]   Oh, yeah.
[01:24:37.620 --> 01:24:40.420]   Yeah, the teacher-student methods.
[01:24:40.420 --> 01:24:42.380]   Real distillation is different.
[01:24:42.380 --> 01:24:43.340]   Real distillation.
[01:24:43.340 --> 01:24:43.900]   Yeah, yeah.
[01:24:43.900 --> 01:24:46.420]   Oh, so I guess maybe that's a reason
[01:24:46.420 --> 01:24:51.100]   to run your own models to be able to get
[01:24:51.100 --> 01:24:54.300]   the raw output of the model is not tokens.
[01:24:54.300 --> 01:24:57.180]   It's probability for every token.
[01:24:57.180 --> 01:25:01.060]   And so that's a much richer signal for fine-tuning off of.
[01:25:01.060 --> 01:25:04.580]   And so that's what people prefer.
[01:25:04.580 --> 01:25:07.980]   But I guess I was thinking of it in the looser sense
[01:25:07.980 --> 01:25:10.260]   that most people talk about today, which is just like
[01:25:10.260 --> 01:25:12.460]   training to mimic the outputs of the model.
[01:25:12.460 --> 01:25:16.180]   Yeah, create a synthetic corpus of text.
[01:25:16.180 --> 01:25:18.120]   Yeah, yeah.
[01:25:18.120 --> 01:25:19.780]   And when you run a model in production,
[01:25:19.780 --> 01:25:23.780]   you're creating a synthetic corpus of text, you know?
[01:25:23.780 --> 01:25:29.540]   Synthetic corpus of text is somewhat intimidating sounding.
[01:25:29.540 --> 01:25:31.620]   I say as somebody who's used a lot of intimidating
[01:25:31.620 --> 01:25:34.780]   sounding jargon.
[01:25:34.780 --> 01:25:38.460]   But really, the simplest synthetic corpus of text
[01:25:38.460 --> 01:25:41.660]   is all the outputs that the API returned
[01:25:41.660 --> 01:25:43.620]   while it was running in prod.
[01:25:43.620 --> 01:25:47.180]   That's a great thing to fine-tune on.
[01:25:47.180 --> 01:25:51.620]   I just linked to an example here where
[01:25:51.620 --> 01:25:54.900]   someone distilled from R1.
[01:25:54.900 --> 01:25:56.420]   And it was pretty effective.
[01:25:56.420 --> 01:26:00.500]   And it took 48 hours and a few H100s.
[01:26:00.500 --> 01:26:02.180]   And that was it, not that expensive.
[01:26:02.180 --> 01:26:03.460]   Nice.
[01:26:03.460 --> 01:26:04.620]   Yeah, yeah.
[01:26:04.620 --> 01:26:08.460]   So Modal will do fine-tuning jobs up to eight H100s
[01:26:08.460 --> 01:26:10.420]   and up to 24 hours.
[01:26:10.420 --> 01:26:13.820]   We're working on features for bigger scale training,
[01:26:13.820 --> 01:26:19.020]   and both longer in time and larger in number.
[01:26:19.020 --> 01:26:20.900]   But yeah, I would say there's also
[01:26:20.900 --> 01:26:24.500]   a pretty strong argument for keeping your fine-tunes as small
[01:26:24.500 --> 01:26:28.700]   and fast as possible to be able to iterate more effectively
[01:26:28.700 --> 01:26:29.540]   and quickly.
[01:26:29.540 --> 01:26:35.660]   Because it's fun to run on 1,000 GPUs or whatever.
[01:26:35.660 --> 01:26:39.660]   There's this frisson of making machines go brr.
[01:26:39.660 --> 01:26:42.980]   But then when you need to regularly execute that job
[01:26:42.980 --> 01:26:45.900]   to maintain a service that you've promised people
[01:26:45.900 --> 01:26:50.140]   that you will keep up, then it starts to get painful.
[01:26:50.140 --> 01:26:54.700]   Because reliability, cost, it's ungodly slow.
[01:26:54.700 --> 01:26:57.860]   It's 48 hours is a long time to wait for a computer
[01:26:57.860 --> 01:27:02.860]   to do something, even if it is an eggs a flop of operations.
[01:27:02.860 --> 01:27:06.420]   So definitely, when starting out with fine-tuning,
[01:27:06.420 --> 01:27:10.540]   go for the smallest job you can.
[01:27:10.540 --> 01:27:11.180]   Got it.
[01:27:11.180 --> 01:27:12.620]   OK.
[01:27:12.620 --> 01:27:14.500]   All right, I've hogged the mic enough.
[01:27:14.500 --> 01:27:15.220]   Who has questions?
[01:27:15.220 --> 01:27:15.740]   Anyone?
[01:27:15.740 --> 01:27:24.260]   No?
[01:27:24.260 --> 01:27:25.100]   OK, great.
[01:27:25.100 --> 01:27:26.820]   Well, awesome, everybody.
[01:27:26.820 --> 01:27:28.900]   Thanks to you so much, Charles, for coming.
[01:27:28.900 --> 01:27:29.900]   We really appreciate it.

