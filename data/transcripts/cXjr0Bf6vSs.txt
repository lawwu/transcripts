
[00:00:00.000 --> 00:00:03.040]   The talk I'm going to give has the provocative title,
[00:00:03.040 --> 00:00:06.200]   "Neural Networks are for Typecasting."
[00:00:06.200 --> 00:00:10.520]   So the TLDR here is that the embeddings
[00:00:10.520 --> 00:00:13.120]   of a neural network obtained by training on a difficult task
[00:00:13.120 --> 00:00:16.000]   are often more useful than the outputs.
[00:00:16.000 --> 00:00:18.320]   And the main contribution here is
[00:00:18.320 --> 00:00:23.240]   that I like to think about these embeddings actually as types,
[00:00:23.240 --> 00:00:26.760]   that they're sort of analogous to like 32-bit floats
[00:00:26.760 --> 00:00:28.200]   or 8-bit integers.
[00:00:28.200 --> 00:00:32.240]   And the only difference is that they are learned rather than
[00:00:32.240 --> 00:00:34.200]   designed.
[00:00:34.200 --> 00:00:36.440]   And so the goal here is basically
[00:00:36.440 --> 00:00:41.280]   to present an intuition for embeddings in neural networks
[00:00:41.280 --> 00:00:44.760]   that are maybe closer to the intuition
[00:00:44.760 --> 00:00:47.280]   that people who have a programming and CS background
[00:00:47.280 --> 00:00:49.720]   bring, rather than sort of like a math and physics
[00:00:49.720 --> 00:00:54.000]   background, which is where the idea of embeddings comes from.
[00:00:54.000 --> 00:00:57.600]   Or if you're not one for nuance, if I
[00:00:57.600 --> 00:00:59.880]   wanted to put this talk out there on the internet
[00:00:59.880 --> 00:01:03.760]   to get clout and put out my hot takes,
[00:01:03.760 --> 00:01:08.720]   the purpose of neural networks is just glorified typecasting.
[00:01:08.720 --> 00:01:11.720]   So to be clear, when I say types,
[00:01:11.720 --> 00:01:14.440]   like a type is just a collection of data values with a name.
[00:01:14.440 --> 00:01:16.560]   So integers, this is a collection
[00:01:16.560 --> 00:01:19.720]   of bit strings that have a particular name.
[00:01:19.720 --> 00:01:22.680]   Dictionaries, phone numbers, these are the same thing.
[00:01:22.680 --> 00:01:25.520]   And that name basically just describes how those values
[00:01:25.520 --> 00:01:26.840]   are to be interpreted.
[00:01:26.840 --> 00:01:30.360]   Maybe it suggests, OK, what do you do with a phone number?
[00:01:30.360 --> 00:01:32.640]   Well, maybe you pull off the area code,
[00:01:32.640 --> 00:01:35.920]   or maybe you send it to a phone app.
[00:01:35.920 --> 00:01:38.600]   Whereas an integer, even though it might have the same underlying
[00:01:38.600 --> 00:01:40.960]   bit representation, you would do something very different
[00:01:40.960 --> 00:01:41.640]   with it.
[00:01:41.640 --> 00:01:44.100]   And it's important to note that though types are completely
[00:01:44.100 --> 00:01:46.240]   unnecessary for computation, you can do anything
[00:01:46.240 --> 00:01:49.120]   that you could do with types without them,
[00:01:49.120 --> 00:01:52.720]   they're extremely useful for thinking about computation.
[00:01:52.720 --> 00:01:54.640]   And so when we want to organize our thoughts
[00:01:54.640 --> 00:01:59.600]   around our computation, when we want to communicate
[00:01:59.600 --> 00:02:02.720]   what's important, what's not important in our programs,
[00:02:02.720 --> 00:02:05.360]   or when we want to obtain a mathematical understanding
[00:02:05.360 --> 00:02:08.080]   of what is and is not possible with different kinds
[00:02:08.080 --> 00:02:11.000]   of computing, adding types is very useful,
[00:02:11.000 --> 00:02:14.280]   even down at the level of models of computation,
[00:02:14.280 --> 00:02:17.920]   like the typed lambda calculus as opposed to the just baseline
[00:02:17.920 --> 00:02:21.560]   lambda calculus, these very low-level models
[00:02:21.560 --> 00:02:23.200]   of what computation is.
[00:02:23.200 --> 00:02:25.920]   So it's a really useful idea and abstraction.
[00:02:25.920 --> 00:02:28.040]   And one of the points of this talk
[00:02:28.040 --> 00:02:30.360]   is to bring some of these ideas about types
[00:02:30.360 --> 00:02:33.240]   into neural networks.
[00:02:33.240 --> 00:02:34.880]   So the fundamental purpose of types
[00:02:34.880 --> 00:02:36.720]   is to give meaning to binary values.
[00:02:36.720 --> 00:02:41.680]   So this binary string that I've got there, 11001001, et cetera,
[00:02:41.680 --> 00:02:43.600]   could be interpreted as two different things.
[00:02:43.600 --> 00:02:45.880]   It could be interpreted as a bitmap, in which case
[00:02:45.880 --> 00:02:48.680]   you get the image in the bottom left corner of the slide.
[00:02:48.680 --> 00:02:52.160]   Or it could be interpreted as a real number, a binary sequence
[00:02:52.160 --> 00:02:54.800]   that corresponds to a particular real number.
[00:02:54.800 --> 00:02:57.440]   And in that case, it would be interpreted as pi.
[00:02:57.440 --> 00:03:00.280]   This is the binary expansion of pi.
[00:03:00.280 --> 00:03:04.080]   And once we have types, we can convert in between types.
[00:03:04.080 --> 00:03:06.680]   So maybe at one point, I need to, say,
[00:03:06.680 --> 00:03:10.960]   communicate this number pi through a QR code type
[00:03:10.960 --> 00:03:11.800]   mechanism.
[00:03:11.800 --> 00:03:13.440]   And then this image representation of pi
[00:03:13.440 --> 00:03:14.760]   would be really useful.
[00:03:14.760 --> 00:03:17.040]   Or maybe I want to compress it, try compressing it,
[00:03:17.040 --> 00:03:20.520]   and I want to use, say, an image compression algorithm to do it.
[00:03:20.520 --> 00:03:22.520]   So being able to convert it to a different type
[00:03:22.520 --> 00:03:26.280]   gives me access to a whole bunch of different algorithms,
[00:03:26.280 --> 00:03:28.520]   algorithms that are not available for working
[00:03:28.520 --> 00:03:32.640]   with just binary sequences that are real numbers.
[00:03:32.640 --> 00:03:35.240]   And new types can just, in general, make your life easier.
[00:03:35.240 --> 00:03:38.200]   So for floats, multiplying is easy, but adding is hard,
[00:03:38.200 --> 00:03:39.600]   because fundamentally, floats are
[00:03:39.600 --> 00:03:42.440]   a logarithmic representation of numbers.
[00:03:42.440 --> 00:03:44.920]   Whereas for binary numbers, just binary sequences
[00:03:44.920 --> 00:03:47.960]   to represent our numbers, multiplying becomes hard,
[00:03:47.960 --> 00:03:50.320]   but adding is really, really easy.
[00:03:50.320 --> 00:03:52.760]   Adding just basically becomes the XOR operation.
[00:03:52.760 --> 00:03:54.920]   It's not even just a single line of Python.
[00:03:54.920 --> 00:03:57.080]   It's like a single CPU instruction,
[00:03:57.080 --> 00:03:59.760]   which is amazingly simple.
[00:03:59.760 --> 00:04:02.080]   And so just switching to a different type
[00:04:02.080 --> 00:04:04.680]   can sometimes take a problem that was really difficult
[00:04:04.680 --> 00:04:07.200]   and turn it into a problem that is really easy.
[00:04:07.200 --> 00:04:09.920]   So for example, if you're manipulating a directory
[00:04:09.920 --> 00:04:12.960]   by its name, if that name is a string,
[00:04:12.960 --> 00:04:14.400]   then there are lots of problems that
[00:04:14.400 --> 00:04:16.400]   can be kind of difficult to solve,
[00:04:16.400 --> 00:04:18.640]   unless you convert it to a path first.
[00:04:18.640 --> 00:04:22.200]   So the path object available in Python's pathlib
[00:04:22.200 --> 00:04:24.240]   is like a representation--
[00:04:24.240 --> 00:04:25.880]   it's basically you put in a string,
[00:04:25.880 --> 00:04:28.960]   and you get out a structured representation
[00:04:28.960 --> 00:04:31.480]   in which you can easily make the manipulations that you
[00:04:31.480 --> 00:04:32.960]   need to make with directories.
[00:04:32.960 --> 00:04:34.880]   It's much harder to treat it the way you would normally
[00:04:34.880 --> 00:04:35.960]   treat a string.
[00:04:35.960 --> 00:04:37.960]   It's harder to, say, make it all uppercase.
[00:04:37.960 --> 00:04:39.520]   But you don't need to do that with a path.
[00:04:39.520 --> 00:04:41.680]   You need to do things like get rid of the directory,
[00:04:41.680 --> 00:04:44.800]   or append a file extension, something like that.
[00:04:44.800 --> 00:04:46.840]   So I'm going to work through an example of this
[00:04:46.840 --> 00:04:50.040]   to make really clear just how important
[00:04:50.040 --> 00:04:54.400]   it can be to think in terms of different types.
[00:04:54.400 --> 00:04:56.440]   And so the example is going to be based off
[00:04:56.440 --> 00:05:00.880]   of a failed Python enhancement proposal, PEP 313, which
[00:05:00.880 --> 00:05:04.680]   was an April Fool's suggestion to add Roman numeral
[00:05:04.680 --> 00:05:06.800]   literals to Python.
[00:05:06.800 --> 00:05:11.160]   So unfortunately, this PEP was rejected and not added
[00:05:11.160 --> 00:05:12.280]   to Python.
[00:05:12.280 --> 00:05:15.680]   And so there are no Roman numeral literals in Python.
[00:05:15.680 --> 00:05:17.120]   So if we want Roman numerals, we're
[00:05:17.120 --> 00:05:20.240]   going to need to roll our own.
[00:05:20.240 --> 00:05:23.400]   So what we want to be able to do, at least to just get started
[00:05:23.400 --> 00:05:25.560]   with this Roman numerals type, is
[00:05:25.560 --> 00:05:30.120]   we would want to take the built-in integer type in Python,
[00:05:30.120 --> 00:05:33.520]   which can handle Arabic numerals, so the numerals
[00:05:33.520 --> 00:05:36.360]   that folks who write their numbers in English
[00:05:36.360 --> 00:05:37.960]   are very familiar with.
[00:05:37.960 --> 00:05:40.520]   And we want to take those kinds of integers
[00:05:40.520 --> 00:05:43.000]   and turn them into Roman numerals.
[00:05:43.000 --> 00:05:44.800]   Now, a fun fact about Roman numerals
[00:05:44.800 --> 00:05:48.360]   is actually they stop at 3,999.
[00:05:48.360 --> 00:05:50.320]   The Romans did not need to count things
[00:05:50.320 --> 00:05:57.080]   as big as we need to count in the year MMXX, the year 2020.
[00:05:57.080 --> 00:06:00.280]   But so we only need to focus on these numbers.
[00:06:00.280 --> 00:06:02.480]   But even just in this smaller set of numbers
[00:06:02.480 --> 00:06:05.560]   that the Roman numerals properly cover,
[00:06:05.560 --> 00:06:07.680]   there's actually some interesting stuff going on.
[00:06:07.680 --> 00:06:09.960]   So it starts off looking like a tally system.
[00:06:09.960 --> 00:06:12.040]   One is a single i.
[00:06:12.040 --> 00:06:13.280]   Two is two i's.
[00:06:13.280 --> 00:06:14.360]   Three is three i's.
[00:06:14.360 --> 00:06:17.480]   But then four comes out of nowhere with an i, v.
[00:06:17.480 --> 00:06:18.680]   And then five is a v.
[00:06:18.680 --> 00:06:20.600]   After that, six is a vi.
[00:06:20.600 --> 00:06:24.640]   Then these more complicated patterns start to arise.
[00:06:24.640 --> 00:06:29.040]   And by the time we get to 3,999, the mapping
[00:06:29.040 --> 00:06:32.200]   between the number in Arabic and the Roman numeral
[00:06:32.200 --> 00:06:34.200]   representation is not so simple.
[00:06:34.200 --> 00:06:40.600]   So we go from those four numbers, 3, 9, 9, to MMCMXCIX.
[00:06:40.600 --> 00:06:42.320]   And so just looking at this problem
[00:06:42.320 --> 00:06:46.000]   as a single step, as a type conversion from Arabic
[00:06:46.000 --> 00:06:50.080]   to Roman numerals, this looks pretty challenging.
[00:06:50.080 --> 00:06:52.000]   But there's this actually great blog post
[00:06:52.000 --> 00:06:56.040]   by Sandy Metz, who is a Ruby programmer, who
[00:06:56.040 --> 00:06:58.280]   described an insight that actually there
[00:06:58.280 --> 00:07:00.360]   used to be several kinds of Roman numerals.
[00:07:00.360 --> 00:07:04.120]   And one of them is what she calls additive Roman numerals.
[00:07:04.120 --> 00:07:06.640]   And so converting-- the additive Roman numerals
[00:07:06.640 --> 00:07:08.060]   are actually really, really simple.
[00:07:08.060 --> 00:07:11.480]   So the Roman numeral for four, or the additive Roman numeral
[00:07:11.480 --> 00:07:14.440]   for four, is just I, I, I, I. It's four Is.
[00:07:14.440 --> 00:07:18.120]   And then the additive Roman numeral for nine is V, I, I, I,
[00:07:18.120 --> 00:07:19.040]   I.
[00:07:19.040 --> 00:07:22.240]   And so it's much simpler to convert from Arabic
[00:07:22.240 --> 00:07:24.880]   to these additive Roman numerals than to convert from Arabic
[00:07:24.880 --> 00:07:29.120]   to the Roman numerals, like the ones everybody knows about,
[00:07:29.120 --> 00:07:30.960]   which she calls subtractive Roman numerals,
[00:07:30.960 --> 00:07:33.380]   because they've got this special thing where sometimes you
[00:07:33.380 --> 00:07:35.160]   subtract one of the values.
[00:07:35.160 --> 00:07:40.000]   I from V gives you four in normal Roman numerals.
[00:07:40.000 --> 00:07:41.760]   And so it's really easy, actually,
[00:07:41.760 --> 00:07:46.080]   to translate from additive Roman numerals to Roman numerals.
[00:07:46.080 --> 00:07:48.920]   It's very straightforward to just
[00:07:48.920 --> 00:07:52.560]   replace those specific patterns with a subtraction.
[00:07:52.560 --> 00:07:56.480]   And so her insight was actually converting from Arabic
[00:07:56.480 --> 00:08:01.200]   to Roman is one idea, but that should actually be sort of
[00:08:01.200 --> 00:08:02.720]   split up into two pieces.
[00:08:02.720 --> 00:08:05.200]   Arabic is much closer to additive Roman numerals
[00:08:05.200 --> 00:08:08.200]   than it is to Roman numerals, so let's do that step first,
[00:08:08.200 --> 00:08:11.320]   rather than sort of conflating these ideas into two steps.
[00:08:11.320 --> 00:08:13.880]   So I actually had read this blog post a while ago,
[00:08:13.880 --> 00:08:17.320]   but tracked it back down for the purposes of this talk
[00:08:17.320 --> 00:08:19.280]   and decided to implement it myself.
[00:08:19.280 --> 00:08:21.080]   And in the end, it turned out to be actually
[00:08:21.080 --> 00:08:23.240]   just as easy as advertised.
[00:08:23.240 --> 00:08:26.000]   I needed to do a little bit of chin scratching,
[00:08:26.000 --> 00:08:29.800]   but in the end, I got this really, really simple 20
[00:08:29.800 --> 00:08:34.480]   something lines of Python to convert Roman numerals
[00:08:34.480 --> 00:08:38.560]   from their integer representations.
[00:08:38.560 --> 00:08:41.160]   And just as she said, it was so much easier
[00:08:41.160 --> 00:08:43.200]   once I thought of it as a multi-step process
[00:08:43.200 --> 00:08:46.360]   with multiple types in it, not just Arabic numerals
[00:08:46.360 --> 00:08:48.960]   and Roman numerals, but also this new additive Roman
[00:08:48.960 --> 00:08:49.800]   numeral type.
[00:08:49.800 --> 00:08:52.480]   And I even added an additional type in there,
[00:08:52.480 --> 00:08:54.960]   a semi-additive string to make it even simpler,
[00:08:54.960 --> 00:09:01.320]   and turned it into basically just three steps, essentially.
[00:09:01.320 --> 00:09:06.200]   First to this additive type, then to a semi-additive type,
[00:09:06.200 --> 00:09:09.400]   then finally to this subtractive Roman numeral type,
[00:09:09.400 --> 00:09:14.480]   using only the sort of very most basic of Python operations.
[00:09:14.480 --> 00:09:16.800]   So new types can just make your life a lot easier.
[00:09:16.800 --> 00:09:19.200]   We already saw that add Roman is--
[00:09:19.200 --> 00:09:21.320]   additive Roman is much easier to translate
[00:09:21.320 --> 00:09:22.800]   into the full Roman numerals.
[00:09:22.800 --> 00:09:24.920]   But it's also true that addition and multiplication
[00:09:24.920 --> 00:09:27.280]   are actually kind of easier in additive Roman.
[00:09:27.280 --> 00:09:30.680]   So to implement addition on Roman numerals, which you
[00:09:30.680 --> 00:09:32.720]   actually probably want to do, is convert them
[00:09:32.720 --> 00:09:35.840]   to this additive Roman format, then implement addition there,
[00:09:35.840 --> 00:09:40.200]   and then maybe transmit it back to Roman numerals.
[00:09:40.200 --> 00:09:42.720]   So the addition of this type gives us--
[00:09:42.720 --> 00:09:44.720]   not only does it make our initial problem easier,
[00:09:44.720 --> 00:09:46.880]   but it actually makes a whole bunch of other things
[00:09:46.880 --> 00:09:49.080]   we might want to do with Roman numerals easier,
[00:09:49.080 --> 00:09:52.320]   because we've got this nice representation,
[00:09:52.320 --> 00:09:55.560]   this alternate representation in a different type.
[00:09:55.560 --> 00:09:58.840]   And so what this means is this additive Roman type
[00:09:58.840 --> 00:10:00.200]   becomes kind of like a hub.
[00:10:00.200 --> 00:10:01.600]   We want to switch between things.
[00:10:01.600 --> 00:10:04.080]   So if we want to switch over to a Babylonian number system,
[00:10:04.080 --> 00:10:07.440]   which is sexagesimal, meaning it is base 60,
[00:10:07.440 --> 00:10:09.120]   it's where our time system comes from.
[00:10:09.120 --> 00:10:10.600]   If we want to switch over to that,
[00:10:10.600 --> 00:10:12.760]   additive Roman, easier to switch over to Babylonian
[00:10:12.760 --> 00:10:13.640]   than from Roman.
[00:10:13.640 --> 00:10:16.160]   Similarly, if we wanted to switch over to pure tally marks,
[00:10:16.160 --> 00:10:18.120]   it's extremely trivial to switch over
[00:10:18.120 --> 00:10:19.920]   to tally marks from additive Roman,
[00:10:19.920 --> 00:10:22.680]   but much harder in regular Roman.
[00:10:22.680 --> 00:10:25.840]   So if we make something that can convert between these types,
[00:10:25.840 --> 00:10:28.080]   then we actually give ourselves essentially
[00:10:28.080 --> 00:10:31.080]   like an exponential increase in the number of things
[00:10:31.080 --> 00:10:35.160]   that we can do just by doing type conversions.
[00:10:35.160 --> 00:10:37.640]   The issue with this is that new types are actually
[00:10:37.640 --> 00:10:38.880]   kind of hard to find.
[00:10:38.880 --> 00:10:41.000]   So we were able to make this additive Roman type,
[00:10:41.000 --> 00:10:43.720]   and actually, I think in Sandy's description,
[00:10:43.720 --> 00:10:45.520]   she found it in the Wikipedia page
[00:10:45.520 --> 00:10:48.240]   describing the different versions of Roman numerals
[00:10:48.240 --> 00:10:49.280]   that are around there.
[00:10:49.280 --> 00:10:51.800]   But the types that we can make are actually pretty limited,
[00:10:51.800 --> 00:10:53.800]   as always when we're doing traditional computing
[00:10:53.800 --> 00:10:57.400]   with programming in Python rather than machine learning,
[00:10:57.400 --> 00:10:58.880]   by our capacity to reason.
[00:10:58.880 --> 00:11:00.920]   We have to reason our way to a type.
[00:11:00.920 --> 00:11:04.240]   We have to say, OK, here's the way this binary representation
[00:11:04.240 --> 00:11:04.740]   is used.
[00:11:04.740 --> 00:11:07.160]   Let's turn it into a different binary representation
[00:11:07.160 --> 00:11:10.200]   and use it in a different way.
[00:11:10.200 --> 00:11:13.480]   And we have to think through that explicitly.
[00:11:13.480 --> 00:11:16.640]   It wouldn't be great if we could actually just sort of discover
[00:11:16.640 --> 00:11:19.200]   the types that we need, if we could automate
[00:11:19.200 --> 00:11:22.160]   the process of finding the types that we
[00:11:22.160 --> 00:11:24.160]   need for our computer programs.
[00:11:24.160 --> 00:11:26.120]   That would be really, really great.
[00:11:26.120 --> 00:11:29.320]   And the problem, I guess, for traditional machine learning
[00:11:29.320 --> 00:11:32.720]   is that linear models cannot discover new types.
[00:11:32.720 --> 00:11:34.520]   So if I do a linear prediction on data--
[00:11:34.520 --> 00:11:37.520]   so let's imagine a classic example of predicting
[00:11:37.520 --> 00:11:40.000]   whether something is a picture of a cat or a dog.
[00:11:40.000 --> 00:11:42.320]   If I do a linear prediction on that data,
[00:11:42.320 --> 00:11:44.200]   all that happens is the data comes in.
[00:11:44.200 --> 00:11:46.720]   I do one operation on it.
[00:11:46.720 --> 00:11:50.200]   And actually, after that linear operation, I'm basically done.
[00:11:50.200 --> 00:11:52.360]   People include a softmax.
[00:11:52.360 --> 00:11:55.240]   That's not really strictly necessary to get out
[00:11:55.240 --> 00:11:55.960]   your prediction.
[00:11:55.960 --> 00:11:57.840]   Really, once you've done the linear model--
[00:11:57.840 --> 00:12:00.080]   or sorry, once you've done that linear transformation
[00:12:00.080 --> 00:12:02.800]   of taking the weights and dot-producting them
[00:12:02.800 --> 00:12:05.040]   with the data, you're basically just--
[00:12:05.040 --> 00:12:06.320]   you're done.
[00:12:06.320 --> 00:12:06.920]   That's it.
[00:12:06.920 --> 00:12:08.720]   So there's no-- there's none of that ability
[00:12:08.720 --> 00:12:11.220]   to sort of say, oh, what if this data were a different type?
[00:12:11.220 --> 00:12:14.080]   Maybe my prediction problem would be easier.
[00:12:14.080 --> 00:12:16.920]   Whereas neural networks can actually discover new types.
[00:12:16.920 --> 00:12:17.880]   And so if I build--
[00:12:17.880 --> 00:12:20.120]   if I think about what my neural network is doing,
[00:12:20.120 --> 00:12:22.760]   it's applying a couple of different transformations
[00:12:22.760 --> 00:12:24.000]   in sequence.
[00:12:24.000 --> 00:12:25.800]   Right here, I've basically only done one.
[00:12:25.800 --> 00:12:29.840]   But one could do it for a much deeper network.
[00:12:29.840 --> 00:12:33.040]   And then at the very end, once I've gotten that--
[00:12:33.040 --> 00:12:35.680]   once I've gotten that--
[00:12:35.680 --> 00:12:38.260]   once I'm-- just before I'm ready to make my prediction,
[00:12:38.260 --> 00:12:40.720]   I actually apply that linear predict function.
[00:12:40.720 --> 00:12:41.880]   I take some weights.
[00:12:41.880 --> 00:12:43.920]   I multiply them by the activations
[00:12:43.920 --> 00:12:44.960]   of the previous layer.
[00:12:44.960 --> 00:12:46.540]   And then I pass it through a softmax.
[00:12:46.540 --> 00:12:47.840]   And that gives me my prediction.
[00:12:47.840 --> 00:12:52.000]   So the final layer, actually, the top of the neural network
[00:12:52.000 --> 00:12:54.200]   is this linear prediction step.
[00:12:54.200 --> 00:12:56.960]   And so you could think of it as actually what's going on
[00:12:56.960 --> 00:12:59.260]   is that every other part of the neural network
[00:12:59.260 --> 00:13:03.640]   is doing a type conversion from data--
[00:13:03.640 --> 00:13:06.120]   the type data, which here is images,
[00:13:06.120 --> 00:13:09.060]   to the type embedding or embeddings of images.
[00:13:09.060 --> 00:13:11.780]   And those embeddings are going to be some complicated, high
[00:13:11.780 --> 00:13:13.400]   dimensional representation.
[00:13:13.400 --> 00:13:17.800]   They're hard to understand what's going on in an embedding.
[00:13:17.800 --> 00:13:19.760]   But the property that they have is
[00:13:19.760 --> 00:13:22.040]   that it's really easy to use them to do the downstream
[00:13:22.040 --> 00:13:26.320]   prediction task by design.
[00:13:26.320 --> 00:13:28.560]   So this is typically called an embedding.
[00:13:28.560 --> 00:13:33.200]   So this view that what we're doing is creating a new type,
[00:13:33.200 --> 00:13:36.200]   the result of that type is typically called an embedding.
[00:13:36.200 --> 00:13:38.520]   And the intuition there is that our data,
[00:13:38.520 --> 00:13:43.000]   sitting in its natural form, in its natural binary format
[00:13:43.000 --> 00:13:47.520]   or in its natural matrix format, is in this tangled up state
[00:13:47.520 --> 00:13:50.400]   where maybe pictures of some foxes and dogs
[00:13:50.400 --> 00:13:52.800]   are very close to pictures of some cats
[00:13:52.800 --> 00:13:54.120]   because they share colors.
[00:13:54.120 --> 00:13:56.240]   And then other pictures of things that are very much
[00:13:56.240 --> 00:13:59.240]   like dogs are very far away from dogs.
[00:13:59.240 --> 00:14:03.360]   And so it can be very difficult to, with a simple model,
[00:14:03.360 --> 00:14:08.840]   split out our dogs from our cats or solve, in general, our task.
[00:14:08.840 --> 00:14:12.160]   And so what happens instead is that in the course
[00:14:12.160 --> 00:14:15.760]   of that neural network's application of transformations
[00:14:15.760 --> 00:14:20.040]   to data, it produces an alternative coordinate space,
[00:14:20.040 --> 00:14:22.320]   an alternative representation of the data,
[00:14:22.320 --> 00:14:25.200]   such that all the dogs are on, say, the left-hand side
[00:14:25.200 --> 00:14:28.080]   and all the cats are on the right-hand side,
[00:14:28.080 --> 00:14:30.360]   and things that are kind of ambiguous right there,
[00:14:30.360 --> 00:14:32.960]   which we're not sure whether this pangolin is
[00:14:32.960 --> 00:14:33.880]   a dog or a cat.
[00:14:33.880 --> 00:14:38.240]   That's not one of our possible responses is not pangolin.
[00:14:38.240 --> 00:14:40.440]   And so it sort of ends up right here in the middle.
[00:14:40.440 --> 00:14:42.120]   And so this intuition that it's embedding
[00:14:42.120 --> 00:14:43.920]   comes from the world of topology,
[00:14:43.920 --> 00:14:48.080]   comes from physics, the idea of taking a collection of points
[00:14:48.080 --> 00:14:50.600]   and then putting them in a different shape
[00:14:50.600 --> 00:14:52.640]   in a different space.
[00:14:52.640 --> 00:14:55.160]   So it's an intuition that doesn't bring ideas
[00:14:55.160 --> 00:14:57.320]   from computer science so much as it does bring ideas
[00:14:57.320 --> 00:14:58.840]   from applied math.
[00:14:58.840 --> 00:15:00.960]   One of the consequences of this perspective
[00:15:00.960 --> 00:15:03.360]   is that we should think of neural networks as being cut
[00:15:03.360 --> 00:15:04.160]   differently.
[00:15:04.160 --> 00:15:06.400]   The traditional way of representing a neural network--
[00:15:06.400 --> 00:15:10.880]   and this is from a Conde Nast/Tech article--
[00:15:10.880 --> 00:15:14.160]   is to think of them as an input layer, then hidden layers,
[00:15:14.160 --> 00:15:15.720]   then an output layer.
[00:15:15.720 --> 00:15:19.120]   And that emphasizes this split between things
[00:15:19.120 --> 00:15:21.200]   that are visible and things that are invisible,
[00:15:21.200 --> 00:15:22.880]   sort of things that are on the interior and things
[00:15:22.880 --> 00:15:24.280]   that are on the exterior.
[00:15:24.280 --> 00:15:26.760]   But this is, I think, not the right way to split them up.
[00:15:26.760 --> 00:15:28.960]   The way we should sort of split them up is actually--
[00:15:28.960 --> 00:15:30.400]   so four parts.
[00:15:30.400 --> 00:15:34.520]   So rather than tripartite, a tetrapartite partition
[00:15:34.520 --> 00:15:37.960]   of our neural networks, where we have data and predictions
[00:15:37.960 --> 00:15:39.680]   on the ends there.
[00:15:39.680 --> 00:15:41.000]   We can still keep those.
[00:15:41.000 --> 00:15:44.200]   But then we split, basically, between that final linearly
[00:15:44.200 --> 00:15:47.080]   separable embedding, that embedding that is really
[00:15:47.080 --> 00:15:48.720]   useful for a linear model.
[00:15:48.720 --> 00:15:51.320]   And then we split that off from basically everything else,
[00:15:51.320 --> 00:15:54.040]   which is the sort of internal representations of this model,
[00:15:54.040 --> 00:15:56.840]   which need not necessarily be a useful embedding
[00:15:56.840 --> 00:16:00.000]   for any linear model.
[00:16:00.000 --> 00:16:03.200]   So we should focus on that embedding part there.
[00:16:03.200 --> 00:16:05.480]   And actually, this connects to some ways
[00:16:05.480 --> 00:16:06.860]   of thinking about neural networks
[00:16:06.880 --> 00:16:10.080]   that are popular in the sort of traditional stats community.
[00:16:10.080 --> 00:16:14.160]   So this is two tweets from Daniela Witten,
[00:16:14.160 --> 00:16:18.880]   who was at one point the emcee of this Women in Statistics
[00:16:18.880 --> 00:16:21.360]   and Data Science Twitter account, which
[00:16:21.360 --> 00:16:25.760]   every week features a different woman in data science.
[00:16:25.760 --> 00:16:31.280]   And so during her week, she made sure to basically kind
[00:16:31.280 --> 00:16:33.280]   of troll the internet as much as possible.
[00:16:33.280 --> 00:16:34.940]   And one thing she did was point out
[00:16:34.940 --> 00:16:38.160]   how many things are just linear models, where you're
[00:16:38.160 --> 00:16:40.280]   trying to guess something.
[00:16:40.280 --> 00:16:45.400]   And you're trying to make a model that guesses an output.
[00:16:45.400 --> 00:16:48.980]   And she'll say whether it's just a linear model or not.
[00:16:48.980 --> 00:16:51.360]   And her answer for deep learning was kind of interesting.
[00:16:51.360 --> 00:16:54.680]   She said, as a function of the nonlinear activations,
[00:16:54.680 --> 00:16:56.960]   it's just a linear model.
[00:16:56.960 --> 00:17:01.360]   That is, once you only think about that last layer there,
[00:17:01.360 --> 00:17:03.440]   basically everything we think about neural networks
[00:17:03.440 --> 00:17:06.320]   is pulled from linear models, things
[00:17:06.320 --> 00:17:09.560]   like SGD and regularization.
[00:17:09.560 --> 00:17:13.080]   We draw those ideas from the world of linear modeling.
[00:17:13.080 --> 00:17:16.340]   And what we've changed is just the representation
[00:17:16.340 --> 00:17:17.720]   that goes into the linear model.
[00:17:17.720 --> 00:17:19.880]   We've changed the type of data that the linear model
[00:17:19.880 --> 00:17:20.760]   operates on.
[00:17:20.760 --> 00:17:22.560]   And that's actually a much smaller change
[00:17:22.560 --> 00:17:27.640]   than you would think for how impressive the behavior
[00:17:27.640 --> 00:17:29.440]   and performance of neural networks are.
[00:17:29.440 --> 00:17:32.880]   I think it sort of suggests how useful a good linear model can
[00:17:32.880 --> 00:17:36.880]   be once it's fed good data.
[00:17:36.880 --> 00:17:40.040]   Though the one important difference I want to point out
[00:17:40.040 --> 00:17:42.520]   is that the intermediate layers of a neural network
[00:17:42.520 --> 00:17:44.920]   are also typecasting functions.
[00:17:44.920 --> 00:17:46.520]   So it's almost as though if we wanted
[00:17:46.520 --> 00:17:49.880]   to convert from a float 32 to, say, a float 16,
[00:17:49.880 --> 00:17:53.840]   so from single precision floats to half precision floats,
[00:17:53.840 --> 00:17:56.560]   on our way to doing that, we also spun off,
[00:17:56.560 --> 00:17:59.280]   oh, this is what it would look like as a 24-bit float.
[00:17:59.280 --> 00:18:02.960]   And this isn't the way our normal typecasting functions
[00:18:02.960 --> 00:18:03.720]   behave.
[00:18:03.720 --> 00:18:06.640]   Normally, we would just go straight from float 32
[00:18:06.640 --> 00:18:08.480]   to float 16.
[00:18:08.480 --> 00:18:10.060]   But neural networks are not like that.
[00:18:10.060 --> 00:18:11.940]   Neural networks actually produce effectively
[00:18:11.940 --> 00:18:14.600]   a series of embeddings that get progressively more
[00:18:14.600 --> 00:18:17.040]   useful to a linear model.
[00:18:17.040 --> 00:18:19.080]   So the way that I actually like to think about it
[00:18:19.080 --> 00:18:22.160]   is that a neural network is a composable typecaster.
[00:18:22.160 --> 00:18:25.280]   It's self-composed of composable typecasters.
[00:18:25.280 --> 00:18:27.120]   So each chunk of your neural network,
[00:18:27.120 --> 00:18:30.480]   as you slice off each layer, is a different casting
[00:18:30.480 --> 00:18:31.840]   to a different type.
[00:18:31.840 --> 00:18:33.680]   And you can compose these together
[00:18:33.680 --> 00:18:36.360]   to get a casting into the destination
[00:18:36.360 --> 00:18:42.000]   type, this linearly separable data at the end.
[00:18:42.000 --> 00:18:44.800]   So I think it's important to take a moment
[00:18:44.800 --> 00:18:48.640]   to explicitly state why I think this perspective is useful.
[00:18:48.640 --> 00:18:51.320]   I think the biggest thing is that it makes it more clear
[00:18:51.320 --> 00:18:53.640]   how to compose neural networks.
[00:18:53.640 --> 00:18:55.480]   So one of the advantages of neural networks,
[00:18:55.480 --> 00:18:58.680]   you hear people talk about, OK, why are neural networks so good?
[00:18:58.680 --> 00:19:00.960]   And why did the people who worked on neural networks
[00:19:00.960 --> 00:19:03.440]   back in the late '90s to early 2000s,
[00:19:03.440 --> 00:19:05.440]   when they weren't that impressive in terms
[00:19:05.440 --> 00:19:07.520]   of their practical performance, why did they think
[00:19:07.520 --> 00:19:09.080]   neural networks were what it was?
[00:19:09.080 --> 00:19:11.520]   Why did they think that this was the important technology?
[00:19:11.520 --> 00:19:14.560]   And the answer almost all of them
[00:19:14.560 --> 00:19:16.940]   give is that neural networks are composable.
[00:19:16.940 --> 00:19:19.160]   I can take little modules and combine them together.
[00:19:19.160 --> 00:19:21.320]   But if you look at the way we write neural networks,
[00:19:21.320 --> 00:19:23.840]   we often write them such that the output
[00:19:23.840 --> 00:19:26.080]   is this final prediction.
[00:19:26.080 --> 00:19:28.160]   The output of the network is just
[00:19:28.160 --> 00:19:33.640]   dog or a one hot vector that is the dog or the softmax
[00:19:33.640 --> 00:19:38.160]   of the logits, whatever it is.
[00:19:38.160 --> 00:19:40.000]   And that's actually not something
[00:19:40.000 --> 00:19:41.960]   you'd want to put into another neural network.
[00:19:41.960 --> 00:19:43.600]   You wouldn't actually really want
[00:19:43.600 --> 00:19:47.640]   to take that output, that softmax output directly,
[00:19:47.640 --> 00:19:50.480]   and put it into another neural network.
[00:19:50.480 --> 00:19:55.600]   That's basically just a distribution over your classes.
[00:19:55.600 --> 00:19:58.480]   And that's not always the really useful thing.
[00:19:58.480 --> 00:20:00.960]   The thing that's actually much more useful and actually makes
[00:20:00.960 --> 00:20:03.480]   a good input to a downstream neural network
[00:20:03.480 --> 00:20:07.680]   is that layer before the linear transformation, the softmax,
[00:20:07.680 --> 00:20:08.600]   that embedding.
[00:20:08.600 --> 00:20:11.360]   Or even just concatenate together several layers
[00:20:11.360 --> 00:20:15.080]   and turn that into, say, a multi-channel image that
[00:20:15.080 --> 00:20:17.480]   represents the input to the neural network.
[00:20:17.480 --> 00:20:21.040]   That is a really good input to a downstream neural network.
[00:20:21.040 --> 00:20:24.760]   So this allows us to compose, to combine in a chain,
[00:20:24.760 --> 00:20:27.200]   multiple neural networks.
[00:20:27.200 --> 00:20:29.560]   It also encourages you to de-emphasize
[00:20:29.560 --> 00:20:31.000]   supervised learning.
[00:20:31.000 --> 00:20:33.080]   If the goal of training a neural network
[00:20:33.080 --> 00:20:35.280]   is to get this embedding here at the end,
[00:20:35.280 --> 00:20:37.920]   rather than directly to solve the task,
[00:20:37.920 --> 00:20:40.160]   then the task is actually less important.
[00:20:40.160 --> 00:20:42.240]   The important part of what the task, I think,
[00:20:42.240 --> 00:20:44.400]   really brings us is that it tries
[00:20:44.400 --> 00:20:49.200]   to encourage a rich, useful embedding representation.
[00:20:49.200 --> 00:20:51.360]   So it encourages you to instead think
[00:20:51.360 --> 00:20:54.080]   about unsupervised learning, or as I have it here,
[00:20:54.080 --> 00:20:57.400]   fun supervised learning, which is,
[00:20:57.400 --> 00:21:00.760]   in addition to probably generating better embeddings,
[00:21:00.760 --> 00:21:02.480]   is also less expensive.
[00:21:02.480 --> 00:21:07.200]   If I want to do supervised learning on every image posted
[00:21:07.200 --> 00:21:08.760]   on the internet, not only do I need
[00:21:08.760 --> 00:21:10.800]   to download every image posted on the internet,
[00:21:10.800 --> 00:21:13.600]   I need to get humans to label every single image posted
[00:21:13.600 --> 00:21:14.400]   on the internet.
[00:21:14.400 --> 00:21:16.080]   And humans have written a bunch of stuff
[00:21:16.080 --> 00:21:18.320]   around images on the internet, but they
[00:21:18.320 --> 00:21:21.520]   haven't done it in a way that's quite useful enough to be--
[00:21:21.520 --> 00:21:23.160]   or that's quite the right format to be
[00:21:23.160 --> 00:21:24.960]   useful for supervised learning.
[00:21:24.960 --> 00:21:27.720]   But if I do something like an autoencoder,
[00:21:27.720 --> 00:21:29.960]   so an unsupervised learning task on images,
[00:21:29.960 --> 00:21:32.240]   where the goal is to take an image in and return it
[00:21:32.240 --> 00:21:35.880]   at the end, then I don't actually
[00:21:35.880 --> 00:21:37.000]   need those labels anymore.
[00:21:37.000 --> 00:21:38.380]   And so I remove, actually, what's
[00:21:38.380 --> 00:21:42.160]   the most expensive and difficult step in my supervised learning
[00:21:42.160 --> 00:21:43.880]   pipeline.
[00:21:43.880 --> 00:21:47.880]   So by focusing on generating good embeddings,
[00:21:47.880 --> 00:21:51.200]   you focus-- you actually, I think,
[00:21:51.200 --> 00:21:53.560]   recognize that there are better ways
[00:21:53.560 --> 00:21:57.400]   to get the downstream final thing that you need.
[00:21:57.400 --> 00:22:00.160]   When it comes time to do the downstream supervised task,
[00:22:00.160 --> 00:22:02.320]   you could take those embeddings that you learned
[00:22:02.320 --> 00:22:05.720]   in an easier, cheaper, more scalable, unsupervised manner,
[00:22:05.720 --> 00:22:12.480]   and then fine tune them to get that final supervised task
[00:22:12.480 --> 00:22:13.120]   solved.
[00:22:13.120 --> 00:22:16.080]   And in that case, you're just training a linear model.
[00:22:16.080 --> 00:22:19.560]   And all the tools and ideas from linear models and statistics
[00:22:19.560 --> 00:22:22.040]   come in handy.
[00:22:22.040 --> 00:22:25.200]   This perspective also pushes you towards this model
[00:22:25.200 --> 00:22:26.800]   amalgamation style.
[00:22:26.800 --> 00:22:31.680]   So this is a graph of a fake machine learning pipeline
[00:22:31.680 --> 00:22:35.280]   that takes in raw data and then generates
[00:22:35.280 --> 00:22:37.040]   three different things-- a prediction of,
[00:22:37.040 --> 00:22:39.600]   let's say this is a social network, who
[00:22:39.600 --> 00:22:41.640]   might be friends with a user.
[00:22:41.640 --> 00:22:43.080]   So this is raw data about a user,
[00:22:43.080 --> 00:22:45.000]   maybe what they've been clicking on,
[00:22:45.000 --> 00:22:47.720]   like their email and information about them.
[00:22:47.720 --> 00:22:50.720]   And it produces, first, a user embedding, and then
[00:22:50.720 --> 00:22:53.040]   a prediction of who they are friends with, and also
[00:22:53.040 --> 00:22:54.940]   a recommendation of maybe other people
[00:22:54.940 --> 00:22:56.720]   they could be friends with or other content
[00:22:56.720 --> 00:22:59.060]   on this social media network that they might like to see.
[00:22:59.060 --> 00:23:01.140]   It also predicts what they're going to click next.
[00:23:01.140 --> 00:23:03.800]   Maybe that might be helpful for your content delivery network
[00:23:03.800 --> 00:23:06.440]   to come up with smarter things to cache.
[00:23:06.440 --> 00:23:08.040]   But all of them pass through.
[00:23:08.040 --> 00:23:10.080]   Rather than going straight from raw user data
[00:23:10.080 --> 00:23:12.120]   to click prediction or straight from raw data
[00:23:12.120 --> 00:23:14.640]   to friend prediction, they pass through this central node
[00:23:14.640 --> 00:23:16.440]   here of the user embedding.
[00:23:16.440 --> 00:23:19.680]   And so this is behaving kind of like that additive Roman type
[00:23:19.680 --> 00:23:21.080]   that we had before.
[00:23:21.080 --> 00:23:27.000]   It's a useful intermediate type on which the operations we
[00:23:27.000 --> 00:23:28.960]   really want to do are easier.
[00:23:28.960 --> 00:23:31.780]   And so all of these arrows here are actually DNNs.
[00:23:31.780 --> 00:23:34.300]   But the difference is that the most important DNN
[00:23:34.300 --> 00:23:36.940]   in the entire pipeline is this first one here
[00:23:36.940 --> 00:23:39.740]   that generates from the raw data a user embedding.
[00:23:39.740 --> 00:23:43.100]   And that neural network is the most important one.
[00:23:43.100 --> 00:23:45.900]   And that's the one that's best understood
[00:23:45.900 --> 00:23:48.420]   as just a typecasting network.
[00:23:48.420 --> 00:23:51.260]   And in fact, all of this click prediction, recommendation,
[00:23:51.260 --> 00:23:52.360]   and friend prediction, these could even
[00:23:52.360 --> 00:23:54.180]   just be linear models on the user embedding
[00:23:54.180 --> 00:23:55.020]   if it's good enough.
[00:23:55.020 --> 00:23:59.160]   And actually, this is not just a sort of toy example.
[00:23:59.160 --> 00:24:01.600]   Or at least, it's not just for toy examples.
[00:24:01.600 --> 00:24:04.720]   So I've talked with folks at Google and folks at Twitter.
[00:24:04.720 --> 00:24:06.160]   And this is actually what they do.
[00:24:06.160 --> 00:24:09.440]   They may have like 150 models in production.
[00:24:09.440 --> 00:24:14.000]   But actually, the core of it is an embedding model
[00:24:14.000 --> 00:24:14.940]   that then gets--
[00:24:14.940 --> 00:24:16.740]   a model that produces these embeddings that
[00:24:16.740 --> 00:24:19.360]   gets fed to everything else.
[00:24:19.360 --> 00:24:21.020]   It also raises some important questions
[00:24:21.020 --> 00:24:23.440]   for the future of ML power technology.
[00:24:23.440 --> 00:24:26.260]   So just like three kind of research almost questions
[00:24:26.260 --> 00:24:30.680]   or broad scale questions that come up out of this.
[00:24:30.680 --> 00:24:35.040]   The first is sort of how can we document our types?
[00:24:35.040 --> 00:24:39.320]   An IEEE of NNLearn types would actually be really great.
[00:24:39.320 --> 00:24:42.160]   So IEEE is the ones who put out that floating point standard.
[00:24:42.160 --> 00:24:43.600]   And they define how you're supposed
[00:24:43.600 --> 00:24:45.520]   to behave on a bunch of--
[00:24:45.520 --> 00:24:49.440]   like if you implement floating point, what does that mean?
[00:24:49.440 --> 00:24:52.240]   How does that constrict the behavior of your type?
[00:24:52.240 --> 00:24:55.160]   An IEEE standard for say embeddings of images,
[00:24:55.160 --> 00:24:59.440]   embeddings of natural languages, like there's
[00:24:59.440 --> 00:25:01.840]   lots of data types out there that get used all the time
[00:25:01.840 --> 00:25:02.720]   in machine learning.
[00:25:02.720 --> 00:25:04.440]   And having a centralized standard
[00:25:04.440 --> 00:25:06.640]   would both level the ML playing field
[00:25:06.640 --> 00:25:09.760]   and make it easier to develop ML applications without having
[00:25:09.760 --> 00:25:11.520]   Google scale resources.
[00:25:11.520 --> 00:25:14.380]   And it would actually centralize some of the really hard work
[00:25:14.380 --> 00:25:16.940]   that's out there, like removing bias.
[00:25:16.940 --> 00:25:20.760]   Extremely difficult to do on your own.
[00:25:20.760 --> 00:25:24.240]   The techniques are very sort of like experimental.
[00:25:24.240 --> 00:25:26.360]   And there's not a clear choice.
[00:25:26.360 --> 00:25:28.080]   There's not a cookbook.
[00:25:28.080 --> 00:25:31.280]   But if we had a centralized repository for doing this,
[00:25:31.280 --> 00:25:33.060]   then we could come to a consensus
[00:25:33.060 --> 00:25:35.680]   on what an unbiased embedding of an image is.
[00:25:35.680 --> 00:25:37.240]   And that would be extremely useful.
[00:25:37.240 --> 00:25:39.680]   Also, things like compression to get high performance.
[00:25:39.680 --> 00:25:41.120]   This is non-trivial.
[00:25:41.120 --> 00:25:42.600]   Putting this all in one place, we
[00:25:42.600 --> 00:25:44.240]   could end up with a neural network that
[00:25:44.240 --> 00:25:47.120]   does that first embedding step and does it in a millisecond
[00:25:47.120 --> 00:25:49.040]   instead of a second.
[00:25:49.040 --> 00:25:52.440]   And so I think that adopting an approach in which we have
[00:25:52.440 --> 00:25:54.360]   defined types that are--
[00:25:54.360 --> 00:25:58.280]   and an embedding image available in the Python standard library
[00:25:58.280 --> 00:26:00.920]   could be really useful for machine learning.
[00:26:00.920 --> 00:26:03.480]   Also, on a more research level, there
[00:26:03.480 --> 00:26:05.600]   are a bunch of ideas in algebraic data types.
[00:26:05.600 --> 00:26:08.560]   So once you have types, you can actually--
[00:26:08.560 --> 00:26:10.920]   there's like a mathematics of working with types.
[00:26:10.920 --> 00:26:12.840]   So there are some types, products types,
[00:26:12.840 --> 00:26:13.960]   exponential types.
[00:26:13.960 --> 00:26:15.480]   There's also been some exotic ideas
[00:26:15.480 --> 00:26:18.920]   in, I think, the last 10 years or so of calculus on data types
[00:26:18.920 --> 00:26:21.680]   to generate things like lists arise
[00:26:21.680 --> 00:26:25.320]   from applying Taylor expansion to expressions
[00:26:25.320 --> 00:26:26.760]   in the algebra of data types.
[00:26:26.760 --> 00:26:29.180]   And I think that there might be some low-hanging fruit out
[00:26:29.180 --> 00:26:31.000]   there for ways that we can combine
[00:26:31.000 --> 00:26:35.280]   the results of neural networks in a smart way or extend them.
[00:26:35.280 --> 00:26:37.520]   And then lastly, now that we-- if we really
[00:26:37.520 --> 00:26:40.160]   think that what's going on here is that the neural networks are
[00:26:40.160 --> 00:26:42.560]   learning a smart embedding, maybe neural networks
[00:26:42.560 --> 00:26:43.560]   aren't the right choice.
[00:26:43.560 --> 00:26:46.680]   It hurts me to say this as a big fan of neural networks
[00:26:46.680 --> 00:26:49.040]   and somebody who's all in on them.
[00:26:49.040 --> 00:26:51.880]   But maybe there's actually better ways to learn types.
[00:26:51.880 --> 00:26:53.920]   The fundamental goal here in machine learning
[00:26:53.920 --> 00:26:57.320]   is to learn programs, is to learn computer programs that
[00:26:57.320 --> 00:27:00.200]   take inputs and produce the appropriate outputs.
[00:27:00.200 --> 00:27:03.080]   And if we actually take a more typed approach to this
[00:27:03.080 --> 00:27:05.200]   and we realize that what we're really trying to do
[00:27:05.200 --> 00:27:07.700]   most of the time is learning types that make our programs
[00:27:07.700 --> 00:27:09.680]   trivial to implement, that really
[00:27:09.680 --> 00:27:13.040]   changes what we think are the most important research
[00:27:13.040 --> 00:27:16.440]   directions to pursue.
[00:27:16.440 --> 00:27:18.600]   So there's a nice little blog post there
[00:27:18.600 --> 00:27:22.040]   that indicates what that calculus of data types,
[00:27:22.040 --> 00:27:26.000]   where that arises from.
[00:27:26.000 --> 00:27:28.680]   So this is just a perspective.
[00:27:28.680 --> 00:27:30.800]   I've come from combining together
[00:27:30.800 --> 00:27:34.040]   a couple of different ideas that are out there in the world of ML
[00:27:34.040 --> 00:27:36.440]   that you may have come across and trying to take this idea
[00:27:36.440 --> 00:27:38.280]   that the embeddings of a neural network
[00:27:38.280 --> 00:27:40.080]   are actually more useful than the outputs
[00:27:40.080 --> 00:27:42.920]   and trying to give it a bit of a more computer science flavor
[00:27:42.920 --> 00:27:45.660]   and to think of these embeddings actually as types.
[00:27:45.660 --> 00:27:47.360]   And I think this has actually helped
[00:27:47.360 --> 00:27:49.200]   me interpret a lot of the things I've
[00:27:49.200 --> 00:27:52.400]   learned in the recent salons that we've had
[00:27:52.400 --> 00:27:54.240]   and in recent papers that I've read.
[00:27:54.240 --> 00:27:57.040]   And hopefully, you'll find this useful as well.
[00:27:57.040 --> 00:28:01.800]   All right, so if there's any questions,
[00:28:01.800 --> 00:28:04.200]   if anybody has any questions they want to ask,
[00:28:04.200 --> 00:28:07.760]   I'll stick around for a little bit to answer them.
[00:28:07.760 --> 00:28:11.240]   If not, we are a little bit over our time.
[00:28:11.240 --> 00:28:13.200]   And so I'll just end it.
[00:28:13.700 --> 00:28:16.600]   [AUDIO OUT]
[00:28:16.600 --> 00:28:21.580]   - That was a great talk.
[00:28:21.580 --> 00:28:22.420]   I have one comment.
[00:28:22.420 --> 00:28:24.420]   I guess I'm curious about--
[00:28:24.420 --> 00:28:27.220]   there's probably an analog here between neural networks that
[00:28:27.220 --> 00:28:29.460]   are explicitly designed to be compositional or modular.
[00:28:29.460 --> 00:28:32.580]   So I'm thinking of modular neural network architectures
[00:28:32.580 --> 00:28:34.860]   that, for example, Jacob Andreas has worked on, where you
[00:28:34.860 --> 00:28:37.980]   explicitly have different neural modules which you can piece
[00:28:37.980 --> 00:28:39.460]   together in various ways.
[00:28:39.460 --> 00:28:41.900]   And the output of one neural module, for example,
[00:28:41.900 --> 00:28:44.580]   is some sort of embedding that's useful for identifying
[00:28:44.580 --> 00:28:46.540]   the color of an object in an image,
[00:28:46.540 --> 00:28:48.660]   some sort of embedding that's useful for identifying
[00:28:48.660 --> 00:28:51.020]   the count, the number of objects in an image.
[00:28:51.020 --> 00:28:53.440]   And so there probably are some tie-ins between the kinds
[00:28:53.440 --> 00:28:55.940]   of types you've mentioned here and these neural network
[00:28:55.940 --> 00:28:58.540]   architectures that specialize for producing
[00:28:58.540 --> 00:29:01.100]   certain outputs of specific types.
[00:29:01.100 --> 00:29:02.860]   - Yeah, yeah, that's actually a good point.
[00:29:02.860 --> 00:29:06.100]   I think one thing that is suggested by this
[00:29:06.100 --> 00:29:08.400]   is that maybe actually you want those intermediate types
[00:29:08.400 --> 00:29:12.160]   to also be, say, useful for linearly predicting features,
[00:29:12.160 --> 00:29:13.920]   like you mentioned, the color of an image
[00:29:13.920 --> 00:29:14.840]   or something like that.
[00:29:14.840 --> 00:29:17.640]   Or it could also aid with problems of explainability
[00:29:17.640 --> 00:29:18.560]   and interpretability.
[00:29:18.560 --> 00:29:21.520]   If you have submodules of the network that are explicitly
[00:29:21.520 --> 00:29:24.040]   dedicated by means of, like, they
[00:29:24.040 --> 00:29:26.600]   must produce an embedding that's useful for a particular task
[00:29:26.600 --> 00:29:30.640]   to, say, shape inference or texture inference
[00:29:30.640 --> 00:29:34.480]   or being useful downstream for segmentation
[00:29:34.480 --> 00:29:36.560]   or not being useful for segmentation.
[00:29:36.560 --> 00:29:38.520]   You could also explicitly say, do not
[00:29:38.520 --> 00:29:41.400]   be useful for this particular downstream task.
[00:29:41.400 --> 00:29:44.960]   I think, yeah, you could enrich these intermediate embeddings
[00:29:44.960 --> 00:29:47.640]   that the neural networks are producing and make them--
[00:29:47.640 --> 00:29:54.280]   yeah, make them more powerful, make them more useful,
[00:29:54.280 --> 00:29:55.520]   and make them more modular.
[00:29:55.520 --> 00:29:58.080]   So I think it's an idea a lot of people
[00:29:58.080 --> 00:29:59.920]   are kind of circling around.
[00:29:59.920 --> 00:30:02.160]   And I'm excited to see where it goes.
[00:30:02.160 --> 00:30:05.520]   [MUSIC PLAYING]
[00:30:05.520 --> 00:30:08.880]   [MUSIC PLAYING]
[00:30:09.040 --> 00:30:12.400]   [MUSIC PLAYING]
[00:30:12.560 --> 00:30:15.920]   [MUSIC PLAYING]
[00:30:15.920 --> 00:30:17.400]   (upbeat music)

