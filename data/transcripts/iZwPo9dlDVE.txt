
[00:00:00.000 --> 00:00:10.760]   Hey there, everyone. Hope you're all doing well today. So today, we're going to get started
[00:00:10.760 --> 00:00:16.440]   with the event, Accelerating Diffusion with Hugging Face. So I'm Anish. I work as a machine
[00:00:16.440 --> 00:00:20.880]   learning engineer on the growth team at Weights & Biases, which I'll walk through later. Don't
[00:00:20.880 --> 00:00:25.680]   want to spoil the fun ahead. But I want to say I'm very excited to be joined with Patrick
[00:00:25.680 --> 00:00:31.280]   and Zach from Hugging Face. And so before we start off today's presentation itself,
[00:00:31.280 --> 00:00:35.560]   let's talk about what we're going to learn. We're going to learn how you can accelerate
[00:00:35.560 --> 00:00:41.200]   your diffusion by using the Accelerate package by Hugging Face, a diffuser package by Hugging
[00:00:41.200 --> 00:00:45.440]   Face, a collection of tools that they released that make it very easy for you to do your
[00:00:45.440 --> 00:00:51.160]   generative workflows in a very streamlined fashion. With that, you're also going to learn
[00:00:51.160 --> 00:00:55.160]   about built-in experiment tracking, model checkpointing, and rich media logging provided
[00:00:55.160 --> 00:00:59.920]   by Weights & Biases. So at any point, if you want to engage with any of the speakers here,
[00:00:59.920 --> 00:01:05.440]   you have the option to ask any questions in the chat on your right-hand side. And while
[00:01:05.440 --> 00:01:09.440]   we're going on, let us know what you think. And at the very end, we'll have a few minutes
[00:01:09.440 --> 00:01:15.840]   for Q&A. And looking forward to all of your opinions about this talk. With that, I'll
[00:01:15.840 --> 00:01:17.440]   hand it over to Zach.
[00:01:17.440 --> 00:01:26.080]   >> Great. Thank you so much, Anish. My name is Zachary Mueller. I'm a deep learning software
[00:01:26.080 --> 00:01:33.920]   engineer at Hugging Face. And I'm going to be talking to you about the Accelerate library.
[00:01:33.920 --> 00:01:40.140]   Also known as three powerful sub-libraries for PyTorch. So as I mentioned, I'm a deep
[00:01:40.140 --> 00:01:45.880]   learning software engineer at Hugging Face. I'm also very much an API design geek. I love
[00:01:45.880 --> 00:01:51.880]   learning how software libraries are built and how communities engage with them, such
[00:01:51.880 --> 00:02:02.080]   as TensorFlow, Fast.AI, Hugging Face Transformers, and whatnot, which all centers around Accelerate.
[00:02:02.080 --> 00:02:08.440]   Accelerate is designed to be a library that provides three different subcategories. So
[00:02:08.440 --> 00:02:16.240]   one is what I'll call a launching interface. The next is a training library. And then finally,
[00:02:16.240 --> 00:02:24.080]   the big model inference package that is provided. Which is sort of the foundations for these
[00:02:24.080 --> 00:02:30.220]   big models doing inference on small compute. Now, first let's talk about this launching
[00:02:30.220 --> 00:02:37.020]   interface. Because Accelerate is designed to help you train and run Python scripts on
[00:02:37.020 --> 00:02:45.280]   any compute. But can't you just do Python run my file? Not quite. So, for example, if
[00:02:45.280 --> 00:02:52.360]   I wanted to take a Python file and just call Python some script, what winds up happening
[00:02:52.360 --> 00:02:58.720]   is this will use at most a single GPU on your system. In order to actually do what we call
[00:02:58.720 --> 00:03:05.720]   multi GPU training or multi node training, you have to use a command called torch run
[00:03:05.720 --> 00:03:13.000]   out of PyTorch. And specify how many GPUs you want to use. And nodes means number of
[00:03:13.000 --> 00:03:20.080]   computers and prox means number of GPUs you're using. And then it can get even more complex
[00:03:20.080 --> 00:03:25.300]   when you bring in different high level wrappers, such as DeepSpeed, which has its own command
[00:03:25.300 --> 00:03:31.320]   line interface. And so now in order to have code that can be launched on GPU, TPU, and
[00:03:31.320 --> 00:03:35.680]   what have you, you need to memorize all these different commands. There has to be an easier
[00:03:35.680 --> 00:03:43.480]   way. As a result, we have Accelerate launch. Accelerate launch is designed to be a sort
[00:03:43.480 --> 00:03:50.840]   of central command for you to launch on DeepSpeed, fully sharded data parallelism, multiple GPUs,
[00:03:50.840 --> 00:03:58.560]   single GPUs, TPUs, and CPUs without actually having to use the training aspect of Accelerate.
[00:03:58.560 --> 00:04:05.040]   You could use Accelerate solely to make it a simple way for you to launch your scripts.
[00:04:05.040 --> 00:04:09.440]   Now how does this work? Well, as a result, or as it might sound like, we need a way to
[00:04:09.440 --> 00:04:13.760]   know what your system has. And so there's a simple command line interface that will
[00:04:13.760 --> 00:04:18.160]   ask you a series of questions saying what type of machine are you using? Are you going
[00:04:18.160 --> 00:04:22.120]   to be doing distributed training and other sorts of configurations that we can keep track
[00:04:22.120 --> 00:04:30.320]   of and can use quickly whenever we call this Accelerate launch function? Or don't. You
[00:04:30.320 --> 00:04:36.480]   absolutely don't have to go through this very mildly cumbersome command line interface.
[00:04:36.480 --> 00:04:41.680]   Instead, you can just use Accelerate launch and pass in all the inputs that are needed.
[00:04:41.680 --> 00:04:48.720]   And what you'll find is these parameters basically fall in line with whatever we would
[00:04:48.720 --> 00:04:54.040]   be wrapping under the hood. So for example, with training on multiple GPUs or launching
[00:04:54.040 --> 00:04:59.000]   on multiple GPUs, Torch Run looks like that first line there where we have those end nodes
[00:04:59.000 --> 00:05:06.080]   and proc. And the Accelerate version of that is passing the multi GPU flag and the number
[00:05:06.080 --> 00:05:12.040]   of GPUs you're training on. Another option for you to do instead is if you don't want
[00:05:12.040 --> 00:05:16.520]   to have to pass in the multi GPU stuff and whatnot, but instead want us to figure out
[00:05:16.520 --> 00:05:22.080]   roughly what your computer takes, we have a very quick Accelerate config default that
[00:05:22.080 --> 00:05:28.440]   will just write a very flat default configuration, finding the number of GPUs you have and writing
[00:05:28.440 --> 00:05:36.880]   this out for you. The last and personally my favorite part of our launching interface
[00:05:36.880 --> 00:05:42.480]   is we have something called the notebook launcher. So before this existed, it wasn't really possible
[00:05:42.480 --> 00:05:47.600]   to train on multiple GPUs out of a Jupyter notebook. And so you couldn't make use of
[00:05:47.600 --> 00:05:53.960]   say Kabel's new two GPU instance that they provide. With the notebook launcher, however,
[00:05:53.960 --> 00:06:00.900]   you write a training loop function and make sure that you don't share any or initialize
[00:06:00.900 --> 00:06:06.000]   any CUDA memory before you start it, pass in the arguments and how many GPUs you want
[00:06:06.000 --> 00:06:12.200]   to use. And then Accelerate can go and launch that command on multiple GPUs and still print
[00:06:12.200 --> 00:06:18.320]   out everything REPL style directly in your Jupyter notebook.
[00:06:18.320 --> 00:06:26.520]   So that was talking about how do we launch these Python scripts on TPUs, GPUs and whatnot.
[00:06:26.520 --> 00:06:32.160]   But when I do that, without modifying my code, am I actually going to be training on multiple
[00:06:32.160 --> 00:06:40.680]   GPUs? And that answer might surprise you. No, it won't. Because what winds up happening
[00:06:40.680 --> 00:06:47.640]   is you need to customize your script in order to wrap around whatever different libraries
[00:06:47.640 --> 00:06:54.400]   might need to do this multiple GPU or TPU training. And usually it's large chunks of
[00:06:54.400 --> 00:06:57.760]   code. You have to wrap your model around a few things, your data loaders around a few
[00:06:57.760 --> 00:07:05.600]   things. Be careful with how batches are done. Accelerate solves this by just saying use
[00:07:05.600 --> 00:07:12.200]   the same code, add in three lines of code of our library and be on your way. Let's look
[00:07:12.200 --> 00:07:17.000]   more about what that's like. So here's a very basic training loop, right?
[00:07:17.000 --> 00:07:26.360]   You have iterating over a data loader, optimizer, zero grading, moving inputs to devices, getting
[00:07:26.360 --> 00:07:32.800]   the model outputs, calculating the loss, backwards, pass and going through the optimizer steps
[00:07:32.800 --> 00:07:42.400]   and scheduler steps. Let's look at how accelerate would modify this. As you can see, it's basically
[00:07:42.400 --> 00:07:48.120]   printed here about five to ten lines of code. But you could, of course, clump this together.
[00:07:48.120 --> 00:07:56.080]   I have it here for readability. You essentially add three things. You import the accelerator,
[00:07:56.080 --> 00:08:03.080]   the busy accelerator, and call the most important function, accelerate, accelerator.prepare.
[00:08:03.080 --> 00:08:08.880]   In this, you would pass any sort of PyTorch specific objects. That would be your data
[00:08:08.880 --> 00:08:14.600]   loader, your model, your optimizer, and your scheduler. And then from here, accelerate
[00:08:14.600 --> 00:08:20.420]   will first make sure that the data is automatically pushed to the right device. That is why inputs
[00:08:20.420 --> 00:08:29.020]   and targets is commented out there. And then second, replace loss.backwards with accelerator.backwards.
[00:08:29.020 --> 00:08:38.300]   With these basically four lines of code, accelerate will let you automatically work with distributed
[00:08:38.300 --> 00:08:45.160]   data parallelism, PyTorch XLA, or even just regular PyTorch on a single GPU without having
[00:08:45.160 --> 00:08:51.660]   to change your code at all between the various setups. But there's also a lot more that we
[00:08:51.660 --> 00:08:56.860]   can do with that as well. First, let's sort of go in depth on, like, what actually happened
[00:08:56.860 --> 00:09:01.500]   in there, though. Because I said we wrapped around a few things. But what? So, accelerator
[00:09:01.500 --> 00:09:07.740]   looked at how our configuration is set up. So, are we on multiple GPUs? Are we on TPUs?
[00:09:07.740 --> 00:09:14.140]   And whatnot. And then the data loader was converted into one that can dispatch each
[00:09:14.140 --> 00:09:20.380]   batch onto a separate GPU. Now, I don't mean there's four copies of your data at any given
[00:09:20.380 --> 00:09:27.020]   time. Instead, as the batch gets pulled from the data loader, it will say, okay, if I have
[00:09:27.020 --> 00:09:37.540]   8 GPUs in a batch of 64, each GPU has a batch of 8. And when all of the gradients are calculated,
[00:09:37.540 --> 00:09:44.380]   they'll get clumped together as though you did one batch of 64. Third thing that happened
[00:09:44.380 --> 00:09:49.780]   was the model was wrapped in the appropriate distributed data parallelism wrapper. This
[00:09:49.780 --> 00:09:54.220]   could either be from Torch Distributed or Torch XLA, as I mentioned earlier. Because
[00:09:54.220 --> 00:10:00.900]   each of them have their own way of handling the model and specifically the gradients from
[00:10:00.900 --> 00:10:06.180]   those model outputs. And then lastly, the optimizer and the scheduler were converted
[00:10:06.180 --> 00:10:12.540]   to what we call accelerated optimizers and accelerated schedulers. The TLDR version
[00:10:12.540 --> 00:10:18.140]   of this is stepping with the optimizer and the scheduler are a little bit different depending
[00:10:18.140 --> 00:10:24.860]   on how your distribution is distributed, computed, set up. And so, we handle all of that for
[00:10:24.860 --> 00:10:31.100]   you. Now, let's go a bit more into what I was mentioning earlier, the extra benefits
[00:10:31.100 --> 00:10:35.820]   that Accelerate can provide. Because first I showed, you know, it can help you centralize
[00:10:35.820 --> 00:10:41.540]   your code. But what about all of the other things you might want to do that could require
[00:10:41.540 --> 00:10:46.660]   some different setups? You'll notice a trend here where Accelerate has features specifically
[00:10:46.660 --> 00:10:52.300]   for that. The first one is what we call automatic mixed precision. When you create the accelerator
[00:10:52.300 --> 00:10:58.740]   object, you can specify a mixed precision type such as brain float 16 or floating point
[00:10:58.740 --> 00:11:05.860]   16. And what Accelerate will do is when we call backward, it will automatically calculate
[00:11:05.860 --> 00:11:13.420]   the loss with respect to mixed precision. On top of this, if we're doing mixed precision
[00:11:13.420 --> 00:11:18.380]   as well, when you prepare the model, it will also convert the model outputs to mixed precision
[00:11:18.380 --> 00:11:25.740]   as well. And have the model be able to handle it. But wrap it so that the model outputs
[00:11:25.740 --> 00:11:29.980]   full precision. It's a bit weird, I know. But basically, Accelerate will handle full
[00:11:29.980 --> 00:11:35.240]   precision for you efficiently without you having to actually worry about any of these
[00:11:35.240 --> 00:11:41.280]   little details I just mentioned. Next is gradient accumulation. This is something that was recently
[00:11:41.280 --> 00:11:48.740]   added in the last few months. Gradient accumulation, for those who don't know, is basically simulating
[00:11:48.740 --> 00:11:55.260]   larger batch sizes by training over multiple smaller batches. And then combining those
[00:11:55.260 --> 00:12:01.060]   gradients and averaging them to say rather than doing a batch of eight, I did eight batches
[00:12:01.060 --> 00:12:11.060]   of eight. So, I did a step with 64 items. It lets you train a bit more stable when you're
[00:12:11.060 --> 00:12:16.200]   handling these smaller and smaller batch sizes. And one thing we found is when it comes to
[00:12:16.200 --> 00:12:22.380]   distributed compute and doing this, gradient accumulation has a lot of steps involved.
[00:12:22.380 --> 00:12:28.700]   And so, we just decided that Accelerator will just have an accumulate function. You pass
[00:12:28.700 --> 00:12:34.980]   the model in, specify the gradient accumulation steps back in the Accelerator initialization.
[00:12:34.980 --> 00:12:39.860]   And Accelerator will handle all of this for you and automatically do gradient accumulation.
[00:12:39.860 --> 00:12:47.860]   But in a way that's not necessarily too hit. What do I mean by this? The gradient accumulation
[00:12:47.860 --> 00:12:54.260]   layer specifically has a lower level to it. Because Accelerator is designed to be a lower
[00:12:54.260 --> 00:13:00.340]   level library. This isn't going to be a high level framework like say Trainer in Transformers
[00:13:00.340 --> 00:13:06.100]   or the Lerner in Fast AI where you wrap around everything and it's a black box. Accelerator
[00:13:06.100 --> 00:13:11.020]   is designed to be the lowest part of that where we expose everything you can do without
[00:13:11.020 --> 00:13:16.340]   changing your source code as much as possible. So, that gradient accumulation wrapper I just
[00:13:16.340 --> 00:13:22.740]   showed does this. Which is basically checking if the current batch number is divisible by
[00:13:22.740 --> 00:13:28.800]   4 and we should step. Or if we're on the last batch. And if we're not, then there's this
[00:13:28.800 --> 00:13:35.660]   Accelerator no sync that gets called. This is a method that really comes from distributed
[00:13:35.660 --> 00:13:41.700]   data parallelism called no sync. So, Accelerator isn't actually doing anything. It is the model
[00:13:41.700 --> 00:13:48.260]   doing something and we're just at PyTorch code again. We use the PyTorch no sync wrapper,
[00:13:48.260 --> 00:13:52.620]   call backwards and call backwards when the gradients finally sync. That's what it's truly
[00:13:52.620 --> 00:13:58.820]   wrapping around. Which is one of the things I enjoy about Accelerator specifically. It's
[00:13:58.820 --> 00:14:06.580]   never too much magic. And every piece of thing you might call magic can be found and explained
[00:14:06.580 --> 00:14:11.460]   very, very quickly by either reading the documentation or source code.
[00:14:11.460 --> 00:14:18.740]   Now, let's talk about that third part of Accelerator. That third sub library. Big model inference.
[00:14:18.740 --> 00:14:26.780]   Or in other words, how you're able to run stable diffusion on your home GPU. So, a trend
[00:14:26.780 --> 00:14:32.100]   that was starting to come out is more and more big models were being released. And as
[00:14:32.100 --> 00:14:37.760]   a result, the average user just had no way to run these big models on their local system.
[00:14:37.760 --> 00:14:44.420]   They had to rely on paying for big compute costs. Not really using it as much. And as
[00:14:44.420 --> 00:14:52.100]   a result, it was kind of left to the experts can use it and maybe in production big companies.
[00:14:52.100 --> 00:14:56.860]   But Hugging Face is obviously very big on decentralization of machine learning and deep
[00:14:56.860 --> 00:15:03.740]   learning. So, the question became, how can we get these models to train and not train,
[00:15:03.740 --> 00:15:10.740]   but do inference on smaller and smaller compute? And born out of this by Sylvain Gugger was
[00:15:10.740 --> 00:15:17.900]   the big model inference framework inside Accelerator. Let's talk about the basic idea here. So,
[00:15:17.900 --> 00:15:24.500]   in PyTorch, there now exists something called a meta device. The way to think about that
[00:15:24.500 --> 00:15:30.740]   is, it's a way to load the model into memory without loading the weights in. So, as a result,
[00:15:30.740 --> 00:15:37.940]   it's using a small amount of memory at first. And then as an input goes through each layer
[00:15:37.940 --> 00:15:45.180]   of the model, you can take the weights of that layer and load it in from, say, the CPU
[00:15:45.180 --> 00:15:50.940]   onto the GPU quickly. The input goes through and then that layer gets unloaded immediately
[00:15:50.940 --> 00:15:59.180]   afterwards. So, as a result, the largest memory comes from how big your biggest layer is.
[00:15:59.180 --> 00:16:05.020]   So, as a result, stable diffusion version 1 could be ran on less than a gigabyte of
[00:16:05.020 --> 00:16:12.800]   VRAM versus the multitudes of gigs it normally would require. So, now let's look at what
[00:16:12.800 --> 00:16:17.620]   that looks like. How does it look like in the code? So, usually you would start off
[00:16:17.620 --> 00:16:22.620]   with a situation like I presented before you, where we have a model class, we load in the
[00:16:22.620 --> 00:16:28.620]   checkpoint file we want to use, and then we load the state dictionary into the model itself
[00:16:28.620 --> 00:16:34.620]   and update those weights. There's two issues with that, though. The full version of the
[00:16:34.620 --> 00:16:39.340]   model is being loaded twice. First, when we initialize that class, and then again when
[00:16:39.340 --> 00:16:45.020]   we bring in the checkpoint file. For an example of how bad that can go, if, say, we had a
[00:16:45.020 --> 00:16:50.740]   6 billion parameter model, then magically you're going to need about 48 gigabytes of
[00:16:50.740 --> 00:16:57.060]   video card memory in order to bring that model in. And the average user, let alone the average
[00:16:57.060 --> 00:17:05.140]   gamer, does not have 48 gigabytes of GPU RAM on them at any given time. So, how do we fix
[00:17:05.140 --> 00:17:11.540]   this? Well, first, we load in that empty model skeleton. Now, with Accelerate, this looks
[00:17:11.540 --> 00:17:17.940]   like a function called init empty weights. It's a context manager where essentially any
[00:17:17.940 --> 00:17:25.400]   PyTorch model initialized under it will have all of their parameters and layers be pushed
[00:17:25.400 --> 00:17:32.100]   on this meta device that I mentioned earlier. Now, in its current state, though, don't try
[00:17:32.100 --> 00:17:39.060]   and run it and get an inference off of it, because not all tensor operations are currently
[00:17:39.060 --> 00:17:46.380]   supported on the meta device. And so, as a result, we basically have this data frame
[00:17:46.380 --> 00:17:52.660]   of sorts, this empty PyTorch model that we can't do anything with. But it exists, and
[00:17:52.660 --> 00:18:00.020]   we can load weights into it. So, now let's talk about sort of a pre-processing step of
[00:18:00.020 --> 00:18:08.280]   sorts in order to do this layer by layer unload and load efficiently. There's something called
[00:18:08.280 --> 00:18:16.180]   a sharded checkpoint. So, if, say, you're doing, like, torch.save my model, that wouldn't
[00:18:16.180 --> 00:18:23.260]   be able to really work well with Accelerate. Accelerate requires that we save smaller chunks
[00:18:23.260 --> 00:18:29.700]   of the model weights into multiple different files that we can load at one time. That way,
[00:18:29.700 --> 00:18:35.300]   it takes up less, say, CPU memory for us to then be able to move these weights back and
[00:18:35.300 --> 00:18:41.380]   forth. So, it's not like saying save a quarter of the model on a particular file, and then
[00:18:41.380 --> 00:18:46.620]   that entire quarter of the model is brought into GPU memory, it is brought into CPU memory
[00:18:46.620 --> 00:18:52.820]   or your RAM. And so, the smaller we can get that, the more accessible it can be to smaller
[00:18:52.820 --> 00:19:02.160]   and smaller machines. And so, what does that actually look like in the code? So, the second
[00:19:02.160 --> 00:19:09.460]   function is something called load checkpoint and dispatch. As it sounds like, we take the
[00:19:09.460 --> 00:19:15.860]   PyTorch model that we just initialized with empty weights, you pass in the folder that
[00:19:15.860 --> 00:19:23.580]   has your weights that are split up, and you pass in what is called a device map. Now,
[00:19:23.580 --> 00:19:30.500]   the automatic device map, which is the default, will tell Accelerate that it needs to find,
[00:19:30.500 --> 00:19:40.580]   based on your model's layers, where it can best allocate storage of the layers in memory.
[00:19:40.580 --> 00:19:45.920]   That's not GPU memory that is sitting there without passing any inputs in. All of these
[00:19:45.920 --> 00:19:51.880]   layers will then have hooks onto each of them. And that's what when moving inputs in and
[00:19:51.880 --> 00:19:59.140]   out of the layers, will actually move them to the proper devices. But the basic order
[00:19:59.140 --> 00:20:05.120]   of it is first, it will prioritize utilizing your GPU as much as possible. Then it will
[00:20:05.120 --> 00:20:13.420]   utilize your CPU space before finally utilizing your disk space. And the reason for this is,
[00:20:13.420 --> 00:20:17.600]   this is all in terms of speed. So, one is obviously the fastest, because everything
[00:20:17.600 --> 00:20:22.580]   is on the GPU at that point, if it all fits. If it all doesn't fit, then it's on the CPU,
[00:20:22.580 --> 00:20:27.680]   and moving from the CPU to the GPU is kind of fast. It's got some slowdown to it, but
[00:20:27.680 --> 00:20:32.760]   it's still pretty okay. And then that third part is the slowest of them all, because it
[00:20:32.760 --> 00:20:39.360]   has to read it from your computer, right from your hard drive, onto CPU, and then move that
[00:20:39.360 --> 00:20:44.260]   onto GPU. So, rather than one step, there's two. So, generally, you prefer that if you
[00:20:44.260 --> 00:20:50.260]   can keep it between steps one and two, you're better off from a speed perspective and efficiency
[00:20:50.260 --> 00:20:56.820]   perspective. Now, all of that put together in a full workflow would look something like
[00:20:56.820 --> 00:21:02.900]   so. So, you initialize the empty weights in the model, load the checkpoint and dispatch,
[00:21:02.900 --> 00:21:07.580]   convert the model to its eval form, and then do some inference. Because, again, this is
[00:21:07.580 --> 00:21:13.420]   big model inference. Training is not yet there, due to a lot of various factors. But for right
[00:21:13.420 --> 00:21:19.460]   now, this is inference only. Now, that was a lot of code. Is there a better way to do
[00:21:19.460 --> 00:21:25.780]   this? Yes. The Transformers library, combined with the Hugging Face Hub, makes this much
[00:21:25.780 --> 00:21:33.060]   easier for you to do by wrapping it in the pipeline class. So, this is an example of
[00:21:33.060 --> 00:21:39.820]   ten lines of code, which really are four, saying specify a task, specify a model. Here,
[00:21:39.820 --> 00:21:45.340]   we can then also specify that device map parameter I just showed a moment ago. And then to save
[00:21:45.340 --> 00:21:54.060]   on even more memory, we can also tell it how we want the torch layers and the data type
[00:21:54.060 --> 00:22:02.140]   to be done. So, this is like loading things in half precision versus full precision. And
[00:22:02.140 --> 00:22:10.740]   so, then you can just use this very quickly to perform your text generation. What about
[00:22:10.740 --> 00:22:16.740]   stable diffusion, though? I mentioned this, right? How do we translate this into doing
[00:22:16.740 --> 00:22:21.780]   these awesome things with stable diffusion on low resource? And next, we have a demo
[00:22:21.780 --> 00:22:27.700]   with doing exactly that with diffusers and WaitTombioses. I'll hand it off to Patrick
[00:22:27.700 --> 00:22:30.980]   now as he can sort of show you what that's going to be like.
[00:22:30.980 --> 00:22:41.180]   >> Hi, everybody. I'm super excited to speak here today. To present myself real quick,
[00:22:41.180 --> 00:22:46.140]   I'm Patrick. I'm one of the core maintainers of the Diffusers library. And I just want
[00:22:46.140 --> 00:22:54.500]   to give you a short hands-on demo now for stable diffusion in Accelerate and also stable
[00:22:54.500 --> 00:23:01.940]   diffusion in WaitTombioses. So, I think Zach gave you a very nice overview now of how Accelerate
[00:23:01.940 --> 00:23:07.300]   can be used to make your code more efficient. And I just want to show you now how this applies
[00:23:07.300 --> 00:23:13.340]   to a real-world case. So, here's just a quick demo where I'm going to run stable diffusion
[00:23:13.340 --> 00:23:21.860]   now on the Google Colab. You can see I'm using GPU. Let's see what I got. I got a T4. Okay,
[00:23:21.860 --> 00:23:28.300]   cool. Then you can install diffusers here with pip install very easily. Diffusers also
[00:23:28.300 --> 00:23:33.100]   have a reliable load on other libraries. So, transformers is necessary for stable diffusion
[00:23:33.100 --> 00:23:38.900]   because of the text encoder, which is a clip. And then we also have some other packages
[00:23:38.900 --> 00:23:44.860]   we like for faster schedulers, like SciPy. And we now also more or less actually have
[00:23:44.860 --> 00:23:52.180]   Accelerate as a required dependency for diffusers simply because of the more memory efficient
[00:23:52.180 --> 00:23:56.300]   model loading, which I'm going to show you now. Cool. I think they're already installed
[00:23:56.300 --> 00:24:05.220]   here in this notebook. And I'm going to use the stable diffusion V1.5 here. You can actually
[00:24:05.220 --> 00:24:12.900]   also check it out here. It's a pretty popular model by now. It has over a million downloads
[00:24:12.900 --> 00:24:18.580]   last month and lots of demos are built on it. All right, cool. So, now what I'm going
[00:24:18.580 --> 00:24:24.660]   to show you is just the difference between not using Accelerate for loading the whole
[00:24:24.660 --> 00:24:32.700]   pipeline and using Accelerate. So, in the first cell here, I'm just going to run it
[00:24:32.700 --> 00:24:39.300]   without using Accelerate. What I do is I pass the model ID, then I tell the model to use
[00:24:39.300 --> 00:24:47.700]   the Pro16 Dtype. I don't need a safety check for the demo now. And as you can see, I pass
[00:24:47.700 --> 00:24:52.020]   low CPU memory usage equals to false. The reason why I do this is because we actually
[00:24:52.020 --> 00:24:57.500]   use Accelerate now always by default, which means if you don't pass that, you already
[00:24:57.500 --> 00:25:03.980]   use the fast way of loading the model with Accelerate. So, now Accelerate is disabled.
[00:25:03.980 --> 00:25:08.060]   Just for demo purpose, I disabled here with false. You can see it takes 17 seconds to
[00:25:08.060 --> 00:25:13.020]   load it. And the reason now, I think it's exactly about memory. So, the memory benefits
[00:25:13.020 --> 00:25:18.660]   actually I cannot show you here so nicely now because it's a Google app and I just wanted
[00:25:18.660 --> 00:25:24.140]   to focus a bit on the speed part. So, what happens now, it takes 17 seconds just to load
[00:25:24.140 --> 00:25:28.940]   the model and that can be annoying if you switch out between a lot of different fine-tuned
[00:25:28.940 --> 00:25:34.060]   checkpoints. And the reason is actually to a big part because what Zach talked about
[00:25:34.060 --> 00:25:40.900]   before is if you don't use a Torch meta device, what you do is you initialize a model and
[00:25:40.900 --> 00:25:45.700]   this model is going to allocate memory for its weights because you're going to randomly
[00:25:45.700 --> 00:25:50.580]   initialize weights. So, this memory allocation on one end takes time. And the second important
[00:25:50.580 --> 00:25:56.580]   aspect is also when you do this in PyTorch, you also run only like an initialization.
[00:25:56.580 --> 00:26:02.700]   And this algorithm, you know, like adding a Gaussian random weight takes quite some
[00:26:02.700 --> 00:26:08.780]   time. So, now if I just use the default case here, you can see just the loading time should
[00:26:08.780 --> 00:26:15.380]   drop pretty significantly. So, I think we had with 17 seconds and now I hope it's going
[00:26:15.380 --> 00:26:22.300]   to be less. Yeah, you can see 7 seconds, right? So, this makes quite a bit of difference because
[00:26:22.300 --> 00:26:28.180]   if you switch out models in a web UI, you want it to be fast. That's also why we just
[00:26:28.180 --> 00:26:32.940]   have it enabled by default now. If you don't have Accelerate installed and you use Diffusers,
[00:26:32.940 --> 00:26:36.820]   you actually get a warning now because it's pretty important for us though.
[00:26:36.820 --> 00:26:42.060]   Cool. Now, let's see what DeviceMapAuto does. In the next cell here, I actually just do
[00:26:42.060 --> 00:26:49.460]   the same thing, but I add DeviceAuto. And the difference is that the model will be automatically
[00:26:49.460 --> 00:26:55.180]   replaced on the GPU. So, you can see it takes a bit more time, but the nice thing now is
[00:26:55.180 --> 00:26:59.940]   already on GPU, right? Like the device here is cooler, it doesn't CPU anymore. And this
[00:26:59.940 --> 00:27:06.220]   can be extremely, extremely nice when you have multiple GPUs because then, Accelerate
[00:27:06.220 --> 00:27:13.060]   will automatically display the different components on the GPUs. Or if you, as Zach said before,
[00:27:13.060 --> 00:27:19.500]   if you don't have enough GPU memory, we'll actually use both CPU and GPU. So, quite often
[00:27:19.500 --> 00:27:23.180]   we always just use DeviceMapAuto because it's pretty convenient.
[00:27:23.180 --> 00:27:28.580]   The big design difference here is the pipeline is completely on GPU now, right? So, like
[00:27:28.580 --> 00:27:35.900]   you reload it and it's right away on GPU if you have a GPU. If you don't do it with DeviceMapAuto,
[00:27:35.900 --> 00:27:40.900]   it's on CPU, which is a bit more pytorchy by design, which is also why this is currently
[00:27:40.900 --> 00:27:44.860]   our default. And a nice add-on is adding DeviceMapAuto.
[00:27:44.860 --> 00:27:50.300]   Great. Okay, cool. Now, the second part of my hands-on demo is about how you can use
[00:27:50.300 --> 00:27:58.660]   diffuse and weights and biases to do some prompt engineering. So, what I want to show
[00:27:58.660 --> 00:28:06.500]   you here now is just how you can use, I guess, your subjective feeling for what images are
[00:28:06.500 --> 00:28:12.300]   good and what aren't good, and combine with weights and biases, nice visualization.
[00:28:12.300 --> 00:28:16.220]   So what we're going to do here now is, first of all, we could take a faster sample, which
[00:28:16.220 --> 00:28:20.460]   is then loaded into the pipeline quite easily here. And now what I want to do is I want
[00:28:20.460 --> 00:28:27.180]   to generate the flower garden blend, which is spelled incorrectly, "gaur-ben". And I
[00:28:27.180 --> 00:28:32.460]   want to add different prefixes and suffixes. I think you probably noticed that there's
[00:28:32.460 --> 00:28:37.100]   a lot of prompt hacking, prompt engineering, going on with Servedifusion. And it's actually
[00:28:37.100 --> 00:28:42.500]   not easy. And some prompts, they work much better just because you add something beautiful
[00:28:42.500 --> 00:28:48.860]   or hyper-realistic to it. And what we're going to do here now is just resample some prefixes
[00:28:48.860 --> 00:28:53.980]   and some suffixes. So, in general, these things like adding highly detailed or just a painting,
[00:28:53.980 --> 00:28:59.780]   they don't really change the content of the image that's being generated, but they will
[00:28:59.780 --> 00:29:04.860]   give it a nicer touch. So quite often, just adding something like high realistic can make
[00:29:04.860 --> 00:29:10.900]   your picture look nicer. Cool. All right. So weights and biases is already installed.
[00:29:10.900 --> 00:29:19.620]   I'm logged in. And now I'm just going to inlet a project with weights and biases. So cool.
[00:29:19.620 --> 00:29:24.180]   That's great for Cloud 3. I want to click on this afterwards. I just want to start my
[00:29:24.180 --> 00:29:30.740]   job now. So what I do here now is I just want to generate 25 different images. And the prompts
[00:29:30.740 --> 00:29:36.580]   are going to be randomly sampled because I randomly sampled a prefix, a suffix. I form
[00:29:36.580 --> 00:29:44.540]   it my base prompt to have those. And then I run my pipeline. It's also a pretty easy
[00:29:44.540 --> 00:29:50.500]   call here. It's just a pipe that we initialized before. Then you pass the prompt. And then
[00:29:50.500 --> 00:29:55.180]   because it's a better scheduler, we don't have to use 50 inference steps. We just use
[00:29:55.180 --> 00:30:02.500]   25. And then the image will be very nicely locked right away on the weights and biases
[00:30:02.500 --> 00:30:08.540]   page. And after it's done, so we're generating 25 images here, we also log a nice table for
[00:30:08.540 --> 00:30:14.660]   the overview. Cool. All right, let's start this. And now you can see it's generating
[00:30:14.660 --> 00:30:21.300]   images, right? 25 steps for every image. This is also going to run 25 times. I think while
[00:30:21.300 --> 00:30:26.620]   this is running, this should take, I don't know, this is 25 times per second. So it's
[00:30:26.620 --> 00:30:34.940]   a bit more than a minute. We can probably already jump into the page here. Cool. So
[00:30:34.940 --> 00:30:40.460]   what's very nice is you can see the images that have been generated right away here.
[00:30:40.460 --> 00:30:45.940]   So this is the zero image, the first image. And you can also click on it. So it looks
[00:30:45.940 --> 00:30:53.180]   actually not bad, pretty nice. And then you can just advance the slider here for the next
[00:30:53.180 --> 00:30:59.420]   step. And then I think if I refresh, we're also going to get some more. So now you can
[00:30:59.420 --> 00:31:05.060]   see I have some more steps here. So that's super nice, right? So in this example, I'm
[00:31:05.060 --> 00:31:12.860]   only kind of prompt engineering 25 images. But people are actually, I mean, they spend
[00:31:12.860 --> 00:31:17.300]   hours on doing this, right? Also, when you actually do things like you, when you create
[00:31:17.300 --> 00:31:21.980]   movies with Stability Fusion, you generate hundreds or even thousands of images. And
[00:31:21.980 --> 00:31:29.400]   then the prompt is super important. So people do this for quite some time. And then it's
[00:31:29.400 --> 00:31:36.620]   extremely nice to be able to visualize this here nicely right away on Weights and Biases.
[00:31:36.620 --> 00:31:40.260]   So there's an application for inference now. Another one is obviously training, right?
[00:31:40.260 --> 00:31:48.020]   When you train, it's pretty important to be able to subjectively kind of state when the
[00:31:48.020 --> 00:31:54.140]   model is better. Because the metrics we use for image generation, the FID metric, for
[00:31:54.140 --> 00:31:58.900]   example, is not always the best metric when it comes to how good that image looks. The
[00:31:58.900 --> 00:32:03.820]   best metric you could often use is pre-filing maybe like a hundred different prompts, and
[00:32:03.820 --> 00:32:08.780]   then logging them and edit every epoch. And for this, you can also obviously use Weights
[00:32:08.780 --> 00:32:16.260]   and Biases. All right, I think I should be almost done here. And then I can also check
[00:32:16.260 --> 00:32:26.140]   out the table afterwards. All right, cool. This is finished. I think as soon as it's
[00:32:26.140 --> 00:32:34.700]   finished, I also have a nice table on Weights and Biases. I just refresh that. Cool. Okay.
[00:32:34.700 --> 00:32:39.020]   And now I have a very nice table here. That's also cool, right? Because here I can now see
[00:32:39.020 --> 00:32:48.880]   my prompt that I assembled. So we have the epic visual painting and yeah, another one.
[00:32:48.880 --> 00:32:51.900]   So then you can also just compare them quite nicely right away. Like, well, what looks
[00:32:51.900 --> 00:32:58.260]   better? If a certain suffix or prefix just always looks better, then you could see this
[00:32:58.260 --> 00:33:06.180]   quite nicely here, by the way. Actually, I think they all look, the cinematics were interesting.
[00:33:06.180 --> 00:33:12.460]   Yeah, so you can just kind of check that out here very nicely. So it's a very smooth integration
[00:33:12.460 --> 00:33:18.260]   between Diffusers and Weights and Biases. Yeah. Yeah, I think that's it from my part.
[00:33:18.260 --> 00:33:25.900]   I'm just going to stop sharing my screen. Cool. And then I think we have Anish as the
[00:33:25.900 --> 00:33:33.460]   last speaker now, and I can hand it over.
[00:33:33.460 --> 00:33:37.860]   Thanks for that great talk, Patrick. So yeah, let me show you right now, actually, as a
[00:33:37.860 --> 00:33:44.500]   perfect allude to how well Accelerate Diffusers and Weights and Biases work together. So as
[00:33:44.500 --> 00:33:49.380]   we saw on that dashboard earlier, and then from the code itself, the code is very minimal
[00:33:49.380 --> 00:33:54.220]   to actually work with Weights and Biases and Accelerate slash Diffusers. In fact, with
[00:33:54.220 --> 00:34:01.140]   Accelerate, to log all your training information, it all it takes is one simple argument of
[00:34:01.140 --> 00:34:07.940]   log width equals Weights and Biases. And so in this, by simply adding in this one line,
[00:34:07.940 --> 00:34:12.500]   all of your metrics during training, so I'll be talking about training right now, during
[00:34:12.500 --> 00:34:18.240]   training will flow in to your dashboard, and you do no other extra work other than log
[00:34:18.240 --> 00:34:23.820]   in. And so that means you get things such as your metrics, as seen in the charts below,
[00:34:23.820 --> 00:34:28.580]   and you log other things such as a table with your images and some other information about
[00:34:28.580 --> 00:34:33.700]   those images, and the image itself. And also, this is not shown here, also things such as
[00:34:33.700 --> 00:34:41.380]   model checkpoints. And so with one line, you can do all the metric tracking automatically.
[00:34:41.380 --> 00:34:46.020]   And nicely, Accelerate actually provides a really straightforward interface to build
[00:34:46.020 --> 00:34:51.180]   out extended functionality for a lot of our trackers. So built in is the Weights and Biases
[00:34:51.180 --> 00:34:56.500]   tracker. But if you want to, theoretically, you could add in a lot of complex functionality
[00:34:56.500 --> 00:35:01.260]   if you wanted to log very other interesting bits of information that doesn't naturally
[00:35:01.260 --> 00:35:07.100]   come as a process of training across the board. So in this case, if we want to log images,
[00:35:07.100 --> 00:35:12.500]   log our model, or finalize all the information that we may be storing during our training
[00:35:12.500 --> 00:35:17.180]   and evaluation, we just build that into the tracker itself. And right here, we can see
[00:35:17.180 --> 00:35:22.020]   that just inherits the ability to log all this information directly to Weights and Biases.
[00:35:22.020 --> 00:35:26.740]   So that's very convenient and easy. And so the only change you'd have to make back to
[00:35:26.740 --> 00:35:31.020]   your original training code is just changing what you're actually logging with. And so
[00:35:31.020 --> 00:35:36.260]   what benefit do we actually get from this? Well, the benefit we get is grabbing that
[00:35:36.260 --> 00:35:41.780]   functionality is extremely straightforward and almost feels natural as part of your process.
[00:35:41.780 --> 00:35:48.140]   You don't want to write extra function to write all the information and metrics or images
[00:35:48.140 --> 00:35:53.900]   that are output from your model during training to be another thing that you worry and manage.
[00:35:53.900 --> 00:35:58.740]   Instead, you just build that into the tracking process and have that be functionality that
[00:35:58.740 --> 00:36:04.540]   is already part of the trainer that Accelerate is providing. And so with Accelerate, you're
[00:36:04.540 --> 00:36:09.460]   getting this extended functionality to go ahead and distribute your work extremely efficiently
[00:36:09.460 --> 00:36:14.460]   across your resources and with the data that's provided. At the same time, all the information
[00:36:14.460 --> 00:36:20.480]   about your model is being stored efficiently and in a way that, like I mentioned, works
[00:36:20.480 --> 00:36:24.940]   well with the process that you're already used to. And so what are some things that
[00:36:24.940 --> 00:36:30.780]   Weights and Biases provides that's useful for our model training process? Well, so we
[00:36:30.780 --> 00:36:35.280]   offer a suite of tools. And so one of the integrations we offer and we love to maintain
[00:36:35.280 --> 00:36:40.860]   is the Hugging Face one. And so just to give a brief overview, the three ones that I'll
[00:36:40.860 --> 00:36:48.100]   talk about today are experiments, artifacts, and tables. And so in general, as we mentioned
[00:36:48.100 --> 00:36:56.860]   experiment tracking, when doing your modeling, we're collecting metrics from our model to
[00:36:56.860 --> 00:37:00.780]   help us understand, one, how well the model is doing, and two, understand things about
[00:37:00.780 --> 00:37:05.940]   the performance of the model in terms of not only just how well it's doing from the data
[00:37:05.940 --> 00:37:12.140]   itself, but also resource utilization and such. And so having all this information stored
[00:37:12.140 --> 00:37:16.940]   inside of a CSV becomes really intractable as you scale out the amount of experiments
[00:37:16.940 --> 00:37:21.660]   that you have and the amount of people on your team. And so Accelerate provides a really
[00:37:21.660 --> 00:37:29.500]   nice interface to scale out your training workflow directly via code. And Weights and
[00:37:29.500 --> 00:37:33.300]   Biases lets you organize these things a lot more organizationally, too. So you get this
[00:37:33.300 --> 00:37:40.420]   nice scale on both ends. And so that's what this shows, that all of your experiments from
[00:37:40.420 --> 00:37:45.980]   Accelerator can all be deposited into one centralized resource that can easily be managed
[00:37:45.980 --> 00:37:52.180]   between you, yourself, and also any team members if you so choose to. And similarly, like for
[00:37:52.180 --> 00:37:57.340]   instance, in that example that Patrick showed, every run would theoretically have a nice
[00:37:57.340 --> 00:38:01.400]   collection of images in that table. But what if you wanted to try out prompt engineering
[00:38:01.400 --> 00:38:08.060]   every day for a year, maybe four times a day for a year? That run, that one run, quickly
[00:38:08.060 --> 00:38:14.700]   becomes thousands of runs. And so being able to manage all the outputs of your stable diffusion
[00:38:14.700 --> 00:38:20.900]   into one centralized location and have ability to go through and filter out using interesting
[00:38:20.900 --> 00:38:28.340]   metadata is extremely essential for organizational aspects, especially in the deep learning world.
[00:38:28.340 --> 00:38:32.780]   And so what this looks like is three really simple steps. You start with the biases, and
[00:38:32.780 --> 00:38:36.380]   you log the things that you want. And sorry, not three simple steps, two simple steps.
[00:38:36.380 --> 00:38:41.140]   Start the experiment and log what other metrics that you want. And so like I mentioned, it
[00:38:41.140 --> 00:38:46.900]   helps speed up the development cycle and helps you collaborate across your team, or your
[00:38:46.900 --> 00:38:54.620]   projects itself. Secondly, we use artifacts. So the best way to think about it is model
[00:38:54.620 --> 00:39:00.260]   checkpoints or dataset checkpoint versions, for instance. So in this case, we are able
[00:39:00.260 --> 00:39:05.660]   to easily save our trained model and then store it into a location for ourselves to
[00:39:05.660 --> 00:39:12.940]   retrieve in such that we can go through and be able to see the output of all of our different
[00:39:12.940 --> 00:39:17.500]   model training runs in one centralized location also, and make decisions about whether or
[00:39:17.500 --> 00:39:23.980]   not we want to use this model for any tooling that we may make down the line. So in essence,
[00:39:23.980 --> 00:39:27.940]   this is a nice visual graphic of that. You have data that may go into a model, and it
[00:39:27.940 --> 00:39:34.060]   spits out something. You want to make sure you track all that information.
[00:39:34.060 --> 00:39:39.820]   And lastly, tables. Patrick also showed this in the previous workspace. But tables is our
[00:39:39.820 --> 00:39:45.260]   way to query large tabular datasets and use it to also do interesting analysis of rich
[00:39:45.260 --> 00:39:51.380]   media. So to show-- I guess, even show it directly, here's an example of that. So let's
[00:39:51.380 --> 00:39:56.300]   say in the previous one, where we're trying out different suffixes and prefixes, I want
[00:39:56.300 --> 00:40:05.620]   to say, I want to grab only the ones that were octane renders. So I'm going to say,
[00:40:05.620 --> 00:40:10.900]   grab only prompts with octane. And now here, we can start to analyze our prompts directly
[00:40:10.900 --> 00:40:15.940]   using this table. So we can go in and say, OK, here are all of our octane renders. And
[00:40:15.940 --> 00:40:21.620]   so we can go in and actually look at it. If we were to log other function, other fields,
[00:40:21.620 --> 00:40:27.700]   we can also do complex behaviors, such as group bys and whatnot. So very, very strong
[00:40:27.700 --> 00:40:34.520]   table manipulations. And also can be utilized to also organize insights in ways that make
[00:40:34.520 --> 00:40:45.580]   sense as a table. Sorry. Go back to here. And so an example of a training run, though.
[00:40:45.580 --> 00:40:50.260]   So what Patrick showed earlier was how easy it is to utilize diffusers and weights and
[00:40:50.260 --> 00:40:55.660]   biases together for inference. I want to quickly show you how easy it is to also do from training.
[00:40:55.660 --> 00:41:02.300]   And so to go back to what I showed earlier, it's as simple as if I want to use weights
[00:41:02.300 --> 00:41:08.300]   and biases, I throw in the WANDB flag. If I want to extend out the functionality, I
[00:41:08.300 --> 00:41:14.300]   can write some code that will inherit the functionality of the original WANDB tracker
[00:41:14.300 --> 00:41:19.660]   and just pass that in itself. I re-mention this just so when I show you the code, it's
[00:41:19.660 --> 00:41:24.020]   not overwhelming. But as you see here, you pass in this extended tracker. And now I can
[00:41:24.020 --> 00:41:29.940]   log images and log the model. So one example I'm going to be using is this unconditional
[00:41:29.940 --> 00:41:36.940]   image generation, which utilizes both aspects of diffusers and accelerate. Would recommend
[00:41:36.940 --> 00:41:42.220]   looking at the examples section under diffusers. Has a lot of great examples showing both.
[00:41:42.220 --> 00:41:47.060]   But for this one specifically, to just give a head nod to what I mentioned earlier, we
[00:41:47.060 --> 00:41:54.340]   build out extended tracker to do a little bit more extra functionality. And to go ahead
[00:41:54.340 --> 00:41:58.740]   and actually use weights and biases with your model, so a lot of code to essentially say,
[00:41:58.740 --> 00:42:04.220]   hey, we have a model that we're trying to train. So let's make sure we track all the
[00:42:04.220 --> 00:42:14.900]   information of our model properly inside of our tracker. And during training, to accelerate--
[00:42:14.900 --> 00:42:26.660]   sorry, wrong button. You can see that we log interesting metrics to whatever tracker we're
[00:42:26.660 --> 00:42:31.780]   using. So in this case, weights and biases. And during sections of our training, like
[00:42:31.780 --> 00:42:37.540]   over here, where we decide to do some evaluation, we can go ahead and grab the ability to log
[00:42:37.540 --> 00:42:44.540]   images and use that right away. Sorry, I realized how small the screen is. But as I mentioned
[00:42:44.540 --> 00:42:51.780]   earlier, all that's built into a workflow that is no different from what you would already
[00:42:51.780 --> 00:42:59.340]   be using with Accelerator. And so with this code, we're able to create a separate dashboard
[00:42:59.340 --> 00:43:04.020]   where from here, you automatically get all these metrics logged. But with our extended
[00:43:04.020 --> 00:43:09.980]   functionality, we also get to generate these tables that I showed earlier. But in this
[00:43:09.980 --> 00:43:18.740]   case, how each of these rows are a different degree of how many training steps was done.
[00:43:18.740 --> 00:43:26.940]   So the best example is initially, the images looked something like this. And by the end,
[00:43:26.940 --> 00:43:32.500]   they look like this. Well, it doesn't look as good zoomed out-- zoomed into the resolution.
[00:43:32.500 --> 00:43:39.240]   But they look a lot more Pokemon-like. And so the example I'm showing here is training
[00:43:39.240 --> 00:43:47.300]   unconditionally a unit 2D model to unconditionally generate Pokemon images based on noise past
[00:43:47.300 --> 00:43:52.380]   them. And so if you were to go to the dashboard for your workspace, you can see that just
[00:43:52.380 --> 00:43:57.940]   by adding in some extra functionality, like I mentioned with log images, where it's something
[00:43:57.940 --> 00:44:04.340]   as straightforward as log a generated image, which is a 1DB image, something that we have
[00:44:04.340 --> 00:44:10.500]   a first class data type for. And something as simple as logging a model is simply saying,
[00:44:10.500 --> 00:44:16.360]   log that folder that exists. And so having this general functionality allows us to go
[00:44:16.360 --> 00:44:22.460]   in and just add it as a one-liner directly to our training scripts so that when we do
[00:44:22.460 --> 00:44:29.220]   things such as evaluation, all of these images get stored in a way that makes sense in context
[00:44:29.220 --> 00:44:33.940]   for us for prompt engineering or training or whatnot. So in this case, we're trying
[00:44:33.940 --> 00:44:40.980]   to see how well our unconditional image training is going for all of the different-- for this
[00:44:40.980 --> 00:44:49.860]   original enter noise. And then we can see over time directly the evolution of this with
[00:44:49.860 --> 00:44:55.180]   the ability to also query row by row every 10 epochs what the generated images also look
[00:44:55.180 --> 00:45:05.620]   like and do that alongside the loss curves and understanding of what GPU-- what resources
[00:45:05.620 --> 00:45:14.860]   were utilized, code that's automatically logged that utilizes that specific concept, and every
[00:45:14.860 --> 00:45:23.580]   checkpoint that was generated during the training. So in this case, we generate checkpoints every--
[00:45:23.580 --> 00:45:31.900]   I think every 10 epochs also. So we can directly go in here and actually see the different
[00:45:31.900 --> 00:45:38.020]   trapped versions of our model where we have in here the scheduler, the unit, and the model
[00:45:38.020 --> 00:45:42.860]   index to be reloaded directly in as a checkpoint to accelerator.
[00:45:42.860 --> 00:45:49.260]   So accelerate and weights and biases work really well together. And diffusers and weights
[00:45:49.260 --> 00:45:54.380]   and biases work really well together. And so as you can see, not only the usefulness
[00:45:54.380 --> 00:45:58.700]   of accelerate for distributing and scaling out your workflows in terms of training and
[00:45:58.700 --> 00:46:04.660]   your research itself, but you also can scale out your experimentation workflow, your team
[00:46:04.660 --> 00:46:08.340]   workflows and your organizational workflows using weights and biases.
[00:46:08.340 --> 00:46:12.460]   Thanks for listening, everyone.
[00:46:12.460 --> 00:46:15.040]   (upbeat music)
[00:46:15.040 --> 00:46:17.620]   (upbeat music)

