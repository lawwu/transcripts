
[00:00:00.000 --> 00:00:03.440]   Okay, first of all, it's incredible to be here.
[00:00:03.440 --> 00:00:06.120]   I have a few notes, so I make sure we cover everything.
[00:00:06.120 --> 00:00:11.060]   I wanna take this opportunity to introduce Jonathan.
[00:00:11.060 --> 00:00:13.020]   Obviously, a lot of you guys have heard of his company,
[00:00:13.020 --> 00:00:15.560]   but you may not know his origin story,
[00:00:15.560 --> 00:00:16.760]   which is quite honestly,
[00:00:16.760 --> 00:00:20.120]   having been in Silicon Valley for 25 years,
[00:00:20.120 --> 00:00:23.520]   one of the most unique origin stories
[00:00:23.520 --> 00:00:25.620]   and founder stories you're going to hear.
[00:00:25.620 --> 00:00:28.960]   We're gonna talk about some of the things
[00:00:28.960 --> 00:00:31.200]   that he's accomplished both at Google and Grok.
[00:00:31.200 --> 00:00:33.540]   We're gonna compare Grok and NVIDIA,
[00:00:33.540 --> 00:00:36.440]   because I think it's probably one of the most important
[00:00:36.440 --> 00:00:38.680]   technical considerations that people should know.
[00:00:38.680 --> 00:00:40.480]   We'll talk about the software stack,
[00:00:40.480 --> 00:00:44.760]   and we'll leave with a few data points and bullets,
[00:00:44.760 --> 00:00:45.920]   which I think are pretty impressive.
[00:00:45.920 --> 00:00:50.920]   So, I wanna start by something that you do every week,
[00:00:50.920 --> 00:00:54.140]   which is you typically tweet out
[00:00:54.140 --> 00:00:56.040]   some sort of developer metric.
[00:00:56.040 --> 00:00:58.800]   Where are you as of this morning,
[00:00:58.800 --> 00:01:01.580]   and why are developers so important?
[00:01:01.580 --> 00:01:05.280]   - So, we're at, can you hear me?
[00:01:05.280 --> 00:01:08.520]   - Try this one.
[00:01:08.520 --> 00:01:10.320]   - Testing, ah, perfect.
[00:01:10.320 --> 00:01:14.240]   So, we are at 75,000 developers,
[00:01:14.240 --> 00:01:17.280]   and that is slightly over 30 days
[00:01:17.280 --> 00:01:20.020]   from launching our developer console.
[00:01:20.020 --> 00:01:24.300]   For comparison, it took NVIDIA seven years
[00:01:24.300 --> 00:01:27.220]   to get to 100,000 developers,
[00:01:27.220 --> 00:01:32.120]   and we're at 75,000 in about 30-ish days.
[00:01:32.120 --> 00:01:35.920]   So, the reason this matters is the developers, of course,
[00:01:35.920 --> 00:01:37.400]   are building all the applications,
[00:01:37.400 --> 00:01:40.160]   so every developer is a multiplicative effect
[00:01:40.160 --> 00:01:42.160]   on the total number of users that you can have,
[00:01:42.160 --> 00:01:43.980]   but it's all about developers.
[00:01:43.980 --> 00:01:47.880]   - Let's go all the way back, so just with that backdrop.
[00:01:47.880 --> 00:01:50.040]   This is not an overnight success story.
[00:01:50.040 --> 00:01:54.420]   This is eight years of plotting through the wild wilderness,
[00:01:55.400 --> 00:01:58.580]   punctuated, frankly, with a lot of misfires,
[00:01:58.580 --> 00:02:02.240]   which is really the sign of a great entrepreneur.
[00:02:02.240 --> 00:02:05.280]   But I want people to hear this story.
[00:02:05.280 --> 00:02:07.660]   Jonathan may be, you guys have all heard
[00:02:07.660 --> 00:02:10.220]   about entrepreneurs who have dropped out of college
[00:02:10.220 --> 00:02:12.500]   to start billion-dollar companies.
[00:02:12.500 --> 00:02:15.060]   Jonathan may be the only high school dropout
[00:02:15.060 --> 00:02:17.420]   to have also started a billion-dollar company.
[00:02:17.420 --> 00:02:20.060]   So, let's start with just two minutes.
[00:02:20.060 --> 00:02:22.480]   Just give us the background of you,
[00:02:22.480 --> 00:02:25.940]   because it was a very circuitous path
[00:02:25.940 --> 00:02:27.140]   to being an entrepreneur.
[00:02:27.140 --> 00:02:29.900]   - So, I dropped out of high school, as mentioned,
[00:02:29.900 --> 00:02:34.300]   and I ended up getting a job as a programmer,
[00:02:34.300 --> 00:02:37.700]   and my boss noticed that I was clever
[00:02:37.700 --> 00:02:41.480]   and told me that I should be taking classes at a university
[00:02:41.480 --> 00:02:42.940]   despite having dropped out of college.
[00:02:42.940 --> 00:02:46.540]   So, unmatriculated, I didn't actually get enrolled.
[00:02:46.540 --> 00:02:50.420]   I started going to Hunter College as a side thing,
[00:02:50.420 --> 00:02:53.220]   and then I sort of fell under the wing
[00:02:53.220 --> 00:02:56.980]   of one of the professors there, did well, transferred to NYU,
[00:02:56.980 --> 00:03:00.020]   and then I started taking PhD courses,
[00:03:00.020 --> 00:03:03.180]   but as an undergrad, and then I dropped out of that.
[00:03:03.180 --> 00:03:05.140]   - So, do you technically have a high school diploma?
[00:03:05.140 --> 00:03:05.980]   - No.
[00:03:05.980 --> 00:03:07.180]   - Okay, this is perfect.
[00:03:07.180 --> 00:03:09.740]   - Nor do I have an undergrad degree, but yeah.
[00:03:09.740 --> 00:03:12.420]   - So, from NYU, how did you end up at Google?
[00:03:12.420 --> 00:03:14.380]   - Well, actually, if it hadn't been for NYU,
[00:03:14.380 --> 00:03:15.940]   I don't think I would have ended up at Google,
[00:03:15.940 --> 00:03:18.360]   so this is interesting, even though I didn't have the degree.
[00:03:18.360 --> 00:03:20.800]   And I happened to go to an event at Google,
[00:03:20.800 --> 00:03:23.720]   and one of the people at Google recognized me
[00:03:23.720 --> 00:03:27.160]   because they also went to NYU, and then they referred me.
[00:03:27.160 --> 00:03:30.000]   So, you can make some great connections in university,
[00:03:30.000 --> 00:03:33.200]   even if you don't graduate, but it was one of the people
[00:03:33.200 --> 00:03:35.400]   that I was taking the PhD courses with.
[00:03:35.400 --> 00:03:36.960]   - And when you first went there,
[00:03:36.960 --> 00:03:39.840]   sort of what kind of stuff were you working on?
[00:03:39.840 --> 00:03:41.660]   - Ads, but testing.
[00:03:41.660 --> 00:03:44.160]   So, we were building giant test systems,
[00:03:44.160 --> 00:03:47.760]   and if you think that it's hard to build production systems,
[00:03:47.760 --> 00:03:49.920]   test systems have to test everything
[00:03:49.920 --> 00:03:52.360]   the production system does, we did it live.
[00:03:52.360 --> 00:03:57.360]   So, every single ads query, we would run 100 tests on that,
[00:03:57.360 --> 00:04:00.640]   but we didn't have the budget of the production system.
[00:04:00.640 --> 00:04:02.620]   So, we had to write our own threading library,
[00:04:02.620 --> 00:04:04.480]   we had to do all sorts of crazy stuff,
[00:04:04.480 --> 00:04:06.880]   which you don't think of in ads, but yeah,
[00:04:06.880 --> 00:04:08.520]   it was actually harder engineering
[00:04:08.520 --> 00:04:10.360]   than the production itself.
[00:04:10.360 --> 00:04:13.300]   - And so, Google's very famous for 20% time
[00:04:13.300 --> 00:04:15.480]   where you kind of can do whatever you want,
[00:04:15.480 --> 00:04:18.440]   and is that what led to the birth of TPU,
[00:04:18.440 --> 00:04:21.420]   which is now, I think, or what most of you guys know
[00:04:21.420 --> 00:04:23.960]   is Google's sort of leading custom silicon
[00:04:23.960 --> 00:04:25.360]   that they use internally.
[00:04:25.360 --> 00:04:29.440]   - So, 20% time is famous, I called it MCI time,
[00:04:29.440 --> 00:04:31.360]   which probably isn't gonna transfer as a joke here,
[00:04:31.360 --> 00:04:34.160]   but there was these advertisements for this phone company,
[00:04:34.160 --> 00:04:37.400]   free nights and weekends, so you could work on 20% time
[00:04:37.400 --> 00:04:40.140]   so long as it wasn't during your work time, yeah.
[00:04:40.140 --> 00:04:42.880]   But every single night, I would go up
[00:04:42.880 --> 00:04:45.680]   and work with the speech team.
[00:04:45.680 --> 00:04:47.820]   So, this was separate from my main project,
[00:04:47.820 --> 00:04:50.200]   and they bought me some hardware,
[00:04:50.200 --> 00:04:55.200]   and I started what was called the TPU as a side project,
[00:04:55.200 --> 00:04:59.640]   and it was funded out of what a VP referred to
[00:04:59.640 --> 00:05:02.280]   as his slush fund or leftover money,
[00:05:02.280 --> 00:05:03.640]   and it was never expected,
[00:05:03.640 --> 00:05:05.480]   there were actually two other projects
[00:05:05.480 --> 00:05:07.440]   to build AI accelerators,
[00:05:07.440 --> 00:05:09.440]   it was never expected to be successful,
[00:05:09.440 --> 00:05:11.900]   which gave us the cover that we needed
[00:05:11.900 --> 00:05:15.160]   to do some really counterintuitive and innovative things.
[00:05:15.160 --> 00:05:16.840]   Once that became successful,
[00:05:16.840 --> 00:05:19.080]   they brought in the adult supervision.
[00:05:19.080 --> 00:05:20.720]   - Okay, take a step back though,
[00:05:20.720 --> 00:05:23.200]   what problem were you trying to solve in AI
[00:05:23.200 --> 00:05:25.240]   when those words weren't even being used,
[00:05:25.240 --> 00:05:26.760]   and what was Google trying to do at the time
[00:05:26.760 --> 00:05:28.560]   where you saw an opportunity to build something?
[00:05:28.560 --> 00:05:30.580]   - So, this started in 2012,
[00:05:30.580 --> 00:05:32.520]   and at the time, there had never been
[00:05:32.520 --> 00:05:34.800]   a machine learning model that outperformed
[00:05:34.800 --> 00:05:36.360]   a human being on any task,
[00:05:36.360 --> 00:05:38.340]   and the speech team trained a model
[00:05:38.340 --> 00:05:41.400]   that transcribed speech better than human beings.
[00:05:41.400 --> 00:05:43.720]   The problem was they couldn't afford
[00:05:43.720 --> 00:05:45.320]   to put it into production,
[00:05:45.320 --> 00:05:48.360]   and so this led to a very famous engineer, Jeff Dean,
[00:05:48.360 --> 00:05:50.200]   giving a presentation to the leadership team,
[00:05:50.200 --> 00:05:51.680]   it was just two slides.
[00:05:51.680 --> 00:05:55.820]   The first slide was good news, machine learning works.
[00:05:55.820 --> 00:05:58.880]   The second slide, bad news, we can't afford it.
[00:05:58.880 --> 00:06:01.280]   So, they were gonna have to double or triple
[00:06:01.280 --> 00:06:04.760]   the entire global data center footprint of Google
[00:06:04.760 --> 00:06:07.120]   at an average of a billion dollars per data center,
[00:06:07.120 --> 00:06:10.300]   20 to 40 data centers, so 20 to 40 billion dollars,
[00:06:10.300 --> 00:06:12.180]   and that was just for speech recognition.
[00:06:12.180 --> 00:06:13.420]   If they wanted to do anything else,
[00:06:13.420 --> 00:06:16.300]   like search ads, it was gonna cost more.
[00:06:16.300 --> 00:06:17.900]   That was uneconomical,
[00:06:17.900 --> 00:06:19.620]   and that's been the history with inference.
[00:06:19.620 --> 00:06:21.660]   You train it, and then you can't afford
[00:06:21.660 --> 00:06:22.960]   to put it into production.
[00:06:22.960 --> 00:06:26.380]   - So, against that backdrop,
[00:06:26.380 --> 00:06:28.460]   what did you do that was so unique
[00:06:28.460 --> 00:06:31.820]   that allowed TPU to be one of the three projects
[00:06:31.820 --> 00:06:33.380]   that actually won?
[00:06:33.380 --> 00:06:36.260]   - The biggest thing was Jeff Dean noticed
[00:06:36.260 --> 00:06:39.480]   that the main algorithm that was consuming
[00:06:39.480 --> 00:06:42.860]   most of the CPU cycles at Google was matrix multiply,
[00:06:42.860 --> 00:06:45.740]   and we decided, okay, let's accelerate that,
[00:06:45.740 --> 00:06:47.980]   but let's build something around that,
[00:06:47.980 --> 00:06:52.360]   and so we built a massive matrix multiplication engine.
[00:06:52.360 --> 00:06:55.900]   When doing this, there were those two other competing teams.
[00:06:55.900 --> 00:06:58.500]   They took more traditional approaches to do the same thing.
[00:06:58.500 --> 00:07:00.600]   One of them was led by a Turing Award winner,
[00:07:00.600 --> 00:07:02.460]   and then what we did was we came up
[00:07:02.460 --> 00:07:04.860]   with what's called a systolic array,
[00:07:04.860 --> 00:07:07.480]   and I remember when that Turing Award winner
[00:07:07.480 --> 00:07:09.060]   was talking about the TPU, he said,
[00:07:09.060 --> 00:07:11.860]   "Whoever came up with this must have been really old
[00:07:11.860 --> 00:07:14.940]   "because systolic arrays have fallen out of favor,"
[00:07:14.940 --> 00:07:15.780]   and it was actually me.
[00:07:15.780 --> 00:07:17.820]   I just didn't know what a systolic array was.
[00:07:17.820 --> 00:07:20.020]   Someone had to explain to me what the terminology was.
[00:07:20.020 --> 00:07:21.620]   It was just kind of the obvious way to do it,
[00:07:21.620 --> 00:07:24.260]   and so the lesson is if you come at things
[00:07:24.260 --> 00:07:25.540]   knowing how to do them,
[00:07:25.540 --> 00:07:27.780]   you might know how to do them the wrong way.
[00:07:27.780 --> 00:07:30.300]   It's helpful to have people who don't know
[00:07:30.300 --> 00:07:32.140]   what should and should not be done.
[00:07:33.000 --> 00:07:35.800]   - So as TPU scales, there's probably a lot
[00:07:35.800 --> 00:07:38.180]   of internal recognition at Google.
[00:07:38.180 --> 00:07:41.660]   How do you walk away from that,
[00:07:41.660 --> 00:07:43.760]   and why did you walk away from that?
[00:07:43.760 --> 00:07:45.280]   - Well, all big companies end up
[00:07:45.280 --> 00:07:46.840]   becoming political in the end,
[00:07:46.840 --> 00:07:48.920]   and when you have something that successful,
[00:07:48.920 --> 00:07:50.200]   a lot of people want to own it,
[00:07:50.200 --> 00:07:51.800]   and there's always more senior people
[00:07:51.800 --> 00:07:54.040]   who start grabbing for it.
[00:07:54.040 --> 00:07:58.320]   I moved on to the Google X team, the rapid eval team,
[00:07:58.320 --> 00:07:59.280]   which is the team that comes up
[00:07:59.280 --> 00:08:01.700]   with all the crazy ideas at Google X,
[00:08:01.700 --> 00:08:04.320]   and I was having fun there,
[00:08:04.320 --> 00:08:07.300]   but nothing was turning into a production system.
[00:08:07.300 --> 00:08:10.280]   It was all a bunch of playing around,
[00:08:10.280 --> 00:08:12.580]   and I wanted to go and do something real again
[00:08:12.580 --> 00:08:13.420]   from start to finish.
[00:08:13.420 --> 00:08:16.480]   I wanted to take something from concept to production,
[00:08:16.480 --> 00:08:19.200]   and so I started looking outside,
[00:08:19.200 --> 00:08:21.000]   and that's when we met.
[00:08:21.000 --> 00:08:22.400]   - Well, that is when we met,
[00:08:22.400 --> 00:08:26.000]   but the thing is you had two ideas.
[00:08:26.000 --> 00:08:29.000]   One was more of let me build an image classifier,
[00:08:29.000 --> 00:08:31.300]   and you thought you could out-ResNet ResNet at the time,
[00:08:31.300 --> 00:08:33.360]   which was the best thing in town,
[00:08:33.360 --> 00:08:35.300]   and then you had this hardware path.
[00:08:35.300 --> 00:08:38.960]   - Well, actually, I had zero intention of building a chip.
[00:08:38.960 --> 00:08:42.960]   What happened was I had also built
[00:08:42.960 --> 00:08:47.960]   the highest performing image classifier,
[00:08:47.960 --> 00:08:51.560]   but I had noticed that all of the software
[00:08:51.560 --> 00:08:53.340]   was being given away for free.
[00:08:53.340 --> 00:08:55.080]   TensorFlow was being given away for free.
[00:08:55.080 --> 00:08:56.440]   The models were being given away for free.
[00:08:56.440 --> 00:08:59.880]   It was pretty clear that machine learning AI
[00:08:59.880 --> 00:09:01.560]   was gonna be open source, and it was gonna be--
[00:09:01.560 --> 00:09:02.960]   - Even back then. - Even back then.
[00:09:02.960 --> 00:09:06.000]   That was 2016, and so I just couldn't imagine
[00:09:06.000 --> 00:09:07.280]   building a business around that,
[00:09:07.280 --> 00:09:09.240]   and it would just be hard scrabble.
[00:09:09.240 --> 00:09:11.560]   Chips, it takes so long to build them
[00:09:11.560 --> 00:09:13.960]   that if you build something innovative and you launch it,
[00:09:13.960 --> 00:09:16.740]   it's gonna be four years before anyone can even copy it,
[00:09:16.740 --> 00:09:18.560]   let alone pull ahead of it,
[00:09:18.560 --> 00:09:21.040]   so that just felt like a much better approach,
[00:09:21.040 --> 00:09:21.920]   and it's atoms.
[00:09:21.920 --> 00:09:24.680]   You can monetize that more easily,
[00:09:24.680 --> 00:09:28.840]   so right around that time, the TPU paper came out.
[00:09:28.840 --> 00:09:30.160]   My name was in it.
[00:09:30.160 --> 00:09:32.080]   People started asking about it,
[00:09:32.080 --> 00:09:34.360]   and you asked me what I would do differently.
[00:09:34.360 --> 00:09:39.360]   - Well, I was investing in public markets
[00:09:39.360 --> 00:09:41.440]   as well at the time, a little dalliance
[00:09:41.440 --> 00:09:44.000]   in the public markets, and Sundar goes on
[00:09:44.000 --> 00:09:47.320]   in a press release and starts talking about TPU,
[00:09:47.320 --> 00:09:48.280]   and I was so shocked.
[00:09:48.280 --> 00:09:50.360]   I thought there is no conceivable world
[00:09:50.360 --> 00:09:52.560]   in which Google should be building their own hardware.
[00:09:52.560 --> 00:09:56.200]   They must know something that the rest of us don't know,
[00:09:56.200 --> 00:09:58.520]   and so we need to know that
[00:09:58.520 --> 00:10:00.200]   so that we can go and commercialize that
[00:10:00.200 --> 00:10:01.560]   for the rest of the world,
[00:10:01.560 --> 00:10:03.940]   and I probably met you a few weeks afterwards,
[00:10:03.940 --> 00:10:07.480]   and that was probably the fastest investment I'd ever made.
[00:10:07.480 --> 00:10:12.300]   I remember the key moment is you did not have a company,
[00:10:12.300 --> 00:10:14.080]   and so we had to incorporate the company
[00:10:14.080 --> 00:10:15.240]   after the check was written,
[00:10:15.240 --> 00:10:18.280]   which is always either a sign of complete stupidity
[00:10:18.280 --> 00:10:21.280]   or in 15 or 20 years, you'll look like a genius,
[00:10:21.280 --> 00:10:24.120]   but the odds of the latter are quite small.
[00:10:24.120 --> 00:10:25.740]   Okay, so you start the business.
[00:10:25.740 --> 00:10:27.740]   Tell us about the design decisions you were making
[00:10:27.740 --> 00:10:30.280]   in Grok at the time, knowing what you knew then,
[00:10:30.280 --> 00:10:32.160]   because at the time, it's very different
[00:10:32.160 --> 00:10:33.640]   from what it is now.
[00:10:33.640 --> 00:10:35.680]   Well, again, when we started fundraising,
[00:10:35.680 --> 00:10:37.480]   we actually weren't even 100% sure
[00:10:37.480 --> 00:10:39.320]   that we were gonna do something in hardware,
[00:10:39.320 --> 00:10:41.580]   but it was something that I think you asked, Shamath,
[00:10:41.580 --> 00:10:43.680]   which is what would you do differently,
[00:10:43.680 --> 00:10:46.180]   and my answer was the software,
[00:10:46.180 --> 00:10:47.680]   because the big problem we had
[00:10:47.680 --> 00:10:50.080]   was we could build these chips in Google,
[00:10:50.080 --> 00:10:53.280]   but programming them, every single team at Google
[00:10:53.280 --> 00:10:57.000]   had a dedicated person who was hand-optimizing the models,
[00:10:57.000 --> 00:10:59.440]   and I'm like, this is absolutely crazy.
[00:10:59.440 --> 00:11:02.080]   Right around then, we had started hiring some people
[00:11:02.080 --> 00:11:03.900]   from NVIDIA, and they're like, no, no, no,
[00:11:03.900 --> 00:11:04.740]   you don't understand.
[00:11:04.740 --> 00:11:05.560]   This is just how it works.
[00:11:05.560 --> 00:11:06.400]   This is how we do it, too.
[00:11:06.400 --> 00:11:08.880]   We've got these things called kernels, CUDA kernels,
[00:11:08.880 --> 00:11:09.840]   and we hand-optimize them.
[00:11:09.840 --> 00:11:12.300]   We just make it look like we're not doing that,
[00:11:12.300 --> 00:11:15.600]   but the scale, like, all of you understand algorithms
[00:11:15.600 --> 00:11:17.580]   and big O complexity.
[00:11:17.580 --> 00:11:18.840]   That's linear complexity.
[00:11:18.840 --> 00:11:21.160]   For every application, you need an engineer.
[00:11:21.160 --> 00:11:24.040]   NVIDIA now has 50,000 people in their ecosystem.
[00:11:24.040 --> 00:11:26.360]   How does any, and these are like really low-level
[00:11:26.360 --> 00:11:28.200]   kernel-writing, assembly-writing hackers
[00:11:28.200 --> 00:11:30.100]   who understand GPUs and ML and everything.
[00:11:30.100 --> 00:11:32.420]   Not gonna scale, so we focused on the compiler
[00:11:32.420 --> 00:11:34.280]   for the first six months.
[00:11:34.280 --> 00:11:35.960]   We banned whiteboards at Grok
[00:11:35.960 --> 00:11:38.560]   because people kept trying to draw pictures of chips.
[00:11:38.560 --> 00:11:39.520]   Like, yeah.
[00:11:39.520 --> 00:11:45.180]   - So why is it that LLMs prefer Grok?
[00:11:45.180 --> 00:11:47.220]   Like, what was the design decision,
[00:11:47.220 --> 00:11:49.840]   or what happened in the design of LLMs?
[00:11:49.840 --> 00:11:51.280]   Some part of it is skill, obviously,
[00:11:51.280 --> 00:11:52.760]   but some part of it was a little bit of luck,
[00:11:52.760 --> 00:11:54.840]   but where, what exactly happened
[00:11:54.840 --> 00:11:57.960]   that makes you so much faster than NVIDIA
[00:11:57.960 --> 00:11:59.760]   and why there's all of these developers?
[00:11:59.760 --> 00:12:00.720]   What is the?
[00:12:00.720 --> 00:12:03.080]   - The crux of it, we didn't know
[00:12:03.080 --> 00:12:04.600]   that it was gonna be language,
[00:12:04.600 --> 00:12:07.880]   but the inspiration, the last thing that I worked on
[00:12:07.880 --> 00:12:11.800]   was getting the AlphaGo software,
[00:12:11.800 --> 00:12:16.000]   the Go playing software at DeepMind working on TPU,
[00:12:16.000 --> 00:12:19.400]   and having watched that, it was very clear
[00:12:19.400 --> 00:12:22.800]   that inference was going to be a scaled problem.
[00:12:22.800 --> 00:12:24.720]   Everyone else had been looking at inference
[00:12:24.720 --> 00:12:27.600]   as you take one chip, you run a model on it,
[00:12:27.600 --> 00:12:28.960]   it runs whatever.
[00:12:28.960 --> 00:12:31.820]   But what happened with AlphaGo
[00:12:31.820 --> 00:12:34.320]   was we ported the software over,
[00:12:34.320 --> 00:12:38.920]   and even though we had 170 GPUs versus 48 TPUs,
[00:12:38.920 --> 00:12:42.080]   the 48 TPUs won 99 out of 100 games
[00:12:42.080 --> 00:12:44.580]   with the exact same software.
[00:12:44.580 --> 00:12:47.360]   What that meant was compute was going to result
[00:12:47.360 --> 00:12:49.240]   in better performance.
[00:12:49.240 --> 00:12:53.880]   And so the insight was, let's build scaled inference.
[00:12:53.880 --> 00:12:57.540]   So we built in the interconnect, we built it for scale,
[00:12:57.540 --> 00:12:58.560]   and that's what we do now
[00:12:58.560 --> 00:13:00.240]   when we're running one of these models,
[00:13:00.240 --> 00:13:02.440]   we have hundreds or thousands of chips contributing
[00:13:02.440 --> 00:13:04.600]   just like we did with AlphaGo,
[00:13:04.600 --> 00:13:07.840]   but it's built for this as opposed to cobbled together.
[00:13:07.840 --> 00:13:09.220]   - I think this is a good jumping off point.
[00:13:09.220 --> 00:13:11.840]   A lot of people, and I think this company
[00:13:11.840 --> 00:13:12.760]   deserves a lot of respect,
[00:13:12.760 --> 00:13:16.080]   but NVIDIA has been toiling for decades,
[00:13:16.080 --> 00:13:19.480]   and they have clearly built an incredible business.
[00:13:19.480 --> 00:13:21.760]   But in some ways, when you get into the details,
[00:13:21.760 --> 00:13:24.420]   the business is slightly misunderstood.
[00:13:24.420 --> 00:13:26.680]   So can you break down, first of all,
[00:13:26.680 --> 00:13:29.720]   where is NVIDIA natively good,
[00:13:29.720 --> 00:13:31.980]   and where is it more trying to be good?
[00:13:31.980 --> 00:13:36.520]   - So natively good, the classic saying is
[00:13:36.520 --> 00:13:37.600]   you don't have to outrun the bear,
[00:13:37.600 --> 00:13:39.240]   you just have to outrun your friends.
[00:13:39.240 --> 00:13:41.720]   So NVIDIA outruns all of the other chip companies
[00:13:41.720 --> 00:13:42.800]   when it comes to software,
[00:13:42.800 --> 00:13:44.520]   but they're not a software-first company.
[00:13:44.520 --> 00:13:47.360]   They actually have a very expensive approach,
[00:13:47.360 --> 00:13:48.360]   as we discussed.
[00:13:48.360 --> 00:13:50.800]   But they have the ecosystem.
[00:13:50.800 --> 00:13:52.040]   It's a double-sided market.
[00:13:52.040 --> 00:13:54.560]   If you have a kernel-based approach, they've already won.
[00:13:54.560 --> 00:13:56.200]   There's no catching up.
[00:13:56.200 --> 00:13:58.280]   Hence why we have a kernel-free approach.
[00:13:58.280 --> 00:14:00.480]   But the other way that they're very good
[00:14:00.480 --> 00:14:04.400]   is vertical integration and forward integration.
[00:14:04.400 --> 00:14:07.320]   What happens is NVIDIA, over and over again,
[00:14:07.320 --> 00:14:09.800]   decides that they wanna move up the stack,
[00:14:09.800 --> 00:14:11.160]   and whatever their customers are doing,
[00:14:11.160 --> 00:14:12.220]   they start doing it.
[00:14:12.220 --> 00:14:14.120]   So for example, I think it was Gigabyte
[00:14:14.120 --> 00:14:17.160]   or one of these other PCI board manufacturers
[00:14:17.160 --> 00:14:18.240]   who recently announced,
[00:14:18.240 --> 00:14:20.840]   even though 80% of their revenue came from NVIDIA,
[00:14:20.840 --> 00:14:23.520]   NVIDIA boards that they were building,
[00:14:23.520 --> 00:14:26.400]   they're exiting that market because NVIDIA moved up
[00:14:26.400 --> 00:14:28.120]   and started doing a much lower-margin thing.
[00:14:28.120 --> 00:14:29.240]   And you just see that over and over again.
[00:14:29.240 --> 00:14:30.080]   They started building systems.
[00:14:30.080 --> 00:14:31.440]   - Yeah, I think the other thing is
[00:14:31.440 --> 00:14:33.800]   that NVIDIA's incredible at training.
[00:14:33.800 --> 00:14:36.600]   And I think the design decisions that they made,
[00:14:36.600 --> 00:14:38.440]   including things like HBM,
[00:14:38.440 --> 00:14:41.460]   were really oriented around a world back then,
[00:14:41.460 --> 00:14:43.080]   which was everything was about training.
[00:14:43.080 --> 00:14:44.920]   There weren't any real-world applications.
[00:14:44.920 --> 00:14:47.440]   None of you guys were really building anything in the wild
[00:14:47.440 --> 00:14:49.160]   where you needed super-fast inference.
[00:14:49.160 --> 00:14:50.640]   And I think that's another.
[00:14:50.640 --> 00:14:53.160]   - Absolutely, and what we saw over and over again
[00:14:53.160 --> 00:14:56.800]   was you would spend 100% of your compute on training.
[00:14:56.800 --> 00:14:58.400]   You would get something that would work well enough
[00:14:58.400 --> 00:14:59.760]   to go into production.
[00:14:59.760 --> 00:15:04.200]   And then it would flip to about 5% to 10% training
[00:15:04.200 --> 00:15:07.080]   and 90% to 95% inference.
[00:15:07.080 --> 00:15:09.120]   But the amount of training would stay the same.
[00:15:09.120 --> 00:15:11.520]   The inference would grow massively.
[00:15:11.520 --> 00:15:14.840]   And so every time we would have a success at Google,
[00:15:14.840 --> 00:15:16.840]   all of a sudden, we would have a disaster.
[00:15:16.840 --> 00:15:18.320]   We called it the success disaster
[00:15:18.320 --> 00:15:22.280]   where we can't afford to get enough compute for inference
[00:15:22.280 --> 00:15:26.520]   'cause it goes 1020x immediately, over and over.
[00:15:26.520 --> 00:15:28.480]   - And if you take that 1020x and multiply it
[00:15:28.480 --> 00:15:31.320]   by the cost of NVIDIA's leading-class solutions,
[00:15:31.320 --> 00:15:33.000]   you're talking just an enormous amount of money.
[00:15:33.000 --> 00:15:36.520]   So just maybe explain to folks what HBM is
[00:15:36.520 --> 00:15:37.860]   and why these systems,
[00:15:37.860 --> 00:15:39.920]   like what NVIDIA just announced as B200,
[00:15:39.920 --> 00:15:41.800]   the complexity and the cost, actually,
[00:15:41.800 --> 00:15:43.840]   if you're trying to do something.
[00:15:43.840 --> 00:15:46.600]   - Yeah, the complexity spans every part of the stack,
[00:15:46.600 --> 00:15:48.120]   but there's a couple of components
[00:15:48.120 --> 00:15:50.760]   which are in very limited supply.
[00:15:50.760 --> 00:15:53.560]   And NVIDIA has locked up the market on these.
[00:15:53.560 --> 00:15:55.040]   One of them is HBM.
[00:15:55.040 --> 00:15:57.840]   HBM is this high-bandwidth memory
[00:15:57.840 --> 00:16:00.720]   which is required to get performance
[00:16:00.720 --> 00:16:04.640]   because the speed at which you can run these applications
[00:16:04.640 --> 00:16:06.520]   depends on how quickly you can read that memory.
[00:16:06.520 --> 00:16:08.720]   And this is the fastest memory.
[00:16:08.720 --> 00:16:10.000]   There is a finite supply.
[00:16:10.000 --> 00:16:11.840]   It is only for data centers.
[00:16:11.840 --> 00:16:14.560]   So they can't reach into the supply for mobile
[00:16:14.560 --> 00:16:16.800]   or other things like you can with other parts.
[00:16:16.800 --> 00:16:18.400]   But also interposers.
[00:16:18.400 --> 00:16:22.320]   Also, NVIDIA's the largest buyer of super caps in the world
[00:16:22.320 --> 00:16:24.040]   and all sorts of other components.
[00:16:24.040 --> 00:16:24.880]   - Cables.
[00:16:24.880 --> 00:16:27.520]   - Cables, the 400-gigabit cables,
[00:16:27.520 --> 00:16:28.920]   they've bought them all out.
[00:16:28.920 --> 00:16:30.280]   So if you want to compete,
[00:16:30.280 --> 00:16:32.720]   it doesn't matter how good of a product you design.
[00:16:32.720 --> 00:16:35.460]   They've bought out the entire supply chain.
[00:16:35.460 --> 00:16:36.300]   - For years.
[00:16:36.300 --> 00:16:37.240]   - For years.
[00:16:38.080 --> 00:16:40.440]   - So what do you do?
[00:16:40.440 --> 00:16:42.640]   - You don't use the same things they do.
[00:16:42.640 --> 00:16:43.480]   - Right.
[00:16:43.480 --> 00:16:45.120]   - And that's where we come in.
[00:16:45.120 --> 00:16:47.400]   - So how do you design a chip, then?
[00:16:47.400 --> 00:16:49.240]   If you look at the leading solution
[00:16:49.240 --> 00:16:50.560]   and they're using certain things
[00:16:50.560 --> 00:16:52.480]   and they're clearly being successful,
[00:16:52.480 --> 00:16:55.680]   how do you, is it just a technical bet
[00:16:55.680 --> 00:16:57.360]   to be totally orthogonal and different?
[00:16:57.360 --> 00:16:59.760]   Or was it something very specific
[00:16:59.760 --> 00:17:01.920]   where you said we cannot be reliant on the same supply chain
[00:17:01.920 --> 00:17:04.480]   'cause we'll just get forced out of business at some point?
[00:17:04.480 --> 00:17:06.840]   - It was actually a really simple observation
[00:17:06.840 --> 00:17:07.800]   at the beginning,
[00:17:07.800 --> 00:17:11.720]   which is most chip architectures compete
[00:17:11.720 --> 00:17:14.600]   on small percentages difference in performance,
[00:17:14.600 --> 00:17:18.000]   like 15% is considered amazing.
[00:17:18.000 --> 00:17:21.360]   And what we realized was if we were 15% better,
[00:17:21.360 --> 00:17:22.780]   no one was gonna change
[00:17:22.780 --> 00:17:24.760]   to a radically different architecture.
[00:17:24.760 --> 00:17:27.400]   We needed to be five to 10x.
[00:17:27.400 --> 00:17:30.780]   Therefore, the small percentages that you get
[00:17:30.780 --> 00:17:34.600]   chasing the leading edge technologies was irrelevant.
[00:17:34.600 --> 00:17:37.560]   So we used an older technology, 14 nanometer,
[00:17:37.560 --> 00:17:39.560]   which is underutilized.
[00:17:39.560 --> 00:17:41.480]   We didn't use external memory.
[00:17:41.480 --> 00:17:44.080]   We used older interconnect
[00:17:44.080 --> 00:17:46.760]   because our architecture needed to provide the advantage
[00:17:46.760 --> 00:17:49.140]   and it needed to be so overwhelming
[00:17:49.140 --> 00:17:51.720]   that we didn't need to be at the leading edge.
[00:17:51.720 --> 00:17:54.840]   - So how do you measure sort of speed and value today?
[00:17:54.840 --> 00:17:56.500]   And just give us some comparisons
[00:17:56.500 --> 00:17:59.740]   for you versus some other folks running
[00:17:59.740 --> 00:18:01.120]   what these guys are probably using,
[00:18:01.120 --> 00:18:02.600]   Lama, Mistral, et cetera.
[00:18:02.600 --> 00:18:07.600]   - Yeah, so we run, we compare on two sides of this.
[00:18:07.600 --> 00:18:09.400]   One is the tokens per dollar
[00:18:09.400 --> 00:18:12.160]   and one is the tokens per second per user.
[00:18:12.160 --> 00:18:14.480]   So tokens per second per user is the experience.
[00:18:14.480 --> 00:18:16.040]   That's the differentiation.
[00:18:16.040 --> 00:18:18.640]   And tokens per dollar is the cost.
[00:18:18.640 --> 00:18:20.640]   And then also, of course, tokens per watt
[00:18:20.640 --> 00:18:22.880]   because power is very limited at the moment.
[00:18:22.880 --> 00:18:23.720]   - Right.
[00:18:23.720 --> 00:18:26.880]   - If you were to compare us to GPUs,
[00:18:26.880 --> 00:18:30.160]   we're typically five to 10x faster.
[00:18:30.160 --> 00:18:33.600]   Apples to apples, like without using speculative decode
[00:18:33.600 --> 00:18:34.900]   and other things.
[00:18:34.900 --> 00:18:39.360]   So right now, on a 180 billion parameter model,
[00:18:39.360 --> 00:18:42.640]   we run about 200 tokens per second,
[00:18:42.640 --> 00:18:44.880]   which I think is less than 50
[00:18:44.880 --> 00:18:47.920]   on the next generation GPU that's coming out.
[00:18:47.920 --> 00:18:48.760]   - From NVIDIA?
[00:18:48.760 --> 00:18:49.960]   - From NVIDIA.
[00:18:49.960 --> 00:18:54.240]   - So your current generation is 4x better than the B200?
[00:18:54.240 --> 00:18:55.240]   - Yeah, yeah.
[00:18:55.240 --> 00:18:59.560]   And then in total cost, we're about 1/10 the cost
[00:18:59.560 --> 00:19:02.600]   versus a modern GPU per token.
[00:19:02.600 --> 00:19:08.280]   I want that to sink in for a moment, 1/10 of the cost.
[00:19:08.280 --> 00:19:09.680]   - Yeah, I mean, I think the value of that
[00:19:09.680 --> 00:19:11.720]   really comes down to the fact that
[00:19:11.720 --> 00:19:13.720]   you guys are gonna go and have ideas,
[00:19:13.720 --> 00:19:16.880]   and especially if you are part of the venture community
[00:19:16.880 --> 00:19:18.640]   and ecosystem and you raise money,
[00:19:18.640 --> 00:19:21.160]   folks like me who will give you money
[00:19:21.160 --> 00:19:23.240]   will expect you to be investing that wisely.
[00:19:23.240 --> 00:19:26.280]   Last decade, we went into a very negative cycle
[00:19:26.280 --> 00:19:28.680]   where almost 50 cents of every dollar
[00:19:28.680 --> 00:19:30.320]   we would give a startup would go right back
[00:19:30.320 --> 00:19:33.160]   into the hands of Google, Amazon, and Facebook.
[00:19:33.160 --> 00:19:34.360]   You're spending it on compute
[00:19:34.360 --> 00:19:36.120]   and you're spending it on ads.
[00:19:36.120 --> 00:19:38.960]   This time around, the power of AI should be
[00:19:38.960 --> 00:19:40.760]   that you can build companies for 1/10
[00:19:40.760 --> 00:19:43.960]   or 1/100 of the cost, but that won't be possible
[00:19:43.960 --> 00:19:45.600]   if you're, again, just shipping the money back out,
[00:19:45.600 --> 00:19:47.160]   except just now, in this case,
[00:19:47.160 --> 00:19:49.200]   to NVIDIA versus somebody else.
[00:19:49.200 --> 00:19:51.720]   So we will be pushing to make sure
[00:19:51.720 --> 00:19:56.160]   that this is kind of the low-cost alternative that happens.
[00:19:56.160 --> 00:19:59.840]   - So NVIDIA had a huge splashy announcement a few weeks ago.
[00:19:59.840 --> 00:20:02.040]   They showed charts, things going up and to the right.
[00:20:02.040 --> 00:20:03.320]   They showed huge dyes.
[00:20:03.320 --> 00:20:05.140]   They showed huge packaging.
[00:20:05.140 --> 00:20:08.240]   Tell us about the B200 and compare it
[00:20:08.240 --> 00:20:09.960]   to what you're doing right now.
[00:20:09.960 --> 00:20:12.120]   - Well, the first thing is the B200
[00:20:12.120 --> 00:20:13.720]   is a marvel of engineering.
[00:20:13.720 --> 00:20:16.940]   The level of complexity, the level of integration,
[00:20:16.940 --> 00:20:19.960]   the amount of different components in silicon.
[00:20:19.960 --> 00:20:22.860]   They spent $10 billion developing it,
[00:20:22.860 --> 00:20:25.120]   but when it was announced,
[00:20:25.120 --> 00:20:28.240]   I got some pings from NVIDIA engineers who said,
[00:20:28.240 --> 00:20:30.640]   we were a little embarrassed that they were claiming 30X
[00:20:30.640 --> 00:20:31.680]   'cause it's nowhere near that.
[00:20:31.680 --> 00:20:33.040]   And we as engineers felt
[00:20:33.040 --> 00:20:36.040]   that that was hurting our credibility.
[00:20:36.040 --> 00:20:39.960]   The 30X claim was, let's put it into perspective.
[00:20:39.960 --> 00:20:42.300]   There was this one image that showed a claim
[00:20:42.300 --> 00:20:46.080]   of up to 50 tokens per second from the user experience
[00:20:46.080 --> 00:20:49.000]   and 140 throughput.
[00:20:49.000 --> 00:20:52.440]   That sort of gives you the value or the cost.
[00:20:52.440 --> 00:20:54.600]   If you were to compare that to the previous generation,
[00:20:54.600 --> 00:20:56.200]   that would be saying that the users,
[00:20:56.200 --> 00:20:58.040]   if you divide 50 by 30,
[00:20:58.040 --> 00:21:00.400]   are getting less than two tokens per second,
[00:21:00.400 --> 00:21:05.320]   which would be slow, right?
[00:21:05.320 --> 00:21:07.020]   There's nothing running that slow.
[00:21:07.020 --> 00:21:08.920]   And then also from a throughput perspective,
[00:21:08.920 --> 00:21:10.720]   that would make the cost so astronomical,
[00:21:10.720 --> 00:21:11.560]   it would be unbelievable.
[00:21:11.560 --> 00:21:13.120]   - I mean, how many of you guys use
[00:21:13.120 --> 00:21:15.120]   any of these chat agents right now?
[00:21:15.120 --> 00:21:17.320]   Just raise your hand if you use them.
[00:21:17.320 --> 00:21:19.280]   And how many of you keep your hands raised
[00:21:19.280 --> 00:21:21.780]   if you're satisfied with the speed of performance?
[00:21:21.780 --> 00:21:24.400]   You're satisfied.
[00:21:24.400 --> 00:21:26.160]   One hand or two?
[00:21:26.160 --> 00:21:27.840]   - There's like two or three, yeah.
[00:21:27.840 --> 00:21:28.680]   That's nice.
[00:21:28.680 --> 00:21:34.160]   My experience has been that these things,
[00:21:34.160 --> 00:21:37.120]   if you want to actually make hallucinations go to zero
[00:21:37.120 --> 00:21:40.200]   and the quality of these models really fine-tuned,
[00:21:40.200 --> 00:21:42.000]   you have to get back to kind of like
[00:21:42.000 --> 00:21:43.420]   a traditional web experience
[00:21:43.420 --> 00:21:44.840]   or a traditional mobile app experience
[00:21:44.840 --> 00:21:48.100]   where you have a window of probably 300 milliseconds
[00:21:48.100 --> 00:21:49.360]   to get an answer back.
[00:21:49.360 --> 00:21:50.200]   In the absence of that,
[00:21:50.200 --> 00:21:53.360]   the user experience doesn't scale and it kind of sucks.
[00:21:53.360 --> 00:21:56.440]   - How much effort did you spend at Meta and Facebook
[00:21:56.440 --> 00:21:57.600]   getting latency down?
[00:21:57.600 --> 00:21:58.960]   - I mean, look, at Facebook at one point,
[00:21:58.960 --> 00:22:01.520]   I had a team, I was so disgusted with the speed.
[00:22:01.520 --> 00:22:03.160]   So in a cold cache,
[00:22:03.160 --> 00:22:05.640]   we were approaching 1,000 milliseconds.
[00:22:05.640 --> 00:22:10.160]   And I was so disgusted that I took a small team
[00:22:10.160 --> 00:22:13.520]   off to the side, rebuilt the entire website
[00:22:13.520 --> 00:22:17.360]   and launched it in India for the Indian market
[00:22:17.360 --> 00:22:22.800]   just to prove that we could get it under 500 milliseconds.
[00:22:22.800 --> 00:22:26.880]   And it was a huge technical feat that the team did.
[00:22:26.880 --> 00:22:28.800]   It was also very poorly received
[00:22:28.800 --> 00:22:30.360]   by the mainline engineering team
[00:22:30.360 --> 00:22:31.920]   because it was somewhat embarrassing.
[00:22:31.920 --> 00:22:34.280]   But that's the level of intensity
[00:22:34.280 --> 00:22:36.520]   we had to approach this problem with.
[00:22:36.520 --> 00:22:37.520]   And it wasn't just us.
[00:22:37.520 --> 00:22:39.760]   Google realized it, everybody has realized it.
[00:22:39.760 --> 00:22:41.120]   There's an economic equation
[00:22:41.120 --> 00:22:44.080]   where if you deliver an experience to users
[00:22:44.080 --> 00:22:48.720]   under about 250 to 300 milliseconds, you maximize revenue.
[00:22:48.720 --> 00:22:50.720]   So if you actually want to be successful,
[00:22:50.720 --> 00:22:52.520]   that is the number you have to get to.
[00:22:52.520 --> 00:22:55.280]   So the idea that you can wait and fetch an answer
[00:22:55.280 --> 00:22:59.800]   in three and five seconds is completely ridiculous.
[00:22:59.800 --> 00:23:02.840]   It's a non-starter.
[00:23:02.840 --> 00:23:03.920]   - Here's the actual number.
[00:23:03.920 --> 00:23:06.240]   The number is every 100 milliseconds
[00:23:06.240 --> 00:23:11.240]   leads to 8% more engagement on desktop, 34% on mobile.
[00:23:11.240 --> 00:23:14.760]   We're talking about 100 milliseconds,
[00:23:14.760 --> 00:23:16.920]   which is 1/10 of a second.
[00:23:16.920 --> 00:23:19.280]   Right now, these things take 10 seconds.
[00:23:19.280 --> 00:23:21.320]   So think about how much less engagement
[00:23:21.320 --> 00:23:24.440]   we were getting today than you otherwise could.
[00:23:24.440 --> 00:23:26.440]   - So why don't you now break down this difference?
[00:23:26.440 --> 00:23:28.760]   Because this is now where I think a good place
[00:23:28.760 --> 00:23:31.360]   so that people leave really understanding.
[00:23:31.360 --> 00:23:32.600]   There's an enormous difference
[00:23:32.600 --> 00:23:37.600]   between training and inference and what is required.
[00:23:37.600 --> 00:23:40.080]   And why don't you define the differences
[00:23:40.080 --> 00:23:43.280]   so that then we can contrast where things are gonna go?
[00:23:43.280 --> 00:23:45.440]   - The biggest is when you're training,
[00:23:45.440 --> 00:23:48.400]   the number of tokens that you're training on
[00:23:48.400 --> 00:23:50.120]   is measured in months.
[00:23:50.120 --> 00:23:53.040]   Like how many tokens can we train on this month?
[00:23:53.040 --> 00:23:56.600]   It doesn't matter if it takes a second, 10 seconds,
[00:23:56.600 --> 00:24:00.000]   100 seconds in a batch, how many per month.
[00:24:00.000 --> 00:24:03.080]   In inference, what matters is how many tokens
[00:24:03.080 --> 00:24:07.840]   you can generate per millisecond or a couple of milliseconds.
[00:24:07.840 --> 00:24:10.520]   It's not in seconds, it's not in months.
[00:24:10.520 --> 00:24:12.280]   - Is it fair to say then
[00:24:12.280 --> 00:24:15.800]   that NVIDIA is the exemplar in training?
[00:24:15.800 --> 00:24:16.640]   - Yes.
[00:24:16.640 --> 00:24:20.520]   - And then is it fair to say that there really isn't yet
[00:24:20.520 --> 00:24:25.560]   the equivalent scaled winner in inference?
[00:24:25.560 --> 00:24:26.560]   - Not yet.
[00:24:26.560 --> 00:24:29.200]   - And do you think that it will be NVIDIA?
[00:24:29.200 --> 00:24:31.520]   - I don't think it'll be NVIDIA.
[00:24:31.520 --> 00:24:35.400]   - But specifically, why do you not think it won't work
[00:24:35.400 --> 00:24:38.760]   for that market, even though it's clearly working in training
[00:24:38.760 --> 00:24:42.840]   - In order to get the latency down, what we had to do,
[00:24:42.840 --> 00:24:45.840]   we had to design a completely new chip architecture.
[00:24:45.840 --> 00:24:48.720]   We had to design a completely new networking architecture,
[00:24:48.720 --> 00:24:51.160]   an entirely new system, an entirely new runtime,
[00:24:51.160 --> 00:24:52.320]   an entirely new compiler,
[00:24:52.320 --> 00:24:55.040]   an entirely new orchestration layer.
[00:24:55.040 --> 00:24:57.180]   We had to throw everything away.
[00:24:57.180 --> 00:25:01.240]   And it had to be compatible with PyTorch
[00:25:01.240 --> 00:25:03.320]   and what other people actually developing.
[00:25:03.320 --> 00:25:06.400]   Now we're talking about innovators dilemma on steroids.
[00:25:06.400 --> 00:25:08.200]   It's hard enough to give up one of those,
[00:25:08.200 --> 00:25:10.120]   which if you were to do one of those successfully
[00:25:10.120 --> 00:25:12.720]   would be a very valuable company.
[00:25:12.720 --> 00:25:16.520]   But to throw all six of those away is nearly impossible.
[00:25:16.520 --> 00:25:18.520]   And also you have to maintain what you have
[00:25:18.520 --> 00:25:20.520]   if you wanna keep training.
[00:25:20.520 --> 00:25:22.520]   And so now you have to have a completely different
[00:25:22.520 --> 00:25:26.640]   architecture for networking, for training versus inference,
[00:25:26.640 --> 00:25:30.320]   for your chip, for networking, for everything.
[00:25:30.320 --> 00:25:32.120]   - So let's say that the market today
[00:25:32.120 --> 00:25:35.760]   is 100 units of training or 95 units of training,
[00:25:35.760 --> 00:25:37.000]   five units of inference.
[00:25:37.000 --> 00:25:39.840]   I should say that's roughly where most of the revenue
[00:25:39.840 --> 00:25:41.400]   and the dollars are being made.
[00:25:42.360 --> 00:25:45.360]   What does it look like in four or five years from now?
[00:25:45.360 --> 00:25:48.600]   - Well, actually NVIDIA's latest earnings, 40% inference.
[00:25:48.600 --> 00:25:50.800]   It's already starting to climb.
[00:25:50.800 --> 00:25:52.720]   Where it's gonna end is it will end
[00:25:52.720 --> 00:25:57.720]   at somewhere between 90 to 95% or 90 to 95 units inference.
[00:25:57.720 --> 00:26:00.600]   And so that trajectory is gonna take off rapidly
[00:26:00.600 --> 00:26:02.480]   now that we have these open source models
[00:26:02.480 --> 00:26:04.280]   that everyone is giving away
[00:26:04.280 --> 00:26:06.680]   and you can download a model and run it.
[00:26:06.680 --> 00:26:07.840]   You don't need to train it.
[00:26:07.840 --> 00:26:09.840]   - Yeah, one of the things about these open source models
[00:26:09.840 --> 00:26:14.040]   is building useful applications,
[00:26:14.040 --> 00:26:18.560]   you have to either understand or be able to work with CUDA.
[00:26:18.560 --> 00:26:20.680]   With you, it doesn't even matter because you can just port.
[00:26:20.680 --> 00:26:23.400]   So maybe explain to folks the importance
[00:26:23.400 --> 00:26:25.000]   in the inference market of being able to rip
[00:26:25.000 --> 00:26:26.120]   and replace these models
[00:26:26.120 --> 00:26:28.280]   and where you think these models are going.
[00:26:28.280 --> 00:26:33.280]   - So for the inference market, every two weeks or so,
[00:26:33.280 --> 00:26:38.800]   there is a completely new model that has to be run.
[00:26:38.800 --> 00:26:40.720]   It's important, it matters.
[00:26:40.720 --> 00:26:45.720]   Either it's setting the best quality bar across the board
[00:26:45.720 --> 00:26:48.920]   or it's good at a particular task.
[00:26:48.920 --> 00:26:50.960]   If you are writing kernels,
[00:26:50.960 --> 00:26:52.480]   it's almost impossible to keep up.
[00:26:52.480 --> 00:26:55.640]   In fact, when LLAMA2 70 billion was launched,
[00:26:55.640 --> 00:26:58.520]   it officially had support for AMD.
[00:26:58.520 --> 00:27:03.080]   However, the first support we actually saw implemented
[00:27:03.080 --> 00:27:08.080]   was after about a week and we had it in I think two days.
[00:27:08.080 --> 00:27:09.400]   And so that speed,
[00:27:09.400 --> 00:27:11.760]   now everyone develops for NVIDIA hardware.
[00:27:11.760 --> 00:27:14.840]   So by default, anything launched will work there.
[00:27:14.840 --> 00:27:17.720]   But if you want anything else to work,
[00:27:17.720 --> 00:27:19.720]   you can't be writing these kernels by hand.
[00:27:19.720 --> 00:27:21.480]   And remember, AMD had official support
[00:27:21.480 --> 00:27:22.680]   and it still took about a week.
[00:27:22.680 --> 00:27:23.680]   - Right.
[00:27:23.680 --> 00:27:26.960]   So if you're starting a company today,
[00:27:26.960 --> 00:27:32.360]   you clearly wanna have the ability to swap from LLAMA
[00:27:32.360 --> 00:27:36.080]   to Mistral to Anthropic back as often as possible.
[00:27:36.080 --> 00:27:37.240]   - Whatever's latest.
[00:27:37.240 --> 00:27:40.920]   - And just as somebody who sees these models run,
[00:27:40.920 --> 00:27:42.880]   do you have any comment on the quality of these models
[00:27:42.880 --> 00:27:44.800]   and where you think some of these companies are going
[00:27:44.800 --> 00:27:47.840]   or what you see some doing well versus others?
[00:27:47.840 --> 00:27:51.000]   - So they're all starting to catch up with each other.
[00:27:51.000 --> 00:27:52.600]   You're starting to see some leapfrogging.
[00:27:52.600 --> 00:27:55.840]   It started off with GPT-4 pulling ahead
[00:27:55.840 --> 00:27:58.600]   and it had a lead for about a year over everyone else.
[00:27:58.600 --> 00:28:00.840]   And now of course Anthropic has caught up.
[00:28:00.840 --> 00:28:03.360]   We're seeing some great stuff from Mistral.
[00:28:03.360 --> 00:28:05.200]   But across the board,
[00:28:05.200 --> 00:28:08.240]   they're all starting to bunch up in quality.
[00:28:08.240 --> 00:28:09.840]   And so one of the interesting things,
[00:28:09.840 --> 00:28:11.600]   Mistral in particular,
[00:28:11.600 --> 00:28:14.120]   has been able to get closer to quality
[00:28:14.120 --> 00:28:16.120]   with smaller, less expensive models to run,
[00:28:16.120 --> 00:28:18.920]   which I think gives them a huge advantage.
[00:28:18.920 --> 00:28:21.680]   I think Cohere has an interesting take
[00:28:21.680 --> 00:28:24.600]   on a sort of rag optimized model.
[00:28:24.600 --> 00:28:26.560]   So people are finding niches.
[00:28:26.560 --> 00:28:28.440]   And there's gonna be a couple
[00:28:28.440 --> 00:28:30.960]   that are gonna be the best across the board
[00:28:30.960 --> 00:28:33.000]   at the highest end.
[00:28:33.000 --> 00:28:34.720]   But what we're seeing is a lot of complaints
[00:28:34.720 --> 00:28:36.920]   about the cost to run these models.
[00:28:36.920 --> 00:28:38.680]   They're just astronomical.
[00:28:38.680 --> 00:28:42.360]   And you're not gonna be able to scale up applications
[00:28:42.360 --> 00:28:43.480]   for users with them.
[00:28:43.480 --> 00:28:48.480]   - OpenAI has published or has disclosed,
[00:28:48.480 --> 00:28:51.640]   as has Meta, as has Tesla and a couple of others,
[00:28:51.640 --> 00:28:55.640]   just the total quantum of GPU capacity that they're buying.
[00:28:55.640 --> 00:28:56.920]   And you can kind of work backwards
[00:28:56.920 --> 00:29:00.400]   to figure out how big the inference market can be,
[00:29:00.400 --> 00:29:02.760]   because it's really only supported by them
[00:29:02.760 --> 00:29:03.960]   as you guys scale up.
[00:29:03.960 --> 00:29:06.360]   Can you give people a sense of the scale
[00:29:06.360 --> 00:29:08.320]   of what folks are fighting for?
[00:29:08.320 --> 00:29:12.480]   - So I think Facebook announced
[00:29:12.480 --> 00:29:14.160]   that by the end of this year,
[00:29:14.160 --> 00:29:18.280]   they're gonna have the equivalent of 650,000 H100s.
[00:29:18.280 --> 00:29:21.760]   By the end of this year,
[00:29:21.760 --> 00:29:25.000]   Grok will have deployed 100,000 of our LPUs,
[00:29:25.000 --> 00:29:28.840]   which do outperform the H100s on a throughput
[00:29:28.840 --> 00:29:31.000]   and on a latency basis.
[00:29:31.000 --> 00:29:33.200]   So we will probably get pretty close
[00:29:33.200 --> 00:29:36.840]   to the equivalent of Meta ourselves.
[00:29:36.840 --> 00:29:37.840]   By the end of next year,
[00:29:37.840 --> 00:29:41.320]   we're going to deploy 1.5 million LPUs.
[00:29:41.320 --> 00:29:43.600]   For comparison, last year,
[00:29:43.600 --> 00:29:47.520]   NVIDIA deployed a total of 500,000 H100s.
[00:29:47.520 --> 00:29:51.800]   So 1.5 million means that Grok will probably have
[00:29:51.800 --> 00:29:55.440]   more inference generative AI capacity
[00:29:55.440 --> 00:29:57.640]   than all of the hyperscalers
[00:29:57.640 --> 00:30:00.680]   and cloud service providers combined.
[00:30:00.680 --> 00:30:03.960]   So probably about 50% of the inference compute in the world.
[00:30:03.960 --> 00:30:07.600]   - That's just great.
[00:30:07.600 --> 00:30:12.280]   Tell us about team building in Silicon Valley.
[00:30:12.280 --> 00:30:15.440]   How hard is it to get folks that are real AI folks
[00:30:15.440 --> 00:30:17.640]   in the backdrop of you could go work at Tesla,
[00:30:17.640 --> 00:30:19.560]   you could go work at Google, open AI,
[00:30:19.560 --> 00:30:21.200]   all these people we are hearing,
[00:30:21.200 --> 00:30:22.800]   multimillion dollar pay packages
[00:30:22.800 --> 00:30:25.280]   that rival playing professional sports.
[00:30:25.280 --> 00:30:28.360]   Like what is going on in finding the people?
[00:30:28.360 --> 00:30:30.240]   Now, by the way, you have this interesting thing
[00:30:30.240 --> 00:30:31.800]   'cause your initial chip,
[00:30:31.800 --> 00:30:34.160]   you were trying to find folks that knew Haskell.
[00:30:34.160 --> 00:30:37.320]   So just tell us, how hard is it to build a team
[00:30:37.320 --> 00:30:39.360]   in the Valley to do this?
[00:30:39.360 --> 00:30:41.120]   - Impossible.
[00:30:41.120 --> 00:30:43.120]   So if you want to know how to do it,
[00:30:43.120 --> 00:30:44.440]   you have to start getting creative,
[00:30:44.440 --> 00:30:46.040]   just like anything you want to do well.
[00:30:46.040 --> 00:30:47.800]   Don't just compete directly.
[00:30:47.800 --> 00:30:50.760]   But yeah, these pay packages are astronomical
[00:30:50.760 --> 00:30:54.920]   because everyone views this as a winner take all market.
[00:30:54.920 --> 00:30:56.080]   I mean, just that's it.
[00:30:56.080 --> 00:30:58.360]   It's not about, am I going to be number two?
[00:30:58.360 --> 00:30:59.360]   Am I going to be number three?
[00:30:59.360 --> 00:31:01.400]   They're all going, I got to be number one.
[00:31:01.400 --> 00:31:03.120]   So if you don't have the best talent, you're out.
[00:31:03.120 --> 00:31:04.800]   Now, here's the mistake.
[00:31:04.800 --> 00:31:09.760]   A lot of these AI researchers are amazing at AI,
[00:31:09.760 --> 00:31:11.760]   but they're still kind of green.
[00:31:11.760 --> 00:31:13.720]   They're new, they're young, right?
[00:31:13.720 --> 00:31:15.240]   This is a new field.
[00:31:15.240 --> 00:31:17.240]   And what I always recommend to people
[00:31:17.240 --> 00:31:22.080]   is go hire the best, most grizzled engineers
[00:31:22.080 --> 00:31:26.080]   who know how to ship stuff and on time
[00:31:26.080 --> 00:31:28.320]   and let them learn AI
[00:31:28.320 --> 00:31:30.800]   because they will be able to do that faster
[00:31:30.800 --> 00:31:32.920]   than you will be able to take the AI researchers
[00:31:32.920 --> 00:31:34.760]   and give them the 20 years of experience
[00:31:34.760 --> 00:31:36.480]   of deploying production code.
[00:31:36.480 --> 00:31:41.880]   - You were on stage in Saudi Arabia with Saudi Aramco
[00:31:41.880 --> 00:31:45.400]   a month ago and announced some big deal.
[00:31:45.400 --> 00:31:48.920]   Can you just, like, what is going on with deals like that?
[00:31:48.920 --> 00:31:51.560]   Like, where is that market going?
[00:31:51.560 --> 00:31:56.000]   Is that you competing with Amazon and Google and Microsoft?
[00:31:56.000 --> 00:31:57.320]   Is that what that is?
[00:31:57.320 --> 00:31:58.360]   - It's not competing.
[00:31:58.360 --> 00:32:01.080]   It's actually complimentary.
[00:32:01.080 --> 00:32:03.680]   The announcement was that we are going
[00:32:03.680 --> 00:32:06.320]   to be doing a deal together with Aramco Digital.
[00:32:06.320 --> 00:32:10.080]   And we haven't announced how large exactly,
[00:32:10.080 --> 00:32:14.240]   but it will be large in terms of the amount of compute
[00:32:14.240 --> 00:32:15.640]   that we're gonna deploy.
[00:32:15.640 --> 00:32:19.920]   And in total, we've done deals that get us to past 10%
[00:32:19.920 --> 00:32:22.440]   of that 1.5 million LPU goal.
[00:32:22.440 --> 00:32:26.280]   And of course, the hard part is the first deals.
[00:32:26.280 --> 00:32:27.800]   So once we announced that,
[00:32:27.800 --> 00:32:30.000]   a lot of other deals are now coming through.
[00:32:30.000 --> 00:32:33.120]   But the, yeah, go ahead.
[00:32:33.120 --> 00:32:36.000]   - So, no, no, I was just, I was just, second, Tim.
[00:32:36.000 --> 00:32:41.000]   - So the scale of these deals is that these are larger
[00:32:41.000 --> 00:32:46.640]   than the amount of compute that Meta has, right?
[00:32:46.640 --> 00:32:50.040]   And a lot of these tech companies right now,
[00:32:50.040 --> 00:32:51.680]   they think that they have such an advantage
[00:32:51.680 --> 00:32:54.000]   'cause they've locked up the supply.
[00:32:54.000 --> 00:32:55.640]   They don't want it to be true
[00:32:55.640 --> 00:32:58.560]   that there is another alternative out there.
[00:32:58.560 --> 00:33:00.960]   And so we're actually doing deals with folks
[00:33:00.960 --> 00:33:04.280]   where they're gonna have more compute than a hyperscaler.
[00:33:04.280 --> 00:33:07.080]   - Right, that's a crazy idea.
[00:33:07.080 --> 00:33:12.080]   Last question, everybody's worried about what AI means.
[00:33:12.080 --> 00:33:17.040]   You've been in it for a very long time.
[00:33:17.040 --> 00:33:22.040]   Just end with your perspectives on what we should be thinking
[00:33:22.240 --> 00:33:24.400]   and what your perspectives are on the future of AI,
[00:33:24.400 --> 00:33:26.480]   our future jobs, all of this typical stuff
[00:33:26.480 --> 00:33:27.680]   that people worry about.
[00:33:27.680 --> 00:33:31.640]   - So I get asked a lot, should we be afraid of AI?
[00:33:31.640 --> 00:33:36.640]   And my answer to that is, if you think back to Galileo,
[00:33:36.640 --> 00:33:38.520]   someone who got in a lot of trouble,
[00:33:38.520 --> 00:33:41.440]   the reason he got in trouble was he invented the telescope,
[00:33:41.440 --> 00:33:44.920]   popularized it, and made some claims
[00:33:44.920 --> 00:33:48.400]   that we were much smaller than everyone wanted to believe.
[00:33:48.400 --> 00:33:50.080]   We were supposed to be the center of the universe
[00:33:50.080 --> 00:33:52.440]   and it turns out we weren't.
[00:33:52.440 --> 00:33:53.920]   And the better the telescope got,
[00:33:53.920 --> 00:33:56.360]   the more obvious it became that we were small.
[00:33:56.360 --> 00:34:00.000]   And in a large sense, large language models
[00:34:00.000 --> 00:34:02.920]   are the telescope for the mind.
[00:34:02.920 --> 00:34:07.920]   It's become clear that intelligence is larger than we are.
[00:34:07.920 --> 00:34:12.520]   And it makes us feel really, really small.
[00:34:12.520 --> 00:34:14.240]   And it's scary.
[00:34:14.240 --> 00:34:16.720]   But what happened over time was as we realized
[00:34:16.720 --> 00:34:18.640]   the universe was larger than we thought,
[00:34:18.640 --> 00:34:20.640]   and we got used to that, we started to realize
[00:34:20.640 --> 00:34:24.840]   how beautiful it was, and our place in the universe.
[00:34:24.840 --> 00:34:26.240]   And I think that's what's gonna happen.
[00:34:26.240 --> 00:34:29.160]   We're gonna realize intelligence is more vast
[00:34:29.160 --> 00:34:30.840]   than we ever imagined,
[00:34:30.840 --> 00:34:32.800]   and we're gonna understand our place in it.
[00:34:32.800 --> 00:34:34.680]   And we're not gonna be afraid of it.
[00:34:34.680 --> 00:34:36.880]   - That's a beautiful way to end.
[00:34:36.880 --> 00:34:38.200]   Jonathan Ross, everybody.
[00:34:38.200 --> 00:34:39.040]   Thanks, guys.
[00:34:39.040 --> 00:34:42.200]   (audience applauding)
[00:34:48.240 --> 00:34:49.960]   - Thank you very, very much.
[00:34:49.960 --> 00:34:52.720]   I was told drug means to understand deeply with empathy.
[00:34:52.720 --> 00:34:54.920]   That was embodying this definition.
[00:34:54.920 --> 00:35:04.920]   [BLANK_AUDIO]

