
[00:00:00.000 --> 00:00:02.560]   Today I'm going to talk about just a couple of features
[00:00:02.560 --> 00:00:05.600]   that I've been really excited about.
[00:00:05.600 --> 00:00:09.600]   And yeah, I won't geek out too much,
[00:00:09.600 --> 00:00:11.160]   but hopefully just the right amount.
[00:00:11.160 --> 00:00:14.400]   So as most of you know, we provide Weights and Biases
[00:00:14.400 --> 00:00:16.000]   as a hosted web service.
[00:00:16.000 --> 00:00:18.920]   So we actually host our service on Google Cloud Platform,
[00:00:18.920 --> 00:00:21.960]   and that's where app.wmb.ai sits.
[00:00:21.960 --> 00:00:26.000]   But when we go out into the corporate world
[00:00:26.000 --> 00:00:29.880]   and talk to potential customers of our software,
[00:00:29.880 --> 00:00:31.380]   it's often a requirement that they're
[00:00:31.380 --> 00:00:34.920]   able to run their own server or their own version of local.
[00:00:34.920 --> 00:00:38.200]   So over the last year or two as we've built this company,
[00:00:38.200 --> 00:00:40.080]   we've tried a number of different approaches
[00:00:40.080 --> 00:00:41.600]   to providing our service local.
[00:00:41.600 --> 00:00:46.080]   And we recently relaunched WMB local,
[00:00:46.080 --> 00:00:48.720]   which is now a part of our Python library.
[00:00:48.720 --> 00:00:51.360]   So if you do a pip install WMB, you'll have this.
[00:00:51.360 --> 00:00:55.120]   And behind the scenes, all it's doing is it's running Docker,
[00:00:55.120 --> 00:00:57.280]   and it's going to run our Docker container,
[00:00:57.280 --> 00:01:01.280]   WMB/local, which contains our entire server
[00:01:01.280 --> 00:01:04.640]   and allows us to run a fully featured Weights and Biases
[00:01:04.640 --> 00:01:08.200]   service on our own machine in our own private network.
[00:01:08.200 --> 00:01:12.040]   So I'm going to go ahead and just do that live now.
[00:01:12.040 --> 00:01:16.600]   And I'll also sneak in a new feature
[00:01:16.600 --> 00:01:18.920]   that I'm excited about as well.
[00:01:18.920 --> 00:01:23.920]   So first, I'm in this directory, WMB demo.
[00:01:23.920 --> 00:01:28.640]   And I've got WMB installed, so I can just run WMB local.
[00:01:28.640 --> 00:01:30.480]   And I already have the Docker image pulled.
[00:01:30.480 --> 00:01:32.400]   If this were the first time I were running it,
[00:01:32.400 --> 00:01:36.400]   it would actually pull the latest image from Docker Hub.
[00:01:36.400 --> 00:01:38.640]   And now it actually launched me into the browser.
[00:01:38.640 --> 00:01:44.320]   And as this boots, we're going to see that we're actually
[00:01:44.320 --> 00:01:45.440]   loading the environment.
[00:01:45.440 --> 00:01:48.720]   So Weights and Biases local--
[00:01:48.720 --> 00:01:49.920]   and I'm seeing comments here.
[00:01:49.920 --> 00:01:52.280]   I will increase the text size.
[00:01:52.280 --> 00:01:56.520]   Weights and Biases local is actually
[00:01:56.520 --> 00:01:59.360]   running a number of services inside this Docker container.
[00:01:59.360 --> 00:02:03.200]   So we use MySQL to store metadata,
[00:02:03.200 --> 00:02:05.800]   and we use Cloud Storage to store all the artifacts
[00:02:05.800 --> 00:02:07.200]   associated with experiments.
[00:02:07.200 --> 00:02:09.880]   And we've actually launched our own instances
[00:02:09.880 --> 00:02:12.440]   of those services inside the Docker container.
[00:02:12.440 --> 00:02:18.320]   So the first time I launch local,
[00:02:18.320 --> 00:02:20.200]   it's going to ask me to create an account.
[00:02:20.200 --> 00:02:23.600]   So now the install can be secured.
[00:02:23.600 --> 00:02:25.600]   So I just created my account.
[00:02:25.600 --> 00:02:28.080]   And now it's giving me my API key.
[00:02:28.080 --> 00:02:32.760]   So I can go back to my terminal, and now it will log me
[00:02:32.760 --> 00:02:34.920]   into my local instance.
[00:02:34.920 --> 00:02:37.680]   To show you the actual local instance in action,
[00:02:37.680 --> 00:02:39.800]   I'm going to go ahead and, from this same directory,
[00:02:39.800 --> 00:02:42.640]   start a JupyterLab instance.
[00:02:42.640 --> 00:02:43.960]   I'll make this a little bigger.
[00:02:43.960 --> 00:02:47.600]   Or too big.
[00:02:47.600 --> 00:02:48.100]   OK.
[00:02:48.100 --> 00:02:55.520]   So if I were JupyterLab--
[00:02:55.520 --> 00:02:57.680]   JupyterLab, not JupyterLac.
[00:02:57.680 --> 00:02:58.180]   OK.
[00:02:58.180 --> 00:03:07.120]   Cool.
[00:03:07.120 --> 00:03:09.800]   So now I've got a demo notebook.
[00:03:09.800 --> 00:03:13.760]   And inside this notebook, I just have a very simple neural
[00:03:13.760 --> 00:03:14.240]   network.
[00:03:14.240 --> 00:03:15.880]   So I'm pulling in Fashion Hymnist.
[00:03:16.880 --> 00:03:20.200]   And I'm importing the WMB library.
[00:03:20.200 --> 00:03:24.680]   And I'm going to go ahead and load the data and normalize it.
[00:03:24.680 --> 00:03:29.280]   And then I've got a super simple convolutional neural net here
[00:03:29.280 --> 00:03:30.680]   that I'll compile.
[00:03:30.680 --> 00:03:33.080]   And then lastly, I'm going to init my WMB project
[00:03:33.080 --> 00:03:35.840]   and then use the Weights and Biases callback,
[00:03:35.840 --> 00:03:37.160]   specifying that I have images.
[00:03:37.160 --> 00:03:39.480]   So they'll automatically be logged with labels.
[00:03:39.480 --> 00:03:42.680]   And this should be good to go.
[00:03:42.680 --> 00:03:45.040]   So now you can see, because I actually installed local,
[00:03:45.040 --> 00:03:48.760]   my local system is configured to talk to local host instead
[00:03:48.760 --> 00:03:52.920]   of app.wmb.ai.
[00:03:52.920 --> 00:03:57.240]   And it's gone ahead and put it into my default entity, which
[00:03:57.240 --> 00:04:00.440]   is vanPelt, that username that I created on signup.
[00:04:00.440 --> 00:04:04.680]   And now if I click here, I can actually see the run
[00:04:04.680 --> 00:04:06.960]   before I need to step through my new features
[00:04:06.960 --> 00:04:09.000]   here and my brand new install of local.
[00:04:09.000 --> 00:04:12.000]   And now we can see the same Weights and Biases local
[00:04:12.000 --> 00:04:15.600]   experience that most of you have seen before.
[00:04:15.600 --> 00:04:18.080]   But now instead of running inside of our Google Cloud
[00:04:18.080 --> 00:04:23.480]   servers, we're actually running directly on my laptop.
[00:04:23.480 --> 00:04:26.280]   So we can see that we're logging some examples from our fashion
[00:04:26.280 --> 00:04:27.280]   data set here.
[00:04:27.280 --> 00:04:30.520]   And all of our loss metrics are updating in real time.
[00:04:30.520 --> 00:04:34.040]   And if I were to turn off my network right now,
[00:04:34.040 --> 00:04:36.240]   which I won't do because, of course, I would stop
[00:04:36.240 --> 00:04:37.880]   streaming to you all, all of this
[00:04:37.880 --> 00:04:40.400]   would continue to function because all communication is
[00:04:40.400 --> 00:04:45.240]   happening over local host just on this machine.
[00:04:45.240 --> 00:04:53.280]   Just for fun, we can actually see what's
[00:04:53.280 --> 00:04:55.680]   going on inside of local here.
[00:04:55.680 --> 00:04:57.920]   So if I list all of the processes that
[00:04:57.920 --> 00:05:00.460]   are running on my system, we could see I've got one container
[00:05:00.460 --> 00:05:03.800]   named wmb/local running on my system.
[00:05:03.800 --> 00:05:05.480]   And now I can actually shell into it.
[00:05:05.480 --> 00:05:11.080]   So I could say docker exec IT.
[00:05:11.080 --> 00:05:13.040]   Let's go into 719.
[00:05:13.040 --> 00:05:16.440]   And we'll run bash.
[00:05:16.440 --> 00:05:17.920]   So now I'm inside of local.
[00:05:17.920 --> 00:05:18.800]   And we can see that we're actually
[00:05:18.800 --> 00:05:20.200]   running a number of services.
[00:05:20.200 --> 00:05:22.040]   So Weights and Biases itself is actually
[00:05:22.040 --> 00:05:27.080]   running my SQL and then Minio for a cloud file store,
[00:05:27.080 --> 00:05:29.440]   as well as a number of our internal Go services
[00:05:29.440 --> 00:05:35.920]   that actually make all of the systems work together.
[00:05:35.920 --> 00:05:38.400]   So one of the new features I wanted to share
[00:05:38.400 --> 00:05:41.840]   was better version control inside of Jupyter.
[00:05:41.840 --> 00:05:43.200]   So we just released a new version
[00:05:43.200 --> 00:05:47.920]   of our client, version 0834 today,
[00:05:47.920 --> 00:05:56.200]   that has much better support for Jupyter versioning.
[00:05:56.200 --> 00:05:58.800]   So here, let's say I change my dense layer
[00:05:58.800 --> 00:06:07.320]   maybe to 64 and maybe reduce my convolution from 32 to 16.
[00:06:07.320 --> 00:06:10.840]   And then let's run for three epochs this time as well.
[00:06:10.840 --> 00:06:14.440]   Now, if I redefine my model and I re-init
[00:06:14.440 --> 00:06:23.280]   and call my callback again, I can go to my runs
[00:06:23.280 --> 00:06:25.320]   and see now that I have two runs, honest win one
[00:06:25.320 --> 00:06:26.800]   and resonance two.
[00:06:26.800 --> 00:06:29.040]   If I jump into honest win one, I can see the files
[00:06:29.040 --> 00:06:31.760]   that I've actually saved with this instance.
[00:06:31.760 --> 00:06:34.280]   So here we can see things like our config, which
[00:06:34.280 --> 00:06:35.840]   will contain the number of epochs
[00:06:35.840 --> 00:06:38.920]   as well as some metadata about my experiment.
[00:06:38.920 --> 00:06:42.920]   But if I go into the code, we'll see two new files here.
[00:06:42.920 --> 00:06:45.040]   So one is demo.ipy.
[00:06:45.040 --> 00:06:46.920]   And this is really a sneak peek feature, guys.
[00:06:46.920 --> 00:06:49.600]   So this currently isn't available in the cloud
[00:06:49.600 --> 00:06:52.480]   product, but we will be making it
[00:06:52.480 --> 00:06:54.200]   available in the next day or two.
[00:06:54.200 --> 00:06:55.960]   We now render Python notebooks.
[00:06:55.960 --> 00:06:59.400]   So we can see the exact Python notebook that was run.
[00:06:59.400 --> 00:07:02.480]   But we also create this session history Python notebook.
[00:07:02.480 --> 00:07:04.360]   So what's happened here is behind the scenes,
[00:07:04.360 --> 00:07:06.640]   we actually tracked exactly which cells
[00:07:06.640 --> 00:07:08.920]   were run in your current session.
[00:07:08.920 --> 00:07:12.320]   And we can see the exact code as it was run.
[00:07:12.320 --> 00:07:17.240]   So here this was run when I hadn't changed these values.
[00:07:17.240 --> 00:07:19.280]   So we can see epochs five and dense 10.
[00:07:19.280 --> 00:07:24.400]   If I go to my other run, which would be floral resonance,
[00:07:24.400 --> 00:07:27.600]   we'll see that this actually saved my entire history,
[00:07:27.600 --> 00:07:30.360]   including the cells that I ran when I actually
[00:07:30.360 --> 00:07:31.880]   changed the values.
[00:07:31.880 --> 00:07:33.560]   So now this can really be a great way
[00:07:33.560 --> 00:07:35.560]   to add a layer of version control
[00:07:35.560 --> 00:07:37.560]   on top of your notebook so that you can go back
[00:07:37.560 --> 00:07:40.320]   and see exactly what code you ran
[00:07:40.320 --> 00:07:43.560]   to produce any given experiment.
[00:07:43.560 --> 00:07:45.160]   Now, I haven't done this yet.
[00:07:45.160 --> 00:07:46.320]   So this will be trying live.
[00:07:46.320 --> 00:07:48.360]   And I'm worried this might break,
[00:07:48.360 --> 00:07:50.760]   but it wouldn't be a demo without potentially breaking.
[00:07:50.760 --> 00:07:53.240]   So we also have a code comparer panel.
[00:07:53.240 --> 00:07:57.400]   And notebooks behind the scenes are actually storing JSON.
[00:07:57.400 --> 00:08:01.680]   But for our cases, we actually extract all of the code
[00:08:01.680 --> 00:08:03.480]   from the notebook and then can show you
[00:08:03.480 --> 00:08:05.280]   the actual lines of code from the notebook
[00:08:05.280 --> 00:08:07.240]   so that you could see the difference between two
[00:08:07.240 --> 00:08:08.040]   given runs.
[00:08:08.040 --> 00:08:10.540]   And in this case, we're actually looking at the source code,
[00:08:10.540 --> 00:08:12.080]   which I hadn't saved.
[00:08:12.080 --> 00:08:13.880]   So we're not going to see a difference.
[00:08:13.880 --> 00:08:20.040]   If I had saved this, I think, and ran it again, we would.
[00:08:20.040 --> 00:08:25.040]   So with that, I am more than happy to take any questions
[00:08:25.040 --> 00:08:31.880]   folks have about WB Local or our new Jupyter Notebook support.
[00:08:31.880 --> 00:08:34.360]   And yeah, one thing to note, someone was asking about Colab.
[00:08:34.360 --> 00:08:35.400]   One of the issues with Colab is we
[00:08:35.400 --> 00:08:37.320]   couldn't get the source notebook itself
[00:08:37.320 --> 00:08:39.840]   because it's actually not stored on the Colab instance.
[00:08:39.840 --> 00:08:42.120]   But now we will be able to actually store the session
[00:08:42.120 --> 00:08:43.040]   history.
[00:08:43.040 --> 00:08:47.120]   So all of the cells that you run within a Colab context
[00:08:47.120 --> 00:08:50.120]   will be exposed in that session.ipyind.
[00:08:50.120 --> 00:08:53.040]   And you could use that for reproducibility or sharing
[00:08:53.040 --> 00:08:54.640]   down the road.
[00:08:54.640 --> 00:08:58.320]   So with that, I'll see you guys in the Slack room.
[00:08:58.320 --> 00:09:00.760]   And I'm really excited to--
[00:09:00.760 --> 00:09:02.840]   yeah, go ahead, Lavanya.
[00:09:02.840 --> 00:09:04.600]   I can ask you some of the questions
[00:09:04.600 --> 00:09:06.640]   they're asking in the Slack.
[00:09:06.640 --> 00:09:08.680]   Sure, let's do it.
[00:09:08.680 --> 00:09:09.520]   Cool.
[00:09:09.520 --> 00:09:11.640]   Someone asked, can the local instance
[00:09:11.640 --> 00:09:18.160]   be used completely offline after the container is pulled?
[00:09:18.160 --> 00:09:19.200]   Yeah, absolutely.
[00:09:19.200 --> 00:09:21.000]   So the only requirement is going to be
[00:09:21.000 --> 00:09:23.160]   wherever you're training your models,
[00:09:23.160 --> 00:09:25.640]   that needs to be able to talk to this container.
[00:09:25.640 --> 00:09:27.960]   So if you were running the container on the same machine,
[00:09:27.960 --> 00:09:29.800]   you could just talk to it over local host.
[00:09:29.800 --> 00:09:32.260]   But most of our customers run the container inside of, say,
[00:09:32.260 --> 00:09:35.520]   a Kubernetes cluster on Amazon ECS.
[00:09:35.520 --> 00:09:38.840]   But you could do that on a private VPC network
[00:09:38.840 --> 00:09:41.960]   so that you would never have any data go out on the internet.
[00:09:41.960 --> 00:09:45.600]   It could all stay on your own internal networks.
[00:09:45.600 --> 00:09:46.920]   Cool.
[00:09:46.920 --> 00:09:51.920]   Someone else asked, how are we different from TensorBoard?
[00:09:51.920 --> 00:09:55.480]   Or how is this different from TensorBoard?
[00:09:55.480 --> 00:09:56.920]   So there's a whole bunch of things
[00:09:56.920 --> 00:09:58.840]   that are different from TensorBoard.
[00:09:58.840 --> 00:10:00.180]   And I'll highlight just a couple.
[00:10:00.180 --> 00:10:03.760]   So one is beyond just the kind of metrics
[00:10:03.760 --> 00:10:05.600]   that you would normally see in TensorBoard,
[00:10:05.600 --> 00:10:07.760]   we're actually capturing system-level metrics
[00:10:07.760 --> 00:10:08.840]   as well behind the scenes.
[00:10:08.840 --> 00:10:12.160]   So you can see how much CPU or memory that you're using.
[00:10:12.160 --> 00:10:13.920]   We're also capturing a bunch of metadata
[00:10:13.920 --> 00:10:16.200]   about the code and the environment that you ran in.
[00:10:16.200 --> 00:10:20.040]   So for instance, I showed the actual source code here.
[00:10:20.040 --> 00:10:23.840]   But alongside any run, you can have arbitrary artifacts.
[00:10:23.840 --> 00:10:27.160]   Another example of some code that we track is requirements.txt,
[00:10:27.160 --> 00:10:29.560]   which is showing me exactly what Python libraries I
[00:10:29.560 --> 00:10:32.720]   had inside of this Python environment when this ran.
[00:10:32.720 --> 00:10:35.560]   So we have a number of features really around reproducibility
[00:10:35.560 --> 00:10:37.880]   that you wouldn't find in something like TensorBoard.
[00:10:37.880 --> 00:10:39.500]   And then when we really start to shine
[00:10:39.500 --> 00:10:41.580]   is after you've done a whole bunch of experiments.
[00:10:41.580 --> 00:10:44.120]   So we've heard from the majority of our customers
[00:10:44.120 --> 00:10:46.400]   that when using TensorBoard, if they have more than a few
[00:10:46.400 --> 00:10:48.960]   handfuls of experiments that they're trying to compare,
[00:10:48.960 --> 00:10:50.460]   that it tends to get really laggy
[00:10:50.460 --> 00:10:52.480]   and become difficult to use.
[00:10:52.480 --> 00:10:54.640]   And we've tried to make our interfaces really snappy
[00:10:54.640 --> 00:10:57.920]   when you have even thousands of experiments.
[00:10:57.920 --> 00:10:59.600]   And we've really made it so that you
[00:10:59.600 --> 00:11:01.600]   can collaborate with other people on your team,
[00:11:01.600 --> 00:11:03.000]   whereas TensorBoard is really just
[00:11:03.000 --> 00:11:05.680]   for an individual practitioner generally.
[00:11:05.680 --> 00:11:08.600]   The last thing I'd add is if folks really like TensorBoard,
[00:11:08.600 --> 00:11:10.520]   you can use it with Weights and Biases.
[00:11:10.520 --> 00:11:12.560]   And we'll actually run TensorBoard for you.
[00:11:12.560 --> 00:11:17.120]   And you would see that as a tab inside of an individual run.
[00:11:17.120 --> 00:11:18.240]   Thanks, Chris.
[00:11:18.240 --> 00:11:20.040]   There's a couple more questions.
[00:11:20.040 --> 00:11:23.680]   So someone asked, can I push the runs I do locally
[00:11:23.680 --> 00:11:24.680]   to Weights and Biases?
[00:11:24.680 --> 00:11:29.640]   Yeah, absolutely.
[00:11:29.640 --> 00:11:34.400]   So right now, there's no easy sync interface.
[00:11:34.400 --> 00:11:38.960]   But from our CLI itself, there is a WNB sync command.
[00:11:38.960 --> 00:11:42.000]   So you can always sync a local directory back up to our cloud
[00:11:42.000 --> 00:11:43.760]   or to another instance of local.
[00:11:43.760 --> 00:11:46.560]   In long term for our local offering,
[00:11:46.560 --> 00:11:50.200]   we envision having a way to just programmatically
[00:11:50.200 --> 00:11:51.800]   with a button inside of local say,
[00:11:51.800 --> 00:11:56.600]   maybe move this to the cloud version or vice versa.
[00:11:56.600 --> 00:11:59.840]   So that's something that we hope to come down the road.
[00:11:59.840 --> 00:12:02.360]   Thanks.
[00:12:02.360 --> 00:12:05.000]   Someone asked, does this not expose your IP
[00:12:05.000 --> 00:12:11.080]   to the outside world by releasing the Docker container?
[00:12:11.080 --> 00:12:14.760]   No, so the Docker container will just listen on the IP address
[00:12:14.760 --> 00:12:17.760]   of the host that you're running the Docker container on.
[00:12:17.760 --> 00:12:19.920]   So most of our customers would instead of say,
[00:12:19.920 --> 00:12:22.040]   go to local host, they'd create their own DNS that
[00:12:22.040 --> 00:12:24.880]   would point to that host.
[00:12:24.880 --> 00:12:26.880]   But if that host were on a private network that
[00:12:26.880 --> 00:12:29.680]   wasn't exposed to the internet, then that IP address would only
[00:12:29.680 --> 00:12:31.880]   be known to that network.
[00:12:31.880 --> 00:12:33.200]   Cool.
[00:12:33.200 --> 00:12:36.920]   And then this one last question, how does the callback
[00:12:36.920 --> 00:12:38.960]   change for PyTorch?
[00:12:38.960 --> 00:12:41.000]   Maybe we can share the code for that.
[00:12:41.000 --> 00:12:43.480]   I'll answer that in the Slack.
[00:12:43.480 --> 00:12:47.240]   Yeah, so check out our documentation here.
[00:12:47.240 --> 00:12:49.480]   And there are examples for PyTorch,
[00:12:49.480 --> 00:12:51.600]   or especially if you're using Lightning or Ignite,
[00:12:51.600 --> 00:12:53.040]   there's some nice callback examples
[00:12:53.040 --> 00:12:57.120]   that look a lot like that Keras callback.
[00:12:57.120 --> 00:13:00.440]   So the person who asked the question about the IP
[00:13:00.440 --> 00:13:04.920]   specified that by IP, he meant intellectual property.
[00:13:04.920 --> 00:13:06.520]   Oh, I see.
[00:13:06.520 --> 00:13:08.480]   Yeah, I mean, the whole point of local
[00:13:08.480 --> 00:13:10.840]   is to really isolate all of the data
[00:13:10.840 --> 00:13:12.120]   to be fully in your control.
[00:13:12.120 --> 00:13:18.000]   So no data would leak beyond your networks or infrastructure.
[00:13:18.000 --> 00:13:18.880]   Cool.
[00:13:18.880 --> 00:13:20.360]   Those are all the questions we have.
[00:13:20.360 --> 00:13:22.920]   Thank you, Chris, for stopping by.
[00:13:22.920 --> 00:13:25.560]   - You bet, thanks guys, looking forward to the talks.

