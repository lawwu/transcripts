
[00:00:00.000 --> 00:00:05.600]   Jeremy, thank you so much for being here.
[00:00:05.600 --> 00:00:09.760]   You're a long time friend of the company and we really appreciate you coming.
[00:00:09.760 --> 00:00:11.960]   Oh, it's always a pleasure.
[00:00:11.960 --> 00:00:18.080]   It's nice to get an upgrade to chatting to you after all these chats with Lucas, you
[00:00:18.080 --> 00:00:19.080]   know.
[00:00:19.080 --> 00:00:20.080]   He's watching.
[00:00:20.080 --> 00:00:23.080]   Oh, I didn't say that, Lucas.
[00:00:23.080 --> 00:00:24.080]   Not at all.
[00:00:24.080 --> 00:00:32.160]   So I want to talk about Fast.ai, I want to talk about MedArc, which is a new thing that
[00:00:32.160 --> 00:00:33.200]   you're working on.
[00:00:33.200 --> 00:00:37.880]   I want to talk about what learning, machine learning today in the world of large models
[00:00:37.880 --> 00:00:39.640]   and this and that looks like.
[00:00:39.640 --> 00:00:44.280]   And also like you've built one of the best machine learning communities out there.
[00:00:44.280 --> 00:00:45.960]   So I just want to touch on that.
[00:00:45.960 --> 00:00:47.680]   Does that sound like a good set of topics?
[00:00:47.680 --> 00:00:48.680]   That sounds great.
[00:00:48.680 --> 00:00:49.680]   Let's do it.
[00:00:49.680 --> 00:00:50.680]   Cool.
[00:00:50.680 --> 00:00:51.680]   So let's start with Fast.ai.
[00:00:51.680 --> 00:00:53.760]   I love your mission.
[00:00:53.760 --> 00:00:58.360]   Like you want to make deep learning accessible to everyone.
[00:00:58.360 --> 00:01:03.400]   And now there have been these breakthroughs with like stable diffusion, where more people
[00:01:03.400 --> 00:01:06.000]   are consuming models than training models.
[00:01:06.000 --> 00:01:08.520]   So does that influence your mission at all?
[00:01:08.520 --> 00:01:10.800]   And if so, how?
[00:01:10.800 --> 00:01:19.760]   Yeah, it's tough, you know, like the most flashy applications, generally speaking, are
[00:01:19.760 --> 00:01:25.720]   going to be the ones that use the most compute, because by definition, you do the same thing
[00:01:25.720 --> 00:01:29.080]   twice, one with more compute and more data and one with less, the one with more compute
[00:01:29.080 --> 00:01:35.520]   and more data is going to get better results.
[00:01:35.520 --> 00:01:41.680]   And that often, people often misunderstand that as thinking, oh, the research has to
[00:01:41.680 --> 00:01:45.520]   happen on lots of data and lots of compute, and I can't do good research otherwise.
[00:01:45.520 --> 00:01:48.120]   But that's not how it works at all.
[00:01:48.120 --> 00:01:52.880]   The research happens on much smaller data sets, on much smaller amounts of compute.
[00:01:52.880 --> 00:01:56.320]   The researchers get things working really well.
[00:01:56.320 --> 00:02:01.760]   And then the final step is the kind of engineering task of scaling that up once the research
[00:02:01.760 --> 00:02:05.120]   is basically done.
[00:02:05.120 --> 00:02:11.120]   And so it's unfortunate that the research work tends to get a lot less coverage than
[00:02:11.120 --> 00:02:13.260]   that engineering step, you know.
[00:02:13.260 --> 00:02:22.000]   So for example, my friend Waseem pointed out on Twitter yesterday that the actual research
[00:02:22.000 --> 00:02:27.320]   behind the CLIP model, that's the thing that allows us to type in a sentence and have it
[00:02:27.320 --> 00:02:31.760]   understood in a way that a picture can be then drawn of it.
[00:02:31.760 --> 00:02:36.640]   The actual research for that was done in the medical imaging community, and that research
[00:02:36.640 --> 00:02:45.020]   has thousands of times less citations than CLIP, which actually simplified it and scaled
[00:02:45.020 --> 00:02:50.000]   it up to more data and basically reported, oh here's what happens if we take this medical
[00:02:50.000 --> 00:02:53.320]   imaging insight and scale it up, you know.
[00:02:53.320 --> 00:03:00.160]   Or by the same kind of, I mean not as much, but somewhat, you know, my own ULM fit algorithm,
[00:03:00.160 --> 00:03:06.600]   which pointed out look how effective it is to fine-tune large language models, you know,
[00:03:06.600 --> 00:03:14.040]   and OpenAI greatly scaled it up and switched it from a LSTM to Transformers, and the result,
[00:03:14.040 --> 00:03:22.040]   which is GPT, got far more publicity than the original ULM fit and far more citations.
[00:03:22.040 --> 00:03:27.940]   So yeah, I mean that's just the nature of human interest, I guess.
[00:03:27.940 --> 00:03:33.640]   People are interested in the flashy applications and not necessarily the research work that
[00:03:33.640 --> 00:03:36.920]   goes in behind them.
[00:03:36.920 --> 00:03:42.400]   So how do we, I guess this is a big question, but how do we change the incentives of the
[00:03:42.400 --> 00:03:48.560]   research world to get that first CLIP paper to get as much attention or get people to
[00:03:48.560 --> 00:03:54.120]   at least study that before they study the engineering version of it?
[00:03:54.120 --> 00:03:55.120]   I don't know.
[00:03:55.120 --> 00:03:56.120]   I mean it's not a big problem.
[00:03:56.120 --> 00:04:03.120]   Like in the end, the research gets done and people get grants and whatever else, and we
[00:04:03.120 --> 00:04:07.040]   don't do it for media attention.
[00:04:07.040 --> 00:04:09.000]   Like it's just, it's human nature.
[00:04:09.000 --> 00:04:10.120]   I think it's fine.
[00:04:10.120 --> 00:04:18.560]   I think the main thing that we try to do at Fast.ai is to help people understand that
[00:04:18.560 --> 00:04:26.040]   to become effective researchers, they're not really limited by compute or data the vast
[00:04:26.040 --> 00:04:28.680]   majority of the time.
[00:04:28.680 --> 00:04:34.000]   That impression is an accident of how things are reported, but it's not really the truth
[00:04:34.000 --> 00:04:39.760]   of how the vast majority of effective research is done.
[00:04:39.760 --> 00:04:40.760]   That makes sense.
[00:04:40.760 --> 00:04:44.760]   And is it changing the things that you teach in the course itself?
[00:04:44.760 --> 00:04:50.600]   So like, are you focusing more on like how to deal with like distributed compute and
[00:04:50.600 --> 00:04:52.480]   like these engineering type tasks?
[00:04:52.480 --> 00:04:55.640]   Or are you still focusing on the research tasks of Fast.ai?
[00:04:55.640 --> 00:05:01.640]   Yeah, no, I don't focus on the engineering type tasks because the vast majority of people
[00:05:01.640 --> 00:05:03.920]   don't need it the vast majority of the time.
[00:05:03.920 --> 00:05:08.240]   You know, that they, again, they get a lot of attention.
[00:05:08.240 --> 00:05:10.820]   Engineers love using lots of machines.
[00:05:10.820 --> 00:05:15.440]   So engineers always want to like be doing distributed computing or whatever, but I don't
[00:05:15.440 --> 00:05:21.340]   think that's where the vast majority of kind of impact happens.
[00:05:21.340 --> 00:05:28.680]   Impact happens when you understand the opportunities and constraints of a problem space and successfully
[00:05:28.680 --> 00:05:35.680]   figure out how to use, you know, ML to do something in that space that's useful and
[00:05:35.680 --> 00:05:38.900]   wasn't done before.
[00:05:38.900 --> 00:05:45.800]   So certainly the things I teach in the course, yeah, they keep changing, but they change
[00:05:45.800 --> 00:05:56.200]   to focus on like what's the, you know, most effective techniques for training and using
[00:05:56.200 --> 00:05:58.600]   deep learning.
[00:05:58.600 --> 00:06:00.120]   And they actually haven't changed that much.
[00:06:00.120 --> 00:06:05.660]   Our main focus has always been and continues to be transfer learning and fine tuning, which
[00:06:05.660 --> 00:06:06.660]   actually it's the opposite.
[00:06:06.660 --> 00:06:09.880]   I feel like the rest of the world is coming around to our way of seeing things, which
[00:06:09.880 --> 00:06:15.720]   is with things like instruction fine tuning and RLHF and the stuff behind chat GPT.
[00:06:15.720 --> 00:06:21.200]   I think people are starting to realize finally like, oh, maybe we should be actually taking
[00:06:21.200 --> 00:06:26.400]   notice of fine tuned models more than we have been.
[00:06:26.400 --> 00:06:33.280]   What is the best way for ML engineers entering the field today to learn to interact with
[00:06:33.280 --> 00:06:34.800]   these large models?
[00:06:34.800 --> 00:06:36.600]   Is it like to learn to fine tune them?
[00:06:36.600 --> 00:06:42.680]   And if so, where would they learn about like what, one, how to do it and two, what's possible
[00:06:42.680 --> 00:06:45.000]   once they fine tune it?
[00:06:45.000 --> 00:06:47.960]   Yeah, that's basically what our course is about.
[00:06:47.960 --> 00:06:52.760]   So they would do the fast.ai course.
[00:06:52.760 --> 00:06:55.200]   You know, we attack it from two angles.
[00:06:55.200 --> 00:07:02.620]   The first is a kind of applications first angle of, yeah, how do you fine tune a vision
[00:07:02.620 --> 00:07:05.280]   or a language model for a task?
[00:07:05.280 --> 00:07:07.080]   How do you capture the data set for that?
[00:07:07.080 --> 00:07:11.680]   How do you deploy that as a web-based application?
[00:07:11.680 --> 00:07:12.760]   And so that's part one of the course.
[00:07:12.760 --> 00:07:16.800]   And then part two of the course is, okay, what if you want to be a researcher to be
[00:07:16.800 --> 00:07:18.720]   somebody who actually improves the underlying technologies?
[00:07:18.720 --> 00:07:22.840]   Well, in that case, you have to understand in detail all of the pieces.
[00:07:22.840 --> 00:07:30.280]   So we build all the pieces of stable diffusion back up from scratch, like literally just
[00:07:30.280 --> 00:07:33.960]   Python and its standard library, not even starting with NumPy.
[00:07:33.960 --> 00:07:35.360]   We built everything up from scratch.
[00:07:35.360 --> 00:07:41.960]   And so you end up recreating stable diffusion in its entirety, at which point you obviously
[00:07:41.960 --> 00:07:44.520]   now deeply understand it.
[00:07:44.520 --> 00:07:48.400]   You write your own machine learning framework from scratch.
[00:07:48.400 --> 00:07:51.960]   So you obviously end up understanding that.
[00:07:51.960 --> 00:07:58.640]   And then that, I think that's helpful, you know, when you want to go beyond, you know,
[00:07:58.640 --> 00:08:06.120]   using well-trodden applications of fine tuning in well-understood ways to doing things that
[00:08:06.120 --> 00:08:10.080]   maybe haven't been done before or doing it in new ways.
[00:08:10.080 --> 00:08:12.080]   You've got the knowledge you need to do that.
[00:08:12.080 --> 00:08:17.720]   You know, I've always, I took your course now years ago, but like I've always found
[00:08:17.720 --> 00:08:22.600]   it so fascinating that you're, you cut through all this theory and you're like, here, I'll
[00:08:22.600 --> 00:08:26.200]   give you the minimal amount and you can start playing with the model and you know, you change
[00:08:26.200 --> 00:08:30.880]   some hyperparameters and you can viscerally see how the model changes.
[00:08:30.880 --> 00:08:32.360]   And I just love that.
[00:08:32.360 --> 00:08:36.800]   How did you come, but not everyone was like teaching machine learning like that.
[00:08:36.800 --> 00:08:42.240]   What about your experience in your life made you decide like that's how I'm going to teach?
[00:08:42.240 --> 00:08:49.600]   I think it's, you know, I, maybe it's just, I'm not smart enough to handle these super
[00:08:49.600 --> 00:08:56.860]   abstract bottom-up courses that they've never worked for me, you know.
[00:08:56.860 --> 00:09:02.400]   And furthermore, I started trying to do them in undergrad at university.
[00:09:02.400 --> 00:09:09.720]   I wanted to do physics, but I just, I just, they didn't work for me, you know, start with
[00:09:09.720 --> 00:09:15.720]   like all this, like, it's a bunch of definitions and lemmas and whatever, and nobody ever says
[00:09:15.720 --> 00:09:17.080]   what it's for.
[00:09:17.080 --> 00:09:19.040]   And it just didn't.
[00:09:19.040 --> 00:09:25.960]   Yeah, I kept on, I had so many questions, but the answers are always like, oh, you know,
[00:09:25.960 --> 00:09:30.640]   by the time you start doing your, you know, postdoc or whatever, you'll know how to do
[00:09:30.640 --> 00:09:33.120]   these things and just like, yeah.
[00:09:33.120 --> 00:09:40.280]   So I guess I figured, yeah, I can't be the only person who doesn't find that the best
[00:09:40.280 --> 00:09:42.800]   way to learn stuff.
[00:09:42.800 --> 00:09:46.560]   And I did some research and actually it turns out in the, you know, the educational world,
[00:09:46.560 --> 00:09:50.760]   it's pretty well known now, actually most people don't learn particularly well that
[00:09:50.760 --> 00:09:52.800]   way.
[00:09:52.800 --> 00:09:58.960]   The people who do have a monopoly on our academic system, because everybody in academia are
[00:09:58.960 --> 00:10:03.440]   by definition people who succeeded in going through the traditional approach.
[00:10:03.440 --> 00:10:06.760]   And so they have this huge filter bubble of being surrounded by people who all went through
[00:10:06.760 --> 00:10:08.720]   that traditional approach.
[00:10:08.720 --> 00:10:12.200]   So I guess, yeah, I kind of built the thing I wanted.
[00:10:12.200 --> 00:10:18.000]   And also I, I wanted to understand everything extremely deeply.
[00:10:18.000 --> 00:10:23.280]   And I felt like the only way to do that was to pick apart everything and try changing
[00:10:23.280 --> 00:10:24.280]   it.
[00:10:24.280 --> 00:10:28.780]   And as I started doing that, I realized a lot of the common beliefs about what works
[00:10:28.780 --> 00:10:32.880]   and why and what doesn't work and why were, were faulty.
[00:10:32.880 --> 00:10:37.800]   So it kind of turns out to be a great way to do R&D as well.
[00:10:37.800 --> 00:10:41.720]   And I kind of end up doing it with this community of students, because then we're all kind of
[00:10:41.720 --> 00:10:45.360]   like pulling things apart and asking each other questions.
[00:10:45.360 --> 00:10:50.000]   We're like, well, this paper says this, but if I tried this, this thing happened, why
[00:10:50.000 --> 00:10:51.000]   would that be?
[00:10:51.000 --> 00:10:54.960]   And then, you know, we end up kind of talking to the authors of the paper and they say,
[00:10:54.960 --> 00:10:58.200]   oh, we never got around to trying that or whatever.
[00:10:58.200 --> 00:11:04.080]   So it's been like a great journey that we've been on as a community for quite a few years
[00:11:04.080 --> 00:11:05.080]   now.
[00:11:05.080 --> 00:11:06.080]   Yeah.
[00:11:06.080 --> 00:11:09.000]   Where did you find this first few students?
[00:11:09.000 --> 00:11:13.840]   And like, what was that moment like, you know, when it was you and your wife, I think you
[00:11:13.840 --> 00:11:15.600]   guys launched this course.
[00:11:15.600 --> 00:11:19.880]   And how did it go from there to, as someone who's very interested in growth, I just want
[00:11:19.880 --> 00:11:21.280]   to know what that journey was like.
[00:11:21.280 --> 00:11:25.920]   I mean, it's like, thank God, my wife, Rachel, was interested as well.
[00:11:25.920 --> 00:11:34.920]   You know, like it started, I guess, I mean, I'd done stuff with neural nets back in my
[00:11:34.920 --> 00:11:38.600]   early 20s, you know, but it had been decades.
[00:11:38.600 --> 00:11:45.200]   I kind of put them aside for a long time and used other approaches, feeling like at some
[00:11:45.200 --> 00:11:49.960]   point they'll, they'll be back and then I'll want to dive in.
[00:11:49.960 --> 00:11:56.040]   And Rachel didn't, you know, Rachel's background was more of a pure math, you know, math PhD
[00:11:56.040 --> 00:11:57.040]   person.
[00:11:57.040 --> 00:11:59.000]   So she didn't have the background in neural nets that I did.
[00:11:59.000 --> 00:12:08.680]   So we both decided together to, in kind of from 2013-ish, to really understand what modern
[00:12:08.680 --> 00:12:14.040]   neural nets looked like as at that time.
[00:12:14.040 --> 00:12:21.520]   And yeah, as we tried to understand that together, we found, you know, we had so many questions,
[00:12:21.520 --> 00:12:23.120]   so many things that didn't seem very clear.
[00:12:23.120 --> 00:12:26.920]   We asked the kind of authors of papers about things and they would say like, oh, that's
[00:12:26.920 --> 00:12:32.000]   like, you know, unpublished details that aren't available.
[00:12:32.000 --> 00:12:38.160]   And we just thought, okay, yeah, there's no way normal people can reproduce any of the
[00:12:38.160 --> 00:12:41.400]   results in these papers.
[00:12:41.400 --> 00:12:47.320]   So yeah, so we, so once we got to a point where we felt like, okay, well we can understand
[00:12:47.320 --> 00:12:51.680]   some results in some papers, reproduce some results in some papers now, and maybe even
[00:12:51.680 --> 00:12:55.800]   go further, we thought we'll teach people what we've learned.
[00:12:55.800 --> 00:13:01.280]   That was basically lesson one, you know, of part one, the first time around.
[00:13:01.280 --> 00:13:10.320]   And I'm trying to think, I, you know, I guess we, you know, we just created a website called
[00:13:10.320 --> 00:13:14.960]   fast.ai and it's actually just a blog, and we put up a blog post and said, we're going
[00:13:14.960 --> 00:13:17.400]   to teach this.
[00:13:17.400 --> 00:13:24.200]   And I think just there was a lot of interest in learning how to do that.
[00:13:24.200 --> 00:13:27.360]   So a lot of people joined that course.
[00:13:27.360 --> 00:13:31.800]   I think also, you know, we did it with the University of San Francisco, so the master's
[00:13:31.800 --> 00:13:38.080]   students were given the option to join for free.
[00:13:38.080 --> 00:13:43.200]   And so, I don't know, maybe half of them or something did.
[00:13:43.200 --> 00:13:50.400]   And yeah, so we had, I'm trying to remember, maybe a hundred or so students at first.
[00:13:50.400 --> 00:13:57.200]   And yeah, it's terrifying building something the first time, it hasn't been built before,
[00:13:57.200 --> 00:13:58.560]   we don't know if it's going to work.
[00:13:58.560 --> 00:14:02.240]   You know, we had no idea if any student would get anything out of it, if it would be a total
[00:14:02.240 --> 00:14:05.520]   failure, I didn't know if anybody would turn up.
[00:14:05.520 --> 00:14:08.120]   But the students, yeah, they enjoyed it.
[00:14:08.120 --> 00:14:09.440]   They said they enjoyed it.
[00:14:09.440 --> 00:14:17.400]   They would, they shared their projects on the forum, we created a forum for it, and
[00:14:17.400 --> 00:14:18.840]   the projects were impressive, you know.
[00:14:18.840 --> 00:14:24.840]   So I guess when we then said, launched the recordings of that course as our first MOOC,
[00:14:24.840 --> 00:14:28.440]   we were able to point at those and say, look, here's some actual students, here's their
[00:14:28.440 --> 00:14:33.640]   backgrounds, here's who they are, here's pictures of them, here's the projects they made.
[00:14:33.640 --> 00:14:37.960]   And I think a lot of people kind of said like, oh, that person's not that different to me,
[00:14:37.960 --> 00:14:40.120]   you know, maybe if they can do that, I can do that too.
[00:14:40.120 --> 00:14:42.480]   So they gave it a go.
[00:14:42.480 --> 00:14:47.920]   And so yeah, you know, even that very first course was actually really popular.
[00:14:47.920 --> 00:14:50.920]   And it was, it was the only one of its kind.
[00:14:50.920 --> 00:14:57.960]   So if you wanted to learn deep learning with no previous background in the field, not like
[00:14:57.960 --> 00:15:02.920]   you had a lot of options, you know, there was Andrej Karpathy's fantastic Stanford class
[00:15:02.920 --> 00:15:09.000]   was online, which was very good, but it was, didn't cover everything, and had a bit of
[00:15:09.000 --> 00:15:10.480]   a different focus.
[00:15:10.480 --> 00:15:18.320]   Yours was the only programming focus, and like you get to play with the neural networks
[00:15:18.320 --> 00:15:20.120]   within the first half an hour course.
[00:15:20.120 --> 00:15:21.120]   Yeah, yeah.
[00:15:21.120 --> 00:15:25.760]   And it was in Python, not in Octave or Lua, or you know, the other languages that like
[00:15:25.760 --> 00:15:29.280]   Andrew Ng's course used, or Andrej's course used, or whatever.
[00:15:29.280 --> 00:15:30.280]   Yeah.
[00:15:30.280 --> 00:15:32.800]   So that, I think that helped a lot too.
[00:15:32.800 --> 00:15:38.280]   And also using a higher level framework, we use Keras on top of Theano, which made it,
[00:15:38.280 --> 00:15:41.840]   you know, we could really, there's this thing called cognitive load theory in education,
[00:15:41.840 --> 00:15:47.640]   which is the idea that you've only got a certain amount of mental capacity to think about stuff.
[00:15:47.640 --> 00:15:53.480]   And if you're thinking about, you know, a whole bunch of ceremony to get to a point
[00:15:53.480 --> 00:15:57.640]   where something's vaguely working, you don't have any cognitive capacity left to think
[00:15:57.640 --> 00:16:00.680]   about like, what's the main thing in the lesson.
[00:16:00.680 --> 00:16:07.400]   So like by using Keras, it allowed people to use less of their cognitive capacity to
[00:16:07.400 --> 00:16:12.000]   like build the basic model and do the training loop, and therefore they had more cognitive
[00:16:12.000 --> 00:16:15.040]   capacity to understand the lesson.
[00:16:15.040 --> 00:16:20.600]   And so, you know, that's always how we've done things, is really trying to focus on,
[00:16:20.600 --> 00:16:27.200]   yeah, leveraging higher level frameworks at appropriate times so that your brain can focus
[00:16:27.200 --> 00:16:30.040]   on the bit that we're actually trying to teach you right now.
[00:16:30.040 --> 00:16:31.040]   Yeah.
[00:16:31.040 --> 00:16:32.040]   Absolutely love that.
[00:16:32.040 --> 00:16:37.320]   I've always taught myself like programming and everything else that I've learned in my
[00:16:37.320 --> 00:16:38.320]   life.
[00:16:38.320 --> 00:16:42.080]   I don't like traditional schools and like your approach makes so much more sense.
[00:16:42.080 --> 00:16:45.640]   I wish every subject was taught like that.
[00:16:45.640 --> 00:16:46.640]   Yeah, me too.
[00:16:46.640 --> 00:16:50.760]   And now that I'm homeschooling my daughter, that's something I deal with every day, you
[00:16:50.760 --> 00:16:58.760]   know, as we're always trying to find ways to teach things in context and project based
[00:16:58.760 --> 00:17:07.120]   and you know, like for math, for example, I have a user calculator as much as possible,
[00:17:07.120 --> 00:17:10.200]   which is the exact opposite of how primary school teaching is normally done.
[00:17:10.200 --> 00:17:13.520]   It's normally like do everything in your head or pieces of paper.
[00:17:13.520 --> 00:17:17.480]   Yeah, I feel like a lot of education is upside down.
[00:17:17.480 --> 00:17:18.480]   Yeah.
[00:17:18.480 --> 00:17:24.920]   Maybe that can be your next startup after all of this fixing schools.
[00:17:24.920 --> 00:17:29.680]   You remind me a lot of like, I feel like anytime you read about how Feynman thinks about things,
[00:17:29.680 --> 00:17:33.520]   he was also like, let's play with it and let's learn about theory later.
[00:17:33.520 --> 00:17:34.960]   So it's really nice.
[00:17:34.960 --> 00:17:38.480]   Yeah, no, he's he's very inspiring role model in a lot of ways.
[00:17:38.480 --> 00:17:46.480]   His work on, you know, quantum spin and stuff that he says maybe somewhat tongue in cheek
[00:17:46.480 --> 00:17:49.960]   was based on his study of how dinner plates spin.
[00:17:49.960 --> 00:17:52.960]   You know, it's nice.
[00:17:52.960 --> 00:17:57.920]   Yeah, it makes it approachable and like anyone can do it and it makes it more fun and less
[00:17:57.920 --> 00:17:59.920]   the serious thing.
[00:17:59.920 --> 00:18:00.920]   Yeah.
[00:18:00.920 --> 00:18:05.720]   Also, like his way of talking about reading papers was always like saying, OK, well, I
[00:18:05.720 --> 00:18:10.760]   would focus 100% of my time on finding a physical analogy for the thing being described, no
[00:18:10.760 --> 00:18:11.760]   matter how abstract.
[00:18:11.760 --> 00:18:14.800]   And then I would spend all of my time thinking about the physical analogy.
[00:18:14.800 --> 00:18:19.480]   And he said in his autobiography that the authors of these papers would often be very
[00:18:19.480 --> 00:18:24.080]   surprised at how quickly they had developed an understanding of the paper and even seen
[00:18:24.080 --> 00:18:25.080]   holes in it.
[00:18:25.080 --> 00:18:30.440]   And he always felt like it's not that he was particularly smart.
[00:18:30.440 --> 00:18:36.360]   It's just that he used the brain's natural intuitions by putting things into a framework
[00:18:36.360 --> 00:18:40.560]   that allowed those intuitions to be used.
[00:18:40.560 --> 00:18:42.240]   Do you do that with models?
[00:18:42.240 --> 00:18:43.720]   Yeah, all the time.
[00:18:43.720 --> 00:18:49.600]   You know, like, you know, when I'm thinking about training, I'm always imagining a little
[00:18:49.600 --> 00:18:55.520]   ball falling down a, you know, bumpy surface and thinking about how to smooth it out.
[00:18:55.520 --> 00:19:03.400]   And yeah, I'm always using analogies, both when I'm trying to teach and when I'm trying
[00:19:03.400 --> 00:19:08.680]   to understand and trying to try things.
[00:19:08.680 --> 00:19:10.960]   Do you apply this to your real life?
[00:19:10.960 --> 00:19:17.280]   I mean, the rest of your life also, like this quick feedback and of like, try things like,
[00:19:17.280 --> 00:19:23.040]   you know, because I feel like this is something I try to teach my team, because growth is
[00:19:23.040 --> 00:19:24.040]   like that too.
[00:19:24.040 --> 00:19:25.040]   You don't have answers.
[00:19:25.040 --> 00:19:26.040]   It's like, while you're training, you try something.
[00:19:26.040 --> 00:19:27.040]   Right.
[00:19:27.040 --> 00:19:28.040]   Yeah, yeah.
[00:19:28.040 --> 00:19:29.480]   No, I mean, exactly.
[00:19:29.480 --> 00:19:35.560]   So back when I ran an email company called Fastmail, it was really nice because on the
[00:19:35.560 --> 00:19:39.880]   growth side of things, it was all about that.
[00:19:39.880 --> 00:19:48.640]   You know, in fact, as far as I know, we might have been the first company to do real A/B
[00:19:48.640 --> 00:19:49.880]   tests on the Internet.
[00:19:49.880 --> 00:19:56.080]   So we created 10 different versions of our homepage and put a different cookie on each
[00:19:56.080 --> 00:19:57.080]   one.
[00:19:57.080 --> 00:20:01.120]   And, you know, I know this stuff's all very normal now, but nobody had done it at that
[00:20:01.120 --> 00:20:02.120]   point as far as I knew.
[00:20:02.120 --> 00:20:05.200]   And we tracked conversion.
[00:20:05.200 --> 00:20:09.640]   And yeah, it's like nice because you get that, like, by the next day, you can see.
[00:20:09.640 --> 00:20:15.640]   And in that case, it turned out it didn't really matter much at all, which was interesting.
[00:20:15.640 --> 00:20:20.120]   It told us like, OK, don't spend ages fine tuning our homepage, because for the particular
[00:20:20.120 --> 00:20:23.520]   people we were reaching, it doesn't seem to make much difference.
[00:20:23.520 --> 00:20:25.280]   And, you know, I did similar things.
[00:20:25.280 --> 00:20:27.000]   Again, it's like pretty unheard of.
[00:20:27.000 --> 00:20:33.480]   But those days, Google ads had recently come out, you know, maybe they'd just come out.
[00:20:33.480 --> 00:20:34.480]   I can't remember.
[00:20:34.480 --> 00:20:35.480]   And we're like, oh, let's try some.
[00:20:35.480 --> 00:20:41.360]   So we tried lots and different URLs and tracked them.
[00:20:41.360 --> 00:20:50.720]   And yeah, I enjoyed that about running a consumer startup, being able to get instant feedback
[00:20:50.720 --> 00:20:52.880]   and try things each day.
[00:20:52.880 --> 00:20:57.880]   Do you also do that in your personal life or like when you're teaching your daughter?
[00:20:57.880 --> 00:21:02.000]   Do you have examples?
[00:21:02.000 --> 00:21:07.040]   I mean, yeah, it happens very naturally when you're teaching a family member because they'll
[00:21:07.040 --> 00:21:12.200]   look at you and they'll go, I'm bored, you know, or they'll, you know, lie down and close
[00:21:12.200 --> 00:21:13.960]   their eyes or whatever.
[00:21:13.960 --> 00:21:19.200]   So, you know, that's, you're like, OK.
[00:21:19.200 --> 00:21:21.960]   And I don't, I guess some people might try to push through.
[00:21:21.960 --> 00:21:23.000]   I definitely don't.
[00:21:23.000 --> 00:21:30.120]   You know, my approach is like, OK, what I'm trying to teach here is boring.
[00:21:30.120 --> 00:21:33.800]   And so the problem is not I need to make my daughter listen more.
[00:21:33.800 --> 00:21:36.500]   My problem is I need to stop being boring.
[00:21:36.500 --> 00:21:41.960]   So yeah, I just throw it away and we'll do something else totally different.
[00:21:41.960 --> 00:21:42.960]   Yeah.
[00:21:42.960 --> 00:21:47.680]   That's good parenting advice.
[00:21:47.680 --> 00:21:51.600]   I'm wondering how you measure the success of Fast.ai.
[00:21:51.600 --> 00:21:54.280]   Like, is it like the number of users?
[00:21:54.280 --> 00:21:56.480]   Is it like the intrinsic mission?
[00:21:56.480 --> 00:21:57.480]   Yeah.
[00:21:58.120 --> 00:22:03.400]   So yeah, so I've, I've never been a fan of metrics, you know, back when I did management
[00:22:03.400 --> 00:22:09.640]   consulting, you know, started 30 years ago.
[00:22:09.640 --> 00:22:17.440]   Particularly when I joined AT Kearney, you know, they were very focused on metrics, you
[00:22:17.440 --> 00:22:21.760]   know, around marketing, around everything.
[00:22:21.760 --> 00:22:29.840]   And yeah, we really noticed though that companies, that, that employees and managers managed
[00:22:29.840 --> 00:22:37.120]   to the metrics and, you know, it really seemed to work out very well.
[00:22:37.120 --> 00:22:47.700]   So you know, my approach with Fast.ai is like, it's very, it's very much based on like talking
[00:22:47.700 --> 00:22:51.880]   to people, you know, and understanding, like, and trying to form good relationships with
[00:22:51.880 --> 00:22:57.320]   people and knowing that there are people who are the kind of people who will say to me,
[00:22:57.320 --> 00:23:03.960]   like, you know what, Jeremy, that, you know, that lesson didn't really resonate with me.
[00:23:03.960 --> 00:23:10.920]   Or you know what, this, this, this API in this library, it's pretty complicated, you
[00:23:10.920 --> 00:23:12.240]   know, or whatever.
[00:23:12.240 --> 00:23:18.940]   So and also, you know, to talk to people and hear, hear stories of like, okay, yeah, we
[00:23:18.940 --> 00:23:22.420]   tried using your library and it worked really well.
[00:23:22.420 --> 00:23:27.700]   And now we've built this product on top of it, or we've tried using your library and
[00:23:27.700 --> 00:23:29.260]   we couldn't understand how to do this.
[00:23:29.260 --> 00:23:34.540]   And we totally gave up and we ended up writing something from scratch, you know, that kind
[00:23:34.540 --> 00:23:42.540]   of thing is the most helpful way I have of understanding what's working and what's not.
[00:23:42.540 --> 00:23:49.540]   I'm curious, what drives you as a person?
[00:23:49.540 --> 00:23:52.180]   Yeah, I don't know.
[00:23:52.180 --> 00:23:54.220]   I sometimes wonder that myself.
[00:23:54.220 --> 00:23:56.020]   Yeah, yeah.
[00:23:56.020 --> 00:24:04.820]   Yeah, yeah, I mean, I mean, basically, in terms of like, what I enjoy, for whatever
[00:24:04.820 --> 00:24:13.020]   reason, I get a huge amount of pleasure from learning new things, you know, of any kind.
[00:24:13.020 --> 00:24:19.180]   So like, I really enjoy like, kind of adventure sports type things, like, you know, like,
[00:24:19.180 --> 00:24:23.980]   I really enjoyed like, learning to ride a motorbike extremely fast around a racetrack,
[00:24:23.980 --> 00:24:31.820]   for example, like, it's, it's looks really difficult, but when you hook everything up
[00:24:31.820 --> 00:24:39.100]   just nicely, you like, get this feeling of, you know, this beautiful Zen feeling of everything's
[00:24:39.100 --> 00:24:48.380]   flying past you, and you've got it under control, you know, you know, with like, you know, skiing
[00:24:48.380 --> 00:24:55.380]   in France, you know, down a double black diamond run, and that's, you know, you remember not
[00:24:55.380 --> 00:24:57.740]   long ago, you're thinking like, I could never do that.
[00:24:57.740 --> 00:25:02.140]   And now you're just thinking like, everything's just hooking up, you know, and now I don't
[00:25:02.140 --> 00:25:06.580]   understand, I don't know why I could couldn't do it, because now it's like, straightforward,
[00:25:06.580 --> 00:25:16.100]   you know, so like, yeah, that's what I enjoy, you know, is, is, is taking difficult things
[00:25:16.100 --> 00:25:21.820]   that feel difficult, and getting to a point where they don't feel difficult, and I think
[00:25:21.820 --> 00:25:24.380]   teaching helps a lot.
[00:25:24.380 --> 00:25:29.940]   Like often I find I can't understand something, and still I, till I try to explain it to somebody
[00:25:29.940 --> 00:25:33.660]   else, and then I suddenly realize, okay, I guess I do understand it.
[00:25:33.660 --> 00:25:42.340]   So yeah, I think that's something that really drives me, and doing it with other like-minded
[00:25:42.340 --> 00:25:48.740]   people also particularly drives me, so I kind of have a, you know, a couple of times a week
[00:25:48.740 --> 00:25:54.500]   regular chat with some friends now, where we like, kind of read and implement papers
[00:25:54.500 --> 00:26:00.940]   and stuff like that together, and yeah, definitely get a lot of enjoyment of doing that, and
[00:26:00.940 --> 00:26:05.980]   you know, even when we're not chatting, we've got our little discord group that we're, you
[00:26:05.980 --> 00:26:11.180]   know, bouncing ideas around, and yeah, I definitely like that.
[00:26:11.180 --> 00:26:16.060]   And how does that translate into the projects that you choose, for example, like MedArc,
[00:26:16.060 --> 00:26:21.060]   you know, this new research work that you have, like, is it because you didn't know,
[00:26:21.060 --> 00:26:25.620]   I mean, anything about the medical industry, and you're like, oh, this sounds like a hard
[00:26:25.620 --> 00:26:28.820]   problem, let me figure it out, or how does it happen?
[00:26:28.820 --> 00:26:36.220]   Yeah, so MedArc is a new project that actually a guy called Tanishq Abraham launched.
[00:26:36.220 --> 00:26:41.660]   Tanishq is a 19 year old, about to be a PhD graduate.
[00:26:41.660 --> 00:26:44.020]   Insane, that guy.
[00:26:44.020 --> 00:26:48.740]   And his area of expertise is using deep learning for microscopy.
[00:26:48.740 --> 00:26:58.340]   He's a fast.ai alum, and now key part of the fast.ai community.
[00:26:58.340 --> 00:27:07.180]   And so I kind of, yeah, told him I would like to be a part of it, basically, and he said
[00:27:07.180 --> 00:27:13.740]   yes, which is nice, and it's, so in this case, this is an area I'm very familiar with, because
[00:27:13.740 --> 00:27:18.700]   I started the first medical deep learning company years ago, that was called Analytic.
[00:27:18.700 --> 00:27:25.300]   Now at that time, that was definitely something I knew nothing about, but it, in that, in
[00:27:25.300 --> 00:27:35.020]   that case, in this case, it was more about like, I don't know, feeling like there's this
[00:27:35.020 --> 00:27:40.180]   huge gap in the amount, you know, in the amount of medical experts available in the world,
[00:27:40.180 --> 00:27:47.820]   and it seems like a gap that's not, like, can be fixed by helping the existing medical
[00:27:47.820 --> 00:27:52.220]   experts become much more productive by leveraging AI.
[00:27:52.220 --> 00:27:58.060]   And so it's more, this is more like, I feel like it's something I ought to be doing.
[00:27:58.060 --> 00:28:06.020]   So you know, I had a similar thing early in the COVID crisis, when I discovered, much
[00:28:06.020 --> 00:28:11.540]   to my surprise, that basically all the science I could find, and all the scientists I could
[00:28:11.540 --> 00:28:19.060]   speak to, who were experts on, like, aerosol transmission of pandemics and whatnot, all
[00:28:19.060 --> 00:28:23.780]   thought that masks would be really effective, and I did a whole bunch of research, and yeah,
[00:28:23.780 --> 00:28:27.220]   discovered to my surprise that that seemed to be true.
[00:28:27.220 --> 00:28:38.540]   It's like, okay, I feel like I should spend time studying and teaching people about this,
[00:28:38.540 --> 00:28:43.300]   because then it seems like nobody else is going to, if it's not me.
[00:28:43.300 --> 00:28:49.900]   So yeah, I definitely, I'm kind of driven partly, it's quite strongly by, you know,
[00:28:49.900 --> 00:28:55.820]   if there's some kind of service I feel that I ought to be performing, then I do that,
[00:28:55.820 --> 00:29:00.820]   even though, like, particularly in the masks case, I didn't enjoy it at all.
[00:29:00.820 --> 00:29:06.220]   I did it, you know, I didn't make any money out of it, it's just entirely, I thought,
[00:29:06.220 --> 00:29:08.260]   this is something I should be doing.
[00:29:08.260 --> 00:29:13.140]   In the MedArt case, it's something I enjoy a lot, and particularly I, like I say, I like
[00:29:13.140 --> 00:29:18.180]   working with like-minded people, so the opportunity to work with Tanishq is a big part of that.
[00:29:18.180 --> 00:29:22.140]   I want to ask a spicy question, and then I'll come back to MedArt.
[00:29:22.140 --> 00:29:31.340]   I feel like I ask myself this often, too, like, would you be fine if anyone, like, built
[00:29:31.340 --> 00:29:35.420]   MedArt, like in this case it was Tanishq Baling, he was a friend, and you joined him, and like,
[00:29:35.420 --> 00:29:38.100]   you know, now you guys are working on it together.
[00:29:38.100 --> 00:29:43.860]   Does it have to be you, or are you happy to support anyone doing it, even if it's like
[00:29:43.860 --> 00:29:47.140]   someone else who ultimately is the one to build it?
[00:29:47.140 --> 00:29:49.540]   Yeah, no, it definitely doesn't have to be me.
[00:29:49.540 --> 00:29:56.420]   So in fact, you know, I've got a good example of that, you know, we felt, you know, me and
[00:29:56.420 --> 00:30:01.780]   my friend Hamill felt like there was a big gap, that there wasn't any easy enough to
[00:30:01.780 --> 00:30:07.140]   use kind of technical blogging options, you know, like really easy to use, the kind of
[00:30:07.140 --> 00:30:13.980]   thing that somebody who's a chemist, or a journalist, or whatever, can use without any
[00:30:13.980 --> 00:30:14.980]   technical background.
[00:30:14.980 --> 00:30:21.260]   So we built one called Fast Pages, and yeah, we built it because that was a gap that needed
[00:30:21.260 --> 00:30:30.660]   to be filled, and so we filled the gap, and then some years later, my friend JJ and his
[00:30:30.660 --> 00:30:38.060]   team at our studio now called Posit built something better called Quarto, and so, you
[00:30:38.060 --> 00:30:43.460]   know, a common response to that, I guess, would be like to dig in and try to like copy
[00:30:43.460 --> 00:30:48.260]   Quarto's better features, and make Fast Pages better, and compete with Quarto, but I was
[00:30:48.260 --> 00:30:52.420]   like, you know, I chatted to Hamill about it, like, okay, well we built Fast Pages to
[00:30:52.420 --> 00:30:55.940]   meet an unmet need.
[00:30:55.940 --> 00:31:02.380]   That need is now met better by something else, so we put a, you know, we archived Fast Pages,
[00:31:02.380 --> 00:31:06.660]   we put a message on the Fast Pages page saying, oh, Quarto, you should use Quarto, it's better.
[00:31:06.660 --> 00:31:13.620]   We built a Fast Pages to Quarto migration script that would let people switch, and now
[00:31:13.620 --> 00:31:17.860]   people use Quarto, and we use Quarto, and we're very happy with it.
[00:31:17.860 --> 00:31:23.020]   I don't know if I would be as nice as you, if you're definitely a better man than I am.
[00:31:23.020 --> 00:31:29.100]   Okay, coming back to MedArc, I briefly like scanned the page, so I haven't engaged with
[00:31:29.100 --> 00:31:33.420]   it too much, but like, you are training large models, like, what kinds of models are you
[00:31:33.420 --> 00:31:34.420]   training?
[00:31:34.420 --> 00:31:39.180]   Well, it's very early days, Lavanya, so, you know, basically the plan, there's only plans
[00:31:39.180 --> 00:31:48.420]   at this point, is to train multimodal models that will bring together medical imaging,
[00:31:48.420 --> 00:31:59.940]   you know, billing records, doctor's notes, radiology reports, medical sensor data, you
[00:31:59.940 --> 00:32:00.940]   know, as much as possible.
[00:32:00.940 --> 00:32:10.540]   The, you know, the goal being to support medical staff, you know, doctors, nurses, community
[00:32:10.540 --> 00:32:20.860]   health workers, so forth, so that they can do their work more, more quickly, more accurately,
[00:32:20.860 --> 00:32:25.100]   by leveraging, yeah, leveraging models of that data.
[00:32:25.100 --> 00:32:30.900]   And how do you get these models into production?
[00:32:30.900 --> 00:32:38.200]   We have no idea, you know, I mean, that's, that's hard, and it's not our area of expertise,
[00:32:38.200 --> 00:32:45.180]   because that is all about dealing with regulatory, you know, groups and stuff like that, which
[00:32:45.180 --> 00:32:47.100]   Tanishka and I are not experts at.
[00:32:47.100 --> 00:32:50.900]   I mean, I have a little bit of expertise there, but not, there are plenty of people with a
[00:32:50.900 --> 00:32:54.320]   lot more, you know, there are plenty of lawyers that that's their job.
[00:32:54.320 --> 00:32:59.140]   So our focus is on trying to show what's possible.
[00:32:59.140 --> 00:33:06.340]   So as researchers working with academic medical hospitals, so for example, we've got a partnership
[00:33:06.340 --> 00:33:12.340]   at the moment with Stanford, doctors at Stanford have a huge amount of leeway, nearly total
[00:33:12.340 --> 00:33:13.340]   leeway.
[00:33:13.340 --> 00:33:18.940]   This is part of the US medical system, is deciding how they should diagnose and treat
[00:33:18.940 --> 00:33:20.220]   their patients.
[00:33:20.220 --> 00:33:28.380]   So if they wish to use Stanford's data to build a model that then they use to help them
[00:33:28.380 --> 00:33:33.340]   diagnose their patients, it's not, there's not, you know, nobody in the government can
[00:33:33.340 --> 00:33:35.220]   tell them they're not allowed to do that.
[00:33:35.220 --> 00:33:41.300]   That's, that's their, their doctors and they get to decide how to do doctoring, you know.
[00:33:41.300 --> 00:33:46.700]   So yeah, so we can work with these groups to help them, you know, help doctors do what
[00:33:46.700 --> 00:33:48.300]   they want to do.
[00:33:48.300 --> 00:33:50.780]   And we can publish the results of that.
[00:33:50.780 --> 00:33:56.220]   And then if, if people decide like, you know what, that would be good if other people could
[00:33:56.220 --> 00:33:59.180]   get to do that as well, then they can figure that out.
[00:33:59.180 --> 00:34:05.980]   And it's probably not going to be in America where that happens, because America is not
[00:34:05.980 --> 00:34:10.300]   really ahead of the game when it comes to how the regulatory system works, or more importantly,
[00:34:10.300 --> 00:34:11.700]   how medical incentives work.
[00:34:11.700 --> 00:34:18.100]   But, you know, we might well see in, in China or in India, for example, where there's a
[00:34:18.100 --> 00:34:26.060]   lot of digital health going on, and maybe a bit more flexibility around thinking about
[00:34:26.060 --> 00:34:29.460]   how best to, you know, and also a huge bit much more of a need, there's a shortage of
[00:34:29.460 --> 00:34:31.460]   doctors in these, these jurisdictions.
[00:34:31.460 --> 00:34:37.780]   Maybe there will be more of an opportunity to roll out these models more quickly.
[00:34:37.780 --> 00:34:38.780]   Interesting.
[00:34:38.780 --> 00:34:47.020]   And there you could have a much more, like you could help them define the system, the
[00:34:47.020 --> 00:34:50.860]   EHR records, what they look like, because they don't even have anything today.
[00:34:50.860 --> 00:34:55.100]   Yeah, well, I mean, they, they actually have a lot, you know.
[00:34:55.100 --> 00:34:59.460]   So developing nations are caught that because they're developing.
[00:34:59.460 --> 00:35:05.060]   And in fact, you know, China, for example, is pretty developed at this point, but they
[00:35:05.060 --> 00:35:06.700]   developed more recently.
[00:35:06.700 --> 00:35:14.500]   So their medical systems don't have the same, you know, kind of archaic foundations that
[00:35:14.500 --> 00:35:18.160]   Australia's or America's do.
[00:35:18.160 --> 00:35:25.300]   So they have a lot of very high-tech equipment, and they're connected to much more high-tech
[00:35:25.300 --> 00:35:29.740]   digital backbones to some extent.
[00:35:29.740 --> 00:35:36.240]   So yeah, it's, you know, for example, last time I checked Apollo, which is an Indian
[00:35:36.240 --> 00:35:42.480]   group, had like the largest kind of digital health system in the world, you know, fully
[00:35:42.480 --> 00:35:43.480]   digital.
[00:35:43.480 --> 00:35:44.480]   But there are pockets, you know.
[00:35:44.480 --> 00:35:49.440]   For example, actually, one of the world's largest pathology companies, Sullivan Nicolades,
[00:35:49.440 --> 00:35:51.960]   is actually an Australian company.
[00:35:51.960 --> 00:35:59.720]   And actually, I would say, you know, I'm pretty sure would be considered by far the most advanced
[00:35:59.720 --> 00:36:06.120]   in the world when it comes to a digital pipeline with AI helping it.
[00:36:06.120 --> 00:36:10.920]   And that was all actually built in my home state, in Queensland.
[00:36:10.920 --> 00:36:18.040]   So there are interesting pockets of brilliance around the world.
[00:36:18.040 --> 00:36:26.040]   And hopefully, by MedArc being a very open, community-driven approach, you know, it allows
[00:36:26.040 --> 00:36:30.800]   us to work with those pockets of brilliance.
[00:36:30.800 --> 00:36:37.800]   And you know, there's a lot of brilliant people at Stanford, for example, who we can work
[00:36:37.800 --> 00:36:47.440]   with on these partnerships and drag along the rest of the world when they're ready.
[00:36:47.440 --> 00:36:51.360]   I love looking at the world as like pockets of brilliance.
[00:36:51.360 --> 00:36:58.640]   It's such a happy way to look at the world.
[00:36:58.640 --> 00:37:03.800]   Instead of pits of darkness and despair in the rest of the world, I don't know.
[00:37:03.800 --> 00:37:08.560]   Oh, yeah, and then I was gonna ask, speaking of that, like, there's something on your website
[00:37:08.560 --> 00:37:13.880]   around like, you want to build an interdisciplinary team for this.
[00:37:13.880 --> 00:37:16.120]   Like, what kinds of people are you looking for?
[00:37:16.120 --> 00:37:17.120]   Do you have a strategy?
[00:37:17.120 --> 00:37:22.000]   Or is it like, hey, if doctors want to work with us, we'll like tweak our program to that.
[00:37:22.000 --> 00:37:23.000]   And you know?
[00:37:23.000 --> 00:37:24.000]   Yeah.
[00:37:24.000 --> 00:37:30.880]   My approach to everything nowadays tends to be like, very community and demand driven
[00:37:30.880 --> 00:37:32.440]   or availability driven.
[00:37:32.440 --> 00:37:41.920]   So you know, we have a discord, people find us, you know, they, you know, so we've got
[00:37:41.920 --> 00:37:47.800]   hundreds of doctors on our discord saying, hey, I'm X, I'm in this part of the world,
[00:37:47.800 --> 00:37:52.400]   I work in this area, this is the data sets I have, these are the problems I'm personally
[00:37:52.400 --> 00:37:53.400]   interested in.
[00:37:53.400 --> 00:37:57.400]   And we can help hook them up together with like some PhD student who said, you know,
[00:37:57.400 --> 00:38:00.880]   I'm this person, I'm at this university in this part of the world, I'm interested, you
[00:38:00.880 --> 00:38:02.680]   know, my expertise is over here.
[00:38:02.680 --> 00:38:08.680]   And Tanishka and I can help to coordinate these groups and supervise them and then identify
[00:38:08.680 --> 00:38:14.000]   places where we can combine data sets together, you know, or leverage a pre-trained model
[00:38:14.000 --> 00:38:15.280]   that we've been working on.
[00:38:15.280 --> 00:38:25.160]   So yeah, you really need some kind of central place for people to come, you know, and provide
[00:38:25.160 --> 00:38:32.440]   their problems, their expertise, their data, and someone that needs to help these groups
[00:38:32.440 --> 00:38:34.080]   to work together, you know.
[00:38:34.080 --> 00:38:41.760]   There's lots of PhD students, for example, who really want to work in deep learning,
[00:38:41.760 --> 00:38:46.060]   they really want to work in medicine, they can't find anybody in their local universities
[00:38:46.060 --> 00:38:50.000]   who know about these things, but you know, they've got people who are prepared to support
[00:38:50.000 --> 00:38:55.480]   them, and you know, so we can say like, oh, here's a group that you can work in, you know,
[00:38:55.480 --> 00:38:59.680]   got exactly the kind of expertise you're interested in developing.
[00:38:59.680 --> 00:39:06.760]   So yeah, it's very iterative, you know, and I find things tend to, as long as you're in
[00:39:06.760 --> 00:39:11.960]   the right general space, things tend to fall out pretty nicely.
[00:39:11.960 --> 00:39:16.480]   You know, I've never personally, you know, as long as Fast.ai has been alive, there's
[00:39:16.480 --> 00:39:22.320]   never been any kind of projects we've done that didn't go anywhere, because you know,
[00:39:22.320 --> 00:39:28.120]   there's so much white space available, so much stuff that you can do, you can really
[00:39:28.120 --> 00:39:34.360]   pick almost anything and find lots of high impact things you can do.
[00:39:34.360 --> 00:39:41.760]   It just, that kind of makes me think of like, you're so good at building communities, and
[00:39:41.760 --> 00:39:48.760]   I'm wondering what makes you so good at building communities, like even with Fast.ai, a couple
[00:39:48.760 --> 00:39:53.360]   of things that I've heard from people are like, you're in there answering questions
[00:39:53.360 --> 00:39:58.640]   by people, no matter who it is, you know, but I'm wondering, yeah, what is your secret?
[00:39:58.640 --> 00:40:03.160]   Yeah, people often ask me that, because if somebody, like I know people who are like,
[00:40:03.160 --> 00:40:08.480]   at their jobs is to be community builders, and they always talk about things I've never
[00:40:08.480 --> 00:40:12.640]   heard of, I don't, I don't feel like I have, I know anything about this space.
[00:40:12.640 --> 00:40:16.640]   It's been totally an accident, but yeah, I've built lots of communities, and they have all
[00:40:16.640 --> 00:40:26.040]   been successful, but like, they were always, I don't know, me trying to like, just find
[00:40:26.040 --> 00:40:29.560]   people of a certain type that I could like, talk to, or who could, they could talk to
[00:40:29.560 --> 00:40:35.560]   each other, and just very genuinely talk to them, you know.
[00:40:35.560 --> 00:40:42.680]   So like, I mean, certainly I tried to pick a software platform that's as easy to use,
[00:40:42.680 --> 00:40:49.200]   and flexible, and nice as possible, and you know, so with Fast.ai, we started with discourse
[00:40:49.200 --> 00:40:54.560]   forums, and they kind of had only just come out, but I really thought like, this is a
[00:40:54.560 --> 00:41:01.240]   nice platform, I like using it, I think other people will like using it, and then tried
[00:41:01.240 --> 00:41:07.200]   to like, really genuinely answer every question that people asked.
[00:41:07.200 --> 00:41:15.360]   If there's anybody being like, not particularly nice, you know, even if it's like, you can't,
[00:41:15.360 --> 00:41:19.560]   it's not that they're swearing, or maybe they're just being a bit passive, it's like, you just,
[00:41:19.560 --> 00:41:21.160]   it just, it's not particularly nice.
[00:41:21.160 --> 00:41:29.120]   I step on it very quickly, you know, I, it's definitely not a like, you know, freedom of
[00:41:29.120 --> 00:41:32.680]   speech platform, where anybody gets to say whatever they like, however they want, you
[00:41:32.680 --> 00:41:40.920]   know, I want everybody to have a lot of pleasure in interacting with people there.
[00:41:40.920 --> 00:41:46.440]   And if somebody's making interactions not pleasurable, I try to be understanding, but
[00:41:46.440 --> 00:41:53.920]   I'll send them a direct message, and say, look, this message you sent, I didn't come
[00:41:53.920 --> 00:41:58.760]   away from it feeling happy, you know, do you feel like people would come away from that
[00:41:58.760 --> 00:41:59.760]   message feeling happy?
[00:41:59.760 --> 00:42:03.920]   And, you know, generally they'll say like, no, I was pissed off, I wanted people to know
[00:42:03.920 --> 00:42:08.720]   I was pissed off, and I'm going to say, well, it sucks to be pissed off, but do you really
[00:42:08.720 --> 00:42:12.160]   think it helped to like, make other people feel pissed off?
[00:42:12.160 --> 00:42:19.520]   Like it's, you know, like, let's not go there, and 99% of the time, people will be like,
[00:42:19.520 --> 00:42:25.160]   yeah, you know what, I don't want to make more people pissed off, you know, you know,
[00:42:25.160 --> 00:42:26.160]   you're right.
[00:42:26.160 --> 00:42:32.280]   But 1% of the time, like maybe once a year or two, there'll be somebody who's just like,
[00:42:32.280 --> 00:42:36.280]   I hope everybody's pissed off, you know, I've just, you know, I think everybody deserves
[00:42:36.280 --> 00:42:42.360]   to be pissed off, and I'll just be like, okay, that's a point of view, I'm not okay with
[00:42:42.360 --> 00:42:48.440]   having it here, so you have to leave, you know, this is not what we're building and
[00:42:48.440 --> 00:42:51.720]   the people we're building it with, so don't come back.
[00:42:51.720 --> 00:42:56.160]   But yeah, so have nice people being nice to each other.
[00:42:56.160 --> 00:43:01.720]   I mean, I try to be really like, understanding that everybody has bad days, and sometimes
[00:43:01.720 --> 00:43:07.560]   times like, smooth over things, people argue a bit with each other, I'll just be like,
[00:43:07.560 --> 00:43:11.400]   let's, you know, start a three way DM, talk to them both.
[00:43:11.400 --> 00:43:14.720]   It's like, look, you know, you know, each other, I can understand why you felt this
[00:43:14.720 --> 00:43:16.720]   way, why you felt this way, but.
[00:43:16.720 --> 00:43:19.720]   Wow, so you're playing therapist there too, Tim.
[00:43:19.720 --> 00:43:21.280]   Yeah, yeah, definitely.
[00:43:21.280 --> 00:43:27.000]   It only seems like you, you love being in communities, it's not like you're setting
[00:43:27.000 --> 00:43:32.640]   out to build a community, so it's not like, I need to run three events, you know, just,
[00:43:32.640 --> 00:43:36.560]   you're just like having fun, and then that invites the interest.
[00:43:36.560 --> 00:43:41.960]   Well, you know, like, particularly now I'm in Australia, in San Francisco, you have a
[00:43:41.960 --> 00:43:46.840]   natural geographic community, you know, because a lot of people who are, you know, pretty
[00:43:46.840 --> 00:43:52.640]   bold and who are interested in technical subjects, particularly AI, they tend to like, head there,
[00:43:52.640 --> 00:43:55.820]   because there's this intellectual sensor of gravity.
[00:43:55.820 --> 00:44:02.240]   On the whole, they don't head to the regional part of Queensland that I've moved to, which
[00:44:02.240 --> 00:44:10.040]   is fine, because like, when you have an online community, chat to people all the time, both
[00:44:10.040 --> 00:44:15.920]   through text and through video, and build relationships, I get the best of both worlds.
[00:44:15.920 --> 00:44:22.120]   I get to, you know, live by the beach, and, you know, go windsurfing, or swimming, or
[00:44:22.120 --> 00:44:27.080]   whatever, when I want to have a break, and then train some models, and then chat to people
[00:44:27.080 --> 00:44:33.680]   in the AI community, and, you know, it's a nice way to be, I think.
[00:44:33.680 --> 00:44:38.400]   So like, yeah, like, most people are not lucky enough to live somewhere where they have a
[00:44:38.400 --> 00:44:44.400]   local geographic concentration of people who are interested in their particular interests,
[00:44:44.400 --> 00:44:48.400]   but thanks to the internet, you can make that happen now.
[00:44:48.400 --> 00:44:53.360]   And like, that's most people.
[00:44:53.360 --> 00:44:59.800]   Do you have advice for like, someone in the middle of Ireland, or like, India, or Australia,
[00:44:59.800 --> 00:45:03.720]   who's like, doesn't know anyone, maybe they're still young in their careers, and not even
[00:45:03.720 --> 00:45:07.960]   enough people are learning machine learning in their own class, you know, but they're
[00:45:07.960 --> 00:45:10.840]   learning about large models, and they just want to talk to someone about it.
[00:45:10.840 --> 00:45:11.840]   Yeah, exactly.
[00:45:11.840 --> 00:45:17.400]   So come, you know, come to forums.fast.ai, or come to the fast.ai discord, and just start
[00:45:17.400 --> 00:45:22.400]   by waving, you know, and somebody will wave back, and just be like, go to the introduction
[00:45:22.400 --> 00:45:28.440]   thread and say, "Hello, I'm whatever, you know, from wherever, and this is what I'm
[00:45:28.440 --> 00:45:31.800]   interested in," and share something you've done.
[00:45:31.800 --> 00:45:36.840]   It's like, "Oh, I just built this simple little classifier after reading chapter one
[00:45:36.840 --> 00:45:42.440]   of the fast.ai book," and you know, share a bit of yourself, share a bit of your work,
[00:45:42.440 --> 00:45:48.000]   and then generally speaking, somebody's going to be like, "Wow, that's amazing, you know,
[00:45:48.000 --> 00:45:52.880]   had you ever considered enhancing it by doing this?" or like, "Oh, that's a, that's a great
[00:45:52.880 --> 00:45:59.360]   start," you know, the next, the next step would be to try this technique, and now you're,
[00:45:59.360 --> 00:46:06.280]   now you're having a conversation, you know, you found yourself a bit of a mentor, and
[00:46:06.280 --> 00:46:11.840]   you know, we've got so many people who have come through our course and become key parts
[00:46:11.840 --> 00:46:16.640]   of our community from, from parts of the world where they have, like, like, so for example,
[00:46:16.640 --> 00:46:24.800]   I remember there was one woman, I think she was still a teenager, from Bangladesh, and
[00:46:24.800 --> 00:46:30.760]   she didn't know any other women who coded, you know, let alone who were interested in
[00:46:30.760 --> 00:46:37.600]   AI, and so, you know, it was quite early on, through fast.ai, she found a community of
[00:46:37.600 --> 00:46:42.400]   people who didn't care that she was young and a woman and in Bangladesh, they just wanted
[00:46:42.400 --> 00:46:48.280]   to talk about AI, you know, and I think I, she ended up, you know, after a couple of
[00:46:48.280 --> 00:46:58.360]   years joining a hot AI startup, and blossomed, you know, and I definitely like to provide
[00:46:58.360 --> 00:47:04.960]   that kind of platform where people who, you know, don't necessarily have the resources
[00:47:04.960 --> 00:47:16.440]   and so forth can, can, can nonetheless, given some tenacity and belief, can find a way through.
[00:47:16.440 --> 00:47:21.760]   That's such a nice thing to do for people that are otherwise not seen, or like, don't
[00:47:21.760 --> 00:47:30.240]   have the opportunity. So, I have a question, if you had more time to do research, not in
[00:47:30.240 --> 00:47:37.320]   medical field, and like, like, what other field would you spend time in? Well, I basically
[00:47:37.320 --> 00:47:44.520]   spend all my time doing research, so I'd have to clone myself to find more. I, I'm extremely
[00:47:44.520 --> 00:47:51.440]   interested in education, you know, I would love to do research and product development
[00:47:51.440 --> 00:47:58.960]   in education around using, you know, there's so many great academic papers in education
[00:47:58.960 --> 00:48:05.720]   that the ideas aren't really being leveraged at schools, because schools are really limited
[00:48:05.720 --> 00:48:09.640]   in what they can do, you know, but, you know, there's, there's nothing you can do about
[00:48:09.640 --> 00:48:13.880]   that other than like, change how schools are structured, which that's going to probably
[00:48:13.880 --> 00:48:18.280]   take a hundred years. So, I think that's really interesting to think about, like, well, what
[00:48:18.280 --> 00:48:28.960]   could we do differently in education? Yeah, I, in terms of applications of AI, I think
[00:48:28.960 --> 00:48:36.840]   yeah, medicine is certainly my, my number one interest, and I'm definitely interested
[00:48:36.840 --> 00:48:43.200]   in like, multimodal more generally, but I'm also, yeah, really interested, I'm actually
[00:48:43.200 --> 00:48:47.320]   hoping sometime soonish to get back into a deep dive into NLP, because it's been a few
[00:48:47.320 --> 00:48:55.600]   years to try to really understand, like, yeah, what, what are the opportunities now, like,
[00:48:55.600 --> 00:49:02.680]   particularly around, like, how can we fine-tune these large language models better to do,
[00:49:02.680 --> 00:49:11.400]   like, to be more accurate in what they say, to, to guide them more, you know, in a particular
[00:49:11.400 --> 00:49:20.640]   way. At the moment, that stuff's all extremely basic, how it's being done. It works surprisingly
[00:49:20.640 --> 00:49:25.600]   well because the underlying idea of a fine-tuned language model is such a powerful idea, but
[00:49:25.600 --> 00:49:35.040]   I think there are, you know, lots of low-hanging fruit around doing that better.
[00:49:35.040 --> 00:49:41.080]   Do you see a path to these large language models being used in search engines anytime
[00:49:41.080 --> 00:49:45.720]   soon? Because they're so inaccurate, and like, you know, if you like, with search, it's like,
[00:49:45.720 --> 00:49:50.320]   here are 10 links that I think might work, but I'm not sure you go and look at them and
[00:49:50.320 --> 00:49:54.200]   see which one, but the large language model is like, here's what I think the answer is,
[00:49:54.200 --> 00:49:58.320]   and I'm pretty certain, you know. Yeah, no, I think there's a lot of room to
[00:49:58.320 --> 00:50:05.440]   improve them greatly. So I'd say the answer is yes. I mean, I, I'm not sure that you'll
[00:50:05.440 --> 00:50:11.880]   think of using them exactly the same way as you use a search engine. I think the UX needs
[00:50:11.880 --> 00:50:18.080]   a whole lot of work, you know, and the people working on these kind of bings and chat GPTs
[00:50:18.080 --> 00:50:27.840]   and stuff generally aren't UX experts, and so the UX has been on the whole pretty poor.
[00:50:27.840 --> 00:50:33.000]   As you say, they, they're really overconfident. One of the problems is that there's a huge
[00:50:33.000 --> 00:50:39.320]   lack of educational materials around helping UX people understand the capabilities and
[00:50:39.320 --> 00:50:45.280]   constraints of deep learning, you know, or helping deep learning models understand UX,
[00:50:45.280 --> 00:50:49.920]   which is hard still. So that would certainly be another area I'd be interested in doing
[00:50:49.920 --> 00:50:57.280]   research in, is like, really taking the best ideas from the UX world and thinking about
[00:50:57.280 --> 00:51:06.760]   how do we build and roll out models which take advantage of the opportunities and avoid
[00:51:06.760 --> 00:51:13.520]   the constraints based on a really deep understanding of human-computer interaction.
[00:51:13.520 --> 00:51:19.160]   Yeah, and like that wouldn't make the models more accurate, but that would just frame them
[00:51:19.160 --> 00:51:23.640]   as what they are, which is like, these are my best guesses and like, let's.
[00:51:23.640 --> 00:51:29.320]   Yeah, exactly. A bit of that, and also a bit of like, not just throwing the tokens in that
[00:51:29.320 --> 00:51:34.640]   have the highest p-values, but understanding like, that this is actually an area where
[00:51:34.640 --> 00:51:38.720]   maybe I shouldn't just be throwing in high value, high p-value tokens, but I should be
[00:51:38.720 --> 00:51:46.080]   like saying like, yeah, you know, to answer your question, I would be looking for things
[00:51:46.080 --> 00:51:55.680]   like this. This isn't something I'm great at, but here's three like, you know, queries
[00:51:55.680 --> 00:52:00.520]   you could click on to get back lists, which I will help you summarize, and then come back
[00:52:00.520 --> 00:52:05.920]   to me and we'll talk about your understanding of those summaries, and you know, more of
[00:52:05.920 --> 00:52:15.000]   that kind of using humans, you know, using the web, using paraphrasing, using summarization,
[00:52:15.000 --> 00:52:22.240]   you know, using calculation engines together, and even multimodal, you know, like I don't
[00:52:22.240 --> 00:52:26.800]   quite know what you mean, like here's a canvas, could you like, sketch it out, and being able
[00:52:26.800 --> 00:52:32.800]   to like, or like, take a screenshot for me, or let me turn on the video camera on your
[00:52:32.800 --> 00:52:36.480]   computer, so whatever, so I can get this information.
[00:52:36.480 --> 00:52:45.480]   Yeah, that sounds really fun. Cool. Let's try to end with two quick questions, maybe.
[00:52:45.480 --> 00:52:49.960]   The first is like, if you're not a machine learning engineer, if you're like a doctor,
[00:52:49.960 --> 00:52:54.600]   lawyer, someone, and you want to benefit from AI, like, where do you get started?
[00:52:54.600 --> 00:52:58.640]   Yeah, so that's, I mean, that's exactly what Fast.ai is meant to be. So we've got lots
[00:52:58.640 --> 00:53:03.680]   of doctors and radiologists and stuff who have done that course. Unfortunately, we're
[00:53:03.680 --> 00:53:08.840]   still at a point where you need to be able to code to use these tools effectively. We're
[00:53:08.840 --> 00:53:13.840]   hoping to get past that point, but honestly, it still feels like it's years away. I was
[00:53:13.840 --> 00:53:19.640]   hoping we'd be closer by now, but we're not. So it's going to be a long journey. It's really
[00:53:19.640 --> 00:53:26.160]   hard to do when you're, you know, at med school, or doing your residency, or working as a GP,
[00:53:26.160 --> 00:53:32.160]   or whatever, but there are lots of doctors who have done that. So we know it's possible,
[00:53:32.160 --> 00:53:39.280]   and they've gone on to be like world-leading experts in the medical AI field. So you should
[00:53:39.280 --> 00:53:45.640]   expect it to be hard, and to take sacrifices, and to take a lot of time, but it's, there
[00:53:45.640 --> 00:53:52.920]   are lots of role models that show it's possible. And then the last question is, how has it
[00:53:52.920 --> 00:53:58.320]   been like moving to Brisbane from SF? Should people still be trying to move to America
[00:53:58.320 --> 00:54:04.240]   in general, SF in particular, or can they work from anywhere? You know, it's a great
[00:54:04.240 --> 00:54:10.320]   question. I would say people should definitely try to spend a couple of years in SF, if you're
[00:54:10.320 --> 00:54:18.880]   interested in this field. There's a lot of things I don't like about America, you know,
[00:54:18.880 --> 00:54:30.200]   like the kind of the values, and the individualism, and you know. But, you know, SF is a unique
[00:54:30.200 --> 00:54:39.480]   place and time in history. You know, it's like it's a collection of brilliant people
[00:54:39.480 --> 00:54:46.120]   who have come together with a lot of kind of common goals. Now that can, that can and
[00:54:46.120 --> 00:54:50.680]   does lead to a lot of toxicity, and a lot of over-competitiveness. There's also a lot
[00:54:50.680 --> 00:55:00.480]   of great cooperation, and fantastic communities that are building there. To be amongst like-minded
[00:55:00.480 --> 00:55:07.880]   people in person, yeah, it's really nice, you know. But I'd say a couple of years, and
[00:55:07.880 --> 00:55:13.360]   then get out, because, you know, I think it can also do your head in a little bit, and
[00:55:13.360 --> 00:55:18.200]   I think you can then, once you've got that confidence in those connections, you can then
[00:55:18.200 --> 00:55:25.040]   leverage those digitally. Like, I don't, I don't feel like now I'm missing out at all
[00:55:25.040 --> 00:55:30.760]   by not being in SF. But if I hadn't been in SF, I don't think I would be doing the things
[00:55:30.760 --> 00:55:37.240]   I'm doing now. That's a good way to end the interview, Jeremy. Thank you so much for being
[00:55:37.240 --> 00:55:44.680]   here. It was so much fun. No worries, Levonia. Thank you.
[00:55:44.680 --> 00:55:47.180]   (light music)

