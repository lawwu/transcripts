
[00:00:00.000 --> 00:00:02.680]   The following is a conversation with Michael Stevens,
[00:00:02.680 --> 00:00:04.480]   the creator of Vsauce,
[00:00:04.480 --> 00:00:07.000]   one of the most popular educational YouTube channels
[00:00:07.000 --> 00:00:10.080]   in the world, with over 15 million subscribers
[00:00:10.080 --> 00:00:13.040]   and over 1.7 billion views.
[00:00:13.040 --> 00:00:16.160]   His videos often ask and answer questions
[00:00:16.160 --> 00:00:18.760]   that are both profound and entertaining,
[00:00:18.760 --> 00:00:21.780]   spanning topics from physics to psychology.
[00:00:21.780 --> 00:00:23.640]   Popular questions include,
[00:00:23.640 --> 00:00:25.440]   what if everyone jumped at once?
[00:00:25.440 --> 00:00:27.680]   Or what if the sun disappeared?
[00:00:27.680 --> 00:00:29.820]   Or why are things creepy?
[00:00:29.820 --> 00:00:32.840]   Or what if the earth stopped spinning?
[00:00:32.840 --> 00:00:34.080]   As part of his channel,
[00:00:34.080 --> 00:00:36.320]   he created three seasons of "Minefield,"
[00:00:36.320 --> 00:00:38.760]   a series that explored human behavior.
[00:00:38.760 --> 00:00:41.600]   His curiosity and passion are contagious
[00:00:41.600 --> 00:00:44.080]   and inspiring to millions of people.
[00:00:44.080 --> 00:00:45.240]   And so as an educator,
[00:00:45.240 --> 00:00:47.540]   his impact and contribution to the world
[00:00:47.540 --> 00:00:49.780]   is truly immeasurable.
[00:00:49.780 --> 00:00:52.840]   This is the Artificial Intelligence Podcast.
[00:00:52.840 --> 00:00:55.200]   If you enjoy it, subscribe on YouTube,
[00:00:55.200 --> 00:00:57.160]   give it five stars on Apple Podcast,
[00:00:57.160 --> 00:00:58.600]   support it on Patreon,
[00:00:58.600 --> 00:01:00.640]   or simply connect with me on Twitter,
[00:01:00.640 --> 00:01:03.800]   @LexFriedman, spelled F-R-I-D-M-A-N.
[00:01:03.800 --> 00:01:06.380]   I recently started doing ads
[00:01:06.380 --> 00:01:08.140]   at the end of the introduction.
[00:01:08.140 --> 00:01:10.780]   I'll do one or two minutes after introducing the episode,
[00:01:10.780 --> 00:01:12.740]   and never any ads in the middle
[00:01:12.740 --> 00:01:14.780]   that break the flow of the conversation.
[00:01:14.780 --> 00:01:16.120]   I hope that works for you
[00:01:16.120 --> 00:01:19.000]   and doesn't hurt the listening experience.
[00:01:19.000 --> 00:01:21.500]   This show is presented by Cash App,
[00:01:21.500 --> 00:01:24.000]   the number one finance app in the App Store.
[00:01:24.000 --> 00:01:26.580]   I personally use Cash App to send money to friends,
[00:01:26.580 --> 00:01:28.300]   but you can also use it to buy, sell,
[00:01:28.300 --> 00:01:30.740]   and deposit Bitcoin in just seconds.
[00:01:30.740 --> 00:01:33.520]   Cash App also has a new investing feature.
[00:01:33.520 --> 00:01:34.980]   You can buy fractions of a stock,
[00:01:34.980 --> 00:01:38.300]   say $1 worth, no matter what the stock price is.
[00:01:38.300 --> 00:01:40.940]   Brokerage services are provided by Cash App Investing,
[00:01:40.940 --> 00:01:44.260]   a subsidiary of Square and member SIPC.
[00:01:44.260 --> 00:01:46.220]   I'm excited to be working with Cash App
[00:01:46.220 --> 00:01:49.160]   to support one of my favorite organizations called FIRST,
[00:01:49.160 --> 00:01:52.480]   best known for their FIRST Robotics and Lego competitions.
[00:01:52.480 --> 00:01:56.020]   They educate and inspire hundreds of thousands of students
[00:01:56.020 --> 00:01:57.740]   in over 110 countries
[00:01:57.740 --> 00:02:00.340]   and have a perfect rating on Charity Navigator,
[00:02:00.340 --> 00:02:01.660]   which means the donated money
[00:02:01.660 --> 00:02:04.300]   is used to maximum effectiveness.
[00:02:04.300 --> 00:02:07.220]   When you get Cash App from the App Store, Google Play,
[00:02:07.220 --> 00:02:09.940]   and use code LEXPODCAST,
[00:02:09.940 --> 00:02:13.780]   you'll get $10 and Cash App will also donate $10 to FIRST,
[00:02:13.780 --> 00:02:15.540]   which again is an organization
[00:02:15.540 --> 00:02:18.400]   that I've personally seen inspire girls and boys
[00:02:18.400 --> 00:02:21.700]   to dream of engineering a better world.
[00:02:21.700 --> 00:02:25.980]   And now here's my conversation with Michael Stevens.
[00:02:27.020 --> 00:02:30.260]   One of your deeper interests is psychology,
[00:02:30.260 --> 00:02:31.760]   understanding human behavior.
[00:02:31.760 --> 00:02:35.500]   You've pointed out how messy studying human behavior is
[00:02:35.500 --> 00:02:37.420]   and that it's far from the scientific rigor
[00:02:37.420 --> 00:02:40.900]   of something like physics, for example.
[00:02:40.900 --> 00:02:43.660]   How do you think we can take psychology
[00:02:43.660 --> 00:02:45.680]   from where it's been in the 20th century
[00:02:45.680 --> 00:02:49.100]   to something more like what the physicists,
[00:02:49.100 --> 00:02:50.460]   theoretical physicists are doing,
[00:02:50.460 --> 00:02:52.380]   something precise, something rigorous?
[00:02:53.280 --> 00:02:55.400]   - Well, we could do it by
[00:02:55.400 --> 00:03:01.200]   finding the physical foundations of psychology, right?
[00:03:01.200 --> 00:03:05.840]   If all of our emotions and moods and feelings and behaviors
[00:03:05.840 --> 00:03:10.400]   are the result of mechanical behaviors
[00:03:10.400 --> 00:03:12.800]   of atoms and molecules in our brains,
[00:03:12.800 --> 00:03:15.400]   then can we find correlations?
[00:03:15.400 --> 00:03:17.760]   Perhaps chaos makes that really difficult
[00:03:17.760 --> 00:03:19.600]   and the uncertainty principle and all these things.
[00:03:19.600 --> 00:03:22.760]   We can't know the position and velocity
[00:03:22.760 --> 00:03:27.160]   of every single quantum state in a brain, probably.
[00:03:27.160 --> 00:03:32.160]   But I think that if we can get to that point with psychology,
[00:03:32.160 --> 00:03:37.400]   then we can start to think about consciousness
[00:03:37.400 --> 00:03:41.000]   in a physical and mathematical way.
[00:03:41.000 --> 00:03:44.800]   When we ask questions like, well, what is self-reference?
[00:03:44.800 --> 00:03:47.360]   How can you think about your self-thinking?
[00:03:47.360 --> 00:03:49.200]   What are some mathematical structures
[00:03:49.200 --> 00:03:52.440]   that could bring that about?
[00:03:52.440 --> 00:03:55.560]   - There's ideas of, in terms of consciousness
[00:03:55.560 --> 00:03:59.240]   and breaking it down into physics,
[00:03:59.240 --> 00:04:03.480]   there's ideas of panpsychism where people believe that
[00:04:03.480 --> 00:04:07.480]   whatever consciousness is is a fundamental part of reality.
[00:04:07.480 --> 00:04:08.840]   It's almost like a physics law.
[00:04:08.840 --> 00:04:11.560]   Do you think, what's your views on consciousness?
[00:04:11.560 --> 00:04:15.600]   Do you think it has this deep part of reality
[00:04:15.600 --> 00:04:17.840]   or is it something that's deeply human
[00:04:17.840 --> 00:04:19.500]   and constructed by us humans?
[00:04:21.640 --> 00:04:24.240]   - Start nice and light and easy.
[00:04:24.240 --> 00:04:28.320]   - Nothing I ask you today has actually proven answer,
[00:04:28.320 --> 00:04:29.760]   so we're just hypothesizing.
[00:04:29.760 --> 00:04:31.680]   - So yeah, I mean, I should clarify,
[00:04:31.680 --> 00:04:33.800]   this is all speculation and I'm not an expert
[00:04:33.800 --> 00:04:36.600]   in any of these topics and I'm not God.
[00:04:36.600 --> 00:04:41.160]   But I think that consciousness is probably
[00:04:41.160 --> 00:04:45.720]   something that can be fully explained
[00:04:45.720 --> 00:04:49.080]   within the laws of physics.
[00:04:49.080 --> 00:04:52.720]   I think that our bodies and brains and the universe
[00:04:52.720 --> 00:04:57.080]   and at the quantum level is so rich and complex,
[00:04:57.080 --> 00:04:59.320]   I'd be surprised if we couldn't find a room
[00:04:59.320 --> 00:05:00.720]   for consciousness there.
[00:05:00.720 --> 00:05:05.200]   And why should we be conscious?
[00:05:05.200 --> 00:05:06.960]   Why are we aware of ourselves?
[00:05:06.960 --> 00:05:11.200]   That is a very strange and interesting
[00:05:11.200 --> 00:05:12.720]   and important question.
[00:05:12.720 --> 00:05:15.700]   And I think for the next few thousand years,
[00:05:15.700 --> 00:05:20.240]   we're going to have to believe in answers purely on faith.
[00:05:20.240 --> 00:05:25.120]   But my guess is that we will find that
[00:05:25.120 --> 00:05:27.120]   within the configuration space
[00:05:27.120 --> 00:05:29.880]   of possible arrangements of the universe,
[00:05:29.880 --> 00:05:34.280]   there are some that contain memories of others.
[00:05:34.280 --> 00:05:38.180]   Literally, Julian Barber calls them time capsule states
[00:05:38.180 --> 00:05:40.440]   where you're like, yeah, not only do I have a scratch
[00:05:40.440 --> 00:05:43.200]   on my arm, but also this state of the universe
[00:05:43.200 --> 00:05:45.760]   also contains a memory in my head
[00:05:45.760 --> 00:05:48.600]   of being scratched by my cat three days ago.
[00:05:48.600 --> 00:05:52.960]   And for some reason, those kinds of states of the universe
[00:05:52.960 --> 00:05:55.800]   are more plentiful or more likely.
[00:05:55.800 --> 00:05:57.160]   - When you say those states,
[00:05:57.160 --> 00:06:00.020]   the ones that contain memories of its past
[00:06:00.020 --> 00:06:02.780]   or ones that contain memories of its past
[00:06:02.780 --> 00:06:05.420]   and have degrees of consciousness?
[00:06:05.420 --> 00:06:06.840]   - Just the first part,
[00:06:06.840 --> 00:06:10.240]   because I think the consciousness then emerges
[00:06:10.240 --> 00:06:13.200]   from the fact that a state of the universe
[00:06:13.200 --> 00:06:18.200]   that contains fragments or memories of other states
[00:06:18.200 --> 00:06:22.600]   is one where you're going to feel like there's time.
[00:06:22.600 --> 00:06:24.160]   You're going to feel like, yeah,
[00:06:24.160 --> 00:06:26.440]   things happened in the past.
[00:06:26.440 --> 00:06:28.000]   And I don't know what'll happen in the future
[00:06:28.000 --> 00:06:29.640]   because these states don't contain information
[00:06:29.640 --> 00:06:30.800]   about the future.
[00:06:30.800 --> 00:06:34.040]   For some reason, those kinds of states
[00:06:34.040 --> 00:06:38.280]   are either more common, more plentiful,
[00:06:38.280 --> 00:06:40.200]   or you could use the anthropic principle
[00:06:40.200 --> 00:06:42.520]   and just say, well, they're extremely rare.
[00:06:42.520 --> 00:06:45.760]   But until you are in one, or if you are in one,
[00:06:45.760 --> 00:06:46.880]   then you can ask questions
[00:06:46.880 --> 00:06:48.860]   like you're asking me on this podcast.
[00:06:48.860 --> 00:06:50.720]   - Why questions?
[00:06:50.720 --> 00:06:52.600]   - But yeah, it's like, why are we conscious?
[00:06:52.600 --> 00:06:53.440]   Well, because if we weren't,
[00:06:53.440 --> 00:06:55.240]   we wouldn't be asking why we were.
[00:06:55.240 --> 00:06:59.520]   - You've kind of implied that you have a sense,
[00:06:59.520 --> 00:07:02.400]   again, hypothesis, theorizing,
[00:07:02.400 --> 00:07:05.580]   that the universe is deterministic.
[00:07:05.580 --> 00:07:08.080]   What's your thoughts about free will?
[00:07:08.080 --> 00:07:10.360]   Do you think of the universe as deterministic
[00:07:10.360 --> 00:07:14.040]   in a sense that it's unrolling a particular,
[00:07:14.040 --> 00:07:17.960]   like it's operating under a specific set of physical laws,
[00:07:17.960 --> 00:07:21.380]   and when you have set the initial conditions,
[00:07:21.380 --> 00:07:23.540]   it will unroll in the exact same way
[00:07:23.540 --> 00:07:28.480]   in our particular line of the universe every time?
[00:07:28.480 --> 00:07:31.660]   - That is a very useful way to think about the universe.
[00:07:31.660 --> 00:07:32.500]   It's done us well.
[00:07:32.500 --> 00:07:33.780]   It's brought us to the moon.
[00:07:33.780 --> 00:07:35.980]   It's brought us to where we are today, right?
[00:07:36.960 --> 00:07:40.320]   I would not say that I believe in determinism
[00:07:40.320 --> 00:07:43.240]   in that kind of an absolute form,
[00:07:43.240 --> 00:07:45.600]   or actually, I just don't care.
[00:07:45.600 --> 00:07:46.780]   Maybe it's true,
[00:07:46.780 --> 00:07:48.880]   but I'm not gonna live my life like it is.
[00:07:48.880 --> 00:07:50.940]   - What in your sense,
[00:07:50.940 --> 00:07:52.120]   'cause you've studied kind of
[00:07:52.120 --> 00:07:55.880]   how we humans think of the world,
[00:07:55.880 --> 00:07:58.000]   what's in your view is the difference
[00:07:58.000 --> 00:07:59.260]   between our perception,
[00:07:59.260 --> 00:08:02.360]   like how we think the world is, and reality?
[00:08:02.360 --> 00:08:04.180]   Do you think there's a huge gap there?
[00:08:04.180 --> 00:08:05.440]   Like we delude ourselves
[00:08:05.440 --> 00:08:07.280]   that the whole thing is an illusion,
[00:08:07.280 --> 00:08:09.400]   just everything about human psychology,
[00:08:09.400 --> 00:08:10.760]   the way we see things,
[00:08:10.760 --> 00:08:12.920]   and how things actually are.
[00:08:12.920 --> 00:08:14.120]   All the things you've studied,
[00:08:14.120 --> 00:08:14.960]   what's your sense?
[00:08:14.960 --> 00:08:16.960]   How big is the gap between reality and perception?
[00:08:16.960 --> 00:08:18.960]   - Well, again, purely speculative.
[00:08:18.960 --> 00:08:21.000]   I think that we will never know the answer.
[00:08:21.000 --> 00:08:22.640]   We cannot know the answer.
[00:08:22.640 --> 00:08:27.000]   There is no experiment to find an answer to that question.
[00:08:27.000 --> 00:08:30.280]   Everything we experience is an event in our brain.
[00:08:30.280 --> 00:08:32.900]   When I look at a cat, I'm not even,
[00:08:34.100 --> 00:08:36.700]   I can't prove that there's a cat there.
[00:08:36.700 --> 00:08:40.940]   All I am experiencing is the perception of a cat
[00:08:40.940 --> 00:08:43.140]   inside my own brain.
[00:08:43.140 --> 00:08:46.340]   I am only a witness to the events of my mind.
[00:08:46.340 --> 00:08:49.340]   I think it is very useful to infer
[00:08:49.340 --> 00:08:54.100]   that if I witness the event of cat in my head,
[00:08:54.100 --> 00:08:57.220]   it's because I'm looking at a cat that is literally there
[00:08:57.220 --> 00:09:00.060]   and has its own feelings and motivations
[00:09:00.060 --> 00:09:03.100]   and should be pet and given food and water and love.
[00:09:03.100 --> 00:09:05.900]   I think that's the way you should live your life.
[00:09:05.900 --> 00:09:09.460]   But whether or not we live in a simulation
[00:09:09.460 --> 00:09:13.020]   on the brain in a vat, I don't know.
[00:09:13.020 --> 00:09:13.860]   - Do you care?
[00:09:13.860 --> 00:09:17.460]   - I don't really, well, I care
[00:09:17.460 --> 00:09:19.540]   because it's a fascinating question.
[00:09:19.540 --> 00:09:22.020]   And it's a fantastic way to get people excited
[00:09:22.020 --> 00:09:26.220]   about all kinds of topics, physics, psychology,
[00:09:26.220 --> 00:09:28.060]   consciousness, philosophy.
[00:09:28.060 --> 00:09:31.020]   But at the end of the day, what would the difference be?
[00:09:31.020 --> 00:09:31.840]   If you--
[00:09:31.840 --> 00:09:33.820]   - The cat needs to be fed at the end of the day.
[00:09:33.820 --> 00:09:35.660]   Otherwise, it'll be a dead cat.
[00:09:35.660 --> 00:09:38.460]   - Right, but if it's not even a real cat,
[00:09:38.460 --> 00:09:40.380]   then it's just like a video game cat.
[00:09:40.380 --> 00:09:42.460]   And, right, so what's the difference
[00:09:42.460 --> 00:09:45.820]   between killing a digital cat in a video game
[00:09:45.820 --> 00:09:48.340]   because of neglect versus a real cat?
[00:09:48.340 --> 00:09:50.580]   It seems very different to us psychologically.
[00:09:50.580 --> 00:09:51.940]   Like I don't really feel bad about,
[00:09:51.940 --> 00:09:54.420]   oh my gosh, I forgot to feed my Tamagotchi, right?
[00:09:54.420 --> 00:09:55.820]   But I would feel terrible
[00:09:55.820 --> 00:09:58.980]   if I forgot to feed my actual cats.
[00:09:58.980 --> 00:10:03.440]   - So can you just touch on the topic of simulation?
[00:10:03.440 --> 00:10:05.640]   Do you find this thought experiment
[00:10:05.640 --> 00:10:08.800]   that we're living in a simulation useful,
[00:10:08.800 --> 00:10:11.560]   inspiring, or constructive in any kind of way?
[00:10:11.560 --> 00:10:12.960]   Do you think it's ridiculous?
[00:10:12.960 --> 00:10:15.000]   Do you think it could be true?
[00:10:15.000 --> 00:10:17.520]   Or is it just a useful thought experiment?
[00:10:17.520 --> 00:10:20.920]   - I think it is extremely useful as a thought experiment
[00:10:20.920 --> 00:10:24.680]   because it makes sense to everyone,
[00:10:24.680 --> 00:10:28.520]   especially as we see virtual reality and computer games
[00:10:28.520 --> 00:10:30.600]   getting more and more complex.
[00:10:30.600 --> 00:10:33.880]   You're not talking to an audience in like Newton's time
[00:10:33.880 --> 00:10:36.660]   where you're like, imagine a clock
[00:10:36.660 --> 00:10:38.800]   that has mechanics in it that are so complex
[00:10:38.800 --> 00:10:40.240]   that it can create love.
[00:10:40.240 --> 00:10:42.400]   And everyone's like, no.
[00:10:42.400 --> 00:10:45.920]   But today you really start to feel,
[00:10:45.920 --> 00:10:48.960]   man, at what point is this little robot friend of mine
[00:10:48.960 --> 00:10:53.960]   gonna be like someone I don't want to cancel plans with?
[00:10:56.000 --> 00:10:59.040]   And so it's a great, the thought experiment
[00:10:59.040 --> 00:11:00.320]   of do we live in a simulation?
[00:11:00.320 --> 00:11:03.800]   Am I a brain in a vat that is just being given
[00:11:03.800 --> 00:11:08.800]   electrical impulses from some nefarious other beings
[00:11:08.800 --> 00:11:11.040]   so that I believe that I live on Earth
[00:11:11.040 --> 00:11:13.100]   and that I have a body and all of this?
[00:11:13.100 --> 00:11:15.520]   And the fact that you can't prove it either way
[00:11:15.520 --> 00:11:17.520]   is a fantastic way to introduce people
[00:11:17.520 --> 00:11:19.840]   to some of the deepest questions.
[00:11:19.840 --> 00:11:23.120]   - So you mentioned a little buddy
[00:11:23.120 --> 00:11:25.720]   that you would want to cancel an appointment with.
[00:11:25.720 --> 00:11:27.820]   So that's a lot of our conversations.
[00:11:27.820 --> 00:11:30.760]   That's what my research is, it's artificial intelligence.
[00:11:30.760 --> 00:11:34.560]   And I apologize, but you're such a fun person
[00:11:34.560 --> 00:11:36.920]   to ask these big questions with.
[00:11:36.920 --> 00:11:40.240]   - Well, I hope I can give some answers that are interesting.
[00:11:40.240 --> 00:11:45.240]   - Well, because of you've sharpened your brain's ability
[00:11:45.240 --> 00:11:47.960]   to explore some of the most, some of the questions
[00:11:47.960 --> 00:11:51.160]   that many scientists are actually afraid of even touching,
[00:11:51.160 --> 00:11:52.440]   which is fascinating.
[00:11:52.440 --> 00:11:54.040]   I think you're, in that sense,
[00:11:54.040 --> 00:11:56.640]   ultimately a great scientist
[00:11:56.640 --> 00:11:58.960]   through this process of sharpening your brain.
[00:11:58.960 --> 00:12:01.760]   - Well, I don't know if I am a scientist.
[00:12:01.760 --> 00:12:04.960]   I think science is a way of knowing.
[00:12:04.960 --> 00:12:09.480]   And there are a lot of questions I investigate
[00:12:09.480 --> 00:12:12.000]   that are not scientific questions.
[00:12:12.000 --> 00:12:14.240]   On like mine field, we have definitely done
[00:12:14.240 --> 00:12:16.700]   scientific experiments and studies
[00:12:16.700 --> 00:12:18.800]   that had hypotheses and all of that.
[00:12:18.800 --> 00:12:22.520]   But not to be too precious about
[00:12:22.520 --> 00:12:24.240]   what does the word science mean.
[00:12:24.240 --> 00:12:27.640]   But I think I would just describe myself as curious,
[00:12:27.640 --> 00:12:29.960]   and I hope that that curiosity is contagious.
[00:12:29.960 --> 00:12:31.880]   - So to you, the scientific method
[00:12:31.880 --> 00:12:33.760]   is deeply connected to science,
[00:12:33.760 --> 00:12:38.320]   because your curiosity took you to asking questions.
[00:12:38.320 --> 00:12:40.680]   To me, asking a good question,
[00:12:40.680 --> 00:12:45.440]   even if you feel, society feels that it's not a question
[00:12:45.440 --> 00:12:47.360]   within the reach of science currently,
[00:12:47.360 --> 00:12:51.360]   to me, asking the question is the biggest step
[00:12:51.360 --> 00:12:53.360]   of the scientific process.
[00:12:53.360 --> 00:12:57.280]   The scientific method is the second part,
[00:12:57.280 --> 00:12:59.440]   and that may be what traditionally is called science.
[00:12:59.440 --> 00:13:00.880]   But to me, asking the questions,
[00:13:00.880 --> 00:13:03.040]   being brave enough to ask the questions,
[00:13:03.040 --> 00:13:05.040]   being curious and not constrained
[00:13:05.040 --> 00:13:07.360]   by what you're supposed to think,
[00:13:07.360 --> 00:13:11.600]   is just true what it means to be a scientist to me.
[00:13:11.600 --> 00:13:15.280]   - It's certainly a huge part of what it means to be a human.
[00:13:15.280 --> 00:13:19.120]   If I were to say, you know what, I don't believe in forces.
[00:13:19.120 --> 00:13:22.160]   I think that when I push on a massive object,
[00:13:22.160 --> 00:13:25.720]   a ghost leaves my body and enters the object I'm pushing,
[00:13:25.720 --> 00:13:28.080]   and these ghosts happen to just get really lazy
[00:13:28.080 --> 00:13:29.800]   when they're around massive things.
[00:13:29.800 --> 00:13:31.260]   And that's why F equals MA.
[00:13:31.260 --> 00:13:34.400]   Oh, and by the way, the laziness of the ghost
[00:13:34.400 --> 00:13:36.360]   is in proportion to the mass of the object.
[00:13:36.360 --> 00:13:37.800]   So boom, proved me wrong.
[00:13:37.800 --> 00:13:41.100]   Every experiment, well, you can never find the ghost.
[00:13:41.100 --> 00:13:45.920]   And so none of that theory is scientific.
[00:13:45.920 --> 00:13:49.440]   But once I start saying, can I see the ghost?
[00:13:49.440 --> 00:13:50.920]   Why should there be a ghost?
[00:13:50.920 --> 00:13:53.120]   And if there aren't ghosts, what might I expect?
[00:13:53.120 --> 00:13:56.580]   And I start to do different tests to see,
[00:13:56.580 --> 00:13:58.040]   is this falsifiable?
[00:13:58.040 --> 00:14:01.520]   Are there things that should happen if there are ghosts,
[00:14:01.520 --> 00:14:02.780]   or are there things that shouldn't happen?
[00:14:02.780 --> 00:14:05.160]   And do they, you know, what do I observe?
[00:14:05.160 --> 00:14:06.960]   Now I'm thinking scientifically.
[00:14:06.960 --> 00:14:11.000]   I don't think of science as, wow, a picture of a black hole.
[00:14:11.000 --> 00:14:12.280]   That's just a photograph.
[00:14:12.280 --> 00:14:13.100]   That's an image.
[00:14:13.100 --> 00:14:16.480]   That's data, that's a sensory and perception experience.
[00:14:16.480 --> 00:14:19.640]   Science is how we got that and how we understand it
[00:14:19.640 --> 00:14:22.180]   and how we believe in it and how we reduce our uncertainty
[00:14:22.180 --> 00:14:24.040]   around what it means.
[00:14:24.040 --> 00:14:26.060]   - But I would say I'm deeply
[00:14:26.060 --> 00:14:28.260]   within the scientific community
[00:14:28.260 --> 00:14:31.600]   and I'm sometimes disheartened by the elitism
[00:14:31.600 --> 00:14:34.640]   of the thinking, sort of not allowing yourself
[00:14:34.640 --> 00:14:36.280]   to think outside the box.
[00:14:36.280 --> 00:14:38.860]   So allowing the possibility of going against
[00:14:38.860 --> 00:14:40.380]   the conventions of science, I think,
[00:14:40.980 --> 00:14:43.940]   is a beautiful part of some
[00:14:43.940 --> 00:14:46.300]   of the greatest scientists in history.
[00:14:46.300 --> 00:14:47.140]   - I don't know.
[00:14:47.140 --> 00:14:49.860]   I'm impressed by scientists every day.
[00:14:49.860 --> 00:14:54.860]   And revolutions in our knowledge of the world occur
[00:14:54.860 --> 00:15:00.760]   only under very special circumstances.
[00:15:00.760 --> 00:15:05.700]   It is very scary to challenge conventional thinking
[00:15:05.700 --> 00:15:10.240]   and risky because, let's go back to elitism and ego,
[00:15:10.240 --> 00:15:11.460]   right, if you just say, you know what,
[00:15:11.460 --> 00:15:14.020]   I believe in the spirits of my body
[00:15:14.020 --> 00:15:17.900]   and all forces are actually created by invisible creatures
[00:15:17.900 --> 00:15:21.160]   that transfer themselves between objects.
[00:15:21.160 --> 00:15:26.700]   If you ridicule every other theory
[00:15:26.700 --> 00:15:28.820]   and say that you're correct,
[00:15:28.820 --> 00:15:31.700]   then ego gets involved and you just don't go anywhere.
[00:15:31.700 --> 00:15:34.340]   But fundamentally, the question of, well,
[00:15:34.340 --> 00:15:38.580]   what is a force is incredibly important.
[00:15:38.580 --> 00:15:40.040]   We need to have that conversation,
[00:15:40.040 --> 00:15:42.660]   but it needs to be done in this very political way
[00:15:42.660 --> 00:15:44.660]   of like, let's be respectful of everyone
[00:15:44.660 --> 00:15:46.980]   and let's realize that we're all learning together
[00:15:46.980 --> 00:15:49.060]   and not shutting out other people.
[00:15:49.060 --> 00:15:54.060]   And so when you look at a lot of revolutionary ideas,
[00:15:54.060 --> 00:15:57.500]   they were not accepted right away.
[00:15:57.500 --> 00:16:00.580]   And, you know, Galileo had a couple of problems
[00:16:00.580 --> 00:16:04.220]   with the authorities and later thinkers,
[00:16:04.220 --> 00:16:05.580]   Descartes was like, all right, look,
[00:16:05.580 --> 00:16:07.020]   I kind of agree with Galileo,
[00:16:07.020 --> 00:16:11.400]   but I'm gonna have to not say that.
[00:16:11.400 --> 00:16:13.760]   I'll have to create and invent and write different things
[00:16:13.760 --> 00:16:15.120]   that keep me from being in trouble,
[00:16:15.120 --> 00:16:17.280]   but we still slowly made progress.
[00:16:17.280 --> 00:16:19.280]   - Revolutions are difficult in all forms
[00:16:19.280 --> 00:16:20.480]   and certainly in science.
[00:16:20.480 --> 00:16:23.840]   Before we get to AI, on topic of revolutionary ideas,
[00:16:23.840 --> 00:16:26.640]   let me ask, on a Reddit AMA,
[00:16:26.640 --> 00:16:28.960]   you said that is the earth flat
[00:16:28.960 --> 00:16:31.600]   is one of the favorite questions you've ever answered.
[00:16:31.600 --> 00:16:33.640]   Speaking of revolutionary ideas.
[00:16:33.640 --> 00:16:36.020]   So your video on that,
[00:16:36.020 --> 00:16:38.820]   people should definitely watch, is really fascinating.
[00:16:38.820 --> 00:16:41.680]   Can you elaborate why you enjoyed
[00:16:41.680 --> 00:16:43.820]   answering this question so much?
[00:16:43.820 --> 00:16:45.640]   - Yeah, well, it's a long story.
[00:16:45.640 --> 00:16:49.400]   I remember a long time ago,
[00:16:49.400 --> 00:16:50.940]   I was living in New York at the time.
[00:16:50.940 --> 00:16:53.980]   So it had to have been like 2009 or something.
[00:16:53.980 --> 00:16:57.600]   I visited the flat earth forums
[00:16:57.600 --> 00:17:00.280]   and this was before the flat earth theories
[00:17:00.280 --> 00:17:03.160]   became as sort of mainstream as they are.
[00:17:03.160 --> 00:17:05.400]   - Sorry to ask the dumb question.
[00:17:05.400 --> 00:17:06.740]   Forums, online forums.
[00:17:06.740 --> 00:17:09.060]   - Yeah, the flat earth society.
[00:17:09.060 --> 00:17:10.540]   I don't know if it's .com or .org,
[00:17:10.540 --> 00:17:14.220]   but I went there and I was reading their ideas
[00:17:14.220 --> 00:17:17.900]   and how they responded to typical criticisms of,
[00:17:17.900 --> 00:17:20.300]   well, the earth isn't flat because what about this?
[00:17:20.300 --> 00:17:23.800]   And I could not tell, and I mentioned this in my video,
[00:17:23.800 --> 00:17:28.680]   I couldn't tell how many of these community members
[00:17:28.680 --> 00:17:32.420]   actually believe the earth was flat or were just trolling.
[00:17:32.420 --> 00:17:35.300]   And I realized that the fascinating thing
[00:17:35.300 --> 00:17:38.500]   is how do we know anything?
[00:17:38.500 --> 00:17:41.660]   And what makes for a good belief
[00:17:41.660 --> 00:17:45.460]   versus a maybe not so tenable or good belief.
[00:17:45.460 --> 00:17:47.600]   And so that's really what my video
[00:17:47.600 --> 00:17:49.960]   about earth being flat is about.
[00:17:49.960 --> 00:17:52.460]   It's about, look, there are a lot of reasons.
[00:17:52.460 --> 00:17:55.720]   The earth is probably not flat,
[00:17:55.720 --> 00:18:00.640]   but a flat earth believer can respond
[00:18:00.640 --> 00:18:04.140]   to every single one of them, but it's all in an ad hoc way.
[00:18:04.140 --> 00:18:06.540]   And all of their rebuttals aren't necessarily
[00:18:06.540 --> 00:18:10.780]   gonna form a cohesive non-contradictory whole.
[00:18:10.780 --> 00:18:13.100]   And I believe that's the episode where I talk
[00:18:13.100 --> 00:18:17.420]   about Occam's razor and Newton's flaming laser sword.
[00:18:17.420 --> 00:18:19.500]   And then I say, well, you know what, wait a second,
[00:18:19.500 --> 00:18:24.500]   we know that space contracts as you move.
[00:18:24.500 --> 00:18:27.220]   And so to a particle moving near the speed of light
[00:18:27.220 --> 00:18:29.620]   towards earth, earth would be flattened
[00:18:29.620 --> 00:18:32.220]   in the direction of that particle's travel.
[00:18:32.220 --> 00:18:35.540]   So to them, earth is flat.
[00:18:35.540 --> 00:18:40.540]   Like we need to be really generous to even wild ideas
[00:18:40.540 --> 00:18:43.840]   because they're all thinking.
[00:18:43.840 --> 00:18:45.860]   They're all the communication of ideas
[00:18:45.860 --> 00:18:48.300]   and what else can it mean to be a human?
[00:18:48.300 --> 00:18:51.000]   - Yeah, and I think I'm a huge fan
[00:18:51.000 --> 00:18:54.900]   of the flat earth theory, quote unquote,
[00:18:54.900 --> 00:18:57.540]   in the sense that to me, it feels harmless
[00:18:57.540 --> 00:18:59.020]   to explore some of the questions
[00:18:59.020 --> 00:19:00.500]   of what it means to believe something,
[00:19:00.500 --> 00:19:04.700]   what it means to explore the edge of science and so on.
[00:19:04.700 --> 00:19:07.140]   It's 'cause it's a harm, it's to me,
[00:19:07.140 --> 00:19:09.680]   nobody gets hurt whether the earth is flat or round,
[00:19:09.680 --> 00:19:11.820]   not literally, but I mean, intellectually
[00:19:11.820 --> 00:19:13.340]   when we're just having a conversation.
[00:19:13.340 --> 00:19:15.760]   That said, again, to elitism,
[00:19:15.760 --> 00:19:20.060]   I find that scientists roll their eyes way too fast
[00:19:20.060 --> 00:19:21.420]   on the flat earth.
[00:19:21.420 --> 00:19:25.540]   The kind of dismissal that I see to this even notion,
[00:19:25.540 --> 00:19:27.780]   they haven't like sat down and say,
[00:19:27.780 --> 00:19:30.260]   what are the arguments that are being proposed?
[00:19:30.260 --> 00:19:32.580]   And this is why these arguments are incorrect.
[00:19:32.580 --> 00:19:37.460]   So that should be something that scientists should always do
[00:19:37.460 --> 00:19:42.140]   even to the most sort of ideas that seem ridiculous.
[00:19:42.140 --> 00:19:45.880]   So I like this, it's almost my test
[00:19:45.880 --> 00:19:48.420]   when I ask people what they think about flat earth theory
[00:19:48.420 --> 00:19:51.100]   to see how quickly they roll their eyes.
[00:19:51.100 --> 00:19:53.900]   - Well, yeah, I mean, let me go on record
[00:19:53.900 --> 00:19:58.380]   and say that the earth is not flat.
[00:19:58.380 --> 00:20:02.020]   It is a three-dimensional spheroid.
[00:20:02.020 --> 00:20:06.980]   However, I don't know that and it has not been proven.
[00:20:06.980 --> 00:20:08.820]   Science doesn't prove anything.
[00:20:08.820 --> 00:20:10.660]   It just reduces uncertainty.
[00:20:10.660 --> 00:20:12.300]   Could the earth actually be flat?
[00:20:12.300 --> 00:20:18.940]   Extremely unlikely, extremely unlikely.
[00:20:18.940 --> 00:20:21.660]   And so it is a ridiculous notion
[00:20:21.660 --> 00:20:26.660]   if we care about how probable and certain our ideas might be
[00:20:26.660 --> 00:20:28.380]   but I think it's incredibly important
[00:20:28.380 --> 00:20:32.100]   to talk about science in that way
[00:20:32.100 --> 00:20:35.260]   and to not resort to, well, it's true.
[00:20:35.260 --> 00:20:38.260]   It's true in the same way
[00:20:38.260 --> 00:20:41.300]   that a mathematical theorem is true.
[00:20:41.300 --> 00:20:46.300]   And I think we're kind of like being pretty pedantic
[00:20:46.300 --> 00:20:49.100]   about defining this stuff, but like,
[00:20:49.100 --> 00:20:51.660]   sure, I could take a rocket ship out
[00:20:51.660 --> 00:20:53.680]   and I could orbit earth and look at it
[00:20:53.680 --> 00:20:55.900]   and it would look like a ball, right?
[00:20:56.900 --> 00:20:59.380]   But I still can't prove that I'm not living in a simulation,
[00:20:59.380 --> 00:21:00.540]   that I'm not a brain in a vat,
[00:21:00.540 --> 00:21:02.660]   that this isn't all an elaborate ruse
[00:21:02.660 --> 00:21:04.580]   created by some technologically advanced
[00:21:04.580 --> 00:21:06.540]   extraterrestrial civilization.
[00:21:06.540 --> 00:21:11.020]   So there's always some doubt and that's fine.
[00:21:11.020 --> 00:21:12.300]   That's exciting.
[00:21:12.300 --> 00:21:14.020]   - And I think that kind of doubt,
[00:21:14.020 --> 00:21:15.380]   practically speaking, is useful
[00:21:15.380 --> 00:21:17.700]   when you start talking about quantum mechanics
[00:21:17.700 --> 00:21:20.340]   or string theory, sort of, it helps.
[00:21:20.340 --> 00:21:21.900]   To me, that kind of little,
[00:21:21.900 --> 00:21:25.460]   adds a little spice into the thinking process
[00:21:25.460 --> 00:21:26.460]   of scientists.
[00:21:26.460 --> 00:21:30.060]   So, I mean, just as a thought experiment,
[00:21:30.060 --> 00:21:33.420]   your video kind of, okay, say the earth is flat,
[00:21:33.420 --> 00:21:35.940]   what would the forces when you walk about
[00:21:35.940 --> 00:21:38.380]   this flat earth feel like to the human?
[00:21:38.380 --> 00:21:40.620]   That's a really nice thought experiment to think about.
[00:21:40.620 --> 00:21:42.420]   - Right, 'cause what's really nice about it
[00:21:42.420 --> 00:21:45.380]   is that it's a funny thought experiment,
[00:21:45.380 --> 00:21:48.060]   but you actually wind up accidentally learning
[00:21:48.060 --> 00:21:51.620]   a whole lot about gravity and about relativity
[00:21:51.620 --> 00:21:55.700]   and geometry and I think that's really the goal
[00:21:55.700 --> 00:21:56.540]   of what I'm doing.
[00:21:56.540 --> 00:21:58.820]   I'm not trying to convince people that the earth is round.
[00:21:58.820 --> 00:22:01.300]   I feel like you either believe that it is or you don't
[00:22:01.300 --> 00:22:04.660]   and that's, you know, how can I change that?
[00:22:04.660 --> 00:22:06.900]   What I can do is change how you think
[00:22:06.900 --> 00:22:10.940]   and how you are introduced to important concepts
[00:22:10.940 --> 00:22:13.740]   like, well, how does gravity operate?
[00:22:13.740 --> 00:22:16.500]   Oh, it's all about the center of mass of an object.
[00:22:16.500 --> 00:22:17.780]   So right, on a sphere,
[00:22:17.780 --> 00:22:19.460]   we're all pulled towards the middle,
[00:22:19.460 --> 00:22:21.460]   essentially the centroid, geometrically,
[00:22:21.460 --> 00:22:23.820]   but on a disc, ooh, you're gonna be pulled
[00:22:23.820 --> 00:22:25.940]   at a weird angle if you're out near the edge
[00:22:25.940 --> 00:22:28.420]   and that stuff's fascinating.
[00:22:28.420 --> 00:22:32.900]   - Yeah, and to me, that particular video
[00:22:32.900 --> 00:22:37.500]   opened my eyes even more to what gravity is.
[00:22:37.500 --> 00:22:40.020]   It's just a really nice visualization tool
[00:22:40.020 --> 00:22:43.060]   'cause you always imagine gravity with spheres,
[00:22:43.060 --> 00:22:44.620]   with masses that are spheres.
[00:22:44.620 --> 00:22:46.340]   - Yeah. - And imagining gravity
[00:22:46.340 --> 00:22:49.740]   on masses that are not spherical, some other shape,
[00:22:49.740 --> 00:22:54.100]   but in here, a plate, a flat object is really interesting.
[00:22:54.100 --> 00:22:56.300]   It makes you really kind of visualize
[00:22:56.300 --> 00:22:57.740]   in a three-dimensional way the force.
[00:22:57.740 --> 00:23:02.740]   - Yeah, even if a disc, the size of Earth would be impossible.
[00:23:02.740 --> 00:23:09.020]   I think anything larger than like the moon, basically,
[00:23:09.020 --> 00:23:13.740]   needs to be a sphere because gravity will round it out.
[00:23:13.740 --> 00:23:18.140]   So you can't have a teacup the size of Jupiter, right?
[00:23:18.140 --> 00:23:21.020]   There's a great book about a teacup in the universe
[00:23:21.020 --> 00:23:22.900]   that I highly recommend.
[00:23:22.900 --> 00:23:24.700]   I don't remember the author.
[00:23:24.700 --> 00:23:26.780]   I forget her name, but it's a wonderful book,
[00:23:26.780 --> 00:23:28.180]   so look it up.
[00:23:28.180 --> 00:23:30.180]   I think it's called "Teacup in the Universe."
[00:23:30.180 --> 00:23:32.680]   - Just to link on this point briefly,
[00:23:32.680 --> 00:23:37.100]   your videos are generally super, people love them, right?
[00:23:37.100 --> 00:23:39.940]   If you look at the sort of number of likes versus dislikes,
[00:23:39.940 --> 00:23:44.940]   this measure of YouTube, right, is incredible, as do I,
[00:23:45.220 --> 00:23:48.180]   but this particular flat Earth video
[00:23:48.180 --> 00:23:51.380]   has more dislikes than usual.
[00:23:51.380 --> 00:23:56.920]   On that topic in general, what's your sense,
[00:23:56.920 --> 00:23:58.820]   how big is the community,
[00:23:58.820 --> 00:24:00.740]   not just who believes in flat Earth,
[00:24:00.740 --> 00:24:03.720]   but sort of the anti-scientific community
[00:24:03.720 --> 00:24:07.620]   that naturally distrusts scientists in a way
[00:24:07.620 --> 00:24:12.180]   that's not an open-minded way,
[00:24:12.180 --> 00:24:13.700]   like really just distrust scientists,
[00:24:13.700 --> 00:24:17.060]   like they're bought by some, it's the kind of mechanism
[00:24:17.060 --> 00:24:18.860]   of some kind of bigger system
[00:24:18.860 --> 00:24:21.100]   that's trying to manipulate human beings.
[00:24:21.100 --> 00:24:24.060]   What's your sense of the size of that community?
[00:24:24.060 --> 00:24:28.940]   You're one of the sort of great educators in the world
[00:24:28.940 --> 00:24:33.940]   that educates people on the exciting power of science,
[00:24:33.940 --> 00:24:38.100]   so you're kind of up against this community.
[00:24:38.100 --> 00:24:39.980]   What's your sense of it?
[00:24:39.980 --> 00:24:41.980]   - I really have no idea.
[00:24:41.980 --> 00:24:44.220]   I haven't looked at the likes and dislikes
[00:24:44.220 --> 00:24:47.300]   on the flat Earth video, and so I would wonder
[00:24:47.300 --> 00:24:51.380]   if it has a greater percentage of dislikes than usual,
[00:24:51.380 --> 00:24:53.580]   is that because of people disliking it
[00:24:53.580 --> 00:24:58.580]   because they think that it's a video about Earth being flat
[00:24:58.580 --> 00:25:02.020]   and they find that ridiculous and they dislike it
[00:25:02.020 --> 00:25:04.260]   without even really watching much?
[00:25:04.260 --> 00:25:07.380]   Do they wish that I was more dismissive
[00:25:07.380 --> 00:25:08.620]   of flat Earth theories?
[00:25:08.620 --> 00:25:10.660]   - That's possible too.
[00:25:10.660 --> 00:25:12.140]   There are a lot of response videos
[00:25:12.140 --> 00:25:17.140]   that kind of go through the episode and are pro-flat Earth,
[00:25:17.140 --> 00:25:21.900]   but I don't know if there's a larger community
[00:25:21.900 --> 00:25:25.180]   of unorthodox thinkers today
[00:25:25.180 --> 00:25:27.500]   than there have been in the past.
[00:25:27.500 --> 00:25:29.980]   And I just wanna not lose them.
[00:25:29.980 --> 00:25:32.620]   I want them to keep listening and thinking.
[00:25:32.620 --> 00:25:36.660]   And by calling them all idiots or something,
[00:25:36.660 --> 00:25:41.060]   that does no good because how idiotic are they really?
[00:25:41.060 --> 00:25:45.380]   I mean, the Earth isn't a sphere at all.
[00:25:45.380 --> 00:25:47.780]   We know that it's an oblate spheroid,
[00:25:47.780 --> 00:25:50.380]   and that in and of itself is really interesting.
[00:25:50.380 --> 00:25:52.300]   And I investigated that in which way is down,
[00:25:52.300 --> 00:25:54.260]   where I'm like, really, down does not point
[00:25:54.260 --> 00:25:56.660]   towards the center of the Earth.
[00:25:56.660 --> 00:25:58.940]   It points in a different direction
[00:25:58.940 --> 00:26:01.660]   depending on what's underneath you and what's above you
[00:26:01.660 --> 00:26:02.500]   and what's around you.
[00:26:02.500 --> 00:26:06.060]   The whole universe is tugging on me.
[00:26:06.060 --> 00:26:08.420]   - And then you also show that gravity
[00:26:08.420 --> 00:26:11.740]   is non-uniform across the globe.
[00:26:11.740 --> 00:26:14.420]   Like if you, there's this, I guess, thought experiment,
[00:26:14.420 --> 00:26:19.420]   if you build a bridge all the way across the Earth
[00:26:19.420 --> 00:26:23.500]   and then just knock out its pillars, what would happen?
[00:26:23.500 --> 00:26:24.340]   - Yeah.
[00:26:24.340 --> 00:26:25.820]   - And you describe how it would be
[00:26:25.820 --> 00:26:28.980]   like a very chaotic, unstable thing that's happening
[00:26:28.980 --> 00:26:31.820]   because gravity is non-uniform throughout the Earth.
[00:26:31.820 --> 00:26:36.620]   - Yeah, in small spaces, like the ones we work in,
[00:26:36.620 --> 00:26:39.340]   we can essentially assume that gravity is uniform.
[00:26:39.340 --> 00:26:40.900]   But it's not.
[00:26:40.900 --> 00:26:43.040]   It is weaker the further you are from the Earth.
[00:26:43.040 --> 00:26:47.580]   And it also is going to be,
[00:26:47.580 --> 00:26:50.060]   it's radially pointed towards the middle of the Earth.
[00:26:50.060 --> 00:26:54.140]   So a really large object will feel tidal forces
[00:26:54.140 --> 00:26:55.660]   because of that non-uniformness.
[00:26:55.660 --> 00:26:58.500]   And we can take advantage of that with satellites, right?
[00:26:58.500 --> 00:27:00.300]   Gravitational induced torque.
[00:27:00.300 --> 00:27:01.700]   It's a great way to align your satellite
[00:27:01.700 --> 00:27:05.580]   without having to use fuel or any kind of engine.
[00:27:05.580 --> 00:27:07.060]   - So let's jump back to it.
[00:27:07.060 --> 00:27:08.340]   Artificial intelligence.
[00:27:08.340 --> 00:27:11.180]   What's your thought of the state of where we are at
[00:27:11.180 --> 00:27:12.980]   currently with artificial intelligence?
[00:27:12.980 --> 00:27:15.900]   And what do you think it takes to build human level
[00:27:15.900 --> 00:27:17.980]   or superhuman level intelligence?
[00:27:17.980 --> 00:27:20.420]   - I don't know what intelligence means.
[00:27:20.420 --> 00:27:22.860]   That's my biggest question at the moment.
[00:27:22.860 --> 00:27:25.060]   And I think it's 'cause my instinct is always to go,
[00:27:25.060 --> 00:27:27.980]   well, what are the foundations here of our discussion?
[00:27:27.980 --> 00:27:31.060]   What does it mean to be intelligent?
[00:27:31.060 --> 00:27:35.540]   How do we measure the intelligence of an artificial machine
[00:27:35.540 --> 00:27:37.500]   or a program or something?
[00:27:37.500 --> 00:27:39.860]   - Can we say that humans are intelligent?
[00:27:39.860 --> 00:27:42.620]   Because there's also a fascinating field
[00:27:42.620 --> 00:27:44.260]   of how do you measure human intelligence?
[00:27:44.260 --> 00:27:45.380]   - Of course.
[00:27:45.380 --> 00:27:47.060]   - But if we just take that for granted,
[00:27:47.060 --> 00:27:50.020]   saying that whatever this fuzzy intelligence thing
[00:27:50.020 --> 00:27:52.220]   we're talking about, humans kind of have it.
[00:27:52.220 --> 00:27:56.660]   What would be a good test for you?
[00:27:56.660 --> 00:27:58.380]   So Turing developed a test
[00:27:58.380 --> 00:28:00.420]   that's natural language conversation.
[00:28:00.420 --> 00:28:01.580]   Would that impress you?
[00:28:01.580 --> 00:28:04.540]   A chatbot that you'd want to hang out and have a beer with
[00:28:04.540 --> 00:28:08.380]   for a bunch of hours or have dinner plans with.
[00:28:08.380 --> 00:28:09.220]   Is that a good test?
[00:28:09.220 --> 00:28:10.220]   Natural language conversation.
[00:28:10.220 --> 00:28:12.260]   Is there something else that would impress you?
[00:28:12.260 --> 00:28:13.620]   Or is that also too difficult to think about?
[00:28:13.620 --> 00:28:15.820]   - Oh yeah, I'm pretty much impressed by everything.
[00:28:15.820 --> 00:28:17.740]   I think that if-
[00:28:17.740 --> 00:28:18.740]   - Roomba?
[00:28:18.740 --> 00:28:22.180]   - If there was a chatbot that was like incredibly,
[00:28:22.180 --> 00:28:24.460]   I don't know, really had a personality.
[00:28:24.460 --> 00:28:27.940]   And if I didn't, the Turing test, right?
[00:28:27.940 --> 00:28:32.940]   Like if I'm unable to tell that it's not another person,
[00:28:32.940 --> 00:28:36.620]   but then I was shown a bunch of wires
[00:28:36.620 --> 00:28:39.100]   and mechanical components,
[00:28:39.100 --> 00:28:42.700]   and it was like, that's actually what you're talking to.
[00:28:42.700 --> 00:28:46.460]   I don't know if I would feel that guilty destroying it.
[00:28:46.460 --> 00:28:49.260]   I would feel guilty because clearly it's well-made
[00:28:49.260 --> 00:28:51.100]   and it's a really cool thing.
[00:28:51.100 --> 00:28:53.820]   It's like destroying a really cool car or something.
[00:28:53.820 --> 00:28:56.300]   But I would not feel like I was a murderer.
[00:28:56.300 --> 00:28:58.820]   So yeah, at what point would I start to feel that way?
[00:28:58.820 --> 00:29:02.700]   And this is such a subjective psychological question.
[00:29:02.700 --> 00:29:05.020]   If you give it movement,
[00:29:05.020 --> 00:29:08.620]   or if you have it act as though,
[00:29:08.620 --> 00:29:11.860]   or perhaps really feel pain as I destroy it
[00:29:11.860 --> 00:29:15.660]   and scream and resist, then I'd feel bad.
[00:29:15.660 --> 00:29:16.700]   - Yeah, it's beautifully put.
[00:29:16.700 --> 00:29:20.540]   And let's just say act like it's a pain.
[00:29:20.540 --> 00:29:25.540]   So if you just have a robot that not screams,
[00:29:25.660 --> 00:29:28.500]   just like moans in pain, if you kick it,
[00:29:28.500 --> 00:29:32.900]   that immediately just puts it in a class that we humans,
[00:29:32.900 --> 00:29:35.300]   it becomes, it anthropomorphizes it.
[00:29:35.300 --> 00:29:37.980]   It almost immediately becomes human.
[00:29:37.980 --> 00:29:39.380]   So that's a psychology question
[00:29:39.380 --> 00:29:40.980]   as opposed to sort of a physics question.
[00:29:40.980 --> 00:29:43.140]   - Right, I think that's a really good instinct to have.
[00:29:43.140 --> 00:29:44.140]   If the robot-
[00:29:44.140 --> 00:29:46.620]   - Screams.
[00:29:46.620 --> 00:29:48.480]   - Screams and moans,
[00:29:48.480 --> 00:29:52.700]   even if you don't believe that it has the mental experience,
[00:29:52.700 --> 00:29:55.340]   the qualia of pain and suffering,
[00:29:55.340 --> 00:29:56.820]   I think it's still a good instinct to say,
[00:29:56.820 --> 00:30:00.020]   you know what, I'd rather not hurt it.
[00:30:00.020 --> 00:30:02.620]   - The problem is that instinct can get us in trouble
[00:30:02.620 --> 00:30:05.620]   because then robots can manipulate that.
[00:30:05.620 --> 00:30:08.300]   And there's different kinds of robots.
[00:30:08.300 --> 00:30:10.700]   There's robots like the Facebook and the YouTube algorithm
[00:30:10.700 --> 00:30:11.980]   that recommends the video,
[00:30:11.980 --> 00:30:14.500]   and they can manipulate in the same kind of way.
[00:30:14.500 --> 00:30:16.300]   Well, let me ask you just to stick
[00:30:16.300 --> 00:30:17.900]   on artificial intelligence for a second.
[00:30:17.900 --> 00:30:21.780]   Do you have worries about existential threats from AI
[00:30:21.780 --> 00:30:23.740]   or existential threats from other technologies
[00:30:23.740 --> 00:30:27.780]   like nuclear weapons that could potentially destroy life
[00:30:27.780 --> 00:30:31.260]   on earth or damage it to a very significant degree?
[00:30:31.260 --> 00:30:32.340]   - Yeah, of course I do,
[00:30:32.340 --> 00:30:35.340]   especially the weapons that we create.
[00:30:35.340 --> 00:30:38.140]   There's all kinds of famous ways to think about this.
[00:30:38.140 --> 00:30:39.300]   And one is that, wow,
[00:30:39.300 --> 00:30:43.980]   what if we don't see advanced alien civilizations
[00:30:43.980 --> 00:30:47.980]   because of the danger of technology?
[00:30:47.980 --> 00:30:51.860]   What if we reach a point,
[00:30:51.860 --> 00:30:55.060]   and I think there's a channel, Thotty2.
[00:30:55.060 --> 00:30:57.460]   Jeez, I wish I remembered the name of the channel.
[00:30:57.460 --> 00:31:00.020]   But he delves into this kind of limit
[00:31:00.020 --> 00:31:05.020]   of maybe once you discover radioactivity and its power,
[00:31:05.020 --> 00:31:07.220]   you've reached this important hurdle.
[00:31:07.220 --> 00:31:09.460]   And the reason that the skies are so empty
[00:31:09.460 --> 00:31:13.940]   is that no one's ever managed to survive as a civilization
[00:31:13.940 --> 00:31:16.900]   once they have that destructive power.
[00:31:16.900 --> 00:31:21.900]   And when it comes to AI, I'm not really very worried
[00:31:21.900 --> 00:31:24.820]   because I think that there are plenty of other people
[00:31:24.820 --> 00:31:26.500]   that are already worried enough.
[00:31:26.500 --> 00:31:30.580]   And oftentimes these worries are just,
[00:31:30.580 --> 00:31:32.940]   they just get in the way of progress.
[00:31:32.940 --> 00:31:36.740]   And they're questions that we should address later.
[00:31:36.740 --> 00:31:42.020]   And I think I talk about this in my interview
[00:31:42.020 --> 00:31:47.020]   with the self-driving autonomous vehicle guy
[00:31:47.020 --> 00:31:48.660]   as I think it was a bonus scene
[00:31:48.660 --> 00:31:52.140]   from the "Trolley Problem" episode.
[00:31:52.140 --> 00:31:54.260]   And I'm like, wow, what should a car do
[00:31:54.260 --> 00:31:57.040]   if this really weird contrived scenario happens
[00:31:57.040 --> 00:32:00.200]   where it has to swerve and save the driver, but kill a kid?
[00:32:00.200 --> 00:32:03.540]   And he's like, well, what would a human do?
[00:32:03.540 --> 00:32:07.280]   And if we resist technological progress
[00:32:07.280 --> 00:32:10.420]   because we're worried about all of these little issues,
[00:32:10.420 --> 00:32:12.020]   then it gets in the way.
[00:32:12.020 --> 00:32:14.340]   And we shouldn't avoid those problems,
[00:32:14.340 --> 00:32:16.780]   but we shouldn't allow them to be stumbling blocks
[00:32:16.780 --> 00:32:18.940]   to advancement.
[00:32:18.940 --> 00:32:22.500]   - So the folks like Sam Harris or Elon Musk
[00:32:22.500 --> 00:32:24.540]   are saying that we're not worried enough.
[00:32:24.540 --> 00:32:28.560]   So worry should not paralyze technological progress,
[00:32:28.560 --> 00:32:32.580]   but we're sort of marching, technology is marching forward
[00:32:32.580 --> 00:32:37.580]   without the key scientists, the developing of technology,
[00:32:37.900 --> 00:32:42.100]   worrying about the overnight having some effects
[00:32:42.100 --> 00:32:45.220]   that would be very detrimental to society.
[00:32:45.220 --> 00:32:49.540]   So to push back on your thought of the idea
[00:32:49.540 --> 00:32:51.260]   that there's enough people worrying about it,
[00:32:51.260 --> 00:32:54.620]   Elon Musk says there's not enough people worrying about it.
[00:32:54.620 --> 00:32:56.600]   That's the kind of balance is,
[00:32:56.600 --> 00:33:01.260]   you know, it's like folks who really focus
[00:33:01.260 --> 00:33:03.680]   on non-nuclear deterrence are saying
[00:33:03.680 --> 00:33:04.860]   there's not enough people worried
[00:33:04.860 --> 00:33:06.100]   about nuclear deterrence, right?
[00:33:06.100 --> 00:33:10.220]   So it's an interesting question of what is a good threshold
[00:33:10.220 --> 00:33:12.580]   of people to worry about these?
[00:33:12.580 --> 00:33:15.140]   And if it's too many people that are worried, you're right.
[00:33:15.140 --> 00:33:18.740]   It'll be like the press would over report on it
[00:33:18.740 --> 00:33:21.820]   and there'll be technological, halt technological progress.
[00:33:21.820 --> 00:33:24.420]   If not enough, then we can march straight ahead
[00:33:24.420 --> 00:33:29.420]   into that abyss that human beings might be destined for
[00:33:29.420 --> 00:33:31.340]   with the progress of technology.
[00:33:31.340 --> 00:33:33.700]   - Yeah, I don't know what the right balance is
[00:33:33.700 --> 00:33:35.700]   of how many people should be worried
[00:33:35.700 --> 00:33:36.980]   and how worried should they be,
[00:33:36.980 --> 00:33:40.460]   but we're always worried about new technology.
[00:33:40.460 --> 00:33:42.980]   We know that Plato was worried about the written word.
[00:33:42.980 --> 00:33:45.020]   He's like, we shouldn't teach people to write
[00:33:45.020 --> 00:33:48.100]   because then they won't use their minds to remember things.
[00:33:48.100 --> 00:33:51.260]   There have been concerns over technology
[00:33:51.260 --> 00:33:55.140]   and its advancement since the beginning of recorded history.
[00:33:55.140 --> 00:33:59.980]   And so, I think, however, these conversations
[00:33:59.980 --> 00:34:01.100]   are really important to have,
[00:34:01.100 --> 00:34:03.420]   because again, we learn a lot about ourselves.
[00:34:03.420 --> 00:34:06.220]   If we're really scared of some kind of AI
[00:34:06.220 --> 00:34:09.380]   like coming into being that is conscious or whatever
[00:34:09.380 --> 00:34:13.180]   and can self-replicate, we already do that every day.
[00:34:13.180 --> 00:34:14.580]   It's called humans being born.
[00:34:14.580 --> 00:34:15.980]   They're not artificial.
[00:34:15.980 --> 00:34:18.100]   They're humans, but they're intelligent.
[00:34:18.100 --> 00:34:20.180]   And I don't wanna live in a world
[00:34:20.180 --> 00:34:21.980]   where we're worried about babies being born
[00:34:21.980 --> 00:34:24.260]   because what if they become evil?
[00:34:24.260 --> 00:34:25.100]   - Right.
[00:34:25.100 --> 00:34:25.940]   - What if they become mean people?
[00:34:25.940 --> 00:34:27.720]   What if they're thieves?
[00:34:27.720 --> 00:34:31.820]   Maybe we should just like, what, not have babies born?
[00:34:31.820 --> 00:34:34.020]   Like maybe we shouldn't create AI?
[00:34:34.020 --> 00:34:39.020]   It's like, we'll want to have safeguards in place
[00:34:39.020 --> 00:34:41.780]   in the same way that we know, look,
[00:34:41.780 --> 00:34:44.420]   a kid could be born that becomes some kind of evil person,
[00:34:44.420 --> 00:34:47.940]   but we have laws, right?
[00:34:47.940 --> 00:34:51.620]   - And it's possible that with advanced genetics in general,
[00:34:51.620 --> 00:34:56.620]   be able to, it's a scary thought to say that
[00:34:59.020 --> 00:35:04.020]   my child, if born, would have an 83% chance
[00:35:04.020 --> 00:35:08.740]   of being a psychopath, right?
[00:35:08.740 --> 00:35:11.500]   Like being able to, if it's something genetic,
[00:35:11.500 --> 00:35:15.060]   if there's some sort of, and what to use that information,
[00:35:15.060 --> 00:35:16.220]   what to do with that information
[00:35:16.220 --> 00:35:20.020]   is a difficult ethical thought.
[00:35:20.020 --> 00:35:22.060]   - Yeah, I'd like to find an answer that isn't,
[00:35:22.060 --> 00:35:24.940]   well, let's not have them live.
[00:35:24.940 --> 00:35:26.860]   You know, I'd like to find an answer that is,
[00:35:26.860 --> 00:35:30.340]   well, all human life is worthy.
[00:35:30.340 --> 00:35:33.640]   And if you have an 83% chance of becoming a psychopath,
[00:35:33.640 --> 00:35:37.780]   well, you still deserve dignity.
[00:35:37.780 --> 00:35:38.620]   - Yeah.
[00:35:38.620 --> 00:35:42.380]   - And you still deserve to be treated well.
[00:35:42.380 --> 00:35:43.340]   You still have rights.
[00:35:43.340 --> 00:35:45.980]   - At least at this part of the world, at least in America,
[00:35:45.980 --> 00:35:49.540]   there's a respect for individual life in that way.
[00:35:49.540 --> 00:35:52.600]   That's, well, to me, but again,
[00:35:52.600 --> 00:35:55.740]   I'm in this bubble, is a beautiful thing.
[00:35:55.740 --> 00:35:58.980]   But there's other cultures where individual human life
[00:35:58.980 --> 00:36:02.660]   is not that important, where a society,
[00:36:02.660 --> 00:36:04.700]   so I was born in Soviet Union,
[00:36:04.700 --> 00:36:07.420]   where the strength of nation and society together
[00:36:07.420 --> 00:36:10.260]   is more important than any one particular individual.
[00:36:10.260 --> 00:36:12.020]   So it's an interesting also notion,
[00:36:12.020 --> 00:36:13.500]   the stories we tell ourselves.
[00:36:13.500 --> 00:36:15.940]   I like the one where individuals matter,
[00:36:15.940 --> 00:36:19.160]   but it's unclear that that was what the future holds.
[00:36:19.160 --> 00:36:21.440]   - Well, yeah, and I mean, let me even throw this out.
[00:36:21.440 --> 00:36:23.800]   Like what is artificial intelligence?
[00:36:23.800 --> 00:36:25.180]   How can it be artificial?
[00:36:25.180 --> 00:36:28.000]   I really think that we get pretty obsessed
[00:36:28.000 --> 00:36:30.740]   and stuck on the idea that there is some thing
[00:36:30.740 --> 00:36:34.300]   that is a wild human, a pure human organism
[00:36:34.300 --> 00:36:35.780]   without technology.
[00:36:35.780 --> 00:36:38.380]   But I don't think that's a real thing.
[00:36:38.380 --> 00:36:43.380]   I think that humans and human technology are one organism.
[00:36:43.380 --> 00:36:45.220]   Look at my glasses, okay?
[00:36:45.220 --> 00:36:48.620]   If an alien came down and saw me,
[00:36:48.620 --> 00:36:51.340]   would they necessarily know that this is an invention,
[00:36:51.340 --> 00:36:53.940]   that I don't grow these organically from my body?
[00:36:53.940 --> 00:36:56.240]   They wouldn't know that right away.
[00:36:56.240 --> 00:37:01.020]   And the written word and spoons and cups,
[00:37:01.020 --> 00:37:03.140]   these are all pieces of technology.
[00:37:03.140 --> 00:37:08.520]   We are not alone as an organism.
[00:37:08.520 --> 00:37:10.860]   And so the technology we create,
[00:37:10.860 --> 00:37:13.780]   whether it be video games or artificial intelligence
[00:37:13.780 --> 00:37:15.840]   that can self-replicate and hate us,
[00:37:15.840 --> 00:37:18.700]   it's actually all the same organism.
[00:37:18.700 --> 00:37:21.040]   When you're in a car, where do you end and the car begin?
[00:37:21.040 --> 00:37:22.900]   It seems like a really easy question to answer,
[00:37:22.900 --> 00:37:24.500]   but the more you think about it,
[00:37:24.500 --> 00:37:25.740]   the more you realize, wow,
[00:37:25.740 --> 00:37:29.840]   we are in this symbiotic relationship with our inventions.
[00:37:29.840 --> 00:37:31.940]   And there are plenty of people who are worried about it,
[00:37:31.940 --> 00:37:35.700]   and there should be, but it's inevitable.
[00:37:35.700 --> 00:37:38.800]   - And I think that even just us think of ourselves
[00:37:38.800 --> 00:37:43.800]   as individual intelligences may be silly notion
[00:37:43.800 --> 00:37:48.500]   because it's much better to think of the entirety
[00:37:48.500 --> 00:37:51.100]   of human civilization, all living organisms on earth
[00:37:51.100 --> 00:37:53.740]   as a single living organism,
[00:37:53.740 --> 00:37:55.140]   as a single intelligent creature,
[00:37:55.140 --> 00:37:57.340]   'cause you're right, everything's intertwined.
[00:37:57.340 --> 00:38:00.060]   Everything is deeply connected.
[00:38:00.060 --> 00:38:01.500]   So we mentioned Elon Musk.
[00:38:01.500 --> 00:38:06.140]   So you're a curious lover of science.
[00:38:06.140 --> 00:38:09.860]   What do you think of the efforts that Elon Musk is doing
[00:38:09.860 --> 00:38:13.180]   with space exploration, with electric vehicles,
[00:38:13.180 --> 00:38:16.580]   with autopilot, sort of getting into the space
[00:38:16.580 --> 00:38:20.320]   of autonomous vehicles, with boring under LA,
[00:38:20.320 --> 00:38:24.020]   and Neuralink trying to communicate
[00:38:24.020 --> 00:38:27.100]   brain machine interfaces, communicate between machines
[00:38:27.100 --> 00:38:28.720]   and human brains?
[00:38:28.720 --> 00:38:30.940]   - Well, it's really inspiring.
[00:38:30.940 --> 00:38:35.820]   I mean, look at the fandom that he's amassed.
[00:38:35.820 --> 00:38:40.060]   It's not common for someone like that
[00:38:40.060 --> 00:38:41.460]   to have such a following.
[00:38:41.460 --> 00:38:43.240]   And so it's-- - Engineering nerd.
[00:38:43.240 --> 00:38:45.520]   - Yeah, so it's really exciting,
[00:38:45.520 --> 00:38:47.380]   but I also think that a lot of responsibility
[00:38:47.380 --> 00:38:48.540]   comes with that kind of power.
[00:38:48.540 --> 00:38:51.100]   So like if I met him, I would love to hear
[00:38:51.100 --> 00:38:53.880]   how he feels about the responsibility he has.
[00:38:53.880 --> 00:39:00.020]   When there are people who are such a fan of your ideas
[00:39:00.020 --> 00:39:04.660]   and your dreams and share them so closely with you,
[00:39:04.660 --> 00:39:06.460]   you have a lot of power.
[00:39:06.460 --> 00:39:09.720]   And he didn't always have that.
[00:39:09.720 --> 00:39:12.040]   He wasn't born as Elon Musk.
[00:39:12.040 --> 00:39:14.000]   Well, he was, but well, he was named that later.
[00:39:14.000 --> 00:39:18.460]   But the point is that I wanna know
[00:39:18.460 --> 00:39:23.460]   the psychology of becoming a figure like him.
[00:39:23.460 --> 00:39:25.700]   Well, I don't even know how to phrase the question right,
[00:39:25.700 --> 00:39:28.020]   but it's a question about what do you do
[00:39:28.020 --> 00:39:33.020]   when you're following, your fans become so large
[00:39:33.020 --> 00:39:38.000]   that it's almost bigger than you?
[00:39:38.000 --> 00:39:41.060]   And how do you responsibly manage that?
[00:39:41.060 --> 00:39:43.540]   And maybe it doesn't worry him at all, and that's fine too.
[00:39:43.540 --> 00:39:45.500]   But I'd be really curious.
[00:39:45.500 --> 00:39:47.700]   And I think there are a lot of people that go through this
[00:39:47.700 --> 00:39:50.420]   when they realize, whoa, there are a lot of eyes on me.
[00:39:50.420 --> 00:39:53.940]   There are a lot of people who really take what I say
[00:39:53.940 --> 00:39:57.740]   very earnestly and take it to heart and will defend me.
[00:39:57.740 --> 00:40:00.240]   And whew, that can be dangerous.
[00:40:00.240 --> 00:40:07.540]   And you have to be responsible with it.
[00:40:07.540 --> 00:40:09.300]   - Both in terms of impact on society
[00:40:09.300 --> 00:40:11.300]   and psychologically for the individual,
[00:40:11.300 --> 00:40:15.060]   just the burden psychologically on Elon?
[00:40:15.060 --> 00:40:18.860]   - Yeah, yeah, how does he think about that
[00:40:18.860 --> 00:40:21.220]   part of his persona?
[00:40:21.220 --> 00:40:23.380]   - Well, let me throw that right back at you
[00:40:23.380 --> 00:40:27.540]   because in some ways you're just a funny guy
[00:40:27.540 --> 00:40:31.700]   that's gotten a humongous following,
[00:40:31.700 --> 00:40:33.580]   a funny guy with a curiosity.
[00:40:33.580 --> 00:40:36.580]   You've got a huge following.
[00:40:36.580 --> 00:40:40.100]   How do you psychologically deal with the responsibility?
[00:40:40.100 --> 00:40:42.020]   In many ways, you have a reach
[00:40:42.020 --> 00:40:44.560]   in many ways bigger than Elon Musk.
[00:40:44.560 --> 00:40:49.340]   What is the burden that you feel in educating,
[00:40:49.340 --> 00:40:51.980]   being one of the biggest educators in the world
[00:40:51.980 --> 00:40:53.500]   where everybody's listening to you?
[00:40:53.500 --> 00:40:58.380]   And actually everybody, like most of the world
[00:40:58.380 --> 00:41:01.060]   that uses YouTube for educational material
[00:41:01.060 --> 00:41:05.980]   trust you as a source of good, strong scientific thinking.
[00:41:05.980 --> 00:41:11.020]   - It's a burden and I try to approach it
[00:41:11.020 --> 00:41:16.020]   with a lot of humility and sharing.
[00:41:16.020 --> 00:41:20.320]   I'm not out there doing a lot of scientific experiments.
[00:41:20.320 --> 00:41:23.200]   I am sharing the work of real scientists
[00:41:23.200 --> 00:41:26.440]   and I'm celebrating their work and the way that they think
[00:41:26.440 --> 00:41:29.480]   and the power of curiosity.
[00:41:29.480 --> 00:41:32.200]   But I wanna make it clear at all times that like,
[00:41:32.200 --> 00:41:35.240]   look, we don't know all the answers
[00:41:35.240 --> 00:41:37.640]   and I don't think we're ever going to reach a point
[00:41:37.640 --> 00:41:39.480]   where we're like, wow, and there you go.
[00:41:39.480 --> 00:41:40.880]   That's the universe.
[00:41:40.880 --> 00:41:42.740]   You solve this equation, you plug in some conditions
[00:41:42.740 --> 00:41:44.640]   or whatever and you do the math
[00:41:44.640 --> 00:41:46.120]   and you know what's gonna happen tomorrow.
[00:41:46.120 --> 00:41:47.920]   I don't think we're ever gonna reach that point
[00:41:47.920 --> 00:41:51.920]   but I think that there is a tendency
[00:41:51.920 --> 00:41:56.100]   to sometimes believe in science and become elitist
[00:41:56.100 --> 00:41:58.860]   and become, I don't know, hard when in reality
[00:41:58.860 --> 00:42:01.780]   it should humble you and make you feel smaller.
[00:42:01.780 --> 00:42:03.080]   I think there's something very beautiful
[00:42:03.080 --> 00:42:07.420]   about feeling very, very small and very weak
[00:42:07.420 --> 00:42:10.940]   and to feel that you need other people.
[00:42:10.940 --> 00:42:13.220]   So I try to keep that in mind and say,
[00:42:13.220 --> 00:42:14.340]   look, thanks for watching.
[00:42:14.340 --> 00:42:16.700]   Vsauce is not, I'm not Vsauce, you are.
[00:42:16.700 --> 00:42:20.500]   When I start the episodes, I say, hey, Vsauce, Michael here.
[00:42:20.500 --> 00:42:22.780]   Vsauce and Michael are actually a different thing in my mind.
[00:42:22.780 --> 00:42:24.380]   I don't know if that's always clear
[00:42:24.380 --> 00:42:26.900]   but yeah, I have to approach it that way
[00:42:26.900 --> 00:42:29.040]   because it's not about me.
[00:42:29.040 --> 00:42:31.860]   - Yeah, so it's not even,
[00:42:31.860 --> 00:42:33.620]   you're not feeling the responsibility.
[00:42:33.620 --> 00:42:36.100]   You're just sort of plugging into this big thing
[00:42:36.100 --> 00:42:40.020]   that is scientific exploration of our reality
[00:42:40.020 --> 00:42:42.660]   and you're a voice that represents a bunch
[00:42:42.660 --> 00:42:47.660]   but you're just plugging into this big Vsauce ball
[00:42:47.660 --> 00:42:49.860]   that others, millions of others are plugged into.
[00:42:49.860 --> 00:42:53.060]   - Yeah, and I'm just hoping to encourage curiosity
[00:42:53.060 --> 00:42:56.380]   and responsible thinking
[00:42:56.380 --> 00:43:01.380]   and an embracement of doubt and being okay with that.
[00:43:05.020 --> 00:43:08.200]   - So next week talking to Christos Goudreau.
[00:43:08.200 --> 00:43:09.980]   I'm not sure if you're familiar who he is
[00:43:09.980 --> 00:43:11.660]   but he's the VP of engineering,
[00:43:11.660 --> 00:43:14.660]   head of the "YouTube algorithm"
[00:43:14.660 --> 00:43:16.140]   or the search and discovery.
[00:43:16.140 --> 00:43:20.180]   So let me ask, first high level,
[00:43:20.180 --> 00:43:25.100]   do you have a question for him
[00:43:25.100 --> 00:43:28.860]   that if you can get an honest answer that you would ask
[00:43:28.860 --> 00:43:30.140]   but more generally,
[00:43:30.140 --> 00:43:32.620]   how do you think about the YouTube algorithm
[00:43:32.620 --> 00:43:36.060]   that drives some of the motivation behind,
[00:43:36.060 --> 00:43:38.900]   no, some of the design decisions you make
[00:43:38.900 --> 00:43:42.220]   as you ask and answer some of the questions you do?
[00:43:42.220 --> 00:43:43.780]   How would you improve this algorithm
[00:43:43.780 --> 00:43:45.140]   in your mind in general?
[00:43:45.140 --> 00:43:47.540]   So just what would you ask him?
[00:43:47.540 --> 00:43:49.500]   And outside of that,
[00:43:49.500 --> 00:43:52.660]   how would you like to see the algorithm improve?
[00:43:52.660 --> 00:43:56.780]   - Well, I think of the algorithm as a mirror.
[00:43:56.780 --> 00:43:58.940]   It reflects what people put in
[00:43:58.940 --> 00:44:01.140]   and we don't always like what we see in that mirror.
[00:44:01.140 --> 00:44:02.780]   From the individual mirror
[00:44:02.780 --> 00:44:05.460]   to the individual mirror to the society.
[00:44:05.460 --> 00:44:06.300]   - Both.
[00:44:06.300 --> 00:44:07.120]   In the aggregate,
[00:44:07.120 --> 00:44:11.400]   it's reflecting back what people on average want to watch.
[00:44:11.400 --> 00:44:15.380]   And when you see things being recommended to you,
[00:44:15.380 --> 00:44:19.260]   it's reflecting back what it thinks you want to see.
[00:44:19.260 --> 00:44:22.340]   And specifically, I would guess that it's
[00:44:22.340 --> 00:44:23.600]   not just what you want to see,
[00:44:23.600 --> 00:44:25.600]   but what you will click on
[00:44:25.600 --> 00:44:27.980]   and what you will watch some of
[00:44:27.980 --> 00:44:31.080]   and stay on YouTube.
[00:44:31.080 --> 00:44:32.500]   Because of.
[00:44:32.500 --> 00:44:33.340]   I don't think that,
[00:44:33.340 --> 00:44:34.880]   this is all me guessing,
[00:44:34.880 --> 00:44:39.000]   but I don't think that YouTube cares
[00:44:39.000 --> 00:44:41.720]   if you only watch like a second of a video.
[00:44:41.720 --> 00:44:45.140]   As long as the next thing you do is open another video.
[00:44:45.140 --> 00:44:49.420]   If you close the app or close the site,
[00:44:49.420 --> 00:44:50.800]   that's a problem for them.
[00:44:50.800 --> 00:44:52.980]   Because they're not a subscription platform.
[00:44:52.980 --> 00:44:53.820]   They're not like, look,
[00:44:53.820 --> 00:44:56.160]   you're giving us 20 bucks a month no matter what,
[00:44:56.160 --> 00:44:57.700]   so who cares?
[00:44:57.700 --> 00:45:00.940]   They need you to watch and spend time there
[00:45:00.940 --> 00:45:02.140]   and see ads.
[00:45:02.140 --> 00:45:03.680]   - So one of the things I'm curious about
[00:45:03.680 --> 00:45:07.680]   whether they do consider longer term,
[00:45:07.680 --> 00:45:09.740]   sort of develop,
[00:45:09.740 --> 00:45:12.680]   your longer term development as a human being,
[00:45:12.680 --> 00:45:15.180]   which I think ultimately will make you feel better
[00:45:15.180 --> 00:45:17.520]   about using YouTube in the long term
[00:45:17.520 --> 00:45:20.000]   and allowing you to stick with it for longer.
[00:45:20.000 --> 00:45:22.440]   Because even if you feed the dopamine rush
[00:45:22.440 --> 00:45:23.360]   in the short term,
[00:45:23.360 --> 00:45:25.600]   and you keep clicking on cat videos,
[00:45:25.600 --> 00:45:29.020]   eventually you sort of wake up like from a drug
[00:45:29.020 --> 00:45:30.900]   and say, I need to quit this.
[00:45:30.900 --> 00:45:32.720]   So I wonder how much they're trying to optimize
[00:45:32.720 --> 00:45:34.040]   for the long term.
[00:45:34.040 --> 00:45:35.840]   Because when I look at the,
[00:45:35.840 --> 00:45:39.440]   your videos aren't exactly sort of, no offense,
[00:45:39.440 --> 00:45:41.760]   but they're not the most clickable.
[00:45:41.760 --> 00:45:44.200]   They're both the most clickable
[00:45:44.200 --> 00:45:47.320]   and I feel I watched the entire thing
[00:45:47.320 --> 00:45:49.680]   and I feel a better human after I watched it.
[00:45:49.680 --> 00:45:54.000]   So they're not just optimizing for the clickability.
[00:45:54.000 --> 00:45:59.920]   So my thought is, how do you think of it?
[00:45:59.920 --> 00:46:02.260]   And does it affect your own content?
[00:46:02.260 --> 00:46:03.300]   Like how deep you go,
[00:46:03.300 --> 00:46:07.020]   how profound you explore the directions and so on.
[00:46:07.020 --> 00:46:11.500]   - I've been really lucky in that I don't worry too much
[00:46:11.500 --> 00:46:12.520]   about the algorithm.
[00:46:12.520 --> 00:46:13.780]   I mean, look at my thumbnails.
[00:46:13.780 --> 00:46:17.220]   I don't really go too wild with them.
[00:46:17.220 --> 00:46:18.140]   And with Minefield,
[00:46:18.140 --> 00:46:20.860]   where I'm in partnership with YouTube on the thumbnails,
[00:46:20.860 --> 00:46:22.420]   I'm often like, let's pull this back.
[00:46:22.420 --> 00:46:23.980]   Let's be mysterious.
[00:46:23.980 --> 00:46:25.660]   But usually I'm just trying to do
[00:46:25.660 --> 00:46:27.540]   what everyone else is not doing.
[00:46:27.540 --> 00:46:29.740]   So if everyone's doing crazy Photoshop,
[00:46:29.740 --> 00:46:31.040]   what kind of thumbnails?
[00:46:31.040 --> 00:46:34.200]   I'm like, what if the thumbnails just align?
[00:46:34.200 --> 00:46:37.820]   And what if the title is just a word?
[00:46:37.820 --> 00:46:41.240]   And I kind of feel like all of the Vsauce channels
[00:46:41.240 --> 00:46:43.240]   have cultivated an audience that expects that.
[00:46:43.240 --> 00:46:45.400]   And so they would rather Jake make a video
[00:46:45.400 --> 00:46:47.080]   that's just called stains
[00:46:47.080 --> 00:46:51.000]   than one called I explored stains, shocking.
[00:46:51.000 --> 00:46:53.360]   But there are other audiences out there that want that.
[00:46:53.360 --> 00:46:57.120]   And I think most people kind of want
[00:46:57.120 --> 00:46:58.840]   what you see the algorithm favoring,
[00:46:58.840 --> 00:47:02.300]   which is mainstream traditional celebrity
[00:47:02.300 --> 00:47:03.700]   and news kind of information.
[00:47:03.700 --> 00:47:05.240]   I mean, that's what makes YouTube really different
[00:47:05.240 --> 00:47:06.720]   than other streaming platforms.
[00:47:06.720 --> 00:47:08.820]   No one's like, what's going on in the world?
[00:47:08.820 --> 00:47:10.560]   I'll open up Netflix to find out.
[00:47:10.560 --> 00:47:12.760]   But you do open up Twitter to find that out.
[00:47:12.760 --> 00:47:14.880]   You open up Facebook, you can open up YouTube
[00:47:14.880 --> 00:47:16.280]   'cause you'll see that the trending videos
[00:47:16.280 --> 00:47:19.640]   are like what happened amongst the traditional
[00:47:19.640 --> 00:47:22.180]   mainstream people in different industries.
[00:47:22.180 --> 00:47:24.100]   That's what's being shown.
[00:47:24.100 --> 00:47:27.600]   And it's not necessarily YouTube saying,
[00:47:27.600 --> 00:47:29.280]   we want that to be what you see.
[00:47:29.280 --> 00:47:31.440]   It's that that's what people click on.
[00:47:31.440 --> 00:47:34.280]   When they see Ariana Grande reads a love letter
[00:47:34.280 --> 00:47:36.320]   from like her high school sweetheart,
[00:47:36.320 --> 00:47:38.040]   they're like, I wanna see that.
[00:47:38.040 --> 00:47:39.360]   And when they see a video from me
[00:47:39.360 --> 00:47:40.560]   that's got some lines in math
[00:47:40.560 --> 00:47:41.840]   and it's called law and causes,
[00:47:41.840 --> 00:47:46.000]   they're like, well, I mean, I'm just on the bus.
[00:47:46.000 --> 00:47:48.680]   Like I don't have time to dive into a whole lesson.
[00:47:48.680 --> 00:47:52.400]   So before you get super mad at YouTube,
[00:47:52.400 --> 00:47:53.680]   you should say, really,
[00:47:53.680 --> 00:47:55.560]   they're just reflecting back human behavior.
[00:47:55.560 --> 00:47:59.180]   - Is there something you would improve about the algorithm?
[00:47:59.180 --> 00:48:02.800]   Knowing of course, that as far as we're concerned,
[00:48:02.800 --> 00:48:04.600]   it's a black box or we don't know how it works.
[00:48:04.600 --> 00:48:06.800]   - Right, and I don't think that even anyone at YouTube
[00:48:06.800 --> 00:48:07.920]   really knows what it's doing.
[00:48:07.920 --> 00:48:09.840]   They know what they've tweaked, but then it learns.
[00:48:09.840 --> 00:48:13.840]   I think that it learns and it decides how to behave.
[00:48:13.840 --> 00:48:16.680]   And sometimes the YouTube employees are left going,
[00:48:16.680 --> 00:48:19.640]   I don't know, maybe we should like change the value
[00:48:19.640 --> 00:48:22.680]   of how much it worries about watch time
[00:48:22.680 --> 00:48:24.720]   and maybe it should worry more about something.
[00:48:24.720 --> 00:48:27.420]   I don't know, but I mean, I would like to see,
[00:48:27.420 --> 00:48:30.720]   I don't know what they're doing and not doing.
[00:48:30.720 --> 00:48:32.760]   - Well, is there a conversation
[00:48:32.760 --> 00:48:35.760]   that you think they should be having just internally,
[00:48:35.760 --> 00:48:37.340]   whether they're having it or not?
[00:48:37.340 --> 00:48:38.960]   Is there something,
[00:48:38.960 --> 00:48:41.160]   should they be thinking about the long-term future?
[00:48:41.160 --> 00:48:44.480]   Should they be thinking about educational content
[00:48:44.480 --> 00:48:47.360]   and whether that's educating
[00:48:47.360 --> 00:48:48.960]   about what just happened in the world today,
[00:48:48.960 --> 00:48:51.560]   news or educational content, like what you're providing,
[00:48:51.560 --> 00:48:54.400]   which is asking big sort of timeless questions
[00:48:54.400 --> 00:48:56.600]   about how the way the world works.
[00:48:56.600 --> 00:48:59.440]   - Well, it's interesting, what should they think about?
[00:48:59.440 --> 00:49:02.640]   Because it's called YouTube, not our tube.
[00:49:02.640 --> 00:49:04.720]   And that's why I think they have
[00:49:04.720 --> 00:49:08.400]   so many phenomenal educational creators.
[00:49:08.400 --> 00:49:11.720]   You don't have shows like "Three Blue, One Brown"
[00:49:11.720 --> 00:49:14.240]   or "Physics Girl" or "Looking Glass Universe"
[00:49:14.240 --> 00:49:16.560]   or "Up and Atom" or "Brain Scoop" or,
[00:49:16.560 --> 00:49:18.760]   I mean, I could go on and on.
[00:49:18.760 --> 00:49:21.240]   They aren't on Amazon Prime and Netflix
[00:49:21.240 --> 00:49:24.040]   and they don't have commissioned shows from those platforms.
[00:49:24.040 --> 00:49:25.560]   It's all organically happening
[00:49:25.560 --> 00:49:26.680]   because there are people out there
[00:49:26.680 --> 00:49:30.160]   that want to share their passion for learning,
[00:49:30.160 --> 00:49:32.540]   that wanna share their curiosity.
[00:49:32.540 --> 00:49:37.480]   And YouTube could promote those kinds of shows more,
[00:49:37.480 --> 00:49:39.080]   but like, first of all,
[00:49:39.080 --> 00:49:43.280]   they probably wouldn't get as many clicks
[00:49:43.280 --> 00:49:45.360]   and YouTube needs to make sure that the average user
[00:49:45.360 --> 00:49:47.760]   is always clicking and staying on the site.
[00:49:47.760 --> 00:49:51.080]   They could still promote it more for the good of society,
[00:49:51.080 --> 00:49:52.760]   but then we're making some really weird claims
[00:49:52.760 --> 00:49:54.000]   about what's good for society
[00:49:54.000 --> 00:49:55.640]   because I think that cat videos
[00:49:55.640 --> 00:49:58.080]   are also an incredibly important part
[00:49:58.080 --> 00:50:00.400]   of what it means to be a human.
[00:50:00.400 --> 00:50:02.920]   I mentioned this quote before from Unumuno about,
[00:50:02.920 --> 00:50:05.440]   look, I've seen a cat like estimate distances
[00:50:05.440 --> 00:50:09.480]   and calculate a jump more often than I've seen a cat cry.
[00:50:09.480 --> 00:50:12.480]   And so things that play with our emotions
[00:50:12.480 --> 00:50:15.400]   and make us feel things can be cheesy and can feel cheap,
[00:50:15.400 --> 00:50:18.040]   but like, man, that's very human.
[00:50:18.040 --> 00:50:23.040]   And so even the dumbest vlog is still so important
[00:50:23.800 --> 00:50:27.400]   that I don't think I have a better claim to take its spot
[00:50:27.400 --> 00:50:29.880]   than it has to have that spot.
[00:50:29.880 --> 00:50:31.640]   - It puts a mirror to us,
[00:50:31.640 --> 00:50:33.960]   the beautiful parts, the ugly parts,
[00:50:33.960 --> 00:50:36.520]   the shallow parts, the deep parts, you're right.
[00:50:36.520 --> 00:50:38.520]   - What I would like to see is,
[00:50:38.520 --> 00:50:43.400]   I miss the days when engaging with content on YouTube
[00:50:43.400 --> 00:50:47.640]   helped push it into my subscribers timelines.
[00:50:47.640 --> 00:50:49.600]   It used to be that when I liked a video,
[00:50:49.600 --> 00:50:51.200]   say from Veritasium,
[00:50:51.200 --> 00:50:56.120]   it would show up in the feed on the front page of the app
[00:50:56.120 --> 00:50:58.380]   or the website of my subscribers.
[00:50:58.380 --> 00:51:00.480]   And I knew that if I liked a video,
[00:51:00.480 --> 00:51:03.520]   I could send it 100,000 views or more.
[00:51:03.520 --> 00:51:05.320]   That no longer is true.
[00:51:05.320 --> 00:51:07.340]   But I think that was a good user experience.
[00:51:07.340 --> 00:51:09.840]   When I subscribe to someone, when I'm following them,
[00:51:09.840 --> 00:51:13.080]   I want to see more of what they like.
[00:51:13.080 --> 00:51:15.400]   I want them to also curate the feed for me.
[00:51:15.400 --> 00:51:17.920]   And I think that Twitter and Facebook are doing that
[00:51:17.920 --> 00:51:20.360]   in also some ways that are kind of annoying,
[00:51:20.360 --> 00:51:22.380]   but I would like that to happen more.
[00:51:22.380 --> 00:51:25.200]   And I think we would see communities being stronger
[00:51:25.200 --> 00:51:27.320]   on YouTube if it was that way, instead of YouTube going,
[00:51:27.320 --> 00:51:29.800]   well, technically Michael liked this Veritasium video,
[00:51:29.800 --> 00:51:33.620]   but people are way more likely to click on Carpool Karaoke.
[00:51:33.620 --> 00:51:36.200]   So I don't even care who they are, just give them that.
[00:51:36.200 --> 00:51:38.960]   Not saying anything against Carpool Karaoke,
[00:51:38.960 --> 00:51:43.280]   that is a extremely important part of our society,
[00:51:43.280 --> 00:51:46.800]   what it means to be a human on earth, you know, but-
[00:51:46.800 --> 00:51:48.400]   - I'll say it, it sucks, but-
[00:51:48.400 --> 00:51:49.680]   (both laughing)
[00:51:49.680 --> 00:51:51.360]   - But a lot of people would disagree with you
[00:51:51.360 --> 00:51:53.860]   and they should be able to see as much of that as they want.
[00:51:53.860 --> 00:51:55.680]   And I think even people who don't think they like it
[00:51:55.680 --> 00:51:57.080]   should still be really aware of it
[00:51:57.080 --> 00:51:59.400]   'cause it's such an important thing
[00:51:59.400 --> 00:52:00.920]   and such an influential thing.
[00:52:00.920 --> 00:52:03.320]   But yeah, I just wish that like new channels I discover
[00:52:03.320 --> 00:52:04.380]   and that I subscribe to,
[00:52:04.380 --> 00:52:06.960]   I wish that my subscribers found out about that
[00:52:06.960 --> 00:52:10.040]   because especially in the education community,
[00:52:10.040 --> 00:52:11.620]   a rising tide floats all boats.
[00:52:11.620 --> 00:52:14.040]   If you watch a video from Numberphile,
[00:52:14.040 --> 00:52:16.760]   you're just more likely to wanna watch an episode from me,
[00:52:16.760 --> 00:52:18.560]   whether it be on Vsauce1 or Ding.
[00:52:18.560 --> 00:52:21.880]   It's not competitive in the way that traditional TV was,
[00:52:21.880 --> 00:52:23.240]   where it's like, well, if you tune into that show,
[00:52:23.240 --> 00:52:24.600]   it means you're not watching mine
[00:52:24.600 --> 00:52:26.160]   'cause they both air at the same time.
[00:52:26.160 --> 00:52:29.360]   So helping each other out through collaborations
[00:52:29.360 --> 00:52:31.760]   takes a lot of work, but just through engaging,
[00:52:31.760 --> 00:52:34.040]   commenting on their videos, liking their videos,
[00:52:34.040 --> 00:52:36.720]   subscribing to them, whatever,
[00:52:36.720 --> 00:52:41.540]   that I would love to see become easier and more powerful.
[00:52:41.540 --> 00:52:46.080]   - So a quick and impossibly deep question,
[00:52:46.080 --> 00:52:48.960]   last question about mortality.
[00:52:48.960 --> 00:52:52.600]   You've spoken about death as an interesting topic.
[00:52:52.600 --> 00:52:55.960]   Do you think about your own mortality?
[00:52:55.960 --> 00:52:57.760]   - Yeah, every day.
[00:52:57.760 --> 00:52:59.740]   It's really scary.
[00:52:59.740 --> 00:53:04.020]   - So what do you think is the meaning of life
[00:53:04.020 --> 00:53:07.320]   that mortality makes very explicit?
[00:53:07.320 --> 00:53:12.320]   So why are you here on earth, Michael?
[00:53:12.320 --> 00:53:14.520]   What's the point of this whole thing?
[00:53:14.520 --> 00:53:17.360]   (Michael mumbles)
[00:53:17.360 --> 00:53:24.960]   What does mortality in the context of the whole universe
[00:53:24.960 --> 00:53:26.400]   make you realize about yourself?
[00:53:26.400 --> 00:53:28.080]   Just you, Michael Stevens.
[00:53:28.080 --> 00:53:31.320]   - Well, it makes me realize
[00:53:31.320 --> 00:53:35.620]   that I am destined to become a notion.
[00:53:35.620 --> 00:53:37.960]   I'm destined to become a memory.
[00:53:37.960 --> 00:53:39.600]   And we can extend life.
[00:53:39.600 --> 00:53:42.880]   I think there's really exciting things being done
[00:53:42.880 --> 00:53:46.600]   to extend life, but we still don't know how to like,
[00:53:46.600 --> 00:53:48.860]   protect you from some accident that could happen,
[00:53:48.860 --> 00:53:50.280]   some unforeseen thing.
[00:53:50.280 --> 00:53:54.120]   Maybe we could like save my connectome
[00:53:54.120 --> 00:53:56.560]   and like recreate my consciousness digitally.
[00:53:56.560 --> 00:53:59.400]   But even that could be lost
[00:53:59.400 --> 00:54:02.580]   if it's stored on a physical medium or something.
[00:54:02.580 --> 00:54:04.520]   So basically I just think that
[00:54:04.520 --> 00:54:09.000]   embracing and realizing how cool it is
[00:54:09.000 --> 00:54:11.560]   that like someday I will just be an idea
[00:54:11.560 --> 00:54:13.400]   and there won't be a Michael anymore
[00:54:13.400 --> 00:54:16.240]   that can be like, no, that's not what I meant.
[00:54:16.240 --> 00:54:17.560]   It'll just be what people like,
[00:54:17.560 --> 00:54:19.640]   they have to guess what I meant.
[00:54:19.640 --> 00:54:21.760]   And they'll remember me
[00:54:21.760 --> 00:54:25.840]   and how I live on as that memory
[00:54:25.840 --> 00:54:29.640]   will maybe not even be who I wanted to be.
[00:54:29.640 --> 00:54:31.980]   But there's something powerful about that.
[00:54:31.980 --> 00:54:34.320]   And there's something powerful about letting
[00:54:34.320 --> 00:54:39.000]   future people run the show themselves.
[00:54:39.000 --> 00:54:42.000]   I think I'm glad to get out of their way at some point
[00:54:42.000 --> 00:54:43.720]   and say, all right, it's your world now.
[00:54:43.720 --> 00:54:47.320]   - So you, the physical entity, Michael,
[00:54:47.320 --> 00:54:50.360]   have ripple effects in the space of ideas
[00:54:50.360 --> 00:54:54.320]   that far outlives you in ways that you can't control,
[00:54:54.320 --> 00:54:56.160]   but it's nevertheless fascinating to think,
[00:54:56.160 --> 00:54:57.640]   I mean, especially with you,
[00:54:57.640 --> 00:54:59.240]   you can imagine an alien species
[00:54:59.240 --> 00:55:01.800]   when they finally arrive and destroy all of us
[00:55:01.800 --> 00:55:04.640]   would watch your videos to try to figure out
[00:55:04.640 --> 00:55:06.040]   what were the questions that these people--
[00:55:06.040 --> 00:55:07.480]   - But even if they didn't,
[00:55:08.700 --> 00:55:11.560]   I still think that there will be ripples.
[00:55:11.560 --> 00:55:14.640]   When I say memory, I don't specifically mean
[00:55:14.640 --> 00:55:17.640]   people remember my name and my birth date
[00:55:17.640 --> 00:55:19.840]   and there's a photo of me on Wikipedia.
[00:55:19.840 --> 00:55:21.840]   All that can be lost, but I still would hope
[00:55:21.840 --> 00:55:25.840]   that people ask questions and teach concepts
[00:55:25.840 --> 00:55:28.400]   in some of the ways that I have found useful and satisfying.
[00:55:28.400 --> 00:55:29.840]   Even if they don't know that I was the one
[00:55:29.840 --> 00:55:32.640]   who tried to popularize it, that's fine.
[00:55:32.640 --> 00:55:35.320]   But if Earth was completely destroyed,
[00:55:35.320 --> 00:55:38.720]   like burnt to a crisp, everything on it today,
[00:55:38.720 --> 00:55:42.760]   the universe wouldn't care.
[00:55:42.760 --> 00:55:45.640]   Like Jupiter is not gonna go, "Oh no."
[00:55:45.640 --> 00:55:49.720]   And that could happen.
[00:55:49.720 --> 00:55:50.560]   - That could happen.
[00:55:50.560 --> 00:55:55.160]   - So we do, however, have the power to launch things
[00:55:55.160 --> 00:56:00.160]   into space to try to extend how long our memory exists.
[00:56:02.960 --> 00:56:04.680]   And what I mean by that is,
[00:56:04.680 --> 00:56:06.960]   we are recording things about the world
[00:56:06.960 --> 00:56:08.480]   and we're learning things and writing stories
[00:56:08.480 --> 00:56:10.680]   and all of this and preserving that
[00:56:10.680 --> 00:56:16.560]   is truly what I think is the essence of being a human.
[00:56:16.560 --> 00:56:20.640]   We are autobiographers of the universe
[00:56:20.640 --> 00:56:21.720]   and we're really good at it.
[00:56:21.720 --> 00:56:25.560]   We're better than fossils, we're better than light spectrum,
[00:56:25.560 --> 00:56:26.800]   we're better than any of that.
[00:56:26.800 --> 00:56:31.480]   We collect much more detailed memories of what's happening,
[00:56:31.480 --> 00:56:32.880]   much better data.
[00:56:32.880 --> 00:56:37.360]   And so that should be our legacy.
[00:56:37.360 --> 00:56:40.200]   And I hope that that's kind of mine too,
[00:56:40.200 --> 00:56:42.480]   in terms of people remembering something
[00:56:42.480 --> 00:56:44.840]   or having some kind of effect.
[00:56:44.840 --> 00:56:47.600]   But even if I don't, you can't not have an effect.
[00:56:47.600 --> 00:56:49.240]   That's the thing, this is not me feeling like,
[00:56:49.240 --> 00:56:50.880]   I hope that I have this powerful legacy.
[00:56:50.880 --> 00:56:53.040]   It's like, no matter who you are, you will.
[00:56:53.040 --> 00:56:57.760]   But you also have to embrace the fact
[00:56:57.760 --> 00:57:01.400]   that that impact might look really small and that's okay.
[00:57:01.400 --> 00:57:04.520]   One of my favorite quotes is from Tess of the D'Urbervilles
[00:57:04.520 --> 00:57:08.240]   and it's along the lines of the measure of your life
[00:57:08.240 --> 00:57:10.960]   depends on not your external displacement,
[00:57:10.960 --> 00:57:13.120]   but your subjective experience.
[00:57:13.120 --> 00:57:15.800]   If I am happy and those that I love are happy,
[00:57:15.800 --> 00:57:17.740]   can that be enough?
[00:57:17.740 --> 00:57:20.120]   Because if so, excellent.
[00:57:20.120 --> 00:57:23.480]   - I think there's no better place to end it, Michael.
[00:57:23.480 --> 00:57:25.040]   Thank you so much, it was an honor to meet you.
[00:57:25.040 --> 00:57:25.880]   Thanks for talking to me.
[00:57:25.880 --> 00:57:27.720]   - Thank you, it was a pleasure.
[00:57:27.720 --> 00:57:29.200]   - Thanks for listening to this conversation
[00:57:29.200 --> 00:57:30.480]   with Michael Stevens.
[00:57:30.480 --> 00:57:33.340]   And thank you to our presenting sponsor, Cash App.
[00:57:33.340 --> 00:57:35.920]   Download it, use code LEXPODCAST,
[00:57:35.920 --> 00:57:39.000]   you'll get $10 and $10 will go to FIRST,
[00:57:39.000 --> 00:57:41.320]   a STEM education nonprofit that inspires
[00:57:41.320 --> 00:57:43.300]   hundreds of thousands of young minds
[00:57:43.300 --> 00:57:47.040]   to learn, to dream of engineering our future.
[00:57:47.040 --> 00:57:49.960]   If you enjoy this podcast, subscribe on YouTube,
[00:57:49.960 --> 00:57:51.920]   give it five stars on Apple Podcast,
[00:57:51.920 --> 00:57:55.520]   support it on Patreon or connect with me on Twitter.
[00:57:55.520 --> 00:57:58.600]   And now let me leave you with some words of wisdom
[00:57:58.600 --> 00:58:00.720]   from Albert Einstein.
[00:58:00.720 --> 00:58:03.960]   "The important thing is not to stop questioning.
[00:58:03.960 --> 00:58:06.860]   "Curiosity has its own reason for existence.
[00:58:06.860 --> 00:58:09.160]   "One cannot help but be in awe
[00:58:09.160 --> 00:58:11.660]   "when he contemplates the mysteries of eternity,
[00:58:11.660 --> 00:58:14.920]   "of life, the marvelous structure of reality.
[00:58:14.920 --> 00:58:18.020]   "It is enough if one tries merely to comprehend
[00:58:18.020 --> 00:58:21.080]   "a little of this mystery every day."
[00:58:21.080 --> 00:58:24.180]   Thank you for listening and hope to see you next time.
[00:58:24.180 --> 00:58:26.760]   (upbeat music)
[00:58:26.760 --> 00:58:29.340]   (upbeat music)
[00:58:29.340 --> 00:58:39.340]   [BLANK_AUDIO]

