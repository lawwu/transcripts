
[00:00:00.000 --> 00:00:03.400]   The following is a conversation with Ayana Howard.
[00:00:03.400 --> 00:00:06.200]   She's a roboticist, professor at Georgia Tech
[00:00:06.200 --> 00:00:09.840]   and director of the Human Automation Systems Lab
[00:00:09.840 --> 00:00:12.800]   with research interests in human-robot interaction,
[00:00:12.800 --> 00:00:16.000]   assistive robots in the home, therapy gaming apps,
[00:00:16.000 --> 00:00:20.280]   and remote robotic exploration of extreme environments.
[00:00:20.280 --> 00:00:23.440]   Like me, in her work, she cares a lot
[00:00:23.440 --> 00:00:26.360]   about both robots and human beings.
[00:00:26.360 --> 00:00:29.560]   And so I really enjoyed this conversation.
[00:00:29.560 --> 00:00:32.600]   This is the Artificial Intelligence Podcast.
[00:00:32.600 --> 00:00:34.960]   If you enjoy it, subscribe on YouTube,
[00:00:34.960 --> 00:00:36.960]   give it five stars on Apple Podcast,
[00:00:36.960 --> 00:00:39.600]   follow on Spotify, support it on Patreon,
[00:00:39.600 --> 00:00:41.720]   or simply connect with me on Twitter,
[00:00:41.720 --> 00:00:45.640]   Alex Friedman, spelled F-R-I-D-M-A-N.
[00:00:45.640 --> 00:00:48.680]   I recently started doing ads at the end of the introduction.
[00:00:48.680 --> 00:00:51.640]   I'll do one or two minutes after introducing the episode
[00:00:51.640 --> 00:00:53.200]   and never any ads in the middle
[00:00:53.200 --> 00:00:55.480]   that can break the flow of the conversation.
[00:00:55.480 --> 00:00:56.880]   I hope that works for you
[00:00:56.880 --> 00:00:59.320]   and doesn't hurt the listening experience.
[00:01:00.200 --> 00:01:02.280]   This show is presented by Cash App,
[00:01:02.280 --> 00:01:04.800]   the number one finance app in the App Store.
[00:01:04.800 --> 00:01:07.560]   I personally use Cash App to send money to friends,
[00:01:07.560 --> 00:01:09.320]   but you can also use it to buy, sell,
[00:01:09.320 --> 00:01:11.720]   and deposit Bitcoin in just seconds.
[00:01:11.720 --> 00:01:14.600]   Cash App also has a new investing feature.
[00:01:14.600 --> 00:01:17.560]   You can buy fractions of a stock, say $1 worth,
[00:01:17.560 --> 00:01:19.680]   no matter what the stock price is.
[00:01:19.680 --> 00:01:22.600]   Brokerage services are provided by Cash App Investing,
[00:01:22.600 --> 00:01:25.880]   a subsidiary of Square and member SIPC.
[00:01:25.880 --> 00:01:28.160]   I'm excited to be working with Cash App
[00:01:28.160 --> 00:01:31.600]   to support one of my favorite organizations called FIRST,
[00:01:31.600 --> 00:01:35.120]   best known for their FIRST robotics and Lego competitions.
[00:01:35.120 --> 00:01:38.400]   They educate and inspire hundreds of thousands of students
[00:01:38.400 --> 00:01:40.240]   in over 110 countries
[00:01:40.240 --> 00:01:42.840]   and have a perfect rating at Charity Navigator,
[00:01:42.840 --> 00:01:44.160]   which means that donated money
[00:01:44.160 --> 00:01:46.880]   is used to maximum effectiveness.
[00:01:46.880 --> 00:01:49.520]   When you get Cash App from the App Store or Google Play
[00:01:49.520 --> 00:01:53.520]   and use code LEXPODCAST, you'll get $10
[00:01:53.520 --> 00:01:56.440]   and Cash App will also donate $10 to FIRST,
[00:01:56.440 --> 00:01:58.280]   which again is an organization
[00:01:58.280 --> 00:02:01.080]   that I've personally seen inspire girls and boys
[00:02:01.080 --> 00:02:04.280]   to dream of engineering a better world.
[00:02:04.280 --> 00:02:08.320]   And now here's my conversation with Ayana Howard.
[00:02:08.320 --> 00:02:13.640]   What or who is the most amazing robot you've ever met
[00:02:13.640 --> 00:02:16.760]   or perhaps had the biggest impact on your career?
[00:02:16.760 --> 00:02:21.120]   - I haven't met her, but I grew up with her,
[00:02:21.120 --> 00:02:22.760]   but of course Rosie.
[00:02:22.760 --> 00:02:25.240]   So, and I think it's because also--
[00:02:25.240 --> 00:02:26.160]   - Who's Rosie?
[00:02:26.160 --> 00:02:27.800]   - Rosie from the Jetsons.
[00:02:27.800 --> 00:02:31.000]   She is all things to all people, right?
[00:02:31.000 --> 00:02:32.920]   Think about it, like anything you wanted,
[00:02:32.920 --> 00:02:35.120]   it was like magic, it happened.
[00:02:35.120 --> 00:02:37.920]   - So people not only anthropomorphize,
[00:02:37.920 --> 00:02:41.960]   but project whatever they wish for the robot to be onto.
[00:02:41.960 --> 00:02:44.600]   - Onto Rosie, but also, I mean, think about it.
[00:02:44.600 --> 00:02:46.840]   She was socially engaging.
[00:02:46.840 --> 00:02:50.040]   She every so often had an attitude, right?
[00:02:50.040 --> 00:02:51.960]   She kept us honest.
[00:02:51.960 --> 00:02:53.800]   She would push back sometimes
[00:02:53.800 --> 00:02:57.080]   when George was doing some weird stuff,
[00:02:57.080 --> 00:02:59.880]   but she cared about people, especially the kids.
[00:02:59.880 --> 00:03:04.040]   She was like the perfect robot.
[00:03:04.040 --> 00:03:06.560]   - And you've said that people don't want
[00:03:06.560 --> 00:03:08.360]   their robots to be perfect.
[00:03:08.360 --> 00:03:11.200]   Can you elaborate that?
[00:03:11.200 --> 00:03:12.080]   What do you think that is?
[00:03:12.080 --> 00:03:14.840]   Just like you said, Rosie pushed back a little bit
[00:03:14.840 --> 00:03:15.800]   every once in a while.
[00:03:15.800 --> 00:03:18.320]   - Yeah, so I think it's that,
[00:03:18.320 --> 00:03:19.920]   so if you think about robotics in general,
[00:03:19.920 --> 00:03:24.000]   we want them because they enhance our quality of life.
[00:03:24.000 --> 00:03:27.080]   And usually that's linked to something that's functional.
[00:03:27.080 --> 00:03:28.720]   Even if you think of self-driving cars,
[00:03:28.720 --> 00:03:30.080]   why is there a fascination?
[00:03:30.080 --> 00:03:31.600]   Because people really do hate to drive.
[00:03:31.600 --> 00:03:35.400]   Like there's the Saturday driving where I can just speed,
[00:03:35.400 --> 00:03:37.600]   but then there's the, I have to go to work every day
[00:03:37.600 --> 00:03:39.080]   and I'm in traffic for an hour.
[00:03:39.080 --> 00:03:40.480]   I mean, people really hate that.
[00:03:40.480 --> 00:03:45.480]   And so robots are designed to basically enhance our ability
[00:03:47.480 --> 00:03:49.800]   to increase our quality of life.
[00:03:49.800 --> 00:03:54.400]   And so the perfection comes from this aspect of interaction.
[00:03:54.400 --> 00:04:00.120]   If I think about how we drive, if we drove perfectly,
[00:04:00.120 --> 00:04:02.240]   we would never get anywhere, right?
[00:04:02.240 --> 00:04:07.200]   So think about how many times you had to run past the light
[00:04:07.200 --> 00:04:09.080]   because you see the car behind you
[00:04:09.080 --> 00:04:10.440]   is about to crash into you,
[00:04:10.440 --> 00:04:15.400]   or that little kid kind of runs into the street
[00:04:15.400 --> 00:04:17.360]   and so you have to cross on the other side.
[00:04:17.360 --> 00:04:18.440]   See, there's no cars, right?
[00:04:18.440 --> 00:04:21.200]   Like if you think about it, we are not perfect drivers.
[00:04:21.200 --> 00:04:23.560]   Some of it is because it's our world.
[00:04:23.560 --> 00:04:26.760]   And so if you have a robot that is perfect
[00:04:26.760 --> 00:04:28.720]   in that sense of the word,
[00:04:28.720 --> 00:04:31.200]   they wouldn't really be able to function with us.
[00:04:31.200 --> 00:04:34.520]   - Can you linger a little bit on the word perfection?
[00:04:34.520 --> 00:04:39.440]   So from the robotics perspective, what does that word mean?
[00:04:39.440 --> 00:04:42.880]   And how is sort of the optimal behavior
[00:04:42.880 --> 00:04:44.480]   as you're describing different
[00:04:44.480 --> 00:04:46.640]   than what we think is perfection?
[00:04:46.640 --> 00:04:49.480]   - Yeah, so perfection, if you think about it
[00:04:49.480 --> 00:04:52.000]   in the more theoretical point of view,
[00:04:52.000 --> 00:04:54.080]   it's really tied to accuracy, right?
[00:04:54.080 --> 00:04:55.640]   So if I have a function,
[00:04:55.640 --> 00:04:59.520]   can I complete it at 100% accuracy with zero errors?
[00:04:59.520 --> 00:05:04.200]   And so that's kind of, if you think about perfection
[00:05:04.200 --> 00:05:05.040]   in the sense of the word.
[00:05:05.040 --> 00:05:07.560]   - And in the self-driving car realm,
[00:05:07.560 --> 00:05:10.480]   do you think from a robotics perspective,
[00:05:10.480 --> 00:05:13.960]   we kind of think that perfection means
[00:05:13.960 --> 00:05:15.600]   following the rules perfectly,
[00:05:15.600 --> 00:05:19.640]   sort of defining, staying in the lane, changing lanes.
[00:05:19.640 --> 00:05:20.920]   When there's a green light, you go,
[00:05:20.920 --> 00:05:22.320]   when there's a red light, you stop.
[00:05:22.320 --> 00:05:26.680]   And that's the, and be able to perfectly see
[00:05:26.680 --> 00:05:29.160]   all the entities in the scene.
[00:05:29.160 --> 00:05:32.000]   That's the limit of what we think of as perfection.
[00:05:32.000 --> 00:05:33.760]   - And I think that's where the problem comes
[00:05:33.760 --> 00:05:38.360]   is that when people think about perfection for robotics,
[00:05:38.360 --> 00:05:40.840]   the ones that are the most successful
[00:05:40.840 --> 00:05:43.280]   are the ones that are quote unquote perfect.
[00:05:43.280 --> 00:05:44.680]   Like I said, Rosie is perfect,
[00:05:44.680 --> 00:05:47.440]   but she actually wasn't perfect in terms of accuracy,
[00:05:47.440 --> 00:05:50.400]   but she was perfect in terms of how she interacted
[00:05:50.400 --> 00:05:51.520]   and how she adapted.
[00:05:51.520 --> 00:05:53.320]   And I think that's some of the disconnect
[00:05:53.320 --> 00:05:56.480]   is that we really want perfection
[00:05:56.480 --> 00:06:00.040]   with respect to its ability to adapt to us.
[00:06:00.040 --> 00:06:01.720]   We don't really want perfection
[00:06:01.720 --> 00:06:03.560]   with respect to 100% accuracy,
[00:06:03.560 --> 00:06:06.480]   with respect to the rules that we just made up anyway.
[00:06:06.480 --> 00:06:09.560]   Right, and so I think there's this disconnect sometimes
[00:06:09.560 --> 00:06:13.320]   between what we really want and what happens.
[00:06:13.320 --> 00:06:16.000]   And we see this all the time, like in my research, right?
[00:06:16.000 --> 00:06:20.440]   Like the optimal quote unquote optimal interactions
[00:06:20.440 --> 00:06:24.400]   are when the robot is adapting based on the person,
[00:06:24.400 --> 00:06:29.400]   not 100% following what's optimal based on the rules.
[00:06:29.400 --> 00:06:32.720]   - Just to linger on autonomous vehicles for a second,
[00:06:32.720 --> 00:06:35.280]   just your thoughts, maybe off the top of your head,
[00:06:35.280 --> 00:06:38.040]   how hard is that problem do you think
[00:06:38.040 --> 00:06:39.920]   based on what we just talked about?
[00:06:39.920 --> 00:06:41.680]   You know, there's a lot of folks
[00:06:41.680 --> 00:06:43.880]   in the automotive industry that are very confident
[00:06:43.880 --> 00:06:47.640]   from Elon Musk to Waymo to all these companies.
[00:06:47.640 --> 00:06:50.480]   How hard is it to solve that last piece?
[00:06:50.480 --> 00:06:51.440]   - The last mile.
[00:06:51.440 --> 00:06:56.440]   - The gap between the perfection and the human definition
[00:06:56.440 --> 00:06:59.480]   of how you actually function in this world?
[00:06:59.480 --> 00:07:00.600]   - Yeah, so this is a moving target.
[00:07:00.600 --> 00:07:04.480]   So I remember when all the big companies
[00:07:04.480 --> 00:07:06.800]   started to heavily invest in this.
[00:07:06.800 --> 00:07:09.880]   And there was a number of, even roboticists,
[00:07:09.880 --> 00:07:13.240]   as well as folks who were putting in the VCs
[00:07:13.240 --> 00:07:15.400]   and corporations, Elon Musk being one of them,
[00:07:15.400 --> 00:07:18.360]   that said, you know, self-driving cars on the road
[00:07:18.360 --> 00:07:20.920]   with people within five years.
[00:07:20.920 --> 00:07:24.600]   That was a little while ago.
[00:07:24.600 --> 00:07:29.600]   And now people are saying five years, 10 years, 20 years.
[00:07:29.600 --> 00:07:31.560]   Some are saying never, right?
[00:07:31.560 --> 00:07:33.760]   I think if you look at some of the things
[00:07:33.760 --> 00:07:37.800]   that are being successful is these,
[00:07:39.480 --> 00:07:41.240]   basically fixed environments
[00:07:41.240 --> 00:07:44.080]   where you still have some anomalies, right?
[00:07:44.080 --> 00:07:46.560]   You still have people walking, you still have stores,
[00:07:46.560 --> 00:07:50.160]   but you don't have other drivers, right?
[00:07:50.160 --> 00:07:51.760]   Like other human drivers are,
[00:07:51.760 --> 00:07:55.680]   is a dedicated space for the cars.
[00:07:55.680 --> 00:07:57.240]   Because if you think about robotics in general,
[00:07:57.240 --> 00:07:59.080]   where it's always been successful is in,
[00:07:59.080 --> 00:08:00.640]   I mean, you can say manufacturing,
[00:08:00.640 --> 00:08:02.320]   like way back in the day, right?
[00:08:02.320 --> 00:08:03.360]   It was a fixed environment.
[00:08:03.360 --> 00:08:05.280]   Humans were not part of the equation.
[00:08:05.280 --> 00:08:07.240]   We're a lot better than that.
[00:08:07.240 --> 00:08:11.000]   But like when we can carve out scenarios
[00:08:11.000 --> 00:08:13.840]   that are closer to that space,
[00:08:13.840 --> 00:08:16.720]   then I think that it's where we are.
[00:08:16.720 --> 00:08:20.600]   So a closed campus where you don't have self-driving cars
[00:08:20.600 --> 00:08:23.800]   and maybe some protection so that the students
[00:08:23.800 --> 00:08:27.280]   don't jet in front just because they wanna see what happens.
[00:08:27.280 --> 00:08:29.000]   Like having a little bit,
[00:08:29.000 --> 00:08:31.440]   I think that's where we're gonna see the most success
[00:08:31.440 --> 00:08:32.280]   in the near future.
[00:08:32.280 --> 00:08:33.720]   - And be slow moving.
[00:08:33.720 --> 00:08:37.880]   - Right, not 55, 60, 70 miles an hour,
[00:08:37.880 --> 00:08:42.080]   but the speed of a golf cart, right?
[00:08:42.080 --> 00:08:45.200]   - So that said, the most successful
[00:08:45.200 --> 00:08:47.880]   in the automotive industry robots operating today
[00:08:47.880 --> 00:08:50.280]   in the hands of real people
[00:08:50.280 --> 00:08:53.920]   are ones that are traveling over 55 miles an hour
[00:08:53.920 --> 00:08:55.560]   and in unconstrained environments,
[00:08:55.560 --> 00:08:58.880]   which is Tesla vehicles, so Tesla autopilot.
[00:08:58.880 --> 00:09:01.720]   So I just, I would love to hear sort of your,
[00:09:01.720 --> 00:09:04.240]   just thoughts of two things.
[00:09:04.240 --> 00:09:06.960]   So one, I don't know if you've gotten to see,
[00:09:06.960 --> 00:09:10.120]   you've heard about something called Smart Summon,
[00:09:10.120 --> 00:09:13.480]   where Tesla system, autopilot system,
[00:09:13.480 --> 00:09:15.880]   where the car drives zero occupancy,
[00:09:15.880 --> 00:09:17.960]   no driver in the parking lot,
[00:09:17.960 --> 00:09:20.480]   slowly sort of tries to navigate the parking lot
[00:09:20.480 --> 00:09:22.680]   to find itself to you.
[00:09:22.680 --> 00:09:25.840]   And there's some incredible amounts of videos
[00:09:25.840 --> 00:09:27.640]   and just hilarity that happens
[00:09:27.640 --> 00:09:30.880]   as it awkwardly tries to navigate this environment,
[00:09:30.880 --> 00:09:33.560]   but it's a beautiful nonverbal communication
[00:09:33.560 --> 00:09:37.400]   between machine and human that I think is a,
[00:09:37.400 --> 00:09:39.320]   it's like, it's some of the work that you do
[00:09:39.320 --> 00:09:42.040]   in this kind of interesting human-robot interaction space.
[00:09:42.040 --> 00:09:43.800]   So what are your thoughts in general about it?
[00:09:43.800 --> 00:09:46.100]   - So I do have that feature.
[00:09:46.100 --> 00:09:47.840]   - Do you drive a Tesla?
[00:09:47.840 --> 00:09:52.120]   - I do, mainly because I'm a gadget freak, right?
[00:09:52.120 --> 00:09:55.640]   So I'd say it's a gadget that happens to have some wheels.
[00:09:55.640 --> 00:09:58.200]   And yeah, I've seen some of the videos.
[00:09:58.200 --> 00:09:59.400]   - But what's your experience like?
[00:09:59.400 --> 00:10:02.680]   I mean, you're a human-robot interaction roboticist.
[00:10:02.680 --> 00:10:05.560]   You're a legit sort of expert in the field.
[00:10:05.560 --> 00:10:08.040]   So what does it feel for a machine to come to you?
[00:10:08.040 --> 00:10:11.880]   - It's one of these very fascinating things,
[00:10:11.880 --> 00:10:16.080]   but also I am hyper, hyper alert, right?
[00:10:16.080 --> 00:10:20.560]   Like I'm hyper alert, like my thumb is like,
[00:10:20.560 --> 00:10:23.200]   oh, okay, I'm ready to take over.
[00:10:23.200 --> 00:10:25.440]   Even when I'm in my car,
[00:10:25.440 --> 00:10:28.760]   I'm doing things like automated backing into,
[00:10:28.760 --> 00:10:30.640]   so there's like a feature where you can do this
[00:10:30.640 --> 00:10:33.200]   automating backing into a parking space,
[00:10:33.200 --> 00:10:35.720]   or bring the car out of your garage,
[00:10:35.720 --> 00:10:40.320]   or even, you know, pseudo autopilot on the freeway, right?
[00:10:40.320 --> 00:10:42.280]   I am hypersensitive.
[00:10:42.280 --> 00:10:44.720]   I can feel like as I'm navigating,
[00:10:44.720 --> 00:10:46.960]   I'm like, yeah, that's an error right there.
[00:10:46.960 --> 00:10:51.960]   Like I am very aware of it, but I'm also fascinated by it.
[00:10:51.960 --> 00:10:54.360]   And it does get better.
[00:10:54.360 --> 00:10:57.480]   Like I look and see it's learning
[00:10:57.480 --> 00:11:00.440]   from all of these people who are cutting it on.
[00:11:00.440 --> 00:11:04.160]   Like every time I cut it on, it's getting better, right?
[00:11:04.160 --> 00:11:07.160]   And so I think that's what's amazing about it is that.
[00:11:07.160 --> 00:11:10.360]   - This nice dance of you're still hyper vigilant.
[00:11:10.360 --> 00:11:12.840]   So you're still not trusting it at all.
[00:11:12.840 --> 00:11:13.680]   - Yeah.
[00:11:13.680 --> 00:11:14.600]   - And yet you're using it.
[00:11:14.600 --> 00:11:17.600]   On the highway, if I were to, like what,
[00:11:17.600 --> 00:11:20.300]   as a roboticist, we'll talk about trust a little bit.
[00:11:20.300 --> 00:11:23.680]   How do you explain that?
[00:11:23.680 --> 00:11:25.080]   You still use it.
[00:11:25.080 --> 00:11:26.520]   Is it the gadget freak part?
[00:11:26.520 --> 00:11:30.760]   Like where you just enjoy exploring technology
[00:11:30.760 --> 00:11:33.720]   or is that the right actually balance
[00:11:33.720 --> 00:11:36.920]   between robotics and humans is where you use it,
[00:11:36.920 --> 00:11:38.360]   but don't trust it.
[00:11:38.360 --> 00:11:40.120]   And somehow there's this dance
[00:11:40.120 --> 00:11:42.160]   that ultimately is a positive.
[00:11:42.160 --> 00:11:47.160]   - Yeah, so I think I just don't necessarily trust technology,
[00:11:47.160 --> 00:11:50.180]   but I'm an early adopter, right?
[00:11:50.180 --> 00:11:54.280]   So when it first comes out, I will use everything,
[00:11:54.280 --> 00:11:57.400]   but I will be very, very cautious of how I use it.
[00:11:57.400 --> 00:12:01.040]   - Do you read about it or do you explore it by just try it?
[00:12:01.040 --> 00:12:05.000]   Do you do like crudely, to put it crudely,
[00:12:05.000 --> 00:12:07.960]   do you read the manual or do you learn through exploration?
[00:12:07.960 --> 00:12:08.800]   - I'm an explorer.
[00:12:08.800 --> 00:12:12.320]   If I have to read the manual, then I do design.
[00:12:12.320 --> 00:12:14.200]   Then it's a bad user interface.
[00:12:14.200 --> 00:12:15.040]   It's a failure.
[00:12:15.040 --> 00:12:19.560]   - Elon Musk is very confident that you kind of take it
[00:12:19.560 --> 00:12:21.800]   from where it is now to full autonomy.
[00:12:21.800 --> 00:12:24.480]   So from this human robot interaction,
[00:12:24.480 --> 00:12:26.680]   where you don't really trust and then you try
[00:12:26.680 --> 00:12:29.160]   and then you catch it when it fails to,
[00:12:29.160 --> 00:12:33.920]   it's going to incrementally improve itself into full,
[00:12:33.920 --> 00:12:36.520]   where you don't need to participate.
[00:12:36.520 --> 00:12:39.860]   What's your sense of that trajectory?
[00:12:39.860 --> 00:12:41.040]   Is it feasible?
[00:12:41.040 --> 00:12:44.560]   So the promise there is by the end of next year,
[00:12:44.560 --> 00:12:47.200]   by the end of 2020, is the current promise.
[00:12:47.200 --> 00:12:52.200]   What's your sense about that journey that Tesla's on?
[00:12:52.200 --> 00:12:56.760]   - So there's kind of three things going on now.
[00:12:56.760 --> 00:13:01.760]   I think in terms of will people go,
[00:13:01.760 --> 00:13:04.800]   like as a user, as a adopter,
[00:13:04.800 --> 00:13:08.440]   will you trust going to that point?
[00:13:08.440 --> 00:13:10.080]   I think so, right?
[00:13:10.080 --> 00:13:13.000]   Like there are some users and it's because what happens is
[00:13:13.000 --> 00:13:16.720]   when you're hypersensitive at the beginning
[00:13:16.720 --> 00:13:19.360]   and then the technology tends to work,
[00:13:19.360 --> 00:13:23.880]   your apprehension slowly goes away.
[00:13:23.880 --> 00:13:28.280]   And as people, we tend to swing to the other extreme, right?
[00:13:28.280 --> 00:13:30.960]   Because like, oh, I was like hyper, hyper fearful
[00:13:30.960 --> 00:13:33.980]   or hypersensitive and it was awesome.
[00:13:33.980 --> 00:13:37.400]   And we just tend to swing, that's just human nature.
[00:13:37.400 --> 00:13:38.880]   And so you will have, I mean, and I--
[00:13:38.880 --> 00:13:41.560]   - It's a scary notion because most people
[00:13:41.560 --> 00:13:45.040]   are now extremely untrusting of autopilot.
[00:13:45.040 --> 00:13:46.500]   They use it, but they don't trust it.
[00:13:46.500 --> 00:13:48.920]   And it's a scary notion that there's a certain point
[00:13:48.920 --> 00:13:51.400]   where you allow yourself to look at the smartphone
[00:13:51.400 --> 00:13:53.160]   for like 20 seconds.
[00:13:53.160 --> 00:13:55.440]   And then there'll be this phase shift
[00:13:55.440 --> 00:13:57.600]   where it'll be like 20 seconds, 30 seconds,
[00:13:57.600 --> 00:13:58.980]   one minute, two minutes.
[00:13:58.980 --> 00:14:02.040]   It's a scary proposition.
[00:14:02.040 --> 00:14:03.520]   - But that's people, right?
[00:14:03.520 --> 00:14:05.600]   That's just, that's humans.
[00:14:05.600 --> 00:14:10.000]   I mean, I think of even our use of,
[00:14:10.000 --> 00:14:12.400]   I mean, just everything on the internet, right?
[00:14:12.780 --> 00:14:16.900]   Think about how reliant we are on certain apps
[00:14:16.900 --> 00:14:19.420]   and certain engines, right?
[00:14:19.420 --> 00:14:22.700]   20 years ago, people have been like, oh yeah, that's stupid.
[00:14:22.700 --> 00:14:23.980]   Like that makes no sense.
[00:14:23.980 --> 00:14:25.900]   Like, of course that's false.
[00:14:25.900 --> 00:14:29.100]   Like now it's just like, oh, of course I've been using it.
[00:14:29.100 --> 00:14:30.780]   It's been correct all this time.
[00:14:30.780 --> 00:14:34.380]   Of course, aliens, I didn't think they existed,
[00:14:34.380 --> 00:14:37.620]   but now it says they do, obviously.
[00:14:37.620 --> 00:14:39.520]   - 100%, earth is flat.
[00:14:41.920 --> 00:14:43.840]   Okay, but you said three things.
[00:14:43.840 --> 00:14:44.680]   So one is the human kind of structure.
[00:14:44.680 --> 00:14:45.800]   - Okay, so one is the human.
[00:14:45.800 --> 00:14:47.820]   And I think there will be a group of individuals
[00:14:47.820 --> 00:14:50.160]   that will swing, right?
[00:14:50.160 --> 00:14:51.720]   - Teenagers.
[00:14:51.720 --> 00:14:54.360]   - I mean, it'll be adults.
[00:14:54.360 --> 00:14:56.400]   There's actually an age demographic
[00:14:56.400 --> 00:15:00.160]   that's optimal for technology adoption.
[00:15:00.160 --> 00:15:02.280]   And you can actually find them.
[00:15:02.280 --> 00:15:03.960]   And they're actually pretty easy to find
[00:15:03.960 --> 00:15:07.000]   just based on their habits, based on...
[00:15:07.000 --> 00:15:10.400]   So someone like me who wasn't a roboticist
[00:15:10.400 --> 00:15:13.560]   would probably be the optimal kind of person, right?
[00:15:13.560 --> 00:15:15.640]   Early adopter, okay with technology,
[00:15:15.640 --> 00:15:20.000]   very comfortable and not hypersensitive, right?
[00:15:20.000 --> 00:15:23.560]   I'm just hypersensitive 'cause I designed this stuff.
[00:15:23.560 --> 00:15:25.920]   So there is a target demographic that will swing.
[00:15:25.920 --> 00:15:29.800]   The other one though is you still have these humans
[00:15:29.800 --> 00:15:31.360]   that are on the road.
[00:15:31.360 --> 00:15:34.680]   That one is a harder thing to do.
[00:15:34.680 --> 00:15:40.360]   And as long as we have people that are on the same streets,
[00:15:40.360 --> 00:15:42.560]   that's gonna be the big issue.
[00:15:42.560 --> 00:15:45.320]   And it's just because you can't possibly,
[00:15:45.320 --> 00:15:47.000]   I won't say, you can't possibly map
[00:15:47.000 --> 00:15:51.480]   some of the silliness of human drivers, right?
[00:15:51.480 --> 00:15:56.320]   Like as an example, when you're next to that car
[00:15:56.320 --> 00:15:59.880]   that has that big sticker called student driver, right?
[00:15:59.880 --> 00:16:04.680]   Like you are like, oh, either I am going to like go around.
[00:16:04.680 --> 00:16:07.960]   Like we know that that person is just gonna make mistakes
[00:16:07.960 --> 00:16:09.320]   that make no sense, right?
[00:16:09.320 --> 00:16:11.000]   How do you map that information?
[00:16:11.000 --> 00:16:14.360]   Or if I am in a car and I look over
[00:16:14.360 --> 00:16:19.280]   and I see two fairly young looking individuals
[00:16:19.280 --> 00:16:21.160]   and there's no student driver bumper
[00:16:21.160 --> 00:16:22.880]   and I see them chit chatting to each other,
[00:16:22.880 --> 00:16:26.200]   I'm like, oh, that's an issue, right?
[00:16:26.200 --> 00:16:28.560]   So how do you get that kind of information
[00:16:28.560 --> 00:16:33.560]   and that experience into basically an autopilot?
[00:16:33.560 --> 00:16:37.320]   - Yeah, and there's millions of cases like that
[00:16:37.320 --> 00:16:41.240]   where we take little hints to establish context.
[00:16:41.240 --> 00:16:44.400]   I mean, you said kind of beautifully poetic human things,
[00:16:44.400 --> 00:16:47.160]   but there's probably subtle things about the environment,
[00:16:47.160 --> 00:16:52.160]   about it being maybe time for commuters
[00:16:52.160 --> 00:16:55.360]   to start going home from work
[00:16:55.360 --> 00:16:57.200]   and therefore you can make some kind of judgment
[00:16:57.200 --> 00:17:00.520]   about the group behavior of pedestrians, blah, blah, blah.
[00:17:00.520 --> 00:17:02.720]   - Yes, yes, or even cities, right?
[00:17:02.720 --> 00:17:07.160]   Like if you're in Boston, how people cross the street,
[00:17:07.160 --> 00:17:10.680]   like lights are not an issue versus other places
[00:17:10.680 --> 00:17:15.680]   where people will actually wait for the crosswalk.
[00:17:15.680 --> 00:17:18.240]   - Seattle or somewhere peaceful.
[00:17:18.240 --> 00:17:22.560]   What I've also seen, so just even in Boston,
[00:17:22.560 --> 00:17:25.520]   that intersection to intersection is different.
[00:17:25.520 --> 00:17:28.960]   So every intersection has a personality of its own.
[00:17:28.960 --> 00:17:30.880]   So certain neighborhoods of Boston are different.
[00:17:30.880 --> 00:17:35.320]   So we're kind of, based on different timing of day,
[00:17:35.320 --> 00:17:40.320]   at night, it's all, there's a dynamic to human behavior
[00:17:40.320 --> 00:17:42.480]   that we kind of figure out ourselves.
[00:17:42.480 --> 00:17:46.120]   We're not able to introspect and figure it out,
[00:17:46.120 --> 00:17:49.360]   but somehow our brain learns it.
[00:17:49.360 --> 00:17:50.360]   - We do.
[00:17:50.360 --> 00:17:54.760]   - And so you're saying, is there a shortcut?
[00:17:54.760 --> 00:17:56.440]   Is there a shortcut though for,
[00:17:56.440 --> 00:17:59.080]   is there something that could be done, you think,
[00:17:59.080 --> 00:18:02.680]   that, you know, that's what we humans do.
[00:18:02.680 --> 00:18:04.680]   It's just like bird flight, right?
[00:18:04.680 --> 00:18:06.520]   That's the example they give for flight.
[00:18:06.520 --> 00:18:09.280]   Do you necessarily need to build a bird that flies
[00:18:09.280 --> 00:18:10.740]   or can you do an airplane?
[00:18:10.740 --> 00:18:13.040]   Is there a shortcut to it?
[00:18:13.040 --> 00:18:15.400]   - So I think that the shortcut is,
[00:18:15.400 --> 00:18:19.360]   and I kind of, I talk about it as a fixed space,
[00:18:19.360 --> 00:18:23.320]   where, so imagine that there's a neighborhood
[00:18:23.320 --> 00:18:26.840]   that's a new smart city or a new neighborhood that says,
[00:18:26.840 --> 00:18:31.520]   you know what, we are going to design this new city
[00:18:31.520 --> 00:18:33.840]   based on supporting self-driving cars.
[00:18:33.840 --> 00:18:37.680]   And then doing things, knowing that there's anomalies,
[00:18:37.680 --> 00:18:39.640]   knowing that people are like this, right?
[00:18:39.640 --> 00:18:42.120]   And designing it based on that assumption
[00:18:42.120 --> 00:18:43.960]   that like, we're going to have this,
[00:18:43.960 --> 00:18:45.560]   that would be an example of a shortcut.
[00:18:45.560 --> 00:18:49.320]   So you still have people, but you do very specific things
[00:18:49.320 --> 00:18:53.840]   to try to minimize the noise a little bit, as an example.
[00:18:53.840 --> 00:18:56.200]   - And the people themselves become accepting of the notion
[00:18:56.200 --> 00:18:57.760]   that there's autonomous cars, right?
[00:18:57.760 --> 00:18:59.720]   - Right, like they move into,
[00:18:59.720 --> 00:19:01.480]   so right now you have like a,
[00:19:01.480 --> 00:19:03.600]   you will have a self-selection bias, right?
[00:19:03.600 --> 00:19:06.280]   Like individuals will move into this neighborhood
[00:19:06.280 --> 00:19:09.520]   knowing like this is part of like the real estate pitch.
[00:19:09.520 --> 00:19:10.680]   Right?
[00:19:10.680 --> 00:19:14.200]   And so I think that's a way to do a shortcut.
[00:19:14.200 --> 00:19:17.640]   One, it allows you to deploy.
[00:19:17.640 --> 00:19:19.640]   It allows you to collect then data
[00:19:19.640 --> 00:19:22.640]   with these variances and anomalies,
[00:19:22.640 --> 00:19:24.080]   'cause people are still people,
[00:19:24.080 --> 00:19:28.880]   but it's a safer space and is more of an accepting space.
[00:19:28.880 --> 00:19:32.000]   I.e. when something in that space might happen,
[00:19:32.000 --> 00:19:34.200]   because things do,
[00:19:34.200 --> 00:19:36.120]   because you already have the self-selection,
[00:19:36.120 --> 00:19:37.560]   like people would be, I think,
[00:19:37.560 --> 00:19:40.800]   a little more forgiving than other places.
[00:19:40.800 --> 00:19:43.160]   - And you said three things, did we cover all of them?
[00:19:43.160 --> 00:19:46.440]   - The third is legal law, liability,
[00:19:46.440 --> 00:19:47.920]   which I don't really want to touch,
[00:19:47.920 --> 00:19:51.000]   but it's still of concern.
[00:19:51.000 --> 00:19:53.360]   - And the mishmash with like, with policy as well,
[00:19:53.360 --> 00:19:55.840]   sort of government, all that whole--
[00:19:55.840 --> 00:19:57.880]   - That big ball of mess.
[00:19:57.880 --> 00:19:59.200]   - Yeah, gotcha.
[00:19:59.200 --> 00:20:01.800]   - So that's, so we're out of time now.
[00:20:01.800 --> 00:20:06.040]   Do you think from a robotics perspective,
[00:20:06.040 --> 00:20:09.840]   you know, if you're kind of honest of what cars do,
[00:20:09.840 --> 00:20:14.840]   they kind of threaten each other's life all the time.
[00:20:14.840 --> 00:20:17.680]   So cars are very, I mean,
[00:20:17.680 --> 00:20:20.560]   in order to navigate intersections, there's an assertiveness,
[00:20:20.560 --> 00:20:22.360]   there's a risk taking,
[00:20:22.360 --> 00:20:25.320]   and if you were to reduce it to an objective function,
[00:20:25.320 --> 00:20:28.800]   there's a probability of murder in that function,
[00:20:28.800 --> 00:20:31.960]   meaning you killing another human being,
[00:20:31.960 --> 00:20:34.440]   and you're using that, first of all,
[00:20:34.440 --> 00:20:38.480]   it has to be low enough to be acceptable to you
[00:20:38.480 --> 00:20:41.360]   on an ethical level as an individual human being,
[00:20:41.360 --> 00:20:45.360]   but it has to be high enough for people to respect you,
[00:20:45.360 --> 00:20:47.600]   to not sort of take advantage of you completely
[00:20:47.600 --> 00:20:49.680]   and jaywalk and funny and so on.
[00:20:49.680 --> 00:20:53.180]   So, I mean, I don't think there's a right answer here,
[00:20:53.180 --> 00:20:56.160]   but what's, how do we solve that?
[00:20:56.160 --> 00:20:58.040]   How do we solve that from a robotics perspective
[00:20:58.040 --> 00:21:00.240]   when danger and human life is at stake?
[00:21:00.240 --> 00:21:02.080]   - Yeah, as they say, cars don't kill people,
[00:21:02.080 --> 00:21:02.920]   people kill people.
[00:21:02.920 --> 00:21:05.160]   - People kill people.
[00:21:05.160 --> 00:21:08.720]   - Right, so I think--
[00:21:08.720 --> 00:21:10.840]   - Now robotic algorithms would be killing people.
[00:21:10.840 --> 00:21:14.480]   - Right, so it will be robotics algorithms that are,
[00:21:14.480 --> 00:21:17.080]   no, it will be robotic algorithms don't kill people,
[00:21:17.080 --> 00:21:19.840]   developers of robotic algorithms kill people, right?
[00:21:19.840 --> 00:21:23.020]   I mean, one of the things is people are still in the loop,
[00:21:23.020 --> 00:21:26.640]   and at least in the near and midterm,
[00:21:26.640 --> 00:21:29.480]   I think people will still be in the loop at some point,
[00:21:29.480 --> 00:21:30.360]   even if it's a developer.
[00:21:30.360 --> 00:21:33.600]   Like we're not necessarily at the stage where robots
[00:21:33.600 --> 00:21:36.840]   are programming autonomous robots
[00:21:36.840 --> 00:21:39.400]   with different behaviors quite yet.
[00:21:39.400 --> 00:21:42.360]   - It's a scary notion, sorry to interrupt,
[00:21:42.360 --> 00:21:47.360]   that a developer has some responsibility
[00:21:47.360 --> 00:21:49.720]   in the death of a human being.
[00:21:49.720 --> 00:21:50.640]   That's a heavy burden.
[00:21:50.640 --> 00:21:55.520]   - I mean, I think that's why the whole aspect of ethics
[00:21:55.520 --> 00:21:59.040]   in our community is so, so important, right?
[00:21:59.040 --> 00:22:00.240]   Because it's true.
[00:22:00.240 --> 00:22:04.880]   If you think about it, you can basically say,
[00:22:04.880 --> 00:22:07.520]   I'm not going to work on weaponized AI, right?
[00:22:07.520 --> 00:22:09.900]   Like people can say, that's not what I'm gonna do.
[00:22:09.900 --> 00:22:12.760]   But yet you are programming algorithms
[00:22:12.760 --> 00:22:15.660]   that might be used in healthcare algorithms
[00:22:15.660 --> 00:22:17.280]   that might decide whether this person
[00:22:17.280 --> 00:22:19.040]   should get this medication or not,
[00:22:19.040 --> 00:22:20.660]   and they don't, and they die.
[00:22:20.660 --> 00:22:25.120]   Okay, so that is your responsibility, right?
[00:22:25.120 --> 00:22:27.360]   And if you're not conscious and aware
[00:22:27.360 --> 00:22:30.040]   that you do have that power when you're coding
[00:22:30.040 --> 00:22:35.040]   and things like that, I think that's just not a good thing.
[00:22:35.040 --> 00:22:38.080]   Like we need to think about this responsibility
[00:22:38.080 --> 00:22:41.880]   as we program robots and computing devices
[00:22:41.880 --> 00:22:43.520]   much more than we are.
[00:22:43.520 --> 00:22:47.000]   - Yeah, so it's not an option to not think about ethics.
[00:22:47.000 --> 00:22:50.940]   I think it's a majority, I would say, of computer science.
[00:22:50.940 --> 00:22:54.080]   It's kind of a hot topic now, I think,
[00:22:54.080 --> 00:22:57.760]   about bias and so on, and we'll talk about it,
[00:22:57.760 --> 00:22:59.160]   but usually it's kind of,
[00:22:59.160 --> 00:23:02.680]   it's like a very particular group of people
[00:23:02.680 --> 00:23:04.280]   that work on that.
[00:23:04.280 --> 00:23:06.960]   And then people who do robotics are like,
[00:23:06.960 --> 00:23:09.360]   well, I don't have to think about that.
[00:23:09.360 --> 00:23:11.160]   There's other smart people thinking about it.
[00:23:11.160 --> 00:23:14.560]   It seems that everybody has to think about it.
[00:23:14.560 --> 00:23:17.040]   It's not, you can't escape the ethics,
[00:23:17.040 --> 00:23:21.120]   whether it's bias or just every aspect of ethics
[00:23:21.120 --> 00:23:22.680]   that has to do with human beings.
[00:23:22.680 --> 00:23:23.520]   - Everyone.
[00:23:23.520 --> 00:23:25.720]   - So think about, I'm gonna age myself,
[00:23:25.720 --> 00:23:30.120]   but I remember when we didn't have testers, right?
[00:23:30.120 --> 00:23:31.080]   And so what did you do?
[00:23:31.080 --> 00:23:33.600]   As a developer, you had to test your own code, right?
[00:23:33.600 --> 00:23:36.120]   Like you had to go through all the cases and figure it out,
[00:23:36.120 --> 00:23:37.840]   and then they realized that,
[00:23:37.840 --> 00:23:40.640]   we probably need to have testing
[00:23:40.640 --> 00:23:42.440]   because we're not getting all the things.
[00:23:42.440 --> 00:23:45.560]   And so from there, what happens is most developers,
[00:23:45.560 --> 00:23:47.320]   they do a little bit of testing,
[00:23:47.320 --> 00:23:49.800]   but it's usually like, okay, did my compiler bug out?
[00:23:49.800 --> 00:23:51.140]   Let me look at the warnings.
[00:23:51.140 --> 00:23:52.920]   Okay, is that acceptable or not?
[00:23:52.920 --> 00:23:53.760]   Right?
[00:23:53.760 --> 00:23:55.880]   Like that's how you typically think about it as a developer
[00:23:55.880 --> 00:23:58.280]   and you're just assume that it's going to go
[00:23:58.280 --> 00:24:01.160]   to another process and they're gonna test it out.
[00:24:01.160 --> 00:24:04.400]   But I think we need to go back to those early days
[00:24:04.400 --> 00:24:07.640]   when you're a developer, you're developing,
[00:24:07.640 --> 00:24:09.560]   there should be like this, say,
[00:24:09.560 --> 00:24:12.240]   okay, let me look at the ethical outcomes of this
[00:24:12.240 --> 00:24:13.840]   because there isn't a second,
[00:24:13.840 --> 00:24:16.440]   like testing ethical testers, right?
[00:24:16.440 --> 00:24:17.280]   It's you.
[00:24:17.280 --> 00:24:21.240]   We did it back in the early coding days.
[00:24:21.240 --> 00:24:23.320]   I think that's where we are with respect to ethics.
[00:24:23.320 --> 00:24:26.360]   Like, let's go back to what was good practices
[00:24:26.360 --> 00:24:30.080]   only because we were just developing the field.
[00:24:30.080 --> 00:24:34.400]   - Yeah, and it's a really heavy burden.
[00:24:34.400 --> 00:24:37.560]   I've had to feel it recently in the last few months,
[00:24:37.560 --> 00:24:39.480]   but I think it's a good one to feel like.
[00:24:39.480 --> 00:24:42.660]   I've gotten a message, more than one from people.
[00:24:42.660 --> 00:24:47.480]   I've unfortunately gotten some attention recently
[00:24:47.480 --> 00:24:50.480]   and I've gotten messages that say that
[00:24:50.480 --> 00:24:52.360]   I have blood in my hands
[00:24:52.360 --> 00:24:56.320]   because of working on semi-autonomous vehicles.
[00:24:56.320 --> 00:24:58.960]   So the idea that you have semi-autonomy
[00:24:58.960 --> 00:25:02.040]   means people would lose vigilance and so on.
[00:25:02.040 --> 00:25:05.200]   That's actually, be humans, as we describe.
[00:25:05.200 --> 00:25:08.160]   And because of that, because of this idea
[00:25:08.160 --> 00:25:10.080]   that we're creating automation,
[00:25:10.080 --> 00:25:12.840]   there'll be people be hurt because of it.
[00:25:12.840 --> 00:25:14.560]   And I think that's a beautiful thing.
[00:25:14.560 --> 00:25:17.800]   I mean, it's many nights where I wasn't able to sleep
[00:25:17.800 --> 00:25:19.080]   because of this notion.
[00:25:19.080 --> 00:25:22.400]   You really do think about people that might die
[00:25:22.400 --> 00:25:23.840]   because of this technology.
[00:25:23.840 --> 00:25:26.560]   Of course, you can then start rationalizing and saying,
[00:25:26.560 --> 00:25:27.400]   well, you know what?
[00:25:27.400 --> 00:25:29.640]   40,000 people die in the United States every year
[00:25:29.640 --> 00:25:32.400]   and we're trying to ultimately try to save lives.
[00:25:32.400 --> 00:25:35.800]   But the reality is your code you've written
[00:25:35.800 --> 00:25:36.680]   might kill somebody.
[00:25:36.680 --> 00:25:38.920]   And that's an important burden to carry with you
[00:25:38.920 --> 00:25:40.200]   as you design the code.
[00:25:40.200 --> 00:25:43.800]   - I don't even think of it as a burden
[00:25:43.800 --> 00:25:47.520]   if we train this concept correctly from the beginning.
[00:25:47.520 --> 00:25:49.640]   And I use, and not to say that coding
[00:25:49.640 --> 00:25:52.400]   is like being a medical doctor, but think about it.
[00:25:52.400 --> 00:25:56.080]   Medical doctors, if they've been in situations
[00:25:56.080 --> 00:25:58.280]   where their patient didn't survive, right?
[00:25:58.280 --> 00:26:00.800]   Do they give up and go away?
[00:26:00.800 --> 00:26:02.480]   No, every time they come in,
[00:26:02.480 --> 00:26:05.440]   they know that there might be a possibility
[00:26:05.440 --> 00:26:07.240]   that this patient might not survive.
[00:26:07.240 --> 00:26:10.080]   And so when they approach every decision,
[00:26:10.080 --> 00:26:11.920]   like that's in their back of their head.
[00:26:11.920 --> 00:26:15.840]   And so why isn't that we aren't teaching?
[00:26:15.840 --> 00:26:17.200]   And those are tools though, right?
[00:26:17.200 --> 00:26:19.720]   They are given some of the tools to address that
[00:26:19.720 --> 00:26:21.480]   so that they don't go crazy.
[00:26:21.480 --> 00:26:24.240]   But we don't give those tools
[00:26:24.240 --> 00:26:26.160]   so that it does feel like a burden
[00:26:26.160 --> 00:26:28.720]   versus something of I have a great gift
[00:26:28.720 --> 00:26:31.120]   and I can do great, awesome good,
[00:26:31.120 --> 00:26:33.360]   but with it comes great responsibility.
[00:26:33.360 --> 00:26:35.840]   I mean, that's what we teach in terms of,
[00:26:35.840 --> 00:26:37.440]   if you think about the medical schools, right?
[00:26:37.440 --> 00:26:39.560]   Great gift, great responsibility.
[00:26:39.560 --> 00:26:42.160]   I think if we just change the messaging a little,
[00:26:42.160 --> 00:26:45.600]   great gift, being a developer, great responsibility.
[00:26:45.600 --> 00:26:48.400]   And this is how you combine those.
[00:26:48.400 --> 00:26:51.200]   - But do you think, I mean, this is really interesting.
[00:26:51.200 --> 00:26:54.320]   It's outside, I actually have no friends
[00:26:54.320 --> 00:26:57.300]   who are sort of surgeons or doctors.
[00:26:57.300 --> 00:27:01.320]   I mean, what does it feel like to make a mistake
[00:27:01.320 --> 00:27:04.800]   in a surgery and somebody to die because of that?
[00:27:04.800 --> 00:27:07.040]   Like, is that something you could be taught
[00:27:07.040 --> 00:27:10.560]   in medical school, sort of how to be accepting of that risk?
[00:27:10.560 --> 00:27:14.960]   - So, because I do a lot of work with healthcare robotics,
[00:27:14.960 --> 00:27:18.480]   I have not lost a patient, for example.
[00:27:18.480 --> 00:27:20.920]   The first one's always the hardest, right?
[00:27:20.920 --> 00:27:25.920]   But they really teach the value, right?
[00:27:25.920 --> 00:27:30.840]   So they teach responsibility, but they also teach the value.
[00:27:30.840 --> 00:27:34.920]   Like you're saving 40,000,
[00:27:34.920 --> 00:27:38.320]   but in order to really feel good about that,
[00:27:38.320 --> 00:27:40.160]   when you come to a decision,
[00:27:40.160 --> 00:27:42.280]   you have to be able to say at the end,
[00:27:42.280 --> 00:27:45.360]   I did all that I could possibly do, right?
[00:27:45.360 --> 00:27:48.840]   Versus a, well, I just picked the first widget and did,
[00:27:48.840 --> 00:27:52.280]   right, like, so every decision is actually thought through.
[00:27:52.280 --> 00:27:53.840]   It's not a habit, it's not a,
[00:27:53.840 --> 00:27:55.400]   let me just take the best algorithm
[00:27:55.400 --> 00:27:57.160]   that my friend gave me, right?
[00:27:57.160 --> 00:27:59.600]   It's a, is this it, is this the best?
[00:27:59.600 --> 00:28:03.200]   Have I done my best to do good, right?
[00:28:03.200 --> 00:28:04.040]   And so-- - You're right.
[00:28:04.040 --> 00:28:06.360]   And I think burden is the wrong word.
[00:28:06.360 --> 00:28:10.800]   It's a gift, but you have to treat it extremely seriously.
[00:28:10.800 --> 00:28:11.640]   - Correct.
[00:28:12.640 --> 00:28:15.520]   So on a slightly related note,
[00:28:15.520 --> 00:28:16.360]   in a recent paper,
[00:28:16.360 --> 00:28:20.160]   "The Ugly Truth About Ourselves and Our Robot Creations,"
[00:28:20.160 --> 00:28:24.320]   you discuss, you highlight some biases
[00:28:24.320 --> 00:28:27.120]   that may affect the function of various robotic systems.
[00:28:27.120 --> 00:28:30.120]   Can you talk through, if you remember examples of some?
[00:28:30.120 --> 00:28:31.320]   - There's a lot of examples.
[00:28:31.320 --> 00:28:33.080]   I usually-- - What is bias, first of all?
[00:28:33.080 --> 00:28:37.080]   - Yeah, so bias is this,
[00:28:37.080 --> 00:28:38.840]   and so bias, which is different than prejudice.
[00:28:38.840 --> 00:28:41.880]   So bias is that we all have these preconceived notions
[00:28:41.880 --> 00:28:45.960]   about particular, everything from particular groups
[00:28:45.960 --> 00:28:49.720]   to habits to identity, right?
[00:28:49.720 --> 00:28:51.400]   So we have these predispositions,
[00:28:51.400 --> 00:28:54.080]   and so when we address a problem,
[00:28:54.080 --> 00:28:56.040]   we look at a problem, we make a decision,
[00:28:56.040 --> 00:29:01.040]   those preconceived notions might affect our outputs,
[00:29:01.040 --> 00:29:02.240]   our outcomes.
[00:29:02.240 --> 00:29:04.680]   - So there, the bias could be positive and negative,
[00:29:04.680 --> 00:29:07.640]   and then is prejudice the negative kind of bias?
[00:29:07.640 --> 00:29:09.200]   - Prejudice is the negative, right?
[00:29:09.200 --> 00:29:13.560]   So prejudice is that not only are you aware of your bias,
[00:29:13.560 --> 00:29:18.560]   but you are then take it and have a negative outcome,
[00:29:18.560 --> 00:29:20.680]   even though you are aware.
[00:29:20.680 --> 00:29:23.000]   - And there could be gray areas too.
[00:29:23.000 --> 00:29:24.680]   - There's always gray areas.
[00:29:24.680 --> 00:29:27.600]   - That's the challenging aspect of all ethical questions.
[00:29:27.600 --> 00:29:30.040]   - So I always like, so there's a funny one,
[00:29:30.040 --> 00:29:31.760]   and in fact, I think it might be in the paper,
[00:29:31.760 --> 00:29:34.200]   'cause I think I talk about self-driving cars,
[00:29:34.200 --> 00:29:35.240]   but think about this.
[00:29:37.160 --> 00:29:40.600]   For teenagers, right, typically,
[00:29:40.600 --> 00:29:44.560]   insurance companies charge quite a bit of money
[00:29:44.560 --> 00:29:46.760]   if you have a teenage driver.
[00:29:46.760 --> 00:29:50.880]   So you could say that's an age bias, right?
[00:29:50.880 --> 00:29:54.080]   But no one will, I mean, parents will be grumpy,
[00:29:54.080 --> 00:29:58.640]   but no one really says that that's not fair.
[00:29:58.640 --> 00:29:59.480]   - That's interesting.
[00:29:59.480 --> 00:30:01.720]   We don't, that's right, that's right.
[00:30:01.720 --> 00:30:06.200]   Everybody in human factors and safety research
[00:30:07.120 --> 00:30:12.120]   almost, I mean, is quite ruthlessly critical of teenagers.
[00:30:12.120 --> 00:30:15.040]   And we don't question, is that okay?
[00:30:15.040 --> 00:30:17.160]   Is that okay to be ageist in this kind of way?
[00:30:17.160 --> 00:30:18.600]   - It is, and it is ageist, right?
[00:30:18.600 --> 00:30:20.800]   It's definitely age, there's no question about it.
[00:30:20.800 --> 00:30:24.960]   And so this is the gray area, right?
[00:30:24.960 --> 00:30:29.840]   'Cause you know that teenagers are more likely
[00:30:29.840 --> 00:30:33.080]   to be in accidents, and so there's actually some data to it.
[00:30:33.080 --> 00:30:35.000]   But then if you take that same example,
[00:30:35.000 --> 00:30:39.400]   and you say, well, I'm going to make the insurance higher
[00:30:39.400 --> 00:30:44.400]   for an area of Boston, because there's a lot of accidents.
[00:30:44.400 --> 00:30:48.280]   And then they find out that that's correlated
[00:30:48.280 --> 00:30:50.240]   with socioeconomics.
[00:30:50.240 --> 00:30:52.440]   Well, then it becomes a problem, right?
[00:30:52.440 --> 00:30:56.720]   Like that is not acceptable, but yet the teenager,
[00:30:56.720 --> 00:31:01.720]   which is age, it's against age, is, right?
[00:31:01.720 --> 00:31:03.960]   - And the way we figure that out as a society
[00:31:03.960 --> 00:31:06.200]   by having conversations, by having discourse,
[00:31:06.200 --> 00:31:08.360]   I mean, throughout history, the definition
[00:31:08.360 --> 00:31:11.360]   of what is ethical and not has changed,
[00:31:11.360 --> 00:31:14.360]   and hopefully always for the better.
[00:31:14.360 --> 00:31:15.480]   - Correct, correct.
[00:31:15.480 --> 00:31:20.480]   - So in terms of bias or prejudice in algorithms,
[00:31:20.480 --> 00:31:25.560]   what examples do you sometimes think about?
[00:31:25.560 --> 00:31:28.960]   - So I think about quite a bit the medical domain,
[00:31:28.960 --> 00:31:31.320]   just because historically, right?
[00:31:31.320 --> 00:31:34.520]   The healthcare domain has had these biases,
[00:31:34.520 --> 00:31:39.520]   typically based on gender and ethnicity, primarily,
[00:31:39.520 --> 00:31:42.320]   a little in age, but not so much.
[00:31:42.320 --> 00:31:48.680]   Historically, if you think about FDA and drug trials,
[00:31:48.680 --> 00:31:54.240]   it's harder to find women that aren't childbearing,
[00:31:54.240 --> 00:31:57.160]   and so you may not test on drugs at the same level, right?
[00:31:57.160 --> 00:31:58.920]   So there's these things.
[00:31:59.320 --> 00:32:02.920]   And so if you think about robotics, right?
[00:32:02.920 --> 00:32:04.880]   Something as simple as,
[00:32:04.880 --> 00:32:07.760]   I'd like to design an exoskeleton, right?
[00:32:07.760 --> 00:32:09.200]   What should the material be?
[00:32:09.200 --> 00:32:10.160]   What should the weight be?
[00:32:10.160 --> 00:32:12.560]   What should the form factor be?
[00:32:12.560 --> 00:32:16.960]   Who are you gonna design it around?
[00:32:16.960 --> 00:32:21.120]   I will say that in the US, women average height
[00:32:21.120 --> 00:32:23.440]   and weight is slightly different than guys.
[00:32:23.440 --> 00:32:25.880]   So who are you gonna choose?
[00:32:25.880 --> 00:32:29.080]   Like, if you're not thinking about it from the beginning,
[00:32:29.080 --> 00:32:32.080]   as, you know, okay, when I design this,
[00:32:32.080 --> 00:32:33.640]   and I look at the algorithms,
[00:32:33.640 --> 00:32:34.960]   and I design the control system,
[00:32:34.960 --> 00:32:37.080]   and the forces, and the torques,
[00:32:37.080 --> 00:32:38.280]   if you're not thinking about,
[00:32:38.280 --> 00:32:41.720]   well, you have different types of body structure,
[00:32:41.720 --> 00:32:44.600]   you're gonna design to what you're used to.
[00:32:44.600 --> 00:32:48.240]   Oh, this fits all the folks in my lab, right?
[00:32:48.240 --> 00:32:51.520]   - So think about it from the very beginning is important.
[00:32:51.520 --> 00:32:52.920]   What about sort of algorithms
[00:32:52.920 --> 00:32:56.120]   that train on data kind of thing?
[00:32:56.120 --> 00:33:01.120]   Sadly, our society already has a lot of negative bias.
[00:33:01.120 --> 00:33:03.360]   And so if we collect a lot of data,
[00:33:03.360 --> 00:33:06.320]   even if it's a balanced way,
[00:33:06.320 --> 00:33:09.040]   it's going to contain the same bias that a society contains.
[00:33:09.040 --> 00:33:13.760]   And so, yeah, is there things there that bother you?
[00:33:13.760 --> 00:33:15.640]   - Yeah, so you actually said something.
[00:33:15.640 --> 00:33:19.960]   You had said how we have biases,
[00:33:19.960 --> 00:33:21.520]   but hopefully we learn from them,
[00:33:21.520 --> 00:33:23.120]   and we become better, right?
[00:33:23.120 --> 00:33:25.160]   And so that's where we are now, right?
[00:33:25.160 --> 00:33:28.640]   So the data that we're collecting is historic.
[00:33:28.640 --> 00:33:30.160]   So it's based on these things
[00:33:30.160 --> 00:33:32.640]   when we knew it was bad to discriminate,
[00:33:32.640 --> 00:33:33.960]   but that's the data we have,
[00:33:33.960 --> 00:33:36.120]   and we're trying to fix it now,
[00:33:36.120 --> 00:33:37.880]   but we're fixing it based on the data
[00:33:37.880 --> 00:33:39.760]   that was used in the first place to discriminate.
[00:33:39.760 --> 00:33:40.680]   - Fix it in post.
[00:33:40.680 --> 00:33:43.800]   - Right, and so the decisions,
[00:33:43.800 --> 00:33:45.000]   and you can look at everything
[00:33:45.000 --> 00:33:49.320]   from the whole aspect of predictive policing,
[00:33:49.320 --> 00:33:51.440]   criminal recidivism.
[00:33:51.440 --> 00:33:54.360]   There was a recent paper that had the healthcare algorithms,
[00:33:54.360 --> 00:33:58.240]   which had a kind of a sensational titles.
[00:33:58.240 --> 00:34:01.240]   I'm not pro sensationalism in titles,
[00:34:01.240 --> 00:34:03.760]   but again, you read it, right?
[00:34:03.760 --> 00:34:05.800]   So it makes you read it,
[00:34:05.800 --> 00:34:07.040]   but I'm like, really?
[00:34:07.040 --> 00:34:08.960]   Like, ah, you could have--
[00:34:08.960 --> 00:34:10.960]   - What's the topic of the sensationalism?
[00:34:10.960 --> 00:34:12.680]   I mean, what's underneath it?
[00:34:12.680 --> 00:34:16.320]   If you could sort of educate me
[00:34:16.320 --> 00:34:19.200]   on what kind of bias creeps into the healthcare space.
[00:34:19.200 --> 00:34:20.040]   - Yeah, so--
[00:34:20.040 --> 00:34:21.440]   - I mean, you already kind of mentioned.
[00:34:21.440 --> 00:34:23.120]   - Yeah, so this one was,
[00:34:23.120 --> 00:34:27.560]   the headline was racist AI algorithms.
[00:34:27.560 --> 00:34:30.920]   Okay, like, okay, that's totally a clickbait title.
[00:34:30.920 --> 00:34:32.200]   And so you looked at it,
[00:34:32.200 --> 00:34:36.720]   and so there was data that these researchers had collected.
[00:34:36.720 --> 00:34:39.480]   I believe, I wanna say it was either science or nature.
[00:34:39.480 --> 00:34:40.720]   It was just published,
[00:34:40.720 --> 00:34:42.680]   but they didn't have the sensational title.
[00:34:42.680 --> 00:34:44.960]   It was like the media.
[00:34:44.960 --> 00:34:47.560]   And so they had looked at demographics,
[00:34:47.560 --> 00:34:52.240]   I believe, between black and white women, right?
[00:34:52.240 --> 00:34:56.920]   And they showed that there was a discrepancy
[00:34:56.920 --> 00:34:59.280]   in the outcomes, right?
[00:34:59.280 --> 00:35:02.480]   And so, and it was tied to ethnicity, tied to race.
[00:35:02.480 --> 00:35:04.880]   The piece that the researchers did
[00:35:04.880 --> 00:35:07.480]   actually went through the whole analysis,
[00:35:07.480 --> 00:35:08.880]   but of course--
[00:35:08.880 --> 00:35:11.120]   - I mean, the journalists with AI
[00:35:11.120 --> 00:35:14.320]   are problematic across the board, let's say.
[00:35:14.320 --> 00:35:16.240]   - And so this is a problem, right?
[00:35:16.240 --> 00:35:18.280]   And so there's this thing about,
[00:35:18.280 --> 00:35:20.600]   oh, AI, it has all these problems.
[00:35:20.600 --> 00:35:22.920]   We're doing it on historical data,
[00:35:22.920 --> 00:35:26.040]   and the outcomes aren't even based on gender
[00:35:26.040 --> 00:35:28.120]   or ethnicity or age.
[00:35:28.120 --> 00:35:30.280]   But I'm always saying is like,
[00:35:30.280 --> 00:35:32.520]   yes, we need to do better, right?
[00:35:32.520 --> 00:35:33.640]   We need to do better.
[00:35:33.640 --> 00:35:35.400]   It is our duty to do better,
[00:35:35.400 --> 00:35:39.880]   but the worst AI is still better than us.
[00:35:39.880 --> 00:35:41.960]   Like, you take the best of us,
[00:35:41.960 --> 00:35:44.200]   and we're still worse than the worst AI,
[00:35:44.200 --> 00:35:45.680]   at least in terms of these things.
[00:35:45.680 --> 00:35:48.040]   And that's actually not discussed, right?
[00:35:48.040 --> 00:35:51.680]   And so I think, and that's why the sensational title,
[00:35:51.680 --> 00:35:53.080]   right, and so it's like,
[00:35:53.080 --> 00:35:54.400]   so then you can have individuals go like,
[00:35:54.400 --> 00:35:55.600]   oh, we don't need to use this AI.
[00:35:55.600 --> 00:35:56.840]   I'm like, oh, no, no, no, no.
[00:35:56.840 --> 00:36:01.040]   I want the AI instead of the doctors
[00:36:01.040 --> 00:36:02.080]   that provided that data,
[00:36:02.080 --> 00:36:04.280]   'cause it's still better than that, right?
[00:36:04.280 --> 00:36:06.840]   - I think that's really important to linger on.
[00:36:06.840 --> 00:36:09.640]   Is the idea that this AI is racist,
[00:36:09.640 --> 00:36:14.200]   it's like, well, compared to what?
[00:36:14.320 --> 00:36:16.400]   (laughs)
[00:36:16.400 --> 00:36:20.160]   I think we set, unfortunately,
[00:36:20.160 --> 00:36:23.320]   way too high of a bar for AI algorithms.
[00:36:23.320 --> 00:36:26.040]   And in the ethical space where perfect is,
[00:36:26.040 --> 00:36:28.160]   I would argue, probably impossible,
[00:36:28.160 --> 00:36:33.080]   then if we set the bar of perfection, essentially,
[00:36:33.080 --> 00:36:36.280]   of it has to be perfectly fair, whatever that means,
[00:36:36.280 --> 00:36:39.640]   it means we're setting it up for failure.
[00:36:39.640 --> 00:36:42.040]   But that's really important to say what you just said,
[00:36:42.040 --> 00:36:45.000]   which is, well, it's still better than some things.
[00:36:45.000 --> 00:36:46.960]   - And one of the things I think
[00:36:46.960 --> 00:36:49.440]   that we don't get enough credit for,
[00:36:49.440 --> 00:36:52.240]   just in terms of as developers,
[00:36:52.240 --> 00:36:55.920]   is that you can now poke at it, right?
[00:36:55.920 --> 00:36:58.920]   So it's harder to say, you know, is this hospital,
[00:36:58.920 --> 00:37:01.120]   is this city doing something, right,
[00:37:01.120 --> 00:37:04.480]   until someone brings in a civil case, right?
[00:37:04.480 --> 00:37:07.160]   Well, with AI, it can process through all this data
[00:37:07.160 --> 00:37:12.160]   and say, hey, yes, there was an issue here,
[00:37:12.160 --> 00:37:14.560]   but here it is, we've identified it,
[00:37:14.560 --> 00:37:16.240]   and then the next step is to fix it.
[00:37:16.240 --> 00:37:18.160]   I mean, that's a nice feedback loop
[00:37:18.160 --> 00:37:21.400]   versus like waiting for someone to sue someone else
[00:37:21.400 --> 00:37:22.840]   before it's fixed, right?
[00:37:22.840 --> 00:37:25.160]   And so I think that power,
[00:37:25.160 --> 00:37:27.680]   we need to capitalize on a little bit more, right?
[00:37:27.680 --> 00:37:29.760]   Instead of having the sensational titles,
[00:37:29.760 --> 00:37:33.400]   have the, okay, this is a problem,
[00:37:33.400 --> 00:37:34.640]   and this is how we're fixing it,
[00:37:34.640 --> 00:37:36.600]   and people are putting money to fix it
[00:37:36.600 --> 00:37:38.680]   because we can make it better.
[00:37:38.680 --> 00:37:43.040]   I look at like facial recognition, how Joy,
[00:37:43.040 --> 00:37:45.880]   she basically called out a couple of companies and said,
[00:37:45.880 --> 00:37:50.560]   hey, and most of them were like, oh, embarrassment,
[00:37:50.560 --> 00:37:53.400]   and the next time it had been fixed, right?
[00:37:53.400 --> 00:37:54.960]   It had been fixed better, right?
[00:37:54.960 --> 00:37:56.880]   And then it was like, oh, here's some more issues.
[00:37:56.880 --> 00:38:01.880]   And I think that conversation then moves that needle
[00:38:01.880 --> 00:38:06.880]   to having much more fair and unbiased and ethical aspects,
[00:38:06.880 --> 00:38:10.640]   as long as both sides, the developers are willing to say,
[00:38:10.640 --> 00:38:14.080]   okay, I hear you, yes, we are going to improve,
[00:38:14.080 --> 00:38:16.160]   and you have other developers who are like,
[00:38:16.160 --> 00:38:19.720]   hey, AI, it's wrong, but I love it, right?
[00:38:19.720 --> 00:38:23.080]   - Yes, so speaking of this really nice notion
[00:38:23.080 --> 00:38:27.080]   that AI is maybe flawed but better than humans,
[00:38:27.080 --> 00:38:29.240]   so just made me think of it,
[00:38:29.240 --> 00:38:34.160]   one example of flawed humans is our political system.
[00:38:34.160 --> 00:38:38.760]   Do you think, or you said judicial as well,
[00:38:38.760 --> 00:38:43.760]   do you have a hope for AI sort of being elected
[00:38:43.760 --> 00:38:49.840]   for president or running our Congress
[00:38:49.840 --> 00:38:54.040]   or being able to be a powerful representative of the people?
[00:38:54.040 --> 00:38:57.160]   - So I mentioned, and I truly believe
[00:38:57.160 --> 00:39:01.480]   that this whole world of AI is in partnerships with people.
[00:39:01.480 --> 00:39:02.680]   And so what does that mean?
[00:39:02.680 --> 00:39:07.680]   I don't believe, or maybe I just don't,
[00:39:07.680 --> 00:39:11.640]   I don't believe that we should have an AI for president,
[00:39:11.640 --> 00:39:13.760]   but I do believe that a president
[00:39:13.760 --> 00:39:16.120]   should use AI as an advisor, right?
[00:39:16.120 --> 00:39:17.640]   Like if you think about it,
[00:39:17.640 --> 00:39:22.120]   every president has a cabinet of individuals
[00:39:22.120 --> 00:39:23.880]   that have different expertise
[00:39:23.880 --> 00:39:26.280]   that they should listen to, right?
[00:39:26.280 --> 00:39:28.040]   That's kind of what we do.
[00:39:28.040 --> 00:39:31.160]   And you put smart people with smart expertise
[00:39:31.160 --> 00:39:33.520]   around certain issues and you listen.
[00:39:33.520 --> 00:39:35.760]   I don't see why AI can't function
[00:39:35.760 --> 00:39:39.320]   as one of those smart individuals giving input.
[00:39:39.320 --> 00:39:41.080]   So maybe there's an AI on healthcare,
[00:39:41.080 --> 00:39:43.880]   maybe there's an AI on education and right?
[00:39:43.880 --> 00:39:48.800]   Like all of these things that a human is processing, right?
[00:39:48.800 --> 00:39:50.480]   Because at the end of the day,
[00:39:50.480 --> 00:39:53.600]   there's people that are human
[00:39:53.600 --> 00:39:55.560]   that are going to be at the end of the decision.
[00:39:55.560 --> 00:39:59.360]   And I don't think as a world, as a culture, as a society,
[00:39:59.360 --> 00:40:03.080]   that we would totally believe, and this is us,
[00:40:03.080 --> 00:40:05.360]   like this is some fallacy about us,
[00:40:05.360 --> 00:40:10.360]   but we need to see that leader, that person as human.
[00:40:10.360 --> 00:40:15.480]   And most people don't realize that like leaders
[00:40:15.480 --> 00:40:17.040]   have a whole lot of advice, right?
[00:40:17.040 --> 00:40:18.240]   Like when they say something,
[00:40:18.240 --> 00:40:20.480]   it's not that they woke up, well, usually,
[00:40:20.480 --> 00:40:22.960]   they don't wake up in the morning and be like,
[00:40:22.960 --> 00:40:24.440]   I have a brilliant idea, right?
[00:40:24.440 --> 00:40:26.840]   It's usually a, okay, let me listen.
[00:40:26.840 --> 00:40:27.680]   I have a brilliant idea,
[00:40:27.680 --> 00:40:30.000]   but let me get a little bit of feedback on this.
[00:40:30.000 --> 00:40:31.120]   Like, okay.
[00:40:31.120 --> 00:40:33.240]   And then it's a, yeah, that was an awesome idea.
[00:40:33.240 --> 00:40:36.000]   Or it's like, yeah, let me go back.
[00:40:36.000 --> 00:40:37.520]   - We already talked through a bunch of them,
[00:40:37.520 --> 00:40:41.600]   but are there some possible solutions
[00:40:41.600 --> 00:40:45.280]   to the bias that's present in our algorithms
[00:40:45.280 --> 00:40:46.760]   beyond what we just talked about?
[00:40:46.760 --> 00:40:49.400]   - So I think there's two paths.
[00:40:49.400 --> 00:40:53.800]   One is to figure out how to systematically
[00:40:53.800 --> 00:40:56.600]   do the feedback and corrections.
[00:40:56.600 --> 00:40:58.160]   So right now it's ad hoc, right?
[00:40:58.160 --> 00:41:02.480]   It's a researcher identifies some outcomes
[00:41:02.480 --> 00:41:05.440]   that are not, don't seem to be fair, right?
[00:41:05.440 --> 00:41:07.960]   They publish it, they write about it,
[00:41:07.960 --> 00:41:11.480]   and either the developer or the companies
[00:41:11.480 --> 00:41:14.280]   that have adopted the algorithms may try to fix it, right?
[00:41:14.280 --> 00:41:18.880]   And so it's really ad hoc and it's not systematic.
[00:41:18.880 --> 00:41:22.480]   There's, it's just, it's kind of like, I'm a researcher.
[00:41:22.480 --> 00:41:24.760]   That seems like an interesting problem,
[00:41:24.760 --> 00:41:26.600]   which means that there's a whole lot out there
[00:41:26.600 --> 00:41:29.160]   that's not being looked at, right?
[00:41:29.160 --> 00:41:31.080]   'Cause it's kind of researcher driven.
[00:41:31.080 --> 00:41:35.720]   And I don't necessarily have a solution,
[00:41:35.720 --> 00:41:40.720]   but that process I think could be done a little bit better.
[00:41:40.720 --> 00:41:45.080]   One way is I'm going to poke a little bit
[00:41:45.080 --> 00:41:48.320]   at some of the corporations, right?
[00:41:48.320 --> 00:41:51.760]   Like maybe the corporations, when they think about a product
[00:41:51.760 --> 00:41:56.760]   they should, instead of, in addition to hiring these bug,
[00:41:56.760 --> 00:41:59.120]   they give these--
[00:41:59.120 --> 00:42:01.280]   - Oh yeah, yeah, yeah.
[00:42:01.280 --> 00:42:03.000]   Like awards when you find a bug.
[00:42:03.000 --> 00:42:05.840]   - Yeah, security bug.
[00:42:05.840 --> 00:42:08.600]   Let's put it like, we will give the,
[00:42:08.600 --> 00:42:10.320]   whatever the award is that we give
[00:42:10.320 --> 00:42:12.680]   for the people who find these security holes,
[00:42:12.680 --> 00:42:14.040]   find an ethics hole, right?
[00:42:14.040 --> 00:42:15.440]   Like find an unfairness hole
[00:42:15.440 --> 00:42:17.920]   and we will pay you X for each one you find.
[00:42:17.920 --> 00:42:19.680]   I mean, why can't they do that?
[00:42:19.680 --> 00:42:21.120]   One is a win-win.
[00:42:21.120 --> 00:42:23.160]   They show that they're concerned about it,
[00:42:23.160 --> 00:42:24.240]   that this is important,
[00:42:24.240 --> 00:42:26.440]   and they don't have to necessarily dedicate
[00:42:26.440 --> 00:42:28.880]   their own internal resources.
[00:42:28.880 --> 00:42:31.240]   And it also means that everyone who has
[00:42:31.240 --> 00:42:34.680]   their own bias lens, like I'm interested in age
[00:42:34.680 --> 00:42:36.640]   and so I'll find the ones based on age
[00:42:36.640 --> 00:42:38.480]   and I'm interested in gender, right?
[00:42:38.480 --> 00:42:39.840]   Which means that you get
[00:42:39.840 --> 00:42:41.640]   all of these different perspectives.
[00:42:41.640 --> 00:42:43.440]   - But you think of it in a data driven way.
[00:42:43.440 --> 00:42:48.440]   So like, sort of, if we look at a company like Twitter,
[00:42:48.800 --> 00:42:51.680]   it gets, it's under a lot of fire
[00:42:51.680 --> 00:42:54.800]   for discriminating against certain political beliefs.
[00:42:54.800 --> 00:42:55.880]   - Correct.
[00:42:55.880 --> 00:42:58.080]   - And sort of, there's a lot of people,
[00:42:58.080 --> 00:42:59.240]   this is the sad thing,
[00:42:59.240 --> 00:43:00.720]   'cause I know how hard the problem is
[00:43:00.720 --> 00:43:03.080]   and I know the Twitter folks are working really hard at it.
[00:43:03.080 --> 00:43:04.960]   Even Facebook, that everyone seems to hate,
[00:43:04.960 --> 00:43:06.880]   are working really hard at this.
[00:43:06.880 --> 00:43:09.320]   You know, the kind of evidence that people bring
[00:43:09.320 --> 00:43:11.240]   is basically anecdotal evidence.
[00:43:11.240 --> 00:43:15.000]   Well, me or my friend, all we said is X,
[00:43:15.000 --> 00:43:17.120]   and for that we got banned.
[00:43:17.120 --> 00:43:20.960]   And that's kind of a discussion of saying,
[00:43:20.960 --> 00:43:23.280]   well, look, that's usually, first of all,
[00:43:23.280 --> 00:43:25.480]   the whole thing is taken out of context.
[00:43:25.480 --> 00:43:28.640]   So they present sort of anecdotal evidence.
[00:43:28.640 --> 00:43:31.120]   And how are you supposed to, as a company,
[00:43:31.120 --> 00:43:33.080]   in a healthy way, have a discourse
[00:43:33.080 --> 00:43:36.000]   about what is and isn't ethical?
[00:43:36.000 --> 00:43:38.040]   How do we make algorithms ethical
[00:43:38.040 --> 00:43:40.760]   when people are just blowing everything?
[00:43:40.760 --> 00:43:45.120]   Like, they're outraged about a particular
[00:43:45.120 --> 00:43:47.200]   anecdotal piece of evidence
[00:43:47.200 --> 00:43:49.440]   that's very difficult to sort of contextualize
[00:43:49.440 --> 00:43:51.620]   in a big data-driven way.
[00:43:51.620 --> 00:43:55.760]   Do you have a hope for companies like Twitter and Facebook?
[00:43:55.760 --> 00:43:59.760]   - Yeah, so I think there's a couple of things going on.
[00:43:59.760 --> 00:44:04.760]   First off, remember this whole aspect
[00:44:04.760 --> 00:44:09.360]   of we are becoming reliant on technology.
[00:44:09.360 --> 00:44:14.360]   We're also becoming reliant on a lot of these,
[00:44:14.360 --> 00:44:18.000]   the apps and the resources that are provided.
[00:44:18.000 --> 00:44:21.640]   So some of it is kind of anger, like, I need you, right?
[00:44:21.640 --> 00:44:23.320]   And you're not working for me, right?
[00:44:23.320 --> 00:44:24.680]   - Yeah, not working for me, right.
[00:44:24.680 --> 00:44:27.280]   - But I think, and so some of it,
[00:44:27.280 --> 00:44:31.400]   and I wish that there was a little bit
[00:44:31.400 --> 00:44:32.840]   of change of rethinking.
[00:44:32.840 --> 00:44:35.560]   So some of it is like, oh, we'll fix it in-house.
[00:44:35.560 --> 00:44:39.000]   No, that's like, okay, I'm a fox,
[00:44:39.000 --> 00:44:40.920]   and I'm going to watch these hens
[00:44:40.920 --> 00:44:44.080]   because I think it's a problem that foxes eat hens.
[00:44:44.080 --> 00:44:45.200]   No, right?
[00:44:45.200 --> 00:44:48.640]   Like, use, like, be good citizens and say,
[00:44:48.640 --> 00:44:49.920]   look, we have a problem,
[00:44:49.920 --> 00:44:54.840]   and we are willing to open ourselves up
[00:44:54.840 --> 00:44:57.080]   for others to come in and look at it
[00:44:57.080 --> 00:44:58.760]   and not try to fix it in-house.
[00:44:58.760 --> 00:45:00.500]   Because if you fix it in-house,
[00:45:00.500 --> 00:45:02.000]   there's conflict of interest.
[00:45:02.000 --> 00:45:04.480]   If I find something, I'm probably going to want to fix it,
[00:45:04.480 --> 00:45:07.360]   and hopefully the media won't pick it up, right?
[00:45:07.360 --> 00:45:09.360]   And that then causes distrust
[00:45:09.360 --> 00:45:11.920]   because someone inside is going to be mad at you
[00:45:11.920 --> 00:45:13.640]   and go out and talk about how,
[00:45:13.640 --> 00:45:17.800]   yeah, they can the resume survey because, right?
[00:45:17.800 --> 00:45:19.360]   Like, be best people.
[00:45:19.360 --> 00:45:22.800]   Like, just say, look, we have this issue.
[00:45:22.800 --> 00:45:24.480]   Community, help us fix it,
[00:45:24.480 --> 00:45:25.840]   and we will give you, like, you know,
[00:45:25.840 --> 00:45:28.160]   the bug finder fee if you do.
[00:45:28.160 --> 00:45:31.320]   - Did you ever hope that the community,
[00:45:31.320 --> 00:45:35.360]   us as a human civilization on the whole is good
[00:45:35.360 --> 00:45:39.560]   and can be trusted to guide the future of our civilization
[00:45:39.560 --> 00:45:41.000]   into a positive direction?
[00:45:41.000 --> 00:45:41.920]   - I think so.
[00:45:41.920 --> 00:45:44.160]   So I'm an optimist, right?
[00:45:44.160 --> 00:45:48.520]   And, you know, there were some dark times in history,
[00:45:48.520 --> 00:45:50.040]   always.
[00:45:50.040 --> 00:45:52.960]   I think now we're in one of those dark times.
[00:45:52.960 --> 00:45:53.800]   I truly do.
[00:45:53.800 --> 00:45:54.680]   - In which aspect?
[00:45:54.680 --> 00:45:56.320]   - The polarization.
[00:45:56.320 --> 00:45:57.600]   And it's not just US, right?
[00:45:57.600 --> 00:46:00.080]   So if it was just US, I'd be like, yeah, it's a US thing,
[00:46:00.080 --> 00:46:03.520]   but we're seeing it, like, worldwide, this polarization.
[00:46:03.520 --> 00:46:06.580]   And so I worry about that.
[00:46:06.580 --> 00:46:10.240]   But I do fundamentally believe
[00:46:10.240 --> 00:46:13.480]   that at the end of the day, people are good, right?
[00:46:13.480 --> 00:46:14.840]   And why do I say that?
[00:46:14.840 --> 00:46:17.760]   Because anytime there's a scenario
[00:46:17.760 --> 00:46:20.880]   where people are in danger, and I will use,
[00:46:20.880 --> 00:46:24.320]   so Atlanta, we had a snowmageddon,
[00:46:24.320 --> 00:46:26.680]   and people can laugh about that.
[00:46:26.680 --> 00:46:30.520]   People at the time, so the city closed for, you know,
[00:46:30.520 --> 00:46:33.480]   little snow, but it was ice, and the city closed down.
[00:46:33.480 --> 00:46:35.800]   But you had people opening up their homes and saying,
[00:46:35.800 --> 00:46:39.120]   hey, you have nowhere to go, come to my house, right?
[00:46:39.120 --> 00:46:41.840]   Hotels were just saying, like, sleep on the floor.
[00:46:41.840 --> 00:46:44.440]   Like, places like, you know, the grocery stores were like,
[00:46:44.440 --> 00:46:45.960]   hey, here's food.
[00:46:45.960 --> 00:46:47.960]   There was no like, oh, how much are you gonna pay me?
[00:46:47.960 --> 00:46:50.520]   It was like this, such a community.
[00:46:50.520 --> 00:46:52.160]   And like, people who didn't know each other,
[00:46:52.160 --> 00:46:55.560]   strangers were just like, can I give you a ride home?
[00:46:55.560 --> 00:46:57.920]   And that was a point I was like, you know what?
[00:46:57.920 --> 00:47:03.120]   - That reveals that the deeper thing is,
[00:47:03.120 --> 00:47:06.960]   there's a compassionate love that we all have within us.
[00:47:06.960 --> 00:47:09.480]   It's just that when all of that is taken care of
[00:47:09.480 --> 00:47:11.280]   and get bored, we love drama.
[00:47:11.280 --> 00:47:14.800]   And that's, I think almost like the division
[00:47:14.800 --> 00:47:17.040]   is a sign of the times being good,
[00:47:17.040 --> 00:47:19.000]   is that it's just entertaining
[00:47:19.000 --> 00:47:22.840]   on some unpleasant mammalian level
[00:47:22.840 --> 00:47:26.120]   to watch, to disagree with others.
[00:47:26.120 --> 00:47:28.120]   And Twitter and Facebook are actually
[00:47:28.120 --> 00:47:31.360]   taking advantage of that in a sense
[00:47:31.360 --> 00:47:34.040]   because it brings you back to the platform,
[00:47:34.040 --> 00:47:36.120]   and they're advertiser-driven,
[00:47:36.120 --> 00:47:37.680]   so they make a lot of money.
[00:47:37.680 --> 00:47:39.240]   - So you go back and you click.
[00:47:39.240 --> 00:47:41.240]   - Love doesn't sell quite as well
[00:47:41.240 --> 00:47:42.680]   in terms of advertisement.
[00:47:42.680 --> 00:47:44.920]   - It doesn't.
[00:47:44.920 --> 00:47:46.960]   - So you've started your career
[00:47:46.960 --> 00:47:49.080]   at NASA Jet Propulsion Laboratory,
[00:47:49.080 --> 00:47:51.960]   but before I ask a few questions there,
[00:47:51.960 --> 00:47:54.440]   have you happened to have ever seen Space Odyssey,
[00:47:54.440 --> 00:47:55.840]   2001 Space Odyssey?
[00:47:55.840 --> 00:47:58.040]   - Yes.
[00:47:58.040 --> 00:48:01.440]   - Okay, do you think Hal 9000,
[00:48:01.440 --> 00:48:03.440]   so we're talking about ethics,
[00:48:03.440 --> 00:48:06.720]   do you think Hal did the right thing
[00:48:06.720 --> 00:48:08.600]   by taking the priority of the mission
[00:48:08.600 --> 00:48:10.280]   over the lives of the astronauts?
[00:48:10.280 --> 00:48:12.420]   Do you think Hal is good or evil?
[00:48:12.420 --> 00:48:16.960]   Easy questions.
[00:48:16.960 --> 00:48:17.800]   - Yeah.
[00:48:17.800 --> 00:48:21.440]   Hal was misguided.
[00:48:21.440 --> 00:48:23.440]   - You're one of the people that would be
[00:48:23.440 --> 00:48:26.160]   in charge of an algorithm like Hal.
[00:48:26.160 --> 00:48:27.000]   - Yeah.
[00:48:27.000 --> 00:48:28.360]   - So how would you do better?
[00:48:28.360 --> 00:48:32.280]   - If you think about what happened was
[00:48:32.280 --> 00:48:35.400]   there was no fail safe, right?
[00:48:35.400 --> 00:48:37.800]   So perfection, right?
[00:48:37.800 --> 00:48:38.640]   Like what is that?
[00:48:38.640 --> 00:48:40.880]   I'm gonna make something that I think is perfect,
[00:48:40.880 --> 00:48:44.640]   but if my assumptions are wrong,
[00:48:44.640 --> 00:48:47.600]   it'll be perfect based on the wrong assumptions, right?
[00:48:47.600 --> 00:48:51.760]   That's something that you don't know until you deploy
[00:48:51.760 --> 00:48:53.880]   and then you're like, oh yeah, messed up.
[00:48:53.880 --> 00:48:58.400]   But what that means is that when we design software,
[00:48:58.400 --> 00:49:00.720]   such as in Space Odyssey,
[00:49:00.720 --> 00:49:02.200]   when we put things out,
[00:49:02.200 --> 00:49:04.120]   that there has to be a fail safe.
[00:49:04.120 --> 00:49:07.800]   There has to be the ability that once it's out there,
[00:49:07.800 --> 00:49:11.440]   we can grade it as an F and it fails
[00:49:11.440 --> 00:49:13.160]   and it doesn't continue, right?
[00:49:13.160 --> 00:49:16.120]   There's some way that it can be brought in
[00:49:16.120 --> 00:49:19.720]   and removed and that's aspect.
[00:49:19.720 --> 00:49:21.160]   Because that's what happened with Hal.
[00:49:21.160 --> 00:49:23.840]   It was like assumptions were wrong.
[00:49:23.840 --> 00:49:27.920]   It was perfectly correct based on those assumptions.
[00:49:27.920 --> 00:49:31.080]   And there was no way to change it,
[00:49:31.080 --> 00:49:34.080]   change the assumptions at all.
[00:49:34.080 --> 00:49:37.080]   - And the change, the fallback would be to a human.
[00:49:37.080 --> 00:49:40.080]   So you ultimately think like human should be,
[00:49:40.080 --> 00:49:45.640]   it's not turtles or AI all the way down.
[00:49:45.640 --> 00:49:47.280]   It's at some point there's a human
[00:49:47.280 --> 00:49:48.120]   that actually makes a decision.
[00:49:48.120 --> 00:49:49.840]   - I still think that, and again,
[00:49:49.840 --> 00:49:51.440]   because I do human robot interaction,
[00:49:51.440 --> 00:49:55.040]   I still think the human needs to be part of the equation
[00:49:55.040 --> 00:49:56.480]   at some point.
[00:49:56.480 --> 00:49:58.480]   - So what, just looking back,
[00:49:58.480 --> 00:50:01.960]   what are some fascinating things in robotic space
[00:50:01.960 --> 00:50:03.520]   that NASA was working at the time?
[00:50:03.520 --> 00:50:07.720]   Or just in general, what have you gotten to play with
[00:50:07.720 --> 00:50:10.080]   and what are your memories from working at NASA?
[00:50:10.080 --> 00:50:12.600]   - Yeah, so one of my first memories
[00:50:12.600 --> 00:50:18.640]   was they were working on a surgical robot system
[00:50:18.640 --> 00:50:21.880]   that could do eye surgery, right?
[00:50:21.880 --> 00:50:25.680]   And this was back in, oh my gosh, it must've been,
[00:50:25.680 --> 00:50:30.560]   oh, maybe '92, '93, '94.
[00:50:30.560 --> 00:50:32.840]   - So it's like almost like a remote operation.
[00:50:32.840 --> 00:50:34.480]   - Yeah, it was remote operation.
[00:50:34.480 --> 00:50:38.360]   And in fact, you can even find some old tech reports on it.
[00:50:38.360 --> 00:50:41.600]   So think of it, like now we have Da Vinci, right?
[00:50:41.600 --> 00:50:45.840]   Like think of it, but these were like the late '90s, right?
[00:50:45.840 --> 00:50:48.200]   And I remember going into the lab one day
[00:50:48.200 --> 00:50:50.960]   and I was like, what's that, right?
[00:50:50.960 --> 00:50:53.880]   And of course it wasn't pretty, right?
[00:50:53.880 --> 00:50:56.600]   'Cause the technology, but it was like functional
[00:50:56.600 --> 00:50:59.200]   and you had this individual that could use
[00:50:59.200 --> 00:51:01.920]   a version of haptics to actually do the surgery.
[00:51:01.920 --> 00:51:04.320]   And they had this mock-up of a human face
[00:51:04.320 --> 00:51:08.400]   and like the eyeballs and you can see this little drill.
[00:51:08.400 --> 00:51:11.640]   And I was like, oh, that is so cool.
[00:51:11.640 --> 00:51:13.680]   That one I vividly remember
[00:51:13.680 --> 00:51:18.600]   because it was so outside of my like possible thoughts
[00:51:18.600 --> 00:51:20.000]   of what could be done.
[00:51:20.000 --> 00:51:21.320]   - Just the kind of precision.
[00:51:21.320 --> 00:51:26.080]   And I mean, what's the most amazing of a thing like that?
[00:51:26.080 --> 00:51:28.200]   - I think it was the precision.
[00:51:28.200 --> 00:51:33.200]   It was the kind of first time that I had physically seen
[00:51:33.200 --> 00:51:39.600]   this robot machine human interface, right?
[00:51:39.600 --> 00:51:42.360]   Versus, 'cause manufacturing had been,
[00:51:42.360 --> 00:51:44.520]   you saw those kinds of big robots, right?
[00:51:44.520 --> 00:51:48.040]   But this was like, oh, this is in a person.
[00:51:48.040 --> 00:51:51.400]   There's a person and a robot like in the same space.
[00:51:51.400 --> 00:51:53.040]   - The meeting them in person.
[00:51:53.040 --> 00:51:56.040]   Like for me, it was a magical moment that I can't,
[00:51:56.040 --> 00:51:59.640]   as life transforming that I recently met Spot Mini
[00:51:59.640 --> 00:52:01.320]   from Boston Dynamics.
[00:52:01.320 --> 00:52:04.720]   I don't know why, but on the human robot interaction,
[00:52:04.720 --> 00:52:07.800]   for some reason I realized how easy it is
[00:52:07.800 --> 00:52:09.800]   to anthropomorphize.
[00:52:09.800 --> 00:52:13.520]   And it was, I don't know, it was almost like falling in love
[00:52:13.520 --> 00:52:14.840]   this feeling of meeting.
[00:52:14.840 --> 00:52:17.440]   And I've obviously seen these robots a lot
[00:52:17.440 --> 00:52:19.280]   in video and so on, but meeting in person,
[00:52:19.280 --> 00:52:21.000]   just having that one-on-one time.
[00:52:21.000 --> 00:52:22.440]   - It's different. - It's different.
[00:52:22.440 --> 00:52:25.160]   So have you had a robot like that in your life
[00:52:25.160 --> 00:52:28.400]   that made you maybe fall in love with robotics?
[00:52:28.400 --> 00:52:30.600]   Sort of like meeting in person?
[00:52:30.600 --> 00:52:35.040]   - I mean, I loved robotics.
[00:52:35.040 --> 00:52:37.000]   - From the beginning. - Yeah, so.
[00:52:37.000 --> 00:52:39.600]   I was a 12-year-old, like I'm gonna be a roboticist.
[00:52:39.600 --> 00:52:41.280]   Actually, I called it cybernetics.
[00:52:41.280 --> 00:52:44.800]   But so my motivation was bionic woman.
[00:52:44.800 --> 00:52:46.480]   I don't know if you know that.
[00:52:46.480 --> 00:52:49.600]   And so, I mean, that was like a seminal moment,
[00:52:49.600 --> 00:52:52.440]   but I didn't meet, like that was TV, right?
[00:52:52.440 --> 00:52:54.640]   Like it wasn't like I was in the same space and I met,
[00:52:54.640 --> 00:52:56.640]   I was like, oh my gosh, you're like real.
[00:52:56.640 --> 00:52:58.920]   - Just linking on bionic woman, which by the way,
[00:52:58.920 --> 00:53:03.320]   because I read that about you, I watched a bit of it
[00:53:03.320 --> 00:53:05.640]   and it's just so, no offense, terrible.
[00:53:05.640 --> 00:53:06.480]   (laughing)
[00:53:06.480 --> 00:53:07.880]   - It's cheesy. - It's cheesy.
[00:53:07.880 --> 00:53:08.960]   - If you look at it now. - It's cheesy.
[00:53:08.960 --> 00:53:10.800]   - I've seen a couple of reruns lately.
[00:53:10.800 --> 00:53:11.640]   (laughing)
[00:53:11.640 --> 00:53:14.280]   - But it's, but of course at the time,
[00:53:14.280 --> 00:53:16.120]   it was probably-- - But the sound effects.
[00:53:16.120 --> 00:53:16.960]   - It was probably the sound of my imagination.
[00:53:16.960 --> 00:53:19.200]   (laughing)
[00:53:19.200 --> 00:53:23.160]   Especially when you're younger, it just catches you.
[00:53:23.160 --> 00:53:24.760]   But which aspect, did you think of it,
[00:53:24.760 --> 00:53:27.760]   you mentioned cybernetics, did you think of it as robotics
[00:53:27.760 --> 00:53:29.440]   or did you think of it as almost
[00:53:29.440 --> 00:53:31.680]   constructing artificial beings?
[00:53:31.680 --> 00:53:34.240]   Like is it the intelligent part
[00:53:34.240 --> 00:53:37.040]   that captured your fascination
[00:53:37.040 --> 00:53:39.400]   or was it the whole thing, like even just the limbs
[00:53:39.400 --> 00:53:40.520]   and just the-- - So for me,
[00:53:40.520 --> 00:53:42.960]   it would have, in another world,
[00:53:42.960 --> 00:53:46.840]   I probably would have been more of a biomedical engineer
[00:53:46.840 --> 00:53:50.040]   because what fascinated me was the parts,
[00:53:50.040 --> 00:53:55.040]   like the bionic parts, the limbs, those aspects of it.
[00:53:55.040 --> 00:53:59.640]   - Are you especially drawn to humanoid or human-like robots?
[00:53:59.640 --> 00:54:03.040]   - I would say human-like, not humanoid, right?
[00:54:03.040 --> 00:54:04.200]   And when I say human-like,
[00:54:04.200 --> 00:54:07.800]   I think it's this aspect of that interaction,
[00:54:07.800 --> 00:54:10.680]   whether it's social and it's like a dog, right?
[00:54:10.680 --> 00:54:14.120]   Like that's human-like because it understands us,
[00:54:14.120 --> 00:54:17.680]   it interacts with us at that very social level.
[00:54:17.680 --> 00:54:21.860]   Humanoids are part of that,
[00:54:21.860 --> 00:54:26.860]   but only if they interact with us as if we are human.
[00:54:26.860 --> 00:54:30.920]   - But just to linger on NASA for a little bit,
[00:54:30.920 --> 00:54:34.080]   what do you think, maybe if you have other memories,
[00:54:34.080 --> 00:54:38.560]   but also what do you think is the future of robots in space?
[00:54:38.560 --> 00:54:41.880]   We mentioned how, but there's incredible robots
[00:54:41.880 --> 00:54:43.400]   that NASA's working on in general,
[00:54:43.400 --> 00:54:48.160]   thinking about in our, as we venture out,
[00:54:48.160 --> 00:54:50.440]   human civilization ventures out into space.
[00:54:50.440 --> 00:54:52.240]   What do you think the future of robots is there?
[00:54:52.240 --> 00:54:53.680]   - Yeah, so I mean, there's the near term.
[00:54:53.680 --> 00:54:57.280]   For example, they just announced the rover
[00:54:57.280 --> 00:55:00.720]   that's going to the moon, which, you know,
[00:55:00.720 --> 00:55:05.200]   that's kind of exciting, but that's like near term.
[00:55:06.040 --> 00:55:11.040]   You know, my favorite, favorite, favorite series
[00:55:11.040 --> 00:55:13.300]   is "Star Trek," right?
[00:55:13.300 --> 00:55:17.160]   You know, I really hope, and even "Star Trek,"
[00:55:17.160 --> 00:55:20.060]   like if I calculate the years, I wouldn't be alive,
[00:55:20.060 --> 00:55:25.060]   but I would really, really love to be in that world.
[00:55:25.060 --> 00:55:28.440]   Like even if it's just at the beginning,
[00:55:28.440 --> 00:55:33.160]   like, you know, like "Voyage," like "Adventure 1."
[00:55:33.160 --> 00:55:35.720]   - So basically living in space.
[00:55:35.720 --> 00:55:36.560]   - Yeah.
[00:55:36.560 --> 00:55:39.760]   - With what robots, what do robots--
[00:55:39.760 --> 00:55:40.600]   - With data.
[00:55:40.600 --> 00:55:41.420]   - What role--
[00:55:41.420 --> 00:55:42.840]   - The data would have to be, even though that wasn't,
[00:55:42.840 --> 00:55:44.760]   you know, that was like later, but--
[00:55:44.760 --> 00:55:49.160]   - So data is a robot that has human-like qualities.
[00:55:49.160 --> 00:55:51.080]   - Right, without the emotion ship, yeah.
[00:55:51.080 --> 00:55:52.200]   - You don't like emotion in your robots.
[00:55:52.200 --> 00:55:54.240]   - Well, so data with the emotion ship
[00:55:54.240 --> 00:55:58.440]   was kind of a mess, right?
[00:55:58.440 --> 00:56:03.440]   It took a while for that, him to adapt,
[00:56:04.640 --> 00:56:08.600]   but, and so why was that an issue?
[00:56:08.600 --> 00:56:13.600]   The issue is, is that emotions make us irrational agents.
[00:56:13.600 --> 00:56:15.200]   That's the problem.
[00:56:15.200 --> 00:56:20.040]   And yet he could think through things,
[00:56:20.040 --> 00:56:23.440]   even if it was based on an emotional scenario, right?
[00:56:23.440 --> 00:56:25.100]   Based on pros and cons.
[00:56:25.100 --> 00:56:28.520]   But as soon as you made him emotional,
[00:56:28.520 --> 00:56:31.160]   one of the metrics he used for evaluation
[00:56:31.160 --> 00:56:33.280]   was his own emotions.
[00:56:33.280 --> 00:56:35.480]   Not people around him, right?
[00:56:35.480 --> 00:56:37.280]   Like, and so--
[00:56:37.280 --> 00:56:39.000]   - We do that as children, right?
[00:56:39.000 --> 00:56:40.920]   So we're very egocentric when we're young.
[00:56:40.920 --> 00:56:42.320]   - We are very egocentric.
[00:56:42.320 --> 00:56:44.900]   - And so isn't that just an early version
[00:56:44.900 --> 00:56:46.400]   of the emotion ship then?
[00:56:46.400 --> 00:56:48.240]   I haven't watched much "Star Trek."
[00:56:48.240 --> 00:56:50.740]   - Except I have also met adults.
[00:56:50.740 --> 00:56:54.600]   Right, and so that is a developmental process,
[00:56:54.600 --> 00:56:57.600]   and I'm sure there's a bunch of psychologists
[00:56:57.600 --> 00:57:00.640]   that could go through, like you can have a 60-year-old adult
[00:57:00.640 --> 00:57:04.640]   who has the emotional maturity of a 10-year-old, right?
[00:57:04.640 --> 00:57:08.880]   And so there's various phases that people should go through
[00:57:08.880 --> 00:57:11.460]   in order to evolve, and sometimes you don't.
[00:57:11.460 --> 00:57:14.840]   - So how much psychology do you think,
[00:57:14.840 --> 00:57:17.600]   a topic that's rarely mentioned in robotics,
[00:57:17.600 --> 00:57:19.720]   but how much does psychology come to play
[00:57:19.720 --> 00:57:23.600]   when you're talking about HRI, human-robot interaction,
[00:57:23.600 --> 00:57:25.000]   when you have to have robots
[00:57:25.000 --> 00:57:26.120]   that actually interact with humans?
[00:57:26.120 --> 00:57:26.960]   - Tons.
[00:57:27.800 --> 00:57:30.400]   Like my group, as well as I,
[00:57:30.400 --> 00:57:33.320]   read a lot in the cognitive science literature
[00:57:33.320 --> 00:57:36.200]   as well as the psychology literature,
[00:57:36.200 --> 00:57:41.200]   because they understand a lot about human-human relations
[00:57:41.200 --> 00:57:45.960]   and developmental milestones and things like that.
[00:57:45.960 --> 00:57:50.960]   And so we tend to look to see what's been done out there.
[00:57:50.960 --> 00:57:55.640]   Sometimes what we'll do is we'll try to match that
[00:57:55.640 --> 00:57:58.440]   to see is that human-human relationship
[00:57:58.440 --> 00:58:01.020]   the same as human-robot?
[00:58:01.020 --> 00:58:03.100]   Sometimes it is, and sometimes it's different.
[00:58:03.100 --> 00:58:05.880]   And then when it's different, we try to figure out,
[00:58:05.880 --> 00:58:09.060]   okay, why is it different in this scenario,
[00:58:09.060 --> 00:58:11.920]   but it's the same in the other scenario, right?
[00:58:11.920 --> 00:58:15.360]   And so we try to do that quite a bit.
[00:58:15.360 --> 00:58:17.840]   - Would you say that's, if we're looking at the future
[00:58:17.840 --> 00:58:19.160]   of human-robot interaction,
[00:58:19.160 --> 00:58:21.800]   would you say the psychology piece is the hardest?
[00:58:23.120 --> 00:58:25.440]   I mean, it's a funny notion for you as,
[00:58:25.440 --> 00:58:27.400]   I don't know if you consider, yeah.
[00:58:27.400 --> 00:58:28.440]   I mean, one way to ask it,
[00:58:28.440 --> 00:58:32.040]   do you consider yourself a roboticist or a psychologist?
[00:58:32.040 --> 00:58:33.640]   - Oh, I consider myself a roboticist
[00:58:33.640 --> 00:58:36.280]   that plays the act of a psychologist.
[00:58:36.280 --> 00:58:38.160]   - But if you were to look at yourself
[00:58:38.160 --> 00:58:42.400]   sort of 20, 30 years from now,
[00:58:42.400 --> 00:58:45.400]   do you see yourself more and more wearing the psychology hat?
[00:58:45.400 --> 00:58:49.040]   Another way to put it is,
[00:58:49.040 --> 00:58:51.640]   are the hard problems in human-robot interactions
[00:58:51.640 --> 00:58:55.840]   fundamentally psychology, or is it still robotics,
[00:58:55.840 --> 00:58:57.760]   the perception, manipulation, planning,
[00:58:57.760 --> 00:58:59.500]   all that kind of stuff?
[00:58:59.500 --> 00:59:01.720]   - It's actually neither.
[00:59:01.720 --> 00:59:05.200]   The hardest part is the adaptation and the interaction.
[00:59:05.200 --> 00:59:07.200]   So-- - The learning.
[00:59:07.200 --> 00:59:08.880]   - It's the interface, it's the learning.
[00:59:08.880 --> 00:59:10.820]   And so if I think of,
[00:59:10.820 --> 00:59:16.880]   I've become much more of a roboticist/AI person
[00:59:16.880 --> 00:59:19.080]   than when I, like originally, again,
[00:59:19.080 --> 00:59:20.200]   I was about the bionics.
[00:59:20.200 --> 00:59:24.080]   I was electrical engineer, I was control theory, right?
[00:59:24.080 --> 00:59:28.840]   And then I started realizing that my algorithms
[00:59:28.840 --> 00:59:30.640]   needed human data, right?
[00:59:30.640 --> 00:59:32.800]   And so then I was like, okay, what is this human thing?
[00:59:32.800 --> 00:59:34.400]   How do I incorporate human data?
[00:59:34.400 --> 00:59:38.720]   And then I realized that human perception had,
[00:59:38.720 --> 00:59:41.080]   there was a lot in terms of how we perceive the world,
[00:59:41.080 --> 00:59:41.960]   and so trying to figure out
[00:59:41.960 --> 00:59:44.440]   how do I model human perception for my,
[00:59:44.440 --> 00:59:47.600]   and so I became a HRI person,
[00:59:47.600 --> 00:59:49.360]   human-robot interaction person,
[00:59:49.360 --> 00:59:51.200]   from being a control theory
[00:59:51.200 --> 00:59:54.340]   and realizing that humans actually offered quite a bit.
[00:59:54.340 --> 00:59:56.080]   And then when you do that,
[00:59:56.080 --> 00:59:59.320]   you become more of an artificial intelligence, AI,
[00:59:59.320 --> 01:00:04.320]   and so I see myself evolving more in this AI world
[01:00:04.320 --> 01:00:09.600]   under the lens of robotics,
[01:00:09.600 --> 01:00:12.140]   having hardware, interacting with people.
[01:00:12.140 --> 01:00:17.140]   - So you're a world-class expert researcher in robotics,
[01:00:17.880 --> 01:00:21.160]   and yet others, there's a few,
[01:00:21.160 --> 01:00:24.200]   it's a small but fierce community of people,
[01:00:24.200 --> 01:00:26.640]   but most of them don't take the journey
[01:00:26.640 --> 01:00:29.440]   into the H of HRI, into the human.
[01:00:29.440 --> 01:00:34.440]   So why did you brave into the interaction with humans?
[01:00:34.440 --> 01:00:36.880]   It seems like a really hard problem.
[01:00:36.880 --> 01:00:39.880]   - It's a hard problem, and it's very risky as an academic.
[01:00:39.880 --> 01:00:41.120]   - Yes.
[01:00:41.120 --> 01:00:45.340]   - And I knew that when I started down that journey,
[01:00:46.240 --> 01:00:49.960]   that it was very risky as an academic
[01:00:49.960 --> 01:00:53.520]   in this world that was nuanced, it was just developing.
[01:00:53.520 --> 01:00:56.800]   We didn't even have a conference, right, at the time.
[01:00:56.800 --> 01:01:00.160]   Because it was the interesting problems.
[01:01:00.160 --> 01:01:01.600]   That was what drove me.
[01:01:01.600 --> 01:01:06.600]   It was the fact that I looked at what interests me
[01:01:06.600 --> 01:01:10.440]   in terms of the application space and the problems,
[01:01:10.440 --> 01:01:14.960]   and that pushed me into trying to figure out
[01:01:14.960 --> 01:01:16.840]   what people were and what humans were
[01:01:16.840 --> 01:01:18.180]   and how to adapt to them.
[01:01:18.180 --> 01:01:21.280]   If those problems weren't so interesting,
[01:01:21.280 --> 01:01:26.320]   I'd probably still be sending rovers to glaciers, right?
[01:01:26.320 --> 01:01:28.120]   But the problems were interesting.
[01:01:28.120 --> 01:01:30.640]   And the other thing was that they were hard, right?
[01:01:30.640 --> 01:01:35.640]   So I like having to go into a room and being like,
[01:01:35.640 --> 01:01:37.040]   "I don't know what to do."
[01:01:37.040 --> 01:01:38.020]   And then going back and saying,
[01:01:38.020 --> 01:01:39.820]   "Okay, I'm gonna figure this out."
[01:01:39.820 --> 01:01:42.280]   I'm not driven when I go in like,
[01:01:42.280 --> 01:01:44.080]   "Oh, there are no surprises."
[01:01:44.080 --> 01:01:47.360]   Like, I don't find that satisfying.
[01:01:47.360 --> 01:01:48.200]   If that was the case,
[01:01:48.200 --> 01:01:51.060]   I'd go someplace and make a lot more money, right?
[01:01:51.060 --> 01:01:55.020]   I think I stay an academic and choose to do this
[01:01:55.020 --> 01:01:58.320]   because I can go into a room and like, "That's hard."
[01:01:58.320 --> 01:02:01.760]   - Yeah, I think just from my perspective,
[01:02:01.760 --> 01:02:03.240]   maybe you can correct me on it,
[01:02:03.240 --> 01:02:06.760]   but if I just look at the field of AI broadly,
[01:02:06.760 --> 01:02:10.080]   it seems that human-robot interaction
[01:02:10.080 --> 01:02:15.080]   has one of the most number of open problems.
[01:02:15.080 --> 01:02:20.320]   People, especially relative to how many people
[01:02:20.320 --> 01:02:22.480]   are willing to acknowledge that there are.
[01:02:22.480 --> 01:02:26.160]   Because most people are just afraid of the humans,
[01:02:26.160 --> 01:02:27.240]   so they don't even acknowledge
[01:02:27.240 --> 01:02:28.200]   how many open problems there are.
[01:02:28.200 --> 01:02:30.880]   But in terms of difficult problems to solve,
[01:02:30.880 --> 01:02:35.800]   exciting spaces, it seems to be incredible for that.
[01:02:35.800 --> 01:02:38.720]   - It is, and it's exciting.
[01:02:38.720 --> 01:02:40.020]   - You've mentioned trust before.
[01:02:40.020 --> 01:02:43.320]   What role does trust,
[01:02:43.320 --> 01:02:46.840]   from interacting with autopilot
[01:02:46.840 --> 01:02:48.440]   to in the medical context,
[01:02:48.440 --> 01:02:49.720]   what role does trust play
[01:02:49.720 --> 01:02:51.320]   in the human-robot interaction space?
[01:02:51.320 --> 01:02:53.920]   - So some of the things I study in this domain
[01:02:53.920 --> 01:02:56.920]   is not just trust, but it really is overtrust.
[01:02:56.920 --> 01:02:58.120]   - How do you think about overtrust?
[01:02:58.120 --> 01:02:59.520]   Like, what is, first of all,
[01:02:59.520 --> 01:03:03.360]   what is trust and what is overtrust?
[01:03:03.360 --> 01:03:05.800]   - Basically, the way I look at it is
[01:03:05.800 --> 01:03:08.040]   trust is not what you click on a survey.
[01:03:08.040 --> 01:03:09.560]   Trust is about your behavior.
[01:03:09.560 --> 01:03:11.900]   So if you interact with the technology
[01:03:11.900 --> 01:03:17.280]   based on the decision or the actions of the technology,
[01:03:17.280 --> 01:03:19.680]   as if you trust that decision,
[01:03:19.680 --> 01:03:20.720]   then you're trusting.
[01:03:20.720 --> 01:03:26.620]   Even in my group, we've done surveys that,
[01:03:26.620 --> 01:03:28.260]   on the thing, do you trust robots?
[01:03:28.260 --> 01:03:29.100]   Of course not.
[01:03:29.100 --> 01:03:31.660]   Would you follow this robot in an abandoned building?
[01:03:31.660 --> 01:03:32.920]   Of course not.
[01:03:32.920 --> 01:03:34.440]   And then you look at their actions,
[01:03:34.440 --> 01:03:37.220]   and you're like, clearly your behavior
[01:03:37.220 --> 01:03:39.640]   does not match what you think, right?
[01:03:39.640 --> 01:03:41.980]   Or what you think you would like to think, right?
[01:03:41.980 --> 01:03:44.040]   And so I'm really concerned about the behavior,
[01:03:44.040 --> 01:03:45.800]   'cause that's really, at the end of the day,
[01:03:45.800 --> 01:03:47.340]   when you're in the world,
[01:03:47.340 --> 01:03:50.480]   that's what will impact others around you.
[01:03:50.480 --> 01:03:52.920]   It's not whether before you went onto the street,
[01:03:52.920 --> 01:03:55.680]   you clicked on, like, I don't trust self-driving cars.
[01:03:55.680 --> 01:03:58.680]   - Yeah, that, from an outsider perspective,
[01:03:58.680 --> 01:04:00.600]   it's always frustrating to me.
[01:04:00.600 --> 01:04:01.480]   Well, I read a lot,
[01:04:01.480 --> 01:04:04.160]   so I'm insider in a certain philosophical sense.
[01:04:06.040 --> 01:04:10.700]   It's frustrating to me how often trust is used in surveys,
[01:04:10.700 --> 01:04:14.420]   and how people say, make claims
[01:04:14.420 --> 01:04:16.220]   out of any kind of finding they make
[01:04:16.220 --> 01:04:18.720]   about somebody clicking on answer.
[01:04:18.720 --> 01:04:23.460]   Because trust is, yeah, behavior,
[01:04:23.460 --> 01:04:24.640]   just, you said it beautifully,
[01:04:24.640 --> 01:04:28.140]   I mean, the action, your own behavior is what trust is.
[01:04:28.140 --> 01:04:30.800]   I mean, everything else is not even close.
[01:04:30.800 --> 01:04:34.800]   It's almost like absurd, comedic poetry,
[01:04:35.640 --> 01:04:38.560]   poetry that you weave around your actual behavior.
[01:04:38.560 --> 01:04:42.120]   So some people can say they trust,
[01:04:42.120 --> 01:04:46.100]   I trust my wife, husband, or not, whatever,
[01:04:46.100 --> 01:04:48.280]   but the actions is what speaks volumes.
[01:04:48.280 --> 01:04:49.900]   - You bug their car.
[01:04:49.900 --> 01:04:51.080]   (laughing)
[01:04:51.080 --> 01:04:52.320]   You probably don't trust them.
[01:04:52.320 --> 01:04:53.840]   - I trust them, I'm just making sure.
[01:04:53.840 --> 01:04:55.640]   No, no, that's, yeah.
[01:04:55.640 --> 01:04:57.320]   - Like, even if you think about cars,
[01:04:57.320 --> 01:04:58.600]   I think it's a beautiful case.
[01:04:58.600 --> 01:05:01.280]   I came here at some point, I'm sure,
[01:05:01.280 --> 01:05:03.640]   on either Uber or Lyft, right?
[01:05:03.640 --> 01:05:05.840]   I remember when it first came out.
[01:05:05.840 --> 01:05:08.080]   I bet if they had had a survey,
[01:05:08.080 --> 01:05:11.480]   would you get in the car with a stranger and pay them?
[01:05:11.480 --> 01:05:12.720]   - Yes.
[01:05:12.720 --> 01:05:15.360]   - How many people do you think would have said,
[01:05:15.360 --> 01:05:16.720]   like, really?
[01:05:16.720 --> 01:05:18.760]   Wait, even worse, would you get in the car
[01:05:18.760 --> 01:05:22.000]   with a stranger at 1 a.m. in the morning
[01:05:22.000 --> 01:05:25.680]   to have them drop you home as a single female?
[01:05:25.680 --> 01:05:29.360]   Like, how many people would say, that's stupid?
[01:05:29.360 --> 01:05:30.200]   - Yeah.
[01:05:30.200 --> 01:05:31.640]   - And now look at where we are.
[01:05:31.640 --> 01:05:34.040]   I mean, people put kids, right?
[01:05:34.040 --> 01:05:37.760]   Like, oh yeah, my child has to go to school,
[01:05:37.760 --> 01:05:40.640]   and I, yeah, I'm gonna put my kid in this car
[01:05:40.640 --> 01:05:42.420]   with a stranger.
[01:05:42.420 --> 01:05:45.320]   I mean, it's just fascinating how,
[01:05:45.320 --> 01:05:47.160]   like, what we think we think
[01:05:47.160 --> 01:05:49.760]   is not necessarily matching our behavior.
[01:05:49.760 --> 01:05:52.400]   - Yeah, and certainly with robots, with autonomous vehicles,
[01:05:52.400 --> 01:05:54.760]   and all the kinds of robots you work with,
[01:05:54.760 --> 01:05:59.760]   that's, it's, yeah, it's, the way you answer it,
[01:06:00.440 --> 01:06:03.440]   especially if you've never interacted with that robot before.
[01:06:03.440 --> 01:06:05.720]   If you haven't had the experience,
[01:06:05.720 --> 01:06:09.640]   you being able to respond correctly on a survey is impossible.
[01:06:09.640 --> 01:06:13.440]   But what role does trust play in the interaction,
[01:06:13.440 --> 01:06:14.280]   do you think?
[01:06:14.280 --> 01:06:19.280]   Like, is it good to, is it good to trust a robot?
[01:06:19.280 --> 01:06:21.720]   What does overtrust mean?
[01:06:21.720 --> 01:06:24.080]   Or is it good to, kind of how you feel
[01:06:24.080 --> 01:06:26.560]   about autopilot currently, which is like,
[01:06:26.560 --> 01:06:29.440]   from a robotics perspective, is like--
[01:06:29.440 --> 01:06:30.280]   - So cautious.
[01:06:30.280 --> 01:06:31.640]   - So very cautious.
[01:06:31.640 --> 01:06:35.040]   - Yeah, so this is still an open area of research,
[01:06:35.040 --> 01:06:40.040]   but basically what I would like in a perfect world
[01:06:40.040 --> 01:06:45.000]   is that people trust the technology when it's working 100%,
[01:06:45.000 --> 01:06:49.160]   and people will be hypersensitive and identify when it's not.
[01:06:49.160 --> 01:06:51.080]   But of course we're not there.
[01:06:51.080 --> 01:06:52.800]   That's the ideal world.
[01:06:52.800 --> 01:06:56.600]   And, but we find is that people swing, right?
[01:06:56.600 --> 01:06:58.160]   They tend to swing,
[01:06:58.400 --> 01:07:01.400]   which means that if my first,
[01:07:01.400 --> 01:07:03.000]   and like, we have some papers,
[01:07:03.000 --> 01:07:05.360]   like first impressions is everything, right?
[01:07:05.360 --> 01:07:07.760]   If my first instance with technology,
[01:07:07.760 --> 01:07:12.760]   with robotics is positive, it mitigates any risk,
[01:07:12.760 --> 01:07:16.960]   it correlates with like best outcomes,
[01:07:16.960 --> 01:07:21.600]   it means that I'm more likely to either not see it
[01:07:21.600 --> 01:07:24.320]   when it makes some mistakes or faults,
[01:07:24.320 --> 01:07:27.400]   or I'm more likely to forgive it.
[01:07:28.240 --> 01:07:30.400]   And so this is a problem
[01:07:30.400 --> 01:07:32.680]   because technology is not 100% accurate, right?
[01:07:32.680 --> 01:07:35.120]   It's not 100% accurate, although it may be perfect.
[01:07:35.120 --> 01:07:37.720]   - How do you get that first moment right, do you think?
[01:07:37.720 --> 01:07:40.720]   There's also an education about the capabilities
[01:07:40.720 --> 01:07:42.520]   and limitations of the system.
[01:07:42.520 --> 01:07:45.760]   Do you have a sense of how you educate people correctly
[01:07:45.760 --> 01:07:47.160]   in that first interaction?
[01:07:47.160 --> 01:07:50.280]   - Again, this is an open-ended problem.
[01:07:50.280 --> 01:07:55.040]   So one of the study that actually has given me some hope
[01:07:55.040 --> 01:07:57.680]   that I was trying to figure out how to put in robotics.
[01:07:57.680 --> 01:08:01.320]   So there was a research study
[01:08:01.320 --> 01:08:03.480]   that has showed for medical AI systems,
[01:08:03.480 --> 01:08:07.880]   giving information to radiologists about,
[01:08:07.880 --> 01:08:12.880]   here, you need to look at these areas on the X-ray.
[01:08:12.880 --> 01:08:19.360]   What they found was that when the system provided one choice,
[01:08:20.600 --> 01:08:25.600]   there was this aspect of either no trust or overtrust, right?
[01:08:25.600 --> 01:08:32.000]   Like, I don't believe it at all, or a yes, yes, yes, yes.
[01:08:32.000 --> 01:08:36.440]   And they would miss things, right?
[01:08:36.440 --> 01:08:40.680]   Instead, when the system gave them multiple choices,
[01:08:40.680 --> 01:08:42.680]   like here are the three, even if it knew,
[01:08:42.680 --> 01:08:45.320]   like it had estimated that the top area
[01:08:45.320 --> 01:08:49.880]   that you need to look at was some place on the X-ray.
[01:08:49.880 --> 01:08:54.160]   If it gave like one plus others,
[01:08:54.160 --> 01:08:57.600]   the trust was maintained
[01:08:57.600 --> 01:09:02.600]   and the accuracy of the entire population increased, right?
[01:09:02.600 --> 01:09:07.600]   So basically it was a, you're still trusting the system,
[01:09:07.600 --> 01:09:09.280]   but you're also putting in a little bit
[01:09:09.280 --> 01:09:11.600]   of like your human expertise,
[01:09:11.600 --> 01:09:15.640]   like your human decision processing into the equation.
[01:09:15.640 --> 01:09:18.640]   So it helps to mitigate that overtrust risk.
[01:09:18.640 --> 01:09:21.640]   - Yeah, so there's a fascinating balance to have to strike.
[01:09:21.640 --> 01:09:22.720]   - I haven't figured out, again,
[01:09:22.720 --> 01:09:24.680]   in robotics, it's still an open research.
[01:09:24.680 --> 01:09:26.760]   - Open area research, exactly.
[01:09:26.760 --> 01:09:29.000]   So what are some exciting applications
[01:09:29.000 --> 01:09:30.200]   of human-robot interaction?
[01:09:30.200 --> 01:09:31.920]   You started a company, maybe you can talk
[01:09:31.920 --> 01:09:35.720]   about the exciting efforts there,
[01:09:35.720 --> 01:09:39.600]   but in general also, what other space can robots interact
[01:09:39.600 --> 01:09:41.080]   with humans and help?
[01:09:41.080 --> 01:09:42.400]   - Yeah, so besides healthcare,
[01:09:42.400 --> 01:09:44.560]   'cause that's my bias lens,
[01:09:44.560 --> 01:09:47.120]   my other bias lens is education.
[01:09:47.120 --> 01:09:52.040]   I think that, well, one, we definitely,
[01:09:52.040 --> 01:09:54.800]   in the US, we're doing okay with teachers,
[01:09:54.800 --> 01:09:56.920]   but there's a lot of school districts
[01:09:56.920 --> 01:09:58.320]   that don't have enough teachers.
[01:09:58.320 --> 01:10:01.960]   If you think about the teacher-student ratio
[01:10:01.960 --> 01:10:04.840]   for at least public education,
[01:10:04.840 --> 01:10:06.720]   in some districts, it's crazy.
[01:10:06.720 --> 01:10:10.400]   It's like, how can you have learning in that classroom?
[01:10:10.400 --> 01:10:13.000]   Because you just don't have the human capital.
[01:10:13.000 --> 01:10:15.520]   And so if you think about robotics,
[01:10:15.520 --> 01:10:18.480]   bringing that in to classrooms,
[01:10:18.480 --> 01:10:20.360]   as well as the after-school space,
[01:10:20.360 --> 01:10:25.120]   where they offset some of this lack of resources
[01:10:25.120 --> 01:10:28.720]   in certain communities, I think that's a good place.
[01:10:28.720 --> 01:10:30.920]   And then turning, on the other end,
[01:10:30.920 --> 01:10:35.280]   is using these systems then for workforce retraining
[01:10:35.280 --> 01:10:38.960]   and dealing with some of the things
[01:10:38.960 --> 01:10:43.040]   that are going to come out later on of job loss,
[01:10:43.040 --> 01:10:45.880]   like thinking about robots and in AI systems
[01:10:45.880 --> 01:10:48.320]   for retraining and workforce development.
[01:10:48.320 --> 01:10:53.200]   I think that's exciting areas that can be pushed even more,
[01:10:53.200 --> 01:10:56.760]   and it would have a huge, huge impact.
[01:10:56.760 --> 01:10:59.640]   - What would you say are some of the open problems
[01:10:59.640 --> 01:11:03.200]   in education, sort of, it's exciting.
[01:11:03.200 --> 01:11:08.200]   So young kids and the older folks
[01:11:08.720 --> 01:11:12.560]   or just folks of all ages who need to be retrained,
[01:11:12.560 --> 01:11:14.240]   who need to sort of open themselves up
[01:11:14.240 --> 01:11:17.680]   to a whole 'nother area of work.
[01:11:17.680 --> 01:11:20.040]   What are the problems to be solved there?
[01:11:20.040 --> 01:11:22.400]   How do you think robots can help?
[01:11:22.400 --> 01:11:24.800]   - We have the engagement aspect, right?
[01:11:24.800 --> 01:11:26.440]   So we can figure out the engagement.
[01:11:26.440 --> 01:11:27.480]   That's not a-- - What do you mean
[01:11:27.480 --> 01:11:28.880]   by engagement?
[01:11:28.880 --> 01:11:33.880]   - So identifying whether a person is focused is,
[01:11:33.880 --> 01:11:37.840]   like, that we can figure out.
[01:11:38.840 --> 01:11:40.800]   What we can figure out,
[01:11:40.800 --> 01:11:44.640]   and there's some positive results in this,
[01:11:44.640 --> 01:11:49.640]   is that personalized adaptation based on any concepts.
[01:11:49.640 --> 01:11:54.680]   So imagine I think about, I have an agent,
[01:11:54.680 --> 01:11:59.680]   and I'm working with a kid learning, I don't know,
[01:11:59.680 --> 01:12:01.600]   algebra two.
[01:12:01.600 --> 01:12:05.880]   Can that same agent then switch and teach
[01:12:05.880 --> 01:12:10.880]   some type of new coding skill to a displaced mechanic?
[01:12:10.880 --> 01:12:14.520]   Like, what does that actually look like, right?
[01:12:14.520 --> 01:12:17.680]   Like, hardware might be the same,
[01:12:17.680 --> 01:12:21.360]   content is different, two different target demographics
[01:12:21.360 --> 01:12:24.640]   of engagement, like, how do you do that?
[01:12:24.640 --> 01:12:26.880]   - How important do you think personalization
[01:12:26.880 --> 01:12:28.640]   is in human-robot interaction?
[01:12:28.640 --> 01:12:32.040]   And not just mechanic or student,
[01:12:32.040 --> 01:12:35.400]   but like literally to the individual human being?
[01:12:35.400 --> 01:12:37.600]   - I think personalization is really important,
[01:12:37.600 --> 01:12:42.160]   but a caveat is that I think we'd be okay
[01:12:42.160 --> 01:12:44.720]   if we can personalize to the group, right?
[01:12:44.720 --> 01:12:49.720]   And so if I can label you as along some certain dimensions,
[01:12:49.720 --> 01:12:56.520]   then even though it may not be you specifically,
[01:12:56.520 --> 01:12:58.240]   I can put you in this group.
[01:12:58.240 --> 01:13:00.520]   So the sample size, this is how they best learn,
[01:13:00.520 --> 01:13:02.080]   this is how they best engage.
[01:13:03.240 --> 01:13:06.840]   Even at that level, it's really important.
[01:13:06.840 --> 01:13:09.680]   And it's because, I mean, it's one of the reasons
[01:13:09.680 --> 01:13:13.400]   why educating in large classrooms is so hard, right?
[01:13:13.400 --> 01:13:18.200]   You teach to the median, but there's these individuals
[01:13:18.200 --> 01:13:20.480]   that are struggling, and then you have
[01:13:20.480 --> 01:13:22.400]   highly intelligent individuals,
[01:13:22.400 --> 01:13:26.400]   and those are the ones that are usually kind of left out.
[01:13:26.400 --> 01:13:28.960]   So highly intelligent individuals may be disruptive,
[01:13:28.960 --> 01:13:30.920]   and those who are struggling might be disruptive
[01:13:30.920 --> 01:13:33.040]   because they're both bored.
[01:13:33.040 --> 01:13:35.560]   - Yeah, and if you narrow the definition of the group
[01:13:35.560 --> 01:13:37.560]   or in the size of the group enough,
[01:13:37.560 --> 01:13:40.400]   you'll be able to address their individual,
[01:13:40.400 --> 01:13:42.680]   it's not individual needs, but really the most--
[01:13:42.680 --> 01:13:43.960]   - Group needs. - Group,
[01:13:43.960 --> 01:13:45.720]   most important group needs.
[01:13:45.720 --> 01:13:47.320]   Right, and that's kind of what a lot
[01:13:47.320 --> 01:13:51.000]   of successful recommender systems do, Spotify and so on.
[01:13:51.000 --> 01:13:53.840]   So it's sad to believe, but I'm, as a music listener,
[01:13:53.840 --> 01:13:55.880]   probably in some sort of large group.
[01:13:55.880 --> 01:13:56.920]   (laughing)
[01:13:56.920 --> 01:13:58.120]   It's very sadly predictable.
[01:13:58.120 --> 01:13:59.280]   - You have been labeled.
[01:13:59.280 --> 01:14:02.120]   - Yeah, I've been labeled, and successfully so,
[01:14:02.120 --> 01:14:04.640]   because they're able to recommend stuff that I--
[01:14:04.640 --> 01:14:07.760]   - Yeah, but applying that to education, right?
[01:14:07.760 --> 01:14:09.800]   There's no reason why it can't be done.
[01:14:09.800 --> 01:14:13.120]   - Do you have a hope for our education system?
[01:14:13.120 --> 01:14:16.240]   - I have more hope for workforce development,
[01:14:16.240 --> 01:14:19.720]   and that's because I'm seeing investments.
[01:14:19.720 --> 01:14:23.320]   Even if you look at VC investments in education,
[01:14:23.320 --> 01:14:26.200]   the majority of it has lately been going
[01:14:26.200 --> 01:14:28.600]   to workforce retraining, right?
[01:14:28.600 --> 01:14:32.960]   And so I think that government investments is increasing.
[01:14:32.960 --> 01:14:36.160]   There's like a claim, and some of it's based on fear, right?
[01:14:36.160 --> 01:14:38.080]   Like AI's gonna come and take over all these jobs.
[01:14:38.080 --> 01:14:41.560]   What are we gonna do with all these non-paying taxes
[01:14:41.560 --> 01:14:44.400]   that aren't coming to us by our citizens?
[01:14:44.400 --> 01:14:47.240]   And so I think I'm more hopeful for that.
[01:14:47.240 --> 01:14:51.840]   Not so hopeful for early education,
[01:14:51.840 --> 01:14:56.440]   because it's this, it's still a who's gonna pay for it,
[01:14:56.440 --> 01:15:01.440]   and you won't see the results for like 16 to 18 years.
[01:15:01.440 --> 01:15:06.040]   It's hard for people to wrap their heads around that.
[01:15:06.040 --> 01:15:10.680]   - But on the retraining part, what are your thoughts?
[01:15:10.680 --> 01:15:14.880]   There's a candidate, Andrew Yang, running for president,
[01:15:14.880 --> 01:15:19.040]   saying that sort of AI automation robots--
[01:15:19.040 --> 01:15:21.080]   - Universal basic income.
[01:15:21.080 --> 01:15:24.000]   - Universal basic income in order to support us
[01:15:24.000 --> 01:15:26.760]   as we kind of automation takes people's jobs
[01:15:26.760 --> 01:15:30.200]   and allows you to explore and find other means.
[01:15:30.200 --> 01:15:35.200]   Like, do you have a concern of society transforming effects
[01:15:35.200 --> 01:15:40.520]   of automation and robots and so on?
[01:15:40.520 --> 01:15:41.360]   - I do.
[01:15:41.360 --> 01:15:46.200]   I do know that AI robotics will displace workers.
[01:15:46.200 --> 01:15:48.000]   Like, we do know that.
[01:15:48.000 --> 01:15:49.520]   But there'll be other workers
[01:15:49.520 --> 01:15:54.520]   that will be defined new jobs.
[01:15:54.520 --> 01:15:57.480]   What I worry about is, that's not what I worry about,
[01:15:57.480 --> 01:15:59.520]   like, will all the jobs go away?
[01:15:59.520 --> 01:16:02.240]   What I worry about is the type of jobs that will come out.
[01:16:02.240 --> 01:16:05.040]   Right, like people who graduate from Georgia Tech
[01:16:05.040 --> 01:16:06.400]   will be okay, right?
[01:16:06.400 --> 01:16:08.560]   We give them the skills, they will adapt
[01:16:08.560 --> 01:16:10.720]   even if their current job goes away.
[01:16:10.720 --> 01:16:13.640]   I do worry about those that don't have
[01:16:13.640 --> 01:16:15.480]   that quality of an education, right?
[01:16:15.480 --> 01:16:19.560]   Will they have the ability, the background
[01:16:19.560 --> 01:16:21.760]   to adapt to those new jobs?
[01:16:21.760 --> 01:16:23.080]   That, I don't know.
[01:16:23.080 --> 01:16:25.720]   That I worry about, which will create
[01:16:25.720 --> 01:16:29.600]   even more polarization in our society,
[01:16:29.600 --> 01:16:31.320]   internationally, and everywhere.
[01:16:31.320 --> 01:16:33.000]   I worry about that.
[01:16:33.000 --> 01:16:36.920]   I also worry about not having equal access
[01:16:36.920 --> 01:16:39.640]   to all these wonderful things that AI can do
[01:16:39.640 --> 01:16:41.160]   and robotics can do.
[01:16:41.160 --> 01:16:43.160]   I worry about that.
[01:16:43.160 --> 01:16:47.320]   You know, people like me from Georgia Tech,
[01:16:47.320 --> 01:16:50.400]   from say, MIT, will be okay, right?
[01:16:50.400 --> 01:16:53.400]   But that's such a small part of the population
[01:16:53.400 --> 01:16:56.000]   that we need to think much more globally
[01:16:56.000 --> 01:16:58.560]   of having access to the beautiful things,
[01:16:58.560 --> 01:17:01.600]   whether it's AI in healthcare, AI in education,
[01:17:01.600 --> 01:17:05.200]   AI in politics, right?
[01:17:05.200 --> 01:17:06.040]   I worry about that.
[01:17:06.040 --> 01:17:08.200]   - And that's part of the thing that you were talking about
[01:17:08.200 --> 01:17:09.680]   is people that build the technology
[01:17:09.680 --> 01:17:12.480]   have to be thinking about ethics,
[01:17:12.480 --> 01:17:15.240]   have to be thinking about access and all those things,
[01:17:15.240 --> 01:17:17.920]   and not just a small subset.
[01:17:17.920 --> 01:17:22.160]   Let me ask some philosophical, slightly romantic questions.
[01:17:22.160 --> 01:17:24.560]   - All right. - People that listen to this
[01:17:24.560 --> 01:17:26.320]   will be like, here he goes again.
[01:17:26.320 --> 01:17:31.320]   Okay, do you think one day we'll build an AI system
[01:17:31.320 --> 01:17:35.640]   that a person can fall in love with
[01:17:35.640 --> 01:17:38.040]   and it would love them back?
[01:17:38.040 --> 01:17:39.960]   Like in the movie "Her," for example.
[01:17:39.960 --> 01:17:43.400]   - Yeah, although she kind of didn't fall in love with him,
[01:17:43.400 --> 01:17:45.680]   or she fell in love with like a million other people,
[01:17:45.680 --> 01:17:46.840]   something like that.
[01:17:46.840 --> 01:17:48.600]   - You're the jealous type, I see.
[01:17:48.600 --> 01:17:49.880]   (laughing)
[01:17:49.880 --> 01:17:50.960]   We humans are the jealous type.
[01:17:50.960 --> 01:17:55.120]   - Yes, so I do believe that we can design systems
[01:17:55.120 --> 01:17:59.480]   where people would fall in love with their robot,
[01:17:59.480 --> 01:18:03.280]   with their AI partner.
[01:18:03.280 --> 01:18:05.160]   That I do believe.
[01:18:05.160 --> 01:18:06.360]   Because it's actually,
[01:18:06.360 --> 01:18:09.000]   and I don't like to use the word manipulate,
[01:18:09.000 --> 01:18:12.360]   but as we see, there are certain individuals
[01:18:12.360 --> 01:18:13.400]   that can be manipulated
[01:18:13.400 --> 01:18:16.360]   if you understand the cognitive science about it, right?
[01:18:16.360 --> 01:18:18.560]   - Right, so I mean, if you could think
[01:18:18.560 --> 01:18:21.440]   of all close relationship and love in general
[01:18:21.440 --> 01:18:24.800]   as a kind of mutual manipulation,
[01:18:24.800 --> 01:18:27.200]   that dance, the human dance.
[01:18:27.200 --> 01:18:29.560]   I mean, manipulation is a negative connotation.
[01:18:29.560 --> 01:18:32.880]   - And that's why I don't like to use that word particularly.
[01:18:32.880 --> 01:18:34.320]   - I guess another way to phrase it is,
[01:18:34.320 --> 01:18:36.000]   you're getting at it as it could be
[01:18:36.000 --> 01:18:37.440]   algorithmatized or something.
[01:18:37.440 --> 01:18:38.480]   It could be--
[01:18:38.480 --> 01:18:40.720]   - The relationship building part can be.
[01:18:40.720 --> 01:18:41.920]   I mean, just think about it.
[01:18:41.920 --> 01:18:44.920]   We have, and I don't use dating sites,
[01:18:44.920 --> 01:18:46.400]   but from what I heard,
[01:18:46.400 --> 01:18:50.480]   there are some individuals that have been dating
[01:18:50.480 --> 01:18:52.920]   that have never saw each other, right?
[01:18:52.920 --> 01:18:54.200]   In fact, there's a show I think
[01:18:54.200 --> 01:18:57.640]   that tries to weed out fake people.
[01:18:57.640 --> 01:18:59.560]   Like there's a show that comes out, right?
[01:18:59.560 --> 01:19:02.080]   Because people start faking.
[01:19:02.080 --> 01:19:05.240]   Like, what's the difference of that person
[01:19:05.240 --> 01:19:08.160]   on the other end being an AI agent, right?
[01:19:08.160 --> 01:19:09.440]   And having a communication,
[01:19:09.440 --> 01:19:12.280]   are you building a relationship remotely?
[01:19:12.280 --> 01:19:14.920]   Like there's no reason why that can't happen.
[01:19:14.920 --> 01:19:17.680]   - In terms of human-robot interaction,
[01:19:17.680 --> 01:19:20.640]   so what role, you've kind of mentioned with data,
[01:19:20.640 --> 01:19:25.440]   emotion being, can be problematic if not implemented well,
[01:19:25.440 --> 01:19:26.320]   I suppose.
[01:19:26.320 --> 01:19:30.600]   What role does emotion and some other human-like things,
[01:19:30.600 --> 01:19:32.880]   the imperfect things come into play here
[01:19:32.880 --> 01:19:37.400]   for good human-robot interaction and something like love?
[01:19:37.400 --> 01:19:39.880]   - Yeah, so in this case, and you had asked,
[01:19:39.880 --> 01:19:43.800]   can an AI agent love a human back?
[01:19:43.800 --> 01:19:47.440]   I think they can emulate love back, right?
[01:19:47.440 --> 01:19:49.040]   And so what does that actually mean?
[01:19:49.040 --> 01:19:52.320]   It just means that if you think about their programming,
[01:19:52.320 --> 01:19:56.280]   they might put the other person's needs in front of theirs
[01:19:56.280 --> 01:19:58.040]   in certain situations, right?
[01:19:58.040 --> 01:20:00.440]   You look at, think about it as return on investment.
[01:20:00.440 --> 01:20:01.800]   Like, what's my return on investment?
[01:20:01.800 --> 01:20:04.600]   As part of that equation, that person's happiness,
[01:20:04.600 --> 01:20:08.000]   has some type of algorithm waiting to it.
[01:20:08.000 --> 01:20:11.440]   And the reason why is because I care about them, right?
[01:20:11.440 --> 01:20:13.760]   That's the only reason, right?
[01:20:13.760 --> 01:20:15.600]   But if I care about them and I show that,
[01:20:15.600 --> 01:20:18.320]   then my final objective function
[01:20:18.320 --> 01:20:20.600]   is length of time of the engagement, right?
[01:20:20.600 --> 01:20:24.040]   So you can think of how to do this actually quite easily.
[01:20:24.040 --> 01:20:24.880]   And so--
[01:20:24.880 --> 01:20:26.520]   - But that's not love?
[01:20:26.520 --> 01:20:28.880]   - Well, so that's the thing.
[01:20:30.000 --> 01:20:32.600]   I think it emulates love
[01:20:32.600 --> 01:20:37.600]   because we don't have a classical definition of love.
[01:20:37.600 --> 01:20:41.640]   - Right, but, and we don't have the ability
[01:20:41.640 --> 01:20:45.480]   to look into each other's minds to see the algorithm.
[01:20:45.480 --> 01:20:48.760]   And I mean, I guess what I'm getting at is,
[01:20:48.760 --> 01:20:51.040]   is it possible that, especially if that's learned,
[01:20:51.040 --> 01:20:52.600]   especially if there's some mystery
[01:20:52.600 --> 01:20:55.240]   and black box nature to the system,
[01:20:55.240 --> 01:20:57.680]   how is that, you know--
[01:20:57.680 --> 01:20:58.600]   - How is it any different?
[01:20:58.600 --> 01:21:00.680]   How is it any different in terms of sort of,
[01:21:00.680 --> 01:21:04.160]   if the system says, "I'm conscious, I'm afraid of death,"
[01:21:04.160 --> 01:21:10.160]   and it does indicate that it loves you,
[01:21:10.160 --> 01:21:12.520]   another way to sort of phrase it,
[01:21:12.520 --> 01:21:14.200]   I'd be curious to see what you think.
[01:21:14.200 --> 01:21:15.720]   Do you think there'll be a time
[01:21:15.720 --> 01:21:20.160]   when robots should have rights?
[01:21:20.160 --> 01:21:23.440]   You've kind of phrased the robot in a very roboticist way,
[01:21:23.440 --> 01:21:25.680]   and just a really good way, but saying,
[01:21:25.680 --> 01:21:27.920]   "Okay, well, there's an objective function,
[01:21:27.920 --> 01:21:30.600]   "and I could see how you can create
[01:21:30.600 --> 01:21:33.360]   "a compelling human-robot interaction experience
[01:21:33.360 --> 01:21:36.280]   "that makes you believe that the robot cares for your needs,
[01:21:36.280 --> 01:21:39.040]   "and even something like loves you."
[01:21:39.040 --> 01:21:43.840]   But what if the robot says, "Please don't turn me off"?
[01:21:43.840 --> 01:21:46.560]   What if the robot starts making you feel
[01:21:46.560 --> 01:21:50.160]   like there's an entity, a being, a soul there, right?
[01:21:50.160 --> 01:21:52.200]   Do you think there'll be a future,
[01:21:52.200 --> 01:21:55.800]   hopefully you won't laugh too much at this,
[01:21:55.800 --> 01:22:00.120]   but where they do ask for rights?
[01:22:00.120 --> 01:22:05.120]   - So I can see a future if we don't address it
[01:22:05.120 --> 01:22:10.400]   in the near term, where these agents,
[01:22:10.400 --> 01:22:12.360]   as they adapt and learn, could say,
[01:22:12.360 --> 01:22:15.920]   "Hey, this should be something that's fundamental."
[01:22:15.920 --> 01:22:18.960]   I hopefully think that we would address it
[01:22:18.960 --> 01:22:20.200]   before it gets to that point.
[01:22:20.200 --> 01:22:22.280]   - You think that's a bad future?
[01:22:22.280 --> 01:22:25.440]   Is that a negative thing, where they ask,
[01:22:25.440 --> 01:22:27.840]   "We're being discriminated against"?
[01:22:27.840 --> 01:22:31.200]   - I guess it depends on what role
[01:22:31.200 --> 01:22:34.440]   have they attained at that point, right?
[01:22:34.440 --> 01:22:35.960]   And so if I think about now--
[01:22:35.960 --> 01:22:39.320]   - Careful what you say, because the robots 50 years from now
[01:22:39.320 --> 01:22:42.200]   will be listening to this, and you'll be on TV saying,
[01:22:42.200 --> 01:22:44.520]   "This is what roboticists used to believe."
[01:22:44.520 --> 01:22:45.360]   - Well, right?
[01:22:45.360 --> 01:22:48.800]   And so this is my, and as I said, I have a biased lens,
[01:22:48.800 --> 01:22:50.920]   and my robot friends will understand that.
[01:22:50.920 --> 01:22:54.000]   But so if you think about it,
[01:22:54.000 --> 01:22:59.000]   and I actually put this in kind of the, as a roboticist,
[01:22:59.000 --> 01:23:02.600]   you don't necessarily think of robots as human,
[01:23:02.600 --> 01:23:05.120]   with human rights, but you could think of them
[01:23:05.120 --> 01:23:09.280]   either in the category of property,
[01:23:09.280 --> 01:23:12.960]   or you could think of them in the category of animals.
[01:23:12.960 --> 01:23:18.440]   And so both of those have different types of rights.
[01:23:18.440 --> 01:23:22.840]   So animals have their own rights as a living being,
[01:23:22.840 --> 01:23:25.160]   but they can't vote, they can't write,
[01:23:25.160 --> 01:23:28.240]   they can be euthanized.
[01:23:28.240 --> 01:23:32.080]   But as humans, if we abuse them, we go to jail.
[01:23:32.080 --> 01:23:36.120]   So they do have some rights that protect them,
[01:23:36.120 --> 01:23:39.320]   but don't give them the rights of citizenship.
[01:23:39.320 --> 01:23:42.400]   And then if you think about property,
[01:23:42.400 --> 01:23:45.840]   property, the rights are associated with the person.
[01:23:45.840 --> 01:23:49.640]   So if someone vandalizes your property,
[01:23:49.640 --> 01:23:53.960]   or steals your property, there are some rights,
[01:23:53.960 --> 01:23:57.820]   but it's associated with the person who owns that.
[01:23:57.820 --> 01:24:01.640]   If you think about it, back in the day,
[01:24:01.640 --> 01:24:03.480]   and if you remember, we talked about
[01:24:03.480 --> 01:24:08.280]   how society has changed, women were property, right?
[01:24:08.280 --> 01:24:11.960]   They were not thought of as having rights.
[01:24:11.960 --> 01:24:15.880]   They were thought of as property of, like their--
[01:24:15.880 --> 01:24:18.880]   - Yeah, assaulting a woman meant assaulting the property
[01:24:18.880 --> 01:24:20.160]   of somebody else's person.
[01:24:20.160 --> 01:24:22.960]   - Exactly, and so what I envision is,
[01:24:22.960 --> 01:24:27.880]   is that we will establish some type of norm at some point,
[01:24:27.880 --> 01:24:29.640]   but that it might evolve, right?
[01:24:29.640 --> 01:24:32.000]   Like if you look at women's rights now,
[01:24:32.000 --> 01:24:35.840]   like there are still some countries that don't have,
[01:24:35.840 --> 01:24:37.160]   and the rest of the world is like,
[01:24:37.160 --> 01:24:39.760]   why that makes no sense, right?
[01:24:39.760 --> 01:24:42.560]   And so I do see a world where we do establish
[01:24:42.560 --> 01:24:44.600]   some type of grounding.
[01:24:44.600 --> 01:24:46.200]   It might be based on property rights,
[01:24:46.200 --> 01:24:48.080]   it might be based on animal rights.
[01:24:48.080 --> 01:24:51.200]   And if it evolves that way,
[01:24:51.200 --> 01:24:54.960]   I think we will have this conversation at that time,
[01:24:54.960 --> 01:24:56.560]   because that's the way our society
[01:24:56.560 --> 01:24:59.040]   traditionally has evolved.
[01:24:59.040 --> 01:25:01.960]   - Beautifully put.
[01:25:01.960 --> 01:25:06.960]   Just out of curiosity, Anki, Jibo, Mayfield Robotics,
[01:25:06.960 --> 01:25:10.240]   with their robot Curie, Sci-Fi Works, Rethink Robotics,
[01:25:10.240 --> 01:25:12.480]   were all these amazing robotics companies
[01:25:12.480 --> 01:25:16.240]   led, created by incredible roboticists,
[01:25:16.240 --> 01:25:21.240]   and they've all went out of business recently.
[01:25:21.240 --> 01:25:23.520]   Why do you think they didn't last longer?
[01:25:23.520 --> 01:25:26.960]   Why is it so hard to run a robotics company,
[01:25:26.960 --> 01:25:29.800]   especially one like these,
[01:25:29.800 --> 01:25:33.440]   which are fundamentally HRI,
[01:25:33.440 --> 01:25:36.000]   Human Robot Interaction Robots?
[01:25:36.000 --> 01:25:37.360]   - Yeah. - Or personal robots.
[01:25:37.360 --> 01:25:38.960]   - Each one has a story.
[01:25:38.960 --> 01:25:43.040]   Only one of them I don't understand, and that was Anki.
[01:25:43.040 --> 01:25:45.240]   That's actually the only one I don't understand.
[01:25:45.240 --> 01:25:46.320]   - I don't understand it either.
[01:25:46.320 --> 01:25:47.160]   It's-- - No, no, I mean,
[01:25:47.160 --> 01:25:49.000]   I look at it from the outside.
[01:25:49.000 --> 01:25:50.920]   I've looked at their sheets.
[01:25:50.920 --> 01:25:52.280]   I've looked at the data that's--
[01:25:52.280 --> 01:25:53.560]   - Oh, you mean business-wise,
[01:25:53.560 --> 01:25:55.240]   you don't understand, gotcha. - Yeah, yeah.
[01:25:55.240 --> 01:25:59.200]   And I look at that data,
[01:25:59.200 --> 01:26:02.720]   and I'm like, they seem to have product-market fit.
[01:26:02.720 --> 01:26:05.720]   So that's the only one I don't understand.
[01:26:05.720 --> 01:26:08.280]   The rest of it was product-market fit.
[01:26:08.280 --> 01:26:09.880]   - What's product-market fit,
[01:26:09.880 --> 01:26:11.960]   just out of, how do you think about it?
[01:26:11.960 --> 01:26:15.680]   - Yeah, so although we think robotics was getting there,
[01:26:15.680 --> 01:26:17.640]   but I think it's just the timing,
[01:26:17.640 --> 01:26:20.400]   their clock just timed out.
[01:26:20.400 --> 01:26:23.160]   I think if they'd been given a couple of more years,
[01:26:23.160 --> 01:26:25.120]   they would've been okay.
[01:26:25.120 --> 01:26:28.680]   But the other ones were still fairly early
[01:26:28.680 --> 01:26:30.160]   by the time they got into the market.
[01:26:30.160 --> 01:26:32.760]   And so product-market fit is,
[01:26:32.760 --> 01:26:37.200]   I have a product that I wanna sell at a certain price.
[01:26:37.200 --> 01:26:40.080]   Are there enough people out there, the market,
[01:26:40.080 --> 01:26:42.800]   that are willing to buy the product at that market price
[01:26:42.800 --> 01:26:46.000]   for me to be a functional, viable,
[01:26:46.000 --> 01:26:48.920]   profit-bearing company, right?
[01:26:48.920 --> 01:26:50.440]   So product-market fit.
[01:26:50.440 --> 01:26:55.360]   If it costs you $1,000, and everyone wants it,
[01:26:55.360 --> 01:26:57.400]   and only is willing to pay a dollar,
[01:26:57.400 --> 01:26:59.320]   you have no product-market fit,
[01:26:59.320 --> 01:27:01.960]   even if you could sell it for, you know,
[01:27:01.960 --> 01:27:03.720]   it's enough for a dollar, 'cause you can't--
[01:27:03.720 --> 01:27:05.440]   - So how hard is it for robots?
[01:27:05.440 --> 01:27:07.640]   So if maybe, if you look at iRobot,
[01:27:07.640 --> 01:27:10.840]   the company that makes Roombas, vacuum cleaners,
[01:27:10.840 --> 01:27:14.200]   can you comment on, did they find the right product,
[01:27:14.200 --> 01:27:15.200]   market-product fit?
[01:27:15.200 --> 01:27:18.680]   Like, are people willing to pay for robots
[01:27:18.680 --> 01:27:20.360]   is also another kind of question underlying all this.
[01:27:20.360 --> 01:27:23.800]   - So if you think about iRobot and their story, right?
[01:27:23.800 --> 01:27:28.760]   Like when they first, they had enough of a runway, right?
[01:27:28.760 --> 01:27:29.880]   When they first started,
[01:27:29.880 --> 01:27:31.440]   they weren't doing vacuum cleaners, right?
[01:27:31.440 --> 01:27:35.320]   They were a military, they were contracts, primarily,
[01:27:35.320 --> 01:27:37.840]   government contracts, designing robots.
[01:27:37.840 --> 01:27:38.680]   - Military robots.
[01:27:38.680 --> 01:27:39.520]   - Yeah, I mean, that's what they were.
[01:27:39.520 --> 01:27:40.960]   That's how they started, right?
[01:27:40.960 --> 01:27:41.800]   And then-- - They still do
[01:27:41.800 --> 01:27:42.880]   a lot of incredible work there.
[01:27:42.880 --> 01:27:44.800]   But yeah, that was the initial thing
[01:27:44.800 --> 01:27:46.760]   that gave them enough funding to--
[01:27:46.760 --> 01:27:50.880]   - To then try to, the vacuum cleaner is what I've been told
[01:27:50.880 --> 01:27:54.080]   was not like their first rendezvous
[01:27:54.080 --> 01:27:56.640]   in terms of designing a product, right?
[01:27:56.640 --> 01:27:59.440]   And so they were able to survive
[01:27:59.440 --> 01:28:01.800]   until they got to the point that they found
[01:28:02.360 --> 01:28:05.640]   a product price market, right?
[01:28:05.640 --> 01:28:09.200]   And even with, if you look at the Roomba,
[01:28:09.200 --> 01:28:10.640]   the price point now is different
[01:28:10.640 --> 01:28:12.360]   than when it was first released, right?
[01:28:12.360 --> 01:28:13.560]   It was an early adopter price,
[01:28:13.560 --> 01:28:16.760]   but they found enough people who were willing to fund it.
[01:28:16.760 --> 01:28:20.400]   And I mean, I forgot what their loss profile was
[01:28:20.400 --> 01:28:22.240]   for the first couple of years,
[01:28:22.240 --> 01:28:25.920]   but they became profitable in sufficient time
[01:28:25.920 --> 01:28:28.200]   that they didn't have to close their doors.
[01:28:28.200 --> 01:28:29.240]   - So they found the right,
[01:28:29.240 --> 01:28:32.760]   there's still people willing to pay a large amount of money,
[01:28:32.760 --> 01:28:36.040]   sort of over $1,000 for a vacuum cleaner.
[01:28:36.040 --> 01:28:37.840]   Unfortunately for them,
[01:28:37.840 --> 01:28:39.240]   now that they've proved everything out
[01:28:39.240 --> 01:28:40.920]   and figured it all out, now there's competitors.
[01:28:40.920 --> 01:28:43.560]   - Yeah, and so that's the next thing, right?
[01:28:43.560 --> 01:28:46.680]   The competition, and they have quite a number,
[01:28:46.680 --> 01:28:50.240]   even internationally, like there's some products out there
[01:28:50.240 --> 01:28:52.480]   you can go to Europe and be like,
[01:28:52.480 --> 01:28:55.120]   "Oh, I didn't even know this one existed."
[01:28:55.120 --> 01:28:56.880]   So this is the thing though,
[01:28:56.880 --> 01:29:00.400]   like with any market, I would,
[01:29:00.400 --> 01:29:03.680]   this is not a bad time,
[01:29:03.680 --> 01:29:06.480]   although as a roboticist, it's kind of depressing,
[01:29:06.480 --> 01:29:10.640]   but I actually think about things like with,
[01:29:10.640 --> 01:29:13.160]   I would say that all of the companies
[01:29:13.160 --> 01:29:15.880]   that are now in the top five or six,
[01:29:15.880 --> 01:29:19.720]   they weren't the first to the stage, right?
[01:29:19.720 --> 01:29:22.880]   Like Google was not the first search engine,
[01:29:22.880 --> 01:29:24.880]   sorry AltaVista, right?
[01:29:24.880 --> 01:29:28.440]   Facebook was not the first, sorry, MySpace, right?
[01:29:28.440 --> 01:29:31.200]   Like think about it, they were not the first players.
[01:29:31.200 --> 01:29:35.560]   Those first players, like they're not in the top five,
[01:29:35.560 --> 01:29:39.520]   10 of Fortune 500 companies, right?
[01:29:39.520 --> 01:29:44.080]   They proved, they started to prove out the market,
[01:29:44.080 --> 01:29:46.480]   they started to get people interested,
[01:29:46.480 --> 01:29:48.440]   they started the buzz,
[01:29:48.440 --> 01:29:50.200]   but they didn't make it to that next level.
[01:29:50.200 --> 01:29:52.400]   But the second batch, right?
[01:29:52.400 --> 01:29:57.400]   The second batch, I think might make it to the next level.
[01:29:57.400 --> 01:30:02.480]   - When do you think the Facebook of, ugh.
[01:30:02.480 --> 01:30:03.800]   - The Facebook of robotics.
[01:30:03.800 --> 01:30:07.360]   - Sorry, I take that phrase back
[01:30:07.360 --> 01:30:09.640]   because people deeply, for some reason,
[01:30:09.640 --> 01:30:11.760]   well, I know why, but it's, I think,
[01:30:11.760 --> 01:30:13.840]   exaggerated distrust Facebook
[01:30:13.840 --> 01:30:15.640]   because of the privacy concerns and so on.
[01:30:15.640 --> 01:30:18.560]   And with robotics, one of the things you have to make sure
[01:30:18.560 --> 01:30:20.240]   is all the things we've talked about
[01:30:20.240 --> 01:30:23.160]   is to be transparent and have people deeply trust you
[01:30:23.160 --> 01:30:25.960]   to let a robot into their lives, into their home.
[01:30:25.960 --> 01:30:28.800]   When do you think the second batch of robots,
[01:30:28.800 --> 01:30:32.320]   is it five, 10 years, 20 years,
[01:30:32.320 --> 01:30:34.880]   that we'll have robots in our homes
[01:30:34.880 --> 01:30:36.720]   and robots in our hearts?
[01:30:36.720 --> 01:30:40.200]   - So if I think about, 'cause I try to follow the VC
[01:30:40.200 --> 01:30:43.360]   kind of space in terms of robotic investments.
[01:30:43.360 --> 01:30:45.080]   And right now, and I don't know
[01:30:45.080 --> 01:30:46.120]   if they're gonna be successful,
[01:30:46.120 --> 01:30:48.160]   I don't know if this is the second batch,
[01:30:49.400 --> 01:30:51.600]   but there's only one batch that's focused on
[01:30:51.600 --> 01:30:53.040]   the first batch, right?
[01:30:53.040 --> 01:30:56.440]   And then there's all these self-driving Xs, right?
[01:30:56.440 --> 01:30:59.720]   And so I don't know if they're a first batch of something
[01:30:59.720 --> 01:31:03.240]   or if, I don't know quite where they fit in,
[01:31:03.240 --> 01:31:05.680]   but there's a number of companies,
[01:31:05.680 --> 01:31:08.600]   the co-robot, I call them co-robots,
[01:31:08.600 --> 01:31:11.480]   that are still getting VC investments.
[01:31:11.480 --> 01:31:14.640]   Some of them have some of the flavor
[01:31:14.640 --> 01:31:15.880]   of like Rethink Robotics,
[01:31:15.880 --> 01:31:19.120]   some of them have some of the flavor of like Curie.
[01:31:19.120 --> 01:31:20.840]   - What's a co-robot?
[01:31:20.840 --> 01:31:25.840]   - So basically a robot and human working in the same space.
[01:31:25.840 --> 01:31:30.600]   So some of the companies are focused on manufacturing.
[01:31:30.600 --> 01:31:35.600]   So having a robot and human working together in a factory,
[01:31:35.600 --> 01:31:40.000]   some of these co-robots are robots and humans
[01:31:40.000 --> 01:31:42.200]   working in the home, working in clinics.
[01:31:42.200 --> 01:31:44.080]   Like there's different versions of these companies
[01:31:44.080 --> 01:31:47.040]   in terms of their products, but they're all,
[01:31:47.040 --> 01:31:50.320]   so Rethink Robotics would be like one of the first,
[01:31:50.320 --> 01:31:54.720]   at least well-known companies focused on this space.
[01:31:54.720 --> 01:31:56.840]   So I don't know if this is a second batch
[01:31:56.840 --> 01:32:01.080]   or if this is still part of the first batch,
[01:32:01.080 --> 01:32:02.120]   that I don't know.
[01:32:02.120 --> 01:32:03.880]   And then you have all these other companies
[01:32:03.880 --> 01:32:06.960]   in this self-driving space.
[01:32:06.960 --> 01:32:09.520]   And I don't know if that's a first batch
[01:32:09.520 --> 01:32:11.280]   or again, a second batch.
[01:32:11.280 --> 01:32:14.000]   - Yeah, so there's a lot of mystery about this now.
[01:32:14.000 --> 01:32:16.520]   Of course, it's hard to say that this is the second batch
[01:32:16.520 --> 01:32:18.520]   until it proves out, right?
[01:32:18.520 --> 01:32:19.360]   - Correct. - Yeah, exactly.
[01:32:19.360 --> 01:32:20.640]   - Yeah, we need a unicorn.
[01:32:20.640 --> 01:32:21.740]   - Yeah, exactly.
[01:32:21.740 --> 01:32:26.840]   Why do you think people are so afraid,
[01:32:26.840 --> 01:32:30.520]   at least in popular culture of legged robots
[01:32:30.520 --> 01:32:32.440]   like those worked in Boston Dynamics
[01:32:32.440 --> 01:32:34.200]   or just robotics in general?
[01:32:34.200 --> 01:32:36.360]   If you were to psychoanalyze that fear,
[01:32:36.360 --> 01:32:38.040]   what do you make of it?
[01:32:38.040 --> 01:32:39.840]   And should they be afraid, sorry?
[01:32:39.840 --> 01:32:41.520]   - So should people be afraid?
[01:32:41.520 --> 01:32:45.400]   I don't think people should be afraid, but with a caveat.
[01:32:45.400 --> 01:32:47.160]   I don't think people should be afraid
[01:32:47.160 --> 01:32:51.520]   given that most of us in this world
[01:32:51.520 --> 01:32:55.760]   understand that we need to change something, right?
[01:32:55.760 --> 01:32:58.200]   So given that.
[01:32:58.200 --> 01:33:01.600]   Now, if things don't change, be very afraid.
[01:33:01.600 --> 01:33:04.520]   - Which is the dimension of change that's needed?
[01:33:04.520 --> 01:33:07.880]   - So thinking about the ramifications,
[01:33:07.880 --> 01:33:09.520]   thinking about like the ethics,
[01:33:09.520 --> 01:33:12.840]   thinking about like the conversation is going on, right?
[01:33:12.840 --> 01:33:17.760]   It's no longer a, we're gonna deploy it and forget that,
[01:33:17.760 --> 01:33:20.440]   this is a car that can kill pedestrians
[01:33:20.440 --> 01:33:22.600]   that are walking across the street, right?
[01:33:22.600 --> 01:33:24.440]   We're not in that stage.
[01:33:24.440 --> 01:33:25.920]   We're putting these roads out.
[01:33:25.920 --> 01:33:27.600]   There are people out there.
[01:33:27.600 --> 01:33:28.920]   A car could be a weapon.
[01:33:28.920 --> 01:33:33.240]   People are now, solutions aren't there yet,
[01:33:33.240 --> 01:33:35.440]   but people are thinking about this
[01:33:35.440 --> 01:33:38.560]   as we need to be ethically responsible
[01:33:38.560 --> 01:33:40.920]   as we send these systems out,
[01:33:40.920 --> 01:33:43.160]   robotics, medical, self-driving.
[01:33:43.160 --> 01:33:44.560]   - And military too.
[01:33:44.560 --> 01:33:45.400]   - And military.
[01:33:45.400 --> 01:33:47.120]   - Which is not as often talked about,
[01:33:47.120 --> 01:33:50.360]   but it's really where probably these robots
[01:33:50.360 --> 01:33:52.040]   will have a significant impact as well.
[01:33:52.040 --> 01:33:53.240]   - Correct, correct, right?
[01:33:53.240 --> 01:33:57.480]   Making sure that they can think rationally,
[01:33:57.480 --> 01:33:58.840]   even having the conversations,
[01:33:58.840 --> 01:34:01.360]   who should pull the trigger, right?
[01:34:01.360 --> 01:34:03.720]   - But overall, you're saying if we start to think more
[01:34:03.720 --> 01:34:05.840]   and more as a community about these ethical issues,
[01:34:05.840 --> 01:34:07.080]   people should not be afraid.
[01:34:07.080 --> 01:34:08.760]   - Yeah, I don't think people should be afraid.
[01:34:08.760 --> 01:34:10.640]   I think that the return on investment,
[01:34:10.640 --> 01:34:14.160]   the positive impact will outweigh
[01:34:14.160 --> 01:34:17.600]   any of the potentially negative impacts.
[01:34:17.600 --> 01:34:21.800]   - Do you have worries of existential threats of robots
[01:34:21.800 --> 01:34:25.800]   or AI that some people kind of talk about
[01:34:25.800 --> 01:34:28.920]   and romanticize about in the next decade,
[01:34:28.920 --> 01:34:30.240]   next few decades?
[01:34:30.240 --> 01:34:31.600]   - No, I don't.
[01:34:31.600 --> 01:34:33.960]   Singularity would be an example.
[01:34:33.960 --> 01:34:36.640]   So my concept is that, so remember,
[01:34:36.640 --> 01:34:39.840]   robots, AI is designed by people.
[01:34:39.840 --> 01:34:41.520]   It has our values.
[01:34:41.520 --> 01:34:45.360]   And I always correlate this with a parent and a child, right?
[01:34:45.360 --> 01:34:47.400]   So think about it, as a parent, what do we want?
[01:34:47.400 --> 01:34:50.120]   We want our kids to have a better life than us.
[01:34:50.120 --> 01:34:52.560]   We want them to expand.
[01:34:52.560 --> 01:34:56.080]   We want them to experience the world.
[01:34:56.080 --> 01:35:00.000]   And then as we grow older, our kids think and know
[01:35:00.000 --> 01:35:03.280]   they're smarter and better and more intelligent
[01:35:03.280 --> 01:35:05.040]   and have better opportunities.
[01:35:05.040 --> 01:35:08.480]   And they may even stop listening to us.
[01:35:08.480 --> 01:35:10.800]   They don't go out and then kill us, right?
[01:35:10.800 --> 01:35:11.640]   Like think about it.
[01:35:11.640 --> 01:35:14.480]   It's because it's instilled in them values.
[01:35:14.480 --> 01:35:17.720]   We instilled in them this whole aspect of community.
[01:35:17.720 --> 01:35:20.120]   And yes, even though you're maybe smarter
[01:35:20.120 --> 01:35:22.760]   and have more money and da, da, da,
[01:35:22.760 --> 01:35:27.080]   it's still about this love, caring relationship.
[01:35:27.080 --> 01:35:28.040]   And so that's what I believe.
[01:35:28.040 --> 01:35:30.840]   So even if like, you know, we've created the singularity
[01:35:30.840 --> 01:35:34.120]   in some archaic system back in like 1980
[01:35:34.120 --> 01:35:35.680]   that suddenly evolves,
[01:35:35.680 --> 01:35:40.560]   the fact is it might say, I am smarter, I am sentient.
[01:35:40.560 --> 01:35:43.600]   These humans are really stupid,
[01:35:43.600 --> 01:35:45.600]   but I think it'll be like, yeah,
[01:35:45.600 --> 01:35:48.000]   but I just can't destroy them.
[01:35:48.000 --> 01:35:49.600]   - Yeah, for sentimental value.
[01:35:49.600 --> 01:35:53.280]   Still just to come back for Thanksgiving dinner
[01:35:53.280 --> 01:35:54.120]   every once in a while.
[01:35:54.120 --> 01:35:55.440]   - Exactly.
[01:35:55.440 --> 01:35:57.560]   - That's so beautifully put.
[01:35:57.560 --> 01:35:59.840]   You've also said that "The Matrix"
[01:35:59.840 --> 01:36:03.800]   may be one of your more favorite AI-related movies.
[01:36:03.800 --> 01:36:05.680]   Can you elaborate why?
[01:36:05.680 --> 01:36:07.920]   - Yeah, it is one of my favorite movies.
[01:36:07.920 --> 01:36:11.280]   And it's because it represents
[01:36:11.280 --> 01:36:14.160]   kind of all the things I think about.
[01:36:14.160 --> 01:36:16.160]   So there's a symbiotic relationship
[01:36:16.160 --> 01:36:20.200]   between robots and humans, right?
[01:36:20.200 --> 01:36:22.560]   That symbiotic relationship is that they don't destroy us,
[01:36:22.560 --> 01:36:24.680]   they enslave us, right?
[01:36:24.680 --> 01:36:25.680]   But think about it.
[01:36:25.680 --> 01:36:30.360]   Even though they enslaved us,
[01:36:30.360 --> 01:36:32.920]   they needed us to be happy, right?
[01:36:32.920 --> 01:36:33.960]   And in order to be happy,
[01:36:33.960 --> 01:36:35.520]   they had to create this crude world
[01:36:35.520 --> 01:36:37.080]   that they then had to live in, right?
[01:36:37.080 --> 01:36:38.720]   That's the whole premise.
[01:36:38.720 --> 01:36:44.480]   But then there were humans that had a choice, right?
[01:36:44.480 --> 01:36:47.760]   Like you had a choice to stay in this horrific,
[01:36:47.760 --> 01:36:51.320]   horrific world where it was your fantasy life
[01:36:51.320 --> 01:36:54.840]   with all of the anomalies, perfection, but not accurate.
[01:36:54.840 --> 01:36:58.040]   Or you can choose to be on your own
[01:36:58.040 --> 01:37:02.600]   and like have maybe no food for a couple of days,
[01:37:02.600 --> 01:37:05.280]   but you were totally autonomous.
[01:37:05.280 --> 01:37:08.080]   And so I think of that as, and that's why.
[01:37:08.080 --> 01:37:09.800]   So it's not necessarily us being enslaved,
[01:37:09.800 --> 01:37:13.160]   but I think about us having the symbiotic relationship.
[01:37:13.160 --> 01:37:15.880]   Robots and AI, even if they become sentient,
[01:37:15.880 --> 01:37:17.200]   they're still part of our society
[01:37:17.200 --> 01:37:20.840]   and they will suffer just as much as we.
[01:37:20.840 --> 01:37:23.920]   - And there will be some kind of equilibrium
[01:37:23.920 --> 01:37:26.800]   that we'll have to find some symbiotic relationship.
[01:37:26.800 --> 01:37:28.320]   - Right, and then you have the ethicists,
[01:37:28.320 --> 01:37:30.280]   the robotics folks that were like,
[01:37:30.280 --> 01:37:32.400]   no, this has got to stop.
[01:37:32.400 --> 01:37:36.320]   I will take the other pill in order to make a difference.
[01:37:36.320 --> 01:37:41.200]   - So if you could hang out for a day with a robot,
[01:37:41.200 --> 01:37:45.320]   real or from science fiction, movies, books, safely,
[01:37:45.320 --> 01:37:50.320]   and get to pick his or her, their brain, who would you pick?
[01:37:50.320 --> 01:37:58.240]   - Gotta say it's Data.
[01:37:58.240 --> 01:37:59.360]   - Data.
[01:37:59.360 --> 01:38:01.600]   - I was gonna say Rosie, but I don't,
[01:38:01.600 --> 01:38:04.200]   I'm not really interested in her brain.
[01:38:04.200 --> 01:38:06.360]   I'm interested in Data's brain.
[01:38:06.360 --> 01:38:09.000]   - Data pre or post-emotionship?
[01:38:09.000 --> 01:38:09.840]   - Pre.
[01:38:09.840 --> 01:38:14.720]   - But don't you think it'd be a more interesting
[01:38:14.720 --> 01:38:16.640]   conversation post-emotionship?
[01:38:16.640 --> 01:38:18.200]   - Yeah, it would be drama.
[01:38:18.200 --> 01:38:20.440]   And I, you know, I'm human.
[01:38:20.440 --> 01:38:23.320]   I deal with drama all the time.
[01:38:23.320 --> 01:38:25.320]   But the reason why I wanna pick Data's brain
[01:38:25.320 --> 01:38:30.040]   is because I could have a conversation with him
[01:38:30.040 --> 01:38:32.880]   and ask, for example,
[01:38:32.880 --> 01:38:35.680]   how can we fix this ethics problem, right?
[01:38:35.680 --> 01:38:39.280]   And he could go through like the rational thinking
[01:38:39.280 --> 01:38:40.280]   and through that,
[01:38:40.280 --> 01:38:43.080]   he could also help me think through it as well.
[01:38:43.080 --> 01:38:44.760]   And so that's, there's like these questions,
[01:38:44.760 --> 01:38:47.280]   fundamental questions I think I could ask him
[01:38:47.280 --> 01:38:50.800]   that he would help me also learn from.
[01:38:50.800 --> 01:38:52.480]   And that fascinates me.
[01:38:52.480 --> 01:38:55.880]   - I don't think there's a better place to end it.
[01:38:55.880 --> 01:38:57.360]   Aiyana, thank you so much for talking to me.
[01:38:57.360 --> 01:38:58.200]   It was an honor.
[01:38:58.200 --> 01:38:59.040]   - Thank you, thank you.
[01:38:59.040 --> 01:38:59.880]   This was fun.
[01:38:59.880 --> 01:39:03.280]   - Thanks for listening to this conversation
[01:39:03.280 --> 01:39:06.480]   and thank you to our presenting sponsor, Cash App.
[01:39:06.480 --> 01:39:09.120]   Download it, use code LexPodcast.
[01:39:09.120 --> 01:39:11.920]   You'll get $10 and $10 will go to FIRST,
[01:39:11.920 --> 01:39:14.720]   a STEM education nonprofit that inspires hundreds
[01:39:14.720 --> 01:39:16.400]   of thousands of young minds
[01:39:16.400 --> 01:39:19.280]   to become future leaders and innovators.
[01:39:19.280 --> 01:39:22.120]   If you enjoy this podcast, subscribe on YouTube,
[01:39:22.120 --> 01:39:24.120]   get five stars on Apple Podcast,
[01:39:24.120 --> 01:39:26.760]   follow on Spotify, support on Patreon,
[01:39:26.760 --> 01:39:28.800]   or simply connect with me on Twitter.
[01:39:29.800 --> 01:39:32.400]   And now let me leave you with some words of wisdom
[01:39:32.400 --> 01:39:33.960]   from Arthur C. Clarke.
[01:39:33.960 --> 01:39:38.520]   Whether we are based on carbon or on silicon
[01:39:38.520 --> 01:39:40.560]   makes no fundamental difference.
[01:39:40.560 --> 01:39:43.600]   We should each be treated with appropriate respect.
[01:39:43.600 --> 01:39:47.720]   Thank you for listening and hope to see you next time.
[01:39:47.720 --> 01:39:50.320]   (upbeat music)
[01:39:50.320 --> 01:39:52.920]   (upbeat music)
[01:39:52.920 --> 01:40:02.920]   [BLANK_AUDIO]

