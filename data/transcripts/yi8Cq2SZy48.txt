
[00:00:00.000 --> 00:00:05.800]   Many times we hear of AI news that we only realize was significant in hindsight.
[00:00:05.800 --> 00:00:10.320]   This week came developments that are big from any perspective.
[00:00:10.320 --> 00:00:15.020]   I don't just mean the whole new category of use cases for GPT vision,
[00:00:15.020 --> 00:00:18.940]   or meta bringing language models to billions of people.
[00:00:18.940 --> 00:00:24.400]   There's also Autogen as the new AutoGPT, what I'm calling the Altman phone,
[00:00:24.400 --> 00:00:26.980]   Mistral's new 7 billion parameter model,
[00:00:26.980 --> 00:00:30.420]   Orca potentially replacing OpenAI models at Microsoft,
[00:00:30.420 --> 00:00:34.340]   and yesterday's fascinating GPT Fathom paper.
[00:00:34.340 --> 00:00:38.920]   I'm going to start with a use case for GPT-4 vision that I didn't see coming.
[00:00:38.920 --> 00:00:45.340]   If you spot a user interface online that you like, you can try to recreate it with GPT-4.
[00:00:45.340 --> 00:00:52.360]   As at Scorano demonstrated on X, you can ask it to imitate the layout and give you the HTML code.
[00:00:52.360 --> 00:00:55.760]   Now I know many of you will say that doesn't look too impressive,
[00:00:55.760 --> 00:00:56.760]   but wait until you...
[00:00:56.760 --> 00:00:58.480]   see the next demo.
[00:00:58.480 --> 00:01:02.160]   GPT-4 with vision can iterate on its designs.
[00:01:02.160 --> 00:01:06.720]   Because it can see its own output, it can recognize flaws and improve on them.
[00:01:06.720 --> 00:01:10.520]   This is from Matt Schumer, CEO of Hyperight AI.
[00:01:10.520 --> 00:01:16.760]   You can see it trying to design that futuristic Google homepage and improving with each generation.
[00:01:16.760 --> 00:01:20.880]   We've gone beyond text feedback now into a visual feedback loop.
[00:01:20.880 --> 00:01:25.020]   I honestly wonder if someone is going to do something similar for DALI 3.
[00:01:25.020 --> 00:01:26.520]   Use GPT-4 vision to...
[00:01:26.520 --> 00:01:31.000]   continue to iterate outputs until it matches your prompt perfectly.
[00:01:31.000 --> 00:01:35.680]   Now the next development probably won't impress too many of the viewers watching my channel,
[00:01:35.680 --> 00:01:42.680]   but it might have a big impact on the up to 4 billion people who use a meta product or service.
[00:01:42.680 --> 00:01:45.760]   We're talking Instagram, WhatsApp and of course Facebook.
[00:01:45.760 --> 00:01:48.280]   Now I'm not going to play you all the promotional materials,
[00:01:48.280 --> 00:01:53.640]   but essentially they've got a bunch of celebrities including Mr Beast and quite a few others you can see here,
[00:01:53.640 --> 00:01:56.320]   to put their name to a series of...
[00:01:56.320 --> 00:01:58.120]   28 AI chatbots.
[00:01:58.120 --> 00:02:00.560]   Now I don't particularly want to speak to any of these people,
[00:02:00.560 --> 00:02:06.040]   but I'm sure that millions if not billions of people want to pretend to do so at least.
[00:02:06.040 --> 00:02:14.120]   Now remember it was only yesterday that Character AI that deals in fictional chatbots was valued at more than $5 billion.
[00:02:14.120 --> 00:02:18.160]   And Zuckerberg is clearly taking aim at Character AI.
[00:02:18.160 --> 00:02:21.680]   Here he is in the metaverse with Lex Friedman.
[00:02:21.680 --> 00:02:26.080]   I don't think anyone out there is really doing...
[00:02:26.080 --> 00:02:27.080]   what we're doing here.
[00:02:27.080 --> 00:02:32.880]   I think that there are people who are doing kind of like fictional or consumer oriented character type stuff,
[00:02:32.880 --> 00:02:39.680]   but the extent to which we're building it out with the avatars and expressiveness
[00:02:39.680 --> 00:02:45.520]   and making it so that they can interact across all the different apps and they'll have profiles.
[00:02:45.520 --> 00:02:49.720]   Let's be honest, if the Apple Vision Pro takes VR mainstream,
[00:02:49.720 --> 00:02:55.840]   then a lot of people will want to sit down and have an artificial chat with their favorite celebrity.
[00:02:55.840 --> 00:02:59.000]   And it doesn't take much to imagine other use cases.
[00:02:59.000 --> 00:03:04.360]   Now one group who can definitely think of some use cases for chatbots is the CIA.
[00:03:04.360 --> 00:03:08.280]   They are apparently getting their own chat GPT style tool.
[00:03:08.280 --> 00:03:13.360]   Now of course we already know that the CIA monitors data that passes through the US,
[00:03:13.360 --> 00:03:17.200]   but there's so much of it that it must have been impossible to sort through.
[00:03:17.200 --> 00:03:20.280]   With a large language model, I don't think that's going to be true anymore.
[00:03:20.280 --> 00:03:25.600]   That's going to be useful for catching criminals and the FBI is getting this chatbot too.
[00:03:25.600 --> 00:03:32.480]   But it does remind me a lot of this article about large language models being great for state censorship.
[00:03:32.480 --> 00:03:39.280]   For similar reasons, it would have been impossible for a country like China to monitor all of its civilian communications.
[00:03:39.280 --> 00:03:43.280]   But now as the article says, there is no real way to prevent this.
[00:03:43.280 --> 00:03:48.960]   It's only a matter of time before well-resourced state actors begin implementing and advancing such systems.
[00:03:48.960 --> 00:03:52.560]   But in lighter news, yesterday we had this from The Verge.
[00:03:52.560 --> 00:03:55.360]   Donny Ive of Apple fame, together with Sam Altman of
[00:03:55.360 --> 00:04:00.720]   OpenAI fame, are coming up with the iPhone of Artificial Intelligence.
[00:04:00.720 --> 00:04:04.960]   Fueled by over $1 billion in funding from the Softbank CEO.
[00:04:04.960 --> 00:04:12.400]   It would be OpenAI's first consumer device and we might even have a few clues about what would distinguish it.
[00:04:12.400 --> 00:04:20.240]   Donny Ive has previously said that Apple had a moral responsibility to mitigate the addictive nature of its technology.
[00:04:20.240 --> 00:04:25.120]   And according to the Financial Times, the project with OpenAI could allow
[00:04:25.120 --> 00:04:30.800]   Ive to create an interactive computing device that's less reliant on screens.
[00:04:30.800 --> 00:04:35.360]   Of course we saw this week that ChatGPT can now take audio and visual input.
[00:04:35.360 --> 00:04:38.720]   Now apparently the discussions are said to be serious.
[00:04:38.720 --> 00:04:43.760]   So let me know in the comments what you would want out of let's call it the Altman phone.
[00:04:43.760 --> 00:04:47.520]   But now I want to talk about Autogen from Microsoft.
[00:04:47.520 --> 00:04:51.520]   Many of you wondered if I'd been following this development and yes I have been.
[00:04:51.520 --> 00:04:54.880]   But rather than just talk about what they claim, I wanted to try
[00:04:54.880 --> 00:04:56.000]   it out for myself.
[00:04:56.000 --> 00:05:02.960]   I had heard that Autogen is poised to fundamentally transform and extend what large language models are capable of.
[00:05:02.960 --> 00:05:08.880]   And it would do this through multi-agent conversations, joint chats and agent customization.
[00:05:08.880 --> 00:05:16.160]   Think of it as a more sophisticated AutoGPT allowing the easy creation of sub-agents to achieve a goal.
[00:05:16.160 --> 00:05:24.640]   One of the agents or models could be an engineer writing code, another one an executor executing code, or a product manager coming up with the
[00:05:24.640 --> 00:05:25.440]   implementation plan.
[00:05:25.440 --> 00:05:29.600]   But to be honest I was like I've heard of this kind of thing before, does it actually work?
[00:05:29.600 --> 00:05:35.520]   I discussed use cases with the amazing AI architect Nico Giraud and we came up with this demo.
[00:05:35.520 --> 00:05:41.520]   A maths question that GPT-4 with code interpreter or advanced data analysis almost always gets wrong.
[00:05:41.520 --> 00:05:45.440]   For anyone who's interested is in the style of GMAT data sufficiency.
[00:05:45.440 --> 00:05:51.120]   Autogen was able to easily create sub-agents to delegate tasks to and it could break down these
[00:05:51.120 --> 00:05:54.400]   compositional problems in coding or here for example math.
[00:05:54.400 --> 00:06:04.880]   Now the Autogen system can be run with a human in the loop essentially as one of the agents or maybe as the commander and Autogen can use tools and execute code.
[00:06:04.880 --> 00:06:11.440]   All these agents are essentially in a group chat working together chipping in when necessary or when called for by a planner.
[00:06:11.440 --> 00:06:17.840]   It got this difficult mathematical problem right three times out of three and yes this is just an anecdotal demo.
[00:06:17.840 --> 00:06:24.160]   I'm going to be doing much more research on Autogen and I'm reaching out to some of the authors.
[00:06:24.160 --> 00:06:29.200]   I'm going to be collecting the results I found in my previous video on reasoning in LLMs.
[00:06:29.200 --> 00:06:34.160]   I must say that Autogen made me think again about this recent tweet from Sam Altman.
[00:06:34.160 --> 00:06:39.200]   He said "short timelines and slow takeoff will be a pretty good call I think."
[00:06:39.200 --> 00:06:43.680]   Short timelines meaning artificial general intelligence not being several decades away.
[00:06:43.680 --> 00:06:49.520]   And slow takeoff as in not exponential self-improvement on the scale of days,
[00:06:49.520 --> 00:06:52.000]   weeks or months when AGI comes out.
[00:06:52.000 --> 00:06:53.920]   But the quote continues "but the way
[00:06:53.920 --> 00:06:58.000]   people define the start of the takeoff may make it seem otherwise."
[00:06:58.000 --> 00:07:02.960]   I read that as saying that if you put too strict a definition on what AGI means.
[00:07:02.960 --> 00:07:08.960]   If you continually move the goalposts such that AGI means the exact same thing as super intelligence.
[00:07:08.960 --> 00:07:11.120]   Being smarter than all humans combined.
[00:07:11.120 --> 00:07:17.120]   Well under that definition the moment AGI arrives things could start changing extremely rapidly.
[00:07:17.120 --> 00:07:23.680]   But if we stop moving the goalposts and admit that things like Autogen could be pretty radical now.
[00:07:23.680 --> 00:07:28.720]   According to definitions from say 10 years ago many people would argue we have AGI now.
[00:07:28.720 --> 00:07:34.720]   Well in that scenario you could argue we might be in a fairly slow takeoff that could be measured in years.
[00:07:34.720 --> 00:07:36.720]   Potentially even more than a decade.
[00:07:36.720 --> 00:07:38.800]   Now of course I might be misinterpreting his words.
[00:07:38.800 --> 00:07:44.800]   But I know that Philip from 10 years ago would have looked at GPT-4 with vision and Autogen.
[00:07:44.800 --> 00:07:47.040]   And been absolutely gobsmacked.
[00:07:47.040 --> 00:07:53.440]   And I do think that some combination of the techniques that we have available today together with scaling.
[00:07:53.440 --> 00:07:54.880]   Will bring us to AGI.
[00:07:54.880 --> 00:07:59.040]   That's why I would agree with the majority of forecasters on Metaculous.
[00:07:59.040 --> 00:08:02.400]   That the first AGI will be based on deep learning.
[00:08:02.400 --> 00:08:06.400]   It's free to sign up to Metaculous using the link in the description.
[00:08:06.400 --> 00:08:10.560]   And you get the double benefit of seeing what other people are predicting.
[00:08:10.560 --> 00:08:14.240]   And also you get the chance to put your forecast in writing.
[00:08:14.240 --> 00:08:18.960]   That way when you're right you get to boast in my comments about how right you were.
[00:08:18.960 --> 00:08:22.000]   Thanks as always to Metaculous for sponsoring the video.
[00:08:22.000 --> 00:08:23.200]   And I wonder if the next video will be about AGI.
[00:08:23.200 --> 00:08:26.320]   And if the next development is going to change any of these predictions.
[00:08:26.320 --> 00:08:31.840]   That development is the release of Mistral 7B for 7 billion parameters.
[00:08:31.840 --> 00:08:38.480]   Now apparently it outperforms Llama 2 13 billion parameters on all benchmarks.
[00:08:38.480 --> 00:08:41.360]   And based on my limited tests with Perplexity Chat.
[00:08:41.360 --> 00:08:44.240]   Where you can pick the Mistral 7 billion model.
[00:08:44.240 --> 00:08:47.120]   I can roughly believe that that is the performance level.
[00:08:47.120 --> 00:08:49.120]   It does make plenty of mistakes of course.
[00:08:49.120 --> 00:08:52.960]   Here though you can see it beating a range of Llama models including.
[00:08:52.960 --> 00:08:56.960]   Llama 2 13 billion on a range of benchmarks.
[00:08:56.960 --> 00:08:59.200]   As I've pointed out several times on the channel.
[00:08:59.200 --> 00:09:01.440]   There are problems with the benchmarks though.
[00:09:01.440 --> 00:09:04.800]   And we'll have to wait and see about data contamination.
[00:09:04.800 --> 00:09:08.880]   Now Mistral does admit that there are no moderation mechanisms.
[00:09:08.880 --> 00:09:12.160]   And I tried pretty much any example you can think of.
[00:09:12.160 --> 00:09:13.600]   And it always helped out.
[00:09:13.600 --> 00:09:15.280]   For pretty much obvious reasons.
[00:09:15.280 --> 00:09:18.080]   I can't even demonstrate the kind of things that I asked.
[00:09:18.080 --> 00:09:19.600]   And it happily replied to.
[00:09:19.600 --> 00:09:22.000]   But remember this is only their teaser model.
[00:09:22.000 --> 00:09:22.720]   Much larger model.
[00:09:22.720 --> 00:09:26.320]   And I'm sure it will be more capable.
[00:09:26.320 --> 00:09:29.280]   And it's released under the Apache 2 license.
[00:09:29.280 --> 00:09:31.200]   Which is extremely permissive.
[00:09:31.200 --> 00:09:34.080]   When the Mistral models are further fine tuned.
[00:09:34.080 --> 00:09:37.040]   I'm sure we are going to see more benchmarks broken.
[00:09:37.040 --> 00:09:38.560]   But I would ask the question.
[00:09:38.560 --> 00:09:41.040]   Does this constitute a race to the bottom.
[00:09:41.040 --> 00:09:45.600]   In which the company that spends the least amount of money on protections wins out.
[00:09:45.600 --> 00:09:48.240]   Of course because it's only 7 billion parameters.
[00:09:48.240 --> 00:09:50.160]   It's not capable of that much.
[00:09:50.160 --> 00:09:52.480]   But a future Mistral 70 billion.
[00:09:52.480 --> 00:09:54.960]   Or 700 billion parameter model.
[00:09:54.960 --> 00:09:58.640]   Might make headlines for good and bad reasons.
[00:09:58.640 --> 00:10:00.960]   But smaller models do seem to be the trend.
[00:10:00.960 --> 00:10:03.680]   With even Microsoft looking to downsize.
[00:10:03.680 --> 00:10:05.120]   As the information put it.
[00:10:05.120 --> 00:10:08.400]   Microsoft is trying to lessen its addiction to open AI.
[00:10:08.400 --> 00:10:09.840]   As AI costs soar.
[00:10:09.840 --> 00:10:14.160]   The article talks about how Microsoft researchers are making distilled models.
[00:10:14.160 --> 00:10:16.880]   That mimic larger models like GPT-4.
[00:10:16.880 --> 00:10:19.680]   But are smaller and cost far less to operate.
[00:10:19.680 --> 00:10:22.240]   Notably they mention Orca and the PHY series.
[00:10:22.240 --> 00:10:23.040]   Of models.
[00:10:23.040 --> 00:10:26.560]   I am proud to have been one of the first people to cover the Orca model.
[00:10:26.560 --> 00:10:27.840]   And my video on it called.
[00:10:27.840 --> 00:10:29.680]   The model few people saw coming.
[00:10:29.680 --> 00:10:32.720]   Was retweeted by the lead author of Orca.
[00:10:32.720 --> 00:10:36.000]   Of course I also interviewed one of the lead creators.
[00:10:36.000 --> 00:10:37.840]   On the PHY series of models.
[00:10:37.840 --> 00:10:39.040]   In a separate video.
[00:10:39.040 --> 00:10:40.160]   As the article points out.
[00:10:40.160 --> 00:10:45.200]   The fine-tuned Orca model performs much better than the base model of Llama 2.
[00:10:45.200 --> 00:10:47.840]   And as I remember the Orca paper pointing out.
[00:10:47.840 --> 00:10:52.000]   It performs nearly as well as GPT-4 on certain tasks.
[00:10:52.000 --> 00:10:53.440]   In fact now I speak of it.
[00:10:53.440 --> 00:10:56.000]   I remember predicting at the end of the Orca video.
[00:10:56.000 --> 00:10:57.280]   You can go and watch it.
[00:10:57.280 --> 00:10:59.040]   I remember saying something like.
[00:10:59.040 --> 00:11:01.440]   I wonder if Microsoft is funding Orca.
[00:11:01.440 --> 00:11:05.920]   To see if it can substitute for GPT-4 or the next GPT-5.
[00:11:05.920 --> 00:11:06.960]   Actually now I look back.
[00:11:06.960 --> 00:11:08.640]   That was a pretty amazing prediction.
[00:11:08.640 --> 00:11:12.720]   Indeed it looks like Microsoft might well substitute Orca for GPT-4.
[00:11:12.720 --> 00:11:14.160]   For certain use cases.
[00:11:14.160 --> 00:11:19.120]   Particularly as it uses less than a tenth of the computing power that GPT-4 uses.
[00:11:19.120 --> 00:11:20.800]   But speaking of being ahead of the curve.
[00:11:20.800 --> 00:11:21.760]   Here is a fact.
[00:11:21.760 --> 00:11:23.600]   This is a fascinating paper.
[00:11:23.600 --> 00:11:25.760]   That I don't think anyone has talked about.
[00:11:25.760 --> 00:11:27.920]   Of course that's probably because it came out yesterday.
[00:11:27.920 --> 00:11:28.880]   But nevertheless.
[00:11:28.880 --> 00:11:30.960]   It achieves part of what I talked about.
[00:11:30.960 --> 00:11:33.600]   Toward the end of my smart GPT video.
[00:11:33.600 --> 00:11:37.600]   About systematically and fairly evaluating the leading LLMs.
[00:11:37.600 --> 00:11:39.040]   On a range of benchmarks.
[00:11:39.040 --> 00:11:41.200]   Not this patchwork of different conditions.
[00:11:41.200 --> 00:11:41.920]   Benchmarks.
[00:11:41.920 --> 00:11:43.760]   Settings and models that we currently have.
[00:11:43.760 --> 00:11:46.560]   I'm hoping to speak to one of the lead authors of this paper.
[00:11:46.560 --> 00:11:49.680]   But for now let me just give you a taste of the highlights.
[00:11:49.680 --> 00:11:51.520]   First we have this epic chart.
[00:11:51.520 --> 00:11:54.080]   I want you to focus on one part in particular.
[00:11:54.080 --> 00:11:58.480]   At the top right you can see the 1st of March version of GPT 3.5.
[00:11:58.480 --> 00:12:00.400]   And then the 13th of June version.
[00:12:00.400 --> 00:12:03.200]   For GPT-4 we have the 14th of March version.
[00:12:03.200 --> 00:12:04.880]   And the 13th of June version.
[00:12:04.880 --> 00:12:08.000]   And the paper later describes the seesaw progress.
[00:12:08.000 --> 00:12:09.360]   It's not a straight line upwards.
[00:12:09.360 --> 00:12:12.560]   In other words the conspiracy that ChatGPT is getting dumber.
[00:12:12.560 --> 00:12:15.440]   In some areas is actually correct.
[00:12:15.440 --> 00:12:18.160]   Take the first row which is natural questions.
[00:12:18.160 --> 00:12:21.280]   Performance went down slightly between these two versions.
[00:12:21.280 --> 00:12:23.840]   And then the 4 trivia QA for GPT-4.
[00:12:23.840 --> 00:12:25.200]   It went down slightly.
[00:12:25.200 --> 00:12:28.720]   And you can look across the benchmarks to see the same things happening.
[00:12:28.720 --> 00:12:31.680]   And this is not just noise as the paper later points out.
[00:12:31.680 --> 00:12:40.480]   The June version of GPT 3.5 saw its score on math dramatically degrade from 32.0 to 15.0.
[00:12:40.480 --> 00:12:42.720]   That's compared to the March 1st version.
[00:12:42.720 --> 00:12:44.720]   In contrast on the drop benchmark.
[00:12:44.720 --> 00:12:49.840]   GPT-4 performance increases from 78.7 to 87.2.
[00:12:49.840 --> 00:12:51.040]   Which is on par with state of the art.
[00:12:51.040 --> 00:12:53.040]   Between the two versions.
[00:12:53.040 --> 00:12:55.520]   However between those same two versions.
[00:12:55.520 --> 00:13:00.720]   Its score on a different benchmark plummeted from 82.2 to 68.7.
[00:13:00.720 --> 00:13:01.920]   This is not just noise.
[00:13:01.920 --> 00:13:04.800]   And OpenAI admits that when they release a new model.
[00:13:04.800 --> 00:13:07.680]   While the majority of metrics might improve.
[00:13:07.680 --> 00:13:10.560]   There may be some tasks where the performance gets worse.
[00:13:10.560 --> 00:13:15.680]   Which brings me to the web based version of GPT-4 and ChatGPT.
[00:13:15.680 --> 00:13:18.480]   The paper notes that the dated API models.
[00:13:18.480 --> 00:13:20.800]   That's those with the four numbers at the end.
[00:13:20.800 --> 00:13:24.800]   Consistently perform slightly better than their front end counterparts.
[00:13:24.800 --> 00:13:27.040]   I.e. the web version of the models.
[00:13:27.040 --> 00:13:31.280]   To be honest I had noticed dissimilarities when benchmarking SmartGPT.
[00:13:31.280 --> 00:13:33.360]   They do say that code interpreter.
[00:13:33.360 --> 00:13:35.520]   Now called advanced data analysis.
[00:13:35.520 --> 00:13:38.880]   Has significantly improved the coding benchmark performance.
[00:13:38.880 --> 00:13:42.320]   Getting 85.2 pass at one on human eval.
[00:13:42.320 --> 00:13:46.880]   So you're not hallucinating when you see differences between the web GPT models.
[00:13:46.880 --> 00:13:48.320]   And the API models.
[00:13:48.320 --> 00:13:50.560]   Anyway there is so much more from this paper.
[00:13:50.560 --> 00:13:51.840]   That I want to cover.
[00:13:51.840 --> 00:13:54.960]   Preferably after I've spoken to one of the lead authors.
[00:13:54.960 --> 00:13:57.360]   But for now the video is long enough.
[00:13:57.360 --> 00:14:00.320]   We have covered some pretty epic topics today.
[00:14:00.320 --> 00:14:03.200]   If you enjoyed or learnt anything from this overview.
[00:14:03.200 --> 00:14:05.200]   Please do let me know in the comments.
[00:14:05.200 --> 00:14:07.600]   Shameless plug at this point for my Patreon.
[00:14:07.600 --> 00:14:09.600]   Which is also linked in the description.
[00:14:09.600 --> 00:14:10.320]   And as ever.
[00:14:10.320 --> 00:14:12.560]   Thank you so much for watching to the end.
[00:14:12.560 --> 00:14:14.080]   And have a wonderful day.

