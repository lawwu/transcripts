
[00:00:00.000 --> 00:00:03.440]   [MUSIC PLAYING]
[00:00:03.440 --> 00:00:12.200]   Hello.
[00:00:12.200 --> 00:00:13.400]   This is Hamil.
[00:00:13.400 --> 00:00:15.120]   I'm going to introduce this session.
[00:00:15.120 --> 00:00:20.040]   So today, we're going to be talking about this paper,
[00:00:20.040 --> 00:00:22.520]   Operationalizing Machine Learning in Interview Study,
[00:00:22.520 --> 00:00:26.440]   which was written by Shreya Shankar and her colleagues
[00:00:26.440 --> 00:00:27.840]   at UC Berkeley.
[00:00:27.840 --> 00:00:32.240]   And we also have today Josh Wills,
[00:00:32.240 --> 00:00:35.120]   who's a really experienced data scientist currently
[00:00:35.120 --> 00:00:39.600]   at Weave Grid, working as a software engineer.
[00:00:39.600 --> 00:00:45.640]   Just to kind of set the stage a bit around what
[00:00:45.640 --> 00:00:46.600]   we're talking about.
[00:00:46.600 --> 00:00:50.560]   So why this paper is really interesting
[00:00:50.560 --> 00:00:56.160]   is there's hundreds of surveys done about machine learning
[00:00:56.160 --> 00:00:57.040]   all the time.
[00:00:57.040 --> 00:01:01.480]   If you look at LinkedIn, kind of inundated with it.
[00:01:01.480 --> 00:01:04.000]   A lot of these surveys are not really that interesting,
[00:01:04.000 --> 00:01:05.280]   honestly.
[00:01:05.280 --> 00:01:08.080]   But this survey hit really hard.
[00:01:08.080 --> 00:01:13.480]   And the reason it did is because it kind of boldly states
[00:01:13.480 --> 00:01:16.560]   a lot of things that people are too afraid to talk about.
[00:01:16.560 --> 00:01:18.720]   And it's written very well.
[00:01:18.720 --> 00:01:23.000]   It weaves together anecdotes with data.
[00:01:23.000 --> 00:01:27.080]   And it just really communicates everything very nicely.
[00:01:27.080 --> 00:01:29.360]   And I think a lot of data professionals
[00:01:29.360 --> 00:01:32.800]   have nodded their head in agreement when they read this.
[00:01:32.800 --> 00:01:37.760]   But before we get it kicked off, I kind of
[00:01:37.760 --> 00:01:41.600]   want to let you all introduce yourself, because I just
[00:01:41.600 --> 00:01:42.960]   had a really short introduction.
[00:01:42.960 --> 00:01:45.280]   And then I think we also just want
[00:01:45.280 --> 00:01:47.520]   to talk a little bit about how the paper was written
[00:01:47.520 --> 00:01:50.040]   before we get into questions.
[00:01:50.040 --> 00:01:52.040]   So I'll let you go.
[00:01:52.040 --> 00:01:53.240]   Shreya, you want to go first?
[00:01:53.240 --> 00:01:54.160]   Please.
[00:01:54.160 --> 00:01:54.840]   Sure.
[00:01:54.840 --> 00:01:55.440]   I'm Shreya.
[00:01:55.440 --> 00:01:56.320]   Thanks for having me.
[00:01:56.320 --> 00:01:59.640]   I'm super excited to talk to both of you.
[00:01:59.640 --> 00:02:04.280]   I am a PhD student at UC Berkeley in databases
[00:02:04.280 --> 00:02:08.160]   and interested in machine learning,
[00:02:08.160 --> 00:02:10.480]   how do we apply data management principles
[00:02:10.480 --> 00:02:14.280]   to build better, sustainable, reliable machine learning
[00:02:14.280 --> 00:02:16.920]   systems.
[00:02:16.920 --> 00:02:18.800]   And I am Josh Wills.
[00:02:18.800 --> 00:02:21.120]   I'm a software engineer at WeaveGrid.
[00:02:21.120 --> 00:02:24.720]   I think, as Hamil pointed out, I am sort of a data scientist
[00:02:24.720 --> 00:02:28.200]   that made a long series of really bad life choices.
[00:02:28.200 --> 00:02:32.280]   And so it went from data science down to data engineering,
[00:02:32.280 --> 00:02:34.920]   and then kind of just kept on going down.
[00:02:34.920 --> 00:02:37.040]   My fall, my descent into software engineering
[00:02:37.040 --> 00:02:38.520]   has not stopped.
[00:02:38.520 --> 00:02:40.440]   Maybe at some point, I will be designing hardware
[00:02:40.440 --> 00:02:41.900]   for all I know, that kind of stuff.
[00:02:41.900 --> 00:02:45.080]   But I love this paper as well, and I'm
[00:02:45.080 --> 00:02:47.680]   super excited to be able to talk to Shreya about it.
[00:02:47.680 --> 00:02:50.960]   Hamil, why don't you kick us off with some questions?
[00:02:50.960 --> 00:02:52.960]   You did your best.
[00:02:52.960 --> 00:02:56.520]   Yeah, so I'm going to get right into the middle of the paper
[00:02:56.520 --> 00:03:00.040]   with some of the most burning questions I have,
[00:03:00.040 --> 00:03:01.640]   our observations.
[00:03:01.640 --> 00:03:04.000]   So one of the things that you kind of point out
[00:03:04.000 --> 00:03:06.880]   in the paper, Shreya, is this importance
[00:03:06.880 --> 00:03:10.360]   of having a sandbox and this mismatch between development
[00:03:10.360 --> 00:03:12.360]   and production environments, and then
[00:03:12.360 --> 00:03:17.100]   how people are struggling creating a similar development
[00:03:17.100 --> 00:03:20.880]   environment and production environments.
[00:03:20.880 --> 00:03:24.080]   And there's tension between this velocity
[00:03:24.080 --> 00:03:29.000]   and validating things.
[00:03:29.000 --> 00:03:32.960]   So when I read this, it really triggered me,
[00:03:32.960 --> 00:03:37.920]   because I feel like I've kind of been gaslighted for so long
[00:03:37.920 --> 00:03:39.160]   that, hey, we have the cloud.
[00:03:39.160 --> 00:03:40.600]   It should be so easy to experiment
[00:03:40.600 --> 00:03:41.960]   with development environment.
[00:03:41.960 --> 00:03:44.920]   Yet every company I have ever worked at
[00:03:44.920 --> 00:03:49.560]   is actually really hard, still right now.
[00:03:49.560 --> 00:03:52.120]   But I feel like we're all told that it should be easy,
[00:03:52.120 --> 00:03:55.320]   and it is easy, but we don't really talk about it.
[00:03:55.320 --> 00:03:59.040]   I was really excited to see that in the paper.
[00:03:59.040 --> 00:04:02.240]   And it seems like a fundamental problem.
[00:04:02.240 --> 00:04:05.320]   It's kind of like a brass tacks of doing anything,
[00:04:05.320 --> 00:04:09.000]   not even data science, just doing any software-related work
[00:04:09.000 --> 00:04:11.640]   at all.
[00:04:11.640 --> 00:04:14.440]   And so I found it pretty--
[00:04:14.440 --> 00:04:17.960]   it was kind of also a slap in the face, I feel like,
[00:04:17.960 --> 00:04:20.000]   in some sense, to the industry.
[00:04:20.000 --> 00:04:23.360]   So it feels like we're building a lot of tools downstream
[00:04:23.360 --> 00:04:27.600]   from this, but we're still struggling on this sandbox.
[00:04:27.600 --> 00:04:34.360]   So what did-- yeah, just see if y'all can respond to that.
[00:04:34.360 --> 00:04:35.880]   I wonder what y'all think.
[00:04:35.880 --> 00:04:39.840]   I can tell you how the section came about.
[00:04:39.840 --> 00:04:41.960]   So we were doing these semi-structured interviews.
[00:04:41.960 --> 00:04:43.920]   Basically, it's a one-hour-long Zoom call.
[00:04:43.920 --> 00:04:46.920]   And the questions were not leading in any way.
[00:04:46.920 --> 00:04:49.840]   It was like, tell us about a bug you fixed last week.
[00:04:49.840 --> 00:04:52.160]   Or it was-- there was nothing.
[00:04:52.160 --> 00:04:55.560]   We didn't say any buzzwords, no environment.
[00:04:55.560 --> 00:04:57.000]   We never said Jupyter Notebooks.
[00:04:57.000 --> 00:05:01.360]   People come to us saying Jupyter Notebooks for this reason.
[00:05:01.360 --> 00:05:04.280]   And we got-- we annotated these transcripts
[00:05:04.280 --> 00:05:06.880]   with all the different types of bugs that came up.
[00:05:06.880 --> 00:05:08.880]   And one of the things for me was--
[00:05:08.880 --> 00:05:12.640]   and also Rolando, we had some 5,000 different annotations.
[00:05:12.640 --> 00:05:15.320]   And we were like, oh my god, we cannot sit here
[00:05:15.320 --> 00:05:17.320]   through these 5,000 different codes.
[00:05:17.320 --> 00:05:19.520]   How do we group them together?
[00:05:19.520 --> 00:05:21.440]   Look at all these different types of bugs
[00:05:21.440 --> 00:05:23.160]   that we have found.
[00:05:23.160 --> 00:05:25.160]   Is there anything that connects them
[00:05:25.160 --> 00:05:29.200]   that we can say to the crowd?
[00:05:29.200 --> 00:05:33.200]   And a lot of them happened to be like, oh man,
[00:05:33.200 --> 00:05:36.880]   if they just were using the same environment that they
[00:05:36.880 --> 00:05:38.920]   had in production, this bug wouldn't be there.
[00:05:38.920 --> 00:05:40.960]   Or they were assuming access to data
[00:05:40.960 --> 00:05:43.000]   that they didn't have in production,
[00:05:43.000 --> 00:05:44.760]   but they did this during development time.
[00:05:44.760 --> 00:05:46.480]   So a lot of these types of--
[00:05:46.480 --> 00:05:49.920]   once we framed it as that, then it dawned on us that, oh,
[00:05:49.920 --> 00:05:54.160]   we should write about this mismatch between development
[00:05:54.160 --> 00:05:56.280]   and production.
[00:05:56.280 --> 00:06:00.280]   And then why is it that there's a mismatch goes back
[00:06:00.280 --> 00:06:03.920]   to our ML engineering is highly experimental thing
[00:06:03.920 --> 00:06:04.960]   that we said.
[00:06:04.960 --> 00:06:09.880]   When you have such an iterative experimental process
[00:06:09.880 --> 00:06:12.520]   during development, it's important to have
[00:06:12.520 --> 00:06:16.960]   high velocity so you can throw away bad ideas quickly.
[00:06:16.960 --> 00:06:21.240]   I want to have local GPUs so I can try different ideas.
[00:06:21.240 --> 00:06:22.840]   I want to try things locally.
[00:06:22.840 --> 00:06:26.120]   I want to use notebooks so I can load my data frame once
[00:06:26.120 --> 00:06:28.680]   instead of repeatedly.
[00:06:28.680 --> 00:06:30.960]   All of these small, small things matter.
[00:06:30.960 --> 00:06:37.640]   But it's really hard to support that in a production
[00:06:37.640 --> 00:06:39.440]   environment.
[00:06:39.440 --> 00:06:43.440]   And I think that's where the discrepancy happens.
[00:06:43.440 --> 00:06:46.560]   In the beginning, you want to train models, get something,
[00:06:46.560 --> 00:06:50.480]   prove some promise to some client.
[00:06:50.480 --> 00:06:52.640]   But that's a fundamentally different mode
[00:06:52.640 --> 00:06:57.880]   of working than the production pipelines.
[00:06:57.880 --> 00:07:00.400]   I loved that section of the paper as well, Shreya.
[00:07:00.400 --> 00:07:03.120]   And it made me think about the places
[00:07:03.120 --> 00:07:06.200]   I have worked where that actually is not true, where
[00:07:06.200 --> 00:07:08.560]   we did, in fact, have that sandbox environment.
[00:07:08.560 --> 00:07:12.280]   And I had three of them in my experience.
[00:07:12.280 --> 00:07:14.360]   The first one was Google.
[00:07:14.360 --> 00:07:17.120]   So Google, when I worked on ad CTR prediction,
[00:07:17.120 --> 00:07:20.920]   the system for doing it back in the day was called SmartAss.
[00:07:20.920 --> 00:07:24.360]   And SmartAss, it was the smart ad selection system
[00:07:24.360 --> 00:07:25.360]   or something like that.
[00:07:25.360 --> 00:07:27.440]   It was developed way back in 2004.
[00:07:27.440 --> 00:07:29.800]   So it was a fairly mature piece of technology
[00:07:29.800 --> 00:07:32.920]   by the time I got there in 2007.
[00:07:32.920 --> 00:07:37.440]   And SmartAss was like Valpo, Wabbit, plus, plus, plus.
[00:07:37.440 --> 00:07:40.280]   Pamela, everyone knows my deep affection for Valpo, Wabbit,
[00:07:40.280 --> 00:07:41.040]   I assume.
[00:07:41.040 --> 00:07:42.200]   Yes, we do, yeah.
[00:07:42.200 --> 00:07:43.520]   Yes, exactly.
[00:07:43.520 --> 00:07:45.400]   And so what that meant was SmartAss
[00:07:45.400 --> 00:07:49.320]   was in production updating its weights.
[00:07:49.320 --> 00:07:52.320]   Every 15 minutes, it would publish a new model
[00:07:52.320 --> 00:07:54.640]   based on what it had learned in the last 15 minutes
[00:07:54.640 --> 00:07:57.360]   about what ads where people were clicking on and stuff.
[00:07:57.360 --> 00:08:01.800]   And so there was a whole massive infrastructure
[00:08:01.800 --> 00:08:06.240]   for running offline sandbox style simulations runs
[00:08:06.240 --> 00:08:08.440]   of SmartAss models over the exact same data
[00:08:08.440 --> 00:08:10.560]   that's on production and so on and so forth.
[00:08:10.560 --> 00:08:12.280]   And the other case where I've seen this
[00:08:12.280 --> 00:08:14.600]   is in high frequency trading.
[00:08:14.600 --> 00:08:16.240]   High frequency trading is another place
[00:08:16.240 --> 00:08:19.240]   where, again, there is code that is learning online
[00:08:19.240 --> 00:08:20.280]   from data it sees.
[00:08:20.280 --> 00:08:22.920]   And then being able to back test that and run stuff offline
[00:08:22.920 --> 00:08:24.240]   is super, super important.
[00:08:24.240 --> 00:08:25.960]   And so they invest a ton of energy in it.
[00:08:25.960 --> 00:08:27.460]   And then finally, my favorite by far
[00:08:27.460 --> 00:08:30.520]   was Netflix, which ran into this problem as well.
[00:08:30.520 --> 00:08:34.160]   And so Netflix, being Netflix, actually
[00:08:34.160 --> 00:08:39.240]   connected to production when they were training their models
[00:08:39.240 --> 00:08:41.800]   to fetch their data.
[00:08:41.800 --> 00:08:45.200]   It was like the most Netflix, crazy chaos monkey kind
[00:08:45.200 --> 00:08:48.160]   of thing I'd ever heard of.
[00:08:48.160 --> 00:08:50.800]   And so I think what is special to me about this
[00:08:50.800 --> 00:08:54.280]   is that the reason we're bad at this most of the time
[00:08:54.280 --> 00:08:57.400]   is because model training is not something
[00:08:57.400 --> 00:08:59.800]   we typically do in production.
[00:08:59.800 --> 00:09:01.320]   We do inference in production.
[00:09:01.320 --> 00:09:02.480]   And we do inference online.
[00:09:02.480 --> 00:09:05.640]   We don't do model training and actual model updating
[00:09:05.640 --> 00:09:08.040]   online in production anymore.
[00:09:08.040 --> 00:09:10.280]   And so the only time you ever see this kind of sandbox
[00:09:10.280 --> 00:09:11.840]   pattern happen is with companies that
[00:09:11.840 --> 00:09:15.640]   are doing online model training, where literally the model is
[00:09:15.640 --> 00:09:16.640]   learning in production.
[00:09:16.640 --> 00:09:19.800]   And so there is this huge, huge engineering impetus
[00:09:19.800 --> 00:09:21.520]   to train stuff, to train stuff online.
[00:09:21.520 --> 00:09:24.160]   That, to me, is the fundamental difference there.
[00:09:24.160 --> 00:09:28.280]   That was my meta level, yeah, thing anyway.
[00:09:28.280 --> 00:09:29.400]   That's interesting.
[00:09:29.400 --> 00:09:29.900]   Yeah.
[00:09:29.900 --> 00:09:33.760]   I guess it feels like the Googles and the Netflix
[00:09:33.760 --> 00:09:39.520]   of the world, they have this utopia that project outwards
[00:09:39.520 --> 00:09:45.480]   onto whatever, Silicon Valley, or even like all of tech.
[00:09:45.480 --> 00:09:51.600]   And things that they don't discuss,
[00:09:51.600 --> 00:09:53.400]   I feel like maybe sometimes people
[00:09:53.400 --> 00:09:56.560]   take for granted, like the sandbox thing, perhaps.
[00:09:56.560 --> 00:09:57.480]   I don't know.
[00:09:57.480 --> 00:09:58.160]   Exactly right.
[00:09:58.160 --> 00:10:01.360]   There's so much social stuff that is not--
[00:10:01.360 --> 00:10:03.440]   it's all technology and tools and not
[00:10:03.440 --> 00:10:05.800]   the social organizational structures around it
[00:10:05.800 --> 00:10:09.000]   that make the tools work.
[00:10:09.000 --> 00:10:11.640]   Anyway, back to Shreya, though.
[00:10:11.640 --> 00:10:13.840]   Let's me and you shut up for a bit.
[00:10:13.840 --> 00:10:14.640]   No, definitely.
[00:10:14.640 --> 00:10:17.360]   I totally agree.
[00:10:17.360 --> 00:10:19.680]   That's not what we're trying to do in this panel, though.
[00:10:19.680 --> 00:10:21.840]   We're trying to disagree with each other violently.
[00:10:21.840 --> 00:10:24.160]   So let's talk about notebooks more, then.
[00:10:24.160 --> 00:10:25.800]   Yeah, yeah, let's talk about notebooks.
[00:10:25.800 --> 00:10:28.920]   OK, I would just like to follow up on that,
[00:10:28.920 --> 00:10:33.560]   to go deeper into that area, to linger on this a little bit
[00:10:33.560 --> 00:10:36.120]   more, this development environment.
[00:10:36.120 --> 00:10:42.800]   So a very related thing is this idea of software engineering
[00:10:42.800 --> 00:10:44.600]   for machine learning.
[00:10:44.600 --> 00:10:48.480]   And so this is discussed extensively in the paper.
[00:10:48.480 --> 00:10:52.200]   So the first thing that caught my attention
[00:10:52.200 --> 00:10:59.400]   is this section that has strong opinions on Jupyter Notebooks.
[00:10:59.400 --> 00:11:02.000]   And it has anecdotes from people that says,
[00:11:02.000 --> 00:11:04.240]   we want to put notebooks in production.
[00:11:04.240 --> 00:11:07.200]   And then it contrasts that with other people that says,
[00:11:07.200 --> 00:11:09.160]   we don't allow notebooks at all.
[00:11:09.160 --> 00:11:11.720]   Even there's a quote that says, this is garbage.
[00:11:11.720 --> 00:11:16.760]   And maybe I'm misquoting.
[00:11:16.760 --> 00:11:22.080]   But someone used that adjective in the context of notebooks.
[00:11:22.080 --> 00:11:23.160]   This was all garbage.
[00:11:23.160 --> 00:11:26.040]   Yeah, yeah, this is all garbage.
[00:11:26.040 --> 00:11:29.680]   And so in the kind of--
[00:11:29.680 --> 00:11:36.800]   you summarize as, OK, this is like an artifact
[00:11:36.800 --> 00:11:39.560]   of these competing priorities.
[00:11:39.560 --> 00:11:44.320]   But then also to compound on that, this notebook thing,
[00:11:44.320 --> 00:11:49.720]   then we talk about code review and how people felt
[00:11:49.720 --> 00:11:51.840]   like code review is not helpful.
[00:11:51.840 --> 00:11:53.400]   And then there's this other quote
[00:11:53.400 --> 00:11:57.880]   that I found very interesting, probably my favorite quote
[00:11:57.880 --> 00:11:59.280]   of the whole paper.
[00:11:59.280 --> 00:12:00.480]   And I'll just read it.
[00:12:00.480 --> 00:12:04.000]   It says, I'm feeling more convinced
[00:12:04.000 --> 00:12:07.400]   that software engineering practices--
[00:12:07.400 --> 00:12:09.120]   or I'll just read it from the beginning.
[00:12:09.120 --> 00:12:10.880]   I used to see a lot of people complaining
[00:12:10.880 --> 00:12:13.320]   that model developers don't follow software engineering
[00:12:13.320 --> 00:12:14.000]   practices.
[00:12:14.000 --> 00:12:16.240]   At this point, I'm feeling more convinced
[00:12:16.240 --> 00:12:18.520]   that they don't follow software engineering practices,
[00:12:18.520 --> 00:12:21.000]   not because they're lazy, but because software engineering
[00:12:21.000 --> 00:12:23.040]   practices are contradictory to the agility
[00:12:23.040 --> 00:12:25.240]   analysis of exploration.
[00:12:25.240 --> 00:12:26.480]   So wow.
[00:12:26.480 --> 00:12:30.640]   So you take that all together, like the sandbox thing,
[00:12:30.640 --> 00:12:31.760]   notebooks.
[00:12:31.760 --> 00:12:34.960]   People don't even-- first of all,
[00:12:34.960 --> 00:12:38.440]   they're having a problem setting up a development environment.
[00:12:38.440 --> 00:12:40.400]   And then there's a lot of contention
[00:12:40.400 --> 00:12:46.000]   around what kind of development environment people should use.
[00:12:46.000 --> 00:12:49.200]   That's encapsulated in this notebook.
[00:12:49.200 --> 00:12:52.360]   And also, what their software engineering workflow
[00:12:52.360 --> 00:12:54.360]   should look like.
[00:12:54.360 --> 00:12:56.120]   Should you even use notebooks?
[00:12:56.120 --> 00:12:59.600]   Or how does code review fit in?
[00:12:59.600 --> 00:13:01.680]   And then there's this repudiation
[00:13:01.680 --> 00:13:04.400]   of the whole software development workflow.
[00:13:04.400 --> 00:13:06.240]   And this really resonates with me,
[00:13:06.240 --> 00:13:09.600]   because I feel like, at least for me,
[00:13:09.600 --> 00:13:13.120]   software developers have shoved software development
[00:13:13.120 --> 00:13:16.760]   into my face as a machine learning engineer.
[00:13:16.760 --> 00:13:20.160]   And it's only after a certain amount of time,
[00:13:20.160 --> 00:13:22.040]   maybe in the last two years, I've
[00:13:22.040 --> 00:13:25.480]   started pushing back to the extent
[00:13:25.480 --> 00:13:29.880]   where I'm building tools that reject some of these things.
[00:13:29.880 --> 00:13:35.720]   So I just want to throw that out there and have both of you
[00:13:35.720 --> 00:13:36.960]   react.
[00:13:36.960 --> 00:13:41.480]   The number of times I've seen data science code that's
[00:13:41.480 --> 00:13:44.800]   software engineered, they shove things in a class
[00:13:44.800 --> 00:13:46.880]   because it's software engineering.
[00:13:46.880 --> 00:13:48.000]   Oh my god.
[00:13:48.000 --> 00:13:49.840]   It's not the data science.
[00:13:49.840 --> 00:13:51.320]   It's just like, why?
[00:13:51.320 --> 00:13:52.840]   You don't mash.
[00:13:52.840 --> 00:13:55.640]   This is another problem I have with the MLOps industry, where
[00:13:55.640 --> 00:13:57.800]   it's like, oh, we can solve problems
[00:13:57.800 --> 00:14:00.920]   if we throw software engineering principle.
[00:14:00.920 --> 00:14:03.720]   A lot of people talk about, oh, we throw unit testing
[00:14:03.720 --> 00:14:04.560]   at machine learning.
[00:14:04.560 --> 00:14:06.600]   We solve MLOps.
[00:14:06.600 --> 00:14:10.440]   And I'm like, you literally picked two buzzwords
[00:14:10.440 --> 00:14:11.680]   and fused them together.
[00:14:11.680 --> 00:14:13.120]   That's not a product.
[00:14:13.120 --> 00:14:15.040]   But anyways, I don't know.
[00:14:15.040 --> 00:14:17.680]   It's a separate tangent.
[00:14:17.680 --> 00:14:22.720]   One thing I wish I had written about--
[00:14:22.720 --> 00:14:24.360]   I could have written about in the paper.
[00:14:24.360 --> 00:14:25.760]   I don't know how relevant it is because it's
[00:14:25.760 --> 00:14:28.160]   an academic paper submitted to an academic conference.
[00:14:28.160 --> 00:14:31.000]   But there is a large cultural context
[00:14:31.000 --> 00:14:36.200]   around software engineering, machine learning principles.
[00:14:36.200 --> 00:14:38.040]   Like, for example, in the 2000s, there
[00:14:38.040 --> 00:14:41.840]   was an ethos around moving fast and breaking things.
[00:14:41.840 --> 00:14:45.160]   So when you think about this, it screams velocity.
[00:14:45.160 --> 00:14:46.160]   Let's iterate fast.
[00:14:46.160 --> 00:14:47.880]   Let's ship quickly.
[00:14:47.880 --> 00:14:51.560]   And then think about the consequences later.
[00:14:51.560 --> 00:14:55.840]   And a lot of software engineering, even data science,
[00:14:55.840 --> 00:14:58.840]   or just iterating quickly falls under this.
[00:14:58.840 --> 00:15:04.040]   But for a lot of ML tasks, you cannot have that kind of ethos.
[00:15:04.040 --> 00:15:06.560]   Autonomous vehicles are a great example of--
[00:15:06.560 --> 00:15:09.120]   we interviewed some people from AV companies.
[00:15:09.120 --> 00:15:11.040]   And they said, at the beginning, we
[00:15:11.040 --> 00:15:12.440]   were having too high velocity.
[00:15:12.440 --> 00:15:15.600]   And people were maintaining separate forks of evaluation
[00:15:15.600 --> 00:15:17.960]   because they wanted to ship to production fast.
[00:15:17.960 --> 00:15:20.640]   But the consequence of an on-the-road failure
[00:15:20.640 --> 00:15:21.720]   is so high.
[00:15:21.720 --> 00:15:23.160]   There's a quote in there that said,
[00:15:23.160 --> 00:15:27.440]   we'd much rather gate models going to production
[00:15:27.440 --> 00:15:29.160]   than put something bad there.
[00:15:29.160 --> 00:15:33.400]   So I think going back to your Jupyter notebooks,
[00:15:33.400 --> 00:15:35.880]   that tension between velocity and validation
[00:15:35.880 --> 00:15:37.040]   is like a spectrum.
[00:15:37.040 --> 00:15:40.480]   People rely on different places in that spectrum.
[00:15:40.480 --> 00:15:42.720]   And it's not just personal philosophy.
[00:15:42.720 --> 00:15:45.880]   I think it's also the broader cultural context.
[00:15:45.880 --> 00:15:47.760]   What's the ethos that their organization has?
[00:15:47.760 --> 00:15:49.520]   What is the task that they're working on?
[00:15:49.520 --> 00:15:55.400]   Everything has different consequences.
[00:15:55.400 --> 00:15:58.480]   Going into the study, I was always like,
[00:15:58.480 --> 00:16:01.280]   why do people have such polarizing opinions?
[00:16:01.280 --> 00:16:03.040]   I think now it makes sense why people
[00:16:03.040 --> 00:16:07.520]   have polarizing opinions.
[00:16:07.520 --> 00:16:11.440]   I don't know if there's any correct opinion based
[00:16:11.440 --> 00:16:16.960]   on the diversity of teams and use cases out there.
[00:16:16.960 --> 00:16:17.520]   That's right.
[00:16:17.520 --> 00:16:19.080]   I mean, that doesn't work in a panel.
[00:16:19.080 --> 00:16:20.800]   We don't have nuanced opinions here.
[00:16:20.800 --> 00:16:23.600]   Now, we make different statements.
[00:16:23.600 --> 00:16:25.920]   I'm going to say, no, software engineering principles are
[00:16:25.920 --> 00:16:26.960]   always right.
[00:16:26.960 --> 00:16:28.480]   Hamlet is completely wrong.
[00:16:28.480 --> 00:16:29.680]   Notebooks are bad.
[00:16:29.680 --> 00:16:30.720]   You should not use them.
[00:16:30.720 --> 00:16:32.000]   No, I kid.
[00:16:32.000 --> 00:16:32.880]   But also not.
[00:16:32.880 --> 00:16:34.280]   Yeah, anyway.
[00:16:34.280 --> 00:16:35.720]   Oh, nuance.
[00:16:35.720 --> 00:16:37.360]   What fun is that?
[00:16:37.360 --> 00:16:43.720]   So do we think this development workflow issue can be solved?
[00:16:43.720 --> 00:16:50.960]   I mean, this polarizing thing is sort of a signal to me
[00:16:50.960 --> 00:16:54.320]   that we haven't arrived at the right thing yet.
[00:16:54.320 --> 00:16:56.480]   Otherwise, it wouldn't be so polarizing.
[00:16:56.480 --> 00:17:00.240]   So do we think that it can be solved?
[00:17:00.240 --> 00:17:04.240]   I think that there will be schools of thought that
[00:17:04.240 --> 00:17:09.000]   will have more opinionated guardrails on notebooks.
[00:17:09.000 --> 00:17:11.600]   I like to think back about spreadsheets.
[00:17:11.600 --> 00:17:15.800]   Sadly, it was not alive when spreadsheets first came out.
[00:17:15.800 --> 00:17:18.200]   There's a camp of people who will
[00:17:18.200 --> 00:17:19.800]   use spreadsheets for everything.
[00:17:19.800 --> 00:17:21.680]   They will put mistakes in their spreadsheets.
[00:17:21.680 --> 00:17:22.840]   It is OK to have mistakes.
[00:17:22.840 --> 00:17:26.240]   It is OK to present crappy results to their boss.
[00:17:26.240 --> 00:17:27.640]   And they'll get promoted for it.
[00:17:27.640 --> 00:17:32.080]   And that is just the nature of life in certain industries.
[00:17:32.080 --> 00:17:34.400]   They will make spreadsheets to manage spreadsheets.
[00:17:34.400 --> 00:17:38.120]   It's amazing the level of spreadsheet confusion
[00:17:38.120 --> 00:17:39.440]   going on.
[00:17:39.440 --> 00:17:40.880]   I have a feeling that it's probably
[00:17:40.880 --> 00:17:44.960]   going to be the same in some ways.
[00:17:44.960 --> 00:17:48.240]   Data management tool is kind of like this.
[00:17:48.240 --> 00:17:50.400]   There's some camp of people that are going
[00:17:50.400 --> 00:17:51.840]   to use them no matter what.
[00:17:51.840 --> 00:17:54.640]   And the consequence of a failure is actually not that drastic.
[00:17:54.640 --> 00:17:57.320]   Because at the end of the day, the organization
[00:17:57.320 --> 00:17:58.680]   is all about the people.
[00:17:58.680 --> 00:18:00.160]   What do the people want?
[00:18:00.160 --> 00:18:02.280]   What is the gut instinct of the leader?
[00:18:02.280 --> 00:18:04.360]   If they want to change the metric,
[00:18:04.360 --> 00:18:05.640]   they could change the metric.
[00:18:05.640 --> 00:18:10.200]   At some big organizations we interviewed for this paper,
[00:18:10.200 --> 00:18:11.920]   they'll say things like, oh, our top line
[00:18:11.920 --> 00:18:13.720]   metric changes every quarter.
[00:18:13.720 --> 00:18:16.800]   And it's like, you know that click-through rate
[00:18:16.800 --> 00:18:20.440]   and number of seconds I spend watching a piece of media,
[00:18:20.440 --> 00:18:22.480]   two completely different metrics.
[00:18:22.480 --> 00:18:29.240]   But they will switch from one to another in the span of a week.
[00:18:29.240 --> 00:18:31.040]   Yeah, so I don't know.
[00:18:31.040 --> 00:18:33.360]   It doesn't feel like something that I would personally
[00:18:33.360 --> 00:18:34.920]   do if I were running a team.
[00:18:34.920 --> 00:18:36.720]   But I think it's out there.
[00:18:36.720 --> 00:18:39.720]   And there's enough people that are like this.
[00:18:39.720 --> 00:18:42.080]   It makes the space of data management really interesting,
[00:18:42.080 --> 00:18:42.920]   I think.
[00:18:42.920 --> 00:18:44.640]   So sorry, you wouldn't personally do what
[00:18:44.640 --> 00:18:45.840]   if you're running a team?
[00:18:45.840 --> 00:18:50.000]   Oh, I don't think I would personally--
[00:18:50.000 --> 00:18:52.440]   I would want to have a lot of guardrails on the data
[00:18:52.440 --> 00:18:57.280]   management tools and make the product decisions less
[00:18:57.280 --> 00:19:00.560]   instinct-driven and more--
[00:19:00.560 --> 00:19:01.880]   when you get to a certain point.
[00:19:01.880 --> 00:19:03.340]   In the beginning, you probably need
[00:19:03.340 --> 00:19:06.200]   to have more instinct-driven product decisions.
[00:19:06.200 --> 00:19:09.560]   But when you're running a Google-sized company--
[00:19:09.560 --> 00:19:12.120]   not that I'm ever wanting to run a Google-sized company.
[00:19:12.120 --> 00:19:14.200]   But when you're running something like that,
[00:19:14.200 --> 00:19:18.680]   you need to make really justified decisions.
[00:19:18.680 --> 00:19:22.440]   And a spreadsheet that you can't trust,
[00:19:22.440 --> 00:19:24.440]   like a Jupyter Notebook that you can't trust,
[00:19:24.440 --> 00:19:27.040]   it's not going to cut it for me.
[00:19:27.040 --> 00:19:30.360]   If I may make an analogy, I guess my takeaway from this,
[00:19:30.360 --> 00:19:32.600]   really, and I think what's most useful about the doc,
[00:19:32.600 --> 00:19:35.520]   is really broadly that software engineers do not
[00:19:35.520 --> 00:19:42.640]   understand the exploration model building workflow.
[00:19:42.640 --> 00:19:45.760]   They really just deeply, profoundly don't get it.
[00:19:45.760 --> 00:19:49.160]   And I think when you ask a software engineer to look
[00:19:49.160 --> 00:19:52.360]   at the model development workflow, what comes out
[00:19:52.360 --> 00:19:54.000]   is AutoML.
[00:19:54.000 --> 00:19:56.120]   Yeah, thank you.
[00:19:56.120 --> 00:19:56.600]   Right?
[00:19:56.600 --> 00:19:57.800]   I mean, I'm kind of serious.
[00:19:57.800 --> 00:19:58.300]   Yeah.
[00:19:58.300 --> 00:20:01.280]   It's like, well, this is a bunch of--
[00:20:01.280 --> 00:20:02.880]   it's kind of like SRE.
[00:20:02.880 --> 00:20:04.840]   SRE emerged when software engineers looked
[00:20:04.840 --> 00:20:07.520]   at what the sysadmins were doing and said, OK,
[00:20:07.520 --> 00:20:10.560]   we're going to automate all this stuff with software.
[00:20:10.560 --> 00:20:12.760]   And that's kind of how I feel about AutoML right now.
[00:20:12.760 --> 00:20:14.920]   AutoML is like, let's go automate
[00:20:14.920 --> 00:20:17.720]   all of this exploratory stuff and just brute force
[00:20:17.720 --> 00:20:19.040]   the hell out of it and compute.
[00:20:19.040 --> 00:20:20.560]   And that's kind of what we get.
[00:20:20.560 --> 00:20:24.560]   And that, I mean, I think mixed opinions on AutoML here.
[00:20:24.560 --> 00:20:28.640]   Hamel, how do you feel about AutoML as a neutral observer?
[00:20:28.640 --> 00:20:29.440]   What do you think?
[00:20:29.440 --> 00:20:30.880]   I'm not that neutral on AutoML.
[00:20:30.880 --> 00:20:33.040]   Yeah, I mean, AutoML--
[00:20:33.040 --> 00:20:34.920]   yeah, I mean, I really like AutoML.
[00:20:34.920 --> 00:20:37.840]   I think it augments the data scientist really nicely
[00:20:37.840 --> 00:20:39.920]   if used properly.
[00:20:39.920 --> 00:20:43.080]   But I think people sometimes misunderstand AutoML.
[00:20:43.080 --> 00:20:45.080]   Yeah, it doesn't replace a data scientist, right?
[00:20:45.080 --> 00:20:48.120]   Because part of the point of the exploratory process
[00:20:48.120 --> 00:20:50.360]   is to say, what do I not have here?
[00:20:50.360 --> 00:20:51.960]   What information do I need to get
[00:20:51.960 --> 00:20:53.240]   which would augment this stuff?
[00:20:53.240 --> 00:20:57.320]   And AutoML can't do that because it doesn't know what it doesn't--
[00:20:57.320 --> 00:21:00.160]   what doesn't exist yet.
[00:21:00.160 --> 00:21:02.080]   But what about, is it just AutoML?
[00:21:02.080 --> 00:21:03.640]   Or is it like--
[00:21:03.640 --> 00:21:05.080]   OK, this is one--
[00:21:05.080 --> 00:21:07.240]   let me give you an example.
[00:21:07.240 --> 00:21:08.840]   I feel like software engineer--
[00:21:08.840 --> 00:21:12.000]   OK, I feel like--
[00:21:12.000 --> 00:21:15.320]   OK, I'm going to give a very specific example.
[00:21:15.320 --> 00:21:20.440]   I feel like Kubeflow was a software engineer saying--
[00:21:20.440 --> 00:21:23.360]   or I know that Kubeflow is a software engineering saying,
[00:21:23.360 --> 00:21:27.440]   hey, Kubernetes has one per software.
[00:21:27.440 --> 00:21:28.880]   We should just use ML with that.
[00:21:28.880 --> 00:21:35.120]   But I don't know if that was the best fit.
[00:21:35.120 --> 00:21:36.840]   Nope.
[00:21:36.840 --> 00:21:38.080]   Like, what was XML?
[00:21:38.080 --> 00:21:42.440]   XML was, hey, look, HTML has one for web pages.
[00:21:42.440 --> 00:21:45.880]   So let's use something HTML-like to do all data interchange
[00:21:45.880 --> 00:21:46.840]   formats forever.
[00:21:46.840 --> 00:21:47.720]   You know what I mean?
[00:21:47.720 --> 00:21:49.040]   It's the same kind of--
[00:21:49.040 --> 00:21:52.440]   I agree with you, Kubeflow is not the right solution
[00:21:52.440 --> 00:21:55.480]   because it's made from the same sort of bad reasoning
[00:21:55.480 --> 00:21:58.520]   by analogy or reasoning by hype cycle
[00:21:58.520 --> 00:22:00.440]   or whatever kind of nonsense.
[00:22:00.440 --> 00:22:04.000]   I guess I'm kind of more--
[00:22:04.000 --> 00:22:06.440]   I feel like I'm aligned with Shrey here, her more optimistic
[00:22:06.440 --> 00:22:10.080]   perspective, which is I think Kubeflow was a first attempt
[00:22:10.080 --> 00:22:11.360]   and it was bad.
[00:22:11.360 --> 00:22:14.480]   AutoML is another attempt and it is bad.
[00:22:14.480 --> 00:22:15.840]   And we will keep working, though.
[00:22:15.840 --> 00:22:17.120]   We're not done yet.
[00:22:17.120 --> 00:22:17.760]   We don't have to stop.
[00:22:17.760 --> 00:22:18.520]   We can keep going.
[00:22:18.520 --> 00:22:22.040]   We can keep at this until finally someone gets it right.
[00:22:22.040 --> 00:22:24.640]   And I guess I am optimistic that someone will.
[00:22:24.640 --> 00:22:26.800]   Here's one thing I'm super intrigued about.
[00:22:26.800 --> 00:22:28.800]   There isn't-- people talk about the need
[00:22:28.800 --> 00:22:31.920]   for opinionated workflows in data science and machine
[00:22:31.920 --> 00:22:32.640]   learning.
[00:22:32.640 --> 00:22:36.880]   I have yet to see an actual opinionated one.
[00:22:36.880 --> 00:22:39.200]   No, I'm legit really serious.
[00:22:39.200 --> 00:22:40.440]   OK.
[00:22:40.440 --> 00:22:41.600]   What about Kubeflow?
[00:22:41.600 --> 00:22:43.040]   It's like, you should use Kubernetes.
[00:22:43.040 --> 00:22:43.800]   That's an opinion.
[00:22:43.800 --> 00:22:44.300]   OK, sorry.
[00:22:44.300 --> 00:22:46.920]   An actual opinionated one based on principles
[00:22:46.920 --> 00:22:49.720]   that are not tied to the hype cycle, as you say.
[00:22:49.720 --> 00:22:51.040]   Gotcha.
[00:22:51.040 --> 00:22:51.680]   Gotcha.
[00:22:51.680 --> 00:22:52.400]   I see.
[00:22:52.400 --> 00:22:53.920]   That's a higher bar to clear.
[00:22:53.920 --> 00:22:54.480]   I like that.
[00:22:54.480 --> 00:22:56.200]   I should also say I have a lot of friends
[00:22:56.200 --> 00:22:58.240]   in the Kubeflow community.
[00:22:58.240 --> 00:22:59.760]   Don't want to throw shade on anybody,
[00:22:59.760 --> 00:23:02.560]   but it's just a good example of--
[00:23:02.560 --> 00:23:03.800]   I think it is a good example.
[00:23:03.800 --> 00:23:04.520]   I think it is.
[00:23:04.520 --> 00:23:07.920]   But I mean, I like that we're trying.
[00:23:07.920 --> 00:23:09.960]   We're not always going to succeed and that's OK.
[00:23:09.960 --> 00:23:11.960]   But we do have to keep trying.
[00:23:11.960 --> 00:23:13.000]   We don't have it yet.
[00:23:13.000 --> 00:23:15.400]   We need to keep at it, I think.
[00:23:15.400 --> 00:23:20.680]   So OK, why do you think that we have been bamboozled
[00:23:20.680 --> 00:23:23.200]   into thinking that the traditional software
[00:23:23.200 --> 00:23:25.440]   engineering workflow is good for ML?
[00:23:25.440 --> 00:23:28.560]   Outside this panel, I feel like everywhere I go,
[00:23:28.560 --> 00:23:31.360]   there's somebody always on a soapbox.
[00:23:31.360 --> 00:23:34.840]   And when I say everywhere I go, I mean at every company
[00:23:34.840 --> 00:23:36.920]   that I visit or I'm a part of, there's
[00:23:36.920 --> 00:23:41.880]   someone on a soapbox that is screaming very, very loudly
[00:23:41.880 --> 00:23:47.200]   that, hey, you idiots, use software engineering workflows
[00:23:47.200 --> 00:23:50.840]   if you're just a good software engineer.
[00:23:50.840 --> 00:23:55.760]   In fact, we're just going to throw software engineers
[00:23:55.760 --> 00:23:56.280]   on this.
[00:23:56.280 --> 00:23:58.080]   We're just going to hire software engineers
[00:23:58.080 --> 00:23:59.560]   and they're just going to learn ML.
[00:23:59.560 --> 00:24:06.160]   That's how strongly this view is projected.
[00:24:06.160 --> 00:24:12.040]   But why-- is it because we are, as a profession,
[00:24:12.040 --> 00:24:17.200]   maybe we are insecure about something or we're afraid?
[00:24:17.200 --> 00:24:19.440]   Or why do we let this get to us?
[00:24:19.440 --> 00:24:22.120]   I want to riff on that question for Shreya, I think.
[00:24:22.120 --> 00:24:25.120]   Which is, Shreya, in the paper, the panelists you interviewed,
[00:24:25.120 --> 00:24:27.360]   the people you interviewed, the participants, everyone
[00:24:27.360 --> 00:24:31.880]   is either an ML engineer or a ML engineering manager.
[00:24:31.880 --> 00:24:34.320]   And I'm curious, what were they really?
[00:24:34.320 --> 00:24:37.200]   Were they software engineers, like Hamill's saying,
[00:24:37.200 --> 00:24:38.680]   who picked up some ML?
[00:24:38.680 --> 00:24:42.560]   Or were they ML people who learned a lot of software?
[00:24:42.560 --> 00:24:44.000]   Who were these people?
[00:24:44.000 --> 00:24:44.520]   Both.
[00:24:44.520 --> 00:24:46.120]   Almost 50/50 split.
[00:24:46.120 --> 00:24:48.520]   And the MLE managers, I will say,
[00:24:48.520 --> 00:24:52.640]   they were MLEs before they were MLE managers.
[00:24:52.640 --> 00:24:55.800]   They were an MLE last year, so a lot of the anecdotes
[00:24:55.800 --> 00:24:59.280]   were about them being on calls.
[00:24:59.280 --> 00:25:02.040]   That's what we really focused on.
[00:25:02.040 --> 00:25:04.760]   But there was a nice mix of people
[00:25:04.760 --> 00:25:07.760]   from different data science or software engineering
[00:25:07.760 --> 00:25:08.840]   backgrounds.
[00:25:08.840 --> 00:25:12.960]   And I think when you're in that space where
[00:25:12.960 --> 00:25:17.280]   you have to do the model training and you be on call,
[00:25:17.280 --> 00:25:19.840]   you develop the nuanced perspectives.
[00:25:19.840 --> 00:25:21.920]   And you develop-- you can't--
[00:25:21.920 --> 00:25:24.480]   none of them are preaching the software engineering soapbox
[00:25:24.480 --> 00:25:25.320]   story because--
[00:25:25.320 --> 00:25:25.920]   Interesting.
[00:25:25.920 --> 00:25:26.920]   They can't.
[00:25:26.920 --> 00:25:30.480]   They-- I think they do this on a day-to-day.
[00:25:30.480 --> 00:25:33.520]   And they realize that you cannot just throw buzzwords
[00:25:33.520 --> 00:25:37.200]   at a problem and it solves it.
[00:25:37.200 --> 00:25:37.720]   So that's--
[00:25:37.720 --> 00:25:38.200]   I'm curious--
[00:25:38.200 --> 00:25:41.400]   So I was deeply curious if in your data, yeah,
[00:25:41.400 --> 00:25:41.880]   there was--
[00:25:41.880 --> 00:25:44.160]   do these people, despite their different backgrounds,
[00:25:44.160 --> 00:25:46.240]   they converge to the same set of opinions,
[00:25:46.240 --> 00:25:47.440]   which is really interesting.
[00:25:47.440 --> 00:25:50.400]   Well, different opinions, but same--
[00:25:50.400 --> 00:25:52.480]   well, different opinions on different things,
[00:25:52.480 --> 00:25:55.200]   like Jupyter Notebooks or whatnot.
[00:25:55.200 --> 00:25:56.960]   So I want to go into that.
[00:25:56.960 --> 00:25:59.200]   So do the software engineers who do model training,
[00:25:59.200 --> 00:26:02.640]   do they like Jupyter Notebooks or not?
[00:26:02.640 --> 00:26:05.720]   I mean, again--
[00:26:05.720 --> 00:26:08.280]   I don't think we sliced by that.
[00:26:08.280 --> 00:26:09.280]   So I can't--
[00:26:09.280 --> 00:26:10.320]   Can you do it real quick?
[00:26:10.320 --> 00:26:11.160]   Can you just do it?
[00:26:11.160 --> 00:26:12.400]   Hey, I got a quick question.
[00:26:12.400 --> 00:26:13.880]   Can you just--
[00:26:13.880 --> 00:26:14.720]   Oh.
[00:26:14.720 --> 00:26:18.000]   The software takes five minutes to load on my M1 Mac,
[00:26:18.000 --> 00:26:19.920]   the social sciences software.
[00:26:19.920 --> 00:26:21.520]   Yeah, that's--
[00:26:21.520 --> 00:26:23.480]   We need to apply software engineering principles
[00:26:23.480 --> 00:26:24.320]   to social sciences.
[00:26:24.320 --> 00:26:27.920]   Oh, god.
[00:26:27.920 --> 00:26:32.520]   OK, the one really strong quote that I don't like software--
[00:26:32.520 --> 00:26:34.760]   and I don't like Jupyter Notebook quotes
[00:26:34.760 --> 00:26:39.040]   were-- this came in the context of I
[00:26:39.040 --> 00:26:43.000]   had to reproduce somebody else's Jupyter Notebook,
[00:26:43.000 --> 00:26:45.160]   and it was the worst experience of my life,
[00:26:45.160 --> 00:26:46.640]   and I had to do this three times,
[00:26:46.640 --> 00:26:48.920]   and I hate Jupyter Notebooks.
[00:26:48.920 --> 00:26:52.640]   All of the quotes were kind of from a story like that.
[00:26:52.640 --> 00:26:53.640]   100% makes sense.
[00:26:53.640 --> 00:26:54.960]   Yes, totally.
[00:26:54.960 --> 00:26:57.880]   And I care so much--
[00:26:57.880 --> 00:27:01.040]   I care so much now that I'm so happy my team is never
[00:27:01.040 --> 00:27:04.760]   using Jupyter Notebooks again, things like that.
[00:27:04.760 --> 00:27:07.320]   I think most of them were software engineers,
[00:27:07.320 --> 00:27:10.720]   but honestly, I don't even know, because everyone is training
[00:27:10.720 --> 00:27:12.080]   models in some fashion.
[00:27:12.080 --> 00:27:14.080]   Everyone is deploying models in some fashion.
[00:27:14.080 --> 00:27:16.080]   So it's like--
[00:27:16.080 --> 00:27:18.800]   I don't think people really identify--
[00:27:18.800 --> 00:27:21.480]   at this point, when you're really years deep
[00:27:21.480 --> 00:27:23.680]   into ML engineering, you cannot really identify
[00:27:23.680 --> 00:27:25.080]   with one or the other.
[00:27:25.080 --> 00:27:27.680]   I mean, if you do, you shouldn't be in ML engineering anymore,
[00:27:27.680 --> 00:27:28.600]   right?
[00:27:28.600 --> 00:27:30.280]   Yeah, you should go to the other thing.
[00:27:30.280 --> 00:27:30.760]   So there's--
[00:27:30.760 --> 00:27:32.360]   [INTERPOSING VOICES]
[00:27:32.360 --> 00:27:35.280]   Yeah, I'm always puzzled by that reaction to notebooks,
[00:27:35.280 --> 00:27:40.040]   like using it as a scapegoat of a really bad process, which
[00:27:40.040 --> 00:27:42.240]   I don't really--
[00:27:42.240 --> 00:27:45.120]   someone can't reproduce someone else's notebook.
[00:27:45.120 --> 00:27:47.000]   Well, why didn't that person restart and run all
[00:27:47.000 --> 00:27:49.680]   their notebook or do something sane?
[00:27:49.680 --> 00:27:52.280]   No, I mean, it's the thing here, which is that--
[00:27:52.280 --> 00:27:54.080]   and this is a thing, and this does happen.
[00:27:54.080 --> 00:27:56.560]   It's happened to me, which is like I'm--
[00:27:56.560 --> 00:28:00.120]   data scientist or whatever throws some garbage notebook
[00:28:00.120 --> 00:28:00.960]   over the wall to me.
[00:28:00.960 --> 00:28:02.080]   And it's like, here you go.
[00:28:02.080 --> 00:28:04.640]   Productionize this.
[00:28:04.640 --> 00:28:06.960]   I'm like, I don't know if I can curse in this thing,
[00:28:06.960 --> 00:28:09.320]   but did you just tell me to go F myself, basically?
[00:28:09.320 --> 00:28:10.960]   Is that what you just said?
[00:28:10.960 --> 00:28:13.600]   That's how it feels to me when someone does that.
[00:28:13.600 --> 00:28:14.120]   So I--
[00:28:14.120 --> 00:28:14.720]   Totally, yeah.
[00:28:14.720 --> 00:28:16.080]   Right?
[00:28:16.080 --> 00:28:17.800]   So I mean, is it a garbage process?
[00:28:17.800 --> 00:28:19.680]   Yes.
[00:28:19.680 --> 00:28:21.440]   But it's like the tangible manifestation
[00:28:21.440 --> 00:28:23.120]   of the garbage process is the notebook.
[00:28:23.120 --> 00:28:24.440]   And that's what people hate.
[00:28:24.440 --> 00:28:25.640]   Because you can't hate the person,
[00:28:25.640 --> 00:28:27.340]   but you can definitely hate the notebook.
[00:28:27.340 --> 00:28:28.200]   Yeah.
[00:28:28.200 --> 00:28:29.440]   I think it's also interesting.
[00:28:29.440 --> 00:28:33.040]   I can't say I hate notebooks because I use notebooks.
[00:28:33.040 --> 00:28:34.640]   Like if I need to do analysis, if I
[00:28:34.640 --> 00:28:37.840]   need to do something really quick and prototype something,
[00:28:37.840 --> 00:28:39.400]   I'm going to use a notebook.
[00:28:39.400 --> 00:28:40.800]   Sure.
[00:28:40.800 --> 00:28:44.000]   Like I put notebooks on GitHub repos all the time,
[00:28:44.000 --> 00:28:44.880]   send them to people.
[00:28:44.880 --> 00:28:48.480]   Because it's like I have a hex account,
[00:28:48.480 --> 00:28:51.680]   and there's like 50 people from UC Berkeley
[00:28:51.680 --> 00:28:56.160]   that have been-- preached the gospel for me
[00:28:56.160 --> 00:28:57.920]   and my collaborators.
[00:28:57.920 --> 00:28:58.600]   But it's there.
[00:28:58.600 --> 00:29:02.800]   Like you want-- it's good for a certain kind of work.
[00:29:02.800 --> 00:29:05.440]   I think the problem is when you try to move it
[00:29:05.440 --> 00:29:07.400]   to other kind of work.
[00:29:07.400 --> 00:29:11.760]   I mean, when I read that quote about the Jupyter notebook,
[00:29:11.760 --> 00:29:13.840]   I mean, maybe I should just read it real quick.
[00:29:13.840 --> 00:29:14.840]   Please.
[00:29:14.840 --> 00:29:15.640]   Let me find it.
[00:29:15.640 --> 00:29:20.280]   OK, so it says, "There were all sorts of manual issues.
[00:29:20.280 --> 00:29:23.560]   Someone would run something with the wrong sort of inputs
[00:29:23.560 --> 00:29:24.640]   from the notebook.
[00:29:24.640 --> 00:29:27.240]   And I'm debugging for like a day and a half.
[00:29:27.240 --> 00:29:30.440]   Then I'd figure out this was all garbage.
[00:29:30.440 --> 00:29:32.920]   Eight months ago, we realized this is not working.
[00:29:32.920 --> 00:29:34.760]   And we needed to put the engineering effort
[00:29:34.760 --> 00:29:37.160]   to create non-notebook pipelines."
[00:29:37.160 --> 00:29:40.840]   And then somewhere else it says like, people don't--
[00:29:40.840 --> 00:29:43.200]   there's some people don't even allow notebooks.
[00:29:43.200 --> 00:29:45.760]   So when I read that people--
[00:29:45.760 --> 00:29:48.120]   there's like organizations that don't allow notebooks,
[00:29:48.120 --> 00:29:49.720]   that seems very hostile.
[00:29:49.720 --> 00:29:51.920]   That seems like the software engineers have completely
[00:29:51.920 --> 00:29:53.160]   taken over.
[00:29:53.160 --> 00:29:59.760]   And I don't even understand how you could iterate properly
[00:29:59.760 --> 00:30:01.040]   in that environment.
[00:30:01.040 --> 00:30:02.720]   Right, it's like, it's--
[00:30:02.720 --> 00:30:04.200]   those places, it's not like they don't really have--
[00:30:04.200 --> 00:30:05.400]   they totally still have notebooks.
[00:30:05.400 --> 00:30:07.240]   It's just not like you're not advertising the fact
[00:30:07.240 --> 00:30:08.200]   that you have notebooks.
[00:30:08.200 --> 00:30:09.920]   A notebook is not an acceptable way
[00:30:09.920 --> 00:30:11.800]   to write a spec, for instance.
[00:30:11.800 --> 00:30:13.200]   But it is-- but of course they still exist.
[00:30:13.200 --> 00:30:14.000]   Of course they do.
[00:30:14.000 --> 00:30:17.200]   Like you can't commit a notebook as part of a PR
[00:30:17.200 --> 00:30:18.440]   at a lot of organizations.
[00:30:18.440 --> 00:30:20.280]   Like, yeah, you can put it on your computer.
[00:30:20.280 --> 00:30:23.160]   And they have some reserved instances maybe for notebooks.
[00:30:23.160 --> 00:30:25.160]   But it's not like--
[00:30:25.160 --> 00:30:29.720]   your job, you're not evaluated on your PRs.
[00:30:29.720 --> 00:30:31.120]   You can't have notebooks in there.
[00:30:31.120 --> 00:30:33.560]   I think that's what people say, that we don't have notebooks
[00:30:33.560 --> 00:30:36.800]   in our organization.
[00:30:36.800 --> 00:30:38.680]   Yeah, that really burns me, by the way.
[00:30:38.680 --> 00:30:41.080]   But we don't have to--
[00:30:41.080 --> 00:30:42.000]   I'm super OK with it.
[00:30:42.000 --> 00:30:42.920]   And it makes me happy.
[00:30:42.920 --> 00:30:45.120]   So sorry, I'm just up--
[00:30:45.120 --> 00:30:46.560]   yeah.
[00:30:46.560 --> 00:30:47.560]   Notebooks are mean.
[00:30:47.560 --> 00:30:49.160]   X is great, but notebooks are mean.
[00:30:49.160 --> 00:30:49.720]   So anyway.
[00:30:49.720 --> 00:30:52.520]   Thank you.
[00:30:52.520 --> 00:30:55.080]   Let's talk about-- can we talk about YAML a little bit?
[00:30:55.080 --> 00:30:55.800]   Yeah.
[00:30:55.800 --> 00:30:57.400]   Let's talk about YAML.
[00:30:57.400 --> 00:30:59.680]   I'm curious about--
[00:30:59.680 --> 00:31:04.200]   I think it was interesting how much of--
[00:31:04.200 --> 00:31:06.720]   I think I've heard it joked for a while
[00:31:06.720 --> 00:31:11.440]   that ML engineering is really like YAML engineering.
[00:31:11.440 --> 00:31:14.480]   You code in YAML.
[00:31:14.480 --> 00:31:15.720]   And I guess I was--
[00:31:15.720 --> 00:31:17.920]   I was happy to see that.
[00:31:17.920 --> 00:31:19.840]   People dunk on YAML a lot, I think.
[00:31:19.840 --> 00:31:21.600]   And yet it was cool to see so many people like, yeah,
[00:31:21.600 --> 00:31:24.140]   no, we pretty much just do everything in YAML, more or less.
[00:31:24.140 --> 00:31:26.280]   Like, it's all-- like, anyway, the pipeline work
[00:31:26.280 --> 00:31:26.920]   is done and stuff.
[00:31:26.920 --> 00:31:29.000]   And YAML is our primary development environment,
[00:31:29.000 --> 00:31:30.400]   which I just kind of loved.
[00:31:30.400 --> 00:31:31.080]   Yeah.
[00:31:31.080 --> 00:31:35.200]   I was surprised to see so much love for YAML, because I--
[00:31:35.200 --> 00:31:40.400]   there's few things in this world I hate more than YAML all
[00:31:40.400 --> 00:31:40.960]   the time.
[00:31:40.960 --> 00:31:44.560]   I find it very difficult. First of all,
[00:31:44.560 --> 00:31:47.280]   when I'm doing this YAML development,
[00:31:47.280 --> 00:31:49.760]   I always have trouble figuring out what the spec is.
[00:31:49.760 --> 00:31:51.600]   It's like, look at some documentation.
[00:31:51.600 --> 00:31:53.360]   Does this thing go under this key?
[00:31:53.360 --> 00:31:56.120]   What are the allowed values here?
[00:31:56.120 --> 00:31:57.520]   Like, for whatever reason, I don't
[00:31:57.520 --> 00:31:59.680]   feel like I have a good development environment.
[00:31:59.680 --> 00:32:02.680]   Or there doesn't exist one where it tells you
[00:32:02.680 --> 00:32:05.360]   what all the possibilities are for the YAML you're writing.
[00:32:05.360 --> 00:32:07.080]   I don't know why that doesn't exist.
[00:32:07.080 --> 00:32:09.920]   Someone should make that.
[00:32:09.920 --> 00:32:12.600]   And then, yeah, and then it's very--
[00:32:12.600 --> 00:32:14.480]   it's also associated a lot of times
[00:32:14.480 --> 00:32:17.880]   with very long iteration cycles.
[00:32:17.880 --> 00:32:22.120]   Like, change some YAML, wait 10 minutes to see what happens.
[00:32:22.120 --> 00:32:24.280]   And then I have no idea what is--
[00:32:24.280 --> 00:32:27.200]   like, if it's something errors, like, I don't know,
[00:32:27.200 --> 00:32:28.920]   is it the YAML, maybe?
[00:32:28.920 --> 00:32:30.960]   Like, I don't know.
[00:32:30.960 --> 00:32:34.880]   So Shreya, what makes for a successful YAML workflow?
[00:32:34.880 --> 00:32:37.240]   Like, when these people use YAML and it makes them happy,
[00:32:37.240 --> 00:32:38.960]   how-- what about it makes them happy?
[00:32:38.960 --> 00:32:40.200]   What do they like about it?
[00:32:40.200 --> 00:32:41.560]   How does it work?
[00:32:41.560 --> 00:32:43.960]   The positive anecdotes came from,
[00:32:43.960 --> 00:32:46.840]   we have-- we make all changes in YAML,
[00:32:46.840 --> 00:32:50.320]   so the space of possible bugs is much smaller.
[00:32:50.320 --> 00:32:54.040]   Like, if you can only make five-character change,
[00:32:54.040 --> 00:32:56.440]   there's not so much that you can do, right?
[00:32:56.440 --> 00:33:00.320]   Like, a lot of people--
[00:33:00.320 --> 00:33:02.840]   a lot of those same people said, like, beforehand, people
[00:33:02.840 --> 00:33:08.240]   will, like, fork the repo and then run something.
[00:33:08.240 --> 00:33:10.840]   You don't-- like, when you have-- when you're maintaining
[00:33:10.840 --> 00:33:14.680]   that much of a diff, right, like, it's easy for discrepancies
[00:33:14.680 --> 00:33:15.520]   to come up, right?
[00:33:15.520 --> 00:33:18.280]   It's not synced with the main branch, all sorts of stuff.
[00:33:18.280 --> 00:33:20.520]   So I think that's where, like, the love for--
[00:33:20.520 --> 00:33:24.120]   not particularly YAML, let's say, but--
[00:33:24.120 --> 00:33:25.640]   I think one person said YAML, but, like,
[00:33:25.640 --> 00:33:27.880]   the idea of constraining your changes,
[00:33:27.880 --> 00:33:29.560]   making them really small, at least that's
[00:33:29.560 --> 00:33:32.520]   what we said in the paper.
[00:33:32.520 --> 00:33:33.400]   That was really nice.
[00:33:33.400 --> 00:33:36.440]   Also, another nice side effect was now
[00:33:36.440 --> 00:33:40.160]   you can trust your collaborators' work a lot more,
[00:33:40.160 --> 00:33:44.080]   because you know they also made small changes.
[00:33:44.080 --> 00:33:47.440]   So I think it's a very team-based kind of attitude
[00:33:47.440 --> 00:33:47.960]   there.
[00:33:47.960 --> 00:33:51.440]   I don't know if anyone said they love YAML, per se, but--
[00:33:51.440 --> 00:33:54.800]   They love that workflow, which is very software engineering-y,
[00:33:54.800 --> 00:33:56.080]   if I may say so myself.
[00:33:56.080 --> 00:33:58.240]   Like, that sounds like a good--
[00:33:58.240 --> 00:34:00.720]   in any language, a good PR is a tiny PR.
[00:34:00.720 --> 00:34:02.400]   Yeah, a tiny PR.
[00:34:02.400 --> 00:34:05.360]   And the other thing is, like, a lot of these people
[00:34:05.360 --> 00:34:10.000]   also talk about paranoia when debugging and verifying.
[00:34:10.000 --> 00:34:13.760]   Not just debugging, but verifying other new PRs
[00:34:13.760 --> 00:34:15.360]   or referring new experiments.
[00:34:15.360 --> 00:34:16.760]   Like, there's the paranoia.
[00:34:16.760 --> 00:34:19.200]   Like, can we make the code change as small as possible?
[00:34:19.200 --> 00:34:23.720]   Can we make the data diffs super transparent?
[00:34:23.720 --> 00:34:26.200]   Then I can exhaustively enumerate
[00:34:26.200 --> 00:34:29.560]   through all the possible bugs that I have in my mind.
[00:34:29.560 --> 00:34:32.080]   I think that's kind of where people are trying to get
[00:34:32.080 --> 00:34:33.720]   with these config layers.
[00:34:33.720 --> 00:34:34.400]   I love that.
[00:34:34.400 --> 00:34:35.280]   I love that.
[00:34:35.280 --> 00:34:35.880]   I love that.
[00:34:35.880 --> 00:34:37.880]   I guess I also kind of was wondering, like,
[00:34:37.880 --> 00:34:40.080]   you know, Shrey, we talk about there's sort of, like,
[00:34:40.080 --> 00:34:41.520]   platform ML engineering.
[00:34:41.520 --> 00:34:44.680]   And then there's, like, specific task-level ML engineering.
[00:34:44.680 --> 00:34:48.240]   What I read from that paper with sort of the config-driven stuff
[00:34:48.240 --> 00:34:50.920]   was, like, people creating kind of their own little domain-
[00:34:50.920 --> 00:34:55.200]   specific platforms for their area, basically.
[00:34:55.200 --> 00:34:56.960]   Is that-- did that speak to you?
[00:34:56.960 --> 00:34:58.560]   Is that right?
[00:34:58.560 --> 00:34:59.120]   Totally.
[00:34:59.120 --> 00:35:00.560]   I think this doesn't scale, though,
[00:35:00.560 --> 00:35:03.360]   whenever you're, like, at a Google size or a Facebook size,
[00:35:03.360 --> 00:35:05.720]   because every task is, like, a different org.
[00:35:05.720 --> 00:35:08.160]   So then you have, like, a platform.
[00:35:08.160 --> 00:35:09.880]   Or not every task is different.
[00:35:09.880 --> 00:35:11.720]   But, like, Google--
[00:35:11.720 --> 00:35:14.280]   an organization like Google will have multiple orgs within it,
[00:35:14.280 --> 00:35:14.800]   right?
[00:35:14.800 --> 00:35:17.760]   And they will all have their own different platforms.
[00:35:17.760 --> 00:35:18.520]   Yes.
[00:35:18.520 --> 00:35:20.880]   So, like, what is the level of granularity, I guess,
[00:35:20.880 --> 00:35:22.840]   is the--
[00:35:22.840 --> 00:35:24.400]   I guess, for me, it speaks to the fact
[00:35:24.400 --> 00:35:27.680]   that, like, being a task- or, like, domain-oriented ML
[00:35:27.680 --> 00:35:29.760]   person is a terrible job.
[00:35:29.760 --> 00:35:31.320]   And it's deeply unfun.
[00:35:31.320 --> 00:35:34.640]   And, like, building platforms, though, super fun.
[00:35:34.640 --> 00:35:36.280]   Like, love the platforms, right?
[00:35:36.280 --> 00:35:37.720]   And so it's kind of like--
[00:35:37.720 --> 00:35:38.680]   like, it's this human--
[00:35:38.680 --> 00:35:39.400]   [INAUDIBLE]
[00:35:39.400 --> 00:35:41.440]   So, like, that's your opinion.
[00:35:41.440 --> 00:35:43.840]   Other people would much rather train models.
[00:35:43.840 --> 00:35:44.800]   And then they will be--
[00:35:44.800 --> 00:35:45.760]   I was trying-- I know.
[00:35:45.760 --> 00:35:46.680]   I mean, it's a panel, right?
[00:35:46.680 --> 00:35:48.680]   I'm trying to, like, inflict my opinions on you.
[00:35:48.680 --> 00:35:49.200]   Oops.
[00:35:49.200 --> 00:35:49.920]   It's-- anyway.
[00:35:49.920 --> 00:35:50.280]   OK.
[00:35:50.280 --> 00:35:50.920]   Yeah, all right.
[00:35:50.920 --> 00:35:51.560]   You caught me.
[00:35:51.560 --> 00:35:52.280]   Fair.
[00:35:52.280 --> 00:35:52.720]   Fair enough.
[00:35:52.720 --> 00:35:53.160]   All right.
[00:35:53.160 --> 00:35:53.680]   Granted.
[00:35:53.680 --> 00:35:55.960]   I think that brings us to the next question, is, like--
[00:35:55.960 --> 00:35:59.760]   so, like, the paper focuses a lot around, like,
[00:35:59.760 --> 00:36:01.240]   broken processes.
[00:36:01.240 --> 00:36:03.240]   Nothing to do with, like, tools or anything, like,
[00:36:03.240 --> 00:36:06.600]   processes that are, like, not correct.
[00:36:06.600 --> 00:36:09.040]   You know, and just in terms of, like, standardizing things,
[00:36:09.040 --> 00:36:13.600]   communicating properly, documenting things.
[00:36:13.600 --> 00:36:16.760]   And, you know, it's--
[00:36:16.760 --> 00:36:19.600]   so it does bring up the question, like,
[00:36:19.600 --> 00:36:22.720]   it seems like we don't-- we, like, really focus on tools
[00:36:22.720 --> 00:36:28.240]   a lot with MLOps, almost to the extent where it feels like we
[00:36:28.240 --> 00:36:31.160]   are trying to fix broken processes with tools.
[00:36:31.160 --> 00:36:31.660]   Yes.
[00:36:31.660 --> 00:36:35.960]   And also to Josh's point earlier, so he mentioned--
[00:36:35.960 --> 00:36:38.360]   like, Josh mentioned that, OK, for him,
[00:36:38.360 --> 00:36:41.920]   it's more fun a lot of times to build platforms
[00:36:41.920 --> 00:36:43.280]   than train models.
[00:36:43.280 --> 00:36:45.120]   I think there's also, like, some incentive--
[00:36:45.120 --> 00:36:46.640]   there's a lot of incentives, like--
[00:36:46.640 --> 00:36:48.360]   at least I'll talk about some incentive,
[00:36:48.360 --> 00:36:49.640]   like, inside tech companies.
[00:36:49.640 --> 00:36:53.720]   I feel like the engineering class is the highest class.
[00:36:53.720 --> 00:36:56.840]   And, like, data science, machine learning sometimes
[00:36:56.840 --> 00:36:59.240]   is, like, a service function.
[00:36:59.240 --> 00:37:00.780]   And, like, if you can--
[00:37:00.780 --> 00:37:02.780]   the way you can, as a machine learning person,
[00:37:02.780 --> 00:37:04.780]   like, kind of get into the engineering class
[00:37:04.780 --> 00:37:07.700]   is to create tools, which has, like, a lot--
[00:37:07.700 --> 00:37:10.180]   it pulls you in into that.
[00:37:10.180 --> 00:37:14.660]   Like, the incentives, like, they strongly pull you there.
[00:37:14.660 --> 00:37:16.980]   And then, yeah, so--
[00:37:16.980 --> 00:37:20.620]   but then also, like, you know, I feel
[00:37:20.620 --> 00:37:22.460]   like we focus a lot on tools.
[00:37:22.460 --> 00:37:25.820]   And a lot of people gravitate towards making tools.
[00:37:28.660 --> 00:37:30.700]   You know, and there's this tool explosion.
[00:37:30.700 --> 00:37:34.140]   So just want to have you all react to that a bit.
[00:37:34.140 --> 00:37:35.900]   Do you want me to give my honest thought?
[00:37:35.900 --> 00:37:36.400]   Like--
[00:37:36.400 --> 00:37:37.100]   Yeah, yeah, yes.
[00:37:37.100 --> 00:37:38.020]   100%.
[00:37:38.020 --> 00:37:40.060]   Yeah, like, market incentives.
[00:37:40.060 --> 00:37:42.220]   It's much more lucrative for somebody
[00:37:42.220 --> 00:37:44.780]   to go start a company on a platform
[00:37:44.780 --> 00:37:47.940]   than to go do ML engineering at a company.
[00:37:47.940 --> 00:37:49.700]   Like, that's--
[00:37:49.700 --> 00:37:50.260]   Much easier--
[00:37:50.260 --> 00:37:53.020]   That's how you get tool explosions, so--
[00:37:53.020 --> 00:37:55.580]   Much easier to get promoted at the Googles and Facebooks
[00:37:55.580 --> 00:37:58.540]   of the world, to develop a tool that is used by lots of people,
[00:37:58.540 --> 00:38:00.580]   gets lots and lots of leverage, without a doubt.
[00:38:00.580 --> 00:38:03.500]   And it's a perverse incentive.
[00:38:03.500 --> 00:38:06.700]   And it drives people way, way off
[00:38:06.700 --> 00:38:08.980]   into the deep end of complexity for--
[00:38:08.980 --> 00:38:09.620]   I don't know.
[00:38:09.620 --> 00:38:10.900]   I call-- I guess, I don't know.
[00:38:10.900 --> 00:38:11.620]   I call it, like--
[00:38:11.620 --> 00:38:14.460]   in tweets, I prefer to this as, like, technical pyrotechnics,
[00:38:14.460 --> 00:38:15.340]   or whatever.
[00:38:15.340 --> 00:38:18.860]   It's like, you're doing things that are absurdly complicated
[00:38:18.860 --> 00:38:20.420]   for, honestly, no reason.
[00:38:20.420 --> 00:38:22.380]   You know, just-- I mean, you're doing it for a reason.
[00:38:22.380 --> 00:38:23.180]   You're doing it to get promoted.
[00:38:23.180 --> 00:38:24.380]   You're doing it for money.
[00:38:24.380 --> 00:38:26.580]   But it inflicts, like, a terrible cost.
[00:38:26.580 --> 00:38:27.860]   Like, why does--
[00:38:27.860 --> 00:38:29.780]   I mean, Facebook and Google have to have, like,
[00:38:29.780 --> 00:38:31.060]   100,000 people working there.
[00:38:31.060 --> 00:38:34.420]   It's because of all this stupid shit they've built, basically.
[00:38:34.420 --> 00:38:37.140]   Anyway, yeah.
[00:38:37.140 --> 00:38:40.660]   I mean, like, kind of without a doubt, without a question.
[00:38:40.660 --> 00:38:41.180]   Yeah.
[00:38:41.180 --> 00:38:43.500]   I would say, not for nothing, it is not always the case
[00:38:43.500 --> 00:38:46.860]   that engineers are the end-all, be-all at any company.
[00:38:46.860 --> 00:38:50.100]   It just happens to be true at a lot of tech companies.
[00:38:50.100 --> 00:38:53.260]   Slack, by far, design was by far the highest status
[00:38:53.260 --> 00:38:55.340]   profession at Slack.
[00:38:55.340 --> 00:38:57.620]   Apple-- I don't really know what it is at Apple,
[00:38:57.620 --> 00:38:58.060]   to be honest with you.
[00:38:58.060 --> 00:38:58.740]   I'm kind of curious.
[00:38:58.740 --> 00:38:59.260]   But anyway--
[00:38:59.260 --> 00:38:59.940]   By design.
[00:38:59.940 --> 00:39:00.780]   That is the design.
[00:39:00.780 --> 00:39:01.500]   Is it design?
[00:39:01.500 --> 00:39:01.940]   Is it design?
[00:39:01.940 --> 00:39:02.420]   There's--
[00:39:02.420 --> 00:39:02.900]   No, no, no.
[00:39:02.900 --> 00:39:04.500]   By design, we're not supposed to know.
[00:39:04.500 --> 00:39:05.660]   Sorry.
[00:39:05.660 --> 00:39:07.260]   Oh, why was design?
[00:39:07.260 --> 00:39:10.020]   No, by design, we're not supposed to know.
[00:39:10.020 --> 00:39:12.220]   Gotcha.
[00:39:12.220 --> 00:39:12.940]   Yeah, OK.
[00:39:12.940 --> 00:39:13.700]   It's a good point.
[00:39:13.700 --> 00:39:16.540]   So I guess it's correct that I don't know what it is at Apple.
[00:39:16.540 --> 00:39:18.860]   But that's OK.
[00:39:18.860 --> 00:39:20.140]   Anyway, so it's not--
[00:39:20.140 --> 00:39:21.460]   it is that way at a lot of companies.
[00:39:21.460 --> 00:39:22.700]   But it's not universally true.
[00:39:22.700 --> 00:39:24.660]   But certainly, anyway.
[00:39:24.660 --> 00:39:26.340]   No, no.
[00:39:26.340 --> 00:39:26.940]   Yeah.
[00:39:26.940 --> 00:39:29.620]   So are you all going to create tools companies?
[00:39:29.620 --> 00:39:32.540]   Ah, damn it.
[00:39:32.540 --> 00:39:33.980]   I'm trying not to.
[00:39:33.980 --> 00:39:35.180]   I really am doing my best.
[00:39:35.180 --> 00:39:35.700]   I really am.
[00:39:35.700 --> 00:39:36.580]   I'm really trying to--
[00:39:36.580 --> 00:39:37.780]   What the market?
[00:39:37.780 --> 00:39:39.580]   Look at the market right now.
[00:39:39.580 --> 00:39:40.100]   Oh, god.
[00:39:40.100 --> 00:39:41.100]   Wait, what is that?
[00:39:41.100 --> 00:39:42.300]   Is it--
[00:39:42.300 --> 00:39:43.020]   Huh?
[00:39:43.020 --> 00:39:45.180]   You could raise $60 million tomorrow
[00:39:45.180 --> 00:39:47.100]   if you started a company, probably.
[00:39:47.100 --> 00:39:49.500]   I have actually been in conversations with VCs.
[00:39:49.500 --> 00:39:50.980]   They ask me what I'm working on.
[00:39:50.980 --> 00:39:52.660]   And I throw off a few sentences.
[00:39:52.660 --> 00:39:54.140]   And they're like, sounds great.
[00:39:54.140 --> 00:39:55.980]   [LAUGHTER]
[00:39:55.980 --> 00:39:57.540]   420, let's do it.
[00:39:57.540 --> 00:39:59.740]   I'm like, what the hell's the matter with you people?
[00:39:59.740 --> 00:40:02.420]   Anyway, I'm really trying not to.
[00:40:02.420 --> 00:40:04.460]   I'm trying not to become a venture capitalist.
[00:40:04.460 --> 00:40:06.860]   I'm trying not to start yet another data tools company.
[00:40:06.860 --> 00:40:07.820]   I'm doing my best.
[00:40:07.820 --> 00:40:08.500]   I really am.
[00:40:08.500 --> 00:40:11.100]   Yeah, it's yet another data--
[00:40:11.100 --> 00:40:11.780]   Exactly.
[00:40:11.780 --> 00:40:13.460]   Yet another data tools company.
[00:40:13.460 --> 00:40:14.700]   If I start one, that's what I'll call it.
[00:40:14.700 --> 00:40:15.340]   How about that?
[00:40:15.340 --> 00:40:16.660]   Yeah, that's great.
[00:40:16.660 --> 00:40:18.500]   I'll invest.
[00:40:18.500 --> 00:40:20.180]   I appreciate that, I think.
[00:40:20.180 --> 00:40:21.820]   [LAUGHTER]
[00:40:21.820 --> 00:40:23.060]   Trey, but what about you?
[00:40:23.060 --> 00:40:25.180]   What company are you starting that I can invest in?
[00:40:25.180 --> 00:40:27.940]   [LAUGHTER]
[00:40:27.940 --> 00:40:31.100]   No, I haven't thought--
[00:40:31.100 --> 00:40:34.660]   I don't have a great idea right now
[00:40:34.660 --> 00:40:37.380]   that I think is actually a long term.
[00:40:37.380 --> 00:40:39.180]   If I do, down to start.
[00:40:39.180 --> 00:40:41.060]   But I think it's hard.
[00:40:41.060 --> 00:40:43.700]   Do you have to find something that's
[00:40:43.700 --> 00:40:47.500]   a good idea that actually will help the situation that
[00:40:47.500 --> 00:40:49.420]   will last and is independent of the hype cycle?
[00:40:49.420 --> 00:40:51.980]   I think that's an incredibly hard thing to do.
[00:40:51.980 --> 00:40:53.220]   I completely agree.
[00:40:53.220 --> 00:40:53.940]   Completely.
[00:40:53.940 --> 00:40:54.580]   Totally.
[00:40:54.580 --> 00:40:56.900]   But to that end, I want to ask about something
[00:40:56.900 --> 00:40:58.700]   that I am passionate about, which
[00:40:58.700 --> 00:41:01.060]   you mentioned in your paper, which is the industry
[00:41:01.060 --> 00:41:05.060]   classroom mismatch, which I absolutely loved
[00:41:05.060 --> 00:41:07.420]   and very much spoke to me based on my experiences
[00:41:07.420 --> 00:41:08.620]   and stuff like that.
[00:41:08.620 --> 00:41:12.300]   What we are teaching kids about machine learning in school
[00:41:12.300 --> 00:41:14.340]   is not remotely reflective of reality
[00:41:14.340 --> 00:41:16.860]   and how jarring that experience is for people to come out.
[00:41:16.860 --> 00:41:18.860]   And so I was curious if you have thoughts,
[00:41:18.860 --> 00:41:20.500]   what should we be doing about this?
[00:41:20.500 --> 00:41:22.460]   What is the way forward here?
[00:41:22.460 --> 00:41:24.620]   My thoughts have actually flipped a little bit
[00:41:24.620 --> 00:41:26.060]   since I wrote that paper.
[00:41:26.060 --> 00:41:28.380]   When I wrote that paper, I thought, oh my god,
[00:41:28.380 --> 00:41:31.780]   the classes are not teaching enough airflow and not
[00:41:31.780 --> 00:41:33.220]   enough DVT.
[00:41:33.220 --> 00:41:35.500]   And we should be empowering people
[00:41:35.500 --> 00:41:38.980]   with real world technologies and tools.
[00:41:38.980 --> 00:41:40.860]   And to that extent, a lot of colleges
[00:41:40.860 --> 00:41:45.700]   have started sub-programs like data science within CS
[00:41:45.700 --> 00:41:49.020]   or within broader computing, like data science
[00:41:49.020 --> 00:41:51.900]   or information sciences.
[00:41:51.900 --> 00:41:54.540]   I don't know, like various names.
[00:41:54.540 --> 00:41:56.980]   Yeah, and so I'll give you an anecdote.
[00:41:56.980 --> 00:42:00.660]   I'm TAing this version of databases
[00:42:00.660 --> 00:42:06.340]   but for the data science major, like a different version
[00:42:06.340 --> 00:42:09.340]   of databases, where the idea is we
[00:42:09.340 --> 00:42:13.580]   want to be more practical, more applied in the real world.
[00:42:13.580 --> 00:42:16.060]   And I was like, oh man, it's going to be 10 weeks of SQL,
[00:42:16.060 --> 00:42:18.780]   and they're going to come out of it knowing how to do every join
[00:42:18.780 --> 00:42:19.460]   and every window.
[00:42:19.460 --> 00:42:23.020]   Because no one teaches SQL at school, right?
[00:42:23.020 --> 00:42:26.460]   And I was very surprised to find out
[00:42:26.460 --> 00:42:29.820]   that it is more important to actually teach them
[00:42:29.820 --> 00:42:34.260]   the internals of SQL and query optimization and whatnot.
[00:42:34.260 --> 00:42:37.140]   And the reason is because when you go out in the industry,
[00:42:37.140 --> 00:42:41.540]   you can learn the syntax, but you will come across a bug.
[00:42:41.540 --> 00:42:44.700]   And if you can't get the answer from Stack Overflow,
[00:42:44.700 --> 00:42:47.620]   you need to figure out how to reason about it.
[00:42:47.620 --> 00:42:49.740]   And when you don't know things under the hood,
[00:42:49.740 --> 00:42:51.660]   like if you don't know how indexes work,
[00:42:51.660 --> 00:42:55.140]   or even what the concept of an index is, if you don't know--
[00:42:55.140 --> 00:42:57.740]   like there is a thing called query optimization.
[00:42:57.740 --> 00:43:00.300]   Like if I write the queries two different ways,
[00:43:00.300 --> 00:43:03.300]   it will be pushed down to the same thing, right?
[00:43:03.300 --> 00:43:07.140]   Like these kinds of things are not given to you in Stack
[00:43:07.140 --> 00:43:08.900]   Overflow answers, but you are going
[00:43:08.900 --> 00:43:11.180]   to look up when you're like, man,
[00:43:11.180 --> 00:43:13.580]   my boss is complaining about my AWS bill.
[00:43:13.580 --> 00:43:15.620]   I need to make this Spark job run faster.
[00:43:15.620 --> 00:43:18.980]   Like you don't even know how to reason about that if you
[00:43:18.980 --> 00:43:20.500]   don't know some sort of internals.
[00:43:20.500 --> 00:43:24.300]   So for that reason, I feel like it is totally OK to sacrifice
[00:43:24.300 --> 00:43:29.900]   like some tools, some syntax for kind of deeper level
[00:43:29.900 --> 00:43:30.660]   understanding.
[00:43:30.660 --> 00:43:33.620]   And I know a lot of people will disagree with that.
[00:43:33.620 --> 00:43:36.140]   Now I'm more academic in a way.
[00:43:36.140 --> 00:43:37.220]   Emil, what do you think?
[00:43:37.220 --> 00:43:39.420]   What was your opinion on this?
[00:43:39.420 --> 00:43:41.780]   Yeah, my main question is, is it different
[00:43:41.780 --> 00:43:43.460]   than any other profession?
[00:43:43.460 --> 00:43:47.380]   Like I feel like it sounds like all professions say that
[00:43:47.380 --> 00:43:48.020]   to some degree.
[00:43:48.020 --> 00:43:50.660]   Like, hey, the internet teaches the right things in school,
[00:43:50.660 --> 00:43:52.860]   you know?
[00:43:52.860 --> 00:43:53.380]   Yeah.
[00:43:53.380 --> 00:43:54.860]   You know, whatever it might be.
[00:43:54.860 --> 00:43:59.780]   So I was just curious, is it like acute for ML in some way?
[00:43:59.780 --> 00:44:00.820]   Or is it different?
[00:44:00.820 --> 00:44:02.620]   So I think ML is also different from data.
[00:44:02.620 --> 00:44:05.340]   I think the pace at which ML is moving is unbelievable.
[00:44:05.340 --> 00:44:07.820]   Like tomorrow, a new invention will come out.
[00:44:07.820 --> 00:44:09.860]   And two hours later, you'll see like a company
[00:44:09.860 --> 00:44:13.340]   that's literally at that speed.
[00:44:13.340 --> 00:44:16.300]   I think ML is also different in that there's
[00:44:16.300 --> 00:44:18.780]   like a few key players in machine learning that
[00:44:18.780 --> 00:44:23.820]   kind of dictate the education around machine learning.
[00:44:23.820 --> 00:44:27.980]   And that makes it really hard.
[00:44:27.980 --> 00:44:31.940]   Like I'll say, like some Stanford professors are very--
[00:44:31.940 --> 00:44:33.300]   Am I one of these people, Trey?
[00:44:33.300 --> 00:44:33.820]   Is it me?
[00:44:33.820 --> 00:44:34.660]   No, no, no, no, no.
[00:44:34.660 --> 00:44:36.340]   Like academics, I guess.
[00:44:36.340 --> 00:44:38.740]   Like when we're talking about industry classroom,
[00:44:38.740 --> 00:44:41.300]   there's a few classroom people in machine learning
[00:44:41.300 --> 00:44:44.500]   who like almost 100% dictate what people are learning.
[00:44:44.500 --> 00:44:46.180]   Or like fundamental textbooks and stuff.
[00:44:46.180 --> 00:44:48.780]   Yeah, yeah, exactly.
[00:44:48.780 --> 00:44:50.940]   And maybe that's not different for different fields.
[00:44:50.940 --> 00:44:53.820]   Like other sciences fields are also probably monopolized
[00:44:53.820 --> 00:44:56.100]   by individual professors.
[00:44:56.100 --> 00:44:57.860]   But it's really weird in machine learning
[00:44:57.860 --> 00:45:00.340]   when it's like so big, moving so fast.
[00:45:00.340 --> 00:45:02.500]   Like the whole data-centric AI thing
[00:45:02.500 --> 00:45:07.260]   really got me a bit upset because like--
[00:45:07.260 --> 00:45:10.300]   OK, so the model-centric AI, right?
[00:45:10.300 --> 00:45:14.180]   The philosophy of like, let's iterate on the model by hook
[00:45:14.180 --> 00:45:14.780]   or by crook.
[00:45:14.780 --> 00:45:16.740]   We're going to do something until we eke out
[00:45:16.740 --> 00:45:19.140]   some performance on the validation set.
[00:45:19.140 --> 00:45:21.780]   They have just purported that to data.
[00:45:21.780 --> 00:45:24.860]   I will take a fixed data set by hook or by crook.
[00:45:24.860 --> 00:45:27.620]   I will add some examples, remove some examples,
[00:45:27.620 --> 00:45:31.420]   clean some of the data to get a small boost on the validation
[00:45:31.420 --> 00:45:31.940]   set.
[00:45:31.940 --> 00:45:32.820]   Oh, fuck.
[00:45:32.820 --> 00:45:33.620]   God, you're right.
[00:45:33.620 --> 00:45:34.900]   That's exactly what they did.
[00:45:34.900 --> 00:45:35.460]   Damn it.
[00:45:35.460 --> 00:45:37.220]   Yeah, I never realized that.
[00:45:37.220 --> 00:45:39.020]   That's like a brilliant marketing strategy.
[00:45:39.020 --> 00:45:39.620]   That's so true.
[00:45:39.620 --> 00:45:40.100]   That's nice.
[00:45:40.100 --> 00:45:43.660]   And it's like, no, I had a problem with the by hook
[00:45:43.660 --> 00:45:44.860]   or by crook philosophy.
[00:45:44.860 --> 00:45:45.380]   Exactly.
[00:45:45.380 --> 00:45:49.460]   I don't care about the model or the data.
[00:45:49.460 --> 00:45:51.660]   We want something that gives large gain
[00:45:51.660 --> 00:45:53.220]   over a long period of time.
[00:45:53.220 --> 00:45:56.980]   And nobody has framed ML education in that way.
[00:45:56.980 --> 00:45:58.820]   So--
[00:45:58.820 --> 00:46:00.940]   That's some real talk right there.
[00:46:00.940 --> 00:46:02.460]   God, wow.
[00:46:02.460 --> 00:46:03.220]   Yeah.
[00:46:03.220 --> 00:46:04.340]   You're totally right.
[00:46:04.340 --> 00:46:05.220]   That's messed up.
[00:46:05.220 --> 00:46:06.020]   That is right.
[00:46:06.020 --> 00:46:07.660]   It's hard to argue with that.
[00:46:08.460 --> 00:46:10.740]   Maybe it's kind of-- is it related to the tools thing?
[00:46:10.740 --> 00:46:13.500]   Like, this narrative is in service
[00:46:13.500 --> 00:46:15.220]   of some tools or some companies?
[00:46:15.220 --> 00:46:16.340]   It's narrative in service.
[00:46:16.340 --> 00:46:17.700]   I think it's back to the promotions thing.
[00:46:17.700 --> 00:46:19.860]   But for academia, promotions is getting published.
[00:46:19.860 --> 00:46:22.100]   And this is how you get published.
[00:46:22.100 --> 00:46:25.060]   So it's like, we haven't changed the incentive of academia.
[00:46:25.060 --> 00:46:26.860]   So they've relabeled it or whatever.
[00:46:26.860 --> 00:46:28.500]   But they're basically just doing exactly the same thing
[00:46:28.500 --> 00:46:30.100]   they were doing before in the model-centric world,
[00:46:30.100 --> 00:46:32.180]   because we haven't changed the incentives.
[00:46:32.180 --> 00:46:33.180]   Right?
[00:46:33.180 --> 00:46:34.820]   So shit, man.
[00:46:34.820 --> 00:46:35.580]   OK.
[00:46:35.580 --> 00:46:36.100]   Damn.
[00:46:36.260 --> 00:46:38.740]   It's like, get rid of the concept of this holdout
[00:46:38.740 --> 00:46:39.580]   validation set.
[00:46:39.580 --> 00:46:42.860]   Like, the number of these competitions in Europe
[00:46:42.860 --> 00:46:45.420]   or whatever, like the data-centric competitions--
[00:46:45.420 --> 00:46:47.580]   I mean, it's nice that people are thinking about data.
[00:46:47.580 --> 00:46:48.380]   I'll give them that.
[00:46:48.380 --> 00:46:48.860]   Yeah.
[00:46:48.860 --> 00:46:49.820]   That's true.
[00:46:49.820 --> 00:46:53.300]   But just-- the competition should not be,
[00:46:53.300 --> 00:46:57.860]   take ImageNet and change whatever examples
[00:46:57.860 --> 00:47:00.700]   so that you win the competition.
[00:47:00.700 --> 00:47:02.220]   Exactly.
[00:47:02.220 --> 00:47:02.940]   Horrible.
[00:47:02.940 --> 00:47:04.340]   Just horrible.
[00:47:04.340 --> 00:47:05.820]   It's very data-centric, though.
[00:47:05.820 --> 00:47:08.060]   So I don't know.
[00:47:08.060 --> 00:47:09.860]   I always have these mixed feelings, though.
[00:47:09.860 --> 00:47:12.300]   It's like, oh, it's good that you're saying the word data.
[00:47:12.300 --> 00:47:14.180]   But like, oh, no.
[00:47:14.180 --> 00:47:17.780]   Like, everything else about it.
[00:47:17.780 --> 00:47:19.500]   Hamel, I want to give you the last question.
[00:47:19.500 --> 00:47:19.740]   I do.
[00:47:19.740 --> 00:47:21.540]   Because you did all the work to make this happen.
[00:47:21.540 --> 00:47:22.060]   So I appreciate it.
[00:47:22.060 --> 00:47:22.500]   What do you say?
[00:47:22.500 --> 00:47:22.700]   Oh, no.
[00:47:22.700 --> 00:47:22.940]   Yeah.
[00:47:22.940 --> 00:47:24.260]   No, this is very interesting.
[00:47:24.260 --> 00:47:25.460]   Yeah, it's super interesting.
[00:47:25.460 --> 00:47:27.780]   And also, I can't think of a question to top the answer
[00:47:27.780 --> 00:47:28.620]   to this quiz.
[00:47:28.620 --> 00:47:30.300]   So like-- shit.
[00:47:30.300 --> 00:47:31.420]   Yeah.
[00:47:31.420 --> 00:47:32.860]   So OK, one question I want to ask
[00:47:32.860 --> 00:47:35.460]   is, like, OK, you highlighted data leakage, which
[00:47:35.460 --> 00:47:37.020]   I thought is amazing.
[00:47:37.020 --> 00:47:43.260]   Because from my anecdotal experience,
[00:47:43.260 --> 00:47:46.900]   I would say 75% of all pre-existing machine learning
[00:47:46.900 --> 00:47:49.420]   pipelines I encounter in the wild have data leakage.
[00:47:49.420 --> 00:47:59.020]   And my question is, like, it stems from this situation
[00:47:59.020 --> 00:48:02.660]   where models are trained on analytics data.
[00:48:02.660 --> 00:48:05.420]   Like, there's some table out there.
[00:48:05.420 --> 00:48:07.100]   And someone decides to grab that.
[00:48:07.100 --> 00:48:08.380]   They're like, oh, that's low-hanging fruit.
[00:48:08.380 --> 00:48:10.540]   This data is already curated in this thing.
[00:48:10.540 --> 00:48:11.900]   Like, whatever, we can trust it.
[00:48:11.900 --> 00:48:13.300]   Use it for the report.
[00:48:13.300 --> 00:48:19.060]   But it's not backfilled in a way that really pays attention
[00:48:19.060 --> 00:48:20.300]   to time.
[00:48:20.300 --> 00:48:24.100]   And it's like, there's some data leakage there.
[00:48:24.100 --> 00:48:26.260]   So do you think-- is that the problem?
[00:48:26.260 --> 00:48:28.940]   Like, from my anecdote, is that a big part of the problem?
[00:48:28.940 --> 00:48:30.580]   Are there other problems?
[00:48:30.580 --> 00:48:33.420]   Is there some-- is there, like--
[00:48:33.420 --> 00:48:37.060]   is the solution to, like, inject some kind of constraints
[00:48:37.060 --> 00:48:38.700]   into analytics and data pipelines?
[00:48:38.700 --> 00:48:41.500]   Or is this trend, like, we're experiencing now
[00:48:41.500 --> 00:48:43.340]   to make, like, separate feature stores?
[00:48:43.340 --> 00:48:44.620]   Like, good thing?
[00:48:44.620 --> 00:48:46.620]   I wrote this in the DevProd mismatch
[00:48:46.620 --> 00:48:49.420]   because, like, when you get rid of the assumptions gap,
[00:48:49.420 --> 00:48:50.700]   you won't have this problem.
[00:48:50.700 --> 00:48:54.940]   Like, you should be pulling from prod data
[00:48:54.940 --> 00:48:56.740]   and then do your transforms on it.
[00:48:56.740 --> 00:48:59.900]   Then you will not have this leakage, in a way.
[00:48:59.900 --> 00:49:01.580]   And when I say pulling from prod data,
[00:49:01.580 --> 00:49:05.060]   like, I think some databases companies are talking
[00:49:05.060 --> 00:49:07.180]   about this, but time travel--
[00:49:07.180 --> 00:49:08.900]   or at least talking about it more recently.
[00:49:08.900 --> 00:49:11.380]   But I should take my prod snapshot
[00:49:11.380 --> 00:49:14.700]   and be able to travel back in time to three months ago,
[00:49:14.700 --> 00:49:17.540]   and then pull data from that and start training.
[00:49:17.540 --> 00:49:19.980]   Then I don't get the assumptions gap, right?
[00:49:19.980 --> 00:49:21.740]   So, like, when you say prod data,
[00:49:21.740 --> 00:49:23.740]   I'm imagining a situation, like, where you still
[00:49:23.740 --> 00:49:26.460]   need to, like, join a bunch of things and aggregate things
[00:49:26.460 --> 00:49:27.820]   and do all this stuff.
[00:49:27.820 --> 00:49:29.460]   And, like, I'm the skeptic in me is, like,
[00:49:29.460 --> 00:49:30.500]   no one's going to do that.
[00:49:30.500 --> 00:49:31.500]   I agree.
[00:49:31.500 --> 00:49:33.900]   They just want to go to the thing that's there.
[00:49:33.900 --> 00:49:34.420]   That's fine.
[00:49:34.420 --> 00:49:34.900]   I agree.
[00:49:34.900 --> 00:49:37.180]   I think the interfaces are very--
[00:49:37.180 --> 00:49:38.700]   well, both the interfaces are bad,
[00:49:38.700 --> 00:49:40.420]   and the latency is unacceptable, right?
[00:49:40.420 --> 00:49:43.940]   Like, if there's already prematerialized data,
[00:49:43.940 --> 00:49:47.220]   like these analytics tables, I would much rather read
[00:49:47.220 --> 00:49:51.180]   from that than, like, even call an Airflow DAG
[00:49:51.180 --> 00:49:53.940]   or run an Airflow DAG that will take eight hours to go
[00:49:53.940 --> 00:49:56.660]   from, like, last snapshot of data
[00:49:56.660 --> 00:49:59.020]   to some margin of features, right?
[00:49:59.020 --> 00:50:01.420]   Because it's back to, like, velocity validation again.
[00:50:01.420 --> 00:50:01.900]   Yeah.
[00:50:01.900 --> 00:50:02.420]   Yeah.
[00:50:02.420 --> 00:50:02.900]   Right?
[00:50:02.900 --> 00:50:03.820]   Like, OK.
[00:50:03.820 --> 00:50:04.740]   Yeah.
[00:50:04.740 --> 00:50:05.540]   Gotcha.
[00:50:05.540 --> 00:50:07.100]   This is always sort of foreign to me,
[00:50:07.100 --> 00:50:09.900]   that I came from, like, the school of log and wait.
[00:50:09.900 --> 00:50:11.500]   You all know log and wait?
[00:50:11.500 --> 00:50:12.220]   No.
[00:50:12.220 --> 00:50:13.660]   Log and wait.
[00:50:13.660 --> 00:50:14.980]   So log and wait is like a--
[00:50:14.980 --> 00:50:16.620]   maybe I'll rant about this a little bit.
[00:50:16.620 --> 00:50:17.140]   I'm sorry.
[00:50:17.140 --> 00:50:19.260]   I'm out of coffee.
[00:50:19.260 --> 00:50:20.780]   Log and wait was what we did at Slack
[00:50:20.780 --> 00:50:24.740]   and what we did at Google, where we just, like, had logs that
[00:50:24.740 --> 00:50:25.260]   were our--
[00:50:25.260 --> 00:50:25.980]   Oh, yeah, yeah.
[00:50:25.980 --> 00:50:26.500]   --training.
[00:50:26.500 --> 00:50:27.260]   Log and wait.
[00:50:27.260 --> 00:50:27.500]   OK.
[00:50:27.500 --> 00:50:28.460]   Let's just try some new features.
[00:50:28.460 --> 00:50:30.060]   Let's just, like, add them to the logs
[00:50:30.060 --> 00:50:31.940]   and, like, let the logs run for a while.
[00:50:31.940 --> 00:50:33.860]   And then eventually, we'll have training data.
[00:50:33.860 --> 00:50:35.300]   And I always felt like that was great,
[00:50:35.300 --> 00:50:37.700]   because it wasn't like we were just sitting around
[00:50:37.700 --> 00:50:39.220]   with nothing to do.
[00:50:39.220 --> 00:50:41.180]   We had, like, 100 other things to do.
[00:50:41.180 --> 00:50:44.100]   And so you could just, like, kind of pipeline things,
[00:50:44.100 --> 00:50:46.500]   I guess, roughly, is, like, log your data,
[00:50:46.500 --> 00:50:47.660]   wait a couple of weeks.
[00:50:47.660 --> 00:50:50.160]   Then you had lots of data, and you could, like, train on it
[00:50:50.160 --> 00:50:51.260]   and stuff like that.
[00:50:51.260 --> 00:50:54.060]   And so that was, like, a great solution as far as I'm
[00:50:54.060 --> 00:50:54.560]   concerned.
[00:50:54.560 --> 00:50:57.180]   But I understand that it doesn't make everyone happy.
[00:50:57.180 --> 00:50:59.940]   It doesn't make people happy, and it's high latency,
[00:50:59.940 --> 00:51:01.860]   especially for a lot of these companies
[00:51:01.860 --> 00:51:04.700]   where they've, like, not found good product market fit.
[00:51:04.700 --> 00:51:06.580]   Like, you'll have this series of companies
[00:51:06.580 --> 00:51:08.040]   that don't have product market fit,
[00:51:08.040 --> 00:51:10.540]   and they're, like, velocity is, like, utmost importance,
[00:51:10.540 --> 00:51:11.580]   right, to get--
[00:51:11.580 --> 00:51:12.940]   Yeah, for sure.
[00:51:12.940 --> 00:51:14.020]   --something.
[00:51:14.020 --> 00:51:14.580]   That's right.
[00:51:14.580 --> 00:51:16.780]   They're willing to wait weeks.
[00:51:16.780 --> 00:51:18.900]   I have never worked at unsuccessful companies,
[00:51:18.900 --> 00:51:20.300]   so I don't know what that's like.
[00:51:20.300 --> 00:51:20.800]   [LAUGHTER]
[00:51:20.800 --> 00:51:23.300]   But I can imagine that as a problem for people.
[00:51:23.300 --> 00:51:23.820]   I get that.
[00:51:23.820 --> 00:51:24.320]   Yeah.
[00:51:24.320 --> 00:51:24.820]   Yeah.
[00:51:24.820 --> 00:51:26.220]   Yeah.
[00:51:26.220 --> 00:51:27.460]   Cool.
[00:51:27.460 --> 00:51:29.020]   I would say-- I guess I'm going to--
[00:51:29.020 --> 00:51:30.260]   I'll stay to the last question.
[00:51:30.260 --> 00:51:31.260]   We have a little more time, which was--
[00:51:31.260 --> 00:51:33.060]   Yeah, please do.
[00:51:33.060 --> 00:51:34.460]   Shreya, I was curious, and I think
[00:51:34.460 --> 00:51:37.940]   it's especially appropriate for a panel to talk about this.
[00:51:37.940 --> 00:51:40.140]   Seeking to explain the unknown, like,
[00:51:40.140 --> 00:51:43.860]   retrofitting explanations onto things, right?
[00:51:43.860 --> 00:51:46.460]   What is a panel if it is not three people looking
[00:51:46.460 --> 00:51:48.140]   to retrofit explanations onto some things?
[00:51:48.140 --> 00:51:48.980]   But, like--
[00:51:48.980 --> 00:51:50.060]   We love doing that.
[00:51:50.060 --> 00:51:50.560]   We do.
[00:51:50.560 --> 00:51:51.820]   [INAUDIBLE]
[00:51:51.820 --> 00:51:54.420]   We love-- I don't know what exactly you said, Hamel,
[00:51:54.420 --> 00:51:58.020]   but it was like, we love doing this, or something.
[00:51:58.020 --> 00:51:59.060]   We can't help ourselves.
[00:51:59.060 --> 00:51:59.700]   We are humans.
[00:51:59.700 --> 00:52:00.820]   We love telling stories.
[00:52:00.820 --> 00:52:01.340]   It's great.
[00:52:01.340 --> 00:52:01.840]   Yeah.
[00:52:01.840 --> 00:52:02.340]   Yeah.
[00:52:02.340 --> 00:52:05.260]   I mean, like, when I read the section in the paper,
[00:52:05.260 --> 00:52:09.100]   retrofitting an explanation, I was like, OK,
[00:52:09.100 --> 00:52:11.540]   this is definitely human nature.
[00:52:11.540 --> 00:52:12.700]   But then also, I wonder--
[00:52:12.700 --> 00:52:14.780]   because it happens in ML research, too.
[00:52:14.780 --> 00:52:15.300]   Oh, yeah.
[00:52:15.300 --> 00:52:15.800]   Oh, yeah.
[00:52:15.800 --> 00:52:21.940]   So is this just bleeding from there to here into applied ML?
[00:52:21.940 --> 00:52:23.220]   Interesting.
[00:52:23.220 --> 00:52:24.060]   Maybe.
[00:52:24.060 --> 00:52:28.700]   I think also, like, putting myself in the data science
[00:52:28.700 --> 00:52:30.260]   shoes, like, when you retrofit, then
[00:52:30.260 --> 00:52:32.580]   you can create new heuristics for yourself
[00:52:32.580 --> 00:52:33.860]   for future projects.
[00:52:33.860 --> 00:52:37.420]   Like, this worked because X. And so now I'm
[00:52:37.420 --> 00:52:39.580]   going to find similar projects in the future
[00:52:39.580 --> 00:52:41.660]   because it's the same.
[00:52:41.660 --> 00:52:44.420]   So maybe there's a reason that we
[00:52:44.420 --> 00:52:48.540]   want to retrofit that's not just our human nature.
[00:52:48.540 --> 00:52:49.180]   Yeah.
[00:52:49.180 --> 00:52:52.580]   I think it's-- I mean, I very much felt that way running
[00:52:52.580 --> 00:52:54.540]   ML models and experiments at Google and stuff.
[00:52:54.540 --> 00:52:56.140]   It was like, this compulsive need
[00:52:56.140 --> 00:52:58.820]   to build a mental model of what the system was doing.
[00:52:58.820 --> 00:52:59.820]   Exactly.
[00:52:59.820 --> 00:53:00.320]   Exactly.
[00:53:00.320 --> 00:53:02.540]   I could not help myself, broadly speaking.
[00:53:02.540 --> 00:53:04.700]   It makes you look smart, for sure, in conversation,
[00:53:04.700 --> 00:53:06.140]   especially with non-ML people.
[00:53:06.140 --> 00:53:07.740]   They're like, oh, that person knows.
[00:53:07.740 --> 00:53:10.700]   They know why everything is happening.
[00:53:10.700 --> 00:53:12.660]   But I think it's important, though, to call out
[00:53:12.660 --> 00:53:14.460]   is, like, what I loved about doing, like, you know,
[00:53:14.460 --> 00:53:16.300]   A/B testing and experiment-driven research
[00:53:16.300 --> 00:53:18.940]   in production, especially with, like, real money on the line,
[00:53:18.940 --> 00:53:20.900]   was that you could come up with experiments that
[00:53:20.900 --> 00:53:22.820]   could prove your hypotheses.
[00:53:22.820 --> 00:53:23.620]   Yeah, I love that.
[00:53:23.620 --> 00:53:24.820]   You're actually wrong about this,
[00:53:24.820 --> 00:53:25.780]   and, like, your model was incorrect.
[00:53:25.780 --> 00:53:28.260]   And that was, like-- that's always a revelation and stuff.
[00:53:28.260 --> 00:53:30.860]   And that's sort of what I loved about this stuff,
[00:53:30.860 --> 00:53:33.140]   was that, like, adherence to ground truth and reality,
[00:53:33.140 --> 00:53:34.560]   especially when money is involved,
[00:53:34.560 --> 00:53:39.020]   just makes things very real and very low, you know,
[00:53:39.020 --> 00:53:41.660]   religious, whatever, apocryphal beliefs.
[00:53:41.660 --> 00:53:43.660]   Like, you know, just, like, sort of nonsense, I guess,
[00:53:43.660 --> 00:53:44.160]   basically.
[00:53:44.160 --> 00:53:45.300]   And I love that.
[00:53:45.300 --> 00:53:46.140]   Yeah.
[00:53:46.140 --> 00:53:48.540]   Yeah, I don't think there's enough A/B testing in machine
[00:53:48.540 --> 00:53:49.500]   learning.
[00:53:49.500 --> 00:53:52.820]   Like, people know that they should be doing it more,
[00:53:52.820 --> 00:53:53.940]   but it's not--
[00:53:53.940 --> 00:53:57.140]   like, the tools aren't really there.
[00:53:57.140 --> 00:53:58.980]   It's not really discussed that much.
[00:53:58.980 --> 00:54:00.780]   So many people have been saying they
[00:54:00.780 --> 00:54:03.180]   want to start these, like, A/B testing companies,
[00:54:03.180 --> 00:54:05.620]   but you need to, like, sell to people
[00:54:05.620 --> 00:54:09.680]   who have enough predictions that they're making in order
[00:54:09.680 --> 00:54:11.340]   to A/B test appropriately, right?
[00:54:11.340 --> 00:54:14.740]   Like, some companies will have, like, only five customers.
[00:54:14.740 --> 00:54:17.060]   And then at that point, it's, like,
[00:54:17.060 --> 00:54:19.700]   how do you, like, appropriately A/B test?
[00:54:19.700 --> 00:54:21.460]   I don't even think that's possible.
[00:54:21.460 --> 00:54:22.380]   It's not.
[00:54:22.380 --> 00:54:23.380]   Yeah.
[00:54:23.380 --> 00:54:24.220]   I mean, I don't know.
[00:54:24.220 --> 00:54:26.420]   You can maybe do something with interleaving a little bit,
[00:54:26.420 --> 00:54:27.380]   or, like, contextual events.
[00:54:27.380 --> 00:54:28.300]   But I mean, if you're talking about, like,
[00:54:28.300 --> 00:54:29.820]   five people or five customers, like,
[00:54:29.820 --> 00:54:31.700]   the A/B test is, are the customers happy or not?
[00:54:31.700 --> 00:54:32.300]   Like, that's--
[00:54:32.300 --> 00:54:32.800]   [LAUGHTER]
[00:54:32.800 --> 00:54:35.140]   [INAUDIBLE]
[00:54:35.140 --> 00:54:36.100]   Yeah.
[00:54:36.100 --> 00:54:36.820]   No, that's true.
[00:54:36.820 --> 00:54:37.660]   That's a good point.
[00:54:37.660 --> 00:54:39.940]   That's a good point.
[00:54:39.940 --> 00:54:41.380]   So we're a little over time.
[00:54:41.380 --> 00:54:42.300]   That's OK.
[00:54:42.300 --> 00:54:43.020]   It's OK, right?
[00:54:43.020 --> 00:54:45.540]   Yeah, I think we're a little bit over time.
[00:54:45.540 --> 00:54:45.940]   OK.
[00:54:45.940 --> 00:54:47.860]   Andrew is not shutting us down yet or anything.
[00:54:47.860 --> 00:54:49.660]   So I think-- can we keep talking?
[00:54:49.660 --> 00:54:50.300]   Is that--
[00:54:50.300 --> 00:54:51.340]   I have no idea.
[00:54:51.340 --> 00:54:51.860]   OK.
[00:54:51.860 --> 00:54:52.380]   Yeah, Andrew--
[00:54:52.380 --> 00:54:53.500]   She said we can, yeah.
[00:54:53.500 --> 00:54:54.140]   We can go nuts.
[00:54:54.140 --> 00:54:56.100]   I don't know how long to keep talking, but--
[00:54:56.100 --> 00:54:56.600]   [LAUGHTER]
[00:54:56.600 --> 00:54:58.180]   I have only 10 minutes.
[00:54:58.180 --> 00:55:00.940]   So that's my hard stuff.
[00:55:00.940 --> 00:55:01.780]   We have a hard stuff.
[00:55:01.780 --> 00:55:03.580]   I'm probably supposed to be in a meeting right now,
[00:55:03.580 --> 00:55:05.340]   but I rarely show up for meetings anymore.
[00:55:05.340 --> 00:55:05.840]   [LAUGHTER]
[00:55:05.840 --> 00:55:06.340]   That's fine.
[00:55:06.340 --> 00:55:07.700]   That's my dream.
[00:55:07.700 --> 00:55:09.980]   People are like, oh, like, do you want to go to academia
[00:55:09.980 --> 00:55:12.540]   or start a company or whatever?
[00:55:12.540 --> 00:55:14.340]   And I'm like, I want the job where
[00:55:14.340 --> 00:55:15.980]   I don't have to go to meetings.
[00:55:15.980 --> 00:55:17.700]   Right now, my job is amazing.
[00:55:17.700 --> 00:55:20.580]   I don't have to go to meetings unless they're useful.
[00:55:20.580 --> 00:55:22.020]   So--
[00:55:22.020 --> 00:55:22.940]   Braggy, Shreya.
[00:55:22.940 --> 00:55:24.660]   Braggy, but fair.
[00:55:24.660 --> 00:55:26.020]   It's being a junior--
[00:55:26.020 --> 00:55:29.420]   I really hate meetings, too, yeah.
[00:55:29.420 --> 00:55:32.140]   And both of you have solved that by academia on one hand
[00:55:32.140 --> 00:55:33.540]   and not having a job on the other.
[00:55:33.540 --> 00:55:34.620]   So I applaud you.
[00:55:34.620 --> 00:55:35.380]   Yeah.
[00:55:35.380 --> 00:55:38.740]   I'm clearly, clearly the dumbest person in this panel.
[00:55:38.740 --> 00:55:40.980]   So that allowed.
[00:55:40.980 --> 00:55:43.100]   I just-- like, let me ask a follow-up.
[00:55:43.100 --> 00:55:44.220]   Like, this was just--
[00:55:44.220 --> 00:55:45.780]   I mean, the paper was just awesome.
[00:55:45.780 --> 00:55:46.940]   I just love it so much.
[00:55:46.940 --> 00:55:48.460]   And I'm sorry to be like that guy,
[00:55:48.460 --> 00:55:50.540]   but like, so how are you going to top it, Shreya?
[00:55:50.540 --> 00:55:53.020]   What's-- what's-- what have you done for me lately?
[00:55:53.020 --> 00:55:55.100]   What's the next paper?
[00:55:55.100 --> 00:55:55.620]   OK.
[00:55:55.620 --> 00:55:56.980]   Well, there's one paper coming out
[00:55:56.980 --> 00:55:58.860]   on automatic data validation.
[00:55:58.860 --> 00:56:01.620]   I'm submitting that next month.
[00:56:01.620 --> 00:56:04.540]   We talk about the problem of, like, false positives
[00:56:04.540 --> 00:56:06.460]   when it comes to data validation,
[00:56:06.460 --> 00:56:10.300]   like when you alert on missing data, broken columns, data
[00:56:10.300 --> 00:56:12.900]   drift, all of these things.
[00:56:12.900 --> 00:56:14.620]   Can I have a copy of that paper right now?
[00:56:14.620 --> 00:56:15.140]   Can I have it early?
[00:56:15.140 --> 00:56:16.740]   Because I've got to teach this class.
[00:56:16.740 --> 00:56:17.240]   You have.
[00:56:17.240 --> 00:56:18.460]   Yes.
[00:56:18.460 --> 00:56:19.820]   I'll follow up with you.
[00:56:19.820 --> 00:56:20.860]   I'll send you the draft.
[00:56:20.860 --> 00:56:22.500]   I would love, love, love to read this.
[00:56:22.500 --> 00:56:23.000]   Yeah.
[00:56:23.000 --> 00:56:23.900]   Yes, thank you.
[00:56:23.900 --> 00:56:24.420]   Yes.
[00:56:24.420 --> 00:56:27.780]   So we benchmark a bunch of, like, existing methods,
[00:56:27.780 --> 00:56:30.380]   both from databases and, like, traditional--
[00:56:30.380 --> 00:56:33.740]   canonical, like, your KS test and two-sample testing
[00:56:33.740 --> 00:56:35.540]   and stuff.
[00:56:35.540 --> 00:56:37.620]   I have some nice anecdotes on why
[00:56:37.620 --> 00:56:41.100]   nobody should be using KS test for monitoring ML models.
[00:56:41.100 --> 00:56:43.660]   All of these things, I think, just needed to be in a paper.
[00:56:43.660 --> 00:56:44.540]   So that's, like--
[00:56:44.540 --> 00:56:46.820]   I'm wrapping that up.
[00:56:46.820 --> 00:56:50.680]   We have spoiler alert, our own method that does really well.
[00:56:50.680 --> 00:56:52.980]   That sounds like it's going to be the start of a company,
[00:56:52.980 --> 00:56:53.480]   Shreya.
[00:56:53.480 --> 00:56:55.300]   I'm not going to lie.
[00:56:55.300 --> 00:56:55.860]   I don't know.
[00:56:55.860 --> 00:56:58.540]   It's going to blow a bunch of MLOps monitoring tools out
[00:56:58.540 --> 00:56:59.300]   of the water.
[00:56:59.300 --> 00:57:01.020]   This seems like there's a lot of tension
[00:57:01.020 --> 00:57:02.580]   towards creating a company for--
[00:57:02.580 --> 00:57:04.700]   Yeah, it's very suspicious, Shreya.
[00:57:04.700 --> 00:57:05.220]   Suspicious.
[00:57:05.220 --> 00:57:10.660]   No, I don't want to create a monitoring-only company.
[00:57:10.660 --> 00:57:13.020]   It's just, at least--
[00:57:13.020 --> 00:57:17.340]   you know my advisors, and you know Berkeley Database Group.
[00:57:17.340 --> 00:57:18.980]   Like, you got to--
[00:57:18.980 --> 00:57:20.620]   if you don't, like, own the data,
[00:57:20.620 --> 00:57:24.100]   it's really hard to do anything useful.
[00:57:24.100 --> 00:57:26.300]   So I don't know about, like, companies sake.
[00:57:26.300 --> 00:57:27.620]   So that paper is coming out.
[00:57:27.620 --> 00:57:31.900]   And then also thinking a lot about kind
[00:57:31.900 --> 00:57:35.420]   of detecting, diagnosing performance drops
[00:57:35.420 --> 00:57:36.820]   in production.
[00:57:36.820 --> 00:57:38.620]   How do we do this when there are no labels?
[00:57:38.620 --> 00:57:41.740]   So one of the undergrads I'm working with has--
[00:57:41.740 --> 00:57:45.380]   we have a paper on that coming probably next semester.
[00:57:45.380 --> 00:57:48.580]   And then some other stuff around, like, diagnosing ML
[00:57:48.580 --> 00:57:50.820]   models.
[00:57:50.820 --> 00:57:52.500]   At least in databases, we talk a lot
[00:57:52.500 --> 00:57:54.780]   about provenance in general.
[00:57:54.780 --> 00:57:58.980]   Like, can we identify the source of things?
[00:57:58.980 --> 00:58:00.860]   Maybe this will help us for debugging.
[00:58:00.860 --> 00:58:03.580]   Maybe this will help us for data set distillation,
[00:58:03.580 --> 00:58:06.940]   throwing away unnecessary data, et cetera.
[00:58:06.940 --> 00:58:09.620]   When ML models come into the picture,
[00:58:09.620 --> 00:58:12.180]   everything is really hard because now somehow
[00:58:12.180 --> 00:58:14.860]   every piece of data is used in an ML model,
[00:58:14.860 --> 00:58:18.060]   but you don't really know how.
[00:58:18.060 --> 00:58:20.900]   So we're working on a paper that's, like,
[00:58:20.900 --> 00:58:23.820]   given some single test point.
[00:58:23.820 --> 00:58:25.820]   Can you identify the top training examples?
[00:58:25.820 --> 00:58:28.020]   Can you do this cheaply in one pass?
[00:58:28.020 --> 00:58:28.980]   That's awesome, Shreya.
[00:58:28.980 --> 00:58:29.660]   That's awesome.
[00:58:29.660 --> 00:58:30.540]   It's really exciting.
[00:58:30.540 --> 00:58:32.020]   We have fun, fun stuff coming up.
[00:58:32.020 --> 00:58:33.060]   I'm really excited.
[00:58:33.060 --> 00:58:33.900]   That's fantastic.
[00:58:33.900 --> 00:58:34.700]   That is exciting.
[00:58:34.700 --> 00:58:36.660]   And Josh, what kind of course are you teaching?
[00:58:36.660 --> 00:58:38.260]   You said you might use it in a course.
[00:58:38.260 --> 00:58:40.100]   So I teach a class on data engineering
[00:58:40.100 --> 00:58:42.500]   for machine learning, which is sort of how
[00:58:42.500 --> 00:58:46.420]   I think about building machine learning systems and stuff,
[00:58:46.420 --> 00:58:50.460]   from a very data-centric perspective.
[00:58:50.460 --> 00:58:52.820]   I'm not trying to publish any papers or improve anything.
[00:58:52.820 --> 00:58:54.660]   I'm just trying to help people build
[00:58:54.660 --> 00:58:57.700]   useful models in reality.
[00:58:57.700 --> 00:59:00.420]   So we have a whole section--
[00:59:00.420 --> 00:59:02.300]   basically, one of the sessions we do is really
[00:59:02.300 --> 00:59:05.660]   focus entirely on the sort of plethora of data quality
[00:59:05.660 --> 00:59:08.140]   and validation tools there, and how do we link stuff,
[00:59:08.140 --> 00:59:10.620]   information we find during training,
[00:59:10.620 --> 00:59:12.580]   from the data warehouse to the production system
[00:59:12.580 --> 00:59:13.740]   where we're doing inference.
[00:59:13.740 --> 00:59:16.140]   Because I guess, to my experience,
[00:59:16.140 --> 00:59:19.980]   Shreya, for what it's worth, most of the truly catastrophic
[00:59:19.980 --> 00:59:23.580]   ML errors I've encountered has been when the data changed
[00:59:23.580 --> 00:59:25.980]   dramatically instantaneously.
[00:59:25.980 --> 00:59:27.580]   You know what I mean?
[00:59:27.580 --> 00:59:30.220]   Read the intro of our automatic data file paper.
[00:59:30.220 --> 00:59:31.060]   I'm going to send it to you.
[00:59:31.060 --> 00:59:32.220]   I would love your feedback.
[00:59:32.220 --> 00:59:33.540]   I cannot wait.
[00:59:33.540 --> 00:59:34.260]   I'm so excited.
[00:59:34.260 --> 00:59:35.540]   So thank you so much for that.
[00:59:35.540 --> 00:59:37.420]   That's fantastic.
[00:59:37.420 --> 00:59:38.820]   But anyway, that's awesome.
[00:59:38.820 --> 00:59:42.700]   Would you consider doing another interview study like this?
[00:59:42.700 --> 00:59:44.940]   I mean, given that the social sciences software takes
[00:59:44.940 --> 00:59:47.980]   five minutes to load on an M1--
[00:59:47.980 --> 00:59:49.660]   It's really-- it's funny.
[00:59:49.660 --> 00:59:53.700]   The way that we got into this was, before grad school,
[00:59:53.700 --> 00:59:56.460]   I was like, oh, maybe should I start building
[00:59:56.460 --> 00:59:57.780]   my own tool or something?
[00:59:57.780 --> 01:00:01.180]   Like, let me do some customer user research or something.
[01:00:01.180 --> 01:00:02.380]   I talked to people.
[01:00:02.380 --> 01:00:04.260]   And then I was like, oh, this was a mistake,
[01:00:04.260 --> 01:00:05.580]   because there's way too much out there.
[01:00:05.580 --> 01:00:07.580]   I don't have good mental models, like what's
[01:00:07.580 --> 01:00:09.380]   going on in the world, like who to build for.
[01:00:09.380 --> 01:00:11.820]   ML is a long tail of applications,
[01:00:11.820 --> 01:00:13.660]   debugging patterns, organizational patterns,
[01:00:13.660 --> 01:00:15.180]   everything right now.
[01:00:15.180 --> 01:00:16.900]   So it's really hard to kind of focus
[01:00:16.900 --> 01:00:19.140]   on where the tail you want.
[01:00:19.140 --> 01:00:20.860]   Then when we started--
[01:00:20.860 --> 01:00:24.500]   when I started the PhD, a professor emailed our lab
[01:00:24.500 --> 01:00:28.380]   asking, hey, can you contribute to this MLOps repository
[01:00:28.380 --> 01:00:30.180]   with some best practices?
[01:00:30.180 --> 01:00:31.100]   And we're like, OK.
[01:00:31.100 --> 01:00:32.220]   And we start writing things.
[01:00:32.220 --> 01:00:33.600]   And we're like, there's absolutely
[01:00:33.600 --> 01:00:34.760]   nothing to back anything up.
[01:00:34.760 --> 01:00:38.540]   And we're like, academics preaching, this feels terrible.
[01:00:38.540 --> 01:00:41.140]   Let's try to build something that can back things up
[01:00:41.140 --> 01:00:42.940]   so we can go see this paper.
[01:00:42.940 --> 01:00:44.540]   We did this.
[01:00:44.540 --> 01:00:47.700]   And then Joe and Odethia were like, yeah,
[01:00:47.700 --> 01:00:48.940]   you can do an interview study.
[01:00:48.940 --> 01:00:51.300]   But let me warn you that it'll take up
[01:00:51.300 --> 01:00:53.340]   like hundreds and hundreds of hours.
[01:00:53.340 --> 01:00:54.940]   And I was like, there's no way it can take
[01:00:54.940 --> 01:00:56.420]   hundreds and hundreds of hours.
[01:00:56.420 --> 01:00:58.580]   Companies do interview studies all the time.
[01:00:58.580 --> 01:01:01.660]   And it's like super fine.
[01:01:01.660 --> 01:01:03.180]   Hold on.
[01:01:03.180 --> 01:01:06.220]   I'm so wrong.
[01:01:06.220 --> 01:01:08.740]   A number of times we iterated on the questions.
[01:01:08.740 --> 01:01:11.660]   You have to be kind of as unbiased as possible
[01:01:11.660 --> 01:01:13.980]   in these interview questions.
[01:01:13.980 --> 01:01:15.980]   All the coding, there's hundreds of hours
[01:01:15.980 --> 01:01:18.100]   that are just in the coding of the transcripts.
[01:01:18.100 --> 01:01:22.940]   I have utmost respect for HCI researchers.
[01:01:22.940 --> 01:01:26.380]   I don't know every single paper that they do.
[01:01:26.380 --> 01:01:29.060]   It's just like, oh my god.
[01:01:29.060 --> 01:01:30.020]   It's so much work.
[01:01:30.020 --> 01:01:33.020]   So much work.
[01:01:33.020 --> 01:01:34.020]   Never again.
[01:01:34.020 --> 01:01:34.520]   That's it.
[01:01:34.520 --> 01:01:36.460]   You're done.
[01:01:36.460 --> 01:01:39.820]   The next time I do an interview study paper,
[01:01:39.820 --> 01:01:44.900]   I am considering pivoting to a different area of work.
[01:01:44.900 --> 01:01:46.740]   It was great in that I learned a ton.
[01:01:46.740 --> 01:01:50.300]   But I will not be doing them willy nilly.
[01:01:50.300 --> 01:01:51.060]   Gotcha.
[01:01:51.060 --> 01:01:51.980]   It's very thoughtful.
[01:01:51.980 --> 01:01:54.220]   Again, I think just speaking for Hamill
[01:01:54.220 --> 01:01:56.460]   and I think for a lot of us in the industry,
[01:01:56.460 --> 01:01:58.180]   thank you so much.
[01:01:58.180 --> 01:01:59.500]   Incredibly, incredibly grateful.
[01:01:59.500 --> 01:02:01.100]   It's just beautiful work.
[01:02:01.100 --> 01:02:02.580]   It means so much to so many people.
[01:02:02.580 --> 01:02:04.140]   So thank you so much for it.
[01:02:04.140 --> 01:02:04.660]   Thank you.
[01:02:04.660 --> 01:02:08.020]   I've been able to use the paper quite a bit in conversations.
[01:02:08.020 --> 01:02:09.300]   Thank you.
[01:02:09.300 --> 01:02:11.100]   Sometimes throw it in someone's face
[01:02:11.100 --> 01:02:14.740]   when they try to shove software engineering down my throat.
[01:02:14.740 --> 01:02:15.660]   It's literally me.
[01:02:15.660 --> 01:02:19.140]   I was the person he's talking about.
[01:02:19.140 --> 01:02:19.780]   No, thank you.
[01:02:19.780 --> 01:02:20.500]   That's super kind.
[01:02:20.500 --> 01:02:21.580]   It wasn't just me.
[01:02:21.580 --> 01:02:23.860]   It was Rolando, Joe, and Adithya.
[01:02:23.860 --> 01:02:27.180]   Super, super, everybody was involved.
[01:02:27.180 --> 01:02:30.100]   I'm very lucky to have an advisor, Adithya,
[01:02:30.100 --> 01:02:35.340]   who's very good at mentoring and very good at writing.
[01:02:35.340 --> 01:02:37.500]   I'm learning a lot from this.
[01:02:37.500 --> 01:02:38.940]   I like-- I don't know.
[01:02:38.940 --> 01:02:40.780]   I love being a PhD student for that reason.
[01:02:40.780 --> 01:02:43.860]   You just develop a breadth of skills.
[01:02:43.860 --> 01:02:47.420]   So definitely shout out to them, too.
[01:02:47.420 --> 01:02:48.780]   Absolutely.
[01:02:48.780 --> 01:02:50.020]   We will let you go.
[01:02:50.020 --> 01:02:51.140]   Thank you so much, Rhea.
[01:02:51.140 --> 01:02:52.060]   I really, really appreciate it.
[01:02:52.060 --> 01:02:53.020]   Thanks for taking the time.
[01:02:53.020 --> 01:02:53.860]   Yeah, thanks a lot.
[01:02:53.860 --> 01:02:54.820]   Thanks for having me.
[01:02:54.820 --> 01:02:58.180]   [MUSIC PLAYING]
[01:02:58.180 --> 01:03:01.540]   [MUSIC PLAYING]
[01:03:01.540 --> 01:03:04.120]   (upbeat music)
[01:03:04.120 --> 01:03:06.700]   (upbeat music)

