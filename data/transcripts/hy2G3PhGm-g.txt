
[00:00:00.000 --> 00:00:03.520]   If one site is hacked, you can just unleash all hell.
[00:00:03.520 --> 00:00:06.840]   We have stumbled into this new era
[00:00:06.840 --> 00:00:08.960]   of mutually assured digital destruction.
[00:00:08.960 --> 00:00:11.200]   How far are people willing to go?
[00:00:11.200 --> 00:00:13.040]   You can capture their location,
[00:00:13.040 --> 00:00:16.160]   you can capture their contacts
[00:00:16.160 --> 00:00:18.000]   that record their telephone calls,
[00:00:18.000 --> 00:00:20.760]   record their camera without them knowing about it.
[00:00:20.760 --> 00:00:24.400]   Basically, you can put an invisible ankle bracelet
[00:00:24.400 --> 00:00:26.000]   on someone without them knowing.
[00:00:26.000 --> 00:00:31.000]   You could sell that to a zero-day broker for $2 million.
[00:00:31.000 --> 00:00:37.320]   - The following is a conversation with Nicole Perleroth,
[00:00:37.320 --> 00:00:40.080]   cybersecurity journalist and author
[00:00:40.080 --> 00:00:42.560]   of "This Is How They Tell Me The World Ends,"
[00:00:42.560 --> 00:00:44.920]   the cyber weapons arm race.
[00:00:44.920 --> 00:00:46.960]   This is the Alex Friedman Podcast.
[00:00:46.960 --> 00:00:49.080]   To support it, please check out our sponsors
[00:00:49.080 --> 00:00:50.360]   in the description.
[00:00:50.360 --> 00:00:54.320]   And now, dear friends, here's Nicole Perleroth.
[00:00:55.220 --> 00:00:58.560]   You've interviewed hundreds of cybersecurity hackers,
[00:00:58.560 --> 00:01:01.040]   activists, dissidents, computer scientists,
[00:01:01.040 --> 00:01:03.760]   government officials, forensic investigators,
[00:01:03.760 --> 00:01:05.700]   and mercenaries.
[00:01:05.700 --> 00:01:09.400]   So let's talk about cybersecurity and cyber war.
[00:01:09.400 --> 00:01:10.500]   Start with the basics.
[00:01:10.500 --> 00:01:13.560]   What is a zero-day vulnerability
[00:01:13.560 --> 00:01:16.800]   and then a zero-day exploit or attack?
[00:01:16.800 --> 00:01:22.560]   - So at the most basic level, let's say I'm a hacker
[00:01:22.560 --> 00:01:27.560]   and I find a bug in your iPhone iOS software
[00:01:27.560 --> 00:01:31.260]   that no one else knows about, especially Apple.
[00:01:31.260 --> 00:01:34.360]   That's called a zero-day because the minute it's discovered,
[00:01:34.360 --> 00:01:36.560]   engineers have had zero days to fix it.
[00:01:36.560 --> 00:01:40.920]   If I can study that zero-day,
[00:01:40.920 --> 00:01:44.940]   I could potentially write a program to exploit it.
[00:01:44.940 --> 00:01:48.860]   And that program would be called a zero-day exploit.
[00:01:48.860 --> 00:01:53.860]   And for iOS, the dream is that you craft a zero-day exploit
[00:01:53.860 --> 00:01:57.380]   that can remotely exploit someone else's iPhone
[00:01:57.380 --> 00:01:59.940]   without them ever knowing about it.
[00:01:59.940 --> 00:02:01.900]   And you can capture their location,
[00:02:01.900 --> 00:02:04.980]   you can capture their contacts
[00:02:04.980 --> 00:02:06.860]   that record their telephone calls,
[00:02:06.860 --> 00:02:09.620]   record their camera without them knowing about it.
[00:02:09.620 --> 00:02:13.240]   Basically, you can put an invisible ankle bracelet
[00:02:13.240 --> 00:02:15.060]   on someone without them knowing.
[00:02:15.060 --> 00:02:17.220]   And you can see why that capability,
[00:02:17.220 --> 00:02:20.500]   that zero-day exploit would have immense value
[00:02:20.500 --> 00:02:23.740]   for a spy agency or a government
[00:02:23.740 --> 00:02:27.700]   that wants to monitor its critics or dissidents.
[00:02:27.700 --> 00:02:30.620]   And so there's a very lucrative market now
[00:02:30.620 --> 00:02:32.100]   for zero-day exploits.
[00:02:32.100 --> 00:02:33.460]   - So you said a few things there.
[00:02:33.460 --> 00:02:36.360]   One is iOS, why iOS?
[00:02:36.360 --> 00:02:37.660]   Which operating system?
[00:02:37.660 --> 00:02:40.220]   Which one is the sexier thing to try to get to
[00:02:40.220 --> 00:02:42.740]   or the most impactful thing?
[00:02:42.740 --> 00:02:45.500]   And the other thing you mentioned is remote
[00:02:45.500 --> 00:02:49.300]   versus having to actually come in physical contact with it.
[00:02:49.300 --> 00:02:50.900]   Is that the distinction?
[00:02:50.900 --> 00:02:54.540]   - So iPhone exploits have just been
[00:02:54.540 --> 00:02:58.260]   a government's number one priority.
[00:02:58.260 --> 00:03:02.100]   Recently, actually, the price of an Android
[00:03:02.100 --> 00:03:03.540]   remote zero-day exploit,
[00:03:03.540 --> 00:03:06.580]   something that can get you into Android phones,
[00:03:06.580 --> 00:03:08.140]   is actually higher.
[00:03:08.140 --> 00:03:09.660]   The value of that is now higher
[00:03:09.660 --> 00:03:12.540]   on this underground market for zero-day exploits
[00:03:12.540 --> 00:03:15.400]   than an iPhone iOS exploit.
[00:03:15.400 --> 00:03:16.740]   So things are changing.
[00:03:16.740 --> 00:03:20.100]   - So there's probably more Android devices,
[00:03:20.100 --> 00:03:21.780]   so that's why it's better.
[00:03:21.780 --> 00:03:23.260]   But on the iPhone side,
[00:03:23.260 --> 00:03:28.420]   so I'm an Android person 'cause I'm a man of the people,
[00:03:28.420 --> 00:03:31.180]   but it seems like all the elites use iPhone,
[00:03:31.180 --> 00:03:33.060]   all the people at nice dinner parties.
[00:03:33.060 --> 00:03:37.300]   So is that the reason that the more powerful people
[00:03:37.300 --> 00:03:38.820]   use iPhones, is that why?
[00:03:38.820 --> 00:03:39.820]   - I don't think so.
[00:03:39.820 --> 00:03:42.500]   I actually, so it was about two years ago
[00:03:42.500 --> 00:03:43.660]   that the prices flipped.
[00:03:43.660 --> 00:03:46.760]   It used to be that if you could craft
[00:03:46.760 --> 00:03:51.760]   a remote zero-click exploit for iOS,
[00:03:51.760 --> 00:03:55.420]   then that was about as good as it gets.
[00:03:55.420 --> 00:04:00.420]   You could sell that to a zero-day broker for $2 million.
[00:04:00.420 --> 00:04:04.680]   The caveat is you can never tell anyone about it
[00:04:04.680 --> 00:04:07.180]   because the minute you tell someone about it,
[00:04:07.180 --> 00:04:08.880]   Apple learns about it,
[00:04:08.880 --> 00:04:12.560]   they patch it in that $2.5 million investment
[00:04:12.560 --> 00:04:16.120]   that that zero-day broker just made goes to dust.
[00:04:16.120 --> 00:04:20.780]   So a couple years ago, and don't quote me on the prices,
[00:04:20.780 --> 00:04:25.780]   but an Android zero-click remote exploit
[00:04:25.780 --> 00:04:29.020]   for the first time topped the iOS.
[00:04:29.020 --> 00:04:32.180]   And actually a lot of people's read on that
[00:04:32.180 --> 00:04:37.180]   was that it might be a sign that Apple security was falling
[00:04:40.280 --> 00:04:43.060]   and that it might actually be easier
[00:04:43.060 --> 00:04:46.700]   to find an iOS zero-day exploit
[00:04:46.700 --> 00:04:48.940]   than find an Android zero-day exploit.
[00:04:48.940 --> 00:04:51.140]   The other thing is market share.
[00:04:51.140 --> 00:04:52.860]   There are just more people around the world
[00:04:52.860 --> 00:04:54.700]   that use Android.
[00:04:54.700 --> 00:04:58.540]   And a lot of governments that are paying top dollar
[00:04:58.540 --> 00:05:01.340]   for zero-day exploits these days
[00:05:01.340 --> 00:05:05.280]   are deep-pocketed governments in the Gulf
[00:05:05.280 --> 00:05:08.920]   that wanna use these exploits to monitor their own citizens,
[00:05:08.920 --> 00:05:10.780]   monitor their critics.
[00:05:10.780 --> 00:05:12.400]   And so it's not necessarily
[00:05:12.400 --> 00:05:14.660]   that they're trying to find elites.
[00:05:14.660 --> 00:05:17.300]   It's that they wanna find out who these people are
[00:05:17.300 --> 00:05:18.540]   that are criticizing them
[00:05:18.540 --> 00:05:21.220]   or perhaps planning the next Arab Spring.
[00:05:21.220 --> 00:05:23.300]   - So in your experience,
[00:05:23.300 --> 00:05:24.980]   are most of these attack targeted
[00:05:24.980 --> 00:05:26.540]   to cover a large population
[00:05:26.540 --> 00:05:29.140]   or is there attacks that are targeted
[00:05:29.140 --> 00:05:31.340]   towards specific individuals?
[00:05:31.340 --> 00:05:32.780]   - So I think it's both.
[00:05:32.780 --> 00:05:37.020]   Some of the zero-day exploits that have fetched top dollar
[00:05:37.020 --> 00:05:39.400]   that I've heard of in my reporting in the United States
[00:05:39.400 --> 00:05:41.320]   were highly targeted.
[00:05:41.320 --> 00:05:43.680]   There was a potential terrorist attack.
[00:05:43.680 --> 00:05:45.520]   They wanted to get into this person's phone.
[00:05:45.520 --> 00:05:48.120]   It had to be done in the next 24 hours.
[00:05:48.120 --> 00:05:50.880]   They approached hackers and say, "We'll pay you
[00:05:50.880 --> 00:05:53.840]   X millions of dollars if you can do this."
[00:05:53.840 --> 00:05:57.480]   But then you look at when we've discovered
[00:05:57.480 --> 00:06:00.800]   iOS zero-day exploits in the wild,
[00:06:00.800 --> 00:06:02.400]   some of them have been targeting
[00:06:02.400 --> 00:06:05.280]   large populations like Uyghurs.
[00:06:05.280 --> 00:06:10.280]   So a couple of years ago, there was a watering hole attack.
[00:06:10.280 --> 00:06:12.180]   Okay, what's a watering hole attack?
[00:06:12.180 --> 00:06:13.300]   There was a website.
[00:06:13.300 --> 00:06:17.900]   It was actually, it had information aimed at Uyghurs
[00:06:17.900 --> 00:06:21.000]   and you could access it all over the world.
[00:06:21.000 --> 00:06:23.280]   And if you visited this website,
[00:06:23.280 --> 00:06:29.180]   it would drop an iOS zero-day exploit onto your phone.
[00:06:29.180 --> 00:06:32.260]   And so anyone that visited this website
[00:06:32.260 --> 00:06:34.500]   that was about Uyghurs, anywhere,
[00:06:34.500 --> 00:06:37.420]   I mean, Uyghurs living abroad,
[00:06:37.420 --> 00:06:39.800]   basically the Uyghur diaspora,
[00:06:39.800 --> 00:06:43.980]   would have gotten infected with this zero-day exploit.
[00:06:43.980 --> 00:06:48.980]   So in that case, they were targeting huge swaths
[00:06:48.980 --> 00:06:51.740]   of this one population or people interested
[00:06:51.740 --> 00:06:54.780]   in this one population basically in real time.
[00:06:54.780 --> 00:06:59.380]   - Who are these attackers?
[00:06:59.380 --> 00:07:02.720]   From the individual level to the group level,
[00:07:02.720 --> 00:07:05.180]   psychologically speaking, what's their motivation?
[00:07:05.180 --> 00:07:07.460]   Is it purely money?
[00:07:07.460 --> 00:07:09.260]   Is it the challenge?
[00:07:09.260 --> 00:07:10.620]   Are they malevolent?
[00:07:10.620 --> 00:07:12.220]   Is it power?
[00:07:12.220 --> 00:07:15.460]   These are big philosophical human questions, I guess.
[00:07:15.460 --> 00:07:18.940]   - So these are the questions I set out to answer
[00:07:18.940 --> 00:07:20.380]   for my book.
[00:07:20.380 --> 00:07:22.040]   I wanted to know,
[00:07:22.040 --> 00:07:25.620]   are these people that are just after money?
[00:07:25.620 --> 00:07:29.700]   If they're just after money, how do they sleep at night
[00:07:29.700 --> 00:07:31.900]   not knowing whether that zero-day exploit
[00:07:31.900 --> 00:07:34.740]   they just sold to a broker is being used
[00:07:34.740 --> 00:07:37.280]   to basically make someone's life a living hell?
[00:07:37.280 --> 00:07:41.220]   And what I found was there's kind of this long-sorted
[00:07:41.220 --> 00:07:43.660]   history to this question.
[00:07:43.660 --> 00:07:46.920]   It started out in the '80s and '90s
[00:07:46.920 --> 00:07:51.200]   when hackers were just finding holes and bugs in software
[00:07:51.200 --> 00:07:54.380]   for curiosity's sake, really as a hobby.
[00:07:54.380 --> 00:07:56.820]   And some of them would go to the tech companies
[00:07:56.820 --> 00:08:01.780]   like Microsoft or Sun Microsystems at the time, or Oracle.
[00:08:01.780 --> 00:08:04.500]   And they'd say, "Hey, I just found this zero-day
[00:08:04.500 --> 00:08:08.140]   in your software and I can use it to break into NASA."
[00:08:08.140 --> 00:08:11.140]   And the general response at the time wasn't,
[00:08:11.140 --> 00:08:13.940]   "Thank you so much for pointing out this flaw
[00:08:13.940 --> 00:08:17.240]   and our software, we'll get it fixed as soon as possible."
[00:08:17.240 --> 00:08:21.200]   It was, "Don't ever poke around our software ever again,
[00:08:21.200 --> 00:08:24.160]   or we'll stick our general counsel on you."
[00:08:24.160 --> 00:08:29.160]   And that was really sort of the common thread for years.
[00:08:30.380 --> 00:08:34.420]   And so hackers who set out to do the right thing
[00:08:34.420 --> 00:08:37.660]   were basically told to shut up
[00:08:37.660 --> 00:08:40.140]   and stop doing what you're doing.
[00:08:40.140 --> 00:08:44.340]   And what happened next was they basically started
[00:08:44.340 --> 00:08:46.220]   trading this information online.
[00:08:46.220 --> 00:08:48.180]   Now, when you go back and interview people
[00:08:48.180 --> 00:08:53.020]   from those early days, they all tell a very similar story,
[00:08:53.020 --> 00:08:57.260]   which is they're curious, they're tinkers.
[00:08:57.260 --> 00:08:59.740]   You know, they remind me of like the kid down the block
[00:08:59.740 --> 00:09:03.500]   that was constantly poking around the hood of his dad's car.
[00:09:03.500 --> 00:09:06.140]   You know, they just couldn't help themselves.
[00:09:06.140 --> 00:09:09.820]   They wanted to figure out how a system is designed
[00:09:09.820 --> 00:09:11.860]   and how they could potentially exploit it
[00:09:11.860 --> 00:09:13.080]   for some other purpose.
[00:09:13.080 --> 00:09:14.900]   It doesn't have to be good or bad.
[00:09:14.900 --> 00:09:20.620]   But they were basically kind of beat down for so long
[00:09:20.620 --> 00:09:22.940]   by these big tech companies
[00:09:22.940 --> 00:09:26.620]   that they started just silently trading them
[00:09:26.620 --> 00:09:28.060]   with other hackers.
[00:09:28.060 --> 00:09:32.620]   And that's how you got these really heated debates
[00:09:32.620 --> 00:09:34.740]   in the '90s about disclosure.
[00:09:34.740 --> 00:09:38.140]   Should you just dump these things online?
[00:09:38.140 --> 00:09:40.300]   Because any script kiddie can pick them up
[00:09:40.300 --> 00:09:42.540]   and use it for all kinds of mischief.
[00:09:42.540 --> 00:09:46.100]   But, you know, don't you wanna just stick a middle finger
[00:09:46.100 --> 00:09:47.260]   to all these companies
[00:09:47.260 --> 00:09:50.020]   that are basically threatening you all the time?
[00:09:50.020 --> 00:09:53.580]   So there was this really interesting dynamic at play.
[00:09:53.580 --> 00:09:57.060]   And what I learned in the course of doing my book
[00:09:57.060 --> 00:10:01.740]   was that government agencies and their contractors
[00:10:01.740 --> 00:10:06.180]   sort of tapped into that frustration and that resentment.
[00:10:06.180 --> 00:10:09.140]   And they started quietly reaching out to hackers
[00:10:09.140 --> 00:10:11.100]   on these forums.
[00:10:11.100 --> 00:10:13.820]   And they said, "Hey, you know that zero day
[00:10:13.820 --> 00:10:14.940]   you just dropped online?
[00:10:14.940 --> 00:10:17.980]   Could you come up with something custom for me?
[00:10:17.980 --> 00:10:20.060]   And I'll pay you six figures for it
[00:10:20.060 --> 00:10:22.300]   so long as you shut up and never tell anyone
[00:10:22.300 --> 00:10:24.740]   that I paid you for this."
[00:10:24.740 --> 00:10:27.060]   And that's what happened.
[00:10:27.060 --> 00:10:28.420]   So throughout the '90s,
[00:10:28.420 --> 00:10:31.460]   there was a bunch of boutique contractors
[00:10:31.460 --> 00:10:34.660]   that started reaching out to hackers on these forums
[00:10:34.660 --> 00:10:37.340]   and saying, "Hey, I'll pay you six figures
[00:10:37.340 --> 00:10:39.620]   for that bug you were trying to get Microsoft
[00:10:39.620 --> 00:10:41.420]   to fix for free."
[00:10:41.420 --> 00:10:45.540]   And sort of so began or so catalyzed this market
[00:10:45.540 --> 00:10:48.020]   where governments and their intermediaries
[00:10:48.020 --> 00:10:50.420]   started reaching out to these hackers
[00:10:50.420 --> 00:10:53.060]   and buying their bugs for free.
[00:10:53.060 --> 00:10:55.100]   And in those early days, I think a lot of it
[00:10:55.100 --> 00:10:57.620]   was just for quiet counterintelligence,
[00:10:57.620 --> 00:11:00.060]   traditional espionage.
[00:11:00.060 --> 00:11:04.260]   But as we started baking the software,
[00:11:04.260 --> 00:11:07.060]   Windows software, Schneider Electric,
[00:11:07.060 --> 00:11:11.300]   Siemens Industrial Software, into our nuclear plants
[00:11:11.300 --> 00:11:14.940]   and our factories and our power grid
[00:11:14.940 --> 00:11:18.720]   and our petrochemical facilities and our pipelines,
[00:11:18.720 --> 00:11:22.060]   those same zero days came to be just as valuable
[00:11:22.060 --> 00:11:25.140]   for sabotage and war planning.
[00:11:25.140 --> 00:11:27.220]   - Does the fact that the market sprung up
[00:11:27.220 --> 00:11:28.500]   and you can now make a lot of money
[00:11:28.500 --> 00:11:32.100]   change the nature of the attackers that came to the table
[00:11:32.100 --> 00:11:34.540]   or grow the number of attackers?
[00:11:34.540 --> 00:11:35.740]   I mean, what is, I guess,
[00:11:35.740 --> 00:11:40.580]   you told the psychology of the hackers in the '90s,
[00:11:40.580 --> 00:11:44.020]   what is the culture today and where is it heading?
[00:11:44.020 --> 00:11:47.140]   - So I think there are people who will tell you
[00:11:47.140 --> 00:11:49.100]   they would never sell a zero day
[00:11:49.100 --> 00:11:52.100]   to a zero day broker or a government.
[00:11:52.100 --> 00:11:54.580]   One, because they don't know how it's gonna get used
[00:11:54.580 --> 00:11:56.380]   when they throw it over the fence.
[00:11:56.380 --> 00:11:58.820]   Most of these get rolled into classified programs
[00:11:58.820 --> 00:12:01.300]   and you don't know how they get used.
[00:12:01.300 --> 00:12:02.700]   If you sell it to a zero day broker,
[00:12:02.700 --> 00:12:05.500]   you don't even know which nation state might use it
[00:12:05.500 --> 00:12:09.340]   or potentially which criminal group might use it
[00:12:09.340 --> 00:12:11.400]   if you sell it on the dark web.
[00:12:11.400 --> 00:12:15.420]   The other thing that they say is that
[00:12:15.420 --> 00:12:18.020]   they wanna be able to sleep at night
[00:12:18.020 --> 00:12:19.940]   and they lose a lot of sleep
[00:12:19.940 --> 00:12:22.140]   if they found out their zero day was being used
[00:12:22.140 --> 00:12:24.860]   to make a dissidence life living hell.
[00:12:24.860 --> 00:12:28.500]   But there are a lot of people, good people,
[00:12:28.500 --> 00:12:32.620]   who also say, no, this is not my problem.
[00:12:32.620 --> 00:12:35.380]   This is the technology company's problem.
[00:12:35.380 --> 00:12:37.020]   If they weren't writing new bugs
[00:12:37.020 --> 00:12:39.180]   into their software every day,
[00:12:39.180 --> 00:12:41.060]   then there wouldn't be a market,
[00:12:41.060 --> 00:12:42.860]   then there wouldn't be a problem.
[00:12:42.860 --> 00:12:44.780]   But they continue to write bugs
[00:12:44.780 --> 00:12:46.180]   into their software all the time
[00:12:46.180 --> 00:12:48.300]   and they continue to profit off that software.
[00:12:48.300 --> 00:12:53.300]   So why shouldn't I profit off my labor too?
[00:12:53.300 --> 00:12:55.740]   And one of the things that has happened,
[00:12:55.740 --> 00:12:57.980]   which is I think a positive development
[00:12:57.980 --> 00:13:02.260]   over the last 10 years, are bug bounty programs.
[00:13:02.260 --> 00:13:06.180]   Companies like Google and Facebook and then Microsoft
[00:13:06.180 --> 00:13:10.180]   and finally Apple, which resisted it for a really long time,
[00:13:10.180 --> 00:13:14.780]   have said, okay, we are gonna shift our perspective
[00:13:14.780 --> 00:13:17.400]   about hackers, we're no longer going to treat them
[00:13:17.400 --> 00:13:18.960]   as the enemy here.
[00:13:18.960 --> 00:13:20.340]   We're going to start paying them
[00:13:20.340 --> 00:13:23.800]   for what it's essentially free quality assurance.
[00:13:23.800 --> 00:13:26.820]   And we're gonna pay them good money in some cases,
[00:13:26.820 --> 00:13:28.900]   six figures in some cases.
[00:13:28.900 --> 00:13:32.380]   We're never gonna be able to bid against a zero day broker
[00:13:32.380 --> 00:13:34.740]   who sells to government agencies.
[00:13:34.740 --> 00:13:38.900]   But we can reward them and hopefully get to that bug earlier
[00:13:38.900 --> 00:13:40.980]   where we can neutralize it
[00:13:40.980 --> 00:13:43.060]   so that they don't have to spend another year
[00:13:43.060 --> 00:13:44.760]   developing the zero day exploit.
[00:13:44.760 --> 00:13:48.180]   And in that way, we can keep our software more secure.
[00:13:48.180 --> 00:13:53.180]   But every week I get messages from some hacker that says,
[00:13:53.180 --> 00:13:55.940]   I tried to see this zero day exploit
[00:13:55.940 --> 00:13:58.020]   that was just found in the wild,
[00:13:58.020 --> 00:14:00.380]   being used by this nation state.
[00:14:00.380 --> 00:14:04.540]   I tried to tell Microsoft about this two years ago
[00:14:04.540 --> 00:14:08.900]   and they were gonna pay me peanuts, so it never got fixed.
[00:14:08.900 --> 00:14:12.700]   There are all sorts of those stories that can continue on.
[00:14:12.700 --> 00:14:16.700]   And I think just generally,
[00:14:16.700 --> 00:14:19.860]   hackers are not very good at diplomacy.
[00:14:19.860 --> 00:14:24.500]   They tend to be pretty snipey, technical crowd
[00:14:24.500 --> 00:14:28.260]   and very philosophical in my experience.
[00:14:28.260 --> 00:14:31.900]   But diplomacy is not their strong suit.
[00:14:31.900 --> 00:14:33.700]   - Well, there almost has to be a broker
[00:14:33.700 --> 00:14:35.980]   between companies and hackers
[00:14:35.980 --> 00:14:38.000]   where you can translate effectively,
[00:14:38.000 --> 00:14:39.540]   just like you have a zero day broker
[00:14:39.540 --> 00:14:41.700]   between governments and hackers.
[00:14:41.700 --> 00:14:43.100]   You have to speak their language.
[00:14:43.100 --> 00:14:45.900]   - Yeah, and there have been some of those companies
[00:14:45.900 --> 00:14:47.780]   who've risen up to meet that demand
[00:14:47.780 --> 00:14:52.460]   and HackerOne is one of them, BugCrowd is another.
[00:14:52.460 --> 00:14:54.280]   Synack has an interesting model.
[00:14:54.280 --> 00:14:56.900]   So that's a company that you pay
[00:14:56.900 --> 00:14:59.900]   for a private bug bounty program, essentially.
[00:14:59.900 --> 00:15:00.940]   So you pay this company,
[00:15:00.940 --> 00:15:04.460]   they tap hackers all over the world
[00:15:04.460 --> 00:15:07.580]   to come hack your software, hack your system.
[00:15:07.580 --> 00:15:10.820]   And then they'll quietly tell you what they found.
[00:15:10.820 --> 00:15:13.740]   And I think that's a really positive development.
[00:15:13.740 --> 00:15:16.300]   And actually the Department of Defense
[00:15:16.300 --> 00:15:20.460]   hired all three of those companies I just mentioned
[00:15:20.460 --> 00:15:22.020]   to help secure their systems.
[00:15:22.020 --> 00:15:24.140]   Now, I think they're still a little timid
[00:15:24.140 --> 00:15:25.780]   in terms of letting those hackers
[00:15:25.780 --> 00:15:30.300]   into the really sensitive high side classified stuff,
[00:15:30.300 --> 00:15:33.080]   but baby steps. (laughs)
[00:15:33.080 --> 00:15:34.740]   - Just to understand what you were saying,
[00:15:34.740 --> 00:15:37.780]   you think it's impossible for companies
[00:15:37.780 --> 00:15:40.540]   to financially compete with the zero day brokers,
[00:15:40.540 --> 00:15:42.020]   with governments.
[00:15:42.020 --> 00:15:47.020]   So like the defense can't outpay the hackers?
[00:15:47.020 --> 00:15:51.860]   - It's interesting, they shouldn't outpay them
[00:15:51.860 --> 00:15:56.180]   because what would happen if they started offering
[00:15:56.180 --> 00:16:01.180]   $2.5 million at Apple for any zero day exploit
[00:16:01.180 --> 00:16:04.740]   that governments would pay that much for
[00:16:04.740 --> 00:16:06.260]   is their own engineers would say,
[00:16:06.260 --> 00:16:10.300]   "Why the hell am I working for less than that?"
[00:16:10.300 --> 00:16:12.420]   And doing my nine to five every day.
[00:16:12.420 --> 00:16:14.740]   So you would create a perverse incentive.
[00:16:14.740 --> 00:16:16.940]   And I didn't think about that
[00:16:16.940 --> 00:16:19.220]   until I started this research and I realized,
[00:16:19.220 --> 00:16:20.540]   okay, yeah, that makes sense.
[00:16:20.540 --> 00:16:25.300]   You don't want to incentivize offense so much
[00:16:25.300 --> 00:16:27.500]   that it's to your own detriment.
[00:16:27.500 --> 00:16:29.540]   And so I think what they have though,
[00:16:29.540 --> 00:16:32.660]   what the companies have on government agencies
[00:16:32.660 --> 00:16:36.460]   is if they pay you, you get to talk about it.
[00:16:36.460 --> 00:16:38.180]   You get the street cred.
[00:16:38.180 --> 00:16:41.260]   You get to brag about the fact you just found
[00:16:41.260 --> 00:16:46.260]   that $2.5 million iOS zero day that no one else did.
[00:16:46.260 --> 00:16:48.780]   And if you sell it to a broker,
[00:16:48.780 --> 00:16:50.100]   you never get to talk about it.
[00:16:50.100 --> 00:16:52.300]   And I think that really does eat at people.
[00:16:52.300 --> 00:16:55.140]   - Can I ask you a big philosophical question
[00:16:55.140 --> 00:16:56.540]   about human nature here?
[00:16:56.540 --> 00:16:57.380]   (laughing)
[00:16:57.380 --> 00:17:00.820]   So if you have, in what you've seen,
[00:17:00.820 --> 00:17:03.460]   if a human being has a zero day,
[00:17:03.460 --> 00:17:06.460]   they found a zero day vulnerability
[00:17:06.460 --> 00:17:09.860]   that can hack into, I don't know,
[00:17:09.860 --> 00:17:11.980]   what's the worst thing you can hack into?
[00:17:11.980 --> 00:17:14.900]   Something that could launch nuclear weapons.
[00:17:14.900 --> 00:17:16.980]   Which percentage of the people in the world
[00:17:16.980 --> 00:17:20.660]   that have the skill would not share that with anyone,
[00:17:20.660 --> 00:17:23.420]   with any bad party?
[00:17:23.420 --> 00:17:27.740]   I guess how many people are completely devoid
[00:17:27.740 --> 00:17:31.780]   of ethical concerns in your sense?
[00:17:31.780 --> 00:17:36.340]   So my belief is all the ultra competent people
[00:17:36.340 --> 00:17:39.580]   or very, very high percentage of ultra competent people
[00:17:39.580 --> 00:17:41.580]   are also ethical people.
[00:17:41.580 --> 00:17:42.980]   That's been my experience.
[00:17:42.980 --> 00:17:45.900]   But then again, my experience is narrow.
[00:17:45.900 --> 00:17:48.500]   What's your experience been like?
[00:17:48.500 --> 00:17:52.620]   - So this was another question I wanted to answer.
[00:17:52.620 --> 00:17:57.740]   Who are these people who would sell a zero day exploit
[00:17:57.740 --> 00:18:01.420]   that would neutralize a Schneider Electric safety lock
[00:18:01.420 --> 00:18:03.100]   at a petrochemical plant?
[00:18:03.100 --> 00:18:05.380]   Basically the last thing you would need to neutralize
[00:18:05.380 --> 00:18:07.980]   before you trigger some kind of explosion.
[00:18:07.980 --> 00:18:09.100]   Who would sell that?
[00:18:09.100 --> 00:18:14.100]   And I got my answer.
[00:18:14.100 --> 00:18:16.780]   Well, the answer was different.
[00:18:16.780 --> 00:18:19.860]   A lot of people said, I would never even look there
[00:18:19.860 --> 00:18:21.020]   'cause I don't even wanna know.
[00:18:21.020 --> 00:18:23.020]   I don't even wanna have that capability.
[00:18:23.020 --> 00:18:26.580]   I don't even wanna have to make that decision
[00:18:26.580 --> 00:18:29.940]   about whether I'm gonna profit off of that knowledge.
[00:18:29.940 --> 00:18:31.820]   I went down to Argentina
[00:18:31.860 --> 00:18:36.860]   and this whole kind of moral calculus I had in my head
[00:18:36.860 --> 00:18:39.260]   was completely flipped around.
[00:18:39.260 --> 00:18:41.620]   So just to back up for a moment.
[00:18:41.620 --> 00:18:46.460]   So Argentina actually is a real hacker's paradise.
[00:18:46.460 --> 00:18:50.820]   People grew up in Argentina and I went down there,
[00:18:50.820 --> 00:18:54.260]   I guess I was there around 2015, 2016,
[00:18:54.260 --> 00:18:56.620]   but you still couldn't get an iPhone.
[00:18:56.620 --> 00:18:58.940]   They didn't have Amazon Prime.
[00:18:58.940 --> 00:19:00.860]   You couldn't get access to any of the apps
[00:19:00.860 --> 00:19:02.620]   we all take for granted.
[00:19:02.620 --> 00:19:05.300]   To get those things in Argentina as a kid,
[00:19:05.300 --> 00:19:07.620]   you have to find a way to hack 'em.
[00:19:07.620 --> 00:19:12.100]   And it's the whole culture is really like a hacker culture.
[00:19:12.100 --> 00:19:15.300]   They say like, it's really like a MacGyver culture.
[00:19:15.300 --> 00:19:17.620]   You have to figure out how to break into something
[00:19:17.620 --> 00:19:19.500]   with wire and tape.
[00:19:19.500 --> 00:19:24.460]   And that means that there are a lot of really good hackers
[00:19:24.460 --> 00:19:29.460]   in Argentina who specialize in developing zero-day exploits.
[00:19:30.580 --> 00:19:34.020]   And I went down to this Argentina conference
[00:19:34.020 --> 00:19:35.740]   called Echo Party.
[00:19:35.740 --> 00:19:38.220]   And I asked the organizer,
[00:19:38.220 --> 00:19:40.060]   okay, can you introduce me to someone
[00:19:40.060 --> 00:19:43.100]   who's selling zero-day exploits to governments?
[00:19:43.100 --> 00:19:45.020]   And he was like, just throw a stone.
[00:19:45.020 --> 00:19:48.940]   Throw a stone anywhere and you're gonna hit someone.
[00:19:48.940 --> 00:19:50.700]   And all over this conference,
[00:19:50.700 --> 00:19:54.140]   you saw these guys who were clearly from these Gulf States
[00:19:54.140 --> 00:19:55.700]   who only spoke Arabic.
[00:19:55.700 --> 00:19:59.380]   What are they doing at a young hacking conference
[00:19:59.380 --> 00:20:01.180]   in Buenos Aires? - Oh boy.
[00:20:01.180 --> 00:20:03.260]   - And so I went out to lunch
[00:20:03.260 --> 00:20:07.340]   with kind of this godfather of the hacking scene there.
[00:20:07.340 --> 00:20:10.260]   And I asked this really dumb question
[00:20:10.260 --> 00:20:13.180]   and I'm still embarrassed about how I phrased it.
[00:20:13.180 --> 00:20:14.620]   But I said, so, you know,
[00:20:14.620 --> 00:20:17.700]   well, these guys only sell these zero-day exploits
[00:20:17.700 --> 00:20:20.380]   to good Western governments.
[00:20:20.380 --> 00:20:22.380]   And he said, Nicole, last time I checked,
[00:20:22.380 --> 00:20:25.660]   the United States wasn't a good Western government.
[00:20:25.660 --> 00:20:28.260]   You know, the last country that bombed another country
[00:20:28.260 --> 00:20:31.900]   into oblivion wasn't China or Iran.
[00:20:31.900 --> 00:20:33.580]   It was the United States.
[00:20:33.580 --> 00:20:36.380]   So if we're gonna go by your whole moral calculus,
[00:20:36.380 --> 00:20:37.540]   you know, just know that we have
[00:20:37.540 --> 00:20:39.860]   a very different calculus down here.
[00:20:39.860 --> 00:20:44.780]   And we'd actually rather sell to Iran or Russia or China,
[00:20:44.780 --> 00:20:46.900]   maybe, than the United States.
[00:20:46.900 --> 00:20:48.860]   And that just blew me away.
[00:20:48.860 --> 00:20:50.020]   Like, wow.
[00:20:50.020 --> 00:20:51.860]   You know, he's like, we'll just sell
[00:20:51.860 --> 00:20:53.940]   to whoever brings us the biggest bag of cash.
[00:20:53.940 --> 00:20:57.980]   Have you checked into our inflation situation recently?
[00:20:57.980 --> 00:20:59.980]   So, you know, I had some of those,
[00:20:59.980 --> 00:21:02.260]   like, reality checks along the way.
[00:21:02.260 --> 00:21:04.500]   You know, we tend to think of things as,
[00:21:04.500 --> 00:21:06.980]   is this moral, you know, is this ethical,
[00:21:06.980 --> 00:21:08.780]   especially as journalists.
[00:21:08.780 --> 00:21:11.020]   You know, we kind of sit on our high horse sometimes
[00:21:11.020 --> 00:21:13.660]   and write about a lot of things
[00:21:13.660 --> 00:21:16.380]   that seem to push the moral bounds.
[00:21:16.380 --> 00:21:17.820]   But in this market,
[00:21:17.820 --> 00:21:20.220]   which is essentially an underground market,
[00:21:20.220 --> 00:21:22.420]   that, you know, the one rule is like Fight Club,
[00:21:22.420 --> 00:21:24.420]   you know, no one talks about Fight Club.
[00:21:24.420 --> 00:21:25.940]   First rule of the zero-day market,
[00:21:25.940 --> 00:21:27.820]   nobody talks about the zero-day market.
[00:21:27.820 --> 00:21:29.180]   On both sides.
[00:21:29.180 --> 00:21:30.900]   Because the hacker doesn't wanna lose
[00:21:30.900 --> 00:21:33.940]   their $2.5 million bounty.
[00:21:33.940 --> 00:21:36.860]   And governments roll these into classified programs
[00:21:36.860 --> 00:21:39.180]   and they don't want anyone to know what they have.
[00:21:39.180 --> 00:21:41.220]   So no one talks about this thing.
[00:21:41.220 --> 00:21:43.860]   And when you're operating in the dark like that,
[00:21:43.860 --> 00:21:47.000]   it's really easy to put aside your morals sometimes.
[00:21:47.000 --> 00:21:52.060]   - Can I, as a small tangent, ask you, by way of advice,
[00:21:52.060 --> 00:21:55.580]   you must have done some incredible interviews.
[00:21:55.580 --> 00:21:58.300]   And you've also spoken about how serious
[00:21:58.300 --> 00:22:00.060]   you take protecting your sources.
[00:22:00.060 --> 00:22:04.420]   If you were to give me advice for interviewing
[00:22:04.420 --> 00:22:07.980]   when you're recording on mic with a video camera,
[00:22:07.980 --> 00:22:13.060]   how is it possible to get into this world?
[00:22:13.060 --> 00:22:16.060]   Like, is it basically impossible?
[00:22:16.060 --> 00:22:19.020]   So you've spoken with a few people,
[00:22:19.020 --> 00:22:23.300]   what is it, like the godfather of cyber war, cybersecurity?
[00:22:23.300 --> 00:22:25.260]   So people that are already out.
[00:22:25.260 --> 00:22:28.440]   And they still have to be pretty brave to speak publicly.
[00:22:28.440 --> 00:22:32.740]   But is it virtually impossible to really talk to anybody
[00:22:32.740 --> 00:22:34.660]   who's a current hacker?
[00:22:34.660 --> 00:22:37.740]   You're always like 10, 20 years behind?
[00:22:37.740 --> 00:22:38.740]   - It's a good question.
[00:22:38.740 --> 00:22:40.880]   And this is why I'm a print journalist.
[00:22:40.880 --> 00:22:45.940]   But, you know, a lot, when I've seen people do it,
[00:22:45.940 --> 00:22:49.260]   it's always the guy who's behind the shadows,
[00:22:49.260 --> 00:22:51.580]   whose voice has been altered.
[00:22:51.580 --> 00:22:53.420]   You know, when they've gotten someone on camera,
[00:22:53.420 --> 00:22:55.160]   that's usually how they do it.
[00:22:55.160 --> 00:22:59.020]   You know, very, very few people talk in this space.
[00:22:59.020 --> 00:23:02.060]   And there's actually a pretty well known case study
[00:23:02.060 --> 00:23:04.460]   in why you don't talk publicly in this space
[00:23:04.460 --> 00:23:06.100]   and you don't get photographed.
[00:23:06.100 --> 00:23:07.420]   And that's the gruck.
[00:23:07.420 --> 00:23:12.060]   So, you know, the gruck is or was this zero day broker,
[00:23:12.060 --> 00:23:15.260]   South African guy, lives in Thailand.
[00:23:15.260 --> 00:23:18.860]   And right when I was starting on this subject
[00:23:18.860 --> 00:23:22.700]   at the New York Times, he'd given an interview to Forbes.
[00:23:22.700 --> 00:23:25.580]   And he talked about being a zero day broker.
[00:23:25.580 --> 00:23:29.180]   And he even posed next to this giant duffel bag
[00:23:29.180 --> 00:23:31.740]   filled with cash, ostensibly.
[00:23:31.740 --> 00:23:35.480]   And later he would say he was speaking off the record,
[00:23:35.480 --> 00:23:38.020]   he didn't understand the rules of the game.
[00:23:38.020 --> 00:23:41.020]   But what I heard from people who did business with him
[00:23:41.020 --> 00:23:43.120]   was that the minute that that story came out,
[00:23:43.120 --> 00:23:45.320]   he became PNG'd.
[00:23:45.320 --> 00:23:47.300]   No one did business with him.
[00:23:47.300 --> 00:23:50.100]   You know, his business plummeted by at least half.
[00:23:50.100 --> 00:23:52.140]   No one wants to do business with anyone
[00:23:52.140 --> 00:23:54.420]   who's gonna get on camera and talk about
[00:23:54.420 --> 00:23:56.680]   how they're selling zero days to governments.
[00:23:56.680 --> 00:23:59.740]   It puts you at danger.
[00:23:59.740 --> 00:24:01.660]   And I did hear that he got some visits
[00:24:01.660 --> 00:24:04.040]   from some security folks.
[00:24:04.040 --> 00:24:05.060]   And, you know, it's another thing
[00:24:05.060 --> 00:24:06.940]   for these people to consider.
[00:24:06.940 --> 00:24:11.940]   If they have those zero day exploits at their disposal,
[00:24:11.940 --> 00:24:16.420]   they become a huge target for nation states
[00:24:16.420 --> 00:24:18.140]   all over the world.
[00:24:18.140 --> 00:24:20.660]   You know, talk about having perfect OPSEC.
[00:24:20.660 --> 00:24:23.580]   You know, you better have some perfect OPSEC
[00:24:23.580 --> 00:24:25.620]   if people know that you have access
[00:24:25.620 --> 00:24:27.900]   to those zero day exploits.
[00:24:27.900 --> 00:24:32.900]   - Which sucks because, I mean, transparency here
[00:24:32.900 --> 00:24:36.460]   would be really powerful for educating the world
[00:24:36.460 --> 00:24:40.180]   and also inspiring other engineers to do good.
[00:24:40.180 --> 00:24:42.780]   It just feels like when you're operating in shadows,
[00:24:42.780 --> 00:24:46.380]   it doesn't help us move in the positive direction
[00:24:46.380 --> 00:24:48.980]   in terms of like getting more people on the defense side
[00:24:48.980 --> 00:24:50.380]   versus on the attack side.
[00:24:50.380 --> 00:24:51.220]   - Right.
[00:24:51.220 --> 00:24:52.060]   - But of course, what can you do?
[00:24:52.060 --> 00:24:53.660]   I mean, the best you can possibly do
[00:24:53.660 --> 00:24:57.060]   is have great journalists, just like you did,
[00:24:57.060 --> 00:24:58.900]   interview and write books about it
[00:24:58.900 --> 00:25:01.060]   and integrate the information you get
[00:25:01.060 --> 00:25:02.940]   while hiding the sources.
[00:25:02.940 --> 00:25:04.580]   - Yeah, and I think, you know,
[00:25:04.580 --> 00:25:07.620]   what HackerOne has told me was,
[00:25:07.620 --> 00:25:09.180]   okay, let's just put away the people
[00:25:09.180 --> 00:25:13.240]   that are finding and developing zero day exploits
[00:25:13.240 --> 00:25:15.540]   all day long, let's put that aside.
[00:25:15.540 --> 00:25:18.540]   What about the, you know, however many millions
[00:25:18.540 --> 00:25:20.740]   of programmers all over the world
[00:25:20.740 --> 00:25:23.300]   who've never even heard of a zero day exploit?
[00:25:23.300 --> 00:25:25.980]   Why not tap into them and say,
[00:25:25.980 --> 00:25:29.220]   "Hey, we'll start paying you if you can find a bug
[00:25:29.220 --> 00:25:34.140]   in United Airlines software or in Schneider Electric
[00:25:34.140 --> 00:25:36.880]   or in Ford or Tesla."
[00:25:36.880 --> 00:25:39.940]   And I think that is a really smart approach.
[00:25:39.940 --> 00:25:43.940]   Let's go find this untapped army of programmers
[00:25:43.940 --> 00:25:46.900]   to neutralize these bugs before the people
[00:25:46.900 --> 00:25:49.160]   who will continue to sell these to governments
[00:25:49.160 --> 00:25:50.860]   can find them and exploit them.
[00:25:50.860 --> 00:25:53.220]   - Okay, I have to ask you about this.
[00:25:53.220 --> 00:25:55.780]   From a personal side, it's funny enough,
[00:25:55.780 --> 00:25:59.320]   after we agreed to talk, I've gotten,
[00:25:59.320 --> 00:26:01.180]   for the first time in my life,
[00:26:01.180 --> 00:26:03.920]   was a victim of a cyber attack.
[00:26:03.920 --> 00:26:08.580]   So this is ransomware, it's called Deadbolt.
[00:26:08.580 --> 00:26:10.060]   People can look it up.
[00:26:10.060 --> 00:26:15.060]   I have a QNAP device for basically kind of coldish storage.
[00:26:15.620 --> 00:26:20.260]   So it's about 60 terabytes with 50 terabytes of data on it
[00:26:20.260 --> 00:26:25.260]   in RAID 5 and apparently about four to 5,000 QNAP devices
[00:26:25.260 --> 00:26:30.660]   were hacked and taken over with this ransomware.
[00:26:30.660 --> 00:26:35.540]   And what ransomware does there is it goes file by file,
[00:26:35.540 --> 00:26:39.020]   almost all the files on the QNAP storage device
[00:26:39.020 --> 00:26:40.420]   and encrypts them.
[00:26:40.420 --> 00:26:43.020]   And then there's this very eloquently
[00:26:43.020 --> 00:26:45.240]   and politely written page that pops up.
[00:26:45.240 --> 00:26:48.300]   It describes what happened.
[00:26:48.300 --> 00:26:50.060]   All your files have been encrypted.
[00:26:50.060 --> 00:26:52.140]   This includes, but is not limited to photos,
[00:26:52.140 --> 00:26:53.820]   documents, and spreadsheets.
[00:26:53.820 --> 00:26:54.740]   Why me?
[00:26:54.740 --> 00:26:57.980]   This is, a lot of people commented
[00:26:57.980 --> 00:27:00.780]   about how friendly and eloquent this is written.
[00:27:00.780 --> 00:27:01.820]   And I have to commend them.
[00:27:01.820 --> 00:27:04.060]   It is, and it's pretty user-friendly.
[00:27:04.060 --> 00:27:06.860]   Why me?
[00:27:06.860 --> 00:27:08.100]   This is not a personal attack.
[00:27:08.100 --> 00:27:11.060]   You have been targeted because of the inadequate security
[00:27:11.060 --> 00:27:13.580]   provided by your vendor, QNAP.
[00:27:13.580 --> 00:27:16.340]   What now?
[00:27:16.340 --> 00:27:19.820]   You can make a payment of exactly 0.03 Bitcoin,
[00:27:19.820 --> 00:27:21.540]   which is about a thousand dollars,
[00:27:21.540 --> 00:27:23.380]   to the following address.
[00:27:23.380 --> 00:27:24.860]   Once the payment has been made,
[00:27:24.860 --> 00:27:27.540]   we'll follow up with transaction to the same address,
[00:27:27.540 --> 00:27:28.600]   blah, blah, blah.
[00:27:28.600 --> 00:27:31.540]   They give you instructions of what happens next
[00:27:31.540 --> 00:27:34.620]   and they'll give you a decryption key that you can then use.
[00:27:34.620 --> 00:27:37.600]   And then there's another message for QNAP that says,
[00:27:37.600 --> 00:27:40.900]   all your affected customers have been targeted
[00:27:40.900 --> 00:27:43.860]   using a zero-day vulnerability in your product.
[00:27:43.860 --> 00:27:48.420]   We offer you two options to mitigate this and future damage.
[00:27:48.420 --> 00:27:51.780]   One, make a Bitcoin payment of five Bitcoin
[00:27:51.780 --> 00:27:53.820]   to the following address,
[00:27:53.820 --> 00:27:56.340]   and that will reveal to QNAP the,
[00:27:56.340 --> 00:27:57.940]   I'm summarizing things here,
[00:27:57.940 --> 00:28:00.180]   what the actual vulnerability is.
[00:28:00.180 --> 00:28:03.900]   Or you can make a Bitcoin payment of 50 Bitcoin
[00:28:03.900 --> 00:28:07.060]   to get a master decryption key for all your customers.
[00:28:07.060 --> 00:28:09.140]   50 Bitcoin is about $1.8 million.
[00:28:10.380 --> 00:28:11.980]   - Okay.
[00:28:11.980 --> 00:28:14.500]   So first of all, on a personal level,
[00:28:14.500 --> 00:28:16.300]   this one hurt for me.
[00:28:16.300 --> 00:28:21.100]   There's, I mean, I learned a lot
[00:28:21.100 --> 00:28:23.820]   'cause I wasn't, for the most part,
[00:28:23.820 --> 00:28:27.020]   backing up much of that data
[00:28:27.020 --> 00:28:30.900]   because I thought I can afford to lose that data.
[00:28:30.900 --> 00:28:32.460]   It's not like horrible.
[00:28:32.460 --> 00:28:35.900]   I mean, I think you've spoken about the crown jewels,
[00:28:35.900 --> 00:28:38.380]   like making sure there's things you really protect.
[00:28:38.380 --> 00:28:43.300]   And I have, I'm very conscious security-wise
[00:28:43.300 --> 00:28:45.100]   on the crown jewels.
[00:28:45.100 --> 00:28:49.020]   But there's a bunch of stuff, like personal videos.
[00:28:49.020 --> 00:28:50.900]   They're not, I don't have anything creepy,
[00:28:50.900 --> 00:28:53.340]   but just fun things I did
[00:28:53.340 --> 00:28:56.100]   that because they're very large or 4K or something like that,
[00:28:56.100 --> 00:28:59.740]   I kept them on there thinking RAID 5 will protect it.
[00:28:59.740 --> 00:29:01.260]   And just, I lost a bunch of stuff,
[00:29:01.260 --> 00:29:06.100]   including raw footage from interviews
[00:29:06.100 --> 00:29:07.460]   and all that kind of stuff.
[00:29:08.340 --> 00:29:09.540]   So it's painful.
[00:29:09.540 --> 00:29:12.100]   And I'm sure there's a lot of painful stuff like that
[00:29:12.100 --> 00:29:15.580]   for the four to 5,000 people that use QNAP.
[00:29:15.580 --> 00:29:18.460]   And there's a lot of interesting ethical questions here.
[00:29:18.460 --> 00:29:19.460]   Do you pay them?
[00:29:19.460 --> 00:29:22.020]   Does QNAP pay them?
[00:29:22.020 --> 00:29:25.660]   Do the individuals pay them?
[00:29:25.660 --> 00:29:29.060]   Especially when you don't know if it's going to work or not.
[00:29:29.060 --> 00:29:30.260]   Do you wait?
[00:29:30.260 --> 00:29:33.840]   So QNAP said that please don't pay them.
[00:29:35.860 --> 00:29:39.060]   We're working very hard day and night to solve this.
[00:29:39.060 --> 00:29:44.100]   It's so philosophically interesting to me
[00:29:44.100 --> 00:29:46.580]   because I also project onto them thinking,
[00:29:46.580 --> 00:29:48.260]   what is their motivation?
[00:29:48.260 --> 00:29:51.940]   Because the way they phrased it on purpose, perhaps,
[00:29:51.940 --> 00:29:53.360]   but I'm not sure if that actually reflects
[00:29:53.360 --> 00:29:54.820]   their real motivation,
[00:29:54.820 --> 00:29:59.260]   is maybe they're trying to help themselves sleep at night.
[00:29:59.260 --> 00:30:01.300]   Basically saying, this is not about you.
[00:30:01.300 --> 00:30:04.380]   This is about the company with the vulnerabilities.
[00:30:04.380 --> 00:30:07.340]   Just like you mentioned, this is the justification they have
[00:30:07.340 --> 00:30:09.580]   but they're hurting real people.
[00:30:09.580 --> 00:30:12.060]   They hurt me but I'm sure there's a few others
[00:30:12.060 --> 00:30:13.420]   that are really hurt.
[00:30:13.420 --> 00:30:17.220]   - And the zero day factor is a big one.
[00:30:17.220 --> 00:30:22.340]   QNAP right now is trying to figure out
[00:30:22.340 --> 00:30:23.880]   what the hell is wrong with their system
[00:30:23.880 --> 00:30:25.520]   that would let this in.
[00:30:25.520 --> 00:30:28.580]   And even if they pay,
[00:30:28.580 --> 00:30:30.820]   if they still don't know where the zero day is,
[00:30:30.820 --> 00:30:32.780]   what's to say that they won't just hit them again
[00:30:32.780 --> 00:30:34.260]   and hit you again?
[00:30:34.260 --> 00:30:36.580]   So that really complicates things.
[00:30:36.580 --> 00:30:40.860]   And that is a huge advancement for ransomware.
[00:30:40.860 --> 00:30:44.740]   It's really only been, I think, in the last 18 months
[00:30:44.740 --> 00:30:48.420]   that we've ever really seen ransomware exploit zero days
[00:30:48.420 --> 00:30:49.260]   to pull these off.
[00:30:49.260 --> 00:30:54.260]   Usually 80% of them, I think the data shows 80% of them
[00:30:54.260 --> 00:30:58.460]   come down to a lack of two-factor authentication.
[00:30:58.460 --> 00:31:01.420]   So when someone gets hit by a ransomware attack,
[00:31:01.420 --> 00:31:04.420]   they don't have two-factor authentication on,
[00:31:04.420 --> 00:31:07.660]   their employees were using stupid passwords.
[00:31:07.660 --> 00:31:09.740]   You can mitigate that in the future.
[00:31:09.740 --> 00:31:10.940]   This one, they don't know.
[00:31:10.940 --> 00:31:11.860]   They probably don't know.
[00:31:11.860 --> 00:31:14.380]   - Yeah, and it was, I guess it's zero click
[00:31:14.380 --> 00:31:16.220]   'cause I didn't have to do anything.
[00:31:16.220 --> 00:31:20.020]   The only thing, well, here's the thing.
[00:31:20.020 --> 00:31:26.220]   I did basics of, I put it behind a firewall,
[00:31:26.220 --> 00:31:27.980]   I followed instructions.
[00:31:27.980 --> 00:31:30.380]   But I didn't really pay attention.
[00:31:30.380 --> 00:31:34.260]   So maybe there's a misconfiguration of some sort
[00:31:34.260 --> 00:31:36.280]   that's easy to make.
[00:31:36.280 --> 00:31:37.120]   It's difficult.
[00:31:37.120 --> 00:31:39.120]   We have a personal NAS.
[00:31:39.120 --> 00:31:43.500]   So I'm not willing to sort of say
[00:31:43.500 --> 00:31:45.540]   that I did everything I possibly could.
[00:31:45.540 --> 00:31:49.820]   But I did a lot of reasonable stuff
[00:31:49.820 --> 00:31:51.540]   and they still hit it with zero clicks.
[00:31:51.540 --> 00:31:52.540]   I didn't have to do anything.
[00:31:52.540 --> 00:31:54.140]   - Yeah, well, it's like a zero day
[00:31:54.140 --> 00:31:55.880]   and it's a supply chain attack.
[00:31:55.880 --> 00:31:59.260]   You're getting hit from your supplier.
[00:31:59.260 --> 00:32:01.700]   You're getting hit because of your vendor.
[00:32:01.700 --> 00:32:04.340]   And it's also a new thing for ransomware groups
[00:32:04.340 --> 00:32:07.700]   to go to the individuals to pressure them to pay.
[00:32:07.700 --> 00:32:09.900]   There was this really interesting case,
[00:32:09.900 --> 00:32:12.020]   I think it was in Norway,
[00:32:12.020 --> 00:32:16.140]   where there was a mental health clinic that got hit.
[00:32:16.140 --> 00:32:17.940]   And the cyber criminals were going
[00:32:17.940 --> 00:32:20.620]   to the patients themselves to say,
[00:32:20.620 --> 00:32:25.460]   pay this or we're going to release your psychiatric records.
[00:32:25.460 --> 00:32:27.000]   I mean, talk about hell.
[00:32:28.220 --> 00:32:30.620]   In terms of whether to pay,
[00:32:30.620 --> 00:32:33.740]   that is on the cheaper end of the spectrum.
[00:32:33.740 --> 00:32:35.620]   - From the individual or from the company?
[00:32:35.620 --> 00:32:36.740]   - Both.
[00:32:36.740 --> 00:32:39.420]   We've seen, for instance,
[00:32:39.420 --> 00:32:43.020]   there was an Apple supplier in Taiwan.
[00:32:43.020 --> 00:32:47.380]   They got hit and the ransom demand was 50 million.
[00:32:47.380 --> 00:32:49.460]   I'm surprised it's only 1.8 million.
[00:32:49.460 --> 00:32:50.900]   I'm sure it's going to go up.
[00:32:50.900 --> 00:32:53.340]   And it's hard.
[00:32:53.340 --> 00:32:55.660]   There's obviously governments
[00:32:55.660 --> 00:32:57.100]   and maybe in this case,
[00:32:57.100 --> 00:32:58.940]   the company are going to tell you,
[00:32:58.940 --> 00:33:02.160]   we recommend you don't pay or please don't pay.
[00:33:02.160 --> 00:33:04.940]   But the reality on the ground
[00:33:04.940 --> 00:33:08.020]   is that some businesses can't operate.
[00:33:08.020 --> 00:33:09.700]   Some countries can't function.
[00:33:09.700 --> 00:33:13.740]   I mean, the under reported storyline
[00:33:13.740 --> 00:33:18.740]   of Colonial Pipeline was after the company got hit
[00:33:18.740 --> 00:33:22.180]   and took the preemptive step of shutting down the pipeline
[00:33:22.180 --> 00:33:24.260]   because their billing systems were frozen,
[00:33:24.260 --> 00:33:26.340]   they couldn't charge customers downstream.
[00:33:27.180 --> 00:33:29.100]   My colleague, David Singer,
[00:33:29.100 --> 00:33:32.820]   and I got our hands on a classified assessment
[00:33:32.820 --> 00:33:35.780]   that said that as a country,
[00:33:35.780 --> 00:33:38.500]   we could have only afforded two to three more days
[00:33:38.500 --> 00:33:40.700]   of Colonial Pipeline being down.
[00:33:40.700 --> 00:33:42.100]   And it was really interesting.
[00:33:42.100 --> 00:33:44.300]   I thought it was the gas and the jet fuel,
[00:33:44.300 --> 00:33:45.660]   but it wasn't.
[00:33:45.660 --> 00:33:47.460]   We were sort of prepared for that.
[00:33:47.460 --> 00:33:49.020]   It was the diesel.
[00:33:49.020 --> 00:33:52.300]   Without the diesel, the refineries couldn't function
[00:33:52.300 --> 00:33:54.780]   and it would have totally screwed up the economy.
[00:33:54.780 --> 00:33:59.540]   And so there was almost this like national security,
[00:33:59.540 --> 00:34:04.540]   economic impetus for them to pay this ransom.
[00:34:04.540 --> 00:34:06.980]   And the other one I always think about is Baltimore.
[00:34:06.980 --> 00:34:09.100]   You know, when the city of Baltimore got hit,
[00:34:09.100 --> 00:34:11.420]   I think the initial ransom demand
[00:34:11.420 --> 00:34:13.820]   was something around 76,000.
[00:34:13.820 --> 00:34:16.780]   It may have even started smaller than that.
[00:34:16.780 --> 00:34:20.060]   And Baltimore stood its ground and didn't pay.
[00:34:20.060 --> 00:34:24.260]   But ultimately the cost to remediate was $18 million.
[00:34:24.260 --> 00:34:26.820]   It's a lot for the city of Baltimore.
[00:34:26.820 --> 00:34:28.100]   That's money that could have gone
[00:34:28.100 --> 00:34:32.500]   to public school education and roads and public health.
[00:34:32.500 --> 00:34:35.180]   And instead it just went to rebuilding
[00:34:35.180 --> 00:34:36.340]   these systems from scratch.
[00:34:36.340 --> 00:34:39.180]   And so a lot of residents in Baltimore were like,
[00:34:39.180 --> 00:34:43.540]   why the hell didn't you pay the $76,000?
[00:34:43.540 --> 00:34:45.700]   So it's not obvious.
[00:34:45.700 --> 00:34:48.140]   You know, it's easy to say don't pay
[00:34:48.140 --> 00:34:51.780]   because why you're funding their R&D for the next go round.
[00:34:51.780 --> 00:34:56.940]   But it's too often, it's too complicated.
[00:34:56.940 --> 00:34:59.740]   - So on the individual level, just like, you know,
[00:34:59.740 --> 00:35:03.260]   the way I feel personally from this attack,
[00:35:03.260 --> 00:35:05.300]   have you talked to people that were kind of victims
[00:35:05.300 --> 00:35:06.400]   in the same way I was,
[00:35:06.400 --> 00:35:08.540]   but maybe more dramatic ways or so on?
[00:35:08.540 --> 00:35:13.100]   You know, in the same way that violence hurts people.
[00:35:13.100 --> 00:35:13.940]   - Yeah. - How much does this
[00:35:13.940 --> 00:35:16.700]   hurt people in your sense and the way you researched it?
[00:35:16.700 --> 00:35:21.020]   - The worst ransomware attack I've covered
[00:35:21.020 --> 00:35:26.020]   on a personal level was an attack on a hospital in Vermont.
[00:35:26.020 --> 00:35:30.140]   And, you know, you think of this as like,
[00:35:30.140 --> 00:35:31.820]   okay, it's hitting their IT networks.
[00:35:31.820 --> 00:35:34.660]   They should still be able to treat patients.
[00:35:34.660 --> 00:35:37.380]   But it turns out that cancer patients
[00:35:37.380 --> 00:35:39.260]   couldn't get their chemo anymore
[00:35:39.260 --> 00:35:43.000]   because the protocol of who gets what is very complicated.
[00:35:43.000 --> 00:35:47.080]   And without it, nurses and doctors couldn't access it.
[00:35:47.080 --> 00:35:50.520]   So they were turning chemo patients away,
[00:35:50.520 --> 00:35:52.160]   cancer patients away.
[00:35:52.160 --> 00:35:54.640]   One nurse told us,
[00:35:54.640 --> 00:35:57.240]   "I don't know why people aren't screaming about this.
[00:35:57.240 --> 00:35:59.680]   That the only thing I've seen that even compares
[00:35:59.680 --> 00:36:02.040]   to what we're seeing at this hospital right now
[00:36:02.040 --> 00:36:04.440]   was when I worked in the burn unit
[00:36:04.440 --> 00:36:06.840]   after the Boston Marathon bombing."
[00:36:06.840 --> 00:36:10.520]   You know, they really put it in these super dramatic terms.
[00:36:10.520 --> 00:36:15.040]   And last year there was a report in the Wall Street Journal
[00:36:15.040 --> 00:36:18.880]   where they attributed an infant death
[00:36:18.880 --> 00:36:23.560]   to a ransomware attack because a mom came in
[00:36:23.560 --> 00:36:28.480]   and whatever device they were using to monitor the fetus
[00:36:28.480 --> 00:36:30.680]   wasn't working because of the ransomware attack.
[00:36:30.680 --> 00:36:33.600]   And so they attributed this infant death
[00:36:33.600 --> 00:36:34.680]   to the ransomware attack.
[00:36:34.680 --> 00:36:37.940]   Now on a bigger scale, but less personal,
[00:36:39.120 --> 00:36:41.320]   when there was the NotPetya attack.
[00:36:41.320 --> 00:36:45.520]   So this was an attack by Russia on Ukraine
[00:36:45.520 --> 00:36:49.580]   that came at them through a supplier,
[00:36:49.580 --> 00:36:53.080]   a tax software company in that case,
[00:36:53.080 --> 00:36:56.800]   that didn't just hit any government agency
[00:36:56.800 --> 00:36:59.680]   or business in Ukraine that used this tax software.
[00:36:59.680 --> 00:37:02.680]   It actually hit any business all over the world
[00:37:02.680 --> 00:37:07.360]   that had even a single employee working remotely in Ukraine.
[00:37:07.360 --> 00:37:09.520]   So it hit Maersk, the shipping company,
[00:37:09.520 --> 00:37:11.680]   but hit Pfizer, hit FedEx,
[00:37:11.680 --> 00:37:14.560]   but the one I will never forget is Merck.
[00:37:14.560 --> 00:37:17.760]   It paralyzed Merck's factories.
[00:37:17.760 --> 00:37:20.300]   I mean, it really created an existential crisis
[00:37:20.300 --> 00:37:21.600]   for the company.
[00:37:21.600 --> 00:37:25.400]   Merck had to tap into the CDC's emergency supplies
[00:37:25.400 --> 00:37:27.800]   of the Gardasil vaccine that year
[00:37:27.800 --> 00:37:29.920]   because their whole vaccine production line
[00:37:29.920 --> 00:37:32.160]   had been paralyzed in that attack.
[00:37:32.160 --> 00:37:36.040]   Imagine if that was gonna happen right now
[00:37:36.040 --> 00:37:39.480]   to Pfizer or Moderna or Johnson & Johnson.
[00:37:39.480 --> 00:37:41.200]   You know, imagine.
[00:37:41.200 --> 00:37:43.720]   I mean, that would really create
[00:37:43.720 --> 00:37:47.240]   a global cyber terrorist attack, essentially.
[00:37:47.240 --> 00:37:49.360]   - And that's almost unintentional.
[00:37:49.360 --> 00:37:51.280]   - I thought for a long time,
[00:37:51.280 --> 00:37:53.960]   I always labeled it as collateral damage.
[00:37:53.960 --> 00:37:57.300]   But actually just today,
[00:37:57.300 --> 00:38:02.300]   there was a really impressive threat researcher at Cisco,
[00:38:02.300 --> 00:38:05.400]   which has this threat intelligence division called Talos,
[00:38:05.400 --> 00:38:09.000]   who said, "Stop calling it collateral damage."
[00:38:09.000 --> 00:38:12.280]   They could see who was gonna get hit
[00:38:12.280 --> 00:38:15.640]   before they deployed that malware.
[00:38:15.640 --> 00:38:17.960]   It wasn't collateral damage.
[00:38:17.960 --> 00:38:19.080]   It was intentional.
[00:38:19.080 --> 00:38:21.680]   They meant to hit any business
[00:38:21.680 --> 00:38:23.240]   that did business with Ukraine.
[00:38:23.240 --> 00:38:26.320]   It was to send a message to them, too.
[00:38:26.320 --> 00:38:28.760]   So I don't know if that's accurate.
[00:38:28.760 --> 00:38:29.940]   I always thought of it
[00:38:29.940 --> 00:38:32.000]   as sort of the sloppy collateral damage,
[00:38:32.000 --> 00:38:33.960]   but it definitely made me think.
[00:38:34.840 --> 00:38:37.040]   - So how much of this between states
[00:38:37.040 --> 00:38:40.080]   is going to be a part of war,
[00:38:40.080 --> 00:38:43.980]   these kinds of attacks on Ukraine,
[00:38:43.980 --> 00:38:47.800]   between Russia and US,
[00:38:47.800 --> 00:38:49.980]   Russia and China, China and US?
[00:38:49.980 --> 00:38:53.240]   Let's look at China and US.
[00:38:53.240 --> 00:38:58.240]   Do you think China and US are going to escalate
[00:38:58.240 --> 00:39:01.360]   something that would be called a war
[00:39:01.360 --> 00:39:03.020]   purely in the space of cyber?
[00:39:04.240 --> 00:39:08.840]   - I believe any geopolitical conflict
[00:39:08.840 --> 00:39:15.200]   from now on is guaranteed to have some cyber element to it.
[00:39:15.200 --> 00:39:20.480]   The Department of Justice recently declassified a report
[00:39:20.480 --> 00:39:22.700]   that said China's been hacking into our pipelines,
[00:39:22.700 --> 00:39:25.260]   and it's not for intellectual property theft.
[00:39:25.260 --> 00:39:26.980]   It's to get a foothold
[00:39:26.980 --> 00:39:30.460]   so that if things escalate in Taiwan, for example,
[00:39:30.460 --> 00:39:33.280]   they are where they need to be to shut our pipelines down,
[00:39:33.280 --> 00:39:34.960]   and we just got a little glimpse
[00:39:34.960 --> 00:39:37.840]   of what that looked like with Colonial Pipeline
[00:39:37.840 --> 00:39:40.920]   and the panic buying and the jet fuel shortages
[00:39:40.920 --> 00:39:44.560]   and that assessment I just mentioned about the diesel.
[00:39:44.560 --> 00:39:48.180]   So they're there, they've gotten there.
[00:39:48.180 --> 00:39:53.440]   Anytime I read a report about new aggression
[00:39:53.440 --> 00:39:57.120]   from fighter jets, Chinese fighter jets in Taiwan,
[00:39:57.120 --> 00:40:00.700]   or what's happening right now with Russia's buildup
[00:40:00.700 --> 00:40:04.660]   on the Ukraine border, or India, Pakistan,
[00:40:04.660 --> 00:40:07.440]   I'm always looking at it through a cyber lens,
[00:40:07.440 --> 00:40:10.400]   and it really bothers me that other people aren't
[00:40:10.400 --> 00:40:15.520]   because there is no way that these governments
[00:40:15.520 --> 00:40:19.260]   and these nation states are not going to use their access
[00:40:19.260 --> 00:40:23.760]   to gain some advantage in those conflicts.
[00:40:23.760 --> 00:40:28.640]   And I'm now in a position where I'm an advisor
[00:40:28.640 --> 00:40:33.640]   to the Cybersecurity Infrastructure Security Agency at DHS.
[00:40:33.640 --> 00:40:37.540]   So I'm not saying anything classified here,
[00:40:37.540 --> 00:40:41.220]   but I just think that it's really important
[00:40:41.220 --> 00:40:46.020]   to understand just generally what the collateral damage
[00:40:46.020 --> 00:40:50.020]   could be for American businesses and critical infrastructure
[00:40:50.020 --> 00:40:54.060]   in any of these escalated conflicts around the world.
[00:40:54.060 --> 00:40:59.060]   Because just generally, our adversaries have learned
[00:40:59.060 --> 00:41:02.660]   that they might never be able to match us
[00:41:02.660 --> 00:41:05.080]   in terms of our traditional military spending
[00:41:05.080 --> 00:41:08.060]   on traditional weapons and fighter jets.
[00:41:08.060 --> 00:41:12.980]   But we have a very soft underbelly when it comes to cyber.
[00:41:12.980 --> 00:41:17.400]   80% or more of America's critical infrastructure,
[00:41:17.400 --> 00:41:22.400]   so pipelines, power grid, nuclear plants, water systems,
[00:41:23.600 --> 00:41:26.860]   is owned and operated by the private sector.
[00:41:26.860 --> 00:41:30.500]   And for the most part, there is nothing out there
[00:41:30.500 --> 00:41:33.820]   legislating that those companies
[00:41:33.820 --> 00:41:35.760]   share the fact they've been breached.
[00:41:35.760 --> 00:41:37.320]   They don't even have to tell the government
[00:41:37.320 --> 00:41:38.760]   they've been hit.
[00:41:38.760 --> 00:41:40.840]   There's nothing mandating that they even meet
[00:41:40.840 --> 00:41:43.740]   a bare minimum standard of cybersecurity.
[00:41:43.740 --> 00:41:46.700]   And that's it.
[00:41:46.700 --> 00:41:49.020]   So even when there are these attacks,
[00:41:49.020 --> 00:41:51.360]   most of the time we don't even know about it.
[00:41:51.360 --> 00:41:54.360]   So that is, if you were gonna design a system
[00:41:54.360 --> 00:41:57.980]   to be as blind and vulnerable as possible,
[00:41:57.980 --> 00:42:00.680]   that's pretty good.
[00:42:00.680 --> 00:42:01.840]   That's what it looks like,
[00:42:01.840 --> 00:42:04.480]   is what we have here in the United States.
[00:42:04.480 --> 00:42:08.440]   And everyone here is just operating like,
[00:42:08.440 --> 00:42:12.200]   let's just keep hooking up everything for convenience.
[00:42:12.200 --> 00:42:13.840]   Software eats the world.
[00:42:13.840 --> 00:42:18.640]   Let's just keep going for cost, for convenience sake,
[00:42:18.640 --> 00:42:20.760]   just because we can.
[00:42:20.760 --> 00:42:22.840]   And when you study these issues,
[00:42:22.840 --> 00:42:24.400]   and you study these attacks,
[00:42:24.400 --> 00:42:26.960]   and you study the advancement,
[00:42:26.960 --> 00:42:29.400]   and the uptick in frequency,
[00:42:29.400 --> 00:42:32.320]   and the lower barrier to entry
[00:42:32.320 --> 00:42:34.760]   that we see every single year,
[00:42:34.760 --> 00:42:39.760]   you realize just how dumb software eats world is.
[00:42:39.760 --> 00:42:43.180]   And no one has ever stopped to pause and think,
[00:42:43.180 --> 00:42:46.240]   should we be hooking up these systems to the internet?
[00:42:48.040 --> 00:42:49.760]   They've just been saying, can we?
[00:42:49.760 --> 00:42:51.280]   Let's do it.
[00:42:51.280 --> 00:42:52.480]   And that's a real problem.
[00:42:52.480 --> 00:42:54.680]   And just in the last year,
[00:42:54.680 --> 00:42:56.920]   we've seen a record number of zero-day attacks.
[00:42:56.920 --> 00:42:59.080]   I think there were 80 last year,
[00:42:59.080 --> 00:43:02.920]   which is probably more than double what it was in 2019.
[00:43:02.920 --> 00:43:05.240]   A lot of those were nation states.
[00:43:05.240 --> 00:43:08.220]   We live in a world
[00:43:08.220 --> 00:43:11.640]   with a lot of geopolitical hot points right now.
[00:43:11.640 --> 00:43:15.120]   And where those geopolitical hot points are,
[00:43:15.120 --> 00:43:19.120]   are places where countries have been investing heavily
[00:43:19.120 --> 00:43:21.760]   in offensive cyber tools.
[00:43:21.760 --> 00:43:23.440]   - If you're a nation state,
[00:43:23.440 --> 00:43:29.520]   the goal would be to maximize the footprint of zero-day,
[00:43:29.520 --> 00:43:33.360]   like super-secret zero-day that nobody's aware of.
[00:43:33.360 --> 00:43:36.000]   And whenever war is initiated,
[00:43:36.000 --> 00:43:38.920]   the huge negative effects of shutting down infrastructure,
[00:43:38.920 --> 00:43:42.000]   or any kind of zero-day, is the chaos it creates.
[00:43:42.000 --> 00:43:43.600]   So if you just, there's a certain threshold
[00:43:43.600 --> 00:43:45.240]   when you create the chaos,
[00:43:45.240 --> 00:43:48.920]   the markets plummet, just everything goes to hell.
[00:43:48.920 --> 00:43:52.880]   - So it's not just zero-days.
[00:43:52.880 --> 00:43:56.640]   We make it so easy for threat actors.
[00:43:56.640 --> 00:44:00.440]   I mean, we're not using two-factor authentication.
[00:44:00.440 --> 00:44:01.780]   We're not patching.
[00:44:01.780 --> 00:44:04.880]   There was the shell shock vulnerability
[00:44:04.880 --> 00:44:08.200]   that was discovered a couple years ago.
[00:44:08.200 --> 00:44:09.920]   It's still being exploited,
[00:44:09.920 --> 00:44:12.280]   because so many people haven't fixed it.
[00:44:13.280 --> 00:44:16.600]   So the zero-days are really the sexy stuff.
[00:44:16.600 --> 00:44:19.240]   And what really drew me to the zero-day market
[00:44:19.240 --> 00:44:21.440]   was the moral calculus we talked about.
[00:44:21.440 --> 00:44:26.240]   Particularly from the US government's point of view.
[00:44:26.240 --> 00:44:31.240]   How do they justify leaving these systems so vulnerable
[00:44:31.240 --> 00:44:33.560]   when we use them here,
[00:44:33.560 --> 00:44:36.080]   and we're baking more of our critical infrastructure
[00:44:36.080 --> 00:44:38.160]   with this vulnerable software?
[00:44:38.160 --> 00:44:41.120]   It's not like we're using one set of technology,
[00:44:41.120 --> 00:44:43.800]   and Russia's using another, and China's using this.
[00:44:43.800 --> 00:44:45.960]   We're all using the same technology.
[00:44:45.960 --> 00:44:48.920]   So when you find a zero-day in Windows,
[00:44:48.920 --> 00:44:52.400]   you're not just leaving it open so you can spy on Russia
[00:44:52.400 --> 00:44:54.600]   or implant yourself in the Russian grid.
[00:44:54.600 --> 00:44:57.520]   You're leaving Americans vulnerable too.
[00:44:57.520 --> 00:45:02.240]   But zero-days are like, that is the secret sauce.
[00:45:02.240 --> 00:45:04.440]   That's the superpower.
[00:45:04.440 --> 00:45:07.920]   And I always say, every country now,
[00:45:07.920 --> 00:45:09.440]   with the exception of Antarctica,
[00:45:09.440 --> 00:45:11.960]   someone added the Vatican to my list,
[00:45:11.960 --> 00:45:16.760]   is trying to find offensive hacking tools in zero-days
[00:45:16.760 --> 00:45:17.600]   to make 'em work.
[00:45:17.600 --> 00:45:20.680]   And those that don't have the skills
[00:45:20.680 --> 00:45:23.520]   now have this market that they can tap into
[00:45:23.520 --> 00:45:26.480]   where $2.5 million, that's chump change
[00:45:26.480 --> 00:45:28.200]   for a lot of these nation states.
[00:45:28.200 --> 00:45:29.400]   It's a hell of a lot less
[00:45:29.400 --> 00:45:31.940]   than trying to build the next fighter jet.
[00:45:31.940 --> 00:45:34.520]   But yeah, the goal is chaos.
[00:45:34.520 --> 00:45:38.460]   I mean, why did Russia turn off the lights twice in Ukraine?
[00:45:39.060 --> 00:45:42.740]   You know, I think part of it is chaos.
[00:45:42.740 --> 00:45:46.220]   I think part of it is to sow the seeds of doubt
[00:45:46.220 --> 00:45:47.940]   in their current government.
[00:45:47.940 --> 00:45:50.380]   Your government can't even keep your lights on.
[00:45:50.380 --> 00:45:52.420]   Why are you sticking with them?
[00:45:52.420 --> 00:45:54.140]   You know, come over here
[00:45:54.140 --> 00:45:56.180]   and we'll keep your lights on at least.
[00:45:56.180 --> 00:45:58.300]   You know, there's like a little bit of that.
[00:45:58.300 --> 00:46:03.300]   - Nuclear weapons seems to have helped prevent nuclear war.
[00:46:03.300 --> 00:46:08.260]   Is it possible that we have so many vulnerabilities
[00:46:08.260 --> 00:46:11.260]   and so many attack vectors on each other
[00:46:11.260 --> 00:46:15.220]   that it will kind of achieve the same kind of equilibrium
[00:46:15.220 --> 00:46:17.060]   like mutually assured destruction?
[00:46:17.060 --> 00:46:18.700]   - Yeah.
[00:46:18.700 --> 00:46:20.700]   - That's one hopeful solution to this.
[00:46:20.700 --> 00:46:23.780]   Do you have any hope for this particular solution?
[00:46:23.780 --> 00:46:26.480]   - You know, nuclear analogies always tend to fall apart
[00:46:26.480 --> 00:46:27.500]   when it comes to cyber,
[00:46:27.500 --> 00:46:30.940]   mainly because you don't need fissile material.
[00:46:30.940 --> 00:46:33.260]   You know, you just need a laptop and the skills
[00:46:33.260 --> 00:46:34.580]   and you're in the game.
[00:46:34.580 --> 00:46:36.820]   So it's a really low barrier to entry.
[00:46:38.220 --> 00:46:40.940]   The other thing is attributions harder.
[00:46:40.940 --> 00:46:44.300]   And we've seen countries muck around with attribution.
[00:46:44.300 --> 00:46:47.120]   We've seen, you know, nation states piggyback
[00:46:47.120 --> 00:46:49.300]   on other countries' spy operations
[00:46:49.300 --> 00:46:53.360]   and just sit there and siphon out whatever they're getting.
[00:46:53.360 --> 00:46:56.160]   We learned some of that from the Snowden documents.
[00:46:56.160 --> 00:46:58.500]   We've seen Russia hack into Iran's
[00:46:58.500 --> 00:47:01.380]   command and control attack servers.
[00:47:01.380 --> 00:47:05.420]   We've seen them hit a Saudi petrochemical plant
[00:47:05.420 --> 00:47:08.180]   where they did neutralize the safety locks at the plant
[00:47:08.180 --> 00:47:10.140]   and everyone assumed that it was Iran,
[00:47:10.140 --> 00:47:13.640]   given Iran had been targeting Saudi oil companies forever.
[00:47:13.640 --> 00:47:15.220]   But nope, it turned out that it was
[00:47:15.220 --> 00:47:17.760]   a graduate research institute outside Moscow.
[00:47:17.760 --> 00:47:20.260]   So you see countries kind of playing around
[00:47:20.260 --> 00:47:21.100]   with attribution.
[00:47:21.100 --> 00:47:22.260]   Why?
[00:47:22.260 --> 00:47:25.380]   I think because they think, okay, if I do this,
[00:47:25.380 --> 00:47:27.800]   like how am I gonna cover up that it came from me
[00:47:27.800 --> 00:47:30.880]   because I don't wanna risk the response.
[00:47:30.880 --> 00:47:33.180]   So people are sort of dancing around this.
[00:47:33.180 --> 00:47:34.980]   It's just in a very different way.
[00:47:34.980 --> 00:47:39.620]   And, you know, at the Times, I'd covered the Chinese hacks
[00:47:39.620 --> 00:47:42.860]   of infrastructure companies like pipelines.
[00:47:42.860 --> 00:47:46.120]   I'd covered the Russian probes of nuclear plants.
[00:47:46.120 --> 00:47:50.100]   I'd covered the Russian attacks on the Ukraine grid.
[00:47:50.100 --> 00:47:53.860]   And then in 2018, my colleague David Singer and I
[00:47:53.860 --> 00:47:57.140]   covered the fact that US Cyber Command
[00:47:57.140 --> 00:47:59.620]   had been hacking into the Russian grid
[00:47:59.620 --> 00:48:02.220]   and making a pretty loud show of it.
[00:48:02.220 --> 00:48:05.260]   And when we went to the National Security Council,
[00:48:05.260 --> 00:48:06.820]   because that's what journalists do
[00:48:06.820 --> 00:48:08.100]   before they publish a story,
[00:48:08.100 --> 00:48:11.400]   they give the other side a chance to respond.
[00:48:11.400 --> 00:48:14.500]   I assumed we would be in for that really awkward,
[00:48:14.500 --> 00:48:17.020]   painful conversation where they would say,
[00:48:17.020 --> 00:48:18.460]   "You will have blood on your hands
[00:48:18.460 --> 00:48:20.280]   "if you publish this story."
[00:48:20.280 --> 00:48:23.000]   And instead, they gave us the opposite answer.
[00:48:23.000 --> 00:48:25.060]   They said, "We have no problem
[00:48:25.060 --> 00:48:27.180]   "with you publishing this story."
[00:48:27.180 --> 00:48:28.060]   Why?
[00:48:28.060 --> 00:48:29.360]   Well, they didn't say it out loud,
[00:48:29.360 --> 00:48:33.140]   but it was pretty obvious they wanted Russia to know
[00:48:33.140 --> 00:48:35.340]   that we're hacking into their power grid too,
[00:48:35.340 --> 00:48:38.460]   and they better think twice before they do to us
[00:48:38.460 --> 00:48:40.220]   what they had done to Ukraine.
[00:48:40.220 --> 00:48:44.660]   So yeah, you know, we have stumbled into this new era
[00:48:44.660 --> 00:48:47.780]   of mutually assured digital destruction.
[00:48:47.780 --> 00:48:52.780]   I think another sort of quasi-norm we've stumbled into
[00:48:52.780 --> 00:48:56.940]   is proportional responses.
[00:48:56.940 --> 00:49:00.560]   You know, there's this idea that if you get hit,
[00:49:00.560 --> 00:49:03.480]   you're allowed to respond proportionally
[00:49:03.480 --> 00:49:05.480]   at a time and place of your choosing.
[00:49:05.480 --> 00:49:08.440]   You know, that is how the language always goes.
[00:49:08.440 --> 00:49:12.800]   That's what Obama said after North Korea hit Sony.
[00:49:12.800 --> 00:49:15.700]   "We will respond at a time and place of our choosing."
[00:49:15.700 --> 00:49:21.120]   But no one really knows what that response looks like.
[00:49:21.120 --> 00:49:22.760]   And so what you see a lot of the time
[00:49:22.760 --> 00:49:27.140]   are just these like just short of war attacks.
[00:49:27.140 --> 00:49:29.320]   You know, Russia turned off the power in Ukraine,
[00:49:29.320 --> 00:49:31.860]   but it wasn't like it stayed off for a week.
[00:49:31.860 --> 00:49:34.820]   You know, it stayed off for a number of hours.
[00:49:34.820 --> 00:49:39.660]   You know, NotPetya hit those companies pretty hard,
[00:49:39.660 --> 00:49:41.380]   but no one died, you know?
[00:49:41.380 --> 00:49:44.560]   And the question is, what's gonna happen when someone dies?
[00:49:44.560 --> 00:49:49.560]   And can a nation-state masquerade as a cyber-criminal group,
[00:49:49.560 --> 00:49:51.620]   as a ransomware group?
[00:49:51.620 --> 00:49:53.500]   And that's what really complicates
[00:49:53.500 --> 00:49:57.140]   coming to some sort of digital Geneva Convention.
[00:49:57.140 --> 00:50:01.140]   Like there's been a push from Brad Smith at Microsoft.
[00:50:01.140 --> 00:50:03.700]   We need a digital Geneva Convention.
[00:50:03.700 --> 00:50:06.060]   And on its face, it sounds like a no-brainer.
[00:50:06.060 --> 00:50:08.860]   Yeah, why wouldn't we all agree to stop hacking
[00:50:08.860 --> 00:50:11.140]   into each other's civilian hospital systems,
[00:50:11.140 --> 00:50:15.580]   elections, power grid, pipelines?
[00:50:15.580 --> 00:50:19.780]   But when you talk to people in the West,
[00:50:19.780 --> 00:50:20.900]   officials in the West, they'll say,
[00:50:20.900 --> 00:50:24.220]   "We would never, we'd love to agree to it,
[00:50:24.220 --> 00:50:28.460]   but we'd never do it when you're dealing with Xi or Putin
[00:50:28.460 --> 00:50:30.540]   or Kim Jong-un."
[00:50:30.540 --> 00:50:32.860]   Because a lot of times,
[00:50:32.860 --> 00:50:37.060]   they outsource these operations to cyber-criminals.
[00:50:37.060 --> 00:50:39.340]   In China, we see a lot of these attacks come
[00:50:39.340 --> 00:50:43.060]   from this loose satellite network of private citizens
[00:50:43.060 --> 00:50:46.660]   that work at the behest of the Ministry of State Security.
[00:50:46.660 --> 00:50:51.340]   So how do you come to some sort of state-to-state agreement
[00:50:51.340 --> 00:50:55.700]   when you're dealing with transnational actors
[00:50:55.700 --> 00:50:59.140]   and cyber-criminals, where it's really hard to pin down
[00:50:59.140 --> 00:51:01.660]   whether that person was acting alone
[00:51:01.660 --> 00:51:04.980]   or whether they were acting at the behest of the MSS
[00:51:04.980 --> 00:51:06.580]   or the FSB?
[00:51:06.580 --> 00:51:09.420]   And a couple of years ago, I remember,
[00:51:09.420 --> 00:51:11.740]   can't remember if it was before or after NotPetya,
[00:51:11.740 --> 00:51:14.740]   but Putin said, "Hackers are like artists
[00:51:14.740 --> 00:51:16.540]   who wake up in the morning in a good mood
[00:51:16.540 --> 00:51:18.020]   and start painting."
[00:51:18.020 --> 00:51:21.420]   In other words, I have no say over what they do or don't do.
[00:51:21.420 --> 00:51:24.300]   So how do you come to some kind of norm
[00:51:24.300 --> 00:51:26.900]   when that's how he's talking about these issues
[00:51:26.900 --> 00:51:30.180]   and he's just decimated Merck and Pfizer
[00:51:30.180 --> 00:51:34.220]   and another however many thousand companies?
[00:51:34.220 --> 00:51:35.660]   - That is the fundamental difference
[00:51:35.660 --> 00:51:38.860]   between nuclear weapons and cyber-attacks
[00:51:38.860 --> 00:51:40.380]   is the attribution,
[00:51:40.380 --> 00:51:42.540]   or one of the fundamental differences.
[00:51:42.540 --> 00:51:45.180]   If you can fix one thing in the world
[00:51:45.180 --> 00:51:47.180]   in terms of cybersecurity,
[00:51:47.180 --> 00:51:48.940]   that would make the world a better place,
[00:51:48.940 --> 00:51:51.100]   what would you fix?
[00:51:51.100 --> 00:51:54.100]   So you're not allowed to fix like authoritarian regimes
[00:51:54.100 --> 00:51:55.420]   and you can't. (laughs)
[00:51:55.420 --> 00:51:56.620]   - Right.
[00:51:56.620 --> 00:51:57.940]   - You have to keep that,
[00:51:57.940 --> 00:52:00.580]   you have to keep human nature as it is.
[00:52:00.580 --> 00:52:02.820]   In terms of on the security side,
[00:52:02.820 --> 00:52:05.100]   technologically speaking,
[00:52:05.100 --> 00:52:07.860]   you mentioned there's no regulation on companies,
[00:52:07.860 --> 00:52:09.020]   United States,
[00:52:09.020 --> 00:52:14.860]   what if you could just fix with the snap of a finger,
[00:52:14.860 --> 00:52:15.740]   what would you fix?
[00:52:15.740 --> 00:52:17.620]   - Two-factor authentication,
[00:52:17.620 --> 00:52:19.820]   multi-factor authentication.
[00:52:19.820 --> 00:52:24.780]   It's ridiculous how many of these attacks come in
[00:52:24.780 --> 00:52:27.620]   because someone didn't turn on multi-factor authentication.
[00:52:27.620 --> 00:52:30.700]   I mean, Colonial Pipeline, okay,
[00:52:30.700 --> 00:52:34.260]   they took down the biggest conduit
[00:52:34.260 --> 00:52:35.860]   for gas, jet fuel, and diesel
[00:52:35.860 --> 00:52:39.180]   to the East Coast of the United States of America, how?
[00:52:39.180 --> 00:52:42.180]   Because they forgot to deactivate an old employee account
[00:52:42.180 --> 00:52:44.700]   whose password had been traded on the dark web
[00:52:44.700 --> 00:52:48.020]   and they'd never turned on two-factor authentication.
[00:52:48.020 --> 00:52:50.140]   This water treatment facility outside Florida
[00:52:50.140 --> 00:52:53.220]   was hacked last year, how did it happen?
[00:52:53.220 --> 00:52:56.500]   They were using Windows XP from like a decade ago
[00:52:56.500 --> 00:52:59.300]   that can't even get patches if you want it to
[00:52:59.300 --> 00:53:01.700]   and they didn't have two-factor authentication.
[00:53:01.700 --> 00:53:02.700]   Time and time again,
[00:53:02.700 --> 00:53:06.740]   if they just switched on two-factor authentication,
[00:53:06.740 --> 00:53:08.340]   some of these attacks wouldn't have been possible.
[00:53:08.340 --> 00:53:10.060]   Now, if I could snap my fingers,
[00:53:10.060 --> 00:53:11.780]   that's the thing I would do right now.
[00:53:11.780 --> 00:53:15.100]   But of course, this is a cat and mouse game
[00:53:15.100 --> 00:53:17.540]   and then the attacker's onto the next thing.
[00:53:17.540 --> 00:53:21.700]   But I think right now, that is like bar none,
[00:53:21.700 --> 00:53:24.660]   that is just, that is the easiest, simplest way
[00:53:24.660 --> 00:53:25.900]   to deflect the most attacks.
[00:53:25.900 --> 00:53:29.780]   And the name of the game right now isn't perfect security.
[00:53:29.780 --> 00:53:32.220]   Perfect security is impossible.
[00:53:32.220 --> 00:53:34.340]   They will always find a way in.
[00:53:34.340 --> 00:53:35.880]   The name of the game right now is
[00:53:35.880 --> 00:53:39.060]   make yourself a little bit harder to attack
[00:53:39.060 --> 00:53:41.460]   than your competitor or than anyone else out there
[00:53:41.460 --> 00:53:44.300]   so that they just give up and move along.
[00:53:44.300 --> 00:53:48.740]   And maybe if you are a target for an advanced nation state
[00:53:48.740 --> 00:53:53.740]   or the SVR, you're gonna get hacked no matter what.
[00:53:53.740 --> 00:53:56.540]   But you can make cyber criminal groups,
[00:53:56.540 --> 00:53:59.780]   Deadbolt is it, you can make their jobs a lot harder
[00:53:59.780 --> 00:54:03.180]   simply by doing the bare basics.
[00:54:03.180 --> 00:54:05.260]   And the other thing is stop reusing your passwords.
[00:54:05.260 --> 00:54:08.060]   But if I only get one, then two-factor authentication.
[00:54:08.060 --> 00:54:10.460]   - So what is two-factor authentication?
[00:54:10.460 --> 00:54:13.260]   Factor one is what, logging in with a password?
[00:54:13.260 --> 00:54:15.900]   And factor two is like have another device
[00:54:15.900 --> 00:54:18.420]   or another channel through which you can confirm,
[00:54:18.420 --> 00:54:19.460]   yeah, that's me.
[00:54:19.460 --> 00:54:23.500]   - Yes, usually this happens through some kind of text.
[00:54:23.500 --> 00:54:26.700]   You get your one-time code from Bank of America
[00:54:26.700 --> 00:54:28.700]   or from Google.
[00:54:28.700 --> 00:54:31.460]   The better way to do it is spend $20
[00:54:31.460 --> 00:54:34.240]   buying yourself a Fido key on Amazon.
[00:54:34.240 --> 00:54:36.060]   That's a hardware device.
[00:54:36.060 --> 00:54:39.460]   And if you don't have that hardware device with you,
[00:54:39.460 --> 00:54:41.300]   then you're not gonna get in.
[00:54:41.300 --> 00:54:43.780]   And the whole goal is, I mean, basically,
[00:54:43.780 --> 00:54:46.020]   my first half of my decade at the Times
[00:54:46.020 --> 00:54:48.980]   was spent covering like the cop beat.
[00:54:48.980 --> 00:54:51.340]   It was like Home Depot got breached,
[00:54:51.340 --> 00:54:54.380]   News at 11, Target, Neiman Marcus,
[00:54:54.380 --> 00:54:58.380]   like who wasn't hacked over the course of those five years?
[00:54:58.380 --> 00:55:01.060]   And a lot of those companies that got hacked,
[00:55:01.060 --> 00:55:02.160]   what did hackers take?
[00:55:02.160 --> 00:55:05.480]   They took the credentials, they took the passwords.
[00:55:05.480 --> 00:55:08.980]   They can make a pretty penny selling them on the dark web.
[00:55:08.980 --> 00:55:11.540]   And people reuse their passwords.
[00:55:11.540 --> 00:55:15.540]   So you get one from God knows who, I don't know,
[00:55:15.540 --> 00:55:19.340]   last pass, the worst case example, actually last pass.
[00:55:19.340 --> 00:55:21.860]   But you get one and then you go test it
[00:55:21.860 --> 00:55:23.420]   on their email account.
[00:55:23.420 --> 00:55:25.580]   And you go test it on their brokerage account.
[00:55:25.580 --> 00:55:28.620]   And you test it on their cold storage account.
[00:55:28.620 --> 00:55:29.580]   That's how it works.
[00:55:29.580 --> 00:55:32.900]   But if you have multi-factor authentication,
[00:55:32.900 --> 00:55:34.500]   then they can't get in
[00:55:34.500 --> 00:55:36.740]   because they might have your password,
[00:55:36.740 --> 00:55:38.040]   but they don't have your phone,
[00:55:38.040 --> 00:55:39.740]   they don't have your Fido key.
[00:55:39.740 --> 00:55:42.700]   So you keep them out.
[00:55:42.700 --> 00:55:46.540]   And I get a lot of alerts that tell me
[00:55:46.540 --> 00:55:49.860]   someone is trying to get into your Instagram account
[00:55:49.860 --> 00:55:52.060]   or your Twitter account or your email account.
[00:55:52.060 --> 00:55:55.840]   And I don't worry because I use multi-factor authentication.
[00:55:55.840 --> 00:55:57.200]   They can try all day.
[00:55:57.200 --> 00:55:59.500]   Okay, I worry a little bit.
[00:55:59.500 --> 00:56:04.500]   But it's the simplest thing to do and we don't even do it.
[00:56:04.500 --> 00:56:06.860]   - Well, there's an interface aspect to it
[00:56:06.860 --> 00:56:09.900]   'cause it's pretty annoying if it's implemented poorly.
[00:56:09.900 --> 00:56:11.420]   - Yeah, true.
[00:56:11.420 --> 00:56:13.060]   - So actually bad implementation
[00:56:13.060 --> 00:56:16.400]   of two-factor authentication, not just bad,
[00:56:16.400 --> 00:56:19.040]   but just something that adds friction
[00:56:19.040 --> 00:56:21.360]   is a security vulnerability, I guess,
[00:56:21.360 --> 00:56:23.480]   because it's really annoying.
[00:56:23.480 --> 00:56:27.560]   Like I think MIT for a while had two-factor authentication.
[00:56:27.560 --> 00:56:28.680]   It was really annoying.
[00:56:28.680 --> 00:56:32.260]   The number of times it pings you,
[00:56:35.040 --> 00:56:39.960]   it asks to re-authenticate across multiple subdomains.
[00:56:39.960 --> 00:56:41.720]   It just feels like a pain.
[00:56:41.720 --> 00:56:44.120]   I don't know what the right balance there.
[00:56:44.120 --> 00:56:48.680]   - Yeah, it feels like friction in our frictionless society.
[00:56:48.680 --> 00:56:49.800]   It feels like friction.
[00:56:49.800 --> 00:56:51.040]   It's annoying.
[00:56:51.040 --> 00:56:52.840]   That's security's biggest problem.
[00:56:52.840 --> 00:56:54.520]   It's annoying.
[00:56:54.520 --> 00:56:57.880]   We need the Steve Jobs of security to come along
[00:56:57.880 --> 00:56:59.600]   and we need to make it painless.
[00:56:59.600 --> 00:57:02.060]   And actually, on that point,
[00:57:02.060 --> 00:57:07.060]   Apple has probably done more for security than anyone else
[00:57:07.060 --> 00:57:10.880]   simply by introducing biometric authentication,
[00:57:10.880 --> 00:57:13.640]   first with the fingerprint and then with Face ID.
[00:57:13.640 --> 00:57:17.440]   And it's not perfect, but if you think just eight years ago,
[00:57:17.440 --> 00:57:20.400]   everyone was running around with either no passcode,
[00:57:20.400 --> 00:57:22.960]   an optional passcode, or a four-digit passcode
[00:57:22.960 --> 00:57:24.500]   on their phone that anyone,
[00:57:24.500 --> 00:57:27.320]   think of what you can get when you get someone's iPhone,
[00:57:27.320 --> 00:57:29.060]   if you steal someone's iPhone.
[00:57:29.060 --> 00:57:32.600]   And props to them for introducing the fingerprint
[00:57:32.600 --> 00:57:33.440]   and Face ID.
[00:57:33.440 --> 00:57:34.880]   And again, it wasn't perfect,
[00:57:34.880 --> 00:57:36.960]   but it was a huge step forward.
[00:57:36.960 --> 00:57:40.200]   Now it's time to make another huge step forward.
[00:57:40.200 --> 00:57:42.880]   I wanna see the password die.
[00:57:42.880 --> 00:57:46.880]   I mean, it's gotten us as far as it was ever gonna get us
[00:57:46.880 --> 00:57:49.560]   and I hope whatever we come up with next
[00:57:49.560 --> 00:57:52.360]   is not gonna be annoying, is gonna be seamless.
[00:57:52.360 --> 00:57:55.080]   - When I was at Google, that's what we worked on is,
[00:57:55.080 --> 00:57:58.200]   and there's a lot of ways to call this active authentication
[00:57:58.200 --> 00:57:59.900]   or passive authentication.
[00:57:59.900 --> 00:58:02.500]   So basically use biometric data,
[00:58:02.500 --> 00:58:03.780]   not just like a fingerprint,
[00:58:03.780 --> 00:58:07.180]   but everything from your body to identify who you are,
[00:58:07.180 --> 00:58:09.140]   like movement patterns.
[00:58:09.140 --> 00:58:12.700]   So basically create a lot of layers of protection
[00:58:12.700 --> 00:58:15.420]   where it's very difficult to fake,
[00:58:15.420 --> 00:58:18.780]   including like face unlock,
[00:58:18.780 --> 00:58:21.260]   checking that it's your actual face,
[00:58:21.260 --> 00:58:23.020]   like the liveness tests.
[00:58:23.020 --> 00:58:26.260]   So like from video, so unlocking it with video,
[00:58:26.260 --> 00:58:30.040]   voice, the way you move the phone,
[00:58:30.040 --> 00:58:33.280]   the way you take it out of the pocket, that kind of thing.
[00:58:33.280 --> 00:58:35.040]   All of those factors.
[00:58:35.040 --> 00:58:37.360]   It's a really hard problem though.
[00:58:37.360 --> 00:58:42.080]   And ultimately it's very difficult to beat the password
[00:58:42.080 --> 00:58:43.560]   during the security.
[00:58:43.560 --> 00:58:46.160]   - Well, there's a company that I actually will call out
[00:58:46.160 --> 00:58:48.220]   and that's Abnormal Security.
[00:58:48.220 --> 00:58:51.440]   So they work on email attacks.
[00:58:51.440 --> 00:58:55.080]   And it was started by a couple of guys
[00:58:55.080 --> 00:58:59.360]   who were doing, I think, ad tech at Twitter.
[00:58:59.360 --> 00:59:01.560]   So, you know, ad technology now,
[00:59:01.560 --> 00:59:03.780]   like it's a joke how much they know about us.
[00:59:03.780 --> 00:59:05.840]   You know, you always hear the conspiracy theories
[00:59:05.840 --> 00:59:08.240]   that you saw someone's shoes
[00:59:08.240 --> 00:59:10.400]   and next thing you know, it's on your phone.
[00:59:10.400 --> 00:59:12.300]   It's amazing what they know about you.
[00:59:12.300 --> 00:59:15.800]   And they're basically taking that
[00:59:15.800 --> 00:59:19.680]   and they're applying it to attacks.
[00:59:19.680 --> 00:59:21.920]   So they're saying, okay, you know,
[00:59:21.920 --> 00:59:24.960]   if you're, this is what your email patterns are.
[00:59:24.960 --> 00:59:26.360]   It might be different for you and me
[00:59:26.360 --> 00:59:28.640]   because we're emailing strangers all the time.
[00:59:28.640 --> 00:59:31.000]   But for most people,
[00:59:31.000 --> 00:59:33.920]   their email patterns are pretty predictable.
[00:59:33.920 --> 00:59:38.400]   And if something strays from that pattern, that's abnormal.
[00:59:38.400 --> 00:59:41.760]   And they'll block it, they'll investigate it, you know,
[00:59:41.760 --> 00:59:43.560]   and that's great.
[00:59:43.560 --> 00:59:46.820]   You know, let's start using that kind of targeted
[00:59:46.820 --> 00:59:50.560]   ad technology to protect people.
[00:59:50.560 --> 00:59:52.980]   And yeah, I mean, it's not gonna get us away
[00:59:52.980 --> 00:59:56.320]   from the password and using multi-factor authentication,
[00:59:56.320 --> 00:59:59.960]   but you know, the technology is out there
[00:59:59.960 --> 01:00:02.080]   and we just have to figure out how to use it
[01:00:02.080 --> 01:00:03.400]   in a really seamless way,
[01:00:03.400 --> 01:00:05.680]   because it doesn't matter
[01:00:05.680 --> 01:00:07.600]   if you have the perfect security solution
[01:00:07.600 --> 01:00:08.440]   if no one uses it.
[01:00:08.440 --> 01:00:10.360]   I mean, when I started at the Times,
[01:00:10.360 --> 01:00:12.200]   when I was trying to be really good
[01:00:12.200 --> 01:00:14.880]   about protecting sources,
[01:00:14.880 --> 01:00:17.760]   I was trying to use PGP encryption
[01:00:17.760 --> 01:00:19.480]   and it's like, it didn't work.
[01:00:19.480 --> 01:00:22.440]   You know, the number of mistakes I would probably make
[01:00:22.440 --> 01:00:27.100]   just trying to email someone with PGP just wasn't worth it.
[01:00:27.100 --> 01:00:31.360]   And then Signal came along and Signal made it,
[01:00:31.360 --> 01:00:34.700]   Wicker, you know, they made it a lot easier
[01:00:34.700 --> 01:00:37.040]   to send someone an encrypted text message.
[01:00:37.040 --> 01:00:42.040]   So we have to start investing in creative minds
[01:00:42.040 --> 01:00:45.180]   in good security design.
[01:00:45.180 --> 01:00:46.940]   You know, I really think that's the hack
[01:00:46.940 --> 01:00:50.100]   that's gonna get us out of where we are today.
[01:00:50.100 --> 01:00:52.420]   - What about social engineering?
[01:00:52.420 --> 01:00:56.760]   Do you worry about this sort of hacking people?
[01:00:56.760 --> 01:01:00.940]   - Yes, I mean, this is the worst nightmare
[01:01:00.940 --> 01:01:04.200]   of every chief information security officer out there.
[01:01:04.200 --> 01:01:09.040]   You know, social engineering, we work from home now.
[01:01:09.040 --> 01:01:15.100]   I saw this woman posted online about how her husband,
[01:01:15.100 --> 01:01:18.620]   it went viral today, but it was her husband
[01:01:18.620 --> 01:01:20.020]   had this problem at work.
[01:01:20.020 --> 01:01:23.660]   They hired a guy named John and now the guy
[01:01:23.660 --> 01:01:27.540]   that shows up for work every day doesn't act like John.
[01:01:27.540 --> 01:01:31.060]   I mean, think about that.
[01:01:31.060 --> 01:01:34.020]   Like think about the potential for social engineering
[01:01:34.020 --> 01:01:35.540]   in that context.
[01:01:35.540 --> 01:01:38.940]   You know, you apply for a job and you put on a pretty face,
[01:01:38.940 --> 01:01:40.420]   you hire an actor or something,
[01:01:40.420 --> 01:01:42.680]   and then you just get inside the organization
[01:01:42.680 --> 01:01:45.580]   and get access to all that organization's data.
[01:01:45.580 --> 01:01:47.460]   You know, a couple of years ago,
[01:01:47.460 --> 01:01:51.180]   Saudi Arabia planted spies inside Twitter.
[01:01:51.180 --> 01:01:52.020]   You know, why?
[01:01:52.020 --> 01:01:54.500]   Probably because they were trying to figure out
[01:01:54.500 --> 01:01:56.460]   who these people were who were criticizing
[01:01:56.460 --> 01:01:58.020]   the regime on Twitter.
[01:01:58.020 --> 01:02:00.040]   You know, they couldn't do it with a hack from the outside,
[01:02:00.040 --> 01:02:02.260]   so why not plant people on the inside?
[01:02:02.260 --> 01:02:04.540]   And that's like the worst nightmare.
[01:02:04.540 --> 01:02:09.540]   And it also, unfortunately, creates all kinds of xenophobia
[01:02:09.540 --> 01:02:11.340]   at a lot of these organizations.
[01:02:11.340 --> 01:02:14.780]   I mean, if you're gonna have to take that into consideration
[01:02:14.780 --> 01:02:16.700]   then organizations are gonna start looking
[01:02:16.700 --> 01:02:19.420]   really skeptically and suspiciously
[01:02:19.420 --> 01:02:23.020]   at someone who applies for that job from China.
[01:02:23.020 --> 01:02:25.780]   And we've seen that go really badly
[01:02:25.780 --> 01:02:28.540]   at places like the Department of Commerce
[01:02:28.540 --> 01:02:31.180]   where they basically accuse people of being spies
[01:02:31.180 --> 01:02:32.020]   that aren't spies.
[01:02:32.020 --> 01:02:35.380]   So it is the hardest problem to solve.
[01:02:35.380 --> 01:02:37.380]   And it's never been harder to solve
[01:02:37.380 --> 01:02:39.180]   than right at this very moment
[01:02:39.180 --> 01:02:41.300]   when there's so much pressure for companies
[01:02:41.300 --> 01:02:43.940]   to let people work remotely.
[01:02:43.940 --> 01:02:46.260]   - That's actually why I'm single, I'm suspicious
[01:02:46.260 --> 01:02:49.660]   of China and Russia every time I meet somebody
[01:02:49.660 --> 01:02:52.940]   or trying to plant and get insider information.
[01:02:52.940 --> 01:02:54.700]   So I'm very, very suspicious.
[01:02:54.700 --> 01:02:58.420]   I keep putting the Turing test in front, no.
[01:02:58.420 --> 01:03:02.700]   - No, I have a friend who worked inside NSA
[01:03:02.700 --> 01:03:04.820]   and was one of their top hackers.
[01:03:04.820 --> 01:03:08.500]   And he's like, "Every time I go to Russia,
[01:03:08.500 --> 01:03:10.820]   "I get hit on by these 10s."
[01:03:10.820 --> 01:03:12.160]   And I come home, my friends are like,
[01:03:12.160 --> 01:03:13.820]   "I'm sorry, you're not a 10."
[01:03:13.820 --> 01:03:17.020]   Like, it's a common story.
[01:03:17.020 --> 01:03:20.940]   - I mean, it's difficult to trust humans
[01:03:20.940 --> 01:03:23.820]   in this day and age online.
[01:03:23.820 --> 01:03:27.460]   So we're working remotely, that's one thing.
[01:03:27.460 --> 01:03:31.340]   But just interacting with people on the internet,
[01:03:31.340 --> 01:03:32.500]   it sounds ridiculous.
[01:03:32.500 --> 01:03:35.300]   But because of this podcast in part,
[01:03:35.300 --> 01:03:37.920]   I've gotten to meet some incredible people.
[01:03:37.920 --> 01:03:42.400]   But it makes you nervous to trust folks.
[01:03:43.300 --> 01:03:47.240]   And I don't know how to solve that problem.
[01:03:47.240 --> 01:03:51.460]   So I'm talking with Mark Zuckerberg,
[01:03:51.460 --> 01:03:53.500]   who dreams about creating the metaverse.
[01:03:53.500 --> 01:03:56.820]   What do you do about that world
[01:03:56.820 --> 01:04:01.500]   where more and more our lives is in the digital sphere?
[01:04:01.500 --> 01:04:05.320]   Like, one way to phrase it is,
[01:04:05.320 --> 01:04:07.940]   most of our meaningful experiences
[01:04:07.940 --> 01:04:11.280]   at some point will be online.
[01:04:12.140 --> 01:04:15.800]   Like falling in love, getting a job,
[01:04:15.800 --> 01:04:19.900]   or experiencing a moment of happiness with a friend,
[01:04:19.900 --> 01:04:21.900]   with a new friend made online.
[01:04:21.900 --> 01:04:23.100]   All of those things.
[01:04:23.100 --> 01:04:25.540]   Like more and more, the fun we do,
[01:04:25.540 --> 01:04:29.100]   the things that make us love life will happen online.
[01:04:29.100 --> 01:04:32.960]   And if those things have an avatar that's digital,
[01:04:32.960 --> 01:04:35.820]   that's like a way to hack into people's minds.
[01:04:35.820 --> 01:04:39.500]   Whether it's with AI or kind of troll farms
[01:04:39.500 --> 01:04:40.980]   or something like that.
[01:04:40.980 --> 01:04:43.540]   I don't know if there's a way to protect against that.
[01:04:43.540 --> 01:04:49.420]   That might fundamentally rely on our faith
[01:04:49.420 --> 01:04:51.940]   in how good human nature is.
[01:04:51.940 --> 01:04:54.900]   So if most people are good, we're going to be okay.
[01:04:54.900 --> 01:04:59.220]   But if people will tend towards manipulation
[01:04:59.220 --> 01:05:03.300]   and malevolent behavior in search of power,
[01:05:03.300 --> 01:05:04.520]   then we're screwed.
[01:05:04.520 --> 01:05:07.860]   So I don't know if you can comment
[01:05:07.860 --> 01:05:10.380]   on how to keep the metaverse secure.
[01:05:10.380 --> 01:05:13.700]   - Yeah, I mean, all I thought about
[01:05:13.700 --> 01:05:16.740]   when you were talking just now is my three-year-old son.
[01:05:16.740 --> 01:05:17.580]   - Yeah.
[01:05:17.580 --> 01:05:22.540]   - Yeah, he asked me the other day, what's the internet, mom?
[01:05:22.540 --> 01:05:25.860]   And I just almost wanted to cry.
[01:05:25.860 --> 01:05:28.020]   (laughs)
[01:05:28.020 --> 01:05:30.380]   I don't want that for him.
[01:05:30.380 --> 01:05:33.200]   I don't want all of his most meaningful experiences
[01:05:33.200 --> 01:05:34.260]   to be online.
[01:05:34.260 --> 01:05:36.120]   By the time that happens,
[01:05:36.120 --> 01:05:39.700]   how do you know that person's human?
[01:05:40.420 --> 01:05:42.500]   That avatar's human?
[01:05:42.500 --> 01:05:43.620]   I believe in free speech.
[01:05:43.620 --> 01:05:47.540]   I don't believe in free speech for robots and bots.
[01:05:47.540 --> 01:05:52.540]   And look what just happened over the last six years.
[01:05:52.540 --> 01:05:56.940]   We had bots pretending to be Black Lives Matter activists
[01:05:56.940 --> 01:05:59.380]   just to sow some division,
[01:05:59.380 --> 01:06:01.780]   or Texas secessionists,
[01:06:01.780 --> 01:06:06.620]   or organizing anti-Hillary protests,
[01:06:06.620 --> 01:06:09.020]   or just to sow more division,
[01:06:09.020 --> 01:06:12.460]   to tie us up in our own politics
[01:06:12.460 --> 01:06:15.700]   so that we're so paralyzed we can't get anything done.
[01:06:15.700 --> 01:06:17.260]   We can't make any progress,
[01:06:17.260 --> 01:06:19.980]   and we definitely can't handle our adversaries
[01:06:19.980 --> 01:06:21.600]   and their long-term thinking.
[01:06:21.600 --> 01:06:25.300]   It really scares me.
[01:06:25.300 --> 01:06:28.340]   And here's where I just come back to,
[01:06:28.340 --> 01:06:32.540]   just because we can create the metaverse,
[01:06:32.540 --> 01:06:36.420]   just because it sounds like the next logical step
[01:06:36.420 --> 01:06:39.820]   in our digital revolution.
[01:06:39.820 --> 01:06:43.980]   Do I really want my child's most significant moments
[01:06:43.980 --> 01:06:45.580]   to be online?
[01:06:45.580 --> 01:06:46.760]   They weren't for me.
[01:06:46.760 --> 01:06:51.940]   So maybe I'm just stuck in that old-school thinking,
[01:06:51.940 --> 01:06:54.580]   or maybe I've seen too much.
[01:06:54.580 --> 01:06:59.580]   And I'm really sick of being the guinea pig parent generation
[01:06:59.580 --> 01:07:01.860]   for these things.
[01:07:01.860 --> 01:07:04.660]   I mean, it's hard enough with screen time.
[01:07:04.660 --> 01:07:09.660]   But thinking about how to manage the metaverse as a parent
[01:07:09.660 --> 01:07:13.740]   to a young boy, I can't even let my head go there.
[01:07:13.740 --> 01:07:16.400]   That's so terrifying for me.
[01:07:16.400 --> 01:07:21.360]   But we've never stopped any new technology
[01:07:21.360 --> 01:07:24.040]   just because it introduces risks.
[01:07:24.040 --> 01:07:27.920]   We've always said, okay, the promise of this technology
[01:07:27.920 --> 01:07:31.760]   means we should keep going, keep pressing ahead.
[01:07:31.760 --> 01:07:35.560]   We just need to figure out new ways to manage that risk.
[01:07:35.560 --> 01:07:39.520]   And that's the blockchain right now.
[01:07:39.520 --> 01:07:44.880]   When I was covering all of these ransomware attacks,
[01:07:44.880 --> 01:07:49.000]   I thought, okay, this is gonna be it for cryptocurrency.
[01:07:49.000 --> 01:07:51.400]   Governments are gonna put the kibosh down.
[01:07:51.400 --> 01:07:54.780]   They're gonna put the hammer down and say, enough is enough.
[01:07:54.780 --> 01:07:56.900]   We have to put this genie back in the bottle
[01:07:56.900 --> 01:07:58.560]   because it's enabled ransomware.
[01:07:59.480 --> 01:08:02.680]   Five years ago, they would hijack your PC
[01:08:02.680 --> 01:08:05.520]   and they'd say, go to the local pharmacy,
[01:08:05.520 --> 01:08:08.340]   get a e-gift card and tell us what the pin is
[01:08:08.340 --> 01:08:10.440]   and then we'll get your $200.
[01:08:10.440 --> 01:08:13.420]   Now it's pay us five Bitcoin.
[01:08:13.420 --> 01:08:16.160]   And so there's no doubt cryptocurrencies
[01:08:16.160 --> 01:08:17.840]   enabled ransomware attacks.
[01:08:17.840 --> 01:08:22.600]   But after the colonial pipeline, ransom was seized.
[01:08:22.600 --> 01:08:25.760]   'Cause if you remember, the FBI was actually able to go in
[01:08:25.760 --> 01:08:28.120]   and claw some of it back from dark side,
[01:08:28.120 --> 01:08:30.280]   which was the ransomware group that hit it.
[01:08:30.280 --> 01:08:34.080]   And I spoke to these guys at TRM Labs.
[01:08:34.080 --> 01:08:37.480]   So they're one of these blockchain intelligence companies.
[01:08:37.480 --> 01:08:38.680]   And a lot of people that work there
[01:08:38.680 --> 01:08:40.880]   used to work at the treasury.
[01:08:40.880 --> 01:08:42.160]   And what they said to me was,
[01:08:42.160 --> 01:08:46.440]   yeah, cryptocurrency has enabled ransomware.
[01:08:46.440 --> 01:08:51.440]   But to track down that ransom payment would have taken,
[01:08:51.440 --> 01:08:54.760]   if we were dealing with fiat currency,
[01:08:54.760 --> 01:08:58.100]   would have taken us years to get to that one bank account
[01:08:58.100 --> 01:09:01.560]   or belonging to that one front company in the Seychelles.
[01:09:01.560 --> 01:09:04.040]   And now thanks to the blockchain,
[01:09:04.040 --> 01:09:08.320]   we can track the movement of those funds in real time.
[01:09:08.320 --> 01:09:09.960]   And you know what?
[01:09:09.960 --> 01:09:13.300]   These payments are not as anonymous as people think.
[01:09:13.300 --> 01:09:16.640]   Like we still can use our old hacking ways and zero days
[01:09:16.640 --> 01:09:19.560]   and old school intelligence methods
[01:09:19.560 --> 01:09:21.800]   to find out who owns that private wallet
[01:09:21.800 --> 01:09:23.480]   and how to get to it.
[01:09:23.480 --> 01:09:27.640]   So it's a curse in some ways in that it's an enabler,
[01:09:27.640 --> 01:09:29.440]   but it's also a blessing.
[01:09:29.440 --> 01:09:32.080]   And they said that same thing to me that I just said to you.
[01:09:32.080 --> 01:09:37.080]   They said, we've never shut down a promising new technology
[01:09:37.080 --> 01:09:39.160]   because it introduced risk.
[01:09:39.160 --> 01:09:42.720]   We just figured out how to manage that risk.
[01:09:42.720 --> 01:09:44.400]   - And I think that's where the conversation,
[01:09:44.400 --> 01:09:45.880]   unfortunately, has to go,
[01:09:45.880 --> 01:09:48.800]   is how do we, in the metaverse,
[01:09:48.800 --> 01:09:53.600]   use technology to fix things.
[01:09:53.600 --> 01:09:57.000]   So maybe we'll finally be able to, not finally,
[01:09:57.000 --> 01:10:00.960]   but figure out a way to solve the identity problem
[01:10:00.960 --> 01:10:03.400]   on the internet, meaning like a blue check mark
[01:10:03.400 --> 01:10:06.960]   for actual human and connect it to identity
[01:10:06.960 --> 01:10:11.360]   like a fingerprint so you can prove you're you,
[01:10:11.360 --> 01:10:15.120]   and yet do it in a way that doesn't involve
[01:10:15.120 --> 01:10:17.580]   the company having all your data.
[01:10:17.580 --> 01:10:20.760]   So giving you, allowing you to maintain control
[01:10:20.760 --> 01:10:23.800]   over your data, or if you don't,
[01:10:23.800 --> 01:10:25.960]   then there's complete transparency
[01:10:25.960 --> 01:10:28.880]   of how that data is being used, all those kinds of things.
[01:10:28.880 --> 01:10:32.020]   And maybe as you educate more and more people,
[01:10:32.020 --> 01:10:36.080]   they would demand in a capitalist society
[01:10:36.080 --> 01:10:38.440]   that the companies that they give their data to
[01:10:38.440 --> 01:10:41.040]   will respect that data.
[01:10:41.040 --> 01:10:43.600]   - Yeah, I mean, there is this company,
[01:10:43.600 --> 01:10:48.520]   and I hope they succeed, their name's PIIANO, P-I-I-O,
[01:10:48.520 --> 01:10:52.160]   and they wanna create a vault for your personal information
[01:10:52.160 --> 01:10:54.440]   inside every organization.
[01:10:54.440 --> 01:10:58.000]   And ultimately, if I'm gonna call Delta Airlines
[01:10:58.000 --> 01:11:00.360]   to book a flight, they don't need to know
[01:11:00.360 --> 01:11:02.520]   my social security number,
[01:11:02.520 --> 01:11:05.540]   they don't need to know my birth date,
[01:11:05.540 --> 01:11:08.920]   they're just gonna send me a one-time token to my phone.
[01:11:08.920 --> 01:11:11.680]   My phone's gonna say, or my Fido key is gonna say,
[01:11:11.680 --> 01:11:13.520]   "Yep, it's her."
[01:11:13.520 --> 01:11:16.600]   And then we're gonna talk about my identity like a token,
[01:11:16.600 --> 01:11:17.660]   you know, some random token.
[01:11:17.660 --> 01:11:20.120]   They don't need to know exactly who I am,
[01:11:20.120 --> 01:11:23.240]   they just need to know the system trust
[01:11:23.240 --> 01:11:24.880]   that I am who I say I am,
[01:11:24.880 --> 01:11:27.960]   but they don't get access to my PII data,
[01:11:27.960 --> 01:11:30.440]   they don't get access to my social security number,
[01:11:30.440 --> 01:11:34.580]   my location, or the fact I'm a Times journalist.
[01:11:34.580 --> 01:11:37.380]   You know, I think that's the way the world's gonna go.
[01:11:37.380 --> 01:11:39.960]   We have, enough is enough,
[01:11:39.960 --> 01:11:44.160]   sort of losing our personal information everywhere,
[01:11:44.160 --> 01:11:48.320]   letting data marketing companies track our every move.
[01:11:48.320 --> 01:11:50.680]   You know, they don't need to know who I am.
[01:11:50.680 --> 01:11:52.280]   You know, okay, I get it.
[01:11:52.280 --> 01:11:53.680]   You know, we're stuck in this world
[01:11:53.680 --> 01:11:57.480]   where the internet runs on ads,
[01:11:57.480 --> 01:11:59.960]   so ads are not gonna go away,
[01:11:59.960 --> 01:12:03.040]   but they don't need to know I'm Nicole Perlera.
[01:12:03.040 --> 01:12:07.040]   They can know that I am token number, you know, x567.
[01:12:07.040 --> 01:12:11.040]   - And they can let you know what they know
[01:12:11.040 --> 01:12:14.080]   and give you control about removing the things they know.
[01:12:14.080 --> 01:12:15.880]   - Yeah, right, to be forgotten.
[01:12:15.880 --> 01:12:17.960]   - To me, you should be able to walk away
[01:12:17.960 --> 01:12:20.300]   with a single press of a button.
[01:12:20.300 --> 01:12:22.080]   And I also believe that most people,
[01:12:22.080 --> 01:12:25.240]   given the choice to walk away, won't walk away.
[01:12:25.240 --> 01:12:28.560]   They'll just feel better about having the option
[01:12:28.560 --> 01:12:30.720]   to walk away when they understand the trade-offs.
[01:12:30.720 --> 01:12:32.580]   If you walk away, you're not gonna get
[01:12:32.580 --> 01:12:34.160]   some of the personalized experiences
[01:12:34.160 --> 01:12:36.000]   that you would otherwise get,
[01:12:36.000 --> 01:12:38.620]   like a personalized feed and all those kinds of things.
[01:12:38.620 --> 01:12:40.920]   But the freedom to walk away is,
[01:12:40.920 --> 01:12:44.240]   I think, really powerful.
[01:12:44.240 --> 01:12:45.600]   And obviously, what you're saying,
[01:12:45.600 --> 01:12:48.660]   there's all of these HTML forms
[01:12:48.660 --> 01:12:50.220]   where you have to enter your phone number
[01:12:50.220 --> 01:12:52.840]   and email and private information
[01:12:52.840 --> 01:12:54.700]   from every single airline.
[01:12:54.700 --> 01:12:56.780]   New York Times.
[01:12:56.780 --> 01:13:00.640]   I have so many opinions on this.
[01:13:00.640 --> 01:13:03.440]   Just the friction and the sign-up
[01:13:03.440 --> 01:13:04.840]   and all of those kinds of things.
[01:13:04.840 --> 01:13:07.200]   I should be able to, this has to do with everything.
[01:13:07.200 --> 01:13:08.840]   This has to do with payment, too.
[01:13:08.840 --> 01:13:11.800]   Payment should be trivial.
[01:13:11.800 --> 01:13:13.920]   It should be one click,
[01:13:13.920 --> 01:13:16.960]   and one click to unsubscribe and subscribe,
[01:13:16.960 --> 01:13:19.560]   and one click to provide all of your information
[01:13:19.560 --> 01:13:21.700]   that's necessary for the subscription service,
[01:13:21.700 --> 01:13:24.140]   for the transaction service, whatever that is,
[01:13:24.140 --> 01:13:25.940]   getting a ticket, as opposed to,
[01:13:25.940 --> 01:13:28.120]   I have all of these fake phone numbers and emails
[01:13:28.120 --> 01:13:29.380]   that I use now to sign out.
[01:13:29.380 --> 01:13:31.940]   'Cause you never know.
[01:13:31.940 --> 01:13:34.320]   If one site is hacked,
[01:13:34.320 --> 01:13:37.580]   then it's just going to propagate to everything else.
[01:13:37.580 --> 01:13:38.780]   - Yeah.
[01:13:38.780 --> 01:13:41.080]   And there's low-hanging fruit,
[01:13:41.080 --> 01:13:44.440]   and I hope Congress does something.
[01:13:44.440 --> 01:13:46.620]   And frankly, I think it's negligent they haven't
[01:13:46.620 --> 01:13:51.620]   on the fact that elderly people are getting spammed to death
[01:13:51.620 --> 01:13:56.980]   on their phones these days with fake car warranty scams.
[01:13:56.980 --> 01:13:59.540]   I mean, my dad was in the hospital last year,
[01:13:59.540 --> 01:14:00.820]   and I was in the hospital room,
[01:14:00.820 --> 01:14:03.660]   and his phone kept buzzing, and I look at it,
[01:14:03.660 --> 01:14:08.260]   and it's just spam attack after spam attack,
[01:14:08.260 --> 01:14:13.200]   people nonstop calling about his freaking car warranty,
[01:14:13.200 --> 01:14:16.020]   why they're trying to get his social security number.
[01:14:16.020 --> 01:14:17.700]   They're trying to get his PII.
[01:14:17.700 --> 01:14:19.900]   They're trying to get this information.
[01:14:19.900 --> 01:14:24.060]   We need to figure out how to put those people
[01:14:24.060 --> 01:14:26.720]   in jail for life. (laughs)
[01:14:26.720 --> 01:14:30.300]   And we need to figure out why in the hell
[01:14:30.300 --> 01:14:34.800]   we are being required or asked to hand over
[01:14:34.800 --> 01:14:38.860]   our social security number, and our home address,
[01:14:38.860 --> 01:14:41.420]   and our passport, all of that information
[01:14:41.420 --> 01:14:43.360]   to every retailer who asks.
[01:14:43.360 --> 01:14:45.240]   I mean, that's insanity.
[01:14:46.240 --> 01:14:49.760]   And there's no question they're not protecting it
[01:14:49.760 --> 01:14:52.760]   because it keeps showing up in spam,
[01:14:52.760 --> 01:14:57.080]   or identity theft, or credit card theft, or worse.
[01:14:57.080 --> 01:14:58.280]   - Well, the spam is getting better,
[01:14:58.280 --> 01:15:01.160]   and maybe I need to, as a side note,
[01:15:01.160 --> 01:15:02.560]   make a public announcement.
[01:15:02.560 --> 01:15:07.360]   Please clip this out, which is if you get an email
[01:15:07.360 --> 01:15:12.360]   or a message from Lex Friedman saying how much I, Lex,
[01:15:13.960 --> 01:15:16.920]   appreciate you and love you and so on,
[01:15:16.920 --> 01:15:19.640]   and please connect with me on my WhatsApp number,
[01:15:19.640 --> 01:15:23.440]   and I will give you Bitcoin or something like that,
[01:15:23.440 --> 01:15:25.020]   please do not click.
[01:15:25.020 --> 01:15:29.040]   And I'm aware that there's a lot of this going on,
[01:15:29.040 --> 01:15:30.120]   a very large amount.
[01:15:30.120 --> 01:15:32.200]   I can't do anything about it.
[01:15:32.200 --> 01:15:33.800]   This is on every single platform.
[01:15:33.800 --> 01:15:36.040]   It's happening more and more and more,
[01:15:36.040 --> 01:15:38.880]   which I've been recently informed
[01:15:38.880 --> 01:15:40.960]   that they're now emailing.
[01:15:40.960 --> 01:15:42.920]   So it's cross-platform.
[01:15:42.920 --> 01:15:46.040]   They're taking people's, they're somehow,
[01:15:46.040 --> 01:15:47.360]   this is fascinating to me,
[01:15:47.360 --> 01:15:51.360]   because they are taking people who comment
[01:15:51.360 --> 01:15:53.840]   on various social platforms,
[01:15:53.840 --> 01:15:56.040]   and they somehow reverse engineer,
[01:15:56.040 --> 01:15:57.760]   they figure out what their email is,
[01:15:57.760 --> 01:16:00.320]   and they send an email to that person
[01:16:00.320 --> 01:16:02.240]   saying, "From Lex Friedman,"
[01:16:02.240 --> 01:16:05.160]   and it's like a heartfelt email with links.
[01:16:05.160 --> 01:16:07.440]   It's fascinating because it's cross-platform now.
[01:16:07.440 --> 01:16:11.040]   It's not just a spam bot that's messaging us
[01:16:11.040 --> 01:16:13.480]   and a comment that's in a reply.
[01:16:13.480 --> 01:16:16.320]   They are saying, "Okay, this person cares
[01:16:16.320 --> 01:16:18.420]   "about this other person on social media,
[01:16:18.420 --> 01:16:20.440]   "so I'm going to find another channel,"
[01:16:20.440 --> 01:16:22.600]   which in their mind probably increases,
[01:16:22.600 --> 01:16:25.080]   and it does, the likelihood
[01:16:25.080 --> 01:16:28.960]   that they'll get the people to click, and they do.
[01:16:28.960 --> 01:16:30.160]   I don't know what to do about that.
[01:16:30.160 --> 01:16:32.120]   It makes me really, really sad,
[01:16:32.120 --> 01:16:33.680]   especially with podcasting.
[01:16:33.680 --> 01:16:36.720]   There's an intimacy that people feel connected,
[01:16:36.720 --> 01:16:37.760]   and they get really excited.
[01:16:37.760 --> 01:16:40.560]   "Okay, cool, I wanna talk to Lex."
[01:16:41.560 --> 01:16:42.840]   (sighs)
[01:16:42.840 --> 01:16:43.840]   And they click.
[01:16:43.840 --> 01:16:50.360]   And I get angry at the people that do this.
[01:16:50.360 --> 01:16:57.240]   It's like the John that gets hired, the fake employee.
[01:16:57.240 --> 01:16:58.440]   I don't know what to do about that.
[01:16:58.440 --> 01:17:02.160]   I suppose the solution is education.
[01:17:02.160 --> 01:17:06.240]   It's telling people to be skeptical on stuff they click.
[01:17:06.240 --> 01:17:09.480]   That balance with the technology solution
[01:17:09.480 --> 01:17:14.120]   of creating maybe two-factor authentication
[01:17:14.120 --> 01:17:17.880]   and maybe helping identify things
[01:17:17.880 --> 01:17:20.360]   that are likely to be spam, I don't know.
[01:17:20.360 --> 01:17:21.980]   But then the machine learning there is tricky,
[01:17:21.980 --> 01:17:25.280]   'cause you don't wanna add a lot of extra friction
[01:17:25.280 --> 01:17:28.200]   that just annoys people, because they'll turn it off,
[01:17:28.200 --> 01:17:30.720]   'cause you have the accept cookies thing, right,
[01:17:30.720 --> 01:17:32.440]   that everybody has to click on now,
[01:17:32.440 --> 01:17:34.600]   so now they completely ignore the accept cookies.
[01:17:34.600 --> 01:17:38.560]   This is very difficult (laughs)
[01:17:38.560 --> 01:17:41.160]   to find that frictionless security.
[01:17:41.160 --> 01:17:43.760]   You mentioned Snowden.
[01:17:43.760 --> 01:17:48.360]   You've talked about looking through the NSA documents
[01:17:48.360 --> 01:17:52.000]   he leaked and doing the hard work of that.
[01:17:52.000 --> 01:17:54.520]   What do you make of Edward Snowden?
[01:17:54.520 --> 01:17:56.720]   What have you learned from those documents?
[01:17:56.720 --> 01:17:57.960]   What do you think of him?
[01:17:57.960 --> 01:18:02.440]   In the long arc of history,
[01:18:02.440 --> 01:18:05.520]   is Edward Snowden a hero or a villain?
[01:18:05.520 --> 01:18:07.480]   - I think he's neither.
[01:18:07.480 --> 01:18:11.460]   I have really complicated feelings about Edward Snowden.
[01:18:11.460 --> 01:18:15.760]   On the one hand, I'm a journalist at heart,
[01:18:15.760 --> 01:18:19.600]   and more transparency is good.
[01:18:19.600 --> 01:18:24.040]   And I'm grateful for the conversations that we had
[01:18:24.040 --> 01:18:29.040]   in the post-Snowden era about the limits to surveillance
[01:18:29.640 --> 01:18:33.120]   and how critical privacy is.
[01:18:33.120 --> 01:18:35.640]   And when you have no transparency
[01:18:35.640 --> 01:18:38.080]   and you don't really know, in that case,
[01:18:38.080 --> 01:18:40.060]   what our secret courts were doing,
[01:18:40.060 --> 01:18:45.680]   how can you truly believe that our country
[01:18:45.680 --> 01:18:47.880]   is taking our civil liberties seriously?
[01:18:47.880 --> 01:18:51.320]   So on the one hand, I'm grateful
[01:18:51.320 --> 01:18:54.920]   that he cracked open these debates.
[01:18:56.280 --> 01:19:01.280]   On the other hand, when I walked into the storage closet
[01:19:01.280 --> 01:19:05.920]   of classified NSA secrets,
[01:19:05.920 --> 01:19:09.480]   I had just spent two years
[01:19:09.480 --> 01:19:14.240]   covering Chinese cyber espionage almost every day
[01:19:14.240 --> 01:19:19.240]   and the sort of advancement of Russian attacks.
[01:19:19.240 --> 01:19:23.200]   They were just getting worse and worse and more destructive.
[01:19:23.200 --> 01:19:27.440]   And there were no limits to Chinese cyber espionage
[01:19:27.440 --> 01:19:30.880]   and Chinese surveillance of its own citizens.
[01:19:30.880 --> 01:19:32.840]   And there seemed to be no limit
[01:19:32.840 --> 01:19:37.280]   to what Russia was willing to do in terms of cyber attacks
[01:19:37.280 --> 01:19:41.280]   and also, in some cases, assassinating journalists.
[01:19:41.280 --> 01:19:44.020]   So when I walked into that room,
[01:19:44.020 --> 01:19:46.760]   there was a part of me, quite honestly,
[01:19:46.760 --> 01:19:50.040]   that was relieved to know that the NSA
[01:19:50.040 --> 01:19:52.640]   was as good as I hoped they were.
[01:19:52.640 --> 01:19:57.880]   And we weren't using that knowledge
[01:19:57.880 --> 01:20:02.220]   to, as far as I know, assassinate journalists.
[01:20:02.220 --> 01:20:06.240]   We weren't using our access
[01:20:06.240 --> 01:20:11.000]   to take out pharmaceutical companies.
[01:20:11.000 --> 01:20:12.740]   For the most part, we were using it
[01:20:12.740 --> 01:20:15.600]   for traditional espionage.
[01:20:15.600 --> 01:20:18.160]   Now, that set of documents
[01:20:18.160 --> 01:20:20.200]   also set me on the journey of my book
[01:20:20.200 --> 01:20:24.440]   because to me, the American people's reaction
[01:20:24.440 --> 01:20:28.000]   to the Snowden documents was a little bit misplaced.
[01:20:28.000 --> 01:20:29.920]   They were upset
[01:20:29.920 --> 01:20:34.020]   about the phone call metadata collection program.
[01:20:34.020 --> 01:20:36.360]   Angela Merkel, I think, rightfully was upset
[01:20:36.360 --> 01:20:38.340]   that we were hacking her cell phone.
[01:20:38.340 --> 01:20:42.580]   But in sort of the spy eats spy world,
[01:20:42.580 --> 01:20:44.360]   hacking world leader's cell phones
[01:20:44.360 --> 01:20:47.320]   is pretty much what most spy agencies do.
[01:20:47.320 --> 01:20:51.560]   And there wasn't a lot that I saw in those documents
[01:20:51.560 --> 01:20:56.560]   that was beyond what I thought a spy agency does.
[01:20:56.560 --> 01:21:01.000]   And I think if there was another 9/11 tomorrow,
[01:21:01.000 --> 01:21:03.520]   God forbid, we would all say,
[01:21:03.520 --> 01:21:05.720]   how did the NSA miss this?
[01:21:05.720 --> 01:21:07.900]   Why weren't they spying on those terrorists?
[01:21:07.900 --> 01:21:10.760]   Why weren't they spying on those world leaders?
[01:21:10.760 --> 01:21:13.160]   You know, there's some of that too.
[01:21:13.160 --> 01:21:16.960]   But I think that there was great damage
[01:21:16.960 --> 01:21:21.680]   done to the US's reputation.
[01:21:21.680 --> 01:21:25.480]   I think we really lost our halo
[01:21:25.480 --> 01:21:30.440]   in terms of a protector of civil liberties.
[01:21:30.440 --> 01:21:33.680]   And I think a lot of what was reported
[01:21:33.680 --> 01:21:37.000]   was unfortunately reported in a vacuum.
[01:21:37.000 --> 01:21:39.080]   That was my biggest gripe,
[01:21:39.080 --> 01:21:42.320]   that we were always reporting,
[01:21:42.320 --> 01:21:45.220]   the NSA has this program and here's what it does.
[01:21:45.220 --> 01:21:48.680]   And the NSA is in Angela Merkel's cell phone
[01:21:48.680 --> 01:21:50.760]   and the NSA can do this.
[01:21:50.760 --> 01:21:55.640]   And no one was saying, and by the way,
[01:21:55.640 --> 01:22:00.240]   China has been hacking into our pipelines
[01:22:00.240 --> 01:22:01.480]   and they've been making off
[01:22:01.480 --> 01:22:04.400]   with all of our intellectual property.
[01:22:04.400 --> 01:22:07.800]   And Russia has been hacking into our energy infrastructure.
[01:22:07.800 --> 01:22:11.340]   And they've been using the same methods to spy on track
[01:22:11.340 --> 01:22:13.960]   and in many cases, kill their own journalists.
[01:22:13.960 --> 01:22:15.760]   And the Saudis have been doing this
[01:22:15.760 --> 01:22:17.320]   to their own critics and dissidents.
[01:22:17.320 --> 01:22:19.840]   And so you can't talk about
[01:22:19.840 --> 01:22:22.720]   any of these countries in isolation.
[01:22:22.720 --> 01:22:25.840]   It is really like spy, spy out there.
[01:22:25.840 --> 01:22:29.040]   And so I just have complicated feelings.
[01:22:29.040 --> 01:22:30.320]   You know, and the other thing is,
[01:22:30.320 --> 01:22:32.000]   and I'm sorry, this is a little bit of a tangent,
[01:22:32.000 --> 01:22:36.760]   but the amount of documents that we had,
[01:22:36.760 --> 01:22:39.800]   like thousands of documents,
[01:22:39.800 --> 01:22:41.880]   most of which were just crap,
[01:22:41.880 --> 01:22:45.980]   but had people's names on them.
[01:22:45.980 --> 01:22:48.900]   You know, part of me wishes that those documents
[01:22:48.900 --> 01:22:52.980]   had been released in a much more targeted, limited way.
[01:22:52.980 --> 01:22:56.340]   It's just a lot of it just felt like a PowerPoint
[01:22:56.340 --> 01:22:58.580]   that was taken out of context.
[01:22:58.580 --> 01:23:03.020]   And you just sort of wish
[01:23:03.020 --> 01:23:05.880]   that there had been a little bit more thought
[01:23:05.880 --> 01:23:07.700]   into what was released.
[01:23:07.700 --> 01:23:10.060]   Because I think a lot of the impact from Sony
[01:23:10.060 --> 01:23:13.380]   was just the volume of the reporting.
[01:23:13.380 --> 01:23:17.040]   But I think, you know, based on what I saw personally,
[01:23:17.040 --> 01:23:20.380]   there was a lot of stuff that I just,
[01:23:20.380 --> 01:23:24.100]   I don't know why that particular thing got released.
[01:23:24.100 --> 01:23:26.780]   - As a whistleblower, what's a better way to do it?
[01:23:26.780 --> 01:23:28.660]   'Cause I mean, there's fear, there's,
[01:23:28.660 --> 01:23:33.560]   it takes a lot of effort to do a more targeted release.
[01:23:33.560 --> 01:23:34.960]   You know, if there's proper channels,
[01:23:34.960 --> 01:23:38.220]   you're afraid that those channels will be manipulated.
[01:23:38.220 --> 01:23:39.460]   Like, who do you trust?
[01:23:39.460 --> 01:23:43.540]   What's a better way to do this, do you think?
[01:23:43.540 --> 01:23:46.580]   As a journalist, this is almost a good journalistic question.
[01:23:46.580 --> 01:23:49.620]   Reveal some fundamental flaw in the system
[01:23:49.620 --> 01:23:51.220]   without destroying the system.
[01:23:51.220 --> 01:23:54.860]   I bring up, you know, again, Mark Zuckerberg and Meta.
[01:23:54.860 --> 01:23:59.100]   There was a whistleblower that came out
[01:23:59.100 --> 01:24:02.100]   about Instagram internal studies.
[01:24:02.100 --> 01:24:06.980]   And I also am torn about how to feel about that whistleblower
[01:24:06.980 --> 01:24:10.800]   because from a company perspective, that's an open culture.
[01:24:10.800 --> 01:24:14.880]   How can you operate successfully if you have an open culture
[01:24:14.880 --> 01:24:17.900]   where any one whistleblower can come out,
[01:24:17.900 --> 01:24:19.360]   out of context, take a study,
[01:24:19.360 --> 01:24:21.800]   whether it represents a larger context or not.
[01:24:21.800 --> 01:24:25.400]   And the press eats it up.
[01:24:25.400 --> 01:24:28.980]   And then that creates a narrative that is,
[01:24:28.980 --> 01:24:30.660]   just like with the NSA, you said,
[01:24:30.660 --> 01:24:34.560]   that's out of context, very targeted to where,
[01:24:34.560 --> 01:24:38.880]   well, Facebook is evil, clearly, because of this one leak.
[01:24:38.880 --> 01:24:40.520]   It's really hard to know what to do there
[01:24:40.520 --> 01:24:42.040]   'cause we're now in a society
[01:24:42.040 --> 01:24:44.520]   that's deeply distressed institutions.
[01:24:44.520 --> 01:24:49.120]   And so narratives by whistleblowers make that whistleblower
[01:24:49.120 --> 01:24:52.240]   and their forthcoming book very popular.
[01:24:52.240 --> 01:24:54.680]   And so there's a huge incentive to take stuff
[01:24:54.680 --> 01:24:56.800]   out of context and to tell stories
[01:24:56.800 --> 01:25:00.340]   that don't represent the full context, the full truth.
[01:25:00.340 --> 01:25:03.100]   It's hard to know what to do with that
[01:25:03.100 --> 01:25:06.940]   'cause then that forces Facebook and Meta and governments
[01:25:06.940 --> 01:25:09.800]   to be much more conservative, much more secretive.
[01:25:09.800 --> 01:25:13.180]   It's like a race to the bottom.
[01:25:13.180 --> 01:25:14.580]   I don't know.
[01:25:14.580 --> 01:25:16.260]   I don't know if you can comment on any of that,
[01:25:16.260 --> 01:25:19.320]   how to be a whistleblower ethically and properly.
[01:25:19.320 --> 01:25:21.620]   - I don't know.
[01:25:21.620 --> 01:25:23.420]   I mean, these are hard questions.
[01:25:23.420 --> 01:25:27.280]   And even for myself, in some ways,
[01:25:27.280 --> 01:25:31.740]   I think of my book as sort of blowing the whistle
[01:25:31.740 --> 01:25:33.980]   on the underground zero-day market.
[01:25:33.980 --> 01:25:38.860]   But it's not like I was in the market myself.
[01:25:38.860 --> 01:25:41.460]   It's not like I had access to classified data
[01:25:41.460 --> 01:25:43.460]   when I was reporting out that book.
[01:25:43.460 --> 01:25:46.380]   As I say in the book,
[01:25:46.380 --> 01:25:49.500]   "Listen, I'm just trying to scrape the surface here
[01:25:49.500 --> 01:25:53.040]   "so we can have these conversations before it's too late."
[01:25:53.040 --> 01:25:57.100]   And I'm sure there's plenty in there
[01:25:57.100 --> 01:26:01.800]   that someone who's US intelligence agency's
[01:26:01.800 --> 01:26:03.340]   preeminent zero-day broker
[01:26:03.340 --> 01:26:05.940]   probably has some voodoo doll of me out there.
[01:26:05.940 --> 01:26:10.700]   And you're never gonna get it 100%.
[01:26:10.700 --> 01:26:14.900]   But I really applaud whistleblowers
[01:26:14.900 --> 01:26:19.060]   like the whistleblower who blew the whistle
[01:26:19.060 --> 01:26:22.340]   on the Trump call with Zelensky.
[01:26:22.340 --> 01:26:25.340]   I mean, people needed to know about that,
[01:26:25.340 --> 01:26:30.340]   that we were basically, in some ways, blackmailing an ally
[01:26:30.340 --> 01:26:33.780]   to try to influence an election.
[01:26:33.780 --> 01:26:37.300]   I mean, they went through the proper channels.
[01:26:37.300 --> 01:26:39.480]   They weren't trying to profit off of it, right?
[01:26:39.480 --> 01:26:42.100]   There was no book that came out afterwards
[01:26:42.100 --> 01:26:44.100]   from that whistleblower.
[01:26:44.100 --> 01:26:45.780]   That whistleblower's not like,
[01:26:45.780 --> 01:26:47.900]   they went through the channels.
[01:26:47.900 --> 01:26:49.340]   They're not living in Moscow.
[01:26:49.340 --> 01:26:51.180]   You know, let's put it that way.
[01:26:51.180 --> 01:26:52.020]   - Can I ask you a question?
[01:26:52.020 --> 01:26:54.300]   You mentioned NSA, one of the things it showed
[01:26:54.300 --> 01:26:58.180]   is they're pretty good at what they do.
[01:26:58.180 --> 01:27:03.460]   Again, this is a touchy subject, I suppose,
[01:27:03.460 --> 01:27:06.260]   but there's a lot of conspiracy theories
[01:27:06.260 --> 01:27:08.020]   about intelligence agencies.
[01:27:08.020 --> 01:27:11.180]   From your understanding of intelligence agencies,
[01:27:11.180 --> 01:27:15.980]   the CIA, NSA, and the equivalent in other countries,
[01:27:15.980 --> 01:27:19.980]   are they, one question, this could be a dangerous question,
[01:27:19.980 --> 01:27:23.560]   are they competent, are they good at what they do?
[01:27:23.560 --> 01:27:28.880]   And two, are they malevolent in any way?
[01:27:28.880 --> 01:27:32.620]   Sort of, I recently had a conversation
[01:27:32.620 --> 01:27:35.140]   about tobacco companies.
[01:27:35.140 --> 01:27:39.500]   They kind of see their customers as dupes.
[01:27:39.500 --> 01:27:42.440]   Like, they can just play games with people.
[01:27:42.440 --> 01:27:46.400]   Conspiracy theories tell that similar story
[01:27:46.400 --> 01:27:48.500]   about intelligence agencies,
[01:27:48.500 --> 01:27:51.700]   that they're interested in manipulating the populace
[01:27:51.700 --> 01:27:56.700]   for whatever ends the powerful in dark rooms,
[01:27:56.700 --> 01:28:01.280]   cigarette smoke, cigar smoke-filled rooms.
[01:28:01.280 --> 01:28:04.620]   What's your sense?
[01:28:04.620 --> 01:28:09.620]   Do these conspiracy theories have kind of any truth to them?
[01:28:09.620 --> 01:28:14.420]   Or are intelligence agencies, for the most part,
[01:28:14.420 --> 01:28:15.700]   good for society?
[01:28:15.700 --> 01:28:17.340]   - Okay, well, that's an easy one.
[01:28:17.340 --> 01:28:18.660]   (Bridget laughing)
[01:28:18.660 --> 01:28:20.140]   - Is it? - No.
[01:28:20.140 --> 01:28:23.780]   I think, you know, depends which intelligence agency.
[01:28:23.780 --> 01:28:25.340]   Think about the Mossad.
[01:28:25.340 --> 01:28:30.340]   You know, they're killing every Iranian nuclear scientist
[01:28:30.340 --> 01:28:34.540]   they can over the years, you know?
[01:28:34.540 --> 01:28:38.740]   But have they delayed the time horizon
[01:28:38.740 --> 01:28:40.540]   before Iran gets the bomb?
[01:28:40.540 --> 01:28:41.380]   Yeah.
[01:28:41.380 --> 01:28:45.860]   Have they probably staved off terror attacks
[01:28:45.860 --> 01:28:47.400]   on their own citizens? Yeah.
[01:28:47.400 --> 01:28:53.320]   You know, none of these, intelligence is intelligence.
[01:28:53.320 --> 01:28:54.820]   You know, you can't just say, like,
[01:28:54.820 --> 01:28:59.820]   they're malevolent or they're heroes, you know?
[01:28:59.820 --> 01:29:02.620]   Everyone I have met in this space
[01:29:02.620 --> 01:29:07.420]   is not like the pound-your-chest patriot
[01:29:07.420 --> 01:29:11.580]   that you see on the beach on the 4th of July.
[01:29:11.580 --> 01:29:15.180]   A lot of them have complicated feelings
[01:29:15.180 --> 01:29:17.700]   about their former employers.
[01:29:17.700 --> 01:29:20.540]   Well, at least at the NSA reminded me
[01:29:20.540 --> 01:29:25.540]   to do what we were accused of doing after Snowden,
[01:29:25.540 --> 01:29:28.820]   to spy on Americans.
[01:29:28.820 --> 01:29:33.340]   You have no idea the amount of red tape
[01:29:33.340 --> 01:29:37.820]   and paperwork and bureaucracy it would have taken
[01:29:37.820 --> 01:29:41.180]   to do what everyone thinks that we were supposedly doing.
[01:29:42.500 --> 01:29:45.060]   But then, you know, we find out
[01:29:45.060 --> 01:29:46.860]   in the course of the Snowden reporting
[01:29:46.860 --> 01:29:49.480]   about a program called Lovin'
[01:29:49.480 --> 01:29:51.900]   where a couple of the NSA analysts
[01:29:51.900 --> 01:29:55.380]   were using their access to spy on their ex-girlfriends.
[01:29:55.380 --> 01:29:59.280]   So, you know, there's an exception to every case.
[01:29:59.280 --> 01:30:05.020]   Generally, I will probably get, you know,
[01:30:05.020 --> 01:30:07.620]   accused of my Western bias here again,
[01:30:07.620 --> 01:30:12.620]   but I think you can almost barely compare
[01:30:12.620 --> 01:30:17.300]   some of these Western intelligence agencies
[01:30:17.300 --> 01:30:20.040]   to China, for instance.
[01:30:20.040 --> 01:30:25.040]   And the surveillance that they're deploying on the Uyghurs
[01:30:25.040 --> 01:30:28.140]   to the level they're deploying it,
[01:30:28.140 --> 01:30:32.160]   and the surveillance they're starting to export abroad
[01:30:32.160 --> 01:30:34.620]   with some of the programs like the watering hole attack
[01:30:34.620 --> 01:30:35.540]   I mentioned earlier,
[01:30:35.540 --> 01:30:39.020]   where it's not just hitting the Uyghurs inside China,
[01:30:39.020 --> 01:30:40.380]   it's hitting anyone interested
[01:30:40.380 --> 01:30:42.100]   in the Uyghur plight outside China.
[01:30:42.100 --> 01:30:44.620]   I mean, it could be an American high school student
[01:30:44.620 --> 01:30:46.560]   writing a paper on the Uyghurs.
[01:30:46.560 --> 01:30:49.140]   They wanna spy on that person too.
[01:30:49.140 --> 01:30:51.780]   You know, there's no rules in China
[01:30:51.780 --> 01:30:55.820]   really limiting the extent of that surveillance.
[01:30:55.820 --> 01:30:58.620]   And we all better pay attention
[01:30:58.620 --> 01:30:59.980]   to what's happening with the Uyghurs
[01:30:59.980 --> 01:31:04.060]   because just as Ukraine has been to Russia
[01:31:04.060 --> 01:31:07.420]   in terms of a test kitchen for its cyber attacks,
[01:31:07.420 --> 01:31:12.900]   the Uyghurs are China's test kitchen for surveillance.
[01:31:12.900 --> 01:31:15.300]   And there's no doubt in my mind
[01:31:15.300 --> 01:31:17.660]   that they're testing them on the Uyghurs.
[01:31:17.660 --> 01:31:19.140]   Uyghurs are their Petri dish,
[01:31:19.140 --> 01:31:21.220]   and eventually they will export
[01:31:21.220 --> 01:31:23.860]   that level of surveillance overseas.
[01:31:23.860 --> 01:31:28.860]   I mean, in 2015, Obama and Xi Jinping reached a deal
[01:31:31.700 --> 01:31:34.460]   where basically the White House said,
[01:31:34.460 --> 01:31:38.620]   "You better cut it out on intellectual property theft."
[01:31:38.620 --> 01:31:40.140]   And so they made this agreement
[01:31:40.140 --> 01:31:43.620]   that they would not hack each other for commercial benefit.
[01:31:43.620 --> 01:31:45.700]   And for a period of about 18 months,
[01:31:45.700 --> 01:31:49.060]   we saw this huge drop-off in Chinese cyber attacks
[01:31:49.060 --> 01:31:53.100]   on American companies, but some of them continued.
[01:31:53.100 --> 01:31:54.300]   Where did they continue?
[01:31:54.300 --> 01:31:58.420]   They continued on aviation companies,
[01:31:58.420 --> 01:32:01.100]   on hospitality companies like Marriott,
[01:32:01.980 --> 01:32:02.900]   why?
[01:32:02.900 --> 01:32:05.700]   Because that was still considered fair game to China.
[01:32:05.700 --> 01:32:07.420]   It wasn't IP theft they were after.
[01:32:07.420 --> 01:32:11.820]   They wanted to know who was staying in this city
[01:32:11.820 --> 01:32:15.020]   at this time when Chinese citizens were staying there
[01:32:15.020 --> 01:32:17.420]   so they could cross match for counterintelligence
[01:32:17.420 --> 01:32:20.220]   who might be a likely Chinese spy.
[01:32:20.220 --> 01:32:22.740]   I'm sure we're doing some of that too.
[01:32:22.740 --> 01:32:24.780]   Counterintelligence is counterintelligence.
[01:32:24.780 --> 01:32:26.300]   It's considered fair game.
[01:32:26.300 --> 01:32:30.420]   But where I think it gets evil
[01:32:30.420 --> 01:32:34.220]   is when you use it for censorship,
[01:32:34.220 --> 01:32:36.100]   to suppress any dissent,
[01:32:36.100 --> 01:32:41.780]   to do what I've seen the UAE do to its citizens
[01:32:41.780 --> 01:32:44.180]   where people who've gone on Twitter
[01:32:44.180 --> 01:32:47.580]   just to advocate for better voting rights,
[01:32:47.580 --> 01:32:49.340]   more enfranchisement,
[01:32:49.340 --> 01:32:52.480]   suddenly find their passports confiscated.
[01:32:52.480 --> 01:32:57.340]   I talked to one critic, Ahmed Mansour,
[01:32:57.340 --> 01:33:00.980]   and he told me, "You might find yourself a terrorist,
[01:33:00.980 --> 01:33:02.320]   "labeled a terrorist one day,
[01:33:02.320 --> 01:33:04.700]   "you don't even know how to operate a gun."
[01:33:04.700 --> 01:33:06.500]   I mean, he'd been beaten up
[01:33:06.500 --> 01:33:08.020]   every time he tried to go somewhere.
[01:33:08.020 --> 01:33:09.660]   His passport had been confiscated.
[01:33:09.660 --> 01:33:10.600]   By that point, it turned out
[01:33:10.600 --> 01:33:12.100]   they'd already hacked into his phone
[01:33:12.100 --> 01:33:14.160]   so they were listening to us talking.
[01:33:14.160 --> 01:33:16.300]   They'd hacked into his baby monitor
[01:33:16.300 --> 01:33:18.540]   so they're spying on his child.
[01:33:18.540 --> 01:33:21.340]   And they stole his car.
[01:33:21.340 --> 01:33:24.680]   And then they created a new law
[01:33:24.680 --> 01:33:27.780]   that you couldn't criticize the ruling family
[01:33:27.780 --> 01:33:29.620]   or the ruling party on Twitter.
[01:33:29.620 --> 01:33:32.300]   And he's been in solitary confinement every day
[01:33:32.300 --> 01:33:34.180]   since on hunger strike.
[01:33:34.180 --> 01:33:36.260]   So that's evil.
[01:33:36.260 --> 01:33:37.760]   That's evil.
[01:33:37.760 --> 01:33:40.780]   And we don't do that here.
[01:33:40.780 --> 01:33:42.060]   We have rules here.
[01:33:42.060 --> 01:33:44.500]   We don't cross that line.
[01:33:44.500 --> 01:33:47.860]   So yeah, in some cases, I won't go to Dubai.
[01:33:47.860 --> 01:33:50.000]   I won't go to Abu Dhabi.
[01:33:50.000 --> 01:33:52.660]   If I ever wanna go to the Maldives, too bad.
[01:33:52.660 --> 01:33:54.920]   Most of the flights go through Dubai.
[01:33:54.920 --> 01:33:57.200]   - So there's some lines we're not willing to cross.
[01:33:57.200 --> 01:33:59.000]   But then again, just like you said,
[01:33:59.000 --> 01:34:02.720]   there's individuals within NSA, within CIA,
[01:34:02.720 --> 01:34:05.800]   and they may have power.
[01:34:05.800 --> 01:34:07.880]   And to me, there's levels of evil.
[01:34:07.880 --> 01:34:11.360]   To me personally, this is the stuff of conspiracy theories,
[01:34:11.360 --> 01:34:13.920]   is the things you've mentioned as evil
[01:34:13.920 --> 01:34:16.200]   are more direct attacks.
[01:34:16.200 --> 01:34:19.200]   But there's also psychological warfare.
[01:34:19.200 --> 01:34:20.840]   So blackmail.
[01:34:20.840 --> 01:34:25.600]   So what does spying allow you to do?
[01:34:25.600 --> 01:34:27.860]   Allow you to collect information
[01:34:27.860 --> 01:34:30.180]   if you have something that's embarrassing.
[01:34:30.180 --> 01:34:33.620]   Or if you have like Jeffrey Epstein conspiracy theories,
[01:34:33.620 --> 01:34:38.500]   active, what is it, manufacture of embarrassing things,
[01:34:38.500 --> 01:34:41.120]   and then use blackmail to manipulate the population
[01:34:41.120 --> 01:34:42.900]   or all the powerful people involved.
[01:34:42.900 --> 01:34:45.860]   It troubles me deeply that MIT allowed somebody
[01:34:45.860 --> 01:34:48.620]   like Jeffrey Epstein in their midst,
[01:34:48.620 --> 01:34:51.660]   especially some of the scientists I admire
[01:34:51.660 --> 01:34:54.300]   that they would hang out with that person at all.
[01:34:54.300 --> 01:34:57.880]   And so I'll talk about it sometimes.
[01:34:57.880 --> 01:35:00.220]   And then a lot of people tell me,
[01:35:00.220 --> 01:35:01.340]   "Well, obviously,
[01:35:01.340 --> 01:35:04.380]   "Jeffrey Epstein is a front for intelligence."
[01:35:04.380 --> 01:35:09.380]   And I just, I struggle to see that level of competence
[01:35:09.380 --> 01:35:10.760]   and malevolence.
[01:35:10.760 --> 01:35:16.340]   But who the hell am I?
[01:35:17.260 --> 01:35:21.180]   And I guess I was trying to get to that point.
[01:35:21.180 --> 01:35:23.020]   You said that there's bureaucracy and so on,
[01:35:23.020 --> 01:35:25.740]   which makes some of these things very difficult.
[01:35:25.740 --> 01:35:27.420]   I wonder how much malevolence,
[01:35:27.420 --> 01:35:31.660]   how much competence there is in these institutions.
[01:35:31.660 --> 01:35:34.860]   Like how far, this takes us back to the hacking question.
[01:35:34.860 --> 01:35:39.860]   How far are people willing to go if they have the power?
[01:35:39.860 --> 01:35:41.700]   This has to do with social engineering.
[01:35:41.700 --> 01:35:42.780]   This has to do with hacking.
[01:35:42.780 --> 01:35:45.380]   This has to do with manipulating people,
[01:35:45.380 --> 01:35:47.340]   attacking people, doing evil onto people,
[01:35:47.340 --> 01:35:50.260]   psychological warfare and stuff like that.
[01:35:50.260 --> 01:35:51.460]   I don't know.
[01:35:51.460 --> 01:35:53.980]   I believe that most people are good.
[01:35:53.980 --> 01:35:59.380]   And I don't think that's possible in a free society.
[01:35:59.380 --> 01:36:00.420]   There's something that happens
[01:36:00.420 --> 01:36:02.540]   when you have a centralized government
[01:36:02.540 --> 01:36:05.580]   where power corrupts over time
[01:36:05.580 --> 01:36:08.940]   and you start surveillance programs.
[01:36:08.940 --> 01:36:12.940]   It's like a slippery slope that over time
[01:36:12.940 --> 01:36:17.940]   starts to both use fear and direct manipulation
[01:36:17.940 --> 01:36:20.020]   to control the populace.
[01:36:20.020 --> 01:36:22.200]   But in a free society, I just,
[01:36:22.200 --> 01:36:25.100]   it's difficult for me to imagine
[01:36:25.100 --> 01:36:27.660]   that you can have somebody like a Jeffrey Epstein
[01:36:27.660 --> 01:36:29.340]   in front for intelligence.
[01:36:29.340 --> 01:36:31.700]   I don't know what I'm asking you, but I'm just,
[01:36:31.700 --> 01:36:36.900]   I have a hope that for the most part,
[01:36:36.900 --> 01:36:39.860]   intelligence agencies are trying to do good
[01:36:39.860 --> 01:36:41.960]   and are actually doing good for the world.
[01:36:42.960 --> 01:36:45.680]   When you view it in the full context
[01:36:45.680 --> 01:36:47.680]   of the complexities of the world.
[01:36:47.680 --> 01:36:55.160]   But then again, if they're not, would we know?
[01:36:55.160 --> 01:36:58.360]   That's why Edward Snowden might be a good thing.
[01:36:58.360 --> 01:37:00.480]   Let me ask you on a personal question.
[01:37:00.480 --> 01:37:03.240]   You have investigated some of the most powerful organizations
[01:37:03.240 --> 01:37:07.600]   and people in the world of cyber warfare, cybersecurity.
[01:37:07.600 --> 01:37:09.600]   Are you ever afraid for your own life,
[01:37:09.600 --> 01:37:13.280]   your own well-being, digital or physical?
[01:37:13.280 --> 01:37:14.920]   - I mean, I've had my moments.
[01:37:14.920 --> 01:37:20.120]   I've had our security team at the Times
[01:37:20.120 --> 01:37:21.920]   called me at one point and said,
[01:37:21.920 --> 01:37:25.760]   "Someone's on the dark web offering good money
[01:37:25.760 --> 01:37:28.980]   "to anyone who can hack your phone or your laptop."
[01:37:28.980 --> 01:37:32.660]   I describe in my book how when I was
[01:37:32.660 --> 01:37:34.480]   at that hacking conference in Argentina,
[01:37:34.480 --> 01:37:38.680]   I came back and I brought a burner laptop with me,
[01:37:38.680 --> 01:37:40.760]   but I'd kept it in the safe anyway
[01:37:40.760 --> 01:37:42.480]   and it didn't have anything on it,
[01:37:42.480 --> 01:37:45.320]   but someone had broken in and it was moved.
[01:37:45.320 --> 01:37:51.720]   I've had all sorts of sort of scary moments.
[01:37:51.720 --> 01:37:55.520]   And then I've had moments where I think I went
[01:37:55.520 --> 01:37:58.920]   just way too far into the paranoid side.
[01:37:58.920 --> 01:38:03.920]   I mean, I remember writing about the Times hack by China
[01:38:03.920 --> 01:38:07.460]   and I just covered a number of Chinese cyber attacks
[01:38:07.460 --> 01:38:10.080]   where they'd gotten into the thermostat
[01:38:10.080 --> 01:38:11.600]   at someone's corporate apartment
[01:38:11.600 --> 01:38:15.800]   and they'd gotten into all sorts of stuff.
[01:38:15.800 --> 01:38:17.760]   And I was living by myself.
[01:38:17.760 --> 01:38:19.960]   I was single in San Francisco
[01:38:19.960 --> 01:38:24.680]   and my cable box on my television started making
[01:38:24.680 --> 01:38:26.960]   some weird noises in the middle of the night.
[01:38:26.960 --> 01:38:29.720]   And I got up and I ripped it out of the wall
[01:38:29.720 --> 01:38:32.080]   and I think I said something like embarrassing,
[01:38:32.080 --> 01:38:33.560]   like, "Fuck you, China."
[01:38:33.560 --> 01:38:35.720]   (both laughing)
[01:38:35.720 --> 01:38:39.360]   And then I went back to bed and I woke up
[01:38:39.360 --> 01:38:41.680]   and this beautiful morning light,
[01:38:41.680 --> 01:38:42.760]   I mean, I'll never forget it.
[01:38:42.760 --> 01:38:44.640]   This is like glimmering morning light
[01:38:44.640 --> 01:38:48.120]   is shining on my cable box, which has now been ripped out
[01:38:48.120 --> 01:38:50.680]   and is sitting on my floor and the morning light.
[01:38:50.680 --> 01:38:54.160]   And I was just like, "No, no, no.
[01:38:54.160 --> 01:38:57.240]   "I'm not going down that road."
[01:38:57.240 --> 01:39:02.240]   Basically, I came to a fork in the road
[01:39:03.440 --> 01:39:06.160]   where I could either go full tinfoil hat,
[01:39:06.160 --> 01:39:10.080]   go live off the grid, never have a car with navigation,
[01:39:10.080 --> 01:39:12.520]   never use Google Maps, never own an iPhone,
[01:39:12.520 --> 01:39:17.520]   never order diapers off Amazon, create an alias,
[01:39:17.520 --> 01:39:22.280]   or I could just do the best I can
[01:39:22.280 --> 01:39:26.100]   and live in this new digital world we're living in.
[01:39:26.100 --> 01:39:28.120]   And what does that look like for me?
[01:39:28.120 --> 01:39:30.800]   I mean, what are my crown jewels?
[01:39:30.800 --> 01:39:31.680]   This is what I tell people.
[01:39:31.680 --> 01:39:32.800]   What are your crown jewels?
[01:39:32.800 --> 01:39:34.200]   'Cause just focus on that.
[01:39:34.200 --> 01:39:35.680]   You can't protect everything,
[01:39:35.680 --> 01:39:37.520]   but you can protect your crown jewels.
[01:39:37.520 --> 01:39:39.040]   For me, for the longest time,
[01:39:39.040 --> 01:39:42.360]   my crown jewels were my sources.
[01:39:42.360 --> 01:39:44.560]   I was nothing without my sources.
[01:39:44.560 --> 01:39:46.400]   So I had some sources.
[01:39:46.400 --> 01:39:49.440]   I would meet the same dim sum place,
[01:39:49.440 --> 01:39:51.680]   or maybe it was a different restaurant,
[01:39:51.680 --> 01:39:55.460]   on the same date every quarter.
[01:39:55.460 --> 01:39:59.080]   And we would never drive there.
[01:39:59.080 --> 01:40:00.400]   We would never Uber there.
[01:40:00.400 --> 01:40:02.080]   We wouldn't bring any devices.
[01:40:02.080 --> 01:40:05.040]   I could bring a pencil and a notepad.
[01:40:05.040 --> 01:40:07.440]   And if someone wasn't in town,
[01:40:07.440 --> 01:40:09.160]   there were a couple times where I'd show up
[01:40:09.160 --> 01:40:11.160]   and the source never came,
[01:40:11.160 --> 01:40:14.040]   but we never communicated digitally.
[01:40:14.040 --> 01:40:16.480]   And those were the lengths I was willing to go
[01:40:16.480 --> 01:40:19.560]   to protect that source, but you can't do it for everyone.
[01:40:19.560 --> 01:40:22.320]   So for everyone else, it was signal,
[01:40:22.320 --> 01:40:24.760]   using two-factor authentication,
[01:40:24.760 --> 01:40:26.980]   keeping my devices up to date,
[01:40:26.980 --> 01:40:28.760]   not clicking on phishing emails,
[01:40:28.760 --> 01:40:30.680]   using a password manager,
[01:40:30.680 --> 01:40:34.520]   all the things that we know we're supposed to do.
[01:40:34.520 --> 01:40:36.120]   And that's what I tell everyone.
[01:40:36.120 --> 01:40:39.320]   Don't go crazy, because then that's the ultimate hack.
[01:40:39.320 --> 01:40:41.160]   Then they've hacked your mind,
[01:40:41.160 --> 01:40:43.640]   whoever they is for you.
[01:40:43.640 --> 01:40:45.200]   But just do the best you can.
[01:40:45.200 --> 01:40:49.720]   Now, my whole risk model changed when I had a kid.
[01:40:49.720 --> 01:40:54.760]   Now it's, oh God,
[01:40:54.760 --> 01:40:58.640]   if anyone threatened my family,
[01:40:58.640 --> 01:40:59.820]   God help them.
[01:40:59.820 --> 01:41:02.760]   (both laughing)
[01:41:02.760 --> 01:41:07.560]   But it changes you.
[01:41:07.560 --> 01:41:12.560]   And unfortunately, there are some things
[01:41:12.560 --> 01:41:15.040]   I was really scared to go deep on,
[01:41:15.040 --> 01:41:20.040]   like Russian cybercrime, like Putin himself.
[01:41:20.040 --> 01:41:22.560]   And it's interesting, I have a mentor
[01:41:22.560 --> 01:41:24.600]   who's an incredible person
[01:41:24.600 --> 01:41:29.560]   who was the Times Moscow Bureau Chief during the Cold War.
[01:41:29.560 --> 01:41:32.040]   And after I wrote a series of stories
[01:41:32.040 --> 01:41:35.040]   about Chinese cyberespionage, he took me out to lunch.
[01:41:35.040 --> 01:41:37.960]   And he told me that when he was living in Moscow,
[01:41:37.960 --> 01:41:40.100]   he would drop his kids off at preschool
[01:41:40.100 --> 01:41:42.660]   when they were my son's age now,
[01:41:42.660 --> 01:41:44.800]   and the KGB would follow him.
[01:41:44.800 --> 01:41:47.720]   And they would make a really loud show of it.
[01:41:47.720 --> 01:41:51.360]   They'd tail him, they'd honk,
[01:41:51.360 --> 01:41:55.240]   they'd just be a wreck, make a ruckus.
[01:41:55.240 --> 01:41:56.080]   And he said, "You know what?
[01:41:56.080 --> 01:41:57.540]   "They never actually did anything,
[01:41:57.620 --> 01:42:00.720]   "but they wanted me to know that they were following me.
[01:42:00.720 --> 01:42:03.000]   "And I operated accordingly."
[01:42:03.000 --> 01:42:05.760]   And he says, "That's how you should operate
[01:42:05.760 --> 01:42:08.140]   "in the digital world.
[01:42:08.140 --> 01:42:11.580]   "Know that there are probably people following you.
[01:42:11.580 --> 01:42:14.320]   "Sometimes they'll make a little bit of noise.
[01:42:14.320 --> 01:42:17.460]   "But one thing you need to know is that
[01:42:17.460 --> 01:42:18.540]   "while you're at the New York Times,
[01:42:18.540 --> 01:42:21.300]   "you have a little bit of an invisible shield on you.
[01:42:21.300 --> 01:42:23.580]   "You know, if something were to happen to you,
[01:42:23.580 --> 01:42:25.000]   "that would be a really big deal.
[01:42:25.000 --> 01:42:27.000]   "That would be an international incident."
[01:42:27.000 --> 01:42:29.380]   So I kind of carried that invisible shield
[01:42:29.380 --> 01:42:31.540]   with me for years.
[01:42:31.540 --> 01:42:34.360]   And then Jamal Khashoggi happened.
[01:42:34.360 --> 01:42:39.340]   And that destroyed my vision of my invisible shield.
[01:42:39.340 --> 01:42:42.820]   You know, sure, he was a Saudi,
[01:42:42.820 --> 01:42:45.620]   but he was a Washington Post columnist.
[01:42:45.620 --> 01:42:46.940]   You know, for the most part,
[01:42:46.940 --> 01:42:48.140]   he was living in the United States.
[01:42:48.140 --> 01:42:50.020]   He was a journalist.
[01:42:50.020 --> 01:42:53.620]   And for them to do what they did to him
[01:42:53.620 --> 01:42:58.200]   pretty much in the open and get away with it,
[01:42:58.200 --> 01:43:01.880]   and for the United States to let them get away with it
[01:43:01.880 --> 01:43:04.680]   because we wanted to preserve diplomatic relations
[01:43:04.680 --> 01:43:06.080]   with the Saudis,
[01:43:06.080 --> 01:43:10.520]   that really threw my worldview upside down.
[01:43:10.520 --> 01:43:13.760]   And, you know, I think that sent a message
[01:43:13.760 --> 01:43:15.600]   to a lot of countries
[01:43:15.600 --> 01:43:19.520]   that it was sort of open season on journalists.
[01:43:19.520 --> 01:43:23.000]   And to me, that was one of the most destructive things
[01:43:23.000 --> 01:43:27.400]   that happened under the previous administration.
[01:43:27.400 --> 01:43:30.280]   And, you know, I don't really know
[01:43:30.280 --> 01:43:32.320]   what to think of my invisible shield anymore.
[01:43:32.320 --> 01:43:33.160]   - Take a second.
[01:43:33.160 --> 01:43:34.920]   That really worries me on the journalism side
[01:43:34.920 --> 01:43:37.400]   that people would be afraid to dig deep
[01:43:37.400 --> 01:43:39.080]   on fascinating topics.
[01:43:39.080 --> 01:43:44.400]   And, you know, I have my own,
[01:43:44.400 --> 01:43:48.280]   part of the reason,
[01:43:48.280 --> 01:43:50.160]   I would love to have kids,
[01:43:50.160 --> 01:43:51.680]   I would love to have a family.
[01:43:52.720 --> 01:43:56.680]   Part of the reason I'm a little bit afraid,
[01:43:56.680 --> 01:43:57.940]   there's many ways to phrase this,
[01:43:57.940 --> 01:44:01.120]   but the loss of freedom in the way of
[01:44:01.120 --> 01:44:05.000]   doing all the crazy shit that I naturally do,
[01:44:05.000 --> 01:44:07.880]   which I would say the ethic of journalism
[01:44:07.880 --> 01:44:08.840]   is kind of not,
[01:44:08.840 --> 01:44:11.720]   is doing crazy shit without really thinking about it,
[01:44:11.720 --> 01:44:13.440]   is letting your curiosity
[01:44:13.440 --> 01:44:18.480]   really allow you to be free and explore.
[01:44:18.480 --> 01:44:22.040]   It's, I mean, whether it's stupidity or fearlessness,
[01:44:22.040 --> 01:44:25.280]   whatever it is, that's what great journalism is.
[01:44:25.280 --> 01:44:30.240]   And all the concerns about security risks
[01:44:30.240 --> 01:44:32.800]   have made me become a better person.
[01:44:32.800 --> 01:44:35.000]   The way I approach it is
[01:44:35.000 --> 01:44:37.320]   just make sure you don't have anything to hide.
[01:44:37.320 --> 01:44:39.040]   I know this is not a thing,
[01:44:39.040 --> 01:44:41.920]   this is not an approach to security.
[01:44:41.920 --> 01:44:44.920]   I'm just, this is like a motivational speech or something.
[01:44:44.920 --> 01:44:47.360]   It's just like, if you can lose,
[01:44:47.360 --> 01:44:49.280]   you can be hacked at any moment.
[01:44:49.280 --> 01:44:51.580]   Just don't be a douchebag secretly.
[01:44:52.140 --> 01:44:54.420]   Just be like a good person.
[01:44:54.420 --> 01:44:56.660]   'Cause then, I see this actually
[01:44:56.660 --> 01:44:58.660]   with social media in general.
[01:44:58.660 --> 01:45:03.820]   Just present yourself in the most authentic way possible,
[01:45:03.820 --> 01:45:06.860]   meaning be the same person online as you are privately,
[01:45:06.860 --> 01:45:08.060]   have nothing to hide.
[01:45:08.060 --> 01:45:09.960]   That's one, not the only,
[01:45:09.960 --> 01:45:13.540]   but one of the ways to achieve security.
[01:45:13.540 --> 01:45:15.780]   Maybe I'm totally wrong on this,
[01:45:15.780 --> 01:45:19.500]   but don't be secretly weird.
[01:45:19.500 --> 01:45:21.860]   If you're weird, be publicly weird
[01:45:21.860 --> 01:45:24.300]   so it's impossible to blackmail you.
[01:45:24.300 --> 01:45:25.420]   That's my approach to security.
[01:45:25.420 --> 01:45:26.580]   - Yeah, well, they call it
[01:45:26.580 --> 01:45:29.900]   the New York Times front page phenomenon.
[01:45:29.900 --> 01:45:31.660]   Don't put anything in email,
[01:45:31.660 --> 01:45:33.300]   or I guess social media these days,
[01:45:33.300 --> 01:45:35.780]   that you wouldn't want to read
[01:45:35.780 --> 01:45:37.900]   on the front page of the New York Times.
[01:45:37.900 --> 01:45:41.620]   And that works, but sometimes I even get carried,
[01:45:41.620 --> 01:45:45.540]   I mean, I have not as many followers as you,
[01:45:45.540 --> 01:45:47.060]   but a lot of followers,
[01:45:47.060 --> 01:45:49.100]   and sometimes even I get carried away.
[01:45:49.100 --> 01:45:51.300]   - You have to be emotional and stuff and say something.
[01:45:51.300 --> 01:45:56.300]   - Yeah, I mean, just the cortisol response on Twitter.
[01:45:56.300 --> 01:46:01.580]   Twitter is basically designed to elicit those responses.
[01:46:01.580 --> 01:46:04.740]   I mean, every day I turn on my computer,
[01:46:04.740 --> 01:46:07.740]   I look at my phone, I look at what's trending on Twitter,
[01:46:07.740 --> 01:46:10.060]   and it's like, what are the topics
[01:46:10.060 --> 01:46:13.700]   that are gonna make people the most angry today?
[01:46:13.700 --> 01:46:16.140]   (both laughing)
[01:46:16.140 --> 01:46:19.180]   And it's easy to get carried away,
[01:46:19.180 --> 01:46:22.340]   but it's also just, that sucks too,
[01:46:22.340 --> 01:46:25.300]   that you have to be constantly censoring yourself.
[01:46:25.300 --> 01:46:26.620]   And maybe it's for the better,
[01:46:26.620 --> 01:46:29.500]   maybe you can't be a secret asshole,
[01:46:29.500 --> 01:46:31.380]   and we can put that in the good bucket.
[01:46:31.380 --> 01:46:35.820]   But at the same time, there is a danger
[01:46:35.820 --> 01:46:40.820]   to that other voice, to creativity, to being weird.
[01:46:40.820 --> 01:46:45.580]   There's a danger to that little whispered voice
[01:46:45.580 --> 01:46:49.100]   that's like, well, how would people read that?
[01:46:49.100 --> 01:46:51.180]   How could that be manipulated?
[01:46:51.180 --> 01:46:53.540]   How could that be used against you?
[01:46:53.540 --> 01:46:58.540]   And that stifles creativity and innovation and free thought.
[01:46:58.540 --> 01:47:05.300]   And that is on a very micro level.
[01:47:05.300 --> 01:47:08.940]   And that's something I think about a lot.
[01:47:08.940 --> 01:47:10.300]   And that's actually something
[01:47:10.300 --> 01:47:13.220]   that Tim Cook has talked about a lot,
[01:47:13.220 --> 01:47:17.660]   and why he has said he goes full force on privacy
[01:47:17.660 --> 01:47:21.020]   is it's just that little voice
[01:47:21.020 --> 01:47:24.860]   that is at some level censoring you.
[01:47:24.860 --> 01:47:28.300]   And what is sort of the long-term impact
[01:47:28.300 --> 01:47:31.020]   of that little voice over time?
[01:47:31.020 --> 01:47:32.300]   - I think there's a ways,
[01:47:32.300 --> 01:47:36.460]   I think that self-censorship is an attack factor
[01:47:36.460 --> 01:47:37.740]   that there are solutions to.
[01:47:37.740 --> 01:47:40.220]   The way I'm really inspired by Elon Musk,
[01:47:40.220 --> 01:47:43.740]   the solution to that is just be privately
[01:47:43.740 --> 01:47:46.780]   and publicly the same person and be ridiculous.
[01:47:46.780 --> 01:47:49.780]   Embrace the full weirdness and show it more and more.
[01:47:49.780 --> 01:47:54.020]   So there's memes that has ridiculous humor.
[01:47:54.020 --> 01:47:59.300]   And if there is something you really wanna hide,
[01:47:59.300 --> 01:48:02.500]   deeply consider if you wanna be that.
[01:48:02.500 --> 01:48:05.300]   Like, why are you hiding it?
[01:48:05.300 --> 01:48:07.420]   What exactly are you afraid of?
[01:48:07.420 --> 01:48:10.700]   Because I think my hopeful vision for the internet
[01:48:10.700 --> 01:48:13.340]   is the internet loves authenticity.
[01:48:13.340 --> 01:48:15.180]   They wanna see you weird.
[01:48:15.180 --> 01:48:18.300]   So be that and live that fully.
[01:48:18.300 --> 01:48:20.260]   Because I think that gray area
[01:48:20.260 --> 01:48:22.260]   where you're kind of censoring yourself,
[01:48:22.260 --> 01:48:25.260]   that's where the destruction is.
[01:48:25.260 --> 01:48:28.780]   You have to go all the way, step over, be weird.
[01:48:28.780 --> 01:48:31.060]   And then it feels, it can be painful
[01:48:31.060 --> 01:48:33.740]   'cause people can attack you and so on, but just ride it.
[01:48:33.740 --> 01:48:35.980]   I mean, that's just a skill
[01:48:35.980 --> 01:48:38.060]   on the social psychological level
[01:48:38.060 --> 01:48:42.100]   that ends up being an approach to security,
[01:48:42.100 --> 01:48:45.060]   which is like remove the attack vector
[01:48:45.060 --> 01:48:46.980]   of having private information
[01:48:46.980 --> 01:48:50.080]   by being your full weird self publicly.
[01:48:50.080 --> 01:48:56.420]   What advice would you give to young folks today
[01:48:56.420 --> 01:49:00.660]   operating in this complicated space
[01:49:00.660 --> 01:49:02.500]   about how to have a successful life,
[01:49:02.500 --> 01:49:03.940]   a life they can be proud of,
[01:49:03.980 --> 01:49:05.580]   a career they can be proud of?
[01:49:05.580 --> 01:49:09.460]   Maybe somebody in high school and college
[01:49:09.460 --> 01:49:11.580]   thinking about what they're going to do.
[01:49:11.580 --> 01:49:12.580]   - Be a hacker.
[01:49:12.580 --> 01:49:16.740]   If you have any interest, become a hacker
[01:49:16.740 --> 01:49:19.020]   and apply yourself to defense.
[01:49:19.020 --> 01:49:21.780]   Every time, like we do have
[01:49:21.780 --> 01:49:24.580]   these amazing scholarship programs, for instance,
[01:49:24.580 --> 01:49:26.580]   where they find you early,
[01:49:26.580 --> 01:49:30.020]   they'll pay your college as long as you commit
[01:49:30.020 --> 01:49:31.940]   to some kind of federal commitment
[01:49:31.940 --> 01:49:35.300]   to sort of help federal agencies with cybersecurity.
[01:49:35.300 --> 01:49:37.860]   And where does everyone wanna go every year
[01:49:37.860 --> 01:49:39.060]   from the scholarship program?
[01:49:39.060 --> 01:49:42.220]   They wanna go work at the NSA or Cyber Command.
[01:49:42.220 --> 01:49:44.180]   They wanna go work on offense.
[01:49:44.180 --> 01:49:46.180]   They wanna go do the sexy stuff.
[01:49:46.180 --> 01:49:49.900]   It's really hard to get people to work on defense.
[01:49:49.900 --> 01:49:52.580]   It's just, it's always been more fun to be a pirate
[01:49:52.580 --> 01:49:54.540]   than be in the Coast Guard.
[01:49:54.540 --> 01:49:59.100]   And so we have a huge deficit
[01:49:59.100 --> 01:50:01.300]   when it comes to filling those roles.
[01:50:01.300 --> 01:50:06.300]   There's 3.5 million unfilled cybersecurity positions
[01:50:06.300 --> 01:50:07.940]   around the world.
[01:50:07.940 --> 01:50:09.660]   I mean, talk about job security.
[01:50:09.660 --> 01:50:12.700]   Like be a hacker and work on cybersecurity.
[01:50:12.700 --> 01:50:14.260]   You will always have a job.
[01:50:14.260 --> 01:50:18.540]   And we're actually at a huge deficit
[01:50:18.540 --> 01:50:21.260]   and disadvantage as a free market economy
[01:50:21.260 --> 01:50:26.740]   because we can't match cybersecurity salaries
[01:50:26.740 --> 01:50:30.020]   at Palantir or Facebook or Google or Microsoft.
[01:50:30.020 --> 01:50:32.460]   And so it's really hard for the United States
[01:50:32.460 --> 01:50:33.820]   to fill those roles.
[01:50:33.820 --> 01:50:38.420]   And other countries have had this workaround
[01:50:38.420 --> 01:50:41.820]   where they basically have forced conscription on some level.
[01:50:41.820 --> 01:50:44.380]   China tells people like,
[01:50:44.380 --> 01:50:46.860]   you do whatever you're gonna do during the day,
[01:50:46.860 --> 01:50:48.860]   work at Alibaba.
[01:50:48.860 --> 01:50:51.340]   If you need to do some ransomware, okay.
[01:50:51.340 --> 01:50:53.980]   But the minute we tap you on the shoulder
[01:50:53.980 --> 01:50:57.180]   and ask you to come do this sensitive operation for us,
[01:50:57.180 --> 01:50:58.420]   the answer is yes.
[01:50:59.380 --> 01:51:00.820]   Same with Russia.
[01:51:00.820 --> 01:51:03.660]   You know, a couple of years ago when Yahoo was hacked
[01:51:03.660 --> 01:51:05.700]   and they laid it all out in an indictment,
[01:51:05.700 --> 01:51:07.660]   it came down to two cyber criminals
[01:51:07.660 --> 01:51:09.580]   and two guys from the FSB.
[01:51:09.580 --> 01:51:12.180]   Cyber criminals were allowed to have their fun,
[01:51:12.180 --> 01:51:14.860]   but the minute they came across the username and password
[01:51:14.860 --> 01:51:16.740]   for someone's personal Yahoo account
[01:51:16.740 --> 01:51:19.460]   that worked at the White House or the State Department
[01:51:19.460 --> 01:51:23.660]   or military, they were expected to pass that over to the FSB.
[01:51:23.660 --> 01:51:24.980]   So we don't do that here.
[01:51:24.980 --> 01:51:27.420]   And it's even worse on defense.
[01:51:27.420 --> 01:51:29.900]   You really can't fill these positions.
[01:51:29.900 --> 01:51:33.020]   So, you know, if you are a hacker,
[01:51:33.020 --> 01:51:36.620]   if you're interested in code, if you're a tinker,
[01:51:36.620 --> 01:51:38.460]   you know, learn how to hack.
[01:51:38.460 --> 01:51:42.700]   There are all sorts of amazing hacking competitions
[01:51:42.700 --> 01:51:47.500]   you can do through the SANS org, for example, S-A-N-S.
[01:51:47.500 --> 01:51:50.900]   And then use those skills for good.
[01:51:50.900 --> 01:51:53.500]   You know, neuter the bugs in that code
[01:51:53.500 --> 01:51:56.300]   that get used by autocratic regimes
[01:51:56.300 --> 01:51:59.300]   to make people's life, you know, a living prison.
[01:51:59.300 --> 01:52:01.940]   You know, plug those holes, you know,
[01:52:01.940 --> 01:52:03.700]   defend industrial systems,
[01:52:03.700 --> 01:52:06.020]   defend our water treatment facilities
[01:52:06.020 --> 01:52:07.820]   from hacks where people are trying to come in
[01:52:07.820 --> 01:52:09.500]   and poison the water.
[01:52:09.500 --> 01:52:12.660]   You know, that I think is just an amazing,
[01:52:12.660 --> 01:52:16.260]   it's an amazing job on so many levels.
[01:52:16.260 --> 01:52:19.700]   It's intellectually stimulating.
[01:52:19.700 --> 01:52:22.900]   You can tell yourself you're serving your country.
[01:52:22.900 --> 01:52:24.700]   You can tell yourself you're saving lives
[01:52:24.700 --> 01:52:26.140]   and keeping people safe.
[01:52:26.140 --> 01:52:28.460]   And you'll always have amazing job security.
[01:52:28.460 --> 01:52:30.980]   And if you need to go get that job that pays you,
[01:52:30.980 --> 01:52:33.540]   you know, two million bucks a year, you can do that too.
[01:52:33.540 --> 01:52:34.860]   - And you can have a public profile,
[01:52:34.860 --> 01:52:38.780]   more so of a public profile, you can be a public rockstar.
[01:52:38.780 --> 01:52:42.780]   I mean, it's the same thing as sort of the military.
[01:52:42.780 --> 01:52:44.020]   There's a lot of,
[01:52:44.020 --> 01:52:49.980]   there's a lot of well-known sort of people
[01:52:49.980 --> 01:52:51.700]   commenting on the fact that veterans
[01:52:51.700 --> 01:52:54.060]   are not treated as well as they should be.
[01:52:54.060 --> 01:52:56.140]   But it's still the fact that soldiers
[01:52:56.140 --> 01:53:00.380]   are deeply respected for defending the country,
[01:53:00.380 --> 01:53:02.820]   the freedoms, the ideals that we stand for.
[01:53:02.820 --> 01:53:05.820]   And in the same way, I mean, in some ways,
[01:53:05.820 --> 01:53:09.020]   the cybersecurity defense are the soldiers of the future.
[01:53:09.020 --> 01:53:10.740]   - Yeah, and you know what's interesting?
[01:53:10.740 --> 01:53:14.140]   I mean, in cybersecurity, the difference is
[01:53:14.140 --> 01:53:17.180]   oftentimes you see the more interesting threats
[01:53:17.180 --> 01:53:18.660]   in the private sector
[01:53:18.660 --> 01:53:20.380]   because that's where the attacks come.
[01:53:20.740 --> 01:53:24.340]   When cyber criminals and nation state adversaries
[01:53:24.340 --> 01:53:25.580]   come for the United States,
[01:53:25.580 --> 01:53:29.060]   they don't go directly for Cyber Command or the NSA.
[01:53:29.060 --> 01:53:32.940]   No, they go for banks, they go for Google,
[01:53:32.940 --> 01:53:36.660]   they go for Microsoft, they go for critical infrastructure.
[01:53:36.660 --> 01:53:39.580]   And so those companies, those private sector companies
[01:53:39.580 --> 01:53:41.820]   get to see some of the most advanced,
[01:53:41.820 --> 01:53:45.700]   sophisticated attacks out there.
[01:53:45.700 --> 01:53:48.620]   And if you're working at FireEye
[01:53:48.620 --> 01:53:51.580]   and you're calling out the SolarWinds attack, for instance,
[01:53:51.580 --> 01:53:56.140]   I mean, you just saved God knows how many systems
[01:53:56.140 --> 01:53:59.980]   from that compromise turning into something
[01:53:59.980 --> 01:54:02.560]   that more closely resembles sabotage.
[01:54:02.560 --> 01:54:08.700]   So go be a hacker or go be a journalist.
[01:54:08.700 --> 01:54:10.980]   (both laughing)
[01:54:10.980 --> 01:54:13.100]   - So you wrote the book,
[01:54:13.100 --> 01:54:15.900]   "This is How They Tell Me the World Ends,"
[01:54:15.900 --> 01:54:17.540]   as we've been talking about,
[01:54:17.540 --> 01:54:20.260]   of course, referring to cyber war, cybersecurity.
[01:54:20.260 --> 01:54:25.380]   What gives you hope about the future of our world?
[01:54:25.380 --> 01:54:28.040]   If it doesn't end, how will it not end?
[01:54:28.040 --> 01:54:32.820]   - That's a good question.
[01:54:32.820 --> 01:54:34.600]   I mean, I have to have hope, right?
[01:54:34.600 --> 01:54:37.420]   Because I have a kid and I have another on the way.
[01:54:37.420 --> 01:54:41.040]   And if I didn't have hope, I wouldn't be having kids.
[01:54:41.040 --> 01:54:45.540]   But it's a scary time to be having kids.
[01:54:46.580 --> 01:54:50.620]   And it's like pandemic, climate change,
[01:54:50.620 --> 01:54:54.820]   disinformation, increasingly advanced,
[01:54:54.820 --> 01:54:56.860]   perhaps deadly cyber attacks.
[01:54:56.860 --> 01:55:01.500]   What gives me hope is that I share your worldview
[01:55:01.500 --> 01:55:04.060]   that I think people are fundamentally good.
[01:55:04.060 --> 01:55:07.620]   And sometimes, and this is why the metaverse
[01:55:07.620 --> 01:55:10.820]   scares me to death, but when I'm reminded of that
[01:55:10.820 --> 01:55:12.200]   is not online.
[01:55:12.200 --> 01:55:15.500]   Like online, I get the opposite.
[01:55:15.500 --> 01:55:17.420]   You start to lose hope in humanity
[01:55:17.420 --> 01:55:19.360]   when you're on Twitter half your day.
[01:55:19.360 --> 01:55:22.620]   It's like when I go to the grocery store
[01:55:22.620 --> 01:55:26.440]   or I go on a hike or like someone smiles at me,
[01:55:26.440 --> 01:55:30.140]   or someone just says something nice,
[01:55:30.140 --> 01:55:33.300]   people are fundamentally good.
[01:55:33.300 --> 01:55:37.140]   We just don't hear from those people enough.
[01:55:37.140 --> 01:55:42.140]   And my hope is, I just think our current political climate,
[01:55:42.140 --> 01:55:44.900]   we've hit rock bottom.
[01:55:44.900 --> 01:55:46.680]   This is as bad as it gets.
[01:55:46.680 --> 01:55:47.860]   We can't do anything.
[01:55:47.860 --> 01:55:48.860]   - Don't jinx it.
[01:55:48.860 --> 01:55:49.780]   (laughs)
[01:55:49.780 --> 01:55:52.460]   - But I think it's a generational thing.
[01:55:52.460 --> 01:55:56.060]   I think baby boomers, it's time to move along.
[01:55:56.060 --> 01:56:01.140]   I think it's time for a new generation to come in.
[01:56:01.140 --> 01:56:06.040]   And I actually have a lot of hope when I look at,
[01:56:06.040 --> 01:56:08.820]   I'm sort of like this, I guess they call me
[01:56:08.820 --> 01:56:12.120]   a geriatric millennial or a young Gen X.
[01:56:12.120 --> 01:56:14.500]   But we have this unique responsibility
[01:56:14.500 --> 01:56:17.620]   because I grew up without the internet
[01:56:17.620 --> 01:56:21.020]   and without social media, but I'm native to it.
[01:56:21.020 --> 01:56:25.300]   So I know the good and I know the bad.
[01:56:25.300 --> 01:56:28.660]   And that's true on so many different things.
[01:56:28.660 --> 01:56:32.100]   I grew up without climate change anxiety
[01:56:32.100 --> 01:56:34.900]   and now I'm feeling it and I know it's not a given.
[01:56:34.900 --> 01:56:38.980]   We don't have to just resign ourselves to climate change.
[01:56:38.980 --> 01:56:41.240]   Same with disinformation.
[01:56:41.240 --> 01:56:44.140]   And I think a lot of the problems we face today
[01:56:44.140 --> 01:56:47.660]   have just exposed the sort of inertia
[01:56:47.660 --> 01:56:49.980]   that there's been on so many of these issues.
[01:56:49.980 --> 01:56:52.940]   And I really think it's a generational shift
[01:56:52.940 --> 01:56:54.540]   that has to happen.
[01:56:54.540 --> 01:56:57.660]   And I think this next generation is gonna come in
[01:56:57.660 --> 01:56:59.540]   and say like, we're not doing business
[01:56:59.540 --> 01:57:01.380]   like you guys did it anymore.
[01:57:01.380 --> 01:57:03.860]   We're not just gonna like rape and pillage the earth
[01:57:03.860 --> 01:57:06.540]   and try and turn everyone against each other
[01:57:06.540 --> 01:57:09.620]   and play dirty tricks and let lobbyists dictate
[01:57:09.620 --> 01:57:13.660]   what we do or don't do as a country anymore.
[01:57:14.100 --> 01:57:16.540]   And that's really where I see the hope.
[01:57:16.540 --> 01:57:19.220]   - It feels like there's a lot of low hanging fruit
[01:57:19.220 --> 01:57:23.780]   for young minds to step up and create solutions and lead.
[01:57:23.780 --> 01:57:28.780]   So whenever like politicians or leaders that are older,
[01:57:28.780 --> 01:57:32.980]   like you said, are acting shitty,
[01:57:32.980 --> 01:57:34.180]   I see that as a positive.
[01:57:34.180 --> 01:57:38.140]   They're inspiring a large number of young people
[01:57:38.140 --> 01:57:39.700]   to replace them.
[01:57:39.700 --> 01:57:41.900]   And so I think you're right.
[01:57:41.900 --> 01:57:44.260]   There's going to be, it's almost like you need people
[01:57:44.260 --> 01:57:47.860]   to act shitty to remind them, oh, wow, we need good leaders.
[01:57:47.860 --> 01:57:51.340]   We need great creators and builders and entrepreneurs
[01:57:51.340 --> 01:57:54.140]   and scientists and engineers and journalists.
[01:57:54.140 --> 01:57:56.540]   You know, all the discussions about how the journalism
[01:57:56.540 --> 01:57:58.660]   is quote unquote broken and so on.
[01:57:58.660 --> 01:58:02.100]   That's just an inspiration for new institutions to rise up
[01:58:02.100 --> 01:58:03.760]   that do journalism better,
[01:58:03.760 --> 01:58:06.300]   new journalists to step up and do journalism better.
[01:58:06.300 --> 01:58:08.660]   So I, and I've been constantly,
[01:58:08.660 --> 01:58:11.780]   when I talk to young people, I'm constantly impressed
[01:58:11.780 --> 01:58:16.420]   by the ones that dream to build solutions.
[01:58:16.420 --> 01:58:21.300]   And so that's ultimately why I put the hope.
[01:58:21.300 --> 01:58:23.140]   But the world is a messy place,
[01:58:23.140 --> 01:58:24.940]   like we've been talking about.
[01:58:24.940 --> 01:58:26.180]   It's a scary place.
[01:58:26.180 --> 01:58:29.700]   - Yeah, and I think you hit something,
[01:58:29.700 --> 01:58:33.100]   hit on something earlier, which is authenticity.
[01:58:33.100 --> 01:58:38.100]   Like no one is going to rise above that is plastic anymore.
[01:58:40.060 --> 01:58:43.100]   You know, people are craving authenticity.
[01:58:43.100 --> 01:58:46.500]   You know, the benefit of the internet is it's really hard
[01:58:46.500 --> 01:58:49.940]   to hide who you are on every single platform.
[01:58:49.940 --> 01:58:51.820]   You know, on some level it's going to come out
[01:58:51.820 --> 01:58:53.460]   who you really are.
[01:58:53.460 --> 01:58:57.380]   And so you hope that, you know,
[01:58:57.380 --> 01:59:01.380]   by the time my kids are grown, like no one's going to care
[01:59:01.380 --> 01:59:04.060]   if they made one mistake online,
[01:59:04.060 --> 01:59:06.980]   so long as they're authentic, you know?
[01:59:06.980 --> 01:59:09.580]   And I used to worry about this.
[01:59:09.580 --> 01:59:13.260]   My nephew was born the day I graduated from college.
[01:59:13.260 --> 01:59:17.740]   And I just always, you know, he's like born into Facebook.
[01:59:17.740 --> 01:59:20.980]   And I just think like, how is a kid like that
[01:59:20.980 --> 01:59:24.020]   ever going to be president of the United States of America?
[01:59:24.020 --> 01:59:27.740]   Because if Facebook had been around when I was in college,
[01:59:27.740 --> 01:59:30.980]   you know, like Jesus, you know,
[01:59:30.980 --> 01:59:34.220]   how are those kids going to ever be president?
[01:59:34.220 --> 01:59:37.460]   There's going to be some photo of them at some point
[01:59:37.460 --> 01:59:39.340]   making some mistake,
[01:59:39.340 --> 01:59:41.780]   and that's going to be all over for them.
[01:59:41.780 --> 01:59:43.100]   And now I take that back.
[01:59:43.100 --> 01:59:46.700]   Now it's like, no, everyone's going to make mistakes.
[01:59:46.700 --> 01:59:49.380]   There's going to be a picture for everyone.
[01:59:49.380 --> 01:59:53.020]   And we're all going to have to come and grow up
[01:59:53.020 --> 01:59:56.500]   to the view that as humans, we're going to make huge mistakes
[01:59:56.500 --> 01:59:58.140]   and hopefully they're not so big
[01:59:58.140 --> 02:00:00.660]   that they're going to ruin the rest of your life.
[02:00:00.660 --> 02:00:02.980]   But we're going to have to come around to this view
[02:00:02.980 --> 02:00:04.500]   that we're all human,
[02:00:04.500 --> 02:00:07.300]   and we're going to have to be a little bit more forgiving
[02:00:07.300 --> 02:00:10.300]   and a little bit more tolerant when people mess up.
[02:00:10.300 --> 02:00:12.100]   And we're going to have to be a little bit more humble
[02:00:12.100 --> 02:00:15.620]   when we do and like keep moving forward.
[02:00:15.620 --> 02:00:17.620]   Otherwise you can't like cancel everyone.
[02:00:17.620 --> 02:00:21.700]   - Nicole, this was an incredible, hopeful conversation.
[02:00:21.700 --> 02:00:26.700]   Also one that reveals that in the shadows,
[02:00:26.700 --> 02:00:30.220]   there's a lot of challenges to be solved.
[02:00:30.220 --> 02:00:32.340]   So I really appreciate that you took on
[02:00:32.340 --> 02:00:34.300]   this really difficult subject with your book.
[02:00:34.300 --> 02:00:35.860]   That's journalism at its best.
[02:00:35.860 --> 02:00:37.860]   So I'm really grateful that you did it,
[02:00:37.860 --> 02:00:40.180]   that you took the risk, that you took that on.
[02:00:40.180 --> 02:00:42.580]   And that you plugged the cable box back in.
[02:00:42.580 --> 02:00:43.820]   That means you have hope.
[02:00:43.820 --> 02:00:46.540]   And thank you so much for spending
[02:00:46.540 --> 02:00:47.900]   your valuable time with me today.
[02:00:47.900 --> 02:00:48.740]   - Thank you.
[02:00:48.740 --> 02:00:49.860]   Thanks for having me.
[02:00:49.860 --> 02:00:52.180]   - Thanks for listening to this conversation
[02:00:52.180 --> 02:00:53.660]   with Nicole Perlroth.
[02:00:53.660 --> 02:00:54.900]   To support this podcast,
[02:00:54.900 --> 02:00:57.660]   please check out our sponsors in the description.
[02:00:57.660 --> 02:01:00.020]   And now let me leave you with some words
[02:01:00.020 --> 02:01:01.780]   from Nicole herself.
[02:01:01.780 --> 02:01:05.540]   "Here we are, entrusting our entire digital lives
[02:01:05.540 --> 02:01:08.940]   "passwords, texts, love letters, banking records,
[02:01:08.940 --> 02:01:10.740]   "health records, credit cards, sources,
[02:01:10.740 --> 02:01:13.900]   "and deepest thoughts to this mystery box
[02:01:13.900 --> 02:01:17.660]   "whose inner circuitry most of us would never vet.
[02:01:17.660 --> 02:01:19.580]   "Run by code, written in a language
[02:01:19.580 --> 02:01:22.460]   "most of us will never fully understand."
[02:01:22.460 --> 02:01:26.420]   Thank you for listening and hope to see you next time.
[02:01:26.420 --> 02:01:29.020]   (upbeat music)
[02:01:29.020 --> 02:01:31.620]   (upbeat music)
[02:01:31.620 --> 02:01:41.620]   [BLANK_AUDIO]

