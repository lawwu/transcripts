
[00:00:00.000 --> 00:00:05.000]   [MUSIC PLAYING]
[00:00:05.000 --> 00:00:08.880]   One of the best ways to understand large language model
[00:00:08.880 --> 00:00:12.920]   APIs is to interact with them via code
[00:00:12.920 --> 00:00:14.920]   and experiment with them.
[00:00:14.920 --> 00:00:17.440]   And this is what we're going to do in this video.
[00:00:17.440 --> 00:00:20.920]   We will use Jupyter Notebooks via VS Code.
[00:00:20.920 --> 00:00:24.360]   And the reason we use Jupyter Notebooks in this lesson
[00:00:24.360 --> 00:00:27.560]   is because this is an interactive environment which
[00:00:27.560 --> 00:00:30.000]   allows us to experiment with these models.
[00:00:30.000 --> 00:00:32.400]   We are not building a production-grade application
[00:00:32.400 --> 00:00:33.360]   yet.
[00:00:33.360 --> 00:00:34.360]   We're just exploring.
[00:00:34.360 --> 00:00:35.720]   We're experimenting.
[00:00:35.720 --> 00:00:40.560]   And seeing the output of these LLMs in an interactive way
[00:00:40.560 --> 00:00:44.120]   will help us better understand these models.
[00:00:44.120 --> 00:00:47.320]   In this video, we will use OpenAI API.
[00:00:47.320 --> 00:00:51.840]   I encourage you to also experiment with other APIs.
[00:00:51.840 --> 00:00:54.920]   In order to use OpenAI API, we will
[00:00:54.920 --> 00:00:56.880]   need to install some libraries.
[00:00:56.880 --> 00:01:02.680]   We'll also install 1db for tracking our experiments.
[00:01:02.680 --> 00:01:06.760]   Then we need to import all of the libraries
[00:01:06.760 --> 00:01:08.640]   that we'll need in these experiments.
[00:01:08.640 --> 00:01:12.160]   And we need to make sure that we have OpenAI API key
[00:01:12.160 --> 00:01:14.360]   environment variable set.
[00:01:14.360 --> 00:01:15.320]   So let's double check.
[00:01:15.320 --> 00:01:17.640]   I already set it on my system.
[00:01:17.640 --> 00:01:22.080]   If you don't have this key, you will see an instruction
[00:01:22.080 --> 00:01:24.840]   on how to get that key below this video.
[00:01:25.600 --> 00:01:28.600]   Because we are tracking our work,
[00:01:28.600 --> 00:01:31.800]   because we want to get into the habit of recording
[00:01:31.800 --> 00:01:34.280]   what we're doing with LLMs, we will also
[00:01:34.280 --> 00:01:36.560]   enable logging to Weights & Biases.
[00:01:36.560 --> 00:01:40.160]   And we have the convenient autolog function for OpenAI,
[00:01:40.160 --> 00:01:44.560]   which will log the results, the calls to the API,
[00:01:44.560 --> 00:01:48.760]   and the results in a Weights & Biases table.
[00:01:48.760 --> 00:01:50.520]   Now, the first thing that we would
[00:01:50.520 --> 00:01:52.960]   like to experiment with is the ability
[00:01:53.240 --> 00:01:56.400]   we'd like to experiment with is tokenization.
[00:01:56.400 --> 00:01:59.280]   OpenAI has this tick token library,
[00:01:59.280 --> 00:02:01.560]   which allows us to tokenize text,
[00:02:01.560 --> 00:02:04.240]   to decode text that has been tokenized.
[00:02:04.240 --> 00:02:09.000]   And it's good to get an intuition on how sentences
[00:02:09.000 --> 00:02:11.560]   or words are split into tokens.
[00:02:11.560 --> 00:02:15.760]   So let's use tick token to decode--
[00:02:15.760 --> 00:02:18.280]   to encode and decode a sentence.
[00:02:18.280 --> 00:02:20.200]   Weights & Biases is awesome.
[00:02:20.200 --> 00:02:23.240]   And see how this is going to be split in tokens.
[00:02:23.240 --> 00:02:25.000]   If you think about splitting this,
[00:02:25.000 --> 00:02:29.520]   probably the most natural way is to just use spaces
[00:02:29.520 --> 00:02:32.160]   as a way of indicating the tokens.
[00:02:32.160 --> 00:02:34.600]   However, if you think about all of the text that
[00:02:34.600 --> 00:02:38.920]   is on the internet and on GitHub and in Wikipedia,
[00:02:38.920 --> 00:02:42.200]   and just splitting it on spaces, that
[00:02:42.200 --> 00:02:45.360]   may potentially result in millions of tokens.
[00:02:45.360 --> 00:02:48.320]   And we know that we need to have a set vocabulary size,
[00:02:48.320 --> 00:02:51.400]   because this is how the LLM is predicting
[00:02:51.400 --> 00:02:54.240]   the output probabilities.
[00:02:54.240 --> 00:02:57.680]   So we need to contain the size of our vocabulary.
[00:02:57.680 --> 00:03:00.880]   And for that reason, some of the words that we have
[00:03:00.880 --> 00:03:04.200]   may be split into subwords, into units.
[00:03:04.200 --> 00:03:06.840]   And this will create tokens.
[00:03:06.840 --> 00:03:09.560]   So let's run this and see what happens.
[00:03:09.560 --> 00:03:13.200]   By the way, different models may have different tokenizers.
[00:03:13.200 --> 00:03:16.680]   So in this case, we're picking a tokenizer for text DaVinci
[00:03:16.680 --> 00:03:19.120]   003 model.
[00:03:19.120 --> 00:03:21.520]   But in case you're using a different model,
[00:03:21.520 --> 00:03:24.520]   you may need to pick a different tokenizer.
[00:03:24.520 --> 00:03:28.560]   So we can see now that text Weights and Biases is Awesome
[00:03:28.560 --> 00:03:34.040]   is a combination of tokens with the following numbers.
[00:03:34.040 --> 00:03:37.200]   We can see this as a list of numbers.
[00:03:37.200 --> 00:03:41.040]   But that's still difficult to understand what the tokens are.
[00:03:41.040 --> 00:03:43.760]   So we can decode the tokens one by one
[00:03:43.760 --> 00:03:47.440]   and see what each of the values corresponds to.
[00:03:47.440 --> 00:03:51.000]   And we can see the word We is a separate token.
[00:03:51.000 --> 00:03:54.600]   And then Heights-- so Weights actually
[00:03:54.600 --> 00:03:58.520]   was split into two tokens with the following numbers.
[00:03:58.520 --> 00:04:01.680]   Then the Ampersand is its own token.
[00:04:01.680 --> 00:04:05.600]   Then Biases is, again, combined of two tokens.
[00:04:05.600 --> 00:04:07.880]   Is is a single token.
[00:04:07.880 --> 00:04:10.520]   Awesome, again, a popular word, is a token.
[00:04:10.520 --> 00:04:14.960]   And the exclamation mark is also a token with a value 0.
[00:04:14.960 --> 00:04:18.640]   It's interesting that the whole vocabulary of tokens
[00:04:18.640 --> 00:04:22.200]   starts with the exclamation mark.
[00:04:22.200 --> 00:04:24.760]   And you can also see that some of these tokens
[00:04:24.760 --> 00:04:27.280]   contain spacing.
[00:04:27.280 --> 00:04:30.160]   And that's because once we decode these tokens,
[00:04:30.160 --> 00:04:32.480]   once we want to put them together,
[00:04:32.480 --> 00:04:35.240]   we want to make sure that we also preserve spaces.
[00:04:35.240 --> 00:04:39.080]   So spaces are part of the tokens and maybe its own token
[00:04:39.080 --> 00:04:39.600]   as well.
[00:04:39.600 --> 00:04:42.960]   [MUSIC PLAYING]
[00:04:42.960 --> 00:04:46.320]   [MUSIC ENDS]
[00:04:46.320 --> 00:04:48.900]   (upbeat music)
[00:04:48.900 --> 00:04:50.900]   You

