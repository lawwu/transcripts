
[00:00:00.000 --> 00:00:07.600]   I've been telling everybody who will listen that I feel like we're in the middle of a
[00:00:07.600 --> 00:00:13.100]   significant spike in technological capability right now.
[00:00:13.100 --> 00:00:18.760]   And so if you're not doing that, you're missing out on being at the forefront of something
[00:00:18.760 --> 00:00:24.640]   that's substantially changing what humans are able to do.
[00:00:24.640 --> 00:00:29.020]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:29.020 --> 00:00:31.360]   and I'm your host, Lukas Biewald.
[00:00:31.360 --> 00:00:37.200]   Jeremy Howard is the founding researcher at Fast.ai, which is a research institute dedicated
[00:00:37.200 --> 00:00:39.620]   to making deep learning more accessible.
[00:00:39.620 --> 00:00:44.860]   They make an incredible Python repository that people use for lots and lots of deep
[00:00:44.860 --> 00:00:49.080]   learning projects, and they make an incredible set of classes that many people I know have
[00:00:49.080 --> 00:00:52.260]   taken and is almost universally loved.
[00:00:52.260 --> 00:00:57.640]   He was also the CEO and founder of Analytic, the president of Kaggle, and has done a whole
[00:00:57.640 --> 00:01:01.020]   bunch of diverse, amazing things in his career.
[00:01:01.020 --> 00:01:04.560]   It's always super inspiring to talk to Jeremy, and this interview is no different.
[00:01:04.560 --> 00:01:07.080]   I really hope you enjoy it.
[00:01:07.080 --> 00:01:12.260]   You are the first person to be on this podcast two times, and I think you are the most popular
[00:01:12.260 --> 00:01:15.700]   guest that we've had based on our YouTube metrics.
[00:01:15.700 --> 00:01:18.700]   So it's great to have you.
[00:01:18.700 --> 00:01:25.440]   And I guess I wanted to start with actually the most memorable part of our interview for
[00:01:25.440 --> 00:01:33.100]   me personally was the amount of time that you set aside every day to work on just learning
[00:01:33.100 --> 00:01:36.900]   and just said like undirected sort of learning new things, which I really thought was an
[00:01:36.900 --> 00:01:40.300]   amazing thing that I always aspire to do more of.
[00:01:40.300 --> 00:01:45.080]   But I was curious, lately, what have you been learning?
[00:01:45.080 --> 00:01:51.340]   I'm spending all my spare time at the moment on generative modeling around the kind of
[00:01:51.340 --> 00:01:56.580]   stable diffusion or diffusion modeling space.
[00:01:56.580 --> 00:01:58.060]   Hence the new course, I guess.
[00:01:58.060 --> 00:02:00.380]   Is that part of the learning process?
[00:02:00.380 --> 00:02:02.220]   Yeah, it's a bit of a chicken and the egg thing.
[00:02:02.220 --> 00:02:05.540]   It's partly the new courses because of the learning and partly the learnings because
[00:02:05.540 --> 00:02:09.180]   of the new course.
[00:02:09.180 --> 00:02:15.820]   I've been telling everybody who will listen that I feel like we're in the middle of a
[00:02:15.820 --> 00:02:21.340]   significant spike in technological capability right now.
[00:02:21.340 --> 00:02:27.420]   And so if you're not doing that, you're missing out on being at the forefront of something
[00:02:27.420 --> 00:02:33.940]   that's substantially changing what humans are able to do.
[00:02:33.940 --> 00:02:41.820]   And so when there's such a technological shift, it creates all kinds of opportunities for
[00:02:41.820 --> 00:02:51.100]   startups and for scientific progress and also opportunities to screw up society, which hopefully
[00:02:51.100 --> 00:02:53.580]   you can figure out how to avoid and stuff like that.
[00:02:53.580 --> 00:02:59.660]   So I'm very keen to do what I can to be on the forefront of that and to help others who
[00:02:59.660 --> 00:03:02.820]   are interested in doing the same thing.
[00:03:02.820 --> 00:03:07.820]   And when you say spike, do you mean diffusion models specifically or do you mean machine
[00:03:07.820 --> 00:03:08.820]   learning more broadly?
[00:03:08.820 --> 00:03:11.780]   I mean diffusion models specifically.
[00:03:11.780 --> 00:03:12.780]   Interesting.
[00:03:12.780 --> 00:03:13.780]   Interesting.
[00:03:13.780 --> 00:03:14.780]   Yeah.
[00:03:14.780 --> 00:03:23.260]   It's a simple but profound insight, which is that it's very difficult for a model to
[00:03:23.260 --> 00:03:34.340]   generate something creative and aesthetic and correct from nothing or from nothing but
[00:03:34.340 --> 00:03:39.180]   a prompt or a question or whatever.
[00:03:39.180 --> 00:03:44.780]   And the profound insight is to say, well, given that that's hard, why don't we not ask
[00:03:44.780 --> 00:03:50.460]   a model to do that directly, but why don't we train a model to do something a little
[00:03:50.460 --> 00:03:52.820]   bit better than nothing.
[00:03:52.820 --> 00:03:56.860]   And then make a model that if we run it multiple times, takes the thing that's a little bit
[00:03:56.860 --> 00:04:01.460]   better than nothing and makes that a little bit better still and a little bit better still.
[00:04:01.460 --> 00:04:06.940]   And so if you run the model multiple times, as long as it's capable of improving the previous
[00:04:06.940 --> 00:04:12.220]   output each time, and it's just a case of running it lots of times.
[00:04:12.220 --> 00:04:17.560]   And that's the insight behind diffusion models.
[00:04:17.560 --> 00:04:19.860]   As you'd be well aware, Lucas, it's not a new insight.
[00:04:19.860 --> 00:04:25.060]   It's the same basic insight that all that belongs to this class of models called boosted
[00:04:25.060 --> 00:04:26.060]   models.
[00:04:26.060 --> 00:04:31.820]   Boosted models are when you train a model to fix a previous model, to find its errors
[00:04:31.820 --> 00:04:33.820]   and reduce them.
[00:04:33.820 --> 00:04:38.340]   And so we use lots of boosted models, gradient boosting machines in particular, are particularly
[00:04:38.340 --> 00:04:39.340]   popular.
[00:04:39.340 --> 00:04:44.980]   But any model can be turned into a boosted model by training it to fix the previous model's
[00:04:44.980 --> 00:04:45.980]   errors.
[00:04:45.980 --> 00:04:52.380]   But yeah, we haven't really done that in generative models before.
[00:04:52.380 --> 00:04:56.380]   And we now have a whole infrastructure for how to do it well.
[00:04:56.380 --> 00:05:01.380]   And the interesting thing is that having started to get deep into the area, I've realized we're
[00:05:01.380 --> 00:05:05.540]   not close at all to doing that in an optimal way.
[00:05:05.540 --> 00:05:10.700]   So the fantastic results you're seeing at the moment are based on what in a year's time
[00:05:10.700 --> 00:05:17.060]   or you know, or two, will be considered extremely primitive approaches.
[00:05:17.060 --> 00:05:19.860]   Could you say a little more about that?
[00:05:19.860 --> 00:05:21.980]   Sure.
[00:05:21.980 --> 00:05:32.120]   So broadly speaking we're looking to create a function that if we apply it to an input,
[00:05:32.120 --> 00:05:34.860]   it returns a better version of that input.
[00:05:34.860 --> 00:05:40.780]   So for example if we try to create a picture that represents a cute photo of a teddy bear,
[00:05:40.780 --> 00:05:44.740]   then we want a function that takes anything that's not yet a really great cute photo of
[00:05:44.740 --> 00:05:48.300]   a teddy bear and makes it something a little bit more like a cute photo of a teddy bear
[00:05:48.300 --> 00:05:50.820]   than what it started with.
[00:05:50.820 --> 00:05:55.060]   And furthermore that can take the output of a previous version of running this model and
[00:05:55.060 --> 00:06:00.900]   run it again to create something that's even more like a cute version of a teddy bear.
[00:06:00.900 --> 00:06:07.820]   It's a little harder than it first sounds because of this problem of out of distribution
[00:06:07.820 --> 00:06:08.820]   inputs.
[00:06:08.820 --> 00:06:15.020]   The thing is if the result of running the model once is something that does look a little
[00:06:15.020 --> 00:06:21.380]   bit more like a teddy bear, that output needs to be valid as input to running the model
[00:06:21.380 --> 00:06:22.380]   again.
[00:06:22.380 --> 00:06:27.580]   You know if it's not something the model's been trained to recognize, it's not going
[00:06:27.580 --> 00:06:29.840]   to do a good job.
[00:06:29.840 --> 00:06:39.140]   So the tricky way that current approaches generally do that is that they basically do
[00:06:39.140 --> 00:06:47.100]   the same thing that we taught in our 2018-2019 course, which is what we call 'crappification',
[00:06:47.100 --> 00:06:52.220]   which is to take a perfectly good image and make it crappy.
[00:06:52.220 --> 00:07:00.020]   In the course what we did was we added JPEG noise to it and reduced its resolution and
[00:07:00.020 --> 00:07:02.340]   scrawled text over the top of it.
[00:07:02.340 --> 00:07:09.980]   The approach that's used today is actually much more rigorous but in some ways less flexible.
[00:07:09.980 --> 00:07:12.980]   It's to sprinkle Gaussian noise all over it.
[00:07:12.980 --> 00:07:19.220]   So basically add or subtract random numbers for every pixel.
[00:07:19.220 --> 00:07:25.900]   And the key thing is then that one step of inference, so making it slightly more like
[00:07:25.900 --> 00:07:31.780]   a cute teddy bear, is basically to do your best to create a cute teddy bear and then
[00:07:31.780 --> 00:07:36.940]   sprinkle a whole bunch of noise back onto the pixels, but a bit less noise than you
[00:07:36.940 --> 00:07:38.660]   had before.
[00:07:38.660 --> 00:07:42.540]   And so that's by definition at least going to be pretty close to being in distribution
[00:07:42.540 --> 00:07:48.900]   in the sense that you train a model that learns to take pictures which have varying amounts
[00:07:48.900 --> 00:07:52.020]   of noise sprinkled over them and to remove that noise.
[00:07:52.020 --> 00:07:57.380]   So yeah, so you can just add a bit less noise and then you run the model again and add a
[00:07:57.380 --> 00:08:00.780]   bit of noise back but a bit less noise and then run the model again and add a bit noise
[00:08:00.780 --> 00:08:03.500]   in the back but a bit less noise and so forth.
[00:08:03.500 --> 00:08:10.060]   So it's really neat but it's like a lot of it's done this way because of kind of theoretical
[00:08:10.060 --> 00:08:14.300]   convenience I guess, and it's worked really well because we can use that theoretical convenience
[00:08:14.300 --> 00:08:20.060]   to figure out like what good hyperparameters are and you know get a lot of the details
[00:08:20.060 --> 00:08:23.820]   working pretty well.
[00:08:23.820 --> 00:08:27.420]   But there's totally different ways you can do things and you can see like even in the
[00:08:27.420 --> 00:08:32.100]   last week there's been two very significant papers that have dramatically improved the
[00:08:32.100 --> 00:08:33.860]   state of the art.
[00:08:33.860 --> 00:08:39.260]   Both of which don't run the same model each time during this boosting phase, during this
[00:08:39.260 --> 00:08:45.220]   diffusion phase, but they have different models for different amounts of noise or there are
[00:08:45.220 --> 00:08:50.220]   some which will have like super resolution stages.
[00:08:50.220 --> 00:08:53.300]   So you're basically creating something small and then making it bigger and you have different
[00:08:53.300 --> 00:08:56.780]   models for those.
[00:08:56.780 --> 00:09:01.300]   Basically what I think, yeah, what we're starting to see is like that gradual move away from
[00:09:01.300 --> 00:09:07.900]   the stuff that's theoretically convenient to stuff that like is more flexible, has more
[00:09:07.900 --> 00:09:12.420]   fiddly hyperparameters to tune, but then people spending more time tuning those hyperparameters,
[00:09:12.420 --> 00:09:20.060]   creating more complex mixture of experts or ensembles.
[00:09:20.060 --> 00:09:24.420]   And I think yeah there's going to be a lot more of that happening.
[00:09:24.420 --> 00:09:29.980]   And also the biggest piece I think will be this whole question of like well how do we
[00:09:29.980 --> 00:09:34.460]   use them with humans in the loop most effectively.
[00:09:34.460 --> 00:09:42.660]   Because like you know the purpose of these is to create stuff and currently it's like
[00:09:42.660 --> 00:09:46.940]   it's almost an accident that we can ask for a photo of a particular kind of thing, you
[00:09:46.940 --> 00:09:50.500]   know like a cute teddy bear.
[00:09:50.500 --> 00:09:54.000]   The models are trained with what's called conditioning, where they're conditioned on
[00:09:54.000 --> 00:09:59.300]   these captions, but like the captions are known to be wrong because they come from the
[00:09:59.300 --> 00:10:02.060]   alt tags in HTML web pages.
[00:10:02.060 --> 00:10:06.300]   Those alt tags are very rarely accurate descriptions of pictures.
[00:10:06.300 --> 00:10:09.180]   So the whole thing, you know, and then the way the conditioning is done is kind of really
[00:10:09.180 --> 00:10:15.220]   got nothing to do with actually trying to create something that will respond to prompts.
[00:10:15.220 --> 00:10:18.380]   So the prompts themselves are a bit of an accident and the conditioning is kind of a
[00:10:18.380 --> 00:10:19.380]   bit of an accident.
[00:10:19.380 --> 00:10:25.080]   The fact that we can use prompts at all, it's a bit of an accident and as a result it's
[00:10:25.080 --> 00:10:31.220]   a huge art right now to figure out like how, you know, trending on ArtStation, 8k ultra
[00:10:31.220 --> 00:10:37.400]   realistic, you know, portrait of Lucas Biewald looking thoughtful or whatever.
[00:10:37.400 --> 00:10:41.260]   There's whole books, you know, of like here's lots of prompts we tried and here's what the
[00:10:41.260 --> 00:10:43.780]   outputs look like.
[00:10:43.780 --> 00:10:49.380]   And how do you like customize that because actually, you know, you're trying to create
[00:10:49.380 --> 00:10:57.420]   a storybook about, you know, Lucas Biewald's progress in creating a new startup and you
[00:10:57.420 --> 00:11:00.260]   want, you know, you want it to fit into this particular box here.
[00:11:00.260 --> 00:11:02.900]   Do you want a picture of a robot in the background there?
[00:11:02.900 --> 00:11:13.180]   And you know, like how do you get, you know, the same style, the same character content,
[00:11:13.180 --> 00:11:17.700]   the particular composition, you know, it's all about this interaction between human and
[00:11:17.700 --> 00:11:18.700]   machine.
[00:11:18.700 --> 00:11:27.020]   There's so many things, you know, which we're just starting to understand how to do.
[00:11:27.020 --> 00:11:33.740]   And so in the coming years, I think it will turn into a powerful tool for, you know, computer
[00:11:33.740 --> 00:11:39.960]   assisted human creativity rather than what it is now, which is more of a hand something
[00:11:39.960 --> 00:11:44.820]   off to the machine and hope that it's useful.
[00:11:44.820 --> 00:11:51.220]   Do you think the same approach applies across domains or is there something about images,
[00:11:51.220 --> 00:11:56.540]   the way it's sort of obvious how to add noise and maybe the dataset that we have?
[00:11:56.540 --> 00:12:02.420]   I mean, certainly the way you described diffusion, you could, there's a natural application of
[00:12:02.420 --> 00:12:07.820]   that to almost any domain, but I guess Gaussian noise on text, it's a little unclear to me
[00:12:07.820 --> 00:12:10.260]   like what that really means.
[00:12:10.260 --> 00:12:13.780]   So last week a paper showing diffusion for text came out.
[00:12:13.780 --> 00:12:18.460]   There's already diffusion models for like proteins.
[00:12:18.460 --> 00:12:20.780]   There's already diffusion models for audio.
[00:12:20.780 --> 00:12:28.020]   The audio ones use, or some of them use a fairly hacky, obvious, but neat approach of
[00:12:28.020 --> 00:12:33.020]   using diffusion to generate spectrograms, which are images, and then having something
[00:12:33.020 --> 00:12:37.540]   like a super resolution model, but it's not doing super resolution, it's doing spectrograms
[00:12:37.540 --> 00:12:41.460]   to sound.
[00:12:41.460 --> 00:12:44.140]   So yeah, these things are already starting to exist.
[00:12:44.140 --> 00:12:48.380]   They haven't had as much resources put into them yet, so they're still not that great.
[00:12:48.380 --> 00:12:53.660]   But yeah, that's the thing, Lucas, this is not just images at all.
[00:12:53.660 --> 00:13:01.540]   It'll be used in medicine, it'll be used in copywriting.
[00:13:01.540 --> 00:13:06.740]   I mean, you know, the way we currently do generative text models, again, it's kind of
[00:13:06.740 --> 00:13:08.820]   a happy accident.
[00:13:08.820 --> 00:13:14.380]   So when I did ULMfit, the whole reason I created a language model was for the purpose of fine
[00:13:14.380 --> 00:13:16.940]   tuning it to create a classifier, you know.
[00:13:16.940 --> 00:13:21.540]   And GPT then took that idea and scaled it up with transformers.
[00:13:21.540 --> 00:13:25.540]   You know, what Alec Radford was trying to do there was not generate text, but trying
[00:13:25.540 --> 00:13:29.420]   to solve other problems by fine tuning it.
[00:13:29.420 --> 00:13:33.580]   There was this kind of discovery almost with GPT-3 that when you take this and you scale
[00:13:33.580 --> 00:13:39.620]   it far enough, it actually starts generating reasonable sounding text.
[00:13:39.620 --> 00:13:41.980]   But the text is not necessarily correct.
[00:13:41.980 --> 00:13:44.620]   In fact, it's very often wildly incorrect.
[00:13:44.620 --> 00:13:50.060]   And so yeah, you know, intentionally working on text generation approaches, which are specifically
[00:13:50.060 --> 00:13:56.100]   designed for generating text, is something that there's a lot of room to improve.
[00:13:56.100 --> 00:14:00.420]   And generally speaking, the way I see it is this, is you've got a generative model that's
[00:14:00.420 --> 00:14:06.300]   trying to do something difficult, and it's pretty good at it, or at least better than
[00:14:06.300 --> 00:14:07.300]   nothing.
[00:14:07.300 --> 00:14:12.300]   It'll be better at it if you can do it in a way that it runs multiple times during inference,
[00:14:12.300 --> 00:14:17.900]   because you're giving it more opportunities to do its thing.
[00:14:17.900 --> 00:14:23.180]   So I think, you know, that means that these multi-step inference models, you know, which
[00:14:23.180 --> 00:14:28.060]   may or may not be diffusion models, but kind of boosted generative models, are here to
[00:14:28.060 --> 00:14:33.980]   stay because no matter how good your model, generative model is, you can always make it
[00:14:33.980 --> 00:14:40.340]   better if you can find a way to write it multiple times.
[00:14:40.340 --> 00:14:44.980]   I guess that is a good segue to another question I had, which is, I think one of the really
[00:14:44.980 --> 00:14:50.660]   fun things about deep learning in the early days was it was so tangible.
[00:14:50.660 --> 00:14:54.500]   And you had, you know, you have this fantastic class where you can just kind of, you know,
[00:14:54.500 --> 00:14:57.580]   build these models and see how they work and play with them.
[00:14:57.580 --> 00:15:00.740]   And so I think we both have a very similar learning approach.
[00:15:00.740 --> 00:15:04.700]   But one thing I've personally been struggling with, honestly, with these bigger models is
[00:15:04.700 --> 00:15:08.500]   just actually engaging with them in a meaningful way.
[00:15:08.500 --> 00:15:15.740]   You know, it's fun to run the various image generating models, but it feels kind of daunting.
[00:15:15.740 --> 00:15:21.180]   I'm not sure I have the money myself to buy the compute to make one that really works.
[00:15:21.180 --> 00:15:24.860]   We actually had one person on this podcast who did it for fun, Boris, which is a super
[00:15:24.860 --> 00:15:29.060]   fun episode, and I felt really jealous of how much fun he had building it.
[00:15:29.060 --> 00:15:33.660]   But I'm curious how you turn that problem into something like tractable that you can
[00:15:33.660 --> 00:15:35.140]   actually engage with.
[00:15:35.140 --> 00:15:36.140]   Yeah.
[00:15:36.140 --> 00:15:38.580]   Well, you know, I mean, remember Boris is one of our alumni.
[00:15:38.580 --> 00:15:44.740]   He's part of our fast.ai community and he showed what is possible for a single tenacious
[00:15:44.740 --> 00:15:45.740]   person to do.
[00:15:45.740 --> 00:15:49.860]   Although I think Google like donated like a hundred thousand dollars of compute to him.
[00:15:49.860 --> 00:15:50.860]   So it wasn't totally.
[00:15:50.860 --> 00:15:51.860]   Yeah, absolutely.
[00:15:51.860 --> 00:15:56.060]   If you know that you're doing useful work, then there's plenty of compute out there which
[00:15:56.060 --> 00:15:57.260]   you can get donated to.
[00:15:57.260 --> 00:16:01.340]   But having said that, you know, what he was largely trying to do, at least at the outset,
[00:16:01.340 --> 00:16:05.420]   was to replicate, you know, what OpenAI had done.
[00:16:05.420 --> 00:16:09.980]   I take a very different approach, which is I always assume that the best thing out there
[00:16:09.980 --> 00:16:12.660]   right now is far short of what the best thing could be.
[00:16:12.660 --> 00:16:15.140]   You know, that in five to ten years time, there'll be something better.
[00:16:15.140 --> 00:16:17.980]   And I always look for improving that.
[00:16:17.980 --> 00:16:21.660]   So yeah, so you should take our new course, Lucas.
[00:16:21.660 --> 00:16:22.660]   I was looking.
[00:16:22.660 --> 00:16:28.340]   Which we're in the middle of, because what I've been working on is exactly what you described,
[00:16:28.340 --> 00:16:36.500]   which is how to train and play with the state of the art image generative model in a notebook
[00:16:36.500 --> 00:16:38.820]   on a single GPU.
[00:16:38.820 --> 00:16:44.060]   And as with all of these things, the trick is to start with an easier but equivalent
[00:16:44.060 --> 00:16:45.500]   problem.
[00:16:45.500 --> 00:16:53.260]   So I'm doing all my work just about on the fashion MNIST dataset, which rather than being
[00:16:53.260 --> 00:17:00.820]   512 by 512 pixel images of literally anything in the world, including artworks, they're
[00:17:00.820 --> 00:17:03.420]   20, they're three channel.
[00:17:03.420 --> 00:17:10.380]   Fashion MNIST is 28 by 28 single channel images of one of 10 types of clothing.
[00:17:10.380 --> 00:17:15.700]   And now I always tell people like whether you're doing a Kaggle competition or a project
[00:17:15.700 --> 00:17:24.260]   at work or whatever, the most important two steps are to create a rapid feedback loop
[00:17:24.260 --> 00:17:32.180]   where you can iterate and test fast, and to have a test which is highly correlated with
[00:17:32.180 --> 00:17:34.300]   the final thing you're going to be doing.
[00:17:34.300 --> 00:17:39.140]   So that if you have those two things, you can quickly try lots of ideas and see if they're
[00:17:39.140 --> 00:17:45.440]   probably going to work, you know, on the bigger dataset or the bigger, harder problem or whatever.
[00:17:45.440 --> 00:17:51.300]   So it turns out fashion MNIST basically, yeah, I've kind of like replicated a bunch of different
[00:17:51.300 --> 00:17:54.660]   approaches from the literature on fashion MNIST.
[00:17:54.660 --> 00:18:02.340]   The relative effectiveness of those different approaches on fashion MNIST mirrors basically
[00:18:02.340 --> 00:18:10.340]   exactly their relative effectiveness on Cocoa or ImageNet or Lion or whatever.
[00:18:10.340 --> 00:18:16.580]   But I can train a model on a single GPU to a point where I can see relative differences
[00:18:16.580 --> 00:18:19.860]   in about two minutes.
[00:18:19.860 --> 00:18:25.060]   And that means I can, yeah, like very rapidly try things.
[00:18:25.060 --> 00:18:31.380]   And so I've started building notebooks where I show every single little step.
[00:18:31.380 --> 00:18:37.340]   And also it helps a lot to use notebooks, which almost nobody working in the generative
[00:18:37.340 --> 00:18:39.220]   modeling field seems to be doing at the moment.
[00:18:39.220 --> 00:18:45.100]   So what they do is they have, you know, the normal approach is to, you know, do ImageNet
[00:18:45.100 --> 00:18:54.260]   64 pixel or, you know, Cypher 32 pixel, which is still better than doing 512 by 512 Lion.
[00:18:54.260 --> 00:19:04.000]   But it still takes, you know, ImageNet 64 pixel takes many hours on an 8 GPU machine.
[00:19:04.000 --> 00:19:10.300]   You can't do a fast iteration loop, you know.
[00:19:10.300 --> 00:19:16.380]   So yeah, in a notebook, you know, I can like run a single iteration of diffusion.
[00:19:16.380 --> 00:19:20.540]   I can see what the outputs look like because the pictures are all there in front of me.
[00:19:20.540 --> 00:19:23.820]   You know, if you're not using this kind of approach, instead you're switching back and
[00:19:23.820 --> 00:19:27.500]   forth between a terminal and then you need some way of actually viewing the images.
[00:19:27.500 --> 00:19:32.660]   And given that you're probably not sitting directly on that 8 GPU box, you're probably
[00:19:32.660 --> 00:19:33.940]   SSHing into it.
[00:19:33.940 --> 00:19:38.740]   So now you've got to find a way to show those pictures.
[00:19:38.740 --> 00:19:41.740]   There are ways, by the way, of showing pictures in the terminal.
[00:19:41.740 --> 00:19:45.820]   For example, if you use iTerm2, there's something called ImageCat.
[00:19:45.820 --> 00:19:51.940]   If you use other terminals, they probably support something called Sixel, Sixel graphics.
[00:19:51.940 --> 00:19:57.660]   But there's, you know, they're not going to be as good an exploration environment for
[00:19:57.660 --> 00:20:00.740]   this kind of stuff than a notebook is.
[00:20:00.740 --> 00:20:05.940]   So yeah, I think there's lots of opportunities for, you know, people like you and me to play
[00:20:05.940 --> 00:20:06.940]   in this field.
[00:20:06.940 --> 00:20:12.220]   I mean, I know there is because I've, you know, started spending time talking to some
[00:20:12.220 --> 00:20:16.900]   of the folks who were the primary researchers responsible for the key components of stable
[00:20:16.900 --> 00:20:23.260]   diffusion and I'm already telling them things that they hadn't thought of before by virtue
[00:20:23.260 --> 00:20:29.460]   of weird little experiments I've done with Fashioned MNIST on my single GPU Jupyter notebook.
[00:20:29.460 --> 00:20:30.460]   Yeah, that makes sense.
[00:20:30.460 --> 00:20:33.060]   I mean, a fast feedback loop is so important.
[00:20:33.060 --> 00:20:34.740]   I mean, that's very cool.
[00:20:34.740 --> 00:20:39.340]   I was curious broadly if you have thoughts on stable diffusion in general.
[00:20:39.340 --> 00:20:43.740]   I feel like, you know, we're sitting here in November, you know, 2022, and I think they've
[00:20:43.740 --> 00:20:49.100]   done an amazing job of bringing awareness to generative models.
[00:20:49.100 --> 00:20:53.780]   I don't know, what do you think of a stable diffusion?
[00:20:53.780 --> 00:21:00.100]   I mean, it's been great for progress in the field, clearly.
[00:21:00.100 --> 00:21:06.700]   Yeah, I mean, generally speaking, I'm all about democratization and accessibility, as
[00:21:06.700 --> 00:21:09.980]   you know.
[00:21:09.980 --> 00:21:17.220]   I don't love the fact that before stable diffusion was released, you know, a small number of
[00:21:17.220 --> 00:21:23.540]   people in the world had access to the full generative models and then other people could
[00:21:23.540 --> 00:21:29.020]   like pay for cut down versions of them, use them in small quantities.
[00:21:29.020 --> 00:21:36.660]   The thing is, accessing these things through a web-based API is extremely limiting, you
[00:21:36.660 --> 00:21:41.100]   know, when you've actually got the weights, you can really play with both the engineering
[00:21:41.100 --> 00:21:46.540]   and the artistic side of doing things that no one's done before.
[00:21:46.540 --> 00:21:47.580]   So yeah, I think that's great.
[00:21:47.580 --> 00:21:50.300]   I think it's important.
[00:21:50.300 --> 00:21:54.540]   I think, you know, as with any of these things, you release a new powerful technology out
[00:21:54.540 --> 00:21:59.380]   there and a whole bunch of people are going to be using it for, you know, not necessarily
[00:21:59.380 --> 00:22:01.180]   the things that you would have chosen to use it for.
[00:22:01.180 --> 00:22:06.460]   So for example, for stable diffusion, it seems like a very large percentage of people who
[00:22:06.460 --> 00:22:12.100]   are using it to generate lots and lots of images are doing it to generate anime and
[00:22:12.100 --> 00:22:19.780]   specifically nearly entirely, you know, very young women with very few clothes on anime
[00:22:19.780 --> 00:22:21.420]   pictures.
[00:22:21.420 --> 00:22:28.900]   And I'm sure there are people out there who are taking the clothes off entirely, you know.
[00:22:28.900 --> 00:22:35.940]   So that's, I mean, that happens, I guess, with any technology and I don't necessarily
[00:22:35.940 --> 00:22:42.380]   have, I mean, you can't, I guess you can't stop that happening.
[00:22:42.380 --> 00:22:48.460]   But we certainly need appropriate laws around at least, you know, making illegal things,
[00:22:48.460 --> 00:22:51.900]   make sure that things that we don't want to be legal are in fact illegal.
[00:22:51.900 --> 00:22:58.140]   But yeah, I mean, there are obviously huge benefits and you're not going to get stuff
[00:22:58.140 --> 00:23:04.900]   like, you know, protein diffusion models or, you know, pharmaceutical diffusion models
[00:23:04.900 --> 00:23:10.140]   or, you know, none of those are going to develop if the technology is in the hands of like
[00:23:10.140 --> 00:23:12.940]   two or three big organizations.
[00:23:12.940 --> 00:23:17.180]   So it's certainly a very valuable step on the whole for society to have this stuff as
[00:23:17.180 --> 00:23:18.660]   open as possible.
[00:23:18.660 --> 00:23:24.940]   And to be clear, it was all trained at universities, you know, so the one, the main one, most of
[00:23:24.940 --> 00:23:30.660]   the stuff we're using now for stable diffusion is trained in Germany, at German academic
[00:23:30.660 --> 00:23:39.020]   institutions using donated hardware.
[00:23:39.020 --> 00:23:47.300]   I guess it's interesting though, that it was, I think, primarily ethics and AI considerations
[00:23:47.300 --> 00:23:51.660]   that, you know, made folks like OpenAI kind of restrict access to their models, or at
[00:23:51.660 --> 00:23:53.260]   least that's what they said.
[00:23:53.260 --> 00:23:59.780]   Do you think that you would know a priori that that was like the wrong thing to do?
[00:23:59.780 --> 00:24:03.220]   Were you, would you have pushed against it at the time?
[00:24:03.220 --> 00:24:07.940]   I actually read a blog post about that back in when GPT-3 was first like announced and
[00:24:07.940 --> 00:24:09.820]   not released.
[00:24:09.820 --> 00:24:16.140]   And nearly universally the feedback, at least from the AI community was, "Oh, this is lame.
[00:24:16.140 --> 00:24:18.780]   They're just doing it for profits."
[00:24:18.780 --> 00:24:25.540]   And in my blog post I said like, "Well, not necessarily, you know, like there are genuine
[00:24:25.540 --> 00:24:30.580]   things to be thinking about here."
[00:24:30.580 --> 00:24:36.780]   Which is not to say that that means that the motivation wasn't at least partially profit-driven.
[00:24:36.780 --> 00:24:37.780]   It might well have been.
[00:24:37.780 --> 00:24:44.020]   Like it's certainly convenient that the ethical considerations read in this way entirely align
[00:24:44.020 --> 00:24:48.140]   with profit-driven motives as well.
[00:24:48.140 --> 00:24:50.700]   But like I say, it doesn't necessarily mean they're not true.
[00:24:50.700 --> 00:24:53.120]   And I'm pretty sure it's for both reasons.
[00:24:53.120 --> 00:24:57.660]   And if you look at the way OpenAI has behaved since then, they've behaved in a way that
[00:24:57.660 --> 00:25:01.660]   is very increasingly apparently profit-driven.
[00:25:01.660 --> 00:25:10.900]   So I'm less generous in my interpretation now than I was then, based on their continuing
[00:25:10.900 --> 00:25:14.900]   patterns of behavior.
[00:25:14.900 --> 00:25:20.820]   And I think also with the benefit of hindsight, it feels a lot more like, you know, in the
[00:25:20.820 --> 00:25:26.460]   last couple of years, companies keeping models to themselves, the main impact that seems
[00:25:26.460 --> 00:25:34.740]   to have been is to create a bigger bifurcation between haves and have-nots in terms of capability,
[00:25:34.740 --> 00:25:42.020]   requiring more researchers to pay for API access to do things, a decreased amount of
[00:25:42.020 --> 00:25:48.660]   openness that in fact even, you know, what could be argued as being kind of deceitful
[00:25:48.660 --> 00:25:50.420]   behavior.
[00:25:50.420 --> 00:25:57.260]   So like, for example, we now know that the OpenAI models that you can pay to access are
[00:25:57.260 --> 00:26:01.620]   actually not the same as what's been described in their research papers.
[00:26:01.620 --> 00:26:06.740]   And we've now had dozens of people write research papers comparing various work to the OpenAI
[00:26:06.740 --> 00:26:07.740]   models.
[00:26:07.740 --> 00:26:10.380]   And now we've learned that actually we're not comparing to what we thought we were comparing
[00:26:10.380 --> 00:26:17.780]   at all and, you know, thousands of hours of researcher time being wasted and papers being
[00:26:17.780 --> 00:26:23.480]   published with what turns out now to actually be totally wrong information in.
[00:26:23.480 --> 00:26:33.860]   So yeah, I'm definitely, you know, more enthusiastic about the idea of being open than perhaps
[00:26:33.860 --> 00:26:38.580]   more confident about that than I was a couple of years ago.
[00:26:38.580 --> 00:26:43.140]   And I guess, do you have thoughts on the language side of things, like large language models?
[00:26:43.140 --> 00:26:50.940]   Like do you think that, for example, do you think that prompt engineering is headed to
[00:26:50.940 --> 00:26:55.060]   be like an important way of doing machine learning?
[00:26:55.060 --> 00:26:59.500]   Like, you know, you do see these models doing incredibly well in like a wide variety of
[00:26:59.500 --> 00:27:05.300]   NLP tasks, like better than models, you know, trained specifically on these specific tasks
[00:27:05.300 --> 00:27:06.300]   sometimes.
[00:27:06.300 --> 00:27:11.300]   Yeah, I think generative text models have both more opportunities and more threats than
[00:27:11.300 --> 00:27:15.220]   generative image models, for sure.
[00:27:15.220 --> 00:27:18.340]   Like I say, they're kind of, the fact that they work at all is in some ways a bit of
[00:27:18.340 --> 00:27:19.340]   an accident.
[00:27:19.340 --> 00:27:26.620]   They're far, far, far from being optimized for purpose at the moment.
[00:27:26.620 --> 00:27:33.260]   But they're already amazingly good, particularly if you do this kind of stuff where, I mean,
[00:27:33.260 --> 00:27:38.420]   literally there are now dozens of papers, just look at like what kind of prompts happen
[00:27:38.420 --> 00:27:42.860]   to work on these models that we kind of accidentally made generative models, who's, you know, let's
[00:27:42.860 --> 00:27:48.020]   think step by step and whatever else.
[00:27:48.020 --> 00:27:51.980]   We're starting to find ways to actually get them to do a little bit more of what we actually
[00:27:51.980 --> 00:27:52.980]   want them to do.
[00:27:52.980 --> 00:27:59.240]   But so far using really, really basic things like, you know, this instruction tuning.
[00:27:59.240 --> 00:28:02.580]   So you know, rather than just feeding it the entire internet, let's actually fine tune
[00:28:02.580 --> 00:28:05.380]   it with some examples of things that are actually correct.
[00:28:05.380 --> 00:28:09.900]   So it actually represent outputs that we would want for these inputs rather than just whatever
[00:28:09.900 --> 00:28:14.180]   somebody rando wrote on the internet 25 years ago.
[00:28:14.180 --> 00:28:22.340]   Yeah, so my worry is, I'm much more worried about misuse of text models and image models,
[00:28:22.340 --> 00:28:29.660]   because it wouldn't be at all hard to create a million Twitter or Facebook or whatever
[00:28:29.660 --> 00:28:40.700]   accounts and program them to work together to impact the kind of world's discourse in
[00:28:40.700 --> 00:28:45.580]   very substantial ways over time.
[00:28:45.580 --> 00:28:51.500]   And nobody would know, you know, so we could have like, you know, like on Twitter, for
[00:28:51.500 --> 00:28:59.540]   example, some, you know, fairly small number of accounts, often where nobody actually knows
[00:28:59.540 --> 00:29:06.300]   the human who's behind it, can have very substantive effects on like, what people are talking about
[00:29:06.300 --> 00:29:11.600]   and how do people talk about that thing.
[00:29:11.600 --> 00:29:19.180]   And so imagine, yeah, a million of those accounts, which were actually bots that had been trained
[00:29:19.180 --> 00:29:25.420]   to be more compelling than humans, which already for years, we've had bots, which humans rank
[00:29:25.420 --> 00:29:29.100]   as more compelling than actual humans.
[00:29:29.100 --> 00:29:34.500]   And that they've been trained to work together, you know, take alternate points of view in
[00:29:34.500 --> 00:29:35.700]   exactly the right way.
[00:29:35.700 --> 00:29:45.140]   And this bot gradually gets convinced by that bot and whatever else, like, yeah, it could
[00:29:45.140 --> 00:29:52.380]   cause a very small number of people in the world to programmably decide how they want
[00:29:52.380 --> 00:29:56.860]   humanity to think about a topic and pay to make that happen.
[00:29:56.860 --> 00:30:03.420]   Although if I remember right, it seemed like all of fast AI's like sort of broad mandate
[00:30:03.420 --> 00:30:09.020]   was to basically make a no code interface into machine learning so anyone could access
[00:30:09.020 --> 00:30:10.020]   it.
[00:30:10.020 --> 00:30:14.180]   And it does sort of seem like prompt engineering, to the extent that it works, is like a huge
[00:30:14.180 --> 00:30:15.780]   step in that direction.
[00:30:15.780 --> 00:30:16.780]   Right.
[00:30:16.780 --> 00:30:17.780]   Yeah, that's what I'm saying.
[00:30:17.780 --> 00:30:21.900]   It's like, that's why I said it's like both got more opportunities and more threats.
[00:30:21.900 --> 00:30:30.320]   So yeah, the opportunities are vast to, you know, so take for example, the recent thing
[00:30:30.320 --> 00:30:37.660]   that was released like last week or so, explainpaper.com where our students are already, you know,
[00:30:37.660 --> 00:30:42.260]   so with our course, we do, you know, look at a paper or two each week.
[00:30:42.260 --> 00:30:49.020]   And so, and so last week I had told the class's homework to re-implement the diffedit paper.
[00:30:49.020 --> 00:30:51.620]   And so students are saying like, oh, I didn't understand this paragraph.
[00:30:51.620 --> 00:30:54.820]   So I highlighted it and explainpaper.com and here's the summary it gave.
[00:30:54.820 --> 00:30:58.300]   And that's a lot more clear now that I tried to understand that bit.
[00:30:58.300 --> 00:31:00.980]   So I asked for more information.
[00:31:00.980 --> 00:31:05.300]   You know, this is very, very valuable.
[00:31:05.300 --> 00:31:09.860]   And you know, I saw somebody on Twitter a couple of days ago saying they don't really
[00:31:09.860 --> 00:31:13.860]   use Stack Overflow anymore because they created this tiny little simple little script called
[00:31:13.860 --> 00:31:19.740]   ask where they type ask and then something at a prompt, sorry, at a, in the bash, you
[00:31:19.740 --> 00:31:25.860]   know, shell REPL and it would feed that off to OpenAI GPT-3 and return back the result.
[00:31:25.860 --> 00:31:31.460]   And they basically use that instead of searching the internet nowadays.
[00:31:31.460 --> 00:31:37.140]   So yeah, people are definitely using this stuff and it's going to get much, much better.
[00:31:37.140 --> 00:31:42.700]   Do you have a clever way, like with fashion MNIST and image generation to play with large
[00:31:42.700 --> 00:31:47.500]   language models on kind of a bite-sized scale?
[00:31:47.500 --> 00:31:48.500]   Not yet.
[00:31:48.500 --> 00:31:49.500]   No.
[00:31:49.500 --> 00:31:55.020]   I, some, you know, I'll get to that maybe in another part of the course, I guess.
[00:31:55.020 --> 00:31:59.220]   It's definitely a great question and something to think about.
[00:31:59.220 --> 00:32:00.220]   Interesting.
[00:32:00.220 --> 00:32:01.220]   Okay.
[00:32:01.220 --> 00:32:06.500]   A question that I need to revisit because this is unexpectedly, I think one of the reasons
[00:32:06.500 --> 00:32:10.900]   that so many people listened to my interview with you last time, you sort of made an interesting
[00:32:10.900 --> 00:32:15.900]   comment that you felt like Python wasn't the future of ML and you sort of said maybe Julia
[00:32:15.900 --> 00:32:23.100]   is the future of ML and that really seemed to like strike a chord with the internet everywhere.
[00:32:23.100 --> 00:32:27.300]   I think it's kind of the most discussed part of great sense of all time.
[00:32:27.300 --> 00:32:30.660]   So I'm just curious, do you have any more thoughts on that?
[00:32:30.660 --> 00:32:36.940]   Like I, do you, do you sort of still believe that like Julia is the future?
[00:32:36.940 --> 00:32:38.780]   You're sort of on the fence about that.
[00:32:38.780 --> 00:32:47.660]   I mean, I was on the fence about that last time we spoke and I would say I'm a little
[00:32:47.660 --> 00:32:52.500]   less bullish than I was then.
[00:32:52.500 --> 00:32:59.780]   Because I just, I feel like the Julia ecosystem and culture, you know, it's so focused on
[00:32:59.780 --> 00:33:09.500]   these like HPC kind of like huge compute running things on national labs machines and it's
[00:33:09.500 --> 00:33:13.460]   all stuff that's very appealing to engineers.
[00:33:13.460 --> 00:33:19.360]   It feels good, but it's such a tiny audience, you know, and it's not like, I don't care
[00:33:19.360 --> 00:33:25.700]   about whether I can run something on 5,000 nodes.
[00:33:25.700 --> 00:33:31.220]   I just want to run it on my laptop and it's still not great for running on my laptop really.
[00:33:31.220 --> 00:33:36.020]   And it's not great for creating software that I can send you.
[00:33:36.020 --> 00:33:39.500]   I can't, you know, if I created a little CLI tool or whatever, well, it's not great for
[00:33:39.500 --> 00:33:42.340]   creating little CLI tools because it's so slow to start up.
[00:33:42.340 --> 00:33:45.340]   And then how the hell am I going to send it to you to try out?
[00:33:45.340 --> 00:33:51.260]   You'd be like, okay, Lucas, well install the entirety of Julia and then run the REPL and
[00:33:51.260 --> 00:33:56.220]   then type this to go into package management mode and then, okay, now you've got this thing
[00:33:56.220 --> 00:33:57.220]   and now you can run it.
[00:33:57.220 --> 00:33:59.740]   It's like, okay, that's not going to happen.
[00:33:59.740 --> 00:34:05.340]   Or you know, even just deploying a website, you know, it's a lot of fuss and bother and
[00:34:05.340 --> 00:34:10.740]   uses more resources than it should.
[00:34:10.740 --> 00:34:15.660]   It's still got that potential, but you know, I guess the other thing that's become more
[00:34:15.660 --> 00:34:26.420]   clear though in the last couple of years is their grand experiment on type dispatch.
[00:34:26.420 --> 00:34:31.620]   It is more challenging to get that all working properly than perhaps I had realized because
[00:34:31.620 --> 00:34:34.700]   it's still not really quite working properly.
[00:34:34.700 --> 00:34:37.580]   And I think good on them for trying to make it work properly.
[00:34:37.580 --> 00:34:39.580]   It's a vast research project.
[00:34:39.580 --> 00:34:44.460]   But you know, there's a lot of weird little edge cases and trying to make that all run
[00:34:44.460 --> 00:34:46.820]   smoothly is incredibly challenging.
[00:34:46.820 --> 00:34:57.540]   So I suspect, yeah, something needs to replace Python, but maybe it's something that doesn't
[00:34:57.540 --> 00:34:58.540]   exist yet.
[00:34:58.540 --> 00:35:04.460]   Partly though, I mean, so what we're seeing instead, everybody knows we have to replace
[00:35:04.460 --> 00:35:05.460]   Python.
[00:35:05.460 --> 00:35:12.060]   So what instead has been happening is we're using Python to create non-Python artifacts.
[00:35:12.060 --> 00:35:20.660]   So most obviously Jax, you know, Jax uses Python or a subset of Python with a kind of
[00:35:20.660 --> 00:35:28.820]   a embedded DSL written as a library, which only lets you create things that can be expressible
[00:35:28.820 --> 00:35:32.060]   as XLA programs.
[00:35:32.060 --> 00:35:39.500]   And then XLA compiles that to run fast on a GPU, on a TPU.
[00:35:39.500 --> 00:35:42.820]   And that works pretty well.
[00:35:42.820 --> 00:35:48.940]   It's very challenging though for research or hacking or learning or whatever, because
[00:35:48.940 --> 00:35:51.780]   it's actually not Python that's running at all.
[00:35:51.780 --> 00:35:59.300]   So it's extremely difficult to like profile, debug and so forth that code.
[00:35:59.300 --> 00:36:05.140]   Very hard to kind of run it, you know, really nicely in notebooks.
[00:36:05.140 --> 00:36:12.860]   So like in our little team working on diffusion models, we kind of all want to use Jax, but
[00:36:12.860 --> 00:36:17.380]   every time we try, it's always like, because like everything I write is always wrong the
[00:36:17.380 --> 00:36:19.180]   first 14 times.
[00:36:19.180 --> 00:36:26.260]   And with Python, you know, I have 14 goes at making it better by like finding all the
[00:36:26.260 --> 00:36:30.020]   stupid things I did by running one line at a time and checking things and looking at
[00:36:30.020 --> 00:36:31.940]   pictures.
[00:36:31.940 --> 00:36:36.340]   With Jax, I wouldn't know how to fix my broken code, really.
[00:36:36.340 --> 00:36:37.340]   It's difficult.
[00:36:37.340 --> 00:36:43.780]   But you don't think that that flexibility is like fundamentally in conflict with making
[00:36:43.780 --> 00:36:44.780]   a language performant?
[00:36:44.780 --> 00:36:47.900]   Like I think we covered this last time.
[00:36:47.900 --> 00:36:48.900]   It is for Python, I think.
[00:36:48.900 --> 00:36:49.900]   Yeah.
[00:36:49.900 --> 00:36:56.460]   So for Python, like that flexibility is to be able to actually run it as Python code.
[00:36:56.460 --> 00:37:02.740]   So like if you look at where PyTorch is going now, they've got this TorchDynamo stuff where
[00:37:02.740 --> 00:37:07.020]   they're working, you know, they basically, you know, can interface with NVFuser and you
[00:37:07.020 --> 00:37:13.700]   can interface with Triton, the open AI compiler-ish thing.
[00:37:13.700 --> 00:37:16.900]   I'm not exactly sure what you'd call it.
[00:37:16.900 --> 00:37:21.860]   And so clearly PyTorch is heading the same direction as Jax, which is if you want it
[00:37:21.860 --> 00:37:27.460]   to run fast, you'll use TorchDynamo or whatever it ends up being called.
[00:37:27.460 --> 00:37:31.380]   That's actually now, you know, integrated into the PyTorch tree.
[00:37:31.380 --> 00:37:33.300]   That's clearly where we're heading.
[00:37:33.300 --> 00:37:36.420]   And again you end up with, you know, probably you'll be using Triton.
[00:37:36.420 --> 00:37:40.740]   So you end up, Triton's amazing, super cool, super fantastic.
[00:37:40.740 --> 00:37:45.700]   But you know, you still end up with this thing that's running compiled code.
[00:37:45.700 --> 00:37:51.660]   It's not the same code you wrote, but a version of it.
[00:37:51.660 --> 00:37:55.780]   Again, more difficult to hack on.
[00:37:55.780 --> 00:37:59.740]   I think, you know, if you look at how this works, you know, there's a whole world of
[00:37:59.740 --> 00:38:04.020]   software that's written in languages which are explicitly designed to work this way.
[00:38:04.020 --> 00:38:11.340]   They're compiled languages, you know, languages like C++ and Swift and Rust.
[00:38:11.340 --> 00:38:16.020]   And they have something very nice, which is they have flags you can pass the compiler.
[00:38:16.020 --> 00:38:20.500]   So you can pass the D flag to run it in the debugger, or you can pass the O flag to run
[00:38:20.500 --> 00:38:22.740]   it, you know, in the optimized version.
[00:38:22.740 --> 00:38:29.500]   And so basically you get to choose how close the code that's actually running is to the
[00:38:29.500 --> 00:38:32.060]   actual lines of code that you wrote.
[00:38:32.060 --> 00:38:36.060]   So that for debugging you can actually, you know, it'll run slower, but it's actually
[00:38:36.060 --> 00:38:39.620]   running the lines of code that you wrote.
[00:38:39.620 --> 00:38:41.460]   And I think we want something like that.
[00:38:41.460 --> 00:38:43.700]   Something that, yeah, it looks like Python.
[00:38:43.700 --> 00:38:45.580]   It's pretty compatible with Python.
[00:38:45.580 --> 00:38:54.660]   But you know, you can still run it as Python, but you can also run it in an optimized way.
[00:38:54.660 --> 00:38:59.100]   You know, maybe something that actually takes better advantage of these kind of type hints
[00:38:59.100 --> 00:39:01.700]   that we can provide.
[00:39:01.700 --> 00:39:13.540]   Yeah, that's my guess is what's going to happen, is we'll see Python-esque languages, you know,
[00:39:13.540 --> 00:39:18.060]   continue to see these Python-esque languages appear that may begin to look less and less
[00:39:18.060 --> 00:39:21.300]   like pure Python.
[00:39:21.300 --> 00:39:29.620]   And you know, are designed to work better and better with these back-end linear algebra
[00:39:29.620 --> 00:39:34.140]   accelerators and compilers.
[00:39:34.140 --> 00:39:39.420]   Is there some language out there right now that has that feel for you?
[00:39:39.420 --> 00:39:46.380]   No, they're all, they're all basically these embedded DSLs, you know, like TVM or like
[00:39:46.380 --> 00:39:49.020]   Halide.
[00:39:49.020 --> 00:39:57.100]   You know, we have the MLIR, you know, project, which is kind of providing the back-end needed
[00:39:57.100 --> 00:39:59.740]   for these kinds of things.
[00:39:59.740 --> 00:40:06.420]   And you know, Chris Latner has a new company, which, you know, presumably going to be placed
[00:40:06.420 --> 00:40:10.740]   better than any other to create what we need for this kind of thing.
[00:40:10.740 --> 00:40:13.980]   And so he's the guy behind MLIR.
[00:40:13.980 --> 00:40:20.660]   Yeah, but it feels like a big open area to me at the moment.
[00:40:20.660 --> 00:40:21.660]   Interesting.
[00:40:21.660 --> 00:40:27.020]   Okay, on a totally different topic that I kind of can't believe we didn't cover last
[00:40:27.020 --> 00:40:28.020]   time.
[00:40:28.020 --> 00:40:29.020]   I feel like we must have been right in the middle of it.
[00:40:29.020 --> 00:40:34.420]   You know, I think Guy, along with many other people in the world, sort of watched you kind
[00:40:34.420 --> 00:40:38.860]   of advocate for wearing masks in the early days of COVID.
[00:40:38.860 --> 00:40:42.980]   And you know, I think you had one of the most, I mean, some of the most like, you know, high-profile
[00:40:42.980 --> 00:40:47.100]   like articles on this, like the second most popular article on like free print.
[00:40:47.100 --> 00:40:53.860]   And I'm just kind of curious if you could sort of tell that story from your perspective
[00:40:53.860 --> 00:40:59.740]   and maybe, you know, like what you were seeing that other people were missing and how you're
[00:40:59.740 --> 00:41:02.260]   kind of approaching that problem differently.
[00:41:02.260 --> 00:41:07.140]   I mean, it's hard for me Lucas, because like, I don't understand why, and I still don't
[00:41:07.140 --> 00:41:14.420]   understand why it's not reasonably obvious to everybody, like what's everybody else missing
[00:41:14.420 --> 00:41:15.420]   and why?
[00:41:15.420 --> 00:41:18.980]   Because like, to me, from my point of view, well, okay, so like, let me go back.
[00:41:18.980 --> 00:41:28.680]   So February 2020, you know, mid-ish February 2020, like February 2020, I had a course coming
[00:41:28.680 --> 00:41:33.900]   up at the University of San Francisco that I was going to be teaching.
[00:41:33.900 --> 00:41:47.260]   And I had heard, you know, increasing chatter about this, whatever, Chinese virus thing.
[00:41:47.260 --> 00:41:53.660]   And I guess, you know, what then happened was it hit Italy and there was a lot more
[00:41:53.660 --> 00:41:57.020]   information in English about what was happening in Italy.
[00:41:57.020 --> 00:41:59.060]   And there was what was happening in China.
[00:41:59.060 --> 00:42:02.900]   So it suddenly was much more accessible to see what was going on, you know, particularly
[00:42:02.900 --> 00:42:06.420]   because a lot of the Italian doctors and whatever were actually on Twitter and stuff.
[00:42:06.420 --> 00:42:08.280]   So I could read what was happening.
[00:42:08.280 --> 00:42:13.020]   And that, you know, a whole bunch of people were saying like, you know, this is a disaster,
[00:42:13.020 --> 00:42:17.800]   you know, I can't remember what it was, the president of the, you know, main Italian medical
[00:42:17.800 --> 00:42:24.700]   body just died of COVID and, you know, there's not enough hospital beds.
[00:42:24.700 --> 00:42:29.860]   And then I knew it had kind of just, I think, starting to get detected in New York.
[00:42:29.860 --> 00:42:34.220]   And I thought, oh, well, it seems like it might be quite likely to come here.
[00:42:34.220 --> 00:42:35.580]   What does that mean for our course?
[00:42:35.580 --> 00:42:41.620]   You know, it's very, like, not at all altruistic, just like, are we still going to do our course?
[00:42:41.620 --> 00:42:45.860]   So my wife and I kind of started reading about it to try to figure out what should happen
[00:42:45.860 --> 00:42:47.980]   with the course.
[00:42:47.980 --> 00:42:52.060]   And as we did it, we were, yeah, it's like very obvious that it was going to be a global
[00:42:52.060 --> 00:42:58.060]   pandemic and it was going to sweep through San Francisco within weeks.
[00:42:58.060 --> 00:43:04.100]   And so like within two days, I guess I wrote an email to everybody who had registered,
[00:43:04.100 --> 00:43:09.180]   you know, I think registered for the course and put, you know, put out a blog post and
[00:43:09.180 --> 00:43:16.740]   said we're not doing the course live, we're going to do it virtually.
[00:43:16.740 --> 00:43:21.780]   This is well before, you know, our university or I think any university had decided to do
[00:43:21.780 --> 00:43:23.460]   that, which again, I already thought was weird.
[00:43:23.460 --> 00:43:28.820]   Like I thought like, okay, it's not yet here, but obviously it's going to be.
[00:43:28.820 --> 00:43:31.860]   So why are people acting as if it's not going to be?
[00:43:31.860 --> 00:43:35.980]   And so, yeah, Rachel and I ended up writing a long blog post, you know, because we were
[00:43:35.980 --> 00:43:38.180]   kind of like, okay, it's not just our course.
[00:43:38.180 --> 00:43:42.500]   It's like, we know we've got all these friends in San Francisco who are doing things that
[00:43:42.500 --> 00:43:45.220]   we're pretty sure they're going to look back on in hindsight and think that's a terrible
[00:43:45.220 --> 00:43:48.420]   idea because I put myself and my community at risk.
[00:43:48.420 --> 00:43:51.380]   And so we said like, okay, you know, we didn't know much about it.
[00:43:51.380 --> 00:43:56.380]   So we just said, look, as data scientists, here's what we can see so far in the data.
[00:43:56.380 --> 00:43:59.340]   You know, it does seem to grow exponentially, at least at first.
[00:43:59.340 --> 00:44:03.900]   And you know, this is the impact it's been having in Lombardy and here's the early impact
[00:44:03.900 --> 00:44:04.900]   in New York.
[00:44:04.900 --> 00:44:11.180]   And here's like how the math of these kinds of, you know, things work.
[00:44:11.180 --> 00:44:15.500]   And so here's like, not just a prediction, but an almost certainty as to what's going
[00:44:15.500 --> 00:44:17.220]   to happen here.
[00:44:17.220 --> 00:44:22.940]   And that got a lot of attention and we had no idea how to avoid it ourselves.
[00:44:22.940 --> 00:44:28.580]   We were worried that like, historically, you know, when there is global pandemics, it can
[00:44:28.580 --> 00:44:33.300]   lead to violence, it can lead to societal disharmony, whatever.
[00:44:33.300 --> 00:44:38.160]   So we decided to get out of San Francisco for a while.
[00:44:38.160 --> 00:44:40.620]   We also, it was clear that it was going to be, there was going to be a lockdown at some
[00:44:40.620 --> 00:44:44.500]   point because, you know, I mean, why wouldn't there be?
[00:44:44.500 --> 00:44:47.940]   Again, none of our friends seem to believe any of this is going to happen.
[00:44:47.940 --> 00:44:51.980]   It's really, I thought it was weird, you know, like it just seemed very obvious.
[00:44:51.980 --> 00:44:54.780]   And then yeah, there was a lockdown like a week or two later.
[00:44:54.780 --> 00:44:58.940]   We had told our daughter's school, like, oh, that's probably going to be a lockdown.
[00:44:58.940 --> 00:45:04.220]   They, you know, sent back this rather annoyed email about interrupting learning or something.
[00:45:04.220 --> 00:45:05.220]   Yeah.
[00:45:05.220 --> 00:45:09.980]   And so the schools were closed for a year in the end in San Francisco.
[00:45:09.980 --> 00:45:16.620]   So then we were like, oh, well, how do we, yeah, how do we like not get COVID?
[00:45:16.620 --> 00:45:19.260]   Because we probably don't want to get COVID because it seems like getting COVID can be
[00:45:19.260 --> 00:45:20.260]   bad.
[00:45:20.260 --> 00:45:23.380]   We started to hear from people who would like, you know, saying maybe there could be longer
[00:45:23.380 --> 00:45:27.220]   term implications of some of these kinds of SARS viruses.
[00:45:27.220 --> 00:45:30.580]   So I started looking into like how it was spread and I discovered that there's all these
[00:45:30.580 --> 00:45:36.060]   countries around China that had avoided getting hit by COVID and particularly Hong Kong.
[00:45:36.060 --> 00:45:43.420]   There's like literally a train line away from Wuhan and that just seems amazing, you know,
[00:45:43.420 --> 00:45:50.780]   and that's when I discovered that like Mongolia, Taiwan and Hong Kong all had this kind of,
[00:45:50.780 --> 00:45:58.500]   you know, either universal mask policy or universal mask usage kind of culturally.
[00:45:58.500 --> 00:46:01.860]   And I thought, oh, that's weird because I thought masks were this kind of like weird
[00:46:01.860 --> 00:46:05.300]   thing that, I don't know, for some reason you go to Chinatown, you see people wearing
[00:46:05.300 --> 00:46:07.620]   masks and I was like, that's weird.
[00:46:07.620 --> 00:46:10.340]   You know, I didn't really took much notice of it.
[00:46:10.340 --> 00:46:14.140]   But then as I, yeah, I started learning it was this respiratory, you know, infection
[00:46:14.140 --> 00:46:17.420]   and it's kind of seemed to make sense.
[00:46:17.420 --> 00:46:25.940]   And so I read something in the Washington Post talking about how in the Czech Republic,
[00:46:25.940 --> 00:46:31.660]   particularly the populace had independently decided to wear masks, you know, heavily driven
[00:46:31.660 --> 00:46:38.860]   by a kind of a popular science YouTuber.
[00:46:38.860 --> 00:46:42.860]   And basically, yeah, within like three or four days, you know, the whole country had
[00:46:42.860 --> 00:46:48.500]   made enough masks for everybody and their president was like talking about how proud
[00:46:48.500 --> 00:46:57.540]   he was and their, again, their like infection rate was going the opposite direction to other
[00:46:57.540 --> 00:46:58.540]   countries.
[00:46:58.540 --> 00:46:59.540]   I thought that was interesting.
[00:46:59.540 --> 00:47:02.440]   So yeah, I kind of read an article about that.
[00:47:02.440 --> 00:47:09.800]   And then I talked to a guy who used to be very, you know, high up in the government
[00:47:09.800 --> 00:47:11.240]   on the science policy side.
[00:47:11.240 --> 00:47:13.120]   And I asked him what's going on with masks.
[00:47:13.120 --> 00:47:18.940]   And he said like, well, you know, nobody thinks it's very convincing science about it.
[00:47:18.940 --> 00:47:22.660]   So he said, if you want to convince people to wear masks, then you'd need to, you know,
[00:47:22.660 --> 00:47:24.740]   find some better science.
[00:47:24.740 --> 00:47:33.060]   So I contacted basically the 18 smartest scientific researchers I knew.
[00:47:33.060 --> 00:47:40.540]   You know, everybody from Max Friedman to Zeynep Tufekci and said, you know, not just scientific
[00:47:40.540 --> 00:47:44.380]   researchers and in this case, sociological researcher that said like, you want to help
[00:47:44.380 --> 00:47:47.740]   me put together the evidence.
[00:47:47.740 --> 00:47:51.020]   So that's where our paper came from.
[00:47:51.020 --> 00:47:53.260]   Basically everybody said yes, they all agreed.
[00:47:53.260 --> 00:47:56.140]   So suddenly we had this huge author group.
[00:47:56.140 --> 00:48:02.940]   So we kind of set up a Slack channel and yeah, none of us like had a really strong opinion
[00:48:02.940 --> 00:48:03.940]   going in.
[00:48:03.940 --> 00:48:07.020]   I had one of the world's best aerosol scientists.
[00:48:07.020 --> 00:48:10.580]   He was probably the strongest opinion going in because this is his job.
[00:48:10.580 --> 00:48:15.620]   And he was like, well, let me explain aerosols to you.
[00:48:15.620 --> 00:48:20.700]   And then what happened was there was this amazing couple of papers that actually used
[00:48:20.700 --> 00:48:26.420]   this laser scattering light chamber thing to actually literally take videos of, you
[00:48:26.420 --> 00:48:30.740]   know, respiratory particles suspended in the air, not suspended, but just they just float
[00:48:30.740 --> 00:48:31.740]   in the air.
[00:48:31.740 --> 00:48:34.020]   It showed that they float in the air for up to an hour.
[00:48:34.020 --> 00:48:38.340]   And it showed that when somebody wears a mask, they don't appear.
[00:48:38.340 --> 00:48:42.980]   And that was the point where I went from like curious and interested to a hundred percent
[00:48:42.980 --> 00:48:43.980]   convinced.
[00:48:43.980 --> 00:48:48.140]   Because it'd be like, as somebody said, like, I promise you, Lucas, if you throw this ball
[00:48:48.140 --> 00:48:51.260]   at that wall, it won't bounce off.
[00:48:51.260 --> 00:48:53.100]   It will go through.
[00:48:53.100 --> 00:48:58.540]   And then you'd be like, well, Jeremy, I'm not sure, but I'll give it a go.
[00:48:58.540 --> 00:49:01.620]   You throw the ball at the wall and it bounces off.
[00:49:01.620 --> 00:49:06.860]   And you go like, Jeremy, I am very sure you're wrong about your theorem.
[00:49:06.860 --> 00:49:08.460]   And that's how it was with masks.
[00:49:08.460 --> 00:49:13.740]   There were people who said like, our masks don't provide respiratory protection from
[00:49:13.740 --> 00:49:16.380]   these airborne particles.
[00:49:16.380 --> 00:49:20.220]   And there is a video of them not going through the mask.
[00:49:20.220 --> 00:49:23.820]   So I was like, okay, that's, I don't need any RCTs.
[00:49:23.820 --> 00:49:24.820]   I don't like it.
[00:49:24.820 --> 00:49:28.420]   It's like, there's a video, it's a picture of it working.
[00:49:28.420 --> 00:49:32.940]   So yeah, so then I kind of went all in on just trying to say to people, oh, no, there's
[00:49:32.940 --> 00:49:38.020]   actually a thing that stops the thing that infects us.
[00:49:38.020 --> 00:49:41.300]   So we should wear them.
[00:49:41.300 --> 00:49:46.900]   And I found it extraordinarily bizarre that everybody didn't just go, oh, look at that
[00:49:46.900 --> 00:49:49.620]   video of it working.
[00:49:49.620 --> 00:49:53.180]   Therefore it works.
[00:49:53.180 --> 00:49:54.940]   So it was like a super frustrating experience.
[00:49:54.940 --> 00:49:59.300]   Like I don't, there's nothing I enjoy about researching masks and there's nothing I enjoy
[00:49:59.300 --> 00:50:01.100]   about political advocacy.
[00:50:01.100 --> 00:50:05.180]   You know, the former is boring and the latter is stressful.
[00:50:05.180 --> 00:50:11.900]   But when there's something that so obviously can like save millions of lives and also like
[00:50:11.900 --> 00:50:18.740]   can avoid who knows what long-term harm, it just seems absolutely ethically required to
[00:50:18.740 --> 00:50:20.660]   act on that.
[00:50:20.660 --> 00:50:28.420]   And so I, you know, spoke with all kinds of like world leaders and politicians and celebrities
[00:50:28.420 --> 00:50:32.100]   and whatever.
[00:50:32.100 --> 00:50:36.500]   And in every jurisdiction, it was like this, like a whole new conversation, you know, it's
[00:50:36.500 --> 00:50:38.340]   like talking to people in South Africa.
[00:50:38.340 --> 00:50:40.140]   And so I go, oh, we don't believe in masks.
[00:50:40.140 --> 00:50:41.900]   It's like, talk to people in London.
[00:50:41.900 --> 00:50:42.900]   We don't believe in masks.
[00:50:42.900 --> 00:50:43.900]   Talk to people in Australia.
[00:50:43.900 --> 00:50:44.900]   We don't believe in masks.
[00:50:44.900 --> 00:50:45.900]   Talk to people in Florida.
[00:50:45.900 --> 00:50:46.900]   We don't believe in masks.
[00:50:46.900 --> 00:50:53.580]   And like each one, I discovered this horrible thing, which is everybody decided they didn't
[00:50:53.580 --> 00:51:00.020]   believe in masks until their personal jurisdiction got hit hard by COVID, until the hospital
[00:51:00.020 --> 00:51:01.700]   started filling up.
[00:51:01.700 --> 00:51:06.660]   And then they would get back to me and say like, oh, tell me more about this masks thing,
[00:51:06.660 --> 00:51:07.740]   Jeremy.
[00:51:07.740 --> 00:51:13.420]   And that was like infuriating because of course the answer is, well, if you had of put in
[00:51:13.420 --> 00:51:16.420]   mask mandates two months ago, then this wouldn't have happened.
[00:51:16.420 --> 00:51:24.060]   And now it's too late because masks can reduce R by a bit, but not enough to reverse a full-on
[00:51:24.060 --> 00:51:26.940]   pandemic once it's there.
[00:51:26.940 --> 00:51:33.060]   So honestly, I got really burned out by the process.
[00:51:33.060 --> 00:51:37.960]   It was like, in some ways it was successful, but in the end the pandemic still happened.
[00:51:37.960 --> 00:51:46.420]   And in the end I'm still flabbergasted, particularly now that high quality medical masks are widely
[00:51:46.420 --> 00:51:48.220]   available.
[00:51:48.220 --> 00:51:53.980]   Demand is so low that factories have been shutting down.
[00:51:53.980 --> 00:52:00.380]   So yeah, I've never had COVID, like literally nobody I know who has worn a high quality
[00:52:00.380 --> 00:52:03.740]   mask at all times indoors.
[00:52:03.740 --> 00:52:08.340]   None of them have got COVID, and everybody I know who doesn't have all had COVID.
[00:52:08.340 --> 00:52:14.420]   There's a point at which you just kind of say like, okay, I've done what I can, you
[00:52:14.420 --> 00:52:16.620]   do you.
[00:52:16.620 --> 00:52:19.980]   So you continue to wear a mask indoors at all times?
[00:52:19.980 --> 00:52:20.980]   Of course.
[00:52:20.980 --> 00:52:26.940]   So I guess what would change, when would you stop wearing a mask indoors?
[00:52:26.940 --> 00:52:32.060]   I mean, I suspect like the same as the answer to the question, when would I stop drinking
[00:52:32.060 --> 00:52:33.300]   clean water?
[00:52:33.300 --> 00:52:38.460]   I'd rather keep drinking clean water.
[00:52:38.460 --> 00:52:47.620]   We decided, I mean, remember it took decades, even after the Jon Snow experiment, for big
[00:52:47.620 --> 00:52:52.140]   cities to decide to invest in clean water infrastructure.
[00:52:52.140 --> 00:52:57.180]   So presumably after some number of years, we will invest in clear air infrastructure.
[00:52:57.180 --> 00:52:58.460]   So China's already done it.
[00:52:58.460 --> 00:53:03.460]   They now have, I believe HIPAA filters in pretty much all their public buildings, and
[00:53:03.460 --> 00:53:08.180]   they're putting in UV sterilization in pretty much all their public buildings.
[00:53:08.180 --> 00:53:11.980]   So hopefully at some point the West will do the same thing, and then it'll be like, okay,
[00:53:11.980 --> 00:53:14.740]   I'm in an environment with clean air.
[00:53:14.740 --> 00:53:18.380]   So I don't have to like self-clean the air.
[00:53:18.380 --> 00:53:20.100]   So that'd be one option.
[00:53:20.100 --> 00:53:22.020]   Another would be, again, China's ahead of us on this.
[00:53:22.020 --> 00:53:28.500]   They have nasal vaccines, which, you know, probably much more effective.
[00:53:28.500 --> 00:53:34.980]   So if we eventually get those, and you know, I think they can actually make a significant
[00:53:34.980 --> 00:53:42.420]   dent on transmission, which the injected vaccines don't make much of a big impact on transmission.
[00:53:42.420 --> 00:53:47.780]   So yeah, there are technologies that should allow us to be able to be, I think, pretty
[00:53:47.780 --> 00:53:52.420]   safe in indoor spaces.
[00:53:52.420 --> 00:53:55.780]   But you don't wear masks in an outdoor space?
[00:53:55.780 --> 00:53:56.780]   Is that the...
[00:53:56.780 --> 00:54:00.340]   No, I mean, it's not exactly a hard and fast rule.
[00:54:00.340 --> 00:54:04.300]   We went to a birthday party recently, for example, where it was like a karaoke thing,
[00:54:04.300 --> 00:54:08.100]   and it was outdoors, but all the kids were singing and they were tightly packed and whatever.
[00:54:08.100 --> 00:54:13.300]   So our family wore a mask because there's a high amount of aerosolizing activities going
[00:54:13.300 --> 00:54:15.660]   on with a high density of people.
[00:54:15.660 --> 00:54:24.680]   But yeah, broadly speaking, I'm not too concerned about outdoors because the airborne particles
[00:54:24.680 --> 00:54:26.660]   disperse much more quickly.
[00:54:26.660 --> 00:54:27.660]   I see.
[00:54:27.660 --> 00:54:32.300]   So I guess the interesting thing about that story maybe is that there maybe was a fairly
[00:54:32.300 --> 00:54:39.460]   broad scientific consensus, but no one was really ready to advocate for it.
[00:54:39.460 --> 00:54:41.500]   Is that a better summary of what was happening?
[00:54:41.500 --> 00:54:46.020]   If you got all these scientists together and they actually all agreed with what you're
[00:54:46.020 --> 00:54:47.020]   saying?
[00:54:47.020 --> 00:54:48.500]   They didn't, unfortunately.
[00:54:48.500 --> 00:54:52.140]   What happened was it was highly polarized by area.
[00:54:52.140 --> 00:54:55.420]   So the people that actually understood this are the aerosol scientists.
[00:54:55.420 --> 00:55:03.500]   And the aerosol science community was basically 100% all on the same page of like, okay, you
[00:55:03.500 --> 00:55:08.360]   know, talking, breathing, these aerosolizing activities.
[00:55:08.360 --> 00:55:12.940]   We have loads of evidence that this is transmitted through aerosols.
[00:55:12.940 --> 00:55:20.100]   We have loads of evidence that in the droplet nuclei, you know, that are suspended in the
[00:55:20.100 --> 00:55:24.500]   air, masks block those from getting into your lungs.
[00:55:24.500 --> 00:55:28.500]   Like all those were pretty much understood in that community.
[00:55:28.500 --> 00:55:35.900]   But then the challenge is, Lucas, that we haven't had a major respiratory pandemic in
[00:55:35.900 --> 00:55:39.580]   the West really since the Spanish flu.
[00:55:39.580 --> 00:55:44.700]   So none of our infectious disease community has any background in that.
[00:55:44.700 --> 00:55:52.120]   So I spent a lot of time advocating, you know, including speaking directly to the WHO's infection
[00:55:52.120 --> 00:55:56.940]   control groups or the folks that, you know, kind of ran the response at the WHO.
[00:55:56.940 --> 00:56:02.860]   And they were overwhelmingly people who had a background in infectious diseases that were
[00:56:02.860 --> 00:56:08.860]   spread through contact, you know, the kind of stuff that hand washing helps with.
[00:56:08.860 --> 00:56:15.620]   So they were just coming from a totally different direction and had decades of experience on
[00:56:15.620 --> 00:56:19.440]   treating different kinds of diseases in a different way.
[00:56:19.440 --> 00:56:22.720]   And they were doing their best to learn and understand.
[00:56:22.720 --> 00:56:25.400]   But for some, that was a very difficult experience.
[00:56:25.400 --> 00:56:30.760]   And one in particular, John Connolly, like his financial stake was very high in this
[00:56:30.760 --> 00:56:33.960]   fomite, you know, transfer.
[00:56:33.960 --> 00:56:38.760]   This transmission is not through the air, but by contact because, you know, he has financial
[00:56:38.760 --> 00:56:42.160]   interests in that being the case.
[00:56:42.160 --> 00:56:47.840]   So very difficult for him to come to terms with the idea that this is a respiratory infection
[00:56:47.840 --> 00:56:52.000]   through respiratory particles requiring respiratory protection.
[00:56:52.000 --> 00:56:57.920]   So yeah, that was a big challenge is this worldview difference between different scientific
[00:56:57.920 --> 00:57:05.800]   groups and the aerosol scientists, there were actually none of them on the WHO's infection
[00:57:05.800 --> 00:57:10.560]   protection committee, you know, infection control, whatever it was.
[00:57:10.560 --> 00:57:17.280]   So I noticed when I was talking to WHO, it was a total lack of diversity.
[00:57:17.280 --> 00:57:21.320]   Every single one had the same kind of academic background and the same way of thinking about
[00:57:21.320 --> 00:57:23.560]   things and they all knew each other very well.
[00:57:23.560 --> 00:57:33.240]   And they also all saw being involved in the WHO as being a very strong status signal in
[00:57:33.240 --> 00:57:34.320]   their career.
[00:57:34.320 --> 00:57:37.000]   So everybody wants to be invited to those kinds of things.
[00:57:37.000 --> 00:57:42.200]   And so you really want to like have all the other people on the committee think you're
[00:57:42.200 --> 00:57:44.000]   a good, nice person.
[00:57:44.000 --> 00:57:47.280]   And so it creates this real monoculture.
[00:57:47.280 --> 00:57:49.480]   So that was another big part of the problem.
[00:57:49.480 --> 00:57:55.760]   And it was all like, it definitely made me a lot more cynical than I was before to see
[00:57:55.760 --> 00:58:01.160]   like how the WHO works and even like our big paper, like how to get it published.
[00:58:01.160 --> 00:58:04.360]   It took a year from being written to being published.
[00:58:04.360 --> 00:58:09.480]   So by the time it was published, it was basically too late.
[00:58:09.480 --> 00:58:18.560]   And the process of getting it published was much more about politics than about science.
[00:58:18.560 --> 00:58:23.800]   And it was disappointing for me to discover that systems that I had thought of as being
[00:58:23.800 --> 00:58:33.440]   like very much focused on rationality and data and correctness and rigor.
[00:58:33.440 --> 00:58:38.200]   So much of it turned out to be about politics and networks and stuff.
[00:58:38.200 --> 00:58:41.840]   So I guess I was probably pretty naive before all that happened.
[00:58:41.840 --> 00:58:48.920]   I mean, I guess my sense is that people broadly believe that masks reduce the spread of COVID
[00:58:48.920 --> 00:58:49.920]   at this point.
[00:58:49.920 --> 00:58:53.280]   I mean, I'm not sure that I know like exactly to what degree.
[00:58:53.280 --> 00:58:59.920]   It sounds like you're saying to like a really massive degree, but I think you had a part
[00:58:59.920 --> 00:59:00.920]   in that.
[00:59:00.920 --> 00:59:05.520]   Or maybe just, I just follow you on Twitter and we're just watching you talk about it.
[00:59:05.520 --> 00:59:06.520]   But I don't know.
[00:59:06.520 --> 00:59:08.360]   It does seem like it's mainstream.
[00:59:08.360 --> 00:59:13.840]   I was leading the masks are all group globally, like, and we were the most substantive group
[00:59:13.840 --> 00:59:14.840]   doing that.
[00:59:14.840 --> 00:59:15.840]   Absolutely.
[00:59:15.840 --> 00:59:17.040]   I mean, it feels like it was successful though.
[00:59:17.040 --> 00:59:20.200]   I mean, I just do not.
[00:59:20.200 --> 00:59:21.200]   Successful ish.
[00:59:21.200 --> 00:59:26.320]   Like, I think it's, I mean, if you're in San Francisco, it'll look more successful than
[00:59:26.320 --> 00:59:30.840]   if you're in Australia, for example, in Australia, from time to time, we've had mask mandates
[00:59:30.840 --> 00:59:32.600]   and everybody wears them when they're told to.
[00:59:32.600 --> 00:59:36.080]   The rest of the time it's strongly recommended, but nobody does.
[00:59:36.080 --> 00:59:41.560]   But like at San Francisco, I'm told like, I don't know, maybe 30% of kicks at schools
[00:59:41.560 --> 00:59:44.040]   or some schools are wearing them.
[00:59:44.040 --> 00:59:49.080]   Like it's definitely like it's disappearing.
[00:59:49.080 --> 00:59:55.760]   And also like people on a lot of people, maybe most people I see wearing masks, at least
[00:59:55.760 --> 00:59:58.920]   in Australia are wearing masks that don't work very well, even though they're the good
[00:59:58.920 --> 01:00:02.400]   masks are really easy to get.
[01:00:02.400 --> 01:00:08.480]   And a lot of people don't realize like, Oh, if you get a high quality and 95 respirator,
[01:00:08.480 --> 01:00:12.160]   you can wear that as many times as you like until the straps wear out.
[01:00:12.160 --> 01:00:14.560]   You know, a lot of people think, Oh, you can only wear it once.
[01:00:14.560 --> 01:00:16.840]   A lot of people think it has to be fit tested.
[01:00:16.840 --> 01:00:20.880]   Like there's a lot of people think it's like donning and doffing is some complicated thing.
[01:00:20.880 --> 01:00:21.880]   Yeah.
[01:00:21.880 --> 01:00:25.880]   There's all this like wrong information out there.
[01:00:25.880 --> 01:00:33.720]   And so the number of people actually wearing high quality masks is to me, it's surprisingly
[01:00:33.720 --> 01:00:34.720]   low.
[01:00:34.720 --> 01:00:38.440]   Like if everybody wore one whenever they were indoors, you know, particularly if we also
[01:00:38.440 --> 01:00:42.320]   had HIPAA filters in indoor spaces, I suspect we would be done with the virus.
[01:00:42.320 --> 01:00:47.600]   I think it would go away because how would a respiratory virus continue to transmit when
[01:00:47.600 --> 01:00:51.080]   you break the flow of respiratory particles?
[01:00:51.080 --> 01:00:52.080]   Yeah.
[01:00:52.080 --> 01:00:57.920]   I mean, even in China, like every other pictures I see, everybody's wearing surgical masks.
[01:00:57.920 --> 01:01:01.440]   Just like weird to me.
[01:01:01.440 --> 01:01:02.440]   Interesting.
[01:01:02.440 --> 01:01:07.520]   Well, look, we're almost out of time and we always end with two questions, but you're
[01:01:07.520 --> 01:01:08.720]   a little bit of an unusual guest.
[01:01:08.720 --> 01:01:15.440]   I don't know exactly how well these will fit your worldview, but I like to ask people if
[01:01:15.440 --> 01:01:20.720]   you had some extra time to research something completely different, what might it be?
[01:01:20.720 --> 01:01:26.280]   And I feel like you are just like an unending font of this stuff.
[01:01:26.280 --> 01:01:29.760]   What are some things that you're interested in that you haven't had time to look into?
[01:01:29.760 --> 01:01:33.960]   Well, I'll answer a slightly different question because like anytime I'm interested in researching
[01:01:33.960 --> 01:01:34.960]   something I just do.
[01:01:34.960 --> 01:01:35.960]   Fair enough.
[01:01:35.960 --> 01:01:44.960]   The most recent thing I spent a lot of time researching is children's education.
[01:01:44.960 --> 01:01:52.640]   So our daughter missed the first year of school because of COVID in San Francisco, they were
[01:01:52.640 --> 01:01:53.640]   closed.
[01:01:53.640 --> 01:01:57.280]   That would have been her kind of transitional kindergarten year, as they call it in California.
[01:01:57.280 --> 01:02:02.960]   And then we came to Australia and so she went to school, regular school for the first year
[01:02:02.960 --> 01:02:03.960]   here.
[01:02:03.960 --> 01:02:10.320]   She was straight into grade one and she enjoyed it.
[01:02:10.320 --> 01:02:18.400]   She was always happy to go and happy to stay there, but it felt like she had like blossomed
[01:02:18.400 --> 01:02:23.800]   a lot more during her previous year when she was doing stuff over Zoom and on apps and
[01:02:23.800 --> 01:02:29.840]   stuff than the year that she was in person in the classroom, which really surprised me.
[01:02:29.840 --> 01:02:34.480]   And instead she had become much more of a perfectionist and was becoming like much less
[01:02:34.480 --> 01:02:37.680]   resilient after her year at physical school.
[01:02:37.680 --> 01:02:42.640]   And that all seemed really weird to me because I thought that environment would be much more
[01:02:42.640 --> 01:02:45.880]   healthy than the previous one.
[01:02:45.880 --> 01:02:49.140]   So I started, yeah, I started investigating it really carefully and studying a lot of
[01:02:49.140 --> 01:02:51.000]   academic papers about education.
[01:02:51.000 --> 01:02:59.040]   And I was stunned to discover that, yeah, there's kind of like pretty broad consensus
[01:02:59.040 --> 01:03:04.200]   in parts of the academic community or some very strong data that suggests like schools
[01:03:04.200 --> 01:03:10.600]   are not a particularly great place for most kids to really blossom or at least entirely
[01:03:10.600 --> 01:03:12.080]   focus on school learning.
[01:03:12.080 --> 01:03:22.460]   And in fact, tutoring, kids that do tutoring, get tutoring are like in the very top highest
[01:03:22.460 --> 01:03:25.880]   academic performers, regardless of their previous background.
[01:03:25.880 --> 01:03:31.000]   Like it seems like all kids can be really successful given the right tutoring.
[01:03:31.000 --> 01:03:36.600]   And because our daughter was doing all this stuff with like apps and on Zoom and stuff
[01:03:36.600 --> 01:03:43.160]   during her first year, none of that is limited by the speed at which a teacher thinks a kid
[01:03:43.160 --> 01:03:48.080]   should go, but instead the computer is dynamically adjusting difficulty over time.
[01:03:48.080 --> 01:03:54.120]   So weirdly enough, our daughter did, you know, was basically at grade four or grade five
[01:03:54.120 --> 01:03:58.420]   of math after a few months of doing these apps, you know, they're so much more effective
[01:03:58.420 --> 01:04:01.280]   than normal teaching.
[01:04:01.280 --> 01:04:04.680]   So we're also trying to figure out like, well, how do you avoid her getting really
[01:04:04.680 --> 01:04:05.760]   bored and stuff?
[01:04:05.760 --> 01:04:10.580]   So yeah, so I did this really deep dive into education and discovered there's all these
[01:04:10.580 --> 01:04:15.760]   like fascinating different ways of teaching and learning, which are entirely different
[01:04:15.760 --> 01:04:18.520]   to what's done at normal schools.
[01:04:18.520 --> 01:04:24.300]   So eventually, yeah, we decided to take her out of school and, you know, instead switch
[01:04:24.300 --> 01:04:31.520]   to using these kind of more academically driven approaches in the homeschooling environment,
[01:04:31.520 --> 01:04:38.520]   which also seem to generally lead to better social outcomes, you know, better mental outcomes,
[01:04:38.520 --> 01:04:42.760]   better mental health outcomes, and better learning outcomes.
[01:04:42.760 --> 01:04:48.480]   And so again, that's kind of been interesting to me to discover this like whole world of
[01:04:48.480 --> 01:04:58.920]   research that seems really important, you know, for humanity, how kids should learn.
[01:04:58.920 --> 01:05:05.080]   And yeah, it feels like again, it's being largely ignored by the institutions that we
[01:05:05.080 --> 01:05:08.320]   send us kids to.
[01:05:08.320 --> 01:05:12.000]   And so wait, let me just summarize and see if I got the summary of this.
[01:05:12.000 --> 01:05:17.860]   Basically that tutors are much more effective than schools at actually teaching kids things
[01:05:17.860 --> 01:05:20.360]   is that what you're saying?
[01:05:20.360 --> 01:05:21.360]   That would be part of it.
[01:05:21.360 --> 01:05:27.680]   I mean, and then specific, but I mean, there's lots of, so yeah, that's kind of one starting
[01:05:27.680 --> 01:05:33.440]   point is like, yes, even kids that would otherwise have been doing pretty badly at school can
[01:05:33.440 --> 01:05:36.660]   be in the very top performers.
[01:05:36.660 --> 01:05:41.720]   So like that kind of is an existence proof that pretty much all kids can be extremely
[01:05:41.720 --> 01:05:46.060]   successful.
[01:05:46.060 --> 01:05:50.400]   But then there's this also this kind of, yeah, interesting data point for us, which is when
[01:05:50.400 --> 01:05:58.480]   we kind of gave our daughter an iPad and some, you know, math and reading apps and somebody
[01:05:58.480 --> 01:06:01.220]   on the other end of a Zoom to supervise them.
[01:06:01.220 --> 01:06:08.080]   She had a huge amount of fun and learned dramatically more quickly than I thought was possible.
[01:06:08.080 --> 01:06:11.680]   And then when she actually went to school, she basically learned nothing for the whole
[01:06:11.680 --> 01:06:16.120]   year and ended up becoming much less resilient.
[01:06:16.120 --> 01:06:21.680]   And then that, yeah, that there are specific ways of learning that are not particularly
[01:06:21.680 --> 01:06:24.480]   compatible with the normal ways we teach at school.
[01:06:24.480 --> 01:06:31.760]   So for example, we might've talked before about like Anki and repetitive spaced learning,
[01:06:31.760 --> 01:06:34.420]   you know, so my daughter does Anki every day.
[01:06:34.420 --> 01:06:38.680]   So like literally everything she learns, she will remember forever.
[01:06:38.680 --> 01:06:43.520]   If she creates a card for it, if she decides she wants to know it.
[01:06:43.520 --> 01:06:49.560]   So yeah, so like it's, and that's kind of quite difficult to do at a normal school because
[01:06:49.560 --> 01:06:53.680]   you did all of your grade levels to be doing Anki so that in grade five, you're still got
[01:06:53.680 --> 01:06:57.920]   cards from grade one or grade two coming back.
[01:06:57.920 --> 01:07:01.920]   But what happens at school is like each year, so for example, in Australia, the year seven
[01:07:01.920 --> 01:07:08.320]   and year eight math curriculums are nearly entirely a refresh of the primary school curriculum
[01:07:08.320 --> 01:07:11.960]   because they kind of assume the kids are going to need to see it again because they've probably
[01:07:11.960 --> 01:07:13.560]   forgotten a lot of it.
[01:07:13.560 --> 01:07:16.840]   Things like how would you incorporate spaced repetitive learning.
[01:07:16.840 --> 01:07:20.160]   Some schools in England have tried to do something like that using something they call retrieval
[01:07:20.160 --> 01:07:21.160]   practice.
[01:07:21.160 --> 01:07:26.940]   And so I know there's a school called the Michaela school, which I believe had the highest
[01:07:26.940 --> 01:07:29.480]   results academically in the whole country.
[01:07:29.480 --> 01:07:31.000]   They do something like this.
[01:07:31.000 --> 01:07:37.400]   So there's a few, there's a handful of schools here and there, which are trying to use these
[01:07:37.400 --> 01:07:41.960]   kind of research results, but they're, yeah, they're kind of the odd ones out.
[01:07:41.960 --> 01:07:42.960]   All right.
[01:07:42.960 --> 01:07:45.560]   And I guess like, finally, I don't know if this one really applies to you.
[01:07:45.560 --> 01:07:49.960]   We usually ask because, you know, my company in this interview is all about sort of like
[01:07:49.960 --> 01:07:53.360]   making machine learning really work in the real world.
[01:07:53.360 --> 01:07:57.320]   We usually ask like, kind of what's a hard part that you've encountered in sort of like
[01:07:57.320 --> 01:08:02.720]   taking something from research to actually working for some purpose.
[01:08:02.720 --> 01:08:06.040]   And that may not exactly apply to you, but you seem very good at sort of interpreting
[01:08:06.040 --> 01:08:07.560]   my questions in a useful way.
[01:08:07.560 --> 01:08:11.160]   So I pose it in its most abstract form.
[01:08:11.160 --> 01:08:19.400]   I mean, I've, yeah, I've had lots of projects that I've tried to bring into the real world.
[01:08:19.400 --> 01:08:20.400]   Of course, of course.
[01:08:20.400 --> 01:08:21.400]   That's right.
[01:08:21.400 --> 01:08:22.400]   Yeah.
[01:08:22.400 --> 01:08:25.720]   So, and it's difficult, you know.
[01:08:25.720 --> 01:08:26.720]   Yeah.
[01:08:26.720 --> 01:08:35.480]   I mean, I've been doing machine learning projects for over 25 years now, believe it or not.
[01:08:35.480 --> 01:08:41.480]   And in the early days, you know, it was such a challenge because managers didn't believe
[01:08:41.480 --> 01:08:43.520]   in the power of data at all.
[01:08:43.520 --> 01:08:46.480]   And when I would try to tell them that it could be really valuable, they would always
[01:08:46.480 --> 01:08:50.800]   say like, can you point to a role model of a company that's been successful because of
[01:08:50.800 --> 01:08:52.320]   their use of data?
[01:08:52.320 --> 01:08:55.840]   And there were none, you know, and that was tough.
[01:08:55.840 --> 01:08:56.840]   Yeah.
[01:08:56.840 --> 01:09:01.960]   And then Google came along, which was great, you know, because then I could point at this
[01:09:01.960 --> 01:09:07.640]   one company that was like really working hard to use data and they've become very valuable
[01:09:07.640 --> 01:09:08.640]   because of it.
[01:09:08.640 --> 01:09:11.880]   Nowadays, that bit's a lot easier.
[01:09:11.880 --> 01:09:19.400]   But actually, unfortunately, my answer is going to be that I've kind of, for a lot of
[01:09:19.400 --> 01:09:23.740]   companies I've given up on even trying, because I tried to get, particularly when I was at
[01:09:23.740 --> 01:09:30.000]   Singularity University, where all of our students were basically execs from giant companies,
[01:09:30.000 --> 01:09:33.000]   and we were trying to convince them to be more data focused.
[01:09:33.000 --> 01:09:35.440]   And some of them really took that on board.
[01:09:35.440 --> 01:09:41.920]   And then they would like invite me to come and talk to their VP groups and exec groups.
[01:09:41.920 --> 01:09:50.840]   And I saw lots of big companies try to get more data driven, try to use machine learning.
[01:09:50.840 --> 01:09:53.640]   I didn't see any being successful.
[01:09:53.640 --> 01:10:03.280]   And that the issue seemed to be that their entire management teams were people who, that
[01:10:03.280 --> 01:10:05.160]   was not their area of expertise.
[01:10:05.160 --> 01:10:10.440]   They were not promoted because they were good at that.
[01:10:10.440 --> 01:10:16.840]   They would have very smart data driven people down in their kind of business analyst levels,
[01:10:16.840 --> 01:10:20.300]   that they would have no idea which ones knew what they were talking about and have no way
[01:10:20.300 --> 01:10:24.160]   to kind of curate what they were being told.
[01:10:24.160 --> 01:10:32.080]   All of the promotion systems were based on experience and credentialing and things other
[01:10:32.080 --> 01:10:35.240]   than analytical capabilities.
[01:10:35.240 --> 01:10:41.560]   So yeah, so like in those kinds of companies, I eventually decided like, okay, maybe it's
[01:10:41.560 --> 01:10:46.000]   not possible for a legacy company to become a data driven company.
[01:10:46.000 --> 01:10:53.540]   And so nowadays I focused all of my attention on startups created by founders that are already
[01:10:53.540 --> 01:10:59.360]   data driven and have a good understanding of analysis.
[01:10:59.360 --> 01:11:04.160]   And what we're seeing is like increasingly the most valuable companies, particularly
[01:11:04.160 --> 01:11:09.800]   the most valuable companies in America, they're basically all now tech startups.
[01:11:09.800 --> 01:11:13.440]   I mean, they're not startups anymore, but they're all companies that are created by
[01:11:13.440 --> 01:11:18.600]   kind of engineers and data driven people.
[01:11:18.600 --> 01:11:24.600]   So I'd kind of, yeah, I think for like data scientists interested in making an impact,
[01:11:24.600 --> 01:11:28.400]   the best thing to do would be to try and make sure you're at a company where that kind of
[01:11:28.400 --> 01:11:32.680]   work is appreciated and understood by the executive team.
[01:11:32.680 --> 01:11:34.280]   Interesting.
[01:11:34.280 --> 01:11:38.480]   Well, great to talk to you.
[01:11:38.480 --> 01:11:39.480]   That was super fun.
[01:11:39.480 --> 01:11:43.000]   Thanks for answering my wide range of questions.
[01:11:43.000 --> 01:11:44.280]   It's always so inspiring to talk to you.
[01:11:44.280 --> 01:11:45.280]   I really appreciate it.
[01:11:45.280 --> 01:11:46.520]   Well, thank you.
[01:11:46.520 --> 01:11:50.920]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:11:50.920 --> 01:11:55.640]   to the show notes in the description where you can find links to all the papers that
[01:11:55.640 --> 01:12:00.040]   are mentioned, supplemental material, and a transcription that we work really hard to
[01:12:00.040 --> 01:12:01.040]   produce.
[01:12:01.040 --> 01:12:03.240]   So check it out.
[01:12:03.240 --> 01:12:04.920]   And how is everything going at Weights & Biases?
[01:12:04.920 --> 01:12:07.640]   I always hear nothing but good things about it.
[01:12:07.640 --> 01:12:08.640]   Everybody loves it.
[01:12:08.640 --> 01:12:13.120]   I got to admit, actually, the other day I was like talking to my friend, I think it
[01:12:13.120 --> 01:12:17.400]   was Tanishq about like, oh, what's going on with this learning rate here?
[01:12:17.400 --> 01:12:19.440]   I wonder if it's working properly.
[01:12:19.440 --> 01:12:21.880]   And then he's like, oh, well, here's a graph of the learning rate.
[01:12:21.880 --> 01:12:23.920]   I was like, oh, that was quick and great.
[01:12:23.920 --> 01:12:24.920]   Where did that come from?
[01:12:24.920 --> 01:12:25.920]   And he's like, Weights & Biases.
[01:12:25.920 --> 01:12:26.920]   He logs it.
[01:12:26.920 --> 01:12:27.920]   Oh man, are you still recording?
[01:12:27.920 --> 01:12:28.920]   Put that on the...
[01:12:28.920 --> 01:12:35.840]   I probably should have looked at the Weights & Biases team.
[01:12:35.840 --> 01:12:42.200]   I was with like, plot dot plot x equals.
[01:12:42.200 --> 01:12:45.160]   He's already got it pasted into the Discord chat.
[01:12:45.160 --> 01:12:46.160]   All right.
[01:12:46.160 --> 01:12:47.160]   Well, that'd be my day.
[01:12:47.160 --> 01:12:48.160]   Thanks.
[01:12:48.160 --> 01:12:48.160]   Cheers, mate.
[01:12:48.160 --> 01:12:53.160]   Cheers.
[01:12:53.160 --> 01:12:55.740]   (upbeat music)
[01:12:55.740 --> 01:12:57.140]   (upbeat music)

