
[00:00:00.000 --> 00:00:08.080]   Really happy to be here. I'm talking about work that I've been doing with Jacob Andreas who is in the language intelligence group at MIT.
[00:00:08.080 --> 00:00:20.000]   And I personally am a third year CS PhD students at Stanford working with Stanford NLP and Noah Goodman. So the way that we're going to
[00:00:20.840 --> 00:00:28.800]   begin this talk is by looking at kind of our favorite deep model, right, which we're going to call M. It's some sort of black box and we don't really know how it works. And the way M
[00:00:28.800 --> 00:00:45.280]   processes information is by taking some sort of input X and then producing some sort of output Y. So the kinds of tasks that we care about in machine learning are, for example, image processing tasks like scene classification or natural language processing tasks like natural language inference.
[00:00:48.200 --> 00:00:54.840]   The challenge, you know, in machine learning is really identifying what exactly is going on in this model and how does it actually learn to solve the task at hand.
[00:00:54.840 --> 00:01:03.080]   Right. So if we take a closer look, we find that most deep models take some high dimensional input X and then transform it into some lower dimensional space, which we'll call theta.
[00:01:03.080 --> 00:01:16.800]   And the goal of model interpretability is given this representation, theta, what kind of information does a representation encode and does that information closely aligned with human intuitions about the kind of information that model should encode.
[00:01:17.800 --> 00:01:26.840]   So one of the most popular kind of approaches to analyzing the information inside learned deep representations is what I'm going to call representation level analysis or probing.
[00:01:26.840 --> 00:01:36.440]   Which started, you know, very recently, I think. And the basic idea is we take some model which learns representations. Let's imagine here it's for some sort of machine translation task.
[00:01:36.880 --> 00:01:48.720]   And then we train a slightly smaller supervised model called a probe that goes in the representation and tries to predict some sort of property of the inputs. In this case, let's imagine it's trying to predict the part of speech of the word dog.
[00:01:48.720 --> 00:02:00.840]   And if this probe ends up getting high accuracy on this task, then we can then claim that the representation theta encodes information about the part of speech because the probe is able to use those features to predict the property of interest.
[00:02:02.520 --> 00:02:08.000]   So this has been, I think, enormously influential for model interpretability, but there's one primary issue with this kind of work,
[00:02:08.000 --> 00:02:22.480]   which is that there's been a recent debate in the interpretability literature about whether the success of a probe means that the information is encoded in the representation, or rather that we just have a very powerful probe that has memorized the task. And so this has been kind of a back and forth debate.
[00:02:24.800 --> 00:02:33.600]   So instead, I want to advocate for an alternative way of analyzing representations. And this is by analyzing the individual features or neurons of deep representations.
[00:02:33.600 --> 00:02:42.000]   And so analyzing individual neurons, of course, has some advantages in that we can't detect concepts that are distributed across multiple neurons,
[00:02:42.000 --> 00:02:53.720]   but it has several advantages. First, analyzing neurons allows us to measure the extent to which representations are disentangled or decomposed into individual concepts that lie along individual features.
[00:02:53.840 --> 00:03:06.760]   Also, because we've greatly reduced the complexity of the behavior we're looking at, we don't do any sort of transformation, we don't do any sort of supervised learning, we're only inspecting kind of surface level behavior of a neuron, and we avoid those past debates about how complex probing methods should be.
[00:03:06.760 --> 00:03:14.320]   So analyzing individual neurons has also seen a lot of interest in the interpretability literature recently.
[00:03:14.880 --> 00:03:25.280]   Part of it started from this great work by David Bao, also at MIT, called NetDissect. And we're going to describe this method in detail because we build off of this basic idea. And it's also been applied in NLP as well.
[00:03:25.280 --> 00:03:31.480]   But I think one of the fundamental limitations of the existing work on interpretability so far is the following.
[00:03:31.480 --> 00:03:37.480]   So imagine we're trying to explain a neuron in some sort of vision network for seeing classification.
[00:03:37.800 --> 00:03:44.120]   And the way we might do so, and the way NetDissect proposes to do this, is by looking at the images that most maximally activate the neuron.
[00:03:44.120 --> 00:03:59.160]   So in this case, we have four images and the neuron is active in those highlighted regions. And if we take a look at these four images, it becomes obvious to us that it seems like this neuron is detecting ball rings. And this is the explanation that NetDissect assigns this neuron.
[00:04:00.840 --> 00:04:06.000]   However, if we begin to look at the other images for which this neuron activates, we see a very different story.
[00:04:06.000 --> 00:04:14.320]   Right, so now it's clear that this neuron is not just firing for ball rings, but rather for a bunch of different kinds of sports fields, baseball fields and whatnot.
[00:04:14.320 --> 00:04:24.520]   And in so reality, you know, to understand what this neuron is doing, we really need a much richer explanation of what's going on in this neuron. And the focus of this talk is how to generate such an explanation.
[00:04:27.120 --> 00:04:40.320]   In general, we're taking this, you know, a philosophy that neurons, especially later in the network, are not just simple feature detectors, but rather can be considered as implementing complex decision rules, or you can even think of them as programs composed of multiple concepts.
[00:04:40.320 --> 00:04:42.520]   So how do we generate these?
[00:04:42.520 --> 00:04:50.600]   So I'll first describe the basic technique of NetDissect and then propose an extension to that to handle compositional concepts.
[00:04:51.680 --> 00:04:57.800]   The idea is that we have some data set of inputs, which we'll call X, and some sort of model that transform these inputs into representations.
[00:04:57.800 --> 00:05:12.920]   We can inspect individual neurons of the representation, here it's unit 483 in ResNet, and examine the activations of this neuron over the inputs. And the challenge is to try to identify or explain this neurons behavior in human understandable terms.
[00:05:14.880 --> 00:05:31.080]   So the way that NetDissect proposes to do this is by first segmenting the neurons into binary masks. So we determine some threshold, let's say, you know, the top 1% of values this neuron takes, and whenever this neuron exceeds that threshold, we consider the neuron active at that point.
[00:05:33.040 --> 00:05:43.840]   Now to generate an explanation, we need some sort of gold hand annotated inventory of concepts, which are also represented as segmentation masks, such as water or river or even colors.
[00:05:43.840 --> 00:05:49.600]   Then the challenge is to find the explanation that most closely matches the behavior of a neuron.
[00:05:49.600 --> 00:06:01.120]   And the way we do so is by some sort of measure of goodness of an explanation. And for that we'll use the intersection of reunion score or IOU. This is commonly used in say bounding box prediction and computing.
[00:06:02.560 --> 00:06:12.640]   If we measure IOU, we find that the best explanation we have out of this simple concept inventory is the water concept. And this is the explanation that NetDissect obtains.
[00:06:12.640 --> 00:06:24.200]   But of course, we take a closer look, it's clear that like this is not the entire story, right? It seems like the behavior is a little more sophisticated than just detecting all bodies of water because it does not activate for that middle image.
[00:06:25.120 --> 00:06:33.200]   So our contribution is to combinatorially expand the possible concepts under consideration with compositional operators on concepts.
[00:06:33.200 --> 00:06:47.640]   So consider the logical operations and/or and not, and then we can use these logical operations to combine existing concepts to form progressively more complex ones. And we'll iteratively construct these more complex concepts via beam search.
[00:06:49.920 --> 00:07:02.040]   So once at the beam search, we might compose, for example, and construct a water or river concept, which just is the logical or the two masks, or we might invert the blue mass to get non blue colors.
[00:07:02.040 --> 00:07:18.920]   Finally, we keep going. And let's imagine that we stop at this explanation. So this is kind of a length three logical explanation, which has a much higher IOU compared to the water neuron and is more closely descriptive of the water.
[00:07:18.920 --> 00:07:28.880]   And it's more closely descriptive of the behavior of this neuron, right? And in particular, the explanation we have assigned now is that this neuron fires for water that's not blue.
[00:07:28.880 --> 00:07:38.880]   So that's the basic technique. I think there are a number of questions we can begin to explore with this technique. But here are three that we looked at in the paper.
[00:07:38.880 --> 00:07:48.040]   The first is, do neurons learn compositional concepts? So does this explanation technique bias anything over just simple metaseq?
[00:07:48.080 --> 00:07:54.040]   The second is whether the interpretability of individual concepts in a model relates to downstream task performance in any way.
[00:07:54.040 --> 00:07:59.440]   And the third is whether we can use our explanations to begin to probe or manipulate model behavior.
[00:07:59.440 --> 00:08:06.320]   So let's look at these three questions. The first is, do neurons learn compositional concepts?
[00:08:06.320 --> 00:08:12.880]   So here, what I'm going to do is I'm going to plot on the x axis, the maximum formula length of the explanation to generate with our procedure.
[00:08:13.200 --> 00:08:25.320]   And on the y axis, the average explanation quality or IOU of each neuron. So at formula length one, we had metaseq. And as we increase the formula length, we get progressively more complex explanations.
[00:08:25.320 --> 00:08:39.640]   So what we see here is a positive relationship between formula length and IOU, indicating that we are indeed generating higher quality explanations as measured by IOU with about a 68% increase in explanation quality when we go from, say, one to length 10.
[00:08:42.960 --> 00:08:50.200]   We can take a look at qualitatively what kind of concepts are identified in this model. This is a model ResNet trained on a scene classification task.
[00:08:50.200 --> 00:08:59.160]   And we see that a lot of interesting abstractions emerge, right? So I can categorize these kind of broadly into four categories, the first of which are meaningful perceptual abstractions.
[00:08:59.160 --> 00:09:05.400]   We can take a look here, unit 192 fires for towers, unit 310 fires for, say, bathroom appliances.
[00:09:06.160 --> 00:09:16.560]   And we're going to call these abstractions both perceptually meaningful and lexically meaningful because we can take a look at the explanation and understand the kinds of concepts that are being fired for.
[00:09:16.560 --> 00:09:26.520]   We also have maybe about 22% of neurons which fall into this category of learning an abstraction that is perceptually meaningful, but does not exist
[00:09:26.800 --> 00:09:32.640]   in our concept inventory. And so unit 321, for example, is clearly some sort of ball or sphere detector,
[00:09:32.640 --> 00:09:44.360]   but we do not have annotations of balls or spheres in our library and therefore the explanation we have ball pit or orchard or bounce game, it's a little bit less clear how that relates to the activations.
[00:09:44.360 --> 00:09:53.760]   We also see examples of specialization. So this is where neurons activate for more specific versions of concepts that are in our inventory.
[00:09:54.000 --> 00:10:00.280]   This includes that famous non blue water neuron and also an attic neuron that fires only for the top triangular part and not the floor.
[00:10:00.280 --> 00:10:09.680]   And finally, we see examples of what I'm going to call polysemanticity, which is the tendency for neurons to fire for completely separate concepts.
[00:10:09.680 --> 00:10:16.240]   And so that includes neurons that fires for, say, operating rooms or castles or bathrooms or bakeries or shop fronts, etc.
[00:10:19.600 --> 00:10:25.600]   So this has given us an explanation kind of what we're seeing in the behavior of a neural network that's been trained for seeing classification.
[00:10:25.600 --> 00:10:30.040]   Now let's take a look at whether or not the concepts here relate to model performance in any way.
[00:10:30.040 --> 00:10:36.600]   So the way we're going to do this is by obtaining some sort of measure of the reliability or accuracy of the neuron.
[00:10:36.600 --> 00:10:46.560]   And what we're going to do is we're going to measure the model accuracy on inputs when the neuron is maximally active. So when the neuron lights up, when it contributes to a decision, how accurate is the model's decision on average.
[00:10:48.160 --> 00:10:54.880]   Here I'm plotting on the x axis, the average explanation quality of our neuron and on the y axis, the accuracy of the neuron when it fires.
[00:10:54.880 --> 00:11:04.280]   And what we see is a positive relationship, which is noisy, but highly significant. And importantly, this correlation increases as we increase the quality of our explanations.
[00:11:04.280 --> 00:11:16.760]   And so what this is saying that as that at least in the computer vision case, neurons that are more human interpretable that are more compositional in their concepts end up being more reliable for test time performance.
[00:11:17.480 --> 00:11:25.720]   And finally, here's kind of one last question, which I think is the most interesting one, which is can our explanations give us insight into how we can control and manipulate model behavior.
[00:11:25.720 --> 00:11:35.080]   So we've probed the final convolutional layer before the prediction of ResNet, which allows us to look at what kind of explanations contribute to certain class predictions.
[00:11:35.080 --> 00:11:38.120]   So here's an example of class 324 or swimming hole.
[00:11:38.120 --> 00:11:45.080]   And we take a look at the kinds of concepts that feed into this class. A lot of them are sensible. We have foliage and we have waters and we have creeks and deserts.
[00:11:45.840 --> 00:11:48.920]   But in particular, there's a return of that non blue water neuron.
[00:11:48.920 --> 00:11:57.560]   This non blue water neuron is interesting, right, what what happens is if we take a swimming hole image which a bunch of neural networks think assuming whole
[00:11:57.560 --> 00:12:08.000]   And we paint the water blue we manipulate the model prediction to Grotto in three out of the four networks that we explored, including networks that are outside of the probe one which is resonating
[00:12:08.920 --> 00:12:18.600]   So this indicates that, you know, we're latching on to some sort of data set bias here that swimming holes always have non blue water and these explanations has given us insight into how we can then manipulate this behavior.
[00:12:18.600 --> 00:12:32.040]   Another example here is clean room. So it's a little bit less clear what kind of concepts are feeding into this prediction. But in particular, there's one concept here that fires for igloos.
[00:12:32.520 --> 00:12:41.160]   And so if we take a corridor and put an igloo in it, we change the prediction from corridor to clean room, but only here in the probe resonating
[00:12:41.160 --> 00:12:51.960]   And finally, one more example. Here's a viaduct. The, you know, concepts that feed into this prediction are quite sensible, but there's one neuron that fires for washers and laundromats as well.
[00:12:51.960 --> 00:12:57.560]   So we take a forest path we stick a bunch of washing machines there and we change the prediction to viaduct.
[00:12:57.600 --> 00:13:04.320]   So the method that I described here is in general it's task agnostic. We don't have to apply it to vision, we can apply it to any kind of representation
[00:13:04.320 --> 00:13:12.160]   Learned by some deep model and just as proof of concept, we can explore a natural language processing task. So this is the task of natural language inference.
[00:13:12.160 --> 00:13:19.000]   The basic idea is that we're given two sentences. One is a premise sentence, such as a woman in a light blue jacket is riding a bike.
[00:13:19.720 --> 00:13:23.480]   And then there's a hypothesis to a sentence like a woman in a jacket is riding a bike.
[00:13:23.480 --> 00:13:30.520]   And the objective is to determine the relationship between the truth conditions of the two sentences. So in this case, the premise entails the hypothesis.
[00:13:30.520 --> 00:13:39.040]   But if we change it to a bus, for example, then it's a contradiction. And finally, there might be a neutral prediction. There's no relation between the premise and the hypothesis.
[00:13:41.280 --> 00:13:46.520]   So I'm picking this task as a representative NLP task because in recent years, this task has come under scrutiny.
[00:13:46.520 --> 00:13:52.120]   Because it's unclear how much actual inference is happening in natural language inference. So I'm going to surround inference with quotes here.
[00:13:52.120 --> 00:13:54.520]   And here are two hints as to why this might be happening.
[00:13:54.520 --> 00:13:59.920]   Right. The first is we take some model that encodes the premise of the hypothesis and produce some prediction.
[00:13:59.920 --> 00:14:04.440]   Some sort of standard neural model gets us around 78% accuracy. Right. So that's pretty good.
[00:14:06.680 --> 00:14:15.800]   However, if we design an ablated version of the model, which only takes the hypothesis input. So it completely ignores the premise. This model still gets 69% accuracy.
[00:14:15.800 --> 00:14:24.880]   Right, so there's a drop in accuracy, but this is actually still really good. And importantly, it's far above chance. So this is an indicator that there's not actually much inference happening in this model.
[00:14:24.880 --> 00:14:33.360]   Similarly, we can come up with certain heuristics. So heuristic rules such as predict and predict the hypothesis.
[00:14:34.120 --> 00:14:40.640]   Some heuristic rules such as predict entailment when all of the words in the hypothesis are also in the premise, as is the case here.
[00:14:40.640 --> 00:14:50.640]   Turns out if you apply these heuristics when they do apply you get 90% accuracy on examples where this heuristic applies and there are several such heuristics like this.
[00:14:50.640 --> 00:15:01.560]   So this indicates to us that, you know, these models are maybe not really learning the right thing in trying to solve these tasks and are instead latching on to certain spurious correlation or data set biases.
[00:15:01.840 --> 00:15:07.080]   And we wanted to look at whether or not we can uncover these behaviors with our explainability technique.
[00:15:07.080 --> 00:15:23.360]   So here's our NLP model or NLI model and we're going to probe the final layer of the kind of multi-layer perceptron. So we encode the premise and hypothesis with LSTMs, we combine them and then try to produce some prediction. And we're going to probe the final layer before the prediction.
[00:15:25.200 --> 00:15:36.200]   Our concepts will be very, very simple bag of words concept. So given some sort of premise hypothesis pair, we extract these very simple concepts where pre colon woman indicates that the word woman appears in the premise.
[00:15:36.200 --> 00:15:43.200]   Pre colon and then indicates the word, sorry, the noun appears in a sentence premise, anywhere in the premise, and so on.
[00:15:43.200 --> 00:15:53.800]   And additionally, there's this kind of overlap feature which indicates the degree to which the premise and the hypothesis share words. So in this case, they share 75% of unique words.
[00:15:54.800 --> 00:16:02.800]   And finally, as our compositions, we can use and, or, and not. But just as a hint that we can use more unique compositions. I'll also define the neighbors composition,
[00:16:02.800 --> 00:16:17.320]   which the neighbors of a certain token is the logical or across the five most similar words to this token in glove embedding space. So the idea here is we're capturing this intuition that a neuron might fire for semantically similar words.
[00:16:20.800 --> 00:16:32.800]   Let's now answer our three questions. First, do neurons learn compositional concepts. Here is a model trained on the Stanford natural language inference data set. And we again see this positive relationship between our formula length and explanation quality.
[00:16:32.800 --> 00:16:46.800]   Qualitatively, let's take a look at what's going on. So here is unit 870 in this model. Here's the explanation we assign along with its IOU. And here are some examples of the neuron being most active.
[00:16:48.800 --> 00:16:56.800]   So in general, if we take a look here, we would really label this neuron is being gender sensitive so it activates when the premise contains the word man.
[00:16:56.800 --> 00:17:11.800]   And does not contain the word woman and the hypothesis does not contain the word man. So basically, whenever there is a gender switch between the premise and the hypothesis. This neuron votes in favor of contradiction. So we can analyze the weights of this neuron towards the three class predictions here.
[00:17:14.800 --> 00:17:27.800]   Here's another neuron that fires. There's a few, you know, noisy explanations here. But the key here is that overlap 70% neuron. This neuron fires when the premise and the hypothesis share many, many common words and it votes towards entailment.
[00:17:27.800 --> 00:17:33.800]   So these are the kinds of lexical overlap heuristics I was talking about earlier.
[00:17:35.800 --> 00:17:50.800]   Here's a neuron that activates whenever the word sitting is in the hypothesis. So these are words where the verb is actually quite indicative of the class prediction, even though they really shouldn't be. And so this one activates whenever the word sitting is in the hypothesis, but not the premise.
[00:17:50.800 --> 00:17:59.800]   And lastly, we have neurons that are not well explained by a future set. Right. So this one activates whenever there is a noun in the past, which is almost all the time.
[00:18:02.800 --> 00:18:13.800]   We can again relate interpretability to model performance. And here we actually see a negative relationship. So what this is saying is that the better we are able to describe our neurons with our explanation technique, the less accurate the neuron is.
[00:18:13.800 --> 00:18:23.800]   And the reason why is because the kinds of features we're using are very, very simple, right? They're just these really simple bag of words features which you would not expect to really be involved in any sort of robust natural language inference.
[00:18:23.800 --> 00:18:30.800]   And so the better we can describe neurons as, you know, using these very, very simple decision rules, the less accurate they are on average.
[00:18:31.800 --> 00:18:36.800]   And interestingly, the simpler our explanations, the better we capture this anti-correlation.
[00:18:36.800 --> 00:18:46.800]   So one kind of key caveat here is that interpretability is not a priori correlated with performance. It really depends on the space of concepts that we're searching for.
[00:18:46.800 --> 00:18:52.800]   So in the vision case, we really were searching for meaningful abstractions, whereas in this case, we're trying to identify undesirable behaviors.
[00:18:56.800 --> 00:19:05.800]   And finally, let's take a look at how we can now manipulate model behavior in the same way. So here's an explanation of a neuron. This one fires whenever the word "nobody" is in the hypothesis.
[00:19:05.800 --> 00:19:17.800]   So that is a very clear signal that there's a contradiction, at least in this data set. So we take a premise-hypothesis pair, such as three women prepare a meal in the kitchen, and we can modify the hypothesis to "nobody but the ladies are cooking."
[00:19:18.800 --> 00:19:30.800]   And so this should change the true label from entailment to neutral, but it changes the true prediction, sorry, the model prediction from entailment to contradiction. So we've induced adversarial behavior in this model.
[00:19:30.800 --> 00:19:40.800]   Here's another example. This neuron fires for whenever there's some sort of word related to a couch or a table or a seat in the hypothesis.
[00:19:41.800 --> 00:19:52.800]   And so we take this premise-hypothesis pair, and we add the word "couch" in the hypothesis. We change the true prediction from entailment to neutral, but the model prediction from entailment to contradiction.
[00:19:52.800 --> 00:20:06.800]   Finally, one more. Here's that sitting neuron. Here we can modify the premise instead. So we have a blonde woman is holding two golf balls or reaching down into a golf ball, and the hypothesis is a blonde woman is sitting down.
[00:20:07.800 --> 00:20:20.800]   It turns out if we just cut out most of the premise, and we just have a blonde woman is holding two golf balls, the model still votes for contradiction, even though the true label is now neutral. So it's just the presence of sitting there that's determining the model prediction in this case.
[00:20:25.800 --> 00:20:40.800]   So just to summarize, I've described a method for model interpretability and neuron interpretability specifically that generates compositional explanations of the individual neurons inside deep representations, which allow us to identify interesting, more rich behavior in neurons.
[00:20:40.800 --> 00:20:46.800]   So this includes meaningful abstractions in vision, polysematicity, and spurious correlations in language.
[00:20:49.800 --> 00:20:58.800]   These concepts actually have something to do with the downstream task performance of the model. So we can actually use these explanations to disambiguate neurons that are better or worse with respect to performance.
[00:20:58.800 --> 00:21:08.800]   And finally, we've shown some early evidence that we can now predictably manipulate the kind of behaviors of neurons by analyzing the explanations and then staging interventions based on the explanations.
[00:21:12.800 --> 00:21:21.800]   So there are a couple of questions I'd like to explore. One is, can we better look at connections between layers? So here we always probe the final layer of a neural network, but we can definitely look at intermediate layers.
[00:21:21.800 --> 00:21:28.800]   And if you're interested in this kind of work, the circuits work from the OpenAI team with Chris Ola has done really great work in this kind of area.
[00:21:28.800 --> 00:21:39.800]   And finally, insofar as we've identified that more interpretable concepts are maybe more desirable when it comes to performance, can we begin to use interpretability as a sort of training signal?
[00:21:40.800 --> 00:21:43.800]   Can the models be interpretable from the start? And does this lead to better performance?
[00:21:43.800 --> 00:21:53.800]   So that's it. I wanted to thank my collaborators and Jacob and Andreas. Here are some links to code and the preprints, and I'm happy to take questions at this point.
[00:21:53.800 --> 00:22:02.800]   Great. Thanks a lot, Jesse, for really interesting work and well presented also.
[00:22:03.800 --> 00:22:10.800]   I encourage folks on the Zoom to put their questions in the Q&A and folks on YouTube can put them in the live chat.
[00:22:10.800 --> 00:22:19.800]   But to kick us off, I guess, so my actually my one of my questions you already covered in your future direction. So I wanted to probe a little bit deeper on that.
[00:22:19.800 --> 00:22:24.800]   So what do you think the prospects are for using something like this as a form of regularization?
[00:22:25.800 --> 00:22:43.800]   Like, since you have these sort of two sides where it's like, okay, if the explanations are bad heuristics, that's maybe something that you want to penalize. If they are rich compositional explanations, maybe that's something that you want to encourage.
[00:22:43.800 --> 00:22:51.800]   Yeah, so I think it really depends, as you said, in the explanation we're generating and whether or not they're correlated with desirable or undesirable behaviors.
[00:22:52.800 --> 00:23:02.800]   In terms of, you know, in the desirable case, like in the vision case, for example, we might encourage the development of neurons to recognize well defined atomic concepts like dogs and cats.
[00:23:02.800 --> 00:23:09.800]   That seems like something that we can definitely do. But then the question becomes, you know, at what level do we really need to encourage this interpretability, right?
[00:23:09.800 --> 00:23:19.800]   So do we need to encourage it at the level of individual neurons, as we've done before? Or can we maybe encourage it, you know, basically just it needs to encode the concept somewhere, right? Isn't it the individual neurons?
[00:23:20.800 --> 00:23:30.800]   I think the individual neuron thing is interesting. I definitely think that there are ways that we can adapt this metric into something that's explicitly optimizable so that we can actually use it as some sort of auxiliary regularization technique.
[00:23:30.800 --> 00:23:39.800]   And maybe the benefit we get there is like very, very clean interpretability, right? Like we know there is a cat and dog neuron in this model because we've trained the model to expose that feature.
[00:23:39.800 --> 00:23:46.800]   Whether or not it matters for performance at all compared to maybe more general regularization techniques, I have no clue, but it's an interesting avenue for sure.
[00:23:47.800 --> 00:23:57.800]   Yeah, it would be cool. It seems, yeah, the challenge that I immediately see is coming up with a way to quantify it so that you can include it easily in backprop, right?
[00:23:57.800 --> 00:24:03.800]   Yeah, so the basic idea, I think, is to do some sort of continuous version of this discrete thing, right?
[00:24:03.800 --> 00:24:11.800]   So instead of having to segment, we could imagine just doing kind of a soft matching between the activations of a neuron and these ground truth concepts.
[00:24:12.800 --> 00:24:23.800]   And that would allow us to backprop through it. The other challenge then is just figuring out, but when neurons don't learn concepts at all, how do we know or how do we, which concepts are we pointing our neurons to in the first place, right?
[00:24:23.800 --> 00:24:28.800]   And generating these explanations takes time, and that's somewhat unclear.
[00:24:28.800 --> 00:24:40.800]   Definitely. So one thing, I guess I kind of want to ask, what do you think makes individual neurons more explainable versus less explainable?
[00:24:41.800 --> 00:24:49.800]   I mean, things like architecture choices, regularization choices, data augmentation, or its absence. Do you have any sense for that?
[00:24:49.800 --> 00:24:58.800]   Yeah, it's a really tricky question. I think a lot of it is going to be driven by data.
[00:24:58.800 --> 00:25:10.800]   I think a neuron is predisposed to do the easiest possible thing, which is just to latch on to something non-interpretable or spurious, as long as the data doesn't really sufficiently test the capabilities of that neuron in a way, right?
[00:25:10.800 --> 00:25:16.800]   So we can use these cheats to kind of get away with simple cheats.
[00:25:16.800 --> 00:25:29.800]   That, for example, is an explanation of why there might be polysematicity in neurons, that a neuron can serve double duty as a bakery and a shopfront, for example, because it never has to really cleanly distinguish between bakeries and shopfronts, right?
[00:25:30.800 --> 00:25:42.800]   If you had a more robust data set where you really had to differentiate between the two, the neuron might be forced to be more interpretable, or the entire model in general might be forced to be more interpretable and better kind of organize its representation space into meaningful ways.
[00:25:42.800 --> 00:25:58.800]   And is there a clean way to distinguish between apparent polysemanticity of an individual neuron and just representations being, say, holographic or dense in the layer of all the neurons?
[00:25:59.800 --> 00:26:01.800]   That's a great question.
[00:26:01.800 --> 00:26:19.800]   Yeah, I would say not entirely. So, I mean, this work, you know, really looked at individual neurons. And so we really can't determine the degree to which, for example, a neuron is polysemantic for bakeries and shopfronts, right? But maybe it's just like one component of a much larger distributed bakery detector.
[00:26:20.800 --> 00:26:34.800]   I think the most promising work that tries to identify that is by looking at like what kind of neurons activate for individual predictions, right? So given a single bakery image, take a look at the neurons that are firing, maybe it turns out that there are multiple bakery detectors.
[00:26:34.800 --> 00:26:43.800]   And so we can actually identify the specific, you know, subspace that lights up for bakery, for example. That's definitely an avenue of future work, which I think is definitely worth pursuing.
[00:26:44.800 --> 00:26:57.800]   Yeah, it seems like one of the challenges there is that, like, one of the problems you solved elegantly in this paper is like, okay, if we combine a bunch of simple things in a principled fashion, then we get access to a large space of possible explanations.
[00:26:58.800 --> 00:27:15.800]   But then when you try to understand polysematicity, you also have another like exponential explosion problem, which is that all the possible polysemantic combination, or not, all the possible combinations of neurons that could be representing a concept is way bigger than just the set of all possible, the set of all neurons in the layer.
[00:27:15.800 --> 00:27:17.800]   Yeah.
[00:27:18.800 --> 00:27:26.800]   Yeah, that's the challenge. And there's not a clear trade off between the two. I mean, this is why people are advocating for kind of the supervised methods, right?
[00:27:26.800 --> 00:27:34.800]   And maybe we need to look towards that literature to really, you know, identify concepts that basically in any way, shape, or form are somehow encoded in representations.
[00:27:34.800 --> 00:27:40.800]   But of course, that already has its own debate. So I think we're getting to steps where we're getting to understand the kind of information that's encoded.
[00:27:41.800 --> 00:27:50.800]   But certainly, you can definitely make, say, slightly simpler assumptions about the way concepts are encoded, and then begin to look at, for example, multi-neuron concepts, right?
[00:27:50.800 --> 00:28:01.800]   So maybe it's not dramatically too difficult to imagine that individual neurons apply for certain concepts and then consider a limited subset of linear combinations of those neurons and identify to what extent those apply for concepts.
[00:28:01.800 --> 00:28:04.800]   That might not increase the search space too much.
[00:28:05.800 --> 00:28:12.800]   Yeah, that does make sense. What role do you think that like optimization-based explanations of neurons have to play?
[00:28:12.800 --> 00:28:21.800]   So like, you know, basically like Lucid or Deep Dream where it's like, okay, take an image or take a baseline and increase the activation of this neuron, this linear subspace of neurons.
[00:28:21.800 --> 00:28:27.800]   Do you think that's something that could help with other kinds of explanation? Is it orthogonal? Is it maybe misleading? What do you think?
[00:28:28.800 --> 00:28:38.800]   No, I think all that work is really, really excellent. I think the main thing is that there's a lot of really interesting kind of model visualization or interpretability techniques that require that a practitioner
[00:28:38.800 --> 00:28:47.800]   implement this algorithm or runs it and then inspects the activation patterns, right? And so you do some Deep Dream thing, you say, oh, this is the neuron that this is firing for, right?
[00:28:47.800 --> 00:28:55.800]   But the problem is that you have to go in there and actually look at the neuron and say, oh, this is firing for dogs and cats, right? There's no automatic explanation that's generated.
[00:28:56.800 --> 00:28:59.800]   And so what results is that, you know, it can be potentially very time consuming.
[00:28:59.800 --> 00:29:09.800]   So the kind of approach that we're describing here is more of like an automated analysis where it's like, let's automatically identify behaviors that are encoded and maybe try to surface the interesting ones.
[00:29:09.800 --> 00:29:15.800]   And so I think like in some ways are complementary, right? Like you do automated analysis of the type described here.
[00:29:15.800 --> 00:29:21.800]   And then maybe if you need deeper dives or even pretty visualizations, you can inspect individual neurons by themselves.
[00:29:22.800 --> 00:29:38.800]   Yeah, that makes sense. It does seem. Yeah, that Chris said something at a talk I saw once of just like he'd spent like thousands of hours with just the one that they did for circuits.
[00:29:38.800 --> 00:29:46.800]   I forget whether it was Inception V1, I think is what it was. Anyway, he spent thousands of hours with just this neural network, like looking through all of its neurons.
[00:29:47.800 --> 00:29:57.800]   And, you know, he's got friends and enemies like in the units of Inception and that's, you know, useful and we've learned a lot, but it's not scalable the way an automated approach like yours is.
[00:29:57.800 --> 00:30:08.800]   Yeah, I think as a result, like that work has generated much richer understanding of the way that certain concepts are composed together in neural networks to form, you know, more sophisticated object detectors.
[00:30:09.800 --> 00:30:18.800]   But you're right, it's not scalable, right? So the goal, I think, is like to bring this kind of automatic generation closer to the kinds of insights that this kind of hand curated circuits work can bring us.
[00:30:18.800 --> 00:30:33.800]   That's cool. That's a great, that's a great ambition. And I look forward to seeing the rest of your work. That's all the time we have for the first speaker slot. So I'll thank you for coming. Thank you for sharing. I look forward to see the rest of your work in the future.
[00:30:33.800 --> 00:30:35.800]   Yeah, no, really happy. Thanks.
[00:30:35.800 --> 00:30:50.800]   [Music]

