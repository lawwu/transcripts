
[00:00:00.000 --> 00:00:03.200]   side note there, but alright, welcome back, everyone. I won't
[00:00:03.200 --> 00:00:06.520]   take more of the time we'll be talking about how to navigate
[00:00:06.520 --> 00:00:09.520]   the hub and share our own models. Not me, Wade will be
[00:00:09.520 --> 00:00:12.520]   talking about that. So you're in good hands. Wade, please take it
[00:00:12.520 --> 00:00:12.800]   away.
[00:00:12.800 --> 00:00:13.880]   All right.
[00:00:13.880 --> 00:00:18.800]   As another reminder, please keep the questions coming in the
[00:00:18.800 --> 00:00:20.960]   chart. I'll keep monitoring them.
[00:00:20.960 --> 00:00:25.120]   Yeah, and I'll try to make sure to stop every so often. If I
[00:00:25.120 --> 00:00:29.360]   don't, just feel free to yell at me. Everybody can see my screen
[00:00:29.360 --> 00:00:34.760]   over here. Yes. All right, cool. Okay, so we're gonna go ahead
[00:00:34.760 --> 00:00:39.120]   and start with session four. And again, remember the the
[00:00:39.120 --> 00:00:42.800]   structure of the Hugging Face courses. They're essentially
[00:00:42.800 --> 00:00:48.040]   releasing three. It's one course, but there's going to be
[00:00:48.040 --> 00:00:51.160]   three groups. And so we're finishing up the first group
[00:00:51.160 --> 00:00:54.480]   today. And I think each of them are going to be like four
[00:00:54.480 --> 00:01:00.720]   sections. And before we get into it, just want to make sure
[00:01:00.720 --> 00:01:03.560]   everybody has all this information, the study group
[00:01:03.560 --> 00:01:06.760]   registration page. As Sanyam said, we're only going to have
[00:01:06.760 --> 00:01:12.320]   one more session next week. And that's where we're going to be
[00:01:12.320 --> 00:01:15.680]   looking at folks who have submitted blog posts or
[00:01:15.680 --> 00:01:19.800]   applications using the Train Transformer or compete in the
[00:01:19.840 --> 00:01:25.280]   SST2 challenge. We'll hopefully be willing to share their work.
[00:01:25.280 --> 00:01:30.760]   We do a walkthrough and do a Q&A and get feedback from folks in
[00:01:30.760 --> 00:01:35.280]   the group on that work. But yeah, here's the resources again,
[00:01:35.280 --> 00:01:38.520]   depending on where you're starting. If you're in Fast.ai,
[00:01:38.520 --> 00:01:43.560]   the Fast.ai course and Zach's Walk with Fast.ai is a good
[00:01:43.560 --> 00:01:46.640]   place to start as well as the Fastbook. And there's a reading
[00:01:46.640 --> 00:01:51.240]   group that Aman's doing over on Weights and Biases is going
[00:01:51.240 --> 00:01:53.840]   really well. I see a lot of people blogging about what
[00:01:53.840 --> 00:01:55.920]   they're learning there. So check that out. I think he's like a
[00:01:55.920 --> 00:02:00.360]   chapter six right about now. And then the Fast.ai Hugging Face
[00:02:00.360 --> 00:02:04.840]   Libraries, which we saw last week, Adapt NLP, Fast Hugs, and
[00:02:04.840 --> 00:02:08.600]   Blur. Give those a go. Find out what works for you. Try building
[00:02:08.600 --> 00:02:12.720]   something using one of those or look at the source code and
[00:02:12.720 --> 00:02:15.240]   build something yourself. That's a really great way to
[00:02:16.520 --> 00:02:21.760]   learn more about how Fast.ai callbox can be used to basically
[00:02:21.760 --> 00:02:25.680]   train something like Transformers Library. And then
[00:02:25.680 --> 00:02:29.480]   for ML Data Science in general, you got the Chi Time Data
[00:02:29.480 --> 00:02:32.600]   Science Podcast at Weights and Biases. So check all that stuff
[00:02:32.600 --> 00:02:36.760]   out. And by the way, Sanyam promised next week to announce
[00:02:36.760 --> 00:02:41.840]   where he's going. So if for no other reason, come back for week
[00:02:41.840 --> 00:02:44.720]   five to find out where Sanyam will be in the future.
[00:02:45.720 --> 00:02:49.800]   Okay, so agenda, we'll be looking at section four. And
[00:02:49.800 --> 00:02:53.680]   this is probably kind of one of the easier ones to go through.
[00:02:53.680 --> 00:02:57.600]   And we'll probably go through this much quicker than last week
[00:02:57.600 --> 00:03:02.240]   where we did the demos of the three Fast.ai Libraries. But
[00:03:02.240 --> 00:03:05.680]   essentially what we're looking at in section four is how to use
[00:03:05.680 --> 00:03:10.480]   Hugging Face, their model hub, how to find data sets to train
[00:03:10.480 --> 00:03:15.360]   models on, and then also how to find good pre trained models to
[00:03:15.360 --> 00:03:18.800]   start with. And so we'll be looking at that. And we'll go
[00:03:18.800 --> 00:03:23.120]   through that process of picking a data set, picking a checkpoint
[00:03:23.120 --> 00:03:27.440]   and then training. And then the final part of section four is
[00:03:27.440 --> 00:03:34.160]   about sharing models. So we'll actually go through how to share
[00:03:34.160 --> 00:03:38.640]   a model on the hub. And all the particulars with that there's
[00:03:38.680 --> 00:03:42.680]   three ways they cover. We'll actually do a demo of probably
[00:03:42.680 --> 00:03:46.840]   the simplest and most straightforward way. And again,
[00:03:46.840 --> 00:03:53.640]   the deadline for competitions is Tuesday, August 3 12pm Pacific
[00:03:53.640 --> 00:03:57.720]   Standard Time. I will probably be lenient right now we have
[00:03:57.720 --> 00:04:01.880]   none. So if you want to win some good swag from Weights and
[00:04:01.880 --> 00:04:06.680]   Biases, and get some respect, and also be able to demo your
[00:04:06.680 --> 00:04:09.640]   work. This is a great competition is really meant to be
[00:04:09.640 --> 00:04:14.520]   fun. And to enhance your learning. So I hope that folks
[00:04:14.520 --> 00:04:19.080]   will participate in one of the three competitions. And here
[00:04:19.080 --> 00:04:25.160]   they are again, best blog post, best results on SST2 and also
[00:04:25.160 --> 00:04:29.480]   no cheating. And then also most interesting application of a
[00:04:29.480 --> 00:04:35.320]   Fast AI trained transformer. And you can send a link to your
[00:04:35.360 --> 00:04:41.600]   blog post or to your Colab notebook or just to my email at
[00:04:41.600 --> 00:04:46.000]   wgilliam@omeow.com. Any questions coming up on the
[00:04:46.000 --> 00:04:47.040]   competition, Sanyam?
[00:04:47.040 --> 00:04:52.520]   No, not yet. If I may, I just wanted to add that to people
[00:04:52.520 --> 00:04:54.400]   listening and who will be watching this later, these
[00:04:54.400 --> 00:04:56.960]   things really are pretty helpful. Like I know many of
[00:04:56.960 --> 00:05:00.280]   the people tuning in are really starting their journey. But as
[00:05:00.280 --> 00:05:04.000]   someone who's found his career start through all of this stuff,
[00:05:04.080 --> 00:05:07.520]   do blog and do participate in these competitions, you know,
[00:05:07.520 --> 00:05:11.120]   weights and biases, recognizing you for an achievement really
[00:05:11.120 --> 00:05:14.040]   goes ahead and say something on your resume. So all you have to
[00:05:14.040 --> 00:05:18.080]   do is just send an email to Wade and if it's too bad, he won't
[00:05:18.080 --> 00:05:21.560]   mention if it if it's good, he'll mention it. So it's a win
[00:05:21.560 --> 00:05:24.440]   win anyway. So please, please consider participating. That's
[00:05:24.440 --> 00:05:25.160]   what I'm trying to say.
[00:05:25.160 --> 00:05:28.800]   Yeah, absolutely. And the and the other thing is, is that if
[00:05:28.800 --> 00:05:31.560]   you participate and send something in and are willing to
[00:05:31.560 --> 00:05:35.400]   share, we're also willing to give feedback. And so, you know,
[00:05:35.400 --> 00:05:39.520]   I'll be providing feedback. I'm sure Zach, Morgan, other guys,
[00:05:39.520 --> 00:05:43.920]   Arto, Sanyam, other people in the community. So, you know, one
[00:05:43.920 --> 00:05:46.560]   of the hard things is we're all kind of especially with being
[00:05:46.560 --> 00:05:50.040]   virtual, you know, it's hard to get feedback from other people
[00:05:50.040 --> 00:05:52.080]   and to actually work together. And so this is a good opportunity
[00:05:52.080 --> 00:05:55.760]   to actually get some feedback on on what you're doing by
[00:05:55.760 --> 00:05:58.520]   participating in one of these competitions, or all three of
[00:05:58.520 --> 00:05:58.800]   them.
[00:06:00.840 --> 00:06:06.000]   Okay, so. So let's get into section four. So the first part
[00:06:06.000 --> 00:06:11.400]   starts with using the hub. And essentially the hugging face hub
[00:06:11.400 --> 00:06:17.840]   is a place to find data sets find models, pre trained models
[00:06:17.840 --> 00:06:23.240]   to work with. And the other thing that's kind of interesting
[00:06:23.240 --> 00:06:26.280]   to note is that the models in the hub aren't limited to
[00:06:26.320 --> 00:06:30.200]   transformers or NLP. And so you're going to see a lot of
[00:06:30.200 --> 00:06:34.560]   other third party folks pushing models and there's there's a
[00:06:34.560 --> 00:06:38.200]   text to speech, there's speech recognition, there's vision in
[00:06:38.200 --> 00:06:43.000]   there. So it's kind of growing. So it's not just, you know, NLP
[00:06:43.000 --> 00:06:46.560]   transformers, it's just about it's a place for sharing any
[00:06:46.560 --> 00:06:52.880]   type of model, which is actually really cool. And essentially to
[00:06:54.640 --> 00:07:00.800]   get on the hub, you just go to hugging face.co. And the model
[00:07:00.800 --> 00:07:05.320]   hub is, is really, I guess, technically this part right
[00:07:05.320 --> 00:07:10.240]   here, where you go to models, and you can search by task, you
[00:07:10.240 --> 00:07:15.160]   can look by data set languages, you can just like type in like,
[00:07:15.160 --> 00:07:21.280]   if you know, a certain data set like SST, you can go ahead and
[00:07:21.280 --> 00:07:25.080]   just search for it on here. And then the nice thing is once you
[00:07:25.080 --> 00:07:28.160]   once you're here, you can go ahead and look at it on GitHub,
[00:07:28.160 --> 00:07:31.360]   you can explore the data set, which will kind of give you a
[00:07:31.360 --> 00:07:35.160]   GUI to actually look and explore and page through the data. And
[00:07:35.160 --> 00:07:37.400]   you can click this button right here, which will tell you
[00:07:37.400 --> 00:07:44.480]   exactly how to load it using their data set API. And then so
[00:07:44.480 --> 00:07:46.440]   that's actually I'm sorry, that's the data set side of
[00:07:46.440 --> 00:07:49.800]   things, I got navigated over there. But on the model side,
[00:07:49.800 --> 00:07:53.400]   you can go ahead and look at things by category, task
[00:07:53.400 --> 00:07:58.880]   language, same type of thing. And, you know, click in actually
[00:07:58.880 --> 00:08:02.400]   read a little bit more about, you know, how it was trained,
[00:08:02.400 --> 00:08:08.120]   what the data looks like people, not always, but on some of the
[00:08:08.120 --> 00:08:11.160]   bigger data sets have been a really good job at listing like
[00:08:11.160 --> 00:08:15.440]   known biases or, or known kind of potential issues. So
[00:08:15.440 --> 00:08:17.720]   definitely, depending on what you're using, you want to go
[00:08:17.720 --> 00:08:21.720]   through here and see what the authors of these models or
[00:08:21.720 --> 00:08:25.760]   data sets have actually said about what they've what they've
[00:08:25.760 --> 00:08:28.920]   actually created, before you start training yourself so that
[00:08:28.920 --> 00:08:31.600]   you kind of know, maybe limitations and potential
[00:08:31.600 --> 00:08:40.200]   issues. So that's essentially what the hub is. Now, oops,
[00:08:40.200 --> 00:08:46.320]   that's not what I wanted to do. So the first thing that we'll
[00:08:46.320 --> 00:08:54.120]   look at today is using pre trained models. So again,
[00:08:54.120 --> 00:08:57.440]   probably the most important thing is to find a data set
[00:08:57.440 --> 00:09:00.800]   that's suitable to your task that's kind of as close as
[00:09:00.800 --> 00:09:04.480]   possible to what you're going to be doing. So like, today,
[00:09:04.480 --> 00:09:07.680]   like, we'll look at a demo. And let's like, imagine we're doing
[00:09:07.680 --> 00:09:14.920]   sentiment like positive or negative sentiment on something
[00:09:14.920 --> 00:09:20.800]   like concerts or whatever, or, you know, music, we probably
[00:09:20.800 --> 00:09:23.360]   want to find a data set that's looking at something similar.
[00:09:23.360 --> 00:09:26.680]   And if possible, that's actually already targeted for
[00:09:26.680 --> 00:09:30.920]   sentiment task. And as one person found out in the forums,
[00:09:30.920 --> 00:09:34.200]   when you look at pre trained models that are trained on a
[00:09:34.200 --> 00:09:39.240]   specific data set, like, for example, sst two, you can't just
[00:09:39.240 --> 00:09:43.880]   use them and adjust the labels for something other than two
[00:09:43.880 --> 00:09:46.440]   labels. And so if you look at the forums, there's a
[00:09:46.440 --> 00:09:50.120]   discussion, because one of our users, users found that out and
[00:09:50.120 --> 00:09:57.320]   was surprised that they tried to start with a sst two trained
[00:09:57.320 --> 00:10:02.200]   classification model, and tried to then change it for it from
[00:10:02.200 --> 00:10:05.160]   two classes to five. And it basically said, No, it can't
[00:10:05.160 --> 00:10:08.480]   happen. And that's the reason is that these pre trained models
[00:10:08.480 --> 00:10:11.640]   when they're trained on specific classification tasks, hugging
[00:10:11.640 --> 00:10:15.480]   face doesn't just chop off the classification head and put a
[00:10:15.480 --> 00:10:19.720]   randomly trained one, it basically gives you whatever the
[00:10:19.720 --> 00:10:23.640]   user trained it for. So just be aware of that if you see things
[00:10:23.640 --> 00:10:26.680]   like that. But in our case, if we're actually training a
[00:10:26.680 --> 00:10:29.800]   sentiment model with two labels, we could go ahead and use a
[00:10:29.800 --> 00:10:34.600]   model like that and and fine tune it. So yeah, find a data
[00:10:34.600 --> 00:10:39.720]   set that resembles what you're trying to do. And then also try
[00:10:39.720 --> 00:10:46.760]   to find a checkpoint that's suitable for your task. And so
[00:10:46.760 --> 00:10:51.000]   best practices here is after you have your data set and
[00:10:51.000 --> 00:10:56.360]   checkpoint is to use the the auto model, the auto tokenizer
[00:10:56.360 --> 00:11:00.840]   auto config for classes, I'll make it a lot easier to iterate
[00:11:00.840 --> 00:11:04.920]   over different architectures if you want. And then as I said
[00:11:04.920 --> 00:11:08.520]   before, when you're using a pre trained model, see how it was
[00:11:08.520 --> 00:11:11.640]   trained, understand the data sets that was trained on its
[00:11:11.640 --> 00:11:15.720]   limitations, its biases, that will go a long way in helping
[00:11:15.720 --> 00:11:18.360]   you stay out of trouble when you're actually using these
[00:11:18.360 --> 00:11:25.160]   things. So with that said, we're going to go through that
[00:11:25.160 --> 00:11:29.480]   process that I just outlined right there. And so we'll be
[00:11:29.480 --> 00:11:33.800]   sharing this notebook later. So I'm going to start with the
[00:11:33.800 --> 00:11:37.720]   usual just installing transformers data sets. And I'm
[00:11:37.720 --> 00:11:41.880]   going to use blur for this demo, since that's the library
[00:11:41.880 --> 00:11:44.760]   I created. And I'm also more most familiar with that you can
[00:11:44.760 --> 00:11:51.480]   use adapt or fast hugs as you choose. And let's go ahead and
[00:11:51.480 --> 00:11:56.840]   do all the imports. And so step one, right is to find a data
[00:11:56.840 --> 00:12:01.240]   set suitable for your task. So I mentioned that, you know,
[00:12:01.240 --> 00:12:04.760]   potentially, let's think of like doing like, you know,
[00:12:05.320 --> 00:12:09.400]   you know, sentiment for different songs or whatever,
[00:12:09.400 --> 00:12:18.680]   right. So I do know if I go to the model hub, since I'm doing
[00:12:18.680 --> 00:12:21.160]   sentiment, I already kind of know that there is a model
[00:12:21.160 --> 00:12:27.000]   called IMDb that would allow us to that does sentiment on
[00:12:27.000 --> 00:12:31.240]   movies. And so that's pretty similar to what we're trying to
[00:12:31.240 --> 00:12:39.480]   do. So I can just search for IMDb. And it pops up there. I
[00:12:39.480 --> 00:12:44.120]   could read through and, and see like exactly how is it? What's
[00:12:44.120 --> 00:12:48.440]   the data look like it has a label has a text, I can look at
[00:12:48.440 --> 00:12:55.400]   the splits between the train, the test or validation. And
[00:12:55.400 --> 00:12:58.440]   then there's an unsupervised test. So with no, no labels.
[00:12:59.400 --> 00:13:03.640]   And it's still missing some information, but I'm going to
[00:13:03.640 --> 00:13:09.080]   go ahead and go for it. And so just by clicking use in data
[00:13:09.080 --> 00:13:15.720]   set library, I can go ahead and I'm gonna do is just grab the
[00:13:15.720 --> 00:13:22.040]   string you pass into load data set. We'll go back here.
[00:13:25.160 --> 00:13:31.400]   Stick that in there. And then as we saw last week, loading a
[00:13:31.400 --> 00:13:35.160]   data set is as easily easy as calling load data set from the
[00:13:35.160 --> 00:13:41.480]   data sets library. And then we can go ahead and print out what
[00:13:41.480 --> 00:13:44.200]   it looks like we can look at one of the rows in the training
[00:13:44.200 --> 00:13:47.080]   set. And then we can also look at the features so that when we
[00:13:47.080 --> 00:13:50.920]   actually set up our, our prepare our data and actually get ready
[00:13:50.920 --> 00:13:56.760]   to train our model, we know what we need to do. So let this run,
[00:13:56.760 --> 00:14:04.200]   hopefully doesn't take too long. And while that's going, the
[00:14:04.200 --> 00:14:08.600]   second step is to find a checkpoint suitable to our task,
[00:14:08.600 --> 00:14:14.840]   right? So I'm going to go back to hugging face, I'm going to go
[00:14:14.840 --> 00:14:21.560]   to models. And for this example, I'm actually going to try to
[00:14:21.560 --> 00:14:27.000]   find a, I'm going to try to use distill Bert. And I see that
[00:14:27.000 --> 00:14:33.240]   there is a distilled Bert based on case fine tune SST to
[00:14:33.240 --> 00:14:38.040]   English. So this is already doing a text classification. And
[00:14:38.040 --> 00:14:41.400]   it has two labels. And if you're competing in the competition,
[00:14:41.400 --> 00:14:45.160]   you already know that. So, and this may be one you want to
[00:14:45.160 --> 00:14:48.520]   explore if you're doing the competition. But since I am also
[00:14:48.520 --> 00:14:51.560]   going to be training a model that I didn't, that has two
[00:14:51.560 --> 00:14:55.640]   labels, right? One or zero positive or negative. I'm going
[00:14:55.640 --> 00:15:00.680]   to go ahead and just choose this model right here. And again,
[00:15:00.680 --> 00:15:06.280]   you click using transformers. And I'm going to copy the name
[00:15:06.280 --> 00:15:16.440]   of the checkpoint. And I am going to go here and, oops,
[00:15:16.440 --> 00:15:24.360]   put that in there. And then this is something you kind of saw
[00:15:24.360 --> 00:15:29.000]   last week where we just prepare the data set. I'm going to let
[00:15:29.000 --> 00:15:30.920]   this train for a little bit. And we're going to go back to the
[00:15:30.920 --> 00:15:34.280]   slides. But I did want to note the one thing you'll see here
[00:15:34.280 --> 00:15:36.840]   that's different than the examples last week is that I'm
[00:15:36.840 --> 00:15:41.800]   using this random subset splitter. And that is because
[00:15:41.800 --> 00:15:44.600]   the training corpus is pretty large. And it would probably
[00:15:44.600 --> 00:15:49.240]   take like an hour or several hours to actually train on this
[00:15:49.240 --> 00:15:52.680]   data set. And since we don't have that long and no one wants
[00:15:52.680 --> 00:15:56.200]   to sit there and watch my progress bar spinning around for
[00:15:56.200 --> 00:16:00.760]   three to four hours, what you can do is use this fast AI
[00:16:00.760 --> 00:16:05.400]   splitter. And what you do is you specify how much of the
[00:16:05.400 --> 00:16:09.960]   training set do you want to randomly include. And so I'm
[00:16:09.960 --> 00:16:14.280]   saying here 10%. And how much of the validation data set do
[00:16:14.280 --> 00:16:18.280]   you want to randomly include. So I'm saying just include 5%.
[00:16:18.280 --> 00:16:23.080]   And not only is this a good way to demo things in a timely
[00:16:23.080 --> 00:16:25.640]   fashion, but if you're experimenting and you just want
[00:16:25.640 --> 00:16:28.680]   to make sure that your data blocks are set up right, your
[00:16:30.120 --> 00:16:32.920]   learner set up correctly, it's good to start with a small
[00:16:32.920 --> 00:16:35.160]   subset, kind of go through that first to make sure all those
[00:16:35.160 --> 00:16:38.840]   bits are working before you let something train for a long time
[00:16:38.840 --> 00:16:41.880]   only to find out it breaks once it gets to the end of
[00:16:41.880 --> 00:16:47.080]   validation. So I'm going to go ahead and just start this stuff
[00:16:47.080 --> 00:16:52.360]   right here and then we'll come back to it. Any questions coming
[00:16:52.360 --> 00:16:53.320]   up yet, Sonia?
[00:16:53.320 --> 00:16:55.260]   Nope.
[00:16:55.260 --> 00:16:58.920]   All right. Well, as clear as mud, right?
[00:16:59.480 --> 00:17:01.480]   If I can understand, I'm sure everyone can.
[00:17:01.480 --> 00:17:03.720]   All right. That's good. That's good. Yeah, if you don't have
[00:17:03.720 --> 00:17:10.200]   any questions, I feel good. So yeah, so okay, so now we use the
[00:17:10.200 --> 00:17:14.280]   hub to find a data set, find a checkpoint. And now we're
[00:17:14.280 --> 00:17:18.120]   training. So after we're training, the next thing we're
[00:17:18.120 --> 00:17:21.480]   going to do is actually want to share our model with the world,
[00:17:21.480 --> 00:17:27.000]   right? And there's three options presented in the course. And
[00:17:27.000 --> 00:17:30.040]   I really after this, go through, if you haven't got to the
[00:17:30.040 --> 00:17:34.680]   course yet, you should really go through all the course videos
[00:17:34.680 --> 00:17:40.200]   and get an idea of, because there's a lot more details about
[00:17:40.200 --> 00:17:42.760]   how these things all work. But essentially, there's three
[00:17:42.760 --> 00:17:47.960]   options to share your models. One is you can use the push to
[00:17:47.960 --> 00:17:52.680]   hub API. And this is probably the simplest option. And
[00:17:52.680 --> 00:17:56.120]   essentially what you do is on your model and tokenizer
[00:17:56.120 --> 00:17:59.960]   object, you call a method called push to hub. So if you've, if
[00:17:59.960 --> 00:18:03.320]   you're using transformers, you have this, and you essentially
[00:18:03.320 --> 00:18:08.840]   say, this is my model name. And when you do that, it will
[00:18:08.840 --> 00:18:13.240]   actually go through the process of creating your repo on
[00:18:13.240 --> 00:18:17.240]   hugging face and pushing the necessary artifacts for folks
[00:18:17.240 --> 00:18:21.480]   to use your model. So we'll create the repo. And again, the
[00:18:21.480 --> 00:18:25.480]   repo is just a get repo. That's all it is. And so you use it
[00:18:25.480 --> 00:18:28.920]   and it works just like get and probably every single person
[00:18:28.920 --> 00:18:32.760]   here is familiar with, you know, how get works. And so how you
[00:18:32.760 --> 00:18:36.440]   work with a model, you've pushed the hugging face is
[00:18:36.440 --> 00:18:41.000]   exactly how you'd work with any other project you have that is
[00:18:41.000 --> 00:18:46.200]   a virgin controlled by using get. So this kind of automates
[00:18:46.200 --> 00:18:49.240]   the whole process of putting everything up there. And I
[00:18:49.240 --> 00:18:52.040]   think this is kind of the recommended option. Like I said,
[00:18:52.040 --> 00:18:55.080]   super easy to use. And we'll see this in the demo next.
[00:18:55.080 --> 00:19:01.480]   Another option is using the transformer CLI. And the steps
[00:19:01.480 --> 00:19:04.840]   are, there's a little bit more manual steps. So you log in
[00:19:04.840 --> 00:19:10.360]   with the transformer CLI or the hugging face CLI. Again, if you
[00:19:10.360 --> 00:19:15.000]   have installed transformers, the CLI is available to you. The
[00:19:15.000 --> 00:19:18.040]   hugging face CLI is something that you'd have to download
[00:19:18.040 --> 00:19:21.640]   separately. I don't know too much about it. I think it
[00:19:21.640 --> 00:19:25.960]   actually gives you probably greater superpowers. So folks
[00:19:25.960 --> 00:19:28.440]   want to chime in and have played with it. I'd be interested to
[00:19:28.440 --> 00:19:34.360]   hear your feedback. But yeah, once you once you log in, the
[00:19:34.360 --> 00:19:37.880]   next thing you do is you create your repo. And then once that's
[00:19:37.880 --> 00:19:41.320]   created, you're going to upload your model and token artifacts,
[00:19:41.320 --> 00:19:48.040]   just like you would anything else to get. And then the third
[00:19:48.040 --> 00:19:51.480]   option is using the web interface. And if you go to
[00:19:51.480 --> 00:19:56.040]   hugging face.code new, and then you assuming you have logged in,
[00:19:56.040 --> 00:20:00.680]   you basically can create a repo there. And then you can
[00:20:00.680 --> 00:20:04.680]   actually clone that repo locally, or you know, clone it
[00:20:04.680 --> 00:20:12.360]   on the collab or whatever, train your model. And then we saw like
[00:20:12.360 --> 00:20:17.400]   in I think week one or two, where to actually create those
[00:20:17.400 --> 00:20:22.040]   model and token artifacts, you just call safe pre trained. And
[00:20:22.040 --> 00:20:25.960]   so you call safe pre trained on your model, your tokenizer.
[00:20:25.960 --> 00:20:31.240]   And then once you have those, you're going to use get to, you
[00:20:31.240 --> 00:20:35.160]   actually need to do a get add dot to add all those files,
[00:20:35.160 --> 00:20:39.320]   commit them and then push them to the repo that's created right
[00:20:39.320 --> 00:20:42.200]   here. So it's a little bit more manual, but you also have a lot
[00:20:42.200 --> 00:20:44.920]   of control. And you can do a combination of both. So once
[00:20:44.920 --> 00:20:48.360]   you've used the push to hub, if you need to make changes, you
[00:20:48.360 --> 00:20:50.920]   could go ahead and get clone your repository, make changes,
[00:20:50.920 --> 00:20:59.320]   and then push those artifacts up there. So I'm going to go back
[00:20:59.320 --> 00:21:08.120]   here. And looks like my LR finder. That's kind of weird.
[00:21:12.440 --> 00:21:18.440]   Let's see if it trains here. So I don't know if Zach's on the
[00:21:18.440 --> 00:21:23.720]   call. I actually had some issues with LR finder yesterday,
[00:21:23.720 --> 00:21:27.800]   doing something similar. And so I'm not exactly sure what the
[00:21:27.800 --> 00:21:33.000]   problem is here. But if he's on here, take a look, Zach. I
[00:21:33.000 --> 00:21:36.520]   think this is your code. If not, I'll find you on discord.
[00:21:40.760 --> 00:21:45.480]   The professor's not on here. No. Okay. I'll find them later.
[00:21:45.480 --> 00:21:48.760]   It's just something recently I noticed that sometimes I get
[00:21:48.760 --> 00:21:51.320]   these errors. And sometimes I can rerun LR finder and it
[00:21:51.320 --> 00:21:54.280]   works. And so it's kind of a mystery to me as to what's
[00:21:54.280 --> 00:21:57.880]   going on. It could because of the random subset I have, like
[00:21:57.880 --> 00:22:01.800]   maybe there's something with that. That's my guess.
[00:22:01.800 --> 00:22:05.320]   Yeah, I would just shuffle it again. Sometimes it breaks for
[00:22:05.320 --> 00:22:08.120]   me. And I would just like rerun the random split again. And
[00:22:08.120 --> 00:22:12.360]   then it would work. Yeah. Yeah. Like I had something and this
[00:22:12.360 --> 00:22:16.520]   is a good kind of like FYI for folks is where I was doing a
[00:22:16.520 --> 00:22:21.880]   random subset to get things going quickly. And the random
[00:22:21.880 --> 00:22:25.480]   subset wasn't capturing all the classes. And it took me a while
[00:22:25.480 --> 00:22:31.160]   to figure out that's why I put this learn.dls.c because
[00:22:31.160 --> 00:22:33.560]   sometimes it was giving me just one class because that's
[00:22:33.560 --> 00:22:37.960]   randomly I have a very small subset. So it could be something
[00:22:37.960 --> 00:22:41.880]   with that. So LR finder might be fine. It might just be me.
[00:22:41.880 --> 00:22:46.680]   Also to understand how LR finder works, Aman is doing an
[00:22:46.680 --> 00:22:48.680]   awesome study group. You can find more details there. We
[00:22:48.680 --> 00:22:50.200]   won't dive into that.
[00:22:50.200 --> 00:22:54.040]   Oh, cool. Is he doing that this week? Or did he already cover
[00:22:54.040 --> 00:22:54.680]   it?
[00:22:54.680 --> 00:22:57.880]   I think it'll be in the next chapter if I'm not too wrong.
[00:22:57.880 --> 00:23:02.200]   Awesome. If you talk to him, what would be cool is to
[00:23:02.200 --> 00:23:07.080]   actually see because LR finder doesn't always work great for
[00:23:07.080 --> 00:23:10.280]   transformers. Like I noticed that the general principles of
[00:23:10.280 --> 00:23:14.040]   how you choose a learning rate, I typically go further back
[00:23:14.040 --> 00:23:17.640]   from what it says for transformers and it generally
[00:23:17.640 --> 00:23:21.960]   works better. So maybe I'll join that and ask him some tough
[00:23:21.960 --> 00:23:25.240]   questions on transformers and we'll see how good he is.
[00:23:25.240 --> 00:23:26.680]   Let's do that.
[00:23:26.680 --> 00:23:33.240]   So anyway, so yeah, so we're going training. So we got F1
[00:23:33.240 --> 00:23:38.680]   score of 0.8, basically almost 0.9 and 0.9 on accuracy.
[00:23:38.680 --> 00:23:45.560]   You can look at the results. Did pretty good. And so now we
[00:23:45.560 --> 00:23:52.360]   want to share. So if we look at, so this is my account. I
[00:23:52.360 --> 00:23:57.000]   don't have any models or data sets shared so far. And again,
[00:23:57.000 --> 00:23:59.720]   to be able to share, you have to have an account on Hugging
[00:23:59.720 --> 00:24:03.640]   Face. And the first thing, as I mentioned, we're going to do
[00:24:03.640 --> 00:24:08.040]   the push to hub approach is we have to call transformer CLI
[00:24:08.040 --> 00:24:11.960]   login. And again, since we obviously are using
[00:24:11.960 --> 00:24:15.000]   transformers, we have this. There's nothing additional you
[00:24:15.000 --> 00:24:24.440]   have to install. You just need to log in and it actually then
[00:24:24.440 --> 00:24:28.360]   creates a token and stores it for you. So you can actually
[00:24:28.360 --> 00:24:33.400]   push things to the repo that's created. The other thing is you
[00:24:33.400 --> 00:24:39.160]   want to make sure that you have Git LFS installed. So Git LFS
[00:24:39.160 --> 00:24:44.040]   is used for managing larger artifacts. So I think anything
[00:24:44.040 --> 00:24:48.120]   over 400 megabytes. And so obviously, depending on some
[00:24:48.120 --> 00:24:51.000]   of these models, you're going to see some pretty large like
[00:24:51.000 --> 00:24:55.320]   PyTorch.bin files, which is your model weights. And so you
[00:24:55.320 --> 00:24:59.160]   want to use make sure that this is installed either here in
[00:24:59.160 --> 00:25:03.560]   Colab or in your own system so that those things can get pushed
[00:25:03.560 --> 00:25:07.720]   correctly to GitHub. So I'm going to go through and just
[00:25:07.720 --> 00:25:17.320]   make sure that is installed. And then I also notice I usually
[00:25:17.320 --> 00:25:24.120]   have to set these two items, my username and email, just so you
[00:25:24.120 --> 00:25:27.320]   know, it's not going to push to your GitHub repo when you do
[00:25:27.320 --> 00:25:30.760]   this. But it needed this information in order to actually
[00:25:30.760 --> 00:25:35.720]   be able to share things correctly. So I'm going to go
[00:25:35.720 --> 00:25:39.800]   ahead and set both of those parameters. And then in blur, I
[00:25:39.800 --> 00:25:44.600]   have this Git blur TIFM. And it's nice because the blur
[00:25:44.600 --> 00:25:49.000]   transform, whichever one you're using, has access to all the
[00:25:49.000 --> 00:25:54.120]   hugging face objects. And so as I mentioned earlier, is that in
[00:25:54.120 --> 00:25:58.040]   order to push the hub, we have to push the model and tokenizer
[00:25:58.040 --> 00:26:02.200]   artifacts. And so blur TIFM gives us access to both the
[00:26:02.200 --> 00:26:06.840]   model and the tokenizer. So that's why I grabbed it. And so
[00:26:06.840 --> 00:26:09.720]   what we're going to do first is call this and I am going to
[00:26:09.720 --> 00:26:14.120]   create a pre trained model called session for IMDB model.
[00:26:14.120 --> 00:26:18.840]   I also noticed that when I'm using Colab, I have to pass this
[00:26:18.840 --> 00:26:22.680]   use temp directory. I'm not exactly sure why I don't know
[00:26:22.680 --> 00:26:26.680]   what it does. But other folks on the hugging face forums
[00:26:26.680 --> 00:26:32.680]   notice this as well. So let's go ahead and start this process
[00:26:32.680 --> 00:26:38.200]   of pushing our trained transformer to hugging face. And
[00:26:38.200 --> 00:26:42.600]   it's really this easy. It's these two lines of code. And
[00:26:42.600 --> 00:26:47.560]   it's running. I'm going to go back here, hit refresh. So you
[00:26:47.560 --> 00:26:52.680]   can see actually created the repo right there. Right. And
[00:26:52.680 --> 00:26:59.320]   if I go in here, you can see like now you can use like not
[00:26:59.320 --> 00:27:04.120]   quite yet, but you can see that this is just a get repo. And
[00:27:04.120 --> 00:27:07.880]   you can see it added my PyTorch model that's pretty big. The
[00:27:07.880 --> 00:27:13.480]   configuration, it has get attributes file in here. And so
[00:27:13.480 --> 00:27:19.640]   that takes care of the model. And then now we need to upload
[00:27:19.640 --> 00:27:25.320]   the tokenizer bits. So again, on the hugging face tokenizer,
[00:27:25.320 --> 00:27:29.880]   we call push to hub and the name of our model that we want
[00:27:29.880 --> 00:27:37.880]   to share with the rest of the world. And refresh here. And
[00:27:37.880 --> 00:27:40.840]   you can see that now all the tokenizer bits are in here as
[00:27:40.840 --> 00:27:45.800]   well. And so this is something that you can actually use. So
[00:27:45.800 --> 00:27:52.280]   any of you that wants to use my WGPUB session for IMDB model,
[00:27:52.280 --> 00:27:55.320]   it's now available to the entire world here. It was really
[00:27:55.320 --> 00:28:01.000]   that easy. And to show you just how easy it is to use. Once you
[00:28:01.000 --> 00:28:05.400]   have a model up there, you can just click using transformers,
[00:28:05.400 --> 00:28:08.200]   just like we did earlier when we were looking for a good
[00:28:08.200 --> 00:28:15.160]   checkpoint to start with. And we could copy and paste the
[00:28:15.160 --> 00:28:20.200]   name of our pre-trained model. Go here and we could actually
[00:28:20.200 --> 00:28:25.160]   use it. So here I'm just going to use the pipeline. Remember
[00:28:25.160 --> 00:28:30.520]   we did a text classification model. There's the name of my
[00:28:30.520 --> 00:28:35.080]   checkpoint. We could go ahead and run that. It downloads
[00:28:35.080 --> 00:28:43.320]   everything. Just like for any other transformer. And then
[00:28:43.320 --> 00:28:49.480]   hopefully if we built something that's decent, we could go
[00:28:49.480 --> 00:28:56.760]   ahead and say, "Oh, this song really sucked." And it gives us
[00:28:56.760 --> 00:29:03.000]   a negative score. So it's that easy to actually share and then
[00:29:03.000 --> 00:29:08.760]   be able to use the models from the hub. Any questions on any
[00:29:08.760 --> 00:29:13.240]   of that? There's a question from earlier. So Ravi's asking,
[00:29:13.240 --> 00:29:17.240]   how do you, let's say if you want to use a transformer for
[00:29:17.240 --> 00:29:21.000]   image classification and you're not sure where to start, how do
[00:29:21.000 --> 00:29:26.600]   you pick that model from the hub? You know, definitely I'm
[00:29:26.600 --> 00:29:29.640]   not the one to ask. I really have more experience working
[00:29:29.640 --> 00:29:36.520]   with the NLP transformers. So good question. I would hit up
[00:29:36.520 --> 00:29:41.240]   the Hugging Face forum or Twitter and ask folks, if you
[00:29:41.240 --> 00:29:43.960]   know what tasks you're looking to accomplish, let folks know,
[00:29:43.960 --> 00:29:46.200]   like, "Hey, I'm looking to use this. I want to use Hugging
[00:29:46.200 --> 00:29:49.320]   Face. I want to try one of the Vision Transformers." Anybody
[00:29:49.320 --> 00:29:54.040]   have any good links to tutorials or how-to's? But yeah, I don't
[00:29:54.040 --> 00:29:57.080]   know myself. Maybe someone else here on the group could answer
[00:29:57.080 --> 00:30:00.440]   that question too. I can chime in. So if you've just been like,
[00:30:00.440 --> 00:30:02.760]   you don't need to read the paper. If you've been following
[00:30:02.760 --> 00:30:06.040]   Twitter, you would know that, let's say, Convit is used for
[00:30:06.040 --> 00:30:09.320]   Blah or some other transformer is being used for Blah and it's
[00:30:09.320 --> 00:30:13.400]   SOTA or it was SOTA. So if you can just recall those names,
[00:30:13.400 --> 00:30:16.040]   those are a good start. And if you don't remember those, just
[00:30:16.040 --> 00:30:20.760]   try to search on archive sanity or even archive for that matter.
[00:30:20.760 --> 00:30:23.160]   And you should be able to find the paper and just start from
[00:30:23.160 --> 00:30:31.400]   there and you can go from there. Cool. Anything else? No, that's
[00:30:31.400 --> 00:30:40.760]   it. All right. So that's pretty straightforward. So as you're
[00:30:40.760 --> 00:30:45.800]   building these things, oh yeah, just a couple other quick notes.
[00:30:45.800 --> 00:30:50.520]   Make sure you get LFS installed or you're going to see
[00:30:50.520 --> 00:30:53.240]   interesting errors pop up and sometimes they're not super
[00:30:53.240 --> 00:30:57.160]   informative. And we already kind of covered that. And the only
[00:30:57.160 --> 00:31:00.840]   thing that I didn't do that's discussed in the course is to
[00:31:00.840 --> 00:31:04.200]   build a model card. And essentially that's just the
[00:31:04.200 --> 00:31:09.880]   readme markdown file that's at the root of your Git repository
[00:31:09.880 --> 00:31:14.440]   on HuggingFace. And essentially you want to include as much
[00:31:14.440 --> 00:31:17.800]   information you can as about how you trained it, the results
[00:31:17.800 --> 00:31:22.600]   you got, if there's any known issues with the model
[00:31:22.600 --> 00:31:25.800]   potentially or the dataset it was trained on. You want to
[00:31:25.800 --> 00:31:28.280]   share that information so that folks starting with your
[00:31:28.280 --> 00:31:31.800]   pre-trained model know the same information that you hopefully
[00:31:31.800 --> 00:31:36.280]   got from the pre-trained models you started with ahead of time.
[00:31:36.280 --> 00:31:42.280]   So if you look at the HuggingFace course, there's a last
[00:31:42.280 --> 00:31:45.240]   section on building a model card that shows you kind of the
[00:31:45.240 --> 00:31:49.320]   an example of all the bits to include. So you share stuff,
[00:31:49.320 --> 00:31:53.000]   take a look at that and try to do that. And you could build a
[00:31:53.000 --> 00:31:55.080]   model card just through the web interface too, which is
[00:31:55.080 --> 00:32:02.200]   really nice. With that, so homework is to watch the
[00:32:02.200 --> 00:32:06.120]   official course videos from week four, train and share a
[00:32:06.120 --> 00:32:11.240]   model on the Hub, and please consider entering one or more
[00:32:11.240 --> 00:32:14.760]   of the following competitions, best blog posts, best results,
[00:32:15.320 --> 00:32:19.320]   most interesting application. Deadline is August 3rd. You
[00:32:19.320 --> 00:32:26.120]   can email links to your work directly to me. And next week,
[00:32:26.120 --> 00:32:30.120]   we will be looking at what folks have presented, hearing
[00:32:30.120 --> 00:32:34.600]   about where Sanyam is going and doing a general Q&A. So
[00:32:34.600 --> 00:32:39.080]   anything that anybody wants to talk about with regards to the
[00:32:39.080 --> 00:32:43.480]   cores, transformers, fast AI, using those combination, we
[00:32:43.480 --> 00:32:48.520]   will cover all that. And that's all I have for today. So pretty
[00:32:48.520 --> 00:32:51.240]   straightforward. If there's any other questions or things that
[00:32:51.240 --> 00:32:53.160]   folks want to talk about, let's do it.
[00:32:53.160 --> 00:32:57.400]   Um, there's a question by Martin, how would we implement
[00:32:57.400 --> 00:33:00.760]   the ULM fit approach with HuggingFace transformers?
[00:33:00.760 --> 00:33:06.360]   So you really wouldn't. So that's a whole separate. So the
[00:33:06.360 --> 00:33:11.080]   ULM fit uses an LSTM. So it's a recurrent neural network. And
[00:33:11.080 --> 00:33:14.760]   transformers work entirely different. So it's entirely
[00:33:14.760 --> 00:33:19.000]   different architecture. So transformers is kind of built on
[00:33:19.000 --> 00:33:24.840]   this attention mechanism and being able to run things in, in
[00:33:24.840 --> 00:33:29.240]   parallel, whereas ULM fit is built on a recurrent neural
[00:33:29.240 --> 00:33:33.720]   network or an LSTM. And so the whole model, how it's trained
[00:33:33.720 --> 00:33:36.440]   and how you actually get predictions, it's, it's, it's
[00:33:36.440 --> 00:33:39.320]   different. Now what you could do, which would be interesting
[00:33:39.320 --> 00:33:41.960]   is if you look at the fast book is like we have this
[00:33:41.960 --> 00:33:49.240]   competition on SST2. Do a, train two models, train one on a, you
[00:33:49.240 --> 00:33:53.400]   know, state of the art transformer model and train
[00:33:53.400 --> 00:33:59.560]   another model using a ULM fit and present your, your results
[00:33:59.560 --> 00:34:01.960]   and see like, you know, how they both work, if they're
[00:34:01.960 --> 00:34:05.400]   comparable, or if one's better than the other, do some
[00:34:05.400 --> 00:34:08.120]   investigation and try to figure out like why that, why that is
[00:34:08.120 --> 00:34:11.000]   potentially.
[00:34:11.000 --> 00:34:14.280]   A follow up question on that, would there be no benefit in
[00:34:14.280 --> 00:34:17.800]   trying to fine tune transformers on a specific corpus with its
[00:34:17.800 --> 00:34:18.840]   specific language?
[00:34:18.840 --> 00:34:23.880]   Would there be? Say that again.
[00:34:23.880 --> 00:34:28.440]   Sorry, would there be no benefit in trying to fine tune
[00:34:28.440 --> 00:34:31.960]   transformers on a specific corpus with its language?
[00:34:33.240 --> 00:34:38.200]   I wouldn't say there's no benefit. If you have a
[00:34:38.200 --> 00:34:41.480]   transformer that's trained, that if I understand, I want to
[00:34:41.480 --> 00:34:43.400]   make sure I understand the question is that if you have a
[00:34:43.400 --> 00:34:47.080]   transformer that's trained on a specific language, is it
[00:34:47.080 --> 00:34:50.920]   worthwhile to fine tune that on your own data set in that
[00:34:50.920 --> 00:34:51.400]   language?
[00:34:51.400 --> 00:34:58.600]   Martin, can I, yes. So my question is, I mean, ULM fit as
[00:34:58.600 --> 00:35:02.840]   far as I understand it. One of the advantages is that we, if
[00:35:02.840 --> 00:35:05.880]   we have a very specific data set, like, I don't know, book
[00:35:05.880 --> 00:35:09.160]   titles or recipes or something like this, we can take a
[00:35:09.160 --> 00:35:12.200]   general kind of a model trained on general English language,
[00:35:12.200 --> 00:35:16.360]   fine tune the language model on that specific data set to learn
[00:35:16.360 --> 00:35:20.280]   more about the structure. And then on top of that, fine tune
[00:35:20.280 --> 00:35:24.440]   a classifier or whatever, whatever we want to achieve. And
[00:35:24.440 --> 00:35:28.040]   my question is, can we do the same kind of ULM fit type
[00:35:28.040 --> 00:35:30.760]   structure for transformers as well, kind of take a general
[00:35:30.760 --> 00:35:34.600]   English language transformer, fine tune it on recipes or
[00:35:34.600 --> 00:35:37.160]   something, and then train a classifier on that?
[00:35:37.160 --> 00:35:40.760]   Yep. Yeah, actually, in principle, they work the same.
[00:35:40.760 --> 00:35:45.160]   It's just that the language model, you know, when ULM fit
[00:35:45.160 --> 00:35:49.080]   is based on an LSTM, and then you put the, after you have that
[00:35:49.080 --> 00:35:53.560]   model, they put a classification head on top of it. And
[00:35:53.560 --> 00:35:58.120]   transformers work the same way. So all transformers, regardless
[00:35:58.120 --> 00:36:01.400]   of the downstream task, whether it's task classification, or
[00:36:01.400 --> 00:36:06.200]   named entity recognition, all of them start with a language
[00:36:06.200 --> 00:36:09.720]   model. And so you could actually take that language model, or
[00:36:09.720 --> 00:36:12.200]   take any of the language models, fine tune that just like you
[00:36:12.200 --> 00:36:16.680]   would with ULM fit. And then using that language model that
[00:36:16.680 --> 00:36:20.760]   you've even fine tuned on your corpus, then use it for
[00:36:20.760 --> 00:36:24.440]   classification or named entity recognition or whatever. So in
[00:36:24.440 --> 00:36:26.120]   principle, they work the same, they're just different
[00:36:26.120 --> 00:36:30.600]   architectures. But they work exactly like ULM fit where you
[00:36:30.600 --> 00:36:34.840]   can do that same type of process if you want.
[00:36:34.840 --> 00:36:39.320]   Okay, thank you.
[00:36:39.320 --> 00:36:43.640]   Yeah, yeah. So you can even like, kind of like what we did
[00:36:43.640 --> 00:36:47.640]   here, you could actually train a, like a mass language model,
[00:36:47.640 --> 00:36:50.440]   like with BERT, and actually kind of how we went through the
[00:36:50.440 --> 00:36:54.440]   process of push to hub, but we could use the save, pre trained
[00:36:54.440 --> 00:37:01.000]   methods. And then once we have that, we can go ahead and load,
[00:37:01.000 --> 00:37:04.840]   use that model as the basis for text classification in the same
[00:37:04.840 --> 00:37:07.400]   way that we've saw, like in the demos over the last four weeks.
[00:37:07.400 --> 00:37:11.320]   And Hugging Face will add that classification head for us. And
[00:37:11.320 --> 00:37:14.680]   then we can train our downstream classification task.
[00:37:14.680 --> 00:37:20.760]   I've shamelessly shared a link to a summary I had done of a
[00:37:20.760 --> 00:37:25.400]   paper. So this is not for transformers, but it's titled
[00:37:25.400 --> 00:37:29.880]   an embarrassingly simple approach for transfer learning.
[00:37:29.880 --> 00:37:33.480]   What I'm trying to allude here is these tricks might also work
[00:37:33.480 --> 00:37:36.120]   a few of these tricks would like work really nicely with
[00:37:36.120 --> 00:37:39.720]   transformers. So I could talk about them, but it's really
[00:37:39.720 --> 00:37:42.680]   straightforward. So those might also be worth exploring,
[00:37:42.680 --> 00:37:46.920]   because I haven't seen any such discussion with transformers as
[00:37:46.920 --> 00:37:50.200]   well. The authors basically talk about how how should you use
[00:37:50.200 --> 00:37:55.800]   exponential decay with learning rate. And so they also talk
[00:37:55.800 --> 00:37:58.440]   about an auxiliary loss when you're trying to do transfer
[00:37:58.440 --> 00:38:00.600]   learning. Again, I'm going on a tangent here, but I think it's
[00:38:00.600 --> 00:38:04.440]   relevant. So if you've pre trained on a certain task and
[00:38:04.440 --> 00:38:07.720]   you're doing transfer learning, so they talk about auxiliary
[00:38:07.720 --> 00:38:11.800]   loss. You have your original loss when you're fine tuning and
[00:38:11.800 --> 00:38:14.840]   you add an auxiliary loss and you sort of account for both of
[00:38:14.840 --> 00:38:19.080]   these things to make sure that the network works really well.
[00:38:19.080 --> 00:38:21.960]   When you basically you stack a few layers on top when you're
[00:38:21.960 --> 00:38:25.400]   doing transfer learning. So you would want a different loss for
[00:38:25.400 --> 00:38:28.840]   that and the original loss parametric for the original
[00:38:28.840 --> 00:38:32.360]   network. So that's what this paper talks about. And it's just
[00:38:32.360 --> 00:38:34.600]   experiments around those when you should do that when you
[00:38:34.600 --> 00:38:35.000]   shouldn't.
[00:38:35.000 --> 00:38:38.600]   That's cool. Yeah, no, thanks for that. I wasn't even aware of
[00:38:38.600 --> 00:38:40.600]   that. So that's actually really cool.
[00:38:40.600 --> 00:38:47.880]   Awesome. I don't see any other questions. So maybe you can wrap
[00:38:47.880 --> 00:38:48.040]   up.
[00:38:48.760 --> 00:38:52.760]   Yeah, I saw one down at the bottom about training scenarios
[00:38:52.760 --> 00:38:57.000]   that favor ULM fit over transformers. I just pulled up
[00:38:57.000 --> 00:38:57.800]   the chat.
[00:38:57.800 --> 00:39:03.720]   Yeah. And, you know, I've so back in the day, just
[00:39:03.720 --> 00:39:07.800]   anecdotally, I remember folks saying that on certain
[00:39:07.800 --> 00:39:13.800]   classification tasks, where the text is rather short, that they
[00:39:13.800 --> 00:39:19.320]   had better results with ULM fit than using a transformer. That
[00:39:19.320 --> 00:39:24.680]   was a while ago. And there's just been so much development in
[00:39:24.680 --> 00:39:27.800]   terms of transformer architectures, like there's so
[00:39:27.800 --> 00:39:33.240]   many, I don't know if that's the case anymore. And, you know, a
[00:39:33.240 --> 00:39:36.120]   good test would be to actually maybe find some short text
[00:39:36.120 --> 00:39:41.800]   corpus that's labeled and, and trained ULM fit on it and then
[00:39:41.800 --> 00:39:45.560]   and then choose like one of the more state of the art
[00:39:45.560 --> 00:39:51.400]   transformer architectures like Roberta or de Burda and see what
[00:39:51.400 --> 00:39:55.800]   the results look like. To me, I typically just use transformers.
[00:39:55.800 --> 00:40:05.800]   And I haven't seen where ULM fit is outperforming using a
[00:40:05.800 --> 00:40:09.880]   transformer and just my work but it's not, you know, by any
[00:40:09.880 --> 00:40:12.840]   means, you know, comprehensive, I haven't tested all you know,
[00:40:12.840 --> 00:40:15.400]   variations of playing with ULM fit and playing with, you know,
[00:40:15.400 --> 00:40:18.200]   every single transformer architecture. I typically use
[00:40:18.200 --> 00:40:23.080]   Roberta or like de Burda. And the results are as good in my
[00:40:23.080 --> 00:40:23.560]   experience.
[00:40:23.560 --> 00:40:31.720]   Awesome. I think that answers the question. Another one, any
[00:40:31.720 --> 00:40:35.240]   thoughts on using transformers on computer system data, like
[00:40:35.240 --> 00:40:39.480]   making a machine sentence or document from a machines event?
[00:40:39.800 --> 00:40:44.600]   Then train a transformer over it to predict anomaly events or
[00:40:44.600 --> 00:40:48.280]   next word. So are you trying to do language modeling? Just to
[00:40:48.280 --> 00:40:50.920]   be clear on the question or what what are you trying to
[00:40:50.920 --> 00:40:51.420]   predict?
[00:40:51.420 --> 00:40:58.120]   So like, instead of making sentences from normal sentences,
[00:40:58.120 --> 00:41:01.560]   normal words, English words, make sentences or documents
[00:41:01.560 --> 00:41:07.560]   from, let's say, systems binaries, let's say, calculator
[00:41:07.560 --> 00:41:14.840]   dot exe executes, makes network connection to this IP or writes
[00:41:14.840 --> 00:41:19.160]   maker did a right event on the system or let's say chrome dot
[00:41:19.160 --> 00:41:25.480]   exe launched and then did 100 network events, then did 50
[00:41:25.480 --> 00:41:30.760]   described events, something like that, and then see, and then
[00:41:30.760 --> 00:41:34.840]   train a language model over it to predict what will be the next
[00:41:34.840 --> 00:41:39.480]   event. And if let's say, the next event is deviated from the
[00:41:39.480 --> 00:41:45.080]   regular events, then market as an anomaly or things like that.
[00:41:45.080 --> 00:41:50.760]   Yeah, I think, you know, it reminds me of Sonia was in the
[00:41:50.760 --> 00:41:52.760]   fast book and the fast AI course, I think it's like in the
[00:41:52.760 --> 00:41:56.120]   first chapter, where Jeremy was showing like creative approaches
[00:41:56.120 --> 00:42:00.280]   for vision. And he's going to say that, yeah, yeah. And so
[00:42:00.280 --> 00:42:03.800]   like, he shows like, you could take like an audio clip and
[00:42:03.800 --> 00:42:08.200]   convert it, convert it to a visual representation, and then
[00:42:08.200 --> 00:42:12.440]   run it through CNN. And then he actually did something similar
[00:42:12.440 --> 00:42:17.560]   to what you're talking about, which was he actually, there's
[00:42:17.560 --> 00:42:21.880]   an example of where they're looking at computer viruses, and
[00:42:21.880 --> 00:42:24.840]   converted those to a visual representation. And then we use
[00:42:24.840 --> 00:42:25.880]   the map,
[00:42:25.880 --> 00:42:26.120]   Sanyam Bhutani
[00:42:26.120 --> 00:42:30.360]   how the person moves a cursor around? Oh, yeah, screen or
[00:42:30.360 --> 00:42:33.080]   something like that. And when it's a, like, there are very
[00:42:33.080 --> 00:42:35.800]   specific patterns that the model can pick up. And that that
[00:42:35.800 --> 00:42:38.920]   became like a huge success for the person at their company, and
[00:42:38.920 --> 00:42:41.880]   they actually deployed it. So there's this, like, this is an
[00:42:41.880 --> 00:42:43.000]   interesting project for you.
[00:42:43.000 --> 00:42:46.440]   Yeah, I think I think definitely go for it. Because I mean,
[00:42:46.440 --> 00:42:53.080]   really, if you can turn anything into a sequence of text is you
[00:42:53.080 --> 00:42:57.080]   could conceivably do exactly what you want to do and use
[00:42:57.080 --> 00:43:00.600]   transformers to create a language model behind it, and
[00:43:00.600 --> 00:43:04.920]   then actually then build downstream tasks. So I think
[00:43:04.920 --> 00:43:07.560]   that would be an interesting entry for the competition. Just
[00:43:07.560 --> 00:43:11.400]   saying so. But yeah, I think that would work.
[00:43:11.400 --> 00:43:17.160]   Just one more follow up on here. So in this case, I cannot use
[00:43:17.160 --> 00:43:20.840]   any pre trained models, right? Like because my system events
[00:43:20.840 --> 00:43:24.600]   would be like the vocabulary would be very different than the
[00:43:24.600 --> 00:43:25.320]   English words.
[00:43:28.680 --> 00:43:32.760]   If you follow sendex, so he shared, I don't remember the
[00:43:32.760 --> 00:43:37.000]   name of the model, but that was trained on GitHub data. So he's
[00:43:37.000 --> 00:43:40.760]   like trying to open source co pilot in his own way. That might
[00:43:40.760 --> 00:43:44.200]   be a good start. Usually it's a good start to like just pick up
[00:43:44.200 --> 00:43:47.320]   a pre trained model because you don't want it to like learn how
[00:43:47.320 --> 00:43:48.760]   the language is structured at all.
[00:43:48.760 --> 00:43:54.200]   And the other thing is, even if the LM you start with isn't
[00:43:54.920 --> 00:43:59.560]   super related to what you're looking at is really, it's just
[00:43:59.560 --> 00:44:02.920]   it's giving you a starting point and then you can train it to
[00:44:02.920 --> 00:44:09.000]   one degree to another, right? So if it's more, if the pre
[00:44:09.000 --> 00:44:11.880]   trained language model you start with the data set is a lot
[00:44:11.880 --> 00:44:14.040]   different than your data set, you just maybe are going to have
[00:44:14.040 --> 00:44:17.240]   to train it more and spend more time training it. But like
[00:44:17.240 --> 00:44:20.120]   Sonia, I'm saying it doesn't hurt to start with something,
[00:44:20.120 --> 00:44:23.800]   even if it's not exactly representative of the text you
[00:44:23.800 --> 00:44:24.300]   have.
[00:44:24.940 --> 00:44:25.440]   Okay.
[00:44:25.440 --> 00:44:37.740]   Um, I think we have 10 minutes. So I could say as a solution,
[00:44:37.740 --> 00:44:40.700]   just like do a paper summary of the paper actually, because I
[00:44:40.700 --> 00:44:43.020]   think it's interesting and it'll make for like a good
[00:44:43.020 --> 00:44:46.220]   experiment or project for people who might want to submit
[00:44:46.220 --> 00:44:48.140]   along that. That's okay with others.
[00:44:51.260 --> 00:44:52.700]   According to dictionary.com.
[00:44:52.700 --> 00:44:56.540]   You can activate it by Google Sonia. So
[00:44:56.540 --> 00:45:01.420]   I'm sorry, there's a question. Let me quickly read it.
[00:45:01.420 --> 00:45:06.380]   It's a suggestion. You can check that link out by Shane.
[00:45:06.380 --> 00:45:09.900]   Oh, yeah. Yeah, that's a good link. I've looked at it a few
[00:45:09.900 --> 00:45:12.220]   times. So that kind of goes through like the whole process
[00:45:12.220 --> 00:45:16.860]   of building a language model and then applying that to I think
[00:45:16.860 --> 00:45:20.060]   a classification task. So yeah, that's a nice. That's a nice
[00:45:20.060 --> 00:45:20.560]   resource.
[00:45:21.440 --> 00:45:26.720]   Okay, should I do the paper summary? If it's helpful?
[00:45:26.720 --> 00:45:27.520]   Yeah, go for it.
[00:45:27.520 --> 00:45:31.760]   Awesome. Could you let me share? Yeah.
[00:45:31.760 --> 00:45:32.800]   Stop my share.
[00:45:32.800 --> 00:45:37.520]   All right, go. Go for it.
[00:45:37.520 --> 00:45:42.000]   Awesome. So the series, and this was a two year old blog post.
[00:45:42.000 --> 00:45:45.120]   So please forgive me. I'm pretty sure I would forget, forget the
[00:45:45.120 --> 00:45:48.080]   details. But it's called five minute paper summary. So I
[00:45:48.080 --> 00:45:52.000]   will try to summarize this in less than that. The reason I'm
[00:45:52.000 --> 00:45:54.400]   talking about this, even though the paper is slightly dated,
[00:45:54.400 --> 00:46:00.400]   because they just talked about RNN models, but it's it should
[00:46:00.400 --> 00:46:02.720]   be helpful for just like general experiments. And you could
[00:46:02.720 --> 00:46:05.840]   literally do this, do the same experiments and write about
[00:46:05.840 --> 00:46:09.200]   transformers, which I know I haven't seen any such post, I
[00:46:09.200 --> 00:46:12.400]   might not be following everything. So again, the paper
[00:46:12.400 --> 00:46:15.280]   is called an embarrassingly simple approach for transfer
[00:46:15.280 --> 00:46:18.320]   learning from pre trained language models. And for
[00:46:18.320 --> 00:46:22.400]   context, this was around the time when these was just
[00:46:22.400 --> 00:46:26.960]   starting to pick up. For further context, like using
[00:46:26.960 --> 00:46:30.880]   pre trained language models wasn't common at all in NLP.
[00:46:30.880 --> 00:46:34.720]   Jeremy and Sebastian Ruder actually showed us through the
[00:46:34.720 --> 00:46:38.480]   ULM fit paper that this works really well. It's not just for
[00:46:38.480 --> 00:46:40.640]   computer vision. Surprisingly, it was just being used for
[00:46:40.640 --> 00:46:43.840]   computer vision. Now we use it for pretty much, you know,
[00:46:43.840 --> 00:46:48.080]   it for pretty much everything in NLP and even in beyond for
[00:46:48.080 --> 00:46:53.920]   GANs as well. So at that time, the authors, Alexandra et al
[00:46:53.920 --> 00:46:57.920]   showed how you should apply transfer learning and they were
[00:46:57.920 --> 00:47:00.320]   sort of comparing their approaches and calling them
[00:47:00.320 --> 00:47:05.360]   embarrassingly simple. I talk about transfer learning in this
[00:47:05.360 --> 00:47:08.000]   post, I'll skip that because I'll assume everyone knows what
[00:47:08.000 --> 00:47:10.800]   transfer learning is, but you're just trying to utilize a model
[00:47:10.800 --> 00:47:16.240]   and use its domain knowledge or whatever it's been trained on
[00:47:16.240 --> 00:47:19.200]   as a starting point. So you don't have to waste all of that
[00:47:19.200 --> 00:47:25.440]   computation. There are five model details that the authors
[00:47:25.440 --> 00:47:30.320]   say are important and these address three issues. At that
[00:47:30.320 --> 00:47:33.280]   time, transformers are much more computationally expensive,
[00:47:33.280 --> 00:47:37.840]   so this did not age well at all. Sophisticated training
[00:47:37.840 --> 00:47:41.840]   techniques, again, let's say did not age well with that as
[00:47:41.840 --> 00:47:45.680]   well and catastrophic for getting, I would say with
[00:47:45.680 --> 00:47:49.680]   transformers, I think it's still there because you need further
[00:47:49.680 --> 00:47:53.760]   more data, like for most of them. So these are still active
[00:47:53.760 --> 00:47:57.360]   issues, but here's the approach. I spoke about using an auxiliary
[00:47:57.360 --> 00:48:03.760]   loss, but on the left is the LSTM model. Reminder, this was
[00:48:03.760 --> 00:48:06.960]   at the time when LSTMs were all the erased, they were the SOTA
[00:48:06.960 --> 00:48:10.800]   models. So on the left, you can see the pre-trained model and
[00:48:10.800 --> 00:48:14.000]   it's learning how to say quick, brown, fox, jumps, whatever
[00:48:14.000 --> 00:48:17.920]   that sentence is. And now you're adding a linear layer to, let's
[00:48:17.920 --> 00:48:22.160]   say, do a different task. So the first suggestion is using an
[00:48:22.160 --> 00:48:26.000]   auxiliary loss for that and using the loss for the original
[00:48:26.000 --> 00:48:29.840]   model as well, because this has been trained in a certain way
[00:48:29.840 --> 00:48:33.360]   and you would also want to make sure that you don't... When you're
[00:48:33.360 --> 00:48:36.640]   fine tuning, you're also changing these weights inside of
[00:48:36.640 --> 00:48:40.640]   H1, H2 and all of these hidden layers. Sorry, W1, W2, these
[00:48:40.640 --> 00:48:42.880]   weights are being changed and you wouldn't want to change them
[00:48:42.880 --> 00:48:49.200]   too much. So how do we account for that? They call it Seattle
[00:48:49.200 --> 00:48:53.280]   or they call it Seattle. These steps are called single step,
[00:48:53.280 --> 00:48:58.880]   auxiliary, loss and transfer. So first of all, they train a
[00:48:58.880 --> 00:49:02.000]   language model, a word level language model, pretty standard
[00:49:02.000 --> 00:49:05.440]   at the time, hasn't changed since. And they add an auxiliary
[00:49:05.440 --> 00:49:09.920]   loss. So loss isn't simply loss of the task. It's also plus
[00:49:09.920 --> 00:49:14.720]   gamma. I typed Y because I was lazy and I don't know how to
[00:49:14.720 --> 00:49:20.480]   type in latex. Y times loss of the language model. So you
[00:49:20.480 --> 00:49:24.720]   train the language model and now you're fine tuning it. You've
[00:49:24.720 --> 00:49:30.160]   added the task specific layer. And the trick here is using
[00:49:30.160 --> 00:49:35.040]   exponential decay of Y. This Y that I just mentioned, your
[00:49:35.040 --> 00:49:38.480]   exponentially decaying it and logically it makes sense because
[00:49:38.480 --> 00:49:44.400]   when we add these layers, so these few layers Y1, Y2, Y3 and
[00:49:44.400 --> 00:49:48.560]   YT right above the linear layer, these are untrained, right? So
[00:49:48.560 --> 00:49:51.520]   these are randomly or initialized with some level of
[00:49:51.520 --> 00:49:54.800]   smartness weights. You initialize weights with different
[00:49:54.800 --> 00:49:57.360]   methods. That's a completely different discussion. I'll skip
[00:49:57.360 --> 00:50:02.000]   that. But these are untrained and you would want to guide them
[00:50:02.000 --> 00:50:05.520]   and eventually exponentially decay that. So that's why we are
[00:50:05.520 --> 00:50:09.360]   exponentially decaying it. The other thing is sequential
[00:50:09.360 --> 00:50:11.760]   unfreezing. If I remember correctly, this was also there
[00:50:11.760 --> 00:50:17.680]   in the ULMFIT paper. But the argument here is these few
[00:50:17.680 --> 00:50:22.640]   layers closer to the input. So earlier layers in the network,
[00:50:22.640 --> 00:50:25.440]   earlier means closer to the input are trained really well.
[00:50:25.440 --> 00:50:30.640]   The one towards the output, not so much. So first you would
[00:50:30.640 --> 00:50:34.800]   want to train the outer bits in the image, the one on the top,
[00:50:34.800 --> 00:50:40.000]   followed by these, followed by these. So it's sequentially
[00:50:40.000 --> 00:50:44.160]   unfreezing them and training them bit by bit. You could also
[00:50:44.160 --> 00:50:47.760]   apply differential learning rates. FastA does that. So you
[00:50:47.760 --> 00:50:51.360]   could account for, hey, you know what, these layers, they are
[00:50:51.360 --> 00:50:54.720]   like word language models and they do the job pretty well. So
[00:50:54.720 --> 00:50:57.840]   you wouldn't want to change them too much, maybe a little for
[00:50:57.840 --> 00:51:00.880]   your task. So that's another trick you could try.
[00:51:00.880 --> 00:51:07.040]   And I think that's about it. They talk about optimizers. I'm
[00:51:07.040 --> 00:51:11.440]   pretty sure Adam is a more common one than SGD. Again, this
[00:51:11.440 --> 00:51:14.560]   paper was a while ago. That's why when we say that the field
[00:51:14.560 --> 00:51:18.640]   moves really fast, it's quite fast. And then they talk about
[00:51:18.640 --> 00:51:22.640]   how they compared against ULMFIT and the results were pretty
[00:51:22.640 --> 00:51:27.600]   interesting. And I did that in, so I failed myself. I did
[00:51:27.600 --> 00:51:31.360]   that in seven minutes. But are there any questions around this?
[00:51:31.360 --> 00:51:40.240]   I don't see any questions, but that's actually a really
[00:51:40.240 --> 00:51:42.960]   interesting approach right there. And it looks pretty easy
[00:51:42.960 --> 00:51:48.320]   to implement. So basically, if we go back to, if you scroll up
[00:51:48.320 --> 00:51:53.520]   a little bit to the, to the loss visualization. So if I
[00:51:53.520 --> 00:51:56.160]   understand this right, you basically do a cross entropy
[00:51:56.160 --> 00:52:02.800]   loss on the LM, right? Yes. And then you add that to the cross
[00:52:02.800 --> 00:52:06.400]   entropy loss. If you're doing multi-classification multiplied
[00:52:06.400 --> 00:52:10.880]   by this gamma parameter that's decayed over the course of your
[00:52:10.880 --> 00:52:17.280]   training. Yeah, that's super cool. It makes sense too. It's
[00:52:17.280 --> 00:52:20.320]   amazing how easy some of these ideas are initially, initially
[00:52:20.320 --> 00:52:23.360]   when you bring up like, Oh, taking the LM loss with the
[00:52:23.360 --> 00:52:26.240]   classification loss, it sounds like, Oh my gosh, how does that
[00:52:26.240 --> 00:52:32.560]   work? And it's really as easy as loss plus gamma times loss. And
[00:52:32.560 --> 00:52:33.040]   that's it.
[00:52:33.040 --> 00:52:38.320]   Yeah, that's cool. Papers with code is such a cool website.
[00:52:38.320 --> 00:52:41.520]   Like, I hope they get better at it. But if you could just, you
[00:52:41.520 --> 00:52:45.120]   know, like select a equation inside of a paper and literally
[00:52:45.120 --> 00:52:48.240]   see the code pop up like that would be so cool. Because I also
[00:52:48.240 --> 00:52:50.640]   like really suck at understanding what does that
[00:52:50.640 --> 00:52:54.160]   equation mean? Yeah, that's cool. But by the way, someone
[00:52:54.160 --> 00:52:57.360]   in chat asked if we could extend the August 3rd deadline to
[00:52:57.360 --> 00:53:02.240]   Thursday. And I just said, Sure, why not? So the deadline is now
[00:53:02.240 --> 00:53:02.800]   Thursday.
[00:53:02.800 --> 00:53:11.280]   Awesome. Okay. Since there are no further questions, maybe you
[00:53:11.280 --> 00:53:14.560]   can wrap up. As a reminder, there are three different tracks
[00:53:14.560 --> 00:53:17.760]   that are open for submission. So please consider submitting and
[00:53:17.760 --> 00:53:24.160]   thanks for joining. We'll see you all next week. All right. Bye
[00:53:24.160 --> 00:53:25.440]   everyone. Thank you. Thanks.

