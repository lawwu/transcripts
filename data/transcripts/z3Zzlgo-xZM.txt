
[00:00:00.000 --> 00:00:02.580]   All right, everybody. Welcome back to the all in podcast, the
[00:00:02.580 --> 00:00:08.360]   number one podcast in the world. Episode 173. It's objectively
[00:00:08.360 --> 00:00:11.760]   freeberg the number one podcast I checked, I looked online. And
[00:00:11.760 --> 00:00:15.960]   things are cooking over here. So much so that you may have heard
[00:00:15.960 --> 00:00:21.720]   we hired a new CEO. Welcome to the team. Our new fifth bestie
[00:00:21.720 --> 00:00:29.280]   john. Yeah. Clap. Yes. All right, john. Welcome to the
[00:00:29.280 --> 00:00:32.480]   program. Your first day was April 1. It wasn't a joke
[00:00:32.480 --> 00:00:38.880]   literally was your first day. How has week one as CEO for all
[00:00:38.880 --> 00:00:39.560]   in been?
[00:00:39.560 --> 00:00:43.840]   It has been wonderful, super dynamic, really happy to be a
[00:00:43.840 --> 00:00:44.560]   part of the team.
[00:00:44.560 --> 00:00:50.200]   All right, there you have it. Sax you were a huge driver of
[00:00:50.200 --> 00:00:53.520]   this. You spent so much time interviewing everybody going
[00:00:53.520 --> 00:00:57.440]   through the resumes checking the references. You know, and you
[00:00:57.440 --> 00:00:59.400]   really spearheaded this. Oh, wait, you did nothing. I
[00:00:59.400 --> 00:01:02.840]   forgot. It's it's spelled j o n, john.
[00:01:02.840 --> 00:01:04.040]   Who is this?
[00:01:04.040 --> 00:01:10.960]   Second, john. Okay, guys, have a good day. Nice to meet you.
[00:01:10.960 --> 00:01:11.880]   Likewise.
[00:01:11.880 --> 00:01:15.920]   Let your winners ride.
[00:01:15.920 --> 00:01:18.840]   Rain Man David Saxon.
[00:01:18.840 --> 00:01:25.200]   We open source it to the fans and they've just gone crazy.
[00:01:25.600 --> 00:01:32.400]   Love you. So john worked at the Russian embassy. This is just a
[00:01:32.400 --> 00:01:36.520]   coincidence. Russia references, but he worked as an intern for
[00:01:36.520 --> 00:01:39.960]   Putin. Somehow he wound up getting a gig. Freeberg. You
[00:01:39.960 --> 00:01:45.960]   actually led the the incredible search here. We had hundreds of
[00:01:45.960 --> 00:01:50.280]   people apply. Why do you think we wound up with Mr. Hell here?
[00:01:50.280 --> 00:01:52.760]   Yeah, we had a lot of folks. And we met with a lot of folks. But
[00:01:52.760 --> 00:01:57.080]   john really stood out with, I think his experience and his
[00:01:57.080 --> 00:02:01.880]   thoughtfulness about what we can do. So, so much of our work off
[00:02:01.880 --> 00:02:05.200]   the show obviously has gone into putting on the all in summit and
[00:02:05.200 --> 00:02:09.800]   we want to do more live events. And john has a very strong
[00:02:09.800 --> 00:02:13.480]   background in building incredible live experiences and
[00:02:13.480 --> 00:02:15.440]   events, which we think is going to be a really important
[00:02:15.440 --> 00:02:18.560]   extension. I think we've we realized over the last two
[00:02:18.560 --> 00:02:21.200]   summits, how much community matters for all in and how much
[00:02:21.200 --> 00:02:24.560]   getting people together matters and how much the live content
[00:02:24.560 --> 00:02:27.360]   mattered. And so we want to do more of that. And hopefully john
[00:02:27.360 --> 00:02:30.920]   can take us to the promised land. Fantastic, john. Thanks
[00:02:30.920 --> 00:02:33.640]   for thanks for saying yes. It's awesome to have. Yeah. All
[00:02:33.640 --> 00:02:39.000]   right. And as john's first first duty, he is going to next week
[00:02:39.000 --> 00:02:43.880]   announce the details of the all in summit 2024. Our third
[00:02:43.880 --> 00:02:47.480]   edition. Chairman Dictator Chamath Palihapitiya. You've
[00:02:47.480 --> 00:02:52.000]   been running all in with an iron fist in the group chat, your
[00:02:52.000 --> 00:02:57.240]   thoughts on john in working with him and why we selected the
[00:02:57.240 --> 00:02:58.280]   iron fist, please go.
[00:02:58.280 --> 00:03:06.760]   I think that there's a really important trend that we have
[00:03:06.760 --> 00:03:13.440]   stumbled into, which is that content creators are the modern
[00:03:13.440 --> 00:03:18.040]   form of demand generation, for whatever else it is that you're
[00:03:18.040 --> 00:03:21.200]   going to consume. And I think it replaces advertising, and I
[00:03:21.200 --> 00:03:26.240]   think it displaces traditional content. And so I was really
[00:03:26.240 --> 00:03:29.520]   interested in finding somebody that understood how to connect
[00:03:29.520 --> 00:03:32.680]   those dots and all of the different ways in which we can
[00:03:32.680 --> 00:03:37.240]   explore what our brand is capable of. And I thought he was
[00:03:37.240 --> 00:03:40.240]   the best example of having done one thing extremely well at
[00:03:40.240 --> 00:03:44.720]   scale. And curious enough to figure out the other parts. So,
[00:03:44.720 --> 00:03:46.400]   you know, I'm really excited to work with john.
[00:03:46.400 --> 00:03:49.920]   Yeah. And I'll just add to that, you know, been doing events my
[00:03:49.920 --> 00:03:53.760]   whole life. And when I saw john's actual event history in
[00:03:53.760 --> 00:03:57.400]   the event, you've thrown john really spectacular in the
[00:03:57.400 --> 00:04:00.800]   detail. And I think you're gonna the same way freeberg level up
[00:04:00.800 --> 00:04:03.840]   year two of the conference. I'm really excited to see how you
[00:04:03.840 --> 00:04:06.560]   put your stamp on it and level up year three. A couple of
[00:04:06.560 --> 00:04:08.600]   housekeeping things here. You know, we
[00:04:08.640 --> 00:04:10.680]   by the way, sorry, can I say one thing? Absolutely. Like you
[00:04:10.680 --> 00:04:15.800]   said, Yeah, there is so much room to build real communities.
[00:04:15.800 --> 00:04:22.400]   And I think people are just so tired and bored with everything
[00:04:22.400 --> 00:04:28.760]   online. So I think offline experiences will be a huge value
[00:04:28.760 --> 00:04:29.880]   ad in people's lives.
[00:04:29.880 --> 00:04:33.200]   Yeah, there's definitely a bridge from online to offline.
[00:04:33.200 --> 00:04:37.960]   And the all in meetups that Ray's been hosting the best the
[00:04:37.960 --> 00:04:42.720]   best example. No, the best example of at scale of online to
[00:04:42.720 --> 00:04:46.960]   offline are dating apps. And all I can see is that's a pretty
[00:04:46.960 --> 00:04:50.600]   dissatisfying experience for a lot of people. And so outside of
[00:04:50.600 --> 00:04:53.400]   dating apps, there aren't many really great examples where
[00:04:53.400 --> 00:04:59.400]   like minded people can hang out and have fun and you know, talk
[00:04:59.400 --> 00:05:00.920]   learn party.
[00:05:00.920 --> 00:05:03.240]   It's really interesting. You bring this up. I just finished
[00:05:03.880 --> 00:05:08.160]   John hates new book, The Anxious Generation. And if you've
[00:05:08.160 --> 00:05:10.720]   listened to it, but there's a pair of books out right now, bad
[00:05:10.720 --> 00:05:14.760]   therapy. And this one about kids. And the premise of this
[00:05:14.760 --> 00:05:18.960]   book shamoff is screen time, as you referred to, you know,
[00:05:18.960 --> 00:05:24.000]   people being online. Not only is it bad for kids and adults, it's
[00:05:24.000 --> 00:05:28.880]   also blocking to your point real world connection. And so we all
[00:05:28.880 --> 00:05:31.480]   want a little more real world connection. And so john, welcome
[00:05:31.480 --> 00:05:35.680]   to the team. Additionally, I'll just two more housekeeping items
[00:05:35.680 --> 00:05:38.200]   here and we'll get to the show. We're gonna have a 1 million
[00:05:38.200 --> 00:05:40.960]   subscriber party. If you want to be part of that 1 million
[00:05:40.960 --> 00:05:43.120]   subscriber party, you can increase your chances without
[00:05:43.120 --> 00:05:45.600]   announcing how we're going to give the tickets away. But you
[00:05:45.600 --> 00:05:48.360]   can increase your chances by going to YouTube right now pause
[00:05:48.360 --> 00:05:51.760]   the show, subscribe to the all in podcast channel, hit the bell.
[00:05:51.760 --> 00:05:54.560]   So you get the alert for the best chance of getting one of
[00:05:54.560 --> 00:05:57.400]   the golden tickets to the 1 million subscriber party. And we
[00:05:57.400 --> 00:06:02.000]   are at 486,000. When we add 14,000 more, we're going to do a
[00:06:02.000 --> 00:06:06.600]   live q&a with all the besties. So get in on that as well. john,
[00:06:06.600 --> 00:06:11.280]   any any parting thoughts here or comments? Which bestie has been
[00:06:11.280 --> 00:06:14.560]   the most difficult to work with in the first month? Which one
[00:06:14.560 --> 00:06:15.840]   has been the most delightful go?
[00:06:15.840 --> 00:06:20.120]   It's been a pleasure across the board. I think you know, part of
[00:06:20.120 --> 00:06:22.360]   the beauty of the show is that there are so many different
[00:06:22.360 --> 00:06:25.400]   personalities and viewpoints. And really happy to be part of
[00:06:25.400 --> 00:06:27.520]   the team excited for what we're going to continue building.
[00:06:27.520 --> 00:06:29.960]   How diplomatic who's been the worst to deal with? Who's been
[00:06:29.960 --> 00:06:34.000]   the terror? Be honest. Tell me. We'll talk about it in your one
[00:06:34.000 --> 00:06:36.800]   year review. All right, john. Great job.
[00:06:36.800 --> 00:06:41.560]   Like, who's been the most erotic? Likely freeberg? Who's
[00:06:41.560 --> 00:06:45.320]   read the most value add? Likely me. And who's been the most
[00:06:45.320 --> 00:06:48.200]   am I the most unresponsive?
[00:06:51.480 --> 00:06:56.040]   That's why I get to vote proxy. Yeah, exactly. I mean, it's it's
[00:06:56.040 --> 00:06:59.480]   just wild. What's going on here? It really is Game of Thrones.
[00:06:59.480 --> 00:07:02.760]   And you have been put in the center of a john. Congratulations.
[00:07:02.760 --> 00:07:07.200]   Welcome to the Iron Throne. Watch your back. All right,
[00:07:07.200 --> 00:07:11.280]   dismissed, john. Let us know. Thanks, john. We can be helpful.
[00:07:11.280 --> 00:07:15.280]   Salute to you, john. We got to get going here. We got a big
[00:07:15.280 --> 00:07:19.520]   show for you, everybody. Welcome to the show. Officially David
[00:07:19.520 --> 00:07:23.600]   Sachs, your rain man, Chamath Bhaiya Bhatia, chairman
[00:07:23.600 --> 00:07:27.320]   dictator, Sultan of science, David freeberg. I'm the world's
[00:07:27.320 --> 00:07:31.200]   greatest moderator. Welcome to the program. A quick correction
[00:07:31.200 --> 00:07:35.920]   up top that many of you in the crypto space, let us know about
[00:07:35.920 --> 00:07:39.800]   immediately after the episode dropped. We talked about SPF
[00:07:39.800 --> 00:07:43.640]   getting 25 years. And that broke right as we started the show.
[00:07:43.640 --> 00:07:47.000]   And we discussed that the customers of FTX we're going to
[00:07:47.000 --> 00:07:49.400]   get made whole, there's been a lot of speculation about them
[00:07:49.400 --> 00:07:55.800]   being made whole. However, there was an important note, FTX
[00:07:55.800 --> 00:07:58.920]   deposits are getting paid back in US dollars, not the crypto
[00:07:58.920 --> 00:08:02.480]   that dollar amount we've learned is based on the price of their
[00:08:02.480 --> 00:08:04.960]   tokens at the bankruptcy date. The bankruptcy date was November
[00:08:04.960 --> 00:08:11.640]   11, in 2022. super important. Because the report that started
[00:08:11.640 --> 00:08:14.800]   the run on FTX was published November 2, a couple of days in
[00:08:14.800 --> 00:08:19.160]   between those two dates, and a bunch of crypto plummeted Solana
[00:08:19.160 --> 00:08:22.520]   dropped 50% between November 5 and November 11. That's just one
[00:08:22.520 --> 00:08:27.320]   example. But since then, Solana has been up 11 x and bitcoins up
[00:08:27.320 --> 00:08:31.480]   4x, Ethereum doubled. So if you want to these depositors, you
[00:08:31.480 --> 00:08:35.240]   miss that run up. And so FTX customers were rightfully
[00:08:35.240 --> 00:08:39.440]   furious. I'll pause there before I get into more details. Any any
[00:08:39.440 --> 00:08:40.840]   thoughts on this sex?
[00:08:40.840 --> 00:08:45.280]   Yeah, I mean, just to hit the nail on the head here, if you
[00:08:45.320 --> 00:08:50.320]   had left your Solana at FTX, you're going to get $16 per
[00:08:50.320 --> 00:08:54.920]   token back. And that apparently was the price at the time that
[00:08:54.920 --> 00:08:59.960]   they went under. So according to the the judicial proceedings,
[00:08:59.960 --> 00:09:02.680]   you've been quote unquote made whole. But the truth is that
[00:09:02.680 --> 00:09:06.720]   Solana at this moment is trading at $188. So you have not been
[00:09:06.720 --> 00:09:10.480]   made whole. And this is why the crypto community is furious. And
[00:09:10.480 --> 00:09:13.360]   so that that basically is the correction. Now, I thought it
[00:09:13.360 --> 00:09:17.240]   was interesting that the judge played into this notion. And we
[00:09:17.240 --> 00:09:20.680]   talked about that quote from the judge last week, where he said,
[00:09:20.680 --> 00:09:24.960]   that if you go to Vegas, abscond with your customers money, gamble
[00:09:24.960 --> 00:09:28.400]   it and then pay them off with the winnings, then you've still
[00:09:28.400 --> 00:09:32.040]   committed a crime. He seemed to be conceding this idea that the
[00:09:32.040 --> 00:09:36.000]   investors or the depositors at FTX have been made whole. Clearly
[00:09:36.000 --> 00:09:39.240]   they have not been, but his quote kind of lent credence to
[00:09:39.240 --> 00:09:42.120]   that. And the reason the judge was talking about is because
[00:09:42.320 --> 00:09:46.880]   SPF lawyers were clearly making this argument that his sentence
[00:09:46.880 --> 00:09:51.480]   should be commuted or reduced, because it pausers have been
[00:09:51.480 --> 00:09:54.720]   made whole. And I think what you saw in the media coverage is
[00:09:54.720 --> 00:09:59.160]   that the reporters were buying into this idea of depositors
[00:09:59.160 --> 00:10:01.920]   being made whole. I mean, you guys got this from somewhere,
[00:10:01.920 --> 00:10:05.840]   right? I mean, this is what the media coverage. So the media was
[00:10:05.840 --> 00:10:09.120]   doing what has been doing throughout the FTX case, which
[00:10:09.120 --> 00:10:13.960]   is carrying water for SPF. And I believe that this narrative that
[00:10:13.960 --> 00:10:17.080]   they're trying to concoct, which we now know is completely false,
[00:10:17.080 --> 00:10:20.680]   is designed to serve a purpose. And I think that purpose is to
[00:10:20.680 --> 00:10:26.200]   get SPF, either pardoned or have his sentence commuted. Because
[00:10:26.200 --> 00:10:30.420]   Mr. Bankman and Miss Freed are huge Democratic Party bundlers.
[00:10:30.420 --> 00:10:34.880]   And I think the goal here is to create the idea in the public's
[00:10:34.880 --> 00:10:38.920]   mind that people weren't really hurt by this. This was just sort
[00:10:38.920 --> 00:10:43.600]   of youthful indiscretion or hijinks. And, you know, it's a
[00:10:43.600 --> 00:10:47.720]   bunch of hijinks, right? shenanigans, but shenanigans
[00:10:47.720 --> 00:10:50.040]   where no one was really hurt. And if they can create that
[00:10:50.040 --> 00:10:54.160]   impression in the public's mind, pardon my can now set up getting
[00:10:54.160 --> 00:10:58.080]   one of their Democratic Party connections. Yeah, to help push
[00:10:58.080 --> 00:11:00.320]   for a commutation of the sentence. I think there's a
[00:11:00.320 --> 00:11:03.040]   narrative going on. I think there's an agenda behind the
[00:11:03.040 --> 00:11:05.640]   narrative. And it's interesting what I'm saying here.
[00:11:06.040 --> 00:11:08.680]   Yeah, this pardon power is really powerful. Chamath, any
[00:11:08.680 --> 00:11:13.120]   thoughts here on the bankruptcy judge, making that call to sell
[00:11:13.120 --> 00:11:16.840]   the chairs, because obviously, if crypto had tanked since that
[00:11:16.840 --> 00:11:19.120]   time, it would look like he saved the money by selling them,
[00:11:19.120 --> 00:11:21.560]   clearing the positions and giving them cash. But what is
[00:11:21.560 --> 00:11:24.640]   the right thing for the bankruptcy judge to do here,
[00:11:24.640 --> 00:11:28.880]   keep the equities, the tokens, or to sell it and freeze it in
[00:11:28.880 --> 00:11:31.400]   time? It seems like a very difficult one.
[00:11:31.400 --> 00:11:35.280]   We got to correct that for just in a certain way. So the trustee
[00:11:35.280 --> 00:11:40.200]   has been selling the tokens post run up. The point is that he's
[00:11:40.200 --> 00:11:44.520]   selling tokens at current prices, call it 188. And then
[00:11:44.520 --> 00:11:49.600]   using that to pay off depositors at $16. So the only reason the
[00:11:49.600 --> 00:11:52.800]   depositors have been quote unquote made whole, is because
[00:11:52.800 --> 00:11:54.680]   they're getting the benefit of this run up, but they're not
[00:11:54.680 --> 00:11:58.640]   paying them back. At the price of their salon today, they're
[00:11:58.640 --> 00:12:01.480]   paying them at this price that got fixed at the time of the
[00:12:01.480 --> 00:12:05.680]   bankruptcy. The truth is, this is no one's getting made whole.
[00:12:05.680 --> 00:12:09.200]   Yeah, this is the, you know, the really hard thing to track here,
[00:12:09.200 --> 00:12:12.160]   because we couldn't find when they were selling it or how much
[00:12:12.160 --> 00:12:15.560]   they've been selling this seems to be being done in the shadows
[00:12:15.560 --> 00:12:17.640]   or in the background. And so if anybody out there as we
[00:12:17.640 --> 00:12:21.720]   crowdsource what's going on here, wants to keep us up to
[00:12:21.720 --> 00:12:25.160]   date, let us know. But yeah, these tokens, some number of
[00:12:25.160 --> 00:12:27.480]   them got sold at a low price, some of them are getting sold, I
[00:12:27.480 --> 00:12:32.480]   guess, as time goes on, Chamath, your just thoughts on how to do
[00:12:32.480 --> 00:12:35.240]   this properly? What's the proper hygiene here? I mean, it's,
[00:12:35.240 --> 00:12:36.680]   we're not a great experts.
[00:12:36.680 --> 00:12:40.760]   That's a great question. I don't know the differences between
[00:12:40.760 --> 00:12:45.120]   chapter seven and chapter 11 bankruptcy law, but I don't know
[00:12:45.120 --> 00:12:48.280]   what was filed here was a chapter seven or chapter 11. I
[00:12:48.280 --> 00:12:53.280]   don't, I don't know. But it seems that this was the only
[00:12:53.280 --> 00:12:56.440]   thing that they could do, which was to liquidate into a common,
[00:12:57.160 --> 00:13:00.760]   you know, unit of measure. Because at the end of the day,
[00:13:00.760 --> 00:13:05.400]   their auditors had to measure in a standard unit, and that was
[00:13:05.400 --> 00:13:08.080]   probably the US dollar. And so then they were trying to work
[00:13:08.080 --> 00:13:12.280]   backwards from that shareholder equity number to get them back
[00:13:12.280 --> 00:13:17.240]   to that number. So it was kind of logical that that this is the
[00:13:17.240 --> 00:13:20.720]   only thing they could do. And I guess they benefited from the
[00:13:20.720 --> 00:13:23.040]   fact that there was a run up, but it's really unfortunate for
[00:13:23.040 --> 00:13:26.800]   folks. So I don't know, maybe in other countries, had this been a
[00:13:26.800 --> 00:13:29.600]   differently constituted company organized in a different place.
[00:13:29.600 --> 00:13:32.040]   Bankruptcy law could have allowed the liquidator to
[00:13:32.040 --> 00:13:34.840]   actually just distribute the assets on a pro rata basis.
[00:13:34.840 --> 00:13:40.520]   This was a chapter. I got the update here. This is a chapter
[00:13:40.520 --> 00:13:43.920]   11. And in September, the judge allowed FT x to start
[00:13:43.920 --> 00:13:47.560]   liquidating up to 100 million a week in Delaware, right? This is
[00:13:47.560 --> 00:13:51.120]   a chapter 11 Delaware filing. Yeah, chapter 11 in Delaware,
[00:13:51.120 --> 00:13:53.920]   correct. And this could increase to 200 million a week. So it
[00:13:53.920 --> 00:13:57.040]   seems like they did, they did start the liquidation later. So
[00:13:57.040 --> 00:13:59.040]   they might have caught some of the run ups. So they did catch
[00:13:59.040 --> 00:14:01.000]   the run up, then you could be doubly upset.
[00:14:01.000 --> 00:14:03.760]   Right. My point, my point is, in different in different
[00:14:03.760 --> 00:14:07.600]   situations, one could imagine where the shareholders could
[00:14:07.600 --> 00:14:11.240]   have been allowed to vote. Do you want money? Or do you want
[00:14:11.240 --> 00:14:15.320]   in kind? And if in kind, maybe you get a pro rata distribution
[00:14:15.320 --> 00:14:18.400]   of all the assets, which have included a whole bunch of these
[00:14:18.400 --> 00:14:21.040]   coins, but then it probably would have included a bunch of
[00:14:22.440 --> 00:14:25.480]   other assets, not just Solana and Bitcoin, then eat that
[00:14:25.480 --> 00:14:26.600]   ripped. So
[00:14:26.600 --> 00:14:29.520]   I think the point is that if they had distributed in kind,
[00:14:29.520 --> 00:14:32.720]   meaning tokens, that people would have seen, oh, wait, I
[00:14:32.720 --> 00:14:35.520]   only got back one 10th, the number of tokens that I put in.
[00:14:35.520 --> 00:14:39.480]   That's the point, right? Yeah, you put in 100 Solana tokens,
[00:14:39.480 --> 00:14:42.720]   you only get back, call it roughly 10. Because the price
[00:14:42.720 --> 00:14:46.280]   of those tokens was fixed at 16. And they can now sell at
[00:14:46.280 --> 00:14:50.160]   somewhere between 100 and 200. They're able to, quote, make you
[00:14:50.160 --> 00:14:53.240]   hold the $16 price, but that's not being made whole, that was
[00:14:53.240 --> 00:14:55.760]   the what are they doing with the extra money sacks? Because are
[00:14:55.760 --> 00:14:59.800]   they using that to make other people in this whole crater hole
[00:14:59.800 --> 00:15:01.600]   as well? So maybe they're thinking holistically, they're
[00:15:01.600 --> 00:15:04.360]   taking the profits of those Solana holders, let's say, or
[00:15:04.360 --> 00:15:07.520]   Bitcoin holders went for x. And is that going to trickle down to
[00:15:07.520 --> 00:15:09.960]   the equity holders? Who knows? Freeberg, you have thoughts?
[00:15:09.960 --> 00:15:14.200]   Yeah, I think the plan is that that excess capital beyond what
[00:15:14.200 --> 00:15:17.160]   is quote, owed to the account holders goes to the equity
[00:15:17.160 --> 00:15:20.600]   holders, because it's considered excess of the liabilities.
[00:15:20.600 --> 00:15:24.800]   Therefore, it goes to the shareholders. I'll also say, in
[00:15:24.800 --> 00:15:28.920]   a traditional like brokerage, you create an account. And when
[00:15:28.920 --> 00:15:31.520]   you set up an account, your account has a currency
[00:15:31.520 --> 00:15:35.720]   denomination. It's a dollar based account or euro based
[00:15:35.720 --> 00:15:39.400]   account. And then you your account holds a bunch of assets.
[00:15:39.400 --> 00:15:42.640]   And so at any given time, the value of your account is
[00:15:42.640 --> 00:15:46.680]   represented to you in that local currency. The challenge with
[00:15:46.680 --> 00:15:49.600]   crypto exchanges is that there's often this representation of a
[00:15:49.600 --> 00:15:53.000]   wallet, which is meant to hold assets that don't necessarily
[00:15:53.000 --> 00:15:55.920]   have the intention of being translated into a locally
[00:15:55.920 --> 00:15:58.880]   denominated currency. And so I think that's what makes the
[00:15:58.880 --> 00:16:01.840]   system different in the case of a US exchange bankruptcy. And
[00:16:01.840 --> 00:16:04.400]   this happens in commodity trading accounts or commodity
[00:16:04.400 --> 00:16:07.440]   exchanges. Often, there's a freezing of the assets and then
[00:16:07.440 --> 00:16:10.640]   a liquidation of the assets, where the freezing of the assets
[00:16:10.640 --> 00:16:13.920]   sets the price or the value at the moment of what you're
[00:16:13.920 --> 00:16:17.120]   supposed to hold in that account in your currency of your
[00:16:17.120 --> 00:16:21.440]   account. And so the bankruptcy judge and the trustee are
[00:16:21.440 --> 00:16:26.160]   treating this like a liquidation process using a local currency
[00:16:26.160 --> 00:16:29.600]   that was set at the time, whereas many folks don't
[00:16:29.600 --> 00:16:32.360]   consider that the intention of the account that the account was
[00:16:32.360 --> 00:16:35.800]   meant to hold assets, that it's really a portfolio of assets
[00:16:35.800 --> 00:16:39.040]   that shouldn't be liquidated to try and generate local currency
[00:16:39.040 --> 00:16:40.960]   because that's kind of the whole point of many of these crypto
[00:16:41.800 --> 00:16:45.760]   currencies themselves. It was a custodial account, basically,
[00:16:45.760 --> 00:16:50.080]   right, a custodial account of assets versus a trading account
[00:16:50.080 --> 00:16:53.920]   of which is meant to ultimately be converted back into a local
[00:16:53.920 --> 00:16:57.600]   currency, which is typical. And I think that's what makes this
[00:16:57.600 --> 00:17:01.640]   such a challenging process. Yeah, just liquidate everything.
[00:17:01.640 --> 00:17:06.760]   Pool of money arrives, distribute the money. But the
[00:17:06.760 --> 00:17:09.760]   way bankruptcy works is that there's a pecking order that
[00:17:09.760 --> 00:17:12.600]   essentially you you have all the assets of the company, the
[00:17:12.600 --> 00:17:15.040]   job of the trustee is to liquidate them, they have a
[00:17:15.040 --> 00:17:17.600]   fiduciary duty to get the highest price they can for those
[00:17:17.600 --> 00:17:20.360]   assets, we have no reason to believe that they haven't, they
[00:17:20.360 --> 00:17:23.560]   seem to have waited a decent enough amount of time to get to
[00:17:23.560 --> 00:17:26.560]   benefit from this crypto recovery. And then what happens
[00:17:26.560 --> 00:17:28.360]   is, again, there's like a pecking order for the
[00:17:28.360 --> 00:17:31.760]   distribution of the proceeds. And you're going to have debt
[00:17:31.760 --> 00:17:34.440]   holders, they're going to be senior to the equity holders,
[00:17:34.440 --> 00:17:38.360]   the depositors are going to be high up there as well.
[00:17:38.480 --> 00:17:40.800]   Government agencies that are owed fines are going to be high
[00:17:40.800 --> 00:17:43.360]   up there, there's gonna be a very specific pecking order in
[00:17:43.360 --> 00:17:44.800]   which the equity holders are last,
[00:17:44.800 --> 00:17:47.160]   if we want to go into conspiracy corner and put our tinfoil hats
[00:17:47.160 --> 00:17:51.440]   on, you mentioned the IRS, the CFTC, right, these government
[00:17:51.440 --> 00:17:55.680]   agencies are owned like over $20 billion. If all this crypto
[00:17:55.680 --> 00:17:59.240]   profits, if they're higher in the stack, they would be going
[00:17:59.240 --> 00:18:01.120]   directly to the government and the government is handling the
[00:18:01.120 --> 00:18:02.960]   process here. So it doesn't look right.
[00:18:02.960 --> 00:18:05.440]   But I don't I don't think I don't think it's that's driving
[00:18:05.440 --> 00:18:08.360]   it. I think that I think bankruptcy rules are very
[00:18:08.360 --> 00:18:13.040]   specific, and they're completely designed around, again,
[00:18:13.040 --> 00:18:19.440]   assessing what the value of each claimant is, at the time of
[00:18:19.440 --> 00:18:21.680]   bankruptcy, and then creating a pecking order for distribution.
[00:18:21.680 --> 00:18:25.760]   So I don't think there's any foul play here. I think that
[00:18:25.760 --> 00:18:29.120]   this is just the way that the cookie crumbles. But I think
[00:18:29.120 --> 00:18:33.960]   that it's simply wrong, or misleading, to say that the
[00:18:33.960 --> 00:18:37.680]   depositors were made whole. And I think the reason we thought
[00:18:37.680 --> 00:18:41.840]   that is because of these statements that were promulgated
[00:18:41.840 --> 00:18:44.840]   through the media, including the judge to give people that
[00:18:44.840 --> 00:18:48.280]   impression. And I think it really just underscores that the
[00:18:48.280 --> 00:18:52.200]   media always has an agenda. And you got to be so careful about
[00:18:52.200 --> 00:18:56.560]   imbibing their narratives. Because without knowing exactly
[00:18:56.560 --> 00:18:59.960]   what their agenda is, you can imbibe their bias.
[00:18:59.960 --> 00:19:03.400]   Yeah, my understanding of bankruptcy is like the they can
[00:19:03.400 --> 00:19:06.840]   get a little bit creative, and the bankruptcy judges can be a
[00:19:06.840 --> 00:19:09.640]   little creative and trying to holistically think about what's
[00:19:09.640 --> 00:19:11.720]   the best thing for the business and all the stakeholder
[00:19:11.720 --> 00:19:15.240]   shareholders. But anyway, we'll keep monitoring it. And like
[00:19:15.240 --> 00:19:17.760]   we've said before, anytime we make a mistake, we're going to
[00:19:17.760 --> 00:19:20.760]   talk about it right up front. Anytime there's an omission or
[00:19:20.760 --> 00:19:24.280]   breaking news happens, we're going to fill you in. And that
[00:19:24.280 --> 00:19:27.200]   leads us to our second topic, some more news broke about truth
[00:19:27.200 --> 00:19:30.200]   social, we had a nice spicy discussion about it last week.
[00:19:30.440 --> 00:19:36.400]   Since that time, shares of TMTG, or dollar sign D, DJ T have
[00:19:36.400 --> 00:19:41.760]   dropped 30%. And the numbers for their revenue also confirmed it
[00:19:41.760 --> 00:19:44.920]   came out pretty ugly 4 million in revenue, 15 million losses,
[00:19:44.920 --> 00:19:48.680]   floats only 30 million shares. But there are some follow up
[00:19:48.680 --> 00:19:51.160]   stories here we'll get into Trump is suing the two co
[00:19:51.160 --> 00:19:54.560]   founders to reduce their combined 8.6 state to zero his
[00:19:54.560 --> 00:19:56.840]   arguments co founders set the company up improperly and
[00:19:56.840 --> 00:19:59.320]   mishandled the launch of true social so they should forfeit
[00:19:59.320 --> 00:20:02.080]   their stock. These are both co founders previously on The
[00:20:02.080 --> 00:20:06.600]   Apprentice, Trump owned 60%. And he stands for receiving earn out
[00:20:06.600 --> 00:20:10.320]   of 36 million additional shares in the coming week, worth almost
[00:20:10.320 --> 00:20:14.200]   2 billion. So huge windfall for President Trump coming. In
[00:20:14.200 --> 00:20:16.920]   addition to that, Sacha, you're gonna love this Russia, Russia,
[00:20:16.920 --> 00:20:20.800]   Russia, the Guardian is reporting TMTG raised $8
[00:20:20.800 --> 00:20:24.320]   million in emergency funding that might have possibly come
[00:20:24.320 --> 00:20:28.560]   from a Russian linked entity. The SPAC while it was on hold,
[00:20:28.920 --> 00:20:32.360]   was running out of cash, Russia linked entity. You read this all
[00:20:32.360 --> 00:20:34.840]   about this in the Guardian, they tried to raise some money, they
[00:20:34.840 --> 00:20:37.480]   wound up raising 8 million across two convertible notes
[00:20:37.480 --> 00:20:41.080]   from a bank called Paxium located in the Caribbean, owned
[00:20:41.080 --> 00:20:45.920]   by a Russian who is reportedly the nephew of Alexander
[00:20:45.920 --> 00:20:49.080]   Smirnoff, who used to work in Putin's executive office until
[00:20:49.080 --> 00:20:53.320]   2017. Is this a joke? No, I mean, I wish it was a joke,
[00:20:53.320 --> 00:20:56.560]   because I know I'm gonna get barbecued for, like by the by
[00:20:56.560 --> 00:20:59.840]   the Trump supporters in the comments. But it is crazy that
[00:20:59.840 --> 00:21:01.040]   the Russians are
[00:21:01.040 --> 00:21:05.360]   his name, Alexander Smirnoff. Isn't that the name of a vodka?
[00:21:05.360 --> 00:21:10.560]   Kind of is s m i r n o v smirnoff. Maybe from doesn't
[00:21:10.560 --> 00:21:13.640]   even drink. I mean, maybe Yeah, I don't know if Putin drinks or
[00:21:13.640 --> 00:21:20.600]   not. Anyway, Trump doesn't drink. That's true. Yeah. If
[00:21:20.600 --> 00:21:26.440]   it does. Oh, really? Is that true? Yeah. So anyway, important
[00:21:26.440 --> 00:21:29.160]   caveats to all this stuff. Interesting story, though.
[00:21:29.160 --> 00:21:31.960]   There's no indication that Trump or Trump media had any idea
[00:21:31.960 --> 00:21:36.000]   about the nature of these loans, because they were opaque. And
[00:21:36.000 --> 00:21:40.840]   Trump media says it's propaganda and false narrative. Your
[00:21:40.840 --> 00:21:43.680]   thoughts on the Trump's back sacks and in Russia, and it's
[00:21:43.680 --> 00:21:44.400]   your favorite topic?
[00:21:44.400 --> 00:21:48.720]   Well, we're, it's gonna be a really long seven months, if
[00:21:48.720 --> 00:21:51.000]   we're going to bring up every one of these evidence free
[00:21:51.000 --> 00:21:53.960]   stories of some sort of Russia connection to Trump. I mean,
[00:21:54.360 --> 00:21:56.920]   this article doesn't even make sense. Like you said, here's the
[00:21:56.920 --> 00:21:59.720]   giveaway in the middle of the article, they say the Guardian
[00:21:59.720 --> 00:22:03.440]   does, quote, there is no indication that Trump or Trump
[00:22:03.440 --> 00:22:05.800]   media had any idea about the nature of the loans beyond that
[00:22:05.800 --> 00:22:09.160]   they were opaque, nor has the company or its executives been
[00:22:09.160 --> 00:22:12.320]   accused of wrongdoing. So then what are we talking about here?
[00:22:12.320 --> 00:22:15.240]   It just there's some guy with a Russian sounding last name. I
[00:22:15.240 --> 00:22:17.400]   mean, literally, that's the story. It doesn't even make
[00:22:17.400 --> 00:22:20.240]   sense. I don't understand how you can get a loan and not know
[00:22:20.240 --> 00:22:24.200]   who your counterparty is. So this whole thing just seems like
[00:22:24.240 --> 00:22:28.680]   it's part of the milieu of let's create any connections we can
[00:22:28.680 --> 00:22:33.040]   between Trump and Russia. And the giveaway on this was
[00:22:33.040 --> 00:22:36.760]   actually a New York Times story that just came out in the past
[00:22:36.760 --> 00:22:39.600]   week. It was called Russia amps up online campaign against
[00:22:39.600 --> 00:22:43.880]   Ukraine before us elections. Now. The interesting thing about
[00:22:43.880 --> 00:22:49.720]   this story is that it was reporting allegations by Clint
[00:22:49.720 --> 00:22:55.720]   Watts, who has apparently been hired by Microsoft to run
[00:22:55.720 --> 00:22:58.840]   something called the Threat Analysis Center. And his job
[00:22:58.840 --> 00:23:02.160]   basically is to find Russian interference in the election.
[00:23:02.160 --> 00:23:05.600]   Now, here's what I found interesting about this is that
[00:23:05.600 --> 00:23:10.200]   Clint Watts that that name rang a bell for me. And it's because
[00:23:10.200 --> 00:23:15.480]   Clint Watts was involved in the Twitter files. So back in
[00:23:15.640 --> 00:23:22.080]   January of last year, Matt IEB broke this story in the Twitter
[00:23:22.080 --> 00:23:27.360]   files that Clint Watts was running the Hamilton 68
[00:23:27.360 --> 00:23:29.960]   dashboard. He was basically behind that project. He's a
[00:23:29.960 --> 00:23:34.480]   former FBI agent. What was Hamilton 68 Hamilton 68 claim
[00:23:34.480 --> 00:23:39.160]   that it was tracking 500 Russian accounts on social media, who
[00:23:39.160 --> 00:23:44.840]   were engaged in manipulation of online discourse? Well, as it
[00:23:44.840 --> 00:23:48.440]   turns out, executives inside of Twitter knew these accounts were
[00:23:48.440 --> 00:23:50.840]   and they were just American accounts, some Canadian
[00:23:50.840 --> 00:23:53.720]   accounts. And in the words of Twitter executives, the whole
[00:23:53.720 --> 00:23:58.320]   Hamilton 68 dashboard was bullshit. That was their word.
[00:23:58.320 --> 00:24:02.880]   And nevertheless, this Hamilton 68 project that Clint Watts ran,
[00:24:02.880 --> 00:24:07.040]   put out story after story for years about how the Russians
[00:24:07.040 --> 00:24:11.040]   were meddling in American political debates. And these
[00:24:11.040 --> 00:24:15.080]   stories, the Hamilton 68 claims became the basis for 1000s
[00:24:15.080 --> 00:24:19.640]   literally 1000s of mainstream media stories, claiming that
[00:24:19.640 --> 00:24:23.400]   Russia was interfering in American politics. It all turned
[00:24:23.400 --> 00:24:27.300]   out to be a total hoax, a total fraud. Now, the amazing thing to
[00:24:27.300 --> 00:24:31.520]   me is you would think after an expose like this, that it would
[00:24:31.520 --> 00:24:34.480]   at least be mentioned in the New York Times, that the person
[00:24:34.480 --> 00:24:38.520]   they're quoting, as saying that the Russians are meddling in our
[00:24:38.520 --> 00:24:43.560]   elections, has a previous history of setting up Russia
[00:24:43.560 --> 00:24:47.400]   hoaxes. And they don't even mention that, despite the type
[00:24:47.400 --> 00:24:52.520]   of story. Moreover, it's amazing to me that this Watts guy, not
[00:24:52.520 --> 00:24:56.480]   only landed on his feet, he got a cushy job at Microsoft running
[00:24:56.480 --> 00:24:59.280]   their threats analysis center, so that he could put out more of
[00:24:59.280 --> 00:25:02.640]   this threat analysis on how the Russians were meddling. I can't
[00:25:02.640 --> 00:25:06.920]   imagine a less qualified person to be describing Russia threats
[00:25:06.920 --> 00:25:11.040]   than somebody who was caught red handed, manufacturing bogus
[00:25:11.040 --> 00:25:12.160]   threats for years,
[00:25:12.160 --> 00:25:15.280]   for burgers on any event, Jason, I would just say that, you know,
[00:25:15.280 --> 00:25:18.960]   going back to the SVF, the story, the takeaway there is, be
[00:25:18.960 --> 00:25:22.260]   careful what you're imbibing from the mainstream media,
[00:25:22.260 --> 00:25:26.080]   because there's always an agenda. And until this story
[00:25:26.080 --> 00:25:29.220]   about true social from the Guardian has a little bit more
[00:25:29.220 --> 00:25:32.160]   detail to it, that makes sense to me, I'm just going to put it
[00:25:32.160 --> 00:25:33.800]   in the category of more of the same.
[00:25:34.280 --> 00:25:37.560]   Yeah. And you know, as I've said on this program before, Russia's
[00:25:37.560 --> 00:25:40.200]   explicit strategies, just put their fingerprints on everything
[00:25:40.200 --> 00:25:43.320]   and cause chaos like they did with the Internet Research
[00:25:43.320 --> 00:25:45.840]   Agency and all the trolling they were doing the last couple of
[00:25:45.840 --> 00:25:48.600]   elections, free burger thoughts on this, you're concerned about
[00:25:48.600 --> 00:25:49.920]   the interference in the election.
[00:25:49.920 --> 00:25:54.440]   I'm not gonna know my thoughts on this. Yeah, this is this is
[00:25:54.440 --> 00:25:56.640]   nothing I feel like I should be thoughtful about.
[00:25:56.640 --> 00:25:59.600]   Okay, sounds good. All right. And then wrapping up, I had
[00:25:59.600 --> 00:26:03.520]   mentioned last episode, about the insider trading charges in
[00:26:03.520 --> 00:26:09.120]   the company that was the SPAC before they purchased true
[00:26:09.120 --> 00:26:13.600]   social, and the three, two of the three men have been charged
[00:26:13.600 --> 00:26:16.840]   with insider trading just pleaded guilty. So Michael and
[00:26:16.840 --> 00:26:21.200]   Gerald Schwartzman made 23 million in illicit profits
[00:26:21.200 --> 00:26:23.840]   trading shares of DWAC before the merger was announced. They
[00:26:23.840 --> 00:26:28.040]   each pled guilty to one count of securities fraud, face three to
[00:26:28.040 --> 00:26:32.960]   five years, neither was involved with truth. This is the SPAC that
[00:26:32.960 --> 00:26:36.520]   came before truth. And there it is, folks. So there's your
[00:26:36.520 --> 00:26:38.880]   update, what's the relevance of this to being a top issue on the
[00:26:38.880 --> 00:26:41.560]   online pod? I just think this is gonna be a really long seven. I
[00:26:41.560 --> 00:26:43.480]   think it's gonna be a really long seven months for us till
[00:26:43.480 --> 00:26:46.960]   the election if we're going to bring up every mainstream media
[00:26:46.960 --> 00:26:50.320]   story that seeks to create a connection between this is an
[00:26:50.320 --> 00:26:53.320]   SEC filing. Yeah, it's done. This one has nothing to do with
[00:26:53.320 --> 00:26:54.640]   Russia. This is the insider trading one.
[00:26:54.640 --> 00:26:57.840]   But you just said it has no connection to true social. So
[00:26:57.840 --> 00:26:58.520]   why are we even talking?
[00:26:58.520 --> 00:27:02.120]   No, no, it's the these are the people who traded the stock
[00:27:02.160 --> 00:27:05.720]   before truth social merged with it. This is the SPAC that was
[00:27:05.720 --> 00:27:09.120]   social, and they traded when they found out that Trump was
[00:27:09.120 --> 00:27:11.480]   going to be the SPAC that was merged with. So I'm just
[00:27:11.480 --> 00:27:13.800]   following up closing the loop on that breaking news. Google is
[00:27:13.800 --> 00:27:17.000]   reportedly considering making an offer to hire, acquire HubSpot
[00:27:17.000 --> 00:27:20.600]   trading a $34 billion market cap that just came out, reported by
[00:27:20.600 --> 00:27:23.880]   Reuters, who cited anonymous sources shares up 7% on the
[00:27:23.880 --> 00:27:27.840]   news. If you don't know HubSpot, awesome tool, we use it, CRM
[00:27:27.840 --> 00:27:29.760]   that blends marketing, sales, customer service, all that kind
[00:27:29.760 --> 00:27:34.200]   of good stuff. And so here's their quarterly revenue since
[00:27:34.200 --> 00:27:37.600]   IPO. They've been public for 10 years, as you pointed out in the
[00:27:37.600 --> 00:27:40.560]   group chat, Friedberg, and this has been slow and steady
[00:27:40.560 --> 00:27:44.480]   revenue, power of SAS, I guess, and a great product. I'm not a
[00:27:44.480 --> 00:27:46.800]   shareholder. Here's the quarterly revenue growth on a
[00:27:46.800 --> 00:27:51.240]   year over year basis, stock chart, yada, yada. Chamath, I
[00:27:51.240 --> 00:27:53.640]   guess this is something we've been talking about here, which
[00:27:53.640 --> 00:27:59.000]   is M&A and M&A being pushed into the cold plunge, and being
[00:27:59.000 --> 00:28:01.560]   frozen. And now we see something like this. What is what do you
[00:28:01.560 --> 00:28:03.480]   take from this, if it's in fact a true report?
[00:28:03.480 --> 00:28:06.160]   I mean, I think it's a bit of an odd acquisition. And the reason
[00:28:06.160 --> 00:28:10.800]   is that it's become pretty clear for all of big tech that any
[00:28:10.800 --> 00:28:13.640]   acquisition that they do is going to be highly scrutinized.
[00:28:13.640 --> 00:28:19.920]   And so just from an EV expected value perspective, if you're
[00:28:19.920 --> 00:28:22.200]   going to try to acquire something and spend the next
[00:28:22.200 --> 00:28:26.240]   year beating your head against regulators, why not do it for
[00:28:26.240 --> 00:28:28.800]   something that's really valuable. And I would not
[00:28:28.800 --> 00:28:32.120]   characterize HubSpot as strategically valuable for
[00:28:32.120 --> 00:28:35.360]   Google, I think it's an important ecosystem player. And
[00:28:35.360 --> 00:28:37.960]   it's probably better off as an independent company. So if I
[00:28:37.960 --> 00:28:40.200]   were Google, I'd be trying to buy something much more useful
[00:28:40.200 --> 00:28:42.280]   like perplexity or something.
[00:28:42.280 --> 00:28:45.280]   Friedberg, your thoughts on this?
[00:28:45.280 --> 00:28:47.920]   First of all, it's super impressive HubSpot's been
[00:28:47.920 --> 00:28:49.960]   public, they went public at about a billion dollar market
[00:28:49.960 --> 00:28:52.680]   cap 10 years ago, and today they're trading at 34 billion.
[00:28:52.680 --> 00:28:56.880]   So organically develop this business, and they provide
[00:28:56.880 --> 00:28:59.880]   marketing automation software. So basically things like CRM
[00:28:59.880 --> 00:29:04.200]   tools, email marketing tools. So when you use advertising tools,
[00:29:04.200 --> 00:29:06.760]   and you generate leads, those leads come in, what do you do
[00:29:06.760 --> 00:29:09.240]   with them? So let's say you're running a website that sells
[00:29:09.240 --> 00:29:13.160]   bicycles, people want information on bicycles, what do
[00:29:13.160 --> 00:29:15.560]   you do with those people after they get information on your
[00:29:15.560 --> 00:29:17.720]   website? And how do you track them down? And how do you sell
[00:29:17.720 --> 00:29:21.640]   them a bicycle? If you're a big enterprise software company, and
[00:29:21.640 --> 00:29:23.760]   you start to get companies reaching out to you through your
[00:29:23.760 --> 00:29:26.960]   website, how do you then convert them? So you use CRM tools,
[00:29:26.960 --> 00:29:31.440]   customer relationship management tools, Salesforce is obviously a
[00:29:31.440 --> 00:29:34.840]   behemoth in the space. But HubSpot has this integrated
[00:29:34.840 --> 00:29:40.400]   marketing automation and CRM platform for taking leads, and
[00:29:40.400 --> 00:29:43.800]   then converting those leads and selling products to them. So the
[00:29:43.800 --> 00:29:47.440]   sales team and the marketing team uses HubSpot software to
[00:29:47.440 --> 00:29:51.400]   operate and do their work and do it better. When I worked at
[00:29:51.400 --> 00:29:56.680]   Google in 2005, the main thing I worked on is how do we do a
[00:29:56.680 --> 00:30:01.000]   better job taking our advertisers and giving them more
[00:30:01.000 --> 00:30:04.720]   tools that they can then convert the leads that they're getting
[00:30:04.720 --> 00:30:09.760]   into customers. And so one of the first things I worked on in
[00:30:09.760 --> 00:30:12.800]   2004, and then we closed the deal in 2005, was acquiring
[00:30:12.800 --> 00:30:15.920]   urchin, which became Google Analytics, so that companies
[00:30:15.920 --> 00:30:19.440]   could better track how folks were converting on their website
[00:30:19.440 --> 00:30:22.000]   after they spend marketing dollars on Google, how do you
[00:30:22.000 --> 00:30:24.000]   see where those people go on your website and what they're
[00:30:24.000 --> 00:30:26.720]   actually doing on the website, and ultimately make your website
[00:30:26.720 --> 00:30:29.240]   better so you can sell more products and more software. And
[00:30:29.240 --> 00:30:32.200]   one of the things I worked on was CRM. So I had this
[00:30:32.200 --> 00:30:35.000]   conversation with the executive team with Larry and Sergey and
[00:30:35.000 --> 00:30:38.320]   others at that time. And we talked about what should we buy
[00:30:38.320 --> 00:30:42.000]   a CRM company. And we actually had a conversation they did with
[00:30:42.000 --> 00:30:44.840]   Mark Benioff at the time. And we talked about, is there a way to
[00:30:44.840 --> 00:30:48.360]   buy Salesforce and Salesforce went public shortly before
[00:30:48.360 --> 00:30:50.160]   Google, and it was more
[00:30:50.160 --> 00:30:55.200]   richly valued than I think the appetite was at the time to make
[00:30:55.200 --> 00:30:58.640]   this leap to buying a CRM company, we actually spent quite
[00:30:58.640 --> 00:31:00.840]   a bit of time meeting with and I personally spent time meeting
[00:31:00.840 --> 00:31:04.400]   with NetSuite, which ultimately got rolled in, I think it was a
[00:31:04.400 --> 00:31:07.240]   Larry Ellison was the primary owner of NetSuite, we looked at
[00:31:07.240 --> 00:31:10.800]   a few other CRM companies. And this was always meant to be the
[00:31:10.800 --> 00:31:13.960]   next solution that you plug into the advertising platform at
[00:31:13.960 --> 00:31:17.600]   Google, you get all these leads from advertising, and how do you
[00:31:17.600 --> 00:31:20.800]   convert those leads and make them customers. And many of the
[00:31:20.800 --> 00:31:23.640]   other things we looked at were things like checkout software,
[00:31:23.640 --> 00:31:26.880]   and software that would let you run a website to sell your
[00:31:26.880 --> 00:31:29.120]   products to customers on the website. And so which became
[00:31:29.120 --> 00:31:32.640]   Shopify, in a way, which became Shopify. And we had looked very
[00:31:32.640 --> 00:31:36.120]   deeply at doing this at Google is actually building a product
[00:31:36.120 --> 00:31:39.040]   called Google checkout. It wasn't very successful, but it
[00:31:39.040 --> 00:31:42.640]   was to do exactly this, which is to build a basically a Shopify
[00:31:42.640 --> 00:31:46.440]   type competitor. And so this has always been the natural fit for
[00:31:46.440 --> 00:31:49.640]   Google's business, Google made a quarter trillion dollars last
[00:31:49.640 --> 00:31:52.680]   year in advertising revenue. And then they didn't make much
[00:31:52.680 --> 00:31:56.760]   revenue after the advertising generated leads, because Google
[00:31:56.760 --> 00:31:58.840]   doesn't have a great commerce business, and they don't have
[00:31:58.840 --> 00:32:01.240]   any of these other enterprise tools. So this is a very
[00:32:01.240 --> 00:32:05.920]   natural fit into Google's advertising business, and taking
[00:32:05.920 --> 00:32:07.920]   all of the leads from advertising and better
[00:32:07.920 --> 00:32:11.360]   converting them and giving your sales team the tools they need
[00:32:11.360 --> 00:32:14.160]   to convert those leads into customers. So it makes great
[00:32:14.160 --> 00:32:16.480]   strategic sense. It's been a concept that's been around for
[00:32:16.480 --> 00:32:18.840]   20 years at Google, certainly, they're going to face
[00:32:18.840 --> 00:32:20.880]   regulatory scrutiny, but it doesn't have the same sort of
[00:32:20.880 --> 00:32:26.120]   overlap with the ad network businesses, because they're not
[00:32:26.120 --> 00:32:28.960]   very heavily in the ad network business at HubSpot. But it's a
[00:32:28.960 --> 00:32:31.080]   really good kind of enterprise software plug, plug in some
[00:32:31.080 --> 00:32:33.920]   would say acquisitions are one of three types, they are
[00:32:33.920 --> 00:32:38.480]   defensive, they are offensive, or they are about reinforcing
[00:32:38.480 --> 00:32:41.280]   the status quo. If you had to bucket HubSpot into one of those
[00:32:41.280 --> 00:32:44.720]   three things, is this an offensive M&A? Is it a
[00:32:44.720 --> 00:32:48.840]   defensive M&A? Or is this status quo?
[00:32:48.840 --> 00:32:54.760]   I think it gives advertisers more tools that can integrate
[00:32:54.760 --> 00:32:57.680]   with AdWords, which is how advertisers spend ad dollars is
[00:32:57.680 --> 00:33:01.640]   through AdWords platform. And so as a result, it locks the
[00:33:01.640 --> 00:33:05.480]   advertisers on to Google's ad platform, keeps them more
[00:33:05.480 --> 00:33:09.960]   engaged. And so I think that the benefit to Chamath's point is
[00:33:09.960 --> 00:33:13.120]   that they're going to both protect ad revenue at Google by
[00:33:13.120 --> 00:33:18.320]   locking in people on the on the CRM side, and the marginal impact
[00:33:18.320 --> 00:33:21.640]   they'll get from selling CRM services, not that impactful to
[00:33:21.640 --> 00:33:24.560]   the business. I mean, if you could spend, let's say with the
[00:33:24.560 --> 00:33:30.080]   premium $50 billion on HubSpot, or $5 billion on perplexity
[00:33:30.080 --> 00:33:33.200]   today, which one would you buy?
[00:33:33.200 --> 00:33:36.000]   I think you got to buy HubSpot, you're protecting your point,
[00:33:36.000 --> 00:33:38.440]   you're protecting a quarter trillion dollar ad business.
[00:33:38.680 --> 00:33:43.000]   HubSpot makes $2 billion in revenue. So it's 1% of the size
[00:33:43.000 --> 00:33:46.400]   of Google's ad business. And it helps you lock in some
[00:33:46.400 --> 00:33:49.640]   percentage of that $250 billion. So that's an important
[00:33:49.640 --> 00:33:51.640]   strategic acquisition, I think for Google.
[00:33:51.640 --> 00:33:55.400]   So you'd rather buy something in marketing automation versus AI?
[00:33:55.400 --> 00:33:58.200]   I think they should spend the money in AI. But I think you
[00:33:58.200 --> 00:33:59.640]   have to buy something to...
[00:33:59.640 --> 00:34:01.320]   It's a good question.
[00:34:01.320 --> 00:34:02.840]   To further the revenue. Yeah.
[00:34:02.840 --> 00:34:04.920]   What does Google accomplish here that they couldn't do with an
[00:34:04.920 --> 00:34:05.600]   integration?
[00:34:05.600 --> 00:34:08.320]   A partnership, you mean?
[00:34:09.320 --> 00:34:12.280]   Well, I mean, they could just build on HubSpot's platform,
[00:34:12.280 --> 00:34:15.440]   just create a connector between Google Ads and HubSpot.
[00:34:15.440 --> 00:34:16.600]   I think that exists already, actually.
[00:34:16.600 --> 00:34:17.960]   Exactly. So what's the point?
[00:34:17.960 --> 00:34:19.360]   What do you really understand?
[00:34:19.360 --> 00:34:22.520]   I think what Friedberg is saying in nicer languages, they're
[00:34:22.520 --> 00:34:23.680]   going to make it a roach motel.
[00:34:23.680 --> 00:34:24.360]   Lock in.
[00:34:24.360 --> 00:34:26.440]   You get in and you can't get out.
[00:34:26.440 --> 00:34:29.000]   I mean, it gives you the full life cycle of the advertiser
[00:34:29.000 --> 00:34:31.960]   sack. So I mean, you understand the funnel, right? You get the
[00:34:31.960 --> 00:34:33.880]   actual profile of the customer.
[00:34:33.880 --> 00:34:36.680]   Yeah. The problem with the roach motel M&A strategy is that
[00:34:36.680 --> 00:34:40.120]   people sniff that out pretty quickly. And then they start to
[00:34:40.120 --> 00:34:42.880]   carve out these very discrete parts of the product that they
[00:34:42.880 --> 00:34:45.640]   want you to divest in order to get the whole thing done. That's
[00:34:45.640 --> 00:34:49.680]   the thing that surprisingly, I think the regulators have gotten
[00:34:49.680 --> 00:34:52.080]   smart about. And I suspect it's not that the regulators
[00:34:52.080 --> 00:34:55.400]   themselves know these discrete ideas, but that the answers are
[00:34:55.400 --> 00:34:58.600]   fed to them by competitors who want to just slow these
[00:34:58.600 --> 00:35:01.760]   processes down and make these things convoluted and
[00:35:01.760 --> 00:35:02.480]   complicated.
[00:35:02.480 --> 00:35:04.880]   Oh, they snitch? They send the snitch in?
[00:35:04.960 --> 00:35:07.600]   I think it's very smart for a competitor to actually call a
[00:35:07.600 --> 00:35:12.080]   regulator and give them a roadmap of how to make the deal
[00:35:12.080 --> 00:35:16.200]   happen, but in a very convoluted, complicated way. You
[00:35:16.200 --> 00:35:18.400]   remember, I suspect that's what happened in Microsoft
[00:35:18.400 --> 00:35:21.640]   Activision. Microsoft brilliantly fought it off, right?
[00:35:21.640 --> 00:35:23.760]   And so they were able to get the whole thing done completely on
[00:35:23.760 --> 00:35:26.560]   their terms. But in many other cases, you get these discrete
[00:35:26.560 --> 00:35:29.160]   things where it's like, okay, divest this, sell that, keep
[00:35:29.160 --> 00:35:33.560]   this. And it's like, why are we doing this? But I just think the
[00:35:33.600 --> 00:35:36.880]   roach motel strategy is harder to get done these days, because
[00:35:36.880 --> 00:35:39.840]   folks will know how to make it super convoluted.
[00:35:39.840 --> 00:35:43.320]   I will tell you what, when a big company like this makes a big
[00:35:43.320 --> 00:35:46.600]   acquisition offer like this, as much as I know, Google's
[00:35:46.600 --> 00:35:49.480]   business, I think it's a generalization that can be drawn
[00:35:49.480 --> 00:35:54.280]   here. It's usually a negative signal about organic growth.
[00:35:54.280 --> 00:35:58.640]   Meaning if I'm a Google shareholder, I should look at
[00:35:58.640 --> 00:36:01.680]   this. And I should say, why do you need to make this
[00:36:01.680 --> 00:36:06.000]   acquisition? Why is there an indication in this bid, that
[00:36:06.000 --> 00:36:09.520]   there is some advertising revenue leakage going on? And
[00:36:09.520 --> 00:36:12.880]   then I should go spend time trying to understand that. I
[00:36:12.880 --> 00:36:14.760]   think there's a really important question there. Yeah.
[00:36:14.760 --> 00:36:17.400]   But this is why I'm asking you, like, obviously, they're not
[00:36:17.400 --> 00:36:19.520]   going to do this, because things are going perfectly in that
[00:36:19.520 --> 00:36:22.640]   space. And so if you're losing share, you're not going to be
[00:36:22.640 --> 00:36:25.360]   losing it to Bing, you're losing it to some form of AI, which is
[00:36:25.360 --> 00:36:28.600]   why, again, it's a bit of a head scratcher, spend a lot less
[00:36:28.600 --> 00:36:31.400]   money and just buy everything in the space or spend 50 billion
[00:36:31.400 --> 00:36:33.000]   and buy everything possible in the space.
[00:36:33.000 --> 00:36:36.040]   To give you just a little context here, this is going to
[00:36:36.040 --> 00:36:38.640]   be by far their largest acquisition, if it's true, if it
[00:36:38.640 --> 00:36:41.400]   gets closed, this is all speculation right now. Looking
[00:36:41.400 --> 00:36:44.840]   back, Motorola Mobility was bought for 12.5 billion, there
[00:36:44.840 --> 00:36:46.520]   were patents, there was the hardware business, which they
[00:36:46.520 --> 00:36:50.000]   spun out, you can look up the deal details there. But there's
[00:36:50.000 --> 00:36:54.720]   a long tail of companies they've bought Nest Labs, Fitbit, and
[00:36:54.720 --> 00:36:58.320]   YouTube, 3 billion, 2 billion, 1.6 billion, if you remember,
[00:36:58.520 --> 00:37:01.480]   they bought a Mandiant, I've never even heard of the
[00:37:01.480 --> 00:37:04.080]   cybersecurity company, 5.4 billion, they bought Nest for
[00:37:04.080 --> 00:37:06.160]   3 billion, double click 3 billion, that was a long time
[00:37:06.160 --> 00:37:08.320]   ago, it's probably triple that value in today's dollars.
[00:37:08.320 --> 00:37:12.880]   By the way, remember, Fitbit, Fitbit took 18 or 24 months to
[00:37:12.880 --> 00:37:16.960]   close. It took a while. Yeah. Deep, deeply scrutinized. If you
[00:37:16.960 --> 00:37:19.400]   think a wearable on your wrist is going to get scrutinized when
[00:37:19.400 --> 00:37:21.760]   bought by Google, imagine what happens when a $50 billion
[00:37:21.760 --> 00:37:24.640]   marketing automation company gets bought by Google.
[00:37:24.640 --> 00:37:26.240]   Yeah, it's really interesting.
[00:37:26.240 --> 00:37:28.920]   Look, I don't think this acquisition makes any sense. I'm
[00:37:28.920 --> 00:37:31.600]   kind of in Jamas camp. This is this would be a very odd
[00:37:31.600 --> 00:37:33.680]   acquisition. I'm not even sure the story is true. There's been
[00:37:33.680 --> 00:37:36.960]   no confirmation of an actual bid made. This was basically an
[00:37:36.960 --> 00:37:39.880]   anonymous source, saying that Google had hired investment
[00:37:39.880 --> 00:37:42.840]   bankers to, you know, maybe kick the tires.
[00:37:42.840 --> 00:37:46.040]   Well, typically, these bankers float these problems like this,
[00:37:46.040 --> 00:37:48.360]   because they want to pump the deal.
[00:37:48.360 --> 00:37:51.120]   But yeah, imagine what the fees would be on $50 billion
[00:37:51.120 --> 00:37:52.440]   acquisition.
[00:37:52.440 --> 00:37:55.120]   But they want to shake some other buyers loose. Yeah, when
[00:37:55.120 --> 00:37:57.040]   you have these auctions.
[00:37:57.040 --> 00:37:59.920]   Well, look, it's gonna be very hard for any big tech company to
[00:37:59.920 --> 00:38:03.040]   do a $50 billion acquisition of anything just that given that
[00:38:03.040 --> 00:38:07.160]   lean economy FTC are opposed to bigness in and of itself. But I
[00:38:07.160 --> 00:38:10.360]   think from a strategic point of view, I agree this, this deal
[00:38:10.360 --> 00:38:13.000]   would be odd. I don't really see the connection to the Google
[00:38:13.000 --> 00:38:17.400]   ads business. HubSpot is used by companies to manage their
[00:38:17.400 --> 00:38:20.600]   pipelines. It's a competitive Salesforce. I see it all the
[00:38:20.600 --> 00:38:24.360]   time. I see startups using it all the time as the alternative
[00:38:24.360 --> 00:38:27.360]   to Salesforce because it's sort of easier to use more user
[00:38:27.360 --> 00:38:31.040]   friendly. And the main thing it does is you have your pipeline
[00:38:31.040 --> 00:38:34.120]   in there, you bring in the leads at the top of the funnel, and
[00:38:34.120 --> 00:38:36.080]   then you work them down to close deals.
[00:38:36.080 --> 00:38:37.680]   Where do those leads come from sex?
[00:38:37.680 --> 00:38:41.640]   I understand that they can come from Google AdWords, but most of
[00:38:41.640 --> 00:38:45.120]   them come from from ad spend. That's the connection. And it's
[00:38:45.120 --> 00:38:47.320]   always been this idea that those two should be integrated.
[00:38:47.320 --> 00:38:52.160]   Okay, but but the point is that if you think about your top of
[00:38:52.160 --> 00:38:55.720]   funnel, if you're one of the customers of HubSpot, using
[00:38:55.720 --> 00:38:59.640]   this, Google AdWords is just one of a number of channels. That's
[00:38:59.640 --> 00:39:03.560]   right. So you could be getting your leads through inbound, you
[00:39:03.560 --> 00:39:06.400]   could be getting your leads through events, you could be
[00:39:06.400 --> 00:39:09.560]   getting your leads through I mean, there's so many different
[00:39:09.560 --> 00:39:16.280]   sources. So it doesn't make sense to me that somehow, Google
[00:39:16.280 --> 00:39:20.760]   would need to acquire a company to manage not their advertisers,
[00:39:20.760 --> 00:39:24.160]   but the people their advertisers are trying to reach as one of a
[00:39:24.160 --> 00:39:26.680]   dozen different potential marketing channels. It just
[00:39:26.680 --> 00:39:29.160]   doesn't really make sense. I mean, the acquisitions where
[00:39:29.160 --> 00:39:32.680]   Google's been really successful have been broad horizontal
[00:39:32.680 --> 00:39:36.680]   platform plays like Android. This is as vertical as it gets.
[00:39:36.680 --> 00:39:41.160]   This is CRM software. This is basically Google getting into a
[00:39:41.160 --> 00:39:46.760]   vertical app for sales and marketing teams. Yeah. If you
[00:39:46.760 --> 00:39:50.680]   think about the business that this is most like it would be G
[00:39:50.680 --> 00:39:54.440]   suite. And because it's an enterprise play. And that's the
[00:39:54.440 --> 00:39:56.560]   most neglected part of Google's business.
[00:39:56.560 --> 00:39:58.720]   I think you're both missing a key piece to this. I think this
[00:39:58.720 --> 00:40:02.400]   is about the data. You look at these, these are the leads and
[00:40:02.400 --> 00:40:05.520]   the great contacts in the database of the customer which
[00:40:05.520 --> 00:40:08.240]   Google doesn't have access to and they can close the loop and
[00:40:08.240 --> 00:40:12.080]   they can make targeting of ads and get people deeper and they
[00:40:12.080 --> 00:40:14.560]   can fight for a larger percentage. So to your point,
[00:40:14.560 --> 00:40:18.560]   sacks, yes, you've got 20 different inbound feeds coming
[00:40:18.560 --> 00:40:21.800]   in for leads. If Google knows the leads that are already in
[00:40:21.800 --> 00:40:24.440]   there, they can then retarget them across their entire ad
[00:40:24.440 --> 00:40:27.400]   network, which is the largest in the world. That's the value. If
[00:40:27.400 --> 00:40:30.600]   I know your 10,000 best customers, and then I know when
[00:40:30.600 --> 00:40:32.840]   they're on Google searching, and I know that when they're in a
[00:40:32.840 --> 00:40:34.960]   Chrome browser, I know when they're on an Android phone, if
[00:40:34.960 --> 00:40:38.320]   they happen to use that, man, I can just start getting more of
[00:40:38.320 --> 00:40:40.640]   your ad dollars into it. That's what's actually happening here.
[00:40:40.640 --> 00:40:43.480]   It's they don't want to SAS business. They want the data
[00:40:43.520 --> 00:40:47.000]   they want to retarget folks, and then they want to make close
[00:40:47.000 --> 00:40:50.000]   more sales and be more efficient than Tick Tock, and Facebook
[00:40:50.000 --> 00:40:50.920]   meta. It's about
[00:40:50.920 --> 00:40:54.440]   you think they'd be allowed to use their customers data to
[00:40:54.440 --> 00:40:55.800]   somehow target their ad product?
[00:40:55.800 --> 00:40:59.520]   Absolutely. 100%. If you if I'm opting into it, and I say, Hey,
[00:40:59.520 --> 00:41:01.760]   you have 10,000 people in your database, you want to go find
[00:41:01.760 --> 00:41:04.160]   them we have you have 8000 of them connected that came in
[00:41:04.160 --> 00:41:06.480]   through Google search. What about the other 12,000? You want
[00:41:06.480 --> 00:41:08.600]   to try to find them in the Google ad network, we can put
[00:41:08.600 --> 00:41:11.360]   that together for you. That's actually if somebody is going to
[00:41:11.360 --> 00:41:13.160]   snitch on this deal, it's about the data.
[00:41:13.160 --> 00:41:17.400]   I'll tell you back in just improving conversion rates,
[00:41:17.400 --> 00:41:22.640]   drives more ad revenue. Bingo. And the key measurement we had
[00:41:22.640 --> 00:41:26.000]   the year after we bought urchin and launch Google Analytics was
[00:41:26.000 --> 00:41:29.280]   we looked at how much an advertiser spent on Google's ad
[00:41:29.280 --> 00:41:33.360]   words network before and after they installed Google Analytics.
[00:41:33.360 --> 00:41:37.240]   And that year, it was a ton of money. At the time, we saw an
[00:41:37.240 --> 00:41:42.440]   incremental roughly $500 million in revenue in ad spend with
[00:41:42.440 --> 00:41:46.320]   folks who installed Google Analytics from before versus
[00:41:46.320 --> 00:41:49.040]   after, because they then started to change their websites and
[00:41:49.040 --> 00:41:52.400]   tweak their sales flow or their marketing flow on their website
[00:41:52.400 --> 00:41:55.040]   to better convert customers so they could spend more on the
[00:41:55.040 --> 00:41:57.480]   network because higher conversion rates means you can
[00:41:57.480 --> 00:42:01.640]   spend a higher CPM on advertising. So the deeper you
[00:42:01.640 --> 00:42:04.800]   go from an integration perspective in closing sales,
[00:42:05.040 --> 00:42:08.040]   the better you actually can get and the more money you can spend
[00:42:08.040 --> 00:42:10.600]   on marketing on the front end. And so that benefits the growth
[00:42:10.600 --> 00:42:11.120]   in the network.
[00:42:11.120 --> 00:42:14.480]   And to build on your point, Friedberg, this is why Amazon
[00:42:14.480 --> 00:42:18.480]   and Uber and Instacart have become such mayor major players
[00:42:18.480 --> 00:42:22.240]   in advertising. They have the data on sales information, they
[00:42:22.240 --> 00:42:25.960]   still cover drop off and they sell conversions at the point
[00:42:25.960 --> 00:42:30.560]   sacks of the shopping cart about to be clicked on. And Amazon's
[00:42:30.560 --> 00:42:34.040]   been taking some of that in Instacart but intercepting some
[00:42:34.040 --> 00:42:36.520]   of those ads. Yeah, I'll speculate on this for a second.
[00:42:36.520 --> 00:42:39.840]   I think that if I'm sitting inside of Google, I see the
[00:42:39.840 --> 00:42:43.720]   capability of our AI tools, our generative AI tools, in being
[00:42:43.720 --> 00:42:48.600]   able to improve marketing and sales processes. And I'm saying
[00:42:48.600 --> 00:42:51.000]   how do we leverage that? Well, guess what, we don't actually
[00:42:51.000 --> 00:42:53.880]   have access to our advertiser sales and marketing automation
[00:42:53.880 --> 00:42:58.640]   workflow, because we don't have a CRM tool. So then the natural
[00:42:58.640 --> 00:43:02.120]   strategic thing is how do we get a CRM tool? Okay, well, we can't
[00:43:02.120 --> 00:43:05.200]   buy Salesforce. Well, we could buy HubSpot. That makes sense.
[00:43:05.200 --> 00:43:08.200]   Number two, but then we could we could apply our generative AI
[00:43:08.200 --> 00:43:11.200]   capabilities to improve conversion efficiency and HubSpot
[00:43:11.200 --> 00:43:13.880]   and probably grow revenue there as well. So it could be a fairly
[00:43:13.880 --> 00:43:16.560]   quickly accretive deal. And there's the benefit to the ad
[00:43:16.560 --> 00:43:19.520]   network. I think the Roach Motel idea is probably the best
[00:43:19.520 --> 00:43:22.480]   strategy here. Yeah, you're right. I just think I would if I
[00:43:22.480 --> 00:43:26.200]   were them with $50 billion, they're like, 30 billion, by the
[00:43:26.200 --> 00:43:30.640]   way, 35. They got to pay 50% more 25% more, it's already
[00:43:30.640 --> 00:43:32.320]   floated up. So you got to imagine it's probably a $40
[00:43:32.320 --> 00:43:36.760]   billion deal. Yeah. Okay, 40. Okay, so they save 10 billion. I
[00:43:36.760 --> 00:43:39.880]   think the synergies here are negligible. If it took Google
[00:43:39.880 --> 00:43:43.920]   almost two years to get a an accelerometer on a wrist approved
[00:43:43.920 --> 00:43:47.320]   from antitrust. This is going to take three years. Sax. Yeah. So
[00:43:47.320 --> 00:43:50.000]   why bother? So like do something much more disruptive. I have an
[00:43:50.000 --> 00:43:51.360]   answer to that. But I want to hear saxes first.
[00:43:51.360 --> 00:43:55.400]   Well, I just looked up how many customers HubSpot has HubSpot
[00:43:55.400 --> 00:43:59.520]   has 205,000 customers as of the end of 2023. Now you look up
[00:43:59.520 --> 00:44:04.120]   Google AdWords. It has 1.2 million businesses. Okay. And
[00:44:04.120 --> 00:44:07.400]   I'm sure that's not 100% overlapping. Every one of them
[00:44:07.400 --> 00:44:10.720]   should be using HubSpot. That's hugely creative. I agree. Yeah.
[00:44:10.720 --> 00:44:15.200]   I don't think that Google has the leverage to drive all of its
[00:44:15.200 --> 00:44:19.200]   AdWords customers into using all of them. If it gets 5% of them,
[00:44:19.200 --> 00:44:20.440]   it makes this deal make sense.
[00:44:20.440 --> 00:44:23.840]   Interesting. That part is kind of interesting if they can
[00:44:23.840 --> 00:44:26.800]   actually drive their ad customers what they did with
[00:44:26.800 --> 00:44:29.360]   with double click, double click was actually advertising. No,
[00:44:29.360 --> 00:44:31.880]   but they had other other places that they could take their AdWords
[00:44:31.880 --> 00:44:34.800]   advertisers to spend that the double click network had access
[00:44:34.800 --> 00:44:37.920]   to. Anyway, I hear you. Yeah. And there was another set of
[00:44:37.920 --> 00:44:40.320]   advertising audit management tools that double click had
[00:44:40.320 --> 00:44:42.440]   that that's an enterprise software tool that they were
[00:44:42.440 --> 00:44:44.200]   able to sell into their advertisers that they didn't
[00:44:44.200 --> 00:44:44.960]   have themselves.
[00:44:44.960 --> 00:44:49.400]   Chumak, let me ask you a markets question here. If we wind up
[00:44:49.400 --> 00:44:55.000]   with a Republican conservative GOP presidency administration for
[00:44:55.000 --> 00:44:58.160]   the next four years, which seems like a possibility here, a
[00:44:58.160 --> 00:45:01.040]   strong possibility, what does that do for M&A markets? Do you
[00:45:01.040 --> 00:45:04.480]   think they might be opening up here? And the reason Google's
[00:45:04.480 --> 00:45:07.880]   even considering this is because they anticipate this deal might
[00:45:07.880 --> 00:45:11.840]   fall into a new administration that is going to fire Alina Khan
[00:45:11.840 --> 00:45:13.240]   possibly, probably.
[00:45:13.240 --> 00:45:16.920]   I think that I think the Democrats and the Republicans
[00:45:16.920 --> 00:45:20.880]   are really well aligned here. They don't like deals. And I
[00:45:20.880 --> 00:45:23.880]   don't think you're going to see a big sea change. They hate big
[00:45:23.880 --> 00:45:27.200]   tech for different reasons, but they equally want to slow them
[00:45:27.200 --> 00:45:32.440]   down. If you look at the non big tech M&A deals, they're going to
[00:45:32.440 --> 00:45:35.200]   get slowed down for different reasons. So for example, like
[00:45:35.200 --> 00:45:38.240]   the big US deal merger with Nippon Steel that was announced,
[00:45:38.240 --> 00:45:41.520]   Biden has one set of issues, but I suspect, you know, if Donald
[00:45:41.520 --> 00:45:43.800]   Trump were to get elected, his issues will be more about
[00:45:43.800 --> 00:45:48.440]   further hollowing out middle America. And so that deal will
[00:45:48.440 --> 00:45:52.520]   probably get stopped for different reasons. But so I
[00:45:52.520 --> 00:45:56.440]   think that they're both actually, roughly aligned and
[00:45:56.440 --> 00:46:00.080]   not allowing a lot of this big M&A to happen. But for different
[00:46:00.080 --> 00:46:03.000]   reasons, for different reasons, but the outcome is the same. So
[00:46:03.000 --> 00:46:05.120]   this is why I would just kind of think like, if you're going to
[00:46:05.120 --> 00:46:09.800]   do a deal, you got to do something that's like, small
[00:46:09.800 --> 00:46:13.120]   enough where it'll pass muster, you know, it to be really
[00:46:13.120 --> 00:46:16.480]   valuable. And the regulators will be like, whatever, just let
[00:46:16.480 --> 00:46:17.040]   it happen.
[00:46:17.040 --> 00:46:20.120]   Saks, what do you think you think a Trump administration
[00:46:20.120 --> 00:46:22.720]   would become more frisky allow more M&A they seem to be
[00:46:22.720 --> 00:46:25.160]   actually closing up to tech in a major way?
[00:46:25.240 --> 00:46:28.140]   Well, I agree with Jamal that there's a lot of anger on the
[00:46:28.140 --> 00:46:30.520]   Republican side towards big tech, because of censorship and
[00:46:30.520 --> 00:46:33.880]   bias. And Google is as guilty of that as any of these big tech
[00:46:33.880 --> 00:46:38.560]   companies. So I don't expect a Republican administration to
[00:46:38.560 --> 00:46:41.440]   have Google in its good graces. That being said, I do think
[00:46:41.440 --> 00:46:44.520]   Republicans have a more traditional definition of
[00:46:44.520 --> 00:46:50.080]   antitrust than Lena Khan does. And I think that a Republican
[00:46:50.080 --> 00:46:54.960]   antitrust enforcer would probably be guided by market
[00:46:54.960 --> 00:46:59.520]   share considerations, first and foremost. And so in this case,
[00:46:59.520 --> 00:47:03.680]   since Google does not have a CRM play, then based on a
[00:47:03.680 --> 00:47:05.760]   traditional definition of antitrust, they would be able to
[00:47:05.760 --> 00:47:09.240]   make this acquisition, whereas I think, again, Lena Khan is just
[00:47:09.240 --> 00:47:11.320]   opposed to bigness and doesn't want big tech companies getting
[00:47:11.320 --> 00:47:15.040]   any bigger. So there's no guarantee that Republicans would
[00:47:15.040 --> 00:47:18.600]   allow it, I would say that if you got the right FTC
[00:47:18.600 --> 00:47:22.960]   Commissioner, or right, do J, I'd say, there's a possibility
[00:47:22.960 --> 00:47:24.400]   of it being more likely to go through,
[00:47:24.400 --> 00:47:29.880]   to remind everybody, I mean, the cons interpretation of antitrust
[00:47:29.880 --> 00:47:33.000]   is future competition, trying to protect future competition. The
[00:47:33.000 --> 00:47:35.880]   traditional one is consumer based, hey, are consumers
[00:47:35.880 --> 00:47:39.480]   benefiting or not. And so that's actually the that's one of the
[00:47:39.480 --> 00:47:40.560]   top three guests I want to
[00:47:40.560 --> 00:47:43.080]   market share is really the traditional test.
[00:47:43.080 --> 00:47:46.080]   Yeah, market share, and then the impact that has on consumer
[00:47:46.080 --> 00:47:46.960]   choice and price.
[00:47:46.960 --> 00:47:49.440]   Yeah, let's go back on to the synergy point. I just want to
[00:47:49.440 --> 00:47:51.920]   like, please, I just want to brainstorm about this for a
[00:47:51.920 --> 00:47:56.000]   second. Let's say you're one of Google's 1.2 million businesses
[00:47:56.000 --> 00:47:58.720]   that are using AdWords, there's a high chance you're also using
[00:47:58.720 --> 00:48:02.320]   Facebook, there's a high chance you're also doing other kinds of
[00:48:02.320 --> 00:48:05.960]   advertising, you might be doing physical world advertising, you
[00:48:05.960 --> 00:48:08.480]   might be doing events, there might be a dozen different
[00:48:08.480 --> 00:48:11.400]   channels through which you get leads. Now, all of a sudden,
[00:48:11.400 --> 00:48:14.560]   Google sends you an email saying, hey, we acquired HubSpot,
[00:48:14.560 --> 00:48:19.440]   you know, why don't you click here to use us for CRM? Is that
[00:48:19.440 --> 00:48:22.760]   really going to drive a change in behavior from a small
[00:48:22.760 --> 00:48:26.200]   business? Beyond what they would already do today? It doesn't
[00:48:26.200 --> 00:48:29.720]   seem that synergistic to some percent of them. Not all maybe
[00:48:29.720 --> 00:48:35.720]   5%. Again, like 5% is 50,000. That's an email campaign. I mean,
[00:48:35.720 --> 00:48:37.400]   I just don't see the leverage basically.
[00:48:37.400 --> 00:48:40.600]   Yeah, I don't know if I agree. But I mean, that's why we're
[00:48:40.600 --> 00:48:40.920]   here.
[00:48:40.920 --> 00:48:45.880]   If Google did a deep product integration where the leads that
[00:48:45.880 --> 00:48:50.400]   you acquired through Google AdWords magically appeared in
[00:48:50.400 --> 00:48:54.720]   HubSpot, and that's where you went to go work them, then yes,
[00:48:54.720 --> 00:48:57.200]   maybe, well, maybe there'd be synergy.
[00:48:57.200 --> 00:48:59.800]   Just looking at this on the numbers, since we like to do
[00:48:59.800 --> 00:49:02.840]   back of the envelope here $250 billion in revenue, 1.2 million
[00:49:02.840 --> 00:49:06.440]   in advertisers, it's almost exactly 200,000 per advertiser.
[00:49:06.440 --> 00:49:08.320]   Obviously, there's some big ones, obviously, there's a long
[00:49:08.320 --> 00:49:12.240]   tail. Man, if you can get some number of those to spend 10, 20,
[00:49:12.240 --> 00:49:15.320]   30% more, it could be quite accretive. To the bottom line,
[00:49:15.320 --> 00:49:17.520]   it would pay for the acquisition over a short period of time.
[00:49:17.520 --> 00:49:21.000]   And we'll see. It's interesting discussion for sure. All right,
[00:49:21.000 --> 00:49:24.480]   if you missed it, Jon Stewart did a segment on AI on the Daily
[00:49:24.480 --> 00:49:27.720]   Show, he came back, he's doing I think, Mondays every week. And
[00:49:27.720 --> 00:49:32.800]   it went viral. And it was about how AI is going to change our
[00:49:32.800 --> 00:49:36.200]   jobs faster than any previous labor revolution. So it seems
[00:49:36.200 --> 00:49:41.160]   like the public is starting to get an idea about AI wiping out
[00:49:41.160 --> 00:49:44.400]   large swaths of jobs, and it's starting to hit the mainstream.
[00:49:44.400 --> 00:49:48.600]   CEOs like Brian Chesky from Airbnb and Aaron Levy from box.
[00:49:48.600 --> 00:49:51.400]   I had them on this weekend startups in the last year, they
[00:49:51.400 --> 00:49:55.160]   said they anticipate 30 to 50% productivity gains for a lot of
[00:49:55.160 --> 00:49:57.400]   the jobs in their companies, developers, customer support,
[00:49:57.400 --> 00:50:00.160]   all that stuff. And we covered cloners AI customer support
[00:50:00.160 --> 00:50:03.600]   agent, doing the job of 700 full time employees and driving a
[00:50:03.600 --> 00:50:06.640]   $40 million increase in profits this year, yada, yada, yada,
[00:50:06.640 --> 00:50:09.160]   we've we've talked about this over and over again, but it
[00:50:09.160 --> 00:50:13.520]   seems to be tipping over into public consciousness. We Chamath
[00:50:13.520 --> 00:50:16.960]   have talked about whether humans will find more work to do, or if
[00:50:16.960 --> 00:50:19.360]   this is going to truly displace people, I think we all kind of
[00:50:19.360 --> 00:50:22.760]   feel like, at least to the best of my memory, we all feel like
[00:50:22.760 --> 00:50:25.720]   new jobs will be created, but it is entering the public's
[00:50:25.720 --> 00:50:28.520]   consciousness. What impact is that going to have? If the
[00:50:28.520 --> 00:50:31.400]   public starts thinking AI is going to take their job tomorrow?
[00:50:31.400 --> 00:50:35.520]   I mean, I think that social media will make this perception
[00:50:35.520 --> 00:50:39.640]   more widespread than it's been at other moments of revolution
[00:50:39.640 --> 00:50:43.000]   and innovation. But we've gone through this before. I'd like to
[00:50:43.000 --> 00:50:47.080]   summarize my thoughts in three charts. Okay. And I call this
[00:50:47.080 --> 00:50:52.440]   this time, it's not different. So chart number one, for those
[00:50:52.440 --> 00:50:56.560]   watching on YouTube is a look at the components of US GDP. This
[00:50:56.560 --> 00:51:00.720]   is from the Bureau of Economic Analysis. Now, this goes from
[00:51:00.720 --> 00:51:04.040]   1929, up to 2011. So it doesn't go all the way back to the
[00:51:04.040 --> 00:51:07.480]   1800s. And we're missing the last decade. But the point is,
[00:51:07.480 --> 00:51:10.280]   the following, if you can see the chart, if you can't see it,
[00:51:10.280 --> 00:51:14.000]   I'll describe it to you, which is that GDP, the components of
[00:51:14.000 --> 00:51:18.480]   GDP are surprisingly resilient, and roughly the same over long
[00:51:18.480 --> 00:51:21.280]   stretches of time, which is that even though GDP goes up,
[00:51:21.280 --> 00:51:27.440]   consumer consumption is always around 70%. Net exports are a
[00:51:27.440 --> 00:51:31.400]   few percent plus or minus gross domestic investment is around
[00:51:31.400 --> 00:51:35.040]   the 20% level. And then government consumption is around
[00:51:35.040 --> 00:51:40.360]   the 20% level. And that's what adds up to GDP. So that's an
[00:51:40.360 --> 00:51:44.120]   important thing to note. Why? Because in the absence of
[00:51:44.120 --> 00:51:47.160]   something very acute, like World War Two, these things don't
[00:51:47.160 --> 00:51:50.160]   change over long periods of time. Okay, so if that is true,
[00:51:50.160 --> 00:51:55.240]   what happens when you have any kind of a revolution? So let's
[00:51:55.240 --> 00:51:57.080]   look at the Industrial Revolution. So the shift from
[00:51:57.080 --> 00:52:01.960]   farms to factories. And what you saw was exactly what people
[00:52:01.960 --> 00:52:04.520]   should be worried about with respect to AI, which is in
[00:52:04.520 --> 00:52:08.760]   specific job classes, things just fall to zero. So
[00:52:08.760 --> 00:52:12.440]   unemployment basically went to zero. And the income associated
[00:52:12.440 --> 00:52:15.200]   with those jobs also went to zero. So this is what people are
[00:52:15.200 --> 00:52:18.360]   worried about. But if you remember the last chart, the
[00:52:18.360 --> 00:52:24.960]   point is, somehow we found a way to find growth. And this is
[00:52:24.960 --> 00:52:27.280]   what's demonstrated on this final chart, which is when you
[00:52:27.280 --> 00:52:29.920]   look at US productivity and worker compensation, this is
[00:52:29.920 --> 00:52:33.760]   going from World War Two to today, you find that every time
[00:52:33.760 --> 00:52:38.880]   we find a new way of innovating, compensation tends to track it.
[00:52:38.880 --> 00:52:42.400]   So if you take these three things together, number one,
[00:52:42.400 --> 00:52:45.880]   which is the components of GDP rarely change. Number two is
[00:52:45.880 --> 00:52:48.840]   that yes, there are certain categories of jobs that always
[00:52:48.840 --> 00:52:52.360]   get disrupted away. But the third is the most important,
[00:52:52.360 --> 00:52:55.560]   which is that as productivity goes up, which is what AI should
[00:52:55.560 --> 00:52:59.640]   give us, just as we've seen in the past, compensation also goes
[00:52:59.640 --> 00:53:03.920]   up, which means new job classes will be created. So I think the
[00:53:03.920 --> 00:53:07.320]   macro picture, if you look back hundreds of years, is that this
[00:53:07.320 --> 00:53:10.680]   is like many other moments in time, it feels more personal
[00:53:10.680 --> 00:53:13.400]   right now, because we're all living it. Right. None. Few of
[00:53:13.400 --> 00:53:16.800]   us live the agrarian to industrial revolution. Yeah, we
[00:53:16.800 --> 00:53:20.040]   missed it. And few of us live the technological revolution,
[00:53:20.040 --> 00:53:23.480]   right, we kind of came in at the heels of it. But I suspect that
[00:53:23.480 --> 00:53:24.640]   this time is not different.
[00:53:24.640 --> 00:53:28.120]   freeburger thoughts on this. I think you've said something
[00:53:28.120 --> 00:53:33.120]   similar on past episodes. But it is kind of tipping and into
[00:53:33.120 --> 00:53:36.720]   public consciousness. And it's also affecting white collar jobs
[00:53:36.720 --> 00:53:40.800]   this time, not just people in fields picking berries. And so
[00:53:40.800 --> 00:53:43.320]   those people may be a little more vocal. And we've seen
[00:53:43.320 --> 00:53:47.320]   massive layoffs in tech, massive layoffs in media. And those jobs
[00:53:47.480 --> 00:53:50.160]   don't seem to be coming back, people seem to be get taking the
[00:53:50.160 --> 00:53:54.240]   gains and just having people on the team be 30% more efficient,
[00:53:54.240 --> 00:53:57.920]   as Brian told me on my other pod. So what do you think
[00:53:57.920 --> 00:54:01.960]   freeburg? Is this time different? Or is it the same?
[00:54:01.960 --> 00:54:08.320]   So here's this article from June 2 1983, in the New York Times
[00:54:08.320 --> 00:54:13.160]   all about how computers are eliminating jobs in industries
[00:54:13.160 --> 00:54:16.880]   that were effectively offline knowledge work industries at the
[00:54:16.880 --> 00:54:19.880]   time, creating engineering designs, creating architectural
[00:54:19.880 --> 00:54:22.840]   drawings, I think this article spoke to the fact that these
[00:54:22.840 --> 00:54:26.120]   jobs were going to be eliminated. And as we all know,
[00:54:26.120 --> 00:54:29.960]   those jobs actually got enhanced by computers, productivity went
[00:54:29.960 --> 00:54:32.920]   up, and new sub industries emerged. And in fact, the
[00:54:32.920 --> 00:54:35.520]   overall industries actually grew in some cases when we were
[00:54:35.520 --> 00:54:39.120]   fearful of them being replaced due to the automation enabled by
[00:54:39.120 --> 00:54:44.240]   software. So I think that in this particular sense, we can
[00:54:44.240 --> 00:54:46.920]   talk about the Industrial Revolution enabling through
[00:54:46.920 --> 00:54:51.560]   manufacturing systems and centralized production, a
[00:54:51.560 --> 00:54:56.320]   replacement of manual labor with machines, what we're talking
[00:54:56.320 --> 00:54:59.520]   about now is a replacement of knowledge work that has been
[00:54:59.520 --> 00:55:02.560]   aided by computers with machines. So the machines no
[00:55:02.560 --> 00:55:05.280]   longer even need the human but the reality is that these
[00:55:05.280 --> 00:55:08.360]   systems are actually going to give humans 10 to 100 x
[00:55:08.360 --> 00:55:12.080]   leverage. So when you think about that one person could
[00:55:12.080 --> 00:55:14.840]   spend three weeks making an architectural drawing today,
[00:55:14.840 --> 00:55:17.400]   what if that one person could make an architectural drawing
[00:55:17.400 --> 00:55:22.320]   every six hours? So the question then is, do we stop making
[00:55:22.320 --> 00:55:25.680]   architectural drawings and we fire a bunch of architects? Or
[00:55:25.680 --> 00:55:29.320]   does the cost of making an architectural drawing drop by 90%
[00:55:29.320 --> 00:55:32.960]   and it enables us to do more detailed higher resolution,
[00:55:32.960 --> 00:55:37.640]   architectural drawings across more places more frequently, and
[00:55:37.640 --> 00:55:41.000]   the industry actually booms. And what we've seen historically is
[00:55:41.000 --> 00:55:45.240]   that when productivity goes up, costs go down, the actual volume
[00:55:45.240 --> 00:55:49.680]   balloons and the economy grows. So it's a it's an example where
[00:55:49.680 --> 00:55:52.760]   I think in this particular case, we will see these tools creating
[00:55:52.760 --> 00:55:55.760]   more leverage for knowledge work instead of just simply
[00:55:55.760 --> 00:55:58.760]   replacing knowledge work, and that humans will start to shift
[00:55:58.760 --> 00:56:02.160]   to a higher order of work. And we'll see the economy grow and
[00:56:02.160 --> 00:56:07.080]   productivity go up as a result. So so I think that's my kind of
[00:56:07.080 --> 00:56:10.240]   key read on on the story, but it's very hard to connect the
[00:56:10.240 --> 00:56:13.680]   dots for people without having all of these historical cases.
[00:56:13.680 --> 00:56:16.440]   And I think one of the ways to think about doing this usefully
[00:56:16.440 --> 00:56:19.200]   is you go back to the software revolution, and all the stuff
[00:56:19.200 --> 00:56:23.040]   that we were doing with pencils and papers before computers, we
[00:56:23.040 --> 00:56:26.560]   actually didn't lose all those jobs, the people could now do 100
[00:56:26.560 --> 00:56:29.880]   times or 1000 or million times as much work, and new industries
[00:56:29.880 --> 00:56:33.320]   emerged and productivity went up, and the economy grew. And so
[00:56:33.320 --> 00:56:37.240]   we just have to have this. This realization as the starts to
[00:56:37.240 --> 00:56:42.280]   take hold, that the industries will change. And that the
[00:56:42.280 --> 00:56:45.040]   systems will actually provide leverage, not replacement.
[00:56:45.040 --> 00:56:48.800]   Yeah, it's such a good point. And I think what you teach your
[00:56:48.800 --> 00:56:51.920]   kids is like really important at this moment in time, like having
[00:56:51.920 --> 00:56:56.560]   a job that is replaced by AI, or that could be greatly replaced
[00:56:56.560 --> 00:56:59.280]   by AI might be a mistake. And if you think about being a
[00:56:59.280 --> 00:57:03.200]   conductor, Friedberg, or a maestro, conductor of an
[00:57:03.200 --> 00:57:06.160]   orchestra, I think that's the job of the future is can you
[00:57:06.600 --> 00:57:08.920]   work with these agents forget about co pilots, because that's
[00:57:08.920 --> 00:57:11.960]   phase one of all this, but agents are phase two, where you
[00:57:11.960 --> 00:57:16.000]   have an agent who's writing copy, who's the HubSpot example
[00:57:16.000 --> 00:57:18.880]   you get before, you know, a designer who's in the cloud,
[00:57:18.880 --> 00:57:21.840]   who's an agent, an AI agent making you artwork, and then you
[00:57:21.840 --> 00:57:24.400]   stitch all these things together. I've been loading
[00:57:24.400 --> 00:57:28.600]   chat JPT with my kids constantly asking history questions, and
[00:57:28.600 --> 00:57:30.400]   whatever questions they want. I've been teaching them how to
[00:57:30.400 --> 00:57:31.400]   use chat JPT.
[00:57:31.400 --> 00:57:33.600]   That's great. Yeah. I mean, I think it's like, like, there's
[00:57:33.600 --> 00:57:37.680]   this whole transition of humans doing manual labor to doing
[00:57:37.680 --> 00:57:40.720]   knowledge work, where you're using software to create
[00:57:40.720 --> 00:57:44.840]   digital output, to now having more folks spend more of their
[00:57:44.840 --> 00:57:47.600]   time being conceptualists or creators where you can kind of
[00:57:47.600 --> 00:57:52.080]   be an architect or a creator of something, and the system just
[00:57:52.080 --> 00:57:54.840]   generates it, you know, you state your intended objective,
[00:57:54.840 --> 00:57:58.160]   and the system solves for it. Yes. As opposed to, hey, I got
[00:57:58.160 --> 00:58:00.240]   to go build the Excel spreadsheet and check the
[00:58:00.240 --> 00:58:03.400]   formula in every cell and do all the manual. What if I just say,
[00:58:03.400 --> 00:58:05.320]   hey, here's what I want the model to do, please generate it
[00:58:05.320 --> 00:58:09.040]   for me, and you get the result. It enables you to do 100 times
[00:58:09.040 --> 00:58:12.520]   more. That's why I use the analogy conductor, or Augusta
[00:58:12.520 --> 00:58:16.120]   leader, sex, what do you think this is your you're on the
[00:58:16.120 --> 00:58:19.560]   populist side, you really have your finger on what Americans
[00:58:19.560 --> 00:58:25.760]   think. And as a compliment, it's a literal compliment. But I do
[00:58:25.760 --> 00:58:28.160]   think you're I think you've become a pop, especially as the
[00:58:28.160 --> 00:58:30.280]   longer I've known you, we know each other for over 20 years,
[00:58:30.280 --> 00:58:33.120]   you become more populous. So what's the word on the street
[00:58:33.120 --> 00:58:36.680]   here amongst, you know, Jen pop? And when I think they're
[00:58:36.680 --> 00:58:39.360]   taking this news, when they see somebody like Jon Stewart, they
[00:58:39.360 --> 00:58:42.160]   respect when you see somebody like Jonathan's Jon Stewart,
[00:58:42.160 --> 00:58:45.600]   you know, doing this, that's like gonna hit a large swath of
[00:58:45.600 --> 00:58:49.240]   these, you know, you know, elites that we've talked about
[00:58:49.240 --> 00:58:51.280]   before on the show who are losing their jobs, or maybe
[00:58:51.280 --> 00:58:52.480]   their salaries are getting capped.
[00:58:52.480 --> 00:58:56.280]   Well, first of all, Jason, to quote Senator raucous from
[00:58:56.280 --> 00:59:00.000]   gladiator, I may not be a man of the people, but I do try to be a
[00:59:00.000 --> 00:59:04.400]   man for the people. Yes, exactly. So Oh, my God, did you
[00:59:04.400 --> 00:59:08.320]   see the AI? Did you see the AI from some Y combinator company,
[00:59:08.320 --> 00:59:11.880]   but where they like made a little video of us? And like,
[00:59:11.880 --> 00:59:14.280]   we're talking about somebody's nuts. And then they were like,
[00:59:14.280 --> 00:59:18.000]   you said, Oh, I'm gonna ask my butler to ask my assistant to
[00:59:18.000 --> 00:59:21.400]   ask my house manager to then ask my chauffeur to pick those up.
[00:59:21.400 --> 00:59:27.480]   It's like, pretty great. Yeah, it looks funny. Look, to be
[00:59:27.480 --> 00:59:31.400]   frank, no one cares what Jon Stewart thinks. He's never been
[00:59:31.400 --> 00:59:35.920]   less relevant and less funny. This is a story that has been
[00:59:35.920 --> 00:59:39.680]   hyping up for months now. COVID is over. So they need something
[00:59:39.680 --> 00:59:43.000]   else to scare us with. And what they really should be talking is
[00:59:43.000 --> 00:59:46.160]   that we've got two wars that risk spiraling out of control.
[00:59:46.160 --> 00:59:49.960]   And they don't want to go there. I don't want to go there or the
[00:59:49.960 --> 00:59:52.040]   national debt. Exactly right. They don't want to go there
[00:59:52.040 --> 00:59:56.360]   either. Those issues reflect well, on the current
[00:59:56.360 --> 00:59:59.120]   administration and power. So they're going to scare us with
[00:59:59.120 --> 01:00:02.240]   this. Now look, in the short to medium term, AI leads to
[01:00:02.240 --> 01:00:06.080]   productivity gains. In the long term, it may lead to job losses.
[01:00:06.080 --> 01:00:08.800]   But as you guys pointed out, hopefully by then we'll have
[01:00:08.800 --> 01:00:11.920]   lots of other jobs created by the productivity boom that we're
[01:00:11.920 --> 01:00:14.800]   going to get. And this has been the case throughout history with
[01:00:14.800 --> 01:00:17.280]   regard to technology improvements. And if we don't
[01:00:17.280 --> 01:00:19.600]   have these productivity improvements, what's going to
[01:00:19.600 --> 01:00:22.720]   drive the growth in GDP? What's going to allow us to pay off
[01:00:22.720 --> 01:00:26.160]   this enormous national debt that seems to be? Yep, you know, so
[01:00:26.160 --> 01:00:29.280]   large that we it's under payable, we need the productivity
[01:00:29.280 --> 01:00:32.640]   gains that AI is going to unlock without them. We're definitely
[01:00:32.640 --> 01:00:35.760]   toast. So look, I don't put place a lot of stock in this
[01:00:35.760 --> 01:00:40.200]   Jon Stewart story. It's just one of many that the media is is
[01:00:40.200 --> 01:00:41.800]   creating to try and scare us about AI.
[01:00:41.800 --> 01:00:44.000]   And that's actually a great guest. Jon Stewart would be a
[01:00:44.000 --> 01:00:46.360]   great guest along with Lena Khan, put those on the list.
[01:00:46.360 --> 01:00:50.680]   Have you seen this clip where young Lacoon basically says, our
[01:00:50.680 --> 01:00:54.560]   best LLM is 50 times smaller than what a four year old has
[01:00:54.560 --> 01:00:58.200]   processed since they've been four year old is awake, has been
[01:00:58.200 --> 01:01:02.760]   awake a total of 16,000 hours. And you say, okay, 16,000 hours
[01:01:02.760 --> 01:01:07.160]   multiply this by 3600 seconds per hour, and then figure out
[01:01:07.160 --> 01:01:11.600]   like, what's the bandwidth of the optical nerve going to the
[01:01:11.600 --> 01:01:14.680]   cortex, it's about 20 megabytes, where you have 1 million nerve
[01:01:14.680 --> 01:01:19.160]   fibers per, per per eye, and it's about 10 bytes per second,
[01:01:19.160 --> 01:01:23.920]   right, give or take. So multiply, that's 10 to the 15
[01:01:23.920 --> 01:01:27.720]   bytes by the time you're four, 50 times more than whatever LLM,
[01:01:27.720 --> 01:01:30.480]   like the biggest LLM in the world have been trained on. Okay.
[01:01:30.480 --> 01:01:35.160]   So what that tells you is that in the space of a few months, a
[01:01:35.160 --> 01:01:38.560]   baby has seen more information than the biggest LLMs that we
[01:01:38.560 --> 01:01:38.920]   have.
[01:01:38.920 --> 01:01:42.560]   The point is that, and this is one of the foremost experts in AI
[01:01:42.560 --> 01:01:45.920]   and really one of the fathers of modern AI. What he's basically
[01:01:45.920 --> 01:01:50.280]   saying is it's still more artificial than intelligent. And
[01:01:50.280 --> 01:01:53.240]   everybody needs to take a deep breath and understand that
[01:01:53.240 --> 01:01:55.800]   there's just going to be a lot more work before you get to this
[01:01:55.800 --> 01:01:59.280]   omnipresent agent that just replaces and destroys everything
[01:01:59.280 --> 01:02:02.640]   and thinks on its own. Yeah, I'm willing to bet on all of us
[01:02:02.640 --> 01:02:03.960]   versus a bunch of four year olds.
[01:02:03.960 --> 01:02:07.800]   And I just want to say kumbaya to Davos as well. The clip from
[01:02:07.800 --> 01:02:10.920]   Davos, thanks for letting me choose that. Yeah, it's
[01:02:10.920 --> 01:02:16.480]   interesting sacks like the number of jobs that will be
[01:02:16.480 --> 01:02:20.000]   replaced or augmented, and then the creation of jobs and then
[01:02:20.000 --> 01:02:22.360]   you start thinking about well, how many jobs exist in the real
[01:02:22.360 --> 01:02:25.880]   world, I saw Waymo is doing Uber Eats deliveries. And you just
[01:02:25.880 --> 01:02:28.200]   think, well, more people are going to be able to afford Uber
[01:02:28.200 --> 01:02:31.360]   Eats, which is kind of expensive to use. So consumption is going
[01:02:31.360 --> 01:02:33.640]   to go up. And then you think about the optimists. And then
[01:02:33.640 --> 01:02:36.920]   what's the other robot company that's making a general human
[01:02:36.920 --> 01:02:41.240]   robot figure figure. And man, those are starting to get really
[01:02:41.240 --> 01:02:45.160]   interesting. And I think that's going to be the unlock. So maybe
[01:02:45.160 --> 01:02:47.760]   you could speak a little bit to sex. What do you think happens
[01:02:47.760 --> 01:02:50.480]   when we start getting humanoid robots in the mix? And do you
[01:02:50.480 --> 01:02:51.920]   have any investments in that space?
[01:02:51.920 --> 01:02:56.200]   I don't, because I don't do that kind of hardware R&D. I mean,
[01:02:56.200 --> 01:02:59.080]   look, I think you're right that AI does lead to robotics,
[01:02:59.080 --> 01:03:04.240]   because one of the hard things about robotics is just having
[01:03:04.240 --> 01:03:09.040]   the the robot, not just move, but understand what's happening
[01:03:09.040 --> 01:03:12.480]   in the world around it, and then make the right decisions about
[01:03:12.480 --> 01:03:19.280]   how to react to that. And so LLM to start creating a path for the
[01:03:19.280 --> 01:03:22.920]   robot to be able to make intelligent decisions without
[01:03:22.920 --> 01:03:26.360]   having to be programmed with a bunch of if then statements,
[01:03:26.360 --> 01:03:29.560]   right. And I mean, self driving kind of does this too. I mean,
[01:03:29.560 --> 01:03:33.280]   self driving is sort of the early. It's kind of like the
[01:03:33.280 --> 01:03:37.480]   early prototype for these kinds of robots. And that's why it's
[01:03:37.480 --> 01:03:40.520]   not a surprise that Tesla is developing optimists is because
[01:03:40.520 --> 01:03:44.160]   you think about what self driving is, it's, it's a device,
[01:03:44.160 --> 01:03:47.440]   a car with a whole bunch of cameras on it, it takes in all
[01:03:47.440 --> 01:03:50.880]   that visual information, and then it makes decisions about
[01:03:50.880 --> 01:03:55.200]   how to move and how to react. And then it's it's trained based
[01:03:55.200 --> 01:03:59.960]   on mirroring human decisions, all those human decisions that
[01:03:59.960 --> 01:04:02.120]   Tesla's been able to gather through the combination of self
[01:04:02.120 --> 01:04:06.920]   driving with humans intervening, allows it to train the self
[01:04:06.920 --> 01:04:08.920]   driving, I guess, brain, you could say,
[01:04:08.920 --> 01:04:12.240]   well, and it's also sacks moving at two or three miles per hour,
[01:04:12.240 --> 01:04:15.080]   so it can take its time. And if you haven't seen this figure,
[01:04:15.360 --> 01:04:18.400]   this combines the language model with what you're discussing
[01:04:18.400 --> 01:04:21.280]   sacks. So the language models, when you show them a picture,
[01:04:21.280 --> 01:04:25.600]   and you say, Hey, and this is from figure. And there's a, this
[01:04:25.600 --> 01:04:27.920]   is their robot. And it says, Hey, give me something to eat.
[01:04:27.920 --> 01:04:29.080]   Have you guys seen this before?
[01:04:29.080 --> 01:04:31.600]   I've heard some of the founders of these robotics companies talk
[01:04:31.600 --> 01:04:35.400]   about why they create robots in a humanoid shape. And it's not
[01:04:35.400 --> 01:04:39.160]   just because they're trying to create a replacement for humans
[01:04:39.160 --> 01:04:42.280]   or something like that. It's also because now they can point
[01:04:42.280 --> 01:04:45.360]   cameras at the way that humans move. Yes. And so they can
[01:04:45.360 --> 01:04:48.840]   actually train these robots on how humans move and react to
[01:04:48.840 --> 01:04:52.800]   things. So you're able to kind of create a large data set kind
[01:04:52.800 --> 01:04:55.640]   of like with self driving, so that the robots are able to
[01:04:55.640 --> 01:04:59.080]   learn how to how to move. And I've seen a different video
[01:04:59.080 --> 01:05:03.560]   where optimists, the Tesla robot is folding shirts. Yeah,
[01:05:03.560 --> 01:05:04.560]   pretty impressive. Yeah.
[01:05:04.560 --> 01:05:09.160]   What's what's really interesting about this freeberg is, when I
[01:05:09.160 --> 01:05:13.000]   spoke to the people who are making these evolution has made
[01:05:13.000 --> 01:05:18.840]   humans to operate in the world most efficiently over whatever
[01:05:18.840 --> 01:05:24.160]   number of years and creatures before us. So in fact, the world
[01:05:24.160 --> 01:05:29.360]   is optimized for the human body type. And so maybe you could
[01:05:29.360 --> 01:05:32.000]   talk a little bit about what you think is going to happen with
[01:05:32.000 --> 01:05:35.080]   these robots freeberg in the in the short and medium term, when
[01:05:35.080 --> 01:05:38.720]   will we have one of these robots in our houses? What will the
[01:05:38.720 --> 01:05:40.880]   price point be in five to 10 years? And what will they be
[01:05:40.880 --> 01:05:44.200]   doing in five to 10 years? I don't know, we should explore,
[01:05:44.200 --> 01:05:48.000]   we should explore that question at the Olin summit 2024. Okay,
[01:05:48.000 --> 01:05:52.400]   shout out to Ilan. Ilan, can you bring optimists to the event,
[01:05:52.400 --> 01:05:52.880]   please?
[01:05:52.880 --> 01:05:55.200]   Jason, I don't think it's a five year timeframe. I think it's
[01:05:55.200 --> 01:05:58.320]   longer than that. That's just my guess. And one of the reasons is
[01:05:58.320 --> 01:06:03.520]   if you look at the use of robots in call it industrial production
[01:06:03.520 --> 01:06:06.160]   today, they don't want humans getting too close to them.
[01:06:06.160 --> 01:06:07.640]   They're actually kind of dangerous, because you have
[01:06:07.640 --> 01:06:11.040]   these arms flying around, they move quickly, they're very
[01:06:11.040 --> 01:06:13.840]   heavy, you get banged on the head by one of them, it's going
[01:06:13.840 --> 01:06:17.800]   to take you out. So the idea of having a robot in your house
[01:06:17.800 --> 01:06:22.000]   that's capable of freely moving, you have to make that so safe,
[01:06:22.000 --> 01:06:24.080]   to a point that they just haven't gotten to yet with
[01:06:24.080 --> 01:06:27.160]   robots. So there's just gonna be like a lot of fine tuning work
[01:06:27.160 --> 01:06:30.400]   that happens before. This is a domestic product, I think in the
[01:06:30.400 --> 01:06:34.640]   near term, it's all about industrial applications, or
[01:06:34.640 --> 01:06:36.280]   maybe even military applications.
[01:06:36.280 --> 01:06:41.880]   But if you've ever been to the, the giga factories, I was doing
[01:06:41.880 --> 01:06:44.200]   a little tour of one of them once, and somebody grabbed me
[01:06:44.200 --> 01:06:46.760]   because I almost wandered into one of those areas, and they
[01:06:46.760 --> 01:06:50.240]   have tape on the floor, then they have a wall, etc. But if
[01:06:50.240 --> 01:06:54.720]   you even get within a certain closeness with this tape on the
[01:06:54.720 --> 01:06:57.400]   floor, it shuts the whole thing down, because they're afraid
[01:06:57.400 --> 01:06:59.080]   somebody's gonna get crushed behind one of these arms.
[01:06:59.080 --> 01:07:03.000]   Chamath, I'll give it to you. When will we have one of these
[01:07:03.000 --> 01:07:07.520]   robots in our homes for the price of a Prius? Now, by the
[01:07:07.520 --> 01:07:11.960]   way, Prius is a car that costs about $50,000 that common folk
[01:07:11.960 --> 01:07:15.160]   drive. So $50,000 robot in our houses.
[01:07:15.160 --> 01:07:18.760]   I think it'll be less than that. I think it's going to be in the
[01:07:18.760 --> 01:07:22.520]   next two or three years, you'll have a domestic help robot that
[01:07:22.520 --> 01:07:26.120]   you can probably pay 1000 bucks a month for, okay, which would
[01:07:26.120 --> 01:07:29.680]   be two to $3,000 car payment, that would be the equivalent of
[01:07:29.680 --> 01:07:32.920]   a car payment on $100,000 car. So okay, you say under
[01:07:32.920 --> 01:07:36.840]   five, you say three, what do you think freeberg same bet $1,000
[01:07:36.840 --> 01:07:40.080]   a month robot $100,000 sticker price, when we have that in our
[01:07:40.080 --> 01:07:44.520]   homes? No, I think it's 1000 a month, 1000 a month, which would
[01:07:44.520 --> 01:07:47.560]   be the equivalent over whatever number of months and what is it
[01:07:47.560 --> 01:07:50.200]   it does its general purpose does different stuff, general
[01:07:50.200 --> 01:07:54.160]   person's robot $1,000 a month, 50 payments, I think it washes
[01:07:54.160 --> 01:07:58.240]   the dishes, I think it will do the laundry, take out the trash,
[01:07:58.240 --> 01:08:01.480]   there'll be like a whole set of house household tasks that it
[01:08:01.480 --> 01:08:07.280]   will do. Walk the dog. No, no, not responsible for a live
[01:08:07.280 --> 01:08:10.320]   creature. What do you say free bird? $1,000 a month at home
[01:08:10.320 --> 01:08:14.000]   robot does your dishes. There's a great bet for us. Give us the
[01:08:14.000 --> 01:08:17.000]   over under how many years for you? I'm not sure. I think the
[01:08:17.000 --> 01:08:19.160]   salt of science Come on, man, give us a year.
[01:08:19.160 --> 01:08:22.440]   Well, I don't think it necessarily follows this general
[01:08:22.440 --> 01:08:26.160]   purpose model. I think that there are likely going to be
[01:08:26.160 --> 01:08:33.760]   more narrow application ranges. And they're not going to
[01:08:33.760 --> 01:08:39.360]   necessarily be humanoid in form factor. I don't know if you guys
[01:08:39.360 --> 01:08:42.400]   have seen gecko robotics, we got seen this company, are you guys
[01:08:42.400 --> 01:08:47.440]   investors in this? Nope. Pretty impressive, like suite of
[01:08:47.440 --> 01:08:53.440]   autonomous products that do specific things in industrial
[01:08:53.440 --> 01:08:57.520]   settings. So they have like, robots that climb on the outside
[01:08:57.520 --> 01:09:00.840]   of buildings and look for cracks using special scanning
[01:09:00.840 --> 01:09:03.960]   equipment, but they're very autonomous and how they operate
[01:09:03.960 --> 01:09:05.640]   and what they can do. And they've got a whole class of
[01:09:05.640 --> 01:09:10.080]   robots that can then be each one of those robots can do many
[01:09:10.080 --> 01:09:13.680]   different tasks for many different applications. And so
[01:09:13.680 --> 01:09:17.520]   the form factors, they've got kind of a set of form factors,
[01:09:17.520 --> 01:09:20.280]   meaning a set of robots that look differently, and have
[01:09:20.280 --> 01:09:23.480]   different capabilities of them, like little spider legs or arms
[01:09:23.480 --> 01:09:27.920]   or whatever. And then they can be applied to go do something
[01:09:27.920 --> 01:09:30.240]   autonomously. And then they just run and they do it. If you pull
[01:09:30.240 --> 01:09:34.560]   that up, you'll see it climbing walls, riding along pipes. Yeah,
[01:09:34.560 --> 01:09:37.680]   they're built. Well, they're not purpose. That's what's
[01:09:37.680 --> 01:09:41.520]   interesting. They're they're sort of a narrow range of
[01:09:41.520 --> 01:09:45.600]   applications, but they're not specific to do only one thing.
[01:09:45.600 --> 01:09:48.080]   And so they can work in different environments and do
[01:09:48.080 --> 01:09:51.280]   different things. And so you'll kind of pick from their suite of
[01:09:51.280 --> 01:09:55.320]   robots, which ones you want to use to do different, different
[01:09:55.320 --> 01:09:57.680]   tasks, and then they go and do it. It's really interesting.
[01:09:57.680 --> 01:09:59.520]   They're mostly using them for industrial monitoring
[01:09:59.520 --> 01:10:02.000]   applications right now, like looking on bridges for breaks
[01:10:02.000 --> 01:10:06.440]   and cleaning, cleaning windows, cleaning windows, all that kind
[01:10:06.440 --> 01:10:09.680]   of stuff. Oh, cleaning windows. That's a good Yeah. So they've
[01:10:09.680 --> 01:10:11.600]   got like a really cool suite. And I think that's what we're
[01:10:11.600 --> 01:10:14.640]   likely to see in domestic settings as well. Alright, so
[01:10:14.640 --> 01:10:18.040]   you I'm just I'm just not sure over three, by the way, I will
[01:10:18.040 --> 01:10:21.040]   say the success of Gecko indicates that there's far more
[01:10:21.040 --> 01:10:25.520]   money to be made in industrial applications. And there isn't
[01:10:25.520 --> 01:10:30.480]   consumer applications today. I disagree. Yeah, I think every
[01:10:30.480 --> 01:10:32.600]   human is going to have one of these. And I think every
[01:10:32.600 --> 01:10:35.560]   household in America, every middle class household in
[01:10:35.560 --> 01:10:38.760]   America will have one of these $1,000 a month robots in seven
[01:10:38.760 --> 01:10:42.840]   years. I'll give it seven. You say you want to do everything. I
[01:10:42.840 --> 01:10:47.880]   say to do domestic chores, taking out the trash, folding
[01:10:47.880 --> 01:10:51.800]   laundry, domestic tasks. I just think it's hard to justify that
[01:10:51.800 --> 01:10:54.600]   because you're only spending so many hours a week doing that
[01:10:54.600 --> 01:10:57.280]   sort of stuff. Is it really worth 1000 bucks a month?
[01:10:57.280 --> 01:10:59.840]   Whereas in the industrial setting, it makes a lot more
[01:10:59.840 --> 01:11:02.160]   sense about those dangerous tasks, like climbing on a
[01:11:02.160 --> 01:11:05.720]   bridge, looking at the seams and climbing on a building cleaning
[01:11:05.720 --> 01:11:10.000]   the windows. Those tasks take years to do sometimes. Many,
[01:11:10.000 --> 01:11:13.640]   many years of high risk human labor, whereas taking out the
[01:11:13.640 --> 01:11:16.600]   trash and folding laundry might be a little bit more hard to
[01:11:16.600 --> 01:11:19.560]   justify this better. It's more breaking news here during the
[01:11:19.560 --> 01:11:24.320]   program. We'll get our war correspondent, our geopolitical
[01:11:24.320 --> 01:11:27.320]   expert, David Sachs. What are you seeing on the wires?
[01:11:27.320 --> 01:11:30.920]   Well, there's a NATO meeting going on right now. And Blinken
[01:11:30.920 --> 01:11:33.440]   did a press conference where he says that Ukraine will be
[01:11:33.440 --> 01:11:37.640]   joining NATO. That's the big news going viral right now.
[01:11:37.800 --> 01:11:43.360]   Ukraine will become a member of NATO. Our purpose at the summit
[01:11:43.360 --> 01:11:49.120]   is to help build a bridge to that membership and to create a
[01:11:49.120 --> 01:11:54.560]   clear pathway for for Ukraine moving forward. So of course, we
[01:11:54.560 --> 01:11:58.160]   believe that Ukraine deserves to be a member of NATO and that
[01:11:58.160 --> 01:12:01.840]   this should happen sooner rather sooner sooner rather than later.
[01:12:01.840 --> 01:12:06.760]   Chamath, any thoughts on this flip that just broke during the
[01:12:06.760 --> 01:12:12.040]   program? Well, we just, I think NATO just added Sweden, right?
[01:12:12.040 --> 01:12:17.040]   And it was done in pretty record time from application to
[01:12:17.040 --> 01:12:20.760]   admission. So I would like to know whether is this just
[01:12:20.760 --> 01:12:25.000]   rhetoric to just keep everybody at bay and placate the
[01:12:25.000 --> 01:12:29.800]   Ukrainians? Or is this real? The problem that this creates is
[01:12:29.800 --> 01:12:34.800]   that if it is real, and they're admitted, then NATO has to
[01:12:34.800 --> 01:12:40.400]   defend Ukraine, which means that then America and all the
[01:12:40.400 --> 01:12:43.000]   other NATO allies would have to fight, which means that we're in
[01:12:43.000 --> 01:12:46.280]   a war, America should not be in a war.
[01:12:46.280 --> 01:12:49.600]   Just give you the exact fact you are correct. Sweden, Finland
[01:12:49.600 --> 01:12:54.880]   applied to join in May 2022. Following Russia's invasion of
[01:12:54.880 --> 01:12:57.920]   Ukraine, and they had been neutral, as you know, for many
[01:12:57.920 --> 01:13:02.760]   decades, Finland has a massive land border with Russia. And
[01:13:02.760 --> 01:13:06.800]   they joined in April of 2023, after applying in May of 22. So
[01:13:06.800 --> 01:13:09.480]   just a year later, and Sweden became a member in March of
[01:13:09.480 --> 01:13:13.920]   2024. Just two years later, efforts membership was held up
[01:13:13.920 --> 01:13:17.480]   by both Turkey and Hungary. Saks, you're our resident expert
[01:13:17.480 --> 01:13:20.760]   on Ukraine and all things geopolitical, your thoughts.
[01:13:20.760 --> 01:13:23.720]   On the one hand, what Lincoln is saying is more of the same here,
[01:13:23.720 --> 01:13:26.200]   because it's been the administration's policy to seek
[01:13:26.200 --> 01:13:30.360]   to bring Ukraine into NATO since they took office. They've
[01:13:30.400 --> 01:13:34.120]   reiterated that over and over again. And it's one of the major
[01:13:34.120 --> 01:13:36.960]   reasons for this war is that the Russians said over and over
[01:13:36.960 --> 01:13:39.360]   again, this was a red line for them. That's why there's a war
[01:13:39.360 --> 01:13:42.520]   in Ukraine. The idea that you're going to be able to bring
[01:13:42.520 --> 01:13:47.520]   Ukraine into NATO, however, when the war is going so badly, is
[01:13:47.520 --> 01:13:51.000]   now entering the territory of being delusional. I mean, this
[01:13:51.000 --> 01:13:54.160]   is like a delusional comment. And if you just want to
[01:13:54.160 --> 01:13:57.120]   understand how badly things are going, look at yesterday's
[01:13:57.120 --> 01:14:00.440]   politico, which was called Ukraine is at great risk of its
[01:14:00.440 --> 01:14:04.440]   front lines collapsing. The source for this article was
[01:14:04.440 --> 01:14:08.720]   high ranking Ukrainian officials close to Zaluzhny, who's the
[01:14:08.720 --> 01:14:11.480]   former commander in chief. Some people have speculated that
[01:14:11.480 --> 01:14:14.880]   Zaluzhny himself might be the source, but at a minimum, it's
[01:14:14.880 --> 01:14:19.440]   high ranking Ukrainian officers who reported to Zaluzhny. And
[01:14:19.440 --> 01:14:23.520]   what they say in this article is that the prognosis in Ukraine is
[01:14:23.520 --> 01:14:27.400]   grim. They say that the sad truth is that even if the
[01:14:27.400 --> 01:14:31.200]   funding bills approved by the US Congress, a massive resupply
[01:14:31.200 --> 01:14:33.840]   may not be enough to prevent a major battlefield upset. They
[01:14:33.840 --> 01:14:36.800]   say that there is a great risk of the front lines collapsing
[01:14:36.800 --> 01:14:40.480]   wherever Russian generals decide to focus their offensive, which
[01:14:40.480 --> 01:14:44.360]   people expect in the next few months. And there's nothing that
[01:14:44.360 --> 01:14:47.120]   can help Ukraine now because there are no serious
[01:14:47.120 --> 01:14:50.520]   technologies able to compensate Ukraine for the large mass of
[01:14:50.520 --> 01:14:54.600]   troops Russia is likely to hurl at us. This is a quote from one
[01:14:54.600 --> 01:14:58.120]   of the Ukrainian officials. We don't have those technologies
[01:14:58.120 --> 01:15:02.600]   and the West doesn't have them as well, insufficient numbers. So
[01:15:02.600 --> 01:15:06.320]   what they're saying is that even if the funding bill goes through
[01:15:06.320 --> 01:15:10.720]   the 61 billion, it's not going to be enough to save Ukraine. And
[01:15:10.720 --> 01:15:14.520]   at the very moment that that is now being finally honestly
[01:15:14.520 --> 01:15:17.720]   reported by Western media, it's something I've been saying now
[01:15:17.720 --> 01:15:21.840]   for months. Finally, the truth is coming out. You have Blinken
[01:15:21.840 --> 01:15:24.920]   doubling and tripling down on these comments that nevertheless
[01:15:24.920 --> 01:15:27.920]   Ukraine will be joining NATO. And Tomas is right under Article
[01:15:27.920 --> 01:15:32.000]   five, an attack on one is an attack on all. Therefore, if
[01:15:32.000 --> 01:15:35.880]   Ukraine becomes part of NATO, an attack on Ukraine by Russia,
[01:15:35.880 --> 01:15:38.600]   which is currently ongoing, will be concerned an attack on the
[01:15:38.600 --> 01:15:41.880]   United States. Then you have to add to the mix the fact that
[01:15:41.880 --> 01:15:44.720]   Macron and other European leaders have actually been
[01:15:44.720 --> 01:15:48.240]   advocating for NATO to sending ground troops. And he said this
[01:15:48.240 --> 01:15:51.200]   over and over again, he's doubled down on this multiple
[01:15:51.200 --> 01:15:55.200]   times. So you have a dynamic now where this isn't just hot
[01:15:55.200 --> 01:16:00.040]   rhetoric by Blinken. This really has the risk of tipping over
[01:16:00.040 --> 01:16:05.080]   into policy, I would say in a Biden second term, where Biden
[01:16:05.080 --> 01:16:08.880]   agrees to do what our European allies are already calling for,
[01:16:08.880 --> 01:16:13.720]   which is sending NATO troops to Ukraine to save Ukraine from
[01:16:13.720 --> 01:16:17.280]   what Politico calls an imminent collapse. I think this is a
[01:16:17.280 --> 01:16:19.440]   very dangerous situation. I mean, we're really talking about
[01:16:19.440 --> 01:16:23.720]   here is World War Three. So if you want to have a serious
[01:16:23.720 --> 01:16:26.000]   chance of war three in the next four years, and I would say go
[01:16:26.000 --> 01:16:28.480]   ahead and vote for Biden in November. I mean, this is very
[01:16:28.480 --> 01:16:31.440]   clear to me. I'm personally not willing to accept that risk. I'm
[01:16:31.440 --> 01:16:35.560]   not willing to accept a 10% or 1% risk of that chance. But I
[01:16:35.560 --> 01:16:38.120]   think Blinken putting it on the table here, I think people
[01:16:38.120 --> 01:16:41.280]   should be deeply concerned about this. And there should be a lot
[01:16:41.280 --> 01:16:44.160]   of follow up questions for Blinken and the administration
[01:16:44.160 --> 01:16:45.440]   about this freeberg.
[01:16:45.440 --> 01:16:51.120]   Should the free country of Ukraine be able to join NATO on
[01:16:51.120 --> 01:16:54.240]   some timeline? Or should they be banned from ever joining?
[01:16:54.240 --> 01:16:58.760]   I think the statements are correct, that Ukraine joining
[01:16:58.760 --> 01:17:02.360]   NATO escalates conflict. And we will find ourselves in a de
[01:17:02.360 --> 01:17:11.240]   facto global conflict World War. Now, the question is, is that
[01:17:11.240 --> 01:17:15.640]   the cycle, the natural cycle? I will once again, Nick, pull it
[01:17:15.640 --> 01:17:20.520]   up, please reference Ray Dalio is typical big cycle behind
[01:17:20.520 --> 01:17:25.320]   empires rise and decline. As he spoke at length with us in
[01:17:25.320 --> 01:17:29.160]   person about at the All in Summit last year, he points out
[01:17:29.160 --> 01:17:33.960]   that the era of prosperity, that over the last 500 years, we've
[01:17:33.960 --> 01:17:38.200]   seen six major empires go through is followed by a debt
[01:17:38.200 --> 01:17:44.040]   bubble, which drives a wealth gap, which ultimately leads to
[01:17:44.040 --> 01:17:46.720]   economic challenges, which means printing more money, which is
[01:17:46.720 --> 01:17:50.640]   the cycle we are going through right now, with a, as you guys
[01:17:50.640 --> 01:17:55.720]   know, two to $3 trillion annual deficit and explosion in federal
[01:17:55.720 --> 01:18:00.280]   debt levels. And that ultimately leads inevitably to external
[01:18:00.280 --> 01:18:04.840]   conflict to war. Now, the particular motivations in every
[01:18:04.840 --> 01:18:08.040]   case in all six times, this has happened in the last 500 years,
[01:18:08.720 --> 01:18:12.400]   look different when you read the history books about what were
[01:18:12.400 --> 01:18:15.560]   the circumstances that drove us to external conflict that drove
[01:18:15.560 --> 01:18:19.840]   that nation to war. But the truth is, every single one of
[01:18:19.840 --> 01:18:25.160]   them was preceded by a debt bubble, income inequality, wealth
[01:18:25.160 --> 01:18:28.960]   gap, and the printing of money. And there's a relationship
[01:18:28.960 --> 01:18:33.120]   between those economic factors and a desire for conflict. And I
[01:18:33.120 --> 01:18:35.920]   think that is what we are seeing play out over the past couple of
[01:18:35.920 --> 01:18:41.480]   years, starting with our motivated interest in supporting
[01:18:41.480 --> 01:18:45.640]   Ukraine, against the Russia conflict, and now escalating it
[01:18:45.640 --> 01:18:48.920]   towards inviting Ukraine to join NATO to escalate the conflict
[01:18:48.920 --> 01:18:54.960]   itself. Now, I think there's a notion that having a war is
[01:18:54.960 --> 01:18:58.080]   stimulating, having a war is unifying, this should be the
[01:18:58.080 --> 01:19:01.640]   wags the door wag the dog theory. I don't think it's a
[01:19:01.640 --> 01:19:05.200]   wag the dog theory as much as it is what do you do when the
[01:19:05.200 --> 01:19:08.800]   economic condition of the nation is such that the federal
[01:19:08.800 --> 01:19:12.360]   government has to print money to support the economy and or to
[01:19:12.360 --> 01:19:15.600]   bridge the wealth gap. And when under those circumstances in
[01:19:15.600 --> 01:19:20.320]   order to unify the country in order to motivate a system of
[01:19:20.320 --> 01:19:24.480]   unification amongst a fracturing society or fracturing economic
[01:19:24.480 --> 01:19:29.680]   strata, you feel like you have to have an external enemy, and
[01:19:29.680 --> 01:19:34.120]   that the notion of war itself is economically stimulating. I
[01:19:34.120 --> 01:19:36.320]   think that those are the motivating factors that we've
[01:19:36.320 --> 01:19:40.280]   seen play out six times in the last 500 years. And we may be
[01:19:40.280 --> 01:19:43.080]   unfortunately seeing play out here again, as we talked to
[01:19:43.080 --> 01:19:47.160]   Graham Allison, Ray Dalio about last year, we said, what can we
[01:19:47.160 --> 01:19:50.840]   do to avoid this that there have been times historically, where
[01:19:50.840 --> 01:19:54.680]   these things have been avoided. But if we're not being cognizant
[01:19:54.680 --> 01:19:59.200]   of what's going on here, and motivating a different tact and
[01:19:59.200 --> 01:20:03.440]   a different path, whether it's through our electoral cycle, or
[01:20:03.440 --> 01:20:07.520]   through being loud and vocal in whatever media channels we each
[01:20:07.520 --> 01:20:12.040]   have access to, to make folks more aware of this, I think, you
[01:20:12.040 --> 01:20:14.720]   know, we will find ourselves walking down this path of
[01:20:14.720 --> 01:20:16.600]   looking for global conflict and finding it.
[01:20:16.600 --> 01:20:19.080]   Jamal, you look like you wanted to chime in there. Yeah.
[01:20:19.080 --> 01:20:22.720]   Of the three presidential candidates, to be very clear,
[01:20:22.720 --> 01:20:29.680]   one is supportive, then, of some kind of confrontation, because
[01:20:29.680 --> 01:20:33.280]   by proxy, they're supportive of admitting Ukraine into NATO,
[01:20:33.280 --> 01:20:36.280]   which would create a war and two are pretty clearly anti war.
[01:20:36.280 --> 01:20:40.040]   And just for people who know the boom bust cycle behind empires
[01:20:40.040 --> 01:20:43.640]   rise and declines. You can see that if you're on the YouTube
[01:20:43.640 --> 01:20:48.800]   video, but the sixth of eight moments is revolutions and wars.
[01:20:48.800 --> 01:20:51.400]   As freebirds pointing out, there are two more that come after
[01:20:51.400 --> 01:20:55.480]   that debt and political restructuring, and then the new
[01:20:55.480 --> 01:20:59.040]   world order emerges. And so the question here, I guess becomes,
[01:20:59.440 --> 01:21:03.920]   can diplomacy when the day and then is Blinken's point that
[01:21:03.920 --> 01:21:06.280]   they eventually can become a member or that they're
[01:21:06.280 --> 01:21:08.840]   imminently going to become a member and it's breaking news. So
[01:21:08.840 --> 01:21:12.320]   we don't know if he's speaking about
[01:21:12.320 --> 01:21:15.680]   let's just be really clear on this, the words he used were
[01:21:15.680 --> 01:21:20.120]   very carefully chosen. And that means that there was a media and
[01:21:20.120 --> 01:21:24.000]   press strategy conversation that was had by him and his staff,
[01:21:24.000 --> 01:21:27.720]   which obviously found its way into the White House
[01:21:27.720 --> 01:21:30.520]   administration, and that there was a executive conversation
[01:21:30.520 --> 01:21:34.080]   about this, for sure, this is the positioning we need to now be
[01:21:34.080 --> 01:21:37.600]   clearly stating, which means that this is now policy. He did
[01:21:37.600 --> 01:21:41.000]   not slip up on those words. This was not some off the cuff
[01:21:41.000 --> 01:21:45.000]   comment. This was clearly a media trained statement, which
[01:21:45.000 --> 01:21:47.760]   means that it is it is administration was delivered
[01:21:47.760 --> 01:21:52.000]   during exactly at the next White House press conference, you will
[01:21:52.000 --> 01:21:55.040]   hear the question asked by reporters. Is this the White
[01:21:55.040 --> 01:21:58.760]   House position? And they will say yes, it is. Yeah. And just
[01:21:58.760 --> 01:22:01.880]   to be clear that that video had a couple of edits in it. And we
[01:22:01.880 --> 01:22:04.200]   don't have the full press conference here. The quote from
[01:22:04.200 --> 01:22:07.760]   the hill is Ukraine will become a member of NATO period. Our
[01:22:07.760 --> 01:22:10.720]   purpose at the summit is to help build a bridge to that
[01:22:10.720 --> 01:22:14.120]   membership, which then seems like this would this if you're
[01:22:14.120 --> 01:22:17.880]   building a bridge that takes time. So maybe they mean over
[01:22:17.880 --> 01:22:22.760]   time, the fullness of time to be able to do this in the next year
[01:22:22.760 --> 01:22:25.720]   would be if it was on the timeline of Finland would be
[01:22:25.720 --> 01:22:27.320]   insane. I cannot
[01:22:27.320 --> 01:22:29.800]   it would be insane, but they're not ruling it out. And I think
[01:22:29.800 --> 01:22:32.400]   you have to look at the context of what's happening. He's making
[01:22:32.400 --> 01:22:36.920]   these remarks, as all the news from the battlefield is
[01:22:36.920 --> 01:22:41.520]   terrible. Ukraine is losing and it's at risk of collapsing. And
[01:22:41.520 --> 01:22:45.720]   European leaders like Macron are therefore calling for direct
[01:22:45.720 --> 01:22:50.080]   NATO intervention in the war. So for Lincoln to be making this
[01:22:50.080 --> 01:22:53.000]   sort of statement is really adding fuel to the fire. And
[01:22:53.000 --> 01:22:55.480]   let's see if he walks it back. Let's see if he clarifies it. I
[01:22:55.480 --> 01:22:57.120]   predict that he won't, because this is a
[01:22:57.120 --> 01:23:00.360]   clarification for sure. Because this does not feel like it'd be
[01:23:00.360 --> 01:23:03.880]   good for voting because the war is credibly unpopular. It'll
[01:23:03.880 --> 01:23:07.080]   make the election pretty simple. Every other issue. All the
[01:23:07.080 --> 01:23:10.440]   social issues that we fight about will fall away. The debt
[01:23:10.440 --> 01:23:14.280]   will fall away. The border will fall away. If this is true, if
[01:23:14.280 --> 01:23:16.840]   it's true, does America want to go to war?
[01:23:16.840 --> 01:23:19.640]   Well, Chamath, I think I think we're already at that point,
[01:23:19.640 --> 01:23:22.160]   even if there is some sort of clarification. And the reason I
[01:23:22.160 --> 01:23:25.600]   say that is because Biden clearly is very committed to
[01:23:25.600 --> 01:23:28.200]   this Ukraine policy. It didn't just start when he became
[01:23:28.200 --> 01:23:31.400]   president, it started when he became vice president, and was
[01:23:31.400 --> 01:23:35.080]   managing the Ukraine portfolio for Obama. This is why Hunter
[01:23:35.080 --> 01:23:38.200]   Biden got that job in Ukraine, because Biden was running the
[01:23:38.200 --> 01:23:41.280]   show there. And they have been very committed to this idea of
[01:23:41.280 --> 01:23:44.640]   bringing Ukraine into NATO for decades. I mean, he supported
[01:23:44.640 --> 01:23:47.760]   when he was a senator. So this is not like Freeberg said, this
[01:23:47.760 --> 01:23:50.800]   is not like to some randomly chosen words out of the blue,
[01:23:50.800 --> 01:23:53.840]   Blinken measures his words carefully, he knows what he's
[01:23:53.840 --> 01:23:58.480]   saying. And this is something that Biden clearly is passionate
[01:23:58.480 --> 01:24:01.480]   about. And what you have to believe is that in a Biden
[01:24:01.480 --> 01:24:05.160]   second term, he's going to manage this whole situation so
[01:24:05.160 --> 01:24:08.600]   perfectly, that this war is not going to escalate any further.
[01:24:08.600 --> 01:24:12.080]   And I just I have no confidence in that. Remember, if you want
[01:24:12.080 --> 01:24:16.080]   to use a historical analogy, go back to Woodrow Wilson in 1916.
[01:24:16.480 --> 01:24:20.200]   He was elected on literally the catchphrase, he kept us out of
[01:24:20.200 --> 01:24:24.800]   war. Less than one year later, we were in World War One. World
[01:24:24.800 --> 01:24:29.200]   War One. Yeah. So this idea of what Biden wouldn't possibly get
[01:24:29.200 --> 01:24:33.360]   us into a war. I mean, history shows otherwise. History shows
[01:24:33.360 --> 01:24:38.160]   that presidents once they win reelection are more likely to
[01:24:38.160 --> 01:24:41.200]   get us into war, rather than less, because they don't have to
[01:24:41.200 --> 01:24:45.120]   fear voters. So then the question is, well, what is in
[01:24:45.120 --> 01:24:48.440]   Joe Biden's heart? What's he passionate about? He is clearly
[01:24:48.440 --> 01:24:52.600]   passionate about this cause about bringing Ukraine into NATO,
[01:24:52.600 --> 01:24:56.200]   and certainly not having Ukraine collapse or lose this war.
[01:24:56.200 --> 01:24:59.920]   Whereas Trump and Bobby Kennedy have both said that they will
[01:24:59.920 --> 01:25:05.280]   end this war, they will seek a peace deal, if elected. I think
[01:25:05.280 --> 01:25:06.240]   that's enough right there.
[01:25:06.240 --> 01:25:09.320]   Yeah, I mean, and obviously, Lincoln and Biden are looking
[01:25:09.320 --> 01:25:11.280]   for peace as well. They just don't want to lose the war.
[01:25:11.280 --> 01:25:14.680]   No, they're not looking for peace. I just think they are
[01:25:14.680 --> 01:25:17.320]   definitely for peace, you can disagree, but they want peace.
[01:25:17.320 --> 01:25:19.320]   And why do they reject the deal at Istanbul at the beginning of
[01:25:19.320 --> 01:25:19.880]   this war?
[01:25:19.880 --> 01:25:23.720]   Yeah, because they, I think, don't want Russia to determine
[01:25:23.720 --> 01:25:26.200]   who gets to be in NATO, and they want free countries to decide.
[01:25:26.200 --> 01:25:27.800]   Is that worth going to war for?
[01:25:27.800 --> 01:25:31.920]   I mean, that is, I guess, the existential question here is, at
[01:25:31.920 --> 01:25:35.360]   what point do we want to let free democracies determine their
[01:25:35.360 --> 01:25:38.280]   future, and protect them from invading countries? That is,
[01:25:38.280 --> 01:25:40.920]   like, actually, the core of this is, do you believe in
[01:25:40.920 --> 01:25:43.920]   democracy? Do you believe free countries should have the
[01:25:43.920 --> 01:25:47.880]   autonomy to pick their future? And is that worth fighting for?
[01:25:47.880 --> 01:25:50.080]   That is the question the world faces right now.
[01:25:50.080 --> 01:25:53.240]   I think that framing, I think that framing is not totally
[01:25:53.240 --> 01:25:55.760]   accurate. I think, of course, those things are good and right
[01:25:55.760 --> 01:26:00.080]   things. I think the thing is, on the balance of issues, there are
[01:26:00.080 --> 01:26:03.840]   seasons when certain priorities need to be shaped by a country.
[01:26:03.840 --> 01:26:09.520]   And right now, we're in a season where there's tremendous
[01:26:09.560 --> 01:26:14.400]   domestic instability in our country. And, and our country,
[01:26:14.400 --> 01:26:16.600]   you're saying in our country, yeah, in our balance sheet is
[01:26:16.600 --> 01:26:19.640]   breaking. So I think the question isn't that is democracy
[01:26:19.640 --> 01:26:23.320]   important? Of course, it's important. It's how relatively
[01:26:23.320 --> 01:26:28.760]   important is it abroad, relative to these domestic issues here?
[01:26:28.760 --> 01:26:29.880]   Yeah, but I know.
[01:26:29.880 --> 01:26:34.360]   Hold on, let me just finish. Is it worth fighting for is the
[01:26:34.360 --> 01:26:37.080]   issue? And is it worth fighting for when you don't have the
[01:26:37.080 --> 01:26:40.600]   resources to do it? Now, if we were sitting here, and a country
[01:26:40.600 --> 01:26:45.080]   next to Ukraine was invaded, say, Finland, or say, France, or
[01:26:45.080 --> 01:26:49.200]   another country in Europe was invaded, we would absolutely go
[01:26:49.200 --> 01:26:51.800]   to bat for them. But for Ukraine, we won't go to bat for
[01:26:51.800 --> 01:26:55.480]   them. They're not part of part of NATO. And you know, this is
[01:26:55.480 --> 01:26:55.920]   when you say
[01:26:55.920 --> 01:26:59.520]   are you saying send American soldiers, because that's what
[01:26:59.520 --> 01:27:00.080]   we're talking about.
[01:27:00.080 --> 01:27:03.600]   If France or Finland was, would you be opposed to France? If
[01:27:03.600 --> 01:27:06.240]   Russia invaded France? Would you defend France? Would you be in
[01:27:06.240 --> 01:27:09.920]   favor of the course, that's, that's our article five guarantee
[01:27:09.920 --> 01:27:12.200]   under NATO. This is why I don't want to extend an article five
[01:27:12.200 --> 01:27:15.200]   guarantee to Ukraine, because it will put us directly in conflict
[01:27:15.200 --> 01:27:18.360]   with Russia. And I'm not interested in being in war three.
[01:27:18.360 --> 01:27:21.440]   Right. And then so Finland and Sweden come up. And I guess the
[01:27:21.440 --> 01:27:24.480]   argument would be would you be in favor of sending troops to
[01:27:24.480 --> 01:27:26.600]   defend Finland and Sweden, the latest members of NATO? And
[01:27:26.600 --> 01:27:29.520]   would you be in favor of it? Now we're committed. And when you
[01:27:29.520 --> 01:27:31.680]   were in favor of them joining NATO, I guess is the next
[01:27:31.680 --> 01:27:34.200]   question should we discuss it on the pod, I explained that it was
[01:27:34.200 --> 01:27:36.840]   creating a liability, not an asset, but what's done is done.
[01:27:36.840 --> 01:27:40.080]   Should free countries be able to join NATO, I guess is at the end
[01:27:40.080 --> 01:27:43.280]   of the question, actually, makes me make two points on that. The
[01:27:43.280 --> 01:27:47.120]   first one is, countries don't have a right to join NATO any
[01:27:47.120 --> 01:27:50.520]   more than I have a right to join Augusta country club. Just because
[01:27:50.520 --> 01:27:53.720]   I'm a golf player doesn't mean I get to join Augusta. Okay, it's
[01:27:53.720 --> 01:27:59.080]   up to the current membership of Augusta or NATO decide whether
[01:27:59.080 --> 01:28:01.640]   they're going to admit a country based on what is in their
[01:28:01.640 --> 01:28:05.240]   interests. It has never been in our interest to make Ukraine
[01:28:05.240 --> 01:28:07.840]   security dependent of the United States. Sorry, this is the
[01:28:07.840 --> 01:28:12.120]   reality. The second thing I want to point out is that what was
[01:28:12.120 --> 01:28:16.000]   Russia demanding, they were demanding Ukrainian neutrality,
[01:28:16.000 --> 01:28:20.840]   they were not basically looking to conquer Ukraine, they wanted
[01:28:20.840 --> 01:28:24.840]   them to be neutral. So Ukraine did not have to give up its
[01:28:24.840 --> 01:28:28.880]   freedom. Okay, they just had to agree to be neutral. That was
[01:28:28.880 --> 01:28:31.520]   the key issue. That's what makes it very different than some
[01:28:31.520 --> 01:28:35.160]   other historical analogies. And that was not acceptable to us at
[01:28:35.160 --> 01:28:38.000]   the very beginning of the war. Blinken said that we would
[01:28:38.000 --> 01:28:41.320]   insist on an open door policy would be a clearly the right
[01:28:41.320 --> 01:28:43.360]   move here would have been to kick the can down the road and
[01:28:43.360 --> 01:28:46.000]   just tell Putin, we'll take it off. We'll take NATO off the
[01:28:46.000 --> 01:28:49.200]   table for 10 years or 20 years. And then we could have outlasted
[01:28:49.200 --> 01:28:53.800]   Putin. You and I agreed on that before the war started. And then
[01:28:53.800 --> 01:28:56.120]   the minute the war started, everyone forgot that that was
[01:28:56.120 --> 01:29:00.880]   the key costumes Bell live this war, just more diplomacy is
[01:29:00.880 --> 01:29:05.160]   better. Can I just say one thing? Saxe, you would be the
[01:29:05.160 --> 01:29:09.120]   perfect member of Augusta. For one small issue.
[01:29:09.120 --> 01:29:12.480]   Rhymes with snus.
[01:29:12.480 --> 01:29:15.840]   Oh, Augusta. Oh, my gosh.
[01:29:15.840 --> 01:29:22.160]   My favorite from who is the lunatic? deranged guy from New
[01:29:22.160 --> 01:29:25.920]   York, we see in Congress and they expelled him after six
[01:29:25.920 --> 01:29:30.320]   months. George said to us, are you Jewish? He's like, I'm Jew.
[01:29:30.840 --> 01:29:35.760]   Ish ish, like a little pause in the middle. Alright, listen,
[01:29:35.760 --> 01:29:39.880]   another amazing episode, episode 173. Congratulations to our CEO
[01:29:39.880 --> 01:29:42.920]   john hell. He's with us now who will make
[01:29:42.920 --> 01:29:53.080]   a nice callback. Please do us a favor. Do us a favor. 486,000
[01:29:53.080 --> 01:29:56.000]   people following the YouTube channel, get in there and be
[01:29:56.000 --> 01:30:01.240]   part of the q&a when we hit 500,000 and your best chance of
[01:30:01.240 --> 01:30:04.320]   being part of the 1 million subscriber party, which I think
[01:30:04.320 --> 01:30:08.520]   Chamath is going to oversee. It'll be at the Wynn in Las
[01:30:08.520 --> 01:30:11.960]   Vegas there. Oh, okay, there it is. We have a location and then
[01:30:11.960 --> 01:30:15.260]   the all in some make an announcement next week. Go to
[01:30:15.260 --> 01:30:20.240]   YouTube type in all in and subscribe for the rain man,
[01:30:20.240 --> 01:30:23.480]   David Sachs. Yeah, Chamath Palihapitiya, the German
[01:30:23.480 --> 01:30:27.640]   dictator and your sultan of science who loved dune to he
[01:30:27.640 --> 01:30:32.120]   loved it. He saw it twice over a decade. I'm going to see it
[01:30:32.120 --> 01:30:34.360]   twice. I'm gonna go see it again. Go see it. And I got
[01:30:34.360 --> 01:30:41.160]   the comment boards got so angry. Furious, furious, overrated. I
[01:30:41.160 --> 01:30:44.540]   mean, this set them off like nothing has ever set them off.
[01:30:44.540 --> 01:30:48.520]   I don't know. Try making a comment about Trump. Join my
[01:30:48.520 --> 01:30:56.120]   world. You can't attack him. Timothy Shalame. God. Man, they
[01:30:56.120 --> 01:31:00.280]   were really with his defense. I mean, he does look like he
[01:31:00.280 --> 01:31:03.400]   drinks a lot of soy. I'll be honest. I think that guys only I
[01:31:03.400 --> 01:31:08.640]   don't think that guys ever had whole milk. Not skinny. skinny
[01:31:08.640 --> 01:31:11.080]   people like that when you're super skinny like that you
[01:31:11.080 --> 01:31:15.080]   travel in packs to protect each other as a group. Oh, like you
[01:31:15.080 --> 01:31:17.480]   can get in a group like hyenas or something and then just
[01:31:17.480 --> 01:31:20.040]   protect each other. Yeah, it's like oh my god, we got to stick
[01:31:20.040 --> 01:31:23.120]   up for each other. Because if we don't, you know, Hey, are you
[01:31:23.120 --> 01:31:26.240]   standing sideways? Or are you like, there's like, they're like
[01:31:26.240 --> 01:31:31.040]   a pod, a pod of soybeans, like the Roman turtle protection
[01:31:31.040 --> 01:31:34.560]   thing that they were so made. Yeah, shields up as one as one
[01:31:34.560 --> 01:31:39.520]   so he is now up to 630 million. The box office pretty, pretty
[01:31:39.520 --> 01:31:41.960]   good run, huh? You know, they're gonna do do in three
[01:31:41.960 --> 01:31:45.160]   now. Don't it's definitely happening. He's just negotiating
[01:31:45.160 --> 01:31:50.000]   a big price tag. Yeah, big deal. All right, everybody. You know
[01:31:50.000 --> 01:31:53.400]   what to do. And we'll see you next time on love you. The
[01:31:53.400 --> 01:31:56.160]   world's greatest podcast, the orange pockets. Tell your
[01:31:56.160 --> 01:31:56.600]   friends
[01:31:56.600 --> 01:31:59.520]   let your winners ride
[01:31:59.520 --> 01:32:02.080]   brain man David
[01:32:02.080 --> 01:32:09.000]   we open source it to the fans and they've just gone crazy with
[01:32:09.000 --> 01:32:10.840]   it. Love you as a queen of
[01:32:11.840 --> 01:32:12.280]   going
[01:32:12.280 --> 01:32:19.560]   besties are gone
[01:32:19.560 --> 01:32:23.080]   that is my dog taking a notice in your driveway
[01:32:23.080 --> 01:32:31.440]   we should all just get a room and just have one big huge orgy
[01:32:31.440 --> 01:32:33.320]   because they're all just useless. It's like this like
[01:32:33.320 --> 01:32:35.200]   sexual tension, but they just need to release
[01:32:36.760 --> 01:32:40.440]   what you're about to be
[01:32:40.440 --> 01:32:43.360]   waiting to get murkies
[01:32:44.160 --> 01:32:44.800]   going on
[01:32:44.800 --> 01:32:45.320]   late
[01:32:45.320 --> 01:32:52.760]   I'm going on
[01:32:52.760 --> 01:32:54.920]   ♪ Only you ♪
[01:32:54.920 --> 01:33:04.920]   [BLANK_AUDIO]

