
[00:00:00.000 --> 00:00:01.680]   - In the model of the intelligence explosion,
[00:00:01.680 --> 00:00:04.360]   what happens is you replace the AI researchers
[00:00:04.360 --> 00:00:08.240]   and then there's like a bunch of automated AI researchers
[00:00:08.240 --> 00:00:11.040]   who can speed up progress, make more AI researchers,
[00:00:11.040 --> 00:00:12.280]   make further progress.
[00:00:12.280 --> 00:00:13.920]   We should just ask the AI researchers
[00:00:13.920 --> 00:00:15.920]   about whether they think this is plausible.
[00:00:15.920 --> 00:00:16.960]   So let me just ask you,
[00:00:16.960 --> 00:00:21.040]   like if I have a thousand Asian Shotos or Asian Trentons,
[00:00:21.040 --> 00:00:23.120]   are they just, do you think that you get
[00:00:23.120 --> 00:00:24.360]   an intelligence explosion?
[00:00:24.360 --> 00:00:27.840]   - I think we are less at the moment bound
[00:00:27.840 --> 00:00:31.500]   by the sheer engineering work of making these things
[00:00:31.500 --> 00:00:35.640]   than we are by compute to run and get signal
[00:00:35.640 --> 00:00:40.640]   and taste in terms of what the actual like right thing
[00:00:40.640 --> 00:00:41.560]   to do it.
[00:00:41.560 --> 00:00:42.840]   And that like making those difficult inferences
[00:00:42.840 --> 00:00:44.280]   on imperfect information.
[00:00:44.280 --> 00:00:47.600]   - So compute and taste,
[00:00:47.600 --> 00:00:48.680]   that's interesting to think about
[00:00:48.680 --> 00:00:50.800]   because at least the compute part
[00:00:50.800 --> 00:00:53.200]   is not bottlenecked on more intelligence.
[00:00:53.200 --> 00:00:56.440]   It just bottlenecked on Sam 7 trillion or whatever, right?
[00:00:56.440 --> 00:01:00.840]   So if I gave you 10X the H100s to run your experiments,
[00:01:00.840 --> 00:01:02.120]   how much more effective a researcher are you?
[00:01:02.120 --> 00:01:02.960]   - TPUs please.
[00:01:02.960 --> 00:01:04.480]   (all laughing)
[00:01:04.480 --> 00:01:07.080]   - How much more effective a researcher are you?
[00:01:07.080 --> 00:01:11.200]   - I think the Gemini program would probably
[00:01:11.200 --> 00:01:14.640]   be like maybe five times faster
[00:01:14.640 --> 00:01:16.400]   with 10 times more compute or something like that.
[00:01:16.400 --> 00:01:19.600]   - So that's pretty good elasticity of like 0.5.
[00:01:19.600 --> 00:01:20.720]   Wait, that's insane.
[00:01:20.720 --> 00:01:21.960]   - Yeah, I think like more compute
[00:01:21.960 --> 00:01:24.120]   would just like directly convert into progress.
[00:01:24.120 --> 00:01:28.160]   - So you have some fixed size of compute
[00:01:28.160 --> 00:01:30.640]   and some of it goes to inference,
[00:01:30.640 --> 00:01:32.240]   some of it goes to running the experiments
[00:01:32.240 --> 00:01:33.080]   for the full model.
[00:01:33.080 --> 00:01:33.920]   - Yeah, that's right.
[00:01:33.920 --> 00:01:37.080]   - Shouldn't then the fraction goes to experiments be higher
[00:01:37.080 --> 00:01:38.960]   given that you would just be like,
[00:01:38.960 --> 00:01:40.240]   if like the bottleneck is research
[00:01:40.240 --> 00:01:42.080]   and research is bottlenecked by compute.
[00:01:42.080 --> 00:01:43.920]   - One of the strategic decisions
[00:01:43.920 --> 00:01:45.520]   that every pre-training team has to make
[00:01:45.520 --> 00:01:47.160]   is like exactly what amount of compute
[00:01:47.160 --> 00:01:49.640]   do you allocate to different training runs?
[00:01:49.640 --> 00:01:51.840]   Just like to your research program
[00:01:51.840 --> 00:01:56.840]   versus like scaling the last best thing that you landed on.
[00:01:56.840 --> 00:01:59.000]   One of the reasons why you need
[00:01:59.000 --> 00:02:00.560]   to still keep training big models
[00:02:00.560 --> 00:02:01.880]   is that you get information there
[00:02:01.880 --> 00:02:04.000]   that you don't get otherwise.
[00:02:04.000 --> 00:02:06.920]   So scale has all these emergent properties
[00:02:06.920 --> 00:02:08.840]   which you want to understand better.
[00:02:08.840 --> 00:02:11.400]   And if you like are always doing research,
[00:02:11.400 --> 00:02:15.400]   you're not sure what's gonna like fall off the curve, right?
[00:02:15.400 --> 00:02:17.640]   If you like keep doing research in this regime
[00:02:19.440 --> 00:02:22.400]   and like keep on getting more and more compute efficient,
[00:02:22.400 --> 00:02:25.960]   you may have actually like gone off the path
[00:02:25.960 --> 00:02:27.000]   to actually eventually scale.
[00:02:27.000 --> 00:02:28.680]   So you need to constantly be investing
[00:02:28.680 --> 00:02:31.200]   in doing big runs too,
[00:02:31.200 --> 00:02:34.280]   at the frontier of what you sort of expect to work.
[00:02:34.280 --> 00:02:35.720]   - Okay, so then tell me what it looks like
[00:02:35.720 --> 00:02:36.560]   to be in the world
[00:02:36.560 --> 00:02:39.920]   where AI has significantly sped up AI research.
[00:02:39.920 --> 00:02:41.840]   'Cause from this, it doesn't really sound
[00:02:41.840 --> 00:02:45.240]   like the AIs are going off and writing the code from scratch
[00:02:45.240 --> 00:02:46.680]   and that's leading to faster output.
[00:02:46.680 --> 00:02:48.240]   It sounds like they're really augmenting
[00:02:48.240 --> 00:02:50.000]   the top researchers in some way.
[00:02:50.000 --> 00:02:51.200]   Like, yeah, tell me concretely,
[00:02:51.200 --> 00:02:52.360]   are they doing the experiments?
[00:02:52.360 --> 00:02:53.560]   Are they coming up with the ideas?
[00:02:53.560 --> 00:02:55.560]   Are they just like evaluating the outputs
[00:02:55.560 --> 00:02:56.400]   of the experiments?
[00:02:56.400 --> 00:02:57.240]   What's happening?
[00:02:57.240 --> 00:02:59.080]   - So I think there's like two walls
[00:02:59.080 --> 00:02:59.920]   you need to consider here.
[00:02:59.920 --> 00:03:03.160]   One is where AI has meaningfully sped up our ability
[00:03:03.160 --> 00:03:05.040]   to make algorithmic progress.
[00:03:05.040 --> 00:03:07.600]   And one is where the output of the AI itself
[00:03:07.600 --> 00:03:09.720]   is the thing that's like the crucial ingredient
[00:03:09.720 --> 00:03:13.440]   towards like model capability progress.
[00:03:13.440 --> 00:03:15.400]   And like specifically what I mean there is
[00:03:15.400 --> 00:03:17.480]   - Synthetic data. - Synthetic data, right?
[00:03:18.480 --> 00:03:20.880]   And in the first world
[00:03:20.880 --> 00:03:22.760]   where it's meaningfully speeding up algorithmic progress,
[00:03:22.760 --> 00:03:25.680]   I think a necessary component of that is more compute.
[00:03:25.680 --> 00:03:29.640]   And you probably like reach this elasticity point
[00:03:29.640 --> 00:03:33.480]   where like AIs maybe at some point are easier to speed up
[00:03:33.480 --> 00:03:35.680]   and get on to context than yourself,
[00:03:35.680 --> 00:03:37.920]   let's just say than other people.
[00:03:37.920 --> 00:03:40.400]   And so AIs meaningfully speed up your work
[00:03:40.400 --> 00:03:43.080]   because they're like a fantastic copilot,
[00:03:43.080 --> 00:03:46.120]   basically that helps you code multiple times faster.
[00:03:46.120 --> 00:03:48.440]   And that seems like actually quite reasonable,
[00:03:48.440 --> 00:03:51.840]   super long context, super smart model,
[00:03:51.840 --> 00:03:55.000]   it's onboarded immediately and you can like send them off
[00:03:55.000 --> 00:03:58.280]   and to like complete sub tasks and sub goals for you.
[00:03:58.280 --> 00:04:01.720]   - Walk me through like a day in the life of show,
[00:04:01.720 --> 00:04:04.080]   like you're working on an experiment or project
[00:04:04.080 --> 00:04:06.320]   that's going to make the model quote unquote better.
[00:04:06.320 --> 00:04:07.160]   - Right.
[00:04:07.160 --> 00:04:09.440]   - Like what is happening from observation
[00:04:09.440 --> 00:04:12.120]   to experiment to theory to like writing the code,
[00:04:12.120 --> 00:04:13.120]   what is happening?
[00:04:13.120 --> 00:04:15.960]   - I think the most important like part to illustrate
[00:04:15.960 --> 00:04:18.840]   is this cycle of coming up with an idea,
[00:04:18.840 --> 00:04:20.960]   proving it out at different points in scale
[00:04:20.960 --> 00:04:27.280]   and like interpreting and understanding what goes wrong.
[00:04:27.280 --> 00:04:29.160]   And I think most people would be surprised
[00:04:29.160 --> 00:04:31.240]   to learn just how much goes into interpret,
[00:04:31.240 --> 00:04:33.960]   like interpreting and understanding what goes wrong.
[00:04:33.960 --> 00:04:37.080]   'Cause the ideas, people have long lists of ideas
[00:04:37.080 --> 00:04:37.920]   that they wanna try,
[00:04:37.920 --> 00:04:41.000]   not every idea that you think should work will work
[00:04:41.000 --> 00:04:42.840]   and trying to understand why that is quite difficult
[00:04:42.840 --> 00:04:44.680]   and like working out what exactly you need to do
[00:04:44.680 --> 00:04:45.960]   to interrogate it.
[00:04:45.960 --> 00:04:47.880]   So, so much of it is like introspection
[00:04:47.880 --> 00:04:48.720]   about what's going on.
[00:04:48.720 --> 00:04:50.760]   It's not pumping out thousands and thousands
[00:04:50.760 --> 00:04:51.920]   and thousands of line of code.
[00:04:51.920 --> 00:04:56.720]   It's not like the difficulty in coming up with ideas even.
[00:04:56.720 --> 00:04:58.480]   I think many people have a long list of ideas
[00:04:58.480 --> 00:04:59.640]   that they wanna try,
[00:04:59.640 --> 00:05:01.880]   but paring that down and shock calling
[00:05:01.880 --> 00:05:04.000]   under very imperfect information,
[00:05:04.000 --> 00:05:08.360]   what the right ideas to explore further is really hard.
[00:05:08.360 --> 00:05:09.800]   - Tell me more about,
[00:05:09.800 --> 00:05:11.680]   what do you mean by imperfect information?
[00:05:11.680 --> 00:05:13.240]   Are these early experiments?
[00:05:13.240 --> 00:05:15.800]   Are these, like what is the information that you're-
[00:05:15.800 --> 00:05:16.960]   - Like scaling more increments.
[00:05:16.960 --> 00:05:18.280]   And you can see like in the GPT-4 paper,
[00:05:18.280 --> 00:05:19.960]   they have like a bunch of like dots, right?
[00:05:19.960 --> 00:05:21.640]   Where they say we can estimate the performance
[00:05:21.640 --> 00:05:22.600]   of our final model,
[00:05:22.600 --> 00:05:23.880]   like using all of these dots.
[00:05:23.880 --> 00:05:25.800]   And there's a nice curve that like flows through them.
[00:05:25.800 --> 00:05:28.440]   Concretely, why is that imperfect information?
[00:05:28.440 --> 00:05:32.280]   Is you never actually know if the trend will hold.
[00:05:32.280 --> 00:05:33.200]   For certain architectures,
[00:05:33.200 --> 00:05:35.240]   the trend has held really well.
[00:05:35.240 --> 00:05:37.760]   And for certain changes, it's held really well.
[00:05:37.760 --> 00:05:39.680]   But that isn't always the case.
[00:05:39.680 --> 00:05:41.920]   And things which can help at smaller scales
[00:05:41.920 --> 00:05:43.840]   can actually hurt at larger scales.
[00:05:43.840 --> 00:05:50.120]   So making guesses based on what the trend lines look like,
[00:05:50.120 --> 00:05:53.560]   and based on like your intuitive feeling of,
[00:05:53.560 --> 00:05:55.720]   okay, this is actually something that's gonna matter.
[00:05:55.720 --> 00:05:58.160]   - That's interesting to consider that for every chart
[00:05:58.160 --> 00:06:00.640]   you see in a release paper or technical report
[00:06:00.640 --> 00:06:01.960]   that shows that smooth curve,
[00:06:01.960 --> 00:06:04.920]   there's a graveyard of like first neurons
[00:06:04.920 --> 00:06:05.760]   and then it's like flat.
[00:06:05.760 --> 00:06:07.040]   - Yeah, yeah, there's all these like other lines
[00:06:07.040 --> 00:06:08.240]   that go in like different directions,
[00:06:08.240 --> 00:06:10.960]   and you just like tail off and like, that's-
[00:06:10.960 --> 00:06:11.800]   - Yeah, it's crazy.
[00:06:11.800 --> 00:06:13.720]   Both like as a grad student and then also here,
[00:06:13.720 --> 00:06:16.160]   like the number of experiments that you have to run.
[00:06:16.160 --> 00:06:17.480]   - And then intuiting what went wrong
[00:06:17.480 --> 00:06:18.600]   is actually really hard.
[00:06:18.600 --> 00:06:21.560]   Like working out what, like this is in many respects
[00:06:21.560 --> 00:06:23.360]   from the team that Trenton is on
[00:06:23.360 --> 00:06:24.720]   is trying to better understand
[00:06:24.720 --> 00:06:26.800]   is like what is going on inside these models.
[00:06:26.800 --> 00:06:28.720]   We have inferences and understanding
[00:06:28.720 --> 00:06:31.000]   and like head canon for why certain things work,
[00:06:31.000 --> 00:06:33.200]   but it's not an exact science.
[00:06:33.200 --> 00:06:35.240]   And so you have to constantly be making guesses
[00:06:35.240 --> 00:06:36.840]   about why something might've happened,
[00:06:36.840 --> 00:06:39.680]   what experiment might reveal whether that is or isn't true.
[00:06:39.680 --> 00:06:42.200]   And that's probably the most complex part.
[00:06:42.200 --> 00:06:45.600]   - Yeah, I agree with a lot of that.
[00:06:45.600 --> 00:06:47.240]   But even on the interpretability team,
[00:06:47.240 --> 00:06:49.560]   I mean, especially with Chris Ola leading it,
[00:06:49.560 --> 00:06:52.520]   there are just so many ideas that we wanna test.
[00:06:52.520 --> 00:06:56.560]   And it's really just having the engineering skill,
[00:06:56.560 --> 00:06:57.960]   but I'll put engineering in quotes
[00:06:57.960 --> 00:06:59.640]   because a lot of it is research,
[00:06:59.640 --> 00:07:03.560]   to like very quickly iterate on an experiment,
[00:07:03.560 --> 00:07:05.280]   look at the results, interpret it,
[00:07:05.280 --> 00:07:07.200]   try the next thing, communicate them.
[00:07:07.800 --> 00:07:09.600]   And then just ruthlessly prioritizing
[00:07:09.600 --> 00:07:11.640]   what the highest priority things to do are.
[00:07:11.640 --> 00:07:12.720]   - And this is really important.
[00:07:12.720 --> 00:07:14.560]   Like the ruthless prioritization is something
[00:07:14.560 --> 00:07:17.920]   which I think separates a lot of like quality research
[00:07:17.920 --> 00:07:21.000]   from research that doesn't necessarily succeed as much.
[00:07:21.000 --> 00:07:23.040]   We're in this funny field
[00:07:23.040 --> 00:07:27.640]   where so many of our initial theoretical understanding
[00:07:27.640 --> 00:07:29.800]   is like broken down basically.
[00:07:29.800 --> 00:07:31.880]   And so you need to have this simplicity bias
[00:07:31.880 --> 00:07:33.280]   and like ruthless prioritization
[00:07:33.280 --> 00:07:34.400]   over what's actually going wrong.
[00:07:34.400 --> 00:07:35.640]   And I think that's one of the things
[00:07:35.640 --> 00:07:37.040]   that separates the most effective people
[00:07:37.040 --> 00:07:39.760]   is they don't necessarily get like too attached
[00:07:39.760 --> 00:07:44.080]   to using a given solution
[00:07:44.080 --> 00:07:46.200]   that they're necessarily familiar with,
[00:07:46.200 --> 00:07:49.160]   but rather they attack the problem directly.
[00:07:49.160 --> 00:07:51.800]   You see this a lot in like,
[00:07:51.800 --> 00:07:54.000]   maybe people come in with a specific academic background,
[00:07:54.000 --> 00:07:56.720]   they try and solve problems with that toolbox.
[00:07:56.720 --> 00:07:58.280]   And the best people are people
[00:07:58.280 --> 00:08:00.560]   who expand the toolbox dramatically.
[00:08:00.560 --> 00:08:02.920]   They're running around and they're taking ideas
[00:08:02.920 --> 00:08:04.360]   from reinforcement learning,
[00:08:04.360 --> 00:08:05.640]   but also from optimization theory.
[00:08:05.640 --> 00:08:07.480]   And also they have a great understanding of systems.
[00:08:07.480 --> 00:08:09.080]   And so they know what the sort of constraints
[00:08:09.080 --> 00:08:10.320]   that bound the problem are.
[00:08:10.320 --> 00:08:11.320]   And they're good engineers,
[00:08:11.320 --> 00:08:12.720]   they can iterate and try ideas fast.
[00:08:12.720 --> 00:08:15.720]   Like by far the best researchers I've seen,
[00:08:15.720 --> 00:08:17.400]   they all have the ability to try experiments
[00:08:17.400 --> 00:08:19.760]   really, really, really, really fast.
[00:08:19.760 --> 00:08:22.520]   And that is that cycle time at smaller scales,
[00:08:22.520 --> 00:08:24.560]   cycle time separates people.
[00:08:24.560 --> 00:08:27.640]   - I mean, machine learning research is just so empirical.
[00:08:27.640 --> 00:08:28.480]   - Yeah.
[00:08:28.480 --> 00:08:31.360]   - And this is honestly one reason why I think
[00:08:31.360 --> 00:08:33.760]   our solutions might end up looking more brain-like
[00:08:33.760 --> 00:08:35.280]   than otherwise.
[00:08:35.280 --> 00:08:37.680]   It's like, even though we wouldn't want to admit it,
[00:08:37.680 --> 00:08:40.720]   the whole community is kind of doing like,
[00:08:40.720 --> 00:08:44.200]   greedy evolutionary optimization over the landscape
[00:08:44.200 --> 00:08:47.160]   of like possible architectures and everything else.
[00:08:47.160 --> 00:08:48.920]   It's like no better than evolution.

