
[00:00:00.000 --> 00:00:03.120]   48 years ago, two venerable scribes,
[00:00:03.120 --> 00:00:05.500]   by the names of Jagger and Richards,
[00:00:05.500 --> 00:00:09.840]   wrote, "You can't always get what you want,
[00:00:09.840 --> 00:00:11.480]   "but you get what you need."
[00:00:11.480 --> 00:00:13.880]   But turns out, Mick and Keith actually got this
[00:00:13.880 --> 00:00:16.160]   exactly backwards.
[00:00:16.160 --> 00:00:18.240]   In today's market-driven economy,
[00:00:18.240 --> 00:00:20.280]   you can't always get what you need,
[00:00:20.280 --> 00:00:22.840]   but you always get what you want.
[00:00:22.840 --> 00:00:26.200]   So we need better healthcare, clean water,
[00:00:26.200 --> 00:00:29.480]   food security for all, climate change mitigation,
[00:00:29.480 --> 00:00:32.680]   social equality, but what do we get
[00:00:32.680 --> 00:00:36.240]   from the best and brightest of today's technologists?
[00:00:36.240 --> 00:00:41.240]   Angry birds, plants versus zombies, cat videos,
[00:00:41.240 --> 00:00:44.360]   photos of your food that will self-destruct
[00:00:44.360 --> 00:00:45.340]   in five seconds.
[00:00:45.340 --> 00:00:48.040]   And why do we get these?
[00:00:48.040 --> 00:00:50.440]   Because these are the things we want.
[00:00:50.440 --> 00:00:52.080]   Of course, the breakdown is,
[00:00:52.080 --> 00:00:54.560]   these aren't the things that we really want.
[00:00:54.560 --> 00:00:56.840]   So one of our challenges for the future
[00:00:56.840 --> 00:01:00.040]   is to be able to better describe to our markets
[00:01:00.040 --> 00:01:01.480]   and to our high-tech products,
[00:01:01.480 --> 00:01:03.640]   what is it that we really want?
[00:01:03.640 --> 00:01:05.760]   Now, in economics and game theory
[00:01:05.760 --> 00:01:07.300]   and artificial intelligence,
[00:01:07.300 --> 00:01:11.120]   there's a common goal of maximizing expected utility.
[00:01:11.120 --> 00:01:14.240]   And we've spent decades on the expected part.
[00:01:14.240 --> 00:01:16.520]   That's statistics, it's probability theory,
[00:01:16.520 --> 00:01:18.080]   it's machine learning.
[00:01:18.080 --> 00:01:20.880]   And we spend more decades on the maximizing part.
[00:01:20.880 --> 00:01:22.360]   That's the theory of algorithms
[00:01:22.360 --> 00:01:24.360]   as it applies to all these fields.
[00:01:24.360 --> 00:01:28.040]   But mostly, we leave the utility part unstated.
[00:01:28.040 --> 00:01:29.680]   We just take that as a given,
[00:01:29.680 --> 00:01:31.920]   and saying that's what it is that we're trying to maximize,
[00:01:31.920 --> 00:01:33.800]   but we never question what it is.
[00:01:33.800 --> 00:01:35.200]   We haven't developed the tools
[00:01:35.200 --> 00:01:38.320]   to let the public better describe what they want.
[00:01:38.320 --> 00:01:41.320]   So don't just think about data science,
[00:01:41.320 --> 00:01:44.120]   information visualization, statistical inference,
[00:01:44.120 --> 00:01:45.720]   probabilistic reasoning.
[00:01:45.720 --> 00:01:48.400]   Think also about utility science,
[00:01:48.400 --> 00:01:52.080]   desire visualization, and ethical calculus.
[00:01:52.080 --> 00:01:55.480]   Build systems that protect the desires of the few,
[00:01:55.480 --> 00:01:57.160]   as well as the many.
[00:01:57.160 --> 00:01:58.760]   Design algorithms with payoffs
[00:01:58.760 --> 00:02:01.080]   that ensure a sustainable future,
[00:02:01.080 --> 00:02:03.440]   as well as a near-term return on investment.
[00:02:03.440 --> 00:02:06.160]   We have the power to do many things,
[00:02:06.160 --> 00:02:08.040]   but until the public has the power
[00:02:08.040 --> 00:02:10.520]   to better say what it is it really wants,
[00:02:10.520 --> 00:02:13.400]   the market will always choose poorly.
[00:02:13.560 --> 00:02:16.160]   (upbeat music)
[00:02:16.160 --> 00:02:18.760]   (upbeat music)
[00:02:18.760 --> 00:02:21.360]   (upbeat music)
[00:02:21.360 --> 00:02:23.960]   (upbeat music)
[00:02:23.960 --> 00:02:33.960]   [BLANK_AUDIO]

