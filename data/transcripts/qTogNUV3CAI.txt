
[00:00:00.000 --> 00:00:03.200]   So I wouldn't be surprised if we had HEI-like systems
[00:00:03.200 --> 00:00:04.400]   within the next decade.
[00:00:04.400 --> 00:00:06.400]   It was pretty surprising to almost everyone,
[00:00:06.400 --> 00:00:08.440]   including the people who first worked
[00:00:08.440 --> 00:00:10.900]   on the scaling hypotheses, that how far it's gone.
[00:00:10.900 --> 00:00:13.440]   In a way, I look at the large models today
[00:00:13.440 --> 00:00:15.440]   and I think they're almost unreasonably effective
[00:00:15.440 --> 00:00:16.280]   for what they are.
[00:00:16.280 --> 00:00:17.820]   It's an empirical question whether that will hit
[00:00:17.820 --> 00:00:19.800]   an asymptote or a brick wall.
[00:00:19.800 --> 00:00:21.160]   I think no one knows.
[00:00:21.160 --> 00:00:23.040]   - When you think about superhuman intelligence,
[00:00:23.040 --> 00:00:25.720]   is it like still controlled by a private company?
[00:00:25.720 --> 00:00:28.080]   - As Gemini becoming more multimodal
[00:00:28.080 --> 00:00:30.280]   and we start ingesting audio-visual data
[00:00:30.280 --> 00:00:32.880]   as well as text data, I do think our systems
[00:00:32.880 --> 00:00:35.640]   are going to start to understand the physics
[00:00:35.640 --> 00:00:36.960]   of the real world better.
[00:00:36.960 --> 00:00:38.600]   The world's about to become very exciting,
[00:00:38.600 --> 00:00:39.740]   I think, in the next few years,
[00:00:39.740 --> 00:00:41.280]   as we start getting used to the idea
[00:00:41.280 --> 00:00:44.320]   of what true multimodality means.
[00:00:44.320 --> 00:00:46.880]   - Okay, today it is a true honor to speak
[00:00:46.880 --> 00:00:50.320]   with Demis Hassavas, who is the CEO of DeepMind.
[00:00:50.320 --> 00:00:51.720]   Demis, welcome to the podcast.
[00:00:51.720 --> 00:00:52.760]   - Thanks for having me.
[00:00:52.760 --> 00:00:55.520]   - First question, given your neuroscience background,
[00:00:55.520 --> 00:00:56.920]   how do you think about intelligence?
[00:00:56.920 --> 00:00:59.680]   Specifically, do you think it's like one higher level
[00:00:59.680 --> 00:01:02.280]   general reasoning circuit, or do you think it's thousands
[00:01:02.280 --> 00:01:05.600]   of independent subscales and heuristics?
[00:01:05.600 --> 00:01:09.880]   - Well, it's interesting because intelligence is so broad
[00:01:09.880 --> 00:01:14.760]   and what we use it for is so sort of generally applicable.
[00:01:14.760 --> 00:01:17.280]   I think that suggests that there must be
[00:01:17.280 --> 00:01:21.880]   some sort of high-level common things,
[00:01:21.880 --> 00:01:24.160]   common kind of algorithmic themes, I think,
[00:01:24.160 --> 00:01:27.280]   around how the brain processes the world around us.
[00:01:27.280 --> 00:01:31.680]   So, of course, then there are specialized parts of the brain
[00:01:31.680 --> 00:01:35.200]   that do specific things, but I think there are probably
[00:01:35.200 --> 00:01:37.800]   some underlying principles that underpin all of that.
[00:01:37.800 --> 00:01:39.960]   - Yeah, how do you make sense of the fact that,
[00:01:39.960 --> 00:01:42.720]   in these LLMs, though, when you give them a lot of data
[00:01:42.720 --> 00:01:45.580]   in any specific domain, they tend to get asymmetrically
[00:01:45.580 --> 00:01:46.760]   better in that domain?
[00:01:46.760 --> 00:01:50.040]   Wouldn't we expect a sort of general improvement
[00:01:50.040 --> 00:01:51.440]   across all the different areas?
[00:01:51.440 --> 00:01:53.600]   - Well, I think you, first of all, I think you do actually
[00:01:53.600 --> 00:01:56.440]   sometimes get surprising improvement in other domains
[00:01:56.440 --> 00:01:58.040]   when you improve in a specific domain.
[00:01:58.040 --> 00:02:01.120]   So, for example, when these large models
[00:02:01.120 --> 00:02:03.940]   sort of improve at coding, that can actually improve
[00:02:03.940 --> 00:02:05.240]   their general reasoning.
[00:02:05.240 --> 00:02:07.440]   So there is some evidence of some transfer,
[00:02:07.440 --> 00:02:10.560]   although I think we would like a lot more evidence of that.
[00:02:10.560 --> 00:02:14.280]   But also, that's how the human brain learns, too,
[00:02:14.280 --> 00:02:17.440]   is if we experience and practice a lot of things like chess
[00:02:17.440 --> 00:02:20.080]   or writing, creative writing, or whatever that is,
[00:02:20.080 --> 00:02:22.160]   we also tend to specialize and get better
[00:02:22.160 --> 00:02:25.120]   at that specific thing, even though we're using
[00:02:25.120 --> 00:02:26.600]   sort of general learning techniques
[00:02:26.600 --> 00:02:30.280]   and general learning systems in order to get good
[00:02:30.280 --> 00:02:31.120]   at that domain.
[00:02:31.120 --> 00:02:33.200]   - Yeah, well, what's been the most surprising example
[00:02:33.200 --> 00:02:34.960]   of this kind of transfer for you?
[00:02:34.960 --> 00:02:37.840]   Like, when you see language in code or images in text?
[00:02:37.840 --> 00:02:40.960]   - Yeah, I think probably, I mean, I'm hoping we're gonna see
[00:02:40.960 --> 00:02:44.340]   a lot more of this China transfer, but I think things
[00:02:44.340 --> 00:02:47.160]   like getting better at coding and math then generally
[00:02:47.160 --> 00:02:50.280]   improving your reasoning, that is how it works with us
[00:02:50.600 --> 00:02:52.720]   as human learners, but I think it's interesting
[00:02:52.720 --> 00:02:55.880]   seeing that in these artificial systems.
[00:02:55.880 --> 00:02:59.080]   - And can you see the sort of mechanistic way in which,
[00:02:59.080 --> 00:03:01.240]   let's say in the language and code example, there's like,
[00:03:01.240 --> 00:03:03.480]   I found the place in a neural network that's getting better
[00:03:03.480 --> 00:03:04.560]   with both the language and the code,
[00:03:04.560 --> 00:03:07.000]   or is it that too far down the wheat?
[00:03:07.000 --> 00:03:09.800]   - Yeah, well, I don't think our analysis techniques
[00:03:09.800 --> 00:03:13.440]   are quite sophisticated enough to be able to hone in on that.
[00:03:13.440 --> 00:03:16.420]   I think that's actually one of the areas that a lot more
[00:03:16.420 --> 00:03:19.440]   research needs to be done on kind of mechanistic analysis
[00:03:19.440 --> 00:03:21.880]   of the representations that these systems build up.
[00:03:21.880 --> 00:03:25.320]   And I sometimes like to call it virtual brain analytics.
[00:03:25.320 --> 00:03:29.320]   In a way, it's a bit like doing fMRI or single cell
[00:03:29.320 --> 00:03:31.900]   recording from a real brain.
[00:03:31.900 --> 00:03:34.600]   What's the analogous sort of analysis techniques
[00:03:34.600 --> 00:03:36.320]   for these artificial minds?
[00:03:36.320 --> 00:03:38.520]   And there's a lot of great work going on
[00:03:38.520 --> 00:03:39.360]   on this sort of stuff.
[00:03:39.360 --> 00:03:41.800]   People like Chris Ola, I really like his work,
[00:03:41.800 --> 00:03:44.000]   and a lot of computational neuroscience techniques
[00:03:44.000 --> 00:03:47.400]   I think could be brought to bear on analyzing
[00:03:47.400 --> 00:03:48.520]   these current systems we're building.
[00:03:48.520 --> 00:03:50.800]   In fact, I try to encourage a lot of my computational
[00:03:50.800 --> 00:03:53.960]   neuroscience friends to start thinking in that direction
[00:03:53.960 --> 00:03:58.400]   and applying their know-how to the large models.
[00:03:58.400 --> 00:04:01.640]   - Yeah, what do other AI researchers not understand
[00:04:01.640 --> 00:04:04.840]   about human intelligence that you have some sort of
[00:04:04.840 --> 00:04:06.640]   insight on given your neuroscience background?
[00:04:06.640 --> 00:04:10.560]   - I think neuroscience has added a lot.
[00:04:10.560 --> 00:04:13.160]   If you look at the last sort of 10, 20 years
[00:04:13.160 --> 00:04:14.420]   that we've been at it at least,
[00:04:14.420 --> 00:04:18.000]   and I've been thinking about this for 30 plus years,
[00:04:18.000 --> 00:04:21.720]   I think in the earlier days of this sort of new wave of AI,
[00:04:21.720 --> 00:04:24.720]   I think neuroscience was providing a lot of interesting
[00:04:24.720 --> 00:04:26.120]   directional clues.
[00:04:26.120 --> 00:04:28.000]   So things like reinforcement learning,
[00:04:28.000 --> 00:04:29.600]   combining that with deep learning,
[00:04:29.600 --> 00:04:31.560]   some of our pioneering work we did there,
[00:04:31.560 --> 00:04:33.800]   things like experience replay,
[00:04:33.800 --> 00:04:35.400]   even the notion of attention,
[00:04:35.400 --> 00:04:37.800]   which has become super important.
[00:04:37.800 --> 00:04:41.200]   A lot of those original sort of inspirations
[00:04:41.200 --> 00:04:44.040]   come from some understanding about how the brain works.
[00:04:44.040 --> 00:04:46.160]   Not the exact specifics, of course,
[00:04:46.160 --> 00:04:48.320]   one's an engineered system, the other one's a natural system.
[00:04:48.320 --> 00:04:50.800]   So it's not so much about a one-to-one mapping
[00:04:50.800 --> 00:04:52.200]   of a specific algorithm,
[00:04:52.200 --> 00:04:54.440]   it's more kind of inspirational direction,
[00:04:54.440 --> 00:04:57.200]   maybe some ideas for architecture or algorithmic ideas
[00:04:57.200 --> 00:04:58.800]   or representational ideas.
[00:04:58.800 --> 00:05:01.960]   And because you know the brain's an existence proof
[00:05:01.960 --> 00:05:04.160]   that general intelligence is possible at all,
[00:05:04.160 --> 00:05:07.960]   I think the history of human endeavors has been that
[00:05:07.960 --> 00:05:09.600]   once you know something's possible,
[00:05:09.600 --> 00:05:11.720]   it's easier to push hard in that direction
[00:05:11.720 --> 00:05:13.800]   because you know it's a question of effort then
[00:05:13.800 --> 00:05:16.600]   and sort of a question of when, not if.
[00:05:16.600 --> 00:05:18.560]   And that allows you to, you know,
[00:05:18.560 --> 00:05:20.440]   I think make progress a lot more quickly.
[00:05:20.440 --> 00:05:23.200]   So I think neurosciences has had a lot of,
[00:05:23.200 --> 00:05:26.440]   has inspired a lot of the thinking,
[00:05:26.440 --> 00:05:30.040]   at least in a soft way behind where we are today.
[00:05:30.040 --> 00:05:33.360]   But as for, you know, going forwards,
[00:05:33.360 --> 00:05:36.880]   I think that there's still a lot of interesting things
[00:05:36.880 --> 00:05:38.720]   to be resolved around planning
[00:05:38.720 --> 00:05:42.520]   and how does the brain construct the right world models.
[00:05:43.360 --> 00:05:45.280]   You know, I studied, for example,
[00:05:45.280 --> 00:05:47.040]   how the brain does imagination
[00:05:47.040 --> 00:05:50.000]   or you can think of it as a mental simulation.
[00:05:50.000 --> 00:05:51.680]   So how do we create, you know,
[00:05:51.680 --> 00:05:54.600]   very rich visual spatial simulations of the world
[00:05:54.600 --> 00:05:56.080]   in order for us to plan better?
[00:05:56.080 --> 00:05:57.480]   - Yeah, actually I'm curious how you think
[00:05:57.480 --> 00:05:59.280]   that will sort of interface with LLM.
[00:05:59.280 --> 00:06:01.440]   So obviously DeepMind is at the frontier
[00:06:01.440 --> 00:06:02.960]   and has been for many years,
[00:06:02.960 --> 00:06:05.080]   you know, with systems like AlphaZero and so forth,
[00:06:05.080 --> 00:06:06.960]   of having these agents who can like think through
[00:06:06.960 --> 00:06:08.960]   different steps to get to an end outcome.
[00:06:08.960 --> 00:06:11.760]   Will this just be, is a path for LLMs
[00:06:11.760 --> 00:06:13.560]   to have this sort of tree search
[00:06:13.560 --> 00:06:14.720]   kind of thing on top of them?
[00:06:14.720 --> 00:06:15.720]   How do you think about this?
[00:06:15.720 --> 00:06:18.840]   - I think that's a super promising direction in my opinion.
[00:06:18.840 --> 00:06:21.880]   So, you know, we've got to carry on improving
[00:06:21.880 --> 00:06:24.040]   the large models and we've got to carry on
[00:06:24.040 --> 00:06:26.720]   basically making them more and more accurate
[00:06:26.720 --> 00:06:28.000]   predictors of the world.
[00:06:28.000 --> 00:06:29.800]   So in effect, making them more and more
[00:06:29.800 --> 00:06:32.560]   reliable world models, that's clearly a necessary,
[00:06:32.560 --> 00:06:34.720]   but I would say probably not sufficient component
[00:06:34.720 --> 00:06:36.480]   of an AGI system.
[00:06:36.480 --> 00:06:38.720]   And then on top of that, I would, you know,
[00:06:38.720 --> 00:06:41.120]   we're working on things like AlphaZero,
[00:06:41.120 --> 00:06:43.080]   like planning mechanisms on top
[00:06:43.080 --> 00:06:44.680]   that make use of that model
[00:06:44.680 --> 00:06:46.560]   in order to make concrete plans,
[00:06:46.560 --> 00:06:48.840]   to achieve certain goals in the world,
[00:06:48.840 --> 00:06:51.760]   and perhaps sort of chain, you know,
[00:06:51.760 --> 00:06:54.360]   chain thought together or lines of reasoning together
[00:06:54.360 --> 00:06:56.720]   and maybe use search to kind of explore
[00:06:56.720 --> 00:06:58.400]   massive spaces of possibility.
[00:06:58.400 --> 00:06:59.840]   I think that's kind of missing
[00:06:59.840 --> 00:07:02.000]   from our current large models.
[00:07:02.000 --> 00:07:03.600]   - How do you get past the sort of
[00:07:03.600 --> 00:07:06.480]   immense amount of compute that these approaches
[00:07:06.480 --> 00:07:07.320]   tend to require?
[00:07:07.320 --> 00:07:10.040]   So even the AlphaGo system was, you know,
[00:07:10.040 --> 00:07:11.440]   a pretty expensive system
[00:07:11.440 --> 00:07:12.680]   'cause you had to do this sort of
[00:07:12.680 --> 00:07:16.040]   run an LLM on each node of the tree.
[00:07:16.040 --> 00:07:18.240]   How do you anticipate that'll get made more efficient?
[00:07:18.240 --> 00:07:19.840]   - Well, I mean, one thing is Moore's law
[00:07:19.840 --> 00:07:23.680]   tends to help if, you know,
[00:07:23.680 --> 00:07:27.560]   over every year, of course, more computation comes in,
[00:07:27.560 --> 00:07:30.000]   but we focus a lot on efficient, you know,
[00:07:30.000 --> 00:07:34.400]   sample efficient methods and reusing existing data,
[00:07:34.400 --> 00:07:36.160]   things like experience replay,
[00:07:36.160 --> 00:07:39.040]   and also just looking at more efficient ways.
[00:07:39.040 --> 00:07:40.880]   I mean, the better your world model is,
[00:07:40.880 --> 00:07:42.600]   the more efficient your search can be.
[00:07:42.600 --> 00:07:44.520]   So one example I always give with AlphaZero,
[00:07:44.520 --> 00:07:47.560]   our system to play Go and Chess and, you know, any game,
[00:07:47.560 --> 00:07:51.280]   is that it's stronger than world champion level,
[00:07:51.280 --> 00:07:53.840]   human world champion level at all these games.
[00:07:53.840 --> 00:07:57.960]   And it uses a lot less search than a brute force method,
[00:07:57.960 --> 00:07:59.880]   like Deep Blue, say, to play Chess.
[00:07:59.880 --> 00:08:02.360]   Deep Blue, one of these traditional Stockfish
[00:08:02.360 --> 00:08:05.280]   or Deep Blue systems would maybe look at millions
[00:08:05.280 --> 00:08:08.680]   of possible moves for every decision it's gonna make.
[00:08:08.680 --> 00:08:11.600]   AlphaZero and AlphaGo made, you know,
[00:08:11.600 --> 00:08:15.800]   looked at around tens of thousands of possible positions
[00:08:15.800 --> 00:08:18.200]   in order to make a decision about what to move next.
[00:08:18.200 --> 00:08:21.440]   But a human grandmaster, a human world champion,
[00:08:21.440 --> 00:08:23.920]   probably only looks at a few hundreds of moves,
[00:08:23.920 --> 00:08:27.960]   even the top ones, in order to make their very good decision
[00:08:27.960 --> 00:08:29.240]   about what to play next.
[00:08:29.240 --> 00:08:32.480]   So that suggests that obviously the brute force systems
[00:08:32.480 --> 00:08:33.880]   don't have any real model
[00:08:33.880 --> 00:08:35.960]   other than the heuristics about the game.
[00:08:35.960 --> 00:08:39.680]   AlphaZero has quite a decent model,
[00:08:39.680 --> 00:08:44.000]   but the top human players have a much richer,
[00:08:44.000 --> 00:08:46.840]   much more accurate model then of Go or Chess.
[00:08:46.840 --> 00:08:48.480]   So that allows them to make, you know,
[00:08:48.480 --> 00:08:51.760]   world class decisions on a very small amount of search.
[00:08:51.760 --> 00:08:54.160]   So I think there's still, there's a sort of trade off there.
[00:08:54.160 --> 00:08:56.000]   Like, you know, if you improve the models,
[00:08:56.000 --> 00:08:58.400]   then I think your search can be more efficient
[00:08:58.400 --> 00:09:00.120]   and therefore you can get further with your search.
[00:09:00.120 --> 00:09:02.480]   - Yeah, I have two questions based on that.
[00:09:02.480 --> 00:09:04.160]   The first being with AlphaGo,
[00:09:04.160 --> 00:09:06.840]   you had a very concrete win condition of, you know,
[00:09:06.840 --> 00:09:08.560]   at the end of the day, do I win this game or not?
[00:09:08.560 --> 00:09:10.440]   And you can reinforce on that.
[00:09:10.440 --> 00:09:13.480]   When you're just thinking of like an LLM putting out thought,
[00:09:13.480 --> 00:09:15.760]   what will, do you think there'll be this kind of ability
[00:09:15.760 --> 00:09:17.400]   to discriminate in the end,
[00:09:17.400 --> 00:09:19.960]   whether that was like a good thing to reward or not?
[00:09:19.960 --> 00:09:21.480]   - Well, of course, that's why we, you know,
[00:09:21.480 --> 00:09:23.720]   we pioneered and DeepMind's sort of famous
[00:09:23.720 --> 00:09:26.880]   for using games as a proving ground,
[00:09:26.880 --> 00:09:28.600]   partly because obviously it's efficient
[00:09:28.600 --> 00:09:30.080]   to research in that domain.
[00:09:30.080 --> 00:09:31.960]   But the other reason is obviously it's, you know,
[00:09:31.960 --> 00:09:34.040]   extremely easy to specify a reward function
[00:09:34.040 --> 00:09:35.680]   winning the game or improving the score
[00:09:35.680 --> 00:09:38.080]   or something like that sort of built into most games.
[00:09:38.080 --> 00:09:39.680]   So that is the, that is the,
[00:09:39.680 --> 00:09:41.920]   that one of the challenges of real world systems
[00:09:41.920 --> 00:09:44.840]   is how does one define the right objective function,
[00:09:44.840 --> 00:09:47.520]   the right reward function and the right goals
[00:09:47.520 --> 00:09:51.440]   and specify them in a, in, you know, in a general way,
[00:09:51.440 --> 00:09:52.640]   but they're specific enough
[00:09:52.640 --> 00:09:55.640]   and actually points the system in the right direction.
[00:09:55.640 --> 00:09:58.920]   And for real world problems, that can be a lot harder,
[00:09:58.920 --> 00:10:01.160]   but actually, if you think about it
[00:10:01.160 --> 00:10:03.360]   in even scientific problems,
[00:10:03.360 --> 00:10:05.480]   there are usually ways that you can specify
[00:10:05.480 --> 00:10:07.160]   the goal that you're after.
[00:10:07.160 --> 00:10:08.600]   - And then when you think about human intelligence,
[00:10:08.600 --> 00:10:09.800]   you were just saying, well, you know,
[00:10:09.800 --> 00:10:10.880]   the humans thinking about these thoughts
[00:10:10.880 --> 00:10:12.480]   are just super sample efficient.
[00:10:12.480 --> 00:10:15.080]   I understand coming up with relativity, right?
[00:10:15.080 --> 00:10:16.800]   There's just like thousands of possible permutations
[00:10:16.800 --> 00:10:17.880]   of the equations.
[00:10:17.880 --> 00:10:19.600]   Do you think it's also this sort of sense
[00:10:19.600 --> 00:10:20.920]   of like different heuristics of like,
[00:10:20.920 --> 00:10:22.280]   I'm gonna try this approach instead of this,
[00:10:22.280 --> 00:10:25.200]   or is it a totally different way of approaching
[00:10:25.200 --> 00:10:26.960]   coming up with that solution
[00:10:26.960 --> 00:10:29.280]   than, you know, what AlphaGo does to plan the next move?
[00:10:29.280 --> 00:10:30.640]   - Yeah, well, look, I think it's different
[00:10:30.640 --> 00:10:32.400]   because there's, our brains are not built
[00:10:32.400 --> 00:10:35.120]   for doing Monte Carlo research, right?
[00:10:35.120 --> 00:10:39.880]   It's just not the way our organic brains would work.
[00:10:39.880 --> 00:10:42.640]   So I think that in order to compensate for that,
[00:10:42.640 --> 00:10:44.560]   you know, people like Einstein have come up,
[00:10:44.560 --> 00:10:47.040]   you know, their brains have, using their intuition,
[00:10:47.040 --> 00:10:49.160]   and, you know, we can maybe come to what intuition is,
[00:10:49.160 --> 00:10:51.320]   but they use their sort of knowledge
[00:10:51.320 --> 00:10:54.160]   and their experience to build extremely,
[00:10:54.160 --> 00:10:55.320]   you know, in Einstein's case,
[00:10:55.320 --> 00:10:57.400]   extremely accurate models of physics,
[00:10:57.400 --> 00:10:59.760]   including these sort of mental simulations.
[00:10:59.760 --> 00:11:01.080]   I think if you read about Einstein
[00:11:01.080 --> 00:11:02.240]   and how he came up with things,
[00:11:02.240 --> 00:11:05.880]   he used to visualize and sort of really kind of
[00:11:05.880 --> 00:11:09.600]   feel what these physical systems should be like,
[00:11:09.600 --> 00:11:10.840]   not just the mathematics of it,
[00:11:10.840 --> 00:11:12.640]   but have a really intuitive feel
[00:11:12.640 --> 00:11:14.360]   for what they would be like in reality.
[00:11:14.360 --> 00:11:15.360]   And that allowed him to think
[00:11:15.360 --> 00:11:19.040]   these sort of very outlandish thoughts at the time.
[00:11:19.040 --> 00:11:21.640]   So I think that it's the sophistication
[00:11:21.640 --> 00:11:23.280]   of the world models that we're building,
[00:11:23.280 --> 00:11:25.800]   which then, you know, if you imagine your world model
[00:11:25.800 --> 00:11:28.080]   can get you to a certain node in a tree
[00:11:28.080 --> 00:11:29.120]   that you're searching,
[00:11:29.120 --> 00:11:31.160]   and then you just do a little bit of search
[00:11:31.160 --> 00:11:33.440]   around that node, that leaf node,
[00:11:33.440 --> 00:11:35.800]   and that gets you to these original places.
[00:11:35.800 --> 00:11:37.840]   But obviously, if your model is,
[00:11:37.840 --> 00:11:40.880]   and your judgment on that model is very, very good,
[00:11:40.880 --> 00:11:42.800]   then you can pick which leaf nodes
[00:11:42.800 --> 00:11:45.760]   you should sort of expand with search much more accurately.
[00:11:45.760 --> 00:11:48.000]   So therefore, overall, you do a lot less search.
[00:11:48.000 --> 00:11:49.800]   I mean, there's no way that, you know,
[00:11:49.800 --> 00:11:52.360]   any human could do a kind of brute force search
[00:11:52.360 --> 00:11:54.480]   over any kind of significant space.
[00:11:54.480 --> 00:11:55.840]   - Yeah, yeah, yeah.
[00:11:55.840 --> 00:11:57.400]   A big sort of open question right now
[00:11:57.400 --> 00:11:59.600]   is whether RL will allow these models
[00:11:59.680 --> 00:12:01.200]   to do the self-play synthetic data
[00:12:01.200 --> 00:12:02.520]   to get over the data bottleneck.
[00:12:02.520 --> 00:12:04.080]   It sounds like you're optimistic about this.
[00:12:04.080 --> 00:12:05.560]   - Yeah, I'm very optimistic about that.
[00:12:05.560 --> 00:12:07.600]   I mean, I think, well, first of all,
[00:12:07.600 --> 00:12:09.720]   there's still a lot more data, I think, that can be used,
[00:12:09.720 --> 00:12:11.680]   especially if one views like multimodal,
[00:12:11.680 --> 00:12:13.280]   and video, and these kind of things.
[00:12:13.280 --> 00:12:15.280]   And obviously, you know,
[00:12:15.280 --> 00:12:17.520]   society's adding more data all the time.
[00:12:17.520 --> 00:12:21.560]   But I think, to the internet and things like that,
[00:12:21.560 --> 00:12:23.960]   but I think that there's a lot of scope
[00:12:23.960 --> 00:12:26.120]   for creating synthetic data.
[00:12:26.120 --> 00:12:28.040]   We're looking at it in different ways,
[00:12:28.040 --> 00:12:29.600]   partly through simulation,
[00:12:29.600 --> 00:12:32.600]   and using very realistic games environments, for example,
[00:12:32.600 --> 00:12:37.080]   to generate realistic data, but also self-play.
[00:12:37.080 --> 00:12:41.280]   So that's where systems interact with each other,
[00:12:41.280 --> 00:12:43.440]   or converse with each other.
[00:12:43.440 --> 00:12:44.760]   And in the sense of, you know,
[00:12:44.760 --> 00:12:46.600]   worked very well for us with AlphaGo and AlphaZero,
[00:12:46.600 --> 00:12:49.080]   where we got the systems to play against each other,
[00:12:49.080 --> 00:12:50.920]   and actually learn from each other's mistakes,
[00:12:50.920 --> 00:12:52.880]   and build up a knowledge base that way.
[00:12:52.880 --> 00:12:54.800]   And I think there are some good analogies for that.
[00:12:54.800 --> 00:12:55.960]   It's a little bit more complicated,
[00:12:55.960 --> 00:12:59.800]   but to build a general kind of world data.
[00:12:59.800 --> 00:13:01.840]   - How do you get to the point where these models,
[00:13:01.840 --> 00:13:03.880]   the sort of synthetic data they're outputting,
[00:13:03.880 --> 00:13:05.440]   the self-play they're doing,
[00:13:05.440 --> 00:13:07.520]   is not just more of what they've already got
[00:13:07.520 --> 00:13:08.360]   in their data set,
[00:13:08.360 --> 00:13:10.560]   but is something they haven't seen before?
[00:13:10.560 --> 00:13:11.400]   You know what I mean?
[00:13:11.400 --> 00:13:12.280]   To actually improve the abilities.
[00:13:12.280 --> 00:13:15.960]   - Yeah, so there, I think there's a whole science needed,
[00:13:15.960 --> 00:13:17.920]   and I think we're still in the nascent stage of this,
[00:13:17.920 --> 00:13:20.000]   of data curation and data analysis.
[00:13:20.000 --> 00:13:22.360]   So actually analyzing the holes
[00:13:22.360 --> 00:13:24.320]   that you have in your data distribution.
[00:13:24.320 --> 00:13:26.120]   And this is important for things like fairness,
[00:13:26.120 --> 00:13:27.120]   and bias, and other stuff,
[00:13:27.120 --> 00:13:28.320]   to remove that from the system,
[00:13:28.320 --> 00:13:31.480]   is to try and really make sure that your data set
[00:13:31.480 --> 00:13:33.000]   is representative of the distribution
[00:13:33.000 --> 00:13:34.200]   you're trying to learn.
[00:13:34.200 --> 00:13:37.160]   And there are many tricks there one can use,
[00:13:37.160 --> 00:13:40.040]   like over-weighting or replaying certain parts of the data.
[00:13:40.040 --> 00:13:41.880]   Or you could imagine if you identify
[00:13:41.880 --> 00:13:44.040]   some gap in your data set,
[00:13:44.040 --> 00:13:46.120]   that's where you put your synthetic generation
[00:13:46.120 --> 00:13:47.520]   capabilities to work on.
[00:13:47.520 --> 00:13:50.600]   - Yep, so nowadays people are paying attention
[00:13:50.600 --> 00:13:55.440]   to the RL stuff that DeepMind did many years before.
[00:13:55.440 --> 00:13:58.080]   What are the sort of, either early research directions,
[00:13:58.080 --> 00:13:59.880]   or something that was done way back in the past,
[00:13:59.880 --> 00:14:01.800]   but people just haven't been paying attention to,
[00:14:01.800 --> 00:14:03.080]   that you think will be a big deal, right?
[00:14:03.080 --> 00:14:04.160]   Like there was a time where people
[00:14:04.160 --> 00:14:05.560]   weren't paying attention to scaling.
[00:14:05.560 --> 00:14:07.080]   What's the thing now where it's like totally underrated?
[00:14:07.080 --> 00:14:09.480]   - Well actually, I think that there's the history
[00:14:09.480 --> 00:14:11.120]   of the sort of last couple of decades
[00:14:11.120 --> 00:14:13.280]   has been things coming in and out of fashion, right?
[00:14:13.280 --> 00:14:16.000]   And I do feel like a while ago,
[00:14:16.000 --> 00:14:17.520]   when maybe five plus years ago,
[00:14:17.520 --> 00:14:19.240]   when we were pioneering with AlphaGo,
[00:14:19.240 --> 00:14:22.760]   and before that DQN, where it was the first system
[00:14:22.760 --> 00:14:25.320]   that worked on Atari, our first big system really,
[00:14:25.320 --> 00:14:26.760]   more than 10 years ago now,
[00:14:26.760 --> 00:14:28.320]   that scaled up Q-learning
[00:14:28.320 --> 00:14:29.640]   and reinforcement learning techniques
[00:14:29.640 --> 00:14:32.120]   to combine that with deep learning,
[00:14:32.120 --> 00:14:34.000]   to create deep reinforcement learning,
[00:14:34.000 --> 00:14:37.480]   and then use that to scale up to complete some,
[00:14:37.480 --> 00:14:39.160]   master some pretty complex tasks,
[00:14:39.160 --> 00:14:41.360]   like playing Atari games just from the pixels.
[00:14:41.360 --> 00:14:45.480]   And I do actually think a lot of those ideas
[00:14:45.480 --> 00:14:46.920]   need to come back in again.
[00:14:46.920 --> 00:14:48.080]   And as we talked about earlier,
[00:14:48.080 --> 00:14:51.520]   combine it with the new advances in large models
[00:14:51.520 --> 00:14:52.720]   and large multimodal models,
[00:14:52.720 --> 00:14:54.000]   which is obviously very exciting as well.
[00:14:54.000 --> 00:14:56.440]   So I do think there's a lot of potential
[00:14:56.440 --> 00:14:59.080]   for combining some of those older ideas
[00:14:59.080 --> 00:15:00.440]   together with the new ones.
[00:15:00.440 --> 00:15:03.400]   - Is there any potential for something to come,
[00:15:03.400 --> 00:15:06.360]   the AGI to eventually come from just a pure RL approach?
[00:15:06.360 --> 00:15:07.880]   Like the way we're talking about it,
[00:15:07.880 --> 00:15:09.680]   it sounds like there'll be,
[00:15:09.680 --> 00:15:12.120]   the LLM will form the right prior,
[00:15:12.120 --> 00:15:13.800]   and then this sort of research will go on top of that.
[00:15:13.800 --> 00:15:14.640]   - Yeah. - Or is it a possibility
[00:15:14.640 --> 00:15:16.160]   to just like completely out of the dark?
[00:15:16.160 --> 00:15:17.840]   - I certainly, theoretically,
[00:15:17.840 --> 00:15:19.920]   I think there's no reason why you couldn't go
[00:15:19.920 --> 00:15:21.440]   full alpha zero like on it.
[00:15:21.440 --> 00:15:25.280]   And there are some people here at Google DeepMind
[00:15:25.280 --> 00:15:28.080]   and in the RL community who work on that, right?
[00:15:28.080 --> 00:15:32.480]   And fully assuming no priors, no data,
[00:15:32.480 --> 00:15:35.720]   and just build all knowledge from scratch.
[00:15:35.720 --> 00:15:38.560]   And I think that's valuable because of course,
[00:15:38.560 --> 00:15:41.200]   those ideas and those algorithms should also work
[00:15:41.200 --> 00:15:43.400]   when you have some knowledge too.
[00:15:43.400 --> 00:15:44.240]   But having said that,
[00:15:44.240 --> 00:15:46.400]   I think by far probably my betting
[00:15:46.400 --> 00:15:48.200]   would be the quickest way to get to AGI
[00:15:48.200 --> 00:15:49.840]   and the most likely plausible way
[00:15:49.840 --> 00:15:51.680]   is to use all the knowledge
[00:15:51.680 --> 00:15:53.200]   that's existing in the world right now
[00:15:53.200 --> 00:15:55.160]   on things like the web and that we've collected.
[00:15:55.160 --> 00:15:59.840]   And we have these scalable algorithms like transformers
[00:15:59.840 --> 00:16:03.040]   that are capable of ingesting all of that information.
[00:16:03.040 --> 00:16:05.400]   And I don't see why you wouldn't start
[00:16:05.400 --> 00:16:07.640]   with a model as a kind of prior
[00:16:07.640 --> 00:16:09.840]   or to build on and to make predictions
[00:16:09.840 --> 00:16:11.840]   that helps bootstrap your learning.
[00:16:11.840 --> 00:16:13.960]   I just think it doesn't make sense
[00:16:13.960 --> 00:16:15.280]   not to make use of that.
[00:16:15.280 --> 00:16:20.280]   So my betting would be is that the final AGI system
[00:16:20.280 --> 00:16:23.400]   will have these large multimodals models
[00:16:23.400 --> 00:16:26.120]   as part of the overall solution,
[00:16:26.120 --> 00:16:28.480]   but probably won't be enough on their own.
[00:16:28.480 --> 00:16:31.240]   You will need this additional planning search on top.
[00:16:31.240 --> 00:16:32.720]   - Okay, this sounds like the answer
[00:16:32.720 --> 00:16:33.760]   to the question I'm about to ask,
[00:16:33.760 --> 00:16:37.360]   which is as somebody who's been in this field
[00:16:37.360 --> 00:16:39.560]   for a long time and seen different trends come and go,
[00:16:39.560 --> 00:16:41.400]   what do you think that the strong version
[00:16:41.400 --> 00:16:42.800]   of the scaling hypothesis gets right
[00:16:42.800 --> 00:16:43.640]   and what does it get wrong?
[00:16:43.640 --> 00:16:45.040]   It's just the idea that you just throw enough compute
[00:16:45.040 --> 00:16:46.480]   at a wide enough distribution of data
[00:16:46.480 --> 00:16:47.320]   and you get intelligence.
[00:16:47.320 --> 00:16:49.120]   - Yeah, look, my view is this is some kind
[00:16:49.120 --> 00:16:50.560]   of an empirical question right now.
[00:16:50.560 --> 00:16:53.040]   So I think it was pretty surprising to almost everyone,
[00:16:53.040 --> 00:16:55.600]   including the people who first worked
[00:16:55.600 --> 00:16:58.080]   on the scaling hypotheses that how far it's gone.
[00:16:58.080 --> 00:17:01.840]   In a way, I mean, I sort of look at the large models today
[00:17:01.840 --> 00:17:03.840]   and I think they're almost unreasonably effective
[00:17:03.840 --> 00:17:05.600]   for what they are.
[00:17:05.600 --> 00:17:07.760]   I think it's pretty surprising some of the properties
[00:17:07.760 --> 00:17:10.800]   that emerge things like, it's clearly in my opinion,
[00:17:10.800 --> 00:17:13.920]   got some form of concepts and abstractions
[00:17:13.920 --> 00:17:15.040]   and some things like that.
[00:17:15.040 --> 00:17:17.320]   And I think if we were talking five plus years ago,
[00:17:17.320 --> 00:17:18.160]   I would have said to you,
[00:17:18.160 --> 00:17:21.240]   maybe we need an additional algorithmic breakthrough
[00:17:21.240 --> 00:17:23.440]   in order to do that, like, you know,
[00:17:23.440 --> 00:17:24.960]   maybe more like the brain works.
[00:17:24.960 --> 00:17:26.520]   And I think that's still true
[00:17:26.520 --> 00:17:29.520]   if we want explicit abstract concepts, neat concepts,
[00:17:29.520 --> 00:17:32.400]   but it seems that these systems can implicitly learn that.
[00:17:32.400 --> 00:17:35.080]   Another really interesting, I think, unexpected thing
[00:17:35.080 --> 00:17:38.560]   was that these systems have some sort of grounding.
[00:17:38.560 --> 00:17:39.920]   You know, even though they don't experience
[00:17:39.920 --> 00:17:42.080]   the world multimodally, or at least until more recently
[00:17:42.080 --> 00:17:43.800]   when we have the multimodal models.
[00:17:43.800 --> 00:17:46.720]   And that's surprising that the amount of information
[00:17:46.720 --> 00:17:49.560]   that can be, and models that can be built up
[00:17:49.560 --> 00:17:50.680]   just from language.
[00:17:50.680 --> 00:17:52.400]   And I think that I'd have some hypotheses
[00:17:52.400 --> 00:17:53.680]   about why that is.
[00:17:53.680 --> 00:17:54.880]   I think we get some grounding
[00:17:54.880 --> 00:17:57.040]   through the RLHF feedback systems
[00:17:57.040 --> 00:17:59.920]   because obviously the human raters are by definition
[00:17:59.920 --> 00:18:04.360]   grounded people, we're grounded, right, in reality.
[00:18:04.360 --> 00:18:06.400]   So our feedback's also grounded.
[00:18:06.400 --> 00:18:08.600]   So perhaps there's some grounding coming in through there.
[00:18:08.600 --> 00:18:11.040]   And also maybe language contains more grounding,
[00:18:11.040 --> 00:18:13.640]   you know, if you're able to ingest all of it,
[00:18:13.640 --> 00:18:17.120]   than we perhaps thought, or linguists perhaps thought before.
[00:18:17.120 --> 00:18:18.120]   So it's actually some very interesting
[00:18:18.120 --> 00:18:19.320]   philosophical questions.
[00:18:19.320 --> 00:18:21.800]   I think we haven't, people haven't even really
[00:18:21.800 --> 00:18:24.120]   scratched the surface of yet,
[00:18:24.120 --> 00:18:26.880]   looking at the advances that have been made.
[00:18:26.880 --> 00:18:28.200]   You know, it's quite interesting to think about
[00:18:28.200 --> 00:18:29.680]   where it's going to go next.
[00:18:29.680 --> 00:18:31.720]   But in terms of your question of like, you know,
[00:18:31.720 --> 00:18:34.640]   large models, I think we've got to push scaling
[00:18:34.640 --> 00:18:37.480]   as hard as we can, and that's what we're doing here.
[00:18:37.480 --> 00:18:39.120]   And, you know, it's an empirical question
[00:18:39.120 --> 00:18:41.840]   whether that will hit an asymptote or a brick wall.
[00:18:41.840 --> 00:18:43.240]   And there are, you know, different people
[00:18:43.240 --> 00:18:44.240]   that argue about that.
[00:18:44.240 --> 00:18:45.760]   But actually, I think we should just test it.
[00:18:45.760 --> 00:18:47.080]   I think no one knows.
[00:18:47.080 --> 00:18:50.680]   And, but in the meantime, we should also double down
[00:18:50.680 --> 00:18:52.920]   on innovation and invention.
[00:18:52.920 --> 00:18:55.600]   And this is something that the Google Research
[00:18:55.600 --> 00:18:58.680]   and DeepMind and Google Brain have, you know,
[00:18:58.680 --> 00:19:00.880]   we've pioneered many, many things over the last decade.
[00:19:00.880 --> 00:19:02.680]   That's something that's our bread and butter.
[00:19:02.680 --> 00:19:05.320]   And, you know, you can think of half our effort
[00:19:05.320 --> 00:19:07.080]   as to do with scaling and half our efforts
[00:19:07.080 --> 00:19:09.840]   to do with inventing the next architectures,
[00:19:09.840 --> 00:19:12.040]   the next algorithms that will be needed,
[00:19:12.040 --> 00:19:14.960]   knowing that you've got this scaled larger and larger model
[00:19:14.960 --> 00:19:16.280]   coming along the lines.
[00:19:16.280 --> 00:19:19.880]   So my betting right now, but it's a loose betting,
[00:19:19.880 --> 00:19:21.800]   is that you would need both.
[00:19:21.800 --> 00:19:24.720]   But I think, you know, you've got to push both of them
[00:19:24.720 --> 00:19:26.400]   as hard as possible, and we're in a lucky position
[00:19:26.400 --> 00:19:27.240]   that we can do that.
[00:19:27.240 --> 00:19:28.560]   - Yeah, I want to ask more about the grounding.
[00:19:28.560 --> 00:19:30.720]   So you can imagine two things that might change,
[00:19:30.720 --> 00:19:32.520]   which would make the grounding more difficult.
[00:19:32.520 --> 00:19:34.720]   One is that as these models get smarter,
[00:19:34.720 --> 00:19:37.400]   they're going to be able to operate in domains
[00:19:37.400 --> 00:19:39.600]   where we just can't generate enough human labels
[00:19:39.600 --> 00:19:40.880]   just 'cause we're not smart enough, right?
[00:19:40.880 --> 00:19:42.800]   So if it does like a million line pull request,
[00:19:42.800 --> 00:19:44.320]   you know, how do we tell it like,
[00:19:44.320 --> 00:19:46.480]   this is within the constraints of our morality
[00:19:46.480 --> 00:19:48.640]   and the end goal we wanted, and this isn't.
[00:19:48.640 --> 00:19:50.760]   And the other is, it sounds like you're saying
[00:19:50.760 --> 00:19:52.000]   more of the compute.
[00:19:52.000 --> 00:19:53.200]   So far, we've been doing, you know,
[00:19:53.200 --> 00:19:54.720]   next token prediction, and in some sense,
[00:19:54.720 --> 00:19:56.680]   it's a guardrail 'cause you have to talk
[00:19:56.680 --> 00:19:58.840]   as a human would talk and think as a human would think.
[00:19:58.840 --> 00:20:01.560]   Now, if additional compute is going to come
[00:20:01.560 --> 00:20:04.080]   in the form of reinforcement learning,
[00:20:04.080 --> 00:20:06.040]   where it just like get to the end objective,
[00:20:06.040 --> 00:20:08.680]   we can't really trace how you got there.
[00:20:08.680 --> 00:20:09.880]   When you combine those two,
[00:20:09.880 --> 00:20:13.080]   how worried are you that the sort of grounding goes away?
[00:20:13.080 --> 00:20:16.920]   - Well, look, I think if the grounding, you know,
[00:20:16.920 --> 00:20:18.200]   if it's not properly grounded,
[00:20:18.200 --> 00:20:20.920]   the system won't be able to achieve those goals properly.
[00:20:20.920 --> 00:20:21.760]   Right, I think so.
[00:20:21.760 --> 00:20:24.560]   I think in a sense, you sort of have to have the grounding
[00:20:24.560 --> 00:20:26.720]   or at least some of it in order for a system
[00:20:26.720 --> 00:20:29.320]   to actually achieve goals in the real world.
[00:20:29.320 --> 00:20:31.800]   I do actually think that as these systems
[00:20:31.800 --> 00:20:34.560]   and things like Gemini are becoming more multimodal,
[00:20:34.560 --> 00:20:36.880]   and we start ingesting things like video
[00:20:36.880 --> 00:20:41.480]   and, you know, audio visual data, as well as text data.
[00:20:41.480 --> 00:20:43.800]   And then, you know, the system starts correlating
[00:20:43.800 --> 00:20:44.880]   those things together.
[00:20:44.880 --> 00:20:49.840]   I do, I think that is a form of proper grounding actually.
[00:20:49.840 --> 00:20:54.360]   So I do think our systems are going to start to understand,
[00:20:54.360 --> 00:20:56.520]   you know, the physics of the real world better.
[00:20:56.520 --> 00:20:58.800]   And then one could imagine the active version of that
[00:20:58.800 --> 00:21:00.760]   is being in a very realistic simulation
[00:21:00.760 --> 00:21:03.640]   or game environment where you're starting to learn
[00:21:03.640 --> 00:21:06.000]   about what your actions do in the world
[00:21:06.000 --> 00:21:10.320]   and how that affects the world itself,
[00:21:10.320 --> 00:21:11.280]   the world stay itself,
[00:21:11.280 --> 00:21:14.040]   but also what next learning episode you're getting.
[00:21:14.040 --> 00:21:16.600]   So, you know, these RL agents we've always been working on
[00:21:16.600 --> 00:21:19.160]   and pioneered like AlphaZero and AlphaGo,
[00:21:19.160 --> 00:21:21.200]   they actually affect their active learners.
[00:21:21.200 --> 00:21:22.760]   What they decide to do next
[00:21:22.760 --> 00:21:25.920]   affects what the next learning piece of data
[00:21:25.920 --> 00:21:27.560]   or experience they're going to get.
[00:21:27.560 --> 00:21:29.360]   So there's this very interesting sort of feedback loop.
[00:21:29.360 --> 00:21:30.640]   And of course, if we ever want to be good
[00:21:30.640 --> 00:21:31.960]   at things like robotics,
[00:21:31.960 --> 00:21:33.320]   we're going to have to understand
[00:21:33.320 --> 00:21:35.560]   how to act in the real world.
[00:21:35.560 --> 00:21:37.320]   - Yeah, so there's a grounding in terms of,
[00:21:37.320 --> 00:21:39.320]   will the capabilities be able to proceed?
[00:21:39.320 --> 00:21:41.320]   Will they be like enough in touch with reality
[00:21:41.320 --> 00:21:42.800]   to be able to like do the things we want?
[00:21:42.800 --> 00:21:45.320]   And there's another sense of grounding of,
[00:21:45.320 --> 00:21:46.600]   we've gotten lucky in the sense that
[00:21:46.600 --> 00:21:47.960]   since they're trained on human thought,
[00:21:47.960 --> 00:21:49.640]   they like maybe think like a human.
[00:21:49.640 --> 00:21:51.400]   To what extent does that stay true
[00:21:51.400 --> 00:21:53.800]   when more of the compute for training comes from
[00:21:53.800 --> 00:21:56.600]   just did you get the right outcome and not guardrailed by,
[00:21:56.600 --> 00:21:58.560]   like, are you like proceeding on the next token
[00:21:58.560 --> 00:21:59.400]   as a human would?
[00:21:59.400 --> 00:22:01.800]   Maybe the broader question I'll like pose to you is,
[00:22:01.800 --> 00:22:03.040]   and this is what I asked Shane as well,
[00:22:03.040 --> 00:22:04.600]   what would it take to align a system
[00:22:04.600 --> 00:22:05.600]   that's smarter than a human?
[00:22:05.600 --> 00:22:07.880]   Maybe things in alien concepts,
[00:22:07.880 --> 00:22:09.240]   and you can't like really monitor
[00:22:09.240 --> 00:22:10.280]   the million line pull requests
[00:22:10.280 --> 00:22:12.160]   'cause you can't really understand the whole thing.
[00:22:12.160 --> 00:22:13.000]   - Yeah. - And you can't
[00:22:13.000 --> 00:22:13.920]   give labels. - Look, this is something
[00:22:13.920 --> 00:22:15.640]   Shane and I and many others here,
[00:22:15.640 --> 00:22:17.120]   we've had that forefront of our minds
[00:22:17.120 --> 00:22:19.320]   for since before we started DeepMind.
[00:22:19.320 --> 00:22:21.880]   And 'cause we planned for success, crazy.
[00:22:21.880 --> 00:22:23.680]   You know, 2010, no one was thinking about AI,
[00:22:23.680 --> 00:22:25.000]   let alone AGI.
[00:22:25.000 --> 00:22:27.480]   But we already knew that if we could make progress
[00:22:27.480 --> 00:22:29.480]   with these systems and these ideas,
[00:22:29.480 --> 00:22:31.880]   it, you know, the technology that would be created
[00:22:31.880 --> 00:22:33.680]   would be unbelievably transformative.
[00:22:33.680 --> 00:22:36.200]   So we already were thinking, you know, 20 years ago
[00:22:36.200 --> 00:22:37.560]   about, well, what, how, you know,
[00:22:37.560 --> 00:22:39.040]   what would the consequences of that be,
[00:22:39.040 --> 00:22:40.560]   both positive and negative?
[00:22:40.560 --> 00:22:43.120]   Of course, the positive direction is amazing science,
[00:22:43.120 --> 00:22:44.160]   things like AlphaFold,
[00:22:44.160 --> 00:22:46.520]   incredible breakthroughs in health and science,
[00:22:46.520 --> 00:22:50.240]   and maths and discovery, scientific discovery.
[00:22:50.240 --> 00:22:52.080]   But then also we gotta make sure these systems
[00:22:52.080 --> 00:22:54.120]   are sort of understandable and controllable.
[00:22:54.120 --> 00:22:56.160]   And I think there's sort of several, you know,
[00:22:56.160 --> 00:22:58.240]   this would be a whole sort of discussion in itself,
[00:22:58.240 --> 00:23:01.000]   but there are many, many ideas that people have
[00:23:01.000 --> 00:23:03.520]   from much more stringent eval systems.
[00:23:03.520 --> 00:23:05.400]   I think we don't have a good enough evaluations
[00:23:05.400 --> 00:23:07.320]   and benchmarks for things like,
[00:23:07.320 --> 00:23:09.200]   can the system deceive you?
[00:23:09.200 --> 00:23:10.680]   Can it exfiltrate its own code?
[00:23:10.680 --> 00:23:13.120]   Sort of undesirable behaviors.
[00:23:13.120 --> 00:23:18.080]   And then there's, you know, ideas of actually using AI,
[00:23:18.080 --> 00:23:20.840]   maybe narrow AIs, so not general learning ones,
[00:23:20.840 --> 00:23:23.480]   but systems that are specialized for a domain
[00:23:23.480 --> 00:23:27.600]   to help us as the human scientists analyze
[00:23:27.600 --> 00:23:30.600]   and summarize what the more general system is doing.
[00:23:30.600 --> 00:23:33.520]   Right, so kind of narrow AI tools.
[00:23:33.520 --> 00:23:35.360]   I think that there's a lot of promise
[00:23:35.360 --> 00:23:38.520]   in creating hardened sandboxes or simulations
[00:23:38.520 --> 00:23:41.400]   so that they're hardened with cybersecurity
[00:23:41.400 --> 00:23:45.320]   arrangements around the simulation,
[00:23:45.320 --> 00:23:47.480]   both to keep the AI in,
[00:23:47.480 --> 00:23:50.960]   but also as cybersecurity to keep hackers out.
[00:23:50.960 --> 00:23:53.440]   And then you could experiment a lot more
[00:23:53.440 --> 00:23:55.560]   freely within that sandbox domain.
[00:23:55.560 --> 00:23:58.400]   And I think a lot of these ideas are,
[00:23:58.400 --> 00:23:59.880]   and there's many, many others,
[00:23:59.880 --> 00:24:02.000]   including the analysis stuff we talked about earlier,
[00:24:02.000 --> 00:24:04.280]   where can we analyze and understand
[00:24:04.280 --> 00:24:06.480]   what the concepts are that this system's building,
[00:24:06.480 --> 00:24:07.840]   what the representations are,
[00:24:07.840 --> 00:24:09.760]   so maybe they're not so alien to us
[00:24:09.760 --> 00:24:13.160]   and we can actually keep track of the kind of knowledge
[00:24:13.160 --> 00:24:14.000]   that it's building.
[00:24:14.000 --> 00:24:14.840]   - Yeah, yeah.
[00:24:14.840 --> 00:24:15.680]   So getting back a bit,
[00:24:15.680 --> 00:24:16.880]   I'm curious what your timelines are.
[00:24:16.880 --> 00:24:19.560]   So Shane said he's like, I think modal outcome is 2028.
[00:24:19.560 --> 00:24:20.640]   I think that's maybe his median.
[00:24:20.640 --> 00:24:21.480]   - Yeah.
[00:24:21.480 --> 00:24:22.320]   - What is yours?
[00:24:22.440 --> 00:24:24.920]   - You know, I don't have prescribed
[00:24:24.920 --> 00:24:26.080]   kind of specific numbers to it
[00:24:26.080 --> 00:24:28.880]   because I think there's so many unknowns and uncertainties
[00:24:28.880 --> 00:24:32.520]   and, you know, human ingenuity and endeavor
[00:24:32.520 --> 00:24:34.400]   comes up with surprises all the time.
[00:24:34.400 --> 00:24:37.960]   So that could meaningfully move the timelines.
[00:24:37.960 --> 00:24:41.320]   But I will say that when we started DeepMind back in 2010,
[00:24:41.320 --> 00:24:43.560]   you know, we thought of it as a 20-year project
[00:24:43.560 --> 00:24:45.800]   and actually I think we're on track,
[00:24:45.800 --> 00:24:47.840]   which is kind of amazing for 20-year projects
[00:24:47.840 --> 00:24:49.840]   'cause usually they're always 20 years away, right?
[00:24:49.840 --> 00:24:51.240]   So that's the joke about, you know,
[00:24:51.240 --> 00:24:54.520]   whatever it is, quantum AI, you know, take your pick.
[00:24:54.520 --> 00:24:56.960]   And, but I think we, you know, I think we're on track.
[00:24:56.960 --> 00:25:00.760]   So I wouldn't be surprised if we had AGI-like systems
[00:25:00.760 --> 00:25:02.200]   within the next decade.
[00:25:02.200 --> 00:25:04.840]   - And do you buy the model that once you have an AGI,
[00:25:04.840 --> 00:25:06.520]   you have a system that basically speeds up
[00:25:06.520 --> 00:25:07.560]   further AI research?
[00:25:07.560 --> 00:25:09.080]   Maybe not like an overnight sense,
[00:25:09.080 --> 00:25:10.960]   but, you know, over the course of months and years,
[00:25:10.960 --> 00:25:11.840]   you have much faster progress
[00:25:11.840 --> 00:25:12.680]   than you would have by the right side.
[00:25:12.680 --> 00:25:15.240]   - I think that's potentially possible.
[00:25:15.240 --> 00:25:18.240]   I think it partly depends what we decide.
[00:25:18.240 --> 00:25:20.720]   We as a society decide to use the first AGI,
[00:25:20.720 --> 00:25:24.680]   nascent AGI systems, or even proto-AGI systems for.
[00:25:24.680 --> 00:25:28.080]   So, you know, even the current LLMs
[00:25:28.080 --> 00:25:29.600]   seem to be pretty good at coding.
[00:25:29.600 --> 00:25:32.280]   So, and you know, we have systems like AlphaCode.
[00:25:32.280 --> 00:25:34.000]   We also got theorem-proving systems.
[00:25:34.000 --> 00:25:37.920]   So one could imagine combining these ideas together
[00:25:37.920 --> 00:25:39.920]   and making them a lot better.
[00:25:39.920 --> 00:25:43.440]   And then I could imagine these systems being quite good
[00:25:43.440 --> 00:25:46.000]   at designing and helping us
[00:25:46.000 --> 00:25:48.360]   build future versions of themselves.
[00:25:48.360 --> 00:25:49.280]   But we also have to think about
[00:25:49.280 --> 00:25:51.080]   the safety implications of that, of course.
[00:25:51.080 --> 00:25:52.040]   - Yeah, I'm curious what you think about that.
[00:25:52.040 --> 00:25:54.360]   So, I mean, I'm not saying this is happening this year
[00:25:54.360 --> 00:25:56.800]   or anything, but eventually you'll be developing a model
[00:25:56.800 --> 00:25:58.920]   where during the process of development, you think,
[00:25:58.920 --> 00:25:59.960]   you know, there's some chance
[00:25:59.960 --> 00:26:01.360]   that once this is fully developed,
[00:26:01.360 --> 00:26:03.280]   it'll be capable of like an intelligence explosion,
[00:26:03.280 --> 00:26:04.960]   like dynamic.
[00:26:04.960 --> 00:26:07.760]   What would have to be true of that model at that point
[00:26:07.760 --> 00:26:11.040]   where you're like, you know, I've seen these specific evals.
[00:26:11.040 --> 00:26:13.760]   I've like understand it's internal thinking enough
[00:26:13.760 --> 00:26:15.720]   and like it's future thinking that I'm comfortable
[00:26:15.720 --> 00:26:17.160]   continuing development of the system.
[00:26:17.160 --> 00:26:20.320]   - Well, look, we need a lot more understanding
[00:26:20.320 --> 00:26:21.440]   of the systems than we do today
[00:26:21.440 --> 00:26:24.360]   before I would be even confident of even explaining to you
[00:26:24.360 --> 00:26:26.920]   what we would need to tick box there.
[00:26:26.920 --> 00:26:28.640]   So I think actually what we've got to do
[00:26:28.640 --> 00:26:30.320]   in the next few years and the time we have
[00:26:30.320 --> 00:26:32.120]   before those systems start arriving
[00:26:32.120 --> 00:26:36.240]   is come up with the right evaluations and metrics
[00:26:36.240 --> 00:26:38.600]   and maybe ideally formal proofs,
[00:26:38.600 --> 00:26:40.000]   but, you know, it's going to be hard
[00:26:40.000 --> 00:26:40.960]   for these types of systems,
[00:26:40.960 --> 00:26:43.880]   but at least empirical bounds
[00:26:43.880 --> 00:26:46.120]   around what these systems can do.
[00:26:46.120 --> 00:26:49.400]   And that's why I think about things like deception
[00:26:49.400 --> 00:26:52.320]   and as being quite root node traits that you don't want,
[00:26:52.320 --> 00:26:54.560]   because if you're confident that your system
[00:26:54.560 --> 00:26:58.760]   is sort of exposing what it actually thinks,
[00:26:58.760 --> 00:27:00.200]   then you could potentially,
[00:27:00.200 --> 00:27:03.040]   that opens up possibilities of using the system itself
[00:27:03.040 --> 00:27:06.040]   to explain aspects of itself to you.
[00:27:06.040 --> 00:27:08.640]   The way I think about that actually is like,
[00:27:08.640 --> 00:27:11.000]   if I was to play a game of chess against Garry Kasparov,
[00:27:11.000 --> 00:27:13.240]   right, which I've played in the past or Magnus Carlsen,
[00:27:13.240 --> 00:27:16.080]   you know, the amazing chess player, graceful time,
[00:27:16.080 --> 00:27:18.800]   I wouldn't be able to come up with a move that they could,
[00:27:18.800 --> 00:27:20.880]   but they could explain to me
[00:27:20.880 --> 00:27:23.360]   why they came up with that move
[00:27:23.360 --> 00:27:26.680]   and I could understand it post hoc, right?
[00:27:26.680 --> 00:27:29.240]   And that's the sort of thing one could imagine.
[00:27:29.240 --> 00:27:33.800]   One of the capabilities
[00:27:33.800 --> 00:27:35.280]   that we could make use of these systems
[00:27:35.280 --> 00:27:37.480]   is for them to explain it to us
[00:27:37.480 --> 00:27:39.080]   and even maybe the proofs behind
[00:27:39.080 --> 00:27:40.320]   why they're thinking something.
[00:27:40.320 --> 00:27:42.960]   Certainly in a mathematical, any mathematical problem.
[00:27:42.960 --> 00:27:43.840]   - Got it.
[00:27:43.840 --> 00:27:46.240]   Do you have a sense of what the converse answer would be?
[00:27:46.240 --> 00:27:48.240]   So what would have to be true where tomorrow morning
[00:27:48.240 --> 00:27:50.760]   you're like, "Oh man, I didn't anticipate this."
[00:27:50.760 --> 00:27:52.240]   You see some specific observation tomorrow morning
[00:27:52.240 --> 00:27:54.120]   where like, "We got to stop Gemini 2 training."
[00:27:54.120 --> 00:27:55.840]   Like what would specifically-
[00:27:55.840 --> 00:27:57.520]   - Yeah, I could imagine that like,
[00:27:57.520 --> 00:27:58.960]   and this is where, you know,
[00:27:58.960 --> 00:28:00.720]   things like the sandbox simulations.
[00:28:00.720 --> 00:28:03.040]   I would hope we're experimenting
[00:28:03.040 --> 00:28:06.080]   in a safe, secure environment.
[00:28:06.080 --> 00:28:08.720]   And then, you know, something happens in it
[00:28:08.720 --> 00:28:11.240]   where very unexpected happens
[00:28:11.240 --> 00:28:13.080]   and you unexpected capability
[00:28:13.080 --> 00:28:14.360]   or something that we didn't want,
[00:28:14.360 --> 00:28:16.360]   you know, explicitly told the system we didn't want
[00:28:16.360 --> 00:28:18.240]   that it did, but then lied about.
[00:28:18.240 --> 00:28:19.560]   You know, these are the kinds of things
[00:28:19.560 --> 00:28:24.360]   where one would want to then dig in carefully,
[00:28:24.360 --> 00:28:26.520]   you know, now with the systems that are around today,
[00:28:26.520 --> 00:28:29.160]   which are not dangerous in my opinion today,
[00:28:29.160 --> 00:28:32.520]   but in a few years they might be, have potential.
[00:28:32.520 --> 00:28:37.040]   And then you would sort of ideally kind of pause
[00:28:37.040 --> 00:28:38.800]   and then really get to the bottom
[00:28:38.800 --> 00:28:42.560]   of why it was doing those things before one continued.
[00:28:42.560 --> 00:28:44.080]   - Yeah, going back to Gemini,
[00:28:44.080 --> 00:28:47.400]   I'm curious what the bottlenecks were in the development.
[00:28:47.400 --> 00:28:48.640]   Like why not make it immediately
[00:28:48.640 --> 00:28:52.520]   one order of magnitude bigger if like scaling works?
[00:28:52.520 --> 00:28:54.680]   - Well, look, first of all, there are practical limits.
[00:28:54.680 --> 00:28:58.000]   How much compute can you actually fit in one data center?
[00:28:58.000 --> 00:28:59.880]   And actually, you know, you're bumping up
[00:28:59.880 --> 00:29:04.440]   against very interesting, you know,
[00:29:04.440 --> 00:29:06.720]   distributor computing kind of challenges, right?
[00:29:06.720 --> 00:29:08.240]   Where unfortunately we have some of the best people
[00:29:08.240 --> 00:29:09.720]   in the world on those challenges
[00:29:09.720 --> 00:29:12.000]   and, you know, cross data center training,
[00:29:12.000 --> 00:29:13.280]   all of these kinds of things.
[00:29:13.280 --> 00:29:15.200]   Very interesting challenges, hardware challenges.
[00:29:15.200 --> 00:29:17.560]   And we have our TPUs and so on that we're building
[00:29:17.560 --> 00:29:20.600]   and designing all the time, as well as using GPUs.
[00:29:20.600 --> 00:29:22.920]   And so there's all of that.
[00:29:22.920 --> 00:29:25.840]   And then you also have to, the scaling laws, you know,
[00:29:25.840 --> 00:29:27.320]   they don't just work by magic.
[00:29:27.320 --> 00:29:30.160]   You sort of, you still need to scale up the hyper parameters
[00:29:30.160 --> 00:29:32.440]   and various innovations are going in all the time
[00:29:32.440 --> 00:29:33.360]   with each new scale.
[00:29:33.360 --> 00:29:36.080]   It's not just about repeating the same recipe.
[00:29:36.080 --> 00:29:38.480]   Each new scale, you have to adjust the recipe.
[00:29:38.480 --> 00:29:40.920]   And that's a bit of an art form in a way.
[00:29:40.920 --> 00:29:43.280]   And you have to sort of almost get new data points.
[00:29:43.280 --> 00:29:46.520]   If you try and extend your predictions, extrapolate them,
[00:29:46.520 --> 00:29:48.480]   say several orders of magnitude out,
[00:29:48.480 --> 00:29:50.440]   sometimes they don't hold anymore, right?
[00:29:50.440 --> 00:29:54.040]   Because new capabilities, they can be step functions
[00:29:54.040 --> 00:29:57.520]   in terms of new capabilities and some things just,
[00:29:57.520 --> 00:29:59.240]   some things hold and other things don't.
[00:29:59.240 --> 00:30:02.360]   So often you do need those intermediate data points
[00:30:02.360 --> 00:30:05.600]   actually to correct some of your hyper parameters
[00:30:05.600 --> 00:30:06.880]   optimization and other things.
[00:30:06.880 --> 00:30:09.760]   So the scaling law continues to be true.
[00:30:09.760 --> 00:30:13.600]   So there's sort of various practical limitations
[00:30:13.600 --> 00:30:15.320]   on to that.
[00:30:15.320 --> 00:30:18.000]   So, you know, kind of one order of magnitude
[00:30:18.000 --> 00:30:21.360]   is about probably the maximum that you want to carry on.
[00:30:21.360 --> 00:30:24.080]   You want to sort of do between each era.
[00:30:24.080 --> 00:30:25.440]   - Oh, that's so fascinating.
[00:30:25.440 --> 00:30:26.920]   You know, in the GPT-4 technical report,
[00:30:26.920 --> 00:30:30.520]   they say that they were able to predict the training loss,
[00:30:30.520 --> 00:30:32.960]   you know, tens of thousands of times less compute
[00:30:32.960 --> 00:30:34.800]   than GPT-4 that they could see the curve.
[00:30:34.800 --> 00:30:35.640]   But at the point you're making
[00:30:35.640 --> 00:30:38.440]   is that the actual capabilities that loss implies,
[00:30:38.440 --> 00:30:39.280]   it may not be so clear.
[00:30:39.280 --> 00:30:40.200]   - Yeah, the downstream capabilities
[00:30:40.200 --> 00:30:41.640]   sometimes don't follow from the,
[00:30:41.640 --> 00:30:44.400]   you can often predict the core metrics like training loss
[00:30:44.400 --> 00:30:45.640]   or something like that,
[00:30:45.640 --> 00:30:48.760]   but then it doesn't actually translate into MMLU
[00:30:48.760 --> 00:30:53.760]   or math or some other actual capability that you care about.
[00:30:53.760 --> 00:30:56.040]   They're not necessarily linear all the time.
[00:30:56.040 --> 00:30:57.560]   So there's sort of nonlinear effects there.
[00:30:57.560 --> 00:30:58.560]   - What was the biggest surprise to you
[00:30:58.560 --> 00:30:59.960]   during the development of Gemini?
[00:30:59.960 --> 00:31:02.600]   So something like this happening.
[00:31:02.600 --> 00:31:05.440]   - Well, I mean, I wouldn't say there was one big surprise,
[00:31:05.440 --> 00:31:07.160]   but it was very interesting, you know,
[00:31:07.160 --> 00:31:09.360]   trying to train things at that size
[00:31:09.360 --> 00:31:13.840]   and learning about all sorts of things from organization
[00:31:13.840 --> 00:31:16.640]   or how to babysit such a system and to track it.
[00:31:16.640 --> 00:31:20.880]   And I think things like getting a better understanding
[00:31:20.880 --> 00:31:23.160]   of the metrics you're optimizing
[00:31:23.160 --> 00:31:26.760]   versus the final capabilities that you want.
[00:31:26.760 --> 00:31:30.440]   I would say that's still not a perfectly understood mapping,
[00:31:30.440 --> 00:31:31.600]   but it's an interesting one
[00:31:31.600 --> 00:31:32.880]   that we're getting better and better at.
[00:31:32.880 --> 00:31:33.720]   - Yeah, yeah.
[00:31:33.720 --> 00:31:34.920]   There's a perception that maybe other labs
[00:31:34.920 --> 00:31:38.640]   are more compute efficient than DeepMind has been
[00:31:38.640 --> 00:31:39.480]   with Gemini.
[00:31:39.480 --> 00:31:40.560]   I don't know what you make of that perception.
[00:31:40.560 --> 00:31:41.800]   - I don't think that's the case.
[00:31:41.800 --> 00:31:46.640]   I mean, you know, I think that actually Gemini 1
[00:31:46.640 --> 00:31:48.200]   used roughly the same amount of compute,
[00:31:48.200 --> 00:31:50.440]   maybe slightly more than what was rumored for GPT-4.
[00:31:50.440 --> 00:31:51.960]   I don't know exactly what was used.
[00:31:51.960 --> 00:31:55.600]   So I think it was in the same ballpark.
[00:31:55.600 --> 00:31:57.160]   I think we're very efficient with our compute
[00:31:57.160 --> 00:31:59.200]   and we use our compute for many things.
[00:31:59.200 --> 00:32:00.280]   One is not just the scaling,
[00:32:00.280 --> 00:32:02.760]   but going back to earlier to these more innovation
[00:32:02.760 --> 00:32:05.520]   and ideas, you've got to, you know,
[00:32:05.520 --> 00:32:08.280]   it's only useful, a new innovation, a new invention,
[00:32:08.280 --> 00:32:10.360]   if it also can scale.
[00:32:10.360 --> 00:32:13.600]   So in a way, you also need quite a lot of compute
[00:32:13.600 --> 00:32:17.160]   to do new invention because you've got to test many things
[00:32:17.160 --> 00:32:18.960]   at least some reasonable scale
[00:32:18.960 --> 00:32:20.840]   and make sure that they work at that scale.
[00:32:20.840 --> 00:32:24.000]   And also some new ideas may not work at a toy scale,
[00:32:24.000 --> 00:32:26.040]   but do work at a larger scale.
[00:32:26.040 --> 00:32:27.720]   And in fact, those are the more valuable ones.
[00:32:27.720 --> 00:32:30.280]   So you actually, if you think about that exploration process,
[00:32:30.280 --> 00:32:33.480]   you need quite a lot of compute to be able to do that.
[00:32:33.480 --> 00:32:35.480]   I mean, the good news is, is I think, you know,
[00:32:35.480 --> 00:32:38.280]   we're pretty lucky at Google that we,
[00:32:38.280 --> 00:32:40.040]   I think this year certainly we're going to have
[00:32:40.040 --> 00:32:42.840]   the most compute by far of any sort of research lab.
[00:32:42.840 --> 00:32:44.600]   And, you know, we hope to make very efficient
[00:32:44.600 --> 00:32:47.320]   and good use of that in terms of both scaling
[00:32:47.320 --> 00:32:50.960]   and the capability of our systems and also new inventions.
[00:32:50.960 --> 00:32:51.800]   - Yeah.
[00:32:51.800 --> 00:32:53.080]   What's been the biggest surprise to you
[00:32:53.080 --> 00:32:55.520]   if you go back to yourself in 2010
[00:32:55.520 --> 00:32:56.560]   when you were starting DeepMind
[00:32:56.560 --> 00:32:58.640]   in terms of what AI progress has looked like.
[00:32:58.640 --> 00:32:59.960]   Did you anticipate back then
[00:32:59.960 --> 00:33:02.280]   that it would in some large sense amount to spend
[00:33:02.280 --> 00:33:04.520]   as, you know, dumping billions of dollars into these models
[00:33:04.520 --> 00:33:05.920]   or did you have a different sense of what it would look like?
[00:33:05.920 --> 00:33:07.760]   - We thought that initially, you know, if you,
[00:33:07.760 --> 00:33:09.760]   I know you've interviewed my colleague Shane
[00:33:09.760 --> 00:33:14.640]   and he always thought that in terms of like compute curves
[00:33:14.640 --> 00:33:17.400]   and then maybe comparing roughly to like the brain
[00:33:17.400 --> 00:33:19.840]   and how many neurons and synapses there are very loosely.
[00:33:19.840 --> 00:33:22.400]   But we're actually interestingly in that kind of regime
[00:33:22.400 --> 00:33:24.640]   that roughly in the right order of magnitude of,
[00:33:24.640 --> 00:33:26.200]   you know, number of synapses in the brain
[00:33:26.200 --> 00:33:28.800]   and the sort of compute that we have.
[00:33:28.800 --> 00:33:31.000]   But I think more fundamentally, you know,
[00:33:31.000 --> 00:33:35.000]   we always thought that we bet on generality
[00:33:35.000 --> 00:33:36.320]   and learning, right?
[00:33:36.320 --> 00:33:38.360]   So those were always at the core
[00:33:38.360 --> 00:33:39.840]   of any technique we would use.
[00:33:39.840 --> 00:33:42.000]   That's why we triangulated on reinforcement learning
[00:33:42.000 --> 00:33:44.840]   and search and deep learning, right?
[00:33:44.840 --> 00:33:48.800]   As three types of algorithms that would scale
[00:33:48.800 --> 00:33:51.600]   and would be very general
[00:33:51.600 --> 00:33:55.000]   and not require a lot of handcrafted human priors,
[00:33:55.000 --> 00:33:57.600]   which we thought was the sort of failure mode really
[00:33:57.600 --> 00:34:00.960]   of the efforts to build AI in the '90s, right?
[00:34:00.960 --> 00:34:03.520]   Places like MIT, where there are very, you know,
[00:34:03.520 --> 00:34:05.840]   logic-based systems, expert systems,
[00:34:05.840 --> 00:34:08.640]   you know, masses of hand-coded, handcrafted,
[00:34:08.640 --> 00:34:11.080]   human information going into that turned out to be wrong
[00:34:11.080 --> 00:34:12.400]   or too rigid.
[00:34:12.400 --> 00:34:13.920]   So we wanted to move away from that.
[00:34:13.920 --> 00:34:17.320]   I think we spotted that trend early and became, you know,
[00:34:17.320 --> 00:34:19.800]   and obviously we use games as our proving ground
[00:34:19.800 --> 00:34:21.200]   and we did very well with that.
[00:34:21.200 --> 00:34:23.240]   And I think all of that was very successful
[00:34:23.240 --> 00:34:26.160]   and I think maybe inspired others to, you know,
[00:34:26.160 --> 00:34:27.880]   things like AlphaGo, I think was a big moment
[00:34:27.880 --> 00:34:29.760]   for inspiring many others to think,
[00:34:29.760 --> 00:34:32.480]   "Oh, actually these systems are ready to scale."
[00:34:32.480 --> 00:34:34.600]   And then of course, with the advent of transformers
[00:34:34.600 --> 00:34:36.040]   invented by our colleagues at Google,
[00:34:36.040 --> 00:34:37.600]   you know, research and brain,
[00:34:37.600 --> 00:34:40.800]   that was the, then, you know, the type of deep learning
[00:34:40.800 --> 00:34:44.560]   that allowed us to ingest masses of amounts of information.
[00:34:44.560 --> 00:34:46.000]   And that, of course,
[00:34:46.000 --> 00:34:47.800]   has really turbocharged where we are today.
[00:34:47.800 --> 00:34:50.240]   So I think that's all part of the same lineage.
[00:34:50.240 --> 00:34:51.640]   You know, we couldn't have predicted
[00:34:51.640 --> 00:34:52.760]   every twist and turn there,
[00:34:52.760 --> 00:34:56.000]   but I think the general direction we were going in
[00:34:56.000 --> 00:34:56.960]   was the right one.
[00:34:56.960 --> 00:34:59.320]   - Yeah, and in fact, it's like fascinating
[00:34:59.320 --> 00:35:00.840]   'cause actually, if you like read your old papers
[00:35:00.840 --> 00:35:03.520]   or Shane's old papers, Shane's thesis, I think in 2009,
[00:35:03.520 --> 00:35:05.480]   he said like, "Well, you know, the way we would test for AI
[00:35:05.480 --> 00:35:07.040]   "is if you can compress Wikipedia."
[00:35:07.040 --> 00:35:08.800]   And that's like literally the last function of our alarms.
[00:35:08.800 --> 00:35:10.240]   Or like your own paper in like 2016,
[00:35:10.240 --> 00:35:12.440]   before transformers, where we said like,
[00:35:12.440 --> 00:35:14.400]   you were comparing neuroscience and AI
[00:35:14.400 --> 00:35:16.240]   and he said, "Attention is what is needed."
[00:35:16.240 --> 00:35:17.560]   - Yes, exactly, exactly.
[00:35:17.560 --> 00:35:19.000]   So we had these things called out
[00:35:19.000 --> 00:35:22.360]   and actually we had some early attention papers,
[00:35:22.360 --> 00:35:24.480]   but they weren't as elegant as transformers in the end,
[00:35:24.480 --> 00:35:26.760]   like Neural Turing Machines and things like this.
[00:35:26.760 --> 00:35:29.240]   And then transformers was the nicer
[00:35:29.240 --> 00:35:30.800]   and more general architecture of that.
[00:35:30.800 --> 00:35:32.000]   - Yeah, yeah, yeah.
[00:35:32.000 --> 00:35:34.080]   When you extrapolate all this out forward
[00:35:34.080 --> 00:35:36.280]   and you think about superhuman intelligence,
[00:35:36.280 --> 00:35:39.720]   like what does that landscape look like to you?
[00:35:39.720 --> 00:35:42.440]   Is it like still controlled by a private company?
[00:35:42.440 --> 00:35:43.280]   Like what should the governance
[00:35:43.280 --> 00:35:45.400]   of that look like concretely?
[00:35:45.400 --> 00:35:47.480]   - Yeah, look, I would love, you know,
[00:35:47.480 --> 00:35:49.160]   I think that this has to be,
[00:35:49.160 --> 00:35:52.120]   this is so consequential, this technology.
[00:35:52.120 --> 00:35:54.640]   I think it's much bigger than any one company
[00:35:54.640 --> 00:35:57.360]   or even industry in general.
[00:35:57.360 --> 00:36:00.040]   I think it has to be a big collaboration
[00:36:00.040 --> 00:36:03.040]   with many stakeholders from civil society,
[00:36:03.040 --> 00:36:04.560]   academia, government.
[00:36:04.560 --> 00:36:06.560]   And the good news is I think with the popularity
[00:36:06.560 --> 00:36:08.800]   of the recent chatbot systems and so on,
[00:36:08.800 --> 00:36:11.960]   I think that has woken up many of these other parts
[00:36:11.960 --> 00:36:13.640]   of society that this is coming
[00:36:13.640 --> 00:36:16.040]   and what it will be like to interact with these systems.
[00:36:16.040 --> 00:36:16.880]   And that's great.
[00:36:16.880 --> 00:36:20.240]   So it's opened up lots of doors for very good conversations.
[00:36:20.240 --> 00:36:22.520]   I mean, an example of that was the safety summit
[00:36:22.520 --> 00:36:24.600]   in the UK hosted a few months ago,
[00:36:24.600 --> 00:36:25.720]   which I thought was a big success
[00:36:25.720 --> 00:36:28.320]   to start getting this international dialogue going.
[00:36:28.320 --> 00:36:31.040]   And, you know, I think the whole of society
[00:36:31.040 --> 00:36:32.400]   needs to be involved in deciding
[00:36:32.400 --> 00:36:34.680]   what do we want to deploy these models for?
[00:36:34.680 --> 00:36:35.640]   How do we want to use them?
[00:36:35.640 --> 00:36:37.200]   What do we not want to use them for?
[00:36:37.200 --> 00:36:38.320]   You know, I think we've got to try
[00:36:38.320 --> 00:36:40.640]   and get some international consensus around that.
[00:36:40.640 --> 00:36:43.440]   And then also making sure that the benefits
[00:36:43.440 --> 00:36:46.080]   of these systems benefit everyone,
[00:36:46.080 --> 00:36:48.480]   you know, for the good of everyone in society in general.
[00:36:48.480 --> 00:36:51.600]   And that's why I push so hard things like AI for science.
[00:36:51.600 --> 00:36:53.360]   And I hope that, you know,
[00:36:53.360 --> 00:36:55.160]   with things like our spin out isomorphic,
[00:36:55.160 --> 00:36:56.720]   we're going to start curing diseases,
[00:36:56.720 --> 00:36:58.280]   you know, terrible diseases with AI
[00:36:58.280 --> 00:36:59.840]   and accelerate drug discovery.
[00:36:59.840 --> 00:37:01.960]   Amazing things, climate change and other things,
[00:37:01.960 --> 00:37:05.920]   I think big challenges that face us and face humanity.
[00:37:05.920 --> 00:37:07.360]   Massive challenges actually,
[00:37:07.360 --> 00:37:09.560]   which I'm optimistic we can solve
[00:37:09.560 --> 00:37:11.800]   because we've got this incredibly powerful tool
[00:37:11.800 --> 00:37:13.960]   coming along down the line of AI
[00:37:13.960 --> 00:37:15.880]   that we can apply and I think help us
[00:37:15.880 --> 00:37:17.760]   and solve many of these problems.
[00:37:17.760 --> 00:37:22.120]   So, you know, ideally we would have a big consensus
[00:37:22.120 --> 00:37:24.440]   around that and a big discussion, you know,
[00:37:24.440 --> 00:37:27.080]   sort of almost like the UN level if possible.
[00:37:27.080 --> 00:37:28.640]   - You know, one interesting thing is
[00:37:28.640 --> 00:37:29.840]   if you look at these systems,
[00:37:29.840 --> 00:37:31.960]   you chat with them and they're immensely powerful
[00:37:31.960 --> 00:37:35.440]   and intelligent, but it's interesting to the extent
[00:37:35.440 --> 00:37:37.480]   of which they haven't like automated large sections
[00:37:37.480 --> 00:37:38.680]   of the economy yet.
[00:37:38.680 --> 00:37:40.640]   Whereas five years ago, I showed you Gemini,
[00:37:40.640 --> 00:37:42.400]   you'd be like, wow, this is like, you know,
[00:37:42.400 --> 00:37:43.640]   totally coming for a lot of things.
[00:37:43.640 --> 00:37:45.080]   So how do you account for that?
[00:37:45.080 --> 00:37:47.200]   Like what's going on where it hasn't had
[00:37:47.200 --> 00:37:48.040]   the broader impact yet?
[00:37:48.040 --> 00:37:49.400]   - Yeah, I think it's, we're still,
[00:37:49.400 --> 00:37:51.000]   I think that just shows we're still at the beginning
[00:37:51.000 --> 00:37:52.840]   of this new era.
[00:37:52.840 --> 00:37:53.680]   - Yeah.
[00:37:53.680 --> 00:37:55.120]   - And I think that for these systems,
[00:37:55.120 --> 00:37:56.880]   I think there are some interesting use cases,
[00:37:56.880 --> 00:38:00.400]   you know, where you can use things to some,
[00:38:00.400 --> 00:38:04.000]   you know, these chatbot systems to summarize stuff for you
[00:38:04.000 --> 00:38:06.200]   and maybe do some simple writing
[00:38:06.200 --> 00:38:09.960]   and maybe more kind of boilerplate type writing.
[00:38:09.960 --> 00:38:12.320]   But that's only a small part of what, you know,
[00:38:12.320 --> 00:38:13.680]   we all do every day.
[00:38:13.680 --> 00:38:16.720]   So I think for more general use cases,
[00:38:16.720 --> 00:38:19.400]   I think we still need new capabilities,
[00:38:19.400 --> 00:38:21.440]   things like planning and search,
[00:38:21.440 --> 00:38:23.840]   but also maybe things like personalization
[00:38:23.840 --> 00:38:26.040]   and memory, episodic memory.
[00:38:26.040 --> 00:38:27.480]   So not just long context windows,
[00:38:27.480 --> 00:38:28.960]   but actually remembering what I,
[00:38:28.960 --> 00:38:32.240]   what we'd spoke about a hundred conversations ago.
[00:38:32.240 --> 00:38:34.720]   And I think once those start coming in,
[00:38:34.720 --> 00:38:36.160]   I mean, I'm really looking forward to things
[00:38:36.160 --> 00:38:39.560]   like recommendation systems that help me find better,
[00:38:39.560 --> 00:38:40.760]   more enriching material,
[00:38:40.760 --> 00:38:43.480]   whether that's books or films or music and so on.
[00:38:43.480 --> 00:38:45.280]   You know, I would use that type of system every day.
[00:38:45.280 --> 00:38:47.440]   So I think we're just scratching the surface
[00:38:47.440 --> 00:38:52.440]   of what these AI say assistants could actually do
[00:38:52.440 --> 00:38:54.680]   for us in our general everyday lives.
[00:38:54.680 --> 00:38:56.920]   And also in our work context as well,
[00:38:56.920 --> 00:38:58.400]   I think they're not reliable yet enough
[00:38:58.400 --> 00:39:00.480]   to do things like science with them.
[00:39:00.480 --> 00:39:01.520]   But I think one day, you know,
[00:39:01.520 --> 00:39:04.520]   once we fix factuality and grounding and other things,
[00:39:04.520 --> 00:39:06.520]   I think they could end up becoming like, you know,
[00:39:06.520 --> 00:39:10.040]   the world's best research assistant for you as a scientist
[00:39:10.040 --> 00:39:12.880]   or as a clinician.
[00:39:12.880 --> 00:39:15.400]   - Hmm, I want to ask about memory, by the way.
[00:39:15.400 --> 00:39:17.280]   You had this fascinating paper in 2007,
[00:39:17.280 --> 00:39:18.680]   where you talk about the links
[00:39:18.680 --> 00:39:20.200]   between memory and imagination
[00:39:20.200 --> 00:39:22.480]   and how they, in some sense, are very similar.
[00:39:22.480 --> 00:39:26.600]   People often claim that these models are just memorizing.
[00:39:26.600 --> 00:39:29.320]   How do you think about that claim that people make?
[00:39:29.320 --> 00:39:30.920]   Is memorization all you need?
[00:39:30.920 --> 00:39:32.880]   'Cause in some deep sense, that's compression,
[00:39:32.880 --> 00:39:34.360]   or, you know, what's your intuition here?
[00:39:34.360 --> 00:39:35.720]   - Yeah, I mean, sort of at the limit,
[00:39:35.720 --> 00:39:37.720]   one maybe could try and memorize everything,
[00:39:37.720 --> 00:39:40.160]   but it wouldn't generalize out of your distribution.
[00:39:40.160 --> 00:39:41.680]   And I think these systems are clearly,
[00:39:41.680 --> 00:39:46.680]   I think the early criticisms of these early systems
[00:39:46.680 --> 00:39:49.920]   were that they were just regurgitating and memorizing.
[00:39:49.920 --> 00:39:51.600]   But I think, clearly, the new era,
[00:39:51.600 --> 00:39:53.280]   the Gemini, GPT-4 type era,
[00:39:53.280 --> 00:39:56.480]   they are definitely generalizing to new constructs.
[00:39:56.480 --> 00:40:00.560]   So, but actually, you know, in my thesis and that paper,
[00:40:00.560 --> 00:40:03.280]   particularly, that started that area of imagination
[00:40:03.280 --> 00:40:05.640]   in neuroscience was showing that, you know,
[00:40:05.640 --> 00:40:07.920]   first of all, memory, certainly, at least human memory,
[00:40:07.920 --> 00:40:09.200]   is a reconstructive process.
[00:40:09.200 --> 00:40:10.280]   It's not a videotape, right?
[00:40:10.280 --> 00:40:12.440]   We sort of put it together back from components
[00:40:12.440 --> 00:40:15.160]   that seems familiar to us, the ensemble.
[00:40:15.160 --> 00:40:16.840]   And that's what made me think that imagination
[00:40:16.840 --> 00:40:18.800]   might be the same thing, except in this case,
[00:40:18.800 --> 00:40:21.160]   you're using the same semantic components,
[00:40:21.160 --> 00:40:22.360]   but now you're putting it together
[00:40:22.360 --> 00:40:24.520]   into a way that your brain thinks is novel, right,
[00:40:24.520 --> 00:40:26.320]   for a particular purpose like planning.
[00:40:26.320 --> 00:40:30.120]   And so I do think that that kind of idea
[00:40:30.120 --> 00:40:32.720]   is still probably missing from our current systems,
[00:40:32.720 --> 00:40:36.200]   this sort of pulling together different parts
[00:40:36.200 --> 00:40:39.120]   of your world model to simulate something new
[00:40:39.120 --> 00:40:41.000]   that then helps with your planning,
[00:40:41.000 --> 00:40:43.080]   which is what I would call imagination.
[00:40:43.080 --> 00:40:43.920]   - Yeah, for sure.
[00:40:43.920 --> 00:40:47.360]   So again, now you guys have the best models in the world,
[00:40:47.360 --> 00:40:49.720]   you know, with the Gemini models.
[00:40:49.720 --> 00:40:51.960]   Do you have, do you plan on putting out
[00:40:51.960 --> 00:40:54.120]   some sort of framework like the other two major AI labs have
[00:40:54.120 --> 00:40:56.920]   of, you know, once we see these specific capabilities,
[00:40:56.920 --> 00:40:58.920]   unless we have these specific safeguards,
[00:40:58.920 --> 00:41:00.440]   we're not gonna continue development
[00:41:00.440 --> 00:41:02.640]   or we're not gonna ship the product out?
[00:41:02.640 --> 00:41:04.280]   - Yes, we have actually, I mean,
[00:41:04.280 --> 00:41:06.720]   we have already lots of internal checks and balances,
[00:41:06.720 --> 00:41:08.280]   but we're gonna start publishing,
[00:41:08.280 --> 00:41:10.120]   actually, you know, sort of watch this basis.
[00:41:10.120 --> 00:41:12.480]   We're working on a whole bunch of blog posts
[00:41:12.480 --> 00:41:15.240]   and technical papers that we'll be putting out
[00:41:15.240 --> 00:41:17.400]   in the next few months that, you know,
[00:41:17.400 --> 00:41:18.600]   along the similar lines of things
[00:41:18.600 --> 00:41:20.440]   like responsible scaling laws and so on.
[00:41:20.440 --> 00:41:23.120]   We have those implicitly internally
[00:41:23.120 --> 00:41:25.400]   and various safety councils and so on,
[00:41:25.400 --> 00:41:27.840]   people like Shane Chair and so on,
[00:41:27.840 --> 00:41:30.680]   but it's time for us to talk about that more publicly,
[00:41:30.680 --> 00:41:31.760]   I think, so we'll be doing that
[00:41:31.760 --> 00:41:33.120]   throughout the course of the year.
[00:41:33.120 --> 00:41:33.960]   - That's great to hear.
[00:41:33.960 --> 00:41:36.240]   And another thing I'm curious about is,
[00:41:36.240 --> 00:41:38.400]   so it's not only the risk of like, you know,
[00:41:38.400 --> 00:41:40.400]   the deployed model being something
[00:41:40.400 --> 00:41:41.680]   that people can use to do bad things,
[00:41:41.680 --> 00:41:45.720]   but also rogue actors, foreign agents, so forth,
[00:41:45.720 --> 00:41:46.720]   being able to steal the weights
[00:41:46.720 --> 00:41:48.880]   and then fine tune them to do crazy things.
[00:41:48.880 --> 00:41:51.640]   How do you think about securing the weights
[00:41:51.640 --> 00:41:53.760]   to make sure something like this doesn't happen,
[00:41:53.760 --> 00:41:56.000]   making sure a very like key group of people
[00:41:56.000 --> 00:41:57.160]   have access to them and so forth?
[00:41:57.160 --> 00:41:58.000]   - Yeah, it's interesting.
[00:41:58.000 --> 00:41:59.520]   So first of all, there's sort of two parts of this.
[00:41:59.520 --> 00:42:02.200]   One is security, one is open source, maybe we can discuss,
[00:42:02.200 --> 00:42:04.400]   but the security I think is super key,
[00:42:04.400 --> 00:42:08.120]   like just as sort of normal cybersecurity type things.
[00:42:08.120 --> 00:42:10.160]   And I think we're lucky at Google DeepMind,
[00:42:10.160 --> 00:42:13.560]   we're kind of behind Google's firewall and cloud protection,
[00:42:13.560 --> 00:42:15.360]   which is, you know, I think best, you know,
[00:42:15.360 --> 00:42:17.840]   best in class in the world, corporately.
[00:42:17.840 --> 00:42:19.320]   So we already have that protection.
[00:42:19.320 --> 00:42:20.160]   And then behind that,
[00:42:20.160 --> 00:42:25.160]   we have specific DeepMind protections within our code base.
[00:42:25.160 --> 00:42:27.400]   So it's sort of a double layer of protection.
[00:42:27.400 --> 00:42:29.200]   So I feel pretty good about that, that that's,
[00:42:29.200 --> 00:42:31.760]   I mean, you know, you can never be complacent on that,
[00:42:31.760 --> 00:42:34.800]   but I feel it's already sort of best in the world
[00:42:34.800 --> 00:42:37.640]   in terms of cyber defenses,
[00:42:37.640 --> 00:42:39.560]   but we've got to carry on improving that.
[00:42:39.560 --> 00:42:41.560]   And again, things like the hardened sandboxes
[00:42:41.560 --> 00:42:43.800]   could be a way of doing that as well.
[00:42:43.800 --> 00:42:46.560]   And maybe even there are, you know,
[00:42:46.560 --> 00:42:48.560]   specifically secure data centers
[00:42:48.560 --> 00:42:51.080]   or hardware solutions to this too, that we're thinking about.
[00:42:51.080 --> 00:42:53.560]   I think that maybe in the next three, four, five years,
[00:42:53.560 --> 00:42:56.880]   we would also want air gaps and various other things
[00:42:56.880 --> 00:42:59.000]   that are known in the security community.
[00:42:59.000 --> 00:42:59.840]   So I think that's key.
[00:42:59.840 --> 00:43:02.080]   And I think all frontier labs should be doing that
[00:43:02.080 --> 00:43:04.400]   because otherwise, you know, nation states and other things,
[00:43:04.400 --> 00:43:08.280]   rogue nation states and other dangerous actors,
[00:43:08.280 --> 00:43:10.000]   that there'd be obviously a lot of incentive
[00:43:10.000 --> 00:43:12.080]   for them to steal things like the weights.
[00:43:12.080 --> 00:43:14.640]   And then, you know, of course,
[00:43:14.640 --> 00:43:16.120]   open source is another interesting question,
[00:43:16.120 --> 00:43:18.200]   which is we're huge proponents of open source
[00:43:18.200 --> 00:43:19.040]   and open science.
[00:43:19.040 --> 00:43:20.560]   I mean, almost every, you know,
[00:43:20.560 --> 00:43:21.960]   we've published thousands of papers
[00:43:21.960 --> 00:43:24.960]   and things like AlphaFold and Transformers, of course,
[00:43:24.960 --> 00:43:27.000]   and AlphaGo, all of these things we put out there
[00:43:27.000 --> 00:43:30.920]   into the world, published and open source many of them,
[00:43:30.920 --> 00:43:33.760]   GraphCast most recently, our weather prediction system.
[00:43:33.760 --> 00:43:36.960]   But when it comes to, you know, the core technology,
[00:43:36.960 --> 00:43:40.280]   the foundational technology and very general purpose,
[00:43:40.280 --> 00:43:42.640]   I think the question I would have is,
[00:43:42.640 --> 00:43:45.880]   if, you know, for sort of open source proponents
[00:43:45.880 --> 00:43:49.960]   is that how does one stop bad actors,
[00:43:49.960 --> 00:43:53.240]   individuals or rogues, you know, up to rogue states,
[00:43:53.240 --> 00:43:55.560]   taking those same open source systems
[00:43:55.560 --> 00:43:58.280]   and repurposing them because their general purpose
[00:43:58.280 --> 00:44:00.040]   for harmful ends, right?
[00:44:00.040 --> 00:44:01.640]   So we have to answer that question.
[00:44:01.640 --> 00:44:02.480]   - Yeah, yeah.
[00:44:02.480 --> 00:44:04.480]   - And I haven't heard a compelling,
[00:44:04.480 --> 00:44:06.040]   I mean, I don't know what the answer is to that,
[00:44:06.040 --> 00:44:08.720]   but I haven't heard a compelling, clear answer to that
[00:44:08.720 --> 00:44:12.960]   from proponents of just sort of open sourcing everything.
[00:44:12.960 --> 00:44:15.120]   So I think there has to be some balance there,
[00:44:15.120 --> 00:44:17.160]   but, you know, obviously it's a complex question
[00:44:17.160 --> 00:44:18.280]   of to what that is.
[00:44:18.280 --> 00:44:19.120]   - Yeah, yeah.
[00:44:19.120 --> 00:44:20.480]   I feel like tech doesn't get the credit it deserves
[00:44:20.480 --> 00:44:22.000]   for like funding, you know,
[00:44:22.000 --> 00:44:23.800]   hundreds of billions of dollars worth of R&D.
[00:44:23.800 --> 00:44:24.640]   - Yeah.
[00:44:24.640 --> 00:44:25.920]   - But obviously we have deep bind with systems
[00:44:25.920 --> 00:44:26.760]   like AlphaFold and so on.
[00:44:26.760 --> 00:44:27.600]   - Yeah.
[00:44:27.600 --> 00:44:30.320]   - But when we talk about securing the weights,
[00:44:30.320 --> 00:44:31.800]   you know, as we said, like maybe right now
[00:44:31.800 --> 00:44:33.880]   it's not something that like is gonna cause
[00:44:33.880 --> 00:44:34.720]   the end of the world or anything,
[00:44:34.720 --> 00:44:36.080]   but as these systems get better and better,
[00:44:36.080 --> 00:44:38.040]   the worry that, yes, a foreign agent
[00:44:38.040 --> 00:44:39.680]   or something gets access to them.
[00:44:39.680 --> 00:44:41.520]   Presumably right now there's like dozens to hundreds
[00:44:41.520 --> 00:44:43.360]   of researchers who have access to the weights.
[00:44:43.360 --> 00:44:45.800]   How do you, what's a plan for like getting
[00:44:45.800 --> 00:44:46.720]   into like the situation,
[00:44:46.720 --> 00:44:47.880]   getting the weights in a situation rooms
[00:44:47.880 --> 00:44:49.480]   if you're like, if you need to access to them,
[00:44:49.480 --> 00:44:52.320]   it's like, you know, some extremely strenuous process.
[00:44:52.320 --> 00:44:54.480]   Nobody, individual can really take them out.
[00:44:54.480 --> 00:44:55.320]   - Yeah, yeah.
[00:44:55.320 --> 00:44:57.640]   I mean, one has to balance that with allowing
[00:44:57.640 --> 00:44:59.480]   for collaboration and speed of progress.
[00:44:59.480 --> 00:45:01.280]   Actually, another interesting thing is of course
[00:45:01.280 --> 00:45:04.800]   you want, you know, brilliant independent researchers
[00:45:04.800 --> 00:45:08.520]   from academia or things like the UK AI Safety Institute
[00:45:08.520 --> 00:45:13.520]   and US one to be able to kind of red team these systems.
[00:45:13.520 --> 00:45:17.000]   So one has to expose them to a certain extent,
[00:45:17.000 --> 00:45:18.640]   although that's not necessarily the weights.
[00:45:18.640 --> 00:45:21.400]   And then, you know, we have a lot of processes in place
[00:45:21.400 --> 00:45:25.160]   about making sure that, you know, only if you need them,
[00:45:25.160 --> 00:45:27.200]   that you have access to, you know,
[00:45:27.200 --> 00:45:29.560]   those people who need access have access.
[00:45:29.560 --> 00:45:33.160]   And right now, I think we're still in the early days
[00:45:33.160 --> 00:45:35.440]   of those kinds of systems being at risk.
[00:45:35.440 --> 00:45:37.640]   And as that, as these systems become more powerful
[00:45:37.640 --> 00:45:39.640]   and more general and more capable,
[00:45:39.640 --> 00:45:42.920]   I think one has to look at the access question.
[00:45:42.920 --> 00:45:44.440]   - So some of these other labs have specialized
[00:45:44.440 --> 00:45:46.440]   in different things relative to safety,
[00:45:46.440 --> 00:45:48.480]   like anthropic, for example, with interoperability.
[00:45:48.480 --> 00:45:51.520]   And do you have some sense of where you guys
[00:45:51.520 --> 00:45:53.560]   might have an edge where as so that, you know,
[00:45:53.560 --> 00:45:54.600]   now that you have the frontier model,
[00:45:54.600 --> 00:45:56.640]   you're going to scale up safety where you guys
[00:45:56.640 --> 00:45:58.760]   are going to be able to put out the best frontier research?
[00:45:58.760 --> 00:46:01.680]   - Yeah, I think, you know, well, we helped pioneer RLHF
[00:46:01.680 --> 00:46:03.160]   and other things like that, which can also be
[00:46:03.160 --> 00:46:06.200]   obviously useful performance, but also for safety.
[00:46:06.200 --> 00:46:10.440]   I think that, you know, a lot of the self-play ideas
[00:46:10.440 --> 00:46:13.000]   and these kinds of things could also be used potentially
[00:46:13.000 --> 00:46:18.000]   to auto test a lot of the boundary conditions
[00:46:18.200 --> 00:46:19.680]   that you have with the new systems.
[00:46:19.680 --> 00:46:22.120]   I mean, part of the issue is that with these sort
[00:46:22.120 --> 00:46:25.520]   of very general systems, there's so much surface area
[00:46:25.520 --> 00:46:28.360]   to cover like about how these systems behave.
[00:46:28.360 --> 00:46:31.960]   So I think we are going to need some automated testing.
[00:46:31.960 --> 00:46:34.880]   And again, with things like simulations
[00:46:34.880 --> 00:46:37.920]   and games environment, very realistic environments,
[00:46:37.920 --> 00:46:39.960]   virtual environments, I think we have a long history
[00:46:39.960 --> 00:46:42.680]   in that and using those kinds of systems
[00:46:42.680 --> 00:46:46.320]   and making use of them for building AI algorithms.
[00:46:46.320 --> 00:46:49.360]   So I think we can leverage all of that history.
[00:46:49.360 --> 00:46:51.920]   And then, you know, around at Google, we're very lucky.
[00:46:51.920 --> 00:46:54.880]   We have some of the world's best cybersecurity experts,
[00:46:54.880 --> 00:46:56.160]   hardware designers.
[00:46:56.160 --> 00:46:58.760]   So I think we can bring that to bear in, you know,
[00:46:58.760 --> 00:47:00.600]   for security and safety as well.
[00:47:00.600 --> 00:47:01.560]   - Great, great.
[00:47:01.560 --> 00:47:02.400]   Let's talk about Gemini.
[00:47:02.400 --> 00:47:03.240]   - Yeah.
[00:47:03.240 --> 00:47:04.560]   - So, you know, now you guys have the best model
[00:47:04.560 --> 00:47:05.400]   in the world.
[00:47:05.400 --> 00:47:09.200]   So I'm curious, you know, the default way to interact
[00:47:09.200 --> 00:47:12.240]   with these systems has been through chat so far.
[00:47:12.240 --> 00:47:14.560]   Now that we have multimodal and all these new capabilities,
[00:47:14.560 --> 00:47:15.800]   how do you anticipate that changing?
[00:47:15.800 --> 00:47:17.600]   Or do you think that'll still be the case?
[00:47:17.600 --> 00:47:19.000]   - Yeah, I think we're just at the beginning
[00:47:19.000 --> 00:47:23.560]   of actually understanding what a full multimodal model system,
[00:47:23.560 --> 00:47:25.840]   how exciting that might be to interact with.
[00:47:25.840 --> 00:47:28.480]   And it'll be quite different to, I think,
[00:47:28.480 --> 00:47:30.240]   what we're used to today with the chatbots.
[00:47:30.240 --> 00:47:34.000]   I think the next versions of this over the next year,
[00:47:34.000 --> 00:47:36.720]   18 months, you know, maybe we'll have some contextual
[00:47:36.720 --> 00:47:38.760]   understanding around the environment around you
[00:47:38.760 --> 00:47:41.640]   through a camera or whatever it is, a phone.
[00:47:41.640 --> 00:47:43.680]   You know, I could imagine that as the next awesome glasses
[00:47:43.680 --> 00:47:44.880]   or the next step.
[00:47:44.880 --> 00:47:48.920]   And then I think that we'll start becoming more fluid
[00:47:48.920 --> 00:47:52.240]   in understanding, oh, let's sample from a video.
[00:47:52.240 --> 00:47:53.840]   Let's use voice.
[00:47:53.840 --> 00:47:58.440]   Maybe even eventually things like touch and, you know,
[00:47:58.440 --> 00:48:00.880]   if you think about robotics and other things, you know,
[00:48:00.880 --> 00:48:02.680]   sensors, other types of sensors.
[00:48:02.680 --> 00:48:05.640]   So I think the world's about to become very exciting,
[00:48:05.640 --> 00:48:07.760]   I think, in the next few years as we start getting used
[00:48:07.760 --> 00:48:10.520]   to the idea of what true multimodality means.
[00:48:10.520 --> 00:48:14.520]   - On the robotic subject, Ilya said when he was
[00:48:14.520 --> 00:48:16.800]   in the podcast that the reason OpenAI gave up on robotics
[00:48:16.800 --> 00:48:19.120]   was because they didn't have enough data in that domain,
[00:48:19.120 --> 00:48:21.640]   at least at the time they were pursuing it.
[00:48:21.640 --> 00:48:22.880]   I mean, you guys have put out different things
[00:48:22.880 --> 00:48:25.120]   like RoboTransformer and other things.
[00:48:25.120 --> 00:48:26.600]   Do you think that's still a bottleneck
[00:48:26.600 --> 00:48:27.440]   for robotics progress?
[00:48:27.440 --> 00:48:29.640]   Or will we see progress in the world of atoms
[00:48:29.640 --> 00:48:30.480]   as well as the world of bits?
[00:48:30.480 --> 00:48:31.960]   - Yeah, well, we're very excited about our progress
[00:48:31.960 --> 00:48:35.120]   with things like Gatto and RT2, you know,
[00:48:35.120 --> 00:48:38.920]   Robotic Transformer, and we actually think,
[00:48:38.920 --> 00:48:41.440]   so we've always liked robotics and we've had, you know,
[00:48:41.440 --> 00:48:44.680]   amazing research in them, we still have that going now
[00:48:44.680 --> 00:48:47.960]   because we like the fact that it's a data-poor regime
[00:48:47.960 --> 00:48:50.720]   'cause that pushes us on some very interesting
[00:48:50.720 --> 00:48:52.320]   research directions that we think are gonna be useful
[00:48:52.320 --> 00:48:55.160]   anyway, like sampling efficiency and data efficiency
[00:48:55.160 --> 00:48:58.320]   in general and transfer learning, learning from simulation,
[00:48:58.320 --> 00:49:01.120]   transferring that to reality, all of these very, you know,
[00:49:01.120 --> 00:49:04.040]   sim to real, all of these very interesting,
[00:49:04.040 --> 00:49:06.880]   actually general challenges that we would like to solve.
[00:49:07.880 --> 00:49:11.640]   So the control problem, so we've always pushed hard on that.
[00:49:11.640 --> 00:49:14.720]   And actually I think, so Ilya's right,
[00:49:14.720 --> 00:49:17.520]   that is more challenging because of the data problem,
[00:49:17.520 --> 00:49:19.800]   but it's also, I think we're starting to see
[00:49:19.800 --> 00:49:24.040]   the beginnings of these large models being transferable
[00:49:24.040 --> 00:49:27.080]   to the robotics regime, learning in the general domain,
[00:49:27.080 --> 00:49:28.600]   language domain, and other things,
[00:49:28.600 --> 00:49:30.800]   and then just treating tokens like Gatto
[00:49:30.800 --> 00:49:32.440]   as any type of token.
[00:49:32.440 --> 00:49:34.360]   You know, the token could be an action, it could be a word,
[00:49:34.360 --> 00:49:37.400]   it could be a part of an image, a pixel, or whatever it is.
[00:49:37.400 --> 00:49:39.920]   And that's what I think true multimodality is.
[00:49:39.920 --> 00:49:42.800]   And to begin with, it's harder to train a system like that
[00:49:42.800 --> 00:49:46.600]   than a straightforward text language system.
[00:49:46.600 --> 00:49:49.960]   But actually, you know, going back to our early conversation
[00:49:49.960 --> 00:49:52.120]   of transfer learning, you start seeing
[00:49:52.120 --> 00:49:54.760]   that a true multimodal system,
[00:49:54.760 --> 00:49:58.280]   the other modalities benefit some different modalities.
[00:49:58.280 --> 00:50:01.000]   So you get better at language because you now understand
[00:50:01.000 --> 00:50:02.280]   a little bit about video.
[00:50:02.280 --> 00:50:05.680]   So I do think it's harder to get going,
[00:50:05.680 --> 00:50:08.600]   but actually, ultimately, we'll have a more general,
[00:50:08.600 --> 00:50:10.360]   more capable system like that.
[00:50:10.360 --> 00:50:11.480]   - Whatever happened to Gatto?
[00:50:11.480 --> 00:50:12.560]   Like, that was super fascinating
[00:50:12.560 --> 00:50:13.960]   that you could have, like, play games,
[00:50:13.960 --> 00:50:15.680]   and also do, like, video, and also do text.
[00:50:15.680 --> 00:50:18.480]   - Yeah, we're still working on those kinds of systems,
[00:50:18.480 --> 00:50:20.840]   but you can imagine we're just trying to,
[00:50:20.840 --> 00:50:22.200]   those ideas we're trying to build
[00:50:22.200 --> 00:50:25.160]   into our future generations of Gemini.
[00:50:25.160 --> 00:50:26.040]   - Oh, great. - You know, to be able
[00:50:26.040 --> 00:50:29.280]   to do all of those things, and robotics transformers,
[00:50:29.280 --> 00:50:30.960]   and, you know, things like that are kind of,
[00:50:30.960 --> 00:50:33.760]   you can think of them as sort of follow-ups to that.
[00:50:33.760 --> 00:50:36.000]   - Will we see asymmetric progress towards the domains
[00:50:36.000 --> 00:50:38.360]   in which the self-play kinds of things you're talking about
[00:50:38.360 --> 00:50:39.680]   will be especially powerful?
[00:50:39.680 --> 00:50:41.480]   So math and code, you know, obviously,
[00:50:41.480 --> 00:50:43.680]   recently you have these papers out about this,
[00:50:43.680 --> 00:50:45.600]   where, yeah, you can use these things
[00:50:45.600 --> 00:50:48.320]   to do really cool, novel things.
[00:50:48.320 --> 00:50:49.880]   Will they just be, like, superhuman coders,
[00:50:49.880 --> 00:50:50.720]   but, like, in other ways,
[00:50:50.720 --> 00:50:52.160]   they might be still worse than humans,
[00:50:52.160 --> 00:50:53.000]   or how do you think about that sort of--
[00:50:53.000 --> 00:50:56.000]   - Yeah, so look, I think that, you know,
[00:50:56.000 --> 00:50:57.560]   we're making great progress with math
[00:50:57.560 --> 00:51:00.840]   and things like theorem proving and coding,
[00:51:01.840 --> 00:51:03.880]   but it's still interesting, you know,
[00:51:03.880 --> 00:51:06.960]   if one looks at, I mean, creativity in general
[00:51:06.960 --> 00:51:08.760]   and scientific endeavor in general,
[00:51:08.760 --> 00:51:09.960]   I think we're getting to the stage
[00:51:09.960 --> 00:51:13.120]   where our systems could help the best human scientists
[00:51:13.120 --> 00:51:14.400]   make their breakthroughs quicker,
[00:51:14.400 --> 00:51:17.560]   like almost triage the search space in some ways,
[00:51:17.560 --> 00:51:19.600]   or perhaps find a solution like AlphaFold does
[00:51:19.600 --> 00:51:21.600]   with the protein structure.
[00:51:21.600 --> 00:51:24.120]   But it can't, they're not at the level
[00:51:24.120 --> 00:51:26.520]   where they can create a hypothesis themselves,
[00:51:26.520 --> 00:51:28.320]   or ask the right question.
[00:51:28.320 --> 00:51:30.640]   And as any top scientist will tell you,
[00:51:30.640 --> 00:51:32.360]   that's the hardest part of science,
[00:51:32.360 --> 00:51:34.640]   is actually asking the right question,
[00:51:34.640 --> 00:51:36.120]   boiling down that space to, like,
[00:51:36.120 --> 00:51:37.600]   what's the critical question
[00:51:37.600 --> 00:51:39.320]   we should go after, the critical problem,
[00:51:39.320 --> 00:51:40.800]   and then formulating that problem
[00:51:40.800 --> 00:51:42.320]   in the right way to attack it.
[00:51:42.320 --> 00:51:44.960]   And that's not something our systems,
[00:51:44.960 --> 00:51:46.480]   well, we have really have any idea
[00:51:46.480 --> 00:51:49.800]   how our systems could do, but they can,
[00:51:49.800 --> 00:51:52.160]   they are suitable for searching
[00:51:52.160 --> 00:51:53.640]   large combinatorial spaces,
[00:51:53.640 --> 00:51:56.040]   if one can specify the problem in that way
[00:51:56.040 --> 00:51:57.760]   with a clear objective function.
[00:51:57.760 --> 00:51:59.800]   So that's very useful for already
[00:51:59.800 --> 00:52:01.680]   many of the problems we deal with today,
[00:52:01.680 --> 00:52:04.360]   but not the most high-level creative problems.
[00:52:04.360 --> 00:52:08.280]   - So DeepMind obviously has published
[00:52:08.280 --> 00:52:09.280]   all kinds of interesting stuff,
[00:52:09.280 --> 00:52:12.320]   and speeding up science in different areas.
[00:52:12.320 --> 00:52:14.000]   How do you think about that in the context of,
[00:52:14.000 --> 00:52:15.480]   if you think AGI is gonna happen
[00:52:15.480 --> 00:52:17.160]   in the next 10, 20 years,
[00:52:17.160 --> 00:52:19.480]   why not just wait for the AGI to do it for you?
[00:52:19.480 --> 00:52:21.120]   Why build these domain-specific solutions?
[00:52:21.120 --> 00:52:24.400]   - Yeah, well, I think we don't know
[00:52:24.400 --> 00:52:26.160]   how long AGI is going to be.
[00:52:26.160 --> 00:52:28.160]   And we always used to say,
[00:52:28.160 --> 00:52:29.560]   back even when we started DeepMind,
[00:52:29.560 --> 00:52:33.720]   that we don't have to wait for AGI
[00:52:33.720 --> 00:52:35.960]   in order to bring incredible benefits
[00:52:35.960 --> 00:52:37.560]   to the world.
[00:52:37.560 --> 00:52:41.280]   And especially, my personal passion
[00:52:41.280 --> 00:52:44.400]   has been AI for science and health.
[00:52:44.400 --> 00:52:46.800]   And you can see that with things like AlphaFold
[00:52:46.800 --> 00:52:48.600]   and all of our various nature papers
[00:52:48.600 --> 00:52:49.440]   of different domains,
[00:52:49.440 --> 00:52:50.960]   our material science work and so on.
[00:52:50.960 --> 00:52:52.960]   I think there's lots of exciting directions.
[00:52:52.960 --> 00:52:55.320]   And also impact in the world through products too,
[00:52:55.320 --> 00:52:57.960]   I think is very exciting and a huge opportunity,
[00:52:57.960 --> 00:53:00.200]   a unique opportunity we have as part of Google
[00:53:00.200 --> 00:53:05.200]   of they got dozens of billion user products
[00:53:05.200 --> 00:53:09.440]   that we can immediately ship our advances into,
[00:53:09.440 --> 00:53:14.440]   and then billions of people can improve their daily lives
[00:53:14.440 --> 00:53:16.200]   and enriches their daily lives
[00:53:16.200 --> 00:53:17.400]   and enhances their daily lives.
[00:53:17.400 --> 00:53:20.120]   So I think it's a fantastic opportunity
[00:53:20.120 --> 00:53:21.880]   for impact on all those fronts.
[00:53:21.880 --> 00:53:23.080]   And I think the other reason,
[00:53:23.080 --> 00:53:25.880]   from a point of view of AGI specifically,
[00:53:25.880 --> 00:53:29.440]   is that it battle tests your ideas, right?
[00:53:29.440 --> 00:53:32.440]   So you don't wanna be in a sort of research bunker
[00:53:32.440 --> 00:53:35.080]   where you just, theoretically are pushing things,
[00:53:35.080 --> 00:53:35.920]   some things forward,
[00:53:35.920 --> 00:53:39.200]   but then actually your internal metrics start deviating
[00:53:39.200 --> 00:53:43.440]   from real world things that people would care about, right?
[00:53:43.440 --> 00:53:45.000]   Or real world impact.
[00:53:45.000 --> 00:53:46.680]   So you get a lot of feedback,
[00:53:46.680 --> 00:53:49.520]   direct feedback from these real world applications
[00:53:49.520 --> 00:53:52.200]   that then tells you whether your systems really are scaling
[00:53:52.200 --> 00:53:56.040]   or actually do we need to be more data efficient
[00:53:56.040 --> 00:53:56.880]   or sample efficient
[00:53:56.880 --> 00:54:00.720]   because most real world challenges require that, right?
[00:54:00.720 --> 00:54:04.520]   And so it kind of keeps you honest and pushes you,
[00:54:04.520 --> 00:54:07.760]   keeps sort of nudging and steering your research directions
[00:54:07.760 --> 00:54:09.600]   to make sure they're on the right path.
[00:54:09.600 --> 00:54:11.040]   So I think it's fantastic.
[00:54:11.040 --> 00:54:12.880]   And of course the world benefits from that,
[00:54:12.880 --> 00:54:15.040]   society benefits from that on the way,
[00:54:15.040 --> 00:54:18.200]   many, many, maybe many, many years before AGI arrives.
[00:54:18.200 --> 00:54:19.040]   - Yeah.
[00:54:19.040 --> 00:54:21.160]   Well, the development of Gemini is super interesting
[00:54:21.160 --> 00:54:22.480]   'cause it comes right at the heels
[00:54:22.480 --> 00:54:25.080]   of merging these different organizations,
[00:54:25.080 --> 00:54:26.840]   Brain and DeepMind.
[00:54:26.840 --> 00:54:28.520]   Yeah, I'm curious, what have been the challenges there?
[00:54:28.520 --> 00:54:30.360]   What have been the synergies?
[00:54:30.360 --> 00:54:31.560]   And it's been successful in the sense
[00:54:31.560 --> 00:54:32.640]   that you have the best model in the world now.
[00:54:32.640 --> 00:54:33.480]   What's that been like?
[00:54:33.480 --> 00:54:36.160]   - Well, look, it's been fantastic actually over the last year
[00:54:36.160 --> 00:54:37.480]   'cause it's been challenging to do that,
[00:54:37.480 --> 00:54:40.520]   like any big integration coming together.
[00:54:40.520 --> 00:54:43.680]   But you're talking about two world-class organizations,
[00:54:43.680 --> 00:54:48.680]   long storied histories of inventing many important things
[00:54:48.680 --> 00:54:50.840]   from deep reinforcement learning to transformers.
[00:54:50.840 --> 00:54:52.120]   And so it's very exciting
[00:54:52.120 --> 00:54:54.040]   actually pooling all of that together
[00:54:54.040 --> 00:54:56.000]   and collaborating much more closely.
[00:54:56.000 --> 00:54:57.440]   We always used to be collaborating,
[00:54:57.440 --> 00:55:02.080]   but more on a sort of project by project basis
[00:55:02.080 --> 00:55:05.200]   versus a much deeper, broader collaboration
[00:55:05.200 --> 00:55:06.040]   like we have now.
[00:55:06.040 --> 00:55:10.360]   And Gemini is the first fruit of that collaboration,
[00:55:10.360 --> 00:55:13.320]   including the name Gemini, actually, implying twins.
[00:55:13.320 --> 00:55:15.320]   And of course, a lot of other things
[00:55:15.320 --> 00:55:16.240]   are made more efficient,
[00:55:16.240 --> 00:55:18.280]   like pooling compute resources together
[00:55:18.280 --> 00:55:20.040]   and ideas and engineering,
[00:55:20.040 --> 00:55:22.120]   which I think at the stage we're at now
[00:55:22.120 --> 00:55:24.960]   where there's huge amounts of world-class engineering
[00:55:24.960 --> 00:55:27.800]   that has to go on to build the frontier systems,
[00:55:27.800 --> 00:55:30.800]   I think it makes sense to coordinate that more closely.
[00:55:30.800 --> 00:55:33.760]   - Yeah, so I mean, you and Shane started DeepMind
[00:55:33.760 --> 00:55:36.160]   partly because you were concerned about safety.
[00:55:36.160 --> 00:55:38.760]   You saw AGI coming as like a live possibility.
[00:55:38.760 --> 00:55:42.080]   Do you think the people who were formerly part of Brain,
[00:55:42.080 --> 00:55:43.360]   the half of Google DeepMind now,
[00:55:43.360 --> 00:55:45.200]   do you think they approach it in the same way?
[00:55:45.200 --> 00:55:46.520]   Have there been cultural differences there
[00:55:46.520 --> 00:55:47.360]   in terms of that question?
[00:55:47.360 --> 00:55:48.560]   - Yeah, no, I think overall,
[00:55:48.560 --> 00:55:51.000]   and this is why I think one of the reasons
[00:55:51.000 --> 00:55:53.120]   we joined Forces We Google back in 2014
[00:55:53.120 --> 00:55:56.240]   was I think the entirety of Google and Alphabet,
[00:55:56.240 --> 00:55:57.400]   not just Brain and DeepMind,
[00:55:57.400 --> 00:56:00.320]   take these questions very seriously of responsibility.
[00:56:00.320 --> 00:56:03.160]   And our kind of mantra is to try and be bold
[00:56:03.160 --> 00:56:04.640]   and responsible with these systems.
[00:56:04.640 --> 00:56:06.680]   So I would class it as,
[00:56:06.680 --> 00:56:08.680]   I'm obviously a huge techno-optimist,
[00:56:08.680 --> 00:56:10.960]   but I want us to be cautious with that
[00:56:10.960 --> 00:56:12.520]   given the transformative power
[00:56:12.520 --> 00:56:16.040]   of what we're bringing into the world collectively.
[00:56:16.040 --> 00:56:18.200]   And I think it's important,
[00:56:18.200 --> 00:56:21.160]   I think it's gonna be one of the most important technologies
[00:56:21.160 --> 00:56:22.560]   humanity will ever invent.
[00:56:22.560 --> 00:56:25.880]   So we've got to put all our efforts into getting this right
[00:56:25.880 --> 00:56:28.760]   and to be thoughtful and sort of also humble
[00:56:28.760 --> 00:56:30.360]   about what we know and don't know
[00:56:30.360 --> 00:56:32.640]   about the systems that are coming
[00:56:32.640 --> 00:56:34.160]   and the uncertainties around that.
[00:56:34.160 --> 00:56:36.840]   And in my view, the only sensible approach
[00:56:36.840 --> 00:56:38.720]   when you have huge uncertainty
[00:56:38.720 --> 00:56:40.880]   is to be sort of cautiously optimistic
[00:56:40.880 --> 00:56:42.200]   and use the scientific method
[00:56:42.200 --> 00:56:44.760]   to try and have as much foresight and understanding
[00:56:44.760 --> 00:56:46.080]   about what's coming down the line
[00:56:46.080 --> 00:56:48.440]   and the consequences of that before it happens.
[00:56:48.440 --> 00:56:51.400]   You don't wanna be live A/B testing out in the world
[00:56:51.400 --> 00:56:53.000]   with these very consequential systems
[00:56:53.000 --> 00:56:56.560]   because unintended consequences may be quite severe.
[00:56:56.560 --> 00:57:00.800]   So I want us to move away as a field
[00:57:00.800 --> 00:57:02.880]   from a sort of move fast and break things attitude,
[00:57:02.880 --> 00:57:05.560]   which has maybe served the Valley very well in the past
[00:57:05.560 --> 00:57:08.760]   and obviously created important innovations.
[00:57:08.760 --> 00:57:11.560]   But I think in this case,
[00:57:11.560 --> 00:57:15.680]   we wanna be bold with the positive things that it can do
[00:57:15.680 --> 00:57:18.680]   and make sure we realize things like medicine and science
[00:57:18.680 --> 00:57:20.560]   and advancing all of those things
[00:57:20.560 --> 00:57:23.600]   whilst being responsible and thoughtful
[00:57:23.600 --> 00:57:28.000]   with as far as possible with mitigating the risks.
[00:57:28.000 --> 00:57:28.840]   - Yeah, yeah.
[00:57:28.840 --> 00:57:29.800]   And that's why it seems like
[00:57:29.800 --> 00:57:31.240]   the responsible scaling process
[00:57:31.240 --> 00:57:33.480]   is something that is a very good empirical way
[00:57:33.480 --> 00:57:34.960]   to pre-commit to these kinds of things.
[00:57:34.960 --> 00:57:35.920]   - Yes, exactly.
[00:57:35.920 --> 00:57:37.360]   - Yeah, and I'm curious if you have a sense of like,
[00:57:37.360 --> 00:57:39.080]   for example, when you're doing these evaluations,
[00:57:39.080 --> 00:57:42.280]   if it turns out your next model could help a lay person
[00:57:42.280 --> 00:57:45.120]   build a pandemic classical bioweapon or something,
[00:57:45.120 --> 00:57:46.520]   how you would think, first of all,
[00:57:46.520 --> 00:57:48.800]   of making sure those weights are secure
[00:57:48.800 --> 00:57:50.120]   so that that doesn't get out.
[00:57:50.120 --> 00:57:52.200]   And second, what would have to be true
[00:57:52.200 --> 00:57:54.120]   for you to be comfortable deploying that system?
[00:57:54.120 --> 00:57:55.560]   How comfortable, like how would you make sure
[00:57:55.560 --> 00:57:58.320]   that that latent capability isn't exposed?
[00:57:58.320 --> 00:58:01.160]   - Yeah, well, first, I mean, the secure model part,
[00:58:01.160 --> 00:58:03.160]   I think we've covered with the cyber security
[00:58:03.160 --> 00:58:04.680]   and make sure that's well-classed
[00:58:04.680 --> 00:58:06.400]   and you're monitoring all those things.
[00:58:06.400 --> 00:58:10.280]   I think if a capability was discovered like that
[00:58:10.280 --> 00:58:12.520]   through red teaming or external testing
[00:58:12.520 --> 00:58:17.160]   by government institutes or academia or whatever,
[00:58:17.160 --> 00:58:21.440]   independent testers, then we would have to fix that loophole
[00:58:21.440 --> 00:58:23.200]   depending on what it was, right?
[00:58:23.200 --> 00:58:28.200]   If that required more different kind of perhaps constitution
[00:58:28.200 --> 00:58:32.240]   or different guardrails or more RLHF to avoid that
[00:58:32.240 --> 00:58:34.800]   or removing some training data.
[00:58:34.800 --> 00:58:36.080]   I mean, depending on what the problem is,
[00:58:36.080 --> 00:58:38.920]   I think there could be a number of mitigations.
[00:58:38.920 --> 00:58:41.320]   And so the first part is making sure
[00:58:41.320 --> 00:58:42.920]   you detect it ahead of time.
[00:58:42.920 --> 00:58:44.960]   So that's about the right evaluations
[00:58:44.960 --> 00:58:47.440]   and right benchmarking and right testing.
[00:58:47.440 --> 00:58:50.000]   And then the question is how one would fix that
[00:58:50.000 --> 00:58:52.040]   before you deployed it.
[00:58:52.040 --> 00:58:53.320]   But I think it would need to be fixed
[00:58:53.320 --> 00:58:55.040]   before it was deployed generally, for sure,
[00:58:55.040 --> 00:58:57.240]   if that was an exposure surface.
[00:58:57.240 --> 00:58:58.360]   - Right, right.
[00:58:58.360 --> 00:59:01.840]   Final question, you've been thinking
[00:59:01.840 --> 00:59:03.320]   in terms of like the end goal of AGI
[00:59:03.320 --> 00:59:05.000]   at a time when other people thought it was ridiculous
[00:59:05.000 --> 00:59:08.440]   in 2010, now that we're seeing this like slow takeoff
[00:59:08.440 --> 00:59:10.320]   where we're actually seeing these like generalization
[00:59:10.320 --> 00:59:13.720]   and intelligence, what is like the psychologically
[00:59:13.720 --> 00:59:15.320]   seeing this, what has that been like?
[00:59:15.320 --> 00:59:17.240]   Have you just like sort of priced into a world model
[00:59:17.240 --> 00:59:18.720]   so you like, it's not new news for you
[00:59:18.720 --> 00:59:20.280]   or is it like actually just seeing it live?
[00:59:20.280 --> 00:59:22.160]   You're like, wow, like this is something
[00:59:22.160 --> 00:59:24.040]   that really changed or what does it feel like?
[00:59:24.040 --> 00:59:27.160]   - Yeah, well, for me, yes, it's already priced
[00:59:27.160 --> 00:59:28.880]   into my world model of how things were gonna go
[00:59:28.880 --> 00:59:30.400]   at least from the technology side.
[00:59:30.400 --> 00:59:34.680]   But obviously I didn't, we didn't necessarily anticipate
[00:59:34.680 --> 00:59:37.640]   the general public would be that interested this early
[00:59:37.640 --> 00:59:39.560]   in the sequence, right, of things.
[00:59:39.560 --> 00:59:42.720]   Like maybe one could think of if we were to produce more,
[00:59:42.720 --> 00:59:46.800]   if say like a chat GPT and chatbots hadn't got the,
[00:59:46.800 --> 00:59:48.880]   kind of got the interest they'd ended up getting,
[00:59:48.880 --> 00:59:50.400]   which I think was quite surprising to everyone
[00:59:50.400 --> 00:59:52.800]   that people were ready to use these things,
[00:59:52.800 --> 00:59:55.200]   even though they were lacking in certain directions, right?
[00:59:55.200 --> 00:59:57.440]   Impressive though they are.
[00:59:57.440 --> 00:59:59.560]   Then we would have produced more specialized systems,
[00:59:59.560 --> 01:00:01.400]   I think, built off of the main track,
[01:00:01.400 --> 01:00:04.800]   like AlphaFolds and AlphaGoes and so on
[01:00:04.800 --> 01:00:06.120]   and our scientific work.
[01:00:06.120 --> 01:00:09.960]   And then I think the general public
[01:00:09.960 --> 01:00:13.440]   maybe would have only paid attention later down the road
[01:00:13.440 --> 01:00:14.920]   where in a few years time where we have
[01:00:14.920 --> 01:00:17.760]   more generally useful assistant type systems.
[01:00:17.760 --> 01:00:19.040]   So that's been interesting.
[01:00:19.040 --> 01:00:21.680]   So that's created a different type of environment
[01:00:21.680 --> 01:00:25.480]   that we're now all operating in as a field.
[01:00:25.480 --> 01:00:27.560]   So, and it's a little bit more chaotic
[01:00:27.560 --> 01:00:29.680]   because there's so many more things going on
[01:00:29.680 --> 01:00:31.960]   and there's so much VC money going into it
[01:00:31.960 --> 01:00:35.000]   and everyone's sort of almost losing their minds over it,
[01:00:35.000 --> 01:00:35.840]   I think.
[01:00:35.840 --> 01:00:38.240]   And what I just, the thing that I worry about
[01:00:38.240 --> 01:00:40.480]   is I wanna make sure that as a field,
[01:00:40.480 --> 01:00:42.960]   we act responsibly and thoughtfully
[01:00:42.960 --> 01:00:44.840]   and scientifically about this
[01:00:44.840 --> 01:00:47.800]   and use the scientific method to approach this in a,
[01:00:47.800 --> 01:00:50.720]   as I said, an optimistic, but careful way.
[01:00:50.720 --> 01:00:51.560]   And I think that's the,
[01:00:51.560 --> 01:00:53.600]   I've always believed that's the right approach
[01:00:53.600 --> 01:00:55.560]   for something like AI.
[01:00:55.560 --> 01:00:59.400]   And I just hope that doesn't get lost in this huge rush.
[01:00:59.400 --> 01:01:00.240]   - Sure, sure.
[01:01:00.240 --> 01:01:01.640]   Well, I think that's a great place to close.
[01:01:01.640 --> 01:01:02.480]   Devin, so much, thanks to you.
[01:01:02.480 --> 01:01:03.320]   Thank you so much for your time
[01:01:03.320 --> 01:01:04.240]   and for coming on the podcast.
[01:01:04.240 --> 01:01:05.880]   - Thanks, it's been a real pleasure.
[01:01:05.880 --> 01:01:08.520]   - Hey everybody.
[01:01:08.520 --> 01:01:10.400]   I hope you enjoyed that episode.
[01:01:10.400 --> 01:01:12.600]   As always, the most helpful thing you can do
[01:01:12.600 --> 01:01:14.400]   is to share the podcast.
[01:01:14.400 --> 01:01:16.120]   Send it to people you think might enjoy it,
[01:01:16.120 --> 01:01:18.240]   put it in Twitter, your group chats, et cetera.
[01:01:18.240 --> 01:01:20.120]   Just splits the world.
[01:01:20.120 --> 01:01:21.360]   Appreciate you listening.
[01:01:21.360 --> 01:01:22.480]   I'll see you next time.
[01:01:22.480 --> 01:01:23.320]   Cheers.
[01:01:23.840 --> 01:01:26.440]   (upbeat music)
[01:01:26.440 --> 01:01:29.040]   (upbeat music)
[01:01:29.040 --> 01:01:33.040]   [Music]

