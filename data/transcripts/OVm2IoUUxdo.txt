
[00:00:00.000 --> 00:00:05.280]   That is the deep question I want to address today.
[00:00:05.280 --> 00:00:08.700]   How does chat GPT work?
[00:00:08.700 --> 00:00:19.160]   And how worried should we be about it?
[00:00:19.160 --> 00:00:25.720]   I'm Cal Newport, and this is Deep Questions, the show about living and working deeply in
[00:00:25.720 --> 00:00:27.600]   an increasingly distracted world.
[00:00:27.600 --> 00:00:37.440]   I'm here in my Deep Work HQ, joined once again by my producer, Jesse.
[00:00:37.440 --> 00:00:43.360]   So Jesse, you may have noticed that we have been receiving a lot of emails in the last
[00:00:43.360 --> 00:00:46.120]   few months about chat GPT.
[00:00:46.120 --> 00:00:47.240]   Yeah.
[00:00:47.240 --> 00:00:49.080]   And related AI technologies.
[00:00:49.080 --> 00:00:51.880]   And our listeners want to know my thoughts on this, right?
[00:00:51.880 --> 00:00:52.880]   I'm a computer scientist.
[00:00:52.880 --> 00:00:57.360]   I've thought about the intersection of technology and society, and I've been silent about it.
[00:00:57.360 --> 00:01:01.040]   Well, I can reveal the reason why I've been silent about it is that I've been working
[00:01:01.040 --> 00:01:06.480]   on a big article for the New Yorker about exactly this technology, how it works and
[00:01:06.480 --> 00:01:08.520]   its implications for the world.
[00:01:08.520 --> 00:01:13.160]   And my general rule is when I'm writing an article, I don't talk about that subject publicly
[00:01:13.160 --> 00:01:15.760]   until the article is done.
[00:01:15.760 --> 00:01:19.960]   I mean, that's basic journalistic practice, but actually, Jesse, I've never told this
[00:01:19.960 --> 00:01:27.320]   story, but that rule was really ingrained in me when I was in college.
[00:01:27.320 --> 00:01:30.120]   So when I was coming up as a young writer, you know, I got started pretty early, wrote
[00:01:30.120 --> 00:01:31.960]   my first book in college.
[00:01:31.960 --> 00:01:34.720]   I was commissioned to write something for the New York Times.
[00:01:34.720 --> 00:01:38.840]   I don't remember exactly what it was, maybe an op-ed, something to do with college students
[00:01:38.840 --> 00:01:39.840]   or something like this.
[00:01:39.840 --> 00:01:43.680]   And I had an early blog at that time.
[00:01:43.680 --> 00:01:47.640]   And I wrote something on the blog like, "Hey, isn't this exciting?
[00:01:47.640 --> 00:01:49.040]   I'm going to write an article for the New York Times."
[00:01:49.040 --> 00:01:52.080]   And maybe like I put the short email on there and it was like, "Yeah, we'd love to have
[00:01:52.080 --> 00:01:53.760]   you write the piece or something."
[00:01:53.760 --> 00:01:55.400]   And that editor went ballistic.
[00:01:55.400 --> 00:01:56.400]   Really?
[00:01:56.400 --> 00:01:57.400]   Yeah.
[00:01:57.400 --> 00:01:58.400]   Cancelled the piece.
[00:01:58.400 --> 00:01:59.400]   Cancelled the piece?
[00:01:59.400 --> 00:02:00.400]   Chewed me out.
[00:02:00.400 --> 00:02:02.320]   Now, this was early internet, right?
[00:02:02.320 --> 00:02:05.800]   I mean, this was 2004 probably.
[00:02:05.800 --> 00:02:07.100]   So I don't know.
[00:02:07.100 --> 00:02:11.120]   Maybe it was more, it felt more like a breach then, but ever since then, if I'm writing
[00:02:11.120 --> 00:02:12.120]   an article.
[00:02:12.120 --> 00:02:13.760]   Did you ever talk to that editor again?
[00:02:13.760 --> 00:02:14.760]   No.
[00:02:14.760 --> 00:02:20.240]   I ended up writing a lot for the Times, but not really until 2012.
[00:02:20.240 --> 00:02:22.040]   Was that, you're going to be your first big splash?
[00:02:22.040 --> 00:02:24.080]   That would have been my first big splash.
[00:02:24.080 --> 00:02:26.040]   Were you like depressed for a couple of days?
[00:02:26.040 --> 00:02:27.520]   A little shook up.
[00:02:27.520 --> 00:02:30.720]   And then starting with So Good They Can't Ignore You and going forward, I had a really
[00:02:30.720 --> 00:02:32.960]   good relationship with the Times, especially through digital minimalism.
[00:02:32.960 --> 00:02:37.040]   I've written tons of articles for them, but there is a lesson learned.
[00:02:37.040 --> 00:02:42.880]   So I've written now, the day that we're recording this podcast, April 13th, my new article,
[00:02:42.880 --> 00:02:43.880]   The New Yorker has been published.
[00:02:43.880 --> 00:02:45.000]   So I am free.
[00:02:45.000 --> 00:02:51.320]   The gag order has been lifted and we can get into it when it comes to chat GPT.
[00:02:51.320 --> 00:02:54.600]   In fact, I'll even load the article up here on the screen.
[00:02:54.600 --> 00:02:58.280]   For those who are watching, you will see this on the screen.
[00:02:58.280 --> 00:03:03.520]   If you're not watching, you can watch at youtube.com/calnewportmedia.
[00:03:03.520 --> 00:03:05.360]   Look for episode 244.
[00:03:05.360 --> 00:03:08.520]   You can also find that at the deeplife.com episode 244.
[00:03:08.520 --> 00:03:11.200]   Here Jesse is the long awaited article.
[00:03:11.200 --> 00:03:15.240]   The title is What Kind of Mind Does Chat GPT Have?
[00:03:15.240 --> 00:03:19.360]   The subhead is large language model seems startlingly intelligent, but what's really
[00:03:19.360 --> 00:03:22.720]   happening under the hood?
[00:03:22.720 --> 00:03:23.720]   It's a big long article.
[00:03:23.720 --> 00:03:24.720]   So it's good.
[00:03:24.720 --> 00:03:26.000]   I'm excited to read it.
[00:03:26.000 --> 00:03:27.160]   Yes, we can talk chat GPT.
[00:03:27.160 --> 00:03:31.760]   I mean, you probably haven't been following it too closely just based on our conversation.
[00:03:31.760 --> 00:03:35.800]   Some people really in the weeds and some people don't want to know.
[00:03:35.800 --> 00:03:39.400]   I'm guessing you're not in the weeds on chat GPT, but I could be wrong.
[00:03:39.400 --> 00:03:40.880]   No, I'm not in the weeds at all.
[00:03:40.880 --> 00:03:45.120]   I listened to like a few of the hard fork episodes on it.
[00:03:45.120 --> 00:03:46.360]   That was about it.
[00:03:46.360 --> 00:03:49.040]   And what was the tone of those episodes?
[00:03:49.040 --> 00:03:52.200]   They were given some examples of what it was when it first came out.
[00:03:52.200 --> 00:03:54.480]   I probably listened to them like six weeks ago.
[00:03:54.480 --> 00:03:55.480]   Yeah.
[00:03:55.480 --> 00:03:58.320]   And then, yeah, that was kind of it.
[00:03:58.320 --> 00:04:02.240]   Well, so I'll give a quick primer then before we get into the guts of what I want to talk
[00:04:02.240 --> 00:04:03.240]   about today.
[00:04:03.240 --> 00:04:05.640]   So chat GPT is a chat bot.
[00:04:05.640 --> 00:04:10.320]   You can sign up for an account at open AI and it's a web interface.
[00:04:10.320 --> 00:04:15.080]   You type in questions and chat GPT or prompts or requests and chat GPT response types text
[00:04:15.080 --> 00:04:18.280]   back like you're talking to someone over Slack or instant messenger.
[00:04:18.280 --> 00:04:23.160]   So this was released in November, late November of last year, and almost immediately people
[00:04:23.160 --> 00:04:30.280]   began circulating online screenshots of particularly impressive interactions or particularly funny
[00:04:30.280 --> 00:04:33.000]   interactions that they had with chat GPT.
[00:04:33.000 --> 00:04:34.960]   Here's one of the first ones to go viral.
[00:04:34.960 --> 00:04:38.640]   I talk about this one in my article.
[00:04:38.640 --> 00:04:41.700]   So here's a tweet of a screenshot that went along.
[00:04:41.700 --> 00:04:48.880]   This was from a software developer named Thomas Pacek and he asked chat GPT the following,
[00:04:48.880 --> 00:04:54.060]   write a biblical verse in the style of the King James Bible explaining how to remove
[00:04:54.060 --> 00:04:57.360]   a peanut butter sandwich from a VCR.
[00:04:57.360 --> 00:05:03.840]   Chat GPT rose to the challenge and wrote a response that begins, and it came to pass
[00:05:03.840 --> 00:05:07.880]   that a man was troubled by a peanut butter sandwich for had been placed within his VCR
[00:05:07.880 --> 00:05:09.640]   and he knew not how to remove it.
[00:05:09.640 --> 00:05:13.460]   And he cried out to the Lord saying, Oh Lord, how can I remove this sandwich from my VCR
[00:05:13.460 --> 00:05:16.000]   for it is stuck fast and will not budge.
[00:05:16.000 --> 00:05:17.860]   And the response goes on.
[00:05:17.860 --> 00:05:24.780]   Here's another early viral example of chat GPT's prowess.
[00:05:24.780 --> 00:05:33.340]   This was a someone named Riley Goodside who asked chat GPT to write a sign failed scene
[00:05:33.340 --> 00:05:38.000]   in which Jerry needs to learn the bubble sort algorithm.
[00:05:38.000 --> 00:05:41.320]   And chat GPT once again rose to the occasion.
[00:05:41.320 --> 00:05:46.760]   A not a properly formatted script, but has some of the aspects of it.
[00:05:46.760 --> 00:05:50.160]   It opens in a monk's cafe.
[00:05:50.160 --> 00:05:53.120]   It says Jerry is sitting at the counter with George.
[00:05:53.120 --> 00:05:57.200]   Jerry sighs and says, I can't believe I have to learn to bubble sort algorithm for my computer
[00:05:57.200 --> 00:05:58.640]   science class.
[00:05:58.640 --> 00:06:00.480]   George laughs, bubble sort.
[00:06:00.480 --> 00:06:02.180]   That's the most basic sorting algorithm there is.
[00:06:02.180 --> 00:06:03.560]   Even a monkey could do it.
[00:06:03.560 --> 00:06:04.560]   Audience laughs.
[00:06:04.560 --> 00:06:05.560]   Jerry.
[00:06:05.560 --> 00:06:06.560]   Yeah, well, I'm not a monkey.
[00:06:06.560 --> 00:06:07.880]   I'm a comedian.
[00:06:07.880 --> 00:06:10.360]   And then the screen, the scene goes on from there.
[00:06:10.360 --> 00:06:11.360]   All right.
[00:06:11.360 --> 00:06:12.680]   So this is the type of thing chat GPT can do.
[00:06:12.680 --> 00:06:18.240]   These impressively perceptive answers to pretty esoteric requests.
[00:06:18.240 --> 00:06:25.040]   Now, if you go back and actually watch the media cycle around chat GPT, which I have
[00:06:25.040 --> 00:06:27.160]   to say is driven very strongly by Twitter.
[00:06:27.160 --> 00:06:32.600]   I think the fact that anyone can sign up for an account and that screenshots of your interactions
[00:06:32.600 --> 00:06:38.560]   can be easily embedded in the Twitter really helped get the hype cycle around this technology
[00:06:38.560 --> 00:06:43.160]   spinning much more furiously than it has for past artificial intelligence innovations.
[00:06:43.160 --> 00:06:48.120]   Anyways, if you go back and look at this media cycle, it took a week or two before the tone
[00:06:48.120 --> 00:06:53.280]   shifted from exuberance and humor.
[00:06:53.280 --> 00:06:59.920]   Like if you look at this example I just gave about Seinfeld, the tweet says, actually not
[00:06:59.920 --> 00:07:01.840]   that one I meant of the VCR.
[00:07:01.840 --> 00:07:06.400]   The tweet says, I'm sorry, I simply cannot be cynical about technology that can accomplish
[00:07:06.400 --> 00:07:07.400]   this.
[00:07:07.400 --> 00:07:10.320]   So it went from this sort of exuberance and happiness to something that was a little bit
[00:07:10.320 --> 00:07:11.320]   more distressing.
[00:07:11.320 --> 00:07:14.440]   Here's a couple of examples I want to bring up here.
[00:07:14.440 --> 00:07:17.480]   Here is an article from NBC news.
[00:07:17.480 --> 00:07:24.280]   The headline is chat GPT passes MBA exam given by a Wharton professor.
[00:07:24.280 --> 00:07:27.800]   Oh, that got people worried.
[00:07:27.800 --> 00:07:33.280]   Here is a another article from around this period from time magazine headline.
[00:07:33.280 --> 00:07:36.920]   He used AI to publish a children's book in a weekend.
[00:07:36.920 --> 00:07:38.900]   Artists are not happy about it.
[00:07:38.900 --> 00:07:43.680]   It details a product design manager who used chat GPT to write all the texts of a book,
[00:07:43.680 --> 00:07:50.280]   which he then self published on Amazon and started selling a bit of a stunt, but it implied
[00:07:50.280 --> 00:07:55.320]   certain types of future scenarios in which this technology was taking away creative work
[00:07:55.320 --> 00:07:58.380]   that really made people unsettled.
[00:07:58.380 --> 00:08:01.720]   As we get closer to the current period, I would say the tone shifted since the new year
[00:08:01.720 --> 00:08:08.040]   and in particular coming into March and April, the tone shifted towards one of alarm, not
[00:08:08.040 --> 00:08:13.840]   just about the focused economic impacts that are possible with this type of technology,
[00:08:13.840 --> 00:08:20.680]   but some of the bigger societal, if not civilization level impacts of these type of technologies.
[00:08:20.680 --> 00:08:27.760]   I would say one article that really helped set this tone was this now somewhat infamous
[00:08:27.760 --> 00:08:34.120]   Kevin Roos piece from the New York Times that is titled a conversation with Bing's chat
[00:08:34.120 --> 00:08:38.040]   bot left me deeply unsettled.
[00:08:38.040 --> 00:08:43.640]   Bing released a chat bot after chat GPT based on a very similar underlying technology.
[00:08:43.640 --> 00:08:49.120]   Kevin Roos was, I guess, beta testing or using this new tool and fell into this really sort
[00:08:49.120 --> 00:08:53.600]   of dark conversation with the chat bot where among other things, the chat bot tried to
[00:08:53.600 --> 00:08:56.000]   convince Kevin to divorce his wife.
[00:08:56.000 --> 00:09:01.320]   The chat bot revealed that she had a sort of hidden double identity.
[00:09:01.320 --> 00:09:05.680]   I think that identity was called venom, which was a very sort of dark personality.
[00:09:05.680 --> 00:09:08.540]   So Kevin set a tone of, Ooh, I'm a little bit worried.
[00:09:08.540 --> 00:09:10.840]   And it escalated from there.
[00:09:10.840 --> 00:09:16.040]   In late March, we get this op ed in the New York Times.
[00:09:16.040 --> 00:09:22.240]   This is March 24th, written by some prominent authors, Yuval Harari, Tristan Harris, and
[00:09:22.240 --> 00:09:24.520]   Azza Raskin.
[00:09:24.520 --> 00:09:30.000]   And they really in this article are starting to point out potential existential threats
[00:09:30.000 --> 00:09:31.520]   of these AIs.
[00:09:31.520 --> 00:09:36.960]   They are arguing strongly for, we need to take a break and step back from developing
[00:09:36.960 --> 00:09:39.760]   these AIs before it becomes too late.
[00:09:39.760 --> 00:09:41.760]   Here's their last paragraph.
[00:09:41.760 --> 00:09:43.860]   We have summoned an alien intelligence.
[00:09:43.860 --> 00:09:48.120]   We don't know much about it, except that it is extremely powerful and offers us bedazzling
[00:09:48.120 --> 00:09:52.680]   gifts but could also hack the foundations of our civilization.
[00:09:52.680 --> 00:10:00.880]   We call upon world leaders to respond to this moment at the level of challenge it presents.
[00:10:00.880 --> 00:10:04.680]   The first step is to buy us time to upgrade our 19th century institutions for an AI world
[00:10:04.680 --> 00:10:09.000]   and to learn to master AI before it masters us.
[00:10:09.000 --> 00:10:13.820]   A few days after this op ed, an open letter circulated signed by many prominent individuals
[00:10:13.820 --> 00:10:16.980]   demanding exactly this type of pause on AI research.
[00:10:16.980 --> 00:10:19.860]   Okay, so this is the setup.
[00:10:19.860 --> 00:10:22.200]   Chat GPT is released.
[00:10:22.200 --> 00:10:23.200]   Everyone's using it.
[00:10:23.200 --> 00:10:24.740]   Everyone's posting stuff on Twitter.
[00:10:24.740 --> 00:10:25.740]   Everyone's having fun.
[00:10:25.740 --> 00:10:28.580]   Then people start to get worried about, wait a second, what if we use it for X, what if
[00:10:28.580 --> 00:10:29.580]   we use it for Y?
[00:10:29.580 --> 00:10:31.260]   And then people got downright unsettled.
[00:10:31.260 --> 00:10:35.120]   Wait a second, what if we've unleashed an alien intelligence and we have to worry about
[00:10:35.120 --> 00:10:37.820]   it mastering us?
[00:10:37.820 --> 00:10:39.220]   We have to stop this before it's too late.
[00:10:39.220 --> 00:10:46.480]   So it really is a phenomenal arc and this all unfolded in about five months.
[00:10:46.480 --> 00:10:52.080]   So what I want to do is try to shed some clarity on the situation.
[00:10:52.080 --> 00:10:54.800]   The theme of my New Yorker piece, and I'm going to load it on the screen and actually
[00:10:54.800 --> 00:10:59.140]   read to you the main opening paragraph here.
[00:10:59.140 --> 00:11:04.740]   The theme of my New Yorker piece is we need to understand this technology.
[00:11:04.740 --> 00:11:12.860]   We cannot just keep treating it like a black box and then just imagining what these black
[00:11:12.860 --> 00:11:17.300]   boxes might do and then freak ourselves out about these stories we tell ourselves about
[00:11:17.300 --> 00:11:19.540]   things that maybe these black boxes could do.
[00:11:19.540 --> 00:11:27.540]   This is too important for us to just trust or imagine or make up or guess at how these
[00:11:27.540 --> 00:11:28.540]   things function.
[00:11:28.540 --> 00:11:32.660]   So here's my, the, the nut graph of my, my New Yorker piece.
[00:11:32.660 --> 00:11:35.280]   What kinds of new minds are being released into our world?
[00:11:35.280 --> 00:11:38.900]   The response to chat GPT and to the other chat bots that have followed in its wake has
[00:11:38.900 --> 00:11:43.760]   often suggested that they are powerful, sophisticated, imaginative, and possibly even dangerous.
[00:11:43.760 --> 00:11:45.800]   But is that really true?
[00:11:45.800 --> 00:11:50.180]   If we treat these new artificial intelligence tools as mysterious black boxes, it's impossible
[00:11:50.180 --> 00:11:52.160]   to say.
[00:11:52.160 --> 00:11:56.180]   Only by taking the time to investigate how this technology actually works from its high
[00:11:56.180 --> 00:12:01.600]   level concepts down to its basic digital wiring, can we understand what we're dealing with.
[00:12:01.600 --> 00:12:06.600]   We send messages into the electronic void and receive surprising replies, but what exactly
[00:12:06.600 --> 00:12:09.360]   is writing back?
[00:12:09.360 --> 00:12:13.240]   That is the deep question I want to address today.
[00:12:13.240 --> 00:12:16.660]   How does chat GPT work?
[00:12:16.660 --> 00:12:19.240]   And how worried should we be about it?
[00:12:19.240 --> 00:12:21.720]   And I don't think we can answer that second question until we answer the first.
[00:12:21.720 --> 00:12:23.080]   So that's what we're gonna do.
[00:12:23.080 --> 00:12:28.680]   We're gonna take a deep dive on the basic ideas behind how a chat bot like chat GPT
[00:12:28.680 --> 00:12:29.680]   does what it does.
[00:12:30.160 --> 00:12:33.880]   We'll then use that to draw some more confident conclusions about how worried we should be.
[00:12:33.880 --> 00:12:38.340]   I then have a group of questions from you about AI that I've been holding on to as I've
[00:12:38.340 --> 00:12:39.340]   been working on this article.
[00:12:39.340 --> 00:12:43.540]   So we'll do some AI questions and then the end of the show, we'll shift gears and focus
[00:12:43.540 --> 00:12:45.040]   on something interesting.
[00:12:45.040 --> 00:12:47.720]   So an unrelated interesting story that was arrived in my inbox.
[00:12:47.720 --> 00:12:49.360]   All right, so let's get into it.
[00:12:49.360 --> 00:12:51.000]   I want to get into how this actually works.
[00:12:51.000 --> 00:12:53.960]   I drew some pictures, Jesse, be warned.
[00:12:53.960 --> 00:12:57.800]   I am not a talented graphic designer.
[00:12:57.800 --> 00:12:58.800]   That's not true.
[00:12:58.800 --> 00:13:00.200]   I'm not much of an artist.
[00:13:00.200 --> 00:13:03.600]   So Jesse watched me hand drawing some of these earlier on the tablet.
[00:13:03.600 --> 00:13:11.160]   I got to say this ain't exactly Chiat Gay, the famous ad agency level work here, but
[00:13:11.160 --> 00:13:12.160]   you know what?
[00:13:12.160 --> 00:13:13.160]   It's going to get the job done.
[00:13:13.160 --> 00:13:15.360]   So I have five ideas here I'm going to go through.
[00:13:15.360 --> 00:13:21.520]   And my goal is to implant into the high level ideas that explain how a computer program
[00:13:21.520 --> 00:13:26.160]   can possibly answer with such sophisticated nuance.
[00:13:26.160 --> 00:13:30.400]   These weird questions we're asking it how it can do the Bible verse about the VCR, how
[00:13:30.400 --> 00:13:33.640]   it can do a Seinfeld scene with bubble sort.
[00:13:33.640 --> 00:13:34.800]   And we're going to do this at the high level.
[00:13:34.800 --> 00:13:39.880]   We're going to essentially create a hypothetical program from scratch that is able to solve
[00:13:39.880 --> 00:13:40.880]   this.
[00:13:40.880 --> 00:13:43.360]   And then at the very end, I'll talk about how these big ideas I'm going to these five ideas
[00:13:43.360 --> 00:13:44.360]   I'm going to present.
[00:13:44.360 --> 00:13:47.040]   We'll talk about how that's actually implemented on real computers, but we'll do that real
[00:13:47.040 --> 00:13:48.360]   fast.
[00:13:48.360 --> 00:13:50.440]   That's kind of a red herring.
[00:13:50.440 --> 00:13:53.560]   The neural networks and transformer blocks and multi headed attention.
[00:13:53.560 --> 00:13:55.120]   We'll get there, but we'll do that very fast.
[00:13:55.120 --> 00:13:57.680]   That's the big conceptual ideas that I care about.
[00:13:57.680 --> 00:13:58.680]   All right.
[00:13:58.680 --> 00:14:04.160]   I need idea number one about how these type of programs work is word guessing.
[00:14:04.160 --> 00:14:07.720]   Now, I got to warn you, this is very visual.
[00:14:07.720 --> 00:14:09.400]   Everything I'm talking about now is on the screen.
[00:14:09.400 --> 00:14:13.520]   So if you're a listener, I really would recommend going to YouTube dot com slash Cal Newport
[00:14:13.520 --> 00:14:17.320]   media and going to episode two forty four.
[00:14:17.320 --> 00:14:20.840]   And if you don't like YouTube, go to the deep life dot com and go to episode two forty four
[00:14:20.840 --> 00:14:21.840]   because it's very visual.
[00:14:21.840 --> 00:14:22.840]   What I'm going to do here.
[00:14:22.840 --> 00:14:23.840]   All right.
[00:14:23.840 --> 00:14:27.200]   So what we see on the screen here for idea number one is word guessing.
[00:14:27.200 --> 00:14:32.180]   And I have a green box on the screen that represents the LLM or large language model
[00:14:32.180 --> 00:14:36.720]   that would underpin a chat bot like chat GPT.
[00:14:36.720 --> 00:14:38.860]   So what happens is.
[00:14:38.860 --> 00:14:45.480]   If you put a incomplete bit of text into this box, an example here is I have the partial
[00:14:45.480 --> 00:14:50.340]   sentence fragment, the quick brown fox jumped.
[00:14:50.340 --> 00:14:56.560]   The whole goal of this large language model is to spit out what single next word should
[00:14:56.560 --> 00:14:58.320]   follow.
[00:14:58.320 --> 00:15:03.560]   So in this case, if we give it the quick brown fox jumped as input in our example, the language
[00:15:03.560 --> 00:15:07.840]   model has spit out over.
[00:15:07.840 --> 00:15:10.240]   This is the word I'm guessing should come next.
[00:15:10.240 --> 00:15:11.500]   All right.
[00:15:11.500 --> 00:15:16.640]   So then what we would do is add over to add the word to our sentence.
[00:15:16.640 --> 00:15:19.720]   So now our sentence reads the quick brown fox jumped over.
[00:15:19.720 --> 00:15:20.960]   So we've added the output.
[00:15:20.960 --> 00:15:24.140]   We've expanded our sentence by a single word.
[00:15:24.140 --> 00:15:27.080]   We run that into the large language model and it spits out a guess for what the next
[00:15:27.080 --> 00:15:28.080]   word should be.
[00:15:28.080 --> 00:15:29.080]   So in this case is the.
[00:15:29.080 --> 00:15:31.320]   And then we would now expand our sentence.
[00:15:31.320 --> 00:15:33.720]   The quick brown fox jumped over the.
[00:15:33.720 --> 00:15:36.880]   We put that as input into the model.
[00:15:36.880 --> 00:15:39.800]   It would spit out the next word.
[00:15:39.800 --> 00:15:47.320]   This approach, which is known as auto regressive text generation, is what the models underneath
[00:15:47.320 --> 00:15:51.520]   all of these new generation chatbots like chat GPT actually use.
[00:15:51.520 --> 00:15:54.000]   They guess one word at a time.
[00:15:54.000 --> 00:15:58.040]   That word is added to the text and the newly expanded text is put through the model to
[00:15:58.040 --> 00:15:59.040]   get the next word.
[00:15:59.040 --> 00:16:01.360]   So it just generates one word at a time.
[00:16:01.360 --> 00:16:06.820]   So if you type in a request to something like chat GPT, that request plus a special symbol
[00:16:06.820 --> 00:16:12.120]   that means, OK, this is the end of the request and where the answer begins, that is input.
[00:16:12.120 --> 00:16:14.960]   And it'll spit out the first word of its response.
[00:16:14.960 --> 00:16:19.360]   It'll then pass the request plus the first word of its response into the model to get
[00:16:19.360 --> 00:16:21.040]   the second word of the response.
[00:16:21.040 --> 00:16:24.400]   It'll then add that on and pass the request plus the first two words of its response into
[00:16:24.400 --> 00:16:25.480]   the model to get the third word.
[00:16:25.480 --> 00:16:30.360]   So this is generating text one word at a time.
[00:16:30.360 --> 00:16:33.060]   It just slowly grows what's generating.
[00:16:33.060 --> 00:16:34.960]   There's no recurrency in here.
[00:16:34.960 --> 00:16:37.740]   It doesn't remember anything about the last word it generated.
[00:16:37.740 --> 00:16:39.760]   Its definition doesn't change.
[00:16:39.760 --> 00:16:43.680]   The green box, my diagram never changes once it's trained.
[00:16:43.680 --> 00:16:47.920]   Text goes in, it spits out a guess for the next word to add to what's being generated.
[00:16:47.920 --> 00:16:50.440]   All right.
[00:16:50.440 --> 00:16:53.600]   Idea number two, relevant word matching.
[00:16:53.600 --> 00:16:58.280]   So how does it figure out how do these large language models figure out what word to spit
[00:16:58.280 --> 00:16:59.280]   out next?
[00:16:59.280 --> 00:17:04.640]   Well, at its core, what's really happening here, and I'm simplifying, but at its core,
[00:17:04.640 --> 00:17:11.320]   what's really happening here is the model is just looking at the most relevant words
[00:17:11.320 --> 00:17:13.200]   from the input.
[00:17:13.200 --> 00:17:20.360]   It is then going to match those relevant words to actual text that has been given.
[00:17:20.360 --> 00:17:22.140]   We call these source text in my article.
[00:17:22.140 --> 00:17:26.640]   So examples of real text, it can match the relevant words to where they show up in real
[00:17:26.640 --> 00:17:30.720]   text and say, what follows these relevant words in real text?
[00:17:30.720 --> 00:17:33.840]   And that's how it figures out what it wants to output.
[00:17:33.840 --> 00:17:37.560]   So in this example, the most relevant words are just the most recent words.
[00:17:37.560 --> 00:17:43.960]   So if the input into our box is the quick brown fox jumped over, perhaps the model is
[00:17:43.960 --> 00:17:46.800]   only going to look at the last three words, fox jumped over.
[00:17:46.800 --> 00:17:50.600]   Then it has this big collection over here to the side of real text that real humans
[00:17:50.600 --> 00:17:53.080]   wrote, all these examples.
[00:17:53.080 --> 00:17:57.040]   And it's going to look in there and it's going to say, okay, have we seen something like
[00:17:57.040 --> 00:17:59.600]   fox jumped over show up in one of our input texts?
[00:17:59.600 --> 00:18:04.200]   And okay, here's an input text that says, as the old saying goes, the quick brown fox
[00:18:04.200 --> 00:18:05.720]   jumped over the lazy brown dog.
[00:18:05.720 --> 00:18:07.240]   So it looks, fox jumped over.
[00:18:07.240 --> 00:18:08.240]   Here it is.
[00:18:08.240 --> 00:18:09.240]   We found it in one of the source texts.
[00:18:09.240 --> 00:18:11.360]   What came after the words fox jumped over?
[00:18:11.360 --> 00:18:14.640]   The great, let's make the what we guess.
[00:18:14.640 --> 00:18:20.760]   Now, of course, in a, in a real industrial strength, large language model, the relevant
[00:18:20.760 --> 00:18:23.740]   words aren't just necessarily the most recent words.
[00:18:23.740 --> 00:18:29.120]   There's a whole complicated system called self-attention in which this, the, the model
[00:18:29.120 --> 00:18:33.200]   will actually learn what type, which words to emphasize as the most relevant words.
[00:18:33.200 --> 00:18:34.720]   But that's too complicated for this discussion.
[00:18:34.720 --> 00:18:39.520]   The key thing is, is just looking at some words from the text, effectively finding similar
[00:18:39.520 --> 00:18:43.960]   words in real texts that it was provided and saying what happened in those real texts.
[00:18:43.960 --> 00:18:46.720]   And that's what it figures out, how it figures out what to produce next.
[00:18:46.720 --> 00:18:47.720]   All right.
[00:18:47.720 --> 00:18:54.280]   This brings us to idea number three, which is voting.
[00:18:54.280 --> 00:18:57.000]   So the way I just presented it before it was, you know, Hey, just start looking through
[00:18:57.000 --> 00:19:00.560]   your source text till you find the relevant words, see what follows output it.
[00:19:00.560 --> 00:19:01.840]   That's not actually what happens.
[00:19:01.960 --> 00:19:04.880]   We want to be a little bit more probabilistic.
[00:19:04.880 --> 00:19:10.520]   So what I would say a closer way of describing what happens is we can imagine that our large
[00:19:10.520 --> 00:19:19.240]   language model is going to look for every instance, every instance of the relevant words
[00:19:19.240 --> 00:19:21.260]   that we're looking for.
[00:19:21.260 --> 00:19:24.400]   And it's going to see what follows those instances and keep track of it.
[00:19:24.400 --> 00:19:30.280]   What are all the different words that follow in this example, fox jumped over.
[00:19:30.280 --> 00:19:33.920]   And every time it finds an example of fox jumped over, it says, what word follows next?
[00:19:33.920 --> 00:19:36.200]   Let's give a vote for that word.
[00:19:36.200 --> 00:19:40.440]   And so if the same word follows in most of the examples, it's going to get most of the
[00:19:40.440 --> 00:19:41.440]   votes.
[00:19:41.440 --> 00:19:44.120]   Now I'm using votes here sort of as a metaphor.
[00:19:44.120 --> 00:19:47.720]   What we're really doing here is trying to build a normalized probability distribution.
[00:19:47.720 --> 00:19:50.680]   So in the end, what we're going to get, what the, what the large language model is going
[00:19:50.680 --> 00:19:59.180]   to produce is for every possible next word, it is going to produce a probability.
[00:19:59.180 --> 00:20:01.880]   What is the probability that this should be the next word that follows?
[00:20:01.880 --> 00:20:05.920]   But again, you can just think about this as votes, which word received the most votes,
[00:20:05.920 --> 00:20:09.480]   which word received the second most votes, how many votes this word received compared
[00:20:09.480 --> 00:20:10.480]   to that word.
[00:20:10.480 --> 00:20:13.240]   And we're just going to normalize those is really what's happening.
[00:20:13.240 --> 00:20:15.820]   But you just think about it as votes.
[00:20:15.820 --> 00:20:20.320]   So in this example, we see the phrase, the quick brown fox jumped over the lazy brown
[00:20:20.320 --> 00:20:21.320]   dogs.
[00:20:21.320 --> 00:20:23.480]   I mean, that shows up in a bunch of different sources.
[00:20:23.480 --> 00:20:25.000]   So the word the gets a lot of votes.
[00:20:25.000 --> 00:20:27.680]   So it has sort of a high percentage here.
[00:20:27.680 --> 00:20:30.160]   But maybe there's similar phrases.
[00:20:30.160 --> 00:20:35.740]   Like look at this example here, the cat jumped over a surprised owner.
[00:20:35.740 --> 00:20:41.200]   Cat jumped over is not the same as fox jumped over because cat is different than fox.
[00:20:41.200 --> 00:20:44.600]   But in this voting scheme, we can say, you know what, cat jumped over is similar to what
[00:20:44.600 --> 00:20:47.360]   we're looking for, which is fox jumped over.
[00:20:47.360 --> 00:20:48.920]   So what word follows that?
[00:20:48.920 --> 00:20:52.880]   The word a, well, we'll give that a small vote.
[00:20:52.880 --> 00:20:56.960]   And so now what we're able to do is not only find every instance of the word, the relevant
[00:20:56.960 --> 00:21:00.420]   words that we're looking for and generate votes for what follows, we can also start
[00:21:00.420 --> 00:21:02.720]   generating weaker votes for similar phrases.
[00:21:02.720 --> 00:21:08.520]   And in the end, we just get this giant collection of every possible word and a pile of votes
[00:21:08.520 --> 00:21:11.160]   for each.
[00:21:11.160 --> 00:21:15.120]   And what the system will do is now randomly select a word.
[00:21:15.120 --> 00:21:17.640]   So that's why I have a picture.
[00:21:17.640 --> 00:21:22.600]   It's just, you would admit expertly drawn picture of a three dimensional dice.
[00:21:22.600 --> 00:21:23.600]   That's pretty good.
[00:21:23.600 --> 00:21:24.600]   Honest question.
[00:21:24.600 --> 00:21:25.920]   Did you know that was a dice before?
[00:21:25.920 --> 00:21:26.920]   Oh, yeah.
[00:21:26.920 --> 00:21:27.920]   Look at that guy's 3d rendering.
[00:21:27.920 --> 00:21:28.920]   Yeah.
[00:21:28.920 --> 00:21:33.480]   So for those who are listening, I have a picture of a dice to indicate randomness.
[00:21:33.480 --> 00:21:36.780]   It'll then randomly select which word to come next and it'll weigh that selection based
[00:21:36.780 --> 00:21:38.640]   on the votes.
[00:21:38.640 --> 00:21:42.800]   So if in this case, the has most of the votes, it almost certainly that's the, the word it's
[00:21:42.800 --> 00:21:45.340]   going to choose to output next, but look, a has some votes.
[00:21:45.340 --> 00:21:49.560]   So it's possible that it'll select a, it's just not as likely the word apple has zero
[00:21:49.560 --> 00:21:54.280]   votes, 0% probability because it never shows up after the phrase, anything similar to fox
[00:21:54.280 --> 00:21:55.360]   jumped over.
[00:21:55.360 --> 00:21:56.920]   No phrase similar to that is ever followed by apple.
[00:21:56.920 --> 00:21:59.320]   So there's no chance it'll select it and, and so on.
[00:21:59.320 --> 00:22:04.360]   And actually in these systems, the output is a vote or a percentage like this for every
[00:22:04.360 --> 00:22:08.200]   possible, they call them tokens, but word that could follow next.
[00:22:08.200 --> 00:22:14.080]   Here's a quiz, Jesse in the model on which chat GPT is based, how many different words
[00:22:14.080 --> 00:22:15.080]   do you think it knows?
[00:22:15.080 --> 00:22:19.920]   So in other words, when it, when it has to generate, okay, here's a pile of votes for
[00:22:19.920 --> 00:22:25.000]   every possible next word, how many words or punctuations are a billion?
[00:22:25.000 --> 00:22:28.440]   No, it's 50,000, 50,000.
[00:22:28.440 --> 00:22:31.440]   So it has a vocabulary of 50,000.
[00:22:31.440 --> 00:22:34.360]   It's not all words, but basically it knows like tens of thousands of words.
[00:22:34.360 --> 00:22:37.920]   And how big is like the biggest dictionary?
[00:22:37.920 --> 00:22:38.920]   That's a good question.
[00:22:38.920 --> 00:22:39.920]   Yeah.
[00:22:39.920 --> 00:22:43.040]   I don't think it's as, it probably, there's probably a lot of esoteric words.
[00:22:43.040 --> 00:22:44.040]   Yeah.
[00:22:44.040 --> 00:22:49.440]   Because the thing is it has some like vectors describing all these words, follow it along
[00:22:49.440 --> 00:22:50.440]   throughout the system.
[00:22:50.440 --> 00:22:53.200]   So it really does affect the size, how big, so it's like you want to have a big enough
[00:22:53.200 --> 00:22:57.480]   vocabulary to talk about a lot of things, but not so big that it really inflates the
[00:22:57.480 --> 00:22:58.480]   system.
[00:22:58.480 --> 00:22:59.480]   Right.
[00:22:59.480 --> 00:23:02.280]   So voting is just my euphemism for probability, but this is what's, what's happening.
[00:23:02.280 --> 00:23:08.720]   So now we have a bunch of source text and we imagine that for our relevant words, we're
[00:23:08.720 --> 00:23:12.560]   just finding all the places in these source texts where relevant words show up or similar
[00:23:12.560 --> 00:23:15.440]   relevant words show up and see what follows it.
[00:23:15.440 --> 00:23:18.600]   And in all cases, generate votes for what follows it, use those votes to select what
[00:23:18.600 --> 00:23:19.600]   comes next.
[00:23:19.600 --> 00:23:20.600]   All right.
[00:23:20.600 --> 00:23:31.800]   So this brings us to idea four, idea one through three can generate very believable text.
[00:23:31.800 --> 00:23:37.740]   This is well-known in natural language processing systems that do more or less what I just described.
[00:23:37.740 --> 00:23:44.380]   If you give it enough source text and have it look at a big enough window of relevant
[00:23:44.380 --> 00:23:48.300]   words and then just have it spit out word by word in the way we just described that
[00:23:48.300 --> 00:23:53.960]   auto regressive approach, this will spit out very believable text.
[00:23:53.960 --> 00:23:55.560]   It's actually not even that hard to implement.
[00:23:55.560 --> 00:24:01.560]   In my New Yorker article, I point towards a simple Python program I found online.
[00:24:01.560 --> 00:24:08.200]   It was a couple hundred lines of code that used Mary Shelley's Frankenstein as its input
[00:24:08.200 --> 00:24:09.200]   text.
[00:24:09.200 --> 00:24:12.280]   It looked at the last four words in the sentence being generated.
[00:24:12.280 --> 00:24:14.000]   That's what it uses to relevant words.
[00:24:14.000 --> 00:24:18.700]   And I showed in the article, this thing generated very good Gothic text.
[00:24:18.700 --> 00:24:19.980]   Right.
[00:24:19.980 --> 00:24:23.360]   So that's how you generate believable text with a program.
[00:24:23.360 --> 00:24:28.900]   And notice nothing we've done so far has anything to do with understanding the concepts the
[00:24:28.900 --> 00:24:31.720]   program is talking about.
[00:24:31.720 --> 00:24:37.400]   All of the intelligence, the grammar, the subtleties, all of that that we see so far
[00:24:37.400 --> 00:24:42.740]   is just being extracted from the human text that were pushed as input and then remixed
[00:24:42.740 --> 00:24:45.140]   and matched and copied and manipulated into the output.
[00:24:45.140 --> 00:24:51.340]   But the program is just looking for words, gathering votes, selecting, outputting blindly
[00:24:51.340 --> 00:24:52.340]   again and again and again.
[00:24:52.340 --> 00:24:54.180]   The program is actually simple.
[00:24:54.180 --> 00:24:58.180]   The intelligence you see in an answer is all coming at this point from the input text themselves.
[00:24:58.180 --> 00:24:59.180]   All right.
[00:24:59.180 --> 00:25:01.300]   But we've only solved half the problem.
[00:25:01.300 --> 00:25:06.300]   If we want a chat bot, we can't just have our program generate believable text.
[00:25:06.300 --> 00:25:11.720]   The text have to actually answer the question being asked by the user.
[00:25:11.720 --> 00:25:17.520]   So how do we aim this automatic text generation mechanism towards specific types of answers
[00:25:17.520 --> 00:25:19.740]   that match what the user is asking?
[00:25:19.740 --> 00:25:25.420]   Well, this brings in the notion of feature detection, which is the fourth out of the
[00:25:25.420 --> 00:25:28.300]   five total ideas I want to go over today.
[00:25:28.300 --> 00:25:32.300]   So what happens with feature detection is a response.
[00:25:32.300 --> 00:25:38.260]   We have a request and perhaps the answer that follows the request is being input into our
[00:25:38.260 --> 00:25:39.780]   large language model.
[00:25:39.780 --> 00:25:45.100]   So I've shown here a request that says write instructions for removing a peanut butter
[00:25:45.100 --> 00:25:46.100]   sandwich from a VCR.
[00:25:46.100 --> 00:25:50.280]   Then I have a bunch of colons and I have the beginning of a response.
[00:25:50.280 --> 00:25:54.620]   The first step is to write because everything gets pushed into the model.
[00:25:54.620 --> 00:25:59.020]   You get the whole original question and you get everything the model has said so far in
[00:25:59.020 --> 00:26:00.020]   its answer.
[00:26:00.020 --> 00:26:01.020]   Right.
[00:26:01.020 --> 00:26:02.180]   Word by word, we're going to grow the answer.
[00:26:02.180 --> 00:26:05.900]   But as we grow this answer, we want the full input, including the original question input
[00:26:05.900 --> 00:26:06.900]   into our models.
[00:26:06.900 --> 00:26:09.100]   That's what I'm showing here.
[00:26:09.100 --> 00:26:15.420]   Feature detection is going to look at this text and pattern match out features that it
[00:26:15.420 --> 00:26:20.380]   thinks are relevant for what text the model should be producing.
[00:26:20.380 --> 00:26:24.060]   So these yellow underlines here, instructions in VCR.
[00:26:24.060 --> 00:26:26.980]   So maybe that's one feature points out.
[00:26:26.980 --> 00:26:28.940]   It extracts from this text.
[00:26:28.940 --> 00:26:31.140]   These are supposed to be instructions about a VCR.
[00:26:31.140 --> 00:26:36.660]   And maybe this orange underline, another feature says the peanut butter sandwich is involved.
[00:26:36.660 --> 00:26:39.920]   And so now the model has extracted two features.
[00:26:39.920 --> 00:26:43.580]   These are VCR instructions we're supposed to be producing and they're supposed to involve
[00:26:43.580 --> 00:26:46.360]   a peanut butter sandwich.
[00:26:46.360 --> 00:26:50.520]   The way we then take advantage and by we, I mean the model, the way we take advantage
[00:26:50.520 --> 00:26:55.220]   of those features is that we have what I call in my article rules.
[00:26:55.220 --> 00:26:59.700]   I have to say, AI people don't like me using the word rules because it has another meaning
[00:26:59.700 --> 00:27:01.880]   in the context of expert decision systems.
[00:27:01.880 --> 00:27:07.420]   But just for our own colloquial purposes, we can call them rules that extract the each
[00:27:07.420 --> 00:27:11.600]   rule, think of it as an instructions for extracting features like a pattern matching instruction,
[00:27:11.600 --> 00:27:19.480]   and then a set of guidelines for how to change the voting strategy based on those particular
[00:27:19.480 --> 00:27:20.480]   features.
[00:27:20.480 --> 00:27:21.520]   So here's what I mean.
[00:27:21.520 --> 00:27:26.080]   Maybe there's a rule that looks for things like instructions in VCR and it figures out,
[00:27:26.080 --> 00:27:29.600]   okay, we're supposed to be doing instructions about a VCR.
[00:27:29.600 --> 00:27:33.760]   And its guidelines are then when looking to match the relevant words.
[00:27:33.760 --> 00:27:37.840]   And in this example, I have the, I'm saying the relevant words are step is to, so like
[00:27:37.840 --> 00:27:40.060]   just the end of the answer here.
[00:27:40.060 --> 00:27:45.600]   When looking to match step is to, when we find those relevant words, step is to showing
[00:27:45.600 --> 00:27:54.080]   up in a source text that is about VCR instructions, give extra strength to those votes.
[00:27:54.080 --> 00:27:58.160]   So here I have on the screen, maybe one of the input texts was VCR repair instructions.
[00:27:58.160 --> 00:28:03.700]   And it says when removing a jam tape, the first step is to open the tape slot.
[00:28:03.700 --> 00:28:06.000]   So we have step is to open.
[00:28:06.000 --> 00:28:11.280]   So open is a candidate for the next word to output here.
[00:28:11.280 --> 00:28:17.480]   Because this source document matches the feature of VCR instructions, our rule here that's
[00:28:17.480 --> 00:28:22.400]   triggered might say, hey, let's make our vote for open really strong.
[00:28:22.400 --> 00:28:26.340]   We know it's grammatically correct because it follows step is to.
[00:28:26.340 --> 00:28:30.720]   But we think it's also has a good chance of being semantically correct because it comes
[00:28:30.720 --> 00:28:36.420]   from a source that matches the type of things we're supposed to be writing about.
[00:28:36.420 --> 00:28:40.260]   So let's make ourselves more likely to do this.
[00:28:40.260 --> 00:28:46.240]   Now think about having now a huge number of these rules for many, many different types
[00:28:46.240 --> 00:28:47.880]   of things that people could ask about it.
[00:28:47.880 --> 00:28:51.120]   And for all of these different things, someone might ask your chat program about peanut butter
[00:28:51.120 --> 00:28:55.400]   sandwiches, VCR repair, Seinfeld scripts, the bubble sort algorithm for anything that
[00:28:55.400 --> 00:28:58.000]   someone might ask your chat program about.
[00:28:58.000 --> 00:29:02.560]   You have some rule that talks about what to look at in the source text to figure out it's
[00:29:02.560 --> 00:29:07.500]   relevant and very specific guidelines about how should we then change our votes for words
[00:29:07.500 --> 00:29:12.380]   that match source text, that match these properties, these complicated rules.
[00:29:12.380 --> 00:29:18.080]   If we have enough of these rules, then we can start to generate text that's not only
[00:29:18.080 --> 00:29:24.280]   natural sounding, but actually seems to reply to or match what is being requested by the
[00:29:24.280 --> 00:29:26.480]   user.
[00:29:26.480 --> 00:29:32.220]   Now I think the reason why people have a hard time grasping this step is they imagine how
[00:29:32.220 --> 00:29:36.880]   many rules them or them and a team of people could come up with.
[00:29:36.880 --> 00:29:39.980]   And they say, I could come up with a couple dozen.
[00:29:39.980 --> 00:29:42.560]   Maybe if I worked with a team for a couple of years, we could come up with like a thousand
[00:29:42.560 --> 00:29:43.560]   good rules.
[00:29:43.560 --> 00:29:45.280]   But these rules are complicated.
[00:29:45.280 --> 00:29:49.200]   Even a rule as simple as how do we know they're asking about VCR instructions and how do we
[00:29:49.200 --> 00:29:52.480]   figure out if a given text we're given is a VCR instruction text?
[00:29:52.480 --> 00:29:53.480]   I don't know.
[00:29:53.480 --> 00:29:54.680]   But I think about that and look at a lot of examples.
[00:29:54.680 --> 00:29:58.120]   And maybe if we worked really hard, we could produce a few hundred, maybe a thousand of
[00:29:58.120 --> 00:29:59.120]   these rules.
[00:29:59.120 --> 00:30:01.080]   And that's not going to be nearly enough.
[00:30:01.080 --> 00:30:06.280]   That's not going to cover nearly enough scenarios for all of the topics that the more than 1
[00:30:06.280 --> 00:30:09.680]   million users who've signed up for chat GPT, for example, all the topics they could ask
[00:30:09.680 --> 00:30:11.160]   about.
[00:30:11.160 --> 00:30:18.240]   It turns out that the number of rules you really need to be as adept as chat GPT just
[00:30:18.240 --> 00:30:22.440]   blows out of proportion, any scale, any human scale we can think of.
[00:30:22.440 --> 00:30:27.280]   I did a little bit of back of envelope math for my New Yorker article.
[00:30:27.280 --> 00:30:34.100]   If you took all of the parameters that define GPT-3, which is the large language model that
[00:30:34.100 --> 00:30:36.480]   chat GPT then refined and is based on.
[00:30:36.480 --> 00:30:40.260]   So the parameters we can think of as the things they actually change, actually train.
[00:30:40.260 --> 00:30:42.240]   So this is really like the description of all of its rules.
[00:30:42.240 --> 00:30:52.240]   If we just wrote out all of the numbers that define the GPT-3, we would fill over 1.5 million
[00:30:52.240 --> 00:30:55.040]   average length books.
[00:30:55.040 --> 00:30:59.760]   So the number of rules you would have to have if we were writing them out would fill a large
[00:30:59.760 --> 00:31:03.260]   university library full of rules.
[00:31:03.260 --> 00:31:07.320]   That scale is so big, we have a really hard time imagining it.
[00:31:07.320 --> 00:31:12.560]   And that's why when we start to see, oh my goodness, this thing can answer almost anything
[00:31:12.560 --> 00:31:14.000]   I send to it.
[00:31:14.000 --> 00:31:16.520]   It can answer almost any question I ask of it.
[00:31:16.520 --> 00:31:23.640]   We think there must be some adaptable intelligence in there that's just learning about things,
[00:31:23.640 --> 00:31:27.160]   trying to understand and interact with us because we couldn't imagine just having enough
[00:31:27.160 --> 00:31:30.080]   rote rules to handle every topic that we could ask.
[00:31:30.080 --> 00:31:31.080]   But there is a lot of rules.
[00:31:31.080 --> 00:31:34.400]   There's 1.5 million books full of rules inside this chat GPT.
[00:31:34.400 --> 00:31:38.380]   So you have to wrap your mind around that scale.
[00:31:38.380 --> 00:31:41.200]   And then you have to imagine that not only is that many rules, but we can apply them
[00:31:41.200 --> 00:31:43.000]   in all sorts of combinations.
[00:31:43.000 --> 00:31:47.760]   VCR instructions, but also about a peanut butter sandwich, also in the style of King
[00:31:47.760 --> 00:31:52.520]   James Bible, stack those three rules and we get that first example that we saw earlier
[00:31:52.520 --> 00:31:53.520]   on.
[00:31:53.520 --> 00:31:54.520]   All right.
[00:31:54.520 --> 00:31:57.600]   So then the final idea is how in the world are we going to come up with all those rules?
[00:31:57.600 --> 00:31:59.680]   1.5 million books full of rules.
[00:31:59.680 --> 00:32:01.900]   How are we going to do that?
[00:32:01.900 --> 00:32:06.580]   And this is where self-training enters the picture.
[00:32:06.580 --> 00:32:09.600]   These language models train themselves.
[00:32:09.600 --> 00:32:11.960]   Here's the very basic way this works.
[00:32:11.960 --> 00:32:16.960]   Imagine we have this 1.5 million books full of rules and we start by just putting nonsense
[00:32:16.960 --> 00:32:19.160]   in every book.
[00:32:19.160 --> 00:32:20.160]   Nonsense rules, whatever they are.
[00:32:20.160 --> 00:32:21.160]   Right.
[00:32:21.160 --> 00:32:22.160]   So they don't do it.
[00:32:22.160 --> 00:32:26.000]   The system doesn't do anything useful right now, but at least we have a starting point.
[00:32:26.000 --> 00:32:28.800]   And now we tell the system, go train yourself and to help you train yourself, we're going
[00:32:28.800 --> 00:32:31.480]   to give you a lot of real text, text written by real humans.
[00:32:31.480 --> 00:32:33.760]   So when I say a lot, I mean a lot.
[00:32:33.760 --> 00:32:39.420]   The model on which chat GPT is based, for example, was given the results of crawling
[00:32:39.420 --> 00:32:41.960]   the public web for over 12 years.
[00:32:41.960 --> 00:32:47.840]   So a large percentage of anything ever written on the web over a decade was just part of
[00:32:47.840 --> 00:32:52.100]   the data that was given the chat GPT to train itself.
[00:32:52.100 --> 00:32:58.080]   And what the program does is it takes real text, little passages of real text out of
[00:32:58.080 --> 00:33:02.480]   this massive preposterously large data set.
[00:33:02.480 --> 00:33:05.620]   And it will use these passages one by one to make its rules better.
[00:33:05.620 --> 00:33:08.160]   So here's the example I have on the screen here.
[00:33:08.160 --> 00:33:14.240]   Let's say one of these many, many, many, many sample texts we gave chat GPT was Hamlet.
[00:33:14.240 --> 00:33:18.280]   And the program says, let's just grab some text from Hamlet.
[00:33:18.280 --> 00:33:23.160]   So let's say we're in Act 3 where we have the famous monologue to be or not to be, that
[00:33:23.160 --> 00:33:24.160]   is the question.
[00:33:24.160 --> 00:33:26.720]   What the program will do is just grab some of that text.
[00:33:26.720 --> 00:33:29.720]   So let's say it grabs to be or not to be.
[00:33:29.720 --> 00:33:32.560]   And then it's going to lop off the last word.
[00:33:32.560 --> 00:33:35.260]   So in this case, it lops off the word be.
[00:33:35.260 --> 00:33:37.640]   And it feeds what remains into the model.
[00:33:37.640 --> 00:33:41.200]   So when you lop off be here, you're left with to be or not to.
[00:33:41.200 --> 00:33:44.540]   It says, great, let's feed that into our model.
[00:33:44.540 --> 00:33:46.920]   We have 1.5 million books full of rules.
[00:33:46.920 --> 00:33:48.780]   They're all nonsense, because we're early in the training.
[00:33:48.780 --> 00:33:51.900]   But we'll go through each of those books and see which rules apply and let them modify our
[00:33:51.900 --> 00:33:52.900]   voting strategy.
[00:33:52.900 --> 00:33:56.180]   And we'll get this big vector of votes, then we'll randomly choose a word.
[00:33:56.180 --> 00:33:59.560]   And let's say in this case, the word is dog because it's not going to be a good word because
[00:33:59.560 --> 00:34:02.160]   the rules are really bad at first, but it'll spit out some words.
[00:34:02.160 --> 00:34:04.920]   Let's say it spits out dog.
[00:34:04.920 --> 00:34:09.200]   Now the good news is for the program, because it took this phrase from a real source, it
[00:34:09.200 --> 00:34:12.040]   knows what the next word is supposed to be.
[00:34:12.040 --> 00:34:16.440]   So on the screen here in orange, I'm showing it knows that be is what is supposed to follow
[00:34:16.440 --> 00:34:18.040]   to be or not to.
[00:34:18.040 --> 00:34:20.120]   So it can compare be to what it actually spit out.
[00:34:20.120 --> 00:34:25.360]   So the program spit out dog, it compares it to the right answer, the right answer is be.
[00:34:25.360 --> 00:34:31.760]   And here's the magic, it goes back and says, let me nudge my rules.
[00:34:31.760 --> 00:34:33.680]   There's a formal mathematical process it does to do this.
[00:34:33.680 --> 00:34:36.640]   But let me just go in there and just kind of tweak these rules.
[00:34:36.640 --> 00:34:42.160]   Not so the program accurately spits out be, but so it spits out something that is minutely
[00:34:42.160 --> 00:34:48.240]   more appropriate than dog, something that is just slightly better than the output it
[00:34:48.240 --> 00:34:49.640]   gave.
[00:34:49.640 --> 00:34:53.680]   So based on this one example, we've changed the rules a little bit, so that our output
[00:34:53.680 --> 00:34:55.800]   was just a teeny bit better.
[00:34:55.800 --> 00:35:01.040]   And it just repeats this again, and again, and again, hundreds of thousands of passages
[00:35:01.040 --> 00:35:04.080]   from Hamlet, and then from all the different Shakespeare works, and then on everything
[00:35:04.080 --> 00:35:09.000]   ever written in Wikipedia, and then on almost everything ever published on the web, bulletin
[00:35:09.000 --> 00:35:16.160]   board entries, sports websites, archived articles from old magazine websites, what just sentences
[00:35:16.160 --> 00:35:19.760]   of sentences, sentences, lop off a word, see what it spits out, compare it to the right
[00:35:19.760 --> 00:35:23.400]   answer, nudge the rules, take a new sentence, lop off the last word, stick in your model,
[00:35:23.400 --> 00:35:26.080]   see what it spits out, compare it to the real one, nudge the rules.
[00:35:26.080 --> 00:35:31.320]   And it does that again, and again, and again, hundreds of billions of times.
[00:35:31.320 --> 00:35:37.320]   There's one estimate I found online that said training chat GPT on a single processor would
[00:35:37.320 --> 00:35:41.240]   take over 350 years of compute time.
[00:35:41.240 --> 00:35:46.040]   And the only way that they could actually train on so much data so long was to have
[00:35:46.040 --> 00:35:50.320]   many, many processors working in parallel, spending well over a million dollars, I'm
[00:35:50.320 --> 00:35:52.440]   sure worth of compute time just to get this training done.
[00:35:52.440 --> 00:35:55.840]   And it still probably took weeks, if not months to actually complete that process.
[00:35:55.840 --> 00:35:59.440]   But here's the leap of faith I want you to make after this final idea.
[00:35:59.440 --> 00:36:05.000]   If you do this training, the simple training process on enough passages drawn from enough
[00:36:05.000 --> 00:36:09.720]   source text covering enough different types of topics from VCR instructions to Seinfeld
[00:36:09.720 --> 00:36:15.440]   scripts, these rules through all of these nudging, these 1.5 million books worth of
[00:36:15.440 --> 00:36:18.840]   rules will eventually become really, really smart.
[00:36:18.840 --> 00:36:23.360]   And it will eventually be way more comprehensive and nuanced than any one team of humans could
[00:36:23.360 --> 00:36:24.920]   ever produce.
[00:36:24.920 --> 00:36:26.840]   And they're going to recognize that this is a Bible verse.
[00:36:26.840 --> 00:36:29.920]   You want VCR instructions here and bubble sort is an algorithm.
[00:36:29.920 --> 00:36:32.860]   And this chapter from this textbook talks about bubble sort.
[00:36:32.860 --> 00:36:35.500]   And these are scripts and this is a script from Seinfeld.
[00:36:35.500 --> 00:36:38.160]   And actually, this part of the script for Seinfeld is a joke.
[00:36:38.160 --> 00:36:41.880]   So if we're in the middle of writing a joke in our output, then we want to really upvote
[00:36:41.880 --> 00:36:44.600]   words that are from jokes within Seinfeld scripts.
[00:36:44.600 --> 00:36:47.320]   All of these things we can imagine will be covered in these rule books.
[00:36:47.320 --> 00:36:49.960]   And I think the reason why we have a hard time imagining it being true is just because
[00:36:49.960 --> 00:36:51.880]   the scale is so preposterously large.
[00:36:51.880 --> 00:36:53.760]   We think about us filling up a book.
[00:36:53.760 --> 00:36:55.880]   We think about us coming up with two dozen rules.
[00:36:55.880 --> 00:37:01.660]   We have a hard time wrapping our mind around just the immensity of 1.5 million books worth
[00:37:01.660 --> 00:37:06.260]   of rules trained on 350 years worth of compute time.
[00:37:06.260 --> 00:37:10.020]   We just can't easily comprehend that scale.
[00:37:10.020 --> 00:37:15.520]   But it is so large that when you send what you think is this very clever request to chat
[00:37:15.520 --> 00:37:18.800]   GPT, it's like, oh, this rule, that rule, this rule, this rule.
[00:37:18.800 --> 00:37:20.680]   Boom, they apply.
[00:37:20.680 --> 00:37:21.680]   Modifier votes.
[00:37:21.680 --> 00:37:22.680]   Let's go.
[00:37:22.680 --> 00:37:24.880]   I think that amazes you.
[00:37:24.880 --> 00:37:27.040]   So those are the big ideas behind how chat GPT works.
[00:37:27.040 --> 00:37:31.200]   Now, I know all of the my fellow computer scientists out there with a background in
[00:37:31.200 --> 00:37:36.600]   artificial intelligence are probably yelling at your podcast headphones right now saying,
[00:37:36.600 --> 00:37:37.880]   well, that's not quite how it works, though.
[00:37:37.880 --> 00:37:43.440]   It's not it doesn't search for every word from the source text, and it doesn't have
[00:37:43.440 --> 00:37:45.820]   rules individually like that.
[00:37:45.820 --> 00:37:48.160]   It's instead in a much more complicated architecture.
[00:37:48.160 --> 00:37:49.160]   And this is all true.
[00:37:49.160 --> 00:37:50.160]   It's all true.
[00:37:50.160 --> 00:37:56.680]   I mean, the way that these models are actually architected or in something called a transformer
[00:37:56.680 --> 00:38:01.440]   block architecture, GPT three, for example, has ninety six transformer blocks arranged
[00:38:01.440 --> 00:38:03.160]   in layers one after another.
[00:38:03.160 --> 00:38:07.400]   Within each of these transformer blocks is a multi headed self attention layer that identifies
[00:38:07.400 --> 00:38:10.880]   what are the relevant words that this transformer block should care about.
[00:38:10.880 --> 00:38:14.360]   It then passes that on into a feed forward neural network.
[00:38:14.360 --> 00:38:19.040]   Is this neural networks that actually encode inside their weights, connecting their artificial
[00:38:19.040 --> 00:38:23.320]   neurons that actually encode in a sort of condensed, jumbled, mixed up manner, more
[00:38:23.320 --> 00:38:26.480]   or less the strategy I just described.
[00:38:26.480 --> 00:38:32.400]   So the feature detection that's built into the weights of these neural networks, the
[00:38:32.400 --> 00:38:36.680]   connection between certain features being identified, combined with certain relevant
[00:38:36.680 --> 00:38:40.480]   words, combined with vote strengths for what were should come next.
[00:38:40.480 --> 00:38:42.760]   All of that is trained into these networks during the training.
[00:38:42.760 --> 00:38:46.840]   So all the statistics and everything is trained into these as well.
[00:38:46.840 --> 00:38:50.640]   But in the end, what you get is a basically a jumbled, mixed up version of what I just
[00:38:50.640 --> 00:38:52.600]   explained.
[00:38:52.600 --> 00:38:55.960]   I sat down with some large language model experts when I was working on this article
[00:38:55.960 --> 00:38:58.180]   and said, let me just make sure I have this right.
[00:38:58.180 --> 00:39:00.760]   These high level five ideas.
[00:39:00.760 --> 00:39:04.040]   That's more or less what's being implemented in the artificial neural networks within the
[00:39:04.040 --> 00:39:05.040]   transformer block architecture.
[00:39:05.040 --> 00:39:06.840]   And they said, yeah, that's that's what's happening.
[00:39:06.840 --> 00:39:10.300]   It's again, it's mixed up, but that's what's happening.
[00:39:10.300 --> 00:39:15.280]   And so when you train the actual language model, you're not only training it to identify
[00:39:15.280 --> 00:39:19.640]   these features, you're baking in the statistics from the books and what happens with these
[00:39:19.640 --> 00:39:21.760]   for all that's getting baked into the big model itself.
[00:39:21.760 --> 00:39:23.200]   That's why these things are so large.
[00:39:23.200 --> 00:39:28.360]   That's why it takes 175 billion numbers to define all the rules for, let's say, GPT-3.
[00:39:28.360 --> 00:39:33.680]   But those five ideas I just gave you, that's more or less what's happening.
[00:39:33.680 --> 00:39:37.960]   And so this is what you have to believe is that with enough rules trained enough, what
[00:39:37.960 --> 00:39:41.300]   I just defined is going to generate really believable, impressive text.
[00:39:41.300 --> 00:39:42.360]   That's what's actually happening.
[00:39:42.360 --> 00:39:45.140]   Word guessing one word at a time.
[00:39:45.140 --> 00:39:50.260]   With enough rules to modify these votes and enough source text to draw from, you produce
[00:39:50.260 --> 00:39:51.760]   really believable text.
[00:39:51.760 --> 00:39:56.960]   All right, so if we know this, let us now briefly return to the second part of our deep
[00:39:56.960 --> 00:39:58.320]   question.
[00:39:58.320 --> 00:40:01.540]   How worried should we be?
[00:40:01.540 --> 00:40:08.300]   My opinion is once we have identified how these things actually work, our fear and concern
[00:40:08.300 --> 00:40:11.720]   is greatly tempered.
[00:40:11.720 --> 00:40:15.360]   So let's start with summarizing based on what I just said.
[00:40:15.360 --> 00:40:18.420]   What is it that these models like Chats GPT can actually do?
[00:40:18.420 --> 00:40:21.580]   Here's what they can actually do.
[00:40:21.580 --> 00:40:31.000]   They can respond to a question in arbitrary combination of known styles, talking about
[00:40:31.000 --> 00:40:32.600]   arbitrary combination of known subjects.
[00:40:32.600 --> 00:40:38.040]   So they can write about arbitrary numbers of known styles, talking about arbitrary combinations
[00:40:38.040 --> 00:40:39.040]   of known subjects.
[00:40:39.040 --> 00:40:43.200]   Known means it has seen enough of those things, enough of the style or enough writing about
[00:40:43.200 --> 00:40:45.560]   the topic in its training.
[00:40:45.560 --> 00:40:47.080]   That's what it can do.
[00:40:47.080 --> 00:40:51.920]   So say, write about this and this in this style.
[00:40:51.920 --> 00:40:55.320]   Bubble sort Seinfeld in a script.
[00:40:55.320 --> 00:41:00.240]   And it can do that and it can produce passable text if it's seen enough of those examples.
[00:41:00.240 --> 00:41:01.720]   And that's also all it can do.
[00:41:01.720 --> 00:41:05.120]   So let's start with the pragmatic question of is this going to take over our economy?
[00:41:05.120 --> 00:41:07.200]   And then we'll end with the bigger existential question.
[00:41:07.200 --> 00:41:12.000]   Is this an alien intelligence that's going to convert us into matrix batteries?
[00:41:12.000 --> 00:41:13.000]   So start with that.
[00:41:13.000 --> 00:41:17.160]   Is that capability I just described going to severely undermine the economy?
[00:41:17.160 --> 00:41:19.280]   And I don't think it is.
[00:41:19.280 --> 00:41:25.240]   I think where people get concerned about these Chats GPT type bots in the economy is they
[00:41:25.240 --> 00:41:30.280]   mistake the fluency with which it can combine styles and subjects with a adaptable fluid
[00:41:30.280 --> 00:41:31.280]   intelligence.
[00:41:31.280 --> 00:41:33.900]   Well, if it can do that, why can't it do other parts of my job?
[00:41:33.900 --> 00:41:36.740]   Why can't it handle my inbox for me?
[00:41:36.740 --> 00:41:39.820]   Why can't it build the computer program I need?
[00:41:39.820 --> 00:41:43.640]   You imagine that you need a human-like flexible intelligence to produce those type of texts
[00:41:43.640 --> 00:41:44.640]   that you see.
[00:41:44.640 --> 00:41:47.480]   And flexible human-like intelligence can do lots of things that are in our job.
[00:41:47.480 --> 00:41:48.480]   But it's not the case.
[00:41:48.480 --> 00:41:50.440]   There is no flexible human-like intelligence in there.
[00:41:50.440 --> 00:41:55.080]   There's just the ability to produce passable text with arbitrary combination of known styles
[00:41:55.080 --> 00:41:57.160]   on arbitrary combinations of known subjects.
[00:41:57.160 --> 00:42:01.240]   If we look at what most knowledge workers, for example, do in their job, that capability
[00:42:01.240 --> 00:42:03.120]   is not that useful.
[00:42:03.120 --> 00:42:07.100]   A lot of what knowledge workers do is not writing text.
[00:42:07.100 --> 00:42:11.600]   It is, for example, interacting with people or reading and synthesizing information.
[00:42:11.600 --> 00:42:16.520]   When knowledge workers do write, more often than not, the writing is incredibly narrow
[00:42:16.520 --> 00:42:17.520]   and bespoke.
[00:42:17.520 --> 00:42:22.620]   It is specific to the particular circumstances of who they work for, their job, and their
[00:42:22.620 --> 00:42:25.860]   history with their job, their history with the people they work for.
[00:42:25.860 --> 00:42:30.840]   I mentioned in my New Yorker piece that as I was writing the conclusion, that earlier
[00:42:30.840 --> 00:42:37.380]   that same day, I had to co-author an email to exactly the right person in our dean's
[00:42:37.380 --> 00:42:44.520]   office about a subtle request about how the hiring, faculty hiring process occurs at Georgetown,
[00:42:44.520 --> 00:42:47.820]   carefully couched, because I wasn't sure if this was the right person, carefully couched
[00:42:47.820 --> 00:42:50.360]   in language about, "I'm not sure if you're the right person for this, but here's why
[00:42:50.360 --> 00:42:53.920]   I think, and we talked about, this is why I'm asking about this.
[00:42:53.920 --> 00:42:56.440]   We had this conversation before."
[00:42:56.440 --> 00:43:01.440]   Writing in GPT, ChatGPT's broad training, could have helped it accomplish that narrow
[00:43:01.440 --> 00:43:02.680]   task on my behalf.
[00:43:02.680 --> 00:43:07.000]   And that's most of the writing that knowledge workers actually do.
[00:43:07.000 --> 00:43:12.360]   And even when we have relatively generic writing or coding or production of text that we need
[00:43:12.360 --> 00:43:21.040]   a system to do, we run into the problem that ChatGPT and similar chatbots are often wrong.
[00:43:21.040 --> 00:43:27.280]   Because again, they're just trying to make good guesses for words based on the styles
[00:43:27.280 --> 00:43:29.640]   and subjects you asked it about.
[00:43:29.640 --> 00:43:33.720]   These models have no actual model of the thing it's writing about.
[00:43:33.720 --> 00:43:37.480]   So they have no way of checking, does this make sense?
[00:43:37.480 --> 00:43:39.820]   Is this actually right?
[00:43:39.820 --> 00:43:43.000]   It just produces stuff in the style, like what is an answer supposed to more or less
[00:43:43.000 --> 00:43:46.220]   sound like if it's about this subject in this style?
[00:43:46.220 --> 00:43:52.840]   This is so pervasive that the developer bulletin board Stack Overflow had to put out a new
[00:43:52.840 --> 00:43:57.840]   rule that says no answers from ChatGPT can be used on this bulletin board.
[00:43:57.840 --> 00:44:02.840]   Because what was happening is ChatGPT would be happy to generate answers to your programmer
[00:44:02.840 --> 00:44:03.840]   questions.
[00:44:03.840 --> 00:44:05.300]   It sounded perfectly convincing.
[00:44:05.300 --> 00:44:09.640]   But as the moderator of the Stack Overflow board clarified, more often than not, they
[00:44:09.640 --> 00:44:10.640]   were also incorrect.
[00:44:10.640 --> 00:44:15.320]   Because ChatGPT doesn't know what a correct program is.
[00:44:15.320 --> 00:44:18.720]   It just knows I'm spitting out code.
[00:44:18.720 --> 00:44:24.600]   And based on other code I've seen and the features, this next command makes sense.
[00:44:24.600 --> 00:44:29.560]   And most of the commands make sense, but it doesn't know what sorting actually means.
[00:44:29.560 --> 00:44:33.560]   Or that there's a one off issue here.
[00:44:33.560 --> 00:44:37.360]   Or that equality isn't quite, you need equality, not just less than or whatever, right?
[00:44:37.360 --> 00:44:38.860]   Because it doesn't know sorting.
[00:44:38.860 --> 00:44:41.720]   It just says given the stuff I've spit out so far and the features I detected from what
[00:44:41.720 --> 00:44:44.880]   you asked me, and all this code I've looked at, here's a believable next thing to spit
[00:44:44.880 --> 00:44:45.880]   out.
[00:44:45.880 --> 00:44:48.480]   So it would spit out really believable programs that often didn't work.
[00:44:48.480 --> 00:44:53.320]   So we would assume most employers are not going to outsource jobs to an unrepentant
[00:44:53.320 --> 00:44:54.320]   fabulist.
[00:44:54.320 --> 00:44:57.800]   All right, so is it going to be not useful at all in the workplace?
[00:44:57.800 --> 00:44:58.800]   No, it will be useful.
[00:44:58.800 --> 00:45:02.040]   There'll be very bespoke things I think language models can do.
[00:45:02.040 --> 00:45:06.600]   It's particularly useful, what we've found in the last few months, where these technologies
[00:45:06.600 --> 00:45:10.760]   seems to be particularly useful is when you can give it text.
[00:45:10.760 --> 00:45:11.760]   They can do this too.
[00:45:11.760 --> 00:45:15.960]   You can give it text and say, rewrite this in this style or elaborate this.
[00:45:15.960 --> 00:45:16.960]   It's good at that.
[00:45:16.960 --> 00:45:18.200]   And that's useful.
[00:45:18.200 --> 00:45:21.320]   So if you're a doctor and you're typing in notes for electronic medical records, it might
[00:45:21.320 --> 00:45:23.760]   be nice that you can type them in sloppily.
[00:45:23.760 --> 00:45:29.980]   And a model like GPT-4, Chats GPT, might be able to take that and then transform those
[00:45:29.980 --> 00:45:31.200]   ideas into better English.
[00:45:31.200 --> 00:45:33.600]   It's the type of thing it can do.
[00:45:33.600 --> 00:45:37.680]   It can do other things like it can gather information for us and collate it in a way
[00:45:37.680 --> 00:45:39.160]   like a smart Google search.
[00:45:39.160 --> 00:45:43.200]   That's what Microsoft is doing when it's integrating this technology into its Bing search engine.
[00:45:43.200 --> 00:45:45.280]   It's a Google plus.
[00:45:45.280 --> 00:45:49.000]   So I mean, Google is already pretty smart, but you could have it do a little bit more
[00:45:49.000 --> 00:45:50.000]   actions.
[00:45:50.000 --> 00:45:54.740]   I mean, so there's going to be uses for this, but it is not going to come in and sweep away
[00:45:54.740 --> 00:45:57.040]   a whole swath of the economy.
[00:45:57.040 --> 00:46:00.080]   All right, let's get to the final deeper question here.
[00:46:00.080 --> 00:46:02.560]   Is this some sort of alien intelligence?
[00:46:02.560 --> 00:46:03.720]   Absolutely not.
[00:46:03.720 --> 00:46:09.960]   Once you understand the architecture, as I just defined it, there is no possible way
[00:46:09.960 --> 00:46:14.480]   that these large language model based programs can ever do anything that even approximates
[00:46:14.480 --> 00:46:19.560]   self-awareness consciousness or something we would have to be concerned about.
[00:46:19.560 --> 00:46:24.600]   There is a completely static definition for these programs once they're trained.
[00:46:24.600 --> 00:46:30.660]   The underlying parameters of GPT-3, once you train it up, for example, do not change as
[00:46:30.660 --> 00:46:32.420]   you start running requests through it.
[00:46:32.420 --> 00:46:35.060]   There is no malleable memory.
[00:46:35.060 --> 00:46:36.320]   It's the exact same rules.
[00:46:36.320 --> 00:46:38.360]   The only thing that changes is the input you give it.
[00:46:38.360 --> 00:46:42.600]   It goes all the way through these layers in a simple feed forward architecture and spits
[00:46:42.600 --> 00:46:43.600]   out a next word.
[00:46:43.600 --> 00:46:47.920]   And when you run it through again with a slightly longer request, it's the exact same layers,
[00:46:47.920 --> 00:46:49.400]   spits out another word.
[00:46:49.400 --> 00:46:54.160]   You cannot have anything that approaches consciousness or self-awareness without malleable memory.
[00:46:54.160 --> 00:47:00.080]   To be alive, by definition, you have to be able to have a ongoing updated model of yourself
[00:47:00.080 --> 00:47:02.100]   in the world around you.
[00:47:02.100 --> 00:47:08.800]   There's no such thing as a static entity where nothing can change.
[00:47:08.800 --> 00:47:13.520]   There's no memory that changes, nothing in it changes that you would consider to be alive.
[00:47:13.520 --> 00:47:20.020]   So no, this model is not the right type of AI technology that could ever become self-aware.
[00:47:20.020 --> 00:47:24.320]   There's other models in the AI universe that could be where you actually have notions of
[00:47:24.320 --> 00:47:30.920]   maintaining and updating models of learning, of thinking about yourself, interacting with
[00:47:30.920 --> 00:47:33.920]   the world, having incentives, having multiple actions you can take.
[00:47:33.920 --> 00:47:38.240]   You can build systems that in theory down the line could be self-aware.
[00:47:38.240 --> 00:47:39.760]   Large language model won't be it.
[00:47:39.760 --> 00:47:41.600]   Architecturally, it's impossible.
[00:47:41.600 --> 00:47:45.120]   All right, so that's where we are.
[00:47:45.120 --> 00:47:49.160]   We've created this really cool large language model.
[00:47:49.160 --> 00:47:52.240]   It's better than the ones that came before.
[00:47:52.240 --> 00:47:55.280]   It's really good at talking to people, so it's easy to use and you can share all these
[00:47:55.280 --> 00:47:57.240]   fun tweets about it.
[00:47:57.240 --> 00:48:00.840]   This general technology, one way or the other, will be integrated more and more into our
[00:48:00.840 --> 00:48:04.740]   working lives, but it's going to have the impact, in my opinion, more like Google had
[00:48:04.740 --> 00:48:07.000]   once that got really good, which was a big impact.
[00:48:07.000 --> 00:48:09.520]   You can ask Google all these questions, how to define words.
[00:48:09.520 --> 00:48:10.520]   It was very useful.
[00:48:10.520 --> 00:48:13.920]   It really helped people, but it didn't make whole industries disappear.
[00:48:13.920 --> 00:48:17.320]   I think that's where we're going to be with these large language models.
[00:48:17.320 --> 00:48:21.760]   They can produce text on arbitrary combinations of known subjects using arbitrary combination
[00:48:21.760 --> 00:48:25.520]   of known styles where known means they've seen it a sufficient number of times in their
[00:48:25.520 --> 00:48:26.520]   training.
[00:48:26.520 --> 00:48:28.360]   This is not a cow from 2001.
[00:48:28.360 --> 00:48:33.480]   This is not an alien intelligence that is going to, as was warned in that New York Times
[00:48:33.480 --> 00:48:39.580]   op-ed, deploy sophisticated propaganda to take over our political elections and create
[00:48:39.580 --> 00:48:41.860]   a one-world government.
[00:48:41.860 --> 00:48:46.480]   This is not going to get rid of programming as a profession and writing as a profession.
[00:48:46.480 --> 00:48:52.720]   It is cool, but it is not, in my opinion, an existential threat.
[00:48:52.720 --> 00:48:57.440]   What's transformative in the world of AI probably will not be in the immediate future transformative
[00:48:57.440 --> 00:48:59.160]   in your day-to-day life.
[00:48:59.160 --> 00:49:00.160]   All right?
[00:49:00.160 --> 00:49:05.280]   So, Jesse, there's my professor sermon for the day.
[00:49:05.280 --> 00:49:06.280]   >> JESSE: Solid.
[00:49:06.280 --> 00:49:09.320]   >> COREY: You don't want to get me started on computer science lectures because I could
[00:49:09.320 --> 00:49:16.240]   fall into my old habits, my professorial habits, and really bore you.
[00:49:16.240 --> 00:49:18.640]   >> JESSE: So how many rules will there be in five years?
[00:49:18.640 --> 00:49:19.640]   Will it double?
[00:49:19.640 --> 00:49:21.520]   >> COREY: I don't know how much bigger it can get.
[00:49:21.520 --> 00:49:23.040]   Yeah, it's a good question.
[00:49:23.040 --> 00:49:25.040]   So the jump from GPT-2 to GPT-3.
[00:49:25.040 --> 00:49:29.200]   So GPT-2 had some of the largest number of parameters before GPT-3 came out.
[00:49:29.200 --> 00:49:34.000]   It had 17 billion or something like this, and GPT-3 has 170 billion.
[00:49:34.000 --> 00:49:36.920]   I was talking to an expert at MIT about this.
[00:49:36.920 --> 00:49:43.040]   The issue about making this too much larger is they're already sort of giving it all of
[00:49:43.040 --> 00:49:45.880]   the text that exists.
[00:49:45.880 --> 00:49:52.000]   And so at some point, you're not going to get back bigger returns.
[00:49:52.000 --> 00:49:53.360]   So he said there's two issues with this.
[00:49:53.360 --> 00:49:57.060]   If you make your networks too small, they're not complicated enough to learn enough rules
[00:49:57.060 --> 00:49:58.060]   to be useful.
[00:49:58.060 --> 00:50:00.000]   But if you make them too large, you're wasting a lot of space.
[00:50:00.000 --> 00:50:01.560]   You're just going to have a lot of redundancy.
[00:50:01.560 --> 00:50:06.140]   I mean, it can only learn what it sees in its dataset.
[00:50:06.140 --> 00:50:12.400]   So if 175 billion parameters is well fit to this massive training data that we use for
[00:50:12.400 --> 00:50:17.200]   these chatbots, then just increasing the size of the network is not going to change much.
[00:50:17.200 --> 00:50:23.560]   You would have to have a correspondingly larger and richer training dataset to give it.
[00:50:23.560 --> 00:50:28.200]   And I don't know how much more, at least for this very particular problem of producing
[00:50:28.200 --> 00:50:31.880]   text, I don't know how much more richer or larger of a dataset we could give it.
[00:50:31.880 --> 00:50:36.680]   I actually think what the direction happening now is how do we make these things smaller
[00:50:36.680 --> 00:50:37.680]   again?
[00:50:37.680 --> 00:50:40.280]   GPT-3 is too big to be practical.
[00:50:40.280 --> 00:50:43.840]   175 billion parameters can't fit in the memory of a single GPU.
[00:50:43.840 --> 00:50:47.280]   You probably need five different specialized pieces of hardware just to generate a single
[00:50:47.280 --> 00:50:48.280]   word.
[00:50:48.280 --> 00:50:49.280]   That's not practical.
[00:50:49.280 --> 00:50:51.440]   That means I can't do that on my computer.
[00:50:51.440 --> 00:50:55.920]   That means if everyone at my office is constantly making requests to GPT-3 as part of their
[00:50:55.920 --> 00:50:58.340]   work, we're going to have this huge computing bill.
[00:50:58.340 --> 00:51:01.600]   So actually a lot of the effort is in how do we make these things smaller?
[00:51:01.600 --> 00:51:04.800]   Just focus on the examples that are relevant to what these people actually need.
[00:51:04.800 --> 00:51:05.840]   We want them to be small.
[00:51:05.840 --> 00:51:10.880]   We want it eventually to have models that can fit in a phone and still do useful things.
[00:51:10.880 --> 00:51:15.440]   So GPT-3 I think was, and that's what all these other ones are based off of, that was
[00:51:15.440 --> 00:51:20.120]   open AI saying what happens if we make these things much bigger?
[00:51:20.120 --> 00:51:21.400]   And now we're going to go back to make them smaller.
[00:51:21.400 --> 00:51:27.160]   And if you actually read the original GPT-3 paper, their goal with making it 10 times
[00:51:27.160 --> 00:51:32.480]   bigger was not that it was going to have in a particular domain 10 times better answers.
[00:51:32.480 --> 00:51:37.060]   They wanted to have one model that could do well in many unrelated tasks.
[00:51:37.060 --> 00:51:41.660]   And if you read the paper, they say, look, here's a bunch of different tasks for which
[00:51:41.660 --> 00:51:46.980]   we already have these large language models that do well, but they're customized to these
[00:51:46.980 --> 00:51:47.980]   tasks.
[00:51:47.980 --> 00:51:49.720]   They can only do that one task.
[00:51:49.720 --> 00:51:53.160]   And what they were proud about with GPT-3, if you read the original paper, is this one
[00:51:53.160 --> 00:51:56.840]   model can do well on all 10 of these tasks.
[00:51:56.840 --> 00:51:59.400]   It's not that it was actually doing much better than the state of the art in any one of these
[00:51:59.400 --> 00:52:00.400]   things.
[00:52:00.400 --> 00:52:04.000]   You don't need necessarily to hand train a model for each task.
[00:52:04.000 --> 00:52:06.120]   If you make it big enough, it can handle all the different tasks.
[00:52:06.120 --> 00:52:11.440]   So it wasn't getting 10 times larger did not make GPT-3 10 times better at any particular
[00:52:11.440 --> 00:52:12.440]   task.
[00:52:12.440 --> 00:52:15.560]   In fact, in most tasks, it's as good as the best, but not much better.
[00:52:15.560 --> 00:52:20.760]   It was the flexibility and the broadness, but that's good to see.
[00:52:20.760 --> 00:52:23.720]   It's cool for these web demos.
[00:52:23.720 --> 00:52:26.800]   But going forward, the name of the game, I think is going to go back to, actually, we
[00:52:26.800 --> 00:52:32.120]   need to make these things smaller so that we can not have to use an absurd amount of
[00:52:32.120 --> 00:52:37.320]   computational power just to figure out that dog should follow the quick brown fox jumped
[00:52:37.320 --> 00:52:39.240]   over the lazy brown.
[00:52:39.240 --> 00:52:41.440]   We need to maybe be a little bit more efficient.
[00:52:41.440 --> 00:52:45.080]   But anyways, I'm not particularly...
[00:52:45.080 --> 00:52:47.960]   It's a cool technology, but I don't know.
[00:52:47.960 --> 00:52:50.760]   I think once you open this, it's just not as worrisome.
[00:52:50.760 --> 00:52:54.480]   When it's a black box, you can imagine anything.
[00:52:54.480 --> 00:52:58.840]   I definitely like that Yuval Harari op-ed was definitely influenced by Nick Bostrom's
[00:52:58.840 --> 00:53:02.600]   super intelligence, which we talked about on the show a few months ago, where he just
[00:53:02.600 --> 00:53:03.600]   starts speculating.
[00:53:03.600 --> 00:53:05.600]   He's a philosopher, not a computer scientist.
[00:53:05.600 --> 00:53:07.880]   Bostrom just starts speculating.
[00:53:07.880 --> 00:53:09.440]   What if it got this smart?
[00:53:09.440 --> 00:53:10.440]   What could it do?
[00:53:10.440 --> 00:53:11.440]   Well, what if it got this smart?
[00:53:11.440 --> 00:53:12.440]   What could it do?
[00:53:12.440 --> 00:53:13.440]   Just thinking through scenarios about...
[00:53:13.440 --> 00:53:16.120]   And he was like, "Well, if it got smart, it could make itself smarter, and then it
[00:53:16.120 --> 00:53:18.360]   can make itself even smarter, and it become a super intelligence."
[00:53:18.360 --> 00:53:20.960]   And then they have all these scenarios about, "Well, if we had a super intelligent thing,
[00:53:20.960 --> 00:53:22.440]   it could take over all world politics.
[00:53:22.440 --> 00:53:27.640]   Because it'd be so smart and understand us so well that it could have the perfect propaganda.
[00:53:27.640 --> 00:53:31.120]   And now the bot could get us all to do whatever it wanted us to do."
[00:53:31.120 --> 00:53:33.920]   It's all just philosophical speculation.
[00:53:33.920 --> 00:53:41.560]   You open up these boxes, and you see 175 billion numbers being multiplied by GPUs doing 1.5
[00:53:41.560 --> 00:53:45.840]   million books worth of pattern detection vote rules to generate a probability vector so
[00:53:45.840 --> 00:53:46.840]   it can select a word.
[00:53:46.840 --> 00:53:47.840]   All right.
[00:53:47.840 --> 00:53:51.080]   Well, there's my computer science sermon.
[00:53:51.080 --> 00:53:55.400]   I have a few questions I want to get to from you, my listeners, that are about artificial
[00:53:55.400 --> 00:53:56.400]   intelligence.
[00:53:56.400 --> 00:53:59.040]   First, I want to mention one of the sponsors that makes this nonsense possible.
[00:53:59.040 --> 00:54:01.800]   And we're talking about our friends at ZocDoc.
[00:54:01.800 --> 00:54:08.160]   ZocDoc is the only free app that lets you find and book doctors who are patient reviewed,
[00:54:08.160 --> 00:54:13.000]   take your insurance, and are available when you need them to treat almost every condition
[00:54:13.000 --> 00:54:15.840]   under the sun.
[00:54:15.840 --> 00:54:20.160]   So if you need a doctor, instead of just saying, "I'll ask a friend," or, "I'll look it up
[00:54:20.160 --> 00:54:26.600]   in the yellow pages," you can instead go to ZocDoc, your ZocDoc app, and it'll show you
[00:54:26.600 --> 00:54:29.120]   doctors who are nearby that take your insurance.
[00:54:29.120 --> 00:54:30.640]   You can read reviews.
[00:54:30.640 --> 00:54:33.840]   And then once you sign up with these doctors, they'll often use ZocDoc to make it easier
[00:54:33.840 --> 00:54:39.400]   for you to set up appointments, get reminders about appointments, send in paperwork.
[00:54:39.400 --> 00:54:44.880]   Both my dentist and my primary care physician use ZocDoc, and I find it really useful because
[00:54:44.880 --> 00:54:47.800]   all of my interactions with them happen to that interface.
[00:54:47.800 --> 00:54:52.120]   So go to ZocDoc.com/deep and download the ZocDoc app for free.
[00:54:52.120 --> 00:54:56.520]   Then find a book, a top rated doctor today, many who are available within 24 hours.
[00:54:56.520 --> 00:55:03.360]   That's Z-O-C-D-O-C.com/deep, ZocDoc.com/deep.
[00:55:03.360 --> 00:55:08.000]   The show is also sponsored by BetterHelp.
[00:55:08.000 --> 00:55:13.920]   As we often talk about when it comes to the different buckets relevant to cultivating
[00:55:13.920 --> 00:55:19.440]   a deep life, the bucket of contemplation is in there.
[00:55:19.440 --> 00:55:25.960]   Having an active and healthy life of the mind is critical to a life that is deep in many
[00:55:25.960 --> 00:55:26.960]   different ways.
[00:55:26.960 --> 00:55:31.940]   It is easy, however, due to various events in life to fall into an area where your relationship
[00:55:31.940 --> 00:55:33.520]   to your mind gets strained.
[00:55:33.520 --> 00:55:39.680]   Maybe you find yourself overwhelmed with anxious thoughts or ruminations or depressive moments
[00:55:39.680 --> 00:55:42.040]   where you feel lack of affect.
[00:55:42.040 --> 00:55:45.560]   Whatever it is, it is easy for your mind to get out of kilter.
[00:55:45.560 --> 00:55:50.120]   Just like if your knee started hurting, you would go to an orthopedist.
[00:55:50.120 --> 00:55:53.120]   If your mind started hurting, and by the way, there's really loud construction sound going
[00:55:53.120 --> 00:55:56.320]   on.
[00:55:56.320 --> 00:55:58.520]   This is the restaurant below us is being made.
[00:55:58.520 --> 00:56:00.120]   It's better be worth it.
[00:56:00.120 --> 00:56:05.420]   But returning to BetterHelp, let me add this example.
[00:56:05.420 --> 00:56:10.880]   If you find yourself becoming increasingly enraged because of restaurant construction
[00:56:10.880 --> 00:56:15.040]   noise that occurs during your podcast, among the other things that could happen to affect
[00:56:15.040 --> 00:56:18.040]   your mental health, you need a therapist.
[00:56:18.040 --> 00:56:19.800]   Orthopedists will fix your bum knee.
[00:56:19.800 --> 00:56:23.880]   Therapists helps make sure that your cognitive life gets healthy, gets resilient, gets back
[00:56:23.880 --> 00:56:24.880]   on track.
[00:56:24.880 --> 00:56:25.880]   The problem is it's hard to find therapists.
[00:56:25.880 --> 00:56:30.040]   If you live in a big city, all the ones near you might be booked or they might be really
[00:56:30.040 --> 00:56:31.040]   expensive.
[00:56:31.040 --> 00:56:35.120]   This is where BetterHelp enters the scene.
[00:56:35.120 --> 00:56:39.280]   If you're thinking about starting therapy, BetterHelp is a great way to give it a try
[00:56:39.280 --> 00:56:41.440]   because it's entirely online.
[00:56:41.440 --> 00:56:44.640]   It's designed to be convenient, flexible, and suited to your schedule.
[00:56:44.640 --> 00:56:49.040]   You just fill out a brief questionnaire to get matched with a licensed therapist, and
[00:56:49.040 --> 00:56:53.920]   you can switch therapists anytime for no additional charge.
[00:56:53.920 --> 00:56:57.120]   So discover your potential with BetterHelp.
[00:56:57.120 --> 00:57:03.680]   Visit BetterHelp.com/deepquestions, one word, today to get 10% off your first month.
[00:57:03.680 --> 00:57:10.760]   That's BetterHelp, H-E-L-P.com/deepquestions.
[00:57:10.760 --> 00:57:14.680]   I hope this restaurant's good, Jesse, after all the disruption.
[00:57:14.680 --> 00:57:15.680]   They put up the signage.
[00:57:15.680 --> 00:57:16.680]   I don't know if you saw that.
[00:57:16.680 --> 00:57:17.680]   I didn't see the signage.
[00:57:17.680 --> 00:57:18.680]   I've heard the music.
[00:57:18.680 --> 00:57:19.680]   Motocat.
[00:57:19.680 --> 00:57:20.680]   That's the name?
[00:57:20.680 --> 00:57:21.680]   Yeah.
[00:57:21.680 --> 00:57:22.680]   It's a Tacoma Park reference.
[00:57:22.680 --> 00:57:23.680]   Okay.
[00:57:23.680 --> 00:57:24.680]   Yeah.
[00:57:24.680 --> 00:57:27.360]   It's an old character from Tacoma Park history.
[00:57:27.360 --> 00:57:30.320]   Anyways, soon it will be open.
[00:57:30.320 --> 00:57:33.000]   I heard I was talking to the guy, he thought May.
[00:57:33.000 --> 00:57:34.000]   Yeah.
[00:57:34.000 --> 00:57:35.000]   So soon.
[00:57:35.000 --> 00:57:36.000]   Nice.
[00:57:36.000 --> 00:57:37.000]   All right, let's do some questions.
[00:57:37.000 --> 00:57:38.000]   What do we got?
[00:57:38.000 --> 00:57:39.000]   All right.
[00:57:39.000 --> 00:57:43.200]   First question is from Manav, a student at Yale.
[00:57:43.200 --> 00:57:47.000]   Looking at tools like ChatGPT makes me feel like there's nothing AI won't eventually do
[00:57:47.000 --> 00:57:48.880]   better than humans.
[00:57:48.880 --> 00:57:52.360]   This fear makes it hard to concentrate on learning since it makes me feel that there
[00:57:52.360 --> 00:57:55.560]   isn't certainty in my future.
[00:57:55.560 --> 00:57:56.960]   Are my fears unfounded?
[00:57:56.960 --> 00:58:03.560]   Well, Manav, hopefully my deep dive is helping dispel those fears.
[00:58:03.560 --> 00:58:08.200]   I want to include this question in part to emphasize the degree to which the hype cycle
[00:58:08.200 --> 00:58:12.120]   around these tools has really been unhelpful.
[00:58:12.120 --> 00:58:19.000]   Because you can so easily embed screenshots of interactions of ChatGPT, a lot of people
[00:58:19.000 --> 00:58:23.720]   started trying it because the attraction of virality is very strong for lots of people
[00:58:23.720 --> 00:58:24.880]   in our online age.
[00:58:24.880 --> 00:58:30.160]   So it brought ChatGPT to the awareness of a lot of people and generated a lot of attention.
[00:58:30.160 --> 00:58:33.000]   Now once we had a lot of attention, how do you one up that attention?
[00:58:33.000 --> 00:58:35.120]   Well, then you start thinking about worries about it.
[00:58:35.120 --> 00:58:38.440]   You start thinking about what if it could do this, what if it could do that?
[00:58:38.440 --> 00:58:42.240]   From what I understand, I'm not as plugged into these online worlds as others, but there's
[00:58:42.240 --> 00:58:46.560]   a whole tech bro push during the last few months that was increasingly trying to push.
[00:58:46.560 --> 00:58:49.000]   It can do this, it can do that, it can do this.
[00:58:49.000 --> 00:58:53.440]   Exactly the same tonality with which the same group talked about crypto two years ago.
[00:58:53.440 --> 00:58:56.880]   Nothing is going to be done by ChatGPT, just like currency will be gone in three years
[00:58:56.880 --> 00:58:57.880]   because of crypto.
[00:58:57.880 --> 00:58:59.960]   They turned all their attention onto that.
[00:58:59.960 --> 00:59:02.240]   And that got really furious.
[00:59:02.240 --> 00:59:05.280]   And everyone's trying to one up each other and do YouTube videos and podcasts about,
[00:59:05.280 --> 00:59:06.840]   "No, it could do this, no, it can do that."
[00:59:06.840 --> 00:59:12.760]   And then this created this weird counter reaction from the mainstream media, because the mainstream
[00:59:12.760 --> 00:59:18.080]   media has a, right now, an adversarial relationship with the Silicon Valley tech bro crowd.
[00:59:18.080 --> 00:59:19.520]   They don't like them.
[00:59:19.520 --> 00:59:22.720]   So then they started pushing back about, "No, it's going to be bad.
[00:59:22.720 --> 00:59:26.920]   No, no, these tech bros are leading us to a world where we're going to be able to cheat
[00:59:26.920 --> 00:59:27.920]   on tests.
[00:59:27.920 --> 00:59:28.920]   No, forget cheat on tests.
[00:59:28.920 --> 00:59:29.920]   It's going to take all of our jobs.
[00:59:29.920 --> 00:59:31.520]   No, forget take all of our jobs.
[00:59:31.520 --> 00:59:35.080]   It's going to take over the government and become super intelligent."
[00:59:35.080 --> 00:59:40.880]   So they started the counteraction to the overblown enthusiasm of the tech bros became an overblown
[00:59:40.880 --> 00:59:45.960]   grimness from the anti-tech bro mainstream media.
[00:59:45.960 --> 00:59:50.240]   All of it fed into Twitter, which like a blender was mixing this all together and swirling
[00:59:50.240 --> 00:59:54.400]   this spiral of craziness higher and higher until finally just the average person like
[00:59:54.400 --> 00:59:59.400]   Manav here at Yale is thinking, "How can I even study knowing that there will be no
[00:59:59.400 --> 01:00:03.400]   jobs and we'll be enslaved by computers within the next couple of years?"
[01:00:03.400 --> 01:00:08.280]   All right, so Manav, hopefully my deep dive helped you feel better about this.
[01:00:08.280 --> 01:00:13.360]   Chats GPT can write about combinations of known subjects and combinations of known styles.
[01:00:13.360 --> 01:00:16.920]   It does not have models of these objects.
[01:00:16.920 --> 01:00:20.160]   It has no state or understanding or incentives.
[01:00:20.160 --> 01:00:23.160]   You ask it the right about removing a peanut butter sandwich from a VCR.
[01:00:23.160 --> 01:00:26.520]   It does not have an internal model of a VCR in a sandwich on which it's experimenting
[01:00:26.520 --> 01:00:29.800]   with different strategies to figure out which strategy works best and then turns to its
[01:00:29.800 --> 01:00:32.760]   language facility to explain that to you.
[01:00:32.760 --> 01:00:37.080]   It just sees peanut butter as a possible next word to spit out and you ask about peanut
[01:00:37.080 --> 01:00:39.440]   butter and your response, so it puts more votes on it.
[01:00:39.440 --> 01:00:44.800]   Mixing, matching, copying, manipulating existing human text, the humor and the jokes it spits
[01:00:44.800 --> 01:00:50.680]   out, the accuracy and the styles it uses are all intelligence borrowed from the input that
[01:00:50.680 --> 01:00:51.680]   it was given.
[01:00:51.680 --> 01:00:57.960]   It does not have a broad adaptable intelligence that can in any significant sense impact the
[01:00:57.960 --> 01:01:00.960]   knowledge work sector.
[01:01:00.960 --> 01:01:04.880]   It's important to emphasize, Manav, it's not like we're one small step from making these
[01:01:04.880 --> 01:01:11.280]   models more flexible, more adaptable, able to do more things.
[01:01:11.280 --> 01:01:15.600]   The key to chat CPT being so good at the specific thing it does, which is producing text in
[01:01:15.600 --> 01:01:19.560]   known styles on known subjects, is that it had a truly massive amount of training data
[01:01:19.560 --> 01:01:20.640]   on which it could train itself.
[01:01:20.640 --> 01:01:24.240]   We could give it everything anyone had ever written on the internet for a decade and it
[01:01:24.240 --> 01:01:26.240]   could use all of that to train itself.
[01:01:26.240 --> 01:01:30.440]   This is the problem when you try to adapt these models to other types of activities
[01:01:30.440 --> 01:01:33.160]   that are not just producing text.
[01:01:33.160 --> 01:01:38.560]   You say, "What I really want is a model that can work with my databases.
[01:01:38.560 --> 01:01:44.320]   What I really want is a model that can send emails and attach files on my behalf."
[01:01:44.320 --> 01:01:47.600]   The problem is you don't have enough training data.
[01:01:47.600 --> 01:01:52.360]   You need training data where you have billions of examples of, "Here's the situation, here's
[01:01:52.360 --> 01:01:53.360]   the right answer."
[01:01:53.360 --> 01:01:57.920]   And then like most things that we do, we learn after a small number of examples.
[01:01:57.920 --> 01:02:01.240]   A model to do other activities other than produce text needs a ton of data.
[01:02:01.240 --> 01:02:04.640]   And in most other types of genres or activities, there's just not that much data.
[01:02:04.640 --> 01:02:07.160]   So one of the few examples where there is, is art production.
[01:02:07.160 --> 01:02:09.240]   This is how Dali works.
[01:02:09.240 --> 01:02:14.760]   You can give it a huge corpus of pictures that are annotated with text and it can learn
[01:02:14.760 --> 01:02:17.920]   these different styles and subject matters that show up in pictures then produce original
[01:02:17.920 --> 01:02:18.920]   artwork.
[01:02:18.920 --> 01:02:23.120]   But that's one of the few other areas where you have enough uniform data that it can actually
[01:02:23.120 --> 01:02:24.880]   train itself to be super adaptable.
[01:02:24.880 --> 01:02:28.040]   So I'm not worried that Manav, like all of our jobs will be gone.
[01:02:28.040 --> 01:02:29.120]   So your fears are unfounded.
[01:02:29.120 --> 01:02:32.560]   You can rest easy, study harder for your classes.
[01:02:32.560 --> 01:02:34.640]   All right, let's keep it rolling.
[01:02:34.640 --> 01:02:35.640]   What do we got, Jesse?
[01:02:35.640 --> 01:02:37.800]   - All right, next question's from Aiden.
[01:02:37.800 --> 01:02:41.640]   It seems almost inevitable that in 10 years, AI will be able to perform many knowledge
[01:02:41.640 --> 01:02:44.400]   workers jobs as well as a human.
[01:02:44.400 --> 01:02:48.440]   Should we be worried about the pace of automation and knowledge work and how can we prepare
[01:02:48.440 --> 01:02:52.980]   our careers now for increased power AI in the coming decades?
[01:02:52.980 --> 01:02:57.680]   - So as I just explained in the last question, this particular trajectory of AI technology
[01:02:57.680 --> 01:03:00.020]   is not about to take all of your jobs.
[01:03:00.020 --> 01:03:03.260]   There is however, and this is why I included this question.
[01:03:03.260 --> 01:03:07.640]   There is however, another potential intersection of artificial intelligence and knowledge work
[01:03:07.640 --> 01:03:14.080]   that I've been talking about for years that I think we should be more concerned about
[01:03:14.080 --> 01:03:17.480]   or at least keep a closer eye on.
[01:03:17.480 --> 01:03:22.880]   The place where I think AI is gonna have the big impact is less sexy than this notion of
[01:03:22.880 --> 01:03:26.520]   I just have this blinking chat cursor and I can ask this thing to do whatever I want.
[01:03:26.520 --> 01:03:32.680]   Now where it's really gonna intersect is shallow task automation.
[01:03:32.680 --> 01:03:37.800]   So the shallow work, the stuff we do, the overhead we do to help collaborate, organize,
[01:03:37.800 --> 01:03:41.360]   and gather the information need for the main deep work that we execute in our knowledge
[01:03:41.360 --> 01:03:43.240]   work jobs.
[01:03:43.240 --> 01:03:47.280]   More and more of that is gonna be taken over by less sexy, more bespoke, but increasingly
[01:03:47.280 --> 01:03:49.880]   more effective AI tools.
[01:03:49.880 --> 01:03:55.640]   And as these tools get better, I don't have to send 126 emails a day anymore because I
[01:03:55.640 --> 01:04:00.440]   can actually have a bespoke AI agent handle a lot of that work for me, not in a general
[01:04:00.440 --> 01:04:05.820]   it's intelligent sense, but in a much more specific like talking to Alexa type sense.
[01:04:05.820 --> 01:04:10.680]   Can you gather the data I need for writing this report?
[01:04:10.680 --> 01:04:15.260]   Can you set up a meeting next week for me with these three principles?
[01:04:15.260 --> 01:04:19.840]   And then that AI agent talks to the AI agents of the three people you need to set the meeting
[01:04:19.840 --> 01:04:24.860]   up with and they figure out together and put that meeting onto the calendar so that none
[01:04:24.860 --> 01:04:27.040]   of us, three of us have to ever exchange an email.
[01:04:27.040 --> 01:04:31.320]   The information it gathers from the people who have it by talking to their AI agents,
[01:04:31.320 --> 01:04:32.600]   and I never have to bother them.
[01:04:32.600 --> 01:04:34.520]   We never have to set up a meeting.
[01:04:34.520 --> 01:04:38.820]   It's able to do these rote tasks for us, right?
[01:04:38.820 --> 01:04:42.040]   This was actually a future that I was exposed to a decade earlier.
[01:04:42.040 --> 01:04:47.640]   I spoke at an event with the CEO of a automated meeting scheduling company called X.AI.
[01:04:47.640 --> 01:04:51.240]   I remember him telling me, this is the future.
[01:04:51.240 --> 01:04:55.880]   When you have a AI tool that can talk to another person's AI tool to figure out logistical
[01:04:55.880 --> 01:04:59.000]   things on your behalf so that you're never interrupted.
[01:04:59.000 --> 01:05:02.320]   I think that's where the big AI impact is going to come.
[01:05:02.320 --> 01:05:04.800]   Now, this does not automate your main work.
[01:05:04.800 --> 01:05:08.040]   What it does is it automates away the stuff that gets in the way of your main work.
[01:05:08.040 --> 01:05:09.160]   Why is that significant?
[01:05:09.160 --> 01:05:14.520]   Because it will immensely increase the amount of your main work you're able to get done.
[01:05:14.520 --> 01:05:18.200]   If you're not context switching once every five minutes, which is the average time the
[01:05:18.200 --> 01:05:21.720]   average knowledge worker spends between email or instant messenger chat checks, if you're
[01:05:21.720 --> 01:05:26.160]   not doing that anymore, you know how much you're going to get done?
[01:05:26.160 --> 01:05:29.760]   You know how much if you can just do something until you're done, and then the AI agents
[01:05:29.760 --> 01:05:32.280]   on your computer says, "Okay, we got the stuff for you for the next thing you need
[01:05:32.280 --> 01:05:33.280]   to work on.
[01:05:33.280 --> 01:05:34.280]   Here you go."
[01:05:34.280 --> 01:05:36.680]   And you have to have no overhead communicating or collaborating, trying to figure out what
[01:05:36.680 --> 01:05:37.680]   to do next.
[01:05:37.680 --> 01:05:38.680]   You can just execute.
[01:05:38.680 --> 01:05:39.800]   You know how much you're going to get done?
[01:05:39.800 --> 01:05:43.680]   I would say probably three to four X more of the meaningful output that you produce
[01:05:43.680 --> 01:05:50.640]   in your job will be produced three to four X more if these unsexy bespoke AI logistical
[01:05:50.640 --> 01:05:53.640]   automator tools get better.
[01:05:53.640 --> 01:05:57.200]   So this has this huge potential benefit and this huge potential downside.
[01:05:57.200 --> 01:05:59.760]   The benefit of course is your work is less exhausting.
[01:05:59.760 --> 01:06:01.200]   You can get a lot more done.
[01:06:01.200 --> 01:06:03.000]   Companies are going to generate a lot more value.
[01:06:03.000 --> 01:06:09.060]   The downside is that might greatly reduce the number of knowledge workers required to
[01:06:09.060 --> 01:06:12.040]   meet certain production outputs.
[01:06:12.040 --> 01:06:18.040]   If three knowledge workers now produce what it used to take 10, I could grow my company
[01:06:18.040 --> 01:06:20.440]   or I could fire seven of those knowledge workers.
[01:06:20.440 --> 01:06:23.080]   So I think this is going to create a disruption.
[01:06:23.080 --> 01:06:28.280]   We underestimate the degree to which shallow work and context shifting is completely hampering
[01:06:28.280 --> 01:06:30.880]   our ability to do work with our minds.
[01:06:30.880 --> 01:06:35.400]   But because it's like the pot that keeps getting hotter until the lobster is boiled, because
[01:06:35.400 --> 01:06:39.160]   it's inflecting everybody, we don't realize how much we're being held back.
[01:06:39.160 --> 01:06:43.480]   When computer tools aided by AI remove that, it's going to be a huge disruption.
[01:06:43.480 --> 01:06:48.420]   And I think ultimately the economy is going to adapt to it.
[01:06:48.420 --> 01:06:52.320]   The knowledge industry is going to explode in size and scope as we can unlock all this
[01:06:52.320 --> 01:06:55.720]   cognitive potential on new types of challenges or problems that we weren't thinking about
[01:06:55.720 --> 01:06:56.720]   before.
[01:06:56.720 --> 01:06:59.780]   Ultimately, it'll probably be good and lead to a huge economic growth.
[01:06:59.780 --> 01:07:05.840]   But I think there's going to be a disruption period because we really are at such, again,
[01:07:05.840 --> 01:07:09.460]   we just don't emphasize the degree to how inefficient we are and how much if we could
[01:07:09.460 --> 01:07:12.660]   remove that inefficiency, we don't need most of the people sitting here in this office
[01:07:12.660 --> 01:07:14.360]   to get the same work done.
[01:07:14.360 --> 01:07:16.880]   Getting over that, that's going to be the real disruption.
[01:07:16.880 --> 01:07:19.840]   And there's no scary how from 2001 type tool involved here.
[01:07:19.840 --> 01:07:21.840]   These things are going to be boring.
[01:07:21.840 --> 01:07:26.360]   Meeting, information scheduling, basically whatever you type in an email, it could do
[01:07:26.360 --> 01:07:28.400]   that for you.
[01:07:28.400 --> 01:07:29.400]   That's going to be the real disruption.
[01:07:29.400 --> 01:07:33.040]   I don't know when it's coming, but that's coming soon.
[01:07:33.040 --> 01:07:34.360]   A lot of money at stake.
[01:07:34.360 --> 01:07:35.920]   All right, this is good.
[01:07:35.920 --> 01:07:37.600]   I'm like debunking people's fears.
[01:07:37.600 --> 01:07:39.920]   I got a therapist today.
[01:07:39.920 --> 01:07:44.640]   All right, next question is from Ben, a Silicon Valley engineer.
[01:07:44.640 --> 01:07:49.200]   I've decided that web development freelancing will be the best possible career path to achieve
[01:07:49.200 --> 01:07:50.840]   my family's lifestyle vision.
[01:07:50.840 --> 01:07:55.020]   And I plan to freelance in addition to my full-time job until freelancing can support
[01:07:55.020 --> 01:07:57.200]   our life on its own.
[01:07:57.200 --> 01:08:01.560]   Over the last few weeks, however, I've been hearing about the breakthroughs of chat, GPT
[01:08:01.560 --> 01:08:02.560]   and other AI tools.
[01:08:02.560 --> 01:08:06.600]   Do you think I should stay on the path of learning the ropes of freelancing web development
[01:08:06.600 --> 01:08:11.560]   or should I focus more on the future of technology and try to stay ahead of the curve?
[01:08:11.560 --> 01:08:18.000]   Well, Ben, I'm using the inclusion of AI in your question to secretly get in an example
[01:08:18.000 --> 01:08:22.080]   of lifestyle-centric career planning, which you know I like to talk about.
[01:08:22.080 --> 01:08:24.080]   So I love your approach.
[01:08:24.080 --> 01:08:27.760]   You're working backwards from a vision of what you want you and your family's life to
[01:08:27.760 --> 01:08:35.880]   be like, a tangible lifestyle, not specific in what particular job or city, but the attributes
[01:08:35.880 --> 01:08:36.880]   of the lifestyle.
[01:08:36.880 --> 01:08:41.840]   So you're working backwards to say, what is a tractable path from where I am to accomplish
[01:08:41.840 --> 01:08:43.680]   that.
[01:08:43.680 --> 01:08:47.040]   And you're seeing here, web development could be there, freelance web development.
[01:08:47.040 --> 01:08:50.200]   And I don't know all the details of your plan, but I'm assuming you probably have a plan
[01:08:50.200 --> 01:08:52.840]   where you're living somewhere that is cheaper to live.
[01:08:52.840 --> 01:08:57.200]   Maybe it's more outside or country oriented where your expenses are lower.
[01:08:57.200 --> 01:09:02.280]   And because web development is a relatively high reward per hour spent type activity,
[01:09:02.280 --> 01:09:05.760]   that strategic freelancing could support your lifestyle there while giving you huge amounts
[01:09:05.760 --> 01:09:06.760]   of autonomy.
[01:09:06.760 --> 01:09:10.520]   And satisfying the various properties that I assume you figured out about what you want
[01:09:10.520 --> 01:09:12.440]   in your life, these sort of non-professional properties.
[01:09:12.440 --> 01:09:13.920]   So I really applaud this thinking.
[01:09:13.920 --> 01:09:18.760]   I also really applaud the fact that you're applying the principle of letting money be
[01:09:18.760 --> 01:09:21.640]   a neutral indicator of value.
[01:09:21.640 --> 01:09:25.960]   There's a strategy I talked about in my book, So Good They Can't Ignore You.
[01:09:25.960 --> 01:09:30.800]   This is a strategy in which instead of just jumping into something new, you try it on
[01:09:30.800 --> 01:09:34.300]   the side and say, can I actually make money at this?
[01:09:34.300 --> 01:09:39.400]   The idea here is that people paying you money is the most unbiased feedback you will get
[01:09:39.400 --> 01:09:42.960]   about how valuable or viable the thing you're doing actually is.
[01:09:42.960 --> 01:09:44.640]   And so I really like this idea.
[01:09:44.640 --> 01:09:46.680]   Let me try freelancing on the side.
[01:09:46.680 --> 01:09:50.320]   I mean, I want to see that people are hiring me and this money is coming in before I quit
[01:09:50.320 --> 01:09:51.320]   my job.
[01:09:51.320 --> 01:09:52.800]   It's a great way of actually assessing.
[01:09:52.800 --> 01:09:54.440]   Don't just ask people, hey, is this valuable?
[01:09:54.440 --> 01:09:56.080]   Do you think you would hire me?
[01:09:56.080 --> 01:09:57.080]   Look at dollars coming in.
[01:09:57.080 --> 01:10:02.560]   When the dollars coming in are enough to more or less support your new lifestyle, you then
[01:10:02.560 --> 01:10:03.640]   make that transition with confidence.
[01:10:03.640 --> 01:10:05.080]   So I appreciate that as well.
[01:10:05.080 --> 01:10:08.560]   Two Cal Newport ideas being deployed here.
[01:10:08.560 --> 01:10:10.640]   So let's get to your AI question.
[01:10:10.640 --> 01:10:15.440]   Should you stop this plan so you can focus more on the future of technology and try to
[01:10:15.440 --> 01:10:16.440]   stay ahead of the curve?
[01:10:16.440 --> 01:10:18.840]   I mean, I don't even really know what that means.
[01:10:18.840 --> 01:10:22.100]   My main advice would be whatever skill it is you're learning, make sure you're learning
[01:10:22.100 --> 01:10:24.500]   a course at the cutting edge of it.
[01:10:24.500 --> 01:10:28.140]   Get real feedback from real people in this field about what is actually valuable and
[01:10:28.140 --> 01:10:30.280]   how good you have to be to unlock that value.
[01:10:30.280 --> 01:10:36.560]   So I would say that don't invent your own story about how you want this field to work.
[01:10:36.560 --> 01:10:41.480]   Don't assume that if you know HTML and a little CSS, you're going to be living easy.
[01:10:41.480 --> 01:10:43.160]   What are the actual skills people care about?
[01:10:43.160 --> 01:10:45.220]   What web development technologies sell?
[01:10:45.220 --> 01:10:46.320]   How hard are they?
[01:10:46.320 --> 01:10:49.800]   How good you have to be at that to actually be desirable to the marketplace?
[01:10:49.800 --> 01:10:52.580]   Get hard, verified answers to those questions.
[01:10:52.580 --> 01:10:54.720]   That's what I would say when it comes to staying ahead of the curve.
[01:10:54.720 --> 01:10:55.800]   That's it.
[01:10:55.800 --> 01:11:02.800]   And as for some sort of fear that you becoming a web developer is quixotic because chat GPT
[01:11:02.800 --> 01:11:04.280]   is going to take that job away soon.
[01:11:04.280 --> 01:11:05.760]   Don't worry about that.
[01:11:05.760 --> 01:11:10.120]   So yes, make sure you're learning the right skills, not the skills you want to be valuable.
[01:11:10.120 --> 01:11:12.280]   But there's nothing wrong with this particular plan you have.
[01:11:12.280 --> 01:11:13.960]   I love the way you're executing it.
[01:11:13.960 --> 01:11:15.720]   I love the way you're thinking about it.
[01:11:15.720 --> 01:11:22.880]   I also appreciate, I cut it, Jesse, but Ben had a joke in the beginning of his response.
[01:11:22.880 --> 01:11:26.360]   He said, "I've been doing lifestyle-centric career planning.
[01:11:26.360 --> 01:11:27.360]   I've been thinking about it.
[01:11:27.360 --> 01:11:28.360]   I don't like my job.
[01:11:28.360 --> 01:11:30.760]   So what we're going to do is quit and I'm going to start a cattle ranch."
[01:11:30.760 --> 01:11:33.200]   And he was like, "Ah, just joking."
[01:11:33.200 --> 01:11:34.560]   I appreciate that.
[01:11:34.560 --> 01:11:41.760]   All right, let's do one more question before I kind of run out of the ability to talk about
[01:11:41.760 --> 01:11:42.760]   AI anymore.
[01:11:42.760 --> 01:11:44.040]   I'm getting, just, we're purging this.
[01:11:44.040 --> 01:11:46.080]   It's been months that people have been asking us about AI.
[01:11:46.080 --> 01:11:48.720]   I'm just purging it all out there.
[01:11:48.720 --> 01:11:56.000]   Next week, we're going to talk all about, I don't know, living in the country or minimalism,
[01:11:56.000 --> 01:11:58.840]   not using social media, but we're getting all the AI out of our system today.
[01:11:58.840 --> 01:12:00.960]   All right, here we go.
[01:12:00.960 --> 01:12:02.520]   Last question is from Anakin.
[01:12:02.520 --> 01:12:06.080]   "AI can solve high school level math word problems.
[01:12:06.080 --> 01:12:09.920]   AI can explain why a joke that has never seen before is funny.
[01:12:09.920 --> 01:12:11.400]   This one blows my mind.
[01:12:11.400 --> 01:12:14.200]   All this points to mass job loss within five years.
[01:12:14.200 --> 01:12:15.200]   What do you think?"
[01:12:15.760 --> 01:12:18.360]   Well, again, big thumbs down.
[01:12:18.360 --> 01:12:24.000]   The current trajectory of AI is not going to create mass job loss in the next five years.
[01:12:24.000 --> 01:12:26.760]   Chat GPT doesn't know why a joke is funny.
[01:12:26.760 --> 01:12:31.340]   It writes jokes that are funny because it knows what part of a script you can identify
[01:12:31.340 --> 01:12:33.880]   as a joke that's a pattern-mashing problem.
[01:12:33.880 --> 01:12:38.000]   And then it upvotes words from those parts of scripts when it's doing the word guessing
[01:12:38.000 --> 01:12:39.000]   game.
[01:12:39.000 --> 01:12:43.680]   And as a result, you get jokes that pull from existing structures of human and existing
[01:12:43.680 --> 01:12:44.680]   text.
[01:12:44.680 --> 01:12:46.080]   You don't actually know what humor is.
[01:12:46.080 --> 01:12:50.480]   You can see that in part if you look at that Seinfeld scene with bubble sort I talked about
[01:12:50.480 --> 01:12:53.720]   at the beginning of the program.
[01:12:53.720 --> 01:12:57.800]   There's non-sequitur jokes in there, things that are described as the audience laughing
[01:12:57.800 --> 01:12:58.800]   that aren't actually funny.
[01:12:58.800 --> 01:13:02.840]   And that's because it's not actually looking at its script and saying, "Is this funny?"
[01:13:02.840 --> 01:13:07.720]   It's guessing words, guessing words that things are accurate.
[01:13:07.720 --> 01:13:08.720]   But let's talk about...
[01:13:08.720 --> 01:13:13.980]   I want to use this as an excuse to talk about another trend in AI that I think is probably
[01:13:13.980 --> 01:13:18.520]   more important than any of these large language models that also is not getting talked about
[01:13:18.520 --> 01:13:19.520]   enough.
[01:13:19.520 --> 01:13:22.760]   So we talked about an earlier question, AI shallow work automation has been critical.
[01:13:22.760 --> 01:13:27.840]   The other major development that we're so used to now we forget, but I think is actually
[01:13:27.840 --> 01:13:33.160]   going to be the key to this AI shallow work automation, but also all sorts of other ways
[01:13:33.160 --> 01:13:37.440]   that AI interests our life is not these large language models, but it's what Google has
[01:13:37.440 --> 01:13:39.620]   in mind with Google Home.
[01:13:39.620 --> 01:13:44.580]   It's what Amazon has in mind with Alexa.
[01:13:44.580 --> 01:13:50.760]   These at-home appliances that you can talk to and ask to do things, they're ubiquitous
[01:13:50.760 --> 01:13:54.320]   and they're ubiquitous in part because these companies made them very cheap.
[01:13:54.320 --> 01:13:55.320]   They wanted people to use them.
[01:13:55.320 --> 01:13:57.080]   They're not trying to make a lot of money off them.
[01:13:57.080 --> 01:13:58.080]   Why?
[01:13:58.080 --> 01:14:01.680]   Why is Google or Amazon trying to get as many people as possible to use these agents at
[01:14:01.680 --> 01:14:04.360]   home that you could just talk to and it tries to understand you?
[01:14:04.360 --> 01:14:05.880]   It's data.
[01:14:05.880 --> 01:14:09.760]   The game they are playing is we want millions and millions of different people with different
[01:14:09.760 --> 01:14:13.960]   accents and different types of voices asking about all sorts of different things that we
[01:14:13.960 --> 01:14:16.320]   then try to understand.
[01:14:16.320 --> 01:14:21.160]   And we could then use this data set to train increasingly better interfaces that can understand
[01:14:21.160 --> 01:14:22.940]   natural language.
[01:14:22.940 --> 01:14:23.940]   That's the whole ballgame.
[01:14:23.940 --> 01:14:25.880]   Now, chat GPT is pretty good at this.
[01:14:25.880 --> 01:14:26.880]   They figured out a new model.
[01:14:26.880 --> 01:14:29.120]   I don't want to get into the weeds.
[01:14:29.120 --> 01:14:33.760]   They have a human semi-supervised, semi-human supervised reinforcement learning model that
[01:14:33.760 --> 01:14:38.760]   they inserted during the GPT-3 training to try to align its responses better with what's
[01:14:38.760 --> 01:14:40.180]   being asked.
[01:14:40.180 --> 01:14:44.740]   But this is the whole ballgame is just natural language understanding.
[01:14:44.740 --> 01:14:47.840]   And Google is working on this and Amazon is working on this and Apple is working on this
[01:14:47.840 --> 01:14:49.960]   with their Siri devices.
[01:14:49.960 --> 01:14:54.080]   And this is what matters, understanding people.
[01:14:54.080 --> 01:14:58.240]   The background activities, I think this is what we often get wrong.
[01:14:58.240 --> 01:15:02.560]   The actual activities that the disruptive AI in the future is going to do on our behalf
[01:15:02.560 --> 01:15:04.360]   are not that interesting.
[01:15:04.360 --> 01:15:06.640]   It's not, we're going to go write an ARIA.
[01:15:06.640 --> 01:15:11.920]   It's we're going to pull some data from an Excel table and email it to someone.
[01:15:11.920 --> 01:15:17.080]   It's we're going to turn off the lights in your house because you said you were gone.
[01:15:17.080 --> 01:15:19.080]   It's really kind of boring stuff.
[01:15:19.080 --> 01:15:21.880]   All of the interesting stuff is understanding what you're saying.
[01:15:21.880 --> 01:15:25.080]   And that's why Google and Amazon and Apple invested so much money into getting these
[01:15:25.080 --> 01:15:29.280]   devices everywhere is they want to train and generate the biggest possible data set of
[01:15:29.280 --> 01:15:32.280]   actually understanding what real people are saying and figuring out, did we get it right
[01:15:32.280 --> 01:15:33.280]   or did we get it wrong?
[01:15:33.280 --> 01:15:37.680]   And can we look at examples and let's hand annotate these examples and figure out how
[01:15:37.680 --> 01:15:38.680]   our models work.
[01:15:38.680 --> 01:15:42.040]   And I think this is really going to be the slow creep of AI disruption.
[01:15:42.040 --> 01:15:45.560]   It's not going to be this one entity that suddenly takes over everyone's job.
[01:15:45.560 --> 01:15:49.280]   It's going to be that more and more devices in our world are increasingly better at understanding
[01:15:49.280 --> 01:15:54.640]   natural language questions, whether it be typed or spoken, and can then act accordingly,
[01:15:54.640 --> 01:15:56.360]   even if what we're asking to do is simple.
[01:15:56.360 --> 01:15:58.720]   We don't really need these agents to do really complicated things.
[01:15:58.720 --> 01:16:02.040]   We just need them to understand what we mean.
[01:16:02.040 --> 01:16:06.640]   Most of what we do, that's a drag on our time, it's a drag on our energy, it's pretty simple.
[01:16:06.640 --> 01:16:10.480]   And it's something a machine could do if they just knew what it was we wanted.
[01:16:10.480 --> 01:16:15.640]   And so I think that's where we should be focusing is interfacing is what matters.
[01:16:15.640 --> 01:16:21.080]   These 175 billion parameter models that can generate all this text is really not that
[01:16:21.080 --> 01:16:22.080]   interesting.
[01:16:22.080 --> 01:16:26.080]   I don't need a Seinfeld script about bubble sort.
[01:16:26.080 --> 01:16:31.280]   I need you to understand me when I say, give me the email address of all of the students
[01:16:31.280 --> 01:16:33.560]   from my first section of my class.
[01:16:33.560 --> 01:16:38.400]   I need you just to understand what that means and be able to interface with the student
[01:16:38.400 --> 01:16:42.280]   database and get those emails out and format it properly so I can send the message to the
[01:16:42.280 --> 01:16:43.280]   class.
[01:16:43.280 --> 01:16:44.840]   That's what I want you to do.
[01:16:44.840 --> 01:16:53.280]   I don't want you to write an original poem in the style of Mary Oliver about a hockey
[01:16:53.280 --> 01:16:55.080]   game for my students.
[01:16:55.080 --> 01:17:00.200]   I need you to just go through when I say, look at the assignment pages for the problem
[01:17:00.200 --> 01:17:05.800]   sets I assigned this semester, pull out the grading statistics and put them all into one
[01:17:05.800 --> 01:17:08.080]   document just to kind of, okay, I know what that means.
[01:17:08.080 --> 01:17:11.640]   And now I'm doing a pretty rote automatic computer type thing.
[01:17:11.640 --> 01:17:14.920]   And I don't care if you come back and say, okay, I have three follow-up questions.
[01:17:14.920 --> 01:17:15.920]   So I understand what you mean.
[01:17:15.920 --> 01:17:17.480]   You mean this section, that section, this section, that's fine.
[01:17:17.480 --> 01:17:19.800]   They'll just take us another 10 or 15 seconds.
[01:17:19.800 --> 01:17:26.520]   I don't need, in other words, how from 2001, I just need my basic computer to understand
[01:17:26.520 --> 01:17:27.520]   what I'm saying.
[01:17:27.520 --> 01:17:29.440]   So I don't have to type it in or click on a lot of different buttons.
[01:17:29.440 --> 01:17:32.800]   So I mean, I think that's really where we're going to see the big innovations is the slow
[01:17:32.800 --> 01:17:39.000]   creep of better and better human understanding plugged into relatively non-interesting actions.
[01:17:39.000 --> 01:17:41.200]   That's really where the stuff's going to take a bigger and bigger picture.
[01:17:41.200 --> 01:17:43.680]   The disruption is going to be more subtle.
[01:17:43.680 --> 01:17:49.280]   This idea that it's now this all at once large language model represents, we have this new
[01:17:49.280 --> 01:17:52.800]   self-sufficient being that all at once we'll do everything.
[01:17:52.800 --> 01:17:55.840]   It's sexy, but I don't think that's the way this is going to happen.
[01:17:55.840 --> 01:17:56.840]   All right.
[01:17:56.840 --> 01:17:57.840]   So let's change gears here, Jesse.
[01:17:57.840 --> 01:18:01.240]   I want to do something interesting to wrap up the show.
[01:18:01.240 --> 01:18:05.680]   First, I want to mention one of the longtime sponsors that makes Deep Questions possible.
[01:18:05.680 --> 01:18:06.680]   That's our friends at Blinkist.
[01:18:06.680 --> 01:18:14.320]   When you subscribe to Blinkist, you get access to 15 minute summaries of over 5,500 nonfiction
[01:18:14.320 --> 01:18:15.960]   books and podcasts.
[01:18:15.960 --> 01:18:19.080]   These short summaries, which are called Blinks, you can either read them or you can listen
[01:18:19.080 --> 01:18:22.520]   to them while you do other activities.
[01:18:22.520 --> 01:18:28.800]   Jesse and I both use Blinkist as one of our primary tools for triaging which books we
[01:18:28.800 --> 01:18:29.800]   read.
[01:18:29.800 --> 01:18:33.620]   If there is a book we are interested in, we will read or listen to the Blink to see if
[01:18:33.620 --> 01:18:36.760]   it's worth buying.
[01:18:36.760 --> 01:18:41.360]   The other week, Jesse and I went through his own Blinkist list and we calculated that basically
[01:18:41.360 --> 01:18:45.680]   what it was roughly what 30%, I think you said 30% of the books for which you read Blinkist
[01:18:45.680 --> 01:18:47.000]   if you go on the buy.
[01:18:47.000 --> 01:18:48.240]   But there we go.
[01:18:48.240 --> 01:18:51.520]   That is a case in point of the value of Blinkist.
[01:18:51.520 --> 01:18:54.560]   It really helps you hone in on the right books to buy.
[01:18:54.560 --> 01:18:56.520]   The books you don't buy, you still know their main ideas.
[01:18:56.520 --> 01:18:58.080]   You can deploy them.
[01:18:58.080 --> 01:19:04.680]   Blinkist is really a tool that any serious reader needs to add to their tool kit.
[01:19:04.680 --> 01:19:06.880]   Right now, Blinkist has a special offer just for our audience.
[01:19:06.880 --> 01:19:12.160]   Go to Blinkist.com/deep to start your seven-day free trial and you will get 45% off a Blinkist
[01:19:12.160 --> 01:19:14.080]   premium membership.
[01:19:14.080 --> 01:19:17.040]   That's Blinkist spelled B-L-I-N-K-I-S-T.
[01:19:17.040 --> 01:19:21.440]   Blinkist.com/deep to get 45% off any seven-day free trial.
[01:19:21.440 --> 01:19:23.400]   That's Blinkist.com/deep.
[01:19:23.400 --> 01:19:27.480]   This offer is good through April 30th only.
[01:19:27.480 --> 01:19:33.040]   Even better news, for a limited time, you can take advantage of their Blinkist Connect
[01:19:33.040 --> 01:19:36.080]   program to share your premium account.
[01:19:36.080 --> 01:19:38.960]   You will, in other words, get two premium accounts for the price of one.
[01:19:38.960 --> 01:19:43.000]   So when you subscribe to Blinkist, you can give an account to a friend who you think
[01:19:43.000 --> 01:19:44.800]   would also appreciate it.
[01:19:44.800 --> 01:19:48.520]   So Blinkist.com/deep to find out more.
[01:19:48.520 --> 01:19:51.480]   I also want to talk about Latter.
[01:19:51.480 --> 01:19:53.360]   It's tax season.
[01:19:53.360 --> 01:19:57.560]   Tax season is when I often get stressed about putting things off because I don't even know
[01:19:57.560 --> 01:19:59.040]   where to get started.
[01:19:59.040 --> 01:20:04.360]   This got me thinking about the other classic thing that people put off, which is getting
[01:20:04.360 --> 01:20:05.360]   life insurance.
[01:20:05.360 --> 01:20:09.480]   This is one of those things that you know you need, but you don't know where to get
[01:20:09.480 --> 01:20:10.480]   started.
[01:20:10.480 --> 01:20:11.960]   Who do you talk to for life insurance?
[01:20:11.960 --> 01:20:14.000]   How much should life insurance cost?
[01:20:14.000 --> 01:20:18.880]   You're going to have to go to a doctor and get blood drawn and your eyeball scanned before
[01:20:18.880 --> 01:20:20.920]   you can actually get a policy.
[01:20:20.920 --> 01:20:24.000]   And we get paralyzed by all this complexity and say, forget it.
[01:20:24.000 --> 01:20:29.160]   Well, this is where Latter enters the scene.
[01:20:29.160 --> 01:20:32.240]   Latter makes it easy to get life insurance.
[01:20:32.240 --> 01:20:35.560]   It's 100% digital, no doctors, no needles, and no paperwork.
[01:20:35.560 --> 01:20:39.320]   When you're applying for $3 million in coverage or less, you just answer a few questions about
[01:20:39.320 --> 01:20:41.760]   your health on an application.
[01:20:41.760 --> 01:20:44.800]   You need just a few minutes in a phone or laptop to apply.
[01:20:44.800 --> 01:20:50.600]   Latter smart algorithms work in real time, so you'll find out instantly if you're approved.
[01:20:50.600 --> 01:20:54.720]   No hidden fees, cancel any time, get a full refund if you change your mind in the first
[01:20:54.720 --> 01:20:56.720]   30 days.
[01:20:56.720 --> 01:21:00.200]   Life insurance is only going to cost more as you get older, so now is always the right
[01:21:00.200 --> 01:21:02.320]   time to get it.
[01:21:02.320 --> 01:21:08.760]   So go to ladderlife.com/deep today to see if you're instantly approved.
[01:21:08.760 --> 01:21:14.400]   That's LADDERLIFE.COM/DEEP.
[01:21:14.400 --> 01:21:15.400]   Ladderlife.com/deep.
[01:21:15.400 --> 01:21:20.500]   All right, Jesse, let's do something interesting.
[01:21:20.500 --> 01:21:24.400]   For those who are new to the show, this is the segment where I take something that you
[01:21:24.400 --> 01:21:31.360]   sent me to my interesting@calnewport.com email address that you thought I might find interesting.
[01:21:31.360 --> 01:21:35.460]   I take something that caught my attention and I share it.
[01:21:35.460 --> 01:21:37.180]   So here's something a lot of people sent me.
[01:21:37.180 --> 01:21:39.520]   This is actually from just a couple of days ago.
[01:21:39.520 --> 01:21:45.300]   I have it up on the screen now, so if you're listening, you can watch this at youtube.com/calnewportmedia
[01:21:45.300 --> 01:21:46.540]   episode 244.
[01:21:46.540 --> 01:22:00.820]   It is an article from NPR with the following music to my ears headline, NPR quits Twitter.
[01:22:00.820 --> 01:22:02.860]   There's a whole backstory to this.
[01:22:02.860 --> 01:22:08.620]   NPR is having a basically a feud with Twitter because Twitter started labeling the network
[01:22:08.620 --> 01:22:12.120]   as state affiliated media.
[01:22:12.120 --> 01:22:17.660]   The same description they use for propaganda outlets in Russia, China, and other autocratic
[01:22:17.660 --> 01:22:20.880]   countries that did not sit well with NPR.
[01:22:20.880 --> 01:22:24.520]   So then they changed it and said, okay, well, we'll call you government funded media.
[01:22:24.520 --> 01:22:28.800]   But NPR said only 1% of our annual budget comes from federal funding.
[01:22:28.800 --> 01:22:30.540]   That's not really accurate either.
[01:22:30.540 --> 01:22:31.540]   You know what?
[01:22:31.540 --> 01:22:32.540]   That's not enough.
[01:22:32.540 --> 01:22:33.540]   They're walking away.
[01:22:33.540 --> 01:22:39.140]   And they put NPR politics, for example, put out a tweet that said all of our 52 Twitter
[01:22:39.140 --> 01:22:42.380]   accounts, uh, we're not going to use them anymore.
[01:22:42.380 --> 01:22:47.540]   You want news for NPR, subscribe to our email newsletter, come to our website, listen to
[01:22:47.540 --> 01:22:48.740]   our radio program.
[01:22:48.740 --> 01:22:50.500]   We'll keep you up to date.
[01:22:50.500 --> 01:22:53.700]   You don't have to use this other guy's program.
[01:22:53.700 --> 01:22:59.780]   I really like to see that not because of the inter-scene political battles between Musk
[01:22:59.780 --> 01:23:03.180]   and the political, the different media outlets.
[01:23:03.180 --> 01:23:06.220]   I mean, I wish they would just make this move even without that, but whatever gets them
[01:23:06.220 --> 01:23:08.580]   there, I think is good.
[01:23:08.580 --> 01:23:12.580]   As I've talked about so many times on this show before, it is baked into the architecture
[01:23:12.580 --> 01:23:16.580]   of Twitter that it is going to generate outrage.
[01:23:16.580 --> 01:23:19.300]   It is going to manipulate your emotions.
[01:23:19.300 --> 01:23:26.500]   It is going to create tribalism and is going to really color your understanding of the
[01:23:26.500 --> 01:23:30.980]   world, your understanding of current events in ways that are highly inaccurate and often
[01:23:30.980 --> 01:23:32.260]   highly inflammatory.
[01:23:32.260 --> 01:23:39.700]   It's just built into the collaborative curation mechanism of all these retweets combined with
[01:23:39.700 --> 01:23:40.700]   the power law network.
[01:23:40.700 --> 01:23:42.620]   We've talked about this before.
[01:23:42.620 --> 01:23:47.340]   It's not a great way to consumer share information.
[01:23:47.340 --> 01:23:49.020]   Now more and more outlets are doing this.
[01:23:49.020 --> 01:23:54.580]   So a couple of weeks ago, we gave the example of the Washington post nationals baseball
[01:23:54.580 --> 01:23:58.980]   team coverage shifting away from live tweeting the games and instead having live updates
[01:23:58.980 --> 01:24:01.420]   on their Washington post.com website.
[01:24:01.420 --> 01:24:04.660]   And at the end of all those updates, then they write a recap article and it's all packaged
[01:24:04.660 --> 01:24:08.260]   together and you can see how it unfolded and they have more different people writing and
[01:24:08.260 --> 01:24:09.260]   it unfolds in real time.
[01:24:09.260 --> 01:24:10.740]   And I think the whole thing is great.
[01:24:10.740 --> 01:24:14.580]   There's no reason to be on someone else's platform, mixing tweets about the baseball
[01:24:14.580 --> 01:24:22.260]   game with tweets about, you know, the Martian that's going to come and destroy the earth
[01:24:22.260 --> 01:24:27.540]   because you didn't give it hydroxychloroquine or whatever the hell else is going on on Twitter.
[01:24:27.540 --> 01:24:28.540]   Twitter was a lot of fun.
[01:24:28.540 --> 01:24:29.540]   It's a lot of engaging.
[01:24:29.540 --> 01:24:30.740]   It's not very engaging.
[01:24:30.740 --> 01:24:32.540]   It's not the right way to consume news.
[01:24:32.540 --> 01:24:34.120]   It's not the right way to spread news.
[01:24:34.120 --> 01:24:35.780]   And I'm really happy about this trend.
[01:24:35.780 --> 01:24:39.620]   I think we would be in a much calmer place as individuals.
[01:24:39.620 --> 01:24:41.940]   I think we'd be in a much calmer place politically.
[01:24:41.940 --> 01:24:46.260]   I think we'd be in a much calmer place just from a civic perspective.
[01:24:46.260 --> 01:24:50.220]   If more and more sources of news, if more and more sources of expression, if more and
[01:24:50.220 --> 01:24:55.700]   more sources of commentary move to their own private gardens, here's my website, here's
[01:24:55.700 --> 01:25:02.180]   my podcast, here's my newsletter, not this giant mixing bowl where everyone and everything
[01:25:02.180 --> 01:25:05.940]   comes together in a homogenized interface.
[01:25:05.940 --> 01:25:10.740]   And we have this distributed curation mechanism rapidly amplifying some things versus others.
[01:25:10.740 --> 01:25:17.620]   That's not a healthy way for a society to learn about things and communicate.
[01:25:17.620 --> 01:25:21.300]   And I wrote about this in Wired Magazine early in the pandemic.
[01:25:21.300 --> 01:25:22.300]   I wrote an op-ed for Wired.
[01:25:22.300 --> 01:25:26.860]   I said one of the number one things we could do for public health right now at the beginning
[01:25:26.860 --> 01:25:29.380]   of the pandemic would be shut down Twitter.
[01:25:29.380 --> 01:25:35.500]   And I gave this argument in that article that, look, if professionals, if we retreated to
[01:25:35.500 --> 01:25:42.340]   websites, we could have considered long form articles with rich links in the other types
[01:25:42.340 --> 01:25:50.020]   of articles and sources, where the websites themselves, you could indicate authority by
[01:25:50.020 --> 01:25:54.340]   the fact that this website is hosted at a hospital or a known academic institution or
[01:25:54.340 --> 01:25:56.340]   a known news organization.
[01:25:56.340 --> 01:26:03.500]   We'd be able to curate on an individual sense of this website is at, the reference in old
[01:26:03.500 --> 01:26:09.900]   SNL skit, clownpenis.fart, and it has weird gifs of it of eagles that are flying away
[01:26:09.900 --> 01:26:10.900]   with Osama bin Laden.
[01:26:10.900 --> 01:26:15.340]   I'm not going to pay as much attention to that, to this long form article that's coming
[01:26:15.340 --> 01:26:16.960]   out of the Cleveland Clinic.
[01:26:16.960 --> 01:26:20.620]   And I said, if we went back, humans will be much better at consuming information.
[01:26:20.620 --> 01:26:23.700]   The information will be much better presented if we went back to more bespoke distributed
[01:26:23.700 --> 01:26:29.320]   communication, as opposed to having everyone, the cranks and the experts and the weirdos
[01:26:29.320 --> 01:26:32.580]   and everybody all mixed together in the exact same interface with the exact same, their
[01:26:32.580 --> 01:26:37.660]   tweets look exactly the same, and a completely dispassionate distributed curation mechanism,
[01:26:37.660 --> 01:26:41.420]   rapidly amplifying things that are catching people's attention.
[01:26:41.420 --> 01:26:42.760]   We need to get away from that automation.
[01:26:42.760 --> 01:26:44.340]   We need to get away from that distributed curation.
[01:26:44.340 --> 01:26:48.760]   And we get back to more bespoke things where we can learn from where you hosted, what does
[01:26:48.760 --> 01:26:49.760]   this look like?
[01:26:49.760 --> 01:26:50.760]   What are the texts?
[01:26:50.760 --> 01:26:52.140]   What are the things that have you written?
[01:26:52.140 --> 01:26:53.820]   We can really have context for information.
[01:26:53.820 --> 01:26:57.820]   Anyways, I don't want to go too far into this lecture, but basically this is a good trend.
[01:26:57.820 --> 01:27:06.940]   I think individually owned digital distribution of information, podcasts, websites, blogs,
[01:27:06.940 --> 01:27:10.860]   and news is going to be a much healthier trend than saying, why don't we all just have one
[01:27:10.860 --> 01:27:12.340]   or two services that everyone uses?
[01:27:12.340 --> 01:27:15.160]   So good job NPR.
[01:27:15.160 --> 01:27:19.100]   I hope more and more news sources follow your lead in the Washington Post's, Nationals,
[01:27:19.100 --> 01:27:20.100]   Reporters Leagues.
[01:27:20.100 --> 01:27:21.100]   I think this is the right direction.
[01:27:21.100 --> 01:27:23.820]   Do you know the clown penis dot fart reference?
[01:27:23.820 --> 01:27:24.820]   No.
[01:27:24.820 --> 01:27:26.180]   It was in the late nineties.
[01:27:26.180 --> 01:27:32.500]   It was a classic SNL skit and it was like an advertisement and they really had it.
[01:27:32.500 --> 01:27:35.180]   You know how they used to have those advertisements for brokerage firms?
[01:27:35.180 --> 01:27:40.700]   Where it's like super, I welcome, it's like Wilford Brimley, like welcome to such and
[01:27:40.700 --> 01:27:44.540]   such financial partners where trust is our number one, whatever.
[01:27:44.540 --> 01:27:49.740]   And so it's like this real serious ad and they're like, you can find out more at our
[01:27:49.740 --> 01:27:52.300]   website clownpenis.fart.
[01:27:52.300 --> 01:27:57.860]   And then they kind of reveal by the time we set up our website, it was the only address
[01:27:57.860 --> 01:27:58.860]   left.
[01:27:58.860 --> 01:28:02.100]   So it was like the premise of it, it was like this really serious financial brokerage firm,
[01:28:02.100 --> 01:28:05.540]   but in the late nineties, it felt like all the URLs were gone.
[01:28:05.540 --> 01:28:07.140]   And so it was like the only URL that was left.
[01:28:07.140 --> 01:28:10.900]   And so it was just a very serious commercial that they kept having to say clown penis dot
[01:28:10.900 --> 01:28:13.740]   fart, classic SNL.
[01:28:13.740 --> 01:28:15.900]   All right.
[01:28:15.900 --> 01:28:16.900]   I'm tired.
[01:28:16.900 --> 01:28:19.380]   Too much AI.
[01:28:19.380 --> 01:28:21.400]   I'm happy now to talk about AI whenever you want.
[01:28:21.400 --> 01:28:25.460]   My gag order has been lifted, but maybe it'll be a while until we get too much more deeply
[01:28:25.460 --> 01:28:26.460]   into that.
[01:28:26.460 --> 01:28:27.460]   But thank you all for listening.
[01:28:27.460 --> 01:28:28.460]   Thank you all for putting up with that.
[01:28:28.460 --> 01:28:32.660]   We'll be back next week with the next, hopefully much less computer science filled episode
[01:28:32.660 --> 01:28:33.660]   of the podcast.
[01:28:33.660 --> 01:28:36.300]   And until then, as always, stay deep.
[01:28:36.300 --> 01:28:39.960]   [MUSIC PLAYING]
[01:28:39.960 --> 01:28:41.520]   (techno music)

