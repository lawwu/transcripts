
[00:00:00.000 --> 00:00:04.720]   In the back of my head, I feel like I always have this mystery where a new version of Kudanen
[00:00:04.720 --> 00:00:11.440]   comes out and the models get way faster with just a better library.
[00:00:11.440 --> 00:00:17.400]   I think about what a model does, like a convolution or a matrix multiplication.
[00:00:17.400 --> 00:00:19.440]   It seems so simple to me.
[00:00:19.440 --> 00:00:24.840]   That's kind of how it seems because I feel like I come from a math background.
[00:00:24.840 --> 00:00:30.920]   I'm just like, how could many years in to making a library like Kudanen in a year, how
[00:00:30.920 --> 00:00:34.840]   could there be a 20% speed up on a matrix multiplication?
[00:00:34.840 --> 00:00:35.840]   What's going on?
[00:00:35.840 --> 00:00:36.840]   That's a brilliant question.
[00:00:36.840 --> 00:00:37.840]   Yeah, great question, Lukas.
[00:00:37.840 --> 00:00:41.160]   All right, so we should take a whiteboard out and I'll show it to you.
[00:00:41.160 --> 00:00:44.080]   Because then it gets even closer to my world.
[00:00:44.080 --> 00:00:46.560]   Let's think about computer architecture for a second.
[00:00:46.560 --> 00:00:52.520]   Let's say that you are an execution engine, like a processor or a core and a GPU.
[00:00:52.520 --> 00:00:53.520]   Let's start with one reason.
[00:00:53.520 --> 00:00:55.280]   You have to grab data from somewhere in memory.
[00:00:55.280 --> 00:01:00.880]   So it turns out that computers, memory is organized in ways that depending on where
[00:01:00.880 --> 00:01:06.000]   the data is in memory, which actual address physically in your memory it is, it gives
[00:01:06.000 --> 00:01:09.800]   you much better performance than others, by a huge margin.
[00:01:09.800 --> 00:01:14.720]   Because depending on how you lay it out, the data, you can actually make the most use of
[00:01:14.720 --> 00:01:18.840]   the wires between your memory and your processor, between your cache and your actual execution
[00:01:18.840 --> 00:01:20.680]   engine in the silicon itself.
[00:01:20.680 --> 00:01:25.640]   And figuring out where that goes becomes a combinatorial problem because not only you
[00:01:25.640 --> 00:01:29.220]   have to choose where the data structure go, but also when you have a bunch of nested loops
[00:01:29.220 --> 00:01:34.720]   that implements your convolution, you have to choose, like if you have a four deep nested
[00:01:34.720 --> 00:01:36.400]   loop, in which order should you execute them?
[00:01:36.400 --> 00:01:37.400]   Any order is valid.
[00:01:37.400 --> 00:01:38.400]   Which order should you execute them?
[00:01:38.400 --> 00:01:41.400]   And then within those, you might want to traverse, like what size of blocks are you going to
[00:01:41.400 --> 00:01:43.160]   traverse that?
[00:01:43.160 --> 00:01:47.720]   And all of that is highly dependent on the parameters of your convolution.
[00:01:47.720 --> 00:01:49.080]   I'm just speaking convolution, right?
[00:01:49.080 --> 00:01:52.360]   Or even just general matrix multiplication, right?
[00:01:52.360 --> 00:01:56.260]   So long story short, for any given operator, there's literally potentially billions of
[00:01:56.260 --> 00:02:01.000]   ways in which you can compile the same bit by bit equivalent program in terms of outputs.
[00:02:01.000 --> 00:02:05.040]   But one of them is going to be potentially a thousand times faster than this lowest one.
[00:02:05.040 --> 00:02:07.080]   So picking the right one is hard.
[00:02:07.080 --> 00:02:11.960]   Often this is done today by human intuition and some amount of automatic tuning called
[00:02:11.960 --> 00:02:12.960]   auto tuning, right?
[00:02:12.960 --> 00:02:17.480]   So what's happening in CUDNN as you see your model gets faster is that you have, NVIDIA
[00:02:17.480 --> 00:02:19.640]   can afford a large number of programmers, right?
[00:02:19.640 --> 00:02:22.880]   So a lot of really talented soft engineers, they observe where the models are going.
[00:02:22.880 --> 00:02:24.680]   There's some models that matters to them.
[00:02:24.680 --> 00:02:27.840]   They're going to go, they're going to look at the model, see the parameters of all of
[00:02:27.840 --> 00:02:29.080]   the operators, how they're stitched together.
[00:02:29.080 --> 00:02:31.840]   And then you're going to start tuning the libraries to make sure that they do better
[00:02:31.840 --> 00:02:36.160]   data layouts, they make better loop ordering, they do better tiling of how the data structure
[00:02:36.160 --> 00:02:37.160]   works.
[00:02:37.160 --> 00:02:40.000]   They choose the direction which they're traversing, the data structures and so on.
[00:02:40.000 --> 00:02:42.720]   And that's just one type, that's just one operator.
[00:02:42.720 --> 00:02:45.440]   But now models, operators talk to other operators, right?
[00:02:45.440 --> 00:02:47.080]   So that's why there's something called operator fusion.
[00:02:47.080 --> 00:02:51.720]   If you fuse two operators, for example, like a matrix multiplication, a convolution to
[00:02:51.720 --> 00:02:53.120]   a single operator.
[00:02:53.120 --> 00:02:57.240]   So now you can generate code in a way that it can keep data as close to your processing
[00:02:57.240 --> 00:02:58.440]   engine as much as possible.
[00:02:58.440 --> 00:03:01.520]   So you make much better use of your memory hierarchy.
[00:03:01.520 --> 00:03:04.180]   And that's yet another significant performance bump.
[00:03:04.180 --> 00:03:08.720]   If you're enjoying gradient descent, you might actually be interested in the main thing we
[00:03:08.720 --> 00:03:14.400]   do here at Weights & Biases, which is making tools to help with machine learning.
[00:03:14.400 --> 00:03:18.160]   If you're building models and you want to track the models that you build, or you want
[00:03:18.160 --> 00:03:21.920]   to track the data sets that go into the models you build, or you want to track the models
[00:03:21.920 --> 00:03:25.760]   that you deploy into production, you can do all that with Weights & Biases.
[00:03:25.760 --> 00:03:29.280]   And best of all, it's free for personal and academic use.
[00:03:29.280 --> 00:03:32.440]   Go check us out at WMP.com and let me know how it goes.

