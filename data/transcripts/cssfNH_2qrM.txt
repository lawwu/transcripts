
[00:00:00.000 --> 00:00:07.480]   It's a balance that you have to find between having enough novel content and knowing which
[00:00:07.480 --> 00:00:13.280]   users like more novel content and which users prefer to hear the same old songs.
[00:00:13.280 --> 00:00:17.640]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:17.640 --> 00:00:19.800]   and I'm your host, Lukas Biewald.
[00:00:19.800 --> 00:00:23.480]   Philip is a scientist at Pandora working on recommender systems.
[00:00:23.480 --> 00:00:28.200]   Before that, he was a PhD student working on deep neural networks for acoustic and language
[00:00:28.200 --> 00:00:31.480]   modeling applied to musical audio recordings.
[00:00:31.480 --> 00:00:36.640]   Amelia Naibaki is a software engineer at Pandora where she runs a team responsible for the
[00:00:36.640 --> 00:00:39.560]   production system that serves models to listeners.
[00:00:39.560 --> 00:00:41.640]   I'm super excited to talk to both of them.
[00:00:41.640 --> 00:00:46.880]   Maybe I could start by asking at Pandora, what do the machine learning models actually
[00:00:46.880 --> 00:00:51.520]   do and how would a Pandora user experience the results of the models?
[00:00:51.520 --> 00:00:56.760]   Well, we're a big company, so like Sirius Index and Pandora, we have a lot of different
[00:00:56.760 --> 00:01:01.520]   models that spread across all the product features.
[00:01:01.520 --> 00:01:06.880]   Almost every product, almost every digital product has some science background or some
[00:01:06.880 --> 00:01:09.880]   science model powering it.
[00:01:09.880 --> 00:01:15.480]   We have internal for content understanding, we have advertising models, we have, of course,
[00:01:15.480 --> 00:01:17.680]   our main feature at Pandora are recommendations.
[00:01:17.680 --> 00:01:24.120]   We have a lot of work done in recommendations and figuring out which songs to play next.
[00:01:24.120 --> 00:01:29.040]   Of course, having so many models interact with each other makes it quite a complicated
[00:01:29.040 --> 00:01:30.040]   scenario.
[00:01:30.040 --> 00:01:34.640]   So we need really powerful and strong engineering to make everything work smoothly and prevent
[00:01:34.640 --> 00:01:37.600]   outages and stuff like that.
[00:01:37.600 --> 00:01:41.080]   So my work mainly focuses on the musical side.
[00:01:41.080 --> 00:01:48.480]   So I work for basically the algorithmic programming team that focuses on creating the radio experience.
[00:01:48.480 --> 00:01:53.020]   And what I do is compute similarities between artists or tracks.
[00:01:53.020 --> 00:01:56.680]   So given this track, which tracks are similar and so on.
[00:01:56.680 --> 00:02:03.400]   And working at Pandora is the best place to do that because we have this awesome data
[00:02:03.400 --> 00:02:05.020]   that nobody else really has.
[00:02:05.020 --> 00:02:12.420]   So we employ a lot of music analysts who really listen to the songs and annotate them manually.
[00:02:12.420 --> 00:02:17.240]   So this is the dream of every PhD student in this field, like, "Oh, wow, I have actually
[00:02:17.240 --> 00:02:21.460]   experts looking at the data and annotating them."
[00:02:21.460 --> 00:02:24.400]   And also our users provide us feedback.
[00:02:24.400 --> 00:02:29.060]   And over time, we collected over a hundred billion thumbs up or thumbs down ratings,
[00:02:29.060 --> 00:02:31.360]   if users like the song or not.
[00:02:31.360 --> 00:02:37.680]   So we have very detailed and strong features on one side and very nice explicit feedback
[00:02:37.680 --> 00:02:38.680]   by users on the other side.
[00:02:38.680 --> 00:02:43.840]   This is the perfect scenario to create very powerful models.
[00:02:43.840 --> 00:02:49.480]   Do you have a model that's explicitly trying to understand the similarities between songs?
[00:02:49.480 --> 00:02:51.440]   Yeah, exactly.
[00:02:51.440 --> 00:02:55.360]   That's one of my projects I worked on last year.
[00:02:55.360 --> 00:02:57.760]   Basically this model connects.
[00:02:57.760 --> 00:03:04.000]   We had that all from the beginning, a model that tries to understand which songs are similar.
[00:03:04.000 --> 00:03:11.440]   But of course now with the deep learning revolution, we tried to replace the old models with more
[00:03:11.440 --> 00:03:17.480]   sophisticated neural networks that try to take the features we get from the music analysts
[00:03:17.480 --> 00:03:23.000]   and map them to what people think about similarity.
[00:03:23.000 --> 00:03:29.440]   Because it's not obvious which musical features actually make a song sound similar, right?
[00:03:29.440 --> 00:03:30.960]   Is it the tempo?
[00:03:30.960 --> 00:03:32.060]   Is it the mood?
[00:03:32.060 --> 00:03:36.000]   And by using machine learning, we're able to stop thinking about that and just make
[00:03:36.000 --> 00:03:39.440]   the computer do this work for us.
[00:03:39.440 --> 00:03:44.640]   I guess as a fan of music myself, I really wonder what the similarity really means, right?
[00:03:44.640 --> 00:03:48.760]   You probably long ago when you were famous for having all these annotators, you probably
[00:03:48.760 --> 00:03:52.440]   had to really think deeply about what similarity really means.
[00:03:52.440 --> 00:04:00.040]   Do you have a definition in your head of what makes two songs similar or not similar?
[00:04:00.040 --> 00:04:01.280]   I don't know.
[00:04:01.280 --> 00:04:04.040]   It's very hard to pinpoint that, right?
[00:04:04.040 --> 00:04:07.880]   When Pandora started, I don't know, maybe a million better when it started, it's like
[00:04:07.880 --> 00:04:10.440]   2000 and something.
[00:04:10.440 --> 00:04:11.440]   Something like that.
[00:04:11.440 --> 00:04:13.880]   That's when I first started to hear about it, yeah.
[00:04:13.880 --> 00:04:18.840]   So back then, what people would do is just compute distances manually.
[00:04:18.840 --> 00:04:25.120]   I would take the features and weight them and manually create an algorithm that tells
[00:04:25.120 --> 00:04:26.560]   us what's similar and what's not.
[00:04:26.560 --> 00:04:30.000]   And they would look how the lists look like.
[00:04:30.000 --> 00:04:32.920]   And this is like bootstrapping from nothing, right?
[00:04:32.920 --> 00:04:36.400]   But gradually, of course, we collected more and more feedback.
[00:04:36.400 --> 00:04:42.880]   We can replace that just by using models and we don't have to think about that that hard.
[00:04:42.880 --> 00:04:47.280]   So we have an idea of how this works, which I obviously share.
[00:04:47.280 --> 00:04:52.840]   But it turns out that if you run a model on that, it figures out a little bit better than
[00:04:52.840 --> 00:04:55.160]   humans could do before.
[00:04:55.160 --> 00:05:00.680]   I guess I would imagine though, one definition of similarity would just be these two songs,
[00:05:00.680 --> 00:05:03.120]   people that like one song, like the other song.
[00:05:03.120 --> 00:05:05.920]   Is that how you think of similarity or is it somehow deeper than that?
[00:05:05.920 --> 00:05:08.840]   These two songs are fundamentally similar songs.
[00:05:08.840 --> 00:05:13.640]   I would think a person who likes one recent top 40 song might like another recent top
[00:05:13.640 --> 00:05:16.600]   40 song, but they might be totally different genres.
[00:05:16.600 --> 00:05:21.600]   So then does your model try to say these two songs are similar or not similar?
[00:05:21.600 --> 00:05:24.120]   What does it do in that case?
[00:05:24.120 --> 00:05:27.320]   We are more focused on this radio experience, right?
[00:05:27.320 --> 00:05:33.640]   So you have like you select an artist, for example, or a track to start a radio from.
[00:05:33.640 --> 00:05:39.520]   And this is like maybe a more direct or specific way to define similarity.
[00:05:39.520 --> 00:05:44.440]   It's basically similarity is what kind of songs I can play on that radio.
[00:05:44.440 --> 00:05:52.160]   So if a person likes like charts music, they won't want to listen to, I don't know, some
[00:05:52.160 --> 00:05:56.080]   hip hop song on their, I don't know, dance radio.
[00:05:56.080 --> 00:06:01.800]   So we don't have to, in this case, we don't have to model like the taste of the user,
[00:06:01.800 --> 00:06:05.400]   although we do that, of course, for other things.
[00:06:05.400 --> 00:06:09.360]   But in terms of music similarity, we really think like, okay, how, which songs can we
[00:06:09.360 --> 00:06:11.280]   play on this radio station?
[00:06:11.280 --> 00:06:16.580]   So I would think that your model would need to look at both the sort of like musical elements
[00:06:16.580 --> 00:06:19.300]   of the song and kind of other things, right?
[00:06:19.300 --> 00:06:22.520]   Because our culture kind of affects our sense of similarity.
[00:06:22.520 --> 00:06:29.400]   Like what other things does your model look at besides just the audio of the song?
[00:06:29.400 --> 00:06:33.600]   Well, it's just as Emilia said, we have different models for different aspects of this, of this
[00:06:33.600 --> 00:06:35.600]   whole musical experience, right?
[00:06:35.600 --> 00:06:40.800]   We have models that are just based on the musical features.
[00:06:40.800 --> 00:06:43.420]   We have models that are just based on the audio.
[00:06:43.420 --> 00:06:47.520]   And this is like special because when you do recommendations, everybody, no matter if
[00:06:47.520 --> 00:06:54.740]   it's like Netflix or Pandora or whatever, you have like this long tail of unknown items
[00:06:54.740 --> 00:06:59.240]   that nobody really like, few people have listened to them.
[00:06:59.240 --> 00:07:02.160]   So we can't really understand from user interactions who would like them.
[00:07:02.160 --> 00:07:04.640]   So this is like the way we deal with that.
[00:07:04.640 --> 00:07:09.680]   We go from the content from like audio or from musical features.
[00:07:09.680 --> 00:07:15.220]   But for items that are for songs where we already gathered a lot of feedback, it's easy
[00:07:15.220 --> 00:07:19.280]   for us to just do the classic thing.
[00:07:19.280 --> 00:07:23.840]   Oh, somebody liked this and this song.
[00:07:23.840 --> 00:07:24.840]   They're similar to you.
[00:07:24.840 --> 00:07:25.960]   So maybe you also like them.
[00:07:26.680 --> 00:07:31.680]   Depending like if the song is very popular or not, we can then different recommenders
[00:07:31.680 --> 00:07:33.080]   work better or worse.
[00:07:33.080 --> 00:07:34.080]   Got it.
[00:07:34.080 --> 00:07:36.760]   And so what happens when the model improves?
[00:07:36.760 --> 00:07:42.320]   How would I as like a user of your product experience a better model?
[00:07:42.320 --> 00:07:46.000]   Would I notice like when you put out a new version that does better recommendations?
[00:07:46.000 --> 00:07:47.440]   Well we notice, right?
[00:07:47.440 --> 00:07:53.160]   Because we just, we have a very powerful like A/B testing framework that Emilia works a
[00:07:53.160 --> 00:07:54.840]   lot with, right?
[00:07:54.840 --> 00:07:59.400]   And when we create a new model and we add it to our ensemble of recommenders or we improve
[00:07:59.400 --> 00:08:03.960]   one of the models, we just deploy it very quickly in an A/B test.
[00:08:03.960 --> 00:08:06.800]   And after a few hours, we already get like results.
[00:08:06.800 --> 00:08:11.040]   So we see like, oh, people thumb up more or people spend more time listening.
[00:08:11.040 --> 00:08:12.900]   Emilia worked a lot with this A/B test, right?
[00:08:12.900 --> 00:08:15.560]   So you know a lot about that.
[00:08:15.560 --> 00:08:16.560]   Yeah.
[00:08:16.560 --> 00:08:22.240]   I would expect that you personally would not notice other than, oh, Pandora has been really
[00:08:22.240 --> 00:08:23.840]   getting it right today.
[00:08:23.840 --> 00:08:24.840]   Nice.
[00:08:24.840 --> 00:08:25.840]   Yeah.
[00:08:25.840 --> 00:08:31.720]   But we see things like listeners are thumbing more in one or another direction.
[00:08:31.720 --> 00:08:33.720]   They're spending more time on Pandora.
[00:08:33.720 --> 00:08:36.280]   They're creating more stations.
[00:08:36.280 --> 00:08:41.400]   We have a bunch of different things we can look at to see how we're affecting listener
[00:08:41.400 --> 00:08:42.400]   behavior.
[00:08:42.400 --> 00:08:43.400]   Okay.
[00:08:43.400 --> 00:08:45.800]   So maybe let's get a little more technical for our audience.
[00:08:45.800 --> 00:08:50.080]   I do have a zillion questions as a Pandora user, but the point of this is supposed to
[00:08:50.080 --> 00:08:52.200]   be around how you actually make these models.
[00:08:52.200 --> 00:08:55.120]   So do you actually like chain these models together?
[00:08:55.120 --> 00:08:58.520]   It sounds like you take the output of a lot of these different models and then use all
[00:08:58.520 --> 00:09:01.640]   those outputs to make decisions in your application.
[00:09:01.640 --> 00:09:03.560]   Yeah, absolutely.
[00:09:03.560 --> 00:09:07.880]   Everyone kind of talks about this scenario where like one model changes and it has unintended
[00:09:07.880 --> 00:09:08.880]   consequences.
[00:09:08.880 --> 00:09:10.680]   Like how do you deal with that?
[00:09:10.680 --> 00:09:12.360]   Like all the models connected together?
[00:09:12.360 --> 00:09:14.680]   That's a good question.
[00:09:14.680 --> 00:09:17.680]   One of the ways that we use models is during our song recommendation pipeline.
[00:09:17.680 --> 00:09:23.000]   The Ensemble Recommender System proposes a set of candidate songs and passes them to
[00:09:23.000 --> 00:09:28.160]   a microservice that handles the real-time evaluation of a machine learning model.
[00:09:28.160 --> 00:09:35.680]   And that machine learning model is like our larger overarching model that figures out
[00:09:35.680 --> 00:09:39.120]   how the other models are informing the decision.
[00:09:39.120 --> 00:09:44.320]   Let me see if I can repeat it back to you and tell me if I got this right.
[00:09:44.320 --> 00:09:48.400]   So it sounds like you have an ensemble model or kind of several models that take into account
[00:09:48.400 --> 00:09:51.920]   different things like maybe the actual audio quality of the song.
[00:09:51.920 --> 00:09:57.840]   And you mentioned sort of like non-audio features of the song and it proposes several songs
[00:09:57.840 --> 00:10:00.640]   that you might play next.
[00:10:00.640 --> 00:10:03.800]   And then you have another model that runs as like a microservice that looks at those
[00:10:03.800 --> 00:10:08.160]   options and maybe takes into account more things and decides the actual specific song
[00:10:08.160 --> 00:10:09.360]   that gets played.
[00:10:09.360 --> 00:10:10.960]   Yeah, that's exactly right.
[00:10:10.960 --> 00:10:19.000]   And some of the features coming into that final model are from the previous models,
[00:10:19.000 --> 00:10:21.640]   the models from the Ensemble Recommender.
[00:10:21.640 --> 00:10:26.840]   Do you have to retrain the microservice every time you deploy a new model upstream of it?
[00:10:26.840 --> 00:10:33.440]   I can tell you that the model that the microservice uses is retrained every day.
[00:10:33.440 --> 00:10:39.640]   So with fresh data and we have validation that runs to make sure that our results aren't
[00:10:39.640 --> 00:10:44.800]   totally wacky before we actually upload it to the microservice.
[00:10:44.800 --> 00:10:53.080]   And we have daily reports that show us maybe feature importances and the average values.
[00:10:53.080 --> 00:10:58.720]   So we can keep an eye on how the model is changing day to day.
[00:10:58.720 --> 00:11:02.680]   The nice thing about that is, for example, when I deployed my recommendation system last
[00:11:02.680 --> 00:11:09.280]   year, it's addictive because you look at the numbers every day and like, "Oh yeah, it got
[00:11:09.280 --> 00:11:10.280]   that many.
[00:11:10.280 --> 00:11:14.000]   I recommended that many songs and people liked it that much."
[00:11:14.000 --> 00:11:15.480]   It's really nice how easy that works.
[00:11:15.480 --> 00:11:21.560]   You just add a new model and after you wait a bit and the microservice pulls them in and
[00:11:21.560 --> 00:11:26.200]   selects them and it's really cool.
[00:11:26.200 --> 00:11:29.000]   And I guess, what do you use to keep track of all the versions?
[00:11:29.000 --> 00:11:31.520]   This is something a lot of our users are asking us all the time.
[00:11:31.520 --> 00:11:35.320]   How do you version your models and version the data that the models are trained on?
[00:11:35.320 --> 00:11:37.280]   How do you think about that?
[00:11:37.280 --> 00:11:42.040]   Yeah, everybody asks that because it's a very hard problem, right?
[00:11:42.040 --> 00:11:43.040]   Right.
[00:11:43.040 --> 00:11:45.760]   So if you could walk me through it as much detail as you can, how you do that, because
[00:11:45.760 --> 00:11:48.880]   I'm sure a lot of people are wondering.
[00:11:48.880 --> 00:11:50.560]   For code, it's pretty easy, right?
[00:11:50.560 --> 00:11:53.760]   Everybody uses Git and we use Git for basically everything.
[00:11:53.760 --> 00:12:01.280]   We have our own instance of a product that we use and all the code that trains the models
[00:12:01.280 --> 00:12:06.080]   and all the code that runs in production is on the server.
[00:12:06.080 --> 00:12:10.800]   Model versions, training, tracking model versions is way more difficult, especially during development,
[00:12:10.800 --> 00:12:11.800]   right?
[00:12:11.800 --> 00:12:14.960]   Because you run a lot of experiments, you try to compare them.
[00:12:14.960 --> 00:12:20.040]   What we did until recently, we were just like, everybody wrote their own libraries that stored
[00:12:20.040 --> 00:12:26.600]   the config somewhere, computed hashes and so you can track back if you want to find
[00:12:26.600 --> 00:12:27.960]   something.
[00:12:27.960 --> 00:12:28.960]   But that's a pain.
[00:12:28.960 --> 00:12:31.160]   That was really a pain.
[00:12:31.160 --> 00:12:38.960]   I would have an experiments directory where I had 200 sub directories with different experiments
[00:12:38.960 --> 00:12:46.080]   and I would have another Google Sheet somewhere that stores the names of the important experiments
[00:12:46.080 --> 00:12:50.520]   so I can know which models to use when I want to deploy them.
[00:12:50.520 --> 00:12:57.400]   And yeah, now since we use Ways and Biases, this got way easier because we just log our
[00:12:57.400 --> 00:13:02.280]   experiments, we can filter them easily, we can compare them very easily.
[00:13:02.280 --> 00:13:07.960]   It's like we store, for example, the learned weights there and just pull them when we need
[00:13:07.960 --> 00:13:11.080]   to and we decide, okay, this is the model we want to go with.
[00:13:11.080 --> 00:13:13.720]   Just download the model and that's it.
[00:13:13.720 --> 00:13:21.080]   So it's like all the keeping track of models during development has gotten much easier
[00:13:21.080 --> 00:13:22.080]   through that.
[00:13:22.080 --> 00:13:23.080]   I'm so glad to hear that.
[00:13:23.080 --> 00:13:26.160]   I appreciate the Ways and Biases shout out.
[00:13:26.160 --> 00:13:28.840]   But not trying to make this only about Ways and Biases.
[00:13:28.840 --> 00:13:33.360]   I guess another question we get all the time is what are the other tools that you use day
[00:13:33.360 --> 00:13:38.440]   to day to make your life easier as a machine learning practitioner?
[00:13:38.440 --> 00:13:43.280]   So I think this is more about the development part, where we create the model, we train
[00:13:43.280 --> 00:13:48.120]   and we look at the data and try to figure out what to do.
[00:13:48.120 --> 00:13:54.120]   Almost everybody I work with, we use IntelliJ for development because it's just this one
[00:13:54.120 --> 00:13:56.600]   IGE that rules them all.
[00:13:56.600 --> 00:13:58.280]   It has all the languages.
[00:13:58.280 --> 00:14:04.240]   We mostly work with Python for the experiments and then once we're done, we either use Python
[00:14:04.240 --> 00:14:11.720]   with PySpark or Scala with Spark to deploy the code in production.
[00:14:11.720 --> 00:14:16.360]   And with IntelliJ, all of this gets so much easier because it speaks all these languages.
[00:14:16.360 --> 00:14:19.360]   It has very nice plugins to connect to Google Cloud.
[00:14:19.360 --> 00:14:23.560]   This is the service we're using for almost anything now.
[00:14:23.560 --> 00:14:28.680]   We switched a few months ago and then also made our lives much easier.
[00:14:28.680 --> 00:14:36.440]   There's plugins where you can connect to a Dataproc cluster and inspect all the database
[00:14:36.440 --> 00:14:41.600]   schemas and tables and you get column completion when you write your SQL statements.
[00:14:41.600 --> 00:14:43.600]   That's just so incredible.
[00:14:43.600 --> 00:14:48.920]   The first time I saw that, I was like, "Wow, this changes everything."
[00:14:48.920 --> 00:14:50.640]   So yeah, mostly IntelliJ.
[00:14:50.640 --> 00:14:53.000]   Also a very nice thing is the remote debugging feature.
[00:14:53.000 --> 00:14:58.640]   You don't have to log in with SSH to your training server and try to debug in the command
[00:14:58.640 --> 00:14:59.640]   line.
[00:14:59.640 --> 00:15:01.240]   You just have a visual debugger.
[00:15:01.240 --> 00:15:04.800]   You can inspect the variables in there and run the code remotely still.
[00:15:04.800 --> 00:15:10.480]   So it's a pretty strong tool for me and makes my life much easier.
[00:15:10.480 --> 00:15:12.600]   Can you talk a little bit about how you debug models?
[00:15:12.600 --> 00:15:14.440]   This is another question everyone has.
[00:15:14.440 --> 00:15:16.900]   Can you walk me through your process a little bit?
[00:15:16.900 --> 00:15:21.800]   When that goes wrong and actually the performance goes down, what do you do?
[00:15:21.800 --> 00:15:26.960]   Well, then it's just going through the code and tracing back what you changed and what
[00:15:26.960 --> 00:15:28.560]   might have caused the problem.
[00:15:28.560 --> 00:15:34.000]   But I think it's more important to never come to this point.
[00:15:34.000 --> 00:15:37.000]   I try to do that by being a slow starter.
[00:15:37.000 --> 00:15:43.040]   So I don't try to write the most complex model right from the start.
[00:15:43.040 --> 00:15:44.040]   But I'll start slowly.
[00:15:44.040 --> 00:15:51.400]   First, get some of the data and then I try to make sure the data makes sense.
[00:15:51.400 --> 00:15:56.040]   So I try to select a small model, try to overfit the model on the small dataset.
[00:15:56.040 --> 00:15:57.040]   Is it possible?
[00:15:57.040 --> 00:16:02.000]   I change, for example, I randomize the features to make sure that there is, for example, no
[00:16:02.000 --> 00:16:08.960]   problem with train test splits or if the model actually produces garbage when I put garbage
[00:16:08.960 --> 00:16:10.280]   in because that should happen.
[00:16:10.280 --> 00:16:14.600]   There's a very nice blog post by Andrej Karpathy on that topic.
[00:16:14.600 --> 00:16:17.400]   It's called A Recipe for Training Neural Networks.
[00:16:17.400 --> 00:16:23.280]   And he's pretty good at what he's doing, so I'm just trying to follow this recipe as good
[00:16:23.280 --> 00:16:29.120]   as I can, making sure you understand the data, making actually sure that you don't evaluate
[00:16:29.120 --> 00:16:31.040]   something that you don't care about.
[00:16:31.040 --> 00:16:34.520]   You should make sure that the numbers you get actually reflect what you want to see
[00:16:34.520 --> 00:16:36.520]   in the end.
[00:16:36.520 --> 00:16:38.360]   And yeah, that's basically it.
[00:16:38.360 --> 00:16:44.080]   Just being very defensive with your development and checking things again and again.
[00:16:44.080 --> 00:16:47.640]   It's difficult to debug models because there is no right way.
[00:16:47.640 --> 00:16:51.960]   And if you make a bug in your neural network training code, it will still mostly work.
[00:16:51.960 --> 00:16:54.520]   It's not like it will crash and burn.
[00:16:54.520 --> 00:16:55.760]   It will work, but worse.
[00:16:55.760 --> 00:16:56.760]   Right.
[00:16:56.760 --> 00:17:02.600]   Do you have any bugs that come to mind as particularly difficult or ones that you've
[00:17:02.600 --> 00:17:06.600]   struggled with for a long time?
[00:17:06.600 --> 00:17:12.560]   I was training an embedding network that uses the triplet loss and you have to select positives
[00:17:12.560 --> 00:17:14.920]   and negatives, right?
[00:17:14.920 --> 00:17:18.800]   And the data was stored as a sparse matrix.
[00:17:18.800 --> 00:17:24.840]   So you had a matrix, which items are connected to which is a ground truth.
[00:17:24.840 --> 00:17:29.040]   So it's very easy to understand which positives to sample for a given item because there is
[00:17:29.040 --> 00:17:30.040]   a one in the matrix.
[00:17:30.040 --> 00:17:33.240]   Wait, sorry, I'm not an expert in this space.
[00:17:33.240 --> 00:17:34.240]   What does a one mean here?
[00:17:34.240 --> 00:17:35.240]   That they're connected.
[00:17:35.240 --> 00:17:40.920]   Say, for example, you have tracks and which ones are the same, for example.
[00:17:40.920 --> 00:17:47.960]   But the problem was that when you mask that, because when you do a train test, you have
[00:17:47.960 --> 00:17:49.520]   to mask that matrix.
[00:17:49.520 --> 00:17:55.080]   You don't use any data from the test set in your training set.
[00:17:55.080 --> 00:18:03.080]   But the problem then is that you don't know whether a zero, like no entry means that it's
[00:18:03.080 --> 00:18:09.200]   masked because it's in a different split or whether there is actually no connections.
[00:18:09.200 --> 00:18:17.280]   What ended up happening is that I didn't sample all the negatives that were possible.
[00:18:17.280 --> 00:18:23.080]   And this of course makes your training harder because you're not using all the data.
[00:18:23.080 --> 00:18:24.080]   Yeah.
[00:18:24.080 --> 00:18:27.680]   And finding that out was pretty tough because it still works.
[00:18:27.680 --> 00:18:28.680]   Right.
[00:18:28.680 --> 00:18:29.680]   All right.
[00:18:29.680 --> 00:18:32.480]   So maybe this is a question for both of you, actually.
[00:18:32.480 --> 00:18:35.840]   This is a question that comes up a lot that people always want me to ask is, how do you
[00:18:35.840 --> 00:18:40.960]   communicate the progress you're making with the non-technical people outside of your team,
[00:18:40.960 --> 00:18:41.960]   but in the company?
[00:18:41.960 --> 00:18:51.240]   At least for the system that I'm working on, we have weekly meetings with our PM who communicates
[00:18:51.240 --> 00:18:52.760]   up the ladder.
[00:18:52.760 --> 00:18:58.920]   We occasionally, end of year, maybe end of quarter, will present to the broader product
[00:18:58.920 --> 00:19:05.080]   organization what changes we've been making and how they've been affecting our core metrics.
[00:19:05.080 --> 00:19:09.480]   I think sometimes people tell me that they have this experience where other teams are
[00:19:09.480 --> 00:19:13.280]   kind of working on engineering projects with sort of like add these features that are very
[00:19:13.280 --> 00:19:18.560]   visible, but the stuff that both of you are working on can feel more experimental and
[00:19:18.560 --> 00:19:22.080]   there can be long periods of time where the experiments aren't working and that can be
[00:19:22.080 --> 00:19:23.080]   frustrating.
[00:19:23.080 --> 00:19:26.360]   Is that consistent with your experience or not?
[00:19:26.360 --> 00:19:32.560]   Well, luckily our direct managers, at least in my case, I think in the science department,
[00:19:32.560 --> 00:19:39.160]   every manager used to be a scientist in his previous life, so they know how science works
[00:19:39.160 --> 00:19:44.440]   and you can make a lot of progress in a few weeks and you can be stuck for a month and
[00:19:44.440 --> 00:19:48.280]   just iterate and experiment and nothing really works.
[00:19:48.280 --> 00:19:53.360]   The good thing is because of all this infrastructure we have, the microservice Amelia was talking
[00:19:53.360 --> 00:19:59.080]   about, and we can actually trace back every thumb or every song of somebody like to all
[00:19:59.080 --> 00:20:01.120]   the individual models.
[00:20:01.120 --> 00:20:08.880]   So in the end, after like say every quarter of a year, we can actually put a number on
[00:20:08.880 --> 00:20:14.700]   how many more thumbs we get because of this contribution and how much more time people
[00:20:14.700 --> 00:20:18.160]   actually spend up listening to Pandora.
[00:20:18.160 --> 00:20:28.480]   And since Pandora is a ad based service, that translates very well to money.
[00:20:28.480 --> 00:20:30.160]   Sure.
[00:20:30.160 --> 00:20:33.040]   That makes sense.
[00:20:33.040 --> 00:20:34.200]   Okay.
[00:20:34.200 --> 00:20:39.400]   Another question for both of you is how do you think about tuning and improving your
[00:20:39.400 --> 00:20:40.400]   models?
[00:20:40.400 --> 00:20:44.200]   Do you do that kind of hyper parameter search that a lot of people talk about or is it more
[00:20:44.200 --> 00:20:46.160]   intuitive or is it more structured?
[00:20:46.160 --> 00:20:47.160]   Yeah.
[00:20:47.160 --> 00:20:51.880]   I think when it comes to hyper parameter search, it's like a hybrid, right?
[00:20:51.880 --> 00:20:56.400]   Because we deal with similar problems all the time, oftentimes you already have a good
[00:20:56.400 --> 00:21:02.680]   guess how the model should look like to get reasonable results.
[00:21:02.680 --> 00:21:05.560]   And most of the time, this is just like where I start.
[00:21:05.560 --> 00:21:12.000]   I would just try five different configurations to see how big or how small I can go and then
[00:21:12.000 --> 00:21:14.760]   just settle with a model that works well enough and that's it.
[00:21:14.760 --> 00:21:18.640]   And then I just keep iterating on different things like, okay, which kind of other features
[00:21:18.640 --> 00:21:20.240]   can I use?
[00:21:20.240 --> 00:21:24.720]   Can I pull other data and integrate it somehow?
[00:21:24.720 --> 00:21:31.680]   And once all of this is done, once I'm quite confident, okay, this is the structure, the
[00:21:31.680 --> 00:21:34.680]   model structure that it'll probably work with.
[00:21:34.680 --> 00:21:39.500]   These days with weights and biases, we just create this hyper parameter sweep.
[00:21:39.500 --> 00:21:41.800]   You don't have to change anything in your code.
[00:21:41.800 --> 00:21:47.920]   You start it on a Friday, it runs for over the weekend or longer depending on the size
[00:21:47.920 --> 00:21:51.380]   of the model and then you're done.
[00:21:51.380 --> 00:21:56.640]   So it just saves a lot of headache if you can run this automatically and without much
[00:21:56.640 --> 00:21:57.640]   thinking.
[00:21:57.640 --> 00:22:01.760]   And so you spend most of your time, it sounds like, thinking about more data you could get
[00:22:01.760 --> 00:22:06.760]   or different features you could try than the hyper parameters?
[00:22:06.760 --> 00:22:10.160]   Most of the time, yeah, honestly, because that's where...
[00:22:10.160 --> 00:22:16.120]   This is like a difference between working at academia when I was doing my PhD and actually
[00:22:16.120 --> 00:22:17.120]   working in industry.
[00:22:17.120 --> 00:22:20.980]   Whereas in academia, you have like, okay, this is the dataset that's the standard in
[00:22:20.980 --> 00:22:22.980]   this field.
[00:22:22.980 --> 00:22:29.900]   You take it and you try to improve, you try to create a new method or whatever.
[00:22:29.900 --> 00:22:35.780]   But at an industry like Pandora, it's like, okay, we want to solve that problem.
[00:22:35.780 --> 00:22:37.740]   Here's all the data we have, solve it.
[00:22:37.740 --> 00:22:42.300]   So you have to think about, okay, which data makes sense?
[00:22:42.300 --> 00:22:44.580]   What kind of data makes sense to use?
[00:22:44.580 --> 00:22:49.120]   Which biases would that induce when I use that data?
[00:22:49.120 --> 00:22:54.920]   So thinking about which data to use, how to clean it up, because that's a big problem.
[00:22:54.920 --> 00:22:59.960]   Data is, of course, if you work with real data, you have outliers, you have some problems
[00:22:59.960 --> 00:23:01.440]   here and there.
[00:23:01.440 --> 00:23:05.960]   I spent a lot of time, much more time thinking about data since I started working in industry
[00:23:05.960 --> 00:23:06.960]   than before.
[00:23:06.960 --> 00:23:07.960]   Definitely.
[00:23:07.960 --> 00:23:08.960]   All right.
[00:23:08.960 --> 00:23:11.360]   Well, maybe let's talk a little more about production.
[00:23:11.360 --> 00:23:14.960]   You started to talk about the microservices, but I'd love to hear more about how you actually
[00:23:14.960 --> 00:23:17.560]   serve the models in production.
[00:23:17.560 --> 00:23:24.680]   Yeah, we're generating our production models in GCP and then we upload them to Redis, which
[00:23:24.680 --> 00:23:26.360]   is a key value store.
[00:23:26.360 --> 00:23:28.600]   And that's where the microservice could read them.
[00:23:28.600 --> 00:23:33.080]   And then to avoid having to go to Redis every time we need a model, we stash them in a Guava
[00:23:33.080 --> 00:23:37.860]   cache on heap because at least the models that I work with, we're using them every time
[00:23:37.860 --> 00:23:40.640]   there's a request for more songs on a listener station.
[00:23:40.640 --> 00:23:43.000]   So that's so often.
[00:23:43.000 --> 00:23:46.640]   Are these deep learning models or simpler?
[00:23:46.640 --> 00:23:52.200]   I think the model you're talking about, the microservice, is not just because it has to
[00:23:52.200 --> 00:23:54.560]   serve a lot of requests real time.
[00:23:54.560 --> 00:23:58.840]   So you just can't afford to run a complicated deep learning model at this point.
[00:23:58.840 --> 00:24:03.840]   The recommenders in the ensemble, there are a few deep learning models there, of course.
[00:24:03.840 --> 00:24:08.420]   But for the final selection of the track, I think the models provide enough features
[00:24:08.420 --> 00:24:11.880]   and enough candidates to just have a simpler model like that.
[00:24:11.880 --> 00:24:12.880]   Cool, cool.
[00:24:12.880 --> 00:24:13.880]   I see.
[00:24:13.880 --> 00:24:16.440]   So there's sort of like bigger models that run in batch mode where they don't have to
[00:24:16.440 --> 00:24:17.440]   be real time.
[00:24:17.440 --> 00:24:18.440]   Is that right?
[00:24:18.440 --> 00:24:22.160]   And then the final model you talked about has to do real time, so it's lighter weight?
[00:24:22.160 --> 00:24:23.160]   Yeah.
[00:24:23.160 --> 00:24:24.800]   And we definitely have had the experience.
[00:24:24.800 --> 00:24:29.760]   We've tried increasing the size of the model and had to pull that back because it wasn't
[00:24:29.760 --> 00:24:31.420]   performance enough.
[00:24:31.420 --> 00:24:34.400]   Definitely performance is something that we're always keeping in mind.
[00:24:34.400 --> 00:24:36.680]   We don't want the user to wait around.
[00:24:36.680 --> 00:24:39.400]   So what are the hard parts about getting that model into production?
[00:24:39.400 --> 00:24:42.480]   What are the day to day challenges?
[00:24:42.480 --> 00:24:46.880]   Yeah, efficiency is the biggest thing that I worry about, that latency again.
[00:24:46.880 --> 00:24:52.440]   We're always trying out new changes, things like Philip mentioned, like adding new features.
[00:24:52.440 --> 00:24:55.260]   Sometimes we'll try partitioning listeners in a different way.
[00:24:55.260 --> 00:25:00.480]   So sending different listeners to different models or changing the size of the model.
[00:25:00.480 --> 00:25:08.640]   And sometimes those changes will look really promising offline, but then we try it in production
[00:25:08.640 --> 00:25:11.840]   and we'll see that it's too costly computationally.
[00:25:11.840 --> 00:25:15.960]   Yeah, we're getting hundreds of thousands of requests every minute.
[00:25:15.960 --> 00:25:16.960]   We got to be super fast.
[00:25:16.960 --> 00:25:23.040]   I'm curious, have you always been deploying a new model every day or is that a new process
[00:25:23.040 --> 00:25:24.040]   for you?
[00:25:24.040 --> 00:25:28.040]   For the last couple of years, we've been doing that every day.
[00:25:28.040 --> 00:25:31.360]   Definitely we'll skip days if we don't pass validation that day.
[00:25:31.360 --> 00:25:37.000]   And then somebody will go and look and make sure that it's reasonable or see if we need
[00:25:37.000 --> 00:25:38.000]   to make changes.
[00:25:38.000 --> 00:25:40.920]   But yeah, we're staying pretty up to date.
[00:25:40.920 --> 00:25:42.400]   And I guess what causes that?
[00:25:42.400 --> 00:25:47.920]   Do the songs change every day or the songs people like change every day?
[00:25:47.920 --> 00:25:49.000]   Songs can change every day.
[00:25:49.000 --> 00:25:56.320]   Yeah, I think mostly we just want to make sure we have the latest data, the latest thumbs,
[00:25:56.320 --> 00:26:03.520]   the latest completion rates, the latest way that listeners are reacting to songs.
[00:26:03.520 --> 00:26:04.600]   I see.
[00:26:04.600 --> 00:26:08.200]   So I guess you sort of don't have to worry as much about kind of data drift because you're
[00:26:08.200 --> 00:26:11.400]   retraining every day on the latest data.
[00:26:11.400 --> 00:26:15.920]   Yeah, I suppose that that's less of an issue for the particular model that I'm working
[00:26:15.920 --> 00:26:17.880]   with.
[00:26:17.880 --> 00:26:20.760]   Do you have a production monitoring place for that model?
[00:26:20.760 --> 00:26:25.100]   Do you look for signals that bad things might be happening?
[00:26:25.100 --> 00:26:30.300]   We certainly have dashboards that monitor things like number of requests and latencies
[00:26:30.300 --> 00:26:35.320]   and CPU thread counts, things like that.
[00:26:35.320 --> 00:26:42.040]   But mainly the way that we monitor things are those A/B tests where we're pretty confident
[00:26:42.040 --> 00:26:45.120]   that our control model is pretty darn good.
[00:26:45.120 --> 00:26:49.120]   Any changes that we're making, we're comparing against the control model.
[00:26:49.120 --> 00:26:50.360]   I see.
[00:26:50.360 --> 00:26:54.680]   How many A/B tests, or in a magazine, how many A/B tests can you run in parallel?
[00:26:54.680 --> 00:26:56.480]   I'm jealous of how many users you have.
[00:26:56.480 --> 00:27:00.480]   It must be amazing to get that data.
[00:27:00.480 --> 00:27:01.480]   Yeah.
[00:27:01.480 --> 00:27:07.920]   Our particular group is running tens, probably, maybe hundreds if you look at our broad product
[00:27:07.920 --> 00:27:15.200]   area and then I think thousands if you're looking at the whole company.
[00:27:15.200 --> 00:27:16.600]   Wow.
[00:27:16.600 --> 00:27:21.960]   Is it tricky to swap models in and out in production or is that simple for you?
[00:27:21.960 --> 00:27:23.800]   It's simple mechanically.
[00:27:23.800 --> 00:27:32.320]   We can just overwrite the value in the cache, but in practice we're a lot more careful.
[00:27:32.320 --> 00:27:35.840]   We always run an A/B test.
[00:27:35.840 --> 00:27:40.600]   We never swap anything in without making sure that it's moving metrics in the right way
[00:27:40.600 --> 00:27:43.640]   and not degrading the experience for the users.
[00:27:43.640 --> 00:27:48.080]   But yeah, mechanically it's really simple.
[00:27:48.080 --> 00:27:53.360]   Can you talk about how you take a model, the steps that you go through from taking a model
[00:27:53.360 --> 00:27:59.520]   from experimental to it's the model that's blessed as the one that runs by default in
[00:27:59.520 --> 00:28:02.880]   production?
[00:28:02.880 --> 00:28:10.360]   I can speak of the recommendation models that we use for stations and that's actually also
[00:28:10.360 --> 00:28:17.360]   not that complicated because in the end what you do is you experiment with the model and
[00:28:17.360 --> 00:28:21.640]   then at some point say you think, "Okay, this is the model I want to use."
[00:28:21.640 --> 00:28:28.920]   So then what I would do is just translate that model into an Airflow deck where it can
[00:28:28.920 --> 00:28:35.680]   run weekly or daily or however often I think it's necessary.
[00:28:35.680 --> 00:28:43.680]   And in the easiest case, I would just produce a table on GCP with recommendations.
[00:28:43.680 --> 00:28:48.400]   And this table gets then pulled just by, I'll just ping an engineer and say, "Hey, there
[00:28:48.400 --> 00:28:50.640]   is a new model around.
[00:28:50.640 --> 00:28:52.200]   Look at this table."
[00:28:52.200 --> 00:29:00.400]   And they will pull it into this ensemble where all the candidates are being pulled together.
[00:29:00.400 --> 00:29:06.520]   And for a certain number of users, the microservice will then pick songs by that model.
[00:29:06.520 --> 00:29:09.440]   And in the beginning, it's just this very small percentage, of course.
[00:29:09.440 --> 00:29:15.480]   So we don't throw this new model at all the users because we don't know how it behaves.
[00:29:15.480 --> 00:29:23.080]   So we would then say, try it on 1% of the users and observe the numbers.
[00:29:23.080 --> 00:29:24.880]   Do they like the new recommendations?
[00:29:24.880 --> 00:29:29.280]   Do they thumb up songs recommended by this recommender?
[00:29:29.280 --> 00:29:33.600]   And also, does it make sense to add this recommender to the ensemble at all?
[00:29:33.600 --> 00:29:41.120]   Because maybe it recommends awesome songs, but it doesn't add anything to the mix.
[00:29:41.120 --> 00:29:42.320]   And that's basically it.
[00:29:42.320 --> 00:29:43.320]   And then it's in production.
[00:29:43.320 --> 00:29:44.320]   Okay.
[00:29:44.320 --> 00:29:47.120]   Well, we always end with these questions.
[00:29:47.120 --> 00:29:49.640]   I'd love for both of you to weigh in if you feel comfortable.
[00:29:49.640 --> 00:29:54.300]   The first one is, what's an underrated aspect of machine learning that you think people
[00:29:54.300 --> 00:29:59.240]   should pay more attention to than they do?
[00:29:59.240 --> 00:30:06.360]   Well, the thing that I always think of is maybe not that directly technically related
[00:30:06.360 --> 00:30:13.000]   to machine learning, but in general, it's ethics and diversity and equality.
[00:30:13.000 --> 00:30:17.960]   That's a topic that comes up sometimes now in machine learning and it's getting more
[00:30:17.960 --> 00:30:25.120]   prominent, but I still don't think it's enough because we are just creating all these models
[00:30:25.120 --> 00:30:34.640]   that do very seemingly smart stuff, but few people actually look at, okay, what are the
[00:30:34.640 --> 00:30:38.320]   consequences of these things?
[00:30:38.320 --> 00:30:43.520]   And even some figureheads from academia and industry, they're saying, "Oh, the models,
[00:30:43.520 --> 00:30:44.520]   they're not biased.
[00:30:44.520 --> 00:30:49.280]   We don't really have to care that much about that because the models just, they're neutral."
[00:30:49.280 --> 00:30:51.240]   And it's kind of right, right?
[00:30:51.240 --> 00:30:56.320]   The model has no bias, but it learns the bias from training data.
[00:30:56.320 --> 00:31:01.160]   And the training data we use is stuff that's happening right now or that happened the last
[00:31:01.160 --> 00:31:03.360]   10, 20 years.
[00:31:03.360 --> 00:31:07.440]   So what the model learns is to reproduce that bias.
[00:31:07.440 --> 00:31:16.400]   And I don't see a way to really tackle that from a data perspective, just because let's
[00:31:16.400 --> 00:31:23.360]   take the new GPT-3 model, the language model developed by OpenAI was trained on 410 billion
[00:31:23.360 --> 00:31:24.360]   tokens.
[00:31:24.360 --> 00:31:28.720]   How do you change the training data in a way that it doesn't produce, I don't know, gender
[00:31:28.720 --> 00:31:30.680]   bias or racial bias?
[00:31:30.680 --> 00:31:34.680]   It's impossible, I think.
[00:31:34.680 --> 00:31:45.040]   I think we have to think very carefully about how we use these models and how can we integrate
[00:31:45.040 --> 00:31:51.160]   some way of human decision-making in the whole process and not just blindly trust whatever
[00:31:51.160 --> 00:31:52.160]   the model says.
[00:31:52.160 --> 00:31:55.280]   Can I ask, does that come up at Pandora?
[00:31:55.280 --> 00:32:01.080]   I feel like you have in some ways, this really wonderful kind of fun application of machine
[00:32:01.080 --> 00:32:05.120]   learning and maybe one of the few places where there might be less ethical concerns.
[00:32:05.120 --> 00:32:09.080]   I can imagine some, but do you think about it day to day?
[00:32:09.080 --> 00:32:13.560]   Well, the reason why I got into music and machine learning is that very reason, because
[00:32:13.560 --> 00:32:20.760]   it's just a very, I know it's hard to do bad things in music, but actually we have some
[00:32:20.760 --> 00:32:22.680]   discussions about that.
[00:32:22.680 --> 00:32:25.920]   For example, let me give you two examples.
[00:32:25.920 --> 00:32:28.680]   One is what we call popularity bias.
[00:32:28.680 --> 00:32:32.840]   It's known that basically all recommendation models suffer from popularity bias, meaning
[00:32:32.840 --> 00:32:38.000]   that they recommend popular items more often.
[00:32:38.000 --> 00:32:45.000]   Most of them actually recommend popular items more often than their popularity would suggest.
[00:32:45.000 --> 00:32:49.000]   It's even like they even strengthen, they reinforce the whole thing.
[00:32:49.000 --> 00:32:51.360]   Right, because it's like a safe choice maybe?
[00:32:51.360 --> 00:32:52.360]   Exactly.
[00:32:52.360 --> 00:32:57.520]   It's actually quite hard to beat a recommender that just plays the most popular songs.
[00:32:57.520 --> 00:32:58.520]   Right, right.
[00:32:58.520 --> 00:33:00.720]   Looking just at the numbers.
[00:33:00.720 --> 00:33:06.240]   So we have some functionality included in our, maybe not in the individual models, but
[00:33:06.240 --> 00:33:11.640]   at the end we try to diversify artists.
[00:33:11.640 --> 00:33:19.160]   We try to boost artists that are not very popular because it basically helps everybody.
[00:33:19.160 --> 00:33:21.200]   It helps the user to find new artists.
[00:33:21.200 --> 00:33:27.960]   It helps the artists to get more exposure that they wouldn't get otherwise.
[00:33:27.960 --> 00:33:32.320]   And so I think it's a good thing to do basically.
[00:33:32.320 --> 00:33:36.680]   Plus some of the recommenders, as I said, are just looking purely at the musical information.
[00:33:36.680 --> 00:33:37.680]   Right?
[00:33:37.680 --> 00:33:44.680]   So how does the song sound like, or what characteristics the analysts annotated?
[00:33:44.680 --> 00:33:51.000]   This is just a way to try songs that don't have much feedback data and are hard to recommend
[00:33:51.000 --> 00:33:52.960]   otherwise.
[00:33:52.960 --> 00:33:58.360]   And another thing that we recently started discussing and we intend to explore further
[00:33:58.360 --> 00:34:05.600]   is we found that for some genre stations, we have a very imbalanced distribution between
[00:34:05.600 --> 00:34:08.040]   male and female artists.
[00:34:08.040 --> 00:34:14.920]   And of course, nobody at Pandora decided to make, I don't know, make the country radio
[00:34:14.920 --> 00:34:17.080]   only play male artists.
[00:34:17.080 --> 00:34:23.280]   But this is what just happened because we look at what people listen to.
[00:34:23.280 --> 00:34:28.000]   And we always take care that every listener gets what they want to listen to.
[00:34:28.000 --> 00:34:32.160]   So if somebody just likes hearing male voices, they will just get male artists.
[00:34:32.160 --> 00:34:36.200]   And if somebody just likes female voices, they will get female artists.
[00:34:36.200 --> 00:34:43.120]   But we were discussing about how can we create a better balance of new female artists, pushing
[00:34:43.120 --> 00:34:47.520]   them more in this kind of scenarios where we have a strong imbalance here.
[00:34:47.520 --> 00:34:49.000]   Well, that's really cool.
[00:34:49.000 --> 00:34:52.840]   Amelia, do you have any thoughts on that topic?
[00:34:52.840 --> 00:34:55.240]   I really appreciate that Philip is thinking about it.
[00:34:55.240 --> 00:35:03.960]   I have noticed that the music that I tend to like is male artists, but I, as a woman,
[00:35:03.960 --> 00:35:05.960]   would like to support female artists.
[00:35:05.960 --> 00:35:09.760]   And I would like to be able to find female artists that I enjoy.
[00:35:09.760 --> 00:35:16.640]   And I would like to see that promotion of female artists happen in Pandora.
[00:35:16.640 --> 00:35:24.720]   We do things like in the product that will try to offset some of that imbalance.
[00:35:24.720 --> 00:35:29.720]   For instance, during Women's History Month last year, we created personalized playlists
[00:35:29.720 --> 00:35:34.400]   for our premium users that were only female artists.
[00:35:34.400 --> 00:35:36.600]   My playlist was very good.
[00:35:36.600 --> 00:35:37.600]   Nice.
[00:35:37.600 --> 00:35:42.400]   We did share it in the show notes.
[00:35:42.400 --> 00:35:43.400]   Yeah.
[00:35:43.400 --> 00:35:50.960]   And we do things too, like Black History Month, we had some personalized, I think, Pandora
[00:35:50.960 --> 00:35:53.080]   stories that we shared out.
[00:35:53.080 --> 00:35:54.080]   Yeah.
[00:35:54.080 --> 00:36:01.560]   So we're definitely trying to make a small bit of difference in that bias.
[00:36:01.560 --> 00:36:04.720]   This is a pretty broad question, but do you have any thoughts on...
[00:36:04.720 --> 00:36:10.000]   I feel like sometimes these recommendation systems and machine learning in general gets
[00:36:10.000 --> 00:36:13.360]   a knock for optimizing for our reptile brains.
[00:36:13.360 --> 00:36:17.840]   I could see that with Pandora maybe wanting in the short term to hear the same songs over
[00:36:17.840 --> 00:36:23.920]   and over, but in a more, I don't know, higher brain sense, like wanting to be exposed to
[00:36:23.920 --> 00:36:24.920]   new music.
[00:36:24.920 --> 00:36:27.520]   Do either of you think about that day to day?
[00:36:27.520 --> 00:36:33.760]   Do you feel like it's possible to over-optimize for a thumbs up or a listen times?
[00:36:33.760 --> 00:36:34.760]   Definitely.
[00:36:34.760 --> 00:36:35.760]   Yeah.
[00:36:35.760 --> 00:36:38.120]   But it's something that we always have in mind.
[00:36:38.120 --> 00:36:46.040]   So of course, the direct metrics are time spent listening and so on, but we definitely
[00:36:46.040 --> 00:36:50.960]   hear users saying, "Okay, there's just too much repetition."
[00:36:50.960 --> 00:36:55.920]   And then we just try to...
[00:36:55.920 --> 00:36:59.960]   This is something that's very hard to measure in a very direct way.
[00:36:59.960 --> 00:37:06.440]   And what Amelia said is that if you just blindly reduce repetition, because it's an easy thing
[00:37:06.440 --> 00:37:10.280]   to do, it tends to annoy some people.
[00:37:10.280 --> 00:37:15.520]   It's a balance that you have to find between having enough novel content and knowing which
[00:37:15.520 --> 00:37:21.440]   users like more novel content and which users prefer to hear the same old songs all the
[00:37:21.440 --> 00:37:22.440]   time.
[00:37:22.440 --> 00:37:27.360]   So it's definitely something that we have to keep in mind and we do.
[00:37:27.360 --> 00:37:28.520]   Yeah.
[00:37:28.520 --> 00:37:33.400]   One of the things that we're doing in the product too, related to that specific question,
[00:37:33.400 --> 00:37:34.840]   is the modes.
[00:37:34.840 --> 00:37:40.680]   So if you're on an artist station and you're getting tired of your normal station experience
[00:37:40.680 --> 00:37:45.160]   and you're really wanting to get some new stuff in there, you can go into discovery
[00:37:45.160 --> 00:37:49.200]   mode and you'll get some really fresh songs.
[00:37:49.200 --> 00:37:53.320]   But then when you get tired of hearing new stuff, because that's sort of exhausting,
[00:37:53.320 --> 00:37:58.480]   constantly having new content thrown at you, you can go back to your old experience.
[00:37:58.480 --> 00:37:59.480]   Awesome.
[00:37:59.480 --> 00:38:04.400]   Well, the final question, and we're running out of time, but I want to make sure I ask
[00:38:04.400 --> 00:38:08.760]   it is, what's the biggest challenge of actually getting machine learning models deployed in
[00:38:08.760 --> 00:38:09.760]   the real world?
[00:38:09.760 --> 00:38:14.600]   From the beginning of the kind of conception of it to actually in people's hands, giving
[00:38:14.600 --> 00:38:18.280]   them better music, where are the surprising bottlenecks?
[00:38:18.280 --> 00:38:22.880]   Well, I think we talked about that a little bit already.
[00:38:22.880 --> 00:38:28.400]   For me coming from academia, it was that first of all, different things, you approach the
[00:38:28.400 --> 00:38:32.880]   problem from a different point of view, because before you just have the data set and you
[00:38:32.880 --> 00:38:39.720]   try to improve the model, even if it's just by one percentage point accuracy.
[00:38:39.720 --> 00:38:45.720]   And now it's more like you have a problem and the first step is to find the data that
[00:38:45.720 --> 00:38:46.720]   solves that problem.
[00:38:46.720 --> 00:38:52.600]   So you have this huge data store and, "Okay, how can I find the data that solves the problem?"
[00:38:52.600 --> 00:38:55.360]   And you develop a model, which is pretty similar.
[00:38:55.360 --> 00:38:59.920]   And then at some point you have to ask yourselves, when is the model good enough?
[00:38:59.920 --> 00:39:01.760]   Because you can always keep on tuning.
[00:39:01.760 --> 00:39:02.760]   This is like science, right?
[00:39:02.760 --> 00:39:06.320]   So you can just keep on improving forever.
[00:39:06.320 --> 00:39:11.240]   And here the difference is of course that one or two percentage points improvement in
[00:39:11.240 --> 00:39:13.320]   academia gets you a new paper.
[00:39:13.320 --> 00:39:19.640]   In industry, it might not even matter because the impact on the end user is so small because
[00:39:19.640 --> 00:39:23.240]   you have a hundred other recommenders in the ensemble.
[00:39:23.240 --> 00:39:28.200]   And then for me, the hardest part was to just let it go at some point and just say, "Okay,
[00:39:28.200 --> 00:39:29.200]   this is it.
[00:39:29.200 --> 00:39:32.640]   That's enough."
[00:39:32.640 --> 00:39:37.000]   For me, I've totally already mentioned this, but the biggest challenge is always making
[00:39:37.000 --> 00:39:41.200]   sure your machine learning model is performant enough to make predictions in real time.
[00:39:41.200 --> 00:39:46.280]   I think during the research phase of development, you can focus on the accuracy of predictions
[00:39:46.280 --> 00:39:49.440]   without worrying a ton about the latency of the predictions.
[00:39:49.440 --> 00:39:54.400]   But in production, the prediction latency has to be low enough that a user isn't waiting
[00:39:54.400 --> 00:39:56.160]   around for results.
[00:39:56.160 --> 00:40:01.520]   So there's definitely a balance there between the effectiveness of a model and the efficiency
[00:40:01.520 --> 00:40:02.520]   of a model.
[00:40:02.520 --> 00:40:06.880]   I mean, spoken like someone who really has models in production.
[00:40:06.880 --> 00:40:08.400]   It's so great to talk to both of you.
[00:40:08.400 --> 00:40:09.400]   I really appreciate it.
[00:40:09.400 --> 00:40:10.400]   That was super fun.
[00:40:10.400 --> 00:40:14.000]   I feel so proud that we could help you guys.
[00:40:14.000 --> 00:40:19.200]   At Weights & Biases, we make this podcast Gradient Descent to learn about making machine
[00:40:19.200 --> 00:40:21.280]   learning work in the real world.
[00:40:21.280 --> 00:40:23.480]   But we also have a part to play here.
[00:40:23.480 --> 00:40:30.440]   We are building tools to help all the people that are on this podcast make their work better
[00:40:30.440 --> 00:40:34.360]   and make machine learning models actually run in production.
[00:40:34.360 --> 00:40:38.440]   And if you're interested in joining us on this mission, we are hiring and engineering
[00:40:38.440 --> 00:40:45.120]   sales, growth, product, and customer support, and you should go to wmv.me/hiring and check
[00:40:45.120 --> 00:40:46.120]   out our job postings.
[00:40:46.120 --> 00:40:47.520]   We'd love to talk about working with you.
[00:40:47.520 --> 00:40:49.540]   (upbeat electronic music)

