
[00:00:00.000 --> 00:00:03.540]   I messed up so many times. And every time I say this, I have
[00:00:03.540 --> 00:00:08.580]   like this paranoia where I go and check if I'm live or not.
[00:00:08.580 --> 00:00:10.580]   That's what I'm doing right now.
[00:00:10.580 --> 00:00:12.060]   Okay, no problem.
[00:00:12.060 --> 00:00:19.020]   Let me just make sure real quick. I can hear myself.
[00:00:19.020 --> 00:00:23.100]   Awesome. I didn't mess up this time. Hey, everybody. Thanks.
[00:00:23.100 --> 00:00:26.340]   Thanks for joining us live if you're joining live and thanks
[00:00:26.340 --> 00:00:29.460]   for watching the recording. Thank you, Adam, for joining us
[00:00:29.460 --> 00:00:31.260]   in the evening. I know it's evening time for you.
[00:00:31.260 --> 00:00:34.060]   Thanks for having me. I'm excited.
[00:00:34.060 --> 00:00:37.260]   I'm excited to have chai in the morning. Usually I host
[00:00:37.260 --> 00:00:40.820]   interviews in the evening. I have flexible chai. So this is
[00:00:40.820 --> 00:00:41.460]   afternoon but
[00:00:41.460 --> 00:00:46.140]   yeah, it's a bit late for me to start drinking caffeine I think
[00:00:46.140 --> 00:00:46.420]   but
[00:00:46.420 --> 00:00:48.900]   4pm is not that late.
[00:00:48.900 --> 00:00:53.100]   I guess. Yeah, I mean, I think it lasts like seven hours in
[00:00:53.100 --> 00:00:55.060]   your body. So yeah, I guess I could go to sleep.
[00:00:55.060 --> 00:00:55.580]   It does.
[00:00:55.580 --> 00:00:59.300]   Good. That's what I read that one time. It's not a fact. I
[00:00:59.300 --> 00:00:59.900]   I'm not sure.
[00:00:59.900 --> 00:01:04.460]   It is. Trust me. It's not good to read these things. I try not
[00:01:04.460 --> 00:01:08.860]   to. Okay. I didn't know until recently chai had caffeine. And
[00:01:08.860 --> 00:01:09.900]   I still don't believe that.
[00:01:09.900 --> 00:01:14.860]   Sometimes I think it has a stronger effect than coffee. But
[00:01:14.860 --> 00:01:15.460]   yeah, sure.
[00:01:15.460 --> 00:01:21.500]   That I can agree on. So I'll quickly introduce the session.
[00:01:21.500 --> 00:01:24.420]   And then I'll start by asking Adam about his Sunni. Let me
[00:01:24.420 --> 00:01:27.700]   quickly do that. So this is the 10th in the chai time target
[00:01:27.700 --> 00:01:30.660]   talks. This is essentially chai time data science 2.0 where I
[00:01:30.660 --> 00:01:35.220]   invite my heroes and awesome Kagglers like Adam to tell about
[00:01:35.220 --> 00:01:38.780]   how did they get to the position of achieving a solo gold in a
[00:01:38.780 --> 00:01:42.900]   competition in just one year of getting started. The agenda for
[00:01:42.900 --> 00:01:45.500]   today is I'll introduce the session which I'm doing right
[00:01:45.500 --> 00:01:49.100]   now then I'll ask Adam how did he get started. We'll try to
[00:01:49.100 --> 00:01:51.460]   understand the jigsaw competition and then Adam will
[00:01:51.460 --> 00:01:56.260]   tell us more about his solution. A little bit about Adam he's
[00:01:56.300 --> 00:02:00.340]   achieved the rank of Kaggle competitions master, which
[00:02:00.340 --> 00:02:03.340]   arguably is one of the more challenging categories compared
[00:02:03.340 --> 00:02:09.580]   to the others. His blog which you can find on his Twitter
[00:02:09.580 --> 00:02:12.740]   profile. If you go over there, it's been there on his Twitter
[00:02:12.740 --> 00:02:17.220]   handle is a Monko Mary underscore. His blog on Kaggle
[00:02:17.220 --> 00:02:19.860]   journey to competition master was quite well recognized in the
[00:02:19.860 --> 00:02:22.500]   community when the CEO of Kaggle tweeted it out. I would highly
[00:02:22.500 --> 00:02:24.860]   encourage you to check it out. And you can find him on Twitter
[00:02:24.860 --> 00:02:29.620]   at Monko Mary underscore. So thanks again, Adam, I wanted to
[00:02:29.620 --> 00:02:34.460]   start by asking you at what point did you find yourself
[00:02:34.460 --> 00:02:38.300]   interested? I know you started in NLP. How did that catch your
[00:02:38.300 --> 00:02:38.820]   interest?
[00:02:38.820 --> 00:02:43.140]   Well, yeah, I guess I took a kind of indirect route to
[00:02:43.140 --> 00:02:46.940]   getting into machine learning because originally I got a
[00:02:46.940 --> 00:02:50.260]   undergraduate degree in philosophy, which is kind of
[00:02:50.260 --> 00:02:54.460]   unrelated. And but after I finished that degree, I realized
[00:02:54.460 --> 00:02:56.900]   there's not many companies that are desperate to hire
[00:02:56.900 --> 00:03:00.180]   philosophers, really. So I thought I should do something a
[00:03:00.180 --> 00:03:04.020]   bit more practical. And I got a master's degree in computer
[00:03:04.020 --> 00:03:07.700]   science, at least in the UK. I don't know, maybe they have it
[00:03:07.700 --> 00:03:10.980]   in other countries, too. But it's called a conversion course
[00:03:10.980 --> 00:03:14.820]   where even if you didn't have the right undergraduate degree,
[00:03:14.820 --> 00:03:18.700]   you can still sign up for this master's course. And we had to
[00:03:18.700 --> 00:03:22.300]   take some undergraduate classes and some classes with master
[00:03:22.300 --> 00:03:26.060]   students and get up to speed at the same rate as everyone else.
[00:03:26.060 --> 00:03:31.140]   So I learned a lot from that it was hard work, but because of
[00:03:31.140 --> 00:03:36.580]   the short term nature of it, there isn't really much room to,
[00:03:36.580 --> 00:03:39.220]   you know, try different applications of computer
[00:03:39.220 --> 00:03:42.060]   science. So I never had any machine learning classes or
[00:03:42.060 --> 00:03:45.300]   really any, you know, math source that's classes either for
[00:03:45.300 --> 00:03:51.740]   that matter. So I felt quite behind when I did get into
[00:03:51.740 --> 00:03:56.660]   machine learning, I, everything was new to me. And after I
[00:03:56.660 --> 00:03:59.220]   graduated, I moved to Japan, and I became an English teacher for
[00:03:59.220 --> 00:04:05.580]   a few years. And then before finally, the English teaching
[00:04:05.580 --> 00:04:07.900]   thing was only meant to be temporary. And I thought, okay,
[00:04:07.900 --> 00:04:11.180]   it's time, I'm gonna look for software development jobs. But I
[00:04:11.180 --> 00:04:13.900]   didn't really have any particular skill set. And I
[00:04:13.900 --> 00:04:16.460]   would just apply for anything, you know, front end, back end,
[00:04:16.460 --> 00:04:21.100]   whatever. But that attitude is not very good in job interviews,
[00:04:21.100 --> 00:04:23.300]   you know, I don't know anything, but I'm willing to give it a
[00:04:23.300 --> 00:04:27.700]   try. So I thought I need to develop a specific skill set.
[00:04:27.700 --> 00:04:31.700]   And I thought, if I could pick anything, what would be cool.
[00:04:31.700 --> 00:04:35.860]   And I thought, you know, machine learning seems seems like a cool
[00:04:35.860 --> 00:04:40.460]   thing. So I'll start learning that. And when I found out about
[00:04:40.460 --> 00:04:44.060]   it, and I started finding out about NLP, it kind of made sense
[00:04:44.060 --> 00:04:48.380]   to me, because it's, in some sense, it's kind of a crossover
[00:04:48.380 --> 00:04:50.380]   of different things I've done before, there's some computer
[00:04:50.540 --> 00:04:57.020]   science and some language. And yeah, so it really, it really
[00:04:57.020 --> 00:04:59.500]   got my interest. And I've been I've been interested in ever
[00:04:59.500 --> 00:04:59.860]   since.
[00:04:59.860 --> 00:05:06.140]   Sanyam Bhutani: May I ask as a philosophy major, you must have
[00:05:06.140 --> 00:05:09.220]   been new to programming. How did you go about that? I know for a
[00:05:09.220 --> 00:05:12.540]   fact, I've studied computer science, it didn't at all help
[00:05:12.540 --> 00:05:14.980]   me people have this assumption that you needed to help you
[00:05:14.980 --> 00:05:19.060]   become a better programmer. How did you go about that journey of
[00:05:19.220 --> 00:05:23.660]   learning Python or even frontend or all the software details?
[00:05:23.660 --> 00:05:28.260]   Well, I guess I had always been into computers. And I had had a
[00:05:28.260 --> 00:05:34.020]   few, like, programming classes before university, but not so
[00:05:34.020 --> 00:05:40.540]   much, you know, pretty basic. So for me, I don't know, it was
[00:05:40.540 --> 00:05:44.740]   always just about spending a lot of time messing around with
[00:05:44.740 --> 00:05:51.300]   things and practicing things. So I, I had already taken a few
[00:05:51.300 --> 00:05:56.580]   classes in in learning Python and things. So when I started
[00:05:56.580 --> 00:05:58.900]   learning machine learning, that part didn't seem that
[00:05:58.900 --> 00:06:02.100]   challenging to me, because it's just installing a few new
[00:06:02.100 --> 00:06:06.020]   libraries and learning libraries in Python. But the
[00:06:06.020 --> 00:06:09.460]   challenging part, I think, is really the theory behind it. So
[00:06:09.460 --> 00:06:12.980]   when I first started trying to learn about machine learning, I
[00:06:13.020 --> 00:06:17.180]   I took some, you know, like courses on Udemy and Coursera
[00:06:17.180 --> 00:06:21.180]   and things like that. But I found it easy to follow along.
[00:06:21.180 --> 00:06:24.060]   And they say, Okay, now we're going to install pandas. And now
[00:06:24.060 --> 00:06:27.100]   we're going to do this. And, you know, I can I can do it. But
[00:06:27.100 --> 00:06:29.500]   when I get to the end, and they're like, Okay, here we are,
[00:06:29.500 --> 00:06:32.700]   model has made some predictions. And I thought, like, what, what
[00:06:32.700 --> 00:06:35.260]   am I doing? What does any of this mean? I don't understand
[00:06:35.260 --> 00:06:37.580]   what these algorithms are. And I don't understand what I've just
[00:06:37.580 --> 00:06:42.660]   predicted. So I needed a bit more theory, really, rather than
[00:06:42.660 --> 00:06:46.740]   learning, learning programming was not such. I don't want to
[00:06:46.740 --> 00:06:48.740]   say it wasn't such a challenge. Of course, it is, it's something
[00:06:48.740 --> 00:06:52.580]   that I still have to practice. But I think it's something if
[00:06:52.580 --> 00:06:55.460]   you just do it every day, you get better at it. Whereas
[00:06:55.460 --> 00:06:59.380]   something that didn't come naturally to me was mathematics
[00:06:59.380 --> 00:07:02.300]   and statistics, it's always seemed a bit more scary.
[00:07:02.300 --> 00:07:06.780]   Sanyam Bhutani: That's, that's great to hear. In your blog, you
[00:07:06.780 --> 00:07:10.540]   mentioned you started with the TPS competition. Was that your
[00:07:10.580 --> 00:07:15.020]   start in the world of competing? I also saw this is a silly
[00:07:15.020 --> 00:07:18.100]   question to us. But I saw your picture holding an airport that
[00:07:18.100 --> 00:07:21.780]   you had one was that your point of addiction to competing in
[00:07:21.780 --> 00:07:22.420]   machine learning?
[00:07:22.420 --> 00:07:25.420]   Dr. Gareth Forde: Oh, that's the airports of these ones. Yeah,
[00:07:25.420 --> 00:07:26.740]   no, that was actually something.
[00:07:26.740 --> 00:07:33.980]   Oh, sorry. That was something that was just something from my
[00:07:33.980 --> 00:07:37.060]   my current company. It was like, like an employee of the month
[00:07:37.060 --> 00:07:38.740]   kind of thing. They gave me some airports.
[00:07:39.540 --> 00:07:42.380]   Sanyam Bhutani: Sorry, I assumed that was through Kaggle, or some
[00:07:42.380 --> 00:07:43.260]   form of competition.
[00:07:43.260 --> 00:07:47.140]   Dr. Gareth Forde: No, no, it wasn't. No. Yeah, I didn't get
[00:07:47.140 --> 00:07:50.700]   into Kaggle. Immediately. Of course, I was aware of it from
[00:07:50.700 --> 00:07:53.100]   when I first started getting into machine learning, because
[00:07:53.100 --> 00:07:55.940]   you know, it's a great resource, and everyone knows about it. But
[00:07:55.940 --> 00:08:00.140]   I didn't really do anything with it at first, except, you know,
[00:08:00.140 --> 00:08:06.060]   download a few data sets. And I was mostly into, like my own, I
[00:08:06.060 --> 00:08:09.060]   had my own ideas for side projects, things I wanted to do.
[00:08:09.260 --> 00:08:11.940]   And I would like collect my own data and train a model and
[00:08:11.940 --> 00:08:18.460]   things. What I think really got me into competing was when I
[00:08:18.460 --> 00:08:21.580]   there was like a crossover between my own side project and
[00:08:21.580 --> 00:08:26.220]   a Kaggle competition, it was the common lit one, which was about
[00:08:26.220 --> 00:08:30.300]   predicting the reading difficulty of some texts. And I
[00:08:30.300 --> 00:08:34.500]   had been doing something similar on my own. It was a little
[00:08:34.500 --> 00:08:38.060]   different, though it was from the perspective of someone who's
[00:08:38.060 --> 00:08:40.540]   learning English as a second language. And I wanted to
[00:08:40.540 --> 00:08:47.220]   classify each text in terms of its CEFR level, which is the
[00:08:47.220 --> 00:08:52.500]   common European framework work of reference for languages. I
[00:08:52.500 --> 00:08:54.940]   think that's what it stands for. But basically, it's a leveling
[00:08:54.940 --> 00:08:58.820]   system where you have some classes like A1, A2, B1, B2. So
[00:08:58.820 --> 00:09:02.140]   I made some classifiers like that. But then I saw the common
[00:09:02.140 --> 00:09:05.100]   lit competition, and they were doing something similar, just
[00:09:05.100 --> 00:09:10.900]   predicting American grade school reading levels instead. And I
[00:09:10.900 --> 00:09:13.140]   thought, Oh, this is great. Like I already I've done a bunch of
[00:09:13.140 --> 00:09:16.580]   research about this, and I've already trained some models. So
[00:09:16.580 --> 00:09:21.100]   I can just jump into this and give it a go. And it turned out
[00:09:21.100 --> 00:09:24.500]   that most of the things I did, didn't translate very well into
[00:09:24.500 --> 00:09:26.820]   this competition, because this one was framed as like a
[00:09:26.820 --> 00:09:32.980]   regression task instead of a classification one. But I still
[00:09:32.980 --> 00:09:38.060]   it got my interest. And I guess because it related to NLP, and
[00:09:38.060 --> 00:09:41.500]   it related to education as well, like I'd worked as a teacher for
[00:09:41.500 --> 00:09:44.300]   a few years. So this kind of problem was actually interesting
[00:09:44.300 --> 00:09:48.460]   to me as well. And I kind of, I got quite high up the leader
[00:09:48.460 --> 00:09:51.460]   board early on. And I didn't really realize at the time, but
[00:09:51.460 --> 00:09:54.420]   I was basically just overfitting the leaderboard. But I think
[00:09:54.420 --> 00:09:57.620]   that that's kind of like a rite of passage for new Kagglers,
[00:09:57.620 --> 00:10:01.460]   right? You got the leaderboard, and then the private leaderboard
[00:10:01.460 --> 00:10:03.820]   is revealed, and you come crashing down again, and you
[00:10:03.820 --> 00:10:07.300]   learn to be a bit more humble. But I got lucky in that
[00:10:07.300 --> 00:10:11.980]   competition, because I, because I was high on the leaderboard,
[00:10:11.980 --> 00:10:14.940]   when it gets near to the end, usually at this point, people
[00:10:14.940 --> 00:10:18.900]   start merging teams, you know, in the last few weeks, and I got
[00:10:18.900 --> 00:10:22.740]   a few offers to merge. And at first, I thought, I don't want
[00:10:22.740 --> 00:10:26.180]   to join the team, I can do it by myself. But then everyone
[00:10:26.180 --> 00:10:28.580]   started overtaking me on the leaderboard, because I was just
[00:10:28.580 --> 00:10:31.900]   by myself. And I thought, oh, maybe I've made a mistake here.
[00:10:31.900 --> 00:10:36.380]   So then when I got another team merge offer, I accepted it. And
[00:10:36.380 --> 00:10:40.460]   luckily, it was a really great team, like everyone else on the
[00:10:40.460 --> 00:10:44.180]   team is like a competition's grandmaster or master. And I
[00:10:44.180 --> 00:10:47.020]   learned a lot from them. And we got a silver medal in the end,
[00:10:47.020 --> 00:10:51.260]   and which I was happy with. Yeah, you can see who was on my
[00:10:51.260 --> 00:10:56.500]   team, though. These guys are all very highly ranked Kagglers. So
[00:10:56.500 --> 00:10:59.020]   I learned loads from that. And then after that, I thought,
[00:10:59.020 --> 00:11:01.180]   this is great, you know, I'm going to apply all the things I
[00:11:01.180 --> 00:11:04.020]   learned in this one into onto the next competition. I just
[00:11:04.020 --> 00:11:04.500]   kept going.
[00:11:04.500 --> 00:11:10.380]   Sanyam Bhutani: That's great to hear. So when you jumped onto
[00:11:10.380 --> 00:11:14.100]   Kaggle competition, at least for me, I had this like insane,
[00:11:14.100 --> 00:11:20.860]   overwhelming learning curve where I would like half, half as
[00:11:20.860 --> 00:11:25.100]   my way around learning things, and like build one model in like
[00:11:25.140 --> 00:11:28.500]   two weeks on Kaggle, you're forced to do that in like one
[00:11:28.500 --> 00:11:32.060]   day or like multiple times a day. Was that overwhelming for
[00:11:32.060 --> 00:11:34.620]   you? And how did you get adjusted to that Kaggler
[00:11:34.620 --> 00:11:35.300]   mindset?
[00:11:35.300 --> 00:11:39.780]   Dr. Gareth Williams: Yeah, I guess. I think it can be
[00:11:39.780 --> 00:11:43.980]   overwhelming. But I think for me, I, I kind of narrowed my
[00:11:43.980 --> 00:11:47.140]   focus quite a lot, because you know, machine learning is very
[00:11:47.140 --> 00:11:49.540]   broad. And there's lots of different, depending on what
[00:11:49.540 --> 00:11:51.820]   you're applying it to the type of algorithms is completely
[00:11:51.820 --> 00:11:57.940]   different, right. So I narrowed my focus first to NLP. And then
[00:11:57.940 --> 00:12:01.900]   I guess mostly I've been working with transformer networks, which
[00:12:01.900 --> 00:12:05.420]   just that one architecture is incredibly complex already. But
[00:12:05.420 --> 00:12:09.980]   if I just focus on this one thing, you know, then it
[00:12:09.980 --> 00:12:14.940]   narrows it down a bit. And I'm the kind of person who doesn't
[00:12:14.940 --> 00:12:20.500]   mind like repeating things and refining them. So in fact, I
[00:12:20.900 --> 00:12:24.100]   quite enjoy that. I guess I'm, you know, I'm a bit like a
[00:12:24.100 --> 00:12:26.660]   gradient descent in real life, you know, I just want to keep
[00:12:26.660 --> 00:12:30.900]   doing the same thing over and over again. But so that's more
[00:12:30.900 --> 00:12:35.780]   my approach to Kaggle. It's not about like exploring random
[00:12:35.780 --> 00:12:40.780]   things so much, although I think that is good, too. I'm, I'm more
[00:12:40.780 --> 00:12:44.740]   of someone who likes to come up with like a method and refine
[00:12:44.740 --> 00:12:49.940]   it. So yeah, that's kind of how I approach it. I find a good
[00:12:49.940 --> 00:12:53.140]   baseline. And I just keep running different experiments
[00:12:53.140 --> 00:12:56.220]   and different hyper parameters and improving on it, rather than
[00:12:56.220 --> 00:12:59.020]   reading a bunch of research papers and half implementing a
[00:12:59.020 --> 00:13:03.700]   bunch of half baked ideas, because I don't think I'd make
[00:13:03.700 --> 00:13:06.460]   so much progress that way, or get such good results either.
[00:13:06.460 --> 00:13:07.940]   Although you would learn a lot that way.
[00:13:07.940 --> 00:13:10.900]   Sanyam Bhutani: I feel very seen, Adam, I feel very seen.
[00:13:10.900 --> 00:13:14.740]   Adam Grant: I didn't mean it as a directed attack. It's
[00:13:14.740 --> 00:13:18.060]   something I've seen, you know, across across the board, people
[00:13:18.060 --> 00:13:22.060]   have great ideas, but you kind of if you if you haven't
[00:13:22.060 --> 00:13:27.220]   implemented it, then, you know, do you really know anything? I
[00:13:27.220 --> 00:13:27.500]   think.
[00:13:27.500 --> 00:13:31.860]   Sanyam Bhutani: So as you were overfitting to the public
[00:13:31.860 --> 00:13:35.300]   leaderboard, how are you finding these ideas to iterate on? Was
[00:13:35.300 --> 00:13:37.660]   it through the discussion? Were you reading there and trying to
[00:13:37.660 --> 00:13:39.300]   go back implement them yourself?
[00:13:39.300 --> 00:13:42.940]   Adam Grant: Yeah, yeah, in some sense, I kept up with the
[00:13:42.940 --> 00:13:45.980]   discussions and the public notebooks to see what other
[00:13:45.980 --> 00:13:50.860]   people are doing. I'd have a few of my own little ideas as well.
[00:13:50.860 --> 00:13:58.540]   But I think, yeah, I take a lot of ideas just from what other
[00:13:58.540 --> 00:14:01.940]   people are saying, because people on Kaggle, it's
[00:14:01.940 --> 00:14:04.140]   surprising, you know, in competitions, how much people
[00:14:04.140 --> 00:14:06.620]   share, you would think people would keep it, keep it secret,
[00:14:06.620 --> 00:14:11.300]   right. But they share a lot of great ideas. And you can kind of
[00:14:11.300 --> 00:14:14.660]   just steal their ideas. But I think it's good to have a rule
[00:14:14.660 --> 00:14:18.340]   of never just fork someone else's notebook and run it,
[00:14:18.340 --> 00:14:23.260]   because you don't learn anything from that. And plus, I'm quite
[00:14:23.260 --> 00:14:29.020]   pedantic about the organization of my code in like a, in like a
[00:14:29.020 --> 00:14:33.460]   repo, like a git repo. So if I'm copying and pasting someone
[00:14:33.460 --> 00:14:35.660]   else's code and stick it in mine, it's going to become a
[00:14:35.660 --> 00:14:40.300]   mess. So if I use someone else's idea, I'll either rewrite it from
[00:14:40.300 --> 00:14:44.500]   scratch, or I will at least refactor their code. And when
[00:14:44.500 --> 00:14:48.460]   I'm done refactoring it in to fit in with my code, I kind of
[00:14:48.460 --> 00:14:49.780]   understand how it works.
[00:14:49.780 --> 00:14:53.620]   Sanyam Bhutani: That makes sense. I was trying to find this
[00:14:53.620 --> 00:14:57.660]   discussion post by myself, a bit of self promotion, sorry. But I
[00:14:57.660 --> 00:15:02.380]   had the exact same feeling where I was really surprised that top
[00:15:02.380 --> 00:15:05.700]   performers were really sharing their ideas. And you could just
[00:15:05.700 --> 00:15:07.940]   take them and implement them, although I even struggled with
[00:15:07.940 --> 00:15:11.260]   that. But everyone was really open even my first competition.
[00:15:11.260 --> 00:15:17.660]   Yeah, it's great, isn't it? It, it helps me a lot. Because when
[00:15:17.660 --> 00:15:21.620]   I do an experiment, and I get a result, and then someone else
[00:15:21.620 --> 00:15:25.380]   shares a public notebook, using the roughly the same technique,
[00:15:25.380 --> 00:15:28.940]   but they get a little bit higher than I, I know, okay, so they've
[00:15:28.940 --> 00:15:31.980]   they've done something that I haven't done right here. And
[00:15:31.980 --> 00:15:34.500]   then you can you can go through it comb through their notebook
[00:15:34.500 --> 00:15:38.380]   and find every little difference between that. Make little
[00:15:38.380 --> 00:15:40.460]   incremental improvements like that, I think.
[00:15:40.460 --> 00:15:43.740]   Sanyam Bhutani: Awesome. Thanks. Thanks for sharing that
[00:15:43.740 --> 00:15:46.620]   context. I would now love to switch to talking about the
[00:15:46.620 --> 00:15:50.420]   competition. So I'll try to introduce the competition and
[00:15:50.420 --> 00:15:54.540]   please interrupt me whenever I mess up, which I do a lot. So,
[00:15:54.540 --> 00:15:57.700]   as I understood, again, I haven't taken part because I'm
[00:15:57.700 --> 00:16:00.460]   too terrified of competing. And I just asked stupid questions.
[00:16:01.860 --> 00:16:05.700]   This, as I understand, was the fourth competition where you
[00:16:05.700 --> 00:16:10.220]   were again tasked with predicting relative rankings of
[00:16:10.220 --> 00:16:14.500]   toxicity. So in historically Kaggle has had this competition
[00:16:14.500 --> 00:16:18.740]   where you would tell if a comment is just toxic or not.
[00:16:18.740 --> 00:16:22.380]   Now it's, they keep iterating on it. So now there's like levels
[00:16:22.380 --> 00:16:24.780]   to it where you have to basically predict with that
[00:16:24.780 --> 00:16:31.820]   emoji levels. It'll be just numbers in the reality. I was
[00:16:31.820 --> 00:16:34.460]   blown away to realize there's no data this time. So
[00:16:34.460 --> 00:16:38.140]   there's a validation data, but there's no training data, right?
[00:16:38.140 --> 00:16:38.380]   Yeah.
[00:16:38.380 --> 00:16:42.340]   Yeah. So I usually start by talking about EDA, but there's
[00:16:42.340 --> 00:16:46.300]   no EDA here. So I'll switch to your write up which I'll also
[00:16:46.300 --> 00:16:52.860]   put in the YouTube chat. But you use the Jigsaw for second and
[00:16:52.860 --> 00:16:57.460]   Reddit data. So could you please also talk about these data sets
[00:16:57.460 --> 00:16:58.620]   and how did you pick these?
[00:16:59.060 --> 00:17:02.420]   Right? Yeah. So actually, I think in the competition
[00:17:02.420 --> 00:17:05.460]   overview, they they recommend that you can use the data sets
[00:17:05.460 --> 00:17:09.060]   from the previous Jigsaw competitions. So even though
[00:17:09.060 --> 00:17:11.740]   there's no training data, you can just follow the links and
[00:17:11.740 --> 00:17:18.820]   get these data sets. But the challenge was that really, that
[00:17:18.820 --> 00:17:21.900]   the the other data sets are in, as you mentioned, a different
[00:17:21.900 --> 00:17:26.340]   format, this one's about ranking. Whereas the previous
[00:17:26.340 --> 00:17:30.260]   ones were binary classification or regression, there was, you
[00:17:30.260 --> 00:17:33.020]   know, you can frame this problem in all sorts of different ways.
[00:17:33.020 --> 00:17:37.060]   And I think the reason for that is probably and the reason why
[00:17:37.060 --> 00:17:40.260]   there's been so many of these Jigsaw toxicity competitions is
[00:17:40.260 --> 00:17:46.140]   that toxicity is a very, like, vague idea. What, you know, one
[00:17:46.140 --> 00:17:48.780]   person thinks is toxic might be different to what another person
[00:17:48.780 --> 00:17:51.860]   thinks is toxic. So it's quite hard to come up with like a
[00:17:51.860 --> 00:17:56.820]   consistent, like, way of labelling toxicity. So I guess
[00:17:56.820 --> 00:17:59.620]   that's why they've had so many different competitions. And this
[00:17:59.620 --> 00:18:03.700]   is their latest attempt to define toxicity and measure it
[00:18:03.700 --> 00:18:09.220]   accurately. And, yeah, actually, should I share my screen? I've
[00:18:09.220 --> 00:18:12.060]   got a few descriptions of the competition. Please metric here.
[00:18:12.060 --> 00:18:29.340]   Yeah. Okay. Yeah, okay. That's great. Let's take a look at them.
[00:18:29.340 --> 00:18:34.380]   Yeah, so this is my 14th place submission, as you already
[00:18:34.380 --> 00:18:37.940]   introduced. And as you already mentioned, this was the fourth
[00:18:38.260 --> 00:18:40.420]   Jigsaw toxicity competition.
[00:18:40.420 --> 00:18:43.380]   Sanyam Bhutani: Sorry, let me please allow me to emphasise I'm
[00:18:43.380 --> 00:18:47.260]   sure everyone in the audience would be aware of this but gold
[00:18:47.260 --> 00:18:52.420]   in Kaggle is the like Holy Grail of achievement. And solo gold is
[00:18:52.420 --> 00:18:55.740]   like even one level higher. It's like one of the most difficult
[00:18:55.740 --> 00:18:58.940]   achievements one can achieve on Kaggle. Many people do it
[00:18:58.940 --> 00:19:02.940]   multiple times, but millions struggle with it. Adam got a
[00:19:02.940 --> 00:19:06.860]   solo gold in this competition. So really wanted to emphasise on
[00:19:06.860 --> 00:19:07.900]   that. Sorry to interrupt you.
[00:19:08.300 --> 00:19:11.100]   Yeah, sure. No, thanks. Thanks for for picking me up like that.
[00:19:11.100 --> 00:19:13.420]   Yeah, sure. If you just say 14th place, it doesn't sound that
[00:19:13.420 --> 00:19:16.460]   exciting. But yeah, it was a solo gold. So I was I was very
[00:19:16.460 --> 00:19:22.900]   humble. Yeah, I was ecstatic to actually win this. What? So,
[00:19:22.900 --> 00:19:26.500]   yeah, in this competition, it was about ranked pairs of
[00:19:26.500 --> 00:19:33.220]   comments. So what that means is, when the data is labelled, you
[00:19:33.220 --> 00:19:36.980]   imagine you can imagine they had some hired some annotators or
[00:19:36.980 --> 00:19:40.980]   they had some in house data labelers. And each time an
[00:19:40.980 --> 00:19:45.500]   annotator is shown a pair of comments, and they have to
[00:19:45.500 --> 00:19:51.020]   decide which one of these two comments is more toxic. And each
[00:19:51.020 --> 00:19:55.060]   comment will appear multiple times in the data set. And also
[00:19:55.060 --> 00:19:59.780]   each pair of comments will appear multiple times. So a few
[00:19:59.780 --> 00:20:04.820]   different annotators will rank the same pair of comments. And I
[00:20:04.820 --> 00:20:08.780]   think usually in data labelling, what happens at this stage is,
[00:20:08.780 --> 00:20:12.860]   if you have multiple data labelers labelling the same
[00:20:12.860 --> 00:20:17.220]   example, then you'll aggregate their their labels in some way
[00:20:17.220 --> 00:20:20.260]   to make the labelling more consistent. So if it's a
[00:20:20.260 --> 00:20:24.500]   regression task, you can take the mean of their labels. Or if
[00:20:24.500 --> 00:20:28.220]   it's classification, then you can do like a majority vote. But
[00:20:28.220 --> 00:20:32.540]   in this competition, there was no aggregation at all. So all of
[00:20:32.540 --> 00:20:36.820]   the individual annotations are preserved. And I think they
[00:20:36.820 --> 00:20:40.860]   mentioned that in the description here, they said,
[00:20:40.860 --> 00:20:46.020]   like each, each individual may have their own bar for toxicity.
[00:20:46.020 --> 00:20:49.180]   And we've tried to work around this by aggregating the
[00:20:49.180 --> 00:20:53.420]   decisions with a majority vote in the past. But many researchers
[00:20:53.420 --> 00:20:55.740]   have rightly pointed out that this discards meaningful
[00:20:55.740 --> 00:20:59.740]   information. And this is a bit vague. But I think what they're
[00:20:59.740 --> 00:21:06.180]   trying to say here is that for something like toxicity, what
[00:21:06.180 --> 00:21:09.300]   appears to be toxic to one person might not be toxic to
[00:21:09.300 --> 00:21:13.780]   another person. And especially some of the most like offensive
[00:21:13.780 --> 00:21:17.980]   toxic things people can say are directed at minority specific
[00:21:17.980 --> 00:21:24.780]   minority groups. So if in the data label is there only one or
[00:21:24.780 --> 00:21:27.540]   two people you know, are of this group, then of course, they will
[00:21:27.540 --> 00:21:30.940]   find these toxic comments very offensive and rank them highly.
[00:21:30.940 --> 00:21:34.740]   But the majority of people might not know the meaning of these
[00:21:34.740 --> 00:21:37.900]   things. And they might not care and they'll rate them lowly. So
[00:21:37.900 --> 00:21:41.460]   if we do a majority vote, then the minority
[00:21:41.460 --> 00:21:42.380]   Sanyam Bhutani: Zoom in a bit.
[00:21:42.380 --> 00:21:43.740]   Unknown: Oh, yeah, sure. Sorry.
[00:21:43.740 --> 00:21:48.940]   Why is it not zooming?
[00:21:52.580 --> 00:21:57.780]   Am I? Oh, no, you're here. I'm this command plus or control
[00:21:57.780 --> 00:22:01.140]   plus not. Yeah, I don't know. My hotkeys weren't working for
[00:22:01.140 --> 00:22:05.780]   some reason. Okay. Anyway. Yeah, so so basically, the point is,
[00:22:05.780 --> 00:22:10.180]   if you apply a majority vote to the data, then you kind of lose
[00:22:10.180 --> 00:22:14.340]   minority opinions. And I think the the organizers wanted to
[00:22:14.340 --> 00:22:19.020]   preserve these minority opinions in the data set. So that's why
[00:22:19.340 --> 00:22:23.900]   it came out like this. But the problem with that is, is it
[00:22:23.900 --> 00:22:26.620]   means that there's like contradictions in the labels.
[00:22:26.620 --> 00:22:32.820]   Let's open this again. So I made a few small examples to
[00:22:32.820 --> 00:22:36.500]   demonstrate how it works. So the metric where we're trying to
[00:22:36.500 --> 00:22:40.180]   optimize here is the average agreement with annotators. And,
[00:22:40.180 --> 00:22:45.540]   for example, a model will produce some predictions. So
[00:22:45.540 --> 00:22:48.940]   these are some unique comments here, one, two, and three, and
[00:22:48.940 --> 00:22:52.500]   we make a score, we predict a score for each one. In this
[00:22:52.500 --> 00:22:55.940]   case, I just predict, I put the scores in a range of zero to
[00:22:55.940 --> 00:22:59.460]   one. But actually, this wasn't required by the competition, you
[00:22:59.460 --> 00:23:04.020]   could predict scores in any range, because the relative
[00:23:04.020 --> 00:23:06.980]   positions of the scores is what mattered. So I could multiply
[00:23:06.980 --> 00:23:10.980]   all these scores by 1000. And I'd get the same score at the
[00:23:10.980 --> 00:23:15.620]   end. And the ground truth data has no scores at all. It just
[00:23:15.620 --> 00:23:19.580]   has these ranked pairs, right. And each row represents one
[00:23:19.580 --> 00:23:24.300]   prediction, sorry, one label by an annotator. So in this case,
[00:23:24.300 --> 00:23:27.500]   the annotator decided that comment two is more toxic than
[00:23:27.500 --> 00:23:31.100]   comment one. And in this, maybe a different person here decided
[00:23:31.100 --> 00:23:35.820]   on this pair. And then the metric is how well my models
[00:23:35.820 --> 00:23:41.460]   predictions match up with these. So for example, evaluating on
[00:23:41.460 --> 00:23:45.140]   this first one, we just check two versus one. And here, I
[00:23:45.140 --> 00:23:48.820]   rated two higher than one. So I get this one correct, I get one
[00:23:48.820 --> 00:23:53.220]   point. But when we get to the bottom here, we can see that
[00:23:53.220 --> 00:23:56.460]   it's the same comments again, two and one, but they've been
[00:23:56.460 --> 00:24:01.500]   rated in the opposite way. So it contradicts the first row. And
[00:24:01.500 --> 00:24:04.940]   our model can only make one prediction per comment. So we
[00:24:04.940 --> 00:24:07.820]   get this one wrong. And actually, it's impossible to get
[00:24:07.820 --> 00:24:10.500]   the first one and the second and the last one correct at the
[00:24:10.500 --> 00:24:14.460]   same time, because you can only predict one score here. So in
[00:24:14.460 --> 00:24:18.180]   this little example, naught point seven, five is the maximum
[00:24:18.180 --> 00:24:24.540]   score. One is impossible. And I think in the validation data,
[00:24:24.540 --> 00:24:29.180]   yeah, I think CPMP had a post about it here that someone else,
[00:24:29.180 --> 00:24:32.300]   some other people calculated this before, but he did a good
[00:24:32.300 --> 00:24:36.820]   notebook at the end, where he calculated the taking into
[00:24:36.820 --> 00:24:39.060]   account the contradictions, what's the maximum possible
[00:24:39.060 --> 00:24:43.180]   score on the validation set, and it comes out to be about like
[00:24:43.180 --> 00:24:47.460]   82 83%. This is the best possible score on the
[00:24:47.460 --> 00:24:51.700]   validation. As for the public and private leaderboard, we
[00:24:51.700 --> 00:24:56.620]   don't know what the best possible score is. So but the
[00:24:56.620 --> 00:25:02.100]   public leaderboard one, the best possible score was a bit higher,
[00:25:02.100 --> 00:25:06.340]   because you could see some people got higher than about 80%
[00:25:06.340 --> 00:25:13.380]   right, it went up to 90 something here. So this is, I
[00:25:13.380 --> 00:25:17.220]   think, one of the major problems that people faced in this
[00:25:17.220 --> 00:25:21.380]   competition was that the validation set and the public
[00:25:21.380 --> 00:25:24.940]   leaderboard didn't really agree at all. This, the range of
[00:25:24.940 --> 00:25:29.380]   values seem to be completely different. Often, I think there
[00:25:29.380 --> 00:25:32.100]   was about 50 discussion threads where somebody would say, my
[00:25:32.100 --> 00:25:34.620]   validation score went up, but my leaderboard went down, what's
[00:25:34.620 --> 00:25:38.100]   going on? That was the main thing in this competition.
[00:25:38.100 --> 00:25:40.940]   Sanyam Bhutani: When I looked at the private leaderboard, I
[00:25:40.940 --> 00:25:43.980]   thought this was like a second stage evaluation. That's why
[00:25:43.980 --> 00:25:47.940]   there was such a big shakeup. But then I realized no, there
[00:25:47.940 --> 00:25:49.540]   was an insane split.
[00:25:49.540 --> 00:25:53.580]   Daniel Disney: Right? It's just an insane shakeup, right? You
[00:25:53.580 --> 00:25:57.860]   can see like most of the people here moved up over 1000 places
[00:25:57.860 --> 00:26:01.300]   at the end. Yeah. Except for Manuel here, who seemed to have
[00:26:01.300 --> 00:26:03.980]   done well on both leaderboards. But that's very rare. Most
[00:26:03.980 --> 00:26:04.460]   people...
[00:26:04.460 --> 00:26:07.380]   Sanyam Bhutani: Still is crazy. 40 positions up is still a lot.
[00:26:07.380 --> 00:26:09.780]   Daniel Disney: Right. In a normal competition, you would
[00:26:09.780 --> 00:26:12.620]   consider that a big shakeup. But in this one, it's like he's
[00:26:12.620 --> 00:26:15.820]   barely moved at all, right? The guy who won with, you know,
[00:26:15.820 --> 00:26:19.380]   almost 2000 places, and amazingly, only two submissions
[00:26:19.380 --> 00:26:23.940]   to the competition. Yeah, that was crazy, right? But yeah, me
[00:26:23.940 --> 00:26:27.940]   as well. I was in position like 1500 on the public leaderboard.
[00:26:27.940 --> 00:26:31.300]   And it was mostly just, you know, faith keeping me going in
[00:26:31.300 --> 00:26:37.060]   this competition. I went into the competition, determined to
[00:26:37.060 --> 00:26:40.820]   trust my cross validation score, like that was my mindset,
[00:26:40.820 --> 00:26:46.260]   because in the previous one, I had worked on the Chai Hindi and
[00:26:46.260 --> 00:26:51.420]   Tamil competition. And in that one, I did quite well in that
[00:26:51.420 --> 00:26:54.980]   one, actually, I got a silver, but I looking through my
[00:26:54.980 --> 00:26:58.620]   submissions, I noticed that there was one that had a higher
[00:26:58.780 --> 00:27:03.020]   cross validation score and a higher private leaderboard
[00:27:03.020 --> 00:27:06.300]   score. And if I had chosen that, I would have got a gold, but I
[00:27:06.300 --> 00:27:09.340]   didn't pick it. So I was kind of angry about that. And I thought
[00:27:09.340 --> 00:27:11.900]   in the next competition, no matter what, I'm going to trust
[00:27:11.900 --> 00:27:14.260]   my cross validation score, and I'm going to ignore the public
[00:27:14.260 --> 00:27:17.780]   leaderboard. So that's what I did in this case. But it was a
[00:27:17.780 --> 00:27:21.620]   real like, test of faith on trusting your your cross
[00:27:21.620 --> 00:27:22.780]   validation score this one
[00:27:22.780 --> 00:27:24.500]   Sanyam Bhutani: It did work out really well for you.
[00:27:24.500 --> 00:27:28.380]   Daniel Disney: Yes, yeah, it did. I've got a lot of faith in
[00:27:28.380 --> 00:27:33.900]   cross validation now after this. So that's about the metrics for
[00:27:33.900 --> 00:27:37.260]   this competition. So as you mentioned before, there was no
[00:27:37.260 --> 00:27:41.860]   official training data in this one. But we could use a lot of
[00:27:41.860 --> 00:27:45.540]   different data sets. And particularly the the previous
[00:27:45.540 --> 00:27:48.860]   jigsaw competitions had a bunch of data sets. And you also
[00:27:48.860 --> 00:27:55.140]   mentioned the the Reddit one too. This one is, I think the
[00:27:55.140 --> 00:27:58.460]   comments come from Reddit, and then they've been labeled with
[00:27:58.460 --> 00:28:03.860]   some value between minus one and one. And I also use this one
[00:28:03.860 --> 00:28:08.540]   offensive out, which I think it comes from a conference called
[00:28:08.540 --> 00:28:13.260]   maybe semi vowel semantic evaluation. And it's a massive
[00:28:13.260 --> 00:28:17.980]   data set, like 8 million examples. And the the labels, I
[00:28:17.980 --> 00:28:20.660]   think are the average of some machine learning model
[00:28:20.660 --> 00:28:25.740]   predictions. So this one was very large, but kind of noisy,
[00:28:25.740 --> 00:28:31.700]   because it's not labeled by humans. A difficult point when
[00:28:31.700 --> 00:28:36.260]   selecting data sets was that the labeling is different on each of
[00:28:36.260 --> 00:28:39.100]   them, right? In this one, we want to predict some some
[00:28:39.100 --> 00:28:43.500]   rankings. But we have regression data sets, and we have
[00:28:43.500 --> 00:28:46.740]   classification data sets, and we need to kind of make the labels
[00:28:47.340 --> 00:28:52.380]   work for us. So we can predict things in this range. So we had
[00:28:52.380 --> 00:28:55.500]   to come up with ways of like converting one set of labels to
[00:28:55.500 --> 00:28:59.540]   another one. And I found it was pretty easy to create more
[00:28:59.540 --> 00:29:03.300]   ranking pairs because like the validation set, because if you
[00:29:03.300 --> 00:29:07.100]   have data set in this format, you can easily just pair them up,
[00:29:07.100 --> 00:29:11.140]   right? And then you can create data in this format, which works
[00:29:11.140 --> 00:29:14.860]   but not so well. And another another method,
[00:29:14.860 --> 00:29:18.100]   Sanyam Bhutani: sorry, may I ask how How are you doing that? So
[00:29:18.100 --> 00:29:22.220]   I'm looking at the I switch the screen sharing, I'm looking at
[00:29:22.220 --> 00:29:26.260]   the offense evolve 2020. How are you creating these pairs?
[00:29:26.260 --> 00:29:31.540]   Because I believe these didn't have four evaluator rankings.
[00:29:31.540 --> 00:29:33.580]   Sorry, question.
[00:29:33.580 --> 00:29:37.620]   Unknown: No, no, no, that's fine. Um, I think so I actually
[00:29:37.620 --> 00:29:43.540]   the version of this I used was from I'm sorry, the wrong
[00:29:43.540 --> 00:29:48.460]   competition. Well, I didn't think I'd find it. But anyway,
[00:29:48.460 --> 00:29:52.220]   the version of the status that I used was one, a pre process when
[00:29:52.220 --> 00:29:56.020]   someone had uploaded to Kaggle already. And I think, right, I
[00:29:56.020 --> 00:30:03.420]   think I mentioned it in my solution, that it had labels
[00:30:03.420 --> 00:30:09.140]   that were like an average of annotators. So it was it was, I
[00:30:09.140 --> 00:30:11.260]   was just using it like a regression data set really
[00:30:12.340 --> 00:30:18.060]   founded it by arrowhead. That's the username. Right? I believe
[00:30:18.060 --> 00:30:21.100]   they've already pre processed it. You can see the description.
[00:30:21.100 --> 00:30:23.180]   I'm just quickly sharing it for the audience.
[00:30:23.180 --> 00:30:27.820]   Okay, yeah, thanks. Yeah. Yeah, it was quite hard to keep track
[00:30:27.820 --> 00:30:33.220]   of all the data sets in this, I felt like this was the main
[00:30:33.220 --> 00:30:40.460]   thing to focus on here. Because like, how I mentioned, you know,
[00:30:40.460 --> 00:30:44.660]   different people have different senses of toxicity. I felt maybe
[00:30:44.660 --> 00:30:48.660]   we could simulate that by using different data sets that have
[00:30:48.660 --> 00:30:51.180]   different labelling schemes. And then if you ensemble them
[00:30:51.180 --> 00:30:57.220]   together, at the end, it maybe it's like finding an average of
[00:30:57.220 --> 00:31:00.460]   different people's opinions, you know, using but with data sets
[00:31:00.460 --> 00:31:04.060]   instead of people, right. So that was my original idea that I
[00:31:04.060 --> 00:31:06.460]   would train a bunch of different models on different data sets
[00:31:06.460 --> 00:31:11.100]   and then take the average but in the end, it was a little bit
[00:31:11.100 --> 00:31:16.780]   different. But one of the pre processing thing which I took
[00:31:16.780 --> 00:31:21.020]   from another public notebook, which was using the data from
[00:31:21.020 --> 00:31:26.660]   the first jigsaw competition. So this one had it was binary
[00:31:26.660 --> 00:31:31.060]   classification, but with multiple labels. So each, each
[00:31:31.060 --> 00:31:35.940]   text can have it can be classed as toxic or obscene or an
[00:31:35.940 --> 00:31:41.820]   insult, or any of these, right. And what people were doing was
[00:31:41.820 --> 00:31:44.700]   just taking the sum of these labels and dividing by the
[00:31:44.700 --> 00:31:49.700]   number of labels. So you can just add up all the ones. And in
[00:31:49.700 --> 00:31:52.700]   this case, I've also weighted severe toxic as two. So that's
[00:31:52.700 --> 00:31:55.780]   why there's two here, and it's a seven here. And then you divide
[00:31:55.780 --> 00:31:58.380]   by seven, and you just get a score like this. And then you
[00:31:58.380 --> 00:32:02.260]   can use this like a regression target. And when I saw this, I
[00:32:02.260 --> 00:32:05.140]   thought this is, you know, too crude, surely this won't work.
[00:32:05.220 --> 00:32:10.300]   But it worked quite well, actually. So I stuck with it.
[00:32:10.300 --> 00:32:14.340]   And I noticed some other people tuned the weight of each of
[00:32:14.340 --> 00:32:18.660]   these classes as well. So they didn't just use one, you know,
[00:32:18.660 --> 00:32:22.580]   they multiplied each one by a different value. But because I
[00:32:22.580 --> 00:32:26.100]   was busy iterating on data sets, I didn't really have time to
[00:32:26.100 --> 00:32:28.620]   tune these weights. But given more time, I think that's
[00:32:28.620 --> 00:32:32.060]   probably another another way to get a little advantage as well.
[00:32:33.820 --> 00:32:36.660]   So basically, I tried to make as many data sets as I could into
[00:32:36.660 --> 00:32:40.180]   regression data sets and or ranking data sets, because
[00:32:40.180 --> 00:32:42.260]   classification was a bit harder to use here.
[00:32:42.260 --> 00:32:44.700]   Makes sense.
[00:32:44.700 --> 00:32:49.140]   Yeah. And for the validation data, we already talked about
[00:32:49.140 --> 00:32:56.780]   how it works, but creating a, like a k fold cross validation
[00:32:56.780 --> 00:33:00.540]   split was kind of difficult here as well, because the same text
[00:33:00.540 --> 00:33:03.100]   appears in a bunch of different places, and the same pairs of
[00:33:03.100 --> 00:33:06.580]   text appear in different places, you have to be careful not to
[00:33:06.580 --> 00:33:10.740]   leak texts into different folds. If you do just a simple random,
[00:33:10.740 --> 00:33:14.700]   like naive split, then you'll end up with the same texts and
[00:33:14.700 --> 00:33:17.540]   the same pairs of texts in both in your training and in your
[00:33:17.540 --> 00:33:22.260]   validation codes. So I use this method, this union find
[00:33:22.260 --> 00:33:27.900]   algorithm, which I think I've got it open here. Yeah, it was
[00:33:28.420 --> 00:33:34.660]   this notebook was by this guy column, which it's a really nice
[00:33:34.660 --> 00:33:41.340]   simple implementation, you end up with folds that don't don't
[00:33:41.340 --> 00:33:48.500]   leak texts. So that was good. So I use that. And I didn't do any
[00:33:48.500 --> 00:33:52.580]   aggregation of the validation set. I mentioned this because I
[00:33:52.580 --> 00:33:56.700]   noticed a lot of competitors did in this one. Some people did a
[00:33:56.700 --> 00:34:00.860]   majority voting on or just removed disagreements in the
[00:34:00.860 --> 00:34:08.180]   data set. And I think this can work well, because you kind of
[00:34:08.180 --> 00:34:10.980]   removing the noise from your data set. So if you're training
[00:34:10.980 --> 00:34:16.340]   on the validation data, then if you do majority voting, you're
[00:34:16.340 --> 00:34:20.140]   kind of getting a stronger signal. But the downside is that
[00:34:20.140 --> 00:34:23.340]   the test data has these disagreements in so if you
[00:34:23.340 --> 00:34:26.300]   remove them, then you're creating really like overly
[00:34:26.300 --> 00:34:31.220]   optimistic valid cross validation scores because you've
[00:34:31.220 --> 00:34:33.740]   removed all of the difficult parts from the data set already.
[00:34:33.740 --> 00:34:36.900]   So you get a great score on cross validation, but it might
[00:34:36.900 --> 00:34:39.820]   not translate to the private leaderboard. So I didn't do
[00:34:39.820 --> 00:34:46.860]   that. I kept the data set as it was. For loss functions, I
[00:34:46.860 --> 00:34:49.220]   mentioned these because these ones are standard. Of course, if
[00:34:49.220 --> 00:34:51.540]   you're doing you know, regression, you use mean squared
[00:34:51.540 --> 00:34:54.860]   error, I guess people know that this this margin ranking loss
[00:34:54.860 --> 00:34:57.580]   was new to me in this competition. So I've included
[00:34:57.580 --> 00:35:03.180]   this here. And actually, there's a implementation of it just in
[00:35:03.180 --> 00:35:06.820]   in PyTorch already. So I didn't have to implement anything
[00:35:06.820 --> 00:35:11.620]   myself. And basically how it works is perfect for the
[00:35:11.620 --> 00:35:18.140]   validation that validation set in this competition. You, you
[00:35:18.140 --> 00:35:21.220]   have to do two forward passes for every backwards pass, but
[00:35:21.220 --> 00:35:25.220]   you have text one and text two, and you generate this x one and
[00:35:25.220 --> 00:35:34.340]   x two. And then you your y the the label here is is one if text
[00:35:34.340 --> 00:35:37.900]   one should be ranked higher and minus one if text two should be
[00:35:37.900 --> 00:35:43.660]   ranked higher. And then what happens is if you've ranked
[00:35:43.660 --> 00:35:46.820]   them in the correct order, then this expression will evaluate
[00:35:47.220 --> 00:35:50.340]   to a negative number, and then the max function will return
[00:35:50.340 --> 00:35:52.980]   zero. So if you've ranked them in the correct order, you get
[00:35:52.980 --> 00:35:56.380]   zero for loss. But if you rank them in the wrong order, then
[00:35:56.380 --> 00:35:59.460]   this will be a positive number. So this will be your loss value.
[00:35:59.460 --> 00:36:05.460]   And also there's this margin so you can add like an extra
[00:36:05.460 --> 00:36:09.900]   penalty to the loss. And it also means that even if you rank
[00:36:09.900 --> 00:36:12.780]   them in the correct order, but these two values are too close
[00:36:12.780 --> 00:36:17.180]   together, then this difference will be small, which means that
[00:36:17.180 --> 00:36:20.700]   the margin will still produce a positive value. So it kind of
[00:36:20.700 --> 00:36:24.660]   encourages your model not to predict the not to produce
[00:36:24.660 --> 00:36:28.100]   these two values too close together, there should be more
[00:36:28.100 --> 00:36:30.220]   separation between them, otherwise, you'll still get a
[00:36:30.220 --> 00:36:37.220]   loss. So you can use this one to train on the ranked pairs, even
[00:36:37.220 --> 00:36:43.100]   though there's no, like, explicit regression target
[00:36:43.100 --> 00:36:46.940]   values, but you can still fit a model that will produce a
[00:36:46.940 --> 00:36:51.300]   single numerical value for each text, which is really cool, I
[00:36:51.300 --> 00:36:57.060]   think. Yeah. As for model architectures, I didn't really
[00:36:57.060 --> 00:37:01.180]   do much experimentation here. I guess, as I said earlier, I kind
[00:37:01.180 --> 00:37:07.460]   of like just narrowing my focus and refining something. So I
[00:37:07.460 --> 00:37:10.940]   kind of went with, you know, out of the box model architectures,
[00:37:10.940 --> 00:37:13.980]   which in NLP these days, basically, it means you're using
[00:37:13.980 --> 00:37:17.420]   some BERT variant, if you're doing any kind of classification
[00:37:17.420 --> 00:37:20.500]   or regression tasks, you should be using some BERT variant. If
[00:37:20.500 --> 00:37:23.460]   it's a language generation task, then maybe something different.
[00:37:23.460 --> 00:37:30.060]   But yeah, in this competition, for sure, I ended up using
[00:37:30.060 --> 00:37:35.580]   various BERT variants. Roberta is always a good starting point, I
[00:37:35.580 --> 00:37:40.620]   think, rather than the vanilla, but this one generally does
[00:37:40.620 --> 00:37:45.020]   better and in every way. So anyone else who's thinking of,
[00:37:45.020 --> 00:37:49.140]   you know, starting a NLP Kaggle competition, and you want a
[00:37:49.140 --> 00:37:52.980]   strong baseline, just go with Roberta, it does well on
[00:37:52.980 --> 00:37:59.420]   everything. And it's not so big, like you can fit it easily on
[00:37:59.420 --> 00:38:03.580]   on your GPU and run some experiments, right. I also did,
[00:38:03.580 --> 00:38:09.460]   I also use the BERT quite a lot. But the BERT is another BERT
[00:38:09.460 --> 00:38:13.140]   variant, this one has, I think it's, they call it disentangled
[00:38:13.140 --> 00:38:16.260]   convolution, which is like a modified, sorry, disentangled
[00:38:16.260 --> 00:38:18.980]   attention, not convolution, disentangled attention, which is
[00:38:18.980 --> 00:38:25.060]   a modified attention mechanism, which, practically speaking, it
[00:38:25.060 --> 00:38:29.420]   often or sometimes does better than Roberta, but at the cost of
[00:38:29.420 --> 00:38:34.580]   an increased memory footprint. So if you're like me, and you do
[00:38:34.580 --> 00:38:38.860]   most of your training on Google Colab, or Kaggle, you only got
[00:38:38.900 --> 00:38:42.700]   16 gigabytes of space, if you're trying to load the BERT a large,
[00:38:42.700 --> 00:38:45.780]   then you're going to be training with a very small batch size,
[00:38:45.780 --> 00:38:50.460]   you know, like two or four, which is fine. But considering
[00:38:50.460 --> 00:38:53.220]   all of the some of the large data sets I was using in this
[00:38:53.220 --> 00:38:57.900]   competition, it meant that some of my training runs would time
[00:38:57.900 --> 00:39:02.260]   out in like one after one epoch, they would it would be 24 hours
[00:39:02.260 --> 00:39:02.660]   would pass.
[00:39:02.660 --> 00:39:05.100]   Yeah.
[00:39:06.740 --> 00:39:09.180]   This is a question in the chat. Oh, go ahead.
[00:39:09.180 --> 00:39:14.100]   They just asking if we were just using Kaggle GPUs or external
[00:39:14.100 --> 00:39:14.660]   resources.
[00:39:14.660 --> 00:39:24.420]   Yeah, well, just Kaggle and Colab Pro. So I just not even
[00:39:24.420 --> 00:39:25.700]   ProPlus, just Colab Pro.
[00:39:25.700 --> 00:39:31.020]   Yeah, I don't have, I didn't, I don't really have access to any
[00:39:31.020 --> 00:39:36.260]   other GPUs at the moment. And I use my Kaggle quota up every
[00:39:36.260 --> 00:39:38.180]   week. And then I went over to Colab.
[00:39:38.180 --> 00:39:43.460]   Okay. Sorry, you were talking about deburr eating up all the
[00:39:43.460 --> 00:39:44.500]   memory and time.
[00:39:44.500 --> 00:39:47.500]   Right, right. Yeah, yeah. So deburr ate up most of my memory.
[00:39:47.500 --> 00:39:51.980]   Rember also eats up a lot of memory. This is, I think the
[00:39:51.980 --> 00:39:56.340]   main difference of Rember compared to vanilla BERT is they
[00:39:57.620 --> 00:40:01.940]   what's it called decoupled the input and output embeddings. So
[00:40:01.940 --> 00:40:05.260]   this is another optimization that
[00:40:05.260 --> 00:40:09.860]   potentially improves the performance but comes at a cost
[00:40:09.860 --> 00:40:13.260]   of increased memory. So I was in a similar situation to the BERT
[00:40:13.260 --> 00:40:15.220]   you can only train with a very small batch size.
[00:40:15.220 --> 00:40:20.060]   Another cool thing about Rember and the reason I included it
[00:40:20.060 --> 00:40:25.500]   here was that the the pre trained model is multilingual.
[00:40:25.500 --> 00:40:29.020]   It was trained on a bunch of different languages. And I found
[00:40:29.020 --> 00:40:32.420]   out about this one in the Chai Hindi and Tamil question
[00:40:32.420 --> 00:40:36.740]   answering competition where it performed really well. And I
[00:40:36.740 --> 00:40:40.620]   noticed one of the data sets in this competition, the third
[00:40:40.620 --> 00:40:45.940]   jigsaw competition was multilingual data set too. So I
[00:40:45.940 --> 00:40:50.340]   thought maybe I can fine tune this multilingual checkpoint on
[00:40:50.340 --> 00:40:53.420]   this multilingual data set and I'll have a another cool model
[00:40:53.420 --> 00:40:58.540]   to add to my ensemble but I didn't really get that data set
[00:40:58.540 --> 00:41:04.380]   to work because it was binary classification. But I found the
[00:41:04.380 --> 00:41:08.220]   Rember worked well anyway, just on any data set. And actually, I
[00:41:08.220 --> 00:41:12.140]   think this was my best performing single model. So yeah,
[00:41:12.140 --> 00:41:14.580]   in the future, I'll probably I'll probably try Rember as
[00:41:14.580 --> 00:41:18.900]   well. Those were the only ones I ended up using in my final
[00:41:18.900 --> 00:41:24.500]   submissions. I think probably if I had added some other
[00:41:24.500 --> 00:41:27.700]   different architectures, I could have improved my score a bit
[00:41:27.700 --> 00:41:30.980]   more. But I was mostly as I said, I was mostly focusing on
[00:41:30.980 --> 00:41:34.820]   using different data sets. So I didn't really explore this very
[00:41:34.820 --> 00:41:42.860]   much. So what I did explore was different fine tuning on a
[00:41:42.860 --> 00:41:46.820]   bunch of different data sets. And I've called this strategy I
[00:41:46.820 --> 00:41:50.300]   came up with here multi stage fine tuning, but I don't know,
[00:41:50.300 --> 00:41:53.500]   for lack of a better name, I don't know if this is just
[00:41:53.500 --> 00:41:59.380]   something I tried and it's, you know, doesn't work in general,
[00:41:59.380 --> 00:42:03.700]   or maybe there is some research paper published where they've
[00:42:03.700 --> 00:42:07.260]   done this, but called it by a different name. I'm not sure if
[00:42:07.260 --> 00:42:10.940]   somebody doesn't know, please send me the paper, I guess. But
[00:42:10.940 --> 00:42:15.380]   the idea is basically to just add more fine tuning stages. So
[00:42:15.940 --> 00:42:23.180]   I in NLP these days, I guess most people know that we have
[00:42:23.180 --> 00:42:27.700]   this paradigm of pre training and fine tuning. So usually you
[00:42:27.700 --> 00:42:30.580]   start with a pre trained transformer that you download
[00:42:30.580 --> 00:42:34.900]   off the Hugging Face Hub, and it's already been pre trained on
[00:42:34.900 --> 00:42:40.380]   like a mask, the general mask language modeling, pre training
[00:42:40.380 --> 00:42:43.700]   objective, or maybe an auto regressive one, depending on the
[00:42:43.700 --> 00:42:46.500]   architecture. But then you take that and you fine tune it on
[00:42:46.500 --> 00:42:50.060]   your specific task, and you get a better performance than if you
[00:42:50.060 --> 00:42:54.140]   had just directly trained a model that had randomly
[00:42:54.140 --> 00:43:00.540]   initialized weights. But again, in the the the Chai Hindi and
[00:43:00.540 --> 00:43:05.700]   Tamil competition, I and I think several other people found that
[00:43:05.700 --> 00:43:12.220]   you got better performance if you first fine tuned your model
[00:43:12.260 --> 00:43:18.020]   on a different data set of the same task. So in that
[00:43:18.020 --> 00:43:22.340]   competition, it was question answering. So if you fine tune
[00:43:22.340 --> 00:43:26.500]   on a English question answering data set, and then fine tune
[00:43:26.500 --> 00:43:30.500]   again, on the Hindi and Tamil question answering data set, you
[00:43:30.500 --> 00:43:33.660]   did, you got a better performance. And I thought,
[00:43:33.660 --> 00:43:36.340]   well, that worked well with two data sets. This time, I've got
[00:43:36.340 --> 00:43:40.300]   loads of data sets. So why don't I just keep doing it. So that's
[00:43:40.300 --> 00:43:44.500]   what I did, I, I tried fine tuning on each of the data sets
[00:43:44.500 --> 00:43:49.740]   individually. And then I found the best way to stack them was
[00:43:49.740 --> 00:43:55.260]   to order them from like the worst performing one to the best
[00:43:55.260 --> 00:43:58.700]   performing one in terms of their performance on the validation
[00:43:58.700 --> 00:44:03.740]   set. So I started with the offensive our data set, and I
[00:44:03.740 --> 00:44:08.700]   got about, I think 68% on the validation set, and then the
[00:44:08.700 --> 00:44:11.380]   second jigsaw data set, and then that improved the performance to
[00:44:11.380 --> 00:44:15.180]   about 69. And then the first jigsaw data set the performance
[00:44:15.180 --> 00:44:19.740]   up to about 70. And then finally, do a cross for like a
[00:44:19.740 --> 00:44:21.900]   fivefold split on the validation data.
[00:44:21.900 --> 00:44:25.020]   Sanyam Bhutani: And I think in computer vision, this is called
[00:44:25.020 --> 00:44:29.340]   as progressive learning. Oh, really? That that's the term we
[00:44:29.340 --> 00:44:32.020]   were looking for. I think that might be
[00:44:32.020 --> 00:44:34.940]   Unknown: okay, cool. Great. Thanks. Yeah, I don't know very
[00:44:34.940 --> 00:44:39.060]   much about computer vision. So yeah, I hadn't heard of that
[00:44:39.060 --> 00:44:41.820]   before. Cool. So progressive learning, we call it I don't
[00:44:41.820 --> 00:44:44.140]   know much about machine learning broadly, so I might be totally
[00:44:44.140 --> 00:44:44.340]   wrong.
[00:44:44.340 --> 00:44:49.900]   Thanks. Thanks for giving me the name. Anyway, I'll, I'll look
[00:44:49.900 --> 00:44:53.500]   that up after that after this. And yeah, maybe we can call it
[00:44:53.500 --> 00:44:57.540]   that instead. But yeah, I that's what I did in this case. And I
[00:44:57.540 --> 00:45:02.620]   found it worked quite well for this competition. And I found it
[00:45:02.620 --> 00:45:06.700]   was possible as well to switch between the data sets with
[00:45:06.700 --> 00:45:11.500]   regression targets and the data sets with ranking paired data,
[00:45:11.500 --> 00:45:14.660]   because you don't need to change the model architecture at all to
[00:45:14.660 --> 00:45:19.060]   do that. It doesn't work so well if you try and switch between
[00:45:19.060 --> 00:45:23.460]   that and binary classification data set because you have to
[00:45:23.460 --> 00:45:26.820]   modify the last layer of the network. And you can do that.
[00:45:26.820 --> 00:45:31.660]   But I found I lost some of the performance when I modified the
[00:45:31.660 --> 00:45:35.860]   last layer. So yeah, I just stuck with mostly regression
[00:45:35.860 --> 00:45:42.940]   and ranking data. Yeah, so here's some of my results. This
[00:45:42.940 --> 00:45:47.700]   is some of my better performing models on each sample of the
[00:45:47.700 --> 00:45:51.340]   data. And I've ordered them by cross validation, I won't read
[00:45:51.340 --> 00:45:53.820]   all the numbers out. It's not very interesting. But the idea
[00:45:53.820 --> 00:45:57.700]   is just for to show the basically the lack of
[00:45:57.700 --> 00:46:01.980]   correlation between these two, that my best cross validation
[00:46:01.980 --> 00:46:06.660]   model had not very good public leaderboard score. And then this
[00:46:06.660 --> 00:46:10.860]   one, which had a complete garbage cross validation score
[00:46:10.860 --> 00:46:16.820]   was my highest score on the public leaderboard. And yeah,
[00:46:16.820 --> 00:46:21.140]   this this, particularly this combination of TF idea for
[00:46:21.140 --> 00:46:24.100]   encoding, and then a ridge regression was what a lot of
[00:46:24.100 --> 00:46:28.220]   people use. And it turned out that this one did really well on
[00:46:28.220 --> 00:46:32.380]   the public leaderboard. It really overfit the public
[00:46:32.380 --> 00:46:37.900]   leaderboard really hard. So you if you ran on the validation
[00:46:37.900 --> 00:46:40.700]   set, you would see there is something wrong because the
[00:46:40.700 --> 00:46:44.580]   validation score is really low. And I noticed this when I was
[00:46:44.580 --> 00:46:51.500]   looking at the public notebooks in this competition. Maybe I
[00:46:51.500 --> 00:46:55.540]   can find them actually, I think some of them have probably
[00:46:55.540 --> 00:47:02.940]   disappeared. Now, but I guess if you order by what is it? Best
[00:47:02.940 --> 00:47:06.620]   score, right? I think it should. Yeah, it shows the highest,
[00:47:06.620 --> 00:47:10.540]   like all of these ones. I mean, even here, I think it became a
[00:47:10.540 --> 00:47:13.100]   bit of a joke at the end of the competition, right? Even in the
[00:47:13.100 --> 00:47:16.500]   title, it's just overfitting the leaderboard. That's what they
[00:47:16.500 --> 00:47:20.820]   were trying to do. But I'm not sure why people spend so much
[00:47:20.820 --> 00:47:24.460]   time overfitting. But anyway, you could get like a ridiculous
[00:47:24.460 --> 00:47:27.700]   so I was going to say even if you go to the leaderboard, it's
[00:47:27.700 --> 00:47:30.700]   quite visible there. Right, right. Yeah, I think that's
[00:47:30.700 --> 00:47:33.780]   that's also true. Right? You see these crazily high scores.
[00:47:33.780 --> 00:47:38.460]   Point nine one and the private was point eight one some digit.
[00:47:38.460 --> 00:47:42.380]   Right? Yeah, the Yeah, the top. Yeah, top private score was
[00:47:42.380 --> 00:47:46.420]   point eight one. And then on the validation set, you got another
[00:47:46.420 --> 00:47:50.100]   set of numbers, they were more like point seven something. But
[00:47:50.100 --> 00:47:54.900]   I think a lot of these models that we're getting like 9.9 on
[00:47:54.900 --> 00:47:58.780]   the public leaderboard, they still had less than point seven
[00:47:58.780 --> 00:48:02.740]   on the validation set, they were like point six, five or point
[00:48:02.740 --> 00:48:06.500]   six, seven, something like that. But it seemed like a lot of
[00:48:06.500 --> 00:48:11.100]   people didn't really care. I saw a lot of threads in the
[00:48:11.100 --> 00:48:17.700]   discussion where people would kind of try to explain it. Why
[00:48:17.700 --> 00:48:21.020]   why is this doing so well on the leaderboard? And I thought many
[00:48:21.020 --> 00:48:24.540]   people were kind of asking the wrong questions, they would say
[00:48:24.540 --> 00:48:29.180]   something like, why is ridge regression so much better than
[00:48:29.180 --> 00:48:34.140]   transformer networks in this competition? But I think that
[00:48:34.140 --> 00:48:37.540]   question kind of already assumes the answer, which is that you
[00:48:37.540 --> 00:48:42.700]   think ridges better. But the real question is, why? Why does
[00:48:42.700 --> 00:48:45.700]   ridge perform so well on the public leaderboard, but not on
[00:48:45.700 --> 00:48:49.860]   the validation set? So I think, yeah, really, this was a
[00:48:49.860 --> 00:48:56.700]   competition about the validation set. And I think, yeah, I've
[00:48:56.700 --> 00:48:59.460]   only got one more slide here, which is just about how I
[00:48:59.460 --> 00:49:04.300]   ensembled my models. And it was fairly simple, just a weighted
[00:49:04.300 --> 00:49:10.420]   mean of the model outputs. And I used Optunr to find the weights,
[00:49:10.420 --> 00:49:13.300]   it was my first time trying this library, I'd heard a bunch of
[00:49:13.300 --> 00:49:19.900]   people recommending it, and it's pretty great, I think. You just
[00:49:19.900 --> 00:49:22.820]   define the objective you're trying to maximise or minimise
[00:49:22.820 --> 00:49:27.060]   and just let it run. And it finds some great weights, I'll
[00:49:27.060 --> 00:49:29.660]   probably use it in the future for hyperparameter tuning,
[00:49:29.660 --> 00:49:33.220]   things like that, too. And for selecting which models to
[00:49:33.220 --> 00:49:37.180]   include in the ensemble, I used that kind of simple, I guess, a
[00:49:37.180 --> 00:49:40.860]   greedy algorithm where in a series of rounds, starting with
[00:49:40.860 --> 00:49:46.020]   my best performing single model on the validation set, then I
[00:49:46.020 --> 00:49:48.820]   would try adding each of the other models in turn, and
[00:49:48.820 --> 00:49:52.380]   whichever one increases the out-of-fold score the most, you
[00:49:52.380 --> 00:49:55.060]   take that one, and now you have two models, and then you do
[00:49:55.060 --> 00:49:58.260]   another round and add a third model. And you keep adding one
[00:49:58.260 --> 00:50:01.620]   model at a time until the out-of-fold score stops
[00:50:01.620 --> 00:50:05.620]   improving. And then that's the final ensemble. So with this
[00:50:05.620 --> 00:50:13.620]   one, I got up to about 0.719, almost 0.72. And this was only
[00:50:13.620 --> 00:50:19.780]   like a slight increase over this, but in a Kaggle
[00:50:19.780 --> 00:50:28.820]   competition, this 0.05 increase is still significant. Of course,
[00:50:28.820 --> 00:50:31.420]   if you were going to deploy this model into production, you
[00:50:31.420 --> 00:50:35.020]   wouldn't add six transformer networks for that small
[00:50:35.020 --> 00:50:39.620]   increase. But if you're in a Kaggle competition, it's worth
[00:50:39.620 --> 00:50:42.620]   it, no matter how inefficient the runtime is, as long as it
[00:50:42.620 --> 00:50:47.660]   finishes in under nine hours, you know, it's, it's, it moves
[00:50:47.660 --> 00:50:49.980]   your position up the leaderboard. And I think that's
[00:50:49.980 --> 00:50:50.500]   also,
[00:50:50.500 --> 00:50:53.140]   Sanyam Bhutani: that's also a very interesting constraint,
[00:50:53.140 --> 00:50:55.660]   you know, because many people, nothing against them, many
[00:50:55.660 --> 00:50:58.700]   people get exposed to large computational resources, and
[00:50:58.700 --> 00:51:02.900]   they use a bunch of GPUs. For your solution, you managed to do
[00:51:02.900 --> 00:51:06.180]   it just using Colab and Kaggle, which is within the reach of
[00:51:06.180 --> 00:51:10.020]   many people. I'm spoiled with so many GPUs, which I absolutely
[00:51:10.020 --> 00:51:15.860]   don't use. But that also goes to say how much of a nice solution
[00:51:15.860 --> 00:51:16.540]   this is.
[00:51:16.540 --> 00:51:20.660]   Oh, thanks. Yeah. I mean, I wish I had some nice GPUs like you.
[00:51:20.660 --> 00:51:24.180]   We're talking about your, your, your shiny computer earlier. But
[00:51:25.500 --> 00:51:29.180]   yeah, I, I guess, you know, you make do with the resources that
[00:51:29.180 --> 00:51:33.940]   you have. And I, for the last few years, I guess I've been
[00:51:33.940 --> 00:51:37.500]   training transformer networks, mostly on, on Colab. So I, I
[00:51:37.500 --> 00:51:41.780]   know a few tricks, you know, use mixed precision training, and
[00:51:41.780 --> 00:51:46.180]   gradient accumulation and things like that to compensate for the
[00:51:46.180 --> 00:51:49.900]   fact that you don't have much space. And you can still get
[00:51:49.900 --> 00:51:53.020]   pretty good results. It depends on the competition, of course,
[00:51:53.020 --> 00:51:57.980]   some, some competitions, the computation is like the main
[00:51:57.980 --> 00:52:01.100]   thing. And in others, it's not not a big factor. Yeah. And I
[00:52:01.100 --> 00:52:03.780]   think in this competition, it wasn't such a big factor,
[00:52:03.780 --> 00:52:07.020]   actually. As long as you had access to Colab, I think you
[00:52:07.020 --> 00:52:11.020]   could, you could do well in this competition. And I think this
[00:52:11.020 --> 00:52:14.980]   ensembling technique paid off for me, because even though it
[00:52:14.980 --> 00:52:17.860]   only slightly increased my score, I only barely got a gold,
[00:52:17.860 --> 00:52:21.260]   right. So if I hadn't, if I hadn't done that, then I would
[00:52:21.260 --> 00:52:23.900]   have been down, you know, I would have lost a bunch of
[00:52:23.900 --> 00:52:27.340]   places, because the scores are all very close together here.
[00:52:27.340 --> 00:52:33.460]   Yeah. So yeah, it all paid off in the end. So I'll probably use
[00:52:33.460 --> 00:52:35.340]   this approach again in the future.
[00:52:35.340 --> 00:52:42.340]   Yeah, that's all I had for slides at the moment.
[00:52:42.340 --> 00:52:46.340]   Sanyam Bhutani: I wanted to come back and ask, how are you
[00:52:46.340 --> 00:52:49.940]   tracking your experiments? And how are you organizing your
[00:52:49.940 --> 00:52:50.540]   ideas?
[00:52:51.540 --> 00:52:56.220]   Oh, yeah, that's a good point. So in this competition, I
[00:52:56.220 --> 00:53:00.340]   started using weights and biases for the first time, or maybe I
[00:53:00.340 --> 00:53:03.420]   used a little bit in the previous competition, but I
[00:53:03.420 --> 00:53:07.580]   quite I found I quite liked it. It's very straightforward to use,
[00:53:07.580 --> 00:53:11.900]   right? You just had a few extra lines of code in it. It logs all
[00:53:11.900 --> 00:53:14.820]   of your parameters and creates all these nice visualizations
[00:53:14.820 --> 00:53:17.620]   and stuff. So I started using that and I found it helps a lot.
[00:53:18.580 --> 00:53:24.700]   My, my basic method of recording is just like a spreadsheet, like
[00:53:24.700 --> 00:53:29.420]   a Google Sheet. And I just have some columns for the, you know,
[00:53:29.420 --> 00:53:33.380]   the cross validation and the leaderboard scores and a little
[00:53:33.380 --> 00:53:38.700]   space for notes. And I just one one row per experiment. And then
[00:53:38.700 --> 00:53:46.380]   on on weights and biases, I created a group for with the
[00:53:46.380 --> 00:53:52.500]   same row index, and then it's easy to match up the experiments
[00:53:52.500 --> 00:53:59.100]   like that. And also important to is naming the the when you train
[00:53:59.100 --> 00:54:01.180]   the model, the weights files, you need to label them with the
[00:54:01.180 --> 00:54:04.980]   same group as well. I've learned this kind of thing the hard way
[00:54:04.980 --> 00:54:09.580]   because in earlier competitions, I would just sort of, you know,
[00:54:09.580 --> 00:54:11.820]   train stuff and think, oh, it's fine. I'll remember it. And I
[00:54:11.820 --> 00:54:16.060]   would name give things random names. And but then two months
[00:54:16.060 --> 00:54:18.980]   into a Kaggle competition, you're looking back through the
[00:54:18.980 --> 00:54:22.020]   notes, you're like, oh, that model, maybe I can include this
[00:54:22.020 --> 00:54:24.340]   in my ensemble. Where is it? Where are the files? I can't
[00:54:24.340 --> 00:54:28.940]   find the weights. And then okay, I'll retrain it. But now I
[00:54:28.940 --> 00:54:31.380]   didn't log all of the hyper parameters correctly. And I
[00:54:31.380 --> 00:54:34.740]   can't reproduce it. I didn't set the random seed properly. It,
[00:54:34.740 --> 00:54:38.220]   you know, it becomes a nightmare. So I've learned
[00:54:38.220 --> 00:54:42.260]   through experience that it's important to consistently name
[00:54:42.260 --> 00:54:45.140]   things and label things and log all of the parameters, even if
[00:54:45.140 --> 00:54:46.900]   they don't seem important at the time.
[00:54:46.900 --> 00:54:51.260]   Sanyam Bhutani: No, no, thanks. Thanks for the shout out as well.
[00:54:51.260 --> 00:54:56.260]   I'm sure the team will be happy to hear that. Awesome. I don't
[00:54:56.260 --> 00:54:58.980]   need to summarize the solution. Usually I try to summarize just
[00:54:58.980 --> 00:55:01.020]   for my own understanding as well. It was very well
[00:55:01.020 --> 00:55:04.460]   documented. So I'll, I'll move on to the last few questions
[00:55:04.460 --> 00:55:07.780]   that I usually ask. What's what's next for you? Are you
[00:55:07.780 --> 00:55:10.220]   looking at more Kaggle competitions, a bunch of them
[00:55:10.220 --> 00:55:17.260]   just launched Birdcliffe, H&M, Vail? I guess Birdcliffe would
[00:55:17.260 --> 00:55:20.460]   be NLP-ish. There's also another NLP one going on, which which
[00:55:20.460 --> 00:55:21.420]   one are you looking at?
[00:55:21.420 --> 00:55:25.020]   Daniel Tidwell: Right. I just started working on the the
[00:55:25.020 --> 00:55:28.180]   feedback prize competition, actually, which is another NLP
[00:55:28.180 --> 00:55:36.140]   one. Yeah, I got told about it by panelist Mr. Know Nothing on
[00:55:36.140 --> 00:55:40.220]   Kaggle, who we worked together on the Common Lit competition
[00:55:40.220 --> 00:55:44.660]   before. He contacted me about this one. So it seems like a
[00:55:44.660 --> 00:55:49.860]   really interesting competition. It's another one that's kind of
[00:55:49.860 --> 00:55:53.260]   education related. So it interests me as well. But this
[00:55:53.260 --> 00:55:59.420]   one's about trying to find sections in a like a student's
[00:55:59.420 --> 00:56:03.220]   essay that identify different parts of an argument. So is this
[00:56:03.220 --> 00:56:07.340]   section of evidence or a counter claim? Or is it the
[00:56:07.340 --> 00:56:09.860]   conclusion, and you have to highlight different parts of the
[00:56:09.860 --> 00:56:12.940]   text. So I started working on this one, it seems really
[00:56:12.940 --> 00:56:16.780]   interesting, but it's quite quite a difficult task. And
[00:56:16.780 --> 00:56:22.180]   outside of that, like, in future, I do want to try some
[00:56:22.180 --> 00:56:26.260]   non NLP competitions to like, really, I'm obsessed with NLP.
[00:56:26.260 --> 00:56:30.620]   So if there is another NLP competition, I usually get kind
[00:56:30.620 --> 00:56:34.500]   of dragged into that because it seems the most exciting. And
[00:56:34.500 --> 00:56:42.340]   that's my main. Oh, yeah, here he is. Yeah. Yeah. It's my main
[00:56:42.340 --> 00:56:48.260]   criterion for like determining a competition is what's the most
[00:56:48.260 --> 00:56:52.900]   like interesting one, because it's something that you if if I
[00:56:52.900 --> 00:56:54.620]   commit to it, then it's something I'm going to be
[00:56:54.620 --> 00:56:57.660]   working on for three months. And if I'm not interested in the
[00:56:57.660 --> 00:57:00.500]   content, then I'm going to get demotivated. And I'm not going
[00:57:00.500 --> 00:57:03.340]   to get to the end, I'm not going to do well. So I have to be
[00:57:03.340 --> 00:57:07.100]   ultimately have to be interested. But I've seen, like
[00:57:07.100 --> 00:57:12.420]   advice from other Kagglers on this kind of topic is, they
[00:57:12.420 --> 00:57:15.900]   often say, pick the competition that you're going to learn the
[00:57:15.900 --> 00:57:21.100]   most from, right? I think that's probably good advice. But I
[00:57:21.100 --> 00:57:25.220]   think it's often interpreted to mean something like, pick the
[00:57:25.220 --> 00:57:28.380]   one that you currently know the least about so that you have the
[00:57:28.380 --> 00:57:33.020]   most room to learn. But I don't know if that's necessarily the
[00:57:33.020 --> 00:57:36.660]   one that I would learn the most from because, as we talked about
[00:57:36.660 --> 00:57:41.620]   before, like, if you don't know anything in about this area,
[00:57:41.620 --> 00:57:44.740]   then you might just make a bunch of like half baked ideas and not
[00:57:44.740 --> 00:57:49.700]   make any progress. But if you already do know something about
[00:57:49.700 --> 00:57:53.260]   it, and you are already interested in it, you can use it
[00:57:53.260 --> 00:57:56.060]   as an opportunity to like refine your skills, which is another
[00:57:56.060 --> 00:57:59.220]   kind of learning, I think. So even if it's just another NLP
[00:57:59.220 --> 00:58:02.660]   competition, I think I can still learn a lot from it. You know,
[00:58:02.660 --> 00:58:05.860]   of course, I haven't, I haven't solved NLP yet or anything. So
[00:58:05.860 --> 00:58:06.900]   there's lots of room.
[00:58:06.900 --> 00:58:10.100]   Sanyam Bhutani: Makes sense. Thanks. Thanks for the honest
[00:58:10.100 --> 00:58:13.820]   answer. This is usually the last question I ask, what's your
[00:58:13.820 --> 00:58:16.660]   best advice to anyone who's starting their journey in
[00:58:16.660 --> 00:58:17.100]   Kaggle?
[00:58:19.300 --> 00:58:26.100]   I guess I'd say, kind of, well, partially what I said in the
[00:58:26.100 --> 00:58:29.100]   last answer, which is just find things that seem interesting,
[00:58:29.100 --> 00:58:33.500]   don't like make yourself try and make yourself do things that
[00:58:33.500 --> 00:58:35.500]   you aren't going to be interested in, because if it's
[00:58:35.500 --> 00:58:43.660]   not fun, you won't keep going. And I guess, practically, trust
[00:58:43.660 --> 00:58:46.060]   your cross validation score. I know that's not a very novel
[00:58:46.060 --> 00:58:51.100]   answer from a Kaggler. But yeah, if this competition has shown
[00:58:51.100 --> 00:58:56.340]   anything, it's that chasing the leaderboard doesn't end well.
[00:58:56.340 --> 00:59:01.140]   And if you at the start of the competition, you've got to set
[00:59:01.140 --> 00:59:04.500]   up a good, you know, cross validation split, and stick to
[00:59:04.500 --> 00:59:13.140]   it. And that will give you like some bench, some yardstick to
[00:59:13.140 --> 00:59:15.900]   measure all of your submissions against. So it's the most
[00:59:15.900 --> 00:59:19.700]   important thing to get right at the start. And after that, track
[00:59:19.700 --> 00:59:22.900]   your experiments and carefully label and organize everything as
[00:59:22.900 --> 00:59:24.860]   best as you can. I think.
[00:59:24.860 --> 00:59:29.780]   I wanted to point out it extends to all competitions, the
[00:59:29.780 --> 00:59:38.140]   TensorFlow, great barrier refund just ended and the it was really
[00:59:38.140 --> 00:59:43.940]   funny in hindsight, because let me find that thread. Just give
[00:59:43.940 --> 00:59:47.020]   me one second. It was really funny. I'll try to explain the
[00:59:47.020 --> 00:59:52.860]   context. So at that time, Kishan how was 150th on the
[00:59:52.860 --> 00:59:56.580]   leaderboard. And he made this post saying can't wait to learn
[00:59:56.580 --> 01:00:01.300]   from. And he's first now. So that's absolutely incredible.
[01:00:01.300 --> 01:00:06.700]   Looks like he learned very quickly how to get there. Yeah,
[01:00:06.700 --> 01:00:10.820]   it's a big shakeup. I mean, yeah, because the other number
[01:00:10.860 --> 01:00:14.660]   the other shakeup in this competition weren't so big,
[01:00:14.660 --> 01:00:22.300]   just 150 is huge. And that one. Wow. Yeah, I think it's often
[01:00:22.300 --> 01:00:25.860]   the case, right? Yeah, if you your validation score, if you
[01:00:25.860 --> 01:00:27.820]   have a good validation score, even if it's not good on the
[01:00:27.820 --> 01:00:29.780]   leaderboard, you still be good.
[01:00:29.780 --> 01:00:34.740]   Absolutely. I'll wrap up here by mentioning your Twitter profile,
[01:00:34.740 --> 01:00:39.500]   which is again, @amontcomeri_ on there, you can find Adam's
[01:00:39.500 --> 01:00:43.180]   blog where he writes incredible blog posts. I read about his
[01:00:43.180 --> 01:00:46.380]   journey here. I also read about how he was trying to create this
[01:00:46.380 --> 01:00:51.260]   predicting level of English list that he spoke about, and how he
[01:00:51.260 --> 01:00:54.020]   basically has gone about creating different projects. I'm
[01:00:54.020 --> 01:00:56.620]   sure he'll keep writing this. So you can check this out as well.
[01:00:56.620 --> 01:01:00.500]   He's also on LinkedIn, you can just look him up, you'll be able
[01:01:00.500 --> 01:01:03.300]   to find him there. Any other platforms you want to mention?
[01:01:03.300 --> 01:01:07.780]   No, that's it really. I don't use social media so much. But
[01:01:07.780 --> 01:01:11.580]   yeah, yeah. Thanks for the mentioning my blog. I write on
[01:01:11.580 --> 01:01:12.300]   there occasionally.
[01:01:12.300 --> 01:01:17.100]   Awesome. Thanks again, Adam. And huge congrats to you for the
[01:01:17.100 --> 01:01:20.780]   solo gold and competitions master. I'm hoping you'll soon
[01:01:20.780 --> 01:01:23.260]   reach the golden colour on your profile.
[01:01:23.260 --> 01:01:27.100]   Yeah, well, maybe one day. I think that'll take quite a while
[01:01:27.100 --> 01:01:30.780]   longer. But yeah, thanks for having me. And it's, yeah, it's
[01:01:30.780 --> 01:01:32.540]   been really fun to talk about this.
[01:01:32.540 --> 01:01:36.580]   Thanks. Thanks. And thanks to everyone who joined us live. I'll
[01:01:36.580 --> 01:01:37.780]   end the live stream now.
[01:01:37.780 --> 01:01:47.780]   [BLANK_AUDIO]

