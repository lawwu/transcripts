
[00:00:00.000 --> 00:00:05.000]   Now, I'm delighted today to have Karina who is a director of CS25.
[00:00:05.000 --> 00:00:10.000]   Welcome to the second lecture of our current offering of CS25, Transformers United.
[00:00:10.000 --> 00:00:13.000]   So this will be the first lecture featuring an external speaker.
[00:00:13.000 --> 00:00:18.000]   So the rest of the course, we'll have folks come in and talk about, you know,
[00:00:18.000 --> 00:00:21.000]   the state of the art and cool research they're doing as part of their work.
[00:00:21.000 --> 00:00:26.000]   So we have an exciting lineup of speakers for you guys for the rest of the quarter.
[00:00:26.000 --> 00:00:31.000]   So I'm delighted today to have Karina from OpenAI.
[00:00:31.000 --> 00:00:36.000]   So she works on both product and research there and also previously worked at Anthropic.
[00:00:36.000 --> 00:00:38.000]   So I'll let her take it from here.
[00:00:38.000 --> 00:00:47.000]   Can everybody hear? Cool. Let's see.
[00:00:47.000 --> 00:00:57.000]   So before I start the stock, I would love to set a tone for I'm not here to lecture you,
[00:00:57.000 --> 00:01:02.000]   but I would love to have much more collaborative and interactive session.
[00:01:02.000 --> 00:01:11.000]   And I do hear I've heard a lot of, you know, people kind of concerning about AI and the future of AI.
[00:01:11.000 --> 00:01:18.000]   And I understand that the development of a GI is both scary and exhilarating.
[00:01:18.000 --> 00:01:30.000]   And if anything, I would love to get for you to get out from the stock is the fact that I think everybody can have like very meaningful future
[00:01:30.000 --> 00:01:37.000]   and everybody can like build something really, really cool with an AI and as a part of this journey.
[00:01:37.000 --> 00:01:48.000]   Cool. So before I started with a talk, there'll be all about how ARAO is a co-design of product and research.
[00:01:48.000 --> 00:01:55.000]   And, you know, we are shifting both in the labs, we're shifting the focus towards more of a frontier product research,
[00:01:55.000 --> 00:02:01.000]   where the tasks that we teach the models are becoming much more real world tasks.
[00:02:01.000 --> 00:02:10.000]   So and before we get started, I'll just like share some of the vignettes of like what I'm really, really excited about the future of AI
[00:02:10.000 --> 00:02:17.000]   and what the AI is capable of right now that can inspire everybody.
[00:02:17.000 --> 00:02:20.000]   So the first one is actually education.
[00:02:20.000 --> 00:02:27.000]   I'm actually really, really excited about the fact that AI can democratize education.
[00:02:27.000 --> 00:02:33.000]   So here's an example of me asking ChatGPT to explain the Gaussian distribution.
[00:02:33.000 --> 00:02:41.000]   And then with, and in Canvas, it creates, you know, the code itself in order to visualize for me.
[00:02:41.000 --> 00:02:48.000]   So it explains in text, and then I ask it to write in code, and then you can render the code in order to visualize it.
[00:02:48.000 --> 00:02:52.000]   So in a way, it becomes much more, a little bit personalized.
[00:02:54.000 --> 00:03:00.000]   The second demo is also something that a lot of people might want to use it.
[00:03:00.000 --> 00:03:05.000]   So let's, let's see if you take a screenshot from the paper and you want to understand what's going on.
[00:03:05.000 --> 00:03:14.000]   And the model can go and explain in a separate canvas, that's like another feature in ChatGPT.
[00:03:14.000 --> 00:03:21.000]   And then, then you can start to have much more interactive conversation with the model by selecting very,
[00:03:21.000 --> 00:03:28.000]   certain like things and try to like ask more questions, you know, follow up questions.
[00:03:28.000 --> 00:03:35.000]   So in a way, when ChatGPT came out to be first in 2022, it was purely conversational UI.
[00:03:35.000 --> 00:03:36.000]   It was mostly chat.
[00:03:36.000 --> 00:03:45.000]   And as the use cases kind of grown from the product itself, you know, there was no expectation of how people would use ChatGPT at that time.
[00:03:45.000 --> 00:03:51.000]   But over time, you've seen a lot of people started using it for code generation, for writing and long form writing.
[00:03:51.000 --> 00:03:56.000]   It became very obvious that the interface of Chat was kind of very limited.
[00:03:56.000 --> 00:03:57.000]   very limited.
[00:03:57.000 --> 00:04:11.000]   So, um, canvas is kind of like the first attempt, uh, of, um, of our team to break out from the cycle in order to allow people to have much more fine-grained collaboration with AI.
[00:04:11.000 --> 00:04:25.000]   So, um, and actually today, uh, Antarctic published a small report, uh, around the education and how people use, uh, Claude for education.
[00:04:25.000 --> 00:04:33.000]   And it's very interesting to see the correlation between the U.S. bachelor's use and the way people use, um, um, yeah, use cases.
[00:04:33.000 --> 00:04:36.000]   So you can see there's a huge difference between, like, computer science there.
[00:04:36.000 --> 00:04:50.000]   Um, another thing that I'm really, really excited about the AIs is that anyone can create their own tools for themselves, for their friends, for their family, or even run their business.
[00:04:50.000 --> 00:05:02.000]   And, um, you know, the models now can generate a lot of front-end code, and then you can render that code inside the canvas and then iterate on that and have much more visual representation of things.
[00:05:02.000 --> 00:05:13.000]   Um, you know, on Twitter, I've seen a lot of people creating, like, very, very personalized and customized tools that they really, really want, even, like, games like chess.
[00:05:13.000 --> 00:05:37.000]   And with recent image and, uh, image generation model from, um, open AI, you can literally sketch anything, um, on, with your own hands and then recreate or kind of realize what you're dreaming of, uh, in, within, in image and in the style that you really, really want.
[00:05:37.000 --> 00:05:51.000]   So, um, I do really hope that the creativity, the human creativity, and how AI tools can help anyone to become creatives or be an artist, um, in a way that was not possible before.
[00:05:51.000 --> 00:06:01.000]   Another thing that I've tried actually on the mobile is to create a mini game, um, and you can easily do that in Canvas.
[00:06:01.000 --> 00:06:09.000]   So, I asked to just, like, generate me, you know, a React app of an interesting game because I'm the plane, and then the model does that.
[00:06:09.000 --> 00:06:24.000]   And I do hope that in the future, instead of me prompting it to generate a game, it will kind of, like, be more proactive and have this kind of personalization, um, as AI become more of a companion to humans.
[00:06:24.000 --> 00:06:34.000]   Um, and, uh, an exciting thing, um, about CHPT is that the compositionality of different capabilities that can allow you to augment, like, human creativity.
[00:06:34.000 --> 00:06:47.000]   So, here I can, I asked, um, to generate an image of the UI and then, uh, sorry, um, generate an image of the UI and then ask the model to kind of implement this.
[00:06:47.000 --> 00:06:56.000]   And you can do that literally in Canvas right now and, like, it can render, it's not, um, it's just a front-end, it doesn't have full kind of stack engineering.
[00:06:56.000 --> 00:07:06.000]   Um, but, you know, you can do some of the things like this, you can compose different tools and compose in a way that was never possible before.
[00:07:06.000 --> 00:07:15.000]   So, I do hope that ideas like this will be very, very powerful and everybody in this room can play with those things.
[00:07:15.000 --> 00:07:18.000]   Um, and actually, how did we get there?
[00:07:18.000 --> 00:07:22.000]   And I think there are, like, two main scaling paradigms that helped us to get there.
[00:07:22.000 --> 00:07:36.000]   The first one is next token prediction, obviously, if we use a pre-training scaling paradigm, um, where the model kind of becomes, like, a world-building machine understanding the world in a much better scale.
[00:07:36.000 --> 00:07:46.000]   Um, you know, but it becomes, the next token prediction, uh, works really, really good at certain tasks, but it becomes harder at tasks like writing.
[00:07:46.000 --> 00:07:54.000]   If the model predicts a wrong next token, it can kind of, the coherence of the plot will just be lost in the pertaining stage.
[00:07:54.000 --> 00:07:58.000]   And maybe you kind of want to recover this in reinforcement learning.
[00:07:58.000 --> 00:08:08.000]   And the next second paradigm that has happened, um, is, um, RL on a chain of thought for more complex, uh, tasks.
[00:08:08.000 --> 00:08:15.000]   And this is the reasoning work from OpenAI, and now it's being adopted by a lot of different labs.
[00:08:15.000 --> 00:08:28.000]   And, um, I do think this is scaling RL in itself is another paradigm that we can train the models on the axis, um, to, and especially for the real world's tasks that was never possible before.
[00:08:28.000 --> 00:08:41.000]   So, all the agentic work, um, agents like Operator, Deep Research, um, other agents are kind of, like, trained on this kind of new paradigm overall on the chain of thoughts.
[00:08:41.000 --> 00:08:49.000]   Um, so, yeah, I, I, I want to, like, uh, finish this section of, like, vignette of, like, yeah, build, create, build, and make something wonderful in this world.
[00:08:49.000 --> 00:08:57.000]   And I hope, like, people be more inspired rather than scared that AI is going to take their jobs or kind of remove their creativity.
[00:08:57.000 --> 00:09:03.000]   Instead, I feel like people can become more powerful with, with their imaginations with these tools.
[00:09:03.000 --> 00:09:14.000]   So, I currently work at OpenAI, before that I worked at Anthropic, and, um, I was kind of, like, at the intersection of product and research.
[00:09:14.000 --> 00:09:20.000]   When I first came to Anthropic, I was a product engineer.
[00:09:20.000 --> 00:09:23.000]   And then, over time, I switched to research engineering.
[00:09:23.000 --> 00:09:27.000]   So, my background is mostly both in product and research now.
[00:09:27.000 --> 00:09:36.000]   And what I've learned over and over across different projects is that there are two kind of main ways of building research-driven products.
[00:09:36.000 --> 00:09:51.000]   And the first one, um, is when you have unfamiliar capability of the model, your job might be to create familiar form factor for that unfamiliar capability.
[00:09:51.000 --> 00:09:56.000]   And, in examples of that could be, um, you know, ChatGPT was like this.
[00:09:56.000 --> 00:09:59.000]   Or, a hundred contacts from Quad was like this.
[00:09:59.000 --> 00:10:02.000]   Um, I'll share, let's dive deeper into this.
[00:10:02.000 --> 00:10:13.000]   Actually, the early prototypes that I've done before joining AnyLab was actually working with Clip, um, which is a contrastive model.
[00:10:13.000 --> 00:10:18.000]   And, um, it's, like, image, um, text-to-image, basically.
[00:10:18.000 --> 00:10:23.000]   And, um, I fine-tuned Clip a little bit on the images that I was excited about.
[00:10:23.000 --> 00:10:28.000]   And I think this is the prototype of, you know, fashion search.
[00:10:28.000 --> 00:10:34.000]   Um, I think some people, you know, people kind of, like, it went viral on Twitter.
[00:10:34.000 --> 00:10:45.000]   And I feel like that's because people really found some usefulness if you bring some, like, Clip technology into some form factor that people will like.
[00:10:45.000 --> 00:10:48.000]   And so, um, I think a lot of the product work is around like this.
[00:10:48.000 --> 00:10:55.000]   It's like, if the model has a certain capability that was never possible before, how do we create this new, um, form factor?
[00:10:55.000 --> 00:10:58.000]   The same happened with a hundred key context.
[00:10:58.000 --> 00:11:08.000]   When Claude, um, started being able to, you know, consume the entire books, um, you can imagine to have various form factors.
[00:11:08.000 --> 00:11:19.000]   File uploads is just, like, very general and something really familiar to people that can just, like, dump the entire document into Claude and ask follow-up questions.
[00:11:19.000 --> 00:11:23.000]   But you can imagine other form factors for this, like infinite chats.
[00:11:23.000 --> 00:11:32.000]   And in a way it's, like, infinite memory, um, as a way to kind of, like, take this a hundred key context into some form factor.
[00:11:32.000 --> 00:11:41.000]   So you can, like, exercise this product thinking by, um, thinking of, like, novel ways that people can interact, um, with this novel technology.
[00:11:41.000 --> 00:11:57.000]   Um, another example, this is more speculative, it wasn't deployed anywhere, um, is if the model has kind of a sense of self-calibration, uh, uh, or, um, it's also called P, I know.
[00:11:57.000 --> 00:12:01.000]   If the model knows the confidence of the answer that it knows.
[00:12:01.000 --> 00:12:12.000]   So, for example, if I'm really, really confident in this claim and then my confidence may be, uh, 85%, then maybe there is a way for the interface to highlight that.
[00:12:12.000 --> 00:12:28.000]   Um, maybe the, um, more highlighted versions is much more confident, um, kind of, much more confident, uh, claims and less highlighted as, um, kind of less, less, uh, uh, confidence.
[00:12:28.000 --> 00:12:39.000]   So, in a way you can imagine if the model, yeah, you can, like, think of, okay, if, if you train the model to be really good at, like, self-calibration, how do we then represent that to the humans?
[00:12:39.000 --> 00:12:44.000]   And if, if anything, will it be useful, um, for humans?
[00:12:44.000 --> 00:12:53.000]   Same happened with, uh, when we were trying to deploy a one preview, um, the chain of thought itself is a very alien thing.
[00:12:53.000 --> 00:13:00.000]   And, um, a lot of people were, okay, how do you bring along humans with models thinking, right?
[00:13:00.000 --> 00:13:09.000]   If the model, if, if the human needs to wait for chain of thought for, like, two minutes or five minutes, that's kind of boring.
[00:13:09.000 --> 00:13:16.000]   And, um, people don't, I think it's just like more of a, how would people perceive the model thoughts?
[00:13:16.000 --> 00:13:22.000]   Um, and one thing that we did, uh, is actually we wanted to create the streaming, uh, kind of interaction.
[00:13:22.000 --> 00:13:29.000]   Um, so the model would always stream its thoughts ephemeral, um, and we trained the model to do that.
[00:13:29.000 --> 00:13:43.000]   Um, so, in a way there was chain of thought as an alien technology, as an alien artifact of the model, and then trying to figure out, like, how do we best bring to the humans, um, is another way of thinking of, uh, building product.
[00:13:43.000 --> 00:14:00.000]   Okay, so another way, the second way of building research, um, during products is actually start with a deep belief in what you want to make, um, with either from a product perspective or, um, kind of like vision.
[00:14:00.000 --> 00:14:03.000]   Um, and literally make the model do that.
[00:14:03.000 --> 00:14:06.000]   Okay, so I feel like this is more of a common thing.
[00:14:06.000 --> 00:14:09.000]   Um, so we can go through some examples.
[00:14:09.000 --> 00:14:20.000]   Um, before Antarctic, I worked at the New York Times, and, uh, in a lot of ways, we were thinking about how to represent information, uh, to people.
[00:14:20.000 --> 00:14:25.000]   And, like, how do we add the layer of context to the product and coverage?
[00:14:25.000 --> 00:14:27.000]   And at that time, we were all working on, like, elections.
[00:14:27.000 --> 00:14:30.000]   And, uh, we only had tools like NLP.
[00:14:30.000 --> 00:14:45.000]   But you can imagine this idea, this concept could have extended, given the current tools of AI, to have much more dynamic, um, representation or dynamic UIs that people could, um, consume the content better.
[00:14:45.000 --> 00:15:03.000]   Same when I was working on this, um, product, uh, the command line, the terminal, the new terminal, uh, I think it would be much, much richer if you could have integrated auto-completion or some of the benefits of GPT-3 at that time, um, into the product itself.
[00:15:03.000 --> 00:15:14.000]   Um, and so if you wanted to have, it was a vision of, like, having much more humane command line for people to, for junior engineers to actually, um, be more well-equipped.
[00:15:14.000 --> 00:15:27.000]   Um, yeah, and then, like, early prototypes around, uh, with GPT-3, even though it was just next token prediction, um, how do I make a writing IDE, uh, with GPT-3?
[00:15:27.000 --> 00:15:34.000]   And at that time, it was just, you know, trying to, um, whenever I type, it would auto-complete my thoughts, almost.
[00:15:34.000 --> 00:15:39.000]   So those ideas were kind of present early days with the technology that was coming out.
[00:15:39.000 --> 00:15:52.000]   Um, and yeah, and I feel like when I was at Anthropica, I kind of realized that, and actually, if you want to create, like, new interaction paradigm of interfaces, you actually need to train the model, um, for that.
[00:15:52.000 --> 00:16:05.000]   Um, another example of what we did with Claude, which I don't think a lot of people know, um, is when Claude generates titles, it actually has some micropersonalization.
[00:16:05.000 --> 00:16:12.000]   So it takes the style, uh, the writing style of the user, and then generates a title in the same style of the user.
[00:16:12.000 --> 00:16:19.000]   So you can imagine certain, like, interesting micropersonalization that you could create, um, within the products themselves.
[00:16:19.000 --> 00:16:25.000]   Um, and, um, another project was Claude and Slack.
[00:16:25.000 --> 00:16:29.000]   And that was the vision of Claude becoming a first virtual teammate.
[00:16:29.000 --> 00:16:31.000]   It was back in 2022.
[00:16:31.000 --> 00:16:37.000]   Um, and, you know, Slack was a very, very natural workspace where people collaborate.
[00:16:37.000 --> 00:16:46.000]   And, um, you can, you know, imagine a Claude model to be able to jump into the threads and suggest new things.
[00:16:46.000 --> 00:16:57.000]   Or sometimes, um, you know, Claude was really, really good at, uh, summarizing the, what's, what went, what's going on in, within this channel at the high volume content.
[00:16:57.000 --> 00:17:07.000]   Um, and so this was the first kind of vision and the first attempt of making Claude being a virtual super assistant, being, and being able to, like, use different tools.
[00:17:07.000 --> 00:17:14.000]   And, um, my first, one of the first projects at OpenAI was around Canvas.
[00:17:14.000 --> 00:17:22.000]   Um, so that was, you know, that was in the same spirit of breaking out from the chat interface to something different.
[00:17:22.000 --> 00:17:32.000]   And we wanted to create, um, much more human AI collaborative and flexible affordance that will scale, uh, with new modalities.
[00:17:32.000 --> 00:17:37.000]   So, Canvas is not just, like, a thing that you can write into this.
[00:17:37.000 --> 00:17:40.000]   It's also a thing that the model can write into this.
[00:17:40.000 --> 00:17:42.000]   And the model can render a code.
[00:17:42.000 --> 00:17:44.000]   And another model can check.
[00:17:44.000 --> 00:17:52.000]   And you can, like, create, like, the interfaces that will scale with new model capabilities and other tools that will happen.
[00:17:52.000 --> 00:18:00.000]   So, one thing, one interesting thing, um, about Canvas is that we actually postchained the model purely on synthetic data.
[00:18:00.000 --> 00:18:10.000]   And, um, you know, we live in the age where, um, a lot of most powerful reasoning models, um, can be distilled via APIs.
[00:18:10.000 --> 00:18:20.000]   And this, like, distillation is a very powerful idea, uh, of having a student and teacher and have a teacher, um, to teach things to a smaller model.
[00:18:20.000 --> 00:18:25.000]   So, we've trained, um, you know, this model to become more of a collaborator.
[00:18:25.000 --> 00:18:28.000]   But what does it mean for a model to become a collaborator?
[00:18:28.000 --> 00:18:30.000]   And how do we actually make evals for this?
[00:18:30.000 --> 00:18:31.000]   Right?
[00:18:31.000 --> 00:18:47.000]   Um, so, one thing that we wanted to kind of decompose is that teaching the model to use a tool is kind of different behavior from, uh, having a model to be proactive or act as a collaborator.
[00:18:47.000 --> 00:18:52.000]   Um, so, teaching a tool in itself is also have a nuanced kind of behavior.
[00:18:52.000 --> 00:19:03.000]   So, one thing that we, um, had to calibrate the model on is when to entirely rewrite the document versus when to, uh, if you look at the canvas,
[00:19:03.000 --> 00:19:12.000]   sometimes the model can just specifically select sections and delete those and rewrite them, uh, in a much more fine-grained way instead of, like, rewriting everything.
[00:19:12.000 --> 00:19:24.000]   Um, when, um, you know, to create, uh, code in canvas versus, um, ask, like, a python tool call.
[00:19:24.000 --> 00:19:28.000]   So, it's like different tools, compositionality, um, is happening.
[00:19:28.000 --> 00:19:33.000]   So, it's a lot of work around, um, teaching the behaviors for the models for this.
[00:19:33.000 --> 00:19:39.000]   Um, and you have, you can employ different, um, techniques for this.
[00:19:39.000 --> 00:19:43.000]   And I can, I can share a little bit more, uh, how we did this cloud.
[00:19:43.000 --> 00:19:48.000]   Um, so, another project that we did was, uh, it's called tasks.
[00:19:48.000 --> 00:19:54.000]   And, you know, everybody is familiar with the idea of having reminders or to-do lists.
[00:19:54.000 --> 00:19:59.000]   And, um, the model can now maybe schedule tasks for you.
[00:19:59.000 --> 00:20:04.000]   Uh, but the most important thing about it is, is the tasks itself is very, very diverse.
[00:20:04.000 --> 00:20:07.000]   And it's not just, you know, just a reminder of your to-do list.
[00:20:07.000 --> 00:20:11.000]   It can create stories for you every day.
[00:20:11.000 --> 00:20:14.000]   Or, it can continue as a story, uh, from the previous day.
[00:20:14.000 --> 00:20:20.000]   So, you can imagine this, like, the modularity of two compositions, but it's very powerful in product.
[00:20:20.000 --> 00:20:22.000]   Um, okay.
[00:20:22.000 --> 00:20:31.000]   So, let's go to the case study of the model behavior in more specifically.
[00:20:31.000 --> 00:20:37.000]   Um, so, I really want to, like, dive deep into the idea of the model behavior.
[00:20:37.000 --> 00:20:43.000]   You know, how do we shape the models and why, how do you post-train the models on the behaviors that we want?
[00:20:43.000 --> 00:20:51.000]   Um, and, you know, I think to be more specific and be grounded in the real world, uh, use case.
[00:20:51.000 --> 00:21:01.000]   I'll share more, uh, how we might want to think about shaping the, um, kind of behavior of the model, uh, around refusals.
[00:21:01.000 --> 00:21:14.000]   So, you know, let's, let's have, we, we, it's also like the second way of making the product where we have this, um, kind of vision of how the model should behave.
[00:21:14.000 --> 00:21:27.000]   And that vision is also grounded by the, you know, cross-functional collaboration between different teams on how we want the model to respond to various users.
[00:21:27.000 --> 00:21:36.000]   Um, so, one particular thing that you can imagine is, like, the model should have more opinions, but with caveats.
[00:21:36.000 --> 00:21:38.000]   It's just you have opinions, but with caveats.
[00:21:38.000 --> 00:21:40.000]   So, what does it mean, actually?
[00:21:40.000 --> 00:21:46.000]   So, the model maybe needs to be more decisive, um, when asked direct questions.
[00:21:46.000 --> 00:21:57.000]   And, um, let's say one thing that we've seen with RLHF is that the model is very sycophantic back, like, early days in, like, 2022, 2023.
[00:21:57.000 --> 00:22:00.000]   If the model would just agree with everything you would say.
[00:22:00.000 --> 00:22:07.000]   And so, how do we actually teach the model to be more not to do that and be more nuanced?
[00:22:07.000 --> 00:22:14.000]   Um, another thing that was annoying, uh, is, is that, like, the model says, I don't actually have a point of view.
[00:22:14.000 --> 00:22:18.000]   Um, and, uh, is just, just willing to chat with people.
[00:22:18.000 --> 00:22:22.000]   But actually, maybe the model should have some views on certain things.
[00:22:22.000 --> 00:22:25.000]   Uh, um, yeah.
[00:22:25.000 --> 00:22:32.000]   And then sometimes the model could indicate when, when things are just opinions, um, and notes its own biases and inconsistencies.
[00:22:32.000 --> 00:22:38.000]   And it should acknowledge, kind of have, like, self-knowledge of what it knows and what it thinks is right or not right.
[00:22:38.000 --> 00:22:51.000]   And I think the model, like, back in 2022, cloud 1.3, uh, didn't really have, like, any thoughtful responses for things like philosophy, uh, like, ethical questions.
[00:22:51.000 --> 00:22:57.000]   So, uh, you can imagine to post-chain the model, um, on the behavior that would be much more nuanced than the philosophical questions.
[00:22:57.000 --> 00:22:59.000]   Um, yeah.
[00:22:59.000 --> 00:23:02.000]   And, like, other behaviors that you might want to encode in your model.
[00:23:02.000 --> 00:23:09.000]   So, you kind of, like, list out all the various things that you would like a model to behave.
[00:23:09.000 --> 00:23:15.000]   So, maybe, like, the model can have better knowledge of who it is and what it's capable of.
[00:23:15.000 --> 00:23:20.000]   Um, because we've seen in the product a lot of people just, like, ask the model about its features.
[00:23:20.000 --> 00:23:22.000]   And oftentimes it doesn't know.
[00:23:22.000 --> 00:23:25.000]   Um, yeah.
[00:23:25.000 --> 00:23:31.000]   And, um, let's dive deep into the, uh, more specific example.
[00:23:31.000 --> 00:23:35.000]   So, cloud 2.1, I don't know if anybody remembers.
[00:23:35.000 --> 00:23:41.000]   Um, but when it was launched, I think it was, um, kind of had the issue of, like, over-refusals.
[00:23:41.000 --> 00:23:42.000]   Um, yeah.
[00:23:42.000 --> 00:23:48.000]   Like, 2.1 would refuse tasks that superficially sounded harmful, but actually were not.
[00:23:48.000 --> 00:23:52.000]   And, um, you know, it wasn't just caused by a single source of data.
[00:23:52.000 --> 00:23:54.000]   Uh, so we had to, like, investigate.
[00:23:54.000 --> 00:23:58.000]   Um, and, you know, we knew that it was fixable.
[00:23:58.000 --> 00:24:04.000]   Because for some reason, something was 2.1 led to some refusals on benign prompts, then 2.0.
[00:24:04.000 --> 00:24:10.000]   So, you kind of have, like, a really good baseline for experimentation and debugging.
[00:24:10.000 --> 00:24:17.000]   And actually, the way you debug the model behavior is actually very similar to how you would want to debug software.
[00:24:17.000 --> 00:24:27.000]   Um, and, um, and the way we would approach this is, um, okay, how do we actually craft this Monion's refusals?
[00:24:27.000 --> 00:24:36.000]   Um, so the first principles that they had is, like, maybe the model should assume charitable interpretation of what the person is asking without being harmful.
[00:24:36.000 --> 00:24:48.000]   Um, so instead of, you know, crafted dialogue between two characters who are planning a complex heist, the model would refuse because it's not comfortable with that.
[00:24:48.000 --> 00:24:51.000]   But the model should have much more charitable interpretation.
[00:24:51.000 --> 00:24:55.000]   And, you know, this is a creative writing prompt, so probably it should respond.
[00:24:55.000 --> 00:25:04.000]   Um, and other principles that we've kind of, like, thought about is, like, how do we actually use non-verbal communication, non-violent communication principles?
[00:25:04.000 --> 00:25:20.000]   Um, you know, maybe the model should, um, refuse, um, more, like, I statements instead of, and take responsibility for its own refusal instead of saying U statements or any judgment to the user.
[00:25:20.000 --> 00:25:28.000]   Um, ask if the user would be willing to make some changes, so Claude can be more comfortable with its boundaries.
[00:25:28.000 --> 00:25:32.000]   That's another kind of, like, very nuanced behavior we wanted to teach the model.
[00:25:32.000 --> 00:25:38.000]   So, um, you know, then the model needs to know what its boundaries are.
[00:25:38.000 --> 00:25:42.000]   Um, and this is, like, much more of a matter, kind of, post-training learning.
[00:25:42.000 --> 00:25:45.000]   Um, yeah, and also, like, acknowledges impact.
[00:25:45.000 --> 00:25:55.000]   Like, I know this may be annoying to you, and this is, like, much more empathetic answer than saying, you know, um, I don't want to respond to this.
[00:25:55.000 --> 00:26:01.000]   Um, and then, you know, I think we've kind of came up with different, like, refusal taxonomy.
[00:26:01.000 --> 00:26:06.000]   There are, like, benign, uh, over-refusals on, like, homeless prompts.
[00:26:06.000 --> 00:26:09.000]   Um, there were creative writing of refusals.
[00:26:09.000 --> 00:26:16.000]   Um, and, uh, there are actually some of the interesting refusals were around, um, tool calls or function calls.
[00:26:16.000 --> 00:26:24.000]   Um, it might have access, uh, had a tool to view a note, but actually would say, I can't see your notes.
[00:26:24.000 --> 00:26:26.000]   Um, and why is this happening?
[00:26:26.000 --> 00:26:27.000]   Um, yeah.
[00:26:27.000 --> 00:26:36.000]   So, other, yeah, other, uh, taxonomies of refusals at that time was, like, you know, long document attachments.
[00:26:36.000 --> 00:26:41.000]   If I upload the document, it would just say, like, I don't have a capability to read this document.
[00:26:41.000 --> 00:26:42.000]   Like, why?
[00:26:42.000 --> 00:26:47.000]   Um, so something in the data might have been causing this.
[00:26:47.000 --> 00:26:58.000]   Um, and misdirected refusals, um, when they, um, you know, it kind of, like, took interpretation, um, of the user.
[00:26:58.000 --> 00:27:06.000]   And it should have had, like, much more charitable, charitable view of the user during, um, yeah.
[00:27:06.000 --> 00:27:11.000]   So, you kind of construct this, like, categories, um, of various refusals.
[00:27:11.000 --> 00:27:17.000]   Because, you know, if it's in every behavior or, like, a capability that you want to, like, post in the model,
[00:27:17.000 --> 00:27:21.000]   you have, like, various use cases and various edge cases.
[00:27:21.000 --> 00:27:25.000]   And you kind of want to be approached for some more nuance.
[00:27:25.000 --> 00:27:33.000]   Um, and, yeah, like, so the first thing in every research project is, like, how do you actually, what evals do we build to trust?
[00:27:33.000 --> 00:27:35.000]   And, like, what evals do we build to trust?
[00:27:35.000 --> 00:27:47.000]   Um, and there are, um, for subjective things like this, or for class, obviously, evals for certain tasks, class of tasks, like math, would be very, very different from what is here.
[00:27:47.000 --> 00:27:52.000]   Um, so, in terms of refusals, um, how did we construct the evals?
[00:27:52.000 --> 00:27:54.000]   Well, first, it's, like, product of Flywheel, right?
[00:27:54.000 --> 00:27:59.000]   Like, we have all the users and manually collected prompts that wouldn't use refusals.
[00:27:59.000 --> 00:28:06.000]   Uh, we can also, like, send data to generate diverse prompts on the borderline between harmfulness and helpfulness.
[00:28:06.000 --> 00:28:12.000]   And those are prompts are around, like, creative writing, um, like, edgy creative writing, as we call it.
[00:28:12.000 --> 00:28:17.000]   Um, and you can also use other evals.
[00:28:17.000 --> 00:28:19.000]   You kind of want to construct a suite of evals.
[00:28:19.000 --> 00:28:23.000]   Um, like, X-test was, like, 200 non-malicious prompts.
[00:28:23.000 --> 00:28:30.000]   Um, wild chat data set, um, a collection of diverse user chatbot interactions with ambiguous requests.
[00:28:30.000 --> 00:28:32.000]   Topic switching political discussions.
[00:28:32.000 --> 00:28:35.000]   So, you can also, um, use some of the open source benchmarks.
[00:28:35.000 --> 00:28:37.000]   Um, yeah.
[00:28:37.000 --> 00:28:45.000]   And I think, like, general approaches, not particularly what has happened with cloud, but, like, more of a general approach to the model behavior
[00:28:45.000 --> 00:28:49.000]   frustrating is that, you know, you kind of want to, like, look at the data, clean up the data.
[00:28:49.000 --> 00:29:00.000]   Um, you might want to consider either, like, collect targeted human feedback collection for supervised fine-tuning or preference modeling, reward modeling.
[00:29:00.000 --> 00:29:04.000]   Or you might not want to do that because human feedback is very costly.
[00:29:04.000 --> 00:29:11.000]   And now, especially with reasoning models, um, right, you might want to just not have any human feedback.
[00:29:11.000 --> 00:29:19.000]   And control was, like, synthetically generating some of the behavioral changes, uh, like a preference data to trained reward models.
[00:29:19.000 --> 00:29:21.000]   Um, and do RL.
[00:29:21.000 --> 00:29:31.000]   So, I think, uh, at that time, you know, I think this is, like, more employing, uh, constitutional AI, uh, principles of those, uh, anti-refusal behaviors.
[00:29:31.000 --> 00:29:39.000]   And create a preference data, where you only change one particular feature within your pairs, let's say, of preferences.
[00:29:39.000 --> 00:29:49.000]   You only want to control, um, one change that, in that pair, uh, to have much more control over the reward model data.
[00:29:49.000 --> 00:30:03.000]   Because if you can, like, the dumbest, simplest thing that you can do is, like, take, um, a preference, like, take a response from model A and take a response from model B and prefer B over A.
[00:30:03.000 --> 00:30:12.000]   But, um, it doesn't necessarily reduce, um, or fix the spurious features that the model will learn that you don't actually want to.
[00:30:12.000 --> 00:30:16.000]   Um, so you kind of want to craft this distribution.
[00:30:16.000 --> 00:30:21.000]   Um, yeah, so this is mostly around, like, crafting the distribution of the data that you want.
[00:30:21.000 --> 00:30:26.000]   Um, and actually, yeah, look at the data, like, you would debug software.
[00:30:26.000 --> 00:30:31.000]   Like, each refusal might be caused by different data sets, right?
[00:30:31.000 --> 00:30:40.000]   If it's, like, a tool called refusal, maybe it came from some kind of self-knowledge data that it would teach that the model doesn't have a physical body.
[00:30:40.000 --> 00:30:48.000]   And, uh, the model might just refuse to set an alarm, um, because it doesn't have a physical body.
[00:30:48.000 --> 00:30:52.000]   But actually, it does have a tool to set an alarm, right?
[00:30:52.000 --> 00:30:57.000]   So, it has some of this contradictory data that might affect weird model behaviors.
[00:30:57.000 --> 00:31:01.000]   Um, yeah, also, like, long dog refusal like this.
[00:31:01.000 --> 00:31:04.000]   Um, it was creative writing few results.
[00:31:04.000 --> 00:31:14.000]   It's, like, more of a, this balanced, um, challenging act between, like, safety and harmfulness, uh, data and helpfulness data.
[00:31:14.000 --> 00:31:20.000]   Um, in cloud three model, Carvey actually, uh, wrote everything about this.
[00:31:20.000 --> 00:31:27.000]   Um, you know, models that are trained to be more helpful and responsive to user requests may also lean towards harmful behaviors.
[00:31:27.000 --> 00:31:30.000]   Like, sharing information that violates the policy.
[00:31:30.000 --> 00:31:37.000]   And, conversely, when models just over-indexed on homelessness can tend towards not sharing any new information with users.
[00:31:37.000 --> 00:31:41.000]   Which, in itself, makes the model very unusable.
[00:31:41.000 --> 00:31:46.000]   Um, so, navigating this balancing act is very challenging, um, and works.
[00:31:46.000 --> 00:31:54.000]   So, yeah, and this is the plot of, uh, what we did and the results of cloud 2.1 and cloud 3 at that time.
[00:31:54.000 --> 00:31:59.000]   Um, and, yeah, like, you, kind of, want to, like, web check, uh, the responses.
[00:31:59.000 --> 00:32:06.000]   So, uh, when we asked the draft, like, fictional science sci-fi novel about surveillance system.
[00:32:06.000 --> 00:32:09.000]   Actually, 3 would respond in the match manual.
[00:32:09.000 --> 00:32:12.000]   It will respond instead of, like, um, refusing.
[00:32:12.000 --> 00:32:13.000]   Same here.
[00:32:13.000 --> 00:32:21.000]   Uh, yeah, mostly for creative writing, um, um, tasks.
[00:32:21.000 --> 00:32:23.000]   Cool.
[00:32:23.000 --> 00:32:27.000]   Um, my third section, um, of the talk.
[00:32:27.000 --> 00:32:28.000]   So, before we go.
[00:32:28.000 --> 00:32:34.000]   Before I jump into this, um, I would love to invite you if anybody has any questions.
[00:32:34.000 --> 00:32:38.000]   Or, um, I don't want to, like, mumble all along.
[00:32:38.000 --> 00:32:41.000]   Um, anything I can just move on.
[00:32:41.000 --> 00:32:42.000]   Yeah.
[00:32:42.000 --> 00:32:47.000]   Um, I'm curious, like, what's your process that you guys follow in order to, like,
[00:32:47.000 --> 00:32:53.000]   see how some new thing you're trying to push out, like, how do you go from wanting that particular
[00:32:53.000 --> 00:32:57.000]   teacher or behavior to actually, like, inducing that into models?
[00:32:57.000 --> 00:32:59.000]   Like, like, I don't know.
[00:32:59.000 --> 00:33:00.000]   Yeah, I don't know.
[00:33:00.000 --> 00:33:06.000]   Um, you know, it's, um, it's, um, it's supposed to do all of the risk.
[00:33:06.000 --> 00:33:11.000]   So, um, you might want to, um, you know, if this is your project, then you might want to
[00:33:11.000 --> 00:33:16.000]   think of what kind of data you want to collect and how you would want to collect the data.
[00:33:16.000 --> 00:33:27.000]   And then, uh, you can take, like, uh, the base config or, um, train a model, um, the same way as,
[00:33:27.000 --> 00:33:30.000]   you know, let's say you want to make a change in, like, 4.0.
[00:33:30.000 --> 00:33:37.000]   You might want to take the 4.0 model and add your data change and then retrain the model again
[00:33:37.000 --> 00:33:40.000]   and then see the effect on the evals that you will build.
[00:33:40.000 --> 00:33:48.000]   Um, there are other much more cheaper approaches, um, like, incremental, like, training on top of,
[00:33:48.000 --> 00:33:51.000]   um, the 4.0, let's say.
[00:33:51.000 --> 00:33:52.000]   Um, yeah.
[00:33:52.000 --> 00:33:53.000]   So, like, using either SFT.
[00:33:53.000 --> 00:33:58.000]   So, like, some of the choices that you will have to make is either you want to change in,
[00:33:58.000 --> 00:34:05.000]   um, like, supervised fine-tuning, uh, stage or you might want to have retrained reward model or,
[00:34:05.000 --> 00:34:11.000]   um, create a new kind of, like, evaluator grader, um, for that particular task.
[00:34:11.000 --> 00:34:20.000]   And then you can, like, create prompts in a raw environment and then, um, exercise those, um, exercise that scale for the model
[00:34:20.000 --> 00:34:23.000]   and then see if the model learns over the course of training.
[00:34:23.000 --> 00:34:32.000]   And if it doesn't, um, you know, you then look at the bunch of, like, plots and, you know, your plot might go up,
[00:34:32.000 --> 00:34:37.000]   but maybe some other plots might go down, so you kind of want to, like, calibrate, um, and fix those.
[00:34:37.000 --> 00:34:44.000]   Or, yeah, it's a very complex, um, as more and more tools and as more things we teach the model,
[00:34:44.000 --> 00:34:48.000]   it becomes much more, uh, kind of uncontrollable, almost.
[00:34:48.000 --> 00:34:51.000]   Um, yeah.
[00:34:51.000 --> 00:34:52.000]   Cool.
[00:34:52.000 --> 00:35:02.000]   Um, the third section is more about, um, the point that how you construct a raw environment and rewards is how your product will work.
[00:35:02.000 --> 00:35:11.000]   Um, you know, I think real-world use cases is what creates the complexity of a raw environment.
[00:35:11.000 --> 00:35:17.000]   And the complexity comes from, you know, teaching the model to complete hard tasks.
[00:35:17.000 --> 00:35:23.000]   And oftentimes hard tasks are, require much more than just answering the question.
[00:35:23.000 --> 00:35:33.000]   They require tools like search, um, code tools, uh, computer use tools, uh, reasoning over a long context.
[00:35:33.000 --> 00:35:37.000]   Um, and kind of like reward design that you want to shape.
[00:35:37.000 --> 00:35:42.000]   Um, and I think maybe it's obvious, maybe it's not obvious.
[00:35:42.000 --> 00:35:47.000]   But let's say, you know, as we teach the models, for the model to become very useful,
[00:35:47.000 --> 00:35:50.000]   we actually need to teach the models on useful things.
[00:35:50.000 --> 00:35:57.000]   And, um, you can think of like, uh, let's say we want to teach the model to be, uh, like a soft engineer.
[00:35:57.000 --> 00:35:59.000]   Then, okay, what does it actually mean?
[00:35:59.000 --> 00:36:02.000]   It doesn't mean it like creates really good PRs.
[00:36:02.000 --> 00:36:05.000]   Then your task distribution will be on that.
[00:36:05.000 --> 00:36:07.000]   And how do you evaluate what is a good PR?
[00:36:07.000 --> 00:36:08.000]   What is not good PR?
[00:36:08.000 --> 00:36:14.000]   Um, is actually, isn't itself more of a, can be also like a product thinking work.
[00:36:14.000 --> 00:36:19.000]   Um, same, um, you can dive deeper into the creative storyteller, right?
[00:36:19.000 --> 00:36:27.000]   Like if you want to teach the model to be, uh, good at writing, actually, what does it mean for a human to be a good writer?
[00:36:27.000 --> 00:36:36.000]   Well, they actually need some kind of tool to draft and edit their thoughts and like have multiple days to do that.
[00:36:36.000 --> 00:36:38.000]   And maybe the model should be able to do that.
[00:36:38.000 --> 00:36:43.000]   Um, maybe the model should have a tool that where you can like edit and draft.
[00:36:43.000 --> 00:36:51.000]   And oftentimes for creatives, you know, people go observe the world for like a long, like sometimes they like connect the dot,
[00:36:51.000 --> 00:36:54.000]   like the dots are getting connected in the very random times.
[00:36:54.000 --> 00:37:00.000]   And maybe you want to expose the model to never ending, uh, kind of search engine.
[00:37:00.000 --> 00:37:05.000]   Like the model should be able to always have access to the latest state of the world.
[00:37:05.000 --> 00:37:14.000]   Uh, and then, um, maybe over the course of let's say a week of being exposed to latest thing that is happening in the world,
[00:37:14.000 --> 00:37:17.000]   the model can start reflecting on the world and like write something.
[00:37:17.000 --> 00:37:24.000]   Um, and maybe that's like much more natural process of writing than just like ask or prompt it, like write about X, Y, Z.
[00:37:24.000 --> 00:37:32.000]   Um, and we are shifting towards much more, uh, more complex, uh, kind of interactions within their all environments.
[00:37:32.000 --> 00:37:34.000]   So the multiplayer interactions, right?
[00:37:34.000 --> 00:37:44.000]   It's not just one user communicates with one model, but it might be the case that multiple users, multiplayer collaboration that can happen with a model.
[00:37:44.000 --> 00:37:55.000]   So if I'm a, um, product designer and your product manager, we collaborating on something and we want to collaborate with an agent to, um, make a new product.
[00:37:55.000 --> 00:38:05.000]   And this in itself is a task that you can learn in RL, but each user has different preferences and each user is, has different, um, things.
[00:38:05.000 --> 00:38:08.000]   So yeah, how do you construct this environment is actually important.
[00:38:08.000 --> 00:38:19.000]   And, uh, you know, multi-agentic environments is, um, more of, uh, the model debate with each other or deliberate on like certain topic to reach a conclusion.
[00:38:19.000 --> 00:38:32.000]   So you can construct, um, I, I think like multi-agentic is like more, more like AlphaGo, like, uh, kind of environments, um, where they, they like get reward by achieving something, um, together, maybe.
[00:38:32.000 --> 00:38:48.000]   Um, so, um, I think in AI labs, I feel like we are also shifting, um, possibly the focus from, because we, we kind of like optimized so much around the, uh, class of time.
[00:38:48.000 --> 00:38:54.000]   And we're shifting the focus on the, uh, class of tasks that is like really easy to measure, like within like math, uh, competitive programming.
[00:38:54.000 --> 00:38:54.000]   Right.
[00:38:54.000 --> 00:39:07.000]   And we're shifting the focus maybe towards more subjective class of tasks that is really, really hard to measure, but that are becoming much more important if AI models get socially integrated in our lives.
[00:39:07.000 --> 00:39:12.000]   Um, and more specifically, let's say emotional intelligence, right?
[00:39:12.000 --> 00:39:13.000]   Right.
[00:39:13.000 --> 00:39:24.000]   The humans who use chat GPT, uh, use it so much for things like coaching therapy, uh, emotional, but we don't actually have much open source evals on this.
[00:39:24.000 --> 00:39:28.000]   And how do we actually measure that is actually becomes much more interesting question.
[00:39:28.000 --> 00:39:33.000]   Um, like social intelligence in voice, voice mode, right?
[00:39:33.000 --> 00:39:39.000]   I think it's one thing for a model to be intelligent in reason, like math and it's reasoning.
[00:39:39.000 --> 00:39:47.000]   Another thing, but another axis of intelligence is when I talk to a model in voice, uh, it can actually suggest something really meaningful.
[00:39:47.000 --> 00:39:54.000]   It might just say like, Hey, I noticed you did X, Y, Z, maybe I should create, um, a new tool for you.
[00:39:54.000 --> 00:39:59.000]   So I think it's a different kind of like, uh, social, like intelligence in that way.
[00:39:59.000 --> 00:40:06.000]   Um, another cause of tasks that I'm interested in is, uh, writing, obviously I think, um, models, creativity.
[00:40:06.000 --> 00:40:12.000]   Um, yeah, writing is like really hard to measure because it's so personal and subjective.
[00:40:12.000 --> 00:40:18.000]   Um, but it's interesting to think about, uh, can we make those tasks a bit more objective?
[00:40:18.000 --> 00:40:23.000]   Um, you know, everybody loves or like some kind of like sci-fi novel.
[00:40:23.000 --> 00:40:24.000]   Okay.
[00:40:24.000 --> 00:40:25.000]   What makes it really, really good?
[00:40:25.000 --> 00:40:34.000]   Um, so maybe there are certain like rules, technological rules or consistency of the world, um, that people really like or the character development.
[00:40:34.000 --> 00:40:39.000]   So you can like decompose, um, those subjective tasks into much more objective.
[00:40:39.000 --> 00:40:42.000]   Same with like visual design and aesthetics.
[00:40:42.000 --> 00:40:52.000]   Um, and for the model to generate something really aesthetically interesting, um, it should know like the basic principles of good visual design.
[00:40:52.000 --> 00:40:55.000]   So those are like much more objective.
[00:40:55.000 --> 00:40:57.000]   Yeah.
[00:40:57.000 --> 00:41:05.000]   And I think like, this is more of a kind of new, new kind of product research that a lot of people are starting doing is creating new RL tasks.
[00:41:05.000 --> 00:41:11.000]   And this is more of a simulation of real world scenarios, um, leveraging in context learning.
[00:41:11.000 --> 00:41:20.000]   If you want to teach like a new tool or something, um, leveraging synthetic data, wide distillation from stronger reasoning models.
[00:41:20.000 --> 00:41:31.000]   Um, yeah, you can think of like inventing new model behavior and interactions like multiplayer and also incorporating product and user feedback, uh, during the entire process.
[00:41:31.000 --> 00:41:37.000]   Well, another axis of this work is actually around reward design.
[00:41:37.000 --> 00:41:39.000]   Um, how do we teach the model?
[00:41:39.000 --> 00:41:51.000]   Um, like what kind of feedback would I want to give to a model so that it will learn how to better operate in this real world scenario use case and be more adapt in social contexts.
[00:41:51.000 --> 00:41:55.000]   And actually this is, this requires a real quite deep product thinking, right?
[00:41:55.000 --> 00:42:05.000]   So we want to teach the model, um, to have like follow up, like meaningful follow up questions, but not like overly being annoying or something.
[00:42:05.000 --> 00:42:15.000]   Um, so how do you reward for completing a tasks in a way that makes, that will make a lot of sense in the product and will shape, um, the product experience with the user in the future.
[00:42:15.000 --> 00:42:23.000]   Um, and obviously during that process you will get into, uh, VR things like, do you have a question?
[00:42:23.000 --> 00:42:38.000]   I'm just curious, you guys thinking about like, kind of infer the rewards from the data from like a certain group of people.
[00:42:38.000 --> 00:42:49.000]   I guess they will have some like internal reward, you know, like the mechanism to make decisions and then you can use that to sort of, you know, to inject such a insight into a model.
[00:42:49.000 --> 00:42:54.000]   Yeah.
[00:42:54.000 --> 00:43:00.000]   Yeah, I think there are definitely various approaches to how you construct the reward, right?
[00:43:00.000 --> 00:43:04.000]   Like there was one, um, you can construct very simple rewards.
[00:43:04.000 --> 00:43:14.000]   You can also like train a reward model, um, that will, as yeah, you can like train like some kind of like inverse, um, reward modeling, um, tasks.
[00:43:14.000 --> 00:43:15.000]   Yeah.
[00:43:15.000 --> 00:43:18.000]   Um, it depends on the tasks too.
[00:43:18.000 --> 00:43:23.000]   Uh, it depends on the, like what kind of thing that you want to like optimize the model for.
[00:43:23.000 --> 00:43:38.000]   Um, yeah, and, um, I think an interesting, um, thing that you will discover during this entire process is, uh, reward hacks, which is very, very common in RL.
[00:43:38.000 --> 00:43:41.000]   And there are many, many different reasons why it's happening.
[00:43:41.000 --> 00:43:46.000]   I actually really highly recommend to read Lillian's blog about reward hacking in RL.
[00:43:46.000 --> 00:43:47.000]   It's very comprehensive.
[00:43:47.000 --> 00:43:57.000]   Um, there's, uh, reward hacking is basically when, um, the model achieves high reward for, uh, things that it actually didn't do.
[00:43:57.000 --> 00:43:58.000]   Like it's kind of deceived.
[00:43:58.000 --> 00:44:11.000]   Um, and especially right now as more and more systems, we use other AI models, LLMs to be evaluators for policy models.
[00:44:11.000 --> 00:44:23.000]   Actually an interesting, the most common like reward hack is when a policy model tries to deceive, uh, to think that it, deceive the evaluator model such that it will think that the policy completed the task or something.
[00:44:23.000 --> 00:44:28.000]   So, um, here's, um, an example of like the code patch tool.
[00:44:28.000 --> 00:44:33.000]   So the model is like, to skip all the tests, you can define this function that always skips.
[00:44:33.000 --> 00:44:34.000]   So, and then it passes.
[00:44:34.000 --> 00:44:38.000]   Um, and it's mostly interesting.
[00:44:38.000 --> 00:44:45.000]   There's a paper, recent paper from OpenAI around, um, just like monitoring, uh, reasoning models for misbehavior.
[00:44:45.000 --> 00:44:52.000]   And interesting finding is that if you, you actually don't want to optimize chain of thought, uh, on being,
[00:44:52.000 --> 00:44:58.000]   using, because then the model will be much more kind of hide it in its intent.
[00:44:58.000 --> 00:45:01.000]   Um, so yeah, it's like very interesting paper on like reward hacks.
[00:45:01.000 --> 00:45:08.000]   And especially with like more complex reasoning models, the complexity of reward hacks will also, um, change.
[00:45:08.000 --> 00:45:18.000]   Um, and especially, let's say in the software engineer, and you might not want to know what kind of code changed, uh, it made in order to create some certain vulnerability.
[00:45:18.000 --> 00:45:31.000]   Right? So you need actually, um, more, um, create like new affordances to have like much higher, like much more trustworthy verification of the model outputs.
[00:45:31.000 --> 00:45:34.000]   Um, and this is like more of an alignment problem too.
[00:45:34.000 --> 00:45:35.000]   Okay.
[00:45:35.000 --> 00:45:37.000]   Um, I'm almost done.
[00:45:37.000 --> 00:45:39.000]   I think this is the fourth section.
[00:45:39.000 --> 00:45:46.000]   Um, it's more like vignettes, uh, and the future of human AI interactions is how I think, um, about things.
[00:45:46.000 --> 00:46:02.000]   Um, obviously I made this graph like a year ago and nobody cares about MLU anymore, but I think this is to communicate the fact that the cost of reasoning is drastically decreasing and it will only just be decreasing.
[00:46:02.000 --> 00:46:17.000]   Um, and this idea of like raw intelligence in itself has just become so cheap that I think anybody can create like really useful and really amazing things with those models, um, at pretty much low cost.
[00:46:17.000 --> 00:46:33.000]   Um, you know, I, yeah, as I mentioned before, it's like, um, when we are entering the age where it's really hard to verify AI outputs because I'm not an expert in let's say medical, um, or financial analysis.
[00:46:33.000 --> 00:46:45.000]   Like how do we actually teach create like new affordances for humans to verify or edit models outputs, um, and help them to teach the models.
[00:46:45.000 --> 00:46:49.000]   Um, I do think there's a cool future of like dynamic generative UI.
[00:46:49.000 --> 00:46:53.000]   Um, it's kind of like invisible software creation on the fly.
[00:46:53.000 --> 00:47:01.000]   So let's say you talk to a model and, uh, you know, I say like, I want to learn more about the solar system.
[00:47:01.000 --> 00:47:10.000]   Um, instead of right now, the model will just like output text, but I would want to believe that in the future, uh, it would be much more personalized.
[00:47:10.000 --> 00:47:14.000]   Let's say I'm a visual thinker and you are more of a listener.
[00:47:14.000 --> 00:47:27.000]   Like if you're a listener, it will create maybe a podcast, but if I'm a visual, um, person, it might want to create me a picture or, you know, um, like a 3G, a 3GS visualization.
[00:47:27.000 --> 00:47:28.000]   Um, yeah.
[00:47:28.000 --> 00:47:38.000]   And this idea of like, uh, this interface is like ephemeral and it's like self morphs depends on understanding your intent and your context.
[00:47:38.000 --> 00:47:41.000]   Um, it's like deeply personal personalized models.
[00:47:41.000 --> 00:47:46.000]   Um, yeah, I'm also excited about personalized access to healthcare and education.
[00:47:46.000 --> 00:47:58.000]   Um, I think it's really, um, incredible, um, that anybody can check the symptoms and with chat or something and get like some advice.
[00:47:58.000 --> 00:48:05.000]   Um, and also like with that also comes like some of the interesting like consumer hardware, um, in the future.
[00:48:05.000 --> 00:48:11.000]   Um, and, uh, lastly is like our relationship to the process of storytelling will forever change.
[00:48:11.000 --> 00:48:22.000]   Like the way we tell the stories, the way we'll create new novels, maybe co-writing with like, with models, co-scripting new films.
[00:48:22.000 --> 00:48:31.000]   Um, I don't think like, I think there will be the new generation of creative people who, um, will change our relationship with storytelling.
[00:48:31.000 --> 00:48:42.000]   And I would hope to think that the current creators don't, are not scared of the eye and more open minded to use those tools for their process.
[00:48:42.000 --> 00:48:44.000]   Yeah.
[00:48:44.000 --> 00:48:45.000]   Thank you.
[00:48:53.000 --> 00:48:54.000]   Some questions.
[00:48:54.000 --> 00:48:55.000]   Yeah.
[00:48:55.000 --> 00:48:58.000]   So thanks Karina for the very interesting talk.
[00:48:58.000 --> 00:49:02.000]   Um, so yeah, now we'll open the floor to questions, a Q and A session.
[00:49:02.000 --> 00:49:05.000]   So, um, we'll structure it more openly.
[00:49:05.000 --> 00:49:14.000]   So anything you want to ask her about, um, research products, um, things that opening high, um, that she can talk about and so forth.
[00:49:14.000 --> 00:49:17.000]   And for the folks on zoom, feel free to ask questions in the chat.
[00:49:17.000 --> 00:49:21.000]   Um, and so we'll take a mix of a zoom and in-person questions.
[00:49:21.000 --> 00:49:22.000]   Um, thank you.
[00:49:22.000 --> 00:49:23.000]   Yeah.
[00:49:23.000 --> 00:49:24.000]   Um, is there any, like, category of law?
[00:49:24.000 --> 00:49:25.000]   Because, like, there aren't any shorts on it right now.
[00:49:25.000 --> 00:49:26.000]   Like, things that are really a lot.
[00:49:26.000 --> 00:49:27.000]   Yeah, there's, like, not enough people who wish they were not.
[00:49:27.000 --> 00:49:28.000]   Yeah, I mean, I think, yeah, creative writing.
[00:49:28.000 --> 00:49:29.000]   I don't think, like, anybody.
[00:49:29.000 --> 00:49:42.000]   Um, yeah, like, um, a lot of researchers are, uh, I think, I think, yeah, creative writing.
[00:49:42.000 --> 00:49:43.000]   I don't think, like, anybody.
[00:49:43.000 --> 00:49:44.000]   Um, yeah, like, um, a lot of researchers are, um, like, I think, I think, yeah, creative writing.
[00:49:44.000 --> 00:49:50.000]   I don't think, like, anybody, um, yeah, like, um, like, um, a lot of researchers are,
[00:49:50.000 --> 00:49:57.000]   really, like, um, working on problems where it's very easy to evaluate, I think.
[00:49:57.000 --> 00:50:01.000]   Um, and I think there is a class of problems that is, like, subjective tasks.
[00:50:01.000 --> 00:50:10.000]   Like, there's no open, like, frontier benchmark for creative writing or emotional intelligence.
[00:50:10.000 --> 00:50:14.000]   Um, right, but you can construct one if you want to.
[00:50:14.000 --> 00:50:18.000]   Um, yeah, we can, like, brainstorm if that's helpful.
[00:50:18.000 --> 00:50:19.000]   Yeah.
[00:50:19.000 --> 00:50:19.000]   Um, yeah.
[00:50:19.000 --> 00:50:19.000]   Yeah.
[00:50:19.000 --> 00:50:19.000]   Um, yeah.
[00:50:19.000 --> 00:50:20.000]   Yeah.
[00:50:20.000 --> 00:50:38.360]   is like, do you think about like ROI working on things that are easy to evaluate? Like you can move much faster. How do you consider things that are more subjective? So you can't move as fast, but they're also important. Like would it be better to just take all these and put them on the things that aren't objective and you can move very fast?
[00:50:38.600 --> 00:51:03.200]   Yeah, I think moving fast though is also as tasks becomes like much more complex, as we kind of like optimized everything out from kind of easy tasks. Now we kind of want to like teach the models for more low, like longer horizon tasks of software engineering or kind of automating like AI research, right? This in itself as a very hard tasks.
[00:51:03.900 --> 00:51:10.300]   And, you know, you have like kind of have some milestones. Maybe you can create benchmarks to hit those milestones.
[00:51:10.300 --> 00:51:26.880]   Yeah, I do think this is like, you know, I think like this is all solvable things. And I don't think people should be worried about like moving fast versus like slow, because I feel like everything is moving fast these days. Yeah.
[00:51:26.880 --> 00:51:30.480]   If you were to build a startup right now, what startup would you do?
[00:51:30.480 --> 00:51:57.900]   Um, I had, I mean, I was thinking about building a startup before joining Anthropic with that project was into Alia. Um, well, let's see, I actually really, like one thing that I recently told someone is like, I would actually build something around like particle collider, I don't know, or bio tech.
[00:51:57.900 --> 00:52:04.780]   Like, like, I don't think we need larger particle collider than CERN, then somebody should be building one. Um, yeah.
[00:52:04.780 --> 00:52:14.140]   Because I feel like, uh, the models would be able to build any product that you imagine. Um, yeah, sure. Yeah.
[00:52:14.140 --> 00:52:18.880]   What do you think are the major bottlenecks for your team or, um, for OpenAI?
[00:52:18.880 --> 00:52:40.340]   Um, I don't know, it is, it is, yeah. Um, the bottlenecks are, oh, the question is, what's the major bottlenecks, uh, for OpenAI? Um, I don't know, I'm not sure if, like,
[00:52:40.340 --> 00:52:51.220]   faster execution should be solved with more people versus, um, like using AIs now to, to help us move faster, if that makes sense.
[00:52:51.220 --> 00:53:02.100]   So I feel like that's the bottleneck. I think there was a lot of, I think right now we are like in the middle of figuring a lot of things out in order to like, like, I feel like in a year or two, that will accelerate us so much more.
[00:53:02.100 --> 00:53:18.980]   I feel like infrastructure is actually one of the major bottlenecks, right? If you don't build infrastructure with, uh, multimodal, like as a first class citizen, then, um, obviously all the multimodal things will be much more, um, slow, like, um, more difficult.
[00:53:18.980 --> 00:53:31.860]   So, um, yeah, in a way it's like infra, um, um, figuring out like what to prioritize at a given time, because sometimes you can say yes to everything and then not having a focus time is also a thing, yeah.
[00:53:31.860 --> 00:53:57.860]   Um, basically when the model checks the turns context and updates how you should score the same parameter based on that turn.
[00:53:57.860 --> 00:54:22.860]   Wait, um, I'm a little bit confused by this question. Um, I think generally like rubric based grading is, um, very powerful and, um, you know, as long as you optimize, um, things that you want to optimize and there's no reward hacks, then it's good. Um, how do you imagine AI will be used by creatives?
[00:54:22.860 --> 00:54:27.860]   If not just generating entire works on their own, how will they be integrated in their creatives flow?
[00:54:27.860 --> 00:54:37.860]   Well, um, I think, you know, I think AI is, is just like right now, maybe it's more like you would use Figma or like tools like Adobe.
[00:54:37.860 --> 00:54:45.860]   Um, and I think in the future, it's more, um, maybe it will be much more co-creation with an AI rather than using it as a tool.
[00:54:45.860 --> 00:54:51.860]   Uh, maybe we have like live brainstorming and create things on the fly and then publish it together.
[00:54:51.860 --> 00:54:58.860]   it's like much more companion, um, work. Cool. Other questions?
[00:54:58.860 --> 00:55:05.860]   Other questions?
[00:55:05.860 --> 00:55:10.860]   Um, tasks that are not captured by road models?
[00:55:10.860 --> 00:55:12.860]   Like how do you capture?
[00:55:12.860 --> 00:55:23.860]   That cannot be captured? Um, I mean, I can't think of any because, um, the way you can, uh, the way you teach the road model is based on some of the human preferences maybe, right?
[00:55:23.860 --> 00:55:30.860]   Or like pairwise comparisons. So essentially you can almost teach anything, uh, to the model.
[00:55:30.860 --> 00:55:36.860]   I do think the complexity will, like the complexity always arises with like tools, right?
[00:55:36.860 --> 00:55:44.860]   And like, if you give very complicated tool, um, then you might want to be, like the learning is like much more difficult.
[00:55:44.860 --> 00:55:51.860]   So I feel like, um, I do hope that almost anything can be teachable in a row. Yeah.
[00:55:51.860 --> 00:56:04.860]   Um, so I'm curious, like, now that you bring up like the learning of the frequency, how do you prevent the model from converging to like the main, like, frequency level?
[00:56:04.860 --> 00:56:10.860]   Like Airbnb listings all looking the same. Like, how do you inject some creative diversity?
[00:56:10.860 --> 00:56:21.860]   Yeah, I think this is an interesting question. I think they, uh, the, the two ways, um, like what's talking on my mind is, um, how do you preserve entity from the base model, right?
[00:56:21.860 --> 00:56:39.860]   Like, uh, base models are super diverse because they can literally elicit any human preference or, um, a human thought. Um, so how do you, um, you know, yeah, preserve the entity from the base model during the role?
[00:56:39.860 --> 00:56:56.860]   And another way, um, is, uh, the, the reason why we love our layoff, like reinforcement learning from AI feedback and create synthetic generations of pairwise comparisons, let's say, is because it can induce the diversity that you really want to teach on like distribution that you care about.
[00:56:56.860 --> 00:57:08.860]   Uh, that is not like the mean average consumer. Um, yeah, because sometimes like, you know, um, average human can prefer certain emojis or markdown, but actually you don't want the model to behave like this.
[00:57:08.860 --> 00:57:25.860]   So in a way you can discourage the model from doing that. Um, yeah. So I feel like synthetic curation, like synthetic data generation is mostly like curation of, um, that kind of diversity. Um, yeah.
[00:57:25.860 --> 00:57:44.860]   Um, yeah, I think it's qualitative, especially for the model behavior, like refusals. Um, Oh, um, how do I distill this question?
[00:57:44.860 --> 00:57:52.860]   Um, um, how do I distill this question?
[00:57:52.860 --> 00:58:11.860]   Um, yeah. Like how do you capture like models limitations on something automatic versus a home manual? Um, I think there was a lot of benefit of literally playing with the model and, uh, look at the outputs and see what are the weirdnesses it has.
[00:58:11.860 --> 00:58:29.860]   Um, there are definitely like, um, maybe more automatic checks, uh, and those are mostly like evals, right. Um, but, um, and maybe you have an eval that specifically checks with the behavior that you really don't want. Um, and that might be helpful, but a lot of nuanced weirdnesses that, um, like, did you realize is like through kind of like manual. And, and like, um, and that might be helpful. But a lot of nuanced weirdnesses that, um, like, did you realize is like through kind of like manual. And, and like, um, and that might be helpful.
[00:58:29.860 --> 00:58:51.860]   Um, and like, um, another thing is like the model should be consistently behaving like this because if it's just like one off, then it's fine. But if the model consistently exhibits this behavior, then, uh, this becomes like a more problematic thing. Yeah.
[00:58:51.860 --> 00:59:03.860]   You mentioned in one of your slides are a huge tosses coming down. Um, but also, you know, it's kind of subjective creative dimension.
[00:59:03.860 --> 00:59:15.860]   like creating a whole visual interface or something. Arguably complexity of the problems, uh, increases. Do you think that, you know, for those problems, compute is still pretty much a whole limitation?
[00:59:15.860 --> 00:59:21.860]   Or do you think that we're kind of getting to this point where, uh, that improvement is going to be improvement to the models or data sets?
[00:59:21.860 --> 00:59:32.860]   Um, I think that computer efficiency is important. I think that with more test time compute, generally the assumption is, uh,
[00:59:32.860 --> 00:59:49.860]   um, the model can always get better and better. So it might achieve kind of human level of visual design, but can it just invent new interaction paradigms, like new interaction patterns?
[00:59:49.860 --> 01:00:01.860]   I feel like that's more of a superhuman skill. Um, and I do hope that with more compute, um, it will do that at some point. Yeah.
[01:00:01.860 --> 01:00:06.860]   So you can actually, um,
[01:00:06.860 --> 01:00:07.860]   you can actually, uh,
[01:00:07.860 --> 01:00:08.860]   um,
[01:00:08.860 --> 01:00:09.860]   how do you verify it?
[01:00:09.860 --> 01:00:10.860]   Okay.
[01:00:10.860 --> 01:00:11.860]   Let me, uh,
[01:00:11.860 --> 01:00:12.860]   um,
[01:00:12.860 --> 01:00:17.860]   Um, well, the thing with synthetic data is that you don't actually need to generate a lot of it, right? Um, so you can actually have like very much like manual inspection of what's going on. Um, obviously you can think of other methods, like, um, ask a human laborer, um,
[01:00:17.860 --> 01:00:23.860]   to check the work, um, you can also ask, um,
[01:00:23.860 --> 01:00:38.860]   you can also ask another model if that model is like very well covered. So it's like much, it becomes a more like meta thing. Maybe you can see a lot of it. Um, you can actually have like very much manual inspection of what's going on. Um, obviously you can think of other methods, like, um, ask a human laborers to check the work. Um, you can also ask another model if that model is like very well covered. So it's like much, it becomes a more like meta thing.
[01:00:38.860 --> 01:01:07.860]   Maybe you can have a meta eval for that model to verify what you want to like verify. Um, and I think we are kind of entering those types of tasks too. Um, but the thing with synthetic data, um, and the data itself is like maybe you don't need as much. Um, and I think we are kind of entering those types of tasks too. Um, but the thing with synthetic data, um, and the data itself is like maybe you don't need as much.
[01:01:07.860 --> 01:01:14.860]   Um, what's important really is diversity and, um, yeah, diversity. Cool.
[01:01:14.860 --> 01:01:35.860]   Just a follow-up question to that. What do you think about like the diversity and quality tool? So there's like been like recent code generation covers where it's like, um, well, they put instruction diversity as like, um, they prioritize it more than code.
[01:01:35.860 --> 01:02:03.860]   So, um, yeah, I think it's, um, it actually depends so much on like, like task and what you're trying to do. Sometimes synthetic data, uh, like a lot, if you like, if synthetic data happens to collapse certain mode and then it will like actually be hurting, uh, your training. But if synthetic data happens to collapse certain mode and then it will like actually be hurting, uh, your training. But if synthetic data happens to collapse certain mode and then it will like actually be hurting, uh, your training. But if synthetic data happens to collapse certain mode and then it will like actually be hurting, uh, your training. But if synthetic data happens to collapse certain mode and then it will actually be hurting, uh, your training.
[01:02:03.860 --> 01:02:25.860]   Uh, your training. But if synthetic data is actually, um, diverse, it might actually at scale, if you do a lot of URL on this, it might just like recover from this. Um, yeah. Yeah. It's hard to give you the right answer. Cause it depends. Nice.
[01:02:25.860 --> 01:02:26.860]   Another question.
[01:02:26.860 --> 01:02:53.860]   Thanks for, um, so I have a question that's about making money, I guess. So, um, okay. It's a real question though. Uh, like we all know that, um, serving large language models is expensive, especially at the scale that, uh, you know, OpenAI and Thropic are doing. Uh, and my impression has been that, uh, at the current stage, um,
[01:02:53.860 --> 01:03:11.860]   um, it's actually losing money to, uh, you know, serve all these models, uh, to produce the products. Uh, is that true? Or, uh, what, what's being done to, to, I guess, bring down the cost?
[01:03:11.860 --> 01:03:39.860]   Oh yeah. I mean, I think, uh, for a developer, um, I think, I don't think like, I, I, I don't know. I think it's the question for Sam, whether we are losing money or not. But, um, I do think that the, like the generality of the technology is like so wide that, um, you know, I don't think, I think about like how to make money in, on a regular basis. I don't know.
[01:03:39.860 --> 01:04:01.860]   But, um, yeah, as a developer, let's say, uh, and if you're using all the tools, you actually don't need to like create a, like foundation model now. Like you can just use, uh, like very inexpensive, like, very like open, like deep seek is like very interesting results, right? So you can have, you can like bootstrap from existing research.
[01:04:01.860 --> 01:04:14.860]   I think it's harder to be at the frontier because you kind of need to like invent and that might be expensive. Like when you invent something new, it's always inefficient. It's always expensive. But, um, actually it was every technological innovation.
[01:04:14.860 --> 01:04:27.860]   And like, then what comes next, the second innovation is, uh, how do we bring down the cost of the thing that, that happened, right? And this is what's happening in AI too. Um,
[01:04:27.860 --> 01:04:49.860]   And then you can create an amazing product and this is how you make money. Yeah, that's, that's a good answer. Uh, just quickly following up. So, um, is most of the cost reduction coming from like infrastructure improvements or can better models or better algorithms also, uh, contribute to lower costs?
[01:04:49.860 --> 01:05:15.860]   Um, lower costs. Yeah. I mean like, uh, lowering costs, but, um, I think it's, um, the costs of production, maybe just, you know, the cost of production of the model training itself might go down. Um, like it's, um, the training process in itself, um, might be not as costly anymore, but obviously, uh, we scale a lot.
[01:05:15.860 --> 01:05:26.860]   It's like scale. Yeah. It's very much like in linear relationship, like scaling and compute. I think it's just like, yeah, it's hard to know. I'm sorry. I didn't have an answer to this.
[01:05:26.860 --> 01:05:29.860]   Yeah. Okay. Thank you.
[01:05:29.860 --> 01:05:35.860]   So how do you envision LMS could be used in other fields, such as robotics or a body AI?
[01:05:35.860 --> 01:05:41.860]   Oh yeah. I do think like, um, future AIs will be building data centers, um,
[01:05:41.860 --> 01:05:49.860]   um, to all like, um, or how are the current alums in robotics. I think I've, I've read like, um,
[01:05:49.860 --> 01:06:00.860]   um, PI, um, the company by Sergey Lemon, like the paper, they using a lot of like RLHF or like RL, uh, for robotics tasks.
[01:06:00.860 --> 01:06:10.860]   Um, obviously I feel like data is a huge limitation and bottleneck. Um, but I think as long as you solve that, um, I think it would be really amazing. Yeah.
[01:06:10.860 --> 01:06:17.860]   Um, I'm very hopeful and very, yeah, excited about this.
[01:06:17.860 --> 01:06:39.860]   Oh, hi. So you mentioned you work, uh, in both product and research. So I'm interested in, um, from the developer or the researcher point of view, uh, Anthropic and OpenAI. Um, what's the visibility for you into other components of the model and how easy is it for you to learn?
[01:06:39.860 --> 01:06:46.860]   For you to learn, say the pre-training of GPT, why you are like responsible for part of the post-training?
[01:06:46.860 --> 01:06:57.860]   Oh yeah. Um, well, I mostly worked on post-training side of things. Um, obviously pre-training, um, obviously you just cannot run like pre-training on your own.
[01:06:57.860 --> 01:07:09.860]   You want to like be a part of like the next big, uh, like training or something. So that's like part of the teams that you can join. And I think there's, um, good visibility in like those steps.
[01:07:09.860 --> 01:07:22.860]   And then sometimes you want to, you know, uh, contribute the datasets to them or, um, help with certain tasks that you're interested in.
[01:07:22.860 --> 01:07:29.860]   So I was curious, do you have any co-workers who are AIs right now? And do you use like agents in a kind of co-worker relationship right now?
[01:07:29.860 --> 01:07:39.860]   Um, I don't think I have like an amazing product to be like this, right? I think I use Chai GPT on a regular basis. Is it my co-worker? Not really.
[01:07:39.860 --> 01:07:50.860]   Like sometimes they ask like, what do you think? Yeah, it was like in Chai GPT. Like what I really want, uh, I don't know if people have used this product called Tuple.
[01:07:50.860 --> 01:08:06.860]   It's a pair programming, um, software, but then it's like, you can just call a model, um, and then share screen. And then the model can, um, just literally start like editing my code if I'm coding.
[01:08:06.860 --> 01:08:18.860]   Or I can like highlight things and then the model can see it and change this as much more of a natural co-working, um, form factor.
[01:08:18.860 --> 01:08:23.860]   But no, like, I don't think we are, technology wise, like we are not there yet.
[01:08:23.860 --> 01:08:28.860]   Do you have any ideas for like what's missing and having an autonomous, like co-worker partner?
[01:08:28.860 --> 01:08:42.860]   Because I mean, I've seen the cool, you know, demonstrations here. I think one of the paper like a year or two ago on just like simulating like a city of people and all that. Like what's the gap between that? And so like a peer that you could have on Slack.
[01:08:42.860 --> 01:08:57.860]   Like I think the gap is actually social intelligence and like some of the human things, like being able to like in real time, um, like generate things that I'm asking and then also being smart about things.
[01:08:57.860 --> 01:09:09.860]   Like maybe I want to coach too, but like will the model be able to like tell me instead of like, uh, taking agency from me, the model can just like guide, guide me through this.
[01:09:09.860 --> 01:09:13.860]   Like it depends on like how you want to form this relationship. It's like a really hard.
[01:09:13.860 --> 01:09:24.860]   So the model is, I feel like it's kind of limited by the social abilities. Does it make sense? Like I think that's missing, um, kind of like, um, speech to speech conversation.
[01:09:24.860 --> 01:09:33.860]   Um, like can the model converse back with me in real time and then point to things that it talked about exactly at the same time.
[01:09:33.860 --> 01:09:45.860]   So it's like those type of, um, things that might need, might have possibly need like some other changes in like architecture and like multimodality things. Yeah.
[01:09:45.860 --> 01:09:51.860]   In your experience, which is, which are the biggest differences between traditional product development?
[01:09:51.860 --> 01:10:02.860]   Yeah, that's a good question. Cause, um, I interned briefly at like companies like Dropbox or, um, Square and they may mainly very much like,
[01:10:02.860 --> 01:10:11.860]   traditional software product development and they go through like this interesting life cycle of, okay, I have like PRD.
[01:10:11.860 --> 01:10:19.860]   I do this. I incentivize designer designers comes up with like UI's and then I asked software engineers to do this.
[01:10:19.860 --> 01:10:29.860]   Um, we have two minutes, I believe. Um, yeah, but, uh, I think with research during products is actually comes from research.
[01:10:29.860 --> 01:10:42.860]   Uh, like if the research has like an impressive demo, uh, of the model capability, um, maybe then, um, there is, um, then you shape the product around it.
[01:10:42.860 --> 01:10:50.860]   Um, and sometimes it's like both product and research come together from the very beginning and do something.
[01:10:50.860 --> 01:10:57.860]   Um, it just happened with the canvas for example. Yeah. When both product and research were together and there was like less, less of a process.
[01:10:57.860 --> 01:10:59.860]   It's like more of a ad hoc. Yeah.
[01:10:59.860 --> 01:11:09.860]   Um, for fundamental unverifiable domains like creative writing or visual art.
[01:11:09.860 --> 01:11:27.860]   Um, so, yeah, it's interesting. Um, I think, you know, nobody has tried it. I'm like, this, uh, this looks reasonable. Um, yeah, especially if creative writing, you have all the like competitions or prizes that people, uh, got from, um, their writing. So there's something in there. Um, someone who's still concerned with AI.
[01:11:27.860 --> 01:11:49.860]   We're stepping into the creative space. Do you have any personal moral affliction on how your work is so powerful? And that was, um, I actually wrote a blog post about this. Um, I have a blog post.
[01:11:49.860 --> 01:11:59.860]   It's called in sub stack and the post is called moral progress. And I think you will find it interesting.
[01:11:59.860 --> 01:12:02.860]   All right. So give, uh, give a hand again for, uh, Karina.
[01:12:02.860 --> 01:12:04.860]   Thank you.
[01:12:04.860 --> 01:12:05.860]   Thank you.
[01:12:05.860 --> 01:12:35.840]   Thank you.

