
[00:00:00.000 --> 00:00:08.720]   Robert is a graduate of my alma mater UC Berkeley, where he worked in the rise lab, I believe, with Michael Jordan as his primary advisor.
[00:00:08.720 --> 00:00:18.520]   And yeah, now is the co founder of any scale co creator of Ray and open source library for doing scalable machine learning. So lots of
[00:00:18.800 --> 00:00:30.320]   Folks in the community are really interested in taking their code and scaling it up to big clusters doing big jobs training big models. So I'm really excited to hear from him. So, Robert, go ahead and take it away.
[00:00:30.320 --> 00:00:35.640]   I'll share my screen. Thanks a lot. And yeah, thanks a ton for for inviting me.
[00:00:35.640 --> 00:00:46.040]   So I'm going to tell you a bit about Ray, which it's a it's an open source library. It's a Python library for scaling up, you know, doing parallel and distributed Python.
[00:00:46.840 --> 00:00:51.600]   And it started out as a research project at UC Berkeley when I was a grad student
[00:00:51.600 --> 00:01:04.720]   And now any scale is the company behind the project. Of course, we're doing a bunch of development on Ray at any scale, but it's a it's an open source project and, you know, being there are serious contributions from a number of different companies.
[00:01:04.720 --> 00:01:14.440]   So before I dive into the details at a high level, you know, Ray consists of three parts. It's the core system consists of
[00:01:15.040 --> 00:01:24.560]   Tools for scaling up Python applications. So essentially taking arbitrary Python applications and running those paralyzing those across multiple cores or multiple machines.
[00:01:24.560 --> 00:01:35.560]   And then on top of those primitives. There's an ecosystem of libraries. So some of these are libraries that are developed natively as part of Ray. Some of those are libraries that are
[00:01:35.560 --> 00:01:43.840]   Third party libraries that run on top of Ray, like you may be familiar with Horovod or hugging face or spacey or a number of different
[00:01:44.840 --> 00:02:05.840]   Libraries in the machine learning ecosystem. And then lastly, Ray consists of tools for launching clusters on any cloud provider running, you know, running your Ray cluster on Kubernetes or AWS or GCP or Azure and really trying to take a lot of the friction out of running scalable experiments or applications.
[00:02:06.960 --> 00:02:15.080]   So a natural question is, you know, why are we, why are we doing this. The reason we started Ray and then we started any scale is that we think
[00:02:15.080 --> 00:02:24.800]   Looking at hardware trends that the future of computing is distributed basically that more and more applications will need to be to run on clusters.
[00:02:24.800 --> 00:02:30.480]   And there are a number of reasons for that one is hardware trends, looking at the amount of compute or the amount of
[00:02:31.040 --> 00:02:41.400]   Memory or computation required to do machine learning and to build machine learning applications. Another has to do with the nature of machine learning applications and how
[00:02:41.400 --> 00:02:49.520]   AI and machine learning are being integrated into more and more applications. So to give you an example, you know,
[00:02:49.520 --> 00:02:59.840]   Of what's changed right distributed systems are not new. So you may ask, look, why build a new distributed system. So going back to, you know, distributed
[00:03:00.560 --> 00:03:08.960]   Computing has been around for quite a long time, at least since the 80s, you have high performance computing, you know, tools for simulating molecular dynamics or stimulating
[00:03:08.960 --> 00:03:19.640]   You know, like the weather and climate, you have more recently in the 90s, you have the emergence of the internet and and distributed systems to support large scale web traffic.
[00:03:19.640 --> 00:03:28.440]   More recently in the 2000s, you have big data and the emergence of systems like MapReduce and Hadoop and and then more recently Spark.
[00:03:29.200 --> 00:03:46.200]   And then even more recently, you have the rise of deep learning and tools like systems like TensorFlow and PyTorch for scaling deep learning applications. So you have all these different workloads that need to be distributed and you have systems for for accomplishing that for each of them.
[00:03:46.200 --> 00:03:48.600]   And
[00:03:48.600 --> 00:03:50.800]   Sorry about that.
[00:03:50.800 --> 00:03:52.520]   And
[00:03:58.360 --> 00:04:00.360]   Sorry, I just needed to turn off this.
[00:04:00.360 --> 00:04:10.360]   I think it's gone. Okay. Apologies. Um, okay. So you have all these different distributed systems for different kinds of workloads.
[00:04:10.360 --> 00:04:19.360]   So what's changed, you know, what's different now is that we no longer have, they're no longer isolated workloads that each exists separately.
[00:04:19.720 --> 00:04:28.800]   Right. So whereas historically you have all these these separate workloads and you have separate systems for each computational pattern right you like you have TensorFlow and PyTorch for deep learning.
[00:04:28.800 --> 00:04:37.360]   You have different systems for microservices and serving you have big data systems like Spark and and streaming systems like Kafka and Flink.
[00:04:37.360 --> 00:04:45.640]   And you have different tools for high performance computing. What's changed now is that you're starting to see these workloads overlap.
[00:04:46.080 --> 00:04:51.200]   So if you look at applications like reinforcement learning, for example, that requires training as well as
[00:04:51.200 --> 00:04:56.640]   large scale simulations. So you have high performance computing and deep learning overlapping.
[00:04:56.640 --> 00:05:04.880]   If you look at applications and like online learning where you have machine learning models that are interacting with the real world and learning from that experience.
[00:05:04.880 --> 00:05:11.120]   You have training, of course, but also serving and all sorts of other application logic and big data, you know,
[00:05:12.000 --> 00:05:18.080]   interacts with all of these. You have you use data to power your deep learning, you use it to power your simulations,
[00:05:18.080 --> 00:05:30.480]   the serving feeds into the data collection. And so you start to have applications that touch all of these different kinds of computational patterns. And so each of these systems individually, whether it's TensorFlow or Flink
[00:05:30.480 --> 00:05:36.240]   has trouble supporting the whole, you know, these, these more hybrid applications.
[00:05:36.960 --> 00:05:43.920]   And so there's a clear need for a system, you know, a general purpose system that can support all of these different workloads.
[00:05:43.920 --> 00:05:50.000]   And the natural question is, you know, what should that system look like? What, what are the requirements?
[00:05:50.000 --> 00:06:01.200]   And that's exactly the question that we posed for ourselves when we began working on Ray. So essentially, you know, that's why we started working on Ray, why we started at any scale.
[00:06:02.160 --> 00:06:11.640]   The goal with Ray is to build this general purpose distributed system that can really be, that can really support the whole, you know, this whole diversity of different kinds of
[00:06:11.640 --> 00:06:14.120]   applications on top.
[00:06:14.120 --> 00:06:30.760]   So to give you an example of what that looks like, or an example of an application that requires this kind of generality, I'll give you an example from a Ray user. So this is a large fintech company that wants to build an online learning recommendation system.
[00:06:31.360 --> 00:06:41.160]   So the anatomy of this application, they have users, there's data streaming in from their users, they're processing that data in a streaming fashion to extract features.
[00:06:41.160 --> 00:06:45.560]   And then they're taking those features and they're feeding them into training. So they're
[00:06:45.560 --> 00:06:51.160]   incrementally training and updating the recommendation models using those features.
[00:06:51.160 --> 00:06:58.600]   And then they're taking those recommendation models and serving recommendations back to their users and then that feeds back into the data collection.
[00:06:59.360 --> 00:07:06.400]   So you have all these different, you know, to build a single application, you have a number of different moving parts and computational patterns.
[00:07:06.400 --> 00:07:18.440]   And one question they were asking themselves, because they were doing online learning, was how quickly can they update the model, how frequently can they push new models into production, and what is the benefit of doing that faster?
[00:07:18.440 --> 00:07:25.920]   And so they found when they initially implemented this system, it took about a day. They were updating the model about once a day.
[00:07:26.520 --> 00:07:33.760]   With a lot of optimizations, they were able to get it down to about once an hour. And that was, and that, and when they
[00:07:33.760 --> 00:07:41.880]   did that optimization going from one day to one hour for pushing out new models, that led to a 5% increase in click through rate, which was huge.
[00:07:41.880 --> 00:07:52.800]   So the natural question was, you know, could they go further? And they wanted to, but given the way they had architected their application, they couldn't realistically do it.
[00:07:53.600 --> 00:08:02.480]   So, and to understand what the bottlenecks were there, what the limitations were, we have to talk a little bit about how they had architected their application.
[00:08:02.480 --> 00:08:16.720]   The way they were doing this, and in fact, you know, one of the most common ways of building this kind of application is to essentially take each component of the pipeline, each different computational pattern, and use the state of the art, off the shelf
[00:08:17.320 --> 00:08:27.480]   distributed system for that component. So you can use Spark or Flink for your stream processing. You can use TensorFlow or Horovod or PyTorch for training, and you can use
[00:08:27.480 --> 00:08:37.400]   a model serving system for the serving part. And then you essentially take all these different distributed systems, and you have to glue them together and stitch them together.
[00:08:37.400 --> 00:08:46.080]   And this, you know, turns out this is a really difficult way to develop applications. You have to learn how to use all these different systems.
[00:08:46.600 --> 00:08:51.800]   You have to have potentially have infrastructure teams that are managing these different systems and maintaining them.
[00:08:51.800 --> 00:08:58.400]   You have to have, you know, for the application itself, it can be expensive because you're moving data between different systems.
[00:08:58.400 --> 00:09:10.640]   If you're trying to reason about handling failures, it's quite complex because each of these systems has a different approach to handling failures and some of them, you know, don't handle failures at all. And so,
[00:09:11.880 --> 00:09:21.880]   you know, and of course, if you have, if the engineers are specializing in different systems, then it becomes harder for people working on one part of the application to help out with another. And this is all just to build a single application.
[00:09:21.880 --> 00:09:32.560]   So what they were able to do here, you know, going back to this picture, they were able to take this whole application and build it on top of Ray.
[00:09:32.560 --> 00:09:39.280]   And there, instead of using a, you know, a stream processing system and a training system and a serving system, they were able to use
[00:09:40.200 --> 00:09:46.480]   you know, libraries that are part of the Ray ecosystem. So for a stream processing or training or serving and
[00:09:46.480 --> 00:09:55.360]   and then when they did that, you know, the application got simpler, got faster and of course led to a boost in click-through rate.
[00:09:55.360 --> 00:10:03.560]   So this was a huge win for them. And this is the kind of example of an application that it would be very, it's, you know, quite difficult to do
[00:10:04.120 --> 00:10:15.840]   without, when you're combining specialized systems together. The typical approach is either to, you know, stitch a bunch of existing systems together or to build a new system from scratch.
[00:10:15.840 --> 00:10:22.400]   Okay, so that's a little bit about the kind of application where we think Ray can really excel.
[00:10:22.400 --> 00:10:32.320]   So now I'm going to tell you a little bit more about Ray. So I want to emphasize just a few key ideas. So
[00:10:33.760 --> 00:10:39.040]   one is the API, the concepts that Ray introduces. You know, with distributed systems,
[00:10:39.040 --> 00:10:43.840]   the kinds of applications that you can support on top of your system
[00:10:43.840 --> 00:10:53.280]   are really, you know, affected by the concepts that that system is built around. So with something like Spark, the core concept is a data set or data frame.
[00:10:53.280 --> 00:11:02.760]   And then the API reflects that. With TensorFlow, you know, it's about neural networks or computation graphs. So that's the core abstraction and then all the APIs are built around that.
[00:11:02.880 --> 00:11:08.520]   With Ray, and of course, when you have these higher level concepts like a data set or neural network, that makes it,
[00:11:08.520 --> 00:11:10.800]   you know, that makes it
[00:11:10.800 --> 00:11:24.440]   easy to build applications that are like based on data processing or based on neural networks. But if you're trying to build something that doesn't fit neatly into that abstraction, then you end up having to
[00:11:25.160 --> 00:11:37.320]   do a lot of things to work around it. And so with Ray, the way we're trying to achieve generality here is by not really introducing new concepts. So not introducing a higher level concept like a data set or neural network, but rather
[00:11:37.320 --> 00:11:45.720]   taking the existing concepts of functions and classes, which we know are general enough to, you know, to express all kinds of workloads,
[00:11:45.720 --> 00:11:52.600]   and then translating those into the distributed setting as tasks and actors. And I'll give you a code example in a few slides.
[00:11:53.880 --> 00:12:01.120]   The second idea is about enabling parallelism through futures, through asynchronous computation. I'll show you that in the API.
[00:12:01.120 --> 00:12:12.040]   And then the third, something we care a lot about is performance, because if you are, you know, if you're trying to build a general purpose system that can support lots of different workloads, then
[00:12:12.040 --> 00:12:21.880]   you inherit all the performance requirements of all those different workloads. So to be more general, you actually have to often be a lot more performance in a lot of dimensions.
[00:12:22.200 --> 00:12:30.280]   And that's one way that sort of manifests itself here is with our distributed object store, and I'll talk more about that as well.
[00:12:30.280 --> 00:12:44.120]   Okay, so I'll start with the API. So this is a, you know, a couple Python functions, just regular Python code, reading a couple arrays from a file and adding them together to get the sum.
[00:12:44.840 --> 00:12:55.000]   And here's a Python class which we instantiate and then it just has a counter that can get incremented and we increment the counter twice. So just kind of a toy example.
[00:12:55.000 --> 00:13:10.560]   So what we can do with Ray is, like I said, we translate the functions and classes into the distributed setting. We do that by adding this ray.remote decorator. So that turns a regular Python function into a Python function that can be executed
[00:13:12.400 --> 00:13:30.960]   asynchronously and execute somewhere in the cluster. And now when we actually want to execute that Python function, we do it by appending the dot remote suffix to the function name. Similarly, when we are, when we instantiate the counter, because it now has the ray.remote decorator,
[00:13:30.960 --> 00:13:39.440]   what it does is it creates a copy of that counter object as a service or a little microservice or actor somewhere in the cluster. And now that
[00:13:40.320 --> 00:13:49.520]   you know, other actors or other tasks or can invoke methods on that actor, which are essentially send messages to that actor to execute its methods.
[00:13:49.520 --> 00:14:00.840]   And of course, when we invoke these methods or these remote functions, what gets returned is essentially a reference to the output. It's not the actual output of the computation. It's just
[00:14:01.640 --> 00:14:19.040]   like a future. And then if we want to retrieve the actual value from the computation, we can call ray.get. So this is the API in a nutshell. It's just, you know, a few, it's not a huge API. It's just a few methods, but it's general enough to express all kinds of computations on top.
[00:14:19.040 --> 00:14:21.000]   Okay.
[00:14:21.000 --> 00:14:24.480]   So I will
[00:14:25.920 --> 00:14:37.360]   walk through what happens if you actually call this. So if you call the first function read array.remote, what happens is that a task gets scheduled somewhere in the cluster, running on one of these machines.
[00:14:37.360 --> 00:14:45.320]   You know, it immediately returns a future. So you can call, invoke the second copy of the function before the first one has finished executing.
[00:14:45.320 --> 00:14:53.840]   You know, we can take the two futures that were passed, that were returned by the first function calls, pass those into a third add task.
[00:14:54.880 --> 00:15:05.840]   And then that gets scheduled somewhere in the cluster. That third task is not going to run, you know, until the first two tasks have finished running and then their outputs will have, the outputs of the first two read array tasks will be
[00:15:05.840 --> 00:15:14.000]   shipped under the hood to wherever the add task is getting executed. And then once those inputs have arrived, the add task will execute.
[00:15:14.000 --> 00:15:23.800]   And then lastly, if you want to retrieve the results, you can call ray.get to actually fetch the value. So that's what it looks like under the hood.
[00:15:24.800 --> 00:15:29.320]   So to say just quickly about the distributed object store.
[00:15:29.320 --> 00:15:39.880]   We have, when we actually execute one of these methods, it gets invoked on some machine and it gets executed on that machine. And then that machine
[00:15:39.880 --> 00:15:50.600]   runs the actual Python function which produces the output. And then when that happens, that output stays on that machine. It doesn't get shipped back to the original node.
[00:15:50.960 --> 00:15:59.280]   And what that means is that you can then pass this IDX into a second function, which can then run somewhere else.
[00:15:59.280 --> 00:16:12.000]   And if that second function uses the output of the first function, that output will already be there. So it doesn't need to get shipped back and forth unnecessarily or, you know, go through the go through node one if it doesn't have to.
[00:16:12.760 --> 00:16:21.560]   And so the benefit there is that, especially if we're using, you know, Ray makes use of shared memory so that X can actually live in shared memory. We don't even need
[00:16:21.560 --> 00:16:33.440]   to copy X from one worker process to another worker process, especially with a lot of machine learning workloads using a lot of computations, very heavy in numerical data
[00:16:34.240 --> 00:16:48.000]   or neural network weights or things like that. Having automatically storing this stuff in shared memory so it can be accessed by a bunch of different worker processes without creating copies or without doing expensive deserialization can be a big win.
[00:16:48.000 --> 00:16:51.160]   So that's what I want to say about performance.
[00:16:52.440 --> 00:17:02.200]   The architecture is, you know, similar to some other distributed systems. There are worker processes on each machine that actually execute the computation.
[00:17:02.200 --> 00:17:11.560]   There's a shared memory object store on each machine that holds objects in shared memory so that we don't have to create copies. There's a scheduler on each machine.
[00:17:11.560 --> 00:17:21.440]   This is important for, you know, having this decentralized or distributed scheduler is important for avoiding centralized bottlenecks so you can have
[00:17:22.320 --> 00:17:34.960]   a very high task throughput, which is important for lots of kinds of workloads, especially in model serving where you may have tons and tons of different requests coming in that you're responding to. And then
[00:17:34.960 --> 00:17:44.400]   lastly, I want to say there's this global control store which stores important system metadata which can be used for recovering from failures.
[00:17:46.520 --> 00:17:47.040]   Okay.
[00:17:47.040 --> 00:17:59.560]   And that'll just show one performance slide, which is about task throughput. So this is a difference from going from Ray 0.7 to Ray 0.8 when we introduced direct actor calls which
[00:17:59.560 --> 00:18:05.840]   basically a way for workers to talk directly from one worker to another without going through the scheduler.
[00:18:06.480 --> 00:18:22.240]   So this made a huge difference in terms of task throughput scaling to hundreds of thousands of tasks per second on a small cluster with a couple hundred CPU cores. And, you know, of course, you can scale much higher given larger clusters.
[00:18:22.240 --> 00:18:24.840]   Okay.
[00:18:26.200 --> 00:18:37.320]   So one thing I want to say quickly is that Ray comes with a bunch of different libraries. They're native libraries that are developed as part of Ray. So, RLib is one of the first we started building for reinforcement learning.
[00:18:37.320 --> 00:18:47.160]   Tune is one that actually integrates with weights and biases and it's used for hyperparameter tuning. We started a library recently for
[00:18:48.040 --> 00:18:55.000]   deploying machine learning models in production. This is at a much earlier stage, but it's seeing a lot of use.
[00:18:55.000 --> 00:18:58.000]   And we recently started a library for
[00:18:58.000 --> 00:19:04.960]   elastic distributed training of machine learning models using PyTorch and TensorFlow under the hood.
[00:19:04.960 --> 00:19:14.840]   But perhaps the even more exciting part is all the libraries in the ecosystem that are integrating with Ray. So you may be familiar with
[00:19:15.680 --> 00:19:29.640]   Optuna and HyperOpt, which are popular hyperparameter tuning libraries. Horovod for training. These all are integrating with Ray. Tune for hyperparameter tuning. Hugging Face and Spacey, you know, two of the most popular NLP libraries, integrate with Ray as well.
[00:19:29.640 --> 00:19:40.920]   A lot of machine learning platforms, so cloud machine learning platforms like AWS SageMaker, Azure Machine Learning are starting to use RLib and Tune as well.
[00:19:42.160 --> 00:19:53.080]   Dask is a popular open source library for distributed Python and it has a popular DataFrames library and you can run Dask on top of Ray to scale more.
[00:19:53.080 --> 00:20:06.720]   We integrate with weights and biases for tracking your experiments and there's other libraries from Intel as well. And lastly, you know, there are a number of other libraries, I don't have time to mention, but
[00:20:07.600 --> 00:20:15.640]   that really that are, you know, form part of this ecosystem. And the cool thing here is not just that, you know, you can run one of these libraries on top of Ray,
[00:20:15.640 --> 00:20:22.600]   but rather that you can, when you're building your applications, you can pick and choose the state of the art libraries that you want to use off the shelf
[00:20:22.600 --> 00:20:34.000]   and integrate those to build a single application. Kind of like how on your laptop, to develop your application, you can import NumPy and Pandas and use them all together to build your application.
[00:20:34.480 --> 00:20:42.720]   Right now in the distributed setting, there's nothing like that. You know, you have standalone distributed systems and sort of that's the value we see with building an ecosystem.
[00:20:42.720 --> 00:20:49.760]   So if you're interested in learning more about Ray, and I think we can send a link on the in the chat,
[00:20:49.760 --> 00:20:59.520]   we're going to, we're hosting a summit this at the end of this month. So we'll be showcasing a lot of interesting use cases, how companies are using Ray in production to
[00:21:00.360 --> 00:21:09.120]   scale their applications and we'll also be have there'll be talks from Weights and Biases and Hugging Face and Spacey and a number of different libraries that are using Ray.
[00:21:09.120 --> 00:21:13.080]   And so, you know, highly encourage you to check it out.
[00:21:13.080 --> 00:21:22.800]   And yeah, so I just want to leave it at that and say Ray is a universal framework for distributed computing and we're building this rich ecosystem on top of it.
[00:21:23.400 --> 00:21:38.480]   So if you're a library developer and are working to, you know, interested in scaling your library or making it easy to for people to run your library and the distributed setting, we'd love to, Ray may be a good fit for that and we'd love to help out with that. So thanks a lot.
[00:21:38.480 --> 00:21:52.400]   Great, thanks, Robert for a really great presentation. It's very exciting. I guess I'd never seen the sort of like high level vision behind Ray. Like I just heard over the phone.
[00:21:52.400 --> 00:21:56.400]   Oh, you know, Ray helps you run at any scale, you know, Ray helps you run
[00:21:56.400 --> 00:22:06.000]   You know, on your computer, all the way up to 1000 GPUs and this idea that you said at the end about, you know, it's sort of like how on your laptop, you're not stuck, you know, in any, you know,
[00:22:06.000 --> 00:22:12.000]   building these monolithic services, but you can compose an app together. That's a really cool vision and thanks for sharing it with us.
[00:22:12.000 --> 00:22:13.280]   Thank you.
[00:22:14.920 --> 00:22:21.160]   I have a couple of questions that I just wrote down while we were going and we'll get more from folks on YouTube and zoom
[00:22:21.160 --> 00:22:29.000]   But just to start off. So you mentioned, you know, that one of the motivations here was that the future is distributed computing and you sort of motivated it by
[00:22:29.000 --> 00:22:38.840]   The particular workloads that people are doing now that it's, you know, machine learning is a data heavy workflow that like kind of everybody wants to use the internet is a famous example of successful distributed computing.
[00:22:39.600 --> 00:22:48.640]   Do you think that that's driven in part by the sort of like bending away from the Moore's law curve that a lot of people have talked about, or do you think that that's those are two separate phenomena.
[00:22:48.640 --> 00:23:04.440]   You know, it's, it's, that's a really good point. So there are a couple things going on here. One is the there. So, yes. So we know Moore's law is, you know, has kind of flattened out right and it's, it's, it's not keeping up with what it used to be doing.
[00:23:05.800 --> 00:23:14.320]   Now, even if it were continuing at its original pace, it still wouldn't be enough because if you look at the amount of compute required to do machine learning.
[00:23:14.320 --> 00:23:16.120]   You know, going
[00:23:16.120 --> 00:23:33.600]   Whether that's like from Alex net a while back to things like AlphaGo and and GPT three now to do that, that it's that's going way, way faster than Moore's law. And so even if Moore's law had continued, there's still a huge gap and
[00:23:34.520 --> 00:23:41.240]   And so, so, and it's not just compute. It's also memory, you know, the model sizes are increasing a huge amount
[00:23:41.240 --> 00:23:50.920]   And this, you might ask, like, what about specialized hardware like accelerators GPUs or TPUs, you know, when you
[00:23:50.920 --> 00:23:57.680]   Have these different architectures right like a GPU or TPU, you do get a huge, you know, big boost in terms of compute
[00:23:58.080 --> 00:24:06.360]   But then when you fix the architecture, it still increases, you know, it still grows the performance improvements for GPUs or TPUs are still growing
[00:24:06.360 --> 00:24:24.400]   In a Moore's law like fashion. So while it is giving you a boost. It's not, it's not closing the gap. So, and when you're scaling, you know, subsequent generations of TPUs typically are actually essentially embedded, you know,
[00:24:25.840 --> 00:24:37.600]   Distributed themselves. They're putting multiple chips and single TPU and things like that. So we have. So that's a big factor, right, just the, the, yes, the fact that
[00:24:37.600 --> 00:24:48.520]   The Moore's law is over the fact that the amount of compute required to do machine learning is growing a ton. And then furthermore, and the point I was trying to emphasize
[00:24:48.920 --> 00:25:00.760]   That machine learning is getting embedded in more and more different kinds of applications. So it's not just, it's not just it, you know, you could say, oh, it's, it's just if you want to do machine learning training, then you need to
[00:25:00.760 --> 00:25:09.240]   Use all this computer. That's not the case. Machine Learning is it's not really this isolated workload where you just go and train a model, right, it's being
[00:25:09.240 --> 00:25:15.920]   Integrated into more and more different kinds of applications, whether that's offline or online and
[00:25:16.920 --> 00:25:20.040]   And that's, you know, combined with all other sorts of business logic.
[00:25:20.040 --> 00:25:30.000]   Yeah, that's actually it's interesting you bring that up because I was a major point of a recent presentation in the salon by Andy for which who's at Google Developer Program.
[00:25:30.000 --> 00:25:41.720]   Who's big on this idea that, you know, now the main like use for machine learning models is to integrate multiple models together, not to do sort of like the end to end training that people initially thought was the main
[00:25:42.000 --> 00:25:50.960]   Utility or the main sort of workflow for for a machine learning model. So it's good to see that this this idea is more broadly spread
[00:25:50.960 --> 00:25:52.880]   And a lot of people, you know, I mean,
[00:25:52.880 --> 00:25:58.240]   Even if you want to do something as simple as using TensorFlow to
[00:25:58.240 --> 00:26:09.160]   Train your neural networks and then using spark to do your data processing and then of course you want to be able to do both. But you want to be able to process your data and then feed it into training.
[00:26:09.720 --> 00:26:13.480]   Even that there's there's there's aren't great ways to do that off the shelf.
[00:26:13.480 --> 00:26:15.560]   Definitely.
[00:26:15.560 --> 00:26:22.360]   Yeah, the, the, I want to make sure we actually get to these couple questions that came in.
[00:26:22.360 --> 00:26:31.280]   They're mostly actually about tune and they're sort of more more technical questions about optimization. So I hope you brought your, your optimization hat here.
[00:26:32.000 --> 00:26:41.440]   Is basically there's some curiosity about, you know, what, what's going on under the hood to sort of terminate bad hyper parameter values during a search and then relatedly, you know, which
[00:26:41.440 --> 00:26:50.000]   Which search methods would you recommend for folks who have limited compute and maybe are like very aggressively trying to cut down on their compute budgets.
[00:26:50.000 --> 00:26:58.800]   You know, versus between like Bayesian optimization with hyper band or population based training or, you know, something else. Yeah, so
[00:26:59.560 --> 00:27:08.120]   I'll say the answer that with the caveat that I might say the wrong thing. And I'm not 100% I'm not actually the best person to answer the question.
[00:27:08.120 --> 00:27:14.200]   So what's going on under the hood when it comes to terminating bad trials.
[00:27:14.200 --> 00:27:25.800]   So there are two aspects of that right. One is like, what is the mechanism for terminating bad trials and then the other is how is the decision made to terminate a bad trial. I think for
[00:27:27.000 --> 00:27:40.720]   The making the decision. There are certain, you know, metrics that you can report when you're using tune and track and then if those fall below, you know, a certain amount of those are are much lower than other
[00:27:40.720 --> 00:27:46.640]   You know the performance of other trials and then that's how we choose to which ones to terminate
[00:27:46.640 --> 00:27:51.480]   And the other aspect of like how we actually terminate it
[00:27:52.800 --> 00:28:04.640]   That's so tune is implemented using Ray actors, which is one of the primitives that we showed here and I think the way it's done is by essentially terminating the the actors, although it's also possible to
[00:28:04.640 --> 00:28:09.880]   Pause some of the actors and then and then resume the competent the training that they're doing later.
[00:28:09.880 --> 00:28:17.120]   When it comes to what would you use if you're trying to not spend all your money on on compute.
[00:28:18.040 --> 00:28:25.200]   A couple things there. So one, which is not part of the question, but spot and you know you can use spot instances and this is a great
[00:28:25.200 --> 00:28:37.280]   Case where you if you have a system like Ray or tune, which is resilient to failures and preemption of instances, then you can actually use cheaper spot instances. And then the second thing is
[00:28:37.280 --> 00:28:43.800]   As for the actual the answer. I think the answer is not going to be grid search.
[00:28:44.760 --> 00:28:51.920]   And as a simple baseline random search is probably pretty good, but I've seen good results from population based training.
[00:28:51.920 --> 00:29:03.000]   Yeah, I think, you know, I would agree. I think with your assessment that random search is always, you know, it's a great baseline. It's a lot simpler than than Bayesian
[00:29:03.520 --> 00:29:10.760]   Methods often and the addition of, you know, something like hyper bands to cut off trials early is usually enough to to, you know, make that
[00:29:10.760 --> 00:29:18.880]   Easy, easy use. And that's something that's built into the tools like weights and biases and Ray tune and all kinds of other libraries. So you don't have to reinvent the wheel on that.
[00:29:18.880 --> 00:29:26.200]   Absolutely. Yeah. And one, you know, one thing that's nice about well about random search is is just that it's
[00:29:26.200 --> 00:29:29.440]   It's hard to implement it incorrectly.
[00:29:29.800 --> 00:29:34.120]   There you go. Yeah, it's it's stateless, which is, which is really nice. Yeah.
[00:29:34.120 --> 00:29:42.520]   So actually, you know, I hate to cut you off, but you do have your, your 530 deadline. So I don't want to make you late to anything.
[00:29:42.520 --> 00:29:44.640]   Oh, thank you so much. It's really a lot of fun.
[00:29:44.640 --> 00:29:46.760]   Yeah, thanks for coming. Robert. Take care.
[00:29:46.920 --> 00:29:48.440]   Thanks for coming. Robert. Take care.
[00:29:48.440 --> 00:29:49.440]   Bye.
[00:29:49.440 --> 00:29:50.440]   Bye.
[00:29:50.440 --> 00:29:51.440]   Bye.
[00:29:51.440 --> 00:29:52.440]   Bye.
[00:29:52.440 --> 00:29:53.440]   Bye.
[00:29:53.440 --> 00:29:54.440]   Bye.
[00:29:54.440 --> 00:29:55.440]   Bye.
[00:29:55.440 --> 00:29:56.440]   Bye.
[00:29:56.440 --> 00:29:57.440]   Bye.
[00:29:57.440 --> 00:29:58.440]   Bye.
[00:29:58.440 --> 00:30:08.440]   [BLANK_AUDIO]

