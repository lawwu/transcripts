
[00:00:00.000 --> 00:00:06.000]   Taking us a little bit out of the realm of applications that Mohamed Reza and Stacey had us in,
[00:00:06.000 --> 00:00:11.800]   I wanted to talk about a core algorithm, a core idea from linear algebra
[00:00:11.800 --> 00:00:15.800]   that shows up in machine learning quite a bit, the singular value decomposition.
[00:00:15.800 --> 00:00:21.800]   And what I want to convince you by the end of this talk is that the singular value decomposition is a refactor.
[00:00:21.800 --> 00:00:28.500]   So, the singular value decomposition, if you're not familiar with it, it's a very important algorithm.
[00:00:28.500 --> 00:00:33.500]   Just some examples of the SVD in action. Principal components analysis is an application of it.
[00:00:33.500 --> 00:00:38.500]   The discrete Fourier transform is an application of singular value decomposition,
[00:00:38.500 --> 00:00:41.500]   which includes JPEG, the image encoding technique.
[00:00:41.500 --> 00:00:48.500]   Computing the pseudo-inverse, which actually came up, I don't know if you noticed it, in Mohamed Reza's talk.
[00:00:48.500 --> 00:00:52.000]   Computing the pseudo-inverse was an important part of the RBF training step.
[00:00:52.000 --> 00:00:57.000]   And computing the pseudo-inverse is typically done via the singular value decomposition.
[00:00:57.000 --> 00:01:02.000]   It's that is related to why it shows up when we try and solve linear regression.
[00:01:02.000 --> 00:01:06.000]   If all those examples seem a little bit too, perhaps, academic for you,
[00:01:06.000 --> 00:01:11.000]   video for round isolation, recommendation engines, page rank, and more,
[00:01:11.000 --> 00:01:18.000]   all these very useful application domains, the singular value decomposition shows up.
[00:01:18.000 --> 00:01:24.000]   And despite that broad array of applications, it's actually deceptively simple.
[00:01:24.000 --> 00:01:30.000]   The singular value decomposition simply states that we can break a matrix down into three particular pieces,
[00:01:30.000 --> 00:01:34.000]   which usually go by the names U, sigma, and V transpose,
[00:01:34.000 --> 00:01:37.000]   where U is a matrix that's usually kind of tall and skinny.
[00:01:37.000 --> 00:01:43.000]   V transpose is a matrix that's kind of wide and short, a squat.
[00:01:43.000 --> 00:01:50.000]   And then in between is a matrix sigma that is a diagonal matrix.
[00:01:50.000 --> 00:01:52.000]   This is a singular value decomposition.
[00:01:52.000 --> 00:01:56.500]   And this way of writing it sort of emphasizes the idea that this matrix M on the left
[00:01:56.500 --> 00:02:02.000]   is equal to the product of the matrix, matrices on the right.
[00:02:02.000 --> 00:02:06.000]   But I think there's a slightly different view that I want to take
[00:02:06.000 --> 00:02:11.500]   that's going to lead us to this view of SVD not as like an algebraic equation,
[00:02:11.500 --> 00:02:15.000]   but instead as a process akin to refactoring and programming.
[00:02:15.000 --> 00:02:18.500]   And first, we need to change a little bit the way we think about matrices.
[00:02:18.500 --> 00:02:22.000]   So matrix-vector multiplication is more like function application
[00:02:22.000 --> 00:02:25.000]   than it is like the multiplication of scalars.
[00:02:25.000 --> 00:02:28.000]   We should think of a matrix not just as an array of numbers,
[00:02:28.000 --> 00:02:30.000]   though sometimes thinking of it that way is helpful,
[00:02:30.000 --> 00:02:37.000]   but instead as an object that takes in certain kinds of inputs and returns certain kinds of outputs.
[00:02:37.000 --> 00:02:42.000]   So on this slide, I'm showing a matrix M in red, taking in a vector X in blue,
[00:02:42.000 --> 00:02:49.000]   and returning an output, which we call M times X, that is in purple and is also an array.
[00:02:49.000 --> 00:02:54.500]   So this view, sometimes it's useful to think of matrices as a bunch of numbers.
[00:02:54.500 --> 00:02:59.000]   Other times, it's useful to think of them as functions that act on vectors.
[00:02:59.000 --> 00:03:05.500]   So this singular value decomposition is a decomposition of a function.
[00:03:05.500 --> 00:03:10.000]   It's taking the matrix M and undoing the process of composing it.
[00:03:10.000 --> 00:03:12.500]   Functions are composed.
[00:03:12.500 --> 00:03:16.000]   They're combined together, one function after another,
[00:03:16.000 --> 00:03:20.500]   and the end result of composing a bunch of functions is also a function.
[00:03:20.500 --> 00:03:25.000]   So that's expressed by this diagram on the right-hand side here.
[00:03:25.000 --> 00:03:27.000]   So the way to read a diagram like this--
[00:03:27.000 --> 00:03:30.500]   let me pull up my laser pointer--
[00:03:30.500 --> 00:03:35.000]   is that we start up here in the top left with an array with n entries,
[00:03:35.000 --> 00:03:42.500]   and M is a function represented by an arrow that takes us from n entry arrays to m entry arrays,
[00:03:42.500 --> 00:03:47.500]   lowercase m on the size of the array.
[00:03:47.500 --> 00:03:54.000]   But we can alternatively think of this arrow here as a sequence of arrows,
[00:03:54.000 --> 00:03:59.000]   as a sequence of steps, V transpose followed by sigma followed by U.
[00:03:59.000 --> 00:04:04.000]   So the content that is on the left-hand side here expresses an equation
[00:04:04.000 --> 00:04:06.500]   is also expressed by this diagram here.
[00:04:06.500 --> 00:04:09.500]   These diagrams, commutative diagrams is what they're called,
[00:04:09.500 --> 00:04:15.500]   say that if I follow these arrows and end up at the same place as following any other path,
[00:04:15.500 --> 00:04:18.000]   then I must get the same result.
[00:04:18.000 --> 00:04:25.000]   So that's saying that V transpose followed by sigma followed by U is the same as M.
[00:04:25.000 --> 00:04:32.500]   So if we think of our singular value decomposition as a decomposition,
[00:04:32.500 --> 00:04:37.500]   as the reversal of composition, the connection to refactoring becomes a little bit more clear.
[00:04:37.500 --> 00:04:40.500]   Refactoring programs involves a couple of common tricks.
[00:04:40.500 --> 00:04:45.000]   Separation of concerns to say this function does one thing and this function does another.
[00:04:45.000 --> 00:04:50.500]   I don't mix them all together, peas and carrots together, cats and dogs lying together,
[00:04:50.500 --> 00:04:53.000]   you know, these sorts of awful things.
[00:04:53.000 --> 00:04:56.500]   We also sort of remove code that doesn't do anything.
[00:04:56.500 --> 00:05:00.500]   Oh, I don't actually need to check that. It's guaranteed not to have that value.
[00:05:00.500 --> 00:05:04.000]   This code is dead. It no longer refers to anything of use.
[00:05:04.000 --> 00:05:06.000]   Those are examples of refactoring tricks.
[00:05:06.000 --> 00:05:09.500]   Breaking up into functions is another important refactoring trick,
[00:05:09.500 --> 00:05:14.500]   decomposing functions that have been already sort of pushed together too much.
[00:05:14.500 --> 00:05:18.000]   And these all have direct equivalence for matrices.
[00:05:18.000 --> 00:05:23.000]   So for separation of concerns, we have eigenvectors and eigendecomposition.
[00:05:23.000 --> 00:05:26.500]   Sort of split up a matrix from a whole bunch of things happening all at once
[00:05:26.500 --> 00:05:33.000]   to, you know, n things happening on n different eigenspaces or eigenvectors.
[00:05:33.000 --> 00:05:36.500]   For throwing out code that doesn't do anything, we have low-rank approximation.
[00:05:36.500 --> 00:05:39.500]   This part of the matrix is unimportant. I will throw it out.
[00:05:39.500 --> 00:05:42.000]   This part of the code doesn't do anything. I will delete it.
[00:05:42.000 --> 00:05:47.500]   And for breaking up into functions or decomposing, we have singular value decomposition.
[00:05:47.500 --> 00:05:52.000]   When we refactor software, this decomposition step looks something like this.
[00:05:52.000 --> 00:05:55.500]   I have some function here on the right that's very tersely written.
[00:05:55.500 --> 00:05:59.500]   This one here returns the string true if the input is odd
[00:05:59.500 --> 00:06:03.000]   and the string false if the input is not odd.
[00:06:03.000 --> 00:06:07.000]   I can break that up into three steps, just sort of looking at this guy and saying,
[00:06:07.000 --> 00:06:10.000]   "Okay, what are the pieces of this function?"
[00:06:10.000 --> 00:06:16.500]   First, we use mod two to sort of check whether the input is divisible by two or not.
[00:06:16.500 --> 00:06:21.500]   Then we treat that value as a Boolean, not just as a number.
[00:06:21.500 --> 00:06:26.500]   Not just as zero or one, but as true or false. That's in the if-else step.
[00:06:26.500 --> 00:06:30.000]   Then finally, we return a string for our final output.
[00:06:30.000 --> 00:06:36.500]   We return true if the Boolean is true, and we return false as a string if the Boolean is false.
[00:06:36.500 --> 00:06:39.500]   We can break those out and make those all separate functions.
[00:06:39.500 --> 00:06:44.500]   Now our isOdd function at the top is a sequence of three operations that are written out
[00:06:44.500 --> 00:06:50.000]   rather than the sort of tightly compacted thing that's on the left.
[00:06:50.500 --> 00:06:53.500]   It's a matter of taste, maybe, which of these is the right way to write it,
[00:06:53.500 --> 00:06:56.000]   and maybe neither is really the right way to write it.
[00:06:56.000 --> 00:06:58.000]   But the important thing is that we can do it.
[00:06:58.000 --> 00:07:02.000]   We can split this function up if we want, and we can take this function,
[00:07:02.000 --> 00:07:06.000]   this set of functions, and we can collapse them down into a single one.
[00:07:06.000 --> 00:07:10.000]   This is the process of refactoring and rewriting software.
[00:07:10.000 --> 00:07:15.500]   In this case, one function is being decomposed into three pieces.
[00:07:15.500 --> 00:07:21.000]   This should remind you a little bit of that diagram of the SVD I showed just a few slides ago.
[00:07:21.000 --> 00:07:25.000]   We start off with an integer that we want to check whether it's odd.
[00:07:25.000 --> 00:07:29.500]   If we feed it to the function isOdd, what we should get out is a string, true or false.
[00:07:29.500 --> 00:07:34.500]   Maybe we're printing this to a user who wants to know whether their input is odd or not.
[00:07:34.500 --> 00:07:41.000]   What this diagram is saying is that we can apply three separate functions and get the exact same result.
[00:07:41.000 --> 00:07:48.000]   This way of writing isOdd really emphasizes that it is implemented in this particular way,
[00:07:48.000 --> 00:07:51.000]   as mod two, then two bool, then two string.
[00:07:51.000 --> 00:07:57.500]   The two bool function checks whether that mod two output is equal to one,
[00:07:57.500 --> 00:08:02.000]   and the two string one just takes that Boolean and turns it into a string.
[00:08:02.000 --> 00:08:07.000]   Let's talk about what those three pieces are.
[00:08:07.000 --> 00:08:12.000]   The way of understanding those three pieces in a way that is going to generalize
[00:08:12.000 --> 00:08:15.000]   is to say that they do three separate things.
[00:08:15.000 --> 00:08:19.000]   That mod two picks out representative examples,
[00:08:19.000 --> 00:08:21.500]   that two bool reversibly renames them,
[00:08:21.500 --> 00:08:25.000]   and that two string then gives them the right type for the output.
[00:08:25.000 --> 00:08:30.000]   To be more specific, when I say that mod two picks representatives,
[00:08:30.000 --> 00:08:34.000]   what I mean is that we need a representative for each output.
[00:08:34.000 --> 00:08:39.500]   One representative integer that is the example of an odd number,
[00:08:39.500 --> 00:08:42.000]   and one that is the example of an even number.
[00:08:42.000 --> 00:08:47.500]   Mod two corresponds to picking as our examples zero and one.
[00:08:47.500 --> 00:08:50.500]   Somebody else might use one and two as their examples.
[00:08:50.500 --> 00:08:52.500]   They would have a different function than mod two,
[00:08:52.500 --> 00:08:56.000]   but they would be doing in this decomposition the same thing,
[00:08:56.000 --> 00:09:02.000]   saying, "I want to pick out a representative even number and odd number,"
[00:09:02.000 --> 00:09:06.000]   because those are the things that I give different answers for.
[00:09:06.000 --> 00:09:07.500]   It simplifies our function down.
[00:09:07.500 --> 00:09:10.000]   We just need to know how it works on two different inputs
[00:09:10.000 --> 00:09:12.500]   rather than on every possible input.
[00:09:12.500 --> 00:09:16.500]   Then we need to know which group each input falls into.
[00:09:16.500 --> 00:09:23.000]   Then two bool reversibly renames those inputs.
[00:09:23.000 --> 00:09:26.000]   It associates each representative with its output,
[00:09:26.000 --> 00:09:27.500]   sort of one to one,
[00:09:27.500 --> 00:09:32.500]   in a way such that each output is targeted by exactly one representative.
[00:09:32.500 --> 00:09:34.500]   We have zero and one.
[00:09:34.500 --> 00:09:39.500]   We have an example odd number and we have an example even number.
[00:09:39.500 --> 00:09:44.000]   Now we just need to be like, "Okay, which one of these is which?
[00:09:44.000 --> 00:09:47.000]   Is one the odd number or is zero the odd number?"
[00:09:47.000 --> 00:09:50.500]   One is the odd number, so it goes to true, and zero goes to false.
[00:09:50.500 --> 00:09:53.500]   That's this step here, two bool.
[00:09:53.500 --> 00:09:56.000]   Then finally, we need to put them in the correct type.
[00:09:56.000 --> 00:09:59.000]   Our output there was not necessarily the right type.
[00:09:59.000 --> 00:10:01.500]   It was just true and false.
[00:10:01.500 --> 00:10:06.500]   We need to recognize the output of our function inside of our output type.
[00:10:06.500 --> 00:10:09.000]   There are many strings besides just true and false.
[00:10:09.000 --> 00:10:12.000]   There's through and tals for one.
[00:10:12.000 --> 00:10:17.500]   There's all kinds of books and the content of this presentation.
[00:10:17.500 --> 00:10:20.000]   All of that is inside the type string.
[00:10:20.000 --> 00:10:22.500]   Lots of stuff besides true and false.
[00:10:22.500 --> 00:10:24.000]   We need to find inside true.
[00:10:24.000 --> 00:10:28.000]   We have to find the outputs that we want to associate with our two values.
[00:10:28.000 --> 00:10:31.500]   We just say the boolean true becomes the string true.
[00:10:31.500 --> 00:10:34.000]   The boolean false becomes the string false.
[00:10:34.000 --> 00:10:37.500]   Pretty straightforward, relatively simple step.
[00:10:37.500 --> 00:10:40.500]   This may seem a little bit trivial with this function,
[00:10:40.500 --> 00:10:44.500]   but if any function, you can do this three-step composition
[00:10:44.500 --> 00:10:48.500]   where I take some function func and first I do an onto transformation
[00:10:48.500 --> 00:10:50.500]   where I pick out representatives.
[00:10:50.500 --> 00:10:55.500]   I have a representative for each type of output I'm going to give.
[00:10:55.500 --> 00:10:58.000]   Then I apply a reversible function that says,
[00:10:58.000 --> 00:11:01.500]   "This representative is the one that gets this output."
[00:11:01.500 --> 00:11:03.500]   One is an even number.
[00:11:03.500 --> 00:11:06.500]   Two or zero is--
[00:11:06.500 --> 00:11:08.000]   Sorry, one is an odd number.
[00:11:08.000 --> 00:11:10.000]   Zero is an even number.
[00:11:10.000 --> 00:11:12.000]   That's the reversible step.
[00:11:12.000 --> 00:11:15.000]   Then lastly, we have a one-to-one step
[00:11:15.000 --> 00:11:18.000]   where we recognize that output that we had,
[00:11:18.000 --> 00:11:22.000]   which might not be every possible value of the output type,
[00:11:22.000 --> 00:11:25.500]   we recognize that as a subset of the output type.
[00:11:25.500 --> 00:11:27.500]   That's the final step here.
[00:11:27.500 --> 00:11:30.000]   I broke it down into foo, bar, baz
[00:11:30.000 --> 00:11:36.000]   for cultural reasons of how programmers like to write these things.
[00:11:36.000 --> 00:11:41.500]   But those three steps onto reversible one-to-one
[00:11:41.500 --> 00:11:45.000]   are a general way to break down literally any function.
[00:11:45.000 --> 00:11:47.500]   If you need to refactor a function,
[00:11:47.500 --> 00:11:52.000]   consider this three-step breakdown as a way of breaking down the function.
[00:11:52.000 --> 00:11:57.500]   Any matrix can also be decomposed into three pieces.
[00:11:57.500 --> 00:12:02.000]   Because matrices are simpler than any possible function,
[00:12:02.000 --> 00:12:05.000]   we can do a lot more than just breaking them down
[00:12:05.000 --> 00:12:09.000]   into those three very generic types of pieces.
[00:12:09.000 --> 00:12:14.000]   If we apply this decomposition to a matrix,
[00:12:14.000 --> 00:12:17.500]   we get that we can split it up into three pieces.
[00:12:17.500 --> 00:12:21.000]   A wide matrix, a square matrix, and a tall matrix.
[00:12:21.000 --> 00:12:25.500]   The wide matrix has inputs that are bigger than its outputs.
[00:12:25.500 --> 00:12:28.500]   How wide a matrix is tells you how big its inputs are.
[00:12:28.500 --> 00:12:32.000]   How tall it is tells you how big its outputs are.
[00:12:32.000 --> 00:12:36.500]   What this is doing is the same thing of picking representatives
[00:12:36.500 --> 00:12:41.000]   that was done in the previous example by mod two.
[00:12:42.000 --> 00:12:45.000]   For linear functions, for things that are implemented by matrices,
[00:12:45.000 --> 00:12:48.500]   if two inputs are sent to the same output by the matrix,
[00:12:48.500 --> 00:12:50.500]   both of them are sent to zero.
[00:12:50.500 --> 00:12:54.000]   And zero is sent to zero by any linear function.
[00:12:54.000 --> 00:12:58.000]   What this is basically doing is this matrix
[00:12:58.000 --> 00:13:01.500]   is going to take everything that gets sent to zero by m
[00:13:01.500 --> 00:13:05.000]   and send it to zero, but basically not do much else.
[00:13:05.000 --> 00:13:09.500]   That's the first step. That's the pick representatives step.
[00:13:09.500 --> 00:13:12.500]   Then we have the reversible relabeling step.
[00:13:12.500 --> 00:13:15.500]   That's the matrix in the middle. Here it's B.
[00:13:15.500 --> 00:13:20.000]   It's reversible, so it's square. Its inputs are the same size as its outputs.
[00:13:20.000 --> 00:13:24.000]   If it were not square, then there'd be no way to match inputs and outputs.
[00:13:24.000 --> 00:13:28.000]   It's sort of a "you can't put toothpaste back in the tube" kind of principle.
[00:13:28.000 --> 00:13:32.500]   This is our relabeling step, this middle one.
[00:13:32.500 --> 00:13:36.000]   This is where we're really doing most of the meat of the work.
[00:13:36.000 --> 00:13:41.000]   Finally, we have this tall matrix where the outputs can be bigger than the inputs.
[00:13:41.000 --> 00:13:46.000]   This one basically finds a copy of all arrays with some number of entries
[00:13:46.000 --> 00:13:49.500]   among the set of all arrays with a larger number of entries.
[00:13:49.500 --> 00:13:54.000]   I could do that where maybe the first r entries all have a value,
[00:13:54.000 --> 00:13:57.000]   and then the last m minus r are all zero,
[00:13:57.000 --> 00:14:00.000]   or they're all two, or they're all whatever I want.
[00:14:00.000 --> 00:14:05.500]   Or I could do it where the last r are the ones that have entries.
[00:14:05.500 --> 00:14:11.000]   It's going to depend on exactly the details of the matrix M and what function it does.
[00:14:11.000 --> 00:14:18.000]   Basically, A is going to find a copy of this r-dimensional space
[00:14:18.000 --> 00:14:22.000]   that our middle matrix mapped onto inside the outputs.
[00:14:22.000 --> 00:14:26.000]   It's going to do that in such a way that it gets the same answers
[00:14:26.000 --> 00:14:30.000]   as M applied to the original input.
[00:14:30.000 --> 00:14:34.500]   This is the part where we recognize that true and false aren't the only strings that exist.
[00:14:34.500 --> 00:14:37.000]   There are many strings out there.
[00:14:37.000 --> 00:14:40.000]   Let's take true and false and turn them into strings,
[00:14:40.000 --> 00:14:47.000]   even though we know our function can't produce all possible strings.
[00:14:47.000 --> 00:14:50.000]   In this breakdown, that number r there is the rank of the matrix.
[00:14:50.000 --> 00:14:55.500]   It's the dimension of things that don't get mapped to zero by that matrix.
[00:14:55.500 --> 00:15:03.000]   That's going to be the size of the input of that last array there, A.
[00:15:03.000 --> 00:15:05.000]   In order to get the singular value decomposition,
[00:15:05.000 --> 00:15:07.000]   we just need to make some special choices.
[00:15:07.000 --> 00:15:09.500]   I said A, B, and C were pretty free to change around.
[00:15:09.500 --> 00:15:12.500]   I could multiply C by a number and divide A by a number,
[00:15:12.500 --> 00:15:14.500]   and I would get the same result.
[00:15:14.500 --> 00:15:18.500]   If we make some specific choices, then we get the singular value decomposition.
[00:15:18.500 --> 00:15:26.000]   If we make B diagonal and we make A and C unitary, then we get this SPD.
[00:15:26.000 --> 00:15:29.500]   Unitary means no growing or shrinking of anything.
[00:15:29.500 --> 00:15:35.500]   One motivation for doing that is that our diagonal matrix in the middle, that B,
[00:15:35.500 --> 00:15:37.500]   can do all the growing and shrinking for us.
[00:15:37.500 --> 00:15:40.500]   Our first step was just picking out representatives,
[00:15:40.500 --> 00:15:47.500]   and our last step was just recognizing our outputs inside another type.
[00:15:47.500 --> 00:15:49.500]   There's no need to really grow or shrink there.
[00:15:49.500 --> 00:15:56.500]   We can put that all in the middle in that diagonal matrix.
[00:15:56.500 --> 00:16:01.500]   So no growing and shrinking, but maybe some turning around
[00:16:01.500 --> 00:16:04.500]   and some reflecting and things like that.
[00:16:04.500 --> 00:16:06.500]   That's what's allowed by unitary transformation.
[00:16:06.500 --> 00:16:11.500]   If we do that, that gives this generic style of decomposition
[00:16:11.500 --> 00:16:14.500]   that can be applied to any possible function.
[00:16:14.500 --> 00:16:16.500]   If we make these specific choices,
[00:16:16.500 --> 00:16:20.500]   then we get the singular value decomposition as the output.
[00:16:20.500 --> 00:16:25.500]   As a technical note, this specific choice of the exact shapes here
[00:16:25.500 --> 00:16:27.500]   is what's called a compact SPD.
[00:16:27.500 --> 00:16:29.500]   You might see something slightly different,
[00:16:29.500 --> 00:16:32.500]   but the principle of breaking a matrix down into these three pieces
[00:16:32.500 --> 00:16:35.500]   that are doing effectively the same three things
[00:16:35.500 --> 00:16:39.500]   is going to hold no matter what kind of SPD it is that you're computing.
[00:16:39.500 --> 00:16:41.500]   The compact SPD happens to be most useful
[00:16:41.500 --> 00:16:45.500]   when you're doing maybe a little bit more algebra-type stuff,
[00:16:45.500 --> 00:16:47.500]   whereas the other kinds are maybe more useful
[00:16:47.500 --> 00:16:51.500]   when you're doing actual numerical computing.
[00:16:51.500 --> 00:16:54.500]   This is just one perspective on the SPD,
[00:16:54.500 --> 00:16:57.500]   and it's an uncommon one.
[00:16:57.500 --> 00:17:00.500]   I came up with it while trying to understand pseudoinverses
[00:17:00.500 --> 00:17:03.500]   and then found a couple of other people talking about this,
[00:17:03.500 --> 00:17:08.500]   but it doesn't seem to be the most common way to think about the SPD.
[00:17:08.500 --> 00:17:11.500]   But I do think it gives me a different and interesting insight
[00:17:11.500 --> 00:17:13.500]   into what's going on.
[00:17:13.500 --> 00:17:18.500]   I guess the only last point I'd make is that this style of decomposition
[00:17:18.500 --> 00:17:21.500]   comes from something called the first isomorphism theorem,
[00:17:21.500 --> 00:17:24.500]   which gets used in abstract algebra all over the place
[00:17:24.500 --> 00:17:26.500]   and actually gives you different insights
[00:17:26.500 --> 00:17:29.500]   depending on which structure it is you apply it to.
[00:17:29.500 --> 00:17:35.500]   And it tends to show up and sort of develop really interesting concepts
[00:17:35.500 --> 00:17:37.500]   no matter what it is you apply it to,
[00:17:37.500 --> 00:17:39.500]   whether it's group theory or fields or ring theory
[00:17:39.500 --> 00:17:41.500]   or whatever it is you're doing with your algebra.
[00:17:41.500 --> 00:17:44.500]   This first isomorphism theorem shows up,
[00:17:44.500 --> 00:17:47.500]   and it gets generalized very broadly in category theory
[00:17:47.500 --> 00:17:52.500]   to be applicable to just an absolute bewildering array of mathematical objects.
[00:17:52.500 --> 00:17:56.500]   And every time it gets applied, a new interesting concept pops up.
[00:17:56.500 --> 00:17:59.500]   So it's cool to me that this singular value decomposition,
[00:17:59.500 --> 00:18:02.500]   which is also this workhorse algorithm,
[00:18:02.500 --> 00:18:10.500]   shows up as this deep mathematical concept.
[00:18:10.500 --> 00:18:15.500]   So with that, I'll close out my talk,
[00:18:15.500 --> 00:18:21.500]   and I will take any questions if folks have them.

