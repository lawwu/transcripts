
[00:00:00.000 --> 00:00:05.560]   So I always watch myself go live and it looks like I'm I'm like
[00:00:05.560 --> 00:00:08.800]   a squirrel looking around and people don't realize luckily no
[00:00:08.800 --> 00:00:12.160]   comment has come across but it's just this monitor that I'm
[00:00:12.160 --> 00:00:16.880]   trying to look across and make sure things are working. And I
[00:00:16.880 --> 00:00:20.320]   have to wait until I hear an echo. So I am just waiting for
[00:00:20.320 --> 00:00:20.600]   that.
[00:00:20.600 --> 00:00:23.560]   Are you sitting down or are you standing now?
[00:00:23.560 --> 00:00:27.920]   I have a fancy setup. So I'm standing up I have standing
[00:00:27.920 --> 00:00:34.640]   desk. I got into that cult. Does it have your back? It does.
[00:00:34.640 --> 00:00:39.480]   Yes. Yeah. Awesome. We're live. So I'll quickly start by
[00:00:39.480 --> 00:00:44.400]   introducing the session for the speakers. Hi, everyone. Welcome
[00:00:44.400 --> 00:00:48.640]   back to Chai Time Kaggle Talks. This is Chai Time Data Science
[00:00:48.640 --> 00:00:52.800]   2.0 because we get into more depth of first of all learning
[00:00:52.800 --> 00:00:55.760]   about the great guests that agreed to come on here today. We
[00:00:55.760 --> 00:00:58.160]   have two incredible guests that I'll just introduce in a minute.
[00:00:58.160 --> 00:01:04.360]   But also about the solution and today's talk is titled moving
[00:01:04.360 --> 00:01:09.480]   from 1100 to 40th in the last three days of the competition. I
[00:01:09.480 --> 00:01:13.120]   remember I'm I'm a new Kaggler. So I remember every day I would
[00:01:13.120 --> 00:01:16.400]   submit I would move up the leaderboard a little that would
[00:01:16.400 --> 00:01:19.480]   give me so much rush and I can only imagine the rush you guys
[00:01:19.480 --> 00:01:22.520]   would have had. So that's I'm really excited to learn how you
[00:01:22.520 --> 00:01:29.080]   felt during those those weeks. So I've introduced the sessions
[00:01:29.080 --> 00:01:31.920]   a few times this this has been a running series, you can find the
[00:01:31.920 --> 00:01:35.280]   playlist on my channel. I'll skip that to be respectful of
[00:01:35.280 --> 00:01:39.840]   the guest time. And again, this is really Chai Time Data Science
[00:01:39.840 --> 00:01:44.520]   2.0. I'm skipping across because I'm excited to introduce the
[00:01:44.520 --> 00:01:47.480]   guests for today. So first of all, I want to welcome Dinesh
[00:01:47.480 --> 00:01:54.000]   Vaskaran. He's @TweetTVVaskaran on Twitter. I'm sorry, I wanted
[00:01:54.000 --> 00:01:58.600]   to put expert there. I mixed the profiles, but he's a Kaggle 2x
[00:01:58.600 --> 00:02:03.800]   expert and previously principal data scientist. Vignesh, thank
[00:02:03.800 --> 00:02:05.000]   you so much for joining us.
[00:02:05.000 --> 00:02:10.160]   Thanks, Sanyam. But some some mistakes are really nice to
[00:02:10.160 --> 00:02:11.000]   have.
[00:02:11.000 --> 00:02:15.200]   I'm sure it won't be a mistake in a few days.
[00:02:15.800 --> 00:02:21.520]   I'm sure that the next slide is going to be a mistake soon.
[00:02:21.520 --> 00:02:25.320]   That's going to be epic.
[00:02:25.320 --> 00:02:31.560]   I didn't mess up here. Thank God. Shahul, I remember I was
[00:02:31.560 --> 00:02:34.760]   just telling Shahul this off the record, but first time I
[00:02:34.760 --> 00:02:38.440]   interacted with someone from India on Kaggle, it was Shahul.
[00:02:38.440 --> 00:02:42.360]   So I'm really excited to have him on the talk today. He's
[00:02:42.360 --> 00:02:46.280]   @Shahul786 on Twitter. He's a 2x master. This is a mistake
[00:02:46.280 --> 00:02:48.600]   because he'll be a grandmaster really soon. As you can see,
[00:02:48.600 --> 00:02:52.760]   there are 14 gold medals. Just want to go really close to that.
[00:02:52.760 --> 00:02:56.920]   I can only imagine what you're going through right now. And he
[00:02:56.920 --> 00:02:59.560]   has this incredible strength at the intersection of machine
[00:02:59.560 --> 00:03:02.240]   learning and NLP. That's the domain where he works on. We
[00:03:02.240 --> 00:03:05.360]   learn more about this. He's been a Kaggle addict since college.
[00:03:05.360 --> 00:03:07.640]   Welcome, Shahul. Thanks for joining us.
[00:03:07.640 --> 00:03:10.880]   Thanks for having us, Sanyam.
[00:03:12.080 --> 00:03:15.400]   Yeah, really, really excited to learn about your journey. We,
[00:03:15.400 --> 00:03:18.760]   the Indian community is slightly smaller on Kaggle due to some
[00:03:18.760 --> 00:03:21.520]   reason, but you all are really changing that. So that that
[00:03:21.520 --> 00:03:24.640]   really makes me super happy. And I'm excited to learn about your
[00:03:24.640 --> 00:03:30.440]   journey. Before this, I want to point out the audience to a
[00:03:30.440 --> 00:03:34.000]   link. Let me share that link in the zoom chat.
[00:03:41.000 --> 00:03:42.040]   Do you think it's good?
[00:03:42.040 --> 00:03:49.920]   This is a good solution link as well so people can follow.
[00:03:49.920 --> 00:03:53.320]   Sure, I'll post that in the YouTube chat real quick.
[00:03:53.320 --> 00:03:54.760]   Perfect.
[00:03:54.760 --> 00:04:01.400]   I'm gonna put this link in the YouTube chat, please head over
[00:04:01.400 --> 00:04:05.040]   to this link. This should take you all to our community where
[00:04:05.040 --> 00:04:09.400]   we host the questions. So this will take you to this thread
[00:04:09.400 --> 00:04:11.560]   where you can find all of the links you can connect with our
[00:04:11.560 --> 00:04:15.080]   guests, you can find the right app. And also a few dashboards
[00:04:15.080 --> 00:04:19.200]   that I'll be using for EDA because I'm quite lazy. And feel
[00:04:19.200 --> 00:04:22.760]   free to ask your questions here if you'd like. Since it's being
[00:04:22.760 --> 00:04:25.080]   live streamed to YouTube, I can't monitor the chat. So I'll
[00:04:25.080 --> 00:04:30.000]   just look at the questions coming in here. And that is how
[00:04:30.000 --> 00:04:35.400]   you ask questions after signing up. Awesome. Sorry for this. I
[00:04:35.400 --> 00:04:38.480]   have to do a five minute intro every time because I feel really
[00:04:38.480 --> 00:04:40.760]   impatient while doing that. Because I'm always curious to
[00:04:40.760 --> 00:04:45.600]   learn about your journey. The reason why I do this is you all
[00:04:45.600 --> 00:04:48.640]   have reached the 40th position on the leaderboard. Shahul is
[00:04:48.640 --> 00:04:52.280]   one of the top ranked Indian Kagglers. And I really want to
[00:04:52.280 --> 00:04:55.640]   know how did you all do it? So how did what happened behind the
[00:04:55.640 --> 00:04:58.880]   scenes? How did you get started? So let me start by asking you
[00:04:58.880 --> 00:05:01.280]   Vignesh, how did you get interested in data science,
[00:05:01.280 --> 00:05:04.160]   broadly speaking, and what made you join this? I think this was
[00:05:04.160 --> 00:05:06.160]   your first competition. So why did you join it?
[00:05:07.480 --> 00:05:19.240]   Yeah, so I, I don't know, Kaggler is something that I came
[00:05:19.240 --> 00:05:24.560]   across it, I think, six years ago or something like that. So I
[00:05:24.560 --> 00:05:29.160]   studied in India, I finished my bachelor's in Chennai. And then
[00:05:29.160 --> 00:05:34.840]   I moved to Belgium to study at the University of Doha. It's a
[00:05:34.960 --> 00:05:40.120]   it's a small country, it's a but it's a beautiful city. So I
[00:05:40.120 --> 00:05:42.880]   chose to move there. And I studied my master's and at the
[00:05:42.880 --> 00:05:45.400]   end of the master's, so I studied my master's in
[00:05:45.400 --> 00:05:48.640]   artificial intelligence. And at the end of the master, there is
[00:05:48.640 --> 00:05:51.880]   a university level hackathon that happens where everyone
[00:05:51.880 --> 00:05:56.840]   competes on a machine learning competition, and then it's for
[00:05:56.840 --> 00:06:00.880]   one day. And so the beginning of the day, you get a problem
[00:06:00.880 --> 00:06:02.880]   statement, and then you participate and things like
[00:06:02.880 --> 00:06:05.680]   that. And that's the first time it was hosted on Kaggle. And
[00:06:05.680 --> 00:06:08.320]   that's the first time that I came up, I came to know about
[00:06:08.320 --> 00:06:14.080]   Kaggle. But luckily, I'm pretty sure you, you would probably
[00:06:14.080 --> 00:06:23.000]   know this guy, Ahmad Erdem. Yes, he was my classmate. And yeah,
[00:06:23.000 --> 00:06:30.040]   we studied together. He was the smartest of the university, as
[00:06:30.040 --> 00:06:32.840]   you can imagine. Anyway, so we studied together, we were in the
[00:06:32.840 --> 00:06:36.360]   same class, and we were really good friends. So we both
[00:06:36.360 --> 00:06:40.760]   competed in this competition. And that was the only
[00:06:40.760 --> 00:06:44.760]   competition where I did better than Ahmad. So I won the first
[00:06:44.760 --> 00:06:52.160]   competition. Ahmad did not make it. Anyways, like two weeks
[00:06:52.160 --> 00:06:55.400]   ago, I met Ahmad and I was jokingly telling him, your first
[00:06:55.400 --> 00:06:58.040]   Kaggle competition you lost to me, you could be a grandmother.
[00:07:01.040 --> 00:07:04.040]   Yeah, Ahmad is an incredibly funny person also to talk to.
[00:07:04.040 --> 00:07:07.280]   He's been on the podcast. And he's, he's very smart and funny
[00:07:07.280 --> 00:07:07.680]   also.
[00:07:07.680 --> 00:07:14.320]   Yeah, yeah. So he has been on the podcast as well. So that's
[00:07:14.320 --> 00:07:22.120]   it. And I, you know, I'm a serious Indian boy who wanted to
[00:07:22.120 --> 00:07:26.120]   graduate and then go ahead and find a job and then be sincere
[00:07:26.120 --> 00:07:31.400]   at work, work, work, work. I was a workaholic. And so I joined a
[00:07:31.400 --> 00:07:34.560]   company called Darts IP. And I was the first data scientist of
[00:07:34.560 --> 00:07:41.360]   the company. So I had this chance. So it I worked with them.
[00:07:41.360 --> 00:07:43.800]   My boss was the CEO of the company, that was one big
[00:07:43.800 --> 00:07:49.080]   advantage for me that I did not have several hierarchies to go
[00:07:49.080 --> 00:07:52.760]   or something like that. So I had this opportunity to build the
[00:07:52.760 --> 00:07:57.400]   machine learning team at the company. I so we built a team
[00:07:57.400 --> 00:08:05.560]   and the first four years it was fantastic. But things happened,
[00:08:05.560 --> 00:08:10.080]   some of our algorithms got much traction, and the company was
[00:08:10.080 --> 00:08:15.280]   acquired. So once the company was acquired, we Yeah, all of
[00:08:15.280 --> 00:08:18.200]   us made good money, but we did not have any more interesting
[00:08:18.200 --> 00:08:23.920]   projects. So I thought, okay, fine. One year, I did not do
[00:08:23.920 --> 00:08:27.120]   much with my life. So what more to do, and then I wanted to take
[00:08:27.120 --> 00:08:30.880]   a break. And I wanted to start my own company or do something
[00:08:30.880 --> 00:08:37.520]   about something like that. And that's when I happened to again
[00:08:37.520 --> 00:08:40.320]   talk to Ahmed, and then I checked his Kaggle profile. And
[00:08:40.320 --> 00:08:44.920]   then I saw he was a grandmaster then Wow. Now Ahmed is a
[00:08:44.920 --> 00:08:48.600]   grandmaster. And then I thought, okay, why not try a Kaggle
[00:08:48.600 --> 00:08:53.920]   competition. And I, I, and that's how I started doing the
[00:08:53.920 --> 00:08:59.800]   common let's stuff. The, the thing was, I worked for an IP
[00:08:59.800 --> 00:09:01.920]   company intellectual property company. So it was like
[00:09:01.920 --> 00:09:05.200]   extremely, we were very, very careful about our code and
[00:09:05.200 --> 00:09:08.800]   things like that. We don't speak with much, we don't speak
[00:09:08.800 --> 00:09:11.640]   outside, we don't showcase anything outside and things like
[00:09:11.640 --> 00:09:14.400]   that everything was closed. And we don't publish code or
[00:09:14.400 --> 00:09:17.720]   something like that. Because we were really, really careful
[00:09:17.720 --> 00:09:20.640]   about the competition and things like that. But you see, after
[00:09:20.640 --> 00:09:25.600]   coming out of the company, it was like a huge open space. And
[00:09:25.600 --> 00:09:28.320]   then I'm like, whoa, so many cool data scientists around and
[00:09:28.320 --> 00:09:33.360]   people are doing crazy stuff. And I cannot get to do it. I
[00:09:33.360 --> 00:09:37.720]   didn't build a lot of fantastic algorithms, we took, we took
[00:09:37.720 --> 00:09:40.240]   over 100 algorithms to production, and it was
[00:09:40.240 --> 00:09:44.240]   fantastic. But I couldn't showcase anything that was a
[00:09:44.240 --> 00:09:48.000]   big problem to me. And I could not learn from the community as
[00:09:48.000 --> 00:09:51.040]   well. You see, when you have a problem, I cannot speak to
[00:09:51.040 --> 00:09:53.880]   anyone and ask for tips as well. All these things were going on
[00:09:53.880 --> 00:09:58.840]   in my mind. And then I, it was like, all hell broke loose. And
[00:09:58.840 --> 00:10:04.080]   I was on Kaggle. And then I was, I really love it. I'm really
[00:10:04.080 --> 00:10:10.240]   grateful that I happen to be a very small part of the Kaggle
[00:10:10.240 --> 00:10:10.840]   community.
[00:10:11.600 --> 00:10:14.080]   So it's safe to say you've become addicted to Kaggle now?
[00:10:14.080 --> 00:10:15.440]   Has the Kaggle got better name?
[00:10:15.440 --> 00:10:19.200]   No, I'm addicted to the Kaggle community. Yes.
[00:10:19.200 --> 00:10:23.200]   Awesome. Shaul, I'll just come to your journey. But I have, I
[00:10:23.200 --> 00:10:25.320]   have another question. So you've, you've worked in the
[00:10:25.320 --> 00:10:28.440]   industry, Vignesh, you've put these incredible algorithms to
[00:10:28.440 --> 00:10:32.000]   production that were of great value. Is there a perspective
[00:10:32.000 --> 00:10:34.920]   change as you came to your first competition? People are always
[00:10:34.920 --> 00:10:37.520]   complaining, some people, not Kagglers, of course, that
[00:10:37.520 --> 00:10:40.560]   industry is really different. Kaggle is really different. Do
[00:10:40.560 --> 00:10:42.280]   you see the value there?
[00:10:42.280 --> 00:10:49.200]   Yes, it's really different. Really. Yeah. But I'm going to
[00:10:49.200 --> 00:10:54.000]   be honest to you. Kaggle is not about solving problems. It's
[00:10:54.000 --> 00:10:58.240]   about learning from the community and learning together.
[00:10:58.240 --> 00:11:02.400]   So there are many more things that you learn on Kaggle that
[00:11:02.400 --> 00:11:07.080]   you would probably not learn in any industry. So here, I don't
[00:11:07.080 --> 00:11:12.040]   know, we all do this. We do. Let's be honest, Sanyam, you
[00:11:12.040 --> 00:11:15.200]   were doing this podcast even before you were working for
[00:11:15.200 --> 00:11:18.920]   business. We all did it because we all felt a part of the
[00:11:18.920 --> 00:11:22.520]   community and the community feeling that we learn together
[00:11:22.520 --> 00:11:26.120]   had a problem we sent to each other, and things like that. And
[00:11:26.120 --> 00:11:33.000]   so I don't think Kaggle is just for machine learning. I think
[00:11:33.000 --> 00:11:37.760]   it's much beyond it. Of course, there are a number of problems
[00:11:37.760 --> 00:11:44.680]   that do not suit. Generative problems are not easy to measure
[00:11:44.680 --> 00:11:48.360]   and things like that. It's not easy. Like you cannot conduct a
[00:11:48.360 --> 00:11:51.840]   competition easily and things like that. So there could be
[00:11:51.840 --> 00:11:54.360]   some shortcomings. And of course, we definitely don't have
[00:11:54.360 --> 00:11:59.760]   50 models in production. We just have one model in production in
[00:11:59.760 --> 00:12:02.120]   the industry. And even that model, maintaining it in
[00:12:02.120 --> 00:12:06.000]   production is very difficult. It's very different. But I
[00:12:06.000 --> 00:12:09.600]   think it's complimentary. I don't think we should compare
[00:12:09.600 --> 00:12:12.760]   either of them. I genuinely think it's complimentary. We
[00:12:12.760 --> 00:12:17.360]   need both. And we and Shahul is in the best place. He works in
[00:12:17.360 --> 00:12:20.640]   industry and still he's agile and active. Sorry, active in
[00:12:20.640 --> 00:12:26.080]   Kaggle. So I think that's the best approach that in my
[00:12:26.080 --> 00:12:27.040]   personal opinion.
[00:12:29.080 --> 00:12:32.120]   I've not Kaggled as much and neither have I put as many
[00:12:32.120 --> 00:12:36.160]   models into production. So I really like how you said it's
[00:12:36.160 --> 00:12:38.840]   really complimentary. But Shahul, I want to learn about
[00:12:38.840 --> 00:12:41.240]   your journey. How did you discover data science your first
[00:12:41.240 --> 00:12:44.120]   competition? I think you explored a bit you tried an
[00:12:44.120 --> 00:12:47.640]   image one, you got your first medal, I think in the ASHRAE
[00:12:47.640 --> 00:12:51.200]   competition. How did you start your Kaggle journey?
[00:12:51.200 --> 00:12:55.720]   Yeah, first of all, I envy Vignesh because I don't have
[00:12:55.720 --> 00:13:02.200]   such a wonderful story to begin with. I randomly landed in
[00:13:02.200 --> 00:13:06.520]   Kaggle during a hackathon searching for a data set. That's
[00:13:06.520 --> 00:13:10.560]   how when I saw Kaggle for the first time in my life. And then
[00:13:10.560 --> 00:13:14.520]   for some reason or another, I went to Kaggle, you know, I
[00:13:14.520 --> 00:13:16.960]   understood that there is a community like this, there is a
[00:13:16.960 --> 00:13:20.960]   huge, you know, a huge branch like data science like this. So
[00:13:20.960 --> 00:13:24.760]   I just wanted to start learning and understanding what is it and
[00:13:25.000 --> 00:13:29.800]   if I want to understand if that's suit my career or not. And
[00:13:29.800 --> 00:13:34.120]   you know, I started back in 2018 when I was doing my undergrad
[00:13:34.120 --> 00:13:38.480]   degree degree in computer science and engineering. I found
[00:13:38.480 --> 00:13:43.080]   so many great people in Kaggle, including the former Kaggle rank
[00:13:43.080 --> 00:13:50.120]   number one, Andrei Lukunin from Russia. So you know, he I
[00:13:50.120 --> 00:13:54.000]   started simply chatting with him and he actually mentored me a
[00:13:54.000 --> 00:13:59.520]   lot. And that helped me to, you know, encouraged me to start
[00:13:59.520 --> 00:14:03.040]   writing kernels. That's why I started writing kernels in
[00:14:03.040 --> 00:14:05.960]   Kaggle. And you know, side by side, I used to learn the
[00:14:05.960 --> 00:14:08.600]   theory and you know, in the other side, I used to do the
[00:14:08.600 --> 00:14:12.520]   practicals in Kaggle. That's how I started my journey in data
[00:14:12.520 --> 00:14:18.120]   science. Yeah, so, right. So currently working in fintech
[00:14:18.120 --> 00:14:21.520]   industry, I am very interested in the application of data
[00:14:21.520 --> 00:14:25.360]   science in fintech industry. And you know, with this competition,
[00:14:25.360 --> 00:14:28.920]   I was actually aiming for a comeback to Kaggle since I was
[00:14:28.920 --> 00:14:32.200]   inactive in Kaggle since the start of the year. So I wanted
[00:14:32.200 --> 00:14:36.240]   to really come back to Kaggle and the overall contributions
[00:14:36.240 --> 00:14:40.120]   that I did to this competition. I think I'm back on track with
[00:14:40.120 --> 00:14:43.000]   my in my old state. So yeah, so I think
[00:14:43.000 --> 00:14:43.280]   Sanyam Bhutani
[00:14:43.280 --> 00:14:48.200]   That's quite the comeback. You just rejoin and you're on the
[00:14:48.200 --> 00:14:53.240]   40th position just like that. Yeah, that's incredible. How was
[00:14:53.240 --> 00:14:56.400]   your journey at that time? I remember I, I was in university
[00:14:56.400 --> 00:14:58.920]   as well. When I signed up for Kaggle, I was just frustrated
[00:14:58.920 --> 00:15:01.200]   with the courses being absolutely meaningless and
[00:15:01.200 --> 00:15:05.440]   nothing making sense. Maybe people had better experiences. I
[00:15:05.440 --> 00:15:09.200]   don't hate my college. I just hated the syllabus. How was your
[00:15:09.200 --> 00:15:13.840]   experience as you were learning these things? I remember you
[00:15:13.840 --> 00:15:17.480]   finally shifted towards NLP competitions. How was that
[00:15:17.480 --> 00:15:17.920]   learning?
[00:15:18.520 --> 00:15:23.480]   Yes, so I had such a wonderful friends community around me
[00:15:23.480 --> 00:15:29.120]   that all of them had something done extra out of their
[00:15:29.120 --> 00:15:33.240]   curriculum. So none of us was focusing on the curriculum. And
[00:15:33.240 --> 00:15:35.640]   all of us was, you know, focusing on something outside
[00:15:35.640 --> 00:15:38.320]   the curriculum either, you know, trying to build something of
[00:15:38.320 --> 00:15:41.840]   our own or learn some some special skill set, skill sets.
[00:15:41.840 --> 00:15:46.560]   So I was in this friends community and there were I also
[00:15:46.560 --> 00:15:49.840]   wanted to learn something and I also wanted to learn something
[00:15:49.840 --> 00:15:53.840]   out of the curriculum that everyone does. So I want to do
[00:15:53.840 --> 00:15:57.720]   something extra. So that's where I found Kaggle and that's where
[00:15:57.720 --> 00:16:02.560]   I found my love to this data science. Initially started with
[00:16:02.560 --> 00:16:07.960]   data analytics and I only used to do EDA then, then slowly
[00:16:07.960 --> 00:16:11.680]   moved to machine learning and started learning concepts of
[00:16:11.680 --> 00:16:15.920]   algorithm and machine learning. Eventually I reached deep
[00:16:15.920 --> 00:16:18.680]   learning and naturally currently, you know, doing
[00:16:18.680 --> 00:16:22.720]   natural language processing. So that's how it goes. And you
[00:16:22.720 --> 00:16:25.200]   know, I'm currently I'm very interested in machine learning
[00:16:25.200 --> 00:16:30.040]   operations as well. Because the industry which I work in I
[00:16:30.040 --> 00:16:34.400]   deploy models in production. So yeah, that's where Yeah, I also
[00:16:34.400 --> 00:16:37.320]   like the industry and support but yeah, it's not it's not in
[00:16:37.320 --> 00:16:40.400]   Kaggle but in the as in the world in data science.
[00:16:40.400 --> 00:16:45.040]   Awesome. Yeah, so my next question, I think to both of you
[00:16:45.040 --> 00:16:49.680]   would be both of you have learned on and off Kaggle. How
[00:16:49.680 --> 00:16:51.880]   would you suggest someone should start their data science
[00:16:51.880 --> 00:16:55.160]   journey? Personally, I was also like very intimidated because I
[00:16:55.160 --> 00:16:59.120]   still am maybe I'm just scared of going to a leaderboard. I
[00:16:59.120 --> 00:17:02.520]   always feel like I don't know enough to compete in this
[00:17:02.520 --> 00:17:07.080]   competition. So any any thoughts on that topic? Maybe you can
[00:17:07.080 --> 00:17:07.960]   start with Vignesh.
[00:17:09.120 --> 00:17:21.240]   Okay. I think the imposter syndrome is a serious issue. And
[00:17:21.240 --> 00:17:28.080]   I think all of us fight with it. I mean, the time of superiority
[00:17:28.080 --> 00:17:32.400]   complex is gone. Now all of us are suffering from imposter
[00:17:32.400 --> 00:17:42.800]   syndrome. Yeah. I will tell you honestly, Sanyam. Because I was
[00:17:42.800 --> 00:17:47.320]   in the industry and because I'm older, much older than both of
[00:17:47.320 --> 00:17:54.800]   you. I learned something. It doesn't matter the position in
[00:17:54.800 --> 00:17:58.920]   the leaderboard or the ranks or anything doesn't matter. I'm
[00:17:58.920 --> 00:18:05.480]   sure I really respect many people in like the last four
[00:18:05.480 --> 00:18:09.400]   months of Kaggle. I met so many people like not in person like
[00:18:09.400 --> 00:18:15.480]   virtually and I came to know about Rohit Singh, Mounesh,
[00:18:15.480 --> 00:18:24.440]   J.C. Skewell and then the professor who works with NVIDIA
[00:18:24.440 --> 00:18:36.840]   now. Chris Deuth. All these people, they never care about
[00:18:36.840 --> 00:18:40.000]   which position they are in. They're all in some of the
[00:18:40.000 --> 00:18:41.800]   competitions, they're at the top, some of the competition
[00:18:41.800 --> 00:18:44.920]   they're at the bottom, it doesn't matter. But at the end
[00:18:44.920 --> 00:18:51.040]   of the day, what all these people contribute to them is
[00:18:51.040 --> 00:18:54.560]   fantastic. I was always thinking before I joined Kaggle, of
[00:18:54.560 --> 00:18:58.360]   course, I was thinking that the competition's grandmaster is the
[00:18:58.360 --> 00:19:02.480]   biggest thing that one could ever achieve or something like
[00:19:02.480 --> 00:19:10.280]   that. But now I genuinely think it's all about sharing kernels
[00:19:10.280 --> 00:19:17.680]   and discussion that is equally important. I would probably go
[00:19:17.680 --> 00:19:21.200]   to the other extreme and say more important than being a
[00:19:21.200 --> 00:19:24.840]   competition grandmaster or something like that's an extreme
[00:19:24.840 --> 00:19:25.760]   view. I agree.
[00:19:25.760 --> 00:19:28.880]   You're telling me my whole life was a lie. I always thought
[00:19:28.880 --> 00:19:31.720]   competition grandmaster was the biggest thing. I'm just I'm
[00:19:31.720 --> 00:19:32.280]   just joking.
[00:19:32.280 --> 00:19:37.680]   No, but I mean, because at the end of the day, it's not about
[00:19:39.160 --> 00:19:44.400]   which position you land in. It's all about which position you
[00:19:44.400 --> 00:19:51.320]   help us land in. We don't, there are 200 and some competition
[00:19:51.320 --> 00:19:53.640]   grandmasters, we don't speak about them today. But we're
[00:19:53.640 --> 00:19:58.960]   speaking about Rohit, we're speaking about Mish, and other
[00:19:58.960 --> 00:20:03.520]   people, although they have never been grandmasters, I think Rohit
[00:20:03.520 --> 00:20:08.320]   is a grandmaster now, right? I think Rohit is a grandmaster
[00:20:08.320 --> 00:20:15.800]   now. So that's what I really think the leaderboard is the
[00:20:15.800 --> 00:20:17.480]   last thing that we should ever look at.
[00:20:17.480 --> 00:20:23.360]   Yeah, that's, that's great advice. I'm still, I think
[00:20:23.360 --> 00:20:26.600]   everyone does this fight in process. I'm still in the phase
[00:20:26.600 --> 00:20:31.680]   of it being really heavy. But what what resources did you find
[00:20:31.680 --> 00:20:34.960]   useful as you went about learning in your journey?
[00:20:35.880 --> 00:20:37.880]   Okay, Sanyam.
[00:20:37.880 --> 00:20:43.120]   Sure. What resources did you find useful as you were learning
[00:20:43.120 --> 00:20:46.320]   these things? And what do you recommend to someone else?
[00:20:46.320 --> 00:20:53.600]   I would, I would first recommend everyone to be active on the
[00:20:53.600 --> 00:20:58.560]   discussion forum. You have any question, you research. I mean,
[00:20:58.560 --> 00:21:01.400]   don't go and post it on discussion forum right away,
[00:21:01.440 --> 00:21:06.200]   research a bit. And if you still do not manage to find a solution,
[00:21:06.200 --> 00:21:11.520]   then post it on the discussion forum, along with the research
[00:21:11.520 --> 00:21:14.040]   that you did and tell them like I did this research, and this is
[00:21:14.040 --> 00:21:16.480]   what I figured out from it, but still, I'm not able to figure
[00:21:16.480 --> 00:21:22.120]   out the solution. And a lot of conversation happens on it. And
[00:21:22.120 --> 00:21:25.920]   people really appreciate when you do a bit of research and
[00:21:25.920 --> 00:21:28.960]   then post it on discussion forum instead of not even doing a
[00:21:28.960 --> 00:21:32.840]   Google search. And that's something like that. That's one
[00:21:32.840 --> 00:21:37.280]   thing that I learned, or if you struggled with something, and
[00:21:37.280 --> 00:21:39.960]   you found the solution, still go ahead and post it on discussion
[00:21:39.960 --> 00:21:43.240]   forum, because it could be helpful to someone else. So I
[00:21:43.240 --> 00:21:47.720]   think discussion forum is one of the most important things that
[00:21:47.720 --> 00:21:56.800]   really helps us. I also think watching the talks from Kaggle
[00:21:56.840 --> 00:22:02.360]   community people, many people post talks, Chaitan is one of
[00:22:02.360 --> 00:22:09.880]   the examples of that. I also know Abhishek hosts some talks
[00:22:09.880 --> 00:22:13.160]   with that. I think when you look at them, and when you start,
[00:22:13.160 --> 00:22:16.200]   when you listen to them, you also understand that, okay,
[00:22:16.200 --> 00:22:19.720]   they're also normal people like us. We can probably they are
[00:22:19.720 --> 00:22:23.160]   approachable or something like that. And one of the most, I
[00:22:23.160 --> 00:22:27.480]   genuinely think, and one of the most respectable communities is
[00:22:27.480 --> 00:22:32.960]   Kaggle community people don't ship post and people don't. You
[00:22:32.960 --> 00:22:38.040]   see people are not arrogant, people try to help very much. So
[00:22:38.040 --> 00:22:42.480]   I think, for me, I would definitely say do your homework
[00:22:42.480 --> 00:22:46.600]   and go to the discussion forum and slowly get started. I have
[00:22:46.600 --> 00:22:49.240]   never done any playground competition because I was
[00:22:49.240 --> 00:22:52.840]   already a data scientist for six years. So I so I don't know
[00:22:52.840 --> 00:22:56.320]   it. So I straight away went to common let. On the other hand,
[00:22:56.320 --> 00:23:00.720]   it depends on which competition you choose. Use the discussion
[00:23:00.720 --> 00:23:03.320]   forum and don't be shy. It's okay. All of us don't know
[00:23:03.320 --> 00:23:04.200]   certain things.
[00:23:04.200 --> 00:23:09.680]   Yeah, it's people don't realize like whenever we as I went
[00:23:09.680 --> 00:23:12.840]   through your write up, this is always something that happens in
[00:23:12.840 --> 00:23:16.400]   every writer will thanking each other. We Kagglers are thanking
[00:23:16.400 --> 00:23:19.040]   each other because everyone has shared their solutions and we
[00:23:19.040 --> 00:23:21.480]   just like building on top of these building blocks that
[00:23:21.480 --> 00:23:24.240]   someone has shared and then further people said it
[00:23:24.240 --> 00:23:27.560]   furthermore. And it's mind blowing at first time you
[00:23:27.560 --> 00:23:28.640]   realize what's going on.
[00:23:28.640 --> 00:23:30.280]   Yeah, yeah.
[00:23:30.280 --> 00:23:36.080]   Awesome. So same question to you. How did you go about
[00:23:36.080 --> 00:23:39.520]   learning to becoming one of the top ranked Kagglers?
[00:23:39.520 --> 00:23:41.240]   What what is so
[00:23:41.240 --> 00:23:47.480]   Yes, I mean, I second award. It's not about it's not about,
[00:23:47.560 --> 00:23:51.440]   you know, getting medals, or winning competitions. It's about
[00:23:51.440 --> 00:23:54.360]   the whole environment that Kaggle provides for the
[00:23:54.360 --> 00:23:58.440]   development for data science and data science enthusiasts. So,
[00:23:58.440 --> 00:24:02.360]   you know, be active in communities, get in touch with
[00:24:02.360 --> 00:24:06.920]   people, talk to new people, you have, you know, 100 ways to get
[00:24:06.920 --> 00:24:11.200]   in touch with someone if you want to do it. You know, and,
[00:24:11.200 --> 00:24:15.240]   and always start, I would say, you know, do your research, go
[00:24:15.240 --> 00:24:18.560]   to discussion forums, try to publish a notebook, don't, you
[00:24:18.560 --> 00:24:21.240]   know, get upset if you if you're not good at this, not
[00:24:21.240 --> 00:24:25.120]   attract traction. Because if you if you look at the initial
[00:24:25.120 --> 00:24:27.680]   notebooks of some of the Kaggle Grandmasters, you will see the
[00:24:27.680 --> 00:24:31.600]   same. So it's it's about, you know, getting that review from
[00:24:31.600 --> 00:24:35.640]   people around in the Kaggle environment, and understanding
[00:24:35.640 --> 00:24:38.360]   that review and taking that review and, you know, putting it
[00:24:38.360 --> 00:24:42.880]   to development again. So I believe what the whole whole,
[00:24:43.000 --> 00:24:45.280]   you know, infrastructure that the whole environment that
[00:24:45.280 --> 00:24:49.960]   Kaggle provides is awesome. And to start with data science, it's
[00:24:49.960 --> 00:24:53.560]   a really good platform with, you know, the recent ML courses and
[00:24:53.560 --> 00:24:57.120]   all that Kaggle itself provides, which was not available back
[00:24:57.120 --> 00:25:02.520]   back when I started. So all these resources help, you know,
[00:25:02.520 --> 00:25:04.920]   enthusiasts start with the community.
[00:25:04.920 --> 00:25:11.000]   Yeah, that's that's awesome advice. Did you? I know, we both
[00:25:11.000 --> 00:25:13.440]   have discussed this on the forums, and we both are upset
[00:25:13.440 --> 00:25:16.520]   that some people tend to overshare they like, it's called
[00:25:16.520 --> 00:25:19.440]   upward begging. That's what it is literally. But you've you've
[00:25:19.440 --> 00:25:23.840]   done this really nice job of sharing what you've learned. So
[00:25:23.840 --> 00:25:28.120]   have you found found it useful your Kaggle progress in your
[00:25:28.120 --> 00:25:30.280]   career? Also as an attractive point?
[00:25:30.280 --> 00:25:34.400]   Yes, of course, this this is this was a very attractive point
[00:25:34.400 --> 00:25:38.480]   in my career till now, because I haven't applied for any jobs
[00:25:38.480 --> 00:25:41.400]   till now, because all the I mean, all the jobs that I did,
[00:25:41.400 --> 00:25:44.440]   even the freelance works and everything that has come to me.
[00:25:44.440 --> 00:25:50.040]   So it's, I think it's mainly due to Kaggle. And of course, I have
[00:25:50.040 --> 00:25:54.040]   some contributions and work in blogs in open source as well.
[00:25:54.040 --> 00:25:58.400]   But I think the main attractive point for me and my profile is
[00:25:58.400 --> 00:26:03.640]   Kaggle. And also the, of course, the network that I built through
[00:26:03.640 --> 00:26:07.120]   Kaggle has been very useful for me in my career.
[00:26:08.120 --> 00:26:11.840]   That's, yeah, it's, it's difficult to explain how much
[00:26:11.840 --> 00:26:13.600]   Kaggle can change your life. That's
[00:26:13.600 --> 00:26:17.480]   I've, it's very difficult to explain. You know, when I
[00:26:17.480 --> 00:26:21.880]   started back in, you know, 2018, I never had this, you know, I
[00:26:21.880 --> 00:26:26.080]   never thought I would end up being interviewed by Sunil. So
[00:26:26.080 --> 00:26:30.920]   this, you know, it's the perseverance that, you know,
[00:26:30.920 --> 00:26:31.720]   changes everything.
[00:26:31.720 --> 00:26:36.800]   You being too kind. I, I asked my first few questions to you on
[00:26:36.880 --> 00:26:40.920]   Kaggle forum, like I said, I never thought I'd get the chance
[00:26:40.920 --> 00:26:46.400]   to interact with you. I remember I saw the Aladdin icon and orange
[00:26:46.400 --> 00:26:49.040]   ring to it. And I was just thinking, Oh my god, just just
[00:26:49.040 --> 00:26:51.440]   some master has replied to me. That's incredible.
[00:26:51.440 --> 00:26:55.800]   I was a student back then.
[00:26:55.800 --> 00:27:04.200]   And you were already on the top of the leaderboard. So that, I
[00:27:04.200 --> 00:27:08.080]   think was, that was the end of the q&a. What I'd like to shift
[00:27:08.080 --> 00:27:11.280]   towards. And there are also general questions that I asked
[00:27:11.280 --> 00:27:14.440]   around this, because I like to understand how did you approach
[00:27:14.440 --> 00:27:17.200]   the competition, what was going on in your mind. So I'll just
[00:27:17.200 --> 00:27:21.880]   start by introducing the problem statement and the data set by
[00:27:21.880 --> 00:27:25.720]   shamelessly using dashboards that a few of our ambassadors
[00:27:25.720 --> 00:27:29.080]   have been. And please feel free to interrupt me anytime if I'm
[00:27:29.080 --> 00:27:38.760]   myself. So, we've already had Anjum said I did introduce the
[00:27:38.760 --> 00:27:42.080]   problem statement there, but I'll quickly go through it. The
[00:27:42.080 --> 00:27:47.480]   challenge as I understood was to help this institute called
[00:27:47.480 --> 00:27:52.640]   common lit understand and create different essays for kids in
[00:27:52.640 --> 00:27:57.080]   grades three to 12. They use some weird metric that I couldn't
[00:27:57.080 --> 00:27:59.920]   understand at all. Apparently, it doesn't make sense to use it.
[00:27:59.920 --> 00:28:03.240]   So that's why they want to use machine learning to really
[00:28:03.240 --> 00:28:06.440]   optimize what they're teaching the kids. So I think that was
[00:28:06.440 --> 00:28:11.120]   the general problem statement here. And since we are
[00:28:11.120 --> 00:28:15.520]   predicting this number, the evaluation metric becomes an
[00:28:15.520 --> 00:28:17.960]   RMSE value, I was thinking it might be Jacquard index, it
[00:28:17.960 --> 00:28:21.040]   might be something else. But because of that, it took me a
[00:28:21.040 --> 00:28:28.480]   second to figure out what it was RMSE. One thing I like to do
[00:28:28.480 --> 00:28:31.760]   here is I'm quite lazy with the EDM not as good as how so I
[00:28:31.760 --> 00:28:35.160]   just Kaggle recently introduced this, I just click on compact
[00:28:35.160 --> 00:28:38.280]   and that shows me everything around the column. So I usually
[00:28:38.280 --> 00:28:43.320]   start there and I'll use that as a cheap way of introducing the
[00:28:43.320 --> 00:28:48.720]   data set, but it had a few, I it had IDs for every single
[00:28:48.720 --> 00:28:52.600]   sentence or paragraph. For some reason, they were also URLs of
[00:28:52.600 --> 00:28:55.760]   Wikipedia, I don't understand why you would eat someone from
[00:28:55.760 --> 00:28:58.880]   Wikipedia in grade three, grade five, those are like really
[00:28:58.880 --> 00:29:03.520]   complex text. There were some licenses, not everyone was
[00:29:03.520 --> 00:29:08.560]   populated with the license. Finally, you had the excerpt and
[00:29:08.560 --> 00:29:14.920]   the target and I'm going to see the name of this column. The
[00:29:14.920 --> 00:29:19.160]   standard error. I'm sorry, I don't understand what the
[00:29:19.160 --> 00:29:21.240]   standard error was. Maybe you can elaborate on this.
[00:29:21.240 --> 00:29:23.240]   Yeah.
[00:29:23.240 --> 00:29:27.800]   So I'm just trying to just
[00:29:27.800 --> 00:29:30.200]   Yeah, you want to go ahead.
[00:29:30.200 --> 00:29:34.440]   I was saying eventually when the competition started, none of us
[00:29:34.440 --> 00:29:36.840]   understand understood what was the standard error.
[00:29:43.160 --> 00:29:46.160]   But what did it stand for? And what's what's being shown here?
[00:29:46.160 --> 00:29:51.560]   So they have calculated this target using Bradley theory
[00:29:51.560 --> 00:29:56.600]   method. And when calculating the when all these scores are
[00:29:56.600 --> 00:30:01.760]   generated from from a set of labeled data, which are which
[00:30:01.760 --> 00:30:06.800]   are labeled by a set of users. So after the set of users do
[00:30:06.800 --> 00:30:10.880]   not label the data as a scope, they label the data in
[00:30:10.880 --> 00:30:13.040]   comparison with another sentence. So there is sentence
[00:30:13.040 --> 00:30:17.000]   A, and then there is sentence B, and the label sits and see if
[00:30:17.000 --> 00:30:20.440]   the sentence A is easier than easier to read than sentence B.
[00:30:20.440 --> 00:30:24.320]   So it was it was made as a binary label. And then to
[00:30:24.320 --> 00:30:28.560]   convert the binary label to a regression target, they have
[00:30:28.560 --> 00:30:32.280]   used Bradley theory and while using Bradley theory, you get a
[00:30:32.280 --> 00:30:36.200]   standard error. Yeah, that's the that's the standard error.
[00:30:36.200 --> 00:30:39.680]   In our solution walkthrough, we explain it.
[00:30:40.680 --> 00:30:42.800]   Awesome. I'll just come to that afterwards.
[00:30:42.800 --> 00:30:49.160]   So what I'm going to do next is I'm going to use a dashboard. I
[00:30:49.160 --> 00:30:53.200]   think this was built by Ruchi, who's a calendar master and also
[00:30:53.200 --> 00:30:57.040]   one of our ambassadors. It's a nice dashboard. So that's why
[00:30:57.040 --> 00:31:01.640]   I'm using it. It makes my laziness reach its epitome. But
[00:31:01.640 --> 00:31:04.440]   I'll quickly point out different things I learned. She also
[00:31:04.440 --> 00:31:08.760]   created trigrams, bigrams and unigrams of different words.
[00:31:08.880 --> 00:31:15.040]   Um, maybe the kids are being taught like zoom in a bit. Maybe
[00:31:15.040 --> 00:31:17.920]   the kids are being taught about environment. So it's great to
[00:31:17.920 --> 00:31:21.320]   see carbon dioxide, similar words. They're learning about
[00:31:21.320 --> 00:31:25.520]   New York. I don't know from what country this was this. So if
[00:31:25.520 --> 00:31:27.360]   it's from India, they're learning about overseas
[00:31:27.360 --> 00:31:32.160]   countries. Unigrams didn't have any interesting words. I think
[00:31:32.160 --> 00:31:39.640]   these are just simple words that I saw. The average word length
[00:31:39.640 --> 00:31:44.760]   or distribution, as I see was quite similar. So not, not many
[00:31:44.760 --> 00:31:48.920]   different or weird sentences that would stand out. Most of
[00:31:48.920 --> 00:31:53.080]   the licenses, at least the one that were annotated were CC by
[00:31:53.080 --> 00:31:56.440]   four is the most open license as I understand it after MIT. So
[00:31:57.280 --> 00:32:03.440]   it's really free to use. I'm just trying to set the stage
[00:32:03.440 --> 00:32:06.240]   here for different things. I will just go through them. So
[00:32:06.240 --> 00:32:08.880]   I'm trying to point out different things that sound
[00:32:08.880 --> 00:32:10.960]   interesting to me. So please feel free to point out anything
[00:32:10.960 --> 00:32:17.160]   else. Yeah, I think I just wanted to point these out that
[00:32:17.160 --> 00:32:19.840]   the sentences aren't too different. The average foot
[00:32:19.840 --> 00:32:23.160]   length is quite similar. And the character count distribution
[00:32:23.160 --> 00:32:27.720]   isn't also too weird. It follows a standard bell shaped curve, if
[00:32:27.720 --> 00:32:34.160]   I may. Do you have anything to add for the EDA bit or anything
[00:32:34.160 --> 00:32:35.640]   else that stood out in the data?
[00:32:35.640 --> 00:32:46.840]   Okay, awesome. Let's, let's move on. So I'll probably pass this
[00:32:46.840 --> 00:32:50.480]   back to you. Now. I'd love to understand I as I understood you
[00:32:50.480 --> 00:32:53.640]   both started separately in this competition. So maybe Vignesh,
[00:32:53.640 --> 00:32:56.240]   how did you first when you saw the problem? What what came to
[00:32:56.240 --> 00:32:59.160]   mind? How did you approach the problem? How did you spend your
[00:32:59.160 --> 00:32:59.520]   time?
[00:32:59.520 --> 00:33:10.880]   Okay, so yes, and I'm so I did not know what Shahul was humans
[00:33:10.880 --> 00:33:15.840]   of the competition. So yeah. So my, my, I did not know about
[00:33:15.840 --> 00:33:19.080]   transformers. I mean, I knew about transformers, but I had
[00:33:19.080 --> 00:33:23.480]   never used one until then at all, because I was deep into
[00:33:23.480 --> 00:33:30.200]   image recognition stuff. So the first three years of my work, I
[00:33:30.200 --> 00:33:35.360]   built a lot of NLP models. And we use them internally for cost
[00:33:35.360 --> 00:33:39.720]   cutting, but we never sold it outside. Whereas and then we
[00:33:39.720 --> 00:33:44.640]   again, the last the fourth year of my work, again, we I prepared
[00:33:44.640 --> 00:33:49.080]   an image recognition system. It was again for internal work. But
[00:33:49.080 --> 00:33:53.480]   what happened was, some people outside work came to know that
[00:33:53.480 --> 00:33:56.480]   we are using such a system and they wanted to use it as well.
[00:33:56.480 --> 00:33:59.800]   And then it became a big hit. Everyone in the market wanted it
[00:33:59.800 --> 00:34:04.080]   and when so I was completely lost in this image world. So I
[00:34:04.080 --> 00:34:07.760]   had no idea about transformers at all. So the state of the art
[00:34:07.760 --> 00:34:12.720]   that I knew four months ago was a recurrent neural network. For
[00:34:12.720 --> 00:34:15.600]   text was like, okay, I came and then people are all using
[00:34:15.600 --> 00:34:18.040]   hugging face transformers. I did not know what is this hugging
[00:34:18.040 --> 00:34:22.440]   face was really weird. Then I decided, okay, I'm not gonna get
[00:34:22.440 --> 00:34:27.840]   into this wave. I cannot do it. So I'm gonna do what I know to
[00:34:27.840 --> 00:34:32.000]   do. So I started with scikit learn. I started building simple
[00:34:32.000 --> 00:34:37.600]   random forest, and then linear regressor. And then an SVM
[00:34:37.600 --> 00:34:42.360]   regressor and things like that I slowly started adding it up. So
[00:34:42.400 --> 00:34:46.240]   it was I was all working with bag of words. And then from bag of
[00:34:46.240 --> 00:34:50.200]   words, once I reached the peak, and I knew I could not improve
[00:34:50.200 --> 00:34:56.440]   anymore, then I took the spacey vectors. So I take the sentence
[00:34:56.440 --> 00:35:01.760]   document, then it was spacey, get back a vector corresponding
[00:35:01.760 --> 00:35:04.640]   to the spacey vector. And so spacey, I used it as a black
[00:35:04.640 --> 00:35:09.000]   box, I don't care about how spacey generated a vector for
[00:35:09.240 --> 00:35:12.720]   each document, I did not care about it. So and I was working
[00:35:12.720 --> 00:35:18.000]   on top of that. And once I hit a certain ceiling, then I removed
[00:35:18.000 --> 00:35:20.480]   the spacey out of it. And then I replaced it with the hugging
[00:35:20.480 --> 00:35:24.520]   face transformer. I used a pre trained transformer as a feature
[00:35:24.520 --> 00:35:29.160]   extractor. So the same way as how I used spacey I did the same
[00:35:29.160 --> 00:35:35.400]   but with spacey I jumped up way ahead when I replaced spacey
[00:35:35.400 --> 00:35:38.800]   with the hugging face transformer and that was the
[00:35:38.800 --> 00:35:43.080]   thing. So and then I realized, okay, I have everything in the
[00:35:43.080 --> 00:35:46.240]   pipeline, I have a frozen transformer, which I use as a
[00:35:46.240 --> 00:35:50.680]   feature extractor. And on top of it, I have an SVM regressor. And
[00:35:50.680 --> 00:35:54.560]   it is giving me a target score and I have I'm optimizing it for
[00:35:54.560 --> 00:35:58.840]   RMSE. Once I had all these in the pipeline, and then I thought
[00:35:58.840 --> 00:36:03.320]   why not train the transformer. So I posted. So every notebook
[00:36:03.320 --> 00:36:06.000]   that I develop, I posted it on Kaggle. But of course, it
[00:36:06.000 --> 00:36:09.680]   doesn't have much traction because who in the world wants
[00:36:09.680 --> 00:36:16.720]   to use an SVM when you have a transformer. So some some
[00:36:16.720 --> 00:36:20.000]   grandmaster guy, he came and he commented that you have a really
[00:36:20.000 --> 00:36:23.560]   nice code, but maybe you should try to train the transformer was
[00:36:23.560 --> 00:36:26.520]   like, okay, now maybe training the transformer is going to be
[00:36:26.520 --> 00:36:33.280]   interesting. So I trained the transformer. And then I, I,
[00:36:33.280 --> 00:36:36.720]   again, I went way up in the leaderboard after training the
[00:36:36.720 --> 00:36:46.320]   transformer. And the next time, so I that's when I came across,
[00:36:46.320 --> 00:36:50.760]   I was very happy that I climbed up. And so I had an error rate
[00:36:50.760 --> 00:36:57.240]   of 0.53. Okay. And then I saw Mounish had done a very similar
[00:36:57.240 --> 00:37:01.680]   notebook, but he had an error rate of 0.48. And you know, in
[00:37:01.680 --> 00:37:07.560]   these competitions, even 0.0001 matters a lot. And now I was, I
[00:37:07.560 --> 00:37:13.200]   was like, how, how the hell can Mounish, who has the same
[00:37:13.200 --> 00:37:16.800]   network like me, and he does the same stuff like me scores so
[00:37:16.800 --> 00:37:20.440]   much higher. So what I did was for one week, I went crazy, I
[00:37:20.440 --> 00:37:25.600]   took Mounish notebook, I re implemented it myself. I took
[00:37:25.600 --> 00:37:29.040]   my notebook re implemented it again by myself. And then I
[00:37:29.040 --> 00:37:35.040]   compared everything. And then I found the trick, there was just
[00:37:35.040 --> 00:37:39.360]   one single trick that Mounish was using, with which he went
[00:37:39.360 --> 00:37:42.560]   way ahead. I used the same trick. So it was one single
[00:37:42.560 --> 00:37:46.520]   line as well. So I used that one single line and I went up.
[00:37:46.520 --> 00:37:47.960]   What was the trick? What was the line?
[00:37:47.960 --> 00:37:53.920]   So he was validating every 10 iterations instead of after
[00:37:53.920 --> 00:37:58.320]   every, every epoch. This is actually something very, very
[00:37:58.320 --> 00:38:03.960]   weird to me, coming from an image recognition world. So what
[00:38:03.960 --> 00:38:06.880]   we usually do is we have a trained data loader, we have a
[00:38:06.880 --> 00:38:11.640]   valid data loader, we train the model for one epoch, again, we
[00:38:11.640 --> 00:38:14.960]   validate it for that epoch, and then see how the performance of
[00:38:14.960 --> 00:38:18.520]   the model is and if it's best we save it. And then we go to the
[00:38:18.520 --> 00:38:23.160]   we do it right. Now for each epoch, there are about 800
[00:38:23.160 --> 00:38:27.000]   iterations or something like that. No, no 200 iterations or
[00:38:27.000 --> 00:38:31.000]   something like that. I saw Mounish was evaluating every 10
[00:38:31.000 --> 00:38:35.720]   iterations. And he was choosing the best model based on every 10
[00:38:35.720 --> 00:38:40.640]   iterations. And that is the thing that he was doing. So I
[00:38:40.640 --> 00:38:44.720]   just instead of evaluating every epoch, I evaluated every 10
[00:38:44.720 --> 00:38:47.760]   iterations and I reached the same performance. Anyways,
[00:38:47.760 --> 00:38:53.320]   that's not a good thing. Because now we are it's like shooting in
[00:38:53.320 --> 00:38:59.120]   the dark. You're trying, you have 1000 apples thrown up and
[00:38:59.120 --> 00:39:01.760]   you have a random knife going and hitting one of the apples.
[00:39:01.760 --> 00:39:06.520]   That's normal, right? It's no magic. And but that is what was
[00:39:06.520 --> 00:39:09.120]   happening. But that is what people were doing. And I
[00:39:09.120 --> 00:39:12.240]   noticed it. But anyways, I used it. And then I went up the
[00:39:12.240 --> 00:39:17.040]   leaderboard. Then I met Shahid and everything changed.
[00:39:17.040 --> 00:39:22.520]   That sounds really dramatic. Shahid, what was happening
[00:39:22.520 --> 00:39:24.920]   before you met Vignesh? How did you?
[00:39:24.920 --> 00:39:30.960]   So I started the competition in the second month, I think. And I
[00:39:30.960 --> 00:39:35.000]   was trying to make a comeback, as I said. So yeah, I saw this
[00:39:35.000 --> 00:39:37.720]   guy Rohit Singh publishing so many notebooks and everything.
[00:39:37.720 --> 00:39:42.160]   So he had he had published a list of concepts, which I do not
[00:39:42.160 --> 00:39:45.760]   know, even in inside the transformers, like differential
[00:39:45.760 --> 00:39:49.720]   learning, right, a lot of stuff that he published. So, you know,
[00:39:49.720 --> 00:39:54.160]   I was searching around, learning all these concepts for the first,
[00:39:54.160 --> 00:39:57.160]   you know, three, three weeks, I spend all the time like that.
[00:39:57.160 --> 00:40:01.600]   Then I started delving into the discussion forums, where I, you
[00:40:01.600 --> 00:40:04.000]   know, where I found again, people sharing a lot of stuff.
[00:40:04.000 --> 00:40:08.960]   In between, I wrote a couple of kernels and shared them. And
[00:40:08.960 --> 00:40:12.000]   also started being active in discussion forums. Then I saw
[00:40:12.000 --> 00:40:15.840]   this guy, you know, Vignesh, sharing almost all the tricks
[00:40:15.840 --> 00:40:20.200]   here and there. So, so I was also noting him. I mean, at that
[00:40:20.200 --> 00:40:23.120]   point, I was also not in a mood to compete towards the leaderboard
[00:40:23.120 --> 00:40:25.560]   but I wanted to, you know, make that comeback, start that
[00:40:25.560 --> 00:40:29.640]   learning process again. And, you know, I then I then I thought,
[00:40:29.640 --> 00:40:33.080]   yeah, I think this guy is in the same mood. He's not, you know,
[00:40:33.080 --> 00:40:38.480]   interested in, you know, running towards the leaderboard. So I
[00:40:38.480 --> 00:40:42.040]   spinned up a conversation with him. And yeah, the rest is
[00:40:42.040 --> 00:40:46.560]   history then. So things synced up well, between us later, you
[00:40:46.560 --> 00:40:53.320]   know, we, we teamed up later, we teamed up. And yeah, we did like
[00:40:53.320 --> 00:40:57.360]   a lot of experiment, a lot of experiment. And yeah, that's how
[00:40:57.360 --> 00:40:57.960]   things went.
[00:40:57.960 --> 00:41:03.200]   So, first of all, why did you decide to make everything
[00:41:03.200 --> 00:41:06.360]   public? As you were learning by why did you want to share it?
[00:41:06.360 --> 00:41:09.320]   Why not keep it to yourself and keep the secrets to yourself?
[00:41:09.320 --> 00:41:11.280]   Okay.
[00:41:11.280 --> 00:41:19.080]   There is a very deep personal reason why I did not do that.
[00:41:19.080 --> 00:41:27.880]   It's because I, you know, I told you, right, I was working for an
[00:41:27.880 --> 00:41:31.040]   intellectual property firm and things like it was very closed
[00:41:31.040 --> 00:41:42.720]   environment. And I was like, in you know, Sanyam, in real life,
[00:41:42.720 --> 00:41:46.640]   you're the main reason why people hide stuff. And people,
[00:41:46.640 --> 00:41:50.160]   they don't share tricks or something is because they want
[00:41:50.160 --> 00:41:59.120]   to make more and more and more and more money. Yeah. And for me,
[00:41:59.160 --> 00:42:06.880]   I, after a point in time, I started suffocating. And for me,
[00:42:06.880 --> 00:42:10.320]   it was hiding stuff and doing stuff and not sharing and not
[00:42:10.320 --> 00:42:13.520]   speaking openly, you go to IP conferences, you cannot speak
[00:42:13.520 --> 00:42:16.040]   openly with anyone. And you know, the other person on your
[00:42:16.040 --> 00:42:20.880]   side is also not speaking to you transparently. Growth doesn't
[00:42:20.880 --> 00:42:26.920]   happen. And for me, I realized it very clearly. And then I came
[00:42:26.920 --> 00:42:29.920]   to Kaggle. And then I realized, I'm not gonna do the same
[00:42:29.920 --> 00:42:36.440]   mistake again. Yes, I made like, how money is over there. That's
[00:42:36.440 --> 00:42:39.920]   how here you have winning points and things like that. I'm not
[00:42:39.920 --> 00:42:44.040]   gonna do that. I'm going here. I am a part of this community. I'm
[00:42:44.040 --> 00:42:46.600]   going to embrace in this community. And I'm going to
[00:42:46.600 --> 00:42:50.600]   develop in this community. And I'm just thinking about this
[00:42:50.600 --> 00:42:55.720]   Sanyam. What would have happened if I hadn't shared my code
[00:42:55.720 --> 00:43:06.440]   publicly? I wouldn't have met Shahul. I wouldn't have. I mean,
[00:43:06.440 --> 00:43:14.400]   it's Yeah, maybe, maybe we would have ended up in different. I
[00:43:14.400 --> 00:43:16.160]   don't know, we would have ended up in different places in our
[00:43:16.160 --> 00:43:19.720]   life or something like that. But at the end of the day, I met
[00:43:19.720 --> 00:43:25.200]   Shahul and we ended up being very good friends. Just because
[00:43:25.240 --> 00:43:30.160]   I did not have this attitude of how do you call it as like,
[00:43:30.160 --> 00:43:36.280]   being opaque and being for myself being very, how do you
[00:43:36.280 --> 00:43:40.160]   call it as I don't know the right word for it, being not
[00:43:40.160 --> 00:43:43.040]   transparent or something like that. At the end of the day,
[00:43:43.040 --> 00:43:44.160]   you know, we
[00:43:44.160 --> 00:43:49.040]   may as at the end of the day, what happened was, Shahul
[00:43:49.040 --> 00:43:52.400]   learned, I learned, the community learned, everyone
[00:43:52.400 --> 00:43:58.480]   learned, everyone got benefited. And because Shahul and I were
[00:43:58.480 --> 00:44:02.760]   very transparent, even in our final solution post that we
[00:44:02.760 --> 00:44:07.880]   landed up here, mainly because of those eight to 10 people whom
[00:44:07.880 --> 00:44:11.200]   we mentioned in our, whom we mentioned the names and all the
[00:44:11.200 --> 00:44:16.480]   other 5500 participants, whose name we did not mention. That's
[00:44:16.480 --> 00:44:20.960]   the reason why we ended up there. So this success is
[00:44:20.960 --> 00:44:25.520]   because many people sharing their code, many people sharing
[00:44:25.520 --> 00:44:28.800]   the discussion and things like that. And very importantly,
[00:44:28.800 --> 00:44:35.320]   here, I would like to speak about someone. It's a Japanese
[00:44:35.320 --> 00:44:40.800]   name, I do not remember the name, but I don't even know if
[00:44:40.800 --> 00:44:46.080]   it's a man or a woman. Okay. What happened was, I'm going to
[00:44:46.600 --> 00:44:49.960]   talk, I'm going to find the name and I'm going to tell you. So
[00:44:49.960 --> 00:44:55.520]   this person, two days before the end of the competition, actually
[00:44:55.520 --> 00:45:05.600]   told me about, so I'm going to, Kurosh, I think, Shahul, do you
[00:45:05.600 --> 00:45:06.280]   remember the name?
[00:45:06.280 --> 00:45:10.120]   It's hard to remember.
[00:45:10.120 --> 00:45:12.400]   Please, please feel free to share your screen. I'm sure we
[00:45:12.400 --> 00:45:14.080]   can recognize them by the profile.
[00:45:15.360 --> 00:45:23.560]   Ah, okay. Okay. So I'm going to try to find the name. Or how
[00:45:23.560 --> 00:45:24.720]   can I share my screen?
[00:45:24.720 --> 00:45:28.240]   If you just hit share screen.
[00:45:28.240 --> 00:45:33.240]   Yeah, okay. I found it. It's an option called share screen.
[00:45:33.240 --> 00:45:37.760]   You need to select the right window as you do. And then just
[00:45:37.760 --> 00:45:38.200]   share.
[00:45:38.200 --> 00:45:42.640]   Okay, there is Chrome. I will share Chrome. Do you see my
[00:45:42.640 --> 00:45:43.120]   Chrome?
[00:45:43.120 --> 00:45:45.120]   Yes, I can see your right up.
[00:45:45.440 --> 00:45:45.680]   Okay.
[00:45:45.680 --> 00:45:55.120]   Okay, Kurupikal. Okay, Kurupikal was the person. So Kurupikal
[00:45:55.120 --> 00:45:59.560]   told, Kurupikal was already in the top five or something like
[00:45:59.560 --> 00:46:02.240]   that. And Shahul and I were around 1000 or something like
[00:46:02.240 --> 00:46:10.040]   that. And Kurupikal told me a trick. So he or she posted it on
[00:46:10.040 --> 00:46:14.080]   the forum that, look, if you use this trick, your problems could
[00:46:14.080 --> 00:46:21.400]   be solved. But what happened was, and then I, I did not show
[00:46:21.400 --> 00:46:26.480]   I tried the trick, but I couldn't use it. But at the end,
[00:46:26.480 --> 00:46:32.840]   what happened was, I found one of the old discussion posts that
[00:46:32.840 --> 00:46:39.080]   Kurupikal had posted, where he or she told about a particular
[00:46:39.080 --> 00:46:42.840]   thing that worked very well for him or her. Okay, that
[00:46:42.840 --> 00:46:47.280]   particular trick did not work for me and Shahul earlier. So
[00:46:47.280 --> 00:46:54.400]   what happened was, I saw, okay, fine. So this works for her or
[00:46:54.400 --> 00:46:57.680]   him, but it did not work for me. So what I'm going to do right
[00:46:57.680 --> 00:47:03.080]   now was, I'm going to go retry it and get it working. And I got
[00:47:03.080 --> 00:47:05.880]   it working. And that's one of the reasons why we climbed up
[00:47:06.120 --> 00:47:15.160]   way ahead in the leaderboard. So things like this, where one
[00:47:15.160 --> 00:47:20.120]   helps the other person and all of us feel happy for it is what
[00:47:20.120 --> 00:47:26.080]   is one of the main reasons why I did it that way.
[00:47:26.080 --> 00:47:30.800]   Yeah, that's, I think that's that can only be experienced as
[00:47:30.800 --> 00:47:33.640]   a join a competition and you, you learn through these
[00:47:33.640 --> 00:47:37.760]   wonderful people like Vignesh as he shares his kernels or like
[00:47:37.760 --> 00:47:41.560]   Shahul. I hope he continues writing after he becomes a
[00:47:41.560 --> 00:47:42.880]   grandmaster. But I think
[00:47:42.880 --> 00:47:51.560]   awesome. So I'd like to continue by diving into a solution. I
[00:47:51.560 --> 00:47:54.040]   just understood how did you approach the competition? Why
[00:47:54.040 --> 00:47:58.160]   did you team up? Would you like to continue walking us through
[00:47:58.160 --> 00:47:58.840]   or should I do?
[00:47:58.840 --> 00:48:03.160]   So I've my screen is frozen. I'm trying to see what I can do
[00:48:03.160 --> 00:48:13.240]   about it. I think I'm going to leave the call and try to join
[00:48:13.240 --> 00:48:13.960]   back. Is it fine?
[00:48:13.960 --> 00:48:15.800]   I can stop the sharing from here.
[00:48:15.800 --> 00:48:19.120]   Stop the sharing. Okay. Okay, perfect.
[00:48:19.120 --> 00:48:24.560]   Let me share the solution right up. This is the thing with live
[00:48:24.560 --> 00:48:27.920]   sessions, everyone gets exposed to a little behind the scenes
[00:48:27.920 --> 00:48:29.200]   as we talk about these issues.
[00:48:29.200 --> 00:48:39.520]   Okay. Maybe can you I sent you another write up from a GitHub
[00:48:39.520 --> 00:48:45.040]   post. Let me put on the tab on the right.
[00:48:45.040 --> 00:48:48.560]   Right. I have a few tabs open.
[00:48:48.560 --> 00:48:52.880]   Perfect. Okay.
[00:48:54.320 --> 00:48:58.280]   Okay, so you okay. Please, please continue. I was just
[00:48:58.280 --> 00:49:00.120]   going to do the walkthrough but please take the
[00:49:00.120 --> 00:49:06.160]   Okay, no problem. So maybe like Shahul will start I'm going to
[00:49:06.160 --> 00:49:07.120]   close the window.
[00:49:07.120 --> 00:49:10.400]   Sure.
[00:49:10.400 --> 00:49:18.040]   Quickly skim through I believe you've introduced the objective
[00:49:18.040 --> 00:49:19.200]   and the competition.
[00:49:20.200 --> 00:49:25.440]   I'll skip past and the scores I did not realize this but the
[00:49:25.440 --> 00:49:30.480]   scores range from minus three to two. I'm guessing it again
[00:49:30.480 --> 00:49:33.440]   depends on how readable it is for different. Yes.
[00:49:33.440 --> 00:49:38.520]   So, okay, so
[00:49:38.520 --> 00:49:41.720]   please continue.
[00:49:41.720 --> 00:49:43.560]   Shahul would you like to chat Shahul?
[00:49:43.560 --> 00:49:45.960]   Yes.
[00:49:47.000 --> 00:49:53.080]   Yes. Right. So, yeah, we have the competition data here and
[00:49:53.080 --> 00:49:57.200]   competition data mainly consisted of the excerpt, which
[00:49:57.200 --> 00:50:02.920]   contains a text to be rated, to be rated. And then there is a
[00:50:02.920 --> 00:50:08.080]   target. And yeah, and then there was this standard error. Yeah,
[00:50:08.080 --> 00:50:11.600]   which signifies the standard error in the Brad Teteri model.
[00:50:12.680 --> 00:50:17.120]   If you see the histogram of the scores, you can see that it's
[00:50:17.120 --> 00:50:22.360]   like a normal. It's distributed like normally. And yeah, and
[00:50:22.360 --> 00:50:26.920]   then you have different ratings for text. So the very easy text
[00:50:26.920 --> 00:50:32.120]   is rated towards the positive and you can see that you can see
[00:50:32.120 --> 00:50:37.960]   a very easy text here, which is rated almost. Yeah, almost you
[00:50:37.960 --> 00:50:43.200]   know, 1.6. And then there is a very difficult text down there,
[00:50:43.200 --> 00:50:48.400]   which is very negative. So this is the yeah, this
[00:50:48.400 --> 00:50:50.280]   Yeah, I wouldn't read this in high school.
[00:50:50.280 --> 00:50:55.680]   Astronomical distances, I would just give up.
[00:50:55.680 --> 00:51:03.200]   Sorry. So this is what the competition host wants us wants
[00:51:03.640 --> 00:51:09.720]   us to predict and wants us to build a model to predict on. And
[00:51:09.720 --> 00:51:13.680]   yes, then there is two approaches to split the letters
[00:51:13.680 --> 00:51:14.720]   and then firstly that
[00:51:14.720 --> 00:51:21.720]   I would like to add here because I am doing some name Can you
[00:51:21.720 --> 00:51:27.800]   please go to the top of the notebook? A little bit. A little
[00:51:27.800 --> 00:51:29.560]   bit about Yeah, here. Yeah, yeah.
[00:51:31.440 --> 00:51:37.520]   So I would like to briefly discuss about how did the magic
[00:51:37.520 --> 00:51:42.480]   bus came the target scores from minus three to two. How did it
[00:51:42.480 --> 00:51:47.280]   get computed? Actually, Bradley Terry, I spent a couple of days
[00:51:47.280 --> 00:51:50.560]   in trying to understand Bradley Terry and I implemented it on
[00:51:50.560 --> 00:51:56.120]   Python by myself. So I understood how the how the thing
[00:51:56.160 --> 00:52:01.320]   worked or something like that. So, Sanyam, can you please go to
[00:52:01.320 --> 00:52:08.480]   the screen? Sure. Yeah, thank you. So what happens is that
[00:52:08.480 --> 00:52:17.160]   there are about 100,000 text snippets in the competition. I'm
[00:52:17.160 --> 00:52:21.800]   sorry, no, there are about I don't know, 6000 text snippets
[00:52:22.040 --> 00:52:27.560]   in the competition data. But only 3000 around 3000 was given
[00:52:27.560 --> 00:52:32.360]   to us and the other 3000 wasn't given to us. Anyways, the 3000
[00:52:32.360 --> 00:52:38.880]   what they did is they randomly make parts of it. So for
[00:52:38.880 --> 00:52:42.000]   instance, someone gave you a single piece of text and asked
[00:52:42.000 --> 00:52:45.520]   you, can you say if this is easy to read or difficult to read?
[00:52:45.520 --> 00:52:50.800]   You cannot say it's difficult, but it's easy to read. So
[00:52:50.800 --> 00:52:54.240]   you can say it's difficult. Whereas when when someone gives
[00:52:54.240 --> 00:52:57.920]   you two pieces of text and then says, can you tell us which one
[00:52:57.920 --> 00:53:01.520]   is easier to read, then you can read both of them, compare and
[00:53:01.520 --> 00:53:05.480]   easily identify which one is easy. So what happens is out of
[00:53:05.480 --> 00:53:11.760]   this 6000 text snippets, they randomly made ads and gave it to
[00:53:11.760 --> 00:53:15.200]   multiple examiners. And they asked, can you read these two
[00:53:15.200 --> 00:53:20.120]   then say which one is difficult. So what happens is it becomes
[00:53:20.160 --> 00:53:24.560]   two binary classification problems. So that is what
[00:53:24.560 --> 00:53:29.960]   happens. Now, once they did it, there is this mathematical
[00:53:29.960 --> 00:53:33.200]   formula, which actually basically works like a logistic
[00:53:33.200 --> 00:53:38.720]   regression. So you can take a matrix of comparisons. So a
[00:53:38.720 --> 00:53:44.680]   matrix, yeah, maybe there are some names. So you can pick up
[00:53:44.680 --> 00:53:53.600]   a matrix. So where I is a particular text, and J is the
[00:53:53.600 --> 00:53:56.840]   text. You try to compute.
[00:53:56.840 --> 00:53:58.800]   Are we looking at this formula? Sorry.
[00:53:58.800 --> 00:54:04.520]   Exactly. Yeah, exactly. So what you try to figure here out is
[00:54:04.520 --> 00:54:11.480]   what is the probability that the text I will be rated more
[00:54:11.480 --> 00:54:16.600]   difficult than text J. That's what Bradley theory computes
[00:54:16.600 --> 00:54:21.840]   based on these pair of classifications which the
[00:54:21.840 --> 00:54:23.000]   examiners did.
[00:54:23.000 --> 00:54:27.480]   Right, that makes sense. So we have different examiners reading
[00:54:27.480 --> 00:54:31.520]   the text, then we take this, put it into probabilistic model that
[00:54:31.520 --> 00:54:34.040]   compares this and that's how we get these ratings. Did I
[00:54:34.040 --> 00:54:34.640]   summarize that?
[00:54:35.960 --> 00:54:43.600]   Yes. So it's like, you know, writing teams in a tournament.
[00:54:43.600 --> 00:54:49.280]   So in each game, a team plays with another team, and then
[00:54:49.280 --> 00:54:53.880]   there is a binary label if which team won that. And then at last
[00:54:53.880 --> 00:54:58.840]   if we want to assign scores to each team, we have to make this
[00:54:58.840 --> 00:55:04.240]   binary labels to some scores using some probabilistic model.
[00:55:04.560 --> 00:55:09.040]   So that model here is Bradley theory. And yeah, that's how
[00:55:09.040 --> 00:55:13.600]   they finally ended up rating each of the texts.
[00:55:13.600 --> 00:55:19.080]   Makes sense. And I'll probably come back to this, but it seems
[00:55:19.080 --> 00:55:21.960]   like you also created a library for just this.
[00:55:21.960 --> 00:55:28.320]   Right. So yeah, yeah. So for this competition, what we did
[00:55:28.320 --> 00:55:34.520]   was, we had a list of stuff to experiment on, and 90% of
[00:55:34.520 --> 00:55:37.720]   the stuff we know, I mean, we know that 90% of the stuff we
[00:55:37.720 --> 00:55:43.000]   have written down one work. So we obviously that's how Kaggle
[00:55:43.000 --> 00:55:48.520]   works. So what we did was, we created a library, a small
[00:55:48.520 --> 00:55:57.680]   library, which is very, you know, flexible. And this will
[00:55:57.680 --> 00:56:01.920]   serve as a template to us. And then we, you know, once we have
[00:56:01.920 --> 00:56:04.600]   to change something, we only have to change it in the
[00:56:04.600 --> 00:56:08.800]   configuration file. So we just change, you know, once we have
[00:56:08.800 --> 00:56:12.320]   to move from one scheduler to other, we just change it in the
[00:56:12.320 --> 00:56:16.160]   configuration file. And that's, that's all done. So we build
[00:56:16.160 --> 00:56:20.120]   this common let and then kit for for that purpose.
[00:56:20.120 --> 00:56:22.040]   That makes sense.
[00:56:22.040 --> 00:56:24.200]   Yeah, Vignesh. Yeah.
[00:56:24.200 --> 00:56:31.120]   Okay, perfect. Okay. Sanyam, can we please go down a little
[00:56:31.120 --> 00:56:31.360]   bit?
[00:56:31.800 --> 00:56:36.000]   Sure. Also, before we go on, could you please explain maybe
[00:56:36.000 --> 00:56:38.760]   to the beginners by having this normal distribution is
[00:56:38.760 --> 00:56:42.200]   important? And why do, why does everyone look at making sure
[00:56:42.200 --> 00:56:43.400]   it's normally distributed?
[00:56:43.400 --> 00:56:49.880]   Can I go up?
[00:56:49.880 --> 00:56:51.480]   Sure.
[00:56:51.480 --> 00:57:00.200]   Yeah, perfect. Okay. So one of the things that one of the
[00:57:00.200 --> 00:57:09.120]   implications of having this normal distribution, you see,
[00:57:09.120 --> 00:57:19.560]   everything that we do is some kind of we kind of have an
[00:57:19.560 --> 00:57:23.280]   assumption that it's it has a normal distribution or something
[00:57:23.280 --> 00:57:28.840]   like that. So in this competition, particularly, it's
[00:57:28.960 --> 00:57:35.720]   kind of very interesting that we had to really, really make
[00:57:35.720 --> 00:57:42.600]   sure that we have a proper normal distribution, mainly
[00:57:42.600 --> 00:57:51.040]   because let's take up a regression problem. And I don't
[00:57:51.040 --> 00:57:58.640]   know, I'm not able to explain it clearly. Okay, so maybe
[00:57:58.960 --> 00:58:04.720]   maybe Sanyam, can you go a little bit down and I have, we
[00:58:04.720 --> 00:58:09.760]   have explained a little bit of the implications of this
[00:58:09.760 --> 00:58:13.400]   distribution and how we split the data based on this
[00:58:13.400 --> 00:58:15.760]   distribution so we can understand it even better. So
[00:58:15.760 --> 00:58:18.480]   there are two techniques by which, you know, stratified
[00:58:18.480 --> 00:58:22.160]   k-fold splitting and simple k-fold splitting. Okay, so
[00:58:22.160 --> 00:58:26.800]   there are two techniques by which we split the data.
[00:58:27.800 --> 00:58:31.440]   Basically, because the given data was very small, about
[00:58:31.440 --> 00:58:39.000]   2800 samples, we had to split the data into five folds. So
[00:58:39.000 --> 00:58:43.760]   each fold consisted of equal number of samples. Okay. And
[00:58:43.760 --> 00:58:48.040]   what happens? So what is the difference between a stratified
[00:58:48.040 --> 00:58:53.160]   splitting and simple random splitting is the fact that when
[00:58:53.160 --> 00:58:57.040]   we do it in a stratified fashion, we take into account
[00:58:57.040 --> 00:59:04.200]   what is the distribution of the label, the target label, and
[00:59:04.200 --> 00:59:09.280]   then we make sure that the same distribution exists in every
[00:59:09.280 --> 00:59:16.720]   single, every single fold of the data, so that when you train
[00:59:16.720 --> 00:59:21.720]   on a particular fold, and then predict on another fold, you
[00:59:21.720 --> 00:59:26.280]   can make sure that your model has had a fair chance of
[00:59:26.280 --> 00:59:29.600]   looking at all the possible kind of data when it is getting
[00:59:29.600 --> 00:59:36.600]   trained, and when it is, when it is getting tested as well. So
[00:59:36.600 --> 00:59:44.040]   this is some kind of a logic that makes sure that you, you
[00:59:44.240 --> 00:59:55.920]   can your model in a family. So let's maybe let's go back. Okay,
[00:59:55.920 --> 00:59:58.560]   so, Sanyam, now, what is your question?
[00:59:58.560 --> 01:00:02.640]   Yeah, I was just trying to understand why the normal
[01:00:02.640 --> 01:00:04.920]   distribution is important, just trying to get that point
[01:00:04.920 --> 01:00:07.960]   across. But yeah, that makes sense. As we create these
[01:00:07.960 --> 01:00:11.120]   buckets or folds, we would want to make sure that every fold
[01:00:11.120 --> 01:00:15.000]   has a uniform distribution. And since it's already there, we
[01:00:15.000 --> 01:00:16.400]   don't have to worry too much about it.
[01:00:16.400 --> 01:00:18.360]   Okay.
[01:00:18.360 --> 01:00:24.120]   Right. So, yeah, so, yeah, like, I'd like to add on a point
[01:00:24.120 --> 01:00:30.160]   there. So as this was a regression problem, and
[01:00:30.160 --> 01:00:34.520]   stratified k-fold and simple k-fold, by default, you know,
[01:00:34.520 --> 01:00:38.520]   we use them in classification problem that we, we had to, you
[01:00:38.520 --> 01:00:44.600]   know, improvise a bit so that we can use the stratified k-fold
[01:00:44.600 --> 01:00:50.080]   on regression problems. So we found, you know, the community
[01:00:50.080 --> 01:00:55.040]   found an old competition in which we had the same, same
[01:00:55.040 --> 01:00:58.440]   issue. And yeah, we actually ended up splitting the
[01:00:58.440 --> 01:01:02.720]   regression targets into different buckets, and then
[01:01:02.720 --> 01:01:06.520]   stratifying on the buckets to get a proper stratified folds.
[01:01:07.520 --> 01:01:10.280]   That makes sense. Okay.
[01:01:10.280 --> 01:01:17.960]   So, one of the things I told you, Sanyam, that we learned
[01:01:17.960 --> 01:01:22.760]   from Kuruppikal, two days before the competition, end of the
[01:01:22.760 --> 01:01:28.120]   competition was training it on external data. So what happened
[01:01:28.120 --> 01:01:33.400]   was, Shahul and I, we, we assembled a lot of external
[01:01:33.400 --> 01:01:36.720]   data. So we had, we decided we are going to approach it in
[01:01:36.720 --> 01:01:44.760]   this way. We have three models to be developed. One was a
[01:01:44.760 --> 01:01:47.760]   Roberta base. So Roberta was one of the best performing
[01:01:47.760 --> 01:01:51.040]   transformers that we learned from the forum. So we used
[01:01:51.040 --> 01:01:55.800]   Roberta base. So one model is going to be trained with Roberta
[01:01:55.800 --> 01:01:59.400]   base on the competition data set that was provided by the
[01:01:59.400 --> 01:02:03.440]   competition host. Another model is going to be trained with
[01:02:03.440 --> 01:02:08.560]   Roberta large, again on the competition data. And then
[01:02:08.560 --> 01:02:13.440]   another model, again, we will train it with external data. So
[01:02:13.440 --> 01:02:16.160]   these were the three things that we had in our mind. I'm going
[01:02:16.160 --> 01:02:19.560]   to reason out why did we have these three things in our mind.
[01:02:19.560 --> 01:02:23.320]   The first one was because the Robert training Roberta large
[01:02:23.320 --> 01:02:27.160]   takes a lot of time. So to train a five fold Roberta large on
[01:02:27.160 --> 01:02:34.000]   Kaggle GPU, we had to spend at least three hours I think. If I
[01:02:34.000 --> 01:02:36.120]   remember it correctly, it was around two and a half hours to
[01:02:36.120 --> 01:02:39.720]   three hours that it took to complete one model training and
[01:02:39.720 --> 01:02:46.600]   you of course know, after training one model, we had to
[01:02:46.600 --> 01:02:51.520]   do it over and over again. And so what happened was, we, Shahul
[01:02:51.520 --> 01:02:55.920]   and I decided that Shahul will be responsible for training
[01:02:55.960 --> 01:02:59.440]   Roberta base models. So he's going to experiment with
[01:02:59.440 --> 01:03:04.960]   parameters on Roberta base, and then figure out which parameter
[01:03:04.960 --> 01:03:07.720]   which combination works well, and he will tell me and then I
[01:03:07.720 --> 01:03:11.920]   will run it on Roberta large. So this is what we understood in
[01:03:11.920 --> 01:03:13.840]   the beginning of the competition. This is the
[01:03:13.840 --> 01:03:17.680]   decision that we made consciously, because we were
[01:03:17.680 --> 01:03:21.520]   naive and we thought that the learnings from Roberta base will
[01:03:21.520 --> 01:03:24.880]   be transferred to Roberta large, but that's not true. What we
[01:03:24.880 --> 01:03:27.480]   learned from Roberta base need not be true for Roberta large.
[01:03:27.480 --> 01:03:30.120]   This is something that we realized as the competition
[01:03:30.120 --> 01:03:34.360]   progressed. Anyways, the other thing was,
[01:03:34.360 --> 01:03:38.080]   I was just going to point out that three hours feels fast, but
[01:03:38.080 --> 01:03:43.080]   that's like a lot for just 3000 examples. So that puts it in
[01:03:43.080 --> 01:03:43.680]   context.
[01:03:43.680 --> 01:03:45.200]   Exactly. Yeah.
[01:03:45.200 --> 01:03:49.680]   And the three hours is for K fold training, I guess. So you
[01:03:49.680 --> 01:03:53.280]   have to consider, you know, we are training the model for
[01:03:53.280 --> 01:04:00.280]   items. Yeah. And so, because of the size of the data set, we
[01:04:00.280 --> 01:04:04.280]   decided we are going to pre train our models on some
[01:04:04.280 --> 01:04:06.920]   external data. So basically, to people who are new to
[01:04:06.920 --> 01:04:10.920]   transformers, what transformer, why does transformer shine is
[01:04:10.920 --> 01:04:15.960]   because of transfer learning. You start from a very good
[01:04:15.960 --> 01:04:20.080]   initial point where your model has already been trained on
[01:04:20.480 --> 01:04:25.840]   another related task very well, so that your model already has
[01:04:25.840 --> 01:04:30.200]   an understanding of language. So you don't teach English to the
[01:04:30.200 --> 01:04:34.240]   model anymore, the model already somehow has a sense of the
[01:04:34.240 --> 01:04:38.040]   language English itself, and things like that. So from that
[01:04:38.040 --> 01:04:43.520]   point, you, you start training on top of it. So it's something
[01:04:43.520 --> 01:04:47.800]   like how we go to high school, where all of us are exposed to
[01:04:47.800 --> 01:04:51.840]   a certain knowledge in all the domains. So we know, we know
[01:04:51.840 --> 01:04:57.240]   quite well about how to study all the domains a bit. Whereas
[01:04:57.240 --> 01:04:59.760]   when you want to specialize, you go to an engineering course or
[01:04:59.760 --> 01:05:01.800]   an arts course, and then you specialize on top of it
[01:05:01.800 --> 01:05:05.800]   exclusively on that. So that's how transformers work as well.
[01:05:05.800 --> 01:05:09.280]   Although in India, we always go to engineering school.
[01:05:09.280 --> 01:05:15.240]   I was just going to say that. That's a really nice.
[01:05:15.760 --> 01:05:18.160]   We are we are all fine. Do you want to for engineering?
[01:05:18.160 --> 01:05:20.160]   Yeah, exactly.
[01:05:20.160 --> 01:05:27.360]   We thought, let's do the same. But the challenge was, because
[01:05:27.360 --> 01:05:33.080]   this competition had this weird target, which it's not
[01:05:33.080 --> 01:05:36.640]   interpretable. We really couldn't find an external data
[01:05:36.640 --> 01:05:40.400]   set, which consisted of the same kind of targets and things like
[01:05:40.400 --> 01:05:45.000]   that. So what we decided was we will probably take up a
[01:05:45.000 --> 01:05:50.600]   different approach. But if you if we look at Bradley theory,
[01:05:50.600 --> 01:05:53.640]   how Bradley theory works is it takes pairs of text and compares
[01:05:53.640 --> 01:05:56.080]   the text and then tells which one is better, which one is
[01:05:56.080 --> 01:05:59.600]   easier to read and things like that. So Sanyam, when he
[01:05:59.600 --> 01:06:04.920]   introduced the Explorer EDA, he spoke about the URLs that were
[01:06:04.920 --> 01:06:10.000]   assigned to the text snippets. So we used that information.
[01:06:10.440 --> 01:06:16.400]   In some of the URLs, we had text coming from Wikipedia, whereas
[01:06:16.400 --> 01:06:20.880]   in some of the URLs, we had text coming from another domain
[01:06:20.880 --> 01:06:25.600]   called Simple Wikipedia. And we had no idea about it. There is
[01:06:25.600 --> 01:06:29.000]   another Wikipedia called Simple Wikipedia. Okay. And then we
[01:06:29.000 --> 01:06:33.280]   started looking into what is Simple Wikipedia. And then we
[01:06:33.280 --> 01:06:39.480]   found out that Wikipedia, Simple Wikipedia is a sister project of
[01:06:39.560 --> 01:06:44.000]   Wikipedia, where the same information is written in a much
[01:06:44.000 --> 01:06:47.840]   simpler form, so that everyone can understand and things like
[01:06:47.840 --> 01:06:54.040]   that. So for instance, if you look at the Ford Pinto, the
[01:06:54.040 --> 01:06:59.840]   third row, so if you look at it, you will see the vehicle was
[01:06:59.840 --> 01:07:03.200]   marketed under Ford Motor Company in the States and
[01:07:03.200 --> 01:07:07.840]   Canada. It was made from here to that year. All simple sentences,
[01:07:07.840 --> 01:07:11.240]   no complex sentences, no complex words. Whereas when you look at
[01:07:11.240 --> 01:07:15.320]   the other example, you look at it as the smallest American
[01:07:15.320 --> 01:07:18.040]   vehicle, the Pinto. You see, it's already a complex sentence
[01:07:18.040 --> 01:07:22.440]   because it has a comma, it has a class which supports another
[01:07:22.440 --> 01:07:28.520]   class and things like that. So this is something that we
[01:07:28.520 --> 01:07:31.240]   identified and then we thought, okay, we have to use this data
[01:07:31.240 --> 01:07:35.400]   to pre-train our model. On the other hand, we have to remember
[01:07:35.400 --> 01:07:38.520]   that our competition data is trained for a regression task,
[01:07:38.520 --> 01:07:43.240]   whereas this is not a regression task. So we used a loss
[01:07:43.240 --> 01:07:47.000]   function called ranking loss. So in PyTorch, we have an
[01:07:47.000 --> 01:07:49.960]   implementation already in the library. So what it does is it
[01:07:49.960 --> 01:07:54.520]   takes two vectors, sorry, it takes two scalars and then
[01:07:54.520 --> 01:07:58.760]   compares which scalar should be scored higher in comparison to
[01:07:58.760 --> 01:08:08.680]   the other scalar. Margin ranking loss. Yeah, the first one.
[01:08:08.680 --> 01:08:13.640]   So that's the thing. So it can take the values x1 and x2 and
[01:08:13.640 --> 01:08:17.400]   then it will take the value. So x1 and x2 are two scalars and
[01:08:17.400 --> 01:08:20.920]   then y is the ground truth, which says whether x1 is
[01:08:20.920 --> 01:08:25.240]   greater than x2 or x1 should be lesser than x2 or whatever it
[01:08:25.240 --> 01:08:29.240]   does. So we identified this. When you look at this, this is
[01:08:29.240 --> 01:08:35.640]   something similar to you take the simple Wikipedia text and
[01:08:35.640 --> 01:08:39.640]   find out the readability score. Let's say that is x1. You take
[01:08:39.640 --> 01:08:44.200]   the normal Wikipedia text and find a readability score. It is
[01:08:44.200 --> 01:08:47.800]   x2. Now you are pretty sure that the simple Wikipedia must
[01:08:47.800 --> 01:08:52.040]   be easier than Wikipedia. So you know the target value y as
[01:08:52.040 --> 01:08:57.560]   well. So this is how we pre-trained our data a lot. So
[01:08:57.560 --> 01:09:03.640]   what happened was we tried this at the beginning of the at the
[01:09:03.640 --> 01:09:06.360]   beginning when we joined together as a team, we tried it,
[01:09:06.360 --> 01:09:11.560]   but this did not work. We were going crazy. Why does this make
[01:09:11.560 --> 01:09:14.760]   sense that you pre-train on so much large samples because
[01:09:14.760 --> 01:09:18.120]   Wikipedia and simple Wikipedia, they had more than 100,000
[01:09:18.120 --> 01:09:22.920]   pairs that were present in the data. So you train it on 100,000
[01:09:22.920 --> 01:09:26.200]   pairs and then you further train it on the competition data. The
[01:09:26.200 --> 01:09:29.480]   result should be way better, but that wasn't the case and we did
[01:09:29.480 --> 01:09:34.760]   not know why what happened was two days before. So we parked it.
[01:09:34.760 --> 01:09:37.400]   We did not use it any further. Two days before the competition
[01:09:37.400 --> 01:09:42.040]   when we ran out of ideas and I was going around this discussion
[01:09:42.040 --> 01:09:45.480]   forum and commenting on people's posts like nice, really nice
[01:09:45.480 --> 01:09:47.880]   work, looking forward to the end of the competition and things
[01:09:47.880 --> 01:09:52.040]   like that. Kurupikal posted something and then I went looking
[01:09:52.040 --> 01:09:55.560]   at Kurupikal's old post and then I saw in one place Kurupikal
[01:09:55.560 --> 01:10:01.320]   says this technique kind of works well for him. So and then
[01:10:01.320 --> 01:10:03.880]   I was like, okay, if it works for that person, it should
[01:10:03.880 --> 01:10:07.000]   probably work for me as well. Then we looked into it and then
[01:10:07.000 --> 01:10:09.640]   we got it working on the last day of the competition and then
[01:10:09.640 --> 01:10:11.800]   we submitted it.
[01:10:11.800 --> 01:10:16.120]   I can imagine the group, the Slack conversations that must
[01:10:16.120 --> 01:10:18.920]   have been going on and you must have heard.
[01:10:18.920 --> 01:10:26.040]   Yeah. So the change that Vignesh did on it to make it work
[01:10:26.040 --> 01:10:33.000]   was to match the statistics of this data set with the
[01:10:33.000 --> 01:10:37.160]   distribution of the given data set, like the sentence length,
[01:10:37.160 --> 01:10:42.360]   the number of words, stuff like that. The characteristics of
[01:10:42.360 --> 01:10:47.080]   the text were matched and then he pre-trained on the data to
[01:10:47.080 --> 01:10:51.720]   make it work. So after the end of the competition, we found a
[01:10:51.720 --> 01:10:56.760]   better method, which was one of the other solutions. I think,
[01:10:56.760 --> 01:11:01.560]   I'm not sure who did it, but the person used sentence
[01:11:01.560 --> 01:11:05.160]   transformers to match, to do similar matching and he got
[01:11:05.160 --> 01:11:08.040]   way better results. Yeah.
[01:11:09.480 --> 01:11:13.400]   So probably if we would have spent much more time doing it,
[01:11:13.400 --> 01:11:17.640]   then we could have done better, but this is every Kaggler's
[01:11:17.640 --> 01:11:20.440]   regret. If we had more time, we would have done better.
[01:11:20.440 --> 01:11:25.480]   Yeah. I mean, yeah, this made a very good difference in the
[01:11:25.480 --> 01:11:29.960]   leaderboard because pre-training on the same task always helps
[01:11:29.960 --> 01:11:30.440]   very much.
[01:11:30.440 --> 01:11:34.200]   Yeah. I can imagine that.
[01:11:34.200 --> 01:11:39.000]   Moving forward in your solution and that's one thing I really
[01:11:39.000 --> 01:11:41.960]   love. That's why I really want to do these deep dives because
[01:11:41.960 --> 01:11:45.480]   we learned, I didn't think of this technique ever before.
[01:11:45.480 --> 01:11:47.960]   Like you could have an external data. My intuition would be
[01:11:47.960 --> 01:11:51.400]   just train on that. Just throw the model at that, you're good.
[01:11:51.400 --> 01:11:55.080]   But no, you need to match the stats of it. I didn't think of
[01:11:55.080 --> 01:11:57.320]   that extra step. So thanks. Thanks for sharing that.
[01:11:57.320 --> 01:12:05.960]   So I'll keep moving forward. So these are usually for anyone
[01:12:05.960 --> 01:12:08.200]   who's gone through a solution, these are the standard boiler
[01:12:08.200 --> 01:12:11.320]   plate. I'll keep scrolling and please feel free to point out
[01:12:11.320 --> 01:12:11.640]   anything.
[01:12:11.640 --> 01:12:14.760]   So one of the things that
[01:12:14.760 --> 01:12:20.040]   Shahul and I were very particular about was about the
[01:12:20.040 --> 01:12:23.960]   cleanliness of the code. We really wanted to make sure that
[01:12:23.960 --> 01:12:29.640]   we, so our idea was we run, so Shahul runs experiments on the
[01:12:29.640 --> 01:12:33.240]   base and he tells me which works, which doesn't work. And I
[01:12:33.240 --> 01:12:39.000]   replicate the same for large. For this to happen, we really
[01:12:39.000 --> 01:12:42.760]   had to make sure that we had, we had been using the same
[01:12:42.760 --> 01:12:47.800]   environment. We use the same code. We use the same random
[01:12:47.800 --> 01:12:51.960]   state. We use the same version of the libraries. Everything
[01:12:51.960 --> 01:12:54.680]   should be exactly the same. Just the word base should be
[01:12:54.680 --> 01:12:59.960]   changed with the word large. So this is, this is something that
[01:12:59.960 --> 01:13:03.560]   we were very particular about. So we spent over, I don't know,
[01:13:03.560 --> 01:13:09.000]   like two weeks and it took, I don't know, like 70, 80 commits
[01:13:09.000 --> 01:13:12.920]   to make sure that our code was robust. So at that point in
[01:13:12.920 --> 01:13:16.280]   time for the competition, we just joined together for the
[01:13:16.280 --> 01:13:20.120]   last one month together. And we already spent two weeks just on
[01:13:20.120 --> 01:13:24.760]   making the code perfect. So we already spent a lot of time,
[01:13:24.760 --> 01:13:28.120]   but we are really happy we did that because that made sure
[01:13:28.120 --> 01:13:33.080]   that we have a very good experimental setup where we can
[01:13:33.080 --> 01:13:36.040]   replicate each other's work. I can just send the configuration
[01:13:36.040 --> 01:13:39.480]   file and I can be confident that Shahul will exactly be able
[01:13:39.480 --> 01:13:42.520]   to replicate the same thing at any point in time or something
[01:13:42.520 --> 01:13:47.320]   like that. So this code snippet is also an example for it. It
[01:13:47.320 --> 01:13:49.880]   was like quite documented and things like that to make sure
[01:13:49.880 --> 01:13:52.680]   that we are aligned with each other.
[01:13:52.680 --> 01:13:56.840]   This is one of those real Kaggle code instances where you
[01:13:56.840 --> 01:13:59.720]   can see comments as well and not just like one single like
[01:13:59.720 --> 01:14:01.160]   comment, but probably dog things.
[01:14:01.160 --> 01:14:09.640]   Okay, so looks like this is all just setting up the dataset.
[01:14:09.640 --> 01:14:12.680]   You're doing the things you need to do for Torch.
[01:14:12.680 --> 01:14:22.680]   I just want to point out that one thing that clean code really
[01:14:22.680 --> 01:14:25.480]   makes it easy is if you're doing a live stream here, this is the
[01:14:25.480 --> 01:14:28.520]   first time I'm reading your code and I can just nod to it and
[01:14:28.520 --> 01:14:32.440]   like skim past it because it's not quite common in Kaggle
[01:14:32.440 --> 01:14:34.920]   competitions. You have to spend some time to understand
[01:14:34.920 --> 01:14:35.880]   solutions sometimes.
[01:14:35.880 --> 01:14:47.640]   You know, Sanyam, I want beautiful stuff on my screen.
[01:14:47.640 --> 01:14:53.640]   So that is one of the main reasons why I like OVANN DB as
[01:14:53.640 --> 01:14:57.640]   well. I don't know, for some reason, it is so beautiful when
[01:14:57.640 --> 01:15:00.680]   you look at the dashboards. You feel proud of yourself that,
[01:15:00.680 --> 01:15:05.000]   wow, I created this and this looks so nice in the dashboard.
[01:15:05.000 --> 01:15:07.720]   Like even today, I was really surprised. We were creating a
[01:15:07.720 --> 01:15:13.320]   1DB report as well. That's a gift that we gave ourselves
[01:15:13.320 --> 01:15:17.240]   because we wanted to beautify our solution even more. And the
[01:15:17.240 --> 01:15:20.680]   results, when you look at the dashboard and everything, it no
[01:15:20.680 --> 01:15:24.600]   work, no time, it looks beautiful. And for me, it is
[01:15:24.600 --> 01:15:28.600]   very important to have something pleasing. So the code, the
[01:15:28.600 --> 01:15:30.920]   dashboard, everything is a consequence of it.
[01:15:30.920 --> 01:15:35.720]   I can't take credit for that. I wasn't remotely involved in
[01:15:35.720 --> 01:15:38.120]   that. The team has come up with it, but I'll make sure I pass
[01:15:38.120 --> 01:15:41.960]   that feedback. Thanks. That really makes me happy to hear
[01:15:41.960 --> 01:15:42.460]   that.
[01:15:42.460 --> 01:15:45.720]   I'm pretty sure I'm not the first person who tells that 1DB
[01:15:45.720 --> 01:15:46.680]   looks beautiful.
[01:15:49.240 --> 01:15:54.600]   Really appreciate that. Thanks for that. Okay, so I think I
[01:15:54.600 --> 01:15:57.320]   also want to point out the attention trick that had come
[01:15:57.320 --> 01:16:00.680]   up. I don't think this is it. But maybe if I go back to your
[01:16:00.680 --> 01:16:02.840]   solution, it should be there.
[01:16:02.840 --> 01:16:06.120]   No, no, I have also, no, Sanyam, you can go down and you
[01:16:06.120 --> 01:16:14.040]   will see something weird. You can even go down. Still down.
[01:16:15.480 --> 01:16:21.160]   Still, still, still, still, still. Yeah, here it comes. We
[01:16:21.160 --> 01:16:23.320]   made pictures of how it works as well.
[01:16:23.320 --> 01:16:28.840]   This is perfect. Awesome. So this is the explanation of...
[01:16:28.840 --> 01:16:31.020]   Sorry, please.
[01:16:31.020 --> 01:16:36.360]   Maybe we will go to the tokenizer and maybe Shahul can
[01:16:36.360 --> 01:16:38.600]   explain the tokenizer and then we'll go to the model.
[01:16:38.600 --> 01:16:41.800]   Sounds good. Going back to the tokenizer.
[01:16:42.520 --> 01:16:46.760]   Right. In this competition, yeah, as in like we used
[01:16:46.760 --> 01:16:50.520]   Roberta. Roberta was the only model that we used. We used
[01:16:50.520 --> 01:16:54.440]   Roberta base and large and the tokenizer we used the default
[01:16:54.440 --> 01:17:01.320]   Roberta tokenizer for converting the text into the input in which
[01:17:01.320 --> 01:17:06.360]   Roberta takes it. So Roberta needs input IDs and also an
[01:17:06.360 --> 01:17:11.800]   optional attention mask, which specifies if the actual token is
[01:17:11.800 --> 01:17:17.560]   there or not. So yeah, this is normal tokenizer that Roberta
[01:17:17.560 --> 01:17:18.360]   uses. Yes.
[01:17:18.360 --> 01:17:27.000]   So from the Roberta tokenizer, we get two outputs. One is the
[01:17:27.000 --> 01:17:30.600]   input IDs and then the other one is the attention mask. So what
[01:17:30.600 --> 01:17:38.040]   happens is basically the tokenizer takes list of text and
[01:17:38.040 --> 01:17:41.400]   then it gives out input IDs. So input IDs are the vocabulary
[01:17:41.400 --> 01:17:45.240]   indices corresponding to the tokens and the attention mask
[01:17:45.240 --> 01:17:48.200]   basically tells you whether a word was present there or a
[01:17:48.200 --> 01:17:53.320]   packed token was added because of the length constraints. So
[01:17:53.320 --> 01:17:53.800]   that's it.
[01:17:53.800 --> 01:17:59.320]   Okay. That makes sense. All right. So now comes the
[01:17:59.320 --> 01:18:02.840]   exciting part for at least people who enjoy modeling.
[01:18:02.840 --> 01:18:10.040]   So maybe let's start from the attention head represent. Then
[01:18:10.120 --> 01:18:16.360]   it's or the next one. Last hidden state mean puller because
[01:18:16.360 --> 01:18:21.800]   each one builds upon on top of each other. So if we start from
[01:18:21.800 --> 01:18:24.840]   here, then it's easier to understand that. So basically
[01:18:24.840 --> 01:18:29.480]   what we do is, Sanyam, can you highlight the self.roberta line?
[01:18:29.480 --> 01:18:38.520]   Yeah, exactly. So this is the first thing that we do is we
[01:18:38.520 --> 01:18:45.320]   load a pre-trained transformer model from HuggingFace and we
[01:18:45.320 --> 01:18:50.840]   can assign a dropout probability to the layers of the
[01:18:50.840 --> 01:18:54.360]   transformer. So this is the most basic thing that all of us
[01:18:54.360 --> 01:19:00.200]   start with. And then the transformer takes in the input
[01:19:00.200 --> 01:19:05.720]   IDs and the attention mask and then it gives out two inputs.
[01:19:05.720 --> 01:19:09.080]   One is the puller output, two outputs. One is the puller
[01:19:09.080 --> 01:19:16.040]   output or it gives you the last hidden state. So we did not use
[01:19:16.040 --> 01:19:19.960]   the puller output, but we use the last hidden state because of
[01:19:19.960 --> 01:19:23.720]   some reasons that I will explain to you later on. Whereas the
[01:19:23.720 --> 01:19:30.280]   last hidden state is basically, now we can go to the diagram
[01:19:30.280 --> 01:19:31.640]   that I have drawn below.
[01:19:33.720 --> 01:19:34.760]   This is incredible.
[01:19:34.760 --> 01:19:47.480]   We drew something on the notebooks, scanned it and put
[01:19:47.480 --> 01:19:58.360]   it here. Anyway, so what happens is a batch of inputs
[01:19:58.360 --> 01:20:05.400]   consists of on the right. So a batch of inputs consists of
[01:20:05.400 --> 01:20:10.360]   text and each text has been tokenized and you get tokens
[01:20:10.360 --> 01:20:14.200]   corresponding to it. When you look at it in a matrix form,
[01:20:14.200 --> 01:20:19.560]   what happens is the number of tokens is the maximum number of
[01:20:19.560 --> 01:20:23.720]   tokens that can be presented in the batch. And then the input
[01:20:23.720 --> 01:20:27.880]   dimension is basically the hidden dimension that comes from
[01:20:27.880 --> 01:20:34.680]   the transformer. So each column corresponds to a vector that is
[01:20:34.680 --> 01:20:39.000]   corresponding to a token. So basically this you can think of
[01:20:39.000 --> 01:20:43.000]   it as if you gave a single document, you tokenize the
[01:20:43.000 --> 01:20:47.640]   documents and for each token you had a word vector. And when
[01:20:47.640 --> 01:20:50.760]   you concatenate all the word vectors together, it becomes a
[01:20:50.760 --> 01:20:54.280]   matrix. And that is what you see over there. So it has the
[01:20:54.280 --> 01:20:59.720]   shape, hidden number of tokens as the number of columns and
[01:20:59.720 --> 01:21:02.840]   input dimension, which is the hidden dimension of the
[01:21:02.840 --> 01:21:07.160]   transformer model. So for instance, for Roberta Bayes, the
[01:21:07.160 --> 01:21:10.120]   hidden dimension, which is the input dimension in this case is
[01:21:10.120 --> 01:21:16.360]   512, whereas for Roberta Lodge it's 1024. So if your matrix,
[01:21:16.360 --> 01:21:23.640]   if your input batch had maximum number of 200 tokens, then
[01:21:24.520 --> 01:21:32.520]   you will have a matrix of 1024 by 200 to represent that
[01:21:32.520 --> 01:21:38.680]   particular text. Okay, so that is what you will. And then what
[01:21:38.680 --> 01:21:43.400]   you do is you, that is what comes as output from the
[01:21:43.400 --> 01:21:50.040]   transformer. Now can you know, Sanyam, about the matrix value,
[01:21:50.040 --> 01:21:55.160]   the matrix value that you see on the left. So the matrix value
[01:21:55.160 --> 01:22:00.680]   is basically the attention head. So what you do is the matrix
[01:22:00.680 --> 01:22:03.720]   W is of dimension, input dimension is the number of
[01:22:03.720 --> 01:22:06.440]   columns and the attention head, hidden dimension. So the
[01:22:06.440 --> 01:22:11.320]   attention head can also take a hidden dimension. So when you,
[01:22:11.320 --> 01:22:14.360]   when you do a matrix multiplication between the
[01:22:14.360 --> 01:22:22.920]   W and the last one, you get some output. So the output is
[01:22:22.920 --> 01:22:26.440]   what you see on attention head, hidden dimension, number of
[01:22:26.440 --> 01:22:31.720]   rows, and then the number of tokens. So that's the output
[01:22:31.720 --> 01:22:36.680]   from the transformer. And then instead you take that output,
[01:22:36.680 --> 01:22:43.480]   and then the attention head has, the attention head has another
[01:22:43.480 --> 01:22:49.720]   single vector that runs through the entire output. Okay. And
[01:22:49.720 --> 01:22:53.240]   then a simple math multiplication and you get a
[01:22:53.240 --> 01:23:01.240]   simple vector, responding to the, if you think about it
[01:23:01.240 --> 01:23:07.320]   logically, what it means is this attention head is basically
[01:23:07.320 --> 01:23:14.120]   trying to learn weights for each token in the document. So
[01:23:14.120 --> 01:23:17.880]   the attention scores, the attention scores that you see
[01:23:17.880 --> 01:23:21.880]   is a weight that the model has learned. Exactly. The
[01:23:21.880 --> 01:23:25.000]   attention score is a weight that the model has learned
[01:23:25.000 --> 01:23:31.080]   corresponding to each token in the text document, which helped
[01:23:31.080 --> 01:23:36.120]   the model to do a better regression. That's it. So for
[01:23:36.120 --> 01:23:40.920]   instance, so that's it. As simple as that. Basically, you're
[01:23:40.920 --> 01:23:46.280]   trying to learn feature importance vector. Okay. And
[01:23:46.280 --> 01:23:50.840]   then finally, you simply do a linear, the last step is where
[01:23:50.840 --> 01:23:56.200]   you take this attention score and you take the last hidden
[01:23:56.200 --> 01:24:00.440]   state from the transformer. You do a linear combination
[01:24:00.440 --> 01:24:02.840]   between these two things, and then you get a vector for the
[01:24:02.840 --> 01:24:07.800]   document. So what you're basically doing is you're
[01:24:07.800 --> 01:24:11.640]   trying to learn the feature importance for each feature in
[01:24:11.640 --> 01:24:15.480]   your document, where the feature is word. So you're
[01:24:15.480 --> 01:24:18.440]   learning the importance of every single word that is
[01:24:18.440 --> 01:24:24.440]   present in your document, and then concatenating them
[01:24:24.440 --> 01:24:29.400]   together into one single vector, which will represent the
[01:24:29.400 --> 01:24:33.000]   entire document. This is basically like doing a TF-IDF
[01:24:33.000 --> 01:24:40.280]   thing. Got it. Yeah. So this is how the attention head
[01:24:40.280 --> 01:24:44.920]   actually works. Now, if you look into it deeply, you will
[01:24:44.920 --> 01:24:48.920]   realize there is a big problem with that. The big problem is
[01:24:48.920 --> 01:24:53.800]   that at the end of the document, based on the batch
[01:24:53.800 --> 01:24:58.840]   that a particular document fell into, you will have bad
[01:24:58.840 --> 01:25:04.280]   tokens corresponding to it. Now, you're assigning
[01:25:04.280 --> 01:25:08.600]   attention scores to the bad tokens as well, whereas you
[01:25:08.600 --> 01:25:13.960]   know that the bad token are not at all important. You know
[01:25:13.960 --> 01:25:17.400]   that the bad tokens are not important in your final. So
[01:25:17.400 --> 01:25:20.200]   what you basically do is depending upon the batch size,
[01:25:20.200 --> 01:25:26.680]   which batch. So you will have a representation for each
[01:25:26.680 --> 01:25:31.960]   document, which will change based on which batch it fell
[01:25:31.960 --> 01:25:37.240]   into. So this is okay. So we realized that and we changed it
[01:25:37.240 --> 01:25:43.480]   based on masking. So what we do is we take the attention
[01:25:43.480 --> 01:25:47.000]   scores and then we take the attention mask. We superimpose
[01:25:47.000 --> 01:25:49.960]   on each other. So we make sure that the attention scores
[01:25:49.960 --> 01:25:54.280]   corresponding to the bad tokens is very, very low. So the bad
[01:25:54.280 --> 01:25:57.480]   tokens get almost zero importance in making the
[01:25:57.480 --> 01:26:01.480]   decision. So this is the trick that we used and this helped
[01:26:01.480 --> 01:26:04.280]   us climb up the leaderboard by a lot.
[01:26:04.280 --> 01:26:06.200]   That makes sense.
[01:26:06.200 --> 01:26:07.480]   And
[01:26:07.480 --> 01:26:14.060]   Shahul?
[01:26:14.060 --> 01:26:15.500]   Yes.
[01:26:15.500 --> 01:26:22.200]   I think it's clear from, yeah,
[01:26:22.840 --> 01:26:25.000]   your explanation.
[01:26:25.000 --> 01:26:28.440]   I always say like, I am the benchmark. If I can understand
[01:26:28.440 --> 01:26:33.960]   it, anyone can. So far it was it was clear to me. Also, also
[01:26:33.960 --> 01:26:37.400]   real quick, Vignesh, we might run 15 minutes over the time.
[01:26:37.400 --> 01:26:38.200]   Is that fine?
[01:26:38.200 --> 01:26:42.520]   Yeah, yeah, that's fine. Of course.
[01:26:42.520 --> 01:26:48.360]   Thanks. All right. I'll try to wind up quickly. But so far,
[01:26:48.360 --> 01:26:51.000]   things make sense. There's this problem, or we've come up with
[01:26:51.000 --> 01:26:55.240]   this scores, these attention scores. And to address the
[01:26:55.240 --> 01:26:59.080]   padding problem, you superimposing attention mask
[01:26:59.080 --> 01:27:02.680]   code. So far, it makes sense. And all of this is being done on
[01:27:02.680 --> 01:27:03.640]   top of Coperta.
[01:27:03.640 --> 01:27:05.720]   Exactly. Yeah.
[01:27:05.720 --> 01:27:14.120]   So basically, this is how the model has been designed. And
[01:27:18.440 --> 01:27:21.000]   all right, another trick I want to point out is also the layer
[01:27:21.000 --> 01:27:24.520]   wise learning rate decay. This was shared in FastA at one
[01:27:24.520 --> 01:27:27.000]   point in time, but I don't think it's come out of that
[01:27:27.000 --> 01:27:32.440]   community a lot. So maybe I can explain it at a higher level.
[01:27:32.440 --> 01:27:36.440]   So whenever we apply learning rates, they're applied to the
[01:27:36.440 --> 01:27:40.600]   complete model. Sometimes, at least this is done in FastA,
[01:27:40.600 --> 01:27:45.320]   they group the layers into certain buckets, and they apply
[01:27:45.320 --> 01:27:49.000]   different learning rates to it. So as I understand, that's
[01:27:49.000 --> 01:27:54.280]   what's going on here as well. We've created these groups, and
[01:27:54.280 --> 01:27:56.200]   we're applying different learning rates to them. Am I
[01:27:56.200 --> 01:27:57.480]   correct? Am I understanding?
[01:27:57.480 --> 01:28:05.000]   Yes. So this idea has been there for a long. And yeah, with
[01:28:05.000 --> 01:28:08.200]   this competition, I think it became very famous because Roy
[01:28:08.200 --> 01:28:12.120]   Etzing published a journal with all these techniques, which
[01:28:12.120 --> 01:28:15.880]   included differential learning rate for different transform
[01:28:15.880 --> 01:28:20.360]   blocks. So what happens is that different layers or different
[01:28:20.360 --> 01:28:24.440]   blocks in the whole architecture learns different
[01:28:24.440 --> 01:28:29.160]   information. So when we assign one specific learning rate to
[01:28:29.160 --> 01:28:33.400]   the whole architecture, it might not fit the model. So the
[01:28:33.400 --> 01:28:38.760]   idea is to assign different learning rates to different
[01:28:38.760 --> 01:28:44.040]   parts of the model according to the layer or according to the
[01:28:44.040 --> 01:28:49.880]   layer so that the model learns. So it is assumed that the lower
[01:28:49.880 --> 01:28:55.400]   layers learn more general information. And as we come down
[01:28:55.400 --> 01:28:59.080]   towards the bottom, the model learns more specific
[01:28:59.080 --> 01:29:04.200]   information. So at that point is where we need a faster
[01:29:04.200 --> 01:29:08.280]   learning rate. So that's the basic idea. And yeah, with this
[01:29:08.280 --> 01:29:11.480]   competition, I think it became pretty famous due to this
[01:29:11.480 --> 01:29:13.080]   kernel of Roy Etzing.
[01:29:13.080 --> 01:29:20.440]   That makes sense. Thanks for clarifying that. So I'm quickly
[01:29:20.440 --> 01:29:22.760]   reading through the code and scrolling down and trying to
[01:29:22.760 --> 01:29:25.000]   understand and point out the things that are interesting.
[01:29:25.000 --> 01:29:32.840]   So far, things make sense. You set up a scheduler, you updated
[01:29:32.840 --> 01:29:36.920]   the config, because unlike me, you are really strict about the
[01:29:36.920 --> 01:29:42.840]   standards of the code. I just, I don't do any of this. I'm just
[01:29:42.840 --> 01:29:48.440]   intimidated by how much effort you all have put in there. I
[01:29:48.440 --> 01:29:52.040]   think I'd like to understand this as well, because this was a
[01:29:52.040 --> 01:29:54.280]   part of your write up as well. So if you could please explain
[01:29:54.280 --> 01:29:55.080]   what's going on here.
[01:29:55.080 --> 01:30:02.920]   Yeah, sure. So basically, when you treat a problem as a
[01:30:02.920 --> 01:30:08.200]   regressor problem, which is what the competition data was, on the
[01:30:08.200 --> 01:30:13.640]   left, you see, you read the text, you send the text to a
[01:30:13.640 --> 01:30:19.160]   vectorizer. So Samyam, can you point your pointer to the text
[01:30:19.160 --> 01:30:25.720]   regressor? Yeah. So you take a text and then you pass it to a
[01:30:25.720 --> 01:30:28.840]   vectorizer. The vectorizer can be anything. So it can be a
[01:30:28.840 --> 01:30:31.880]   basic TF-IDF vectorizer, or it can be a Hugging Phase
[01:30:31.880 --> 01:30:34.520]   Transformer, or a Space-E vectorizer, it doesn't matter.
[01:30:34.520 --> 01:30:39.240]   You get a vector that represents the piece of text, and then you
[01:30:39.240 --> 01:30:41.720]   have a regressor. A regressor, again, can be a very simple
[01:30:41.720 --> 01:30:44.680]   regressor, like a linear regressor or an SVM regressor.
[01:30:44.680 --> 01:30:51.320]   So a regressor which takes the vector and spits out a scalar
[01:30:51.320 --> 01:30:56.440]   as a prediction, and then you see how well the model has
[01:30:56.440 --> 01:31:01.560]   learned based on the ground truth. So you have a prediction
[01:31:01.560 --> 01:31:06.600]   and then you have a target, and you compute the root mean
[01:31:06.600 --> 01:31:10.040]   squared error, and then you read, you update the model. So
[01:31:10.040 --> 01:31:13.400]   that's how it works. Whereas in a text comparator approach,
[01:31:13.400 --> 01:31:18.280]   what we do is, instead of learning the readability
[01:31:18.280 --> 01:31:23.080]   prediction from one single input, you do it from two
[01:31:23.080 --> 01:31:28.920]   inputs. So on the right, you have text one and text two. So
[01:31:28.920 --> 01:31:31.480]   you do the same process like what you do on the left, you
[01:31:31.480 --> 01:31:33.800]   take the text one, pass it to the vectorizer, you get a
[01:31:33.800 --> 01:31:38.920]   vector, and you do the same for text two. Whereas instead of
[01:31:38.920 --> 01:31:42.440]   sending it to a regressor, you send it to a comparator. The
[01:31:42.440 --> 01:31:48.120]   comparator, what it does is, it tries to take both the vectors
[01:31:48.120 --> 01:31:55.560]   and then see which of these vectors is easier to read or
[01:31:55.560 --> 01:31:57.720]   which of these vectors is difficult to read or whatever it
[01:31:57.720 --> 01:32:02.760]   is. And you compute a ranking loss in accordance to it, and
[01:32:02.760 --> 01:32:05.240]   then you update the weights of the model. So this is how the
[01:32:05.240 --> 01:32:09.160]   text comparator works. So basically, we used the text
[01:32:09.160 --> 01:32:13.880]   regressor problem to train on the competition data, whereas
[01:32:13.880 --> 01:32:17.560]   the text comparator approach, we used it for the external data.
[01:32:17.560 --> 01:32:21.960]   And this helped us leverage over, I don't know, we trained
[01:32:21.960 --> 01:32:27.240]   on 20,000 more samples, just because we used the text
[01:32:27.240 --> 01:32:31.240]   comparator approach in comparison to the 3,000 samples
[01:32:31.240 --> 01:32:35.000]   from. So we trained on more than seven times the data, and
[01:32:35.000 --> 01:32:37.560]   of course, had a huge benefit.
[01:32:37.560 --> 01:32:42.600]   Yeah, this is really incredible. Again, as I said, as someone
[01:32:42.600 --> 01:32:46.440]   who just reads solutions and not understands them so sincerely,
[01:32:46.440 --> 01:32:50.360]   I would just assume you take a data set, throw your model at
[01:32:50.360 --> 01:32:54.120]   it, you're good. But this really explains it that, okay, first
[01:32:54.120 --> 01:32:57.160]   of all, you pre-process the data set as well. And then
[01:32:57.160 --> 01:33:01.240]   furthermore, you also created this comparator model that was
[01:33:01.240 --> 01:33:04.600]   comparing those two texts and predicting the difficulty
[01:33:04.600 --> 01:33:05.800]   ranking. That's great.
[01:33:05.800 --> 01:33:08.380]   So.
[01:33:08.380 --> 01:33:13.640]   So I think that summarizes the solution.
[01:33:13.640 --> 01:33:17.480]   And we have a WandaVie dashboard link at the bottom.
[01:33:22.040 --> 01:33:25.160]   My keyboard has died because it needs a charge.
[01:33:25.160 --> 01:33:28.520]   Sanyam, can you quickly go through this dashboard and
[01:33:28.520 --> 01:33:35.000]   can you guess how much time we took to do this?
[01:33:35.000 --> 01:33:39.160]   This is--
[01:33:39.160 --> 01:33:42.120]   I don't want to disrespect you, so I won't guess the time.
[01:33:42.120 --> 01:33:49.000]   I will tell you, this is the first time that we are using
[01:33:49.000 --> 01:33:52.040]   the WandaVie report and we did it in less than 10 minutes.
[01:33:52.040 --> 01:33:55.320]   It was incredibly easy.
[01:33:55.320 --> 01:34:00.440]   Don't tell my co-workers, I've been skipping on it because I
[01:34:00.440 --> 01:34:02.280]   assumed it'll take like 10, 15 hours.
[01:34:02.280 --> 01:34:04.920]   I'll do something this weekend then.
[01:34:04.920 --> 01:34:07.240]   No, it's super easy to do this.
[01:34:07.240 --> 01:34:11.800]   Anyways, can you go to the second point? Because that's one
[01:34:11.800 --> 01:34:14.280]   of the interesting points that we want to discuss with people.
[01:34:14.280 --> 01:34:16.840]   Yeah, here. Okay, yes.
[01:34:16.840 --> 01:34:21.320]   So, if you look at this graph, you will see on the x-axis,
[01:34:21.320 --> 01:34:22.760]   it's the iteration number.
[01:34:22.760 --> 01:34:25.960]   On the y-axis, it's the validation score, which is the
[01:34:25.960 --> 01:34:26.920]   RMSE score.
[01:34:26.920 --> 01:34:33.400]   If you look at the blue line and then the brown line, they
[01:34:33.400 --> 01:34:34.920]   come from two different experiments.
[01:34:34.920 --> 01:34:41.160]   We evaluate every 10 iterations and when you look at this
[01:34:41.160 --> 01:34:43.400]   graph, Sanyam, you will understand why most people
[01:34:43.400 --> 01:34:47.080]   evaluated every 10 iterations because the performance
[01:34:47.080 --> 01:34:49.960]   drastically changes every other iteration.
[01:34:49.960 --> 01:34:54.120]   The performance just crazily changes every other iteration.
[01:34:54.120 --> 01:34:58.120]   So, when Shahul and I looked at it, we were really, really
[01:34:58.120 --> 01:35:01.640]   worried that transformers are very unstable.
[01:35:01.640 --> 01:35:06.280]   The way how transformers are trained today is very unstable.
[01:35:06.280 --> 01:35:09.000]   So, people evaluate every 10 iterations because it's like
[01:35:09.000 --> 01:35:11.000]   throwing 1000 apples in the dark.
[01:35:12.200 --> 01:35:16.040]   You are shooting in the dark to get an apple or something
[01:35:16.040 --> 01:35:16.540]   like that.
[01:35:16.540 --> 01:35:18.360]   We did not want to do that.
[01:35:18.360 --> 01:35:20.600]   We were very, very uncomfortable doing it.
[01:35:20.600 --> 01:35:25.880]   So, we spent, I told you, we spent over two weeks in
[01:35:25.880 --> 01:35:26.920]   strengthening the code.
[01:35:26.920 --> 01:35:31.320]   When we did that, the other half of the time, we tried to
[01:35:31.320 --> 01:35:33.800]   figure out the basic reason.
[01:35:33.800 --> 01:35:38.600]   We tried to identify the underlying reason why this
[01:35:38.600 --> 01:35:43.800]   happened because I saw in many, most of the other top
[01:35:43.800 --> 01:35:47.080]   performing solutions also evaluated every 10 iterations.
[01:35:47.080 --> 01:35:50.280]   We were one of the very few solutions who did not do it.
[01:35:50.280 --> 01:35:56.600]   We evaluated at the last, we trained for three epochs.
[01:35:56.600 --> 01:35:59.080]   We saved the model at the third epoch and we used it right
[01:35:59.080 --> 01:35:59.320]   away.
[01:35:59.320 --> 01:36:01.720]   We did not use any intermediate evaluation at all.
[01:36:01.720 --> 01:36:03.160]   Got you.
[01:36:03.160 --> 01:36:06.120]   The reason is because of two reasons.
[01:36:06.120 --> 01:36:10.680]   The first one is the experiment 15, which is the blue line
[01:36:10.680 --> 01:36:13.880]   comes from a model that was trained with layer-wise
[01:36:13.880 --> 01:36:14.840]   learning rate decay.
[01:36:14.840 --> 01:36:18.600]   And it was also trained with no dropout.
[01:36:18.600 --> 01:36:22.920]   Apparently, transformers don't like dropout or something
[01:36:22.920 --> 01:36:23.720]   like that.
[01:36:23.720 --> 01:36:27.800]   We did not dig deep into the reason why that was happening.
[01:36:27.800 --> 01:36:31.080]   But this was very weird to us that most of the trainings
[01:36:31.080 --> 01:36:33.160]   that were happening were unstable.
[01:36:33.720 --> 01:36:39.960]   And this is one of the reasons why people had to train 100
[01:36:39.960 --> 01:36:44.040]   models and put it in an ensemble because of the instability
[01:36:44.040 --> 01:36:46.920]   that was raising in the training.
[01:36:46.920 --> 01:36:49.320]   No, but we were uncomfortable.
[01:36:49.320 --> 01:36:51.800]   Shahul and I, we both come from industry.
[01:36:51.800 --> 01:36:54.760]   For us, training two models is already like, ah, no, no, no,
[01:36:54.760 --> 01:36:55.160]   no, no.
[01:36:55.160 --> 01:36:56.600]   It's one model.
[01:36:56.600 --> 01:36:58.280]   Something like that.
[01:36:58.280 --> 01:37:02.360]   We were thinking, should we also have a base and a large
[01:37:02.360 --> 01:37:03.080]   in our ensemble?
[01:37:03.080 --> 01:37:06.040]   We were thinking, no, no, we'll just have one or something
[01:37:06.040 --> 01:37:06.440]   like that.
[01:37:06.440 --> 01:37:07.880]   We were that crazy.
[01:37:07.880 --> 01:37:12.440]   We were because we don't believe in smoothening with an
[01:37:12.440 --> 01:37:12.940]   ensemble.
[01:37:12.940 --> 01:37:16.920]   So this is one weird thing that we identified.
[01:37:16.920 --> 01:37:20.840]   And the top, if you go to the top, you will see another
[01:37:20.840 --> 01:37:21.160]   graph.
[01:37:21.160 --> 01:37:23.480]   This graph.
[01:37:23.480 --> 01:37:28.360]   So these are identical experiments, completely identical
[01:37:28.360 --> 01:37:30.520]   experiments, but completely different results.
[01:37:31.080 --> 01:37:32.760]   Just because of the seats?
[01:37:32.760 --> 01:37:33.960]   Exactly.
[01:37:33.960 --> 01:37:34.460]   Yes.
[01:37:34.460 --> 01:37:39.560]   I mean, even this, this even reflected in the LB, in the
[01:37:39.560 --> 01:37:40.200]   public LB.
[01:37:40.200 --> 01:37:43.320]   And people were going crazy on it.
[01:37:43.320 --> 01:37:46.680]   I think, you know, we believe that the root cause of this
[01:37:46.680 --> 01:37:51.320]   issue is, you know, the number of samples to train on.
[01:37:51.320 --> 01:37:54.040]   It's just 2500 samples.
[01:37:54.040 --> 01:37:58.360]   After that, after what we get, after, you know, splitting
[01:37:58.360 --> 01:38:00.520]   validation, train and validation.
[01:38:01.480 --> 01:38:05.400]   So yeah, this can be the, that can be one of the root causes,
[01:38:05.400 --> 01:38:09.480]   but causes, but yeah, this, you know, if we, if keeping
[01:38:09.480 --> 01:38:13.000]   everything same, just the randoms, just the difference in
[01:38:13.000 --> 01:38:16.120]   random seed cost this huge difference in training.
[01:38:16.120 --> 01:38:16.760]   And yeah.
[01:38:16.760 --> 01:38:20.520]   I'm sorry, I'm laughing at it, but it's, it's quite funny.
[01:38:20.520 --> 01:38:23.240]   I'm sure it took a lot of effort to understand this.
[01:38:23.240 --> 01:38:26.040]   And I just wanted to point out you, how you said you were
[01:38:26.040 --> 01:38:30.440]   interested in ML Ops and be it 1db or any other tool that
[01:38:30.440 --> 01:38:33.160]   sort of this highlights, like as someone, as me, who hasn't
[01:38:33.160 --> 01:38:35.960]   gone to this competition or hasn't understood this, I can
[01:38:35.960 --> 01:38:38.920]   just look at this graph and understand, wow, this is really
[01:38:38.920 --> 01:38:42.600]   incredible by, by you all were doing this and it makes easier
[01:38:42.600 --> 01:38:45.480]   to understand experiments, which is where the real, real
[01:38:45.480 --> 01:38:46.200]   knowledge, right?
[01:38:46.200 --> 01:38:49.560]   No one would know that Dopout isn't something that you
[01:38:49.560 --> 01:38:50.840]   wouldn't apply to transformers.
[01:38:50.840 --> 01:38:52.200]   It's, it's a standard technique, right?
[01:38:52.200 --> 01:38:55.160]   But looking at this graph, it's, it's obvious we shouldn't
[01:38:55.160 --> 01:38:56.280]   be applying Dopout here.
[01:38:57.800 --> 01:38:58.040]   Right.
[01:38:58.040 --> 01:39:01.560]   We can generalize it, but yeah, for this competition, for this
[01:39:01.560 --> 01:39:06.440]   data set for, you know, maybe this, it can, it can be because
[01:39:06.440 --> 01:39:11.160]   of the, you know, number of samples, dropping Dopout,
[01:39:11.160 --> 01:39:13.480]   dropping Dopout was one of the, you know, magic techniques.
[01:39:13.480 --> 01:39:16.460]   Yeah.
[01:39:16.460 --> 01:39:17.480]   That makes sense.
[01:39:17.480 --> 01:39:25.640]   So this was a result from pre-training on the external
[01:39:25.640 --> 01:39:32.040]   data, again, the pink line comes from a pre-trained model.
[01:39:32.040 --> 01:39:36.120]   So you pre-train on a large amount of external data
[01:39:36.120 --> 01:39:40.280]   and the performance worsens.
[01:39:40.280 --> 01:39:45.480]   Whereas you don't do any pre-training, the performance
[01:39:45.480 --> 01:39:48.440]   is really, all of these things are crazy.
[01:39:48.440 --> 01:39:51.720]   But when we submitted it on the public LB, we saw that
[01:39:51.720 --> 01:39:54.280]   it worked.
[01:39:54.280 --> 01:39:55.960]   So pre-training went way ahead.
[01:39:55.960 --> 01:39:57.080]   Yeah.
[01:39:57.080 --> 01:40:02.280]   So what happens is the LB and the CV had no correlation again.
[01:40:02.280 --> 01:40:08.680]   So that's one of the, so this is one of the reasons.
[01:40:08.680 --> 01:40:11.000]   So we see that the CV doesn't improve, then we don't end
[01:40:11.000 --> 01:40:11.640]   up submitting.
[01:40:11.640 --> 01:40:17.000]   But we had some few models, we had a few submissions left
[01:40:17.000 --> 01:40:20.360]   over, so we submitted and then we saw, oh no, it does improve.
[01:40:20.360 --> 01:40:23.000]   So these are all certain things that we learned.
[01:40:23.000 --> 01:40:25.080]   And yeah.
[01:40:25.080 --> 01:40:31.640]   This one is important, so I'll skip that.
[01:40:31.640 --> 01:40:34.680]   But sorry, what does SW stand for?
[01:40:34.680 --> 01:40:38.440]   It's Stochastic Weighted Averaging.
[01:40:38.440 --> 01:40:42.520]   Yeah, it's a state of the art technique which we use
[01:40:42.520 --> 01:40:48.520]   with optimizers to get better generalization while training.
[01:40:48.520 --> 01:40:49.880]   So yeah, we used it.
[01:40:49.880 --> 01:40:52.760]   I mean, it has been there since a long time, but this
[01:40:52.760 --> 01:40:54.600]   is when I got to use it correctly.
[01:40:54.600 --> 01:40:55.480]   Yeah.
[01:40:55.480 --> 01:40:56.600]   Okay.
[01:40:56.600 --> 01:41:00.280]   All right, I think we covered this dashboard.
[01:41:00.280 --> 01:41:02.360]   Let me go back to the solution real quick.
[01:41:02.360 --> 01:41:06.840]   I believe we've covered everything.
[01:41:06.840 --> 01:41:09.080]   Yeah, yeah, it's the same.
[01:41:09.080 --> 01:41:10.600]   I think I repeated it twice, yeah.
[01:41:10.600 --> 01:41:12.520]   Incredible.
[01:41:12.520 --> 01:41:14.920]   All right, I quickly want to point the audience.
[01:41:14.920 --> 01:41:17.720]   I know we ran out of time, we ran overboard actually, but I'll
[01:41:17.720 --> 01:41:19.480]   quickly point the people to your profile.
[01:41:20.040 --> 01:41:25.640]   So if you go to Shahul's profile on Kaggle, you can find his
[01:41:25.640 --> 01:41:27.080]   LinkedIn link off there.
[01:41:27.080 --> 01:41:30.120]   This should take you to his LinkedIn profile.
[01:41:30.120 --> 01:41:31.400]   Make sure you connect with him there.
[01:41:31.400 --> 01:41:32.920]   He's also on Twitter.
[01:41:32.920 --> 01:41:35.160]   Let me see if I can find that tab here.
[01:41:35.160 --> 01:41:37.240]   If not, I'll share the link across later on.
[01:41:37.240 --> 01:41:40.360]   And same goes for Vignesh.
[01:41:40.360 --> 01:41:43.960]   If you hover to his Kaggle profile, you can find his Twitter
[01:41:43.960 --> 01:41:47.080]   and his LinkedIn or his website as well.
[01:41:47.080 --> 01:41:49.880]   And make sure you connect with him on there.
[01:41:50.360 --> 01:41:55.640]   So his Twitter handle is @ptvbhaskaran.
[01:41:55.640 --> 01:41:59.960]   And for Shahul, it's @shahul8786.
[01:41:59.960 --> 01:42:03.320]   I just wanted to point those profiles out.
[01:42:03.320 --> 01:42:05.720]   Anything else that I missed where people can connect with you?
[01:42:05.720 --> 01:42:12.920]   No, Sanyam, we just really, really have to thank you for
[01:42:12.920 --> 01:42:13.720]   organizing this.
[01:42:13.720 --> 01:42:18.520]   I know it's late in India, but thank you very much for
[01:42:18.520 --> 01:42:20.040]   organizing this in a weekend show.
[01:42:20.040 --> 01:42:24.920]   I'm grateful to you that you're giving me your weekend.
[01:42:24.920 --> 01:42:26.600]   Shahul is in India as well.
[01:42:26.600 --> 01:42:28.120]   I know he's at a co-working space.
[01:42:28.120 --> 01:42:31.800]   I'm doing it from the comfort of my chai studio, so I can just
[01:42:31.800 --> 01:42:32.840]   go to the bed right here.
[01:42:32.840 --> 01:42:35.480]   But thank you so much, honestly, for everything you've done
[01:42:35.480 --> 01:42:36.920]   for the community as well, Shahul.
[01:42:36.920 --> 01:42:39.560]   You've been sharing so much incredible stuff.
[01:42:39.560 --> 01:42:41.640]   And for sharing your journey today, both of you.
[01:42:41.640 --> 01:42:41.960]   Thanks.
[01:42:41.960 --> 01:42:42.680]   Thanks for your time.
[01:42:42.680 --> 01:42:44.680]   I learned a lot and I'm sure the audience did.
[01:42:44.680 --> 01:42:46.760]   Thanks for having us.
[01:42:46.760 --> 01:42:48.140]   Thank you.
[01:42:48.860 --> 01:42:49.500]   Yeah, thank you.
[01:42:49.500 --> 01:42:54.780]   Make sure I end the live stream.

