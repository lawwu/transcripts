
[00:00:00.000 --> 00:00:08.000]   [Music]
[00:00:08.000 --> 00:00:10.000]   Do you think we're living in a simulation?
[00:00:10.000 --> 00:00:15.000]   Yes, but it may be unfalsifiable.
[00:00:15.000 --> 00:00:17.000]   What do you mean by unfalsifiable?
[00:00:17.000 --> 00:00:25.000]   So if the simulation is designed in such a way that they did like a formal proof
[00:00:25.000 --> 00:00:27.000]   to show that no information can get in and out,
[00:00:27.000 --> 00:00:33.000]   and if their hardware is designed for anything in the simulation to always keep the hardware in spec,
[00:00:33.000 --> 00:00:36.000]   it may be impossible to prove whether we're in a simulation or not.
[00:00:36.000 --> 00:00:42.000]   So they've designed it such that it's a closed system, you can't get outside the system.
[00:00:42.000 --> 00:00:44.000]   Well, maybe it's one of three worlds.
[00:00:44.000 --> 00:00:46.000]   We're either in a simulation which can be exploited,
[00:00:46.000 --> 00:00:49.000]   we're in a simulation which not only can't be exploited,
[00:00:49.000 --> 00:00:51.000]   but like the same thing is true about VMs.
[00:00:51.000 --> 00:00:55.000]   A really well-designed VM, you can't even detect if you're in a VM or not.
[00:00:56.000 --> 00:00:57.000]   That's brilliant.
[00:00:57.000 --> 00:01:01.000]   So the simulation is running on a virtual machine.
[00:01:01.000 --> 00:01:04.000]   Yeah, but now in reality, all VMs have ways to detect.
[00:01:04.000 --> 00:01:05.000]   That's the point.
[00:01:05.000 --> 00:01:09.000]   I mean, you've done quite a bit of hacking yourself,
[00:01:09.000 --> 00:01:15.000]   and so you should know that really any complicated system will have ways in and out.
[00:01:15.000 --> 00:01:19.000]   So this isn't necessarily true going forward.
[00:01:19.000 --> 00:01:24.000]   I spent my time away from Kama, I learned Kalk.
[00:01:24.000 --> 00:01:28.000]   It's a dependently typed, it's a language for writing math proofs.
[00:01:28.000 --> 00:01:33.000]   And if you write code that compiles in a language like that,
[00:01:33.000 --> 00:01:35.000]   it is correct by definition.
[00:01:35.000 --> 00:01:38.000]   The types check its correctness.
[00:01:38.000 --> 00:01:41.000]   So it's possible that the simulation is written in a language like this,
[00:01:41.000 --> 00:01:43.000]   in which case...
[00:01:43.000 --> 00:01:48.000]   Yeah, but that can't be sufficiently expressive a language like that.
[00:01:48.000 --> 00:01:49.000]   Oh, it can.
[00:01:49.000 --> 00:01:50.000]   It can be?
[00:01:50.000 --> 00:01:51.000]   Oh, yeah.
[00:01:51.000 --> 00:01:52.000]   Okay.
[00:01:53.000 --> 00:01:54.000]   All right, so...
[00:01:54.000 --> 00:01:57.000]   The simulation doesn't have to be Turing complete if it has a scheduled end date.
[00:01:57.000 --> 00:01:59.000]   Looks like it does actually with entropy.
[00:01:59.000 --> 00:02:07.000]   I don't think that a simulation that results in something as complicated as the universe
[00:02:07.000 --> 00:02:11.000]   would have a formal proof of correctness.
[00:02:11.000 --> 00:02:15.000]   It's possible, of course.
[00:02:15.000 --> 00:02:17.000]   We have no idea how good their tooling is,
[00:02:17.000 --> 00:02:21.000]   and we have no idea how complicated the universe computer really is.
[00:02:21.000 --> 00:02:22.000]   It may be quite simple.
[00:02:22.000 --> 00:02:24.000]   It's just very large, right?
[00:02:24.000 --> 00:02:26.000]   It's definitely very large.
[00:02:26.000 --> 00:02:29.000]   But the fundamental rules might be super simple.
[00:02:29.000 --> 00:02:31.000]   Yeah, Conway's Game of Life kind of stuff.
[00:02:31.000 --> 00:02:32.000]   Right.
[00:02:32.000 --> 00:02:35.000]   So if you could hack...
[00:02:35.000 --> 00:02:37.000]   So imagine a simulation that is hackable.
[00:02:37.000 --> 00:02:43.000]   If you could hack it, what would you change about the universe?
[00:02:43.000 --> 00:02:45.000]   How would you approach hacking a simulation?
[00:02:45.000 --> 00:02:48.000]   The reason I gave that talk...
[00:02:48.000 --> 00:02:51.000]   By the way, I'm not familiar with the talk you gave.
[00:02:51.000 --> 00:02:56.000]   I just read that you talked about escaping the simulation or something like that.
[00:02:56.000 --> 00:03:00.000]   So maybe you can tell me a little bit about the theme and the message there too.
[00:03:00.000 --> 00:03:05.000]   It wasn't a very practical talk about how to actually escape a simulation.
[00:03:05.000 --> 00:03:10.000]   It was more about a way of restructuring an us versus them narrative.
[00:03:10.000 --> 00:03:17.000]   If we continue on the path we're going with technology,
[00:03:17.000 --> 00:03:19.000]   I think we're in big trouble.
[00:03:19.000 --> 00:03:24.000]   As a species, and not just as a species, but even as me as an individual member of the species.
[00:03:24.000 --> 00:03:30.000]   So if we could change rhetoric to be more like to think upwards.
[00:03:30.000 --> 00:03:35.000]   Like to think about that we're in a simulation and how we could get out.
[00:03:35.000 --> 00:03:37.000]   Already we'd be on the right path.
[00:03:37.000 --> 00:03:40.000]   What you actually do once you do that,
[00:03:40.000 --> 00:03:43.000]   well, I assume I would have acquired way more intelligence in the process of doing that.
[00:03:43.000 --> 00:03:45.000]   So I'll just ask that.
[00:03:45.000 --> 00:03:53.000]   So the thinking upwards, what kind of breakthrough ideas do you think thinking in that way could inspire?
[00:03:53.000 --> 00:03:55.000]   And why did you say upwards?
[00:03:55.000 --> 00:03:56.000]   Upwards.
[00:03:56.000 --> 00:03:59.000]   Into space? Are you thinking sort of exploration in all forms?
[00:03:59.000 --> 00:04:05.000]   The space narrative that held for the modernist generation
[00:04:05.000 --> 00:04:08.000]   doesn't hold as well for the postmodern generation.
[00:04:08.000 --> 00:04:12.000]   What's the space narrative? Are we talking about the same space?
[00:04:12.000 --> 00:04:14.000]   No, no, space, like going into space.
[00:04:14.000 --> 00:04:19.000]   Like Elon Musk, like we're going to build rockets, we're going to go to Mars, we're going to colonize the universe.
[00:04:19.000 --> 00:04:23.000]   And the narrative you're referring, I was born in the Soviet Union, you're referring to the race to space.
[00:04:23.000 --> 00:04:24.000]   The race to space, yes.
[00:04:24.000 --> 00:04:25.000]   Explore, okay.
[00:04:25.000 --> 00:04:27.000]   That was a great modernist narrative.
[00:04:27.000 --> 00:04:28.000]   Yeah.
[00:04:28.000 --> 00:04:32.000]   It doesn't seem to hold the same weight in today's culture.
[00:04:32.000 --> 00:04:37.000]   I'm hoping for good postmodern narratives that replace it.
[00:04:37.000 --> 00:04:41.000]   So let's think, so you work a lot with AI.
[00:04:41.000 --> 00:04:44.000]   So AI is one formulation of that narrative.
[00:04:44.000 --> 00:04:47.000]   There could be also, I don't know how much you do in VR and AR.
[00:04:47.000 --> 00:04:48.000]   Yeah.
[00:04:48.000 --> 00:04:55.000]   That's another, I know less about it, but every time I play with it in our research, it's fascinating, that virtual world.
[00:04:55.000 --> 00:04:57.000]   Are you interested in the virtual world?
[00:04:57.000 --> 00:05:00.000]   I would like to move to virtual reality.
[00:05:00.000 --> 00:05:02.000]   In terms of your work?
[00:05:02.000 --> 00:05:04.000]   No, I would like to physically move there.
[00:05:04.000 --> 00:05:08.000]   The apartment I can rent in the cloud is way better than the apartment I can rent in the real world.
[00:05:08.000 --> 00:05:10.000]   Well, it's all relative, isn't it?
[00:05:10.000 --> 00:05:14.000]   Because others will have very nice apartments too, so you'll be inferior in the virtual world as well.
[00:05:14.000 --> 00:05:16.000]   No, but that's not how I view the world, right?
[00:05:16.000 --> 00:05:21.000]   I don't view the world, I mean, it's a very like almost zero sum-ish way to view the world.
[00:05:21.000 --> 00:05:25.000]   Say like my great apartment isn't great because my neighbor has one too.
[00:05:25.000 --> 00:05:29.000]   No, my great apartment is great because like, look at this dishwasher, man.
[00:05:29.000 --> 00:05:32.000]   You just touch the dish and it's washed, right?
[00:05:32.000 --> 00:05:37.000]   And that is great in and of itself if I have the only apartment or if everybody had the apartment.
[00:05:37.000 --> 00:05:38.000]   I don't care.
[00:05:38.000 --> 00:05:40.000]   So you have fundamental gratitude.
[00:05:40.000 --> 00:05:41.000]   Yes.
[00:05:41.000 --> 00:05:46.000]   [ Silence ]
[00:05:46.000 --> 00:05:51.000]   [ Silence ]
[00:05:51.000 --> 00:05:56.000]   [ Silence ]
[00:05:56.000 --> 00:06:06.000]   [BLANK_AUDIO]

