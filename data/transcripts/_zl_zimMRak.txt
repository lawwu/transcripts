
[00:00:00.500 --> 00:00:03.360]   JASPREET BOOTH:
[00:00:03.360 --> 00:00:16.480]   Hi, I'm Jaspreet.
[00:00:16.480 --> 00:00:18.460]   I'm a senior staff engineer at Intuit.
[00:00:18.460 --> 00:00:21.040]   I work on Gen AI for TurboTax.
[00:00:21.040 --> 00:00:22.760]   And today, we'll be talking about how
[00:00:22.760 --> 00:00:28.240]   we use LLMs at Intuit to help you understand your taxes better.
[00:00:28.240 --> 00:00:33.820]   So I think just to understand the scale, right?
[00:00:33.820 --> 00:00:36.300]   Intuit TurboTax successfully processed
[00:00:36.300 --> 00:00:40.600]   44 million tax returns for tax year 23.
[00:00:40.600 --> 00:00:42.800]   And that's really the scale we're going for.
[00:00:42.800 --> 00:00:45.680]   We want everybody to have high confidence in how
[00:00:45.680 --> 00:00:48.480]   their taxes are filed and understand them,
[00:00:48.480 --> 00:00:52.640]   that they're getting the best deductions that they can.
[00:00:52.640 --> 00:01:01.440]   So this is the experience that we work on.
[00:01:01.440 --> 00:01:04.320]   So you go into TurboTax.
[00:01:04.320 --> 00:01:06.520]   You enter your information.
[00:01:06.520 --> 00:01:09.980]   Then you go through what credits you are eligible for and so on.
[00:01:09.980 --> 00:01:14.300]   And we basically help you expand on to how
[00:01:14.300 --> 00:01:16.400]   you are getting the tax breaks that you are,
[00:01:16.400 --> 00:01:18.900]   help you understand them better, and so on.
[00:01:18.900 --> 00:01:26.760]   And this is another example.
[00:01:26.760 --> 00:01:29.280]   This is basically the overall tax outcome.
[00:01:29.280 --> 00:01:31.500]   Like, what is your overall refund for this year?
[00:01:31.500 --> 00:01:41.340]   Now, Intuit's Gen AI experiences are built on top of our
[00:01:41.340 --> 00:01:42.360]   propriety Gen OS.
[00:01:42.360 --> 00:01:45.700]   That's the generative iOS that we have built as a platform
[00:01:45.700 --> 00:01:46.820]   capability.
[00:01:46.820 --> 00:01:52.980]   And it has a lot of different pieces that you see over here.
[00:01:52.980 --> 00:01:56.380]   The key goal is that we found that a lot of the Gen OS
[00:01:56.380 --> 00:01:59.000]   tooling that comes out of the box is not supporting
[00:01:59.000 --> 00:02:00.140]   all our use cases.
[00:02:00.140 --> 00:02:02.780]   We want to-- most prominently working in tax.
[00:02:02.780 --> 00:02:04.660]   We are in the regulatory business.
[00:02:04.660 --> 00:02:09.080]   Safety, security is very, very important.
[00:02:09.080 --> 00:02:10.420]   So we want to focus on that.
[00:02:10.420 --> 00:02:13.300]   At the same time, we want to build a piece that a company
[00:02:13.300 --> 00:02:15.800]   at the scale of Intuit can use end to end,
[00:02:15.800 --> 00:02:17.260]   a really large scale.
[00:02:17.260 --> 00:02:19.880]   So that's where Gen OS comes in.
[00:02:19.880 --> 00:02:21.020]   We have different pieces.
[00:02:21.020 --> 00:02:23.760]   There's on the UI side, which is the Gen UX.
[00:02:23.760 --> 00:02:24.860]   Then there's orchestrator.
[00:02:24.860 --> 00:02:27.260]   That's basically the piece where different teams are
[00:02:27.260 --> 00:02:29.480]   working on different components, different pieces,
[00:02:29.480 --> 00:02:30.620]   different LLM solutions.
[00:02:30.620 --> 00:02:32.000]   How do you find the right solution
[00:02:32.000 --> 00:02:33.800]   to answer the right question?
[00:02:33.800 --> 00:02:37.040]   And Intuit calls the entire experience
[00:02:37.040 --> 00:02:39.380]   that we powered through this Intuit Assist.
[00:02:39.380 --> 00:02:42.380]   So I'm going to deep dive into specific pieces
[00:02:42.380 --> 00:02:45.360]   that our team used to build out the experience
[00:02:45.360 --> 00:02:46.200]   for terror attacks.
[00:02:46.200 --> 00:02:54.120]   So as I said earlier, we have millions and millions of customers
[00:02:54.120 --> 00:02:54.860]   who are coming in.
[00:02:54.860 --> 00:02:56.920]   So we're trying to build a scalable solution that
[00:02:56.920 --> 00:02:58.440]   can work end to end.
[00:02:58.440 --> 00:02:59.660]   So on the slide here, I'm basically
[00:02:59.660 --> 00:03:01.660]   going to talk about different pieces that
[00:03:01.660 --> 00:03:03.820]   are powering the experience.
[00:03:03.820 --> 00:03:05.820]   Of course, to begin with, the first iteration
[00:03:05.820 --> 00:03:09.800]   was the prompt tooling, basically a prompt-based solution
[00:03:09.800 --> 00:03:12.700]   to try and go through what's your tax situation going on.
[00:03:12.700 --> 00:03:15.040]   Let's take an example of what I was showing earlier, which
[00:03:15.040 --> 00:03:16.660]   was your tax refund.
[00:03:16.660 --> 00:03:19.540]   So your tax refund has many constituents.
[00:03:19.540 --> 00:03:20.660]   These are your deductions.
[00:03:20.660 --> 00:03:21.460]   These are your credits.
[00:03:21.460 --> 00:03:24.860]   The standard deduction, W2B holding, and so on.
[00:03:24.860 --> 00:03:27.260]   So we want to make sure that you understand all of that.
[00:03:27.260 --> 00:03:30.280]   So we built a prompt-based solution around it
[00:03:30.280 --> 00:03:31.940]   and work from there.
[00:03:31.940 --> 00:03:34.240]   The production model that we went
[00:03:34.240 --> 00:03:37.500]   with is Claude for this use case.
[00:03:37.500 --> 00:03:41.580]   Intuit is one of the biggest users of Claude.
[00:03:41.580 --> 00:03:44.360]   We had a multimillion-dollar contract for this year as well.
[00:03:44.360 --> 00:03:46.700]   And you'll also see OpenAI over there.
[00:03:46.700 --> 00:03:50.620]   So OpenAI is where we used for other question and answering.
[00:03:50.620 --> 00:03:52.360]   So you'll see on the slide, we're talking
[00:03:52.360 --> 00:03:54.100]   about static and dynamic type of queries.
[00:03:54.100 --> 00:03:58.480]   So static queries would be what I was showing earlier,
[00:03:58.480 --> 00:04:01.660]   that we know you are looking at your summary.
[00:04:01.660 --> 00:04:04.260]   You want to see what happened overall.
[00:04:04.260 --> 00:04:05.740]   So that would be a static prompt.
[00:04:05.740 --> 00:04:08.740]   Think of it like a prepared statement.
[00:04:08.740 --> 00:04:11.240]   However, the additional information that we're gathering
[00:04:11.240 --> 00:04:13.660]   is the tax info for when the user comes in.
[00:04:13.660 --> 00:04:16.960]   Now, dynamic query would be users have questions
[00:04:16.960 --> 00:04:18.880]   about the tax situation.
[00:04:18.880 --> 00:04:20.360]   Can I deduct my dog?
[00:04:20.360 --> 00:04:22.700]   Well, you can't, but you can try.
[00:04:22.700 --> 00:04:24.820]   So things like that, that's what we're
[00:04:24.820 --> 00:04:27.980]   trying to answer more dynamically.
[00:04:27.980 --> 00:04:30.680]   OpenAI, as GPT-4 mini, had been the model of choice
[00:04:30.680 --> 00:04:31.820]   for until a few months ago.
[00:04:31.820 --> 00:04:34.120]   We are now iterating on the newer versions.
[00:04:34.120 --> 00:04:36.620]   Of course, models change every year--
[00:04:36.620 --> 00:04:38.600]   every month, I should say.
[00:04:38.600 --> 00:04:41.900]   So we're trying to focus on that.
[00:04:41.900 --> 00:04:43.620]   Same for the dynamic piece again.
[00:04:43.620 --> 00:04:47.520]   Another important aspect is tax information.
[00:04:47.520 --> 00:04:49.680]   IRS changes forms every year.
[00:04:49.680 --> 00:04:52.920]   Intuit has proprietary tax information, tax engines
[00:04:52.920 --> 00:04:54.240]   that we want to use.
[00:04:54.240 --> 00:04:56.860]   So we have a rack-based and, of course,
[00:04:56.860 --> 00:04:59.200]   graph-rack-based solutions around it as well.
[00:04:59.200 --> 00:05:03.240]   So they help us answer users' questions much better.
[00:05:03.240 --> 00:05:06.420]   And one thing that we also piloted recently
[00:05:06.420 --> 00:05:08.700]   was actually having a fine-tuned LLM.
[00:05:08.700 --> 00:05:11.880]   So we went to Claude because that's
[00:05:11.880 --> 00:05:13.620]   the primary one we are using there.
[00:05:13.620 --> 00:05:15.780]   And we stuck to static queries, and we tested it out.
[00:05:15.780 --> 00:05:18.840]   And it does well.
[00:05:18.840 --> 00:05:20.520]   It definitely does well.
[00:05:20.520 --> 00:05:22.020]   Quality is there.
[00:05:22.020 --> 00:05:25.200]   It takes effort to fine-tune the model.
[00:05:25.200 --> 00:05:28.140]   However, we found that it was a little too specialized
[00:05:28.140 --> 00:05:29.820]   in the specific use case.
[00:05:29.820 --> 00:05:33.180]   And one thing I want to highlight a deep dive further on
[00:05:33.180 --> 00:05:34.340]   is evals.
[00:05:34.340 --> 00:05:38.840]   So you want to make sure that we evaluate everything we do.
[00:05:38.840 --> 00:05:40.900]   You want to make sure what's happening in production.
[00:05:40.900 --> 00:05:43.360]   You want to make sure in the development lifecycle,
[00:05:43.360 --> 00:05:45.320]   you're doing everything you need to do to make sure
[00:05:45.320 --> 00:05:48.620]   that you have the best prompts out there.
[00:05:48.620 --> 00:05:52.160]   And with that, moving on to the next slide.
[00:05:52.160 --> 00:05:54.440]   So I'll just summarize it a little bit.
[00:05:54.440 --> 00:05:56.600]   These are the key pillars that we have.
[00:05:56.600 --> 00:05:58.300]   I already spoke about some of them before.
[00:05:58.300 --> 00:06:00.620]   I want to highlight here that--
[00:06:00.620 --> 00:06:02.580]   the bottom part in this slide, actually--
[00:06:02.580 --> 00:06:03.820]   the human domain expert.
[00:06:03.820 --> 00:06:06.680]   So Intuit has a lot of tax analysts
[00:06:06.680 --> 00:06:10.980]   that we work with, of course, that work with us,
[00:06:10.980 --> 00:06:14.600]   decoding changes year over year, making changes, and so on.
[00:06:14.600 --> 00:06:18.300]   So they are the experts that provide us the information,
[00:06:18.300 --> 00:06:21.580]   make sure the evaluations are correctly done.
[00:06:21.580 --> 00:06:23.880]   So we have a phased evaluation system.
[00:06:23.880 --> 00:06:25.800]   We have manual evaluations initially
[00:06:25.800 --> 00:06:28.660]   in the development lifecycle.
[00:06:28.660 --> 00:06:30.480]   And another thing that we have done is actually
[00:06:30.480 --> 00:06:33.040]   using the tax analysts as the prompt engineers.
[00:06:33.040 --> 00:06:35.940]   So that allows us, the folks in data science and ML world,
[00:06:35.940 --> 00:06:40.440]   to actually focus on the quality, defining the metrics,
[00:06:40.440 --> 00:06:43.200]   making sure we have a nice data set that we can iterate on
[00:06:43.200 --> 00:06:45.020]   and test on.
[00:06:45.020 --> 00:06:47.280]   As we go along, as I said, models change.
[00:06:47.280 --> 00:06:49.120]   We want to try out different models.
[00:06:49.120 --> 00:06:54.960]   We want to see the loss change in the IRS, say tax year 23 to 24,
[00:06:54.960 --> 00:06:56.160]   what happened.
[00:06:56.160 --> 00:06:59.840]   So those changes, we focus on that.
[00:06:59.840 --> 00:07:02.020]   And human experts bring their expertise
[00:07:02.020 --> 00:07:04.800]   and are able to both help with prompt engineering
[00:07:04.800 --> 00:07:07.660]   and get the initial evaluations done.
[00:07:07.660 --> 00:07:11.600]   That then becomes the basis for automated evaluations.
[00:07:11.600 --> 00:07:14.440]   LLM as a judge is what we use as well.
[00:07:14.440 --> 00:07:18.880]   I'm going to talk a little bit more about that.
[00:07:18.880 --> 00:07:23.060]   I'm going to take-- going back then to what I was telling earlier
[00:07:23.060 --> 00:07:25.580]   about the Cloud3 Haiku and fine tuning.
[00:07:25.580 --> 00:07:31.400]   So fine tuning-- as part of GenOS, we built out a lot of tool
[00:07:31.400 --> 00:07:32.260]   sets.
[00:07:32.260 --> 00:07:35.020]   One more thing that we want to do is support fine tuning.
[00:07:35.020 --> 00:07:38.060]   So for our use case, we actually stuck to just fine tuning
[00:07:38.060 --> 00:07:41.020]   on Cloud3 Haiku powered by AWS Bedrock.
[00:07:41.020 --> 00:07:43.540]   And the goal there was that we wanted
[00:07:43.540 --> 00:07:47.900]   to see if we can actually improve the quality of responses.
[00:07:47.900 --> 00:07:52.780]   Biggest driver, of course, is fewer instructions needed
[00:07:52.780 --> 00:07:55.620]   once you have fine tuned the model.
[00:07:55.620 --> 00:07:57.000]   Latencies are a big concern.
[00:07:57.000 --> 00:07:59.740]   So we want to see if we can squeeze down the prompt size,
[00:07:59.740 --> 00:08:03.160]   and at the same time keep the quality that we need
[00:08:03.160 --> 00:08:05.120]   and keep going there.
[00:08:05.120 --> 00:08:07.280]   So this is roughly what it looks like.
[00:08:07.280 --> 00:08:11.020]   We build out-- we have different tests, AWS accounts,
[00:08:11.020 --> 00:08:15.440]   different environments that are provided by the platform teams
[00:08:15.440 --> 00:08:16.640]   that we work with.
[00:08:16.640 --> 00:08:21.100]   We look at the data and brief not to regulations--
[00:08:21.100 --> 00:08:23.500]   7 to 16 regulations.
[00:08:23.500 --> 00:08:26.380]   So we only use consented data from users.
[00:08:26.380 --> 00:08:29.700]   Make sure we're on the right.
[00:08:29.700 --> 00:08:33.740]   And to double down on the evaluation part, right?
[00:08:33.740 --> 00:08:34.960]   You want to evaluate everything.
[00:08:34.960 --> 00:08:38.760]   So the key pillars are accuracy, relevancy, and coherence.
[00:08:38.760 --> 00:08:41.660]   So we have both manual and automated systems.
[00:08:41.660 --> 00:08:44.760]   We also have broad monitoring automated systems,
[00:08:44.760 --> 00:08:49.180]   basically look at sample data on what the LLM is basically
[00:08:49.180 --> 00:08:51.540]   giving real users in real time.
[00:08:51.540 --> 00:08:56.760]   And for this tooling that we've built out here, LLM as a judge
[00:08:56.760 --> 00:08:58.700]   comes in, in the auto-eval side.
[00:08:58.700 --> 00:09:02.360]   We've also developed some tooling in-house
[00:09:02.360 --> 00:09:05.860]   to basically do some automated prompt change during.
[00:09:05.860 --> 00:09:11.140]   And that actually really helps to update our LLM as a judge.
[00:09:11.140 --> 00:09:14.240]   Basically, LLM as a judge operates on top of a prompt.
[00:09:14.240 --> 00:09:16.140]   It needs different information.
[00:09:16.140 --> 00:09:18.180]   It needs some manual samples, which
[00:09:18.180 --> 00:09:20.020]   are the golden data set.
[00:09:20.020 --> 00:09:24.040]   We use AWS Ground Truth for that and take on that.
[00:09:24.040 --> 00:09:27.500]   One more thing that I want to highlight here is models.
[00:09:27.500 --> 00:09:32.900]   So we made the move from Anthropic Cloud Instant
[00:09:32.900 --> 00:09:37.960]   to Anthropic Cloud Haiku for the next year for Taxia 24.
[00:09:37.960 --> 00:09:39.900]   And that takes some effort.
[00:09:39.900 --> 00:09:41.900]   And the only way it's possible is because we
[00:09:41.900 --> 00:09:45.440]   have clear evals in place so that we can test out
[00:09:45.440 --> 00:09:46.940]   whatever we are changing.
[00:09:46.940 --> 00:09:49.980]   And model changes are not as smooth as you would think.
[00:09:49.980 --> 00:09:56.540]   These are some more details on what
[00:09:56.540 --> 00:09:58.580]   we're talking about on the automated evals.
[00:09:58.580 --> 00:10:03.240]   As you can see, the key output is we
[00:10:03.240 --> 00:10:04.700]   want to make sure it stacks accurate.
[00:10:04.700 --> 00:10:08.040]   That's the main thing we want to aim for and focus on that.
[00:10:08.040 --> 00:10:09.440]   I'm going to move on here.
[00:10:09.440 --> 00:10:12.600]   So let's talk about some major learnings that we have.
[00:10:12.600 --> 00:10:15.800]   So the contracts are really expensive.
[00:10:15.800 --> 00:10:18.320]   And the only way they are slightly cheaper
[00:10:18.320 --> 00:10:20.260]   if you have long-term contracts.
[00:10:20.260 --> 00:10:23.180]   So you are tied into the vendor.
[00:10:23.180 --> 00:10:27.620]   So it helps to have strong partners on the vendor side
[00:10:27.620 --> 00:10:31.320]   who work with you to help iterate, help improve.
[00:10:31.320 --> 00:10:34.100]   And I think I was in this conference last year.
[00:10:34.100 --> 00:10:36.980]   And this was one thing called out then, as well, that
[00:10:36.980 --> 00:10:40.140]   essentially vendors are a form of lock-in.
[00:10:40.140 --> 00:10:41.920]   The prompts are a form of lock-in.
[00:10:41.920 --> 00:10:43.220]   It's not easy.
[00:10:43.220 --> 00:10:46.020]   And we found out it's not even easy to upgrade this model
[00:10:46.020 --> 00:10:48.480]   from the same vendor going into the next year.
[00:10:48.480 --> 00:10:51.560]   So we want to focus on that.
[00:10:51.560 --> 00:10:54.220]   Another thing I really want to highlight here is the latency.
[00:10:54.220 --> 00:11:00.520]   So LLM models, of course, they don't have the SLAs of back-end services.
[00:11:00.520 --> 00:11:03.460]   We're not looking at 100 milliseconds, 200 milliseconds.
[00:11:03.460 --> 00:11:06.720]   We're talking about three seconds, five seconds, 10 seconds.
[00:11:06.720 --> 00:11:11.460]   So as the user's tax information comes in, maybe they
[00:11:11.460 --> 00:11:15.020]   have a complicated situation like me that they own a home.
[00:11:15.020 --> 00:11:16.660]   They have maybe something in stocks.
[00:11:16.660 --> 00:11:18.660]   And they're trying to file--
[00:11:18.660 --> 00:11:20.720]   they have-- their spouse have their jobs as well.
[00:11:20.720 --> 00:11:21.880]   A lot of things going on.
[00:11:21.880 --> 00:11:25.140]   So the prompts really balloon up if you're
[00:11:25.140 --> 00:11:26.740]   trying to figure out the outcome.
[00:11:26.740 --> 00:11:29.720]   And as you go into tax day, everybody's
[00:11:29.720 --> 00:11:31.280]   trying to file on tax day, right?
[00:11:31.280 --> 00:11:32.040]   April 15.
[00:11:32.040 --> 00:11:37.440]   So latency really is shooting through the roof.
[00:11:37.440 --> 00:11:40.220]   So we design a product around that.
[00:11:40.220 --> 00:11:44.520]   We want to make sure we have the right fallback mechanisms,
[00:11:44.520 --> 00:11:48.280]   the right user design, product design,
[00:11:48.280 --> 00:11:50.020]   to make sure that the user experience
[00:11:50.020 --> 00:11:52.380]   is seamless and useful.
[00:11:52.380 --> 00:11:54.520]   We want to make sure that the explanations are helpful
[00:11:54.520 --> 00:11:56.400]   more than anything else.
[00:11:56.400 --> 00:11:59.420]   And I think I covered all the other places.
[00:11:59.420 --> 00:12:01.800]   But once again, I cannot say that enough.
[00:12:01.800 --> 00:12:03.580]   Evals are a must-do launch.
[00:12:03.580 --> 00:12:04.480]   Focus on evals.
[00:12:04.480 --> 00:12:07.800]   Make sure you have clear guidelines on what you're building.
[00:12:07.800 --> 00:12:10.200]   Have clear golden data set.
[00:12:10.200 --> 00:12:11.800]   I've heard that from other talks as well.
[00:12:11.800 --> 00:12:13.020]   That's really a key point.
[00:12:13.020 --> 00:12:16.780]   That's all.
[00:12:16.780 --> 00:12:20.360]   I'm going to pause here for questions.
[00:12:20.360 --> 00:12:22.120]   If you're going to be asking questions,
[00:12:22.120 --> 00:12:23.520]   please come to one of the microphones
[00:12:23.520 --> 00:12:25.180]   so that we can capture the audio.
[00:12:25.180 --> 00:12:25.680]   Thank you.
[00:12:25.680 --> 00:12:31.160]   Yeah, hi.
[00:12:31.160 --> 00:12:34.200]   You said evaluate everything, right?
[00:12:34.200 --> 00:12:38.580]   But with Gen AI systems, there could be very small changes.
[00:12:38.580 --> 00:12:40.820]   You're going to make a small change to a prompt.
[00:12:40.820 --> 00:12:43.820]   And evaluations can get very expensive
[00:12:43.820 --> 00:12:47.740]   or slow down your whole development process, right?
[00:12:47.740 --> 00:12:50.160]   So maybe could you dive a little bit deeper
[00:12:50.160 --> 00:12:52.980]   into when do you bring in different types of evaluations?
[00:12:52.980 --> 00:12:56.160]   Are there anything that you just say,
[00:12:56.160 --> 00:12:59.540]   we ran some regression tests, and it looks fine, so you launch?
[00:12:59.540 --> 00:13:02.160]   Or do you always go with an expert?
[00:13:02.160 --> 00:13:03.420]   Sure, sure.
[00:13:03.420 --> 00:13:04.760]   Thank you for the question.
[00:13:04.760 --> 00:13:08.260]   So just to reiterate, so the evaluations are different types.
[00:13:08.260 --> 00:13:10.420]   I would say when we are in the initial phase of development,
[00:13:10.420 --> 00:13:14.300]   we are looking more on the manual relations with tax experts
[00:13:14.300 --> 00:13:15.880]   so we can get a baseline in place.
[00:13:15.880 --> 00:13:18.600]   Then as we are tweaking different things in the prompts,
[00:13:18.600 --> 00:13:20.700]   that's where auto-evaluation comes in.
[00:13:20.700 --> 00:13:25.540]   So we basically take the input from the tax experts
[00:13:25.540 --> 00:13:30.120]   and use that to train a judge prompt for the LLM.
[00:13:30.120 --> 00:13:32.660]   So that LLM is, once again, expensive.
[00:13:32.660 --> 00:13:36.000]   We would go for the GPT-4 series until recently on that one.
[00:13:36.000 --> 00:13:39.940]   And then minor iterations we can do with auto-eval.
[00:13:39.940 --> 00:13:42.300]   So we have clear understanding with product.
[00:13:42.300 --> 00:13:44.640]   We want to make sure that the quality is there.
[00:13:44.640 --> 00:13:46.380]   And maybe once we have major changes,
[00:13:46.380 --> 00:13:49.740]   for example, we went from tax year 23 to tax year 24,
[00:13:49.740 --> 00:13:51.840]   then we definitely reiterate.
[00:13:51.840 --> 00:13:55.120]   If the prompt changes a lot, we would go for manual evaluations.
[00:13:55.120 --> 00:13:59.940]   Thank you for the technical deep dive.
[00:13:59.940 --> 00:14:02.620]   I was more interested in the product side of it.
[00:14:02.620 --> 00:14:03.120]   Sure.
[00:14:03.120 --> 00:14:06.080]   We also do taxes, so I was curious.
[00:14:06.080 --> 00:14:08.600]   What are the kind of LLM interactions
[00:14:08.600 --> 00:14:09.960]   that the users are having?
[00:14:09.960 --> 00:14:12.200]   What are the kind of questions they're asking?
[00:14:12.200 --> 00:14:15.380]   Is it more like critical parts of the workflow,
[00:14:15.380 --> 00:14:18.200]   or more like what are my taxes?
[00:14:18.200 --> 00:14:22.240]   So we have question answering for all types of questions.
[00:14:22.240 --> 00:14:24.260]   That includes both the product question,
[00:14:24.260 --> 00:14:26.940]   as in how do I do this in TurboTax,
[00:14:26.940 --> 00:14:29.120]   or also their tax situation.
[00:14:29.120 --> 00:14:33.600]   So for example, I paid the tuition from my grandchild.
[00:14:33.600 --> 00:14:35.420]   Can I claim that on my taxes?
[00:14:35.420 --> 00:14:36.640]   So things like that.
[00:14:36.640 --> 00:14:39.020]   So our goal is we have different teams going
[00:14:39.020 --> 00:14:39.960]   after different pieces.
[00:14:39.960 --> 00:14:42.200]   Our goal is we want to answer all of these questions.
[00:14:42.200 --> 00:14:46.640]   And accordingly, different types of questions
[00:14:46.640 --> 00:14:47.700]   need different solutions.
[00:14:47.700 --> 00:14:53.020]   And that's where maybe I would reiterate, go back to here.
[00:14:53.020 --> 00:15:01.100]   So this piece here, planner.
[00:15:01.100 --> 00:15:02.800]   So essentially, this is where it comes in.
[00:15:02.800 --> 00:15:04.400]   We want to make sure when the query comes in,
[00:15:04.400 --> 00:15:06.380]   we understand what the user is trying to ask.
[00:15:06.380 --> 00:15:08.880]   And then we have different kind of solutions for different kind
[00:15:08.880 --> 00:15:11.880]   of questions and go through that.
[00:15:11.880 --> 00:15:16.320]   So you mentioned about the evaluation.
[00:15:16.320 --> 00:15:17.380]   So one quick question.
[00:15:17.380 --> 00:15:21.120]   So TurboTax, I'm sure it involves a lot of numbers, the answers.
[00:15:21.120 --> 00:15:24.020]   So how do you verify those numbers in terms of the evaluation?
[00:15:24.020 --> 00:15:28.320]   Let's say the actual tax number is 11,235.
[00:15:28.320 --> 00:15:30.220]   And if it's something like 11,100.
[00:15:30.220 --> 00:15:33.820]   So it's quite difficult to catch this with a manual evaluation.
[00:15:33.820 --> 00:15:34.320]   Yes.
[00:15:34.320 --> 00:15:34.820]   Yes.
[00:15:34.820 --> 00:15:35.320]   Yes.
[00:15:35.320 --> 00:15:36.420]   Thank you for the question.
[00:15:36.420 --> 00:15:38.320]   So that's the key thing that we work on.
[00:15:38.320 --> 00:15:41.140]   So TurboTax, of course, has a tax knowledge engine
[00:15:41.140 --> 00:15:44.080]   that we have built proprietary in-house, managed over the years,
[00:15:44.080 --> 00:15:45.240]   built and developed.
[00:15:45.240 --> 00:15:47.180]   And that's really what's providing these numbers.
[00:15:47.180 --> 00:15:50.260]   The tax profile information is all coming from these numbers.
[00:15:50.260 --> 00:15:53.260]   We are not having LLMs do the calculations at all.
[00:15:53.260 --> 00:15:55.900]   We're basically using the ground truth that is already
[00:15:55.900 --> 00:15:58.780]   existing in our systems as the numbers that we see.
[00:15:58.780 --> 00:16:01.420]   And we have safety guardrails.
[00:16:01.420 --> 00:16:04.660]   Maybe this piece here I would probably call out.
[00:16:04.660 --> 00:16:06.460]   We have a lot of safety guardrails on what's
[00:16:06.460 --> 00:16:08.220]   the raw LLM response.
[00:16:08.220 --> 00:16:11.380]   Make sure we are not hallucinating numbers
[00:16:11.380 --> 00:16:12.980]   before we send to the user.
[00:16:12.980 --> 00:16:13.720]   Got it.
[00:16:13.720 --> 00:16:17.180]   So the data is coming from the tax engine itself.
[00:16:17.180 --> 00:16:19.540]   But when you formulate the final explanation,
[00:16:19.540 --> 00:16:20.800]   the answer itself.
[00:16:20.800 --> 00:16:23.200]   So how do you make sure that the numbers
[00:16:23.200 --> 00:16:25.480]   that actually in the final answer are, you know,
[00:16:25.480 --> 00:16:27.360]   the same as that's coming from data?
[00:16:27.360 --> 00:16:29.320]   So basically, we have ML models that
[00:16:29.320 --> 00:16:32.560]   are working under the hood as part of the security aspect
[00:16:32.560 --> 00:16:35.200]   that you see here that basically make sure we did not
[00:16:35.200 --> 00:16:37.020]   hallucinate any numbers that we built on.
[00:16:37.020 --> 00:16:37.520]   Got it.
[00:16:37.520 --> 00:16:38.020]   Yeah.
[00:16:38.020 --> 00:16:38.520]   Thank you.
[00:16:38.520 --> 00:16:48.480]   Could you give an overview of how you use both just
[00:16:48.480 --> 00:16:51.320]   a traditional rack and graph rack, like a hybrid,
[00:16:51.320 --> 00:16:52.140]   in your workflow?
[00:16:52.140 --> 00:16:53.140]   Sure, sure.
[00:16:53.140 --> 00:16:53.640]   And sorry.
[00:16:53.640 --> 00:16:57.300]   One more question is, now with the new model cloud 4 coming out,
[00:16:57.300 --> 00:16:59.580]   do you think the fine tuning might be getting easier
[00:16:59.580 --> 00:17:01.260]   where it needs needed?
[00:17:01.260 --> 00:17:02.980]   I'll take the first one.
[00:17:02.980 --> 00:17:05.940]   So Graph Rack, I think we've definitely
[00:17:05.940 --> 00:17:09.980]   seen better response quality with Graph Rack.
[00:17:09.980 --> 00:17:14.220]   Even more than that, though, I think for end user helpfulness,
[00:17:14.220 --> 00:17:18.420]   getting personalized answer is the key piece, I would say.
[00:17:18.420 --> 00:17:23.640]   Graph Rack definitely outperforms regular Rack.
[00:17:23.640 --> 00:17:27.260]   And what even more outperforms is personalizing the answers?
[00:17:27.260 --> 00:17:29.760]   And to your second question, we are constantly evaluating
[00:17:29.760 --> 00:17:31.860]   the models.
[00:17:31.860 --> 00:17:34.500]   This is really the time that April is just behind us.
[00:17:34.500 --> 00:17:36.600]   We are trying to look at what new things we can do.
[00:17:36.600 --> 00:17:40.360]   We also have some in-house models that Intuit trains and develops.
[00:17:40.360 --> 00:17:44.520]   So we are constantly evaluating, and I don't have an answer now
[00:17:44.520 --> 00:17:49.800]   what we'll do for the next tax year, but yes, we keep working on that.
[00:17:49.800 --> 00:17:53.180]   You mentioned you have different situations, tax situations,
[00:17:53.180 --> 00:17:55.020]   and you come up with an answer.
[00:17:55.020 --> 00:18:00.520]   So if I describe my situation, it's complicated and it comes up with an answer.
[00:18:00.520 --> 00:18:06.520]   Is that answer being generated using the LLM, or is it going back to the tax engine?
[00:18:06.520 --> 00:18:10.620]   And how do you explain how you came up with that answer?
[00:18:10.620 --> 00:18:16.200]   And I assume there's going to be a lot of legal challenges to the wrong answers.
[00:18:16.200 --> 00:18:16.800]   Right, right, right, absolutely.
[00:18:16.800 --> 00:18:22.060]   I mean, Intuit focuses heavily on legal and privacy controls.
[00:18:22.060 --> 00:18:27.520]   So, the solution for this one, right, what we worked on here, this is specific,
[00:18:27.520 --> 00:18:29.600]   this is more of the static variety of questions.
[00:18:29.600 --> 00:18:35.280]   So once again, what I was saying earlier, the inherent numbers are coming in from tax knowledge engine,
[00:18:35.280 --> 00:18:38.840]   and we have tax experts who actually crafted these prompts.
[00:18:38.840 --> 00:18:42.540]   So they are specifically tested for each piece that you see here.
[00:18:42.540 --> 00:18:47.580]   So, that's basically when we do the evals, we make sure it doesn't happen what you're suggesting.
[00:18:47.580 --> 00:18:50.580]   Okay, great.
[00:18:50.580 --> 00:18:51.080]   Thank you.
[00:18:51.080 --> 00:18:52.680]   Thank you so much. What a great talk.
[00:18:52.680 --> 00:18:58.480]   Thank you.

