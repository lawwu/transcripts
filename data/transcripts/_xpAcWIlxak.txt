
[00:00:00.000 --> 00:00:06.300]   Once you start dropping a number of parameters, you have to start being a little bit more
[00:00:06.300 --> 00:00:12.080]   intelligent about the curation of the data because you really only get so much data per
[00:00:12.080 --> 00:00:14.040]   parameter in these models.
[00:00:14.040 --> 00:00:18.420]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:18.420 --> 00:00:20.440]   and I'm your host, Lukas Biewald.
[00:00:20.440 --> 00:00:26.440]   Brandon Duderstadt is the CEO and co-founder of Gnomic, maker of GPT4ALL, which is one
[00:00:26.440 --> 00:00:29.400]   of the fastest growing open source projects of all time.
[00:00:29.400 --> 00:00:35.120]   We talk about curating training data for LLMs, evaluating LLM performance in the wild, and
[00:00:35.120 --> 00:00:36.120]   he asked me for life advice.
[00:00:36.120 --> 00:00:37.720]   I hope you enjoyed this episode.
[00:00:37.720 --> 00:00:45.800]   So, Brandon, I feel like you're best known, at least to me, for being the creator of GPT4ALL,
[00:00:45.800 --> 00:00:49.600]   but you also run Gnomic, which is the company behind this.
[00:00:49.600 --> 00:00:54.320]   Could you give us a little rundown on what GPT4ALL is and what the company is and how
[00:00:54.320 --> 00:00:55.760]   you guys fit that together?
[00:00:55.760 --> 00:00:57.120]   Yeah, for sure.
[00:00:57.120 --> 00:00:59.440]   It's a misnomer to call me the creator.
[00:00:59.440 --> 00:01:04.840]   Really, I would say my co-founder, Andre, was the one that really pushed it forward
[00:01:04.840 --> 00:01:07.440]   with help from some of the people in the open source community.
[00:01:07.440 --> 00:01:13.840]   It was originally almost, GPT4ALL was originally almost just designed as a weekend reaction
[00:01:13.840 --> 00:01:15.920]   to the GPT4 paper.
[00:01:15.920 --> 00:01:20.160]   When that came out, we were pretty annoyed that it didn't have a method section and that
[00:01:20.160 --> 00:01:25.080]   there was this move towards closed science and less transparency into these models.
[00:01:25.080 --> 00:01:29.880]   For us, it was more about just trying to, I guess, demonstrate to the community how
[00:01:29.880 --> 00:01:34.560]   we think science should be done out in the open so that people can see it and actually
[00:01:34.560 --> 00:01:38.960]   explore what is happening in these models and explain maybe why they're doing the things
[00:01:38.960 --> 00:01:39.960]   that they're doing.
[00:01:39.960 --> 00:01:44.840]   It started as sort of this weekend side project and is really now, as it's grown in popularity,
[00:01:44.840 --> 00:01:49.400]   become a dual focus for Gnomic, which is the company that Andre and I formed together.
[00:01:49.400 --> 00:01:50.480]   And so what does it do?
[00:01:50.480 --> 00:01:53.240]   What does GPT4ALL provide to an end user?
[00:01:53.240 --> 00:01:57.880]   Yeah, so I think that there's a variety of different value props and reasons why people
[00:01:57.880 --> 00:01:59.700]   seem to care about it.
[00:01:59.700 --> 00:02:04.720]   One is the fact that it's not just one model nowadays, but it's this ecosystem of models.
[00:02:04.720 --> 00:02:09.320]   So we see this really incredible outpouring of these great models for like Wizard, the
[00:02:09.320 --> 00:02:13.380]   Falcon, to all of the Llama variants and things like this.
[00:02:13.380 --> 00:02:17.560]   And people want to be able to access these models and they might not always be technically
[00:02:17.560 --> 00:02:18.880]   capable of doing it.
[00:02:18.880 --> 00:02:21.520]   They might not have the computational resources to do it.
[00:02:21.520 --> 00:02:26.600]   And part of the GPT4ALL initiative has really been collecting these models that industries
[00:02:26.600 --> 00:02:31.040]   are creating and groups are creating that are open source and compressing them so that
[00:02:31.040 --> 00:02:36.480]   people can run them on device and providing them this easy to use launcher so that even
[00:02:36.480 --> 00:02:40.240]   if you are not comfortable on a command line, you can go to the website, double click a
[00:02:40.240 --> 00:02:42.360]   thing and then get up and running with an open source.
[00:02:42.360 --> 00:02:46.200]   And I guess Hugging Face is well known for having a huge collection of models.
[00:02:46.200 --> 00:02:48.720]   How does GPT4ALL integrate with that?
[00:02:48.720 --> 00:02:51.160]   Yeah, I would say pretty collaboratively.
[00:02:51.160 --> 00:02:52.160]   We like to work with Hugging Face.
[00:02:52.160 --> 00:02:53.720]   I love the people there.
[00:02:53.720 --> 00:02:58.720]   I think it's an incredible organization that's acted as a North Star for us in terms of what
[00:02:58.720 --> 00:03:00.920]   we want to achieve with Gnomic.
[00:03:00.920 --> 00:03:06.320]   And so you see a lot of the actual model files themselves in the GPT4ALL ecosystem get hosted
[00:03:06.320 --> 00:03:08.040]   on the Hugging Face hub.
[00:03:08.040 --> 00:03:13.280]   Honestly, in the long term, I think Gnomic and Hugging Face have a lot of room to do
[00:03:13.280 --> 00:03:18.240]   some really interesting collaborations with not only the model hosting side of things,
[00:03:18.240 --> 00:03:19.640]   but also on the data set side of things.
[00:03:19.640 --> 00:03:23.600]   But I guess today, what's the situation where I would go to Hugging Face to use a model
[00:03:23.600 --> 00:03:26.560]   versus go to GPT4ALL to use a model?
[00:03:26.560 --> 00:03:31.600]   So I think the overlap is the model file is stored on Hugging Face, but that might not
[00:03:31.600 --> 00:03:35.160]   have say bindings where you can programmatically interact with it.
[00:03:35.160 --> 00:03:37.000]   Those are things that we provide.
[00:03:37.000 --> 00:03:40.600]   And beyond that, it might not have an interface on Hugging Face, right?
[00:03:40.600 --> 00:03:42.200]   Perhaps there's a space, perhaps there's not.
[00:03:42.200 --> 00:03:45.960]   If you do not have internet, you can't even access a space if it's there.
[00:03:45.960 --> 00:03:52.240]   And so we've got these files on Hugging Face that are somewhat inaccessible and GPT4ALL
[00:03:52.240 --> 00:03:57.280]   really builds this ecosystem around them to help one, programmers interact with it and
[00:03:57.280 --> 00:04:01.360]   to help people to be able to access it, even if they're not as comfortable on the front
[00:04:01.360 --> 00:04:02.360]   line.
[00:04:02.360 --> 00:04:06.000]   And so what are the most popular models on GPT4ALL and why?
[00:04:06.000 --> 00:04:07.960]   Oh man, it changes constantly.
[00:04:07.960 --> 00:04:11.600]   We recently released a couple of models that have gotten really popular.
[00:04:11.600 --> 00:04:15.800]   One is we compressed the Repl.it code model so it can run locally.
[00:04:15.800 --> 00:04:20.640]   And a lot of people seem to be really attracted to the idea of having a co-pilot that doesn't
[00:04:20.640 --> 00:04:24.280]   require them sending their code to somebody else's model.
[00:04:24.280 --> 00:04:29.300]   I think we've seen some pretty high profile situations with Samsung and others where sending
[00:04:29.300 --> 00:04:33.080]   code to an external entity might end up backfiring on the engineers.
[00:04:33.080 --> 00:04:37.680]   A lot of people seem to be getting a lot of joy out of the Repl.it model running locally.
[00:04:37.680 --> 00:04:41.680]   And then the Falcon models as of recent, I think are just the newest, hottest thing.
[00:04:41.680 --> 00:04:46.640]   Before that it was the MPT7B, before that it was GPT4ALLJ.
[00:04:46.640 --> 00:04:48.320]   And I think this trend will continue.
[00:04:48.320 --> 00:04:53.040]   I think the open source community will continue to really push the limits of these models
[00:04:53.040 --> 00:04:55.800]   and continue to close the gap with the closed source vendors.
[00:04:55.800 --> 00:04:58.760]   I guess these language models, I think it's always tough.
[00:04:58.760 --> 00:05:04.440]   One question I always get everywhere I go is, "Hey, which language model should I use
[00:05:04.440 --> 00:05:05.440]   and why?"
[00:05:05.440 --> 00:05:07.640]   I feel like you're in a great position to talk about that.
[00:05:07.640 --> 00:05:11.440]   What do you recommend if someone comes to you and says, "Hey, I want to play with a
[00:05:11.440 --> 00:05:12.880]   language model.
[00:05:12.880 --> 00:05:13.880]   Which one should I use?"
[00:05:13.880 --> 00:05:16.360]   It really depends on the downstream task.
[00:05:16.360 --> 00:05:20.920]   I think for, and also the constraints of their use case.
[00:05:20.920 --> 00:05:24.280]   I believe in a future where there's going to be a proliferation of different models
[00:05:24.280 --> 00:05:26.780]   that are all good at different things.
[00:05:26.780 --> 00:05:30.800]   And it's going to be very individualized, almost case by case, what model someone wants
[00:05:30.800 --> 00:05:31.800]   to use.
[00:05:31.800 --> 00:05:37.080]   I think nowadays we're in this interesting micro environment where everyone is just experimenting.
[00:05:37.080 --> 00:05:39.320]   So they don't know quite what the use case is.
[00:05:39.320 --> 00:05:42.400]   And so maybe the easiest thing is a massive model that can do everything.
[00:05:42.400 --> 00:05:48.520]   But as the roles that these models play become increasingly clear in these organizations,
[00:05:48.520 --> 00:05:53.000]   I think there's going to be a move towards using more domain specific models.
[00:05:53.000 --> 00:06:00.560]   And as an analogy I offer, you wouldn't necessarily hire a PhD in neuroscience to do a clerical
[00:06:00.560 --> 00:06:01.560]   job.
[00:06:01.560 --> 00:06:02.560]   Right?
[00:06:02.560 --> 00:06:06.120]   But that's effectively what's going on when you run GPT-4 to write copy.
[00:06:06.120 --> 00:06:10.520]   I think what's going to happen is as these use cases solidify, we're going to see a move
[00:06:10.520 --> 00:06:12.320]   towards domain specific models.
[00:06:12.320 --> 00:06:13.600]   That analogy makes some sense.
[00:06:13.600 --> 00:06:20.460]   And obviously I've heard it before, but I think in a world where copying and intelligence
[00:06:20.460 --> 00:06:26.520]   doesn't take much, you might actually just want to use, I don't know, maybe the smartest
[00:06:26.520 --> 00:06:29.160]   person or something to do all your tasks.
[00:06:29.160 --> 00:06:34.600]   We don't see necessarily a proliferation of software products.
[00:06:34.600 --> 00:06:39.240]   People tend to gravitate towards the best software products in most domains.
[00:06:39.240 --> 00:06:45.000]   So it doesn't seem necessarily obvious to me that everyone would want a custom model.
[00:06:45.000 --> 00:06:50.000]   Can you say more about why you think there'll be such a proliferation of models versus everyone
[00:06:50.000 --> 00:06:54.440]   gravitating to using the biggest model or the model trained on the most data?
[00:06:54.440 --> 00:06:56.960]   Yeah, I think there's a couple of reasons.
[00:06:56.960 --> 00:07:00.040]   The first is just the unit economics of running the model.
[00:07:00.040 --> 00:07:04.320]   GPT-4 is very expensive to run and strides are being taken to compress it.
[00:07:04.320 --> 00:07:08.000]   But at the end of the day, smaller models are always going to be more efficient.
[00:07:08.000 --> 00:07:10.480]   They're always going to have better unit economics.
[00:07:10.480 --> 00:07:15.640]   So as the space solidifies and there's more pressure to actually get something that's
[00:07:15.640 --> 00:07:19.000]   more cost effective out there, I think you're going to see this movement to maybe paying
[00:07:19.000 --> 00:07:23.640]   an upfront cost to getting a model and then having that pay off in the downstream.
[00:07:23.640 --> 00:07:29.000]   I also think people are going to become increasingly wary of sending their data to a third party
[00:07:29.000 --> 00:07:30.000]   service.
[00:07:30.000 --> 00:07:32.360]   We hear all the time in the startup community, "Oh, data is your moat."
[00:07:32.360 --> 00:07:37.560]   And yet people are really sending away their data to these mega companies that are just
[00:07:37.560 --> 00:07:39.240]   promising not to compete with them.
[00:07:39.240 --> 00:07:42.680]   We've heard a lot of promises from companies in the past.
[00:07:42.680 --> 00:07:45.280]   So I don't know how much I personally trust that.
[00:07:45.280 --> 00:07:49.600]   And so I think from a privacy and from an economic standpoint, there's going to be pressure
[00:07:49.600 --> 00:07:54.560]   to move these models on-prem and into specialized applications.
[00:07:54.560 --> 00:08:00.240]   I guess you could also imagine a world where there's one best open source model that everybody
[00:08:00.240 --> 00:08:01.240]   uses.
[00:08:01.240 --> 00:08:06.960]   I understand there's an axis of model size and complexity versus performance.
[00:08:06.960 --> 00:08:11.040]   But I guess other than that, why do you think there'll be specialized models for so many
[00:08:11.040 --> 00:08:15.860]   different applications versus the kind of current trend, which seems to be more prompt
[00:08:15.860 --> 00:08:18.760]   engineering one model to do the thing that you want?
[00:08:18.760 --> 00:08:23.880]   I think a good sort of case study here is maybe like Falcon versus Rebolit.
[00:08:23.880 --> 00:08:29.320]   So looking at Falcon 40B is a pretty great overall model at a variety of tasks, probably
[00:08:29.320 --> 00:08:32.320]   the best current open source, arguably, model.
[00:08:32.320 --> 00:08:37.920]   But for code and code completion and copilot tasks specifically, and in particular, local
[00:08:37.920 --> 00:08:41.840]   copilot tasks, you're going to want a model that runs much faster.
[00:08:41.840 --> 00:08:43.760]   So it's going to have to be much smaller.
[00:08:43.760 --> 00:08:45.560]   And so that's where the Rebolit 3B.
[00:08:45.560 --> 00:08:50.440]   And then even beyond that, once you start dropping the number of parameters, you have
[00:08:50.440 --> 00:08:54.560]   to start being a little bit more intelligent about the curation of the data, because you
[00:08:54.560 --> 00:08:59.680]   really only get so much data per parameter in these models.
[00:08:59.680 --> 00:09:06.600]   And so as the models get smaller, I think, and their domains become increasingly specified,
[00:09:06.600 --> 00:09:11.360]   people will start being more intelligent about what data they curate to actually train them.
[00:09:11.360 --> 00:09:15.960]   And that is what's going to enable some of these like sort of edge real time, low cost
[00:09:15.960 --> 00:09:17.040]   solutions.
[00:09:17.040 --> 00:09:19.120]   What about the cost in training a model?
[00:09:19.120 --> 00:09:25.600]   Right now, we see large organizations with tons of money, open sourcing models like NVIDIA
[00:09:25.600 --> 00:09:32.360]   and Meta and others, but it does seem like without an economic model, it might be hard
[00:09:32.360 --> 00:09:38.240]   to imagine the huge compute costs getting footed by someone who doesn't see a return
[00:09:38.240 --> 00:09:39.240]   there.
[00:09:39.240 --> 00:09:40.600]   Do you have any thoughts on how that might play out?
[00:09:40.600 --> 00:09:41.600]   Yeah.
[00:09:41.600 --> 00:09:46.200]   So training is such a broad spectrum of things that you can do, right?
[00:09:46.200 --> 00:09:50.360]   So you've got pre-training, you've got fine tuning, you've got RLHF, these are all almost
[00:09:50.360 --> 00:09:52.920]   broadly referred to under the umbrella of training.
[00:09:52.920 --> 00:09:58.000]   And I think what you've seen over the past year or so is a strong move towards really
[00:09:58.000 --> 00:09:59.760]   efficient fine tuning techniques.
[00:09:59.760 --> 00:10:04.320]   So first, you started to see things like adapters with LoRa, and now you're even seeing things
[00:10:04.320 --> 00:10:10.000]   like QLoRa, where you can take a quantized base model, which is compressed and runs very
[00:10:10.000 --> 00:10:11.000]   fast.
[00:10:11.000 --> 00:10:15.120]   And you can use that to train a very small set of adapter weights very efficiently for
[00:10:15.120 --> 00:10:17.120]   particular downstream tasks.
[00:10:17.120 --> 00:10:22.480]   And so I think there's going to be a place for base model training and open source still.
[00:10:22.480 --> 00:10:25.640]   And I think there will be a couple of main base models.
[00:10:25.640 --> 00:10:30.440]   One might be like a code base model, one might be say a creative base model, one might be
[00:10:30.440 --> 00:10:32.080]   a chat base model.
[00:10:32.080 --> 00:10:36.160]   And then I think individual organizations will adapt those using these sort of more
[00:10:36.160 --> 00:10:42.240]   efficient, more cost-effective fine tuning methodologies for their particular use cases.
[00:10:42.240 --> 00:10:47.680]   Can you say more about the state of the art of fine tuning right now and how that works?
[00:10:47.680 --> 00:10:51.920]   Yeah, so I would say the most, and I mentioned this just a moment ago, but the most exciting
[00:10:51.920 --> 00:10:54.760]   recent development in my eyes is QLoRa.
[00:10:54.760 --> 00:10:59.240]   So I guess just going through roughly the history of LoRa, these are these sort of like
[00:10:59.240 --> 00:11:04.120]   weights that you can, it's a very small number of weights compared to the number of weights
[00:11:04.120 --> 00:11:05.120]   in the model.
[00:11:05.120 --> 00:11:09.520]   You can just add them into the model, like literally add them in a couple of places to
[00:11:09.520 --> 00:11:11.480]   really modify the model behavior.
[00:11:11.480 --> 00:11:15.760]   The problem with the original LoRa is you still actually have to run the computation
[00:11:15.760 --> 00:11:18.280]   on the entire base model, which can be quite large.
[00:11:18.280 --> 00:11:23.000]   Simultaneously with GGML and these other sort of like quantization libraries that are popping
[00:11:23.000 --> 00:11:26.840]   up all over the place, what you're starting to see are people that are becoming cognizant
[00:11:26.840 --> 00:11:30.320]   of ways to run these base models much faster.
[00:11:30.320 --> 00:11:33.160]   And these two trends intersected with QLoRa.
[00:11:33.160 --> 00:11:38.360]   And what that paper showed is you can take a base model that you've quantized and still
[00:11:38.360 --> 00:11:44.440]   use that sort of low cost quantized base model, train LoRa adapters, which massively decreases
[00:11:44.440 --> 00:11:49.200]   the sort of cost from a compute standpoint of fine tuning these things, which is very
[00:11:49.200 --> 00:11:50.200]   exciting.
[00:11:50.200 --> 00:11:54.720]   I'm not sure that everyone listening to this will even know what quantization is.
[00:11:54.720 --> 00:11:59.400]   Maybe could you start with what quantization is and what it does, and then go into more
[00:11:59.400 --> 00:12:02.160]   of the details on like how to make it work effectively?
[00:12:02.160 --> 00:12:03.160]   Yeah.
[00:12:03.160 --> 00:12:06.080]   So I'll keep it relatively high level in that case.
[00:12:06.080 --> 00:12:10.520]   A lot of people joke around that these machine learning models are big piles of linear algebra
[00:12:10.520 --> 00:12:12.640]   that you stir to get an answer.
[00:12:12.640 --> 00:12:18.100]   And it turns out that the actual data type of the thing that you're stirring are matrices
[00:12:18.100 --> 00:12:20.080]   with floating point precision in them.
[00:12:20.080 --> 00:12:25.760]   And this is just using say 32 bits or 64 bits of computer memory to represent a particular
[00:12:25.760 --> 00:12:26.960]   weight in the network.
[00:12:26.960 --> 00:12:31.640]   What you can achieve with quantization is a roughly equivalent sort of behavior of the
[00:12:31.640 --> 00:12:37.040]   network, but instead of those weights maybe being 32 bit or 64 bit like floating point
[00:12:37.040 --> 00:12:41.640]   numbers, you get something that is like a four bit or an eight bit int.
[00:12:41.640 --> 00:12:46.760]   And this is from a computational standpoint, much cheaper to operate on.
[00:12:46.760 --> 00:12:48.760]   And that's where a lot of the efficiency gains are.
[00:12:48.760 --> 00:12:53.260]   And I guess, so are the efficiency gains just directly like, okay, we've made these numbers
[00:12:53.260 --> 00:12:54.260]   a quarter of the size.
[00:12:54.260 --> 00:12:58.900]   So now the model's a quarter smaller, so it runs four times as fast.
[00:12:58.900 --> 00:12:59.900]   How big are the gains?
[00:12:59.900 --> 00:13:01.600]   And you can get really significant gains.
[00:13:01.600 --> 00:13:05.880]   The exact curves are somewhat complicated because there's a lot of questions about what
[00:13:05.880 --> 00:13:11.840]   exact operations get called and some exact operations are more efficient than others.
[00:13:11.840 --> 00:13:17.540]   So one thing that comes to mind here is this idea of two-four sparsity, which is this particular
[00:13:17.540 --> 00:13:23.520]   way that you can add zeros to the weight matrix of a non-quantized model that can be taken
[00:13:23.520 --> 00:13:29.040]   advantage of by like the software suite of NVIDIA at large to actually get like an efficiency
[00:13:29.040 --> 00:13:30.040]   gains.
[00:13:30.040 --> 00:13:32.640]   There's like these very particular patterns that you can induce that allow you to get
[00:13:32.640 --> 00:13:37.120]   these very particular kernel calls that help you make these things faster.
[00:13:37.120 --> 00:13:39.800]   But generally you see, that's a bit of a tangent.
[00:13:39.800 --> 00:13:44.400]   Generally what in the quantization space is you can get pretty significant speed ups.
[00:13:44.400 --> 00:13:49.640]   So for instance, for the Repl.it 3B model, we have that running at something like 25
[00:13:49.640 --> 00:13:51.440]   tokens per second on a Mac.
[00:13:51.440 --> 00:13:55.600]   And that is for a 3 billion parameter model, that was something that was frankly unheard
[00:13:55.600 --> 00:13:56.600]   of.
[00:13:56.600 --> 00:14:01.160]   So that would have been just running the raw original model, how many tokens per second
[00:14:01.160 --> 00:14:02.160]   roughly?
[00:14:02.160 --> 00:14:05.360]   If you can even load it in the memory, probably a couple, ones or twos.
[00:14:05.360 --> 00:14:06.360]   I see.
[00:14:06.360 --> 00:14:11.960]   And I guess I've heard even quantization down to a single bit being tried.
[00:14:11.960 --> 00:14:16.200]   How far can you take it before the model performance starts to suffer?
[00:14:16.200 --> 00:14:17.200]   You definitely can do that.
[00:14:17.200 --> 00:14:21.480]   In binary neural networks are this super interesting active area of research that's been around
[00:14:21.480 --> 00:14:23.040]   for quite some time.
[00:14:23.040 --> 00:14:27.800]   Obviously people have cared about these things since I got into the field in 2017.
[00:14:27.800 --> 00:14:34.640]   But really the main idea of all of these quantization sparsity methods is the performance compression
[00:14:34.640 --> 00:14:38.620]   curve of a lot of these models is super advantageous.
[00:14:38.620 --> 00:14:44.440]   So you can in many cases compress a model, say 50% with almost no drop in performance.
[00:14:44.440 --> 00:14:48.680]   Now it gets quite steep in terms of performance drop off when you start getting to the extremes,
[00:14:48.680 --> 00:14:51.200]   like the binary and the 4-bit.
[00:14:51.200 --> 00:14:56.320]   But there are really great intermediary states where you can compress it quite a bit and
[00:14:56.320 --> 00:14:58.560]   still maintain a lot of performance.
[00:14:58.560 --> 00:15:03.560]   And this points to, I think one of the things that makes the sort of new, I don't want to
[00:15:03.560 --> 00:15:09.280]   use the phrase software 2.0, but the software 2.0 style computing paradigm so interesting
[00:15:09.280 --> 00:15:12.400]   is that your compute resources are very elastic.
[00:15:12.400 --> 00:15:19.200]   So you can really very finely tune the trade-off between sort of efficiency and accuracy.
[00:15:19.200 --> 00:15:24.400]   Whereas with previous sort of code systems, if you wanted to get more efficient, it would
[00:15:24.400 --> 00:15:28.040]   require this sort of massive intervention into the structure of the code base.
[00:15:28.040 --> 00:15:31.760]   Whereas now you can just dial up the compression or the sparsity of the network.
[00:15:31.760 --> 00:15:35.640]   And that's a very interesting property of this new paradigm of software that I think
[00:15:35.640 --> 00:15:38.040]   has been slightly underexplored to this point.
[00:15:38.040 --> 00:15:44.300]   Now when you do this compression, do you need to also do it in the context of underlying
[00:15:44.300 --> 00:15:48.960]   training data or can you just take the model and compress it?
[00:15:48.960 --> 00:15:50.360]   And like it's a spectrum, right?
[00:15:50.360 --> 00:15:53.080]   The techniques range widely.
[00:15:53.080 --> 00:15:58.440]   So you can get away with just taking the model and compressing it, but it won't be as good
[00:15:58.440 --> 00:16:05.800]   as if you can make decisions about the entire training and sort of inference process.
[00:16:05.800 --> 00:16:10.720]   And the more control over the entire model creation process that you have, the better
[00:16:10.720 --> 00:16:15.120]   kind of downstream performance you're going to get because you'll be able to do things
[00:16:15.120 --> 00:16:19.960]   like say curate the input data or design the model in an architecture that's more amenable
[00:16:19.960 --> 00:16:26.040]   to compression or even say equip it out of the gate with something like a retrieval mechanism.
[00:16:26.040 --> 00:16:30.040]   There's a lot of really cool work that we can chat about on more efficient methods of
[00:16:30.040 --> 00:16:33.400]   retrieval that I think is going to become increasingly common in the next couple of
[00:16:33.400 --> 00:16:34.400]   months.
[00:16:34.400 --> 00:16:35.400]   Interesting.
[00:16:35.400 --> 00:16:36.400]   I'm curious about fine tuning.
[00:16:36.400 --> 00:16:39.640]   We also see kind of a wide variety of these on that, right?
[00:16:39.640 --> 00:16:46.160]   Like notably GPT is hard to fine tune, prohibitively expensive to fine tune the cloud-based models,
[00:16:46.160 --> 00:16:50.240]   but I would imagine your models are like typically fine tuned.
[00:16:50.240 --> 00:16:54.400]   Do you have any sense on if someone asks you, Hey, should I fine tune or not?
[00:16:54.400 --> 00:16:57.320]   How you would advise someone to approach that question?
[00:16:57.320 --> 00:16:58.320]   Yeah.
[00:16:58.320 --> 00:17:01.800]   So I think the first question becomes like, what is your operating domain?
[00:17:01.800 --> 00:17:03.640]   What kind of data do you have?
[00:17:03.640 --> 00:17:06.120]   Do you know what you're really optimizing for?
[00:17:06.120 --> 00:17:10.640]   I think a lot of organizations right now are just trying to get the first model out there.
[00:17:10.640 --> 00:17:14.520]   And that's a very different problem from like the first engineering iteration cycle.
[00:17:14.520 --> 00:17:16.000]   I have a model deployed.
[00:17:16.000 --> 00:17:19.160]   I now have a data flywheel where people are interacting with it.
[00:17:19.160 --> 00:17:23.000]   Maybe I'm even collecting ratings of whether or not those interactions were good or those
[00:17:23.000 --> 00:17:24.600]   behaviors were valid.
[00:17:24.600 --> 00:17:30.920]   And so it would really depend, I think a lot on the operating domain specifically, as well
[00:17:30.920 --> 00:17:35.520]   as how mature their AI deployment is.
[00:17:35.520 --> 00:17:39.640]   If this is their first time getting something out there or if they have something deployed
[00:17:39.640 --> 00:17:42.080]   and they're starting to have that data flywheel.
[00:17:42.080 --> 00:17:46.320]   And so I guess your view is like, Hey, get something deployed without fine tuning and
[00:17:46.320 --> 00:17:48.040]   then fine tune it over time.
[00:17:48.040 --> 00:17:49.560]   Is this your general suggestion?
[00:17:49.560 --> 00:17:50.760]   I think that's right.
[00:17:50.760 --> 00:17:53.080]   And this harkens back to what I was talking about earlier.
[00:17:53.080 --> 00:17:58.400]   We really, I think are in the early days of this, what is frankly a new paradigm of computing.
[00:17:58.400 --> 00:18:03.040]   And so everyone is just trying to get something out there and see where it fits in their organization
[00:18:03.040 --> 00:18:04.740]   and see what works.
[00:18:04.740 --> 00:18:09.320]   And for doing that, having these massive generalist models that are at some level subsidized by
[00:18:09.320 --> 00:18:11.880]   venture capital right now is really wonderful.
[00:18:11.880 --> 00:18:17.220]   But over time, it's going to become increasingly clear what particular roles those models are
[00:18:17.220 --> 00:18:20.920]   going to fill in the flow chart of information through an organization.
[00:18:20.920 --> 00:18:25.680]   And at that point, once you've deployed something in that role has solidified, it becomes time
[00:18:25.680 --> 00:18:30.120]   to start thinking about, Oh, maybe I can fine tune a more efficient model to fill this role
[00:18:30.120 --> 00:18:35.240]   and really drop the cost of this thing and really start to get it to operate at a very
[00:18:35.240 --> 00:18:37.880]   high level of proficiency in my operating domain.
[00:18:37.880 --> 00:18:44.080]   And now GPT4ALL really came out of the gate with an absolutely astounding growth in kind
[00:18:44.080 --> 00:18:46.300]   of usage and GitHub stars and all that.
[00:18:46.300 --> 00:18:51.680]   Was that something that you tried to make happen or did you feel like it happened to
[00:18:51.680 --> 00:18:52.680]   you?
[00:18:52.680 --> 00:18:53.680]   No.
[00:18:53.680 --> 00:18:56.760]   GPT4ALL was very much like an accident in a lot of ways.
[00:18:56.760 --> 00:19:01.440]   It grew out of, like I said earlier, this weekend project that was born in sort of the
[00:19:01.440 --> 00:19:05.800]   frustration of the lack of transparency in the GPT4 paper.
[00:19:05.800 --> 00:19:10.760]   And I think the response was unlike anything that we could have ever predicted.
[00:19:10.760 --> 00:19:11.760]   Yeah.
[00:19:11.760 --> 00:19:14.960]   I'm just really happy that people seem to care about it and seem to want to keep these
[00:19:14.960 --> 00:19:15.960]   models open.
[00:19:15.960 --> 00:19:19.120]   I think that in the long run, that is the best way that this can go.
[00:19:19.120 --> 00:19:23.000]   And so I guess, how does it fit into your plans for the whole company?
[00:19:23.000 --> 00:19:25.040]   Yeah, it's definitely changed them.
[00:19:25.040 --> 00:19:29.680]   I joke around about how for a long time, Gnomic was a weird company because we had three employees
[00:19:29.680 --> 00:19:34.440]   and two products being Atlas, which is our data mapping engine, and GPT4ALL, which is
[00:19:34.440 --> 00:19:36.640]   the open source language model ecosystem.
[00:19:36.640 --> 00:19:39.280]   But they're actually quite harmonious with each other.
[00:19:39.280 --> 00:19:42.640]   So first of all, GPT4ALL wouldn't be possible without Atlas.
[00:19:42.640 --> 00:19:47.880]   There was such, if people take one thing away from the original GPT4ALL model, it's like
[00:19:47.880 --> 00:19:51.560]   how important data curation is for downstream quality.
[00:19:51.560 --> 00:19:55.120]   Like it shocks me to this day that there are practitioners in machine learning that don't
[00:19:55.120 --> 00:19:59.120]   have mental models of their training data before they feed it to these systems.
[00:19:59.120 --> 00:20:02.880]   Really all model behavior is inherited from the properties of the training data.
[00:20:02.880 --> 00:20:05.640]   And so knowing what is in there is really critical.
[00:20:05.640 --> 00:20:11.280]   But even beyond that, we are now able to loop and connect GPT4ALL style models back into
[00:20:11.280 --> 00:20:12.280]   the Atlas system.
[00:20:12.280 --> 00:20:17.200]   And so we've got this, for example, with this one capability where you put in someone's
[00:20:17.200 --> 00:20:22.040]   unstructured data to Atlas and we will automatically attract like an ontology over it.
[00:20:22.040 --> 00:20:26.640]   So we'll give you class labels and then a structure about how those classes connect.
[00:20:26.640 --> 00:20:29.440]   And that's all powered by GPT4ALL style models.
[00:20:29.440 --> 00:20:35.160]   And that is a situation where we can really start to provide exploratory value in terms
[00:20:35.160 --> 00:20:40.920]   of the data exploration process to users by leveraging GPT4ALL style models.
[00:20:40.920 --> 00:20:46.820]   And I think what you'll see is increasingly these models will be used for subsequent kind
[00:20:46.820 --> 00:20:47.820]   of downstream tooling.
[00:20:47.820 --> 00:20:52.240]   So they'll be present in every application from data exploration applications to say
[00:20:52.240 --> 00:20:54.720]   video conferencing and posting applications.
[00:20:54.720 --> 00:20:58.920]   So before we go too far down this path, can you talk about what Atlas does?
[00:20:58.920 --> 00:21:04.640]   In a sentence, Atlas allows you to do exploratory data analysis on massive unstructured data
[00:21:04.640 --> 00:21:06.000]   sets in a web browser.
[00:21:06.000 --> 00:21:11.240]   The reason why I cared about this was due to my experiences at RAD AI, which was this
[00:21:11.240 --> 00:21:17.680]   generative medical AI company that I dropped out of my PhD to join in 2018.
[00:21:17.680 --> 00:21:23.040]   Essentially at that company, we were looking at applying transformers to radiology data.
[00:21:23.040 --> 00:21:28.180]   And one of the things that we needed was for the transformers that we were building to
[00:21:28.180 --> 00:21:31.140]   actually outperform radiologists on double blind tests.
[00:21:31.140 --> 00:21:34.860]   If we couldn't achieve that, not only was there not a company, but it was unsafe to
[00:21:34.860 --> 00:21:35.860]   deploy.
[00:21:35.860 --> 00:21:39.960]   And so we really had from day one, this impetus to make these models very accurate in their
[00:21:39.960 --> 00:21:40.960]   operating domain.
[00:21:40.960 --> 00:21:47.680]   And what we found is effectively every problem that a model had could be traced back to some
[00:21:47.680 --> 00:21:50.800]   underlying error in the training data for that model.
[00:21:50.800 --> 00:21:55.880]   And so we went through this incredibly arduous process of trying to do exploratory data analysis
[00:21:55.880 --> 00:22:00.180]   on the model outputs and sort of the ratings that we were getting back and trying to attribute
[00:22:00.180 --> 00:22:01.340]   that back to the training data.
[00:22:01.340 --> 00:22:06.580]   And this was all in one-off scripts and Jupyter notebooks and like Matplotlib visualizations
[00:22:06.580 --> 00:22:09.400]   and Plotly visualizations when we were lucky.
[00:22:09.400 --> 00:22:13.640]   But those kinds of systems don't scale to machine learning size datasets.
[00:22:13.640 --> 00:22:18.000]   Like I don't know if you've ever tried to do a hundred thousand point like Plotly scatter
[00:22:18.000 --> 00:22:19.840]   plot and actually navigate it.
[00:22:19.840 --> 00:22:21.720]   It's not going to work.
[00:22:21.720 --> 00:22:26.320]   And so it became really clear to me that there was this need for tooling around like visual
[00:22:26.320 --> 00:22:31.200]   analysis of these massive datasets and that it would be incredibly high impact in sort
[00:22:31.200 --> 00:22:36.360]   of the five to 10 year horizon as people started to deploy these systems in fields like medicine
[00:22:36.360 --> 00:22:38.080]   where they have to be accurate.
[00:22:38.080 --> 00:22:40.520]   And so that was the genesis of building Atlas.
[00:22:40.520 --> 00:22:45.040]   And so now with Atlas, we're pushing scales of like 20 million points.
[00:22:45.040 --> 00:22:49.200]   The bottleneck for us right now is just like the time it takes to plug all the holes and
[00:22:49.200 --> 00:22:50.200]   scaling it.
[00:22:50.200 --> 00:22:52.000]   I think we can go much larger.
[00:22:52.000 --> 00:22:57.200]   We've done billion point scatter plots of the night sky that we'll release at some point,
[00:22:57.200 --> 00:23:01.200]   but doing it on the actual vectors coming out of these models requires a little more
[00:23:01.200 --> 00:23:02.200]   legwork.
[00:23:02.200 --> 00:23:05.520]   But I'm happy to see that we've upped the order of magnitude that we can operate on.
[00:23:05.520 --> 00:23:09.320]   I'm happy to hear that datasets still matter for ML.
[00:23:09.320 --> 00:23:12.680]   That was always my background and interest.
[00:23:12.680 --> 00:23:18.760]   I'm curious when you were building the first datasets for GPT for all, how did you use
[00:23:18.760 --> 00:23:22.720]   Atlas to curate those datasets or look at those datasets?
[00:23:22.720 --> 00:23:26.560]   What kind of visualizations do you really need and what kind of choices are you making
[00:23:26.560 --> 00:23:28.520]   around building a high quality dataset?
[00:23:28.520 --> 00:23:29.520]   Yeah.
[00:23:29.520 --> 00:23:33.180]   So if you go look in the technical report, you'll see like a before and after snapshot
[00:23:33.180 --> 00:23:34.760]   of the data in Atlas.
[00:23:34.760 --> 00:23:39.320]   The first thing is just, so for an illustrative example with GPT for all, there was a lot
[00:23:39.320 --> 00:23:42.520]   of data in a particular subset that was very short responses.
[00:23:42.520 --> 00:23:43.920]   It was like three or four word responses.
[00:23:43.920 --> 00:23:47.760]   And I think the prompts were something like, here is an article and then you're asking
[00:23:47.760 --> 00:23:49.760]   the model, what is this article about?
[00:23:49.760 --> 00:23:51.360]   Like business or art or sports or something.
[00:23:51.360 --> 00:23:53.800]   The model would be like, this article is about business period.
[00:23:53.800 --> 00:23:59.000]   If you train your model on a ton of that data, it's going to produce short responses.
[00:23:59.000 --> 00:24:01.440]   You're teaching it to answer in a very concise way.
[00:24:01.440 --> 00:24:03.840]   And so first of all, realizing that's a thing that's happening.
[00:24:03.840 --> 00:24:08.160]   And second of all, actually modifying your dataset to remove those sets of things is
[00:24:08.160 --> 00:24:09.720]   I think quite important.
[00:24:09.720 --> 00:24:14.520]   Another thing is the amount of deduplication and the semantic coverage of your dataset.
[00:24:14.520 --> 00:24:17.720]   You want to make sure that you've got, if you're making a general purpose chatbot, say
[00:24:17.720 --> 00:24:24.320]   a wide variety of topics represented and that those topics occur in roughly equal sort of
[00:24:24.320 --> 00:24:25.720]   concentrations.
[00:24:25.720 --> 00:24:29.880]   So for instance, if you happen to pull in a bunch of different datasets and one of them
[00:24:29.880 --> 00:24:33.800]   is much larger than the others, your model is going to spend an outsized amount of capacity
[00:24:33.800 --> 00:24:34.800]   fitting to that topic.
[00:24:34.800 --> 00:24:39.080]   And that's going to hurt its ability to speak to the rest of the topics in the dataset.
[00:24:39.080 --> 00:24:42.640]   And so there's this sort of balancing act you play by concentrating different topics
[00:24:42.640 --> 00:24:43.640]   in this mixture.
[00:24:43.640 --> 00:24:44.640]   That's quite important.
[00:24:44.640 --> 00:24:49.240]   So I could imagine on a huge dataset, it would be hard to find these things.
[00:24:49.240 --> 00:24:50.760]   Like there's these short responses.
[00:24:50.760 --> 00:24:55.200]   So how does Atlas, like how are you using Atlas to find these situations?
[00:24:55.200 --> 00:24:59.920]   Yeah, the first step is just giving people an interface where they can get a sense of
[00:24:59.920 --> 00:25:02.320]   a lot of data at once and rapidly iterate.
[00:25:02.320 --> 00:25:07.520]   Pre Atlas, people were literally like opening up like text files or like looking at the
[00:25:07.520 --> 00:25:12.040]   head of a panda's data frame and you get maybe 10 samples of data or something in a spot
[00:25:12.040 --> 00:25:15.880]   check and perhaps maybe once in a while you'll get like some aggregate plots.
[00:25:15.880 --> 00:25:20.420]   I think one of the first keys is Atlas shows you all the data on one screen at once.
[00:25:20.420 --> 00:25:24.840]   And then we start to have these like Google Maps style, like annotations floating above
[00:25:24.840 --> 00:25:27.880]   the map describing what is in different places.
[00:25:27.880 --> 00:25:30.780]   And from those you can find like very obvious things.
[00:25:30.780 --> 00:25:37.880]   So one example that I talk about a decent amount here is the Anthropic HH dataset.
[00:25:37.880 --> 00:25:41.820]   So if you look at in the open source model domain, there's this model called Koala that
[00:25:41.820 --> 00:25:43.180]   came out of Berkeley.
[00:25:43.180 --> 00:25:48.240]   And one of the things that they did was they did sort of language modeling on the chosen
[00:25:48.240 --> 00:25:50.980]   responses of the Anthropic HH dataset.
[00:25:50.980 --> 00:25:55.300]   If you pull up that dataset in Atlas, there's this sort of massive label that's just right
[00:25:55.300 --> 00:25:58.500]   in front of you and the top of it that just says like racism.
[00:25:58.500 --> 00:26:01.260]   And that's like a pretty obvious flag of, oh, maybe there's something here we don't
[00:26:01.260 --> 00:26:02.940]   want to train our language model on.
[00:26:02.940 --> 00:26:07.180]   And the reason why that data is included in the Anthropic HH dataset is a little bit subtle.
[00:26:07.180 --> 00:26:12.820]   It turns out that dataset is for building a reward model for RLHF.
[00:26:12.820 --> 00:26:14.620]   It's not for language modeling.
[00:26:14.620 --> 00:26:19.100]   But these are the kinds of errors that are really easy to miss if you're not like building
[00:26:19.100 --> 00:26:20.740]   a mental model of your training data.
[00:26:20.740 --> 00:26:24.460]   And it's very easy to have things like that slip through the cracks.
[00:26:24.460 --> 00:26:28.340]   And how are you generating these descriptions of what's inside the data?
[00:26:28.340 --> 00:26:29.500]   GPT for all.
[00:26:29.500 --> 00:26:34.900]   So this is what I was talking about earlier when I say we will use the inherent geometry
[00:26:34.900 --> 00:26:39.500]   of the data, of the embeddings of the data to cluster it into different concepts and
[00:26:39.500 --> 00:26:42.160]   build this linkage on top of those concepts.
[00:26:42.160 --> 00:26:47.720]   And then we can use GPT for all style models to actually summarize those concepts in like
[00:26:47.720 --> 00:26:49.300]   a human interpretable way.
[00:26:49.300 --> 00:26:53.500]   And so what results is essentially a Google Map style view of your data where you have
[00:26:53.500 --> 00:26:55.900]   these high level ideas labeled floating.
[00:26:55.900 --> 00:27:00.380]   And then as you dive in finer and finer grain categories start to emerge.
[00:27:00.380 --> 00:27:06.100]   And now is there anything different that you do in training a code model versus training
[00:27:06.100 --> 00:27:07.100]   a language model?
[00:27:07.100 --> 00:27:08.100]   Yeah.
[00:27:08.100 --> 00:27:11.660]   So code, training code and language models is actually very interesting because it's
[00:27:11.660 --> 00:27:17.820]   only been recently realized in a widespread way that including code improves the language
[00:27:17.820 --> 00:27:20.680]   modeling capabilities of even non-code models.
[00:27:20.680 --> 00:27:24.940]   And so there's really this open question right now that we're very interested in, which is
[00:27:24.940 --> 00:27:29.840]   how does the mixture of data in the training set of a model affect its capabilities?
[00:27:29.840 --> 00:27:34.480]   It becomes quite difficult to actually tease this out because the mechanisms of these models
[00:27:34.480 --> 00:27:35.500]   are unknown.
[00:27:35.500 --> 00:27:39.740]   But some of the work that we've done here is this idea of causal intervention where
[00:27:39.740 --> 00:27:45.100]   you take a training set, you train a model on all of it, and then you take that same
[00:27:45.100 --> 00:27:49.700]   training set, ablate part of it or remove part of it, train the exact same style of
[00:27:49.700 --> 00:27:52.500]   model on it and look at how their representations differ.
[00:27:52.500 --> 00:27:55.820]   This is something that we did in a recent paper we published to archive, which you can
[00:27:55.820 --> 00:27:57.060]   find on our website.
[00:27:57.060 --> 00:28:01.340]   And essentially what you find is there is this sort of like interesting semantic bleed
[00:28:01.340 --> 00:28:02.340]   effect.
[00:28:02.340 --> 00:28:07.540]   So one experiment we did is we trained a BERT model on DBPTO, which is this encyclopedia
[00:28:07.540 --> 00:28:08.540]   data set.
[00:28:08.540 --> 00:28:12.180]   And then we trained in a bladed model, same BERT model, but a bladed data set, I should
[00:28:12.180 --> 00:28:16.380]   say on the DBPTO 14 data set without plant articles.
[00:28:16.380 --> 00:28:20.060]   Those two models not only have a differing view of what plants are, but they also have
[00:28:20.060 --> 00:28:24.260]   differing views on how they represent animals and natural places.
[00:28:24.260 --> 00:28:30.020]   And so there's this interesting kind of like semantic diffusion effect where removing data
[00:28:30.020 --> 00:28:34.100]   of something that is related to these other classes, but not exactly the same, changes
[00:28:34.100 --> 00:28:35.740]   the representations of other things.
[00:28:35.740 --> 00:28:36.740]   And so teasing this out-
[00:28:36.740 --> 00:28:37.740]   Yeah.
[00:28:37.740 --> 00:28:38.740]   How would you know that?
[00:28:38.740 --> 00:28:43.380]   How would you know that the model has a different sense of what animals are when the plant data
[00:28:43.380 --> 00:28:44.380]   is removed?
[00:28:44.380 --> 00:28:45.380]   What does that even mean?
[00:28:45.380 --> 00:28:46.380]   Yeah.
[00:28:46.380 --> 00:28:51.180]   Without getting super technical, one thing that you can look at are the associations
[00:28:51.180 --> 00:28:52.660]   that the model is making.
[00:28:52.660 --> 00:28:59.180]   And you can roughly map this to how close two representations that a model makes of
[00:28:59.180 --> 00:29:01.540]   two pieces of data are to each other.
[00:29:01.540 --> 00:29:07.220]   And what we find is that if you use that proximity as a measure of relatedness, when you ablate
[00:29:07.220 --> 00:29:11.980]   sort of plant articles from the training data, all of the proximity relationships in the
[00:29:11.980 --> 00:29:15.180]   natural place and animal articles start to get messed up as well.
[00:29:15.180 --> 00:29:18.060]   So two animals might be far apart from each other?
[00:29:18.060 --> 00:29:19.060]   Yeah.
[00:29:19.060 --> 00:29:23.340]   So one of the first things that Gnomic ever published was this article at VisXAI, which
[00:29:23.340 --> 00:29:27.300]   is an awesome conference that everyone should aspire to publish at, I think.
[00:29:27.300 --> 00:29:31.700]   It's a great inspiration to me, at least, where we showed that there's this super weird
[00:29:31.700 --> 00:29:37.940]   property of BERT embeddings of Wikipedia, where the Ferrari is closer in representation
[00:29:37.940 --> 00:29:43.340]   space to an M1 Garand rifle than it is to Enzo Ferrari, the human that the car is named
[00:29:43.340 --> 00:29:44.340]   after.
[00:29:44.340 --> 00:29:50.620]   And so these sort of very interesting quirks of how the model is defining relatedness can
[00:29:50.620 --> 00:29:54.020]   really show up and wreak havoc on these downstream systems.
[00:29:54.020 --> 00:29:56.820]   How do you evaluate an embedding?
[00:29:56.820 --> 00:30:01.580]   It seems like I don't really have an a priori feeling about which thing the Ferrari should
[00:30:01.580 --> 00:30:02.580]   be closer to.
[00:30:02.580 --> 00:30:04.420]   Probably depends on what you're trying to do.
[00:30:04.420 --> 00:30:07.660]   Do you have some metrics for saying if an embedding is good or not?
[00:30:07.660 --> 00:30:09.740]   I guess at the end of the day, it's useful or not.
[00:30:09.740 --> 00:30:15.180]   So this is very much an analysis that we run to see what associations a model are making
[00:30:15.180 --> 00:30:16.180]   out of the box.
[00:30:16.180 --> 00:30:21.100]   A lot of people take these, what are now being called foundation models, and just apply them
[00:30:21.100 --> 00:30:23.300]   to whatever task they can throw at it.
[00:30:23.300 --> 00:30:27.340]   And sometimes the associations that the foundation model has learned are good for that task,
[00:30:27.340 --> 00:30:28.500]   and sometimes they're not.
[00:30:28.500 --> 00:30:32.420]   But certainly we should know what they are so we can evaluate if they're good or not
[00:30:32.420 --> 00:30:33.420]   for the task.
[00:30:33.420 --> 00:30:38.660]   And it turns out that the process of fine tuning these models effectively updates those
[00:30:38.660 --> 00:30:42.340]   associations so that they happen to be good for the task.
[00:30:42.340 --> 00:30:48.420]   And do you quantify the quality of the embeddings themselves?
[00:30:48.420 --> 00:30:55.180]   So one of the challenges of model evaluation, we'll say, is defining that kind of downstream
[00:30:55.180 --> 00:30:56.180]   metric.
[00:30:56.180 --> 00:30:57.900]   And that really depends on your domain.
[00:30:57.900 --> 00:31:03.860]   One of the things that we endeavor to do is find ways to characterize the behavior of
[00:31:03.860 --> 00:31:08.040]   embeddings that are not sensitive to this downstream metric so that they can be applied
[00:31:08.040 --> 00:31:10.540]   in various different capacities.
[00:31:10.540 --> 00:31:15.300]   And one of the things that we happen to look at is this idea of concept proximity.
[00:31:15.300 --> 00:31:20.340]   And for instance, going back to the Ferrari example, if you're in an operating domain
[00:31:20.340 --> 00:31:25.900]   where you're maybe trying to classify artificial versus biological, perhaps that's a good association
[00:31:25.900 --> 00:31:26.900]   to be making.
[00:31:26.900 --> 00:31:31.300]   But if you're in an operating domain where you are maybe trying to classify the geographic
[00:31:31.300 --> 00:31:34.940]   origin of an article, that's probably a terrible association to be making.
[00:31:34.940 --> 00:31:37.540]   And it really depends on the downstream domain.
[00:31:37.540 --> 00:31:40.940]   And I guess you have a leaderboard of these models, right?
[00:31:40.940 --> 00:31:45.580]   And there's lots of different ways that people evaluate these models, like Hugging Face has
[00:31:45.580 --> 00:31:46.580]   a leaderboard.
[00:31:46.580 --> 00:31:49.780]   There's plenty of benchmarking datasets.
[00:31:49.780 --> 00:31:52.380]   I guess, how do you think about those?
[00:31:52.380 --> 00:31:54.660]   How do you think about evaluation today?
[00:31:54.660 --> 00:31:55.660]   Yeah.
[00:31:55.660 --> 00:31:56.900]   I think there's a lot of work to be done.
[00:31:56.900 --> 00:31:58.160]   It's very subtle.
[00:31:58.160 --> 00:32:04.260]   We use the Aluther AI evaluation harness just because it's roughly standard right now, but
[00:32:04.260 --> 00:32:09.660]   obviously it is not a complete characterization of the behavior of a model.
[00:32:09.660 --> 00:32:14.260]   My kind of dream in the long-term for machine learning is that it turns into something that
[00:32:14.260 --> 00:32:16.460]   looks more like a taxonomic science.
[00:32:16.460 --> 00:32:20.780]   And what I mean by that is instead of studying an evaluation of one model at a particular
[00:32:20.780 --> 00:32:27.220]   point, we start to build family trees of these models and perform investigations on particular
[00:32:27.220 --> 00:32:32.920]   models that sit in strategic places on that family tree and try to generalize our findings
[00:32:32.920 --> 00:32:35.300]   about those things to families of models.
[00:32:35.300 --> 00:32:40.300]   So you can imagine a whole field of science that is based around measuring properties
[00:32:40.300 --> 00:32:44.460]   of machine learning models and then trying to see how they diffuse through the family
[00:32:44.460 --> 00:32:45.660]   tree, so to speak.
[00:32:45.660 --> 00:32:49.420]   And I guess if fine tuning is rampant, you also might really have a family tree or you
[00:32:49.420 --> 00:32:51.660]   could define clades of models.
[00:32:51.660 --> 00:32:52.660]   Yeah.
[00:32:52.660 --> 00:32:57.020]   So one of the things I keep referencing this paper we put out, it's called comparing foundation
[00:32:57.020 --> 00:32:58.540]   models using data kernels.
[00:32:58.540 --> 00:33:03.580]   It's myself, we had a research intern Hayden Helm and Johns Hopkins professor Kerry Preeb.
[00:33:03.580 --> 00:33:08.260]   One of the things we do in this paper is we show how you actually construct this family
[00:33:08.260 --> 00:33:12.600]   tree for a particular set of models, a constructed set of models.
[00:33:12.600 --> 00:33:17.020]   And what we essentially demonstrate is the family tree in some sense is very related
[00:33:17.020 --> 00:33:21.500]   to the concentrations of different concepts in the training data of these models.
[00:33:21.500 --> 00:33:26.460]   And so as you like, say, up weight plants in your training data and you maybe down weight
[00:33:26.460 --> 00:33:31.460]   artists or natural places, it like pushes you towards a different family of model.
[00:33:31.460 --> 00:33:37.540]   Do you also do RLHF on your models and how does that fit in with fine tuning?
[00:33:37.540 --> 00:33:40.500]   Do you view that as a kind of a specialist type of fine tuning?
[00:33:40.500 --> 00:33:41.500]   Yeah.
[00:33:41.500 --> 00:33:48.580]   So RLHF is in my mind, fine tuning when it's hard to define your objective function.
[00:33:48.580 --> 00:33:54.060]   So if you can get, say gold standard labels, so maybe let's take the classic cat dog example.
[00:33:54.060 --> 00:33:57.940]   It's pretty easy to get unambiguous cat dog labels on a data set.
[00:33:57.940 --> 00:34:01.200]   If it's only pets, you can just go the standard fine tuning approach.
[00:34:01.200 --> 00:34:06.220]   The thing that's difficult about people is that it's often very confusing to try and
[00:34:06.220 --> 00:34:07.700]   tease out what they want.
[00:34:07.700 --> 00:34:13.060]   And so what RLHF does is it first builds a model that tries to predict what people want,
[00:34:13.060 --> 00:34:15.300]   almost a model of that objective function.
[00:34:15.300 --> 00:34:20.140]   And then it trains a model to maximize the reward according to that objective function.
[00:34:20.140 --> 00:34:22.140]   And it's not something that we do.
[00:34:22.140 --> 00:34:28.700]   One thing that we are interested in is understanding how RLHF impacts the embedding spaces of these
[00:34:28.700 --> 00:34:29.700]   models.
[00:34:29.700 --> 00:34:32.700]   And I'm happy to talk more about that if you want to go in that direction.
[00:34:32.700 --> 00:34:37.940]   But I think it's a good tool to have in the case where your objective function is difficult
[00:34:37.940 --> 00:34:38.940]   to access.
[00:34:38.940 --> 00:34:41.820]   And do you actually do that with the models that you generate?
[00:34:41.820 --> 00:34:42.820]   We do not.
[00:34:42.820 --> 00:34:48.300]   So we did not do it on the GPT for all or the GPT for all J models.
[00:34:48.300 --> 00:34:51.300]   And a lot of the open source models nowadays are not.
[00:34:51.300 --> 00:34:52.300]   How come?
[00:34:52.300 --> 00:34:53.300]   I guess we haven't needed it.
[00:34:53.300 --> 00:34:54.300]   Interesting.
[00:34:54.300 --> 00:35:00.800]   I wonder, my understanding is that RLHF also allows you to nudge the models towards a more
[00:35:00.800 --> 00:35:03.340]   useful mode of interaction.
[00:35:03.340 --> 00:35:07.900]   And I wonder if that type of nudging could make the models feel better.
[00:35:07.900 --> 00:35:14.020]   One thing that I observe as an enthusiastic user of lots of these models is that the GPT
[00:35:14.020 --> 00:35:19.700]   for model, a lot of models claim to have very similar benchmarks to GPT for, but when I
[00:35:19.700 --> 00:35:24.580]   use GPT for, it feels more immediately useful to me.
[00:35:24.580 --> 00:35:30.600]   It's like the opposite for me of using mid journey, which feels like awesome versus Dolly,
[00:35:30.600 --> 00:35:32.580]   which feels like a little bit lame.
[00:35:32.580 --> 00:35:35.120]   And I also play with them with my daughter who's three.
[00:35:35.120 --> 00:35:38.720]   So she has maybe a more beginner mind on this stuff.
[00:35:38.720 --> 00:35:43.640]   And she really loves the interactions with GPT for and mid journey actually, because
[00:35:43.640 --> 00:35:49.200]   I think maybe they've spent more time pushing them into more, I don't know, like friendly
[00:35:49.200 --> 00:35:50.200]   mode.
[00:35:50.200 --> 00:35:53.280]   So I guess it obviously depends on what you're doing, but I'm just curious about the thinking
[00:35:53.280 --> 00:35:54.280]   there.
[00:35:54.280 --> 00:35:58.120]   I'll give you an interesting anecdote, which is the flip side of this.
[00:35:58.120 --> 00:36:03.600]   A lot of people in the AI writing communities refer to models that have gone through RLHF
[00:36:03.600 --> 00:36:04.600]   as lobotomized.
[00:36:04.600 --> 00:36:09.360]   And the reason why is because they become unable to produce certain genres of content.
[00:36:09.360 --> 00:36:13.320]   So I remember I was at this event in Brooklyn recently called Word Hack, where all of these
[00:36:13.320 --> 00:36:18.700]   sort of artists that use these models for various sort of random projects get together
[00:36:18.700 --> 00:36:20.180]   and like talk about what they're doing.
[00:36:20.180 --> 00:36:25.460]   And one of the people was lamenting that they couldn't write villains using GPT for because
[00:36:25.460 --> 00:36:29.820]   the villains would always realize the error of their ways immediately and then go and
[00:36:29.820 --> 00:36:30.860]   be good people.
[00:36:30.860 --> 00:36:33.980]   And it just depends on what you want, right?
[00:36:33.980 --> 00:36:39.500]   RLHF has been highly effective at nudging these models toward being like the ideal sort
[00:36:39.500 --> 00:36:40.500]   of corporate chatbot.
[00:36:40.500 --> 00:36:42.460]   And that's, there's definitely a place for that.
[00:36:42.460 --> 00:36:44.960]   That's probably a very viable use of these things.
[00:36:44.960 --> 00:36:51.000]   But I think it's such a small slice of what is possible with these models.
[00:36:51.000 --> 00:36:54.280]   And I think it's a little bit foolish to think it's the only thing that...
[00:36:54.280 --> 00:36:55.280]   Yeah, totally fair.
[00:36:55.280 --> 00:37:00.520]   I guess maybe there's a different kind of, I agree that GPT-4 has an annoying Boy Scout
[00:37:00.520 --> 00:37:04.400]   voice that's, that you could, now is like totally recognizable to me when people are
[00:37:04.400 --> 00:37:06.000]   sending me emails generated by it.
[00:37:06.000 --> 00:37:07.840]   I completely can tell.
[00:37:07.840 --> 00:37:12.840]   But I wonder if that's like an artifact of the particular like reward function that it's
[00:37:12.840 --> 00:37:15.000]   being given versus the process itself.
[00:37:15.000 --> 00:37:16.560]   Yeah, I'm sure it is.
[00:37:16.560 --> 00:37:20.680]   And I think this speaks more broadly to what I think will become sort of an industry level
[00:37:20.680 --> 00:37:26.320]   trend in that you'll have different foundation model providers that are interested in catering
[00:37:26.320 --> 00:37:28.920]   to different sort of groups of users.
[00:37:28.920 --> 00:37:33.080]   And so you'll have the foundation model provider that is maybe the best corporate chatbot,
[00:37:33.080 --> 00:37:35.700]   and that will probably be Anthropic or OpenAI.
[00:37:35.700 --> 00:37:40.920]   And then there's, I think, room for providers that want to be the best for say, creative
[00:37:40.920 --> 00:37:41.920]   writers.
[00:37:41.920 --> 00:37:45.320]   And that sort of gets back to this idea of a proliferation of models, right?
[00:37:45.320 --> 00:37:48.480]   There's a vast number of people that want to do a vast number of different things.
[00:37:48.480 --> 00:37:52.640]   And one optimization, I think, is not going to serve all of them.
[00:37:52.640 --> 00:37:59.200]   When you think of the open source models, I feel like most of them have come from still
[00:37:59.200 --> 00:38:01.800]   big corporations.
[00:38:01.800 --> 00:38:07.400]   Do you think there's room for startups or smaller companies to be building their own
[00:38:07.400 --> 00:38:08.400]   open source models?
[00:38:08.400 --> 00:38:09.760]   How would that actually happen?
[00:38:09.760 --> 00:38:13.280]   Yeah, if you look at the most original one, it was very grassroots.
[00:38:13.280 --> 00:38:15.360]   It's basically GPTJ, right?
[00:38:15.360 --> 00:38:20.640]   Which I think the exact quote was like, on Discord, someone was like, let's non-ironically
[00:38:20.640 --> 00:38:22.240]   give OpenAI a run for their money.
[00:38:22.240 --> 00:38:25.480]   It started as a meme, and then it worked, right?
[00:38:25.480 --> 00:38:29.600]   And now, a Luther is like a serious nonprofit organization doing really good work.
[00:38:29.600 --> 00:38:33.740]   And so I think it's feasible that open source is able to build some of these really great
[00:38:33.740 --> 00:38:34.800]   base models.
[00:38:34.800 --> 00:38:40.600]   I think it becomes challenging, especially as the race for compute heats up.
[00:38:40.600 --> 00:38:45.240]   You see every day a new $100 million round in the sum team of engineers because they're
[00:38:45.240 --> 00:38:48.240]   going to spend 80% of it on GPUs.
[00:38:48.240 --> 00:38:53.400]   And so the quip has become AI is a function from venture capital to Jensen and NVIDIA,
[00:38:53.400 --> 00:38:56.360]   which I think is probably pretty real in a lot of senses.
[00:38:56.360 --> 00:38:59.280]   And that really shuts down the access, for sure.
[00:38:59.280 --> 00:39:03.640]   I think this is why I'm interested in these more efficient training methodologies, because
[00:39:03.640 --> 00:39:09.120]   there's a lot that you can do, like a hacker can do at home with a commodity GPU, or maybe
[00:39:09.120 --> 00:39:14.400]   even a CPU soon with a quantized base model and some of these things like KubeLora.
[00:39:14.400 --> 00:39:16.480]   And I guess, can you say more about what you can do?
[00:39:16.480 --> 00:39:23.200]   Because just a 4x or a 20x or even a 100x speedup doesn't really seem like a hacker
[00:39:23.200 --> 00:39:27.400]   at home could really do much to compete with these gigantic models.
[00:39:27.400 --> 00:39:29.280]   So what really could you do?
[00:39:29.280 --> 00:39:33.480]   So you're never going to, I don't think we are going to see in the near future in open
[00:39:33.480 --> 00:39:36.240]   source GPT-4, right?
[00:39:36.240 --> 00:39:37.240]   It's too big.
[00:39:37.240 --> 00:39:39.200]   It requires too many resources, requires too much collaboration.
[00:39:39.200 --> 00:39:43.520]   But I think back to the early models that we built at RAD, and we were beating doctors
[00:39:43.520 --> 00:39:47.200]   on double blinds with models in the ones of billions of parameters at the time, because
[00:39:47.200 --> 00:39:50.040]   that was like big at the time, which is, it's weird to look back and think that was big
[00:39:50.040 --> 00:39:51.040]   at the time.
[00:39:51.040 --> 00:39:56.280]   But, and so if you can have a model do something as complex as medicine at the 1 billion parameter
[00:39:56.280 --> 00:40:01.840]   level, if you, like I said, start to define the particular use cases of these models,
[00:40:01.840 --> 00:40:07.280]   I think you can cover a wide variety of the task space with a little bit of intent.
[00:40:07.280 --> 00:40:13.100]   Do you think it's important as part of your business model to train models yourself?
[00:40:13.100 --> 00:40:14.100]   It's an evolving question.
[00:40:14.100 --> 00:40:16.640]   Look, Gnomic is a super early company.
[00:40:16.640 --> 00:40:18.200]   We've existed for less than two years.
[00:40:18.200 --> 00:40:20.640]   And so our business model is also evolving.
[00:40:20.640 --> 00:40:22.000]   We've certainly talked about it.
[00:40:22.000 --> 00:40:27.460]   We have some conversations with computer providers that are excited about the idea.
[00:40:27.460 --> 00:40:33.220]   It's unclear to me right now if we need another open source base model, right?
[00:40:33.220 --> 00:40:38.360]   We have a wide variety of really great ones, like between Falcon and MPT in particular.
[00:40:38.360 --> 00:40:46.000]   Yeah, I think in some sense, it's not worth omitting the carbon to make another open source
[00:40:46.000 --> 00:40:47.960]   base model trained on the pile.
[00:40:47.960 --> 00:40:51.240]   There's been some talk of, okay, if we can get a compelling enough set of tokens and
[00:40:51.240 --> 00:40:54.920]   there's a compelling enough use case, maybe it is worth doing it because we're serving
[00:40:54.920 --> 00:40:59.480]   some population that does not have access to a model that can assist them right now.
[00:40:59.480 --> 00:41:00.760]   So it's an evolving question.
[00:41:00.760 --> 00:41:04.520]   I guess one organization you haven't mentioned that seems to be funding a lot of this is
[00:41:04.520 --> 00:41:05.520]   Stability.
[00:41:05.520 --> 00:41:07.120]   Do you have thoughts on their role?
[00:41:07.120 --> 00:41:10.680]   I've only interacted a little bit with Stability.
[00:41:10.680 --> 00:41:13.600]   I don't have a ton of thoughts on their role.
[00:41:13.600 --> 00:41:17.720]   When I mentioned Aluther, Stability provides a lot of their compute and I think Ahmad sits
[00:41:17.720 --> 00:41:21.660]   on their board even to this day and in some sense, they're proximal.
[00:41:21.660 --> 00:41:25.120]   But yeah, I think their role is definitely evolving in the community.
[00:41:25.120 --> 00:41:31.000]   I guess, do you have thoughts on prompt engineering or chaining prompts?
[00:41:31.000 --> 00:41:35.360]   How do you think about the impact of that versus fine tuning models?
[00:41:35.360 --> 00:41:37.580]   Yeah, so I do have thoughts on this.
[00:41:37.580 --> 00:41:39.600]   The first thing I'll talk about is prompt chaining.
[00:41:39.600 --> 00:41:44.560]   I think something that people are only now starting to realize at large is that if you
[00:41:44.560 --> 00:41:50.440]   have a task, so a box in the flow chart of things that you want to do, and you have multiple
[00:41:50.440 --> 00:41:54.360]   calls to a language model to achieve that task, the performance goes up.
[00:41:54.360 --> 00:41:58.720]   So the first kind of easy example of this is something that people are calling validators
[00:41:58.720 --> 00:41:59.720]   nowadays.
[00:41:59.720 --> 00:42:03.640]   So you ask the model to complete a task and then you ask it to check its work.
[00:42:03.640 --> 00:42:06.840]   And it turns out it can detect when it's wrong a bunch of the time and your performance goes
[00:42:06.840 --> 00:42:07.840]   way up.
[00:42:07.840 --> 00:42:12.200]   And in general, I think you're going to see this trend of people starting to call the
[00:42:12.200 --> 00:42:14.920]   base models more and more for any particular task.
[00:42:14.920 --> 00:42:18.640]   And a recent paper that talks about this that I liked a lot is something called tree of
[00:42:18.640 --> 00:42:24.000]   thought prompting, where you've got your classic chain of thought, which is one branch of reasoning,
[00:42:24.000 --> 00:42:25.000]   so to speak.
[00:42:25.000 --> 00:42:29.760]   What tree of thought does is it says, let's have multiple hypotheses, multiple chains
[00:42:29.760 --> 00:42:33.760]   of thought being run in parallel with a validator prompt.
[00:42:33.760 --> 00:42:38.760]   So I can look at each of the chains of thought and evaluate whether or not I as the model,
[00:42:38.760 --> 00:42:40.440]   I'm going in a good direction.
[00:42:40.440 --> 00:42:42.440]   And that approach is incredibly powerful.
[00:42:42.440 --> 00:42:47.520]   And so I think we will continue to see prompting and chaining techniques proliferate more widely.
[00:42:47.520 --> 00:42:51.200]   And this is why things like, like Lang chain is really exciting to me for this reason.
[00:42:51.200 --> 00:42:55.760]   Cause I think you will need a really clean orchestration layer over these things.
[00:42:55.760 --> 00:42:59.560]   You also see things like the generative agents paper from Stanford, which starts to integrate
[00:42:59.560 --> 00:43:03.480]   things like external memory and like reasoning and reflection into these models.
[00:43:03.480 --> 00:43:09.680]   And these are all almost this like higher level of abstraction over calling the model
[00:43:09.680 --> 00:43:14.840]   and it can lead to these massive like gains in capabilities and like interesting emergent
[00:43:14.840 --> 00:43:15.840]   behaviors.
[00:43:15.840 --> 00:43:20.720]   And so I think all of that is going to be explored at great depth with great reward
[00:43:20.720 --> 00:43:21.920]   in the very near future.
[00:43:21.920 --> 00:43:28.480]   And if you were trying to make an LLM application today, like how much energy would you spend
[00:43:28.480 --> 00:43:34.160]   on fine tuning the model itself versus getting the right prompt out of the model to send
[00:43:34.160 --> 00:43:35.240]   into the model?
[00:43:35.240 --> 00:43:40.280]   I would spend, this is a non-answer, but I would spend all of my energy trying to define
[00:43:40.280 --> 00:43:47.080]   as exactly as I can, what role the model is playing in the flow chart of information that
[00:43:47.080 --> 00:43:48.900]   I'm trying to intervene in.
[00:43:48.900 --> 00:43:53.480]   Because only with a clear understanding of what box in the flow chart is that model filling,
[00:43:53.480 --> 00:43:54.480]   will you be able to-
[00:43:54.480 --> 00:43:55.920]   Can you give me some possible answers to that question?
[00:43:55.920 --> 00:44:00.880]   Give me an application that you've seen and how you would answer that for a common application.
[00:44:00.880 --> 00:44:06.960]   Probably the abstract answer to your question is you probably want to do a proof of concept
[00:44:06.960 --> 00:44:08.720]   with prompt engineering.
[00:44:08.720 --> 00:44:12.280]   And then once you have your data flywheel set up, that's when you want to start thinking
[00:44:12.280 --> 00:44:13.280]   about fine tuning.
[00:44:13.280 --> 00:44:14.280]   I see.
[00:44:14.280 --> 00:44:17.280]   Metric definition is a big part of the problem for a lot of these things.
[00:44:17.280 --> 00:44:21.160]   I'll actually, okay, let me tell you one of my favorite stories.
[00:44:21.160 --> 00:44:25.880]   So the first NeurIPS I ever went to, oh no, I think it was the second NeurIPS I ever went
[00:44:25.880 --> 00:44:27.600]   to, it was in Canada that year.
[00:44:27.600 --> 00:44:31.720]   I was a young starry eyed new PhD student sitting there trying to figure out what cool
[00:44:31.720 --> 00:44:34.120]   things I wanted to see at a poster session.
[00:44:34.120 --> 00:44:37.720]   And Oriol Vignoles walks by, who's essentially the deep mind guy.
[00:44:37.720 --> 00:44:38.720]   And I was starstruck.
[00:44:38.720 --> 00:44:39.720]   I was like, "Oh my God, it's Oriol Vignoles.
[00:44:39.720 --> 00:44:41.080]   I need to go talk to him."
[00:44:41.080 --> 00:44:43.480]   So I run up to him and start talking to him a little bit.
[00:44:43.480 --> 00:44:46.320]   And at one point, I just ask him for life advice.
[00:44:46.320 --> 00:44:48.520]   I like to do this with interesting people that I meet.
[00:44:48.520 --> 00:44:50.600]   And I'm interested in what you might say to me as well.
[00:44:50.600 --> 00:44:55.800]   And he ponders for a minute and he goes, "You should know your objective function."
[00:44:55.800 --> 00:44:59.080]   And I think that's one of the wisest things anyone has ever said to me.
[00:44:59.080 --> 00:45:00.080]   Wow.
[00:45:00.080 --> 00:45:01.120]   Do you know your objective function?
[00:45:01.120 --> 00:45:02.920]   I think it is constantly...
[00:45:02.920 --> 00:45:03.920]   Okay.
[00:45:03.920 --> 00:45:07.080]   On June 30th, 2023, what is your objective function?
[00:45:07.080 --> 00:45:10.120]   The objective function for Brandon the human, the objective function for Nomik.
[00:45:10.120 --> 00:45:13.560]   I feel like the advice was to know your objective function as a human.
[00:45:13.560 --> 00:45:17.320]   I'd be curious about the objective function for Nomik, but I may be more curious about
[00:45:17.320 --> 00:45:19.440]   the objective function for you as a human.
[00:45:19.440 --> 00:45:20.440]   Yeah.
[00:45:20.440 --> 00:45:23.720]   So Nomics is pretty straightforward because it's on our website and we've thought quite
[00:45:23.720 --> 00:45:24.720]   a bit about it.
[00:45:24.720 --> 00:45:28.520]   We want to improve the explainability and accessibility of AI and do what we can there.
[00:45:28.520 --> 00:45:32.400]   For Brandon the human, it becomes slightly more complicated because as a physical being,
[00:45:32.400 --> 00:45:37.640]   I have needs like being engaged in my life and getting sustenance and providing for people
[00:45:37.640 --> 00:45:39.600]   in my support network and my community.
[00:45:39.600 --> 00:45:43.080]   And so there's like this additional term that you have to tack on to the Nomic mission,
[00:45:43.080 --> 00:45:47.220]   which is ensure that Brandon and his community are also flourishing.
[00:45:47.220 --> 00:45:51.640]   So it's basically the Brandon objective function is maybe 70% Nomic objective function plus
[00:45:51.640 --> 00:45:54.840]   30% some level of personal and community fulfillment.
[00:45:54.840 --> 00:45:55.840]   That's a good.
[00:45:55.840 --> 00:45:56.840]   Do you know your objective function?
[00:45:56.840 --> 00:45:57.840]   No.
[00:45:57.840 --> 00:46:00.560]   I mean, I would say I think about it a lot, so I don't think it's necessarily bad advice
[00:46:00.560 --> 00:46:05.720]   to, I think the part that I agree with is I guess being clear about what you're trying
[00:46:05.720 --> 00:46:10.400]   to do and being practical about trying to get the things that you want.
[00:46:10.400 --> 00:46:13.800]   And I think sometimes people get a little bit, or I think, yeah, I guess I do think
[00:46:13.800 --> 00:46:15.240]   that's good advice.
[00:46:15.240 --> 00:46:18.280]   For me, I break it into years.
[00:46:18.280 --> 00:46:22.240]   Every year I think about what's my sort of theme for the year, which is maybe more of
[00:46:22.240 --> 00:46:26.080]   a nudge of what I'm trying to do than radically.
[00:46:26.080 --> 00:46:29.760]   Maybe it's just like a piece of the, maybe it's like the derivative of the function that
[00:46:29.760 --> 00:46:32.880]   I'm trying to change versus the function itself, if that makes sense.
[00:46:32.880 --> 00:46:35.880]   So every year you take a gradient step in a direction.
[00:46:35.880 --> 00:46:36.880]   Exactly.
[00:46:36.880 --> 00:46:37.880]   That's what I try to do.
[00:46:37.880 --> 00:46:40.640]   So this year I've been trying to learn more actually, which is, I guess doing this interview
[00:46:40.640 --> 00:46:45.320]   is a good example of things I'm trying to do to move in that direction.
[00:46:45.320 --> 00:46:49.600]   But yeah, like last year I was trying to enjoy the growth of my company more.
[00:46:49.600 --> 00:46:51.320]   But again, these aren't the only things I care about.
[00:46:51.320 --> 00:46:57.080]   I have a family and kids and lots of things that are like probably higher weight, but
[00:46:57.080 --> 00:47:02.680]   I feel like it's in the deltas that I can maybe make little steps to improve the function.
[00:47:02.680 --> 00:47:04.360]   And then the question still stands for you.
[00:47:04.360 --> 00:47:05.560]   Do you have any advice for me?
[00:47:05.560 --> 00:47:10.800]   I guess for you, it's, I think I see you as someone with a really exciting company that's
[00:47:10.800 --> 00:47:11.800]   growing.
[00:47:11.800 --> 00:47:18.120]   So I think my main advice is more professional than, that's where my brain goes with something
[00:47:18.120 --> 00:47:19.920]   like I might be able to offer.
[00:47:19.920 --> 00:47:22.360]   And all these things feel like cliches.
[00:47:22.360 --> 00:47:26.520]   Maybe I don't have anything quite as pithy as know your objective function.
[00:47:26.520 --> 00:47:31.120]   But I think one thing that's hard about having a fast growing startup is that the important
[00:47:31.120 --> 00:47:35.560]   things to do change at each place that the company goes.
[00:47:35.560 --> 00:47:40.840]   And I think that you can get really bad advice from people when they think you're in a different
[00:47:40.840 --> 00:47:43.040]   phase than where you're at.
[00:47:43.040 --> 00:47:46.360]   Like when you're small, man, you really just want to ignore everything and make a good
[00:47:46.360 --> 00:47:47.360]   product.
[00:47:47.360 --> 00:47:51.000]   Like worrying about OKRs, worrying about anything else is such a trap.
[00:47:51.000 --> 00:47:54.880]   And so I think it's easy to get in this habit of just, man, these lunatics are giving me
[00:47:54.880 --> 00:47:55.880]   terrible advice.
[00:47:55.880 --> 00:47:59.800]   If you talk to the CEO of a bigger company, just be like, I don't get it.
[00:47:59.800 --> 00:48:03.920]   But then I think actually you will get to a stage where I think like running the company
[00:48:03.920 --> 00:48:07.760]   well becomes like a much more important thing to focus on.
[00:48:07.760 --> 00:48:12.320]   And I think like showing up well for your company starts to get really important, maybe
[00:48:12.320 --> 00:48:16.800]   more so than any individual contribution that you can make to the company in most cases.
[00:48:16.800 --> 00:48:19.720]   I guess that'd be my high level professional advice.
[00:48:19.720 --> 00:48:24.880]   But now that you have established yourself more, and I think a lot of our listeners are
[00:48:24.880 --> 00:48:26.760]   in college from the ones that reach out to me.
[00:48:26.760 --> 00:48:27.760]   I don't know.
[00:48:27.760 --> 00:48:30.040]   Do you have any advice to a younger version of yourself?
[00:48:30.040 --> 00:48:33.960]   Or what if you walked by somebody at NeurIPS and they grabbed you by your paper and asked
[00:48:33.960 --> 00:48:34.960]   you for advice?
[00:48:34.960 --> 00:48:35.960]   What would you tell them?
[00:48:35.960 --> 00:48:37.960]   I would say it's towards building.
[00:48:37.960 --> 00:48:45.120]   I think I look back at how Nomik got created and like where the skillset to actually build
[00:48:45.120 --> 00:48:46.680]   these things came from.
[00:48:46.680 --> 00:48:50.440]   And certainly like the standardized education I've had helped.
[00:48:50.440 --> 00:48:56.660]   But the thing that by and large was the most useful was just like spending time hacking
[00:48:56.660 --> 00:48:58.880]   on side projects, like going to hackathons.
[00:48:58.880 --> 00:49:02.480]   Maybe the concrete thing is just go to more hackathons and just actually get your hands
[00:49:02.480 --> 00:49:04.540]   dirty like doing things.
[00:49:04.540 --> 00:49:06.600]   And these things don't have to be great.
[00:49:06.600 --> 00:49:12.320]   Often they probably won't work, but create something that like zero to one process has
[00:49:12.320 --> 00:49:19.560]   such an incredible ability to make you learn and really start to unlock systems that the
[00:49:19.560 --> 00:49:21.920]   more you invest in that, I think the better off you are.
[00:49:21.920 --> 00:49:22.920]   That's funny.
[00:49:22.920 --> 00:49:27.640]   Like for me, I find it hard to learn really in any other way.
[00:49:27.640 --> 00:49:32.240]   I get nervous interviewing someone like you when I haven't actually used your library.
[00:49:32.240 --> 00:49:35.620]   I feel like my younger self would have just had time to use every single library.
[00:49:35.620 --> 00:49:37.400]   And it's scary to me.
[00:49:37.400 --> 00:49:40.960]   Like when you just look at something, but you haven't actually tried it.
[00:49:40.960 --> 00:49:45.960]   I just feel like I wish I had a more efficient process for learning things, but I just, I
[00:49:45.960 --> 00:49:46.960]   really don't.
[00:49:46.960 --> 00:49:50.400]   Like even with my own product, when we launch something and I haven't used it myself, it
[00:49:50.400 --> 00:49:54.380]   gives me a lot of anxiety until I've actually engaged with it as an engineer.
[00:49:54.380 --> 00:49:57.400]   And most of it is like fighting with error messages that I don't understand.
[00:49:57.400 --> 00:50:00.040]   I think that's 90% of the building process these days.
[00:50:00.040 --> 00:50:04.360]   But somehow I feel like I need that suffering to feel like I actually understand something.
[00:50:04.360 --> 00:50:05.480]   I think that's right.
[00:50:05.480 --> 00:50:11.920]   And hearkening back to this idea of how companies change over time, a new thing for me has been
[00:50:11.920 --> 00:50:16.000]   Gnomic getting to the point where I can't keep the entire state of the company in my
[00:50:16.000 --> 00:50:17.200]   brain at once.
[00:50:17.200 --> 00:50:18.600]   And that has been very scary, right?
[00:50:18.600 --> 00:50:23.560]   Like back when we were a couple of people like hacking on a code base and touching every
[00:50:23.560 --> 00:50:26.400]   part of the system, like all the early engineers at Gnomic were full stack.
[00:50:26.400 --> 00:50:27.720]   And so everyone touched everything.
[00:50:27.720 --> 00:50:30.200]   I could tell you exactly the state of every partnership.
[00:50:30.200 --> 00:50:32.400]   I could tell you exactly the state of every employee.
[00:50:32.400 --> 00:50:35.320]   I could tell you exactly what's in every file on the code base.
[00:50:35.320 --> 00:50:36.320]   And that was wonderful.
[00:50:36.320 --> 00:50:40.080]   And now it's just infeasible to allocate my attention such that I can keep up with it,
[00:50:40.080 --> 00:50:41.780]   especially in open source.
[00:50:41.780 --> 00:50:45.640]   It's like I wake up in the morning and there's just like another 50 pull requests.
[00:50:45.640 --> 00:50:50.640]   I'm just like, Oh man, this is another thing that's impressive about Lang chain is like
[00:50:50.640 --> 00:50:56.480]   the pace at which they keep up with the community, like their ability to just continue aggregating
[00:50:56.480 --> 00:50:59.280]   everybody's opinion at such a rapid cadence.
[00:50:59.280 --> 00:51:00.280]   It really is amazing.
[00:51:00.280 --> 00:51:05.040]   It reminds me of actually good advice that I got that I'll pass along to you and to anybody
[00:51:05.040 --> 00:51:06.040]   listening.
[00:51:06.040 --> 00:51:09.660]   I talked to a lot of CEOs running bigger companies to try to learn from them.
[00:51:09.660 --> 00:51:11.760]   And mostly the advice is hard to use.
[00:51:11.760 --> 00:51:16.400]   But one practical story that I think wasn't even framed as advice was Lou Cern, the CEO
[00:51:16.400 --> 00:51:17.400]   of New Relic.
[00:51:17.400 --> 00:51:22.180]   When it was like a public company, he told me that he spends one week every month basically
[00:51:22.180 --> 00:51:24.160]   by himself coding.
[00:51:24.160 --> 00:51:29.240]   And that was like a real wake up call to me just as someone who runs a much, much smaller
[00:51:29.240 --> 00:51:33.280]   company who feels like overwhelmed by the day to day work and probably spends less of
[00:51:33.280 --> 00:51:37.640]   the time doing projects to at least take like once a quarter to get like an uninterrupted
[00:51:37.640 --> 00:51:39.800]   block of time to work on something.
[00:51:39.800 --> 00:51:41.760]   I think that's really served me well.
[00:51:41.760 --> 00:51:42.760]   Yeah.
[00:51:42.760 --> 00:51:44.360]   I might try and implement that.
[00:51:44.360 --> 00:51:49.840]   Like I always joke that my GitHub commit history tells the story of my startup better than
[00:51:49.840 --> 00:51:52.440]   any other could, any other piece of media could.
[00:51:52.440 --> 00:51:56.720]   Because in the early days when it's like a hack project before anyone else was on board,
[00:51:56.720 --> 00:52:00.320]   you just see two green strips on Saturdays and Sundays.
[00:52:00.320 --> 00:52:03.920]   And then you can tell exactly when I left my full-time job to go all in on it because
[00:52:03.920 --> 00:52:05.240]   it's the green block.
[00:52:05.240 --> 00:52:09.920]   You can tell when we raised seed because there's a little white sliver in the middle of the
[00:52:09.920 --> 00:52:12.660]   green blocks and then it goes back to green.
[00:52:12.660 --> 00:52:16.580]   And then something very interesting happened a couple of months ago now with the launch
[00:52:16.580 --> 00:52:19.780]   of GPT for all where suddenly it became very white.
[00:52:19.780 --> 00:52:22.080]   And that is something I've been learning to try and deal with.
[00:52:22.080 --> 00:52:25.360]   We always end with two questions and I want to make sure I get them in.
[00:52:25.360 --> 00:52:31.320]   One is, what do you think is currently an underrated topic in machine learning that
[00:52:31.320 --> 00:52:34.760]   you think people should pay more attention to or you would like to spend more time on?
[00:52:34.760 --> 00:52:35.760]   That's a really good question.
[00:52:35.760 --> 00:52:40.720]   I think it's starting to become more popular, but policy in some sense.
[00:52:40.720 --> 00:52:47.880]   I was at this event the other day and talking to someone that does policy at OpenAI.
[00:52:47.880 --> 00:52:54.920]   And one of the things they said that resonated with me most was this idea of how economically
[00:52:54.920 --> 00:52:58.740]   empowering and disempowering this technology is going to be.
[00:52:58.740 --> 00:53:04.440]   And so I think trying to really take a moment and understand how we can implement these
[00:53:04.440 --> 00:53:10.060]   tools in a way that is maximally empowering to everyone is going to be like increasingly
[00:53:10.060 --> 00:53:11.060]   important.
[00:53:11.060 --> 00:53:15.760]   There's a great book called Player Piano by Kurt Vonnegut that feels very appropriate
[00:53:15.760 --> 00:53:21.240]   for this time and that I think about quite a lot because it feels like we're moving into
[00:53:21.240 --> 00:53:22.240]   that world.
[00:53:22.240 --> 00:53:25.580]   I don't know personally what the solution is.
[00:53:25.580 --> 00:53:30.920]   My conjecture is that giving as many people as much access to these models as possible
[00:53:30.920 --> 00:53:35.920]   is the best way to try and allow them to reap the benefits of these things.
[00:53:35.920 --> 00:53:36.920]   Interesting.
[00:53:36.920 --> 00:53:41.360]   Do you have any papers on policy that you think are interesting?
[00:53:41.360 --> 00:53:44.100]   I feel like policy is obviously so important.
[00:53:44.100 --> 00:53:48.120]   But the thing that's hard for me about engaging with policy is it's hard for me to find stuff
[00:53:48.120 --> 00:53:51.280]   where it feels tangible or useful.
[00:53:51.280 --> 00:53:52.280]   Do you know what I mean?
[00:53:52.280 --> 00:53:56.640]   No doubt, what could have a higher impact than creating better policies around all the
[00:53:56.640 --> 00:53:58.520]   big changes that are coming to our society?
[00:53:58.520 --> 00:54:02.640]   But I find it hard to find stuff that I can really sink my teeth into.
[00:54:02.640 --> 00:54:04.840]   Do you have any recommendations there?
[00:54:04.840 --> 00:54:05.840]   No.
[00:54:05.840 --> 00:54:09.920]   I think this is one of the reasons why I cite it as being underexplored because all of what
[00:54:09.920 --> 00:54:14.320]   I've learned about it is through conversations with people that work at these places that
[00:54:14.320 --> 00:54:16.000]   I happen to have access to.
[00:54:16.000 --> 00:54:18.200]   And I'm glad they're at least thinking about the problem.
[00:54:18.200 --> 00:54:22.920]   But I think this is going to sound like a broken record, but I think going back and
[00:54:22.920 --> 00:54:28.560]   engaging with Player Piano by Vonnegut is the best thing that you can do in terms of
[00:54:28.560 --> 00:54:31.600]   getting intuition for the cautionary tale.
[00:54:31.600 --> 00:54:36.600]   This is the case that we need to maybe think that we should be avoiding with this new technology.
[00:54:36.600 --> 00:54:38.280]   And it's funny, I read that book as a teenager.
[00:54:38.280 --> 00:54:39.280]   I'm trying to remember it.
[00:54:39.280 --> 00:54:41.120]   I think it's almost nobody has jobs at that point.
[00:54:41.120 --> 00:54:45.160]   And somebody builds an automation system and automates himself out of a job and then they
[00:54:45.160 --> 00:54:46.160]   don't have a job, right?
[00:54:46.160 --> 00:54:48.720]   Like they get some bonus and it's dystopian in that way.
[00:54:48.720 --> 00:54:49.720]   That's the caution.
[00:54:49.720 --> 00:54:50.720]   Yeah.
[00:54:50.720 --> 00:54:56.680]   It's the main premise of the book is details of society where technology has progressed
[00:54:56.680 --> 00:55:00.720]   to the point where most people cannot work on the automaton.
[00:55:00.720 --> 00:55:04.040]   And so there's this very small number of like hyper-specialized people that are actually
[00:55:04.040 --> 00:55:06.720]   functionally able to work on these things.
[00:55:06.720 --> 00:55:11.080]   And everybody else is just pushed to the outskirts of the manufacturing facility.
[00:55:11.080 --> 00:55:14.840]   And there is a character that at one point actually does automate themselves out of a
[00:55:14.840 --> 00:55:16.440]   job as I recall.
[00:55:16.440 --> 00:55:21.520]   And I think we need to figure out how that does not become the case, right?
[00:55:21.520 --> 00:55:25.600]   We need to figure out how to make sure that these tools are being integrated with people
[00:55:25.600 --> 00:55:27.360]   as opposed to replacing them.
[00:55:27.360 --> 00:55:28.360]   Okay.
[00:55:28.360 --> 00:55:31.640]   And my final question, a little bit of a different direction, is when you're trying to get these
[00:55:31.640 --> 00:55:35.520]   models to work for some useful task, what's the biggest practical challenge?
[00:55:35.520 --> 00:55:37.040]   It's another good question.
[00:55:37.040 --> 00:55:40.520]   I think I'm going to give you two answers.
[00:55:40.520 --> 00:55:41.520]   Specification is very important.
[00:55:41.520 --> 00:55:43.600]   And I think we've hit on that a lot.
[00:55:43.600 --> 00:55:47.200]   Really having a clear idea of what exactly you want the model to be doing, what exact
[00:55:47.200 --> 00:55:49.980]   role it's filling is very critical.
[00:55:49.980 --> 00:55:51.680]   And then visibility, right?
[00:55:51.680 --> 00:55:56.600]   These systems are still widely, their mechanisms are widely unknown.
[00:55:56.600 --> 00:56:02.760]   And so getting as accurate of a mental model as possible about what is actually going on
[00:56:02.760 --> 00:56:08.160]   so that you can start to reason about it is, I think, highly critical and really was the
[00:56:08.160 --> 00:56:10.560]   impetus for the first generation of Atlas.
[00:56:10.560 --> 00:56:11.560]   All right.
[00:56:11.560 --> 00:56:12.560]   Thanks so much, Brandon.
[00:56:12.560 --> 00:56:14.480]   Real pleasure to talk to you today.
[00:56:14.480 --> 00:56:15.480]   Yeah.
[00:56:15.480 --> 00:56:16.480]   Thank you.
[00:56:16.480 --> 00:56:19.440]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:56:19.440 --> 00:56:24.160]   to the show notes in the description where you can find links to all the papers that
[00:56:24.160 --> 00:56:28.560]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:56:28.560 --> 00:56:29.560]   produce.
[00:56:29.560 --> 00:56:29.560]   So check it out.
[00:56:29.560 --> 00:56:32.140]   (upbeat music)
[00:56:32.140 --> 00:56:42.120]   [BLANK_AUDIO]

