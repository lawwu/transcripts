
[00:00:00.000 --> 00:00:05.000]   - Let me ask you about a field of machine learning,
[00:00:05.000 --> 00:00:06.560]   deep learning.
[00:00:06.560 --> 00:00:08.920]   There's been a lot of progress in neural networks
[00:00:08.920 --> 00:00:11.520]   based neural network based machine learning
[00:00:11.520 --> 00:00:14.000]   in the recent decade.
[00:00:14.000 --> 00:00:17.560]   Of course, neural network research goes back many decades.
[00:00:17.560 --> 00:00:23.220]   What do you think are the limits of deep learning,
[00:00:23.220 --> 00:00:26.120]   of neural network based machine learning?
[00:00:26.120 --> 00:00:28.760]   - Well, to give a real answer to that,
[00:00:28.760 --> 00:00:32.520]   you'd have to understand the exact processes
[00:00:32.520 --> 00:00:35.520]   that are taking place, and those are pretty opaque.
[00:00:35.520 --> 00:00:37.840]   So it's pretty hard to prove a theorem
[00:00:37.840 --> 00:00:41.640]   about what can be done and what can't be done.
[00:00:41.640 --> 00:00:44.400]   But I think it's reasonably clear.
[00:00:44.400 --> 00:00:46.800]   I mean, putting technicalities aside,
[00:00:46.800 --> 00:00:49.520]   what deep learning is doing
[00:00:49.520 --> 00:00:52.940]   is taking huge numbers of examples
[00:00:52.940 --> 00:00:55.320]   and finding some patterns.
[00:00:55.320 --> 00:00:59.560]   - Okay, that could be interesting. In some areas it is.
[00:00:59.560 --> 00:01:02.680]   But we have to ask here a certain question.
[00:01:02.680 --> 00:01:05.820]   Is it engineering or is it science?
[00:01:05.820 --> 00:01:08.160]   Engineering in the sense of just trying
[00:01:08.160 --> 00:01:10.420]   to build something that's useful,
[00:01:10.420 --> 00:01:12.400]   or science in the sense that it's trying
[00:01:12.400 --> 00:01:16.320]   to understand something about elements of the world.
[00:01:16.320 --> 00:01:19.400]   So it takes a Google parser.
[00:01:19.400 --> 00:01:21.480]   We can ask that question.
[00:01:21.480 --> 00:01:22.760]   Is it useful?
[00:01:22.760 --> 00:01:24.040]   Yeah, it's pretty useful.
[00:01:24.040 --> 00:01:27.000]   You know, I use a Google translator.
[00:01:27.000 --> 00:01:29.480]   So on engineering grounds,
[00:01:29.480 --> 00:01:32.500]   it's kind of worth having, like a bulldozer.
[00:01:32.500 --> 00:01:36.560]   Does it tell you anything about human language?
[00:01:36.560 --> 00:01:37.400]   Zero.
[00:01:37.400 --> 00:01:39.420]   Nothing.
[00:01:39.420 --> 00:01:41.760]   And in fact, it's very striking.
[00:01:41.760 --> 00:01:44.420]   It's from the very beginning,
[00:01:44.420 --> 00:01:47.960]   it's just totally remote from science.
[00:01:47.960 --> 00:01:50.220]   So what is a Google parser doing?
[00:01:50.220 --> 00:01:52.800]   It's taking an enormous text,
[00:01:52.800 --> 00:01:55.320]   let's say the Wall Street Journal corpus,
[00:01:55.320 --> 00:01:58.200]   and asking how close can we come
[00:01:58.200 --> 00:02:01.760]   to getting the right description
[00:02:01.760 --> 00:02:04.040]   of every sentence in the corpus.
[00:02:04.040 --> 00:02:06.160]   Well, every sentence in the corpus
[00:02:06.160 --> 00:02:08.320]   is essentially an experiment.
[00:02:08.320 --> 00:02:12.380]   Each sentence that you produce is an experiment,
[00:02:12.380 --> 00:02:15.320]   which is, am I a grammatical sentence?
[00:02:15.320 --> 00:02:17.280]   The answer is usually yes.
[00:02:17.280 --> 00:02:20.880]   So most of the stuff in the corpus is grammatical sentences.
[00:02:20.880 --> 00:02:22.580]   But now ask yourself,
[00:02:22.580 --> 00:02:27.580]   is there any science which takes random experiments,
[00:02:27.580 --> 00:02:31.320]   which are carried out for no reason whatsoever,
[00:02:31.320 --> 00:02:34.160]   and tries to find out something from them?
[00:02:34.160 --> 00:02:37.280]   Like if you're, say, a chemistry PhD student,
[00:02:37.280 --> 00:02:38.840]   you wanna get a thesis, can you say,
[00:02:38.840 --> 00:02:41.040]   well, I'm just gonna do a lot of,
[00:02:41.040 --> 00:02:44.120]   mix a lot of things together, no purpose,
[00:02:44.120 --> 00:02:47.280]   just, and maybe I'll find something.
[00:02:47.280 --> 00:02:50.040]   You'd be laughed out of the department.
[00:02:50.040 --> 00:02:53.820]   Science tries to find critical experiments,
[00:02:53.820 --> 00:02:56.740]   ones that answer some theoretical question.
[00:02:56.740 --> 00:03:00.580]   Doesn't care about coverage of millions of experiments.
[00:03:00.580 --> 00:03:03.840]   So it just begins by being very remote from science,
[00:03:03.840 --> 00:03:05.860]   and it continues like that.
[00:03:05.860 --> 00:03:08.940]   So the usual question that's asked
[00:03:08.940 --> 00:03:11.140]   about, say, a Google parser,
[00:03:11.140 --> 00:03:13.780]   is how well does it do, or some parser,
[00:03:13.780 --> 00:03:15.940]   how well does it do on a corpus?
[00:03:15.940 --> 00:03:18.740]   But there's another question that's never asked.
[00:03:18.740 --> 00:03:20.520]   How well does it do on something
[00:03:20.520 --> 00:03:23.700]   that violates all the rules of language?
[00:03:23.700 --> 00:03:26.320]   So for example, take the structure dependence case
[00:03:26.320 --> 00:03:27.280]   that I mentioned.
[00:03:27.280 --> 00:03:29.800]   Suppose there was a language in which
[00:03:29.800 --> 00:03:34.800]   you used linear proximity as the mode of interpretation.
[00:03:34.800 --> 00:03:39.320]   These deep learning would work very easily on that.
[00:03:39.320 --> 00:03:42.380]   In fact, much more easily than an actual language.
[00:03:42.380 --> 00:03:43.560]   Is that a success?
[00:03:43.560 --> 00:03:45.200]   No, that's a failure.
[00:03:45.200 --> 00:03:48.000]   From a scientific point of view, it's a failure.
[00:03:48.000 --> 00:03:51.160]   It shows that we're not discovering
[00:03:51.160 --> 00:03:53.440]   the nature of the system at all,
[00:03:53.440 --> 00:03:55.360]   'cause it does just as well or even better
[00:03:55.360 --> 00:03:58.480]   on things that violate the structure of the system.
[00:03:58.480 --> 00:04:00.320]   And it goes on from there.
[00:04:00.320 --> 00:04:02.400]   It's not an argument against doing it.
[00:04:02.400 --> 00:04:04.800]   It is useful to have devices like this.
[00:04:04.800 --> 00:04:08.280]   - So yes, so neural networks are kind of approximators
[00:04:08.280 --> 00:04:11.800]   that look, there's echoes of the behavioral debates,
[00:04:11.800 --> 00:04:13.760]   right, behavioralism.
[00:04:13.760 --> 00:04:15.200]   - More than echoes.
[00:04:15.200 --> 00:04:17.680]   Many of the people in deep learning
[00:04:17.680 --> 00:04:22.200]   say they've vindicated, Terry Sanyoski, for example,
[00:04:22.200 --> 00:04:25.160]   in his recent books, says this vindicates
[00:04:25.160 --> 00:04:27.160]   Skinnerian behaviors.
[00:04:27.160 --> 00:04:29.080]   It doesn't have anything to do with it.
[00:04:29.080 --> 00:04:31.400]   - Yes, but I think there's something
[00:04:31.400 --> 00:04:35.920]   actually fundamentally different when the data set is huge.
[00:04:35.920 --> 00:04:38.800]   But your point is extremely well taken.
[00:04:38.800 --> 00:04:43.040]   But do you think we can learn, approximate,
[00:04:43.040 --> 00:04:46.420]   that interesting complex structure of language
[00:04:46.420 --> 00:04:48.440]   with neural networks that will somehow
[00:04:48.440 --> 00:04:50.380]   help us understand the science?
[00:04:50.380 --> 00:04:52.120]   - It's possible.
[00:04:52.120 --> 00:04:54.880]   I mean, you find patterns that you hadn't noticed,
[00:04:54.880 --> 00:04:57.360]   let's say, could be.
[00:04:57.360 --> 00:05:01.240]   In fact, it's very much like a kind of linguistics
[00:05:01.240 --> 00:05:05.720]   that's done, what's called corpus linguistics.
[00:05:05.720 --> 00:05:08.720]   When you, suppose you have some language
[00:05:08.720 --> 00:05:11.080]   where all the speakers have died out,
[00:05:11.080 --> 00:05:12.760]   but you have records.
[00:05:12.760 --> 00:05:15.720]   So you just look at the records
[00:05:15.720 --> 00:05:18.200]   and see what you can figure out from that.
[00:05:18.200 --> 00:05:21.280]   It's much better to have actual speakers
[00:05:21.280 --> 00:05:23.680]   where you can do critical experiments.
[00:05:23.680 --> 00:05:26.120]   But if they're all dead, you can't do them.
[00:05:26.120 --> 00:05:28.400]   So you have to try to see what you can find out
[00:05:28.400 --> 00:05:31.480]   from just looking at the data that's around.
[00:05:31.480 --> 00:05:32.640]   You can learn things.
[00:05:32.640 --> 00:05:36.000]   Actually, paleoanthropology is very much like that.
[00:05:36.000 --> 00:05:38.240]   You can't do a critical experiment
[00:05:38.240 --> 00:05:41.120]   on what happened two million years ago.
[00:05:41.120 --> 00:05:44.160]   So you're kind of forced just to take what data's around
[00:05:44.160 --> 00:05:46.800]   and see what you can figure out from it.
[00:05:46.800 --> 00:05:48.880]   Okay, it's a serious study.
[00:05:48.880 --> 00:05:51.460]   (upbeat music)
[00:05:51.460 --> 00:05:54.040]   (upbeat music)
[00:05:54.040 --> 00:05:56.620]   (upbeat music)
[00:05:56.620 --> 00:05:59.200]   (upbeat music)
[00:05:59.200 --> 00:06:01.780]   (upbeat music)
[00:06:01.780 --> 00:06:04.360]   (upbeat music)
[00:06:04.360 --> 00:06:14.360]   [BLANK_AUDIO]

