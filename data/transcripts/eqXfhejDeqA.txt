
[00:00:00.000 --> 00:00:05.000]   I obviously DeepMind is at the frontier
[00:00:05.000 --> 00:00:10.000]   and has been for many years with systems like AlphaZero
[00:00:10.000 --> 00:00:15.000]   and so forth of having these agents who can think through
[00:00:15.000 --> 00:00:20.000]   different steps to get to an end outcome.
[00:00:20.000 --> 00:00:25.000]   Will this just be is a path for LLMs to have this sort of tree search kind of thing on top of them?
[00:00:25.000 --> 00:00:30.000]   Basically making them more and more accurate predictors of the world.
[00:00:30.000 --> 00:00:35.000]   So in effect making them more and more reliable world models.
[00:00:35.000 --> 00:00:40.000]   That's clearly a necessary but I would say probably not sufficient component of an AGI system.
[00:00:40.000 --> 00:00:45.000]   And then on top of that we're working on things like AlphaZero like planning mechanisms on top
[00:00:45.000 --> 00:00:50.000]   that make use of that model in order to make concrete plans to achieve certain goals in the world
[00:00:50.000 --> 00:00:55.000]   and perhaps sort of chain thought together or lines of reasoning together
[00:00:55.000 --> 00:01:00.000]   and maybe use search to kind of explore massive spaces of possibility.
[00:01:00.000 --> 00:01:05.000]   I think that's kind of missing from our current large models.
[00:01:05.000 --> 00:01:10.000]   Is there any potential for the AGI to eventually come from just a pure RL approach?
[00:01:10.000 --> 00:01:15.000]   Like the way we're talking about it, it sounds like the LLM will form right prior
[00:01:15.000 --> 00:01:20.000]   completely out of the blue.
[00:01:20.000 --> 00:01:25.000]   There's no reason why you couldn't go full AlphaZero like on it.
[00:01:25.000 --> 00:01:30.000]   And there are some people here at Google DeepMind and in the RL community who work on that.
[00:01:30.000 --> 00:01:35.000]   Fully assuming no priors, no data, and just build all knowledge from scratch.
[00:01:35.000 --> 00:01:40.000]   And I think that's valuable because of course those ideas and those algorithms
[00:01:40.000 --> 00:01:45.000]   should also work when you have some knowledge too.
[00:01:45.000 --> 00:01:50.000]   But having said that, I think by far probably my betting would be the quickest way to get to AGI
[00:01:50.000 --> 00:01:55.000]   and the most likely plausible way is to use all the knowledge that's existing in the world right now
[00:01:55.000 --> 00:02:00.000]   on things like the web and that we've collected and we have these scalable algorithms
[00:02:00.000 --> 00:02:05.000]   like transformers that are capable of ingesting all of that information.
[00:02:05.000 --> 00:02:10.000]   You can't start with a model as a kind of prior or to build on and to make predictions
[00:02:10.000 --> 00:02:15.000]   that helps bootstrap your learning.
[00:02:15.000 --> 00:02:20.000]   I just think it doesn't make sense not to make use of that.
[00:02:20.000 --> 00:02:25.000]   So my betting would be is that the final AGI system will have these large multimodals
[00:02:25.000 --> 00:02:30.000]   models as part of the overall solution, but probably won't be enough on their own.
[00:02:30.000 --> 00:02:35.000]   So that's my planning search on top.
[00:02:35.000 --> 00:02:40.000]   How do you get past the sort of immense amount of compute that these approaches tend to require?
[00:02:40.000 --> 00:02:45.000]   So even the off-the-go system was a pretty expensive system because you had to do this sort of
[00:02:45.000 --> 00:02:50.000]   running an LLM on each node of the tree.
[00:02:50.000 --> 00:02:55.000]   How do you anticipate that will get made more efficient?
[00:02:55.000 --> 00:03:00.000]   And also just looking at more efficient ways.
[00:03:00.000 --> 00:03:05.000]   I mean, the better your world model is, the more efficient your search can be.
[00:03:05.000 --> 00:03:10.000]   So one example I always give with AlphaZero, our system to play Go and chess and any game
[00:03:10.000 --> 00:03:15.000]   is that it's stronger than world champion level, human world champion level at all these games.
[00:03:15.000 --> 00:03:20.000]   And it uses a lot less search than a brute force method like Deep Blue, say, to play chess.
[00:03:20.000 --> 00:03:25.000]   Traditional Stockfish or Deep Blue systems would maybe look at millions of possible moves
[00:03:25.000 --> 00:03:27.000]   for every decision it's going to make.
[00:03:27.000 --> 00:03:34.000]   AlphaZero and AlphaGo looked at around tens of thousands of possible positions in order
[00:03:34.000 --> 00:03:36.000]   to make a decision about what to move next.
[00:03:36.000 --> 00:03:42.000]   But a human grandmaster, a human world champion, probably only looks at a few hundreds of moves,
[00:03:42.000 --> 00:03:47.000]   even the top ones, in order to make their very good decision about what to play next.
[00:03:47.000 --> 00:03:52.000]   So that suggests that obviously the brute force systems don't have any real model other
[00:03:52.000 --> 00:03:54.000]   than the heuristics about the game.
[00:03:54.000 --> 00:04:02.000]   AlphaZero has quite a decent model, but the top human players have a much richer,
[00:04:02.000 --> 00:04:05.000]   much more accurate model then of Go or chess.
[00:04:05.000 --> 00:04:10.000]   So that allows them to make world class decisions on a very small amount of search.
[00:04:10.000 --> 00:04:12.000]   So I think there's a sort of tradeoff there.
[00:04:12.000 --> 00:04:17.000]   If you improve the models, then I think your search can be more efficient,
[00:04:17.000 --> 00:04:19.000]   and therefore you can get further with your search.
[00:04:19.000 --> 00:04:21.000]   I have two questions based on that.
[00:04:21.000 --> 00:04:25.000]   The first being, with AlphaGo, you had a very concrete win condition of,
[00:04:25.000 --> 00:04:27.000]   at the end of the day, do I win this game of Go or not?
[00:04:27.000 --> 00:04:29.000]   And you can reinforce on that.
[00:04:29.000 --> 00:04:32.000]   When you're just thinking of an LLM putting out thought,
[00:04:32.000 --> 00:04:36.000]   do you think there will be this kind of ability to discriminate in the end,
[00:04:36.000 --> 00:04:38.000]   whether that was a good thing to reward or not?
[00:04:38.000 --> 00:04:42.000]   Well, of course, that's why we pioneered and DeepMind's sort of famous
[00:04:42.000 --> 00:04:46.000]   for using games as a proving ground, partly because, obviously,
[00:04:46.000 --> 00:04:48.000]   it's efficient to research in that domain.
[00:04:48.000 --> 00:04:51.000]   But the other reason is, obviously, it's extremely easy to specify
[00:04:51.000 --> 00:04:54.000]   a reward function, winning the game or improving the score,
[00:04:54.000 --> 00:04:56.000]   something like that sort of built into most games.
[00:04:56.000 --> 00:05:00.000]   So that is one of the challenges of real world systems,
[00:05:00.000 --> 00:05:03.000]   is how does one define the right objective function,
[00:05:03.000 --> 00:05:06.000]   the right reward function, and the right goals,
[00:05:06.000 --> 00:05:11.000]   and specify them in a general way, but that's specific enough
[00:05:11.000 --> 00:05:14.000]   and actually points the system in the right direction.

