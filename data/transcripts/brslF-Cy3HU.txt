
[00:00:00.000 --> 00:00:03.100]   "The following is a conversation with Manolis Kellis.
[00:00:03.100 --> 00:00:04.780]   "He's a professor at MIT
[00:00:04.780 --> 00:00:08.420]   "and head of the MIT Computational Biology Group.
[00:00:08.420 --> 00:00:11.280]   "He's interested in understanding the human genome
[00:00:11.280 --> 00:00:14.180]   "from a computational, evolutionary, biological,
[00:00:14.180 --> 00:00:17.020]   "and other cross-disciplinary perspectives.
[00:00:17.020 --> 00:00:20.160]   "He has more big, impactful papers and awards
[00:00:20.160 --> 00:00:22.480]   "than I can list, but most importantly,
[00:00:22.480 --> 00:00:26.240]   "he's a kind, curious, brilliant human being
[00:00:26.240 --> 00:00:28.700]   "and just someone I really enjoy talking to.
[00:00:28.700 --> 00:00:32.940]   "His passion for science and life in general is contagious.
[00:00:32.940 --> 00:00:34.620]   "The hours honestly flew by,
[00:00:34.620 --> 00:00:37.800]   "and I'm sure we'll talk again on this podcast soon."
[00:00:37.800 --> 00:00:39.180]   Quick summary of the ads.
[00:00:39.180 --> 00:00:43.240]   Three sponsors, Blinkist, 8sleep, and Masterclass.
[00:00:43.240 --> 00:00:45.040]   Please consider supporting this podcast
[00:00:45.040 --> 00:00:49.920]   by going to blinkist.com/lex, 8sleep.com/lex,
[00:00:49.920 --> 00:00:53.400]   and signing up at masterclass.com/lex.
[00:00:53.400 --> 00:00:56.200]   Click the links, buy the stuff, get the discount.
[00:00:56.200 --> 00:00:58.920]   It's the best way to support this podcast.
[00:00:58.920 --> 00:01:01.220]   If you enjoy this thing, subscribe on YouTube,
[00:01:01.220 --> 00:01:03.440]   review it with Five Stars and Apple Podcasts,
[00:01:03.440 --> 00:01:04.760]   support it on Patreon,
[00:01:04.760 --> 00:01:08.220]   or connect with me on Twitter @LexFriedman.
[00:01:08.220 --> 00:01:10.360]   As usual, I'll do a few minutes of ads now
[00:01:10.360 --> 00:01:11.880]   and never any ads in the middle
[00:01:11.880 --> 00:01:14.580]   that can break the flow of the conversation.
[00:01:14.580 --> 00:01:17.240]   This episode is supported by Blinkist,
[00:01:17.240 --> 00:01:19.840]   my favorite app for learning new things.
[00:01:19.840 --> 00:01:24.080]   Get it at blinkist.com/lex for a seven-day free trial
[00:01:24.080 --> 00:01:27.000]   and 25% off afterwards.
[00:01:27.000 --> 00:01:28.440]   Blinkist takes the key ideas
[00:01:28.440 --> 00:01:30.400]   from thousands of nonfiction books
[00:01:30.400 --> 00:01:33.240]   and condenses them down into just 15 minutes
[00:01:33.240 --> 00:01:35.440]   that you can read or listen to.
[00:01:35.440 --> 00:01:39.000]   I'm a big believer in reading at least an hour every day.
[00:01:39.000 --> 00:01:41.960]   As part of that, I use Blinkist every day
[00:01:41.960 --> 00:01:43.960]   to try out a book I may otherwise
[00:01:43.960 --> 00:01:45.600]   never have a chance to read.
[00:01:45.600 --> 00:01:48.400]   And in general, it's a great way to broaden your view
[00:01:48.400 --> 00:01:50.640]   of the idea landscape out there
[00:01:50.640 --> 00:01:54.000]   and find books that you may want to read more deeply.
[00:01:54.000 --> 00:01:56.200]   With Blinkist, you get unlimited access
[00:01:56.200 --> 00:01:58.680]   to read or listen to a massive library
[00:01:58.680 --> 00:02:00.720]   of condensed nonfiction books.
[00:02:00.720 --> 00:02:05.320]   Go to blinkist.com/lex to try it free for seven days
[00:02:05.320 --> 00:02:08.360]   and save 25% off your new subscription.
[00:02:08.360 --> 00:02:13.360]   That's blinkist.com/lex, Blinkist spelled B-L-I-N-K-I-S-T.
[00:02:13.360 --> 00:02:18.240]   This show is also sponsored by 8Sleep
[00:02:18.240 --> 00:02:19.840]   and its PodPro mattress.
[00:02:19.840 --> 00:02:22.480]   You can check out at 8Sleep.com/lex
[00:02:22.480 --> 00:02:24.640]   to get $200 off.
[00:02:24.640 --> 00:02:26.760]   It controls temperature with an app
[00:02:26.760 --> 00:02:29.240]   and can cool down to as low as 55 degrees
[00:02:29.240 --> 00:02:31.520]   on each side of the bed separately.
[00:02:31.520 --> 00:02:33.880]   Research shows that temperature has a big impact
[00:02:33.880 --> 00:02:35.720]   on the quality of our sleep.
[00:02:35.720 --> 00:02:37.880]   Anecdotally, it's been true for me,
[00:02:37.880 --> 00:02:39.880]   it's truly been a game changer.
[00:02:39.880 --> 00:02:40.920]   I love it.
[00:02:40.920 --> 00:02:43.440]   The PodPro is packed with sensors that track heart rate,
[00:02:43.440 --> 00:02:46.120]   heart rate variability, and respiratory rate,
[00:02:46.120 --> 00:02:48.160]   showing it all in their app.
[00:02:48.160 --> 00:02:50.120]   The app's health metrics are amazing,
[00:02:50.120 --> 00:02:52.920]   but the cooling alone is honestly worth the money.
[00:02:52.920 --> 00:02:57.500]   Check it out at 8Sleep.com/lex to get $200 off.
[00:02:57.500 --> 00:03:00.320]   This show is also sponsored by Masterclass.
[00:03:00.320 --> 00:03:03.560]   Sign up at masterclass.com/lex to get a discount
[00:03:03.560 --> 00:03:05.500]   and to support this podcast.
[00:03:05.500 --> 00:03:07.080]   When I first heard about Masterclass,
[00:03:07.080 --> 00:03:09.040]   I thought it was too good to be true.
[00:03:09.040 --> 00:03:12.120]   For 180 bucks a year, you get an all-access pass
[00:03:12.120 --> 00:03:15.480]   to watch courses from, to list some of my favorites,
[00:03:15.480 --> 00:03:17.720]   Chris Hadfield on space exploration,
[00:03:17.720 --> 00:03:19.600]   Neil deGrasse Tyson on scientific thinking
[00:03:19.600 --> 00:03:21.640]   and communication, Will Wright,
[00:03:21.640 --> 00:03:23.440]   one of my favorite game designers,
[00:03:23.440 --> 00:03:26.480]   Carlos Santana, one of my favorite guitar players,
[00:03:26.480 --> 00:03:27.960]   Garry Kasparov, of course,
[00:03:27.960 --> 00:03:30.800]   the greatest chess player of all time, I'm not biased,
[00:03:30.800 --> 00:03:33.760]   Daniel Negrano on poker, and many more.
[00:03:33.760 --> 00:03:35.880]   Chris Hadfield explaining how rockets work
[00:03:35.880 --> 00:03:38.680]   and the experience of being launched into space alone
[00:03:38.680 --> 00:03:40.040]   is worth the money.
[00:03:40.040 --> 00:03:43.120]   By the way, you can watch it on basically any device.
[00:03:43.120 --> 00:03:46.160]   Once again, sign up at masterclass.com/lex
[00:03:46.160 --> 00:03:48.920]   to get a discount and to support this podcast.
[00:03:49.640 --> 00:03:53.500]   And now, here's my conversation with Manolis Callas.
[00:03:53.500 --> 00:03:56.920]   What to you is the most beautiful aspect
[00:03:56.920 --> 00:03:58.640]   of the human genome?
[00:03:58.640 --> 00:04:00.080]   - Don't get me started.
[00:04:00.080 --> 00:04:02.040]   (both laughing)
[00:04:02.040 --> 00:04:03.280]   So-- - We got time.
[00:04:03.280 --> 00:04:06.400]   - The first answer is that the beauty of genomes
[00:04:06.400 --> 00:04:07.680]   transcends humanity.
[00:04:07.680 --> 00:04:09.480]   So it's not just about the human genome.
[00:04:09.480 --> 00:04:12.680]   Genomes in general are amazingly beautiful.
[00:04:12.680 --> 00:04:14.080]   And again, I'm obviously biased.
[00:04:14.080 --> 00:04:18.720]   So in my view, the way that I like to introduce
[00:04:18.720 --> 00:04:20.880]   the human genome and the way that I like to introduce
[00:04:20.880 --> 00:04:22.960]   genomics to my class is by telling them,
[00:04:22.960 --> 00:04:25.000]   you know, we're not the inventors
[00:04:25.000 --> 00:04:26.600]   of the first digital computer.
[00:04:26.600 --> 00:04:29.340]   We are the descendants of the first digital computer.
[00:04:29.340 --> 00:04:32.380]   Basically, life is digital.
[00:04:32.380 --> 00:04:34.640]   And that's absolutely beautiful about life.
[00:04:34.640 --> 00:04:37.120]   The fact that at every replication step,
[00:04:37.120 --> 00:04:38.520]   you don't lose any information
[00:04:38.520 --> 00:04:40.080]   because that information is digital.
[00:04:40.080 --> 00:04:43.200]   If it was analog, if it was just brought in concentrations,
[00:04:43.200 --> 00:04:44.680]   you'd lose it after a few generations.
[00:04:44.680 --> 00:04:46.400]   It would just dissolve away.
[00:04:46.400 --> 00:04:48.240]   And that's what the ancients
[00:04:48.240 --> 00:04:50.120]   didn't understand about inheritance.
[00:04:50.120 --> 00:04:52.160]   The first person to understand digital inheritance
[00:04:52.160 --> 00:04:54.440]   was Mendel, of course.
[00:04:54.440 --> 00:04:57.600]   And his theory, in fact, stayed in a bookshelf
[00:04:57.600 --> 00:05:00.760]   for like 50 years while Darwin was getting famous
[00:05:00.760 --> 00:05:02.480]   about natural selection.
[00:05:02.480 --> 00:05:05.480]   But the missing component was this digital inheritance,
[00:05:05.480 --> 00:05:09.760]   the mechanism of evolution that Mendel had discovered.
[00:05:09.760 --> 00:05:13.140]   So that aspect, in my view, is the most beautiful aspect,
[00:05:13.140 --> 00:05:14.920]   but it transcends all of life.
[00:05:14.920 --> 00:05:18.080]   - And can you elaborate maybe the inheritance part?
[00:05:18.080 --> 00:05:22.520]   What was the key thing that the ancients didn't understand?
[00:05:22.520 --> 00:05:27.520]   - So the very theory of inheritance as discrete units.
[00:05:27.520 --> 00:05:32.920]   Throughout the life of Mendel and well after his writing,
[00:05:32.920 --> 00:05:35.280]   people thought that his p experiments
[00:05:35.280 --> 00:05:36.640]   were just a little fluke,
[00:05:36.640 --> 00:05:38.800]   that they were just a little exception
[00:05:38.800 --> 00:05:41.880]   that would normally not even apply to humans.
[00:05:41.880 --> 00:05:46.880]   That basically what they saw is this continuum of eye color,
[00:05:46.880 --> 00:05:49.800]   this continuum of skin color,
[00:05:49.800 --> 00:05:52.560]   this continuum of hair color, this continuum of height.
[00:05:52.560 --> 00:05:55.120]   And all of these continuums did not fit
[00:05:55.120 --> 00:05:56.840]   with a discrete type of inheritance
[00:05:56.840 --> 00:05:58.960]   that Mendel was describing.
[00:05:58.960 --> 00:06:00.520]   But what's unique about genomics
[00:06:00.520 --> 00:06:01.680]   and what's unique about the genome
[00:06:01.680 --> 00:06:03.880]   is really that there are two copies
[00:06:03.880 --> 00:06:06.280]   and that you get a combination of these,
[00:06:06.280 --> 00:06:08.240]   but for every trait,
[00:06:08.240 --> 00:06:10.760]   there are dozens of contributing variables.
[00:06:10.760 --> 00:06:14.080]   And it was only Ronald Fisher in the 20th century
[00:06:14.080 --> 00:06:19.080]   that basically recognized that even five Mendelian traits
[00:06:19.080 --> 00:06:25.000]   would add up to a continuum-like inheritance pattern.
[00:06:25.000 --> 00:06:27.560]   And he wrote a series of papers
[00:06:27.560 --> 00:06:30.400]   that still are very relevant today
[00:06:30.400 --> 00:06:33.000]   about sort of this Mendelian inheritance
[00:06:33.000 --> 00:06:35.240]   of continuum-like traits.
[00:06:35.240 --> 00:06:38.600]   And I think that was the missing step in inheritance.
[00:06:38.600 --> 00:06:41.600]   So well before the discovery of the structure of DNA,
[00:06:41.600 --> 00:06:44.480]   which is again another amazingly beautiful aspect,
[00:06:44.480 --> 00:06:46.280]   the double helix, what I like to call
[00:06:46.280 --> 00:06:48.560]   the most noble molecule of our time,
[00:06:48.560 --> 00:06:54.160]   holds within it the secret of that discrete inheritance.
[00:06:54.160 --> 00:06:58.560]   But the conceptualization of discrete elements
[00:06:58.560 --> 00:06:59.760]   is something that precedes that.
[00:06:59.760 --> 00:07:01.480]   - So even though it's discrete,
[00:07:01.480 --> 00:07:06.480]   when it materializes itself into actual traits that we see,
[00:07:06.480 --> 00:07:07.520]   it can be continuous,
[00:07:07.520 --> 00:07:10.920]   it can basically arbitrarily rich and complex.
[00:07:10.920 --> 00:07:15.080]   - So if you have five genes that contribute to human height,
[00:07:15.080 --> 00:07:17.040]   and there aren't five, there's a thousand.
[00:07:17.040 --> 00:07:18.520]   If there's only five genes
[00:07:18.520 --> 00:07:20.960]   and you inherit some combination of them
[00:07:20.960 --> 00:07:23.160]   and everyone makes you two inches taller
[00:07:23.160 --> 00:07:24.600]   or two inches shorter,
[00:07:24.600 --> 00:07:27.520]   it'll look like a continuum trait, a continuous trait.
[00:07:27.520 --> 00:07:30.520]   But instead of five, there are thousands
[00:07:30.520 --> 00:07:33.960]   and every one of them contributes to less than one millimeter.
[00:07:33.960 --> 00:07:37.000]   We change in height more during the day
[00:07:37.000 --> 00:07:39.400]   than each of these genetic variants contributes.
[00:07:39.400 --> 00:07:43.200]   So by the evening, you're shorter than you were,
[00:07:43.200 --> 00:07:44.040]   you woke up with.
[00:07:44.040 --> 00:07:45.360]   - Isn't that weird then
[00:07:45.360 --> 00:07:48.120]   that we're not more different than we are?
[00:07:48.120 --> 00:07:49.720]   Why are we all so similar
[00:07:49.720 --> 00:07:52.640]   if there's so much possibility to be different?
[00:07:52.640 --> 00:07:57.520]   - Yeah, so there are selective advantages to being medium.
[00:07:57.520 --> 00:07:59.960]   If you're extremely tall or extremely short,
[00:07:59.960 --> 00:08:02.320]   you run into selective disadvantages.
[00:08:02.320 --> 00:08:04.160]   So you have trouble breathing, you have trouble running,
[00:08:04.160 --> 00:08:06.360]   you have trouble sitting if you're too tall.
[00:08:06.360 --> 00:08:08.280]   If you're too short, you might, I don't know,
[00:08:08.280 --> 00:08:11.040]   have other selective pressures acting against that.
[00:08:11.040 --> 00:08:13.640]   If you look at natural history of human population,
[00:08:13.640 --> 00:08:17.040]   there's actually selection for height in Northern Europe
[00:08:17.040 --> 00:08:19.960]   and selection against height in Southern Europe.
[00:08:19.960 --> 00:08:21.840]   So there might actually be advantages
[00:08:21.840 --> 00:08:25.040]   to actually being not super tall.
[00:08:25.040 --> 00:08:27.800]   And if you look across the entire human population,
[00:08:27.800 --> 00:08:29.880]   for many, many traits,
[00:08:29.880 --> 00:08:32.120]   there's a lot of push towards the middle.
[00:08:32.120 --> 00:08:35.320]   Balancing selection is the usual term
[00:08:35.320 --> 00:08:39.720]   for selection that sort of seeks to not be extreme
[00:08:39.720 --> 00:08:43.640]   and to sort of have a combination of alleles
[00:08:43.640 --> 00:08:46.080]   that sort of keep recombining.
[00:08:46.080 --> 00:08:48.680]   And if you look at mate selection,
[00:08:48.680 --> 00:08:51.320]   super, super tall people will not tend
[00:08:51.320 --> 00:08:53.400]   to sort of marry super, super tall people.
[00:08:53.400 --> 00:08:55.240]   Very often you see these couples
[00:08:55.240 --> 00:08:58.040]   that are kind of compensating for each other.
[00:08:58.040 --> 00:09:00.480]   And the best predictor of the kid's age
[00:09:00.480 --> 00:09:03.560]   is very often just take the average of the two parents
[00:09:03.560 --> 00:09:07.160]   and then adjust for sex and boom, you get it.
[00:09:07.160 --> 00:09:08.560]   It's extremely heritable.
[00:09:08.560 --> 00:09:11.520]   - Let me ask, you kind of took a step back
[00:09:11.520 --> 00:09:13.960]   to the genome outside of just humans,
[00:09:13.960 --> 00:09:15.960]   but is there something that you find beautiful
[00:09:15.960 --> 00:09:18.680]   about the human genome specifically?
[00:09:18.680 --> 00:09:21.440]   - So I think the genome,
[00:09:21.440 --> 00:09:24.520]   if more people understood the beauty of the human genome,
[00:09:24.520 --> 00:09:26.680]   there would be so many fewer wars,
[00:09:26.680 --> 00:09:29.040]   so much less anger in the world.
[00:09:29.040 --> 00:09:31.480]   I mean, what's really beautiful about the human genome
[00:09:31.480 --> 00:09:34.760]   is really the variation that teaches us
[00:09:34.760 --> 00:09:38.280]   both about individuality and about similarity.
[00:09:38.280 --> 00:09:42.440]   So any two people on the planet are 99.9% identical.
[00:09:42.440 --> 00:09:45.640]   How can you fight with someone
[00:09:45.640 --> 00:09:47.320]   who's 99.9% identical to you?
[00:09:47.320 --> 00:09:49.760]   It's just counterintuitive.
[00:09:49.760 --> 00:09:53.960]   And yet any two siblings of the same parent
[00:09:53.960 --> 00:09:57.160]   differ in millions of locations.
[00:09:57.160 --> 00:10:01.080]   So every one of them is basically two to the million unique
[00:10:01.080 --> 00:10:03.200]   from any pair of parents,
[00:10:03.200 --> 00:10:05.840]   let alone any two random parents on the planet.
[00:10:05.840 --> 00:10:08.600]   So that's, I think, something that teaches us
[00:10:08.600 --> 00:10:11.280]   about sort of the nature of humanity in many ways,
[00:10:11.280 --> 00:10:14.720]   that every one of us is as unique as any star
[00:10:14.720 --> 00:10:17.240]   and way more unique in actually many ways.
[00:10:17.240 --> 00:10:22.240]   And yet we're all brothers and sisters.
[00:10:22.240 --> 00:10:23.360]   - Yeah, just like stars,
[00:10:23.360 --> 00:10:26.200]   most of it is just fusion reactions.
[00:10:26.200 --> 00:10:28.640]   - Yeah, you only have a few parameters to describe stars.
[00:10:28.640 --> 00:10:29.480]   - Yeah, exactly.
[00:10:29.480 --> 00:10:33.120]   So mass size, initial size, and stage of life.
[00:10:33.120 --> 00:10:36.360]   Whereas for humans, it's thousands of parameters
[00:10:36.360 --> 00:10:38.080]   scattered across our genome.
[00:10:38.080 --> 00:10:41.360]   So the other thing that makes humans unique,
[00:10:41.360 --> 00:10:45.240]   the other things that makes inheritance unique in humans
[00:10:45.240 --> 00:10:50.240]   is that most species inherit things vertically.
[00:10:50.240 --> 00:10:54.480]   Basically, instinct is a huge part of their behavior.
[00:10:54.480 --> 00:10:57.560]   The way that, I mean, with my kids,
[00:10:57.560 --> 00:11:01.000]   we've been watching this nest of birds
[00:11:01.000 --> 00:11:03.600]   with two little eggs outside our window
[00:11:03.600 --> 00:11:05.240]   for the last few months,
[00:11:05.240 --> 00:11:07.480]   for the last few weeks as they've been growing.
[00:11:07.480 --> 00:11:12.280]   And there's so much behavior that's hard-coded.
[00:11:12.280 --> 00:11:15.280]   Birds don't just learn as they grow.
[00:11:15.280 --> 00:11:17.160]   They don't, there's no culture.
[00:11:17.160 --> 00:11:20.200]   Like a bird that's born in Boston will be the same
[00:11:20.200 --> 00:11:22.040]   as a bird that's born in California.
[00:11:22.040 --> 00:11:27.040]   So there's not as much inheritance of ideas, of customs.
[00:11:27.960 --> 00:11:30.560]   A lot of it is hard-coded in their genome.
[00:11:30.560 --> 00:11:32.320]   What's really beautiful about the human genome
[00:11:32.320 --> 00:11:35.040]   is that if you take a person from today
[00:11:35.040 --> 00:11:37.080]   and you place them back in ancient Egypt,
[00:11:37.080 --> 00:11:39.160]   or if you take a person from ancient Egypt
[00:11:39.160 --> 00:11:41.200]   and you place them here today,
[00:11:41.200 --> 00:11:43.300]   they will grow up to be completely normal.
[00:11:43.300 --> 00:11:47.720]   That is not genetics.
[00:11:47.720 --> 00:11:51.880]   This is the other type of inheritance in humans.
[00:11:51.880 --> 00:11:53.920]   So on one hand, we have genetic inheritance,
[00:11:53.920 --> 00:11:56.200]   which is vertical from your parents down.
[00:11:56.200 --> 00:11:58.440]   On the other hand, we have horizontal inheritance,
[00:11:58.440 --> 00:12:02.440]   which is the ideas that are built up at every generation
[00:12:02.440 --> 00:12:04.600]   are horizontally transmitted.
[00:12:04.600 --> 00:12:07.440]   And the huge amount of time that we spend
[00:12:07.440 --> 00:12:11.960]   in educating ourselves, a concept known as neoteny,
[00:12:11.960 --> 00:12:15.220]   neo for newborn and then teny for holding.
[00:12:15.220 --> 00:12:18.360]   So if you look at humans, I mean, the little birds,
[00:12:18.360 --> 00:12:20.400]   they were eggs two weeks ago,
[00:12:20.400 --> 00:12:22.720]   and now one of them has already flown off.
[00:12:22.720 --> 00:12:24.560]   The other one's ready to fly off.
[00:12:24.560 --> 00:12:27.320]   In two weeks, they're ready to just fend for themselves.
[00:12:27.320 --> 00:12:30.040]   Humans, 16 years.
[00:12:30.040 --> 00:12:31.280]   (both laughing)
[00:12:31.280 --> 00:12:33.040]   18 years, 24, getting out of college.
[00:12:33.040 --> 00:12:34.440]   - I'm still learning.
[00:12:34.440 --> 00:12:35.800]   So that's so fascinating,
[00:12:35.800 --> 00:12:38.720]   this picture of a vertical and a horizontal.
[00:12:38.720 --> 00:12:40.120]   When you talk about the horizontal,
[00:12:40.120 --> 00:12:42.000]   is it in the realm of ideas?
[00:12:42.000 --> 00:12:42.840]   - Exactly.
[00:12:42.840 --> 00:12:45.360]   - Okay, so it's the actual social interactions.
[00:12:45.360 --> 00:12:47.120]   - That's exactly right, that's exactly right.
[00:12:47.120 --> 00:12:49.240]   So basically, the concept of neoteny
[00:12:49.240 --> 00:12:52.800]   is that you spend acquiring characteristics
[00:12:52.800 --> 00:12:56.120]   from your environment in an extremely malleable state
[00:12:56.120 --> 00:12:58.520]   of your brain and the wiring of your brain
[00:12:58.520 --> 00:13:00.680]   for a long period of your life.
[00:13:00.680 --> 00:13:03.520]   Compared to primates, we are useless.
[00:13:03.520 --> 00:13:05.360]   You take any primate at seven weeks
[00:13:05.360 --> 00:13:08.480]   and any human at seven weeks, we lose the battle.
[00:13:08.480 --> 00:13:11.680]   But at 18 years, you know, all bets are off.
[00:13:11.680 --> 00:13:14.880]   Like, basically, our brain continues to develop
[00:13:14.880 --> 00:13:17.800]   in an extremely malleable form till very late.
[00:13:17.800 --> 00:13:20.400]   And this is what allows education.
[00:13:20.400 --> 00:13:22.520]   This is what allows the person from Egypt
[00:13:22.520 --> 00:13:24.720]   to do extremely well now.
[00:13:24.720 --> 00:13:29.720]   And the reason for that is that the wiring of our brain
[00:13:29.720 --> 00:13:35.000]   and the development of that wiring is actually delayed.
[00:13:35.000 --> 00:13:37.520]   So, you know, the longer you delay that,
[00:13:37.520 --> 00:13:40.880]   the more opportunity you have to pass on knowledge,
[00:13:40.880 --> 00:13:44.280]   to pass on concepts, ideals, ideas
[00:13:44.280 --> 00:13:46.080]   from the parents to the child.
[00:13:46.080 --> 00:13:49.160]   And what's really absolutely beautiful about humans today
[00:13:49.160 --> 00:13:52.160]   is that that lateral transfer of ideas and culture
[00:13:52.160 --> 00:13:55.760]   is not just from uncles and aunts and teachers at school,
[00:13:55.760 --> 00:14:00.360]   but it's from Wikipedia and review articles on the web
[00:14:00.360 --> 00:14:02.680]   and thousands of journals
[00:14:02.680 --> 00:14:05.560]   that are sort of putting out information for free
[00:14:05.560 --> 00:14:08.920]   and podcasts and video casts and all of that stuff
[00:14:08.920 --> 00:14:12.920]   where you can basically learn about any topic,
[00:14:12.920 --> 00:14:15.920]   pretty much everything that would be
[00:14:15.920 --> 00:14:19.560]   in any super advanced textbook in a matter of days,
[00:14:19.560 --> 00:14:22.960]   instead of having to go to the library of Alexandria
[00:14:22.960 --> 00:14:24.720]   and sail there to read three books
[00:14:24.720 --> 00:14:26.160]   and then sail for another few days
[00:14:26.160 --> 00:14:28.660]   to get to Athens and et cetera, et cetera, et cetera.
[00:14:28.660 --> 00:14:31.400]   So the democratization of knowledge
[00:14:31.400 --> 00:14:34.280]   and the spread, the speed of spread of knowledge
[00:14:34.280 --> 00:14:38.920]   is what defines, I think, the human inheritance pattern.
[00:14:38.920 --> 00:14:41.560]   - So you sound excited about it.
[00:14:41.560 --> 00:14:43.760]   Are you also a little bit afraid
[00:14:43.760 --> 00:14:46.480]   or are you more excited by the power
[00:14:46.480 --> 00:14:49.920]   of this kind of distributed spread of information?
[00:14:49.920 --> 00:14:51.320]   So you put it very kindly
[00:14:51.320 --> 00:14:53.560]   that most people are kind of using the internet
[00:14:53.560 --> 00:14:56.800]   and looking Wikipedia, reading articles,
[00:14:56.800 --> 00:14:58.120]   reading papers and so on,
[00:14:58.120 --> 00:15:02.040]   but if we're honest, most people online,
[00:15:02.040 --> 00:15:03.240]   especially when they're younger,
[00:15:03.240 --> 00:15:05.840]   probably looking at five second clips on TikTok
[00:15:05.840 --> 00:15:08.520]   or whatever the new social network is,
[00:15:08.520 --> 00:15:12.520]   are you, given this power of horizontal inheritance,
[00:15:12.520 --> 00:15:16.520]   are you optimistic or a little bit pessimistic
[00:15:16.520 --> 00:15:21.520]   about this new effect of the internet
[00:15:21.520 --> 00:15:25.560]   and democratization of knowledge on our,
[00:15:25.560 --> 00:15:28.120]   what would you call this?
[00:15:28.120 --> 00:15:30.760]   This genome, like would you use the term genome,
[00:15:30.760 --> 00:15:31.600]   by the way, for this?
[00:15:31.600 --> 00:15:32.420]   - Yeah, yeah.
[00:15:32.420 --> 00:15:36.200]   I think, you know, we use the genome to talk about DNA,
[00:15:36.200 --> 00:15:39.000]   but very often we say, you know, I mean, I'm Greek,
[00:15:39.000 --> 00:15:40.800]   so people ask me, "Hey, what's in the Greek genome?"
[00:15:40.800 --> 00:15:42.760]   And I'm like, "Well, yeah, what's in the Greek genome
[00:15:42.760 --> 00:15:44.720]   "is both our genes and also our ideas
[00:15:44.720 --> 00:15:46.680]   "and our ideals and our culture."
[00:15:46.680 --> 00:15:48.240]   - The poetic meaning of the word.
[00:15:48.240 --> 00:15:50.120]   - Exactly, exactly, yeah.
[00:15:50.120 --> 00:15:55.120]   So I think that there's a beauty
[00:15:55.120 --> 00:15:57.760]   to the democratization of knowledge,
[00:15:57.760 --> 00:16:00.240]   the fact that you can reach as many people
[00:16:00.240 --> 00:16:02.840]   as any other person on the planet
[00:16:02.840 --> 00:16:04.280]   and it's not who you are,
[00:16:04.280 --> 00:16:06.680]   it's really your ideas that matter,
[00:16:06.680 --> 00:16:09.700]   is a beautiful aspect of the internet.
[00:16:09.700 --> 00:16:14.200]   The, I think there's of course a danger
[00:16:14.200 --> 00:16:18.260]   of my ignorance is as important as your expertise.
[00:16:18.260 --> 00:16:21.360]   The fact that with this democratization
[00:16:21.360 --> 00:16:25.160]   comes the abolishment of respecting expertise.
[00:16:25.160 --> 00:16:27.120]   Just because you've spent, you know,
[00:16:27.120 --> 00:16:29.920]   10,000 hours of your life studying,
[00:16:29.920 --> 00:16:33.320]   I don't know, human brain circuitry,
[00:16:33.320 --> 00:16:34.160]   why should I trust you?
[00:16:34.160 --> 00:16:35.640]   I'm just gonna make up my own theories
[00:16:35.640 --> 00:16:37.240]   and they'll be just as good as yours,
[00:16:37.240 --> 00:16:39.660]   is an attitude that sort of counteracts
[00:16:39.660 --> 00:16:42.480]   the beauty of the democratization.
[00:16:42.480 --> 00:16:47.400]   And I think that within our educational system
[00:16:47.400 --> 00:16:49.720]   and within the upbringing of our children,
[00:16:49.720 --> 00:16:52.340]   we have to not only teach them knowledge,
[00:16:52.340 --> 00:16:55.780]   but we have to teach them the means to get to knowledge.
[00:16:55.780 --> 00:16:58.000]   And that, you know, it's very similar to sort of,
[00:16:58.000 --> 00:17:01.400]   you fish, you catch a fish for a man for one day,
[00:17:01.400 --> 00:17:03.880]   you fed them for one day, you teach them how to fish,
[00:17:03.880 --> 00:17:05.580]   you fed them for the rest of their life.
[00:17:05.580 --> 00:17:08.240]   So instead of just gathering the knowledge they need
[00:17:08.240 --> 00:17:10.160]   for any one task, we can just tell them,
[00:17:10.160 --> 00:17:12.520]   all right, here's how you Google it.
[00:17:12.520 --> 00:17:14.640]   Here's how to figure out what's real and what's not.
[00:17:14.640 --> 00:17:16.440]   Here's how you check the sources.
[00:17:16.440 --> 00:17:19.320]   Here's how you form a basic opinion for yourself.
[00:17:19.320 --> 00:17:24.320]   And I think that inquisitive nature is paramount
[00:17:24.320 --> 00:17:29.320]   to being able to sort through this huge wealth of knowledge.
[00:17:29.320 --> 00:17:32.560]   So you need a basic educational foundation
[00:17:32.560 --> 00:17:35.520]   based on which you can then add on
[00:17:35.520 --> 00:17:38.240]   the sort of domain specific knowledge,
[00:17:38.240 --> 00:17:39.720]   but that basic educational foundation
[00:17:39.720 --> 00:17:42.400]   should not just be knowledge,
[00:17:42.400 --> 00:17:45.240]   but it should also be epistemology,
[00:17:45.240 --> 00:17:47.240]   the way to acquire knowledge.
[00:17:47.240 --> 00:17:49.760]   - I'm not sure any of us know how to do that
[00:17:49.760 --> 00:17:50.600]   in this modern day.
[00:17:50.600 --> 00:17:51.680]   We're actually learning.
[00:17:51.680 --> 00:17:53.580]   One of the big surprising thing to me
[00:17:53.580 --> 00:17:57.280]   about the coronavirus, for example,
[00:17:57.280 --> 00:18:01.080]   is that Twitter has been one of the best sources
[00:18:01.080 --> 00:18:04.960]   of information, basically like building your own network
[00:18:04.960 --> 00:18:09.800]   of experts, as opposed to the traditional
[00:18:09.800 --> 00:18:12.840]   centralized expertise of the WHO and the CDC
[00:18:12.840 --> 00:18:17.840]   and the, or maybe any one particular respectable person
[00:18:17.840 --> 00:18:21.800]   at the top of a department, some kind of institution.
[00:18:21.800 --> 00:18:26.520]   You instead look at 10, 20, hundreds of people,
[00:18:26.520 --> 00:18:30.280]   some of whom are young kids with just,
[00:18:30.280 --> 00:18:33.760]   that are incredibly good at aggregating data
[00:18:33.760 --> 00:18:35.800]   and plotting and visualizing that data.
[00:18:35.800 --> 00:18:37.240]   That's been really surprising to me.
[00:18:37.240 --> 00:18:38.840]   I don't know what to make of it.
[00:18:38.840 --> 00:18:44.320]   I don't know how that matures into something stable.
[00:18:44.320 --> 00:18:46.600]   I don't know if you have ideas.
[00:18:46.600 --> 00:18:49.960]   Like what, if you were to try to explain to your kids
[00:18:49.960 --> 00:18:54.960]   of how, where should you go to learn about coronavirus?
[00:18:54.960 --> 00:18:56.800]   What would you say?
[00:18:56.800 --> 00:18:58.080]   - It's such a beautiful example.
[00:18:58.080 --> 00:18:59.920]   And I think the current pandemic
[00:18:59.920 --> 00:19:03.280]   and the speed at which the scientific community has moved
[00:19:04.000 --> 00:19:04.840]   in the current pandemic,
[00:19:04.840 --> 00:19:08.080]   I think exemplifies this horizontal transfer
[00:19:08.080 --> 00:19:10.840]   and the speed of horizontal transfer of information.
[00:19:10.840 --> 00:19:15.360]   The fact that, you know, the genome was first sequenced
[00:19:15.360 --> 00:19:16.400]   in early January.
[00:19:16.400 --> 00:19:20.400]   The first sample was obtained December 29, 2019,
[00:19:20.400 --> 00:19:23.600]   a week after the publication of the first genome sequence.
[00:19:23.600 --> 00:19:26.760]   Moderna had already finalized its vaccine design
[00:19:26.760 --> 00:19:29.520]   and was moving to production.
[00:19:29.520 --> 00:19:31.840]   I mean, this is phenomenal.
[00:19:31.840 --> 00:19:34.960]   The fact that we go from not knowing
[00:19:34.960 --> 00:19:38.920]   what the heck is killing people in Wuhan to, wow,
[00:19:38.920 --> 00:19:41.880]   it's SARS-CoV-2 and here's the set of genes,
[00:19:41.880 --> 00:19:43.560]   here's the genome, here's the sequence,
[00:19:43.560 --> 00:19:45.640]   here are the polymorphisms, et cetera,
[00:19:45.640 --> 00:19:48.200]   in the matter of weeks is phenomenal.
[00:19:48.200 --> 00:19:52.720]   In that incredible pace of transfer of knowledge,
[00:19:52.720 --> 00:19:54.400]   there have been many mistakes.
[00:19:54.400 --> 00:19:56.640]   So, you know, some of those mistakes
[00:19:56.640 --> 00:19:57.960]   may have been politically motivated,
[00:19:57.960 --> 00:20:00.880]   or other mistakes may have just been innocuous errors.
[00:20:00.880 --> 00:20:02.880]   Others may have been misleading the public
[00:20:02.880 --> 00:20:05.560]   for the greater good, such as don't wear masks
[00:20:05.560 --> 00:20:07.240]   because we don't want the mask to run out.
[00:20:07.240 --> 00:20:09.080]   I mean, that was very silly in my view
[00:20:09.080 --> 00:20:10.480]   and a very big mistake.
[00:20:10.480 --> 00:20:15.120]   But the spread of knowledge
[00:20:15.120 --> 00:20:17.320]   from the scientific community was phenomenal.
[00:20:17.320 --> 00:20:20.720]   And some people will point out to bogus articles
[00:20:20.720 --> 00:20:22.920]   that snuck in and made the front page.
[00:20:22.920 --> 00:20:26.280]   Yeah, they did, but within 24 hours, they were debunked
[00:20:26.280 --> 00:20:27.520]   and went out of the front page.
[00:20:27.520 --> 00:20:30.200]   And I think that's the beauty of science today.
[00:20:30.200 --> 00:20:33.240]   The fact that it's not, oh, knowledge is fixed.
[00:20:33.240 --> 00:20:37.000]   It's the ability to embrace that nothing is permanent
[00:20:37.000 --> 00:20:37.920]   when it comes to knowledge,
[00:20:37.920 --> 00:20:40.040]   that everything is the current best hypothesis
[00:20:40.040 --> 00:20:42.960]   and the current best model that best fits the current data
[00:20:42.960 --> 00:20:45.840]   and the willingness to be wrong.
[00:20:45.840 --> 00:20:48.320]   The expectation that we're gonna be wrong
[00:20:48.320 --> 00:20:50.720]   and the celebration of success based on
[00:20:50.720 --> 00:20:52.760]   how long was I not proven wrong for,
[00:20:52.760 --> 00:20:55.640]   rather than, wow, I was exactly right.
[00:20:55.640 --> 00:20:57.080]   'Cause no one is gonna be exactly right
[00:20:57.080 --> 00:20:59.000]   with partial knowledge.
[00:20:59.000 --> 00:21:03.160]   But the arc towards perfection,
[00:21:03.160 --> 00:21:05.320]   I think is so much more important
[00:21:05.320 --> 00:21:08.720]   than how far you are in your first step.
[00:21:08.720 --> 00:21:10.440]   And I think that's what sort of
[00:21:10.440 --> 00:21:13.440]   the current pandemic has taught us.
[00:21:13.440 --> 00:21:14.800]   The fact that, yeah, no, of course,
[00:21:14.800 --> 00:21:16.200]   we're gonna make mistakes,
[00:21:16.200 --> 00:21:18.360]   but at least we're gonna learn from those mistakes
[00:21:18.360 --> 00:21:20.360]   and become better and learn better
[00:21:20.360 --> 00:21:21.360]   and spread information better.
[00:21:21.360 --> 00:21:23.360]   So if I were to answer the question of
[00:21:23.360 --> 00:21:26.480]   where would you go to learn about coronavirus?
[00:21:27.760 --> 00:21:28.880]   First, textbook.
[00:21:28.880 --> 00:21:30.000]   It all starts with a textbook.
[00:21:30.000 --> 00:21:32.640]   Just open up a chapter on virology
[00:21:32.640 --> 00:21:34.440]   and how coronaviruses work.
[00:21:34.440 --> 00:21:37.000]   Then some basic epidemiology
[00:21:37.000 --> 00:21:39.920]   and sort of how pandemics have worked in the past.
[00:21:39.920 --> 00:21:41.080]   What are the basic principles
[00:21:41.080 --> 00:21:43.520]   surrounding these first wave, second wave?
[00:21:43.520 --> 00:21:45.480]   Why do they even exist?
[00:21:45.480 --> 00:21:47.280]   Then understanding about growth,
[00:21:47.280 --> 00:21:50.280]   understanding about the R naught and R T
[00:21:50.280 --> 00:21:52.440]   at various time points.
[00:21:52.440 --> 00:21:55.320]   And then understanding the means of spread,
[00:21:55.320 --> 00:21:57.360]   how it spreads from person to person.
[00:21:57.360 --> 00:22:00.120]   Then how does it get into your cells
[00:22:00.120 --> 00:22:01.640]   from when it gets into the cells?
[00:22:01.640 --> 00:22:03.280]   What are the paths that it takes?
[00:22:03.280 --> 00:22:05.240]   What are the cell types that express
[00:22:05.240 --> 00:22:07.360]   the particular ACE2 receptor?
[00:22:07.360 --> 00:22:09.960]   How is your immune system interacting with the virus?
[00:22:09.960 --> 00:22:12.320]   And once your immune system launches its defense,
[00:22:12.320 --> 00:22:15.560]   how is that helping or actually hurting your health?
[00:22:15.560 --> 00:22:16.960]   What about the cytokine storm?
[00:22:16.960 --> 00:22:18.600]   What are most people dying from?
[00:22:18.600 --> 00:22:22.760]   Why are the comorbidities and these risk factors
[00:22:22.760 --> 00:22:23.920]   even applying?
[00:22:23.920 --> 00:22:26.000]   What makes obese people respond more
[00:22:26.000 --> 00:22:28.600]   or elderly people respond more to the virus
[00:22:28.600 --> 00:22:30.640]   while kids are completely,
[00:22:30.640 --> 00:22:36.400]   very often not even aware that they're spreading it?
[00:22:36.400 --> 00:22:41.200]   So I think there's some basic questions
[00:22:41.200 --> 00:22:42.760]   that you would start from.
[00:22:42.760 --> 00:22:44.400]   And then I'm sorry to say,
[00:22:44.400 --> 00:22:45.960]   but Wikipedia is pretty awesome.
[00:22:45.960 --> 00:22:47.040]   Google is pretty awesome.
[00:22:47.040 --> 00:22:47.880]   (laughs)
[00:22:47.880 --> 00:22:48.720]   - There used to be a time,
[00:22:48.720 --> 00:22:50.560]   there used to be a time maybe five years ago,
[00:22:50.560 --> 00:22:52.040]   I forget when,
[00:22:52.040 --> 00:22:54.280]   but people kind of made fun of Wikipedia
[00:22:54.280 --> 00:22:57.280]   for being an unreliable source.
[00:22:57.280 --> 00:22:58.560]   I never quite understood it.
[00:22:58.560 --> 00:23:01.280]   I thought from the early days, it was pretty reliable.
[00:23:01.280 --> 00:23:03.680]   They're better than a lot of the alternatives.
[00:23:03.680 --> 00:23:04.800]   But at this point,
[00:23:04.800 --> 00:23:08.360]   it's kind of like a solid accessible survey paper
[00:23:08.360 --> 00:23:10.080]   on every subject ever.
[00:23:10.080 --> 00:23:14.680]   - There's an ascertainment bias and a writing bias.
[00:23:14.680 --> 00:23:17.840]   So I think this is related to sort of people saying,
[00:23:17.840 --> 00:23:19.880]   oh, so many nature papers are wrong.
[00:23:19.880 --> 00:23:22.600]   And they're like, why would you publish in nature?
[00:23:22.600 --> 00:23:23.720]   So many nature papers are wrong.
[00:23:23.720 --> 00:23:26.160]   And my answer is no, no, no.
[00:23:26.160 --> 00:23:29.480]   So many nature papers are scrutinized.
[00:23:29.480 --> 00:23:32.000]   And just because more of them are being proven wrong
[00:23:32.000 --> 00:23:35.440]   than in other articles is actually evidence
[00:23:35.440 --> 00:23:37.080]   that they're actually better papers overall
[00:23:37.080 --> 00:23:39.200]   because they're being scrutinized at a rate
[00:23:39.200 --> 00:23:41.080]   much higher than any other journal.
[00:23:41.080 --> 00:23:45.600]   So if you basically judge Wikipedia
[00:23:45.600 --> 00:23:49.880]   by not the initial content,
[00:23:49.880 --> 00:23:52.480]   but by the number of revisions,
[00:23:52.480 --> 00:23:53.960]   then of course it's gonna be the best source
[00:23:53.960 --> 00:23:55.360]   of knowledge eventually.
[00:23:55.360 --> 00:23:57.160]   It's still very superficial.
[00:23:57.160 --> 00:23:58.800]   You then have to go into the review papers,
[00:23:58.800 --> 00:24:00.200]   et cetera, et cetera, et cetera.
[00:24:00.200 --> 00:24:03.400]   But I mean, for most scientific topics,
[00:24:03.400 --> 00:24:05.080]   it's extremely superficial.
[00:24:05.080 --> 00:24:09.240]   But it is quite authoritative because it is the place
[00:24:09.240 --> 00:24:11.680]   that everybody likes to criticize as being wrong.
[00:24:11.680 --> 00:24:13.640]   - You say that it's superficial.
[00:24:13.640 --> 00:24:18.400]   On a lot of topics that I've studied a lot of,
[00:24:18.400 --> 00:24:22.240]   I find it, I don't know if superficial is the right word.
[00:24:23.240 --> 00:24:27.680]   'Cause superficial kind of implies that it's not correct.
[00:24:27.680 --> 00:24:29.120]   - No, no, no.
[00:24:29.120 --> 00:24:31.640]   I don't mean any implication of it not being correct.
[00:24:31.640 --> 00:24:32.840]   It's just superficial.
[00:24:32.840 --> 00:24:35.520]   It's basically only scratching the surface.
[00:24:35.520 --> 00:24:37.120]   For depth, you don't go to Wikipedia.
[00:24:37.120 --> 00:24:38.320]   You go to the review articles.
[00:24:38.320 --> 00:24:41.840]   - But it can be profound in the way that articles rarely,
[00:24:41.840 --> 00:24:43.280]   one of the frustrating things to me
[00:24:43.280 --> 00:24:46.600]   about certain computer science,
[00:24:46.600 --> 00:24:48.360]   like in the machine learning world,
[00:24:48.360 --> 00:24:53.360]   articles, they don't as often take the bigger picture view.
[00:24:53.360 --> 00:24:57.440]   There's a kind of data set and you show that it works
[00:24:57.440 --> 00:24:59.680]   and you kind of show that here's an architectural thing
[00:24:59.680 --> 00:25:02.280]   that creates an improvement and so on and so forth.
[00:25:02.280 --> 00:25:05.280]   But you don't say, well, what does this mean
[00:25:05.280 --> 00:25:08.580]   for the nature of intelligence for future data sets
[00:25:08.580 --> 00:25:10.080]   we haven't even thought about?
[00:25:10.080 --> 00:25:11.960]   Or if you were trying to implement this,
[00:25:11.960 --> 00:25:15.920]   like if we took this data set of 100,000 examples
[00:25:15.920 --> 00:25:19.840]   and scale it to 100 billion examples with this method,
[00:25:19.840 --> 00:25:21.240]   like look at the bigger picture,
[00:25:21.240 --> 00:25:25.560]   which is what a Wikipedia article would actually try to do,
[00:25:25.560 --> 00:25:28.560]   which is like, what does this mean in the context
[00:25:28.560 --> 00:25:32.360]   of the broad field of computer vision or something like that?
[00:25:32.360 --> 00:25:33.200]   - Yeah, yeah.
[00:25:33.200 --> 00:25:34.840]   No, I agree with you completely,
[00:25:34.840 --> 00:25:36.100]   but it depends on the topic.
[00:25:36.100 --> 00:25:38.440]   I mean, for some topics, there's been a huge amount of work.
[00:25:38.440 --> 00:25:40.320]   For other topics, it's just a stub.
[00:25:40.320 --> 00:25:41.560]   So, you know.
[00:25:41.560 --> 00:25:42.560]   - I got it. - Yeah.
[00:25:42.600 --> 00:25:46.400]   - Well, yeah, actually, which we'll talk on,
[00:25:46.400 --> 00:25:48.080]   genomics was not--
[00:25:48.080 --> 00:25:49.360]   - Yeah, it's very shallow.
[00:25:49.360 --> 00:25:50.480]   Yeah, yeah.
[00:25:50.480 --> 00:25:51.880]   It's not wrong, it's just shallow.
[00:25:51.880 --> 00:25:53.000]   - It's shallow. - Yeah.
[00:25:53.000 --> 00:25:54.720]   Every time I criticize something,
[00:25:54.720 --> 00:25:56.360]   I should feel partly responsible.
[00:25:56.360 --> 00:25:58.280]   Basically, if more people from my community
[00:25:58.280 --> 00:26:01.160]   went there and edited, it would not be shallow.
[00:26:01.160 --> 00:26:04.040]   It's just that there's different modes of communication
[00:26:04.040 --> 00:26:05.320]   in different fields.
[00:26:05.320 --> 00:26:09.040]   And in some fields, the experts have embraced Wikipedia.
[00:26:09.040 --> 00:26:11.200]   In other fields, it's relegated
[00:26:11.200 --> 00:26:14.080]   and perhaps the reason is that
[00:26:14.080 --> 00:26:16.540]   if it was any better to start with,
[00:26:16.540 --> 00:26:18.040]   people would invest more time.
[00:26:18.040 --> 00:26:20.000]   But if it's not great to start with,
[00:26:20.000 --> 00:26:22.400]   then you need a few initial pioneers
[00:26:22.400 --> 00:26:24.180]   who will basically go in and say,
[00:26:24.180 --> 00:26:26.640]   ah, enough, we're just gonna fix that.
[00:26:26.640 --> 00:26:29.160]   And then I think it'll catch on much more.
[00:26:29.160 --> 00:26:32.260]   - So, if it's okay, before we go on to genomics,
[00:26:32.260 --> 00:26:34.280]   can we linger a little bit longer
[00:26:34.280 --> 00:26:37.160]   on the beauty of the human genome?
[00:26:37.160 --> 00:26:38.600]   You've given me a few notes.
[00:26:38.600 --> 00:26:41.680]   What else do you find beautiful about the human genome?
[00:26:41.680 --> 00:26:44.900]   - So, the last aspect of what makes the human genome unique,
[00:26:44.900 --> 00:26:49.900]   in addition to the similarity and the differences
[00:26:49.900 --> 00:26:52.800]   and the individuality, is that,
[00:26:52.800 --> 00:26:57.360]   so, very early on, people would basically say,
[00:26:57.360 --> 00:26:59.280]   oh, you don't do that experiment in human.
[00:26:59.280 --> 00:27:01.240]   You have to learn about that in fly.
[00:27:01.240 --> 00:27:03.120]   Or you have to learn about that in yeast first,
[00:27:03.120 --> 00:27:05.880]   or in mouse first, or in a primate first.
[00:27:05.880 --> 00:27:07.800]   And the human genome was, in fact,
[00:27:07.800 --> 00:27:09.960]   relegated to sort of, oh, the last place
[00:27:09.960 --> 00:27:12.640]   that you're gonna go to learn something new.
[00:27:12.640 --> 00:27:14.240]   That has dramatically changed.
[00:27:14.240 --> 00:27:17.440]   And the reason that changed is human genetics.
[00:27:17.440 --> 00:27:22.520]   We are the species in the planet
[00:27:22.520 --> 00:27:24.680]   that's the most studied right now.
[00:27:24.680 --> 00:27:26.260]   It's embarrassing to say that,
[00:27:26.260 --> 00:27:28.400]   but this was not the case a few years ago.
[00:27:28.400 --> 00:27:31.840]   It used to be, you know, first viruses,
[00:27:31.840 --> 00:27:35.200]   then bacteria, then yeast,
[00:27:35.200 --> 00:27:37.840]   then the fruit fly and the worm,
[00:27:37.840 --> 00:27:42.360]   then the mouse, and eventually, human was very far last.
[00:27:42.360 --> 00:27:44.680]   - So, it's embarrassing that it took us this long
[00:27:44.680 --> 00:27:46.520]   to focus on it, or the--
[00:27:46.520 --> 00:27:49.160]   - It's embarrassing that the model organisms
[00:27:49.160 --> 00:27:52.600]   have been taken over because of the power of human genetics.
[00:27:52.600 --> 00:27:54.760]   That, right now, it's actually simpler
[00:27:54.760 --> 00:27:57.240]   to figure out the phenotype of something
[00:27:57.240 --> 00:28:01.360]   by mining this massive amount of human data
[00:28:01.360 --> 00:28:03.960]   than by going back to any of the other species.
[00:28:03.960 --> 00:28:05.480]   And the reason for that is that if you look
[00:28:05.480 --> 00:28:07.320]   at the natural variation that happens
[00:28:07.320 --> 00:28:09.660]   in a population of seven billion,
[00:28:09.660 --> 00:28:13.360]   you basically have a mutation in almost every nucleotide.
[00:28:13.360 --> 00:28:15.640]   So, every nucleotide you wanna perturb,
[00:28:15.640 --> 00:28:18.760]   you can go find a living, breathing human being
[00:28:18.760 --> 00:28:20.320]   and go test the function of that nucleotide
[00:28:20.320 --> 00:28:22.600]   by sort of searching the database and finding that person.
[00:28:22.600 --> 00:28:23.600]   - Wait, why is that embarrassing?
[00:28:23.600 --> 00:28:24.680]   It's a beautiful data set.
[00:28:24.680 --> 00:28:26.360]   - It's a beautiful data set.
[00:28:26.360 --> 00:28:29.320]   It's embarrassing for the model organism.
[00:28:29.320 --> 00:28:31.240]   - For the flies and-- - Yeah, exactly.
[00:28:31.240 --> 00:28:34.960]   - I mean, do you feel, on a small tangent,
[00:28:34.960 --> 00:28:39.960]   is there something of value in the genome of a fly
[00:28:39.960 --> 00:28:43.760]   and other of these model organisms that you miss
[00:28:43.760 --> 00:28:47.400]   that we wish we would be looking at deeper?
[00:28:47.400 --> 00:28:49.880]   - So, directed perturbation, of course.
[00:28:49.880 --> 00:28:54.120]   So, I think the place where humans are still lagging
[00:28:54.120 --> 00:28:55.720]   is the fact that in an animal model,
[00:28:55.720 --> 00:28:57.360]   you can go and say, well, let me knock out
[00:28:57.360 --> 00:28:58.640]   this gene completely. - Got it.
[00:28:58.640 --> 00:29:00.600]   - And let me knock out these three genes completely.
[00:29:00.600 --> 00:29:02.760]   And at the moment you get into combinatorics,
[00:29:02.760 --> 00:29:04.200]   it's something you can't do in the human
[00:29:04.200 --> 00:29:07.080]   because there just simply aren't enough humans on the planet
[00:29:07.080 --> 00:29:08.840]   and, again, let me be honest,
[00:29:08.840 --> 00:29:11.200]   we haven't sequenced all seven billion people.
[00:29:11.200 --> 00:29:12.800]   It's not like we have every mutation,
[00:29:12.800 --> 00:29:15.040]   but we know that there's a carrier out there.
[00:29:15.040 --> 00:29:17.480]   So, if you look at the trend and the speed
[00:29:17.480 --> 00:29:19.480]   with which human genetics has progressed,
[00:29:19.480 --> 00:29:22.440]   we can now find thousands of genes
[00:29:22.440 --> 00:29:27.080]   involved in human cognition, in human psychology,
[00:29:27.080 --> 00:29:29.080]   in the emotions and the feelings
[00:29:29.080 --> 00:29:31.800]   that we used to think are uniquely learned.
[00:29:31.800 --> 00:29:34.360]   Turns out there's a genetic basis to a lot of that.
[00:29:34.360 --> 00:29:39.360]   So, the human genome has continued to elucidate
[00:29:39.360 --> 00:29:44.880]   through these studies of genetic variation
[00:29:44.880 --> 00:29:47.520]   so many different processes that we previously thought
[00:29:47.520 --> 00:29:52.320]   were something that, like free will.
[00:29:52.320 --> 00:29:54.280]   Free will is this beautiful concept
[00:29:54.280 --> 00:29:56.760]   that humans have had for a long time.
[00:29:57.840 --> 00:29:59.880]   You know, in the end, it's just a bunch of chemical reactions
[00:29:59.880 --> 00:30:00.760]   happening in your brain
[00:30:00.760 --> 00:30:03.160]   and the particular abundance of receptors
[00:30:03.160 --> 00:30:06.120]   that you have this day based on what you ate yesterday
[00:30:06.120 --> 00:30:08.400]   or that you have been wired with
[00:30:08.400 --> 00:30:12.600]   based on your parents and your upbringing, et cetera,
[00:30:12.600 --> 00:30:15.720]   determines a lot of that, quote unquote, free will component
[00:30:15.720 --> 00:30:20.720]   to sort of narrow and narrow, sort of slices.
[00:30:20.720 --> 00:30:25.680]   - So, on that point, how much freedom do you think we have
[00:30:25.680 --> 00:30:30.400]   to escape the constraints of our genome?
[00:30:30.400 --> 00:30:31.960]   You're making it sound like more and more
[00:30:31.960 --> 00:30:33.400]   we're discovering that our genome
[00:30:33.400 --> 00:30:37.760]   actually has a lot of the story already encoded into it.
[00:30:37.760 --> 00:30:39.200]   How much freedom do we have?
[00:30:39.200 --> 00:30:45.120]   - So, let me describe what that freedom would look like.
[00:30:45.120 --> 00:30:47.600]   That freedom would be my saying,
[00:30:47.600 --> 00:30:51.520]   ooh, I'm gonna resist the urge to eat that apple
[00:30:51.520 --> 00:30:53.220]   because I choose not to.
[00:30:54.500 --> 00:30:56.460]   But there are chemical receptors
[00:30:56.460 --> 00:30:59.360]   that made me not resist the urge
[00:30:59.360 --> 00:31:02.360]   to prove my individuality and my free will
[00:31:02.360 --> 00:31:04.100]   by resisting the apple.
[00:31:04.100 --> 00:31:05.580]   So, then the next question is,
[00:31:05.580 --> 00:31:08.220]   well, maybe now I'll resist the urge to resist the apple
[00:31:08.220 --> 00:31:09.560]   and I'll go for the chocolate instead
[00:31:09.560 --> 00:31:10.780]   to prove my individuality.
[00:31:10.780 --> 00:31:14.420]   But then, what about those other receptors that, you know?
[00:31:14.420 --> 00:31:16.060]   (laughing)
[00:31:16.060 --> 00:31:17.860]   - That might be all encoded in there.
[00:31:17.860 --> 00:31:19.500]   - So, it's kicking the bucket down the road
[00:31:19.500 --> 00:31:22.060]   and basically saying, well, your choice
[00:31:22.060 --> 00:31:24.940]   will may have actually been driven by other things
[00:31:24.940 --> 00:31:26.700]   that you actually are not choosing.
[00:31:26.700 --> 00:31:30.060]   So, that's why it's very hard to answer that question.
[00:31:30.060 --> 00:31:31.420]   - It's hard to know what to do with that.
[00:31:31.420 --> 00:31:33.360]   I mean, if the genome has,
[00:31:33.360 --> 00:31:38.540]   if there's not much freedom, it's--
[00:31:38.540 --> 00:31:40.540]   - It's the butterfly effect.
[00:31:40.540 --> 00:31:42.940]   It's basically that in the short term,
[00:31:42.940 --> 00:31:45.720]   you can predict something extremely well
[00:31:45.720 --> 00:31:48.120]   by knowing the current state of the system.
[00:31:48.120 --> 00:31:50.740]   But a few steps down, it's very hard to predict
[00:31:50.740 --> 00:31:52.460]   based on the current knowledge.
[00:31:52.460 --> 00:31:55.300]   Is that because the system is truly free?
[00:31:55.300 --> 00:31:56.420]   When I look at weather patterns,
[00:31:56.420 --> 00:31:57.940]   I can predict the next 10 days.
[00:31:57.940 --> 00:32:00.320]   Is it because the weather has a lot of freedom
[00:32:00.320 --> 00:32:03.500]   and after 10 days, it chooses to do something else?
[00:32:03.500 --> 00:32:07.360]   Or is it because, in fact, the system is fully deterministic
[00:32:07.360 --> 00:32:10.180]   and there's just a slightly different magnetic field
[00:32:10.180 --> 00:32:12.460]   of the Earth, slightly more energy arriving from the sun,
[00:32:12.460 --> 00:32:13.740]   a slightly different spin
[00:32:13.740 --> 00:32:16.100]   of the gravitational pull of Jupiter
[00:32:16.100 --> 00:32:18.900]   that is now causing all kinds of tides
[00:32:18.900 --> 00:32:20.900]   and slight deviation of the moon, et cetera.
[00:32:20.900 --> 00:32:22.980]   Maybe all of that can be fully modeled.
[00:32:22.980 --> 00:32:25.760]   Maybe the fact that China is emitting
[00:32:25.760 --> 00:32:27.300]   a little more carbon today
[00:32:27.300 --> 00:32:31.480]   is actually gonna affect the weather in Egypt in three weeks
[00:32:31.480 --> 00:32:33.900]   and all of that could be fully modeled.
[00:32:33.900 --> 00:32:36.820]   In the same way, if you take a complete view
[00:32:36.820 --> 00:32:41.260]   of a human being now, I model everything about you.
[00:32:41.260 --> 00:32:44.940]   The question is, can I predict your next step?
[00:32:44.940 --> 00:32:47.820]   Probably, but at how far?
[00:32:47.820 --> 00:32:49.420]   And if it's a little further,
[00:32:49.420 --> 00:32:51.340]   is that because of stochasticity
[00:32:51.340 --> 00:32:54.580]   and sort of chaos properties of unpredictability
[00:32:54.580 --> 00:32:56.180]   of beyond a certain level
[00:32:56.180 --> 00:32:58.300]   or was that actually true free will?
[00:32:58.300 --> 00:33:01.300]   - Yeah, so the number of variables might be so,
[00:33:01.300 --> 00:33:03.780]   you might need to build an entire universe
[00:33:03.780 --> 00:33:06.620]   to be able to model. - To simulate a human
[00:33:06.620 --> 00:33:09.460]   and then maybe that human will be fully simulatable,
[00:33:09.460 --> 00:33:12.260]   but maybe aspects of free will will exist
[00:33:12.260 --> 00:33:13.420]   and where's that free will coming from?
[00:33:13.420 --> 00:33:15.020]   It's still coming from the same neurons
[00:33:15.020 --> 00:33:17.620]   or maybe from a spirit inhabiting these neurons,
[00:33:17.620 --> 00:33:19.760]   but again, it's very difficult empirically
[00:33:19.760 --> 00:33:22.580]   to sort of evaluate where does free will begin
[00:33:22.580 --> 00:33:25.820]   and sort of chemical reactions and electric signals and.
[00:33:25.820 --> 00:33:29.900]   - So on that topic, let me ask the most absurd question
[00:33:29.900 --> 00:33:33.940]   that most MIT faculty rolled their eyes on,
[00:33:33.940 --> 00:33:38.260]   but what do you think about the simulation hypothesis
[00:33:38.260 --> 00:33:40.260]   and the idea that we live in a simulation?
[00:33:40.260 --> 00:33:41.580]   - I think it's complete BS.
[00:33:41.580 --> 00:33:44.020]   (both laughing)
[00:33:44.020 --> 00:33:45.700]   - Okay. - There's no empirical evidence.
[00:33:45.700 --> 00:33:47.060]   - No, there's not. - Absolutely not.
[00:33:47.060 --> 00:33:49.020]   - Not in terms of empirical evidence, not,
[00:33:49.020 --> 00:33:52.380]   but in terms of a thought experiment,
[00:33:52.380 --> 00:33:54.860]   does it help you think about the universe?
[00:33:54.860 --> 00:33:57.500]   I mean, so if you look at the genome,
[00:33:57.500 --> 00:33:59.180]   it's encoding a lot of the information
[00:33:59.180 --> 00:34:01.500]   that is required to create some of the beautiful
[00:34:01.500 --> 00:34:04.220]   human complexity that we see around us.
[00:34:04.220 --> 00:34:05.940]   It's an interesting thought experiment.
[00:34:05.940 --> 00:34:10.940]   How much parameters do we need to have
[00:34:10.940 --> 00:34:15.300]   in order to model this full human experience?
[00:34:15.300 --> 00:34:17.540]   Like if we were to build a video game,
[00:34:17.540 --> 00:34:19.980]   how hard it would be to build a video game
[00:34:19.980 --> 00:34:22.660]   that's convincing enough and fun enough
[00:34:22.660 --> 00:34:27.660]   and has consistent laws of physics, all that stuff?
[00:34:27.660 --> 00:34:31.380]   It's not interesting to you as a thought experiment?
[00:34:31.380 --> 00:34:35.060]   - I mean, it's cute, but it's Occam's razor.
[00:34:35.060 --> 00:34:36.820]   I mean, what's more realistic,
[00:34:36.820 --> 00:34:38.340]   the fact that you're actually a machine
[00:34:38.340 --> 00:34:39.940]   or that you're a person?
[00:34:39.940 --> 00:34:43.340]   The fact that all of my experiences exist
[00:34:43.340 --> 00:34:45.540]   inside the chemical molecules that I have
[00:34:45.540 --> 00:34:48.660]   or that somebody's actually simulating all that?
[00:34:48.660 --> 00:34:49.500]   I mean, to me--
[00:34:49.500 --> 00:34:50.860]   - Well, you did refer to humans
[00:34:50.860 --> 00:34:52.540]   as a digital computer earlier, so--
[00:34:52.540 --> 00:34:54.260]   - Of course, of course, but that does not--
[00:34:54.260 --> 00:34:55.260]   - It's a kind of a machine, right?
[00:34:55.260 --> 00:35:00.260]   - I know, I know, but I think the probability
[00:35:00.260 --> 00:35:03.500]   of all that is nil and let the machines wake me up
[00:35:03.500 --> 00:35:05.500]   and just terminate me now if it's not.
[00:35:05.500 --> 00:35:07.540]   (laughing)
[00:35:07.540 --> 00:35:08.860]   I challenge you machines.
[00:35:08.860 --> 00:35:10.380]   - They're gonna wait a little bit
[00:35:10.380 --> 00:35:12.380]   to see what you're gonna do next.
[00:35:12.380 --> 00:35:14.540]   - It's fun, it's fun to watch,
[00:35:14.540 --> 00:35:16.280]   especially the clever humans.
[00:35:16.280 --> 00:35:19.540]   What's the difference to you between the way
[00:35:19.540 --> 00:35:21.300]   a computer stores information
[00:35:21.300 --> 00:35:23.980]   and the human genome stores information?
[00:35:23.980 --> 00:35:26.980]   So you also have roots and your work.
[00:35:26.980 --> 00:35:28.700]   Would you say you're,
[00:35:28.700 --> 00:35:30.700]   when you introduce yourself at a bar--
[00:35:30.700 --> 00:35:34.020]   - It depends who I'm talking to.
[00:35:34.020 --> 00:35:36.140]   - Would you say it's computational biology?
[00:35:36.140 --> 00:35:41.140]   Do you reveal your expertise in computer science
[00:35:41.300 --> 00:35:43.300]   or your expertise in computers?
[00:35:43.300 --> 00:35:45.340]   - It depends who I'm talking to, truly.
[00:35:45.340 --> 00:35:47.700]   I mean, basically, if I meet someone who's in computers,
[00:35:47.700 --> 00:35:51.100]   I'll say, oh, I'm a professor in computer science.
[00:35:51.100 --> 00:35:52.460]   If I meet someone who's in engineering,
[00:35:52.460 --> 00:35:54.740]   I say computer science and electrical engineering.
[00:35:54.740 --> 00:35:56.180]   If I meet someone in biology, I'll say,
[00:35:56.180 --> 00:35:57.180]   hey, I work in genomics.
[00:35:57.180 --> 00:35:58.460]   If I meet someone in medicine, I'm like,
[00:35:58.460 --> 00:36:00.740]   hey, I work on genetics.
[00:36:00.740 --> 00:36:03.260]   - You're a fun person to meet at a bar, I got you.
[00:36:03.260 --> 00:36:04.100]   So--
[00:36:04.100 --> 00:36:07.420]   - No, no, but what I'm trying to say is that I don't,
[00:36:07.420 --> 00:36:09.060]   I mean, there's no single attribute
[00:36:09.060 --> 00:36:10.820]   that I will define myself as.
[00:36:10.820 --> 00:36:12.060]   You know, there's a few things I know,
[00:36:12.060 --> 00:36:13.100]   there's a few things I study,
[00:36:13.100 --> 00:36:15.060]   there's a few things I have degrees on,
[00:36:15.060 --> 00:36:17.940]   and there's a few things that I grant degrees in.
[00:36:17.940 --> 00:36:22.660]   And, you know, I publish papers across the whole gamut,
[00:36:22.660 --> 00:36:25.300]   you know, the whole spectrum of computation
[00:36:25.300 --> 00:36:26.340]   to biology, et cetera.
[00:36:26.340 --> 00:36:31.340]   I mean, the complete answer is that I use computer science
[00:36:31.340 --> 00:36:32.860]   to understand biology.
[00:36:32.860 --> 00:36:38.700]   So I'm a, you know, I develop methods in AI
[00:36:38.700 --> 00:36:41.660]   and machine learning, statistics and algorithms, et cetera.
[00:36:41.660 --> 00:36:44.020]   But the ultimate goal of my career
[00:36:44.020 --> 00:36:45.660]   is to really understand biology.
[00:36:45.660 --> 00:36:49.620]   If these things don't advance our understanding of biology,
[00:36:49.620 --> 00:36:51.940]   I'm not as fascinated by them.
[00:36:51.940 --> 00:36:54.900]   Although there are some beautiful computational problems
[00:36:54.900 --> 00:36:57.900]   by themselves, I've sort of made it my mission
[00:36:57.900 --> 00:37:01.620]   to apply the power of computer science
[00:37:01.620 --> 00:37:06.620]   to truly understand the human genome, health, disease,
[00:37:06.980 --> 00:37:10.060]   you know, and the whole gamut of how our brain works,
[00:37:10.060 --> 00:37:11.700]   how our body works, and all of that,
[00:37:11.700 --> 00:37:13.900]   which is so fascinating.
[00:37:13.900 --> 00:37:14.980]   (laughs)
[00:37:14.980 --> 00:37:16.980]   - So the dream, there's not a equivalent
[00:37:16.980 --> 00:37:20.980]   sort of complementary dream of understanding
[00:37:20.980 --> 00:37:23.380]   human biology in order to create an artificial life,
[00:37:23.380 --> 00:37:26.100]   an artificial brain, an artificial intelligence
[00:37:26.100 --> 00:37:27.700]   that supersedes the intelligence
[00:37:27.700 --> 00:37:29.720]   and the capabilities of us humans.
[00:37:29.720 --> 00:37:31.620]   - It's an interesting question.
[00:37:31.620 --> 00:37:33.300]   It's a fascinating question.
[00:37:33.300 --> 00:37:38.300]   So understanding the human brain is undoubtedly coupled
[00:37:38.300 --> 00:37:42.220]   to how do we make better AI,
[00:37:42.220 --> 00:37:47.220]   because so much of AI has in fact been inspired by the brain.
[00:37:47.220 --> 00:37:49.100]   It may have taken 50 years
[00:37:49.100 --> 00:37:51.140]   since the early days of neural networks
[00:37:51.140 --> 00:37:55.460]   till we have, you know, all of these amazing progress
[00:37:55.460 --> 00:38:00.460]   that we've seen with, you know, deep belief networks
[00:38:00.860 --> 00:38:05.860]   and, you know, all of these advances in Go and chess,
[00:38:05.860 --> 00:38:10.480]   in image synthesis, in deep fakes, in you name it.
[00:38:10.480 --> 00:38:15.380]   And, but the underlying architecture
[00:38:15.380 --> 00:38:18.060]   is very much inspired by the human brain,
[00:38:18.060 --> 00:38:20.900]   which actually posits a very, very interesting question.
[00:38:20.900 --> 00:38:27.220]   Why are neural networks performing so well?
[00:38:27.220 --> 00:38:28.980]   And they perform amazingly well.
[00:38:28.980 --> 00:38:31.740]   Is it because they can simulate any possible function?
[00:38:31.740 --> 00:38:34.420]   And the answer is no, no.
[00:38:34.420 --> 00:38:37.180]   They simulate a very small number of functions.
[00:38:37.180 --> 00:38:38.420]   Is it because they can simulate
[00:38:38.420 --> 00:38:40.540]   every possible function in the universe?
[00:38:40.540 --> 00:38:41.540]   And that's where it gets interesting.
[00:38:41.540 --> 00:38:44.740]   The answer is actually, yeah, a little closer to that.
[00:38:44.740 --> 00:38:46.580]   And here's where it gets really fun.
[00:38:46.580 --> 00:38:51.720]   If you look at human brain and human cognition,
[00:38:51.720 --> 00:38:54.020]   it didn't evolve in a vacuum.
[00:38:54.020 --> 00:38:57.640]   It evolved in a world with physical constraints,
[00:38:58.820 --> 00:39:00.700]   like the world that inhabits us.
[00:39:00.700 --> 00:39:02.300]   It is the world that we inhabit.
[00:39:02.300 --> 00:39:08.220]   And if you look at our senses, what do they perceive?
[00:39:08.220 --> 00:39:10.340]   They perceive different, you know,
[00:39:10.340 --> 00:39:12.780]   parts of the electromagnetic spectrum.
[00:39:12.780 --> 00:39:17.300]   You know, the hearing is just different movements in air.
[00:39:17.300 --> 00:39:18.860]   The touch, et cetera.
[00:39:18.860 --> 00:39:21.700]   I mean, all of these things, we've built intuitions
[00:39:21.700 --> 00:39:24.020]   for the physical world that we inhabit.
[00:39:24.020 --> 00:39:26.140]   And our brains and the brains of all animals
[00:39:26.140 --> 00:39:29.220]   evolved for that world.
[00:39:29.220 --> 00:39:32.740]   And the AI systems that we have built
[00:39:32.740 --> 00:39:35.380]   happen to work well with images of the type
[00:39:35.380 --> 00:39:38.460]   that we encounter in the physical world that we inhabit.
[00:39:38.460 --> 00:39:42.480]   Whereas if you just take noise and you add random signal
[00:39:42.480 --> 00:39:44.500]   that doesn't match anything in our world,
[00:39:44.500 --> 00:39:47.020]   neural networks will not do as well.
[00:39:47.020 --> 00:39:52.020]   And that actually basically has this whole loop around this,
[00:39:52.980 --> 00:39:57.700]   which is this was designed by studying our own brain,
[00:39:57.700 --> 00:39:59.660]   which was evolved for our own world.
[00:39:59.660 --> 00:40:02.020]   And they happen to do well in our own world.
[00:40:02.020 --> 00:40:04.220]   And they happen to make the same types of mistakes
[00:40:04.220 --> 00:40:07.100]   that humans make many times.
[00:40:07.100 --> 00:40:08.820]   And of course you can engineer images
[00:40:08.820 --> 00:40:11.140]   by adding just the right amount of, you know,
[00:40:11.140 --> 00:40:14.500]   sort of pixel deviations to make a zebra look like a bamboo
[00:40:14.500 --> 00:40:17.140]   and stuff like that, or like a table.
[00:40:18.460 --> 00:40:22.780]   But ultimately the undoctored images at least
[00:40:22.780 --> 00:40:26.020]   are very often mistaken, I don't know,
[00:40:26.020 --> 00:40:28.940]   between muffins and dogs, for example,
[00:40:28.940 --> 00:40:31.300]   in the same way that humans make those mistakes.
[00:40:31.300 --> 00:40:35.900]   So it's, you know, there's no doubt in my view
[00:40:35.900 --> 00:40:38.660]   that the more we understand about the tricks
[00:40:38.660 --> 00:40:40.660]   that our human brain has evolved
[00:40:40.660 --> 00:40:42.980]   to understand the physical world around us,
[00:40:42.980 --> 00:40:44.820]   the more we will be able to bring
[00:40:44.820 --> 00:40:48.820]   new computational primitives in our AI systems
[00:40:48.820 --> 00:40:52.260]   to again, better understand not just the world around us,
[00:40:52.260 --> 00:40:54.460]   but maybe even the world inside us.
[00:40:54.460 --> 00:40:56.420]   And maybe even the computational problems
[00:40:56.420 --> 00:40:58.380]   that arise from new types of data
[00:40:58.380 --> 00:41:00.380]   that we haven't been exposed to,
[00:41:00.380 --> 00:41:03.460]   but are yet inhabiting the same universe that we live in
[00:41:03.460 --> 00:41:06.100]   with a very tiny little subset of functions
[00:41:06.100 --> 00:41:08.140]   from all possible mathematical functions.
[00:41:08.140 --> 00:41:10.220]   - Yeah, and that small subset of functions,
[00:41:10.220 --> 00:41:11.700]   all that matters to us humans, really.
[00:41:11.700 --> 00:41:12.940]   That's what makes-
[00:41:12.940 --> 00:41:14.860]   - It's all that has mattered so far.
[00:41:14.860 --> 00:41:17.100]   And even within our scientific realm,
[00:41:17.100 --> 00:41:19.740]   it's all that seems to continue to matter.
[00:41:19.740 --> 00:41:24.740]   But I mean, I always like to think about our senses
[00:41:24.740 --> 00:41:29.660]   and how much of the physical world around us we perceive.
[00:41:29.660 --> 00:41:34.660]   And if you look at the LIGO experiment
[00:41:34.660 --> 00:41:38.220]   over the last year and a half has been all over the news.
[00:41:38.220 --> 00:41:39.660]   What did LIGO do?
[00:41:39.660 --> 00:41:42.900]   It created a new sense for human beings.
[00:41:42.900 --> 00:41:45.820]   A sense that has never been sensed
[00:41:45.820 --> 00:41:47.300]   in the history of our planet.
[00:41:47.300 --> 00:41:51.620]   Gravitational waves have been traversing the earth
[00:41:51.620 --> 00:41:55.180]   since its creation a few billion years ago.
[00:41:55.180 --> 00:42:00.340]   Life has evolved senses to sense things
[00:42:00.340 --> 00:42:02.140]   that were never before sensed.
[00:42:02.140 --> 00:42:06.660]   Light was not perceived by early life.
[00:42:06.660 --> 00:42:07.500]   No one cared.
[00:42:07.500 --> 00:42:12.140]   And eventually, photoreceptors evolved
[00:42:12.140 --> 00:42:15.060]   and the ability to sense colors
[00:42:15.060 --> 00:42:16.900]   by sort of catching different parts
[00:42:16.900 --> 00:42:19.180]   of that electromagnetic spectrum.
[00:42:19.180 --> 00:42:24.180]   And hearing evolved and touch evolved, et cetera.
[00:42:24.180 --> 00:42:27.740]   But no organism evolved a way to sense neutrinos
[00:42:27.740 --> 00:42:28.780]   floating through earth
[00:42:28.780 --> 00:42:31.340]   or gravitational waves flowing through earth, et cetera.
[00:42:31.340 --> 00:42:33.900]   And I find it so beautiful in the history
[00:42:33.900 --> 00:42:37.060]   of not just humanity, but life on the planet,
[00:42:37.060 --> 00:42:40.500]   that we are now able to capture additional signals
[00:42:40.500 --> 00:42:43.660]   from the physical world than we ever knew before.
[00:42:43.660 --> 00:42:46.380]   And axioms, for example, have been all over the news
[00:42:46.380 --> 00:42:47.500]   in the last few weeks.
[00:42:47.500 --> 00:42:52.940]   The concept that we can capture
[00:42:52.940 --> 00:42:54.940]   and perceive more of that physical world
[00:42:54.940 --> 00:43:01.060]   is as exciting as the fact that we were blind to it
[00:43:01.060 --> 00:43:04.620]   is traumatizing before.
[00:43:04.620 --> 00:43:08.300]   Because that also tells us, you know, we're in 2020.
[00:43:09.380 --> 00:43:12.820]   Picture yourself in 3020 or in 20, you know--
[00:43:12.820 --> 00:43:15.460]   - What new senses might we discover?
[00:43:15.460 --> 00:43:21.940]   - Could it be that we're missing 9/10 of physics?
[00:43:21.940 --> 00:43:24.020]   That there's a lot of physics out there
[00:43:24.020 --> 00:43:27.940]   that we're just blind to, completely oblivious to it,
[00:43:27.940 --> 00:43:29.420]   and yet they're permeating us all the time.
[00:43:29.420 --> 00:43:31.580]   - Yeah, so it might be right in front of us.
[00:43:31.580 --> 00:43:34.260]   - So when you're thinking about premonitions,
[00:43:34.260 --> 00:43:37.620]   yeah, a lot of that is ascertainment bias.
[00:43:37.620 --> 00:43:39.420]   Like, yeah, every now and then you're like,
[00:43:39.420 --> 00:43:41.060]   "Oh, I remember my friend,"
[00:43:41.060 --> 00:43:42.700]   and then my friend doesn't appear,
[00:43:42.700 --> 00:43:44.580]   and I'll forget that I remember my friend.
[00:43:44.580 --> 00:43:46.060]   But every now and then, my friend will actually appear.
[00:43:46.060 --> 00:43:48.340]   I'm like, "Oh my God, I thought about you a minute ago.
[00:43:48.340 --> 00:43:50.180]   "You just called me, that's amazing."
[00:43:50.180 --> 00:43:52.020]   So, you know, some of that is this,
[00:43:52.020 --> 00:43:55.100]   but some of that might be that there are,
[00:43:55.100 --> 00:44:00.100]   within our brain, sensors for waves that we emit
[00:44:00.100 --> 00:44:03.260]   that we're not even aware of.
[00:44:03.260 --> 00:44:07.060]   And this whole concept of when I hug my children,
[00:44:07.060 --> 00:44:10.500]   there's such an emotional transfer there
[00:44:10.500 --> 00:44:12.260]   that we don't comprehend.
[00:44:12.260 --> 00:44:15.140]   I mean, sure, yeah, of course, we're all like hardwired
[00:44:15.140 --> 00:44:16.740]   for all kinds of touchy-feely things
[00:44:16.740 --> 00:44:18.260]   between parents and kids, it's beautiful,
[00:44:18.260 --> 00:44:20.700]   between partners, it's beautiful, et cetera.
[00:44:20.700 --> 00:44:25.700]   But then there are intangible aspects of human communication
[00:44:25.700 --> 00:44:30.060]   that I don't think it's unfathomable
[00:44:30.060 --> 00:44:32.900]   that our brain has actually evolved ways and sensors for it
[00:44:32.900 --> 00:44:33.980]   that we just don't capture.
[00:44:33.980 --> 00:44:35.260]   We don't understand the function
[00:44:35.260 --> 00:44:37.460]   of the vast majority of our neurons.
[00:44:37.460 --> 00:44:40.140]   And maybe our brain is already sensing it,
[00:44:40.140 --> 00:44:43.980]   but even worse, maybe our brain is not sensing it at all,
[00:44:43.980 --> 00:44:46.620]   and we're oblivious to this until we build a machine
[00:44:46.620 --> 00:44:48.300]   that suddenly is able to sort of capture
[00:44:48.300 --> 00:44:50.380]   so much more of what's happening in the natural world.
[00:44:50.380 --> 00:44:52.260]   - So what you're saying is we're going,
[00:44:52.260 --> 00:44:54.780]   physics is going to discover a sensor for love.
[00:44:54.780 --> 00:45:00.220]   - And maybe dogs are off scale for that.
[00:45:00.220 --> 00:45:01.460]   (Zubin laughs)
[00:45:01.460 --> 00:45:04.140]   And we've been oblivious to it the whole time
[00:45:04.140 --> 00:45:05.780]   'cause we didn't have the right sensor.
[00:45:05.780 --> 00:45:07.380]   And now you're going to have a little wrist that says,
[00:45:07.380 --> 00:45:09.620]   "Oh my God, I feel all this love in the house.
[00:45:09.620 --> 00:45:11.820]   "I sense a disturbance in the forest."
[00:45:11.820 --> 00:45:12.820]   (Zubin laughs)
[00:45:12.820 --> 00:45:13.700]   - It's all around us.
[00:45:13.700 --> 00:45:15.740]   And dogs and cats will have zero.
[00:45:15.740 --> 00:45:16.580]   - None. - None.
[00:45:16.580 --> 00:45:17.420]   - None.
[00:45:17.420 --> 00:45:18.260]   - It's just, yeah. - Oh, looks like you lost it.
[00:45:18.260 --> 00:45:20.140]   (both laugh)
[00:45:20.140 --> 00:45:24.540]   - But let's take a step back to our unfortunate place.
[00:45:24.540 --> 00:45:26.980]   - To one of the 400 topics that we had actually planned for.
[00:45:26.980 --> 00:45:29.580]   (both laugh)
[00:45:29.580 --> 00:45:31.820]   - But to our sad time in 2020
[00:45:31.820 --> 00:45:33.860]   when we only have just a few sensors
[00:45:33.860 --> 00:45:37.620]   and we're very primitive early computers.
[00:45:37.620 --> 00:45:41.820]   So you have a foot in computer science
[00:45:41.820 --> 00:45:43.500]   and a foot in biology.
[00:45:43.500 --> 00:45:48.300]   In your sense, how do computers represent information
[00:45:48.300 --> 00:45:52.300]   differently than the genome or biological systems?
[00:45:52.300 --> 00:45:55.900]   - So first of all, let me correct that,
[00:45:55.900 --> 00:45:58.300]   no, we're in an amazing time in 2020.
[00:45:58.300 --> 00:46:00.340]   (both laugh)
[00:46:00.340 --> 00:46:02.460]   Computer science is totally awesome
[00:46:02.460 --> 00:46:03.980]   and physics is totally awesome
[00:46:03.980 --> 00:46:06.900]   and we have understood so much of the natural world
[00:46:06.900 --> 00:46:08.500]   than ever before.
[00:46:08.500 --> 00:46:13.140]   So I am extremely grateful and feeling extremely lucky
[00:46:13.140 --> 00:46:16.180]   to be living in the time that we are.
[00:46:16.180 --> 00:46:20.060]   'Cause first of all, who knows when the asteroid will hit?
[00:46:20.060 --> 00:46:21.860]   (Zubin laughs)
[00:46:21.860 --> 00:46:26.140]   And second, of all times in humanity,
[00:46:26.140 --> 00:46:29.420]   this is probably the best time to be a human being
[00:46:29.420 --> 00:46:31.100]   and this might actually be the best place
[00:46:31.100 --> 00:46:31.940]   to be a human being.
[00:46:31.940 --> 00:46:34.460]   So anyway, for anyone who loves science,
[00:46:34.460 --> 00:46:36.980]   this is it, this is awesome, it's a great time.
[00:46:36.980 --> 00:46:39.340]   - At the same time, just a quick comment.
[00:46:39.340 --> 00:46:43.620]   All I meant is that if we look several hundred years from now
[00:46:43.620 --> 00:46:48.540]   and we end up somehow not destroying ourselves,
[00:46:48.540 --> 00:46:50.340]   people will probably look back at this time
[00:46:50.340 --> 00:46:55.340]   in computer science and at your work of Manos at MIT.
[00:46:55.340 --> 00:46:56.660]   - As infantile.
[00:46:56.660 --> 00:46:59.700]   - As infantile and silly and how ignorant it all was.
[00:46:59.700 --> 00:47:02.580]   I like to joke very often with my students
[00:47:02.580 --> 00:47:04.300]   that we've written so many papers,
[00:47:04.300 --> 00:47:06.540]   we've published so much, we've been cited so much
[00:47:06.540 --> 00:47:08.460]   and every single time I tell my students,
[00:47:08.460 --> 00:47:09.780]   the best is ahead of us.
[00:47:09.780 --> 00:47:12.540]   What we're working on now is the most exciting thing
[00:47:12.540 --> 00:47:13.940]   I've ever worked on.
[00:47:13.940 --> 00:47:16.260]   So in a way, I do have this sense of,
[00:47:16.260 --> 00:47:18.600]   yeah, even the papers I wrote 10 years ago,
[00:47:18.600 --> 00:47:20.380]   they were awesome at the time,
[00:47:20.380 --> 00:47:22.460]   but I'm so much more excited about where we're heading now.
[00:47:22.460 --> 00:47:24.580]   And I don't mean to minimize any of the stuff
[00:47:24.580 --> 00:47:28.500]   we've done in the past, but there's just this sense
[00:47:28.500 --> 00:47:31.060]   of excitement about what you're working on now
[00:47:31.060 --> 00:47:33.460]   that as soon as a paper is submitted,
[00:47:33.460 --> 00:47:35.260]   it's like, ugh, it's old.
[00:47:35.260 --> 00:47:37.180]   Like, you know, I can't talk about that anymore.
[00:47:37.180 --> 00:47:38.020]   I'm not gonna talk about it.
[00:47:38.020 --> 00:47:39.820]   - At the same time, you probably are not going
[00:47:39.820 --> 00:47:44.300]   to be able to predict what are the most impactful papers
[00:47:44.300 --> 00:47:47.380]   and ideas when people look back 200 years from now
[00:47:47.380 --> 00:47:50.780]   at your work, what would be the most exciting papers.
[00:47:50.780 --> 00:47:54.260]   And it may very well be not the thing that you expected.
[00:47:54.260 --> 00:47:58.100]   Or the things you got awards for or, you know,
[00:47:58.100 --> 00:48:00.020]   - This might be true in some fields.
[00:48:00.020 --> 00:48:01.380]   I don't know, I feel slightly differently
[00:48:01.380 --> 00:48:02.380]   about it in our field.
[00:48:02.380 --> 00:48:05.660]   I feel that I kind of know what are the important ones.
[00:48:05.660 --> 00:48:07.340]   And there's a very big difference
[00:48:07.340 --> 00:48:09.220]   between what the press picks up on
[00:48:09.220 --> 00:48:11.660]   and what's actually fundamentally important for the field.
[00:48:11.660 --> 00:48:13.420]   And I think for the fundamentally important ones,
[00:48:13.420 --> 00:48:15.620]   we kind of have a pretty good idea what they are.
[00:48:15.620 --> 00:48:18.180]   And it's hard to sometimes get the press excited
[00:48:18.180 --> 00:48:21.380]   about the fundamental advances, but you know,
[00:48:21.380 --> 00:48:24.780]   we take what we get and celebrate what we get.
[00:48:24.780 --> 00:48:27.220]   And sometimes, you know, one of our papers,
[00:48:27.220 --> 00:48:30.220]   which was in a minor journal, made the front page of Reddit
[00:48:30.220 --> 00:48:33.580]   and suddenly had like hundreds of thousands of views.
[00:48:33.580 --> 00:48:35.020]   Even though it was in a minor journal,
[00:48:35.020 --> 00:48:37.060]   because, you know, somebody pitched it the right way
[00:48:37.060 --> 00:48:39.380]   that it suddenly caught everybody's attention.
[00:48:39.380 --> 00:48:42.060]   Whereas other papers that are sort of truly fundamental,
[00:48:42.060 --> 00:48:44.500]   you know, we have a hard time getting the editors
[00:48:44.500 --> 00:48:47.900]   even excited about them when so many hundreds of people
[00:48:47.900 --> 00:48:50.900]   are already using the results and building upon them.
[00:48:50.900 --> 00:48:54.420]   So I do appreciate that there's a discrepancy
[00:48:54.420 --> 00:48:57.460]   between the perception and the perceived success
[00:48:57.460 --> 00:48:59.540]   and the awards that you get for various papers.
[00:48:59.540 --> 00:49:02.300]   But I think that fundamentally, I know that,
[00:49:02.300 --> 00:49:04.500]   you know, some paper, so when you're right--
[00:49:04.500 --> 00:49:06.860]   - So is there a paper that you're most proud of?
[00:49:06.860 --> 00:49:09.380]   See, now you just, you trapped yourself.
[00:49:09.380 --> 00:49:10.220]   - No, no, no, no.
[00:49:10.220 --> 00:49:13.580]   - I mean, is there a line of work that you have a sense
[00:49:13.580 --> 00:49:17.620]   is really powerful that you've done to date?
[00:49:17.620 --> 00:49:20.220]   You've done so much work in so many directions,
[00:49:20.220 --> 00:49:21.900]   which is interesting.
[00:49:21.900 --> 00:49:24.920]   Is there something where you think is quite special?
[00:49:24.920 --> 00:49:28.820]   - I mean, it's like asking me to say
[00:49:28.820 --> 00:49:30.460]   which of my three children I love best.
[00:49:30.460 --> 00:49:32.660]   I mean. (laughs)
[00:49:32.660 --> 00:49:34.980]   - Exactly.
[00:49:34.980 --> 00:49:38.600]   - So, I mean, and it's such a gimme question
[00:49:38.600 --> 00:49:42.660]   that it's so difficult not to brag
[00:49:42.660 --> 00:49:44.820]   about the awesome work that my team
[00:49:44.820 --> 00:49:46.140]   and my students have done.
[00:49:46.140 --> 00:49:50.060]   And I'll just mention a few off the top of my head.
[00:49:50.060 --> 00:49:53.180]   I mean, basically there's a few landmark papers
[00:49:53.180 --> 00:49:56.900]   that I think have shaped my scientific path.
[00:49:56.900 --> 00:50:00.500]   And, you know, I like to somehow describe it
[00:50:00.500 --> 00:50:03.740]   as a linear continuation of one thing led to another
[00:50:03.740 --> 00:50:05.460]   and led to another, led to another.
[00:50:05.460 --> 00:50:09.140]   And, you know, it kind of all started with,
[00:50:09.140 --> 00:50:12.420]   skip, skip, skip, skip, skip.
[00:50:12.420 --> 00:50:15.340]   Let me try to start somewhere in the middle. (laughs)
[00:50:15.340 --> 00:50:20.020]   So my first PhD paper was the first comparative
[00:50:20.020 --> 00:50:21.900]   analysis of multiple species.
[00:50:21.900 --> 00:50:23.660]   So multiple complete genomes.
[00:50:23.660 --> 00:50:27.380]   So for the first time, we basically developed a concept
[00:50:27.380 --> 00:50:30.020]   of genome-wide evolutionary signatures.
[00:50:30.020 --> 00:50:32.980]   The fact that you could look across the entire genome
[00:50:32.980 --> 00:50:35.700]   and understand how things evolve.
[00:50:35.700 --> 00:50:38.300]   And from these signatures of evolution,
[00:50:38.300 --> 00:50:42.460]   you could go back and study any one region and say,
[00:50:42.460 --> 00:50:44.060]   that's a protein coding gene.
[00:50:44.060 --> 00:50:45.580]   That's an RNA gene.
[00:50:45.580 --> 00:50:47.300]   That's a regulatory motif.
[00:50:47.300 --> 00:50:50.140]   That's a binding site and so on and so forth.
[00:50:50.140 --> 00:50:51.340]   So-- - Oh, sorry.
[00:50:51.340 --> 00:50:53.780]   So comparing different-- - Different species.
[00:50:53.780 --> 00:50:56.300]   - Species of the same, so-- - So take human, mouse,
[00:50:56.300 --> 00:50:58.060]   rat, and dog. - Yep.
[00:50:58.060 --> 00:50:59.060]   - You know, they're all animals.
[00:50:59.060 --> 00:50:59.980]   They're all mammals.
[00:50:59.980 --> 00:51:02.820]   They're all performing similar functions with their heart,
[00:51:02.820 --> 00:51:05.180]   with their brain, with their lungs, et cetera, et cetera,
[00:51:05.180 --> 00:51:06.020]   et cetera.
[00:51:06.020 --> 00:51:08.140]   So there's many functional elements
[00:51:08.140 --> 00:51:10.900]   that make us uniquely mammalian.
[00:51:10.900 --> 00:51:14.620]   And those mammalian elements are actually conserved.
[00:51:14.620 --> 00:51:17.740]   99% of our genome does not code for protein.
[00:51:17.740 --> 00:51:20.780]   1% codes for protein.
[00:51:20.780 --> 00:51:25.100]   The other 99%, we frankly didn't know what it does
[00:51:25.100 --> 00:51:28.140]   until we started doing this comparative genomic studies.
[00:51:28.140 --> 00:51:32.060]   So basically, these series of papers in my career
[00:51:32.060 --> 00:51:34.540]   have basically first developed that concept
[00:51:34.540 --> 00:51:37.460]   of evolutionary signatures and then applied them to yeast,
[00:51:37.460 --> 00:51:40.140]   applied them to flies, applied them to four mammals,
[00:51:40.140 --> 00:51:41.620]   applied them to 17 fungi,
[00:51:41.620 --> 00:51:43.700]   applied them to 12 Drosophila species,
[00:51:43.700 --> 00:51:46.900]   applied them to then 29 mammals, and now 200 mammals.
[00:51:46.900 --> 00:51:48.860]   - So sorry, so can we,
[00:51:48.860 --> 00:51:50.700]   so the evolutionary signatures,
[00:51:50.700 --> 00:51:53.580]   it seems like such a fascinating idea.
[00:51:53.580 --> 00:51:57.380]   I'm probably gonna linger on your early PhD work
[00:51:57.380 --> 00:51:58.220]   for two hours.
[00:51:58.220 --> 00:52:03.220]   But what is, how can you reveal something interesting
[00:52:03.220 --> 00:52:09.280]   about the genome by looking at the multiple species
[00:52:09.280 --> 00:52:11.940]   and looking at the evolutionary signatures?
[00:52:11.940 --> 00:52:12.780]   - Yeah.
[00:52:12.780 --> 00:52:17.780]   So you basically align the matching regions.
[00:52:17.780 --> 00:52:24.060]   So everything evolved from a common ancestor way, way back.
[00:52:24.060 --> 00:52:26.100]   And mammals evolved from a common ancestor
[00:52:26.100 --> 00:52:27.940]   about 60 million years back.
[00:52:27.940 --> 00:52:32.940]   So after the meteor that killed off the dinosaurs
[00:52:32.940 --> 00:52:38.860]   landed near Machu Picchu, we know the crater.
[00:52:38.860 --> 00:52:40.620]   It didn't allegedly land.
[00:52:40.620 --> 00:52:41.740]   (laughing)
[00:52:41.740 --> 00:52:42.860]   - That was the aliens, okay.
[00:52:42.860 --> 00:52:44.700]   - No, just slightly north of Machu Picchu
[00:52:44.700 --> 00:52:47.140]   in the Gulf of Mexico, there's a giant hole
[00:52:47.140 --> 00:52:49.100]   that that meteor impact.
[00:52:49.100 --> 00:52:51.380]   - Sorry, is that definitive to people?
[00:52:51.380 --> 00:52:56.380]   Have people conclusively figured out
[00:52:56.380 --> 00:52:58.220]   what killed the dinosaurs?
[00:52:58.220 --> 00:52:59.260]   - I think so.
[00:52:59.260 --> 00:53:00.540]   - So it was a meteor?
[00:53:00.540 --> 00:53:03.460]   - Well, you know, volcanic activity,
[00:53:03.460 --> 00:53:06.820]   all kinds of other stuff is coinciding.
[00:53:06.820 --> 00:53:09.580]   But the meteor is pretty unique.
[00:53:09.580 --> 00:53:11.180]   And we now have-- - That's also terrifying.
[00:53:11.180 --> 00:53:13.220]   (laughing)
[00:53:13.220 --> 00:53:14.980]   - We still have a lot of 2020 left.
[00:53:14.980 --> 00:53:16.100]   So if anything comes-- - No, no,
[00:53:16.100 --> 00:53:17.260]   but think about it this way.
[00:53:17.260 --> 00:53:22.260]   So the dinosaurs ruled the earth for 175 million years.
[00:53:22.260 --> 00:53:28.420]   We humans have been around for, what,
[00:53:28.420 --> 00:53:31.020]   less than one million years, if you're super generous
[00:53:31.020 --> 00:53:32.940]   about what you call humans.
[00:53:32.940 --> 00:53:34.580]   And you include chimps, basically.
[00:53:34.580 --> 00:53:38.780]   So we are just getting warmed up.
[00:53:39.660 --> 00:53:42.540]   And we've ruled the planet much more ruthlessly
[00:53:42.540 --> 00:53:44.100]   than Tyrannosaurus Rex.
[00:53:44.100 --> 00:53:46.260]   (laughing)
[00:53:46.260 --> 00:53:49.580]   T-Rex had much less of an environmental impact than we did.
[00:53:49.580 --> 00:53:54.060]   And if you give us another 174 million years,
[00:53:54.060 --> 00:53:58.380]   humans will look very different if we make it that far.
[00:53:58.380 --> 00:54:02.180]   So I think dinosaurs basically are much more
[00:54:02.180 --> 00:54:06.100]   of life history on earth than we are in all respects.
[00:54:06.100 --> 00:54:07.700]   But look at the bright side.
[00:54:07.700 --> 00:54:10.340]   When they were killed off, another life form emerged,
[00:54:10.340 --> 00:54:12.940]   mammals. - And that's that whole
[00:54:12.940 --> 00:54:15.100]   evolutionary branching that's happened.
[00:54:15.100 --> 00:54:17.340]   So you kind of have, when you have
[00:54:17.340 --> 00:54:21.180]   these evolutionary signatures, is there basically a map
[00:54:21.180 --> 00:54:22.660]   of how the genome changed?
[00:54:22.660 --> 00:54:23.500]   - Yeah, exactly. - Throughout?
[00:54:23.500 --> 00:54:26.180]   - So now you can go back to this early mammal
[00:54:26.180 --> 00:54:29.260]   that was hiding in caves, and you can basically ask
[00:54:29.260 --> 00:54:31.300]   what happened after the dinosaurs were wiped out.
[00:54:31.300 --> 00:54:34.060]   A ton of evolutionary niches opened up.
[00:54:34.060 --> 00:54:37.500]   And the mammals started populating all of these niches.
[00:54:37.500 --> 00:54:42.500]   And in that diversification, there was room for expansion
[00:54:42.500 --> 00:54:44.820]   of new types of functions.
[00:54:44.820 --> 00:54:49.820]   So some of them populated the air with bats flying,
[00:54:49.820 --> 00:54:51.740]   a new evolution of flight.
[00:54:51.740 --> 00:54:57.520]   Some populated the oceans with dolphins and whales
[00:54:57.520 --> 00:54:58.780]   going off to swim, et cetera.
[00:54:58.780 --> 00:55:01.420]   But we all are fundamentally mammals.
[00:55:01.420 --> 00:55:04.300]   So you can take the genomes of all these species
[00:55:04.300 --> 00:55:06.340]   and align them on top of each other.
[00:55:06.340 --> 00:55:11.340]   And basically create nucleotide resolution correspondences.
[00:55:11.340 --> 00:55:14.300]   What my PhD work showed is that when you do that,
[00:55:14.300 --> 00:55:17.260]   when you line up species on top of each other,
[00:55:17.260 --> 00:55:19.900]   you can see that within protein-coding genes,
[00:55:19.900 --> 00:55:22.040]   there's a particular pattern of evolution
[00:55:22.040 --> 00:55:24.820]   that is dictated by the level
[00:55:24.820 --> 00:55:27.760]   at which evolutionary selection acts.
[00:55:27.760 --> 00:55:31.580]   If I'm coding for a protein, and I change
[00:55:31.580 --> 00:55:35.180]   the third codon position of a triplet
[00:55:35.180 --> 00:55:37.420]   that codes for that amino acid,
[00:55:37.420 --> 00:55:39.780]   the same amino acid will be encoded.
[00:55:39.780 --> 00:55:42.840]   So that basically means that any kind of mutation
[00:55:42.840 --> 00:55:45.540]   that preserves that translation,
[00:55:45.540 --> 00:55:50.360]   that is invariant to that ultimate functional assessment
[00:55:50.360 --> 00:55:53.180]   that evolution will give, is tolerated.
[00:55:53.180 --> 00:55:55.860]   So for any function that you're trying to achieve,
[00:55:55.860 --> 00:55:58.540]   there's a set of sequences that encode it.
[00:55:58.540 --> 00:56:01.040]   You can now look at the mapping,
[00:56:01.040 --> 00:56:05.100]   the graph isomorphism, if you wish,
[00:56:05.100 --> 00:56:08.100]   between all of the possible DNA encodings
[00:56:08.100 --> 00:56:10.460]   of a particular function and that function.
[00:56:10.460 --> 00:56:13.020]   And instead of having just that exact sequence
[00:56:13.020 --> 00:56:15.660]   at the protein level, you can think of the set
[00:56:15.660 --> 00:56:18.660]   of protein sequences that all fulfill the same function.
[00:56:18.660 --> 00:56:20.020]   What's evolution doing?
[00:56:20.020 --> 00:56:21.460]   Evolution has two components.
[00:56:21.460 --> 00:56:25.940]   One component is random, blind, and stupid mutation.
[00:56:25.940 --> 00:56:30.940]   The other component is super smart, ruthless selection.
[00:56:30.940 --> 00:56:34.420]   That's my mom calling from Greece.
[00:56:34.620 --> 00:56:35.860]   (both laughing)
[00:56:35.860 --> 00:56:38.000]   Yes, I might be a fully grown man,
[00:56:38.000 --> 00:56:40.620]   but I am a Greek.
[00:56:40.620 --> 00:56:42.140]   - Did you just cancel the call?
[00:56:42.140 --> 00:56:43.540]   Wow, you're in trouble. - I know, I'm in trouble.
[00:56:43.540 --> 00:56:45.500]   No, she's gonna be calling the cops.
[00:56:45.500 --> 00:56:47.980]   - I'm gonna edit this clip out and send it to her.
[00:56:47.980 --> 00:56:50.740]   (both laughing)
[00:56:50.740 --> 00:56:51.660]   - So. - So yeah,
[00:56:51.660 --> 00:56:53.080]   so there's a lot of encoding
[00:56:53.080 --> 00:56:54.340]   for the same kind of function.
[00:56:54.340 --> 00:56:56.660]   - Yeah, so you now have this mapping
[00:56:56.660 --> 00:56:58.800]   between all of the set of functions
[00:56:58.800 --> 00:57:00.920]   that could all encode the same,
[00:57:00.920 --> 00:57:02.220]   all of the set of sequences
[00:57:02.220 --> 00:57:04.300]   that can all encode the same function.
[00:57:04.300 --> 00:57:06.620]   What evolutionary signatures does
[00:57:06.620 --> 00:57:09.020]   is that it basically looks at the shape
[00:57:09.020 --> 00:57:11.220]   of that distribution of sequences
[00:57:11.220 --> 00:57:13.100]   that all encode the same thing.
[00:57:13.100 --> 00:57:15.260]   And based on that shape, you can basically say,
[00:57:15.260 --> 00:57:17.940]   ooh, proteins have a very different shape
[00:57:17.940 --> 00:57:21.360]   than RNA structures, than regulatory motifs, et cetera.
[00:57:21.360 --> 00:57:24.520]   So just by scanning a sequence, ignoring the sequence,
[00:57:24.520 --> 00:57:26.760]   and just looking at the patterns of change,
[00:57:26.760 --> 00:57:29.500]   I'm like, wow, this thing is evolving like a protein.
[00:57:29.500 --> 00:57:31.700]   And that thing is evolving like a motif,
[00:57:31.700 --> 00:57:33.180]   and that thing is evolving.
[00:57:33.180 --> 00:57:35.620]   So that's exactly what we just did for COVID.
[00:57:35.620 --> 00:57:39.020]   So our paper that we post in a bioarchive about coronavirus
[00:57:39.020 --> 00:57:42.020]   basically took this concept of evolutionary signatures
[00:57:42.020 --> 00:57:45.740]   and applied it on the SARS-CoV-2 genome
[00:57:45.740 --> 00:57:48.540]   that is responsible for the COVID-19 pandemic.
[00:57:48.540 --> 00:57:50.500]   - And comparing it to?
[00:57:50.500 --> 00:57:53.700]   - To 44 Cervicovirus species, so this is the beta.
[00:57:53.700 --> 00:57:56.260]   - What word did you just use?
[00:57:56.260 --> 00:58:00.500]   - Cervicovirus, so SARS-related beta coronavirus.
[00:58:00.500 --> 00:58:01.460]   It's a portmanteau of a bunch.
[00:58:01.460 --> 00:58:03.100]   - So that whole family of viruses.
[00:58:03.100 --> 00:58:05.100]   - Yeah, so. - How big is that family?
[00:58:05.100 --> 00:58:07.500]   - We have 44 species that are--
[00:58:07.500 --> 00:58:09.340]   - 44 species in the family?
[00:58:09.340 --> 00:58:11.100]   - Yeah. - Virus is a clever bunch.
[00:58:11.100 --> 00:58:12.900]   - No, no, but there's just 44,
[00:58:12.900 --> 00:58:15.700]   and again, we don't call them species in viruses,
[00:58:15.700 --> 00:58:18.140]   we call them strains, but anyway, there's 44 strains,
[00:58:18.140 --> 00:58:22.340]   and that's a tiny little subset of maybe another 50 strains
[00:58:22.340 --> 00:58:24.460]   that are just far too distantly related.
[00:58:24.460 --> 00:58:29.080]   Most of those only infect bats as the host,
[00:58:29.080 --> 00:58:34.040]   and a subset of only four or five have ever infected humans.
[00:58:34.040 --> 00:58:36.800]   And we basically took all of those and we aligned them
[00:58:36.800 --> 00:58:39.000]   in the same exact way that we've aligned mammals,
[00:58:39.000 --> 00:58:42.360]   and then we looked at what proteins are,
[00:58:42.360 --> 00:58:45.000]   which of the currently hypothesized genes
[00:58:45.000 --> 00:58:49.040]   for the coronavirus genome are in fact evolving like proteins
[00:58:49.040 --> 00:58:50.280]   and which ones are not.
[00:58:50.280 --> 00:58:52.980]   And what we found is that ORF10,
[00:58:52.980 --> 00:58:54.680]   the last little open reading frame,
[00:58:54.680 --> 00:58:57.040]   the last little gene in the genome, is bogus.
[00:58:57.040 --> 00:58:58.760]   That's not a protein at all.
[00:58:58.760 --> 00:58:59.960]   - What is it?
[00:58:59.960 --> 00:59:01.840]   - It's an RNA structure.
[00:59:01.840 --> 00:59:03.600]   - That doesn't have a--
[00:59:03.600 --> 00:59:05.720]   - It doesn't get translated into amino acids.
[00:59:05.720 --> 00:59:08.300]   - And that, so it's important to narrow down
[00:59:08.300 --> 00:59:10.840]   to basically discover what's useful and what's not.
[00:59:10.840 --> 00:59:13.640]   - Exactly, basically what is even the set of genes?
[00:59:13.640 --> 00:59:15.560]   The other thing that this evolutionary signature showed
[00:59:15.560 --> 00:59:20.560]   is that within ORF3A lies a tiny little additional gene
[00:59:20.560 --> 00:59:22.740]   encoded within the other gene.
[00:59:22.740 --> 00:59:24.560]   So you can translate a DNA sequence
[00:59:24.560 --> 00:59:26.880]   in three different reading frames.
[00:59:26.880 --> 00:59:30.160]   If you start in the first one, it's ATG, et cetera.
[00:59:30.160 --> 00:59:33.000]   If you start on the second one, it's TGC, et cetera.
[00:59:33.000 --> 00:59:36.680]   And there's a gene within a gene.
[00:59:36.680 --> 00:59:39.480]   So there's a whole other protein that we didn't know about
[00:59:39.480 --> 00:59:41.200]   that might be super important.
[00:59:41.200 --> 00:59:45.680]   So we don't even know the building blocks of SARS-CoV-2.
[00:59:45.680 --> 00:59:48.340]   So if we want to understand coronavirus biology
[00:59:48.340 --> 00:59:50.560]   and eventually fight it successfully,
[00:59:50.560 --> 00:59:51.960]   we need to even have the set of genes.
[00:59:51.960 --> 00:59:53.720]   And these evolutionary signatures
[00:59:53.720 --> 00:59:55.600]   that I developed in my PhD work--
[00:59:55.600 --> 00:59:56.440]   - Are you really useful here?
[00:59:56.440 --> 00:59:57.280]   - Are really useful here.
[00:59:57.280 --> 00:59:58.100]   - Recently used.
[00:59:58.100 --> 00:59:59.560]   - You know what, let's run with that tangent
[00:59:59.560 --> 01:00:01.160]   for a little bit, if it's okay.
[01:00:01.160 --> 01:00:08.280]   Can we talk about the COVID-19 a little bit more?
[01:00:08.280 --> 01:00:13.160]   What's your sense about the genome, the proteins,
[01:00:13.160 --> 01:00:16.320]   the functions that we understand about COVID-19?
[01:00:16.320 --> 01:00:18.880]   Where do we stand in your sense?
[01:00:18.880 --> 01:00:21.400]   What are the big open problems?
[01:00:21.400 --> 01:00:25.340]   And also, you kind of said it's important to understand
[01:00:25.340 --> 01:00:29.800]   what are the important proteins,
[01:00:29.800 --> 01:00:32.120]   and why is that important?
[01:00:32.120 --> 01:00:39.120]   - So what else does the comparison of these species tell us?
[01:00:39.120 --> 01:00:43.000]   What it tells us is how fast are things evolving.
[01:00:43.000 --> 01:00:46.640]   It tells us about at what level is the acceleration
[01:00:46.640 --> 01:00:50.800]   or deceleration pedal set for every one of these proteins.
[01:00:50.800 --> 01:00:53.220]   So the genome has 30-some genes.
[01:00:54.120 --> 01:00:56.580]   Some genes evolve super, super fast.
[01:00:56.580 --> 01:00:59.020]   Others evolve super, super slow.
[01:00:59.020 --> 01:01:00.460]   If you look at the polymerase gene
[01:01:00.460 --> 01:01:01.980]   that basically replicates the genome,
[01:01:01.980 --> 01:01:04.220]   that's a super slow evolving one.
[01:01:04.220 --> 01:01:06.340]   If you look at the nucleocapsid protein,
[01:01:06.340 --> 01:01:08.360]   that's also super slow evolving.
[01:01:08.360 --> 01:01:11.460]   If you look at the spike one protein,
[01:01:11.460 --> 01:01:13.380]   this is the part of the spike protein
[01:01:13.380 --> 01:01:15.740]   that actually touches the H2 receptor
[01:01:15.740 --> 01:01:20.740]   and then enables the virus to attach to your cells.
[01:01:20.740 --> 01:01:23.820]   - That's the thing that gives it that visual--
[01:01:23.820 --> 01:01:24.920]   - Yeah, the corona look, basically.
[01:01:24.920 --> 01:01:26.040]   - The corona look, yeah.
[01:01:26.040 --> 01:01:28.560]   - So basically, the spike protein sticks out of the virus,
[01:01:28.560 --> 01:01:31.200]   and there's a first part of the protein, S1,
[01:01:31.200 --> 01:01:34.560]   which basically attaches to the H2 receptor,
[01:01:34.560 --> 01:01:39.520]   and then S2 is the latch that sort of pushes and channels
[01:01:39.520 --> 01:01:42.400]   the fusion of the membranes and then the incorporation
[01:01:42.400 --> 01:01:47.120]   of the viral RNA inside our cells,
[01:01:47.120 --> 01:01:50.520]   which then gets translated into all of these 30 proteins.
[01:01:50.520 --> 01:01:55.520]   - So the S1 protein is evolving ridiculously fast.
[01:01:55.520 --> 01:02:00.540]   So if you look at the stop, there's this gas pedal.
[01:02:00.540 --> 01:02:02.240]   The gas pedal is all the way down.
[01:02:02.240 --> 01:02:06.180]   Orf8 is also evolving super fast,
[01:02:06.180 --> 01:02:07.640]   and Orf6 is evolving super fast.
[01:02:07.640 --> 01:02:09.000]   We have no idea what they do.
[01:02:09.000 --> 01:02:12.300]   We have some idea, but nowhere near what S1 is.
[01:02:12.300 --> 01:02:13.140]   So what the--
[01:02:13.140 --> 01:02:15.220]   - Isn't that terrifying that S1 is evolving?
[01:02:15.220 --> 01:02:17.940]   That means that's a really useful function,
[01:02:17.940 --> 01:02:19.780]   and if it's evolving fast,
[01:02:19.780 --> 01:02:21.740]   doesn't that mean new strains could be created,
[01:02:21.740 --> 01:02:22.580]   or it does something?
[01:02:22.580 --> 01:02:25.220]   - That means that it's searching for how to match,
[01:02:25.220 --> 01:02:26.920]   how to best match the host.
[01:02:26.920 --> 01:02:29.460]   So basically, anything, in general, in evolution,
[01:02:29.460 --> 01:02:30.300]   if you look at genomes,
[01:02:30.300 --> 01:02:32.500]   anything that's contacting the environment
[01:02:32.500 --> 01:02:35.220]   is evolving much faster than anything that's internal,
[01:02:35.220 --> 01:02:37.340]   and the reason is that the environment changes.
[01:02:37.340 --> 01:02:42.340]   So if you look at the evolution of these Cerbicoviruses,
[01:02:42.340 --> 01:02:44.620]   the S1 protein has evolved very rapidly
[01:02:44.620 --> 01:02:47.620]   because it's attaching to different hosts each time.
[01:02:47.620 --> 01:02:48.720]   We think of them as bats,
[01:02:48.720 --> 01:02:50.820]   but there's thousands of species of bats,
[01:02:50.820 --> 01:02:53.100]   and to go from one species of bat to another species of bat,
[01:02:53.100 --> 01:02:56.180]   you have to adjust S1 to the new ACE2 receptor
[01:02:56.180 --> 01:02:58.300]   that you're gonna be facing in that new species.
[01:02:58.300 --> 01:02:59.740]   - Sorry, quick tangent.
[01:02:59.740 --> 01:03:00.580]   - Yeah.
[01:03:00.580 --> 01:03:03.740]   - Is it fascinating to you that viruses are doing this?
[01:03:03.740 --> 01:03:07.100]   I mean, it feels like they're this intelligent organism.
[01:03:07.100 --> 01:03:10.460]   I mean, is it, like, does it give you pause
[01:03:10.460 --> 01:03:14.060]   how incredible it is that they are,
[01:03:14.060 --> 01:03:16.700]   that the evolutionary dynamics that you're describing
[01:03:16.700 --> 01:03:18.100]   is actually happening,
[01:03:18.100 --> 01:03:22.340]   and they're figuring out how to jump from bats to humans
[01:03:22.340 --> 01:03:24.380]   all in this distributed fashion,
[01:03:24.380 --> 01:03:25.820]   and then most of us don't even say
[01:03:25.820 --> 01:03:27.780]   they're alive or intelligent or whatever?
[01:03:27.780 --> 01:03:31.260]   - So intelligence is in the eye of the beholder.
[01:03:31.260 --> 01:03:33.460]   You know, stupid is as stupid does,
[01:03:33.460 --> 01:03:35.140]   as Forrest Gump would say,
[01:03:35.140 --> 01:03:36.780]   and intelligent is as intelligent does.
[01:03:36.780 --> 01:03:39.420]   So basically, if the virus is finding solutions
[01:03:39.420 --> 01:03:41.100]   that we think of as intelligent,
[01:03:41.100 --> 01:03:42.460]   yeah, it's probably intelligent,
[01:03:42.460 --> 01:03:44.140]   but that's, again, in the eye of the beholder.
[01:03:44.140 --> 01:03:45.900]   - Do you think viruses are intelligent?
[01:03:45.900 --> 01:03:47.500]   - Oh, of course not.
[01:03:47.500 --> 01:03:48.340]   - Really?
[01:03:48.340 --> 01:03:49.180]   - No.
[01:03:49.180 --> 01:03:50.380]   - It's so incredible.
[01:03:50.380 --> 01:03:51.940]   - So remember when I was talking
[01:03:51.940 --> 01:03:53.540]   about the two components of evolution?
[01:03:53.540 --> 01:03:57.100]   One is the stupid mutation, which is completely blind,
[01:03:57.100 --> 01:04:00.300]   and the other one is the super smart selection,
[01:04:00.300 --> 01:04:01.860]   which is ruthless.
[01:04:01.860 --> 01:04:04.820]   So it's not viruses who are smart.
[01:04:04.820 --> 01:04:06.880]   It's this component of evolution that's smart.
[01:04:06.880 --> 01:04:10.380]   So it's evolution that sort of appears smart.
[01:04:10.380 --> 01:04:12.060]   And how is that happening?
[01:04:12.060 --> 01:04:15.500]   By huge parallel search
[01:04:15.500 --> 01:04:18.820]   across thousands of, you know,
[01:04:18.820 --> 01:04:21.700]   parallel infections throughout the world right now.
[01:04:21.700 --> 01:04:23.980]   - Yes, but so to push back on that,
[01:04:23.980 --> 01:04:27.980]   so yes, so then the intelligence is in the mechanism.
[01:04:27.980 --> 01:04:31.380]   But then by that argument,
[01:04:31.380 --> 01:04:32.860]   viruses would be more intelligent
[01:04:32.860 --> 01:04:34.660]   because there's just more of them.
[01:04:34.660 --> 01:04:38.740]   So the search, they're basically the brute force search
[01:04:38.740 --> 01:04:40.340]   that's happening with viruses,
[01:04:40.340 --> 01:04:43.220]   because there's so many more of them than humans,
[01:04:43.220 --> 01:04:47.540]   then they're taken as a whole are more intelligent.
[01:04:47.540 --> 01:04:50.740]   I mean, so you don't think it's possible that,
[01:04:50.740 --> 01:04:55.580]   I mean, who runs, would we even be here if viruses weren't?
[01:04:55.580 --> 01:04:58.340]   I mean, who runs this thing?
[01:04:58.340 --> 01:05:03.040]   - So let me answer, yeah, let me answer your question.
[01:05:03.040 --> 01:05:08.040]   So we would not be here if it wasn't for viruses.
[01:05:08.040 --> 01:05:11.820]   And part of the reason is that
[01:05:11.820 --> 01:05:14.340]   if you look at mammalian evolution early on
[01:05:14.340 --> 01:05:16.100]   in this mammalian radiation
[01:05:16.100 --> 01:05:18.580]   that basically happened after the death of the dinosaurs,
[01:05:18.580 --> 01:05:22.740]   is that some of the viruses that we had in our genome
[01:05:22.740 --> 01:05:24.580]   spread throughout our genome
[01:05:24.580 --> 01:05:27.260]   and created binding sites
[01:05:27.260 --> 01:05:30.340]   for new classes of regulatory proteins.
[01:05:30.340 --> 01:05:33.340]   And these binding sites that landed all over our genome
[01:05:33.340 --> 01:05:36.860]   are now control elements that basically control our genes
[01:05:36.860 --> 01:05:40.420]   and sort of help the complexity of the circuitry
[01:05:40.420 --> 01:05:42.220]   of mammalian genomes.
[01:05:42.220 --> 01:05:45.100]   So, you know, everything's co-evolution.
[01:05:45.100 --> 01:05:47.780]   - That's fascinating, we're working together.
[01:05:47.780 --> 01:05:49.620]   And yet you say they're dumb. - We've co-opted them.
[01:05:49.620 --> 01:05:51.300]   No, I never said they're dumb.
[01:05:51.300 --> 01:05:53.620]   They just don't care.
[01:05:53.620 --> 01:05:54.700]   They don't care.
[01:05:54.700 --> 01:05:56.940]   Another thing, oh, is the virus trying to kill us?
[01:05:56.940 --> 01:05:58.020]   No, it's not.
[01:05:58.020 --> 01:05:59.940]   The virus is not trying to kill you.
[01:05:59.940 --> 01:06:02.820]   It's actually actively trying to not kill you.
[01:06:02.820 --> 01:06:04.940]   So when you get infected, if you die,
[01:06:04.940 --> 01:06:07.300]   Pomeroy killed him,
[01:06:07.300 --> 01:06:09.140]   is what the reaction of the virus will be.
[01:06:09.140 --> 01:06:11.020]   Why? Because that virus won't spread.
[01:06:11.020 --> 01:06:13.740]   Many people have a misconception of,
[01:06:13.740 --> 01:06:16.740]   oh, viruses are smart or, oh, viruses are mean.
[01:06:16.740 --> 01:06:17.580]   They don't care.
[01:06:17.580 --> 01:06:20.740]   It's like you have to clean yourself
[01:06:20.740 --> 01:06:23.220]   of any kind of anthropomorphism out there.
[01:06:23.220 --> 01:06:25.420]   - I don't know. - Oh, yes.
[01:06:25.420 --> 01:06:29.660]   - So there's a sense when taken as a whole that there's a...
[01:06:29.660 --> 01:06:32.940]   - It's in the eye of the beholder.
[01:06:32.940 --> 01:06:34.140]   Stupid is as stupid does,
[01:06:34.140 --> 01:06:35.900]   intelligent is as intelligent does.
[01:06:35.900 --> 01:06:38.420]   So if you wanna call them intelligent, that's fine.
[01:06:38.420 --> 01:06:40.140]   Because the end result
[01:06:40.140 --> 01:06:42.700]   is that they're finding amazing solutions.
[01:06:42.700 --> 01:06:44.300]   I mean, I am in awe.
[01:06:44.300 --> 01:06:45.620]   - They're so dumb about it.
[01:06:45.620 --> 01:06:46.980]   They're just doing dumb. - They don't care.
[01:06:46.980 --> 01:06:48.300]   They're not dumb and they're not...
[01:06:48.300 --> 01:06:50.060]   They just don't care. - They don't care.
[01:06:50.060 --> 01:06:51.820]   The care word is really interesting.
[01:06:51.820 --> 01:06:54.380]   I mean, there could be an argument that they're conscious.
[01:06:54.380 --> 01:06:55.500]   - They're just dividing.
[01:06:55.500 --> 01:06:57.660]   They're not, they're just dividing.
[01:06:57.660 --> 01:06:59.220]   They're just a little entity
[01:06:59.220 --> 01:07:02.700]   which happens to be dividing and spreading.
[01:07:02.700 --> 01:07:04.460]   It doesn't want to kill us.
[01:07:04.460 --> 01:07:06.380]   In fact, it prefers not to kill us.
[01:07:06.380 --> 01:07:07.740]   It just wants to spread.
[01:07:07.740 --> 01:07:11.060]   And when I say wants, again, I'm anthropomorphizing,
[01:07:11.060 --> 01:07:15.100]   but it's just that if you have two versions of a virus,
[01:07:15.100 --> 01:07:17.460]   one acquires a mutation that spreads more,
[01:07:17.460 --> 01:07:18.660]   that's gonna spread more.
[01:07:18.660 --> 01:07:20.300]   One acquires a mutation that spreads less,
[01:07:20.300 --> 01:07:21.820]   that's gonna be lost.
[01:07:21.820 --> 01:07:24.060]   One acquires a mutation that enters faster,
[01:07:24.060 --> 01:07:25.140]   that's gonna be kept.
[01:07:25.140 --> 01:07:27.060]   One acquires a mutation that kills you right away,
[01:07:27.060 --> 01:07:28.460]   it's gonna be lost.
[01:07:28.460 --> 01:07:30.540]   So over evolutionary time,
[01:07:30.540 --> 01:07:32.740]   the viruses that spread super well,
[01:07:32.740 --> 01:07:33.980]   but don't kill the host,
[01:07:33.980 --> 01:07:36.300]   are the ones that are gonna survive.
[01:07:36.300 --> 01:07:39.100]   - Yeah, but so you brilliantly described
[01:07:39.100 --> 01:07:41.100]   the basic mechanisms of how it all happens,
[01:07:41.100 --> 01:07:43.380]   but when you zoom out and you see the,
[01:07:43.380 --> 01:07:46.500]   you know, the entirety of viruses,
[01:07:46.500 --> 01:07:49.980]   maybe across different strains of viruses,
[01:07:49.980 --> 01:07:52.380]   it seems like a living organism.
[01:07:52.380 --> 01:07:55.020]   - I am in awe of biology.
[01:07:55.020 --> 01:07:58.380]   I find biology amazingly beautiful.
[01:07:58.380 --> 01:08:01.100]   I find the design of the current coronavirus,
[01:08:01.100 --> 01:08:04.260]   however lethal it is, amazingly beautiful.
[01:08:04.260 --> 01:08:06.340]   The way that it is encoded,
[01:08:06.340 --> 01:08:08.980]   the way that it tricks your cells
[01:08:08.980 --> 01:08:12.340]   into making 30 proteins from a single RNA.
[01:08:12.340 --> 01:08:14.300]   Human cells don't do that.
[01:08:14.300 --> 01:08:18.180]   Human cells make one protein from each RNA molecule.
[01:08:18.180 --> 01:08:20.220]   They don't make two, they make one.
[01:08:20.220 --> 01:08:22.300]   We are hardwired to make only one protein
[01:08:22.300 --> 01:08:23.780]   from every RNA molecule.
[01:08:23.780 --> 01:08:25.700]   And yet this virus goes in,
[01:08:25.700 --> 01:08:27.620]   throws in a single messenger RNA.
[01:08:27.620 --> 01:08:29.940]   Just like any messenger RNA,
[01:08:29.940 --> 01:08:32.140]   we have tens of thousands of messenger RNAs
[01:08:32.140 --> 01:08:34.140]   in our cells in any one time.
[01:08:34.140 --> 01:08:35.820]   In every one of our cells.
[01:08:35.820 --> 01:08:40.620]   It throws in one RNA and that RNA is so,
[01:08:40.620 --> 01:08:42.620]   I'm gonna use your word here, not my word,
[01:08:42.620 --> 01:08:46.660]   intelligent that it hijacks the entire machinery
[01:08:46.660 --> 01:08:49.300]   of your human cell.
[01:08:49.300 --> 01:08:52.460]   It basically has at the beginning
[01:08:52.460 --> 01:08:54.460]   a giant open reading frame.
[01:08:54.460 --> 01:08:57.140]   That's a giant protein that gets translated.
[01:08:57.140 --> 01:09:01.540]   Two thirds of that RNA make a single giant protein.
[01:09:01.540 --> 01:09:03.340]   That single protein is basically
[01:09:03.340 --> 01:09:04.540]   what a human cell would make.
[01:09:04.540 --> 01:09:06.300]   It's like, oh, here's a start codon.
[01:09:06.300 --> 01:09:07.660]   I'm gonna start translating here.
[01:09:07.660 --> 01:09:08.980]   Human cells are kind of dumb, I'm sorry.
[01:09:08.980 --> 01:09:12.380]   Again, this is not the word that we'd normally use.
[01:09:12.380 --> 01:09:13.380]   But the human cell basically is,
[01:09:13.380 --> 01:09:15.900]   oh, this is an RNA, must be mine, let me translate.
[01:09:15.900 --> 01:09:18.420]   And it starts translating it and then you're in trouble.
[01:09:18.420 --> 01:09:19.260]   Why?
[01:09:19.260 --> 01:09:22.300]   Because that one protein, as it's growing,
[01:09:22.300 --> 01:09:26.940]   gets cleaved into about 20 different peptides.
[01:09:26.940 --> 01:09:30.900]   The first peptide and the second peptide start interacting
[01:09:30.900 --> 01:09:32.700]   and the third one and the fourth one.
[01:09:32.700 --> 01:09:37.700]   And they shut off the ribosome of the whole cell
[01:09:37.700 --> 01:09:42.780]   to not translate human RNAs anymore.
[01:09:42.780 --> 01:09:46.540]   So the virus basically hijacks your cells
[01:09:46.540 --> 01:09:50.740]   and it cuts, it cleaves every one of your human RNAs
[01:09:50.740 --> 01:09:52.060]   to basically say to the ribosome,
[01:09:52.060 --> 01:09:53.460]   don't translate this one, junk.
[01:09:53.460 --> 01:09:55.100]   Don't look at this one, junk.
[01:09:55.100 --> 01:09:58.660]   And it only spares its own RNAs
[01:09:58.660 --> 01:10:01.220]   because they have a particular mark that it spares.
[01:10:01.220 --> 01:10:04.300]   Then all of the ribosomes that normally make protein
[01:10:04.300 --> 01:10:06.780]   in your human cells are now only able
[01:10:06.780 --> 01:10:09.180]   to translate viral RNAs.
[01:10:09.180 --> 01:10:11.380]   They have more and more and more and more of them.
[01:10:11.380 --> 01:10:13.020]   That's the first 20 proteins.
[01:10:13.020 --> 01:10:16.220]   In fact, halfway down about protein 11,
[01:10:16.220 --> 01:10:17.860]   between 11 and 12,
[01:10:17.860 --> 01:10:20.940]   you basically have a translational slippage
[01:10:20.940 --> 01:10:23.340]   where the ribosome skips reading frame
[01:10:23.340 --> 01:10:24.900]   and it translates from one reading frame
[01:10:24.900 --> 01:10:25.740]   to another reading frame.
[01:10:25.740 --> 01:10:27.020]   That means that about half of them
[01:10:27.020 --> 01:10:29.220]   are gonna be translated from one to 11
[01:10:29.220 --> 01:10:32.700]   and the other half are gonna be translated from 12 to 16.
[01:10:32.700 --> 01:10:34.260]   It's gorgeous.
[01:10:34.260 --> 01:10:37.380]   And then, then you're done.
[01:10:37.380 --> 01:10:40.420]   Then that mRNA will never translate the last 10 proteins,
[01:10:40.420 --> 01:10:42.540]   but spike is the one right after that one.
[01:10:42.540 --> 01:10:45.140]   So how does spike even get translated?
[01:10:45.140 --> 01:10:50.020]   This positive strand RNA virus has a reverse transcriptase,
[01:10:50.020 --> 01:10:52.340]   which is an RNA-based reverse transcriptase.
[01:10:52.340 --> 01:10:54.460]   So from the RNA on the positive strand,
[01:10:54.460 --> 01:10:56.940]   it makes an RNA on the negative strand.
[01:10:56.940 --> 01:10:59.620]   And in between every single one of these genes,
[01:10:59.620 --> 01:11:01.060]   these open reading frames,
[01:11:01.060 --> 01:11:05.580]   there's a little signal, AACGCA or something like that,
[01:11:05.580 --> 01:11:09.940]   that basically loops over to the beginning of the RNA.
[01:11:09.940 --> 01:11:11.700]   And basically, instead of sort of having
[01:11:11.700 --> 01:11:14.500]   a single full negative strand RNA,
[01:11:14.500 --> 01:11:16.820]   it basically has a partial negative strand RNA
[01:11:16.820 --> 01:11:19.700]   that ends right before the beginning of that gene.
[01:11:19.700 --> 01:11:20.860]   And another one that ends right before
[01:11:20.860 --> 01:11:21.980]   the beginning of that gene.
[01:11:21.980 --> 01:11:25.340]   These negative strand RNAs now make positive strand RNAs
[01:11:25.340 --> 01:11:27.460]   that then loop to the human host cell,
[01:11:27.460 --> 01:11:29.740]   just like any other human mRNA.
[01:11:29.740 --> 01:11:31.740]   It's like, "Oh, great, I'm gonna translate that one."
[01:11:31.740 --> 01:11:32.980]   'Cause it doesn't have the cleaving
[01:11:32.980 --> 01:11:36.300]   that the virus has now put on all your human genes.
[01:11:36.300 --> 01:11:38.300]   And then you've lost the battle.
[01:11:38.300 --> 01:11:42.500]   That cell is now only making proteins for the virus
[01:11:42.500 --> 01:11:45.500]   that will then create the spike protein,
[01:11:45.500 --> 01:11:47.620]   the envelope protein, the membrane protein,
[01:11:47.620 --> 01:11:50.300]   the nucleocapsid protein that will package up the RNA,
[01:11:50.300 --> 01:11:53.820]   and then sort of create new viral envelopes.
[01:11:53.820 --> 01:11:57.780]   And these will then be secreted out of that cell
[01:11:57.780 --> 01:11:59.260]   in new little packages
[01:11:59.260 --> 01:12:00.580]   that will then infect the rest of the cells.
[01:12:00.580 --> 01:12:01.900]   - Repeat the whole process again.
[01:12:01.900 --> 01:12:03.300]   - It's beautiful, right?
[01:12:03.300 --> 01:12:04.140]   It's mind-boggling.
[01:12:04.140 --> 01:12:05.660]   - It's hard not to anthropomorphize it.
[01:12:05.660 --> 01:12:06.500]   (laughing)
[01:12:06.500 --> 01:12:08.100]   - I know, but it's so gorgeous.
[01:12:08.100 --> 01:12:09.900]   - So there is a beauty to it.
[01:12:09.900 --> 01:12:10.740]   - Of course.
[01:12:10.740 --> 01:12:13.980]   - Is it terrifying to you?
[01:12:13.980 --> 01:12:16.900]   - So this is something that has happened throughout history.
[01:12:16.900 --> 01:12:19.700]   Humans have been nearly wiped out
[01:12:19.700 --> 01:12:21.140]   over and over and over again,
[01:12:21.140 --> 01:12:23.260]   and yet never fully wiped out.
[01:12:23.260 --> 01:12:25.940]   So I'm not concerned about the human race.
[01:12:25.940 --> 01:12:29.340]   I'm not even concerned about the impact
[01:12:29.340 --> 01:12:32.460]   on sort of our survival as a species.
[01:12:32.460 --> 01:12:35.620]   This is absolutely something,
[01:12:35.620 --> 01:12:38.660]   I mean, human life is so invaluable,
[01:12:38.660 --> 01:12:40.060]   and every one of us is so invaluable,
[01:12:40.060 --> 01:12:42.260]   but if you think of it as sort of,
[01:12:42.260 --> 01:12:44.100]   is this the end of our species?
[01:12:44.100 --> 01:12:46.420]   By no means, basically.
[01:12:46.420 --> 01:12:48.100]   So let me explain.
[01:12:48.100 --> 01:12:51.260]   The Black Death killed what, 30% of Europe?
[01:12:52.220 --> 01:12:55.780]   That has left a tremendous imprint,
[01:12:55.780 --> 01:13:00.300]   a huge hole, a horrendous hole
[01:13:00.300 --> 01:13:04.540]   in the genetic makeup of humans.
[01:13:04.540 --> 01:13:07.580]   There's been series of wiping out
[01:13:07.580 --> 01:13:10.780]   of huge fractions of entire species,
[01:13:10.780 --> 01:13:13.100]   or just entire species altogether,
[01:13:13.100 --> 01:13:18.100]   and that has a consequence on the human immune repertoire.
[01:13:19.580 --> 01:13:23.300]   If you look at how Europe was shaped,
[01:13:23.300 --> 01:13:27.220]   and how Africa was shaped by malaria, for example,
[01:13:27.220 --> 01:13:29.460]   all the individuals that carry a mutation
[01:13:29.460 --> 01:13:31.820]   that protects you from malaria
[01:13:31.820 --> 01:13:33.660]   were able to survive much more.
[01:13:33.660 --> 01:13:36.420]   And if you look at the frequency of sickle cell disease
[01:13:36.420 --> 01:13:38.180]   and the frequency of malaria,
[01:13:38.180 --> 01:13:40.860]   the maps are actually showing the same pattern,
[01:13:40.860 --> 01:13:42.860]   the same imprint on Africa,
[01:13:42.860 --> 01:13:44.820]   and that basically led people to hypothesize
[01:13:44.820 --> 01:13:46.300]   that the reason why sickle cell disease
[01:13:46.300 --> 01:13:50.100]   is so much more frequent in Americans of African descent
[01:13:50.100 --> 01:13:54.100]   is because there was selection in Africa against malaria,
[01:13:54.100 --> 01:13:57.860]   leading to sickle cell, because when the cells sickle,
[01:13:57.860 --> 01:14:01.420]   malaria is not able to replicate inside your cells as well,
[01:14:01.420 --> 01:14:03.140]   and therefore you protect against that.
[01:14:03.140 --> 01:14:05.420]   So if you look at human disease,
[01:14:05.420 --> 01:14:06.900]   all of the genetic associations
[01:14:06.900 --> 01:14:09.260]   that we do with human disease,
[01:14:09.260 --> 01:14:13.700]   you basically see the imprint
[01:14:13.700 --> 01:14:15.700]   of these waves of selection,
[01:14:15.700 --> 01:14:18.380]   killing off gazillions of humans.
[01:14:18.380 --> 01:14:23.260]   And there's so many immune processes that are coming up
[01:14:23.260 --> 01:14:25.940]   as associated with so many different diseases.
[01:14:25.940 --> 01:14:27.580]   The reason for that is similar
[01:14:27.580 --> 01:14:28.700]   to what I was describing earlier,
[01:14:28.700 --> 01:14:33.700]   where the outward facing proteins evolve much more rapidly
[01:14:33.700 --> 01:14:35.940]   because the environment is always changing.
[01:14:35.940 --> 01:14:37.660]   But what's really interesting, the human genome
[01:14:37.660 --> 01:14:40.380]   is that we have co-opted many of these immune genes
[01:14:40.380 --> 01:14:42.460]   to carry out non-immune functions.
[01:14:42.460 --> 01:14:44.020]   For example, in our brain,
[01:14:44.020 --> 01:14:48.900]   we use immune cells to cleave off neuronal connections
[01:14:48.900 --> 01:14:50.180]   that don't get used.
[01:14:50.180 --> 01:14:52.860]   This whole use it or lose it, we know the mechanism.
[01:14:52.860 --> 01:14:54.620]   It's microglia that cleave off
[01:14:54.620 --> 01:14:59.940]   neuronal synaptic connections that are just not utilized.
[01:14:59.940 --> 01:15:02.060]   When you utilize them, you mark them in a particular way
[01:15:02.060 --> 01:15:04.420]   that basically when the microglia come,
[01:15:04.420 --> 01:15:07.860]   tell it, "Don't kill this one, it's used now."
[01:15:07.860 --> 01:15:08.980]   And the microglia will go off
[01:15:08.980 --> 01:15:10.380]   and kill the ones you don't use.
[01:15:10.380 --> 01:15:12.820]   This is an immune function,
[01:15:12.820 --> 01:15:15.020]   which is co-opted to do non-immune things.
[01:15:15.020 --> 01:15:16.820]   If you look at our adipocytes,
[01:15:16.820 --> 01:15:19.940]   M1 versus M2 macrophages inside our fat
[01:15:19.940 --> 01:15:22.660]   will basically determine whether you're obese or not.
[01:15:22.660 --> 01:15:24.780]   And these are again immune cells that are resident
[01:15:24.780 --> 01:15:27.100]   and living within these tissues.
[01:15:27.100 --> 01:15:30.260]   So many disease association.
[01:15:30.260 --> 01:15:33.700]   - Fascinating that we co-opt these kinds of things
[01:15:33.700 --> 01:15:36.700]   for incredibly complicated functions.
[01:15:36.700 --> 01:15:39.900]   - Exactly, evolution works in so many different ways,
[01:15:39.900 --> 01:15:42.020]   which are all beautiful and mysterious.
[01:15:42.020 --> 01:15:43.300]   - But not intelligent.
[01:15:43.300 --> 01:15:45.780]   - Not intelligent, it's in the eye of the beholder.
[01:15:45.780 --> 01:15:47.580]   (both laughing)
[01:15:47.580 --> 01:15:50.340]   But the point that I'm trying to make
[01:15:50.340 --> 01:15:54.260]   is that if you look at the imprint that COVID will have,
[01:15:54.260 --> 01:15:55.980]   hopefully it will not be big.
[01:15:55.980 --> 01:15:57.980]   Hopefully the US will get attacked together
[01:15:57.980 --> 01:16:00.420]   and stop the virus from spreading further.
[01:16:00.420 --> 01:16:03.500]   But if it doesn't, it's having an imprint
[01:16:03.500 --> 01:16:07.340]   on individuals who have particular genetic repertoires.
[01:16:07.340 --> 01:16:10.080]   So if you look at now the genetic associations
[01:16:10.080 --> 01:16:13.620]   of blood type and immune function cells, et cetera,
[01:16:13.620 --> 01:16:15.740]   there's actually association, genetic variation
[01:16:15.740 --> 01:16:18.540]   that basically says how much more likely am I or you to die
[01:16:18.540 --> 01:16:20.220]   if we contact the virus.
[01:16:20.220 --> 01:16:24.540]   And it's through these rounds of shaping the human genome
[01:16:24.540 --> 01:16:27.380]   that humans have basically made it so far.
[01:16:27.380 --> 01:16:32.380]   And selection is ruthless and it's brutal
[01:16:32.380 --> 01:16:34.380]   and it only comes with a lot of killing.
[01:16:34.380 --> 01:16:38.120]   But this is the way that viruses and environments
[01:16:38.120 --> 01:16:39.500]   have shaped the human genome.
[01:16:39.500 --> 01:16:41.420]   Basically when you go through periods of famine,
[01:16:41.420 --> 01:16:43.660]   you select for particular genes.
[01:16:43.660 --> 01:16:46.540]   And what's left is not necessarily better,
[01:16:46.540 --> 01:16:49.020]   it's just whatever survived.
[01:16:49.020 --> 01:16:51.980]   And it may have been the surviving one back then,
[01:16:51.980 --> 01:16:53.140]   not because it was better,
[01:16:53.140 --> 01:16:54.980]   maybe the ones that ran slower survived.
[01:16:54.980 --> 01:16:57.400]   I mean, again, not necessarily better,
[01:16:57.400 --> 01:17:00.020]   but the surviving ones are basically the ones
[01:17:00.020 --> 01:17:02.420]   that then are shaped for any kind
[01:17:02.420 --> 01:17:05.380]   of subsequent evolutionary condition
[01:17:05.380 --> 01:17:07.260]   and environmental condition.
[01:17:07.260 --> 01:17:09.580]   But if you look at, for example, obesity,
[01:17:09.580 --> 01:17:11.780]   obesity was selected for,
[01:17:11.780 --> 01:17:14.420]   basically the genes that now predisposes to obesity
[01:17:14.420 --> 01:17:16.660]   were at 2% frequency in Africa.
[01:17:16.660 --> 01:17:19.020]   They rose to 44% frequency in Europe.
[01:17:19.020 --> 01:17:20.300]   - Wow, that's fascinating.
[01:17:20.300 --> 01:17:22.940]   - Because you basically went through the ice ages
[01:17:22.940 --> 01:17:24.620]   and there was a scarcity of food.
[01:17:24.620 --> 01:17:27.140]   So there was a selection to being able to store
[01:17:27.140 --> 01:17:29.280]   every single calorie you consume.
[01:17:30.740 --> 01:17:33.360]   Eventually, environment changes.
[01:17:33.360 --> 01:17:36.560]   So the better allele, which was the fat storing allele,
[01:17:36.560 --> 01:17:38.060]   became the worst allele,
[01:17:38.060 --> 01:17:40.340]   because it's the fat storing allele.
[01:17:40.340 --> 01:17:42.500]   It still has the same function.
[01:17:42.500 --> 01:17:45.660]   So if you look at my genome, speaking of mom calling,
[01:17:45.660 --> 01:17:49.940]   mom gave me a bad copy of that gene, this FTO locus.
[01:17:49.940 --> 01:17:51.260]   Basically makes me-- - The one that has to do
[01:17:51.260 --> 01:17:52.540]   with the-- - Obesity.
[01:17:52.540 --> 01:17:53.460]   - With obesity.
[01:17:53.460 --> 01:17:56.000]   - Yeah, I basically now have a bad copy from mom
[01:17:56.000 --> 01:17:57.940]   that makes me more likely to be obese.
[01:17:57.940 --> 01:18:00.380]   And I also have a bad copy from dad
[01:18:00.380 --> 01:18:01.580]   that makes me more likely to be obese.
[01:18:01.580 --> 01:18:03.340]   So I'm homozygous.
[01:18:03.340 --> 01:18:07.420]   And that's the allele, it's still the minor allele,
[01:18:07.420 --> 01:18:10.660]   but it's at 44% frequency in Southeast Asia,
[01:18:10.660 --> 01:18:14.260]   42% frequency in Europe, even though it started at 2%.
[01:18:14.260 --> 01:18:17.560]   It was an awesome allele to have 100 years ago.
[01:18:17.560 --> 01:18:19.400]   Right now, it's a pretty terrible allele.
[01:18:19.400 --> 01:18:23.540]   So the other concept is that diversity matters.
[01:18:23.540 --> 01:18:26.780]   If we had 100 million nuclear physicists
[01:18:26.780 --> 01:18:29.300]   living on the earth right now, we'd be in trouble.
[01:18:29.900 --> 01:18:30.740]   (Zubin laughs)
[01:18:30.740 --> 01:18:32.860]   You need diversity, you need artists,
[01:18:32.860 --> 01:18:34.940]   and you need musicians, and you need mathematicians,
[01:18:34.940 --> 01:18:38.140]   and you need politicians, yes, even those.
[01:18:38.140 --> 01:18:40.140]   And you need-- - Oh, let's not get crazy.
[01:18:40.140 --> 01:18:40.980]   (Zubin laughs)
[01:18:40.980 --> 01:18:44.300]   But because then if a virus comes along or whatever--
[01:18:44.300 --> 01:18:45.900]   - Exactly, exactly.
[01:18:45.900 --> 01:18:47.020]   So no, there's two reasons.
[01:18:47.020 --> 01:18:49.900]   Number one, you want diversity in the immune repertoire,
[01:18:49.900 --> 01:18:51.940]   and we have built-in diversity.
[01:18:51.940 --> 01:18:54.420]   So basically, they are the most diverse.
[01:18:54.420 --> 01:18:55.780]   Basically, if you look at our immune system,
[01:18:55.780 --> 01:18:57.880]   there's layers and layers of diversity.
[01:18:57.880 --> 01:19:02.340]   Like the way that you create your cells generates diversity
[01:19:02.340 --> 01:19:05.680]   because of the selection for the VDJ recombination
[01:19:05.680 --> 01:19:07.360]   that basically eventually leads
[01:19:07.360 --> 01:19:08.940]   to a huge number of repertoires.
[01:19:08.940 --> 01:19:10.980]   But that's only one small component of diversity.
[01:19:10.980 --> 01:19:12.340]   The blot type is another one.
[01:19:12.340 --> 01:19:16.620]   The major histocompatibility complex, the HLA alleles,
[01:19:16.620 --> 01:19:18.760]   are another source of diversity.
[01:19:18.760 --> 01:19:21.080]   So the immune system of humans
[01:19:21.080 --> 01:19:24.140]   is by nature incredibly diverse.
[01:19:24.140 --> 01:19:26.600]   And that basically leads to resilience.
[01:19:26.600 --> 01:19:28.420]   So basically, what I'm saying that I don't worry
[01:19:28.420 --> 01:19:30.400]   for the human species.
[01:19:30.400 --> 01:19:33.580]   Because we are so diverse immunologically,
[01:19:33.580 --> 01:19:35.940]   we are likely to be very resilient
[01:19:35.940 --> 01:19:40.020]   against so many different attacks like this current virus.
[01:19:40.020 --> 01:19:43.100]   - So you're saying natural pandemics may not be something
[01:19:43.100 --> 01:19:45.940]   that you're really afraid of because of the diversity
[01:19:45.940 --> 01:19:49.420]   in our genetic makeup.
[01:19:49.420 --> 01:19:51.420]   What about engineered pandemics?
[01:19:51.420 --> 01:19:56.420]   Do you have fears of us messing with the makeup of viruses
[01:19:56.680 --> 01:19:59.880]   or, well, yeah, let's say with the makeup of viruses
[01:19:59.880 --> 01:20:01.800]   to create something that we can't control
[01:20:01.800 --> 01:20:03.760]   and would be much more destructive
[01:20:03.760 --> 01:20:06.280]   than it would come about naturally?
[01:20:06.280 --> 01:20:08.960]   - Remember how we were talking about how smart evolution is?
[01:20:08.960 --> 01:20:10.160]   Humans are much dumber.
[01:20:10.160 --> 01:20:12.680]   - You mean like human scientists, engineers?
[01:20:12.680 --> 01:20:14.240]   - Yeah, humans, humans just like--
[01:20:14.240 --> 01:20:15.080]   - Humans overall?
[01:20:15.080 --> 01:20:17.080]   - Yeah, humans overall.
[01:20:17.080 --> 01:20:21.540]   But I mean, even the sort of synthetic biologists.
[01:20:21.540 --> 01:20:25.240]   Basically, if you were to create
[01:20:26.100 --> 01:20:30.900]   a virus like SARS that will kill a lot of people,
[01:20:30.900 --> 01:20:33.200]   you would probably start with SARS.
[01:20:33.200 --> 01:20:38.620]   So whoever would like to design such a thing
[01:20:38.620 --> 01:20:41.180]   would basically start with a SARS tree
[01:20:41.180 --> 01:20:43.540]   or at least some relative of SARS.
[01:20:43.540 --> 01:20:46.720]   The source genome for the current virus
[01:20:46.720 --> 01:20:48.300]   was something completely different.
[01:20:48.300 --> 01:20:50.600]   It was something that has never infected humans.
[01:20:50.600 --> 01:20:52.860]   No one in their right mind would have started there.
[01:20:52.860 --> 01:20:54.980]   - But when you say source, it's like the nearest--
[01:20:54.980 --> 01:20:58.700]   - The nearest relative is in a whole other branch,
[01:20:58.700 --> 01:21:01.700]   no species of which has ever infected humans in that branch.
[01:21:01.700 --> 01:21:05.300]   So let's put this to rest.
[01:21:05.300 --> 01:21:08.540]   This was not designed by someone to kill off the human race.
[01:21:08.540 --> 01:21:10.900]   - So you don't believe it was engineered?
[01:21:10.900 --> 01:21:13.060]   - The-- - Well, likely.
[01:21:13.060 --> 01:21:16.080]   - Yeah, the path to engineering a deadly virus
[01:21:16.080 --> 01:21:21.080]   would not come from this strain that was used.
[01:21:21.140 --> 01:21:26.140]   Moreover, there's been various claims of,
[01:21:26.140 --> 01:21:29.220]   ha-ha, this was mixed and matched in a lab
[01:21:29.220 --> 01:21:32.480]   because the S1 protein has three different components,
[01:21:32.480 --> 01:21:34.620]   each of which has a different evolutionary tree.
[01:21:34.620 --> 01:21:37.220]   So a lot of popular press basically said,
[01:21:37.220 --> 01:21:39.140]   aha, this came from pangolin
[01:21:39.140 --> 01:21:41.820]   and this came from all kinds of other species.
[01:21:41.820 --> 01:21:44.820]   This is what has been happening
[01:21:44.820 --> 01:21:46.860]   throughout the coronavirus tree.
[01:21:46.860 --> 01:21:49.300]   So basically, the S1 protein has been recombining
[01:21:49.300 --> 01:21:50.380]   across species all the time.
[01:21:50.380 --> 01:21:51.980]   Remember when I was talking about the positive strand,
[01:21:51.980 --> 01:21:54.300]   the negative strand, subgenomic RNAs?
[01:21:54.300 --> 01:21:55.740]   These can actually recombine.
[01:21:55.740 --> 01:21:57.100]   And if you have two different viruses
[01:21:57.100 --> 01:21:58.500]   infecting the same cell,
[01:21:58.500 --> 01:21:59.740]   they can actually mix and match
[01:21:59.740 --> 01:22:01.280]   between the positive strand and the negative strand
[01:22:01.280 --> 01:22:04.660]   and basically create a new hybrid virus with recombination
[01:22:04.660 --> 01:22:06.660]   that now has the S1 from one
[01:22:06.660 --> 01:22:08.740]   and the rest of the genome from another.
[01:22:08.740 --> 01:22:10.540]   And this is something that happens a lot in S1,
[01:22:10.540 --> 01:22:12.020]   in OrphA, et cetera.
[01:22:12.020 --> 01:22:13.900]   And that's something that's true of the whole tree.
[01:22:13.900 --> 01:22:14.980]   - For the whole family of-- - Exactly.
[01:22:14.980 --> 01:22:16.500]   - Coronaviruses. - So it's not like
[01:22:16.500 --> 01:22:19.420]   someone has been messing with this for millions of years
[01:22:19.420 --> 01:22:21.620]   and changing the-- - This happens naturally.
[01:22:21.620 --> 01:22:24.420]   That's, again, beautiful that that somehow happens,
[01:22:24.420 --> 01:22:25.900]   that they recombine.
[01:22:25.900 --> 01:22:27.700]   So two different strands can infect the body
[01:22:27.700 --> 01:22:28.640]   and then recombine.
[01:22:28.640 --> 01:22:34.540]   So all of this actually magic happens inside hosts.
[01:22:34.540 --> 01:22:39.220]   - Yeah, that's why classification-wise,
[01:22:39.220 --> 01:22:40.700]   virus is not thought to be alive
[01:22:40.700 --> 01:22:43.020]   because it doesn't self-replicate, it's not autonomous.
[01:22:43.020 --> 01:22:45.740]   It's something that enters a living cell
[01:22:45.740 --> 01:22:48.760]   and then co-opts it to basically make it its own.
[01:22:48.760 --> 01:22:51.580]   But by itself, people ask me, how do we kill this bastard?
[01:22:51.580 --> 01:22:54.180]   I'm like, you stop it from replicating.
[01:22:54.180 --> 01:22:57.660]   It's not like a bacterium that will just live
[01:22:57.660 --> 01:23:01.060]   in a puddle or something.
[01:23:01.060 --> 01:23:02.460]   It's a virus.
[01:23:02.460 --> 01:23:04.420]   Viruses don't live without their host.
[01:23:04.420 --> 01:23:07.360]   And they only live without their host for very little time.
[01:23:07.360 --> 01:23:09.100]   So if you stop it from replicating,
[01:23:09.100 --> 01:23:11.260]   it'll stop from spreading.
[01:23:11.260 --> 01:23:12.300]   I mean, it's not like HIV,
[01:23:12.300 --> 01:23:13.900]   which can stay dormant for a long time.
[01:23:13.900 --> 01:23:15.580]   Basically, coronaviruses just don't do that.
[01:23:15.580 --> 01:23:16.780]   They're not integrating genomes.
[01:23:16.780 --> 01:23:18.060]   They're RNA genomes.
[01:23:18.060 --> 01:23:20.220]   So if it's not expressed, it degrades.
[01:23:20.220 --> 01:23:21.180]   RNA degrades.
[01:23:21.180 --> 01:23:23.380]   It doesn't just stick around.
[01:23:23.380 --> 01:23:27.340]   - Well, let me ask also about the immune system you mentioned.
[01:23:27.340 --> 01:23:29.360]   A lot of people kind of ask,
[01:23:29.360 --> 01:23:34.120]   how can we strengthen the immune system
[01:23:34.120 --> 01:23:36.300]   to respond to this particular virus,
[01:23:36.300 --> 01:23:37.740]   but in viruses in general?
[01:23:37.740 --> 01:23:40.420]   Do you have, from a biological perspective,
[01:23:40.420 --> 01:23:43.140]   thoughts on what we can do as humans
[01:23:43.140 --> 01:23:43.980]   to strengthen our immune system?
[01:23:43.980 --> 01:23:46.620]   - If you look at the death rates across different countries,
[01:23:46.620 --> 01:23:49.700]   people with less vaccination have been dying more.
[01:23:49.700 --> 01:23:51.380]   If you look at North Italy,
[01:23:51.380 --> 01:23:53.940]   the vaccination rates are abysmal there.
[01:23:53.940 --> 01:23:55.860]   And a lot of people have been dying.
[01:23:55.860 --> 01:23:58.780]   If you look at Greece, very good vaccination rates.
[01:23:58.780 --> 01:24:00.300]   Almost no one has been dying.
[01:24:00.300 --> 01:24:03.580]   So yes, there's a policy component.
[01:24:03.580 --> 01:24:05.980]   So Italy reacted very slowly.
[01:24:05.980 --> 01:24:07.460]   Greece reacted very fast.
[01:24:07.460 --> 01:24:09.760]   So yeah, many fewer people died in Greece.
[01:24:09.760 --> 01:24:11.720]   But there might actually be a component
[01:24:11.720 --> 01:24:14.080]   of genetic immune repertoire.
[01:24:14.080 --> 01:24:18.140]   Basically, how did people die off in the history
[01:24:18.140 --> 01:24:20.700]   of the Greek population versus the Italian population?
[01:24:20.700 --> 01:24:21.540]   - Wow.
[01:24:21.540 --> 01:24:22.380]   - There's a--
[01:24:22.380 --> 01:24:24.620]   - That's interesting to think about.
[01:24:24.620 --> 01:24:25.980]   - And then there's a component
[01:24:25.980 --> 01:24:28.940]   of what vaccinations did you have as a kid,
[01:24:28.940 --> 01:24:32.480]   and what are the off-target effects of those vaccinations?
[01:24:32.480 --> 01:24:34.900]   So basically, a vaccination can have two components.
[01:24:34.900 --> 01:24:37.620]   One is training your immune system
[01:24:37.620 --> 01:24:39.480]   against that specific insult.
[01:24:39.480 --> 01:24:42.140]   The second one is boosting up your immune system
[01:24:42.140 --> 01:24:43.640]   for all kinds of other things.
[01:24:44.600 --> 01:24:47.100]   If you look at allergies,
[01:24:47.100 --> 01:24:50.200]   Northern Europe, super clean environments,
[01:24:50.200 --> 01:24:51.420]   tons of allergies.
[01:24:51.420 --> 01:24:54.040]   Southern Europe, my kids grew up eating dirt.
[01:24:54.040 --> 01:24:55.800]   No allergies.
[01:24:55.800 --> 01:24:57.060]   (both laugh)
[01:24:57.060 --> 01:25:00.380]   So growing up, I never had even heard of what allergies are.
[01:25:00.380 --> 01:25:01.940]   Like, was it really allergies?
[01:25:01.940 --> 01:25:03.580]   And the reason is that I was playing in the garden.
[01:25:03.580 --> 01:25:05.560]   I was putting all kinds of stuff in my mouth
[01:25:05.560 --> 01:25:07.360]   from all kinds of dirt and stuff.
[01:25:07.360 --> 01:25:09.920]   Tons of viruses there, tons of bacteria there.
[01:25:09.920 --> 01:25:11.480]   My immune system was built up.
[01:25:11.480 --> 01:25:16.480]   So the more you protect your immune system from exposure,
[01:25:16.480 --> 01:25:21.700]   the less opportunity it has to learn about non-self repertoire
[01:25:21.700 --> 01:25:24.400]   in a way that prepares it for the next insult.
[01:25:24.400 --> 01:25:26.760]   - So that's the horizontal thing, too?
[01:25:26.760 --> 01:25:28.120]   So it's throughout your lifetime
[01:25:28.120 --> 01:25:33.120]   and the lifetime of the people that, your ancestors?
[01:25:33.120 --> 01:25:34.080]   - Yeah, yeah, absolutely.
[01:25:34.080 --> 01:25:37.920]   - What about, so again, it returns against free will.
[01:25:37.920 --> 01:25:39.560]   On the free will side of things,
[01:25:39.560 --> 01:25:41.000]   is there something we could do
[01:25:41.000 --> 01:25:44.760]   to strengthen our immune system in 2020?
[01:25:44.760 --> 01:25:49.760]   Is there exercise, diet, all that kind of stuff?
[01:25:49.760 --> 01:25:52.880]   - So it's kind of funny.
[01:25:52.880 --> 01:25:55.920]   There's a cartoon that basically shows two windows
[01:25:55.920 --> 01:25:58.280]   with a teller in each window.
[01:25:58.280 --> 01:26:02.200]   One has a humongous line, and the other one has no one.
[01:26:02.200 --> 01:26:04.720]   The one that has no one above says health.
[01:26:04.720 --> 01:26:07.240]   No, it says exercise and diet.
[01:26:07.240 --> 01:26:09.000]   And the other one says pill.
[01:26:09.000 --> 01:26:10.280]   (Zubin laughs)
[01:26:10.280 --> 01:26:12.160]   And there's a huge line for pill.
[01:26:12.160 --> 01:26:13.960]   So we're looking basically for magic bullets
[01:26:13.960 --> 01:26:17.640]   for sort of ways that we can beat cancer
[01:26:17.640 --> 01:26:19.480]   and beat coronavirus and beat this and beat that.
[01:26:19.480 --> 01:26:23.240]   And it turns out that the window with just diet and exercise
[01:26:23.240 --> 01:26:26.120]   is the best way to boost every aspect of your health.
[01:26:26.120 --> 01:26:31.120]   If you look at Alzheimer's, exercise and nutrition.
[01:26:31.120 --> 01:26:32.600]   I mean, you're like, really?
[01:26:32.600 --> 01:26:34.720]   For my brain, neurodegeneration?
[01:26:34.720 --> 01:26:36.160]   Absolutely.
[01:26:36.160 --> 01:26:39.600]   If you look at cancer, exercise and nutrition.
[01:26:39.600 --> 01:26:40.440]   (Zubin laughs)
[01:26:40.440 --> 01:26:43.800]   If you look at coronavirus, exercise and nutrition.
[01:26:43.800 --> 01:26:47.280]   Every single aspect of human health gets improved.
[01:26:47.280 --> 01:26:48.600]   And one of the studies we're doing now
[01:26:48.600 --> 01:26:51.240]   is basically looking at what are the effects
[01:26:51.240 --> 01:26:52.920]   of diet and exercise?
[01:26:52.920 --> 01:26:55.320]   How similar are they to each other?
[01:26:55.320 --> 01:26:58.400]   We basically take in diet intervention
[01:26:58.400 --> 01:27:01.400]   and exercise intervention in human and in mice,
[01:27:01.400 --> 01:27:03.560]   and we're basically doing single cell profiling
[01:27:03.560 --> 01:27:04.960]   of a bunch of different tissues
[01:27:04.960 --> 01:27:08.040]   to basically understand how are the cells,
[01:27:08.040 --> 01:27:10.920]   both the stromal cells and the immune cells
[01:27:10.920 --> 01:27:12.360]   of each of these tissues,
[01:27:12.360 --> 01:27:15.120]   responding to the effect of exercise.
[01:27:15.120 --> 01:27:18.560]   What are the communication networks between different cells
[01:27:18.560 --> 01:27:23.560]   where the muscle that exercises sends signals
[01:27:23.560 --> 01:27:26.000]   through the bloodstream, through the lymphatic system,
[01:27:26.000 --> 01:27:27.680]   through all kinds of other systems
[01:27:27.680 --> 01:27:31.600]   that give signals to other cells that I have exercised
[01:27:31.600 --> 01:27:34.000]   and you should change in this particular way,
[01:27:34.000 --> 01:27:37.640]   which basically reconfigure those receptor cells
[01:27:37.640 --> 01:27:39.880]   with the effect of exercise.
[01:27:39.880 --> 01:27:43.880]   - So how well understood is those reconfigurations?
[01:27:43.880 --> 01:27:44.720]   - Very little.
[01:27:44.720 --> 01:27:47.400]   We're just starting now, basically.
[01:27:47.400 --> 01:27:51.060]   - Is the hope there to understand the effect on,
[01:27:51.060 --> 01:27:54.360]   so like the effect on the immune system?
[01:27:54.360 --> 01:27:56.280]   - On the immune system, the effect on brain,
[01:27:56.280 --> 01:27:59.100]   the effect on your liver, on your digestive system,
[01:27:59.100 --> 01:28:00.960]   on your adipocytes.
[01:28:00.960 --> 01:28:03.640]   Adipose, you know, the most misunderstood organ.
[01:28:03.640 --> 01:28:05.800]   Everybody thinks, "Ugh, fat, terrible."
[01:28:05.800 --> 01:28:07.480]   No, fat is awesome.
[01:28:07.480 --> 01:28:09.960]   Your fat cells is what's keeping you alive
[01:28:09.960 --> 01:28:11.440]   because if you didn't have your fat cells,
[01:28:11.440 --> 01:28:13.960]   all those lipids and all those calories
[01:28:13.960 --> 01:28:15.440]   would be floating around in your blood
[01:28:15.440 --> 01:28:16.960]   and you'd be dead by now.
[01:28:16.960 --> 01:28:18.480]   Your adipocytes are your best friend.
[01:28:18.480 --> 01:28:21.840]   They're basically storing all these excess calories
[01:28:21.840 --> 01:28:24.960]   so that they don't hurt all of the rest of the body.
[01:28:24.960 --> 01:28:28.960]   And they're also fat-burning in many ways.
[01:28:28.960 --> 01:28:30.600]   So, you know, again,
[01:28:30.600 --> 01:28:33.520]   when you don't have the homozygous version that I have,
[01:28:33.520 --> 01:28:36.560]   your cells are able to burn calories much more easily
[01:28:36.560 --> 01:28:40.080]   by sort of flipping a master metabolic switch
[01:28:40.080 --> 01:28:42.460]   that involves these FTO locus that I mentioned earlier
[01:28:42.460 --> 01:28:45.120]   and these target genes, RX3 and RX5,
[01:28:45.120 --> 01:28:47.600]   that basically switch your adipocytes
[01:28:47.600 --> 01:28:50.840]   during their three first days of differentiation
[01:28:50.840 --> 01:28:52.360]   as they're becoming mature adipocytes
[01:28:52.360 --> 01:28:54.380]   to basically become either fat-burning
[01:28:54.380 --> 01:28:57.140]   or fat-storing fat cells.
[01:28:57.140 --> 01:28:59.040]   And the fat-burning fat cells are your best friend.
[01:28:59.040 --> 01:29:00.520]   They're much closer to muscle
[01:29:00.520 --> 01:29:02.880]   than they are to white adipocytes.
[01:29:02.880 --> 01:29:04.600]   - Is there a lot of difference between people
[01:29:04.600 --> 01:29:06.400]   like that you could give,
[01:29:06.400 --> 01:29:09.520]   science could eventually give advice
[01:29:09.520 --> 01:29:12.260]   that is very generalizable?
[01:29:12.260 --> 01:29:16.100]   Or is our differences in our genetic makeup,
[01:29:16.100 --> 01:29:16.940]   like you mentioned,
[01:29:16.940 --> 01:29:19.080]   is that going to be basically something
[01:29:19.080 --> 01:29:22.800]   we have to be very specialized to individuals?
[01:29:22.800 --> 01:29:24.880]   Any advice we give in terms of diet,
[01:29:24.880 --> 01:29:25.880]   like what we were just talking about?
[01:29:25.880 --> 01:29:26.720]   - Believe it or not,
[01:29:26.720 --> 01:29:29.640]   the most personalized advice that you give for nutrition
[01:29:29.640 --> 01:29:31.440]   don't have to do with your genome.
[01:29:31.440 --> 01:29:34.400]   They have to do with your gut microbiome,
[01:29:34.400 --> 01:29:35.900]   with the bacteria that live inside you.
[01:29:35.900 --> 01:29:37.960]   So most of your digestion is actually happening
[01:29:37.960 --> 01:29:40.800]   by species that are not human inside you.
[01:29:40.800 --> 01:29:43.080]   You have more non-human cells than you have human cells.
[01:29:43.080 --> 01:29:46.720]   You're basically a giant bag of bacteria
[01:29:46.720 --> 01:29:48.320]   with a few human cells along.
[01:29:48.320 --> 01:29:53.120]   - And those do not necessarily have to do
[01:29:53.120 --> 01:29:54.900]   with your genetic makeup?
[01:29:54.900 --> 01:29:56.760]   - They interact with your genetic makeup.
[01:29:56.760 --> 01:29:58.000]   They interact with your epigenome.
[01:29:58.000 --> 01:29:59.600]   They interact with your nutrition.
[01:29:59.600 --> 01:30:01.280]   They interact with your environment.
[01:30:01.280 --> 01:30:06.280]   They're basically an additional source of variation.
[01:30:06.280 --> 01:30:08.120]   So when you're thinking about
[01:30:08.120 --> 01:30:10.080]   personalized nutritional advice,
[01:30:10.080 --> 01:30:13.640]   part of that is actually how do you match your microbiome?
[01:30:13.640 --> 01:30:17.080]   And part of that is how do we match your genetics?
[01:30:17.080 --> 01:30:22.080]   But again, this is a very diverse set of contributors.
[01:30:22.080 --> 01:30:24.640]   And the effect sizes are not enormous.
[01:30:24.640 --> 01:30:27.920]   So I think the science for that is not fully developed yet.
[01:30:27.920 --> 01:30:28.760]   - Speaking of diets,
[01:30:28.760 --> 01:30:30.640]   'cause I've wrestled in combat sports,
[01:30:30.640 --> 01:30:32.640]   sports my whole life, where weight matters.
[01:30:32.640 --> 01:30:35.320]   So you have to cut and all that stuff.
[01:30:35.320 --> 01:30:38.200]   One thing I've learned a lot about my body,
[01:30:38.200 --> 01:30:40.120]   and it seems to be, I think, true
[01:30:40.120 --> 01:30:41.640]   about other people's bodies,
[01:30:41.640 --> 01:30:45.080]   is that you can adjust to a lot of things.
[01:30:45.080 --> 01:30:48.240]   That's the miraculous thing about this biological system
[01:30:48.240 --> 01:30:52.340]   is I fast often.
[01:30:52.340 --> 01:30:55.000]   I used to eat five, six times a day
[01:30:55.000 --> 01:30:57.000]   and thought that was absolutely necessary.
[01:30:57.000 --> 01:30:59.000]   How could you not eat that often?
[01:30:59.000 --> 01:31:01.320]   And then when I started fasting,
[01:31:01.320 --> 01:31:02.720]   your body adjusts to that,
[01:31:02.720 --> 01:31:04.200]   and you learn how to not eat.
[01:31:04.200 --> 01:31:07.600]   And it was, if you just give it a chance
[01:31:07.600 --> 01:31:09.140]   for a few weeks, actually,
[01:31:09.140 --> 01:31:10.320]   over a period of a few weeks,
[01:31:10.320 --> 01:31:11.800]   your body can adjust to anything.
[01:31:11.800 --> 01:31:14.120]   And that's such a beautiful thing.
[01:31:14.120 --> 01:31:15.480]   - So I'm a computer scientist,
[01:31:15.480 --> 01:31:18.040]   and I've basically gone through periods of 24 hours
[01:31:18.040 --> 01:31:19.680]   without eating or stopping.
[01:31:19.680 --> 01:31:22.080]   And then I'm like, ooh, must eat.
[01:31:22.080 --> 01:31:23.080]   And I eat a ton.
[01:31:23.080 --> 01:31:25.480]   I used to order two pizzas just with my brother.
[01:31:27.440 --> 01:31:29.720]   So I've gone through these extremes as well,
[01:31:29.720 --> 01:31:32.180]   and I've gone the whole intermittent fasting thing.
[01:31:32.180 --> 01:31:34.040]   So I can sympathize with you both
[01:31:34.040 --> 01:31:36.760]   on the seven meals a day to the zero meals a day.
[01:31:36.760 --> 01:31:40.840]   So I think when I say everything with moderation,
[01:31:40.840 --> 01:31:44.360]   I actually think your body responds interestingly
[01:31:44.360 --> 01:31:47.360]   to these different changes in diet.
[01:31:47.360 --> 01:31:49.880]   I think part of the reason why we lose weight
[01:31:49.880 --> 01:31:52.200]   with pretty much every kind of change in behavior
[01:31:52.200 --> 01:31:55.920]   is because our epigenome and the set of proteins
[01:31:55.920 --> 01:31:58.600]   and enzymes that are expressed in our microbiome
[01:31:58.600 --> 01:32:02.040]   are not well suited to that nutritional source.
[01:32:02.040 --> 01:32:03.840]   And therefore, they will not be able
[01:32:03.840 --> 01:32:06.660]   to sort of catch everything that you give them.
[01:32:06.660 --> 01:32:09.160]   And then a lot of that will go undigested.
[01:32:09.160 --> 01:32:10.720]   And that basically means that your body
[01:32:10.720 --> 01:32:13.200]   can then lose weight in the short term,
[01:32:13.200 --> 01:32:16.160]   but very quickly will adjust to that new normal.
[01:32:16.160 --> 01:32:18.200]   And then we'll be able to sort of perhaps gain
[01:32:18.200 --> 01:32:20.400]   a lot of weight from the diet.
[01:32:20.400 --> 01:32:24.400]   So anyway, I mean, there's also studies in factories
[01:32:24.400 --> 01:32:27.200]   where basically people dim the lights,
[01:32:27.200 --> 01:32:28.720]   and then suddenly everybody started working better.
[01:32:28.720 --> 01:32:30.160]   It was like, wow, that's amazing.
[01:32:30.160 --> 01:32:32.840]   Three weeks later, they made the lights a little brighter.
[01:32:32.840 --> 01:32:34.240]   Everybody started working better.
[01:32:34.240 --> 01:32:35.360]   (laughs)
[01:32:35.360 --> 01:32:39.420]   So any kind of intervention has a placebo effect of,
[01:32:39.420 --> 01:32:40.600]   wow, now I'm healthier,
[01:32:40.600 --> 01:32:42.120]   now I'm gonna be running more often, et cetera.
[01:32:42.120 --> 01:32:44.600]   So it's very hard to uncouple the placebo effect
[01:32:44.600 --> 01:32:47.080]   of, wow, I'm doing something to intervene on my diet
[01:32:47.080 --> 01:32:50.280]   from the, wow, this is actually the right thing for me.
[01:32:50.280 --> 01:32:51.960]   So, you know. - Yeah, from the perspective
[01:32:51.960 --> 01:32:54.760]   from a nutrition science, psychology,
[01:32:54.760 --> 01:32:57.080]   both things I'm interested in, especially psychology,
[01:32:57.080 --> 01:33:01.240]   it seems that it's extremely difficult to do good science
[01:33:01.240 --> 01:33:04.200]   because there's so many variables involved.
[01:33:04.200 --> 01:33:06.560]   It's so difficult to control the variables.
[01:33:06.560 --> 01:33:10.300]   So difficult to do sufficiently large-scale experiments,
[01:33:10.300 --> 01:33:14.200]   both sort of in terms of number of subjects and temporal,
[01:33:14.200 --> 01:33:17.020]   like how long you do the study for,
[01:33:17.020 --> 01:33:21.480]   that it just seems like it's not even a real science for now,
[01:33:21.480 --> 01:33:22.640]   like nutrition science.
[01:33:22.640 --> 01:33:24.820]   - I wanna jump into the whole placebo effect
[01:33:24.820 --> 01:33:26.960]   for a little bit here and basically
[01:33:26.960 --> 01:33:30.200]   talk about the implications of that.
[01:33:30.200 --> 01:33:33.200]   If I give you a sugar pill and I tell you it's a sugar pill,
[01:33:33.200 --> 01:33:35.520]   you won't get any better.
[01:33:35.520 --> 01:33:38.200]   But if I tell you a sugar pill and I tell you,
[01:33:38.200 --> 01:33:40.000]   wow, this is an amazing drug
[01:33:40.000 --> 01:33:42.240]   and it actually will stop your cancer,
[01:33:42.240 --> 01:33:46.280]   your cancer will actually stop with much higher probability.
[01:33:46.280 --> 01:33:47.120]   What does that mean?
[01:33:47.120 --> 01:33:47.940]   - That's so amazing, by the way.
[01:33:47.940 --> 01:33:49.920]   - That means that if I can trick your brain
[01:33:49.920 --> 01:33:51.840]   into thinking that I'm healing you,
[01:33:51.840 --> 01:33:54.580]   your brain will basically figure out a way to heal itself,
[01:33:54.580 --> 01:33:56.040]   to heal the body.
[01:33:56.040 --> 01:33:58.600]   And that tells us that there's so much
[01:33:58.600 --> 01:34:01.440]   that we don't understand in the interplay
[01:34:01.440 --> 01:34:04.120]   between our cognition and our biology
[01:34:04.120 --> 01:34:08.400]   that if we were able to better harvest
[01:34:08.400 --> 01:34:12.320]   the power of our brain to sort of impact the body
[01:34:12.320 --> 01:34:14.200]   through the placebo effect,
[01:34:14.200 --> 01:34:17.300]   we would be so much better in so many different things.
[01:34:17.300 --> 01:34:19.220]   Just by tricking yourself into thinking
[01:34:19.220 --> 01:34:21.560]   that you're doing better, you're actually doing better.
[01:34:21.560 --> 01:34:22.640]   So there's something to be said
[01:34:22.640 --> 01:34:25.020]   about sort of positive thinking, about optimism,
[01:34:25.020 --> 01:34:30.020]   about sort of just getting your brain and your mind
[01:34:30.020 --> 01:34:34.920]   into the right mindset that helps your body
[01:34:34.920 --> 01:34:37.200]   and helps your entire biology.
[01:34:37.200 --> 01:34:40.120]   - Yeah, from a science perspective, that's just fascinating.
[01:34:40.120 --> 01:34:41.880]   Obviously, most things about the brain
[01:34:41.880 --> 01:34:43.920]   is a total mystery for now,
[01:34:43.920 --> 01:34:46.440]   but that's a fascinating interplay
[01:34:46.440 --> 01:34:49.920]   that the brain can reduce,
[01:34:49.920 --> 01:34:53.120]   the brain can help cure cancer.
[01:34:53.120 --> 01:34:56.100]   I don't even know what to do with that.
[01:34:56.100 --> 01:34:59.400]   - I mean, the way to think about that is the following.
[01:34:59.400 --> 01:35:01.060]   The converse of the equation
[01:35:01.060 --> 01:35:03.420]   is something that we are much more comfortable with.
[01:35:03.420 --> 01:35:06.020]   Like, oh, if you're stressed,
[01:35:06.020 --> 01:35:08.500]   then your heart rate might rise
[01:35:08.500 --> 01:35:11.260]   and all kinds of sort of toxins might be released
[01:35:11.260 --> 01:35:13.920]   and that can have a detrimental effect in your body,
[01:35:13.920 --> 01:35:15.100]   et cetera, et cetera, et cetera.
[01:35:15.100 --> 01:35:18.340]   So maybe it's easier to understand your body
[01:35:18.340 --> 01:35:20.340]   healing from your mind
[01:35:20.340 --> 01:35:23.340]   by your mind is not killing your body,
[01:35:23.340 --> 01:35:24.860]   or at least it's killing it less.
[01:35:24.860 --> 01:35:28.300]   So I think that aspect of the stress equation
[01:35:28.300 --> 01:35:31.780]   is a little easier for most of us to conceptualize,
[01:35:31.780 --> 01:35:35.140]   but then the healing part is perhaps the same pathways,
[01:35:35.140 --> 01:35:36.140]   perhaps different pathways,
[01:35:36.140 --> 01:35:39.500]   but again, something that is totally untapped scientifically.
[01:35:39.500 --> 01:35:42.860]   - I think we tried to bring this question up a couple of times
[01:35:42.860 --> 01:35:44.580]   but let's return to it again.
[01:35:44.580 --> 01:35:46.460]   Is what do you think is the difference
[01:35:46.460 --> 01:35:49.460]   between the way a computer represents information,
[01:35:49.460 --> 01:35:53.100]   the human genome represents and stores information?
[01:35:53.100 --> 01:35:55.500]   And maybe broadly, what is the difference
[01:35:55.500 --> 01:35:57.820]   between how you think about computers
[01:35:57.820 --> 01:36:00.420]   and how you think about biological systems?
[01:36:00.420 --> 01:36:02.500]   - So I made a very provocative claim earlier
[01:36:02.500 --> 01:36:04.340]   that we are a digital computer,
[01:36:04.340 --> 01:36:06.340]   like that at the core lies a digital code.
[01:36:06.340 --> 01:36:07.540]   And that's true in many ways,
[01:36:07.540 --> 01:36:09.460]   but surrounding that digital core,
[01:36:09.460 --> 01:36:11.460]   there's a huge amount of analog.
[01:36:11.460 --> 01:36:13.660]   If you look at our brain, it's not really digital.
[01:36:13.660 --> 01:36:15.780]   If you look at our sort of RNA
[01:36:15.780 --> 01:36:17.260]   and all of that stuff inside ourselves,
[01:36:17.260 --> 01:36:18.100]   it's not really digital.
[01:36:18.100 --> 01:36:19.860]   It's really analog in many ways.
[01:36:19.860 --> 01:36:22.780]   But let's start with a code
[01:36:22.780 --> 01:36:24.820]   and then we'll expand to the rest.
[01:36:24.820 --> 01:36:27.820]   So the code itself is digital.
[01:36:27.820 --> 01:36:28.660]   So there's genes.
[01:36:28.660 --> 01:36:30.820]   You can think of the genes as, I don't know,
[01:36:30.820 --> 01:36:33.900]   the procedures, the functions inside your language.
[01:36:33.900 --> 01:36:36.420]   And then somehow you have to turn these functions on.
[01:36:36.420 --> 01:36:37.340]   How do you call a gene?
[01:36:37.340 --> 01:36:39.380]   How do you call that function?
[01:36:39.380 --> 01:36:41.900]   The way that you would do it in old programming languages
[01:36:41.900 --> 01:36:44.580]   is go to address, whatever in your memory,
[01:36:44.580 --> 01:36:46.260]   and then you'd start running from there.
[01:36:46.260 --> 01:36:48.940]   And modern programming languages
[01:36:48.940 --> 01:36:50.820]   have encapsulated this into functions
[01:36:50.820 --> 01:36:52.020]   and objects and all of that.
[01:36:52.020 --> 01:36:54.580]   And it's nice and cute, but in the end, deep down,
[01:36:54.580 --> 01:36:55.980]   there's still an assembly code that says
[01:36:55.980 --> 01:36:58.700]   go to that instruction and it runs that instruction.
[01:36:58.700 --> 01:37:01.580]   If you look at the human genome
[01:37:01.580 --> 01:37:04.980]   and the genome of pretty much most species out there,
[01:37:04.980 --> 01:37:08.140]   there's no go-to function.
[01:37:08.140 --> 01:37:11.180]   You just don't start transcribing
[01:37:11.180 --> 01:37:13.900]   in position 13,500,
[01:37:13.900 --> 01:37:18.780]   13,527 in chromosome 12.
[01:37:18.780 --> 01:37:21.940]   You instead have content-based indexing.
[01:37:21.940 --> 01:37:25.140]   So at every location in the genome,
[01:37:25.140 --> 01:37:28.820]   in front of the genes that need to be turned on,
[01:37:28.820 --> 01:37:30.620]   I don't know, when you drink coffee,
[01:37:30.620 --> 01:37:34.500]   there's a little coffee marker in front of all of them.
[01:37:34.500 --> 01:37:38.540]   And whenever your cells that metabolize coffee
[01:37:38.540 --> 01:37:39.820]   need to metabolize coffee,
[01:37:39.820 --> 01:37:41.260]   they basically see coffee and they're like,
[01:37:41.260 --> 01:37:44.780]   "Ooh, let's go turn on all the coffee marked genes."
[01:37:44.780 --> 01:37:48.100]   So there's basically these small motifs,
[01:37:48.100 --> 01:37:50.860]   these small sequences that we call regulatory motifs.
[01:37:50.860 --> 01:37:52.100]   They're like patterns of DNA.
[01:37:52.100 --> 01:37:54.700]   They're only eight characters long or so,
[01:37:54.700 --> 01:37:57.100]   like GAT, GCA, et cetera.
[01:37:57.100 --> 01:38:01.620]   And these motifs work in combinations
[01:38:01.620 --> 01:38:06.380]   and every one of them has some recruitment affinity
[01:38:06.380 --> 01:38:09.700]   for a different protein that will then come and bind it.
[01:38:09.700 --> 01:38:11.900]   And together, collections of these motifs
[01:38:11.900 --> 01:38:15.500]   create regions that we call regulatory regions
[01:38:15.500 --> 01:38:19.340]   that can be either promoters near the beginning of the gene,
[01:38:19.340 --> 01:38:20.180]   and that basically tells you
[01:38:20.180 --> 01:38:22.540]   where the function actually starts, where you call it,
[01:38:22.540 --> 01:38:26.260]   and then enhancers that are looping around of the DNA
[01:38:26.260 --> 01:38:28.220]   that basically bring the machinery
[01:38:28.220 --> 01:38:29.820]   that binds those enhancers
[01:38:29.820 --> 01:38:32.540]   and then bring it onto the promoter,
[01:38:32.540 --> 01:38:34.660]   which then recruits the right,
[01:38:34.660 --> 01:38:36.780]   sort of the ribosome and the polymerase
[01:38:36.780 --> 01:38:37.860]   and all of that thing,
[01:38:37.860 --> 01:38:39.620]   which will first transcribe
[01:38:39.620 --> 01:38:41.660]   and then export and then eventually translate
[01:38:41.660 --> 01:38:44.540]   in the cytoplasm, whatever, RNA molecule.
[01:38:44.540 --> 01:38:50.580]   So the beauty of the way
[01:38:50.580 --> 01:38:55.220]   that the digital computer, that's the genome, works
[01:38:55.220 --> 01:38:58.780]   is that it's extremely fault tolerant.
[01:38:58.780 --> 01:39:00.340]   If I took your hard drive
[01:39:00.340 --> 01:39:03.900]   and I messed with 20% of the letters in it,
[01:39:03.900 --> 01:39:06.660]   of the zeros and ones, and I flipped them,
[01:39:06.660 --> 01:39:08.180]   you'd be in trouble.
[01:39:08.180 --> 01:39:11.060]   If I take the genome and I flip 20% of the letters,
[01:39:11.060 --> 01:39:13.740]   you probably won't even notice.
[01:39:13.740 --> 01:39:16.020]   And that resilience-
[01:39:16.020 --> 01:39:17.460]   - That's fascinating, yeah.
[01:39:17.460 --> 01:39:19.580]   - Is a key design principle,
[01:39:19.580 --> 01:39:21.460]   and again, I'm thermo-morphizing here,
[01:39:21.460 --> 01:39:23.100]   but it's a key driving principle
[01:39:23.100 --> 01:39:25.020]   of how biological systems work.
[01:39:25.020 --> 01:39:28.420]   They're first resilient and then anything else.
[01:39:28.420 --> 01:39:32.820]   And when you look at this incredible beauty of life
[01:39:32.820 --> 01:39:36.180]   from the most, I don't know, beautiful,
[01:39:36.180 --> 01:39:38.940]   I don't know, human genome maybe, of humanity
[01:39:38.940 --> 01:39:41.500]   and all of the ideals that should come with it,
[01:39:41.500 --> 01:39:44.060]   to the most terrifying genome, like, I don't know,
[01:39:44.060 --> 01:39:47.260]   COVID-19, SARS-CoV-2, and the current pandemic,
[01:39:47.260 --> 01:39:50.820]   you basically see this elegance
[01:39:50.820 --> 01:39:55.820]   as the epitome of clean design, but it's dirty.
[01:39:55.820 --> 01:39:57.820]   It's a mess.
[01:39:57.820 --> 01:40:02.660]   It's, you know, the way to get there is hugely messy.
[01:40:02.660 --> 01:40:04.940]   And that's something that we as computer scientists
[01:40:04.940 --> 01:40:06.660]   don't embrace.
[01:40:06.660 --> 01:40:08.340]   We like to have clean code.
[01:40:08.340 --> 01:40:11.500]   Like in engineering, they teach you
[01:40:11.500 --> 01:40:12.780]   about compartmentalization,
[01:40:12.780 --> 01:40:14.260]   about sort of separating functions,
[01:40:14.260 --> 01:40:17.660]   about modularity, about hierarchical design.
[01:40:17.660 --> 01:40:19.060]   None of that applies in biology.
[01:40:19.060 --> 01:40:19.900]   - Testing.
[01:40:19.900 --> 01:40:21.340]   (laughing)
[01:40:21.340 --> 01:40:24.260]   - Testing, sure, yeah, biology does plenty of that,
[01:40:24.260 --> 01:40:26.820]   but I mean, through evolutionary exploration.
[01:40:26.820 --> 01:40:31.100]   But if you look at biological systems,
[01:40:31.100 --> 01:40:33.460]   first, they are robust,
[01:40:33.460 --> 01:40:36.740]   and then they specialize to become anything else.
[01:40:36.740 --> 01:40:38.260]   And if you look at viruses,
[01:40:38.260 --> 01:40:41.060]   the reason why they're so elegant,
[01:40:41.060 --> 01:40:44.620]   when you look at the design of this, you know, genome,
[01:40:44.620 --> 01:40:46.180]   it seems so elegant.
[01:40:46.180 --> 01:40:49.740]   And the reason for that is that it's been stripped down
[01:40:49.740 --> 01:40:51.660]   from something much larger
[01:40:51.660 --> 01:40:53.980]   because of the pressure to keep it compact.
[01:40:53.980 --> 01:40:56.060]   So many compact genomes out there
[01:40:56.060 --> 01:40:58.720]   have ancestors that were much larger.
[01:40:58.720 --> 01:41:00.820]   You don't start small and become big.
[01:41:00.820 --> 01:41:03.660]   You go through a loop of add a bunch of stuff,
[01:41:03.660 --> 01:41:07.260]   increase complexity, and then slim it down.
[01:41:07.260 --> 01:41:10.420]   And one of my early papers was, in fact,
[01:41:10.420 --> 01:41:12.140]   on genome duplication.
[01:41:12.140 --> 01:41:14.100]   One of the things we found is that baker's yeast,
[01:41:14.100 --> 01:41:17.620]   which is the yeast that you use to make bread,
[01:41:17.620 --> 01:41:19.460]   but also the yeast that you use to make wine,
[01:41:19.460 --> 01:41:20.940]   which is basically the dominant species
[01:41:20.940 --> 01:41:22.380]   when you go in the fields of Tuscany
[01:41:22.380 --> 01:41:24.020]   and you say, you know, what's out there,
[01:41:24.020 --> 01:41:26.300]   is basically Saccharomyces cerevisiae,
[01:41:26.300 --> 01:41:28.020]   or the way my Italian friends say,
[01:41:28.020 --> 01:41:30.020]   Saccharomyces cerevisiae.
[01:41:30.020 --> 01:41:32.280]   (laughing)
[01:41:32.280 --> 01:41:34.500]   - Which means what?
[01:41:34.500 --> 01:41:36.660]   - Oh, Saccharomyces, okay, I'm sorry, I'm Greek.
[01:41:36.660 --> 01:41:39.660]   So yeah, zaharo, mykis, zaharo is sugar,
[01:41:39.660 --> 01:41:41.100]   mykis is fungus.
[01:41:41.100 --> 01:41:41.940]   - Yes.
[01:41:41.940 --> 01:41:44.580]   - Cerevisiae, cerveza, beer.
[01:41:44.580 --> 01:41:47.260]   So it means the sugar fungus of beer.
[01:41:47.260 --> 01:41:48.100]   - Yeah.
[01:41:48.100 --> 01:41:49.780]   - You know, less, less, less,
[01:41:49.780 --> 01:41:51.100]   less than sounding to the--
[01:41:51.100 --> 01:41:52.900]   - Still poetic, yep.
[01:41:52.900 --> 01:41:55.020]   - So anyway, Saccharomyces cerevisiae,
[01:41:55.020 --> 01:41:57.100]   basically the major baker's yeast out there,
[01:41:57.100 --> 01:42:00.500]   is the descendant of a whole genome duplication.
[01:42:00.500 --> 01:42:02.940]   Why would a whole genome duplication even happen?
[01:42:02.940 --> 01:42:06.140]   When it happened is coinciding
[01:42:06.140 --> 01:42:08.300]   with about 100 million years ago
[01:42:08.300 --> 01:42:12.920]   and the emergence of fruit-bearing plants.
[01:42:12.920 --> 01:42:15.580]   Why fruit-bearing plants?
[01:42:15.580 --> 01:42:19.060]   Because animals would eat the fruit,
[01:42:19.060 --> 01:42:23.660]   would walk around and poop huge amounts of nutrients
[01:42:23.660 --> 01:42:26.520]   along with a seed for the plants to spread.
[01:42:26.520 --> 01:42:29.060]   Before that, plants were not spreading through animals,
[01:42:29.060 --> 01:42:30.500]   they were spreading through wind
[01:42:30.500 --> 01:42:32.400]   and all kinds of other ways.
[01:42:32.400 --> 01:42:35.100]   But basically, the moment you have fruit-bearing plants,
[01:42:35.100 --> 01:42:38.220]   these plants are basically creating
[01:42:38.220 --> 01:42:40.400]   this abundance of sugar in the environment.
[01:42:40.400 --> 01:42:43.100]   So there's an evolutionary niche that gets created.
[01:42:43.100 --> 01:42:44.260]   And in that evolutionary niche,
[01:42:44.260 --> 01:42:46.820]   you basically have enough sugar
[01:42:46.820 --> 01:42:48.860]   that a whole genome duplication,
[01:42:48.860 --> 01:42:51.260]   which initially is a very messy event,
[01:42:51.260 --> 01:42:53.900]   allows you to then, you know,
[01:42:53.900 --> 01:42:55.900]   relieve some of that complexity.
[01:42:55.900 --> 01:42:57.340]   - So I had to pause.
[01:42:57.340 --> 01:42:59.660]   What does genome duplication mean?
[01:42:59.660 --> 01:43:03.380]   - That basically means that instead of having eight chromosomes
[01:43:03.380 --> 01:43:05.080]   you can now have 16 chromosomes.
[01:43:05.080 --> 01:43:08.940]   - So, but the duplication,
[01:43:08.940 --> 01:43:13.940]   at first when you go to 16, you're not using that.
[01:43:13.940 --> 01:43:15.300]   - Oh yeah, you are.
[01:43:15.300 --> 01:43:17.460]   Yeah, so basically from one day to the next,
[01:43:17.460 --> 01:43:18.780]   you went from having eight chromosomes
[01:43:18.780 --> 01:43:20.420]   to having 16 chromosomes.
[01:43:20.420 --> 01:43:23.060]   Probably a non-disjunction event during a duplication,
[01:43:23.060 --> 01:43:24.340]   during a division.
[01:43:24.340 --> 01:43:25.940]   So you basically divide the cell.
[01:43:25.940 --> 01:43:27.940]   Instead of half the genome going this way
[01:43:27.940 --> 01:43:29.180]   and half the genome going the other way
[01:43:29.180 --> 01:43:30.800]   after duplication of the genome,
[01:43:30.800 --> 01:43:33.180]   you basically have all of it going to one cell.
[01:43:33.180 --> 01:43:36.000]   And then there's sufficient messiness there
[01:43:36.000 --> 01:43:38.500]   that you end up with slight differences
[01:43:38.500 --> 01:43:41.400]   that make most of these chromosomes be actually preserved.
[01:43:41.400 --> 01:43:43.260]   It's a long story short to basically--
[01:43:43.260 --> 01:43:45.180]   - But it's a big upgrade, right?
[01:43:45.180 --> 01:43:46.000]   So that's--
[01:43:46.000 --> 01:43:46.900]   - Not necessarily,
[01:43:46.900 --> 01:43:48.640]   because what happens immediately thereafter
[01:43:48.640 --> 01:43:50.480]   is that you start massively losing
[01:43:50.480 --> 01:43:52.460]   tons of those duplicated genes.
[01:43:52.460 --> 01:43:55.520]   So 90% of those genes were actually lost
[01:43:55.520 --> 01:43:58.300]   very rapidly after whole genome duplication.
[01:43:58.300 --> 01:44:01.820]   And the reason for that is that biology is not intelligent.
[01:44:01.820 --> 01:44:06.620]   It's just ruthless selection, random mutation.
[01:44:06.620 --> 01:44:08.820]   So the ruthless selection basically means that
[01:44:08.820 --> 01:44:11.540]   as soon as one of the random mutations hit one gene,
[01:44:11.540 --> 01:44:13.500]   ruthless selection just kills off that gene.
[01:44:13.500 --> 01:44:14.940]   It's just, you know,
[01:44:14.940 --> 01:44:19.660]   if you have a pressure to maintain a small compact genome,
[01:44:19.660 --> 01:44:21.820]   you will very rapidly lose the second copy
[01:44:21.820 --> 01:44:22.820]   of most of your genes.
[01:44:22.820 --> 01:44:25.900]   And a small number, 10%, were kept in two copies.
[01:44:25.900 --> 01:44:28.940]   And those had to do a lot with environment adaptation,
[01:44:28.940 --> 01:44:31.220]   with the speed of replication,
[01:44:31.220 --> 01:44:32.540]   with the speed of translation,
[01:44:32.540 --> 01:44:34.380]   and with sugar processing.
[01:44:34.380 --> 01:44:36.160]   So I'm making a long story short
[01:44:36.160 --> 01:44:38.860]   to basically say that evolution is messy.
[01:44:38.860 --> 01:44:41.160]   The only way, like so, you know,
[01:44:41.160 --> 01:44:44.060]   the example that I was giving of messing with 20%
[01:44:44.060 --> 01:44:47.340]   of your bits in your computer, totally bogus.
[01:44:47.340 --> 01:44:48.860]   Duplicating all your functions
[01:44:48.860 --> 01:44:51.580]   and just throwing them out there in the same, you know,
[01:44:51.580 --> 01:44:52.980]   function, just totally bogus.
[01:44:52.980 --> 01:44:55.360]   Like this would never work in an engineer system.
[01:44:55.360 --> 01:44:57.100]   But biological systems,
[01:44:57.100 --> 01:44:59.260]   because of this content-based indexing
[01:44:59.260 --> 01:45:02.060]   and because of this modularity that comes
[01:45:02.060 --> 01:45:04.400]   from the fact that the gene is controlled
[01:45:04.400 --> 01:45:05.380]   by a series of tags,
[01:45:05.380 --> 01:45:08.360]   and now if you need this gene in another setting,
[01:45:08.360 --> 01:45:09.940]   you just add some more tags
[01:45:09.940 --> 01:45:12.740]   that will basically turn it on also in those settings.
[01:45:12.740 --> 01:45:17.500]   So this gene is now pressured to do two different functions.
[01:45:17.500 --> 01:45:19.980]   And it builds up complexity.
[01:45:19.980 --> 01:45:21.460]   I see whole-genome duplication
[01:45:21.460 --> 01:45:22.780]   and gene duplication in general
[01:45:22.780 --> 01:45:24.740]   as a way to relieve that complexity.
[01:45:24.740 --> 01:45:26.860]   So you have this gradual buildup of complexity
[01:45:26.860 --> 01:45:30.940]   as functions get sort of added onto the existing genes.
[01:45:30.940 --> 01:45:34.380]   And then boom, you duplicate your workforce.
[01:45:34.380 --> 01:45:36.940]   And you now have two copies of this gene.
[01:45:36.940 --> 01:45:38.980]   One will probably specialize to do one,
[01:45:38.980 --> 01:45:40.620]   and the other one will specialize to do the other,
[01:45:40.620 --> 01:45:42.380]   or one will maintain the ancestral function,
[01:45:42.380 --> 01:45:45.020]   the other one will sort of be free to evolve
[01:45:45.020 --> 01:45:47.940]   and specialize while losing the ancestral function,
[01:45:47.940 --> 01:45:48.860]   and so on and so forth.
[01:45:48.860 --> 01:45:50.180]   So that's how genomes evolve.
[01:45:50.180 --> 01:45:52.260]   They're just messy things,
[01:45:52.260 --> 01:45:54.780]   but they're extremely fault-tolerant,
[01:45:54.780 --> 01:45:58.580]   and they're extremely able to deal with mutations
[01:45:58.580 --> 01:46:03.580]   because that's the very way that you generate new functions.
[01:46:03.580 --> 01:46:05.640]   So new functionalization comes
[01:46:05.640 --> 01:46:07.920]   from the very thing that breaks it.
[01:46:07.920 --> 01:46:09.340]   So even in the current pandemic,
[01:46:09.340 --> 01:46:10.620]   many people are asking me,
[01:46:10.620 --> 01:46:12.780]   "Which mutations matter the most?"
[01:46:12.780 --> 01:46:13.940]   And what I tell them is,
[01:46:13.940 --> 01:46:16.420]   "Well, we can study the evolutionary dynamics
[01:46:16.420 --> 01:46:19.640]   "of the current genome to then understand
[01:46:19.640 --> 01:46:23.340]   "which mutations have previously happened or not,
[01:46:23.340 --> 01:46:27.040]   "and which mutations happen in genes
[01:46:27.040 --> 01:46:29.220]   "that evolve rapidly or not."
[01:46:29.220 --> 01:46:30.860]   And one of the things we found, for example,
[01:46:30.860 --> 01:46:34.980]   is that the genes that evolved rapidly in the past
[01:46:34.980 --> 01:46:37.860]   are still evolving rapidly now in the current pandemic.
[01:46:37.860 --> 01:46:39.820]   The genes that evolved slowly in the past
[01:46:39.820 --> 01:46:41.120]   are still evolving slowly.
[01:46:41.120 --> 01:46:42.860]   - Which means that they're useful.
[01:46:42.860 --> 01:46:44.140]   - Which means that they're
[01:46:44.140 --> 01:46:46.400]   under the same evolutionary pressures.
[01:46:46.400 --> 01:46:47.820]   But then the question is,
[01:46:47.820 --> 01:46:50.540]   what happens in specific mutations?
[01:46:50.540 --> 01:46:53.440]   So if you look at the D614 gene mutation
[01:46:53.440 --> 01:46:54.420]   that's been all over the news,
[01:46:54.420 --> 01:46:58.380]   so in position 614, in the amino acid 614,
[01:46:58.380 --> 01:47:00.300]   of the S protein,
[01:47:00.300 --> 01:47:02.660]   there's a D to gene mutation
[01:47:02.660 --> 01:47:06.420]   that sort of has creeped over the population.
[01:47:06.420 --> 01:47:10.580]   That mutation, we found out through my work,
[01:47:10.580 --> 01:47:14.020]   disrupts a perfectly conserved nucleotide position
[01:47:14.020 --> 01:47:16.340]   that has never been changed in the history
[01:47:16.340 --> 01:47:20.420]   of millions of years of equivalent mammalian evolution
[01:47:20.420 --> 01:47:23.080]   of these viruses.
[01:47:23.080 --> 01:47:23.920]   That basically means
[01:47:23.920 --> 01:47:27.500]   that it's a completely new adaptation to human.
[01:47:27.500 --> 01:47:30.820]   And that mutation has now gone from 1% frequency
[01:47:30.820 --> 01:47:33.820]   to 90% frequency in almost all outbreaks.
[01:47:33.820 --> 01:47:35.100]   - So there's a mutation,
[01:47:35.100 --> 01:47:39.620]   I like how you say the 416, what was it?
[01:47:39.620 --> 01:47:40.660]   - Yeah, 614, sorry.
[01:47:40.660 --> 01:47:42.180]   - 614, all right.
[01:47:42.180 --> 01:47:43.700]   - D614 gene.
[01:47:43.700 --> 01:47:46.580]   - So literally, so what you're saying
[01:47:46.580 --> 01:47:48.460]   is this is like a chess move.
[01:47:48.460 --> 01:47:50.580]   So it just mutated one letter to another.
[01:47:50.580 --> 01:47:51.420]   - Exactly.
[01:47:51.420 --> 01:47:52.980]   - And that hasn't happened before.
[01:47:52.980 --> 01:47:54.300]   - Yeah, never.
[01:47:54.300 --> 01:47:58.140]   - And this somehow, this mutation is really useful.
[01:47:58.140 --> 01:48:00.620]   - It's really useful in the current environment
[01:48:00.620 --> 01:48:04.900]   of the genome, which is moving from human to human.
[01:48:04.900 --> 01:48:06.840]   When it was moving from bat to bat,
[01:48:06.840 --> 01:48:08.680]   it couldn't care less for that mutation.
[01:48:08.680 --> 01:48:09.980]   But it's environment specific,
[01:48:09.980 --> 01:48:11.780]   so now that it's moving from human to human,
[01:48:11.780 --> 01:48:14.100]   hoo-hoo, it's moving way better,
[01:48:14.100 --> 01:48:15.940]   like by orders of magnitude.
[01:48:15.940 --> 01:48:18.460]   - What do you, okay, so you're like tracking
[01:48:18.460 --> 01:48:22.540]   this evolutionary dynamics, which is fascinating.
[01:48:22.540 --> 01:48:24.220]   But what do you do with that?
[01:48:24.220 --> 01:48:25.340]   So what does that mean?
[01:48:25.340 --> 01:48:27.580]   What does this mean, what do you make,
[01:48:27.580 --> 01:48:29.160]   what do you make of this mutation
[01:48:29.160 --> 01:48:31.500]   in trying to anticipate, I guess?
[01:48:31.500 --> 01:48:34.140]   Is one of the things you're trying to do
[01:48:34.140 --> 01:48:37.860]   is anticipate where, how this unrolls into the future,
[01:48:37.860 --> 01:48:39.740]   this evolutionary dynamic?
[01:48:39.740 --> 01:48:40.660]   - Such a great question.
[01:48:40.660 --> 01:48:42.940]   So there's two things.
[01:48:42.940 --> 01:48:44.740]   Remember when I was saying earlier,
[01:48:44.740 --> 01:48:47.140]   mutation is the path to new things,
[01:48:47.140 --> 01:48:49.820]   but also the path to break old things.
[01:48:49.820 --> 01:48:53.060]   So what we know is that this position
[01:48:53.060 --> 01:48:56.700]   was extremely preserved through gazillions of mutations.
[01:48:56.700 --> 01:48:58.540]   That mutation was never tolerated
[01:48:58.540 --> 01:49:00.300]   when it was moving from bat to bat.
[01:49:00.300 --> 01:49:01.620]   So that basically means that that mutation,
[01:49:01.620 --> 01:49:04.120]   that position is extremely important
[01:49:04.120 --> 01:49:05.700]   in the function of that protein.
[01:49:05.700 --> 01:49:07.040]   That's the first thing it tells.
[01:49:07.040 --> 01:49:09.380]   The second one is that that position
[01:49:09.380 --> 01:49:12.460]   was very well suited to bat transmission,
[01:49:12.460 --> 01:49:14.800]   but now is not well suited to human transmission,
[01:49:14.800 --> 01:49:15.920]   so it got rid of it.
[01:49:15.920 --> 01:49:18.920]   And it now has a new version of that amino acid
[01:49:18.920 --> 01:49:21.000]   that basically makes it much easier
[01:49:21.000 --> 01:49:22.800]   to transmit from human to human.
[01:49:22.800 --> 01:49:27.520]   So in terms of the evolutionary history
[01:49:27.520 --> 01:49:29.920]   teaching us about the future,
[01:49:29.920 --> 01:49:32.040]   it basically tells us here's the regions
[01:49:32.040 --> 01:49:34.880]   that are currently mutating,
[01:49:34.880 --> 01:49:36.480]   here's the regions that are most likely
[01:49:36.480 --> 01:49:37.960]   to mutate going forward.
[01:49:37.960 --> 01:49:39.480]   As you're building a vaccine,
[01:49:39.480 --> 01:49:41.760]   here's what you should be focusing on
[01:49:41.760 --> 01:49:43.620]   in terms of the most stable regions
[01:49:43.620 --> 01:49:45.520]   that are the least likely to mutate,
[01:49:45.520 --> 01:49:48.280]   or here's the newly evolved functions
[01:49:48.280 --> 01:49:50.320]   that are the most likely to be important
[01:49:50.320 --> 01:49:54.640]   because they've overcome this local maximum
[01:49:54.640 --> 01:49:59.440]   that it had reached in the bat transmission.
[01:49:59.440 --> 01:50:01.840]   So anyway, it's a tangent to basically say
[01:50:01.840 --> 01:50:04.280]   that evolution works in messy ways,
[01:50:04.280 --> 01:50:06.360]   and the thing that you would break
[01:50:06.360 --> 01:50:10.360]   is the thing that actually allows you
[01:50:10.360 --> 01:50:12.240]   to first go through a lull
[01:50:12.240 --> 01:50:15.240]   and then reach a new local maximum.
[01:50:15.240 --> 01:50:19.000]   And I often like to say that if engineers
[01:50:19.000 --> 01:50:21.260]   had basically designed evolution,
[01:50:21.260 --> 01:50:24.560]   we would still be perfectly replicating bacteria
[01:50:24.560 --> 01:50:29.480]   because it's by making the bacterium worse
[01:50:29.480 --> 01:50:32.240]   that you allow evolution to reach a new optimum.
[01:50:32.240 --> 01:50:34.560]   - That's just a pause on that,
[01:50:34.560 --> 01:50:35.860]   that's so profound.
[01:50:35.860 --> 01:50:39.400]   That's so profound for the entirety
[01:50:39.400 --> 01:50:43.720]   of this scientific and engineering disciplines.
[01:50:43.720 --> 01:50:45.520]   - Exactly.
[01:50:45.520 --> 01:50:48.520]   We as engineers need to embrace breaking things.
[01:50:48.520 --> 01:50:50.960]   We as engineers need to embrace robustness
[01:50:50.960 --> 01:50:54.240]   as the first principle beyond perfection
[01:50:54.240 --> 01:50:56.040]   'cause nothing's gonna ever be perfect.
[01:50:56.040 --> 01:50:58.440]   And when you're sending a satellite to Mars,
[01:50:58.440 --> 01:51:01.160]   when something goes wrong, it'll break down
[01:51:01.160 --> 01:51:04.560]   as opposed to building systems that tolerate failure
[01:51:04.560 --> 01:51:08.840]   and are resilient to that,
[01:51:08.840 --> 01:51:11.080]   and in fact, get better through that.
[01:51:11.080 --> 01:51:14.600]   - So the SpaceX approach versus NASA for the...
[01:51:14.600 --> 01:51:15.640]   (laughing)
[01:51:15.640 --> 01:51:16.480]   - For example.
[01:51:16.480 --> 01:51:21.280]   - Is there something we can learn about the incredible,
[01:51:21.280 --> 01:51:23.920]   take lessons from the incredible biological systems
[01:51:23.920 --> 01:51:27.600]   in their resilience, in the mushiness, the messiness
[01:51:27.600 --> 01:51:31.880]   to our computing systems, to our computers?
[01:51:31.880 --> 01:51:35.280]   - It would basically be starting from scratch in many ways.
[01:51:35.280 --> 01:51:38.960]   It would basically be building new paradigms
[01:51:38.960 --> 01:51:42.760]   that don't try to get the right answer all the time,
[01:51:42.760 --> 01:51:45.600]   but try to get the right answer most of the time
[01:51:45.600 --> 01:51:47.000]   or a lot of the time.
[01:51:47.000 --> 01:51:48.560]   - Do you see deep learning systems
[01:51:48.560 --> 01:51:49.960]   and the whole world of machine learning
[01:51:49.960 --> 01:51:52.000]   as kind of taking a step in that direction?
[01:51:52.000 --> 01:51:53.640]   - Absolutely, absolutely.
[01:51:53.640 --> 01:51:57.560]   Basically by allowing this much more natural evolution
[01:51:57.560 --> 01:52:01.120]   of these parameters, you basically,
[01:52:01.120 --> 01:52:03.240]   and if you look at sort of deep learning systems,
[01:52:03.240 --> 01:52:07.480]   again, they're not inspired by the genome aspect of biology,
[01:52:07.480 --> 01:52:10.200]   they're inspired by the brain aspect of biology.
[01:52:10.200 --> 01:52:12.600]   And again, I want you to pause for a second
[01:52:12.600 --> 01:52:17.600]   and realize the complexity of the entire human brain
[01:52:17.600 --> 01:52:22.800]   with trillions of connections within our neurons,
[01:52:22.800 --> 01:52:26.720]   with millions of cells talking to each other,
[01:52:26.720 --> 01:52:29.080]   is still encoded within that same genome.
[01:52:29.080 --> 01:52:32.440]   (Zubin laughs)
[01:52:32.440 --> 01:52:36.160]   That same genome encodes every single freaking cell type
[01:52:36.160 --> 01:52:38.000]   of the entire body.
[01:52:38.000 --> 01:52:41.120]   Every single cell is encoded by the same code.
[01:52:41.120 --> 01:52:45.320]   And yet specialization allows you to have
[01:52:45.320 --> 01:52:50.120]   the single viral-like genome that self-replicates,
[01:52:50.120 --> 01:52:54.280]   the single module, modular automaton,
[01:52:54.280 --> 01:52:56.560]   work with other copies of itself.
[01:52:56.560 --> 01:52:57.560]   It's mind-boggling.
[01:52:57.560 --> 01:53:02.760]   Create complex organs through which blood flows.
[01:53:02.760 --> 01:53:03.800]   And what is that blood?
[01:53:03.800 --> 01:53:05.240]   The same freaking genome.
[01:53:05.240 --> 01:53:06.760]   (Zubin laughs)
[01:53:06.760 --> 01:53:10.960]   Create organs that communicate with each other.
[01:53:10.960 --> 01:53:12.360]   And what are these organs?
[01:53:12.360 --> 01:53:14.280]   The exact same genome.
[01:53:14.280 --> 01:53:17.640]   Create a brain that is innervated
[01:53:17.640 --> 01:53:22.440]   by massive amounts of blood pumping energy to it,
[01:53:22.440 --> 01:53:24.320]   20% of our energetic needs,
[01:53:25.440 --> 01:53:28.240]   to the brain from the same genome.
[01:53:28.240 --> 01:53:30.120]   And all of the neuronal connections,
[01:53:30.120 --> 01:53:33.920]   all of the auxiliary cells, all of the immune cells,
[01:53:33.920 --> 01:53:35.920]   the astrocytes, the ligandrocytes, the neurons,
[01:53:35.920 --> 01:53:37.360]   the excitatory, the inhibitory neurons,
[01:53:37.360 --> 01:53:39.480]   all of the different classes of pericytes,
[01:53:39.480 --> 01:53:42.880]   the blood-brain barrier, all of that, same genome.
[01:53:42.880 --> 01:53:46.600]   - One way to see that in a sad,
[01:53:46.600 --> 01:53:49.240]   this one is beautiful, the sad thing is thinking about
[01:53:49.240 --> 01:53:54.240]   the trillions of organisms that died to create that.
[01:53:55.280 --> 01:53:56.920]   - You mean on the evolutionary path?
[01:53:56.920 --> 01:53:59.640]   - On the evolutionary path to humans.
[01:53:59.640 --> 01:54:02.720]   - It's crazy, there's two descendant of apes
[01:54:02.720 --> 01:54:05.380]   just talking on a podcast, okay.
[01:54:05.380 --> 01:54:07.080]   (Zubin laughs)
[01:54:07.080 --> 01:54:08.520]   So mind-boggling.
[01:54:08.520 --> 01:54:11.160]   - Just to boggle our minds a little bit more.
[01:54:11.160 --> 01:54:12.560]   Us talking to each other,
[01:54:12.560 --> 01:54:18.460]   we are basically generating a series of vocal utterances
[01:54:18.460 --> 01:54:23.460]   through our pulsating of vocal cords received through this.
[01:54:23.480 --> 01:54:26.200]   The people who listen to this
[01:54:26.200 --> 01:54:29.280]   are taking a completely different path
[01:54:29.280 --> 01:54:32.920]   to that information transfer, yet through language.
[01:54:32.920 --> 01:54:36.180]   But imagine if we could connect these brains
[01:54:36.180 --> 01:54:37.540]   directly to each other.
[01:54:37.540 --> 01:54:41.640]   The amount of information that I'm condensing
[01:54:41.640 --> 01:54:46.400]   into a small number of words is a huge funnel,
[01:54:46.400 --> 01:54:49.500]   which then you receive and you expand
[01:54:49.500 --> 01:54:52.880]   into a huge number of thoughts from that small funnel.
[01:54:53.560 --> 01:54:55.760]   (Zubin laughs)
[01:54:55.760 --> 01:54:58.420]   In many ways, engineers would love to have
[01:54:58.420 --> 01:54:59.900]   the whole information transfer,
[01:54:59.900 --> 01:55:02.660]   just take the whole set of neurons and throw them away.
[01:55:02.660 --> 01:55:04.560]   I mean, throw them to the other person.
[01:55:04.560 --> 01:55:07.300]   This might actually not be better
[01:55:07.300 --> 01:55:11.780]   because in your misinterpretation of every word
[01:55:11.780 --> 01:55:14.680]   that I'm saying, you are creating new interpretation
[01:55:14.680 --> 01:55:16.080]   that might actually be way better
[01:55:16.080 --> 01:55:17.920]   than what I meant in the first place.
[01:55:17.920 --> 01:55:20.520]   The ambiguity of language,
[01:55:20.520 --> 01:55:24.160]   perhaps might be the secret to creativity.
[01:55:24.160 --> 01:55:28.400]   Every single time you work on a project by yourself,
[01:55:28.400 --> 01:55:31.120]   you only bounce ideas with one person
[01:55:31.120 --> 01:55:33.760]   and your neurons are basically fully cognizant
[01:55:33.760 --> 01:55:35.880]   of what these ideas are.
[01:55:35.880 --> 01:55:37.720]   But the moment you interact with another person,
[01:55:37.720 --> 01:55:41.080]   the misinterpretations that happen
[01:55:41.080 --> 01:55:43.760]   might be the most creative part of the process.
[01:55:43.760 --> 01:55:45.600]   With my students, every time we have a research meeting,
[01:55:45.600 --> 01:55:47.520]   I very often pause and say,
[01:55:47.520 --> 01:55:50.400]   let me repeat what you just said in a different way.
[01:55:50.400 --> 01:55:52.400]   And I sort of go on and brainstorm
[01:55:52.400 --> 01:55:53.660]   with what they were saying,
[01:55:53.660 --> 01:55:58.000]   but by the third time, it's not what they were saying at all.
[01:55:58.000 --> 01:55:59.480]   And when they pick up what I'm saying,
[01:55:59.480 --> 01:56:01.160]   they're like, oh, well, da-da-da,
[01:56:01.160 --> 01:56:04.140]   now they've sort of learned something very different
[01:56:04.140 --> 01:56:05.000]   from what I was saying.
[01:56:05.000 --> 01:56:08.480]   And that is the same kind of messiness
[01:56:08.480 --> 01:56:10.960]   that I'm describing in the genome itself.
[01:56:10.960 --> 01:56:13.560]   It's sort of embracing the messiness.
[01:56:13.560 --> 01:56:15.360]   - And that's a feature, not a book.
[01:56:15.360 --> 01:56:16.200]   - Exactly.
[01:56:16.200 --> 01:56:17.520]   And in the same way, when you're thinking
[01:56:17.520 --> 01:56:19.920]   about sort of these deep learning systems
[01:56:19.920 --> 01:56:23.560]   that will allow us to sort of be more creative perhaps
[01:56:23.560 --> 01:56:27.520]   or learn better approximations of these complex functions,
[01:56:27.520 --> 01:56:29.760]   again, tuned to the universe that we inhabit,
[01:56:29.760 --> 01:56:33.660]   you have to embrace the breaking.
[01:56:33.660 --> 01:56:35.360]   You have to embrace the,
[01:56:35.360 --> 01:56:37.960]   how do we get out of these local optima?
[01:56:37.960 --> 01:56:40.940]   And a lot of the design paradigms
[01:56:40.940 --> 01:56:43.360]   that have made deep learning so successful
[01:56:43.360 --> 01:56:45.360]   are ways to get away from that,
[01:56:45.360 --> 01:56:47.320]   ways to get better training
[01:56:47.320 --> 01:56:50.480]   by sort of sending long range messages,
[01:56:50.480 --> 01:56:55.480]   these LSTM models and the sort of feed forward loops
[01:56:55.480 --> 01:56:59.260]   that sort of jump through layers
[01:56:59.260 --> 01:57:00.880]   of a convolutional neural network.
[01:57:00.880 --> 01:57:04.200]   All of these things are basically ways
[01:57:04.200 --> 01:57:06.360]   to push you out of these local maxima.
[01:57:06.360 --> 01:57:08.800]   And that's sort of what evolution does,
[01:57:08.800 --> 01:57:09.800]   that's what language does,
[01:57:09.800 --> 01:57:12.320]   that's what conversation and brainstorming does,
[01:57:12.320 --> 01:57:14.080]   that's what our brain does.
[01:57:14.080 --> 01:57:18.280]   So this design paradigm is something that's pervasive
[01:57:18.280 --> 01:57:20.520]   and yet not taught in schools,
[01:57:20.520 --> 01:57:22.240]   not taught in engineering schools
[01:57:22.240 --> 01:57:24.480]   where everything's minutely modularized
[01:57:24.480 --> 01:57:26.000]   to make sure that we never deviate
[01:57:26.000 --> 01:57:28.600]   from whatever signal we're trying to emit
[01:57:28.600 --> 01:57:31.400]   as opposed to let all hell breaks loose
[01:57:31.400 --> 01:57:33.960]   'cause that's the path to paradise.
[01:57:33.960 --> 01:57:35.400]   - The path to paradise.
[01:57:35.400 --> 01:57:37.960]   Yeah, I mean, it's difficult to know how to teach that
[01:57:37.960 --> 01:57:39.240]   and what to do with it.
[01:57:39.240 --> 01:57:43.640]   I mean, it's difficult to know how to build up
[01:57:43.640 --> 01:57:46.600]   the scientific method around messiness.
[01:57:46.600 --> 01:57:48.280]   (Lex laughing)
[01:57:48.280 --> 01:57:49.920]   - I mean, it's not all messiness.
[01:57:49.920 --> 01:57:51.880]   We need some cleanness.
[01:57:51.880 --> 01:57:54.320]   And going back to the example with Mars,
[01:57:54.320 --> 01:57:56.720]   that's probably the place where I want to sort of
[01:57:56.720 --> 01:57:58.760]   moderate error as much as possible
[01:57:58.760 --> 01:58:01.000]   and sort of control the environment as much as possible.
[01:58:01.000 --> 01:58:03.120]   But if you're trying to repopulate Mars,
[01:58:03.120 --> 01:58:05.280]   well, maybe messiness is a good thing then.
[01:58:06.200 --> 01:58:09.320]   - On that, you quickly mentioned this
[01:58:09.320 --> 01:58:14.320]   in terms of us using our vocal cords to speak on a podcast.
[01:58:14.320 --> 01:58:20.120]   So Elon Musk and Neuralink are working on trying to plug,
[01:58:20.120 --> 01:58:24.920]   as per our discussion with computers and biological systems,
[01:58:24.920 --> 01:58:25.840]   to connect the two.
[01:58:25.840 --> 01:58:30.640]   He's trying to connect our brain to a computer
[01:58:30.640 --> 01:58:32.840]   to create a brain-computer interface
[01:58:32.840 --> 01:58:34.940]   where they can communicate back and forth.
[01:58:36.160 --> 01:58:38.240]   On this line of thinking,
[01:58:38.240 --> 01:58:42.480]   do you think this is possible to bridge the gap
[01:58:42.480 --> 01:58:45.240]   between our engineered computing systems
[01:58:45.240 --> 01:58:48.280]   and the messy biological systems?
[01:58:48.280 --> 01:58:51.240]   - My answer would be absolutely.
[01:58:51.240 --> 01:58:55.040]   There's no doubt that we can understand more and more
[01:58:55.040 --> 01:58:57.160]   about what goes on in the brain,
[01:58:57.160 --> 01:59:00.320]   and we can sort of train the brain.
[01:59:00.320 --> 01:59:03.600]   I don't know if you remember the Palm Pilot.
[01:59:03.600 --> 01:59:04.720]   - Yeah, Palm Pilot, yeah.
[01:59:04.720 --> 01:59:08.480]   - Remember this whole sort of alphabet that they had created?
[01:59:08.480 --> 01:59:10.960]   Am I thinking of the same thing?
[01:59:10.960 --> 01:59:13.320]   It's basically, you had a little pen,
[01:59:13.320 --> 01:59:17.040]   and for every character, you had a little scribble
[01:59:17.040 --> 01:59:19.840]   that was unique that the machine could understand,
[01:59:19.840 --> 01:59:23.680]   and that instead of trying to teach the machine
[01:59:23.680 --> 01:59:25.400]   to recognize human characters,
[01:59:25.400 --> 01:59:27.240]   you had basically, they figured out
[01:59:27.240 --> 01:59:30.020]   that it's better and easier to train humans
[01:59:30.020 --> 01:59:31.880]   to create human-like characters
[01:59:31.880 --> 01:59:34.020]   that the machine is better at recognizing.
[01:59:34.740 --> 01:59:38.300]   So in the same way, I think what will happen
[01:59:38.300 --> 01:59:40.580]   is that humans will be trained
[01:59:40.580 --> 01:59:43.200]   to be able to create the mind pattern
[01:59:43.200 --> 01:59:45.140]   that the machine will respond to
[01:59:45.140 --> 01:59:47.760]   before the machine truly comprehends our thoughts.
[01:59:47.760 --> 01:59:50.140]   So the first human brain interfaces
[01:59:50.140 --> 01:59:53.640]   will be tricking humans to speak the machine language,
[01:59:53.640 --> 01:59:55.620]   where with the right set of electrodes,
[01:59:55.620 --> 01:59:57.620]   I can sort of trick my brain into doing this.
[01:59:57.620 --> 02:00:00.260]   And this is the same way that many people teach,
[02:00:00.260 --> 02:00:02.980]   like learn to control artificial limbs.
[02:00:02.980 --> 02:00:04.540]   You basically try a bunch of stuff,
[02:00:04.540 --> 02:00:06.920]   and eventually you figure out how your limbs work.
[02:00:06.920 --> 02:00:08.240]   That might not be very different
[02:00:08.240 --> 02:00:11.500]   from how humans learn to use their natural limbs
[02:00:11.500 --> 02:00:13.100]   when they first grow up.
[02:00:13.100 --> 02:00:16.460]   Basically, you have these neoteny period
[02:00:16.460 --> 02:00:21.340]   of this puddle of soup inside your brain,
[02:00:21.340 --> 02:00:23.880]   trying to figure out how to even make neuronal connections
[02:00:23.880 --> 02:00:28.500]   before you're born, and then learning sounds in utero
[02:00:28.500 --> 02:00:31.540]   of all kinds of echoes,
[02:00:31.540 --> 02:00:35.900]   and eventually getting out in the real world.
[02:00:35.900 --> 02:00:37.360]   And I don't know if you've seen newborns,
[02:00:37.360 --> 02:00:39.180]   but they just stare around a lot.
[02:00:39.180 --> 02:00:42.900]   One way to think about this as a machine learning person
[02:00:42.900 --> 02:00:46.140]   is, oh, they're just training their edge detectors.
[02:00:46.140 --> 02:00:47.380]   And eventually, they figure out
[02:00:47.380 --> 02:00:48.740]   how to train their edge detectors.
[02:00:48.740 --> 02:00:50.860]   They work through the second layer of the visual cortex
[02:00:50.860 --> 02:00:52.700]   and the third layer and so on and so forth.
[02:00:52.700 --> 02:00:56.180]   And you basically have this
[02:00:57.920 --> 02:00:59.440]   learning how to control your limbs
[02:00:59.440 --> 02:01:01.080]   that probably comes at the same time.
[02:01:01.080 --> 02:01:03.360]   You're sort of throwing random things there,
[02:01:03.360 --> 02:01:04.760]   and you realize that, ooh, wow,
[02:01:04.760 --> 02:01:07.200]   when I do this thing, my limb moves.
[02:01:07.200 --> 02:01:09.320]   Let's do the following experiment.
[02:01:09.320 --> 02:01:10.140]   Take a breath.
[02:01:10.140 --> 02:01:13.560]   What muscles did you flex?
[02:01:13.560 --> 02:01:14.920]   Now take another breath and think
[02:01:14.920 --> 02:01:16.640]   what muscles do I flex?
[02:01:16.640 --> 02:01:18.000]   The first thing that you're thinking
[02:01:18.000 --> 02:01:19.920]   when you're taking a breath
[02:01:19.920 --> 02:01:22.400]   is the impact that it has on your lungs.
[02:01:22.400 --> 02:01:24.160]   You're like, oh, I'm now gonna increase my lungs,
[02:01:24.160 --> 02:01:25.480]   or I'm now gonna bring air in.
[02:01:25.480 --> 02:01:26.420]   But what you're actually doing
[02:01:26.420 --> 02:01:28.080]   is just changing your diaphragm.
[02:01:28.080 --> 02:01:31.920]   That's not conscious, of course.
[02:01:31.920 --> 02:01:35.000]   You never think of the diaphragm as a thing.
[02:01:35.000 --> 02:01:36.040]   And why is that?
[02:01:36.040 --> 02:01:37.400]   That's probably the same reason
[02:01:37.400 --> 02:01:38.880]   why I think of moving my finger
[02:01:38.880 --> 02:01:40.600]   when I actually move my finger.
[02:01:40.600 --> 02:01:42.600]   I think of the effect instead of actually thinking
[02:01:42.600 --> 02:01:44.160]   of whatever muscle is twitching
[02:01:44.160 --> 02:01:46.500]   that actually causes my finger to move.
[02:01:46.500 --> 02:01:49.280]   So we basically, in our first years of life,
[02:01:49.280 --> 02:01:52.400]   build up this massive lookup table
[02:01:52.400 --> 02:01:55.440]   between whatever neuronal firing we do
[02:01:55.440 --> 02:02:00.440]   and whatever action happens in our body that we control.
[02:02:00.440 --> 02:02:03.180]   If you have a kid grow up with a third limb,
[02:02:03.180 --> 02:02:06.600]   I'm sure they'll figure out how to control them
[02:02:06.600 --> 02:02:09.440]   probably at the same rate as their natural limbs.
[02:02:09.440 --> 02:02:13.360]   - And a lot of the work would be done by the,
[02:02:13.360 --> 02:02:15.560]   if a third limb is a computer,
[02:02:15.560 --> 02:02:18.520]   you kind of have a, not a faith,
[02:02:18.520 --> 02:02:23.520]   but a thought that the brain might be able to figure out.
[02:02:23.880 --> 02:02:26.480]   Like the plasticity would come from the brain.
[02:02:26.480 --> 02:02:29.000]   Like the brain would be cleverer than the machine at first.
[02:02:29.000 --> 02:02:30.000]   - When I talk about a third limb,
[02:02:30.000 --> 02:02:30.920]   that's exactly what I'm saying.
[02:02:30.920 --> 02:02:33.600]   An artificial limb that basically just controls your mouse
[02:02:33.600 --> 02:02:34.600]   while you're typing.
[02:02:34.600 --> 02:02:36.640]   Perfectly natural thing.
[02:02:36.640 --> 02:02:39.020]   I mean, again, in a few hundred years.
[02:02:39.020 --> 02:02:41.600]   - Maybe sooner than that.
[02:02:41.600 --> 02:02:46.080]   - But basically, as long as the machine is consistent
[02:02:46.080 --> 02:02:49.800]   in the way that it will respond to your brain impulses,
[02:02:49.800 --> 02:02:51.680]   you'll figure out how to control that
[02:02:51.680 --> 02:02:53.920]   and you could play tennis with your third limb.
[02:02:53.920 --> 02:02:57.520]   And let me go back to consistency.
[02:02:57.520 --> 02:03:01.280]   People who have dramatic accidents
[02:03:01.280 --> 02:03:03.920]   that basically take out a whole chunk of their brain
[02:03:03.920 --> 02:03:07.040]   can be taught to co-opt other parts of the brain
[02:03:07.040 --> 02:03:08.560]   to then control that part.
[02:03:08.560 --> 02:03:10.840]   You can basically build up that tissue again
[02:03:10.840 --> 02:03:13.480]   and eventually train your body how to walk again
[02:03:13.480 --> 02:03:15.400]   and how to read again and how to play again
[02:03:15.400 --> 02:03:16.240]   and how to think again,
[02:03:16.240 --> 02:03:18.080]   how to speak a language again, et cetera.
[02:03:18.080 --> 02:03:21.280]   So there's a massive amount of malleability
[02:03:21.280 --> 02:03:26.280]   that happens naturally in our way of controlling our body,
[02:03:26.280 --> 02:03:29.040]   our brain, our thoughts, our vocal cords,
[02:03:29.040 --> 02:03:30.760]   our limbs, et cetera.
[02:03:30.760 --> 02:03:35.640]   And human-machine interfaces are all inevitable
[02:03:35.640 --> 02:03:39.240]   if we sort of figure out how to read these electric impulses
[02:03:39.240 --> 02:03:43.420]   but the resolution at which we can understand human thought
[02:03:43.420 --> 02:03:46.560]   right now is nil, is ridiculous.
[02:03:46.560 --> 02:03:49.160]   So how are human thoughts encoded?
[02:03:49.160 --> 02:03:53.560]   It's basically combinations of neurons that co-fire
[02:03:53.560 --> 02:03:55.720]   and these create these things called engrams
[02:03:55.720 --> 02:03:58.940]   that eventually form memories and so on and so forth.
[02:03:58.940 --> 02:04:01.940]   We know nothing of all that stuff.
[02:04:01.940 --> 02:04:05.600]   So before we can actually read into your brain
[02:04:05.600 --> 02:04:07.120]   that you wanna build a program that does this
[02:04:07.120 --> 02:04:10.960]   and this and this and that, we need a lot of neuroscience.
[02:04:10.960 --> 02:04:13.480]   - Well, so to push back on that,
[02:04:13.480 --> 02:04:16.680]   do you think it's possible that without understanding
[02:04:16.680 --> 02:04:20.000]   the functionally about the brain or from the neuroscience
[02:04:20.000 --> 02:04:22.080]   or the cognitive science or psychology,
[02:04:22.080 --> 02:04:24.220]   whichever level of the brain we'll look at,
[02:04:24.220 --> 02:04:26.700]   do you think if we just connect them,
[02:04:26.700 --> 02:04:29.200]   just like per your previous point,
[02:04:29.200 --> 02:04:30.840]   if we just have a high enough resolution
[02:04:30.840 --> 02:04:34.400]   between connection between Wikipedia and your brain,
[02:04:34.400 --> 02:04:38.160]   the brain will just figure it out with less understanding?
[02:04:38.160 --> 02:04:40.320]   Because that's one of the innovations of Neuralink
[02:04:40.320 --> 02:04:44.040]   is they're increasing the number of connections to the brain
[02:04:44.040 --> 02:04:48.000]   to several thousand, which before was in the dozens
[02:04:48.000 --> 02:04:48.840]   or whatever.
[02:04:48.840 --> 02:04:51.040]   - You're still off by a few orders of magnitude,
[02:04:51.040 --> 02:04:52.080]   on the order of seven.
[02:04:52.080 --> 02:04:54.320]   (both laughing)
[02:04:54.320 --> 02:04:57.520]   - Right, but the thing is, the hope is if you increase
[02:04:57.520 --> 02:04:58.820]   that number more and more and more,
[02:04:58.820 --> 02:05:00.640]   maybe you don't need to understand anything
[02:05:00.640 --> 02:05:04.520]   about the actual, how human thought is represented
[02:05:04.520 --> 02:05:05.340]   in the brain.
[02:05:05.340 --> 02:05:08.040]   You can just let it figure it out by itself.
[02:05:08.040 --> 02:05:09.800]   - Yeah, like when Keanu Reeves waking up and saying,
[02:05:09.800 --> 02:05:10.720]   "I know ku-ku-fu."
[02:05:10.720 --> 02:05:11.560]   - Yeah, exactly.
[02:05:11.560 --> 02:05:13.200]   (both laughing)
[02:05:13.200 --> 02:05:14.640]   So yeah, sure.
[02:05:14.640 --> 02:05:16.760]   - You don't have faith in the plasticity of the brain
[02:05:16.760 --> 02:05:18.280]   to that degree?
[02:05:18.280 --> 02:05:19.840]   - It's not about brain plasticity.
[02:05:19.840 --> 02:05:21.920]   It's about the input aspect.
[02:05:21.920 --> 02:05:23.760]   Basically, I think on the output aspect,
[02:05:23.760 --> 02:05:25.480]   being able to control a machine is something
[02:05:25.480 --> 02:05:28.480]   that you can probably train your neural impulses
[02:05:28.480 --> 02:05:30.960]   that you're sending out to sort of match
[02:05:30.960 --> 02:05:33.320]   whatever response you see in the environment.
[02:05:33.320 --> 02:05:35.640]   If this thing moved every single time I thought
[02:05:35.640 --> 02:05:37.360]   a particular thought, then I could figure out,
[02:05:37.360 --> 02:05:39.560]   I could hack my way into moving this thing
[02:05:39.560 --> 02:05:40.960]   with just a series of thoughts.
[02:05:40.960 --> 02:05:44.640]   I could think, "Guitar, piano, tennis ball."
[02:05:44.640 --> 02:05:45.880]   (both laughing)
[02:05:45.880 --> 02:05:47.560]   And then this thing would be moving.
[02:05:47.560 --> 02:05:50.640]   And then I would just have the series of thoughts
[02:05:50.640 --> 02:05:52.640]   that would sort of result in the impulses
[02:05:52.640 --> 02:05:53.960]   that will move this thing the way that I want.
[02:05:53.960 --> 02:05:55.560]   And then eventually it'll become natural
[02:05:55.560 --> 02:05:57.640]   'cause I won't even think about it.
[02:05:57.640 --> 02:05:59.120]   I mean, the same way that we control our limbs
[02:05:59.120 --> 02:06:00.240]   in a very natural way.
[02:06:00.240 --> 02:06:01.360]   But babies don't do that.
[02:06:01.360 --> 02:06:03.160]   Babies have to figure it out.
[02:06:03.160 --> 02:06:04.860]   And some of that is hard-coded,
[02:06:04.860 --> 02:06:06.800]   but some of that is actually learned
[02:06:06.800 --> 02:06:10.320]   based on whatever soup of neurons you ended up with,
[02:06:10.320 --> 02:06:13.440]   whatever connections you pruned them to,
[02:06:13.440 --> 02:06:15.360]   and eventually you were born with.
[02:06:15.360 --> 02:06:17.740]   A lot of that is coded in the genome,
[02:06:17.740 --> 02:06:19.680]   but a huge chunk of that is stochastic
[02:06:19.680 --> 02:06:21.320]   in sort of the way that you sort of create
[02:06:21.320 --> 02:06:23.440]   all these neurons, they migrate, they form connections,
[02:06:23.440 --> 02:06:25.140]   they sort of spread out,
[02:06:25.140 --> 02:06:26.520]   they have particular branching patterns,
[02:06:26.520 --> 02:06:28.200]   but then the connectivity itself,
[02:06:28.200 --> 02:06:30.120]   unique in every single new person.
[02:06:30.120 --> 02:06:34.000]   All this to say that on the output side,
[02:06:34.000 --> 02:06:37.320]   absolutely, I'm very, very hopeful
[02:06:37.320 --> 02:06:40.000]   that we can have machines that read
[02:06:40.000 --> 02:06:42.800]   thousands of these neuronal connections on the output side,
[02:06:42.800 --> 02:06:45.580]   but on the input side, oh boy.
[02:06:45.580 --> 02:06:51.240]   I don't expect any time in the near future
[02:06:51.240 --> 02:06:53.400]   we'll be able to sort of send a series of impulses
[02:06:53.400 --> 02:06:56.280]   that will tell me, oh, Earth to sun distance,
[02:06:56.280 --> 02:07:00.720]   7.5 million, et cetera, like nowhere.
[02:07:00.720 --> 02:07:04.480]   I mean, I think language will still be the input way
[02:07:04.480 --> 02:07:07.340]   rather than sort of any kind of more complex.
[02:07:07.340 --> 02:07:08.760]   - It's a really interesting notion
[02:07:08.760 --> 02:07:11.840]   that the ambiguity of language is a feature.
[02:07:11.840 --> 02:07:12.680]   - Yeah.
[02:07:12.680 --> 02:07:16.520]   - And we evolved for millions of years
[02:07:16.520 --> 02:07:19.520]   to take advantage of that ambiguity.
[02:07:19.520 --> 02:07:20.520]   - Exactly.
[02:07:20.520 --> 02:07:23.380]   And yet no one teaches us the subtle differences
[02:07:23.380 --> 02:07:26.100]   between words that are near cognates,
[02:07:26.100 --> 02:07:30.760]   and yet evoke so much more than one from the other.
[02:07:30.760 --> 02:07:34.520]   And yet, when you're choosing words
[02:07:34.520 --> 02:07:36.840]   from a list of 20 synonyms,
[02:07:36.840 --> 02:07:40.040]   you know exactly the connotation of every single one of them.
[02:07:40.040 --> 02:07:42.600]   And that's something that is there.
[02:07:42.600 --> 02:07:45.100]   So yes, there's ambiguity,
[02:07:45.100 --> 02:07:46.800]   but there's all kinds of connotations.
[02:07:46.800 --> 02:07:48.880]   And in the way that we select our words,
[02:07:48.880 --> 02:07:51.320]   we have so much baggage that we're sending along,
[02:07:51.320 --> 02:07:52.980]   the way that we're emoting,
[02:07:52.980 --> 02:07:54.720]   the way that we're moving our hands
[02:07:54.720 --> 02:07:56.080]   every single time we speak,
[02:07:56.080 --> 02:07:58.880]   the pauses, the eye contact, et cetera,
[02:07:58.880 --> 02:08:01.800]   so much higher baud rate than just a vocal,
[02:08:01.800 --> 02:08:04.040]   you know, string of characters.
[02:08:04.040 --> 02:08:07.120]   - Well, let me just take a small tangent on that.
[02:08:07.120 --> 02:08:08.760]   - Oh, tangent, we haven't done that yet.
[02:08:08.760 --> 02:08:09.600]   - We haven't done that.
[02:08:09.600 --> 02:08:10.440]   - That's a good idea, let's do a tangent.
[02:08:10.440 --> 02:08:12.320]   (laughing)
[02:08:12.320 --> 02:08:14.480]   - We'll return to the origin of life after.
[02:08:14.480 --> 02:08:17.800]   So, I mean, you're Greek,
[02:08:17.800 --> 02:08:20.860]   but I'm going on this personal journey.
[02:08:20.860 --> 02:08:25.080]   I'm going to Paris for the explicit purpose
[02:08:25.080 --> 02:08:29.360]   of talking to one of the most famous,
[02:08:29.360 --> 02:08:33.200]   a couple who's a famous translators of Russian literature,
[02:08:33.200 --> 02:08:36.280]   Dostoevsky, Tolstoy, and they go,
[02:08:36.280 --> 02:08:38.560]   that's their art, is the translation.
[02:08:38.560 --> 02:08:44.320]   Everything I've learned about the translation art,
[02:08:44.320 --> 02:08:46.100]   it makes me feel,
[02:08:46.100 --> 02:08:53.160]   it's so profound in a way that's so much more profound
[02:08:53.240 --> 02:08:55.400]   than the natural language processing papers
[02:08:55.400 --> 02:08:57.440]   I read in the machine learning community,
[02:08:57.440 --> 02:09:00.440]   that there's such depth to language
[02:09:00.440 --> 02:09:03.160]   that I don't know what to do with.
[02:09:03.160 --> 02:09:05.720]   I don't know if you've experienced that in your own life
[02:09:05.720 --> 02:09:07.960]   with knowing multiple languages.
[02:09:07.960 --> 02:09:11.720]   I don't know what to, I don't know how to make sense of it,
[02:09:11.720 --> 02:09:13.640]   but there's so much loss in translation
[02:09:13.640 --> 02:09:17.440]   between Russian and English, and getting a sense of that.
[02:09:17.440 --> 02:09:20.440]   Like, for example, there's like,
[02:09:20.440 --> 02:09:23.400]   just taking a single sentence from Dostoevsky,
[02:09:23.400 --> 02:09:25.440]   and there's a lot of them.
[02:09:25.440 --> 02:09:27.560]   You could talk for hours about
[02:09:27.560 --> 02:09:30.120]   how to translate that sentence properly.
[02:09:30.120 --> 02:09:34.360]   That captures the meaning, the period,
[02:09:34.360 --> 02:09:36.540]   the culture, the humor, the wit,
[02:09:36.540 --> 02:09:39.760]   the suffering that was in the context of the time,
[02:09:39.760 --> 02:09:42.280]   all of that could be a single sentence.
[02:09:42.280 --> 02:09:45.680]   You could talk forever about
[02:09:45.680 --> 02:09:47.160]   what it takes to translate that correctly.
[02:09:47.160 --> 02:09:48.720]   I don't know what to do with that.
[02:09:48.720 --> 02:09:53.440]   So being Greek, it's very hard for me to think of a sentence
[02:09:53.440 --> 02:09:58.200]   or even a word without going into the full etymology
[02:09:58.200 --> 02:10:03.200]   of that word, breaking up every single atom of that sentence,
[02:10:03.200 --> 02:10:07.080]   and every single atom of these words,
[02:10:07.080 --> 02:10:08.860]   and rebuilding it back up.
[02:10:08.860 --> 02:10:13.680]   I have three kids, and the way that I teach them Greek
[02:10:13.680 --> 02:10:17.620]   is the same way that the documentary was mentioning earlier
[02:10:17.620 --> 02:10:19.720]   about sort of understanding the deep roots
[02:10:19.720 --> 02:10:21.900]   of all of these words.
[02:10:21.900 --> 02:10:28.880]   And it's very interesting
[02:10:28.880 --> 02:10:31.320]   that every single time I hear a new word
[02:10:31.320 --> 02:10:33.020]   that I've never heard before,
[02:10:33.020 --> 02:10:34.720]   I go and figure out the etymology of that word,
[02:10:34.720 --> 02:10:36.760]   because I will never appreciate that word
[02:10:36.760 --> 02:10:39.260]   without understanding how it was initially formed.
[02:10:39.260 --> 02:10:41.000]   - Interesting.
[02:10:41.000 --> 02:10:42.080]   But how does that help?
[02:10:42.080 --> 02:10:44.080]   Because that's not the full picture.
[02:10:44.080 --> 02:10:44.920]   - No, no, of course, of course.
[02:10:44.920 --> 02:10:48.360]   But what I'm trying to say is that knowing the components
[02:10:48.360 --> 02:10:52.260]   teaches you about the context of the formation of that word
[02:10:52.260 --> 02:10:55.120]   and sort of the original usage of that word.
[02:10:55.120 --> 02:10:57.360]   And then of course, the word takes new meaning
[02:10:57.360 --> 02:11:00.820]   as you create it from its parts.
[02:11:00.820 --> 02:11:04.120]   And that meaning then gets augmented,
[02:11:04.120 --> 02:11:08.160]   and two synonyms that sort of have different roots
[02:11:08.160 --> 02:11:09.200]   will actually have implications
[02:11:09.200 --> 02:11:11.440]   that carry a lot of that baggage
[02:11:11.440 --> 02:11:14.220]   of the historical provenance of these words.
[02:11:14.220 --> 02:11:16.620]   So before working on genome evolution,
[02:11:16.620 --> 02:11:19.920]   my passion was evolution of language
[02:11:19.920 --> 02:11:23.720]   and sort of tracing cognates across different languages
[02:11:23.720 --> 02:11:27.280]   through their etymologies.
[02:11:27.280 --> 02:11:30.280]   - And that's fascinating that there's parallels between,
[02:11:30.280 --> 02:11:34.260]   I mean, the idea that there's evolutionary dynamics
[02:11:34.260 --> 02:11:35.500]   to our language.
[02:11:35.500 --> 02:11:37.920]   - Yeah.
[02:11:37.920 --> 02:11:42.600]   In every single word that you utter, parallels, parallels.
[02:11:42.600 --> 02:11:43.880]   What does parallels mean?
[02:11:43.880 --> 02:11:47.560]   Para means side by side, alleles from alleles,
[02:11:47.560 --> 02:11:50.800]   which means identical twins, parallels.
[02:11:50.800 --> 02:11:54.240]   I mean, name any word, and there's so much baggage,
[02:11:54.240 --> 02:11:58.040]   so much beauty in how that word came to be
[02:11:58.040 --> 02:12:00.040]   and how this word took a new meaning
[02:12:00.040 --> 02:12:01.420]   than the sum of its parts.
[02:12:01.420 --> 02:12:06.120]   - Yeah, and they're just words.
[02:12:06.120 --> 02:12:07.920]   They don't have any physical grounding.
[02:12:07.920 --> 02:12:10.240]   - Exactly, and now you take these words
[02:12:10.240 --> 02:12:12.640]   and you weave them into a sentence.
[02:12:13.600 --> 02:12:18.600]   The emotional invocations of that weaving are fathomless.
[02:12:18.600 --> 02:12:23.280]   - And all of those emotions all live
[02:12:23.280 --> 02:12:25.480]   in the brains of humans.
[02:12:25.480 --> 02:12:27.060]   - In the eye of the beholder.
[02:12:27.060 --> 02:12:30.840]   No, seriously, you have to embrace this concept
[02:12:30.840 --> 02:12:32.440]   of the eye of the beholder.
[02:12:32.440 --> 02:12:37.440]   It's the conceptualization that nothing takes meaning
[02:12:37.440 --> 02:12:39.400]   with one person creating it.
[02:12:39.400 --> 02:12:42.480]   Everything takes meaning in the receiving end.
[02:12:42.480 --> 02:12:45.160]   And the emergent properties
[02:12:45.160 --> 02:12:47.760]   of these communication networks,
[02:12:47.760 --> 02:12:50.960]   where every single, if you look at the network of our cells
[02:12:50.960 --> 02:12:52.480]   and how they're communicating with each other,
[02:12:52.480 --> 02:12:54.200]   every cell has its own code.
[02:12:54.200 --> 02:12:56.200]   This code is modulated by the epigenome.
[02:12:56.200 --> 02:12:57.960]   This creates a bunch of different cell types.
[02:12:57.960 --> 02:13:00.000]   Each cell type now has its own identity,
[02:13:00.000 --> 02:13:02.200]   yet they all have the common root of the stem cells
[02:13:02.200 --> 02:13:03.660]   that sort of led to them.
[02:13:03.660 --> 02:13:06.600]   Each of these identities is now communicating
[02:13:06.600 --> 02:13:08.120]   with each other.
[02:13:08.120 --> 02:13:11.800]   They take meaning in their interaction.
[02:13:11.800 --> 02:13:13.760]   There's an emergent property that comes
[02:13:13.760 --> 02:13:15.680]   from a bunch of cells being together
[02:13:15.680 --> 02:13:17.920]   that is not in any one of the parts.
[02:13:17.920 --> 02:13:19.320]   If you look at neurons communicating,
[02:13:19.320 --> 02:13:23.360]   again, these engrams don't exist in any one neuron.
[02:13:23.360 --> 02:13:26.440]   They exist in the connection, in the combination of neurons.
[02:13:26.440 --> 02:13:29.000]   And the meaning of the words that I'm telling you
[02:13:29.000 --> 02:13:33.000]   is empty until it reaches you
[02:13:33.000 --> 02:13:35.200]   and it affects you in a very different way
[02:13:35.200 --> 02:13:38.480]   than it affects whoever's listening to this conversation now.
[02:13:38.480 --> 02:13:41.400]   Because of the emotional baggage that I've grown up with,
[02:13:41.400 --> 02:13:42.240]   that you've grown up with,
[02:13:42.240 --> 02:13:44.320]   and that they've grown up with.
[02:13:44.320 --> 02:13:47.800]   And that's, I think, the magic of translation.
[02:13:47.800 --> 02:13:49.720]   If you start thinking of translation
[02:13:49.720 --> 02:13:54.720]   as just simply capturing that emotional set of reactions
[02:13:54.720 --> 02:14:00.960]   that you evoke, you need a different set of words
[02:14:00.960 --> 02:14:04.320]   to evoke that same set of reactions to a French person
[02:14:04.320 --> 02:14:05.680]   than to a Russian person,
[02:14:05.680 --> 02:14:08.360]   because of the baggage of the culture that we grew up in.
[02:14:08.360 --> 02:14:09.880]   - Yeah, I mean, there's--
[02:14:09.880 --> 02:14:13.400]   - So basically, you shouldn't find the best word.
[02:14:13.400 --> 02:14:16.000]   Sometimes it's a completely different sentence structure
[02:14:16.000 --> 02:14:17.000]   that you will need,
[02:14:17.000 --> 02:14:21.960]   matched to the cultural context
[02:14:21.960 --> 02:14:23.480]   of the target audience that you have.
[02:14:23.480 --> 02:14:26.180]   - Yeah, I mean, you're just,
[02:14:26.180 --> 02:14:27.400]   I usually don't think about this,
[02:14:27.400 --> 02:14:29.680]   but right now there's this feeling,
[02:14:29.680 --> 02:14:32.340]   as a reminder, there's just you and I talking,
[02:14:32.340 --> 02:14:34.960]   but there's several hundred thousand people
[02:14:34.960 --> 02:14:36.080]   will listen to this.
[02:14:36.080 --> 02:14:38.320]   There's some guy in Russia right now
[02:14:38.320 --> 02:14:43.320]   running, like in Moscow, listening to us.
[02:14:43.320 --> 02:14:46.360]   And there's somebody in India, I guarantee you,
[02:14:46.360 --> 02:14:48.340]   there's somebody in China and South America,
[02:14:48.340 --> 02:14:50.360]   there's somebody in Texas,
[02:14:50.360 --> 02:14:52.960]   and they all have different--
[02:14:52.960 --> 02:14:53.960]   - Emotional baggage.
[02:14:53.960 --> 02:14:56.040]   - They probably got angry earlier on
[02:14:56.040 --> 02:14:58.240]   about the whole discussion about coronavirus
[02:14:58.240 --> 02:15:01.960]   and about some aspect of it.
[02:15:01.960 --> 02:15:04.840]   Yeah, and there's that network effect.
[02:15:04.840 --> 02:15:05.680]   - Yeah, yeah, yeah.
[02:15:05.680 --> 02:15:06.840]   - That's--
[02:15:06.840 --> 02:15:07.880]   - It's a beautiful thing.
[02:15:07.880 --> 02:15:10.760]   - And this lateral transfer of information,
[02:15:10.760 --> 02:15:12.800]   that's what makes the collective, quote-unquote,
[02:15:12.800 --> 02:15:17.800]   genome of humanity so unique from any other species.
[02:15:17.800 --> 02:15:22.600]   - So you somehow miraculously wrapped it back
[02:15:22.600 --> 02:15:25.120]   to the very beginning of when we were talking
[02:15:25.120 --> 02:15:27.820]   about the beauty of the human genome.
[02:15:27.820 --> 02:15:31.220]   So I think this is the right time,
[02:15:31.220 --> 02:15:34.840]   unless we wanna go for a six to eight hour conversation.
[02:15:34.840 --> 02:15:35.960]   We're gonna have to talk again,
[02:15:35.960 --> 02:15:39.100]   but I think for now, to wrap it up,
[02:15:39.100 --> 02:15:40.900]   this is the right time to talk about
[02:15:40.900 --> 02:15:44.920]   the biggest, most ridiculous question of all,
[02:15:44.920 --> 02:15:45.880]   meaning of life.
[02:15:45.880 --> 02:15:50.120]   Off mic, you mentioned to me that you had
[02:15:50.120 --> 02:15:55.120]   your 42nd birthday, 42nd being a very special,
[02:15:55.120 --> 02:15:59.520]   absurdly special number, and you had a kind of
[02:15:59.520 --> 02:16:04.360]   get together with friends to discuss the meaning of life.
[02:16:04.360 --> 02:16:08.400]   So let me ask you, in your, as a biologist,
[02:16:08.400 --> 02:16:11.320]   as a computer scientist, and as a human,
[02:16:11.320 --> 02:16:14.660]   what is the meaning of life?
[02:16:14.660 --> 02:16:18.960]   - I've been asking this question for a long time,
[02:16:18.960 --> 02:16:22.120]   ever since my 42nd birthday, but well before that,
[02:16:22.120 --> 02:16:25.320]   in even planning the Meaning of Life Symposium.
[02:16:25.320 --> 02:16:29.800]   And symposium, sym means together,
[02:16:29.800 --> 02:16:31.560]   posy actually means to drink together.
[02:16:31.560 --> 02:16:33.560]   So symposium is actually a drinking party.
[02:16:33.560 --> 02:16:35.640]   (laughing)
[02:16:35.640 --> 02:16:37.320]   - Can you actually elaborate about this
[02:16:37.320 --> 02:16:39.520]   Meaning of Life Symposium that you put together?
[02:16:39.520 --> 02:16:42.320]   It's like the most genius idea I've ever heard.
[02:16:42.320 --> 02:16:44.640]   - So 42 is obviously the answer to life,
[02:16:44.640 --> 02:16:45.600]   the universe, and everything,
[02:16:45.600 --> 02:16:47.660]   from the Hitchhiker's Guide to the Galaxy.
[02:16:47.660 --> 02:16:50.680]   And as I was turning 42, I've had the theme
[02:16:50.680 --> 02:16:51.800]   for every one of my birthdays.
[02:16:51.800 --> 02:16:54.920]   When I was turning 32, it's one zero zero,
[02:16:54.920 --> 02:16:56.640]   zero zero zero in binary.
[02:16:56.640 --> 02:17:00.080]   So I celebrated my 100,000th binary birthday,
[02:17:00.080 --> 02:17:02.760]   and I had the theme of going back 100,000 years,
[02:17:02.760 --> 02:17:07.160]   you know, let's dress something in the last 100,000 years.
[02:17:07.160 --> 02:17:09.560]   Anyway, I've always had these--
[02:17:09.560 --> 02:17:12.280]   - That's such an interesting human being.
[02:17:12.280 --> 02:17:13.120]   Okay, that's awesome.
[02:17:13.120 --> 02:17:16.240]   - I've always had these sort of numerology
[02:17:16.240 --> 02:17:20.400]   related announcements for my birthday party.
[02:17:20.400 --> 02:17:21.740]   (laughing)
[02:17:21.740 --> 02:17:26.740]   So what came out of that Meaning of Life Symposium
[02:17:26.740 --> 02:17:29.680]   is that I basically asked 42 of my colleagues,
[02:17:29.680 --> 02:17:33.040]   42 of my friends, 42 of my collaborators,
[02:17:33.040 --> 02:17:35.480]   to basically give seven-minute speeches
[02:17:35.480 --> 02:17:38.480]   on the meaning of life, each from their perspective.
[02:17:38.480 --> 02:17:40.600]   And I really encourage you to go there,
[02:17:40.600 --> 02:17:44.200]   'cause it's mind-boggling that every single person
[02:17:44.200 --> 02:17:46.280]   said a different answer.
[02:17:46.280 --> 02:17:48.400]   Every single person started with,
[02:17:48.400 --> 02:17:50.920]   "I don't know what the meaning of life is, but,"
[02:17:50.920 --> 02:17:54.240]   and then gave this beautifully, eloquently answer,
[02:17:54.240 --> 02:17:55.440]   eloquent answer.
[02:17:55.440 --> 02:17:57.280]   And they were all different,
[02:17:57.280 --> 02:18:01.300]   but they all were consistent with each other
[02:18:01.300 --> 02:18:04.340]   and mutually synergistic and together forming
[02:18:04.340 --> 02:18:07.520]   a beautiful view of what it means to be human in many ways.
[02:18:07.520 --> 02:18:12.280]   Some people talked about the loss of their loved one,
[02:18:12.280 --> 02:18:14.520]   their life partner for many, many years,
[02:18:14.520 --> 02:18:16.520]   and how their life changed through that.
[02:18:16.520 --> 02:18:19.260]   Some people talked about the origin of life.
[02:18:19.260 --> 02:18:21.080]   Some people talked about the difference
[02:18:21.080 --> 02:18:23.000]   between purpose and meaning.
[02:18:24.160 --> 02:18:28.560]   I'll maybe quote one of the answers,
[02:18:28.560 --> 02:18:30.840]   which is this linguistics professor,
[02:18:30.840 --> 02:18:32.480]   a friend of mine at Harvard,
[02:18:32.480 --> 02:18:36.600]   who basically said that she was gonna,
[02:18:36.600 --> 02:18:37.800]   she's Greek as well,
[02:18:37.800 --> 02:18:40.120]   and she said it will give a very Pythian answer.
[02:18:40.120 --> 02:18:42.960]   So Pythia was the oracle of Delphi,
[02:18:42.960 --> 02:18:45.280]   who would basically give these very cryptic answers,
[02:18:45.280 --> 02:18:48.320]   very short, but interpretable in many different ways.
[02:18:48.320 --> 02:18:50.480]   There was this whole set of priests
[02:18:50.480 --> 02:18:53.440]   who were tasked with interpreting what Pythia had said,
[02:18:53.440 --> 02:18:56.440]   and very often you would not get a clean interpretation,
[02:18:56.440 --> 02:18:58.920]   but she said, "I will be like Pythia
[02:18:58.920 --> 02:19:00.840]   "and give you a very short
[02:19:00.840 --> 02:19:02.480]   "and multiply interpretable answer,
[02:19:02.480 --> 02:19:04.100]   "but unlike her, I will actually
[02:19:04.100 --> 02:19:06.040]   "also give you three interpretations."
[02:19:06.040 --> 02:19:09.740]   And she said, "The answer to the meaning of life
[02:19:09.740 --> 02:19:11.140]   "is become one."
[02:19:11.140 --> 02:19:15.080]   And the first interpretation is,
[02:19:15.080 --> 02:19:17.660]   like a child, become one year old
[02:19:17.660 --> 02:19:19.340]   with the excitement of discovering
[02:19:19.340 --> 02:19:21.400]   everything about the world.
[02:19:21.400 --> 02:19:25.020]   Second interpretation, in whatever you take on,
[02:19:25.020 --> 02:19:28.840]   become one, the first, the best, excel,
[02:19:28.840 --> 02:19:32.760]   drive yourself to perfection for every one of your tasks.
[02:19:32.760 --> 02:19:37.760]   And become one when people are separate,
[02:19:37.760 --> 02:19:42.040]   become one, come together, learn to understand each other.
[02:19:42.040 --> 02:19:45.400]   - Damn, that's an answer.
[02:19:45.400 --> 02:19:46.880]   - And one way to summarize
[02:19:46.880 --> 02:19:48.760]   this whole meaning of life symposium
[02:19:48.760 --> 02:19:52.920]   is that the very symposium was illustrating
[02:19:52.920 --> 02:19:54.680]   the quest for meaning,
[02:19:54.680 --> 02:19:58.120]   which might itself be the meaning of life.
[02:19:58.120 --> 02:20:01.400]   This constant quest for something sublime,
[02:20:01.400 --> 02:20:04.900]   something human, something intangible,
[02:20:04.900 --> 02:20:08.600]   some aspect of what defines us
[02:20:08.600 --> 02:20:11.320]   as a species and as an individual,
[02:20:11.320 --> 02:20:16.320]   both the quest of me as a person through my own life,
[02:20:16.360 --> 02:20:19.200]   but the meaning of life could also be
[02:20:19.200 --> 02:20:20.840]   the meaning of all of life.
[02:20:20.840 --> 02:20:22.040]   What is the whole point of life?
[02:20:22.040 --> 02:20:22.880]   Why life?
[02:20:22.880 --> 02:20:24.480]   Why life itself?
[02:20:24.480 --> 02:20:26.680]   'Cause we've been talking about the history
[02:20:26.680 --> 02:20:28.320]   and evolution of life,
[02:20:28.320 --> 02:20:31.040]   but we haven't talked about why life in the first place.
[02:20:31.040 --> 02:20:32.480]   Is life inevitable?
[02:20:32.480 --> 02:20:35.840]   Is life part of physics?
[02:20:35.840 --> 02:20:37.680]   Does life transcend physics?
[02:20:37.680 --> 02:20:40.280]   By fighting against entropy,
[02:20:40.280 --> 02:20:42.920]   by compartmentalizing and increasing concentrations
[02:20:42.920 --> 02:20:44.380]   rather than diluting away,
[02:20:45.280 --> 02:20:50.280]   is life a distinct entity in the universe
[02:20:50.280 --> 02:20:55.080]   beyond the traditional, very simple physical rules
[02:20:55.080 --> 02:20:58.520]   that govern gravity and electromagnetism
[02:20:58.520 --> 02:21:00.640]   and all of these forces?
[02:21:00.640 --> 02:21:02.120]   Is life another force?
[02:21:02.120 --> 02:21:03.100]   Is there a life force?
[02:21:03.100 --> 02:21:05.920]   Is there a unique kind of set of principles that emerge,
[02:21:05.920 --> 02:21:09.100]   of course, built on top of the hardware of physics,
[02:21:09.100 --> 02:21:11.820]   but is it sort of a new layer of software
[02:21:11.820 --> 02:21:14.400]   or a new layer of a computer system?
[02:21:14.400 --> 02:21:18.480]   So that's at the level of big questions.
[02:21:18.480 --> 02:21:21.200]   There's another aspect of gratitude,
[02:21:21.200 --> 02:21:26.200]   of basically, what I like to say is,
[02:21:26.200 --> 02:21:27.920]   during this pandemic,
[02:21:27.920 --> 02:21:30.800]   I've basically worked from 6 a.m. until 7 p.m.
[02:21:30.800 --> 02:21:34.280]   every single day, nonstop, including Saturday and Sunday.
[02:21:34.280 --> 02:21:36.440]   I've basically broken all boundaries
[02:21:36.440 --> 02:21:41.440]   of where personal life begins and work life ends.
[02:21:42.000 --> 02:21:46.280]   And that has been exhilarating for me,
[02:21:46.280 --> 02:21:50.480]   just the intellectual pleasure that I get
[02:21:50.480 --> 02:21:53.820]   from a day of exhaustion,
[02:21:53.820 --> 02:21:55.520]   where at the end of the day, my brain is hurting,
[02:21:55.520 --> 02:21:59.400]   I'm telling my wife, "Wow, I was useful today."
[02:21:59.400 --> 02:22:04.720]   And there's a certain pleasure
[02:22:04.720 --> 02:22:08.360]   that comes from feeling useful.
[02:22:08.360 --> 02:22:09.880]   And there's a certain pleasure
[02:22:09.880 --> 02:22:12.480]   that comes from feeling grateful.
[02:22:12.480 --> 02:22:16.440]   So I've written this little sort of prayer for my kids
[02:22:16.440 --> 02:22:19.520]   to say at bedtime every night,
[02:22:19.520 --> 02:22:20.940]   where they basically say,
[02:22:20.940 --> 02:22:24.720]   "Thank you, God, for all you have given me
[02:22:24.720 --> 02:22:28.580]   and give me the strength to give unto others
[02:22:28.580 --> 02:22:31.320]   with the same love that you have given unto me."
[02:22:31.320 --> 02:22:35.680]   We as a species are so special.
[02:22:36.560 --> 02:22:39.520]   The only ones who worry about the meaning of life.
[02:22:39.520 --> 02:22:43.320]   And maybe that's what makes us human.
[02:22:43.320 --> 02:22:47.960]   And what I like to say to my wife and to my students
[02:22:47.960 --> 02:22:52.240]   during this pandemic work extravaganza
[02:22:52.240 --> 02:22:56.400]   is every now and then they ask me, "But how do you do this?"
[02:22:56.400 --> 02:22:58.880]   And I'm like, "I'm a workaholic.
[02:22:58.880 --> 02:23:00.880]   I love this.
[02:23:00.880 --> 02:23:04.800]   This is me in the most unfiltered way.
[02:23:04.800 --> 02:23:07.280]   The ability to do something useful,
[02:23:07.280 --> 02:23:09.640]   to feel that my brain's being used,
[02:23:09.640 --> 02:23:12.580]   to interact with the smartest people on the planet
[02:23:12.580 --> 02:23:14.120]   day in, day out,
[02:23:14.120 --> 02:23:17.120]   and to help them discover aspects of the human genome,
[02:23:17.120 --> 02:23:21.940]   of the human brain, of human disease and the human condition
[02:23:21.940 --> 02:23:24.560]   that no one has seen before
[02:23:24.560 --> 02:23:27.000]   with data that we're capturing
[02:23:27.000 --> 02:23:29.860]   that has never been observed.
[02:23:29.860 --> 02:23:34.480]   And there's another aspect, which is on the personal life.
[02:23:34.480 --> 02:23:36.120]   Many people say, "Oh, I'm not gonna have kids."
[02:23:36.120 --> 02:23:37.560]   Why bother?
[02:23:37.560 --> 02:23:39.160]   I can tell you as a father,
[02:23:39.160 --> 02:23:44.580]   they're missing half the picture, if not the whole picture.
[02:23:44.580 --> 02:23:50.160]   Teaching my kids about my view of the world
[02:23:50.160 --> 02:23:52.360]   and watching through their eyes
[02:23:52.360 --> 02:23:54.600]   the naivete with which they start
[02:23:54.600 --> 02:23:57.040]   and the sophistication with which they end up,
[02:23:57.040 --> 02:24:01.160]   the understanding that they have
[02:24:01.160 --> 02:24:04.000]   of not just the natural world around them, but of me too.
[02:24:05.000 --> 02:24:10.000]   The unfiltered criticism that you get
[02:24:10.000 --> 02:24:16.320]   from your own children that knows no bounds of honesty.
[02:24:16.320 --> 02:24:22.840]   And I've grown components of my heart
[02:24:22.840 --> 02:24:24.840]   that I didn't know I had
[02:24:24.840 --> 02:24:29.020]   until you sense that fragility,
[02:24:29.020 --> 02:24:32.680]   that vulnerability of the children
[02:24:33.280 --> 02:24:37.000]   that immense love and passion,
[02:24:37.000 --> 02:24:42.200]   the unfiltered egoism that we as adults
[02:24:42.200 --> 02:24:44.280]   learn how to hide so much better.
[02:24:44.280 --> 02:24:48.080]   It's just this back of emotions
[02:24:48.080 --> 02:24:50.980]   that tell me about the raw materials
[02:24:50.980 --> 02:24:53.000]   that make a human being
[02:24:53.000 --> 02:24:55.280]   and how these raw materials can be arranged
[02:24:55.280 --> 02:24:58.600]   with more sophistication that we learn through life
[02:24:58.600 --> 02:25:01.080]   to become truly human adults.
[02:25:02.160 --> 02:25:04.320]   But there's something so beautiful
[02:25:04.320 --> 02:25:07.260]   about seeing that progression between them,
[02:25:07.260 --> 02:25:10.600]   the complexity of the language growing
[02:25:10.600 --> 02:25:12.800]   as more neuronal connections are formed,
[02:25:12.800 --> 02:25:18.540]   to realize that the hardware is getting rearranged
[02:25:18.540 --> 02:25:21.580]   as their software is getting implemented on that hardware,
[02:25:21.580 --> 02:25:24.860]   that their frontal cortex continues to grow
[02:25:24.860 --> 02:25:26.140]   for another 10 years.
[02:25:26.140 --> 02:25:29.920]   There's neuronal connections that are continuing to form,
[02:25:29.920 --> 02:25:33.080]   new neurons that actually get replicated and formed.
[02:25:33.080 --> 02:25:38.080]   And it's just incredible that we have these,
[02:25:38.080 --> 02:25:40.680]   not just you grow the hardware for 30 years
[02:25:40.680 --> 02:25:42.640]   and then you feed it all of the knowledge.
[02:25:42.640 --> 02:25:45.200]   No, no, the knowledge is fed throughout
[02:25:45.200 --> 02:25:48.480]   and is shaping these neural connections as they're forming.
[02:25:48.480 --> 02:25:52.840]   So seeing that transformation from either your own blood
[02:25:52.840 --> 02:25:54.560]   or from an adopted child
[02:25:54.560 --> 02:25:57.520]   is the most beautiful thing you can do as a human being.
[02:25:57.520 --> 02:25:58.640]   And it completes you,
[02:25:58.640 --> 02:26:00.760]   it completes that path, that journey.
[02:26:00.760 --> 02:26:04.880]   The create life, oh sure, that's at conception, that's easy.
[02:26:04.880 --> 02:26:08.400]   But create human life to add the human part,
[02:26:08.400 --> 02:26:13.160]   that takes decades of compassion, of sharing,
[02:26:13.160 --> 02:26:18.160]   of love and of anger and of impatience and patience.
[02:26:18.160 --> 02:26:21.880]   And as a parent,
[02:26:21.880 --> 02:26:25.060]   I think I've become a very different kind of teacher.
[02:26:25.060 --> 02:26:27.080]   Because again, I'm a professor,
[02:26:27.080 --> 02:26:31.040]   my first role is to bring adult human beings
[02:26:31.040 --> 02:26:34.440]   into a more mature level of adulthood,
[02:26:34.440 --> 02:26:37.040]   where they learn not just to do science,
[02:26:37.040 --> 02:26:39.840]   but they learn the process of discovery
[02:26:39.840 --> 02:26:41.200]   and the process of collaboration,
[02:26:41.200 --> 02:26:42.280]   the process of sharing,
[02:26:42.280 --> 02:26:44.840]   the process of conveying the knowledge,
[02:26:44.840 --> 02:26:48.000]   of encapsulating something incredibly complex
[02:26:48.000 --> 02:26:51.200]   and sort of giving it up in sort of bite-sized chunks
[02:26:51.200 --> 02:26:54.400]   that the rest of humanity can appreciate.
[02:26:54.400 --> 02:26:55.680]   I tell my students all the time,
[02:26:55.680 --> 02:26:58.760]   if you, you know, like when an apple falls,
[02:26:58.760 --> 02:27:00.840]   when a tree falls in the forest
[02:27:00.840 --> 02:27:03.040]   and no one's there to listen, has it really fallen?
[02:27:03.040 --> 02:27:05.280]   The same way you do this awesome research,
[02:27:05.280 --> 02:27:06.700]   if you write an impenetrable paper
[02:27:06.700 --> 02:27:08.640]   that no one will understand,
[02:27:08.640 --> 02:27:11.040]   it's as if you never did the awesome research.
[02:27:11.040 --> 02:27:12.520]   So conveying of knowledge,
[02:27:12.520 --> 02:27:15.200]   conveying this lateral transfer
[02:27:15.200 --> 02:27:17.540]   that I was talking about at the very beginning
[02:27:17.540 --> 02:27:22.500]   of sort of humanity and sort of the sharing of information,
[02:27:22.500 --> 02:27:27.220]   all of that has gotten so much more rich
[02:27:27.220 --> 02:27:30.980]   by seeing human beings grow in my own home,
[02:27:30.980 --> 02:27:35.100]   because that makes me a better parent
[02:27:35.100 --> 02:27:36.980]   and that makes me a better teacher
[02:27:36.980 --> 02:27:41.980]   and a better mentor to the nurturing of my adult children,
[02:27:41.980 --> 02:27:44.020]   which are my research group.
[02:27:44.020 --> 02:27:45.780]   - First of all, beautifully put,
[02:27:45.780 --> 02:27:49.480]   connects beautifully to the vertical
[02:27:49.480 --> 02:27:52.260]   and the horizontal inheritance of ideas
[02:27:52.260 --> 02:27:54.460]   that we talked about at the very beginning.
[02:27:54.460 --> 02:27:57.380]   I don't think there's a better way to end it
[02:27:57.380 --> 02:28:01.340]   on this poetic and powerful note.
[02:28:01.340 --> 02:28:03.100]   Manolis, thank you so much for talking to us.
[02:28:03.100 --> 02:28:03.940]   A huge honor.
[02:28:03.940 --> 02:28:07.280]   We'll have to talk again about the origin of life,
[02:28:07.280 --> 02:28:10.540]   about epigenetics, epigenomics,
[02:28:10.540 --> 02:28:13.620]   and some of the incredible research you're doing.
[02:28:13.620 --> 02:28:14.460]   Truly an honor.
[02:28:14.460 --> 02:28:15.300]   Thanks so much for talking to me.
[02:28:15.300 --> 02:28:16.460]   - Thank you, such a pleasure.
[02:28:16.460 --> 02:28:17.280]   It's such a pleasure.
[02:28:17.280 --> 02:28:19.100]   I mean, your questions are outstanding.
[02:28:19.100 --> 02:28:20.580]   I've had such a blast here.
[02:28:20.580 --> 02:28:21.920]   I can't wait to be back.
[02:28:21.920 --> 02:28:23.260]   - Awesome.
[02:28:23.260 --> 02:28:24.860]   Thanks for listening to this conversation
[02:28:24.860 --> 02:28:25.980]   with Manolis Kellis,
[02:28:25.980 --> 02:28:29.060]   and thank you to our sponsors, Blinkist,
[02:28:29.060 --> 02:28:31.420]   8sleep, and Masterclass.
[02:28:31.420 --> 02:28:33.260]   Please consider supporting this podcast
[02:28:33.260 --> 02:28:37.680]   by going to blinkist.com/lex, 8sleep.com/lex,
[02:28:37.680 --> 02:28:41.100]   and masterclass.com/lex.
[02:28:41.100 --> 02:28:44.220]   Click the links, buy the stuff, get the discount.
[02:28:44.220 --> 02:28:47.100]   It's the best way to support this podcast.
[02:28:47.100 --> 02:28:48.820]   If you enjoy this thing, subscribe on YouTube,
[02:28:48.820 --> 02:28:50.940]   review it with the five stars on Apple Podcasts,
[02:28:50.940 --> 02:28:52.300]   support it on Patreon,
[02:28:52.300 --> 02:28:55.480]   or connect with me on Twitter @LexFriedman.
[02:28:55.480 --> 02:28:57.380]   And now let me leave you with some words
[02:28:57.380 --> 02:28:58.940]   from Charles Darwin
[02:28:58.940 --> 02:29:01.680]   that I think Manolis represents quite beautifully.
[02:29:01.680 --> 02:29:04.780]   "If I had my life to live over again,
[02:29:04.780 --> 02:29:07.460]   "I would have made a rule to read some poetry
[02:29:07.460 --> 02:29:11.660]   "and listen to some music at least once every week."
[02:29:11.660 --> 02:29:14.900]   Thank you for listening, and hope to see you next time.
[02:29:14.900 --> 02:29:17.480]   (upbeat music)
[02:29:17.480 --> 02:29:20.060]   (upbeat music)
[02:29:20.060 --> 02:29:30.060]   [BLANK_AUDIO]

