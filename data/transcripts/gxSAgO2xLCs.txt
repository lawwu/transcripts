
[00:00:00.000 --> 00:00:11.360]   Everyone is being exposed to a little behind the scenes.
[00:00:11.360 --> 00:00:16.880]   Awesome.
[00:00:16.880 --> 00:00:18.080]   I think we're live on YouTube.
[00:00:18.080 --> 00:00:18.960]   So that's perfect.
[00:00:18.960 --> 00:00:21.040]   Hello, everyone.
[00:00:21.040 --> 00:00:22.160]   Welcome to the first.
[00:00:22.160 --> 00:00:25.600]   This is the second session, but the first one where we actually
[00:00:25.600 --> 00:00:26.480]   go to the material.
[00:00:26.480 --> 00:00:29.360]   The previous one was a special session with the author.
[00:00:29.360 --> 00:00:33.760]   I would recommend checking that recording out just because
[00:00:33.760 --> 00:00:35.760]   Thomas had shared so much of his wisdom.
[00:00:35.760 --> 00:00:37.760]   But it's great to see all of you on a Sunday.
[00:00:37.760 --> 00:00:39.920]   I was worried about posting these on a Sunday, and it's
[00:00:39.920 --> 00:00:43.760]   incredible to see that everyone is joining on a Sunday as well.
[00:00:43.760 --> 00:00:47.360]   I'm excited to learn more about PyTorch along with you.
[00:00:47.360 --> 00:00:50.800]   I started my journey with TensorFlow.
[00:00:50.800 --> 00:00:54.240]   Not a good choice in hindsight.
[00:00:54.240 --> 00:00:55.440]   Nothing against TensorFlow.
[00:00:55.440 --> 00:00:58.800]   At that time, it was slightly different framework than PyTorch.
[00:00:59.440 --> 00:01:02.720]   Since then, I switched to PyTorch, and it's been an incredible journey for me.
[00:01:02.720 --> 00:01:08.320]   So, yes, I'm taking the session while standing.
[00:01:08.320 --> 00:01:09.760]   I have a fancy setup.
[00:01:09.760 --> 00:01:13.760]   I enjoy playing around with my setup.
[00:01:13.760 --> 00:01:20.000]   So quick reminder to everyone, please redirect your questions to this link.
[00:01:20.000 --> 00:01:21.360]   I'll be posting in the chat.
[00:01:21.360 --> 00:01:28.640]   Since we'll be live across YouTube and also Zoom, I won't be able to monitor
[00:01:28.640 --> 00:01:32.320]   both of those, and I'll quickly walk you through how the setup works.
[00:01:32.320 --> 00:01:34.000]   Awesome.
[00:01:34.000 --> 00:01:41.280]   So we're one minute past the mark, which means I can get started.
[00:01:41.280 --> 00:01:43.440]   This was the link I had pasted in the chat.
[00:01:43.440 --> 00:01:47.840]   Make sure you head over to this link, and then I'll walk you through how the forums work.
[00:01:48.480 --> 00:01:48.480]   Great.
[00:01:48.480 --> 00:02:04.080]   So the agenda for today or the title for today is PyTorch Basics, Sensors, and Neural Networks.
[00:02:04.080 --> 00:02:06.480]   I might not get to the last bit.
[00:02:06.480 --> 00:02:10.800]   And also, quick note to everyone, I was a little sick yesterday.
[00:02:10.800 --> 00:02:12.560]   I was quite dehydrated.
[00:02:12.560 --> 00:02:13.760]   I had a stomach infection.
[00:02:13.760 --> 00:02:17.440]   I was down 2 kgs in one day, which means my energy will be low a little bit.
[00:02:17.920 --> 00:02:23.360]   But please don't mistake that for my enthusiasm for PyTorch being low.
[00:02:23.360 --> 00:02:27.680]   So if you find my energy being a little low, please excuse that.
[00:02:27.680 --> 00:02:33.200]   I again quickly want to mention our previous session with Thomas Freeman.
[00:02:33.200 --> 00:02:38.720]   He's one of the co-developers of PyTorch, also the co-authors of the book that we're reading.
[00:02:38.720 --> 00:02:41.840]   He was very generous with his time, and he joined us last week.
[00:02:42.480 --> 00:02:47.360]   He has also-- you can find the recording on our page.
[00:02:47.360 --> 00:02:51.040]   And he has also-- I'm trying to find the link.
[00:02:51.040 --> 00:03:04.480]   On his website, written a blog about the questions he had answered.
[00:03:04.480 --> 00:03:08.480]   This is an incredible write-up about community, learning with community,
[00:03:09.120 --> 00:03:14.080]   and ways of contributing to PyTorch, which I'm sure everyone would like to do with time.
[00:03:14.080 --> 00:03:17.600]   So please make sure that you check the blog post out.
[00:03:17.600 --> 00:03:21.760]   I'll again post this in the forum link as well.
[00:03:21.760 --> 00:03:23.600]   And I'll quickly go over what the forums are.
[00:03:23.600 --> 00:03:30.800]   So this is the second session of our PyTorch book reading group.
[00:03:30.800 --> 00:03:32.720]   This is being hosted by Weights and Biases.
[00:03:32.720 --> 00:03:38.800]   I vote there, but as you know, we're trying to create this community where all of us,
[00:03:38.800 --> 00:03:43.040]   by all of us, I mean deep learning enthusiasts, get to hang out, get to learn together.
[00:03:43.040 --> 00:03:46.240]   And I'm really excited about hosting all of these sessions.
[00:03:46.240 --> 00:03:52.000]   I know a few of you have been joining Aman's session, and he's just set up such a high mark
[00:03:52.000 --> 00:03:54.080]   for me to continue at.
[00:03:54.080 --> 00:03:59.280]   So I'm excited to continue the journey and carry the torch forward, pun intended.
[00:03:59.280 --> 00:04:02.800]   Here's the agenda for today.
[00:04:02.800 --> 00:04:04.640]   I'll introduce the structure.
[00:04:04.640 --> 00:04:07.360]   We'll glance through chapter 1, 2, 3.
[00:04:07.360 --> 00:04:11.760]   What I mean by that is I won't be reading through all of the chapters to be respectful
[00:04:11.760 --> 00:04:12.720]   of your time.
[00:04:12.720 --> 00:04:16.240]   So if you've read ahead of time-- I know Ravi did.
[00:04:16.240 --> 00:04:18.240]   I saw his comment.
[00:04:18.240 --> 00:04:23.840]   If you have read ahead of time, I'll sort of give a high-level revision and a glance
[00:04:23.840 --> 00:04:25.520]   through of these chapters.
[00:04:25.520 --> 00:04:30.160]   If you've not read ahead of time, I'll be, again, going over the important bits, in my
[00:04:30.160 --> 00:04:30.660]   opinion.
[00:04:30.660 --> 00:04:33.600]   And you can read over the week.
[00:04:34.480 --> 00:04:38.960]   I would also spend time understanding the code in these chapters, explaining what's
[00:04:38.960 --> 00:04:41.520]   going on, or just glancing through the important bits.
[00:04:41.520 --> 00:04:47.600]   For the first three chapters, it's on the lower end, but it'll get pretty dense really
[00:04:47.600 --> 00:04:49.040]   fast, starting from chapter 4.
[00:04:49.040 --> 00:04:54.560]   The reason we'll be going over the first three chapters for this session is-- the simple
[00:04:54.560 --> 00:04:58.240]   reason is many people are new to PyTorch, and I also want to be respectful of their
[00:04:58.240 --> 00:04:58.640]   learning.
[00:04:58.640 --> 00:05:05.920]   So if you're new to deep learning or PyTorch, we'll make sure that you understand everything.
[00:05:05.920 --> 00:05:08.640]   And how will we make sure of that?
[00:05:08.640 --> 00:05:14.320]   If you could head over to this link, this should take you to our forums.
[00:05:14.320 --> 00:05:19.600]   I'll, again, post this in the chart once I'm able to copy it correctly.
[00:05:19.600 --> 00:05:27.120]   Let me make sure it's there in the chart as well.
[00:05:27.120 --> 00:05:32.800]   And in the YouTube chart.
[00:05:32.800 --> 00:05:34.800]   Awesome.
[00:05:34.800 --> 00:05:36.560]   So these are discourse forums.
[00:05:36.560 --> 00:05:37.600]   I really like these.
[00:05:37.600 --> 00:05:40.560]   Fast.ai uses these, so I'm quite accustomed to them.
[00:05:40.560 --> 00:05:44.080]   And these are incredible for me because I don't have to monitor three different sources
[00:05:44.080 --> 00:05:44.560]   of charts.
[00:05:44.560 --> 00:05:49.360]   And the chart actually does get retained, so someone who's watching the recording later
[00:05:49.360 --> 00:05:52.800]   on, someone who didn't join us on a Sunday won't be worried about what are these guys
[00:05:52.800 --> 00:05:53.280]   talking about.
[00:05:53.280 --> 00:05:54.960]   They can see the questions.
[00:05:56.880 --> 00:05:58.640]   And I can ask questions right here.
[00:05:58.640 --> 00:06:03.360]   How long have you all been working in PyTorch?
[00:06:03.360 --> 00:06:13.520]   Make sure you like my topic because I'm trying to beat Aman in the number of likes.
[00:06:13.520 --> 00:06:14.560]   He's an incredible guy.
[00:06:14.560 --> 00:06:15.200]   I'm just kidding.
[00:06:15.200 --> 00:06:18.160]   This is how these forums work.
[00:06:18.160 --> 00:06:19.680]   So please ask the questions there.
[00:06:19.680 --> 00:06:24.000]   Quick reminder about the reading group structure.
[00:06:24.000 --> 00:06:26.240]   You'd already signed up if you're in the Zoom call.
[00:06:26.240 --> 00:06:32.720]   If not, if you're watching on YouTube, you can sign up using this link, bondb.me/pytorchreading.
[00:06:32.720 --> 00:06:34.720]   That should take you to a registration page.
[00:06:34.720 --> 00:06:39.040]   That'll be helpful to just send you reminders every weekend.
[00:06:39.040 --> 00:06:42.240]   Although I hope you keep coming back to learn along with us.
[00:06:42.240 --> 00:06:45.680]   This will be on Sundays at 8am Pacific.
[00:06:45.680 --> 00:06:48.640]   The recording will be made available shortly after.
[00:06:48.640 --> 00:06:52.880]   And this will be paced independent of the meetings.
[00:06:52.880 --> 00:06:56.800]   By that I mean, since the first three chapters are somewhat introductory,
[00:06:56.800 --> 00:06:59.760]   I won't spend the first three meetings on the first three chapters.
[00:06:59.760 --> 00:07:03.600]   I'll rather sift through them in a pace that makes sense,
[00:07:03.600 --> 00:07:06.640]   depending on how dense the content is.
[00:07:06.640 --> 00:07:11.120]   As I mentioned, I was a little sick yesterday.
[00:07:11.120 --> 00:07:15.600]   So I did plan to do some exercises, I think, that'll be released over the week.
[00:07:15.600 --> 00:07:18.640]   We'll be releasing walkthroughs and exercises throughout the weeks to
[00:07:19.600 --> 00:07:23.920]   keep you all in the loop and make sure you all practice.
[00:07:23.920 --> 00:07:26.640]   So you can find those on our forums.
[00:07:26.640 --> 00:07:30.640]   And we'll also be giving away swag bondibi t-shirts
[00:07:30.640 --> 00:07:34.400]   to the winners of these quizzes and exercises.
[00:07:34.400 --> 00:07:38.880]   I'm still teasing this, so we're still finalizing all of the details.
[00:07:38.880 --> 00:07:43.040]   But there'll be at least as of now five guest sessions with PyTorch-based developers.
[00:07:43.040 --> 00:07:43.920]   So a lot to learn.
[00:07:43.920 --> 00:07:46.160]   A lot of things that I'm excited about.
[00:07:46.160 --> 00:07:52.640]   [BLANK_AUDIO]
[00:07:52.640 --> 00:07:53.920]   Awesome. So here's the book structure.
[00:07:53.920 --> 00:07:55.600]   The book is structured in three parts.
[00:07:55.600 --> 00:07:57.680]   I do have the physical copy on my desk.
[00:07:57.680 --> 00:07:59.600]   I also have the digital copy.
[00:07:59.600 --> 00:08:03.040]   You can, again, find these links in the forum post I had linked off.
[00:08:03.040 --> 00:08:04.800]   It's structured in three parts.
[00:08:04.800 --> 00:08:06.800]   The first one is just the basics.
[00:08:06.800 --> 00:08:10.480]   It talks about what is Torch, the history of the framework,
[00:08:10.480 --> 00:08:12.560]   what are pre-trained models.
[00:08:13.120 --> 00:08:17.600]   I'm just glancing over these topics, and we'll get into the details as we get to the chapters.
[00:08:17.600 --> 00:08:23.600]   PyTorch has these fundamental building blocks, also the native data structure called tensors.
[00:08:23.600 --> 00:08:29.600]   So we look at those, followed by all of the nitty-gritty details of what a neural network is.
[00:08:29.600 --> 00:08:30.560]   A little caveat.
[00:08:30.560 --> 00:08:35.600]   This book doesn't get into the NLP world of things.
[00:08:35.600 --> 00:08:36.400]   And for that--
[00:08:36.400 --> 00:08:40.000]   [BLANK_AUDIO]
[00:08:40.000 --> 00:08:41.360]   Let me see if I can--
[00:08:41.360 --> 00:08:44.000]   [BLANK_AUDIO]
[00:08:44.000 --> 00:08:46.960]   take you to our YouTube page and point out the correct--
[00:08:46.960 --> 00:08:50.800]   [BLANK_AUDIO]
[00:08:50.800 --> 00:08:51.360]   playlist.
[00:08:51.360 --> 00:08:56.320]   We did run a study group a little while ago.
[00:08:56.320 --> 00:08:59.840]   This was the FastA/HuggingFace study group.
[00:08:59.840 --> 00:09:04.800]   So if you're interested in learning something PyTorch and NLP, you can check these out.
[00:09:04.800 --> 00:09:11.200]   But this book doesn't get into a lot of details of NLP.
[00:09:11.920 --> 00:09:19.280]   The part two of the book actually teaches us how to build an end-to-end project.
[00:09:19.280 --> 00:09:28.080]   So we learn how to build a CT image model from scratch right up to the end.
[00:09:28.080 --> 00:09:30.880]   And we build upon it block by block.
[00:09:30.880 --> 00:09:33.600]   And towards the end of these chapters I've read ahead,
[00:09:33.600 --> 00:09:36.560]   we're able to have this really nice and complete pipeline.
[00:09:38.160 --> 00:09:41.200]   The third part, which not a lot of books teach really well--
[00:09:41.200 --> 00:09:42.560]   the FastA book does.
[00:09:42.560 --> 00:09:46.560]   This book also luckily does-- is deploying your model.
[00:09:46.560 --> 00:09:50.000]   So how do you take this model, this pipeline that you've created,
[00:09:50.000 --> 00:09:51.920]   and how do you serve it to users?
[00:09:51.920 --> 00:09:53.840]   Whatever that means.
[00:09:53.840 --> 00:09:57.040]   If you're new to these jargons, you'll learn very soon.
[00:09:57.040 --> 00:09:59.600]   What are the prerequisites?
[00:09:59.600 --> 00:10:01.520]   Some Python knowledge is good to have.
[00:10:01.520 --> 00:10:04.080]   Now, do you need to know inheritance?
[00:10:04.080 --> 00:10:05.680]   Do you need to know OOP concepts?
[00:10:06.240 --> 00:10:07.120]   Not really.
[00:10:07.120 --> 00:10:08.240]   You could get around with it.
[00:10:08.240 --> 00:10:13.440]   A GPU environment for the second part is actually quite important.
[00:10:13.440 --> 00:10:15.360]   In my opinion, Colab is pretty great.
[00:10:15.360 --> 00:10:18.560]   I have a nice box set up in my background.
[00:10:18.560 --> 00:10:20.000]   Don't let that discourage you.
[00:10:20.000 --> 00:10:22.320]   Colab will still get you really far with this book.
[00:10:22.320 --> 00:10:26.560]   Most importantly, you should enjoy this.
[00:10:26.560 --> 00:10:28.720]   That's my job to make sure that you all enjoy this.
[00:10:28.720 --> 00:10:31.200]   So please let me know if something's not clear.
[00:10:31.200 --> 00:10:34.400]   We are a community, and I'm happy to take questions any time.
[00:10:35.280 --> 00:10:38.320]   So as long as you spend enough time,
[00:10:38.320 --> 00:10:41.440]   my suggestion is spend at least twice as much time
[00:10:41.440 --> 00:10:44.080]   playing with notebooks as much as you spend reading.
[00:10:44.080 --> 00:10:47.360]   That's one of my regrets also with my deep learning journey.
[00:10:47.360 --> 00:10:48.720]   You'll be fine.
[00:10:48.720 --> 00:10:49.680]   I can guarantee that.
[00:10:49.680 --> 00:10:54.080]   And I'll also keep dropping a few tips on how to do that.
[00:10:54.080 --> 00:10:57.200]   So what is AI?
[00:10:57.200 --> 00:10:59.920]   Now I'm going through the first three chapters.
[00:10:59.920 --> 00:11:02.320]   Let me also make sure I'm not missing anything
[00:11:02.320 --> 00:11:04.800]   or if the live stream is working correctly.
[00:11:05.040 --> 00:11:09.120]   I love this reply.
[00:11:09.120 --> 00:11:11.440]   Nothing against TensorFlow.
[00:11:11.440 --> 00:11:15.360]   All right, looks like no one's complaining, so things are fine.
[00:11:15.360 --> 00:11:18.080]   Now what I'm doing is I've highlighted different sections
[00:11:18.080 --> 00:11:20.800]   from the book that I'll be just summarizing here
[00:11:20.800 --> 00:11:25.120]   as in order of appearance in the chapter.
[00:11:25.120 --> 00:11:26.640]   So this is from the first chapter.
[00:11:26.640 --> 00:11:28.960]   What is AI?
[00:11:28.960 --> 00:11:32.080]   Broadly speaking, it's this huge subset of different techniques.
[00:11:32.080 --> 00:11:35.840]   AI is more of the marketing jargon, as I would like to say.
[00:11:35.840 --> 00:11:39.040]   Machine learning is somewhat of a broader example.
[00:11:39.040 --> 00:11:43.200]   And deep learning is this nice little smallest subset.
[00:11:43.200 --> 00:11:45.440]   Now this is also quite huge in itself.
[00:11:45.440 --> 00:11:49.840]   But this is to show you how broad the field generally is.
[00:11:49.840 --> 00:11:52.320]   And there are quite a few things outside of deep learning.
[00:11:52.320 --> 00:11:55.360]   We won't be covering those, of course.
[00:11:55.360 --> 00:11:58.160]   These include autoencoders, regression problems.
[00:11:59.280 --> 00:12:02.400]   We might have a few study groups launching at bits and bytes soon.
[00:12:02.400 --> 00:12:06.320]   So don't mind the fact that we're just learning deep learning.
[00:12:06.320 --> 00:12:08.800]   Although it's quite a huge field, so we're not missing out.
[00:12:08.800 --> 00:12:11.920]   Why is deep learning so important?
[00:12:11.920 --> 00:12:14.800]   I'll go to the book to talk about this.
[00:12:14.800 --> 00:12:17.680]   But I'd like to ask your perspective on--
[00:12:17.680 --> 00:12:22.240]   let's say you're trying to predict if an image has a dog in it.
[00:12:22.240 --> 00:12:26.080]   What are the ideal steps that we should be taking for deep learning?
[00:12:26.080 --> 00:12:30.320]   And as you answer these questions, I'll go to them once your answers appear.
[00:12:30.320 --> 00:12:34.560]   So I'm asking you right now, please share your answers on what do you do
[00:12:34.560 --> 00:12:38.080]   or what are the logical steps of trying to predict what's in an image.
[00:12:38.080 --> 00:12:43.760]   So about this deep learning versus traditional machine learning paradigm.
[00:12:43.760 --> 00:12:47.600]   Deep learning is really exciting because earlier--
[00:12:47.600 --> 00:12:49.920]   or at least with traditional machine learning,
[00:12:49.920 --> 00:12:53.120]   there used to be a lot of hand crafting of features.
[00:12:53.120 --> 00:12:56.640]   So there used to be a lot of knobs you were required to find.
[00:12:56.640 --> 00:13:00.560]   That's not the case with deep learning anymore.
[00:13:00.560 --> 00:13:03.360]   There are hyper parameters.
[00:13:03.360 --> 00:13:08.720]   There are a lot of things, but not as many as traditional machine learning.
[00:13:08.720 --> 00:13:09.920]   What do I mean by that?
[00:13:09.920 --> 00:13:14.560]   Let me switch my screens.
[00:13:14.560 --> 00:13:17.360]   One note.
[00:13:20.560 --> 00:13:23.360]   So let's say I had asked a question about deep learning.
[00:13:23.360 --> 00:13:26.560]   I'll try to answer that in terms of traditional machine learning.
[00:13:26.560 --> 00:13:28.800]   Now let's say our target is to--
[00:13:28.800 --> 00:13:32.800]   please forgive my handwriting--
[00:13:32.800 --> 00:13:34.960]   to predict if there's a dog in an image.
[00:13:34.960 --> 00:13:38.080]   In the traditional world of machine learning,
[00:13:38.080 --> 00:13:41.440]   you'd first, let's say, take the image.
[00:13:41.440 --> 00:13:46.000]   Once that's in the memory, you try to extract the edges.
[00:13:46.240 --> 00:13:48.720]   Or maybe even before that--
[00:13:48.720 --> 00:13:50.320]   so let's say this is step two.
[00:13:50.320 --> 00:13:56.400]   Step one would be you'd want to run some form of blurring on the image.
[00:13:56.400 --> 00:14:00.640]   You'd want to make the image more blurred, less blurred,
[00:14:00.640 --> 00:14:02.560]   somewhat sharp, somewhat less sharp.
[00:14:02.560 --> 00:14:08.080]   So you'd like to correct the--
[00:14:08.080 --> 00:14:11.600]   let's say blur.
[00:14:13.040 --> 00:14:16.080]   Or image details.
[00:14:16.080 --> 00:14:19.760]   After that, you might want to convert it to black and white.
[00:14:19.760 --> 00:14:22.400]   Let's say then you get to extracting the edges.
[00:14:22.400 --> 00:14:28.880]   And then you would have some form of, let's say, an SVM that performs these tasks.
[00:14:28.880 --> 00:14:32.400]   As you can see, this is four steps.
[00:14:32.400 --> 00:14:33.280]   And all of these--
[00:14:33.280 --> 00:14:37.120]   I'll maybe look up a few things to show you how much of an effort this requires.
[00:14:37.120 --> 00:14:39.200]   For deep learning, all you have to do--
[00:14:39.200 --> 00:14:41.440]   there are a few more details.
[00:14:41.440 --> 00:14:44.320]   But for now, you can simply input your data.
[00:14:44.320 --> 00:14:45.440]   It goes into a model.
[00:14:45.440 --> 00:14:46.880]   And you get the output.
[00:14:46.880 --> 00:14:49.760]   The deep learning model learns the representations.
[00:14:49.760 --> 00:14:52.720]   And you don't have to fine tune as many things.
[00:14:52.720 --> 00:14:56.880]   I'll switch back to the correct screen.
[00:14:56.880 --> 00:15:08.960]   What I'm trying to show here is how many number of parameters
[00:15:08.960 --> 00:15:11.120]   are required to be manually tuned.
[00:15:11.120 --> 00:15:14.320]   So this is known as smoothing of images.
[00:15:14.320 --> 00:15:18.880]   And OpenCV2 is one of the go-to frameworks for anything
[00:15:18.880 --> 00:15:21.520]   relating to image processing.
[00:15:21.520 --> 00:15:31.520]   I was trying to show how many different parameters
[00:15:31.520 --> 00:15:33.440]   need to be taken care of here.
[00:15:33.440 --> 00:15:35.200]   So I'm just trying to find the correct--
[00:15:37.200 --> 00:15:39.200]   documentation.
[00:15:39.200 --> 00:15:42.480]   So for every single step that I pointed out earlier,
[00:15:42.480 --> 00:15:44.800]   you need to know all of these details.
[00:15:44.800 --> 00:15:48.160]   And these are required to be manually tuned by you.
[00:15:48.160 --> 00:15:51.120]   And as we learn for deep learning, it's much more--
[00:15:51.120 --> 00:15:54.080]   much less, actually, much less--
[00:15:54.080 --> 00:15:56.160]   sorry about that--
[00:15:56.160 --> 00:15:57.600]   detailed.
[00:15:57.600 --> 00:15:59.840]   And it's much better than that.
[00:15:59.840 --> 00:16:03.760]   The other reason being deep learning systems are much better at
[00:16:03.760 --> 00:16:08.240]   not getting confused or are less brittle compared to
[00:16:08.240 --> 00:16:10.720]   traditional machine learning algorithms, since, again,
[00:16:10.720 --> 00:16:11.840]   these are quite fine.
[00:16:11.840 --> 00:16:15.920]   Now I'm also putting these into two different buckets.
[00:16:15.920 --> 00:16:17.040]   That's not really the case.
[00:16:17.040 --> 00:16:19.040]   But generally speaking, this applies really well.
[00:16:19.040 --> 00:16:27.920]   So how do we create a deep learning model?
[00:16:27.920 --> 00:16:28.480]   I will--
[00:16:30.720 --> 00:16:32.720]   see if anyone had pointed the answers.
[00:16:32.720 --> 00:16:33.280]   No, they did not.
[00:16:33.280 --> 00:16:34.240]   So I'll go through this.
[00:16:34.240 --> 00:16:39.600]   All we need to do is you need to load the data from the memory.
[00:16:39.600 --> 00:16:43.440]   So you need to figure out a way of taking images.
[00:16:43.440 --> 00:16:45.520]   Let's say you're trying to build an image classifier.
[00:16:45.520 --> 00:16:48.480]   You load it to your computer's memory, because remember,
[00:16:48.480 --> 00:16:51.600]   it needs to be on the memory and not on your hard drive.
[00:16:51.600 --> 00:16:55.440]   Some pre-processing is required.
[00:16:55.440 --> 00:16:56.880]   I didn't lie earlier.
[00:16:57.760 --> 00:17:02.160]   This is much lesser than what's going on in traditional machine learning.
[00:17:02.160 --> 00:17:09.120]   Maria is asking which book are we reading.
[00:17:09.120 --> 00:17:10.240]   We're reading the PyTorch--
[00:17:10.240 --> 00:17:11.440]   deep learning with PyTorch book.
[00:17:11.440 --> 00:17:12.640]   I'm summarizing the chapters.
[00:17:12.640 --> 00:17:16.880]   So some pre-processing is required.
[00:17:16.880 --> 00:17:20.800]   Let's say if I stop sharing my screen and I move over
[00:17:20.800 --> 00:17:26.640]   to the edge of the monitor, I can see that I have a lot of
[00:17:26.640 --> 00:17:30.960]   data, the model may not be best at predicting what's going on.
[00:17:30.960 --> 00:17:36.080]   Or if I move further away to the edge, now the center point is the monitor.
[00:17:36.080 --> 00:17:39.120]   Or let's say the CPU, if the model is trained on that.
[00:17:39.120 --> 00:17:42.720]   For this case, assuming you go through the data and you realize that,
[00:17:42.720 --> 00:17:45.920]   hey, every time I'm standing in the edge of the image--
[00:17:45.920 --> 00:17:49.760]   and this happens a lot with Kaggle competitions--
[00:17:49.760 --> 00:17:53.040]   you'd want to pre-process the images to be cropped
[00:17:53.040 --> 00:17:54.800]   towards the edge where I'm standing.
[00:17:56.240 --> 00:18:04.800]   So this is lesser of pre-processing than what is required for deep learning,
[00:18:04.800 --> 00:18:06.400]   for traditional machine learning.
[00:18:06.400 --> 00:18:09.520]   But still, some amount of pre-processing is still required.
[00:18:09.520 --> 00:18:22.400]   All right, so we've loaded our, let's say, images onto the memory.
[00:18:22.400 --> 00:18:25.760]   We've created these into tensors, since computers only
[00:18:25.760 --> 00:18:26.720]   understand numbers.
[00:18:26.720 --> 00:18:27.360]   What are tensors?
[00:18:27.360 --> 00:18:30.000]   We'll learn about that very soon.
[00:18:30.000 --> 00:18:34.800]   Now we need to figure out a way to train the model and then finally evaluate it.
[00:18:34.800 --> 00:18:40.160]   Or understand how good is our CYM classifier, let's say.
[00:18:40.160 --> 00:18:48.640]   There are a gazillion frameworks and a lot of ways to do this.
[00:18:48.640 --> 00:18:54.400]   If you just look up deep learning frameworks,
[00:18:55.280 --> 00:19:01.760]   this should give you an insane number of lists, if I Google that correctly.
[00:19:01.760 --> 00:19:07.440]   Yep, as you can see, we already have eight deep learning frameworks.
[00:19:07.440 --> 00:19:10.640]   I'm sure you can find at least 20 today.
[00:19:10.640 --> 00:19:17.680]   I can tell you JAX, recently came up JAX, is up and coming.
[00:19:17.680 --> 00:19:19.440]   TensorFlow is up and coming.
[00:19:19.440 --> 00:19:21.520]   And PyTorch is what we're here to learn.
[00:19:22.080 --> 00:19:24.480]   I'm also here to tell you it doesn't really matter.
[00:19:24.480 --> 00:19:31.120]   Because once you've understood these concepts, and as a reminder,
[00:19:31.120 --> 00:19:37.120]   the reason we're trying to learn all of this is not to learn PyTorch.
[00:19:37.120 --> 00:19:43.040]   It's a great framework, but it's to learn and apply these techniques to deep learning.
[00:19:43.040 --> 00:19:48.000]   So once you've mastered a framework and you've really understood the concepts
[00:19:48.000 --> 00:19:52.080]   and you know how to apply these things, emphasis on application,
[00:19:52.080 --> 00:19:55.760]   you can take your knowledge and apply it to any framework.
[00:19:55.760 --> 00:20:01.360]   Most of the industry likes TensorFlow, and there's no reason.
[00:20:01.360 --> 00:20:03.200]   The framework came out earlier.
[00:20:03.200 --> 00:20:04.960]   A lot of industries adopted it.
[00:20:04.960 --> 00:20:07.520]   And as you all know, software engineers are stubborn,
[00:20:07.520 --> 00:20:09.600]   so they won't shift over very early.
[00:20:09.600 --> 00:20:11.280]   Also, enterprises have invested.
[00:20:11.280 --> 00:20:16.080]   And I'm also here to tell you that once you've mastered this book,
[00:20:16.080 --> 00:20:19.040]   once you've gone through this book, you can simply take the knowledge
[00:20:19.040 --> 00:20:22.160]   and also learn TensorFlow in a week or two weeks
[00:20:22.160 --> 00:20:25.440]   and work very well with your colleagues.
[00:20:25.440 --> 00:20:31.440]   PyTorch also has, as its name gives away, this nice Pythonic API.
[00:20:31.440 --> 00:20:36.880]   TensorFlow also now defaults to Keras, which also has a similar API structure now.
[00:20:36.880 --> 00:20:41.440]   Tensor is its core data structure.
[00:20:42.800 --> 00:20:46.560]   As I said earlier, when we're loading images from the memory
[00:20:46.560 --> 00:20:50.320]   onto-- from our hard drives onto our computer memory
[00:20:50.320 --> 00:20:55.280]   or onto our graphic card, we're converting it into a number,
[00:20:55.280 --> 00:20:57.040]   since computers only understand numbers.
[00:20:57.040 --> 00:21:00.160]   So we need to put it into a tensor.
[00:21:00.160 --> 00:21:03.040]   Everything in PyTorch is quite structured around that.
[00:21:03.040 --> 00:21:08.640]   As we learn, it also builds on top of an Autograd engine.
[00:21:08.640 --> 00:21:10.000]   Why is that useful?
[00:21:10.000 --> 00:21:12.000]   I leave those details out for now.
[00:21:12.000 --> 00:21:15.600]   We will get to it in, I think, the next [INAUDIBLE]
[00:21:15.600 --> 00:21:20.880]   PyTorch also became really famous since it is, in a way, NumPy for GPU.
[00:21:20.880 --> 00:21:24.000]   If you-- and I'd suggest this as a homework.
[00:21:24.000 --> 00:21:27.440]   You can just write some NumPy code for what we're doing today.
[00:21:27.440 --> 00:21:30.800]   The documentation is really nice.
[00:21:30.800 --> 00:21:33.600]   And you can also try to write the same in PyTorch.
[00:21:33.600 --> 00:21:39.200]   If you really compare it side by side, it's actually pretty damn similar.
[00:21:39.760 --> 00:21:43.280]   So PyTorch is really NumPy for the GPU.
[00:21:43.280 --> 00:21:47.680]   And for deep learning, it's really important to have these accelerators.
[00:21:47.680 --> 00:21:49.600]   So it's quite useful.
[00:21:49.600 --> 00:21:51.200]   And it, of course, adds more.
[00:21:51.200 --> 00:21:54.320]   So I'll again ask this question.
[00:21:54.320 --> 00:21:59.920]   What do you all think is required for a deep learning pipeline
[00:21:59.920 --> 00:22:01.920]   where we're trying to predict images?
[00:22:01.920 --> 00:22:05.360]   And I'll hop over to the forums to find a few answers.
[00:22:05.360 --> 00:22:06.800]   Awesome.
[00:22:06.800 --> 00:22:10.560]   Vinayak, he's a pro of deep learning.
[00:22:10.560 --> 00:22:12.080]   I don't know why he's joining our study groups.
[00:22:12.080 --> 00:22:13.200]   But he's answered the question.
[00:22:13.200 --> 00:22:16.240]   We need to collect the data and label the items.
[00:22:16.240 --> 00:22:17.520]   Select a pre-trained model.
[00:22:17.520 --> 00:22:20.640]   We'll learn what a pre-trained model is in a bit.
[00:22:20.640 --> 00:22:24.720]   And then fine-tune the model, which also we'll learn in a second.
[00:22:24.720 --> 00:22:27.520]   And maybe apply some pre-processing.
[00:22:27.520 --> 00:22:36.080]   No, NumPy will not work on GPU.
[00:22:36.880 --> 00:22:42.240]   I'll wait for these two persons to reply.
[00:22:42.240 --> 00:22:48.240]   So the question again is, what does a pipeline for a deep learning task look like?
[00:22:48.240 --> 00:22:58.640]   You need to clean the data, yes.
[00:22:58.640 --> 00:23:05.600]   This is one of the toughest tasks in deep learning.
[00:23:05.600 --> 00:23:06.640]   It's quite overlooked.
[00:23:06.640 --> 00:23:12.800]   As a reminder to the people watching on YouTube,
[00:23:12.800 --> 00:23:15.600]   I've put the link in the chat to the forum thread.
[00:23:15.600 --> 00:23:19.360]   I'm keeping an eye out on the discussion on there.
[00:23:19.360 --> 00:23:29.280]   There's a question that I saw earlier.
[00:23:29.280 --> 00:23:30.720]   Where do we crop into the image?
[00:23:30.720 --> 00:23:32.640]   It depends when you're looking at the data.
[00:23:34.400 --> 00:23:38.160]   I like Ravi's answer, load the data into memory, pre-process the data,
[00:23:38.160 --> 00:23:42.160]   train the model, and once you've deployed it, you start getting money.
[00:23:42.160 --> 00:23:50.640]   I'll take one more response before I highlight what's in the book.
[00:24:01.520 --> 00:24:06.320]   Ravi, also someone who joins all of our study groups, it's great to see you.
[00:24:06.320 --> 00:24:09.520]   He says pre-process the data, train the model, evaluate.
[00:24:09.520 --> 00:24:13.600]   Ravi knows a lot more deep learning than most of us,
[00:24:13.600 --> 00:24:16.400]   so he knows these details, which we'll get to soon.
[00:24:16.400 --> 00:24:20.000]   But you need a metric to measure how well your model is performing,
[00:24:20.000 --> 00:24:22.160]   and then you make sure its accuracy goes up.
[00:24:22.160 --> 00:24:27.360]   And then you deploy it, and like Ravi said, you make money.
[00:24:30.480 --> 00:24:34.320]   So here's how that pipeline would look in PyTorch.
[00:24:34.320 --> 00:24:37.280]   You get the data, as you all have pointed out correctly.
[00:24:37.280 --> 00:24:38.560]   All of the answers are almost right.
[00:24:38.560 --> 00:24:42.640]   You convert it into tenses for which you use the dataset class,
[00:24:42.640 --> 00:24:45.200]   which is under torch.utils.data.
[00:24:45.200 --> 00:24:51.680]   Now, when I said earlier that please spend at least twice as much time--
[00:24:51.680 --> 00:24:55.760]   I'm quite dehydrated still, so I apologize for messing up.
[00:24:55.760 --> 00:24:59.280]   But when I said earlier, please spend twice as much time
[00:24:59.280 --> 00:25:05.920]   with code as you do with theory, what I want you to do is just look this class up,
[00:25:05.920 --> 00:25:08.160]   and if it doesn't take me to a URL, which it did--
[00:25:08.160 --> 00:25:21.440]   So if you don't Google it with the dots, your browser won't assume it's the URL,
[00:25:21.440 --> 00:25:23.600]   and you would probably land to the documentation.
[00:25:23.920 --> 00:25:29.600]   So take an example, go through the documentation,
[00:25:29.600 --> 00:25:36.800]   and see all of these arguments that you can pass to a function.
[00:25:36.800 --> 00:25:42.960]   So when I said that you can create a dataset class,
[00:25:42.960 --> 00:25:49.120]   or a data loader class, just glance through all of these,
[00:25:49.120 --> 00:25:53.440]   try to go through the examples, and try changing all of these
[00:25:53.440 --> 00:25:59.120]   parameters that you pass to this function.
[00:25:59.120 --> 00:26:03.920]   If you're new to Python, that's why I'm highlighting these keywords.
[00:26:03.920 --> 00:26:05.600]   These are parameters that we're passing.
[00:26:05.600 --> 00:26:10.080]   But again, in my opinion, as you spend time
[00:26:10.080 --> 00:26:13.760]   breaking things or going through enough errors--
[00:26:15.440 --> 00:26:20.720]   sorry-- you would start to build this intuition around all of these details.
[00:26:20.720 --> 00:26:27.520]   So once we have the dataset, we have the data converted into tensors,
[00:26:27.520 --> 00:26:35.440]   loaded into the memory, we use the data loader class, which I just pointed out to do that.
[00:26:35.440 --> 00:26:40.080]   We need to figure out a way to create a neural network.
[00:26:40.080 --> 00:26:44.400]   Anything in deep learning is structured usually around neural networks, sometimes not.
[00:26:44.400 --> 00:26:45.440]   Most of the times, yes.
[00:26:45.440 --> 00:26:49.760]   So you will use the torch.nn class for that.
[00:26:49.760 --> 00:26:54.080]   The reason I'm pointing these out is, again, these are highlighted in the chapters.
[00:26:54.080 --> 00:26:58.080]   And I would encourage you to go through the documentation.
[00:26:58.080 --> 00:27:00.800]   The documentation for PyTorch is quite awesome.
[00:27:00.800 --> 00:27:06.800]   And even, again, remember, don't look up what these words mean.
[00:27:06.800 --> 00:27:10.720]   Instead, just try to understand what's going in, what's coming out of these.
[00:27:12.160 --> 00:27:15.120]   So what is torch.-- I just randomly clicked on this.
[00:27:15.120 --> 00:27:17.360]   This is the first time I'm looking at it also, honestly.
[00:27:17.360 --> 00:27:22.720]   But what is torch.nn.parameter.uninitializedparameter doing?
[00:27:22.720 --> 00:27:29.200]   And then try to figure out a way of using this in any example that you're playing around with.
[00:27:29.200 --> 00:27:34.160]   Or at least with whatever example you're playing around,
[00:27:34.160 --> 00:27:38.000]   just try to understand what functions are being used there.
[00:27:38.000 --> 00:27:41.680]   That, in my opinion, is one of the best ways of learning.
[00:27:42.000 --> 00:27:47.280]   So once you've created this neural network, you need to figure out a way to
[00:27:47.280 --> 00:27:50.960]   understand how well is it performing.
[00:27:50.960 --> 00:27:54.560]   So for that, you need a way to optimize it, quite literally.
[00:27:54.560 --> 00:27:56.160]   You need an optimizer.
[00:27:56.160 --> 00:27:59.920]   And you need a loss function, or you need to measure its accuracy.
[00:27:59.920 --> 00:28:04.000]   So for that, you use torch.optim module.
[00:28:04.000 --> 00:28:09.440]   And again, these are modules which contain all of these functions that we can use.
[00:28:09.440 --> 00:28:14.640]   So the whole reason for using PyTorch is you don't need to write everything from scratch.
[00:28:14.640 --> 00:28:16.000]   Scratch is a very broad term.
[00:28:16.000 --> 00:28:17.680]   You could be writing the C++ code.
[00:28:17.680 --> 00:28:19.520]   You could be writing the CUDA code.
[00:28:19.520 --> 00:28:21.760]   CUDA is a framework that operates on graphic cards.
[00:28:21.760 --> 00:28:25.760]   Or you could also be writing these neural network modules.
[00:28:25.760 --> 00:28:30.080]   PyTorch has a lot of these written already.
[00:28:30.080 --> 00:28:32.400]   You don't need to write them again.
[00:28:32.400 --> 00:28:33.760]   PyTorch takes care of you.
[00:28:33.760 --> 00:28:36.000]   You don't need to worry about that.
[00:28:36.000 --> 00:28:40.720]   So that's why we go to these modules that almost all of the time have all of the functions that we
[00:28:40.720 --> 00:28:41.520]   need.
[00:28:41.520 --> 00:28:43.280]   And we import something from these.
[00:28:43.280 --> 00:28:45.360]   And we go ahead with creating our modules.
[00:28:45.360 --> 00:28:53.440]   So as an additional homework, consider spending some time going through these modules this week.
[00:28:53.440 --> 00:28:58.320]   Now, I've been talking about this for a few minutes.
[00:28:58.320 --> 00:29:00.000]   But what is a model?
[00:29:00.000 --> 00:29:02.560]   Quite simply put, it's this input.
[00:29:02.560 --> 00:29:04.480]   You take an input.
[00:29:04.480 --> 00:29:05.520]   You pre-process it.
[00:29:05.520 --> 00:29:08.000]   Someone just mentioned you need to crop the image to the--
[00:29:08.000 --> 00:29:10.480]   I'm guessing I'm at your right.
[00:29:10.480 --> 00:29:11.680]   It's towards my left.
[00:29:11.680 --> 00:29:12.800]   Left hand's inverted.
[00:29:12.800 --> 00:29:18.480]   But for this, you need to crop the image to the right.
[00:29:18.480 --> 00:29:24.960]   Assuming all of the images have a person standing towards the right edge,
[00:29:24.960 --> 00:29:27.520]   you need to sometimes pre-process the image.
[00:29:27.520 --> 00:29:28.480]   Let's say I'm too bright.
[00:29:28.480 --> 00:29:29.760]   I have a light shining on me.
[00:29:29.760 --> 00:29:32.480]   You would want to turn down the brightness.
[00:29:32.480 --> 00:29:36.400]   So in that case, you would also need to play around with the brightness a little bit.
[00:29:36.400 --> 00:29:40.480]   You sometimes pre-process the input, like I said,
[00:29:40.480 --> 00:29:43.280]   and then pass it to this model that has millions of numbers.
[00:29:43.280 --> 00:29:44.240]   And you get an output.
[00:29:44.240 --> 00:29:48.240]   That's how it works in the broad sense.
[00:29:48.240 --> 00:29:50.320]   But let's take a deep look at it.
[00:29:50.320 --> 00:29:55.920]   Let me try to explain this again in OneNote once I get to the right page.
[00:29:56.080 --> 00:30:05.280]   [AUDIO OUT]
[00:30:05.280 --> 00:30:05.760]   I'm sorry.
[00:30:05.760 --> 00:30:07.200]   I have a lot of things on my desk.
[00:30:07.200 --> 00:30:10.080]   And I'm just moving everything around as I try to explain this.
[00:30:10.080 --> 00:30:10.960]   It takes a second.
[00:30:10.960 --> 00:30:16.000]   This is how PyTorch supports deep learning projects.
[00:30:16.000 --> 00:30:25.440]   And when we say a model is trained, it has these weights, quite literally,
[00:30:25.440 --> 00:30:27.280]   which you can think are knobs.
[00:30:27.280 --> 00:30:32.480]   So every knob-- this is a simpler explanation.
[00:30:32.480 --> 00:30:35.840]   And this comes from-- this is not my original idea.
[00:30:35.840 --> 00:30:39.520]   To credit, this comes from the book "Crocking Deep Learning."
[00:30:39.520 --> 00:30:45.680]   I do task one of the leaders of our field explaining it like so.
[00:30:45.680 --> 00:30:51.120]   But you can think of every weight as a single knob that we adjust.
[00:30:52.880 --> 00:30:57.840]   Now, when we are using a pre-trained model, you just have these numbers figured out.
[00:30:57.840 --> 00:31:02.880]   And as we are training these models, these numbers adjust themselves.
[00:31:02.880 --> 00:31:08.640]   And when we download a pre-trained model, this is what we are downloading.
[00:31:08.640 --> 00:31:12.240]   So we download these numbers that have already seen a lot of examples,
[00:31:12.240 --> 00:31:14.880]   work really well for that particular problem,
[00:31:14.880 --> 00:31:20.880]   have, let's say, 98% accuracy for the particular given problem.
[00:31:21.680 --> 00:31:22.800]   Excuse my handwriting.
[00:31:22.800 --> 00:31:28.160]   We load that and just try to predict based on our problem.
[00:31:28.160 --> 00:31:38.160]   So that's what happens inside of model parameters and weights.
[00:31:38.160 --> 00:31:44.240]   I'm again trying to switch back to the right screen.
[00:31:44.240 --> 00:31:45.200]   Please give me a second.
[00:31:45.200 --> 00:31:50.160]   And I'm trying to find the correct Jupyter Notebook here.
[00:31:50.400 --> 00:31:51.920]   All right.
[00:31:51.920 --> 00:31:55.360]   So I think this was a bit of chapter one and two.
[00:31:55.360 --> 00:31:56.800]   Now, I'll quickly glance through this.
[00:31:56.800 --> 00:31:57.760]   Let me make sure it's running.
[00:31:57.760 --> 00:31:58.720]   Yes, it is.
[00:31:58.720 --> 00:32:01.440]   I'm on the latest version of PyTorch.
[00:32:01.440 --> 00:32:02.800]   This is running on my server.
[00:32:02.800 --> 00:32:09.120]   Now, when I mentioned earlier that I would highly recommend you all
[00:32:09.120 --> 00:32:13.760]   spend a lot of time playing around with different things, it's again--
[00:32:13.760 --> 00:32:19.520]   So let's say you're creating A and B, whatever is happening here.
[00:32:20.000 --> 00:32:24.800]   I would highly encourage also doing this one little trick that I really love in
[00:32:24.800 --> 00:32:26.240]   Jupyter Notebooks.
[00:32:26.240 --> 00:32:28.640]   If you put a single question mark ahead of a function,
[00:32:28.640 --> 00:32:33.120]   and assuming you type it with the correct spelling,
[00:32:33.120 --> 00:32:44.000]   and not put brackets against it--
[00:32:44.000 --> 00:32:46.240]   like I said, I'm a little sick, so you all can excuse me--
[00:32:47.760 --> 00:32:51.280]   you'll get the doc string of the function,
[00:32:51.280 --> 00:32:54.800]   and it'll tell you what's going on there.
[00:32:54.800 --> 00:32:58.480]   Please play around with all of these parameters and understand what's going on.
[00:32:58.480 --> 00:33:00.400]   Try to play with different values.
[00:33:00.400 --> 00:33:05.280]   One of my mistakes of going through any book is I would just look at these,
[00:33:05.280 --> 00:33:07.680]   nod at the cells, and keep going.
[00:33:07.680 --> 00:33:16.080]   That's fine if you want the thought of encouragement that you're learning,
[00:33:16.080 --> 00:33:18.880]   but not really for the true understanding.
[00:33:18.880 --> 00:33:22.880]   I've written this article called "How Not to Do Fast AI or Any Machine Learning MOOC."
[00:33:22.880 --> 00:33:28.960]   I would recommend giving this a read, because I really talk about all of my pitfalls here.
[00:33:28.960 --> 00:33:41.920]   I'm going to post this here.
[00:33:44.880 --> 00:33:49.520]   So what we are doing in this example-- this, I think, is from the first or second chapter--
[00:33:49.520 --> 00:33:56.000]   when I quickly pointed out that we download weights of a model or different knobs,
[00:33:56.000 --> 00:33:59.600]   we are downloading pre-trained weights.
[00:33:59.600 --> 00:34:01.280]   So I've imported the TorchVision--
[00:34:01.280 --> 00:34:04.720]   I've imported models from TorchVision module here.
[00:34:04.720 --> 00:34:10.640]   And if you list all of the models using DIR, you can see it's a terribly long list,
[00:34:10.640 --> 00:34:12.080]   and it's quite a few functions.
[00:34:13.200 --> 00:34:16.720]   These are right from AlexNet.
[00:34:16.720 --> 00:34:22.160]   AlexNet was the model that showed us how deep learning is going to take over the world.
[00:34:22.160 --> 00:34:27.040]   This is one of the first papers that ever came out.
[00:34:27.040 --> 00:34:31.120]   Fun fact-- I think Jeremy also points this out in deep learning.
[00:34:31.120 --> 00:34:40.160]   AlexNet has this particular cropped image that comes from the paper.
[00:34:40.880 --> 00:34:46.080]   So in every single image that you find of AlexNet, it's cropped off a little.
[00:34:46.080 --> 00:34:51.360]   For this book, that's not the case, because this book has really nice hand-drawn examples.
[00:34:51.360 --> 00:34:59.040]   So this book, Deep Learning with PyTorch, is the only one where you won't find cropped images.
[00:35:06.160 --> 00:35:13.120]   So what we're doing here is we're creating AlexNet instance using this model.
[00:35:13.120 --> 00:35:19.600]   Oh, this was supposed to be an exercise, and I forgot to comment it out.
[00:35:19.600 --> 00:35:23.760]   So one thing I recommend-- and this is, again, an example I wanted to point out.
[00:35:23.760 --> 00:35:28.560]   So in the book, they teach you how to download the weights of your model.
[00:35:28.560 --> 00:35:30.320]   It's not too difficult.
[00:35:30.320 --> 00:35:31.680]   All you have to do is pass through.
[00:35:32.720 --> 00:35:36.080]   But I was going to recommend as an exercise, when I said,
[00:35:36.080 --> 00:35:38.960]   please play around with all of the parameters and options,
[00:35:38.960 --> 00:35:43.360]   try changing this to AlexNet.
[00:35:43.360 --> 00:35:45.360]   Try changing this to VGG.
[00:35:45.360 --> 00:35:47.360]   Try changing it to Inception v3.
[00:35:47.360 --> 00:35:48.160]   See what happens.
[00:35:48.160 --> 00:35:50.320]   Don't go through the theory.
[00:35:50.320 --> 00:35:51.600]   Don't understand what it is.
[00:35:51.600 --> 00:35:53.200]   See what goes in, what comes out.
[00:35:53.200 --> 00:35:58.480]   So what we're doing here is we've created a ResNet-101.
[00:36:00.000 --> 00:36:04.000]   And we've asked PyTorch, please download the weights for us.
[00:36:04.000 --> 00:36:05.840]   Remember the knobs I told you?
[00:36:05.840 --> 00:36:10.400]   So ResNet-101 is trained on ImageNet, which is one of the most famous data sets.
[00:36:10.400 --> 00:36:13.680]   And when we say pre-trained equals 2, we say, please, PyTorch,
[00:36:13.680 --> 00:36:14.800]   download the weights for us.
[00:36:14.800 --> 00:36:19.680]   If true, returns a model trained on ImageNet.
[00:36:19.680 --> 00:36:23.120]   And if you don't know what I did, I placed my cursor inside here,
[00:36:23.120 --> 00:36:25.600]   and I pressed Shift-Tab twice.
[00:36:25.600 --> 00:36:27.120]   That shows you the top string as well.
[00:36:27.120 --> 00:36:29.840]   You could also do it with two question marks in JupyterNode.
[00:36:29.920 --> 00:36:34.320]   [COUGHS]
[00:36:34.320 --> 00:36:36.800]   And when I try to print this, as you can see, this is--
[00:36:36.800 --> 00:36:39.780]   sorry.
[00:36:39.780 --> 00:36:43.700]   Sorry.
[00:36:43.700 --> 00:36:48.720]   This creates a model with many layers.
[00:36:48.720 --> 00:36:51.200]   So we're getting a peek inside of the model.
[00:36:51.200 --> 00:36:53.520]   As you can see, there's a convolutional layer.
[00:36:53.520 --> 00:36:55.120]   You don't need to know these terms.
[00:36:55.120 --> 00:36:58.880]   But again, this points out how much PyTorch can really do for you.
[00:36:59.840 --> 00:37:03.440]   And all of these layers are created inside of a neural network.
[00:37:03.440 --> 00:37:04.560]   We learn what layers are.
[00:37:04.560 --> 00:37:06.560]   We learn what neural networks are.
[00:37:06.560 --> 00:37:12.880]   But let's also appreciate the API that with single lines, we can create this model.
[00:37:12.880 --> 00:37:17.200]   Now, what we're trying to do here is predict this image has a golden retriever,
[00:37:17.200 --> 00:37:20.080]   which is why I had asked you all, and almost everyone--
[00:37:20.080 --> 00:37:21.520]   I think every single one--
[00:37:21.520 --> 00:37:24.880]   got the answer, right?
[00:37:24.880 --> 00:37:27.120]   So we have the data.
[00:37:27.120 --> 00:37:29.200]   Still, I'm taking Ramjaju's answer.
[00:37:29.200 --> 00:37:33.200]   We'll clean the data, train the model, evaluate, and deploy.
[00:37:33.200 --> 00:37:36.960]   We won't be doing so much, but we'll be doing a few steps out of this.
[00:37:36.960 --> 00:37:42.480]   So one of the things we need to do here is resize the image.
[00:37:42.480 --> 00:37:48.160]   Again, if you remember, as I move towards the side, you're looking at me--
[00:37:48.160 --> 00:37:48.960]   not right now.
[00:37:48.960 --> 00:37:51.920]   When I'm in full screen, you're looking at me in 1080p.
[00:37:53.440 --> 00:38:00.880]   You maybe want to pull that size down to 256 or maybe 224, depending on what your model is trained.
[00:38:00.880 --> 00:38:04.400]   Although most recent models can handle any image size.
[00:38:04.400 --> 00:38:13.440]   So this is the size that YouTube shows you.
[00:38:13.440 --> 00:38:16.400]   It keeps going down from there.
[00:38:16.400 --> 00:38:21.360]   One of the most famous sizes with deep learning is this, 224 by 224.
[00:38:22.160 --> 00:38:24.640]   So what we're doing here is first of all, we're taking our image,
[00:38:24.640 --> 00:38:27.360]   resizing it to 256 by 256.
[00:38:27.360 --> 00:38:35.760]   And then we center crop.
[00:38:35.760 --> 00:38:39.840]   So as you see this image, we crop right into the center of it.
[00:38:39.840 --> 00:38:42.320]   How do I know that?
[00:38:42.320 --> 00:38:45.760]   Because I pressed shift tab twice inside of center crop.
[00:38:45.760 --> 00:38:51.760]   And I learned that this comes from PIL, which is another framework.
[00:38:52.080 --> 00:38:56.560]   So we're not just learning PyTorch, we're also learning a few frameworks around it.
[00:38:56.560 --> 00:38:58.400]   I would encourage doing this.
[00:38:58.400 --> 00:39:03.440]   Please look inside a function, see what they do, change these values, see what that does.
[00:39:03.440 --> 00:39:10.960]   I'll also-- I saw in the Zoom chat that someone had posted the link.
[00:39:10.960 --> 00:39:14.080]   Let me also post the link to the GitHub repo.
[00:39:14.080 --> 00:39:15.440]   This is from the GitHub repo.
[00:39:15.440 --> 00:39:16.080]   Sorry, guys.
[00:39:16.080 --> 00:39:20.480]   I'm just stealing code from the authors for credit.
[00:39:20.480 --> 00:39:23.440]   This is coming from the GitHub repo of the book.
[00:39:23.440 --> 00:39:34.480]   I will be releasing a few exercises over the week, so please keep an eye out on the forums.
[00:39:34.480 --> 00:39:36.720]   Let me go back to the right tab.
[00:39:36.720 --> 00:39:44.560]   So now what we've done is we have our image of a cute dog named Bobby for some reason.
[00:39:44.560 --> 00:39:48.640]   I would assume it's just doggy or something like that.
[00:39:48.640 --> 00:39:56.400]   But OK, we take our image, we resize it to 256 by 256, crop into the center of it,
[00:39:56.400 --> 00:40:03.200]   into this size, convert it into tensor-- because remember, PyTorch works with tensor.
[00:40:03.200 --> 00:40:06.880]   PyTorch's core data structure is tensor, like I said a minute ago.
[00:40:06.880 --> 00:40:09.200]   And then we normalize the image.
[00:40:09.200 --> 00:40:13.040]   What do you mean when you say normalize?
[00:40:13.040 --> 00:40:13.760]   Here's what I mean.
[00:40:13.760 --> 00:40:16.000]   I'll tell you because I'll press shift twice.
[00:40:16.800 --> 00:40:18.800]   I'm emphasizing on this a little.
[00:40:18.800 --> 00:40:23.920]   I'm sorry if you were experienced, but this is one of the pitfalls.
[00:40:23.920 --> 00:40:26.080]   Please keep looking at dog strings.
[00:40:26.080 --> 00:40:31.840]   Unlike my code, the authors of PyTorch make sure it's really well-documented,
[00:40:31.840 --> 00:40:33.760]   and it's there for a reason.
[00:40:33.760 --> 00:40:35.520]   So please take your time.
[00:40:35.520 --> 00:40:37.680]   This is actually from PIL, I think.
[00:40:37.680 --> 00:40:42.080]   No, this is from TorchVision, sorry.
[00:40:42.080 --> 00:40:45.360]   But the authors of TorchVision have made sure it's quite well-documented,
[00:40:45.360 --> 00:40:47.120]   so make the most use out of that.
[00:40:47.120 --> 00:40:52.880]   So what this does is it normalizes a tensor image with a mean and standard deviation.
[00:40:52.880 --> 00:41:01.040]   We're normalizing this image of Bobby with these values.
[00:41:01.040 --> 00:41:01.840]   Why?
[00:41:01.840 --> 00:41:05.040]   Because when we said pre-trained is equal to true above,
[00:41:05.040 --> 00:41:10.480]   we asked TorchVision, hey, TorchVision, please download those knob values,
[00:41:10.480 --> 00:41:15.280]   the ones that have been created by whoever pre-trained this model on image,
[00:41:15.280 --> 00:41:21.200]   and imageNet expects your image to be normalized by this number.
[00:41:21.200 --> 00:41:24.480]   How do I find these numbers?
[00:41:24.480 --> 00:41:25.120]   I don't know.
[00:41:25.120 --> 00:41:26.080]   The authors have put it.
[00:41:26.080 --> 00:41:27.920]   I can't remember them.
[00:41:27.920 --> 00:41:29.920]   So we just use these numbers instead.
[00:41:29.920 --> 00:41:33.920]   We pass this inside of transforms.compose.
[00:41:33.920 --> 00:41:37.840]   This composes several transforms together,
[00:41:37.840 --> 00:41:41.920]   and these are lists of transform objects.
[00:41:43.840 --> 00:41:47.120]   You can, again, look what transform objects are, but I'll skip that.
[00:41:47.120 --> 00:41:52.800]   At a higher level, we import transforms from TorchVision.
[00:41:52.800 --> 00:41:57.360]   We need our image to be transformed to a certain size, to a certain value.
[00:41:57.360 --> 00:41:59.280]   An image is nothing.
[00:41:59.280 --> 00:42:05.120]   If I look at this image of Bobby--
[00:42:05.120 --> 00:42:09.520]   I have to admit the name is growing on me now.
[00:42:09.520 --> 00:42:16.240]   Save.size.
[00:42:16.240 --> 00:42:28.320]   So this is the image size, and it's also RGB, so it also has red, green, and blue.
[00:42:28.320 --> 00:42:36.960]   An image is just numbers, and the computer doesn't see a doggo, it sees these numbers.
[00:42:36.960 --> 00:42:40.800]   So that's why we need to normalize the image with these numbers as well.
[00:42:40.800 --> 00:42:49.920]   I'm sorry.
[00:42:49.920 --> 00:42:51.120]   I see a comment in Zoom.
[00:42:51.120 --> 00:42:53.760]   Is my video and audio not clear to everyone?
[00:42:53.760 --> 00:42:58.080]   Should I zoom in a bit?
[00:43:05.920 --> 00:43:08.080]   OK, I'll zoom in a little bit.
[00:43:08.080 --> 00:43:08.640]   Sorry, guys.
[00:43:08.640 --> 00:43:10.240]   It's just me running the show today.
[00:43:10.240 --> 00:43:14.880]   My wonderful colleague Andrea is on a vacation, and like I said, I might mess up,
[00:43:14.880 --> 00:43:16.000]   so this is me messing up.
[00:43:16.000 --> 00:43:20.560]   So now we have the image loaded into the memory,
[00:43:20.560 --> 00:43:24.800]   and when we type image, it displays the image of Bobby,
[00:43:24.800 --> 00:43:27.920]   and we can pre-process our image.
[00:43:27.920 --> 00:43:32.800]   One thing I'd really like you to do, again, as a suggestion,
[00:43:32.800 --> 00:43:37.120]   look at what image is, and after you've passed it to pre-process,
[00:43:37.120 --> 00:43:38.080]   what is pre-process?
[00:43:38.080 --> 00:43:39.840]   We just defined pre-process above.
[00:43:39.840 --> 00:43:44.800]   It's pre-processing this image with all of these transforms that we had defined.
[00:43:44.800 --> 00:43:47.280]   See how that changes it.
[00:43:47.280 --> 00:43:50.000]   Spoiler alert, you won't see anything.
[00:43:50.000 --> 00:43:51.840]   It's just numbers that are being changed.
[00:43:51.840 --> 00:43:55.360]   But these are the things I would encourage you to look at.
[00:43:55.360 --> 00:43:58.400]   So see what's going in of pre-process.
[00:43:58.400 --> 00:44:00.160]   Understand what's coming out.
[00:44:00.160 --> 00:44:02.400]   This will help you build your intuition over time.
[00:44:02.640 --> 00:44:08.320]   [AUDIO OUT]
[00:44:08.320 --> 00:44:17.040]   Next thing we'd like to do is we'd unsqueeze this transformed image.
[00:44:17.040 --> 00:44:24.000]   So image_t is the transformed image, because we passed it to a list of transforms, remember?
[00:44:24.000 --> 00:44:29.120]   And we unsqueeze it with a parameter 0.
[00:44:29.120 --> 00:44:31.520]   What's going on there?
[00:44:31.520 --> 00:44:37.520]   This returns a new tensor with dimension of size 1 inserted at the specified position.
[00:44:37.520 --> 00:44:43.840]   So this will essentially convert this image into a batch.
[00:44:43.840 --> 00:44:51.200]   As I print this, and as I understand its shape,
[00:45:01.040 --> 00:45:05.520]   if you all can see the difference, these are also the things I would encourage you to check out.
[00:45:05.520 --> 00:45:13.600]   A single image-- I'm sorry, I'm also keeping an eye out on the chat.
[00:45:13.600 --> 00:45:14.800]   So that's why you see me, boss.
[00:45:14.800 --> 00:45:20.720]   I just printed the shape of the image, and I also printed the shape of the batch.
[00:45:20.720 --> 00:45:22.880]   So remember our module?
[00:45:22.880 --> 00:45:25.840]   It's called-- what's it called?
[00:45:25.840 --> 00:45:28.080]   I think ResNet-101.
[00:45:28.080 --> 00:45:29.120]   Yes, we're working with that.
[00:45:29.920 --> 00:45:32.400]   These are commented out, because you're supposed to try this.
[00:45:32.400 --> 00:45:33.200]   So remember that.
[00:45:33.200 --> 00:45:35.680]   You can also follow along.
[00:45:35.680 --> 00:45:36.800]   I'm quite slow, as you can tell.
[00:45:36.800 --> 00:45:44.400]   You can print these shapes out, and our model, the ResNet-101 that we had just downloaded,
[00:45:44.400 --> 00:45:46.800]   it expects images to be in a batch.
[00:45:46.800 --> 00:45:52.560]   So what we're doing here is we have a single batch-- a single image.
[00:45:52.560 --> 00:45:54.960]   So we create a batch of a single image.
[00:45:55.760 --> 00:45:58.160]   So our image is 224 by 224.
[00:45:58.160 --> 00:46:00.320]   Why is it in that size?
[00:46:00.320 --> 00:46:03.600]   That's not a standard size, and you're right.
[00:46:03.600 --> 00:46:09.280]   That's because we had asked our pipeline-- pre-processing pipeline-- to please crop it
[00:46:09.280 --> 00:46:11.280]   to a size of 224 by 224.
[00:46:11.280 --> 00:46:18.000]   So we create the size and pop it onto a batch, and now we are ready to make our prediction.
[00:46:18.000 --> 00:46:22.800]   Now we're ready to make some money using our state-of-the-art Bobby predictor or a golden
[00:46:22.800 --> 00:46:23.680]   retriever predictor.
[00:46:23.680 --> 00:46:29.520]   One thing we need to do here is we need to put this model into evaluation mode.
[00:46:29.520 --> 00:46:30.640]   How do we do that?
[00:46:30.640 --> 00:46:33.600]   By this option.
[00:46:33.600 --> 00:46:35.440]   How do I know that?
[00:46:35.440 --> 00:46:37.360]   Because I'm looking at the Docs things.
[00:46:37.360 --> 00:46:42.800]   It sets the model into evaluation mode, and this has a certain effect only on certain
[00:46:42.800 --> 00:46:47.760]   modules, especially the ones that have dropout and batch norm.
[00:46:47.760 --> 00:46:51.200]   If you don't do this for those modules, it won't work well.
[00:46:51.200 --> 00:46:53.360]   What are dropout and batch norms, Sayem?
[00:46:53.520 --> 00:46:54.640]   You'll find out later.
[00:46:54.640 --> 00:46:56.400]   If you don't know these terms, don't freak out.
[00:46:56.400 --> 00:46:57.280]   We'll cover those.
[00:46:57.280 --> 00:46:57.780]   Thanks.
[00:46:57.780 --> 00:47:00.320]   Now we need our output.
[00:47:00.320 --> 00:47:05.600]   So we pass this batch that we had created and get an output.
[00:47:05.600 --> 00:47:14.160]   This gives us a wonderful answer, and if you're not in AI, you probably can't say that this
[00:47:14.160 --> 00:47:15.200]   is a golden retriever.
[00:47:15.200 --> 00:47:18.800]   So our model is giving us all of these numbers.
[00:47:18.800 --> 00:47:21.920]   What do we need to do with that?
[00:47:21.920 --> 00:47:24.160]   We need to grab these labels.
[00:47:24.160 --> 00:47:27.920]   So with every single image, we get a label.
[00:47:27.920 --> 00:47:32.640]   For this problem, it's a supervised problem, which means we have labels and the images.
[00:47:32.640 --> 00:47:35.120]   So we open our ImageNet class labels.
[00:47:35.120 --> 00:47:41.120]   And our model gives us a list of probabilities.
[00:47:41.120 --> 00:47:44.480]   So what we do here is we get the most probable output.
[00:47:44.480 --> 00:47:50.160]   Tosh.max returns the maximum value of all elements.
[00:47:50.160 --> 00:47:52.080]   So these are the list of probabilities.
[00:47:52.080 --> 00:48:02.080]   And we pass this through something known as a softmax function.
[00:48:02.080 --> 00:48:03.120]   What is that?
[00:48:03.120 --> 00:48:04.480]   You can again read the doc strings.
[00:48:04.480 --> 00:48:08.400]   I'll keep pointing that out because that's one of the-- I feel-- biggest skills.
[00:48:08.400 --> 00:48:10.800]   And you get a percentage.
[00:48:10.800 --> 00:48:18.560]   We just look at the 0th item of the index that we've created above.
[00:48:19.120 --> 00:48:21.280]   So Tosh.max will return two values.
[00:48:21.280 --> 00:48:22.160]   How do I know that?
[00:48:22.160 --> 00:48:22.880]   I looked at this.
[00:48:22.880 --> 00:48:25.920]   You need to understand why are we throwing this value away.
[00:48:25.920 --> 00:48:28.960]   So if you're new to Python, underscore denotes dummy variables.
[00:48:28.960 --> 00:48:32.400]   Anything we'd like to throw away, we name it as underscore.
[00:48:32.400 --> 00:48:33.920]   And we never look at it again.
[00:48:33.920 --> 00:48:38.480]   And we grab the index of the highest probability item.
[00:48:38.480 --> 00:48:45.280]   Then we create the percentage out of this by passing it through softmax for the 0th label
[00:48:45.280 --> 00:48:46.880]   and multiply it by 100.
[00:48:48.400 --> 00:48:50.880]   I'm sorry if this is obvious to you all.
[00:48:50.880 --> 00:48:54.000]   I'm also being respectful of people who are new to all of these notations.
[00:48:54.000 --> 00:48:56.080]   So please bear with me for the first session, at least.
[00:48:56.080 --> 00:49:01.280]   And once we've done that, we can ask what is the percentage of this item?
[00:49:01.280 --> 00:49:04.000]   And what is it labeled as?
[00:49:04.000 --> 00:49:07.440]   Correctly enough, it's labeled as a golden retriever.
[00:49:07.440 --> 00:49:17.360]   Now this time, instead, if we sort the output from above with descending equals to 2,
[00:49:17.360 --> 00:49:19.040]   how do you know you can pass that parameter?
[00:49:19.040 --> 00:49:20.480]   By looking at the documentation.
[00:49:20.480 --> 00:49:24.720]   And you grab the first five indices.
[00:49:24.720 --> 00:49:26.400]   How is it first five?
[00:49:26.400 --> 00:49:27.680]   Python starts from 0.
[00:49:27.680 --> 00:49:32.400]   So we're grabbing the indices from 0, 1, 2, 3, 4.
[00:49:32.400 --> 00:49:33.440]   That is the first five.
[00:49:33.440 --> 00:49:38.320]   And that gives us all of these other predictions.
[00:49:38.320 --> 00:49:42.800]   Our model also thought with lesser probability that this is a Labrador retriever.
[00:49:42.800 --> 00:49:44.960]   Let's look at that.
[00:49:44.960 --> 00:49:54.720]   I mean, I would say this is somewhat similar to Bobby, right?
[00:49:54.720 --> 00:49:57.040]   So it's not too wrong there.
[00:49:57.040 --> 00:50:01.520]   And as you can see, as you go down this list, it starts to mess up.
[00:50:01.520 --> 00:50:04.160]   So this is one of the ways of analyzing your model.
[00:50:04.160 --> 00:50:05.840]   There are many more ways, of course.
[00:50:05.840 --> 00:50:09.360]   But you can also simply look at the probabilities.
[00:50:12.160 --> 00:50:16.080]   Sai is asking the shortcut to look at the documentation.
[00:50:16.080 --> 00:50:22.480]   Inside of any function on Jupyter, press shift tab once to get the shorter version
[00:50:22.480 --> 00:50:25.440]   and shift tab twice to get the complete docstring.
[00:50:25.440 --> 00:50:28.240]   This is for lazy people like me.
[00:50:28.240 --> 00:50:32.400]   If you're not lazy, you can also put a question mark.
[00:50:32.400 --> 00:50:41.040]   And if you don't mess up the second time and actually not put the brackets after it,
[00:50:41.040 --> 00:50:42.160]   you'll get the docstring.
[00:50:42.160 --> 00:50:48.800]   Although, with time, I hope you'll have the documentation link open.
[00:50:48.800 --> 00:50:54.160]   So this was the complete process to loading a pre-trained model,
[00:50:54.160 --> 00:50:59.600]   downloading its weights, putting it into evaluation mode,
[00:50:59.600 --> 00:51:04.560]   and making a state-of-the-art classifier for doggos.
[00:51:04.560 --> 00:51:07.280]   We also investigated our model.
[00:51:07.280 --> 00:51:09.120]   It's working as expected.
[00:51:09.120 --> 00:51:11.760]   We can now sell this to the stakeholders and make some money.
[00:51:11.760 --> 00:51:13.360]   Awesome.
[00:51:13.360 --> 00:51:16.480]   Let's see if there are any questions.
[00:51:16.480 --> 00:51:17.440]   I don't see any questions.
[00:51:17.440 --> 00:51:20.000]   I do see Ravi typing, so I'll wait for that.
[00:51:20.000 --> 00:51:23.120]   But please feel free to ask any questions.
[00:51:23.120 --> 00:51:36.960]   I'll take a break to take questions.
[00:51:36.960 --> 00:51:43.200]   [BREAK]
[00:51:43.200 --> 00:51:46.080]   I see a few people asking what's the name of the book.
[00:51:46.080 --> 00:51:48.240]   It's called Deep Learning with PyTorch.
[00:51:48.240 --> 00:51:54.320]   If you are on top of this forum thread, you should be able to find the link to purchase it.
[00:51:54.320 --> 00:51:56.960]   There's a discount coupon, not an affiliate link.
[00:51:56.960 --> 00:51:59.760]   They're friends of the "Chai Time Data Science" podcast.
[00:51:59.760 --> 00:52:04.800]   And you can also find the free link to download the book through the PyTorch website.
[00:52:05.440 --> 00:52:10.080]   [BREAK]
[00:52:10.080 --> 00:52:12.560]   We usually retrain a pre-trained model.
[00:52:12.560 --> 00:52:13.280]   Oh, I'm sorry.
[00:52:13.280 --> 00:52:14.160]   I missed your question.
[00:52:14.160 --> 00:52:18.640]   Why do we need to retain a pre-trained model?
[00:52:18.640 --> 00:52:23.520]   In this case, we are not retaining the model.
[00:52:23.520 --> 00:52:24.400]   Sometimes we do.
[00:52:24.400 --> 00:52:28.800]   And the reason for that, why we are not retaining here,
[00:52:28.800 --> 00:52:34.720]   is because we are predicting on something that is from the task on which the model was
[00:52:34.720 --> 00:52:35.600]   trained.
[00:52:35.600 --> 00:52:39.920]   So ImageNet does have the class Golden Retriever in it.
[00:52:39.920 --> 00:52:41.440]   We didn't have to retain our model.
[00:52:41.440 --> 00:52:45.760]   Sometimes you're making predictions on newer target tasks.
[00:52:45.760 --> 00:52:47.200]   So this is the target task.
[00:52:47.200 --> 00:52:49.680]   You have your source task on which the model was trained.
[00:52:49.680 --> 00:52:51.360]   And this is the target task.
[00:52:51.360 --> 00:52:54.720]   If your target task is different, you would have to retain the model.
[00:52:54.720 --> 00:53:00.640]   That's what Ravi had already answered.
[00:53:00.640 --> 00:53:01.280]   Thanks, Ravi.
[00:53:01.280 --> 00:53:04.320]   Awesome.
[00:53:04.320 --> 00:53:07.440]   Let me see where I'm at.
[00:53:07.440 --> 00:53:09.040]   Right.
[00:53:09.040 --> 00:53:13.840]   So the next thing the authors teach us in the book-- again, I'm not reading through the book,
[00:53:13.840 --> 00:53:17.040]   since I assume you all will be doing that, and I won't waste your time.
[00:53:17.040 --> 00:53:19.280]   If you read ahead of time, I'm summarizing everything.
[00:53:19.280 --> 00:53:23.840]   So the next thing they teach us-- and this is shamelessly stolen from the book.
[00:53:23.840 --> 00:53:25.760]   This is not my handwriting.
[00:53:25.760 --> 00:53:27.360]   This artwork is from the book.
[00:53:27.360 --> 00:53:30.960]   They teach us how to use a pre-trained model for a GAN.
[00:53:30.960 --> 00:53:32.640]   What is a GAN?
[00:53:32.640 --> 00:53:36.080]   And I absolutely loved the humor inside of the book.
[00:53:36.080 --> 00:53:39.280]   They talk about how we're trying to forge this image.
[00:53:39.280 --> 00:53:43.680]   I really love this book, because it keeps making you smile here and there,
[00:53:43.680 --> 00:53:45.200]   which is rare for a technical book.
[00:53:45.200 --> 00:53:53.440]   A GAN is actually two models that are trying to constantly fight each other.
[00:53:53.440 --> 00:53:57.360]   And for this particular example, they're trying to create an image.
[00:53:57.360 --> 00:53:59.680]   So I said two models.
[00:53:59.680 --> 00:54:00.240]   Where are they?
[00:54:00.240 --> 00:54:02.480]   There's one model known as the generator.
[00:54:02.480 --> 00:54:06.720]   This generates an image, and there's a discriminator.
[00:54:06.720 --> 00:54:10.400]   So think of this as a detective who looks at these generated images.
[00:54:10.400 --> 00:54:15.440]   The discriminator's task is to tell if the image is real or fake.
[00:54:15.440 --> 00:54:18.000]   The generator then learns of this.
[00:54:18.000 --> 00:54:20.080]   How does it do that?
[00:54:20.080 --> 00:54:21.280]   We'll learn later on.
[00:54:21.280 --> 00:54:22.240]   Don't worry about that.
[00:54:22.240 --> 00:54:23.760]   Just have a high-level understanding.
[00:54:23.760 --> 00:54:29.120]   And it again tries to create a better generated image.
[00:54:29.120 --> 00:54:33.280]   Its goal is to fool the discriminator to say that it's a real image.
[00:54:33.280 --> 00:54:39.360]   This keeps on going and both, as you can imagine, continue getting better.
[00:54:39.360 --> 00:54:41.040]   So both learn from each other.
[00:54:41.040 --> 00:54:49.520]   I can close this tab.
[00:54:49.520 --> 00:54:51.360]   I can close the Labrador Retriever.
[00:54:56.480 --> 00:54:59.600]   So the next example we look at is CycleGAN.
[00:54:59.600 --> 00:55:02.080]   I need another help from you all.
[00:55:02.080 --> 00:55:03.840]   I need a prompt to run.
[00:55:03.840 --> 00:55:06.080]   I have a Colab notebook open somewhere.
[00:55:06.080 --> 00:55:07.200]   If I can find the tab.
[00:55:07.200 --> 00:55:07.520]   Yes.
[00:55:07.520 --> 00:55:12.480]   So I need a prompt for you all to tell me what to run here.
[00:55:12.480 --> 00:55:16.720]   And this will generate an image with the text.
[00:55:16.720 --> 00:55:18.000]   So please give me a prompt.
[00:55:18.000 --> 00:55:21.680]   I'll wait for it for the VQGAN model.
[00:55:21.680 --> 00:55:25.280]   I'm just trying to show the promise of GAN models here.
[00:55:25.280 --> 00:55:29.040]   So if anyone could share a prompt for this particular model,
[00:55:29.040 --> 00:55:31.520]   I'll input the text here and we'll get an image out of it.
[00:55:31.520 --> 00:55:38.560]   I see Ravi is typing.
[00:55:38.560 --> 00:55:41.520]   If anyone else would like to share, I can take the top one.
[00:55:41.520 --> 00:55:45.200]   I'll share the Colab link after our session.
[00:55:45.760 --> 00:55:47.840]   I love this one, so I'll take it.
[00:55:47.840 --> 00:55:48.560]   Thanks, Sunayak.
[00:55:48.560 --> 00:55:51.360]   Watching a session, drinking chai.
[00:55:51.360 --> 00:55:52.480]   Drinking.
[00:55:52.480 --> 00:55:54.080]   It's making chai.
[00:55:54.080 --> 00:55:58.000]   And I'll leave this to run.
[00:55:58.000 --> 00:55:58.880]   This takes a while.
[00:55:58.880 --> 00:56:06.800]   This is one of the most famous examples of a Colab model.
[00:56:06.800 --> 00:56:08.240]   So I'll show you how to run it.
[00:56:08.240 --> 00:56:09.280]   I'll show you how to run it.
[00:56:09.280 --> 00:56:10.240]   I'll show you how to run it.
[00:56:10.240 --> 00:56:11.280]   I'll show you how to run it.
[00:56:11.280 --> 00:56:12.240]   I'll show you how to run it.
[00:56:12.240 --> 00:56:13.040]   I'll show you how to run it.
[00:56:13.040 --> 00:56:13.840]   I'll show you how to run it.
[00:56:13.840 --> 00:56:17.040]   This is one of the most famous recent developments.
[00:56:17.040 --> 00:56:19.520]   Still training, so that's fine.
[00:56:19.520 --> 00:56:20.880]   It's downloading the page.
[00:56:20.880 --> 00:56:25.040]   The process is somewhat similar again as to any pre-trained model.
[00:56:25.040 --> 00:56:31.520]   I'm trying to find the right tab.
[00:56:31.520 --> 00:56:35.280]   As you can tell.
[00:56:35.280 --> 00:56:42.480]   So for any given prompt, this is also one of my favorite Twitter accounts
[00:56:42.480 --> 00:56:46.400]   for any deep learning papers.
[00:56:46.400 --> 00:56:48.800]   Make sure you check the profile out.
[00:56:48.800 --> 00:56:50.400]   AK92501.
[00:56:50.400 --> 00:56:52.880]   They tweet about awesome papers.
[00:56:52.880 --> 00:56:58.880]   And the stuff that they tweet about is usually VQGanart lately or great papers.
[00:56:58.880 --> 00:57:01.200]   So as you can see, it's quite incredible.
[00:57:01.200 --> 00:57:02.240]   I really love this one.
[00:57:02.240 --> 00:57:05.920]   Detective Pikachu 4K resolution ray tracing.
[00:57:05.920 --> 00:57:07.440]   So these are a little prompts.
[00:57:07.440 --> 00:57:10.000]   These are a few prompts that can make sure the image is really nice.
[00:57:10.560 --> 00:57:15.600]   And something similar to this will come out of that.
[00:57:15.600 --> 00:57:19.040]   The reason I'm pointing all of these details out is again,
[00:57:19.040 --> 00:57:21.760]   we just learned about these things and all of this is operating
[00:57:21.760 --> 00:57:23.520]   on a fundamentally similar level.
[00:57:23.520 --> 00:57:28.640]   So that's also an example of GAN, which is why we're also looking at CycleGAN.
[00:57:28.640 --> 00:57:31.040]   Somewhat similar, slightly different.
[00:57:31.040 --> 00:57:37.920]   And all of these accomplish the same task, which is to create images.
[00:57:38.320 --> 00:57:41.920]   Now, if you don't understand all of this, that's totally fine.
[00:57:41.920 --> 00:57:45.920]   We'll come back to all of this, maybe towards the later half of this study session
[00:57:45.920 --> 00:57:48.000]   and understand every single block.
[00:57:48.000 --> 00:57:53.920]   But for now, all you need to know is, remember, I just spoke about all of these modules.
[00:57:53.920 --> 00:57:56.720]   We are creating these, we are defining these in code here.
[00:57:56.720 --> 00:58:00.640]   So we create a generator.
[00:58:00.640 --> 00:58:04.320]   And we instantialize it.
[00:58:05.920 --> 00:58:09.120]   Remember when I said we download the pre-trained weights?
[00:58:09.120 --> 00:58:10.560]   That's what is going on here.
[00:58:10.560 --> 00:58:13.040]   So the pre-trained weights were being downloaded here.
[00:58:13.040 --> 00:58:15.040]   And I think it failed.
[00:58:15.040 --> 00:58:16.480]   Yes, it did.
[00:58:16.480 --> 00:58:18.400]   This is embarrassing.
[00:58:18.400 --> 00:58:20.480]   As you can see, guys, this is a live demo.
[00:58:20.480 --> 00:58:23.360]   So this is expected.
[00:58:23.360 --> 00:58:28.000]   What I was hoping to achieve here was generate a live image and show it to you all.
[00:58:28.000 --> 00:58:30.400]   Let me see if I can restart it real quick.
[00:58:30.400 --> 00:58:33.440]   I do not have Colab Pro, so.
[00:58:35.440 --> 00:58:36.800]   It might take a while.
[00:58:36.800 --> 00:58:43.200]   I'll leave it at that and maybe share the art afterwards and also encourage you to do that.
[00:58:43.200 --> 00:58:45.520]   I've wasted a lot of time doing that for this module.
[00:58:45.520 --> 00:58:49.120]   But we load the weights for this particular cycle GAN.
[00:58:49.120 --> 00:58:50.480]   It's called horse to zebra.
[00:58:50.480 --> 00:58:53.760]   So this is one of the nicest things.
[00:58:53.760 --> 00:58:56.480]   You can save these weights and load them up.
[00:58:56.480 --> 00:59:02.240]   And we put it into evaluation mode.
[00:59:02.240 --> 00:59:04.720]   Biggest caveats ever.
[00:59:04.720 --> 00:59:07.680]   As a homework, try not doing this and see what happens.
[00:59:07.680 --> 00:59:09.280]   Tell us what happens.
[00:59:09.280 --> 00:59:11.200]   You'll laugh at the model.
[00:59:11.200 --> 00:59:13.600]   And this also shows you how brittle these things can be.
[00:59:13.600 --> 00:59:20.800]   Or make you feel embarrassed when you tap your colleague and they tell you,
[00:59:20.800 --> 00:59:22.560]   you didn't put your model into evaluate mode.
[00:59:22.560 --> 00:59:23.760]   I'm speaking from experience.
[00:59:23.760 --> 00:59:24.880]   You all can make fun of me.
[00:59:24.880 --> 00:59:26.720]   But it's one of the pitfalls.
[00:59:26.720 --> 00:59:30.080]   Again, press shift-tab twice to understand what's going on.
[00:59:30.080 --> 00:59:33.040]   So we put the model into evaluation mode.
[00:59:34.240 --> 00:59:35.760]   We load the image.
[00:59:35.760 --> 00:59:37.440]   This time, we just do a few transforms.
[00:59:37.440 --> 00:59:41.600]   We convert it into 256 by 256 and create a tensor.
[00:59:41.600 --> 00:59:43.120]   This is our tensor.
[00:59:43.120 --> 00:59:46.960]   A person who is-- I don't understand trying to do what.
[00:59:46.960 --> 00:59:49.600]   Maybe the horse is trying to kick them off their back.
[00:59:49.600 --> 00:59:54.160]   We pre-process this, convert this into a batch.
[00:59:54.160 --> 00:59:58.880]   Your neural networks expect images to be in batches,
[00:59:58.880 --> 01:00:00.560]   just because they've been trained on so much.
[01:00:00.560 --> 01:00:01.360]   We don't have a batch.
[01:00:01.360 --> 01:00:04.000]   So we'll just create a batch of single image.
[01:00:04.000 --> 01:00:05.040]   How do I know that?
[01:00:05.040 --> 01:00:07.600]   Because I printed this out before showing up.
[01:00:07.600 --> 01:00:09.440]   I do my homework before the sessions.
[01:00:09.440 --> 01:00:13.280]   And this time, I remember not to put the parentheses.
[01:00:13.280 --> 01:00:16.800]   But as you can see, it's a single batch.
[01:00:16.800 --> 01:00:21.440]   So this time, the image size is 314 by 256.
[01:00:21.440 --> 01:00:28.640]   And since it's RGB image, red, green, and blue, there are three layers to it.
[01:00:28.640 --> 01:00:32.240]   I'm calling it layers because we haven't understood what tensors are right now.
[01:00:33.040 --> 01:00:34.960]   And there's a single image in this batch.
[01:00:34.960 --> 01:00:37.600]   So now we get the output.
[01:00:37.600 --> 01:00:42.400]   And the output wonderfully converts everything into a zebra, also the personality.
[01:00:42.400 --> 01:00:51.760]   So this is the promise also of pre-trained models and deep learning or transfer learning.
[01:00:51.760 --> 01:00:54.560]   We're just transferring all of these weights.
[01:00:54.560 --> 01:00:55.920]   Here, we're not.
[01:00:55.920 --> 01:00:57.120]   We're just using them.
[01:00:57.120 --> 01:00:59.920]   But you can take these weights and do interesting things with them.
[01:01:01.840 --> 01:01:06.080]   And now you have this model that someone spent a large amount of time training.
[01:01:06.080 --> 01:01:12.240]   And you can sell this horse to zebra module to anyone who's willing to buy it.
[01:01:12.240 --> 01:01:13.680]   There's a market for NFTs.
[01:01:13.680 --> 01:01:15.040]   I'm sure there's a market for this.
[01:01:15.040 --> 01:01:22.800]   There's also the example of MNIST.
[01:01:22.800 --> 01:01:27.200]   I think I'll skip this because I've just gone through all of these.
[01:01:27.200 --> 01:01:32.080]   So if you have any questions, as a reminder, please ask them in the thread here.
[01:01:32.080 --> 01:01:36.720]   I'll again go through the questions.
[01:01:36.720 --> 01:01:43.040]   Other than ImageNet, what other data might the models be trained on?
[01:01:43.040 --> 01:01:43.520]   Let's see.
[01:01:43.520 --> 01:01:52.640]   I'm trying to find this link if I can.
[01:01:56.080 --> 01:01:59.840]   For a particular Kaggle competition, I remember looking at this link.
[01:01:59.840 --> 01:02:04.000]   But I know there are medical data sets that have been made available.
[01:02:04.000 --> 01:02:11.200]   Usually, it's a great suggested starting point to use data from ImageNet.
[01:02:11.200 --> 01:02:18.320]   Sorry, to use models that have been trained on ImageNet.
[01:02:18.320 --> 01:02:20.160]   My throat is drying up, so I'll take another sip.
[01:02:20.160 --> 01:02:22.400]   Give me a minute, please.
[01:02:22.400 --> 01:02:32.720]   I'm sorry, I should be more organized.
[01:02:32.720 --> 01:02:35.280]   But like I said, I was quite dehydrated yesterday.
[01:02:35.280 --> 01:02:43.040]   So if your model, usually a model that has been trained on ImageNet
[01:02:43.040 --> 01:02:50.880]   is a great starting point because ImageNet has 1,000, I think 1,000 plus classes, for example.
[01:02:50.880 --> 01:02:59.040]   Sorry, like I said, I'm still a little sick.
[01:02:59.040 --> 01:03:02.240]   So that's a great starting point.
[01:03:02.240 --> 01:03:08.880]   And usually, your model has learned a lot of things or the model trained on this.
[01:03:08.880 --> 01:03:13.760]   But for images that are really different from this, so ImageNet has all of these classes,
[01:03:13.760 --> 01:03:14.880]   you can look at those.
[01:03:14.880 --> 01:03:19.920]   If it's insanely different from that, you might not want to use that.
[01:03:19.920 --> 01:03:25.280]   So examples are satellite images, examples are X-ray images.
[01:03:25.280 --> 01:03:32.720]   Sorry, those are also made available as treated models.
[01:03:32.720 --> 01:03:39.280]   I think these are usually-- the normalization values are usually
[01:03:39.280 --> 01:03:40.880]   supplied along with the models.
[01:03:40.880 --> 01:03:45.280]   Thanks, Sai, for answering the question as well.
[01:03:48.080 --> 01:03:50.960]   So GAN is the model with Niche.
[01:03:50.960 --> 01:03:55.280]   And what we are transferring here are the weights.
[01:03:55.280 --> 01:03:58.160]   We just created the same model.
[01:03:58.160 --> 01:04:00.320]   We loaded its weights.
[01:04:00.320 --> 01:04:02.480]   And now we're making a prediction.
[01:04:02.480 --> 01:04:05.520]   So we're just using the pre-trained models.
[01:04:05.520 --> 01:04:08.560]   And when we take this and apply it to a different task,
[01:04:08.560 --> 01:04:11.840]   then we're transferring its knowledge, hence transfer learning.
[01:04:11.840 --> 01:04:16.720]   When doing preprocessing, what is the best practice?
[01:04:16.720 --> 01:04:19.920]   Should we do it on the fly while training or save them and load?
[01:04:19.920 --> 01:04:26.400]   Dennis, I have burnt up a lot of AWS credits.
[01:04:26.400 --> 01:04:29.360]   So it's always great to save things every now and then.
[01:04:29.360 --> 01:04:33.920]   If you lose your connection, your money will be wasted.
[01:04:33.920 --> 01:04:35.840]   So try to save things every now and then.
[01:04:41.360 --> 01:04:46.480]   And if the follow-up question is, if I recollect correctly,
[01:04:46.480 --> 01:04:47.760]   FastAid does it on fly?
[01:04:47.760 --> 01:04:56.400]   So for this example, you don't need to save while preprocessing.
[01:04:56.400 --> 01:05:00.400]   And preprocessing does happen on the fly here as well.
[01:05:00.400 --> 01:05:02.560]   When we created-- I've closed the tabs.
[01:05:02.560 --> 01:05:04.320]   So I'll speak at a high level.
[01:05:04.320 --> 01:05:06.720]   When we created that preprocessing pipeline,
[01:05:08.320 --> 01:05:12.720]   we had defined a structure that would happen whenever we load an image.
[01:05:12.720 --> 01:05:14.800]   So there, as well, it's happening on the fly.
[01:05:14.800 --> 01:05:17.280]   And why is it so good?
[01:05:17.280 --> 01:05:20.320]   Because the creators of PyTorch have made sure the code works really well.
[01:05:20.320 --> 01:05:22.720]   It doesn't bottleneck.
[01:05:22.720 --> 01:05:26.080]   All of that is taken care of, similar to how it is in FastAid.
[01:05:26.080 --> 01:05:34.880]   It's for the only part where it's not recommended to do it on fly.
[01:05:34.880 --> 01:05:38.080]   I'm trying to think of examples to make sure you have a better understanding.
[01:05:38.800 --> 01:05:46.240]   [AUDIO OUT]
[01:05:46.240 --> 01:05:50.240]   So let's say you have a lot of CT scans or DICOM images.
[01:05:50.240 --> 01:05:57.520]   [AUDIO OUT]
[01:05:57.520 --> 01:06:00.480]   You need specific ways of loading.
[01:06:00.480 --> 01:06:02.480]   I was trying to find the form.
[01:06:02.480 --> 01:06:04.880]   Digital imaging and communication in medicine.
[01:06:04.880 --> 01:06:08.000]   This is the format in which medical images are presented.
[01:06:08.000 --> 01:06:13.280]   So when you are processing these images, it takes a large time to load them into memory.
[01:06:13.280 --> 01:06:16.080]   And oftentimes, you have to resize them.
[01:06:16.080 --> 01:06:20.800]   For that, you could maybe preprocess all of these images and then save them.
[01:06:20.800 --> 01:06:24.000]   For that particular example, I can think of--
[01:06:24.000 --> 01:06:27.600]   you don't need to do it on the fly.
[01:06:32.480 --> 01:06:35.600]   Rahul, I will get to your question after the study session.
[01:06:35.600 --> 01:06:45.280]   Yuvraj mentions he had used ResNet on satellite imaging.
[01:06:45.280 --> 01:06:47.760]   That was pre-trained on ImageNet, and it worked very well.
[01:06:47.760 --> 01:06:50.400]   Thanks for sharing that.
[01:06:50.400 --> 01:06:54.800]   And like I said, that's why ImageNet works really well for a lot of the examples.
[01:06:54.800 --> 01:06:58.800]   So when I said for satellite images or galactic images,
[01:06:58.800 --> 01:07:02.000]   oftentimes, you'll find it works really well for that as well.
[01:07:02.000 --> 01:07:04.000]   And this will, again, come from practice.
[01:07:04.000 --> 01:07:17.600]   Riya, this is not covered in the book, so working with digital sounds.
[01:07:17.600 --> 01:07:22.960]   But I'm guessing it would be covered in one of the future talks.
[01:07:22.960 --> 01:07:29.520]   So what I'm trying to do for the invited talks is invite authors or developers
[01:07:29.520 --> 01:07:31.760]   who are working on open source frameworks.
[01:07:31.760 --> 01:07:36.400]   And one of those is along the lines of digital sound processing.
[01:07:36.400 --> 01:07:43.120]   Gautam has answered the question perfectly.
[01:07:43.120 --> 01:07:46.720]   So when the preprocessing is quite CPU intensive,
[01:07:46.720 --> 01:07:49.280]   how do you find that out with time?
[01:07:49.280 --> 01:07:50.960]   You learn how to benchmark your code.
[01:07:50.960 --> 01:07:53.600]   And that's a really nice segue into this.
[01:07:53.600 --> 01:07:56.640]   So Andrej Karpathy, he's the director of AI at Tesla.
[01:07:56.640 --> 01:07:59.360]   So we all can say that he really knows what he's doing.
[01:07:59.360 --> 01:08:02.000]   He also spends a lot of time doing this.
[01:08:02.000 --> 01:08:06.320]   So he pointed out that for this particular trick,
[01:08:06.320 --> 01:08:11.520]   as you can see, it's insanely faster when you do
[01:08:11.520 --> 01:08:17.920]   math.squareroot compared to numpy.squareroot.
[01:08:17.920 --> 01:08:22.160]   So every now and then, you would want to benchmark your code
[01:08:22.160 --> 01:08:24.880]   and see what's working faster, slow.
[01:08:24.880 --> 01:08:29.280]   Now, there are, again, I'll emphasize on this by stopping sharing my screen.
[01:08:30.000 --> 01:08:31.520]   There are steps to all of this.
[01:08:31.520 --> 01:08:33.600]   There's this method to this madness.
[01:08:33.600 --> 01:08:35.280]   First, you make sure things work well.
[01:08:35.280 --> 01:08:38.320]   You understand what's going on at a level.
[01:08:38.320 --> 01:08:42.080]   And then you try to figure out how to make it faster,
[01:08:42.080 --> 01:08:44.960]   how to make it work better.
[01:08:44.960 --> 01:08:48.640]   So first, we're at the very early stage of seeing what's going on.
[01:08:48.640 --> 01:08:52.000]   At the later stage, you'd start benchmarking your code.
[01:08:52.000 --> 01:08:54.000]   You try to understand how things are working.
[01:08:54.000 --> 01:08:55.840]   You try to understand if it's--
[01:08:55.840 --> 01:08:58.080]   what's slow in your code, what function is slow.
[01:08:58.880 --> 01:09:00.720]   For me, usually, it's the parts I've written.
[01:09:00.720 --> 01:09:02.720]   You'd want to rewrite those.
[01:09:02.720 --> 01:09:12.240]   Awesome.
[01:09:12.240 --> 01:09:17.600]   So I'll move on, and I'll get back to the questions,
[01:09:17.600 --> 01:09:20.480]   since we also have a few more things to cover.
[01:09:20.480 --> 01:09:23.440]   Can you suggest anything about signal processing in PyTorch?
[01:09:23.440 --> 01:09:25.440]   Let's see.
[01:09:25.440 --> 01:09:27.760]   So if I go over to the docs,
[01:09:28.720 --> 01:09:34.000]   and I search signal processing, does that bring up anything?
[01:09:34.000 --> 01:09:43.920]   I'm trying to encourage this habit of just looking up the documentation.
[01:09:43.920 --> 01:09:47.840]   No, it does not.
[01:09:47.840 --> 01:09:50.240]   So what do I do next?
[01:09:56.880 --> 01:09:58.800]   So Torch does not have an audio module.
[01:09:58.800 --> 01:10:02.240]   If I remember correctly, there's a separate framework for that.
[01:10:02.240 --> 01:10:06.000]   Torch audio.
[01:10:06.000 --> 01:10:10.000]   Yes, it's called Torch audio.
[01:10:10.000 --> 01:10:12.480]   And now we're at its docs.
[01:10:12.480 --> 01:10:15.840]   So here, I would encourage you to look at Torch audio documentation
[01:10:15.840 --> 01:10:17.040]   and see what's going on here.
[01:10:17.040 --> 01:10:21.520]   I'm assuming there would be a few examples in the future,
[01:10:21.520 --> 01:10:24.800]   not on a promise date, but I do guarantee you'll come back to this.
[01:10:26.640 --> 01:10:32.480]   Now we're on the topic, which is the bread and butter of PyTorch, which is tensors.
[01:10:32.480 --> 01:10:37.440]   Once I get to the right tab, we'll get there slowly.
[01:10:37.440 --> 01:10:41.360]   This is the third chapter that we're in.
[01:10:41.360 --> 01:10:43.440]   So I think we've covered some good ground,
[01:10:43.440 --> 01:10:45.120]   at least more than one chapter.
[01:10:45.120 --> 01:10:46.560]   That is more than I was expecting.
[01:10:46.560 --> 01:10:49.600]   What is A?
[01:10:49.600 --> 01:10:50.960]   It's just a simple list.
[01:10:50.960 --> 01:10:53.360]   Nothing fancy going on here.
[01:10:53.360 --> 01:10:55.600]   It's a Python list.
[01:10:56.560 --> 01:10:57.840]   And you can index into it.
[01:10:57.840 --> 01:11:00.400]   You can change its elements.
[01:11:00.400 --> 01:11:08.560]   I think what the authors had shown somewhere, if I've not changed the code,
[01:11:08.560 --> 01:11:13.920]   you can also create a Torch tensor from a list.
[01:11:13.920 --> 01:11:19.680]   But we import Torch and we call this function called once.
[01:11:19.680 --> 01:11:20.480]   What does it do?
[01:11:20.480 --> 01:11:23.280]   I press shift tab twice to look at it.
[01:11:23.280 --> 01:11:26.560]   This returns a tensor filled with the scalar value one,
[01:11:26.560 --> 01:11:30.080]   with the shape defined by the value argument.
[01:11:30.080 --> 01:11:35.120]   So we ask Torch, hey Torch, please create a tensor with three numbers,
[01:11:35.120 --> 01:11:36.720]   all three being one, please.
[01:11:36.720 --> 01:11:38.160]   And it does that.
[01:11:38.160 --> 01:11:44.480]   The shortcut I pressed here is I press escape,
[01:11:44.480 --> 01:11:47.120]   and that takes Jupyter into command mode.
[01:11:47.120 --> 01:11:49.840]   If I press A, it'll add a cell above.
[01:11:49.840 --> 01:11:54.400]   If I press D twice, DD, it deletes the cell above.
[01:11:54.400 --> 01:11:58.640]   And if I press B, it adds a cell below.
[01:11:58.640 --> 01:12:01.600]   My favorite shortcut here is escape.
[01:12:01.600 --> 01:12:06.080]   And if you press 0 twice, that restarts your kernel.
[01:12:06.080 --> 01:12:09.200]   I thought I'd just show that.
[01:12:09.200 --> 01:12:15.600]   This time A is a Torch tensor.
[01:12:15.600 --> 01:12:18.240]   We can index into it.
[01:12:19.280 --> 01:12:20.880]   Convert it into float value.
[01:12:20.880 --> 01:12:24.320]   You can change the values inside it.
[01:12:24.320 --> 01:12:27.520]   We can assign all sorts of values.
[01:12:27.520 --> 01:12:34.400]   We can create floating point tensors by passing floating values.
[01:12:34.400 --> 01:12:41.600]   And this is an example from the book, so there's more context to it.
[01:12:41.600 --> 01:12:44.400]   But you can also define different lengths.
[01:12:44.400 --> 01:12:48.640]   So this is 2 by 3, 3 by 2.
[01:12:48.640 --> 01:12:51.760]   3 by 2, sorry.
[01:12:51.760 --> 01:12:52.960]   It's right in the cell.
[01:12:52.960 --> 01:12:56.400]   3 by 2, so rows by columns.
[01:12:56.400 --> 01:12:57.440]   I always forget that.
[01:12:57.440 --> 01:12:59.840]   There's a song to remember it as well.
[01:12:59.840 --> 01:13:03.680]   So rows by columns, we can also define the tensor like so.
[01:13:03.680 --> 01:13:06.320]   And we get this.
[01:13:06.320 --> 01:13:13.440]   There are a few questions.
[01:13:13.440 --> 01:13:17.200]   Yes, I skipped the neural network bits because in chapter 2,
[01:13:17.200 --> 01:13:18.400]   it's at a higher level.
[01:13:18.400 --> 01:13:19.760]   I would encourage you to read it.
[01:13:19.760 --> 01:13:21.200]   But I've covered the meat.
[01:13:21.200 --> 01:13:24.320]   Please feel free to ask the questions if any come up.
[01:13:24.320 --> 01:13:27.440]   And we will get to much more detail of that.
[01:13:27.440 --> 01:13:31.520]   Yes, I have used VS Code.
[01:13:31.520 --> 01:13:33.920]   But for some reason, I just stick to Jupyter Notebooks.
[01:13:33.920 --> 01:13:38.800]   Plain old Jupyter Notebooks, not the one inside VS Code.
[01:13:38.800 --> 01:13:42.480]   I'm just lazy that way.
[01:13:43.920 --> 01:13:47.200]   This time, there's context inside of the book which says,
[01:13:47.200 --> 01:13:49.360]   these are the coordinates that we're looking at.
[01:13:49.360 --> 01:13:51.920]   What they're trying to show through that example is you can
[01:13:51.920 --> 01:13:54.080]   also create this tensor of different lengths.
[01:13:54.080 --> 01:13:57.840]   Now we create a tensor filled with 0 values.
[01:13:57.840 --> 01:14:00.880]   So if you look at the docstring, it will fill every value with 0
[01:14:00.880 --> 01:14:02.400]   for the size 3 by 2.
[01:14:02.400 --> 01:14:04.800]   And we do it like so.
[01:14:04.800 --> 01:14:13.520]   In the book, they also talk about how much storage does this consume.
[01:14:14.480 --> 01:14:16.720]   So let me quickly go to that chapter.
[01:14:16.720 --> 01:14:17.760]   I'm sorry.
[01:14:17.760 --> 01:14:21.120]   Please give me a minute as I scroll through the book painfully
[01:14:21.120 --> 01:14:24.080]   and find the right page while trying not to embarrass myself.
[01:14:24.080 --> 01:14:38.240]   Remember earlier, I had mentioned that the authors have this only
[01:14:38.240 --> 01:14:40.640]   complete image of AlexNet that I've seen?
[01:14:40.640 --> 01:14:42.240]   This is the one.
[01:14:42.560 --> 01:14:46.000]   The paper has this cropped image quite funnily.
[01:14:46.000 --> 01:14:48.480]   I'm sorry.
[01:14:48.480 --> 01:14:52.000]   I'm just scrolling down to the correct point as I find it.
[01:14:52.000 --> 01:15:05.680]   So one thing also I really point out about the book is they have--
[01:15:05.680 --> 01:15:09.120]   usually, I end up taking a lot of notes along the margins.
[01:15:09.120 --> 01:15:10.640]   But they have everything annotated.
[01:15:10.640 --> 01:15:15.760]   I would highly encourage you to do this in Jupyter Notebooks,
[01:15:15.760 --> 01:15:16.720]   but delete the code.
[01:15:16.720 --> 01:15:18.160]   So just leave the comments there.
[01:15:18.160 --> 01:15:22.560]   And then using active recall, force yourself to add those lines.
[01:15:22.560 --> 01:15:25.520]   So just add a comment that--
[01:15:25.520 --> 01:15:27.760]   let's say Vinayak is doing that.
[01:15:27.760 --> 01:15:29.280]   He's really good, so he doesn't need to.
[01:15:29.280 --> 01:15:30.320]   But let's say he's doing this.
[01:15:30.320 --> 01:15:35.600]   Vinayak, please create a tensor that has 1 values of this size.
[01:15:35.600 --> 01:15:38.560]   And then later, the next day, come back.
[01:15:39.360 --> 01:15:41.760]   Delete-- make sure you've deleted this cell.
[01:15:41.760 --> 01:15:44.960]   And try to recall what the function was.
[01:15:44.960 --> 01:15:46.560]   You're allowed to Google it up.
[01:15:46.560 --> 01:15:49.520]   But don't cheat by looking at the original notebooks.
[01:15:49.520 --> 01:15:51.760]   That has really helped me in my journey.
[01:15:51.760 --> 01:15:58.560]   So what they talk about in this chapter is how tensors and lists are stored.
[01:15:58.560 --> 01:16:01.120]   Lists can be allocated throughout the memory,
[01:16:01.120 --> 01:16:04.320]   so it doesn't need to be in a continuous allocation.
[01:16:04.320 --> 01:16:06.320]   This becomes important towards later.
[01:16:06.320 --> 01:16:10.480]   And as we get to a million images, as we get to very large images,
[01:16:10.480 --> 01:16:13.680]   not images that are as small as Bobby, this becomes a problem.
[01:16:13.680 --> 01:16:17.680]   Since you start to run out of GPU memory,
[01:16:17.680 --> 01:16:21.760]   you face one of the worst errors any deep learning practitioner could ask for,
[01:16:21.760 --> 01:16:23.600]   which is known as a code out of memory error.
[01:16:23.600 --> 01:16:27.600]   If this made you smile, I understand your pain, friend.
[01:16:27.600 --> 01:16:31.520]   But you need to understand how memory is allocated.
[01:16:31.520 --> 01:16:35.040]   Depending-- remember I said these are floating point
[01:16:35.040 --> 01:16:37.200]   these are floating point tensors.
[01:16:37.200 --> 01:16:43.280]   Depending if you use floats or some other data type,
[01:16:43.280 --> 01:16:44.800]   how do you know which data type is?
[01:16:44.800 --> 01:16:45.920]   Let's see.
[01:16:45.920 --> 01:16:48.800]   I'll again switch to Safari.
[01:16:48.800 --> 01:16:57.440]   I look up TOS tensor data types.
[01:16:57.440 --> 01:16:59.440]   I find the documentation.
[01:16:59.440 --> 01:17:01.760]   And I see all of these data types.
[01:17:03.040 --> 01:17:08.800]   So all of these occupy more or less memory, depending on which one you use.
[01:17:08.800 --> 01:17:12.800]   Depending on your problem, you would have to be careful of that.
[01:17:12.800 --> 01:17:15.280]   Usually 32-bit floating points are great.
[01:17:15.280 --> 01:17:18.640]   Sometimes you have to use 16-bit.
[01:17:18.640 --> 01:17:21.600]   It is faster on the GPU.
[01:17:21.600 --> 01:17:25.200]   We'll get into those details later on.
[01:17:25.200 --> 01:17:28.320]   But again, it's important to understand this data allocation,
[01:17:28.320 --> 01:17:30.960]   because if you use the incorrect data type,
[01:17:30.960 --> 01:17:34.480]   we'll use more memory, you'll get the wonderful good out of memory error.
[01:17:34.480 --> 01:17:42.400]   So that's how these are allocated.
[01:17:42.400 --> 01:17:46.160]   We'll quickly understand indexing.
[01:17:46.160 --> 01:17:55.200]   But I was trying to find this example of stride that is shown in this example.
[01:17:55.200 --> 01:17:58.800]   In this particular chapter.
[01:17:58.800 --> 01:17:59.300]   Sorry.
[01:18:00.240 --> 01:18:01.120]   Yes, here it is.
[01:18:01.120 --> 01:18:04.880]   So this is how these numbers are stored in memory.
[01:18:04.880 --> 01:18:08.320]   Remember, we just saw a 3 by 2 array.
[01:18:08.320 --> 01:18:14.320]   Vinayak seconds my approach.
[01:18:14.320 --> 01:18:15.600]   So that's awesome to hear, Vinayak.
[01:18:15.600 --> 01:18:16.100]   Thanks.
[01:18:16.100 --> 01:18:23.920]   In memory, these are just stored like so in a continuous allocation.
[01:18:23.920 --> 01:18:25.040]   We just looked at that.
[01:18:25.040 --> 01:18:26.960]   I won't embarrass myself by scrolling up.
[01:18:27.520 --> 01:18:30.960]   But it's this continuous allocation inside of memory.
[01:18:30.960 --> 01:18:34.000]   So how does Torch know how to return the correct variable?
[01:18:34.000 --> 01:18:37.280]   There's a formula for that, that it takes care of.
[01:18:37.280 --> 01:18:44.960]   But it's through stride and offset.
[01:18:44.960 --> 01:18:49.120]   So for this particular shape of 3 by 3,
[01:18:49.120 --> 01:18:54.720]   if you need to jump to the next row, the stride is 3.
[01:18:55.680 --> 01:18:58.960]   So if I need to go from 5 to 1, I take a stride of 3.
[01:18:58.960 --> 01:19:02.880]   So 1, 2, 3, and we go towards 3.
[01:19:02.880 --> 01:19:11.200]   If I don't mess up the page anymore, and if I need to go to the next element after,
[01:19:11.200 --> 01:19:16.640]   sorry, it should be 1, 1, 2, 3, and it takes us to 1.
[01:19:16.640 --> 01:19:20.720]   And if I need to go to the next value, the stride is 1.
[01:19:21.680 --> 01:19:26.320]   So if I add 1 to this, it takes me to 3, which is the next element.
[01:19:26.320 --> 01:19:28.880]   So that's how these are stored inside of memory.
[01:19:28.880 --> 01:19:33.280]   Usually, you never run into a scenario where you need to understand
[01:19:33.280 --> 01:19:36.400]   how these are stored, but it's useful to know.
[01:19:36.400 --> 01:19:39.760]   And the most important thing there, again, becomes how do you
[01:19:39.760 --> 01:19:43.840]   set up these data sizes or data types.
[01:19:43.840 --> 01:19:46.240]   OK.
[01:19:46.240 --> 01:19:48.720]   Back to where I was.
[01:19:49.840 --> 01:19:51.360]   If I can find the correct tab.
[01:19:51.360 --> 01:19:52.240]   Yes, I did.
[01:19:52.240 --> 01:19:52.800]   I think.
[01:19:52.800 --> 01:19:53.120]   Yes.
[01:19:53.120 --> 01:20:00.480]   So when we say storage, it tells us it's in float storage.
[01:20:00.480 --> 01:20:10.000]   And if we look at the storage offset, this is, again,
[01:20:10.000 --> 01:20:14.960]   talked in much more detail inside of the chapter, but it tells us
[01:20:18.880 --> 01:20:23.600]   the tensors offset underlying storage in terms of number of storage elements.
[01:20:23.600 --> 01:20:26.480]   Shift-tab, please remember.
[01:20:26.480 --> 01:20:31.280]   We can look at the size of the tensors.
[01:20:31.280 --> 01:20:33.760]   We can also look at the shape.
[01:20:33.760 --> 01:20:36.400]   We can take a look at the stride.
[01:20:36.400 --> 01:20:46.800]   We can-- what's going on here is when we copy this over,
[01:20:48.480 --> 01:20:50.880]   it creates a shallow copy, if I remember correctly.
[01:20:50.880 --> 01:20:55.760]   So even though these share the same value, if I remember correctly, if we--
[01:20:55.760 --> 01:20:57.840]   sorry, that's not for this case.
[01:20:57.840 --> 01:21:01.200]   We're just showing how to copy one particular element.
[01:21:01.200 --> 01:21:01.840]   I'm sorry.
[01:21:01.840 --> 01:21:03.280]   This is a different concept.
[01:21:03.280 --> 01:21:09.280]   So when we create or when we return a particular index view out of a tensor,
[01:21:09.280 --> 01:21:11.520]   that's when we create a shallow copy.
[01:21:11.520 --> 01:21:12.480]   I'm getting confused.
[01:21:12.480 --> 01:21:15.840]   Please, please excuse that.
[01:21:16.560 --> 01:21:27.280]   And if we want to make sure that if we change points or if we change second point,
[01:21:27.280 --> 01:21:32.320]   that does not get reflected in original points tensor, we clone it.
[01:21:32.320 --> 01:21:40.800]   So what's going on here is if we simply copy this over, or if we don't add this,
[01:21:40.800 --> 01:21:44.560]   both of these will point to the same memory address.
[01:21:45.680 --> 01:21:49.040]   So to make sure that does not happen, we clone it.
[01:21:49.040 --> 01:21:52.400]   And that creates a nice copy of it.
[01:21:52.400 --> 01:21:54.480]   And these are two different elements.
[01:21:54.480 --> 01:21:59.360]   If you're from a mathematical background, you can create this transpose.
[01:21:59.360 --> 01:22:02.800]   What a transpose does, it inverts the rows by columns.
[01:22:02.800 --> 01:22:07.520]   So this becomes-- the first column becomes the first row.
[01:22:07.520 --> 01:22:12.320]   This is much better understood if you actually write this on pen and paper.
[01:22:12.320 --> 01:22:15.520]   I won't embarrass myself with one note, but you can try that.
[01:22:15.600 --> 01:22:20.400]   And the second column becomes the second row.
[01:22:20.400 --> 01:22:28.160]   What we are trying to check here is are both pointing to the same storage?
[01:22:28.160 --> 01:22:28.480]   Yes.
[01:22:28.480 --> 01:22:35.280]   So even though points_t is the transpose of points, it's the same thing.
[01:22:35.280 --> 01:22:38.480]   It's pointing to the same storage.
[01:22:38.480 --> 01:22:42.240]   And this is important because if you change an element here,
[01:22:42.240 --> 01:22:46.480]   and your state-of-the-art Bobby detector goes haywire, now you know why.
[01:22:46.480 --> 01:22:48.560]   Because you're changing the same element.
[01:22:48.560 --> 01:22:50.320]   So remember to create a copy of it.
[01:22:50.320 --> 01:22:53.200]   One of my pitfalls.
[01:22:53.200 --> 01:22:55.760]   I'm just embarrassing myself by sharing all of these details.
[01:22:55.760 --> 01:23:00.080]   But please make sure you clone when you go through this.
[01:23:00.080 --> 01:23:05.040]   I'm quickly scrolling through trying to understand how much is left.
[01:23:05.040 --> 01:23:06.960]   I think a little bit is left.
[01:23:06.960 --> 01:23:09.440]   So since we've run out of time--
[01:23:09.440 --> 01:23:11.680]   Oh, no, we haven't.
[01:23:11.680 --> 01:23:13.280]   I started a few minutes earlier.
[01:23:13.280 --> 01:23:16.640]   So Zoom starts the timer right when I start.
[01:23:16.640 --> 01:23:17.600]   So we have a few minutes.
[01:23:17.600 --> 01:23:20.080]   I'll try to cover this without rushing too much.
[01:23:20.080 --> 01:23:31.600]   I'll skip contiguous because it just returns a contiguous memory containing the same.
[01:23:31.600 --> 01:23:34.480]   This is explained in the book in much more detail.
[01:23:37.600 --> 01:23:41.840]   When you're creating a tensor, you can pass on Dtype.
[01:23:41.840 --> 01:23:48.800]   Actually, let me find the correct tab and look at the documentation.
[01:23:48.800 --> 01:23:57.760]   Where is the PyTorch documentation?
[01:24:01.200 --> 01:24:10.480]   So when you go to tensor, the reason I'm showing this is it's quite important to also spend time.
[01:24:10.480 --> 01:24:14.560]   And PyTorch has a very standard way of showing the documentation.
[01:24:14.560 --> 01:24:18.640]   But the thing I would encourage you to do is just spend much more time here
[01:24:18.640 --> 01:24:21.040]   than trying to look up the theory.
[01:24:21.040 --> 01:24:24.000]   So I would be inclined towards--
[01:24:24.000 --> 01:24:27.360]   and I shared my article earlier, How Not to Do Any Machine Learning.
[01:24:27.360 --> 01:24:30.080]   I would be inclined towards understanding how strides work.
[01:24:31.040 --> 01:24:32.320]   Don't do that.
[01:24:32.320 --> 01:24:33.840]   Spend your time here.
[01:24:33.840 --> 01:24:35.600]   So now we're looking at tensors.
[01:24:35.600 --> 01:24:41.440]   And almost every single function of tensor takes this argument known as Dtype,
[01:24:41.440 --> 01:24:46.880]   which tells the desired data type of the return tensor.
[01:24:46.880 --> 01:24:52.560]   So if you are from a coding background, we are passing this to the constructor.
[01:24:52.560 --> 01:24:56.880]   If you're not, a constructor simply constructs an object.
[01:24:57.920 --> 01:25:04.880]   Double points is an object of Torch.DoubleType.
[01:25:04.880 --> 01:25:10.240]   It's a tensor of size 10 by 2 with all values as 1.
[01:25:10.240 --> 01:25:14.800]   You could also pass Torch.Short.
[01:25:14.800 --> 01:25:19.760]   One example I would suggest you to do is for our doggy classifier,
[01:25:19.760 --> 01:25:22.880]   try to convert that tensor from--
[01:25:22.880 --> 01:25:26.880]   I think it would be floating into Torch.Short, Torch.Int,
[01:25:26.880 --> 01:25:30.160]   and see if the predictions fluctuate from that.
[01:25:30.160 --> 01:25:35.120]   These are the experiments you should run as you try to play around the API.
[01:25:35.120 --> 01:25:40.160]   You can also convert this to different data types--
[01:25:40.160 --> 01:25:42.960]   sorry--
[01:25:42.960 --> 01:25:46.480]   by passing-- by attaching this function.
[01:25:46.480 --> 01:25:53.760]   So if I create Torch once, an array of 10 by 2, a tensor of 10 by 2.
[01:25:53.760 --> 01:25:56.640]   Tensors are multidimensional arrays.
[01:25:56.640 --> 01:25:59.680]   And if I convert it to short, we'll get a tensor of type short.
[01:25:59.680 --> 01:26:08.640]   You could also use .to function.
[01:26:08.640 --> 01:26:14.080]   And this performs data type conversion or device conversion.
[01:26:14.080 --> 01:26:16.640]   What is device conversion, Saeem?
[01:26:16.640 --> 01:26:19.520]   Let's see.
[01:26:20.160 --> 01:26:31.680]   It's harder to type when you're standing.
[01:26:31.680 --> 01:26:35.440]   Device.
[01:26:35.440 --> 01:26:38.480]   So this is on the CPU memory.
[01:26:38.480 --> 01:26:48.080]   If I were to change this to device equals to CUDA, it would go to a graphic card.
[01:26:48.080 --> 01:26:51.360]   And this tensor would live on the graphic card's memory.
[01:26:51.360 --> 01:27:00.080]   So that's how we learned what Torch devices are by looking at the docs.
[01:27:00.080 --> 01:27:03.840]   And there are more, I think, functions-- more arguments that you can pass.
[01:27:03.840 --> 01:27:11.600]   This is one of the most important tricks, things that deep learning practitioners do,
[01:27:11.600 --> 01:27:15.440]   which is slicing or indexing.
[01:27:15.440 --> 01:27:19.680]   I think I should end here because I don't want to rush through this part.
[01:27:19.680 --> 01:27:21.680]   So we'll pick up next week from here.
[01:27:21.680 --> 01:27:24.640]   I'll glance through the questions and try to answer them.
[01:27:24.640 --> 01:27:39.760]   So Dennis's original question was for reproducibility.
[01:27:39.760 --> 01:27:44.320]   Should you make sure that you save the preprocessing?
[01:27:45.280 --> 01:27:47.280]   Thanks for answering that, Harveen.
[01:27:47.280 --> 01:27:53.360]   If you set up a seed and keep the seed fixed, your experiment becomes reproducible.
[01:27:53.360 --> 01:27:57.040]   One trick that Jeremy Howard taught us in fast.ca lectures--
[01:27:57.040 --> 01:27:58.560]   I think Aman covered it as well.
[01:27:58.560 --> 01:28:03.920]   If you don't set up a random seed, the randomness in your experiments is also quite useful.
[01:28:03.920 --> 01:28:06.320]   Try running that as an experiment.
[01:28:06.320 --> 01:28:08.480]   Wherever you see a random seed, change it.
[01:28:08.480 --> 01:28:10.000]   See how much that changes things.
[01:28:10.880 --> 01:28:16.080]   In Jeremy's opinion, as I recall, it's important to see that the randomness isn't too crazy.
[01:28:16.080 --> 01:28:20.480]   So a lot of the times, this is a joke inside of the research community.
[01:28:20.480 --> 01:28:23.600]   But a random seed gets you the state-of-the-art result.
[01:28:23.600 --> 01:28:27.760]   And if you change that seed, there's not a lot of interesting things going on.
[01:28:27.760 --> 01:28:33.120]   You don't want that for your model, if you want a model that works really well in the real world,
[01:28:33.120 --> 01:28:34.000]   whatever that means.
[01:28:34.080 --> 01:28:35.760]   Thanks for sharing that, Ravi.
[01:28:35.760 --> 01:28:38.400]   Make it work, make it right, and then make it fast.
[01:28:38.400 --> 01:28:40.720]   This is wonderful for deep learning as well.
[01:28:40.720 --> 01:28:47.440]   Yes, I should mention my colleague, Will Charles,
[01:28:47.440 --> 01:28:50.560]   who's a wonderful educator in deep learning.
[01:28:50.560 --> 01:28:54.720]   He has been creating these amazing videos here that I have to live up to.
[01:28:54.720 --> 01:28:58.960]   But he's also hosting a webinar on deep learning.
[01:28:58.960 --> 01:29:02.640]   And he's going to be talking about deep learning in the next few weeks.
[01:29:02.640 --> 01:29:06.720]   He's also hosting a webinar on, I think, the profiler.
[01:29:06.720 --> 01:29:10.640]   He's done a lot of deep dive and written an article on it as well.
[01:29:10.640 --> 01:29:15.280]   I'll share the link to this in the chat in this particular thread.
[01:29:15.280 --> 01:29:23.520]   Thanks for sharing that, Adil.
[01:29:23.520 --> 01:29:28.660]   Awesome.
[01:29:28.660 --> 01:29:32.560]   I'll just share the sign-up to the webinar.
[01:29:32.560 --> 01:29:37.440]   Webinar link is for understanding PyTorch profilers, if I remember correctly.
[01:29:37.440 --> 01:29:42.160]   But I think I'll wrap up here since I don't want to rush through the last bit.
[01:29:42.160 --> 01:29:44.480]   And I want to be respectful of everyone's time.
[01:29:44.480 --> 01:29:46.160]   So thanks again, everyone, for joining.
[01:29:46.160 --> 01:29:48.640]   We've gone through the first two and a half chapters.
[01:29:48.640 --> 01:29:51.120]   We'll pick up right where we left off next week.
[01:29:51.120 --> 01:29:56.240]   I'll try to share our homework over the next week once I am not as dehydrated.
[01:29:56.240 --> 01:29:57.280]   And I'll see you next week.
[01:29:57.280 --> 01:29:58.080]   Thanks for joining.
[01:29:58.400 --> 01:29:58.900]   Bye.
[01:29:59.700 --> 01:30:00.200]   Bye.
[01:30:01.000 --> 01:30:01.500]   Bye.
[01:30:02.300 --> 01:30:02.800]   Bye.
[01:30:03.360 --> 01:30:03.860]   Bye.
[01:30:04.660 --> 01:30:05.160]   Bye.
[01:30:05.160 --> 01:30:15.160]   [BLANK_AUDIO]

