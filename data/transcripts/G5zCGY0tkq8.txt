
[00:00:00.000 --> 00:00:08.000]   C++ is an object-oriented programming language that creates, especially with its newer versions,
[00:00:08.000 --> 00:00:11.840]   as we'll talk about, higher and higher levels of abstraction.
[00:00:11.840 --> 00:00:18.640]   So how do you design, let's even go back to the origin of C++, how do you design something
[00:00:18.640 --> 00:00:29.780]   with so much abstraction that's still efficient and is still something that you can manage,
[00:00:29.780 --> 00:00:35.540]   do static analysis on, you can have constraints on, they can be reliable, all those things
[00:00:35.540 --> 00:00:38.140]   we've talked about.
[00:00:38.140 --> 00:00:46.620]   To me, there's a slight tension between high-level abstraction and efficiency.
[00:00:46.620 --> 00:00:47.620]   That's a good question.
[00:00:47.620 --> 00:00:52.500]   I could probably have a year's course just trying to answer it.
[00:00:52.500 --> 00:01:00.260]   Yes, there's a tension between efficiency and abstraction, but you also get the interesting
[00:01:00.260 --> 00:01:05.200]   situation that you get the best efficiency out of the best abstraction.
[00:01:05.200 --> 00:01:13.160]   And my main tool for efficiency, for performance, actually is abstraction.
[00:01:13.160 --> 00:01:17.020]   So let's go back to how C++ got there.
[00:01:17.020 --> 00:01:19.620]   You said it was an object-oriented programming language.
[00:01:19.620 --> 00:01:22.160]   I actually never said that.
[00:01:22.160 --> 00:01:24.340]   It's always quoted, but I never did.
[00:01:24.340 --> 00:01:31.460]   I said C++ supports object-oriented programming and other techniques.
[00:01:31.460 --> 00:01:38.740]   And that's important because I think that the best solution to most complex, interesting
[00:01:38.740 --> 00:01:49.820]   problems require ideas and techniques from things that have been called object-oriented,
[00:01:49.820 --> 00:01:59.040]   abstraction, functional, traditional C-style code, all of the above.
[00:01:59.040 --> 00:02:08.520]   And so when I was designing C++, I soon realized I couldn't just add features.
[00:02:08.520 --> 00:02:15.000]   If you just add what looks pretty or what people ask for or what you think is good,
[00:02:15.000 --> 00:02:18.720]   one by one, you're not going to get a coherent whole.
[00:02:18.720 --> 00:02:25.360]   What you need is a set of guidelines that guides your decisions.
[00:02:25.360 --> 00:02:28.780]   Should this feature be in or should this feature be out?
[00:02:28.780 --> 00:02:33.720]   How should a feature be modified before it can go in and such?
[00:02:33.720 --> 00:02:40.460]   And in the book I wrote about that, the design evolution of C++, there's a whole bunch of
[00:02:40.460 --> 00:02:41.800]   rules like that.
[00:02:41.800 --> 00:02:45.920]   Most of them are not language technical.
[00:02:45.920 --> 00:02:51.920]   There are things like don't violate static type system because I like static type system
[00:02:51.920 --> 00:03:02.040]   for the obvious reason that I like things to be reliable on reasonable amounts of hardware.
[00:03:02.040 --> 00:03:06.320]   But one of these rules is the zero overhead principle.
[00:03:06.320 --> 00:03:07.320]   The what kind of principle?
[00:03:07.320 --> 00:03:09.560]   The zero overhead principle.
[00:03:09.560 --> 00:03:18.640]   It basically says that if you have an abstraction, it should not cost anything compared to write
[00:03:18.640 --> 00:03:24.960]   the equivalent code at a lower level.
[00:03:24.960 --> 00:03:35.040]   So if I have, say, a matrix multiply, it should be written in such a way that you could not
[00:03:35.040 --> 00:03:42.720]   drop to the C level of abstraction and use arrays and pointers and such and run faster.
[00:03:42.720 --> 00:03:50.400]   And so people have written such matrix multiplications and they've actually gotten code that ran
[00:03:50.400 --> 00:03:58.000]   faster than Fortran because once you had the right abstraction, you can eliminate temporaries
[00:03:58.000 --> 00:04:03.000]   and you can do loop fusion and other good stuff like that.
[00:04:03.000 --> 00:04:07.120]   It's quite hard to do by hand and in a lower level language.
[00:04:07.120 --> 00:04:10.560]   And there's some really nice examples of that.
[00:04:10.560 --> 00:04:18.640]   And the key here is that that matrix multiplication, the matrix abstraction allows you to write
[00:04:18.640 --> 00:04:20.760]   code that's simple and easy.
[00:04:20.760 --> 00:04:22.600]   You can do that in any language.
[00:04:22.600 --> 00:04:27.920]   But with C++, it has the features so that you can also have this thing run faster than
[00:04:27.920 --> 00:04:30.320]   if you hand coded it.
[00:04:30.320 --> 00:04:37.640]   Now people have given that lecture many times, I and others, and a very common question after
[00:04:37.640 --> 00:04:42.440]   the talk where you have demonstrated that you can outperform Fortran for dense matrix
[00:04:42.440 --> 00:04:46.640]   multiplication, people come up and say, "Yeah, but that was C++.
[00:04:46.640 --> 00:04:50.960]   If I rewrote your code in C, how much faster would it run?"
[00:04:50.960 --> 00:04:54.960]   The answer is much slower.
[00:04:54.960 --> 00:04:59.840]   This happened the first time actually back in the 80s with a friend of mine called Doug
[00:04:59.840 --> 00:05:05.320]   McElroy who demonstrated exactly this effect.
[00:05:05.320 --> 00:05:12.920]   And so the principle is you should give programmers the tools so that their abstractions can follow
[00:05:12.920 --> 00:05:15.200]   the zero-overhead principle.
[00:05:15.200 --> 00:05:21.280]   Furthermore, when you put in a language feature in C++ or a standard library feature, you
[00:05:21.280 --> 00:05:22.920]   try to meet this.
[00:05:22.920 --> 00:05:29.000]   It doesn't mean it's absolutely optimal, but it means if you hand code it with the usual
[00:05:29.000 --> 00:05:36.360]   facilities in the language, in C++, in C, you should not be able to better it.
[00:05:36.360 --> 00:05:44.120]   Usually you can do better if you use embedded assembler for machine code for some of the
[00:05:44.120 --> 00:05:49.640]   details to utilize part of a computer that the compiler doesn't know about.
[00:05:49.640 --> 00:05:54.120]   But you should get to that point before you beat to the abstraction.
[00:05:54.120 --> 00:05:58.120]   So that's a beautiful ideal to reach for.
[00:05:58.120 --> 00:05:59.720]   And we meet it quite often.
[00:05:59.720 --> 00:06:01.080]   Quite often.
[00:06:01.080 --> 00:06:04.400]   So where's the magic of that coming from?
[00:06:04.400 --> 00:06:08.600]   Some of it is the compilation process, so the implementation of C++.
[00:06:08.600 --> 00:06:14.420]   Some of it is the design of the feature itself, the guidelines.
[00:06:14.420 --> 00:06:21.980]   So I've recently and often talked to Chris Ladner, so Clang.
[00:06:21.980 --> 00:06:28.420]   Just out of curiosity, is your relationship in general with the different implementations
[00:06:28.420 --> 00:06:34.540]   of C++ as you think about you and committee and other people in C++, think about the design
[00:06:34.540 --> 00:06:40.200]   of new features or design of previous features?
[00:06:40.200 --> 00:06:49.280]   Even trying to reach the ideal of zero overhead, does the magic come from the design, the guidelines
[00:06:49.280 --> 00:06:51.780]   or from the implementations?
[00:06:51.780 --> 00:06:56.520]   And not all.
[00:06:56.520 --> 00:07:02.180]   You go for programming technique, program language features and implementation techniques.
[00:07:02.180 --> 00:07:04.020]   You need all three.
[00:07:04.020 --> 00:07:08.660]   And how can you think about all three at the same time?
[00:07:08.660 --> 00:07:13.500]   It takes some experience, takes some practice and sometimes you get it wrong, but after
[00:07:13.500 --> 00:07:16.020]   a while you sort of get it right.
[00:07:16.020 --> 00:07:24.420]   I don't write compilers anymore, but Brian Kernighan pointed out that one of the reasons
[00:07:24.420 --> 00:07:35.880]   C++ succeeded was some of the craftsmanship I put into the early compilers.
[00:07:35.880 --> 00:07:39.980]   And of course I did the language design and of course I wrote a fair amount of code using
[00:07:39.980 --> 00:07:41.860]   this kind of stuff.
[00:07:41.860 --> 00:07:49.580]   And I think most of the successes involves progress in all three areas together.
[00:07:49.580 --> 00:07:53.340]   A small group of people can do that.
[00:07:53.340 --> 00:07:57.060]   Two, three people can work together to do something like that.
[00:07:57.060 --> 00:08:02.020]   It's ideal if it's one person that has all the skills necessary, but nobody has all the
[00:08:02.020 --> 00:08:06.780]   skills necessary in all the fields where C++ is used.
[00:08:06.780 --> 00:08:13.100]   So if you want to approach my ideal in say concurrent programming, you need to know about
[00:08:13.100 --> 00:08:15.380]   algorithms from concurrent programming.
[00:08:15.380 --> 00:08:19.260]   You need to know the trigger of lock-free programming.
[00:08:19.260 --> 00:08:23.020]   You need to know something about the compiler techniques.
[00:08:23.020 --> 00:08:31.980]   And then you have to know some of the application areas where this is, like some forms of grammar,
[00:08:31.980 --> 00:08:40.660]   graphics or some forms of what we call a web-serving kind of stuff.
[00:08:40.660 --> 00:08:45.500]   And that's very hard to get into a single head, but small groups can do it too.
[00:08:45.500 --> 00:08:46.500]   [ Silence ]
[00:08:46.500 --> 00:08:46.500]   ------------------------------fe4f6f6f6f6--
[00:08:46.500 --> 00:08:51.500]   ------------------------------fe4f6f6f6f6--
[00:08:51.500 --> 00:08:56.500]   ------------------------------fe4f6f6f6f6--
[00:08:56.500 --> 00:09:01.500]   ------------------------------fe4f6f6f6f6--
[00:09:01.500 --> 00:09:11.500]   [BLANK_AUDIO]

