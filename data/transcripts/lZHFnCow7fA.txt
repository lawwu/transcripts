
[00:00:00.000 --> 00:00:05.000]   Yeah, so thanks for the kind introduction, Lavanya.
[00:00:05.000 --> 00:00:16.000]   I added a little bit to this talk at the end, so it's not just alligator pairs and random variables, but also stochastic gradient descent.
[00:00:16.000 --> 00:00:24.000]   This is a collection of things whose names do not actually describe the thing that they claim to describe.
[00:00:24.000 --> 00:00:32.000]   So I want you to imagine what you picture in your head when you hear the phrase "alligator pair."
[00:00:32.000 --> 00:00:40.000]   And actually, before I go into this talk, just to be clear, so this salon, we bring in people to talk about research.
[00:00:40.000 --> 00:00:49.000]   And then in addition, I spend a little bit of time in each salon talking about some of the fundamental mathematical ideas that go into machine learning
[00:00:49.000 --> 00:00:58.000]   and presenting intuitions and ideas around them to give something for people who are just starting out on their journey into machine learning
[00:00:58.000 --> 00:01:01.000]   and then also something for the experts.
[00:01:01.000 --> 00:01:13.000]   So hopefully you've taken the time while I was explaining that to picture an alligator pair in your head.
[00:01:13.000 --> 00:01:25.000]   Perhaps you were picturing something that was the shape of a pear that was covered in alligator scales or a pear that was eaten by alligators or something like that.
[00:01:25.000 --> 00:01:37.000]   So an alligator pair is not any of those things. An alligator pair is an avocado, which is neither an alligator or related to alligators in any way, nor is it a pair.
[00:01:37.000 --> 00:01:40.000]   It's very, very different from a pair.
[00:01:40.000 --> 00:01:47.000]   So I've got helpful pictures and diagrams so that in case you don't recall what alligators, pears, and avocados look like,
[00:01:47.000 --> 00:01:52.000]   you can see that there's like some slight resemblance between these things.
[00:01:52.000 --> 00:01:56.000]   There's no inherent connection.
[00:01:56.000 --> 00:02:04.000]   Now I'd like you to picture something corresponding to the phrase "white ant."
[00:02:04.000 --> 00:02:12.000]   So white ant, which you've got in mind, is probably an ant that is the color white.
[00:02:12.000 --> 00:02:15.000]   And that's actually very reasonable.
[00:02:15.000 --> 00:02:19.000]   But a white ant is neither white nor in fact an ant.
[00:02:19.000 --> 00:02:23.000]   So that's a picture of a white ant on the right side.
[00:02:23.000 --> 00:02:31.000]   It's a termite and it is brown, the same color as regular ants.
[00:02:31.000 --> 00:02:42.000]   So for various reasons, these names have been attached to these things, despite the fact that they aren't natural.
[00:02:42.000 --> 00:02:47.000]   They don't come about sort of the way you might expect.
[00:02:47.000 --> 00:02:53.000]   This kind of regrettable moment clature is also a problem in mathematics and machine learning.
[00:02:53.000 --> 00:02:57.000]   Really, most of the time in math, combinations of words work like you'd expect.
[00:02:57.000 --> 00:03:04.000]   So if I tell you that I have an abelian group, if you know what group and abelian mean,
[00:03:04.000 --> 00:03:09.000]   then you could say, "Oh, that is probably a group that is also abelian."
[00:03:09.000 --> 00:03:11.000]   And you would in fact be right.
[00:03:11.000 --> 00:03:13.000]   But not always.
[00:03:13.000 --> 00:03:22.000]   There are a lot of things that are in math where the names of the individual components of the name refer to something,
[00:03:22.000 --> 00:03:25.000]   but the combination refers to something quite different.
[00:03:25.000 --> 00:03:30.000]   These exceptions are often where people get tripped up and confused.
[00:03:30.000 --> 00:03:36.000]   And I recommend sort of leaning into this confusion and trying to understand what are these other words,
[00:03:36.000 --> 00:03:41.000]   the words that go into this compound word, what are their definitions,
[00:03:41.000 --> 00:03:46.000]   and then what is the definition of the overall thing, why are these different, how are they different.
[00:03:46.000 --> 00:03:51.000]   And by sort of leaning into that confusion, we can actually really enrich our understanding.
[00:03:51.000 --> 00:04:00.000]   If we just sort of try and shy away from this confusion, we're actually, I think, going to end up missing lots of important pieces.
[00:04:00.000 --> 00:04:06.000]   So the primary example that I want to work through on this front is on random variables.
[00:04:06.000 --> 00:04:13.000]   So random variables may sound like a variable that is random,
[00:04:13.000 --> 00:04:18.000]   and in some sense, that's the way they behave, much like the alligator pair is kind of like,
[00:04:18.000 --> 00:04:23.000]   it's almost like a pair, but it's got this like alligator-y skin.
[00:04:23.000 --> 00:04:27.000]   But mathematically, it's not like an abelian group.
[00:04:27.000 --> 00:04:31.000]   They are not variables that are random. They're actually deterministic functions.
[00:04:31.000 --> 00:04:36.000]   And so we're going to go through the typical definition of random variables
[00:04:36.000 --> 00:04:44.000]   and see how understanding this definition and recognizing how and why it's different from the intuitive definition
[00:04:44.000 --> 00:04:51.000]   helps us better understand the things we can do with random variables.
[00:04:51.000 --> 00:04:58.000]   So this example actually comes from this famous quote from this book, Probability and Introduction,
[00:04:58.000 --> 00:05:06.000]   by Samuel Goldberg, that's a probability textbook, that mentions that these alligator pair and the white ant
[00:05:06.000 --> 00:05:11.000]   are similar to the random variable. So that's where this quote came from.
[00:05:11.000 --> 00:05:14.000]   This is a solid introductory probability book, I think.
[00:05:14.000 --> 00:05:19.000]   It's a bit old, as may be indicated by the quality of the font, so there's maybe better stuff now.
[00:05:19.000 --> 00:05:29.000]   But it launched this phrase into this idea of the weirdness of the definition of the random variable.
[00:05:29.000 --> 00:05:34.000]   So random variables are a lot easier to exhibit than they are to define.
[00:05:34.000 --> 00:05:38.000]   So if somebody was like, what's a random variable? It's a lot easier to just list off, like,
[00:05:38.000 --> 00:05:42.000]   oh, you know, you roll a die and a number comes up. You don't know what it's going to be.
[00:05:42.000 --> 00:05:46.000]   There it is once you've rolled. How much rain there's going to be tomorrow.
[00:05:46.000 --> 00:05:50.000]   You can maybe predict that better than a dice roll.
[00:05:50.000 --> 00:05:55.000]   But famously, weather reports are not perfect at predicting whether there's going to be rain
[00:05:55.000 --> 00:05:59.000]   and how much there will be. The number of tweets in the next millisecond,
[00:05:59.000 --> 00:06:05.000]   depending on the moods of a bunch of various influencers, this number can go way up and way down
[00:06:05.000 --> 00:06:10.000]   and can be difficult to predict. The fourth decimal place of the time right now,
[00:06:10.000 --> 00:06:16.000]   that one gets used as a random variable in computer programs all the time.
[00:06:16.000 --> 00:06:22.000]   And a famous random variable people who do machine learning and deep learning encounter
[00:06:22.000 --> 00:06:27.000]   is their next gradient update. It would be great if we knew our gradient updates
[00:06:27.000 --> 00:06:32.000]   without having to calculate them, but they behave somewhat randomly,
[00:06:32.000 --> 00:06:36.000]   and we have to calculate them to know what they are.
[00:06:36.000 --> 00:06:39.000]   So these are all examples, but they aren't a definition.
[00:06:39.000 --> 00:06:44.000]   So the standard definition is kind of a strange one.
[00:06:44.000 --> 00:06:48.000]   It first asks you to imagine a collection of all possible universes.
[00:06:48.000 --> 00:06:54.000]   This gets given a very fancy capital omega letter.
[00:06:54.000 --> 00:07:00.000]   And then asks you to define a function that, given a universe, spits out the outcome of some event.
[00:07:00.000 --> 00:07:06.000]   So imagine it's a scientist with a microscope that can look into --
[00:07:06.000 --> 00:07:10.000]   or a telescope that can look into universes, alternate universes, and say,
[00:07:10.000 --> 00:07:16.000]   oh, in this universe, the die came up a one. In this universe, the number of tweets was 3,000
[00:07:16.000 --> 00:07:20.000]   in the next millisecond. So it's spitting out the outcome of the event.
[00:07:20.000 --> 00:07:25.000]   That function is what we call a random variable.
[00:07:25.000 --> 00:07:31.000]   So there's some minor restrictions on this function, but unless you are a hardcore analysis kind of person,
[00:07:31.000 --> 00:07:36.000]   you aren't going to -- you aren't going to accidentally come up with one that makes this break.
[00:07:36.000 --> 00:07:39.000]   So don't worry about it too much.
[00:07:39.000 --> 00:07:44.000]   So just in picture form, we've got our collection of all possible universes on the left,
[00:07:44.000 --> 00:07:50.000]   the collection of objects of type universe, and then we define a function that takes an object
[00:07:50.000 --> 00:07:56.000]   of type universe and returns an object of type integer or type Boolean or whatever.
[00:07:56.000 --> 00:08:00.000]   So in the case of the number of tweets in the next millisecond, it's something --
[00:08:00.000 --> 00:08:04.000]   it's a function that takes in a universe and returns an integer.
[00:08:04.000 --> 00:08:08.000]   So that's written in the bottom right-hand corner in this sort of type notation.
[00:08:08.000 --> 00:08:13.000]   Number of tweets is a function that goes from universes to integers.
[00:08:13.000 --> 00:08:19.000]   So maybe there's 6,000 in a couple of universes. Maybe there's 10,000 in another universe.
[00:08:19.000 --> 00:08:23.000]   That's the one where, you know, Donald Trump just tweeted in the last second,
[00:08:23.000 --> 00:08:27.000]   and the other one is one where that didn't happen.
[00:08:27.000 --> 00:08:31.000]   Maybe there's one where there's zero tweets in the next millisecond because Twitter went down.
[00:08:31.000 --> 00:08:33.000]   So these are all the different possibilities.
[00:08:33.000 --> 00:08:38.000]   This is where the sort of randomness in the number of tweets in the next millisecond comes from,
[00:08:38.000 --> 00:08:43.000]   the fact that we don't know which of these universes we're in, you might think of.
[00:08:43.000 --> 00:08:47.000]   So the same thing can be done for the next gradient update.
[00:08:47.000 --> 00:08:49.000]   It now returns floating-point numbers.
[00:08:49.000 --> 00:08:54.000]   Maybe a lot of the times our gradients are very sparse, and so we get zero gradients.
[00:08:54.000 --> 00:08:58.000]   But then other times we get, you know, relatively small numbers like 10 to the minus third,
[00:08:58.000 --> 00:09:03.000]   and, you know, all possible numbers maybe come out of all these different universes
[00:09:03.000 --> 00:09:10.000]   in which different -- our process of sampling the next batch draws a different set of elements
[00:09:10.000 --> 00:09:13.000]   to go into our network.
[00:09:13.000 --> 00:09:21.000]   So that -- so number of tweets and next gradient are properly considered functions,
[00:09:21.000 --> 00:09:26.000]   but we call them random variables because we want to manipulate them like variables.
[00:09:26.000 --> 00:09:30.000]   We want to be able to add them together and multiply them and apply functions to them
[00:09:30.000 --> 00:09:35.000]   and things like that the same way we work with things that we actually know the value of.
[00:09:35.000 --> 00:09:39.000]   But in order to actually define them, we needed to define them as functions.
[00:09:39.000 --> 00:09:45.000]   So now we're going to go through how using that definition we can understand what lets you add
[00:09:45.000 --> 00:09:50.000]   and multiply and divide random variables.
[00:09:50.000 --> 00:09:52.000]   But first a quick summary.
[00:09:52.000 --> 00:09:56.000]   So we've got our universe on the left-hand side.
[00:09:56.000 --> 00:10:00.000]   This is, you know, maybe this whole collection of universes is omega.
[00:10:00.000 --> 00:10:01.000]   We've got our outcomes.
[00:10:01.000 --> 00:10:06.000]   Maybe it's all real numbers on the right-hand side.
[00:10:06.000 --> 00:10:13.000]   And then our random variable, our random real number is X, which is actually a function
[00:10:13.000 --> 00:10:16.000]   that spits out real numbers.
[00:10:16.000 --> 00:10:19.000]   So I think I've already talked a little bit about why this matters,
[00:10:19.000 --> 00:10:22.000]   so maybe this slide has become a little bit redundant.
[00:10:22.000 --> 00:10:27.000]   But the biggest thing that I want to point out is that strong command of this unintuitive definition
[00:10:27.000 --> 00:10:33.000]   will help you when it comes time to extend this definition and work in places where your intuition
[00:10:33.000 --> 00:10:37.000]   might fail you or lead you astray.
[00:10:37.000 --> 00:10:45.000]   So the first thing is that it's clear how to define new random variables from old ones based off of this definition.
[00:10:45.000 --> 00:10:52.000]   And the way we define a new random variable based off an old one is maybe we want the number of tweets
[00:10:52.000 --> 00:11:04.000]   in the next millisecond, but maybe we want to say whether that number is above 10,000 or below 10,000, right?
[00:11:04.000 --> 00:11:08.000]   Because that's what's going to cause higher load on our server, right?
[00:11:08.000 --> 00:11:17.000]   So normally if we want to check the numbers above or below 10,000, we pass it to a function that returns true or false
[00:11:17.000 --> 00:11:20.000]   if it's above or below that cutoff.
[00:11:20.000 --> 00:11:27.000]   So we can't pass a random variable directly to a function because a random variable is a function.
[00:11:27.000 --> 00:11:32.000]   So if I try to ask, "Is this function bigger than 10,000?"
[00:11:32.000 --> 00:11:35.000]   Python or whatever my language is, is going to throw an error.
[00:11:35.000 --> 00:11:39.000]   What we do instead with functions is we compose them.
[00:11:39.000 --> 00:11:49.000]   So I've represented that here by drawing these sort of like arrows for the functions and putting them tip to tail.
[00:11:49.000 --> 00:11:52.000]   So something comes out. Let me do a little pointing here.
[00:11:52.000 --> 00:11:54.000]   Something comes out of this function.
[00:11:54.000 --> 00:11:59.000]   This is a random, an outcome of, say, number of tweets.
[00:11:59.000 --> 00:12:05.000]   It goes into our function, say, "is bigger than 10,000," and out comes something else,
[00:12:05.000 --> 00:12:10.000]   a different type possibly, like a Boolean that says true or false.
[00:12:10.000 --> 00:12:22.000]   And this defines a new random variable that we introduce this notation for, which is kind of a strange notation.
[00:12:22.000 --> 00:12:27.000]   It says instead of saying, "Oh, this is f of x of something," which is how we'd normally do it,
[00:12:27.000 --> 00:12:29.000]   we write it as f of x.
[00:12:29.000 --> 00:12:36.000]   We pretend that our random variable is a normal variable that can go into functions, and then we rewrite it this way.
[00:12:36.000 --> 00:12:41.000]   But when we write it that way, we really mean something like this.
[00:12:41.000 --> 00:12:45.000]   So this has the utility of making it easier to write things once we understand what we're talking about,
[00:12:45.000 --> 00:12:51.000]   but it can kind of trip people up when they're trying to learn these definitions for the first time.
[00:12:51.000 --> 00:12:54.000]   We can also get rules for the distribution of f of x based off of this,
[00:12:54.000 --> 00:12:59.000]   and it's much easier to understand where they're coming from from looking at a diagram like this
[00:12:59.000 --> 00:13:01.000]   than from the baseline definition.
[00:13:01.000 --> 00:13:10.000]   So if you've learned maybe the Jacobian rule for -- Jacobian determinant rule for changes of variables,
[00:13:10.000 --> 00:13:14.000]   it comes out of this.
[00:13:14.000 --> 00:13:17.000]   And then this is where we get to define our algebra of random variables,
[00:13:17.000 --> 00:13:22.000]   where we get to take our algebra that we normally use, we can add numbers together, multiply them,
[00:13:22.000 --> 00:13:25.000]   we can do it with random variables also.
[00:13:25.000 --> 00:13:31.000]   So the idea is if we have two random variables that both operate on this same collection of universes,
[00:13:31.000 --> 00:13:40.000]   then we can define their sum by, say, applying both of them, getting now two possible outcomes,
[00:13:40.000 --> 00:13:45.000]   an outcome of x and an outcome of y, maybe the number of tweets in the next millisecond
[00:13:45.000 --> 00:13:51.000]   and the number of tweets a second from now, and then we can add them together.
[00:13:51.000 --> 00:13:57.000]   So the way that this plus works is it takes these two -- this is actually your normal plus.
[00:13:57.000 --> 00:14:03.000]   It takes two arguments and adds them together, so like the dunder add function in Python
[00:14:03.000 --> 00:14:10.000]   is maybe what you should picture, a function that takes in two things and returns the sum of those two things.
[00:14:10.000 --> 00:14:17.000]   And we can define now the sum of x and y by composing these two arrows together,
[00:14:17.000 --> 00:14:21.000]   by first getting two outcomes, then adding those outcomes together.
[00:14:21.000 --> 00:14:28.000]   Now this is a new map, which this combination of these two maps is a new map that goes from universes
[00:14:28.000 --> 00:14:38.000]   to outcomes, but now it's computing a sum instead of computing just the outcome of one or the other.
[00:14:38.000 --> 00:14:41.000]   So this is a new random variable.
[00:14:41.000 --> 00:14:47.000]   So this works for times, this works for minus, this works for more other possible things.
[00:14:47.000 --> 00:14:50.000]   It's also the case that we can do it for data types.
[00:14:50.000 --> 00:14:54.000]   We can make random lists, we can make random dictionaries, random tuples,
[00:14:54.000 --> 00:14:57.000]   all the kinds of data types we work with in computer science,
[00:14:57.000 --> 00:15:04.000]   using the sort of reasoning with diagrams like this, we can define what it means to, say,
[00:15:04.000 --> 00:15:07.000]   take a pair of random variables.
[00:15:07.000 --> 00:15:12.000]   And this actually leads to a really powerful approach for understanding these kinds of data types
[00:15:12.000 --> 00:15:17.000]   and for understanding what it is we're doing when we're combining these arrows together.
[00:15:17.000 --> 00:15:22.000]   And lastly, this is where it's really valuable, is we can even define random functions.
[00:15:22.000 --> 00:15:28.000]   So just as random variables were actually these deterministic functions,
[00:15:28.000 --> 00:15:32.000]   random functions are actually deterministic higher-order functions.
[00:15:32.000 --> 00:15:35.000]   So now what's happening here is we're looking into a universe,
[00:15:35.000 --> 00:15:40.000]   and that universe is spitting out not just a single number, but a function.
[00:15:40.000 --> 00:15:43.000]   So I've represented that here by this tiny arrow.
[00:15:43.000 --> 00:15:46.000]   So notice that this tiny arrow is much smaller than these big arrows.
[00:15:46.000 --> 00:15:50.000]   This is maybe bad notation, but bear with me.
[00:15:50.000 --> 00:15:52.000]   So we look into our universe.
[00:15:52.000 --> 00:15:57.000]   It spits out a function from A to B.
[00:15:57.000 --> 00:16:02.000]   We write that now with a little f instead of like the capital X that I was using in the past
[00:16:02.000 --> 00:16:06.000]   that represented a variable or capital Y or something like that.
[00:16:06.000 --> 00:16:10.000]   So this is now a function that returns a function.
[00:16:10.000 --> 00:16:15.000]   So in computer science, we think of those as higher-order functions.
[00:16:15.000 --> 00:16:17.000]   So it's like a lambda in Python.
[00:16:17.000 --> 00:16:21.000]   It can take an argument and give it to another function.
[00:16:21.000 --> 00:16:27.000]   So when we get this random function out,
[00:16:27.000 --> 00:16:31.000]   if we want to get a random variable, a single value,
[00:16:31.000 --> 00:16:33.000]   we pass an argument to that function.
[00:16:33.000 --> 00:16:41.000]   So it turns out you can represent the argument that goes into a function also with an arrow.
[00:16:41.000 --> 00:16:49.000]   So this guy takes a function that goes from A to B, gives it the argument A, and returns B.
[00:16:49.000 --> 00:16:54.000]   So this is something that maybe you would use a lambda for in Python.
[00:16:54.000 --> 00:16:58.000]   And then this whole thing we write as f applied to the value A.
[00:16:58.000 --> 00:17:03.000]   And that's evaluating a random function at a point A.
[00:17:03.000 --> 00:17:06.000]   And so random functions, you may not have come across them that much
[00:17:06.000 --> 00:17:09.000]   or may not have realized you were coming across them that much,
[00:17:09.000 --> 00:17:14.000]   but random functions are actually part of some of these really important recent models
[00:17:14.000 --> 00:17:18.000]   for understanding neural networks based off of the neural tangent kernel
[00:17:18.000 --> 00:17:21.000]   and the Gaussian process model of neural networks.
[00:17:21.000 --> 00:17:24.000]   Gaussian processes are random functions,
[00:17:24.000 --> 00:17:28.000]   and neural networks with random weights are also random functions.
[00:17:28.000 --> 00:17:32.000]   And people tend to get tripped up when they try and understand these things
[00:17:32.000 --> 00:17:37.000]   because their understanding of random variables is maybe incomplete
[00:17:37.000 --> 00:17:45.000]   because they haven't gone all the way through and understood this definition thoroughly.
[00:17:45.000 --> 00:17:50.000]   So I had a little section here on stochastic gradient descent,
[00:17:50.000 --> 00:17:55.000]   but actually I think I want to keep the focus of the talk just on that random variables material,
[00:17:55.000 --> 00:17:59.000]   and maybe we'll cover this stochastic gradient descent stuff later.
[00:17:59.000 --> 00:18:02.000]   But you'll find if you look really closely,
[00:18:02.000 --> 00:18:06.000]   stochastic gradient descent is none of those things.
[00:18:06.000 --> 00:18:09.000]   But we're going to skip over that
[00:18:09.000 --> 00:18:16.000]   and take any questions from the audience about this material.
[00:18:16.000 --> 00:18:22.000]   I would actually love for us to go over the stochastic gradient descent stuff if you want to.
[00:18:22.000 --> 00:18:25.000]   Okay, maybe just a quick pause.
[00:18:25.000 --> 00:18:29.000]   I'll give it maybe a few seconds for the YouTube people to catch up.
[00:18:29.000 --> 00:18:34.000]   If anybody on Slack has questions.
[00:18:34.000 --> 00:18:36.000]   I have a question.
[00:18:36.000 --> 00:18:41.000]   So you said the sum of two random variables is also a random variable.
[00:18:41.000 --> 00:18:47.000]   And I'm wondering, does this randomness increase when you combine two random variables?
[00:18:47.000 --> 00:18:54.000]   And also, what happens to the underlying distribution that you're pulling random variables from?
[00:18:54.000 --> 00:18:56.000]   Yeah, that's a great question.
[00:18:56.000 --> 00:19:03.000]   So one thing that didn't come up that much in this was what are the distributions of these random variables, right?
[00:19:03.000 --> 00:19:10.000]   So there is a way to calculate these distributions.
[00:19:10.000 --> 00:19:12.000]   It's basically called a push forward.
[00:19:12.000 --> 00:19:16.000]   You take the distribution and you push it through the function.
[00:19:16.000 --> 00:19:20.000]   And then you watch how things get stretched and scaled.
[00:19:20.000 --> 00:19:28.000]   So there's a definition involving integrals and things to get you what that actual expression is.
[00:19:28.000 --> 00:19:38.000]   So for adding random variables, if the two variables are independent, you convolve their distributions.
[00:19:38.000 --> 00:19:48.000]   So you literally take the distribution and you flip it around and slide it by multiplying and summing.
[00:19:48.000 --> 00:19:51.000]   The same way you do a convolution in a neural network.
[00:19:51.000 --> 00:19:54.000]   So there's a way to see where that comes from.
[00:19:54.000 --> 00:20:00.000]   That basically falls out as the trick that you can use when you're adding two independent random variables.
[00:20:00.000 --> 00:20:03.000]   If the variables are dependent on each other, all bets are off.
[00:20:03.000 --> 00:20:08.000]   And it can become basically arbitrarily difficult to figure out what their distribution is.
[00:20:08.000 --> 00:20:21.000]   But in general, I think you can't say that the variance either goes up or goes down when you add two random variables.
[00:20:21.000 --> 00:20:23.000]   They usually go up.
[00:20:23.000 --> 00:20:26.000]   The baseline case is that they go up.
[00:20:26.000 --> 00:20:28.000]   But say I have two random variables.
[00:20:28.000 --> 00:20:32.000]   One is defined as a coin flip that gives me plus one and minus one.
[00:20:32.000 --> 00:20:37.000]   And the other one is defined as check that same coin.
[00:20:37.000 --> 00:20:40.000]   But if it comes up heads, give me the opposite answer.
[00:20:40.000 --> 00:20:43.000]   So now if I add those two variables together, they're always zero.
[00:20:43.000 --> 00:20:45.000]   So the variance is gone.
[00:20:45.000 --> 00:20:47.000]   So it's now deterministic.
[00:20:47.000 --> 00:20:51.000]   So you have to be a little bit careful about making general statements.
[00:20:51.000 --> 00:20:55.000]   But most people think of adding random variables as increasing randomness.
[00:20:55.000 --> 00:20:57.000]   >> Gotcha.
[00:20:57.000 --> 00:20:59.000]   That makes it a lot more intuitive.
[00:20:59.000 --> 00:21:01.000]   Also, does it break?
[00:21:01.000 --> 00:21:14.000]   Is there any point, like when you're creating functions of random variables, that this paradigm breaks and the result that you get from these functions becomes meaningless because there's so much randomness?
[00:21:14.000 --> 00:21:16.000]   Or does that not matter?
[00:21:16.000 --> 00:21:22.000]   >> Yeah, so what I would say is, yeah, you can compose functions together.
[00:21:22.000 --> 00:21:31.000]   You can take -- and people, in fact, do take things as complicated as neural networks and treat them as random variables or random functions.
[00:21:31.000 --> 00:21:34.000]   And there are many, many functions lined up together.
[00:21:34.000 --> 00:21:38.000]   You can do arbitrary combinations of random variables.
[00:21:38.000 --> 00:21:48.000]   They do -- I mean, you know, if you're combining things that are independent, what you find is that you end up -- you lose lots of structure.
[00:21:48.000 --> 00:21:53.000]   So like the central limit theorem says if I add a bunch of independent things, the thing that comes out is Gaussian.
[00:21:53.000 --> 00:21:55.000]   And so it doesn't matter.
[00:21:55.000 --> 00:22:05.000]   There's random matrix theory that says if I take a really big random matrix and all the entries are independent, the resulting eigenvalues always follow a specific distribution.
[00:22:05.000 --> 00:22:14.000]   So there's a general thing that says, whoa, if you're combining a bunch of independent random things, what you get is actually something that's less complicated than all of its pieces.
[00:22:14.000 --> 00:22:17.000]   So those are the main examples I would think of.
[00:22:17.000 --> 00:22:34.000]   The only way this paradigm breaks down is there is this restriction of measurability on functions that's important for anybody who's trying to come up with, I don't know, like a Bonnock-Tarski paradox kind of thing or trying to, you know, come up with some really weird function that returns --
[00:22:34.000 --> 00:22:39.000]   yeah.
[00:22:39.000 --> 00:22:44.000]   Non-measurable functions are actually pretty hard to construct, so I'm not going to come up with an example right now.
[00:22:44.000 --> 00:22:48.000]   We should tell you just how, like, broad this thing covers.
[00:22:48.000 --> 00:22:52.000]   It's the kind of thing mathematicians and analysts really worry about.
[00:22:52.000 --> 00:22:57.000]   But when you're doing machine learning and working with random variables, it basically never comes up.
[00:22:57.000 --> 00:22:59.000]   >> Gotcha.
[00:22:59.000 --> 00:23:00.000]   All right.
[00:23:00.000 --> 00:23:01.000]   Cool.
[00:23:01.000 --> 00:23:02.000]   Let's see.
[00:23:02.000 --> 00:23:10.000]   I know Kyle had a question, so Kyle, if you want to jump in and ask -- there it is.
[00:23:10.000 --> 00:23:12.000]   Go ahead.
[00:23:12.000 --> 00:23:15.000]   >> Yeah, it looks like Kyle asked the question in the Q&A, yeah?
[00:23:15.000 --> 00:23:23.000]   So when we talk about neural networks as random functions, is this still the case once the network has undergone training?
[00:23:23.000 --> 00:23:29.000]   So to think of a neural network as a random function, you usually think of the weights as being random, right?
[00:23:29.000 --> 00:23:33.000]   So you're doing, like, a Bayesian neural network.
[00:23:33.000 --> 00:23:44.000]   So you're saying the -- you're saying I started off with a distribution over possible weights, like my prior told me my weights were somewhere between --
[00:23:44.000 --> 00:23:51.000]   were somewhere between minus 1 and plus 1 uniformly, or they came from a Gaussian distribution.
[00:23:51.000 --> 00:23:57.000]   And a Bayesian -- a fully Bayesian approach to neural network training never drops that randomness.
[00:23:57.000 --> 00:24:06.000]   It says, oh, each gradient update tells me to move this Gaussian around, usually make it much, much skinnier and put it around a specific value,
[00:24:06.000 --> 00:24:08.000]   but I still keep a distribution.
[00:24:08.000 --> 00:24:15.000]   And so then the neural network remains a random function in that to evaluate it, you would have to draw a specific set of weights,
[00:24:15.000 --> 00:24:21.000]   and that specific set of weights is drawn at random.
[00:24:21.000 --> 00:24:27.000]   So -- but what we do normally, right, is we just draw some specific thing, and then we use the gradient updates to change them,
[00:24:27.000 --> 00:24:32.000]   and the network remains a deterministic function all the time, unless you use dropout,
[00:24:32.000 --> 00:24:38.000]   in which case that network is also a random function.
[00:24:38.000 --> 00:24:48.000]   Yeah, but I think -- but that doesn't come up quite as much, the idea of thinking of dropout as something that makes your network a random function.
[00:24:48.000 --> 00:24:53.000]   So I would say that it's really more the idea that if you keep a distribution over weights,
[00:24:53.000 --> 00:24:58.000]   then you get a function that is -- then your neural network is a random function,
[00:24:58.000 --> 00:25:06.000]   and this leads to this Gaussian processes view of neural networks and their loss surfaces
[00:25:06.000 --> 00:25:16.000]   that has been making a lot of waves in the theoretical community in the form of the neural tangent kernel over the past year or so.
[00:25:16.000 --> 00:25:24.000]   What if you pulled the nodes that you drop out from a distribution, then you would remove the randomness?
[00:25:24.000 --> 00:25:30.000]   So let's see. If you're pulling the nodes that you drop out from a distribution, and those nodes are, yeah, random,
[00:25:30.000 --> 00:25:34.000]   then, yeah, the neural network is going to be a random function.
[00:25:34.000 --> 00:25:39.000]   I think -- so it seems like one other -- you know, some other people want actually to go through the SGD example,
[00:25:39.000 --> 00:25:53.000]   so we might as well just do it. Yeah, so -- okay, so stochastic gradient descent.
[00:25:53.000 --> 00:26:01.000]   It's a common algorithm, and yet it is one that this alligator pair problem hits pretty badly,
[00:26:01.000 --> 00:26:10.000]   especially on the gradient part, to a lesser extent on the descent part, and still a bit on the stochastic part.
[00:26:10.000 --> 00:26:13.000]   So I have another little motivation slide.
[00:26:13.000 --> 00:26:20.000]   So Voltaire, who was an influencer in the 18th century, wrote in his history of Europe,
[00:26:20.000 --> 00:26:26.000]   the agglomeration which was called and which still calls itself the Holy Roman Empire, pictured here on the right,
[00:26:26.000 --> 00:26:30.000]   was neither holy nor Roman nor an empire.
[00:26:30.000 --> 00:26:36.000]   So it wasn't holy because it actually spent most of its existence fighting with the Pope, which is not very holy.
[00:26:36.000 --> 00:26:43.000]   It wasn't Roman because it was, you know, in Germany, which was never really part of the Roman Empire,
[00:26:43.000 --> 00:26:48.000]   and nor were the people in there ever Romans, and it wasn't really an empire because the entire thing was basically
[00:26:48.000 --> 00:26:56.000]   a huge mess of princes and bishops squabbling with each other, and it never really behaved as a coherent unit.
[00:26:56.000 --> 00:27:02.000]   So my update to that is that this algorithm which was called and is still called stochastic gradient descent
[00:27:02.000 --> 00:27:08.000]   is neither stochastic nor of a gradient nor a descent method.
[00:27:08.000 --> 00:27:13.000]   So the first part, stochastic gradient descent is not a descent method.
[00:27:13.000 --> 00:27:24.000]   A descent method for minimizing a function f is a method such that if I evaluate the function on each consecutive iterate
[00:27:24.000 --> 00:27:36.000]   of that method, so the method produces theta's parameter values over time t from 0 to capital T or from 0 on out,
[00:27:36.000 --> 00:27:49.000]   a descent method says the next value of the parameters will give me a lower value for the function.
[00:27:49.000 --> 00:27:56.000]   And SGD only promises this on average. SGD can in fact go up. It doesn't in fact descend.
[00:27:56.000 --> 00:28:05.000]   It goes up sometimes. Anybody who's looked at a neural network training curve has seen this phenomenon occur.
[00:28:05.000 --> 00:28:13.000]   And gradient descent, where you don't have the stochastic part, almost does this,
[00:28:13.000 --> 00:28:19.000]   but it only does it if you have a small enough step size or if you're shrinking the step size.
[00:28:19.000 --> 00:28:23.000]   So even gradient descent is actually not a descent method.
[00:28:23.000 --> 00:28:29.000]   And the reason why this is important is because if you actually go and you look at the analysis of descent methods,
[00:28:29.000 --> 00:28:34.000]   how do we prove they converge, in what cases do they converge, what causes them to break,
[00:28:34.000 --> 00:28:41.000]   the theoretical analysis and the practical -- the theoretical analysis is just very different for descent methods
[00:28:41.000 --> 00:28:47.000]   and for something like SGD. They're completely different, actually, when you look at the details of the math.
[00:28:47.000 --> 00:28:53.000]   And then the practical behavior is also different. Seeing values go slightly up in gradient descent
[00:28:53.000 --> 00:28:59.000]   and especially stochastic gradient descent, par for the course, seeing values go up for a descent method
[00:28:59.000 --> 00:29:07.000]   means you implemented it wrong. So this is -- these are very, very different things
[00:29:07.000 --> 00:29:13.000]   when it comes time to debug or to analyze.
[00:29:13.000 --> 00:29:18.000]   The second one, and this is perhaps the strongest leg I have to stand on in this rant,
[00:29:18.000 --> 00:29:22.000]   is that stochastic gradient descent does not use gradients.
[00:29:22.000 --> 00:29:29.000]   So a gradient for a function f is a function nabla f that satisfies this relationship here,
[00:29:29.000 --> 00:29:37.000]   which basically says if I evaluate the function at a certain -- if I want to know -- oh, shoot.
[00:29:37.000 --> 00:29:41.000]   There's a little bit of an error here. This nabla here should not be there.
[00:29:41.000 --> 00:29:51.000]   So ignore that guy. If I look at the function's value at some point epsilon away from a point theta,
[00:29:51.000 --> 00:30:01.000]   then it should be approximately equal to a linear function times how far away I went.
[00:30:01.000 --> 00:30:06.000]   So that's what this inner product here says. So this applies for scalar-valued functions
[00:30:06.000 --> 00:30:11.000]   that take in scalars, vectors, and matrices.
[00:30:11.000 --> 00:30:16.000]   And importantly, this is true for every choice of theta.
[00:30:16.000 --> 00:30:21.000]   But stochastic gradient descent, as we use in NML, works perfectly well for functions
[00:30:21.000 --> 00:30:26.000]   that don't have gradients everywhere. So I have two examples on the right-hand side,
[00:30:26.000 --> 00:30:29.000]   both of which are very popular in machine learning.
[00:30:29.000 --> 00:30:35.000]   One is the rectified linear unit on the top, and the other is the absolute value,
[00:30:35.000 --> 00:30:39.000]   which in high-dimensional cases, this would be the L1 norm,
[00:30:39.000 --> 00:30:49.000]   which shows up as a sparseness penalty in sparse activations or sparse parameter regularization schemes.
[00:30:49.000 --> 00:30:53.000]   So these functions get used, but if I try and ask, is there a function here
[00:30:53.000 --> 00:30:59.000]   that is a good linear approximation of this function, it breaks apart.
[00:30:59.000 --> 00:31:05.000]   Actually, it's easiest to see here. There's no good linear approximation to this function at the point 0.
[00:31:05.000 --> 00:31:10.000]   I could get this side perfectly right, but then just completely whiff this side,
[00:31:10.000 --> 00:31:13.000]   or I could do the same on the other.
[00:31:13.000 --> 00:31:16.000]   So this function does not have gradients everywhere.
[00:31:16.000 --> 00:31:19.000]   What it has everywhere is something called a subgradient.
[00:31:19.000 --> 00:31:25.000]   And more broadly, when you want to go to high-dimensional cases
[00:31:25.000 --> 00:31:31.000]   and you want to go to functions that are non-convex, it's something called a subdifferential.
[00:31:31.000 --> 00:31:38.000]   So these subgradients are actually super, super important for convex analysis,
[00:31:38.000 --> 00:31:43.000]   the study of convex functions, which is where all the ideas in optimization come from.
[00:31:43.000 --> 00:31:51.000]   All of our ideas of how do we optimize functions, how do we find the lowest values of functions,
[00:31:51.000 --> 00:31:55.000]   even when we apply them on non-convex functions, on generic functions,
[00:31:55.000 --> 00:32:00.000]   we use ideas that came from convex optimization and convex analysis.
[00:32:00.000 --> 00:32:06.000]   And so one of those central ideas is the subgradient.
[00:32:06.000 --> 00:32:12.000]   So it's not only a central -- it's actually what's really being implemented in something like TensorFlow
[00:32:12.000 --> 00:32:15.000]   when they compute gradients for you.
[00:32:15.000 --> 00:32:19.000]   It's also something that is a key theoretical tool,
[00:32:19.000 --> 00:32:26.000]   and the fact that it's being used is completely swept to the side by the name gradient descent.
[00:32:26.000 --> 00:32:32.000]   And then lastly, and this is the weakest leg of my three-leg argument against stochastic gradient descent,
[00:32:32.000 --> 00:32:33.000]   but bear with me.
[00:32:33.000 --> 00:32:39.000]   A stochastic method is one that generates or uses random variables.
[00:32:39.000 --> 00:32:42.000]   And we just talked about the definition of random variables.
[00:32:42.000 --> 00:32:46.000]   One thing that, depending on your definition of randomness,
[00:32:46.000 --> 00:32:51.000]   really the things that go on inside of a computer are at best pseudo-random,
[00:32:51.000 --> 00:32:55.000]   which means they appear random to a person who's not looking very closely,
[00:32:55.000 --> 00:32:58.000]   but they are in fact entirely deterministic.
[00:32:58.000 --> 00:33:03.000]   If you were to run your code again in the exact same setting,
[00:33:03.000 --> 00:33:08.000]   then the code would produce the exact same output.
[00:33:08.000 --> 00:33:16.000]   So the way that this is managed is that there is a seed number that determines the behavior of your code.
[00:33:16.000 --> 00:33:22.000]   So if you set a seed, the behavior of stochastic gradient descent becomes entirely deterministic.
[00:33:22.000 --> 00:33:29.000]   So if I want, I can actually recreate the exact training process for a network
[00:33:29.000 --> 00:33:36.000]   if I know the seeds for the pseudo-random number generators that went into training that network.
[00:33:36.000 --> 00:33:44.000]   This is very different from an actual stochastic algorithm that has actual access to random values.
[00:33:44.000 --> 00:33:48.000]   So one that, say, queries the behavior of this--
[00:33:48.000 --> 00:33:54.000]   there's a famous qubit. I forget who measures it, but there's a famous quantum system
[00:33:54.000 --> 00:33:56.000]   that produces honest-to-God random values.
[00:33:56.000 --> 00:34:00.000]   And if somebody checked that qubit before running stochastic gradient descent
[00:34:00.000 --> 00:34:03.000]   and used that in order to run their algorithm,
[00:34:03.000 --> 00:34:09.000]   you'd never be able to guarantee you'd recreated their training process.
[00:34:09.000 --> 00:34:13.000]   So this is important for a couple of reasons,
[00:34:13.000 --> 00:34:15.000]   and the biggest one is that it's important for debugging.
[00:34:15.000 --> 00:34:18.000]   We can actually--because the algorithm is deterministic,
[00:34:18.000 --> 00:34:21.000]   we can recreate it and we can interrogate it at a later point in time
[00:34:21.000 --> 00:34:26.000]   the same way we can any other deterministic algorithm in a computer.
[00:34:26.000 --> 00:34:28.000]   So we can do testing, we can do unit testing,
[00:34:28.000 --> 00:34:33.000]   we can do searching through and printing statements--
[00:34:33.000 --> 00:34:37.000]   doing print statements to pull out variables and looking at what their values were.
[00:34:37.000 --> 00:34:41.000]   We can understand what happened during the algorithm with perfect clarity,
[00:34:41.000 --> 00:34:43.000]   no stochasticity needed.
[00:34:43.000 --> 00:34:49.000]   Verifying the behavior of a stochastic algorithm with tests is really, really hard.
[00:34:49.000 --> 00:34:53.000]   And it's actually quite lucky for us that our stochastic gradient descent
[00:34:53.000 --> 00:34:58.000]   is not so stochastic that it cannot be made deterministic.
[00:34:58.000 --> 00:35:02.000]   And another actual argument is that you can remove all the stochasticity
[00:35:02.000 --> 00:35:11.000]   from gradient descent by just--if you pick a fixed order for the values to come in
[00:35:11.000 --> 00:35:19.000]   and that order is not, say, all the instances of one class,
[00:35:19.000 --> 00:35:22.000]   then all the instances of another class, all the instances of a third class.
[00:35:22.000 --> 00:35:31.000]   If you just pick that order to be some permutation of the inputs,
[00:35:31.000 --> 00:35:33.000]   then you drop all the stochasticity.
[00:35:33.000 --> 00:35:36.000]   You don't have to randomly load batches and keep flipping coins
[00:35:36.000 --> 00:35:39.000]   to decide which elements go into your gradient descent
[00:35:39.000 --> 00:35:41.000]   and which ones don't on a given batch.
[00:35:41.000 --> 00:35:49.000]   You just go through it in order, but that order is just--
[00:35:49.000 --> 00:35:53.000]   it just can't be a really, really dumb order.
[00:35:53.000 --> 00:35:55.000]   And this is actually a weird behavior of gradient descent
[00:35:55.000 --> 00:35:57.000]   that people don't understand very well.
[00:35:57.000 --> 00:36:00.000]   That version of it does just as well as stochastic gradient descent.
[00:36:00.000 --> 00:36:04.000]   And there's basically--once the order of the data set is set,
[00:36:04.000 --> 00:36:06.000]   there's no randomness.
[00:36:06.000 --> 00:36:10.000]   The algorithm is completely deterministic.
[00:36:10.000 --> 00:36:22.000]   So I have here now a change.org petition, a call to arms to rename SGD to APSD.
[00:36:22.000 --> 00:36:27.000]   So hopefully folks will join me in referring it to instead
[00:36:27.000 --> 00:36:32.000]   as approximate pseudorandom subdifferential descent.
[00:36:32.000 --> 00:36:36.000]   Approximate descent being how we get around the problem of descent
[00:36:36.000 --> 00:36:38.000]   not being what we're doing.
[00:36:38.000 --> 00:36:42.000]   Pseudorandom to emphasize that this is not a stochastic algorithm
[00:36:42.000 --> 00:36:46.000]   and subdifferential so we actually name the thing that we are using
[00:36:46.000 --> 00:36:50.000]   rather than calling it a gradient just because that's easier.
[00:36:50.000 --> 00:36:57.000]   So that concludes my pedantic and completely unnecessary rant
[00:36:57.000 --> 00:37:00.000]   about the nature of stochastic gradient descent.
[00:37:00.000 --> 00:37:02.000]   That was amazing.
[00:37:02.000 --> 00:37:06.000]   A bunch of people said in the comments that you are hilarious,
[00:37:06.000 --> 00:37:07.000]   which we agree with.
[00:37:07.000 --> 00:37:10.000]   There's a question in the chat if you want to take it.
[00:37:10.000 --> 00:37:14.000]   Yeah, so Nicola V. asks, "What are some pros and cons
[00:37:14.000 --> 00:37:21.000]   of making the training process deterministic by fixing a 'random seed'?"
[00:37:21.000 --> 00:37:29.000]   So the biggest con is that there are some times that you really do want
[00:37:29.000 --> 00:37:33.000]   the network to behave randomly.
[00:37:33.000 --> 00:37:37.000]   So I focused on the fact that if you wanted to uncover some bugs
[00:37:37.000 --> 00:37:41.000]   in your implementation, you want to be able to recreate a given run
[00:37:41.000 --> 00:37:49.000]   in order to make sure that you could figure out what caused the problem.
[00:37:49.000 --> 00:37:55.000]   But if you just fix a single random seed, then you might actually miss
[00:37:55.000 --> 00:38:00.000]   that your algorithm doesn't have the random behavior that you would expect.
[00:38:00.000 --> 00:38:03.000]   So sometimes we can do things like take averages or expectations
[00:38:03.000 --> 00:38:08.000]   or otherwise remove out the effect of randomness by considering
[00:38:08.000 --> 00:38:10.000]   a large number of examples.
[00:38:10.000 --> 00:38:13.000]   And often we do want our algorithm to behave differently
[00:38:13.000 --> 00:38:17.000]   when it gets different values for the seed.
[00:38:17.000 --> 00:38:22.000]   So that would be, I think, the biggest con for making the training process
[00:38:22.000 --> 00:38:25.000]   deterministic by fixing a random seed.
[00:38:25.000 --> 00:38:34.000]   The other thing is that when -- there's actually been some recent papers
[00:38:34.000 --> 00:38:39.000]   that say that if you treat your random seed as a hyperparameter,
[00:38:39.000 --> 00:38:43.000]   you can get better performance on your validation set.
[00:38:43.000 --> 00:38:47.000]   So it's important that you make sure not to do that on the test set
[00:38:47.000 --> 00:38:50.000]   because now you're really, really just overfitting.
[00:38:50.000 --> 00:38:55.000]   But there are aspects of maybe some initializations of neural networks
[00:38:55.000 --> 00:38:59.000]   for certain kinds of problems are actually better than others.
[00:38:59.000 --> 00:39:04.000]   So instead of just trying multiple runs with the same hyperparameters,
[00:39:04.000 --> 00:39:08.000]   you might include a single random seed.
[00:39:08.000 --> 00:39:12.000]   Maybe there's an interaction between where you start on the loss surface,
[00:39:12.000 --> 00:39:15.000]   where you start your neural network, and what your learning rate is
[00:39:15.000 --> 00:39:20.000]   or how much dropout you do or whatever other -- your atom hyperparameters
[00:39:20.000 --> 00:39:22.000]   or whatever.
[00:39:22.000 --> 00:39:28.000]   So that's a somewhat controversial new approach in machine learning.
[00:39:28.000 --> 00:39:32.000]   People worry that we're really starting to just chase a few percentage points
[00:39:32.000 --> 00:39:34.000]   of accuracy.
[00:39:34.000 --> 00:39:37.000]   But that would be a con for setting a fixed random seed.
[00:39:37.000 --> 00:39:44.000]   You can't do that.
[00:39:44.000 --> 00:39:47.000]   >> Do you want to take the one in the Q&A section?
[00:39:47.000 --> 00:39:49.000]   >> Yeah.
[00:39:49.000 --> 00:39:54.000]   Kyle asks, "Is there a relationship between the fact that SGD isn't truly
[00:39:54.000 --> 00:39:57.000]   a descent method and the effectiveness of early stopping when training
[00:39:57.000 --> 00:40:00.000]   neural networks?"
[00:40:00.000 --> 00:40:02.000]   So not directly.
[00:40:02.000 --> 00:40:08.000]   So early -- so for folks, early stopping means that you sometimes want to stop
[00:40:08.000 --> 00:40:11.000]   training earlier rather than running forever.
[00:40:11.000 --> 00:40:15.000]   A naive analysis of optimization would say the longer you can run your algorithm
[00:40:15.000 --> 00:40:19.000]   for, the better you will do, even for stochastic gradient descent, even for
[00:40:19.000 --> 00:40:24.000]   non-descent methods, longer tends to just mean better.
[00:40:24.000 --> 00:40:28.000]   Because basically all our algorithms are motivated by calculus, which works
[00:40:28.000 --> 00:40:33.000]   when things are infinitely small or run for infinitely long.
[00:40:33.000 --> 00:40:37.000]   So -- but the reason for early stopping is actually not -- it's actually that
[00:40:37.000 --> 00:40:39.000]   we're optimizing the wrong thing.
[00:40:39.000 --> 00:40:44.000]   We're optimizing the training loss, whereas the thing we really want to optimize
[00:40:44.000 --> 00:40:46.000]   is the test loss.
[00:40:46.000 --> 00:40:49.000]   Or actually the test loss isn't really what we want to optimize.
[00:40:49.000 --> 00:40:55.000]   What we really want to optimize is our social or business outcome of, you know,
[00:40:55.000 --> 00:41:01.000]   increasing dollars per ad or decreasing the number of deaths to heart disease in
[00:41:01.000 --> 00:41:03.000]   the world.
[00:41:03.000 --> 00:41:07.000]   And so, you know, the training loss that we're actually optimizing is very far
[00:41:07.000 --> 00:41:10.000]   away from the thing that we want to make good.
[00:41:10.000 --> 00:41:13.000]   And so early stopping is one of those things that you include in order to try
[00:41:13.000 --> 00:41:17.000]   and do better on the thing that you don't have direct access to.
[00:41:17.000 --> 00:41:20.000]   So it's a little bit separate from this point about descent methods.
[00:41:20.000 --> 00:41:24.000]   But I think that's another -- that's a little bit of a tricky thing.
[00:41:24.000 --> 00:41:28.000]   As somebody who basically, you know, did my Ph.D. in optimization,
[00:41:28.000 --> 00:41:32.000]   optimization is not the most important part of making your neural network
[00:41:32.000 --> 00:41:34.000]   actually do the thing you want it to do.
[00:41:34.000 --> 00:41:39.000]   It's maybe the most fun part for people who like math, like me.
[00:41:39.000 --> 00:41:43.000]   But it's actually, you know, you're going to be missing a lot of the important
[00:41:43.000 --> 00:41:48.000]   context, whether it's out of distribution and test performance or the overall,
[00:41:48.000 --> 00:41:55.000]   you know, impact of your algorithm and its deployability and things like that.
[00:41:55.000 --> 00:42:02.000]   >> This is not related to your talk a lot, but you alluded to how the loss
[00:42:02.000 --> 00:42:06.000]   function is so far removed from what we're actually trying to optimize, which is
[00:42:06.000 --> 00:42:08.000]   the business objective.
[00:42:08.000 --> 00:42:13.000]   So do you have advice on bridging that gap that people, especially in the
[00:42:13.000 --> 00:42:17.000]   beginning of their careers or even far along, should be thinking about and how
[00:42:17.000 --> 00:42:19.000]   to close that gap?
[00:42:19.000 --> 00:42:22.000]   >> Yeah, I think in the beginning of your career, I think the thing that most
[00:42:22.000 --> 00:42:26.000]   people are missing is an understanding of the technical aspects of machine
[00:42:26.000 --> 00:42:28.000]   learning.
[00:42:28.000 --> 00:42:31.000]   And so I think at the beginning of your career, you should really focus on
[00:42:31.000 --> 00:42:33.000]   understanding those things better.
[00:42:33.000 --> 00:42:36.000]   Because to be honest, whatever you were doing when you weren't learning machine
[00:42:36.000 --> 00:42:41.000]   learning probably gave you the right, you know, the sense for whatever it is that
[00:42:41.000 --> 00:42:43.000]   you're going to apply it for.
[00:42:43.000 --> 00:42:47.000]   Maybe you did a -- I did a biology undergrad, so when I go to do machine
[00:42:47.000 --> 00:42:52.000]   learning applied to biology context, I have context that helps me understand what
[00:42:52.000 --> 00:42:56.000]   the real goals are and what the failure modes are.
[00:42:56.000 --> 00:43:01.000]   So at the beginning of your career, I think you should really focus on getting
[00:43:01.000 --> 00:43:05.000]   these technical aspects right, because they can bite you, and when they do bite
[00:43:05.000 --> 00:43:07.000]   you, they bite hard.
[00:43:07.000 --> 00:43:11.000]   So but for more people who are more experts and they're at the point they're
[00:43:11.000 --> 00:43:18.000]   actually deploying models, I would say the only solution is, like, careful
[00:43:18.000 --> 00:43:20.000]   thought.
[00:43:20.000 --> 00:43:25.000]   Like, maybe spend less time on fiddling with your hyperparameters and more time
[00:43:25.000 --> 00:43:30.000]   thinking about what will this model look like when it's in production.
[00:43:30.000 --> 00:43:35.000]   Get models to production as quickly as possible, unless you're in a security
[00:43:35.000 --> 00:43:39.000]   critical regime like medicine, and then learn from what happens when the model is
[00:43:39.000 --> 00:43:43.000]   deployed with sort of continuous -- there's, like, continuous sort of testing
[00:43:43.000 --> 00:43:48.000]   strategies to ensure that the model is not -- that the model is not starting to
[00:43:48.000 --> 00:43:50.000]   fail completely.
[00:43:50.000 --> 00:43:53.000]   There's a lot of people who are doing work in fairness and ethics in AI.
[00:43:53.000 --> 00:43:58.000]   This is another aspect in which our loss, which encourages us to do well in the
[00:43:58.000 --> 00:44:03.000]   aggregate, can cause us to do things that we actually did not want to do, which is
[00:44:03.000 --> 00:44:08.000]   to fail for groups that are underrepresented in the data set.
[00:44:08.000 --> 00:44:13.000]   And so to look out for that, among all the many other things that fairness and
[00:44:13.000 --> 00:44:19.000]   ethics researchers have pointed out as problems with machine learning algorithms.
[00:44:19.000 --> 00:44:23.000]   So, yeah, those would be, I think, my pieces of advice for people who are more
[00:44:23.000 --> 00:44:25.000]   -- who are out there deploying.
[00:44:25.000 --> 00:44:30.000]   But to be honest, you know, my expertise is in the actual understanding of the,
[00:44:30.000 --> 00:44:34.000]   you know, the mathematics and the pipeline of training the model.
[00:44:34.000 --> 00:44:37.000]   And so that's where I have the best advice to give.
[00:44:37.000 --> 00:44:40.000]   >> Thanks.
[00:44:40.000 --> 00:44:44.000]   Another thing that you could do to bridge the gap between business objectives and
[00:44:44.000 --> 00:44:48.000]   the last function is to capture more things that are closer to the business
[00:44:48.000 --> 00:44:51.000]   objectives in your training data.
[00:44:51.000 --> 00:44:56.000]   And make sure your training data captures all the features or all the variables
[00:44:56.000 --> 00:45:00.000]   that affect your business objectives, and then you'll optimize the right things.
[00:45:00.000 --> 00:45:07.000]   And somehow in this convoluted process, you'll optimize your business objective.
[00:45:07.000 --> 00:45:09.000]   Yes, Sean, you were saying something?
[00:45:09.000 --> 00:45:15.000]   >> Oh, no, yeah, definitely quality of the data comes absolutely first.
[00:45:15.000 --> 00:45:19.000]   This is something that I think gets lost on folks with an academic background
[00:45:19.000 --> 00:45:24.000]   like me, where the data set is some, you know, fixed benchmark usually.
[00:45:24.000 --> 00:45:27.000]   And the idea that we need to think very carefully about where the data came
[00:45:27.000 --> 00:45:36.000]   from and what that collection process implies about what the test distribution
[00:45:36.000 --> 00:45:39.000]   means that this thing might be optimizing.
[00:45:39.000 --> 00:45:44.000]   That, yeah, that's an important aspect that gets missed a lot in academia.
[00:45:44.000 --> 00:45:48.000]   In part because it's really hard to say anything satisfyingly academic about it.

