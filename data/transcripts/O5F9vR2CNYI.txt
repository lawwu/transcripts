
[00:00:00.000 --> 00:00:03.200]   Welcome back. We're going to be talking today about
[00:00:03.200 --> 00:00:08.240]   Random forests, we're going to finish building our own random forest from scratch
[00:00:08.240 --> 00:00:12.400]   But before we do I wanted to
[00:00:12.400 --> 00:00:16.840]   Tackle a few things that have come up during the week a few questions that I've had
[00:00:16.840 --> 00:00:22.600]   And I want to start with kind of the position of random forests in general, so we spent
[00:00:24.240 --> 00:00:30.040]   About half of this course doing random forests, and then after today the second half of this course will be
[00:00:30.040 --> 00:00:33.200]   neural networks broadly defined
[00:00:33.200 --> 00:00:40.800]   This is because these these two
[00:00:40.800 --> 00:00:46.260]   Represent like the tea the two key classes of techniques which cover
[00:00:46.260 --> 00:00:49.200]   Nearly everything that you're likely to need to do
[00:00:49.200 --> 00:00:53.880]   Random forests belong to the class of techniques of decision tree ensembles
[00:00:53.880 --> 00:01:00.680]   Along with gradient boosting machines being the other key type and some variants like extremely randomized trees
[00:01:00.680 --> 00:01:03.600]   They
[00:01:03.600 --> 00:01:06.560]   Have the benefit that they're highly interpretable
[00:01:06.560 --> 00:01:08.960]   scalable
[00:01:08.960 --> 00:01:11.400]   Flexible work well for most kinds of data
[00:01:11.400 --> 00:01:16.720]   They have the downside that they don't extrapolate at all to like
[00:01:16.720 --> 00:01:21.980]   Data that's outside the range that you've seen as we looked at at the end of last week's session
[00:01:23.840 --> 00:01:26.160]   but
[00:01:26.160 --> 00:01:29.160]   You know they're they're they're a great starting point and so
[00:01:29.160 --> 00:01:34.680]   I think you know there's a huge catalog of
[00:01:34.680 --> 00:01:39.280]   Machine learning tools out there and so and like a lot of courses and books
[00:01:39.280 --> 00:01:46.800]   Don't attempt to kind of curate that down and say like for these kinds of problems use this for these kinds of problems use that
[00:01:46.800 --> 00:01:49.600]   Finished you know, but they're rather like
[00:01:49.600 --> 00:01:52.600]   here's a description of a hundred different algorithms and
[00:01:53.960 --> 00:01:55.960]   You just don't need them
[00:01:55.960 --> 00:02:01.240]   You know like I don't see why you would ever use in support vector machine today for instance
[00:02:01.240 --> 00:02:04.120]   Like I know no reason at all
[00:02:04.120 --> 00:02:11.360]   I could think of doing that people loved studying them in the 90s because they are like very theoretically elegant and like you can really
[00:02:11.360 --> 00:02:18.760]   Write a lot of math about support vector machines and people did but you know in practice. I don't see them as having any place
[00:02:22.600 --> 00:02:28.880]   So there's like a lot of techniques that you could include in an exhaustive list of every way that people have looked at machine learning problems
[00:02:28.880 --> 00:02:34.480]   But I would rather tell you like how to actually solve machine learning problems in practice
[00:02:34.480 --> 00:02:40.720]   I think they you know we've we've about to finish today the first class which is you know one type of decision tree ensembles
[00:02:40.720 --> 00:02:44.320]   In in part two your net will tell you about the other key type
[00:02:44.320 --> 00:02:48.800]   They're being gradient boosting and we're about to launch next lesson into
[00:02:49.600 --> 00:02:53.360]   neural nets which includes all kinds of GLM
[00:02:53.360 --> 00:02:56.480]   Ridge regression elastic net Lasso
[00:02:56.480 --> 00:03:01.600]   Logistic aggression etc are all variants of neural nets
[00:03:01.600 --> 00:03:06.480]   You know interestingly
[00:03:06.480 --> 00:03:13.960]   Leo Bremen who created random forests did so very late in his life and
[00:03:13.960 --> 00:03:17.520]   Unfortunately passed away not many years later
[00:03:19.240 --> 00:03:24.760]   So partly because of that very little has been written about them in the academic literature
[00:03:24.760 --> 00:03:30.480]   Partly because SVMs were just taken over at that point. You know other people didn't look at them
[00:03:30.480 --> 00:03:40.560]   And also like just because they're like quite hard to grasp at a theoretical level like analyze them theoretically
[00:03:40.560 --> 00:03:44.520]   It's quite of hard to write conference papers about them or academic papers about them
[00:03:44.520 --> 00:03:46.840]   So there hasn't been that much written about them
[00:03:47.320 --> 00:03:55.320]   But there's been a real resurgence or not resurgence a new wave in recent years of empirical machine learning like what actually works
[00:03:55.320 --> 00:04:01.020]   Kaggle's been part of that, but also just part of it. It's just been like
[00:04:01.020 --> 00:04:06.240]   companies using machine learning to make shit loads of money like Amazon and Google and
[00:04:06.240 --> 00:04:09.920]   So nowadays a lot of people are writing about
[00:04:10.520 --> 00:04:17.080]   decision tree ensembles and creating better software for decision tree ensembles like like GBM and XG boost and
[00:04:17.080 --> 00:04:20.080]   Ranger for our and
[00:04:20.080 --> 00:04:22.160]   Psych hit learn and so forth
[00:04:22.160 --> 00:04:26.320]   But a lot of this is being done in industry rather than academia
[00:04:26.320 --> 00:04:30.800]   But you know it's it's encouraging to see
[00:04:30.800 --> 00:04:34.080]   There's certainly
[00:04:34.080 --> 00:04:38.520]   More work being done in deep learning than in decision tree ensembles
[00:04:39.400 --> 00:04:45.040]   Particularly in in academia, but but there's a lot of progress being made in both
[00:04:45.040 --> 00:04:51.400]   You know if you look at like of the packages being used today for decision tree ensembles
[00:04:51.400 --> 00:04:56.800]   Like all the best ones the top five or six, and I don't know that any of them really existed
[00:04:56.800 --> 00:05:04.280]   Five years ago, you know maybe other than like SK learn or even three years ago, so it's that's that's been good
[00:05:04.280 --> 00:05:07.640]   But I think there's a lot of work
[00:05:08.040 --> 00:05:10.400]   Still to be done now. We talked about for example
[00:05:10.400 --> 00:05:15.880]   Figuring out what interactions are the most important last week and
[00:05:15.880 --> 00:05:21.400]   Some of you pointed out in the forums that actually there is such a project already for gradient boosting machines
[00:05:21.400 --> 00:05:25.640]   Which is great, but it doesn't seem that there's anything like that yet for random forests
[00:05:25.640 --> 00:05:30.740]   And you know random forests do have a nice benefit over GBMs that they're kind of
[00:05:30.740 --> 00:05:34.480]   Harder to screw up you know and easier to scale
[00:05:37.000 --> 00:05:40.520]   So hopefully that's something that you know this community might help fix
[00:05:40.520 --> 00:05:47.240]   Another question I had during the week was about the size of your validation set
[00:05:47.240 --> 00:05:51.840]   How big should it be
[00:05:51.840 --> 00:06:00.000]   So like to answer this question about how big does your validation set need to be you first need to answer the question
[00:06:00.000 --> 00:06:03.520]   How
[00:06:03.800 --> 00:06:11.400]   How accurate do I need help how precisely do I need to know the accuracy of this algorithm right so like
[00:06:11.400 --> 00:06:18.520]   If the validation set that you have is saying like this is 70% accurate and
[00:06:18.520 --> 00:06:27.160]   If somebody said well is it 75% or 65% or 70% and the answer was I don't know anything in that range is close enough
[00:06:27.800 --> 00:06:35.120]   Like that would be one answer where else if it's like is it 70% or 70.01% or 69.99%
[00:06:35.120 --> 00:06:41.160]   Like then that's something else again, right? So you need to kind of start out by saying like how?
[00:06:41.160 --> 00:06:43.760]   How accurate do I need this?
[00:06:43.760 --> 00:06:49.080]   So like for example in the deep learning course we've been looking at dogs versus cats
[00:06:49.080 --> 00:06:51.680]   images and
[00:06:51.680 --> 00:06:55.320]   The models that we're looking at had about a ninety nine point
[00:06:56.480 --> 00:07:03.200]   Four ninety nine point five percent accuracy on the validation set okay, and our validation set size
[00:07:03.200 --> 00:07:06.680]   was
[00:07:06.680 --> 00:07:11.560]   2000 okay, in fact let's do this in Excel that'll be a bit easier
[00:07:11.560 --> 00:07:16.720]   So our validation set
[00:07:16.720 --> 00:07:19.600]   Size was
[00:07:19.600 --> 00:07:21.600]   2000 and
[00:07:21.600 --> 00:07:23.560]   accuracy was
[00:07:23.560 --> 00:07:26.280]   ninety nine point four percent
[00:07:26.840 --> 00:07:30.040]   Right so the number of incorrect is
[00:07:30.040 --> 00:07:34.040]   Something around
[00:07:34.040 --> 00:07:40.840]   One minus accuracy times n so we were getting about 12 wrong
[00:07:40.840 --> 00:07:44.040]   right and
[00:07:44.040 --> 00:07:47.320]   The number of cats we had is
[00:07:47.320 --> 00:07:51.240]   Half and
[00:07:51.240 --> 00:07:54.920]   So the number of wrong cats is about
[00:07:56.040 --> 00:07:58.040]   six
[00:07:58.040 --> 00:08:06.520]   Okay, so then like we we run a new model and we find instead that the accuracy
[00:08:06.520 --> 00:08:09.200]   has gone to
[00:08:09.200 --> 00:08:11.200]   ninety nine point two percent
[00:08:11.200 --> 00:08:18.600]   Right and then it's like okay. Is this less good at finding cats. That's like well. It got two more cats wrong
[00:08:18.600 --> 00:08:21.840]   So it's like probably not
[00:08:21.840 --> 00:08:24.000]   right so
[00:08:24.360 --> 00:08:29.640]   But then it's like well does this matter there's ninety nine point four versus ninety nine point two
[00:08:29.640 --> 00:08:35.320]   Matter and if this was like it wasn't about cats and dogs, but it was about finding fraud
[00:08:35.320 --> 00:08:41.120]   Right then the difference between a point six percent error rate and a point eight percent error rate
[00:08:41.120 --> 00:08:43.560]   It's like twenty five percent of your cost of fraud
[00:08:43.560 --> 00:08:46.240]   So like that can be huge
[00:08:46.240 --> 00:08:52.640]   Like it was really interesting like when image net came out earlier this year the new competition results came out and
[00:08:53.440 --> 00:08:56.320]   The accuracy had gone down from three percent
[00:08:56.320 --> 00:08:59.840]   So the error went down from three percent to two percent
[00:08:59.840 --> 00:09:04.920]   And I saw a lot of people on the internet like famous machine learning researchers being like man
[00:09:04.920 --> 00:09:10.080]   Some Chinese guys got it better from like ninety seven percent to one ninety eight percent
[00:09:10.080 --> 00:09:15.840]   It's like statistically not even significant who cares kind of a thing, but actually I thought like
[00:09:15.840 --> 00:09:22.880]   Holy crap this Chinese team just blew away the state-of-the-art and image recognition like the old one was
[00:09:22.880 --> 00:09:27.960]   Fifty percent less accurate than the new one like that's that's actually the right way to think about it
[00:09:27.960 --> 00:09:32.020]   Isn't it because it's like you know we were trying to recognize you know like
[00:09:32.020 --> 00:09:40.120]   Which tomatoes were ripe and which ones weren't and like our new approach. You know the old approach like
[00:09:40.120 --> 00:09:42.840]   50% of the time more
[00:09:42.840 --> 00:09:49.440]   Was like letting in the unripe tomatoes or you know 50% more of the time we were like
[00:09:49.920 --> 00:09:53.000]   Accepting fraudulent customers like that's a really big difference
[00:09:53.000 --> 00:10:01.280]   So just because like this particular validation set we can't really see six versus eight doesn't mean the point two percent different
[00:10:01.280 --> 00:10:08.920]   Isn't important it could be so my kind of rule of thumb is that this like this number of like how many?
[00:10:08.920 --> 00:10:14.980]   Observations you actually looking at I want that generally to be somewhere higher than 22
[00:10:15.520 --> 00:10:22.120]   Why 22 because 22 is the magic number where the t distribution roughly turns into the normal distribution?
[00:10:22.120 --> 00:10:28.720]   All right, so as you may have learned the t distribution is is the normal distribution for small
[00:10:28.720 --> 00:10:36.420]   Datasets right and so in other words once we have 22 of something or more it kind of starts to behave
[00:10:36.420 --> 00:10:39.040]   kind of
[00:10:39.040 --> 00:10:44.200]   Normally in both sense of the words like it's kind of more stable, and you can kind of understand it better, so
[00:10:45.240 --> 00:10:51.000]   That's my magic number when somebody says do I have enough of something I kind of start out by saying like do you have?
[00:10:51.000 --> 00:10:53.280]   22 observations of the thing of interest
[00:10:53.280 --> 00:10:56.200]   so if you were looking at like
[00:10:56.200 --> 00:11:05.240]   Line cancer you know and you had a data set that had like a thousand people without lung cancer and 20 people with lung cancer
[00:11:05.240 --> 00:11:11.300]   I'd be like I very much doubt we're going to make much progress. You know because we haven't even got 20 of the thing you want
[00:11:11.960 --> 00:11:17.020]   So ditto with a validation set if you don't have 20 of the thing you want that is very unlikely to be useful
[00:11:17.020 --> 00:11:21.680]   Or if like the at the level of accuracy we need it's not plus or minus 20
[00:11:21.680 --> 00:11:24.360]   It's just it's that that's the point where I'm thinking like
[00:11:24.360 --> 00:11:26.560]   Be a bit careful
[00:11:26.560 --> 00:11:31.560]   so just to be clear you want 22 to be the number of
[00:11:31.560 --> 00:11:40.560]   Samples in each set like in the validation the test and the train or so what I'm saying is like if there's if there's less
[00:11:40.560 --> 00:11:46.800]   than 22 of a class in any of the sets then it's
[00:11:46.800 --> 00:11:50.640]   It's going to get it's getting pretty unstable at that point, right?
[00:11:50.640 --> 00:11:54.320]   And so like that's just like the first rule of thumb
[00:11:54.320 --> 00:12:00.620]   but then what I would actually do is like start practicing what we learned about the
[00:12:00.620 --> 00:12:05.840]   binomial distribution or actually then we distribution so
[00:12:05.840 --> 00:12:09.280]   What's the?
[00:12:10.280 --> 00:12:17.680]   What is the mean of the binomial distribution of n samples and probability P?
[00:12:17.680 --> 00:12:23.660]   And times P. Okay. Thank you n times P is our mean
[00:12:23.660 --> 00:12:31.640]   Right, so if you've got a 50% chance of getting ahead and you toss it a hundred times on average you get 50 heads
[00:12:31.640 --> 00:12:34.860]   Okay, and then what's the standard deviation?
[00:12:37.560 --> 00:12:39.560]   and P1 minus P
[00:12:39.560 --> 00:12:45.240]   Okay, so these are like
[00:12:45.240 --> 00:12:52.280]   Two numbers well the first number you don't really have to remember. It's intuitively obvious the second one is one that try to remember
[00:12:52.280 --> 00:12:54.160]   forevermore
[00:12:54.160 --> 00:12:59.000]   Because not only does it come up all the time the people that you work with will all have forgotten it
[00:12:59.000 --> 00:13:03.840]   So you'll be like the one person in the conversation who could immediately go we don't have to run this a hundred times
[00:13:03.840 --> 00:13:09.080]   I can tell you straight away. It's binomial. It's going to be n PQ and P1 minus P
[00:13:09.080 --> 00:13:13.120]   Then there's the standard error
[00:13:13.120 --> 00:13:20.560]   The standard error is if you run a bunch of trials each time getting a mean
[00:13:20.560 --> 00:13:24.280]   What is the standard deviation of the mean? I?
[00:13:24.280 --> 00:13:27.880]   Don't think you guys have covered this yet. Is that right?
[00:13:28.920 --> 00:13:35.620]   No, so this is really important because this means like if you train a hundred models
[00:13:35.620 --> 00:13:41.660]   right each time the validation set accuracy is like the meaning of a distribution and
[00:13:41.660 --> 00:13:48.920]   So therefore the standard deviation of that validation set accuracy it can be calculated with the standard error
[00:13:48.920 --> 00:13:52.000]   And this is equal to the standard deviation
[00:13:52.000 --> 00:13:54.360]   divided by
[00:13:54.360 --> 00:13:56.360]   square root n
[00:13:57.640 --> 00:14:00.040]   Right so this tells you
[00:14:00.040 --> 00:14:06.280]   So like one approach to figuring out like is my validation set big enough is train your model five times
[00:14:06.280 --> 00:14:14.600]   with exactly the same hyper parameters each time and look at the validation set accuracy each time and you know there's like a
[00:14:14.600 --> 00:14:19.600]   Mean and a standard deviation of five numbers you could use or a maximum and minimum you can choose
[00:14:19.600 --> 00:14:24.600]   But save yourself some time you can figure out straight away that like
[00:14:24.600 --> 00:14:27.400]   okay, well
[00:14:27.440 --> 00:14:30.280]   I I have a point nine nine
[00:14:30.280 --> 00:14:36.000]   Accuracy as to you know whether I get the cat correct or not correct
[00:14:36.000 --> 00:14:42.240]   So therefore the standard deviation is equal to 0.99 times 0.01
[00:14:42.240 --> 00:14:45.640]   Okay, and then I can get the
[00:14:45.640 --> 00:14:49.720]   Standard error of that
[00:14:49.720 --> 00:14:54.160]   Right so so basically the size of the validation set you need
[00:14:54.880 --> 00:14:57.800]   It's like however big it has to be
[00:14:57.800 --> 00:15:03.840]   Such that your insights about its accuracy are good enough for your particular
[00:15:03.840 --> 00:15:11.160]   Business problem and so like I say like the simple way to do it is to pick a validation set of like a size a thousand
[00:15:11.160 --> 00:15:13.480]   train five models and
[00:15:13.480 --> 00:15:20.160]   See how much the validation set accuracy varies and if it's like if they're if it's they're all close enough for what you need
[00:15:20.160 --> 00:15:23.840]   Then you're fine. If it's not maybe you should make it bigger
[00:15:24.480 --> 00:15:26.480]   Or maybe you should consider
[00:15:26.480 --> 00:15:29.280]   using cross validation
[00:15:29.280 --> 00:15:31.040]   instead
[00:15:31.040 --> 00:15:32.680]   Okay
[00:15:32.680 --> 00:15:37.320]   So like as you can see it really depends on what it is you're trying to do
[00:15:37.320 --> 00:15:44.640]   How common your less common class is and how accurate your model is could you pass that back to Melissa, please?
[00:15:44.640 --> 00:15:53.120]   Thank you, I have a question about the less common classes if you have less than 22
[00:15:53.120 --> 00:15:55.320]   Let's say you have one sample of something
[00:15:55.320 --> 00:16:01.780]   Let's say it's a face and I only have one representation from that particular country
[00:16:01.780 --> 00:16:09.560]   Do I toss that into the training set and it adds variety do I pull it out completely out of the data set or?
[00:16:09.560 --> 00:16:16.000]   Do I put it in a test set instead of the validation set?
[00:16:16.000 --> 00:16:20.240]   So you certainly couldn't put it in the test or the validation set because you're asking
[00:16:20.240 --> 00:16:24.280]   Can I mean in general because you're asking can I recognize something? I've never seen before
[00:16:24.280 --> 00:16:31.940]   But actually this this question of like can I recognize something I've not seen before there's actually a whole class of models specifically
[00:16:31.940 --> 00:16:35.080]   For that purpose it's called other one-shot learning
[00:16:35.080 --> 00:16:39.700]   Which is you get to see something once and then you have to recognize it again or zero shot learning
[00:16:39.700 --> 00:16:44.020]   Which is where you have to recognize something you've never seen before we're not going to cover them in this course
[00:16:47.040 --> 00:16:50.840]   But that can be useful for things like face recognition
[00:16:50.840 --> 00:16:55.720]   You know like is this the same person I've seen before and so generally speaking
[00:16:55.720 --> 00:17:00.800]   Obviously for something like that to work. It's not that you've never seen our face before
[00:17:00.800 --> 00:17:07.400]   It's that you've never seen Melissa's face before you know and so you see Melissa's face once and you have to recognize it again
[00:17:07.400 --> 00:17:14.960]   Yeah, so in general you know your validation set and test set need to have the same
[00:17:16.000 --> 00:17:18.160]   mix or frequency
[00:17:18.160 --> 00:17:22.740]   Observations that you're going to see in production in the real world
[00:17:22.740 --> 00:17:25.600]   and then your training set
[00:17:25.600 --> 00:17:31.960]   Should have an equal number in each class and if you don't
[00:17:31.960 --> 00:17:34.960]   Just replicate the less common one
[00:17:34.960 --> 00:17:41.280]   Until it is equal so this is I think we've mentioned this paper before very recent paper that came out
[00:17:41.280 --> 00:17:47.000]   They tried lots of different approaches to training with unbalanced data sets and found consistently that
[00:17:47.000 --> 00:17:54.480]   Oversampling the less common class until it is the same size as the more common class is always the right thing to do
[00:17:54.480 --> 00:18:03.280]   So you could literally copy you know so like I've only got a thousand you know ten examples of people with cancer and a hundred
[00:18:03.280 --> 00:18:05.280]   Without so I could just copy those ten
[00:18:05.280 --> 00:18:07.800]   And other you know 90 times
[00:18:09.080 --> 00:18:14.120]   That's kind of a little memory inefficient so a lot of things including
[00:18:14.120 --> 00:18:20.560]   I think SK learns random forests have a class weights parameter that says each time you're bootstrapping or resampling
[00:18:20.560 --> 00:18:24.880]   I want you to sample the less common class with a higher probability
[00:18:24.880 --> 00:18:29.320]   Or ditto if you're doing deep learning you know make sure in your mini batch
[00:18:29.320 --> 00:18:35.640]   It's not randomly sampled, but it's a stratified sample so the less common class is picked more often
[00:18:35.640 --> 00:18:38.640]   Okay
[00:18:38.640 --> 00:18:41.800]   Okay, so let's get back to finishing off
[00:18:41.800 --> 00:18:48.280]   Our random forests and so what we're going to do today is we're going to finish off writing our random forest
[00:18:48.280 --> 00:18:53.120]   and then after day your your after today your homework will be to take this class and
[00:18:53.120 --> 00:19:00.640]   To add to it all of the random forest interpretation algorithms that we've learned, okay, so
[00:19:00.640 --> 00:19:05.720]   Obviously to be able to do that you're going to need to totally understand how this class works
[00:19:05.880 --> 00:19:09.800]   So please you know ask lots of questions as necessary as we go along
[00:19:09.800 --> 00:19:13.520]   So just to remind you
[00:19:13.520 --> 00:19:20.680]   We we're doing the the bulldozers Kaggle competition data set again
[00:19:20.680 --> 00:19:25.240]   We split it as before into 12,000 validation the last 12,000
[00:19:25.240 --> 00:19:31.360]   Records and then just to make it easier for us to keep track of what we're doing
[00:19:31.360 --> 00:19:36.120]   We're going to just pick two columns out to start with year made and machine hours on the meter
[00:19:36.120 --> 00:19:42.440]   okay, and so what we did last time was we started out by creating a tree ensemble and
[00:19:42.440 --> 00:19:48.120]   the tree ensemble had a bunch of trees which was literally a list of
[00:19:48.120 --> 00:19:53.800]   Entries trees where each time we just called create tree
[00:19:53.800 --> 00:19:58.320]   Okay, and create tree contained a
[00:19:59.560 --> 00:20:03.000]   sample size number of random indexes
[00:20:03.000 --> 00:20:07.080]   Okay, this one was drawn without replacement
[00:20:07.080 --> 00:20:12.600]   So remember bootstrapping means sampling with replacement
[00:20:12.600 --> 00:20:16.240]   So normally with scikit learn if you've got n rows
[00:20:16.240 --> 00:20:23.400]   We grab n rows with replacement, which means many of them will appear more than once
[00:20:23.760 --> 00:20:31.120]   So each time we get a different sample, but it's always the same size as the original data set and then we have our
[00:20:31.120 --> 00:20:33.480]   set RF samples
[00:20:33.480 --> 00:20:40.440]   Function that we can use which does with replacement sampling of less than n rows
[00:20:40.440 --> 00:20:45.360]   This is doing something again, which is it's sampling without replacement
[00:20:45.360 --> 00:20:48.840]   Sample size rows okay because we're permuting
[00:20:49.440 --> 00:20:55.640]   The numbers from naught to self dot y minus 1 and then grabbing the first self dot sample size of them
[00:20:55.640 --> 00:21:01.960]   Actually, there's a faster way to do this. You can just use NP dot random dot choice, which is a slightly more direct way
[00:21:01.960 --> 00:21:04.400]   But this way works as well
[00:21:04.400 --> 00:21:09.200]   All right. So this is our random sample for this one of our
[00:21:09.200 --> 00:21:12.240]   entries trees
[00:21:12.240 --> 00:21:16.600]   And so then we're going to create a decision tree and our decision tree
[00:21:17.000 --> 00:21:24.220]   We don't pass it all of x we pass it these specific indexes and remember x is a panda's data frame
[00:21:24.220 --> 00:21:31.280]   So if we want to index into it with a bunch of integers, we have to use I log integer locations
[00:21:31.280 --> 00:21:35.540]   And that makes it behave indexing wise just like numpy
[00:21:35.540 --> 00:21:45.140]   Why vector is numpy so we can just index into it directly and then we're going to keep track of our minimum precise
[00:21:45.260 --> 00:21:48.260]   and
[00:21:48.260 --> 00:21:51.820]   So then the only other thing we really need an ensemble is some way to make a prediction
[00:21:51.820 --> 00:21:56.200]   And so we were just going to do the mean of the tree prediction
[00:21:56.200 --> 00:21:59.300]   For each tree
[00:21:59.300 --> 00:22:03.520]   All right, so that was that and so then in order to be able to run that
[00:22:03.520 --> 00:22:08.240]   We need a decision tree class because it's being called here
[00:22:08.240 --> 00:22:10.860]   And so there we go
[00:22:12.020 --> 00:22:15.740]   Okay, so that's the starting point. So
[00:22:15.740 --> 00:22:22.140]   The next thing we need to do is to flesh out our decision tree
[00:22:22.140 --> 00:22:25.960]   So the important thing to remember is all of our randomness
[00:22:25.960 --> 00:22:28.740]   Happened back here in the tree ensemble
[00:22:28.740 --> 00:22:31.900]   The decision tree class we're going to create
[00:22:31.900 --> 00:22:34.660]   Doesn't have randomness in it
[00:22:36.580 --> 00:22:43.540]   Okay, so right now we are building a randomly regressor, right so that's why we're taking the mean of the tree
[00:22:43.540 --> 00:22:47.800]   The output if we were to work with classification
[00:22:47.800 --> 00:22:52.820]   Do we take the max like the classifier will give you either zeros or ones?
[00:22:52.820 --> 00:23:01.380]   No, I would still take the mean so the so each tree is going to tell you what percentage of that leaf node
[00:23:01.380 --> 00:23:05.400]   Contains cats and what percentage take contains dogs
[00:23:05.820 --> 00:23:12.740]   so then I would average all those percentages and say across the trees on average there is 19% cats and
[00:23:12.740 --> 00:23:16.260]   81% dogs
[00:23:16.260 --> 00:23:18.540]   Good question, so you know
[00:23:18.540 --> 00:23:27.240]   Random tree classifiers are almost identical or can be almost identical to random tree regressors
[00:23:27.240 --> 00:23:34.340]   The technique we're going to use to build this today will basically exactly work for a classification
[00:23:34.340 --> 00:23:41.900]   It's certainly for binary classification. You can do with exactly the same code for multi-class classification. You just need to change your data structure
[00:23:41.900 --> 00:23:47.140]   so that like you have like a one hot encoded matrix or a
[00:23:47.140 --> 00:23:51.620]   List of integers that you treat as a one hot encoded matrix
[00:23:51.620 --> 00:23:57.160]   Okay, so our decision tree
[00:23:57.160 --> 00:24:03.020]   So remember our idea here is that we're going to like try to avoid thinking
[00:24:03.260 --> 00:24:10.100]   So we're going to basically write it as if everything we need already exists, okay, so we know
[00:24:10.100 --> 00:24:16.480]   From when we created the decision tree, we're going to pass in the X the Y and the minimum leaf size
[00:24:16.480 --> 00:24:23.980]   So here we need to make sure we've got the X and the Y and the minimum leaf size. Okay, so then there's one other thing
[00:24:23.980 --> 00:24:25.980]   which is as we
[00:24:25.980 --> 00:24:30.700]   split our tree into sub trees, we're going to need to keep track of
[00:24:31.980 --> 00:24:37.740]   Which of the row indexes went into the left-hand side of the tree which went into the right-hand side of the tree
[00:24:37.740 --> 00:24:39.860]   Okay, so we're going to have this thing called
[00:24:39.860 --> 00:24:42.020]   indexes as
[00:24:42.020 --> 00:24:46.660]   Well, right so at first we just didn't bother passing in indexes at all
[00:24:46.660 --> 00:24:50.980]   So if indexes is not passed in if it's none, then we're just going to set it to
[00:24:50.980 --> 00:24:59.040]   Everything the entire length of y right so NP dot a range is the same as just range in Python
[00:24:59.040 --> 00:25:04.160]   But it returns a numpy array right so that the root of a decision tree
[00:25:04.160 --> 00:25:09.960]   Contains all the rows. That's the definition really of the root of a decision tree
[00:25:09.960 --> 00:25:15.460]   So all the rows is row 0 row 1 row 2 etc up to row y minus 1
[00:25:15.460 --> 00:25:21.340]   Okay, and then we're just going to store away all that information that we were given
[00:25:21.340 --> 00:25:23.980]   We're going to keep track of how many?
[00:25:23.980 --> 00:25:27.820]   Rows are there and how many columns are there? Okay?
[00:25:28.620 --> 00:25:30.740]   so then the
[00:25:30.740 --> 00:25:31.860]   every
[00:25:31.860 --> 00:25:40.140]   leaf and every node in a tree has a value it has a prediction that prediction is just equal to the average of
[00:25:40.140 --> 00:25:42.980]   the dependent variable
[00:25:42.980 --> 00:25:45.260]   Okay, so
[00:25:45.260 --> 00:25:47.260]   every node in the tree
[00:25:47.260 --> 00:25:50.780]   Why indexed with the indexes is?
[00:25:50.780 --> 00:25:57.380]   the values of the dependent variable that are in this branch of the tree and so here is the mean
[00:25:58.340 --> 00:26:00.340]   okay
[00:26:00.340 --> 00:26:01.780]   some
[00:26:01.780 --> 00:26:07.920]   Nodes in a tree also have a score which is like how effective was the split?
[00:26:07.920 --> 00:26:15.500]   Here right, but that's only going to be true if it's not a leaf node right a leaf node has no further splits
[00:26:15.500 --> 00:26:22.420]   And at this point when we create a tree we haven't done any splits yet, so its score starts out as being infinity
[00:26:24.060 --> 00:26:32.060]   so having built that the root of the tree our next job is to find out which variable should we split on and
[00:26:32.060 --> 00:26:38.060]   What level of that variable should we split on so let's pretend that there's something that does that
[00:26:38.060 --> 00:26:40.700]   Find bar split
[00:26:40.700 --> 00:26:47.660]   So then we're done, okay, so how do we find a
[00:26:49.420 --> 00:26:54.460]   variable to split on so well we could just go through each
[00:26:54.460 --> 00:26:59.380]   Potential variable so C contains the number of columns
[00:26:59.380 --> 00:27:06.380]   We have so go through each one and see if we can find a better split than we have so far on that column
[00:27:06.380 --> 00:27:09.500]   Okay
[00:27:09.500 --> 00:27:11.620]   Now notice this is like
[00:27:11.620 --> 00:27:13.700]   not
[00:27:13.700 --> 00:27:20.500]   The full random forest definition this is assuming that max features is set to all right remember
[00:27:20.500 --> 00:27:25.700]   We could set max features to like zero point five in which case we wouldn't check all the numbers should not to see
[00:27:25.700 --> 00:27:31.420]   We would check half the numbers at random from not to see so if you want to turn this into like a
[00:27:31.420 --> 00:27:34.220]   random forest that has the max features
[00:27:34.220 --> 00:27:42.060]   Support you could easily like add one line of code to do that, but we're not going to do it in our implementation today
[00:27:42.060 --> 00:27:44.060]   so
[00:27:44.060 --> 00:27:50.140]   Then we just need to find better split and since we're not interested in thinking at the moment for now
[00:27:50.140 --> 00:27:52.140]   We're just going to leave that empty all right
[00:27:52.140 --> 00:27:55.500]   so
[00:27:55.500 --> 00:27:57.700]   The one other thing I like to do
[00:27:57.700 --> 00:28:02.860]   With my kind of where I start writing a class is I like to have some way to print out
[00:28:02.860 --> 00:28:09.500]   What's in that class right and so if you type print followed by an object or if at Jupiter notebook?
[00:28:09.500 --> 00:28:11.500]   You just type the name of the object
[00:28:12.460 --> 00:28:14.900]   At the moment. It's just printing out
[00:28:14.900 --> 00:28:20.540]   Underscore underscore main underscore underscore dot decision tree at blah blah blah which is not very helpful
[00:28:20.540 --> 00:28:23.660]   Right so if we want to replace this with something helpful
[00:28:23.660 --> 00:28:26.300]   We have to define the special
[00:28:26.300 --> 00:28:30.140]   Python method name dunder Repra
[00:28:30.140 --> 00:28:36.780]   To get a representation of this object so when we type when we sickly just
[00:28:37.500 --> 00:28:42.020]   Write the name like this behind the scenes that calls that function and the default
[00:28:42.020 --> 00:28:46.100]   Implementation of that method is just to print out this
[00:28:46.100 --> 00:28:52.060]   Unhelpful stuff so we can replace it by instead saying let's create a format string
[00:28:52.060 --> 00:28:59.300]   Where we're going to print out n and then show n and then print Val and then show Val okay, so how many?
[00:28:59.300 --> 00:29:06.180]   How many rows are in this node and what's the average of the dependent variable okay?
[00:29:07.180 --> 00:29:08.580]   then
[00:29:08.580 --> 00:29:14.060]   If it's not a leaf node, so if it has a split then we should also be able to print out the score
[00:29:14.060 --> 00:29:18.780]   The value we split out and the variable that we split on
[00:29:18.780 --> 00:29:22.380]   Now you'll notice here
[00:29:22.380 --> 00:29:28.240]   Self dot is leaf is leaf is defined as a method, but I don't have any parentheses after it
[00:29:28.240 --> 00:29:35.340]   This is a special kind of method called a property and so a property is something that kind of looks like
[00:29:35.940 --> 00:29:37.940]   a regular variable
[00:29:37.940 --> 00:29:45.620]   But it's actually calculated on the fly so when I call its leaf it actually calls this function
[00:29:45.620 --> 00:29:48.060]   Right, but I've got this special decorator
[00:29:48.060 --> 00:29:54.800]   Property okay, and what this says is basically you don't have to include the parentheses when you call it
[00:29:54.800 --> 00:30:03.380]   Okay, and so it's going to say all right is this a leaf or not so a leaf is something that we don't split on
[00:30:03.940 --> 00:30:09.060]   If we haven't split on it then its score is still set to infinity, but that's my logic
[00:30:09.060 --> 00:30:12.220]   That makes sense
[00:30:12.220 --> 00:30:18.340]   So this this at notation is called a decorator
[00:30:18.340 --> 00:30:23.280]   It's basically a way of telling Python more information about your method
[00:30:23.280 --> 00:30:28.140]   So anybody here remember where you have seen decorators before?
[00:30:30.460 --> 00:30:36.980]   You pass it over here, yeah, where have you seen where have you seen decorators before tell us more about flask and
[00:30:36.980 --> 00:30:41.860]   Yeah, what does that do?
[00:30:41.860 --> 00:30:53.940]   So flask so anybody who's done any web programming before with something like flask or a similar framework
[00:30:53.940 --> 00:31:00.120]   Would have had to have said like this method is going to respond to this bit of the URL
[00:31:00.200 --> 00:31:05.000]   And either the post or to get and he put it in a special decorator
[00:31:05.000 --> 00:31:07.640]   so
[00:31:07.640 --> 00:31:09.640]   Behind the scenes that's telling
[00:31:09.640 --> 00:31:15.000]   Python to treat this method in a special way, so here's another decorator, okay?
[00:31:15.000 --> 00:31:20.880]   And so you know if you get more advanced with Python you can actually learn how to write your own decorators
[00:31:20.880 --> 00:31:24.320]   Which as was mentioned you know basically insert some additional code
[00:31:24.320 --> 00:31:29.440]   but for now just know there's a bunch of predefined decorators we can use to
[00:31:30.320 --> 00:31:36.600]   Change how our methods behave and one of them is at property which basically means you don't have to put parentheses anymore
[00:31:36.600 --> 00:31:40.160]   Which of course means you can't add any more parameters beyond self
[00:31:40.160 --> 00:31:43.040]   Yeah
[00:31:43.040 --> 00:31:50.600]   Why if it's not a leaf why is this for infinity because doesn't infinity mean you're at the root
[00:31:50.600 --> 00:31:57.900]   Why no infinity means that you're not at the root. It means that you're at a leaf so the root will have a split
[00:31:58.520 --> 00:32:02.560]   Assuming we find one right everything will have a split till we get all the way to the bottom
[00:32:02.560 --> 00:32:07.880]   The leaf and so the leaves will have a score of infinity because they won't split
[00:32:07.880 --> 00:32:11.640]   Great all right
[00:32:11.640 --> 00:32:14.520]   So that's our
[00:32:14.520 --> 00:32:22.200]   Decision tree it doesn't do very much, but at least we can like create an ensemble right ten trees sample size a thousand
[00:32:22.200 --> 00:32:26.000]   Right and we can like print out so now when I go m trees zero
[00:32:26.200 --> 00:32:29.320]   It doesn't say blah blah blah blah blah. It says
[00:32:29.320 --> 00:32:32.520]   What we asked it to say n called the thousand
[00:32:32.520 --> 00:32:40.200]   Val Colin 10.8. Oh wait, okay, and this is the leaf because we haven't spit on it yet
[00:32:40.200 --> 00:32:42.320]   So we've got nothing more to say
[00:32:42.320 --> 00:32:48.780]   Okay, so then the indexes are all the numbers from 0 to 1,000
[00:32:48.780 --> 00:32:54.520]   Okay, because the base of the tree has everything this is like everything in
[00:32:55.080 --> 00:32:59.840]   The random sample that was passed to it because remember by the time we get to the point where it's a decision tree
[00:32:59.840 --> 00:33:05.280]   Where we don't have to worry about any of the randomness in the random forest anymore, okay?
[00:33:05.280 --> 00:33:09.200]   All right, so
[00:33:09.200 --> 00:33:12.080]   Let's try to write
[00:33:12.080 --> 00:33:16.680]   The thing which finds a split okay, so we need to implement
[00:33:18.520 --> 00:33:26.260]   Find better split okay, and so it's going to take the index of a variable variable number one variable number three
[00:33:26.260 --> 00:33:28.260]   Whatever and it's going to figure out
[00:33:28.260 --> 00:33:31.440]   What's the best split point?
[00:33:31.440 --> 00:33:34.040]   Is that better than any split we have so far and?
[00:33:34.040 --> 00:33:40.840]   For the first variable the answer will always be yes because the best one so far is none at all infinity bad, okay?
[00:33:40.840 --> 00:33:47.240]   So let's start by making sure we've got something to compare to so the thing we're going to compare to
[00:33:47.760 --> 00:33:48.960]   will be
[00:33:48.960 --> 00:33:50.960]   scikit learns random forest and
[00:33:50.960 --> 00:33:58.140]   So we need to make sure that scikit learns random forest gets exactly the same data that we have so we start out by creating ensemble
[00:33:58.140 --> 00:34:06.180]   Grab a tree out of it and then find out which particular random sample of x and y did this tree use?
[00:34:06.180 --> 00:34:12.920]   Okay, and we're going to store them away so that we can pass them to scikit learn so we have exactly the same information
[00:34:14.440 --> 00:34:19.040]   So let's go ahead and now create a random forest using scikit learn so one tree
[00:34:19.040 --> 00:34:21.600]   one decision
[00:34:21.600 --> 00:34:26.240]   No, bootstrapping so the whole the whole data set that so this should
[00:34:26.240 --> 00:34:30.640]   Be exactly the same as the thing that we're going to create this tree
[00:34:30.640 --> 00:34:34.680]   So let's try
[00:34:34.680 --> 00:34:38.960]   So we need to define find better split
[00:34:38.960 --> 00:34:43.420]   Okay, so find better split takes a variable
[00:34:44.060 --> 00:34:49.700]   Okay, so let's define our x independent variables and say okay
[00:34:49.700 --> 00:34:53.260]   Well, it's everything inside our tree, but only
[00:34:53.260 --> 00:35:02.760]   Those indexes that are in this node right which at the top of the tree is everything right and just this one variable
[00:35:02.760 --> 00:35:10.140]   Okay, and then for our wise it's just whatever our dependent variable is at the indexes in this node
[00:35:10.140 --> 00:35:12.300]   Okay, so there's our x and y
[00:35:12.660 --> 00:35:13.940]   so
[00:35:13.940 --> 00:35:20.580]   Let's now go through every single value in our independent variable and
[00:35:20.580 --> 00:35:26.420]   So I'll show you what's going to happen. So let's say our independent variable is year made
[00:35:26.420 --> 00:35:34.700]   And not going to be an order
[00:35:41.460 --> 00:35:48.020]   Right and so we're going to go to the very first row and we're going to say okay year made here is three
[00:35:48.020 --> 00:35:51.780]   Right and so what I'm going to do is I'm going to try and calculate
[00:35:51.780 --> 00:35:56.980]   The score if we decided to branch on the number three
[00:35:56.980 --> 00:36:02.180]   Right so I need to know which rows are greater than three
[00:36:02.180 --> 00:36:07.340]   Which rows are less than and equal to three and they're going to become my left-hand side my right-hand side
[00:36:07.340 --> 00:36:09.460]   Right and then we need a score
[00:36:09.700 --> 00:36:11.420]   right, so
[00:36:11.420 --> 00:36:17.380]   There's lots of scores we could use so in random forests. We call this the information gain, right?
[00:36:17.380 --> 00:36:22.260]   The information gain is like how much better does our score get because we split it into these two groups of data
[00:36:22.260 --> 00:36:28.420]   There's lots of ways we could calculate it Gini cross entropy root mean squared error, whatever
[00:36:28.420 --> 00:36:34.260]   If you think about it, there is an alternative formulation of root mean squared error
[00:36:34.540 --> 00:36:41.060]   Which is mathematically the same to within a constant scale, but it's a little bit easier to deal with which is
[00:36:41.060 --> 00:36:43.380]   We're going to try and find a split
[00:36:43.380 --> 00:36:49.820]   Which the causes the two groups to each have as lower standard deviation as possible
[00:36:49.820 --> 00:36:54.980]   All right, so like I want to find a split that puts all the cats over here and all the dogs over here
[00:36:54.980 --> 00:36:58.060]   Right, so if these are all cats and these are all dogs
[00:36:58.060 --> 00:37:02.220]   Then this has a standard deviation of zero and this has a standard deviation of zero
[00:37:02.660 --> 00:37:07.500]   Or else this is like a totally random mix of cats and dogs. This is a totally random mix of cats and dogs
[00:37:07.500 --> 00:37:09.820]   They're going to have a much higher standard deviation
[00:37:09.820 --> 00:37:18.020]   That makes sense. And so it turns out if you find a split that minimizes those group standard deviations or specifically the
[00:37:18.020 --> 00:37:24.260]   Weighted average of the two standard deviations. It's mathematically the same as minimizing the root mean squared error
[00:37:24.260 --> 00:37:27.820]   That's something you can prove to yourself after class if you want to
[00:37:27.820 --> 00:37:30.900]   All right, so we're going to need to find
[00:37:31.940 --> 00:37:36.700]   First of all split this into two groups. So where's all the stuff that is greater than three?
[00:37:36.700 --> 00:37:41.700]   So greater than three is this one this one and this one. So we need the standard deviation of that
[00:37:41.700 --> 00:37:45.820]   so let's go ahead and say standard deviation of
[00:37:45.820 --> 00:37:49.380]   Greater than three that one
[00:37:49.380 --> 00:37:58.500]   That one and that one. Okay, and then the next will be the standard deviation of
[00:37:59.940 --> 00:38:02.780]   Less than or equal to three. So that would be that one
[00:38:02.780 --> 00:38:10.860]   That one that one and then we just take the weighted average of those two and that's our score
[00:38:10.860 --> 00:38:15.140]   That would be our score if we split on three
[00:38:15.140 --> 00:38:19.740]   That makes sense. And so then the next step would be try to split on four
[00:38:19.740 --> 00:38:22.740]   Try splitting on one try splitting on six
[00:38:22.740 --> 00:38:26.780]   Redundantly try splitting on four again
[00:38:27.900 --> 00:38:30.580]   Redundantly try splitting on one again and find out which one works best
[00:38:30.580 --> 00:38:37.700]   So that's our code here is we're going to go through every row. And so let's say okay left-hand side is
[00:38:37.700 --> 00:38:44.420]   Any values in X that are less than or equal to this particular value
[00:38:44.420 --> 00:38:50.860]   Our right hand side is every value in X that are greater than this particular value
[00:38:50.860 --> 00:39:05.180]   Okay, so what's the data type that's going to be in LHS and RHS? What are they actually going to contain?
[00:39:05.180 --> 00:39:11.540]   They're going to be arrays arrays of what?
[00:39:11.540 --> 00:39:19.740]   Raise of a raise of billions. Yeah, which we can treat a zero and one. Okay, so LHS will be an array of
[00:39:20.740 --> 00:39:23.980]   False every time it's not less than or equal to and true
[00:39:23.980 --> 00:39:32.180]   Otherwise and RHS will be a boolean array of the opposite. Okay, and now we can't take a standard deviation of an empty set
[00:39:32.180 --> 00:39:35.140]   Right. So if there's nothing that's greater than
[00:39:35.140 --> 00:39:41.700]   This number then these will all be false which means the sum will be zero
[00:39:41.700 --> 00:39:43.740]   Okay, and in that case
[00:39:43.740 --> 00:39:49.460]   Let's not go any further with this step because there's nothing to take the standard deviation of and it's obviously not a useful
[00:39:49.460 --> 00:39:51.100]   Split, okay
[00:39:51.100 --> 00:39:55.980]   so assuming we've got this far we can now calculate the standard deviation of the left hand side and
[00:39:55.980 --> 00:39:58.500]   of the right hand side and
[00:39:58.500 --> 00:40:04.220]   Take the weighted average or the sums the same thing to us to a scalar
[00:40:04.220 --> 00:40:11.260]   Right and so there's our score and so we can then check is this better than our best score so far and our best score
[00:40:11.260 --> 00:40:16.780]   So far we initially initialized it to infinity, right? So initially this is this is better
[00:40:17.780 --> 00:40:20.560]   So if it's better, let's store away
[00:40:20.560 --> 00:40:29.260]   All of the information we need which variable has found this better split what was the score we found and
[00:40:29.260 --> 00:40:32.860]   What was the value that we split on?
[00:40:32.860 --> 00:40:36.140]   Okay, so there it is
[00:40:36.140 --> 00:40:40.100]   So if we run that
[00:40:40.100 --> 00:40:47.580]   And I'm using time it so what time it does is it sees how long this command takes to run and it tries to give you
[00:40:47.580 --> 00:40:52.900]   A kind of statistically valid measure of that so you can see here. It's run run at ten times
[00:40:52.900 --> 00:40:59.660]   To get an average and then it's done that seven times to get a mean and standard deviation across runs
[00:40:59.660 --> 00:41:02.900]   And so it's taking me 75 milliseconds plus or minus ten
[00:41:02.900 --> 00:41:05.420]   Okay
[00:41:05.420 --> 00:41:07.420]   So let's check that this works
[00:41:07.420 --> 00:41:14.500]   Find better split tree zero. So zero is year made one is machine hours current meter
[00:41:17.140 --> 00:41:19.140]   So with one
[00:41:19.140 --> 00:41:25.680]   we got back machine hours current meter 37 4 4 with this score and then we ran it again with
[00:41:25.680 --> 00:41:33.600]   0 that's year made and we've got a better score 658 and split 1974 and so in 1974
[00:41:33.600 --> 00:41:36.020]   Let's compare
[00:41:36.020 --> 00:41:40.800]   Yeah, that was what this tree did as well. Okay, so we've got we've confirmed that this
[00:41:40.800 --> 00:41:46.540]   Method is doing is giving the same result that SK learns random forest did
[00:41:47.140 --> 00:41:49.540]   And you can also see here the value
[00:41:49.540 --> 00:41:55.060]   Ten point oh eight and again matching here the value ten point oh eight
[00:41:55.060 --> 00:42:01.180]   Okay, so we've got something that can find one split. Could you pass that to your net please?
[00:42:01.180 --> 00:42:07.340]   So Jeremy, why don't we put a unique on the X there?
[00:42:07.340 --> 00:42:15.180]   Because I'm not trying to optimize the performance yet, but you see that no like he's doing more
[00:42:15.540 --> 00:42:21.300]   Yeah, so it's like and you can see in the Excel. I like checked this one twice. I check this for twice unnecessarily
[00:42:21.300 --> 00:42:24.140]   Yeah
[00:42:24.140 --> 00:42:31.180]   Okay, so and so again, it's already thinking about performance, which is good
[00:42:31.180 --> 00:42:36.900]   So tell me what is the computational complexity of
[00:42:36.900 --> 00:42:39.860]   this
[00:42:39.860 --> 00:42:46.180]   section of the code and I like have a think about it, but also like feel free to talk us through it if you want to
[00:42:46.180 --> 00:42:47.380]   kind of
[00:42:47.380 --> 00:42:49.380]   Think and talk at the same time
[00:42:49.380 --> 00:42:53.140]   What's the computational complexity of this piece of code?
[00:42:53.140 --> 00:43:00.480]   Can I pass it over there yes
[00:43:00.480 --> 00:43:04.100]   Alright Jade take us through your thought process
[00:43:04.740 --> 00:43:10.940]   I think you have to take each different values through the column to calculate it
[00:43:10.940 --> 00:43:15.200]   Once to see the splits so and then compare
[00:43:15.200 --> 00:43:21.980]   All the cup like all the possible combinations between these different values so that can be expensive
[00:43:21.980 --> 00:43:27.580]   Like cuz you're uh-huh. Can you or does somebody else gonna tell us the actual computational complexity?
[00:43:27.580 --> 00:43:30.100]   So like yeah quite high Jade's thinking
[00:43:30.100 --> 00:43:32.740]   How high?
[00:43:33.740 --> 00:43:35.740]   I
[00:43:35.740 --> 00:43:42.060]   Think it's n squared. Okay, so tell me why is it n squared because for the for loop it is in yes
[00:43:42.060 --> 00:43:46.820]   And I think I guess the standard deviation will take in so it's in square. Okay, or
[00:43:46.820 --> 00:43:52.540]   This one maybe is even easier to know like this is like which ones are less than xi
[00:43:52.540 --> 00:43:56.580]   I'm gonna have to check every value that if it's less than xi. Okay, and so
[00:43:56.580 --> 00:43:59.180]   So it's useful to know like
[00:43:59.540 --> 00:44:02.220]   How do I quickly calculate computational complexity?
[00:44:02.220 --> 00:44:05.560]   I can guarantee most of the interviews you do are going to ask you to calculate
[00:44:05.560 --> 00:44:10.980]   Computational complexity on the fly and it's also like when you're coding you want it to be second nature
[00:44:10.980 --> 00:44:14.180]   So the technique is basically is there a loop?
[00:44:14.180 --> 00:44:17.680]   Okay, we're then we're obviously doing this n times
[00:44:17.680 --> 00:44:23.740]   Okay, so there's an n involved. Is there a loop inside the loop if there is then you need to multiply those two together
[00:44:23.740 --> 00:44:27.460]   In this case there's not is there anything inside the loop?
[00:44:27.460 --> 00:44:30.060]   That's not a constant time thing
[00:44:30.060 --> 00:44:34.460]   so you might see a sort in there and you just need to know that sort is n log n like
[00:44:34.460 --> 00:44:40.180]   That should be second nature if you see a matrix multiply you need to know what that is in this case
[00:44:40.180 --> 00:44:44.980]   There are some things that are doing element wise array operations, right?
[00:44:44.980 --> 00:44:50.100]   So keep an eye out for anything where numpy is doing something to every value of an array in this case
[00:44:50.100 --> 00:44:55.540]   It's checking every value of x against a constant. So it's going to have to do that n times
[00:44:55.540 --> 00:44:58.300]   So to flesh this out into a computational complexity
[00:44:58.300 --> 00:45:05.820]   You just take the number of things in the loop and you multiply it by the highest computational complexity inside the loop
[00:45:05.820 --> 00:45:08.300]   n times n is n squared you pass that
[00:45:08.300 --> 00:45:12.220]   In this case couldn't we just
[00:45:12.220 --> 00:45:17.580]   Presort the list and then do like one and log n computation and there's lots of things we could do to speed this up
[00:45:17.580 --> 00:45:20.500]   So at this stage, it's just like what is the computational complexity we have?
[00:45:21.340 --> 00:45:26.820]   But absolutely it's certainly not as good as it can be. Okay, so and that's where we're going to go next
[00:45:26.820 --> 00:45:31.620]   It's like alright n squared is not it's not great. So let's try and make it better
[00:45:31.620 --> 00:45:37.380]   So here's my attempt at making it better
[00:45:37.380 --> 00:45:41.340]   And the idea is this
[00:45:41.340 --> 00:45:48.740]   Okay, who wants to first of all tell me what's the equation for a standard deviation?
[00:45:48.740 --> 00:45:53.940]   Marsha, can you grab the box?
[00:45:53.940 --> 00:46:01.180]   So for the standard deviation, it's the difference between the value and it's mean
[00:46:01.180 --> 00:46:06.280]   It's we take a square root of that. So we take the
[00:46:06.280 --> 00:46:15.540]   The power of two, yeah, then we sum up all of these observations and we take the square root out of all this sum
[00:46:15.820 --> 00:46:20.580]   Yeah, you have to divide divide by n. Yep. Yep. Great. Good. Okay now
[00:46:20.580 --> 00:46:27.820]   In practice, we don't normally use that formulation because it kind of requires us calculating
[00:46:27.820 --> 00:46:33.900]   You know X minus the mean lots of times. Does anybody know the formulation that just requires?
[00:46:33.900 --> 00:46:39.560]   X and X squared anybody happened to know that one. Yes at the back turn up house that back there
[00:46:39.560 --> 00:46:44.060]   Square root of mean of squares minus
[00:46:44.340 --> 00:46:46.340]   a
[00:46:46.340 --> 00:46:50.580]   Square of mean. Yeah, great mean of squares minus the square of the means
[00:46:50.580 --> 00:46:57.540]   All right, so that's a really good one at divided by it. That's a really good one to know because like you can now calculate
[00:46:57.540 --> 00:47:04.740]   Variances or standard deviations of anything. You just have to first of all grab the column as it is
[00:47:04.740 --> 00:47:10.260]   The column squared right and as long as you've got those stored away somewhere you can
[00:47:10.940 --> 00:47:17.980]   Immediately calculate the standard deviation. So the reason this is handy for us is that if we first of all
[00:47:17.980 --> 00:47:21.560]   Sort our data, right?
[00:47:21.560 --> 00:47:24.940]   Let's go ahead and sort our data
[00:47:24.940 --> 00:47:30.020]   Then if you think about it as we kind of start going down one step at a time
[00:47:30.020 --> 00:47:37.580]   Right, then each group is exactly the same as the previous group on the left hand side with one more thing in it
[00:47:37.820 --> 00:47:40.220]   And on the right hand side with one less thing in it
[00:47:40.220 --> 00:47:44.500]   So given that we just have to keep track of some of X and some of X squared
[00:47:44.500 --> 00:47:51.140]   We can just add one more thing to X one more thing to X squared on the left and remove one thing on the right
[00:47:51.140 --> 00:47:58.300]   Okay, so we don't have to go through the whole lot each time and so we can turn this into a order n
[00:47:58.300 --> 00:48:05.560]   Algorithm so that's all I do here is I sort the data right and they're going to keep track of the count of things on
[00:48:05.560 --> 00:48:06.500]   the right
[00:48:06.500 --> 00:48:10.020]   The sum of things on the right and the sum of squares on the right
[00:48:10.020 --> 00:48:13.540]   And initially everything's in the right hand side
[00:48:13.540 --> 00:48:16.540]   Okay, so initially n is the count
[00:48:16.540 --> 00:48:23.680]   Y sum is the sum on the right and Y squared sum is the sum of squares on the right and
[00:48:23.680 --> 00:48:30.020]   Then nothing is initially on the left. So it's zeros. Okay, and then we just have to loop through
[00:48:30.020 --> 00:48:33.940]   each observation right and
[00:48:36.140 --> 00:48:41.980]   Add one to the left hand count subtract one from the right hand count add the value to the left hand count
[00:48:41.980 --> 00:48:47.860]   Subtract it from the right hand count add the value squared to the left hand subtract it from the right hand
[00:48:47.860 --> 00:48:50.020]   Okay
[00:48:50.020 --> 00:48:57.620]   Now we do need to be careful though because if we're saying less than or equal to one say we're not stopping here
[00:48:57.620 --> 00:49:00.700]   We're stopping here like we have to have everything in that group
[00:49:01.140 --> 00:49:06.820]   So the other thing I'm going to do is I'm just going to make sure that the next value is not the same as this
[00:49:06.820 --> 00:49:10.620]   Value if it is I'm going to skip over it, right? So I'm just going to double-check
[00:49:10.620 --> 00:49:14.060]   That this value and the next one aren't the same
[00:49:14.060 --> 00:49:18.380]   Okay, so as long as they're not the same
[00:49:18.380 --> 00:49:21.700]   I can keep going ahead and calculate my standard deviation now
[00:49:21.700 --> 00:49:27.620]   Passing in the count the sum and the sum squared right and there's that formula
[00:49:29.100 --> 00:49:36.420]   Okay, the sum of squared divided by the square of the sum sorry minus the square of the sum
[00:49:36.420 --> 00:49:43.020]   Do that for the right hand side and so now we can calculate the weighted average score
[00:49:43.020 --> 00:49:48.300]   Just like before and all of these lines are now the same. Okay, so we've turned our order and
[00:49:48.300 --> 00:49:52.980]   Squared algorithm into an order n algorithm and in general
[00:49:54.620 --> 00:50:00.700]   stuff like this is going to get you a lot more value than like pushing something onto a spark cluster or
[00:50:00.700 --> 00:50:05.980]   Ordering faster RAM or using normal cause in your CPU or whatever, right?
[00:50:05.980 --> 00:50:11.500]   This is the way you want to be, you know improving your code and specifically
[00:50:11.500 --> 00:50:14.940]   Write your code
[00:50:14.940 --> 00:50:22.860]   Right without thinking too much about performance run it. Is it fast enough for what you need then you're done if not
[00:50:23.860 --> 00:50:25.860]   profile it right so in
[00:50:25.860 --> 00:50:32.340]   Jupiter instead of saying percent time it you say percent p run and
[00:50:32.340 --> 00:50:35.820]   It will tell you exactly
[00:50:35.820 --> 00:50:43.540]   where the time was spent in your algorithm, and then you can go to the bit that's actually taking the time and think about like
[00:50:43.540 --> 00:50:45.340]   Okay, is this?
[00:50:45.340 --> 00:50:49.180]   Is this algorithmically as efficient as it can be?
[00:50:49.180 --> 00:50:52.540]   Okay, so in this case we run it
[00:50:53.500 --> 00:50:55.500]   and we've gone down from
[00:50:55.500 --> 00:50:59.860]   76 milliseconds to less than 2 milliseconds and
[00:50:59.860 --> 00:51:05.220]   Now some people that are new to programming think like oh great. I've saved
[00:51:05.220 --> 00:51:09.740]   60 something milliseconds, but the point is this is going to get run
[00:51:09.740 --> 00:51:13.060]   Like tens of millions of times
[00:51:13.060 --> 00:51:18.820]   Okay, so the 76 millisecond version is so slow that it's going to be
[00:51:19.660 --> 00:51:27.940]   Practical for any random forest you use in in practice right whereas the 1 millisecond version. I found is actually quite quite acceptable
[00:51:27.940 --> 00:51:34.420]   And then check the numbers should be exactly the same as before and they are okay
[00:51:34.420 --> 00:51:37.420]   so now that we have a
[00:51:37.420 --> 00:51:40.940]   function find better split that does what we want I
[00:51:40.940 --> 00:51:47.160]   Want to insert it into my decision tree class and this is a really cool Python trick
[00:51:47.540 --> 00:51:51.900]   Python does everything dynamically right so we can actually say
[00:51:51.900 --> 00:51:56.100]   The method called find better split in
[00:51:56.100 --> 00:51:59.300]   decision tree is
[00:51:59.300 --> 00:52:05.020]   That function I just created and that might sticks it inside that class
[00:52:05.020 --> 00:52:07.180]   now
[00:52:07.180 --> 00:52:12.100]   I'll tell you what's slightly confusing about this is that this thing this word here and
[00:52:12.100 --> 00:52:15.980]   This word here. They actually have no relationship to each other
[00:52:15.980 --> 00:52:21.700]   They just happen to have the same letters in the same order right so like I could call this find better split
[00:52:21.700 --> 00:52:24.300]   underscore foo
[00:52:24.300 --> 00:52:27.420]   Right and then I could like call that
[00:52:27.420 --> 00:52:32.740]   Right and call that
[00:52:32.740 --> 00:52:40.340]   Right so now my function is actually called find better split underscore foo, but my method
[00:52:42.580 --> 00:52:47.740]   I'm expecting to call something called decision tree dot find better split
[00:52:47.740 --> 00:52:54.420]   Right so here. I could say decision tree dot find better split equals find better split underscore foo
[00:52:54.420 --> 00:52:57.940]   Okay, you see that's the same thing so like
[00:52:57.940 --> 00:53:06.060]   It's important to understand how namespaces work like in in every language that you use
[00:53:06.060 --> 00:53:11.780]   One of the most important things is kind of understanding how how it figures out what a name refers to
[00:53:12.020 --> 00:53:13.820]   So this here
[00:53:13.820 --> 00:53:20.940]   Means find better split as defined inside this class right and note nowhere else right well
[00:53:20.940 --> 00:53:27.340]   I mean a parent class, but never mind about that this one here means find better split foo in
[00:53:27.340 --> 00:53:34.900]   The global namespace a lot of languages don't have a global namespace, but Python does okay, and so
[00:53:34.900 --> 00:53:40.300]   The two are like even if they happen to have the same letters in the same order
[00:53:40.300 --> 00:53:42.300]   They're not referring in any way to the same thing
[00:53:42.300 --> 00:53:44.980]   That makes sense. It's like
[00:53:44.980 --> 00:53:52.460]   This family over here may have somebody called Jeremy and my family has somebody called Jeremy and our names happen to be the same
[00:53:52.460 --> 00:53:55.320]   But we're not the same person, okay
[00:53:55.320 --> 00:54:00.820]   Great so now that we've stuck the decision tree
[00:54:00.820 --> 00:54:08.660]   Sorry the find better split method inside the decision tree with this new definition when I now call the tree ensemble constructor
[00:54:09.660 --> 00:54:14.320]   All right the decision tree ensemble instructor called create tree
[00:54:14.320 --> 00:54:16.700]   create tree
[00:54:16.700 --> 00:54:18.700]   instantiated decision tree
[00:54:18.700 --> 00:54:25.460]   decision tree called find vast split which went through every column to see if it could find a better split and
[00:54:25.460 --> 00:54:28.860]   we've now defined find better split and
[00:54:28.860 --> 00:54:34.860]   Therefore tree ensemble when we create it has gone ahead and done the split
[00:54:37.580 --> 00:54:44.820]   That makes sense that you have any anybody have any questions or uncertainties about that like we're only creating one single split
[00:54:44.820 --> 00:54:47.700]   so far
[00:54:47.700 --> 00:54:55.340]   All right, so this is pretty pretty neat right we kind of just do a little bit at a time testing everything as we go
[00:54:55.340 --> 00:55:02.260]   And so it's as as as you all implement the random forest interpretation techniques
[00:55:02.260 --> 00:55:07.300]   You may want to try programming this way to like every step check that you know
[00:55:07.300 --> 00:55:12.580]   What you're doing matches up with what scikit-learn does or with a test that you've built or whatever?
[00:55:12.580 --> 00:55:16.700]   So at this point we should try to go deeper
[00:55:16.700 --> 00:55:21.940]   Very inception right so let's go now max depth is 2
[00:55:21.940 --> 00:55:27.700]   And so here is what scikit-learn did after breaking at year made 74
[00:55:27.700 --> 00:55:30.460]   It then broke at machine hours meter
[00:55:30.460 --> 00:55:33.540]   2956
[00:55:33.540 --> 00:55:35.780]   So we had this thing called
[00:55:36.940 --> 00:55:38.940]   Find vast split
[00:55:38.940 --> 00:55:44.340]   Right we just went through every column and try to see if there's a better split there
[00:55:44.340 --> 00:55:46.580]   right, but actually
[00:55:46.580 --> 00:55:48.580]   We need to go a bit further than that
[00:55:48.580 --> 00:55:54.500]   Not only do we have to go through every column and see if there's a better split in this node
[00:55:54.500 --> 00:56:01.200]   But then we also have to see whether there's a better split in the left and the right sides that we just created
[00:56:01.700 --> 00:56:07.940]   Right or in other words the left right side and the right hand side should become decision trees themselves
[00:56:07.940 --> 00:56:13.220]   Right so there's no difference at all between what we do here to create this tree
[00:56:13.220 --> 00:56:16.540]   And what we do here to create this tree other than this one contains
[00:56:16.540 --> 00:56:20.220]   159 samples this one contains a thousand
[00:56:20.220 --> 00:56:24.820]   So this row of codes exactly the same as we had before
[00:56:24.820 --> 00:56:31.220]   Right and then we check actually we could do this a little bit easier. We could say if self dot
[00:56:32.220 --> 00:56:35.180]   Is leaf right would be the same thing?
[00:56:35.180 --> 00:56:40.600]   Okay, but I'll just leave it here for now. So it's self dot score. So if the score is
[00:56:40.600 --> 00:56:43.900]   Infinite still, but let's write it properly
[00:56:43.900 --> 00:56:46.740]   Yes
[00:56:46.740 --> 00:56:49.020]   So let's go back up and just remind ourselves
[00:56:49.020 --> 00:56:53.060]   Is leaf is
[00:56:53.060 --> 00:56:55.660]   Self that score equals in okay?
[00:56:55.660 --> 00:56:59.820]   So since there we might as well use it so if it's a leaf node
[00:57:00.820 --> 00:57:06.940]   Then we have nothing further to do right so that means we're right at the bottom. There's no split
[00:57:06.940 --> 00:57:12.180]   That's been made okay, so we don't have to do anything further on the other hand if it's not a leaf node
[00:57:12.180 --> 00:57:14.180]   So it's somewhere back earlier on
[00:57:14.180 --> 00:57:21.580]   Then we need to split it into the left hand side and the right hand side now earlier on we created a left hand side
[00:57:21.580 --> 00:57:23.620]   And a right hand side array of Booleans
[00:57:23.620 --> 00:57:26.100]   right now
[00:57:26.100 --> 00:57:29.360]   Better would be to have here would be have an array of indexes
[00:57:29.360 --> 00:57:34.080]   And that's because we don't want to have a full array of all the Booleans in every single
[00:57:34.080 --> 00:57:39.600]   Node right because remember although it doesn't look like there are many nodes when you see a tree of this size
[00:57:39.600 --> 00:57:42.160]   when it's fully expanded
[00:57:42.160 --> 00:57:48.000]   the bottom level if there's a minimum leaf size of one contains the same number of nodes as
[00:57:48.000 --> 00:57:55.080]   The entire data set and so if every one of those contained a full Boolean array of size of the whole data set
[00:57:55.080 --> 00:57:58.120]   You've got squared memory requirements, which would be bad
[00:57:58.520 --> 00:58:04.720]   Right on the other hand if we just store the indexes with the things in this node, and that's going to get smaller and smaller
[00:58:04.720 --> 00:58:07.480]   Okay
[00:58:07.480 --> 00:58:13.560]   So NP dot non zero is exactly the same as just this thing which gets the Boolean array
[00:58:13.560 --> 00:58:17.200]   But it turns it into the indexes of the trues
[00:58:17.200 --> 00:58:21.040]   Okay, so this is now a list of indexes for the left hand side and
[00:58:21.040 --> 00:58:23.560]   Indexes the right hand side
[00:58:23.560 --> 00:58:27.760]   Okay, so now that we have the indexes the left hand side and the right hand side
[00:58:28.760 --> 00:58:33.280]   We can now just go ahead and create a decision tree. Okay, so there's a decision tree for the left
[00:58:33.280 --> 00:58:36.240]   And there's our decision tree for the right
[00:58:36.240 --> 00:58:38.920]   Okay, and we don't have to do anything else. We've already written these
[00:58:38.920 --> 00:58:41.880]   we already have a
[00:58:41.880 --> 00:58:44.920]   Function of a constructor that can create a decision tree
[00:58:44.920 --> 00:58:52.720]   so like when you really think about what this is doing it kind of hurts your head right because
[00:58:52.720 --> 00:58:56.580]   the reason the whole reason that find vast split got called
[00:58:57.540 --> 00:58:59.540]   is because
[00:58:59.540 --> 00:59:06.780]   Find vast split is called by the decision tree constructor
[00:59:06.780 --> 00:59:15.060]   But then the decision tree that but then find vast split itself then causes the decision tree constructor, so we actually have
[00:59:15.060 --> 00:59:17.980]   circular recursion and
[00:59:17.980 --> 00:59:22.900]   I'm not nearly smart enough to be able to think through recursion
[00:59:23.420 --> 00:59:28.020]   So I just choose not to right like I just write what I mean and
[00:59:28.020 --> 00:59:30.740]   Then I don't think about it anymore
[00:59:30.740 --> 00:59:37.420]   Right like what do I want to find a variable split? I've got to go through every column see if there's something better
[00:59:37.420 --> 00:59:44.260]   If it managed to do a split figure out left hand side of the right hand side and make them into decision trees
[00:59:44.260 --> 00:59:51.220]   Okay, but now try to think through how these two methods call each other would just drive me crazy
[00:59:51.220 --> 00:59:55.020]   But I don't need to right I know I have a decision tree constructor that works
[00:59:55.020 --> 01:00:00.560]   Right I know I have a violent find vast bit that works, so that's it right. That's how I
[01:00:00.560 --> 01:00:03.300]   do recursive programming is
[01:00:03.300 --> 01:00:07.460]   By pretending I don't I just just ignore it
[01:00:07.460 --> 01:00:12.780]   That's my advice a lot of you are probably smart enough to be able to think through it better than I can so that's fine
[01:00:12.780 --> 01:00:18.140]   If you can all right so now that I've written that again, I can patch it into the decision tree class
[01:00:18.140 --> 01:00:20.140]   and
[01:00:20.140 --> 01:00:23.260]   As soon as I do
[01:00:23.260 --> 01:00:30.900]   The tree ensemble constructor will now use that right because Python's dynamic right that's just happens automatically
[01:00:30.900 --> 01:00:33.740]   So now I can check
[01:00:33.740 --> 01:00:35.980]   my left hand side
[01:00:35.980 --> 01:00:39.540]   Should have a hundred and fifty nine samples
[01:00:39.540 --> 01:00:43.540]   Right and a value of nine point six six
[01:00:43.540 --> 01:00:47.020]   There it is hundred fifty nine samples nine point six six
[01:00:47.620 --> 01:00:49.620]   right hand side
[01:00:49.620 --> 01:00:54.680]   Eight forty one ten point one five the left hand side of the left hand side
[01:00:54.680 --> 01:00:58.100]   Hundred and fifty samples nine point six two
[01:00:58.100 --> 01:01:03.260]   Hundred and fifty samples nine point six two okay, so you can see it like
[01:01:03.260 --> 01:01:05.020]   I'm
[01:01:05.020 --> 01:01:11.220]   Because I'm not nearly clever enough to write machine learning algorithms like not only can I not write them correctly the first time
[01:01:11.220 --> 01:01:17.460]   Often like every single line. I write will be wrong right so I always start from the assumption
[01:01:17.460 --> 01:01:23.380]   That the the line of code. I just typed is almost certainly wrong, and I just have to see why and how
[01:01:23.380 --> 01:01:28.660]   Right and so like I just make sure and so eventually I get to the point where like much to my surprise
[01:01:28.660 --> 01:01:30.580]   It's not broken anymore
[01:01:30.580 --> 01:01:35.340]   You know so here I can feel like okay this it would be surprising if all of these things
[01:01:35.340 --> 01:01:39.140]   Accidentally happen to be exactly the same as I kit learn so this is looking pretty good
[01:01:39.140 --> 01:01:42.300]   Okay
[01:01:42.300 --> 01:01:46.540]   So now that we have something that can build a whole tree
[01:01:47.260 --> 01:01:49.840]   We want to have something that can calculate predictions
[01:01:49.840 --> 01:01:56.100]   Right and so to remind you we already have something that calculates predictions for a tree ensemble
[01:01:56.100 --> 01:01:58.580]   by calling tree dot predict
[01:01:58.580 --> 01:02:03.820]   But there is nothing called tree dot predict, so we're going to have to write that
[01:02:03.820 --> 01:02:10.620]   To make this more interesting let's start bringing up the number of columns that we use
[01:02:14.060 --> 01:02:18.820]   Let's create our tree ensemble again, and this time. Let's go to a maximum depth of three
[01:02:18.820 --> 01:02:23.140]   Okay, so now our tree is getting more interesting
[01:02:23.140 --> 01:02:29.820]   And let's now define how do we
[01:02:29.820 --> 01:02:38.520]   Create a set of predictions for a tree and so a set of predictions for a tree is simply the prediction for a row
[01:02:38.520 --> 01:02:41.780]   for every row
[01:02:42.180 --> 01:02:46.180]   That's it all right. That's our predictions so the predictions for a tree
[01:02:46.180 --> 01:02:52.400]   every rows predictions in an array, okay, so again, we're like
[01:02:52.400 --> 01:02:58.900]   Skipping thinking thinking is hard. You know so let's just like keep pushing it back
[01:02:58.900 --> 01:03:05.780]   This is kind of handy right notice that you can do for
[01:03:07.700 --> 01:03:13.500]   Blah in array with a numpy array regardless of the rank of the array
[01:03:13.500 --> 01:03:16.820]   regardless of the number of axes in
[01:03:16.820 --> 01:03:21.940]   The array and what it does is it will loop through the leading axis
[01:03:21.940 --> 01:03:28.900]   Right these these concepts are going to be very very important as we get into more and more neural networks because we're going to be
[01:03:28.900 --> 01:03:35.500]   All doing tensor computations all the time so the leading axis of a vector is the vector itself
[01:03:35.860 --> 01:03:43.140]   The leading axis of a matrix are the rows the leading axis axis of a three-dimensional tensor
[01:03:43.140 --> 01:03:50.180]   The matrices that represent the slices and so forth right so in this case because x is a matrix
[01:03:50.180 --> 01:03:53.220]   This is going to loop through the rows and if you write your
[01:03:53.220 --> 01:03:58.620]   Kind of tensor code this way, then it'll kind of tend to
[01:03:58.620 --> 01:04:04.620]   Generalize nicely to higher dimensions like it doesn't really mention matter how many dimensions are in x
[01:04:04.620 --> 01:04:08.220]   This is going to loop through each of the leading axis, right?
[01:04:08.220 --> 01:04:13.020]   Okay, so we can now call that decision tree dot predict
[01:04:13.020 --> 01:04:20.780]   Right so all I need to do is write predict row
[01:04:20.780 --> 01:04:26.620]   Right and I've delayed thinking so much which is great that the actual point where I actually have to do the work
[01:04:26.620 --> 01:04:28.620]   It's now basically trivial
[01:04:28.620 --> 01:04:31.300]   So if we're at a leaf
[01:04:31.820 --> 01:04:35.140]   No, then the prediction is just equal to
[01:04:35.140 --> 01:04:37.780]   Whatever that value was
[01:04:37.780 --> 01:04:43.180]   Which we calculated right back in the original tree constructor. It's just the average of the wise, right?
[01:04:43.180 --> 01:04:45.900]   If it's not a leaf node
[01:04:45.900 --> 01:04:51.900]   Then we have to figure out whether to go down the left-hand path or the right-hand path to get the prediction, right? So
[01:04:51.900 --> 01:04:55.220]   if
[01:04:55.580 --> 01:05:01.860]   This variable in this row is less than or equal to the thing we decided the amount we decided to split on
[01:05:01.860 --> 01:05:04.500]   Then we go down the left path
[01:05:04.500 --> 01:05:07.100]   Otherwise we go down the right path
[01:05:07.100 --> 01:05:14.260]   Okay, and then having figured out what path we want which tree we want then we can just call predict row on that
[01:05:14.260 --> 01:05:16.620]   right and again
[01:05:16.620 --> 01:05:19.260]   We've accidentally created something recursive
[01:05:19.260 --> 01:05:22.260]   Again, I don't want to think about
[01:05:22.260 --> 01:05:24.820]   how that
[01:05:24.820 --> 01:05:32.420]   Works control flow wise or whatever, but I don't need to because like I just it just does like I just told it
[01:05:32.420 --> 01:05:34.260]   What I wanted so I'll trust it to work, right?
[01:05:34.260 --> 01:05:40.820]   If it's a leaf return the value otherwise return the prediction for the left-hand side or the right-hand side as appropriate
[01:05:40.820 --> 01:05:50.180]   Notice this here this if has nothing to do with this if
[01:05:51.340 --> 01:05:54.280]   All right, this if is a control flow statement
[01:05:54.280 --> 01:05:59.860]   That tells Python to go down that path or that path to do some calculation
[01:05:59.860 --> 01:06:03.140]   this if is an
[01:06:03.140 --> 01:06:05.860]   operator that
[01:06:05.860 --> 01:06:07.860]   returns a value
[01:06:07.860 --> 01:06:13.540]   So those of you that have done C or C++ will recognize it as being identical to that
[01:06:13.540 --> 01:06:20.340]   It's called the ternary operator, right? If you haven't that's fine. Basically what we're doing is we're going to get a value
[01:06:20.780 --> 01:06:25.800]   where we're going to say it's this value if this thing is true and
[01:06:25.800 --> 01:06:28.660]   that value otherwise
[01:06:28.660 --> 01:06:32.000]   And so you could write it
[01:06:32.000 --> 01:06:34.740]   This way
[01:06:34.740 --> 01:06:35.300]   All right
[01:06:35.300 --> 01:06:42.380]   But that would require writing four lines of code to do one thing and also require you to have code that if you read it
[01:06:42.380 --> 01:06:47.100]   To yourself or to somebody else is not at all naturally the way you would express it, right?
[01:06:47.100 --> 01:06:50.420]   I want to say the tree I got to go down is
[01:06:50.420 --> 01:06:56.000]   The left-hand side if the variables less than the split or the right-hand side, otherwise
[01:06:56.000 --> 01:07:01.300]   Right, so I want to write my code the way I would think about or the way I would say my code
[01:07:01.300 --> 01:07:07.580]   Okay, so this kind of ternary operator can be quite helpful for that
[01:07:07.580 --> 01:07:10.140]   All right
[01:07:10.140 --> 01:07:13.460]   So now that I've got a prediction for row I can dump that into my class
[01:07:14.900 --> 01:07:17.800]   And now I can create calculate predictions
[01:07:17.800 --> 01:07:24.220]   And I can now plot my actuals against my predictions
[01:07:24.220 --> 01:07:28.100]   When you do a scatterplot
[01:07:28.100 --> 01:07:31.380]   You'll often have a lot of dots sitting on top of each other
[01:07:31.380 --> 01:07:37.580]   So a good trick is to use alpha alpha means how transparent the things not just a matplotlib
[01:07:37.580 --> 01:07:42.060]   But like in every graphics package in the world pretty much and so if you set alpha to less than 1
[01:07:42.660 --> 01:07:47.340]   Then this is saying you would need 20 dots on top of each other for it to be fully blue
[01:07:47.340 --> 01:07:49.880]   And so this is a good way to kind of see
[01:07:49.880 --> 01:07:54.640]   How much things are sitting on top of each other? So it's a good trick good trick for scatter plots
[01:07:54.640 --> 01:07:59.300]   There's my R squared not bad
[01:07:59.300 --> 01:08:03.540]   And so let's now go ahead and
[01:08:03.540 --> 01:08:06.820]   Do a random forest
[01:08:06.820 --> 01:08:11.500]   With no max amount of splitting
[01:08:11.820 --> 01:08:13.820]   and
[01:08:13.820 --> 01:08:15.780]   Our
[01:08:15.780 --> 01:08:20.060]   Tree ensemble had no max amount of splitting we can compare our R squared
[01:08:20.060 --> 01:08:24.300]   To their R squared and so they're not the same
[01:08:24.300 --> 01:08:30.940]   But actually ours is a little better, so I don't know what we did differently, but we'll take it
[01:08:30.940 --> 01:08:39.260]   Okay, so we have now something which for a forest with a single tree in is giving as
[01:08:39.820 --> 01:08:41.820]   good accuracy
[01:08:41.820 --> 01:08:46.940]   On a validation set using an actual real-world data set you know for Bluetooth is
[01:08:46.940 --> 01:08:49.740]   Compared to psychic learn
[01:08:49.740 --> 01:08:53.260]   So let's go ahead and round this out
[01:08:53.260 --> 01:08:59.420]   So what I would want to do now is to create a package that has this code in and I created it by like creating
[01:08:59.420 --> 01:09:02.460]   A method here a method here a method here and patching them together
[01:09:02.460 --> 01:09:07.340]   So what I did with now is I went back through my notebook and collected up all the cells
[01:09:07.580 --> 01:09:12.140]   That implemented methods and pasted them all together right, and I've just pasted them down here
[01:09:12.140 --> 01:09:17.620]   So here's this is my original tree ensemble and here is all the cells from the decision tree
[01:09:17.620 --> 01:09:20.620]   I just dumped them all into one place without any change
[01:09:20.620 --> 01:09:29.060]   So that was it that was the code we wrote together so now I can go ahead and I can create a
[01:09:31.980 --> 01:09:39.440]   Tree ensemble I can calculate my predictions. I can do my scatter plot. I can get my R squared
[01:09:39.440 --> 01:09:42.020]   Right and this is now with
[01:09:42.020 --> 01:09:45.020]   five trees
[01:09:45.020 --> 01:09:51.700]   Right and here we are we have a model of blue dook for bulldozers with a 71% R squared
[01:09:51.700 --> 01:09:54.300]   With a random forest we wrote
[01:09:54.300 --> 01:09:56.900]   entirely from scratch
[01:09:56.900 --> 01:09:59.220]   That's pretty cool
[01:10:00.420 --> 01:10:05.900]   Any questions about that and I know there's like quite a got to get through so like during the week
[01:10:05.900 --> 01:10:08.340]   Feel free to ask on the forum
[01:10:08.340 --> 01:10:14.240]   About any bits of code you come across can somebody pass the box to measure. Oh, there it is
[01:10:14.240 --> 01:10:22.660]   Can we get back to the probably to the top of maybe
[01:10:23.980 --> 01:10:31.380]   At the decision tree when we set the score equal to infinity, right? Yes, do we calculate this car this score?
[01:10:31.380 --> 01:10:37.080]   Further, I mean like I lost track of that and specifically I wonder
[01:10:37.080 --> 01:10:40.740]   What do we implement?
[01:10:40.740 --> 01:10:42.660]   when we implement
[01:10:42.660 --> 01:10:49.260]   Find var split we check for self score equal to whether it's equal to infinity or not
[01:10:49.260 --> 01:10:53.300]   it seems to me it seems like unclear whether we
[01:10:54.100 --> 01:10:56.100]   fall out of this I
[01:10:56.100 --> 01:10:59.140]   Mean like if we ever implement
[01:10:59.140 --> 01:11:03.980]   The methods if if our initial value is infinity
[01:11:03.980 --> 01:11:07.660]   So okay, let's talk through the logic. So
[01:11:07.660 --> 01:11:10.460]   so the decision tree
[01:11:10.460 --> 01:11:17.020]   Starts out with a score of infinity. So in other words at this point when we've created the node there is no split
[01:11:17.020 --> 01:11:19.620]   So it's infinitely bad
[01:11:20.100 --> 01:11:27.220]   Okay, that's why the score is infinity and then we try to find a variable and a split
[01:11:27.220 --> 01:11:29.740]   that is better and
[01:11:29.740 --> 01:11:33.460]   to do that we loop through each column and
[01:11:33.460 --> 01:11:40.460]   Say hey column. Do you have a split which is better than the best one we have so far?
[01:11:40.460 --> 01:11:44.260]   And so then we implement that
[01:11:47.860 --> 01:11:50.700]   Let's do the slow way since it's a bit simpler
[01:11:50.700 --> 01:11:55.680]   find better split we do that by looping through each row and
[01:11:55.680 --> 01:12:00.420]   Finding out this is the current score if we split here
[01:12:00.420 --> 01:12:04.020]   Is it better than the current score the current score is infinitely bad?
[01:12:04.020 --> 01:12:04.820]   So yes
[01:12:04.820 --> 01:12:11.140]   It is and so now we set the new score equal to what we just calculated and we keep track of which variable we chose
[01:12:11.140 --> 01:12:13.140]   And the split we split on
[01:12:13.140 --> 01:12:15.460]   Okay, no worries
[01:12:15.460 --> 01:12:17.460]   you
[01:12:17.460 --> 01:12:25.500]   Okay, great, let's take a five-minute break and I'll see you back here at 22
[01:12:25.500 --> 01:12:34.060]   So when I tried comparing the performance of this
[01:12:34.060 --> 01:12:38.500]   Against scikit-learn
[01:12:38.500 --> 01:12:41.300]   This is quite a lot slower
[01:12:41.820 --> 01:12:49.180]   And the reason why is that although like a lot of the works being done by numpy
[01:12:49.180 --> 01:12:57.620]   Which is nicely optimized C code think about like the very bottom level of a tree if we've got a
[01:12:57.620 --> 01:13:00.500]   million data points
[01:13:00.500 --> 01:13:03.140]   And the bottom level of the tree has something like
[01:13:03.140 --> 01:13:05.980]   500,000
[01:13:05.980 --> 01:13:10.820]   decision points with a million leaves underneath right and so that's like
[01:13:11.700 --> 01:13:19.300]   Five hundred thousand split methods being called each one of contain which contains multiple calls to numpy which only have like
[01:13:19.300 --> 01:13:21.300]   one
[01:13:21.300 --> 01:13:28.460]   Item that's actually being calculated on and so it's like that's like very inefficient and it's the kind of thing that python is
[01:13:28.460 --> 01:13:34.380]   Particularly not good at performance-wise right like calling lots of functions lots of times
[01:13:34.380 --> 01:13:39.100]   I mean we can see it's it's not bad right you know for a kind of a
[01:13:39.900 --> 01:13:41.900]   random forest which
[01:13:41.900 --> 01:13:48.300]   15 years ago would have been considered pretty big this would be considered pretty good performance right, but nowadays this is
[01:13:48.300 --> 01:13:53.060]   Some hundreds of times at least slower than than it should be so
[01:13:53.060 --> 01:14:01.460]   What the scikit-learn folks did to avoid this problem was that they wrote
[01:14:01.460 --> 01:14:05.220]   their implementation in something called scythe on and
[01:14:07.020 --> 01:14:13.100]   scythe on is a super set of python so any python you've written
[01:14:13.100 --> 01:14:16.460]   pretty much
[01:14:16.460 --> 01:14:19.580]   You can use as scythe on
[01:14:19.580 --> 01:14:28.120]   But then what happens is scythe on runs it in a very different way rather than passing it to the kind of the
[01:14:28.120 --> 01:14:32.780]   Python interpreter it instead converts it to C
[01:14:34.100 --> 01:14:40.260]   Compiles that and then runs that C code right which means the first time you run it
[01:14:40.260 --> 01:14:47.620]   It takes a little longer because it has to go through the the kind of translation and compilation, but then after that it can be
[01:14:47.620 --> 01:14:50.260]   quite a bit faster
[01:14:50.260 --> 01:14:54.100]   and so I wanted just to quickly show you what that looks like because
[01:14:54.100 --> 01:15:01.380]   You are absolutely going to be in a position where scythe on is going to help you with your work and
[01:15:02.180 --> 01:15:06.820]   Most of the people you're working with will have never used it may not even know it exists
[01:15:06.820 --> 01:15:08.820]   And so this is like a great superpower to have
[01:15:08.820 --> 01:15:15.580]   So to use scytheon in a notebook you say load, ext, load extension, scytheon, right?
[01:15:15.580 --> 01:15:18.720]   And so here's a Python function
[01:15:18.720 --> 01:15:21.940]   fib1
[01:15:21.940 --> 01:15:31.720]   Here is the same as a scytheon function is exactly the same thing with percent percent scytheon at the top
[01:15:32.200 --> 01:15:34.200]   you
[01:15:34.200 --> 01:15:41.120]   This actually runs about twice as fast as this right just because it does the compilation
[01:15:41.120 --> 01:15:48.220]   Here is the same version again where I've used a special scytheon
[01:15:48.220 --> 01:15:57.160]   extension called cdef which defines the C data type of the return value and of each variable
[01:15:57.160 --> 01:16:00.120]   right and so
[01:16:00.200 --> 01:16:03.360]   Basically, that's the trick that you can use to start making things
[01:16:03.360 --> 01:16:08.460]   Run quickly right and at that point now. It knows it's not just some
[01:16:08.460 --> 01:16:14.880]   Python object called t in fact I probably should put one here as well
[01:16:14.880 --> 01:16:20.940]   Let's try that so we've got fib2 we'll call that fib3
[01:16:20.940 --> 01:16:26.560]   So for fib3
[01:16:28.800 --> 01:16:34.560]   Yeah, so it's exactly the same as before but we say what the data type of the thing we passed to it was is and then
[01:16:34.560 --> 01:16:39.400]   Define the data types of each of the variables and so then if we call that
[01:16:39.400 --> 01:16:49.160]   Okay, we've now got something that's 10 times faster right so
[01:16:49.160 --> 01:16:52.360]   Yeah, it doesn't really take that much extra
[01:16:52.360 --> 01:16:56.360]   And it's just it's just Python with a few little bits of markup
[01:16:56.600 --> 01:17:00.280]   so that's like it's it's good to know that that exists because
[01:17:00.280 --> 01:17:03.440]   If there's something custom you're trying to do
[01:17:03.440 --> 01:17:09.680]   It's actually I find it kind of painful having to go out and you know going to see and compile it and link it back
[01:17:09.680 --> 01:17:13.560]   And all that where it's doing it here is pretty easy. Can you pass that just to your right please musher?
[01:17:13.560 --> 01:17:22.920]   So when you're doing like for the scytheon version of it so in the case of an array or an MP array
[01:17:23.520 --> 01:17:28.520]   This is a specific C type of yeah, so there's like a lot of
[01:17:28.520 --> 01:17:34.040]   Specific stuff for integrating scytheon with numpy
[01:17:34.040 --> 01:17:38.720]   And there's a whole page about it
[01:17:38.720 --> 01:17:42.080]   Yeah, so we won't worry about going over it
[01:17:42.080 --> 01:17:47.760]   But you can read that and you can basically see the basic ideas. There's this C import which basically
[01:17:47.760 --> 01:17:49.800]   imports a
[01:17:49.800 --> 01:17:57.880]   Certain types of Python library into the kind of the C bit of the code and you can then use it in your site them
[01:17:57.880 --> 01:18:01.080]   Yeah, it's
[01:18:01.080 --> 01:18:03.320]   It's pretty straightforward
[01:18:03.320 --> 01:18:10.520]   Good question. Thank you all right so your your mission now
[01:18:10.520 --> 01:18:15.320]   Is to implement
[01:18:18.520 --> 01:18:24.160]   Confidence based on tree variants feature importance partial dependence and tree interpreter
[01:18:24.160 --> 01:18:27.080]   for that random first
[01:18:27.080 --> 01:18:31.120]   Removing redundant features doesn't use a random first at all
[01:18:31.120 --> 01:18:33.760]   So you don't have to worry about that
[01:18:33.760 --> 01:18:38.160]   Extrapolation is not an interpretation technique, so you don't have to worry about that, so it's just the other ones
[01:18:38.160 --> 01:18:41.920]   So confidence based on tree variants. We've already written that code
[01:18:41.960 --> 01:18:48.520]   So I suspect that the exact same code we have in the notebook should continue to work so you can try and make sure it
[01:18:48.520 --> 01:18:50.160]   Get that working
[01:18:50.160 --> 01:18:55.480]   Feature importance is with the variable shuffling technique and once you have that working
[01:18:55.480 --> 01:19:02.920]   Partial dependence will just be a couple of lines of code away because it rather than you know rather than shuffling a column
[01:19:02.920 --> 01:19:06.640]   You're just replacing it with a constant value that it's nearly the same code and
[01:19:06.640 --> 01:19:09.640]   then tree interpreter
[01:19:09.720 --> 01:19:14.480]   It's going to require you writing some code and thinking about that well once you've written tree interpreter
[01:19:14.480 --> 01:19:19.040]   You're very close if you want to to creating the second approach to
[01:19:19.040 --> 01:19:26.300]   Feature importance the one where you add up the importance across all of the rows
[01:19:26.300 --> 01:19:30.480]   Which means you would then be very close to doing interaction importance
[01:19:30.480 --> 01:19:38.360]   So it turns out that there are actually there's actually a very good library for interaction importance for xg boost
[01:19:39.080 --> 01:19:41.880]   But there doesn't seem to be one for random forest
[01:19:41.880 --> 01:19:45.760]   so you could like start by getting it working on our version and
[01:19:45.760 --> 01:19:50.040]   If you want to do interaction importance, and then you could like get it working on the original
[01:19:50.040 --> 01:19:55.280]   Sklearn version and that would be a cool contribution
[01:19:55.280 --> 01:20:01.440]   Like sometimes writing it against your own implementation is kind of nicer because you can see exactly what's going on
[01:20:01.440 --> 01:20:08.320]   All right, so that's that's your job. You don't have to rewrite the random forest feel free to if you want to you know practice
[01:20:08.920 --> 01:20:10.920]   so
[01:20:10.920 --> 01:20:13.320]   If you
[01:20:13.320 --> 01:20:18.200]   Get stuck at any point you know ask on the forum, right?
[01:20:18.200 --> 01:20:21.520]   there is a
[01:20:21.520 --> 01:20:26.320]   Whole page here on wiki.fast.ai about how to ask for help
[01:20:26.320 --> 01:20:29.080]   so when you
[01:20:29.080 --> 01:20:30.760]   ask your
[01:20:30.760 --> 01:20:36.640]   Co-workers on slack for help when you ask people in a technical community on github or
[01:20:37.080 --> 01:20:39.080]   discourse to help or whatever
[01:20:39.080 --> 01:20:43.400]   Asking for help the right way will go a long way towards
[01:20:43.400 --> 01:20:48.360]   You know having people want to help you and be able to help you right so
[01:20:48.360 --> 01:20:55.120]   So like search for your answer like search for the area you're getting see if somebody's already asked about it
[01:20:55.120 --> 01:21:03.440]   You know how have you tried to fix it already? What do you think is going wrong?
[01:21:04.240 --> 01:21:08.160]   What kind of computer are you on? How is it set up? What are the software versions?
[01:21:08.160 --> 01:21:13.200]   Exactly. What did you type and exactly what happened right now? You could
[01:21:13.200 --> 01:21:16.520]   do that by
[01:21:16.520 --> 01:21:22.880]   Taking a screenshot, so you know make sure you've got some screenshot software. That's really easy to use
[01:21:22.880 --> 01:21:24.880]   So if I were to take a screenshot, I just hit a button
[01:21:24.880 --> 01:21:30.720]   Select the area copy the clipboard go to my forum
[01:21:31.200 --> 01:21:37.360]   Paste it in and there we go right that looks a little bit too big, so let's make it a little smaller
[01:21:37.360 --> 01:21:41.880]   Right and so now I've got a screenshot people can see exactly what happened
[01:21:41.880 --> 01:21:44.280]   better still
[01:21:44.280 --> 01:21:50.120]   If there's a few lines of code and error messages to look at and create a gist
[01:21:50.120 --> 01:21:53.000]   Just is a handy little
[01:21:53.000 --> 01:21:58.720]   GitHub thing which basically lets you share code, so if I wanted to
[01:21:59.520 --> 01:22:01.520]   create a gist of this I
[01:22:01.520 --> 01:22:09.680]   Actually have a extension area that little extension, so if I click on here
[01:22:09.680 --> 01:22:14.120]   Give it a name
[01:22:14.120 --> 01:22:16.280]   Say make public
[01:22:16.280 --> 01:22:22.500]   Okay, and that takes my Jupyter notebook shares it publicly. I can then grab that URL
[01:22:22.500 --> 01:22:25.200]   copy link location
[01:22:25.200 --> 01:22:28.080]   Right and paste it into my forum post
[01:22:28.960 --> 01:22:30.960]   right and then when people
[01:22:30.960 --> 01:22:33.080]   click on it
[01:22:33.080 --> 01:22:36.600]   Then they'll immediately see
[01:22:36.600 --> 01:22:40.860]   My notebook when it renders
[01:22:40.860 --> 01:22:50.760]   Okay, so that's a really good way now that particular button is an extension so on Jupyter
[01:22:50.760 --> 01:22:54.200]   You need to click nv extensions and click on
[01:22:55.000 --> 01:22:59.440]   Gist it right while you're there. You should also click on collapsible headings
[01:22:59.440 --> 01:23:03.420]   That's this really handy thing. I use that lets me collapse things and open them up
[01:23:03.420 --> 01:23:08.760]   If you go to your Jupyter and don't see this nv extensions button
[01:23:08.760 --> 01:23:11.480]   Then just google for Jupyter nv extensions
[01:23:11.480 --> 01:23:18.480]   It'll show you how to pip install it and and get it set up right, but those two extensions are super duper handy
[01:23:18.480 --> 01:23:22.800]   All right, so
[01:23:24.000 --> 01:23:26.000]   Other than that assignment
[01:23:26.000 --> 01:23:28.640]   We're we're done
[01:23:28.640 --> 01:23:34.820]   With random forests and until the next course when you look at GBMs. We're done with decision tree ensembles
[01:23:34.820 --> 01:23:44.560]   And so we're going to move on to neural networks broadly defined and so
[01:23:44.560 --> 01:23:49.160]   Neural networks are going to allow us to
[01:23:49.720 --> 01:23:55.360]   To go beyond just you know the kind of nearest neighbors approach of random forests
[01:23:55.360 --> 01:23:59.500]   You know all the random forest can do is to average data that it's already seen
[01:23:59.500 --> 01:24:04.360]   It can't extrapolate. It can't they can't calculate right?
[01:24:04.360 --> 01:24:07.240]   linear regression
[01:24:07.240 --> 01:24:12.200]   Can calculate and can extrapolate but only in very limited ways
[01:24:12.200 --> 01:24:16.920]   Neural nets give us the best of both worlds
[01:24:19.200 --> 01:24:21.980]   We're going to start by applying them to
[01:24:21.980 --> 01:24:24.640]   unstructured data
[01:24:24.640 --> 01:24:32.600]   Right so unstructured data means like pixels or the amplitudes of sound waves or words you know data where?
[01:24:32.600 --> 01:24:34.720]   everything in
[01:24:34.720 --> 01:24:36.720]   all the columns
[01:24:36.720 --> 01:24:38.520]   Are all of the same type?
[01:24:38.520 --> 01:24:45.000]   You know as opposed to like a database table where you've got like a revenue and a cost and a zip code and a state
[01:24:45.000 --> 01:24:47.160]   It should be structured data
[01:24:48.280 --> 01:24:52.140]   We're going to use it for structured data as well, but we're going to do that a little bit later
[01:24:52.140 --> 01:24:55.120]   So unstructured data is a little easier
[01:24:55.120 --> 01:25:00.720]   And it's also the area which more people have been applying deep learning to for longer
[01:25:00.720 --> 01:25:08.200]   The
[01:25:08.200 --> 01:25:11.120]   If you're doing the deep learning course as well
[01:25:11.120 --> 01:25:17.400]   You know you'll see that we're going to be approaching kind of the same conclusion from two different directions
[01:25:17.920 --> 01:25:21.320]   So the deep learning course is starting out with
[01:25:21.320 --> 01:25:27.600]   Big complicated convolutional neural networks being solved with you know
[01:25:27.600 --> 01:25:33.200]   Sophisticated optimization schemes and we're going to kind of gradually drill down into like exactly how they work
[01:25:33.200 --> 01:25:39.280]   Where else with the machine learning course we're going to be starting out more with like
[01:25:39.280 --> 01:25:42.720]   How does stochastic gradient descent actually work?
[01:25:43.440 --> 01:25:50.200]   What do we do? What can we do with like one single layer? Which would allow us to create things like logistic regression when we add?
[01:25:50.200 --> 01:25:54.000]   Regularization to that how does that give us things like?
[01:25:54.000 --> 01:26:01.040]   Ridge regression elastic net less. Oh and then as we add additional layers to that how does that let us?
[01:26:01.040 --> 01:26:06.840]   Handle more complex problems, and so we're not going to we're only going to be looking at
[01:26:06.840 --> 01:26:12.560]   fully connected layers in this machine learning course and
[01:26:12.920 --> 01:26:17.800]   Then I think next semester with your net you're probably going to be looking at some more
[01:26:17.800 --> 01:26:22.120]   Sophisticated approaches and so yes on this machine learning
[01:26:22.120 --> 01:26:27.800]   We're going to be looking much more at like what's actually happening with the matrices and how they actually calculate it and the deep learning
[01:26:27.800 --> 01:26:30.840]   It's much more like what are the best practices for actually?
[01:26:30.840 --> 01:26:38.160]   Solving you know at a world-class level real world deep learning problems, right so
[01:26:38.160 --> 01:26:41.360]   Next week we're going to
[01:26:41.440 --> 01:26:43.440]   be
[01:26:43.440 --> 01:26:50.840]   Looking at like the classic Mness problem, which is like how do we recognize digits now?
[01:26:50.840 --> 01:26:58.240]   If you're interested you can like skip ahead and like try and do this with a random forest and you'll find it's not bad
[01:26:58.240 --> 01:27:03.020]   Right given that a random forest is basically a type of nearest neighbors, right?
[01:27:03.020 --> 01:27:06.680]   It's finding like what are the nearest neighbors in in tree space?
[01:27:07.360 --> 01:27:11.800]   Then a random forest can absolutely recognize that this nine those pixels
[01:27:11.800 --> 01:27:18.000]   You know are similar to pixels. We've seen in these other ones and on average they were nines as well
[01:27:18.000 --> 01:27:21.320]   All right, and so like we can absolutely solve
[01:27:21.320 --> 01:27:24.040]   These kinds of problems to an extent
[01:27:24.040 --> 01:27:26.800]   using random forests
[01:27:26.800 --> 01:27:33.280]   But we end up being rather data limited because every time we put in another decision point you know
[01:27:33.280 --> 01:27:40.120]   We're halving our data roughly and so there's this just this limitation and the amount of calculation that we can do
[01:27:40.120 --> 01:27:43.000]   Where else with neural nets?
[01:27:43.000 --> 01:27:47.800]   We're going to be able to use lots and lots and lots of parameters
[01:27:47.800 --> 01:27:51.760]   Using these tricks we don't learn about with regularization
[01:27:51.760 --> 01:27:56.840]   And so we're going to be able to do lots of computation, and there's going to be very little limitation on really
[01:27:57.920 --> 01:28:05.680]   What we can actually end up calculating as a result great good luck with your random forest interpretation, and I will see you next time

