
[00:00:00.000 --> 00:00:04.040]   If I'm not clever, it's about one thing.
[00:00:04.040 --> 00:00:05.320]   It's about open collaboration.
[00:00:05.320 --> 00:00:11.840]   We want people to think that science is not, it's not like science can be associated with
[00:00:11.840 --> 00:00:12.840]   employment.
[00:00:12.840 --> 00:00:16.480]   You join a job, you do science, but science can also just be a thing, be a gig.
[00:00:16.480 --> 00:00:18.000]   You're like, you're an artist.
[00:00:18.000 --> 00:00:22.920]   You can join a studio to become a senior artist in that studio, but you can also just do art
[00:00:22.920 --> 00:00:24.320]   on your side.
[00:00:24.320 --> 00:00:26.960]   And science is at the same time a collective effort.
[00:00:26.960 --> 00:00:27.960]   You need collaborators.
[00:00:27.960 --> 00:00:30.600]   You need people to work together with you.
[00:00:30.600 --> 00:00:34.480]   So for that to happen, if you're taking science as a gig, then you have to be able to work
[00:00:34.480 --> 00:00:38.840]   with other people, but then we don't have really a culture there yet.
[00:00:38.840 --> 00:00:43.000]   If you analyze all the papers out there, Google people are working with Google people, CMU
[00:00:43.000 --> 00:00:44.320]   people are working with CMU people.
[00:00:44.320 --> 00:00:46.880]   Not exactly, but there are clusters.
[00:00:46.880 --> 00:00:48.920]   Welcome to the Gradient Dissent podcast.
[00:00:48.920 --> 00:00:53.560]   I'm here today with Lavanya and we have as a guest, Roseanne Liu.
[00:00:53.560 --> 00:00:56.280]   I'll say I am super excited to talk to Roseanne.
[00:00:56.280 --> 00:01:01.560]   I had heard of her good work at Uber for quite a while, but then she actually came by Weights
[00:01:01.560 --> 00:01:05.440]   and Biases to give an open talk about one of her research papers.
[00:01:05.440 --> 00:01:09.640]   And I was just so impressed with the kind of creativity in the way that she analyzed
[00:01:09.640 --> 00:01:10.640]   the neural networks.
[00:01:10.640 --> 00:01:13.040]   And we'll get into that talk with her today.
[00:01:13.040 --> 00:01:15.960]   But Lavanya, you actually found recent stuff that's even kind of maybe more exciting that
[00:01:15.960 --> 00:01:16.960]   she's working on.
[00:01:16.960 --> 00:01:17.960]   Yeah.
[00:01:17.960 --> 00:01:23.360]   So she has founded this amazing organization called ML Collective, and she's trying to
[00:01:23.360 --> 00:01:26.640]   democratize AI to anyone.
[00:01:26.640 --> 00:01:30.600]   And she's trying to ensure even if you're not tied to one of these super prestigious
[00:01:30.600 --> 00:01:34.080]   institutions, you can still publish really cool research.
[00:01:34.080 --> 00:01:36.240]   And that is just such an important thing to work on.
[00:01:36.240 --> 00:01:38.680]   So I'm super excited to talk to her.
[00:01:38.680 --> 00:01:40.600]   Welcome to Gradient Dissent.
[00:01:40.600 --> 00:01:44.960]   I want to start with what made you found ML Collective.
[00:01:44.960 --> 00:01:47.320]   It's such an important organization.
[00:01:47.320 --> 00:01:52.400]   And as someone who cares about diversity and also democratizing AI, I'm curious about your
[00:01:52.400 --> 00:01:53.400]   thoughts.
[00:01:53.400 --> 00:01:54.400]   Yeah, thank you.
[00:01:54.400 --> 00:01:55.400]   Thank you for having me.
[00:01:55.400 --> 00:01:56.920]   And thanks for saying that it's an important organization.
[00:01:56.920 --> 00:01:58.240]   I don't think we're there yet.
[00:01:58.240 --> 00:02:02.720]   We're so new and young that it can really go either way at this point.
[00:02:02.720 --> 00:02:07.560]   One wrong decision we make, you can really turn it to be a not so good organization.
[00:02:07.560 --> 00:02:12.840]   But yeah, so ML Collective is, interestingly, it's like a million things in my head right
[00:02:12.840 --> 00:02:13.840]   now.
[00:02:13.840 --> 00:02:17.680]   Because as someone who runs a company, I'm sure, Lucas, you have the same feelings.
[00:02:17.680 --> 00:02:21.400]   Like there's a narrative always in your head going, like, what is this company I'm trying
[00:02:21.400 --> 00:02:22.400]   to do?
[00:02:22.400 --> 00:02:23.400]   What is this thing?
[00:02:23.400 --> 00:02:25.680]   And then you like little by little add ideas to it.
[00:02:25.680 --> 00:02:31.040]   And then at this point, just like so many ideas all combined together, basically representing
[00:02:31.040 --> 00:02:32.960]   everything I want to do in my life.
[00:02:32.960 --> 00:02:36.480]   One research, I want it to be a research lab that people just do research together.
[00:02:36.480 --> 00:02:39.000]   There should be people better than me, ideally.
[00:02:39.000 --> 00:02:43.280]   And there should be people like better than me, but maybe less experience so I can offer
[00:02:43.280 --> 00:02:45.720]   help to them so I can feel useful.
[00:02:45.720 --> 00:02:47.360]   There should be a wide range of people.
[00:02:47.360 --> 00:02:52.320]   There should be people who like having a home, like I like having a lab feeling.
[00:02:52.320 --> 00:02:55.480]   There are people out there doing things better by themselves, but we are really trying to
[00:02:55.480 --> 00:03:00.120]   attract people that work better with people collectively.
[00:03:00.120 --> 00:03:01.120]   So it's a research lab.
[00:03:01.120 --> 00:03:05.760]   It's also a nonprofit because I also want to do charity, I want to help people.
[00:03:05.760 --> 00:03:09.440]   And I think one dimension of science should be done within nonprofit.
[00:03:09.440 --> 00:03:14.360]   I mean, we don't see a lot of things, a lot of science in terms of ML research going on
[00:03:14.360 --> 00:03:15.360]   in nonprofits.
[00:03:15.360 --> 00:03:18.720]   It's mainly driven by industrial labs because they have all the resources.
[00:03:18.720 --> 00:03:24.200]   But I think if we can set up one small example to show great research done through nonprofits,
[00:03:24.200 --> 00:03:25.200]   that would be great.
[00:03:25.200 --> 00:03:28.640]   Then people can really open their mind when they think about their career choices.
[00:03:28.640 --> 00:03:31.480]   There's like one more avenue for them to choose.
[00:03:31.480 --> 00:03:36.960]   So research lab nonprofit is also just like a co-working space that people just come together
[00:03:36.960 --> 00:03:42.000]   when during their gap year or when people feel like just want to dabble in science a
[00:03:42.000 --> 00:03:45.840]   little bit, or they're moving out of science, but they still want to get involved in, want
[00:03:45.840 --> 00:03:47.880]   to see what's going on in ML.
[00:03:47.880 --> 00:03:48.880]   So it's just that.
[00:03:48.880 --> 00:03:49.880]   Yeah.
[00:03:49.880 --> 00:03:53.680]   It's also something very personal, something that sort of changed my life, but we'll get
[00:03:53.680 --> 00:03:54.680]   to that maybe later.
[00:03:54.680 --> 00:03:55.680]   No, let's go there.
[00:03:55.680 --> 00:04:02.920]   Tell us the story behind how you decided to create it.
[00:04:02.920 --> 00:04:03.920]   Yeah.
[00:04:03.920 --> 00:04:08.480]   Interestingly, it's such a big change in my life, like maybe the biggest change ever in
[00:04:08.480 --> 00:04:09.480]   my life.
[00:04:09.480 --> 00:04:14.800]   So now I just wonder if like every business, every change you see in someone else's life
[00:04:14.800 --> 00:04:18.680]   must be propelled by like a misfortune because that's what happened to me.
[00:04:18.680 --> 00:04:22.200]   It was spurred by a misfortune.
[00:04:22.200 --> 00:04:24.200]   Basically the whole narrative goes like this.
[00:04:24.200 --> 00:04:25.960]   So I was looking for a job.
[00:04:25.960 --> 00:04:31.280]   I was out of a job in first place and I was looking for a job and there's no job really
[00:04:31.280 --> 00:04:36.080]   that offers the things I wanted a working environment to have.
[00:04:36.080 --> 00:04:37.640]   So we decided to build our own.
[00:04:37.640 --> 00:04:42.720]   So that's as easy as it is, but you can imagine not being able to find a job, just feel like
[00:04:42.720 --> 00:04:46.200]   being getting rejected everywhere must be very heartbreaking.
[00:04:46.200 --> 00:04:49.840]   And that's what happened to me and feel like everything went wrong during that period of
[00:04:49.840 --> 00:04:50.840]   time.
[00:04:50.840 --> 00:04:56.080]   But really like changed is a conceptual change in my mind that I feel like I, instead of
[00:04:56.080 --> 00:05:00.760]   changing myself, because what this signal out there is telling me is that I'm not good
[00:05:00.760 --> 00:05:04.840]   enough to fit the hiring rubric of all those places.
[00:05:04.840 --> 00:05:09.480]   So instead of changing myself thinking I should be more and more like what they want, what
[00:05:09.480 --> 00:05:14.520]   I decided to do is just change the hiring system, have a whole system of my own where
[00:05:14.520 --> 00:05:21.760]   we get to hire people or recruit people differently from how they do it.
[00:05:21.760 --> 00:05:25.560]   How does your organization compare to academic organizations?
[00:05:25.560 --> 00:05:29.000]   Because most academic institutions would be non-profits too, right?
[00:05:29.000 --> 00:05:31.400]   Like what's different about what you're doing?
[00:05:31.400 --> 00:05:33.000]   Yeah, they're actually both.
[00:05:33.000 --> 00:05:37.240]   So academic can be profitable or non-profit.
[00:05:37.240 --> 00:05:39.200]   They can be public and private.
[00:05:39.200 --> 00:05:43.400]   So we are strictly non-profit, so we are funded by donations basically.
[00:05:43.400 --> 00:05:44.840]   When we are funded, we're not funded now.
[00:05:44.840 --> 00:05:48.440]   So once we're funded, we will be funded by donations.
[00:05:48.440 --> 00:05:51.520]   Difference was academic is not employment based.
[00:05:51.520 --> 00:05:54.760]   So everyone joined just as a member, they can have their own employment.
[00:05:54.760 --> 00:05:56.560]   That really gives people flexibility.
[00:05:56.560 --> 00:06:02.720]   They can view this more like a hobby, which we found is actually motivates people more
[00:06:02.720 --> 00:06:04.440]   than when they view this as a job.
[00:06:04.440 --> 00:06:08.280]   Like they have to report, they have to keep track on how much they're performing.
[00:06:08.280 --> 00:06:11.440]   They have to report to a manager, stuff like that.
[00:06:11.440 --> 00:06:16.800]   We function very much like academics because the key people in the organization are sort
[00:06:16.800 --> 00:06:17.800]   of like PhDs.
[00:06:17.800 --> 00:06:20.920]   So that's the only way we know how to run a lab.
[00:06:20.920 --> 00:06:24.960]   So we hold research meetings like a lab, everyone gives updates.
[00:06:24.960 --> 00:06:25.960]   There's no graduation.
[00:06:25.960 --> 00:06:27.480]   So that's one difference from academic.
[00:06:27.480 --> 00:06:31.800]   It's not like you have to join and then five or seven years later you graduate.
[00:06:31.800 --> 00:06:32.800]   It's really flexible.
[00:06:32.800 --> 00:06:39.520]   You get paid even less than academia, which is unfortunate, but until we got funded, we
[00:06:39.520 --> 00:06:40.720]   can adjust that.
[00:06:40.720 --> 00:06:43.480]   But now everything's volunteered.
[00:06:43.480 --> 00:06:50.720]   So what are some of the challenges in building a sustainable nonprofit like the one that
[00:06:50.720 --> 00:06:53.280]   you're building?
[00:06:53.280 --> 00:06:56.120]   And not just monetary, but other challenges too?
[00:06:56.120 --> 00:06:58.320]   I haven't made it sustainable yet.
[00:06:58.320 --> 00:07:02.200]   So I feel like I should be asked this question like three years later, like how did you make
[00:07:02.200 --> 00:07:05.000]   it sustainable or why did it fail?
[00:07:05.000 --> 00:07:10.220]   But I can see a little bit of a glimpse of hope there because we're starting to get donations
[00:07:10.220 --> 00:07:16.320]   and get funding even without going into a full donation raising.
[00:07:16.320 --> 00:07:22.920]   We're already getting interest from big companies and personnels that wanted to contribute to
[00:07:22.920 --> 00:07:24.240]   the organization.
[00:07:24.240 --> 00:07:26.400]   So I feel like it's a different ecosystem, right?
[00:07:26.400 --> 00:07:30.680]   Like any profitable world, they have their own ecosystem.
[00:07:30.680 --> 00:07:34.680]   Startups they raise donations, they raise investments and they promise that there's
[00:07:34.680 --> 00:07:37.560]   certain return many years later to give shares out.
[00:07:37.560 --> 00:07:41.400]   In this world, you raise donations because you're selling this concept and you feel this
[00:07:41.400 --> 00:07:43.480]   concept is so important.
[00:07:43.480 --> 00:07:47.760]   It's going to have such a social impact that the donors or philanthropists, they really
[00:07:47.760 --> 00:07:49.960]   buy into this idea.
[00:07:49.960 --> 00:07:51.600]   They want to do something back to the world.
[00:07:51.600 --> 00:07:54.760]   So there's a whole different ecosystem going on there.
[00:07:54.760 --> 00:07:58.320]   Many nonprofits, as far as I can see, survive in that ecosystem.
[00:07:58.320 --> 00:08:00.360]   So I feel like we probably can have a shot there.
[00:08:00.360 --> 00:08:03.120]   I haven't proved it yet, but that's my idea.
[00:08:03.120 --> 00:08:06.560]   Also a lot of donations don't come in monetary, as you said.
[00:08:06.560 --> 00:08:10.840]   A lot of people that are our current members, they're really just donating their time to
[00:08:10.840 --> 00:08:11.840]   work with us.
[00:08:11.840 --> 00:08:16.120]   They're helping others publish papers by donating their expertise at their time.
[00:08:16.120 --> 00:08:19.400]   That's actually way more valuable than money.
[00:08:19.400 --> 00:08:21.680]   And there are also donations of compute credits.
[00:08:21.680 --> 00:08:23.880]   AI research is expensive.
[00:08:23.880 --> 00:08:25.320]   You need all forms of donations.
[00:08:25.320 --> 00:08:30.040]   But when they all come together, I feel like there must be a model that is sustainable.
[00:08:30.040 --> 00:08:34.960]   I need to prove that, but I'm hopeful.
[00:08:34.960 --> 00:08:42.520]   What kinds of research are you doing that you think might not happen somewhere else?
[00:08:42.520 --> 00:08:43.840]   That's actually the best part.
[00:08:43.840 --> 00:08:46.960]   So as a nonprofit, you're not driven by goals or anything.
[00:08:46.960 --> 00:08:51.320]   It's not like you have to prove to someone that I have to make this object detection
[00:08:51.320 --> 00:08:53.800]   thing better than 98% or something.
[00:08:53.800 --> 00:08:55.320]   It's really just curiosity driven.
[00:08:55.320 --> 00:08:57.760]   So you can do anything that interests you.
[00:08:57.760 --> 00:09:02.480]   And also the management of ML Collective is not hierarchical.
[00:09:02.480 --> 00:09:05.360]   We don't have a central manager of any sort.
[00:09:05.360 --> 00:09:09.080]   You can start projects any way you want, and you will be the lead of the project as long
[00:09:09.080 --> 00:09:10.080]   as you're a member.
[00:09:10.080 --> 00:09:13.080]   So it's really driven by individual members' interest.
[00:09:13.080 --> 00:09:14.440]   So that's the best thing.
[00:09:14.440 --> 00:09:18.560]   If you come in from a physics background, you maybe have something to do thinking about
[00:09:18.560 --> 00:09:20.480]   how physics is related to neural networks.
[00:09:20.480 --> 00:09:22.200]   You can do a project with that.
[00:09:22.200 --> 00:09:23.640]   You can tell people about it.
[00:09:23.640 --> 00:09:25.120]   People who are interested will join the project.
[00:09:25.120 --> 00:09:26.760]   Then you form a little team of your own.
[00:09:26.760 --> 00:09:28.560]   You will be the lead of the team of that project.
[00:09:28.560 --> 00:09:29.960]   It puts you through.
[00:09:29.960 --> 00:09:33.200]   Maybe the next project, you want to join someone else's because you want to learn something.
[00:09:33.200 --> 00:09:36.360]   You want to learn, I don't know, how things work in brain.
[00:09:36.360 --> 00:09:40.320]   So you join a more neuroscience project where someone else is leading.
[00:09:40.320 --> 00:09:42.400]   You're more like a happy follower.
[00:09:42.400 --> 00:09:43.400]   So that would work out.
[00:09:43.400 --> 00:09:45.400]   So your role would be very dynamic.
[00:09:45.400 --> 00:09:49.600]   So the answer is we're not limited to any specific topics.
[00:09:49.600 --> 00:09:53.320]   It's really driven by individual members' interests.
[00:09:53.320 --> 00:09:56.480]   Could you tell us about some of the things you're working on?
[00:09:56.480 --> 00:09:58.040]   Yeah.
[00:09:58.040 --> 00:09:59.960]   I think most of the things are published.
[00:09:59.960 --> 00:10:01.720]   We put it on a website.
[00:10:01.720 --> 00:10:06.840]   So looking at my own profile, I feel like I've always been someone in ML, but just dabbled
[00:10:06.840 --> 00:10:07.840]   around different topics.
[00:10:07.840 --> 00:10:11.440]   Because I'm not as patient as most scientists.
[00:10:11.440 --> 00:10:13.320]   I just try different things.
[00:10:13.320 --> 00:10:16.360]   And also neural networks, the whole machine learning is changing so fast that I don't
[00:10:16.360 --> 00:10:22.320]   think anyone can confidently say that the next year, this is going to be the biggest
[00:10:22.320 --> 00:10:23.320]   thing.
[00:10:23.320 --> 00:10:27.720]   Because I feel like any breakthrough from any small field would become the biggest thing
[00:10:27.720 --> 00:10:30.600]   if more people spend time in it.
[00:10:30.600 --> 00:10:34.480]   So in the past, I've done vision projects, NLP.
[00:10:34.480 --> 00:10:37.200]   I recently had an NLP project.
[00:10:37.200 --> 00:10:42.000]   The very latest project is about continual learning, but that's also an interesting concept
[00:10:42.000 --> 00:10:43.040]   that I like.
[00:10:43.040 --> 00:10:44.800]   We have projects about network pruning.
[00:10:44.800 --> 00:10:49.440]   Basically, whatever is going on out there, we take a look, take the recent paper, look
[00:10:49.440 --> 00:10:54.440]   at their code and try to implement it, run it and find faults in it or find things that
[00:10:54.440 --> 00:10:56.880]   we didn't understand it and try to understand it.
[00:10:56.880 --> 00:11:02.320]   So I feel like a lot of our listeners might be thinking this sounds like a great idea
[00:11:02.320 --> 00:11:04.400]   and a great place to get involved in.
[00:11:04.400 --> 00:11:10.240]   Do you have thoughts on who is your ideal person to join ML Collective?
[00:11:10.240 --> 00:11:11.680]   Yeah.
[00:11:11.680 --> 00:11:16.800]   You can think of people as different categories, but of course, every single person is always
[00:11:16.800 --> 00:11:18.400]   a combination of different qualities.
[00:11:18.400 --> 00:11:23.560]   But you can think of people who are really the lead experts in a subfield.
[00:11:23.560 --> 00:11:27.880]   They really want to push that subfield forward, but they're lacking resources in terms of
[00:11:27.880 --> 00:11:28.880]   people.
[00:11:28.880 --> 00:11:34.120]   Maybe they are a really senior researcher, but their job doesn't give them reports.
[00:11:34.120 --> 00:11:38.680]   And for whatever reason, they're not managing people yet, but they want to have their ideas
[00:11:38.680 --> 00:11:40.080]   executed.
[00:11:40.080 --> 00:11:41.760]   They want to influence people.
[00:11:41.760 --> 00:11:45.480]   So that kind of people can join as a thought leader.
[00:11:45.480 --> 00:11:51.520]   They can lead a project and other people can join, work out research project that way.
[00:11:51.520 --> 00:11:57.880]   And there are people who are having free time and want to run code, want to follow all those
[00:11:57.880 --> 00:12:00.000]   projects led by those experts.
[00:12:00.000 --> 00:12:04.920]   Those people were also welcome, but we're mostly welcoming people that are not offered
[00:12:04.920 --> 00:12:07.860]   the opportunity at the big industrial labs.
[00:12:07.860 --> 00:12:10.880]   Because those people, if they can get into industrial lab, they probably wouldn't be
[00:12:10.880 --> 00:12:12.200]   interested in us anyways.
[00:12:12.200 --> 00:12:15.560]   But also they're not the people that we're trying to attract because all we wanted to
[00:12:15.560 --> 00:12:20.600]   do is to serve as a diversifier from the industrial labs, from academia.
[00:12:20.600 --> 00:12:24.040]   So the people that cannot be hired easily by them.
[00:12:24.040 --> 00:12:28.280]   So you can think of what kind of people they are, probably don't have a PhD.
[00:12:28.280 --> 00:12:32.760]   That's one of the biggest reasons that they don't have a resume that looks immediately
[00:12:32.760 --> 00:12:33.820]   hireable.
[00:12:33.820 --> 00:12:38.620]   They started coding since they are a teenager, but then they never really pursue a higher
[00:12:38.620 --> 00:12:39.620]   degree that way.
[00:12:39.620 --> 00:12:43.320]   But it doesn't say that they're not a good researcher at all.
[00:12:43.320 --> 00:12:45.440]   Probably people who are changing fields.
[00:12:45.440 --> 00:12:47.480]   They have a PhD, but it's something else.
[00:12:47.480 --> 00:12:52.160]   They were trying to get into ML and it's much, much harder for them to just immediately get
[00:12:52.160 --> 00:12:54.040]   a researcher's job in those labs.
[00:12:54.040 --> 00:12:55.880]   So we also accept those kinds of people.
[00:12:55.880 --> 00:12:56.880]   Yeah.
[00:12:56.880 --> 00:12:59.200]   So basically we serve, we'll try to serve as a diversifier.
[00:12:59.200 --> 00:13:04.540]   Anyone who is having difficulty getting into those places, but still want to do science,
[00:13:04.540 --> 00:13:08.540]   like if they were in those places, then we welcome those people.
[00:13:08.540 --> 00:13:11.140]   Can I ask another one before?
[00:13:11.140 --> 00:13:15.860]   So you talked about diversity, which we care about a lot as well.
[00:13:15.860 --> 00:13:19.140]   And I feel like every company says they care about diversity, right?
[00:13:19.140 --> 00:13:23.900]   But what's one concrete thing that doesn't require a lot of resources that any company
[00:13:23.900 --> 00:13:26.900]   can do to get more diverse talent?
[00:13:26.900 --> 00:13:29.620]   Yeah, that's a really good point.
[00:13:29.620 --> 00:13:33.580]   I think every company probably cares about, there are people in the company cares about
[00:13:33.580 --> 00:13:37.940]   diversity, but then when it comes to what their main goal is, because the company, if
[00:13:37.940 --> 00:13:41.660]   they're making profits, their main goal is still to make the company run.
[00:13:41.660 --> 00:13:46.060]   Diversity becomes the secondary thing or even third thing that they care about.
[00:13:46.060 --> 00:13:50.620]   And that's when things broke down, because if you're going really just for productivity,
[00:13:50.620 --> 00:13:54.960]   for the speed of producing the next paper, then you wouldn't care about diversity.
[00:13:54.960 --> 00:14:01.140]   You would just hire the person who can quickly, the fastest, produce a paper the fastest.
[00:14:01.140 --> 00:14:07.540]   So you really need organizations or institutions that put diversity as the first, you know,
[00:14:07.540 --> 00:14:11.480]   first word citizen, whatever, like that's our first goal.
[00:14:11.480 --> 00:14:15.800]   So nonprofits is sort of like the thing that I'm thinking about, because they are not going
[00:14:15.800 --> 00:14:18.580]   after profits, they're not going after productivity.
[00:14:18.580 --> 00:14:23.980]   They're not trying to submit to every conference because they want to show status.
[00:14:23.980 --> 00:14:27.460]   Their whole job is to help people to level the play field for people.
[00:14:27.460 --> 00:14:30.220]   Yeah, that's the way I'm thinking about it.
[00:14:30.220 --> 00:14:36.020]   Well, I guess I would love to hear more about some of the research that you're doing currently.
[00:14:36.020 --> 00:14:40.420]   I remember looking at your work on the loss change allocation to try to understand what
[00:14:40.420 --> 00:14:44.060]   neural networks are doing and thinking that was such a cool idea.
[00:14:44.060 --> 00:14:47.700]   I wonder if you've had any chance to follow up on that work.
[00:14:47.700 --> 00:14:48.700]   Yeah.
[00:14:48.700 --> 00:14:53.580]   I remember giving the talk at Weights and Biases and you asking great questions.
[00:14:53.580 --> 00:14:56.580]   Back then I didn't know that you were the running Weights and Biases.
[00:14:56.580 --> 00:15:00.020]   I was like, that person has great questions, but I didn't know that you're the founder
[00:15:00.020 --> 00:15:01.020]   and CEO.
[00:15:01.020 --> 00:15:02.020]   Oh, that's a touch.
[00:15:02.020 --> 00:15:03.020]   I appreciate that.
[00:15:03.020 --> 00:15:04.020]   Yeah.
[00:15:04.020 --> 00:15:05.020]   I was just impressed by your questions.
[00:15:05.020 --> 00:15:06.020]   Yes.
[00:15:06.020 --> 00:15:08.300]   So that work is LCA, loss change allocation.
[00:15:08.300 --> 00:15:11.460]   So that was published in Europe's 2019, a year ago.
[00:15:11.460 --> 00:15:12.460]   Can't believe that.
[00:15:12.460 --> 00:15:17.540]   It was led by Janice, our resident back then when we were in Uber.
[00:15:17.540 --> 00:15:23.380]   So basically the idea is that we can break down the loss change onto each parameters.
[00:15:23.380 --> 00:15:28.140]   So you can clearly visualize and see how much each parameter is contributing to the training
[00:15:28.140 --> 00:15:29.900]   of networks, just in the training sense.
[00:15:29.900 --> 00:15:33.700]   We're not talking about validation or generalization yet.
[00:15:33.700 --> 00:15:37.380]   And surprisingly, you see that half of the parameters hurt the training.
[00:15:37.380 --> 00:15:41.900]   Probably understandably because everything's so stochastic, we add noise into the process
[00:15:41.900 --> 00:15:44.780]   because we use stochastic gradient descent.
[00:15:44.780 --> 00:15:50.240]   We use many batches and we reduce the optimization to be linear.
[00:15:50.240 --> 00:15:54.300]   So all those are contributing to the noise in the process, but still the amount of noise
[00:15:54.300 --> 00:15:57.280]   in the training process was surprising to us.
[00:15:57.280 --> 00:16:01.980]   So the whole work was basically visualizing all those, how many parameters, what percentage
[00:16:01.980 --> 00:16:04.900]   of parameters was hurting and then break it down into each layers.
[00:16:04.900 --> 00:16:10.780]   And we found that some layers hurt more training than other layers, especially the last layer.
[00:16:10.780 --> 00:16:14.540]   So actually a very easy follow-up work would be, we proposed that the last layer should
[00:16:14.540 --> 00:16:17.040]   use a different momentum term.
[00:16:17.040 --> 00:16:20.540]   And we did a small experiment there and show that it improves.
[00:16:20.540 --> 00:16:25.460]   So I don't know if anyone from then on, like training networks were using a different momentum
[00:16:25.460 --> 00:16:28.740]   term for the last layer, but they should.
[00:16:28.740 --> 00:16:34.940]   And this is basically in every single step or is this over a larger period that you see
[00:16:34.940 --> 00:16:37.380]   half of the parameters hurting?
[00:16:37.380 --> 00:16:38.740]   Yeah.
[00:16:38.740 --> 00:16:43.660]   So at any given time, there's over half of the parameters that are hurting.
[00:16:43.660 --> 00:16:48.340]   And then across the whole training, half of the parameters hurt overall.
[00:16:48.340 --> 00:16:53.420]   If you accumulate all the contributions together, which when you add them together, it's the
[00:16:53.420 --> 00:16:58.260]   exact training loss from the beginning of training to the end of training.
[00:16:58.260 --> 00:17:01.860]   So any moment there's half of parameters hurting and then throughout the whole training, it's
[00:17:01.860 --> 00:17:03.500]   also over half.
[00:17:03.500 --> 00:17:09.920]   And also if you track one simple parameter, the thing is that it hurts half of the time.
[00:17:09.920 --> 00:17:15.060]   So it's not really like if you can catch this criminal and then just ban it from making
[00:17:15.060 --> 00:17:19.900]   changes to the loss, because they also jump around the hurting camp and the helping camp.
[00:17:19.900 --> 00:17:22.620]   I mean, it doesn't seem surprising that some of...
[00:17:22.620 --> 00:17:27.140]   And by hurting, you mean the parameters, they change in a way that makes the loss worse,
[00:17:27.140 --> 00:17:28.140]   right?
[00:17:28.140 --> 00:17:29.140]   Do I have that right?
[00:17:29.140 --> 00:17:30.140]   Yes.
[00:17:30.140 --> 00:17:31.780]   It makes the loss go higher.
[00:17:31.780 --> 00:17:37.420]   And so it's funny, it makes sense that in the stochastic process, some would be making
[00:17:37.420 --> 00:17:38.420]   it worse.
[00:17:38.420 --> 00:17:39.420]   But it's not surprising that half, right?
[00:17:39.420 --> 00:17:42.480]   Because overall the loss does improve over the steps.
[00:17:42.480 --> 00:17:46.120]   So what's going on there?
[00:17:46.120 --> 00:17:50.480]   Things that we didn't understand, like many things in neural networks that we didn't...
[00:17:50.480 --> 00:17:54.960]   We sort of get the idea, but then until we see the data, we're like, "Yeah, that makes
[00:17:54.960 --> 00:17:56.400]   sense, but still it's surprising."
[00:17:56.400 --> 00:17:58.560]   I feel like many papers are like that.
[00:17:58.560 --> 00:18:03.440]   And those are the papers that I aspire to write, like papers that you sort of have intuitive
[00:18:03.440 --> 00:18:06.400]   sense that this is something that's going on.
[00:18:06.400 --> 00:18:11.120]   But until you see the data, you're still surprised by the amount of it or the actual extent of
[00:18:11.120 --> 00:18:12.120]   it.
[00:18:12.120 --> 00:18:14.560]   So yeah, I don't think we understand.
[00:18:14.560 --> 00:18:15.560]   That's so cool.
[00:18:15.560 --> 00:18:19.360]   I wish you had a chance to follow up on that.
[00:18:19.360 --> 00:18:23.120]   Running an organization, do you find yourself...
[00:18:23.120 --> 00:18:26.740]   Maybe this is asking for a friend kind of question, but do you find yourself spending
[00:18:26.740 --> 00:18:30.620]   a lot of your time on more kind of administrative tasks and recruiting and things like that
[00:18:30.620 --> 00:18:32.760]   than actually doing research?
[00:18:32.760 --> 00:18:34.680]   Yeah, exactly.
[00:18:34.680 --> 00:18:40.080]   I started doubt whether I'm still a researcher because every day I look at my time, I'm spending
[00:18:40.080 --> 00:18:44.720]   half a day designing a logo because we need to have a logo and just like, "No one's working
[00:18:44.720 --> 00:18:45.720]   on it."
[00:18:45.720 --> 00:18:49.600]   And then the other day, because we're organizing an event, I'm just spending all my night designing
[00:18:49.600 --> 00:18:51.360]   a gather.town layout.
[00:18:51.360 --> 00:18:55.200]   I'm making houses, making rooms, make sure people can go to different rooms and things.
[00:18:55.200 --> 00:18:59.800]   Yeah, there's so many administrative things, but that's also one of my goals.
[00:18:59.800 --> 00:19:04.600]   I feel like, honestly, the next 10 years, I feel like publishing papers wouldn't give
[00:19:04.600 --> 00:19:09.800]   you so much value as before because there are so many people trying to publish papers.
[00:19:09.800 --> 00:19:14.760]   What would give me more reward is actually helping people publish papers.
[00:19:14.760 --> 00:19:18.840]   And a concrete goal actually of mine is just end up in people's papers acknowledgement
[00:19:18.840 --> 00:19:19.840]   section.
[00:19:19.840 --> 00:19:20.840]   That's all my goal.
[00:19:20.840 --> 00:19:23.760]   I'm not trying to be a co-author anymore.
[00:19:23.760 --> 00:19:26.680]   Just because, I don't know, I don't think it's a field that I still want to...
[00:19:26.680 --> 00:19:28.760]   I want to be close to, of course, publishing.
[00:19:28.760 --> 00:19:33.560]   I want to publish as much as I can, but I also want to remind everyone that the publishing
[00:19:33.560 --> 00:19:38.160]   scene is going to be very different the next decade, just because you see this huge influx
[00:19:38.160 --> 00:19:40.960]   of people coming in, trying to publish papers.
[00:19:40.960 --> 00:19:44.320]   Almost every idea has been chewed over a thousand times.
[00:19:44.320 --> 00:19:47.740]   It's so hard to come up with an idea and then social researcher find out, "Oh, no one has
[00:19:47.740 --> 00:19:48.740]   ever done that.
[00:19:48.740 --> 00:19:49.800]   It's impossible.
[00:19:49.800 --> 00:19:52.240]   Someone is doing that somewhere."
[00:19:52.240 --> 00:19:55.880]   Which is to say that researchers right now, ML researchers right now are having a hard
[00:19:55.880 --> 00:20:03.200]   time, if I can help them a little bit, help their paper improve or be different and make
[00:20:03.200 --> 00:20:07.560]   their success rate of their paper getting noticed or published slightly higher, then
[00:20:07.560 --> 00:20:09.400]   I'll be very happy.
[00:20:09.400 --> 00:20:13.720]   I guess what general advice would you have then to someone that's trying to get something
[00:20:13.720 --> 00:20:14.720]   published?
[00:20:14.720 --> 00:20:20.960]   What are the mistakes that you see first time people or outsiders make and what kind of
[00:20:20.960 --> 00:20:25.200]   help do you typically give to someone?
[00:20:25.200 --> 00:20:29.520]   I feel like the thing is that our reward function is delayed.
[00:20:29.520 --> 00:20:35.720]   We go into ML research liking it because we saw other people in research maybe a few years
[00:20:35.720 --> 00:20:39.000]   before us and they gained reward out of that.
[00:20:39.000 --> 00:20:43.120]   They published a paper and it was so recognized and they have such a fame and recognition
[00:20:43.120 --> 00:20:44.120]   and everything.
[00:20:44.120 --> 00:20:48.760]   We want to do the same thing, but the difference is we live in a delayed timeline.
[00:20:48.760 --> 00:20:52.960]   When we get into it, the scene already changed, but we don't know.
[00:20:52.960 --> 00:20:57.160]   I really want to remind everyone that if you're getting into ML research now, publishing is
[00:20:57.160 --> 00:20:59.040]   very different than before.
[00:20:59.040 --> 00:21:05.160]   Before, if you have an accepted paper at, I don't know, ICLR or NeurIPS or CPR, you're
[00:21:05.160 --> 00:21:07.120]   basically there.
[00:21:07.120 --> 00:21:12.760]   You can probably get a job that you would want, get a dream job, get a position of something,
[00:21:12.760 --> 00:21:13.760]   but not anymore.
[00:21:13.760 --> 00:21:16.800]   Now, I think the next people will be looking at citations.
[00:21:16.800 --> 00:21:21.120]   Even if you get a lot of paper published in peer review conferences, people will look
[00:21:21.120 --> 00:21:25.240]   at different metrics now because there are so many papers getting in and so many people
[00:21:25.240 --> 00:21:28.120]   having their papers getting in.
[00:21:28.120 --> 00:21:35.120]   The basic suggestion or advice is that you should try to adjust your reward system to
[00:21:35.120 --> 00:21:38.400]   be different from what you came in the...
[00:21:38.400 --> 00:21:39.400]   Should you adjust it too?
[00:21:39.400 --> 00:21:40.400]   I mean, you're saying...
[00:21:40.400 --> 00:21:45.000]   I mean, it just seems like you should just make things even harder for yourself.
[00:21:45.000 --> 00:21:48.480]   You can't just publish a paper and have to get citations.
[00:21:48.480 --> 00:21:49.480]   Is that a good summary?
[00:21:49.480 --> 00:21:54.000]   Yeah, so no, no, that's why you should be looking at other things.
[00:21:54.000 --> 00:21:56.040]   You should be really just looking at the love of science.
[00:21:56.040 --> 00:21:57.560]   I want to do this for the love of science.
[00:21:57.560 --> 00:21:58.560]   I'm not trying to...
[00:21:58.560 --> 00:21:59.760]   I do this piece of work not to...
[00:21:59.760 --> 00:22:04.560]   Well, if it gets published, that's like a confirmation that it's a good science, but
[00:22:04.560 --> 00:22:07.120]   the basic thing that's important is that it's a good piece of science.
[00:22:07.120 --> 00:22:09.200]   I think that's what I want to say.
[00:22:09.200 --> 00:22:11.600]   You can do a beautiful work, put it in an archive.
[00:22:11.600 --> 00:22:14.440]   Don't worry about whether it gets accepted or not because there are so many noise in
[00:22:14.440 --> 00:22:17.640]   that whole thing, the same as neural network training.
[00:22:17.640 --> 00:22:20.920]   There are so many statistics that the same paper, there's no change, just so many two
[00:22:20.920 --> 00:22:25.160]   or three conferences get rejected, rejected, accepted because it's just random chance.
[00:22:25.160 --> 00:22:28.640]   Every time you're just drawing a lottery ticket of some sort.
[00:22:28.640 --> 00:22:29.640]   So don't care about that.
[00:22:29.640 --> 00:22:34.200]   Don't care about really the true acceptance or not into a conference.
[00:22:34.200 --> 00:22:37.520]   Really care about the quality of the science you put out there because if it's on archive,
[00:22:37.520 --> 00:22:40.040]   you have your name on it, it's going to...
[00:22:40.040 --> 00:22:41.120]   That means something.
[00:22:41.120 --> 00:22:45.560]   So change your reward system to really care about the true quality of science and remind
[00:22:45.560 --> 00:22:48.680]   yourself that you're in here for the love of science, not for...
[00:22:48.680 --> 00:22:52.680]   Of course, some people are in here for it too, so that it promises a better future and
[00:22:52.680 --> 00:22:54.160]   there's nothing wrong with that.
[00:22:54.160 --> 00:23:01.080]   But those will probably stray you a little bit away from the path and maybe make you
[00:23:01.080 --> 00:23:02.080]   a little bit miserable.
[00:23:02.080 --> 00:23:06.840]   So what's the key to doing good science as an outsider?
[00:23:06.840 --> 00:23:09.840]   How do you do that?
[00:23:09.840 --> 00:23:12.480]   Yeah, that's actually the idea of running ML Collective.
[00:23:12.480 --> 00:23:17.480]   I feel like there's so many problems these days in the world that people don't believe
[00:23:17.480 --> 00:23:18.640]   in science.
[00:23:18.640 --> 00:23:22.720]   I'm not saying ML Collective is the way to change that, but I sometimes think if you
[00:23:22.720 --> 00:23:28.120]   can get everyone, not even everyone, the majority of American to publish one paper in their
[00:23:28.120 --> 00:23:30.640]   life, maybe they'll just believe in science more.
[00:23:30.640 --> 00:23:36.080]   Once they go through that publication process, they see like, "Oh, to put this statement
[00:23:36.080 --> 00:23:42.040]   out, I need to try everything around it, do ablation study, compare with all the benchmarks."
[00:23:42.040 --> 00:23:44.840]   So they will become more careful when they put statements out.
[00:23:44.840 --> 00:23:45.840]   I don't know.
[00:23:45.840 --> 00:23:51.000]   This is a weird argument I'm making, but I feel like if I can get more people to do science,
[00:23:51.000 --> 00:23:55.760]   not for life, just publish one paper in their life, I think everyone's attitude towards
[00:23:55.760 --> 00:23:57.320]   science will be better.
[00:23:57.320 --> 00:23:58.320]   They will believe it more.
[00:23:58.320 --> 00:24:02.400]   We probably wouldn't have all those problems out there in America that people don't believe
[00:24:02.400 --> 00:24:03.760]   in science and all the things.
[00:24:03.760 --> 00:24:04.760]   I don't know.
[00:24:04.760 --> 00:24:05.760]   That's my dream, of course.
[00:24:05.760 --> 00:24:06.760]   That's a great idea.
[00:24:06.760 --> 00:24:12.400]   Also, I want to address the other end of the spectrum, which is all of these people who
[00:24:12.400 --> 00:24:15.760]   are trying to keep up with all of the papers that are coming out.
[00:24:15.760 --> 00:24:19.560]   And maybe you can use this opportunity to talk about this amazing paper reading group
[00:24:19.560 --> 00:24:21.840]   that you've been doing for like three years now.
[00:24:21.840 --> 00:24:25.840]   So what's your advice for people who want to keep up and what kinds of papers should
[00:24:25.840 --> 00:24:29.240]   they pick and how should they go about reading them?
[00:24:29.240 --> 00:24:34.320]   Yeah, there's no better way actually, because I think this is like our first time of facing
[00:24:34.320 --> 00:24:35.320]   this problem.
[00:24:35.320 --> 00:24:39.720]   So there's no historic lessons that we can learn from it, that this like huge influx
[00:24:39.720 --> 00:24:41.120]   of paper.
[00:24:41.120 --> 00:24:46.160]   For now, I still trust those that are published at peer review conferences, but we know that
[00:24:46.160 --> 00:24:50.200]   there's a lot of noises in there, but I trust it slightly more than papers that just put
[00:24:50.200 --> 00:24:51.280]   on archive.
[00:24:51.280 --> 00:24:53.320]   I sort of have like a general sense.
[00:24:53.320 --> 00:24:58.600]   So there are many people like me out there running paper clubs or YouTube channels, like
[00:24:58.600 --> 00:24:59.600]   they dissect papers.
[00:24:59.600 --> 00:25:04.380]   Each of them, of course, has their own criteria in judging papers, but if you accumulate more
[00:25:04.380 --> 00:25:09.640]   of them, sort of like the average out to, I think is representative of the overall quality.
[00:25:09.640 --> 00:25:11.560]   Yeah, I think like a shameless plug.
[00:25:11.560 --> 00:25:16.440]   I think I'm not, I think I'm, by now I'm a good discriminator in all the subfields of
[00:25:16.440 --> 00:25:17.440]   ML.
[00:25:17.440 --> 00:25:20.560]   By being a good discriminator, I mean like I can sort of judge like what's a good paper,
[00:25:20.560 --> 00:25:21.560]   what is bad.
[00:25:21.560 --> 00:25:26.480]   I might not be a good generator in all those subfields that I never published in, but being
[00:25:26.480 --> 00:25:30.320]   a good discriminator is the first step of feeling like I can run this thing.
[00:25:30.320 --> 00:25:35.020]   So you can sort of trust the papers that I selected, but then you have to remember to
[00:25:35.020 --> 00:25:37.580]   accumulate it with all the other people's selection.
[00:25:37.580 --> 00:25:40.340]   So to get a more balanced view.
[00:25:40.340 --> 00:25:44.560]   Could you give us a little window into your process for being a good discriminator of
[00:25:44.560 --> 00:25:46.700]   high quality papers?
[00:25:46.700 --> 00:25:49.700]   Yeah, I just read a lot.
[00:25:49.700 --> 00:25:54.100]   Some basic elements, I feel like a lot of papers are missing.
[00:25:54.100 --> 00:25:58.300]   Maybe they're the people that are coming into ML research from different fields or from
[00:25:58.300 --> 00:26:04.660]   a non-researcher's background, which is again, why I feel like ML collective is important.
[00:26:04.660 --> 00:26:08.420]   Get people into this paper publishing process and tell them what are the basic things you
[00:26:08.420 --> 00:26:09.420]   have to do.
[00:26:09.420 --> 00:26:11.260]   Compare with baselines out there.
[00:26:11.260 --> 00:26:14.460]   Try different variations of the method that you're proposing.
[00:26:14.460 --> 00:26:16.300]   And that's ablation study.
[00:26:16.300 --> 00:26:20.180]   I see so many papers out there that have a huge diagram, right?
[00:26:20.180 --> 00:26:23.860]   Signal goes in and then there's so many branches of things and they branch out and then this
[00:26:23.860 --> 00:26:24.860]   is the end result.
[00:26:24.860 --> 00:26:29.180]   And you say that this whole system works much better than existing systems, but that's not
[00:26:29.180 --> 00:26:30.180]   science.
[00:26:30.180 --> 00:26:31.180]   That's a good engineering.
[00:26:31.180 --> 00:26:35.460]   Great that you made it work, but what does it teach us?
[00:26:35.460 --> 00:26:37.420]   Is this branch more important than this branch?
[00:26:37.420 --> 00:26:40.660]   Why did you branch out this way other than that way?
[00:26:40.660 --> 00:26:46.620]   It's a real good science work should be, I think, inspirational rather than intimidating.
[00:26:46.620 --> 00:26:48.940]   So that huge diagram, it just intimidates.
[00:26:48.940 --> 00:26:50.380]   I built this huge thing.
[00:26:50.380 --> 00:26:51.380]   It worked.
[00:26:51.380 --> 00:26:55.300]   I'm not going to tell you how because I hacked them together and it worked.
[00:26:55.300 --> 00:26:58.540]   Maybe there's scientific value in it, but to be a good scientific article, you have
[00:26:58.540 --> 00:27:00.660]   to tell us what things you have tried.
[00:27:00.660 --> 00:27:01.980]   Why this branch other than that branch?
[00:27:01.980 --> 00:27:03.180]   Did you do ablation study?
[00:27:03.180 --> 00:27:04.900]   Did you try to turn off this branch?
[00:27:04.900 --> 00:27:06.900]   What was the thought process behind it?
[00:27:06.900 --> 00:27:11.820]   How does your work inspire other work, maybe in different fields to borrow the same thought
[00:27:11.820 --> 00:27:17.420]   process to produce their science in their subfield?
[00:27:17.420 --> 00:27:24.500]   Do you have a favorite paper over the last few years that exemplifies this, a simple
[00:27:24.500 --> 00:27:26.820]   difference and then a clear insight?
[00:27:26.820 --> 00:27:29.420]   Yeah, there are many amazing papers out there.
[00:27:29.420 --> 00:27:31.980]   Am I allowed to say my own?
[00:27:31.980 --> 00:27:32.980]   Absolutely.
[00:27:32.980 --> 00:27:33.980]   Absolutely.
[00:27:33.980 --> 00:27:34.980]   Yeah, tell us.
[00:27:34.980 --> 00:27:37.940]   Which is the paper that you're most proud of?
[00:27:37.940 --> 00:27:41.900]   I actually really like an early paper of ours called Intrinsic Dimension.
[00:27:41.900 --> 00:27:43.700]   It's many years ago.
[00:27:43.700 --> 00:27:47.460]   Not many, many, but in machine learning, it feels like many years ago.
[00:27:47.460 --> 00:27:53.540]   It was published in Europe, sorry, ICLR 2018.
[00:27:53.540 --> 00:27:56.940]   So Intrinsic Dimension is basically you take all the parameters.
[00:27:56.940 --> 00:27:59.820]   I know, but it feels like forever, right?
[00:27:59.820 --> 00:28:00.820]   That's amazing.
[00:28:00.820 --> 00:28:04.620]   That's nowadays when you look at papers, you're like, this is 2018, probably there are better
[00:28:04.620 --> 00:28:05.620]   words than this.
[00:28:05.620 --> 00:28:08.020]   So probably I shouldn't be reading this paper at all.
[00:28:08.020 --> 00:28:10.220]   But yeah, it's only, wait, is that?
[00:28:10.220 --> 00:28:12.820]   Yeah, I think that's only two years ago.
[00:28:12.820 --> 00:28:17.780]   So that paper has to do with measuring this basic property of a neural network.
[00:28:17.780 --> 00:28:21.620]   So neural network has so many things associated with it.
[00:28:21.620 --> 00:28:25.100]   There's parameters, there's like large parameter counts.
[00:28:25.100 --> 00:28:29.180]   So if you imagine putting every parameter just together into a big vector, it's just
[00:28:29.180 --> 00:28:31.340]   a super long, long vector.
[00:28:31.340 --> 00:28:35.540]   And then you reduce it to a shorter vector, and you only train the shorter vector.
[00:28:35.540 --> 00:28:37.380]   And how do you map from the shorter to the bigger?
[00:28:37.380 --> 00:28:38.380]   It's just through a matrix.
[00:28:38.380 --> 00:28:42.420]   It's a linear mapping back to the big vector.
[00:28:42.420 --> 00:28:46.700]   So basically you're saying that even though this network has 10 million parameters, maybe
[00:28:46.700 --> 00:28:51.180]   the dimensions that you can make changes is much, much smaller than that big number.
[00:28:51.180 --> 00:28:55.540]   And there's a number out there that's much, much smaller that says something about your
[00:28:55.540 --> 00:28:58.580]   network combined with your problem, combined with your data.
[00:28:58.580 --> 00:29:03.260]   That's how easy or hard this network combined with data and problem is.
[00:29:03.260 --> 00:29:05.460]   So that becomes a massive benefit.
[00:29:05.460 --> 00:29:10.380]   So you could actually do that kind of, because that's going to be a lossy compression, right?
[00:29:10.380 --> 00:29:14.740]   You can actually do that, make it much smaller without hurting the performance?
[00:29:14.740 --> 00:29:18.900]   Well, I think now it becomes not surprising because now you can prune.
[00:29:18.900 --> 00:29:19.900]   So pruning is like that.
[00:29:19.900 --> 00:29:22.900]   It's an axis aligned reduction, right?
[00:29:22.900 --> 00:29:28.580]   You reduce big vector to a smaller one by basically masking some of them as zero.
[00:29:28.580 --> 00:29:31.420]   But back then we were just doing a linear projection.
[00:29:31.420 --> 00:29:36.420]   Yeah, you can totally do it because a lot of parameters in your networks are redundant.
[00:29:36.420 --> 00:29:37.420]   Not that they're not useful.
[00:29:37.420 --> 00:29:40.100]   LCA also teaches us that.
[00:29:40.100 --> 00:29:41.860]   Not that they're not useful.
[00:29:41.860 --> 00:29:47.820]   They provide a better or different loss landscape for you to train, but you can definitely train
[00:29:47.820 --> 00:29:49.860]   it within a much smaller landscape.
[00:29:49.860 --> 00:29:55.340]   Well, if you think about it, this huge landscape that all the parameters help construct leads
[00:29:55.340 --> 00:29:57.700]   you to an end point where there's better loss.
[00:29:57.700 --> 00:30:02.460]   If you can draw a line from the starting point to the end point, that's just one dimension.
[00:30:02.460 --> 00:30:06.660]   If you can just travel along that line, that's an intrinsic dimension of one.
[00:30:06.660 --> 00:30:11.660]   So any network would have a dimension of one that is trainable, but that one is very hard
[00:30:11.660 --> 00:30:12.660]   to find.
[00:30:12.660 --> 00:30:14.620]   That's almost just very singular.
[00:30:14.620 --> 00:30:19.020]   So this is intrinsic dimension saying this amount of dimension, however you draw the
[00:30:19.020 --> 00:30:23.900]   line or the plane, it should still lead you to a good enough solution.
[00:30:23.900 --> 00:30:26.420]   Wait, but how could you take a...
[00:30:26.420 --> 00:30:32.620]   Oh, because you can pick the linear function that goes from your simplified representation
[00:30:32.620 --> 00:30:34.420]   to the more complicated representation.
[00:30:34.420 --> 00:30:35.420]   Yes.
[00:30:35.420 --> 00:30:39.540]   No, so the thing is, if you were allowed to pick the linear function, you can reduce the
[00:30:39.540 --> 00:30:42.180]   dimension to however you want, all the way down to one.
[00:30:42.180 --> 00:30:45.700]   But that's not what we want to measure because that's just one for every network.
[00:30:45.700 --> 00:30:46.740]   What does that tell us?
[00:30:46.740 --> 00:30:51.900]   So the things we want to make the projection matrix randomized, so then we measure how
[00:30:51.900 --> 00:30:52.900]   big it is.
[00:30:52.900 --> 00:30:56.580]   Because you know that in a very, very lucky scenario, this can be down to one.
[00:30:56.580 --> 00:30:59.980]   With that knowledge, you should know that by just randomizing, there should be a number
[00:30:59.980 --> 00:31:07.940]   that's larger than one, but should be smaller than the super big vectors to start with.
[00:31:07.940 --> 00:31:09.100]   I see.
[00:31:09.100 --> 00:31:10.580]   And so how much smaller can you go?
[00:31:10.580 --> 00:31:13.660]   And is it like suddenly there's a drop off at a certain size or is it sort of like a
[00:31:13.660 --> 00:31:17.020]   smooth deterioration of performance?
[00:31:17.020 --> 00:31:18.260]   Yeah.
[00:31:18.260 --> 00:31:19.300]   Yeah.
[00:31:19.300 --> 00:31:21.560]   So back then, basically you have to try every number.
[00:31:21.560 --> 00:31:23.580]   So that's more of a science investigation.
[00:31:23.580 --> 00:31:27.060]   It's not something that can help you train the network faster because basically you have
[00:31:27.060 --> 00:31:32.420]   to try every number from big to small, to small to big until it crosses the threshold,
[00:31:32.420 --> 00:31:33.500]   whatever threshold you want to be.
[00:31:33.500 --> 00:31:38.860]   So we pick a threshold that is 90% of the full network performance, or you can do it
[00:31:38.860 --> 00:31:41.260]   99, you can do it 85, it's up to you.
[00:31:41.260 --> 00:31:42.540]   So pick that number.
[00:31:42.540 --> 00:31:43.540]   And that number is interesting.
[00:31:43.540 --> 00:31:46.380]   I want to make sure that I remember the numbers right, probably I wouldn't.
[00:31:46.380 --> 00:31:51.700]   But for MNIST plus FC network, I think it's 750.
[00:31:51.700 --> 00:31:57.180]   It's much lower than 784, which is just the input dimension of MNIST digits, which makes
[00:31:57.180 --> 00:32:01.300]   sense because there are many black pixels in MNIST input digits.
[00:32:01.300 --> 00:32:03.140]   But that number is very interesting.
[00:32:03.140 --> 00:32:06.220]   And then for CIFAR, I think it's like 19,000.
[00:32:06.220 --> 00:32:11.620]   So that sort of gave you a sense, oh, CIFAR is harder than MNIST, but how much harder?
[00:32:11.620 --> 00:32:12.620]   Probably 10 times harder.
[00:32:12.620 --> 00:32:13.620]   But isn't this also-
[00:32:13.620 --> 00:32:14.620]   Sorry, 19,000 is probably the-
[00:32:14.620 --> 00:32:21.060]   Sorry, you're modifying the network or are you modifying the input?
[00:32:21.060 --> 00:32:24.220]   You're modifying the training procedure.
[00:32:24.220 --> 00:32:29.820]   So once you pick a network, you pick a task, data is there, network is there, initialization
[00:32:29.820 --> 00:32:32.920]   is there, then your loss landscape is fixed.
[00:32:32.920 --> 00:32:38.380]   Now you're modifying the training procedure to let the point move not in any direction,
[00:32:38.380 --> 00:32:39.380]   but in the restricted plane.
[00:32:39.380 --> 00:32:41.120]   You can think of it that way.
[00:32:41.120 --> 00:32:42.980]   So you're modifying the training procedure.
[00:32:42.980 --> 00:32:47.440]   And the training procedure means you're first modifying the input data, sort of shrinking
[00:32:47.440 --> 00:32:50.140]   it before you put it into the network?
[00:32:50.140 --> 00:32:56.220]   Or you're only allowed, I see you're only allowed to change these smaller set of numbers
[00:32:56.220 --> 00:33:01.340]   and that changes the network through a linear transformation, that changes the parameters.
[00:33:01.340 --> 00:33:06.580]   So then how can you say MNIST and CIFAR, wouldn't it matter also the network that was being
[00:33:06.580 --> 00:33:07.580]   used?
[00:33:07.580 --> 00:33:09.980]   Wouldn't a bigger network maybe have a different-
[00:33:09.980 --> 00:33:10.980]   Exactly.
[00:33:10.980 --> 00:33:14.040]   That's very true.
[00:33:14.040 --> 00:33:19.100]   But what we found interestingly is that for at least in the scale of our experiments,
[00:33:19.100 --> 00:33:24.460]   MNIST plus FC network, fully connected network, you can make the network bigger, wider.
[00:33:24.460 --> 00:33:25.680]   The number is roughly the same.
[00:33:25.680 --> 00:33:30.020]   So 750 was the number we got from MNIST plus FC type of network.
[00:33:30.020 --> 00:33:32.880]   Of course, if you make it huge, probably the number will change.
[00:33:32.880 --> 00:33:38.140]   But to the extent that we vary the size, they're sort of stable, which gives us confidence
[00:33:38.140 --> 00:33:40.200]   that it is a stable measure.
[00:33:40.200 --> 00:33:43.740]   But then if you change your convolution, it changes drastically, it reduces, as you can
[00:33:43.740 --> 00:33:48.300]   imagine, because convolution is a much better landscape, it gives you a much lower intrinsic
[00:33:48.300 --> 00:33:49.300]   dimension.
[00:33:49.300 --> 00:33:53.700]   So it's the same story when you switch to CIFAR, when you switch to other tasks.
[00:33:53.700 --> 00:33:55.580]   You can also do RL with it.
[00:33:55.580 --> 00:33:56.860]   So that's the interesting part.
[00:33:56.860 --> 00:34:03.180]   You can finally compare RL tasks with computer vision tasks, which people never really do
[00:34:03.180 --> 00:34:09.140]   because people doing RL sort of know that, okay, this Pong is harder than some other
[00:34:09.140 --> 00:34:12.980]   game that I don't really do RL, so I don't know.
[00:34:12.980 --> 00:34:17.740]   And people doing vision knowing that MNIST is easy, CIFAR is harder, sorry, ImageNet
[00:34:17.740 --> 00:34:20.980]   is much harder, but then they don't make this parallel comparison.
[00:34:20.980 --> 00:34:21.980]   But now we can.
[00:34:21.980 --> 00:34:25.860]   Of course, it's not a very strict comparison because they are using different networks,
[00:34:25.860 --> 00:34:29.740]   but we find that some games are much easier than you thought.
[00:34:29.740 --> 00:34:34.140]   This carpool game has only a dimension of four because probably you just need to move
[00:34:34.140 --> 00:34:37.020]   in four dimensions.
[00:34:37.020 --> 00:34:39.740]   Even though, what are the inputs into the carpool game?
[00:34:39.740 --> 00:34:40.740]   There's not that many inputs, right?
[00:34:40.740 --> 00:34:41.740]   It's just the angle of the...
[00:34:41.740 --> 00:34:42.740]   Pixels.
[00:34:42.740 --> 00:34:43.740]   Oh, it's pixels.
[00:34:43.740 --> 00:34:44.740]   Oh, I see.
[00:34:44.740 --> 00:34:45.740]   From the pixels, you can see that.
[00:34:45.740 --> 00:34:46.740]   Yeah, they're actually pixels.
[00:34:46.740 --> 00:34:47.740]   Oh, that's cool.
[00:34:47.740 --> 00:34:48.740]   Yeah.
[00:34:48.740 --> 00:34:49.740]   Oh, that's so cool.
[00:34:49.740 --> 00:34:56.060]   Yeah, but it's an old paper from two years ago.
[00:34:56.060 --> 00:34:57.060]   Two and a half years ago.
[00:34:57.060 --> 00:35:04.860]   A decade ago, it sounds like, from the way you talk about it, how much time has passed.
[00:35:04.860 --> 00:35:08.020]   What are some of the practical applications of this?
[00:35:08.020 --> 00:35:13.620]   Could people use this to maybe take their networks and deploy them on mobile phones
[00:35:13.620 --> 00:35:16.620]   and other applications like that?
[00:35:16.620 --> 00:35:17.620]   Yeah, yeah.
[00:35:17.620 --> 00:35:18.620]   It's a very interesting question.
[00:35:18.620 --> 00:35:22.780]   So back then when we were doing it, we sort of claimed in the paper that this is a scientific
[00:35:22.780 --> 00:35:26.860]   investigation, but there are some implications of reduction.
[00:35:26.860 --> 00:35:34.100]   The whole matrix is randomized, so you can just save one random seed to regenerate that
[00:35:34.100 --> 00:35:35.500]   whole matrix.
[00:35:35.500 --> 00:35:42.140]   And then you train in such a small dimension, so the whole memory usage is much smaller.
[00:35:42.140 --> 00:35:46.980]   But actually speaking of this year, NeurIPS, which is coming up next week, there's a paper
[00:35:46.980 --> 00:35:51.580]   published there that actually took the idea that we had two years ago, a long time ago.
[00:35:51.580 --> 00:35:53.180]   They actually make it more useful.
[00:35:53.180 --> 00:35:54.660]   I think it was their method.
[00:35:54.660 --> 00:35:56.900]   They make a few tricks in the algorithm.
[00:35:56.900 --> 00:36:02.340]   It's no longer measuring this intrinsic property of a network anymore, but it becomes a better
[00:36:02.340 --> 00:36:07.420]   training method that they're able to train in such a sub-dimension, better networks,
[00:36:07.420 --> 00:36:11.940]   or faster, or with all those memory saved.
[00:36:11.940 --> 00:36:15.060]   So yeah, check out that paper and this year's NeurIPS.
[00:36:15.060 --> 00:36:16.060]   Okay, cool.
[00:36:16.060 --> 00:36:17.060]   Yeah, we should put notes.
[00:36:17.060 --> 00:36:21.340]   We should put in the show notes, both of these papers.
[00:36:21.340 --> 00:36:25.660]   Random subspace training, something like that.
[00:36:25.660 --> 00:36:29.540]   And I guess you're also doing something at NeurIPS this year on open collaboration.
[00:36:29.540 --> 00:36:30.540]   Is that right?
[00:36:30.540 --> 00:36:33.220]   Can you say a little bit about what you're trying to do there?
[00:36:33.220 --> 00:36:34.220]   Yeah.
[00:36:34.220 --> 00:36:35.980]   So that's the whole thing with ML Collective.
[00:36:35.980 --> 00:36:38.980]   If ML Collective is about one thing, it's about open collaboration.
[00:36:38.980 --> 00:36:43.940]   We want people to think that science is not...
[00:36:43.940 --> 00:36:46.420]   Science can be associated with employment.
[00:36:46.420 --> 00:36:50.620]   You join a job, you do science, but science can also just be a thing, be a gig.
[00:36:50.620 --> 00:36:51.660]   You're an artist.
[00:36:51.660 --> 00:36:56.540]   You can join a studio to become a senior artist in that studio, but you can also just do art
[00:36:56.540 --> 00:36:57.980]   on your side.
[00:36:57.980 --> 00:37:00.620]   And science is at the same time a collective effort.
[00:37:00.620 --> 00:37:01.620]   You need collaborators.
[00:37:01.620 --> 00:37:04.220]   You need people to work together with you.
[00:37:04.220 --> 00:37:08.100]   So for that to happen, if you're taking science as a gig, then you have to be able to work
[00:37:08.100 --> 00:37:12.460]   with other people, but then we don't have really a culture there yet.
[00:37:12.460 --> 00:37:16.660]   If you analyze all the papers out there, Google people are working with Google people, CMU
[00:37:16.660 --> 00:37:17.940]   people are working with CMU people.
[00:37:17.940 --> 00:37:20.220]   Not exactly, but there are clusters.
[00:37:20.220 --> 00:37:24.740]   And each of us bears our own comfort zone of collaborators.
[00:37:24.740 --> 00:37:30.700]   We rarely go out of the comfort circle because with any new people, there's a friction of
[00:37:30.700 --> 00:37:31.980]   working together.
[00:37:31.980 --> 00:37:33.980]   So we don't do that too often.
[00:37:33.980 --> 00:37:38.860]   And that creates a problem because we're all so little bubbles isolated.
[00:37:38.860 --> 00:37:42.780]   And new people find it really hard to join all those circles.
[00:37:42.780 --> 00:37:47.380]   At least as a new people back then, I find it really hard to find someone and become
[00:37:47.380 --> 00:37:49.940]   their collaborator because there's not a culture like that.
[00:37:49.940 --> 00:37:54.580]   So the whole thing with ML Collective is that we have members coming from all different
[00:37:54.580 --> 00:37:55.580]   kinds of employers.
[00:37:55.580 --> 00:38:00.060]   They are working elsewhere, but they're willing to share their work within ML Collective.
[00:38:00.060 --> 00:38:05.220]   They feel this is a safe space that you can share your work, get feedback, maybe become
[00:38:05.220 --> 00:38:10.300]   co-authors with people you never would have because you work in different teams, different
[00:38:10.300 --> 00:38:11.780]   institutions.
[00:38:11.780 --> 00:38:13.860]   So that's the whole idea.
[00:38:13.860 --> 00:38:16.260]   Many people are carrying this culture around.
[00:38:16.260 --> 00:38:21.140]   That's why we invited all those great people to the social, talking about how they have
[00:38:21.140 --> 00:38:22.140]   done that.
[00:38:22.140 --> 00:38:27.860]   So people that are holding office hours, actively outreaching to people, trying to mentor people
[00:38:27.860 --> 00:38:29.420]   on their spare time.
[00:38:29.420 --> 00:38:34.020]   There are companies that entirely run science in an open way.
[00:38:34.020 --> 00:38:35.460]   They broadcast all their meetings.
[00:38:35.460 --> 00:38:42.780]   They put everything out there on GitHub, like even the everyday commit of codes out there.
[00:38:42.780 --> 00:38:45.380]   So there are many open cultures out there.
[00:38:45.380 --> 00:38:50.300]   So we want to gather people to discuss the pros and cons of this method.
[00:38:50.300 --> 00:38:55.860]   Of course, science is much slower produced this way because you have to uncomfortably
[00:38:55.860 --> 00:39:02.540]   work with people that's not familiar to you, but it really improves the overall wellbeing
[00:39:02.540 --> 00:39:03.540]   of the society.
[00:39:03.540 --> 00:39:04.540]   It might not be slower.
[00:39:04.540 --> 00:39:10.260]   It might be faster if you make more connections in the global brain.
[00:39:10.260 --> 00:39:12.260]   I could imagine that it leads to more interesting insights.
[00:39:12.260 --> 00:39:13.260]   Yeah, I don't know.
[00:39:13.260 --> 00:39:16.460]   But then there must be a reason that people are not doing that a lot.
[00:39:16.460 --> 00:39:22.540]   I feel like it must be slower in the case that there's always this friction period that
[00:39:22.540 --> 00:39:27.020]   you're getting to know each other, what each other's work style is like.
[00:39:27.020 --> 00:39:28.340]   Yeah, I don't know.
[00:39:28.340 --> 00:39:33.460]   I feel like people have tried that so little, must be out of this fear of how hard it would
[00:39:33.460 --> 00:39:34.700]   be to work with other people.
[00:39:34.700 --> 00:39:37.060]   Well, I would think people might just feel shy.
[00:39:37.060 --> 00:39:38.060]   It's hard to go meet a stranger.
[00:39:38.060 --> 00:39:39.060]   Yeah, could be that.
[00:39:39.060 --> 00:39:44.860]   Do you do anything to facilitate just getting people talking to each other?
[00:39:44.860 --> 00:39:45.860]   Yeah.
[00:39:45.860 --> 00:39:50.020]   So for now, we started this organization where people just join from anywhere.
[00:39:50.020 --> 00:39:51.100]   So I'm sort of like the hub.
[00:39:51.100 --> 00:39:54.060]   I know everyone, but they don't know each other to start with.
[00:39:54.060 --> 00:39:58.900]   But then if you do bi-weekly meetings like we do, we talk about research every time.
[00:39:58.900 --> 00:40:02.180]   And then you two can be commenting on the same graph and then you're like, "Oh, we were
[00:40:02.180 --> 00:40:03.860]   thinking the same way."
[00:40:03.860 --> 00:40:05.860]   You're like-minded people, we should talk more.
[00:40:05.860 --> 00:40:06.860]   Then they can talk offline.
[00:40:06.860 --> 00:40:07.860]   Then I'm done.
[00:40:07.860 --> 00:40:11.140]   I'm like a matchmaker, sort of like link people together.
[00:40:11.140 --> 00:40:15.660]   I'm very happy if two people that didn't know each other now work together.
[00:40:15.660 --> 00:40:20.420]   I feel like my satisfaction comes from that.
[00:40:20.420 --> 00:40:28.220]   It's fascinating to me that you're taking the credentialing aspect out of these research
[00:40:28.220 --> 00:40:31.780]   labs and almost replacing them with collaboration.
[00:40:31.780 --> 00:40:38.260]   Is that the reward function or is there a different reward function now?
[00:40:38.260 --> 00:40:40.660]   The reward function for me is definitely just...
[00:40:40.660 --> 00:40:45.340]   In fact, it can reduce the whole thing ML Collective does to one metric.
[00:40:45.340 --> 00:40:48.100]   That would be the number of new collaborations formed.
[00:40:48.100 --> 00:40:49.780]   That would be my reward function.
[00:40:49.780 --> 00:40:52.340]   That's the reward function of the MLC.
[00:40:52.340 --> 00:40:56.580]   But my personal reward function, as I said, is how many papers that have my name in the
[00:40:56.580 --> 00:40:57.580]   acknowledgement.
[00:40:57.580 --> 00:40:59.580]   That will be my near-term reward function.
[00:40:59.580 --> 00:41:03.580]   I'll be very happy if people thank me in their acknowledgement.
[00:41:03.580 --> 00:41:04.580]   That's great.
[00:41:04.580 --> 00:41:07.900]   What about for the researchers in MLC?
[00:41:07.900 --> 00:41:09.460]   What's their reward function do you think?
[00:41:09.460 --> 00:41:14.940]   And how is that different from those people who are working at traditional labs?
[00:41:14.940 --> 00:41:17.740]   So curiosity-driven, that's one.
[00:41:17.740 --> 00:41:18.740]   We're not goal-driven.
[00:41:18.740 --> 00:41:20.700]   We're not trying to beat any benchmarks.
[00:41:20.700 --> 00:41:21.700]   We're allowed to do that.
[00:41:21.700 --> 00:41:24.540]   I mean, other labs probably also have some elements of that.
[00:41:24.540 --> 00:41:26.260]   But yeah, I don't know.
[00:41:26.260 --> 00:41:27.860]   Each lab has its different culture.
[00:41:27.860 --> 00:41:31.900]   Some are more open, some are more goal-driven, trying to make sure...
[00:41:31.900 --> 00:41:35.700]   The whole thing that people talk about on Twitter is like, "I have 28 papers in this
[00:41:35.700 --> 00:41:40.780]   conference, so we would not be saying that because we can never reach there."
[00:41:40.780 --> 00:41:42.740]   But also, that's not our goal.
[00:41:42.740 --> 00:41:45.380]   It's not, "Got this number of many papers in a conference."
[00:41:45.380 --> 00:41:52.180]   It's more like, "We can have this scientific discovery purely driven by curiosity."
[00:41:52.180 --> 00:41:56.740]   Like the intrinsic dimension paper, it was just us thinking, "Hmm, everyone trains neural
[00:41:56.740 --> 00:41:58.540]   network this way with a big vector.
[00:41:58.540 --> 00:42:00.620]   Can we train it with a small vector?"
[00:42:00.620 --> 00:42:05.540]   No reason why we have to do that, but we just thought about it and we said, "Must be."
[00:42:05.540 --> 00:42:09.940]   Because thinking like, if you can draw a line, there is a dimension one out there, but how
[00:42:09.940 --> 00:42:11.500]   hard is it to find that dimension?
[00:42:11.500 --> 00:42:15.340]   How hard is it to find that even with a random initialization?
[00:42:15.340 --> 00:42:19.620]   So yeah, if I were to control it, but again, I don't control the directions of research
[00:42:19.620 --> 00:42:23.620]   in ML Collective, but if I were to control it, I would encourage everyone to see it as
[00:42:23.620 --> 00:42:24.620]   a fun thing.
[00:42:24.620 --> 00:42:27.620]   ML researchers, they're so miserable.
[00:42:27.620 --> 00:42:29.900]   I was part of them, so I know that.
[00:42:29.900 --> 00:42:31.900]   Every day they're like, "Oh, this conference is coming up.
[00:42:31.900 --> 00:42:32.900]   I'm not submitting.
[00:42:32.900 --> 00:42:33.900]   I feel so bad.
[00:42:33.900 --> 00:42:34.900]   I'm such a failure."
[00:42:34.900 --> 00:42:38.860]   So really, I just want to make this a fun thing, a gig that they're doing.
[00:42:38.860 --> 00:42:39.940]   They get to meet new people.
[00:42:39.940 --> 00:42:44.260]   They get to work with people different from them, better than them in some ways.
[00:42:44.260 --> 00:42:48.060]   They get to feel like they're helpful in others' projects.
[00:42:48.060 --> 00:42:52.220]   I think it might be eye-opening for people listening to this, that someone as successful
[00:42:52.220 --> 00:42:54.300]   and credentialed as you could feel like a failure.
[00:42:54.300 --> 00:42:58.940]   I feel like it's an occupational hazard of the field, but I really do think most people
[00:42:58.940 --> 00:43:02.860]   listening or watching this will be surprised to know that.
[00:43:02.860 --> 00:43:03.860]   Oh, exactly.
[00:43:03.860 --> 00:43:04.860]   So much.
[00:43:04.860 --> 00:43:10.500]   I didn't realize it because it's not so miserable that I'm just crying every day, but it's just
[00:43:10.500 --> 00:43:14.940]   a mild level of depression, which is the worst because every time you're confronted, you're
[00:43:14.940 --> 00:43:17.420]   like, "I feel bad, but should I be feeling bad?
[00:43:17.420 --> 00:43:18.940]   I'm having this amazing job.
[00:43:18.940 --> 00:43:23.980]   I get to do science in an industry, getting paid reasonably well."
[00:43:23.980 --> 00:43:26.900]   So you sort of counter yourself of the bad feeling.
[00:43:26.900 --> 00:43:28.660]   That makes things even worse.
[00:43:28.660 --> 00:43:30.460]   So from the outside, everything's glamorous.
[00:43:30.460 --> 00:43:32.700]   I get to publish every now and then.
[00:43:32.700 --> 00:43:33.700]   But yeah, I was miserable.
[00:43:33.700 --> 00:43:39.380]   And I realized one key thing that changed my mindset is that I was viewing everyone
[00:43:39.380 --> 00:43:42.540]   outside of my team as competitors.
[00:43:42.540 --> 00:43:44.940]   And I'm just miserable because I feel I have to compete with them.
[00:43:44.940 --> 00:43:49.760]   And if they're publishing 28 papers and I'm publishing zero, I'm losing.
[00:43:49.760 --> 00:43:54.300]   But now by running MLC, I see them as collaborators or potential collaborators.
[00:43:54.300 --> 00:43:55.620]   So people are out there.
[00:43:55.620 --> 00:44:00.260]   If we have the same ideas at the same time, the past me will be like, "No, I'm scooped."
[00:44:00.260 --> 00:44:01.260]   But now I'll be like, "Great.
[00:44:01.260 --> 00:44:03.420]   That means that that's a great idea.
[00:44:03.420 --> 00:44:04.420]   You can be my potential collaborator.
[00:44:04.420 --> 00:44:07.780]   I can talk to you and you can join MLC and help me and help others."
[00:44:07.780 --> 00:44:12.660]   It's really like a mindset change, at least for me, or maybe just because I'm not getting
[00:44:12.660 --> 00:44:13.660]   paid right now.
[00:44:13.660 --> 00:44:18.860]   So if you let people do something and then not pay them, they start to think that this
[00:44:18.860 --> 00:44:22.460]   thing must be noble because I'm doing it and not getting paid.
[00:44:22.460 --> 00:44:27.460]   I don't know which aspect is the one that changed my mindset to be from that to this.
[00:44:27.460 --> 00:44:29.860]   But yeah, there are many things that have changed.
[00:44:29.860 --> 00:44:33.780]   Well, I have to say, I really admire you creating the world that you want to see.
[00:44:33.780 --> 00:44:36.180]   I think that's super admirable and impressive.
[00:44:36.180 --> 00:44:37.180]   Thank you.
[00:44:37.180 --> 00:44:41.180]   I want to just end with two questions and I want to make sure we have time to do that.
[00:44:41.180 --> 00:44:46.900]   The sort of second to last question that we always ask is, what is something in the ML
[00:44:46.900 --> 00:44:51.540]   field that you feel like doesn't get as much attention as it should?
[00:44:51.540 --> 00:44:54.980]   That's a good question.
[00:44:54.980 --> 00:44:57.180]   I would say understanding of things.
[00:44:57.180 --> 00:45:02.620]   I think the field of ML research publishing would become healthy if we start to see a
[00:45:02.620 --> 00:45:07.140]   wave of papers that just go, "I think this little concept, batch norm, or something like
[00:45:07.140 --> 00:45:08.140]   that, is a dropout."
[00:45:08.140 --> 00:45:11.300]   I studied so extensively that I wrote an eight-page paper out of it.
[00:45:11.300 --> 00:45:15.700]   I tried everything I can with it, without it, in this network, in that network.
[00:45:15.700 --> 00:45:19.940]   The end result is we didn't find anything amazing, but we understand this concept 1%
[00:45:19.940 --> 00:45:20.940]   more.
[00:45:20.940 --> 00:45:21.940]   That's science.
[00:45:21.940 --> 00:45:25.100]   I want to see a wave of papers that's written this way instead of, "We beat this benchmark,
[00:45:25.100 --> 00:45:27.900]   we beat that benchmark," because that's very rare.
[00:45:27.900 --> 00:45:34.100]   Just trying to go for a deeper understanding of one small concept and see why it helps.
[00:45:34.100 --> 00:45:38.620]   There's so many things we don't understand in the way that we train our networks.
[00:45:38.620 --> 00:45:43.380]   Of course, when you say understanding, people have different comfort levels in terms of
[00:45:43.380 --> 00:45:44.380]   understanding.
[00:45:44.380 --> 00:45:48.460]   I can see there are people out there having more of a hacker's attitude.
[00:45:48.460 --> 00:45:52.700]   They would think they understand something if they watched a five-minute video of it.
[00:45:52.700 --> 00:45:57.780]   They're more humble, conservative attitude, which I would say more of my scientist peers
[00:45:57.780 --> 00:45:58.780]   have.
[00:45:58.780 --> 00:46:03.220]   Unless I publish a lead author paper on this subject, I can't say I understand it.
[00:46:03.220 --> 00:46:05.740]   Even if I publish one, I can't say I understand it.
[00:46:05.740 --> 00:46:09.460]   There's different levels of things, but I hope people are going for a better understanding
[00:46:09.460 --> 00:46:11.860]   of things than in benchmarks.
[00:46:11.860 --> 00:46:18.580]   I guess the last question is, what's the biggest challenge of publishing a paper independently
[00:46:18.580 --> 00:46:22.780]   when you're not living in a big lab?
[00:46:22.780 --> 00:46:27.740]   So much of it, the lack of resource, the lack of support, the lack of people just telling
[00:46:27.740 --> 00:46:31.180]   you it's a good idea or bad idea, lack of discriminators.
[00:46:31.180 --> 00:46:34.300]   When you're publishing a paper, you're probably the generator of the paper.
[00:46:34.300 --> 00:46:36.340]   It's like lack of discrimination.
[00:46:36.340 --> 00:46:37.340]   Think about GAN training.
[00:46:37.340 --> 00:46:40.740]   Without a discriminator, you really can't train a good GAN.
[00:46:40.740 --> 00:46:41.780]   All those things.
[00:46:41.780 --> 00:46:47.340]   That's why we want to recreate this great graduate school lab experience for everyone.
[00:46:47.340 --> 00:46:49.700]   You don't have to join a graduate school lab.
[00:46:49.700 --> 00:46:54.260]   You don't have to join a big industry lab to have the same experience.
[00:46:54.260 --> 00:46:58.580]   Mentors or collaborators, peers, people just say, "Awesome," on your plots.
[00:46:58.580 --> 00:47:02.220]   Or, "Yeah, you should add one more line to that plot to make it more awesome."
[00:47:02.220 --> 00:47:03.220]   Stuff like that.
[00:47:03.220 --> 00:47:05.860]   People you can bounce your ideas off of.
[00:47:05.860 --> 00:47:06.860]   All that little things.
[00:47:06.860 --> 00:47:13.340]   Of course, we know how hard it is for individual researchers to thrive over there, out there.
[00:47:13.340 --> 00:47:16.460]   Yeah, if ML Collective can help them a little bit, I'll be very happy.
[00:47:16.460 --> 00:47:19.660]   I guess I'll sneak in one final question.
[00:47:19.660 --> 00:47:24.340]   If people are listening to ML Collective and feeling inspired, what's a next step for them
[00:47:24.340 --> 00:47:28.340]   to get a little more involved or learn a little bit more?
[00:47:28.340 --> 00:47:34.380]   So for a nonprofit, where we really want to get this idea out, this social impact we want
[00:47:34.380 --> 00:47:38.740]   to put out, the idea for us is the open collaboration.
[00:47:38.740 --> 00:47:43.260]   So for people out there, if you're a researcher, if you're an individual researcher, an independent
[00:47:43.260 --> 00:47:45.820]   researcher, you can always come to work with us.
[00:47:45.820 --> 00:47:50.100]   There's many collaborators that here will be happy to work with you.
[00:47:50.100 --> 00:47:54.180]   If you are already a senior researcher, an established researcher, you should think of
[00:47:54.180 --> 00:47:56.720]   this concept actively.
[00:47:56.720 --> 00:47:59.860]   Actually, paper is you think about, did I help someone with this paper?
[00:47:59.860 --> 00:48:05.220]   Did I just work with the same crew of collaborators that I always worked with for the past 10
[00:48:05.220 --> 00:48:06.220]   years?
[00:48:06.220 --> 00:48:09.500]   Or did I put someone new on this paper and really helped their career?
[00:48:09.500 --> 00:48:14.100]   Because having a paper helps so much in someone's career, at least for now.
[00:48:14.100 --> 00:48:17.180]   Did I try to make the world better with this paper?
[00:48:17.180 --> 00:48:21.140]   Aside from the scientific pursuit, of course, you're making the world better by just putting
[00:48:21.140 --> 00:48:22.340]   a scientific work out there.
[00:48:22.340 --> 00:48:26.300]   But did I give other people chances to work in science?
[00:48:26.300 --> 00:48:30.540]   Did I help someone underrepresented or help someone from a non-traditional background
[00:48:30.540 --> 00:48:32.260]   get into science through this paper?
[00:48:32.260 --> 00:48:35.780]   Yeah, I want to get people to think about this question actively.
[00:48:35.780 --> 00:48:36.780]   Awesome.
[00:48:36.780 --> 00:48:39.420]   Well, thank you so much.
[00:48:39.420 --> 00:48:40.420]   Real pleasure to talk to you.
[00:48:40.420 --> 00:48:41.420]   Thanks for taking the time.
[00:48:41.420 --> 00:48:42.420]   That was a lot of fun.
[00:48:42.420 --> 00:48:43.420]   Yeah.
[00:48:43.420 --> 00:48:47.660]   Thank you so much, Lavanya and Lucas.
[00:48:47.660 --> 00:48:50.860]   Thanks for listening to another episode of Gradient Dissent.
[00:48:50.860 --> 00:48:55.140]   Doing these interviews are a lot of fun, and it's especially fun for me when I can actually
[00:48:55.140 --> 00:48:57.900]   hear from the people that are listening to these episodes.
[00:48:57.900 --> 00:49:02.860]   So if you wouldn't mind leaving a comment and telling me what you think or starting a
[00:49:02.860 --> 00:49:05.920]   conversation, that would make me inspired to do more of these episodes.
[00:49:05.920 --> 00:49:09.460]   And also, if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

