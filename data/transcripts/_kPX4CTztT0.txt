
[00:00:00.000 --> 00:00:04.460]   Hey, Armand from Wetsnbass is here.
[00:00:04.460 --> 00:00:09.160]   In a previous video, we've made this RC car drive on a racetrack using what was basically
[00:00:09.160 --> 00:00:10.520]   a lane assist model.
[00:00:10.520 --> 00:00:15.280]   To have full self-driving in a city, we will need it to detect objects and respond to them.
[00:00:15.280 --> 00:00:16.280]   So that's what we're gonna do.
[00:00:16.280 --> 00:00:18.860]   We're gonna make it detect stop signs and stop.
[00:00:18.860 --> 00:00:21.160]   To do so, we're gonna use a pre-trained model.
[00:00:21.160 --> 00:00:24.640]   Training models is expensive, so more and more people are shifting to using pre-trained
[00:00:24.640 --> 00:00:26.320]   models and doing transfer learning.
[00:00:26.320 --> 00:00:30.080]   Okay, so I've made a list of criteria to choose my pre-trained model.
[00:00:30.080 --> 00:00:33.360]   I want it to be lightweight so that it can run on my Jetson Nano.
[00:00:33.360 --> 00:00:37.760]   Ideally, I want it to be pre-trained on the task at hand, which is detecting stop signs.
[00:00:37.760 --> 00:00:42.600]   I also want it to be easy to use so that I can quickly integrate it with my current pipeline.
[00:00:42.600 --> 00:00:47.200]   After a bit of research, the YOLOv5-S matches all of those criteria and it's pre-trained
[00:00:47.200 --> 00:00:50.400]   on the MS-COCO dataset, which includes traffic signs.
[00:00:50.400 --> 00:00:55.600]   The YOLOv5 repository also has a nice Wetsnbass integration, which in regarding model checkpoint
[00:00:55.600 --> 00:00:59.240]   versioning, bbox debugging and ways to resume training out of the box.
[00:00:59.240 --> 00:01:02.960]   Okay, so before I start adding object detection to our pipeline, I'm gonna give you a quick
[00:01:02.960 --> 00:01:04.440]   overview of how it works.
[00:01:04.440 --> 00:01:09.440]   We start by reading images from our camera, then we send them to our perception function,
[00:01:09.440 --> 00:01:13.920]   and finally we pass those perceptions to our control policy, which sets the steering and
[00:01:13.920 --> 00:01:14.920]   throttle of the car.
[00:01:14.920 --> 00:01:19.160]   Okay, so we're gonna start by setting up YOLOv5 and loading the weights.
[00:01:19.160 --> 00:01:23.760]   After that, we're gonna pass our image to the YOLOv5 model.
[00:01:23.760 --> 00:01:27.880]   Now that our car can detect stop signs, we just need to program it to stop when it sees
[00:01:27.880 --> 00:01:30.640]   them.
[00:01:30.640 --> 00:01:31.640]   And that's pretty much it.
[00:01:31.640 --> 00:01:42.440]   Okay, so let's take it for a test drive.
[00:01:42.440 --> 00:01:44.560]   As you can see, that's working fairly well.
[00:01:44.560 --> 00:01:46.520]   Let's take a look at our Wetsnbass dashboard.
[00:01:46.520 --> 00:01:50.440]   I sent a lot of logs to Wetsnbass, but I won't focus on the images.
[00:01:50.440 --> 00:01:54.880]   Looking at those, I can see the lane assist model predictions along with the object detections,
[00:01:54.880 --> 00:01:57.800]   and I can see that it successfully detected the stop sign.
[00:01:57.800 --> 00:02:01.120]   If it had failed, I would be able to determine what it saw instead.
[00:02:01.120 --> 00:02:03.160]   That's it for this video, I hope you enjoyed it.
[00:02:03.160 --> 00:02:07.320]   If you want to learn more about Wetsnbasses and YOLOv5, I recommend you check out our
[00:02:07.320 --> 00:02:08.320]   YOLOv5 series.
[00:02:08.320 --> 00:02:12.280]   If you have any questions, please drop them down below, and don't forget to subscribe
[00:02:12.280 --> 00:02:14.640]   for more interviews, tutorials and talks.
[00:02:14.640 --> 00:02:15.480]   Thanks for watching!

