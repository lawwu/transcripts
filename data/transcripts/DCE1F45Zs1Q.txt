
[00:00:00.000 --> 00:00:09.620]   disciple on the door. Thanks for pointing that out. Maybe I
[00:00:09.620 --> 00:00:15.420]   should just move the light. This should be fine now. Okay.
[00:00:15.420 --> 00:00:27.620]   Okay, let me see if I'm live on YouTube yet. Awesome. For the
[00:00:27.620 --> 00:00:30.420]   people on YouTube, I was having some video issues that were
[00:00:30.420 --> 00:00:34.820]   that's what you walked into as I was trying to debug that. But
[00:00:34.820 --> 00:00:41.700]   we're all set, I think. And let's get started. So to Kisly
[00:00:41.700 --> 00:00:45.500]   is asking in the chat. Has the talk just started? Yes, we're
[00:00:45.500 --> 00:00:48.940]   just starting in actually in a minute. I'm just setting things
[00:00:48.940 --> 00:00:53.740]   up. Eight weeks into this and guys, I'm still still learning
[00:00:53.740 --> 00:00:58.700]   how to set zoom up as you can tell. That's all right. I'll
[00:00:58.700 --> 00:01:01.300]   just take a few more study groups to figure this out.
[00:01:01.300 --> 00:01:05.260]   Awesome. Let me move zoom things out of the way. And could
[00:01:05.260 --> 00:01:08.100]   someone please confirm if my audio is sounding good so that I
[00:01:08.100 --> 00:01:12.660]   can take off my headphones. Either on YouTube or in the zoom
[00:01:12.660 --> 00:01:21.060]   chart. Awesome. Audio is fine. Thanks for confirming that
[00:01:21.060 --> 00:01:31.940]   guys. I will take off my headphones. Incredible. So
[00:01:31.940 --> 00:01:35.340]   welcome back everyone. I am quite excited because we're on
[00:01:35.340 --> 00:01:40.360]   the modeling part today. As you can tell as a real machine
[00:01:40.360 --> 00:01:42.620]   learning or deep learning engineer, I am probably more
[00:01:42.620 --> 00:01:46.620]   excited about the modeling things rather than one of the
[00:01:46.620 --> 00:01:49.340]   pipelining stuff. I'm just kidding this. This is a subtle
[00:01:49.340 --> 00:01:54.860]   joke for anyone in the industry. There's this meme that machine
[00:01:54.860 --> 00:01:57.820]   learning engineers or deep learning engineers just just work
[00:01:57.820 --> 00:02:02.300]   on creating models. That's not always true. Sometimes it is.
[00:02:02.300 --> 00:02:05.460]   But it's definitely exciting times are definitely exciting
[00:02:05.460 --> 00:02:08.380]   session because we learn how to create a first classifier today.
[00:02:08.380 --> 00:02:13.180]   We've been going about this long, long journey. And last
[00:02:13.180 --> 00:02:16.580]   week, we had learned how to work with a data set class for
[00:02:16.580 --> 00:02:20.380]   classifying your tumors based on this particular Lunar 16 data
[00:02:20.380 --> 00:02:23.900]   set that we had decided to work with. Actually, the authors are
[00:02:23.900 --> 00:02:27.060]   decided for us, but we just, we're just putting our faith in
[00:02:27.060 --> 00:02:29.920]   them and following their direction. So that's what we
[00:02:29.920 --> 00:02:34.340]   learned last week. This was the homework for last week. No one
[00:02:34.340 --> 00:02:37.860]   did it. So I'm quite upset. Especially the people in the
[00:02:37.860 --> 00:02:41.220]   zoom call who've been coming every week, guys, please, please
[00:02:41.220 --> 00:02:44.740]   do the homework. It's it's, it should be an awesome learning
[00:02:44.740 --> 00:02:48.940]   experience for you, if you decide to. So as a reminder,
[00:02:48.940 --> 00:02:51.500]   you're at the point I'm just highlighting this so that you
[00:02:51.500 --> 00:02:54.020]   can pick these things, but you're at the point where you
[00:02:54.020 --> 00:02:57.660]   can start getting a feel for Kaggle. And my suggestion was
[00:02:57.660 --> 00:03:05.420]   shamelessly go to any competition. And once that
[00:03:05.420 --> 00:03:12.140]   loads up, let's see, let's click on competitions. Let's go to
[00:03:12.220 --> 00:03:15.100]   brain tumor classification. Yep, that's that's a good one.
[00:03:15.100 --> 00:03:22.420]   Go to code. And this, this is a real trick. So you sort by best
[00:03:22.420 --> 00:03:30.020]   score, which means you'll get the highest scoring kernel
[00:03:30.020 --> 00:03:35.860]   publicly out there. Just take it start tweaking a few things and
[00:03:35.860 --> 00:03:41.420]   submit it. So that was my silly suggestion. Another useful one
[00:03:41.420 --> 00:03:44.980]   would be actually try to take a kernel in TensorFlow and try to
[00:03:44.980 --> 00:03:52.180]   port it over to PyTorch. So I don't see any here. So I'm not
[00:03:52.180 --> 00:03:54.660]   going to like try to find one but usually a lot of the
[00:03:54.660 --> 00:04:00.780]   kernels are written in Keras or TensorFlow. So what I would
[00:04:00.780 --> 00:04:04.660]   encourage you to do is just take a kernel and line by line, try
[00:04:04.660 --> 00:04:09.180]   to understand the code ported to PyTorch. But always, always
[00:04:09.180 --> 00:04:12.700]   remember to give credit. Even if you're using one small idea,
[00:04:12.700 --> 00:04:17.140]   just be mindful of the original person's work. Always try to
[00:04:17.140 --> 00:04:20.420]   give credit wherever it's due. There's no harm in doing that.
[00:04:20.420 --> 00:04:24.220]   And there's no overdoing it. So even if you take one small idea,
[00:04:24.220 --> 00:04:27.900]   please be sincere to the original creators every time.
[00:04:27.900 --> 00:04:34.380]   The other one was as a suggested one was pick your favorite
[00:04:34.380 --> 00:04:37.580]   notebook from this competition that actually use the exact same
[00:04:37.580 --> 00:04:40.540]   data set that we've been working on. So I see a few new people
[00:04:40.540 --> 00:04:45.620]   joining us today, guys, we've been on this journey of reading
[00:04:45.620 --> 00:04:49.820]   this book, deep learning with PyTorch. And we're right at the
[00:04:49.820 --> 00:04:53.140]   11th chapter where we're working with this data set. So that
[00:04:53.140 --> 00:04:55.540]   those are the suggested ideas. And I think you're at that
[00:04:55.540 --> 00:04:58.340]   point where you can start working on these. So just to
[00:04:58.340 --> 00:05:01.180]   remind everyone who's there in the zoom call and also on
[00:05:01.180 --> 00:05:04.460]   YouTube, please, please post your questions on to this link.
[00:05:04.460 --> 00:05:09.220]   It's the pinned comment on YouTube as well. And let me drop
[00:05:09.220 --> 00:05:15.060]   it in the zoom chart as well. So if you head over to this link,
[00:05:15.060 --> 00:05:19.780]   and I just send that for hosts and panelists, I should send it
[00:05:19.780 --> 00:05:24.700]   to everyone. I fixed that issue this time. And I'll drop that in
[00:05:24.700 --> 00:05:30.100]   the YouTube chart as well. So if you head over to this link, it
[00:05:30.100 --> 00:05:33.580]   should take you to our discourse forums. And that's where you can
[00:05:33.580 --> 00:05:34.980]   ask all the questions.
[00:05:34.980 --> 00:05:54.860]   So I also want to make a quick announcement here. This is how
[00:05:54.860 --> 00:05:58.220]   you can basically ask questions on here. You can see everything
[00:05:58.220 --> 00:06:03.420]   structured. But we're my part of my responsibility at Wheatston
[00:06:03.420 --> 00:06:07.220]   biases is also to think of ideas around the community. I really
[00:06:07.220 --> 00:06:11.460]   think one of the best courses on machine learning still to date
[00:06:11.460 --> 00:06:15.660]   is fast a machine learning for coders. But this was taught in
[00:06:15.660 --> 00:06:19.940]   2018. So there's this category I've created, but you won't see
[00:06:19.940 --> 00:06:24.020]   on the forums because right now it's private. A few of us are
[00:06:24.020 --> 00:06:27.300]   collaborating on this and our agenda is to basically port this
[00:06:27.300 --> 00:06:32.420]   course over to its latest format. I am a product of fast
[00:06:32.420 --> 00:06:35.720]   AI, I've learned everything from there. And Jeremy Howard, the
[00:06:35.720 --> 00:06:38.820]   creator actually suggests that this course is now outdated
[00:06:38.820 --> 00:06:44.140]   because it's covered in the current book. Then why the why
[00:06:44.140 --> 00:06:47.740]   the heck am I deciding to even reengage with the course this
[00:06:47.740 --> 00:06:51.140]   this one reason for that this course, in my opinion, I've
[00:06:51.140 --> 00:06:54.820]   stopped sharing my screen. So don't worry. This course
[00:06:54.820 --> 00:06:57.580]   actually really, really showcased you how to get started
[00:06:57.580 --> 00:07:03.100]   on Kaggle. And reading books is fine. Thinking of projects is
[00:07:03.100 --> 00:07:07.820]   also great way. But I think getting addicted to Kaggle as
[00:07:07.820 --> 00:07:11.540]   soon as you can in your deep learning journey is quite
[00:07:11.540 --> 00:07:15.020]   helpful. And this course was I think at least for me, one of
[00:07:15.020 --> 00:07:19.500]   the reasons if not the reason that got me addicted to Kaggle.
[00:07:19.500 --> 00:07:25.700]   So that's why I would love to remaster this course or really
[00:07:25.700 --> 00:07:31.020]   just update its code. If and by the way, why am I announcing
[00:07:31.020 --> 00:07:35.860]   this? If you're interested in contributing to this, just like
[00:07:35.860 --> 00:07:39.980]   this reply, and I'll have you added to the group where we're
[00:07:39.980 --> 00:07:42.860]   currently collaborating on this. So if you're interested in
[00:07:42.860 --> 00:07:48.500]   collaborating on a fast AI based project, just like this message,
[00:07:48.500 --> 00:07:51.220]   and I'll add you to the category where we're currently
[00:07:51.220 --> 00:07:56.380]   discussing and collaborating on this. I also want to announce
[00:07:56.380 --> 00:08:00.820]   another or just mention quickly, another study group that I
[00:08:00.820 --> 00:08:05.660]   believe Ravi has been leading. So a few of the incredible
[00:08:05.660 --> 00:08:09.580]   people on the forums have been working on a movie recommender
[00:08:09.580 --> 00:08:12.700]   project. And I would again encourage you to check that out.
[00:08:12.700 --> 00:08:18.940]   So let me drop that link here as well. You can head over to this
[00:08:18.940 --> 00:08:21.220]   link and you can join the group. I'm sure they're really
[00:08:21.220 --> 00:08:24.180]   welcoming folks as long as you do their your homework, you can
[00:08:24.180 --> 00:08:29.380]   still join and be a part of the project. And spending a lot of
[00:08:29.380 --> 00:08:32.220]   time talking about projects, because that's what you should
[00:08:32.220 --> 00:08:35.100]   be building. At the end, you should become a practitioner.
[00:08:35.100 --> 00:08:39.700]   And a practitioner's job is to work on a lot of projects. So
[00:08:39.700 --> 00:08:43.060]   that's why I spent nine minutes today talking about these ideas.
[00:08:43.060 --> 00:08:47.500]   Awesome.
[00:08:47.500 --> 00:08:52.940]   We're at the point where we'll be classifying tumors today,
[00:08:52.940 --> 00:08:57.220]   we'll be learning about image augmentations. And I'll also be
[00:08:57.220 --> 00:09:00.100]   giving an introduction to weights and biases. My original
[00:09:00.100 --> 00:09:05.660]   plan was to integrate this into the lectures code. I had a lot
[00:09:05.660 --> 00:09:10.780]   of internet issues today and I'm still on mobile net. So I don't
[00:09:10.780 --> 00:09:14.180]   want to create any any problems at all. That's the reason I
[00:09:14.180 --> 00:09:17.300]   couldn't create the demo. It might happen next week. But for
[00:09:17.300 --> 00:09:20.140]   today, I'll share a few resources. And the reason for
[00:09:20.140 --> 00:09:28.100]   using that would be the authors use TensorBoard. And I think I
[00:09:28.100 --> 00:09:30.660]   don't want to compare any frameworks, but I think weights
[00:09:30.660 --> 00:09:34.780]   and biases can be an incredible option for also logging your
[00:09:34.780 --> 00:09:37.900]   experiments. I'm quite biased since I work at weights and
[00:09:37.900 --> 00:09:40.660]   biases, but I'll still still do a walkthrough and let you all
[00:09:40.660 --> 00:09:45.700]   decide. So here's the agenda for today. I'll quickly recap what's
[00:09:45.740 --> 00:09:49.500]   been discussed in the last session. I'll glance through
[00:09:49.500 --> 00:09:56.220]   chapter 11 and 12 today. So quite an ambitious agenda. I'm
[00:09:56.220 --> 00:09:58.620]   sure you all are familiar with me. That's how it goes. And
[00:09:58.620 --> 00:10:01.540]   usually we end up covering about 70% of these things we'll see
[00:10:01.540 --> 00:10:04.780]   today. I'll spend some time understanding and explaining the
[00:10:04.780 --> 00:10:09.180]   code and we'll dive into an Excel workbook, which is quite
[00:10:09.180 --> 00:10:12.700]   exciting, believe me. So as a reminder to people who just
[00:10:12.700 --> 00:10:16.420]   joined or again with us for the first time, please head over to
[00:10:16.420 --> 00:10:20.460]   this link and ask the questions on the forums. I can't monitor
[00:10:20.460 --> 00:10:24.180]   the zoom chat and YouTube chat by maintaining a high level of
[00:10:24.180 --> 00:10:26.780]   concentration about the things I'm talking. So that's why I
[00:10:26.780 --> 00:10:30.540]   request you to keep the questions centralized. So let me
[00:10:30.540 --> 00:10:35.740]   head over to the Excel sheet and I'm not the creator behind this.
[00:10:35.740 --> 00:10:39.900]   I'll share my screen once I have it in full screen in a second.
[00:10:41.740 --> 00:10:45.860]   There we go. I have to Excel workbook opened. So I haven't
[00:10:45.860 --> 00:10:48.940]   created this. This is shamelessly downloaded from
[00:10:48.940 --> 00:10:52.700]   fasta. Fasta is one of the best deep learning courses, like I
[00:10:52.700 --> 00:10:58.300]   said. And I think Jeremy had shown everyone how to actually
[00:10:58.300 --> 00:11:03.260]   understand convolutional neural networks inside of Microsoft
[00:11:03.260 --> 00:11:07.980]   Excel. And I've over time, this has really grown on me. So
[00:11:08.100 --> 00:11:12.020]   initially, I went into this, maybe you know, this time
[00:11:12.020 --> 00:11:15.500]   Jeremy is not giving the best suggestion, Microsoft Excel,
[00:11:15.500 --> 00:11:21.420]   like, really, for a deep learning Microsoft Excel. Maybe
[00:11:21.420 --> 00:11:26.460]   next time and when I actually revisited it, I slowly slowly
[00:11:26.460 --> 00:11:32.620]   grown fond of it. Also for other reasons. But I think it's an
[00:11:32.620 --> 00:11:37.260]   incredible tool, especially for this use case. And this use
[00:11:37.260 --> 00:11:39.980]   case is understanding convolutional neural network
[00:11:39.980 --> 00:11:45.420]   inside of this. So what they've done, I'm doing a walkthrough
[00:11:45.420 --> 00:11:48.460]   again, as a reminder, this has been created by Jeremy Howard,
[00:11:48.460 --> 00:11:55.260]   but they've loaded a number seven. So this is, I think, 28
[00:11:55.260 --> 00:12:01.020]   by 28. That's the size of yes, it's 28 by 28. That's the size
[00:12:01.020 --> 00:12:05.620]   of an MNIST letter. So we've just loaded a number seven from
[00:12:05.660 --> 00:12:12.220]   MNIST. And this sheet essentially shows you how
[00:12:12.220 --> 00:12:15.380]   filters get applied, what outputs come out of it and what
[00:12:15.380 --> 00:12:18.740]   happens from there. So we're applying two filters here that
[00:12:18.740 --> 00:12:24.100]   are of three by three sides, filter one and filter two. Could
[00:12:24.100 --> 00:12:30.220]   anyone please answer why do we get two outputs? It's quite a
[00:12:30.220 --> 00:12:34.420]   simple question, but I just want to make sure. So in the forums,
[00:12:34.420 --> 00:12:41.940]   maybe if someone could answer, why do we get two outputs from
[00:12:41.940 --> 00:12:51.500]   the filters? I'm looking at the forums, I don't see an answer.
[00:12:51.500 --> 00:12:54.420]   So I'll probably spoil this one for you. We're using two
[00:12:54.420 --> 00:12:57.420]   filters. So that gives us two outputs. That's the reason we
[00:12:57.420 --> 00:13:00.660]   get two outputs. So what we're doing here is we've taken this
[00:13:00.660 --> 00:13:05.340]   three by three filter, two of them. And we're actually doing
[00:13:05.340 --> 00:13:09.620]   a dot product, or a matrix multiplication, sorry, a matrix
[00:13:09.620 --> 00:13:16.220]   multiplication, a convolution operation with every single
[00:13:16.220 --> 00:13:20.580]   pixel of the image. And that gives us this output one, so
[00:13:20.580 --> 00:13:24.100]   layer one called which we're calling con one. And since we
[00:13:24.100 --> 00:13:28.100]   have two filters, so it's filtered two by one, because
[00:13:28.100 --> 00:13:32.500]   it's the second filter in the first layer. So that's that's
[00:13:32.500 --> 00:13:36.820]   the notation here. So two by one stands for first layer, and
[00:13:36.820 --> 00:13:42.060]   second is the filter. So get this output. And now because we
[00:13:42.060 --> 00:13:47.980]   have two outputs, we need two filters in every filter layer.
[00:13:47.980 --> 00:13:52.700]   I'm mixing the terminology again. And I see Mateo's answer
[00:13:52.700 --> 00:13:55.540]   on the forum. I'm not sharing my screen, but I just read it out
[00:13:55.540 --> 00:13:59.940]   loud one output for each filter, which is correct. But now
[00:13:59.940 --> 00:14:05.820]   because we have two outputs, we will be applying to filter or
[00:14:05.820 --> 00:14:12.980]   kernel to both of them. So we'll still we'll still have two sets
[00:14:12.980 --> 00:14:17.540]   of kernels. And each inside of each of these will have again
[00:14:17.540 --> 00:14:23.460]   two kernels that will be convolving with. So we get the
[00:14:23.460 --> 00:14:28.860]   outputs from these that are visualized like so. And why are
[00:14:28.860 --> 00:14:32.340]   they just two outputs, not four outputs, because we add these
[00:14:32.340 --> 00:14:35.900]   outputs. And as you can see, we're actually trying to
[00:14:35.900 --> 00:14:40.260]   activate different weights. So we're just looking inside of the
[00:14:40.260 --> 00:14:44.460]   weights of this particular model, where we're just doing
[00:14:44.460 --> 00:14:47.100]   something very simple. This is even simpler than most of the
[00:14:47.100 --> 00:14:52.580]   models that you've created so far. We take a number seven, we
[00:14:52.580 --> 00:14:56.180]   apply this kernel to it, and we get this output. So as you can
[00:14:56.180 --> 00:14:58.980]   see, it's it's sort of understanding different things
[00:14:58.980 --> 00:15:03.460]   about this number seven, which now quite suspiciously looks
[00:15:03.460 --> 00:15:09.260]   like a bird to me. You can't unsee that now. It also looks
[00:15:09.260 --> 00:15:17.220]   like a bird doesn't it? Or a three maybe. See, I just
[00:15:17.220 --> 00:15:20.380]   confused myself. Never mind with that. But the point being we've
[00:15:20.380 --> 00:15:23.860]   applied all of these operations and we get this output. Next
[00:15:23.860 --> 00:15:26.980]   thing you'd want to do would be apply a max pool. So as a
[00:15:26.980 --> 00:15:31.860]   reminder, max pool just takes four numbers and gives the
[00:15:31.860 --> 00:15:35.500]   output of it. That's what it does. In this case, we can
[00:15:35.500 --> 00:15:38.140]   change how many numbers are we looking at, we can change
[00:15:38.140 --> 00:15:42.420]   different things. But that's what's going on here. So let's
[00:15:42.420 --> 00:15:45.500]   look at the top left corner, there are four zeros, highest
[00:15:45.500 --> 00:15:48.220]   of these would be zero. So that's what we get. And as you
[00:15:48.220 --> 00:15:51.940]   can see, this gives the max from here. Let's look at another
[00:15:51.940 --> 00:15:55.340]   function. That's one thing that I really have started liking
[00:15:55.340 --> 00:15:58.580]   about Excel, it's quite easy to look at things there. So let's
[00:15:58.580 --> 00:16:04.260]   see, this particular number is a highlight from here. And for
[00:16:04.260 --> 00:16:09.700]   some reason, we're getting a third, sorry, 20. I was
[00:16:09.700 --> 00:16:12.900]   confusing myself by my seeing the number 13 here, that's just
[00:16:12.900 --> 00:16:17.700]   the cell number 13. So the maximum number from these four
[00:16:17.700 --> 00:16:23.020]   is again 20. And that's why we see that number. Mateo says it
[00:16:23.020 --> 00:16:28.220]   reminds it reminds him of the Swift programming language. I
[00:16:28.220 --> 00:16:33.420]   can now I can't see that also Mateo what what what have you
[00:16:33.420 --> 00:16:38.300]   done? I'm confused as well. Now this is this a seven? Is this a
[00:16:38.300 --> 00:16:41.380]   three? Or is this a logo of the Swift programming language?
[00:16:41.380 --> 00:16:46.020]   What's what's going on? Thanks, Mateo. You've you've confused
[00:16:46.020 --> 00:16:50.820]   us all. Anyways, moving on with the real stuff. So this is how
[00:16:50.820 --> 00:16:54.420]   we apply max pooling. And we apply it to both of these layers.
[00:16:54.420 --> 00:17:02.900]   And what goes on from here, we just take the max of the n four
[00:17:02.900 --> 00:17:08.420]   and where to start. Okay, I'm getting confused, a bit
[00:17:08.420 --> 00:17:15.940]   confused. Sorry, guys. I just want to look at how are we
[00:17:15.940 --> 00:17:19.780]   arriving at this number. And I am not able to understand that.
[00:17:19.780 --> 00:17:24.740]   I'm sorry, I'm confusing myself with the last layer outputs.
[00:17:24.740 --> 00:17:33.980]   And there's no calculation happening either. I'll have to
[00:17:33.980 --> 00:17:37.060]   check this again. I'm sorry. It's it's just I'm just confused
[00:17:37.060 --> 00:17:43.180]   about this. But yeah, I wanted to just showcase these
[00:17:43.180 --> 00:17:47.380]   operations in Excel. And again, I'm sorry for not remembering
[00:17:47.380 --> 00:17:51.460]   the last year details. But we take a number seven or three or
[00:17:51.460 --> 00:17:56.060]   Swift programming languages icon on which we apply two filters
[00:17:56.060 --> 00:17:59.300]   and followed by that we applied two more layers of filter with
[00:17:59.300 --> 00:18:02.500]   two filters in them, which gives us this output, followed by
[00:18:02.500 --> 00:18:06.220]   which we apply max pooling, which just gives us this output.
[00:18:06.220 --> 00:18:10.220]   And when we were talking about receptive fields earlier, right?
[00:18:10.860 --> 00:18:13.020]   This is what we were talking about. So this particular
[00:18:13.020 --> 00:18:17.940]   number is looking at these numbers, which further are
[00:18:17.940 --> 00:18:22.660]   looking at quite a few of them. So this one particular max pool
[00:18:22.660 --> 00:18:26.620]   number or this number inside of this layer might even be looking
[00:18:26.620 --> 00:18:31.060]   at these many numbers. Remember the Zeiler and Fergus paper we
[00:18:31.060 --> 00:18:34.780]   had looked at? That's what they had showcased us also. So as we
[00:18:34.780 --> 00:18:39.380]   go inside of the layers, every single weight, every single
[00:18:39.380 --> 00:18:44.740]   number represents further more details. Weights are just
[00:18:44.740 --> 00:18:49.540]   numbers that that the model learns. And every single number
[00:18:49.540 --> 00:18:52.900]   comes from some calculation, mostly matrix multiplication.
[00:18:52.900 --> 00:18:57.740]   That's how we get these numbers. So this was a high level recap
[00:18:57.740 --> 00:19:03.820]   of CNNs. I'll take a few more sips of chai and go back to the
[00:19:03.820 --> 00:19:08.580]   questions if there are any. I just wanted to quickly recap how
[00:19:08.580 --> 00:19:11.780]   to CNNs work. Okay, I don't see any questions. I'll take
[00:19:11.780 --> 00:19:12.300]   another sip.
[00:19:12.300 --> 00:19:32.140]   Sorry about that. I've observed that I tend to, my throat tends
[00:19:32.140 --> 00:19:37.060]   to dry up a lot. That's why I'll be taking more sips of water.
[00:19:37.460 --> 00:19:40.180]   This time, let me find that link for you. I think someone had
[00:19:40.180 --> 00:19:54.300]   shared it in the last study group. I'm sorry to be doing
[00:19:54.300 --> 00:20:04.860]   this live. I recall it should be somewhere around here. I'm
[00:20:04.860 --> 00:20:07.980]   sorry, I'll post it after the study group. It's there in the
[00:20:07.980 --> 00:20:13.780]   Excel, FastA Excel sheet. And I was just blanking out because
[00:20:13.780 --> 00:20:18.620]   Adil has already posted it. Thanks, Adil. Awesome. So we're
[00:20:18.620 --> 00:20:27.940]   ready to move on. Click to exit full screen. Again, this was
[00:20:27.940 --> 00:20:31.140]   from FastA. So thanks, Jeremy, for creating that Excel sheet
[00:20:31.140 --> 00:20:34.780]   for all of us to learn from. To recap things from the last
[00:20:34.780 --> 00:20:37.940]   study group. And again, as a reminder, I followed this
[00:20:37.940 --> 00:20:41.580]   different order of things where I just recapped this thing, a
[00:20:41.580 --> 00:20:45.260]   concept that we'd studied six weeks ago. And that is to
[00:20:45.260 --> 00:20:47.740]   constantly remind you of different things and try to
[00:20:47.740 --> 00:20:51.060]   embed those in your memory. So to the people joining in, you're
[00:20:51.060 --> 00:20:54.700]   still very welcome to ask any questions at all. We'd love to
[00:20:54.700 --> 00:20:58.100]   answer them. But we're quite into this. And this is the
[00:20:58.100 --> 00:21:02.860]   seventh or eighth session for us depending on how you count. So
[00:21:02.860 --> 00:21:06.140]   we learned about those importance of having your
[00:21:06.140 --> 00:21:10.460]   graphic card setup. And I'd actually joined the study group
[00:21:10.460 --> 00:21:15.260]   I had mentioned a few minutes ago with Ravi and Koryan. And we
[00:21:15.260 --> 00:21:19.060]   spent three hours just trying to get something up and running,
[00:21:19.060 --> 00:21:22.220]   which I know for a fact can be installed in the cloud in just
[00:21:22.220 --> 00:21:26.220]   two minutes. So that's the trade off you tend to make with local
[00:21:26.220 --> 00:21:31.300]   setups and different issues pop up. So the gist of it was
[00:21:31.980 --> 00:21:36.140]   always use the easiest option. Don't compare the charges too
[00:21:36.140 --> 00:21:40.460]   much. And don't become an IT admin like I have. Maybe I
[00:21:40.460 --> 00:21:44.100]   should just change my title from deep learning engineer to IT
[00:21:44.100 --> 00:21:50.700]   admin. Honestly, because based on just how many issues I keep
[00:21:50.700 --> 00:21:53.540]   running into with installation, maybe I just do something really
[00:21:53.540 --> 00:21:58.380]   stupid. But you do end up wasting a lot of time there. And
[00:21:58.380 --> 00:22:00.740]   make sure the setup is flawless. So whenever you want to
[00:22:00.740 --> 00:22:04.500]   experiment with an idea, that's what you should be doing. I
[00:22:04.500 --> 00:22:07.540]   really want to emphasize this. So I'm again, reiterating
[00:22:07.540 --> 00:22:11.420]   reiterating these points from the last session. Just wanted to
[00:22:11.420 --> 00:22:14.020]   get those out of the way. And we've done a Kaggle walkthrough
[00:22:14.020 --> 00:22:17.900]   which also I just repeated in the beginning of this session.
[00:22:17.900 --> 00:22:21.700]   So this is the right time for you all to hop onto Kaggle and
[00:22:21.700 --> 00:22:24.940]   just start messing with things. You don't need to be in the top
[00:22:24.940 --> 00:22:29.260]   10. You probably wouldn't be. If you would be please stop
[00:22:29.260 --> 00:22:33.140]   joining my sessions and ping me separately so that I'll invite
[00:22:33.140 --> 00:22:37.420]   you to host the next one. Thank you. But don't worry about being
[00:22:37.420 --> 00:22:41.980]   in the top 10. Don't worry about being in the top 100. Just start
[00:22:41.980 --> 00:22:44.820]   by messing around and having fun. Start with the things you
[00:22:44.820 --> 00:22:48.780]   know and just go from there and you can't possibly know all of
[00:22:48.780 --> 00:22:53.020]   these things. So you need to start somewhere. So as a
[00:22:53.020 --> 00:22:57.180]   reminder, we're working on the Luna 16 data set. And these were
[00:22:57.180 --> 00:23:00.940]   those steps that we had identified after reading 10
[00:23:00.940 --> 00:23:06.780]   chapters. This is the gist of those. We need five steps. The
[00:23:06.780 --> 00:23:10.460]   first one would be data loading. Second one would be an ability
[00:23:10.460 --> 00:23:16.620]   to segment, maybe group the tumors, classify them and then
[00:23:16.620 --> 00:23:20.740]   analyze them. Now let me exit my full screen.
[00:23:20.740 --> 00:23:27.300]   And close the unnecessary tabs.
[00:23:27.300 --> 00:23:47.620]   So this is a direct fork of the code repository from the
[00:23:47.620 --> 00:23:52.580]   authors. And we'll be looking at p2ch 11 today. That is chapter
[00:23:52.580 --> 00:23:56.340]   11. I'll just be highlighting different code sections. That's
[00:23:56.340 --> 00:23:58.900]   what we tend to follow. And I'll highlight the things that are
[00:23:58.900 --> 00:24:02.580]   quite important. But I want to I'll stop sharing my screen and
[00:24:02.580 --> 00:24:05.860]   reiterate these things. So if you remember looking at the
[00:24:05.860 --> 00:24:12.060]   Luna 16 data set, there were two different files. One was
[00:24:12.060 --> 00:24:16.820]   annotations. Another one was classification dot CSV. And all
[00:24:16.820 --> 00:24:19.780]   of these had different information that we figured out
[00:24:19.780 --> 00:24:24.180]   how to integrate together and create a data set class
[00:24:24.180 --> 00:24:28.180]   together. We created I think Luna data set and a CT class in
[00:24:28.180 --> 00:24:32.260]   the last session. If you weren't part of it, just look at the
[00:24:32.260 --> 00:24:35.860]   chapter 10 code. It's quite straightforward. If it's not to
[00:24:35.860 --> 00:24:40.500]   you, that's totally possible. Ask, ask any questions. But so
[00:24:40.500 --> 00:24:44.380]   far, we figured out how to get this messy data set. So first of
[00:24:44.380 --> 00:24:49.500]   all, we have it's better if I share the one notes in and
[00:24:49.500 --> 00:24:52.500]   write in there. Sorry, I should. I should probably have
[00:24:52.500 --> 00:25:01.980]   done that already. Let me grab my pen. And right in the corner
[00:25:01.980 --> 00:25:17.220]   somewhere. One moment. I like red goes with Pytorch. So we
[00:25:17.220 --> 00:25:23.620]   had annotations dot CSV. I won't write the complete name we had,
[00:25:23.620 --> 00:25:30.380]   I think, candidates dot CSV. And we had a bunch of MST files,
[00:25:30.820 --> 00:25:34.900]   and a few more files that I won't be able to recall. I
[00:25:34.900 --> 00:25:37.660]   won't embarrass myself by mentioning that fact, but I
[00:25:37.660 --> 00:25:41.900]   already did it. So that's all good. So what we were trying to
[00:25:41.900 --> 00:25:44.900]   achieve here was we were trying to figure out a way to first of
[00:25:44.900 --> 00:25:52.020]   all, connect these to find the correct MST file and create a
[00:25:52.020 --> 00:25:57.740]   data set file from these. From there, we were to save this
[00:25:57.740 --> 00:26:04.500]   class in a module, and then just call d set dot by. So ideally,
[00:26:04.500 --> 00:26:07.380]   whenever you're working on a deep learning or machine
[00:26:07.380 --> 00:26:11.720]   learning project, you'd want your code to be structured in a
[00:26:11.720 --> 00:26:14.740]   certain format. Here's how I like it. It's quite open ended.
[00:26:14.740 --> 00:26:19.180]   So you can take what you want from here. But this is what I
[00:26:19.180 --> 00:26:23.580]   tend to do. So I have a code repo, I have a data repo
[00:26:23.580 --> 00:26:27.780]   separately inside of the code repo. And this, if I'm having a
[00:26:27.780 --> 00:26:34.340]   good day gets tracked to get if not, I try to do something that
[00:26:34.340 --> 00:26:38.780]   I usually forget in two days, I in the best case, scenario, you
[00:26:38.780 --> 00:26:41.420]   should be tracking this to get and making sure you commit
[00:26:41.420 --> 00:26:45.020]   every day. Inside of the code file, usually you have a data
[00:26:45.020 --> 00:26:50.980]   set. Dot by oops, zoom is coming in the way. Let me move this
[00:26:50.980 --> 00:27:00.460]   out. Okay, sorry about that. You would have a train dot by
[00:27:00.460 --> 00:27:10.180]   maybe utilities. Let's see what else visualization dot by that
[00:27:10.180 --> 00:27:14.300]   let you visualize stuff. And a few more files. So you'd want
[00:27:14.300 --> 00:27:17.820]   your code to be modular, so that people can iterate upon it.
[00:27:17.820 --> 00:27:20.900]   Usually you're working in a team or if you're working by
[00:27:20.900 --> 00:27:24.820]   yourself, then it's, it's even harder, believe me. So you would
[00:27:24.820 --> 00:27:28.460]   want, you know, these modules to be separate. So you don't want
[00:27:28.460 --> 00:27:33.660]   the data set to be messing up the train file. In a very ideal
[00:27:33.660 --> 00:27:37.660]   world, assuming you even managed to, most of you would, you're
[00:27:37.660 --> 00:27:40.660]   not like me. So if you're using git, the next step would be to
[00:27:40.660 --> 00:27:47.420]   have some form of unit tests or different tests, where you can
[00:27:47.420 --> 00:27:51.540]   ensure your code works like it should. The authors don't do
[00:27:51.540 --> 00:27:55.620]   that. In an ideal case, you should. But what's a really good
[00:27:55.620 --> 00:27:59.500]   test for that case is just making sure you run for a very
[00:27:59.500 --> 00:28:03.580]   small subset of your data. And if things work like they should,
[00:28:03.580 --> 00:28:09.340]   you're quite good. Someone's asking, have you analyzed the
[00:28:09.340 --> 00:28:12.580]   tumor classification problem? Yes, we're in the middle of it.
[00:28:12.580 --> 00:28:15.580]   And I would request you to ask the question on the link that's
[00:28:15.580 --> 00:28:23.940]   pinned in the YouTube chart. So, so far, we've done step one and
[00:28:23.940 --> 00:28:29.420]   step two. Step one was to answer your question. I should look at
[00:28:29.420 --> 00:28:34.420]   the name to answer your question. We had looked at step
[00:28:34.420 --> 00:28:39.060]   one data loading, which skipped segmentation, and we're jumping
[00:28:39.060 --> 00:28:43.900]   to classification. So not not the most ideal workflow, but
[00:28:43.900 --> 00:28:46.780]   we're currently on step four. And this is the second thing
[00:28:46.780 --> 00:28:50.140]   we're looking at. So far, we just have a data loading class
[00:28:50.140 --> 00:28:55.060]   that's ready for us. So from the data loader, we'd have
[00:28:55.060 --> 00:28:59.980]   candidates, some samples and a classification model. And as a
[00:28:59.980 --> 00:29:07.980]   target, we just want to get a positive or a negative output
[00:29:07.980 --> 00:29:13.100]   from here. I'll probably jump again towards the end of this
[00:29:13.540 --> 00:29:17.180]   lecture. And I'll ask this question. So towards the end, in
[00:29:17.180 --> 00:29:24.180]   this particular chapter, we get a 97 some percent accuracy. And
[00:29:24.180 --> 00:29:28.460]   that is actually quite bad. So would anyone like to take a jab
[00:29:28.460 --> 00:29:38.180]   at explaining why this is not a good accuracy number. And this,
[00:29:38.180 --> 00:29:41.860]   I'll give a hint, this depends on the data set. So the hint is
[00:29:41.860 --> 00:29:44.620]   it's because of the data set. And we're just looking at
[00:29:44.620 --> 00:29:50.460]   accuracy. We've also set up a proper validation data set for
[00:29:50.460 --> 00:29:53.980]   this particular example. And we followed some good steps. The
[00:29:53.980 --> 00:29:58.860]   question is, why is this high amount of accuracy not good
[00:29:58.860 --> 00:30:06.900]   enough? Let me jump to Safari where I'll see the answers. So
[00:30:06.900 --> 00:30:17.260]   why is the high number not good enough? That's a great answer.
[00:30:17.260 --> 00:30:20.740]   Ravi, what do you mean by imbalanced data sets? Could you
[00:30:20.740 --> 00:30:27.300]   please elaborate real quick? And I'll wait for Mateo to answer.
[00:30:27.300 --> 00:30:42.580]   To the people that watch the recap of this, I'm sorry,
[00:30:42.580 --> 00:30:47.260]   there's this lag that happens. But we have people joining live
[00:30:47.260 --> 00:30:49.740]   who are actually writing I'm speaking. So it's not an
[00:30:49.740 --> 00:30:53.940]   immediate interaction. That's why it takes 40-50 seconds. I
[00:30:53.940 --> 00:30:56.540]   know it's not a lot for you. But I want this to be the best
[00:30:56.540 --> 00:30:59.060]   experience for you. So sometimes there's a small delay that
[00:30:59.060 --> 00:31:02.740]   happens because of this. And sorry about that. But I want to
[00:31:02.740 --> 00:31:07.220]   keep the session interactive for people that join live. In an
[00:31:07.220 --> 00:31:10.580]   imbalanced data set, we have many, many, many more data
[00:31:10.580 --> 00:31:13.660]   points of one category. That's that's a perfect description. So
[00:31:13.660 --> 00:31:17.220]   for a data set where we have a lot of examples of just one
[00:31:17.220 --> 00:31:23.620]   category, accuracy is not a good metric. And Mateo gives another
[00:31:23.620 --> 00:31:28.380]   perspective. So because we have most examples of one class, it's
[00:31:28.380 --> 00:31:33.500]   not a good metric. That's correct. When percentage age is
[00:31:33.500 --> 00:31:39.100]   one class is quite more than compared to other. Sorry, when
[00:31:39.100 --> 00:31:42.700]   percentage of one class is quite more than others. Yep, that's
[00:31:42.700 --> 00:31:46.700]   right. So the example given by the authors towards the end of
[00:31:46.700 --> 00:31:51.020]   the chapter is imagine there's this test. I've not come up with
[00:31:51.020 --> 00:31:53.220]   this example. This is from the book. I'm just I'm just trying
[00:31:53.220 --> 00:31:57.940]   to act smart while I recall it. So imagine a test of true and
[00:31:57.940 --> 00:32:02.820]   false. We all have gone through that pain, if I may, during
[00:32:02.820 --> 00:32:06.220]   university where you had to answer those questions,
[00:32:06.220 --> 00:32:10.580]   hopefully without our peers help, although I would quite
[00:32:10.580 --> 00:32:16.620]   rely on that. So history aside, imagine a test where almost all
[00:32:16.620 --> 00:32:21.260]   of the answer are always false. And there are two students one
[00:32:21.260 --> 00:32:25.380]   that has really learned and tries to go through the
[00:32:25.380 --> 00:32:28.380]   questions and answers them. Let's say I'm the other one who
[00:32:28.380 --> 00:32:31.740]   just looks at their answers and say, I see most of the false
[00:32:31.740 --> 00:32:35.620]   answers. Let's mark them false. Couldn't couldn't go wrong
[00:32:35.620 --> 00:32:41.180]   there. I put my faith in the other person more. So even if
[00:32:41.180 --> 00:32:45.940]   you just mark false answers for everything, where almost all of
[00:32:45.940 --> 00:32:49.180]   the answers are false, you're still right, you still pass the
[00:32:49.180 --> 00:32:52.900]   test. And that is known as overfitting. And that's what's
[00:32:52.900 --> 00:32:56.580]   happening in this case. So since almost all of the examples,
[00:32:56.580 --> 00:33:02.140]   don't have a tumor, first of all, and don't have a cancerous
[00:33:02.140 --> 00:33:06.540]   tumor. Even if you by default, mark all of them as negative,
[00:33:06.540 --> 00:33:09.740]   you'll have a good solution. I have stopped sharing my screen
[00:33:09.740 --> 00:33:12.220]   because I tried to emphasize on these points. This this is
[00:33:12.220 --> 00:33:16.900]   quite common in Kaggle. Also, so we always talk about having a
[00:33:16.900 --> 00:33:21.900]   good baseline, having a simple idea as your baseline. A good
[00:33:21.900 --> 00:33:27.820]   baseline in this case, would be having a model predict
[00:33:27.820 --> 00:33:32.980]   everything is false. Or maybe if you do some ED and let's say,
[00:33:32.980 --> 00:33:38.900]   95% of the times there's no cancerous tumor, your simple
[00:33:38.900 --> 00:33:43.940]   baseline should just start there. So maybe do a random
[00:33:43.940 --> 00:33:49.500]   prediction where 95% of the time you get the answer no. And use
[00:33:49.500 --> 00:33:52.740]   that accuracy as your baseline. So what do I mean by a baseline?
[00:33:52.740 --> 00:33:57.060]   Now you have this metric, which is the most stupid idea possible,
[00:33:57.060 --> 00:34:00.820]   right? Basically saying every tumor is not cancerous, which is
[00:34:00.820 --> 00:34:05.220]   not true. But it's quite true because of our data set. And now
[00:34:05.220 --> 00:34:08.860]   you start acting a convolutional neural network on top of it that
[00:34:08.860 --> 00:34:13.900]   predicts few things you add some tips, sorry, add some tricks, I
[00:34:13.900 --> 00:34:16.020]   was going to say tips and tricks, you add some tricks
[00:34:16.020 --> 00:34:21.300]   there. And your score improves. But now you have this baseline,
[00:34:21.300 --> 00:34:26.020]   right? Let's, let's share my screen and look at it. And we'll
[00:34:26.020 --> 00:34:29.300]   look at the right matrix in the chapter, I think after this or
[00:34:29.300 --> 00:34:33.780]   the one after that. I've obviously read ahead. So I'm
[00:34:33.780 --> 00:34:36.820]   spoiling things for you. Sorry about that. But this is not a
[00:34:36.820 --> 00:34:42.260]   TV season. So I think I'm fine there. But let's say your silly
[00:34:42.260 --> 00:34:48.260]   model. And hopefully all are adjusted to my acting right now.
[00:34:48.260 --> 00:34:51.220]   I'm sorry, you'll have to go through this every week. But
[00:34:51.220 --> 00:34:54.860]   let's say your silly model gets in. And right now we're still
[00:34:54.860 --> 00:34:58.940]   looking at accuracy, a good metric would be F1. If you want
[00:34:58.940 --> 00:35:03.060]   to read about this, go to the documentation and don't look at
[00:35:03.060 --> 00:35:07.620]   blog posts. Always remember, our docs are better than blog posts
[00:35:07.620 --> 00:35:11.660]   unless the dogs don't explain things to you. For pytos, they
[00:35:11.660 --> 00:35:16.420]   should. So right now we're not looking at F1 score, we're just
[00:35:16.420 --> 00:35:20.020]   looking at accuracy. And let's say our silly model gets us 97%
[00:35:20.020 --> 00:35:23.860]   accuracy. For some reason, it should be like in an ideal world
[00:35:23.860 --> 00:35:26.740]   for the right metric, it should be a very small number. But
[00:35:26.740 --> 00:35:30.500]   let's see you get this number out of it. And now, the first
[00:35:30.500 --> 00:35:34.780]   idea would be to add a CNN model on top, that gets you 98%
[00:35:34.780 --> 00:35:40.380]   accuracy. Let's say you find a very, very fancy paper that
[00:35:40.380 --> 00:35:44.580]   claims to be incredibly highly accurate. And you decide to use
[00:35:44.580 --> 00:35:49.260]   that and that gives you 89% accuracy. Just using that
[00:35:49.260 --> 00:35:52.620]   without other things. Now you have this absolutely stupid
[00:35:52.620 --> 00:35:55.500]   model to compare things to right one that doesn't even learn
[00:35:55.500 --> 00:35:59.340]   anything that just says everything is negative. And it's
[00:35:59.340 --> 00:36:03.060]   lesser accurate than that. So that's why this baseline is
[00:36:03.060 --> 00:36:06.460]   quite important because it's the most stupid idea possible, most
[00:36:06.460 --> 00:36:10.700]   simple idea possible. And if you start there, you can never go
[00:36:10.700 --> 00:36:14.460]   wrong. So this becomes really easy to compare things against.
[00:36:14.460 --> 00:36:20.020]   And any score from there can be directly compared to this. And
[00:36:20.020 --> 00:36:25.100]   you can check if you're doing a good job there. How do you know
[00:36:25.100 --> 00:36:29.740]   if it is reliable, this is known as setting up a good CV. On
[00:36:29.740 --> 00:36:33.700]   Kaggle, you would see a lot of teams calling themselves trust
[00:36:33.700 --> 00:36:39.340]   your CV. There's a reason for that. And always, you need to
[00:36:39.340 --> 00:36:42.260]   set up a good enough validation data set those are nitty gritty
[00:36:42.260 --> 00:36:47.340]   details. You learn those through practice, of course, if that is
[00:36:47.340 --> 00:36:51.420]   in place, this is a good way of approaching things. And how do
[00:36:51.420 --> 00:36:54.620]   you track these I'll after this session for anyone that has
[00:36:54.620 --> 00:37:01.100]   joined or signed up, we'll be emailing you with a few examples
[00:37:01.140 --> 00:37:04.020]   or ways of getting started with weights and biases, which I'll
[00:37:04.020 --> 00:37:08.060]   cover in the next session. You can look at that. And I think
[00:37:08.060 --> 00:37:12.700]   using weights and biases to track your experiments is, is
[00:37:12.700 --> 00:37:15.340]   quite a good approach, because that's really what the tool is
[00:37:15.340 --> 00:37:21.260]   for. So here's what we're going to do today. Enough talk, time
[00:37:21.260 --> 00:37:24.860]   to look at code. But here's what we're going to do today. Inside
[00:37:24.860 --> 00:37:27.380]   of any model, ideally, I would should have asked this question
[00:37:27.380 --> 00:37:29.540]   on the forum, but I'll jump through because I think I've
[00:37:29.540 --> 00:37:32.500]   been quite detailed and thorough with the theory. So I should be
[00:37:32.500 --> 00:37:36.100]   looking at the code. We'll initialize our model, initialize
[00:37:36.100 --> 00:37:40.940]   our data loaders, loop over the epochs, also have a validation
[00:37:40.940 --> 00:37:44.940]   data set. Today, I won't be walking through how to log the
[00:37:44.940 --> 00:37:48.220]   matrix. So this part won't be covered by me. Instead, we'll be
[00:37:48.220 --> 00:37:54.980]   sending you some resources for weights and biases. And I'll
[00:37:54.980 --> 00:37:58.180]   cover this next week. So what we're going to do is we're going
[00:37:58.180 --> 00:38:02.060]   to create a model loop over the epochs. And inside of that we'll
[00:38:02.060 --> 00:38:06.260]   load, we'll load a batch, classify the batch, calculate
[00:38:06.260 --> 00:38:11.060]   the loss, check the matrix, update the weights. Will we do
[00:38:11.060 --> 00:38:15.380]   the same for validation? No. So we won't, we won't be updating
[00:38:15.380 --> 00:38:19.740]   the weights there. We'll just be keeping a track of them. Why is
[00:38:19.740 --> 00:38:22.420]   that important? That's been covered already. If you if you're
[00:38:22.420 --> 00:38:29.300]   not too sure, please feel free to ask. I'm trying to find the
[00:38:29.300 --> 00:38:37.180]   image of the model that has been drawn somewhere. So what I want
[00:38:37.180 --> 00:38:40.740]   to talk about next is what is the head? What is the tail and
[00:38:40.740 --> 00:38:43.500]   what is the backbone of the model, and I've managed to find
[00:38:43.500 --> 00:38:49.980]   that image. So the authors have decided to create a particular
[00:38:49.980 --> 00:38:53.540]   architecture that we'll just get into again. I'm talking about
[00:38:53.540 --> 00:38:56.700]   too much theory today, but it's quite required for what we're
[00:38:56.700 --> 00:39:02.900]   going to discuss. And any model can be defined in three
[00:39:02.900 --> 00:39:07.380]   different parts, the head, the backbone, the tail. Counter
[00:39:07.380 --> 00:39:12.420]   intuitively tail happens to be closest to input. For me, like
[00:39:12.420 --> 00:39:15.380]   it took a minute to understand this, especially the first time
[00:39:15.380 --> 00:39:19.620]   I heard of it and head is closer to the output. So whenever you
[00:39:19.620 --> 00:39:25.460]   hear about regression heads, or taking a model and adding
[00:39:25.460 --> 00:39:30.780]   multihead to it, multihead transformer problems. If you've
[00:39:30.780 --> 00:39:32.780]   been on Twitter, you would have seen those words floating
[00:39:32.780 --> 00:39:37.540]   around head refers to the layers closer to the output. So
[00:39:37.540 --> 00:39:41.100]   whenever you are messing around with the head, you're trying to
[00:39:41.100 --> 00:39:46.380]   change the output of the model. And if you want a regression
[00:39:46.380 --> 00:39:48.580]   problem, this could be a regression head. So now you get
[00:39:48.580 --> 00:39:52.300]   numbers out of it, let's say you get a percentage probability,
[00:39:52.300 --> 00:39:57.860]   which which is the case for us as well. The backbone is
[00:39:57.860 --> 00:40:02.260]   somewhat replaceable. Neural networks aren't human. So we can
[00:40:02.260 --> 00:40:07.020]   replace the backbones in in a way. What do I mean by that?
[00:40:07.020 --> 00:40:11.660]   This is your homework to do. You need to switch this out for
[00:40:11.660 --> 00:40:17.660]   ResNet or try a library, which I'll quickly point out and try
[00:40:17.660 --> 00:40:20.460]   to change this backbone in an attempt to improve upon the
[00:40:20.460 --> 00:40:24.340]   accuracy. You mostly wouldn't but this is a good exercise to
[00:40:24.340 --> 00:40:28.860]   just get a hold of how to fit in different models. And tail is
[00:40:28.860 --> 00:40:32.660]   closest to our input. So we learn how to load the data set
[00:40:32.660 --> 00:40:37.540]   there. This is what a lunar model class basically consists
[00:40:37.540 --> 00:40:41.980]   of. And I think I've covered pretty much all of the theory I
[00:40:41.980 --> 00:40:48.180]   needed to to now start covering the code. So I'll switch back to
[00:40:48.180 --> 00:40:54.140]   Safari. Take a look at the questions and start the code
[00:40:54.140 --> 00:40:54.580]   walkthrough.
[00:40:54.580 --> 00:41:20.620]   I'll, sorry about that. I'll answer your question Ravi with
[00:41:21.300 --> 00:41:33.860]   one reply from one of the best Kagglers I know. I saw their
[00:41:33.860 --> 00:41:53.460]   tweet recently. So I'm going to point that out. They have been
[00:41:53.460 --> 00:41:59.820]   tweeting a lot. So I'll try to find that tweet later. But
[00:41:59.820 --> 00:42:03.100]   basically the gist of it was they've spent a lot of time on
[00:42:03.100 --> 00:42:08.140]   Kaggle. CPMP is one of the best Kagglers out there. And the one
[00:42:08.140 --> 00:42:11.220]   thing they mentioned they've learned from Kaggle is try ideas
[00:42:11.220 --> 00:42:15.780]   instead of just asking if they work well. So not a personal
[00:42:15.780 --> 00:42:18.780]   take on you. But if you think of an idea, it's always good to
[00:42:18.780 --> 00:42:24.020]   just jump on and try that idea. Theoretically, we can never know
[00:42:24.020 --> 00:42:27.420]   for a given data set if that thing should work. I think
[00:42:27.420 --> 00:42:31.780]   that's where the science for data science comes in. But
[00:42:31.780 --> 00:42:35.580]   theoretically speaking, is it a good idea to use few samples
[00:42:35.580 --> 00:42:42.820]   from majority class? Depends, right? Some people also claim
[00:42:42.820 --> 00:42:47.780]   that with not an insane amount of imbalance, models still work
[00:42:47.780 --> 00:42:51.300]   quite well. How can you be sure by just spinning up an
[00:42:51.300 --> 00:42:56.500]   experiment? So, Ravi, this has become a homework for you. A
[00:42:56.500 --> 00:43:00.380]   suggested homework. Please, please try this and tell us if
[00:43:00.900 --> 00:43:02.180]   this works better for you.
[00:43:02.180 --> 00:43:18.700]   Would making a custom loss function also fix this like
[00:43:18.700 --> 00:43:23.500]   making wrong classification of a bad? Yeah, totally. And we'll
[00:43:23.500 --> 00:43:29.580]   be looking at a different metric next week. I'm trying to
[00:43:29.620 --> 00:43:33.220]   consider if I should explain it today. Not really. But we'll be
[00:43:33.220 --> 00:43:36.740]   looking at how to create a better or use a better loss
[00:43:36.740 --> 00:43:40.700]   function for these things. Great question, though. Thank you.
[00:43:40.700 --> 00:43:51.380]   Thank you, Mario. Okay, so time to look at the different
[00:43:51.380 --> 00:43:55.620]   modules. And what I'll try to do again here is try to copy them
[00:43:55.620 --> 00:44:00.260]   line by line and explain the same. As a reminder, we're
[00:44:00.260 --> 00:44:04.740]   looking at the code repo from the authors and we're inside of
[00:44:04.740 --> 00:44:13.140]   p2ch11 folder inside of which we have the following files. Last
[00:44:13.140 --> 00:44:19.860]   week, we had looked at bsets.py. I'll quickly recap this. But we
[00:44:19.860 --> 00:44:23.020]   try to get the candidate info and there's a reason for that
[00:44:23.060 --> 00:44:26.520]   because we have separate MSD files. So these are medical
[00:44:26.520 --> 00:44:31.460]   headers, we have candidate data set. And we also had an
[00:44:31.460 --> 00:44:35.940]   annotation folder. So we get all of the info from here, combine
[00:44:35.940 --> 00:44:44.100]   it together, create a file that can grab their centers. We get
[00:44:44.100 --> 00:44:49.140]   the raw candidate info in CT data set in sorry, in a CT scan.
[00:44:50.060 --> 00:44:52.940]   Finally, we had created this lunar data set class that
[00:44:52.940 --> 00:44:57.860]   satisfied the basic two functions of any pytorch data
[00:44:57.860 --> 00:45:02.120]   set, which would be to return the length and an item. So
[00:45:02.120 --> 00:45:06.420]   whenever you create a data set file, sorry, data set class in
[00:45:06.420 --> 00:45:10.340]   pytorch, you need to return the length and item. And we
[00:45:10.340 --> 00:45:13.860]   basically backtrack from there, right? So how do we return one
[00:45:13.860 --> 00:45:21.220]   given CT scan for this data set. So we have a raw image, we have
[00:45:21.220 --> 00:45:24.620]   our medical image, we have the annotations, we have the
[00:45:24.620 --> 00:45:28.820]   candidate information, we need to combine those together and be
[00:45:28.820 --> 00:45:33.820]   able to iterate on them. That's what we do. Returning the length
[00:45:33.820 --> 00:45:36.660]   is fairly straightforward, doesn't take up too many
[00:45:36.660 --> 00:45:41.020]   experiments, we just create an info list and return that. But
[00:45:41.020 --> 00:45:43.780]   we create all of these functions for the other step, which is
[00:45:43.780 --> 00:45:49.100]   to return the item. So we had looked at this last week, which
[00:45:49.100 --> 00:45:53.140]   is why I just swished through it. If you don't recall or have
[00:45:53.140 --> 00:46:07.260]   any questions, please feel free to ask. Okay, let's look at I
[00:46:07.260 --> 00:46:10.740]   think the model would be forced or I believe the training might
[00:46:10.740 --> 00:46:17.860]   be first. Let's look at the model first. So I'm gonna take
[00:46:17.860 --> 00:46:29.660]   the first few functions and copy these over. Sorry, let me if
[00:46:29.660 --> 00:46:31.980]   you see my mouse hovering, that's because I'm trying to get
[00:46:31.980 --> 00:46:35.700]   these zoom things out of the way. Sorry about that. So let's
[00:46:35.700 --> 00:46:39.060]   paste this we've imported all of the files. I won't talk about
[00:46:39.060 --> 00:46:42.100]   logging because we'll cover it next week. And I'll give a
[00:46:42.100 --> 00:46:46.860]   tutorial about how can you use weights and biases there. We
[00:46:46.860 --> 00:46:52.500]   create a lunar module model class after it. Let's copy this
[00:46:52.500 --> 00:46:57.540]   complete file over and understand it line by line. So
[00:46:57.540 --> 00:47:00.900]   we create a lunar model, which is the model that I had just
[00:47:00.900 --> 00:47:05.700]   showcased to you all. As with anything we're inheriting from
[00:47:05.700 --> 00:47:09.100]   an in module class. So we also have to close, call the
[00:47:09.100 --> 00:47:14.260]   initializer, the constructor of the superclass. That's what we
[00:47:14.260 --> 00:47:18.620]   do here. There's just one input channel and eight convolution
[00:47:18.620 --> 00:47:23.620]   channels. So we set the batch norm for the tail to be batch
[00:47:23.620 --> 00:47:29.220]   norm 3d. And we define four blocks that takes the input
[00:47:29.220 --> 00:47:33.440]   channels, passes on the convolutional channels and
[00:47:33.440 --> 00:47:36.860]   performs all of these operations. I won't go into the
[00:47:36.860 --> 00:47:40.380]   exact dimensions of these. This is quite covered in the chapter
[00:47:40.380 --> 00:47:43.220]   and I don't think it's quite relevant now. But again, you
[00:47:43.220 --> 00:47:47.000]   will have to run through this manually for be able for you to
[00:47:47.000 --> 00:47:50.720]   be able to understand. So towards the end of it, we get
[00:47:50.720 --> 00:47:57.180]   64 channel outputs. And we pass this through linear layer and a
[00:47:57.180 --> 00:48:04.280]   softmax layer. And we also define or just call in it
[00:48:04.280 --> 00:48:11.720]   weights. Let's look at what in it weights is. So for every
[00:48:11.720 --> 00:48:19.560]   single layer inside of modules, or if the layer is part of this,
[00:48:19.560 --> 00:48:24.800]   we call something known as timing underscore normal. Let's
[00:48:24.800 --> 00:48:29.520]   take a look at what that is. So known as timing, her
[00:48:29.520 --> 00:48:34.920]   initialization. This was one of the most impactful papers. The
[00:48:34.920 --> 00:48:38.040]   reason being, if you remember, we had talked a lot about
[00:48:38.040 --> 00:48:44.440]   normalizing weights, or normalizing inputs, where we'd
[00:48:44.440 --> 00:48:47.720]   want the mean to be zero and standard deviation to be one.
[00:48:47.720 --> 00:48:53.160]   And as long as your inputs or outputs are not sorry, as long
[00:48:53.160 --> 00:48:56.120]   as your weights, I'm sorry about that, as long as your weights
[00:48:56.120 --> 00:49:00.680]   aren't too large, aren't too small, your model gets strained.
[00:49:00.680 --> 00:49:04.080]   Otherwise, your weights just become zero or just reach
[00:49:04.080 --> 00:49:10.280]   infinity. This paper had shown you how to do that. And let me
[00:49:10.280 --> 00:49:13.440]   actually talk about one thing. So I've, I've received a few
[00:49:13.440 --> 00:49:17.600]   questions in this study group, particularly around when's a
[00:49:17.600 --> 00:49:21.000]   good time to start reading a paper? And how much time should
[00:49:21.000 --> 00:49:24.800]   you spend there? So I've actually gone through this paper,
[00:49:24.800 --> 00:49:27.280]   and it's one of the most incredible papers, of course.
[00:49:27.280 --> 00:49:32.800]   But I want to emphasize on the implementation part here. So if
[00:49:32.800 --> 00:49:36.800]   you look at this, and I'm just trying to find the main
[00:49:36.800 --> 00:49:40.440]   mathematical portion where things are explained. In
[00:49:40.440 --> 00:49:44.440]   academia, a lot of the details are explained mathematically.
[00:49:44.440 --> 00:49:50.040]   And personally, I've always been extremely terrified of math. And
[00:49:50.040 --> 00:49:55.600]   my trauma goes back to failing a math class. Sometime, which I
[00:49:55.600 --> 00:49:58.640]   did pass, it was just a small test that I had failed since
[00:49:58.640 --> 00:50:02.760]   then, I've been absolutely terrified of it. My point being
[00:50:02.760 --> 00:50:06.400]   if you look at all of these details, and if you look at the
[00:50:06.400 --> 00:50:09.880]   implementation inside of PyTorch, so PyTorch documentation
[00:50:09.880 --> 00:50:15.960]   allows you to go back to the source code as well. This is
[00:50:15.960 --> 00:50:23.680]   the implementation. Nine lines of code that don't even rely on
[00:50:23.680 --> 00:50:29.840]   C++. Sometimes in PyTorch, you can see the code going to C++.
[00:50:29.840 --> 00:50:38.600]   But just nine lines of code for an entire paper. Ideally,
[00:50:38.600 --> 00:50:42.600]   eventually, you should start reading papers, you should start
[00:50:42.600 --> 00:50:46.680]   understanding those ideas. And you should start writing these
[00:50:46.680 --> 00:50:52.920]   functions. But this matters in practice, quite a lot. So you
[00:50:52.920 --> 00:50:56.920]   should be able to understand this code function a lot. That's
[00:50:56.920 --> 00:51:01.640]   why I encourage you to translate TensorFlow or Keras notebooks to
[00:51:01.640 --> 00:51:07.280]   PyTorch first. Because again, even though this is a really
[00:51:07.280 --> 00:51:11.560]   impactful idea in code, it's just a few lines. And that's
[00:51:11.560 --> 00:51:15.600]   what really matters, right? The ability to implement these. In
[00:51:15.600 --> 00:51:18.160]   research, of course, if you're building on top of this, and
[00:51:18.160 --> 00:51:20.960]   even in industry, if you're building on top of things, it's
[00:51:20.960 --> 00:51:25.200]   quite important to be able to work with them. But most
[00:51:25.200 --> 00:51:28.880]   important thing is again, to be able to implement these ideas
[00:51:28.880 --> 00:51:32.800]   in code. So especially given the fact that you've just started,
[00:51:32.800 --> 00:51:39.600]   please consider spending time on code more. Okay, so let's look
[00:51:39.600 --> 00:51:43.280]   at what timing normal does fills the input tensor with values
[00:51:43.280 --> 00:51:46.840]   according to method described in delving deep into rectifiers.
[00:51:46.840 --> 00:51:49.880]   That's the name of the paper using a normal distribution.
[00:51:49.880 --> 00:51:55.400]   Resulting tensor will have values applied from mathematical
[00:51:55.400 --> 00:51:58.440]   symbols that terrify me it's actually a normal distribution
[00:51:58.440 --> 00:52:03.800]   from zero to standard division squared. Sorry, std squared
[00:52:03.800 --> 00:52:07.440]   where std is defined like so. This is known as hope
[00:52:07.840 --> 00:52:12.520]   initialization. That's how we pronounce the name. And it's
[00:52:12.520 --> 00:52:16.640]   just one single line that you call. And in this we can define
[00:52:16.640 --> 00:52:20.520]   the mode fan out preserves the magnitude in the backward pass.
[00:52:20.520 --> 00:52:26.320]   So inside of this definition, we've sent the moon as mode as
[00:52:26.320 --> 00:52:30.640]   fan out because we'd like to preserve the outputs. And if
[00:52:30.640 --> 00:52:35.520]   bias is not none, we'd also account for those and initialize
[00:52:35.520 --> 00:52:39.160]   those. So we initialize our weights so that our model can
[00:52:39.160 --> 00:52:45.520]   train better. And then we define the forward pass. Can anyone
[00:52:45.520 --> 00:52:48.720]   please answer this question again, a question to the live
[00:52:48.720 --> 00:52:54.240]   audience. Why are we calling simply batch norm even though we
[00:52:54.240 --> 00:53:00.920]   should be calling batch norm 3d here. This is one thing I
[00:53:00.920 --> 00:53:03.200]   realized while trying to implement it without looking at
[00:53:03.200 --> 00:53:06.040]   the code and I was trying to implement batch norm. I was
[00:53:06.040 --> 00:53:09.200]   writing batch norm 3d here. But what's the reason for not
[00:53:09.200 --> 00:53:23.280]   writing batch norm 3d? I love that username, by the way.
[00:53:23.280 --> 00:53:29.040]   That's the right answer. Mario, I'll share the paper
[00:53:29.040 --> 00:53:33.120]   afterwards. But if you head over to the documentation, so this
[00:53:33.120 --> 00:53:35.280]   is from the Pytorch documentation, and I'm just
[00:53:35.280 --> 00:53:43.920]   looking at, sorry, timing normal. From there, you can head
[00:53:43.920 --> 00:53:48.480]   over to the source code. So that's what I did there. My
[00:53:48.480 --> 00:53:54.480]   question was, why are we not using batch norm 3d? And since
[00:53:54.480 --> 00:53:56.680]   I don't see an answer, I'll answer that because we've
[00:53:56.680 --> 00:54:02.040]   already defined tail batch norm as batch norm 3d. These little
[00:54:02.040 --> 00:54:05.400]   details that tend to be mistakes, at least for me, are
[00:54:05.400 --> 00:54:08.400]   quite always interesting. So pointing that out for you all,
[00:54:08.400 --> 00:54:11.360]   since we've already defined that in the constructor, we just
[00:54:11.360 --> 00:54:16.320]   call that and not define batch norm 3d again. So in our
[00:54:16.320 --> 00:54:21.080]   forward pass, we just pass all of these outputs and we create
[00:54:21.080 --> 00:54:25.440]   a flat layer or we flatten our outputs from all of these
[00:54:25.440 --> 00:54:30.240]   blocks. And I'm not going into details because again, the
[00:54:30.240 --> 00:54:37.480]   output from here goes here. So we pass batch normal output to
[00:54:37.480 --> 00:54:42.040]   this block, which gets stored in block out, which further gets
[00:54:42.040 --> 00:54:45.560]   passed to block two and that becomes not next block out. So
[00:54:45.560 --> 00:54:51.200]   we're overwriting that. And we finally pass block out or block
[00:54:51.200 --> 00:54:55.840]   output and flatten it out so that we can apply a linear layer.
[00:54:56.800 --> 00:55:00.800]   And then we return this linear output and apply a softmax
[00:55:00.800 --> 00:55:04.640]   function to it. Why do we apply a softmax for probability?
[00:55:04.640 --> 00:55:09.320]   We've already discussed that. After that, we define a lunar
[00:55:09.320 --> 00:55:16.760]   block. So again, since we're importing an indoor module,
[00:55:16.760 --> 00:55:19.480]   we'll have to call the constructor of the superclass.
[00:55:19.480 --> 00:55:25.440]   That's what we do. From there, we define a 3d, conf 3d model
[00:55:25.520 --> 00:55:30.680]   where we define padding as one. We define the kernel size to be
[00:55:30.680 --> 00:55:34.920]   three. Why? I have no idea. That's what the authors decided.
[00:55:34.920 --> 00:55:38.560]   Another homework, play around with these values. See if you
[00:55:38.560 --> 00:55:42.640]   can meet them. See if you can actually even change this
[00:55:42.640 --> 00:55:47.960]   complete model. So instead of lunar model, please go to Tim.
[00:55:50.640 --> 00:55:55.680]   Tim is one of the best repositories on PyTorch image
[00:55:55.680 --> 00:55:58.360]   models. That's the name of the repository, simply one of the
[00:55:58.360 --> 00:56:05.080]   best. And suggested homework for everyone is to try to switch
[00:56:05.080 --> 00:56:09.160]   out the model from any of one of the models that makes the most
[00:56:09.160 --> 00:56:14.120]   sense. There's no rules. Just try to switch out the model for
[00:56:14.120 --> 00:56:16.480]   anything that you think would work better and try to make
[00:56:16.480 --> 00:56:20.880]   things work. That's what a practitioner's job is, right? So
[00:56:20.880 --> 00:56:26.160]   any model should be able to work here. And it's your job to
[00:56:26.160 --> 00:56:29.880]   figure out which one should and how do you make that to work?
[00:56:29.880 --> 00:56:34.560]   I'll take a sip of water with a one minute break.
[00:56:34.560 --> 00:57:00.720]   Great discussion on by having this cute data set might cause
[00:57:00.720 --> 00:57:06.560]   problems. Thanks for that answer. I'll continue further.
[00:57:06.560 --> 00:57:15.800]   So we define the lunar block after this, where for some
[00:57:15.800 --> 00:57:18.640]   reasons that are beyond me, and of course, these are totally
[00:57:18.640 --> 00:57:22.200]   legitimate decisions that have been made by the authors and we
[00:57:22.200 --> 00:57:25.600]   need to figure out why did they do it? And how do we figure
[00:57:25.600 --> 00:57:29.240]   those out by just trying different values? I'm assuming
[00:57:29.240 --> 00:57:32.200]   that's what the authors did as well. So define the first con
[00:57:32.200 --> 00:57:35.400]   layer, followed by the first activation layer for us, it's
[00:57:35.400 --> 00:57:40.040]   ReLU. Again, try something else here. Try ReLU, try something
[00:57:40.040 --> 00:57:43.520]   totally different. Something that you think may not just
[00:57:43.520 --> 00:57:46.560]   work. And if it does, that's an interesting discovery, right?
[00:57:46.560 --> 00:57:51.520]   So after the first activation layer, we repeat the same, pass
[00:57:51.520 --> 00:57:56.400]   it to another activation layer, followed by a Maxpool. And then
[00:57:56.400 --> 00:58:01.920]   again, define a forward function here and return that value. So
[00:58:01.920 --> 00:58:06.760]   this is the definition of lunar model and lunar block from the
[00:58:06.760 --> 00:58:14.520]   model.py module. After this, we'll look at training and let
[00:58:14.520 --> 00:58:23.080]   me get these out of the way. Okay, Mario might be dropping
[00:58:23.080 --> 00:58:29.000]   off. Thanks for joining us, Mario. No questions so far. I'll
[00:58:29.000 --> 00:58:35.680]   assume so. I'll continue further. So now we define the
[00:58:35.680 --> 00:58:38.560]   training loop for this. Remember, we've defined the
[00:58:38.560 --> 00:58:43.520]   datasets file, we've defined the model. Only thing left to do is
[00:58:43.520 --> 00:58:48.480]   define the training model. So again, we import all of these
[00:58:48.480 --> 00:58:53.240]   stops. Also import some utilities that the authors have
[00:58:53.240 --> 00:58:58.960]   defined, import the lunar dataset class from dsets. We
[00:58:58.960 --> 00:59:02.720]   import the lunar model from dot model. If you're new to
[00:59:02.720 --> 00:59:07.680]   Pythonic ways, we're just importing lunar model that we
[00:59:07.680 --> 00:59:11.680]   had literally just defined inside of model.py. So dot
[00:59:11.680 --> 00:59:15.520]   tells, hey, this is from the same folder where we're at. And
[00:59:15.520 --> 00:59:18.440]   this is the name of the module that we're looking at. So from
[00:59:18.440 --> 00:59:24.440]   model.py that is in the same folder where this file should
[00:59:24.440 --> 00:59:28.280]   be, Python, please, please import lunar model class from
[00:59:28.280 --> 00:59:32.280]   there. That's what this tells. I think the interpreter, yes,
[00:59:32.280 --> 00:59:37.760]   interpreter would be the right word. So from there, we define
[00:59:37.760 --> 00:59:42.200]   a training app. And the authors have made an interesting
[00:59:42.200 --> 00:59:46.920]   decision here. There are these two different schools, I would
[00:59:46.920 --> 00:59:51.320]   say, there's a good amount of overlap as well. But two sets of
[00:59:51.320 --> 00:59:55.920]   practitioners, the first like to experiment a lot, and pretty
[00:59:55.920 --> 00:59:59.400]   much try to do as much as they can inside of Jupyter
[00:59:59.400 --> 01:00:03.760]   Notebooks. I'm of that school personally. And there's another
[01:00:03.760 --> 01:00:08.000]   school that tries to start there and eventually create scripts.
[01:00:08.000 --> 01:00:12.920]   So creates different Pythonic files. So when you're doing the
[01:00:12.920 --> 01:00:16.920]   second one, you need to be able to pass inputs, right? You need
[01:00:16.920 --> 01:00:21.000]   to be able to pass, tell the program, hey, Python, this is
[01:00:21.000 --> 01:00:25.840]   where my data set is. Can you please train it for I don't
[01:00:25.840 --> 01:00:30.360]   know, 50,000 epochs if you have a lot of compute power. And for
[01:00:30.360 --> 01:00:33.880]   that, you need to be able to read in command line arguments.
[01:00:33.880 --> 01:00:40.160]   So for that, we use one of the standard ways we call sys args.
[01:00:40.640 --> 01:00:44.480]   And these are whenever you're running any command in bash or
[01:00:44.480 --> 01:00:52.560]   your terminal, you usually call model.py epochs. I don't know,
[01:00:52.560 --> 01:01:05.560]   32. batch size, space 256. Yep, whenever you're passing these,
[01:01:05.560 --> 01:01:08.080]   these are just command line arguments. Let me zoom in a
[01:01:08.080 --> 01:01:14.200]   bit. So we take in the number of workers, batch size, number of
[01:01:14.200 --> 01:01:17.360]   epochs, I'll skip TensorFlow, because we look at bits and
[01:01:17.360 --> 01:01:24.000]   batches instead, next week. And we also set a few functions
[01:01:24.000 --> 01:01:26.560]   that check if you have a GPU, which you should for this
[01:01:26.560 --> 01:01:30.720]   lesson. And we try to move it to the GPU if it actually does
[01:01:30.720 --> 01:01:34.760]   exist. We initialize our model and initialize our optimizer,
[01:01:34.760 --> 01:01:39.280]   where do we get those from? So those are actually grabbed from
[01:01:39.280 --> 01:01:43.840]   the model that we had just defined. So inside of init
[01:01:43.840 --> 01:01:49.720]   model, we call Luna model and move it to CUDA. If we have more
[01:01:49.720 --> 01:01:53.160]   than one CUDA device, which means if you are really rich and
[01:01:53.160 --> 01:01:55.720]   you have more than one GPU, which is quite impossible these
[01:01:55.720 --> 01:02:00.160]   days, you call something known as NN data parallel. What is,
[01:02:00.240 --> 01:02:09.400]   what is that? Let's see. I see the documentation above. Let's
[01:02:09.400 --> 01:02:13.600]   head over there. Implements data parallelism at module level.
[01:02:13.600 --> 01:02:20.760]   The container parallelizes the application of given module by
[01:02:20.760 --> 01:02:24.720]   splitting input across specified devices. In simple English, this
[01:02:24.720 --> 01:02:28.160]   takes your model and puts it across multiple graphic cards so
[01:02:28.160 --> 01:02:33.840]   that you can train it faster. In a very simplified fashion, of
[01:02:33.840 --> 01:02:37.400]   course, there's more to it. But I'll just leave it at that. Let
[01:02:37.400 --> 01:02:42.240]   me close the unnecessary tabs. So we define our optimizer, the
[01:02:42.240 --> 01:02:46.720]   author say SGD works well for this case, homework, fiddle
[01:02:46.720 --> 01:02:49.800]   around with learning rate, fiddle around with momentum, and try
[01:02:49.800 --> 01:02:52.960]   to change it to Adam. The authors have also suggested this
[01:02:52.960 --> 01:02:57.640]   homework, so it's not my original idea. Sebastian Ruder.
[01:02:57.640 --> 01:03:06.360]   I written an incredible blog post on overview of gradient
[01:03:06.360 --> 01:03:11.280]   descent optimization algorithms. So you need to read this and try
[01:03:11.280 --> 01:03:15.680]   as many of these as you can. And you can use weights and biases
[01:03:15.680 --> 01:03:20.560]   to track these experiments if you want. So here's the overview
[01:03:20.560 --> 01:03:25.720]   of the different gradient descent algorithms. Find as many
[01:03:25.720 --> 01:03:29.000]   as you can inside of PyTorch. Almost actually all of them
[01:03:29.000 --> 01:03:32.840]   should be there in PyTorch and try to change them. From there,
[01:03:32.840 --> 01:03:38.480]   we create a lunar data set and set the stride. What is stride?
[01:03:38.480 --> 01:03:43.240]   So every 10th input becomes a part of the validation data set.
[01:03:43.240 --> 01:03:46.560]   Just to remind you, this is how we are defined it in lunar data
[01:03:46.560 --> 01:03:53.760]   set. That means we have a 90% to 10% split across 90% being in
[01:03:53.760 --> 01:03:57.720]   the training data set, 10% being in the validation data set.
[01:03:57.720 --> 01:04:03.560]   Batch size comes from the command line because remember we
[01:04:03.560 --> 01:04:09.560]   had taken in a sysarg and we define a data loader. So this
[01:04:09.560 --> 01:04:13.960]   comes from PyTorch. And we do a few different things here. So
[01:04:13.960 --> 01:04:18.000]   we define the number of workers and we pin the memory. What
[01:04:18.000 --> 01:04:21.080]   just said, look that up in the documentation, please. It's
[01:04:21.080 --> 01:04:26.240]   quite straightforward. We define the validation data loader.
[01:04:26.240 --> 01:04:31.040]   Similarly, and I'll skip over the TensorFlow details. Now we
[01:04:31.040 --> 01:04:38.640]   define the epoch and we track the matrix together. So this is
[01:04:38.640 --> 01:04:43.920]   training matrix gradient. And we initialize this as torch zeros.
[01:04:43.920 --> 01:04:49.040]   Of metric size of the length of data set. And this should be on
[01:04:49.040 --> 01:04:54.480]   a GPU if you're training on a GPU. So we track the training
[01:04:54.480 --> 01:05:02.880]   matrix and perform batch iterations. After a batch is
[01:05:02.880 --> 01:05:06.880]   done, we would always step through the optimizer. I'm
[01:05:06.880 --> 01:05:09.040]   running through these details because I've covered these
[01:05:09.040 --> 01:05:12.720]   extensively many times earlier. But please feel free to ask any
[01:05:12.720 --> 01:05:16.440]   questions. I'll come back at them. We do the same for
[01:05:16.440 --> 01:05:20.520]   validation. But we skip one little detail, which is we don't
[01:05:20.520 --> 01:05:23.640]   update the weights because we're just interested in looking at
[01:05:23.640 --> 01:05:27.520]   the validation losses. And we do something interesting. So we
[01:05:27.520 --> 01:05:31.760]   compute the batch loss. So we compute the loss over the
[01:05:31.760 --> 01:05:39.560]   complete batch this time. I'm trying to see if there are any
[01:05:39.560 --> 01:05:42.240]   other things that I should be covering here. I don't think so.
[01:05:42.240 --> 01:05:48.680]   I've covered most of the parts. And towards the end, you call
[01:05:48.680 --> 01:05:52.200]   the main function from here. So this is again, because we've
[01:05:52.200 --> 01:05:56.560]   created this set of Pythonic files that we need to call.
[01:05:56.560 --> 01:05:58.960]   That's why we call the main function here inside of a
[01:05:58.960 --> 01:06:03.080]   Jupyter Notebook. It's quite different. But if you call
[01:06:03.080 --> 01:06:05.720]   training.py, it should be able to call other modules. So that's
[01:06:05.720 --> 01:06:12.120]   how this is structured. So now we have a full fledged model.
[01:06:12.440 --> 01:06:18.920]   Full fledged data set. And we're able to train on it, we get a
[01:06:18.920 --> 01:06:25.560]   high enough accuracy. And that is 97%, which we've already
[01:06:25.560 --> 01:06:29.200]   discussed isn't quite good. So in the next chapter, the authors
[01:06:29.200 --> 01:06:34.240]   start to discuss techniques of improving that accuracy. And
[01:06:34.240 --> 01:06:38.640]   this time, since we have time, my ambition will be fulfilled,
[01:06:38.640 --> 01:06:41.680]   which means we'll try to cover some of image augmentations. But
[01:06:41.680 --> 01:06:58.280]   before that, let me see if there are any questions. I'll come
[01:06:58.280 --> 01:07:06.920]   back to your question. Because thanks. Any questions at all
[01:07:06.920 --> 01:07:09.840]   from what we've discussed so far? I know I've rushed through
[01:07:09.840 --> 01:07:13.200]   things, but I just want to cover the highlights. This is code I
[01:07:13.200 --> 01:07:19.040]   can't, ideally, for me, it takes like, I would say five hours to
[01:07:19.040 --> 01:07:23.560]   understand every single line, then to replicate it. And I just
[01:07:23.560 --> 01:07:25.880]   want to give the highlights. So that's what I've covered, but
[01:07:25.880 --> 01:07:27.320]   happy to take any questions.
[01:07:27.320 --> 01:07:48.680]   Awesome. Coming back to the right window. Here I am. Next
[01:07:48.680 --> 01:07:54.640]   thing I'd want to discuss is image augmentations. And a few
[01:07:54.640 --> 01:07:57.540]   of the experts who are already a part of our study groups had
[01:07:57.540 --> 01:08:01.640]   suggested something known as image augmentation. I will point
[01:08:01.640 --> 01:08:07.680]   out a new framework called albumentations. And you might
[01:08:07.680 --> 01:08:10.360]   be tempted to ask Saeem, why are we looking at a different
[01:08:10.360 --> 01:08:13.360]   framework from PyTorch? This is a part of the PyTorch
[01:08:13.360 --> 01:08:18.560]   ecosystem. And it works really well with PyTorch. And I think
[01:08:18.560 --> 01:08:21.640]   their examples are absolutely amazing. So that's why we'll be
[01:08:21.640 --> 01:08:30.920]   using that. This is a library by Vladimir Iglovikov, who's a
[01:08:30.920 --> 01:08:35.680]   Kaggle Grandmaster, and a few more Kaggle masters and really
[01:08:35.680 --> 01:08:40.000]   top of the field, folks who've created this framework that
[01:08:40.000 --> 01:08:45.520]   boosts performance of deep CNNs by performing image
[01:08:45.520 --> 01:08:50.120]   augmentation. It's a Python library for fast and flexible
[01:08:50.120 --> 01:08:53.000]   image augmentations. And it's quite efficient in image
[01:08:53.000 --> 01:09:04.560]   transform operations. Here's the link to their website, and I'll
[01:09:04.560 --> 01:09:09.720]   just be referring to that. So the next chapter really covers
[01:09:09.720 --> 01:09:15.480]   this detail of applying image augmentations. I have decided to
[01:09:15.480 --> 01:09:18.520]   look at albumentations because again, we're at the point where
[01:09:18.520 --> 01:09:21.120]   you want to take this knowledge and apply it to different
[01:09:21.120 --> 01:09:25.640]   datasets. Albumentations compared against other options.
[01:09:25.640 --> 01:09:30.440]   So the authors of course use Torch Vision. And so this is how
[01:09:30.440 --> 01:09:35.360]   many images can each framework process in a second higher the
[01:09:35.360 --> 01:09:41.200]   number better it is. Many of the creators of any open source
[01:09:41.200 --> 01:09:45.800]   framework go to insane level of efforts to making things quite
[01:09:45.800 --> 01:09:49.320]   insanely optimized to the extent where they are even sometimes
[01:09:49.320 --> 01:09:53.200]   five times faster for different problems twice as fast, thrice
[01:09:53.200 --> 01:09:59.560]   as fast. So you would want to use the thing that's best or
[01:09:59.560 --> 01:10:05.360]   most efficient for you. Albumentations in this list is
[01:10:05.360 --> 01:10:09.000]   almost every single time the fastest except for equalize
[01:10:09.000 --> 01:10:13.760]   here, where augmentor is faster. I haven't used the framework
[01:10:13.760 --> 01:10:18.640]   ever, but safe to say it's faster than Torch Vision. So
[01:10:18.640 --> 01:10:23.640]   it's a good library to look at. But even before that, what is
[01:10:23.640 --> 01:10:28.840]   even image augmentations, right? Let's look at that. So there
[01:10:28.840 --> 01:10:33.000]   are these different problems on image datasets that we can work
[01:10:33.000 --> 01:10:38.240]   with or usually are working with. And every single time our
[01:10:38.240 --> 01:10:41.560]   end goal is to create a nice model. What does a nice model
[01:10:41.560 --> 01:10:48.120]   mean? A model that's accurate enough, generalizes enough and
[01:10:48.120 --> 01:10:54.320]   makes sure that we're able to work on a problem. So depending
[01:10:54.320 --> 01:10:59.080]   on the problem, and depending on how your dataset could be, now
[01:10:59.080 --> 01:11:01.760]   your model is supposed to work in the real world. It's not
[01:11:01.760 --> 01:11:05.280]   supposed to just sit on a GitHub repo, it's supposed to be
[01:11:05.280 --> 01:11:07.880]   deployed towards the end somewhere. So let's say you're
[01:11:07.880 --> 01:11:12.200]   working on satellite data, right? It should represent
[01:11:12.200 --> 01:11:15.360]   satellite data, because if NASA were to take your model and
[01:11:15.360 --> 01:11:19.160]   deploy it on a satellite, it should work there. And if it's
[01:11:19.160 --> 01:11:22.640]   not like so, first of all, you would go out there and collect
[01:11:22.640 --> 01:11:27.860]   more satellite data. But not everyone is Elon Musk and you
[01:11:27.860 --> 01:11:31.640]   can't afford satellite. So maybe it's better to change the
[01:11:31.640 --> 01:11:36.800]   images so that you have similar satellite datasets, right? So
[01:11:36.800 --> 01:11:39.960]   what we've done here is we've flipped this image vertically,
[01:11:39.960 --> 01:11:43.840]   we flipped it horizontally, we've rotated it and perform
[01:11:43.840 --> 01:11:49.000]   different operations. And this is the same image, exact same
[01:11:49.000 --> 01:11:52.400]   image going through all of these options. To me, these look like
[01:11:52.400 --> 01:11:55.360]   satellite images, right? I'm sure they look the same to you.
[01:11:55.360 --> 01:12:01.360]   And they're quite different. So that means this is quite helpful
[01:12:01.380 --> 01:12:07.620]   for the model to train. You would have to use a lot of
[01:12:07.620 --> 01:12:11.420]   common sense here. So you shouldn't be applying
[01:12:11.420 --> 01:12:14.860]   transforms. Let's say if you're working on something that has
[01:12:14.860 --> 01:12:19.500]   numbers in there, you probably wouldn't be flipping it
[01:12:19.500 --> 01:12:22.940]   horizontally or vertically, right? No one writes seven
[01:12:22.940 --> 01:12:27.540]   upside down, hopefully. So you would have to use some common
[01:12:27.540 --> 01:12:31.940]   sense there, of course. But most of the time, it's it's a good
[01:12:31.940 --> 01:12:37.220]   idea to look into what augmentations can be applied.
[01:12:37.220 --> 01:12:41.620]   Again, for medical images, you could apply these, these
[01:12:41.620 --> 01:12:46.300]   augmentations to me, it seems like this should hurt the model.
[01:12:46.300 --> 01:12:49.940]   That's what my common sense is, I might be wrong. And how do I
[01:12:49.940 --> 01:12:51.620]   find out by again, trying them out.
[01:12:53.500 --> 01:12:57.940]   Augmentation also support segmentation augmentations. So
[01:12:57.940 --> 01:13:01.220]   you're augmenting the mask, what is a mask, I won't cover it
[01:13:01.220 --> 01:13:04.220]   right now. It's we'll we'll be covering it in the next session.
[01:13:04.220 --> 01:13:08.500]   And we probably won't be covering key points, but it
[01:13:08.500 --> 01:13:12.740]   supports all of these use cases. Bottom line, it's quite a good
[01:13:12.740 --> 01:13:16.900]   framework. And that's why I'm using it to do a walkthrough of
[01:13:16.900 --> 01:13:17.700]   all of these.
[01:13:23.020 --> 01:13:26.460]   So let's look at what is an image augmentation. I'm looking
[01:13:26.460 --> 01:13:30.180]   at their documentation. So we have your original image, and
[01:13:30.180 --> 01:13:33.420]   you were tasked with creating the state of the art parrot
[01:13:33.420 --> 01:13:37.340]   detector. Let's say you care about the parrots in a forest.
[01:13:37.340 --> 01:13:40.620]   That's your task as a deep learning practitioner. And you
[01:13:40.620 --> 01:13:45.140]   tasked with creating a good model for that. You could
[01:13:45.140 --> 01:13:47.660]   collect a lot of images. But if you're trying to work with the
[01:13:47.660 --> 01:13:50.420]   given number of images, you could apply all of these
[01:13:50.420 --> 01:13:53.860]   transforms. And here are a few examples, you could flip it,
[01:13:53.860 --> 01:13:57.100]   that's a different image, you could change the contrast,
[01:13:57.100 --> 01:14:00.260]   that's totally possible. If you have a camera in the forest.
[01:14:00.260 --> 01:14:06.820]   This could certainly be a photo that comes out of it. Changing
[01:14:06.820 --> 01:14:10.460]   the saturation still leaves it as a parrot. You know, green
[01:14:10.460 --> 01:14:14.320]   parrots are quite common in my country. But this is still a
[01:14:14.320 --> 01:14:17.580]   parrot. And you could totally have parrots of this color. So
[01:14:17.580 --> 01:14:21.020]   this is quite a helpful augmentation. And you could
[01:14:21.020 --> 01:14:24.300]   apply crop. So you've essentially just cropped into
[01:14:24.300 --> 01:14:28.820]   the image like so you could blur it and you could change the
[01:14:28.820 --> 01:14:32.340]   gamma values. What a gamma value is some image things that people
[01:14:32.340 --> 01:14:35.880]   who work with Photoshop talk a lot about, I have no idea. But
[01:14:35.880 --> 01:14:40.900]   it could help help the model. Bottom line, you could create
[01:14:40.900 --> 01:14:44.900]   almost infinite amount of new training training samples using
[01:14:44.900 --> 01:14:51.220]   this. And here the authors showcase. So if you use
[01:14:51.220 --> 01:14:56.500]   something known as auto and augment augmentations, which is
[01:14:56.500 --> 01:14:59.700]   a specific set of augmentations defined by the creators of the
[01:14:59.700 --> 01:15:04.500]   framework, you always always improve upon the accuracy
[01:15:04.500 --> 01:15:12.420]   compared to just base augmentations. And definitely
[01:15:12.420 --> 01:15:15.500]   even more augmentations. So that's why image augmentation is
[01:15:15.500 --> 01:15:21.460]   quite helpful. I was planning to run through this example.
[01:15:21.460 --> 01:15:27.020]   Actually, I have time so I can do that. Is there a sort of
[01:15:27.020 --> 01:15:29.980]   rulebook which can tell which augmentation should be applied
[01:15:29.980 --> 01:15:38.020]   to what kind of image domain? There should be. But if you're
[01:15:38.020 --> 01:15:43.500]   applying this to quite different domain, you will be writing that
[01:15:43.500 --> 01:15:46.900]   book. So if you're applying this to a domain that you're an
[01:15:46.900 --> 01:15:50.420]   expert of, you will be writing that book, which is why I
[01:15:50.420 --> 01:15:57.380]   mentioned that you should be looking at what things make the
[01:15:57.380 --> 01:16:04.420]   most sense in certain applied fashions. So fortunately,
[01:16:04.420 --> 01:16:08.220]   unfortunately, no, there's no standard book. But it's quite
[01:16:08.220 --> 01:16:13.260]   common sense based so you can rely on that. So I was just
[01:16:13.260 --> 01:16:16.540]   speaking out loud, I wanted to run through an example since we
[01:16:16.540 --> 01:16:23.460]   have time, I'll do that. So this is just from the website, I
[01:16:23.460 --> 01:16:27.940]   clicked run in Colab. And this is what it took me to in this
[01:16:27.940 --> 01:16:31.780]   with just looking at cats and dogs images, this was a Kaggle
[01:16:31.780 --> 01:16:36.020]   competition. And the authors have just showcased how you can
[01:16:36.020 --> 01:16:42.500]   perform augmentations on it. So, so far, they've set up
[01:16:42.500 --> 01:16:45.900]   everything you import the data, you set up the training and
[01:16:45.900 --> 01:16:49.900]   validation sets, define functions to visualize images.
[01:16:49.900 --> 01:16:54.580]   And as you can see, these are quite a lot of I'm not a cat
[01:16:54.580 --> 01:17:00.500]   person. So it's a cute dog and, and cat with quite, quite a
[01:17:00.500 --> 01:17:03.260]   disappointment in my voice. I am totally not a cat person.
[01:17:03.260 --> 01:17:08.260]   Sorry, guys. Cat and dog images. And these are of different
[01:17:08.260 --> 01:17:12.260]   breeds. This is quite a tough problem is almost a toy data
[01:17:12.260 --> 01:17:16.980]   set today, given how advanced our models are. So we'll be
[01:17:16.980 --> 01:17:21.020]   creating a pytorch data set class first of all, and then a
[01:17:21.020 --> 01:17:25.740]   training loop to work with it. Two rules, always return the
[01:17:25.740 --> 01:17:31.020]   length and get items. So we work with them. And this time, we
[01:17:31.020 --> 01:17:36.620]   use augmentations to define sorry about that. Define
[01:17:36.620 --> 01:17:40.300]   transformation functions for training and validation data
[01:17:40.300 --> 01:17:46.820]   set. So we define two pipelines where we take the smallest size
[01:17:46.820 --> 01:17:55.580]   160 by 160 and crop it to 128 by 128, followed by which we
[01:17:55.620 --> 01:18:01.300]   apply more augmentations, normalize the image, divide all
[01:18:01.300 --> 01:18:06.820]   the values by 255. And then subtract mean pixel values for
[01:18:06.820 --> 01:18:09.580]   standard deviation. This is just the part of normalization.
[01:18:09.580 --> 01:18:16.300]   And then we converted to tensor. So this is quite similar to
[01:18:16.300 --> 01:18:20.420]   torch vision, right? But torch vision doesn't even have so many
[01:18:20.420 --> 01:18:24.180]   examples. We're just looking at GitHub. And I'll be meditations
[01:18:24.180 --> 01:18:27.220]   is first of all, faster and supports more functions. So what
[01:18:27.220 --> 01:18:32.260]   are we doing here? We're first of all, resizing the image to
[01:18:32.260 --> 01:18:41.740]   160 by 160. We are rotating it doing a random crop, changing
[01:18:41.740 --> 01:18:46.540]   the RGB colors, and changing the brightness contrast, normalizing
[01:18:46.540 --> 01:18:54.500]   it, and then returning a data tensor. Always fewer
[01:18:54.500 --> 01:18:58.420]   augmentations are applied on validation data set. So this is
[01:18:58.420 --> 01:19:03.100]   quite representative of the real world, right? There's something
[01:19:03.100 --> 01:19:07.580]   known as TTA that stands for test time augmentation, you can
[01:19:07.580 --> 01:19:16.100]   look this up. You should be applying these also. But again,
[01:19:16.700 --> 01:19:22.220]   usually training augmentations are much, much more crazier than
[01:19:22.220 --> 01:19:27.740]   the ones on test or validation data set. So we try to visualize
[01:19:27.740 --> 01:19:31.300]   these and this should give us a good idea, right? So to answer
[01:19:31.300 --> 01:19:38.780]   your question, because this is what is a good measure. So as
[01:19:38.780 --> 01:19:46.260]   you visualize all of these outputs, right? This is just the
[01:19:46.260 --> 01:19:50.900]   deal. This is the cat's body. And it's quite quite a bad
[01:19:50.900 --> 01:19:56.860]   photo. This might not be how someone would take a photo of
[01:19:56.860 --> 01:20:02.260]   their cat, right? A lot of the times having this common
[01:20:02.260 --> 01:20:06.340]   approach of just visualizing everything and just using common
[01:20:06.340 --> 01:20:10.140]   sense of maybe this is not the right approach. This totally
[01:20:10.140 --> 01:20:13.900]   cannot work with medical data. I can't tell anything by looking
[01:20:13.900 --> 01:20:17.300]   at an image. So there this approach of course fails. But
[01:20:17.300 --> 01:20:22.220]   for this example, it works quite well. Just having this insight
[01:20:22.220 --> 01:20:26.020]   into what's working and what's not is also a good way. And
[01:20:26.020 --> 01:20:29.020]   after that the metrics don't lie. So whatever you log,
[01:20:29.020 --> 01:20:33.780]   that's into a weights and biases dashboard should tell you how
[01:20:33.780 --> 01:20:38.500]   good or bad your model is performing. So from there, the
[01:20:38.500 --> 01:20:41.660]   authors define a full blown way of training the model. I'm not
[01:20:41.660 --> 01:20:47.140]   interested in showcasing that I just want to. Okay, it's not
[01:20:47.140 --> 01:20:51.140]   here. I wanted to show this approach of how these images get
[01:20:51.140 --> 01:20:54.580]   augmented. Sorry, here's here's the image I wanted to bring up
[01:20:54.580 --> 01:21:00.980]   next. Now the thing to point out always in practice, whenever
[01:21:00.980 --> 01:21:04.900]   you're performing these augmentations, these are
[01:21:04.900 --> 01:21:09.540]   performed with some probability. And you get to decide that
[01:21:09.540 --> 01:21:13.340]   probability. So essentially, you definitely want to crop into
[01:21:13.340 --> 01:21:16.820]   all images. So 100% probability there, you might flip it
[01:21:16.820 --> 01:21:20.100]   horizontally with a 50% probability. From there, you
[01:21:20.100 --> 01:21:24.820]   could have a 20% probability of changing the brightness 80% of
[01:21:24.820 --> 01:21:30.260]   leaving it unchanged. So we're just doing three transforms
[01:21:30.260 --> 01:21:34.580]   here, cropping, flipping and brightness adjustment. And as
[01:21:34.580 --> 01:21:38.220]   you can see, just from this one image, you have a possibility of
[01:21:38.220 --> 01:21:42.500]   four different images every time you run a batch through. And
[01:21:42.500 --> 01:21:49.100]   that is the power of image augmentations. So every time you
[01:21:49.100 --> 01:21:54.340]   perform an augmentation, the model gets enough of a change in
[01:21:54.340 --> 01:21:58.140]   the image to learn something new, and learn the important
[01:21:58.140 --> 01:22:04.740]   things. So that's why it's good to spend some time here.
[01:22:04.740 --> 01:22:08.380]   Someone pointed out ugly. I haven't looked at that
[01:22:08.380 --> 01:22:11.380]   framework, but I'll definitely check that out. From what I
[01:22:11.380 --> 01:22:14.860]   know, I'll be mentations is one of the best frameworks for image
[01:22:14.860 --> 01:22:18.700]   augmentations. I have actually interviewed one of the
[01:22:18.700 --> 01:22:23.420]   contributors as well. So you can find that interview, but really
[01:22:23.460 --> 01:22:26.740]   you'll get more value by just playing around with the
[01:22:26.740 --> 01:22:31.540]   documentation and demo. So this is another one of the suggested
[01:22:31.540 --> 01:22:34.780]   homeworks. Recently, no one has been doing the homework and I
[01:22:34.780 --> 01:22:39.620]   won't be too mad about that. But here are the homework items for
[01:22:39.620 --> 01:22:42.820]   you. Check out the weights and biases resources that we'll be
[01:22:42.820 --> 01:22:48.820]   emailing you. It's a tool for tracking your experiments. Swap
[01:22:48.820 --> 01:22:53.260]   the classification model for a ResNet. Check out Tim and
[01:22:53.260 --> 01:22:58.380]   augmentations and blog about either or all of the above.
[01:22:58.380 --> 01:23:04.140]   Strong emphasis there. So I wanted to quickly point out the
[01:23:04.140 --> 01:23:08.620]   documentation of weights and biases. We support quite a lot
[01:23:08.620 --> 01:23:14.060]   of applications. And what we're trying to achieve here is help
[01:23:14.060 --> 01:23:18.100]   you track these experiments and understand your data better.
[01:23:19.100 --> 01:23:23.420]   I won't go into much detail since there's a quite detailed
[01:23:23.420 --> 01:23:27.220]   report that one of my colleagues, Stacey has written.
[01:23:27.220 --> 01:23:32.420]   So we'll be sending this over to anyone who's registered. You can
[01:23:32.420 --> 01:23:38.020]   take a look at this and try to figure out how to, if at all, if
[01:23:38.020 --> 01:23:40.900]   if you're interested, how to integrate this into your
[01:23:41.380 --> 01:23:50.780]   pipeline. I would like this. I would, I find, I'm sorry if I'm
[01:23:50.780 --> 01:23:56.260]   messing your name up. I would like this augmentation. Yeah,
[01:23:56.260 --> 01:24:02.820]   maybe, maybe GANs are the way, but who knows. So let's see. I
[01:24:02.820 --> 01:24:05.820]   think that is pretty much all of the things I wanted to cover.
[01:24:05.820 --> 01:24:09.300]   To recap, we've covered a lot of ground today. A lot of theory,
[01:24:09.300 --> 01:24:12.780]   a lot of code, which means I haven't done a good job of
[01:24:12.780 --> 01:24:15.980]   explaining a lot of things, which means you need to do a lot
[01:24:15.980 --> 01:24:19.340]   of homework to understand a lot of things. And that is totally
[01:24:19.340 --> 01:24:23.580]   up to you. We understood what is a model, what is a backbone,
[01:24:23.580 --> 01:24:27.180]   what is a head, what is a tail. You need to work with those,
[01:24:27.180 --> 01:24:31.460]   create those. We understood what our image augmentations, and I
[01:24:31.460 --> 01:24:34.660]   suggested you all to try weights and biases as a way to tracking
[01:24:34.660 --> 01:24:40.020]   experiments. I'll again, wait a few minutes for any questions.
[01:24:40.020 --> 01:24:44.260]   Otherwise, we can wrap today's session up. I think we're
[01:24:44.260 --> 01:24:50.940]   approaching the end again. So we might have one or two more
[01:24:50.940 --> 01:24:56.180]   sessions, really towards the end of this study group. I think
[01:24:56.180 --> 01:24:59.860]   it's been two incredible months with everyone. Without being too
[01:24:59.860 --> 01:25:03.100]   dramatic, I'll probably save a speech for the last one. But
[01:25:03.540 --> 01:25:07.100]   yep, just just want to give a heads up that we're nearing the
[01:25:07.100 --> 01:25:11.860]   end of this study group and together with incredible people
[01:25:11.860 --> 01:25:17.420]   will be working on creating resources for the next one. And
[01:25:17.420 --> 01:25:20.460]   if you're interested in being a part of the fast a machine
[01:25:20.460 --> 01:25:25.060]   learning author group by author, I mean, we're creating resources
[01:25:25.060 --> 01:25:29.780]   for the next 30 group. You can just like the first comment here
[01:25:29.780 --> 01:25:32.420]   and I'll invite you to that group and we can start working
[01:25:32.420 --> 01:25:37.020]   on it together. If at all, it's just an open source project that
[01:25:37.020 --> 01:25:41.900]   I've decided to work on. So let me wait for these two questions.
[01:25:41.900 --> 01:25:50.540]   Can we please work on the Gantt to convert cats to dogs? I like
[01:25:50.540 --> 01:25:58.820]   this idea. Let's let's do it. Ravi. Let me look this up. See
[01:25:58.820 --> 01:26:17.740]   if there's something that didn't bring up anything. The
[01:26:17.740 --> 01:26:21.260]   training happens on the image after all augmentation steps or
[01:26:21.260 --> 01:26:24.220]   the model is trained on every image from each augmentation
[01:26:24.220 --> 01:26:30.380]   step. Great question. So depends on how you implement
[01:26:30.380 --> 01:26:35.380]   this. Ideally, you don't augment and add those to your data set
[01:26:35.380 --> 01:26:39.060]   whenever you're loading these images, sorry. Whenever you're
[01:26:39.060 --> 01:26:42.500]   loading these images to the memory, right? With some
[01:26:42.500 --> 01:26:49.100]   probability, you tend to change these images. So ideally, this
[01:26:49.100 --> 01:26:52.660]   single image that's going into the model along with 50 others,
[01:26:52.740 --> 01:26:56.620]   depending on the batch size, would get augmented with a
[01:26:56.620 --> 01:27:03.540]   certain probability. That's how usually augmentations work. You
[01:27:03.540 --> 01:27:08.180]   might change that depending on your use case. I hope that
[01:27:08.180 --> 01:27:14.540]   answers your question. Okay, I think we're at the end of the
[01:27:14.540 --> 01:27:17.620]   session. Thanks again, everyone for joining. I'll see you next
[01:27:17.620 --> 01:27:23.020]   week. We have the first call for the machine learning
[01:27:23.020 --> 01:27:27.940]   contribution happening tomorrow if you all would like to join.
[01:27:27.940 --> 01:27:31.140]   That's just a high level discussion but throwing it out
[01:27:31.140 --> 01:27:34.940]   there. Apart from that, we'll be studying PyTorch again next
[01:27:34.940 --> 01:27:37.580]   Sunday, same time. Thanks for joining and I hope to see you
[01:27:37.580 --> 01:27:37.900]   then.
[01:27:37.900 --> 01:27:47.900]   [BLANK_AUDIO]

