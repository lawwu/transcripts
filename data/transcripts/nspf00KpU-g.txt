
[00:00:00.000 --> 00:00:07.000]   Okay, let's get started. So, hey everybody, and welcome to another paper reading group.
[00:00:07.000 --> 00:00:12.000]   And this week, we're doing things differently than all the other weeks before. So until
[00:00:12.000 --> 00:00:17.000]   now, what we've been doing was, we were looking after papers, we were going after papers that
[00:00:17.000 --> 00:00:22.000]   were very recent. So, for example, a paper had been released, say, a month ago, and we'd
[00:00:22.000 --> 00:00:27.000]   be discussing that at paper reading group. But now, from this week onwards, there's a
[00:00:27.000 --> 00:00:32.000]   change in that. We're probably going to target more beginner-friendly papers. And we're doing
[00:00:32.000 --> 00:00:37.000]   this because we have another, I'm running another study group at Bridgton Biases, which
[00:00:37.000 --> 00:00:42.000]   is Fastbook Study Group. And everybody from there is now at that level where they could
[00:00:42.000 --> 00:00:49.000]   come and like, we want to upskill and help the community. So, everybody who's been involved
[00:00:49.000 --> 00:00:53.000]   with Fastbook Reading Group is now, sorry, if you guys, do you guys hear a lot of background
[00:00:53.000 --> 00:01:04.000]   noise from my side? I think there's like a, I don't even know what it is. Wow, that's
[00:01:04.000 --> 00:01:15.000]   terrible. Okay, should be fine now. Yes, sorry, I'm also relocated to India. So, I'm just
[00:01:15.000 --> 00:01:20.000]   getting my workstation set up, getting my noise cancellation set up and all that stuff.
[00:01:20.000 --> 00:01:26.000]   So, if this seems like this happened, I think it was like the garbage truck outside. So,
[00:01:26.000 --> 00:01:31.000]   anyway, so let's keep going. So, then now, what I was, as I was saying, we're at that
[00:01:31.000 --> 00:01:38.000]   point where, I think I can still hear it. Anyway, we're at that point where we're going
[00:01:38.000 --> 00:01:43.000]   to start discussing these papers. And we're going to, and I'm going to look at them fresh.
[00:01:43.000 --> 00:01:47.000]   Like, I'm not going to, like, usually I would come prepared, okay, this is what it is, this
[00:01:47.000 --> 00:01:50.000]   is what's that. But actually, what I want to do with this, being in a friendly paper
[00:01:50.000 --> 00:01:55.000]   reading groups, that I want to come in and like read the paper as if I'm reading it for
[00:01:55.000 --> 00:01:59.000]   the first time in front of everybody. So, what that will do is like for somebody who's
[00:01:59.000 --> 00:02:03.000]   been scared of papers, or somebody, if you're someone like you've been scared of papers,
[00:02:03.000 --> 00:02:07.000]   or if you're someone who's like felt, oh, I can't understand what Presnet is, I can't
[00:02:07.000 --> 00:02:11.000]   understand what Ensnet is, I can't understand what EfficientNet is, or like what exactly
[00:02:11.000 --> 00:02:16.000]   goes on in this. So, what I want to do is I want to take that fear away. And what we
[00:02:16.000 --> 00:02:20.000]   want to do together now going forward, is that we want to read these paper reading groups.
[00:02:20.000 --> 00:02:24.000]   And in these paper reading groups, we want to have and develop that skill of being able
[00:02:24.000 --> 00:02:29.000]   to read papers. So, this is, I think this, today is a really, if you're joining me today
[00:02:29.000 --> 00:02:34.000]   for the first time, this is a really, really good point to get started and start reading
[00:02:34.000 --> 00:02:40.000]   these beginner friendly papers. So, we're going to start with Resnet. And one thing
[00:02:40.000 --> 00:02:46.000]   I did want to say is like if you go to, so what we do is we ask all our questions on
[00:02:46.000 --> 00:02:55.000]   the thread. So, if you go to wandb.me/resnet. So, I'm just going to post that link in the
[00:02:55.000 --> 00:03:01.000]   chat as well. I'm just going to mute myself for a second. I think there's a lot of background
[00:03:01.000 --> 00:03:28.000]   noise again. Okay, hopefully it's not going to be any more background noise. So, if you
[00:03:28.000 --> 00:03:35.000]   go to that link, wandb.me/resnet, I'll type in that again. If you go to that link, wandb.me/resnet,
[00:03:35.000 --> 00:03:40.000]   I'll post that in the chat. That should bring everybody to the forum. And as we're going
[00:03:40.000 --> 00:03:48.000]   through the paper, and this is a very similar setup to FASTFOOL, very similar setup to the
[00:03:48.000 --> 00:03:54.000]   PyTorch reading group that, PyTorch book study group that Sangyam's hosting. And then what
[00:03:54.000 --> 00:03:58.000]   we can do is we can just come here, click reply, and then just ask the question. So,
[00:03:58.000 --> 00:04:03.000]   what I want is like, don't be afraid of asking a question. And what I want is like, if you
[00:04:03.000 --> 00:04:08.000]   want to, I'll keep checking at this place. I'll just stop at times. And then what we
[00:04:08.000 --> 00:04:14.000]   can do is we can go through the paper together. So, I'll address those questions as we go.
[00:04:14.000 --> 00:04:19.000]   Okay, with that being said, now let me read this paper as if I'm reading it for the first
[00:04:19.000 --> 00:04:28.000]   time. Okay. So, let's see. Let's see how do we go about that. All right. So, the first
[00:04:28.000 --> 00:04:33.000]   thing, whenever I read a new paper, the first thing that I do is I read the abstract really,
[00:04:33.000 --> 00:04:38.000]   really well. And I've just kind of picked this up. Like I've watched this video by Andrew
[00:04:38.000 --> 00:04:43.000]   Ang and he pretty much explains how to read a paper. And I just picked it up like exactly
[00:04:43.000 --> 00:04:47.000]   how he was doing things. And I've just kind of followed that up. So, but I just want to
[00:04:47.000 --> 00:04:53.000]   show things in practice by reading this one paper. So, let's start with the abstract.
[00:04:53.000 --> 00:04:59.000]   And the abstract says, deeper neural networks are more difficult to train. Okay. Well, in
[00:04:59.000 --> 00:05:06.000]   2015, they may be. In 2021, are they really? So, that's like, that's how I go about this
[00:05:06.000 --> 00:05:10.000]   abstract as well is like, I'll keep asking questions. So, when I read things, I'll be
[00:05:10.000 --> 00:05:15.000]   like, okay, that's what this means. Okay, that's what that means and so on. So, I guess
[00:05:15.000 --> 00:05:23.000]   then in this abstract, what they say is we present a residual learning framework to ease
[00:05:23.000 --> 00:05:27.000]   the training of networks that are substantially deeper than those used previously. So, this
[00:05:27.000 --> 00:05:34.000]   is a good point. Like the main thing that then abstract is generally, if you haven't
[00:05:34.000 --> 00:05:39.000]   written papers before, in abstract, what the researchers actually try to do is they kind
[00:05:39.000 --> 00:05:45.000]   of, the whole research paper is trying, is being summarized in like really easy language
[00:05:45.000 --> 00:05:51.000]   in a really small text. So, abstract is the one where I believe the most amount of effort
[00:05:51.000 --> 00:05:56.000]   is spent when writing a research paper and also like in the introduction and conclusion.
[00:05:56.000 --> 00:06:00.000]   So, this is where we also start. We start with abstract, we start with introduction
[00:06:00.000 --> 00:06:04.000]   and then we look at the diagrams and we go all the way to conclusion. So, the one thing
[00:06:04.000 --> 00:06:09.000]   then they say, okay, one of the main contributions is that they've started, they've introduced
[00:06:09.000 --> 00:06:13.000]   networks that are substantially deeper than the ones before. All right, we don't know
[00:06:13.000 --> 00:06:18.000]   what that means right now, but point noted. And then they say, okay, we reformulate the
[00:06:18.000 --> 00:06:22.000]   layers. So, the deep learning layers and we reformulate them and we say that we're using
[00:06:22.000 --> 00:06:27.000]   residual functions with reference to layer inputs. Okay, I don't exactly know what that
[00:06:27.000 --> 00:06:32.000]   means right now, but I'll keep that in mind. And then finally, we say we provide empirical
[00:06:32.000 --> 00:06:37.000]   evidence that these residual networks are easier to optimize, can gain accuracy from
[00:06:37.000 --> 00:06:43.000]   considerably increased depth, which is again going back to that point that the main contribution
[00:06:43.000 --> 00:06:50.000]   is this idea of having like deeper networks, right? And then this is where the interesting
[00:06:50.000 --> 00:06:54.000]   things, this is where it gets interesting. So, they tried things on the ImageNet dataset.
[00:06:54.000 --> 00:07:01.000]   So, for those of you who don't know what ImageNet dataset is, ImageNet dataset is this 1.5 million
[00:07:01.000 --> 00:07:08.000]   images where what they do in that dataset is like this 1.5 million images of like lots
[00:07:08.000 --> 00:07:14.000]   of different things. So, 1.5 million images of say humans, of fish, of a car, of like
[00:07:14.000 --> 00:07:20.000]   sofa. So, there's like all of these different categories. And that ImageNet, there was a
[00:07:20.000 --> 00:07:25.000]   competition called ImageNet ILC, something like that. Let me just quickly Google what
[00:07:25.000 --> 00:07:33.000]   it was called and I can show you. So, let me just go here and ImageNet, oh sorry, caps
[00:07:33.000 --> 00:07:41.000]   off, ImageNet competition. And see this one? ImageNet large scale visual recognition challenge.
[00:07:41.000 --> 00:07:48.000]   So, what used to happen is that every year from 2012 all the way to 2017, there was an
[00:07:48.000 --> 00:07:52.000]   ImageNet competition that was being hosted for researchers. And that was being hosted
[00:07:52.000 --> 00:07:57.000]   for using the ImageNet dataset, which is about 1.5 million images. And what would happen
[00:07:57.000 --> 00:08:02.000]   is every year researchers would come back, compete with each other, and try and break
[00:08:02.000 --> 00:08:08.000]   the accuracy and try and like make this more, try and generate a model or create a model
[00:08:08.000 --> 00:08:11.000]   that's more accurate than the last one or that has higher performance than the last
[00:08:11.000 --> 00:08:20.000]   one. And the one we're discussing today is ResNet. And ResNet won the ILSVRC 2015. So,
[00:08:20.000 --> 00:08:26.000]   ImageNet large scale competition in 2015. So, if I click on that, you can see like this
[00:08:26.000 --> 00:08:30.000]   is how it started and this is how they say and then this thing. There's more stuff you
[00:08:30.000 --> 00:08:37.000]   can read over here. It also included like object detection, object localization. Can
[00:08:37.000 --> 00:08:47.000]   you hear background noise again? I'm really sorry for this. Now, there's a fruit seller
[00:08:47.000 --> 00:08:52.000]   on the streets. All right, I'll keep going. I'm just going to ignore the background noise.
[00:08:52.000 --> 00:09:03.000]   This is really funny. Cool. All right. So, then that's the main one is that this is what
[00:09:03.000 --> 00:09:10.000]   they've done is like there was a competition. There was basically a competition year after
[00:09:10.000 --> 00:09:14.000]   year. And what the researchers would come, they would come back and they would start
[00:09:14.000 --> 00:09:18.000]   to, they would start to compete with each other and try and develop like these networks
[00:09:18.000 --> 00:09:23.000]   that are better. And one of those then research, one of those research, like a few of those
[00:09:23.000 --> 00:09:28.000]   researchers, especially timing her and from Microsoft research, they kind of introduced
[00:09:28.000 --> 00:09:33.000]   ResNet. So, this is what we, this is like, I'm just giving you background of like how
[00:09:33.000 --> 00:09:37.000]   every year there's like these new competitions. And 2016 was a different winner. 2014 was
[00:09:37.000 --> 00:09:42.000]   a different winner. 2012, I believe was AlexNet. So, that's how like this sort of journey
[00:09:42.000 --> 00:09:47.000]   has happened in computer vision. And it's really helpful to also know about the journey.
[00:09:47.000 --> 00:09:53.000]   So, that's what they say. This result, which means the ResNet paper won the first place
[00:09:53.000 --> 00:09:59.000]   on ILSVRC, which is the ImageNet large scale recognition competition. And it won the first
[00:09:59.000 --> 00:10:05.000]   place. So, this was when they came back and like this particular research paper won that
[00:10:05.000 --> 00:10:10.000]   2015. And then they also show the results on CIFAR 10 with 100 and 1000 layers. So,
[00:10:10.000 --> 00:10:16.000]   CIFAR 10 is just another smaller data set. It's not as big as ImageNet. It's a much,
[00:10:16.000 --> 00:10:23.000]   much smaller data set. But it's much easier to start practicing things on CIFAR 10. So,
[00:10:23.000 --> 00:10:27.000]   and then there's another thing I want to show you. If you haven't looked at Semantic Scholar,
[00:10:27.000 --> 00:10:35.000]   so let me go to Semantic Scholar. And let me just search deep residual learning for
[00:10:35.000 --> 00:10:43.000]   image recognition. So, just search the title of the paper. Deep residual learning for image
[00:10:43.000 --> 00:10:51.000]   recognition. And then what this does is I really like Semantic Scholar for two or three
[00:10:51.000 --> 00:10:57.000]   reasons. Main one being it will tell me like how many citations there are. So, generally,
[00:10:57.000 --> 00:11:01.000]   highly influential papers have really, really large number of citations. So, this paper
[00:11:01.000 --> 00:11:09.000]   has been cited 75,000 times, about 75, 74,500 times, approx. And that's a really, really
[00:11:09.000 --> 00:11:14.000]   big number. Really, really big number. Because like, if you search for something recent,
[00:11:14.000 --> 00:11:21.000]   let's say, vision transformer, or weird. I just don't know what the numbers would be.
[00:11:21.000 --> 00:11:31.000]   I just want to quickly check. Oh, it's called an image is worth 16 cross 16 words, I believe.
[00:11:31.000 --> 00:11:36.000]   Yeah, that's the one. So, let's see how many times that's been cited. So, you can see how
[00:11:36.000 --> 00:11:41.000]   that's at 941. And even though the vision transformer has been a really high impact
[00:11:41.000 --> 00:11:46.000]   paper. So, I just want to show a benchmark. It's like, oh, ResNet has been cited 75,000
[00:11:46.000 --> 00:11:53.000]   times. That's not to say that ResNet is that much better than vision transformer. It's
[00:11:53.000 --> 00:11:58.000]   just to say that's how much impact it has had on computer vision. Because there's been
[00:11:58.000 --> 00:12:03.000]   so many new architectures that have took inspiration from the ResNet paper that we're discussing
[00:12:03.000 --> 00:12:12.000]   today. All right. So, then, this is just where they highlight the results. It's like, okay,
[00:12:12.000 --> 00:12:18.000]   so, due to extremely deep representations in the paper, we obtained 28% relative improvement
[00:12:18.000 --> 00:12:24.000]   on the COCO object detection. So, from the last state of the art result, they got 28%
[00:12:24.000 --> 00:12:29.000]   better with ResNet, which is amazing. And then, again, they're saying we won the first
[00:12:29.000 --> 00:12:35.000]   places on the task of ImageNet and ImageNet localization, COCO detection, and COCO segmentation.
[00:12:35.000 --> 00:12:41.000]   So, they did actually win IL SVRC 2015 competition. They actually won all these other competitions.
[00:12:41.000 --> 00:12:46.000]   They won ImageNet detection, they won ImageNet localization, they won COCO detection, and
[00:12:46.000 --> 00:12:53.000]   even COCO segmentation, which is amazing. So, like, that's how I would read the abstract.
[00:12:53.000 --> 00:12:58.000]   It's like, okay, great, 75,000 citations, perfect. And then, we keep going on. So, now,
[00:12:58.000 --> 00:13:04.000]   what I would do, like, after reading the abstract, I now have a general idea of what this paper
[00:13:04.000 --> 00:13:10.000]   is about. And that general idea is this paper does something with introducing more or deeper
[00:13:10.000 --> 00:13:18.000]   layers. So, what I understand from reading the abstract is that, okay, there was a problem
[00:13:18.000 --> 00:13:23.000]   with training deeper neural networks, because that's what they say in the first line of
[00:13:23.000 --> 00:13:29.000]   text here. And what the researchers from Microsoft Research have done is, like, they've come
[00:13:29.000 --> 00:13:34.000]   back with a network architecture that is now able to train with more number of layers than
[00:13:34.000 --> 00:13:42.000]   before. And what that did is, like, that actually broke all the records. It got, like, a 28%
[00:13:42.000 --> 00:13:47.000]   increase. So, this is how I would read the abstract. It's like, this is my summary after
[00:13:47.000 --> 00:13:51.000]   reading the abstract. And then, what I would generally do is I would start looking at these
[00:13:51.000 --> 00:13:55.000]   figures. So, I would go over to see figure one, and I'm like, okay, training error left,
[00:13:55.000 --> 00:13:59.000]   which is this one on -- which is basically this one on the left, and then there's this
[00:13:59.000 --> 00:14:05.000]   one image on the right. So, it's on CIFAR-10 dataset, which I told you is a smaller one.
[00:14:05.000 --> 00:14:11.000]   And they're pretty much training, like, a 26-layer and 56-layer plane network. Okay.
[00:14:11.000 --> 00:14:18.000]   So, what does this mean? Right. So, this just -- by looking at this, what that means to
[00:14:18.000 --> 00:14:26.000]   me is, like, if you look at this 20-layer curve, and then you look at the 56-layer curve,
[00:14:26.000 --> 00:14:32.000]   what this means is, unless you do something funny, like, as long as you keep going with
[00:14:32.000 --> 00:14:37.000]   a plane network -- plane is the word here -- as long as you keep going with a plane
[00:14:37.000 --> 00:14:44.000]   network, not a ResNet, the deeper architecture -- and this is something unheard of, right,
[00:14:44.000 --> 00:14:49.000]   because today we just know the deep architectures are better. But at the time in 2015, if you
[00:14:49.000 --> 00:14:55.000]   go for a deeper architecture that has a higher training error than something that's a much
[00:14:55.000 --> 00:14:58.000]   shallower architecture. So, when I say "shallow," it just means that the number of layers is
[00:14:58.000 --> 00:15:03.000]   really small. And when I say "deeper," it just means, like, a really high number of
[00:15:03.000 --> 00:15:09.000]   layers in the neural network architecture. And for those of us joining us from Fastbook,
[00:15:09.000 --> 00:15:14.000]   I will just say this, like, by saying "layers," I just mean "conf-batch-conv-relu." So, you'll
[00:15:14.000 --> 00:15:21.000]   see, like, there's generally a repeat. We built our first simple CNN last week, and
[00:15:21.000 --> 00:15:26.000]   this is just, like, basically just saying, okay, we just have, like, 56 layers of that
[00:15:26.000 --> 00:15:31.000]   block, and otherwise we have 20 layers of that block. And same thing happened on the
[00:15:31.000 --> 00:15:35.000]   training and test error. So, this network is actually -- deeper layers are actually
[00:15:35.000 --> 00:15:42.000]   worse. Okay. So, now I've got that in my head. But then, now I'm thinking, okay, why are
[00:15:42.000 --> 00:15:46.000]   deeper layers worse? What's happening there? So, I would then start reading the introduction.
[00:15:46.000 --> 00:15:52.000]   So, let's go about and read the introduction very quickly. So, it says, "Deep convolution
[00:15:52.000 --> 00:15:56.000]   neural networks have led to breakthroughs." Okay, this is just an introduction. It's saying,
[00:15:56.000 --> 00:16:01.000]   okay, convolutional networks have been helpful. "Deep networks naturally integrate low, mid,
[00:16:01.000 --> 00:16:06.000]   high-level features." All good. "And classifies in an end-to-end, multilayer fashion. Levels
[00:16:06.000 --> 00:16:10.000]   of features can be enriched by the number of stack layers." Okay, all good. So, nothing
[00:16:10.000 --> 00:16:15.000]   -- it's, like, I'm still trying to find the answer to why are deeper layers worse and
[00:16:15.000 --> 00:16:20.000]   what does Weissnet do? So, this is all just background, which I can read through quickly.
[00:16:20.000 --> 00:16:24.000]   And then it goes, okay, "Recent evidence reveals that the network depth is of crucial
[00:16:24.000 --> 00:16:28.000]   importance." Okay, so, this is where things start to get interesting for me. It says,
[00:16:28.000 --> 00:16:34.000]   "Network depth is of crucial importance." And then it also cites, like, two papers where
[00:16:34.000 --> 00:16:40.000]   this has been proven, or, like, those research, like, '41 and '44, if you go down to the
[00:16:40.000 --> 00:16:45.000]   references, then we will see that this is where they found that network depth is of
[00:16:45.000 --> 00:16:49.000]   crucial importance when training neural networks. And they said, okay, there's these leading
[00:16:49.000 --> 00:16:55.000]   results. All good. And they go, it has been also used in many non-trivial vision recognition
[00:16:55.000 --> 00:17:00.000]   starts. All good. I'll keep moving on. And now this is where things start to get interesting
[00:17:00.000 --> 00:17:07.000]   for me. "Driven by the significance of depth, a question arises, is learning better networks
[00:17:07.000 --> 00:17:15.000]   as easy as stacking more layers?" Right? So, until now, if researchers have said, or, like,
[00:17:15.000 --> 00:17:21.000]   this prior research that has said, if you go deeper, like, network depth is of crucial
[00:17:21.000 --> 00:17:28.000]   importance to training neural networks, then what if I just create, like, a thousand-layer
[00:17:28.000 --> 00:17:35.000]   deep network? That should do better than a 20-layer network, right? So, that's the question.
[00:17:35.000 --> 00:17:41.000]   Is it, like, learning better networks, is it just as easy as, like, having more layers?
[00:17:41.000 --> 00:17:47.000]   And they say, not really. So, there's an obstacle to answering this question, and that obstacle
[00:17:47.000 --> 00:17:52.000]   is vanishing and exploding gradients. So, what used to happen, I have some background
[00:17:52.000 --> 00:17:58.000]   about vanishing and exploding gradients, but in case, like, if I were, if I really didn't
[00:17:58.000 --> 00:18:03.000]   know about vanishing and exploding gradients, I would just quickly go and read, like, what
[00:18:03.000 --> 00:18:08.000]   are these references, one and nine. So, if I go back to, because that's one node, I can't
[00:18:08.000 --> 00:18:13.000]   really click on links. So, if I go here, and I can just click on one, and I'll see one
[00:18:13.000 --> 00:18:18.000]   is this paper that says, "Learning long-term dependencies with gradient descent is difficult,"
[00:18:18.000 --> 00:18:22.000]   then I would Google that, a paper link would come up, which is this one, I would click
[00:18:22.000 --> 00:18:26.000]   on that, and I would read, like, just the abstracted introduction, just to understand
[00:18:26.000 --> 00:18:30.000]   what vanishing gradient is. So, that's how I would learn about vanishing gradient, and
[00:18:30.000 --> 00:18:35.000]   that's how, like, I keep expanding the knowledge, it's like, okay, one paper references other
[00:18:35.000 --> 00:18:39.000]   papers, and then you just go read the abstract introduction, unless you want to go deeper
[00:18:39.000 --> 00:18:43.000]   and deeper. So, it depends on how deep you want to go. But for ResNets, we just want
[00:18:43.000 --> 00:18:49.000]   to, for the purpose of reading ResNet, we want to go as deep and understand ResNet properly,
[00:18:49.000 --> 00:18:54.000]   but we don't, right now, we just, it's enough for us to understand what vanishing or exploding
[00:18:54.000 --> 00:19:01.000]   gradient problem is. So, what that was, is, like, if you have, like, so many deep layers
[00:19:01.000 --> 00:19:06.000]   all the way, when you provide some input, so let's say that's my input image, when you
[00:19:06.000 --> 00:19:11.000]   provide that input, because there's going to be, this layer will then work on the input
[00:19:11.000 --> 00:19:15.000]   image, and then these are my activations outputs from layer one, and then these are my activation
[00:19:15.000 --> 00:19:19.000]   outputs from layer two, and then these are my activation outputs from layer three, and
[00:19:19.000 --> 00:19:24.000]   then you do a backward pass in your network, you calculate the loss, pretty much, and then
[00:19:24.000 --> 00:19:29.000]   you do a backward pass. And what was happening is, like, the gradients would become zero
[00:19:29.000 --> 00:19:34.000]   in the deeper layers, okay? So, as you go deeper and deeper and deeper, your gradient
[00:19:34.000 --> 00:19:39.000]   started to become zero, and that was a vanishing gradient problem. And when the gradient starts
[00:19:39.000 --> 00:19:43.000]   to become zero, the problem is, like, the network can't really train, because if you
[00:19:43.000 --> 00:19:47.000]   do a loss of backward on zero, like, if you take the derivative of something that's zero,
[00:19:47.000 --> 00:19:51.000]   you're just going to get nothing, like, the model's not going to train anything, so you're
[00:19:51.000 --> 00:19:56.000]   going to get zero. So that's, this is what this problem was, like, if you have a really
[00:19:56.000 --> 00:20:02.000]   deep network, then the deeper layers actually don't learn anything, it's just the shallower
[00:20:02.000 --> 00:20:06.000]   layers that learn something. And that's why you can see, like, the 20 layer is doing better
[00:20:06.000 --> 00:20:11.000]   than 56 layers, because it's much harder to train the 56 layer massive network, where
[00:20:11.000 --> 00:20:19.000]   there's, sorry, can I please ask everybody to mute themselves? Thank you. So, the reason,
[00:20:19.000 --> 00:20:24.000]   so then, this is what, this is why this, like, network is really hard, is, like, in 56 layers,
[00:20:24.000 --> 00:20:28.000]   you have, like, these extra 36 layers that don't really train, because of the vanishing
[00:20:28.000 --> 00:20:34.000]   gradient problem. So that's the background on vanishing gradients. And then it goes,
[00:20:34.000 --> 00:20:40.000]   okay, so this problem has largely been addressed by normalizing, normalized initialization.
[00:20:40.000 --> 00:20:45.000]   So there's some solution for this problem, right? And there's also, like, you can also
[00:20:45.000 --> 00:20:49.000]   do, like, intermediate normalization layers, which enable networks with tens of thousands
[00:20:49.000 --> 00:20:56.000]   layers to start converging for STD with back propagation. Okay, so in introduction right
[00:20:56.000 --> 00:21:01.000]   now, I just realized the problem with deeper layers, deeper networks is vanishing gradient,
[00:21:01.000 --> 00:21:08.000]   and there are some solutions to it. Some solutions, which are, you initialize your network with
[00:21:08.000 --> 00:21:13.000]   some certain weights, because for everybody from Fastbook, or from everybody who's joining
[00:21:13.000 --> 00:21:18.000]   me at PRG today, what we generally do is we create a network, and then we initialize it
[00:21:18.000 --> 00:21:23.000]   with random weights, and then we start training that network, right? So, when you initialize
[00:21:23.000 --> 00:21:27.000]   a network, there's also different ways of initializing a network, like you could have
[00:21:27.000 --> 00:21:32.000]   a normal distribution, or, like, there's, initialization of a network is also really,
[00:21:32.000 --> 00:21:38.000]   really important, right? So this is just, like, these are just, like, couple solutions
[00:21:38.000 --> 00:21:51.000]   that are there. So I'll just quickly head and see if there's any, if there's any question
[00:21:51.000 --> 00:21:57.000]   here. A general question is, like, how does the same network like ResNet do image classification
[00:21:57.000 --> 00:22:03.000]   and also do object detection and segmentation? Okay, you, good question. I don't want to
[00:22:03.000 --> 00:22:08.000]   go much, a lot of, like, I don't want to go into this in a lot of depth, but basically
[00:22:08.000 --> 00:22:13.000]   you just change the head. So you train a network to do classification, then you cut off the
[00:22:13.000 --> 00:22:17.000]   classification head, and you train the network to do object detection. That's just a very
[00:22:17.000 --> 00:22:24.000]   simple and generic answer, but that's exactly how these things happen. All right, moving
[00:22:24.000 --> 00:22:32.000]   on. So when deeper networks start to converge, a degradation problem has been exposed. So
[00:22:32.000 --> 00:22:37.000]   there's now another problem when deeper networks start to converge, and it says accuracy gets
[00:22:37.000 --> 00:22:41.000]   saturated, so basically you can't really hit, like, you hit a benchmark and then you can't
[00:22:41.000 --> 00:22:46.000]   really go above it, and then the accuracy starts to degrade, degrades, it starts to
[00:22:46.000 --> 00:22:52.000]   degrade rapidly. Cool. Such degradation is not caused by overfitting. So, you know, generally
[00:22:52.000 --> 00:22:57.000]   what it's saying is, like, accuracy hits a benchmark, so it's like this, and then it
[00:22:57.000 --> 00:23:03.000]   starts to go really bad, right? And generally this is like a pattern that we would see when
[00:23:03.000 --> 00:23:08.000]   the network's overfitting, but over here they're saying, okay, that's not due to overfitting
[00:23:08.000 --> 00:23:14.000]   because the model also has a higher training error. So this is like, this is where the
[00:23:14.000 --> 00:23:19.000]   general understanding of machine learning is helping us, and, you know, this is where
[00:23:19.000 --> 00:23:25.000]   we realize, okay, overfitting just means this idea of, like, when the network starts to
[00:23:25.000 --> 00:23:29.000]   just understand things and, like, if the training error is also getting worse, then the network
[00:23:29.000 --> 00:23:33.000]   is not really overfitting, because overfitting would only happen if the training error is
[00:23:33.000 --> 00:23:40.000]   really low and the validation error is high. So then keep going forward, I'll keep reading,
[00:23:40.000 --> 00:23:45.000]   okay, the degradation, I'm just going to quickly go through this now. So it says, okay, that
[00:23:45.000 --> 00:23:50.000]   this problem indicates that not all systems are easy to optimize, and then let us consider
[00:23:50.000 --> 00:23:54.000]   a shallower architecture and its deeper counterpart that adds more layers into it. So at this
[00:23:54.000 --> 00:23:58.000]   point now the researchers are like, okay, let's try and understand this degradation
[00:23:58.000 --> 00:24:04.000]   problem a bit more. So now I would just basically, when I'm reading ResNet, I would just read
[00:24:04.000 --> 00:24:09.000]   the abstract and introduction in a lot more detail, and that should be enough for me to
[00:24:09.000 --> 00:24:14.000]   understand the paper, generally any paper. So this is what I'm doing. So I'm just going
[00:24:14.000 --> 00:24:19.000]   to spend the most time today on abstract and introduction. So it goes, okay, there exists
[00:24:19.000 --> 00:24:26.000]   a solution by construction to the deeper model, the added layers are identity mapping, and
[00:24:26.000 --> 00:24:34.000]   the other layers are copied from the lower shallower model. Okay, so at this point now,
[00:24:34.000 --> 00:24:43.000]   the researchers are starting to, like, find solutions to the degradation problem. And
[00:24:43.000 --> 00:24:48.000]   they're just saying there's an identity mapping, and they're saying other layers are copied
[00:24:48.000 --> 00:24:57.000]   from the lower shallower model. So remember, we had a 20 layer model, and we had a 56 layer
[00:24:57.000 --> 00:25:04.000]   model, right? So this is 20, this is 36. So what they're saying is, and this is again
[00:25:04.000 --> 00:25:09.000]   something I don't understand completely because I haven't read the whole paper yet, but my
[00:25:09.000 --> 00:25:15.000]   understanding right now is that there's a way to create an identity mapping. So what
[00:25:15.000 --> 00:25:19.000]   does identity mapping, what does identity mapping mean? It means that you have your
[00:25:19.000 --> 00:25:24.000]   input x, and then you just don't do anything, you just pass that as the output x. So it's
[00:25:24.000 --> 00:25:30.000]   an identity mapping, like you don't do anything to your input. And that's identity mapping.
[00:25:30.000 --> 00:25:34.000]   So let's see where they would use this identity mapping in the network. But they're saying
[00:25:34.000 --> 00:25:39.000]   like using this identity mapping is helpful. And then what they say is like, you train
[00:25:39.000 --> 00:25:49.000]   this 20 deeper layer network. And then what you do from here, sorry, one sec. Could I
[00:25:49.000 --> 00:26:00.000]   please ask everybody to mute themselves? Thank you. Okay. And then what they're saying is,
[00:26:00.000 --> 00:26:04.000]   okay, we're just going to copy the trained weights from here to here, copy the trained
[00:26:04.000 --> 00:26:09.000]   weights from here to here, and then we start to train this model. So let's see, I don't
[00:26:09.000 --> 00:26:13.000]   understand that completely. That's why I've just this vague idea. But now this is where
[00:26:13.000 --> 00:26:18.000]   I would go and read more about this. So I would continue reading and it says, okay,
[00:26:18.000 --> 00:26:23.000]   the existence of this constructed solution indicates that a deeper model should produce
[00:26:23.000 --> 00:26:28.000]   no higher training error than its shallow counterpart. That's actually a good point.
[00:26:28.000 --> 00:26:34.000]   Okay. And then the experiments show that our current solvers on hand are unable to find
[00:26:34.000 --> 00:26:41.000]   solutions that are comparably good or better than the constructed solution. Okay, I see.
[00:26:41.000 --> 00:26:47.000]   Right. I think what they're saying here, and I think that I don't know for sure because
[00:26:47.000 --> 00:26:52.000]   I haven't reading this paper for the first time. So we all go through the same problems
[00:26:52.000 --> 00:26:57.000]   together. And we try and find answers with them together because that's how this beginner
[00:26:57.000 --> 00:27:02.000]   paper reading groups are meant to be. It's like what they're saying is you take a 20
[00:27:02.000 --> 00:27:08.000]   layer network, right? And then you copy that weight to another 20 layer network. And then
[00:27:08.000 --> 00:27:14.000]   you create 36 layers of identity. So 36 layers of identity, which just means like passing
[00:27:14.000 --> 00:27:19.000]   the input, output, input, output, like not doing anything, right? You just pass the input
[00:27:19.000 --> 00:27:26.000]   as is. So now you have a 56 layer network. Correct? So this 20 layers were like CONV,
[00:27:26.000 --> 00:27:30.000]   BatchNorm, Relu, something like that, where each layer was like that. And then you have
[00:27:30.000 --> 00:27:37.000]   this 20 layer deeper architecture. And then you have 56 layers of identity. Or 36 layers
[00:27:37.000 --> 00:27:42.000]   of identity. So this is what they're saying here. Let us consider a shallow architecture
[00:27:42.000 --> 00:27:47.000]   and its deeper counterpart that adds more layers to it. So this is the shallower architecture.
[00:27:47.000 --> 00:27:52.000]   This is the deeper counterpart that adds more layers. And it says the added layers are identity
[00:27:52.000 --> 00:27:59.000]   mapping. Okay, so the added layers, all of this is identity mapping. And the other layers
[00:27:59.000 --> 00:28:06.000]   are copied from learned shallower network. So this weights and these weights are the same.
[00:28:06.000 --> 00:28:11.000]   And then when they say, the next thing that they say is like, if we start treating this
[00:28:11.000 --> 00:28:18.000]   deeper, we start treating this deeper architecture, then because it's just identity, right? And
[00:28:18.000 --> 00:28:24.000]   the 20 layers are like the 20 layers have just been copied. Then in theory, this network
[00:28:24.000 --> 00:28:28.000]   should not be any worse than the 20 layer counterpart, right? So this is the shorter
[00:28:28.000 --> 00:28:33.000]   one, the 20 layer one at the top. And this is the deeper one. I should have said shallower,
[00:28:33.000 --> 00:28:38.000]   but short is fine. So this is the shallower one is at the top with 20 layers. And then
[00:28:38.000 --> 00:28:43.000]   the deeper one is with 56 layers, which has just 36 layers of identity. So the point that
[00:28:43.000 --> 00:28:47.000]   I'm trying to make is like, or the point that the researchers are trying to make rather,
[00:28:47.000 --> 00:28:52.000]   is that the deeper network should not be any difficult to train than the 20 layer network,
[00:28:52.000 --> 00:28:55.000]   right? Because it's just identity. You're not doing anything. You're just copying the
[00:28:55.000 --> 00:29:04.000]   output one after the other. And what they say is that a deeper model should not produce
[00:29:04.000 --> 00:29:10.000]   no higher training error than a shallower counterpart, right? But, and this is where
[00:29:10.000 --> 00:29:15.000]   like things start to get really, really interesting. They go, but experiments show that our current
[00:29:15.000 --> 00:29:20.000]   solvers on hand are unable to find solutions that are comparably good or better than the
[00:29:20.000 --> 00:29:25.000]   constructor solution. So it's actually not able to, like even this network, like this
[00:29:25.000 --> 00:29:31.000]   56 layer network is not able to get the same accuracy as a 20 layer network. So there's
[00:29:31.000 --> 00:29:35.000]   something definitely going wrong as you add more layers. And the researchers are trying
[00:29:35.000 --> 00:29:42.000]   to find the answer for that. I'll just quickly go and check if there's more questions. Okay.
[00:29:42.000 --> 00:29:51.000]   There's a lot of more questions. And keep going, keep going. Okay. I'm going to skip
[00:29:51.000 --> 00:29:56.000]   this for now. Skip the questions for now. I'm going to come back to them later. So I
[00:29:56.000 --> 00:30:01.000]   hope that it is clear by now, like the way they're treating the introduction, like we
[00:30:01.000 --> 00:30:05.000]   should spend most of the time, like if you're going to spend an hour reading a paper, we
[00:30:05.000 --> 00:30:10.000]   should spend 45 minutes just reading introduction, abstract and conclusion. And then the rest
[00:30:10.000 --> 00:30:15.000]   of the paper would become really, really easy. So right now what they're saying is, okay,
[00:30:15.000 --> 00:30:19.000]   our training, this deeper network is really hard. And this is where they start to give
[00:30:19.000 --> 00:30:25.000]   us solutions. So we go, so they go, okay. In this paper, we address the degradation
[00:30:25.000 --> 00:30:29.000]   problem, which is this problem of like, if there's 56 layers, it's still really hard
[00:30:29.000 --> 00:30:34.000]   to train. And they introduce, they do this by introducing a deep residual learning framework.
[00:30:34.000 --> 00:30:39.000]   I don't know what that means. So let's read about it. Instead of hoping each few stack
[00:30:39.000 --> 00:30:44.000]   layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual
[00:30:44.000 --> 00:30:48.000]   mapping. I don't know what that means. I don't understand what that means, but I'm going
[00:30:48.000 --> 00:30:54.000]   to keep reading on. Formally denoting the desired underlying mapping as HX, H of X,
[00:30:54.000 --> 00:31:01.000]   we let the stacked nonlinear layers fit another mapping of FX equals HX minus X. I usually
[00:31:01.000 --> 00:31:06.000]   skip over the map, but maybe this is helpful. Keep going. The original mapping is recast
[00:31:06.000 --> 00:31:18.000]   to FX plus X. Okay. So interesting, I would say. If I know some math, which is not true,
[00:31:18.000 --> 00:31:24.000]   I don't know any math. What they're saying is, and this is like, these are just like
[00:31:24.000 --> 00:31:29.000]   mathematical notations we shouldn't be worried about. And the more papers you read, like
[00:31:29.000 --> 00:31:33.000]   you, you understand, okay, this is something I can skip. This is something I can't skip.
[00:31:33.000 --> 00:31:41.000]   So what they're basically saying here is like, ignore this, ignore H of X or ignore H of
[00:31:41.000 --> 00:31:48.000]   X, all that stuff. All they're saying is we have a layer, right? I have an input X. I'm
[00:31:48.000 --> 00:31:52.000]   going to call this layer F. So what's the output going to be? The output is going to
[00:31:52.000 --> 00:31:59.000]   be F of X, right? What they're saying is this is the plain version. So I'm just going to
[00:31:59.000 --> 00:32:07.000]   call this plain, but they've introduced a residual learning way, which says if my input
[00:32:07.000 --> 00:32:16.000]   is X, my layer is called F, then my output is F of X, but they also add an identity mapping.
[00:32:16.000 --> 00:32:21.000]   So you have the input going through the layer, which is this layer, and then it gives you
[00:32:21.000 --> 00:32:27.000]   an output F of X, but you also take the input as is, and you add it to the output. So you
[00:32:27.000 --> 00:32:35.000]   have your final output as being F of X plus X. So this is all they're saying here. Let's
[00:32:35.000 --> 00:32:42.000]   read that again now. We stacked, it says we stacked this nonlinear layer strict another
[00:32:42.000 --> 00:32:48.000]   mapping using this F of X plus X. So this is the important part. So F of X plus X just
[00:32:48.000 --> 00:32:57.000]   means like this one at the bottom. All right. And we hypothesize that it's easier to optimize
[00:32:57.000 --> 00:33:03.000]   the residual mapping than to optimize the original unreferenced mapping. So cool. Okay.
[00:33:03.000 --> 00:33:08.000]   That's what your hypothesis is. Thanks for letting us know about your hypothesis. And
[00:33:08.000 --> 00:33:12.000]   to the extreme, if an identity mapping were optimal, it would be easier to push the residual
[00:33:12.000 --> 00:33:16.000]   to zero than to fit identity mapping by a stack of nonlinear layers. I don't know what
[00:33:16.000 --> 00:33:22.000]   they're trying to say there. So I'm just going to skip, I think for now. And like, this is
[00:33:22.000 --> 00:33:28.000]   what is called like reading papers in passes. So your first pass, you just do a cursory
[00:33:28.000 --> 00:33:33.000]   read. And in your first pass, you just like read it quickly. And like you skip over things
[00:33:33.000 --> 00:33:36.000]   you don't understand. But again, it depends on like how deep you want to go. So like if
[00:33:36.000 --> 00:33:42.000]   I, if when I was, if I'm doing like, say, if I'm actually trying to do some research
[00:33:42.000 --> 00:33:47.000]   and I'm trying to read a paper, then I would actually just spend a lot of time and I would
[00:33:47.000 --> 00:33:52.000]   do like five passes of a paper, just to, I would like make notes. Okay, this is something
[00:33:52.000 --> 00:33:55.000]   I don't understand. I'm going to read it in my second pass. This is something I don't
[00:33:55.000 --> 00:33:58.000]   understand. I'm going to read it in my third pass. This is something I don't understand.
[00:33:58.000 --> 00:34:01.000]   I'm going to read it in my fourth pass. And like, this is the, this is a really good habit
[00:34:01.000 --> 00:34:06.000]   that works really well for me. And I think I picked it up from that video that I've told
[00:34:06.000 --> 00:34:13.000]   you about, which is from Andrew Ng. Let me just actually show you Andrew Ng's how to
[00:34:13.000 --> 00:34:21.000]   read papers. So there it is. That's the one. How to read research papers and machine learning.
[00:34:21.000 --> 00:34:33.000]   Yeah, I'll just paste that. I think it's a really, really good video. When that opens.
[00:34:33.000 --> 00:34:40.000]   Okay. Nevermind. We'll skip that for now, but you know where to find it. Cool. So it
[00:34:40.000 --> 00:34:49.000]   says, this is where it's interesting. Shortcut connection. Okay. So it says f of x plus x
[00:34:49.000 --> 00:34:54.000]   can be realized by feed forward neural networks with shortcut connections. Figure two. Okay.
[00:34:54.000 --> 00:35:01.000]   Let's look at figure two. Aha. So exactly as I explained things, that's exactly what
[00:35:01.000 --> 00:35:07.000]   it is. You have an input x, you have some weight layer, you get value, some weight layer.
[00:35:07.000 --> 00:35:12.000]   All of this is like my layer, which is like conf, batch, conf value, something like that.
[00:35:12.000 --> 00:35:17.000]   Like just some one basic block. This is called f of x. So your output is f of x, but you
[00:35:17.000 --> 00:35:25.000]   also add the input as is. So your output becomes f of x plus x. Okay. And where x is the identity.
[00:35:25.000 --> 00:35:32.000]   So this connection from there to here is called the shortcut connection. Because in this,
[00:35:32.000 --> 00:35:37.000]   the network isn't really doing like anything. It's not doing any training at all. Right.
[00:35:37.000 --> 00:35:42.000]   The input's coming and it's going as is to the output. That's why it's called the shortcut
[00:35:42.000 --> 00:35:48.000]   connection. And let's read on like the, what the researchers are saying right now is that
[00:35:48.000 --> 00:35:54.000]   it's much easier to train a network like this one, where like it's much easier to train
[00:35:54.000 --> 00:35:59.000]   a network with shortcut connection than a network with no shortcut connection. So if
[00:35:59.000 --> 00:36:13.000]   I kind of like take that, sorry, if I like, there's no shortcut connection. Right. So
[00:36:13.000 --> 00:36:17.000]   if there's no shortcut connection, then the input is going to go always through the weight
[00:36:17.000 --> 00:36:22.000]   layers, then the first layer, the second block, third block, so on all the way to the last
[00:36:22.000 --> 00:36:27.000]   block. But if there's a shortcut connection, then it's going to have the option of either
[00:36:27.000 --> 00:36:30.000]   go through the weight layers or through the shortcut connection. So let's just keep that
[00:36:30.000 --> 00:36:41.000]   in mind. Okay. And it says, right now, now I'm just going to read through, it's like
[00:36:41.000 --> 00:36:45.000]   in our case, the shortcut connection simply perform identity mapping and the outputs are
[00:36:45.000 --> 00:36:48.000]   added to the outputs of the stack layers, which is what they're saying is like, okay,
[00:36:48.000 --> 00:36:54.000]   this is exactly what figure two does in identity mapping. You just add the input to the output
[00:36:54.000 --> 00:36:59.000]   and so on. The entire network can still be trained end to end STD with back propagation,
[00:36:59.000 --> 00:37:05.000]   all good. And it can be implemented using common libraries. Great. We present comprehensive
[00:37:05.000 --> 00:37:11.000]   experiments. Oh, okay. Did I just understand what ResNet is already? Did I just understand
[00:37:11.000 --> 00:37:17.000]   the main contribution? Because I think reading on, this is all it is, right? We just understood
[00:37:17.000 --> 00:37:24.000]   what ResNet is. So in ResNet, they just added the shortcut connection. Like this is the
[00:37:24.000 --> 00:37:30.000]   main research that's been introduced in this massive paper, which is adding the shortcut
[00:37:30.000 --> 00:37:37.000]   connection to your networks. So instead of like, where am I? Here. So instead of like
[00:37:37.000 --> 00:37:43.000]   having my, I have my first block, then I have my second block, then I have my third block
[00:37:43.000 --> 00:37:48.000]   and so on. Like instead of having a network like this, you just take this input, add it
[00:37:48.000 --> 00:37:54.000]   here, take this input, add it here, take this input, add it here, take this input, add it
[00:37:54.000 --> 00:38:01.000]   here and so on. And this becomes residual learning or ResNet. And we've just understood
[00:38:01.000 --> 00:38:07.000]   what ResNet is, which is exactly that. So see how helpful just reading the abstract
[00:38:07.000 --> 00:38:13.000]   introduction has been. Now that we have an understanding of what ResNet is, now that
[00:38:13.000 --> 00:38:16.000]   we have an understanding of vanishing gradient problem is, now that we have an understanding
[00:38:16.000 --> 00:38:20.000]   of what degradation problem is, it's going to be much easier for us to read the whole
[00:38:20.000 --> 00:38:28.000]   paper. So now let's see. We show that our extremely deep residual nets are easy to optimize,
[00:38:28.000 --> 00:38:33.000]   but the counterpart plane nets without the shortcut connection exhibit higher training
[00:38:33.000 --> 00:38:37.000]   error. Okay. So it's much easier to train with shortcut connections. Then our deep residual
[00:38:37.000 --> 00:38:42.000]   nets can easily enjoy accuracy gains from greatly increased depth, producing results
[00:38:42.000 --> 00:38:47.000]   substantially better than previous networks. Great. Awesome. The researchers have fixed
[00:38:47.000 --> 00:38:52.000]   the problem. So what they're saying is in this part, what they're trying to say, what
[00:38:52.000 --> 00:38:58.000]   they're saying is basically that if you have a new train of ResNet, then the more layers
[00:38:58.000 --> 00:39:04.000]   you add, the better the accuracy becomes, which is how we know things in 2021. And we
[00:39:04.000 --> 00:39:09.000]   know things like this, thanks to the ResNet paper in 2015 that solved this problem. Because
[00:39:09.000 --> 00:39:14.000]   until 2015, if you added more layers, it did actually mean that the network's going to
[00:39:14.000 --> 00:39:20.000]   train better. So thanks to the researchers at Microsoft Research for solving this really
[00:39:20.000 --> 00:39:25.000]   massive problem. And we're just now towards the last two paragraphs of the introduction
[00:39:25.000 --> 00:39:31.000]   and we've done with the introduction and abstract. So I will then share what I do after. So I
[00:39:31.000 --> 00:39:35.000]   just spent most of my time reading abstract and introduction. And then it says, okay,
[00:39:35.000 --> 00:39:39.000]   similar phenomena are shown on CIFAR-10. Cool. And on the dataset, it says successfully
[00:39:39.000 --> 00:39:43.000]   trained models. And it says explore models with thousand layers as well. Okay. All good.
[00:39:43.000 --> 00:39:47.000]   So it's just like same results on a different dataset, which is great. Because imagine if
[00:39:47.000 --> 00:39:50.000]   they just got the results on ImageNet and no other dataset, then that means they're
[00:39:50.000 --> 00:39:54.000]   doing something wrong. But the fact that they got the results on ImageNet, they got the
[00:39:54.000 --> 00:40:01.000]   same results on CIFAR-10 and possibly other datasets that we now know of, this just means
[00:40:01.000 --> 00:40:05.000]   that ResNets are awesome. So then they go, okay, on the ImageNet classification dataset,
[00:40:05.000 --> 00:40:13.000]   we obtained excellent results. Our 152 layer network, wow, okay, is the deepest network
[00:40:13.000 --> 00:40:19.000]   ever presented on ImageNet. So until now, there was no network that was deeper than
[00:40:19.000 --> 00:40:26.000]   152 layers. And while still having, it still had lower complexity than VGG. So VGG, I think,
[00:40:26.000 --> 00:40:31.000]   was the winner in 2013 or '14. I can't remember. But VGG was another network that
[00:40:31.000 --> 00:40:37.000]   was before ResNet. And it says our ensemble, ensemble just means like taking the outputs
[00:40:37.000 --> 00:40:44.000]   from multiple models and then kind of combining them. It just says we had 3.5% top five error,
[00:40:44.000 --> 00:40:51.000]   which is great. And it said, okay, this is how we won first place, extremely deep, excellent
[00:40:51.000 --> 00:41:02.000]   generalization performance. Cool. Okay, not much to read here. So what did I do when I
[00:41:02.000 --> 00:41:07.000]   started with ResNet? I actually came in without reading this paper. And I said, look, let's
[00:41:07.000 --> 00:41:13.000]   read this paper together. I started reading the abstract. In the abstract, I spent the
[00:41:13.000 --> 00:41:19.000]   most time just understanding because the abstract would give you a great sense of introduction
[00:41:19.000 --> 00:41:27.000]   in plain English. The introduction section of every paper will give you the problems
[00:41:27.000 --> 00:41:31.000]   that the paper you had or the researchers had to face. And it will also give you a sense
[00:41:31.000 --> 00:41:35.000]   of like, what are the solutions and what are the main contributions. And you can follow
[00:41:35.000 --> 00:41:41.000]   this technique for every paper. So now we know what the problem was. The problem was
[00:41:41.000 --> 00:41:46.000]   that it couldn't, just to summarize, before ResNets, it was really hard to train deep
[00:41:46.000 --> 00:41:51.000]   neural networks. After ResNets, now with these like many, many number of layers, it's much
[00:41:51.000 --> 00:41:54.000]   easier to train networks because they added this shortcut connection. So this shortcut
[00:41:54.000 --> 00:42:00.000]   connection was the main thing that got added to ResNets. And now we can see when we read
[00:42:00.000 --> 00:42:04.000]   the introduction, that once shortcut connection was added, then of course you could train
[00:42:04.000 --> 00:42:08.000]   deeper networks, of course you could train and it had like won the competition and was
[00:42:08.000 --> 00:42:15.000]   much, much better. So I'll just go and check if there's more questions. Are identity layers
[00:42:15.000 --> 00:42:19.000]   not learnable? No, it's just like giving you the output, same as the input. So there's
[00:42:19.000 --> 00:42:25.000]   no learning in identity layers. If not, then how can we claim deeper layers should be worse
[00:42:25.000 --> 00:42:30.000]   than shallower counterpart? If yes, then I understand. Oh yes. So the answer is yes,
[00:42:30.000 --> 00:42:35.000]   they are not learnable. That's why we claim that the deeper should be worse than the shallower
[00:42:35.000 --> 00:42:40.000]   counterpart. Because it's just like identity layers is like means passing the input to
[00:42:40.000 --> 00:42:44.000]   the output. So it's like, just think of it like somebody, like if I'm an identity layer,
[00:42:44.000 --> 00:42:49.000]   on my left, I get my input and I pass it as is to my output. I don't do any processing.
[00:42:49.000 --> 00:42:53.000]   I don't do any math on it. I don't do anything. I just pass it as is. That's the identity
[00:42:53.000 --> 00:43:00.000]   layer. That's my understanding. Identity layers have no parameters. Great. Thanks for answering
[00:43:00.000 --> 00:43:04.000]   that, Namesh. I'm just going to skip this. Identity, results, vanishing gradient problem.
[00:43:04.000 --> 00:43:09.000]   Thanks for that. I tried to summarize most of the points that are shared in the, oh yes.
[00:43:09.000 --> 00:43:17.000]   Thanks, I admit, I should bring this up. So Sai and I believe some other folks from Fastbook
[00:43:17.000 --> 00:43:23.000]   were involved in having beginner pet reading groups at when we were doing Fastbook. And
[00:43:23.000 --> 00:43:29.000]   this is another great blog, which I will share. So if you don't want to watch the whole video,
[00:43:29.000 --> 00:43:34.000]   you could just read this blog on like why you should, or like how you should read research
[00:43:34.000 --> 00:43:38.000]   papers. So there it is. There's the three pass approach. First pass, second pass, third
[00:43:38.000 --> 00:43:47.000]   pass, which I was sharing. 20%, that should last a while. Okay. Can you explain, can you
[00:43:47.000 --> 00:43:52.000]   please explain with an input of two to four by two to four, say two to four by two to
[00:43:52.000 --> 00:43:57.000]   four images, how exactly the identity is added back? Yes, I can. So give me two minutes and
[00:43:57.000 --> 00:44:04.000]   I'll explain. Aren't identity layers and residual paths the same? Yes, they are. Hold on. Residual
[00:44:04.000 --> 00:44:09.000]   path. Oh, let me confirm that. I think identity layers are called, I'm always confused on
[00:44:09.000 --> 00:44:14.000]   like what's a residual path. Because there's one path which the researchers call it as
[00:44:14.000 --> 00:44:20.000]   like the one that goes through the network, the F of X, and then the identity or shortcut
[00:44:20.000 --> 00:44:26.000]   connection that just adds X to F of X. So let me confirm this on what residual path
[00:44:26.000 --> 00:44:30.000]   is. Can you kindly elaborate on the difference between the two? Yes, I could. And then maybe
[00:44:30.000 --> 00:44:36.000]   we can find a way to memorize this because I always keep forgetting. So let's see. I
[00:44:36.000 --> 00:44:43.000]   think what I would do here is like, what is a residual path in ResNet? Let's read that.
[00:44:43.000 --> 00:44:50.000]   So anything important is known as this. Okay, nothing. The output of the shortcut path and
[00:44:50.000 --> 00:44:56.000]   the residual path. Okay, looks like if I go in this one, see, this is how I would like
[00:44:56.000 --> 00:45:00.000]   this. These are real problems I would face when I'm reading a paper. I'm not perfect.
[00:45:00.000 --> 00:45:05.000]   And I don't understand everything that I read. But it's just like reading things over and
[00:45:05.000 --> 00:45:12.000]   over again. Residual learning framework, residual mapping, we expect let these layers would
[00:45:12.000 --> 00:45:17.000]   happen to say that is optimized. I'm just now trying to see if it'd be easy to push
[00:45:17.000 --> 00:45:27.000]   residual to zero. Anything else? Residual, residual, residual nets, net, residual path.
[00:45:27.000 --> 00:45:30.000]   Interesting. I don't know what it is. And I'm just going to skip over this question
[00:45:30.000 --> 00:45:35.000]   because it's very embarrassing that I don't know what the answer is right now. I'll come
[00:45:35.000 --> 00:45:41.000]   back to this when I, I'll reply to that on the forums. But I definitely will come back.
[00:45:41.000 --> 00:45:50.000]   I think that's the answer is the answer to this. No, not in there. All right, I'm just
[00:45:50.000 --> 00:46:00.000]   going to skip this for now. But good question. Okay, and then can you please explain with
[00:46:00.000 --> 00:46:07.000]   an input two to four how the edit is active back? Yes, I can. So let's go quickly to that.
[00:46:07.000 --> 00:46:13.000]   So you have your input, say, two to four by two to four. This is my deep neural network.
[00:46:13.000 --> 00:46:18.000]   When you pass this, let's say this is just my first block. If you have two to four by
[00:46:18.000 --> 00:46:21.000]   two to four and you have a three channel, so you're going to have three by two to four
[00:46:21.000 --> 00:46:26.000]   by two to four, right? That's your input. When you pass that through a conv batch norm
[00:46:26.000 --> 00:46:31.000]   ReLU block, which is my first block, you're going to get an output that looks something
[00:46:31.000 --> 00:46:39.000]   like, say, 64 by, I don't know, like 160 by 160, just as an example. So that's the output
[00:46:39.000 --> 00:46:46.000]   that you're going to get, right? Now what you can do is like, because this output, so
[00:46:46.000 --> 00:46:50.000]   there's like different ways of like adding, there's like different ways of like adding
[00:46:50.000 --> 00:46:54.000]   things to your, actually, this is something that's, I shouldn't discuss it now. This is
[00:46:54.000 --> 00:47:00.000]   something that gets discussed all the way down in three. So we're not done with this
[00:47:00.000 --> 00:47:06.000]   paper yet. So I'll actually, I'll answer this when we come to the section three. So let's
[00:47:06.000 --> 00:47:13.000]   skip, I'm going to skip that question for now. So going forward, I'm also conscious
[00:47:13.000 --> 00:47:17.000]   of time. It's 9.17 already. We've been doing this for an hour, but let me tell you what
[00:47:17.000 --> 00:47:23.000]   I would do next. And I'm going to come back to this question from Ganesh as well. So I've
[00:47:23.000 --> 00:47:28.000]   got two questions to answer. So I'm coming back to now. So if I would read the abstract
[00:47:28.000 --> 00:47:31.000]   and I would read the introduction, the last thing I would read is the conclusion, if there
[00:47:31.000 --> 00:47:36.000]   is one. Is there a conclusion here? That's appendix. I'm just going to skip the appendix.
[00:47:36.000 --> 00:47:43.000]   Is there a conclusion? Is there a conclusion? No conclusion. It's just analysis. Okay. So
[00:47:43.000 --> 00:47:49.000]   it's just analysis, analysis, more experiments that the researchers ran. Great. Then I would
[00:47:49.000 --> 00:47:54.000]   look at this figure. It says, okay, I'm just going to look at this figure and skim through
[00:47:54.000 --> 00:48:00.000]   it. Training on image net, thin curves denote the training error and bold curves denote
[00:48:00.000 --> 00:48:08.000]   the validation error. So you can see now, if it's a plain net, 18 layer performs better
[00:48:08.000 --> 00:48:13.000]   than 34 layer. But if it's a res net with the residual or shortcut connection, the 34
[00:48:13.000 --> 00:48:19.000]   layer performs better than the 18 layer. So the problem has been fixed. Okay, great. And
[00:48:19.000 --> 00:48:25.000]   then this is just like, okay, this is how the blocks are in this. This might just be
[00:48:25.000 --> 00:48:30.000]   like how the different architectures are because ResNet is ResNet 18, ResNet 34, ResNet 50,
[00:48:30.000 --> 00:48:35.000]   101 and so on. So I'm just going to skip over this for now. Great. But I looked at that
[00:48:35.000 --> 00:48:44.000]   figure. Okay. This just says, oh, sorry, what did I do? So this just says VGG-19 looks
[00:48:44.000 --> 00:48:49.000]   like this. So you have a 3x3 by con 64 all the way. Then you keep doing a pooling. Then
[00:48:49.000 --> 00:48:53.000]   you have this and you have this. Okay, that's another block. Then you have this block and
[00:48:53.000 --> 00:48:58.000]   so on. In 34 layer plain, this is what it says. You have an input image. Then you have
[00:48:58.000 --> 00:49:04.000]   your 7x7 con. Then you do the pooling. Then you have a 3x3 con, 3x3 con and so on. But
[00:49:04.000 --> 00:49:11.000]   there's no residual connection, right? In 34 layer residual, oh, I know the answer on
[00:49:11.000 --> 00:49:21.000]   like how things are added. You just add padding. So now you can see in 34, just trying to answer,
[00:49:21.000 --> 00:49:26.000]   just answering this question from Ganesh, what you have is like when you have an input
[00:49:26.000 --> 00:49:31.000]   over here, let's say that's 224, 3x224 by 224. Actually, it's not going to be that because
[00:49:31.000 --> 00:49:39.000]   you have a 7x7 con. But let's just say you have a 3x24, 224 by 224 input. You add enough
[00:49:39.000 --> 00:49:43.000]   padding that your output remains the same shape and then you can add the input to the
[00:49:43.000 --> 00:49:48.000]   output. Does that make sense? Like your output is still going to be something like 64 channels
[00:49:48.000 --> 00:49:57.000]   by 224, 224, I think. But I'll have to confirm that. Yeah, that's what it says, right? 64,
[00:49:57.000 --> 00:50:02.000]   64. Then you go to 128 channels and you divide by 2. This is where you do the pooling. And
[00:50:02.000 --> 00:50:08.000]   then you go 128 and then you go 256 and you divide by 2. Okay, makes sense. So you've
[00:50:08.000 --> 00:50:15.000]   got to read through all this. We've got to understand what this by 2 means. Like that's
[00:50:15.000 --> 00:50:18.000]   the difference. That's why I'm looking at these figures because now I want to like find
[00:50:18.000 --> 00:50:23.000]   things that I want to read and I want to understand. So I want to understand what this by 2 means.
[00:50:23.000 --> 00:50:28.000]   I want to understand what this means. And I also want to understand like how they add
[00:50:28.000 --> 00:50:31.000]   the input to the output. So in this case, they're just adding padding, which means the
[00:50:31.000 --> 00:50:36.000]   output is the same as the input. So in this block, your output feature map is going to
[00:50:36.000 --> 00:50:41.000]   be the same shape as the input feature map. So you can just add the two together. But
[00:50:41.000 --> 00:50:48.000]   in this, when they do the by 2 or the pooling, the output feature map is going to be half
[00:50:48.000 --> 00:50:52.000]   the size of the input feature map. And that's why you can see the skip connection is like
[00:50:52.000 --> 00:50:58.000]   this dotted skip connection. So they're doing something interesting in that part. Okay,
[00:50:58.000 --> 00:51:05.000]   so that's that. So we need to understand now how this architecture is implemented and built.
[00:51:05.000 --> 00:51:13.000]   And that's the last bit like that's the. That's the last bit like after this, we should be
[00:51:13.000 --> 00:51:22.000]   done with with ResNet. So where are we? We should go to section three. Let us consider
[00:51:22.000 --> 00:51:27.000]   HFX. I'm just going to read quickly through this part. HFX is an underlying. Oh, by the
[00:51:27.000 --> 00:51:33.000]   way, if there's any questions like over here, like I was just like showing you the process
[00:51:33.000 --> 00:51:38.000]   of how I would go through reading about it, like I first would go abstract introduction,
[00:51:38.000 --> 00:51:43.000]   conclusion, then look at all the figures, try and get an understanding in my head. So
[00:51:43.000 --> 00:51:48.000]   now I actually just by looking at this figure, I already understand everything that's going
[00:51:48.000 --> 00:51:53.000]   on in the ResNet. But if we are all to look at and understand what's going on in ResNet,
[00:51:53.000 --> 00:52:00.000]   let's just quickly go back to section three. So, okay, this is this is the part I'm going
[00:52:00.000 --> 00:52:04.000]   to skip because we've already read through. We've already read through this part. It's
[00:52:04.000 --> 00:52:11.000]   just saying the is FX, FX plus X. Then it says degradation problem. I'm going to skip
[00:52:11.000 --> 00:52:17.000]   over this. And keep going and then we go identity mapping by shortcuts, we just read like this
[00:52:17.000 --> 00:52:25.000]   is what they introduce is like your output is now FX plus X. And they go represents a
[00:52:25.000 --> 00:52:29.000]   residual mapping to be learned. So this is the mapping to be learned. Okay, so the residual
[00:52:29.000 --> 00:52:36.000]   part is actually the part that is the FX. Because this is what it says, right? The function
[00:52:36.000 --> 00:52:41.000]   FX represents the residual mapping to be learned. So tonight, I think I've also got the answer
[00:52:41.000 --> 00:52:48.000]   for your question. Residual part is the part that goes through the like, that's the network
[00:52:48.000 --> 00:52:54.000]   that goes through the network. And then the identity layer is the skip connection. So
[00:52:54.000 --> 00:53:03.000]   where are we if I have some? Okay, so this F of X here, this is the residual part. And
[00:53:03.000 --> 00:53:10.000]   skip shortcut connection on the side. I hope that helps. Okay, there's more background
[00:53:10.000 --> 00:53:16.000]   noise from my side. So ignore that and we keep going. All right, all good. So now I'm
[00:53:16.000 --> 00:53:22.000]   just going to share what's happening here, right? So they go okay, identity mapping by
[00:53:22.000 --> 00:53:28.000]   shortcut that is all good. And then they're saying, the dimension, this is the important
[00:53:28.000 --> 00:53:32.000]   part where they say the dimension should be same. If they're not, we do something like
[00:53:32.000 --> 00:53:36.000]   this. So let me explain now section three, and that's it, we're going to be done with
[00:53:36.000 --> 00:53:46.000]   we're going to be done with ResNet. So remember, in a deep neural network, you have say, conf,
[00:53:46.000 --> 00:53:52.000]   bash norm, relu, conf, pretty much, like, I'm just going to skip this, I'm just going
[00:53:52.000 --> 00:53:59.000]   to say, conf, relu, and then conf. And this is my one block, right? So if I have my input,
[00:53:59.000 --> 00:54:03.000]   and I'm trying to answer this question from Ganesh, right, right there. So this is the
[00:54:03.000 --> 00:54:09.000]   answer to Ganesh's question. If I have my input as three, by two to four, by two to
[00:54:09.000 --> 00:54:14.000]   four, let's say if I add padding one, or like I add basically, this is a say a three by
[00:54:14.000 --> 00:54:18.000]   three conf, and I add padding one, three by three conf, and I add padding one, then my
[00:54:18.000 --> 00:54:23.000]   output from this is going to be three by two to four by two to four, right? In which case,
[00:54:23.000 --> 00:54:29.000]   I could just add the input to the output, right? So done. That's the first block, like
[00:54:29.000 --> 00:54:36.000]   this is how this thing will work. And this block is represented by all the purples one
[00:54:36.000 --> 00:54:44.000]   at the top. So this, that thing that I just showed you is represented by this block. Actually,
[00:54:44.000 --> 00:54:52.000]   sorry. This is one block. This is one block. And then this is one block. So the first three
[00:54:52.000 --> 00:54:56.000]   blocks add padding. And now suddenly what happens after the three blocks, the first
[00:54:56.000 --> 00:55:02.000]   three blocks is like, now you have a transition block, which is this transition block. So
[00:55:02.000 --> 00:55:08.000]   what happens in this case is like, where's my drawing again? There it is. Because like,
[00:55:08.000 --> 00:55:13.000]   you can't really propagate two to four by two to four all the way down, right? What
[00:55:13.000 --> 00:55:17.000]   you would have to do is you'd have to increase the number of channels. And then you'd have
[00:55:17.000 --> 00:55:21.000]   to go like, when you increase the number of channels, you would use the spatial dimension,
[00:55:21.000 --> 00:55:25.000]   because you can't really increase the number of channels and keep the dimension the same.
[00:55:25.000 --> 00:55:33.000]   So what happens then in deep neural networks is like, now you have say 64 channels, or
[00:55:33.000 --> 00:55:41.000]   like, again, you have the same thing, three by three value, three by three. So your input
[00:55:41.000 --> 00:55:48.000]   is again, three by two to four by two to four. Pass that in. And then let's say the output
[00:55:48.000 --> 00:55:57.000]   line is 64 by, say, 180 by 180, right? That's the number of channels. Oh, sorry, one sec.
[00:55:57.000 --> 00:56:04.000]   Am I correct in saying so? Yeah, it's going from 64 channels to say 128. So in this case,
[00:56:04.000 --> 00:56:10.000]   let's say we're going from three to 64 channels, that should matter. But now if you want to
[00:56:10.000 --> 00:56:16.000]   add this input to this output, what you would have to do is like, you would have to change
[00:56:16.000 --> 00:56:25.000]   the, you would have to change, just give me one sec. That's going to be 64 here. And then
[00:56:25.000 --> 00:56:30.000]   you make that to be 128 channels. And then you're going to pass that through this. Okay.
[00:56:30.000 --> 00:56:40.000]   So then what you need to do next is, where are we? My one note is playing up today. So
[00:56:40.000 --> 00:56:44.000]   the next thing that we do is like, we can't just add an identity connection here, right?
[00:56:44.000 --> 00:56:51.000]   Because you can't just add three by two to four. It's bell ringing. Just ignore that,
[00:56:51.000 --> 00:56:59.000]   guys. So you can't really add the three by two to four to the 64 by 180 by 180. But instead
[00:56:59.000 --> 00:57:04.000]   what you do is you add another layer called H of S. So this has been explained here. You
[00:57:04.000 --> 00:57:14.000]   add another W of S. You add another WS, which is the linear projection. When what that does
[00:57:14.000 --> 00:57:20.000]   is like it makes the input to be the same shape as the output. So this is the main difference
[00:57:20.000 --> 00:57:27.000]   between, like you've, now you're adding like, you're still adding this linear projection
[00:57:27.000 --> 00:57:31.000]   layer that what it will do is it will make your output be the same shape as the input.
[00:57:31.000 --> 00:57:36.000]   And this is how now you can even like increase the number of channels, reduce the spatial
[00:57:36.000 --> 00:57:42.000]   dimension, and still be able to use the identity connection. So I hope that makes things clear.
[00:57:42.000 --> 00:57:48.000]   But what I would recommend now is like, go back, read through this whole research paper,
[00:57:48.000 --> 00:57:53.000]   read and understand. And then what I do plan on doing next week is I do plan on going through
[00:57:53.000 --> 00:57:58.000]   the code. So this is where everything, right now we just have a basic understanding of
[00:57:58.000 --> 00:58:04.000]   what's happening in here. But if we actually try and implement these things in code, that's
[00:58:04.000 --> 00:58:09.000]   this is where things will become really, really, like that's where the understanding will be
[00:58:09.000 --> 00:58:14.000]   completely strong. That is where we will relate everything in code to the paper. So next week
[00:58:14.000 --> 00:58:22.000]   what I'm going to do is if you have, if you haven't heard of TIM, oh not that, TIM,
[00:58:22.000 --> 00:58:29.000]   PyTorch image models, this one. If you haven't heard of TIM, which is the PyTorch image models
[00:58:29.000 --> 00:58:33.000]   repository, what we're going to do in this PyTorch image models repository is that we're
[00:58:33.000 --> 00:58:38.000]   going to go through all of the questions. We're going to go through, sorry, not questions,
[00:58:38.000 --> 00:58:42.000]   we're just going to go through the TIM models and we're going to go through the ResNet code
[00:58:42.000 --> 00:58:48.000]   in here. And we're going to implement ResNet in PyTorch. So that's it for me today.
[00:58:48.000 --> 00:58:53.000]   So Ganesh goes, yes, does the basically the linear layer makes the input shape same as
[00:58:53.000 --> 00:58:58.000]   the output shape? Is my understanding correct? Yes, that is absolutely correct. It makes
[00:58:58.000 --> 00:59:06.000]   it the same as, it makes both the shapes the same so you can add them. So that's pretty
[00:59:06.000 --> 00:59:15.000]   much where I'll stop with ResNet. And I will see you guys next week. So we can all now
[00:59:15.000 --> 00:59:19.000]   take what we've understood today and we can all implement it in code from scratch. So
[00:59:19.000 --> 00:59:27.000]   what I plan to do is I plan to take this understanding and implement it line by line from scratch
[00:59:27.000 --> 00:59:32.000]   in PyTorch. And so that's how we're going to do now the beginner friendly paper reading
[00:59:32.000 --> 00:59:39.000]   groups. And I'm really, I apologize for all the noise in the back. I'm still getting
[00:59:39.000 --> 00:59:45.000]   settled in. It's literally been a day that I've flown into Delhi and I'm still setting
[00:59:45.000 --> 00:59:50.000]   up my workstation and all that stuff on the side. So hopefully by next week, that will
[00:59:50.000 --> 00:59:55.000]   all be fixed. GuruJS, yes, that's next week. It's not next week, it's next week. I will
[00:59:55.000 --> 01:00:02.000]   share the details with everybody on the FastAI forums. So go here, FastAI forums, go to paper
[01:00:02.000 --> 01:00:07.000]   reading group. And this is where I'm going to share all the details. So thanks for joining.
[01:00:07.000 --> 01:00:08.200]   See you guys next week.
[01:00:08.200 --> 01:00:18.180]   [BLANK_AUDIO]

