
[00:00:00.000 --> 00:00:03.060]   [MUSIC PLAYING]
[00:00:03.060 --> 00:00:05.060]   For a lot of these things, it's actually really easy
[00:00:05.060 --> 00:00:06.260]   to make something poisonous.
[00:00:06.260 --> 00:00:10.180]   And as governments, as industry has grown recognition
[00:00:10.180 --> 00:00:12.380]   of this fact, you just have this recurring theme
[00:00:12.380 --> 00:00:16.700]   that all of a sudden, you invent a miracle, something or other,
[00:00:16.700 --> 00:00:17.940]   of plastics.
[00:00:17.940 --> 00:00:20.820]   Plastics were thought to be the way of the future in the 1950s.
[00:00:20.820 --> 00:00:23.540]   They're also a type of just molecular product.
[00:00:23.540 --> 00:00:25.420]   And now we find they choke seagulls.
[00:00:25.420 --> 00:00:28.060]   They choke baby turtles.
[00:00:28.060 --> 00:00:29.980]   There's microplastics everywhere.
[00:00:29.980 --> 00:00:32.860]   And I think this is a type of generalized toxicity issue
[00:00:32.860 --> 00:00:36.500]   that we realize if you make large quantities
[00:00:36.500 --> 00:00:38.900]   of a new substance that the world brought these
[00:00:38.900 --> 00:00:42.020]   and prepared to digest, what happens
[00:00:42.020 --> 00:00:45.140]   is 30 years down the line, you're like, oh, crap.
[00:00:45.140 --> 00:00:46.580]   I killed off the trout.
[00:00:46.580 --> 00:00:49.020]   I killed off the eagles.
[00:00:49.020 --> 00:00:50.740]   So it all kind of comes down to the fact
[00:00:50.740 --> 00:00:54.180]   that I think living systems are extraordinarily complicated.
[00:00:54.180 --> 00:00:58.380]   And making something that is tested and safe for a living
[00:00:58.380 --> 00:01:01.140]   thing to interact is actually very challenging.
[00:01:01.140 --> 00:01:03.540]   You're listening to Gradient Dissent, a show where
[00:01:03.540 --> 00:01:05.780]   we learn about making machine learning models work
[00:01:05.780 --> 00:01:06.780]   in the real world.
[00:01:06.780 --> 00:01:08.660]   I'm your host, Lukas Biewald.
[00:01:08.660 --> 00:01:10.420]   I'm especially excited to talk to Bharat
[00:01:10.420 --> 00:01:14.060]   because he created the DeepChem open source project, which
[00:01:14.060 --> 00:01:15.580]   we've seen a lot of our customers
[00:01:15.580 --> 00:01:17.500]   at Weights & Biases using.
[00:01:17.500 --> 00:01:20.460]   And it seems to be the most popular library for people
[00:01:20.460 --> 00:01:23.980]   working on deep learning applied to chemistry and biology.
[00:01:23.980 --> 00:01:26.660]   He also made an open source data set
[00:01:26.660 --> 00:01:30.180]   called MoleculeNet, which is a benchmark suite to facilitate
[00:01:30.180 --> 00:01:33.020]   the development of molecular algorithms.
[00:01:33.020 --> 00:01:34.900]   He got his PhD in computer science
[00:01:34.900 --> 00:01:37.340]   from Stanford, where he studied deep learning applied
[00:01:37.340 --> 00:01:38.500]   to drug discovery.
[00:01:38.500 --> 00:01:40.260]   And he's the lead author of TensorFlow
[00:01:40.260 --> 00:01:42.260]   for Deep Learning, From Linear Regression
[00:01:42.260 --> 00:01:43.580]   to Reinforcement Learning.
[00:01:43.580 --> 00:01:45.580]   I was really excited to talk to you.
[00:01:45.580 --> 00:01:47.980]   We've been seeing a lot of customers
[00:01:47.980 --> 00:01:53.100]   come in doing drug discovery and other medical applications.
[00:01:53.100 --> 00:01:56.340]   And it's something that I'm not super familiar with,
[00:01:56.340 --> 00:01:58.860]   but seems incredibly meaningful.
[00:01:58.860 --> 00:02:01.620]   So we've gotten a chance to talk to a whole bunch
[00:02:01.620 --> 00:02:04.340]   of our customers and ask them what they're doing.
[00:02:04.340 --> 00:02:05.700]   And one thing that keeps coming up
[00:02:05.700 --> 00:02:09.180]   is actually the DeepChem library that I think
[00:02:09.180 --> 00:02:11.580]   you were the original author of.
[00:02:11.580 --> 00:02:15.980]   So I really wanted to start off by just asking you about that.
[00:02:15.980 --> 00:02:19.380]   What inspired you to make it and what problems
[00:02:19.380 --> 00:02:21.220]   were you trying to solve?
[00:02:21.220 --> 00:02:22.860]   Yeah, absolutely.
[00:02:22.860 --> 00:02:25.180]   First, thank you for having me on the show.
[00:02:25.180 --> 00:02:28.460]   I'm glad, excited to chat as well.
[00:02:28.460 --> 00:02:30.740]   Lots of folks I know have been using weights and biases
[00:02:30.740 --> 00:02:33.380]   to train models and track experiments.
[00:02:33.380 --> 00:02:37.940]   So I think it should be a fun conversation, I hope.
[00:02:37.940 --> 00:02:42.580]   So I think a few years ago, basically, during my PhD,
[00:02:42.580 --> 00:02:46.220]   I did a internship at Google, where
[00:02:46.220 --> 00:02:50.100]   I used their mini computers to train some deep learning
[00:02:50.100 --> 00:02:53.820]   models for molecular design broadly.
[00:02:53.820 --> 00:02:56.780]   But I think what happened was, as with all good things,
[00:02:56.780 --> 00:02:59.420]   the internship came to an end, and I had to head back
[00:02:59.420 --> 00:03:00.060]   to Stanford.
[00:03:00.060 --> 00:03:02.380]   And then I found all of a sudden I no longer had access
[00:03:02.380 --> 00:03:03.260]   to all that code.
[00:03:03.260 --> 00:03:05.220]   I couldn't really reproduce my results.
[00:03:05.220 --> 00:03:06.820]   So I think the starting point was I just
[00:03:06.820 --> 00:03:10.940]   wanted to reproduce the results of my own paper.
[00:03:10.940 --> 00:03:12.780]   And I think to start, basically, it
[00:03:12.780 --> 00:03:18.500]   was just a few scripts in Theano and Keras at that point.
[00:03:18.500 --> 00:03:20.860]   And I put it up on GitHub as why not.
[00:03:20.860 --> 00:03:23.780]   Then a few more people started to use it.
[00:03:23.780 --> 00:03:27.220]   And it's just kind of grown slowly and steadily from there.
[00:03:27.220 --> 00:03:30.940]   I think the original kind of aim of DeepChem
[00:03:30.940 --> 00:03:34.060]   was really to enable answering questions
[00:03:34.060 --> 00:03:36.380]   about so-called small molecules.
[00:03:36.380 --> 00:03:40.260]   So most of the drugs that we take,
[00:03:40.260 --> 00:03:43.020]   your Tylenols, your ibuprofens, things like that
[00:03:43.020 --> 00:03:45.940]   are all small molecules.
[00:03:45.940 --> 00:03:47.700]   But over time, I think pharma has actually
[00:03:47.700 --> 00:03:48.620]   begun to shift a bit.
[00:03:48.620 --> 00:03:51.020]   So now there's neuroclastic medicines.
[00:03:51.020 --> 00:03:53.540]   There's, of course, things like vaccines.
[00:03:53.540 --> 00:03:55.260]   So nowadays, I think DeepChem is slowly
[00:03:55.260 --> 00:03:59.660]   trying to grow out to enable open source medicine
[00:03:59.660 --> 00:04:06.260]   discovery across a broader swath of modern biotech.
[00:04:06.260 --> 00:04:08.460]   So that's just kind of a little bit about the project.
[00:04:08.460 --> 00:04:12.060]   I think there is a fairly active community of users.
[00:04:12.060 --> 00:04:14.780]   There's a number of educational materials and tutorials
[00:04:14.780 --> 00:04:17.620]   built up around it.
[00:04:17.620 --> 00:04:20.420]   I think it's also that a lot of medicine
[00:04:20.420 --> 00:04:23.660]   is quite proprietary, like medicine discovery.
[00:04:23.660 --> 00:04:27.420]   There's biotechs, if you often see their advertising
[00:04:27.420 --> 00:04:29.220]   material, like our proprietary algorithm,
[00:04:29.220 --> 00:04:33.300]   our proprietary technique, which has worked fine
[00:04:33.300 --> 00:04:34.980]   for the industry for a long time.
[00:04:34.980 --> 00:04:37.860]   That's the way most medicine we know was discovered.
[00:04:37.860 --> 00:04:39.580]   But of course, as we know in tech,
[00:04:39.580 --> 00:04:41.900]   there's just been a shift in that open source
[00:04:41.900 --> 00:04:45.060]   is increasingly a foundational part of the way we build
[00:04:45.060 --> 00:04:47.020]   companies, we discover things.
[00:04:47.020 --> 00:04:48.540]   So I think part of the goal of DeepChem
[00:04:48.540 --> 00:04:51.500]   is to bring some of this open source energy
[00:04:51.500 --> 00:04:54.300]   to the biotech drug discovery community
[00:04:54.300 --> 00:04:59.660]   and enable more people to be able to share in these tools.
[00:04:59.660 --> 00:05:02.980]   It seems like you've definitely been successful at that.
[00:05:02.980 --> 00:05:06.620]   Even before I knew of you, I was talking to folks
[00:05:06.620 --> 00:05:10.620]   at Genentech and GSK.
[00:05:10.620 --> 00:05:13.220]   I would say over half of the conversations
[00:05:13.220 --> 00:05:16.180]   I've had with pharma companies have mentioned DeepChem.
[00:05:16.180 --> 00:05:18.180]   I thought it was pretty cool that they're all
[00:05:18.180 --> 00:05:23.340]   using the same platform and contributing IP.
[00:05:23.340 --> 00:05:25.460]   I didn't know that pharma did that at all.
[00:05:25.460 --> 00:05:28.980]   So that seems really wonderful.
[00:05:28.980 --> 00:05:31.820]   I think it definitely is a new shift in thinking.
[00:05:31.820 --> 00:05:34.180]   But of course, pharma has seen the fact
[00:05:34.180 --> 00:05:37.180]   that TensorFlow is open source, PyTorch is open source.
[00:05:37.180 --> 00:05:40.340]   So I think it is the beginnings of a shift.
[00:05:40.340 --> 00:05:43.260]   At the same time, I think IP considerations definitely
[00:05:43.260 --> 00:05:45.220]   do matter a lot.
[00:05:45.220 --> 00:05:48.900]   So I think a lot of folks find they
[00:05:48.900 --> 00:05:51.220]   can't contribute at some workplaces, which is fine.
[00:05:51.220 --> 00:05:53.180]   I think it's kind of just a policy.
[00:05:53.180 --> 00:05:55.300]   But there is still a culture of caution
[00:05:55.300 --> 00:05:57.740]   around potentially releasing valuable IP.
[00:05:57.740 --> 00:05:59.860]   But I think what helps things a bit
[00:05:59.860 --> 00:06:03.420]   is there is this recognition that oftentimes it's
[00:06:03.420 --> 00:06:05.940]   the actual data that's the core IP.
[00:06:05.940 --> 00:06:07.380]   It's not necessarily the algorithm.
[00:06:07.380 --> 00:06:09.780]   That's just kind of calculus.
[00:06:09.780 --> 00:06:12.980]   And so I think there is some favorable shifts
[00:06:12.980 --> 00:06:13.680]   in the industry.
[00:06:13.680 --> 00:06:15.220]   But it's definitely something that's
[00:06:15.220 --> 00:06:16.900]   only beginning to happen.
[00:06:16.900 --> 00:06:18.540]   So just taking a step back, because I
[00:06:18.540 --> 00:06:21.420]   think not everyone necessarily knows the field at all,
[00:06:21.420 --> 00:06:24.060]   and I actually didn't until maybe six months ago
[00:06:24.060 --> 00:06:26.660]   when we started to see our users doing this.
[00:06:26.660 --> 00:06:28.440]   What's sort of the canonical problem here
[00:06:28.440 --> 00:06:31.980]   that pharma is trying to solve?
[00:06:31.980 --> 00:06:35.100]   Yeah, I think it's a great question.
[00:06:35.100 --> 00:06:38.100]   So I think at heart, the goal really
[00:06:38.100 --> 00:06:41.460]   is to design medicine for diseases you care about.
[00:06:41.460 --> 00:06:43.880]   And the reality is this is kind of an extraordinarily
[00:06:43.880 --> 00:06:45.480]   complicated process.
[00:06:45.480 --> 00:06:47.600]   And I'd say even now, machine learning is only
[00:06:47.600 --> 00:06:50.360]   useful for 10% of it.
[00:06:50.360 --> 00:06:54.160]   And what kind of the task here is that you say,
[00:06:54.160 --> 00:06:55.920]   you identify a disease.
[00:06:55.920 --> 00:06:59.580]   Then you want to find a hypothesis for what
[00:06:59.580 --> 00:07:00.660]   causes the disease.
[00:07:00.660 --> 00:07:04.000]   Maybe there is a protein that somehow has become misconfigured
[00:07:04.000 --> 00:07:04.880]   or mutated in the body.
[00:07:04.880 --> 00:07:11.400]   Maybe there can be a whole host of disease-causing factors
[00:07:11.400 --> 00:07:14.360]   that you oftentimes try to take a reductionist view
[00:07:14.360 --> 00:07:16.960]   and narrow that down to one protein target.
[00:07:16.960 --> 00:07:19.200]   So you say that if I somehow could
[00:07:19.200 --> 00:07:21.520]   change the behavior of this protein,
[00:07:21.520 --> 00:07:23.720]   I could potentially cure this disease.
[00:07:23.720 --> 00:07:24.520]   It's a hypothesis.
[00:07:24.520 --> 00:07:25.260]   It might be right.
[00:07:25.260 --> 00:07:26.840]   It might be wrong.
[00:07:26.840 --> 00:07:28.800]   But it's a good starting place.
[00:07:28.800 --> 00:07:31.080]   Then you kind of go out and you say, all right,
[00:07:31.080 --> 00:07:32.080]   now I know this protein.
[00:07:32.080 --> 00:07:34.520]   Can I find a molecule that causes
[00:07:34.520 --> 00:07:36.160]   it to have some interaction?
[00:07:36.160 --> 00:07:39.000]   So there's a few kind of mental models for this.
[00:07:39.000 --> 00:07:41.080]   You can think of it as a lock and key.
[00:07:41.080 --> 00:07:45.720]   You can kind of think of it as basically an interacting agent
[00:07:45.720 --> 00:07:48.360]   that kind of comes in, the drug that is,
[00:07:48.360 --> 00:07:50.040]   and shifts the behavior of the protein
[00:07:50.040 --> 00:07:51.560]   in a way that's favorable.
[00:07:51.560 --> 00:07:53.920]   So the goal computationally at Acruteloft
[00:07:53.920 --> 00:07:56.880]   was saying that design the molecule.
[00:07:56.880 --> 00:07:59.000]   Given the description of this problem,
[00:07:59.000 --> 00:08:01.520]   print out the ideal molecule for this.
[00:08:01.520 --> 00:08:04.000]   Now, the reason this gets challenging is that the ideal
[00:08:04.000 --> 00:08:06.500]   molecule is extremely hard.
[00:08:06.500 --> 00:08:07.880]   I think one of the hardest problems
[00:08:07.880 --> 00:08:10.680]   here is that there's this question of toxicity.
[00:08:10.680 --> 00:08:12.880]   So there's all sorts of--
[00:08:12.880 --> 00:08:14.480]   I think a silly example for this is
[00:08:14.480 --> 00:08:15.880]   if you want to kill cancer cells,
[00:08:15.880 --> 00:08:17.080]   you can pour bleach on them.
[00:08:17.080 --> 00:08:18.880]   It's just that you can't drink that bleach.
[00:08:18.880 --> 00:08:21.040]   That's going to kill you too.
[00:08:21.040 --> 00:08:25.160]   So a lot of medicine is pretty indistinguishable from poison.
[00:08:25.160 --> 00:08:28.720]   It's really targeted poison that goes after one
[00:08:28.720 --> 00:08:32.440]   particular part of the body.
[00:08:32.440 --> 00:08:34.020]   So when you're designing medicine,
[00:08:34.020 --> 00:08:36.320]   you're often just struggling with this challenge
[00:08:36.320 --> 00:08:38.240]   if you're on this very razor thin design
[00:08:38.240 --> 00:08:41.040]   edge of between poison to medicine.
[00:08:41.040 --> 00:08:44.000]   And you also often don't have a precise model
[00:08:44.000 --> 00:08:47.520]   of whether the potential drug works or not
[00:08:47.520 --> 00:08:49.400]   until you try it in real patients.
[00:08:49.400 --> 00:08:51.880]   So you try to make proxy models for this.
[00:08:51.880 --> 00:08:53.880]   Traditionally, you'd have something like a rat
[00:08:53.880 --> 00:08:55.560]   that has some variant of the disease.
[00:08:55.560 --> 00:08:57.960]   Or sometimes it's things like cats or even dogs.
[00:08:57.960 --> 00:09:02.720]   But when you think it's safe, you then
[00:09:02.720 --> 00:09:04.360]   try it out on real patients.
[00:09:04.360 --> 00:09:07.160]   So this is the clinical trial process.
[00:09:07.160 --> 00:09:10.560]   There's phase one, which tests toxicity.
[00:09:10.560 --> 00:09:11.640]   Is it safe for humans?
[00:09:11.640 --> 00:09:13.880]   There's phase two that tests efficacy.
[00:09:13.880 --> 00:09:16.880]   Is this actually showing effect in a group of patients
[00:09:16.880 --> 00:09:18.400]   that I'm trialing this on?
[00:09:18.400 --> 00:09:20.360]   And then phase three is basically, OK,
[00:09:20.360 --> 00:09:21.360]   we think there's effect.
[00:09:21.360 --> 00:09:25.600]   Let's make sure on a big trial with lots of people.
[00:09:25.600 --> 00:09:27.600]   And occasionally, there's things like phase four,
[00:09:27.600 --> 00:09:30.840]   which is after the drug is being used by real people,
[00:09:30.840 --> 00:09:32.920]   let's do more studies to understand the real effects
[00:09:32.920 --> 00:09:35.520]   it's having on patients so we can get better guidance
[00:09:35.520 --> 00:09:37.240]   to doctors.
[00:09:37.240 --> 00:09:39.720]   So I think the heart of the challenge in applying machine
[00:09:39.720 --> 00:09:42.640]   learning here is that we are dealing with a lot of unknowns.
[00:09:42.640 --> 00:09:46.320]   We don't know precisely why things become poisonous.
[00:09:46.320 --> 00:09:47.520]   We know some of the reasons.
[00:09:47.520 --> 00:09:50.720]   But oftentimes, you'll get just these strange factors
[00:09:50.720 --> 00:09:51.280]   that crop up.
[00:09:51.280 --> 00:09:53.320]   We don't know if a potential medicine actually
[00:09:53.320 --> 00:09:56.880]   treats the disease in question until we try it.
[00:09:56.880 --> 00:09:57.380]   So--
[00:09:57.380 --> 00:09:57.680]   Well, I think-- wait.
[00:09:57.680 --> 00:09:59.480]   So just to slow down for a second,
[00:09:59.480 --> 00:10:00.920]   I think it's not even obvious to me
[00:10:00.920 --> 00:10:03.640]   necessarily what the machine learning
[00:10:03.640 --> 00:10:05.560]   problem is within that.
[00:10:05.560 --> 00:10:07.200]   I mean, what's the input data?
[00:10:07.200 --> 00:10:11.160]   And what are we trying to predict?
[00:10:11.160 --> 00:10:13.800]   So that's definitely another great question.
[00:10:13.800 --> 00:10:15.520]   And usually, the challenge here is
[00:10:15.520 --> 00:10:18.920]   that you start with a very narrow sliver
[00:10:18.920 --> 00:10:19.680]   of these problems.
[00:10:19.680 --> 00:10:22.360]   So there are, say, limited models for toxicity
[00:10:22.360 --> 00:10:24.880]   that, given some amount of data--
[00:10:24.880 --> 00:10:26.480]   maybe you have a database of compounds,
[00:10:26.480 --> 00:10:30.440]   and you're like, this molecule induced negative effect,
[00:10:30.440 --> 00:10:31.520]   something.
[00:10:31.520 --> 00:10:33.160]   You can train a machine learning model
[00:10:33.160 --> 00:10:35.320]   that, given the structure of a new molecule,
[00:10:35.320 --> 00:10:39.360]   will predict an output, which is the toxicity label.
[00:10:39.360 --> 00:10:42.320]   The challenge, of course, is generalization.
[00:10:42.320 --> 00:10:43.960]   You know it works on your training set.
[00:10:43.960 --> 00:10:46.880]   But if I give you a new molecule, does it actually work?
[00:10:46.880 --> 00:10:47.960]   That's often the question.
[00:10:47.960 --> 00:10:50.680]   It's just very hard to gauge that.
[00:10:50.680 --> 00:10:52.600]   And then how is it possible--
[00:10:52.600 --> 00:10:53.560]   sorry, dumb questions.
[00:10:53.560 --> 00:10:55.920]   I'll just ask the questions I honestly have.
[00:10:55.920 --> 00:11:00.640]   So how would you possibly have enough training data?
[00:11:00.640 --> 00:11:02.960]   You're not going to keep poisoning cats, right,
[00:11:02.960 --> 00:11:06.760]   to keep finding more and more poisonous molecules?
[00:11:06.760 --> 00:11:08.840]   How does that work?
[00:11:08.840 --> 00:11:10.600]   I think that's another great question.
[00:11:10.600 --> 00:11:13.200]   And the real answer is we don't have enough training data,
[00:11:13.200 --> 00:11:17.160]   which is why I think molecular machine learning is
[00:11:17.160 --> 00:11:18.320]   a bit of an art right now.
[00:11:18.320 --> 00:11:20.440]   Unlike images and speech, where there's
[00:11:20.440 --> 00:11:23.480]   this dramatically larger training sets,
[00:11:23.480 --> 00:11:26.720]   the data sets are fundamentally limited.
[00:11:26.720 --> 00:11:29.080]   There's a few approaches people take to deal with this.
[00:11:29.080 --> 00:11:32.440]   I think one common theme is, let's
[00:11:32.440 --> 00:11:34.640]   use more of the fact that we know a lot about physics
[00:11:34.640 --> 00:11:36.120]   and chemistry.
[00:11:36.120 --> 00:11:38.120]   Toxicity, I think, is a very hard problem.
[00:11:38.120 --> 00:11:39.120]   It's biology.
[00:11:39.120 --> 00:11:41.640]   It's kind of harder.
[00:11:41.640 --> 00:11:44.360]   But in many cases, you'd say that, well, OK, I
[00:11:44.360 --> 00:11:45.760]   know something about the molecule.
[00:11:45.760 --> 00:11:47.880]   I know something about its invariances.
[00:11:47.880 --> 00:11:51.800]   I can encode that into the convolutional network.
[00:11:51.800 --> 00:11:55.120]   So now you have increasingly sophisticated
[00:11:55.120 --> 00:11:57.240]   graph convolutional networks that
[00:11:57.400 --> 00:12:00.640]   encode more factors of known molecular structure.
[00:12:00.640 --> 00:12:06.080]   It's definitely not a solved field.
[00:12:06.080 --> 00:12:08.800]   I think this entire part of machine learning
[00:12:08.800 --> 00:12:11.440]   is far from what I'd call the ImageNet moment.
[00:12:11.440 --> 00:12:13.760]   So there's that point at which something just
[00:12:13.760 --> 00:12:15.840]   crosses over and breaks out.
[00:12:15.840 --> 00:12:17.480]   And I think right now it's useful,
[00:12:17.480 --> 00:12:19.840]   but it isn't that magic bullet.
[00:12:19.840 --> 00:12:21.920]   I actually really would like to go back to that,
[00:12:21.920 --> 00:12:24.600]   but I want to make sure I understand the core problem
[00:12:24.600 --> 00:12:25.080]   here.
[00:12:25.080 --> 00:12:28.880]   So it sounds like you have a molecule,
[00:12:28.880 --> 00:12:32.040]   and you want to predict some kind of property.
[00:12:32.040 --> 00:12:34.320]   I think that is definitely the most common one.
[00:12:34.320 --> 00:12:35.920]   There's a number of variants to this.
[00:12:35.920 --> 00:12:37.520]   You might have a protein, and then you
[00:12:37.520 --> 00:12:41.320]   want to find a molecule that interacts with it.
[00:12:41.320 --> 00:12:43.200]   One way you can do this is the property is,
[00:12:43.200 --> 00:12:45.240]   does it interact with the protein?
[00:12:45.240 --> 00:12:48.080]   There's also generative models where you say that, OK,
[00:12:48.080 --> 00:12:51.720]   given a database of known drugs, use some LSTM or something
[00:12:51.720 --> 00:12:55.120]   to just print out a new potential drug.
[00:12:55.120 --> 00:12:56.640]   This tends to get a little hairy.
[00:12:56.640 --> 00:13:00.240]   It's kind of hot research, but it's not safe to really use
[00:13:00.240 --> 00:13:02.800]   in production, I think.
[00:13:02.800 --> 00:13:05.720]   There's some raging academic debates about that right now.
[00:13:05.720 --> 00:13:08.640]   OK, but sorry, can I ask some more dumb questions before?
[00:13:08.640 --> 00:13:11.240]   So how do you even represent a molecule?
[00:13:11.240 --> 00:13:13.080]   Like text, it seems kind of obvious to me.
[00:13:13.080 --> 00:13:14.560]   But how do you--
[00:13:14.560 --> 00:13:16.720]   I mean, it seems like molecules are variable length,
[00:13:16.720 --> 00:13:18.320]   and they have some structure.
[00:13:18.320 --> 00:13:19.720]   Is it a graph?
[00:13:19.720 --> 00:13:21.200]   Is that--
[00:13:21.200 --> 00:13:23.440]   It's actually a great question.
[00:13:23.440 --> 00:13:27.400]   So thankfully, there's kind of the field of chemoinformatics
[00:13:27.400 --> 00:13:29.840]   where a number of years ago, they defined a language called
[00:13:29.840 --> 00:13:35.080]   SMILES, S-M-I-L-E-S. So SMILES strings are basically
[00:13:35.080 --> 00:13:38.160]   a language that allow you to write down molecules.
[00:13:38.160 --> 00:13:40.280]   It's most often used for small molecules,
[00:13:40.280 --> 00:13:44.160]   but you can write pretty big arbitrary molecules as well.
[00:13:44.160 --> 00:13:46.440]   Often, so there's a number of architectures.
[00:13:46.440 --> 00:13:48.280]   Many architectures take the SMILES
[00:13:48.280 --> 00:13:50.760]   and do convert it into a graph.
[00:13:50.760 --> 00:13:52.800]   And the idea is that the atoms in the molecule
[00:13:52.800 --> 00:13:56.080]   turn into nodes in the graph, and bonds usually
[00:13:56.080 --> 00:13:57.200]   turn to edges.
[00:13:57.200 --> 00:13:59.520]   Also, sometimes you do something like a distance cutoff,
[00:13:59.520 --> 00:14:01.520]   because there's these non-covalent interactions.
[00:14:01.520 --> 00:14:05.320]   So you might say, all atoms that are yay close to each other
[00:14:05.320 --> 00:14:07.280]   are now bonds in my--
[00:14:07.280 --> 00:14:08.600]   or have edges in my graph.
[00:14:08.600 --> 00:14:16.120]   And does that completely represent a molecule?
[00:14:16.120 --> 00:14:17.800]   So honestly, not at all.
[00:14:17.800 --> 00:14:23.040]   So the real molecules are these very complex quantum beasts
[00:14:23.040 --> 00:14:27.920]   that have orbitals and extremely complicated wave functions.
[00:14:27.920 --> 00:14:31.760]   In fact, I'd say that when you get past really teensy
[00:14:31.760 --> 00:14:34.960]   molecules like helium, or there's
[00:14:34.960 --> 00:14:37.440]   probably a few slightly more complicated ones,
[00:14:37.440 --> 00:14:39.720]   you actually don't know the quantum structure of these
[00:14:39.720 --> 00:14:42.520]   things until I think the quantum computers arrive
[00:14:42.520 --> 00:14:44.920]   and we can run these simulations.
[00:14:44.920 --> 00:14:47.080]   We actually do not really have the ability
[00:14:47.080 --> 00:14:50.160]   to grasp the quote unquote "true structure"
[00:14:50.160 --> 00:14:51.760]   of a molecule in most cases.
[00:14:51.760 --> 00:14:53.480]   So it's an approximation.
[00:14:53.480 --> 00:14:56.000]   It's mostly useful for many purposes, though.
[00:14:56.000 --> 00:14:58.880]   But yeah, molecules are more complicated
[00:14:58.880 --> 00:15:01.640]   than we understand in many cases.
[00:15:01.640 --> 00:15:05.600]   But when you talk-- so when you talk about an LSTM generating
[00:15:05.600 --> 00:15:08.920]   a molecule, it's generating--
[00:15:08.920 --> 00:15:10.840]   literally generating a string that gets
[00:15:10.840 --> 00:15:12.480]   interpreted as a molecule?
[00:15:12.480 --> 00:15:13.640]   Exactly.
[00:15:13.640 --> 00:15:16.840]   So the SMILES language I mentioned-- so precisely what
[00:15:16.840 --> 00:15:19.400]   you do is that you just treat it like a sentence generation
[00:15:19.400 --> 00:15:23.120]   task, but you're generating in the SMILES language.
[00:15:23.120 --> 00:15:24.680]   And oftentimes, the challenge there
[00:15:24.680 --> 00:15:27.120]   is that if you do this naively, you'll
[00:15:27.120 --> 00:15:29.360]   generate grammatical errors.
[00:15:29.360 --> 00:15:31.040]   So it's not an actual molecule.
[00:15:31.040 --> 00:15:32.560]   But there's been a lot of research.
[00:15:32.560 --> 00:15:35.520]   There's some groups at MIT in particular and U Toronto
[00:15:35.520 --> 00:15:38.400]   that have worked out ways to constrain the generative model
[00:15:38.400 --> 00:15:44.000]   so that it's more likely to generate real molecules.
[00:15:44.000 --> 00:15:46.840]   So I guess this sounds--
[00:15:46.840 --> 00:15:50.400]   as an ML person, this sounds incredibly appealing,
[00:15:50.400 --> 00:15:54.400]   like a well-formed, tricky ML problem that
[00:15:54.400 --> 00:15:56.640]   has the potential of saving lives.
[00:15:56.640 --> 00:16:02.560]   And I guess I wonder how much of this is real
[00:16:02.560 --> 00:16:05.560]   and how much of it is speculative?
[00:16:05.560 --> 00:16:07.520]   Can you point to an example of a drug
[00:16:07.520 --> 00:16:11.040]   that was created through this process
[00:16:11.040 --> 00:16:13.240]   or helped by this process?
[00:16:13.240 --> 00:16:15.480]   So absolutely not, unfortunately.
[00:16:15.480 --> 00:16:17.880]   So this is where it gets really fuzzy,
[00:16:17.880 --> 00:16:20.360]   because it's on average--
[00:16:20.360 --> 00:16:23.280]   I think COVID might actually speed up discovery
[00:16:23.280 --> 00:16:23.960]   in some cases.
[00:16:23.960 --> 00:16:27.040]   But most of the time, it's 15 years
[00:16:27.040 --> 00:16:29.800]   from the first discovery, starting a project,
[00:16:29.800 --> 00:16:31.760]   to the actual getting to patients.
[00:16:31.760 --> 00:16:37.000]   So there have been simpler computational techniques
[00:16:37.000 --> 00:16:38.600]   in use for decades now.
[00:16:38.600 --> 00:16:40.400]   So there is some degree of evidence
[00:16:40.400 --> 00:16:44.240]   that they help, but I don't think
[00:16:44.240 --> 00:16:45.720]   there's been a smoking gun.
[00:16:45.720 --> 00:16:48.560]   There isn't one molecule that you can really point to and say,
[00:16:48.560 --> 00:16:50.120]   that, an AI made that.
[00:16:50.120 --> 00:16:53.640]   And I think it's more like the process of using this program
[00:16:53.640 --> 00:16:56.800]   helped, in some fuzzy, hard-to-quantify fashion,
[00:16:56.800 --> 00:17:00.680]   the design of this compound.
[00:17:00.680 --> 00:17:04.400]   But it seems like the programs are kind of suggesting--
[00:17:04.400 --> 00:17:07.000]   or at least the framing that I hear from a lot of our customers
[00:17:07.000 --> 00:17:11.440]   is the programs are suggesting compounds to try,
[00:17:11.440 --> 00:17:12.840]   which makes a ton of sense, right?
[00:17:12.840 --> 00:17:14.400]   Because you have to try something.
[00:17:14.400 --> 00:17:20.200]   So I assume that people have some non-random approach
[00:17:20.200 --> 00:17:22.160]   for this.
[00:17:22.160 --> 00:17:24.200]   So is that actually better?
[00:17:24.200 --> 00:17:27.960]   I guess it seems like there must be evidence now
[00:17:27.960 --> 00:17:29.840]   if these deep learning techniques work better
[00:17:29.840 --> 00:17:33.680]   for this kind of suggestion than other techniques.
[00:17:33.680 --> 00:17:35.520]   That seems pretty quantifiable.
[00:17:35.520 --> 00:17:38.480]   Or am I missing something?
[00:17:38.480 --> 00:17:41.680]   So I think part of the challenge here
[00:17:41.680 --> 00:17:46.680]   is that it's hard-- there's many steps in the process.
[00:17:46.680 --> 00:17:48.840]   So there was a paper from Google recently
[00:17:48.840 --> 00:17:51.960]   where they showed that on one particular task,
[00:17:51.960 --> 00:17:56.640]   that when they ran the experiment,
[00:17:56.640 --> 00:18:00.720]   the assay naively, it was like a few percent hit rate.
[00:18:00.720 --> 00:18:02.840]   That is things that actually looked like they
[00:18:02.840 --> 00:18:04.480]   might work in that stage.
[00:18:04.480 --> 00:18:07.360]   And when they bootstrapped it by training and machine learning
[00:18:07.360 --> 00:18:10.160]   model, then made predictions, it was something like 30%.
[00:18:10.160 --> 00:18:12.520]   And that sounds like a giant boost,
[00:18:12.520 --> 00:18:17.240]   but I think that's like one step out of 20 in the process.
[00:18:17.240 --> 00:18:19.040]   So you take the thing that comes from that,
[00:18:19.040 --> 00:18:21.240]   then you go to the next stage where you're like, well,
[00:18:21.240 --> 00:18:23.040]   this molecule is good, but it turns out
[00:18:23.040 --> 00:18:25.680]   that it gets caught up by the liver.
[00:18:25.680 --> 00:18:28.520]   We need to change it somehow so that it avoids that.
[00:18:28.520 --> 00:18:29.960]   And right now, the best way to do
[00:18:29.960 --> 00:18:33.160]   that is still to hire a seasoned team of medicinal chemists
[00:18:33.160 --> 00:18:36.920]   who can guide you through that process.
[00:18:36.920 --> 00:18:39.040]   In the later stages, it gets particularly gnarly
[00:18:39.040 --> 00:18:40.960]   because you have very small amounts of data.
[00:18:40.960 --> 00:18:44.120]   So the Google paper, it was at an early stage
[00:18:44.120 --> 00:18:47.120]   where they could generate programmatic large data
[00:18:47.120 --> 00:18:49.640]   sets, like 50 million data points or something.
[00:18:49.640 --> 00:18:52.360]   But in the later stages, you might have like 100.
[00:18:52.360 --> 00:18:54.880]   And then all of a sudden, you're in that fuzzy no-man's world
[00:18:54.880 --> 00:18:58.680]   in which machine learning is kind of witchcraft.
[00:18:58.680 --> 00:19:01.160]   So that's, I think, part of the reason,
[00:19:01.160 --> 00:19:05.200]   because maybe you started out with something that was AI
[00:19:05.200 --> 00:19:07.880]   generated, but then 10 medicinal chemists came along,
[00:19:07.880 --> 00:19:09.320]   tweaked it here, tweaked it there.
[00:19:09.320 --> 00:19:11.240]   Then what do you have at the end?
[00:19:11.240 --> 00:19:12.720]   And honestly, we don't know.
[00:19:12.720 --> 00:19:14.640]   I think 10 years from now, maybe there
[00:19:14.640 --> 00:19:16.120]   will be a molecule we can point to.
[00:19:16.120 --> 00:19:18.280]   But for now, I think it's still fuzzy.
[00:19:18.280 --> 00:19:20.480]   It's kind of interesting that you said--
[00:19:20.480 --> 00:19:23.120]   I mean, I totally resonate with the ImageNet moment
[00:19:23.120 --> 00:19:25.880]   because I definitely remember the ImageNet moment for vision
[00:19:25.880 --> 00:19:28.800]   because I was running a company that was selling training data.
[00:19:28.800 --> 00:19:32.480]   And suddenly, everyone flipped from wanting text training
[00:19:32.480 --> 00:19:35.520]   data to images because suddenly all the image applications
[00:19:35.520 --> 00:19:36.600]   were working.
[00:19:36.600 --> 00:19:39.800]   But I guess what was kind of interesting was that I actually
[00:19:39.800 --> 00:19:44.720]   feel like the ImageNet moment came a few years after ImageNet.
[00:19:44.720 --> 00:19:47.760]   Like, not only did it have to--
[00:19:47.760 --> 00:19:50.200]   we saw vision starting to work, but it took people a while
[00:19:50.200 --> 00:19:51.000]   to realize it.
[00:19:51.000 --> 00:19:52.620]   And then companies started to staff up.
[00:19:52.620 --> 00:19:56.560]   And now I can go on Pinterest and click on stuff
[00:19:56.560 --> 00:19:57.600]   and buy them right away.
[00:19:57.600 --> 00:20:01.560]   Or I can find all my baby photos in my iPhone.
[00:20:01.560 --> 00:20:06.000]   But it seems like this one, the medical companies
[00:20:06.000 --> 00:20:09.000]   have kind of staffed up maybe before it's
[00:20:09.000 --> 00:20:10.080]   clear that it's working.
[00:20:10.080 --> 00:20:12.400]   Because it does seem like deep learning is now important
[00:20:12.400 --> 00:20:14.560]   to basically every pharma company.
[00:20:14.560 --> 00:20:16.400]   I mean, it seems like this could be set up
[00:20:16.400 --> 00:20:19.400]   for a real serious disappointment also.
[00:20:19.400 --> 00:20:25.080]   I think that's very insightful as an observation.
[00:20:25.080 --> 00:20:27.080]   And I think you're totally right.
[00:20:27.080 --> 00:20:31.400]   I think if you talk to a pharma veteran,
[00:20:31.400 --> 00:20:34.000]   there's like this old Fortune magazine from 1980
[00:20:34.000 --> 00:20:36.760]   where they had some picture of molecules on a computer.
[00:20:36.760 --> 00:20:39.280]   And they said it's going to be like medicine on the computer.
[00:20:39.280 --> 00:20:40.580]   It's going to change everything.
[00:20:40.580 --> 00:20:42.680]   And of course, nothing changed.
[00:20:42.680 --> 00:20:44.840]   And I think even for the Human Genome Project,
[00:20:44.840 --> 00:20:46.040]   there's a lot of hype.
[00:20:46.040 --> 00:20:47.960]   People thought having access to the genome
[00:20:47.960 --> 00:20:48.960]   would change everything.
[00:20:48.960 --> 00:20:51.360]   But I think the recurring theme of biology
[00:20:51.360 --> 00:20:54.320]   is that billions of years of evolution
[00:20:54.320 --> 00:20:57.120]   always have more tricks behind them.
[00:20:57.120 --> 00:20:58.880]   So I think you're right.
[00:20:58.880 --> 00:21:03.080]   I think deep learning is a useful but not magical tool
[00:21:03.080 --> 00:21:04.360]   in this space right now.
[00:21:04.360 --> 00:21:07.760]   And I think that in some cases, that disappointment
[00:21:07.760 --> 00:21:09.600]   has already hit people.
[00:21:09.600 --> 00:21:11.440]   I think in other cases, though, my hope
[00:21:11.440 --> 00:21:12.600]   is that people stick with it.
[00:21:12.600 --> 00:21:15.920]   Because I think these techniques do have a lot to offer.
[00:21:15.920 --> 00:21:19.040]   But yeah, I don't think it's going to magically cure cancer.
[00:21:19.040 --> 00:21:23.160]   I think it'll be one useful tool in the scientist's toolkit
[00:21:23.160 --> 00:21:25.040]   to discover medicine.
[00:21:25.040 --> 00:21:33.160]   But what do you think caused people to feel this optimism?
[00:21:33.160 --> 00:21:34.800]   Because machine learning techniques
[00:21:34.800 --> 00:21:35.880]   have been around for quite a long time.
[00:21:35.880 --> 00:21:38.800]   I presume people were trying these on the same data sets.
[00:21:38.800 --> 00:21:40.680]   Is there something special about deep learning
[00:21:40.680 --> 00:21:45.200]   that it sort of feels more promising in some way?
[00:21:45.200 --> 00:21:48.120]   It's a great question.
[00:21:48.120 --> 00:21:51.840]   I think we all saw this amazing wave of just deep learning
[00:21:51.840 --> 00:21:53.280]   hype.
[00:21:53.280 --> 00:21:55.360]   Because I think that ImageNet moment,
[00:21:55.360 --> 00:21:57.280]   it spread out into all these other fields.
[00:21:57.280 --> 00:22:00.880]   And I think people started hoping.
[00:22:00.880 --> 00:22:04.000]   And I think there are some genuinely new advances
[00:22:04.000 --> 00:22:07.760]   that deep learning on molecules has engendered.
[00:22:07.760 --> 00:22:10.120]   For example, I think the more predictive models,
[00:22:10.120 --> 00:22:12.000]   when you have enough data, they actually start
[00:22:12.000 --> 00:22:14.160]   working considerably better.
[00:22:14.160 --> 00:22:16.760]   This Google paper I mentioned a while back,
[00:22:16.760 --> 00:22:19.680]   it actually gets a considerable boost
[00:22:19.680 --> 00:22:21.680]   over a simpler random forest or something.
[00:22:21.680 --> 00:22:23.200]   Because it has enough data.
[00:22:23.200 --> 00:22:27.520]   The generative models, they can sometimes do clever things.
[00:22:27.520 --> 00:22:32.000]   So I think there is some substance that's not all vapor.
[00:22:32.000 --> 00:22:35.720]   But there isn't that--
[00:22:35.720 --> 00:22:37.760]   I think there is the hope that it
[00:22:37.760 --> 00:22:38.960]   might lead to a breakthrough.
[00:22:38.960 --> 00:22:41.600]   And just speaking for me personally,
[00:22:41.600 --> 00:22:43.140]   when I started working in this field,
[00:22:43.140 --> 00:22:45.480]   I didn't really understand any biology or chemistry.
[00:22:45.480 --> 00:22:48.080]   I think last ninth grade bio class
[00:22:48.080 --> 00:22:52.160]   was my last formal training in the subject.
[00:22:52.160 --> 00:22:52.960]   You and me both.
[00:22:52.960 --> 00:22:53.460]   Yeah.
[00:22:53.460 --> 00:22:56.200]   [LAUGHTER]
[00:22:56.200 --> 00:22:57.720]   Had a good ninth grade bio teacher.
[00:22:57.720 --> 00:22:59.200]   But yeah, I think when you come in,
[00:22:59.200 --> 00:23:02.000]   you're like, well, tech can solve many hard problems.
[00:23:02.000 --> 00:23:03.000]   Why can't it solve this?
[00:23:03.000 --> 00:23:03.600]   Why not?
[00:23:03.600 --> 00:23:05.680]   And I think the answer is evolution
[00:23:05.680 --> 00:23:06.960]   has had billions of years.
[00:23:06.960 --> 00:23:09.760]   And that just builds up irreducible complexities
[00:23:09.760 --> 00:23:10.280]   sometimes.
[00:23:10.280 --> 00:23:14.040]   So I think I'm still hopeful.
[00:23:14.040 --> 00:23:16.640]   I think there is real potential and value.
[00:23:16.640 --> 00:23:21.280]   But I think also, once you've spent some time in it,
[00:23:21.280 --> 00:23:23.360]   I think if you get some humility,
[00:23:23.360 --> 00:23:26.680]   the scope of the problem is much grander than you--
[00:23:26.680 --> 00:23:30.440]   at least I first realized when I was coming into the space.
[00:23:30.440 --> 00:23:32.360]   But yeah, I think it's just the hype train got
[00:23:32.360 --> 00:23:33.920]   ahead of the actual technology.
[00:23:33.920 --> 00:23:35.520]   And then it's like Gartner hype cycle.
[00:23:35.520 --> 00:23:38.080]   I think now we'll end that trough of disappointment
[00:23:38.080 --> 00:23:41.160]   and then that slope of enlightenment
[00:23:41.160 --> 00:23:42.960]   coming up a few years from now.
[00:23:42.960 --> 00:23:43.440]   Interesting.
[00:23:43.440 --> 00:23:47.120]   People seem fairly optimistic for a trough of disappointment.
[00:23:47.120 --> 00:23:50.520]   So it's an interesting perspective.
[00:23:50.520 --> 00:23:52.680]   Yeah, maybe we're still coming down.
[00:23:52.680 --> 00:23:54.960]   I hope not.
[00:23:54.960 --> 00:23:58.880]   I mean, one problem that I've always found in health
[00:23:58.880 --> 00:24:01.600]   applications is missing data.
[00:24:01.600 --> 00:24:03.880]   Are there data sets like ImageNet
[00:24:03.880 --> 00:24:06.680]   for these kinds of applications?
[00:24:06.680 --> 00:24:08.320]   So on a sensor, not really.
[00:24:08.320 --> 00:24:13.080]   So I started a project called MoleculeNet.
[00:24:13.080 --> 00:24:15.240]   I did a number of years back in grad school,
[00:24:15.240 --> 00:24:17.240]   along with one of my co-authors.
[00:24:17.240 --> 00:24:20.000]   And our intent was to gather as many data sets
[00:24:20.000 --> 00:24:23.160]   as we could to try to make something like ImageNet.
[00:24:23.160 --> 00:24:26.960]   And I think the honest answer is we helped a little bit.
[00:24:26.960 --> 00:24:29.240]   I think there was a useful collection of data
[00:24:29.240 --> 00:24:30.720]   and benchmarks we put together.
[00:24:30.720 --> 00:24:34.800]   But the challenge is that molecules are not--
[00:24:34.800 --> 00:24:39.000]   so I think in computer vision, I think object detection,
[00:24:39.000 --> 00:24:43.000]   object localization, they don't cover all vision
[00:24:43.000 --> 00:24:45.600]   because I think there's some hard frontier problems still.
[00:24:45.600 --> 00:24:48.560]   But you get a pretty big chunk of them.
[00:24:48.560 --> 00:24:50.880]   In molecules, it's more like there's
[00:24:50.880 --> 00:24:53.680]   just an entire range of things people want to do with them.
[00:24:53.680 --> 00:24:57.560]   And you have a little bit of data for each task.
[00:24:57.560 --> 00:24:59.400]   And the tasks are often not related.
[00:24:59.400 --> 00:25:01.640]   So if you take a quantum mechanical data set,
[00:25:01.640 --> 00:25:05.560]   you'll find that very different featurizations and algorithms
[00:25:05.560 --> 00:25:09.120]   actually work better than if you take a biophysical task
[00:25:09.120 --> 00:25:11.520]   or a biological task.
[00:25:11.520 --> 00:25:15.600]   So I think there is a reasonable amount of data in aggregate.
[00:25:15.600 --> 00:25:17.360]   It's for different applications.
[00:25:17.360 --> 00:25:22.160]   And you can't easily blend it into one ImageNet-style mono
[00:25:22.160 --> 00:25:23.640]   data set yet.
[00:25:23.640 --> 00:25:24.920]   Interesting.
[00:25:24.920 --> 00:25:27.120]   It kind of reminds me of natural language processing
[00:25:27.120 --> 00:25:31.400]   with all of its different applications, I guess.
[00:25:31.400 --> 00:25:35.360]   I think there is a dream that maybe we can figure out
[00:25:35.360 --> 00:25:38.360]   some type of universal pre-training that,
[00:25:38.360 --> 00:25:41.360]   akin to the GPT-2 models or the like,
[00:25:41.360 --> 00:25:45.000]   actually does get you to that universal molecular model.
[00:25:45.000 --> 00:25:47.640]   I think as of now, we haven't achieved it.
[00:25:47.640 --> 00:25:50.120]   But maybe it's not crazy to think that we can.
[00:25:50.120 --> 00:25:53.600]   We do know that Schrodinger's equation at some deep level
[00:25:53.600 --> 00:25:56.240]   is pretty close to a--
[00:25:56.240 --> 00:25:59.080]   leaving aside relativity, it's the best known model
[00:25:59.080 --> 00:26:00.200]   of these molecules we have.
[00:26:00.200 --> 00:26:03.480]   So maybe the quantum computers will eventually
[00:26:03.480 --> 00:26:04.200]   help solve this.
[00:26:04.200 --> 00:26:08.000]   But it's a ways off for now.
[00:26:08.000 --> 00:26:09.800]   Interesting.
[00:26:09.800 --> 00:26:12.040]   And the experiments presumably are kind of expensive
[00:26:12.040 --> 00:26:13.840]   to run now?
[00:26:13.840 --> 00:26:15.200]   Yeah.
[00:26:15.200 --> 00:26:20.080]   I think there's the rise of mail-order services,
[00:26:20.080 --> 00:26:22.400]   things like Unimin or Mushi, I think,
[00:26:22.400 --> 00:26:25.440]   where you can pick out a molecule out of a catalog.
[00:26:25.440 --> 00:26:27.920]   Then they'll make it for you, and they'll ship it to you.
[00:26:27.920 --> 00:26:29.720]   So it's a little easier than it used to be.
[00:26:29.720 --> 00:26:32.600]   You don't actually need to be a bench chemist.
[00:26:32.600 --> 00:26:35.880]   At the same time, you do still need to run an experiment.
[00:26:35.880 --> 00:26:38.800]   So oftentimes, people will use xenamine to buy it.
[00:26:38.800 --> 00:26:41.400]   Then they'll use a second contract research organization
[00:26:41.400 --> 00:26:43.160]   to run the experiment.
[00:26:43.160 --> 00:26:45.280]   And they'll just keep track of quality control.
[00:26:45.280 --> 00:26:49.840]   So it is possible to do it, not quite in your basement,
[00:26:49.840 --> 00:26:52.480]   I think, but maybe in a well-stocked garage
[00:26:52.480 --> 00:26:55.560]   where you can carefully coordinate many email
[00:26:55.560 --> 00:26:58.600]   threads or something like that.
[00:26:58.600 --> 00:27:00.080]   But yeah, it's expensive.
[00:27:00.080 --> 00:27:03.320]   It'll put you somewhere between a few hundred
[00:27:03.320 --> 00:27:07.400]   to a few thousand dollars per compound, depending.
[00:27:07.400 --> 00:27:10.680]   So how do these startups, because we
[00:27:10.680 --> 00:27:13.680]   have a whole bunch of customers that are startups doing
[00:27:13.680 --> 00:27:16.680]   this type of thing, how do they hope
[00:27:16.680 --> 00:27:20.000]   to compete with bigger companies when they don't
[00:27:20.000 --> 00:27:23.400]   have access to these data sets?
[00:27:23.400 --> 00:27:26.080]   That is a great question.
[00:27:26.080 --> 00:27:27.920]   In many ways, maybe I'm not the right person
[00:27:27.920 --> 00:27:30.120]   to ask because I didn't found one of these startups.
[00:27:30.120 --> 00:27:32.280]   Sure, fair.
[00:27:32.280 --> 00:27:35.280]   So I think there is some advantage to coming at it
[00:27:35.280 --> 00:27:37.600]   with some new eyes.
[00:27:37.600 --> 00:27:39.560]   I think when you're a very big company
[00:27:39.560 --> 00:27:43.080]   and you're trying to introduce just a shift in thinking,
[00:27:43.080 --> 00:27:45.560]   there's, of course, a lot of cultural inertia,
[00:27:45.560 --> 00:27:49.400]   just traditional startup versus big co dynamics.
[00:27:49.400 --> 00:27:52.200]   I think there is some potential to pick up
[00:27:52.200 --> 00:27:56.360]   kind of interesting, potentially low-hanging fruit that just
[00:27:56.360 --> 00:27:59.080]   people haven't looked at.
[00:27:59.080 --> 00:28:01.520]   I think there is also some eventually,
[00:28:01.520 --> 00:28:03.720]   I think, potential for mergers and acquisitions.
[00:28:03.720 --> 00:28:06.120]   I think building a talented machine learning team
[00:28:06.120 --> 00:28:08.880]   can be difficult. And I think if you have a company that
[00:28:08.880 --> 00:28:11.360]   has succeeded and has shown some promise,
[00:28:11.360 --> 00:28:12.960]   maybe it's a good acquisition target.
[00:28:12.960 --> 00:28:16.760]   So I think there are fruitful paths forward
[00:28:16.760 --> 00:28:18.720]   for many of these companies.
[00:28:18.720 --> 00:28:20.960]   I think some of them are actually aiming really high.
[00:28:20.960 --> 00:28:23.080]   They want to be the next Genentech.
[00:28:23.080 --> 00:28:24.440]   And I think it is possible, but I
[00:28:24.440 --> 00:28:27.120]   think that might end up coming down more to your biologists
[00:28:27.120 --> 00:28:29.480]   than it does to your machine learning people.
[00:28:29.480 --> 00:28:33.720]   And perhaps I'm a bit of a pessimist on that front.
[00:28:33.720 --> 00:28:36.520]   I think core biology, the really foundational stuff,
[00:28:36.520 --> 00:28:38.920]   is still beyond our current machine learning and AI
[00:28:38.920 --> 00:28:40.160]   techniques.
[00:28:40.160 --> 00:28:42.600]   I think it's beginning to change,
[00:28:42.600 --> 00:28:45.680]   I think, as you get more genomics data, more
[00:28:45.680 --> 00:28:48.680]   kind of biological material that you can feed into machine
[00:28:48.680 --> 00:28:49.520]   learning models.
[00:28:49.520 --> 00:28:51.440]   There's a lot of companies at that frontier.
[00:28:51.440 --> 00:28:53.320]   But for now, I think it really is
[00:28:53.320 --> 00:28:55.600]   that if you have a crack team of scientists,
[00:28:55.600 --> 00:28:59.200]   that might take you further than a crack team of machine
[00:28:59.200 --> 00:28:59.840]   learning engineers.
[00:28:59.840 --> 00:29:01.320]   Ideally, you have both, and then you
[00:29:01.320 --> 00:29:02.880]   have the best of all worlds.
[00:29:02.880 --> 00:29:05.000]   Well, it just seems like the data collection process
[00:29:05.000 --> 00:29:05.680]   is so hard.
[00:29:05.680 --> 00:29:08.040]   It seems like you might need to innovate there, too.
[00:29:08.040 --> 00:29:12.320]   I mean, I'm coming from my own background of data labeling,
[00:29:12.320 --> 00:29:15.320]   but it just seems so daunting, the idea
[00:29:15.320 --> 00:29:21.800]   that you have to order molecules somehow and run a wet lab
[00:29:21.800 --> 00:29:22.360]   or something.
[00:29:22.360 --> 00:29:26.960]   I mean, I guess, OK, I have a whole bunch
[00:29:26.960 --> 00:29:27.880]   of different questions.
[00:29:27.880 --> 00:29:29.080]   I hope you don't mind.
[00:29:29.080 --> 00:29:31.160]   One thought I have, I guess, is probably
[00:29:31.160 --> 00:29:33.480]   like the dumb things that people think of when they first
[00:29:33.480 --> 00:29:34.400]   hear about this stuff.
[00:29:34.400 --> 00:29:38.600]   But it seems like if you could model things about molecules,
[00:29:38.600 --> 00:29:39.680]   that's so powerful.
[00:29:39.680 --> 00:29:42.440]   That's like the stuff everything's made out of.
[00:29:42.440 --> 00:29:44.800]   There must be applications besides biology
[00:29:44.800 --> 00:29:46.120]   that might be simpler.
[00:29:46.120 --> 00:29:48.360]   Is that true?
[00:29:48.360 --> 00:29:49.760]   I think absolutely.
[00:29:49.760 --> 00:29:51.680]   Now, unfortunately, I think the challenging part
[00:29:51.680 --> 00:29:53.560]   is some of the most interesting applications
[00:29:53.560 --> 00:29:54.800]   are in places like batteries.
[00:29:54.800 --> 00:29:57.040]   And of course, batteries--
[00:29:57.040 --> 00:30:01.040]   so I think there are other fields.
[00:30:01.040 --> 00:30:04.640]   Like, for example, it turns out the multi-crop protection
[00:30:04.640 --> 00:30:06.760]   industry, so if you make pesticides, herbicides,
[00:30:06.760 --> 00:30:09.160]   fungicides, pretty similar techniques.
[00:30:09.160 --> 00:30:10.720]   Really?
[00:30:10.720 --> 00:30:11.920]   So it actually--
[00:30:11.920 --> 00:30:15.160]   Yeah, I guess it's just properties of molecules.
[00:30:15.160 --> 00:30:17.200]   And in fact, this is kind of coming back
[00:30:17.200 --> 00:30:21.440]   to that thin line between poison and medicine.
[00:30:21.440 --> 00:30:23.520]   If you actually take a look at some pesticides
[00:30:23.520 --> 00:30:24.880]   and you look at them, they kind of
[00:30:24.880 --> 00:30:26.320]   look like the same small molecules
[00:30:26.320 --> 00:30:28.880]   you have in medicine, which might explain a few things
[00:30:28.880 --> 00:30:32.600]   about the world, just saying there.
[00:30:32.600 --> 00:30:34.520]   I think there's also other applications.
[00:30:34.520 --> 00:30:38.040]   Like, I think in industrial applications,
[00:30:38.040 --> 00:30:43.240]   probably in petrochemicals, even I think there is a bit.
[00:30:43.240 --> 00:30:45.400]   So there's absolutely kind of other cases.
[00:30:45.400 --> 00:30:47.720]   But I think we in the software industry
[00:30:47.720 --> 00:30:50.200]   are sometimes used to working in our world of bits.
[00:30:50.200 --> 00:30:54.840]   And whereas I think when you get into these industries,
[00:30:54.840 --> 00:30:57.400]   you're like, at the end, you have to make something.
[00:30:57.400 --> 00:30:59.160]   And I think there is that slowdown.
[00:30:59.160 --> 00:31:02.360]   I think maybe batteries is actually the hardest.
[00:31:02.360 --> 00:31:04.000]   Pharma is a little behind that.
[00:31:04.000 --> 00:31:06.680]   I think some of these agricultural applications
[00:31:06.680 --> 00:31:08.280]   are a little easier to get to market,
[00:31:08.280 --> 00:31:10.720]   but still quite daunting.
[00:31:10.720 --> 00:31:12.760]   And I think in general, it just kind of comes down
[00:31:12.760 --> 00:31:15.040]   to, for a lot of these things, it's
[00:31:15.040 --> 00:31:17.080]   actually really easy to make something poisonous.
[00:31:17.080 --> 00:31:20.880]   And as governments, as industry has grown recognition
[00:31:20.880 --> 00:31:23.040]   of this fact, you just have this recurring theme
[00:31:23.040 --> 00:31:27.320]   that all of a sudden, you invent a miracle, something or other,
[00:31:27.320 --> 00:31:28.240]   of plastics.
[00:31:28.240 --> 00:31:30.200]   Like, plastics were thought to be the way
[00:31:30.200 --> 00:31:31.480]   of the future in the 1950s.
[00:31:31.480 --> 00:31:34.120]   They're also a type of just molecular product.
[00:31:34.120 --> 00:31:36.120]   And now we find they choke seagulls.
[00:31:36.120 --> 00:31:38.680]   They choke baby turtles.
[00:31:38.680 --> 00:31:40.520]   There's microplastics everywhere.
[00:31:40.520 --> 00:31:43.480]   And I think this is a type of generalized toxicity issue
[00:31:43.480 --> 00:31:47.120]   that we realize if you make large quantities
[00:31:47.120 --> 00:31:49.520]   of a new substance that the world's brought these
[00:31:49.520 --> 00:31:52.680]   and prepared to digest, what happens
[00:31:52.680 --> 00:31:55.760]   is 30 years down the line, you're like, oh, crap.
[00:31:55.760 --> 00:31:57.200]   I killed off the trout.
[00:31:57.200 --> 00:31:59.640]   I killed off the eagles.
[00:31:59.640 --> 00:32:01.360]   So it all kind of comes down to the fact
[00:32:01.360 --> 00:32:04.800]   that I think living systems are extraordinarily complicated.
[00:32:04.800 --> 00:32:09.000]   And making something that is tested and safe for a living
[00:32:09.000 --> 00:32:12.880]   thing to interact with is actually very challenging.
[00:32:12.880 --> 00:32:16.480]   What about other medical applications?
[00:32:16.480 --> 00:32:18.160]   I think you wrote a book on this, right?
[00:32:18.160 --> 00:32:20.320]   So what are the other categories of things?
[00:32:20.320 --> 00:32:24.560]   And I guess I'd be curious your take on how promising they are.
[00:32:24.560 --> 00:32:27.640]   Since it sounds like it's hard to separate the hype,
[00:32:27.640 --> 00:32:30.360]   but you probably thought deeply about this.
[00:32:30.360 --> 00:32:33.000]   I definitely think there is a whole host of really promising
[00:32:33.000 --> 00:32:33.680]   applications.
[00:32:33.680 --> 00:32:35.760]   I think to name two, I think microscopy
[00:32:35.760 --> 00:32:39.880]   is going to be completely changed by condens.
[00:32:39.880 --> 00:32:42.120]   This is one of those magical places where, hey,
[00:32:42.120 --> 00:32:44.120]   ImageNet works, as in you can actually
[00:32:44.120 --> 00:32:46.960]   take an ImageNet model and stick it on top of a microscope
[00:32:46.960 --> 00:32:50.840]   and start doing pretty sensible things pretty quickly.
[00:32:50.840 --> 00:32:52.560]   What's an example of a thing that you
[00:32:52.560 --> 00:32:54.600]   might do with microscopy?
[00:32:54.600 --> 00:32:58.480]   So it's actually-- so one of the kind of interesting things
[00:32:58.480 --> 00:33:01.640]   about this field is that you can pick up a lot more out
[00:33:01.640 --> 00:33:03.520]   of a microscope than you kind of thought.
[00:33:03.520 --> 00:33:05.400]   So there are some really interesting papers
[00:33:05.400 --> 00:33:08.560]   that show that oftentimes--
[00:33:08.560 --> 00:33:11.280]   so there's some, say, readouts of a cell
[00:33:11.280 --> 00:33:13.800]   that were traditionally you have to kind of destroy the cell,
[00:33:13.800 --> 00:33:16.120]   blow it up, in order to get at it.
[00:33:16.120 --> 00:33:19.320]   But people have started to show that you can instead
[00:33:19.320 --> 00:33:21.640]   get a data set where you take the original cell,
[00:33:21.640 --> 00:33:23.320]   then you blow it up, get the readout.
[00:33:23.320 --> 00:33:25.320]   But then you can train a machine learning model
[00:33:25.320 --> 00:33:27.720]   to start to impute that from the raw cell.
[00:33:27.720 --> 00:33:31.120]   So you can potentially get non-destructive readouts
[00:33:31.120 --> 00:33:32.840]   that enable new things.
[00:33:32.840 --> 00:33:34.640]   And this is kind of more basic science.
[00:33:34.640 --> 00:33:37.360]   Like, it's not clear what the downstream effect is.
[00:33:37.360 --> 00:33:39.440]   There's kind of a number of companies.
[00:33:39.440 --> 00:33:42.480]   I think Recursion Therapeutics is a prominent one,
[00:33:42.480 --> 00:33:44.960]   that have been using microscopy and machine
[00:33:44.960 --> 00:33:50.400]   learning broadly to do so-called phenotypic screens.
[00:33:50.400 --> 00:33:52.840]   Earlier I mentioned you often pick a protein target.
[00:33:52.840 --> 00:33:55.800]   Wait, so I need to slow down for my ninth grade biology.
[00:33:55.800 --> 00:33:57.840]   Phenotypic screen is what?
[00:33:57.840 --> 00:33:59.680]   Yes, so this is--
[00:33:59.680 --> 00:34:00.600]   my apologies.
[00:34:00.600 --> 00:34:01.520]   No, no, I just--
[00:34:01.520 --> 00:34:02.720]   I know the word phenotype.
[00:34:02.720 --> 00:34:04.840]   It's like the expression of a gene.
[00:34:04.840 --> 00:34:05.440]   Is that right?
[00:34:05.440 --> 00:34:06.200]   I should not guess.
[00:34:06.200 --> 00:34:08.040]   Yes, exactly.
[00:34:08.040 --> 00:34:10.000]   So I think it's like, one way to think about it
[00:34:10.000 --> 00:34:14.080]   is maybe bottom-up design versus top-down design.
[00:34:14.080 --> 00:34:17.760]   So kind of the targeted drug discovery is maybe bottom-up.
[00:34:17.760 --> 00:34:19.520]   You say, the human body is complicated.
[00:34:19.520 --> 00:34:20.760]   I'm going to be a reductionist.
[00:34:20.760 --> 00:34:22.520]   I think there's this one magic lever.
[00:34:22.520 --> 00:34:24.480]   And I can swip that lever on and off.
[00:34:24.480 --> 00:34:25.960]   I can really change everything.
[00:34:25.960 --> 00:34:27.880]   And that's kind of-- you come from the bottom.
[00:34:27.880 --> 00:34:29.960]   And then you hope it makes it all the way to the top.
[00:34:29.960 --> 00:34:32.380]   The other way, which is actually the more traditional way
[00:34:32.380 --> 00:34:34.000]   of finding medicine, is like, you know,
[00:34:34.000 --> 00:34:35.640]   some really smart doctor--
[00:34:35.640 --> 00:34:37.920]   like, you know, this is like the penicillin story.
[00:34:37.920 --> 00:34:39.640]   It denotes some effect.
[00:34:39.640 --> 00:34:41.800]   You have no idea what the effect is caused by.
[00:34:41.800 --> 00:34:46.160]   You don't really understand the intricate biophysics,
[00:34:46.160 --> 00:34:47.200]   the chemistry behind it.
[00:34:47.200 --> 00:34:48.000]   But you see it.
[00:34:48.000 --> 00:34:52.160]   Like, maybe there's something that just you observe.
[00:34:52.160 --> 00:34:54.400]   In, I think, this famous case of penicillin,
[00:34:54.400 --> 00:34:57.720]   in what was the mold on the bread.
[00:34:57.720 --> 00:35:00.360]   But I think for a phenotypic screen,
[00:35:00.360 --> 00:35:02.280]   like the ones recursion do, basically,
[00:35:02.280 --> 00:35:04.480]   they have these cell-based assays, where they
[00:35:04.480 --> 00:35:07.200]   grow cells in a Petri dish.
[00:35:07.200 --> 00:35:09.000]   And essentially, they test--
[00:35:09.000 --> 00:35:11.080]   you put a little bit of medicine in there.
[00:35:11.080 --> 00:35:13.760]   And then you see how the cell's state changes.
[00:35:13.760 --> 00:35:17.360]   And you use the microscope and the deep learning system
[00:35:17.360 --> 00:35:18.880]   on that to pick up those changes.
[00:35:18.880 --> 00:35:21.040]   So you can do this very rapidly.
[00:35:21.040 --> 00:35:22.880]   But like, what would be an example of change?
[00:35:22.880 --> 00:35:24.480]   Like, the cells are a different shape?
[00:35:24.480 --> 00:35:25.200]   Is that--
[00:35:25.200 --> 00:35:26.200]   Hmm.
[00:35:26.200 --> 00:35:27.440]   That's a really good question.
[00:35:27.440 --> 00:35:31.720]   I think it often depends on--
[00:35:31.720 --> 00:35:34.200]   so I think it depends on the disease in question.
[00:35:34.200 --> 00:35:37.000]   So like, a common thing, say, for like cancer
[00:35:37.000 --> 00:35:41.840]   is that-- the silly one is, can you kill the tumor cells?
[00:35:41.840 --> 00:35:45.200]   Which, the hard part there is, can you kill it
[00:35:45.200 --> 00:35:47.760]   without just finding bleach?
[00:35:47.760 --> 00:35:50.080]   So that's something that's a medicine.
[00:35:50.080 --> 00:35:52.560]   I think for other readouts, really depend on the disease.
[00:35:52.560 --> 00:35:57.040]   Like, I think the general point there is, like,
[00:35:57.040 --> 00:35:58.200]   disease is complicated.
[00:35:58.200 --> 00:36:00.520]   So there's many proxies people use.
[00:36:00.520 --> 00:36:02.600]   So kind of the hierarchy of proxies
[00:36:02.600 --> 00:36:05.760]   is, if you have a pure test tube, which is molecules,
[00:36:05.760 --> 00:36:07.040]   that's like the weakest.
[00:36:07.040 --> 00:36:09.720]   If you have cells, that's a little better.
[00:36:09.720 --> 00:36:11.760]   If you have a rat, that's a little better.
[00:36:11.760 --> 00:36:16.120]   But I think the gold standard, of course, is like the human.
[00:36:16.120 --> 00:36:17.760]   So you can think of this as like,
[00:36:17.760 --> 00:36:19.400]   it's better than the pure test tube,
[00:36:19.400 --> 00:36:22.680]   but it's absolutely not the same as like--
[00:36:22.680 --> 00:36:24.400]   but as a human.
[00:36:24.400 --> 00:36:28.040]   But it is a useful kind of proxy.
[00:36:28.040 --> 00:36:30.680]   So OK, so then what the machine learning does
[00:36:30.680 --> 00:36:34.840]   is kind of find properties based on the images
[00:36:34.840 --> 00:36:36.840]   from the microscope.
[00:36:36.840 --> 00:36:40.440]   So I think that the way I like to think about it
[00:36:40.440 --> 00:36:43.560]   is that the machine learning is kind of like making
[00:36:43.560 --> 00:36:45.640]   a better microscope.
[00:36:45.640 --> 00:36:48.280]   So in many ways, like, if you go back
[00:36:48.280 --> 00:36:49.920]   to kind of classical signal processing,
[00:36:49.920 --> 00:36:51.480]   like, we have all these, you know,
[00:36:51.480 --> 00:36:54.640]   48 transforms, you have high pass filters, low pass filters,
[00:36:54.640 --> 00:36:55.680]   what have you.
[00:36:55.680 --> 00:36:58.720]   And these traditional signal processing techniques
[00:36:58.720 --> 00:37:04.720]   made things like microscopy even feasible in the first place.
[00:37:04.720 --> 00:37:08.720]   Well, you had purely kind of optical microscopes
[00:37:08.720 --> 00:37:10.480]   back in the day.
[00:37:10.480 --> 00:37:12.360]   But in the last century, I think there's
[00:37:12.360 --> 00:37:13.880]   been a lot of signal processing attached to it.
[00:37:13.880 --> 00:37:16.000]   So I think of deep learning in these applications
[00:37:16.000 --> 00:37:20.320]   as signal processing turned up to 11.
[00:37:20.320 --> 00:37:23.680]   So you can pull things out of the image that you didn't--
[00:37:23.680 --> 00:37:27.840]   there's no obvious way to write down the function.
[00:37:27.840 --> 00:37:31.400]   So I think right now, it's more like this really fascinating
[00:37:31.400 --> 00:37:32.200]   scientific thing.
[00:37:32.200 --> 00:37:34.400]   And you know there's got to be something there.
[00:37:34.400 --> 00:37:36.120]   But I want to make sure I'm like picture--
[00:37:36.120 --> 00:37:37.760]   I want to have a mental model.
[00:37:37.760 --> 00:37:40.200]   So maybe that was evocative of like,
[00:37:40.200 --> 00:37:41.520]   did I kill the tumor cells?
[00:37:41.520 --> 00:37:44.400]   So is the point that the machine learning
[00:37:44.400 --> 00:37:47.360]   could tell me if the tumor cells were killed without me having
[00:37:47.360 --> 00:37:48.600]   to actually look at it?
[00:37:48.600 --> 00:37:50.400]   Or is it that the machine learning
[00:37:50.400 --> 00:37:53.040]   sees something deeper that I couldn't figure out
[00:37:53.040 --> 00:37:55.280]   if I looked at it?
[00:37:55.280 --> 00:37:57.680]   So I'll have to apologize upfront,
[00:37:57.680 --> 00:38:00.560]   because I'm not an expert at cellular biology.
[00:38:00.560 --> 00:38:03.440]   But I'll try to--
[00:38:03.440 --> 00:38:06.000]   So for example, I think for some--
[00:38:06.000 --> 00:38:07.080]   I might be making this up.
[00:38:07.080 --> 00:38:08.640]   So if there's real biologists that are eventually
[00:38:08.640 --> 00:38:09.960]   listening to this, please bear with my--
[00:38:09.960 --> 00:38:11.540]   No, it's a machine learning audience.
[00:38:11.540 --> 00:38:14.120]   You can pontificate.
[00:38:14.120 --> 00:38:15.280]   But I think-- OK, so I think we've
[00:38:15.280 --> 00:38:15.760]   kicked some muscle shells.
[00:38:15.760 --> 00:38:18.320]   And by the way, I think machine learning people will be really
[00:38:18.320 --> 00:38:21.120]   familiar with the idea of just looking at results
[00:38:21.120 --> 00:38:23.000]   and not worrying about the process behind it.
[00:38:23.000 --> 00:38:25.480]   So I feel like this is really appealing to our machine
[00:38:25.480 --> 00:38:27.880]   learning audience.
[00:38:27.880 --> 00:38:30.120]   I don't have to say, I still have no idea
[00:38:30.120 --> 00:38:34.800]   about what happens deep in layer 37 of my cognate.
[00:38:34.800 --> 00:38:38.080]   So I think coming back to this, imagine you have a muscle cell.
[00:38:38.080 --> 00:38:40.720]   And you can often measure some measure
[00:38:40.720 --> 00:38:45.120]   of the stretchiness of the muscle cell.
[00:38:45.120 --> 00:38:50.200]   There is often ways to guess at a proxy for healthiness.
[00:38:50.200 --> 00:38:52.760]   I think the actual thing you measure
[00:38:52.760 --> 00:38:56.280]   depends a lot on the biology of the system.
[00:38:56.280 --> 00:39:02.960]   So I think, for example, one common thing is that you might--
[00:39:02.960 --> 00:39:07.000]   there's these things called fluorescent reporters.
[00:39:07.000 --> 00:39:11.440]   And you can engineer the cell so that if you have the drug
[00:39:11.440 --> 00:39:14.100]   and it actually hits something in the cell that you know about,
[00:39:14.100 --> 00:39:15.160]   it sets off a light.
[00:39:15.160 --> 00:39:17.000]   Here, you have to know a little bit about what's
[00:39:17.000 --> 00:39:18.120]   happening inside the cell.
[00:39:18.120 --> 00:39:20.440]   You have to have a guess already.
[00:39:20.440 --> 00:39:22.640]   I think the cruder version might be like you
[00:39:22.640 --> 00:39:23.560]   have this muscle cell.
[00:39:23.560 --> 00:39:25.400]   You're looking at-- maybe there's
[00:39:25.400 --> 00:39:27.000]   some measure of how stretchy it is.
[00:39:27.000 --> 00:39:32.280]   Oftentimes, it's just kind of obvious to the eye.
[00:39:32.280 --> 00:39:34.440]   It's like that traditional--
[00:39:34.440 --> 00:39:35.840]   you know a dog when you see it.
[00:39:35.840 --> 00:39:36.800]   You see the healthy cells.
[00:39:36.800 --> 00:39:38.320]   They have some nice geometric shape.
[00:39:38.320 --> 00:39:38.880]   It looks good.
[00:39:38.880 --> 00:39:41.320]   Then you see disease, and they're all shriveled up,
[00:39:41.320 --> 00:39:42.840]   and it just looks bad.
[00:39:42.840 --> 00:39:44.760]   And you can't quite write down that function.
[00:39:44.760 --> 00:39:46.360]   You kind of know when you look at it.
[00:39:46.360 --> 00:39:47.280]   Yeah, totally.
[00:39:47.280 --> 00:39:50.640]   So it makes sense to cognates can begin to pick this up.
[00:39:50.640 --> 00:39:51.720]   Right.
[00:39:51.720 --> 00:39:54.320]   And I guess I've seen versions of cancer cells
[00:39:54.320 --> 00:39:57.080]   in kind of different levels in--
[00:39:57.080 --> 00:39:58.200]   what do they call them?
[00:39:58.200 --> 00:40:00.000]   Biopsies?
[00:40:00.000 --> 00:40:02.920]   Where you look at the cells.
[00:40:02.920 --> 00:40:04.120]   This is ninth grade biology.
[00:40:04.120 --> 00:40:05.760]   Sorry, I should probably cut this out.
[00:40:05.760 --> 00:40:07.960]   But I guess I can picture what you're saying,
[00:40:07.960 --> 00:40:11.160]   of there's healthy cells.
[00:40:11.160 --> 00:40:12.840]   But is the point then--
[00:40:12.840 --> 00:40:17.160]   I guess my question is, what is the machine learning helping
[00:40:17.160 --> 00:40:17.920]   with?
[00:40:17.920 --> 00:40:22.320]   Is it sort of reducing the cost of looking at this stuff?
[00:40:22.320 --> 00:40:25.720]   Or is it pulling out other signals that are somehow
[00:40:25.720 --> 00:40:27.040]   useful?
[00:40:27.040 --> 00:40:29.080]   I think it's a bit of both.
[00:40:29.080 --> 00:40:31.920]   So I think traditionally, the traditional labor
[00:40:31.920 --> 00:40:34.840]   was you'd have a grad student whose painful job it
[00:40:34.840 --> 00:40:37.480]   is if you're unfortunate to be stuck in this lab to look at,
[00:40:37.480 --> 00:40:40.560]   of cell one, two, three, 10,000.
[00:40:40.560 --> 00:40:43.440]   Now I think that it's often-- there are, I think,
[00:40:43.440 --> 00:40:46.040]   a number of readouts for that thing where you just look at it
[00:40:46.040 --> 00:40:47.800]   and you kind of know there's a difference.
[00:40:47.800 --> 00:40:49.400]   So I think you can train yourself
[00:40:49.400 --> 00:40:51.040]   to read these things.
[00:40:51.040 --> 00:40:52.600]   Whereas I think this is, again, I
[00:40:52.600 --> 00:40:54.880]   think a lot like the Pinterest example you brought up,
[00:40:54.880 --> 00:41:00.800]   where you're training the model to basically pick out
[00:41:00.800 --> 00:41:02.680]   something and you do it at bigger scale.
[00:41:02.680 --> 00:41:05.000]   So maybe before I could only test 10,000,
[00:41:05.000 --> 00:41:09.200]   because the grad student union would revolt at that point.
[00:41:09.200 --> 00:41:11.880]   But now maybe I can test a billion.
[00:41:11.880 --> 00:41:14.240]   Or I'm limited more by my supplies.
[00:41:14.240 --> 00:41:15.740]   I think the second question you asked
[00:41:15.740 --> 00:41:18.040]   is actually the more exciting one, as in,
[00:41:18.040 --> 00:41:20.960]   is it possible we can pick out something we didn't know?
[00:41:20.960 --> 00:41:25.040]   So I think there's glimmers that this is yes.
[00:41:25.040 --> 00:41:28.480]   I know there's a few papers that are doing things like you
[00:41:28.480 --> 00:41:30.280]   can identify where the organelles are.
[00:41:30.280 --> 00:41:33.120]   You can begin to do some more complex readout.
[00:41:33.120 --> 00:41:36.120]   But I think there is sort of almost a chicken and egg
[00:41:36.120 --> 00:41:39.960]   problem here, as in when you're discovering something,
[00:41:39.960 --> 00:41:42.160]   you kind of have to--
[00:41:42.160 --> 00:41:43.720]   it's like unsupervised learning.
[00:41:43.720 --> 00:41:45.440]   If you know the thing you're looking for,
[00:41:45.440 --> 00:41:47.600]   then you can slot it into buckets pretty easily.
[00:41:47.600 --> 00:41:50.120]   But then if it's like you want to go deeper and find
[00:41:50.120 --> 00:41:52.760]   something you don't know.
[00:41:52.760 --> 00:41:55.720]   So I think, yes, I think there are likely places
[00:41:55.720 --> 00:41:59.400]   the con nuts act as like amplified microscopes
[00:41:59.400 --> 00:42:01.680]   and pick up biology that we don't know.
[00:42:01.680 --> 00:42:05.200]   But if I knew that, I would have gone off and written a nature
[00:42:05.200 --> 00:42:06.240]   paper about it already.
[00:42:06.240 --> 00:42:07.600]   So I think that's--
[00:42:07.600 --> 00:42:09.520]   or I'm sure there's a couple that have already
[00:42:09.520 --> 00:42:12.320]   come out of this vein.
[00:42:12.320 --> 00:42:13.760]   OK, so I have to ask you.
[00:42:13.760 --> 00:42:16.800]   One of the nature papers that blew my mind,
[00:42:16.800 --> 00:42:20.720]   I think a lot of people, was the one, the dermatologist one,
[00:42:20.720 --> 00:42:26.880]   where they fine-tuned an ImageNet classifier on cancer.
[00:42:26.880 --> 00:42:31.320]   That was not under a microscope, right?
[00:42:31.320 --> 00:42:34.200]   That was just literally just photos.
[00:42:34.200 --> 00:42:36.960]   And that seemed so amazing.
[00:42:36.960 --> 00:42:40.960]   I mean, should I be as enamored with that as I felt?
[00:42:40.960 --> 00:42:44.880]   Or is there some gotchas where it's not actually--
[00:42:44.880 --> 00:42:47.560]   should we actually be using doctors for these diagnoses
[00:42:47.560 --> 00:42:48.080]   still?
[00:42:48.080 --> 00:42:50.120]   Or it sort of seemed like from the paper
[00:42:50.120 --> 00:42:55.000]   that it was more accurate than the doctors' diagnoses,
[00:42:55.000 --> 00:42:56.400]   wasn't it?
[00:42:56.400 --> 00:42:59.360]   No, I think that entire field, for sure, I think,
[00:42:59.360 --> 00:43:00.600]   is radiology.
[00:43:00.600 --> 00:43:06.080]   Or I think usually it's, yeah, pathology or dermatology.
[00:43:06.080 --> 00:43:08.640]   You look at some picture, and then you kind of diagnose it.
[00:43:08.640 --> 00:43:11.680]   I think that absolutely is a place con nuts will just
[00:43:11.680 --> 00:43:13.760]   make a big difference.
[00:43:13.760 --> 00:43:15.840]   And I do think that these models do
[00:43:15.840 --> 00:43:18.760]   kind of achieve a striking advance over what
[00:43:18.760 --> 00:43:21.160]   you could do previously.
[00:43:21.160 --> 00:43:23.560]   So my understanding, though, is that the challenge there
[00:43:23.560 --> 00:43:26.080]   is that sometimes these models pick up
[00:43:26.080 --> 00:43:28.520]   things that are kind of silly.
[00:43:28.520 --> 00:43:32.480]   Remember, there's this very excellent blog post written
[00:43:32.480 --> 00:43:35.520]   where it kind of discussed failure modes.
[00:43:35.520 --> 00:43:37.160]   Or it turned out there's some scans
[00:43:37.160 --> 00:43:38.600]   from different trauma centers.
[00:43:38.600 --> 00:43:42.760]   And the model is doing an amazing job, 99% accuracy.
[00:43:42.760 --> 00:43:44.640]   And then any time you see that 99% accuracy,
[00:43:44.640 --> 00:43:45.720]   you know something is up.
[00:43:45.720 --> 00:43:48.000]   It turned out there's some label at the bottom or something
[00:43:48.000 --> 00:43:49.400]   that printed the trauma center.
[00:43:49.400 --> 00:43:51.720]   So there's light trauma, heavy trauma.
[00:43:51.720 --> 00:43:55.240]   And guess what that model learned to do right there?
[00:43:55.240 --> 00:43:58.840]   So I think it kind of comes down to,
[00:43:58.840 --> 00:44:00.120]   what is the model learning?
[00:44:00.120 --> 00:44:01.800]   Is it a fluke?
[00:44:01.800 --> 00:44:04.560]   Is it kind of an actual thing?
[00:44:04.560 --> 00:44:07.000]   Radiologists are kind of tried and tested.
[00:44:07.000 --> 00:44:11.640]   Do you really want to fire your world-class radiologists?
[00:44:11.640 --> 00:44:14.840]   So I think there's a natural caution there, I think,
[00:44:14.840 --> 00:44:17.760]   in part because we don't really understand
[00:44:17.760 --> 00:44:23.360]   what happens deep in layer 37 of the ResNet.
[00:44:23.360 --> 00:44:26.600]   So I think the FDA and some companies are moving forward.
[00:44:26.600 --> 00:44:30.360]   I do think, potentially in places where there aren't
[00:44:30.360 --> 00:44:33.600]   enough doctors, this could be kind of a potentially
[00:44:33.600 --> 00:44:37.720]   revolutionary advance, where you could get world-class scanning
[00:44:37.720 --> 00:44:41.040]   centers available, clinics throughout the world,
[00:44:41.040 --> 00:44:44.840]   and not just places where you have excellent hospitals
[00:44:44.840 --> 00:44:46.200]   already.
[00:44:46.200 --> 00:44:50.040]   But I think it will take some time.
[00:44:50.040 --> 00:44:52.000]   I remember a number of years ago,
[00:44:52.000 --> 00:44:53.700]   I think maybe in the '80s again, there's
[00:44:53.700 --> 00:44:56.440]   a whole wave of hype around expert systems for medicine
[00:44:56.440 --> 00:44:58.400]   and how they could diagnose patients.
[00:44:58.400 --> 00:45:01.040]   And I think there's-- this might have been that same blog,
[00:45:01.040 --> 00:45:03.840]   a retrospective study that found that in many cases,
[00:45:03.840 --> 00:45:05.920]   hospitals that deployed expert systems actually
[00:45:05.920 --> 00:45:09.320]   had a fall in patient kind of well-being afterwards
[00:45:09.320 --> 00:45:12.280]   because there were these complex interactions that no one
[00:45:12.280 --> 00:45:13.840]   thought of in the first study.
[00:45:13.840 --> 00:45:15.640]   And then you find a number of years
[00:45:15.640 --> 00:45:18.280]   later that there's this unexpected side effects.
[00:45:18.280 --> 00:45:20.720]   So yeah, I think I am--
[00:45:20.720 --> 00:45:22.240]   long-winded answer there.
[00:45:22.240 --> 00:45:25.600]   I think it is something to be interested in, excited about.
[00:45:25.600 --> 00:45:28.920]   I think it will also take time to really bet and really
[00:45:28.920 --> 00:45:32.200]   kind of make sure that this is something that
[00:45:32.200 --> 00:45:34.360]   improves patient well-being.
[00:45:34.360 --> 00:45:35.840]   Although, I guess, do you know what
[00:45:35.840 --> 00:45:38.800]   happened with that melanoma model?
[00:45:38.800 --> 00:45:42.440]   Because it does seem like doctors are also not perfect.
[00:45:42.440 --> 00:45:46.600]   And I also cannot inspect my doctor's brain
[00:45:46.600 --> 00:45:49.280]   to really know their decision-making process.
[00:45:49.280 --> 00:45:54.200]   So I wonder, is it unsafe to not change?
[00:45:54.200 --> 00:45:57.840]   Or was there some real flaw or some simplification
[00:45:57.840 --> 00:46:00.680]   that wasn't obvious?
[00:46:00.680 --> 00:46:04.200]   I don't think there is a flaw in the paper.
[00:46:04.200 --> 00:46:07.880]   My guess is that--
[00:46:07.880 --> 00:46:09.880]   this isn't my field, so I'm kind of projecting
[00:46:09.880 --> 00:46:10.880]   a little bit out there.
[00:46:10.880 --> 00:46:17.760]   I know that deploying something in the clinic and the health
[00:46:17.760 --> 00:46:20.880]   care side is actually, I think, quite more complicated even
[00:46:20.880 --> 00:46:22.240]   than the new biotech side.
[00:46:22.240 --> 00:46:23.880]   I think you have to work with insurers.
[00:46:23.880 --> 00:46:25.080]   You have to work with payers.
[00:46:25.080 --> 00:46:26.920]   You have to work with hospitals and doctors.
[00:46:26.920 --> 00:46:29.440]   So I think the American health care system
[00:46:29.440 --> 00:46:32.320]   has many known challenges.
[00:46:32.320 --> 00:46:35.960]   My sense is that this has just been very hard
[00:46:35.960 --> 00:46:38.640]   to actually get out there.
[00:46:38.640 --> 00:46:41.160]   So I think in pharma and biotech,
[00:46:41.160 --> 00:46:43.720]   I think the advantage is if you get something and it works,
[00:46:43.720 --> 00:46:45.960]   there is actually a very well-known path
[00:46:45.960 --> 00:46:47.160]   to get it to people.
[00:46:47.160 --> 00:46:49.520]   I think for advances like this dermatology thing,
[00:46:49.520 --> 00:46:52.920]   there's actually a fuzzier, more ill-defined path
[00:46:52.920 --> 00:46:55.760]   to get it out there in the wild.
[00:46:55.760 --> 00:46:57.560]   So I think it doesn't have--
[00:46:57.560 --> 00:47:00.040]   I think there are some real scientific questions around,
[00:47:00.040 --> 00:47:03.240]   is this actually robust, that are still unanswered.
[00:47:03.240 --> 00:47:06.160]   But I think there's also harder business questions about,
[00:47:06.160 --> 00:47:07.040]   does this make sense?
[00:47:07.040 --> 00:47:09.360]   Is it a viable business?
[00:47:09.360 --> 00:47:11.280]   And I'm sure there's a dozen startups
[00:47:11.280 --> 00:47:12.720]   who are working on this right now.
[00:47:12.720 --> 00:47:14.840]   But I just don't know as much about it.
[00:47:14.840 --> 00:47:17.720]   It's funny, my wife runs a health care startup.
[00:47:17.720 --> 00:47:20.160]   And she tells me that it's the only industry where
[00:47:20.160 --> 00:47:23.160]   you could literally save money and save lives simultaneously
[00:47:23.160 --> 00:47:25.320]   and not have a viable business.
[00:47:25.320 --> 00:47:28.440]   I've had a few friends who left health care
[00:47:28.440 --> 00:47:31.840]   and have formed ostensibly boring, but very successful
[00:47:31.840 --> 00:47:33.880]   startups and are much happier with their lives.
[00:47:33.880 --> 00:47:36.360]   So I sympathize just a little bit.
[00:47:36.360 --> 00:47:39.560]   But you probably know way more about this than I do.
[00:47:39.560 --> 00:47:42.360]   It's a little bit outside my expertise.
[00:47:42.360 --> 00:47:44.280]   Sorry to take you outside of my expertise,
[00:47:44.280 --> 00:47:46.280]   but this is what I was hoping with the podcast.
[00:47:46.280 --> 00:47:49.600]   I could corner guys like you and ask all my dumb questions.
[00:47:49.600 --> 00:47:50.840]   I really appreciate it.
[00:47:50.840 --> 00:47:52.720]   And I think we should kind of wrap up,
[00:47:52.720 --> 00:47:54.160]   because I think this might be just getting long
[00:47:54.160 --> 00:47:54.800]   for the format.
[00:47:54.800 --> 00:47:56.920]   But we always end with two questions
[00:47:56.920 --> 00:47:58.640]   and I'm kind of curious, actually.
[00:47:58.640 --> 00:48:00.080]   I always say this, but really I'm
[00:48:00.080 --> 00:48:01.760]   curious how you're going to answer this.
[00:48:01.760 --> 00:48:06.080]   So what is one really underrated aspect of machine learning
[00:48:06.080 --> 00:48:08.960]   that you think people should pay more attention to?
[00:48:08.960 --> 00:48:10.400]   What comes to mind?
[00:48:10.400 --> 00:48:14.160]   That's a really good question.
[00:48:14.160 --> 00:48:19.240]   I think that machine learning is amplified signal processing.
[00:48:19.240 --> 00:48:23.360]   I think it's a view that is not as commonly celebrated.
[00:48:23.360 --> 00:48:26.440]   But I think there's these really exciting things going on
[00:48:26.440 --> 00:48:29.160]   where machine learning is finding its way
[00:48:29.160 --> 00:48:33.280]   into instruments, into sequencers, into microscopes.
[00:48:33.280 --> 00:48:35.640]   It's a type of internet of things, you could say,
[00:48:35.640 --> 00:48:37.760]   but not the consumer version.
[00:48:37.760 --> 00:48:42.280]   But I think traditionally, new scientific instruments always
[00:48:42.280 --> 00:48:46.880]   are the predecessor to fundamental new scientific
[00:48:46.880 --> 00:48:47.800]   discovery.
[00:48:47.800 --> 00:48:50.440]   So I think that when we find deep learning is making
[00:48:50.440 --> 00:48:52.440]   our instruments better and more capable,
[00:48:52.440 --> 00:48:54.600]   then I think that we're actually setting ourselves up
[00:48:54.600 --> 00:48:56.720]   to discover new fundamental science.
[00:48:56.720 --> 00:48:58.760]   So that's something I'm very excited by,
[00:48:58.760 --> 00:49:01.840]   but it's kind of a longer...
[00:49:01.840 --> 00:49:04.080]   We might have the instrument, we still need the Einstein
[00:49:04.080 --> 00:49:07.800]   or something to come in and work that and really get us
[00:49:07.800 --> 00:49:09.920]   that magical new understanding about the world.
[00:49:09.920 --> 00:49:11.880]   But I'm excited by that.
[00:49:11.880 --> 00:49:13.400]   That's a totally cool answer.
[00:49:13.400 --> 00:49:15.160]   But I guess they might give so many readings
[00:49:15.160 --> 00:49:16.800]   that it's hard to even interpret.
[00:49:16.800 --> 00:49:22.000]   I guess a good algorithm would give you a few high values.
[00:49:22.000 --> 00:49:23.240]   What do you call them?
[00:49:23.240 --> 00:49:25.960]   Process outputs.
[00:49:25.960 --> 00:49:28.520]   I think, yeah, it's definitely...
[00:49:28.520 --> 00:49:31.400]   I think for now, it's still going to be quite a while
[00:49:31.400 --> 00:49:33.800]   before I think we see...
[00:49:33.800 --> 00:49:37.640]   I think we talk a lot about AGI, and I know there's many ways
[00:49:37.640 --> 00:49:39.200]   in which you could get a general intelligence,
[00:49:39.200 --> 00:49:41.920]   but I think the process of induction,
[00:49:41.920 --> 00:49:47.800]   of interpolating things about reality from very few hunches.
[00:49:47.800 --> 00:49:51.760]   This is probably made up by the Newton and the Apple tree.
[00:49:51.760 --> 00:49:54.160]   Yeah, it probably didn't happen that way.
[00:49:54.160 --> 00:49:57.640]   We know it's a just so story, but you could imagine
[00:49:57.640 --> 00:49:59.840]   some machine learning model like seeing that,
[00:49:59.840 --> 00:50:01.720]   and can you somehow interpolate from that out
[00:50:01.720 --> 00:50:04.200]   to a universal law of gravitation?
[00:50:04.200 --> 00:50:06.280]   But yeah, that I think would be amazing.
[00:50:06.280 --> 00:50:10.200]   Just seems far beyond our current science.
[00:50:10.200 --> 00:50:11.040]   It's funny though, I feel like
[00:50:11.040 --> 00:50:12.360]   with all these medical applications,
[00:50:12.360 --> 00:50:16.960]   I guess the reason I naively find them exciting
[00:50:16.960 --> 00:50:20.960]   is that if you're trying to compete with a human
[00:50:20.960 --> 00:50:23.000]   for navigation and driving, it's like,
[00:50:23.000 --> 00:50:25.400]   boy, our brains are designed for that.
[00:50:25.400 --> 00:50:28.800]   Clearly, a huge part of our brain is just to navigate
[00:50:28.800 --> 00:50:30.040]   the world and not crash into stuff.
[00:50:30.040 --> 00:50:32.600]   But it doesn't seem like our brains are designed
[00:50:32.600 --> 00:50:35.800]   for interpreting molecules that we can't see
[00:50:35.800 --> 00:50:37.600]   and what effects they might have.
[00:50:37.600 --> 00:50:39.720]   I mean, I'm still trying to visualize it in my head
[00:50:39.720 --> 00:50:41.000]   and I can't even do it.
[00:50:41.000 --> 00:50:46.000]   So it sort of seems like maybe the bar is lower
[00:50:46.320 --> 00:50:51.320]   for a useful algorithm.
[00:50:51.320 --> 00:50:57.080]   - I think that's a really interesting point there.
[00:50:57.080 --> 00:51:01.440]   I do think understanding quantum mechanics
[00:51:01.440 --> 00:51:05.640]   at least doesn't fit in my head.
[00:51:05.640 --> 00:51:07.680]   There's lots of complicated things going on
[00:51:07.680 --> 00:51:09.840]   in that hidden world there.
[00:51:09.840 --> 00:51:12.800]   Maybe part of the challenge is that it's hard
[00:51:12.800 --> 00:51:15.760]   to validate a discovery.
[00:51:15.760 --> 00:51:17.400]   Many times a model says something,
[00:51:17.400 --> 00:51:20.720]   but after you spend a while, nine times out of 10,
[00:51:20.720 --> 00:51:21.880]   you're like, all right, what bullshit
[00:51:21.880 --> 00:51:23.560]   did the system pick up this time?
[00:51:23.560 --> 00:51:28.640]   I think the challenge there is maybe we have to make
[00:51:28.640 --> 00:51:30.160]   the model, like you said, we have to make the models
[00:51:30.160 --> 00:51:33.240]   robust enough that there's actually high quality signals
[00:51:33.240 --> 00:51:34.080]   coming out.
[00:51:34.080 --> 00:51:35.920]   So we're like, oh, that's a clue and not,
[00:51:35.920 --> 00:51:40.320]   oh, I don't know what pickup happened in step 2000
[00:51:40.320 --> 00:51:41.560]   of gradient descent.
[00:51:41.560 --> 00:51:45.240]   So I think that's maybe the challenge
[00:51:45.240 --> 00:51:48.880]   where we just haven't, I think this was beginning
[00:51:48.880 --> 00:51:52.200]   to change, but it feels like still discovery,
[00:51:52.200 --> 00:51:54.320]   like invention is the province of the human
[00:51:54.320 --> 00:51:56.000]   and not the machine.
[00:51:56.000 --> 00:51:58.160]   But maybe that's like the antiquated line
[00:51:58.160 --> 00:52:00.720]   and 10 years from now, you'll have AI discovered everything
[00:52:00.720 --> 00:52:03.720]   and I'll be like, well, that aged poorly there.
[00:52:03.720 --> 00:52:08.920]   - We'd be an interesting world if that comes to pass.
[00:52:08.920 --> 00:52:13.160]   Sorry, so final question is, so right now in 2020,
[00:52:13.160 --> 00:52:17.600]   and I guess it's already June, what do you think
[00:52:17.600 --> 00:52:20.840]   is currently the biggest challenge of making
[00:52:20.840 --> 00:52:23.160]   machine learning models work in the real world?
[00:52:23.160 --> 00:52:25.640]   Like in your experience, what are the challenges
[00:52:25.640 --> 00:52:26.520]   that you've run into?
[00:52:26.520 --> 00:52:29.440]   Like what have been the surprising hurdles?
[00:52:29.440 --> 00:52:33.520]   - I think things more specific to me are often small data.
[00:52:33.520 --> 00:52:39.560]   Like again, you have 30 data points and oftentimes
[00:52:39.560 --> 00:52:41.480]   it's a very well-meaning scientist who kind of comes
[00:52:41.480 --> 00:52:43.800]   and says, what can you do for us with 30 data points?
[00:52:43.800 --> 00:52:48.800]   Oftentimes I'm like, I wish I had a better answer.
[00:52:48.800 --> 00:52:53.280]   Sometimes you just try seven things,
[00:52:53.280 --> 00:52:55.280]   like you're trying the transfer learning,
[00:52:55.280 --> 00:52:57.880]   you try like the multitask learning, the meta learning
[00:52:57.880 --> 00:52:59.120]   and all the learnings fail.
[00:52:59.120 --> 00:53:01.960]   And then at the end, the random forces like,
[00:53:01.960 --> 00:53:04.160]   yeah, it's all great, but it does something.
[00:53:04.160 --> 00:53:06.560]   So I think for things I'm excited by,
[00:53:06.560 --> 00:53:10.800]   I think like robust transfer learning that actually works
[00:53:10.800 --> 00:53:14.920]   on small data, which I think has occurred in NLP,
[00:53:14.920 --> 00:53:16.800]   but I think has not occurred in molecules.
[00:53:16.800 --> 00:53:18.760]   I think that would be an amazing advance for this field.
[00:53:18.760 --> 00:53:20.000]   - It's so interesting it hasn't occurred,
[00:53:20.000 --> 00:53:22.560]   'cause I feel like it's also totally happened in vision.
[00:53:22.560 --> 00:53:23.560]   Like for sure, right?
[00:53:23.560 --> 00:53:26.400]   And NLP now definitely it's a fact.
[00:53:26.400 --> 00:53:29.120]   So interesting, it doesn't work for molecules.
[00:53:29.120 --> 00:53:30.320]   - It might just be data.
[00:53:30.320 --> 00:53:33.800]   Like I think if someone just found a gigantic trove
[00:53:33.800 --> 00:53:35.600]   of molecular measurements that was high quality
[00:53:35.600 --> 00:53:36.440]   and you had to build it.
[00:53:36.440 --> 00:53:37.280]   (laughs)
[00:53:37.280 --> 00:53:39.960]   - Or collected it, nobody's gonna find that, right?
[00:53:39.960 --> 00:53:41.800]   - Yeah, I think this is the sort of thing
[00:53:41.800 --> 00:53:43.280]   that I think a governmental effort
[00:53:43.280 --> 00:53:45.000]   could do like amazing work at.
[00:53:45.000 --> 00:53:49.280]   And to be fair, I think like governmental agencies
[00:53:49.280 --> 00:53:52.040]   have actually put out most of the open source data out there
[00:53:52.040 --> 00:53:53.720]   so they are actually working hard at this.
[00:53:53.720 --> 00:53:56.920]   But yeah, there's maybe the sort of thing that like,
[00:53:56.920 --> 00:53:58.920]   if you get a $10 million grant or something,
[00:53:58.920 --> 00:54:01.240]   I think you could make a serious dent
[00:54:01.240 --> 00:54:04.600]   at putting together a high quality open data set for this.
[00:54:04.600 --> 00:54:06.720]   But it is just more expensive than ImageNet
[00:54:06.720 --> 00:54:09.600]   and it will take more resources.
[00:54:09.600 --> 00:54:11.960]   This means you need to do the actual experiments.
[00:54:11.960 --> 00:54:14.560]   - Great answer, I love it.
[00:54:14.560 --> 00:54:16.560]   Well, thank you so much.
[00:54:16.560 --> 00:54:18.080]   Is there like some place we should tell people
[00:54:18.080 --> 00:54:22.120]   to contact you or is there anything you wanna promote?
[00:54:22.120 --> 00:54:24.120]   Deep chem, everyone should try it.
[00:54:24.120 --> 00:54:27.240]   - I think, no, absolutely.
[00:54:27.240 --> 00:54:30.320]   I think part of like the goal behind deep chem really
[00:54:30.320 --> 00:54:34.480]   is to make open source more feasible for drug discovery.
[00:54:34.480 --> 00:54:36.800]   So I think we could definitely use more users.
[00:54:36.800 --> 00:54:38.360]   In particular, if you're an engineer
[00:54:38.360 --> 00:54:40.400]   that knows how to handle build processes,
[00:54:40.400 --> 00:54:42.040]   well, please get in touch.
[00:54:42.040 --> 00:54:43.960]   I'm trying to figure out the windows and et cetera builds
[00:54:43.960 --> 00:54:45.080]   and it's just such a pain.
[00:54:45.080 --> 00:54:47.080]   - Oh man, besides me. - Too much of a scientist.
[00:54:47.080 --> 00:54:51.040]   But yeah, we could absolutely use more help.
[00:54:51.040 --> 00:54:53.040]   So if you're interested in open science,
[00:54:53.040 --> 00:54:55.440]   yeah, please do get involved.
[00:54:55.440 --> 00:54:57.360]   - I love it, thanks for asking.
[00:54:57.360 --> 00:54:59.240]   - My pleasure, thank you for inviting me.
[00:54:59.640 --> 00:55:02.240]   (gentle music)
[00:55:02.240 --> 00:55:04.840]   (gentle music)
[00:55:04.840 --> 00:55:07.440]   (gentle music)
[00:55:07.440 --> 00:55:12.440]   [Music]

