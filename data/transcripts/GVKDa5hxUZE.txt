
[00:00:00.000 --> 00:00:03.880]   So in the last video, we built a classifier that could predict whether or not a handwritten
[00:00:03.880 --> 00:00:06.080]   digit was a 5 or not.
[00:00:06.080 --> 00:00:10.160]   And in this video, we're going to take those techniques and generalize them so that we
[00:00:10.160 --> 00:00:14.160]   have a neural network that can predict exactly what digit we're dealing with, not just a
[00:00:14.160 --> 00:00:16.700]   5, but a 4, a 3, and so on.
[00:00:16.700 --> 00:00:20.340]   And along the way, we're going to learn a bunch of new techniques that are generalizable
[00:00:20.340 --> 00:00:22.720]   to making all neural networks work better.
[00:00:22.720 --> 00:00:26.420]   The perception algorithm, if you think about it, it's very, very constrained.
[00:00:26.420 --> 00:00:28.900]   It can only output a single number.
[00:00:28.900 --> 00:00:34.080]   So how do we take the output of a single number and turn that into a prediction for each possible
[00:00:34.080 --> 00:00:35.140]   digit?
[00:00:35.140 --> 00:00:39.960]   You might, if you feel up for it, stop the video now and think about it.
[00:00:39.960 --> 00:00:40.960]   Welcome back.
[00:00:40.960 --> 00:00:44.040]   I'm going to tell you how to do it.
[00:00:44.040 --> 00:00:47.480]   One way you might have thought of, which is not how we're going to do it, is just have
[00:00:47.480 --> 00:00:52.600]   the number be between 0 and 10 instead of 0 and 1, and basically round it down and say,
[00:00:52.600 --> 00:00:55.140]   OK, if it output 8.5, then that's an 8.
[00:00:55.140 --> 00:00:59.200]   If it output 7.2, then we predict the digits to 7.
[00:00:59.200 --> 00:01:02.080]   And for a number of reasons, that's not the normal way to do it.
[00:01:02.080 --> 00:01:06.040]   So we're going to actually do it a different way, which is we're going to have 10 perceptrons
[00:01:06.040 --> 00:01:07.600]   that run in parallel.
[00:01:07.600 --> 00:01:11.720]   And we're basically going to say, whichever perceptron outputs the highest number, that's
[00:01:11.720 --> 00:01:13.360]   going to be the digit that we predict.
[00:01:13.360 --> 00:01:17.320]   So we basically have a perceptron responsible for classifying 0s, and another one responsible
[00:01:17.320 --> 00:01:19.600]   for classifying 1s, and so on.
[00:01:19.600 --> 00:01:23.880]   And whichever one has the highest output, that is the prediction that our whole classifier
[00:01:23.880 --> 00:01:25.280]   is making.
[00:01:25.280 --> 00:01:29.240]   Remember Rosenblatt, the perceptron inventor from the last video?
[00:01:29.240 --> 00:01:30.800]   This is actually the same as what he did.
[00:01:30.800 --> 00:01:35.380]   So he would set up multiple light bulbs corresponding to each class, and whichever light bulb lit
[00:01:35.380 --> 00:01:38.520]   up the brightest was considered to be the class of the image.
[00:01:38.520 --> 00:01:41.820]   In our case, we're going to have 10 perceptrons corresponding to each digit.
[00:01:41.820 --> 00:01:45.960]   And like I said, what we'd like our classifier to do is output a 1, or a number really close
[00:01:45.960 --> 00:01:52.240]   to 1, for the digit that was actually written in the image, and a 0 for all the other digits.
[00:01:52.240 --> 00:01:57.360]   And what we're specifically going to optimize for is our output being close to the target.
[00:01:57.360 --> 00:02:01.560]   In other words, our loss function will be the difference between each of these digits
[00:02:01.560 --> 00:02:04.240]   and each of these outputs that we want.
[00:02:04.240 --> 00:02:08.720]   So here's a quick question to kind of test your understanding.
[00:02:08.720 --> 00:02:12.440]   How many weights does our algorithm have now?
[00:02:12.440 --> 00:02:15.440]   You can stop and think about this one too, and I'll get back to it in a minute.
[00:02:15.440 --> 00:02:16.900]   OK, let's go to the code.
[00:02:16.900 --> 00:02:21.760]   So you should be back in the video intro directory, and you should have checked out my code.
[00:02:21.760 --> 00:02:23.960]   Go back to the previous video if you haven't.
[00:02:23.960 --> 00:02:28.800]   Open up perceptron-linear.py.
[00:02:28.800 --> 00:02:33.040]   Now up to line 20, everything is exactly the same as in our last tutorial.
[00:02:33.040 --> 00:02:38.400]   In fact, it's a little simpler, because we can just use y_test as our output instead
[00:02:38.400 --> 00:02:43.040]   of trying to make a new variable that says if it's a 5 or not.
[00:02:43.040 --> 00:02:49.440]   Now in lines 20 and 21, we need to do something called one-hot encoding our output.
[00:02:49.440 --> 00:02:53.440]   One-hot encoding is super common in machine learning, and especially in neural networks.
[00:02:53.440 --> 00:02:58.180]   You basically transform your data, which is a number, into a list of ones and zeros.
[00:02:58.180 --> 00:03:01.660]   We take the list on the left, and we turn it into the table on the right.
[00:03:01.660 --> 00:03:08.400]   So a 3 becomes 0001000000.
[00:03:08.400 --> 00:03:10.240]   Now why do we do it?
[00:03:10.240 --> 00:03:14.200]   It's because we need to be explicit about what we want our network to output.
[00:03:14.200 --> 00:03:19.120]   Our network is going to output a list of 10 numbers, one from each perceptron.
[00:03:19.120 --> 00:03:28.360]   And if our label is a 3, what we really want it to do is output not a 3, but 0001000000.
[00:03:28.360 --> 00:03:36.400]   So lines 20 and 21 change y_train and y_test into one-hot encoded variables.
[00:03:36.400 --> 00:03:39.820]   What is the dimension of y_train before this transformation?
[00:03:39.820 --> 00:03:42.720]   It's a list of 60,000 numbers.
[00:03:42.720 --> 00:03:46.200]   What is the dimension of y_train after this transformation?
[00:03:46.200 --> 00:03:53.560]   It's a matrix with 60,000 rows and 10 columns, where every entry is either a 1 or a 0.
[00:03:53.560 --> 00:03:56.880]   Stop the video and rewind if you need to think about this.
[00:03:56.880 --> 00:04:00.200]   Or you can go into the code and print out the variables and look at them.
[00:04:00.200 --> 00:04:01.840]   But we'll go on.
[00:04:01.840 --> 00:04:05.960]   We set num_classes from the shape of y_train.
[00:04:05.960 --> 00:04:11.760]   In this case, num_classes is 10, corresponding to 10 digits that we want to predict.
[00:04:11.760 --> 00:04:16.880]   Now our model looks really similar to the model we saw in the last video.
[00:04:16.880 --> 00:04:19.040]   Sequential is the same.
[00:04:19.040 --> 00:04:20.920]   Flatten is the same.
[00:04:20.920 --> 00:04:26.160]   Dense now takes an input num_classes instead of 1.
[00:04:26.160 --> 00:04:31.080]   This means that we're training 10 perceptrons, and our output will be a list of size 10 instead
[00:04:31.080 --> 00:04:33.360]   of 1, like it was before.
[00:04:33.360 --> 00:04:38.600]   Everything else is the same for now, and we can run our model.
[00:04:38.600 --> 00:04:44.160]   And your model may run slower or faster depending on the speed of your computer.
[00:04:44.160 --> 00:04:46.880]   My model gets around 16% accuracy on this run.
[00:04:46.880 --> 00:04:51.480]   And note that this is not deterministic, and neural networks in general are not deterministic,
[00:04:51.480 --> 00:04:53.640]   so you might get a somewhat different number.
[00:04:53.640 --> 00:04:58.760]   It's always really important when doing any kind of machine learning to think about baselines.
[00:04:58.760 --> 00:05:02.480]   What accuracy would random guessing get on this data set?
[00:05:02.480 --> 00:05:03.680]   Maybe stop and think about it.
[00:05:03.680 --> 00:05:06.200]   OK, I'll tell you.
[00:05:06.200 --> 00:05:10.000]   There's 10 digits in our data, and our data is actually evenly distributed.
[00:05:10.000 --> 00:05:15.120]   So random guessing-- say we get 0 every time or just random numbers-- we'd get around 10%
[00:05:15.120 --> 00:05:16.120]   accuracy.
[00:05:16.120 --> 00:05:19.560]   So we can pat ourselves on the back and say that our model is doing better than random,
[00:05:19.560 --> 00:05:25.360]   which is actually better than our model did in the first episode of this series.
[00:05:25.360 --> 00:05:30.920]   So we're off to a good start, but there are a ton of ways to improve this model.
[00:05:30.920 --> 00:05:35.120]   The first thing we should probably do is fix the activation function.
[00:05:35.120 --> 00:05:37.480]   And that's actually something we did in the last video.
[00:05:37.480 --> 00:05:42.560]   So remember that our target values are 0s and 1s, but our weighted sum can be anything.
[00:05:42.560 --> 00:05:46.960]   A sigmoid or a softmax activation function will take the weighted sum and squeeze its
[00:05:46.960 --> 00:05:49.480]   output to a number between 0 and 1.
[00:05:49.480 --> 00:05:53.700]   Rosenblatt, in fact, used the same activation function with perceptrons, and it's a little
[00:05:53.700 --> 00:05:55.520]   unusual that we left it out.
[00:05:55.520 --> 00:06:00.080]   It helped us a lot in the last video, so let's add it back by setting activation equals softmax
[00:06:00.080 --> 00:06:03.560]   to the line where we make the perceptrons.
[00:06:03.560 --> 00:06:07.280]   You basically always want to use a softmax activation function on the last layer of your
[00:06:07.280 --> 00:06:09.640]   network if you're doing classification.
[00:06:09.640 --> 00:06:13.080]   And the simple reason to do this is that you want to constrain your output to be between
[00:06:13.080 --> 00:06:19.040]   0 and 1.
[00:06:19.040 --> 00:06:20.720]   And you can see this works significantly better.
[00:06:20.720 --> 00:06:25.280]   Right off the bat, I'm getting around 37%, 38% accuracy.
[00:06:25.280 --> 00:06:28.480]   This is where we stopped last time, but what else can we do?
[00:06:28.480 --> 00:06:30.320]   Let's take another look at that loss function.
[00:06:30.320 --> 00:06:35.560]   So this gets a little mathy, but loss functions really matter.
[00:06:35.560 --> 00:06:38.320]   In this diagram, I have two linear regressions.
[00:06:38.320 --> 00:06:41.200]   We're used to linear regression optimizing squared error, but actually you can optimize
[00:06:41.200 --> 00:06:42.520]   lots of things.
[00:06:42.520 --> 00:06:47.280]   Here, one graph optimizes the absolute distance between the line and the points, and the other
[00:06:47.280 --> 00:06:49.480]   optimizes the square of the distance.
[00:06:49.480 --> 00:06:50.480]   Can you guess which is which?
[00:06:50.480 --> 00:06:54.160]   I actually do this in the prequel video, but take some time to think about it if you haven't
[00:06:54.160 --> 00:06:55.920]   seen this before.
[00:06:55.920 --> 00:07:00.000]   This surprises lots of students, but the left is actually optimizing squared error, and
[00:07:00.000 --> 00:07:03.040]   the right is optimizing absolute error.
[00:07:03.040 --> 00:07:07.560]   Squared error means that the distance to a point is squared, so further points actually
[00:07:07.560 --> 00:07:12.040]   have a lot more pull on the line than points closer to the line.
[00:07:12.040 --> 00:07:17.440]   Outliers have a big effect on squared error and much less of an effect on absolute error.
[00:07:17.440 --> 00:07:20.280]   Loss functions matter a lot with neural nets also.
[00:07:20.280 --> 00:07:24.420]   We've been using mean squared error, but when you're working with probabilities, categorical
[00:07:24.420 --> 00:07:27.600]   cross entropy is really the thing you want to use.
[00:07:27.600 --> 00:07:32.200]   Now this is a part you can skip ahead of or just let your eyes glaze over, and the takeaway
[00:07:32.200 --> 00:07:35.960]   I'll tell you is that you should always use categorical cross entropy if you're categorizing
[00:07:35.960 --> 00:07:36.960]   things.
[00:07:36.960 --> 00:07:40.880]   But many students always want to know why is it, and I'll try to explain it without
[00:07:40.880 --> 00:07:43.520]   getting into too much math.
[00:07:43.520 --> 00:07:48.280]   Categorical cross entropy loss makes neural networks output their true probability of
[00:07:48.280 --> 00:07:49.440]   an outcome.
[00:07:49.440 --> 00:07:54.200]   So if the answer you're looking for is one, categorical cross entropy will give you an
[00:07:54.200 --> 00:07:58.720]   infinite loss you predict zero and diminishing returns the closer you predict to one.
[00:07:58.720 --> 00:08:02.480]   Remember that with a loss function, lower is better and higher is worse.
[00:08:02.480 --> 00:08:06.260]   I'll tell you, I took a class at Stanford a long time ago where every question was a
[00:08:06.260 --> 00:08:10.580]   multiple choice question and we had to put our probability belief of each answer, not
[00:08:10.580 --> 00:08:13.040]   just the answer that we thought was most likely.
[00:08:13.040 --> 00:08:16.100]   And we were actually scored with categorical cross entropy.
[00:08:16.100 --> 00:08:19.400]   If we put that we thought the probability of an answer was zero and it turned out to
[00:08:19.400 --> 00:08:23.960]   be the right answer, we would get negative infinity points and fail the entire class.
[00:08:23.960 --> 00:08:29.160]   So we would never put a probability of zero unless you're 100% sure it wasn't the answer.
[00:08:29.160 --> 00:08:33.200]   If the teacher had given us just as many points as the probability score we put on the right
[00:08:33.200 --> 00:08:38.120]   answer, we wouldn't have been properly incentivized and we wouldn't have put any probability on
[00:08:38.120 --> 00:08:40.880]   the low probability answers.
[00:08:40.880 --> 00:08:42.520]   You probably need to stop and think about that.
[00:08:42.520 --> 00:08:44.640]   I know I would need to stop and think about that.
[00:08:44.640 --> 00:08:48.560]   And if it really didn't make any sense or if you want to think about more later, it's
[00:08:48.560 --> 00:08:49.560]   totally fine.
[00:08:49.560 --> 00:08:53.480]   The main point again is that you should always use categorical cross entropy as your loss
[00:08:53.480 --> 00:08:55.800]   function when doing classification.
[00:08:55.800 --> 00:08:56.800]   Let's try it.
[00:08:56.800 --> 00:09:04.120]   So where it used to say MSE or mean squared error, we change it to categorical underscore
[00:09:04.120 --> 00:09:13.680]   cross entropy.
[00:09:13.680 --> 00:09:17.120]   And as you can see, this improves things quite a bit.
[00:09:17.120 --> 00:09:23.360]   And another benefit is that we can trust the output of these perceptrons to be guess probabilities
[00:09:23.360 --> 00:09:24.720]   of the different digits.
[00:09:24.720 --> 00:09:27.480]   Okay, but we're not done yet.
[00:09:27.480 --> 00:09:32.960]   There's another thing that tutorials never cover that's super important to getting neural
[00:09:32.960 --> 00:09:36.800]   networks to work well, which is normalizing your data.
[00:09:36.800 --> 00:09:40.680]   So neural networks are not what's known as scale invariant.
[00:09:40.680 --> 00:09:45.000]   If you take your data and you divide it all by two or multiply it by 10, some algorithms
[00:09:45.000 --> 00:09:46.500]   won't change.
[00:09:46.500 --> 00:09:51.320]   Decision trees and random forests, they don't care about the actual magnitude of your data.
[00:09:51.320 --> 00:09:53.880]   But neural nets actually really do.
[00:09:53.880 --> 00:09:57.600]   And when you're working with Keras, actually most of the well-known neural network libraries
[00:09:57.600 --> 00:10:03.200]   like TensorFlow or PyTorch, you'll get much better results if you normalize your data
[00:10:03.200 --> 00:10:06.400]   to be between zero and one or negative one and one.
[00:10:06.400 --> 00:10:10.120]   And actually, in our case, our data is between zero and 255.
[00:10:10.120 --> 00:10:15.480]   So it's way outside of the range that our network is expecting to have the data in.
[00:10:15.480 --> 00:10:19.240]   Right now, all of our data is between zero and 255, and we want it to be between zero
[00:10:19.240 --> 00:10:20.580]   and one.
[00:10:20.580 --> 00:10:28.520]   So let's try just adding x_train divide equals 255, which should divide every value in x_train
[00:10:28.520 --> 00:10:30.280]   by 255.
[00:10:30.280 --> 00:10:32.580]   We also do the same thing for x_test.
[00:10:32.580 --> 00:10:37.240]   We save our file and we run it.
[00:10:37.240 --> 00:10:38.240]   What happens?
[00:10:38.240 --> 00:10:41.340]   We get a really scary looking error message.
[00:10:41.340 --> 00:10:46.560]   It turns out that we need to cast our x_train and x_test to floats.
[00:10:46.560 --> 00:10:50.300]   Right now, they're set to integers.
[00:10:50.300 --> 00:10:59.360]   We can add x_train equals x_train.as type float.
[00:10:59.360 --> 00:11:15.800]   If you're having trouble following along, the complete working code is in perceptron-normalize.py.
[00:11:15.800 --> 00:11:19.560]   And it turns out that this little change helps a ton.
[00:11:19.560 --> 00:11:22.760]   Normalizing data is a messy detail of neural networks that people just don't talk about
[00:11:22.760 --> 00:11:23.760]   enough.
[00:11:23.760 --> 00:11:27.000]   Okay, guys, we've made a ton of progress here.
[00:11:27.000 --> 00:11:30.520]   You'll never make this much progress this fast ever in your career, trust me.
[00:11:30.520 --> 00:11:36.860]   We've gone from 15% accuracy to 90% accuracy in 10 minutes.
[00:11:36.860 --> 00:11:38.460]   So it's worth recapping what we did.
[00:11:38.460 --> 00:11:42.920]   So one is we set our activation function to be softmax, which is a good idea.
[00:11:42.920 --> 00:11:48.500]   We set our loss function to categorical cross-entropy, which is a good idea if you're doing categorization.
[00:11:48.500 --> 00:11:52.040]   And we normalized our data, which is almost always a good idea.
[00:11:52.040 --> 00:11:53.980]   But there's still actually a lot more to go.
[00:11:53.980 --> 00:11:59.360]   Right now, we only have a single layer of perceptrons, which is really far from deep
[00:11:59.360 --> 00:12:01.040]   learning.
[00:12:01.040 --> 00:12:07.240]   Before we go on, how many weights does our model have?
[00:12:07.240 --> 00:12:12.680]   Each label or output is connected to each pixel or input.
[00:12:12.680 --> 00:12:16.500]   And there's 784 inputs, and there's 10 outputs.
[00:12:16.500 --> 00:12:22.200]   So there's going to be a total of 7,840 weights in our model.
[00:12:22.200 --> 00:12:25.200]   And actually, I lied to you, there's a few more.
[00:12:25.200 --> 00:12:29.400]   Each perceptron has what's called a bias term, which is an input always set to one.
[00:12:29.400 --> 00:12:30.720]   You don't really have to worry about that.
[00:12:30.720 --> 00:12:36.960]   But to get the exact answer, there's really 7,850 parameters for this model to learn.
[00:12:36.960 --> 00:12:39.520]   You can always check that by calling model.summary.
[00:12:39.520 --> 00:12:45.260]   In fact, let's do that quickly now.
[00:12:45.260 --> 00:12:50.360]   So as you can see, the flattened layer has no trainable parameters, since it always does
[00:12:50.360 --> 00:12:52.440]   the same thing every time.
[00:12:52.440 --> 00:12:58.120]   And the dense layer has 7,850 trainable parameters.
[00:12:58.120 --> 00:13:03.200]   Despite having all these parameters, our model can actually only look at individual correlations
[00:13:03.200 --> 00:13:05.260]   between a pixel and a label.
[00:13:05.260 --> 00:13:10.120]   If we want to capture interactions between pixels, we need to make our model more complicated.
[00:13:10.120 --> 00:13:15.160]   A multilayer perceptron takes the output of some number of perceptrons and uses them as
[00:13:15.160 --> 00:13:17.600]   input to more perceptrons.
[00:13:17.600 --> 00:13:22.000]   Here the input comes in from the bottom, and there's a hidden intermediate layer.
[00:13:22.000 --> 00:13:26.360]   The intermediate perceptrons work just like the output perceptrons, but they don't classify
[00:13:26.360 --> 00:13:27.360]   anything.
[00:13:27.360 --> 00:13:29.520]   They just output numbers.
[00:13:29.520 --> 00:13:33.880]   Typically these days, hidden layers use an activation function called ReLU, a rectified
[00:13:33.880 --> 00:13:38.460]   linear unit, a complicated name for an unbelievably simple algorithm.
[00:13:38.460 --> 00:13:40.840]   It just truncates the negative values.
[00:13:40.840 --> 00:13:46.000]   Basically, it turns out all nonlinear activation functions work very similarly, and ReLU is
[00:13:46.000 --> 00:13:50.400]   just really fast and has some other benefits, so it's used for intermediate layers most
[00:13:50.400 --> 00:13:51.400]   of the time.
[00:13:51.400 --> 00:13:57.640]   OK, you can try to modify our existing code, or you can take a look at MLP.py, where we
[00:13:57.640 --> 00:14:00.460]   set this up for you.
[00:14:00.460 --> 00:14:04.660]   This code is super similar to our perceptron model, with really just a single important
[00:14:04.660 --> 00:14:07.760]   line added, and that's line 40.
[00:14:07.760 --> 00:14:12.120]   And what this line does is it adds a second dense layer with a number of hidden nodes.
[00:14:12.120 --> 00:14:17.800]   Now we set this in config.hiddenNodes to 100, but you can definitely try other numbers.
[00:14:17.800 --> 00:14:23.480]   Now let's run MLP.py and see what happens.
[00:14:23.480 --> 00:14:27.440]   One thing you'll notice right away is that this model takes a lot longer to train, but
[00:14:27.440 --> 00:14:29.800]   over time it gets better.
[00:14:29.800 --> 00:14:34.500]   Now there's another issue that's more subtle but super important, which is that our accuracy
[00:14:34.500 --> 00:14:38.600]   now starts to get higher than our validation accuracy.
[00:14:38.600 --> 00:14:42.440]   In fact, if you run this model for a while, the accuracy will get a lot better and the
[00:14:42.440 --> 00:14:44.840]   validation accuracy will not improve.
[00:14:44.840 --> 00:14:48.520]   This is a phenomenon known as overfitting.
[00:14:48.520 --> 00:14:52.480]   As models become more complicated, overfitting becomes more and more of an issue, and it
[00:14:52.480 --> 00:14:57.120]   can happen in all kinds of domains, including single variable regression.
[00:14:57.120 --> 00:15:01.000]   Here the line in the middle is a little more free than the straight line is on the left,
[00:15:01.000 --> 00:15:04.320]   but if we let our line move around too much, it can just go through all the points without
[00:15:04.320 --> 00:15:06.880]   really capturing the data.
[00:15:06.880 --> 00:15:08.720]   How do you know if you're overfitting?
[00:15:08.720 --> 00:15:13.160]   When your training accuracy is better than your validation accuracy.
[00:15:13.160 --> 00:15:18.720]   Remember that our validation accuracy is the accuracy on data that the model did not see
[00:15:18.720 --> 00:15:23.920]   while it was training, so the validation accuracy is a better estimate of how our model is actually
[00:15:23.920 --> 00:15:26.620]   going to do.
[00:15:26.620 --> 00:15:31.520]   Try setting the number of hidden nodes to a larger and larger number and see what happens.
[00:15:31.520 --> 00:15:32.520]   What happens?
[00:15:32.520 --> 00:15:33.520]   I'll wait.
[00:15:33.520 --> 00:15:34.520]   Okay, we're back.
[00:15:34.520 --> 00:15:39.120]   Maybe that took you an hour, if you really did it.
[00:15:39.120 --> 00:15:43.640]   Probably the training accuracy gets better and better, but the validation accuracy, or
[00:15:43.640 --> 00:15:48.320]   accuracy on data the model hasn't seen before, gets worse and worse, or at least just flattens
[00:15:48.320 --> 00:15:49.880]   out.
[00:15:49.880 --> 00:15:54.760]   As models get more and more complicated, they have the potential to do better and better,
[00:15:54.760 --> 00:15:58.480]   but overfitting becomes more and more of a problem.
[00:15:58.480 --> 00:16:02.480]   In general, overfitting is a huge problem with neural nets, but there's actually one
[00:16:02.480 --> 00:16:05.080]   awesome silver bullet.
[00:16:05.080 --> 00:16:07.280]   That silver bullet is dropout.
[00:16:07.280 --> 00:16:11.000]   Dropout is a simple, easy to explain algorithm that helps a ton.
[00:16:11.000 --> 00:16:16.080]   On every batch of data, we set some fraction of the inputs in a layer to zero.
[00:16:16.080 --> 00:16:18.000]   Why is this a good idea?
[00:16:18.000 --> 00:16:21.280]   You might want to stop the video and think about this for a second.
[00:16:21.280 --> 00:16:25.720]   The way I like to think about it is that it forces the network to learn more than one
[00:16:25.720 --> 00:16:28.160]   reason for every classification.
[00:16:28.160 --> 00:16:33.920]   Imagine a pixel in the upper left was always lit up if and only if the digit was a seven.
[00:16:33.920 --> 00:16:37.800]   Our network would happily learn that it only needs to look at the upper left-hand digit
[00:16:37.800 --> 00:16:41.680]   to decide if the handwritten number was a seven to optimize the loss in the training
[00:16:41.680 --> 00:16:42.800]   data.
[00:16:42.800 --> 00:16:47.880]   But dropout would force the network to learn multiple pathways for deciding if a digit
[00:16:47.880 --> 00:16:55.280]   is a seven, because some fraction of the time that value would be hidden.
[00:16:55.280 --> 00:17:00.400]   Dropout turns off when you deploy your model or when you run your model on a held-out set.
[00:17:00.400 --> 00:17:01.880]   It's really easy to add.
[00:17:01.880 --> 00:17:05.520]   You should just add dropout layers between your perceptron layers and check out what
[00:17:05.520 --> 00:17:06.120]   it does.
[00:17:06.120 --> 00:17:30.280]   [VIDEO PLAYBACK]
[00:17:30.280 --> 00:17:33.640]   - Now, when we train our model, the training accuracy
[00:17:33.640 --> 00:17:36.720]   is actually worse than the validation accuracy.
[00:17:36.720 --> 00:17:39.040]   But the great thing is that our validation accuracy
[00:17:39.040 --> 00:17:45.160]   can keep going up as we make our model more and more complicated.
[00:17:45.160 --> 00:17:46.880]   A great rule of thumb is this.
[00:17:46.880 --> 00:17:51.560]   If your training accuracy is higher than your validation or test accuracy,
[00:17:51.560 --> 00:17:53.280]   you're overfitting.
[00:17:53.280 --> 00:17:57.360]   If you are overfitting, anything you can do to make your model fancier or more
[00:17:57.360 --> 00:17:59.760]   complicated is only going to hurt you.
[00:17:59.760 --> 00:18:02.640]   You need to stop and fix that overfitting.
[00:18:02.640 --> 00:18:06.360]   And the easiest way to fix overfitting is almost always to add dropout.
[00:18:06.360 --> 00:18:09.960]   It's typically put between every layer with free parameters.
[00:18:09.960 --> 00:18:13.360]   So this is probably the densest video that we're going to make,
[00:18:13.360 --> 00:18:16.200]   and we covered a ton of material.
[00:18:16.200 --> 00:18:18.040]   We covered overfitting.
[00:18:18.040 --> 00:18:20.160]   We covered data normalization.
[00:18:20.160 --> 00:18:25.760]   We covered building a multi-class perceptron and a multi-layer perceptron.
[00:18:25.760 --> 00:18:29.080]   But we actually still haven't squeezed out all the performance
[00:18:29.080 --> 00:18:31.760]   that we can out of a data set like this.
[00:18:31.760 --> 00:18:34.360]   And to get that final ounce of performance,
[00:18:34.360 --> 00:18:36.680]   we're going to have to build a convolutional neural network.
[00:18:36.680 --> 00:18:39.520]   And I know you guys are excited to build a convolutional neural network.
[00:18:39.520 --> 00:18:41.760]   So stay tuned for the next video.
[00:18:41.760 --> 00:18:45.440]   Well, guys, it's my dream to be a YouTube celebrity.
[00:18:45.440 --> 00:18:47.360]   And I need your help.
[00:18:47.360 --> 00:18:48.880]   Subscribe.
[00:18:48.880 --> 00:18:49.960]   That's how it works, right?
[00:18:49.960 --> 00:18:50.920]   Down or up?
[00:18:50.920 --> 00:18:52.080]   You should do a box.
[00:18:52.080 --> 00:18:52.800]   Or I don't know.
[00:18:52.800 --> 00:18:55.560]   We'll put a box over there.
[00:18:55.560 --> 00:18:57.580]   (laughing)

