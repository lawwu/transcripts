
[00:00:00.000 --> 00:00:06.120]   The teams I've seen been really successful at deploying ML products.
[00:00:06.120 --> 00:00:11.960]   They've had people who formally, or when formally, have taken on that hot responsibility for
[00:00:11.960 --> 00:00:17.240]   the whole thing and have the people who are writing the inner loops of the assembly sitting
[00:00:17.240 --> 00:00:20.400]   next to the people who are creating the models.
[00:00:20.400 --> 00:00:24.800]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:24.800 --> 00:00:27.040]   and I'm your host, Lukas Biewald.
[00:00:27.040 --> 00:00:31.200]   This is a conversation with Pete Worden, well-known hacker and blogger.
[00:00:31.200 --> 00:00:35.200]   Among many things that he's done in his life, he started a company Jetpack, which was a
[00:00:35.200 --> 00:00:40.740]   very early mobile machine learning app company that was bought by Google in 2014.
[00:00:40.740 --> 00:00:45.200]   He's also been a tech lead and staff engineer on the TensorFlow team since then.
[00:00:45.200 --> 00:00:48.400]   He's been at TensorFlow since the very beginning.
[00:00:48.400 --> 00:00:53.560]   He's written a book about taking ML models and making them work on embedded devices,
[00:00:53.560 --> 00:00:56.080]   everything from an Arduino to a Raspberry Pi.
[00:00:56.080 --> 00:00:59.000]   It's something that I'm really passionate about, so we really get into it in the technical
[00:00:59.000 --> 00:01:00.000]   details.
[00:01:00.000 --> 00:01:02.400]   I think you'll really enjoy this interview.
[00:01:02.400 --> 00:01:07.040]   Quick disclaimer for this conversation, we had a few glitches in the audio, which are
[00:01:07.040 --> 00:01:08.120]   entirely my fault.
[00:01:08.120 --> 00:01:13.080]   I've been traveling with my family to Big Sur, which is a lot of fun, but I didn't bring
[00:01:13.080 --> 00:01:17.680]   all my podcasting gear, as you can probably see.
[00:01:17.680 --> 00:01:22.480]   If anything's inaudible, please check the transcription, which is provided in the notes.
[00:01:22.480 --> 00:01:23.480]   All right, Pete.
[00:01:23.480 --> 00:01:27.960]   I have a lot of questions for you, but since this is my show, I'm going to start with the
[00:01:27.960 --> 00:01:32.160]   question that I would want to ask if I was listening.
[00:01:32.160 --> 00:01:37.920]   Tell me again about the time that you hacked the Raspberry Pi to train neural nets with
[00:01:37.920 --> 00:01:38.920]   the GPU.
[00:01:38.920 --> 00:01:39.920]   Oh, God.
[00:01:39.920 --> 00:01:48.360]   Yeah, that was really fun.
[00:01:48.360 --> 00:01:57.120]   So back when the Raspberry Pi first came out, it had a GPU in it, but it wasn't a GPU you
[00:01:57.120 --> 00:02:02.200]   could do anything useful with, unless you wanted to draw things.
[00:02:02.200 --> 00:02:06.840]   Who wants to just draw things with a GPU?
[00:02:06.840 --> 00:02:15.160]   But there was some reverse engineering that had been happening, and some crazy engineers
[00:02:15.160 --> 00:02:22.920]   out there on the hardware side who'd actually managed to get a manual describing how to
[00:02:22.920 --> 00:02:27.680]   program the Raspberry Pi GPU at a low level.
[00:02:27.680 --> 00:02:35.800]   And this had been driving me crazy ever since I'd been at Apple years ago, because I was
[00:02:35.800 --> 00:02:44.680]   always able to use GLSL and all of these comparatively high-level languages to program GPUs.
[00:02:44.680 --> 00:02:51.800]   But I was always trying to get them to do things that the designers hadn't intended.
[00:02:51.800 --> 00:02:56.440]   Like when I was at Apple, I was trying to get them to do image processing rather than
[00:02:56.440 --> 00:02:59.680]   just doing straightforward graphics.
[00:02:59.680 --> 00:03:00.680]   And I never...
[00:03:00.680 --> 00:03:04.080]   You may hear a dog in the background.
[00:03:04.080 --> 00:03:07.240]   That is our new puppy, Nutmeg.
[00:03:07.240 --> 00:03:09.720]   But I always wanted to be able to program them.
[00:03:09.720 --> 00:03:15.600]   I knew that there was an Assembler level that I could program them at if I only had access.
[00:03:15.600 --> 00:03:21.800]   And I spent five years at Apple trying to persuade ATI and NVIDIA to give me access,
[00:03:21.800 --> 00:03:27.960]   and I actually managed to persuade them, but then the driver people at Apple were like,
[00:03:27.960 --> 00:03:35.600]   "No, don't give him access, because then we'll have to support the crazy things he's doing."
[00:03:35.600 --> 00:03:37.880]   So when the Raspberry Pi came along...
[00:03:37.880 --> 00:03:41.000]   And if they haven't had one or two or three?
[00:03:41.000 --> 00:03:45.600]   This was back in the Raspberry Pi 1 days.
[00:03:45.600 --> 00:03:54.200]   So it was not long after it first came out, and they actually gave you the datasheet for
[00:03:54.200 --> 00:04:03.520]   the GPU, which described the instruction format for programming all of these weird little
[00:04:03.520 --> 00:04:08.000]   hardware blocks that were inside the GPU.
[00:04:08.000 --> 00:04:12.640]   And there really wasn't anything like an Assembler.
[00:04:12.640 --> 00:04:18.840]   There wasn't basically anything that you would expect to be able to use.
[00:04:18.840 --> 00:04:22.440]   All you had was the raw, like, "Hey, these are the machine code instructions."
[00:04:22.440 --> 00:04:29.920]   And especially back in those days, in the Raspberry Pi 1 days, there wasn't even any
[00:04:29.920 --> 00:04:35.520]   SIMD instructions, really, on the Raspberry Pi, because it was using an ARM v6.
[00:04:35.520 --> 00:04:38.960]   Can I ask, what is a SIMD instruction?
[00:04:38.960 --> 00:04:39.960]   Oh, sorry.
[00:04:39.960 --> 00:04:42.320]   Single Implement Multiple Data.
[00:04:42.320 --> 00:04:50.200]   So if you're familiar with x86, it's things like SCE or AVX.
[00:04:50.200 --> 00:04:58.800]   It's basically a way of saying, "Hey, I've got an array of 32 numbers.
[00:04:58.800 --> 00:05:01.320]   Multiply them all."
[00:05:01.320 --> 00:05:07.760]   And specifying that in one instruction versus having a loop that goes through all 32 instructions
[00:05:07.760 --> 00:05:12.920]   and does them one at a time.
[00:05:12.920 --> 00:05:18.560]   So it's a really nice way of speeding up anything that's doing a lot of number crunching, whether
[00:05:18.560 --> 00:05:25.880]   it's graphics or whether it's, in our case, machine learning.
[00:05:25.880 --> 00:05:31.320]   And I really wanted to get some cool image recognition stuff.
[00:05:31.320 --> 00:05:35.040]   This was back when AlexNet was all the rage.
[00:05:35.040 --> 00:05:42.840]   I wanted to get AlexNet running in less than 30 seconds a frame on this Raspberry Pi.
[00:05:42.840 --> 00:05:45.320]   And the ARM v6 really was.
[00:05:45.320 --> 00:05:53.840]   I think it was just like, Vodcom had some dumpster full of these chips they couldn't
[00:05:53.840 --> 00:05:56.960]   sell because they were so old.
[00:05:56.960 --> 00:05:57.960]   This is not official.
[00:05:57.960 --> 00:06:02.760]   I have no idea if this is true, but it feels true.
[00:06:02.760 --> 00:06:04.160]   And so they were like, "Oh, sure.
[00:06:04.160 --> 00:06:07.400]   Use them for this Raspberry Pi thing that we're thinking about."
[00:06:07.400 --> 00:06:11.400]   And they were so old that it was actually really hard to even find compiler support.
[00:06:11.400 --> 00:06:18.080]   They didn't have especially these kinds of modern optimizations that you would expect
[00:06:18.080 --> 00:06:19.280]   to have.
[00:06:19.280 --> 00:06:27.040]   But I knew that this GPU could potentially do what I wanted.
[00:06:27.040 --> 00:06:29.520]   So I spent some time with the data sheet.
[00:06:29.520 --> 00:06:33.680]   There were a bunch of handful of people doing open source hacking on this stuff.
[00:06:33.680 --> 00:06:39.040]   So I was able to fork some of their projects.
[00:06:39.040 --> 00:06:50.360]   And actually, funnily enough, some of the Raspberry Pi founders were actually very interested
[00:06:50.360 --> 00:06:52.680]   in this too.
[00:06:52.680 --> 00:07:06.880]   So I ended up hacking away and managed to figure out how to do this matrix multiplication.
[00:07:06.880 --> 00:07:15.920]   And funnily enough, one of the people who was really into this was actually Evan Lupton,
[00:07:15.920 --> 00:07:20.640]   the founder of Raspberry Pi.
[00:07:20.640 --> 00:07:24.720]   He was actually one of the few people who actually replied on the forums when I was
[00:07:24.720 --> 00:07:29.200]   sending out distress signals when I was getting stuck on stuff.
[00:07:29.200 --> 00:07:36.040]   So anyway, I ended up being able to use the GPU to do this matrix multiplication.
[00:07:36.040 --> 00:07:45.000]   So I'd actually run AlexNet, recognizing a cat or a dog, in two seconds rather than 30
[00:07:45.000 --> 00:07:46.000]   seconds.
[00:07:46.000 --> 00:07:53.280]   And it was some of the most fun I've had in years, because it really was trying to string
[00:07:53.280 --> 00:07:58.600]   things together with sticky tape and chicken wire.
[00:07:58.600 --> 00:07:59.600]   And yeah, I had a blast.
[00:07:59.600 --> 00:08:01.280]   How does it even work?
[00:08:01.280 --> 00:08:07.920]   You're writing assembly and running it on a GPU.
[00:08:07.920 --> 00:08:13.600]   What environment are you writing this in?
[00:08:13.600 --> 00:08:19.800]   So I was pretty much using a text editor, and then there were a couple of different
[00:08:19.800 --> 00:08:23.400]   people who had done some work on assembly projects.
[00:08:23.400 --> 00:08:28.720]   None of them really worked, or they didn't work for what I needed.
[00:08:28.720 --> 00:08:30.600]   So I ended up hacking them up together.
[00:08:30.600 --> 00:08:39.040]   So I'd then feed in the text into the assembler, which would produce the raw command streams.
[00:08:39.040 --> 00:08:46.040]   And then I had to figure out the right memory addresses to write to from the Raspberry Pi
[00:08:46.040 --> 00:08:49.120]   CPU to upload this program.
[00:08:49.120 --> 00:08:55.400]   And then that program would be sitting there in the, I think there was something like,
[00:08:55.400 --> 00:09:00.840]   some ridiculously small number of instructions, I could run like 64 instructions in there
[00:09:00.840 --> 00:09:04.120]   or something, or 128.
[00:09:04.120 --> 00:09:09.080]   The program would be sitting there on all of these, I think it was like four or eight
[00:09:09.080 --> 00:09:10.080]   cores.
[00:09:10.080 --> 00:09:13.240]   I would then have to kick them off.
[00:09:13.240 --> 00:09:20.000]   I'd have to feed in the memory from the...
[00:09:20.000 --> 00:09:28.960]   And honestly, in terms of software engineering, it was a disaster, but it worked.
[00:09:28.960 --> 00:09:30.920]   What kind of debugging messages do you get?
[00:09:30.920 --> 00:09:33.880]   I'm thinking about the college and writing this, I remember the computer would just crash,
[00:09:33.880 --> 00:09:36.360]   I think, when there was an invalid...
[00:09:36.360 --> 00:09:43.680]   Well, I was actually writing out to a pixel, so I could tell by the pixel color how far
[00:09:43.680 --> 00:09:56.080]   through the program that it had actually got, which I'm colorblind, so that didn't help.
[00:09:56.080 --> 00:10:02.080]   But yeah, it was really getting down and dirty.
[00:10:02.080 --> 00:10:05.840]   It was the sort of thing where you can just lose yourself for a few weeks in some really
[00:10:05.840 --> 00:10:07.880]   obscure technical problems.
[00:10:07.880 --> 00:10:14.840]   I mean, having worked on projects kind of like that, how did you maintain hope that
[00:10:14.840 --> 00:10:16.960]   the project would finish in a way that it would work?
[00:10:16.960 --> 00:10:21.480]   I think that might be the hardest thing for me to do.
[00:10:21.480 --> 00:10:27.720]   Well, at the time I was working on a startup, and this seemed a much more practical problem
[00:10:27.720 --> 00:10:33.960]   than all of the other things I was dealing with at the startup.
[00:10:33.960 --> 00:10:40.040]   So in a lot of ways, it was procrastination on dealing with worse problems.
[00:10:40.040 --> 00:10:41.560]   Great answer.
[00:10:41.560 --> 00:10:42.560]   Yeah.
[00:10:42.560 --> 00:10:50.200]   And I guess, what was the reason that the Raspberry Pi included this GPU that they wouldn't
[00:10:50.200 --> 00:10:52.440]   actually let you directly access?
[00:10:52.440 --> 00:10:55.800]   Was it for streaming video or something?
[00:10:55.800 --> 00:11:01.880]   Yeah, it really was designed for, I think, early 2000s set-top boxes and things.
[00:11:01.880 --> 00:11:12.280]   So you were going to be able to draw a few triangles, but it wasn't designed to run any
[00:11:12.280 --> 00:11:13.960]   shaders or anything on it.
[00:11:13.960 --> 00:11:21.480]   So GLSL and things like that weren't even considered a threat at that time.
[00:11:21.480 --> 00:11:25.720]   I think there's been some work on that since, I think, maybe with some more modern versions
[00:11:25.720 --> 00:11:26.720]   of the GPUs.
[00:11:26.720 --> 00:11:31.760]   But back in the last week, my one day, it's just like, you're going to draw some triangles
[00:11:31.760 --> 00:11:32.760]   and that's...
[00:11:32.760 --> 00:11:37.200]   Have you been following the Raspberry Pi since?
[00:11:37.200 --> 00:11:40.320]   Do you have thoughts on the 4?
[00:11:40.320 --> 00:11:43.600]   Did they talk to you about what to include there, maybe?
[00:11:43.600 --> 00:11:44.600]   No, no.
[00:11:44.600 --> 00:11:54.880]   I think they knew better than that, because I'm not exactly an average user.
[00:11:54.880 --> 00:12:01.440]   As a general developer, it's fantastic because the Raspberry Pi 4 is this beast of a machine
[00:12:01.440 --> 00:12:07.080]   with multi-threading and it's got those SIMD instructions I talked about.
[00:12:07.080 --> 00:12:14.120]   There's I think support for GLSL and all these modern OpenGL things in the GPU.
[00:12:14.120 --> 00:12:18.720]   But as a hacker, I'm like, "Oh, it's just kind of..."
[00:12:18.720 --> 00:12:21.600]   Yeah, it's all kind of...
[00:12:21.600 --> 00:12:24.960]   So yeah, that's...
[00:12:24.960 --> 00:12:25.960]   Exactly.
[00:12:25.960 --> 00:12:31.080]   Well, it's funny because I think I met you when I was trying to get TensorFlow to run
[00:12:31.080 --> 00:12:36.480]   on the Raspberry Pi 3, which is literally just trying to compile it and link in the
[00:12:36.480 --> 00:12:37.480]   proper libraries.
[00:12:37.480 --> 00:12:40.400]   And I remember completely getting stuck.
[00:12:40.400 --> 00:12:44.360]   I'm ashamed to tell you that and reach out to the forums and being like, "Wow, the tech
[00:12:44.360 --> 00:12:50.680]   support from TensorFlow is unbelievably good that it's answering my questions."
[00:12:50.680 --> 00:12:51.680]   But I think you ended up...
[00:12:51.680 --> 00:12:52.680]   You found my email address as well.
[00:12:52.680 --> 00:12:55.680]   I think you dropped me an email.
[00:12:55.680 --> 00:13:00.680]   And again, I think you caught me in the middle of procrastinating on something that I was
[00:13:00.680 --> 00:13:01.680]   supposed to be doing.
[00:13:01.680 --> 00:13:04.520]   And I was like, "Oh, wow, this is way more fun.
[00:13:04.520 --> 00:13:07.800]   Let me spend some time on this."
[00:13:07.800 --> 00:13:16.040]   But no, you shouldn't underestimate that TensorFlow has so many dependencies, which is pretty
[00:13:16.040 --> 00:13:25.360]   normal for a Python cloud server project, because they're essentially free in that environment.
[00:13:25.360 --> 00:13:29.040]   You just do a pip install or something and it will just work.
[00:13:29.040 --> 00:13:39.560]   But as soon as you're moving over to something that's not the vanilla x86 Linux environment
[00:13:39.560 --> 00:13:44.320]   that it's expecting, you suddenly have to pay the price of trying to figure out all
[00:13:44.320 --> 00:13:45.320]   of these.
[00:13:45.320 --> 00:13:48.320]   Where did this come from?
[00:13:48.320 --> 00:13:49.320]   Right.
[00:13:49.320 --> 00:13:50.320]   Right.
[00:13:50.320 --> 00:13:55.920]   So I guess one question that comes to mind for me that I don't know if you feel like
[00:13:55.920 --> 00:14:00.440]   it's a fair question for you to answer, but I'd love your thoughts on it is, it seems
[00:14:00.440 --> 00:14:04.120]   like everyone trains their models, except for people at Google, train their models on
[00:14:04.120 --> 00:14:05.120]   NVIDIA GPUs.
[00:14:05.120 --> 00:14:11.680]   And I'm told that's because of the CUDA library that essentially compiles the code and CUDA
[00:14:11.680 --> 00:14:19.080]   NN that makes a low level language for writing ML components and then compiling them onto
[00:14:19.080 --> 00:14:20.320]   the NVIDIA chip.
[00:14:20.320 --> 00:14:28.720]   But if Pete Worden can just directly write code to do matrix multiplication on a chip
[00:14:28.720 --> 00:14:34.160]   that's not even trying to publish its docs and let anyone use it, where's the disconnect?
[00:14:34.160 --> 00:14:37.840]   Like why don't we see more chips being used for compiling?
[00:14:37.840 --> 00:14:42.800]   Why doesn't TensorFlow work better on top of more different kinds of architecture?
[00:14:42.800 --> 00:14:46.640]   Like I know that was one of the, I think that was one of the original design goals of TensorFlow,
[00:14:46.640 --> 00:14:51.680]   but we haven't seen maybe the explosion of different GPU architectures that I think we
[00:14:51.680 --> 00:14:55.080]   might've been expecting back in like 2016, 2017.
[00:14:55.080 --> 00:14:56.080]   Yeah.
[00:14:56.080 --> 00:15:02.680]   And I can't speak so directly to the TensorFlow experience, but I can say more generally,
[00:15:02.680 --> 00:15:10.960]   what I seen happening, you know, speaking personally is it's the damn researchers.
[00:15:10.960 --> 00:15:17.560]   They keep coming up with new techniques and better ways of training models.
[00:15:17.560 --> 00:15:26.640]   And what generally tends to happen is it follows the same model that sort of Alex Kraszewski
[00:15:26.640 --> 00:15:34.240]   originally did and his colleagues with AlexNet, where the thing that blew me away when I first
[00:15:34.240 --> 00:15:39.040]   started getting into deep learning was, you know, Alex had made his code available and
[00:15:39.040 --> 00:15:47.160]   he had not only been working at the high level model creation side, he'd also been really
[00:15:47.160 --> 00:15:54.920]   hacking on the CUDA kernels to run on the GPU to get stuff running fast enough.
[00:15:54.920 --> 00:16:00.200]   So it was this really interesting having to kind of understand all these high level concepts,
[00:16:00.200 --> 00:16:07.880]   these cutting edge concepts of machine learning, while also being this, you know, in a loop
[00:16:07.880 --> 00:16:13.240]   kind of assembly, essentially, not quite down to that level, but like intrinsic sort of,
[00:16:13.240 --> 00:16:17.560]   you know, really thinking about every cycle.
[00:16:17.560 --> 00:16:24.880]   And what has tended to happen is as new techniques have come in, the researchers tend to just
[00:16:24.880 --> 00:16:31.080]   for their own, to run their own experiments, they have to write things that run as fast
[00:16:31.080 --> 00:16:32.080]   as possible.
[00:16:32.080 --> 00:16:36.500]   So they've had to learn how to, you know, the default for this is CUDA.
[00:16:36.500 --> 00:16:44.720]   So you end up with, you know, new techniques coming in as CUDA implementation.
[00:16:44.720 --> 00:16:50.440]   Usually there's a C++ CPU implementation that may or may not be particularly optimized.
[00:16:50.440 --> 00:16:54.060]   And then there's definitely a CUDA implementation.
[00:16:54.060 --> 00:16:59.240]   And then the techniques that actually catch on, the rest of the world kind of has to then
[00:16:59.240 --> 00:17:06.880]   figure out how to take what's often, you know, great code for its purpose, but is written
[00:17:06.880 --> 00:17:12.320]   by researchers for research purposes, and then figure out how to port it to different
[00:17:12.320 --> 00:17:15.520]   systems, you know, with different precisions.
[00:17:15.520 --> 00:17:23.980]   And there's this whole, you know, hidden sort of amount of work that people have to do to
[00:17:23.980 --> 00:17:29.700]   take all of these emerging techniques and get them running across all architectures.
[00:17:29.700 --> 00:17:36.540]   And I think that's true across the, you know, across the whole ecosystem.
[00:17:36.540 --> 00:17:42.780]   And you know, it's one of the reasons that I really love for experimenting if you're
[00:17:42.780 --> 00:17:48.420]   in the Raspberry Pi sort of form factor, but you can afford to be burning like 10 watts
[00:17:48.420 --> 00:17:54.500]   of power, you know, grab a Jetson or a Jetson Nano or something, because then you've got
[00:17:54.500 --> 00:18:02.780]   essentially the same CPU that you'd be running in a desktop machine, but just on a much smaller
[00:18:02.780 --> 00:18:03.780]   form factor.
[00:18:03.780 --> 00:18:04.780]   Totally.
[00:18:04.780 --> 00:18:05.780]   Yeah.
[00:18:05.780 --> 00:18:11.100]   It makes me a little sad that the Raspberry Pi doesn't have an NVIDIA chip on it.
[00:18:11.100 --> 00:18:16.900]   I just, the heatsink alone would be...
[00:18:16.900 --> 00:18:22.860]   You know, one thing I noticed, so, you know, your book is excellent on kind of embedded
[00:18:22.860 --> 00:18:23.860]   ML.
[00:18:23.860 --> 00:18:27.940]   And actually I was in a different interview, which we should pull that clip of an interview
[00:18:27.940 --> 00:18:31.860]   with Pete Skomarek, and we both had your book on our desk.
[00:18:31.860 --> 00:18:35.020]   So we didn't know if you know him, but...
[00:18:35.020 --> 00:18:39.060]   Yeah, yeah, no, I'm a good, yeah, Pete's awesome.
[00:18:39.060 --> 00:18:40.740]   He's been doing some amazing stuff too.
[00:18:40.740 --> 00:18:46.940]   He's another person who occasionally catches me when I'm procrastinating and I'm able to
[00:18:46.940 --> 00:18:48.940]   offer some advice and vice versa.
[00:18:48.940 --> 00:18:52.780]   Nice, maybe we should have a neighborhood...
[00:18:52.780 --> 00:18:56.940]   Yeah, procrastination, hacking procrastination list.
[00:18:56.940 --> 00:19:01.460]   But I guess it seems like pretty obvious that, you know, you do some interesting projects
[00:19:01.460 --> 00:19:02.860]   in your house or for personal stuff.
[00:19:02.860 --> 00:19:08.300]   I was wondering if you could talk about any of your own kind of personal ML hack projects.
[00:19:08.300 --> 00:19:11.820]   Oh, that's a really...
[00:19:11.820 --> 00:19:19.500]   So I'm obsessed with actually trying to get a magic wand working well.
[00:19:19.500 --> 00:19:29.660]   One of the things I get to see is these applications that are being produced by industry professionals
[00:19:29.660 --> 00:19:35.420]   for things like Android, Android phones, smartphones in general.
[00:19:35.420 --> 00:19:43.660]   And the gesture recognition using accelerometers just works really well on these phones.
[00:19:43.660 --> 00:19:51.860]   And because people are able to get it working really well in the commercial realm, but I
[00:19:51.860 --> 00:19:58.540]   haven't seen that many examples of it actually working well, you know, as open source or
[00:19:58.540 --> 00:20:05.580]   any other way, and even the example that we ship with TensorFlow, like micro, is not good
[00:20:05.580 --> 00:20:06.580]   enough.
[00:20:06.580 --> 00:20:12.540]   Like it's a proof of concept, but it doesn't work nearly as well as I want.
[00:20:12.540 --> 00:20:16.220]   So I have been, you know, sort of...
[00:20:16.220 --> 00:20:20.980]   That's been one of my main projects I keep coming back to is, okay, how can I actually,
[00:20:20.980 --> 00:20:26.460]   you know, just sort of do a sort of, you know, Zorro sign or something holding...
[00:20:26.460 --> 00:20:34.780]   I've got the little Arduino on my desk here and sort of, you know, do that and have it
[00:20:34.780 --> 00:20:35.780]   recognize...
[00:20:35.780 --> 00:20:40.220]   You know, I want to be able to sort of, you know, do that to the TV screen and have it
[00:20:40.220 --> 00:20:43.700]   like change channels or something.
[00:20:43.700 --> 00:20:47.180]   And so what I really want to be able to do...
[00:20:47.180 --> 00:20:53.100]   We actually released some of this stuff as part of Google I/O, so I'll share a link.
[00:20:53.100 --> 00:20:56.580]   Maybe you can put in the description afterwards.
[00:20:56.580 --> 00:21:03.660]   But my end goal, because these things actually have Bluetooth, I want it to be able to emulate
[00:21:03.660 --> 00:21:14.260]   a keyboard or a mouse or gamepad controller and actually be able to sort of, you know,
[00:21:14.260 --> 00:21:15.620]   customize it so that you can...
[00:21:15.620 --> 00:21:20.740]   Or like a MIDI keyboard even as well, and actually customize it so you can do some kind
[00:21:20.740 --> 00:21:26.260]   of gesture and then have it like, you know, you do a Z and it presses the Z key or something
[00:21:26.260 --> 00:21:27.260]   on your virtual keyboard.
[00:21:27.260 --> 00:21:31.580]   And that does something interesting with your, like, whatever you've got it connected up
[00:21:31.580 --> 00:21:33.100]   to.
[00:21:33.100 --> 00:21:40.820]   So that isn't quite working yet, but hopefully I get some tough enough problems in my main
[00:21:40.820 --> 00:21:45.420]   job that I'll procrastinate and spend some more time on that.
[00:21:45.420 --> 00:21:47.340]   Man, I hope for that too.
[00:21:47.340 --> 00:21:54.140]   I guess for people that maybe aren't experts in embedded computing systems, could you describe
[00:21:54.140 --> 00:21:57.940]   the difference between a Raspberry Pi and an Arduino and then the sort of different
[00:21:57.940 --> 00:22:02.340]   challenges in getting ML to run on a Raspberry Pi versus an Arduino?
[00:22:02.340 --> 00:22:03.340]   Yeah.
[00:22:03.340 --> 00:22:09.700]   At a top level, the biggest difference is the amount of memory.
[00:22:09.700 --> 00:22:25.340]   This Arduino Nano BLE Sense 33 is, I think it has like 256K of RAM and either like 512K
[00:22:25.340 --> 00:22:31.380]   or something like that of flash, kind of like, you know, read-only memory.
[00:22:31.380 --> 00:22:37.260]   So it's this really, really small environment that you actually have to run in.
[00:22:37.260 --> 00:22:43.300]   And it means you don't have a lot of things that you would expect to have to an operating
[00:22:43.300 --> 00:22:51.620]   system like files or printf or, you know, you're really having to look at every single
[00:22:51.620 --> 00:22:52.620]   byte.
[00:22:52.620 --> 00:22:57.340]   You know, the printf function itself, in a lot of implementations, it will actually take
[00:22:57.340 --> 00:23:04.380]   up about 25 kilobytes of code size just having printf because printf is essentially this
[00:23:04.380 --> 00:23:08.700]   big switch statement of, oh, have you got a printf?
[00:23:08.700 --> 00:23:13.140]   Oh, here's a printf, you know, a float value.
[00:23:13.140 --> 00:23:18.980]   And there's like hundreds of these modifiers and things you never even think of, or you
[00:23:18.980 --> 00:23:20.380]   can never even imagine.
[00:23:20.380 --> 00:23:25.380]   All that code has to get in if you actually have printf in the system.
[00:23:25.380 --> 00:23:33.620]   So all of these devices that we're aiming at, they often have, you know, only a couple
[00:23:33.620 --> 00:23:39.100]   of hundred kilobytes of space to write your programs in.
[00:23:39.100 --> 00:23:40.780]   So you may be sensing a theme here.
[00:23:40.780 --> 00:23:46.740]   I love trying to sort of fit, you know, take modern stuff and kind of like fit it back
[00:23:46.740 --> 00:23:49.780]   into something that's like a Commodore 64.
[00:23:49.780 --> 00:23:52.820]   So okay, it seems like, you know, Pete Warden doesn't always need a practical reason to
[00:23:52.820 --> 00:23:58.180]   do something, but what might be the practical reason to use an Arduino versus a Raspberry
[00:23:58.180 --> 00:23:59.180]   Pi?
[00:23:59.180 --> 00:24:08.700]   Well, luckily I've actually managed to justify my hobby and turn it into, you know, my full-time
[00:24:08.700 --> 00:24:15.740]   project because one great example of where we use these is, actually, you don't see my
[00:24:15.740 --> 00:24:16.740]   phone here.
[00:24:16.740 --> 00:24:20.720]   It's going to hold up the phone, but you know what a phone looks like.
[00:24:20.720 --> 00:24:25.100]   If you think about things like, and I won't say the full word because it will set off
[00:24:25.100 --> 00:24:34.300]   people's phones, but the OKG wake word, or the wake words on Apple or Amazon, when you're
[00:24:34.300 --> 00:24:43.300]   using a voice interface, you want your phone to wake up when it hears you say that word.
[00:24:43.300 --> 00:24:51.140]   But what it turns out is you can't afford to even run the main ARM application processor
[00:24:51.140 --> 00:24:59.260]   24/7 to listen out for that word because your battery would just be drained.
[00:24:59.260 --> 00:25:06.180]   These main CPUs use maybe sort of somewhere around a watt of power when they're sort of
[00:25:06.180 --> 00:25:11.620]   up and running when you're like browsing the web or interacting with it.
[00:25:11.620 --> 00:25:20.100]   So what they all do instead is actually have what's often called an always-on hub or chip
[00:25:20.100 --> 00:25:26.540]   or sensor hub or something like that, where the main CPU is powered down, so it's not
[00:25:26.540 --> 00:25:36.580]   using any energy, but this much more limited but much more lower energy chip is actually
[00:25:36.580 --> 00:25:44.340]   running and listening to the microphone and running a very, very small, you know, some
[00:25:44.340 --> 00:25:52.460]   on the order of like 30 kilobytes ML model to say, "Hey, has somebody said that word
[00:25:52.460 --> 00:25:58.780]   or that wake word phrase that I'm supposed to be listening out for?"
[00:25:58.780 --> 00:26:03.900]   And they have exactly the same challenges.
[00:26:03.900 --> 00:26:08.140]   You know, you only have like a few hundred kilobytes at most, you're running on a pretty
[00:26:08.140 --> 00:26:13.900]   low-end processor, you don't have an operating system, every byte counts, so you have to
[00:26:13.900 --> 00:26:19.860]   kind of like squeeze the library as small as possible.
[00:26:19.860 --> 00:26:28.020]   And so that's kind of one of the real-world applications where we're actually using this
[00:26:28.020 --> 00:26:31.580]   TensorFlow Lite micro.
[00:26:31.580 --> 00:26:37.780]   And more generally, you know, the Raspberry Pi is, you know, you're probably looking at
[00:26:37.780 --> 00:26:41.820]   $25, something like that.
[00:26:41.820 --> 00:26:48.860]   The equivalent which the Raspberry Pi Foundation just launched, I think last year, or maybe
[00:26:48.860 --> 00:26:55.020]   at the start of this year, that's kind of the equivalent of the Arduino is the Pico,
[00:26:55.020 --> 00:26:59.780]   and that's I think like $3 retail.
[00:26:59.780 --> 00:27:05.740]   And the Raspberry Pi again uses like one or two watts of power, so if you're going to
[00:27:05.740 --> 00:27:11.340]   run it for a day, you essentially need the phone battery that it will kind of, you know,
[00:27:11.340 --> 00:27:17.820]   run down over the course of the day, whereas the Pico is only using like 100 milliwatts,
[00:27:17.820 --> 00:27:20.780]   you know, a tenth of a watt.
[00:27:20.780 --> 00:27:24.300]   And so you can run it for sort of 10 times longer than the same battery, you can run
[00:27:24.300 --> 00:27:28.300]   it on a much smaller battery.
[00:27:28.300 --> 00:27:34.820]   And so these embedded devices tend to be used where there's like power constraints, or there's
[00:27:34.820 --> 00:27:40.620]   cost constraints, or even where there's form factor constraints, because, you know, this
[00:27:40.620 --> 00:27:45.020]   thing is even smaller than a Raspberry Pi Zero.
[00:27:45.020 --> 00:27:50.420]   And you can like kind of stick it anywhere and it will survive being run over and all
[00:27:50.420 --> 00:27:52.220]   of those sorts of things.
[00:27:52.220 --> 00:27:58.340]   So can you describe like, let's take for example, like a speech recognition system.
[00:27:58.340 --> 00:28:01.660]   Can you describe the differences of how you would think about training and deploying if
[00:28:01.660 --> 00:28:08.380]   it was going to like the cloud or a big desktop server versus a Raspberry Pi versus an Arduino?
[00:28:08.380 --> 00:28:17.700]   Yeah, and the theme again, is size and how much space you actually have on these systems.
[00:28:17.700 --> 00:28:25.260]   So you'll be thinking always about how can I make this model as small as possible?
[00:28:25.260 --> 00:28:30.980]   You know, you're looking at making the model probably in the tens of kilobytes for doing,
[00:28:30.980 --> 00:28:33.940]   you know, we have this example of doing speech recognition.
[00:28:33.940 --> 00:28:37.260]   I think it uses like a 20 kilobyte model.
[00:28:37.260 --> 00:28:45.180]   So you're going to be sacrificing accuracy and a whole bunch of other stuff in order
[00:28:45.180 --> 00:28:51.540]   to get something that will actually fit on this really low energy device.
[00:28:51.540 --> 00:28:55.500]   But hopefully it's still accurate enough that it's useful.
[00:28:55.500 --> 00:28:56.500]   Right.
[00:28:56.500 --> 00:28:57.620]   So how do you do that?
[00:28:57.620 --> 00:29:00.980]   Like how do you how do you reduce the size without compromising accuracy?
[00:29:00.980 --> 00:29:03.500]   Can you describe like some of the techniques?
[00:29:03.500 --> 00:29:04.500]   Yeah.
[00:29:04.500 --> 00:29:12.660]   So I actually just blogged about one trick that I've seen used, but I realized I hadn't
[00:29:12.660 --> 00:29:17.700]   seen in the literature very much, which is where, you know, the classic going back to
[00:29:17.700 --> 00:29:25.060]   AlexNet approach after you do a convolution in like an image recognition network, you
[00:29:25.060 --> 00:29:27.700]   often have like a pooling stage.
[00:29:27.700 --> 00:29:34.060]   So that pooling stage, you know, would either do average pooling or max pooling.
[00:29:34.060 --> 00:29:41.660]   And what that's doing is it's taking the output of the convolution, which is often the same
[00:29:41.660 --> 00:29:45.500]   size as the input, but with a lot more channels.
[00:29:45.500 --> 00:29:51.580]   And then it's taking blocks of like two by two values and it's saying, hey, I'm going
[00:29:51.580 --> 00:29:55.100]   to only take the maximum of that two by two block.
[00:29:55.100 --> 00:30:02.140]   So take four values and output one value or do the same, but do averaging.
[00:30:02.140 --> 00:30:06.500]   And that helps with accuracy.
[00:30:06.500 --> 00:30:15.420]   But because you're outputting these very large outputs from the convolution, that means that
[00:30:15.420 --> 00:30:21.100]   you have to have a lot of RAM because you have to hold the input for the convolution.
[00:30:21.100 --> 00:30:25.220]   And you also have to hold the output, which is the same size as the input, but typically
[00:30:25.220 --> 00:30:27.380]   has more channels.
[00:30:27.380 --> 00:30:31.100]   So the memory size is even larger.
[00:30:31.100 --> 00:30:36.220]   So instead of doing that, a common sort of technique that I've seen in the industry is
[00:30:36.220 --> 00:30:42.340]   to use a stride of two on the convolution.
[00:30:42.340 --> 00:30:46.380]   So instead of having the sliding window just slide over one pixel every time as you're
[00:30:46.380 --> 00:30:51.460]   doing the convolutions, you actually sort of have it jump two pixels horizontally and
[00:30:51.460 --> 00:30:53.220]   vertically.
[00:30:53.220 --> 00:31:05.820]   And that has the effect of outputting the same result or the same size, same number
[00:31:05.820 --> 00:31:11.460]   of elements you would get if you did a convolution plus a two by two pooling.
[00:31:11.460 --> 00:31:16.660]   But it means that you actually do less compute and you don't have to have nearly as much
[00:31:16.660 --> 00:31:21.220]   kind of active memory kicking around.
[00:31:21.220 --> 00:31:22.220]   Interesting.
[00:31:22.220 --> 00:31:30.180]   I had thought the size of the model, it was just the size of the model's parameters, but
[00:31:30.180 --> 00:31:33.500]   it sounds like you also, I mean, obviously you need some active memory, but it's hard
[00:31:33.500 --> 00:31:36.820]   to imagine that even could be on the order of magnitude of the size of the model.
[00:31:36.820 --> 00:31:41.860]   Like the pixels of the image and then the kind of intermediate results, I guess, can
[00:31:41.860 --> 00:31:43.740]   be bigger than the model.
[00:31:43.740 --> 00:31:44.740]   Yeah.
[00:31:44.740 --> 00:31:49.780]   I mean, well, that's kind of the nice thing about convolution is you get to reuse the
[00:31:49.780 --> 00:31:57.220]   weights in a way that you really don't with fully connected layers.
[00:31:57.220 --> 00:32:04.580]   So you can actually end up with convolution models, the activation memory, taking up a
[00:32:04.580 --> 00:32:07.820]   substantial amount of space.
[00:32:07.820 --> 00:32:11.340]   And I guess I'm also getting into the weeds a bit here because the obvious answer to your
[00:32:11.340 --> 00:32:16.420]   question is also quantization, like taking these floating point models and just turning
[00:32:16.420 --> 00:32:24.180]   them into eight bit because that immediately slashes all of your memory sizes by 75%.
[00:32:24.180 --> 00:32:28.900]   And what about, I mean, I've seen people get out into four bits or even one bit.
[00:32:28.900 --> 00:32:30.860]   Do you have thoughts on that?
[00:32:30.860 --> 00:32:31.860]   Yeah.
[00:32:31.860 --> 00:32:32.860]   Yeah.
[00:32:32.860 --> 00:32:35.860]   That's been some really, really interesting work.
[00:32:35.860 --> 00:32:41.900]   A colleague of mine, I actually, again, I'll send on a link to the paper, but looked at,
[00:32:41.900 --> 00:32:49.380]   I think it's something about the Pareto optimal, like bit depth for ResNet is like four bits
[00:32:49.380 --> 00:32:52.140]   or something like that.
[00:32:52.140 --> 00:32:57.500]   And there's been some really, really good research about going down to sort of four
[00:32:57.500 --> 00:33:04.540]   bits or two bits, or even going down to sort of binary networks with one bit.
[00:33:04.540 --> 00:33:14.260]   And the biggest challenge from our side is that CPUs aren't generally optimized for anything
[00:33:14.260 --> 00:33:17.860]   other than like eight bits arithmetic.
[00:33:17.860 --> 00:33:27.940]   So going down to these little bit depths, requires some advances in the hardware they're
[00:33:27.940 --> 00:33:29.580]   actually using.
[00:33:29.580 --> 00:33:32.300]   Do you have any thoughts about actually training on the edge?
[00:33:32.300 --> 00:33:36.100]   I feel like people have been talking about this for a long time, but I haven't seen examples
[00:33:36.100 --> 00:33:41.380]   where you actually do some of the training and then it passes that upstream.
[00:33:41.380 --> 00:33:48.140]   What I've seen is that, especially on the kind of embedded edge, it's very hard to get
[00:33:48.140 --> 00:33:51.780]   labeled data.
[00:33:51.780 --> 00:33:59.940]   And right now, there's been some great advances in unsupervised learning, but our workhorse
[00:33:59.940 --> 00:34:06.980]   approach to solving like image and audio and accelerometer kind of recognition problems
[00:34:06.980 --> 00:34:16.180]   is still around actually taking big label datasets and just running them through training.
[00:34:16.180 --> 00:34:21.260]   And so if you don't have some kind of implicit labels on the data that you're gathering on
[00:34:21.260 --> 00:34:26.260]   the edge, which you almost never do, it's very hard to justify training.
[00:34:26.260 --> 00:34:36.860]   The one case where I actually have seen this look like it's pretty promising is for industrial
[00:34:36.860 --> 00:34:38.180]   monitoring.
[00:34:38.180 --> 00:34:42.700]   So when you've got like a piece of machinery and you basically want to know if it's about
[00:34:42.700 --> 00:34:52.220]   to shake itself to bits because it's got kind of a mechanical problem and you have an accelerometer
[00:34:52.220 --> 00:34:57.220]   or a microphone sensor kind of sitting on this device.
[00:34:57.220 --> 00:35:01.980]   And the hard part is telling whether it's actually about to shake itself to bits or
[00:35:01.980 --> 00:35:07.980]   whether that's just how it normally like kind of like vibrates.
[00:35:07.980 --> 00:35:16.180]   And so one promising approach for this kind of predictive maintenance is to actually spend
[00:35:16.180 --> 00:35:24.540]   the first 24 hours just assuming that everything is normal and kind of learning, okay, this
[00:35:24.540 --> 00:35:26.300]   is normality.
[00:35:26.300 --> 00:35:32.700]   And then only after that start to kind of like look for things that are outside of the,
[00:35:32.700 --> 00:35:38.300]   so you're implicitly labeling like the first 24 hours that's okay, this is normal data.
[00:35:38.300 --> 00:35:42.900]   And then you're looking for anything that's kind of like an excursion out beyond that.
[00:35:42.900 --> 00:35:48.180]   So that sort of makes sense for some kind of a training approach.
[00:35:48.180 --> 00:35:58.540]   But even there, I still actually push people to consider things like using embeddings and
[00:35:58.540 --> 00:36:06.300]   other approaches that don't require full back propagation to do the training.
[00:36:06.300 --> 00:36:12.780]   For example, if you have an audio model that has to recognize a particular person saying
[00:36:12.780 --> 00:36:20.980]   a word, try and have that model produce a sort of n-dimensional vector that's an embedding
[00:36:20.980 --> 00:36:24.740]   and then have the person say the word three times.
[00:36:24.740 --> 00:36:32.620]   And then just use k-nearest neighbor sort of approaches to kind of tell if subsequent
[00:36:32.620 --> 00:36:37.740]   utterances are close in that embedding space.
[00:36:37.740 --> 00:36:43.980]   And then you've sort of done something that looks like learning from a user perspective,
[00:36:43.980 --> 00:36:48.980]   but you don't have to have all this machinery of variables and changing the neural network
[00:36:48.980 --> 00:36:53.540]   and you're just doing it as a post-processing action.
[00:36:53.540 --> 00:36:59.460]   Do you see a lot of actual real world uses, like actual companies kind of shipping stuff
[00:36:59.460 --> 00:37:01.860]   like models into microcontrollers?
[00:37:01.860 --> 00:37:02.860]   Yeah.
[00:37:02.980 --> 00:37:16.220]   And again, this is hard to talk about because a lot of these aren't like Android apps and
[00:37:16.220 --> 00:37:21.380]   things where people are fairly open and open source.
[00:37:21.380 --> 00:37:28.020]   A lot of these are pretty well-established old school industrial companies and automotive
[00:37:28.020 --> 00:37:30.700]   companies and things like that.
[00:37:30.700 --> 00:37:38.500]   But we do see there's a bunch of apps that are already, or a bunch of products out there
[00:37:38.500 --> 00:37:41.660]   that are already using ML under the hood.
[00:37:41.660 --> 00:37:48.700]   I mean, one of the examples I like to give is when I joined Google back in 2014, I met
[00:37:48.700 --> 00:37:55.980]   Raziel Alvarez, who's now actually at Facebook doing some very similar stuff, I believe,
[00:37:55.980 --> 00:38:01.980]   but he was responsible for a lot of the OKG work.
[00:38:01.980 --> 00:38:08.980]   And they had been shipping, they've been shipping on billions of phones using ML and specifically
[00:38:08.980 --> 00:38:12.060]   using deep learning to do this kind of recognition.
[00:38:12.060 --> 00:38:19.940]   But I had no idea that they were shipping these 30 kilobyte models to do ML, and they
[00:38:19.940 --> 00:38:22.740]   had been for years.
[00:38:22.740 --> 00:38:26.140]   And from my understanding, from what I've seen of Apple and other companies, they've
[00:38:26.140 --> 00:38:31.740]   been using very similar approaches in the speech world for a long time.
[00:38:31.740 --> 00:38:37.900]   But a lot of these areas don't have the same kind of expectation that you will publish
[00:38:37.900 --> 00:38:44.980]   and publicize work that we tend to in the modern ML world.
[00:38:44.980 --> 00:38:47.380]   So it flies below the radar.
[00:38:47.380 --> 00:38:55.420]   But yeah, there's ML models that will be running in your house almost certainly right now that
[00:38:55.420 --> 00:38:57.540]   are running on embedded hardware.
[00:38:57.540 --> 00:39:01.220]   And I guess besides the audio recognition, what might those ML models in my house be
[00:39:01.220 --> 00:39:02.220]   doing?
[00:39:02.220 --> 00:39:04.620]   Can you give me a little bit of a flavor for that?
[00:39:04.620 --> 00:39:05.620]   Yeah.
[00:39:05.620 --> 00:39:13.820]   So accelerometer recognition, like trying to tell if somebody is doing a gesture or
[00:39:13.820 --> 00:39:20.940]   if a piece of machinery is doing what you're expecting, like the washing machine or the
[00:39:20.940 --> 00:39:27.580]   dishwasher or things like that, trying to actually take in these signals from noisy
[00:39:27.580 --> 00:39:31.860]   sensors and actually try and tell what's actually happening.
[00:39:31.860 --> 00:39:37.700]   Would you use an ML model in my washing machine?
[00:39:37.700 --> 00:39:41.820]   I would not be at all surprised.
[00:39:41.820 --> 00:39:42.820]   Wow.
[00:39:42.820 --> 00:39:43.820]   Yeah.
[00:39:43.820 --> 00:39:52.580]   I guess another question that I had for you thinking about your long tenure on TensorFlow,
[00:39:52.580 --> 00:40:00.380]   which is such a well-known library, is how has that evolved over the time you've been
[00:40:00.380 --> 00:40:01.380]   there?
[00:40:01.380 --> 00:40:04.820]   Have things surprised you in the directions that it's taken?
[00:40:04.820 --> 00:40:11.740]   How do you even think about, with a project like that, what to prioritize into the future?
[00:40:11.740 --> 00:40:25.060]   Honestly, how big TensorFlow got and how fast really blew me away.
[00:40:25.060 --> 00:40:27.380]   That was kind of amazing to see.
[00:40:27.380 --> 00:40:36.940]   I'm used to working on these weird technical problems that I find interesting and following
[00:40:36.940 --> 00:40:39.940]   my curiosity.
[00:40:39.940 --> 00:40:48.260]   I'd been led to TensorFlow by pulling on a piece of yarn and ending up there.
[00:40:48.260 --> 00:40:59.100]   It was really nice to see, not just TensorFlow, but PyTorch, MXNet, all of these other frameworks.
[00:40:59.100 --> 00:41:05.540]   There's been this explosion in the number of people interested, and especially there's
[00:41:05.540 --> 00:41:10.020]   been this explosion in the number of products that have been shipping.
[00:41:10.020 --> 00:41:17.820]   The number of use cases that people have found for these has been really mind-blowing.
[00:41:17.820 --> 00:41:25.220]   I'm used to doing open-source projects which get 10 stars or something, and I'm happy.
[00:41:25.220 --> 00:41:36.900]   But seeing TensorFlow and all these other frameworks get this mass adoption has been...
[00:41:36.900 --> 00:41:47.700]   Yeah, I think it definitely surprised me and has been really nice to see.
[00:41:47.700 --> 00:41:49.740]   What about in terms of what it does?
[00:41:49.740 --> 00:41:51.020]   How has that evolved?
[00:41:51.020 --> 00:41:55.140]   What kinds of new functionality gets added to a library like that?
[00:41:55.140 --> 00:42:00.860]   Why do you make so many breaking changes?
[00:42:00.860 --> 00:42:02.940]   Yes.
[00:42:02.940 --> 00:42:07.900]   I would just like to say I am sorry.
[00:42:07.900 --> 00:42:23.940]   No, it's really such a really interesting problem because we're almost coming back to
[00:42:23.940 --> 00:42:35.660]   what we were talking about with Alex Kraszewski, the classic example of the ML paradigm that
[00:42:35.660 --> 00:42:42.620]   we're in at the moment is you need a lot of flexibility to be able to experiment and create
[00:42:42.620 --> 00:42:47.020]   models and iterate on new approaches.
[00:42:47.020 --> 00:42:50.980]   But all of the approaches need to run really, really, really, really fast because you're
[00:42:50.980 --> 00:43:02.140]   running millions of iterations, millions of data points through each run just in order
[00:43:02.140 --> 00:43:04.300]   to try out one model.
[00:43:04.300 --> 00:43:11.660]   So you've got this really challenging combination of you need all this flexibility, but you
[00:43:11.660 --> 00:43:17.660]   also need this cutting edge performance and you're trying to squeeze out the absolute
[00:43:17.660 --> 00:43:26.260]   maximum amount of throughput you can out of the hardware that you have.
[00:43:26.260 --> 00:43:34.900]   And so you end up with this world where you have Python crawling into these chunks of
[00:43:34.900 --> 00:43:40.180]   these operators or these layers where the actual operators and layers themselves are
[00:43:40.180 --> 00:43:47.340]   highly, highly optimized, but you're expecting to be able to plug them in to each other in
[00:43:47.340 --> 00:43:56.380]   very arbitrary ways and preserve that high performance.
[00:43:56.380 --> 00:44:04.940]   And especially with TensorFlow, you're also expecting to be able to do it across multiple
[00:44:04.940 --> 00:44:20.180]   accelerator targets, things like the TPU, CPUs, and AMD, as well as NVIDIA GPUs.
[00:44:20.180 --> 00:44:25.660]   And honestly, it's just a really hard engineering problem.
[00:44:25.660 --> 00:44:36.300]   It's been a couple of years now since I've been on the mainline TensorFlow team, and
[00:44:36.300 --> 00:44:44.060]   it blew my mind how many dimensions and combinations and permutations and things they had to worry
[00:44:44.060 --> 00:44:53.740]   about in terms of getting this stuff just up and running and working well for people.
[00:44:53.740 --> 00:45:00.820]   And it is tough as a user because you've got this space shuttle control panel, a lot of
[00:45:00.820 --> 00:45:07.860]   complexity and you probably only want to use part of it, but everybody wants a different
[00:45:07.860 --> 00:45:08.860]   one.
[00:45:08.860 --> 00:45:09.860]   Right, right.
[00:45:09.860 --> 00:45:17.540]   Well, maybe this is a naive question, but when I look at the CUDA library, it looks
[00:45:17.540 --> 00:45:21.740]   pretty close to the TensorFlow wrapper.
[00:45:21.740 --> 00:45:22.740]   Is that right?
[00:45:22.740 --> 00:45:27.620]   I mean, it seems like it kind of tries to do the same building blocks that TensorFlow
[00:45:27.620 --> 00:45:28.620]   has.
[00:45:28.620 --> 00:45:33.420]   So I would think with NVIDIA, it'd be a lot of just passing information down into CUDA.
[00:45:33.420 --> 00:45:34.420]   Yeah.
[00:45:34.420 --> 00:45:43.340]   I mean, where I saw a lot of complexity was around things like the networking and the
[00:45:43.340 --> 00:45:55.060]   distribution and the very fast making sure that you didn't end up getting bottlenecked
[00:45:55.060 --> 00:46:01.420]   on data transfer as you're kind of like shuttling stuff around.
[00:46:01.420 --> 00:46:08.420]   And we've had to go in and mess around with JPEG encoding and try different libraries
[00:46:08.420 --> 00:46:11.860]   to figure out which one would be faster because that starts to become the bottleneck at some
[00:46:11.860 --> 00:46:18.940]   point when you're throwing stuff onto the GPU fast enough.
[00:46:18.940 --> 00:46:24.980]   And I have to admit, though, I've looked at that code in wonder.
[00:46:24.980 --> 00:46:31.380]   I have not tried to fix issues there.
[00:46:31.380 --> 00:46:32.380]   Amazing.
[00:46:32.380 --> 00:46:35.340]   I guess one more question on the topic.
[00:46:35.340 --> 00:46:37.860]   How do you test all these hardware environments?
[00:46:37.860 --> 00:46:42.300]   Do you have to set up the hardware somewhere to run all these things before you ship the
[00:46:42.300 --> 00:46:43.300]   library?
[00:46:43.300 --> 00:46:48.380]   Well, that's another pretty...
[00:46:48.380 --> 00:46:54.780]   The task of doing kind of the continuous integration and the testing across all of these different
[00:46:54.780 --> 00:47:01.860]   pieces of hardware and all the different combinations of, oh, if you've got two cards in your machine,
[00:47:01.860 --> 00:47:09.140]   if you've got four, if you've got this version of Linux, are you running on Windows?
[00:47:09.140 --> 00:47:13.020]   Which versions of the drivers do you have?
[00:47:13.020 --> 00:47:17.900]   And which versions of the accelerator, the 2DNN?
[00:47:17.900 --> 00:47:20.580]   And all of these...
[00:47:20.580 --> 00:47:31.580]   There are farms full of these machines where we're trying to test all of these different
[00:47:31.580 --> 00:47:37.780]   combinations and permutations, or as many as we can, to try and actually make sure that
[00:47:37.780 --> 00:47:40.420]   stuff works.
[00:47:40.420 --> 00:47:44.700]   And as you can imagine, it's not a straightforward task.
[00:47:44.700 --> 00:47:46.020]   All right.
[00:47:46.020 --> 00:47:49.900]   Well, we're getting close to time, and we always end with two questions that I want
[00:47:49.900 --> 00:47:51.980]   to save time for.
[00:47:51.980 --> 00:47:56.660]   One question is, what is an underrated topic in machine learning that you would like to
[00:47:56.660 --> 00:47:59.580]   investigate if you had some extra time?
[00:47:59.580 --> 00:48:02.980]   Oh, well, datasets.
[00:48:02.980 --> 00:48:07.820]   The common theme that I've seen throughout all the time I've worked with, I've ended
[00:48:07.820 --> 00:48:14.300]   up working with hundreds of teams who are creating products using machine learning,
[00:48:14.300 --> 00:48:22.020]   and almost always what they find is that investing time in improving their datasets is a much
[00:48:22.020 --> 00:48:28.580]   better return on investment than trying to tweak their architectures or hyperparameters
[00:48:28.580 --> 00:48:31.220]   or things like that.
[00:48:31.220 --> 00:48:40.300]   And there are very few tools out there for actually doing useful things with datasets,
[00:48:40.300 --> 00:48:47.260]   and improving datasets, and understanding datasets, and gathering dataset data points
[00:48:47.260 --> 00:48:51.180]   and cleaning up labels.
[00:48:51.180 --> 00:48:57.420]   So I really think, and I'm starting to see, I think Andrew Ong and some other people have
[00:48:57.420 --> 00:49:05.100]   been talking about data-centric approaches, and I'm starting to see more focus on that.
[00:49:05.100 --> 00:49:10.900]   But I think that that's going to just continue, and it's going to be...
[00:49:10.900 --> 00:49:16.020]   I feel like as the ML world is maturing, and more people are going through that experience
[00:49:16.020 --> 00:49:21.540]   of trying to put a product out and realizing, "Oh my God, we need better data tools," there's
[00:49:21.540 --> 00:49:25.780]   going to be way more demand and way more focus on that.
[00:49:25.780 --> 00:49:31.260]   So that is an extremely interesting area for me.
[00:49:31.260 --> 00:49:35.940]   Well, you may have answered my last question, but I think you're more qualified to answer
[00:49:35.940 --> 00:49:40.740]   it having done a bunch of ML startups and then working on TensorFlow.
[00:49:40.740 --> 00:49:45.740]   When you think about deploying an ML model in the real world and getting it to work for
[00:49:45.740 --> 00:49:50.660]   a useful purpose, what do you see as the major bottlenecks?
[00:49:50.660 --> 00:49:55.500]   I guess datasets is one, I agree, is maybe the biggest one, but do you see other stuff
[00:49:55.500 --> 00:49:56.820]   happening?
[00:49:56.820 --> 00:50:05.620]   So another big problem is there's this kind of artificial distinction between the people
[00:50:05.620 --> 00:50:11.300]   who create models, who often come from a research background, and the people who have to deploy
[00:50:11.300 --> 00:50:12.900]   them.
[00:50:12.900 --> 00:50:21.220]   And what will often happen is that the model creation people will get as far as getting
[00:50:21.220 --> 00:50:30.100]   an eval that shows that their model is reaching a certain level of accuracy in their Python
[00:50:30.100 --> 00:50:31.100]   environment.
[00:50:31.100 --> 00:50:34.900]   And they'll say, "Okay, I'm done.
[00:50:34.900 --> 00:50:40.140]   Here's the checkpoints for this model, which is great," and then just hand that over to
[00:50:40.140 --> 00:50:45.460]   the people who are going to deploy it on an Android application.
[00:50:45.460 --> 00:50:53.140]   And the problem there is that there's all sorts of things, like the actual data in the
[00:50:53.140 --> 00:50:58.820]   application itself may be quite different to the training data.
[00:50:58.820 --> 00:51:02.900]   You're almost certainly going to have to do some stuff to it, like quantization or some
[00:51:02.900 --> 00:51:08.260]   kind of thing that involves retraining in order to have something that's optimal for
[00:51:08.260 --> 00:51:12.180]   the device that you're actually shipping on.
[00:51:12.180 --> 00:51:17.020]   And there's just a lot of really useful feedback that you can get from trying this out in a
[00:51:17.020 --> 00:51:21.660]   real device that someone can hold in their hand and use that you just don't get from
[00:51:21.660 --> 00:51:25.860]   the eval use case.
[00:51:25.860 --> 00:51:32.660]   So coming back to actually Pete Scomerock, and I first met him when he was part of the
[00:51:32.660 --> 00:51:41.540]   whole DJ Patil and the LinkedIn crew doing some of the really early data science stuff.
[00:51:41.540 --> 00:51:53.260]   They had this idea, and I think it was DJ who came up with the naming of data science
[00:51:53.260 --> 00:52:00.340]   and data scientists as somebody who would own the full stack of taking everything from
[00:52:00.340 --> 00:52:07.860]   doing the data analysis to coming up with models and things on it to actually deploying
[00:52:07.860 --> 00:52:13.980]   those on the website and then kind of like taking ownership of that whole end to end
[00:52:13.980 --> 00:52:16.100]   process.
[00:52:16.100 --> 00:52:20.180]   And the teams I've seen been really successful at deploying ML products.
[00:52:20.180 --> 00:52:26.100]   They've had people who formally or informally have taken on that responsibility for the
[00:52:26.100 --> 00:52:30.780]   whole thing and kind of have the people who are writing the inner loops of the assembly
[00:52:30.780 --> 00:52:36.500]   kind of sitting next to the people who are doing the, you know, creating the models.
[00:52:36.500 --> 00:52:41.780]   And the team who created MobileNet, Mobile Vision with Andrew Howard and Von-Wart Jacob,
[00:52:41.780 --> 00:52:43.940]   they're a great example of that.
[00:52:43.940 --> 00:52:48.860]   They all work very, very closely together doing everything from coming up with new model
[00:52:48.860 --> 00:52:55.660]   techniques to figuring out how they're actually going to run on real hardware at the really
[00:52:55.660 --> 00:52:57.940]   low level.
[00:52:57.940 --> 00:53:03.500]   So that's one of the biggest things I'm really hoping to see change in the next few years
[00:53:03.500 --> 00:53:06.700]   is more people kind of adopt that model.
[00:53:06.700 --> 00:53:07.700]   Well said.
[00:53:07.700 --> 00:53:08.700]   Thanks so much, Pete.
[00:53:08.700 --> 00:53:09.700]   That was super fun.
[00:53:09.700 --> 00:53:10.700]   No, thanks Lucas.
[00:53:10.700 --> 00:53:16.580]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:53:16.580 --> 00:53:21.340]   to the show notes in the description where you can find links to all the papers that
[00:53:21.340 --> 00:53:25.580]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:53:25.580 --> 00:53:26.580]   produce.
[00:53:26.580 --> 00:53:26.860]   So check it out.
[00:53:26.860 --> 00:53:28.700]   (upbeat music)

