
[00:00:00.000 --> 00:00:12.500]   Okay for those aren't familiar with me i'm andrew for a work at google cloud i am in developer relations that means i'm primarily external facing.
[00:00:12.500 --> 00:00:25.500]   So you'll see me at events like that sometimes i'm at universities as an adjunct professor other times i'm with some of our very large enterprise clients.
[00:00:25.700 --> 00:00:31.100]   You know helping other teams get over a production problems.
[00:00:31.100 --> 00:00:47.840]   But because i'm external facing and i'm out there a lot i had the opportunity over the last few years really just see first hand what's been transitioning in production at the very largest places.
[00:00:49.500 --> 00:00:55.900]   So i'm going to start here with a little timeline.
[00:00:55.900 --> 00:00:59.900]   Just a quick note the bottom of your slides i think are slightly cut off.
[00:00:59.900 --> 00:01:05.900]   Yeah i hear on some presentations that happens so i'm going to go out of full mode and just go like this.
[00:01:05.900 --> 00:01:09.900]   Okay that way nothing gets cut off right.
[00:01:09.900 --> 00:01:11.900]   Great yeah.
[00:01:11.900 --> 00:01:24.400]   Alright so you know when i started you know in production was like early two thousand seventeen and the marketing data back then said there was about ten thousand data scientists.
[00:01:24.400 --> 00:01:25.400]   Okay.
[00:01:25.400 --> 00:01:43.900]   And AI and ML was just coming on everybody's radar and most of the people involved in the big companies you know you had decision makers engineering staff but they really lack having you know really data science or MLE people.
[00:01:43.900 --> 00:01:47.900]   So most of that workforce was being done as an ML consultant.
[00:01:47.900 --> 00:01:48.900]   Okay.
[00:01:48.900 --> 00:02:02.400]   By two thousand eighteen lot of these companies were moving away from planning and discovery to exploration and prototyping and so now you started to see senior engineering management involved.
[00:02:02.400 --> 00:02:12.400]   Typically by now at least one senior data scientist has been hired as a lead and they're starting to fill in the team you know what data engineers.
[00:02:12.400 --> 00:02:22.900]   I started to notice that they tended to pull people from the DB the database team okay and again as a strong dependence on ML consultant.
[00:02:22.900 --> 00:02:32.900]   Where we really saw the change i was in two thousand and nineteen by now you know a i m l is a fundamental aspect of their business.
[00:02:32.900 --> 00:02:44.400]   You know the CTO is involved you now have really rounded out the data science team hiring you know junior data scientists to support the senior data scientists.
[00:02:44.400 --> 00:02:58.400]   You have machine learning engineers data engineers you're starting to bring in ML ops and we also saw that the start using third party services like ML API services and turnkey.
[00:02:58.400 --> 00:03:04.400]   What turnkey solutions were emerging at the time from integrated system vendors.
[00:03:04.400 --> 00:03:10.900]   By two thousand and twenty you know which is the current or at least before the pandemic.
[00:03:10.900 --> 00:03:29.900]   Really at the enterprise level the entire C suite is involved and you just see you know again a continued growth in the number of people hired into these positions and you also see a lot more use of what we call now managed services.
[00:03:29.900 --> 00:03:48.900]   Those like come not just you know obviously google provides managed services but there are a lot of very large companies that do and then really a big growth in turnkey solutions and also the labor pool is changed a lot we've gone from ten thousand data scientists three years ago.
[00:03:48.900 --> 00:03:57.900]   To now two hundred and fifty thousand data scientists worldwide and two million people who are characterized as ML practitioners.
[00:03:57.900 --> 00:04:02.900]   So it's been quite an explosion in the workforce.
[00:04:02.900 --> 00:04:07.900]   So we'll just quickly go over sort of the transition in the technology.
[00:04:07.900 --> 00:04:18.900]   OK so again when I started in two thousand and seventeen I'm going to our clients and we're talking plant they're talking planning and discovery.
[00:04:18.900 --> 00:04:34.900]   They were still doing what I would call business intelligence sort of that classical form of machine learning they weren't doing deep learning the tools they tended to be using were psychic learn and now.
[00:04:34.900 --> 00:04:44.900]   TK toolkit typically they were doing cart analysis and the background of many of the people involved was a statistics background.
[00:04:44.900 --> 00:05:04.900]   Two thousand and eighteen is really when we started to see a big change starting to move towards deep learning starting to see bringing in tensorflow and I torch start using things like transfer learning again starting to use third party services like ML API's managed training services.
[00:05:04.900 --> 00:05:14.900]   And then the backgrounds really started to expand of the people involved went from stats and now people who are computer science and data backgrounds.
[00:05:14.900 --> 00:05:26.900]   Two thousand and nineteen continue to expand we started to see more and more reinforcement learning coming in more and more different ways of doing automatic learning.
[00:05:26.900 --> 00:05:43.900]   Built in algorithms hyper parameter search and we started to see the use of a lesser expensive version of network architectures search called macro architecture search disorder find new architectures to use for training a model.
[00:05:43.900 --> 00:05:54.900]   And again bringing in managed end to end services turnkey services and you know the backgrounds really you know start expanding include operational people.
[00:05:54.900 --> 00:06:04.900]   What we saw or predicted in two thousand and twenty is companies move into full production is really building on the data validation side.
[00:06:04.900 --> 00:06:10.900]   Okay and on the other end we're really starting to see an explosion what we call automatic learning.
[00:06:10.900 --> 00:06:13.900]   Okay.
[00:06:13.900 --> 00:06:16.900]   Just click this way.
[00:06:16.900 --> 00:06:23.900]   So here in twenty twenty what we see as far as roles at the enterprise level.
[00:06:23.900 --> 00:06:31.900]   They tend to be split across two organizations one of them we typically refer to as the innovation center and the other one is production.
[00:06:31.900 --> 00:06:40.900]   We see our research data scientists in the innovation center are applied data scientists the more senior ones tend to straddle the two.
[00:06:40.900 --> 00:06:43.900]   Some of them are in innovation some of them are production.
[00:06:43.900 --> 00:06:54.900]   We also see that with the machine learning engineers but then on the production side you know we additionally see the machine learning operations we call it ml ops.
[00:06:54.900 --> 00:07:05.900]   We see our data engineers and we're also seeing a new type of software engineer that we refer to as an application engineer.
[00:07:05.900 --> 00:07:19.900]   For the most part they're doing their job the same way they always have you know with the same practices and processes but the packages the libraries they're using now are a libraries.
[00:07:19.900 --> 00:07:24.900]   And the other thing they have to get used to is historically.
[00:07:24.900 --> 00:07:32.900]   You know algorithms always gave a you can say a single answer two plus two was always four.
[00:07:32.900 --> 00:07:39.900]   And now they have to learn how do I deal with an algorithm whose output is really a probability distribution.
[00:07:39.900 --> 00:07:55.900]   And that's also put into question how QA fits in again QA has always been built on this discrete world you know that there's every set of inputs has a certain exact output and you want to get state 100 percent of your tests correct.
[00:07:55.900 --> 00:08:11.900]   Well what's the correct output. OK. And so there's still uncertainty exactly how QA works in a world where our applications now work on probability distributions.
[00:08:11.900 --> 00:08:20.900]   So this is just kind of showing at the end of 2019 what was a typical production flow. OK.
[00:08:20.900 --> 00:08:37.900]   So again you would have some kind of data warehousing here I represent as a data repo your data engineers have designed some system to distribute data on a regular or you say recurrent basis for model training at a scale.
[00:08:37.900 --> 00:08:54.900]   We're not training one model anymore. Organization may be training vast number of different models but even within that one model they will train multiple instances to find out which one will produce the best result.
[00:08:54.900 --> 00:09:09.900]   And even within an instance we have things called warm up training where we do numerical stabilization in the model and we might have sort of short train multiple short trainings to get each one of these ready for full training.
[00:09:09.900 --> 00:09:14.900]   And so there are just massive number of model train.
[00:09:14.900 --> 00:09:20.900]   So once you got an instance of a model train there's usually some kind of validation check.
[00:09:20.900 --> 00:09:32.900]   Typically the organization now has a repository for their train models where they can do you know tracking the same way as source control but now it's for models.
[00:09:32.900 --> 00:09:45.900]   So they're able to go to the repository in their internal evaluation answer the question is this instance of the model better than the previous one and if not I just repeat this process.
[00:09:45.900 --> 00:09:50.900]   But if it is I'm going to go ahead and version control it and put it into the repo.
[00:09:50.900 --> 00:10:00.900]   Then finally we're going to deploy it but even at that point we really don't know that the model is actually going to be better when we put it in the wild.
[00:10:00.900 --> 00:10:06.900]   We made that determination initially based on our value date validation data.
[00:10:06.900 --> 00:10:24.900]   So we end up deploying it and they typically do some form of a B testing that is they have the original version out there the new version some people see their original some people see the new version and they have some metric where they can measure how well it's performing.
[00:10:24.900 --> 00:10:38.900]   And based on that they're going to get insights which leads to more data labeling more data is added to the data warehouse and then the cycle is just repeating it goes on continuous.
[00:10:38.900 --> 00:10:59.900]   So, you know a couple years ago and I'd be talking to people in production at these companies you know how often they retrain a model typically I heard 30 days, you know, by the end of 2019 you know a normal retraining cycle was, you know, every week seven days.
[00:10:59.900 --> 00:11:06.900]   And then in other cases people are retraining literally every single day.
[00:11:06.900 --> 00:11:11.900]   I'll pause for a moment somebody has a question.
[00:11:11.900 --> 00:11:14.900]   Yeah, I actually have a couple ones.
[00:11:14.900 --> 00:11:19.900]   One I'd like to know actually from that 2019 production flow.
[00:11:19.900 --> 00:11:31.900]   You, what are some of the biggest gaps you've seen between validation performance and performance in that a B testing setting where you, you know, you actually have real real world data and real.
[00:11:31.900 --> 00:11:39.900]   Yeah, the, you know, the biggest problems of course we have are falling into two categories serving skew and data drift.
[00:11:39.900 --> 00:11:48.900]   Okay. And again, since you're all big talking about distributions, you can actually re explain those as distributions but we will.
[00:11:48.900 --> 00:11:52.900]   I came back right had a distribution distribution theories. Okay.
[00:11:52.900 --> 00:12:05.900]   So, for our audience, you don't know what serving skew is so your training data. Let's say it's classes, a classification you have 10 classes and you have a certain number per class percentage.
[00:12:05.900 --> 00:12:11.900]   Okay. And you train it in proportion to that you keep a balance. Okay.
[00:12:11.900 --> 00:12:24.900]   And let's say not let's say a 10 classes and nine of them are highly accurate 98%. And then there's a 10th class, that's not so good. It's more like 70%.
[00:12:24.900 --> 00:12:33.900]   But when you report the average across all 10. It looks really high 94% and you're really happy.
[00:12:33.900 --> 00:12:39.900]   And then you put it out there real world you're only getting 70%. You know you guys and what's going on.
[00:12:39.900 --> 00:12:45.900]   Well, it turns out, almost everything is seizes from that 10th class.
[00:12:45.900 --> 00:12:55.900]   Okay, and that's called a serving skew the distribution of what it's not that what it sees is different than when it's trained on, but the frequency the distribution in the frequency.
[00:12:55.900 --> 00:12:59.900]   Okay, so we call that serving skew that's one problem we have.
[00:12:59.900 --> 00:13:02.900]   Another one we had is data drift.
[00:13:02.900 --> 00:13:12.900]   You just see examples we never trained on there from a different. They may be from the same population, but they're from a different distribution.
[00:13:12.900 --> 00:13:17.900]   Okay, so you have to look at it no matter what I'm training on.
[00:13:17.900 --> 00:13:33.900]   I really don't have every possible example out there. So, let's say you're trying to train a model to predict the shoe size of every male in North America, based on some features.
[00:13:33.900 --> 00:13:41.900]   So the population would be if you had every exact adult male and that number, but you don't.
[00:13:41.900 --> 00:13:50.900]   So you got some subpopulation. Okay. And the question is how represented is that subpopulation that's your statistic.
[00:13:50.900 --> 00:13:54.900]   Okay, of the overall population in general is not.
[00:13:54.900 --> 00:14:02.900]   And so in data drift, your model is seeing a subpopulation of the same population, but it's a different subpopulation.
[00:14:02.900 --> 00:14:09.900]   And that's one of the big things we try to find in A/B testing. And then that gives us insight into two things.
[00:14:09.900 --> 00:14:24.900]   Labeling more data, getting more data, it's more representative, but also making validation slices that represent not just the training data, but the serving skew and the data drift.
[00:14:24.900 --> 00:14:27.900]   Hopefully I didn't take too long answering that question.
[00:14:27.900 --> 00:14:35.900]   Those are great, great answers. And yeah, it's interesting to see that those problems are sort of classical statistics in a lot of ways.
[00:14:35.900 --> 00:14:38.900]   Yeah, they're absolutely classical statistics.
[00:14:38.900 --> 00:14:50.900]   Also, one more question. Before you move on from Jan Chang on Zoom, how would architecture search fit into that production flow? Is that part of it or is that something separate?
[00:14:50.900 --> 00:15:03.900]   Yeah, I see that as something separate. Obviously, the architecture search produced that model that's being trained in that production flow diagram.
[00:15:03.900 --> 00:15:09.900]   And my presentation is six parts. I don't know how far we'll go. I'll have you cut me off when we're done.
[00:15:09.900 --> 00:15:17.900]   But each one's independent. But one of them actually talks about the trends and using those types of techniques in production.
[00:15:17.900 --> 00:15:22.900]   Gotcha. Well, I want to hear as much as possible, so I'm going to let you keep going.
[00:15:22.900 --> 00:15:33.900]   We'll continue on. Okay. Okay. So another big thing that really changed in 2019 is, I call this not just a model anymore.
[00:15:33.900 --> 00:15:52.900]   So up until 2017, 2018, typically we trained individual models. And even if an application used multiple models, they were still separate models.
[00:15:52.900 --> 00:16:05.900]   You tended to have some back-end application on a server, like written in Java or C#, and your models tended to be deployed as a microservice or an HTTP endpoint.
[00:16:05.900 --> 00:16:10.900]   And your back-end application would make calls out to them.
[00:16:10.900 --> 00:16:22.900]   Well, we don't really do it that way. Nowadays, we actually build applications that are a composition or what we call an amalgamation of the models, okay?
[00:16:22.900 --> 00:16:34.900]   That where the models are interconnected to themselves. And also, the ultimate goal is that collection, that amalgamation of the models becomes the entire application.
[00:16:34.900 --> 00:16:41.900]   The whole idea of a back-end application, making these calls out to these models go away.
[00:16:41.900 --> 00:16:48.900]   And what we're finding is these models, as we move towards amalgamations, are becoming more multitask.
[00:16:48.900 --> 00:17:01.900]   They're doing more self or meta learning. They're sharing common layers, and they're connecting between each other via learned embeddings.
[00:17:01.900 --> 00:17:05.900]   So I'm going to step you how we get there.
[00:17:05.900 --> 00:17:13.900]   I'm a computer vision person, so I always put everything in the context of computer vision. That's my expertise.
[00:17:13.900 --> 00:17:21.900]   So today, when we look at models, there are standard design patterns.
[00:17:21.900 --> 00:17:31.900]   And in the convolutional neural network world, we look at them as being broken into three major components, a stem, a learner, and a task.
[00:17:31.900 --> 00:17:40.900]   So the stem is really where the image data is coming in, and we do this first course level generation of feature maps.
[00:17:40.900 --> 00:17:54.900]   It's sort of prepping the system, and one of the important things is, depending on the size of the image, it's going to do some dimensionality reduction,
[00:17:54.900 --> 00:18:01.900]   but it wants to do it in a way that matches what our expectations are when we get to the end of the learner.
[00:18:01.900 --> 00:18:10.900]   Because at the end of the learner is what Dillon referred to as the latent space. We want it to be a certain dimensionality.
[00:18:10.900 --> 00:18:19.900]   And so that's the job of the stem. The learner is what we say now does representational learning.
[00:18:19.900 --> 00:18:25.900]   And what we mean by that is we want to learn the essential features of the data,
[00:18:25.900 --> 00:18:32.900]   which is really kind of different than the past where we said we wanted to learn the data or the data set.
[00:18:32.900 --> 00:18:41.900]   Well, that approach really led to what we call memorization, where you really just end up memorizing the data.
[00:18:41.900 --> 00:18:48.900]   And what we really want to do is, quote, not learn the data, but learn what are the essential features that make up the data.
[00:18:48.900 --> 00:18:54.900]   And that's the only thing we want in the latent space. So again, that's not really very different than Dillon.
[00:18:54.900 --> 00:19:08.900]   Sometimes we do pre-training on these models using an encoder portion of an auto encoder or a GAN or synthetic data just to force the model to sort of initially,
[00:19:08.900 --> 00:19:13.900]   so we call, you know, set itself on learning only essential features.
[00:19:13.900 --> 00:19:21.900]   So that's the learner. And then finally, the task. Now that I got this latent space, what is the task I want to learn?
[00:19:21.900 --> 00:19:27.900]   It could be something simple as classification. It could be multitask as an object detection.
[00:19:27.900 --> 00:19:40.900]   You're learning two classes, two tasks. You're learning a bounding box and you're learning to classify what's inside that bounding box.
[00:19:40.900 --> 00:19:43.900]   So, yeah, I brought up object detection.
[00:19:43.900 --> 00:19:53.900]   OK, so this is just to remind people that today models are multi, many models nowadays in production are multitask.
[00:19:53.900 --> 00:19:58.900]   I consider object detection sort of the granddaddy of them all.
[00:19:58.900 --> 00:20:07.900]   OK, so you start your STEM convolutional group and guess those original feature maps ready at the right size, right dimensionality.
[00:20:07.900 --> 00:20:14.900]   It's got that coarse level feature extraction. Then you're going to have this convolutional body or net.
[00:20:14.900 --> 00:20:18.900]   And that's your reuse because you're going to reuse it for multiple purposes.
[00:20:18.900 --> 00:20:24.900]   OK, sometimes they refer to the output of that as the shared feature maps.
[00:20:24.900 --> 00:20:34.900]   And so there's various things you want to do here. So in an RPN is you want to propose regions within the image that might be an object.
[00:20:34.900 --> 00:20:40.900]   So these are like your candidate bounding boxes. And then you want to pull them together.
[00:20:40.900 --> 00:20:46.900]   Ones that are overlapping and so forth to sort of a final, smaller, final set of candidates.
[00:20:46.900 --> 00:20:52.900]   And then you're going to classify each one of those. Is this a bounding box, a foreground or a background?
[00:20:52.900 --> 00:20:58.900]   And if it's a foreground, then you want to classify what's in it.
[00:20:58.900 --> 00:21:03.900]   And on top of that, you want to then fine tune the bounding box around it.
[00:21:03.900 --> 00:21:12.900]   So here you have a classifier and a regressor. OK, so let's kind of move to full scale model amalgamation.
[00:21:12.900 --> 00:21:20.900]   So one of the areas of enterprise customers I've worked with for Google is in sports broadcasting.
[00:21:20.900 --> 00:21:28.900]   And this is the kind of amalgamation that we we help them with. So obviously it's sports and they have some live video.
[00:21:28.900 --> 00:21:33.900]   And so that's going into a shared convolutional neural network.
[00:21:33.900 --> 00:21:41.900]   You got shared layers and that shared layers, the outputs from them go into an object detection model.
[00:21:41.900 --> 00:21:46.900]   OK, so that gives us a chance to say what's in that frame.
[00:21:46.900 --> 00:21:58.900]   So you have everything from detecting a person, detecting a bat, detecting a ball, detecting audience in the stadium and so forth.
[00:21:58.900 --> 00:22:02.900]   So there's all your object detection. But these are also embeddings.
[00:22:02.900 --> 00:22:14.900]   These are those little latent spaces. OK, and what we can do is we can use their location in the original image to crop them out of the latent space.
[00:22:14.900 --> 00:22:20.900]   So it's not the entire latent space, just a little crop outs out of them. We call them object level embeddings.
[00:22:20.900 --> 00:22:26.900]   We can take those object level embeddings and identify the ones that are classified as a person.
[00:22:26.900 --> 00:22:30.900]   Past that embedding in the model is not trained on the original image.
[00:22:30.900 --> 00:22:37.900]   It is trained on the object embedding and the output of this to recognize the players.
[00:22:37.900 --> 00:22:48.900]   OK, and if it's a player, then to take that information, those same embeddings to another model trained on those beddings to do pose estimation.
[00:22:48.900 --> 00:22:54.900]   So it might say, you know, realize the player is standing in a batting position.
[00:22:54.900 --> 00:22:59.900]   So now look at all the things I have. I have the information about who, what player it is.
[00:22:59.900 --> 00:23:04.900]   I know his pose. I have all these object embeddings. I put them together.
[00:23:04.900 --> 00:23:13.900]   We now have a dense embedding and we can put that into another model trained on that output to predict the action.
[00:23:13.900 --> 00:23:22.900]   For example, you know, again, context of baseball player at the, you know, at the mound ready to bat.
[00:23:22.900 --> 00:23:33.900]   OK, because we can predict that action, we can take that predicted action into an image captioning model, produce the closed caption text.
[00:23:33.900 --> 00:23:38.900]   Today, broadcasters send their, you know, the sports games all over the world.
[00:23:38.900 --> 00:23:43.900]   So you get that automatic translation into the particular market.
[00:23:43.900 --> 00:23:53.900]   OK, and then for those who are, say, visual or or maybe it's radio or something, they're able to take that and go text to speech.
[00:23:53.900 --> 00:23:59.900]   The important thing here is this whole process of models. There's no back end application.
[00:23:59.900 --> 00:24:07.900]   This entire amalgamation of models is the application.
[00:24:07.900 --> 00:24:12.900]   OK, just got three more slides on this section and the last questions.
[00:24:12.900 --> 00:24:22.900]   Another area we work on, I call it model fusion because it has some similarities to the autonomous world with sensor fusion.
[00:24:22.900 --> 00:24:26.900]   OK, so we work with utility companies. OK.
[00:24:26.900 --> 00:24:31.900]   And of course, they got transmission, the big giant transmission lines.
[00:24:31.900 --> 00:24:44.900]   Historically, a long time ago for maintenance purposes, they actually would fly periodically a helicopter out, you know, into the mountains where all these big transmission towers are.
[00:24:44.900 --> 00:24:48.900]   There'd be a three person crew. You'd have the pilot.
[00:24:48.900 --> 00:24:56.900]   You have some guy with binoculars looking out, staring at these poles, trying to spot defects or problems.
[00:24:56.900 --> 00:25:05.900]   And then a third person taking those highly expensive. OK, well, eventually they trained drones to do the flying.
[00:25:05.900 --> 00:25:11.900]   But you had two people in back. Otherwise, you know, visually observing against before we had a model.
[00:25:11.900 --> 00:25:16.900]   But you got rid of the high cost of the helicopter and the pilot.
[00:25:16.900 --> 00:25:22.900]   Then eventually, of course, with deep learning, they were able to train object detection models.
[00:25:22.900 --> 00:25:28.900]   OK, that could classify, you know, what it saw there for defects or for maintenance.
[00:25:28.900 --> 00:25:36.900]   And this was a dramatic reduction in cost and is fairly standard throughout the whole industry.
[00:25:36.900 --> 00:25:42.900]   But the thing was, is that there's an older technology on every one of these transmission poles.
[00:25:42.900 --> 00:25:53.900]   OK, when there's a problem, a break or something just not right. OK, it changes the impedance in the line.
[00:25:53.900 --> 00:26:05.900]   And so there's these impedance sensors, this old IOT technology that is continuously transmitting information back to the utility company.
[00:26:05.900 --> 00:26:14.900]   The problem is, is reliability of these sensors, because that impedance value changes in it.
[00:26:14.900 --> 00:26:23.900]   What it means can be caused by, can be affected by things simply by climate and other things.
[00:26:23.900 --> 00:26:29.900]   OK, so you can't use like one set of numbers and say this number always means that.
[00:26:29.900 --> 00:26:37.900]   But it turns out that since they're flying these and these are now already trained and automatically classifying,
[00:26:37.900 --> 00:26:45.900]   the output of these guys now become the labels for this old school IOT technology.
[00:26:45.900 --> 00:26:51.900]   And they're able to train an anomaly detection model with high accuracy,
[00:26:51.900 --> 00:27:06.900]   with these low cost technology using this drone who's been otherwise machine learned to be the labeler for it, further reducing the cost.
[00:27:06.900 --> 00:27:15.900]   So another area I've worked with several large Internet companies whose business has something to do with homes.
[00:27:15.900 --> 00:27:21.900]   Anything from selling them, renting them to renovating them.
[00:27:21.900 --> 00:27:27.900]   OK, and we have one basic pipeline amalgamation that works for all of them.
[00:27:27.900 --> 00:27:38.900]   OK. And the basic premise here is they'll have a website and you as a user, you're going to upload pictures of your home.
[00:27:38.900 --> 00:27:48.900]   OK, so the first step is to have a model, a simple binary classifier model that determines is that picture an interior or an exterior.
[00:27:48.900 --> 00:27:51.900]   And for the moment, I'm going to skip what's inside here.
[00:27:51.900 --> 00:27:55.900]   It's in the next slide, but they're practically a mirror of each other.
[00:27:55.900 --> 00:28:03.900]   But the output of whatever this amalgamation is, that output, that embedding goes into three back end models.
[00:28:03.900 --> 00:28:12.900]   OK, one's going to classify as a multi-class classifier and it's going to classify the market appeal per demographic.
[00:28:12.900 --> 00:28:26.900]   Another model take that same information, the same latent space or embedding, be a regressor and come up with a valuation such as how much the house would sell for or how much it should rent for.
[00:28:26.900 --> 00:28:30.900]   And then another regressor for renovation, like in repairs.
[00:28:30.900 --> 00:28:36.900]   So let's kind of look at the interior of what's inside of this that makes the amalgamation.
[00:28:36.900 --> 00:28:43.900]   OK, so let's go back here. Coming out of the binary classifier isn't just a yes or no interior or exterior.
[00:28:43.900 --> 00:28:49.900]   Remember, I say these are multi-output models that share from different layers.
[00:28:49.900 --> 00:28:56.900]   So you have an embedding. So the embedding coming out of this is now the input to that interior.
[00:28:56.900 --> 00:29:01.900]   So the interior is not seeing the original image anymore, it is seeing that embedding.
[00:29:01.900 --> 00:29:11.900]   This is typically two tier. OK, so first, we want to do a coarse level object detection of what type of room it is, particularly if we're inside the house.
[00:29:11.900 --> 00:29:15.900]   Is it a living room? Is it a dining room? Is it a kitchen?
[00:29:15.900 --> 00:29:23.900]   And then for that room, another object detection on a detail level, what items in the room.
[00:29:23.900 --> 00:29:31.900]   So, you know, if you're talking about a kitchen, you might be talking about, you know, the stove, the refrigerator or the table in the bathroom.
[00:29:31.900 --> 00:29:37.900]   You might be talking about the shower, the toilet, etc. Well, those are our anemones.
[00:29:37.900 --> 00:29:44.900]   OK, and again, it's object detection. It knows the location in the embedding where it is.
[00:29:44.900 --> 00:29:49.900]   It can crop it out, which spatially maps back to the original image.
[00:29:49.900 --> 00:29:53.900]   We've got all these tiny crop embeddings that are your anemones.
[00:29:53.900 --> 00:30:05.900]   At the same time, we take that same information from that embedding and train a multiclass classifier on a per room basis to classify the overall condition.
[00:30:05.900 --> 00:30:15.900]   OK, now you take that condition or first you take these embeddings and then on us from them, just like we did here at a coarse level.
[00:30:15.900 --> 00:30:22.900]   At a fine level, we have three models. One is classifying the condition on individual anemone.
[00:30:22.900 --> 00:30:29.900]   Another one, the market appeal. Another one, the valuation. Collectively, all these anemones come out.
[00:30:29.900 --> 00:30:37.900]   Your exit here, along with the overall condition of the room for one big dense embedding.
[00:30:37.900 --> 00:30:44.900]   And that's what came out here that did that final aggregation. So that's an amalgamation.
[00:30:44.900 --> 00:30:49.900]   So I'll pause for a moment for anybody to ask a question.
[00:30:49.900 --> 00:30:53.900]   Yeah, so it's about six o'clock, which is usually when we wrap up.
[00:30:53.900 --> 00:31:00.900]   So I think we should, I would love to actually ask you a couple questions before we go, but I don't think we'll be able to hear any more of your presentation.
[00:31:00.900 --> 00:31:03.900]   OK, that's fine. Go for some questions.
[00:31:03.900 --> 00:31:06.900]   Yeah, so Gary Kuvich on Zoom asked a really great question.
[00:31:06.900 --> 00:31:17.900]   Are there any papers that you can share with us on this concept of amalgamations and maybe architectures and training styles that are best for this style of machine learning?
[00:31:17.900 --> 00:31:25.900]   Amalgamations isn't really something that people in research, really, the direction they would go.
[00:31:25.900 --> 00:31:29.900]   They're thinking theoretical.
[00:31:29.900 --> 00:31:41.900]   So amalgamations are more of just a practice that has evolved with people in production who have to work with a vast number of moving parts.
[00:31:41.900 --> 00:31:45.900]   OK, and trying to move away, trying to make it more efficient.
[00:31:45.900 --> 00:31:51.900]   And the way to make it more efficient is you got to move away from that back end application on a server.
[00:31:51.900 --> 00:31:55.900]   You just somehow have to get rid of that.
[00:31:55.900 --> 00:32:02.900]   Yeah, that makes sense. And I was going to say that in my time in academia, you know, this wasn't really on people's radar.
[00:32:02.900 --> 00:32:12.900]   I mean, the idea of like transfer learning is definitely something people talk about, but it doesn't it feels a little different than this like amalgamation idea that you're talking about.
[00:32:12.900 --> 00:32:18.900]   Yeah, in my deep learning book, I definitely have a significant sections on amalgamation.
[00:32:18.900 --> 00:32:25.900]   It's one of my, you know, everybody at Google Cloud is an expert at, you know, one or more things.
[00:32:25.900 --> 00:32:37.900]   You know, they're all great people. And one of the things I've made myself an expert at is how at the enterprise level companies are doing amalgamations.
[00:32:37.900 --> 00:32:48.900]   Great. I'll make sure to share your link to your book, the early access version of your book on Manning with the attendees on Zoom and YouTube so they can check that out.
[00:32:48.900 --> 00:32:56.900]   Yeah. And yeah, and I'm not sure if you have my YouTube channel for like the first dozen chapters of the book.
[00:32:56.900 --> 00:33:04.900]   I have a YouTube presentation. It's like, you know, the Cliff Notes for the chapter.
[00:33:04.900 --> 00:33:11.900]   Yeah, I'll make sure that gets that gets shared. And also, if you have a link to the slides, if you could send that to me, I can send that out to our attendees.
[00:33:11.900 --> 00:33:13.900]   I can definitely do that, Charles.
[00:33:13.900 --> 00:33:22.900]   Great. Okay. I haven't seen any other questions besides wanting to get access come in.
[00:33:22.900 --> 00:33:28.900]   I had one question that I want to make sure that we that I don't get it that I don't miss my chance to ask.
[00:33:28.900 --> 00:33:37.900]   In some of your earliest slides, you mentioned the in 2017 there were 10,000 data scientists and now there's 250,000 of them.
[00:33:37.900 --> 00:33:43.900]   So what do you think, you know, what do you think is the 25x growth that's going to happen between 2020 and 2023.
[00:33:43.900 --> 00:33:51.900]   Do you think that there's an opportunity for that kind of explosive growth elsewhere in machine learning and data science or do you think it's somewhere else?
[00:33:51.900 --> 00:34:00.900]   I think we're going to as far as the high end definition of a data scientist, I think that's pretty much going to plateau out.
[00:34:00.900 --> 00:34:08.900]   Okay. I think the advancement of these frameworks and tools.
[00:34:08.900 --> 00:34:13.900]   A lot can be accomplished, you know, with more of a machine learning engineer.
[00:34:13.900 --> 00:34:25.900]   Okay. Okay. But I do see as we gradually teach programmers what I call how to program the graph.
[00:34:25.900 --> 00:34:30.900]   It's a different way of thinking. Once they learn to program a graph, they're a machine learning engineer.
[00:34:30.900 --> 00:34:41.900]   Okay. My belief is just a matter of a few years where effectively all application programmers will be programming the graph.
[00:34:41.900 --> 00:34:45.900]   Somewhere on the graph they'll be programming.
[00:34:45.900 --> 00:34:53.900]   We see numbers and marketing data that currently there are about 25 million programmers in the world.
[00:34:53.900 --> 00:34:58.900]   So given that it should be about a tenfold growth.
[00:34:58.900 --> 00:35:01.900]   Interesting. Okay. That's a...
[00:35:01.900 --> 00:35:10.900]   But again, that's my opinionated view of the world that everybody's going to be programming somewhere on the graph.
[00:35:10.900 --> 00:35:14.900]   And amalgamation is really a graph of graphs.
[00:35:14.900 --> 00:35:17.900]   Or subgraph communicating with each other.
[00:35:17.900 --> 00:35:21.900]   Right. Right. So you're not back propagating through that entire amalgamation graph.
[00:35:21.900 --> 00:35:26.900]   You're just doing it through individual ones. But it still is a graph of interconnected nodes.
[00:35:26.900 --> 00:35:28.900]   Graphs.
[00:35:28.900 --> 00:35:35.900]   That's an interesting perspective. And I certainly would love it if every programmer learned to do a little bit of machine learning.
[00:35:35.900 --> 00:35:40.900]   It would mean definitely job security for people who teach machine learning.
[00:35:40.900 --> 00:35:45.900]   Yeah. That's one of the things I do.
[00:35:45.900 --> 00:35:50.900]   Great. Yeah. Well, it was a pleasure to have you on the salon. Thanks a lot for coming by.
[00:35:50.900 --> 00:35:52.900]   Thank you.
[00:35:52.900 --> 00:35:54.900]   [ Applause ]
[00:35:54.900 --> 00:35:58.900]   [ Silence ]
[00:35:58.900 --> 00:36:00.960]   you

