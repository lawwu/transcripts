
[00:00:00.000 --> 00:00:05.920]   So with this talk and most of the talks that I give at these salons,
[00:00:05.920 --> 00:00:11.120]   I like to go back to the basic ideas in machine learning and deep learning,
[00:00:11.120 --> 00:00:14.240]   and give them a second thought.
[00:00:14.240 --> 00:00:16.800]   Maybe think about them a slightly different way,
[00:00:16.800 --> 00:00:22.120]   and talk about them in a different style.
[00:00:22.120 --> 00:00:24.800]   So in particular, this time,
[00:00:24.800 --> 00:00:30.480]   I am going to talk about the neural network loss,
[00:00:30.480 --> 00:00:34.000]   and how it can be thought of as a unit test.
[00:00:34.000 --> 00:00:37.760]   So the view that I think makes the most sense for thinking about
[00:00:37.760 --> 00:00:41.600]   machine learning is that machine learning is automated programming.
[00:00:41.600 --> 00:00:44.120]   So when we program, what do we do?
[00:00:44.120 --> 00:00:47.640]   We produce computer programs that have some desired behavior.
[00:00:47.640 --> 00:00:52.200]   It doesn't really help if we just produce any old computer program.
[00:00:52.200 --> 00:00:56.400]   And so in humans program, when humans do it, they write code.
[00:00:56.400 --> 00:00:58.760]   So we reason about the desired behavior,
[00:00:58.760 --> 00:01:02.720]   and then we encode text files with that reasoning.
[00:01:02.720 --> 00:01:05.600]   When machine learning algorithms optimize models,
[00:01:05.600 --> 00:01:08.960]   when we automate programming with machine learning,
[00:01:08.960 --> 00:01:11.720]   they observe desired behavior,
[00:01:11.720 --> 00:01:15.120]   and then they encode parameter values to match that behavior.
[00:01:15.120 --> 00:01:16.960]   This is a very different process.
[00:01:16.960 --> 00:01:19.520]   They have the same end result, which is a computer program,
[00:01:19.520 --> 00:01:23.400]   whether it's, you know, ResNet.py or HelloWorld.py.
[00:01:23.400 --> 00:01:28.360]   But the process by which they come about is quite different.
[00:01:28.360 --> 00:01:31.720]   But let's focus on the similarities first,
[00:01:31.720 --> 00:01:34.600]   which is we need in both cases to ensure
[00:01:34.600 --> 00:01:37.920]   that our programs have that desired behavior.
[00:01:37.920 --> 00:01:43.080]   So for machine learning, we use loss functions and optimization algorithms.
[00:01:43.080 --> 00:01:48.640]   The optimizer will write some parameters that make the loss go down.
[00:01:48.640 --> 00:01:50.680]   And that is the way that we try and ensure
[00:01:50.680 --> 00:01:55.000]   that our machine learned algorithm is behaving correctly.
[00:01:55.000 --> 00:01:57.520]   For human programming, we don't use loss functions.
[00:01:57.520 --> 00:02:00.920]   At least I don't when I'm writing Python code.
[00:02:00.920 --> 00:02:03.440]   We use tests of desired behavior.
[00:02:03.440 --> 00:02:09.480]   So humans are going to be writing text files that make the tests pass, right?
[00:02:09.480 --> 00:02:13.120]   Our goal is that the code that we write passes all of the tests
[00:02:13.120 --> 00:02:14.680]   in our testing suite.
[00:02:14.680 --> 00:02:17.080]   What I think is interesting about looking at it this way
[00:02:17.080 --> 00:02:19.000]   is that both of these approaches are flawed.
[00:02:19.000 --> 00:02:20.040]   They have failure modes.
[00:02:20.040 --> 00:02:22.520]   The loss-- making the loss go down on the training set
[00:02:22.520 --> 00:02:24.960]   doesn't always give us a good algorithm.
[00:02:24.960 --> 00:02:28.440]   Passing our tests does not always give us good code.
[00:02:28.440 --> 00:02:30.680]   And I think it's revealing that they actually end up
[00:02:30.680 --> 00:02:32.560]   sharing some of the same flaws.
[00:02:32.560 --> 00:02:36.000]   And some flaws are better known in programming,
[00:02:36.000 --> 00:02:39.040]   in traditional human programming, and some are better known in machine
[00:02:39.040 --> 00:02:41.360]   learning.
[00:02:41.360 --> 00:02:44.920]   So the ethos that tests should drive development
[00:02:44.920 --> 00:02:49.480]   is encapsulated in the principle of test-driven development, which
[00:02:49.480 --> 00:02:52.080]   is a workflow that says, first, I decide
[00:02:52.080 --> 00:02:55.080]   on the behavior of a new feature for my code.
[00:02:55.080 --> 00:02:59.960]   Then I write tests that will only pass if that feature behaves correctly.
[00:02:59.960 --> 00:03:02.760]   Then I write code that passes those tests.
[00:03:02.760 --> 00:03:04.520]   And then I repeat this over and over again.
[00:03:04.520 --> 00:03:06.000]   I come up with a new feature.
[00:03:06.000 --> 00:03:07.200]   I write more tests.
[00:03:07.200 --> 00:03:10.320]   Then I write more code in that order.
[00:03:10.320 --> 00:03:13.040]   So what does it look like to think of machine learning in that way?
[00:03:13.040 --> 00:03:15.800]   Well, in that case, we're actually going
[00:03:15.800 --> 00:03:17.920]   to be using loss-driven development instead
[00:03:17.920 --> 00:03:19.520]   of test-driven development.
[00:03:19.520 --> 00:03:22.000]   First, we decide on the behavior for a new data point.
[00:03:22.000 --> 00:03:25.760]   And that usually means loadings the desired target,
[00:03:25.760 --> 00:03:28.280]   so especially in supervised learning.
[00:03:28.280 --> 00:03:31.640]   Then we'll write code that tests how close we are to the right behavior.
[00:03:31.640 --> 00:03:33.440]   And that's exactly what our loss function
[00:03:33.440 --> 00:03:37.840]   is when we think of our loss as a function of the parameters.
[00:03:37.840 --> 00:03:42.680]   It tells us how close did we get to doing the right thing.
[00:03:42.680 --> 00:03:46.840]   Then we update the model so that we do better on that test.
[00:03:46.840 --> 00:03:50.720]   So we use something like gradient descent or Atom
[00:03:50.720 --> 00:03:52.880]   or whatever optimizer it is that we're using
[00:03:52.880 --> 00:03:56.720]   to update the model to improve our performance on the test.
[00:03:56.720 --> 00:03:58.400]   And then we do that over and over again.
[00:03:58.400 --> 00:04:02.680]   We come up with a new behavior that we want from a new data point
[00:04:02.680 --> 00:04:04.560]   or a batch of data points.
[00:04:04.560 --> 00:04:07.800]   And then we use the loss and the optimizer
[00:04:07.800 --> 00:04:15.320]   once again to improve our behavior on that new test.
[00:04:15.320 --> 00:04:18.520]   So this connection here is clear for an idea that's
[00:04:18.520 --> 00:04:21.600]   maybe taken a while for machine learning
[00:04:21.600 --> 00:04:26.520]   to really internalize, which is the idea of out-of-distribution data.
[00:04:26.520 --> 00:04:29.320]   So the most common failure mode for test-driven development
[00:04:29.320 --> 00:04:32.040]   is that the testing suite that we have is just not broad enough
[00:04:32.040 --> 00:04:35.560]   to cover what's encountered in real life when we get into production.
[00:04:35.560 --> 00:04:37.040]   We tested a whole bunch of things, but we
[00:04:37.040 --> 00:04:39.480]   didn't test what happened when somebody uses a name that's
[00:04:39.480 --> 00:04:42.000]   entirely composed of emojis.
[00:04:42.000 --> 00:04:43.680]   So our tests weren't--
[00:04:43.680 --> 00:04:45.080]   they didn't cover everything.
[00:04:45.080 --> 00:04:48.760]   Similarly, machine learning models can fail when the training set is not
[00:04:48.760 --> 00:04:51.960]   broad enough to cover what is encountered once they're in production.
[00:04:51.960 --> 00:04:55.960]   We don't have pictures of blue stop signs in our self-driving car data set,
[00:04:55.960 --> 00:04:58.800]   but maybe they're out there somewhere in the world.
[00:04:58.800 --> 00:05:02.000]   So this is the problem of generalization to out-of-distribution data,
[00:05:02.000 --> 00:05:03.800]   and it's as unsolved for machine learning
[00:05:03.800 --> 00:05:05.400]   as it is for test-driven development.
[00:05:05.400 --> 00:05:09.520]   But I'd be interested to see if there are any ideas from test-driven development
[00:05:09.520 --> 00:05:12.960]   that could help us rethink how we might handle this out-of-distribution data
[00:05:12.960 --> 00:05:13.460]   problem.
[00:05:13.460 --> 00:05:22.360]   One problem that's actually maybe easier to understand in machine learning
[00:05:22.360 --> 00:05:24.480]   and maybe less common in test-driven development
[00:05:24.480 --> 00:05:26.280]   is the problem of overfitting.
[00:05:26.280 --> 00:05:35.360]   So when I first proposed this idea about the loss being a unit test,
[00:05:35.360 --> 00:05:38.440]   somebody said, well, I don't see how overfitting fits into this idea,
[00:05:38.440 --> 00:05:41.400]   and that's a really important thing for people to understand.
[00:05:41.400 --> 00:05:45.160]   So I thought about it for a while, and I came up with this example.
[00:05:45.160 --> 00:05:48.160]   It's important that we remember, when we think about how this process would
[00:05:48.160 --> 00:05:50.560]   work, that computers are actually just dumb rocks
[00:05:50.560 --> 00:05:52.000]   that we tricked into thinking.
[00:05:52.000 --> 00:05:55.360]   And they follow the letter of what we ask, not the spirit.
[00:05:55.360 --> 00:05:57.880]   So we might think of them as being like golems, which
[00:05:57.880 --> 00:06:02.120]   are a famous machine from Eastern European Jewish folklore,
[00:06:02.120 --> 00:06:06.280]   about these machines that are essentially
[00:06:06.280 --> 00:06:12.360]   a danger to their creators as much as they are a powerful machine.
[00:06:12.360 --> 00:06:16.760]   And this idea that computers are sort of dumb and pedantic
[00:06:16.760 --> 00:06:20.880]   also includes when we ask them to pass tests by writing programs.
[00:06:20.880 --> 00:06:25.520]   So when we ask a human to write code that passed tests,
[00:06:25.520 --> 00:06:27.440]   they look at the tests, and they try to come up
[00:06:27.440 --> 00:06:31.440]   with a good faith effort to write a good program that passes those tests.
[00:06:31.440 --> 00:06:36.240]   They look at the name of what we want, print hello world, or divide by 2,
[00:06:36.240 --> 00:06:39.040]   and they try and come up with a good program.
[00:06:39.040 --> 00:06:40.920]   But computers aren't going to do that.
[00:06:40.920 --> 00:06:45.560]   So to extend this idea, this analogy, so that it covers overfitting,
[00:06:45.560 --> 00:06:48.640]   I imagined what might happen if programmers
[00:06:48.640 --> 00:06:51.920]   were being incentivized the same way our machine learning algorithms were.
[00:06:51.920 --> 00:06:55.080]   So imagine what might happen if programmer salaries were
[00:06:55.080 --> 00:06:56.840]   tied to test passing and nothing else.
[00:06:56.840 --> 00:06:58.960]   Doesn't matter if the website goes down.
[00:06:58.960 --> 00:07:03.120]   Doesn't matter if all the users on the Google Play Store give us zero stars.
[00:07:03.120 --> 00:07:06.040]   If we pass the test, we get paid.
[00:07:06.040 --> 00:07:09.480]   And also, those tests were written by somebody else,
[00:07:09.480 --> 00:07:12.760]   and they're going to execute the tests themselves.
[00:07:12.760 --> 00:07:14.760]   But they're never going to read our code.
[00:07:14.760 --> 00:07:18.440]   They're just going to look, OK, did you pass the test or not?
[00:07:18.440 --> 00:07:20.280]   And I think if you think about it that way,
[00:07:20.280 --> 00:07:24.040]   it's clear that test-driven development is not going to be enough.
[00:07:24.040 --> 00:07:27.240]   And to exemplify just how badly this can go wrong,
[00:07:27.240 --> 00:07:33.080]   I came up with this extremely cursed and unpleasant Python example, which
[00:07:33.080 --> 00:07:36.200]   is-- so when we write our tests, we usually say something like, OK,
[00:07:36.200 --> 00:07:39.440]   there's this function foo that's in my implementation file.
[00:07:39.440 --> 00:07:43.600]   And in my test, what I'm going to do is I'm going to give foo some argument,
[00:07:43.600 --> 00:07:46.400]   and then I'm going to expect that it gives a particular output.
[00:07:46.400 --> 00:07:51.040]   And passing the test means that the output of foo is the expected output.
[00:07:51.040 --> 00:07:54.240]   This is a very common style of test.
[00:07:54.240 --> 00:07:58.200]   So I imagine what I might write if my salary depended
[00:07:58.200 --> 00:08:01.560]   on passing this test and nothing else.
[00:08:01.560 --> 00:08:04.960]   And I came up with this guy, this implementation of foo,
[00:08:04.960 --> 00:08:07.000]   which I checked in PyTest.
[00:08:07.000 --> 00:08:10.080]   It does indeed pass the test on the left without doing
[00:08:10.080 --> 00:08:12.440]   anything sensible whatsoever.
[00:08:12.440 --> 00:08:15.880]   So I used a few pieces of Python dark magic.
[00:08:15.880 --> 00:08:17.560]   So I looked at the traceback.
[00:08:17.560 --> 00:08:21.720]   I did some importing of modules and getting
[00:08:21.720 --> 00:08:25.680]   a hold of the test foo function, and then reading its source code.
[00:08:25.680 --> 00:08:28.440]   And inside that source code, I look for an assert line,
[00:08:28.440 --> 00:08:30.820]   because the tests that are going to be run are probably
[00:08:30.820 --> 00:08:32.960]   going to be assert statements.
[00:08:32.960 --> 00:08:35.160]   And then in that assert statement, I pull out
[00:08:35.160 --> 00:08:40.520]   what I'm supposed to return, and then I just return that.
[00:08:40.520 --> 00:08:43.440]   So whatever the test says they want, return it.
[00:08:43.440 --> 00:08:46.720]   But this is clearly not what any reasonable person
[00:08:46.720 --> 00:08:49.200]   would want code to do.
[00:08:49.200 --> 00:08:52.100]   So this is an example of overfitting to the tests.
[00:08:52.100 --> 00:08:54.000]   I have written code that passes the test,
[00:08:54.000 --> 00:08:55.320]   but it does nothing else.
[00:08:55.320 --> 00:08:58.080]   And this is exactly what our machine learning models do
[00:08:58.080 --> 00:08:59.960]   when they overfit to our training data.
[00:08:59.960 --> 00:09:03.120]   They become good at giving the desired behavior
[00:09:03.120 --> 00:09:06.840]   on the training data, but terrible at everything else.
[00:09:06.840 --> 00:09:09.800]   So what would the solution be, besides coming up
[00:09:09.800 --> 00:09:13.040]   with a better workflow than shackling your programmers
[00:09:13.040 --> 00:09:14.880]   to passing tests?
[00:09:14.880 --> 00:09:18.080]   So if there are some tests that these programmers cannot see,
[00:09:18.080 --> 00:09:20.000]   but on which they'll be evaluated,
[00:09:20.000 --> 00:09:22.440]   then they have no choice but to examine the tests
[00:09:22.440 --> 00:09:25.400]   and try and actually implement the right program, a program
[00:09:25.400 --> 00:09:27.160]   with the behavior we actually desire.
[00:09:27.160 --> 00:09:32.600]   No matter how nefarious they are or how lazy,
[00:09:32.600 --> 00:09:34.960]   this is the only option that they have.
[00:09:34.960 --> 00:09:37.200]   Similarly, if there are data examples
[00:09:37.200 --> 00:09:40.440]   that the model cannot see, but on which it is being evaluated,
[00:09:40.440 --> 00:09:42.280]   then the best performing algorithm
[00:09:42.280 --> 00:09:45.100]   will be the one that examines the pattern of the examples
[00:09:45.100 --> 00:09:49.600]   and selects parameters that result in the desired behavior.
[00:09:49.600 --> 00:09:53.320]   And this is exactly what we do when we hold out a validation
[00:09:53.320 --> 00:09:55.300]   set, when we do cross-validation.
[00:09:55.300 --> 00:09:57.600]   We say, here are some points you've never seen before.
[00:09:57.600 --> 00:09:59.880]   How are you doing on this?
[00:09:59.880 --> 00:10:01.920]   For programming, that would mean take some tests
[00:10:01.920 --> 00:10:03.880]   and apply them to the code and see, OK,
[00:10:03.880 --> 00:10:05.560]   do you pass these tests that you've never
[00:10:05.560 --> 00:10:08.320]   gotten a chance to see?
[00:10:08.320 --> 00:10:12.200]   And this also, I think, brings some interesting insight
[00:10:12.200 --> 00:10:14.960]   into a phenomenon called catastrophic forgetting.
[00:10:14.960 --> 00:10:16.720]   So it's been known for a really long time,
[00:10:16.720 --> 00:10:18.340]   back since neural networks were thought
[00:10:18.340 --> 00:10:21.960]   of as connectionist psychology, so way back in the '80s,
[00:10:21.960 --> 00:10:23.920]   that they're subject to a phenomenon called
[00:10:23.920 --> 00:10:25.640]   catastrophic forgetting, which is
[00:10:25.640 --> 00:10:28.920]   that if you take a network that's been trained to, say,
[00:10:28.920 --> 00:10:33.480]   solve ImageNet and then train it on a task that's, say,
[00:10:33.480 --> 00:10:37.120]   take ImageNet images and tell me whether they're red or green,
[00:10:37.120 --> 00:10:39.040]   then the performance on the first task
[00:10:39.040 --> 00:10:40.640]   will actually plummet over time.
[00:10:40.640 --> 00:10:44.040]   And this will even happen if these two tasks
[00:10:44.040 --> 00:10:46.520]   are very close to each other.
[00:10:46.520 --> 00:10:49.920]   So this is represented in this figure over here.
[00:10:49.920 --> 00:10:53.440]   What this figure represents is on the y-axis
[00:10:53.440 --> 00:10:55.280]   are the performance on three different tasks.
[00:10:55.280 --> 00:10:57.360]   So let's focus on this one up here.
[00:10:57.360 --> 00:10:59.640]   So first, we train the network on task A,
[00:10:59.640 --> 00:11:01.080]   and it gets good at that task.
[00:11:01.080 --> 00:11:04.360]   Then we start training it on task B. It gets good at task B,
[00:11:04.360 --> 00:11:07.120]   but it starts getting worse at task C.
[00:11:07.120 --> 00:11:10.320]   And then we start training it on a third task, task C,
[00:11:10.320 --> 00:11:11.400]   and it gets even worse.
[00:11:11.400 --> 00:11:14.320]   And over time, it will get back to completely random
[00:11:14.320 --> 00:11:18.680]   performance on the original task as it continues
[00:11:18.680 --> 00:11:20.960]   to learn these different things.
[00:11:20.960 --> 00:11:22.840]   So this is a surprising phenomenon.
[00:11:22.840 --> 00:11:25.240]   I think a lot of people aren't necessarily expecting it.
[00:11:25.240 --> 00:11:27.600]   Can we get some intuition to it by thinking
[00:11:27.600 --> 00:11:30.200]   about the loss as a unit test?
[00:11:30.200 --> 00:11:32.080]   So the important thing about unit tests
[00:11:32.080 --> 00:11:35.160]   is actually that they're meant to be run before each commit.
[00:11:35.160 --> 00:11:37.320]   This is why when we're doing test-driven development,
[00:11:37.320 --> 00:11:40.640]   we use continual integration.
[00:11:40.640 --> 00:11:42.360]   We make sure that every time somebody
[00:11:42.360 --> 00:11:45.880]   tries to add code to our code base, the tests get run.
[00:11:45.880 --> 00:11:48.160]   These prevent code regression, where
[00:11:48.160 --> 00:11:50.200]   we lose behavior that we had previously
[00:11:50.200 --> 00:11:52.080]   and we wanted in a new update.
[00:11:52.080 --> 00:11:55.080]   This is something that actually Francois Chollet pointed out
[00:11:55.080 --> 00:11:55.960]   in a recent tweet.
[00:11:55.960 --> 00:11:57.560]   He said that the purpose of unit tests
[00:11:57.560 --> 00:12:00.040]   is not really actually checking correctness now.
[00:12:00.040 --> 00:12:02.480]   It's to ensure that you don't get code regression,
[00:12:02.480 --> 00:12:05.040]   that you don't get future breakages.
[00:12:05.040 --> 00:12:08.680]   So this is the real import of tests.
[00:12:08.680 --> 00:12:12.280]   It's probably the primary utility of writing tests,
[00:12:12.280 --> 00:12:15.120]   is when they are run with each commit.
[00:12:15.120 --> 00:12:17.080]   But catastrophic forgetting actually happens
[00:12:17.080 --> 00:12:19.520]   when we stop testing.
[00:12:19.520 --> 00:12:21.000]   When a network trained on one task
[00:12:21.000 --> 00:12:23.080]   is then trained to perform a new task,
[00:12:23.080 --> 00:12:24.960]   the original task loss is not invoked
[00:12:24.960 --> 00:12:26.520]   while we're training for the new task.
[00:12:26.520 --> 00:12:28.220]   We don't go back and say, oh, by the way,
[00:12:28.220 --> 00:12:30.120]   you also still need to be good at task A.
[00:12:30.120 --> 00:12:31.960]   This is equivalent to no longer testing
[00:12:31.960 --> 00:12:34.300]   for that old desired behavior.
[00:12:34.300 --> 00:12:37.000]   So it's like turning off the tests for your original--
[00:12:37.000 --> 00:12:40.760]   for module A while you're writing code that is in module
[00:12:40.760 --> 00:12:43.600]   B and that both of them end up touching.
[00:12:43.600 --> 00:12:46.720]   So it should be no surprise that we get neural network
[00:12:46.720 --> 00:12:48.360]   performance regression in the same way
[00:12:48.360 --> 00:12:50.200]   that we would get code regression
[00:12:50.200 --> 00:12:57.600]   in a traditional software model with test-driven development.
[00:12:57.600 --> 00:13:01.180]   So the central analogy here of the loss as a unit test
[00:13:01.180 --> 00:13:03.800]   is that we're still programming when
[00:13:03.800 --> 00:13:05.600]   we're doing machine learning.
[00:13:05.600 --> 00:13:10.880]   The optimizer is our programmer, and we are now the managers.
[00:13:10.880 --> 00:13:16.520]   And so instead of using tests to ensure that our programmer is
[00:13:16.520 --> 00:13:19.800]   doing the right thing, we use loss functions and optimizers.
[00:13:19.800 --> 00:13:24.880]   But otherwise, there's a lot of connections,
[00:13:24.880 --> 00:13:29.140]   a lot of similarities to what happens when humans program
[00:13:29.140 --> 00:13:31.520]   and when they use test-driven development.
[00:13:31.520 --> 00:13:33.120]   So they share the same flaws, and they
[00:13:33.120 --> 00:13:36.360]   share the same strengths.
[00:13:36.360 --> 00:13:39.040]   So this is an analogy I'm kind of excited about.
[00:13:39.040 --> 00:13:40.400]   I'm probably going to be thinking
[00:13:40.400 --> 00:13:42.080]   about this a good amount in the future
[00:13:42.080 --> 00:13:44.920]   to try and come up with new ways that this might bring
[00:13:44.920 --> 00:13:47.760]   new insights into different--

