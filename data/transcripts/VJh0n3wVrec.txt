
[00:00:00.000 --> 00:00:06.280]   Well, so you've touched on AI and safety a couple of times in the last few minutes, and
[00:00:06.280 --> 00:00:11.720]   you also operate in a world that I think is considered a gray area to a lot of AI researchers,
[00:00:11.720 --> 00:00:13.720]   defense or military applications.
[00:00:13.720 --> 00:00:19.200]   I'm curious what you think generally about, especially natural language models and safety
[00:00:19.200 --> 00:00:24.680]   and what should be done, how worried you think people should be about misuse of these models
[00:00:24.680 --> 00:00:30.360]   and what role you think you should play as a leading company in the space?
[00:00:30.360 --> 00:00:36.600]   Well, I think first and foremost, if we want to be a global superpower as America, you
[00:00:36.600 --> 00:00:39.000]   have to have defense, you have to have intelligence.
[00:00:39.000 --> 00:00:43.800]   You may not want to have them, but then you don't get to be the global superpower.
[00:00:43.800 --> 00:00:47.480]   So that's the first thing to just accept is that defense and intelligence are part and
[00:00:47.480 --> 00:00:49.940]   parcel of being a global superpower.
[00:00:49.940 --> 00:00:53.520]   It's also part and parcel of defending liberal Western democracy.
[00:00:53.520 --> 00:00:56.680]   And there are plenty of other organizations and governments in the world that don't want
[00:00:56.680 --> 00:00:58.160]   that to exist.
[00:00:58.160 --> 00:01:00.000]   So we need that.
[00:01:00.000 --> 00:01:02.960]   As you come back from that, the second thing you say, well, we want it, but we want it
[00:01:02.960 --> 00:01:03.960]   to be good.
[00:01:03.960 --> 00:01:04.960]   Right.
[00:01:04.960 --> 00:01:08.840]   And so you say, well, if I want it to be good, well, we need to bring artificial intelligence
[00:01:08.840 --> 00:01:12.920]   and the latest technologies that we're developing to bear on that problem space.
[00:01:12.920 --> 00:01:17.160]   It's sort of a strange philosophical ground to say, well, we need to have defense, but
[00:01:17.160 --> 00:01:18.160]   it shouldn't be good.
[00:01:18.160 --> 00:01:19.160]   Right.
[00:01:19.160 --> 00:01:20.720]   It's just a strange position.
[00:01:20.720 --> 00:01:25.520]   Now, as you go through that, you say, well, there are also ethical concerns and moral
[00:01:25.520 --> 00:01:26.520]   concerns.
[00:01:26.520 --> 00:01:31.080]   There are very, very few organizations in the world that think more deeply about the
[00:01:31.080 --> 00:01:37.040]   ethical and moral implications of war than defense and intelligence.
[00:01:37.040 --> 00:01:39.320]   They live and breathe this stuff.
[00:01:39.320 --> 00:01:43.160]   And we can kind of sit here and I'm that quarterback from the valley.
[00:01:43.160 --> 00:01:47.000]   But the reality is, is this is something that is being thought very, very deeply about and
[00:01:47.000 --> 00:01:51.800]   has a lot of care and that kind of rules of engagement, very, very well defined and very,
[00:01:51.800 --> 00:01:58.000]   very well thought through and have been shaped and kind of constructed over many, many years.
[00:01:58.000 --> 00:02:02.880]   Now a lot of them haven't imagined what AI does in that, but there's also been a huge
[00:02:02.880 --> 00:02:09.200]   amount of work going back to me over the last decade with defense, with intelligence, talking
[00:02:09.200 --> 00:02:14.560]   about these exact scenarios of what it means to have artificial intelligence engaging in
[00:02:14.560 --> 00:02:16.580]   this kind of process.
[00:02:16.580 --> 00:02:22.320]   So for me here, bringing this technology to bear and defense and intelligence is something
[00:02:22.320 --> 00:02:25.000]   that I think is the right thing to do.
[00:02:25.000 --> 00:02:28.520]   And it's a very, very important mission for myself and for our company.
[00:02:28.520 --> 00:02:33.720]   As we do that, we also realize we've got a responsibility that it matters if we're generating
[00:02:33.720 --> 00:02:38.360]   models that are classifying things that are unfolding in the world and saying, look, we
[00:02:38.360 --> 00:02:40.120]   identified an event.
[00:02:40.120 --> 00:02:44.880]   And if you misclassify that, that intelligence is now percolating up a chain, which is going
[00:02:44.880 --> 00:02:45.880]   to have consequences.
[00:02:45.880 --> 00:02:49.960]   So there are very real consequences when you talk about the precision of the models that
[00:02:49.960 --> 00:02:50.960]   you're working with.
[00:02:50.960 --> 00:02:54.680]   There are very real consequences when you talk about what the data is being trained
[00:02:54.680 --> 00:02:59.680]   on, what the susceptibility of the models that you've got are to outside adversarial
[00:02:59.680 --> 00:03:00.680]   attacks.
[00:03:00.680 --> 00:03:04.480]   So all of this becomes something that you need to kind of work with and deal with.
[00:03:04.480 --> 00:03:11.000]   I think the sort of the ethical components of this woven into the decisions that we make,
[00:03:11.000 --> 00:03:14.680]   and it's something that's also moving, I think, pretty quickly.
[00:03:14.680 --> 00:03:19.200]   And there's one thing you learn in science and technology is that science and technology
[00:03:19.200 --> 00:03:24.040]   moves a whole lot faster than the philosophical and ideological kind of foundations on which
[00:03:24.040 --> 00:03:26.000]   you can kind of make decisions on top of.
[00:03:26.000 --> 00:03:29.800]   And so you are by nature going to be in gray zones.
[00:03:29.800 --> 00:03:34.360]   And this is something you've got to be kind of open to and say, look, we're going to navigate
[00:03:34.360 --> 00:03:37.920]   where perhaps no one's ever thought about this before.
[00:03:37.920 --> 00:03:43.200]   And there isn't kind of a strong kind of rule that you can fall back to and say, hey, this
[00:03:43.200 --> 00:03:44.200]   is the answer.
[00:03:44.200 --> 00:03:47.960]   This is what you're supposed to do in this situation, because the situation's never existed
[00:03:47.960 --> 00:03:48.960]   before.
[00:03:48.960 --> 00:03:55.640]   So it's something that we spend a lot of time with both ourselves and also our advisors,
[00:03:55.640 --> 00:03:59.520]   spending time each and every week going through this stuff, making decisions, and trying to
[00:03:59.520 --> 00:04:02.200]   kind of navigate the best path that we can through this.
[00:04:02.200 --> 00:04:05.640]   But I think it would be a lie to say that this is really easy and there's this clear
[00:04:05.640 --> 00:04:10.400]   black and white kind of distinctions, because we're dealing with stuff that simply didn't
[00:04:10.400 --> 00:04:11.400]   exist in the world before.
[00:04:11.400 --> 00:04:15.080]   But we're also dealing on the geopolitical scale with stuff that simply didn't exist
[00:04:15.080 --> 00:04:17.300]   in the world before.
[00:04:17.300 --> 00:04:29.400]   And do you think at this moment in time, August 2020, do you think that for governments, natural
[00:04:29.400 --> 00:04:35.560]   language processing, like machine learning, is an important part of their defense capability?
[00:04:35.560 --> 00:04:39.120]   Yeah, I think there's three places where it comes through.
[00:04:39.120 --> 00:04:41.040]   The first is on the intelligence side.
[00:04:41.040 --> 00:04:43.240]   There is too much information coming in.
[00:04:43.240 --> 00:04:47.800]   And simply put, if you don't have machines playing some role in helping you navigate
[00:04:47.800 --> 00:04:51.440]   that information, you're going to have information that no one ever sees.
[00:04:51.440 --> 00:04:54.680]   And if you don't see information, you can't bring it to bear on decisions that you're
[00:04:54.680 --> 00:04:55.680]   making.
[00:04:55.680 --> 00:05:00.040]   So step one, the volume of information requires a natural language toolkit to actually help
[00:05:00.040 --> 00:05:01.680]   navigate.
[00:05:01.680 --> 00:05:06.440]   The second thing here is that the complexity of the world that we're in means that drawing
[00:05:06.440 --> 00:05:11.760]   inferences between something that's happening in Russia and something that's happening in
[00:05:11.760 --> 00:05:16.880]   East Africa is very, very difficult for an individual that has to specialize in, "I'm
[00:05:16.880 --> 00:05:20.480]   an East African specialist," or, "I'm a Russian specialist."
[00:05:20.480 --> 00:05:21.480]   Machines don't have that limitation.
[00:05:21.480 --> 00:05:25.880]   They can look further, they can look wider, they can draw inference across larger set
[00:05:25.880 --> 00:05:29.400]   of data points because they're not fundamentally constrained by the bandwidth of information
[00:05:29.400 --> 00:05:30.400]   they can consume.
[00:05:30.400 --> 00:05:34.880]   So I think as we move to a more complex world, it's essential to have machines that can make
[00:05:34.880 --> 00:05:39.480]   connections across domains that humans aren't necessarily looking at.
[00:05:39.480 --> 00:05:42.600]   The third thing is, and this has sort of, I think, become increasingly important, is
[00:05:42.600 --> 00:05:48.400]   that more and more information is being generated by machines, and that's being used to manipulate.
[00:05:48.400 --> 00:05:52.600]   And if you've got humans that are trying to filter through the output of propaganda from
[00:05:52.600 --> 00:05:56.560]   China that's being machine generated, you've brought a knife to a gunfight.
[00:05:56.560 --> 00:05:57.800]   You're going to lose that.
[00:05:57.800 --> 00:06:04.080]   And so as we look at things like the operations out of Pacific Command, there's a huge volume
[00:06:04.080 --> 00:06:09.040]   of information now that China's got its head around disinformation and manipulation.
[00:06:09.040 --> 00:06:11.480]   You can't navigate this as a set of humans.
[00:06:11.480 --> 00:06:12.480]   It's just not possible.
[00:06:12.480 --> 00:06:14.880]   And if you try and do that, you're going to lose.
[00:06:14.880 --> 00:06:19.080]   So I think the disinformation landscape has necessitated a set of machines that need to
[00:06:19.080 --> 00:06:20.080]   come into this.
[00:06:20.080 --> 00:06:22.120]   Thanks for watching this clip.
[00:06:22.120 --> 00:06:26.880]   You can see the full episode on our YouTube channel, and you can join our friendly Slack
[00:06:26.880 --> 00:06:32.680]   community with over 4,000 ML engineers to participate in paper reading groups, AMAs,
[00:06:32.680 --> 00:06:34.440]   and other fun events.
[00:06:34.440 --> 00:06:37.020]   (upbeat music)

