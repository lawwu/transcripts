
[00:00:00.000 --> 00:00:02.000]   I've been burned by this many a time.
[00:00:02.000 --> 00:00:06.000]   I think it was Andrej Karpathy had a blog being like, be very paranoid and
[00:00:06.000 --> 00:00:08.160]   visualize every single input and output.
[00:00:08.160 --> 00:00:10.860]   And that's something that I've learned the hard way, I think,
[00:00:10.860 --> 00:00:12.040]   doing machine learning projects.
[00:00:12.040 --> 00:00:12.960]   100%.
[00:00:12.960 --> 00:00:20.880]   Welcome back to Math for Machine Learning exercise sessions.
[00:00:20.880 --> 00:00:24.460]   I am your host, deep learning educator, Charles Fry.
[00:00:24.460 --> 00:00:27.480]   I have with me here, Growth ML engineer, Scott Condren.
[00:00:27.480 --> 00:00:27.880]   Hello.
[00:00:27.940 --> 00:00:31.640]   And we're going to jump into now our second session with
[00:00:31.640 --> 00:00:34.120]   the linear algebra exercises.
[00:00:34.120 --> 00:00:39.160]   So in the first session, we got used to this auto grader that would tell us.
[00:00:39.160 --> 00:00:44.080]   Whether our answers were right and give us feedback, hopefully helpful
[00:00:44.080 --> 00:00:48.800]   feedback to help us craft our answers and learn how to answer questions.
[00:00:48.800 --> 00:00:53.160]   And the other thing we did was talk about the core around linear algebra in this
[00:00:53.160 --> 00:00:57.600]   series, which is that linear algebra should be thought of like programming
[00:00:57.640 --> 00:01:03.040]   and not like algebra and the main point that we got across last time was that
[00:01:03.040 --> 00:01:08.020]   matrices should be thought of as functions rather than as data as something that
[00:01:08.020 --> 00:01:10.200]   takes in a vector and returns a vector.
[00:01:10.200 --> 00:01:13.380]   And we're going to dive way deeper on that idea this time.
[00:01:13.380 --> 00:01:16.880]   And if you want a more thorough explanation of those ideas, make sure
[00:01:16.880 --> 00:01:21.640]   to check out the slides and the videos for the lectures that accompany these exercises.
[00:01:21.640 --> 00:01:24.620]   So I'll give a quick reminder of what those ideas are.
[00:01:24.880 --> 00:01:29.800]   So the important thing about functions is not just that functions
[00:01:29.800 --> 00:01:31.100]   take in data and return data.
[00:01:31.100 --> 00:01:35.680]   It's that if I have two functions and the types match, so a function that
[00:01:35.680 --> 00:01:38.620]   outputs a Boolean and another function that takes in a Boolean,
[00:01:38.620 --> 00:01:40.400]   then I can combine them together.
[00:01:40.400 --> 00:01:44.480]   So I did an example with this too long to tweak function that breaks down
[00:01:44.480 --> 00:01:49.440]   checking whether a number is over 140 and checking the length of a string to
[00:01:49.440 --> 00:01:54.120]   take those two pieces and turn them into a single function that checks whether
[00:01:54.120 --> 00:01:56.440]   a string is too long to go into a single tweak.
[00:01:56.440 --> 00:02:02.220]   So we're used to this idea in programming that we combine these functions together
[00:02:02.220 --> 00:02:04.860]   to create our programs, to create our actually useful functions.
[00:02:04.860 --> 00:02:08.520]   This is also a really key idea in linear.
[00:02:08.520 --> 00:02:10.120]   I'll scroll down a little bit there.
[00:02:10.120 --> 00:02:14.080]   What we do to, how do we compose to matrices?
[00:02:14.080 --> 00:02:18.080]   When we compose two functions, we just call the second function
[00:02:18.080 --> 00:02:19.960]   with the output of the first function.
[00:02:19.960 --> 00:02:22.320]   And that's not quite what we do with matrices.
[00:02:22.320 --> 00:02:26.080]   With matrices, we multiply them together.
[00:02:26.080 --> 00:02:30.340]   We use matrix multiplication to multiply the two matrices together.
[00:02:30.340 --> 00:02:33.440]   So I have a matrix A and a matrix B and I want to compose them.
[00:02:33.440 --> 00:02:37.500]   So I apply the transformation A to a vector after applying the
[00:02:37.500 --> 00:02:39.060]   transformation B to a vector.
[00:02:39.060 --> 00:02:43.040]   Then what I do is that matrix multiplication rule that we talked
[00:02:43.040 --> 00:02:46.280]   about where we take the rows, multiply them with the columns, sum it up.
[00:02:46.280 --> 00:02:51.240]   So that is really actually probably the right way to think of where that rule
[00:02:51.240 --> 00:02:54.920]   for matrix multiplication comes from is that it's the right way to define
[00:02:54.920 --> 00:02:59.280]   multiplication of matrices so that it's function composition.
[00:02:59.280 --> 00:03:03.700]   So actually I honestly would prefer if we called it matrix composition or
[00:03:03.700 --> 00:03:06.500]   something like that, rather than calling it multiplication, because I think that
[00:03:06.500 --> 00:03:10.800]   confuses people because it doesn't behave like the way things do when you multiply
[00:03:10.800 --> 00:03:11.280]   numbers.
[00:03:11.280 --> 00:03:17.040]   Our first exercise here, just to connect us back up with writing code and not just
[00:03:17.040 --> 00:03:21.920]   talking about math, is we're going to check whether two matrices are compatible
[00:03:21.920 --> 00:03:22.520]   with each other.
[00:03:22.520 --> 00:03:28.320]   Whether I can do A times B as a matrix multiplication, just like I might write
[00:03:28.320 --> 00:03:30.120]   some checks before I run a function.
[00:03:30.120 --> 00:03:32.560]   I might check if my input actually is a string.
[00:03:32.560 --> 00:03:34.440]   In Python, you got to check stuff like that.
[00:03:34.440 --> 00:03:35.520]   Is it a string?
[00:03:35.520 --> 00:03:37.040]   Is it a non-negative number?
[00:03:37.040 --> 00:03:38.080]   Those kinds of checks.
[00:03:38.080 --> 00:03:42.560]   All right, Scott, our goal here is to return true if A at B is a valid matrix
[00:03:42.560 --> 00:03:43.320]   multiplication.
[00:03:43.640 --> 00:03:47.720]   There's a little hint above this question about how you would solve that.
[00:03:47.720 --> 00:03:49.280]   So what do we need to check here?
[00:03:49.280 --> 00:04:00.320]   The hint, where's the hint that, yeah, so the hint, where's the hint that, yeah,
[00:04:00.320 --> 00:04:01.080]   that part right there.
[00:04:01.080 --> 00:04:03.320]   Sums of A must equal the number of rows.
[00:04:03.320 --> 00:04:03.840]   Okay.
[00:04:03.840 --> 00:04:06.680]   So it's to do with the shape of these two.
[00:04:07.040 --> 00:04:15.080]   And it's just, so I guess we can do A rows is A dot shape and get the first element of it.
[00:04:15.080 --> 00:04:15.520]   Yep.
[00:04:15.520 --> 00:04:22.360]   And then we could do B columns and that would be B dot shape.
[00:04:22.360 --> 00:04:25.560]   And that would be the second part of that dimension of that.
[00:04:25.560 --> 00:04:30.720]   And then we just return if the rows are equal to the columns and that should do it.
[00:04:30.720 --> 00:04:31.160]   Yeah.
[00:04:31.160 --> 00:04:32.000]   Let's check it with the grader.
[00:04:32.000 --> 00:04:33.920]   Let's check it with the grader.
[00:04:33.920 --> 00:04:35.240]   Ooh, interesting.
[00:04:35.240 --> 00:04:36.560]   All right.
[00:04:36.560 --> 00:04:38.280]   When the inner shapes are the same.
[00:04:38.280 --> 00:04:39.240]   Do, do, do.
[00:04:39.240 --> 00:04:40.400]   Ah, okay.
[00:04:40.400 --> 00:04:44.400]   Let's actually, so we didn't quite get it this time.
[00:04:44.400 --> 00:04:45.560]   Okay.
[00:04:45.560 --> 00:04:50.600]   The number of columns of A must equal the number of rows of B.
[00:04:50.600 --> 00:04:52.280]   Okay.
[00:04:52.280 --> 00:04:53.120]   Yeah.
[00:04:53.120 --> 00:04:55.040]   I think we might've said it backwards.
[00:04:55.040 --> 00:04:55.360]   Okay.
[00:04:55.360 --> 00:04:56.560]   We can just fix it.
[00:04:56.560 --> 00:04:57.760]   That should do it.
[00:04:57.760 --> 00:04:58.880]   Yeah.
[00:04:58.880 --> 00:04:59.960]   Great.
[00:04:59.960 --> 00:05:00.160]   Okay.
[00:05:00.160 --> 00:05:01.080]   Yeah.
[00:05:01.200 --> 00:05:07.840]   So that is, that's because we line up each row of A with each column of B, which then
[00:05:07.840 --> 00:05:12.200]   means that the number of columns of A must equal the number of rows of B.
[00:05:12.200 --> 00:05:17.120]   For me, I have to picture like grabbing a hold of one row and grabbing a hold of a
[00:05:17.120 --> 00:05:19.640]   column of the next one and then combining them together.
[00:05:19.640 --> 00:05:24.080]   So that's where the inspiration for the cover image for this video series came from.
[00:05:24.080 --> 00:05:26.280]   This cover image.
[00:05:26.280 --> 00:05:26.680]   Yeah.
[00:05:26.680 --> 00:05:26.880]   Okay.
[00:05:26.880 --> 00:05:27.160]   That one.
[00:05:27.160 --> 00:05:27.720]   Cool.
[00:05:27.880 --> 00:05:31.520]   So that's showing that this is equal to the same length as this.
[00:05:31.520 --> 00:05:32.600]   Yeah, exactly.
[00:05:32.600 --> 00:05:33.160]   Okay.
[00:05:33.160 --> 00:05:33.960]   Okay.
[00:05:33.960 --> 00:05:35.600]   All right.
[00:05:35.600 --> 00:05:37.840]   So that just, that's just to get us back in terms of thinking
[00:05:37.840 --> 00:05:39.240]   about like shapes and types.
[00:05:39.240 --> 00:05:39.720]   Right?
[00:05:39.720 --> 00:05:42.360]   So we use types to determine which functions we combine together.
[00:05:42.360 --> 00:05:45.400]   We use shapes to determine which matrices and tensors and
[00:05:45.400 --> 00:05:46.680]   arrays we can combine together.
[00:05:46.680 --> 00:05:50.720]   So now let's actually start using our idea of composing things to
[00:05:50.720 --> 00:05:52.680]   make a more complicated function.
[00:05:52.840 --> 00:05:57.920]   So last time we defined two matrices, one called set second to
[00:05:57.920 --> 00:06:00.560]   zero and one called repeat three, two.
[00:06:00.560 --> 00:06:04.920]   Repeat three, two took a length two vector and repeated it three times and set second
[00:06:04.920 --> 00:06:08.040]   zero set the second entry of a vector to zero.
[00:06:08.040 --> 00:06:13.560]   So we want to do now is define a new matrix that sets the second entry to
[00:06:13.560 --> 00:06:15.240]   zero and repeats the result three times.
[00:06:15.240 --> 00:06:16.720]   Okay.
[00:06:16.720 --> 00:06:18.240]   That's fine.
[00:06:18.280 --> 00:06:25.200]   So I will not look up and see if maybe I might need to in a bit.
[00:06:25.200 --> 00:06:32.760]   So if I remember correctly, setting to zero was we had an array, a list of lists
[00:06:32.760 --> 00:06:38.640]   and it was the first had a one, zero.
[00:06:38.640 --> 00:06:42.640]   And then I think the second had a zero.
[00:06:42.640 --> 00:06:43.520]   Yep.
[00:06:43.520 --> 00:06:44.960]   That's our set to zero.
[00:06:44.960 --> 00:06:45.600]   Yeah.
[00:06:45.800 --> 00:06:52.240]   And now we need to repeat three times and that was to stack the output.
[00:06:52.240 --> 00:06:53.320]   Is that, is that correct?
[00:06:53.320 --> 00:06:57.040]   Like we want to have the, the number that you get given three times as the output.
[00:06:57.040 --> 00:06:58.040]   Is that correct?
[00:06:58.040 --> 00:06:59.040]   Yeah, exactly.
[00:06:59.040 --> 00:07:03.760]   So just create a new cell above this one and just check to see if those two matrices
[00:07:03.760 --> 00:07:08.160]   set second zero and repeat three, two are defined because I think we made them last time.
[00:07:08.160 --> 00:07:08.920]   Okay.
[00:07:08.920 --> 00:07:10.040]   Yeah.
[00:07:10.040 --> 00:07:12.760]   So we should have those two available.
[00:07:12.760 --> 00:07:13.200]   Okay.
[00:07:13.200 --> 00:07:14.640]   So we don't, I didn't need to do this.
[00:07:14.640 --> 00:07:15.320]   Yeah.
[00:07:15.480 --> 00:07:18.400]   You didn't necessarily have to redefine the matrices and you certainly, what you
[00:07:18.400 --> 00:07:23.880]   definitely don't want to do is try and rewrite this as you know, the actual raw
[00:07:23.880 --> 00:07:25.320]   matrix that you would get.
[00:07:25.320 --> 00:07:28.840]   It's sort of like how you like write code once in a function and then you reuse it
[00:07:28.840 --> 00:07:32.440]   lots of other places, you know, you write a matrix once and then you define all the
[00:07:32.440 --> 00:07:35.680]   other matrices you want to use in terms of matrix multiplications and other
[00:07:35.680 --> 00:07:37.240]   operations on that matrix.
[00:07:37.240 --> 00:07:37.960]   Okay.
[00:07:37.960 --> 00:07:43.760]   So I did fall in nicely to the trap of thinking, okay, I'll do one big matrix to
[00:07:43.760 --> 00:07:48.640]   solve these two where I should have composed them together to make one after the other.
[00:07:48.640 --> 00:07:53.080]   And what I'm now realizing is as you've just explained, that would be a matrix
[00:07:53.080 --> 00:07:55.920]   multiplication to do that composition.
[00:07:55.920 --> 00:07:56.600]   Exactly.
[00:07:56.600 --> 00:07:56.960]   Yeah.
[00:07:56.960 --> 00:07:57.480]   Okay.
[00:07:57.480 --> 00:07:59.320]   So order matters here.
[00:07:59.320 --> 00:07:59.680]   So.
[00:07:59.680 --> 00:08:00.760]   Order does matter.
[00:08:00.760 --> 00:08:04.320]   Unlike normal multiplication with matrix multiplication, order matters.
[00:08:04.320 --> 00:08:06.960]   Just like with function composition, order matters.
[00:08:06.960 --> 00:08:07.480]   Okay.
[00:08:07.480 --> 00:08:09.600]   My intuition is it's backwards.
[00:08:09.600 --> 00:08:12.720]   When we write functions, we write the last function first.
[00:08:12.800 --> 00:08:15.800]   Which is just like this artifact of kind of the history of mathematics and how
[00:08:15.800 --> 00:08:19.480]   people were thinking about functions when they first came up with the notation versus
[00:08:19.480 --> 00:08:20.880]   now, but yeah, you're exactly right.
[00:08:20.880 --> 00:08:25.040]   It works backwards with the normal way of writing matrix multiplication and the
[00:08:25.040 --> 00:08:27.400]   normal conventions with the normal way of writing functions.
[00:08:27.400 --> 00:08:30.360]   There are alternative conventions that make it read left to right.
[00:08:30.360 --> 00:08:34.400]   I guess if you speak maybe Hebrew or Arabic, then this might feel a little bit
[00:08:34.400 --> 00:08:37.320]   more natural to you to go from right to left, but yeah, you're right.
[00:08:37.320 --> 00:08:37.920]   It's reversed.
[00:08:37.920 --> 00:08:38.920]   So let's check this one.
[00:08:38.920 --> 00:08:39.840]   I think this is right.
[00:08:39.840 --> 00:08:40.240]   Okay.
[00:08:40.240 --> 00:08:42.800]   Well then I use the grader to check whether my answers are right too.
[00:08:42.800 --> 00:08:43.240]   Nice.
[00:08:43.240 --> 00:08:45.600]   So I'm pretty happy that I redeemed myself there.
[00:08:45.600 --> 00:08:46.520]   And I nailed it.
[00:08:46.520 --> 00:08:47.080]   Okay.
[00:08:47.080 --> 00:08:47.520]   Nice.
[00:08:47.520 --> 00:08:52.160]   Yeah, no, I've placed many traps to, to maximize the educational impact.
[00:08:52.160 --> 00:08:56.480]   Just as when programming, we can write just one function that does multiple steps or
[00:08:56.480 --> 00:08:58.000]   write a separate function for each step.
[00:08:58.000 --> 00:09:01.560]   When we're in linear algebra, we can either do one matrix to apply all of our
[00:09:01.560 --> 00:09:04.080]   transformations or a separate matrix for each transformation.
[00:09:04.080 --> 00:09:07.760]   And there's just like with programming, sometimes that can have performance.
[00:09:07.840 --> 00:09:10.120]   Either there can be performance reasons for doing one or the other.
[00:09:10.120 --> 00:09:12.640]   And sometimes there's clarity reasons for doing one or the other.
[00:09:12.640 --> 00:09:17.200]   It's clearer often if you've got like four different names for four different
[00:09:17.200 --> 00:09:21.480]   steps of your pipeline to write them all out so that somebody else coming in to
[00:09:21.480 --> 00:09:24.520]   change your code or understand what it's doing can see it.
[00:09:24.520 --> 00:09:29.080]   Other times, maybe that slows you down by a factor of 10 and that's not a good idea.
[00:09:29.080 --> 00:09:32.120]   With that in mind, we're going to do a sort of word problem
[00:09:32.120 --> 00:09:33.760]   version of this coding exercise.
[00:09:34.560 --> 00:09:38.680]   So you're looking through a fellow developer's code and you notice that in
[00:09:38.680 --> 00:09:43.000]   their pipeline, which is written as a function below, a large amount of data is
[00:09:43.000 --> 00:09:45.680]   being passed through four successive matrix operations.
[00:09:45.680 --> 00:09:53.680]   In order, the data vectors, V, are multiplied by W, then X, then Y, and finally by Z.
[00:09:53.680 --> 00:09:57.960]   And you think you can get a, maybe squeeze out some additional performance by turning
[00:09:57.960 --> 00:10:01.480]   this into one matrix multiplication, call it capital V.
[00:10:01.560 --> 00:10:06.120]   So what I want you to do here, Scott, is use matrix multiplication
[00:10:06.120 --> 00:10:09.000]   to define that array V here.
[00:10:09.000 --> 00:10:09.560]   Okay.
[00:10:09.560 --> 00:10:17.320]   So what I'm trying to think of is like, can I just do the last first and then do
[00:10:17.320 --> 00:10:23.000]   times Y times, times X times W.
[00:10:23.000 --> 00:10:26.200]   I'm not, like, first of all, is this legal?
[00:10:26.200 --> 00:10:27.800]   And then second of all, does this work?
[00:10:27.840 --> 00:10:32.520]   I can at least tell you that's legal, but go ahead and run those two cells above
[00:10:32.520 --> 00:10:36.120]   first, so that the variables of those matrices are defined.
[00:10:36.120 --> 00:10:37.360]   So yeah, that's legal.
[00:10:37.360 --> 00:10:37.840]   Great.
[00:10:37.840 --> 00:10:41.160]   I don't actually remember most, you know, most Python syntax.
[00:10:41.160 --> 00:10:41.880]   I don't remember it.
[00:10:41.880 --> 00:10:45.240]   I just have a linter to check it for me or I run the cell and see if it works.
[00:10:45.240 --> 00:10:48.760]   I keep those, I keep that space in my brain for useful information, Lord
[00:10:48.760 --> 00:10:50.240]   of the Rings lore and mathematics.
[00:10:50.240 --> 00:10:51.160]   Let's check if it's right.
[00:10:51.160 --> 00:10:52.480]   Let's run the grader and see what happens.
[00:10:52.480 --> 00:10:53.160]   Oh, great.
[00:10:53.160 --> 00:10:53.920]   Nailed it.
[00:10:53.920 --> 00:10:55.440]   So just a quick note.
[00:10:55.560 --> 00:10:58.080]   One purpose of this question is to make sure that people have figured
[00:10:58.080 --> 00:10:59.880]   out that like left, right thing.
[00:10:59.880 --> 00:11:01.720]   That tricky thing where it's not written in the right way.
[00:11:01.720 --> 00:11:06.920]   So just go ahead and humor me and write W X Y Z instead of Z Y X W.
[00:11:06.920 --> 00:11:07.880]   Oh, okay.
[00:11:07.880 --> 00:11:10.000]   I will drag them around.
[00:11:10.000 --> 00:11:12.680]   W X Y.
[00:11:12.680 --> 00:11:15.680]   It doesn't work.
[00:11:15.680 --> 00:11:18.080]   So make sure you multiply in the right order.
[00:11:18.080 --> 00:11:19.240]   That's the little test there.
[00:11:19.240 --> 00:11:21.680]   So that's intended for you to be like, oh wait, what is the
[00:11:21.680 --> 00:11:23.120]   right order to multiply things?
[00:11:23.120 --> 00:11:29.880]   It looks like what my test is checking is that it's checking if I multiply W X Y Z
[00:11:29.880 --> 00:11:32.880]   with a random vector, is it the same as what happens when I multiply
[00:11:32.880 --> 00:11:34.800]   your V with a random vector?
[00:11:34.800 --> 00:11:41.200]   That shouldn't be the case because your V should be Z Y X W instead of W X Y Z.
[00:11:41.200 --> 00:11:41.720]   Okay.
[00:11:41.720 --> 00:11:45.200]   And am I right in saying that like I can think of it like this?
[00:11:45.200 --> 00:11:46.640]   Does it evaluate in that way?
[00:11:46.640 --> 00:11:51.560]   That is a good question about the like operator precedence in Python, whether
[00:11:51.560 --> 00:11:55.280]   you would do it from right to left or left to right, so the first answer is
[00:11:55.280 --> 00:11:59.840]   that you could add parentheses and make it evaluate in any of those
[00:11:59.840 --> 00:12:01.440]   orders and it would be valid.
[00:12:01.440 --> 00:12:03.520]   So you did left to right.
[00:12:03.520 --> 00:12:05.400]   Um, you could also do it right to left.
[00:12:05.400 --> 00:12:11.280]   So first do X and W, then do Y with that, then do Z with the whole thing.
[00:12:11.280 --> 00:12:11.920]   Okay.
[00:12:11.920 --> 00:12:15.360]   So, so even something like this is right?
[00:12:15.360 --> 00:12:15.880]   Where?
[00:12:15.880 --> 00:12:16.880]   Yeah, go ahead and run that.
[00:12:16.880 --> 00:12:18.440]   I I'm pretty positive that'll work.
[00:12:18.440 --> 00:12:18.760]   Yeah.
[00:12:18.760 --> 00:12:19.320]   Okay, cool.
[00:12:19.360 --> 00:12:24.880]   So it doesn't matter what order they, they happen in terms of which goes first.
[00:12:24.880 --> 00:12:29.040]   It just matters like left to right, which if they're in the right order, then it works.
[00:12:29.040 --> 00:12:29.560]   Yes.
[00:12:29.560 --> 00:12:31.800]   This is called associativity.
[00:12:31.800 --> 00:12:33.200]   That's the math term for this.
[00:12:33.200 --> 00:12:36.960]   And it means that we don't need to care the sort of like order that we do these
[00:12:36.960 --> 00:12:41.240]   things in and it's super, that's like a very common axiomatic property of things
[00:12:41.240 --> 00:12:44.720]   we like to study in math is that the order of parentheses doesn't matter.
[00:12:44.720 --> 00:12:47.200]   It really makes our lives a lot easier to have that.
[00:12:47.480 --> 00:12:52.320]   Importantly, one thing that does mean is we wrote Y as just like a single thing.
[00:12:52.320 --> 00:12:55.520]   You took Y at X, you wrapped it in parentheses and you turn it.
[00:12:55.520 --> 00:12:57.920]   And basically what you did was you turn that into a single matrix.
[00:12:57.920 --> 00:12:58.320]   Yeah.
[00:12:58.320 --> 00:13:02.760]   Um, right now we have a bunch of single matrices here, Z, Y, X, W.
[00:13:02.760 --> 00:13:05.640]   There's no reason why we have to think of those as a single matrix.
[00:13:05.640 --> 00:13:09.320]   I could, I could turn Z into a composition of matrix
[00:13:09.320 --> 00:13:11.000]   multiplications if I wanted to.
[00:13:11.000 --> 00:13:16.480]   I can decompose anything down to it's like atomic elements and turn it basically
[00:13:16.480 --> 00:13:20.800]   into like individual numbers or individual vectors that have just one
[00:13:20.800 --> 00:13:25.200]   number that's not zero as maybe the like atomic element here and then stack them
[00:13:25.200 --> 00:13:28.920]   and combine them and matrix compose them to get the final big matrix.
[00:13:28.920 --> 00:13:32.760]   The nice thing about this view of linear algebras, it really emphasizes that what
[00:13:32.760 --> 00:13:37.600]   we are doing is stacking and composing these small, simple atomic pieces.
[00:13:37.600 --> 00:13:38.000]   Cool.
[00:13:38.000 --> 00:13:41.760]   Uh, so now we're going to, we're going to take a different tack
[00:13:41.760 --> 00:13:44.160]   on understanding what matrices do.
[00:13:44.280 --> 00:13:48.240]   So when you apply a matrix to a vector, it's like the right way to think
[00:13:48.240 --> 00:13:49.720]   about that matrix is as a function.
[00:13:49.720 --> 00:13:53.360]   When you have two matrices that you're multiplying together, there's kind of
[00:13:53.360 --> 00:13:57.120]   two ways that both might be good of thinking about it, and it depends
[00:13:57.120 --> 00:13:58.360]   on which application you're doing.
[00:13:58.360 --> 00:14:03.600]   If you're taking two matrices that are both going to in the end be multiplied
[00:14:03.600 --> 00:14:07.440]   with vectors to give you results, then it's nice to think of it as matrix
[00:14:07.440 --> 00:14:10.800]   composition, like we did with set second to zero and repeat three.
[00:14:10.800 --> 00:14:14.080]   Right way to think of that matrix multiplication as composing functions.
[00:14:14.720 --> 00:14:19.240]   But one common example where people do multiplication of matrices, when you're
[00:14:19.240 --> 00:14:23.640]   doing machine learning, you have batches of inputs, you have multiple inputs going
[00:14:23.640 --> 00:14:27.400]   in at once, and in that case, it is a matrix multiplication because you're
[00:14:27.400 --> 00:14:28.840]   multiplying two matrices together.
[00:14:28.840 --> 00:14:33.640]   But it's maybe more fruitful to think of the matrix multiplication as doing a kind
[00:14:33.640 --> 00:14:39.960]   of for loop, applying the same matrix on the left to a bunch of vectors one after
[00:14:39.960 --> 00:14:42.400]   another, where they're the columns of the input.
[00:14:42.440 --> 00:14:46.640]   So I think you may have come across like this idea of batches or mini batches of
[00:14:46.640 --> 00:14:48.320]   vectors in machine learning, right, Scott?
[00:14:48.320 --> 00:14:48.760]   Yeah.
[00:14:48.760 --> 00:14:49.160]   Yeah.
[00:14:49.160 --> 00:14:54.240]   So that's when you have say a big dataset and you want to just calculate your
[00:14:54.240 --> 00:14:57.600]   gradients on one small subset of that as you train.
[00:14:57.600 --> 00:14:58.280]   Right.
[00:14:58.280 --> 00:14:59.680]   Go ahead and run that cell actually.
[00:14:59.680 --> 00:15:02.440]   So we've got some like some concrete vectors to look at.
[00:15:02.440 --> 00:15:03.280]   Okay.
[00:15:03.280 --> 00:15:07.400]   So on the top there, we've got all the vectors put together into one matrix.
[00:15:07.400 --> 00:15:11.400]   And on the bottom, we have each vector printed out separately.
[00:15:11.640 --> 00:15:16.240]   So now we've got our batch of vectors on the top and then the
[00:15:16.240 --> 00:15:17.720]   separate vectors on the bottom.
[00:15:17.720 --> 00:15:20.880]   Just to confirm, where are each of those vectors in the bottom,
[00:15:20.880 --> 00:15:22.880]   from the bottom thing in our batch?
[00:15:22.880 --> 00:15:23.480]   Oh yeah.
[00:15:23.480 --> 00:15:24.560]   Nice little space there.
[00:15:24.560 --> 00:15:25.360]   Thank you, Scott.
[00:15:25.360 --> 00:15:26.400]   Okay.
[00:15:26.400 --> 00:15:31.440]   So yeah, a mini batch here in, as you're looping over this, is giving me like
[00:15:31.440 --> 00:15:35.320]   these two numbers and then these two numbers and then these, so it's like,
[00:15:35.320 --> 00:15:37.320]   uh, that flips, is that, is that correct?
[00:15:37.320 --> 00:15:37.560]   Yeah.
[00:15:37.560 --> 00:15:39.560]   It's each column is what we get.
[00:15:40.200 --> 00:15:44.800]   So fundamentally what we're doing here is when we think of matrices as functions,
[00:15:44.800 --> 00:15:47.440]   we're really thinking of them as a collection of row vectors.
[00:15:47.440 --> 00:15:47.920]   Yeah.
[00:15:47.920 --> 00:15:52.760]   Each row vector could be applied to a valid input to the matrix, and it would
[00:15:52.760 --> 00:15:54.920]   give you a single number just by the dot product.
[00:15:54.920 --> 00:15:58.400]   When we think of matrices as a batch of vectors, we're thinking of
[00:15:58.400 --> 00:15:59.960]   them in terms of their columns.
[00:15:59.960 --> 00:16:00.320]   Yeah.
[00:16:00.320 --> 00:16:00.640]   Okay.
[00:16:00.640 --> 00:16:04.760]   So both, like the default way that matrices are written in NumPy is to
[00:16:04.760 --> 00:16:06.440]   think of them in terms of their rows.
[00:16:06.440 --> 00:16:09.320]   And so to think of them in terms of functions, which is maybe a nice feature
[00:16:09.320 --> 00:16:12.720]   of NumPy, but we can always think of them in terms of their columns.
[00:16:12.720 --> 00:16:16.480]   And if we use the transpose, then we'll turn the rows into the columns.
[00:16:16.480 --> 00:16:17.000]   Okay.
[00:16:17.000 --> 00:16:21.160]   I guess where I'm thinking is how does this translate into
[00:16:21.160 --> 00:16:23.600]   function composition versus for loop?
[00:16:23.600 --> 00:16:24.160]   Right.
[00:16:24.160 --> 00:16:24.560]   Yeah.
[00:16:24.560 --> 00:16:26.480]   Uh, so maybe scrolling down a little bit.
[00:16:26.480 --> 00:16:31.200]   From this perspective, if we multiply a matrix F with a matrix batch of vectors,
[00:16:31.200 --> 00:16:34.560]   we're defining a new batch of vectors equal to F times each
[00:16:34.560 --> 00:16:35.960]   element of the original batch.
[00:16:36.000 --> 00:16:40.440]   So it's like a for loop where it's a for vector in batch of vectors, apply F to
[00:16:40.440 --> 00:16:44.120]   the first one, apply F to the second one, apply F to the left, to each one, and
[00:16:44.120 --> 00:16:46.600]   then return it as a new batch of vectors.
[00:16:46.600 --> 00:16:49.720]   So each one that I just named is a column of the output.
[00:16:49.720 --> 00:16:50.600]   Okay.
[00:16:50.600 --> 00:16:51.040]   Okay.
[00:16:51.040 --> 00:16:57.120]   So if, for example, say thinking of our previous example of set two to zero, if
[00:16:57.120 --> 00:17:03.040]   we had a bunch of things that we wanted to set two to zero, we would put them in
[00:17:03.040 --> 00:17:08.040]   this format where each column needs to have the set two zero, and then it'll do
[00:17:08.040 --> 00:17:11.720]   that, it'll set that two to zero for each thing in that input.
[00:17:11.720 --> 00:17:15.280]   Let's give that a shot and try set second to zero, just that one.
[00:17:15.280 --> 00:17:17.120]   And then at batch of vectors.
[00:17:17.120 --> 00:17:19.400]   There we go.
[00:17:19.400 --> 00:17:19.920]   Great.
[00:17:19.920 --> 00:17:30.160]   We've given it in these two rows and then it's transposed them and set the second
[00:17:30.760 --> 00:17:35.800]   to zero, which has given us, so that two is gone to zero, that four has gone to
[00:17:35.800 --> 00:17:38.080]   zero and the output is also transposed.
[00:17:38.080 --> 00:17:40.560]   Oh no, the output's the same shape as the output.
[00:17:40.560 --> 00:17:41.320]   It's the same shape.
[00:17:41.320 --> 00:17:41.800]   Yeah.
[00:17:41.800 --> 00:17:46.000]   Like the only reason I've got a transpose in there is just because by default, if
[00:17:46.000 --> 00:17:49.640]   you loop over a matrix in NumPy, you get the rows because NumPy is, it's called
[00:17:49.640 --> 00:17:53.480]   row major, we think of the rows first and the columns second.
[00:17:53.480 --> 00:17:56.840]   So I did that transpose just to loop over it, but inside the matrix
[00:17:56.840 --> 00:18:00.920]   multiplication, there's not necessarily going to be like transposing happening.
[00:18:00.920 --> 00:18:02.600]   Okay.
[00:18:02.600 --> 00:18:03.040]   Okay.
[00:18:03.040 --> 00:18:03.800]   That makes sense.
[00:18:03.800 --> 00:18:08.360]   So this is more just for, for visual, like to print it out.
[00:18:08.360 --> 00:18:13.040]   And because of the way for loops work in NumPy, but this is the right way to think
[00:18:13.040 --> 00:18:16.480]   about matrix multiplication is it is happening on the columns.
[00:18:16.480 --> 00:18:17.720]   Yeah.
[00:18:17.720 --> 00:18:20.680]   Think of the thing on the right as it's happening on the columns and the thing
[00:18:20.680 --> 00:18:22.800]   on the left as it's coming from the rows.
[00:18:22.800 --> 00:18:23.120]   Yeah.
[00:18:23.120 --> 00:18:23.800]   Okay, cool.
[00:18:23.800 --> 00:18:24.440]   Yeah.
[00:18:24.440 --> 00:18:27.840]   If you took a linear algebra class, the way that this idea is expressed is in
[00:18:27.840 --> 00:18:32.120]   terms of the row space and the column space, that's a common way of talking
[00:18:32.120 --> 00:18:36.040]   about it in mathematical terms, which emphasizes which vectors you can
[00:18:36.040 --> 00:18:40.920]   construct and stuff like that, which is, it's useful and it's good for analyzing
[00:18:40.920 --> 00:18:43.880]   whether matrix algorithms are going to converge and all this kind of stuff.
[00:18:43.880 --> 00:18:47.840]   But it's actually not my favorite way of thinking about what's happening in an
[00:18:47.840 --> 00:18:52.080]   actual concrete linear algebra program that I've written in a neural network.
[00:18:52.360 --> 00:18:55.640]   Rather than thinking in terms of these like spaces being transformed into one
[00:18:55.640 --> 00:18:58.440]   or another, I think of them in terms of these computational
[00:18:58.440 --> 00:18:59.720]   operations, like a for loop.
[00:18:59.720 --> 00:19:00.880]   Cool.
[00:19:00.880 --> 00:19:02.120]   Yeah, that, this is nice.
[00:19:02.120 --> 00:19:03.080]   That makes a lot more sense.
[00:19:03.080 --> 00:19:04.600]   It's, it's like a for loop.
[00:19:04.600 --> 00:19:07.720]   It's even maybe even more like a list comprehension.
[00:19:07.720 --> 00:19:10.960]   A list comprehension is all about constructing new lists from old
[00:19:10.960 --> 00:19:12.720]   lists by applying functions.
[00:19:12.720 --> 00:19:13.720]   Okay, cool.
[00:19:13.720 --> 00:19:18.320]   So what we're going to do here is we're going to write a function that makes this
[00:19:18.360 --> 00:19:23.080]   explicit, this idea that we're doing a for loop explicit, rather than just doing
[00:19:23.080 --> 00:19:27.160]   F at batch of vectors, which would apply the matrix F to each column
[00:19:27.160 --> 00:19:28.440]   of the matrix batch of vectors.
[00:19:28.440 --> 00:19:31.000]   We want to do it explicitly here.
[00:19:31.000 --> 00:19:31.520]   Okay.
[00:19:31.520 --> 00:19:35.960]   And I guess I'll try to do it within a list comprehension.
[00:19:35.960 --> 00:19:42.160]   So there's an ethos of programming that I like, which is make it run, make it right,
[00:19:42.160 --> 00:19:46.320]   make it fast, and those are three steps separate from one another, decomposed.
[00:19:46.320 --> 00:19:47.920]   So let's make it run first.
[00:19:48.600 --> 00:19:49.360]   That sounds good.
[00:19:49.360 --> 00:19:49.720]   Okay.
[00:19:49.720 --> 00:19:57.680]   So for that batch in batch of vectors, and then we do our transpose.
[00:19:57.680 --> 00:20:00.160]   I would think of it as vector in batch of vectors.
[00:20:00.160 --> 00:20:00.720]   Does that make sense?
[00:20:00.720 --> 00:20:01.200]   Oh, sorry.
[00:20:01.200 --> 00:20:01.440]   Yeah.
[00:20:01.440 --> 00:20:01.680]   Okay.
[00:20:01.680 --> 00:20:01.920]   Yeah.
[00:20:01.920 --> 00:20:04.600]   Vector in batch of vectors.
[00:20:04.600 --> 00:20:10.000]   And then I want to do out dot append, or I guess it's not append,
[00:20:10.000 --> 00:20:11.280]   it's more like extend, is it?
[00:20:11.280 --> 00:20:12.760]   Append'll, append'll work.
[00:20:12.760 --> 00:20:13.640]   Okay.
[00:20:13.720 --> 00:20:22.560]   Now I want to do F applied to, so can I do F as a function or it's a matrix?
[00:20:22.560 --> 00:20:26.480]   F is a matrix and you can, we know that it's, that it's compatible
[00:20:26.480 --> 00:20:28.120]   with, with the vectors that are coming in.
[00:20:28.120 --> 00:20:32.280]   Is it that like I'm, every time I'm going function and they're
[00:20:32.280 --> 00:20:34.080]   like thinking of it like input.
[00:20:34.080 --> 00:20:34.760]   Yeah.
[00:20:34.760 --> 00:20:36.120]   I have to go, okay, no.
[00:20:36.120 --> 00:20:37.520]   So it's the opposite of that.
[00:20:37.520 --> 00:20:38.160]   Cause.
[00:20:38.160 --> 00:20:42.840]   So it's actually, you had it right the first time it's F at vector.
[00:20:43.240 --> 00:20:43.680]   Oh, is it?
[00:20:43.680 --> 00:20:44.080]   Okay.
[00:20:44.080 --> 00:20:44.840]   Yeah.
[00:20:44.840 --> 00:20:48.360]   So with the inputs come in from the right and then they pass
[00:20:48.360 --> 00:20:49.760]   through a sequence of operations.
[00:20:49.760 --> 00:20:53.280]   You can imagine them going on like a little conveyor belt from right to left.
[00:20:53.280 --> 00:20:54.320]   From right to left.
[00:20:54.320 --> 00:20:54.640]   Okay.
[00:20:54.640 --> 00:20:55.200]   Yes.
[00:20:55.200 --> 00:20:56.080]   That makes sense.
[00:20:56.080 --> 00:20:58.720]   So this is getting applied to this.
[00:20:58.720 --> 00:21:00.800]   F is getting applied to the vector.
[00:21:00.800 --> 00:21:01.280]   Sorry.
[00:21:01.280 --> 00:21:01.400]   Yeah.
[00:21:01.400 --> 00:21:03.000]   Vector is getting fed to F.
[00:21:03.000 --> 00:21:03.280]   Yeah.
[00:21:03.280 --> 00:21:04.000]   That's getting fed.
[00:21:04.000 --> 00:21:05.320]   And that's your conveyor belt.
[00:21:05.320 --> 00:21:05.640]   Okay.
[00:21:05.640 --> 00:21:06.520]   That makes sense.
[00:21:06.520 --> 00:21:08.080]   And then I want to return F.
[00:21:08.080 --> 00:21:09.800]   Have you ever played Factorio, Scott?
[00:21:09.800 --> 00:21:12.160]   I have not played Factorio or heard of it.
[00:21:12.160 --> 00:21:13.160]   What is it?
[00:21:13.320 --> 00:21:16.240]   It's just a, it's a video game and it's got conveyor belts in it.
[00:21:16.240 --> 00:21:20.600]   So one quick thing, we want the return type to be an array.
[00:21:20.600 --> 00:21:24.320]   Just makes it easier for me to check whether your function is implemented correctly.
[00:21:24.320 --> 00:21:25.640]   So I would just.
[00:21:25.640 --> 00:21:26.880]   Oh yeah.
[00:21:26.880 --> 00:21:27.840]   Just wrap it here.
[00:21:27.840 --> 00:21:28.280]   Yeah.
[00:21:28.280 --> 00:21:28.960]   At the end.
[00:21:28.960 --> 00:21:33.680]   50% passed.
[00:21:33.680 --> 00:21:35.080]   50% passed.
[00:21:35.080 --> 00:21:36.200]   We're moving.
[00:21:36.200 --> 00:21:36.800]   Okay.
[00:21:36.800 --> 00:21:42.120]   So what this is, what this test is telling you is that if I apply the identity
[00:21:42.120 --> 00:21:45.360]   matrix to some vectors, I don't get the right answer.
[00:21:45.360 --> 00:21:48.840]   And the particular problem we're getting is rather than getting the wrong answer,
[00:21:48.840 --> 00:21:51.240]   we're getting an error and it's a shape error.
[00:21:51.240 --> 00:21:52.000]   Okay.
[00:21:52.000 --> 00:21:52.920]   I see this.
[00:21:52.920 --> 00:21:55.000]   Operations could not be broadcast with shapes.
[00:21:55.000 --> 00:21:56.000]   10, 5, 5, 10.
[00:21:56.000 --> 00:22:03.520]   Maybe for my own sake, I'll print out the shapes here of this and this.
[00:22:03.520 --> 00:22:07.040]   So I know what I'm actually printing.
[00:22:07.040 --> 00:22:08.520]   I'll print a space.
[00:22:11.320 --> 00:22:14.880]   And before running the grader, go ahead and insert a code cell above this one.
[00:22:14.880 --> 00:22:21.160]   And go ahead and do, use your apply to batch function on one of the functions
[00:22:21.160 --> 00:22:23.160]   we've defined before and our batch of vectors.
[00:22:23.160 --> 00:22:25.040]   And our batch of vectors.
[00:22:25.040 --> 00:22:25.720]   Oh yes.
[00:22:25.720 --> 00:22:26.080]   Okay.
[00:22:26.080 --> 00:22:27.480]   Good.
[00:22:27.480 --> 00:22:27.920]   Yeah.
[00:22:27.920 --> 00:22:28.120]   Okay.
[00:22:28.120 --> 00:22:29.200]   We've already defined that as well.
[00:22:29.200 --> 00:22:31.320]   Two.
[00:22:31.320 --> 00:22:32.760]   Oh, that's not very useful.
[00:22:32.760 --> 00:22:37.040]   Maybe I want to use a different one.
[00:22:37.040 --> 00:22:38.040]   Yeah.
[00:22:38.040 --> 00:22:38.680]   Let's see.
[00:22:38.680 --> 00:22:43.640]   We don't have any non-square matrices ready to go.
[00:22:43.640 --> 00:22:44.640]   Try repeat three, two.
[00:22:44.640 --> 00:22:45.000]   Yeah.
[00:22:45.000 --> 00:22:45.480]   Yeah.
[00:22:45.480 --> 00:22:46.000]   There it is.
[00:22:46.000 --> 00:22:47.040]   That's working.
[00:22:47.040 --> 00:22:48.040]   That works.
[00:22:48.040 --> 00:22:49.240]   Strange.
[00:22:49.240 --> 00:22:50.040]   All right.
[00:22:50.040 --> 00:22:51.280]   What's our bug here?
[00:22:51.280 --> 00:22:51.760]   All right.
[00:22:51.760 --> 00:22:54.040]   So repeat three, two on a batch of vectors.
[00:22:54.040 --> 00:22:56.840]   Go ahead and print batch of vectors for me.
[00:22:56.840 --> 00:22:57.760]   Okay.
[00:22:57.760 --> 00:22:58.160]   Okay.
[00:22:58.160 --> 00:22:59.120]   I think I see the problem.
[00:22:59.120 --> 00:23:04.200]   So remember the batch of vectors, each column is an input.
[00:23:04.360 --> 00:23:08.920]   And what we'd love is if our output, you know, looks like our input in that
[00:23:08.920 --> 00:23:12.320]   each column of the output corresponds to a column of the input.
[00:23:12.320 --> 00:23:13.920]   Nailed it.
[00:23:13.920 --> 00:23:14.320]   Yeah.
[00:23:14.320 --> 00:23:15.040]   Let's try it.
[00:23:15.040 --> 00:23:18.360]   So what Scott did was transpose the output.
[00:23:18.360 --> 00:23:19.000]   Okay.
[00:23:19.000 --> 00:23:21.160]   So let's see how that works then.
[00:23:21.160 --> 00:23:22.080]   Error.
[00:23:22.080 --> 00:23:22.400]   Ah.
[00:23:22.400 --> 00:23:23.480]   Great book, gosh.
[00:23:23.480 --> 00:23:23.800]   Okay.
[00:23:23.800 --> 00:23:26.000]   So we don't, the prints are killing us.
[00:23:26.000 --> 00:23:27.120]   So we have to get rid of that.
[00:23:27.120 --> 00:23:27.600]   Yeah.
[00:23:27.600 --> 00:23:32.320]   The way that this greeter works is it's how people check doc strings in Python.
[00:23:32.480 --> 00:23:35.680]   And it's very sensitive to how that, what the outputs look like very precisely.
[00:23:35.680 --> 00:23:37.360]   Well, we've got two paths there.
[00:23:37.360 --> 00:23:38.440]   So that nailed it.
[00:23:38.440 --> 00:23:38.760]   Yeah.
[00:23:38.760 --> 00:23:42.000]   So tracking those transposes and shapes and keeping track of what's a row and
[00:23:42.000 --> 00:23:44.880]   what's a, what's a column is one of the trickiest parts.
[00:23:44.880 --> 00:23:49.560]   And the unfortunate news is that each matrix library has maybe slightly
[00:23:49.560 --> 00:23:53.240]   different conventions, like PyTorch, I think actually likes to think of it as
[00:23:53.240 --> 00:23:56.400]   going from left to right, rather than right to left, like they, they just
[00:23:56.400 --> 00:23:59.560]   transpose everything relative to the normal convention and you kind of got
[00:23:59.560 --> 00:24:03.560]   to like develop some heuristics, check shapes, like we just did, think through
[00:24:03.560 --> 00:24:06.880]   whiteboard stuff and make sure you've got all your conventions in order
[00:24:06.880 --> 00:24:08.480]   to solve these kinds of problems.
[00:24:08.480 --> 00:24:10.640]   I've been burned by this many a time.
[00:24:10.640 --> 00:24:11.560]   Yeah.
[00:24:11.560 --> 00:24:15.640]   You know, even something like you get your output and then you want to reshape
[00:24:15.640 --> 00:24:17.760]   it to like be the shape you expect it to.
[00:24:17.760 --> 00:24:21.760]   That can get you a lot as well, because you might have accidentally
[00:24:21.760 --> 00:24:23.200]   transposed it somewhere in the middle.
[00:24:23.200 --> 00:24:27.440]   And now your, your batch is actually your feature dimension or something like this.
[00:24:27.920 --> 00:24:30.080]   And that, that has gotten me many times.
[00:24:30.080 --> 00:24:33.880]   I think it was Andrej Karpathy at a blog, be very paranoid and visualize
[00:24:33.880 --> 00:24:35.440]   every single input and output.
[00:24:35.440 --> 00:24:38.120]   And that's something that I've, I've learned the hard way, I think
[00:24:38.120 --> 00:24:39.280]   doing machine learning projects.
[00:24:39.280 --> 00:24:40.080]   100%.
[00:24:40.080 --> 00:24:44.640]   And in fact, the, the future definitely, I think within a year, maybe two years,
[00:24:44.640 --> 00:24:47.880]   depending on how quickly like some research and development on this goes,
[00:24:47.880 --> 00:24:52.560]   it should be easier to work with tensors in some of the main tensor libraries,
[00:24:52.560 --> 00:24:56.080]   because people are working on typing for tensors that keeps track of the shapes
[00:24:56.080 --> 00:24:59.440]   for you and before you even run your code, it'll say, Hey, wait a second.
[00:24:59.440 --> 00:25:00.560]   These two things don't line up.
[00:25:00.560 --> 00:25:04.720]   And there's just some, there's some like really gnarly tricky math and
[00:25:04.720 --> 00:25:07.360]   programming language theory stuff that has to be figured out and then
[00:25:07.360 --> 00:25:10.200]   implemented in Python before that can happen, but it'll make
[00:25:10.200 --> 00:25:11.400]   all of our lives easier soon.
[00:25:11.400 --> 00:25:12.280]   That sounds great.
[00:25:12.280 --> 00:25:12.960]   Yeah.
[00:25:12.960 --> 00:25:14.040]   All right.
[00:25:14.040 --> 00:25:17.640]   Uh, so we talked about one form of parallelization in linear
[00:25:17.640 --> 00:25:19.440]   algebra, which is like doing a for loop.
[00:25:19.440 --> 00:25:24.240]   And kind of like, especially if you run that on the GPU and on a lot of modern
[00:25:24.240 --> 00:25:27.200]   CPUs, the actual computation is going to look very different from what
[00:25:27.200 --> 00:25:28.480]   you just wrote in apply to batch.
[00:25:28.480 --> 00:25:31.720]   In apply to batch, we were very explicit that we want to grab each
[00:25:31.720 --> 00:25:34.560]   vector, apply matrix to it, return it to the output.
[00:25:34.560 --> 00:25:38.160]   And while that's going to give you the same result, it's way slower than the
[00:25:38.160 --> 00:25:43.560]   actual NumPy code will be because it gets parallelized under the hood in a smart way.
[00:25:43.560 --> 00:25:44.000]   Okay.
[00:25:44.000 --> 00:25:47.400]   So what we're going to talk about now is a different type of parallelization.
[00:25:47.400 --> 00:25:52.840]   One type of parallel programming is let's apply the same operation to multiple inputs.
[00:25:52.880 --> 00:25:57.040]   The other type of parallel programming is what if we applied multiple operations
[00:25:57.040 --> 00:25:59.160]   to the same input at the same time?
[00:25:59.160 --> 00:25:59.520]   Yeah.
[00:25:59.520 --> 00:26:03.280]   For example, maybe I have two matrices that I want to apply to an input
[00:26:03.280 --> 00:26:05.920]   and I want to get those two outputs.
[00:26:05.920 --> 00:26:09.720]   Like I want to get both the like length three output of one matrix and the length
[00:26:09.720 --> 00:26:12.480]   four output of the other matrix on some input.
[00:26:12.480 --> 00:26:18.040]   We can do that just by stacking matrices, stacking two matrices on top of each other.
[00:26:18.040 --> 00:26:21.000]   So is that on a diff, under like a new dimension?
[00:26:21.040 --> 00:26:25.800]   If I've got like a two by two matrix that I'm applying and then another two by two,
[00:26:25.800 --> 00:26:31.240]   is it just going to be a two by four or is it going to be two and then two by twos?
[00:26:31.240 --> 00:26:32.360]   That's a great question.
[00:26:32.360 --> 00:26:37.720]   So you could do it either way, but the main way in order to keep things like
[00:26:37.720 --> 00:26:41.600]   matrices and vectors, the sort of like default way to do it is to stack them.
[00:26:41.600 --> 00:26:45.040]   And so you would get a, in the case that you described, you'd get a length four
[00:26:45.040 --> 00:26:49.440]   vector as output instead of getting something with an extra dimension added
[00:26:49.440 --> 00:26:53.680]   onto it, but the fun thing is that so long as the number of entries are the same,
[00:26:53.680 --> 00:26:54.800]   you can always just reshape.
[00:26:54.800 --> 00:26:57.640]   I can take a matrix that's two by two and turn it into a length four vector.
[00:26:57.640 --> 00:27:01.800]   I could take a length 10 vector and turn it into a two by five or a five by two or a 10
[00:27:01.800 --> 00:27:02.360]   by one.
[00:27:02.360 --> 00:27:06.480]   Often you can reshape things and turn what used to be an operation on vectors to an
[00:27:06.480 --> 00:27:10.520]   operation, operation on matrices or upgrade it to tensors with 10 dimensions or
[00:27:10.520 --> 00:27:10.920]   whatever.
[00:27:10.920 --> 00:27:14.640]   And as long as you're keeping track of the shapes, you can always map them onto each
[00:27:14.640 --> 00:27:14.880]   other.
[00:27:14.920 --> 00:27:19.600]   So it's a convention that we do this stacking directly on the vectors rather than
[00:27:19.600 --> 00:27:20.280]   anything else.
[00:27:20.280 --> 00:27:21.120]   Cool.
[00:27:21.120 --> 00:27:22.720]   Let's go ahead and run this cell for me.
[00:27:22.720 --> 00:27:27.920]   So we're doing here is we're taking a matrix that's this one, two, three, four,
[00:27:27.920 --> 00:27:31.760]   five, six, seven, eight, and splitting it up into its rows first.
[00:27:31.760 --> 00:27:38.040]   And then we're checking whether taking the matrix and multiplying it by the vector by
[00:27:38.040 --> 00:27:43.080]   some vector gives us the same result as taking each row and multiplying it with the
[00:27:43.080 --> 00:27:44.520]   vector for all of the rows.
[00:27:44.600 --> 00:27:45.000]   Okay.
[00:27:45.000 --> 00:27:48.520]   I'm going to really quickly print what rows is here.
[00:27:48.520 --> 00:27:49.160]   Good idea.
[00:27:49.160 --> 00:27:50.920]   So it's now each one of these.
[00:27:50.920 --> 00:27:51.120]   Yeah.
[00:27:51.120 --> 00:27:52.920]   So this is what I was hoping it will be.
[00:27:52.920 --> 00:27:53.560]   Cool.
[00:27:53.560 --> 00:28:02.160]   Then we have a matrix times this vector and we want to just check if this is the same
[00:28:02.160 --> 00:28:04.200]   as this stacked.
[00:28:04.200 --> 00:28:04.640]   Right.
[00:28:04.640 --> 00:28:09.040]   We just got to do a quick thing where we, where we make sure that the shapes are
[00:28:09.040 --> 00:28:09.400]   right.
[00:28:09.560 --> 00:28:14.280]   Um, that, that stack there is sort of putting the results back into the right
[00:28:14.280 --> 00:28:15.160]   final shape there.
[00:28:15.160 --> 00:28:15.560]   Okay.
[00:28:15.560 --> 00:28:18.120]   Before I might want to see what this looks like.
[00:28:18.120 --> 00:28:19.840]   It, oh, that's already happening here actually.
[00:28:19.840 --> 00:28:20.120]   Okay.
[00:28:20.120 --> 00:28:20.680]   Yes.
[00:28:20.680 --> 00:28:25.560]   So that is this print that and print that.
[00:28:25.560 --> 00:28:25.880]   Yeah.
[00:28:25.880 --> 00:28:27.440]   Okay.
[00:28:27.440 --> 00:28:32.360]   So it's the same, but by doing stack, it ends up turning it into a, a number
[00:28:32.360 --> 00:28:35.120]   array that's not like this, but it's column.
[00:28:35.120 --> 00:28:36.640]   These are each column.
[00:28:36.640 --> 00:28:37.240]   That one.
[00:28:37.240 --> 00:28:37.800]   Is that correct?
[00:28:37.800 --> 00:28:38.040]   Yeah.
[00:28:38.040 --> 00:28:38.480]   Yeah.
[00:28:38.480 --> 00:28:38.960]   You got it.
[00:28:39.080 --> 00:28:42.080]   So it's an annoying little bit that like H stack there is just like an annoying
[00:28:42.080 --> 00:28:45.560]   little trick to like really quickly turn it into an array and have it be the right
[00:28:45.560 --> 00:28:45.840]   shape.
[00:28:45.840 --> 00:28:49.360]   The important thing is that what, what is the matrix doing?
[00:28:49.360 --> 00:28:52.480]   It's taking each row and applying it to the input vector.
[00:28:52.480 --> 00:28:57.000]   So when I do matrix at vector, it's the same thing as doing a for loop over the
[00:28:57.000 --> 00:28:59.800]   rows where I apply each row to the vector.
[00:28:59.800 --> 00:29:03.160]   So these are the two dual or complimentary.
[00:29:03.160 --> 00:29:04.120]   You might think of it.
[00:29:04.120 --> 00:29:08.920]   Views of matrices either when, when we have two matrices multiplied together, we
[00:29:08.920 --> 00:29:13.440]   can either think of it as a batch of inputs going into the matrix on the left.
[00:29:13.440 --> 00:29:20.040]   And I'm transforming each one, one after another, or I can think of the rows of
[00:29:20.040 --> 00:29:23.440]   the matrix as applying individual functions to each input.
[00:29:23.440 --> 00:29:24.000]   Okay.
[00:29:24.000 --> 00:29:24.920]   Okay.
[00:29:24.920 --> 00:29:28.800]   So in order to get our final matrix, we're kind of building it by stacking
[00:29:28.800 --> 00:29:30.320]   the rows on top of each other here.
[00:29:30.320 --> 00:29:34.400]   But there's not really anything special about a like matrix with only one row here.
[00:29:34.400 --> 00:29:36.920]   We could also stack matrices with multiple rows.
[00:29:37.240 --> 00:29:40.880]   And that's how we're able to get this parallel programming thing where we
[00:29:40.880 --> 00:29:46.160]   stack one matrix on top of another and apply basically two functions to the same input.
[00:29:46.160 --> 00:29:46.920]   Okay.
[00:29:46.920 --> 00:29:50.000]   And even it, and it just matters that they're stackable.
[00:29:50.000 --> 00:29:54.240]   If you stack, say one matrix and another matrix, which are the two functions you
[00:29:54.240 --> 00:29:58.680]   want to apply, what, will they not like bleed in or like be affect each other?
[00:29:58.680 --> 00:29:59.680]   They're completely independent.
[00:29:59.680 --> 00:30:01.120]   They're completely independent.
[00:30:01.120 --> 00:30:05.000]   And importantly, the only thing that matters is that they need to both be
[00:30:05.000 --> 00:30:06.920]   functions that can take the same input.
[00:30:06.960 --> 00:30:11.960]   It wouldn't make sense to try and apply both a function that checks how long a
[00:30:11.960 --> 00:30:16.080]   string is and a function that checks if a number is greater than 140 to the same
[00:30:16.080 --> 00:30:18.840]   input, because you can't be both a string and a number.
[00:30:18.840 --> 00:30:23.120]   So similarly, we want our matrices to have the same number of columns.
[00:30:23.120 --> 00:30:23.600]   Okay.
[00:30:23.600 --> 00:30:24.560]   Okay, cool.
[00:30:24.560 --> 00:30:29.480]   So, so long as your new function has the same number of columns, it can be applied.
[00:30:29.480 --> 00:30:33.880]   So it could have one function that's like this, and then another function like this.
[00:30:34.360 --> 00:30:38.440]   Stack them and it's like being, it's like composing them or doing them in parallel.
[00:30:38.440 --> 00:30:40.000]   Composing them in parallel.
[00:30:40.000 --> 00:30:40.440]   Exactly.
[00:30:40.440 --> 00:30:43.920]   And in fact, you picked the exact example that I'm going to use in this next cell here.
[00:30:43.920 --> 00:30:45.080]   So if you run the next cell.
[00:30:45.080 --> 00:30:45.480]   Okay.
[00:30:45.480 --> 00:30:46.120]   Oh, nice.
[00:30:46.120 --> 00:30:46.640]   This.
[00:30:46.640 --> 00:30:51.800]   So some of those prints are a little ugly there.
[00:30:51.800 --> 00:30:52.720]   Sorry about that.
[00:30:52.720 --> 00:30:54.160]   We've got the matrix, right?
[00:30:54.160 --> 00:30:56.360]   Maybe actually you maybe walk us through what we've printed here.
[00:30:56.360 --> 00:30:56.680]   Okay.
[00:30:56.680 --> 00:30:58.600]   So I'm going to do my space trick.
[00:30:58.600 --> 00:31:01.080]   Here's just this printed.
[00:31:01.080 --> 00:31:02.080]   There's all our columns.
[00:31:02.200 --> 00:31:08.240]   And then you split it and then you do the same here, but with the other side.
[00:31:08.240 --> 00:31:11.160]   So this is from like zero to two and this from two to the end.
[00:31:11.160 --> 00:31:17.120]   And that ends up being this, which is this little sub matrix.
[00:31:17.120 --> 00:31:19.440]   And then this is the final part.
[00:31:19.440 --> 00:31:26.640]   And finally, then you do what we did in the last thing, but this time you're
[00:31:26.640 --> 00:31:30.120]   looping over the splits and you're applying them rather than looping over the rows.
[00:31:30.120 --> 00:31:30.840]   Ah, interesting.
[00:31:30.840 --> 00:31:35.000]   So you're going this and then this as opposed to row, row.
[00:31:35.000 --> 00:31:38.360]   And this kind of ties into the fact what you were telling me earlier
[00:31:38.360 --> 00:31:39.880]   is to how they compose nicely.
[00:31:39.880 --> 00:31:40.440]   Right.
[00:31:40.440 --> 00:31:40.680]   Yeah.
[00:31:40.680 --> 00:31:44.320]   So we just check in that last bit, the bit that we're getting the same answers
[00:31:44.320 --> 00:31:48.560]   here and that H stack again is just to get, if I do these, if I do these, if I do
[00:31:48.560 --> 00:31:52.120]   this form style, like I'm adding some extra dimensions, they just need to get
[00:31:52.120 --> 00:31:55.480]   like removed or squeezed out and that's what's that's what's happening in that H
[00:31:55.480 --> 00:31:55.880]   stack there.
[00:31:55.880 --> 00:31:59.040]   So actually I was, cause last time I was, I was looking at this and I was like, why
[00:31:59.040 --> 00:32:02.800]   isn't this just that because that would have worked just fine for this array.
[00:32:02.800 --> 00:32:05.440]   And now I realize why you did it ahead of time.
[00:32:05.440 --> 00:32:08.640]   Cause it was, you were like, if I do it array here and then I introduce
[00:32:08.640 --> 00:32:10.280]   H stack here, it makes it confusing.
[00:32:10.280 --> 00:32:13.800]   But because you do H stack in both, it's just saying this will just get rid of
[00:32:13.800 --> 00:32:17.160]   these like little annoying middle parentheses.
[00:32:17.160 --> 00:32:18.000]   Is that, am I correct?
[00:32:18.000 --> 00:32:18.400]   Yeah.
[00:32:18.400 --> 00:32:22.880]   This is a list with two length, two arrays in it, which, you know, a human
[00:32:22.880 --> 00:32:25.920]   can look at that and say, oh yeah, those are the, those two things are the same,
[00:32:25.920 --> 00:32:28.160]   but they're not exactly the same.
[00:32:28.160 --> 00:32:29.920]   They're just, there's like an equivalence.
[00:32:29.920 --> 00:32:33.280]   I can turn one into the other really easily and, but computers are very pedantic.
[00:32:33.280 --> 00:32:35.200]   And so they will say, no, those are not the same.
[00:32:35.200 --> 00:32:38.200]   So H stack is what's making them actually the same as each other.
[00:32:38.200 --> 00:32:38.840]   Okay.
[00:32:38.840 --> 00:32:40.000]   So yeah, there's that.
[00:32:40.000 --> 00:32:43.160]   I just added that there just to see that it printed correctly.
[00:32:43.160 --> 00:32:45.080]   And this is interesting.
[00:32:45.080 --> 00:32:46.440]   I don't think I've ever used that.
[00:32:46.440 --> 00:32:47.640]   So this is good to know.
[00:32:47.640 --> 00:32:51.960]   H stack and V stack are these two functions that you can use to
[00:32:51.960 --> 00:32:54.360]   combine arrays in a nice way.
[00:32:54.480 --> 00:32:58.560]   You may not, like if you're using a higher level library on top of, of an array
[00:32:58.560 --> 00:33:01.640]   library, you might not need them that much because they'll do this sort of thing for
[00:33:01.640 --> 00:33:03.960]   you, but it's a useful way to play around with it.
[00:33:03.960 --> 00:33:06.920]   It's, these sorts of things are actually much more common in MATLAB, which is my
[00:33:06.920 --> 00:33:09.760]   first programming language, like these kinds of array operations.
[00:33:09.760 --> 00:33:12.400]   I also did MATLAB in my undergrad.
[00:33:12.400 --> 00:33:14.920]   It's a, it's a gateway drug to neural, neural networks.
[00:33:14.920 --> 00:33:16.760]   All right.
[00:33:16.760 --> 00:33:20.800]   So let's, let's close out here by working using this V stack function.
[00:33:21.480 --> 00:33:26.200]   In addition to H stack, there's also this function V stack that basically undoes
[00:33:26.200 --> 00:33:28.680]   that splitting operation that I did above.
[00:33:28.680 --> 00:33:32.480]   So I had that's that splits equals matrix up to two and two to the end.
[00:33:32.480 --> 00:33:33.760]   So I split it by its rows.
[00:33:33.760 --> 00:33:38.840]   V stack can basically undo any way you might've split up a matrix by its rows
[00:33:38.840 --> 00:33:42.120]   into like maybe, you know, more than two pieces, like 10 pieces or something.
[00:33:42.120 --> 00:33:47.640]   So V stack will vertically stack the input arrays on top of one another.
[00:33:47.720 --> 00:33:52.320]   So it works on any matrices that have the same number of columns.
[00:33:52.320 --> 00:33:54.840]   What does it mean for two matrices to have the same number of columns?
[00:33:54.840 --> 00:33:57.800]   It means that they can operate on the same input.
[00:33:57.800 --> 00:34:01.680]   So V stack is our parallel composition tool.
[00:34:01.680 --> 00:34:05.880]   Just like at was our serial composition tool, that matrix multiplication was in
[00:34:05.880 --> 00:34:10.480]   order to apply to matrices in serial to an input, now we're thinking of applying
[00:34:10.480 --> 00:34:12.840]   matrices in parallel to an input.
[00:34:12.840 --> 00:34:13.360]   Okay.
[00:34:13.360 --> 00:34:17.600]   So just so I understand this here, I I'm going to really quickly try something.
[00:34:17.600 --> 00:34:22.200]   So if I have an array and it's that's a one dimension and another
[00:34:22.200 --> 00:34:25.600]   array that's the same size, okay.
[00:34:25.600 --> 00:34:29.160]   And I want to just stack them.
[00:34:29.160 --> 00:34:33.200]   Could I do MP dot V stack or V stack on that?
[00:34:33.200 --> 00:34:33.480]   Yeah.
[00:34:33.480 --> 00:34:35.080]   You got a comma at the beginning.
[00:34:35.080 --> 00:34:36.080]   Oh, sorry.
[00:34:36.080 --> 00:34:36.920]   Yeah.
[00:34:36.920 --> 00:34:37.200]   Okay.
[00:34:37.200 --> 00:34:39.040]   So that that's, that makes sense.
[00:34:39.040 --> 00:34:44.120]   Instead of merging them where it would be like zero, zero in a list, it puts them
[00:34:44.120 --> 00:34:46.320]   like, it's like merging them column wise.
[00:34:46.320 --> 00:34:46.720]   Is that?
[00:34:46.720 --> 00:34:47.080]   Yeah.
[00:34:47.080 --> 00:34:47.480]   Okay.
[00:34:47.640 --> 00:34:47.880]   Cool.
[00:34:47.880 --> 00:34:48.280]   Yep.
[00:34:48.280 --> 00:34:53.720]   So literally what it does is it like merges them column wise, but my preferred
[00:34:53.720 --> 00:34:59.680]   intuition for what V stack is for is for taking things that I might want to apply
[00:34:59.680 --> 00:35:02.800]   separately to the same input and parallel composing them.
[00:35:02.800 --> 00:35:04.920]   So stacking them on top of each other.
[00:35:04.920 --> 00:35:05.400]   Okay.
[00:35:05.400 --> 00:35:05.840]   Um, yeah.
[00:35:05.840 --> 00:35:06.360]   So sorry.
[00:35:06.360 --> 00:35:11.840]   Just again, that, what, what one example is in, in terms of your functions, that
[00:35:11.840 --> 00:35:17.720]   would be like set second to zero and then another function you want to apply in
[00:35:17.720 --> 00:35:23.960]   parallel, which would be repeat three, two, and then I've got now, you'll want
[00:35:23.960 --> 00:35:30.160]   to turn that into a list, but, and now I have, so one of these was set second to
[00:35:30.160 --> 00:35:37.080]   zero, this one here, and then the rest of these were this repeat three, two, and
[00:35:37.080 --> 00:35:38.960]   then they're composed, stacked on top of each other.
[00:35:38.960 --> 00:35:39.600]   That's cool.
[00:35:39.720 --> 00:35:44.160]   Uh, we're now going to use V stack to define what's kind of like a higher
[00:35:44.160 --> 00:35:46.360]   order function in linear algebra.
[00:35:46.360 --> 00:35:51.400]   This is a function that creates a matrix that repeats vectors of
[00:35:51.400 --> 00:35:53.600]   length in K times in their output.
[00:35:53.600 --> 00:35:58.760]   So this guy is going to create a matrix for you that, that can do something like
[00:35:58.760 --> 00:36:03.640]   repeat three, two, but for any choice of length of input, which is two and number
[00:36:03.640 --> 00:36:05.040]   of times repeated, which was three.
[00:36:05.040 --> 00:36:05.480]   Okay.
[00:36:05.480 --> 00:36:07.680]   Let's say I didn't have to take these into consideration.
[00:36:08.560 --> 00:36:11.840]   If this was just like make a three, two repeater, I could just
[00:36:11.840 --> 00:36:14.440]   return one of these or repeat three.
[00:36:14.440 --> 00:36:14.840]   Yeah.
[00:36:14.840 --> 00:36:18.480]   And that would be for three and two or let's say, is that right?
[00:36:18.480 --> 00:36:18.680]   Yeah.
[00:36:18.680 --> 00:36:19.120]   Yeah.
[00:36:19.120 --> 00:36:19.280]   Yeah.
[00:36:19.280 --> 00:36:20.360]   For three and two.
[00:36:20.360 --> 00:36:20.960]   Cool.
[00:36:20.960 --> 00:36:21.440]   Okay.
[00:36:21.440 --> 00:36:24.400]   I'm going to just remind myself something.
[00:36:24.400 --> 00:36:26.800]   I'm going to remind myself what repeat three, two looks like.
[00:36:26.800 --> 00:36:27.000]   Yeah.
[00:36:27.000 --> 00:36:30.520]   That was repeat three, two.
[00:36:30.520 --> 00:36:35.160]   That applies an input of size N and that repeats at K times.
[00:36:36.080 --> 00:36:38.120]   There's two hints underneath this question.
[00:36:38.120 --> 00:36:40.080]   You might want to just quickly take a look at them.
[00:36:40.080 --> 00:36:40.640]   Okay.
[00:36:40.640 --> 00:36:47.160]   Calling the function mp.i with argument K returns a K times K identity matrix.
[00:36:47.160 --> 00:36:50.240]   A matrix with ones along the diagonal.
[00:36:50.240 --> 00:36:53.720]   When applied to any length K vector returns that vector.
[00:36:53.720 --> 00:36:55.840]   This might be useful in make repeater.
[00:36:55.840 --> 00:36:56.560]   Okay.
[00:36:56.560 --> 00:37:01.400]   And in an earlier exercise, you defined a repeater matrix that repeated length
[00:37:01.400 --> 00:37:03.800]   two inputs three times, repeat three, two.
[00:37:04.080 --> 00:37:07.000]   If you completed that exercise, then you could test your make repeater by
[00:37:07.000 --> 00:37:10.720]   comparing make repeater three, two to repeat three, two, they should be the same.
[00:37:10.720 --> 00:37:15.480]   So I've, so I was on a similar right track, at least to be reminded of that.
[00:37:15.480 --> 00:37:20.920]   This will be my matrix at the end, but I'm struggling to remember what this
[00:37:20.920 --> 00:37:25.640]   means in terms of repeating three, two, cause that was in our last session.
[00:37:25.640 --> 00:37:26.280]   Right.
[00:37:26.280 --> 00:37:26.840]   Right.
[00:37:26.840 --> 00:37:27.280]   Remember.
[00:37:27.280 --> 00:37:27.680]   Yeah.
[00:37:27.680 --> 00:37:32.480]   So what we have here, I think the right way to think about this, when we first
[00:37:32.480 --> 00:37:35.640]   talked about this, right, we actually, I think went like row by row and we're
[00:37:35.640 --> 00:37:37.200]   like, this one will grab the first entry.
[00:37:37.200 --> 00:37:38.880]   This one will grab the second entry.
[00:37:38.880 --> 00:37:40.920]   And then, oh, we want to do that three times.
[00:37:40.920 --> 00:37:43.440]   So we were thinking in this very row by row level, which is
[00:37:43.440 --> 00:37:44.920]   nice for being super concrete.
[00:37:44.920 --> 00:37:49.400]   We're now going to like ascend a level of thinking here and say, what we're
[00:37:49.400 --> 00:37:54.440]   really doing is we're applying the identity matrix to the input sort of
[00:37:54.440 --> 00:38:00.120]   three times, but not three times in a row, but three times on top of each other.
[00:38:00.120 --> 00:38:02.120]   Three times in parallel.
[00:38:02.160 --> 00:38:05.560]   So do you see the three identity matrices there in repeat three, two?
[00:38:05.560 --> 00:38:06.360]   Yes.
[00:38:06.360 --> 00:38:06.720]   Okay.
[00:38:06.720 --> 00:38:07.440]   I do know.
[00:38:07.440 --> 00:38:07.760]   Yeah.
[00:38:07.760 --> 00:38:08.960]   These are three identities.
[00:38:08.960 --> 00:38:10.040]   So when the, okay.
[00:38:10.040 --> 00:38:11.720]   And now I think I've gotten it.
[00:38:11.720 --> 00:38:17.960]   So what I want to do is have N identity matrix, which are of size K.
[00:38:17.960 --> 00:38:23.000]   So where I could be wrong with the K and N there, but what I want to do is have
[00:38:23.000 --> 00:38:27.080]   some way to choose how big the diagonal is and how many times it's
[00:38:27.080 --> 00:38:28.240]   stacked on top of each other.
[00:38:28.240 --> 00:38:28.680]   Yeah.
[00:38:28.680 --> 00:38:29.440]   Exactly.
[00:38:29.840 --> 00:38:32.880]   I guess I'll use MP.I as suggested.
[00:38:32.880 --> 00:38:40.160]   Uh, that function I'll use my hover tool and see what it takes.
[00:38:40.160 --> 00:38:41.400]   Takes N.
[00:38:41.400 --> 00:38:43.040]   So that's just how big is the identity.
[00:38:43.040 --> 00:38:43.480]   Okay.
[00:38:43.480 --> 00:38:44.000]   Sweet.
[00:38:44.000 --> 00:38:49.360]   So that is, and it's, this is repeat three, two.
[00:38:49.360 --> 00:38:53.920]   I want repeat length N vector K times.
[00:38:53.920 --> 00:38:55.920]   N vector K times.
[00:38:55.920 --> 00:38:58.320]   A length N vector comes in.
[00:38:59.280 --> 00:39:00.840]   And that's going to be my identity.
[00:39:00.840 --> 00:39:01.720]   Yeah.
[00:39:01.720 --> 00:39:04.320]   I want to do, yeah, V stack.
[00:39:04.320 --> 00:39:07.200]   And then I can just do this times.
[00:39:07.200 --> 00:39:09.560]   Or I don't want to be too fancy here.
[00:39:09.560 --> 00:39:11.640]   Were you going to do like list multiplication?
[00:39:11.640 --> 00:39:14.480]   Yeah, I was going to do list multiplication, but I think I'll just do a list
[00:39:14.480 --> 00:39:17.520]   comprehension for that in range of K.
[00:39:17.520 --> 00:39:17.880]   Yeah.
[00:39:17.880 --> 00:39:22.000]   So just for the folks at home, just actually make a code cell above Scott and
[00:39:22.000 --> 00:39:25.480]   just while I'm explaining it, demonstrate list multiplication in Python.
[00:39:25.760 --> 00:39:30.200]   So in Python, you can take a list and repeat it as many times as
[00:39:30.200 --> 00:39:32.440]   you want by using multiplication.
[00:39:32.440 --> 00:39:34.840]   That'll give you this, but repeated again.
[00:39:34.840 --> 00:39:36.160]   Yeah, exactly.
[00:39:36.160 --> 00:39:40.320]   So what I was going to do is do that with my NumPy array, but I thought
[00:39:40.320 --> 00:39:41.520]   might be a little bit too fancy.
[00:39:41.520 --> 00:39:43.400]   Might be, might be tripping myself up.
[00:39:43.400 --> 00:39:47.640]   Well, so I went for a simple list comprehension and then wrapped it in V stack.
[00:39:47.640 --> 00:39:48.240]   Yep.
[00:39:48.240 --> 00:39:49.520]   Hoping this is correct.
[00:39:49.520 --> 00:39:50.400]   And it is.
[00:39:50.400 --> 00:39:50.760]   Okay.
[00:39:50.760 --> 00:39:51.240]   Nice.
[00:39:51.240 --> 00:39:51.640]   Yes.
[00:39:51.640 --> 00:39:54.440]   So yeah, so that does exactly what we wanted.
[00:39:54.480 --> 00:39:57.160]   And actually I do like the for loop style.
[00:39:57.160 --> 00:39:58.760]   That's I think a good way of doing it.
[00:39:58.760 --> 00:40:02.840]   It's less terse than doing that multiplication, you know, list and then
[00:40:02.840 --> 00:40:06.080]   multiply that's like way harder to visually parse, it's less obvious
[00:40:06.080 --> 00:40:07.400]   that you're doing this repetition.
[00:40:07.400 --> 00:40:09.840]   So yeah, I like this way of doing it and it's correct.
[00:40:09.840 --> 00:40:11.760]   Uh, so I think that might be all of our exercises.
[00:40:11.760 --> 00:40:13.240]   Would you go ahead and scroll down then Scott?
[00:40:13.240 --> 00:40:14.080]   Mm-hmm.
[00:40:14.080 --> 00:40:19.120]   This is to check our, and that is fine.
[00:40:19.120 --> 00:40:20.560]   That looks correct.
[00:40:20.560 --> 00:40:21.160]   Yeah.
[00:40:21.160 --> 00:40:24.120]   So just to close out, there's no more exercises here, but I
[00:40:24.120 --> 00:40:27.800]   think the insights of the last two sections tell us that a matrix
[00:40:27.800 --> 00:40:31.800]   with shape M comma N can be thought of in two dual ways.
[00:40:31.800 --> 00:40:36.400]   It can be thought of as a length and batch of vectors of size M or as a
[00:40:36.400 --> 00:40:41.560]   sort of V stack of M matrices of shape one comma N. Uh, so depending
[00:40:41.560 --> 00:40:44.240]   on what the matrix is, you know, what it is and what it's being used
[00:40:44.240 --> 00:40:47.120]   for, one of these views can be more useful than the other, but it's a
[00:40:47.120 --> 00:40:50.120]   good idea to have both of those views in your pocket as like an immediate
[00:40:50.120 --> 00:40:52.920]   intuition you can bring when you're working with an array.
[00:40:53.200 --> 00:40:57.400]   So the matrices that define the inputs to a machine learning model are best
[00:40:57.400 --> 00:40:59.240]   thought of as batches of vectors, right?
[00:40:59.240 --> 00:41:02.680]   Because each one's an image and there's a bunch of images, whereas the
[00:41:02.680 --> 00:41:05.720]   matrices that get multiplied with those inputs, for example, in linear
[00:41:05.720 --> 00:41:09.400]   regression or in the first layer of a neural network are best thought of as
[00:41:09.400 --> 00:41:15.760]   those like concatenated row vectors or these concatenated one by N matrices.
[00:41:15.760 --> 00:41:19.640]   Because each one of those is basically computing a different function on the
[00:41:19.640 --> 00:41:21.280]   input in a neural network layer.
[00:41:21.280 --> 00:41:23.160]   We think of it as a feature detector, right?
[00:41:23.160 --> 00:41:25.440]   So I have a feature detector that's looking for one feature.
[00:41:25.440 --> 00:41:29.280]   It's a one by N weight matrix that are the weights of an
[00:41:29.280 --> 00:41:31.120]   individual neuron in the layer.
[00:41:31.120 --> 00:41:34.400]   And so then it becomes really useful to think of these as functions.
[00:41:34.400 --> 00:41:38.080]   And importantly, like, I don't know, we had these kinds of artificial examples
[00:41:38.080 --> 00:41:41.280]   of like repeat three, two and things like that, but when it comes to a neural
[00:41:41.280 --> 00:41:44.800]   network, we actually have these like little weights in there that are doing
[00:41:44.800 --> 00:41:49.360]   some kind of real computation that people really care about, like combining
[00:41:49.360 --> 00:41:53.840]   curves together to make circles in a ConvNet or, you know, computing relationships
[00:41:53.840 --> 00:41:56.040]   between words in a transformer model.
[00:41:56.040 --> 00:41:58.520]   And so they, they really actually do behave like this.
[00:41:58.520 --> 00:42:02.240]   And it's useful to have these ideas from linear algebra to understand what might
[00:42:02.240 --> 00:42:06.160]   be going on inside that goopy black box of a neural network that you're training.
[00:42:06.160 --> 00:42:07.680]   Oh yeah, this has been really great.
[00:42:07.680 --> 00:42:11.200]   And I think it's, it's given me a new intuition to think about, especially
[00:42:11.200 --> 00:42:14.280]   the VStack of functions, that was something I hadn't, hadn't seen before.
[00:42:14.280 --> 00:42:18.360]   I think I'm used to thinking of batches of inputs and stuff because as a machine
[00:42:18.360 --> 00:42:21.680]   learning engineer, you're always reshaping your inputs and outputs.
[00:42:21.680 --> 00:42:25.800]   But I think when it comes to the innards of actually breaking down what's
[00:42:25.800 --> 00:42:28.920]   happening inside the black box, you're right, that this intuition of a VStack
[00:42:28.920 --> 00:42:33.520]   of M matrices or matrices is really useful and I'm definitely going to bring it into my work.
[00:42:33.520 --> 00:42:33.880]   Great.
[00:42:33.880 --> 00:42:34.280]   Yeah.
[00:42:34.280 --> 00:42:34.600]   All right.
[00:42:34.600 --> 00:42:38.400]   Well, next time we'll be talking about calculus to understand what's going on
[00:42:38.400 --> 00:42:40.000]   in gradients and gradient descent better.
[00:42:40.000 --> 00:42:41.280]   So I'm looking forward to that session.
[00:42:41.280 --> 00:42:41.720]   Me too.
[00:42:41.760 --> 00:42:42.040]   Bye.
[00:42:42.040 --> 00:42:42.560]   Bye.
[00:42:43.440 --> 00:42:43.920]   Bye.
[00:42:44.920 --> 00:42:45.440]   All right.
[00:42:45.560 --> 00:42:45.880]   All right.
[00:42:45.880 --> 00:42:46.240]   All right.
[00:42:46.240 --> 00:42:46.600]   All right.
[00:42:46.600 --> 00:42:46.960]   All right.
[00:42:46.960 --> 00:43:01.960]   Bye.

