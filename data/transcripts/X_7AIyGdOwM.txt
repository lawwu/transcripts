
[00:00:00.000 --> 00:00:02.160]   Okay so that's kind of the moment of truth now.
[00:00:02.160 --> 00:00:09.600]   Hey, Armand from Weights & Biases here. In this video, I'm going to show you how to make this
[00:00:09.600 --> 00:00:15.440]   RC car drive by itself. A few months ago, I saw this video by Andy Sloan where he managed to get
[00:00:15.440 --> 00:00:22.800]   his car to run insanely fast. It convinced me to get into autonomous RC cars and I've built this
[00:00:22.800 --> 00:00:27.120]   one. It's based on the NVIDIA Jet Racer project and I've been instrumenting it with Weights &
[00:00:27.120 --> 00:00:31.760]   Biases. Weights & Biases is a lightweight developer toolkit for dataset versioning,
[00:00:31.760 --> 00:00:36.400]   experiment tracking, and model management. In this project, I'm making heavy use of Weights &
[00:00:36.400 --> 00:00:41.520]   Biases Artifacts. Artifacts is basically version control cloud storage for datasets and models.
[00:00:41.520 --> 00:00:46.320]   In the context of this RC car project, I collect data on the car itself, annotate it on my laptop,
[00:00:46.320 --> 00:00:51.200]   train models on Google Codelab, and deploy those back to the car. Artifacts make it painless to
[00:00:51.200 --> 00:00:55.840]   transfer different versions of files between all of these devices and to keep track of what model
[00:00:55.840 --> 00:01:00.480]   was trained on which version of the dataset and how it performed on the actual car. More on that
[00:01:00.480 --> 00:01:04.800]   later. I'm going to show you what goes into making one of these so you can get a sense of how the
[00:01:04.800 --> 00:01:09.520]   system is working end to end. It's actually pretty straightforward. Okay, we start by installing the
[00:01:09.520 --> 00:01:13.920]   most important component of this whole build. That's an NVIDIA Jetson Nano. You can think of
[00:01:13.920 --> 00:01:18.800]   it as a Raspberry Pi with a GPU. That's what we're going to use to analyze data coming from our
[00:01:18.800 --> 00:01:25.200]   sensors. Then we move on to the next most important element. That's a PWM driver that we're going to
[00:01:25.200 --> 00:01:31.600]   use to send signals to the car steering and throttle. After that, we install the remote
[00:01:31.600 --> 00:01:36.480]   receiver along with a multiplexer that's going to let us switch between manual control and automatic
[00:01:36.480 --> 00:01:42.480]   control. Before attaching the electronics to the car, we add an IMU that's comprised of an
[00:01:42.480 --> 00:01:51.680]   accelerometer, a gyroscope, and a magnetometer. Lastly, we install the car's camera.
[00:01:54.720 --> 00:01:58.960]   And that's pretty much it. It took me about 45 minutes to assemble the car.
[00:01:58.960 --> 00:02:03.040]   If you'd like to see more detailed instructions, I'll leave you a link in the description down below.
[00:02:03.040 --> 00:02:10.400]   Now that you know how the car was built, let me show you where we're at.
[00:02:10.400 --> 00:02:18.240]   Okay, so I've turned my living room into a racetrack and I've worked out a pipeline to
[00:02:18.240 --> 00:02:23.200]   collect data, label a data set, train a model, and deploy it to the car. So here is how it's
[00:02:23.200 --> 00:02:36.480]   driving so far. As you can see, that's not good enough yet, so let's go to our 1dB dashboard
[00:02:36.480 --> 00:02:40.560]   to see if we can improve things. Okay, so before I explain everything that's going on,
[00:02:40.560 --> 00:02:44.160]   I'm going to quickly explain what's the idea to get the car to drive by itself.
[00:02:44.160 --> 00:02:48.240]   So that's the view from the car's camera and what we're trying to do is we're trying to train a
[00:02:48.240 --> 00:02:53.520]   model to infer the center of the road and then use that to steer the car. If the center of the road
[00:02:53.520 --> 00:02:56.880]   is on the right, we steer to the right. If it's on the left, we steer to the left. Here's the code
[00:02:56.880 --> 00:03:00.880]   that's doing that. As you can see, that's pretty straightforward. Now we can see a few different
[00:03:00.880 --> 00:03:06.160]   interesting things on this dashboard. First are a few different images from the run along with the
[00:03:06.160 --> 00:03:12.000]   model prediction. That's the green circle here. So we can check to see how the model is doing
[00:03:12.000 --> 00:03:16.640]   in real life. And as you can see, that's not too bad, so I think the problem's not coming from that.
[00:03:16.640 --> 00:03:20.720]   Then we can see the inference time, that's the time it takes to analyze one image. And here it's
[00:03:20.720 --> 00:03:27.280]   about 0.03 seconds, so that's the maximum frame rate of 33 images per second. And if we go into
[00:03:27.280 --> 00:03:32.400]   the runs config, we can see that we're running the model at 10 fps, so that's good. If the frame rate
[00:03:32.400 --> 00:03:38.160]   was above 33, it might introduce delays and the car might crash. Then we can see data coming from
[00:03:38.160 --> 00:03:42.640]   the accelerometer, gyroscope and magnetometer. What's interesting about the accelerometer data
[00:03:42.640 --> 00:03:47.040]   is we can see spikes. They correspond to the times the car hit the wall the first time
[00:03:47.040 --> 00:03:53.040]   and the second time. What's interesting with the gyroscope is we can overlay it on top of the
[00:03:53.040 --> 00:03:58.080]   steering signal. And as you can see, they correlate because when the car's turning,
[00:03:58.080 --> 00:04:03.040]   the acceleration along the z-axis increases. Then we can see the car's throttle and control signal,
[00:04:03.040 --> 00:04:07.120]   that's what we're sending to the steering of the car. We can then take a look at system metric,
[00:04:07.120 --> 00:04:12.400]   GPU usage, CPU temperature, GPU temperature and power consumption. That's a really important one
[00:04:12.400 --> 00:04:16.080]   because if we go above what the battery can supply, the onboard computer will shut down
[00:04:16.080 --> 00:04:21.440]   and the car will crash. So if we look at our control signal, which can range from -1 to 1,
[00:04:21.440 --> 00:04:27.040]   -1 being steering fully to the right and 1 being steering fully to the left, we can see that it
[00:04:27.040 --> 00:04:32.960]   never goes above about 0.7 and while driving the car manually, I usually need to steer fully to
[00:04:32.960 --> 00:04:36.960]   the right or fully to the left. So I think that's our issue. So what we're going to do is we're
[00:04:36.960 --> 00:04:40.960]   going to increase the steering gain. So the steering gain is a magic number we multiply
[00:04:40.960 --> 00:04:44.720]   our steering value with. So what I'm going to do to make the car steer more aggressively
[00:04:44.720 --> 00:04:48.800]   is I'm going to make the steering gain too. Now that this is done, let's try it on the car.
[00:04:48.800 --> 00:04:49.440]   Okay, let's go!
[00:04:49.440 --> 00:05:05.280]   Okay, so that's much better but it seems to be getting a flat just near the couch.
[00:05:05.280 --> 00:05:09.200]   So what I'm going to do is I'm going to collect some more data and join a new model.
[00:05:09.200 --> 00:05:13.360]   Okay, so the first step in the pipeline is to collect images. I'm going to do this while I
[00:05:13.360 --> 00:05:17.120]   drive the car around. I have SSH into the car and I'm going to run this script that's
[00:05:17.120 --> 00:05:20.560]   going to capture images and upload them to Watson Biosys. So let's go!
[00:05:20.560 --> 00:05:33.440]   Now that we have a bunch of new images, the second step in the pipeline is to annotate
[00:05:33.440 --> 00:05:38.400]   these images. So I'm going to download the actual artifact on my laptop and label the images.
[00:05:39.280 --> 00:05:42.480]   For each of these images, I'm going to annotate the center of the track.
[00:05:42.480 --> 00:05:52.960]   Okay, so step two took a while but we are now ready to train a new model. What we're doing is
[00:05:52.960 --> 00:05:58.160]   we're downloading the latest dataset from Watson Biosys and fine-tuning a ResNet34. So let's go!
[00:06:04.240 --> 00:06:08.720]   Okay, so that's looking good. I've logged the test set to Watson Biosys along with the model
[00:06:08.720 --> 00:06:13.040]   prediction and the ground truth. So the green circle is the ground truth and the blue circle
[00:06:13.040 --> 00:06:19.520]   is the model prediction. And I can then sort by IS losses for example to gauge what's difficult
[00:06:19.520 --> 00:06:23.760]   for my model. But that's looking fine so we can now move on to the next step which is optimizing
[00:06:23.760 --> 00:06:27.600]   the model for influence on the Jetson Nano. So let's run the script. It's going to download
[00:06:27.600 --> 00:06:32.720]   the latest model from Watson Biosys, optimize it using TensorRT and then upload it back to
[00:06:32.720 --> 00:06:38.080]   Watson Biosys. Now that that's done, let's move to the track to see how it's doing.
[00:06:38.080 --> 00:06:40.240]   Okay, so that's kind of the moment of truth now.
[00:06:40.240 --> 00:06:55.040]   Okay, so that's working much better now. There is one last thing I want to show you. Okay,
[00:06:55.040 --> 00:06:59.920]   if I go into my 1db project, checking out the artifacts tab for the model I just trained and
[00:06:59.920 --> 00:07:05.040]   used to drive the car, clicking on graph view I can then see the whole pipeline. This graph is
[00:07:05.040 --> 00:07:10.720]   automatically generated when you use 1db artifacts. The squares represent runs and the circles
[00:07:10.720 --> 00:07:15.680]   represent artifacts. The arrows indicate which runs use and produce different artifacts. That's
[00:07:15.680 --> 00:07:20.080]   the data collection step we run on the car, that's the labeling step we run on my laptop,
[00:07:20.080 --> 00:07:24.320]   that's the training step we run on Colab, and finally that's the optimization and influence
[00:07:24.320 --> 00:07:28.400]   step we run on the car. So I think that's really cool and that's definitely much better than what
[00:07:28.400 --> 00:07:32.800]   I was doing before using Watson Biosys. That's it for this video, I strongly encourage you to
[00:07:32.800 --> 00:07:37.200]   give the training notebook a go. Perhaps you could try to train a model using images of real road
[00:07:37.200 --> 00:07:41.440]   and see if it generalizes to the miniature racetrack, or you could try a different architecture
[00:07:41.440 --> 00:07:45.680]   to maybe get faster inference times. If you think you have a really good model or an interesting
[00:07:45.680 --> 00:07:49.840]   experiment, feel free to send me a DM on Twitter and I'll send you a video of your model running
[00:07:49.840 --> 00:07:54.320]   on the car. If you want to dive more into the world of autonomous RC cars, I recommend you
[00:07:54.320 --> 00:07:58.720]   check out the Donkey Car Project, and if you live in California you may want to check out
[00:07:58.720 --> 00:08:04.400]   Circuit Launch, they're organizing races. If you have any questions, please drop them down below,
[00:08:04.400 --> 00:08:10.320]   and consider subscribing to our channel for more tutorials, interviews and talks. Thanks for watching!

