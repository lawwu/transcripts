
[00:00:00.000 --> 00:00:07.920]   I was at an ML startup and I used TensorFlow for that startup for the internship and I
[00:00:07.920 --> 00:00:08.920]   was like blown away.
[00:00:08.920 --> 00:00:13.120]   I was like, "Wow, I didn't realize you could do so many things with it."
[00:00:13.120 --> 00:00:17.480]   So I went to a couple of my professors and was like, "Can you teach a course on that?"
[00:00:17.480 --> 00:00:19.400]   And my professor was like, "I don't have time.
[00:00:19.400 --> 00:00:20.840]   Why don't you teach it?"
[00:00:20.840 --> 00:00:23.400]   So it was okay.
[00:00:23.400 --> 00:00:27.240]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:27.240 --> 00:00:28.880]   models work in the real world.
[00:00:28.880 --> 00:00:31.080]   I'm your host, Lukas Biewald.
[00:00:31.080 --> 00:00:35.760]   Chip is a machine learning expert that currently works at a startup focusing on machine learning
[00:00:35.760 --> 00:00:36.760]   production pipelines.
[00:00:36.760 --> 00:00:41.040]   Prior to that, she worked at Nvidia, Netflix, and Primer.
[00:00:41.040 --> 00:00:44.720]   She also taught the course at Stanford on TensorFlow for deep learning research.
[00:00:44.720 --> 00:00:49.640]   And maybe most fascinatingly, before she became a machine learning expert, she was a best-selling
[00:00:49.640 --> 00:00:51.120]   author in Vietnam.
[00:00:51.120 --> 00:00:52.880]   I'm super excited to talk to her.
[00:00:52.880 --> 00:00:57.440]   My first question for you when I was thinking about interviewing you was actually, I really
[00:00:57.440 --> 00:01:01.400]   want to hear the whole story about how you got into machine learning.
[00:01:01.400 --> 00:01:05.840]   I kind of have bits and pieces of your background that you've told me in the past, but tell
[00:01:05.840 --> 00:01:06.840]   me your life story.
[00:01:06.840 --> 00:01:11.760]   I don't usually ask that, but I really want to hear it.
[00:01:11.760 --> 00:01:12.760]   How much time do we have?
[00:01:12.760 --> 00:01:13.760]   Do you want a full version?
[00:01:13.760 --> 00:01:14.760]   I can cut you down.
[00:01:14.760 --> 00:01:20.800]   But how did you get into tech in the first place, I guess?
[00:01:20.800 --> 00:01:27.800]   So it's a funny story because I come from a very non-tech background, as fuzzy as you
[00:01:27.800 --> 00:01:29.800]   can think of.
[00:01:29.800 --> 00:01:34.680]   So after high school, I didn't go to college and I started traveling.
[00:01:34.680 --> 00:01:37.240]   You know, like the full picture, you can go hippie.
[00:01:37.240 --> 00:01:39.920]   Like that's what's me.
[00:01:39.920 --> 00:01:42.000]   So I did that for three years.
[00:01:42.000 --> 00:01:44.520]   And in the process, I was writing.
[00:01:44.520 --> 00:01:46.440]   I was writing for a newspaper.
[00:01:46.440 --> 00:01:48.520]   I hosted a couple of columns.
[00:01:48.520 --> 00:01:54.080]   And I wrote a couple of books, which got me into more trouble than I wanted to have, than
[00:01:54.080 --> 00:01:55.080]   I wished for.
[00:01:55.080 --> 00:01:56.080]   Wait, really?
[00:01:56.080 --> 00:01:57.080]   It got you into trouble?
[00:01:57.080 --> 00:01:58.080]   Yeah.
[00:01:58.080 --> 00:01:59.080]   Why?
[00:01:59.080 --> 00:02:02.720]   So, you know, like Internet popularity is like a double-edged sword.
[00:02:02.720 --> 00:02:04.560]   So my books got very popular.
[00:02:04.560 --> 00:02:08.040]   And okay, so I say very popular sounds arrogant.
[00:02:08.040 --> 00:02:10.160]   It's like more popular than I thought.
[00:02:10.160 --> 00:02:11.160]   No, they were bestsellers, right?
[00:02:11.160 --> 00:02:12.160]   I think it's fair to say that they were very popular.
[00:02:12.160 --> 00:02:13.160]   What were the books about?
[00:02:13.160 --> 00:02:14.160]   In Vietnam.
[00:02:14.160 --> 00:02:20.040]   It was about traveling, like the people I met on the road.
[00:02:20.040 --> 00:02:21.880]   And I was young.
[00:02:21.880 --> 00:02:24.320]   I didn't know how to handle all the attention.
[00:02:24.320 --> 00:02:27.000]   And people were like, "Oh, it's impossible.
[00:02:27.000 --> 00:02:30.040]   A girl can't possibly travel by herself."
[00:02:30.040 --> 00:02:32.400]   And they were like, "Oh, she didn't write the book.
[00:02:32.400 --> 00:02:34.200]   She didn't write any of that."
[00:02:34.200 --> 00:02:37.520]   She had people like running things for her, doing things for her.
[00:02:37.520 --> 00:02:41.240]   People accused me of having a lot of money, so I wouldn't travel at all.
[00:02:41.240 --> 00:02:46.520]   So there was a lot of controversies and I was a little bit like offended.
[00:02:46.520 --> 00:02:48.280]   It was like, "Who are these people?
[00:02:48.280 --> 00:02:51.840]   Why is it making me answer all these stupid questions?"
[00:02:51.840 --> 00:02:54.240]   But at that time, I did not know how to handle that.
[00:02:54.240 --> 00:02:57.480]   And it caused a lot of backlash.
[00:02:57.480 --> 00:02:59.600]   So I was like, "Okay, I'm so tired of this.
[00:02:59.600 --> 00:03:00.600]   I'm going back to school."
[00:03:00.600 --> 00:03:03.680]   So I went back to Stanford.
[00:03:03.680 --> 00:03:10.840]   And I was thinking of doing something like writing, like group writing or political science.
[00:03:10.840 --> 00:03:16.640]   But then, I was at Stanford and everyone told me the question is not whether you should
[00:03:16.640 --> 00:03:20.040]   take a CS course or when you want to do it.
[00:03:20.040 --> 00:03:24.560]   Because 90% of my Stanford undergrad take the CS course at some point.
[00:03:24.560 --> 00:03:28.200]   So I just took a course in my first quarter and I really liked it.
[00:03:28.200 --> 00:03:30.200]   In your first quarter there?
[00:03:30.200 --> 00:03:31.200]   Mm-hmm.
[00:03:31.200 --> 00:03:32.200]   Wow.
[00:03:32.200 --> 00:03:33.200]   It's an introductory course.
[00:03:33.200 --> 00:03:35.680]   You came to Stanford, how old were you?
[00:03:35.680 --> 00:03:36.680]   You'd already been a best-selling author?
[00:03:36.680 --> 00:03:42.600]   That's a sad question.
[00:03:42.600 --> 00:03:44.080]   So I was older than my classmates.
[00:03:44.080 --> 00:03:47.360]   So I took my first quarter.
[00:03:47.360 --> 00:03:48.360]   It was fun.
[00:03:48.360 --> 00:03:51.760]   And I kept on doing more courses.
[00:03:51.760 --> 00:03:56.120]   Before I knew it, I was a CS major.
[00:03:56.120 --> 00:03:58.120]   I took an AI course.
[00:03:58.120 --> 00:04:00.120]   I cried a lot in the first class.
[00:04:00.120 --> 00:04:01.120]   It was so difficult.
[00:04:01.120 --> 00:04:02.120]   But it was so very flashy.
[00:04:02.120 --> 00:04:07.880]   I think I came to Stanford at a time when it was the peak of the AI hype.
[00:04:07.880 --> 00:04:10.880]   Am I allowed to say AI hype here?
[00:04:10.880 --> 00:04:11.880]   Sure.
[00:04:11.880 --> 00:04:18.880]   You can say literally whatever you want.
[00:04:18.880 --> 00:04:22.520]   So I did that and it was fun.
[00:04:22.520 --> 00:04:23.520]   And yeah, here I am.
[00:04:23.520 --> 00:04:26.400]   I think in my third year, I taught a course.
[00:04:26.400 --> 00:04:28.280]   I created and taught a course.
[00:04:28.280 --> 00:04:29.280]   Your third year as an undergrad?
[00:04:29.280 --> 00:04:30.280]   Yeah.
[00:04:30.280 --> 00:04:35.680]   Wow, I try to think about what I was doing as an undergrad and I feel embarrassed.
[00:04:35.680 --> 00:04:39.840]   Okay, to be fair, I was older than people.
[00:04:39.840 --> 00:04:46.160]   I also didn't have to spend time on frat parties to impress people.
[00:04:46.160 --> 00:04:49.680]   I was pretty much done with the party scene by then.
[00:04:49.680 --> 00:04:50.680]   Right.
[00:04:50.680 --> 00:04:55.280]   But you taught a really popular class, right?
[00:04:55.280 --> 00:04:57.360]   I think you could say it was a popular class.
[00:04:57.360 --> 00:04:58.360]   I could say.
[00:04:58.360 --> 00:05:00.360]   I think people can say that.
[00:05:00.360 --> 00:05:03.960]   I feel like it was unexpected.
[00:05:03.960 --> 00:05:07.160]   I didn't even know that the course was popular.
[00:05:07.160 --> 00:05:09.360]   I was just like ditching it.
[00:05:09.360 --> 00:05:13.080]   And one day, walking to the dining hall, a friend was like, "Did you see my comment
[00:05:13.080 --> 00:05:14.080]   by you on Hiker News?"
[00:05:14.080 --> 00:05:17.080]   And I was like, "Why would anyone say anything about me on Hiker News?"
[00:05:17.080 --> 00:05:20.240]   And it turned out that my course was on the front page of Hiker News.
[00:05:20.240 --> 00:05:21.720]   I was like, "Wow, that's interesting."
[00:05:21.720 --> 00:05:24.720]   And at some point, I didn't even know what happened.
[00:05:24.720 --> 00:05:28.040]   I was not really active on Twitter back then.
[00:05:28.040 --> 00:05:30.600]   And it was just one day I opened Twitter and found 10,000 followers.
[00:05:30.600 --> 00:05:32.480]   I was like, "Wow, who are these people?"
[00:05:32.480 --> 00:05:33.480]   It was great.
[00:05:33.480 --> 00:05:34.480]   Well, it's a timely class.
[00:05:34.480 --> 00:05:36.160]   What was the topic of the class?
[00:05:36.160 --> 00:05:37.840]   Just for people who might not know.
[00:05:37.840 --> 00:05:39.600]   Oh, it was TensorFlow.
[00:05:39.600 --> 00:05:41.360]   I think it was the right time.
[00:05:41.360 --> 00:05:45.680]   TensorFlow was very popular back in 2016.
[00:05:45.680 --> 00:05:46.680]   Wow.
[00:05:46.680 --> 00:05:51.240]   It's crazy how fast things change.
[00:05:51.240 --> 00:05:54.560]   Back then, in 2016, TensorFlow was the only word people talked about.
[00:05:54.560 --> 00:05:58.240]   And now it's what people complain about.
[00:05:58.240 --> 00:05:59.240]   So, yeah.
[00:05:59.240 --> 00:06:02.520]   So, I taught a course on TensorFlow.
[00:06:02.520 --> 00:06:08.320]   I think the official name is TensorFlow for Deep Learning Research, which is also a very
[00:06:08.320 --> 00:06:09.560]   flashy name.
[00:06:09.560 --> 00:06:16.120]   And so, I think I put a lot of other materials online.
[00:06:16.120 --> 00:06:19.360]   And I think it was maybe the first college-level course on TensorFlow.
[00:06:19.360 --> 00:06:22.600]   In 2016, I'm trying to remember.
[00:06:22.600 --> 00:06:27.800]   I think you had to compile it yourself to get it to use the GPU back then.
[00:06:27.800 --> 00:06:28.800]   I'm trying to remember.
[00:06:28.800 --> 00:06:30.520]   I remember just installing TensorFlow.
[00:06:30.520 --> 00:06:33.880]   It was a pretty painful experience for me.
[00:06:33.880 --> 00:06:34.880]   It was not.
[00:06:34.880 --> 00:06:36.880]   I don't remember it to be so painful.
[00:06:36.880 --> 00:06:42.880]   It was just some concept was a little bit hard to grasp, as in a computation graph.
[00:06:42.880 --> 00:06:47.680]   So, you had to build a graph first before you can run it.
[00:06:47.680 --> 00:06:51.640]   So I think with TensorFlow 2.0 right now, it's a bit different.
[00:06:51.640 --> 00:06:52.640]   Yeah.
[00:06:52.640 --> 00:06:53.640]   Got it.
[00:06:53.640 --> 00:06:57.840]   And so, how did you come up with the material for that class?
[00:06:57.840 --> 00:06:59.840]   How did you even think of what to...
[00:06:59.840 --> 00:07:03.840]   So when I started teaching the course, I was just hoping to learn.
[00:07:03.840 --> 00:07:12.040]   As you know, I started thinking about the course as a sophomore, second year.
[00:07:12.040 --> 00:07:13.440]   So I didn't know a lot.
[00:07:13.440 --> 00:07:20.200]   I was at an ML startup and I used TensorFlow for that startup for the internship.
[00:07:20.200 --> 00:07:21.760]   And I was blown away.
[00:07:21.760 --> 00:07:25.840]   I was like, "Wow, I didn't realize you could do so many things with it."
[00:07:25.840 --> 00:07:30.200]   So I went to a couple of my professors and was like, "Can you teach a course on that?"
[00:07:30.200 --> 00:07:31.920]   And my professor was like, "I don't have time.
[00:07:31.920 --> 00:07:33.560]   Why don't you teach it?"
[00:07:33.560 --> 00:07:36.320]   So I was like, "Okay."
[00:07:36.320 --> 00:07:39.220]   And I got a lot of people to help me.
[00:07:39.220 --> 00:07:44.120]   So I have some friends at Google who know a lot about TensorFlow.
[00:07:44.120 --> 00:07:53.280]   I have professors who overlook, who look at my curriculum and I'll just give a lot of
[00:07:53.280 --> 00:07:55.360]   feedback or read lecture notes.
[00:07:55.360 --> 00:07:59.520]   I was really nervous.
[00:07:59.520 --> 00:08:04.840]   So I had really good friends who were coerced into being my fake students.
[00:08:04.840 --> 00:08:12.760]   So for every lecture, I would make them sit and listen to me give them a fake lecture.
[00:08:12.760 --> 00:08:13.760]   Really?
[00:08:13.760 --> 00:08:14.760]   Yeah.
[00:08:14.760 --> 00:08:17.920]   So I think I got a lot of help.
[00:08:17.920 --> 00:08:20.640]   So it was like learning together with my students.
[00:08:20.640 --> 00:08:25.240]   I didn't think of it as teaching as much as a group study.
[00:08:25.240 --> 00:08:27.560]   That's super cool.
[00:08:27.560 --> 00:08:33.360]   When you came to Stanford, you hadn't taken any computer science class before?
[00:08:33.360 --> 00:08:35.120]   So I came from a math background.
[00:08:35.120 --> 00:08:37.720]   So I did math in high school.
[00:08:37.720 --> 00:08:44.280]   So I think we also took some CS courses, but it was more like very, very basic.
[00:08:44.280 --> 00:08:48.120]   I remember it was like blue screen back then.
[00:08:48.120 --> 00:08:49.120]   Wow.
[00:08:49.120 --> 00:08:53.560]   It just seems really amazing.
[00:08:53.560 --> 00:09:01.240]   You went from introductory to computer science type stuff to teaching a TensorFlow class
[00:09:01.240 --> 00:09:02.600]   like two years later.
[00:09:02.600 --> 00:09:04.640]   It's amazing.
[00:09:04.640 --> 00:09:11.120]   Do you have any advice to other people that want to learn this stuff?
[00:09:11.120 --> 00:09:16.440]   I think it's one of the beauty of computer science is the entry barrier is really low.
[00:09:16.440 --> 00:09:25.440]   So also ML, especially with the experiment oriented progress that you actually don't
[00:09:25.440 --> 00:09:30.200]   need to know a lot of theories to make a contribution.
[00:09:30.200 --> 00:09:36.080]   So I've seen people who get into ML for a year and is able to make a pretty great project,
[00:09:36.080 --> 00:09:38.080]   which I'm still ambivalent about.
[00:09:38.080 --> 00:09:43.200]   It's good as it lowers the entry barrier, which means it allows more people from different
[00:09:43.200 --> 00:09:44.200]   backgrounds to join.
[00:09:44.200 --> 00:09:50.920]   So what do you say about fail when somebody who joined for a year can make a pretty mind
[00:09:50.920 --> 00:09:51.920]   blowing experiment?
[00:09:51.920 --> 00:09:52.920]   So I don't know.
[00:09:52.920 --> 00:09:53.920]   Maybe it means there's lots of interesting stuff to try.
[00:09:53.920 --> 00:10:05.560]   So, yeah, I don't know any advice.
[00:10:05.560 --> 00:10:09.560]   I'm so skeptical of giving advice.
[00:10:09.560 --> 00:10:11.800]   I think it's just like get your hands dirty.
[00:10:11.800 --> 00:10:16.160]   Try things, try things out and be friends with smart people.
[00:10:16.160 --> 00:10:19.560]   Be friends with smart people.
[00:10:19.560 --> 00:10:20.920]   I have friends smarter than you.
[00:10:20.920 --> 00:10:25.040]   I don't think I can have got anything done without my friends, really.
[00:10:25.040 --> 00:10:26.800]   That's so cool.
[00:10:26.800 --> 00:10:29.600]   I think that's good advice.
[00:10:29.600 --> 00:10:31.560]   Why did you choose to go into AI?
[00:10:31.560 --> 00:10:36.180]   For me, it was just a promise that AI helped.
[00:10:36.180 --> 00:10:44.440]   So I come from a village in Vietnam and I traveled and over the time I realized that
[00:10:44.440 --> 00:10:49.120]   language barriers can actually, it could be great if you can overcome the language barrier.
[00:10:49.120 --> 00:10:54.240]   For example, the majority of human knowledge online is written in English and people who
[00:10:54.240 --> 00:10:57.320]   don't speak English can't really access that.
[00:10:57.320 --> 00:11:02.800]   People in my hometown can't really read anything that I write in English or my parents are
[00:11:02.800 --> 00:11:07.120]   afraid of visiting me in the US because they wouldn't be able to show how to navigate the
[00:11:07.120 --> 00:11:10.280]   airport or how to get here.
[00:11:10.280 --> 00:11:19.000]   So I think at this time I was very interested in machine translation.
[00:11:19.000 --> 00:11:26.600]   If you can automate the translation process, then it could be really, really helpful.
[00:11:26.600 --> 00:11:33.280]   Then if we can overcome the language barriers and help people from maybe my village can
[00:11:33.280 --> 00:11:40.080]   access human knowledge or just to step out of the border.
[00:11:40.080 --> 00:11:41.080]   That's so cool.
[00:11:41.080 --> 00:11:43.560]   That's what I thought back then.
[00:11:43.560 --> 00:11:44.560]   Very idealistic.
[00:11:44.560 --> 00:11:51.000]   What are the topics that are most interesting to you right now in machine learning?
[00:11:51.000 --> 00:12:00.160]   So I think over time, I think what we are liking is better engineering in machine learning.
[00:12:00.160 --> 00:12:09.800]   So there are two aspects, both in engineering, in research and engineering in production.
[00:12:09.800 --> 00:12:17.640]   So in research, there are a lot of researchers who are amazing at what they do, but who are
[00:12:17.640 --> 00:12:19.640]   also not great engineers.
[00:12:19.640 --> 00:12:25.400]   And it's not because of them, it's just because our time is limited.
[00:12:25.400 --> 00:12:30.840]   If you focus too much on research, we can't expect them to be great engineers.
[00:12:30.840 --> 00:12:40.320]   So I wonder if we can build a good tool set to help researchers carry out their research
[00:12:40.320 --> 00:12:43.360]   more efficiently and help them more.
[00:12:43.360 --> 00:12:51.440]   And also if you have clean code, it's easier to control experiments and help with reproducibility.
[00:12:51.440 --> 00:12:57.080]   And in production, I also think that there are people, there's a gap between researchers
[00:12:57.080 --> 00:12:59.040]   and production engineers.
[00:12:59.040 --> 00:13:02.920]   I think there's a lot of, there have been a lot of progresses in research and machine
[00:13:02.920 --> 00:13:03.920]   learning.
[00:13:03.920 --> 00:13:08.600]   And now it's just a question of how do we bring the research into production?
[00:13:08.600 --> 00:13:10.000]   And that's what I'm very interested in.
[00:13:10.000 --> 00:13:14.880]   And also startups that I'm part of right now is also focusing on that, helping companies
[00:13:14.880 --> 00:13:17.040]   productionize machine learning research.
[00:13:17.040 --> 00:13:18.040]   Cool.
[00:13:18.040 --> 00:13:23.720]   Hi, we'd love to take a moment to tell you guys about Weights & Biases.
[00:13:23.720 --> 00:13:29.280]   Weights & Biases is a tool that helps you track and visualize every detail of your machine
[00:13:29.280 --> 00:13:30.280]   learning models.
[00:13:30.280 --> 00:13:36.240]   We help you debug your machine learning models in real time, collaborate easily and advance
[00:13:36.240 --> 00:13:39.320]   the state of the art in machine learning.
[00:13:39.320 --> 00:13:44.520]   You can integrate Weights & Biases into your models with just a few lines of code.
[00:13:44.520 --> 00:13:49.280]   With hyperparameter sweeps, you can find the best set of hyperparameters for your models
[00:13:49.280 --> 00:13:51.200]   automatically.
[00:13:51.200 --> 00:13:56.600]   You can also track and compare how many GPU resources your models are using.
[00:13:56.600 --> 00:14:03.280]   With one line of code, you can visualize model predictions in form of images, videos, audio,
[00:14:03.280 --> 00:14:09.040]   plotly charts, molecular data, segmentation maps, and 3D point clouds.
[00:14:09.040 --> 00:14:14.880]   You can save everything you need to reproduce your models days, weeks, or even months after
[00:14:14.880 --> 00:14:15.880]   training.
[00:14:15.880 --> 00:14:21.000]   Finally, with reports, you can make your models come alive.
[00:14:21.000 --> 00:14:26.000]   Reports are like blog posts in which your readers can interact with your model metrics
[00:14:26.000 --> 00:14:27.800]   and predictions.
[00:14:27.800 --> 00:14:34.040]   Reports serve as a centralized repository of metrics, predictions, hyperparameter stride,
[00:14:34.040 --> 00:14:35.760]   and accompanying notes.
[00:14:35.760 --> 00:14:41.440]   All of this together gives you a bird's eye view of your machine learning workflow.
[00:14:41.440 --> 00:14:46.940]   You can use reports to share your model insights, keep your team on the same page, and collaborate
[00:14:46.940 --> 00:14:49.000]   effectively remotely.
[00:14:49.000 --> 00:14:53.080]   I'll leave a link in the show notes below to help you get started.
[00:14:53.080 --> 00:14:56.080]   And now let's get back to the episode.
[00:14:56.080 --> 00:14:58.200]   And you've worked at some big companies too.
[00:14:58.200 --> 00:15:03.720]   I guess generally, what kinds of problems do you see when companies try to take research
[00:15:03.720 --> 00:15:05.160]   and productionize it?
[00:15:05.160 --> 00:15:08.560]   What are the main ways that you see companies fail at this?
[00:15:08.560 --> 00:15:16.320]   I think one of the big things is a lot of companies are chasing buzzwords.
[00:15:16.320 --> 00:15:17.320]   For example, when Berkeley-
[00:15:17.320 --> 00:15:18.320]   Chasing buzzwords?
[00:15:18.320 --> 00:15:19.320]   Yeah.
[00:15:19.320 --> 00:15:20.320]   I see.
[00:15:20.320 --> 00:15:21.320]   Everyone's like, "How can we use BERT?
[00:15:21.320 --> 00:15:22.320]   How can we use Transformer?"
[00:15:22.320 --> 00:15:26.200]   And you can look at them and say, "You actually don't need that.
[00:15:26.200 --> 00:15:28.040]   You don't need deep learning.
[00:15:28.040 --> 00:15:33.480]   A lot of the problems can be solved by traditional classical algorithms."
[00:15:33.480 --> 00:15:41.880]   So sometimes companies like, "Yeah, this is why you should use very fancy techniques."
[00:15:41.880 --> 00:15:46.360]   The reason can be because they don't really understand what is happening because I think
[00:15:46.360 --> 00:15:51.640]   there's a lot of misunderstanding in AI reporting.
[00:15:51.640 --> 00:15:58.000]   If you see a lot of journalists or reporters talking about AI, and if they don't have background
[00:15:58.000 --> 00:16:05.560]   in AI, then they might simplify or just doesn't excessively reflect what exactly is going
[00:16:05.560 --> 00:16:06.560]   on.
[00:16:06.560 --> 00:16:10.000]   And the second is some companies might just go and use buzzwords to attract clients.
[00:16:10.000 --> 00:16:16.080]   Instead of, "Oh, we are using state-of-the-art techniques."
[00:16:16.080 --> 00:16:21.040]   Some companies actually go out of their way to try to use that.
[00:16:21.040 --> 00:16:22.040]   So that's one problem.
[00:16:22.040 --> 00:16:26.960]   I think the second is that there's a problem with the lack of data.
[00:16:26.960 --> 00:16:31.800]   And I think Lukas, I think he pretty knows that very well because you also try to solve
[00:16:31.800 --> 00:16:34.000]   the problem, right?
[00:16:34.000 --> 00:16:38.640]   So in research, people work with very clean, static data set.
[00:16:38.640 --> 00:16:43.600]   And it needs data set to be clean and static because you want to focus on different models.
[00:16:43.600 --> 00:16:48.280]   But in productions, you already have to easily have like, I think X models are being more
[00:16:48.280 --> 00:16:51.920]   and more commoditized as you can take off the shelf model.
[00:16:51.920 --> 00:17:00.000]   So the bottleneck now is data and real-world data is nowhere close to research data.
[00:17:00.000 --> 00:17:07.080]   So the problem is collect data and verify data and how to cope with constantly distribution
[00:17:07.080 --> 00:17:10.740]   and shifting data drift.
[00:17:10.740 --> 00:17:12.560]   So there's a problem with data.
[00:17:12.560 --> 00:17:18.400]   And another problem is with interpretability.
[00:17:18.400 --> 00:17:26.680]   So in research, sometimes you don't really care more about state-of-the-art, little boat.
[00:17:26.680 --> 00:17:35.640]   But also in real world, you just don't care about the accuracy or F1 or whatever metric
[00:17:35.640 --> 00:17:42.920]   you are pursuing, but you care about how can we explain the decision that the model is
[00:17:42.920 --> 00:17:43.920]   making.
[00:17:43.920 --> 00:17:52.960]   I think a lot of top people are focusing a lot of time on.
[00:17:52.960 --> 00:17:56.000]   So I think we are making progress.
[00:17:56.000 --> 00:17:59.400]   It's funny, I saw a tweet recently.
[00:17:59.400 --> 00:18:06.560]   I think it was someone at OpenAI who was arguing that folks should not teach anything besides
[00:18:06.560 --> 00:18:07.560]   neural nets.
[00:18:07.560 --> 00:18:10.560]   Oh my God, it's such a clickbait.
[00:18:10.560 --> 00:18:16.520]   It's funny because it reminded me of when I got my first job out of school.
[00:18:16.520 --> 00:18:19.160]   There was sort of a similar debate, but it was a different topic.
[00:18:19.160 --> 00:18:22.760]   So it was basically like machine learning versus rule-based systems.
[00:18:22.760 --> 00:18:27.520]   And there were a lot of older researchers who had kind of bet their careers on logic
[00:18:27.520 --> 00:18:29.920]   and rule-based systems.
[00:18:29.920 --> 00:18:33.120]   And they would say, "Oh, obviously you should do both."
[00:18:33.120 --> 00:18:37.600]   And I was like, "Come on, these systems don't really work in anything.
[00:18:37.600 --> 00:18:43.200]   Can you find me a benchmark where it actually makes sense to do a rule-based system?"
[00:18:43.200 --> 00:18:45.200]   I was thinking that's how I felt at the time.
[00:18:45.200 --> 00:18:49.760]   And I think I might still, I mean, you don't see a lot of rule-based systems in production
[00:18:49.760 --> 00:18:53.360]   in the last decade or two.
[00:18:53.360 --> 00:18:55.480]   Or I don't come across them, I guess.
[00:18:55.480 --> 00:19:00.340]   And then I was thinking, when that person made that clickbait tweet, I was like, "No,
[00:19:00.340 --> 00:19:01.340]   no, it's ridiculous."
[00:19:01.340 --> 00:19:06.560]   But I was wondering, am I now like the old guy who's trying to just justify the things
[00:19:06.560 --> 00:19:08.240]   that I know about?
[00:19:08.240 --> 00:19:09.240]   No.
[00:19:09.240 --> 00:19:10.240]   Did you get bit?
[00:19:10.240 --> 00:19:11.240]   Did you participate in the discussion?
[00:19:11.240 --> 00:19:12.240]   No.
[00:19:12.240 --> 00:19:13.240]   I'm always afraid of controversial topics on Twitter.
[00:19:13.240 --> 00:19:14.240]   It's not even controversial.
[00:19:14.240 --> 00:19:15.240]   It's just wrong.
[00:19:15.240 --> 00:19:16.240]   I mean, if it's not from some open AI, I would think it's not.
[00:19:16.240 --> 00:19:29.640]   I don't know, maybe he's trolling.
[00:19:29.640 --> 00:19:32.320]   I'm not even sure anymore.
[00:19:32.320 --> 00:19:38.160]   But it's funny because I think your work is mostly in, unlike my work, which is, if you
[00:19:38.160 --> 00:19:41.000]   look at my own research, it's not in neural nets at all.
[00:19:41.000 --> 00:19:42.960]   I mean, you actually work in neural nets.
[00:19:42.960 --> 00:19:47.120]   So I guess what are the non-deep learning algorithms that you think are useful that
[00:19:47.120 --> 00:19:53.440]   you would keep around and why would you use a different one?
[00:19:53.440 --> 00:20:00.600]   I mean, HG Boost is still the most popular algorithm to tackle competitions.
[00:20:00.600 --> 00:20:02.400]   So a lot of them are still very good.
[00:20:02.400 --> 00:20:07.960]   Like, 10 nearest neighbors is so very good for anomaly detection.
[00:20:07.960 --> 00:20:09.840]   So a lot of really great algorithms.
[00:20:09.840 --> 00:20:16.640]   A lot of them now, they don't even know what boosting or like back is, which is a bit sad.
[00:20:16.640 --> 00:20:19.880]   So yeah, so a lot of things.
[00:20:19.880 --> 00:20:24.280]   I think, the other day my friend was telling me about how he was interviewing somebody
[00:20:24.280 --> 00:20:31.080]   and that person could explain it perfectly well what a transform model is, but can't
[00:20:31.080 --> 00:20:33.600]   explain what a decision tree is.
[00:20:33.600 --> 00:20:35.800]   And it was just me, I don't know.
[00:20:35.800 --> 00:20:36.800]   Maybe I'm old too.
[00:20:36.800 --> 00:20:37.800]   I mean, I don't even know anymore.
[00:20:37.800 --> 00:20:45.600]   What are the situations where you would recommend using a boosted tree versus a neural network
[00:20:45.600 --> 00:20:46.600]   approach?
[00:20:46.600 --> 00:20:50.360]   I think definitely for baseline, for example, right?
[00:20:50.360 --> 00:20:57.320]   If a simple model does a job reasonably well, I mean, this always value in trying, but in
[00:20:57.320 --> 00:21:06.200]   production, the simpler the model is actually, it's easier to understand and to implement
[00:21:06.200 --> 00:21:09.240]   and to avoid mistake.
[00:21:09.240 --> 00:21:15.960]   So if you don't get improvement from more complicated methods, don't go there?
[00:21:15.960 --> 00:21:23.080]   I mean, it's also hard to tell because a lot of improvements incremental, right?
[00:21:23.080 --> 00:21:26.840]   So you can say, "Oh, this only gives 1% improvement.
[00:21:26.840 --> 00:21:27.840]   It's not worth it."
[00:21:27.840 --> 00:21:32.600]   But even for that 1% improvement, if more time investing, you can get another and another
[00:21:32.600 --> 00:21:37.200]   and another, and over time you can get up to 10% improvement, but then if you just stifle
[00:21:37.200 --> 00:21:41.040]   it from the beginning, then you will never be able to reach the point where it should
[00:21:41.040 --> 00:21:42.040]   be.
[00:21:42.040 --> 00:21:47.640]   So I'm not saying this like, I'm not pro not using deep learning.
[00:21:47.640 --> 00:21:49.680]   I'm actually very pro deep learning.
[00:21:49.680 --> 00:21:54.520]   I'm just saying that to start, we should not forget simpler baselines.
[00:21:54.520 --> 00:21:59.680]   And I don't think we spend enough time talking about baselines or implementing baselines.
[00:21:59.680 --> 00:22:00.720]   Interesting.
[00:22:00.720 --> 00:22:01.840]   A lot of people have said that.
[00:22:01.840 --> 00:22:03.560]   I happen to agree with it.
[00:22:03.560 --> 00:22:08.200]   But if someone is going to ask you, why are baselines important?
[00:22:08.200 --> 00:22:10.760]   How would you answer that?
[00:22:10.760 --> 00:22:14.280]   Because a metric by itself doesn't mean anything, right?
[00:22:14.280 --> 00:22:17.720]   You say 90% accuracy doesn't mean anything.
[00:22:17.720 --> 00:22:21.760]   So we say, "Oh, my model is amazing.
[00:22:21.760 --> 00:22:22.760]   It has this accuracy."
[00:22:22.760 --> 00:22:25.160]   What does it even mean?
[00:22:25.160 --> 00:22:29.080]   And for example, somebody showed me this model, it's like 90% accuracy.
[00:22:29.080 --> 00:22:36.320]   And I said, "Wow, if it's just like predicted at random, it's like 89% accuracy already.
[00:22:36.320 --> 00:22:37.320]   So what's the hope?
[00:22:37.320 --> 00:22:39.800]   What is the point of getting this?"
[00:22:39.800 --> 00:22:49.080]   So baselines are, I think, baselines are landmarks to help you localize where the model performance
[00:22:49.080 --> 00:22:51.720]   is and where you want to get you.
[00:22:51.720 --> 00:22:55.880]   So I'm very interested in human baseline, for example.
[00:22:55.880 --> 00:23:00.080]   If humans do on this dataset, how well it could do.
[00:23:00.080 --> 00:23:03.640]   So maybe saying 90% is also really amazing.
[00:23:03.640 --> 00:23:10.440]   If the human baseline is like 85%, then we say, "Oh, it's like superhuman performance."
[00:23:10.440 --> 00:23:15.880]   But even if the human baseline is 99%, then we know that we still have a lot to go.
[00:23:15.880 --> 00:23:18.240]   Right, right, right.
[00:23:18.240 --> 00:23:23.880]   So the human baseline is kind of like maybe in some sense like a best case scenario.
[00:23:23.880 --> 00:23:30.440]   Yeah, I think it's in some cases, in a lot of cases, the humans baseline is already good,
[00:23:30.440 --> 00:23:32.440]   but it's not always.
[00:23:32.440 --> 00:23:33.440]   Right.
[00:23:33.440 --> 00:23:39.080]   Do you want to talk about another thing I'm kind of curious about is your work on, I mean,
[00:23:39.080 --> 00:23:44.040]   just because I think like, you know, fake news is probably going to be a big topic again
[00:23:44.040 --> 00:23:45.640]   with the election coming up.
[00:23:45.640 --> 00:23:50.120]   I think it was just a fun class project.
[00:23:50.120 --> 00:23:52.200]   So it was after the election.
[00:23:52.200 --> 00:23:53.200]   We're curious to see.
[00:23:53.200 --> 00:23:58.760]   So, you know, I think echo chamber can also help echoing fake news.
[00:23:58.760 --> 00:23:59.760]   Right.
[00:23:59.760 --> 00:24:06.960]   I feel like the same fake news is really circulating in the same echo chamber.
[00:24:06.960 --> 00:24:14.280]   And you know, if one echo chamber shares a certain piece of news and another echo chamber
[00:24:14.280 --> 00:24:19.400]   shares similar news, but with different perspectives, there might be some, I'm not sure how helpful
[00:24:19.400 --> 00:24:24.800]   it's going to be, but it might be interesting to cross share, like bring a piece of news
[00:24:24.800 --> 00:24:28.360]   from one's perspective to another echo chamber.
[00:24:28.360 --> 00:24:31.920]   Not fake news, of course, there's no point in spreading fake news from one echo chamber
[00:24:31.920 --> 00:24:34.640]   to another.
[00:24:34.640 --> 00:24:41.800]   So what we did was that we got a lot of tweets and from hashtag.
[00:24:41.800 --> 00:24:43.600]   So at that time, it was after the election.
[00:24:43.600 --> 00:24:49.640]   So we collected a lot of tweets from during the elections in one certain state.
[00:24:49.640 --> 00:24:56.400]   And what we did was we have some seed hashtags that we know was like whether it's pro-Republican
[00:24:56.400 --> 00:24:59.000]   or pro-Democratic.
[00:24:59.000 --> 00:25:08.040]   So from those seed hashtags, we made an assumption that if two hashtags belong in the same tweet,
[00:25:08.040 --> 00:25:14.840]   then they are likely to have the same sentiment.
[00:25:14.840 --> 00:25:20.760]   So if one hashtag appears next to a hashtag that's pro-Republican, then it's likely to
[00:25:20.760 --> 00:25:22.880]   be pro-Republican.
[00:25:22.880 --> 00:25:28.960]   So from that, we had an algorithm just to resolve conflicts.
[00:25:28.960 --> 00:25:31.560]   It's very simple, like majority voting.
[00:25:31.560 --> 00:25:39.800]   So we were able to label about a thousand hashtags.
[00:25:39.800 --> 00:25:42.840]   And so from those hashtags, we also make some assumptions.
[00:25:42.840 --> 00:25:46.880]   So you label the hashtags as like liberal or conservative, essentially?
[00:25:46.880 --> 00:25:47.880]   Yes.
[00:25:47.880 --> 00:25:49.440]   Based on what they co-occurred with?
[00:25:49.440 --> 00:25:50.440]   Yes.
[00:25:50.440 --> 00:25:52.480]   So after that, we look on the tweets.
[00:25:52.480 --> 00:25:56.440]   So we build a graph of readership between users.
[00:25:56.440 --> 00:26:04.880]   For example, if user A replies to or retweets another user, there's a link between them.
[00:26:04.880 --> 00:26:07.080]   So we build graphs of users.
[00:26:07.080 --> 00:26:11.120]   And we also look at the hashtags they use.
[00:26:11.120 --> 00:26:17.160]   And so we try to predict whether a person is liberal or conservative.
[00:26:17.160 --> 00:26:21.920]   And then we use some graph algorithms to detect communities.
[00:26:21.920 --> 00:26:29.520]   And then we look at the communities and see whether this community has much more conservatives
[00:26:29.520 --> 00:26:30.520]   than liberals.
[00:26:30.520 --> 00:26:37.760]   And it was very fascinating because we found out that about 50% of the communities we found
[00:26:37.760 --> 00:26:38.760]   are neutral.
[00:26:38.760 --> 00:26:46.760]   There's a difference between the number of conservatives or liberals.
[00:26:46.760 --> 00:26:47.760]   It's not that high.
[00:26:47.760 --> 00:26:51.000]   But then about 25% of them are conservative.
[00:26:51.000 --> 00:26:57.640]   For example, the number of conservative members are more than three times higher than the
[00:26:57.640 --> 00:26:58.640]   Democrat.
[00:26:58.640 --> 00:27:04.000]   And there's 25% Democrat community.
[00:27:04.000 --> 00:27:07.360]   So you found the echo chambers?
[00:27:07.360 --> 00:27:15.200]   So we found echo chambers, but people who share similar beliefs definitely have stronger
[00:27:15.200 --> 00:27:19.960]   ties than people with different beliefs.
[00:27:19.960 --> 00:27:22.360]   I see.
[00:27:22.360 --> 00:27:28.480]   So then were you suggesting to spread information between these communities?
[00:27:28.480 --> 00:27:36.320]   So we never had access to that because it would require having access to a certain social
[00:27:36.320 --> 00:27:37.320]   network.
[00:27:37.320 --> 00:27:45.360]   But we would ideally, so we ran into some literature and they say that actually if you
[00:27:45.360 --> 00:27:50.920]   show somebody the opposite, like a news article with the opposite point of view, it's not
[00:27:50.920 --> 00:27:55.360]   actually going to ignore it.
[00:27:55.360 --> 00:28:00.080]   So if you believe in A and show you the opposite of A, it was like, oh, that's fake news.
[00:28:00.080 --> 00:28:06.240]   So the key is you have to slowly show them it's similar but slightly different.
[00:28:06.240 --> 00:28:13.360]   You can't give people a totally opposite viewpoint and expect people to listen to it.
[00:28:13.360 --> 00:28:14.360]   I see.
[00:28:14.360 --> 00:28:15.360]   That makes sense.
[00:28:15.360 --> 00:28:23.720]   So we never had a chance to test out the algorithm, but we thought detecting echo chambers might
[00:28:23.720 --> 00:28:27.840]   be the first step in finding a way to break them.
[00:28:27.840 --> 00:28:28.840]   I see.
[00:28:28.840 --> 00:28:29.840]   Yeah.
[00:28:29.840 --> 00:28:34.040]   Although maybe people just want to live in their echo chamber.
[00:28:34.040 --> 00:28:35.040]   I don't know.
[00:28:35.040 --> 00:28:39.360]   I think Silicon Valley is a massive echo chamber, really.
[00:28:39.360 --> 00:28:40.360]   For sure.
[00:28:40.360 --> 00:28:43.920]   I think we all live in the bubble.
[00:28:43.920 --> 00:28:51.080]   And I feel like somehow this pandemic also makes me realize, to show how different our
[00:28:51.080 --> 00:28:53.440]   bubble is, how strong it is.
[00:28:53.440 --> 00:28:54.440]   Okay.
[00:28:54.440 --> 00:28:57.480]   Another question I wanted to ask you about.
[00:28:57.480 --> 00:29:06.280]   You've recently gone from a bigger company to a startup, which is a little bit of a shift.
[00:29:06.280 --> 00:29:09.520]   Has there been any surprises there?
[00:29:09.520 --> 00:29:13.920]   I guess how does that feel to go from a big company to startup?
[00:29:13.920 --> 00:29:15.920]   Is it a big cultural shift?
[00:29:15.920 --> 00:29:20.800]   It's a big difference.
[00:29:20.800 --> 00:29:23.520]   It's such a big difference.
[00:29:23.520 --> 00:29:26.920]   I think that's what I wanted.
[00:29:26.920 --> 00:29:33.160]   I thought that after graduation, I would like to try a different working environment to
[00:29:33.160 --> 00:29:35.200]   see which one I would like.
[00:29:35.200 --> 00:29:38.120]   So leaving NVIDIA was not a reflection on NVIDIA.
[00:29:38.120 --> 00:29:42.120]   It was a reflection on myself because I just wanted to change.
[00:29:42.120 --> 00:29:43.120]   Totally.
[00:29:43.120 --> 00:29:44.120]   Yeah.
[00:29:44.120 --> 00:29:46.520]   And my co-workers at NVIDIA have been extremely helpful.
[00:29:46.520 --> 00:29:50.920]   And they are really, really kind.
[00:29:50.920 --> 00:29:57.080]   So it was a bit of a shock to join a startup.
[00:29:57.080 --> 00:30:00.560]   I think it's definitely the first is a workload.
[00:30:00.560 --> 00:30:01.560]   It's so much more.
[00:30:01.560 --> 00:30:02.560]   It is so much more.
[00:30:02.560 --> 00:30:03.560]   Interesting.
[00:30:03.560 --> 00:30:04.560]   Which is a good thing.
[00:30:04.560 --> 00:30:13.840]   So at a big company, you might see people leaving work at five, six.
[00:30:13.840 --> 00:30:24.520]   And at startups, you might have people, you might get PR requests at midnight on Saturday.
[00:30:24.520 --> 00:30:25.520]   Which is not a bad thing.
[00:30:25.520 --> 00:30:26.520]   I don't know.
[00:30:26.520 --> 00:30:34.320]   I think I'm still very ambivalent about the work-life balance discussions.
[00:30:34.320 --> 00:30:38.880]   People say, oh, you shouldn't work on the weekend.
[00:30:38.880 --> 00:30:44.560]   Or no company should expect the employees to work on a Saturday evening.
[00:30:44.560 --> 00:30:48.560]   But the point is companies might not expect you.
[00:30:48.560 --> 00:30:50.920]   But it's what you expect out of yourself.
[00:30:50.920 --> 00:30:59.480]   And I don't want to promote the culture of you have to work hard.
[00:30:59.480 --> 00:31:04.600]   But I do believe that when you're early in the career, there are certain compromises
[00:31:04.600 --> 00:31:09.600]   you might have to make, which depends on what you want out of life.
[00:31:09.600 --> 00:31:14.720]   Anyways, that's a very roundabout way of saying that I work on weekend.
[00:31:14.720 --> 00:31:21.080]   It sounds like you're kind of enjoying that experience.
[00:31:21.080 --> 00:31:25.880]   I also don't have much in life, you know.
[00:31:25.880 --> 00:31:31.360]   I guess right now there's less going on for sure.
[00:31:31.360 --> 00:31:37.480]   So the big shock is that people work on the weekend.
[00:31:37.480 --> 00:31:39.360]   And I kind of like it.
[00:31:39.360 --> 00:31:43.720]   I'm not sure how much longer I would like it because I heard that as soon as you have
[00:31:43.720 --> 00:31:46.840]   a family, I heard you had a baby recently.
[00:31:46.840 --> 00:31:47.840]   Congratulations.
[00:31:47.840 --> 00:31:48.840]   I did.
[00:31:48.840 --> 00:31:51.400]   Do you still work on the weekend?
[00:31:51.400 --> 00:31:58.720]   You know, I guess I don't think I have a very strong point of view.
[00:31:58.720 --> 00:32:02.960]   I feel like it's a little bit weird to tell people not to work super hard.
[00:32:02.960 --> 00:32:13.200]   I worked incredibly hard in my 20s and I kind of like to imagine that hard work pays off.
[00:32:13.200 --> 00:32:15.840]   I feel proud of the stuff that I did.
[00:32:15.840 --> 00:32:18.880]   I think you did a great job.
[00:32:18.880 --> 00:32:20.680]   I mean, I have done a lot of great things.
[00:32:20.680 --> 00:32:21.680]   Oh, thank you.
[00:32:21.680 --> 00:32:22.680]   I mean, I don't know.
[00:32:22.680 --> 00:32:23.680]   I'm a fan.
[00:32:23.680 --> 00:32:32.200]   If you're lucky enough to have a job where, I mean, I think in the best situation, working
[00:32:32.200 --> 00:32:36.840]   really hard can be incredibly fun for me.
[00:32:36.840 --> 00:32:44.520]   And I realize I run a company and I'm coming from a particular point of view, but for me,
[00:32:44.520 --> 00:32:48.960]   working really hard can be a real joy.
[00:32:48.960 --> 00:32:53.560]   When I actually started my second company, one of the really fun things for me was that
[00:32:53.560 --> 00:32:57.560]   it actually made sense for me to pull an all-nighter once in a while.
[00:32:57.560 --> 00:32:58.560]   I hadn't done that in a long time.
[00:32:58.560 --> 00:32:59.560]   Really?
[00:32:59.560 --> 00:33:00.560]   Do you still pull all-nighters?
[00:33:00.560 --> 00:33:01.560]   No, I have a baby.
[00:33:01.560 --> 00:33:02.560]   I pull a different kind of all-nighter.
[00:33:02.560 --> 00:33:03.560]   A natural all-nighter.
[00:33:03.560 --> 00:33:04.560]   Yeah, a natural all-nighter.
[00:33:04.560 --> 00:33:05.560]   It's an unproductive all-nighter.
[00:33:05.560 --> 00:33:18.280]   But I also think that, you know, the important thing is a company is trying to do something
[00:33:18.280 --> 00:33:21.920]   and figuring out how to do it is the important thing.
[00:33:21.920 --> 00:33:28.760]   And figuring out how to do that over the long term, it needs to be a sustainable pace because
[00:33:28.760 --> 00:33:32.160]   if you work hard and burn out, that's super counterproductive.
[00:33:32.160 --> 00:33:36.480]   But I don't always think that burnout actually comes from working hard.
[00:33:36.480 --> 00:33:40.600]   I think burnout comes more from working on things that seem pointless or not seeing the
[00:33:40.600 --> 00:33:41.600]   success that you wanted.
[00:33:41.600 --> 00:33:42.600]   I see.
[00:33:42.600 --> 00:33:43.600]   Yeah, that doesn't make sense.
[00:33:43.600 --> 00:33:47.680]   Yeah, I don't know.
[00:33:47.680 --> 00:33:51.200]   For me, seeing people on the weekend is not a bad thing.
[00:33:51.200 --> 00:33:56.480]   Even me, I also feel motivated to go on the weekends because I really like what I do and
[00:33:56.480 --> 00:33:58.640]   I have faith in it.
[00:33:58.640 --> 00:34:00.560]   I also want to contribute.
[00:34:00.560 --> 00:34:04.160]   If on a Saturday night, I could stay at home and watch some Bachelor.
[00:34:04.160 --> 00:34:07.160]   I don't know, watch it.
[00:34:07.160 --> 00:34:10.400]   It's what people seem to be watching.
[00:34:10.400 --> 00:34:14.480]   Or I could just go on our GitHub and check out some PR.
[00:34:14.480 --> 00:34:19.960]   I know that sounds like a horrible analogy.
[00:34:19.960 --> 00:34:22.960]   But yeah, I feel motivated.
[00:34:22.960 --> 00:34:24.760]   So anyway, back to the question.
[00:34:24.760 --> 00:34:32.720]   So the first thing I noticed is the different workload.
[00:34:32.720 --> 00:34:37.840]   And I feel okay working on the weekends because I can still talk to my co-workers on the weekend
[00:34:37.840 --> 00:34:40.360]   and I like it.
[00:34:40.360 --> 00:34:46.480]   And it's a good understanding because I think everyone knows that some people work on the
[00:34:46.480 --> 00:34:49.360]   weekends, so it also gives them more flexibility.
[00:34:49.360 --> 00:34:54.080]   If you're on the weekday, you feel burnt out and tired and want to take a day or two off
[00:34:54.080 --> 00:34:56.960]   during the week and it's fine as well.
[00:34:56.960 --> 00:35:00.520]   So I think I like the understanding of just making the schedule at the book for you.
[00:35:00.520 --> 00:35:07.200]   You don't necessarily follow a typical five-day work week and work on the weekdays and then
[00:35:07.200 --> 00:35:08.200]   take a week off.
[00:35:08.200 --> 00:35:15.200]   I think the second thing is the amount of exposure I have to the entire stack.
[00:35:15.200 --> 00:35:20.800]   So working in a big company, I'm focused on a very specific product and I'm shielded away
[00:35:20.800 --> 00:35:29.000]   from one aspect as in QA or client.
[00:35:29.000 --> 00:35:33.640]   But at startup, I get a chance to see everything and we are building the stack from scratch.
[00:35:33.640 --> 00:35:39.720]   So I'm so exposed to the decisions that we have to make, for example, what CI/CD tools
[00:35:39.720 --> 00:35:42.720]   to use or how we should structure our repo.
[00:35:42.720 --> 00:35:48.080]   Do you want a monorepo or do you want a very small repo?
[00:35:48.080 --> 00:35:53.600]   So these decisions are what tools to use because when you join a big company, usually you have
[00:35:53.600 --> 00:35:57.080]   to use standard tools and somebody already made a decision for you.
[00:35:57.080 --> 00:36:03.560]   But as a startup, you also have a say in choosing the tools which expose you to various problems
[00:36:03.560 --> 00:36:04.560]   as well.
[00:36:04.560 --> 00:36:05.560]   So I really like it.
[00:36:05.560 --> 00:36:06.560]   And also, of course, the size.
[00:36:06.560 --> 00:36:07.560]   At a big company, there are a lot of people.
[00:36:07.560 --> 00:36:16.280]   I think it's nice at a big company as you have access to a lot of people.
[00:36:16.280 --> 00:36:22.040]   But at a startup, you have a small number of colleagues, so you can't randomly message
[00:36:22.040 --> 00:36:27.880]   somebody from another team and ask for a coffee because only you have people on the team.
[00:36:27.880 --> 00:36:28.880]   Cool.
[00:36:28.880 --> 00:36:29.880]   Makes sense.
[00:36:29.880 --> 00:36:36.320]   All right, so I think we're kind of going over time and we always end with two questions.
[00:36:36.320 --> 00:36:39.440]   I'm really kind of curious to hear your takes on these.
[00:36:39.440 --> 00:36:47.540]   So the first question is, what is a topic in machine learning that you think people
[00:36:47.540 --> 00:36:50.640]   don't talk enough about or they don't talk about?
[00:36:50.640 --> 00:36:55.120]   It's an underrated topic that people should talk about more, but they don't.
[00:36:55.120 --> 00:36:58.920]   So I think I have a list of things I usually throw around.
[00:36:58.920 --> 00:37:03.360]   One of them is a graph.
[00:37:03.360 --> 00:37:04.360]   So I love graphs.
[00:37:04.360 --> 00:37:06.640]   I usually think of graphs as underrated.
[00:37:06.640 --> 00:37:11.080]   I think I had a tweet about it a few years ago.
[00:37:11.080 --> 00:37:12.920]   But I think it has changed.
[00:37:12.920 --> 00:37:18.200]   I was at NeurIPS in December and I saw so many papers on graphs and there's a workshop
[00:37:18.200 --> 00:37:19.200]   on graphs.
[00:37:19.200 --> 00:37:21.560]   That was the most attended workshop.
[00:37:21.560 --> 00:37:22.560]   So I think...
[00:37:22.560 --> 00:37:24.560]   You're talking about the computer science sense of graph, right?
[00:37:24.560 --> 00:37:27.560]   Or do you mean like literally like computation?
[00:37:27.560 --> 00:37:28.880]   Graph and network.
[00:37:28.880 --> 00:37:29.880]   So graph theory.
[00:37:29.880 --> 00:37:30.880]   Like a network.
[00:37:30.880 --> 00:37:31.880]   Yeah.
[00:37:31.880 --> 00:37:37.720]   So now we have several, there are a lot of graphs like GNN or GCNN, a graph convolution
[00:37:37.720 --> 00:37:39.160]   network.
[00:37:39.160 --> 00:37:44.440]   So I think there are multiple uses of graphs in deep learning.
[00:37:44.440 --> 00:37:48.200]   So graph is natural representations of many inputs, right?
[00:37:48.200 --> 00:37:51.360]   So we have a data from a network, for example, is a graph.
[00:37:51.360 --> 00:37:55.760]   Or a recommendation system, then we have users and items and it's a bipartite graph.
[00:37:55.760 --> 00:37:59.200]   So graph is a natural representation of input.
[00:37:59.200 --> 00:38:01.400]   And also graphs can also make output.
[00:38:01.400 --> 00:38:08.160]   So a lot of distributions can be represented using graphs, like pictorial graphs.
[00:38:08.160 --> 00:38:11.320]   So graphs can represent both input and output.
[00:38:11.320 --> 00:38:15.340]   And also graphs can also have a lot of relationship with convolutions.
[00:38:15.340 --> 00:38:18.340]   So they're both local.
[00:38:18.340 --> 00:38:23.080]   So they both focus on the local connections.
[00:38:23.080 --> 00:38:28.120]   So graphs are the point connected to one's neighboring points and convolutions when you
[00:38:28.120 --> 00:38:33.200]   have a very local linear transformation.
[00:38:33.200 --> 00:38:36.320]   I'm having a very hand-wavy explanation right now.
[00:38:36.320 --> 00:38:43.280]   So yeah, I really love graphs and I'm so happy to see that it's catching on.
[00:38:43.280 --> 00:38:47.360]   So I think other things underrated is the engineering aspect for machine learning.
[00:38:47.360 --> 00:38:56.040]   I haven't seen much talking about integrations for deep learning or version control.
[00:38:56.040 --> 00:38:58.280]   I think all of those people are so unique.
[00:38:58.280 --> 00:38:59.280]   I'm not unique.
[00:38:59.280 --> 00:39:04.720]   I feel like what I see, other people have already seen and the Combi is catching up
[00:39:04.720 --> 00:39:05.720]   on it.
[00:39:05.720 --> 00:39:06.720]   Cool.
[00:39:06.720 --> 00:39:07.720]   Good answer.
[00:39:07.720 --> 00:39:08.720]   Hopefully.
[00:39:08.720 --> 00:39:09.720]   Obviously, I agree.
[00:39:09.720 --> 00:39:21.000]   The second question is, in your experience, I'm really curious about this one actually.
[00:39:21.000 --> 00:39:23.000]   So in your experience, taking...
[00:39:23.000 --> 00:39:27.560]   Oh wait, can I just add another thing?
[00:39:27.560 --> 00:39:29.960]   I feel like it's underrated.
[00:39:29.960 --> 00:39:31.560]   I'm very production oriented.
[00:39:31.560 --> 00:39:34.360]   So I think it's like monitoring.
[00:39:34.360 --> 00:39:39.960]   So it's not experiment tracking or training, but monitoring in the real world, in deployed
[00:39:39.960 --> 00:39:42.120]   systems and how do you monitor it?
[00:39:42.120 --> 00:39:44.600]   How do you know when you need to return the model?
[00:39:44.600 --> 00:39:48.160]   How do you know the data distribution has shifted?
[00:39:48.160 --> 00:39:51.280]   So I haven't seen a lot of monitoring.
[00:39:51.280 --> 00:39:52.280]   So I think it's still very underrated.
[00:39:52.280 --> 00:39:53.280]   Yeah, totally.
[00:39:53.280 --> 00:39:54.280]   Yeah, sorry.
[00:39:54.280 --> 00:39:55.280]   No, no.
[00:39:55.280 --> 00:39:56.280]   And maybe that's the answer to this.
[00:39:56.280 --> 00:39:57.280]   I'm trying to sound smart.
[00:39:57.280 --> 00:40:03.600]   I think you're successfully sounding smart.
[00:40:03.600 --> 00:40:11.280]   But the second question is, in your experience, taking projects from training into production
[00:40:11.280 --> 00:40:13.280]   deployed systems, what's the biggest bottleneck?
[00:40:13.280 --> 00:40:19.760]   What's the hardest step in that?
[00:40:19.760 --> 00:40:27.120]   One is really right now is when you have very big models, it's really slow to infer anything.
[00:40:27.120 --> 00:40:28.120]   Wow, cool.
[00:40:28.120 --> 00:40:29.680]   Yeah, it's very slow.
[00:40:29.680 --> 00:40:34.080]   And it can be very costly.
[00:40:34.080 --> 00:40:42.800]   So for example, if you try to take a GPT-2 to go into production, and even if you spot
[00:40:42.800 --> 00:40:49.600]   instances, it costs quite a bit for every inferencing, for every time you make a prediction,
[00:40:49.600 --> 00:40:52.200]   like you generate something.
[00:40:52.200 --> 00:40:56.240]   So yeah, so that's hard.
[00:40:56.240 --> 00:41:02.480]   So yes, that's why we haven't seen a lot of GPT-2 in production yet.
[00:41:02.480 --> 00:41:03.840]   And it's a very interesting problem.
[00:41:03.840 --> 00:41:10.480]   So I'm not sure if I can mention the exact company, but some startup told me they're
[00:41:10.480 --> 00:41:17.200]   using GPT-2 in production.
[00:41:17.200 --> 00:41:22.520]   And they said if they could reduce the inference time by half, they could be able to break
[00:41:22.520 --> 00:41:23.520]   even.
[00:41:23.520 --> 00:41:31.120]   So for example, if they can, instead of using the normal precision point, if they can somehow
[00:41:31.120 --> 00:41:38.240]   make it work on, for example, FP16, like half precision point, then it can reduce the inference
[00:41:38.240 --> 00:41:39.320]   time by half.
[00:41:39.320 --> 00:41:44.520]   And therefore, when I help the company make a state of float, I think it makes a really
[00:41:44.520 --> 00:41:45.520]   big difference.
[00:41:45.520 --> 00:41:51.520]   Like you just break even or not, especially in this economy.
[00:41:51.520 --> 00:41:54.120]   Yeah, totally.
[00:41:54.120 --> 00:41:55.440]   Wow.
[00:41:55.440 --> 00:41:56.440]   Great answer.
[00:41:56.440 --> 00:41:57.440]   Interesting.
[00:41:57.440 --> 00:41:59.760]   My final question actually is simple.
[00:41:59.760 --> 00:42:05.440]   So if people want to learn more about your work, do you have a Twitter account or a company
[00:42:05.440 --> 00:42:07.400]   that you want to tell us about?
[00:42:07.400 --> 00:42:10.120]   I spend too much time on Twitter and I'm ashamed.
[00:42:10.120 --> 00:42:12.320]   Follow me on Twitter.
[00:42:12.320 --> 00:42:15.320]   What's your Twitter handle?
[00:42:15.320 --> 00:42:16.320]   It's @chipro.
[00:42:16.320 --> 00:42:25.760]   And it's not chipro as like professional, it's more like RO means crazy in Vietnamese.
[00:42:25.760 --> 00:42:26.760]   Oh really?
[00:42:26.760 --> 00:42:27.760]   I did not know that.
[00:42:27.760 --> 00:42:28.760]   Yeah.
[00:42:28.760 --> 00:42:29.760]   I said dental.
[00:42:29.760 --> 00:42:30.760]   All right.
[00:42:30.760 --> 00:42:31.760]   We should end with that.
[00:42:31.760 --> 00:42:32.760]   Yeah.
[00:42:32.760 --> 00:42:33.760]   Thanks.
[00:42:33.760 --> 00:42:34.760]   I also have a blog about tech and stuff.
[00:42:34.760 --> 00:42:35.760]   I write long form.
[00:42:35.760 --> 00:42:48.320]   My blog takes me like two to three months to write.
[00:42:48.320 --> 00:42:49.320]   Yeah.
[00:42:49.320 --> 00:42:52.320]   So I don't write a lot.
[00:42:52.320 --> 00:42:53.320]   That's great.
[00:42:53.320 --> 00:42:54.320]   Thank you.
[00:42:54.320 --> 00:42:55.320]   Thank you.
[00:42:55.320 --> 00:42:56.320]   Coming from you means a lot.
[00:42:56.320 --> 00:42:56.320]   Yeah.
[00:42:56.880 --> 00:42:57.880]   It's hard.
[00:42:57.880 --> 00:42:57.880]   Yeah.
[00:42:57.880 --> 00:43:02.880]   Thanks.
[00:43:02.880 --> 00:43:07.060]   [Music]

