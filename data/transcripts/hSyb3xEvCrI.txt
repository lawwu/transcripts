
[00:00:00.000 --> 00:00:04.340]   Hey Pete, what are you working on?
[00:00:04.340 --> 00:00:10.400]   So I'm spending a bunch of time at home like everybody else, but I'm doing a bunch of writing,
[00:00:10.400 --> 00:00:15.360]   doing some angel investing, and I'm actually doing a bunch of gardening as well.
[00:00:15.360 --> 00:00:17.800]   And so I've taken this time to like kind of reboot my garden.
[00:00:17.800 --> 00:00:21.320]   And one of the problems that I have that's driving me crazy is raccoons.
[00:00:21.320 --> 00:00:25.280]   So I'm in San Francisco and there's a lot of raccoons in our neighborhood that are just
[00:00:25.280 --> 00:00:27.040]   like tearing everything up.
[00:00:27.040 --> 00:00:34.000]   And so I view this as an opportunity to play around with, Pete Worden has this great book,
[00:00:34.000 --> 00:00:35.000]   Tiny ML.
[00:00:35.000 --> 00:00:36.000]   Oh yeah.
[00:00:36.000 --> 00:00:38.560]   And it uses TensorFlow Lite.
[00:00:38.560 --> 00:00:40.000]   And so I'm going to play around with that.
[00:00:40.000 --> 00:00:42.920]   And actually there are these robots.
[00:00:42.920 --> 00:00:47.000]   So you can get these on Amazon or from Walmart or something.
[00:00:47.000 --> 00:00:48.000]   And so it's really cool.
[00:00:48.000 --> 00:00:49.000]   It's called Mechamon.
[00:00:49.000 --> 00:00:53.800]   So the company unfortunately no longer exists, but you can get these kind of cheap.
[00:00:53.800 --> 00:00:59.480]   And somebody did some Bluetooth sniffing and actually put out a Python API.
[00:00:59.480 --> 00:01:01.000]   So you can actually control it.
[00:01:01.000 --> 00:01:02.000]   It's got servos.
[00:01:02.000 --> 00:01:03.000]   It's pretty robust.
[00:01:03.000 --> 00:01:04.000]   It can move pretty quickly.
[00:01:04.000 --> 00:01:09.160]   And so I'm going to do a project to make these kind of sentinels in my backyard to protect
[00:01:09.160 --> 00:01:11.000]   the garden from raccoons.
[00:01:11.000 --> 00:01:12.000]   Hopefully non-lethal.
[00:01:12.000 --> 00:01:13.000]   Protect.
[00:01:13.000 --> 00:01:14.000]   Protect.
[00:01:14.000 --> 00:01:15.000]   Protect and serve.
[00:01:15.000 --> 00:01:16.000]   Yeah.
[00:01:16.000 --> 00:01:17.000]   Yeah.
[00:01:17.000 --> 00:01:18.000]   Non-lethal.
[00:01:18.000 --> 00:01:20.280]   But I think raccoons, they're afraid of things like dogs.
[00:01:20.280 --> 00:01:24.040]   So if I can get this to bark like a dog, maybe that would do it.
[00:01:24.040 --> 00:01:26.360]   I feel like the San Francisco raccoons are afraid of nothing.
[00:01:26.360 --> 00:01:27.360]   I don't know.
[00:01:27.360 --> 00:01:28.360]   I'm curious how this goes for you.
[00:01:28.360 --> 00:01:29.360]   But I got to show you.
[00:01:29.360 --> 00:01:30.360]   I got the same book.
[00:01:30.360 --> 00:01:31.360]   So this is not quite as big.
[00:01:31.360 --> 00:01:32.360]   I put this together myself.
[00:01:32.360 --> 00:01:33.360]   I'm some kid off Alibaba.
[00:01:33.360 --> 00:01:34.360]   By the way, I have another link.
[00:01:34.360 --> 00:01:49.280]   I found it just like this week.
[00:01:49.280 --> 00:01:54.320]   There's an open source project called Spot Micro.
[00:01:54.320 --> 00:01:57.720]   And so if I go and buy a 3D printer, I could put it together.
[00:01:57.720 --> 00:02:00.400]   But it's like an open source spot mini.
[00:02:00.400 --> 00:02:04.200]   And there's actually, I think it might've been OpenAI or somebody else.
[00:02:04.200 --> 00:02:07.240]   They put it in the simulation framework.
[00:02:07.240 --> 00:02:08.240]   So there's a model.
[00:02:08.240 --> 00:02:09.240]   Oh, called Majoko?
[00:02:09.240 --> 00:02:10.320]   Yeah, I think it might be.
[00:02:10.320 --> 00:02:11.320]   Yeah.
[00:02:11.320 --> 00:02:15.040]   So you can actually train it in a simulated environment.
[00:02:15.040 --> 00:02:16.520]   But I think it's still very nascent.
[00:02:16.520 --> 00:02:19.680]   I haven't seen an actual working video of this thing working.
[00:02:19.680 --> 00:02:21.200]   So I think people are still trying to build it.
[00:02:21.200 --> 00:02:22.960]   It's not quite working yet.
[00:02:22.960 --> 00:02:26.780]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:02:26.780 --> 00:02:28.440]   models work in the real world.
[00:02:28.440 --> 00:02:30.640]   I'm your host, Lukas Biewald.
[00:02:30.640 --> 00:02:33.600]   Pete's been working in data and ML for a really long time.
[00:02:33.600 --> 00:02:36.720]   He was the former head of data products at Workday.
[00:02:36.720 --> 00:02:40.700]   Before that, he founded a deep learning startup called SkipFlag, which was actually acquired
[00:02:40.700 --> 00:02:42.080]   by Workday.
[00:02:42.080 --> 00:02:46.100]   Before that, he was a principal data scientist at LinkedIn, and he's done a bunch of other
[00:02:46.100 --> 00:02:47.100]   awesome stuff.
[00:02:47.100 --> 00:02:48.600]   I'm really excited to talk to him.
[00:02:48.600 --> 00:02:54.720]   I kind of wanted to start off with going back to your first job where you're doing ML data
[00:02:54.720 --> 00:02:55.720]   science.
[00:02:55.720 --> 00:03:00.960]   I guess maybe even before LinkedIn, AOL, I think it was, right?
[00:03:00.960 --> 00:03:03.040]   Well, yeah, even before that.
[00:03:03.040 --> 00:03:09.040]   So I think when we met, I was at AOL or just finishing up at AOL search.
[00:03:09.040 --> 00:03:13.160]   And so that's an interesting conversation in and of itself.
[00:03:13.160 --> 00:03:14.800]   I was working on the search team there.
[00:03:14.800 --> 00:03:19.800]   I had just started, came there from MIT.
[00:03:19.800 --> 00:03:23.280]   In the week I started, there was a release of user search data.
[00:03:23.280 --> 00:03:26.960]   Oh, Abder, man, we got to get him on the podcast.
[00:03:26.960 --> 00:03:27.960]   Oh, yeah, Abder.
[00:03:27.960 --> 00:03:28.960]   Yeah.
[00:03:28.960 --> 00:03:33.120]   So that was weirdest first week on the job ever, for sure.
[00:03:33.120 --> 00:03:36.840]   Everything was in disarray when I started.
[00:03:36.840 --> 00:03:44.000]   But going back, so first job, I mean, when I was an undergrad, I worked in physics and
[00:03:44.000 --> 00:03:46.160]   neuroscience.
[00:03:46.160 --> 00:03:53.480]   And so machine learning was, I think, kind of seen as voodoo to those folks back then.
[00:03:53.480 --> 00:03:58.080]   So there was this thing called, you would do deconvolution of signals.
[00:03:58.080 --> 00:03:59.920]   So I worked on a project.
[00:03:59.920 --> 00:04:02.480]   It was a summer project.
[00:04:02.480 --> 00:04:06.680]   We were working on antimatter, which just sounds cool, right?
[00:04:06.680 --> 00:04:16.560]   But positronium is, it's basically a positron and electron that forms an atom kind of like
[00:04:16.560 --> 00:04:19.400]   hydrogen except with matter and antimatter.
[00:04:19.400 --> 00:04:24.200]   Anyway, long story longer, the hard part was you're taking these measurements, real world
[00:04:24.200 --> 00:04:25.880]   sensor data.
[00:04:25.880 --> 00:04:29.040]   You're trying to detect the decay of this atom, right?
[00:04:29.040 --> 00:04:33.960]   Because when the positron and electron are near each other, eventually they annihilate,
[00:04:33.960 --> 00:04:34.960]   right?
[00:04:34.960 --> 00:04:36.800]   They're going to be cut off radiation.
[00:04:36.800 --> 00:04:41.240]   And so you have all these sensors and equipment set up to measure that annihilation.
[00:04:41.240 --> 00:04:47.840]   But the problem is everything that you're using to measure, it pollutes the signal,
[00:04:47.840 --> 00:04:48.840]   right?
[00:04:48.840 --> 00:04:54.000]   So what you actually measure is this convolution of the raw data with those signals.
[00:04:54.000 --> 00:04:59.440]   And so at the time, my task, one of my projects was to deconvolve that signal.
[00:04:59.440 --> 00:05:06.960]   So deconvolution is one of the kind of core machine learning problems from back in the
[00:05:06.960 --> 00:05:07.960]   day.
[00:05:07.960 --> 00:05:10.040]   It's kind of like the cocktail party problem.
[00:05:10.040 --> 00:05:14.920]   You have a bunch of people speaking and you need to disentangle who's the different voices
[00:05:14.920 --> 00:05:16.920]   in a recording.
[00:05:16.920 --> 00:05:23.360]   Anyway, so that's actually where I think I first, that and the work that I did in neuroscience,
[00:05:23.360 --> 00:05:26.040]   I was really interested in neural networks basically, right?
[00:05:26.040 --> 00:05:30.400]   And interested in process.
[00:05:30.400 --> 00:05:35.120]   I found the processing of the data more interesting than the actual experiments, which are a lot
[00:05:35.120 --> 00:05:38.960]   of hard work and a lot of time in the lab.
[00:05:38.960 --> 00:05:45.180]   So I really just dug deeply into signal processing and machine learning.
[00:05:45.180 --> 00:05:51.480]   And so before I got to AOL, and I actually had two other roles where I cut my teeth on
[00:05:51.480 --> 00:05:52.940]   big data sets.
[00:05:52.940 --> 00:05:58.040]   The first one was actually back in the first dot com crash.
[00:05:58.040 --> 00:06:02.120]   It was a company called ProfitLogic and they ended up getting acquired by Oracle and became
[00:06:02.120 --> 00:06:04.040]   Oracle Retail.
[00:06:04.040 --> 00:06:07.960]   But we basically, this is back in the day when they'd ship the data to you on tapes.
[00:06:07.960 --> 00:06:12.840]   So if you had a customer to get access to their data, people say it's hard right now
[00:06:12.840 --> 00:06:16.440]   to get access to customer data when you're dealing with enterprise customers.
[00:06:16.440 --> 00:06:20.480]   But back then we actually had to get tapes of data sent to us.
[00:06:20.480 --> 00:06:25.440]   And so a lot of point of sale data, retail sale data, and we were building predictive
[00:06:25.440 --> 00:06:28.720]   models for retail sales.
[00:06:28.720 --> 00:06:37.480]   And then I worked in biodefense at MIT, which is unfortunately a relevant topic today with
[00:06:37.480 --> 00:06:39.160]   the pandemic happening.
[00:06:39.160 --> 00:06:46.160]   But we were working on these kinds of things, how do you detect and measure and prevent
[00:06:46.160 --> 00:06:54.160]   these kinds of things, both naturally occurring as well as, you can imagine, a terrorist attack
[00:06:54.160 --> 00:06:57.280]   with some kind of weaponized bio agent.
[00:06:57.280 --> 00:07:06.000]   So doing predictive modeling in the first dot com boom, you get tapes.
[00:07:06.000 --> 00:07:09.640]   How big is the data and what kind of models are you building?
[00:07:09.640 --> 00:07:12.480]   What's your tool set like?
[00:07:12.480 --> 00:07:13.480]   That's a long time ago.
[00:07:13.480 --> 00:07:16.240]   But thinking back, you know what's funny?
[00:07:16.240 --> 00:07:17.840]   We were actually using Python, right?
[00:07:17.840 --> 00:07:19.640]   So we were using Python and C++.
[00:07:19.640 --> 00:07:23.680]   And so at that time, I don't think almost nobody was using Python.
[00:07:23.680 --> 00:07:26.360]   Yeah, Google, right?
[00:07:26.360 --> 00:07:27.600]   Google, that's right.
[00:07:27.600 --> 00:07:33.560]   So Google, it was kind of Google and then like a handful of other startups around that
[00:07:33.560 --> 00:07:36.240]   late 90s, early 2000s.
[00:07:36.240 --> 00:07:41.200]   But in the enterprise, Python was not really widely adopted yet.
[00:07:41.200 --> 00:07:48.000]   But this was really close to the metal kind of work because you get the data.
[00:07:48.000 --> 00:07:53.200]   I don't remember the exact volume, but you can imagine a customer like Walmart, right?
[00:07:53.200 --> 00:08:00.520]   They have thousands of stores, thousands of products, and each of those products has different
[00:08:00.520 --> 00:08:04.160]   sizes and colors and styles.
[00:08:04.160 --> 00:08:10.400]   So it's really at the SKU store level was the granularity of data and then every transaction.
[00:08:10.400 --> 00:08:13.520]   So similar to what something like Square might have now, right?
[00:08:13.520 --> 00:08:18.520]   Where you have somebody buys a coffee, you get that transactional event.
[00:08:18.520 --> 00:08:21.720]   But at the time, the lag was really long, right?
[00:08:21.720 --> 00:08:24.320]   So these are brick and mortar retailers.
[00:08:24.320 --> 00:08:28.720]   And so they were mostly running on Oracle or DB2 or something like that.
[00:08:28.720 --> 00:08:34.360]   And they would have their point of sale system and then that would be aggregated up, all
[00:08:34.360 --> 00:08:40.440]   that raw data usually to something like this shirt, it would be a SKU.
[00:08:40.440 --> 00:08:45.960]   And if you look at the sales of that SKU, you could see the sales nationally.
[00:08:45.960 --> 00:08:48.840]   And then typically what would happen is there's markdown.
[00:08:48.840 --> 00:08:54.040]   So two weeks in, you lower the price and then there's a sales bump depending on the elasticity
[00:08:54.040 --> 00:08:57.280]   of the item, the price elasticity.
[00:08:57.280 --> 00:09:01.840]   And so really the machine learning problem there was, okay, the first few days, actually
[00:09:01.840 --> 00:09:03.080]   I think we cut it pretty close.
[00:09:03.080 --> 00:09:05.320]   So we had a weak turnaround time.
[00:09:05.320 --> 00:09:08.680]   We would get the raw data and then we would run our models.
[00:09:08.680 --> 00:09:16.240]   So some Python and C++ models to do forecasting and optimization and model fitting.
[00:09:16.240 --> 00:09:20.040]   And then we would basically want to spit back price recommendations.
[00:09:20.040 --> 00:09:27.440]   So for every SKU, we're telling the retailer, hey, mark it down 10%, mark it down 15% so
[00:09:27.440 --> 00:09:31.360]   that you'll optimize sales over the entire season.
[00:09:31.360 --> 00:09:37.960]   And many times what ended up happening is it would take a few days to load the data.
[00:09:37.960 --> 00:09:40.040]   We'd have to get the data from tapes.
[00:09:40.040 --> 00:09:41.260]   We'd have to load it.
[00:09:41.260 --> 00:09:43.820]   There could be an issue there.
[00:09:43.820 --> 00:09:48.560]   Then you have to get it into our models, run the models, which I think that could take
[00:09:48.560 --> 00:09:51.560]   like a day or two at that time.
[00:09:51.560 --> 00:09:57.960]   And then at the end of the day, this is going to sound really bad, but QA at that point
[00:09:57.960 --> 00:10:03.720]   in time looked like, so the final product we would send was basically a CSV report that
[00:10:03.720 --> 00:10:07.280]   would go to the retailer that they would then put into their point of sale system or their
[00:10:07.280 --> 00:10:09.920]   buying system.
[00:10:09.920 --> 00:10:15.080]   But there were some nights where we actually would just print out the CSVs and look at
[00:10:15.080 --> 00:10:22.080]   them and then mark, hey, this looks wrong with the marker and then go back and actually
[00:10:22.080 --> 00:10:26.640]   go in with Python and fix and change the models.
[00:10:26.640 --> 00:10:28.600]   And those are late nights.
[00:10:28.600 --> 00:10:29.920]   And what was your title at that point?
[00:10:29.920 --> 00:10:31.640]   What did they call you?
[00:10:31.640 --> 00:10:36.080]   I think back then it was kind of like being a glorified grad student.
[00:10:36.080 --> 00:10:40.640]   But I think my title was analyst or data analyst.
[00:10:40.640 --> 00:10:48.240]   But data scientists didn't come around until 2009 or 2010.
[00:10:48.240 --> 00:10:56.600]   But then you actually were one of the original data scientists at LinkedIn in the early days.
[00:10:56.600 --> 00:10:57.600]   Yeah.
[00:10:57.600 --> 00:11:04.200]   With a team that I think everyone's gone on to do really awesome stuff.
[00:11:04.200 --> 00:11:08.320]   How did you come across LinkedIn that early?
[00:11:08.320 --> 00:11:21.680]   So after I did the stuff at MIT, I went back and did some neural network grad work at MIT.
[00:11:21.680 --> 00:11:23.640]   And then I was at AOL.
[00:11:23.640 --> 00:11:25.160]   I wanted to move into consumer Internet.
[00:11:25.160 --> 00:11:30.100]   So once I saw it was actually like a year after I left Pravlogic that they got acquired.
[00:11:30.100 --> 00:11:36.400]   And so I think I kind of got bit by the startup bug when I saw, you know, hey, there is a
[00:11:36.400 --> 00:11:37.600]   light at the end of the tunnel.
[00:11:37.600 --> 00:11:39.960]   These things can work.
[00:11:39.960 --> 00:11:43.860]   And so I was eager to get into consumer Internet.
[00:11:43.860 --> 00:11:47.040]   So I was at AOL search in the D.C. area.
[00:11:47.040 --> 00:11:52.080]   They're based in Virginia or they were based there.
[00:11:52.080 --> 00:11:56.960]   And so my goal was to just move out to the Bay Area.
[00:11:56.960 --> 00:12:03.380]   And LinkedIn actually when I left AOL, that was the first time I signed up for LinkedIn.
[00:12:03.380 --> 00:12:06.640]   Because when you would back then at least when you would leave a company, you would
[00:12:06.640 --> 00:12:09.700]   get all these LinkedIn invites from your co-workers.
[00:12:09.700 --> 00:12:11.080]   And I hadn't created an account yet.
[00:12:11.080 --> 00:12:12.080]   So this is back.
[00:12:12.080 --> 00:12:13.880]   It must have been like 2008.
[00:12:13.880 --> 00:12:16.880]   I got LinkedIn invite, signed up.
[00:12:16.880 --> 00:12:18.280]   And I actually I like the product.
[00:12:18.280 --> 00:12:21.560]   There was a group, you know, groups was a bigger feature back then.
[00:12:21.560 --> 00:12:27.800]   So I was on a lot of the early like Hadoop groups and machine learning groups on LinkedIn,
[00:12:27.800 --> 00:12:28.800]   connected with a lot of people.
[00:12:28.800 --> 00:12:33.400]   And so I think that's how we actually met was maybe via Mike Driscoll had like a big
[00:12:33.400 --> 00:12:35.120]   data community back then.
[00:12:35.120 --> 00:12:40.360]   It was blogging and he went on to found Metamarkets.
[00:12:40.360 --> 00:12:44.280]   But yeah, so LinkedIn basically I came out to the Bay Area and you know, like a lot of
[00:12:44.280 --> 00:12:49.480]   people, you know, was interviewing, talking to a bunch of different startups.
[00:12:49.480 --> 00:12:58.240]   LinkedIn was interesting to me because I think a big reason was Reid Hoffman, the founder,
[00:12:58.240 --> 00:13:01.760]   is really big on networks.
[00:13:01.760 --> 00:13:09.320]   And I was a big believer in the power of networks and connecting, you know, the people, the
[00:13:09.320 --> 00:13:10.320]   communities.
[00:13:10.320 --> 00:13:12.520]   This was before Twitter.
[00:13:12.520 --> 00:13:17.420]   I think Twitter was out, but Twitter was still pretty early.
[00:13:17.420 --> 00:13:20.320]   So there was LinkedIn, Twitter, Facebook.
[00:13:20.320 --> 00:13:27.000]   I was a big believer in the mission of LinkedIn specifically because it's unfortunately relevant
[00:13:27.000 --> 00:13:28.000]   again now.
[00:13:28.000 --> 00:13:34.040]   I think there's nothing more meaningful you can do than get someone a job.
[00:13:34.040 --> 00:13:41.840]   And people working on things that is important to them and fulfilling and that feels like
[00:13:41.840 --> 00:13:45.760]   it matters, I think is really important.
[00:13:45.760 --> 00:13:49.400]   And so it seemed like a great opportunity to leverage data.
[00:13:49.400 --> 00:13:55.540]   They had amassed this large data set of all these people, their profiles, their connections,
[00:13:55.540 --> 00:13:59.760]   but they hadn't really flipped that switch yet, that machine learning or data science
[00:13:59.760 --> 00:14:04.100]   switch to leverage it to have impact.
[00:14:04.100 --> 00:14:06.580]   And that was just beginning when I got there.
[00:14:06.580 --> 00:14:08.060]   And so what were the early projects?
[00:14:08.060 --> 00:14:10.660]   What were you doing there?
[00:14:10.660 --> 00:14:12.700]   Yeah, so it was interesting.
[00:14:12.700 --> 00:14:19.620]   A lot of the core elements that you see today were there at that point.
[00:14:19.620 --> 00:14:23.700]   So there was a profile, you could connect to people.
[00:14:23.700 --> 00:14:25.860]   There was an early version of people you may know.
[00:14:25.860 --> 00:14:29.460]   So Jonathan Goldman was the first data scientist to build that.
[00:14:29.460 --> 00:14:32.020]   But back then it was running on SQL.
[00:14:32.020 --> 00:14:35.860]   So it was like a big SQL query that would take, I don't know, like a few days to run
[00:14:35.860 --> 00:14:38.900]   or a series of SQL queries.
[00:14:38.900 --> 00:14:40.080]   And the network was much smaller.
[00:14:40.080 --> 00:14:43.780]   So I think there were only maybe like 10 million members or something at that point.
[00:14:43.780 --> 00:14:46.280]   Now there's probably like over 500 million.
[00:14:46.280 --> 00:14:49.660]   I don't know the latest number on LinkedIn.
[00:14:49.660 --> 00:14:53.900]   But at that point, the data was small enough that you could sort of make that work.
[00:14:53.900 --> 00:14:58.740]   But I think it would take, I think it would actually take over a week to run people you
[00:14:58.740 --> 00:15:01.660]   may know at that point.
[00:15:01.660 --> 00:15:08.260]   And so some of the first products I worked on, the first one, actually, DJ Patil, who
[00:15:08.260 --> 00:15:12.380]   was running the team at the time, I was lucky enough, based on some of the stuff I'd done
[00:15:12.380 --> 00:15:14.100]   before, he gave me a bit of latitude.
[00:15:14.100 --> 00:15:19.060]   And he said, you know, just come up with a new product, come up with something that you
[00:15:19.060 --> 00:15:24.660]   think we should do, and we'll pitch it to the board.
[00:15:24.660 --> 00:15:28.260]   And so what I came up with was LinkedIn Skills.
[00:15:28.260 --> 00:15:37.740]   So at this time, I was still basically an IC data scientist, product manager type person.
[00:15:37.740 --> 00:15:43.380]   So I pitched this idea of, hey, you know, like, skills seems like an obvious thing you
[00:15:43.380 --> 00:15:46.660]   should have as an element of somebody's profile.
[00:15:46.660 --> 00:15:48.380]   And there's all these other cool things we could do with it.
[00:15:48.380 --> 00:15:51.340]   We could use it in search and ad targeting.
[00:15:51.340 --> 00:15:55.980]   But we could also, like, you could endorse each other for skills, things like that.
[00:15:55.980 --> 00:15:59.780]   So we had these early notions that we could do stuff like that.
[00:15:59.780 --> 00:16:03.940]   But the first task is how do you bootstrap something like that?
[00:16:03.940 --> 00:16:09.020]   So I'd say the first year was basically bootstrapping and building that from scratch.
[00:16:09.020 --> 00:16:10.660]   And so we put a team together.
[00:16:10.660 --> 00:16:17.260]   Jay Kreps, who's the co-founder of Confluent, he was my first engineering partner on that
[00:16:17.260 --> 00:16:18.420]   project.
[00:16:18.420 --> 00:16:24.720]   And then Sam Shaw, who rewrote from SQL to MapReduce, that people you may know, was then
[00:16:24.720 --> 00:16:30.180]   my second engineering partner and he became my co-founder later at a startup, SkipFlag,
[00:16:30.180 --> 00:16:31.580]   that we did.
[00:16:31.580 --> 00:16:33.280]   But that was the first big project.
[00:16:33.280 --> 00:16:34.280]   It was actually a pretty...
[00:16:34.280 --> 00:16:36.100]   How did you bootstrap it?
[00:16:36.100 --> 00:16:37.100]   Yeah.
[00:16:37.100 --> 00:16:39.300]   So actually, crowdsourcing came in.
[00:16:39.300 --> 00:16:40.700]   I think we may have used CrowdFlower.
[00:16:40.700 --> 00:16:44.740]   We definitely used Mechanical Turk.
[00:16:44.740 --> 00:16:48.820]   Basically, it was a mix of different things.
[00:16:48.820 --> 00:16:53.420]   So we have, maybe in the show notes, I can give you some references for papers on how
[00:16:53.420 --> 00:16:54.660]   we did it.
[00:16:54.660 --> 00:16:56.180]   Yeah, please.
[00:16:56.180 --> 00:17:02.540]   So the core prototype I actually built in, I think, a few weeks.
[00:17:02.540 --> 00:17:04.960]   And I really just slapped it together with duct tape.
[00:17:04.960 --> 00:17:08.600]   Again, some Python, SciPy.
[00:17:08.600 --> 00:17:15.040]   I think, yeah, Scikit-learn existed, but it was still pretty nascent at the time.
[00:17:15.040 --> 00:17:16.400]   And MapReduce, right?
[00:17:16.400 --> 00:17:24.280]   So LinkedIn, the stack back then looked like Hadoop, Pig, some Hive, but we mostly settled
[00:17:24.280 --> 00:17:29.120]   on Pig, which came out of Yahoo.
[00:17:29.120 --> 00:17:31.040]   And it was basically a bunch of batch jobs.
[00:17:31.040 --> 00:17:35.760]   And the idea, the trick, which I had picked up when I was at AOL.
[00:17:35.760 --> 00:17:41.680]   At AOL, I was working on mining patterns from search query data, and then crawling external
[00:17:41.680 --> 00:17:45.440]   websites and trying to actually understand the topics in those sites.
[00:17:45.440 --> 00:17:50.120]   You can imagine, if you're on TripAdvisor, what are the topics on TripAdvisor?
[00:17:50.120 --> 00:17:54.560]   What are people writing about in reviews?
[00:17:54.560 --> 00:17:57.600]   What are the locations that are in search queries?
[00:17:57.600 --> 00:18:01.280]   So I'd spent a lot of time working on NLP and information extraction.
[00:18:01.280 --> 00:18:05.000]   And so that was basically the idea to bootstrap skills.
[00:18:05.000 --> 00:18:09.360]   We had about 10 or 12 million English language profiles.
[00:18:09.360 --> 00:18:20.400]   And basically, it was a bit like Word2Vec, but pre-Word2Vec, extracting commonly co-occurring
[00:18:20.400 --> 00:18:26.760]   phrases from those profiles, and then getting a bunch of candidates for named entities,
[00:18:26.760 --> 00:18:29.040]   essentially, from the raw text.
[00:18:29.040 --> 00:18:35.120]   And from those candidates for named entities, again, similar to how a lot of people do named
[00:18:35.120 --> 00:18:45.480]   entities now, they use Wikipedia or Wikidata or things like that as a source of truth.
[00:18:45.480 --> 00:18:52.040]   If we could map those phrases, those surface forms, to an entity in Wikipedia, then I could
[00:18:52.040 --> 00:18:53.040]   normalize those.
[00:18:53.040 --> 00:19:02.520]   So if you say ROR or Ruby on Rails, we could disambiguate those at LinkedIn down to be
[00:19:02.520 --> 00:19:04.600]   the same entity.
[00:19:04.600 --> 00:19:09.760]   And so it was a really primitive, in some sense, form of things that we went on to do
[00:19:09.760 --> 00:19:10.760]   at our startup, SkipFlag.
[00:19:10.760 --> 00:19:11.760]   But wait, wait.
[00:19:11.760 --> 00:19:18.200]   So you just used Wikipedia to pull in the skills?
[00:19:18.200 --> 00:19:26.440]   So we would use that as a means of normalizing the things that people would say on their
[00:19:26.440 --> 00:19:27.480]   profile.
[00:19:27.480 --> 00:19:31.200]   So you could do named entity disambiguation.
[00:19:31.200 --> 00:19:35.520]   So if somebody says a phrase like, let's say, "angel," right?
[00:19:35.520 --> 00:19:39.000]   So if you say "angel," do you mean you're an angel investor?
[00:19:39.000 --> 00:19:44.720]   There are people on LinkedIn who do psychic healing and say they can talk to angels.
[00:19:44.720 --> 00:19:48.840]   And so you want to be able to disambiguate those two roles.
[00:19:48.840 --> 00:19:52.560]   But so you used Wikipedia for your ontology?
[00:19:52.560 --> 00:19:53.560]   The ontology.
[00:19:53.560 --> 00:19:54.560]   Yeah.
[00:19:54.560 --> 00:19:57.120]   So basically the knowledge graph, it was complicated.
[00:19:57.120 --> 00:20:00.720]   So not everything is in Wikipedia.
[00:20:00.720 --> 00:20:05.440]   The notability criterion for creating a Wikipedia entity is pretty high.
[00:20:05.440 --> 00:20:09.800]   So a lot of jargon, a lot of domain-specific stuff is not in Wikipedia.
[00:20:09.800 --> 00:20:12.840]   So we would only use that as a...
[00:20:12.840 --> 00:20:16.040]   You could, let's say, weights and biases.
[00:20:16.040 --> 00:20:18.360]   People start putting that on their LinkedIn profile.
[00:20:18.360 --> 00:20:21.880]   That's an emerging topic, which can be in the knowledge graph.
[00:20:21.880 --> 00:20:29.240]   But if we can link it to Wikipedia, that gives us a lot more evidence and data for tagging.
[00:20:29.240 --> 00:20:32.720]   Would you manually review new skills?
[00:20:32.720 --> 00:20:39.560]   Say weights and biases become a skill people want to put, how would you even create a new
[00:20:39.560 --> 00:20:40.560]   one?
[00:20:40.560 --> 00:20:41.560]   Yeah.
[00:20:41.560 --> 00:20:43.440]   So initially it was a combination.
[00:20:43.440 --> 00:20:50.760]   So we would have an automated skill discovery job that would detect emerging topics that
[00:20:50.760 --> 00:20:53.680]   probably were skills.
[00:20:53.680 --> 00:20:56.720]   And then we also had...
[00:20:56.720 --> 00:20:58.960]   This is where production machine learning gets really complicated.
[00:20:58.960 --> 00:21:04.880]   If it's user-facing, I think I was maybe overly paranoid, but we really ended up not having
[00:21:04.880 --> 00:21:11.280]   a lot of issues with things like profanity and other things like that, offensive topics.
[00:21:11.280 --> 00:21:15.340]   And part of the reason was we had many layers of vetting.
[00:21:15.340 --> 00:21:23.840]   And so some of that was human curated, meaning we had humans come up with white lists and
[00:21:23.840 --> 00:21:25.840]   black lists and gray lists.
[00:21:25.840 --> 00:21:31.400]   So it might be okay for you to put on your profile, for example, alcoholism.
[00:21:31.400 --> 00:21:38.200]   If you are a psychiatrist, you help deal with alcoholism and alcohol, drinking disorders
[00:21:38.200 --> 00:21:39.720]   and things like that.
[00:21:39.720 --> 00:21:43.480]   But you wouldn't want the machine learning algorithm to automatically suggest that to
[00:21:43.480 --> 00:21:45.360]   someone and be incorrect.
[00:21:45.360 --> 00:21:46.360]   And it could be offensive.
[00:21:46.360 --> 00:21:47.360]   Oh, I see.
[00:21:47.360 --> 00:21:49.320]   So that's an example of a gray list?
[00:21:49.320 --> 00:21:50.320]   That's a gray list.
[00:21:50.320 --> 00:21:56.760]   So we had multiple tiers of where is it appropriate to use this data.
[00:21:56.760 --> 00:22:02.680]   So we may be correct that that person in their profile said alcoholism, but we shouldn't
[00:22:02.680 --> 00:22:06.040]   maybe suggest it as a skill necessarily.
[00:22:06.040 --> 00:22:10.880]   But I think the other thing that is interesting there is the use of crowdsourcing.
[00:22:10.880 --> 00:22:19.320]   So in that first month when I was bootstrapping the system, I was able to get labeled data.
[00:22:19.320 --> 00:22:20.320]   Are these the same?
[00:22:20.320 --> 00:22:21.720]   Are these phrase?
[00:22:21.720 --> 00:22:26.400]   I think the Wikipedia task was something like we would show phrases in context and then
[00:22:26.400 --> 00:22:32.200]   ask them to label, hey, pick which Wikipedia entity is this phrase?
[00:22:32.200 --> 00:22:33.720]   Is this the correct one?
[00:22:33.720 --> 00:22:38.320]   And then that powers the machine learning training, which does it automatically.
[00:22:38.320 --> 00:22:39.880]   Got it.
[00:22:39.880 --> 00:22:40.880]   Cool.
[00:22:40.880 --> 00:22:43.880]   So you worked on it for like a year?
[00:22:43.880 --> 00:22:50.360]   How long did it take you to get something that you could deploy into production?
[00:22:50.360 --> 00:23:04.240]   So I think the prototype end to end with the front end and using SciPy and Hadoop, etc.
[00:23:04.240 --> 00:23:09.760]   That took maybe two to three months to get something reasonable.
[00:23:09.760 --> 00:23:13.120]   And then there was a bunch of design work and a bunch of engineering work.
[00:23:13.120 --> 00:23:19.840]   So the engineering, taking something that runs on your desktop or your laptop to do
[00:23:19.840 --> 00:23:25.760]   a prototype app that recommends skills for people, that's one thing.
[00:23:25.760 --> 00:23:31.880]   But at the time, the way that our production stack worked, we had something called Voldemort,
[00:23:31.880 --> 00:23:34.440]   which was a NoSQL data store.
[00:23:34.440 --> 00:23:39.000]   And we had Hadoop jobs that would then push metadata, essentially.
[00:23:39.000 --> 00:23:45.360]   So let's say you extract suggested skills for all of those 10 million members.
[00:23:45.360 --> 00:23:49.640]   You would compute those suggested skills, push them up to a NoSQL store, and then a
[00:23:49.640 --> 00:23:56.680]   recommender service, which sends data to the front end, would have to pull from that data
[00:23:56.680 --> 00:23:58.840]   store and display it to the user.
[00:23:58.840 --> 00:24:03.000]   And then there's all the logic around, did someone accept the recommendation?
[00:24:03.000 --> 00:24:04.440]   Did they decline it?
[00:24:04.440 --> 00:24:08.320]   Tracking, which would then go to Kafka eventually.
[00:24:08.320 --> 00:24:15.400]   All that machinery and all that engineering, that's where folks like Jay and Sam Shaw came
[00:24:15.400 --> 00:24:17.840]   in and designing that.
[00:24:17.840 --> 00:24:24.040]   And then eventually, the other hard part, I would say, is you always have these choices
[00:24:24.040 --> 00:24:28.840]   of do we do the thing to get it done quickly or do we do the thing to set us up so we can
[00:24:28.840 --> 00:24:31.880]   do 10 more products like this?
[00:24:31.880 --> 00:24:36.480]   And in those early days, we were at this transition point.
[00:24:36.480 --> 00:24:42.320]   They had done in the past a lot of things, like I mentioned, the three-day SQL query.
[00:24:42.320 --> 00:24:44.440]   We wanted to do things in a bit more scalable way.
[00:24:44.440 --> 00:24:50.040]   So we bit the bullet and a lot of things like Kafka and other projects came out of those
[00:24:50.040 --> 00:24:52.200]   efforts to make it more scalable.
[00:24:52.200 --> 00:24:53.200]   Cool.
[00:24:53.200 --> 00:24:56.360]   So it's like two to three months to make the prototype.
[00:24:56.360 --> 00:24:58.960]   And then how long did it take you to get it out?
[00:24:58.960 --> 00:25:02.240]   I think it took about a year to get it.
[00:25:02.240 --> 00:25:03.240]   It was phased.
[00:25:03.240 --> 00:25:10.360]   So this is another good point, which is people should look for opportunities to do an MVP.
[00:25:10.360 --> 00:25:14.480]   And so the way that we approached it was we actually did email first.
[00:25:14.480 --> 00:25:17.080]   So it was much easier.
[00:25:17.080 --> 00:25:21.400]   Changing the front end of LinkedIn at that time was kind of a big, heavy process.
[00:25:21.400 --> 00:25:25.120]   And we had this framework, which is pretty hard to work with.
[00:25:25.120 --> 00:25:29.760]   It would take an intern like a month to learn how to commit a change and push it to the
[00:25:29.760 --> 00:25:31.640]   site.
[00:25:31.640 --> 00:25:33.560]   And so email is much easier.
[00:25:33.560 --> 00:25:39.680]   So if we could push the data out of Hadoop to an email job, you can imagine something
[00:25:39.680 --> 00:25:41.680]   like an email campaign on MailChimp.
[00:25:41.680 --> 00:25:45.360]   We weren't using MailChimp, but something like that.
[00:25:45.360 --> 00:25:46.680]   And you could push the recommendations.
[00:25:46.680 --> 00:25:50.560]   So I could send an email to you saying, "Hey, Lucas, do you have these skills?
[00:25:50.560 --> 00:25:52.500]   Add them to your profile."
[00:25:52.500 --> 00:25:55.880]   And then it's a much lighter weight way to do that.
[00:25:55.880 --> 00:26:02.080]   So that, I think, took another few months to get all the pieces powered, the back end,
[00:26:02.080 --> 00:26:03.960]   so that we could do that.
[00:26:03.960 --> 00:26:11.920]   And then the work to get the front end done and actually roll it out in A/B test was probably
[00:26:11.920 --> 00:26:12.920]   another few months.
[00:26:12.920 --> 00:26:20.400]   So all in, it probably took about six or seven months at that point in time to get this out
[00:26:20.400 --> 00:26:21.960]   for all users on LinkedIn.
[00:26:22.960 --> 00:26:25.600]   It must have been satisfying when it was deployed, though.
[00:26:25.600 --> 00:26:26.600]   It touches so many people.
[00:26:26.600 --> 00:26:29.520]   Yeah, I think it was actually the first Strata.
[00:26:29.520 --> 00:26:38.540]   So I remember DJ Patil had a keynote and he was going to announce it, but it wasn't quite
[00:26:38.540 --> 00:26:40.320]   ready to ship.
[00:26:40.320 --> 00:26:45.360]   And so I think it was, I had a talk a couple of days later.
[00:26:45.360 --> 00:26:48.280]   And yeah, I think we announced it sometime right around then, right around the first
[00:26:48.280 --> 00:26:49.280]   Strata in 2010.
[00:26:49.280 --> 00:26:50.280]   Cool.
[00:26:50.280 --> 00:26:56.360]   Hi, we'd love to take a moment to tell you guys about Weights and Biases.
[00:26:56.360 --> 00:27:01.820]   Weights and Biases is a tool that helps you track and visualize every detail of your machine
[00:27:01.820 --> 00:27:02.820]   learning models.
[00:27:02.820 --> 00:27:08.760]   We help you debug your machine learning models in real time, collaborate easily, and advance
[00:27:08.760 --> 00:27:11.860]   the state of the art in machine learning.
[00:27:11.860 --> 00:27:17.040]   You can integrate Weights and Biases into your models with just a few lines of code.
[00:27:17.040 --> 00:27:21.800]   With hyperparameter sweeps, you can find the best set of hyperparameters for your models
[00:27:21.800 --> 00:27:23.740]   automatically.
[00:27:23.740 --> 00:27:29.140]   You can also track and compare how many GPU resources your models are using.
[00:27:29.140 --> 00:27:35.840]   With one line of code, you can visualize model predictions in form of images, videos, audio,
[00:27:35.840 --> 00:27:41.560]   plotly charts, molecular data, segmentation maps, and 3D point clouds.
[00:27:41.560 --> 00:27:47.400]   You can save everything you need to reproduce your models days, weeks, or even months after
[00:27:47.400 --> 00:27:48.400]   training.
[00:27:48.400 --> 00:27:53.520]   Finally, with reports, you can make your models come alive.
[00:27:53.520 --> 00:27:58.520]   Reports are like blog posts in which your readers can interact with your model metrics
[00:27:58.520 --> 00:28:00.320]   and predictions.
[00:28:00.320 --> 00:28:06.520]   Reports serve as a centralized repository of metrics, predictions, hyperparameter stride,
[00:28:06.520 --> 00:28:08.280]   and accompanying nodes.
[00:28:08.280 --> 00:28:13.960]   All of this together gives you a bird's eye view of your machine learning workflow.
[00:28:13.960 --> 00:28:19.480]   You can use reports to share your model insights, keep your team on the same page, and collaborate
[00:28:19.480 --> 00:28:20.480]   effectively remotely.
[00:28:20.480 --> 00:28:25.600]   I'll leave a link in the show notes below to help you get started.
[00:28:25.600 --> 00:28:28.600]   And now let's get back to the episode.
[00:28:28.600 --> 00:28:32.040]   You went on to start SkipFlag.
[00:28:32.040 --> 00:28:37.320]   What was it like going from a bigger company into your own startup?
[00:28:37.320 --> 00:28:39.320]   How was that experience for you?
[00:28:39.320 --> 00:28:40.320]   It was interesting.
[00:28:40.320 --> 00:28:44.960]   So I think the part that I left out in that transition was moving into management.
[00:28:44.960 --> 00:28:51.200]   So I had managed projects and small teams before, but as LinkedIn grew, when I joined
[00:28:51.200 --> 00:28:53.760]   LinkedIn there were about 300 employees.
[00:28:53.760 --> 00:28:57.200]   When I left there was over, I think, 6,000.
[00:28:57.200 --> 00:29:02.000]   And our data team, I think, obviously the Facebook data team and LinkedIn, there were
[00:29:02.000 --> 00:29:06.160]   a number of data teams at that time that grew fairly large.
[00:29:06.160 --> 00:29:13.560]   And so like everybody else in a hyper growth company, we had to learn a lot about how to
[00:29:13.560 --> 00:29:16.880]   run data teams.
[00:29:16.880 --> 00:29:25.320]   And one of the challenges that I found was that the tools we were using, enterprise tools
[00:29:25.320 --> 00:29:28.760]   essentially and workplace software, was still really dumb.
[00:29:28.760 --> 00:29:34.800]   So you're building all these cool smart systems for Facebook and Google on the front end and
[00:29:34.800 --> 00:29:39.360]   LinkedIn, but the tools that all those were using at tech companies were still pretty
[00:29:39.360 --> 00:29:41.640]   stupid.
[00:29:41.640 --> 00:29:48.000]   And so what I really wanted to do was apply some of that technology to those workplace
[00:29:48.000 --> 00:29:51.840]   problems and more specifically like intelligent assistants.
[00:29:51.840 --> 00:29:57.040]   So moving to a startup, what was it like?
[00:29:57.040 --> 00:30:03.600]   I think that it's not as obvious when you're in these larger companies where you've hit
[00:30:03.600 --> 00:30:05.360]   scale.
[00:30:05.360 --> 00:30:10.320]   Everybody specializes and you have, there was a great joke actually on Twitter last
[00:30:10.320 --> 00:30:11.320]   night.
[00:30:11.320 --> 00:30:19.200]   I forget who said this, but it might have been Pardis.
[00:30:19.200 --> 00:30:22.560]   She was at Twitter in the past.
[00:30:22.560 --> 00:30:26.600]   So she said, "Who came up with data engineer?
[00:30:26.600 --> 00:30:30.840]   It should have been data Lakers."
[00:30:30.840 --> 00:30:36.720]   But those data Lakers or data engineers who are doing all that hard work and creating
[00:30:36.720 --> 00:30:42.000]   those new SQL stores, creating the infrastructure, I think a lot of folks who go off and do a
[00:30:42.000 --> 00:30:48.720]   startup then the reality hits them that, "Wow, there's actually a lot to build."
[00:30:48.720 --> 00:30:51.000]   And this was like 2015.
[00:30:51.000 --> 00:30:55.680]   So even with Amazon Web Services or Google or Azure or whatever, there's still a lot
[00:30:55.680 --> 00:31:01.160]   of pieces and a lot of glue that you rely on in companies like Google and Facebook that
[00:31:01.160 --> 00:31:04.440]   is just not there and you have to kind of put together yourself.
[00:31:04.440 --> 00:31:09.760]   So that was a big journey, was building a lot of that.
[00:31:09.760 --> 00:31:14.360]   But I would say overall, it was extremely fun.
[00:31:14.360 --> 00:31:19.880]   When you're at a big company, you end up spending a lot of time.
[00:31:19.880 --> 00:31:23.600]   There's a lot of impact that you can have, but at the same time, you spend a huge amount
[00:31:23.600 --> 00:31:30.360]   of time on coordination and red tape and getting everyone on the same page.
[00:31:30.360 --> 00:31:34.640]   And one of the advantages of startup, obviously, is you can move a lot faster.
[00:31:34.640 --> 00:31:37.760]   You make mistakes, but they're your mistakes.
[00:31:37.760 --> 00:31:39.760]   And that was really exciting.
[00:31:39.760 --> 00:31:42.080]   I've never asked you this.
[00:31:42.080 --> 00:31:45.120]   I'm actually genuinely curious about this one.
[00:31:45.120 --> 00:31:46.120]   I don't know.
[00:31:46.120 --> 00:31:47.120]   So how did you...
[00:31:47.120 --> 00:31:52.800]   You had kind of an enterprise tool that helped you sort of organize information at SkipFlag.
[00:31:52.800 --> 00:31:53.800]   How did you...
[00:31:53.800 --> 00:31:57.960]   I've never had this problem as an entrepreneur just because of the spaces I've gone into.
[00:31:57.960 --> 00:31:59.960]   But how did you prototype it?
[00:31:59.960 --> 00:32:04.560]   Did you get someone to give you their Slack logs and do it for them?
[00:32:04.560 --> 00:32:08.200]   How do you even build an ML algorithm without the data?
[00:32:08.200 --> 00:32:11.560]   Yeah, so that's the chicken and egg problem.
[00:32:11.560 --> 00:32:14.640]   So I'm a little OCD when it comes to data and data sets.
[00:32:14.640 --> 00:32:23.240]   So back maybe like 2007 or 2008, I wrote a blog post, "Some Data Sets Available on the
[00:32:23.240 --> 00:32:24.360]   Web."
[00:32:24.360 --> 00:32:26.080]   And it was in these early days.
[00:32:26.080 --> 00:32:31.640]   So Aaron Schwartz was another person who was really big on making data sets available and
[00:32:31.640 --> 00:32:39.800]   open and he famously got in trouble for scraping academic journals.
[00:32:39.800 --> 00:32:42.200]   So he was working on some projects in this area.
[00:32:42.200 --> 00:32:46.920]   I had been collecting a lot of data sets, a lot of public data sets for years.
[00:32:46.920 --> 00:32:52.520]   And using the Twitter API, for example, I had been doing firehose crawls for years.
[00:32:52.520 --> 00:32:53.520]   Wikipedia.
[00:32:53.520 --> 00:32:57.060]   I'm an advisor to Common Crawl.
[00:32:57.060 --> 00:33:04.000]   So that was used to create Glove and a lot of these other NLP data sets that we all enjoy
[00:33:04.000 --> 00:33:05.000]   today.
[00:33:05.000 --> 00:33:06.760]   Is the Enron corpus still relevant?
[00:33:06.760 --> 00:33:07.760]   Is that still around?
[00:33:07.760 --> 00:33:09.880]   I remember working on that.
[00:33:09.880 --> 00:33:11.920]   Funnily enough, it really is.
[00:33:11.920 --> 00:33:12.920]   We did some work.
[00:33:12.920 --> 00:33:18.320]   I don't want to give away too much secret sauce or details, but we were working with
[00:33:18.320 --> 00:33:20.600]   one enterprise customer.
[00:33:20.600 --> 00:33:25.920]   So initially, let me say a little more about the tool and then I'll talk about the Enron
[00:33:25.920 --> 00:33:26.920]   data set.
[00:33:26.920 --> 00:33:27.920]   All right.
[00:33:27.920 --> 00:33:28.920]   Yes, sorry.
[00:33:28.920 --> 00:33:29.920]   So how did we get going?
[00:33:29.920 --> 00:33:34.920]   So what SkipFlag was doing, we were a knowledge base that would build itself out of your enterprise
[00:33:34.920 --> 00:33:36.920]   communication.
[00:33:36.920 --> 00:33:45.560]   And so we started actually with Slack because one of our first investors was Excel.
[00:33:45.560 --> 00:33:49.960]   And so I actually did an EIR at Excel and was hanging out there at the time as we put
[00:33:49.960 --> 00:33:51.800]   the company together.
[00:33:51.800 --> 00:33:54.520]   And they were investors in Slack.
[00:33:54.520 --> 00:33:58.440]   And so I was using Slack a little bit before it launched.
[00:33:58.440 --> 00:34:02.800]   So I think that was maybe like 2014, 2015.
[00:34:02.800 --> 00:34:03.800]   And I really liked it.
[00:34:03.800 --> 00:34:09.320]   And I'm pretty picky when it comes to workplace tools.
[00:34:09.320 --> 00:34:10.800]   And I enjoyed it.
[00:34:10.800 --> 00:34:13.560]   Obviously they've been massively successful.
[00:34:13.560 --> 00:34:15.680]   Tons of people use Slack.
[00:34:15.680 --> 00:34:20.080]   But it felt like an opportunity for a data set.
[00:34:20.080 --> 00:34:22.520]   So nobody was really using that data set yet.
[00:34:22.520 --> 00:34:30.240]   It posed some unique challenges similar to Twitter in that there had been many email
[00:34:30.240 --> 00:34:34.080]   startups before and their startups working on documents.
[00:34:34.080 --> 00:34:37.600]   But Slack was interesting because to me it felt closer to Twitter.
[00:34:37.600 --> 00:34:39.720]   Short form messaging data.
[00:34:39.720 --> 00:34:43.320]   Very hard to do the kinds of things that we were working on of knowledge extraction and
[00:34:43.320 --> 00:34:46.840]   any disambiguation.
[00:34:46.840 --> 00:34:49.840]   But there's a lot of data and it's accessible.
[00:34:49.840 --> 00:34:56.280]   So they had a pretty good API in the early days in terms of actually pulling public channel
[00:34:56.280 --> 00:34:59.160]   Slack data.
[00:34:59.160 --> 00:35:04.720]   And so the initial way we bootstrapped was actually using Slack.
[00:35:04.720 --> 00:35:08.840]   And one of the hard parts is if you were to build the whole product, I think we still
[00:35:08.840 --> 00:35:15.360]   have a video online from we did a paper in KDD and they have you do a short video describing
[00:35:15.360 --> 00:35:16.360]   the paper.
[00:35:16.360 --> 00:35:22.160]   So we did a paper on any extraction on noisy text.
[00:35:22.160 --> 00:35:23.760]   And in the video we have a short product demo.
[00:35:23.760 --> 00:35:25.640]   So people will put that in the show notes I guess.
[00:35:25.640 --> 00:35:28.000]   People can check that out.
[00:35:28.000 --> 00:35:33.960]   But before we got to that full blown product, which looked a bit like if you use notion
[00:35:33.960 --> 00:35:40.120]   or other modern wiki like products, it looked a bit like that except it had this AI infused
[00:35:40.120 --> 00:35:44.300]   that could auto organize all your docs and answer questions.
[00:35:44.300 --> 00:35:47.400]   You could upload eventually ultimately after required by workday.
[00:35:47.400 --> 00:35:54.240]   One of the things we worked on was you could give it like a PDF of your workplace HR policies
[00:35:54.240 --> 00:35:58.160]   and it could do fact extraction across all that and then automatically answer questions,
[00:35:58.160 --> 00:36:03.280]   which is pretty cool for an HR person to have this thing automatically answer those questions
[00:36:03.280 --> 00:36:06.120]   based on just a document.
[00:36:06.120 --> 00:36:10.880]   But before you get to that, how do you do this with little data?
[00:36:10.880 --> 00:36:17.280]   So basically I went around to my friends, I got like a hundred or so startups that I
[00:36:17.280 --> 00:36:24.800]   knew, got them in as beta users and said, "Hey, Slack is confusing.
[00:36:24.800 --> 00:36:25.800]   It's noisy.
[00:36:25.800 --> 00:36:27.040]   It's hard to sift through.
[00:36:27.040 --> 00:36:32.400]   What if we gave you a smart email digest and we just summarized what's going on in your
[00:36:32.400 --> 00:36:36.560]   Slack team so that you can keep up to date with what's happening and see interesting
[00:36:36.560 --> 00:36:38.440]   stuff.
[00:36:38.440 --> 00:36:43.600]   And oh, it'll have like news articles recommended based on what you're talking about and things
[00:36:43.600 --> 00:36:44.600]   like that."
[00:36:44.600 --> 00:36:52.200]   So we did that prototype and to do that, we had other auxiliary data sets and we could
[00:36:52.200 --> 00:36:54.080]   do a bit of transfer learning and things like that.
[00:36:54.080 --> 00:37:00.960]   So we had the Wikipedia corpus, Common Crawl, which is a fairly large data set for NLP.
[00:37:00.960 --> 00:37:05.720]   I think it's a few terabytes of web crawl data.
[00:37:05.720 --> 00:37:09.840]   So we were able to train and bootstrap on open data sets and web crawl data and Twitter
[00:37:09.840 --> 00:37:19.640]   data in combination then with customer data to train and do something like smart summarization
[00:37:19.640 --> 00:37:21.040]   and the extraction.
[00:37:21.040 --> 00:37:22.040]   Cool.
[00:37:22.040 --> 00:37:26.440]   You're going to mention the Enron corpus?
[00:37:26.440 --> 00:37:27.880]   Did you also use that?
[00:37:27.880 --> 00:37:28.880]   Oh yeah.
[00:37:28.880 --> 00:37:29.880]   I left the Enron.
[00:37:29.880 --> 00:37:30.880]   So that actually came much later.
[00:37:30.880 --> 00:37:34.560]   So we started with Slack, we did the email digest and then in parallel, we were building
[00:37:34.560 --> 00:37:38.560]   out the product that became SkipFlag.
[00:37:38.560 --> 00:37:46.280]   And one of the things that we found was that Slack is great, but at the time, this is like
[00:37:46.280 --> 00:37:50.640]   2016, 2017, larger companies were still not all in on Slack.
[00:37:50.640 --> 00:37:54.760]   And I think they probably still are not all in on Slack.
[00:37:54.760 --> 00:37:58.880]   Teams is getting a lot of adoption obviously, things like that.
[00:37:58.880 --> 00:38:01.480]   But the world still runs on email, right?
[00:38:01.480 --> 00:38:05.120]   So email is a big deal.
[00:38:05.120 --> 00:38:06.920]   And we had worked with email before.
[00:38:06.920 --> 00:38:11.120]   My co-founder, Sam Shaw, actually ran email relevance at LinkedIn.
[00:38:11.120 --> 00:38:14.520]   And so we're pretty familiar with working with email and the Enron corpus.
[00:38:14.520 --> 00:38:19.000]   I actually took a class, Leslie Cabling, I think, bought back at MIT, she bought the
[00:38:19.000 --> 00:38:24.160]   corpus from, I don't know, she bought it from Enron or they were bankrupt or whatever, right?
[00:38:24.160 --> 00:38:27.320]   So she bought the data set and that's how it became an open data set.
[00:38:27.320 --> 00:38:29.760]   She curated it and put it out there.
[00:38:29.760 --> 00:38:36.320]   Anyway, long story longer, as we got into email, that's one of the few public email
[00:38:36.320 --> 00:38:37.800]   data sets out there.
[00:38:37.800 --> 00:38:44.040]   So when we would want to show a customer how well this could work, that was the data set
[00:38:44.040 --> 00:38:45.520]   that we would benchmark on.
[00:38:45.520 --> 00:38:48.640]   And a lot of academic papers still use that as a benchmark.
[00:38:48.640 --> 00:38:53.120]   I remember working on it in grad school and just feeling kind of sorry for all the employees
[00:38:53.120 --> 00:38:56.080]   that got all the emails released.
[00:38:56.080 --> 00:38:59.760]   But I think it was a good lesson for me because now I'm really religious about keeping work
[00:38:59.760 --> 00:39:02.560]   stuff in work email and personal stuff in personal email.
[00:39:02.560 --> 00:39:05.520]   I think it's good advice, you don't realize that.
[00:39:05.520 --> 00:39:09.840]   There's nobody more careful than a data scientist or machine learning engineer, for sure.
[00:39:09.840 --> 00:39:12.120]   No, and I was totally the same way.
[00:39:12.120 --> 00:39:16.520]   So I think we were very, you know, so that was one of the things.
[00:39:16.520 --> 00:39:19.760]   Sometimes we would work with a customer and it's their own data, right?
[00:39:19.760 --> 00:39:23.240]   So you're processing the data.
[00:39:23.240 --> 00:39:26.560]   And a lot, like sometimes it would happen where we would turn on, you know, say that
[00:39:26.560 --> 00:39:30.960]   email digest or that product and people would say, "Okay, I changed my mind.
[00:39:30.960 --> 00:39:32.560]   Let's turn this off."
[00:39:32.560 --> 00:39:38.320]   And because they didn't realize, you know, maybe it's like customer credit card numbers,
[00:39:38.320 --> 00:39:39.320]   things like that.
[00:39:39.320 --> 00:39:42.640]   There's all these things in the data that they didn't realize are in there.
[00:39:42.640 --> 00:39:47.760]   So if you have a customer support channel in Slack or something like that, the hygiene,
[00:39:47.760 --> 00:39:52.000]   I guess the data hygiene is really important, especially if you're an enterprise company.
[00:39:52.000 --> 00:39:54.880]   And this was right before GDPR.
[00:39:54.880 --> 00:40:00.040]   So after we were acquired, that year after we were acquired was when GDPR hit.
[00:40:00.040 --> 00:40:02.500]   But we were really careful from the beginning and really rigorous.
[00:40:02.500 --> 00:40:06.960]   We had PII scrubbing and all kinds of stuff in our machine learning pipeline from day
[00:40:06.960 --> 00:40:07.960]   one.
[00:40:07.960 --> 00:40:10.240]   How did you do automatic PII scrubbing?
[00:40:10.240 --> 00:40:12.400]   How would you know that it's PII?
[00:40:12.400 --> 00:40:15.020]   So there's a bunch of techniques out there.
[00:40:15.020 --> 00:40:20.100]   So it's obviously nothing as foolproof, but this actually goes back to that first week
[00:40:20.100 --> 00:40:22.060]   at AOL.
[00:40:22.060 --> 00:40:26.040]   I had just started, so I was almost in quarantine, right?
[00:40:26.040 --> 00:40:30.920]   So I hadn't been involved with the release of the search data set.
[00:40:30.920 --> 00:40:36.320]   So for whatever reason, I was tasked with going in and putting in place a bunch of the
[00:40:36.320 --> 00:40:38.000]   PII protection.
[00:40:38.000 --> 00:40:42.720]   So you can imagine in search query logs, what are common things that would be sensitive?
[00:40:42.720 --> 00:40:46.720]   Wait, you put in place the PII stuff for the AOL query log release?
[00:40:46.720 --> 00:40:48.680]   I didn't put it fully in place.
[00:40:48.680 --> 00:40:50.080]   So I was a data scientist.
[00:40:50.080 --> 00:40:58.600]   I wasn't really doing the production engineering, but I did put together the scrubbing layer,
[00:40:58.600 --> 00:41:03.280]   which was things like, how do you detect FedEx IDs?
[00:41:03.280 --> 00:41:05.480]   How do you detect social security numbers?
[00:41:05.480 --> 00:41:06.560]   All that kind of stuff.
[00:41:06.560 --> 00:41:07.560]   And so this is years ago.
[00:41:07.560 --> 00:41:13.240]   If you were going to do it now, Microsoft actually has an open source project for this.
[00:41:13.240 --> 00:41:18.600]   And there's a whole bunch of other, there's about a half dozen open source efforts to
[00:41:18.600 --> 00:41:20.200]   do this kind of thing now.
[00:41:20.200 --> 00:41:21.200]   Oh, cool.
[00:41:21.200 --> 00:41:23.720]   But yeah, you have to try.
[00:41:23.720 --> 00:41:27.040]   What's the AOL query story?
[00:41:27.040 --> 00:41:31.200]   I mean, you should really have Abdur on.
[00:41:31.200 --> 00:41:34.200]   I don't know if he wants to talk about it.
[00:41:34.200 --> 00:41:42.760]   So yeah, Abdur was a chief scientist at Twitter and previously he was driving search at AOL.
[00:41:42.760 --> 00:41:52.200]   I mean, it was a strange situation because, I mean, technically nothing was out of the
[00:41:52.200 --> 00:41:54.080]   norm in that.
[00:41:54.080 --> 00:41:55.840]   So you mentioned the Enron dataset, right?
[00:41:55.840 --> 00:42:02.720]   So at the time in academic research, in the world of search, there were a few datasets.
[00:42:02.720 --> 00:42:09.680]   There was an Excite logs, MSN had a dataset out there of search query logs.
[00:42:09.680 --> 00:42:11.560]   And so there was kind of an accepted format.
[00:42:11.560 --> 00:42:15.520]   There were like two or three, maybe Lycos had a dataset out there.
[00:42:15.520 --> 00:42:19.240]   There were two or three datasets in the academic world that were search query logs.
[00:42:19.240 --> 00:42:22.460]   And so AOL basically released one in the same format.
[00:42:22.460 --> 00:42:28.060]   So I think that they didn't really expect anything like this to happen.
[00:42:28.060 --> 00:42:32.400]   But what ended up happening was Reddit.
[00:42:32.400 --> 00:42:34.540]   This is the early days of Reddit too, right?
[00:42:34.540 --> 00:42:39.380]   So I remember I was actually looking at Reddit on the weekend when I had just started this
[00:42:39.380 --> 00:42:43.740]   job and I saw on the new page where emerging stories are popping up.
[00:42:43.740 --> 00:42:48.860]   I saw the story, AOL releases search logs and some user, I don't know if they were the
[00:42:48.860 --> 00:42:54.900]   first person to see it on Reddit or if it was a reporter, but someone on Reddit said,
[00:42:54.900 --> 00:42:56.480]   "Hey, check this out.
[00:42:56.480 --> 00:42:57.560]   There's search queries here."
[00:42:57.560 --> 00:43:01.340]   And then what happened was a whole bunch of people started putting up sites where they
[00:43:01.340 --> 00:43:06.980]   put a web app in front of the search query logs and you could go in and explore crazy
[00:43:06.980 --> 00:43:07.980]   queries.
[00:43:07.980 --> 00:43:13.860]   And it was, query session logs are deeply private and sensitive things.
[00:43:13.860 --> 00:43:17.780]   So even if you remove the user ID, this is what the world discovered basically when that
[00:43:17.780 --> 00:43:18.780]   happened.
[00:43:18.780 --> 00:43:21.260]   Hey, it was a wake up call.
[00:43:21.260 --> 00:43:26.300]   I think a lot of people in technology already knew that obviously this stuff can be sensitive.
[00:43:26.300 --> 00:43:34.240]   But I think for a lot of the world, it was a wake up call that what I type in to my browser
[00:43:34.240 --> 00:43:38.940]   goes somewhere and it can have an impact.
[00:43:38.940 --> 00:43:44.460]   And so if you think about the advice back in the early 2000s dealing with Google, the
[00:43:44.460 --> 00:43:48.060]   general advice out there was always Google your name.
[00:43:48.060 --> 00:43:51.620]   Every few weeks you should Google your name to make sure that there isn't something bad
[00:43:51.620 --> 00:43:56.420]   on the internet about you or whatever, like reputation management.
[00:43:56.420 --> 00:44:01.660]   But actually what that means then is in those anonymized search query logs, usually one
[00:44:01.660 --> 00:44:05.260]   of the more common things that people were Googling was actually their name.
[00:44:05.260 --> 00:44:10.860]   So then that made it fairly easy to triangulate in many cases individuals.
[00:44:10.860 --> 00:44:13.780]   And so that's what ended up happening.
[00:44:13.780 --> 00:44:17.180]   So ultimately the search data set was taken down.
[00:44:17.180 --> 00:44:22.300]   And then I think we entered into this period where it was much more difficult for academia.
[00:44:22.300 --> 00:44:24.220]   Like you mentioned the Enron data set.
[00:44:24.220 --> 00:44:26.780]   I don't think something like that would happen today.
[00:44:26.780 --> 00:44:28.220]   You wouldn't have an Enron data set.
[00:44:28.220 --> 00:44:32.060]   You wouldn't have the AOL search logs.
[00:44:32.060 --> 00:44:33.260]   And so it became much more locked down.
[00:44:33.260 --> 00:44:36.380]   I think the Netflix prize data set was one of the last big ones.
[00:44:36.380 --> 00:44:37.380]   Yeah.
[00:44:37.380 --> 00:44:41.180]   And I remember those complaints.
[00:44:41.180 --> 00:44:44.900]   I think Netflix didn't actually do a follow up to their competition, right?
[00:44:44.900 --> 00:44:48.340]   Because there was privacy concerns brought up.
[00:44:48.340 --> 00:44:49.340]   Yeah.
[00:44:49.340 --> 00:44:50.860]   I mean it was a good run, right?
[00:44:50.860 --> 00:44:56.500]   And then that was kind of correlated with the rebirth of deep learning.
[00:44:56.500 --> 00:45:01.300]   Because I remember I was actually working on that on the side when it was going on.
[00:45:01.300 --> 00:45:08.060]   So I think I was at AOL at the time and I was working on the Netflix prize.
[00:45:08.060 --> 00:45:10.180]   And I was in all the forums.
[00:45:10.180 --> 00:45:11.780]   And it was like this was before Kaggle.
[00:45:11.780 --> 00:45:15.620]   But it was basically one of the first, you know, there were the data mining competitions
[00:45:15.620 --> 00:45:19.020]   and then there was a Netflix prize and then there was Kaggle.
[00:45:19.020 --> 00:45:24.580]   And it was really interesting to see the progress because deep learning kind of came out of
[00:45:24.580 --> 00:45:27.260]   left field and ended up working really well.
[00:45:27.260 --> 00:45:29.820]   And then it's ensemble techniques.
[00:45:29.820 --> 00:45:35.540]   But I think that that period was kind of the catalyst for a lot of what day to day folks
[00:45:35.540 --> 00:45:38.420]   like you and I deal with in machine learning.
[00:45:38.420 --> 00:45:46.740]   And like this kind of massive surge of progress, I think is largely because of these benchmarks
[00:45:46.740 --> 00:45:48.900]   and then things like ImageNet.
[00:45:48.900 --> 00:45:55.180]   So anyway, I know we're on a tangent here, but I think that, I don't know, I just think
[00:45:55.180 --> 00:45:58.100]   that that was a really exciting period.
[00:45:58.100 --> 00:46:05.100]   And we're seeing like the compounding effect of that now where, you know, the technologies
[00:46:05.100 --> 00:46:10.820]   that people have at their disposal are amazing compared to what we had 10 years ago.
[00:46:10.820 --> 00:46:11.820]   And it's so powerful.
[00:46:11.820 --> 00:46:15.980]   Tell me about, I really want to make sure I get this in before we run out of time.
[00:46:15.980 --> 00:46:21.620]   Like you're now or you've been lately doing some consulting for different companies.
[00:46:21.620 --> 00:46:23.940]   And like, what are you seeing out there?
[00:46:23.940 --> 00:46:29.460]   Like what are, I'm really curious, like what kinds of stuff are people doing and what kind
[00:46:29.460 --> 00:46:31.060]   of technology are they using at this point?
[00:46:31.060 --> 00:46:35.060]   Like is Python and C++ still the standard?
[00:46:35.060 --> 00:46:36.300]   Like what's going on?
[00:46:36.300 --> 00:46:39.660]   I haven't seen C++ in a while actually.
[00:46:39.660 --> 00:46:44.340]   So maybe some people, yeah, if you're using devices and things maybe.
[00:46:44.340 --> 00:46:46.940]   But you know, it's interesting.
[00:46:46.940 --> 00:46:55.660]   So yeah, after the acquisition of the startup, I took a break, started doing some angel investing
[00:46:55.660 --> 00:47:03.500]   and I get people ping me periodically to come in and help with strategy and help with consulting
[00:47:03.500 --> 00:47:09.860]   and kind of running data orgs or rebooting data orgs sometimes.
[00:47:09.860 --> 00:47:11.420]   And so it's interesting.
[00:47:11.420 --> 00:47:18.140]   Deep learning, you know, go back like three, four years ago, it was seen as a risky proposition,
[00:47:18.140 --> 00:47:20.420]   you know, unless you're a small startup.
[00:47:20.420 --> 00:47:28.460]   So bigger companies were not, I think, doing it as much, you know, in like 2014, 2015.
[00:47:28.460 --> 00:47:36.140]   But now it does seem like everybody, you know, everybody wants to be using TensorFlow, PyTorch,
[00:47:36.140 --> 00:47:37.660]   things like that.
[00:47:37.660 --> 00:47:43.700]   I think also, obviously the cloud providers have become, you know, a big player.
[00:47:43.700 --> 00:47:46.740]   So a lot of people are using, you know, SageMaker.
[00:47:46.740 --> 00:47:51.700]   They might be using, you know, the Google Cloud Platform.
[00:47:51.700 --> 00:47:55.100]   But do you have stuff you recommend when you come in?
[00:47:55.100 --> 00:47:59.660]   Do you have an opinion on what people should be using?
[00:47:59.660 --> 00:48:02.980]   I'd say I am still somewhat agnostic.
[00:48:02.980 --> 00:48:10.140]   So generally I tend to use Amazon myself.
[00:48:10.140 --> 00:48:16.820]   But I'm open to using other tools and other platforms.
[00:48:16.820 --> 00:48:20.260]   And so for example, I've got a camera here.
[00:48:20.260 --> 00:48:23.300]   This is pretty cool.
[00:48:23.300 --> 00:48:25.820]   So I've got this Azure smart camera.
[00:48:25.820 --> 00:48:26.820]   I'll plug that.
[00:48:26.820 --> 00:48:29.820]   So I'm going to play around with that.
[00:48:29.820 --> 00:48:34.940]   But that's the cool thing is like, I mean, if you're using TensorFlow, you know, all
[00:48:34.940 --> 00:48:37.540]   the big players use all the same open source stuff.
[00:48:37.540 --> 00:48:42.740]   So I can run TensorFlow on Microsoft or Google or whatever.
[00:48:42.740 --> 00:48:47.220]   And I think, you know, it really depends on the problem and it depends on your company's
[00:48:47.220 --> 00:48:48.220]   stack, right?
[00:48:48.220 --> 00:48:52.100]   I'd say if you're already all in on Google, then using a lot of the Google tooling can
[00:48:52.100 --> 00:48:53.540]   make sense.
[00:48:53.540 --> 00:48:59.460]   And so that's where I think like, you know, I'm not religious about one platform or another.
[00:48:59.460 --> 00:49:03.260]   I think they're all converging to some degree.
[00:49:03.260 --> 00:49:09.340]   But I have a lot more experience with the Amazon stack, probably like most people.
[00:49:09.340 --> 00:49:10.500]   But yeah.
[00:49:10.500 --> 00:49:17.220]   So in terms of like what I see at these companies, I think that what ends up happening, it's
[00:49:17.220 --> 00:49:22.540]   actually similar to how things were a decade ago, I guess, in that data science, when we
[00:49:22.540 --> 00:49:28.020]   changed the branding, right, to from research scientist or machine learning scientist to
[00:49:28.020 --> 00:49:36.140]   data science, a lot of that was because you needed people who could put stuff into production.
[00:49:36.140 --> 00:49:40.320]   And that production, machine learning, engineering and data engineering was different than an
[00:49:40.320 --> 00:49:43.080]   academic who can write a paper.
[00:49:43.080 --> 00:49:52.900]   And so I think that you do have this challenge when it comes to hiring and shipping products.
[00:49:52.900 --> 00:49:57.980]   Hiring research scientists is something that's difficult for anybody, but it's difficult
[00:49:57.980 --> 00:50:00.020]   if that's not your area of expertise.
[00:50:00.020 --> 00:50:04.740]   If you're an enterprise company, if you're building workplace software or even if you're
[00:50:04.740 --> 00:50:08.540]   a consumer company, right.
[00:50:08.540 --> 00:50:13.340]   Typically they'll have like, you know, a VP of end or something, try to manage those teams.
[00:50:13.340 --> 00:50:18.580]   But if they don't have background, it can be really hard because planning is hard, right.
[00:50:18.580 --> 00:50:22.300]   Prioritizing those projects, knowing what's likely to work.
[00:50:22.300 --> 00:50:26.260]   If somebody hasn't done it before and then you hire some people like out of school or
[00:50:26.260 --> 00:50:30.820]   people who've worked on Kaggle competitions, there's a lot of pieces that they're missing
[00:50:30.820 --> 00:50:33.300]   to actually ship and execute.
[00:50:33.300 --> 00:50:37.020]   Do you think there's something different about data stuff than other things?
[00:50:37.020 --> 00:50:43.860]   Because if I'm a VP of engineering, I can't be an expert on DevOps and architecture and
[00:50:43.860 --> 00:50:44.860]   all these things.
[00:50:44.860 --> 00:50:47.100]   So I have to kind of rely on folks.
[00:50:47.100 --> 00:50:51.060]   Is there something that makes data science particularly challenging in this way?
[00:50:51.060 --> 00:50:52.620]   I think part of it.
[00:50:52.620 --> 00:50:57.220]   So look, in all these other areas, the tools and technologies definitely change and people
[00:50:57.220 --> 00:50:58.700]   have to keep up with them.
[00:50:58.700 --> 00:51:05.380]   I think one of the challenges with machine learning is partly because of the people who
[00:51:05.380 --> 00:51:10.900]   work on it and partly just because of the nature of the field and how rapidly it's changing.
[00:51:10.900 --> 00:51:14.660]   They're always trying the latest thing, right.
[00:51:14.660 --> 00:51:17.420]   And I think that's very hard to manage, whether it's even just something as simple as like
[00:51:17.420 --> 00:51:25.380]   library dependencies or methodology.
[00:51:25.380 --> 00:51:30.840]   Things are changing rapidly and that is at the root of it.
[00:51:30.840 --> 00:51:37.700]   I think the other thing is, if you're doing DevOps, DevOps at company A actually probably
[00:51:37.700 --> 00:51:40.780]   looks a lot like DevOps at company B, right.
[00:51:40.780 --> 00:51:47.220]   You choose your stack, you choose your tooling and then you live with the consequences of
[00:51:47.220 --> 00:51:49.380]   those decisions.
[00:51:49.380 --> 00:51:54.580]   But for machine learning, almost every problem is different.
[00:51:54.580 --> 00:51:58.820]   So it's kind of like that saying, what is it?
[00:51:58.820 --> 00:52:01.660]   Every family is dysfunctional in its own unique way.
[00:52:01.660 --> 00:52:07.940]   So I think the same thing is true of machine learning teams and projects.
[00:52:07.940 --> 00:52:16.940]   If you're trying to predict financial fraud and before you were working on detecting porn
[00:52:16.940 --> 00:52:21.860]   and user profile images, those are two vastly different problems.
[00:52:21.860 --> 00:52:27.500]   So SREs may look like SREs across companies, but machine learning problems are not all
[00:52:27.500 --> 00:52:28.500]   identical.
[00:52:28.500 --> 00:52:29.500]   Interesting.
[00:52:29.500 --> 00:52:34.180]   Let's talk about this article you wrote, what you need to know about product management
[00:52:34.180 --> 00:52:35.180]   for AI.
[00:52:35.180 --> 00:52:41.260]   I guess it seems like the best place to start with this is a question I have about a lot
[00:52:41.260 --> 00:52:46.380]   of things written about AI, which is kind of like what makes product management for
[00:52:46.380 --> 00:52:51.840]   AI different than product management in general?
[00:52:51.840 --> 00:52:58.000]   So I think the main difference between product management for AI and product management for
[00:52:58.000 --> 00:53:06.980]   traditional software projects is that machine learning software is inherently probabilistic,
[00:53:06.980 --> 00:53:11.140]   whereas classic software development is more deterministic.
[00:53:11.140 --> 00:53:17.860]   Basically with software, you have this rich methodology around unit tasks and functional
[00:53:17.860 --> 00:53:20.620]   tasks, integration tasks and builds.
[00:53:20.620 --> 00:53:26.500]   And you're working on developing the software and you expect it to always behave the same
[00:53:26.500 --> 00:53:30.460]   way if you've instrumented the right tasks.
[00:53:30.460 --> 00:53:33.940]   And that creates a very clear, comfortable development process.
[00:53:33.940 --> 00:53:39.740]   And both engineering leaders and architects and product managers are comfortable with
[00:53:39.740 --> 00:53:40.940]   that.
[00:53:40.940 --> 00:53:48.380]   So most product managers like to run projects that are predictable.
[00:53:48.380 --> 00:53:52.820]   So they like to be able to commit to deadlines, to work with partner teams and customers and
[00:53:52.820 --> 00:53:55.540]   be able to commit to a date.
[00:53:55.540 --> 00:54:02.340]   And if you are mostly building things that are clear and that are understandable or that
[00:54:02.340 --> 00:54:05.420]   like things that you built before, you can come up with good estimates if you have enough
[00:54:05.420 --> 00:54:06.420]   experience.
[00:54:06.420 --> 00:54:13.700]   I think with machine learning, the uncertainty comes from a bunch of places, but it's all
[00:54:13.700 --> 00:54:16.140]   the way down to the individual algorithm.
[00:54:16.140 --> 00:54:20.900]   So if you're training a model, there's some amount of randomization, different random
[00:54:20.900 --> 00:54:27.700]   weights can lead to different results, all the way to your approach.
[00:54:27.700 --> 00:54:30.380]   Your approach may be different.
[00:54:30.380 --> 00:54:33.740]   Every problem is a little bit different for machine learning.
[00:54:33.740 --> 00:54:35.920]   Otherwise it would be essentially solved.
[00:54:35.920 --> 00:54:37.820]   So there's always something different about it.
[00:54:37.820 --> 00:54:43.300]   Maybe it's a slightly different application, a different dataset that you're using that's,
[00:54:43.300 --> 00:54:45.420]   let's say a movie recommender.
[00:54:45.420 --> 00:54:50.500]   If you're going to recommend or video recommendation, if you're going to recommend videos on TikTok,
[00:54:50.500 --> 00:54:54.540]   that's on the surface seems similar to recommending Netflix movies.
[00:54:54.540 --> 00:54:58.620]   But if you peel back the onion, it's really pretty different.
[00:54:58.620 --> 00:55:00.500]   So they're short videos.
[00:55:00.500 --> 00:55:02.300]   There's not a lot of context.
[00:55:02.300 --> 00:55:06.180]   They're very fresh, very new every day.
[00:55:06.180 --> 00:55:07.820]   And they're user generated.
[00:55:07.820 --> 00:55:11.060]   So you don't know what is going to be in that video.
[00:55:11.060 --> 00:55:12.940]   And there's not a lot of dialogue.
[00:55:12.940 --> 00:55:14.540]   There's just something interesting.
[00:55:14.540 --> 00:55:15.540]   It's more visual.
[00:55:15.540 --> 00:55:21.760]   Versus Netflix has a curated catalog of blockbuster movies or self-produced movies where everything
[00:55:21.760 --> 00:55:24.380]   is very carefully controlled.
[00:55:24.380 --> 00:55:26.780]   So on the surface, those both look like recommender problems.
[00:55:26.780 --> 00:55:31.140]   But for a machine learning person, they would realize, okay, there's a huge set of different
[00:55:31.140 --> 00:55:35.420]   things I would have to do for TikTok than I would have to do for Netflix.
[00:55:35.420 --> 00:55:36.740]   So that's just scratching the surface.
[00:55:36.740 --> 00:55:38.540]   But why are these different?
[00:55:38.540 --> 00:55:44.300]   Really the planning process is often very different because it's very data dependent
[00:55:44.300 --> 00:55:52.600]   and very application dependent versus a user signup flow looks very similar across many
[00:55:52.600 --> 00:55:54.860]   different software applications.
[00:55:54.860 --> 00:56:00.180]   So what does the planning process even look like in the face of this amount of uncertainty?
[00:56:00.180 --> 00:56:01.180]   Yeah.
[00:56:01.180 --> 00:56:04.260]   So I think there's two things.
[00:56:04.260 --> 00:56:07.740]   There's what maybe the planning process should look like and then realistically what it looks
[00:56:07.740 --> 00:56:09.660]   like in most companies.
[00:56:09.660 --> 00:56:13.260]   So I'd say in most companies, the pattern I've seen is people just do what they know.
[00:56:13.260 --> 00:56:19.620]   They continue to try to plan these traditional software products.
[00:56:19.620 --> 00:56:25.940]   I think the better teams are aware of some of these issues.
[00:56:25.940 --> 00:56:32.700]   And they treat the uncertainty from day one and they build it into their planning.
[00:56:32.700 --> 00:56:37.940]   So it's effectively most machine learning projects are much closer to R&D than they
[00:56:37.940 --> 00:56:44.940]   are something very clear and easy to execute on.
[00:56:44.940 --> 00:56:49.660]   So I think that the best ways I've seen to plan involves first starting with what are
[00:56:49.660 --> 00:56:52.300]   the core problems that matter to your business.
[00:56:52.300 --> 00:56:57.500]   So one of the problems could just be the set of machine learning projects you're working
[00:56:57.500 --> 00:57:00.260]   on may not be the right ones.
[00:57:00.260 --> 00:57:04.820]   So typically companies have some kind of product planning process, roadmap building.
[00:57:04.820 --> 00:57:05.820]   They may do this quarterly.
[00:57:05.820 --> 00:57:12.620]   They may do this annually where they come up with a set of funded projects and that
[00:57:12.620 --> 00:57:17.940]   they're going to staff, that they're going to resource and they're going to execute on.
[00:57:17.940 --> 00:57:23.820]   And so I think fundamentally you need to have a clear set of projects that align with your
[00:57:23.820 --> 00:57:25.380]   company strategy.
[00:57:25.380 --> 00:57:31.660]   So let's say your consumer app and growth is important.
[00:57:31.660 --> 00:57:40.860]   So one of your key metrics may be daily active users and time on site and signups, things
[00:57:40.860 --> 00:57:41.860]   like that.
[00:57:41.860 --> 00:57:50.180]   So clear business metrics where if your machine learning project has an impact, you can see
[00:57:50.180 --> 00:57:51.820]   the number change.
[00:57:51.820 --> 00:57:55.660]   So what you don't want to have happen is you don't want to spend six months to a year working
[00:57:55.660 --> 00:57:57.460]   on a machine learning project.
[00:57:57.460 --> 00:58:02.780]   And then at the end of it, you can't see a material impact in any numbers.
[00:58:02.780 --> 00:58:04.300]   And this does happen a lot.
[00:58:04.300 --> 00:58:09.380]   So a lot of people, I'd say, especially in enterprise, machine learning is seen often
[00:58:09.380 --> 00:58:15.020]   more as a feature, like an interesting checkbox to have, but it's not necessarily tied to
[00:58:15.020 --> 00:58:16.020]   a clear business outcome.
[00:58:16.020 --> 00:58:18.660]   Why do you think that happens?
[00:58:18.660 --> 00:58:24.180]   A lot of folks have talked about it that we've talked to, but it sort of seems like connecting
[00:58:24.180 --> 00:58:32.300]   a project to a business outcome is something that's a best practice for any kind of project.
[00:58:32.300 --> 00:58:36.020]   I do hear this over and over, so there must be something going on, but what do you think
[00:58:36.020 --> 00:58:37.400]   it is?
[00:58:37.400 --> 00:58:42.600]   I think some of it is just lack of familiarity with the domain.
[00:58:42.600 --> 00:58:46.460]   So in some ways, unfortunately, it's like blockchain, right?
[00:58:46.460 --> 00:58:51.340]   So people hear a buzzword, they hear blockchain, and they say, "Okay, we need to have a blockchain
[00:58:51.340 --> 00:58:52.340]   story."
[00:58:52.340 --> 00:58:55.960]   So I think when these things start top down sometimes, that happens.
[00:58:55.960 --> 00:59:03.480]   So the company may say, "Okay, our board is pushing us to have a blockchain strategy."
[00:59:03.480 --> 00:59:10.460]   And then they get some consultants in maybe, or they have internal execs come up with something.
[00:59:10.460 --> 00:59:14.160]   And then I think when these things tend to be pushed top down sometimes, it can be good
[00:59:14.160 --> 00:59:16.640]   to have executive support, don't get me wrong.
[00:59:16.640 --> 00:59:21.880]   But I think you do need the bottom up expertise and experience to connect those dots.
[00:59:21.880 --> 00:59:25.480]   And that's really where product management shines.
[00:59:25.480 --> 00:59:30.760]   So I think if you have a good product manager who's very numbers driven, that can help.
[00:59:30.760 --> 00:59:36.040]   And I do think that tends to happen more in these instrumented companies versus enterprises
[00:59:36.040 --> 00:59:38.240]   usually more sales driven.
[00:59:38.240 --> 00:59:43.600]   What about specifically addressing the uncertainty?
[00:59:43.600 --> 00:59:49.280]   Say you have a thing that's connected to a business outcome, but like you said earlier,
[00:59:49.280 --> 00:59:54.200]   with ML, it's often hard to even know how good of a system you can build.
[00:59:54.200 --> 00:59:59.160]   So how do you plan around that level of uncertainty?
[00:59:59.160 --> 01:00:02.440]   Yeah, so there are a number of different strategies.
[01:00:02.440 --> 01:00:09.120]   One of the ones that I'm really honing in on lately is, it's an old strategy.
[01:00:09.120 --> 01:00:12.260]   It's what DARPA used for self-driving cars.
[01:00:12.260 --> 01:00:18.640]   It's what Netflix used for the Netflix prize and all the data mining competitions used
[01:00:18.640 --> 01:00:20.740]   for years, which is benchmarks.
[01:00:20.740 --> 01:00:27.040]   So I think if you have a clear, you could go back to video recommendation again.
[01:00:27.040 --> 01:00:37.340]   Netflix created a benchmark data set and held back some test data set and released training
[01:00:37.340 --> 01:00:42.600]   data for people to train models and then had a clear set of evaluation metrics.
[01:00:42.600 --> 01:00:47.200]   And they said, here's current state of the art or what is in production right now.
[01:00:47.200 --> 01:00:52.080]   And we're going to pay a million dollars for the first team to get a 10% improvement.
[01:00:52.080 --> 01:00:55.440]   I think it was 10%, I don't remember.
[01:00:55.440 --> 01:00:58.160]   And so that makes it the nice thing.
[01:00:58.160 --> 01:01:01.540]   And this will appeal to, I think, a lot of product managers.
[01:01:01.540 --> 01:01:06.960]   They like clear objectives and goals that people can rally around, that your team can
[01:01:06.960 --> 01:01:08.160]   rally around.
[01:01:08.160 --> 01:01:10.440]   So I've found that really effective.
[01:01:10.440 --> 01:01:11.540]   I found two things.
[01:01:11.540 --> 01:01:15.800]   If you can't construct that benchmark, and it's surprising the number of teams that actually
[01:01:15.800 --> 01:01:19.760]   skip ahead, they just don't even bother with that.
[01:01:19.760 --> 01:01:24.840]   They may have some business metrics they're measuring, and they may have model metrics
[01:01:24.840 --> 01:01:29.960]   that they're using internally, but they're not really connecting those in a clear way.
[01:01:29.960 --> 01:01:31.800]   And they're not doing it.
[01:01:31.800 --> 01:01:34.320]   For example, it's like testing and production.
[01:01:34.320 --> 01:01:37.920]   They may do something where they have an AB test, and they say, hey, when we rolled out
[01:01:37.920 --> 01:01:42.800]   this new model code, in our AB test, we saw a 5% lift.
[01:01:42.800 --> 01:01:44.480]   So it's better than the old one.
[01:01:44.480 --> 01:01:45.480]   Good job.
[01:01:45.480 --> 01:01:46.880]   Work on the next model.
[01:01:46.880 --> 01:01:53.400]   And if you're doing that, it's very easy to fool yourself, and it's very hard to debug.
[01:01:53.400 --> 01:01:57.080]   So that's where the uncertainty creeps in, is you don't really know where you stand.
[01:01:57.080 --> 01:02:01.160]   You don't know, well, was there something else happening in the data during that time
[01:02:01.160 --> 01:02:03.800]   period that affected the model?
[01:02:03.800 --> 01:02:08.680]   If we reran that same model on new data, would we get the same result?
[01:02:08.680 --> 01:02:14.060]   And so this is where I think experiment management is one way you could frame this.
[01:02:14.060 --> 01:02:21.760]   It's really critical that you build those benchmarks, and that you hold out some stream
[01:02:21.760 --> 01:02:27.360]   of traffic, for example, and you keep running on multiple models so that you can ensure
[01:02:27.360 --> 01:02:29.600]   you haven't regressed in terms of performance.
[01:02:29.600 --> 01:02:38.720]   I remember one of the things that you said to me in a private conversation was talking
[01:02:38.720 --> 01:02:39.720]   about stand-ups.
[01:02:39.720 --> 01:02:45.600]   ML teams feels a little different than stand-ups with an engineering team, because it's kind
[01:02:45.600 --> 01:02:49.440]   of like with engineering teams, like, hey, I did this feature, I did this feature.
[01:02:49.440 --> 01:02:51.000]   But then with an ML team, it's a little harder.
[01:02:51.000 --> 01:02:53.560]   Do you have any thoughts around that?
[01:02:53.560 --> 01:02:54.560]   Yeah.
[01:02:54.560 --> 01:02:55.560]   Yeah.
[01:02:55.560 --> 01:02:59.840]   So I think I've mentioned this on Twitter a few times as well, and it's interesting
[01:02:59.840 --> 01:03:02.520]   to see the discussion that people have around this.
[01:03:02.520 --> 01:03:09.960]   So I think a lot of data scientists, it resonates in that...
[01:03:09.960 --> 01:03:15.520]   And some of it may just be understanding the nature of the work that scientists and machine
[01:03:15.520 --> 01:03:20.400]   learning researchers are doing, and then how to translate that into that kind of stand-up
[01:03:20.400 --> 01:03:21.960]   format.
[01:03:21.960 --> 01:03:28.360]   I think you see the clash of cultures immediately in a stand-up, because often people have...
[01:03:28.360 --> 01:03:34.960]   Say you're doing, I don't know, Agile development or Scrum or something where you have very
[01:03:34.960 --> 01:03:41.960]   clear chunks of work that may take one to two days in traditional software development.
[01:03:41.960 --> 01:03:46.360]   If all the people working on other parts of the product launch have work that's easily
[01:03:46.360 --> 01:03:51.280]   chunked in that way, then they can close the JIRA tickets more easily.
[01:03:51.280 --> 01:03:58.920]   They say, "Oh yeah, I implemented that API that will talk to the email system, and we're
[01:03:58.920 --> 01:04:02.320]   all set, and it's in testing."
[01:04:02.320 --> 01:04:04.600]   That's very different from...
[01:04:04.600 --> 01:04:10.600]   They get to the data scientists in the stand-up, and they say, "Well, I'm still training the
[01:04:10.600 --> 01:04:14.440]   model and something's not working, but I'm not quite sure what it is.
[01:04:14.440 --> 01:04:21.880]   I'm going to look into the initialization parameters and maybe try to optimize that,
[01:04:21.880 --> 01:04:24.000]   and I'll report back next time."
[01:04:24.000 --> 01:04:25.000]   And then repeat.
[01:04:25.000 --> 01:04:27.240]   It's always something until the model...
[01:04:27.240 --> 01:04:30.800]   The model isn't working until it's working, unfortunately.
[01:04:30.800 --> 01:04:32.320]   I think that can create some stress.
[01:04:32.320 --> 01:04:37.840]   Maybe it's just me, but I feel that stress.
[01:04:37.840 --> 01:04:45.080]   In terms of strategies to deal with that for product managers, I think it's at least good
[01:04:45.080 --> 01:04:46.880]   to call it out.
[01:04:46.880 --> 01:04:51.720]   I think if you don't talk about it, it can start to seem strange.
[01:04:51.720 --> 01:04:54.500]   I think it's at least worth calling out.
[01:04:54.500 --> 01:04:59.920]   This gets to the point of organizational support for these ML projects.
[01:04:59.920 --> 01:05:06.280]   If you listen to chatter, there's all kinds of apps for back channels now, and there's
[01:05:06.280 --> 01:05:14.120]   Slack, and there's other things like Blind, where people talk about their companies.
[01:05:14.120 --> 01:05:19.280]   Especially in an environment like now, where there's economic uncertainty and pressure,
[01:05:19.280 --> 01:05:24.040]   I think increasingly you're going to have this chatter, which is already there, around,
[01:05:24.040 --> 01:05:27.000]   "Hey, what are those ML people actually doing?"
[01:05:27.000 --> 01:05:29.520]   They're getting these big paychecks.
[01:05:29.520 --> 01:05:30.520]   Where's the beef?
[01:05:30.520 --> 01:05:33.920]   What are they delivering?
[01:05:33.920 --> 01:05:36.720]   I think this is critically important.
[01:05:36.720 --> 01:05:43.400]   Stand-ups, I think it'd be good to have more of a clarity around what should machine learning
[01:05:43.400 --> 01:05:48.920]   folks report in stand-ups, and make it clear that the progress meter is going to be a little
[01:05:48.920 --> 01:05:49.920]   different.
[01:05:49.920 --> 01:05:51.920]   It may be research results.
[01:05:51.920 --> 01:05:55.640]   Here's the objective we have this week.
[01:05:55.640 --> 01:05:59.800]   Here are the things that we wanted to put in place, and we accomplished them, even if
[01:05:59.800 --> 01:06:01.520]   the results are not there.
[01:06:01.520 --> 01:06:05.640]   I think if you say, "Hey, we're going to improve by 5% this week, and that's our goal
[01:06:05.640 --> 01:06:09.080]   for the stand-up," that can be very hard, because you may not hit it.
[01:06:09.080 --> 01:06:10.080]   That's funny.
[01:06:10.080 --> 01:06:18.120]   We had Chris Alban on this show also, and he was talking a lot about creating a sense
[01:06:18.120 --> 01:06:21.800]   of emotional security for his ML team.
[01:06:21.800 --> 01:06:31.280]   I think a big part of that for him was not focusing people too much on the external 5%
[01:06:31.280 --> 01:06:32.280]   increase goals.
[01:06:32.280 --> 01:06:39.000]   I have to say, he made some sense, but I was a little bit unconvinced for myself.
[01:06:39.000 --> 01:06:46.360]   I feel like as someone running a company under pressure, I do feel like for me, I think the
[01:06:46.360 --> 01:06:54.200]   way that I run my teams is pretty external metrics focused.
[01:06:54.200 --> 01:06:59.000]   But I think there's downsides to it for sure, and it's very hard to know what a reasonable
[01:06:59.000 --> 01:07:00.000]   goal is.
[01:07:00.000 --> 01:07:04.400]   I go back and forth.
[01:07:04.400 --> 01:07:08.680]   I think it varies depending on the stage of the project and of the company.
[01:07:08.680 --> 01:07:12.360]   When they're just starting with machine learning, I think the hard reality is it's going to
[01:07:12.360 --> 01:07:18.520]   be hard to get to that number when you don't even have your ML infrastructure in place.
[01:07:18.520 --> 01:07:25.440]   I think in the early stages, before you actually have a working product in production, unfortunately,
[01:07:25.440 --> 01:07:27.080]   it's going to be really hard to be metrics driven.
[01:07:27.080 --> 01:07:28.720]   You may have decoupled.
[01:07:28.720 --> 01:07:35.600]   You may have some set of people working on sample data, training a model, and maybe quickly
[01:07:35.600 --> 01:07:38.960]   you can get to a benchmark.
[01:07:38.960 --> 01:07:41.480]   What I would suggest people do is...
[01:07:41.480 --> 01:07:47.240]   I kind of agree with you and I agree with Chris, but I think that you have to encapsulate
[01:07:47.240 --> 01:07:48.240]   it in different ways.
[01:07:48.240 --> 01:07:54.620]   If you have no part of your team that's numbers driven, then you're in trouble.
[01:07:54.620 --> 01:07:57.080]   What I would say is, let's say you're starting a new project.
[01:07:57.080 --> 01:08:03.840]   Let's say it's fraud detection and accounting or something, and you're going to roll out
[01:08:03.840 --> 01:08:05.960]   that model.
[01:08:05.960 --> 01:08:09.420]   It's one of the things when you get back to project management planning and how you run
[01:08:09.420 --> 01:08:16.840]   these projects, as quickly as possible, you need to get to a benchmark data set.
[01:08:16.840 --> 01:08:18.920]   I remember Leslie Cabling.
[01:08:18.920 --> 01:08:23.680]   I think we talked about her before.
[01:08:23.680 --> 01:08:25.200]   She's a professor at MIT.
[01:08:25.200 --> 01:08:29.040]   I took a machine learning course she taught years ago.
[01:08:29.040 --> 01:08:32.800]   One of the things she said, there was a project in the course and people had to pick projects.
[01:08:32.800 --> 01:08:36.240]   In some ways, it's similar to picking a project in your company.
[01:08:36.240 --> 01:08:40.080]   She said, "One of the most important things is if you don't have the data set in hand
[01:08:40.080 --> 01:08:44.560]   now, pick a different problem because you're going to spend the whole semester just gathering
[01:08:44.560 --> 01:08:48.440]   the data set."
[01:08:48.440 --> 01:08:50.520]   I wouldn't necessarily give that advice for a company.
[01:08:50.520 --> 01:08:55.280]   If you don't have the data set, maybe you do need to gather it.
[01:08:55.280 --> 01:09:00.560]   If you can have that data set ready to go and get people working on a benchmark right
[01:09:00.560 --> 01:09:06.160]   away, then you can get on this nice track where you can track progress.
[01:09:06.160 --> 01:09:07.520]   My teams would even put...
[01:09:07.520 --> 01:09:11.280]   We'd have a whiteboard with a goal.
[01:09:11.280 --> 01:09:15.220]   "Hey, in two weeks, this is the number we want to hit."
[01:09:15.220 --> 01:09:19.600]   Maybe it's AUCX.
[01:09:19.600 --> 01:09:23.360]   Somebody would just keep their eye on that number and we'd know what we were shooting
[01:09:23.360 --> 01:09:24.360]   for.
[01:09:24.360 --> 01:09:27.080]   It would be a two-week horizon?
[01:09:27.080 --> 01:09:29.680]   I think, yeah, two to three weeks.
[01:09:29.680 --> 01:09:32.120]   Now, that's once you have a working model.
[01:09:32.120 --> 01:09:33.840]   Once you have a baseline model...
[01:09:33.840 --> 01:09:37.880]   One of the most important things you can do is just start with an MVP.
[01:09:37.880 --> 01:09:45.120]   Get something basic in place so that you can get on that AUC improvement train.
[01:09:45.120 --> 01:09:48.800]   Once you can do that, then you can create this momentum where the team feels like there's
[01:09:48.800 --> 01:09:51.160]   progress.
[01:09:51.160 --> 01:09:55.680]   I think for project planning, then it becomes more clear.
[01:09:55.680 --> 01:09:59.560]   Once something's shipped, and assuming it is tied to a business metric where you improve
[01:09:59.560 --> 01:10:05.080]   AUC and then you see revenue increase or users increase or something, then it becomes very
[01:10:05.080 --> 01:10:07.920]   clear what impact your team is having.
[01:10:07.920 --> 01:10:12.120]   I think this solves that back-channel chatter issue as well.
[01:10:12.120 --> 01:10:16.900]   Part of the PM's job here is to keep people moving towards that objective, but then also
[01:10:16.900 --> 01:10:20.320]   to communicate it to the rest of the company.
[01:10:20.320 --> 01:10:27.120]   Weekly status emails, get in the company, update emails that go out to everybody, and
[01:10:27.120 --> 01:10:34.160]   make it very clear that, "Hey, we have these model improvements which caused X or Y to
[01:10:34.160 --> 01:10:39.520]   our bottom line metrics."
[01:10:39.520 --> 01:10:40.520]   That makes total sense.
[01:10:40.520 --> 01:10:41.520]   Yeah.
[01:10:41.520 --> 01:10:46.500]   Anyway, Chris, I don't know what Chris's comments were, but it sounds like shielding
[01:10:46.500 --> 01:10:53.900]   your team so they don't feel overly stressed by metrics can make sense.
[01:10:53.900 --> 01:10:58.300]   I think that's more important in deep R&D.
[01:10:58.300 --> 01:11:04.580]   For example, at Google, they would have 20% projects.
[01:11:04.580 --> 01:11:06.740]   At LinkedIn, we do this as well.
[01:11:06.740 --> 01:11:11.840]   But it's not as simple as one day a week of someone's time.
[01:11:11.840 --> 01:11:18.400]   Often what would happen is entire sets of people would be 100% on a 20% project.
[01:11:18.400 --> 01:11:22.240]   In ML, that's really what you need to do sometimes.
[01:11:22.240 --> 01:11:23.240]   Shield those people, definitely.
[01:11:23.240 --> 01:11:31.680]   I remember years ago, the Diffbot CEO told me that he basically gave out bonuses to his
[01:11:31.680 --> 01:11:39.720]   ML team based on the lift that they got on the projects that they were working on.
[01:11:39.720 --> 01:11:45.860]   On one hand, it seems like a very fair management strategy where you're pushing down the decision
[01:11:45.860 --> 01:11:49.660]   making to the folks working on it.
[01:11:49.660 --> 01:11:55.460]   I guess on the other hand, I've worked on many projects where I've been really surprised
[01:11:55.460 --> 01:11:59.700]   by not being able to make any progress or being able to make more progress than I wanted.
[01:11:59.700 --> 01:12:04.140]   I think that could be a more stressful environment for sure.
[01:12:04.140 --> 01:12:07.180]   I think in the article, I mentioned something related to this.
[01:12:07.180 --> 01:12:13.720]   I'm not sure if I mentioned OKRs explicitly, but I do talk about setting goals and setting
[01:12:13.720 --> 01:12:16.660]   objectives and then how that can go wrong.
[01:12:16.660 --> 01:12:18.420]   I think this is a great example.
[01:12:18.420 --> 01:12:24.020]   I am a believer in that general framework.
[01:12:24.020 --> 01:12:31.540]   So for people who aren't familiar with product management via measurement via OKRs, what
[01:12:31.540 --> 01:12:38.660]   typically happens is at the beginning of a quarter, everybody signs up for an OKR.
[01:12:38.660 --> 01:12:41.740]   You're not supposed to sandbag.
[01:12:41.740 --> 01:12:43.100]   You're not supposed to set the bar low.
[01:12:43.100 --> 01:12:48.220]   You're supposed to have a reasoned, ambitious goal.
[01:12:48.220 --> 01:12:55.020]   So maybe you typically have something like three OKRs per quarter, and it might be increase
[01:12:55.020 --> 01:13:05.420]   user signups by 20%, increase revenue per user by 10%, something like that.
[01:13:05.420 --> 01:13:06.420]   And they may be a little more granular.
[01:13:06.420 --> 01:13:11.180]   So especially in a larger company, they become more granular and it may be something like
[01:13:11.180 --> 01:13:21.060]   increase search relevance as measured by F1 or whatever by 30%.
[01:13:21.060 --> 01:13:26.140]   And so in any case, those metrics I think are important.
[01:13:26.140 --> 01:13:32.780]   And I think the hard part is among the product leaders, you have to be very careful about
[01:13:32.780 --> 01:13:39.300]   how much latitude you give on just pure ML metrics versus business metrics.
[01:13:39.300 --> 01:13:41.380]   Because it's very easy for teams.
[01:13:41.380 --> 01:13:45.660]   That's how you get this bubble where everybody's just doing R&D.
[01:13:45.660 --> 01:13:50.220]   And then I think what ends up happening is a lot of the business leaders and PMs see
[01:13:50.220 --> 01:13:58.740]   those OKRs and they just shrug and say, "I don't understand how that relates to the business."
[01:13:58.740 --> 01:14:00.260]   But OKRs, I would...
[01:14:00.260 --> 01:14:02.980]   Rewarding people for OKRs is pretty standard.
[01:14:02.980 --> 01:14:08.460]   And where that can go wrong, I think the YouTube example is one of the best known ones where
[01:14:08.460 --> 01:14:15.820]   by all OKR metrics, YouTube has been succeeding wildly over the last five or six years probably.
[01:14:15.820 --> 01:14:22.180]   But the downside is when you manage to a single number, it doesn't...
[01:14:22.180 --> 01:14:23.180]   PMs become machines.
[01:14:23.180 --> 01:14:31.780]   They're like the parable of AI and paperclips where you build an amazing paperclip optimizer
[01:14:31.780 --> 01:14:37.660]   with AI and then it destroys the world to make as many paperclips as it can.
[01:14:37.660 --> 01:14:41.540]   So PMs are the same way where you give them a metric, they're going to hit it.
[01:14:41.540 --> 01:14:43.020]   But there may be a lot of collateral damage.
[01:14:43.020 --> 01:14:47.300]   In the case of YouTube, there was a lot of misinformation and conspiracy theories because
[01:14:47.300 --> 01:14:48.660]   they lead to clicks.
[01:14:48.660 --> 01:14:54.120]   So by the measure of engagement on YouTube, they're doing fantastic.
[01:14:54.120 --> 01:14:55.620]   But at what cost?
[01:14:55.620 --> 01:14:56.620]   Right.
[01:14:56.620 --> 01:15:01.540]   I guess that's a good segue to another topic that you topic about in your paper that might
[01:15:01.540 --> 01:15:06.940]   be interesting to talk about here, which is building infrastructure to make your AI or
[01:15:06.940 --> 01:15:10.540]   machine learning actually scale.
[01:15:10.540 --> 01:15:13.500]   What kinds of recommendations do you have there?
[01:15:13.500 --> 01:15:14.500]   Yeah.
[01:15:14.500 --> 01:15:18.980]   So that's a deep, deep topic.
[01:15:18.980 --> 01:15:21.460]   So I think there's a spectrum of companies.
[01:15:21.460 --> 01:15:29.140]   So companies that are your Google's, Facebook's, A, they already are deep into building ML
[01:15:29.140 --> 01:15:30.260]   and they have all the frameworks.
[01:15:30.260 --> 01:15:37.060]   So it's hard to say what the advice that makes sense for them may not make sense for other
[01:15:37.060 --> 01:15:40.940]   companies, I guess, is one key thing to be aware of.
[01:15:40.940 --> 01:15:46.820]   And so realistically, I would break out a few different types of companies.
[01:15:46.820 --> 01:15:53.060]   So I would say for your hyper growth technology companies that are more consumer facing or
[01:15:53.060 --> 01:15:57.820]   enterprise SaaS apps that are in the cloud from day one, those are kind of your modern
[01:15:57.820 --> 01:16:00.080]   technology stack companies.
[01:16:00.080 --> 01:16:03.860]   In many cases, those companies have good tracking in place.
[01:16:03.860 --> 01:16:06.420]   They're using some modern frameworks and tools.
[01:16:06.420 --> 01:16:09.380]   They have Kafka, they have things like that.
[01:16:09.380 --> 01:16:11.640]   And they probably have good data ETL.
[01:16:11.640 --> 01:16:12.780]   Now it varies quite a bit.
[01:16:12.780 --> 01:16:19.980]   So some companies just move so fast that things are duct taped together, right?
[01:16:19.980 --> 01:16:23.800]   Even for successful startups.
[01:16:23.800 --> 01:16:28.860]   But it tends to be the case that those companies at least have a lot of the raw pieces in place
[01:16:28.860 --> 01:16:32.020]   so that when they do get to a stage where they want to use machine learning to make
[01:16:32.020 --> 01:16:38.860]   their products better, there's some amount of work, but it's maybe one year, 18 months
[01:16:38.860 --> 01:16:41.660]   of work to get things pretty solid.
[01:16:41.660 --> 01:16:48.620]   I think the bigger challenge are more legacy companies or enterprise companies where organizationally
[01:16:48.620 --> 01:16:51.100]   they're very large.
[01:16:51.100 --> 01:16:53.460]   Organizations may have different data systems.
[01:16:53.460 --> 01:16:57.100]   It's typically hard in those companies to get access to data.
[01:16:57.100 --> 01:17:03.820]   People may even hold on to data and not want to give it up without 10 meetings.
[01:17:03.820 --> 01:17:09.660]   And even when you get the data, the idea of getting the data means different things.
[01:17:09.660 --> 01:17:14.540]   So someone gives you a dump of data, which is static, is very different from, "Hey, we
[01:17:14.540 --> 01:17:15.540]   want to do this in production.
[01:17:15.540 --> 01:17:17.780]   We want to do fraud detection.
[01:17:17.780 --> 01:17:20.140]   We need a Kafka feed.
[01:17:20.140 --> 01:17:23.340]   You need all this infra."
[01:17:23.340 --> 01:17:29.580]   And so I think for those companies, my recommendation has been, don't try to reinvent the wheel.
[01:17:29.580 --> 01:17:32.420]   There are 20 or 30 companies.
[01:17:32.420 --> 01:17:38.620]   They see what Uber did with Michelangelo or what Salesforce is doing with Einstein.
[01:17:38.620 --> 01:17:43.220]   And there's everybody trying to build their own ML platform internally.
[01:17:43.220 --> 01:17:44.660]   So that's the wrong...
[01:17:44.660 --> 01:17:47.220]   That typically happens with the top-down guidance.
[01:17:47.220 --> 01:17:49.820]   "Hey, we need an AI strategy."
[01:17:49.820 --> 01:17:53.780]   Somewhere in those early discussions, they jump to the conclusion, "Oh, well, first we
[01:17:53.780 --> 01:17:57.100]   need to build our own AI framework and let's give that project a name."
[01:17:57.100 --> 01:17:58.700]   It's very much like...
[01:17:58.700 --> 01:18:01.980]   This happens a lot in software development in big companies.
[01:18:01.980 --> 01:18:02.980]   Let's come up with a name for the...
[01:18:02.980 --> 01:18:04.660]   It's project name-driven development.
[01:18:04.660 --> 01:18:06.620]   They come up with a name.
[01:18:06.620 --> 01:18:10.580]   That'll be our infrastructure ETL system.
[01:18:10.580 --> 01:18:12.620]   Let's go build that.
[01:18:12.620 --> 01:18:13.940]   And that might take two years.
[01:18:13.940 --> 01:18:14.940]   I don't know.
[01:18:14.940 --> 01:18:18.900]   Does this match what you're seeing in big customers?
[01:18:18.900 --> 01:18:20.380]   Yeah, it's funny.
[01:18:20.380 --> 01:18:26.140]   You put my quote in the article, which was probably the tweet that has made fun of the
[01:18:26.140 --> 01:18:27.140]   most stuff.
[01:18:27.140 --> 01:18:30.740]   I said, basically, big companies shouldn't build their own ML tools.
[01:18:30.740 --> 01:18:36.300]   And it's like, okay, I am selling an ML tool.
[01:18:36.300 --> 01:18:41.940]   I feel like I'm incredibly biased, but it is like...
[01:18:41.940 --> 01:18:48.540]   I would say from my experience, it's just baffling how you go into enterprise companies
[01:18:48.540 --> 01:18:51.380]   in particular, I think, do this.
[01:18:51.380 --> 01:18:57.380]   They build so much infrastructure they could integrate cheaply or for free.
[01:18:57.380 --> 01:19:03.340]   It's actually funny because you go into a Google or Facebook, they've been around longer,
[01:19:03.340 --> 01:19:08.580]   and they actually pull in a lot more open source infrastructure than companies less
[01:19:08.580 --> 01:19:09.580]   far along.
[01:19:09.580 --> 01:19:14.140]   And so, yeah, it's always surprising.
[01:19:14.140 --> 01:19:20.780]   And I think part of it is actually that folks want to view building ML infrastructure inside
[01:19:20.780 --> 01:19:25.100]   a company as a little bit of a career development path.
[01:19:25.100 --> 01:19:26.500]   The incentives are...
[01:19:26.500 --> 01:19:28.340]   Yeah, I totally agree.
[01:19:28.340 --> 01:19:30.860]   I think a big part of it is incentives.
[01:19:30.860 --> 01:19:31.860]   So if you think about...
[01:19:31.860 --> 01:19:33.420]   And this isn't just Silicon Valley.
[01:19:33.420 --> 01:19:34.900]   I think this is all over.
[01:19:34.900 --> 01:19:40.380]   PMs might be rewarded for OKRs and hitting the product metrics.
[01:19:40.380 --> 01:19:46.260]   A lot of engineers are rewarded for releasing a new open source framework, for giving a
[01:19:46.260 --> 01:19:51.180]   talk where there's some new infrastructure piece they built that everybody in the company
[01:19:51.180 --> 01:19:53.060]   is using.
[01:19:53.060 --> 01:19:55.220]   And so I think that's for...
[01:19:55.220 --> 01:19:59.180]   Engineering and product leaders really need to think about what's the rewarding.
[01:19:59.180 --> 01:20:01.980]   And one way to think about it maybe is reward leverage as well.
[01:20:01.980 --> 01:20:09.700]   So if I was in a situation where somebody made the choice to use an open source system
[01:20:09.700 --> 01:20:18.260]   or even to use a vendor and they delivered ahead of schedule and everything's working,
[01:20:18.260 --> 01:20:23.380]   you need to find a way to reward that as well as just rewarding, "Hey, I did a 18-month
[01:20:23.380 --> 01:20:27.500]   sprint to build something that is not as good as what I could get off the shelf."
[01:20:27.500 --> 01:20:28.500]   Right?
[01:20:28.500 --> 01:20:29.500]   Yeah, totally.
[01:20:29.500 --> 01:20:30.500]   Totally.
[01:20:30.500 --> 01:20:31.500]   But it's also fun.
[01:20:31.500 --> 01:20:32.500]   I think the hard part...
[01:20:32.500 --> 01:20:36.220]   You also deal when you roll out to customers, you deal with the engineers on the ground.
[01:20:36.220 --> 01:20:39.660]   And I think there's a lot of...
[01:20:39.660 --> 01:20:42.260]   There are good reservations around...
[01:20:42.260 --> 01:20:45.500]   Sometimes using a third party thing can make people uncomfortable.
[01:20:45.500 --> 01:20:50.180]   They don't feel like they can adapt it or change it to all their needs.
[01:20:50.180 --> 01:20:55.900]   And so that's where I think a lot of the frameworks need to be really responsive to what a customer
[01:20:55.900 --> 01:20:58.940]   needs and how flexible they are.
[01:20:58.940 --> 01:21:02.940]   Because I think we've all been in a situation where you use some third party thing and it's
[01:21:02.940 --> 01:21:04.460]   too rigid.
[01:21:04.460 --> 01:21:08.980]   And then eventually it causes a lot of headaches.
[01:21:08.980 --> 01:21:16.300]   And it does seem like a lot of the tools and infrastructure that comes out is built by
[01:21:16.300 --> 01:21:20.300]   a lot of engineers coming out of Facebook and Uber and others.
[01:21:20.300 --> 01:21:26.300]   And they might not actually realize the different needs that other companies might have.
[01:21:26.300 --> 01:21:27.300]   Yeah.
[01:21:27.300 --> 01:21:30.220]   And I think that part...
[01:21:30.220 --> 01:21:36.540]   When it comes to the infrastructure side, that's another common pattern.
[01:21:36.540 --> 01:21:41.220]   So Jay Kreps is the CEO of Confluent.
[01:21:41.220 --> 01:21:45.420]   Originally he was my engineering partner back at LinkedIn when we were building some of
[01:21:45.420 --> 01:21:47.220]   the data products we built.
[01:21:47.220 --> 01:21:50.820]   And he eventually went on...
[01:21:50.820 --> 01:21:52.700]   We had built a number of these things.
[01:21:52.700 --> 01:21:58.220]   And then his work on a lot of the infrastructure pieces and focus on that and what eventually
[01:21:58.220 --> 01:22:04.300]   became Kafka and other open source projects grew out of, "Hey, we've built this four or
[01:22:04.300 --> 01:22:05.700]   five times now.
[01:22:05.700 --> 01:22:08.340]   I think we should abstract this out."
[01:22:08.340 --> 01:22:15.300]   So that's a very different approach than saying, "Hey, we've never done this, but let's go
[01:22:15.300 --> 01:22:20.140]   design what we think the right thing is and then build this abstract platform."
[01:22:20.140 --> 01:22:21.900]   So I think I agree with him.
[01:22:21.900 --> 01:22:27.940]   I tend to think things that grow out of real experience tend to be better as terms of frameworks.
[01:22:27.940 --> 01:22:34.220]   So if you're selecting, make sure you're selecting something that as a product manager or engineering
[01:22:34.220 --> 01:22:38.980]   leader, make sure that you know the origin of the framework.
[01:22:38.980 --> 01:22:43.060]   And ideally, if you're at these companies like yours building this, one of the best
[01:22:43.060 --> 01:22:48.220]   things you can do is just see more problems and map what you're doing to those customer
[01:22:48.220 --> 01:22:49.220]   problems.
[01:22:49.220 --> 01:22:53.460]   We always end with two questions and I'm wondering how you're going to answer these.
[01:22:53.460 --> 01:22:58.060]   I think they're related to a lot of stuff we talked about, but here's my first one,
[01:22:58.060 --> 01:23:03.820]   which is, what do you think is the topic in data science or machine learning that people
[01:23:03.820 --> 01:23:05.340]   don't talk about enough?
[01:23:05.340 --> 01:23:10.900]   The kind of underrated thing that in your experience matters more than people spend
[01:23:10.900 --> 01:23:13.220]   time thinking about it?
[01:23:13.220 --> 01:23:17.300]   I think it's actually constructing good benchmarks.
[01:23:17.300 --> 01:23:24.460]   So if I were to look at, we talked about teams that are struggling or having trouble.
[01:23:24.460 --> 01:23:31.500]   I would say nine times out of 10, they haven't done the hard work to construct a crisp, clean,
[01:23:31.500 --> 01:23:38.500]   precise benchmark and an evaluation of how well their model is doing.
[01:23:38.500 --> 01:23:42.220]   And so what often happens is people have these notions, "Hey, I'm going to build a recommender.
[01:23:42.220 --> 01:23:43.220]   I know how to do a recommender.
[01:23:43.220 --> 01:23:45.460]   Oh yeah, we'll get this data."
[01:23:45.460 --> 01:23:48.160]   And people actually just start building the model.
[01:23:48.160 --> 01:23:51.860]   And then after the fact, maybe they label some sample data and they say, "Oh, this is
[01:23:51.860 --> 01:23:52.860]   my gold standard.
[01:23:52.860 --> 01:23:54.100]   Okay, this is how we're doing."
[01:23:54.100 --> 01:23:55.100]   And that's maybe.
[01:23:55.100 --> 01:23:58.020]   I'd say a lot of times people don't even do that.
[01:23:58.020 --> 01:24:00.500]   They just launch the thing.
[01:24:00.500 --> 01:24:04.940]   And then it becomes very hard, or they may use proxy metrics like, "Did it increase lift?
[01:24:04.940 --> 01:24:07.500]   Did it increase CTR?"
[01:24:07.500 --> 01:24:12.580]   And the A/B testing is not a benchmark, basically, is what I would say.
[01:24:12.580 --> 01:24:15.240]   So build the benchmark, be rigorous about it.
[01:24:15.240 --> 01:24:19.620]   And if at all possible, because the other thing that happens is when things aren't working,
[01:24:19.620 --> 01:24:22.740]   so say you're six months in and your model isn't working, you don't know why.
[01:24:22.740 --> 01:24:25.700]   You need that benchmark so that you can debug what's going on.
[01:24:25.700 --> 01:24:28.460]   And if you don't have it, you're going to flail.
[01:24:28.460 --> 01:24:33.900]   It's funny, we had a guy from OpenAI on the podcast a while back and he said the exact
[01:24:33.900 --> 01:24:34.900]   same thing.
[01:24:34.900 --> 01:24:41.740]   And he said he was mentioning that the Dota team spent six months building a hand-tuned
[01:24:41.740 --> 01:24:46.100]   benchmark just to say, "Here's a baseline rule-based system."
[01:24:46.100 --> 01:24:49.940]   They really spent six months actually building the benchmarks.
[01:24:49.940 --> 01:24:54.300]   I don't know the exact amount of time, but at our startup, I would almost say we spent
[01:24:54.300 --> 01:24:56.620]   20 to 30 percent of our time on that kind of thing.
[01:24:56.620 --> 01:24:58.340]   I think it totally makes sense.
[01:24:58.340 --> 01:25:02.460]   All right, the second and last question.
[01:25:02.460 --> 01:25:05.380]   When you look at, this is a good one for you actually.
[01:25:05.380 --> 01:25:10.020]   So when you look at all the ML projects you've seen, consulted, been a part of, what do you
[01:25:10.020 --> 01:25:17.580]   think is the hardest part about getting them from a model to a production deployed model?
[01:25:17.580 --> 01:25:20.860]   Where's the biggest bottleneck there?
[01:25:20.860 --> 01:25:22.540]   So from a working model?
[01:25:22.540 --> 01:25:24.140]   Well, actually no.
[01:25:24.140 --> 01:25:27.980]   I would say from conception.
[01:25:27.980 --> 01:25:34.700]   So here's the goal to deploy a model that people can actually use.
[01:25:34.700 --> 01:25:38.980]   I would say there's actually three hard parts.
[01:25:38.980 --> 01:25:41.260]   It's hard for me to pick just one.
[01:25:41.260 --> 01:25:45.660]   I would say one hard part is around actually getting the data.
[01:25:45.660 --> 01:25:49.860]   So a lot of companies, you're asking where did we get the data for the startup?
[01:25:49.860 --> 01:25:53.500]   Even within companies that seemingly have a lot of data, getting the data set you need
[01:25:53.500 --> 01:25:57.400]   to train the model is often really costly and hard.
[01:25:57.400 --> 01:25:59.100]   And there may be a lot of internal roadblocks.
[01:25:59.100 --> 01:26:02.140]   So that's one where I've seen people stumble.
[01:26:02.140 --> 01:26:07.660]   The other hard part about getting things to production is then actually the modeling approach.
[01:26:07.660 --> 01:26:13.460]   And so I don't believe, like a lot of people out there, I see this all the time in blogs
[01:26:13.460 --> 01:26:16.820]   and on Twitter, say, oh, the modeling doesn't actually matter that much.
[01:26:16.820 --> 01:26:18.660]   It's all these other auxiliary things.
[01:26:18.660 --> 01:26:19.660]   And it's commodity.
[01:26:19.660 --> 01:26:20.660]   I don't believe that.
[01:26:20.660 --> 01:26:22.180]   I don't believe modeling is commodity at all.
[01:26:22.180 --> 01:26:26.340]   I think it's actually really hard to get models to work correctly.
[01:26:26.340 --> 01:26:31.300]   And especially when you move beyond that toy or benchmark data set to real world data,
[01:26:31.300 --> 01:26:36.540]   building something robust and that works at scale is actually really difficult.
[01:26:36.540 --> 01:26:42.420]   So I'd say that's the second part is actually the hard elbow grease and research work of
[01:26:42.420 --> 01:26:46.340]   getting a working model is usually harder than people think.
[01:26:46.340 --> 01:26:59.180]   And then the last part, I would say, is actually getting buy-in, I would say, from executives.
[01:26:59.180 --> 01:27:04.100]   So this is a long journey to getting something out to production.
[01:27:04.100 --> 01:27:08.220]   And if you're running your own company and you're a CEO, that's one thing.
[01:27:08.220 --> 01:27:10.220]   And maybe you can push it through.
[01:27:10.220 --> 01:27:11.800]   You think it's really important.
[01:27:11.800 --> 01:27:15.940]   But if you're in any large organization, there's a bunch of stakeholders.
[01:27:15.940 --> 01:27:18.100]   There's a bunch of business units.
[01:27:18.100 --> 01:27:22.780]   There's a bunch of engineering teams kind of juggling resources.
[01:27:22.780 --> 01:27:27.500]   And I think a lot of people struggle just to convince companies that it's actually a
[01:27:27.500 --> 01:27:31.460]   priority to push out their machine learning effort.
[01:27:31.460 --> 01:27:36.340]   And so I think a lot of that's where I'll go back and plug that product management article
[01:27:36.340 --> 01:27:37.340]   that I wrote.
[01:27:37.340 --> 01:27:38.340]   I think it's really important.
[01:27:38.340 --> 01:27:40.780]   You need somebody who's your advocate.
[01:27:40.780 --> 01:27:46.160]   It could be your head of data science or VP of data, or it could be a product leader who's
[01:27:46.160 --> 01:27:47.820]   driving AI.
[01:27:47.820 --> 01:27:52.340]   But if you don't have a seat at the table, at the exec table that believes in this and
[01:27:52.340 --> 01:27:56.700]   is really supporting it and pushing it, a lot of these things will die on the vine.
[01:27:56.700 --> 01:27:57.700]   Interesting.
[01:27:57.700 --> 01:27:58.700]   Cool.
[01:27:58.700 --> 01:27:59.700]   Thank you so much.
[01:27:59.700 --> 01:28:00.700]   That was a lot of fun.
[01:28:00.700 --> 01:28:01.700]   Yeah, man.
[01:28:01.700 --> 01:28:02.700]   I like your garage.
[01:28:02.700 --> 01:28:02.700]   Yeah, thanks.
[01:28:02.900 --> 01:28:03.900]   Yeah, thanks.
[01:28:03.900 --> 01:28:13.900]   [MUSIC PLAYING]

