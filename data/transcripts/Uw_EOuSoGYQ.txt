
[00:00:00.000 --> 00:00:08.280]   The next thing I want to talk about is how to evaluate models from the model registry.
[00:00:08.280 --> 00:00:15.200]   As you might recall from the last lesson, my colleague tagged me in a report asking
[00:00:15.200 --> 00:00:17.680]   me to help peer review a model.
[00:00:17.680 --> 00:00:23.480]   And the way we do this with weights and biases is usually through via a model registry.
[00:00:23.480 --> 00:00:31.280]   And the model registry allows you to organize your models and version them.
[00:00:31.280 --> 00:00:37.080]   And it can be really useful for things like sharing models between colleagues, but also
[00:00:37.080 --> 00:00:44.520]   staging models for production and moving models through a general workflow or evaluation process
[00:00:44.520 --> 00:00:46.280]   that you might have on your team.
[00:00:46.280 --> 00:00:52.120]   And just to remind you what that looks like, let me go to the UI and show you what that
[00:00:52.120 --> 00:00:53.560]   looks like.
[00:00:53.560 --> 00:00:56.840]   And so this is the model registry, if you can remember.
[00:00:56.840 --> 00:01:03.800]   And what happened is, if you recall, my colleague logged this model to the registry, marked
[00:01:03.800 --> 00:01:07.320]   it as staging, and asked me to evaluate it.
[00:01:07.320 --> 00:01:09.240]   Now how would you do that?
[00:01:09.240 --> 00:01:17.840]   Now of course, you can click through these various tabs, for example, like metadata,
[00:01:17.840 --> 00:01:22.080]   and you can look at various metrics here, so on and so forth.
[00:01:22.080 --> 00:01:30.040]   However, the way that we recommend evaluating models is to create an evaluation run.
[00:01:30.040 --> 00:01:33.760]   And so that's what I'm going to discuss in this slide here.
[00:01:33.760 --> 00:01:39.400]   And the way you do that is first you create an evaluation run.
[00:01:39.400 --> 00:01:43.120]   And you do that with the same wandb.init method.
[00:01:43.120 --> 00:01:50.760]   However, what you do is you mark your job type specifically to be something like evaluation.
[00:01:50.760 --> 00:01:56.200]   And that just indicates that you are going to be creating a special kind of run so that
[00:01:56.200 --> 00:01:59.440]   you can filter it on the runs in the UI.
[00:01:59.440 --> 00:02:02.880]   And all this does is allow you to find that run.
[00:02:02.880 --> 00:02:06.640]   The next thing you want to do is you want to grab the artifact.
[00:02:06.640 --> 00:02:13.280]   So we're going to use the run object, and we're going to use the use artifact method,
[00:02:13.280 --> 00:02:14.800]   and we're going to get that artifact.
[00:02:14.800 --> 00:02:17.680]   Now you don't have to memorize this.
[00:02:17.680 --> 00:02:23.820]   And I want to point out if we go back to the model registry, you can click on usage.
[00:02:23.820 --> 00:02:28.440]   And this usage tab will give you the code that you can use to do exactly the same thing
[00:02:28.440 --> 00:02:30.480]   I'm showing you, which is very handy.
[00:02:30.480 --> 00:02:32.960]   I myself go to the UI.
[00:02:32.960 --> 00:02:36.000]   And in particular, I'm going to want to use staging.
[00:02:36.000 --> 00:02:38.520]   And so that's what we show here.
[00:02:38.520 --> 00:02:43.240]   What we show here is you can use the artifact.
[00:02:43.240 --> 00:02:47.160]   And then importantly, you can also see the lineage of the artifact.
[00:02:47.160 --> 00:02:49.920]   So you want to know where the artifact came from.
[00:02:49.920 --> 00:02:56.120]   In this case, this logged by method gives you the run that the artifact was generated
[00:02:56.120 --> 00:02:57.280]   by.
[00:02:57.280 --> 00:03:00.560]   And then what we do here, this is a convenience.
[00:03:00.560 --> 00:03:01.760]   This is for convenience.
[00:03:01.760 --> 00:03:07.920]   You don't have to do this, but this just propagates the config from the parent run or the producer
[00:03:07.920 --> 00:03:09.680]   run into the current run.
[00:03:09.680 --> 00:03:12.320]   So you can just more easily see it in the UI.
[00:03:12.320 --> 00:03:19.800]   This just helps populate all the configs from the run that produced the artifact.
[00:03:19.800 --> 00:03:26.240]   And then this code here is really fast AI, more or less fast AI specific code.
[00:03:26.240 --> 00:03:31.520]   Essentially, what you want to do is download the artifact and then load it.
[00:03:31.520 --> 00:03:33.720]   I removed a lot of the boilerplate code.
[00:03:33.720 --> 00:03:36.960]   So you're going to want to instantiate the model again.
[00:03:36.960 --> 00:03:43.000]   And you can look at the code in the GitHub repository in the lesson three folder.
[00:03:43.000 --> 00:03:46.000]   And the Python script is called eval.py.
[00:03:46.000 --> 00:03:51.160]   So this is a kind of more of a high level overview of the most important parts of the
[00:03:51.160 --> 00:03:59.400]   code with regards to how to get a model from the model registry and how to think about
[00:03:59.400 --> 00:04:00.400]   creating a run.
[00:04:00.400 --> 00:04:04.360]   So you're going to create a special kind of run where you're not going to train a model.
[00:04:04.360 --> 00:04:08.000]   All you're going to be doing is evaluating a model.
[00:04:08.000 --> 00:04:13.400]   And you're going to be uploading some metrics and uploading some charts and plots and other
[00:04:13.400 --> 00:04:14.960]   diagnostics.
[00:04:14.960 --> 00:04:19.400]   And that's a really convenient way to organize your model evaluation.
[00:04:19.400 --> 00:04:21.640]   And that's what we recommend.
[00:04:21.640 --> 00:04:26.520]   So the first thing you're going to want to do is you're going to want to make sure that
[00:04:26.520 --> 00:04:29.800]   your metrics are the same.
[00:04:29.800 --> 00:04:34.600]   So when you originally train the model, you logged your validation metrics.
[00:04:34.600 --> 00:04:39.520]   So the original run right here, I can actually show it in the UI.
[00:04:39.520 --> 00:04:40.520]   So let me open that.
[00:04:40.520 --> 00:04:47.880]   This is the original run in which we trained the final model that you want to evaluate.
[00:04:47.880 --> 00:04:52.760]   And you can see that this is a candidate model that we discussed before.
[00:04:52.760 --> 00:04:56.000]   And this model, again, this is where we logged all the metrics.
[00:04:56.000 --> 00:05:01.720]   The metrics to pay attention to are these final_IOU metrics.
[00:05:01.720 --> 00:05:04.800]   And these are the validation metrics.
[00:05:04.800 --> 00:05:11.200]   And what we want to do is after we download this model, we want to score it on the validation
[00:05:11.200 --> 00:05:12.200]   set again.
[00:05:12.200 --> 00:05:15.080]   And we want to make sure that we're getting the same metrics.
[00:05:15.080 --> 00:05:18.400]   And this is something I personally recommend that you do if you're using weights and biases
[00:05:18.400 --> 00:05:22.320]   just to check yourself, just to make sure you're not making any mistakes.
[00:05:22.320 --> 00:05:24.440]   You know, you can include this in the test.
[00:05:24.440 --> 00:05:26.080]   You don't have to do it manually.
[00:05:26.080 --> 00:05:29.520]   And I'm going to show you in the next slide, I'm going to give you some hints on how you
[00:05:29.520 --> 00:05:30.520]   can do this programmatically.
[00:05:30.520 --> 00:05:35.800]   But I just wanted to drive home the fact that you want to make sure the metrics are consistent.
[00:05:35.800 --> 00:05:41.740]   You want to be able to reproduce the results if you're doing an evaluation run.
[00:05:41.740 --> 00:05:48.000]   And this is just two screenshots with the same metrics from each of these runs.
[00:05:48.000 --> 00:05:49.980]   And you can see that the metrics match.
[00:05:49.980 --> 00:05:51.360]   So we're good here.
[00:05:51.360 --> 00:05:52.880]   And we can move forward.
[00:05:52.880 --> 00:05:55.520]   So how do you get validation metrics with the API?
[00:05:55.520 --> 00:06:01.240]   So you don't necessarily want to do this manually.
[00:06:01.240 --> 00:06:03.920]   You don't want to eyeball it.
[00:06:03.920 --> 00:06:08.040]   It's nice to automate everything you can.
[00:06:08.040 --> 00:06:10.280]   And you can do this programmatically.
[00:06:10.280 --> 00:06:15.720]   The way you do this programmatically is you can access metrics programmatically.
[00:06:15.720 --> 00:06:20.020]   So what you would do in this case is you would start the run.
[00:06:20.020 --> 00:06:27.040]   We already discussed how that might look, a run for an evaluation run.
[00:06:27.040 --> 00:06:30.960]   And you would retrieve the artifact.
[00:06:30.960 --> 00:06:34.480]   And there's this special method called logged by.
[00:06:34.480 --> 00:06:39.400]   And logged by gives you the producer run or the parent run that produced the artifact
[00:06:39.400 --> 00:06:42.560]   in the first place.
[00:06:42.560 --> 00:06:47.920]   And if you access the summary property on that run, you will get all of the summary
[00:06:47.920 --> 00:06:48.920]   metrics.
[00:06:48.920 --> 00:06:52.880]   So this is kind of a code sketch here.
[00:06:52.880 --> 00:07:00.160]   And you can see you can get the summary and kind of a dictionary of all the metrics.
[00:07:00.160 --> 00:07:05.560]   You can compare that to a fresh scoring that you do on the validation set and make sure
[00:07:05.560 --> 00:07:09.720]   that you get the same exact thing.
[00:07:09.720 --> 00:07:14.960]   I'm not going to show you the code exactly how to do that test.
[00:07:14.960 --> 00:07:18.200]   But I'm going to leave that to an exercise to the reader.
[00:07:18.200 --> 00:07:19.400]   But it's fairly straightforward.
[00:07:19.400 --> 00:07:24.360]   What you would do is you would compare this dictionary of metrics to another dictionary
[00:07:24.360 --> 00:07:29.720]   of metrics that you would use by scoring the model against the validation set again.
[00:07:29.720 --> 00:07:34.800]   And these screenshots here are just to remind you of what is happening here in this code.
[00:07:34.800 --> 00:07:41.920]   So again, you can go to the model artifact or the model registry page and click on the
[00:07:41.920 --> 00:07:50.240]   usage tab and you will get the code that you need to actually retrieve the model.
[00:07:50.240 --> 00:07:55.640]   And again, you can use this log by method to get the parent run of that.
[00:07:55.640 --> 00:08:01.640]   And this parent run, this aspect where you're getting the parent run, this is related to
[00:08:01.640 --> 00:08:03.800]   lineage.
[00:08:03.800 --> 00:08:07.020]   And this lineage can also be seen in the UI.
[00:08:07.020 --> 00:08:13.480]   If you click on the lineage tab, it will show you the lineage of this artifact where it
[00:08:13.480 --> 00:08:14.880]   was used.
[00:08:14.880 --> 00:08:22.680]   So this was used, this was created from a training run called Scarlet Armadillo.
[00:08:22.680 --> 00:08:27.200]   And then this artifact is used in all of these evaluation runs here.
[00:08:27.200 --> 00:08:32.640]   We've created quite a few evaluation runs and we're using that model for evaluation
[00:08:32.640 --> 00:08:33.640]   runs.
[00:08:33.640 --> 00:08:37.800]   And these are some runs that I've already done, for example.
[00:08:37.800 --> 00:08:42.520]   So this is a good way to check your understanding and kind of validate at least the first time
[00:08:42.520 --> 00:08:44.420]   you write this code yourself.
[00:08:44.420 --> 00:08:48.800]   You can check your intuition and make sure that you understand what is happening.
[00:08:48.800 --> 00:08:52.960]   I find it really helpful at least the first couple of times that I write this code to
[00:08:52.960 --> 00:08:56.160]   go to the UI and see if everything matches.
[00:08:56.160 --> 00:09:00.400]   And just to validate, give me more confidence that I'm doing everything correctly.
[00:09:00.400 --> 00:09:00.900]   [end]
[00:09:00.900 --> 00:09:01.900]   1
[00:09:01.900 --> 00:09:02.900]   2
[00:09:02.900 --> 00:09:03.900]   3
[00:09:03.900 --> 00:09:04.900]   4
[00:09:04.900 --> 00:09:05.900]   5
[00:09:05.900 --> 00:09:06.900]   6
[00:09:06.900 --> 00:09:07.900]   7
[00:09:07.900 --> 00:09:08.900]   8
[00:09:08.900 --> 00:09:09.900]   9
[00:09:09.900 --> 00:09:10.900]   10
[00:09:10.900 --> 00:09:11.900]   11

