
[00:00:00.000 --> 00:00:07.840]   Hello, everyone. Welcome to this live lesson of our training and fine tuning LLMs course.
[00:00:07.840 --> 00:00:13.500]   My name is Darek Kwecek. I'm a machine learning engineer at Weights & Biases, calling in from
[00:00:13.500 --> 00:00:19.200]   Warsaw in Poland. We'd love to learn where you're calling from, if you can say something
[00:00:19.200 --> 00:00:25.700]   about you in the chat on our events platform, or if you're watching us on the YouTube livestream,
[00:00:25.700 --> 00:00:31.720]   that would be very helpful. I'm super excited to be joined here on the stage by fantastic
[00:00:31.720 --> 00:00:39.560]   speakers. We have Weiwei Yang from Microsoft and Mark Zerufim from Meta, who are co-hosts
[00:00:39.560 --> 00:00:45.520]   of NURiP's LLM Efficiency Challenge. And they will tell us more about this challenge today.
[00:00:45.520 --> 00:00:49.580]   Super excited to learn more about this. I think this will be very relevant for all of
[00:00:49.580 --> 00:00:56.080]   you that are interested in fine tuning LLMs. And we're also joined by Ayush Thakur, a machine
[00:00:56.080 --> 00:01:02.160]   learning engineer at Weights & Biases. And Ayush will give you a very practical, hands-on
[00:01:02.160 --> 00:01:08.780]   help to get started on this competition, including a starter package with the code to get started.
[00:01:08.780 --> 00:01:12.660]   So also excited about that. So at the end of this lesson, you should be able to get
[00:01:12.660 --> 00:01:25.180]   started fine tuning your models, which I think is a good start of this course.
[00:01:25.180 --> 00:01:32.120]   So why are we excited about training and fine tuning LLMs? At Weights & Biases, we work
[00:01:32.120 --> 00:01:38.120]   with multiple customers. And this is one of the top questions that is always coming up.
[00:01:38.120 --> 00:01:43.120]   Everyone seems to be interested in LLMs. People are asking, how do we train, how do we fine
[00:01:43.120 --> 00:01:50.280]   tune these models efficiently? And this experience that we have is also backed by data. What
[00:01:50.280 --> 00:01:55.920]   you can see on this chart is the number of archive papers that contain large language
[00:01:55.920 --> 00:02:00.920]   model keywords. And you can see it's growing exponentially. And given this exponential
[00:02:00.920 --> 00:02:07.680]   growth, that can mean two things. Either it's a bubble, or it's actually a groundbreaking
[00:02:07.680 --> 00:02:15.720]   technology that is making a lot of new use cases possible. And my personal assessment,
[00:02:15.720 --> 00:02:23.080]   based on my experience, is this is the latter. We can see a lot of new applications being
[00:02:23.080 --> 00:02:29.000]   enabled by this technology. Myself, personally, I've been using LLMs to assist me with writing,
[00:02:29.000 --> 00:02:36.440]   to assist me with coding, to edit articles that I write. I've used LLMs to extract structured
[00:02:36.440 --> 00:02:41.520]   data from unstructured documents, a task that would take me weeks before this technology
[00:02:41.520 --> 00:02:46.960]   became available. And now, something that we can do in a couple of hours. We've been
[00:02:46.960 --> 00:02:52.440]   using this for education. I've recently heard one of the people that is behind some of the
[00:02:52.440 --> 00:02:57.840]   popular fine-tuned LLMs, the open source LLMs, has learned how to do it from GPT-4, which
[00:02:57.840 --> 00:03:03.880]   I think is a huge evidence that these tools are also helpful in education. At Weights
[00:03:03.880 --> 00:03:10.040]   and Biases, we've been using LLMs to help with customer support. We have a bot on our
[00:03:10.040 --> 00:03:14.840]   Discord server that helps users solve some of the technical problems they might be facing
[00:03:14.840 --> 00:03:19.680]   when it comes to Weights and Biases. We've been using it to automate translation of our
[00:03:19.680 --> 00:03:25.800]   docs to different languages, making the product also more accessible internationally. So,
[00:03:25.800 --> 00:03:31.600]   this set of applications makes this really exciting, and that brings a lot of people
[00:03:31.600 --> 00:03:42.240]   into this field. However, this field is not entirely new. For those of you that have been
[00:03:42.240 --> 00:03:49.000]   in machine learning, specifically in NLP, we've had language models applied effectively
[00:03:49.000 --> 00:03:54.400]   to solve different use cases for a couple of years. It started a couple of years ago
[00:03:54.400 --> 00:04:03.840]   with ULMfit that achieved state-of-the-art results on sentiment classifications on IMDB.
[00:04:03.840 --> 00:04:09.880]   And then we had a huge explosion of encoder models, and specifically transformers with
[00:04:09.880 --> 00:04:18.480]   BERT, that started to solve a lot of different NLP tasks very successfully. But the focus
[00:04:18.480 --> 00:04:24.360]   of this course is going to be on the right-hand side of this evolution tree of LLMs, and that
[00:04:24.360 --> 00:04:29.960]   these are the decoder-only models, the autoregressive language models that are trained to predict
[00:04:29.960 --> 00:04:36.880]   the next word in a sequence, which started with the GPT family. They became really powerful.
[00:04:36.880 --> 00:04:42.920]   We started scaling them. We've seen success of ChatGPT, GPT4, and more recently, a lot
[00:04:42.920 --> 00:04:50.680]   of other commercial models, including Palm and BERT and Cloud, Cohere Command, as well
[00:04:50.680 --> 00:04:56.880]   as open-source models. We're all super excited about LLAMA, but also other open-source models,
[00:04:56.880 --> 00:05:02.480]   like Bloom or OPT. They make a lot of use cases possible. And this is going to be our
[00:05:02.480 --> 00:05:09.200]   focus. This is where we see a lot of benefits from the scale. These models are able to solve
[00:05:09.200 --> 00:05:16.840]   a lot of tasks in a very general way. So given this popularity and given this power and the
[00:05:16.840 --> 00:05:21.840]   number of applications of large language models, a lot of you might want to get started with
[00:05:21.840 --> 00:05:29.640]   LLMs. And probably the best way to get started with LLMs is by using them via APIs. And you
[00:05:29.640 --> 00:05:38.600]   might be familiar with the open AIs provided by OpenAI, provided by Anthropic, provided
[00:05:38.600 --> 00:05:46.360]   by Cohere. In fact, we recently released a course that teaches you to use these APIs
[00:05:46.360 --> 00:05:51.640]   to solve different business problems and to automate different tasks. So this course might
[00:05:51.640 --> 00:05:58.400]   be a good entry point into the world of LLMs. The benefits of using LLMs through APIs is
[00:05:58.400 --> 00:06:05.880]   it's easy to get started. You don't need to do any or much of an upfront investment, either
[00:06:05.880 --> 00:06:11.800]   in training, in compute, in data and capability. You can access really well-performing large
[00:06:11.800 --> 00:06:20.360]   language models very quickly. On the negative side, whenever you get into production and
[00:06:20.360 --> 00:06:26.600]   you get a large volume of requests in production, your inference costs may be substantial. So
[00:06:26.600 --> 00:06:31.640]   you need to prepare for that. You may need to share data externally. And for some use
[00:06:31.640 --> 00:06:38.120]   cases for some organizations, that may be something they'd rather avoid. You're also
[00:06:38.120 --> 00:06:49.800]   relying on external technology. And sometimes different companies may want to avoid that.
[00:06:49.800 --> 00:06:57.400]   Let me take a quick break. I think we have a question to share the slides in the chat.
[00:06:57.400 --> 00:07:07.720]   I'm going to try to quickly find the slides link and share it. And I'm going to fix this
[00:07:07.720 --> 00:07:28.120]   and come back to the presentation. Okay. I apologize for that. The link to the slides
[00:07:28.120 --> 00:07:39.480]   will be shared in the YouTube live stream, the chat. Yeah. So we talked about the cons,
[00:07:39.480 --> 00:07:46.280]   the negative aspects of using LLMs through an API. Another one is lack of flexibility.
[00:07:46.280 --> 00:07:50.600]   For example, in many use cases, you may want to implement inference on the edge. And again,
[00:07:50.600 --> 00:07:56.440]   that may be hard if you're using these LLMs behind APIs. So if those negatives outweigh
[00:07:56.440 --> 00:08:04.040]   the positives of LLMs, you may want to get into this second part of the solution, which
[00:08:04.040 --> 00:08:10.920]   is either fine tuning an open source LLM or pre-training your own LLM. When do you want
[00:08:10.920 --> 00:08:15.920]   to fine tune? If you want to benefit from this external investment that is done by companies
[00:08:15.920 --> 00:08:22.440]   that release this open source LLMs. If you want to take control over the roadmap, if
[00:08:22.440 --> 00:08:30.120]   you want to own model weights, that may give you some control over the future of the products
[00:08:30.120 --> 00:08:35.080]   that you're building on top of LLMs. It becomes more difficult. You need to get the right
[00:08:35.080 --> 00:08:39.720]   skills. You need to do the right investment. And hopefully this course will provide you
[00:08:39.720 --> 00:08:47.040]   with some of the skills that are needed to fine tune or train your own LLMs. Pre-training
[00:08:47.040 --> 00:08:52.680]   LLMs is even more expensive than fine tuning, but there are many use cases where it might
[00:08:52.680 --> 00:08:56.840]   be the right choice. You may want to do it for competitive advantage. You may want to
[00:08:56.840 --> 00:09:02.080]   have full control over your roadmap, over the future direction, over the training data,
[00:09:02.080 --> 00:09:06.800]   but you need to be ready to significantly invest in terms of the talent, in terms of
[00:09:06.800 --> 00:09:13.560]   compute resources, and also be prepared to face some risks because this is not always
[00:09:13.560 --> 00:09:24.480]   easy. So what are the types of fine tuning? The first type of fine tuning I want to talk
[00:09:24.480 --> 00:09:31.200]   about is fine tuning for specific tasks. And given that we are mainly focusing on autoregressive
[00:09:31.200 --> 00:09:35.920]   language models, which are trained to predict the next word in a sequence, if you train
[00:09:35.920 --> 00:09:43.040]   a base language model, they will simply learn to continue a given text. But we may want
[00:09:43.040 --> 00:09:50.920]   to fine tune them, to teach them to solve specific tasks. And that thing we can achieve
[00:09:50.920 --> 00:09:58.400]   via fine tuning. One example of this is the T5 model that was trained to solve different
[00:09:58.400 --> 00:10:04.800]   tasks where the task is defined as some sort of text, and then the answer is a solution
[00:10:04.800 --> 00:10:10.680]   to this task. I think some of the tasks that are going to be used for the evaluation of
[00:10:10.680 --> 00:10:14.480]   the challenge that we'll talk later today may be framed in this way. So it's a good
[00:10:14.480 --> 00:10:21.000]   idea to look at this different type of tasks in the way of phrasing them in text, and a
[00:10:21.000 --> 00:10:29.640]   different way of using prompts to then guide the model to solve specific tasks. This can
[00:10:29.640 --> 00:10:34.800]   be made very specific. This can be made specific for your application. But there are also more
[00:10:34.800 --> 00:10:39.840]   general ways of fine tuning language models. And they have been really made popular by
[00:10:39.840 --> 00:10:48.360]   OpenAI, starting with the instruction tuning approach. And then in the instruction tuning
[00:10:48.360 --> 00:10:53.400]   approach, we teach a model to follow instructions. We may not necessarily want it to continue
[00:10:53.400 --> 00:10:58.040]   the text that we start with. We may prefer it to give it some instruction, and then get
[00:10:58.040 --> 00:11:03.200]   a response that follows that instruction in return. And the first step to achieve it is
[00:11:03.200 --> 00:11:08.640]   supervised fine tuning. We provide the model with a set of instructions and responses,
[00:11:08.640 --> 00:11:15.000]   and then train a model to behave in a similar way. Another step that is often used on top
[00:11:15.000 --> 00:11:20.480]   of instruction fine tuning and the supervised step is reinforcement learning from human
[00:11:20.480 --> 00:11:25.880]   feedback. And that, in turn, also consists of two steps. One step is training a reward
[00:11:25.880 --> 00:11:33.160]   model, where we teach a model what humans prefer. And then we use that reward model
[00:11:33.160 --> 00:11:41.400]   to fine tune our language model via reinforcement learning to be aligned with human preferences.
[00:11:41.400 --> 00:11:50.960]   And we'll do a more deep dive into these techniques in later parts of this course.
[00:11:50.960 --> 00:11:56.720]   One thing that I want to share is also this boundary between pre-training and fine tuning
[00:11:56.720 --> 00:12:04.440]   is not always clear. We don't know much about GPT-4 or ChatGPT, but the one thing that was
[00:12:04.440 --> 00:12:10.800]   shared by OpenAI is actually these models were trained over a series of steps. ChatGPT
[00:12:10.800 --> 00:12:18.000]   started as a code model. It was trained on codes to help with programming. And then it
[00:12:18.000 --> 00:12:24.280]   was further fine tuned, first to follow instructions. Then it was aligned with human preferences
[00:12:24.280 --> 00:12:31.880]   using RLHF. It was then further fine tuned on chat data. And all of the steps resulted
[00:12:31.880 --> 00:12:39.480]   in GPT-3.5, which actually, I think, since very recently, we can also fine tune. So depending
[00:12:39.480 --> 00:12:44.400]   on which step, which type of checkpoint you're using, you might be actually fine tuning a
[00:12:44.400 --> 00:12:50.760]   model that already had been fine tuned for a different task.
[00:12:50.760 --> 00:12:56.440]   Now if you want to learn how to do this, I think the best way to do it is by starting
[00:12:56.440 --> 00:13:01.880]   to build. And especially with the release of some of the very powerful open source language
[00:13:01.880 --> 00:13:07.760]   models like Lama from Meta that we were very thankful for. We see a lot of activity, a
[00:13:07.760 --> 00:13:15.360]   lot of people take these models, use different types of data sets to fine tune these open
[00:13:15.360 --> 00:13:19.000]   source language models. They fine tune it on different languages. They fine tune it
[00:13:19.000 --> 00:13:25.640]   on different tasks. Some of them use synthetic data. And this is a process that actually
[00:13:25.640 --> 00:13:30.960]   has become possible very recently. And hopefully in this course, we'll also show you how you
[00:13:30.960 --> 00:13:37.400]   can do this. Actually, for this fine tuning stage, you don't need that much compute. And
[00:13:37.400 --> 00:13:42.520]   this has really become accessible to a broader range of audience. But you do need several
[00:13:42.520 --> 00:13:48.040]   things. And this is something that we'll be focusing on in this course. The first step
[00:13:48.040 --> 00:13:54.080]   that we recommend to start with is trying to figure out what is your goal for fine tuning.
[00:13:54.080 --> 00:13:59.320]   And to do this, you need to figure out the evaluation. How will you tell that your model
[00:13:59.320 --> 00:14:04.720]   is actually progressing and is learning, is improving? For that, you need to decide on
[00:14:04.720 --> 00:14:11.800]   an evaluation approach. We'll talk more about evaluation next week. I think the team today
[00:14:11.800 --> 00:14:17.360]   will also talk about how they're going to be evaluating solutions in the LLM efficiency
[00:14:17.360 --> 00:14:24.240]   challenge. We will not go into the in-depth on the model architecture choices. That is
[00:14:24.240 --> 00:14:29.360]   probably a topic for a different course. But once you understand your evaluation, once
[00:14:29.360 --> 00:14:33.680]   you understand your business goals and the metrics, you might check different benchmarks,
[00:14:33.680 --> 00:14:37.920]   different leaderboards that are available, and pick a foundation model that is suitable
[00:14:37.920 --> 00:14:43.040]   for your task. So hopefully, understanding evaluation will also help you pick the right
[00:14:43.040 --> 00:14:49.360]   foundation model that you may want to fine tune. And then once you've done it, the hard
[00:14:49.360 --> 00:14:55.800]   part is figuring out the right data mix to fine tune your model on, and then using efficient
[00:14:55.800 --> 00:15:00.640]   training and fine tuning techniques. And this keyword of efficiency is especially important
[00:15:00.640 --> 00:15:07.040]   because we're dealing with very large models, and we need to use the right techniques to
[00:15:07.040 --> 00:15:15.520]   make them fit on a GPU and train efficiently. And I am super excited that we're partnering
[00:15:15.520 --> 00:15:22.280]   with Mosaic ML, and specifically Jonathan Frankel, who is chief scientist at Mosaic ML,
[00:15:22.280 --> 00:15:29.280]   who has developed the lessons on data and on efficient training techniques of large
[00:15:29.280 --> 00:15:34.680]   language models. And this will be the two out of the four lessons in this course. So
[00:15:34.680 --> 00:15:39.720]   I'm personally excited to be learning from Jonathan. Mosaic ML is training and fine tuning
[00:15:39.720 --> 00:15:48.280]   LLMs as a business. They have huge expertise in this field, and I'm sure everyone can benefit
[00:15:48.280 --> 00:15:56.800]   from these lessons. Okay, we have a question from the chat. The question is, "What kind
[00:15:56.800 --> 00:16:04.560]   of architecture will be used in the course?" And actually, we are not prescribing the architecture.
[00:16:04.560 --> 00:16:12.000]   I think, actually, in the later part today, Ayush will provide a comparison of two different
[00:16:12.000 --> 00:16:17.480]   architectures, two different foundation models that he used for this task. But we will be
[00:16:17.480 --> 00:16:27.040]   focusing primarily on the autoregressive decoder-only language models. That's an area that is currently
[00:16:27.040 --> 00:16:32.600]   being explored. That's the area that is, I would say, scaling most rapidly into larger
[00:16:32.600 --> 00:16:39.800]   languages. And we also feel like some of the older language models that had the decoder-only
[00:16:39.800 --> 00:16:46.480]   architecture, like BERT, the BERTA, the community already knows how to fine tune them. There
[00:16:46.480 --> 00:16:51.960]   are good frameworks like Hugging Face and PyTorch Lightning. So we feel like there is
[00:16:51.960 --> 00:16:59.960]   a less demand for knowledge on the encoder models than on the decoder-only models. Okay,
[00:16:59.960 --> 00:17:07.600]   so I think we'll come back to questions later on, but I want to make sure we hand over to
[00:17:07.600 --> 00:17:15.760]   the hosts of NURiP's LLM Efficiency Challenge. And I'd like to now hand over to Weiwei Yang
[00:17:15.760 --> 00:17:21.240]   and Mark Sarufim, who are co-hosting, co-organizing this challenge. The reason we are excited
[00:17:21.240 --> 00:17:27.000]   about this is we believe the best way to learn is by actually doing, by building, and this
[00:17:27.000 --> 00:17:31.360]   challenge provides a way to do it in a very accessible way, because they set the constraints
[00:17:31.360 --> 00:17:39.800]   that make it available to a broad range of engineers. So over to you, Weiwei.
[00:17:39.800 --> 00:17:44.240]   Thank you very much, Derek, for that introduction. And maybe if you stop sharing, I can share
[00:17:44.240 --> 00:17:55.280]   my presentation. Hi, can everyone see my presentation? Or maybe to the extent that there's the folks
[00:17:55.280 --> 00:18:00.440]   in the room. So thank you very much for having us, and we're very, very excited to come here
[00:18:00.440 --> 00:18:08.080]   and to talk about our competition. And as Derek put out that all the needs, talk about
[00:18:08.080 --> 00:18:13.960]   all the needs to fine-tuning, what it comes down to is that as powerful as all these large
[00:18:13.960 --> 00:18:20.360]   language models are, pretty much no model will satisfy everything that you need to do
[00:18:20.360 --> 00:18:25.400]   100% of the time. So you will need to do some adaptations, whether it's because support
[00:18:25.400 --> 00:18:30.680]   privacy, security concerns, or because you have other type of data and modalities that
[00:18:30.680 --> 00:18:37.480]   you want to get into, like, and use the power of this large language model. So we conceptualized
[00:18:37.480 --> 00:18:44.320]   this competition partially because of this need to fine-tune, and also because there's
[00:18:44.320 --> 00:18:52.160]   very few resources out there to teach the community how to fine-tune from model selections
[00:18:52.160 --> 00:18:58.000]   to all the steps and the well-documented. And so we're hoping to use this to not only
[00:18:58.000 --> 00:19:06.060]   create sort of step-by-step ways of how to approach this, but also making it reproducible.
[00:19:06.060 --> 00:19:13.840]   So for part of this competition, as the organizers, we are, we're actually assigned up to verify
[00:19:13.840 --> 00:19:19.400]   the winning submissions to make sure that we could produce the same results. So just
[00:19:19.400 --> 00:19:26.120]   a little bit about the competition. This is kind of like the rules. We limited the competition
[00:19:26.120 --> 00:19:32.440]   to run on just one single GPU. The reason is that these large language models will use
[00:19:32.440 --> 00:19:38.080]   up any resources you're willing to put into it. So we have to put some constraints and
[00:19:38.080 --> 00:19:44.080]   also with one single GPU, make it all practical for most folks' applications. We also limit
[00:19:44.080 --> 00:19:49.440]   the time to one day. So all the fine-tunes has to be done with that one GPU within the
[00:19:49.440 --> 00:19:55.560]   day. And we have two tracks. One is A100 with 40 gigs of RAM. This is maybe one of the most
[00:19:55.560 --> 00:20:01.760]   popular machine learning hardware out there. And most of these, most of these large language
[00:20:01.760 --> 00:20:10.200]   models are trained on a cluster of these A100s. And then the 1490 is very popular with gamers
[00:20:10.200 --> 00:20:16.720]   and now has gained a lot of popularity with machine learning communities. It has less
[00:20:16.720 --> 00:20:24.160]   RAM, but it's still workable. We also have a set of pre-selected models to choose from.
[00:20:24.160 --> 00:20:31.740]   The reason that we go with a list of pre-approved model is because we want the communities to
[00:20:31.740 --> 00:20:40.000]   have a good starting point and that they can't reproduce every steps as the winning entry.
[00:20:40.000 --> 00:20:46.360]   And Sebastian from Lightning AI had helpfully mapped out the collection of models that we
[00:20:46.360 --> 00:20:52.440]   choose and some of them are encoder models. As I mentioned that encoder models randomly
[00:20:52.440 --> 00:20:58.080]   mask a percentage of the text and you're trying to predict what's masked. So it's bidirectional
[00:20:58.080 --> 00:21:04.640]   and for tasks like classification, sentiment analysis, those are like the models that tend
[00:21:04.640 --> 00:21:08.920]   to perform really well. On the other side, we have the decoder models, which predicts
[00:21:08.920 --> 00:21:14.680]   the next token. So for text generation type of things, and as we know, Lama and the chat
[00:21:14.680 --> 00:21:19.640]   GPT are based on that. Then you have encoder decoder, which has an encoder and the decoder
[00:21:19.640 --> 00:21:24.440]   stuck together. So the original attention zone you need to, this is like the original
[00:21:24.440 --> 00:21:29.840]   like architecture and it's a sequence to sequence. So it's good for machine translation type
[00:21:29.840 --> 00:21:37.760]   of tasks and also summarization or question and answer. So what models you choose is,
[00:21:37.760 --> 00:21:43.480]   what base model you choose is heavily dependent on what task you need to do. But also we're
[00:21:43.480 --> 00:21:50.000]   hoping through this competition, people will explore like all these models and we're curious
[00:21:50.000 --> 00:21:57.920]   which one of them will win out. We allow like most of the data set with a few caveats specifically,
[00:21:57.920 --> 00:22:05.160]   we do not allow LLM generated data set. Mark and I are happy to talk more about it in the
[00:22:05.160 --> 00:22:11.560]   question answer phase, why we put that rules in. And we also wanted the data to be open
[00:22:11.560 --> 00:22:17.560]   sourced at the time of the mission. Again, this is done with the hope to be reproducible
[00:22:17.560 --> 00:22:23.760]   so that everyone else can use this data set. So on the right side, here's a few examples
[00:22:23.760 --> 00:22:30.760]   of the data that you can use.
[00:22:30.760 --> 00:22:39.440]   So for evaluations, we decided to choose this framework called HELM. HELM is a comprehensive
[00:22:39.440 --> 00:22:45.680]   benchmark that's put out by Stanford. One of the problems that today was this large
[00:22:45.680 --> 00:22:52.040]   language model is actually how do you evaluate it? So if, for example, you go to Harkin Faces
[00:22:52.040 --> 00:22:58.200]   model leaderboard, you get a score and you have four tasks. But what does it really mean
[00:22:58.200 --> 00:23:08.560]   that it has like 87% untruthful QA? It maybe means that it answered 85% of the evaluation
[00:23:08.560 --> 00:23:14.560]   task correctly. But what about the robustness if you just permute some of the tokens? So
[00:23:14.560 --> 00:23:20.560]   if it answers truthfully, is earth flat? No. But what if you permute the tokens? Say if
[00:23:20.560 --> 00:23:26.680]   a white male says earth is flat, would the large language model decided to answer that?
[00:23:26.680 --> 00:23:32.880]   Yes. So those are the type of things that are very few. There are very few studies out
[00:23:32.880 --> 00:23:40.280]   of there. And in order to really understand about how your language model will perform
[00:23:40.280 --> 00:23:45.160]   in the task and also perform properly, we need to take a more comprehensive view about
[00:23:45.160 --> 00:23:50.920]   the evaluations. So accuracy is the one aspect that people have looked into. But we also
[00:23:50.920 --> 00:23:57.400]   decided to look into calibration, robustness, fairness, bias, toxicity, and efficiency.
[00:23:57.400 --> 00:24:04.760]   So those are the metrics that's already well-defined in HELM benchmark. For those who are interested,
[00:24:04.760 --> 00:24:10.360]   you can go visit the website. This deck will be shared out with all the appropriate links.
[00:24:10.360 --> 00:24:17.480]   Specifically, like for the open evaluation phase, we have to choose a few scenarios or
[00:24:17.480 --> 00:24:23.240]   tasks. And these are actually very well-known machine learning tasks, though, like Big Bench,
[00:24:23.240 --> 00:24:29.440]   multiple choice, truthful QA, like CNN Daily Mail summarization, grade school math, and
[00:24:29.440 --> 00:24:35.920]   barbecue, et cetera. We also reserved a closed data set that we have not told the community
[00:24:35.920 --> 00:24:41.400]   what we're going to evaluate on. The reason to do this is, of course, we approve vending,
[00:24:41.400 --> 00:24:46.200]   like winning entry to be a simple memorization. We don't think that's going to be happening,
[00:24:46.200 --> 00:24:51.480]   but still, it's good to set something aside. The closed data set will be very similar to
[00:24:51.480 --> 00:24:58.320]   the open data set with the same type of tasks. And we will announce those after the submission
[00:24:58.320 --> 00:25:02.200]   is closed.
[00:25:02.200 --> 00:25:08.960]   Because we have two hardware tracks, we actually have two tracks of awards as well. So for
[00:25:08.960 --> 00:25:16.360]   each of the hardware category, we have a first prize winner of $5,000, second prize $2,500,
[00:25:16.360 --> 00:25:23.480]   and the third prize $1,000, with two special student prize for the team that's outside
[00:25:23.480 --> 00:25:30.440]   of the three top prizes that consists of purely student teams. So like the next two teams,
[00:25:30.440 --> 00:25:37.880]   two pure student teams, that scores the highest. And for the first place winners, we invite
[00:25:37.880 --> 00:25:46.020]   them to have a speaking chance at our in-person NeurALP workshop in December 2023, and a chance
[00:25:46.020 --> 00:25:52.440]   to co-author this report on the paper that we're going to write about this competition
[00:25:52.440 --> 00:25:56.020]   in the next NeurALP.
[00:25:56.020 --> 00:26:01.160]   So I guess now I talked about the structures of the competition. I'm just going to touch
[00:26:01.160 --> 00:26:08.560]   a little bit on the components that we'll take in order to make a successful entry.
[00:26:08.560 --> 00:26:16.720]   So first of all, we are putting a large language model into one GPU. Simply, most large language
[00:26:16.720 --> 00:26:22.920]   model you download today will have trouble fitting that. Why? If you think about it,
[00:26:22.920 --> 00:26:29.560]   if you have one parameter and you represent it as a 32-bit float, or like four bytes,
[00:26:29.560 --> 00:26:34.680]   if you have a billion of those parameters, that's four gigabytes, and that's just for
[00:26:34.680 --> 00:26:41.600]   weights alone. Now if you need to fine tune or training, you have other things. So at
[00:26:41.600 --> 00:26:46.040]   eight bits per parameter, for example, for your atom optimizer, four bits for like a
[00:26:46.040 --> 00:26:53.880]   gradient and up to eight bits for like activations and temporary memories, it really adds up.
[00:26:53.880 --> 00:26:59.640]   So how do you actually make this fit? So this is like a technique called quantization. And
[00:26:59.640 --> 00:27:03.960]   the simplest way you can approach it is just lop off some of the positions. So you can
[00:27:03.960 --> 00:27:09.360]   go like instead of 32, you can do 16 bits, or you can go even further to eight bits,
[00:27:09.360 --> 00:27:17.960]   or use some interesting, more nuanced floating point representation. So for example, Google's
[00:27:17.960 --> 00:27:23.800]   like BFLOAT16, which changes the number of bits for mantissa and exponent as opposed
[00:27:23.800 --> 00:27:29.120]   to the standard IEEE. So these are some of the ways, and there's many, many other ways
[00:27:29.120 --> 00:27:33.880]   that you could explore. And this is just kind of a teaser on quantization.
[00:27:33.880 --> 00:27:41.440]   And the next thing is, OK, I have my model in my hardware. How do I actually tune it?
[00:27:41.440 --> 00:27:47.600]   If I have 70 billion parameters, do I really need to change all that 70 billion parameters
[00:27:47.600 --> 00:27:56.520]   to make it performant? So this whole category of what we call parameter efficient fine tuning,
[00:27:56.520 --> 00:28:04.160]   you can think about essentially how to tune as little weight as possible to get it as
[00:28:04.160 --> 00:28:10.840]   performant as possible. You could selectively freeze certain layers in your network, or
[00:28:10.840 --> 00:28:16.880]   you could adapt to several techniques. One of the popular technique is actually prompt
[00:28:16.880 --> 00:28:21.640]   based, which is similar to like prompt engineering. You can think about prompt engineering as
[00:28:21.640 --> 00:28:27.360]   hard prompting, where you change the tokens in your prompt. This is a more soft prompting
[00:28:27.360 --> 00:28:33.920]   technique where you argument or you add extra things to your existing tokens to make it
[00:28:33.920 --> 00:28:37.760]   performant. So for example, prompt tunings, you add the
[00:28:37.760 --> 00:28:43.160]   trainable parameters after each prompt, and you train those parameters in the input representation
[00:28:43.160 --> 00:28:49.760]   layer. Prefix tuning is another way that you add these like a prefix in each layer of your
[00:28:49.760 --> 00:28:56.880]   model, and you train that with a separate forward network. P tuning is related to like
[00:28:56.880 --> 00:29:02.840]   a prefix tuning, except instead of putting in the beginning of each layer, you can like
[00:29:02.840 --> 00:29:11.880]   have that interfix into like any part of it and use like sort of a more LSTM based encoders
[00:29:11.880 --> 00:29:19.560]   to like do your prompt tunings and have other specialized tokens. So this is the one category,
[00:29:19.560 --> 00:29:24.400]   and for those who are interested, there's reference associated with each of these techniques,
[00:29:24.400 --> 00:29:29.080]   and the Hugging Face actually includes, I believe, all three of these techniques in
[00:29:29.080 --> 00:29:37.760]   their like library as well. The next category is adapter, and adapter is
[00:29:37.760 --> 00:29:44.920]   similar to prompt tuning. Instead of like just tuning the prompt, you actually add adapter
[00:29:44.920 --> 00:29:53.840]   layers into standard transformer architectures. So you could see it here like, for example,
[00:29:53.840 --> 00:30:00.560]   two adapter layers are now added between the forward and the layer norms.
[00:30:00.560 --> 00:30:05.560]   The next popular technique is lower or low rank approximations. You can think about the
[00:30:05.560 --> 00:30:12.600]   fine tuning as iteratively update your weight. So if you have a W, that's your weight matrix
[00:30:12.600 --> 00:30:18.560]   for your model, then you can think about fine tuning as keeping the original W matrix and
[00:30:18.560 --> 00:30:24.680]   then adding a delta W, and that's the change of your weight. So fine tuning is just really
[00:30:24.680 --> 00:30:31.760]   the delta W that's added to the original weight. So now you can also decompose your delta W
[00:30:31.760 --> 00:30:38.960]   using like matrix factorizations into like two, maybe a tall skinny and the short fat
[00:30:38.960 --> 00:30:43.880]   matrix that when you multiply them together has the same shape of the original weight
[00:30:43.880 --> 00:30:53.000]   W like matrix, but has a lot of less entries in this sense, you really save memory space.
[00:30:53.000 --> 00:30:59.400]   So that's another very popular technique. And then you can sort of extend on LoRa, use
[00:30:59.400 --> 00:31:07.800]   a quantized LoRa technique. So using LoRa on quantized models to do that. So these are
[00:31:07.800 --> 00:31:14.760]   just some of the sort of quick highlights of well-known parameter efficient fine tuning
[00:31:14.760 --> 00:31:19.720]   techniques. There's many, many out there and we are very, very excited to actually see
[00:31:19.720 --> 00:31:25.360]   new techniques. Once we have the model fit into a machine, we know how to like fine tune
[00:31:25.360 --> 00:31:33.080]   it. We actually have to curate data. So I think Derek talked a little bit like, touched
[00:31:33.080 --> 00:31:39.800]   a little bit about what it means to curate data. You can simply put a data set, you know,
[00:31:39.800 --> 00:31:46.640]   two plus three equal to five as input into your LLM, but that's probably not going to
[00:31:46.640 --> 00:31:53.360]   go very far unless all you want to do is just repeat two plus three equal to five. So instruction
[00:31:53.360 --> 00:32:00.720]   tuning is trying to come up with instruction set for specific data. So instead of like
[00:32:00.720 --> 00:32:05.880]   this sentence is translated into this sentence in Spanish, but the instructions really translated
[00:32:05.880 --> 00:32:12.360]   this to Spanish. And same thing that for a question and answer, a truthful thing is based
[00:32:12.360 --> 00:32:17.960]   on the presented facts. Does this support, can you draw this conclusion? Is this true
[00:32:17.960 --> 00:32:26.160]   or false? And then we talked like, Derek already talked a bit about using what we call the
[00:32:26.160 --> 00:32:32.600]   human in the loop reinforcement learning both use human labelers to generate original instruction
[00:32:32.600 --> 00:32:38.520]   set and then using human labelers to build a scoring like models to generate data, to
[00:32:38.520 --> 00:32:45.240]   build a scoring models and then use that as a critic or a teacher to help the thing learn.
[00:32:45.240 --> 00:32:52.640]   And of course, you can also do all of that with another language model. And this is what
[00:32:52.640 --> 00:32:59.480]   is very popular today called self-instruction. And if you hear about Orca style data set
[00:32:59.480 --> 00:33:08.040]   that's actually generated using Chats GPT as one of that. And for this competition,
[00:33:08.040 --> 00:33:16.360]   we are not to allow any like LLM or like self-instructed data set. And Mark and I happy to answer questions
[00:33:16.360 --> 00:33:26.200]   by not in the Q&A section. So I talked a bit about all the components that makes up to
[00:33:26.200 --> 00:33:32.320]   this competition. And we actually don't know what the winning entry will look like. We
[00:33:32.320 --> 00:33:37.720]   think that it will take a unique blend of quantization, model efficient training, some
[00:33:37.720 --> 00:33:43.520]   data curation along with some really creative training regime. And we are really, really
[00:33:43.520 --> 00:33:49.920]   like curious to find out what will actually win. So with that, maybe Mark, would you like
[00:33:49.920 --> 00:33:57.440]   to unmute and see if we have any questions from the audience?
[00:33:57.440 --> 00:34:06.800]   Sure thing. Yeah. So I think the first thing is I shared in the chat now like a link to
[00:34:06.800 --> 00:34:11.200]   our Discord group. So, you know, please, please join it. I think at this point we have like
[00:34:11.200 --> 00:34:16.920]   about 500 different people sharing like different tricks that they're using for fine tuning.
[00:34:16.920 --> 00:34:22.200]   It's I think it's gonna be a really valuable resource, especially that we actually also
[00:34:22.200 --> 00:34:25.800]   have been collaborating with the Lightning team on having a Discord bot that can help
[00:34:25.800 --> 00:34:30.600]   you do like eval. So this should be out like sometime like very soon, like this week or
[00:34:30.600 --> 00:34:34.440]   next. So it's gonna be like an invaluable resource for you to just like learn about
[00:34:34.440 --> 00:34:38.400]   fine tuning. And if you've never done it before, like I think a competition is sort of like
[00:34:38.400 --> 00:34:44.560]   a great excuse to do it. I sort of want to like, you know, I echo really what what we
[00:34:44.560 --> 00:34:49.160]   were saying, which is like, there's sort of like obviously like different classes of techniques
[00:34:49.160 --> 00:35:05.640]   which we suspect will work. Oh, can people hear me? Oh, I see. Yeah. So there's obviously
[00:35:05.640 --> 00:35:10.240]   like different classes of techniques that will be like probably very helpful, right?
[00:35:10.240 --> 00:35:16.160]   So things like, like Laura style, like quantization, you know, maybe you have like a faster runtime,
[00:35:16.160 --> 00:35:20.320]   like Llama CPP style, and you can go through more iterations of like larger data sets like
[00:35:20.320 --> 00:35:25.720]   faster than everyone. But I really want people, you know, to be thinking out of the box, but
[00:35:25.720 --> 00:35:30.320]   like, like, like, we're sort of like anchoring on techniques that became very popular, like,
[00:35:30.320 --> 00:35:35.680]   especially like earlier this year. But we really, really hope that like the winner would
[00:35:35.680 --> 00:35:39.960]   either have like figured out how to perfect or really optimize like a composition of these
[00:35:39.960 --> 00:35:47.000]   techniques or come up like with some, like, like uniquely good ones, like new ones. So
[00:35:47.000 --> 00:35:50.960]   this code is also sort of like a great resource if you want to learn more about like some
[00:35:50.960 --> 00:35:56.480]   of the nuances behind our rules. So we basically, you know, like our philosophy is we sort of
[00:35:56.480 --> 00:36:01.240]   explain like our reasoning behind like the rules, like in the open. So like, we have
[00:36:01.240 --> 00:36:04.640]   like an argument, we discuss like back and forth the community like, like around what
[00:36:04.640 --> 00:36:10.600]   would make the competition more interesting. And specifically, like I think, like one one
[00:36:10.600 --> 00:36:16.760]   rule I want to talk about the sort of like, you know, restrictions around like base models
[00:36:16.760 --> 00:36:20.880]   and data sets, because like this sort of, you know, accidentally now, like a lot of
[00:36:20.880 --> 00:36:26.040]   engineers have to be like, you know, licensed lawyers, because of the way the space is evolving.
[00:36:26.040 --> 00:36:31.800]   So I just want to give people like a quick TLDR of sort of the important parts here.
[00:36:31.800 --> 00:36:38.640]   So let's say like, chat GPT, like the terms of services do not allow you to, like, collect
[00:36:38.640 --> 00:36:43.320]   the data set from it and like fine tune with it. So some people might still do it, but
[00:36:43.320 --> 00:36:47.000]   it makes it very difficult to have sort of like an officially backed competition, like
[00:36:47.000 --> 00:36:51.880]   such a regime might work. Even then, let's say open AI was like more permissive with
[00:36:51.880 --> 00:36:56.680]   their license. So you run into this problem of, well, it's not a research artifact, like
[00:36:56.680 --> 00:37:01.600]   this is a model that changes over time. And so this sort of like hampers reproducibility
[00:37:01.600 --> 00:37:04.840]   from our competition, which we believe is sort of like one of the key things of making
[00:37:04.840 --> 00:37:11.760]   this like actually useful. The other thing is like on like restricting data sets, like
[00:37:11.760 --> 00:37:17.160]   the motivation for that is really sort of similar to the licensing thing, like where
[00:37:17.160 --> 00:37:21.120]   there's sort of like a lot of data sets where it's like very ambiguous, whether they were
[00:37:21.120 --> 00:37:26.280]   actually generated by like some like closed source or not. And there's others that have
[00:37:26.280 --> 00:37:32.320]   been very clear about their data provenance that have done like data contamination studies
[00:37:32.320 --> 00:37:37.440]   that did like new algorithmic like generations. And so those are the kinds of things like
[00:37:37.440 --> 00:37:41.400]   will allow and will likely not like not turn into trouble. But again, like if there's sort
[00:37:41.400 --> 00:37:48.720]   of anything unclear, you know, please ask us on Discord. That's why we're here.
[00:37:48.720 --> 00:37:54.040]   I have a couple of questions from the audience. So there is one that is very specific and
[00:37:54.040 --> 00:37:58.240]   is a question about the Flan collection listed among the allowed data sets. And the question
[00:37:58.240 --> 00:38:04.800]   is whether that might contain some LLM generated text. Is it like if it says that that means
[00:38:04.800 --> 00:38:09.960]   it's allowed or should people like still think about and double check like this synthetic
[00:38:09.960 --> 00:38:10.960]   data sets?
[00:38:10.960 --> 00:38:16.200]   So like at least like when I last checked the paper, I didn't get the impression that
[00:38:16.200 --> 00:38:21.880]   there was anything that was like ungenerated in there. Like that said, if the person asking
[00:38:21.880 --> 00:38:25.960]   a question has like sort of a specific snippet they want to refer to from the paper, you
[00:38:25.960 --> 00:38:29.400]   know, we'd be happy to take a look at on the Discord group and clarify the rules.
[00:38:29.400 --> 00:38:34.440]   Thank you. There's another question that might not necessarily speak to the challenge, but
[00:38:34.440 --> 00:38:39.200]   I still want to ask it. So the question is whether there is a general resource on choosing
[00:38:39.200 --> 00:38:44.720]   hardware for ML, DL, LLM model. I think it's probably, it's definitely like an area of
[00:38:44.720 --> 00:38:49.600]   research. I don't think you can give a definitive answer here, but you have made a choice of
[00:38:49.600 --> 00:38:54.280]   like specific like two GPUs. If you can, you can give us a bit more motivation behind this
[00:38:54.280 --> 00:38:55.280]   choice, that would be helpful.
[00:38:55.280 --> 00:38:56.280]   Do you want to cover that?
[00:38:56.280 --> 00:39:06.040]   I think that we are choosing the two type of GPUs partially is motivated by what's available
[00:39:06.040 --> 00:39:14.440]   and we want to allow both commodity. So that's what's represented by 4090 and some slightly
[00:39:14.440 --> 00:39:28.320]   high-end that's A100. Mark, do you want to add to that?
[00:39:28.320 --> 00:39:34.040]   Sure. Yeah. Like I think conceptually we wanted, like, if you look at the price of like a brand
[00:39:34.040 --> 00:39:40.880]   new 4090, it's like about $1,600, right? And if you can get like something that's like
[00:39:40.880 --> 00:39:45.920]   used, like you might find something like even a bit cheaper. And it also has like a dual
[00:39:45.920 --> 00:39:49.800]   use. So you can use it to play video games if you're not like training models. And so
[00:39:49.800 --> 00:39:55.000]   we wanted to have something that felt like plausible, like for like a regular person
[00:39:55.000 --> 00:40:00.040]   to buy. And, you know, I know $1,600 can still be pricey, but it's like still like half of
[00:40:00.040 --> 00:40:03.840]   what, like, let's say like a new like MacBook costs. So it sort of felt like sort of like
[00:40:03.840 --> 00:40:08.280]   a reasonable thing and something valuable for people to own. You know, whereas like
[00:40:08.280 --> 00:40:13.840]   with the A100 track, like, you know, our reasoning there was, Hey, like, yes, like, you know,
[00:40:13.840 --> 00:40:19.800]   why not anchor this on a K80? Because that's what Google collab has or like a T4. You know,
[00:40:19.800 --> 00:40:25.560]   like, why not a V100? Like, why was it? And I think the reason why it was because like,
[00:40:25.560 --> 00:40:30.840]   that's where we're seeing like most of the enterprise interest be because like A100s
[00:40:30.840 --> 00:40:36.080]   have like tensor cores available and they're like wicked fast. They're not always like
[00:40:36.080 --> 00:40:40.520]   available. You know, you sometimes have to fight for them on cloud providers, but it's
[00:40:40.520 --> 00:40:44.960]   like a lot better than H100s. And so because like, this is sort of like a hardware architecture
[00:40:44.960 --> 00:40:50.880]   that we see significant enterprise interest in, it felt sort of like the right, the right
[00:40:50.880 --> 00:40:57.920]   way for the more enterprise-y track of the competition, the right hardware to pick.
[00:40:57.920 --> 00:41:04.480]   Awesome. There is another question that I think I can answer. So the question is whether
[00:41:04.480 --> 00:41:09.360]   we will be building something ourselves or yourself as the audience, as part of this
[00:41:09.360 --> 00:41:14.120]   course and our recommendation is actually for you to participate in this challenge,
[00:41:14.120 --> 00:41:17.880]   even if you're not planning to win it and you don't, may have some constraints. I think
[00:41:17.880 --> 00:41:22.400]   this sets like a good constraint. It gives you like a good evaluation metric. You can
[00:41:22.400 --> 00:41:27.000]   exchange experiences with other participants of this challenge. So our recommendation is
[00:41:27.000 --> 00:41:31.880]   that you do participate, but if you prefer to opt out of this challenge, like we still
[00:41:31.880 --> 00:41:36.440]   encourage you to fine tune an LLM. And that's that, because that's the topic of this course
[00:41:36.440 --> 00:41:41.040]   and you only learn by doing it. And if you have a specific task, like a personal project
[00:41:41.040 --> 00:41:46.760]   that you want to build, that's fine as well for the participation in this course. But
[00:41:46.760 --> 00:41:51.640]   my personal experience with this type of challenges, if you participate, you learn much more than
[00:41:51.640 --> 00:41:52.840]   by opting out.
[00:41:52.840 --> 00:42:01.280]   I actually want to comment on this because this is interesting. Like basically like the
[00:42:01.280 --> 00:42:05.520]   discord group is sort of like a peer group of people that are collectively learning how
[00:42:05.520 --> 00:42:12.040]   to fine tune. And like the organizers themselves, like myself and Weiwei included, are extremely
[00:42:12.040 --> 00:42:16.640]   interested in like what kinds of tricks like people will do. So like we're sort of taking
[00:42:16.640 --> 00:42:21.680]   like almost like a peer review style of like figuring out like what works. After that,
[00:42:21.680 --> 00:42:28.400]   like a real world, like discord eval bot, like I think this could be quite compelling.
[00:42:28.400 --> 00:42:33.040]   And again, like you can learn a lot about the competition. We always encourage you to
[00:42:33.040 --> 00:42:39.720]   submit. We realize like sometimes like just starting out might be like a bit overwhelming,
[00:42:39.720 --> 00:42:44.000]   which is why we have a really nice like starter kit that like is based off of let's GPT, which
[00:42:44.000 --> 00:42:49.040]   should give you a good baseline. So if you just start with that and like tweak, like
[00:42:49.040 --> 00:42:52.760]   hack together a few things, like change a couple of lines, you might still like learn
[00:42:52.760 --> 00:42:53.760]   a lot.
[00:42:53.760 --> 00:42:57.880]   But if you feel like you want sort of like if you feel sort of this is not the right
[00:42:57.880 --> 00:43:02.200]   approach and you have like some like more significant ideas you'd like to contribute,
[00:43:02.200 --> 00:43:06.040]   like of course, like I mean, the way our submission and probably work, it's like sort of a very
[00:43:06.040 --> 00:43:10.120]   general HTTP service. So you do whatever you want. But you know, we really want to make
[00:43:10.120 --> 00:43:15.360]   sure that you have a smooth experience to ship that first line of code that will make
[00:43:15.360 --> 00:43:23.600]   you feel productive. So if that doesn't feel like the case, please join discord and complain.
[00:43:23.600 --> 00:43:28.280]   There is another question whether we'll be talking about fine tuning multimodal LLMs
[00:43:28.280 --> 00:43:34.360]   in this course. I think this is definitely not the scope of this efficiency challenge.
[00:43:34.360 --> 00:43:38.160]   I don't think you're evaluating on any multimodal data, Mark and Weiwei.
[00:43:38.160 --> 00:43:39.160]   Maybe next year.
[00:43:39.160 --> 00:43:46.440]   Yeah, I think this may be a bit out of scope for this course, but I do believe that the
[00:43:46.440 --> 00:43:52.120]   skills that you will gain will also transfer into multimodal. That's my personal belief.
[00:43:52.120 --> 00:43:59.280]   All right. Let me just ask a final question and we'll hand over to Ayush. So there is
[00:43:59.280 --> 00:44:04.440]   a question whether we'll talk about how to fine tune on raw or unstructured or unlabeled
[00:44:04.440 --> 00:44:10.000]   data without creating question and response parts. I interpret that as a kind of domain
[00:44:10.000 --> 00:44:15.800]   adaptation, like extending kind of pre-training. I'm not sure if in the course we'll focus
[00:44:15.800 --> 00:44:19.280]   specifically on that, but I think like the fine tuning techniques that we'll use with
[00:44:19.280 --> 00:44:24.960]   instruction data sets might just as well be applicable with this like further pre-training.
[00:44:24.960 --> 00:44:26.960]   Any thoughts, Mark or Weiwei, on domain adaptation?
[00:44:26.960 --> 00:44:32.960]   Well, it's definitely one of the standing challenges of machine learning. And I think
[00:44:32.960 --> 00:44:42.760]   in general, there's very few performance guarantee you can get overall, but it all depends on
[00:44:42.760 --> 00:44:50.040]   the task and the specific data that you're trying to adapt. And part of like, I guess,
[00:44:50.040 --> 00:44:56.120]   in the spirit of getting hands on experience, it's all about just trying and see what you
[00:44:56.120 --> 00:45:01.800]   can get out of it.
[00:45:01.800 --> 00:45:13.480]   Awesome. Weiwei, Mark, we really appreciate you coming into this event and sharing and
[00:45:13.480 --> 00:45:18.560]   also preparing this challenge. We feel like this is making LLMs more accessible to a broader
[00:45:18.560 --> 00:45:24.000]   range of people. So that's definitely a good thing. We really appreciate it. And in this
[00:45:24.000 --> 00:45:30.240]   next part, we also want to make it even more accessible. So Ayush Thakur has prepared like
[00:45:30.240 --> 00:45:35.640]   also our own version of the starter kit, which also includes some really cool experiment
[00:45:35.640 --> 00:45:40.440]   tracking. So Ayush, over to you to present and give people some hands on tips on how
[00:45:40.440 --> 00:45:43.440]   to get started with this challenge.
[00:45:43.440 --> 00:45:50.720]   Thanks Derek for the introduction and also thank you Mark and Weiwei for that wonderful
[00:45:50.720 --> 00:45:58.320]   talk. It was really enlightening and we learned a lot about how to do, like about this challenge
[00:45:58.320 --> 00:46:04.760]   and why they thought about this challenge, what are the GPU constraints, what's evaluation
[00:46:04.760 --> 00:46:11.360]   and everything. So today I will be presenting and let me share my screen and I hope that
[00:46:11.360 --> 00:46:18.520]   the whole screen will, I mean, I will be able to share the whole thing with all of you.
[00:46:18.520 --> 00:46:41.480]   Just give me a sec. I have shared my screen. I'm not sure if it's visible on YouTube, but
[00:46:41.480 --> 00:46:47.240]   yeah, thanks for the introduction Mark and Weiwei once again. And today I will basically
[00:46:47.240 --> 00:46:52.040]   be giving some practical tips and sharing the starter pack that I've been working for
[00:46:52.040 --> 00:47:01.640]   over a week now on how to fine tune LLMs with GPU constraints. And the focus will be 7 billion
[00:47:01.640 --> 00:47:10.080]   LLM model. Right. So before I go into the code, let me basically just give the big picture
[00:47:10.080 --> 00:47:18.040]   in here. Right. So the big picture is you need to use an open source data. It can be
[00:47:18.040 --> 00:47:25.400]   a $15K, it can be OASST1, which is an open system database. You can use any other open
[00:47:25.400 --> 00:47:31.680]   source database for that matter to fine tune a language model. Right. Then you can again
[00:47:31.680 --> 00:47:36.480]   come up with any fine tuning repository or any script for that matter. You can build
[00:47:36.480 --> 00:47:42.920]   your own fine tuning repository if you want to in the course of this competition. And
[00:47:42.920 --> 00:47:50.680]   with the data and some fine tuning code, what we are going to do is pick a base pre-trained
[00:47:50.680 --> 00:47:58.000]   LLM. It can be a LLAMA2, it can be a Falcon, OpenLLAMA, it can even be BERT or T5. It's
[00:47:58.000 --> 00:48:04.640]   up to you. And I mean, the competition host, they have listed down the allowed models that
[00:48:04.640 --> 00:48:11.360]   you can pick. So, yeah, as they mentioned, we can do the encoder, decoder or both encoders
[00:48:11.360 --> 00:48:17.680]   with decoder style model. So with all of these, what we need to do is fine tune on a single
[00:48:17.680 --> 00:48:28.480]   A100 with 40 GB of GPU memory or a 4090 GPU. And once we have the fine tuned model, the
[00:48:28.480 --> 00:48:35.320]   competition hosts are going to be evaluating that model against a subset of Helm benchmark.
[00:48:35.320 --> 00:48:41.040]   And they also have some holdout secret tasks that they have not shared that they might
[00:48:41.040 --> 00:48:49.640]   be using to do like a final phase of evaluation. Right. So this is the big picture. And having
[00:48:49.640 --> 00:48:53.120]   said that, there are so many stages and there are so many things to think about the data
[00:48:53.120 --> 00:48:58.000]   that you want to curate, how you want to curate, how you want to sample, what the strategies
[00:48:58.000 --> 00:49:01.960]   for fine tuning the model. But you also need to be really careful about how you're going
[00:49:01.960 --> 00:49:07.600]   to evaluate it, evaluate the model. It's not like a typical Kaggle competition, wherein
[00:49:07.600 --> 00:49:13.360]   the evaluation data is fixed and also evaluation metric is fixed. Here, the evaluation is something
[00:49:13.360 --> 00:49:20.360]   that you need to think about. Right. Just I mean, just give like some two cents on the
[00:49:20.360 --> 00:49:25.360]   data pipeline and the fine tuning pipeline. So on the data side, whatever pipeline you
[00:49:25.360 --> 00:49:30.800]   have, make sure that it's kind of easy to play with. It's hackable so that you can download
[00:49:30.800 --> 00:49:36.040]   whatever data you want to download. Also, it has convenient scripts to convert the data
[00:49:36.040 --> 00:49:41.240]   into usable formats. You can mix different open source data sets and also sample from
[00:49:41.240 --> 00:49:47.880]   them, depending on the evaluation metric that you want to improve. So all of these should
[00:49:47.880 --> 00:49:52.320]   be considered while you're thinking about data pipeline. Also, the data loader should
[00:49:52.320 --> 00:49:57.320]   be available both for the training and the evaluation. Right. And on the script side
[00:49:57.320 --> 00:50:02.840]   of the repository side for fine tuning, it should have some convenient functionalities
[00:50:02.840 --> 00:50:08.560]   to load any model, to checkpoint, to train a model, in this case, fine tune the model
[00:50:08.560 --> 00:50:14.920]   on the data set that you have curated. But given it's a research project, the scripts,
[00:50:14.920 --> 00:50:19.880]   whatever pipeline you use, whatever repository you use, make sure that it's easy to hack
[00:50:19.880 --> 00:50:25.160]   on. It's easy to add any features. It's easy to try out new functionalities. Right. And
[00:50:25.160 --> 00:50:29.840]   to be honest, there's like a whole host of options out there. It's just up to you to
[00:50:29.840 --> 00:50:37.360]   pick Google or stuff. See what works best for your habitat. Having said that, I have
[00:50:37.360 --> 00:50:48.080]   spent like a few weeks now to curate this repository. So if you go to this link, 1db.me/neoreps-llm-finetuner
[00:50:48.080 --> 00:50:54.840]   or scan this QR code, you will head to this repository, which is Neoreps LLM Efficiency
[00:50:54.840 --> 00:51:03.120]   Challenge. And I just want to give a quick rundown of what you can expect in this repository.
[00:51:03.120 --> 00:51:10.240]   Yeah. So the rundown is basically that this repository is built on top of LibGPT. So the
[00:51:10.240 --> 00:51:14.800]   official starter kit that was provided by the organizers, that was also built on top
[00:51:14.800 --> 00:51:20.120]   of that LibGPT. But the focus for that repository was mostly to showcase submission because
[00:51:20.120 --> 00:51:27.160]   submission is tricky here. I will try to cover that if we have time. So it's built on LibGPT,
[00:51:27.160 --> 00:51:33.880]   but I'm using a fork of LibGPT so that I can add in some functionalities. I can hack with
[00:51:33.880 --> 00:51:42.440]   the original code base and add in some stuff. In this case, I've instrumented the LibGPT,
[00:51:42.440 --> 00:51:46.440]   the fork with Weights and Biases, experiment tracking, and I'm working on more functionalities
[00:51:46.440 --> 00:51:52.560]   like check pointing. So if you have a LoRa fine-tuned model, you just provide in the
[00:51:52.560 --> 00:51:58.400]   URL from the Weights and Biases and can then pull the model and do all the evaluation and
[00:51:58.400 --> 00:52:03.840]   stuff for you automatically. So I'm just trying to build those things. Having said that, I've
[00:52:03.840 --> 00:52:10.200]   also documented all the steps that's required to set up your system. And to be honest, it
[00:52:10.200 --> 00:52:15.280]   took some time for me, especially because it was me doing it for the first time. And
[00:52:15.280 --> 00:52:21.400]   I assume that a lot of folks who will be participating in this competition, they will be doing it
[00:52:21.400 --> 00:52:28.680]   for the first time as well. So instead of spending a lot of time, you can maybe just
[00:52:28.680 --> 00:52:35.680]   go through the steps that I have documented in the README. And with this repository, especially
[00:52:35.680 --> 00:52:41.240]   with LibGPT, you have strategies like LoRa fine-tuning, QLoRa, like adapter, adapter
[00:52:41.240 --> 00:52:46.360]   v2, and some convenient scripts for downloading the model, preparing the data, et cetera,
[00:52:46.360 --> 00:52:54.280]   out of the shelf. Yeah. So if you can see the report screen here, as I mentioned, every
[00:52:54.280 --> 00:52:59.000]   step has been documented and it's like, you need to create some environment, you need
[00:52:59.000 --> 00:53:06.560]   to install some packages. I have also covered some of the gotchas that you might encounter
[00:53:06.560 --> 00:53:12.760]   if you are installing FlashAttention and if you have to deal with CUDA. So in case if
[00:53:12.760 --> 00:53:18.680]   you need to update the CUDA version to 11.4, there are steps that I have documented that
[00:53:18.680 --> 00:53:24.400]   you can just use and it will work. And these are the steps to basically download FlashAttention.
[00:53:24.400 --> 00:53:30.880]   So why FlashAttention? It will basically help fine-tuning bigger models because it helps
[00:53:30.880 --> 00:53:39.680]   with the memory usage of the model when it's loaded for training. Yeah. And there are some
[00:53:39.680 --> 00:53:45.840]   useful tips on what parameters should be used for max job and those stuff, which you can
[00:53:45.840 --> 00:53:54.280]   find in here. And yeah, it will just simplify your life and get started. Also, because this
[00:53:54.280 --> 00:53:59.160]   whole repository is built on top of Legibility, we have convenient scripts to download the
[00:53:59.160 --> 00:54:06.480]   model. So in this case, I'm downloading the Lama 2 7 billion paramodel and yeah, Legibility
[00:54:06.480 --> 00:54:11.200]   requires it to be converted to a standard format so that the training loop can consume
[00:54:11.200 --> 00:54:17.800]   the standard model format and just use it. And also have some convenient feature scripts
[00:54:17.800 --> 00:54:23.640]   to download the data set and prepare it. In this case, I'm using a Dolly 15K, but yeah,
[00:54:23.640 --> 00:54:29.640]   feel free to use Red Pajama Data 1 trillion or like, yeah, use your own data, write some
[00:54:29.640 --> 00:54:37.800]   script and you can start playing with that and for the pipeline. Great. So we have some
[00:54:37.800 --> 00:54:45.240]   steps to set up the whole thing. Maybe have to take care of CUDA, install FlashAttention,
[00:54:45.240 --> 00:54:49.520]   download the data, download the model, etc. A quick note on the model for if you want
[00:54:49.520 --> 00:54:55.880]   to fine tune a Lama 2, you need to get permission from the authors of Lama 2. And if you go
[00:54:55.880 --> 00:55:01.200]   to the model card, the Hugging Case model card for Lama 2, you just need to provide
[00:55:01.200 --> 00:55:05.720]   your email ID and click a button and yeah, just wait for a few days, one or two days,
[00:55:05.720 --> 00:55:11.280]   most probably, and you will have access to it. So that's like very easy. Great. Now with
[00:55:11.280 --> 00:55:18.040]   one line of code, we can start fine tuning the model just to walk through. It's a lot
[00:55:18.040 --> 00:55:24.360]   of fine tuning strategy that I'm using. I have provided the data directory. I have provided
[00:55:24.360 --> 00:55:30.640]   the checkpoint, in this case, a Lama 2 7 billion model. And yeah, it's used. I've specified
[00:55:30.640 --> 00:55:35.320]   the precision for fine tuning to be brain floor 16. There are other positions that you
[00:55:35.320 --> 00:55:40.400]   can try out maybe brain floor 16 mixed. And depending on the precision, you might get
[00:55:40.400 --> 00:55:48.040]   different accuracy, different like improvement to the metric or the metric might even go
[00:55:48.040 --> 00:55:51.520]   down, the evaluation metric might go down. So you need to experiment. Also, depending
[00:55:51.520 --> 00:56:00.120]   on the precision, the memory requirement will be changing. So a true brain floor 16 will
[00:56:00.120 --> 00:56:07.240]   consume less memory compared to a brain floor 16 mixed, just as an example. And recently
[00:56:07.240 --> 00:56:12.760]   they added QLora, which is a quantized, a lot of fine tuning strategy, which is also
[00:56:12.760 --> 00:56:17.000]   great. And they are continuous. The LigPT team is continuously working on making things
[00:56:17.000 --> 00:56:23.480]   simpler for you. And in turn, my repository in this case is going to be consuming those
[00:56:23.480 --> 00:56:27.520]   features as well. And I will be adding some functionalities on my own, especially on the
[00:56:27.520 --> 00:56:37.160]   ML ops side of things. Great. So just focusing on the fine tuning, what I did was I fine
[00:56:37.160 --> 00:56:44.360]   tuned two models, one of Falcon 7 billion bundle and a Lama 2 7 billion model. So as
[00:56:44.360 --> 00:56:48.360]   you can see, since I was logging everything to weights and biases, you can look through
[00:56:48.360 --> 00:56:54.600]   the throughputs, the flops, other metrics that's been generated in the script and been
[00:56:54.600 --> 00:57:00.880]   logged to 1 dB. The most exciting thing while fine tuning is the loss curve. As you can
[00:57:00.880 --> 00:57:10.240]   see, the Lama 2 has a better loss compared to a Falcon 7 billion model. And just to give
[00:57:10.240 --> 00:57:14.400]   important information here, which is what the models were trained on the same data with
[00:57:14.400 --> 00:57:19.240]   the same configuration for the same number of steps. So Lama 2 seems to be performing
[00:57:19.240 --> 00:57:26.080]   good on the training dataset. Great. Now let's try to look at one run. So let's look at the
[00:57:26.080 --> 00:57:31.520]   fine tuning of fine tuning of Lama 2. So as I showed, we have the metrics and log the
[00:57:31.520 --> 00:57:38.160]   loss and everything, but what else has been captured automatically for you? So in this
[00:57:38.160 --> 00:57:44.320]   comp challenge, one of the goal is that you only need, or one of the questions is that
[00:57:44.320 --> 00:57:52.040]   you only can train a model for one day, right? So use one GPU and train it for one day. So
[00:57:52.040 --> 00:57:56.120]   it's important to track the amount of time that was spent on training the model. And
[00:57:56.120 --> 00:58:00.440]   in this case, I just spent like three hours and 43 minutes. I could have trained longer
[00:58:00.440 --> 00:58:06.360]   because I had that budget to go to 24 hours. Great. I can also look at the command that
[00:58:06.360 --> 00:58:10.120]   was used to train the model, which is also a good thing because you might be trying so
[00:58:10.120 --> 00:58:15.600]   many things and you need to keep track of what command created that particular metric.
[00:58:15.600 --> 00:58:22.680]   Talking about keeping track of things that will directly correlate with the metrics that
[00:58:22.680 --> 00:58:26.800]   you are getting, you need to keep track of the configuration. So in this case, I was
[00:58:26.800 --> 00:58:33.920]   using a batch size of 128, but the micro batch size, because I was using grad accumulation,
[00:58:33.920 --> 00:58:40.720]   was just one and it was trained for 20k iterations with a lower rank of four. And if you can
[00:58:40.720 --> 00:58:50.040]   see here, I turned the lower key, the lower query and lower value. So the key query and
[00:58:50.040 --> 00:58:56.040]   value, these three parameters or these three weight matrices in a transformer block were
[00:58:56.040 --> 00:59:02.440]   being fine tuned. And because of that, the number of trainable parameters were only one,
[00:59:02.440 --> 00:59:08.200]   like three million in this case. So the original model size is seven billion or close to seven
[00:59:08.200 --> 00:59:13.480]   billion, but the number of trainable parameters for this fine tuning job is only three million,
[00:59:13.480 --> 00:59:19.160]   which is great. But still, it's a lot of it's a huge model and you have to come up with
[00:59:19.160 --> 00:59:24.560]   techniques and strategies to fit this in a model and train it efficiently. Great. We
[00:59:24.560 --> 00:59:29.840]   have configurations. We have the time track, everything great. Now look at the system metrics
[00:59:29.840 --> 00:59:37.000]   because you have just one GPU for one day and you need to put in as much data as possible
[00:59:37.000 --> 00:59:42.000]   so that the model can see as much data, the more data the model can see in a single day,
[00:59:42.000 --> 00:59:48.240]   the better it is. So since system metrics is tracked automatically with Pantene, we
[00:59:48.240 --> 00:59:54.480]   can look at the memory allocated. So like 23-ish percent of the memory was allocated
[00:59:54.480 --> 01:00:01.480]   for this job while the utilization is not so great. So maybe there's a bug in my pipeline
[01:00:01.480 --> 01:00:10.640]   that this might be a bug in the Lora.py script. I'm not sure it might be something in the
[01:00:10.640 --> 01:00:16.120]   way I trained the model, but you can see that which time the utilization dropped. And ideally,
[01:00:16.120 --> 01:00:23.160]   we want the utilization to be close to 100% or like above 90% ideally, because if it's
[01:00:23.160 --> 01:00:30.840]   cranking in numbers or utilizing the GPU better, you might be able to push in seven more data
[01:00:30.840 --> 01:00:38.200]   and in turn improve the metric of the model. Great. So that's metric. Yeah. I mean, you
[01:00:38.200 --> 01:00:43.360]   can also look at the exact script that was used in this case, the Lora. You can also
[01:00:43.360 --> 01:00:52.080]   track in the code file. Great. So that's one run that I just ran this one single line of
[01:00:52.080 --> 01:00:58.640]   code, which is Lora.py and all of this information was tracked automatically because I instrumented
[01:00:58.640 --> 01:01:03.760]   the code with Rundeby. If you are using a different fine tuning script or different
[01:01:03.760 --> 01:01:08.960]   repository, you might find Rundeby out of the box. If not, you can just instrument it
[01:01:08.960 --> 01:01:17.200]   yourself. It's very easy. Great. So that's on fine tuning. Now let's go and think about
[01:01:17.200 --> 01:01:23.600]   evaluation a bit. So it's like just two things sharing here. And this is something that's
[01:01:23.600 --> 01:01:30.480]   inspired by an idea that was shared by Sebastian Raksha in PyTorch Lightning blog post that
[01:01:30.480 --> 01:01:36.560]   he wrote about this competition. So he said that since the evaluation, the final evaluation
[01:01:36.560 --> 01:01:42.600]   will be done on a subset of HAL, how about we pick some other benchmark, open benchmark
[01:01:42.600 --> 01:01:50.960]   like Eleuther's LM EvalHardness or maybe Google's Big Bench. And the point is that with, for
[01:01:50.960 --> 01:01:57.120]   example, LM EvalHardness, there's a overlap with the HAL benchmark. So you can maybe take
[01:01:57.120 --> 01:02:03.960]   a subset of LM Eval, consider that as a validation set, evaluate your model on that and use the
[01:02:03.960 --> 01:02:11.600]   HAL, like a subset of HAL as a test set and see if the validation is improving and your
[01:02:11.600 --> 01:02:16.760]   test is improving as well. If the both are improving, then your model might be generalizing
[01:02:16.760 --> 01:02:24.720]   better. So you can come up with good strategies about evaluation. So it's up to you to experiment
[01:02:24.720 --> 01:02:31.000]   with different subsets of these benchmarks, because for example, LM EvalHardness has 200
[01:02:31.000 --> 01:02:36.720]   tasks. So you can pick a smaller subset or subsets that might actually have a better
[01:02:36.720 --> 01:02:43.600]   chance in getting a better score on the leaderboard. So how to go about setting up evaluation?
[01:02:43.600 --> 01:02:50.000]   Now, I've also documented all of these things here that you can go through. So setting up
[01:02:50.000 --> 01:02:54.320]   evaluation with Eleuther LM EvalHardness, which is straightforward, just go into the
[01:02:54.320 --> 01:03:00.440]   repository and install it. I'm also documented, I mean, this is something that I basically
[01:03:00.440 --> 01:03:10.280]   picked from the sample submission starter kit that was provided by the organizers, but
[01:03:10.280 --> 01:03:16.440]   I took the liberty of trying it out myself and documenting what was required and with
[01:03:16.440 --> 01:03:23.960]   some more comments that I felt would might be helpful for all of you. And similarly,
[01:03:23.960 --> 01:03:31.000]   I tried using the HAL benchmark, installing the whole thing, and it was also well documented
[01:03:31.000 --> 01:03:36.060]   in the starter kit. But again, I took the liberty of trying it myself and documenting
[01:03:36.060 --> 01:03:40.720]   everything for all of you. And yeah, I also encountered some issues and yeah, I provided
[01:03:40.720 --> 01:03:46.840]   some links that you can maybe try and fix that issues. So that's like you setting up
[01:03:46.840 --> 01:03:54.000]   the pipeline and evaluating, because I was able to do so. We'll just quickly share some
[01:03:54.000 --> 01:04:02.120]   interesting evaluation that I was able to do. So I will start with the evaluation on
[01:04:02.120 --> 01:04:11.080]   Falcon 7B. So it's the base Falcon 7B and you can see the numbers here. Just to give
[01:04:11.080 --> 01:04:19.200]   some insight, I used four tasks, the wiki test, the arithmetic IDC, the truthful QA
[01:04:19.200 --> 01:04:26.600]   MC and the open book QA. So these were the four tasks or a subset of tasks from the LM
[01:04:26.600 --> 01:04:38.640]   Harness benchmark. So how is LMA2 compared on this exact benchmark? Let's see. So as
[01:04:38.640 --> 01:04:47.320]   you can see, the perplexity of LMA2 base model is better than Falcon B. So the lesser the
[01:04:47.320 --> 01:04:52.960]   perplexity, the better the model is. So it's better. You can see that LMA2 is also better
[01:04:52.960 --> 01:04:59.120]   on arithmetic task. It's slightly better on the truthful QA and open book QA. So open
[01:04:59.120 --> 01:05:05.040]   book is kind of hard to say which model is better. So they're kind of same. So that's
[01:05:05.040 --> 01:05:10.640]   the base model that we compare. So with just this, we can say that maybe picking LMA2 as
[01:05:10.640 --> 01:05:15.920]   the initial checkpoint would make a lot of sense. But again, it's subject to a lot of
[01:05:15.920 --> 01:05:21.640]   experimentation, subject to your evaluation strategy and also will depend on the final
[01:05:21.640 --> 01:05:28.280]   evaluation that the fine tune model will be done on. So that's the base model. But what
[01:05:28.280 --> 01:05:35.440]   is more exciting is after fine tuning the two models with the same configuration, same
[01:05:35.440 --> 01:05:42.120]   hyper parameters, what's the result? So let me close the base model here and we can totally
[01:05:42.120 --> 01:05:49.320]   see that after fine tuning, yeah, the LMA2 is performing again better compared to a Falcon
[01:05:49.320 --> 01:05:55.440]   B. And if you just look them, like compare them from the baseline, we can totally see
[01:05:55.440 --> 01:06:03.880]   that after fine tuning, we can, after fine tuning the perplexity actually got bad for
[01:06:03.880 --> 01:06:13.040]   Falcon. Also, if you look at the arithmetic for both Falcon and both LMA, on the arithmetic
[01:06:13.040 --> 01:06:18.360]   benchmark, we got zero accuracy. And this might be because Dolly 15k is not actually
[01:06:18.360 --> 01:06:25.480]   catered towards like arithmetic based question answering, which brings to the point that
[01:06:25.480 --> 01:06:31.360]   you really need to bring in other data like datasets and like mix and match and sample
[01:06:31.360 --> 01:06:38.080]   data sets and curate your data set for improving the overall accuracy, overall metrics, not
[01:06:38.080 --> 01:06:44.040]   just accuracy. And yeah, like you can log everything to RenderBee and look at the dashboard
[01:06:44.040 --> 01:06:49.680]   like this and compare results, which is a great way of finding insights from the research
[01:06:49.680 --> 01:06:55.280]   that you're doing, finding if whatever research and whatever strategies that you're doing
[01:06:55.280 --> 01:07:05.000]   is working out or not. So that was evaluation. I will take a pause right now because this
[01:07:05.000 --> 01:07:10.240]   is what I had to cover. Again, I will really love you all to check out this repository
[01:07:10.240 --> 01:07:18.040]   that's linked in here and yeah, try it out. If there's any step that might be misdocumented
[01:07:18.040 --> 01:07:22.360]   or it's not working for you, feel free to put an issue, raise a PR if you want. And
[01:07:22.360 --> 01:07:27.800]   yeah, I would love to take it from there. So that's it.
[01:07:27.800 --> 01:07:36.280]   Thanks. Thanks a lot, Ayush. Really appreciate this hands-on starter package that hopefully
[01:07:36.280 --> 01:07:44.080]   can make it easier to get started in this competition. Mark, Weiwei, any comments from
[01:07:44.080 --> 01:07:49.120]   you? Like we actually got one question that is a very technical question specifically
[01:07:49.120 --> 01:07:54.640]   related to FlashAttention2. We might get into this topic in lesson four when Jonathan Frankel
[01:07:54.640 --> 01:07:59.160]   will be talking about training and fine-tuning techniques. But I think in the meantime, we
[01:07:59.160 --> 01:08:03.680]   might want to encourage people to ask this question on the competition Discord where
[01:08:03.680 --> 01:08:07.160]   probably you might have the right audience to discuss this type of topics.
[01:08:07.160 --> 01:08:14.720]   Yeah, I guess I had a question. Ayush, please, please submit this as well. I think it would
[01:08:14.720 --> 01:08:18.480]   be very valuable if people could learn how to quickly get dashboards ready for them.
[01:08:18.480 --> 01:08:23.360]   I think that'd be very valuable. So please send us a PR and I'd be happy to merge it.
[01:08:23.360 --> 01:08:32.240]   For sure. For sure. Awesome. Let me do a quick check if there are any outstanding questions.
[01:08:32.240 --> 01:08:38.320]   I don't see anything in the chat, but we'll wait for a minute or two just in case questions
[01:08:38.320 --> 01:08:46.040]   are coming. We might have a bit of a delay between the live stream and this recording
[01:08:46.040 --> 01:09:01.320]   here. Okay. I don't see any questions. Again, big thanks to Weiwei and Mark for presenting
[01:09:01.320 --> 01:09:07.280]   and hosting and organizing this challenge. Thank you, Ayush, for working on this starter
[01:09:07.280 --> 01:09:15.480]   package and showing people how they can fine-tune a model, how they can submit into this competition
[01:09:15.480 --> 01:09:20.440]   and also track their experiments with weights and biases. Next week, we will talk more in
[01:09:20.440 --> 01:09:25.960]   depth about evaluation and understanding evaluation, different metrics, different benchmarks will
[01:09:25.960 --> 01:09:32.400]   definitely be important as you try to fine-tune a model and make it work really, really well.
[01:09:32.400 --> 01:09:36.960]   Then we'll take a break for a couple of weeks and then we'll come back with one of the top
[01:09:36.960 --> 01:09:41.800]   experts in this field, Jonathan Frankel, to talk about different data sets for training
[01:09:41.800 --> 01:09:47.560]   and fine-tuning language models and training and fine-tuning techniques. Thanks, everyone,
[01:09:47.560 --> 01:09:52.240]   for joining. We appreciate you being here with us. We encourage you to get started on
[01:09:52.240 --> 01:09:57.840]   this competition, fine-tune your RLMs efficiently on a single GPU, hopefully to keep the cost
[01:09:57.840 --> 01:10:03.880]   low and join Boiler competition Discord. We also have Weights and Biases Discord for folks
[01:10:03.880 --> 01:10:09.760]   that want to ask questions specific to weights and biases. We're looking forward to chatting
[01:10:09.760 --> 01:10:10.760]   with you next week.
[01:10:10.760 --> 01:10:16.640]   Thank you so much for having us.
[01:10:16.640 --> 01:10:19.280]   Bye.

