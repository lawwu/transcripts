
[00:00:00.000 --> 00:00:03.440]   The following is a conversation with David Patterson,
[00:00:03.440 --> 00:00:04.800]   Turing Award winner
[00:00:04.800 --> 00:00:07.520]   and professor of computer science at Berkeley.
[00:00:07.520 --> 00:00:09.760]   He's known for pioneering contributions
[00:00:09.760 --> 00:00:11.680]   to risk processor architecture
[00:00:11.680 --> 00:00:14.760]   used by 99% of new chips today
[00:00:14.760 --> 00:00:18.040]   and for co-creating RAID storage.
[00:00:18.040 --> 00:00:20.080]   The impact that these two lines of research
[00:00:20.080 --> 00:00:23.720]   and development have had in our world is immeasurable.
[00:00:23.720 --> 00:00:26.240]   He's also one of the great educators
[00:00:26.240 --> 00:00:28.240]   of computer science in the world.
[00:00:28.240 --> 00:00:31.520]   His book with John Hennessy is how I first learned about
[00:00:31.520 --> 00:00:34.320]   and was humbled by the inner workings of machines
[00:00:34.320 --> 00:00:35.960]   at the lowest level.
[00:00:35.960 --> 00:00:37.520]   Quick summary of the ads.
[00:00:37.520 --> 00:00:42.000]   Two sponsors, the Jordan Harbinger Show and Cash App.
[00:00:42.000 --> 00:00:43.640]   Please consider supporting the podcast
[00:00:43.640 --> 00:00:46.840]   by going to jordanharbinger.com/lex
[00:00:46.840 --> 00:00:51.000]   and downloading Cash App and using code LEXPODCAST.
[00:00:51.000 --> 00:00:53.520]   Click on the links, buy the stuff.
[00:00:53.520 --> 00:00:55.680]   It's the best way to support this podcast
[00:00:55.680 --> 00:00:57.800]   and in general, the journey I'm on
[00:00:57.800 --> 00:00:59.400]   in my research and startup.
[00:00:59.400 --> 00:01:01.960]   This is the Artificial Intelligence Podcast.
[00:01:01.960 --> 00:01:04.000]   If you enjoy it, subscribe on YouTube,
[00:01:04.000 --> 00:01:06.240]   review it, the five stars on Apple Podcast,
[00:01:06.240 --> 00:01:09.200]   support it on Patreon or connect with me on Twitter
[00:01:09.200 --> 00:01:14.200]   at Lex Friedman, spelled without the E, just F-R-I-D-M-A-N.
[00:01:14.200 --> 00:01:17.440]   As usual, I'll do a few minutes of ads now
[00:01:17.440 --> 00:01:18.840]   and never any ads in the middle
[00:01:18.840 --> 00:01:21.320]   that can break the flow of the conversation.
[00:01:21.320 --> 00:01:25.160]   This episode is supported by the Jordan Harbinger Show.
[00:01:25.160 --> 00:01:27.960]   Go to jordanharbinger.com/lex.
[00:01:27.960 --> 00:01:29.760]   It's how he knows I sent you.
[00:01:29.760 --> 00:01:31.720]   On that page, there's links to subscribe to it
[00:01:31.720 --> 00:01:34.840]   on Apple Podcast, Spotify and everywhere else.
[00:01:34.840 --> 00:01:36.640]   I've been binging on this podcast.
[00:01:36.640 --> 00:01:37.480]   It's amazing.
[00:01:37.480 --> 00:01:39.360]   Jordan is a great human being.
[00:01:39.360 --> 00:01:40.920]   He gets the best out of his guests,
[00:01:40.920 --> 00:01:43.400]   dives deep, calls them out when it's needed
[00:01:43.400 --> 00:01:45.440]   and makes the whole thing fun to listen to.
[00:01:45.440 --> 00:01:48.320]   He's interviewed Kobe Bryant, Mark Cuban,
[00:01:48.320 --> 00:01:51.800]   Neil deGrasse Tyson, Garry Kasparov and many more.
[00:01:51.800 --> 00:01:53.680]   I recently listened to his conversation
[00:01:53.680 --> 00:01:57.480]   with Frank Abagnale, author of "Catch Me If You Can"
[00:01:57.480 --> 00:02:00.440]   and one of the world's most famous con men.
[00:02:00.440 --> 00:02:02.160]   Perfect podcast length and topic
[00:02:02.160 --> 00:02:06.000]   for a recent long distance run that I did.
[00:02:06.000 --> 00:02:09.160]   Again, go to jordanharbinger.com/lex.
[00:02:09.160 --> 00:02:13.600]   To give him my love and to support this podcast,
[00:02:13.600 --> 00:02:17.380]   subscribe also on Apple Podcast, Spotify and everywhere else.
[00:02:17.380 --> 00:02:21.080]   This show is presented by Cash App,
[00:02:21.080 --> 00:02:22.920]   the greatest sponsor of this podcast ever
[00:02:22.920 --> 00:02:26.400]   and the number one finance app in the App Store.
[00:02:26.400 --> 00:02:29.120]   When you get it, use code LEXPODCAST.
[00:02:29.120 --> 00:02:31.160]   Cash App lets you send money to friends,
[00:02:31.160 --> 00:02:33.040]   buy Bitcoin and invest in the stock market
[00:02:33.040 --> 00:02:34.280]   with as little as $1.
[00:02:34.280 --> 00:02:37.320]   Since Cash App allows you to buy Bitcoin,
[00:02:37.320 --> 00:02:39.120]   let me mention that cryptocurrency
[00:02:39.120 --> 00:02:41.960]   in the context of the history of money is fascinating.
[00:02:41.960 --> 00:02:43.440]   I recommend "A Scent of Money"
[00:02:43.440 --> 00:02:45.280]   as a great book on this history.
[00:02:45.280 --> 00:02:47.720]   Also, the audio book is amazing.
[00:02:47.720 --> 00:02:49.000]   Debits and credits on Ledger
[00:02:49.000 --> 00:02:51.520]   started around 30,000 years ago.
[00:02:51.520 --> 00:02:54.200]   The US dollar created over 200 years ago
[00:02:54.200 --> 00:02:56.200]   and the first decentralized cryptocurrency
[00:02:56.200 --> 00:02:58.180]   released just over 10 years ago.
[00:02:58.180 --> 00:02:59.460]   So given that history,
[00:02:59.460 --> 00:03:01.280]   cryptocurrency is still very much
[00:03:01.280 --> 00:03:03.120]   in its early days of development,
[00:03:03.120 --> 00:03:04.480]   but it's still aiming to
[00:03:04.480 --> 00:03:08.000]   and just might redefine the nature of money.
[00:03:08.000 --> 00:03:09.640]   So again, if you get Cash App
[00:03:09.640 --> 00:03:11.240]   from the App Store or Google Play
[00:03:11.240 --> 00:03:15.160]   and use the code LEXPODCAST, you get $10
[00:03:15.160 --> 00:03:18.080]   and Cash App will also donate $10 to FIRST,
[00:03:18.080 --> 00:03:20.480]   an organization that is helping to advance robotics
[00:03:20.480 --> 00:03:23.400]   and STEM education for young people around the world.
[00:03:23.400 --> 00:03:27.980]   And now, here's my conversation with David Patterson.
[00:03:27.980 --> 00:03:31.720]   Let's start with the big historical question.
[00:03:31.720 --> 00:03:34.120]   How have computers changed in the past 50 years
[00:03:34.120 --> 00:03:36.600]   at both the fundamental architectural level
[00:03:36.600 --> 00:03:39.240]   and in general, in your eyes?
[00:03:39.240 --> 00:03:40.440]   - Well, the biggest thing that happened
[00:03:40.440 --> 00:03:43.040]   was the invention of the microprocessor.
[00:03:43.040 --> 00:03:46.360]   So computers that used to fill up several rooms
[00:03:46.360 --> 00:03:49.200]   could fit inside your cell phone.
[00:03:49.200 --> 00:03:53.400]   And not only did they get smaller,
[00:03:53.400 --> 00:03:54.740]   they got a lot faster.
[00:03:54.740 --> 00:03:57.240]   So they're a million times faster
[00:03:57.240 --> 00:03:59.640]   than they were 50 years ago,
[00:03:59.640 --> 00:04:02.560]   and they're much cheaper and they're ubiquitous.
[00:04:02.560 --> 00:04:07.600]   There's 7.8 billion people on this planet.
[00:04:07.600 --> 00:04:09.880]   Probably half of them have cell phones right now,
[00:04:09.880 --> 00:04:11.340]   which is remarkable.
[00:04:11.340 --> 00:04:14.080]   - There's probably more microprocessors
[00:04:14.080 --> 00:04:15.280]   than there are people.
[00:04:15.280 --> 00:04:16.980]   - Sure, I don't know what the ratio is,
[00:04:16.980 --> 00:04:18.500]   but I'm sure it's above one.
[00:04:19.420 --> 00:04:22.100]   Maybe it's 10 to one or some number like that.
[00:04:22.100 --> 00:04:24.580]   - What is a microprocessor?
[00:04:24.580 --> 00:04:27.740]   - So a way to say what a microprocessor is
[00:04:27.740 --> 00:04:29.540]   is to tell you what's inside a computer.
[00:04:29.540 --> 00:04:34.040]   So a computer forever has classically had five pieces.
[00:04:34.040 --> 00:04:35.380]   There's input and output,
[00:04:35.380 --> 00:04:37.380]   which kind of naturally, as you'd expect,
[00:04:37.380 --> 00:04:40.540]   is input is like speech or typing,
[00:04:40.540 --> 00:04:42.140]   and output is displays.
[00:04:42.140 --> 00:04:48.420]   There's a memory, and like the name sounds,
[00:04:48.420 --> 00:04:50.220]   it remembers things.
[00:04:50.220 --> 00:04:53.260]   So it's integrated circuits whose job is
[00:04:53.260 --> 00:04:54.460]   you put information in,
[00:04:54.460 --> 00:04:56.000]   then when you ask for it, it comes back out.
[00:04:56.000 --> 00:04:57.380]   That's memory.
[00:04:57.380 --> 00:04:59.580]   And then the third part is the processor,
[00:04:59.580 --> 00:05:01.900]   where the term microprocessor comes from.
[00:05:01.900 --> 00:05:04.580]   And that has two pieces as well,
[00:05:04.580 --> 00:05:06.140]   and that is the control,
[00:05:06.140 --> 00:05:10.100]   which is kind of the brain of the processor,
[00:05:10.100 --> 00:05:13.700]   and what's called the arithmetic unit.
[00:05:13.700 --> 00:05:15.460]   It's kind of the brawn of the computer.
[00:05:15.460 --> 00:05:17.720]   So if you think of the, as a human body,
[00:05:17.720 --> 00:05:19.220]   the arithmetic unit,
[00:05:19.220 --> 00:05:20.660]   the thing that does the number crunching
[00:05:20.660 --> 00:05:23.440]   is the body, and the control is the brain.
[00:05:23.440 --> 00:05:26.040]   So those five pieces, input, output, memory,
[00:05:26.040 --> 00:05:30.700]   arithmetic unit, and control
[00:05:30.700 --> 00:05:33.460]   have been in computers since the very dawn,
[00:05:33.460 --> 00:05:36.540]   and the last two are considered the processor.
[00:05:36.540 --> 00:05:38.820]   So a microprocessor simply means
[00:05:38.820 --> 00:05:40.980]   a processor that fits on a microchip,
[00:05:40.980 --> 00:05:44.660]   and that was invented about 40 years ago,
[00:05:44.660 --> 00:05:46.460]   was the first microprocessor.
[00:05:46.460 --> 00:05:48.660]   - It's interesting that you refer to the arithmetic unit
[00:05:48.660 --> 00:05:52.340]   as the, like you connect it to the body,
[00:05:52.340 --> 00:05:54.240]   and the control is the brain.
[00:05:54.240 --> 00:05:56.420]   So I guess, I never thought of it that way.
[00:05:56.420 --> 00:05:57.800]   It's a nice way to think of it,
[00:05:57.800 --> 00:06:01.900]   because most of the actions the microprocessor does
[00:06:01.900 --> 00:06:05.820]   in terms of literally sort of computation,
[00:06:05.820 --> 00:06:07.500]   microprocessor does computation.
[00:06:07.500 --> 00:06:09.060]   It processes information.
[00:06:09.060 --> 00:06:10.860]   And most of the thing it does
[00:06:10.860 --> 00:06:14.480]   is basic arithmetic operations.
[00:06:14.480 --> 00:06:16.360]   What are the operations, by the way?
[00:06:16.360 --> 00:06:17.900]   - It's a lot like a calculator.
[00:06:17.900 --> 00:06:21.820]   So there are add instructions,
[00:06:21.820 --> 00:06:24.340]   subtract instructions, multiply and divide.
[00:06:24.340 --> 00:06:28.660]   And kind of the brilliance of the invention
[00:06:28.660 --> 00:06:33.040]   of the computer or the processor
[00:06:33.040 --> 00:06:36.080]   is that it performs very trivial operations,
[00:06:36.080 --> 00:06:39.220]   but it just performs billions of them per second.
[00:06:39.220 --> 00:06:42.740]   And what we're capable of doing is writing software
[00:06:42.740 --> 00:06:45.540]   that can take these very trivial instructions
[00:06:45.540 --> 00:06:48.280]   and have them create tasks that can do things better
[00:06:48.280 --> 00:06:50.460]   than human beings can do today.
[00:06:50.460 --> 00:06:52.560]   - Just looking back through your career,
[00:06:52.560 --> 00:06:54.860]   did you anticipate the kind of how good
[00:06:54.860 --> 00:06:56.200]   we would be able to get
[00:06:56.200 --> 00:06:59.400]   at doing these small basic operations?
[00:06:59.400 --> 00:07:03.020]   How many surprises along the way
[00:07:03.020 --> 00:07:05.900]   where you just kind of sat back and said,
[00:07:05.900 --> 00:07:09.840]   wow, I didn't expect it to go this fast, this good?
[00:07:09.840 --> 00:07:12.740]   - Well, the fundamental driving force
[00:07:12.740 --> 00:07:15.380]   is what's called Moore's Law,
[00:07:15.380 --> 00:07:17.700]   which was named after Gordon Moore,
[00:07:17.700 --> 00:07:20.020]   who's a Berkeley alumnus.
[00:07:20.020 --> 00:07:22.820]   And he made this observation very early
[00:07:22.820 --> 00:07:24.420]   in what are called semiconductors.
[00:07:24.420 --> 00:07:26.300]   And semiconductors are these ideas,
[00:07:26.300 --> 00:07:29.300]   you can build these very simple switches
[00:07:29.300 --> 00:07:31.260]   and you can put them on these microchips.
[00:07:31.260 --> 00:07:34.420]   And he made this observation over 50 years ago.
[00:07:34.420 --> 00:07:36.060]   He looked at a few years and said,
[00:07:36.060 --> 00:07:37.460]   I think what's going to happen
[00:07:37.460 --> 00:07:40.660]   is the number of these little switches called transistors
[00:07:40.660 --> 00:07:44.300]   is going to double every year for the next decade.
[00:07:44.300 --> 00:07:46.260]   And he said this in 1965.
[00:07:46.260 --> 00:07:47.660]   And in 1975, he said,
[00:07:47.660 --> 00:07:50.440]   well, maybe it's going to double every two years.
[00:07:50.440 --> 00:07:55.440]   And that what other people since named that Moore's Law
[00:07:55.440 --> 00:07:57.640]   guided the industry.
[00:07:57.640 --> 00:07:59.440]   And when Gordon Moore made that prediction,
[00:07:59.440 --> 00:08:04.440]   he wrote a paper back in, I think in the 70s
[00:08:04.440 --> 00:08:08.900]   and said, not only did this going to happen,
[00:08:08.900 --> 00:08:10.860]   he wrote, what would be the implications of that?
[00:08:10.860 --> 00:08:13.300]   And in this article from 1965,
[00:08:13.300 --> 00:08:17.700]   he shows ideas like computers being in cars
[00:08:17.700 --> 00:08:21.700]   and computers being in something that you would buy
[00:08:21.700 --> 00:08:23.220]   in the grocery store and stuff like that.
[00:08:23.220 --> 00:08:26.460]   So he kind of not only called his shot,
[00:08:26.460 --> 00:08:28.340]   he called the implications of it.
[00:08:28.340 --> 00:08:30.920]   So if you were in the computing field
[00:08:30.920 --> 00:08:33.260]   and if you believed Moore's prediction,
[00:08:33.260 --> 00:08:36.580]   he kind of said what would be happening in the future.
[00:08:36.580 --> 00:08:41.540]   So it's not kind of, it's at one sense,
[00:08:41.540 --> 00:08:43.140]   this is what was predicted.
[00:08:43.140 --> 00:08:44.860]   And you could imagine,
[00:08:44.860 --> 00:08:47.100]   it was easy to believe that Moore's Law was going to continue
[00:08:47.100 --> 00:08:49.540]   and so this would be the implications.
[00:08:49.540 --> 00:08:50.540]   On the other side,
[00:08:50.540 --> 00:08:53.420]   there are these kind of shocking events in your life.
[00:08:53.420 --> 00:08:57.580]   Like I remember driving in a Marine
[00:08:57.580 --> 00:08:59.640]   across the bay in San Francisco
[00:08:59.640 --> 00:09:03.700]   and seeing a bulletin board at a local civic center
[00:09:03.700 --> 00:09:05.220]   and it had a URL on it.
[00:09:05.220 --> 00:09:09.580]   And it was like, for the people at the time,
[00:09:09.580 --> 00:09:13.620]   these first URLs and that's the www select stuff
[00:09:13.620 --> 00:09:18.620]   with the HGP, people thought it looked like alien writing.
[00:09:18.620 --> 00:09:23.620]   They'd see these advertisements and commercials
[00:09:23.620 --> 00:09:25.540]   or bulletin boards that had this alien writing on it.
[00:09:25.540 --> 00:09:26.700]   So for the lay people, it was like,
[00:09:26.700 --> 00:09:28.380]   what the hell is going on here?
[00:09:28.380 --> 00:09:31.240]   And for those people in industry, it was, oh my God,
[00:09:31.240 --> 00:09:33.580]   this stuff is getting so popular,
[00:09:33.580 --> 00:09:36.560]   it's actually leaking out of our nerdy world
[00:09:36.560 --> 00:09:37.940]   into the real world.
[00:09:37.940 --> 00:09:39.620]   So that, I mean, there was events like that.
[00:09:39.620 --> 00:09:41.060]   I think another one was,
[00:09:41.060 --> 00:09:44.080]   I remember in the early days of the personal computer,
[00:09:44.080 --> 00:09:46.940]   when we started seeing advertisements in magazines
[00:09:46.940 --> 00:09:48.180]   for personal computers,
[00:09:48.180 --> 00:09:51.140]   like it's so popular that it's made the newspapers.
[00:09:51.140 --> 00:09:54.380]   So at one hand, Gordon Moore predicted it
[00:09:54.380 --> 00:09:55.860]   and you kind of expected it to happen,
[00:09:55.860 --> 00:09:58.620]   but when it really hit and you saw it affecting society,
[00:09:58.620 --> 00:10:02.580]   it was shocking.
[00:10:02.580 --> 00:10:04.940]   - So maybe taking a step back and looking
[00:10:04.940 --> 00:10:08.140]   at both the engineering and philosophical perspective,
[00:10:08.140 --> 00:10:12.500]   what do you see as the layers of abstraction in a computer?
[00:10:12.500 --> 00:10:16.460]   Do you see a computer as a set of layers of abstractions?
[00:10:16.460 --> 00:10:18.440]   - Yeah, I think that's one of the things
[00:10:18.440 --> 00:10:21.820]   that computer science fundamentals,
[00:10:21.820 --> 00:10:24.380]   is these things are really complicated
[00:10:24.380 --> 00:10:27.460]   in the way we cope with complicated software
[00:10:27.460 --> 00:10:30.060]   and complicated hardware, is these layers of abstraction.
[00:10:30.060 --> 00:10:33.380]   And that simply means that we,
[00:10:34.460 --> 00:10:37.100]   suspend disbelief and pretend
[00:10:37.100 --> 00:10:39.660]   that the only thing you know is that layer
[00:10:39.660 --> 00:10:42.060]   and you don't know anything about the layer below it.
[00:10:42.060 --> 00:10:45.020]   And that's the way we can make very complicated things.
[00:10:45.020 --> 00:10:48.500]   And probably it started with hardware,
[00:10:48.500 --> 00:10:49.900]   that that's the way it was done,
[00:10:49.900 --> 00:10:52.540]   but it's been proven extremely useful.
[00:10:52.540 --> 00:10:55.620]   And I would think in a modern computer today,
[00:10:55.620 --> 00:10:59.140]   there might be 10 or 20 layers of abstraction.
[00:10:59.140 --> 00:11:01.740]   And they're all trying to kind of enforce this contract
[00:11:01.740 --> 00:11:05.140]   is all you know is this interface.
[00:11:05.140 --> 00:11:08.980]   There's a set of commands that you can,
[00:11:08.980 --> 00:11:11.220]   are allowed to use and you stick to those commands
[00:11:11.220 --> 00:11:12.980]   and we will faithfully execute that.
[00:11:12.980 --> 00:11:16.340]   And it's like peeling the layers of a onion,
[00:11:16.340 --> 00:11:19.260]   you get down, there's a new set of layers and so forth.
[00:11:19.260 --> 00:11:23.120]   So for people who wanna study computer science,
[00:11:23.120 --> 00:11:27.180]   the exciting part about it is you can
[00:11:27.180 --> 00:11:28.420]   keep peeling those layers.
[00:11:28.420 --> 00:11:31.420]   You take your first course and you might learn to program
[00:11:31.420 --> 00:11:34.700]   in Python and then you can take a follow on course
[00:11:34.700 --> 00:11:37.860]   and you can get it down to a lower level language like C
[00:11:37.860 --> 00:11:40.100]   and you can go and then you can,
[00:11:40.100 --> 00:11:41.380]   if you want to, you can start getting
[00:11:41.380 --> 00:11:44.380]   into the hardware layers and you keep getting down
[00:11:44.380 --> 00:11:47.160]   all the way to that transistor that I talked about
[00:11:47.160 --> 00:11:49.140]   that Gordon Moore predicted.
[00:11:49.140 --> 00:11:52.380]   And you can understand all those layers all the way up
[00:11:52.380 --> 00:11:54.820]   to the highest level application software.
[00:11:54.820 --> 00:11:59.820]   So it's a very kind of magnetic field.
[00:12:00.620 --> 00:12:03.940]   If you're interested, you can go into any depth
[00:12:03.940 --> 00:12:05.180]   and keep going.
[00:12:05.180 --> 00:12:07.220]   In particular, what's happening right now
[00:12:07.220 --> 00:12:09.700]   or it's happened in software last 20 years
[00:12:09.700 --> 00:12:11.120]   and recently in hardware,
[00:12:11.120 --> 00:12:13.100]   there's getting to be open source versions
[00:12:13.100 --> 00:12:14.280]   of all of these things.
[00:12:14.280 --> 00:12:18.260]   So what open source means is what the engineer,
[00:12:18.260 --> 00:12:22.380]   the programmer designs, it's not secret
[00:12:22.380 --> 00:12:24.140]   the belonging to a company,
[00:12:24.140 --> 00:12:27.340]   it's out there on the worldwide web so you can see it.
[00:12:27.340 --> 00:12:32.180]   So you can look at for lots of pieces of software
[00:12:32.180 --> 00:12:35.340]   that you use, you can see exactly what the programmer does
[00:12:35.340 --> 00:12:37.840]   if you want to get involved.
[00:12:37.840 --> 00:12:39.980]   That used to stop at the hardware.
[00:12:39.980 --> 00:12:43.540]   Recently, there's been an efforts to make
[00:12:43.540 --> 00:12:46.420]   open source hardware and those interfaces open
[00:12:46.420 --> 00:12:47.260]   so you can see that.
[00:12:47.260 --> 00:12:49.400]   So instead of before you had to stop at the hardware,
[00:12:49.400 --> 00:12:52.580]   you can now start going layer by layer below that
[00:12:52.580 --> 00:12:53.960]   and see what's inside there.
[00:12:53.960 --> 00:12:57.320]   So it's a remarkable time that for the interested
[00:12:57.520 --> 00:13:00.160]   individual can really see in great depth
[00:13:00.160 --> 00:13:02.120]   what's really going on in the computers
[00:13:02.120 --> 00:13:05.440]   that power everything that we see around us.
[00:13:05.440 --> 00:13:07.720]   - Are you thinking also when you say open source
[00:13:07.720 --> 00:13:10.800]   at the hardware level, is this going to the
[00:13:10.800 --> 00:13:13.960]   design architecture instruction set level
[00:13:13.960 --> 00:13:18.880]   or is it going to literally the
[00:13:18.880 --> 00:13:23.200]   manufacturer of the actual hardware,
[00:13:23.200 --> 00:13:25.120]   of the actual chips, whether that's ASICs,
[00:13:25.120 --> 00:13:27.600]   specialized to a particular domain or the general?
[00:13:27.600 --> 00:13:30.120]   - Yeah, so let's talk about that a little bit.
[00:13:30.120 --> 00:13:35.120]   So when you get down to the bottom layer of software,
[00:13:35.120 --> 00:13:40.480]   the way software talks to hardware is in a vocabulary.
[00:13:40.480 --> 00:13:43.760]   And what we call that vocabulary, we call that,
[00:13:43.760 --> 00:13:47.640]   the words of that vocabulary are called instructions.
[00:13:47.640 --> 00:13:50.120]   And the technical term for the vocabulary
[00:13:50.120 --> 00:13:51.960]   is instruction set.
[00:13:51.960 --> 00:13:54.400]   So those instructions are like what we talked about earlier,
[00:13:54.400 --> 00:13:56.600]   they can be instructions like add, subtract,
[00:13:56.600 --> 00:13:58.000]   and multiply, divide.
[00:13:58.000 --> 00:14:01.840]   There's instructions to put data into memory,
[00:14:01.840 --> 00:14:03.400]   which is called a store instruction,
[00:14:03.400 --> 00:14:05.640]   and to get data back, which is called a load instructions.
[00:14:05.640 --> 00:14:08.800]   And those simple instructions go back
[00:14:08.800 --> 00:14:10.240]   to the very dawn of computing.
[00:14:10.240 --> 00:14:14.760]   In 1950, the commercial computer had these instructions.
[00:14:14.760 --> 00:14:17.720]   So that's the instruction set that we're talking about.
[00:14:17.720 --> 00:14:20.640]   So up until I'd say 10 years ago,
[00:14:20.640 --> 00:14:22.860]   these instruction sets were all proprietary.
[00:14:22.860 --> 00:14:27.120]   So a very popular one is owned by Intel,
[00:14:27.120 --> 00:14:28.760]   the one that's in the cloud
[00:14:28.760 --> 00:14:30.760]   and in all the PCs in the world.
[00:14:30.760 --> 00:14:32.480]   Intel owns that instruction set.
[00:14:32.480 --> 00:14:35.560]   It's referred to as the x86.
[00:14:35.560 --> 00:14:37.160]   There've been a sequence of ones
[00:14:37.160 --> 00:14:39.800]   that the first number was called 8086.
[00:14:39.800 --> 00:14:42.080]   And since then, there's been a lot of numbers,
[00:14:42.080 --> 00:14:43.240]   but they all end in 86.
[00:14:43.240 --> 00:14:48.040]   So there's been that kind of family of instruction sets.
[00:14:48.040 --> 00:14:49.520]   - And that's proprietary.
[00:14:49.520 --> 00:14:50.760]   - And that's proprietary.
[00:14:50.780 --> 00:14:54.500]   The other one that's very popular is from ARM.
[00:14:54.500 --> 00:14:57.740]   That kind of powers all the cell phones in the world,
[00:14:57.740 --> 00:14:59.500]   all the iPads in the world,
[00:14:59.500 --> 00:15:02.180]   and a lot of things that are so-called
[00:15:02.180 --> 00:15:04.700]   internet of things devices.
[00:15:04.700 --> 00:15:08.140]   ARM and that one is also proprietary.
[00:15:08.140 --> 00:15:12.380]   ARM will license it to people for a fee, but they own that.
[00:15:12.380 --> 00:15:15.260]   So the new idea that got started at Berkeley
[00:15:15.260 --> 00:15:17.780]   kind of unintentionally 10 years ago
[00:15:17.780 --> 00:15:21.180]   is early in my career,
[00:15:21.180 --> 00:15:25.860]   we pioneered a way to do these vocabularies instruction sets
[00:15:25.860 --> 00:15:28.220]   that was very controversial at the time.
[00:15:28.220 --> 00:15:30.340]   At the time in the 1980s,
[00:15:30.340 --> 00:15:32.460]   conventional wisdom was these
[00:15:32.460 --> 00:15:33.900]   vocabularies instruction sets
[00:15:33.900 --> 00:15:36.500]   should have powerful instructions.
[00:15:36.500 --> 00:15:40.360]   So polysyllabic kind of words, you can think of that.
[00:15:40.360 --> 00:15:43.660]   And so that instead of just add, subtract and multiply,
[00:15:43.660 --> 00:15:47.980]   they would have polynomial divide or sort a list.
[00:15:47.980 --> 00:15:51.340]   And the hope was of those powerful vocabularies
[00:15:51.340 --> 00:15:52.980]   that make it easier for software.
[00:15:52.980 --> 00:15:57.740]   So we thought that didn't make sense for microprocessors.
[00:15:57.740 --> 00:16:00.620]   There was people at Berkeley and Stanford and IBM
[00:16:00.620 --> 00:16:02.060]   who argued the opposite.
[00:16:02.060 --> 00:16:06.340]   And we called that was a reduced instruction set computer.
[00:16:06.340 --> 00:16:09.420]   And the abbreviation was RISC
[00:16:09.420 --> 00:16:10.940]   and typical for computer people,
[00:16:10.940 --> 00:16:13.300]   we use the abbreviations that are pronouncing it.
[00:16:13.300 --> 00:16:14.860]   So RISC was the thing.
[00:16:14.860 --> 00:16:17.180]   So we said for microprocessors,
[00:16:17.180 --> 00:16:20.340]   which with Gordon's more is changing really fast.
[00:16:20.340 --> 00:16:22.620]   We think it's better to have a pretty simple
[00:16:22.620 --> 00:16:26.400]   set of instructions, reduced set of instructions.
[00:16:26.400 --> 00:16:29.700]   That that would be a better way to build microprocessors
[00:16:29.700 --> 00:16:32.620]   since they're gonna be changing so fast due to Moore's law.
[00:16:32.620 --> 00:16:36.740]   And then we'll just use standard software
[00:16:36.740 --> 00:16:40.980]   to generate more of those simple instructions.
[00:16:40.980 --> 00:16:43.540]   And one of the pieces of software
[00:16:43.540 --> 00:16:45.280]   that's in that software stack
[00:16:45.280 --> 00:16:47.100]   going between these layers of abstractions
[00:16:47.100 --> 00:16:48.180]   is called a compiler.
[00:16:48.180 --> 00:16:50.100]   And it's basically translates.
[00:16:50.100 --> 00:16:51.420]   It's a translator between levels.
[00:16:51.420 --> 00:16:53.380]   We said the translator will handle that.
[00:16:53.380 --> 00:16:55.340]   So the technical question was,
[00:16:55.340 --> 00:16:59.320]   well, since there are these reduced instructions,
[00:16:59.320 --> 00:17:01.060]   you have to execute more of them.
[00:17:01.060 --> 00:17:02.340]   Yeah, that's right.
[00:17:02.340 --> 00:17:04.100]   But maybe you execute them faster.
[00:17:04.100 --> 00:17:04.940]   Yeah, that's right.
[00:17:04.940 --> 00:17:06.500]   They're simpler so they could go faster,
[00:17:06.500 --> 00:17:07.380]   but you have to do more of them.
[00:17:07.380 --> 00:17:10.580]   So what's that trade off look like?
[00:17:10.580 --> 00:17:13.420]   And it ended up that we ended up executing
[00:17:13.420 --> 00:17:16.300]   maybe 50% more instructions,
[00:17:16.300 --> 00:17:17.940]   maybe a third more instructions,
[00:17:17.940 --> 00:17:19.620]   but they ran four times faster.
[00:17:19.620 --> 00:17:23.900]   So this risk, controversial risk ideas
[00:17:23.900 --> 00:17:26.760]   proved to be maybe factors of three or four better.
[00:17:26.760 --> 00:17:29.980]   - I love that this idea was controversial
[00:17:29.980 --> 00:17:32.640]   and almost kind of like rebellious.
[00:17:32.640 --> 00:17:36.660]   So that's in the context of what was more conventional
[00:17:36.660 --> 00:17:39.180]   is the complex instructional set computing.
[00:17:39.180 --> 00:17:41.260]   So how'd you pronounce that?
[00:17:41.260 --> 00:17:42.100]   - CISC.
[00:17:42.100 --> 00:17:43.060]   - CISC, which is risk.
[00:17:43.060 --> 00:17:44.060]   - Risk versus CISC.
[00:17:44.060 --> 00:17:48.060]   And believe it or not, this sounds very,
[00:17:48.060 --> 00:17:50.420]   who cares about this, right?
[00:17:50.420 --> 00:17:54.780]   It was violently debated at several conferences.
[00:17:54.780 --> 00:17:57.060]   It's like, what's the right way to go?
[00:17:57.060 --> 00:18:01.140]   And people thought risk was a de-evolution.
[00:18:01.140 --> 00:18:02.500]   We're gonna make software worse
[00:18:02.500 --> 00:18:04.580]   by making those instructions simpler.
[00:18:04.580 --> 00:18:06.820]   And there are fierce debates
[00:18:06.820 --> 00:18:09.340]   at several conferences in the 1980s.
[00:18:09.340 --> 00:18:11.140]   And then later in the '80s,
[00:18:11.140 --> 00:18:14.620]   it kind of settled to these benefits.
[00:18:14.620 --> 00:18:16.120]   - It's not completely intuitive to me
[00:18:16.120 --> 00:18:18.780]   why risk has, for the most part, won.
[00:18:18.780 --> 00:18:21.780]   - Yeah, so why did that happen?
[00:18:21.780 --> 00:18:23.380]   - Yeah, yeah, and maybe I can sort of say
[00:18:23.380 --> 00:18:25.460]   a bunch of dumb things that could lay the land
[00:18:25.460 --> 00:18:27.060]   for further commentary.
[00:18:27.060 --> 00:18:30.780]   So to me, this is kind of interesting thing.
[00:18:30.780 --> 00:18:33.340]   If you look at C++ versus C,
[00:18:33.340 --> 00:18:34.940]   with modern compilers,
[00:18:34.940 --> 00:18:38.660]   you really could write faster code with C++.
[00:18:38.660 --> 00:18:42.940]   So relying on the compiler to reduce your complicated code
[00:18:42.940 --> 00:18:44.900]   into something simple and fast.
[00:18:44.900 --> 00:18:48.580]   So to me, comparing risk,
[00:18:48.580 --> 00:18:50.020]   maybe this is a dumb question,
[00:18:50.020 --> 00:18:54.060]   but why is it that focusing the definition,
[00:18:54.060 --> 00:18:55.820]   the design of the instruction set
[00:18:55.820 --> 00:18:58.260]   on very few simple instructions
[00:18:58.260 --> 00:19:02.980]   in the long run provide faster execution
[00:19:02.980 --> 00:19:06.260]   versus coming up with, like you said,
[00:19:06.260 --> 00:19:10.060]   a ton of complicated instructions
[00:19:10.060 --> 00:19:13.940]   that over time, years, maybe decades,
[00:19:13.940 --> 00:19:15.260]   you come up with compilers
[00:19:15.260 --> 00:19:19.180]   that can reduce those into simple instructions for you?
[00:19:19.180 --> 00:19:22.660]   - Yeah, so let's try and split that into two pieces.
[00:19:22.660 --> 00:19:26.260]   So if the compiler can do that for you,
[00:19:26.260 --> 00:19:29.980]   if the compiler can take a complicated program
[00:19:29.980 --> 00:19:32.140]   and produce simpler instructions,
[00:19:33.140 --> 00:19:35.780]   then the programmer doesn't care, right?
[00:19:35.780 --> 00:19:39.900]   Programmer, I don't care just how fast is the computer
[00:19:39.900 --> 00:19:41.860]   I'm using, how much does it cost?
[00:19:41.860 --> 00:19:46.380]   And so what happened kind of in the software industry
[00:19:46.380 --> 00:19:48.700]   is right around before the 1980s,
[00:19:48.700 --> 00:19:51.660]   critical pieces of software were still written
[00:19:51.660 --> 00:19:55.540]   not in languages like C or C++.
[00:19:55.540 --> 00:19:58.180]   They were written in what's called assembly language,
[00:19:58.180 --> 00:20:01.660]   where there's this kind of humans writing exactly
[00:20:01.660 --> 00:20:03.900]   at the instructions at the level
[00:20:03.900 --> 00:20:05.900]   that a computer can understand.
[00:20:05.900 --> 00:20:10.460]   So they were writing add, subtract, multiply instructions.
[00:20:10.460 --> 00:20:14.060]   It's very tedious, but the belief was to write
[00:20:14.060 --> 00:20:17.580]   this lowest level of software that people use,
[00:20:17.580 --> 00:20:18.860]   which are called operating systems,
[00:20:18.860 --> 00:20:21.020]   they had to be written in assembly language
[00:20:21.020 --> 00:20:24.340]   because these high-level languages were just too inefficient.
[00:20:24.340 --> 00:20:28.540]   They were too slow or the programs would be too big.
[00:20:29.620 --> 00:20:34.020]   So that changed with a famous operating system called Unix,
[00:20:34.020 --> 00:20:36.340]   which is kind of the grandfather
[00:20:36.340 --> 00:20:38.740]   of all the operating systems today.
[00:20:38.740 --> 00:20:41.940]   So Unix demonstrated that you could write
[00:20:41.940 --> 00:20:44.060]   something as complicated as an operating system
[00:20:44.060 --> 00:20:46.040]   in a language like C.
[00:20:46.040 --> 00:20:48.580]   So once that was true,
[00:20:48.580 --> 00:20:51.980]   then that meant we could hide the instruction set
[00:20:51.980 --> 00:20:53.640]   from the programmer.
[00:20:53.640 --> 00:20:57.140]   And so that meant then it didn't really matter.
[00:20:57.140 --> 00:20:59.260]   The programmer didn't have to write
[00:20:59.260 --> 00:21:00.940]   lots of these simple instructions.
[00:21:00.940 --> 00:21:02.260]   That was up to the compiler.
[00:21:02.260 --> 00:21:04.180]   So that was part of our arguments for risk,
[00:21:04.180 --> 00:21:06.580]   is if you were still writing in assembly language,
[00:21:06.580 --> 00:21:09.540]   there's maybe a better case for CISC instructions.
[00:21:09.540 --> 00:21:11.260]   But if the compiler can do that,
[00:21:11.260 --> 00:21:14.140]   it's gonna be, that's done once.
[00:21:14.140 --> 00:21:15.940]   The computer translates it once,
[00:21:15.940 --> 00:21:17.780]   and then every time you run the program,
[00:21:17.780 --> 00:21:21.060]   it runs at this potentially simpler instructions.
[00:21:21.060 --> 00:21:24.340]   And so that was the debate, right?
[00:21:25.540 --> 00:21:29.340]   And people would acknowledge that the simpler instructions
[00:21:29.340 --> 00:21:30.980]   could lead to a faster computer.
[00:21:30.980 --> 00:21:33.580]   You can think of monosyllabic instructions.
[00:21:33.580 --> 00:21:35.660]   You could say them, if you think of reading,
[00:21:35.660 --> 00:21:36.900]   you could probably read them faster
[00:21:36.900 --> 00:21:39.060]   or say them faster than long instructions.
[00:21:39.060 --> 00:21:42.700]   The same thing, that analogy works pretty well for hardware.
[00:21:42.700 --> 00:21:44.680]   And as long as you didn't have to read
[00:21:44.680 --> 00:21:47.000]   a lot more of those instructions, you could win.
[00:21:47.000 --> 00:21:50.340]   So that's the basic idea for risk.
[00:21:50.340 --> 00:21:54.340]   - But it's interesting that in that discussion of Unix and C
[00:21:54.340 --> 00:21:59.020]   that there's only one step of levels of abstraction
[00:21:59.020 --> 00:22:03.140]   from the code that's really the closest to the machine
[00:22:03.140 --> 00:22:05.500]   to the code that's written by human.
[00:22:05.500 --> 00:22:09.960]   It's, at least to me again, perhaps a dumb intuition,
[00:22:09.960 --> 00:22:13.440]   but it feels like there might have been more layers,
[00:22:13.440 --> 00:22:15.340]   sort of different kinds of humans
[00:22:15.340 --> 00:22:17.380]   stacked on top of each other.
[00:22:17.380 --> 00:22:21.140]   - So what's true and not true about what you said
[00:22:21.140 --> 00:22:26.140]   is several of the layers of software,
[00:22:26.140 --> 00:22:31.380]   like, so if you, two layers would be,
[00:22:31.380 --> 00:22:32.700]   suppose we just talk about two layers.
[00:22:32.700 --> 00:22:34.060]   That would be the operating system,
[00:22:34.060 --> 00:22:37.460]   like you get from Microsoft or from Apple,
[00:22:37.460 --> 00:22:41.260]   like iOS or the Windows operating system.
[00:22:41.260 --> 00:22:43.580]   And let's say applications that run on top of it,
[00:22:43.580 --> 00:22:46.020]   like Word or Excel.
[00:22:46.020 --> 00:22:50.520]   So both the operating system could be written in C,
[00:22:51.160 --> 00:22:53.440]   and the application could be written in C.
[00:22:53.440 --> 00:22:56.520]   But you could construct those two layers
[00:22:56.520 --> 00:22:58.840]   and the applications absolutely do call up
[00:22:58.840 --> 00:23:00.400]   on the operating system.
[00:23:00.400 --> 00:23:03.000]   And the change was that both of them
[00:23:03.000 --> 00:23:04.940]   could be written in higher level languages.
[00:23:04.940 --> 00:23:07.080]   So it's one step of a translation,
[00:23:07.080 --> 00:23:10.360]   but you can still build many layers of abstraction
[00:23:10.360 --> 00:23:11.760]   of software on top of that.
[00:23:11.760 --> 00:23:13.640]   And that's how things are done today.
[00:23:13.640 --> 00:23:18.640]   So still today, many of the layers that you'll deal with,
[00:23:19.480 --> 00:23:24.040]   you may deal with debuggers, you may deal with linkers.
[00:23:24.040 --> 00:23:27.000]   There's libraries.
[00:23:27.000 --> 00:23:31.200]   Many of those today will be written in C++,
[00:23:31.200 --> 00:23:35.000]   say, even though that language is pretty ancient.
[00:23:35.000 --> 00:23:38.880]   And even the Python interpreter is probably written
[00:23:38.880 --> 00:23:40.280]   in C or C++.
[00:23:40.280 --> 00:23:44.360]   So lots of layers there are probably written in these,
[00:23:44.360 --> 00:23:47.260]   some old fashioned efficient languages
[00:23:47.260 --> 00:23:52.260]   that still take one step to produce these instructions,
[00:23:52.260 --> 00:23:54.820]   produce RISC instructions,
[00:23:54.820 --> 00:23:58.760]   but they're composed, each layer of software invokes
[00:23:58.760 --> 00:24:01.040]   one another through these interfaces,
[00:24:01.040 --> 00:24:04.360]   and you can get 10 layers of software that way.
[00:24:04.360 --> 00:24:07.520]   - So in general, the RISC was developed here at Berkeley?
[00:24:07.520 --> 00:24:11.700]   - It was kind of the three places that were these radicals
[00:24:11.700 --> 00:24:14.600]   that advocated for this against the rest of the community
[00:24:14.600 --> 00:24:16.900]   were IBM, Berkeley, and Stanford.
[00:24:16.900 --> 00:24:20.540]   - You're one of these radicals,
[00:24:20.540 --> 00:24:24.460]   and how radical did you feel?
[00:24:24.460 --> 00:24:26.460]   How confident did you feel?
[00:24:26.460 --> 00:24:31.460]   How doubtful were you that RISC might be the right approach?
[00:24:31.460 --> 00:24:33.820]   'Cause it may, you can also, into it,
[00:24:33.820 --> 00:24:36.700]   that is kind of taking a step back into simplicity,
[00:24:36.700 --> 00:24:38.780]   not forward into simplicity.
[00:24:38.780 --> 00:24:42.620]   - Yeah, no, it was easy to make, yeah.
[00:24:42.620 --> 00:24:44.020]   It was easy to make the argument against it.
[00:24:44.020 --> 00:24:48.140]   Well, this was my colleague, John Hennessy at Stanford,
[00:24:48.140 --> 00:24:49.900]   and we were both assistant professors,
[00:24:49.900 --> 00:24:54.900]   and for me, I just believed in the power of our ideas.
[00:24:54.900 --> 00:24:57.140]   I thought what we were saying made sense.
[00:24:57.140 --> 00:24:58.900]   Moore's law is gonna move fast.
[00:24:58.900 --> 00:25:01.700]   The other thing that I didn't mention
[00:25:01.700 --> 00:25:05.820]   is one of the surprises of these complex instruction sets.
[00:25:05.820 --> 00:25:08.420]   You could certainly write these complex instructions
[00:25:08.420 --> 00:25:11.020]   if the programmer is writing them themselves.
[00:25:11.020 --> 00:25:13.260]   It turned out to be kind of difficult
[00:25:13.260 --> 00:25:15.780]   for the compiler to generate those complex instructions.
[00:25:15.780 --> 00:25:17.980]   Kind of ironically, you'd have to find
[00:25:17.980 --> 00:25:20.740]   the right circumstances that just exactly
[00:25:20.740 --> 00:25:21.940]   fit this complex instruction.
[00:25:21.940 --> 00:25:23.740]   It was actually easier for the compiler
[00:25:23.740 --> 00:25:25.300]   to generate these simple instructions.
[00:25:25.300 --> 00:25:28.620]   So not only did these complex instructions
[00:25:28.620 --> 00:25:31.680]   make the hardware more difficult to build,
[00:25:31.680 --> 00:25:33.900]   often the compiler wouldn't even use them.
[00:25:33.900 --> 00:25:36.620]   And so it's harder to build.
[00:25:36.620 --> 00:25:39.180]   The compiler doesn't use them that much.
[00:25:39.180 --> 00:25:41.740]   The simple instructions go better with Moore's law.
[00:25:41.740 --> 00:25:45.460]   The number of transistors is doubling every two years,
[00:25:45.460 --> 00:25:47.780]   so we're gonna have, you know,
[00:25:47.780 --> 00:25:50.280]   you wanna reduce the time to design the microprocessor,
[00:25:50.280 --> 00:25:52.660]   that may be more important than the number of instructions.
[00:25:52.660 --> 00:25:55.980]   So I think we believed in the,
[00:25:55.980 --> 00:25:59.500]   that we were right, that this was the best idea.
[00:25:59.500 --> 00:26:01.700]   Then the question became in these debates,
[00:26:01.700 --> 00:26:04.020]   well, yeah, that's a good technical idea,
[00:26:04.020 --> 00:26:06.260]   but in the business world, this doesn't matter.
[00:26:06.260 --> 00:26:07.740]   There's other things that matter.
[00:26:07.740 --> 00:26:11.860]   It's like arguing that if there's a standard
[00:26:11.860 --> 00:26:13.540]   with the railroad tracks,
[00:26:13.540 --> 00:26:15.300]   and you've come up with a better width,
[00:26:15.300 --> 00:26:17.420]   but the whole world is covered in railroad tracks,
[00:26:17.420 --> 00:26:22.260]   so your ideas have no chance of success, commercial success.
[00:26:22.260 --> 00:26:23.140]   It was technically right,
[00:26:23.140 --> 00:26:25.640]   but commercially, it'll be insignificant.
[00:26:25.640 --> 00:26:28.660]   - Yeah, it's kind of sad that this world,
[00:26:28.660 --> 00:26:32.540]   the history of human civilization is full of good ideas
[00:26:32.540 --> 00:26:36.020]   that lost because somebody else came along first
[00:26:36.020 --> 00:26:37.700]   with a worse idea.
[00:26:37.700 --> 00:26:39.820]   And it's good that in the computing world,
[00:26:39.820 --> 00:26:42.140]   at least some of these have, well, you could,
[00:26:42.140 --> 00:26:45.340]   I mean, there's probably still CISC people that say--
[00:26:45.340 --> 00:26:46.660]   - Yeah, there still are.
[00:26:46.660 --> 00:26:47.860]   (laughing)
[00:26:47.860 --> 00:26:50.180]   And what happened was, what was interesting,
[00:26:50.180 --> 00:26:51.740]   Intel, a bunch of the CISC,
[00:26:51.740 --> 00:26:56.440]   companies with CISC instruction sets of vocabulary,
[00:26:56.440 --> 00:26:58.640]   they gave up, but not Intel.
[00:26:58.640 --> 00:27:01.100]   What Intel did, to its credit,
[00:27:01.100 --> 00:27:06.100]   because Intel's vocabulary was in the personal computer,
[00:27:07.260 --> 00:27:09.500]   and so that was a very valuable vocabulary
[00:27:09.500 --> 00:27:12.380]   because the way we distribute software
[00:27:12.380 --> 00:27:14.280]   is in those actual instructions.
[00:27:14.280 --> 00:27:16.260]   It's in the instructions of that instruction set.
[00:27:16.260 --> 00:27:19.260]   So you don't get that source code,
[00:27:19.260 --> 00:27:20.900]   what the programmers wrote,
[00:27:20.900 --> 00:27:24.340]   you get, after it's been translated into the lowest level,
[00:27:24.340 --> 00:27:26.300]   that's, if you were to get a floppy disk
[00:27:26.300 --> 00:27:27.220]   or download software,
[00:27:27.220 --> 00:27:29.400]   it's in the instructions of that instruction set.
[00:27:29.400 --> 00:27:33.420]   So the x86 instruction set was very valuable.
[00:27:33.420 --> 00:27:36.860]   So what Intel did cleverly and amazingly
[00:27:36.860 --> 00:27:41.860]   is they had their chips in hardware do a translation step.
[00:27:41.860 --> 00:27:43.940]   They would take these complex instructions
[00:27:43.940 --> 00:27:46.400]   and translate them into essentially in RISC instructions
[00:27:46.400 --> 00:27:51.300]   in hardware on the fly, at gigahertz clock speeds,
[00:27:51.300 --> 00:27:53.860]   and then any good idea that RISC people had,
[00:27:53.860 --> 00:27:56.620]   they could use, and they could still be compatible
[00:27:56.620 --> 00:28:01.620]   with this really valuable PC software base,
[00:28:01.620 --> 00:28:04.820]   which also had very high volumes,
[00:28:04.820 --> 00:28:07.140]   100 million personal computers per year.
[00:28:07.140 --> 00:28:11.460]   So the CISC architecture in the business world
[00:28:11.460 --> 00:28:15.300]   was actually one in this PC era.
[00:28:15.300 --> 00:28:22.180]   - So just going back to the time of designing RISC,
[00:28:22.180 --> 00:28:27.340]   when you design an instruction set architecture,
[00:28:27.340 --> 00:28:29.120]   do you think like a programmer?
[00:28:29.120 --> 00:28:32.380]   Do you think like a microprocessor engineer?
[00:28:32.380 --> 00:28:36.700]   Do you think like a artist, a philosopher?
[00:28:36.700 --> 00:28:38.860]   Do you think in software and hardware?
[00:28:38.860 --> 00:28:40.920]   I mean, is it art, is it science?
[00:28:40.920 --> 00:28:44.340]   - Yeah, I'd say, I think designing a good instruction set
[00:28:44.340 --> 00:28:48.640]   is an art, and I think you're trying to balance
[00:28:48.640 --> 00:28:54.260]   the simplicity and speed of execution
[00:28:54.260 --> 00:28:58.620]   with how well easy it will be for compilers to use it.
[00:28:58.620 --> 00:29:00.920]   You're trying to create an instruction set
[00:29:00.920 --> 00:29:04.620]   that everything in there can be used by compilers.
[00:29:04.620 --> 00:29:07.340]   There's not things that are missing
[00:29:07.340 --> 00:29:09.780]   that'll make it difficult for the program to run,
[00:29:09.780 --> 00:29:11.740]   they run efficiently,
[00:29:11.740 --> 00:29:13.620]   but you want it to be easy to build as well.
[00:29:13.620 --> 00:29:15.540]   So it's that kind of, so you're thinking,
[00:29:15.540 --> 00:29:16.920]   I'd say you're thinking hardware,
[00:29:16.920 --> 00:29:19.300]   trying to find a hardware software compromise
[00:29:19.300 --> 00:29:20.620]   that'll work well.
[00:29:20.620 --> 00:29:25.620]   And it's a matter of taste, right?
[00:29:25.620 --> 00:29:28.180]   It's kind of fun to build instruction sets.
[00:29:29.180 --> 00:29:31.280]   It's not that hard to build an instruction set,
[00:29:31.280 --> 00:29:35.480]   but to build one that catches on and people use,
[00:29:35.480 --> 00:29:38.680]   you have to be fortunate to be the right place
[00:29:38.680 --> 00:29:39.520]   at the right time,
[00:29:39.520 --> 00:29:41.940]   or have a design that people really like.
[00:29:41.940 --> 00:29:43.320]   - Are you using metrics?
[00:29:43.320 --> 00:29:46.320]   So is it quantifiable?
[00:29:46.320 --> 00:29:48.120]   Because you kind of have to anticipate
[00:29:48.120 --> 00:29:50.880]   the kind of programs that people write ahead of time.
[00:29:50.880 --> 00:29:54.400]   So is that, can you use numbers, can you use metrics,
[00:29:54.400 --> 00:29:56.720]   can you quantify something ahead of time,
[00:29:56.720 --> 00:29:58.400]   or is this, again, that's the art part
[00:29:58.400 --> 00:29:59.240]   where you're kind of anticipating?
[00:29:59.240 --> 00:30:03.400]   - No, it's a big change, kind of what happened,
[00:30:03.400 --> 00:30:07.120]   I think from Hennessy's and my perspective in the 1980s,
[00:30:07.120 --> 00:30:10.920]   what happened was going from kind of really,
[00:30:10.920 --> 00:30:16.640]   taste and hunches to quantifiable.
[00:30:16.640 --> 00:30:19.720]   And in fact, he and I wrote a textbook
[00:30:19.720 --> 00:30:21.280]   at the end of the 1980s called
[00:30:21.280 --> 00:30:23.600]   "Computer Architecture, A Quantitative Approach."
[00:30:23.600 --> 00:30:24.440]   - I heard of that.
[00:30:24.440 --> 00:30:27.080]   - And it's the thing,
[00:30:27.080 --> 00:30:30.160]   it had a pretty big impact in the field
[00:30:30.160 --> 00:30:33.920]   'cause we went from textbooks that kind of listed,
[00:30:33.920 --> 00:30:35.760]   so here's what this computer does,
[00:30:35.760 --> 00:30:36.960]   and here's the pros and cons,
[00:30:36.960 --> 00:30:38.640]   and here's what this computer does and pros and cons,
[00:30:38.640 --> 00:30:41.520]   to something where there were formulas and equations
[00:30:41.520 --> 00:30:42.440]   where you could measure things.
[00:30:42.440 --> 00:30:44.700]   So specifically for instruction sets,
[00:30:44.700 --> 00:30:49.640]   what we do and some other fields do
[00:30:49.640 --> 00:30:51.960]   is we agree upon a set of programs,
[00:30:51.960 --> 00:30:53.760]   which we call benchmarks,
[00:30:53.760 --> 00:30:56.100]   and a suite of programs,
[00:30:56.100 --> 00:31:00.160]   and then you develop both the hardware and the compiler,
[00:31:00.160 --> 00:31:05.160]   and you get numbers on how well your computer does,
[00:31:05.160 --> 00:31:07.720]   given its instruction set,
[00:31:07.720 --> 00:31:10.560]   and how well you implemented it in your microprocessor,
[00:31:10.560 --> 00:31:12.760]   and how good your compilers are.
[00:31:12.760 --> 00:31:14.680]   And in computer architecture,
[00:31:14.680 --> 00:31:16.720]   using professors' terms,
[00:31:16.720 --> 00:31:19.240]   we grade on a curve rather than grade on an absolute scale.
[00:31:19.240 --> 00:31:20.140]   So when you say,
[00:31:20.140 --> 00:31:22.820]   these programs run this fast,
[00:31:22.820 --> 00:31:23.940]   well, that's kind of interesting,
[00:31:23.940 --> 00:31:25.240]   but how do you know it's better?
[00:31:25.240 --> 00:31:28.560]   Well, you compare it to other computers of the same time.
[00:31:28.560 --> 00:31:31.200]   So the best way we know how to make,
[00:31:31.200 --> 00:31:34.560]   turn it into a kind of more science
[00:31:34.560 --> 00:31:36.320]   and experimental and quantitative
[00:31:36.320 --> 00:31:39.800]   is to compare yourself to other computers of the same era
[00:31:39.800 --> 00:31:40.880]   that have the same access,
[00:31:40.880 --> 00:31:42.560]   the same kind of technology,
[00:31:42.560 --> 00:31:45.060]   on commonly agreed benchmark programs.
[00:31:45.060 --> 00:31:49.160]   - So maybe to toss up two possible directions we can go,
[00:31:49.160 --> 00:31:51.560]   one is what are the different trade-offs
[00:31:51.560 --> 00:31:54.120]   in designing architectures?
[00:31:54.120 --> 00:31:56.000]   We've been already talking about CISC and RISC,
[00:31:56.000 --> 00:31:58.940]   but maybe a little bit more detail
[00:31:58.940 --> 00:32:02.200]   in terms of specific features that you were thinking about.
[00:32:02.200 --> 00:32:03.760]   And the other side is,
[00:32:03.760 --> 00:32:06.200]   what are the metrics that you're thinking about
[00:32:06.200 --> 00:32:08.280]   when looking at these trade-offs?
[00:32:08.280 --> 00:32:10.040]   - Yeah, let's talk about the metrics.
[00:32:10.040 --> 00:32:12.820]   So during these debates,
[00:32:12.820 --> 00:32:15.580]   we actually had kind of a hard time explaining,
[00:32:15.580 --> 00:32:17.040]   convincing people the ideas,
[00:32:17.040 --> 00:32:20.360]   and partly we didn't have a formula to explain it.
[00:32:20.360 --> 00:32:22.120]   And a few years into it,
[00:32:22.120 --> 00:32:24.760]   we hit upon the formula that helped explain
[00:32:24.760 --> 00:32:25.840]   what was going on.
[00:32:25.840 --> 00:32:28.760]   And I think if we can do this,
[00:32:28.760 --> 00:32:30.480]   see how it works orally to do this.
[00:32:30.480 --> 00:32:35.040]   So, let's see if I can do a formula orally.
[00:32:35.040 --> 00:32:39.760]   So fundamentally, the way you measure performance
[00:32:39.760 --> 00:32:42.440]   is how long does it take a program to run?
[00:32:42.440 --> 00:32:45.600]   Program, if you have 10 programs,
[00:32:45.600 --> 00:32:47.320]   and typically these benchmarks were sweet
[00:32:47.320 --> 00:32:48.880]   'cause you'd wanna have 10 programs
[00:32:48.880 --> 00:32:51.420]   so they could represent lots of different applications.
[00:32:51.420 --> 00:32:53.960]   So for these 10 programs, how long did it take to run?
[00:32:53.960 --> 00:32:55.960]   Well, now, when you're trying to explain
[00:32:55.960 --> 00:32:57.080]   why it took so long,
[00:32:57.080 --> 00:33:00.000]   you could factor how long it takes a program to run
[00:33:00.000 --> 00:33:01.560]   into three factors.
[00:33:01.560 --> 00:33:06.040]   One of the first one is how many instructions
[00:33:06.040 --> 00:33:07.240]   did it take to execute?
[00:33:07.240 --> 00:33:09.960]   So that's what we've been talking about,
[00:33:09.960 --> 00:33:11.240]   the instructions of the academy.
[00:33:11.240 --> 00:33:12.480]   How many did it take?
[00:33:12.480 --> 00:33:14.000]   All right.
[00:33:14.000 --> 00:33:17.240]   The next question is how long did each instruction
[00:33:17.240 --> 00:33:18.840]   take to run on average?
[00:33:18.840 --> 00:33:21.500]   So you'd multiply the number of instructions
[00:33:21.500 --> 00:33:23.420]   times how long it took to run,
[00:33:23.420 --> 00:33:24.620]   and that gets you a whole time.
[00:33:24.620 --> 00:33:28.260]   Okay, so that's, but now let's look at this metric
[00:33:28.260 --> 00:33:29.980]   of how long did it take the instruction to run?
[00:33:29.980 --> 00:33:33.780]   Well, it turns out the way we could build computers today
[00:33:33.780 --> 00:33:35.140]   is they all have a clock.
[00:33:35.140 --> 00:33:37.940]   And you've seen this, if you buy a microprocessor,
[00:33:37.940 --> 00:33:42.300]   it'll say 3.1 gigahertz or 2.5 gigahertz,
[00:33:42.300 --> 00:33:43.920]   and more gigahertz is good.
[00:33:43.920 --> 00:33:46.620]   Well, what that is is the speed of the clock.
[00:33:46.620 --> 00:33:50.920]   So 2.5 gigahertz turns out to be four billionths
[00:33:50.920 --> 00:33:53.420]   of instruction or four nanoseconds.
[00:33:53.420 --> 00:33:55.740]   So that's the clock cycle time.
[00:33:55.740 --> 00:33:58.620]   But there's another factor, which is what's the average
[00:33:58.620 --> 00:34:01.260]   number of clock cycles it takes per instruction?
[00:34:01.260 --> 00:34:03.980]   So it's number of instructions, average number
[00:34:03.980 --> 00:34:06.420]   of clock cycles and the clock cycle time.
[00:34:06.420 --> 00:34:11.060]   So in these RISC-Sys debates, they would concentrate on,
[00:34:11.060 --> 00:34:14.140]   but RISC needs to take more instructions.
[00:34:14.140 --> 00:34:16.820]   And we'd argue what maybe the clock cycle is faster,
[00:34:16.820 --> 00:34:19.260]   but what the real big difference was,
[00:34:19.260 --> 00:34:21.300]   was the number of clock cycles per instruction.
[00:34:21.300 --> 00:34:22.740]   - Per instruction, that's fascinating.
[00:34:22.740 --> 00:34:25.780]   What about the mess of, the beautiful mess of parallelism
[00:34:25.780 --> 00:34:26.860]   in the whole picture?
[00:34:26.860 --> 00:34:28.860]   - Parallelism, which has to do with say,
[00:34:28.860 --> 00:34:31.460]   how many instructions could execute in parallel
[00:34:31.460 --> 00:34:32.720]   and things like that.
[00:34:32.720 --> 00:34:34.980]   You could think of that as affecting the clock cycles
[00:34:34.980 --> 00:34:37.020]   per instruction, 'cause it's the average clock cycles
[00:34:37.020 --> 00:34:38.100]   per instruction.
[00:34:38.100 --> 00:34:39.380]   So when you're running a program,
[00:34:39.380 --> 00:34:42.860]   if it took a hundred billion instructions
[00:34:42.860 --> 00:34:46.020]   and on average, it took two clock cycles per instruction
[00:34:46.020 --> 00:34:48.140]   and they were four nanoseconds, you could multiply that out
[00:34:48.140 --> 00:34:50.060]   and see how long it took to run.
[00:34:50.060 --> 00:34:51.940]   And there's all kinds of tricks to try and reduce
[00:34:51.940 --> 00:34:54.100]   the number of clock cycles per instruction.
[00:34:54.100 --> 00:34:58.120]   But it turned out that the way they would do
[00:34:58.120 --> 00:35:00.820]   these complex instructions is they would actually build
[00:35:00.820 --> 00:35:04.060]   what we would call an interpreter in a simpler,
[00:35:04.060 --> 00:35:05.940]   a very simple hardware interpreter.
[00:35:05.940 --> 00:35:08.900]   But it turned out that for the SISC instructions,
[00:35:08.900 --> 00:35:10.820]   if you had to use one of those interpreters,
[00:35:10.820 --> 00:35:13.300]   it would be like 10 clock cycles per instruction
[00:35:13.300 --> 00:35:16.100]   where the RISC instructions could be two.
[00:35:16.100 --> 00:35:18.300]   So there'd be this factor of five advantage
[00:35:18.300 --> 00:35:20.240]   in clock cycles per instruction.
[00:35:20.240 --> 00:35:23.620]   We have to execute say 25 or 50% more instructions.
[00:35:23.620 --> 00:35:25.140]   So that's where the win would come.
[00:35:25.140 --> 00:35:26.340]   And then you could make an argument
[00:35:26.340 --> 00:35:28.380]   whether the clock cycle times are the same or not.
[00:35:28.380 --> 00:35:32.960]   But pointing out that we could divide the benchmark results
[00:35:32.960 --> 00:35:35.400]   time per program into three factors.
[00:35:35.400 --> 00:35:37.820]   And the biggest difference in RISC and SISC
[00:35:37.820 --> 00:35:40.780]   was the clock cycles per, you execute a few more instructions
[00:35:40.780 --> 00:35:43.380]   but the clock cycles per instruction is much less.
[00:35:43.380 --> 00:35:45.020]   And that was what this debate was.
[00:35:45.020 --> 00:35:48.460]   Once we made that argument, then people said,
[00:35:48.460 --> 00:35:49.900]   oh, okay, I get it.
[00:35:49.900 --> 00:35:54.540]   And so we went from, it was outrageously controversial
[00:35:54.540 --> 00:35:58.660]   in 1982 that maybe probably by 1984 or so,
[00:35:58.660 --> 00:36:00.820]   people said, oh yeah, technically,
[00:36:00.820 --> 00:36:02.180]   they've got a good argument.
[00:36:02.180 --> 00:36:06.280]   - What are the instructions in the RISC instruction set?
[00:36:06.280 --> 00:36:07.680]   Just to get an intuition.
[00:36:08.620 --> 00:36:13.380]   - Okay, 1995, I was asked to predict the future
[00:36:13.380 --> 00:36:14.900]   of what microprocessor future.
[00:36:14.900 --> 00:36:18.300]   So I'd seen these predictions
[00:36:18.300 --> 00:36:20.900]   and usually people predict something outrageous
[00:36:20.900 --> 00:36:22.940]   just to be entertaining, right?
[00:36:22.940 --> 00:36:26.220]   And so my prediction for 2020 was,
[00:36:26.220 --> 00:36:27.740]   things are gonna be pretty much,
[00:36:27.740 --> 00:36:29.900]   they're gonna look very familiar to what they are.
[00:36:29.900 --> 00:36:33.260]   And they are, if you were to read the article,
[00:36:33.260 --> 00:36:34.820]   the things I said are pretty much true.
[00:36:34.820 --> 00:36:37.260]   The instructions that have been around forever
[00:36:37.260 --> 00:36:38.300]   are kind of the same.
[00:36:38.300 --> 00:36:40.740]   - And that's the outrageous prediction actually,
[00:36:40.740 --> 00:36:41.980]   given how fast computers have been growing.
[00:36:41.980 --> 00:36:44.120]   - Well, and Moore's law was gonna go on,
[00:36:44.120 --> 00:36:47.800]   we thought for 25 more years, who knows?
[00:36:47.800 --> 00:36:49.580]   But kind of the surprising thing,
[00:36:49.580 --> 00:36:54.580]   in fact, Hennessy and I won the ACM AM Turing Award
[00:36:54.580 --> 00:36:57.860]   for both the RISC instruction set contributions
[00:36:57.860 --> 00:36:59.780]   and for that textbook I mentioned.
[00:36:59.780 --> 00:37:03.820]   But we are surprised that here we are 35,
[00:37:03.820 --> 00:37:07.400]   40 years later after we did our work,
[00:37:08.320 --> 00:37:10.480]   and the conventional wisdom
[00:37:10.480 --> 00:37:12.680]   of the best way to do instruction sets
[00:37:12.680 --> 00:37:14.520]   is still those RISC instruction sets
[00:37:14.520 --> 00:37:17.160]   that look very similar to what we looked like
[00:37:17.160 --> 00:37:18.400]   we did in the 1980s.
[00:37:18.400 --> 00:37:20.760]   So those, surprisingly,
[00:37:20.760 --> 00:37:23.640]   there hasn't been some radical new idea,
[00:37:23.640 --> 00:37:26.840]   even though we have a million times as many transistors
[00:37:26.840 --> 00:37:28.620]   as we had back then.
[00:37:28.620 --> 00:37:31.640]   - But what are the basic instructions
[00:37:31.640 --> 00:37:33.200]   and how did they change over the years?
[00:37:33.200 --> 00:37:35.280]   So are we talking about addition, subtraction,
[00:37:35.280 --> 00:37:36.120]   these are the--
[00:37:36.120 --> 00:37:40.680]   - It's a specific, so the things that are in a calculator
[00:37:40.680 --> 00:37:41.680]   are in a computer.
[00:37:41.680 --> 00:37:43.840]   So any of the buttons that are in the calculator
[00:37:43.840 --> 00:37:44.720]   in the computer.
[00:37:44.720 --> 00:37:46.120]   So the-- - Nice way to put it.
[00:37:46.120 --> 00:37:48.280]   - So there's a memory function key,
[00:37:48.280 --> 00:37:50.080]   and like I said, those are turns into
[00:37:50.080 --> 00:37:51.720]   putting something in memory is called a store,
[00:37:51.720 --> 00:37:53.080]   bring something back is called a load.
[00:37:53.080 --> 00:37:55.780]   - Just a quick tangent, when you say memory,
[00:37:55.780 --> 00:37:57.000]   what does memory mean?
[00:37:57.000 --> 00:38:00.560]   - Well, I told you there were five pieces of a computer,
[00:38:00.560 --> 00:38:03.440]   and if you remember in a calculator, there's a memory key,
[00:38:03.440 --> 00:38:05.520]   so you wanna have intermediate calculation
[00:38:05.520 --> 00:38:06.720]   and bring it back later.
[00:38:06.720 --> 00:38:09.480]   So you'd hit the memory plus key, M plus maybe,
[00:38:09.480 --> 00:38:10.920]   and it would put that into memory,
[00:38:10.920 --> 00:38:13.680]   and then you'd hit an RM like current instruction,
[00:38:13.680 --> 00:38:15.120]   and it'd bring it back into display,
[00:38:15.120 --> 00:38:16.280]   so you don't have to type it,
[00:38:16.280 --> 00:38:17.960]   you don't have to write it down and bring it back again.
[00:38:17.960 --> 00:38:19.760]   So that's exactly what memory is,
[00:38:19.760 --> 00:38:22.760]   that you can put things into it as temporary storage
[00:38:22.760 --> 00:38:24.760]   and bring it back when you need it later.
[00:38:24.760 --> 00:38:27.400]   So that's memory and loads and stores.
[00:38:27.400 --> 00:38:30.720]   But the big thing, the difference between a computer
[00:38:30.720 --> 00:38:34.680]   and a calculator is that the computer can make decisions.
[00:38:34.680 --> 00:38:38.360]   And amazingly, decisions are as simple as,
[00:38:38.360 --> 00:38:40.560]   is this value less than zero,
[00:38:40.560 --> 00:38:42.960]   or is this value bigger than that value?
[00:38:42.960 --> 00:38:45.400]   So there's, and those instructions,
[00:38:45.400 --> 00:38:47.600]   which are called conditional branch instructions,
[00:38:47.600 --> 00:38:50.280]   is what give computers all its power.
[00:38:50.280 --> 00:38:52.440]   If you were in the early days of computing
[00:38:52.440 --> 00:38:55.160]   before what's called the general purpose microprocessor,
[00:38:55.160 --> 00:39:00.160]   people would write these instructions kind of in hardware,
[00:39:00.160 --> 00:39:01.640]   but it couldn't make decisions,
[00:39:01.640 --> 00:39:03.520]   it would just, it would do the same thing
[00:39:03.520 --> 00:39:04.600]   over and over again.
[00:39:04.600 --> 00:39:08.040]   With the power of having branch instructions,
[00:39:08.040 --> 00:39:10.760]   it can look at things and make decisions automatically.
[00:39:10.760 --> 00:39:12.360]   And it can make these decisions,
[00:39:12.360 --> 00:39:13.840]   billions of times per second.
[00:39:13.840 --> 00:39:16.520]   And amazingly enough, we can get,
[00:39:16.520 --> 00:39:18.080]   thanks to advanced machine learning,
[00:39:18.080 --> 00:39:21.160]   we can create programs that can do something
[00:39:21.160 --> 00:39:22.960]   smarter than human beings can do.
[00:39:22.960 --> 00:39:24.680]   But if you go down that very basic level,
[00:39:24.680 --> 00:39:28.120]   what's the instructions are the keys on the calculator,
[00:39:28.120 --> 00:39:30.440]   plus the ability to make decisions,
[00:39:30.440 --> 00:39:32.360]   these conditional branch instructions.
[00:39:32.360 --> 00:39:34.440]   - And all decisions fundamentally can be reduced
[00:39:34.440 --> 00:39:36.760]   down to these branch instructions.
[00:39:36.760 --> 00:39:39.160]   - Yeah, so in fact, and so,
[00:39:39.160 --> 00:39:42.320]   going way back in the stack, back to,
[00:39:42.320 --> 00:39:45.600]   we did four RISC projects at Berkeley in the 1980s,
[00:39:45.600 --> 00:39:48.960]   they did a couple at Stanford in the 1980s.
[00:39:48.960 --> 00:39:53.920]   In 2010, we decided we wanted to do a new instruction set,
[00:39:53.920 --> 00:39:56.560]   learning from the mistakes of those RISC architectures
[00:39:56.560 --> 00:40:00.040]   in the 1980s, and that was done here at Berkeley.
[00:40:00.040 --> 00:40:01.600]   Almost exactly 10 years ago,
[00:40:01.600 --> 00:40:04.680]   and the people who did it, I participated,
[00:40:04.680 --> 00:40:08.560]   but other, Krzysztof Sanovic and others drove it.
[00:40:08.560 --> 00:40:11.480]   They called it RISC-V to honor those RISC,
[00:40:11.480 --> 00:40:14.000]   the four RISC projects of the 1980s.
[00:40:14.000 --> 00:40:15.960]   - So what does RISC-V involve?
[00:40:15.960 --> 00:40:20.040]   - So RISC-V is another instruction set vocabulary.
[00:40:20.040 --> 00:40:22.200]   It's learned from the mistakes of the past,
[00:40:22.200 --> 00:40:24.440]   but it still has, if you look at the,
[00:40:24.440 --> 00:40:25.800]   there's a core set of instructions
[00:40:25.800 --> 00:40:28.520]   that's very similar to the simplest architectures
[00:40:28.520 --> 00:40:30.880]   from the 1980s, and the big difference
[00:40:30.880 --> 00:40:33.320]   about RISC-V is it's open.
[00:40:33.320 --> 00:40:35.280]   So I talked earlier about proprietary
[00:40:35.280 --> 00:40:40.280]   versus open, kind of software.
[00:40:40.280 --> 00:40:43.480]   So this is an instruction set, so it's a vocabulary.
[00:40:43.480 --> 00:40:47.280]   It's not hardware, but by having an open instruction set,
[00:40:47.280 --> 00:40:50.200]   we can have open source implementations,
[00:40:50.200 --> 00:40:52.880]   open source processors that people can use.
[00:40:52.880 --> 00:40:56.360]   - Where do you see that going?
[00:40:56.360 --> 00:40:58.080]   So it's a really exciting possibility,
[00:40:58.080 --> 00:41:00.200]   but you're just like in the scientific American,
[00:41:00.200 --> 00:41:03.600]   if you were to predict 10, 20, 30 years from now,
[00:41:03.600 --> 00:41:07.840]   that kind of ability to utilize open source
[00:41:07.840 --> 00:41:11.120]   instruction set architectures like RISC-V,
[00:41:11.120 --> 00:41:13.680]   what kind of possibilities might that unlock?
[00:41:13.680 --> 00:41:16.040]   - Yeah, and so just to make it clear,
[00:41:16.040 --> 00:41:20.320]   because this is confusing, the specification of RISC-V
[00:41:20.320 --> 00:41:22.380]   is something that's like in a textbook.
[00:41:22.380 --> 00:41:23.560]   There's books about it.
[00:41:23.560 --> 00:41:26.680]   So that's defining an interface.
[00:41:27.640 --> 00:41:29.880]   There's also the way you build hardware
[00:41:29.880 --> 00:41:31.960]   is you write it in languages.
[00:41:31.960 --> 00:41:34.560]   They're kind of like C, but they're specialized
[00:41:34.560 --> 00:41:38.180]   for hardware that gets translated into hardware.
[00:41:38.180 --> 00:41:42.360]   And so these implementations of this specification
[00:41:42.360 --> 00:41:43.960]   are what are the open source.
[00:41:43.960 --> 00:41:47.520]   So they're written in something that's called Verilog or VHDL,
[00:41:47.520 --> 00:41:50.600]   but it's put up on the web, just like you can see
[00:41:50.600 --> 00:41:54.760]   the C++ code for Linux on the web.
[00:41:54.760 --> 00:41:56.800]   So that's the open instruction set
[00:41:56.800 --> 00:42:00.720]   enables open source implementations of RISC-V.
[00:42:00.720 --> 00:42:02.320]   - So you can literally build a processor
[00:42:02.320 --> 00:42:04.200]   using this instruction set.
[00:42:04.200 --> 00:42:05.540]   - People are, people are.
[00:42:05.540 --> 00:42:08.140]   So what happened to us, the story was,
[00:42:08.140 --> 00:42:11.760]   this was developed here for our use to do our research.
[00:42:11.760 --> 00:42:14.600]   And we made it, we licensed under the Berkeley
[00:42:14.600 --> 00:42:16.080]   software distribution license,
[00:42:16.080 --> 00:42:18.020]   like a lot of things get licensed here.
[00:42:18.020 --> 00:42:20.800]   So other academics use it, they wouldn't be afraid to use it.
[00:42:20.800 --> 00:42:25.760]   And then about 2014, we started getting complaints
[00:42:25.760 --> 00:42:28.560]   that we were using it in our research and in our courses.
[00:42:28.560 --> 00:42:30.880]   And we got complaints from people in industries,
[00:42:30.880 --> 00:42:33.880]   why did you change your instruction set
[00:42:33.880 --> 00:42:36.760]   between the fall and the spring semester?
[00:42:36.760 --> 00:42:38.600]   And well, we get complaints from industrial time.
[00:42:38.600 --> 00:42:40.440]   Why the hell do you care
[00:42:40.440 --> 00:42:42.020]   what we do with our instructions?
[00:42:42.020 --> 00:42:44.000]   And then when we talked to them, we found out
[00:42:44.000 --> 00:42:46.040]   there was this thirst for this idea
[00:42:46.040 --> 00:42:47.720]   of an open instruction set architecture.
[00:42:47.720 --> 00:42:49.360]   And they had been looking for one,
[00:42:49.360 --> 00:42:51.320]   they stumbled upon ours at Berkeley,
[00:42:51.320 --> 00:42:54.120]   thought it was, boy, this looks great.
[00:42:54.120 --> 00:42:55.920]   We should use this one.
[00:42:55.920 --> 00:42:58.560]   And so once we realized there is this need
[00:42:58.560 --> 00:43:00.480]   for an open instruction set architecture,
[00:43:00.480 --> 00:43:02.080]   we thought that's a great idea.
[00:43:02.080 --> 00:43:03.680]   And then we started supporting it
[00:43:03.680 --> 00:43:05.200]   and tried to make it happen.
[00:43:05.200 --> 00:43:09.680]   So this was, we accidentally stumbled into this,
[00:43:09.680 --> 00:43:12.040]   into this need and our timing was good.
[00:43:12.040 --> 00:43:14.640]   And so it's really taking off.
[00:43:14.640 --> 00:43:18.520]   There's, you know, universities are good at starting things,
[00:43:18.520 --> 00:43:20.040]   but they're not good at sustaining things.
[00:43:20.040 --> 00:43:22.480]   So like Linux has a Linux foundation,
[00:43:22.480 --> 00:43:25.360]   there's a RISC-V foundation that we started.
[00:43:25.360 --> 00:43:27.560]   There's an annual conferences.
[00:43:27.560 --> 00:43:31.800]   And the first one was done, I think, January of 2015.
[00:43:31.800 --> 00:43:33.480]   And the one that was just last December,
[00:43:33.480 --> 00:43:35.360]   and it, you know, it had 50 people at it.
[00:43:35.360 --> 00:43:38.880]   And the one last December had, I don't know,
[00:43:38.880 --> 00:43:40.800]   1700 people were at it
[00:43:40.800 --> 00:43:43.660]   and the companies excited all over the world.
[00:43:43.660 --> 00:43:46.640]   So if predicting into the future, you know,
[00:43:46.640 --> 00:43:49.800]   if we were doing 25 years, I would predict that RISC-V
[00:43:49.800 --> 00:43:53.600]   will be, you know, possibly the most popular
[00:43:53.600 --> 00:43:55.440]   instruction set architecture out there,
[00:43:55.440 --> 00:43:58.600]   because it's a pretty good instruction set architecture
[00:43:58.600 --> 00:43:59.880]   and it's open and free.
[00:43:59.880 --> 00:44:04.480]   And there's no reason lots of people shouldn't use it.
[00:44:04.480 --> 00:44:09.160]   And there's benefits, just like Linux is so popular today
[00:44:09.160 --> 00:44:10.680]   compared to 20 years ago.
[00:44:10.680 --> 00:44:15.480]   And, you know, the fact that you can get access to it
[00:44:15.480 --> 00:44:18.000]   for free, you can modify it, you can improve it
[00:44:18.000 --> 00:44:19.800]   for all those same arguments.
[00:44:19.800 --> 00:44:22.600]   And so people collaborate to make it a better system
[00:44:22.600 --> 00:44:24.840]   for everybody to use, and that works in software.
[00:44:24.840 --> 00:44:27.840]   And I expect the same thing will happen in hardware.
[00:44:27.840 --> 00:44:31.240]   - So if you look at ARM, Intel, MIPS,
[00:44:31.240 --> 00:44:34.200]   if you look at just the lay of the land,
[00:44:34.200 --> 00:44:38.240]   and what do you think, just for me,
[00:44:38.240 --> 00:44:41.240]   because I'm not familiar how difficult
[00:44:41.240 --> 00:44:42.740]   this kind of transition would,
[00:44:44.720 --> 00:44:48.040]   how much challenges this kind of transition would entail,
[00:44:48.040 --> 00:44:52.400]   do you see, let me ask my dumb question in another way.
[00:44:52.400 --> 00:44:54.560]   - No, that's, I know where you're headed.
[00:44:54.560 --> 00:44:55.560]   (laughing)
[00:44:55.560 --> 00:44:57.400]   Well, there's a bunch, I think the thing you point out,
[00:44:57.400 --> 00:45:01.160]   there's these very popular proprietary instruction sets,
[00:45:01.160 --> 00:45:02.760]   the x86 and ARM.
[00:45:02.760 --> 00:45:05.680]   - And so how do we move to RISC-V potentially
[00:45:05.680 --> 00:45:09.200]   in sort of, in the span of five, 10, 20 years,
[00:45:09.200 --> 00:45:13.440]   a kind of unification, given that the devices,
[00:45:13.440 --> 00:45:17.600]   the kind of way we use devices, IoT, mobile devices,
[00:45:17.600 --> 00:45:20.240]   and the cloud keeps changing?
[00:45:20.240 --> 00:45:23.140]   - Well, part of it, a big piece of it,
[00:45:23.140 --> 00:45:25.320]   is the software stack.
[00:45:25.320 --> 00:45:28.080]   And what, right now, looking forward,
[00:45:28.080 --> 00:45:31.080]   there seem to be three important markets.
[00:45:31.080 --> 00:45:35.120]   There's the cloud, and the cloud is simply
[00:45:35.120 --> 00:45:40.420]   companies like Alibaba and Amazon and Google,
[00:45:40.420 --> 00:45:43.840]   Microsoft, having these giant data centers
[00:45:43.840 --> 00:45:45.840]   with tens of thousands of servers
[00:45:45.840 --> 00:45:50.320]   and maybe a hundred of these data centers all over the world.
[00:45:50.320 --> 00:45:51.400]   And that's what the cloud is.
[00:45:51.400 --> 00:45:53.520]   So the computer that dominates the cloud
[00:45:53.520 --> 00:45:55.960]   is the x86 instruction set.
[00:45:55.960 --> 00:45:58.280]   So the instruction, or the instruction sets
[00:45:58.280 --> 00:46:00.000]   used in the cloud are the x86,
[00:46:00.000 --> 00:46:05.000]   almost 100% of that today is x86.
[00:46:05.000 --> 00:46:09.760]   The other big thing are cell phones and laptops.
[00:46:09.760 --> 00:46:10.860]   Those are the big things today.
[00:46:10.860 --> 00:46:13.820]   I mean, the PC is also dominated
[00:46:13.820 --> 00:46:15.080]   by the x86 instruction set,
[00:46:15.080 --> 00:46:17.140]   but those sales are dwindling.
[00:46:17.140 --> 00:46:20.460]   You know, there's maybe 200 million PCs a year,
[00:46:20.460 --> 00:46:24.060]   and there's, is there 1.5 billion phones a year?
[00:46:24.060 --> 00:46:25.380]   There's numbers like that.
[00:46:25.380 --> 00:46:29.160]   So for the phones, that's dominated by ARM.
[00:46:29.160 --> 00:46:33.900]   And now, and a reason that,
[00:46:33.900 --> 00:46:35.860]   I talked about the software stacks,
[00:46:35.860 --> 00:46:38.180]   and the third category is internet of things,
[00:46:38.180 --> 00:46:39.620]   which is basically embedded devices,
[00:46:39.620 --> 00:46:43.180]   things in your cars and your microwaves, everywhere.
[00:46:43.180 --> 00:46:46.100]   So what's different about those three categories
[00:46:46.100 --> 00:46:49.420]   is for the cloud, the software that runs in the cloud
[00:46:49.420 --> 00:46:51.140]   is determined by these companies,
[00:46:51.140 --> 00:46:53.980]   Alibaba, Amazon, Google, Microsoft.
[00:46:53.980 --> 00:46:56.820]   So they control that software stack.
[00:46:56.820 --> 00:46:59.820]   For the cell phones, there's both,
[00:46:59.820 --> 00:47:02.500]   for Android and Apple, the software they supply,
[00:47:02.500 --> 00:47:04.340]   but both of them have marketplaces
[00:47:04.340 --> 00:47:07.040]   where anybody in the world can build software.
[00:47:07.040 --> 00:47:10.040]   And that software is translated,
[00:47:10.040 --> 00:47:15.040]   or compiled down and shipped in the vocabulary of ARM.
[00:47:15.040 --> 00:47:18.440]   So that's what's referred to as binary compatible,
[00:47:18.440 --> 00:47:21.540]   because the actual, it's the instructions
[00:47:21.540 --> 00:47:24.100]   are turned into numbers, binary numbers,
[00:47:24.100 --> 00:47:25.020]   and shipped around the world.
[00:47:25.020 --> 00:47:25.860]   So-- - And so,
[00:47:25.860 --> 00:47:27.140]   just a quick interruption.
[00:47:27.140 --> 00:47:28.800]   So ARM, what is ARM?
[00:47:28.800 --> 00:47:32.780]   ARM is an instruction set, like a risk-based--
[00:47:32.780 --> 00:47:34.260]   - Yeah, it's a risk-based instruction set.
[00:47:34.260 --> 00:47:35.380]   It's a proprietary one.
[00:47:35.380 --> 00:47:40.380]   ARM stands for Advanced Risk Machine,
[00:47:40.380 --> 00:47:42.420]   ARM is the name where the company is.
[00:47:42.420 --> 00:47:44.600]   So it's a proprietary risk architecture.
[00:47:44.600 --> 00:47:48.420]   So, and it's been around for a while,
[00:47:48.420 --> 00:47:50.940]   and it's surely the most popular instruction set
[00:47:50.940 --> 00:47:52.200]   in the world right now.
[00:47:52.200 --> 00:47:56.260]   Every year, billions of chips are using the ARM design
[00:47:56.260 --> 00:47:58.660]   in this post-PC era.
[00:47:58.660 --> 00:48:01.860]   - Was it one of the early risk adopters of the risk idea?
[00:48:01.860 --> 00:48:02.700]   - Yeah.
[00:48:02.700 --> 00:48:05.580]   The first ARM goes back, I don't know, '86 or so.
[00:48:05.580 --> 00:48:08.820]   So Berkeley and Stanford did their work in the early '80s.
[00:48:08.820 --> 00:48:11.660]   Their ARM guys needed an instruction set,
[00:48:11.660 --> 00:48:15.500]   and they read our papers, and it heavily influenced them.
[00:48:15.500 --> 00:48:18.180]   So getting back to my story,
[00:48:18.180 --> 00:48:19.140]   what about Internet of Things?
[00:48:19.140 --> 00:48:21.420]   Well, software's not shipped in Internet of Things.
[00:48:21.420 --> 00:48:24.820]   It's the embedded device,
[00:48:24.820 --> 00:48:26.580]   people control that software stack.
[00:48:26.580 --> 00:48:31.120]   So the opportunities for RISC-V, everybody thinks,
[00:48:31.120 --> 00:48:33.500]   is in the Internet of Things embedded things,
[00:48:33.500 --> 00:48:35.620]   because there's no dominant player
[00:48:35.620 --> 00:48:39.820]   like there is in the cloud or the smartphones.
[00:48:39.820 --> 00:48:44.260]   And it doesn't have a lot of licenses associated with,
[00:48:44.260 --> 00:48:46.940]   and you can enhance the instruction set if you want.
[00:48:46.940 --> 00:48:51.140]   And people have looked at instruction sets
[00:48:51.140 --> 00:48:52.940]   and think it's a very good instruction set.
[00:48:52.940 --> 00:48:55.460]   So it appears to be very popular there.
[00:48:55.460 --> 00:48:59.220]   It's possible that in the cloud,
[00:48:59.220 --> 00:49:02.480]   those companies control their software stacks.
[00:49:02.480 --> 00:49:06.560]   So it's possible that they would decide to use RISC-V,
[00:49:06.560 --> 00:49:09.600]   if we're talking about 10 and 20 years in the future.
[00:49:09.600 --> 00:49:11.920]   The one that would be harder would be the cell phones,
[00:49:11.920 --> 00:49:15.120]   since people ship software in the ARM instruction set.
[00:49:15.120 --> 00:49:17.400]   That, you'd think, would be the more difficult one.
[00:49:17.400 --> 00:49:19.800]   But if RISC-V really catches on,
[00:49:19.800 --> 00:49:22.280]   and in a period of a decade,
[00:49:22.280 --> 00:49:24.320]   you can imagine that's changing over too.
[00:49:24.320 --> 00:49:27.720]   - Do you have a sense why RISC-V or ARM is dominated?
[00:49:27.720 --> 00:49:29.120]   You mentioned these three categories.
[00:49:29.120 --> 00:49:31.300]   Why did ARM dominate?
[00:49:31.300 --> 00:49:33.980]   Why does it dominate the mobile device space?
[00:49:33.980 --> 00:49:38.980]   And maybe my naive intuition is that there's some aspects
[00:49:38.980 --> 00:49:41.220]   of power efficiency that are important,
[00:49:41.220 --> 00:49:43.140]   that somehow come along with RISC.
[00:49:43.140 --> 00:49:44.400]   - Well, part of it is,
[00:49:44.400 --> 00:49:49.060]   for these old CISC instruction sets, like in the x86,
[00:49:49.060 --> 00:49:57.420]   it was more expensive to these, for, you know,
[00:49:57.560 --> 00:50:00.620]   they're older, so they have disadvantages in them
[00:50:00.620 --> 00:50:02.940]   because they were designed 40 years ago.
[00:50:02.940 --> 00:50:06.100]   But also, they have to translate in hardware
[00:50:06.100 --> 00:50:08.540]   from CISC instructions to RISC instructions on the fly.
[00:50:08.540 --> 00:50:11.780]   And that costs both silicon area,
[00:50:11.780 --> 00:50:14.100]   the chips are bigger to be able to do that,
[00:50:14.100 --> 00:50:15.700]   and it uses more power.
[00:50:15.700 --> 00:50:18.100]   So ARM has, which has, you know,
[00:50:18.100 --> 00:50:19.420]   followed this RISC philosophy,
[00:50:19.420 --> 00:50:22.080]   is seen to be much more energy efficient.
[00:50:22.080 --> 00:50:24.060]   And in today's computer world,
[00:50:24.060 --> 00:50:28.820]   both in the cloud and the cell phone and, you know, things,
[00:50:28.820 --> 00:50:31.420]   it isn't, the limiting resource
[00:50:31.420 --> 00:50:33.460]   isn't the number of transistors you can fit in the chip,
[00:50:33.460 --> 00:50:36.400]   it's what, how much power can you dissipate
[00:50:36.400 --> 00:50:37.400]   for your application?
[00:50:37.400 --> 00:50:41.260]   So by having a reduced instruction set,
[00:50:41.260 --> 00:50:44.080]   that's possible to have a simpler hardware,
[00:50:44.080 --> 00:50:45.360]   which is more energy efficient.
[00:50:45.360 --> 00:50:48.560]   And energy efficiency is incredibly important in the cloud.
[00:50:48.560 --> 00:50:50.860]   When you have tens of thousands of computers
[00:50:50.860 --> 00:50:51.700]   in a data center,
[00:50:51.700 --> 00:50:54.680]   you wanna have the most energy efficient ones there as well.
[00:50:54.680 --> 00:50:57.000]   And of course, for embedded things running off of batteries,
[00:50:57.000 --> 00:50:58.520]   you want those to be energy efficient,
[00:50:58.520 --> 00:50:59.940]   and the cell phones too.
[00:50:59.940 --> 00:51:04.940]   So I think it's believed that there's a energy disadvantage
[00:51:04.940 --> 00:51:09.720]   of using these more complex instruction set architectures.
[00:51:09.720 --> 00:51:13.640]   - So the other aspect of this is,
[00:51:13.640 --> 00:51:16.360]   if we look at Apple, Qualcomm, Samsung, Huawei,
[00:51:16.360 --> 00:51:19.800]   all use the ARM architecture.
[00:51:19.800 --> 00:51:22.220]   And yet the performance of the systems varies.
[00:51:22.220 --> 00:51:24.700]   I mean, I don't know whose opinion you take on,
[00:51:24.700 --> 00:51:26.700]   but, you know, Apple, for some reason,
[00:51:26.700 --> 00:51:29.980]   seems to perform better in terms of these implementations,
[00:51:29.980 --> 00:51:30.820]   these architectures.
[00:51:30.820 --> 00:51:33.020]   So where's the magic, enter the picture?
[00:51:33.020 --> 00:51:33.860]   - How's that happen?
[00:51:33.860 --> 00:51:36.900]   Yeah, so what ARM pioneered was a new business model.
[00:51:36.900 --> 00:51:38.100]   As they said, well,
[00:51:38.100 --> 00:51:40.020]   here's our proprietary instruction set,
[00:51:40.020 --> 00:51:43.060]   and we'll give you two ways to do it.
[00:51:43.060 --> 00:51:47.020]   We'll give you one of these implementations
[00:51:47.020 --> 00:51:49.860]   written in things like C called Verilog,
[00:51:49.860 --> 00:51:51.860]   and you can just use ours.
[00:51:51.860 --> 00:51:53.700]   You have to pay money for that.
[00:51:53.700 --> 00:51:56.580]   Not only will give you their, you know,
[00:51:56.580 --> 00:51:59.700]   we'll license you to do that, or you could design your own.
[00:51:59.700 --> 00:52:02.820]   And so we're talking about numbers like
[00:52:02.820 --> 00:52:04.080]   tens of millions of dollars
[00:52:04.080 --> 00:52:05.500]   to have the right to design your own,
[00:52:05.500 --> 00:52:08.940]   since the instruction set belongs to them.
[00:52:08.940 --> 00:52:13.220]   So Apple got one of those, the right to build their own.
[00:52:13.220 --> 00:52:15.860]   Most of the other people who build like Android phones
[00:52:15.860 --> 00:52:20.860]   just get one of the designs from ARM to do it themselves.
[00:52:20.860 --> 00:52:24.580]   So Apple developed a really good
[00:52:24.580 --> 00:52:26.740]   microprocessor design team.
[00:52:26.740 --> 00:52:29.980]   They, you know, acquired a very good team
[00:52:29.980 --> 00:52:33.380]   that was building other microprocessors
[00:52:33.380 --> 00:52:36.220]   and brought them into the company to build their designs.
[00:52:36.220 --> 00:52:38.160]   So the instruction sets are the same,
[00:52:38.160 --> 00:52:39.900]   the specifications are the same,
[00:52:39.900 --> 00:52:42.660]   but their hardware design is much more efficient
[00:52:42.660 --> 00:52:44.320]   than I think everybody else's.
[00:52:45.340 --> 00:52:49.740]   And that's given Apple an advantage in the marketplace
[00:52:49.740 --> 00:52:54.260]   in that the iPhones tend to be faster
[00:52:54.260 --> 00:52:57.120]   than most everybody else's phones that are there.
[00:52:57.120 --> 00:52:59.980]   - It'd be nice to be able to jump around
[00:52:59.980 --> 00:53:02.660]   and kind of explore different little sides of this.
[00:53:02.660 --> 00:53:05.680]   But let me ask one sort of romanticized question.
[00:53:05.680 --> 00:53:08.740]   What to you is the most beautiful aspect
[00:53:08.740 --> 00:53:12.500]   or idea of RISC instruction set or instruction sets
[00:53:12.500 --> 00:53:14.900]   or this work that you've done?
[00:53:14.900 --> 00:53:19.740]   - You know, I was always attracted to the idea of,
[00:53:19.740 --> 00:53:21.620]   you know, small is beautiful.
[00:53:21.620 --> 00:53:25.140]   Is that the temptation in engineering,
[00:53:25.140 --> 00:53:27.980]   it's kind of easy to make things more complicated.
[00:53:27.980 --> 00:53:30.160]   It's harder to come up with a,
[00:53:30.160 --> 00:53:31.740]   it's more difficult, surprisingly,
[00:53:31.740 --> 00:53:33.900]   to come up with a simple, elegant solution.
[00:53:33.900 --> 00:53:37.020]   And I think that there's a bunch of small features
[00:53:37.020 --> 00:53:40.840]   of RISC in general that, you know,
[00:53:40.840 --> 00:53:44.260]   where you can see this examples of keeping it simpler
[00:53:44.260 --> 00:53:45.580]   makes it more elegant.
[00:53:45.580 --> 00:53:47.980]   Specifically in RISC-V, which, you know,
[00:53:47.980 --> 00:53:49.980]   I was kind of the mentor in the program,
[00:53:49.980 --> 00:53:52.020]   but it was really driven by Krzysztof Sanovic
[00:53:52.020 --> 00:53:55.940]   and two grad students, Andrew Waterman and Yensip Lee,
[00:53:55.940 --> 00:53:59.220]   is they hit upon this idea of having
[00:53:59.220 --> 00:54:02.820]   a subset of instructions,
[00:54:02.820 --> 00:54:05.300]   a nice simple subset instructions,
[00:54:05.300 --> 00:54:09.120]   like 40-ish instructions that all software,
[00:54:09.120 --> 00:54:11.580]   the software stack for RISC-V
[00:54:11.580 --> 00:54:14.060]   can run just on those 40 instructions.
[00:54:14.060 --> 00:54:17.060]   And then they provide optional features
[00:54:17.060 --> 00:54:20.780]   that could accelerate the performance instructions
[00:54:20.780 --> 00:54:22.720]   that if you needed them could be very helpful,
[00:54:22.720 --> 00:54:24.260]   but you don't need to have them.
[00:54:24.260 --> 00:54:26.840]   And that's a new, really a new idea.
[00:54:26.840 --> 00:54:31.820]   So RISC-V has right now maybe five optional subsets
[00:54:31.820 --> 00:54:34.500]   that you can pull in, but the software runs without them.
[00:54:34.500 --> 00:54:36.260]   If you just want to build the,
[00:54:36.260 --> 00:54:39.200]   just the core 40 instructions, that's fine.
[00:54:39.200 --> 00:54:40.040]   You can do that.
[00:54:40.040 --> 00:54:43.380]   So this is fantastic for educationally
[00:54:43.380 --> 00:54:44.820]   is you can explain computers.
[00:54:44.820 --> 00:54:47.260]   You only have to explain 40 instructions
[00:54:47.260 --> 00:54:48.660]   and not thousands of them.
[00:54:48.660 --> 00:54:52.320]   Also, if you invent some wild and crazy new technology,
[00:54:52.320 --> 00:54:55.740]   like biological computing,
[00:54:55.740 --> 00:54:58.580]   you'd like a nice simple instruction set
[00:54:58.580 --> 00:55:00.540]   and you can RISC-V,
[00:55:00.540 --> 00:55:02.060]   if you implement those core instructions,
[00:55:02.060 --> 00:55:05.420]   you can run really interesting programs on top of that.
[00:55:05.420 --> 00:55:08.020]   So this idea of a core set of instructions
[00:55:08.020 --> 00:55:10.000]   that the software stack runs on,
[00:55:10.000 --> 00:55:13.480]   and then optional features that if you turn them on,
[00:55:13.480 --> 00:55:15.640]   the compilers were used, but you don't have to,
[00:55:15.640 --> 00:55:17.920]   I think is a powerful idea.
[00:55:17.920 --> 00:55:19.920]   What's happened in the past
[00:55:19.920 --> 00:55:22.500]   if for the proprietary instruction sets
[00:55:22.500 --> 00:55:25.160]   is when they add new instructions,
[00:55:25.160 --> 00:55:27.920]   it becomes required piece.
[00:55:27.920 --> 00:55:32.080]   And so that all microprocessors in the future
[00:55:32.080 --> 00:55:33.520]   have to use those instructions.
[00:55:33.520 --> 00:55:35.080]   So it's kind of like,
[00:55:35.080 --> 00:55:36.280]   for a lot of people as they get older,
[00:55:36.280 --> 00:55:38.080]   they gain weight, right?
[00:55:38.080 --> 00:55:38.920]   (laughing)
[00:55:38.920 --> 00:55:41.200]   That weight and age are correlated.
[00:55:41.200 --> 00:55:43.120]   And so you can see these instruction sets
[00:55:43.120 --> 00:55:45.440]   get getting bigger and bigger as they get older.
[00:55:45.440 --> 00:55:50.060]   So RISC-V, lets you be as slim as you as a teenager,
[00:55:50.060 --> 00:55:52.760]   and you only have to add these extra features
[00:55:52.760 --> 00:55:53.880]   if you're really gonna use them,
[00:55:53.880 --> 00:55:55.680]   rather than you have no choice,
[00:55:55.680 --> 00:55:58.320]   you have to keep growing with the instruction set.
[00:55:58.320 --> 00:55:59.680]   - I don't know if the analogy holds up,
[00:55:59.680 --> 00:56:01.080]   but that's a beautiful notion.
[00:56:01.080 --> 00:56:02.560]   (laughing)
[00:56:02.560 --> 00:56:04.560]   That there's, it's almost like a nudge towards,
[00:56:04.560 --> 00:56:07.720]   here's the simple core, that's the essential.
[00:56:07.720 --> 00:56:10.240]   - Yeah, I think the surprising thing is still,
[00:56:10.240 --> 00:56:13.800]   if we brought back the pioneers from the 1950s
[00:56:13.800 --> 00:56:16.040]   and showed them the instruction set architectures,
[00:56:16.040 --> 00:56:16.920]   they'd understand it.
[00:56:16.920 --> 00:56:19.880]   They'd say, "Wow, that doesn't look that different."
[00:56:19.880 --> 00:56:21.880]   Well, yeah, I'm surprised.
[00:56:21.880 --> 00:56:24.440]   And it's, there's, it may be something,
[00:56:24.440 --> 00:56:25.840]   to talk about philosophical things,
[00:56:25.840 --> 00:56:29.240]   I mean, there may be something powerful
[00:56:29.240 --> 00:56:33.560]   about those 40 or 50 instructions
[00:56:33.560 --> 00:56:36.480]   that all you need is these commands,
[00:56:36.480 --> 00:56:38.840]   like these instructions that we talked about,
[00:56:38.840 --> 00:56:41.760]   and that is sufficient to build,
[00:56:41.760 --> 00:56:45.320]   to bring about artificial intelligence.
[00:56:45.320 --> 00:56:49.200]   And so it's a remarkable, surprising to me
[00:56:49.200 --> 00:56:54.200]   that as complicated as it is to build these things,
[00:56:54.200 --> 00:56:58.760]   a microprocessor is where the line widths
[00:56:58.760 --> 00:57:02.520]   are narrower than the wavelength of light,
[00:57:02.520 --> 00:57:07.240]   is this amazing technology is at some fundamental level,
[00:57:07.240 --> 00:57:08.880]   the commands that software executes
[00:57:08.880 --> 00:57:10.200]   are really pretty straightforward
[00:57:10.200 --> 00:57:13.680]   and haven't changed that much in decades,
[00:57:13.680 --> 00:57:16.000]   which, what a surprising outcome.
[00:57:16.000 --> 00:57:19.320]   - So underlying all computation, all Turing machines,
[00:57:19.320 --> 00:57:21.640]   all artificial intelligence systems,
[00:57:21.640 --> 00:57:24.160]   perhaps might be a very simple instruction set,
[00:57:24.160 --> 00:57:26.560]   like a RISC-V, or it's--
[00:57:26.560 --> 00:57:29.680]   - Yeah, I mean, that's kind of what I said.
[00:57:29.680 --> 00:57:30.960]   I was interested to see,
[00:57:30.960 --> 00:57:33.440]   I had another more senior faculty colleague,
[00:57:33.440 --> 00:57:36.600]   and he had written something in Scientific American,
[00:57:36.600 --> 00:57:40.360]   and his 25 years in the future,
[00:57:40.360 --> 00:57:42.840]   and his turned out about when I was a young professor,
[00:57:42.840 --> 00:57:44.600]   and he said, "Yep, I checked it."
[00:57:44.600 --> 00:57:45.520]   And so I was interested to see
[00:57:45.520 --> 00:57:48.200]   how that was gonna turn out for me,
[00:57:48.200 --> 00:57:51.180]   and it's pretty, held up pretty well.
[00:57:51.180 --> 00:57:52.840]   But yeah, so there's probably,
[00:57:52.840 --> 00:57:56.520]   there must be something fundamental
[00:57:56.520 --> 00:58:00.420]   about those instructions that we're capable of,
[00:58:01.140 --> 00:58:06.140]   creating intelligence from pretty primitive operations,
[00:58:06.140 --> 00:58:09.380]   and just doing them really fast.
[00:58:09.380 --> 00:58:12.020]   - You kind of mentioned a different,
[00:58:12.020 --> 00:58:15.300]   maybe radical computational medium, like biological,
[00:58:15.300 --> 00:58:16.500]   and there's other ideas.
[00:58:16.500 --> 00:58:18.540]   So there's a lot of spaces in ASIC,
[00:58:18.540 --> 00:58:20.620]   so it's domain-specific,
[00:58:20.620 --> 00:58:22.140]   and then there could be quantum computers,
[00:58:22.140 --> 00:58:25.780]   and so we can think of all of those different mediums
[00:58:25.780 --> 00:58:27.420]   and types of computation.
[00:58:27.420 --> 00:58:30.780]   What's the connection between swapping out
[00:58:30.780 --> 00:58:34.780]   different hardware systems in the instruction set?
[00:58:34.780 --> 00:58:36.100]   Do you see those as disjoint,
[00:58:36.100 --> 00:58:37.620]   or are they fundamentally coupled?
[00:58:37.620 --> 00:58:39.220]   - Yeah, so what's, so kind of,
[00:58:39.220 --> 00:58:40.800]   if we go back to the history,
[00:58:40.800 --> 00:58:45.460]   you know, when Moore's Law's in full effect,
[00:58:45.460 --> 00:58:48.180]   and you're getting twice as many transistors
[00:58:48.180 --> 00:58:50.820]   every couple of years,
[00:58:50.820 --> 00:58:53.020]   you know, kind of the challenge for computer designers
[00:58:53.020 --> 00:58:54.580]   is how can we take advantage of that?
[00:58:54.580 --> 00:58:56.140]   How can we turn those transistors
[00:58:56.140 --> 00:58:59.340]   into better computers, faster, typically?
[00:58:59.340 --> 00:59:04.100]   And so there was an era, I guess, in the '80s and '90s,
[00:59:04.100 --> 00:59:09.100]   where computers were doubling performance every 18 months,
[00:59:09.100 --> 00:59:11.700]   and if you weren't around then,
[00:59:11.700 --> 00:59:15.020]   what would happen is you had your computer,
[00:59:15.020 --> 00:59:17.260]   and your friend's computer,
[00:59:17.260 --> 00:59:19.740]   which was like a year, year and a half newer,
[00:59:19.740 --> 00:59:21.940]   and it was much faster than your computer,
[00:59:21.940 --> 00:59:24.860]   and he or she could get their work done
[00:59:24.860 --> 00:59:25.700]   much faster than your,
[00:59:25.700 --> 00:59:27.820]   'cause you were, so people took their computers,
[00:59:27.820 --> 00:59:29.420]   perfectly good computers,
[00:59:29.420 --> 00:59:32.700]   and threw them away to buy a newer computer
[00:59:32.700 --> 00:59:35.260]   because the computer, one or two years later,
[00:59:35.260 --> 00:59:36.500]   was so much faster.
[00:59:36.500 --> 00:59:39.660]   So that's what the world was like in the '80s and '90s.
[00:59:39.660 --> 00:59:43.580]   Well, with the slowing down of Moore's Law,
[00:59:43.580 --> 00:59:45.340]   that's no longer true, right?
[00:59:45.340 --> 00:59:47.700]   Now with, you know, not desk-side computers,
[00:59:47.700 --> 00:59:51.580]   but the laptops, I only get a new laptop when it breaks,
[00:59:51.580 --> 00:59:55.060]   right, oh, damn, the disk broke, or this display broke,
[00:59:55.060 --> 00:59:56.020]   you gotta buy a new computer,
[00:59:56.020 --> 00:59:57.820]   but before, you would throw them away
[00:59:57.820 --> 01:00:00.500]   because they were just so sluggish
[01:00:00.500 --> 01:00:03.040]   compared to the latest computers.
[01:00:03.040 --> 01:00:04.220]   So that's, you know,
[01:00:04.220 --> 01:00:10.420]   that's a huge change of what's gone on.
[01:00:10.420 --> 01:00:13.520]   So, but since this lasted for decades,
[01:00:13.520 --> 01:00:16.840]   kind of programmers, and maybe all of society,
[01:00:16.840 --> 01:00:19.640]   is used to computers getting faster regularly.
[01:00:19.640 --> 01:00:24.100]   We now believe, those of us who are in computer design,
[01:00:24.100 --> 01:00:25.580]   it's called computer architecture,
[01:00:25.580 --> 01:00:28.780]   that the path forward is instead,
[01:00:28.780 --> 01:00:33.020]   is to add accelerators that only work well
[01:00:33.020 --> 01:00:35.240]   for certain applications.
[01:00:35.240 --> 01:00:40.060]   So since Moore's Law is slowing down,
[01:00:40.060 --> 01:00:42.220]   we don't think general-purpose computers
[01:00:42.220 --> 01:00:43.660]   are gonna get a lot faster.
[01:00:43.660 --> 01:00:46.740]   So the Intel processors of the world are not gonna,
[01:00:46.740 --> 01:00:48.060]   haven't been getting a lot faster.
[01:00:48.060 --> 01:00:51.860]   They've been barely improving, like a few percent a year.
[01:00:51.860 --> 01:00:54.000]   It used to be doubling every 18 months,
[01:00:54.000 --> 01:00:56.060]   now it's doubling every 20 years.
[01:00:56.060 --> 01:00:57.800]   So it's just shocking.
[01:00:57.800 --> 01:01:00.680]   So to be able to deliver on what Moore's Law used to do,
[01:01:00.680 --> 01:01:02.580]   we think what's gonna happen,
[01:01:02.580 --> 01:01:03.860]   what is happening right now,
[01:01:03.860 --> 01:01:08.800]   is people adding accelerators to their microprocessors
[01:01:08.800 --> 01:01:11.920]   that only work well for some domains.
[01:01:11.920 --> 01:01:14.780]   And by sheer coincidence,
[01:01:14.780 --> 01:01:17.220]   at the same time that this is happening,
[01:01:17.220 --> 01:01:19.980]   has been this revolution in artificial intelligence
[01:01:19.980 --> 01:01:21.820]   called machine learning.
[01:01:21.820 --> 01:01:26.820]   So with, as I'm sure your other guests have said,
[01:01:26.820 --> 01:01:30.920]   AI had these two competing schools of thought,
[01:01:30.920 --> 01:01:33.580]   is that we could figure out artificial intelligence
[01:01:33.580 --> 01:01:35.460]   by just writing the rules top-down,
[01:01:35.460 --> 01:01:38.700]   or that was wrong, you had to look at data
[01:01:38.700 --> 01:01:41.320]   and infer what the rules are in machine learning,
[01:01:41.320 --> 01:01:45.140]   and what's happened in the last decade or eight years
[01:01:45.140 --> 01:01:47.260]   is machine learning has won.
[01:01:47.260 --> 01:01:49.860]   And it turns out that machine learning,
[01:01:49.860 --> 01:01:52.620]   the hardware you build for machine learning
[01:01:52.620 --> 01:01:55.300]   is pretty much multiply.
[01:01:55.300 --> 01:01:58.020]   The matrix multiply is a key feature
[01:01:58.020 --> 01:02:00.560]   for the way machine learning is done.
[01:02:00.560 --> 01:02:04.080]   So that's a godsend for computer designers.
[01:02:04.080 --> 01:02:07.540]   We know how to make matrix multiply run really fast.
[01:02:07.540 --> 01:02:10.180]   So general purpose microprocessors are slowing down,
[01:02:10.180 --> 01:02:12.180]   we're adding accelerators for machine learning
[01:02:12.180 --> 01:02:14.980]   that fundamentally are doing matrix multiplies
[01:02:14.980 --> 01:02:15.980]   much more efficiently
[01:02:15.980 --> 01:02:17.980]   than general purpose computers have done.
[01:02:17.980 --> 01:02:21.580]   So we have to come up with a new way to accelerate things.
[01:02:21.580 --> 01:02:23.820]   The danger of only accelerating one application
[01:02:23.820 --> 01:02:25.700]   is how important is that application.
[01:02:25.700 --> 01:02:28.300]   Turns out machine learning gets used
[01:02:28.300 --> 01:02:29.500]   for all kinds of things.
[01:02:29.500 --> 01:02:34.500]   So serendipitously, we found something to accelerate
[01:02:34.500 --> 01:02:37.140]   that's widely applicable.
[01:02:37.140 --> 01:02:39.580]   And we don't even, we're in the middle of this revolution
[01:02:39.580 --> 01:02:40.500]   of machine learning,
[01:02:40.500 --> 01:02:42.580]   we're not sure what the limits of machine learning are.
[01:02:42.580 --> 01:02:46.060]   So this has been kind of a godsend.
[01:02:46.060 --> 01:02:50.560]   If you're gonna be able to deliver on improved performance,
[01:02:50.560 --> 01:02:53.980]   as long as people are moving their programs
[01:02:53.980 --> 01:02:56.300]   to be embracing more machine learning,
[01:02:56.300 --> 01:02:58.540]   we know how to give them more performance
[01:02:58.540 --> 01:03:00.560]   even as Moore's Law is slowing down.
[01:03:00.560 --> 01:03:02.780]   - And counterintuitively,
[01:03:02.780 --> 01:03:05.780]   the machine learning mechanism,
[01:03:05.780 --> 01:03:07.740]   you can say is domain specific,
[01:03:07.740 --> 01:03:09.900]   but because it's leveraging data,
[01:03:09.900 --> 01:03:12.700]   it's actually could be very broad
[01:03:12.700 --> 01:03:17.700]   in terms of the domains it could be applied in.
[01:03:17.700 --> 01:03:19.580]   - Yeah, that's exactly right.
[01:03:19.580 --> 01:03:21.100]   - Sort of, it's almost,
[01:03:21.100 --> 01:03:23.300]   sort of people sometimes talk about
[01:03:23.300 --> 01:03:25.220]   the idea of software 2.0.
[01:03:25.220 --> 01:03:27.900]   We're almost taking another step up
[01:03:27.900 --> 01:03:29.180]   in the abstraction layer
[01:03:29.180 --> 01:03:33.000]   in designing machine learning systems,
[01:03:33.000 --> 01:03:35.400]   because now you're programming in the space of data,
[01:03:35.400 --> 01:03:37.320]   in the space of hyperparameters.
[01:03:37.320 --> 01:03:40.300]   It's changing fundamentally the nature of programming.
[01:03:40.300 --> 01:03:44.220]   And so the specialized devices that accelerate
[01:03:44.220 --> 01:03:46.300]   the performance, especially neural network based
[01:03:46.300 --> 01:03:47.780]   machine learning systems,
[01:03:47.780 --> 01:03:50.260]   might become the new general.
[01:03:50.260 --> 01:03:53.620]   - Yeah, so the thing that's interesting to point out,
[01:03:53.620 --> 01:03:57.620]   these are not tied together.
[01:03:57.620 --> 01:04:00.620]   The enthusiasm about machine learning,
[01:04:00.620 --> 01:04:03.660]   about creating programs driven from data,
[01:04:03.660 --> 01:04:05.700]   that we should figure out the answers from data
[01:04:05.700 --> 01:04:07.180]   rather than kind of top down,
[01:04:07.180 --> 01:04:10.300]   which classically the way most programming is done
[01:04:10.300 --> 01:04:12.580]   and the way artificial intelligence used to be done.
[01:04:12.580 --> 01:04:15.820]   That's a movement that's going on at the same time.
[01:04:15.820 --> 01:04:19.420]   Coincidentally, and the first word in machine learning
[01:04:19.420 --> 01:04:20.260]   is machines, right?
[01:04:20.260 --> 01:04:24.340]   So that's going to increase the demand for computing,
[01:04:24.340 --> 01:04:26.840]   because instead of programmers being smart,
[01:04:26.840 --> 01:04:28.660]   writing those things down,
[01:04:28.660 --> 01:04:31.460]   we're gonna instead use computers to examine a lot of data
[01:04:31.460 --> 01:04:33.100]   to kind of create the programs.
[01:04:33.100 --> 01:04:34.460]   That's the idea.
[01:04:35.780 --> 01:04:39.060]   And remarkably, this gets used for all kinds of things
[01:04:39.060 --> 01:04:40.060]   very successfully.
[01:04:40.060 --> 01:04:42.540]   The image recognition, the language translation,
[01:04:42.540 --> 01:04:47.540]   the game playing, and it gets into pieces of the software
[01:04:47.540 --> 01:04:50.420]   stack like databases and stuff like that.
[01:04:50.420 --> 01:04:52.540]   We're not quite sure how general purpose is,
[01:04:52.540 --> 01:04:55.100]   but that's going on independent of this hardware stuff.
[01:04:55.100 --> 01:04:57.220]   What's happening on the hardware side is Moore's law
[01:04:57.220 --> 01:05:00.060]   is slowing down right when we need a lot more cycles.
[01:05:00.060 --> 01:05:03.020]   It's failing us right when we need it,
[01:05:03.020 --> 01:05:06.940]   because there's gonna be a greater increase in computing.
[01:05:06.940 --> 01:05:09.140]   And then this idea that we're gonna do
[01:05:09.140 --> 01:05:10.500]   so-called domain specific.
[01:05:10.500 --> 01:05:13.660]   Here's a domain that your greatest fear
[01:05:13.660 --> 01:05:16.380]   is you'll make this one thing work,
[01:05:16.380 --> 01:05:19.620]   and that'll help 5% of the people in the world.
[01:05:19.620 --> 01:05:23.300]   Well, this looks like it's a very general purpose thing.
[01:05:23.300 --> 01:05:26.220]   So the timing is fortuitous that if we can,
[01:05:26.220 --> 01:05:29.700]   perhaps if we can keep building hardware
[01:05:29.700 --> 01:05:34.060]   that will accelerate machine learning, the neural networks,
[01:05:34.060 --> 01:05:36.900]   that'll beat the timing will be right,
[01:05:36.900 --> 01:05:41.460]   that neural network revolution will transform software,
[01:05:41.460 --> 01:05:43.260]   the so-called software 2.0.
[01:05:43.260 --> 01:05:45.820]   And the software of the future will be very different
[01:05:45.820 --> 01:05:47.180]   from the software of the past.
[01:05:47.180 --> 01:05:49.580]   And just as our microprocessors,
[01:05:49.580 --> 01:05:51.700]   even though we're still gonna have that same basic
[01:05:51.700 --> 01:05:55.860]   risk instructions to run a big pieces of the software stack
[01:05:55.860 --> 01:05:58.220]   like user interfaces and stuff like that,
[01:05:58.220 --> 01:06:01.100]   we can accelerate the kind of the small piece
[01:06:01.100 --> 01:06:02.380]   that's computationally intensive.
[01:06:02.380 --> 01:06:04.140]   It's not lots of lines of code,
[01:06:04.140 --> 01:06:07.180]   but it takes a lot of cycles to run that code,
[01:06:07.180 --> 01:06:09.460]   that that's gonna be the accelerator piece.
[01:06:09.460 --> 01:06:13.820]   So that's what makes this from a computer designers
[01:06:13.820 --> 01:06:16.700]   perspective, a really interesting decade.
[01:06:16.700 --> 01:06:19.220]   What Hennessy and I talked about in the title
[01:06:19.220 --> 01:06:21.820]   of our Turing-Warren speech is a new golden age.
[01:06:21.820 --> 01:06:26.020]   We see this as a very exciting decade,
[01:06:26.020 --> 01:06:28.900]   much like when we were assistant professors
[01:06:28.900 --> 01:06:30.500]   and the risk stuff was going on.
[01:06:30.500 --> 01:06:32.020]   That was a very exciting time,
[01:06:32.020 --> 01:06:33.500]   was where we were changing what was going on.
[01:06:33.500 --> 01:06:35.740]   We see this happening again,
[01:06:35.740 --> 01:06:37.980]   tremendous opportunities of people
[01:06:37.980 --> 01:06:41.500]   because we're fundamentally changing how software is built
[01:06:41.500 --> 01:06:42.820]   and how we're running it.
[01:06:42.820 --> 01:06:44.500]   - So which layer of the abstraction
[01:06:44.500 --> 01:06:47.860]   do you think most of the acceleration might be happening?
[01:06:47.860 --> 01:06:50.340]   If you look in the next 10 years,
[01:06:50.340 --> 01:06:52.500]   sort of Google is working on a lot of exciting stuff
[01:06:52.500 --> 01:06:55.700]   with the TPU, sort of there's a closer to the hardware
[01:06:55.700 --> 01:06:58.780]   that could be optimizations around the,
[01:06:58.780 --> 01:07:00.820]   a rut closer to the instruction set
[01:07:00.820 --> 01:07:02.980]   that could be optimization at the compiler level.
[01:07:02.980 --> 01:07:06.220]   It could be even at the higher level software stack.
[01:07:06.220 --> 01:07:07.460]   - Yeah, it's gotta be, I mean,
[01:07:07.460 --> 01:07:09.900]   if you think about the old RISC-Sys debate,
[01:07:09.900 --> 01:07:13.140]   it was both, it was software hardware.
[01:07:13.140 --> 01:07:15.820]   It was the compilers improving
[01:07:15.820 --> 01:07:18.220]   as well as the architecture improving.
[01:07:18.220 --> 01:07:21.820]   And that's likely to be the way things are now.
[01:07:21.820 --> 01:07:23.060]   With machine learning,
[01:07:23.740 --> 01:07:26.620]   they're using domain specific languages,
[01:07:26.620 --> 01:07:30.260]   the languages like TensorFlow and PyTorch
[01:07:30.260 --> 01:07:33.140]   are very popular with the machine learning people.
[01:07:33.140 --> 01:07:35.460]   Those are the raising the level of abstraction.
[01:07:35.460 --> 01:07:37.420]   It's easier for people to write machine learning
[01:07:37.420 --> 01:07:40.140]   in these domain specific languages
[01:07:40.140 --> 01:07:43.980]   like PyTorch and TensorFlow.
[01:07:43.980 --> 01:07:45.860]   - So where the most optimization might be happening?
[01:07:45.860 --> 01:07:49.780]   - Yeah, and so there'll be both the compiler piece
[01:07:49.780 --> 01:07:51.340]   and the hardware piece underneath it.
[01:07:51.340 --> 01:07:54.700]   So as you kind of, the fatal flaw for hardware people
[01:07:54.700 --> 01:07:57.140]   is to create really great hardware,
[01:07:57.140 --> 01:07:59.420]   but not have brought along the compilers.
[01:07:59.420 --> 01:08:01.980]   And what we're seeing right now in the marketplace,
[01:08:01.980 --> 01:08:04.940]   because of this enthusiasm around hardware
[01:08:04.940 --> 01:08:07.460]   for machine learning is getting,
[01:08:07.460 --> 01:08:10.940]   probably billions of dollars invested in startup companies.
[01:08:10.940 --> 01:08:13.540]   We're seeing startup companies go belly up
[01:08:13.540 --> 01:08:15.900]   because they focused on the hardware,
[01:08:15.900 --> 01:08:18.020]   but didn't bring the software stack along.
[01:08:18.020 --> 01:08:20.660]   We talked about benchmarks earlier.
[01:08:20.660 --> 01:08:23.980]   So I participated in machine learning,
[01:08:23.980 --> 01:08:26.500]   didn't really have a set of benchmarks.
[01:08:26.500 --> 01:08:27.460]   I think just two years ago,
[01:08:27.460 --> 01:08:28.620]   they didn't have a set of benchmarks
[01:08:28.620 --> 01:08:31.140]   and we've created something called MLPerf,
[01:08:31.140 --> 01:08:33.940]   which is machine learning benchmark suite.
[01:08:33.940 --> 01:08:37.100]   And pretty much the companies
[01:08:37.100 --> 01:08:38.860]   who didn't invest in the software stack
[01:08:38.860 --> 01:08:40.900]   couldn't run MLPerf very well.
[01:08:40.900 --> 01:08:43.820]   And the ones who did invest in software stack did,
[01:08:43.820 --> 01:08:46.500]   and we're seeing, like kind of in computer architecture,
[01:08:46.500 --> 01:08:47.340]   this is what happens.
[01:08:47.340 --> 01:08:49.380]   You have these arguments about risk versus sys.
[01:08:49.380 --> 01:08:51.460]   People spend billions of dollars in the marketplace
[01:08:51.460 --> 01:08:52.460]   to see who wins.
[01:08:52.460 --> 01:08:54.900]   It's not a perfect comparison,
[01:08:54.900 --> 01:08:56.500]   but it kind of sorts things out.
[01:08:56.500 --> 01:08:58.940]   And we're seeing companies go out of business.
[01:08:58.940 --> 01:09:00.280]   And then companies like,
[01:09:00.280 --> 01:09:04.940]   there's a company in Israel called Habana.
[01:09:04.940 --> 01:09:08.060]   They came up with machine learning accelerators.
[01:09:08.060 --> 01:09:11.140]   They had good MLPerf scores.
[01:09:11.140 --> 01:09:13.220]   Intel had acquired a company earlier
[01:09:13.220 --> 01:09:15.420]   called Nirvana a couple of years ago.
[01:09:15.420 --> 01:09:17.380]   They didn't reveal their MLPerf scores,
[01:09:17.380 --> 01:09:18.980]   which was suspicious.
[01:09:18.980 --> 01:09:20.900]   But a month ago,
[01:09:20.900 --> 01:09:22.540]   Intel announced that they're canceling
[01:09:22.540 --> 01:09:24.340]   the Nirvana product line,
[01:09:24.340 --> 01:09:26.740]   and they bought Habana for $2 billion.
[01:09:26.740 --> 01:09:30.140]   And Intel's gonna be shipping Habana chips,
[01:09:30.140 --> 01:09:32.260]   which have hardware and software
[01:09:32.260 --> 01:09:34.420]   and run the MLPerf programs pretty well.
[01:09:34.420 --> 01:09:36.940]   And that's gonna be their product line in the future.
[01:09:36.940 --> 01:09:37.780]   - Brilliant.
[01:09:37.780 --> 01:09:40.780]   So maybe just to linger briefly on MLPerf.
[01:09:40.780 --> 01:09:41.820]   I love metrics.
[01:09:41.820 --> 01:09:44.420]   I love standards that everyone can gather around.
[01:09:44.420 --> 01:09:46.340]   What are some interesting aspects
[01:09:46.340 --> 01:09:49.020]   to that portfolio of metrics?
[01:09:49.020 --> 01:09:51.020]   - Well, one of the interesting metrics is
[01:09:51.020 --> 01:09:52.900]   what we thought.
[01:09:52.900 --> 01:09:56.140]   I was involved in the start.
[01:09:56.140 --> 01:09:59.700]   But Peter Mattson is leading the effort from Google.
[01:09:59.700 --> 01:10:01.100]   Google got it off the ground,
[01:10:01.100 --> 01:10:03.500]   but we had to reach out to competitors and say,
[01:10:03.500 --> 01:10:06.900]   "There's no benchmarks here.
[01:10:06.900 --> 01:10:08.260]   "We think this is bad for the field.
[01:10:08.260 --> 01:10:10.180]   "It'll be much better if we look at examples."
[01:10:10.180 --> 01:10:11.380]   Like in the risk days,
[01:10:11.380 --> 01:10:13.580]   there was an effort to create a,
[01:10:13.580 --> 01:10:16.620]   for the people in the risk community got together,
[01:10:16.620 --> 01:10:17.580]   competitors got together,
[01:10:17.580 --> 01:10:18.980]   were building risk microprocessors
[01:10:18.980 --> 01:10:21.540]   to agree on a set of benchmarks that were called spec.
[01:10:21.540 --> 01:10:23.460]   And that was good for the industry.
[01:10:23.460 --> 01:10:26.380]   It's rather before the different risk architectures
[01:10:26.380 --> 01:10:28.340]   were arguing, "Well, you can believe my performance, others,
[01:10:28.340 --> 01:10:30.660]   "but those other guys are liars."
[01:10:30.660 --> 01:10:32.260]   And that didn't do any good.
[01:10:32.260 --> 01:10:34.660]   So we agreed on a set of benchmarks,
[01:10:34.660 --> 01:10:36.540]   and then we could figure out who was faster
[01:10:36.540 --> 01:10:38.060]   between the various risk architectures.
[01:10:38.060 --> 01:10:39.660]   But it was a little bit faster.
[01:10:39.660 --> 01:10:41.220]   But that grew the market
[01:10:41.220 --> 01:10:43.340]   rather than people were afraid to buy anything.
[01:10:43.340 --> 01:10:46.980]   So we argued the same thing would happen with ML Perf.
[01:10:46.980 --> 01:10:49.460]   Companies like Nvidia were maybe worried
[01:10:49.460 --> 01:10:50.620]   that it was some kind of trap,
[01:10:50.620 --> 01:10:52.860]   but eventually we all got together
[01:10:52.860 --> 01:10:56.100]   to create a set of benchmarks and do the right thing.
[01:10:56.100 --> 01:10:58.140]   And we agree on the results.
[01:10:58.140 --> 01:11:03.140]   And so we can see whether TPUs or GPUs or CPUs
[01:11:03.140 --> 01:11:05.580]   are really faster and how much the faster.
[01:11:05.580 --> 01:11:08.300]   And I think from an engineer's perspective,
[01:11:08.300 --> 01:11:10.860]   as long as the results are fair, you can live with it.
[01:11:10.860 --> 01:11:12.500]   Okay, you kind of tip your hat
[01:11:12.500 --> 01:11:15.540]   to your colleagues at another institution.
[01:11:15.540 --> 01:11:17.500]   Boy, they did a better job than us.
[01:11:17.500 --> 01:11:19.780]   What you hate is if it's false, right?
[01:11:19.780 --> 01:11:22.460]   They're making claims and it's just marketing bullshit,
[01:11:22.460 --> 01:11:24.100]   and that's affecting sales.
[01:11:24.100 --> 01:11:26.020]   So from an engineer's perspective,
[01:11:26.020 --> 01:11:27.940]   as long as it's a fair comparison
[01:11:27.940 --> 01:11:30.060]   and we don't come in first place, that's too bad,
[01:11:30.060 --> 01:11:31.180]   but it's fair.
[01:11:31.180 --> 01:11:33.740]   So we wanted to create that environment for ML Perf.
[01:11:33.740 --> 01:11:37.740]   And so now there's 10 companies,
[01:11:37.740 --> 01:11:40.620]   I mean, 10 universities and 50 companies involved.
[01:11:40.620 --> 01:11:45.620]   So pretty much ML Perf is the way you measure
[01:11:45.620 --> 01:11:49.020]   machine learning performance.
[01:11:49.020 --> 01:11:52.140]   And it didn't exist even two years ago.
[01:11:52.140 --> 01:11:54.900]   - One of the cool things that I enjoy about the internet,
[01:11:54.900 --> 01:11:57.540]   it has a few downsides, but one of the nice things
[01:11:57.540 --> 01:12:00.980]   is people can see through BS a little better
[01:12:00.980 --> 01:12:03.140]   with the presence of these kinds of metrics.
[01:12:03.140 --> 01:12:05.980]   So it's really nice, companies like Google
[01:12:05.980 --> 01:12:07.580]   and Facebook and Twitter.
[01:12:07.580 --> 01:12:09.420]   Now it's the cool thing to do
[01:12:09.420 --> 01:12:10.940]   is to put your engineers forward
[01:12:10.940 --> 01:12:13.860]   and to actually show off how well you do on these metrics.
[01:12:13.860 --> 01:12:15.340]   There's not sort of,
[01:12:15.340 --> 01:12:20.740]   there's less of a desire to do marketing, less so.
[01:12:20.740 --> 01:12:22.620]   Am I sort of naive?
[01:12:22.620 --> 01:12:25.340]   - No, I think, I was trying to understand
[01:12:25.340 --> 01:12:27.220]   what's changed from the '80s in this era.
[01:12:27.220 --> 01:12:30.460]   I think because of things like social networking,
[01:12:30.460 --> 01:12:31.780]   Twitter and stuff like that,
[01:12:31.780 --> 01:12:36.220]   if you put up bullshit stuff, right,
[01:12:36.220 --> 01:12:39.660]   that's just purposely misleading,
[01:12:39.660 --> 01:12:44.180]   you can get a violent reaction in social media
[01:12:44.180 --> 01:12:47.100]   pointing out the flaws in your arguments, right?
[01:12:47.100 --> 01:12:48.980]   And so from a marketing perspective,
[01:12:48.980 --> 01:12:51.700]   you have to be careful today
[01:12:51.700 --> 01:12:53.260]   that you didn't have to be careful,
[01:12:53.260 --> 01:12:56.620]   that there'll be people who put out the flaw.
[01:12:56.620 --> 01:12:58.900]   You can get the word out about the flaws
[01:12:58.900 --> 01:13:01.220]   in what you're saying much more easily today
[01:13:01.220 --> 01:13:02.700]   than in the past.
[01:13:02.700 --> 01:13:04.980]   It used to be easier to get away with it.
[01:13:04.980 --> 01:13:07.140]   And the other thing that's been happening
[01:13:07.140 --> 01:13:09.380]   in terms of showing off engineers is just,
[01:13:09.380 --> 01:13:11.780]   in the software side,
[01:13:11.780 --> 01:13:15.660]   people have largely embraced open source software.
[01:13:15.660 --> 01:13:19.740]   It was 20 years ago, it was a dirty word at Microsoft,
[01:13:19.740 --> 01:13:22.140]   and today Microsoft is one of the big proponents
[01:13:22.140 --> 01:13:24.060]   of open source software.
[01:13:24.060 --> 01:13:25.420]   The kind of, that's the standard way
[01:13:25.420 --> 01:13:26.820]   most software gets built,
[01:13:26.820 --> 01:13:29.060]   which really shows off your engineers
[01:13:29.060 --> 01:13:31.580]   because you can see, if you look at the source code,
[01:13:31.580 --> 01:13:34.900]   you can see who are making the commits,
[01:13:34.900 --> 01:13:36.100]   who's making the improvements,
[01:13:36.100 --> 01:13:38.700]   who are the engineers at all these companies
[01:13:38.700 --> 01:13:43.700]   who are really great programmers and engineers
[01:13:43.700 --> 01:13:47.260]   and making really solid contributions,
[01:13:47.260 --> 01:13:48.820]   which enhances their reputations
[01:13:48.820 --> 01:13:50.620]   and the reputation of the companies.
[01:13:50.620 --> 01:13:52.860]   - But that's, of course, not everywhere.
[01:13:52.860 --> 01:13:55.540]   Like in the space that I work more
[01:13:55.540 --> 01:13:56.900]   in is autonomous vehicles,
[01:13:56.900 --> 01:14:00.620]   and there's still, the machinery of hype and marketing
[01:14:00.620 --> 01:14:02.060]   is still very strong there,
[01:14:02.060 --> 01:14:04.460]   and there's less willingness to be open
[01:14:04.460 --> 01:14:06.860]   in this kind of open source way and sort of benchmark.
[01:14:06.860 --> 01:14:10.540]   So MLPerf represents the machine learning world
[01:14:10.540 --> 01:14:12.060]   is much better at being open source
[01:14:12.060 --> 01:14:14.900]   about holding itself to standards of different,
[01:14:14.900 --> 01:14:16.940]   the amount of incredible benchmarks
[01:14:16.940 --> 01:14:19.740]   in terms of the different computer vision,
[01:14:19.740 --> 01:14:23.460]   natural language processing tasks is incredible.
[01:14:23.460 --> 01:14:26.900]   - Historically, it wasn't always that way.
[01:14:26.900 --> 01:14:29.940]   I had a graduate student working with me, David Martin.
[01:14:29.940 --> 01:14:32.580]   So in computer, in some fields,
[01:14:32.580 --> 01:14:34.900]   benchmarking has been around forever.
[01:14:34.900 --> 01:14:39.340]   So computer architecture, databases,
[01:14:39.340 --> 01:14:41.700]   maybe operating systems,
[01:14:41.700 --> 01:14:45.420]   benchmarks are the way you measure progress.
[01:14:45.420 --> 01:14:47.700]   But he was working with me
[01:14:47.700 --> 01:14:49.900]   and then started working with Jitendra Malik,
[01:14:49.900 --> 01:14:53.140]   and Jitendra Malik in computer vision space,
[01:14:53.140 --> 01:14:55.580]   I guess you've interviewed Jitendra.
[01:14:55.580 --> 01:14:59.180]   And David Martin told me, "They don't have benchmarks.
[01:14:59.180 --> 01:15:01.020]   "Everybody has their own vision algorithm."
[01:15:01.020 --> 01:15:04.540]   And the way that, here's my image, look at how well I do.
[01:15:04.540 --> 01:15:06.100]   And everybody had their own image.
[01:15:06.100 --> 01:15:10.260]   So David Martin, back when he did his dissertation,
[01:15:10.260 --> 01:15:11.420]   figured out a way to do benchmarks.
[01:15:11.420 --> 01:15:15.620]   He had a bunch of graduate students identify images
[01:15:15.620 --> 01:15:18.300]   and then ran benchmarks to see which algorithms run well.
[01:15:18.300 --> 01:15:19.860]   And that was, as far as I know,
[01:15:19.860 --> 01:15:23.580]   kind of the first time people did benchmarks
[01:15:23.580 --> 01:15:27.620]   in computer vision, which was predated all the things
[01:15:27.620 --> 01:15:29.460]   that eventually led to ImageNet and stuff like that.
[01:15:29.460 --> 01:15:31.940]   But then the vision community got religion.
[01:15:31.940 --> 01:15:33.980]   And then once we got as far as ImageNet,
[01:15:33.980 --> 01:15:38.620]   then that let the guys in Toronto
[01:15:38.620 --> 01:15:41.580]   be able to win the ImageNet competition.
[01:15:41.580 --> 01:15:43.980]   And then that changed the whole world.
[01:15:43.980 --> 01:15:45.120]   - It's a scary step, actually,
[01:15:45.120 --> 01:15:48.340]   because when you enter the world of benchmarks,
[01:15:48.340 --> 01:15:51.100]   you actually have to be good to participate,
[01:15:51.100 --> 01:15:53.780]   as opposed to, yeah, you can just,
[01:15:53.780 --> 01:15:56.080]   you just believe you're the best in the world.
[01:15:56.080 --> 01:15:57.020]   (laughing)
[01:15:57.020 --> 01:15:58.780]   And I think the people,
[01:15:58.780 --> 01:16:00.940]   I think they weren't purposely misleading.
[01:16:00.940 --> 01:16:02.980]   I think if you don't have benchmarks,
[01:16:02.980 --> 01:16:04.340]   I mean, how do you know?
[01:16:04.340 --> 01:16:06.420]   You could have, your intuition,
[01:16:06.420 --> 01:16:07.660]   it's kind of like the way we used to do
[01:16:07.660 --> 01:16:08.700]   computer architecture.
[01:16:08.700 --> 01:16:11.500]   Your intuition is that this is the right instruction set
[01:16:11.500 --> 01:16:12.340]   to do this job.
[01:16:12.340 --> 01:16:16.820]   I believe, in my experience, my hunch is that's true.
[01:16:16.820 --> 01:16:20.060]   We had to get, to make things more quantitative
[01:16:20.060 --> 01:16:21.060]   to make progress.
[01:16:21.060 --> 01:16:24.300]   And so I just don't know how,
[01:16:24.300 --> 01:16:25.700]   in fields that don't have benchmarks,
[01:16:25.700 --> 01:16:27.620]   I don't understand how they figure out
[01:16:27.620 --> 01:16:29.020]   how they're making progress.
[01:16:29.020 --> 01:16:33.020]   - We're kind of in the vacuum tube days
[01:16:33.020 --> 01:16:34.540]   of quantum computing.
[01:16:34.540 --> 01:16:36.900]   What are your thoughts in this wholly different
[01:16:36.900 --> 01:16:38.880]   kind of space of architectures?
[01:16:38.880 --> 01:16:43.100]   - I actually, quantum computing,
[01:16:43.100 --> 01:16:44.180]   idea's been around for a while,
[01:16:44.180 --> 01:16:46.220]   and I actually thought, well, I sure hope
[01:16:46.220 --> 01:16:49.460]   I retire before I have to start teaching this.
[01:16:49.460 --> 01:16:50.460]   (laughing)
[01:16:50.460 --> 01:16:52.660]   I'd say, because I talk about,
[01:16:52.660 --> 01:16:55.140]   give these talks about the slowing of Moore's law,
[01:16:55.140 --> 01:16:58.580]   and when we need to change
[01:16:58.580 --> 01:17:01.180]   by doing domain-specific accelerators,
[01:17:01.180 --> 01:17:03.820]   common questions say, what about quantum computing?
[01:17:03.820 --> 01:17:05.740]   The reason that comes up, it's in the news all the time.
[01:17:05.740 --> 01:17:08.780]   So I think the third thing to keep in mind
[01:17:08.780 --> 01:17:12.140]   is quantum computing is not right around the corner.
[01:17:12.140 --> 01:17:14.260]   There've been two national reports,
[01:17:14.260 --> 01:17:15.700]   one by the National Academy of Engineering,
[01:17:15.700 --> 01:17:17.900]   another by the Computing Consortium,
[01:17:17.900 --> 01:17:21.680]   where they did a frank assessment of quantum computing.
[01:17:21.680 --> 01:17:25.580]   And both of those reports said,
[01:17:25.580 --> 01:17:27.100]   as far as we can tell,
[01:17:27.100 --> 01:17:30.180]   before you get error-corrected quantum computing,
[01:17:30.180 --> 01:17:31.320]   it's a decade away.
[01:17:31.320 --> 01:17:33.720]   So I think of it like nuclear fusion.
[01:17:33.720 --> 01:17:35.660]   There've been people who've been excited
[01:17:35.660 --> 01:17:36.960]   about nuclear fusion a long time.
[01:17:36.960 --> 01:17:38.280]   If we ever get nuclear fusion,
[01:17:38.280 --> 01:17:40.560]   it's gonna be fantastic for the world.
[01:17:40.560 --> 01:17:41.880]   I'm glad people are working on it,
[01:17:41.880 --> 01:17:44.120]   but it's not right around the corner.
[01:17:44.120 --> 01:17:49.360]   Those two reports, to me, say probably it'll be 2030
[01:17:49.360 --> 01:17:54.080]   before quantum computing is something that could happen.
[01:17:54.080 --> 01:17:56.160]   And when it does happen,
[01:17:56.160 --> 01:17:57.880]   this is gonna be big science stuff.
[01:17:57.880 --> 01:18:02.100]   This is micro-Kelvin, almost absolute zero things
[01:18:02.100 --> 01:18:05.840]   that if they vibrate, if a truck goes by, it won't work.
[01:18:05.840 --> 01:18:08.120]   So this'll be in data center stuff.
[01:18:08.120 --> 01:18:10.960]   We're not gonna have a quantum cell phone.
[01:18:10.960 --> 01:18:13.880]   And it's probably a 2030 kind of thing.
[01:18:13.880 --> 01:18:16.240]   So I'm happy that people are working on it,
[01:18:16.240 --> 01:18:19.680]   but just it's hard with all the news about it
[01:18:19.680 --> 01:18:22.640]   not to think that it's right around the corner.
[01:18:22.640 --> 01:18:24.920]   And that's why we need to do something
[01:18:24.920 --> 01:18:28.440]   as Moore's Law is slowing down to provide the computing,
[01:18:28.440 --> 01:18:31.040]   keep computing getting better for this next decade.
[01:18:31.040 --> 01:18:34.440]   And we shouldn't be betting on quantum computing
[01:18:34.440 --> 01:18:38.840]   or expecting quantum computing to deliver
[01:18:38.840 --> 01:18:40.160]   in the next few years.
[01:18:40.160 --> 01:18:42.680]   It's probably further off.
[01:18:42.680 --> 01:18:43.800]   I'd be happy to be wrong.
[01:18:43.800 --> 01:18:45.040]   It'd be great if quantum computing
[01:18:45.040 --> 01:18:46.600]   is gonna commercially viable,
[01:18:46.600 --> 01:18:48.960]   but it will be a set of applications.
[01:18:48.960 --> 01:18:51.200]   It's not a general purpose computation.
[01:18:51.200 --> 01:18:53.560]   So it's gonna do some amazing things,
[01:18:53.560 --> 01:18:55.920]   but there'll be a lot of things that probably,
[01:18:55.920 --> 01:18:58.200]   you know, the old fashioned computers
[01:18:58.200 --> 01:19:01.280]   are gonna keep doing better for quite a while.
[01:19:01.280 --> 01:19:03.480]   - And there'll be a teenager 50 years from now
[01:19:03.480 --> 01:19:05.280]   watching this video saying,
[01:19:05.280 --> 01:19:07.880]   look how silly David Patterson was saying.
[01:19:07.880 --> 01:19:09.920]   - No, I just said, I said 2030.
[01:19:09.920 --> 01:19:12.280]   I didn't say never.
[01:19:12.280 --> 01:19:14.160]   - We're not gonna have quantum cell phones.
[01:19:14.160 --> 01:19:16.080]   So he's gonna be watching it in a quantum cell phone.
[01:19:16.080 --> 01:19:18.720]   - I mean, I think this is such a, you know,
[01:19:18.720 --> 01:19:20.400]   given that we've had Moore's Law,
[01:19:20.400 --> 01:19:24.960]   I just, I feel comfortable trying to do projects
[01:19:24.960 --> 01:19:26.720]   that are thinking about the next decade.
[01:19:26.720 --> 01:19:28.840]   I admire people who are trying to do things
[01:19:28.840 --> 01:19:29.680]   that are 30 years out,
[01:19:29.680 --> 01:19:32.640]   but it's such a fast moving field.
[01:19:32.640 --> 01:19:34.160]   I just don't know how to,
[01:19:34.160 --> 01:19:37.080]   I'm not good enough to figure out
[01:19:37.080 --> 01:19:39.320]   what's the problem's gonna be in 30 years.
[01:19:39.320 --> 01:19:41.680]   You know, 10 years is hard enough for me.
[01:19:41.680 --> 01:19:43.640]   - So maybe if it's possible to untangle
[01:19:43.640 --> 01:19:44.920]   your intuition a little bit,
[01:19:44.920 --> 01:19:46.600]   I spoke with Jim Keller.
[01:19:46.600 --> 01:19:48.600]   I don't know if you're familiar with Jim.
[01:19:48.600 --> 01:19:53.000]   And he is trying to sort of be a little bit rebellious
[01:19:53.000 --> 01:19:54.480]   and to try to think that--
[01:19:54.480 --> 01:19:57.600]   - Yes, he quotes me as being wrong.
[01:19:57.600 --> 01:19:58.680]   - Yeah, so this is--
[01:19:58.680 --> 01:19:59.920]   - What are you, wait, wait, wait, wait,
[01:19:59.920 --> 01:20:04.920]   for the record, Jim talks about that he has an intuition
[01:20:04.920 --> 01:20:08.880]   that Moore's Law is not in fact dead yet
[01:20:08.880 --> 01:20:11.920]   and that it may continue for some time to come.
[01:20:11.920 --> 01:20:14.920]   What are your thoughts about Jim's ideas in this space?
[01:20:14.920 --> 01:20:17.320]   - Yeah, this is just marketing.
[01:20:17.320 --> 01:20:22.080]   So what Gordon Moore said is a quantitative prediction.
[01:20:22.080 --> 01:20:23.640]   We can check the facts, right?
[01:20:23.640 --> 01:20:27.800]   Which is doubling the number of transistors every two years.
[01:20:27.800 --> 01:20:31.040]   So we can look back at Intel for the last five years
[01:20:31.040 --> 01:20:36.040]   and ask him, let's look at DRAM chips six years ago.
[01:20:36.840 --> 01:20:40.140]   So that would be three two-year periods.
[01:20:40.140 --> 01:20:44.220]   So then our DRAM chips have eight times as many transistors
[01:20:44.220 --> 01:20:46.080]   as they did six years ago.
[01:20:46.080 --> 01:20:50.040]   We can look up Intel microprocessors six years ago.
[01:20:50.040 --> 01:20:51.600]   If Moore's Law is continuing,
[01:20:51.600 --> 01:20:54.700]   it should have eight times as many transistors
[01:20:54.700 --> 01:20:55.640]   as six years ago.
[01:20:55.640 --> 01:20:58.760]   The answer in both those cases is no.
[01:20:58.760 --> 01:21:03.560]   The problem has been because Moore's Law
[01:21:03.560 --> 01:21:06.080]   was kind of genuinely embraced
[01:21:06.080 --> 01:21:07.800]   by the semiconductor industry
[01:21:07.800 --> 01:21:10.640]   is they would make investments in similar equipment
[01:21:10.640 --> 01:21:12.460]   to make Moore's Law come true.
[01:21:12.460 --> 01:21:17.200]   Semiconductor improving and Moore's Law
[01:21:17.200 --> 01:21:19.840]   in many people's mind are the same thing.
[01:21:19.840 --> 01:21:23.160]   So when I say, and I'm factually correct,
[01:21:23.160 --> 01:21:26.400]   that Moore's Law is no longer holds,
[01:21:26.400 --> 01:21:29.720]   we are not doubling transistors every year's years,
[01:21:29.720 --> 01:21:31.800]   the downside for a company like Intel
[01:21:31.800 --> 01:21:35.400]   is people think that means it's stopped,
[01:21:35.400 --> 01:21:37.920]   that technology has no longer improved.
[01:21:37.920 --> 01:21:42.920]   And so Jim is trying to counteract the impression
[01:21:42.920 --> 01:21:49.880]   that semiconductors are frozen in 2019
[01:21:49.880 --> 01:21:51.300]   are never gonna get better.
[01:21:51.300 --> 01:21:53.200]   So I never said that.
[01:21:53.200 --> 01:21:56.600]   All I said was Moore's Law is no more.
[01:21:56.600 --> 01:21:58.040]   And I'm-- - Strictly look at
[01:21:58.040 --> 01:21:59.560]   the number of transistors.
[01:21:59.560 --> 01:22:02.200]   - Exactly, that's what Moore's Law is.
[01:22:02.200 --> 01:22:04.040]   There's the, I don't know,
[01:22:04.040 --> 01:22:07.840]   there's been this aura associated with Moore's Law
[01:22:07.840 --> 01:22:10.720]   that they've enjoyed for 50 years
[01:22:10.720 --> 01:22:12.360]   about look at the field we're in,
[01:22:12.360 --> 01:22:14.520]   we're doubling transistors every two years,
[01:22:14.520 --> 01:22:15.440]   what an amazing field,
[01:22:15.440 --> 01:22:18.080]   which is amazing thing that they were able to pull off.
[01:22:18.080 --> 01:22:19.760]   But even as Gordon Moore said,
[01:22:19.760 --> 01:22:21.520]   no exponential can last forever.
[01:22:21.520 --> 01:22:24.040]   It lasted for 50 years, which is amazing.
[01:22:24.040 --> 01:22:26.560]   And this is a huge impact on the industry
[01:22:26.560 --> 01:22:29.640]   because of these changes that we've been talking about.
[01:22:29.640 --> 01:22:32.680]   So he claims, because he's trying to act,
[01:22:32.680 --> 01:22:36.240]   he claims, Patterson says Moore's Law is no more
[01:22:36.240 --> 01:22:38.560]   and look at it, it's still going.
[01:22:38.560 --> 01:22:41.800]   And TSMC, they say it's no longer.
[01:22:41.800 --> 01:22:44.000]   But there's quantitative evidence
[01:22:44.000 --> 01:22:45.400]   that Moore's Law is not continuing.
[01:22:45.400 --> 01:22:47.760]   So what I say now to try and,
[01:22:47.760 --> 01:22:51.040]   okay, I understand the perception problem
[01:22:51.040 --> 01:22:53.460]   when I say Moore's Law has stopped.
[01:22:53.460 --> 01:22:55.940]   Okay, so now I say Moore's Law is slowing down
[01:22:55.940 --> 01:22:59.760]   and I think Jim, which is another way of,
[01:22:59.760 --> 01:23:02.000]   if it's predicting every two years
[01:23:02.000 --> 01:23:03.000]   and I say it's slowing down,
[01:23:03.000 --> 01:23:05.720]   then that's another way of saying it doesn't hold anymore.
[01:23:05.720 --> 01:23:10.600]   And I think Jim wouldn't disagree that it's slowing down
[01:23:10.600 --> 01:23:12.480]   because that sounds like it's,
[01:23:12.480 --> 01:23:14.640]   things are still getting better, just not as fast,
[01:23:14.640 --> 01:23:16.640]   which is another way of saying
[01:23:16.640 --> 01:23:18.560]   Moore's Law isn't working anymore.
[01:23:18.560 --> 01:23:19.880]   - It's still good for marketing.
[01:23:19.880 --> 01:23:22.800]   But what's your, you're not,
[01:23:22.800 --> 01:23:25.900]   you don't like expanding the definition of Moore's Law.
[01:23:25.900 --> 01:23:29.240]   Sort of naturally-- - Well, as an educator,
[01:23:29.240 --> 01:23:32.440]   it's just like modern politics.
[01:23:32.440 --> 01:23:34.200]   Does everybody get their own facts?
[01:23:34.200 --> 01:23:38.200]   Or do we have, Moore's Law was a crisp,
[01:23:38.200 --> 01:23:43.440]   Carver Mead looked at his Moore's Constructions
[01:23:43.440 --> 01:23:46.240]   drawing on a log-log scale, a straight line,
[01:23:46.240 --> 01:23:49.080]   and that's what the definition of Moore's Law is.
[01:23:49.080 --> 01:23:52.080]   There's this other, what Intel did for a while,
[01:23:52.080 --> 01:23:55.120]   interestingly, before Jim joined them,
[01:23:55.120 --> 01:23:57.400]   is they said, oh no, Moore's Law isn't the number of doubling
[01:23:57.400 --> 01:24:00.160]   isn't really doubling transistors every two years.
[01:24:00.160 --> 01:24:03.280]   Moore's Law is the cost of the individual transistor
[01:24:03.280 --> 01:24:08.060]   going down, cutting in half every two years.
[01:24:08.060 --> 01:24:10.240]   Now, that's not what he said, but they reinterpreted it
[01:24:10.240 --> 01:24:14.440]   because they believed that the cost of transistors
[01:24:14.440 --> 01:24:15.640]   was continuing to drop,
[01:24:15.640 --> 01:24:18.760]   even if they couldn't get twice as many chips.
[01:24:18.760 --> 01:24:20.200]   Many people in industry have told me
[01:24:20.200 --> 01:24:22.760]   that's not true anymore, that basically,
[01:24:22.760 --> 01:24:26.200]   in more recent technologies, it got more complicated,
[01:24:26.200 --> 01:24:28.200]   the actual cost of transistor went up.
[01:24:28.200 --> 01:24:32.580]   So even a corollary might not be true,
[01:24:32.580 --> 01:24:35.680]   but certainly, Moore's Law,
[01:24:35.680 --> 01:24:37.120]   that was the beauty of Moore's Law.
[01:24:37.120 --> 01:24:40.680]   It was a very simple, it's like E equals MC squared, right?
[01:24:40.680 --> 01:24:43.540]   It was like, wow, what an amazing prediction.
[01:24:43.540 --> 01:24:46.560]   It's so easy to understand, the implications are amazing,
[01:24:46.560 --> 01:24:50.120]   and that's why it was so famous as a prediction,
[01:24:50.120 --> 01:24:52.800]   and this reinterpretation of what it meant
[01:24:52.800 --> 01:24:56.240]   and changing is revisionist history,
[01:24:56.240 --> 01:24:59.400]   and I'd be happy,
[01:24:59.400 --> 01:25:02.800]   and they're not claiming there's a new Moore's Law.
[01:25:02.800 --> 01:25:05.160]   They're not saying, by the way,
[01:25:05.160 --> 01:25:08.760]   it's instead of every two years, it's every three years.
[01:25:08.760 --> 01:25:11.280]   I don't think they wanna say that.
[01:25:11.280 --> 01:25:13.800]   I think what's gonna happen is new technology revisions,
[01:25:13.800 --> 01:25:15.520]   each one's gonna get a little bit slower.
[01:25:15.520 --> 01:25:18.560]   So it is slowing down,
[01:25:18.560 --> 01:25:21.320]   the improvements won't be as great,
[01:25:21.320 --> 01:25:23.200]   and that's why we need to do new things.
[01:25:23.200 --> 01:25:26.200]   - Yeah, I don't like that the idea of Moore's Law
[01:25:26.200 --> 01:25:28.280]   is tied up with marketing.
[01:25:28.280 --> 01:25:29.920]   It would be nice if--
[01:25:29.920 --> 01:25:31.520]   - Whether it's marketing or it's,
[01:25:31.520 --> 01:25:34.720]   well, it could be affecting business,
[01:25:34.720 --> 01:25:37.720]   but it could also be affecting the imagination of engineers.
[01:25:37.720 --> 01:25:40.880]   If Intel employees actually believe
[01:25:40.880 --> 01:25:42.800]   that we're frozen in 2019,
[01:25:42.800 --> 01:25:46.620]   well, that would be bad for Intel.
[01:25:46.620 --> 01:25:48.080]   - Not just Intel, but everybody.
[01:25:48.080 --> 01:25:49.200]   - Yeah.
[01:25:49.200 --> 01:25:53.000]   - Moore's Law is inspiring to everybody.
[01:25:53.000 --> 01:25:55.640]   - But what's happening right now,
[01:25:55.640 --> 01:25:59.880]   talking to people who have working in national offices
[01:25:59.880 --> 01:26:00.700]   and stuff like that,
[01:26:00.700 --> 01:26:02.920]   a lot of the computer science community
[01:26:02.920 --> 01:26:05.400]   is unaware that this is going on,
[01:26:05.400 --> 01:26:08.960]   that we are in an era that's gonna need radical change
[01:26:08.960 --> 01:26:11.960]   at lower levels that could affect the whole software stack.
[01:26:16.600 --> 01:26:18.040]   If you're using cloud stuff
[01:26:18.040 --> 01:26:20.360]   and the servers that you get next year
[01:26:20.360 --> 01:26:22.260]   are basically only a little bit faster
[01:26:22.260 --> 01:26:24.100]   than the servers you got this year,
[01:26:24.100 --> 01:26:25.120]   you need to know that,
[01:26:25.120 --> 01:26:26.720]   and we need to start innovating
[01:26:26.720 --> 01:26:30.020]   to start delivering on it.
[01:26:30.020 --> 01:26:32.160]   If you're counting on your software
[01:26:32.160 --> 01:26:33.280]   gonna have a lot more features,
[01:26:33.280 --> 01:26:34.760]   assuming the computers are gonna get faster,
[01:26:34.760 --> 01:26:35.880]   that's not true.
[01:26:35.880 --> 01:26:36.920]   So are you gonna have to start
[01:26:36.920 --> 01:26:38.760]   making your software stack more efficient,
[01:26:38.760 --> 01:26:40.040]   or are you gonna have to start learning
[01:26:40.040 --> 01:26:41.160]   about machine learning?
[01:26:41.160 --> 01:26:46.120]   So it's kind of a warning or call for arms
[01:26:46.120 --> 01:26:47.840]   that the world is changing right now,
[01:26:47.840 --> 01:26:49.120]   and a lot of people,
[01:26:49.120 --> 01:26:51.720]   a lot of computer science PhDs are unaware of that.
[01:26:51.720 --> 01:26:54.400]   So a way to try and get their attention
[01:26:54.400 --> 01:26:56.840]   is to say that Moore's law is slowing down,
[01:26:56.840 --> 01:26:59.220]   and that's gonna affect your assumptions.
[01:26:59.220 --> 01:27:01.560]   And we're trying to get the word out.
[01:27:01.560 --> 01:27:04.240]   And when companies like TSMC and Intel say,
[01:27:04.240 --> 01:27:06.600]   "Oh, no, no, no, Moore's law is fine."
[01:27:06.600 --> 01:27:07.920]   Then people think, "Okay,
[01:27:07.920 --> 01:27:09.920]   I don't have to change my behavior.
[01:27:09.920 --> 01:27:11.200]   I'll just get the next servers."
[01:27:11.200 --> 01:27:13.720]   And if they start doing measurements,
[01:27:13.720 --> 01:27:15.240]   they'll realize what's going on.
[01:27:15.240 --> 01:27:16.920]   - It'd be nice to have some transparency
[01:27:16.920 --> 01:27:19.640]   and metrics for the layperson
[01:27:19.640 --> 01:27:23.360]   to be able to know if computers are getting faster
[01:27:23.360 --> 01:27:24.800]   and not to forget Moore's law.
[01:27:24.800 --> 01:27:26.480]   - Yeah, there are a bunch of,
[01:27:26.480 --> 01:27:28.760]   most people kind of use clock rate
[01:27:28.760 --> 01:27:31.800]   as a measure of performance.
[01:27:31.800 --> 01:27:33.400]   You know, it's not a perfect one,
[01:27:33.400 --> 01:27:34.520]   but if you've noticed,
[01:27:34.520 --> 01:27:36.240]   clock rates are more or less the same
[01:27:36.240 --> 01:27:38.760]   as they were five years ago.
[01:27:38.760 --> 01:27:40.960]   Computers are a little better than they are.
[01:27:40.960 --> 01:27:43.080]   They haven't made zero progress,
[01:27:43.080 --> 01:27:44.360]   but they've made small progress.
[01:27:44.360 --> 01:27:46.480]   So there's some indications out there.
[01:27:46.480 --> 01:27:47.520]   And then our behavior, right?
[01:27:47.520 --> 01:27:49.960]   Nobody buys the next laptop
[01:27:49.960 --> 01:27:51.320]   because it's so much faster
[01:27:51.320 --> 01:27:53.400]   than the laptop from the past.
[01:27:53.400 --> 01:27:55.960]   For cell phones, I think,
[01:27:55.960 --> 01:28:00.600]   I don't know why people buy new cell phones,
[01:28:00.600 --> 01:28:02.560]   because a new one's announced.
[01:28:02.560 --> 01:28:03.480]   The cameras are better,
[01:28:03.480 --> 01:28:05.120]   but that's kind of domain specific, right?
[01:28:05.120 --> 01:28:07.080]   They're putting special purpose hardware
[01:28:07.080 --> 01:28:10.240]   to make the processing of images go much better.
[01:28:10.240 --> 01:28:12.400]   So that's the way they're doing it.
[01:28:12.400 --> 01:28:14.040]   They're not particularly,
[01:28:14.040 --> 01:28:16.080]   it's not that the ARM processor in there
[01:28:16.080 --> 01:28:19.520]   is twice as fast as much as they've added accelerators
[01:28:19.520 --> 01:28:22.920]   to help the experience of the phone.
[01:28:22.920 --> 01:28:27.080]   - Can we talk a little bit about one other exciting space,
[01:28:27.080 --> 01:28:29.680]   arguably the same level of impact
[01:28:29.680 --> 01:28:33.280]   as your work with RISC is RAID.
[01:28:33.280 --> 01:28:38.320]   In 1988, you co-authored a paper,
[01:28:38.320 --> 01:28:42.080]   "A Case for Redundant Arrays of Inexpensive Disks,"
[01:28:42.080 --> 01:28:45.000]   hence R-A-I-D, RAID.
[01:28:45.000 --> 01:28:47.440]   So that's where you introduced the idea of RAID.
[01:28:47.440 --> 01:28:49.080]   Incredible that that little,
[01:28:49.080 --> 01:28:53.600]   I mean little, that paper kind of had this ripple effect
[01:28:53.600 --> 01:28:55.920]   and had a really a revolutionary effect.
[01:28:55.920 --> 01:28:58.080]   So first, what is RAID?
[01:28:58.080 --> 01:28:58.920]   - What is RAID?
[01:28:58.920 --> 01:29:01.800]   So this is work I did with my colleague, Randy Katz,
[01:29:01.800 --> 01:29:05.080]   and a star graduate student, Garth Gibson.
[01:29:05.080 --> 01:29:08.560]   So we had just done the fourth generation RISC project,
[01:29:09.480 --> 01:29:14.480]   and Randy Katz, which had an early Apple Macintosh computer,
[01:29:14.480 --> 01:29:20.240]   at this time, everything was done with floppy disks,
[01:29:20.240 --> 01:29:25.240]   which are old technologies that could store things
[01:29:25.240 --> 01:29:27.600]   that didn't have much capacity,
[01:29:27.600 --> 01:29:29.760]   and you had to, to get any work done,
[01:29:29.760 --> 01:29:32.480]   you're always sticking your little floppy disk in and out
[01:29:32.480 --> 01:29:33.880]   'cause they didn't have much capacity.
[01:29:33.880 --> 01:29:37.480]   But they started building what are called hard disk drives,
[01:29:37.480 --> 01:29:39.480]   which is magnetic material
[01:29:39.480 --> 01:29:44.000]   that can remember information storage for the Mac.
[01:29:44.000 --> 01:29:49.000]   And Randy asked the question when he saw this disk
[01:29:49.000 --> 01:29:51.960]   next to his Mac, "Gee, these are brand new small things."
[01:29:51.960 --> 01:29:54.240]   Before that, for the big computers,
[01:29:54.240 --> 01:29:57.680]   the disk would be the size of washing machines.
[01:29:57.680 --> 01:30:00.080]   And here's something the size of a,
[01:30:00.080 --> 01:30:02.520]   kind of the size of a book or so.
[01:30:02.520 --> 01:30:03.760]   He says, "I wonder what we could do with that."
[01:30:03.760 --> 01:30:08.760]   Well, we, Randy was involved in the fourth generation
[01:30:08.760 --> 01:30:11.720]   RISC project here at Brooklyn in the '80s.
[01:30:11.720 --> 01:30:14.720]   So we'd figured out a way how to make the computation part,
[01:30:14.720 --> 01:30:16.800]   the processor part, go a lot faster.
[01:30:16.800 --> 01:30:19.320]   But what about the storage part?
[01:30:19.320 --> 01:30:20.780]   Can we do something to make it faster?
[01:30:20.780 --> 01:30:25.380]   So we hit upon the idea of taking a lot of these disks
[01:30:25.380 --> 01:30:27.400]   developed for personal computers and Macintoshes
[01:30:27.400 --> 01:30:29.400]   and putting many of them together
[01:30:29.400 --> 01:30:31.720]   instead of one of these washing machine-sized things.
[01:30:31.720 --> 01:30:34.660]   And so we wrote the first draft of the paper
[01:30:34.660 --> 01:30:37.280]   and we'd have 40 of these little PC disks
[01:30:37.280 --> 01:30:40.680]   instead of one of these washing machine-sized things.
[01:30:40.680 --> 01:30:43.860]   And they would be much cheaper 'cause they're made for PCs.
[01:30:43.860 --> 01:30:45.520]   And they could actually kind of be faster
[01:30:45.520 --> 01:30:48.240]   'cause there was 40 of them rather than one of them.
[01:30:48.240 --> 01:30:49.520]   And so we wrote a paper like that
[01:30:49.520 --> 01:30:52.480]   and sent it to one of our former Berkeley students at IBM.
[01:30:52.480 --> 01:30:53.800]   And he said, "Well, this is all great and good,
[01:30:53.800 --> 01:30:56.240]   "but what about the reliability of these things?"
[01:30:56.240 --> 01:30:59.080]   Now you have 40 of these devices,
[01:30:59.080 --> 01:31:01.240]   each of which are kind of PC quality,
[01:31:01.240 --> 01:31:03.960]   so they're not as good as these IBM washing machines.
[01:31:03.960 --> 01:31:08.440]   IBM dominated the storage.
[01:31:08.440 --> 01:31:10.600]   So the reliability's gonna be awful.
[01:31:10.600 --> 01:31:12.400]   And so when we calculated it out,
[01:31:12.400 --> 01:31:15.580]   instead of it breaking on average once a year,
[01:31:15.580 --> 01:31:17.640]   it would break every two weeks.
[01:31:17.640 --> 01:31:20.560]   So we thought about the idea and said,
[01:31:20.560 --> 01:31:22.760]   "Well, we gotta address the reliability."
[01:31:22.760 --> 01:31:24.400]   So we did it originally performance,
[01:31:24.400 --> 01:31:25.760]   but we had the reliability.
[01:31:25.760 --> 01:31:29.360]   So the name, Redundant Array of Inexpensive Disks,
[01:31:29.360 --> 01:31:32.980]   is array of these disks, inexpensive like for PCs,
[01:31:32.980 --> 01:31:35.000]   but we have extra copies.
[01:31:35.000 --> 01:31:38.480]   So if one breaks, we won't lose all the information.
[01:31:38.480 --> 01:31:41.640]   We'll have enough redundancy that we could let some break
[01:31:41.640 --> 01:31:43.200]   and we can still preserve the information.
[01:31:43.200 --> 01:31:45.520]   So the name is an Array of Inexpensive Disks.
[01:31:45.520 --> 01:31:48.200]   This is a collection of these PCs.
[01:31:48.200 --> 01:31:51.280]   And the R part of the name was the redundancy
[01:31:51.280 --> 01:31:52.280]   so they'd be reliable.
[01:31:52.280 --> 01:31:55.560]   And it turns out if you put a modest number of extra disks
[01:31:55.560 --> 01:31:56.480]   in one of these arrays,
[01:31:56.480 --> 01:32:00.120]   it could actually not only be as faster and cheaper
[01:32:00.120 --> 01:32:01.520]   than one of these washing machine disks,
[01:32:01.520 --> 01:32:03.480]   it could be actually more reliable
[01:32:03.480 --> 01:32:05.340]   because you could have a couple of breaks
[01:32:05.340 --> 01:32:06.800]   even with these cheap disks,
[01:32:06.800 --> 01:32:09.200]   whereas one failure with the washing machine thing
[01:32:09.200 --> 01:32:10.640]   would knock it out.
[01:32:10.640 --> 01:32:13.400]   - Did you have a sense just like with risk
[01:32:13.400 --> 01:32:17.400]   that in the 30 years that followed,
[01:32:17.400 --> 01:32:22.040]   RAID would take over as a mechanism for storage?
[01:32:22.040 --> 01:32:27.040]   - I'd say, I think I'm naturally an optimist,
[01:32:27.040 --> 01:32:30.480]   but I thought our ideas were right.
[01:32:30.480 --> 01:32:32.960]   I thought kind of like Moore's law,
[01:32:32.960 --> 01:32:34.960]   it seemed to me if you looked at the history
[01:32:34.960 --> 01:32:36.180]   of the disk drives,
[01:32:36.180 --> 01:32:38.120]   they went from washing machine size things
[01:32:38.120 --> 01:32:40.200]   and they were getting smaller and smaller
[01:32:40.200 --> 01:32:43.320]   and the volumes were with the smaller disk drives
[01:32:43.320 --> 01:32:45.280]   because that's where the PCs were.
[01:32:45.280 --> 01:32:48.280]   So we thought that was a technological trend
[01:32:48.280 --> 01:32:51.560]   that disk drives, the volume of disk drives
[01:32:51.560 --> 01:32:54.360]   was gonna be getting smaller and smaller devices,
[01:32:54.360 --> 01:32:56.620]   which were true, they were the size of a,
[01:32:56.620 --> 01:32:58.560]   I don't know, eight inches diameter,
[01:32:58.560 --> 01:33:01.480]   then five inches, then three inches diameters.
[01:33:01.480 --> 01:33:04.000]   And so that it made sense to figure out
[01:33:04.000 --> 01:33:06.280]   how to deal things with an array of disks.
[01:33:06.280 --> 01:33:09.220]   So I think it was one of those things where logically,
[01:33:09.220 --> 01:33:13.600]   we think the technological forces were on our side,
[01:33:13.600 --> 01:33:14.740]   that it made sense.
[01:33:14.740 --> 01:33:17.040]   So we expected it to catch on,
[01:33:17.040 --> 01:33:20.040]   but there was that same kind of business question.
[01:33:20.040 --> 01:33:23.720]   IBM was the big pusher of these disk drives.
[01:33:23.720 --> 01:33:25.960]   In the real world, where the technical advantage
[01:33:25.960 --> 01:33:28.880]   get turned into a business advantage or not.
[01:33:28.880 --> 01:33:30.200]   It proved to be true, it did.
[01:33:30.200 --> 01:33:33.800]   And so we thought we were sound technically
[01:33:33.800 --> 01:33:36.700]   and it was unclear whether the business side,
[01:33:36.700 --> 01:33:38.440]   but we kind of, as academics,
[01:33:38.440 --> 01:33:41.480]   we believe that technology should win and it did.
[01:33:41.480 --> 01:33:44.840]   - And if you look at those 30 years,
[01:33:44.840 --> 01:33:46.060]   just from your perspective,
[01:33:46.060 --> 01:33:49.000]   are there interesting developments in the space of storage
[01:33:49.000 --> 01:33:50.440]   that have happened in that time?
[01:33:50.440 --> 01:33:52.900]   - Yeah, the big thing that happened,
[01:33:52.900 --> 01:33:54.520]   well, a couple of things that happened.
[01:33:54.520 --> 01:33:56.880]   What we did had a modest amount of storage,
[01:33:56.880 --> 01:33:59.360]   so as redundancy,
[01:33:59.360 --> 01:34:02.380]   as people built bigger and bigger storage systems,
[01:34:02.380 --> 01:34:04.400]   they've added more redundancy
[01:34:04.400 --> 01:34:05.880]   so they could add more failures.
[01:34:05.880 --> 01:34:07.960]   And the biggest thing that happened in storage
[01:34:07.960 --> 01:34:12.960]   is for decades, it was based on things physically spinning
[01:34:12.960 --> 01:34:15.600]   called hard disk drives,
[01:34:15.600 --> 01:34:17.240]   where you used to turn on your computer
[01:34:17.240 --> 01:34:18.600]   and it would make a noise.
[01:34:18.600 --> 01:34:21.560]   What that noise was, was the disk drive spinning
[01:34:21.560 --> 01:34:25.720]   and they were rotating at like 60 revolutions per second.
[01:34:25.720 --> 01:34:30.560]   And it's like, if you remember the vinyl records,
[01:34:30.560 --> 01:34:32.880]   if you've ever seen those, that's what it looked like.
[01:34:32.880 --> 01:34:34.280]   And there was like a needle
[01:34:34.280 --> 01:34:36.320]   like on a vinyl record that was reading it.
[01:34:36.320 --> 01:34:38.880]   So the big drive change is switching that over
[01:34:38.880 --> 01:34:41.540]   to a semiconductor technology called flash.
[01:34:41.540 --> 01:34:44.840]   So within the last, I'd say about decade,
[01:34:44.840 --> 01:34:47.720]   is increasing fraction of all the computers in the world
[01:34:47.720 --> 01:34:51.000]   are using semiconductor for storage.
[01:34:51.000 --> 01:34:54.960]   The flash drive, instead of being magnetic,
[01:34:54.960 --> 01:34:59.560]   they're optical, well, they're semiconductor
[01:34:59.560 --> 01:35:02.900]   writing of information very densely.
[01:35:02.900 --> 01:35:05.600]   And that's been a huge difference.
[01:35:05.600 --> 01:35:08.040]   So all the cell phones in the world use flash.
[01:35:08.040 --> 01:35:09.960]   Most of the laptops use flash.
[01:35:09.960 --> 01:35:12.840]   All the embedded devices use flash instead of storage.
[01:35:12.840 --> 01:35:16.400]   Still in the cloud, magnetic disks
[01:35:16.400 --> 01:35:18.320]   are more economical than flash,
[01:35:18.320 --> 01:35:20.240]   but they use both in the cloud.
[01:35:20.240 --> 01:35:23.160]   So it's been a huge change in the storage industry.
[01:35:23.160 --> 01:35:27.360]   Switching from primarily disk to being
[01:35:27.360 --> 01:35:28.520]   primarily semiconductor.
[01:35:28.520 --> 01:35:31.120]   - For the individual disk, but still the RAID mechanism
[01:35:31.120 --> 01:35:32.680]   applies to those different kinds of disks.
[01:35:32.680 --> 01:35:36.040]   - Yes, the people will still use RAID ideas
[01:35:36.040 --> 01:35:38.480]   because it's kind of what's different,
[01:35:38.480 --> 01:35:41.260]   kind of interesting, kind of psychologically,
[01:35:41.260 --> 01:35:42.680]   if you think about it.
[01:35:42.680 --> 01:35:45.360]   People have always worried about the reliability
[01:35:45.360 --> 01:35:47.080]   of computing since the earliest days.
[01:35:47.080 --> 01:35:51.040]   So kind of, but if we're talking about computation,
[01:35:51.040 --> 01:35:56.040]   if your computer makes a mistake and the computer says,
[01:35:56.040 --> 01:35:57.920]   the computer has ways to check and say,
[01:35:57.920 --> 01:36:00.720]   oh, we screwed up, we made a mistake.
[01:36:00.720 --> 01:36:03.360]   What happens is that program that was running,
[01:36:03.360 --> 01:36:06.000]   you have to redo it, which is a hassle.
[01:36:06.000 --> 01:36:11.000]   For storage, if you've sent important information away,
[01:36:11.580 --> 01:36:14.740]   and it loses that information, you go nuts.
[01:36:14.740 --> 01:36:16.660]   This is the worst, oh my God.
[01:36:16.660 --> 01:36:19.700]   So if you have a laptop and you're not backing it up
[01:36:19.700 --> 01:36:21.220]   on the cloud or something like this,
[01:36:21.220 --> 01:36:24.980]   and your disk drive breaks, which it can do,
[01:36:24.980 --> 01:36:26.500]   you'll lose all that information
[01:36:26.500 --> 01:36:27.740]   and you just go crazy, right?
[01:36:27.740 --> 01:36:30.660]   So the importance of reliability for storage
[01:36:30.660 --> 01:36:32.660]   is tremendously higher than the importance
[01:36:32.660 --> 01:36:34.340]   of reliability for computation
[01:36:34.340 --> 01:36:36.260]   because of the consequences of it.
[01:36:36.260 --> 01:36:39.180]   So yes, so RAID ideas are still very popular,
[01:36:39.180 --> 01:36:41.020]   even with the switch of the technology.
[01:36:41.020 --> 01:36:43.640]   Although flash drives are more reliable.
[01:36:43.640 --> 01:36:47.180]   If you're not doing anything like backing it up
[01:36:47.180 --> 01:36:49.140]   to get some redundancy so they handle it,
[01:36:49.140 --> 01:36:51.740]   you're taking great risks.
[01:36:51.740 --> 01:36:56.460]   - You said that for you and possibly for many others,
[01:36:56.460 --> 01:37:00.060]   teaching and research don't conflict with each other
[01:37:00.060 --> 01:37:02.000]   as one might suspect, and in fact,
[01:37:02.000 --> 01:37:03.440]   they kind of complement each other.
[01:37:03.440 --> 01:37:05.980]   So maybe a question I have is,
[01:37:05.980 --> 01:37:08.260]   how has teaching helped you in your research
[01:37:08.260 --> 01:37:12.300]   or just in your entirety as a person
[01:37:12.300 --> 01:37:14.460]   who both teaches and does research
[01:37:14.460 --> 01:37:18.160]   and just thinks and creates new ideas in this world?
[01:37:18.160 --> 01:37:20.740]   - Yes, I think what happens is,
[01:37:20.740 --> 01:37:22.320]   is when you're a college student,
[01:37:22.320 --> 01:37:24.080]   you know there's this kind of tenure system
[01:37:24.080 --> 01:37:24.920]   in doing research.
[01:37:24.920 --> 01:37:29.920]   So kind of this model that is popular in America,
[01:37:29.920 --> 01:37:31.780]   I think America really made it happen,
[01:37:31.780 --> 01:37:34.360]   is we can attract these really great faculty
[01:37:34.360 --> 01:37:37.500]   to research universities because they get to do research
[01:37:37.500 --> 01:37:38.380]   as well as teach.
[01:37:38.380 --> 01:37:40.780]   And that, especially in fast moving fields,
[01:37:40.780 --> 01:37:42.260]   this means people are up to date
[01:37:42.260 --> 01:37:44.260]   and they're teaching those kinds of things.
[01:37:44.260 --> 01:37:46.420]   But when you run into a really bad professor,
[01:37:46.420 --> 01:37:49.220]   a really bad teacher, I think the students think,
[01:37:49.220 --> 01:37:51.460]   well this guy must be a great researcher
[01:37:51.460 --> 01:37:53.500]   'cause why else could he be here?
[01:37:53.500 --> 01:37:57.140]   So as I, you know, after 40 years at Berkeley,
[01:37:57.140 --> 01:37:59.500]   we had a retirement party and I got a chance to reflect
[01:37:59.500 --> 01:38:01.380]   and I looked back at some things.
[01:38:01.380 --> 01:38:03.500]   That is not my experience.
[01:38:03.500 --> 01:38:07.400]   There's a, I saw a photograph of five of us
[01:38:07.400 --> 01:38:10.100]   in the department who won the Distinguished Teaching Award
[01:38:10.100 --> 01:38:11.800]   from campus, a very high honor.
[01:38:11.800 --> 01:38:14.220]   You know, I've got one of those, one of the highest honors.
[01:38:14.220 --> 01:38:16.300]   So there are five of us on that picture.
[01:38:16.300 --> 01:38:21.300]   There's Manuel Blum, Richard Karp, me, Randy Kass
[01:38:21.300 --> 01:38:24.420]   and John Osterhout, contemporaries of mine.
[01:38:24.420 --> 01:38:26.360]   I mentioned Randy already.
[01:38:26.360 --> 01:38:29.180]   All of us are in the National Academy of Engineering.
[01:38:29.180 --> 01:38:32.140]   We've all run the Distinguished Teaching Award.
[01:38:32.140 --> 01:38:35.060]   Blum, Karp and I all have Turing Awards.
[01:38:35.060 --> 01:38:36.540]   - Turing Awards, right.
[01:38:36.540 --> 01:38:38.860]   - You know, the highest award in computing.
[01:38:38.860 --> 01:38:43.100]   So that's the opposite, right?
[01:38:43.100 --> 01:38:46.020]   What happens is if you, it's, they're highly correlated.
[01:38:46.020 --> 01:38:48.900]   So probably, the other way to think of it,
[01:38:48.900 --> 01:38:50.860]   if you're very successful people
[01:38:50.860 --> 01:38:52.780]   or maybe successful at everything they do,
[01:38:52.780 --> 01:38:54.420]   it's not an either or.
[01:38:54.420 --> 01:38:57.580]   - But it's an interesting question whether specifically,
[01:38:57.580 --> 01:39:00.180]   that's probably true, but specifically for teaching,
[01:39:00.180 --> 01:39:02.540]   if there's something in teaching that,
[01:39:02.540 --> 01:39:04.980]   it's the Richard Feynman, right, idea.
[01:39:04.980 --> 01:39:06.380]   Is there something about teaching
[01:39:06.380 --> 01:39:08.380]   that actually makes your research,
[01:39:08.380 --> 01:39:11.500]   makes you think deeper and more outside the box
[01:39:11.500 --> 01:39:12.780]   and more insightful?
[01:39:12.780 --> 01:39:14.260]   - Absolutely, I was gonna bring up Feynman.
[01:39:14.260 --> 01:39:17.180]   I mean, he criticized the Institute of Advanced Studies.
[01:39:17.180 --> 01:39:19.940]   So the Institute of Advanced Studies
[01:39:19.940 --> 01:39:21.940]   was this thing that was created near Princeton
[01:39:21.940 --> 01:39:24.260]   where Einstein and all these smart people went.
[01:39:24.260 --> 01:39:27.420]   And when he was invited, he thought it was a terrible idea.
[01:39:27.420 --> 01:39:30.460]   This is a university, it was supposed to be heaven, right?
[01:39:30.460 --> 01:39:32.700]   A university without any teaching.
[01:39:32.700 --> 01:39:33.940]   But he thought it was a mistake.
[01:39:33.940 --> 01:39:35.620]   It's getting up in the classroom
[01:39:35.620 --> 01:39:37.660]   and having to explain things to students
[01:39:37.660 --> 01:39:39.140]   and having them ask questions.
[01:39:39.140 --> 01:39:40.740]   Like, well, why is that true?
[01:39:40.740 --> 01:39:41.860]   Makes you stop and think.
[01:39:41.860 --> 01:39:45.660]   So he thought, and I agree,
[01:39:45.660 --> 01:39:48.740]   I think that interaction between a research university
[01:39:48.740 --> 01:39:51.140]   and having students with bright young men
[01:39:51.140 --> 01:39:54.620]   asking hard questions the whole time is synergistic.
[01:39:54.620 --> 01:39:58.060]   And a university without teaching
[01:39:58.060 --> 01:40:02.020]   wouldn't be as vital and exciting a place.
[01:40:02.020 --> 01:40:05.220]   And I think it helps stimulate the research.
[01:40:05.220 --> 01:40:08.060]   - Another romanticized question,
[01:40:08.060 --> 01:40:12.380]   but what's your favorite concept or idea to teach?
[01:40:12.380 --> 01:40:15.860]   What inspires you or you see inspire the students?
[01:40:15.860 --> 01:40:17.100]   Is there something that pops to mind?
[01:40:17.100 --> 01:40:19.540]   Or puts the fear of God in them, I don't know.
[01:40:19.540 --> 01:40:20.940]   Whichever is most effective.
[01:40:20.940 --> 01:40:25.380]   - I mean, in general, I think people are surprised.
[01:40:25.380 --> 01:40:26.340]   I've seen a lot of people
[01:40:26.340 --> 01:40:28.860]   who don't think they like teaching
[01:40:28.860 --> 01:40:31.820]   come give guest lectures or teach a course
[01:40:31.820 --> 01:40:34.860]   and get hooked on seeing the lights turn on, right?
[01:40:34.860 --> 01:40:37.620]   Is people, you can explain something to people
[01:40:37.620 --> 01:40:39.140]   that they don't understand
[01:40:39.140 --> 01:40:41.500]   and suddenly they get something, you know,
[01:40:41.500 --> 01:40:44.340]   that's not, that's important and difficult.
[01:40:44.340 --> 01:40:45.900]   And just seeing the lights turn on
[01:40:45.900 --> 01:40:49.140]   is a real satisfaction there.
[01:40:49.140 --> 01:40:53.980]   I don't think there's any specific example of that.
[01:40:53.980 --> 01:40:58.980]   It's just the general joy of seeing them understand.
[01:40:59.300 --> 01:41:02.580]   - I have to talk about this, because I've wrestled.
[01:41:02.580 --> 01:41:03.820]   I do martial arts.
[01:41:03.820 --> 01:41:05.260]   Yeah, of course I love wrestling.
[01:41:05.260 --> 01:41:06.900]   I'm a huge, I'm Russian, so.
[01:41:06.900 --> 01:41:07.780]   - Ah, oh sure.
[01:41:07.780 --> 01:41:10.900]   - I have talked to Dan Gable on the podcast.
[01:41:10.900 --> 01:41:12.420]   (laughing)
[01:41:12.420 --> 01:41:14.860]   - Dan Gable was my era kind of guy.
[01:41:14.860 --> 01:41:16.820]   - So you wrestled at UCLA,
[01:41:16.820 --> 01:41:19.580]   among many other things you've done in your life,
[01:41:19.580 --> 01:41:21.620]   competitively in sports and science and so on.
[01:41:21.620 --> 01:41:25.220]   You've wrestled, maybe, again,
[01:41:25.220 --> 01:41:26.940]   continuing with the romanticized questions,
[01:41:26.940 --> 01:41:29.780]   but what have you learned about life
[01:41:29.780 --> 01:41:32.300]   and maybe even science from wrestling or from?
[01:41:32.300 --> 01:41:36.020]   - Yeah, that's, in fact, I wrestled at UCLA,
[01:41:36.020 --> 01:41:38.340]   but also at El Camino Community College.
[01:41:38.340 --> 01:41:42.140]   And just right now, we were, in the state of California,
[01:41:42.140 --> 01:41:43.780]   we were state champions at El Camino.
[01:41:43.780 --> 01:41:45.900]   And in fact, I was talking to my mom,
[01:41:45.900 --> 01:41:48.100]   and I got into UCLA,
[01:41:48.100 --> 01:41:50.500]   but I decided to go to the community college,
[01:41:50.500 --> 01:41:53.060]   which is, it's much harder to go to UCLA
[01:41:53.060 --> 01:41:54.620]   than the community college.
[01:41:54.620 --> 01:41:56.220]   And I asked, why did I make that decision?
[01:41:56.220 --> 01:41:57.940]   'Cause I thought it was because of my girlfriend.
[01:41:57.940 --> 01:41:59.500]   She said, "Well, it was the girlfriend,
[01:41:59.500 --> 01:42:01.220]   "and you thought the wrestling team was really good."
[01:42:01.220 --> 01:42:02.180]   (laughing)
[01:42:02.180 --> 01:42:04.140]   And we were right, we had a great wrestling team.
[01:42:04.140 --> 01:42:08.460]   We actually wrestled against UCLA at a tournament,
[01:42:08.460 --> 01:42:10.060]   and we beat UCLA.
[01:42:10.060 --> 01:42:11.620]   It's a community college,
[01:42:11.620 --> 01:42:13.940]   which is just freshmen and sophomores.
[01:42:13.940 --> 01:42:15.700]   And part of the reason I brought this up
[01:42:15.700 --> 01:42:18.540]   is I'm gonna go, they've invited me back at El Camino
[01:42:18.540 --> 01:42:22.180]   to give a lecture next month.
[01:42:22.180 --> 01:42:27.180]   And so, my friend who was on the wrestling team,
[01:42:27.180 --> 01:42:28.740]   that we're still together,
[01:42:28.740 --> 01:42:30.620]   we're right now reaching out to other members
[01:42:30.620 --> 01:42:31.460]   of the wrestling team,
[01:42:31.460 --> 01:42:33.460]   so we can get together for a reunion.
[01:42:33.460 --> 01:42:36.300]   But in terms of me, it was a huge difference.
[01:42:36.300 --> 01:42:39.300]   I was both, I was kind of,
[01:42:39.300 --> 01:42:41.340]   the age cutoff, it was December 1st,
[01:42:41.340 --> 01:42:44.960]   and so I was almost always the youngest person in my class.
[01:42:44.960 --> 01:42:49.560]   And I matured later, our family matured later,
[01:42:49.560 --> 01:42:51.580]   so I was almost always the smallest guy.
[01:42:51.580 --> 01:42:56.580]   So, I took kind of nerdy courses, but I was wrestling,
[01:42:56.580 --> 01:43:01.860]   so wrestling was huge for my self-confidence in high school.
[01:43:01.860 --> 01:43:06.060]   And then, I kind of got bigger at El Camino and in college,
[01:43:06.060 --> 01:43:11.060]   and so I had this kind of physical self-confidence.
[01:43:11.060 --> 01:43:16.140]   And it's translated into research self-confidence.
[01:43:16.140 --> 01:43:21.340]   And also kind of, I've had this feeling even today,
[01:43:21.980 --> 01:43:26.700]   in my 70s, if something going on in the streets
[01:43:26.700 --> 01:43:29.340]   that is bad physically, I'm not gonna ignore it, right?
[01:43:29.340 --> 01:43:32.380]   I'm gonna stand up and try and straighten that out.
[01:43:32.380 --> 01:43:34.120]   - And that kind of confidence just carries
[01:43:34.120 --> 01:43:35.460]   through the entirety of your life.
[01:43:35.460 --> 01:43:37.820]   - Yeah, and the same things happens intellectually.
[01:43:37.820 --> 01:43:39.220]   If there's something going on
[01:43:39.220 --> 01:43:41.680]   where people are saying something that's not true,
[01:43:41.680 --> 01:43:43.740]   I feel it's my job to stand up,
[01:43:43.740 --> 01:43:45.300]   just like I would in the street,
[01:43:45.300 --> 01:43:47.060]   if there's something going on.
[01:43:47.060 --> 01:43:48.460]   Somebody attacking some woman or something,
[01:43:48.980 --> 01:43:51.420]   I'm not standing by and letting that get away.
[01:43:51.420 --> 01:43:53.580]   So I feel it's my job to stand up,
[01:43:53.580 --> 01:43:55.900]   so it kind of ironically translates.
[01:43:55.900 --> 01:43:57.380]   The other things that turned out,
[01:43:57.380 --> 01:44:01.460]   for both, I had really great college and high school coaches
[01:44:01.460 --> 01:44:03.580]   and they believed, even though wrestling's
[01:44:03.580 --> 01:44:06.340]   an individual sport, that we'd be more successful
[01:44:06.340 --> 01:44:08.540]   as a team if we bonded together,
[01:44:08.540 --> 01:44:11.140]   you'd do things that we would support each other,
[01:44:11.140 --> 01:44:12.140]   rather than everybody, you know,
[01:44:12.140 --> 01:44:13.660]   in wrestling it's a one-on-one,
[01:44:13.660 --> 01:44:15.460]   and you could be everybody's on their own.
[01:44:15.460 --> 01:44:18.540]   But he felt if we bonded as a team, we'd succeed.
[01:44:18.540 --> 01:44:20.620]   So I kind of picked up those skills
[01:44:20.620 --> 01:44:24.820]   of how to form successful teams from wrestling.
[01:44:24.820 --> 01:44:27.500]   And so I think, most people would say,
[01:44:27.500 --> 01:44:31.740]   one of my strengths is I can create teams of faculty,
[01:44:31.740 --> 01:44:32.980]   large teams of faculty, grad students,
[01:44:32.980 --> 01:44:34.900]   pull all together for a common goal
[01:44:34.900 --> 01:44:38.500]   and often be successful at it.
[01:44:38.500 --> 01:44:41.860]   But I got both of those things from wrestling.
[01:44:41.860 --> 01:44:44.300]   Also, I think, I heard this line about
[01:44:44.300 --> 01:44:48.180]   if people are in kind of collision,
[01:44:48.180 --> 01:44:50.700]   sports with physical contact like wrestling
[01:44:50.700 --> 01:44:51.860]   or football and stuff like that,
[01:44:51.860 --> 01:44:56.020]   people are a little bit more assertive or something.
[01:44:56.020 --> 01:44:59.340]   And so I think that also comes through.
[01:44:59.340 --> 01:45:03.740]   I didn't shy away from the risk-assist debates.
[01:45:03.740 --> 01:45:07.340]   I enjoyed taking on the arguments and stuff like that.
[01:45:07.340 --> 01:45:10.620]   So it was, I'm really glad I did wrestling.
[01:45:10.620 --> 01:45:12.860]   I think it was really good for my self-image
[01:45:12.860 --> 01:45:14.020]   and I learned a lot from it.
[01:45:14.020 --> 01:45:17.420]   So I think that's, sports done well,
[01:45:17.420 --> 01:45:20.540]   there's really lots of positives you can take about it,
[01:45:20.540 --> 01:45:25.540]   of leadership, how to form teams and how to be successful.
[01:45:25.540 --> 01:45:28.580]   - So we've talked about metrics a lot.
[01:45:28.580 --> 01:45:29.860]   There's a really cool,
[01:45:29.860 --> 01:45:31.580]   in terms of bench press and weightlifting,
[01:45:31.580 --> 01:45:33.300]   pioneers metric that you've developed
[01:45:33.300 --> 01:45:34.820]   that we don't have time to talk about,
[01:45:34.820 --> 01:45:37.420]   but it's a really cool one that people should look into.
[01:45:37.420 --> 01:45:38.900]   It's rethinking the way we think
[01:45:38.900 --> 01:45:40.500]   about metrics and weightlifting.
[01:45:40.500 --> 01:45:42.500]   But let me talk about metrics more broadly,
[01:45:42.500 --> 01:45:45.420]   since that appeals to you in all forms.
[01:45:45.420 --> 01:45:47.700]   Let's look at the most ridiculous,
[01:45:47.700 --> 01:45:50.600]   the biggest question of the meaning of life.
[01:45:50.600 --> 01:45:54.040]   If you were to try to put metrics on a life well-lived,
[01:45:54.040 --> 01:45:55.440]   what would those metrics be?
[01:45:55.440 --> 01:45:59.740]   - Yeah, a friend of mine, Randy Katz said this.
[01:45:59.740 --> 01:46:04.340]   He said, "When it's time to sign off,
[01:46:04.340 --> 01:46:08.580]   "the measure isn't the number of zeros in your bank account,
[01:46:08.580 --> 01:46:10.880]   "it's the number of inches in the obituary
[01:46:10.880 --> 01:46:12.340]   "in the New York Times."
[01:46:12.340 --> 01:46:13.180]   (Dave laughs)
[01:46:13.180 --> 01:46:14.000]   That's what he said.
[01:46:14.000 --> 01:46:15.680]   I think having,
[01:46:15.680 --> 01:46:18.940]   and this is a cliche,
[01:46:18.940 --> 01:46:20.620]   is that people don't die
[01:46:20.620 --> 01:46:23.420]   wishing they'd spent more time in the office, right?
[01:46:23.420 --> 01:46:25.740]   As I reflect upon my career,
[01:46:25.740 --> 01:46:28.620]   there have been a half a dozen,
[01:46:28.620 --> 01:46:30.900]   or a dozen things, say, I've been proud of.
[01:46:30.900 --> 01:46:33.780]   A lot of them aren't papers or scientific results.
[01:46:33.780 --> 01:46:36.060]   Certainly my family, my wife,
[01:46:36.060 --> 01:46:39.220]   we've been married more than 50 years,
[01:46:39.220 --> 01:46:41.520]   kids and grandkids, that's really precious.
[01:46:42.480 --> 01:46:46.680]   The education things I've done, I'm very proud of.
[01:46:46.680 --> 01:46:49.400]   Books and courses.
[01:46:49.400 --> 01:46:51.840]   I did some help with underrepresented groups
[01:46:51.840 --> 01:46:52.840]   that was effective.
[01:46:52.840 --> 01:46:54.360]   So it was interesting to see
[01:46:54.360 --> 01:46:56.200]   what were the things I reflected.
[01:46:56.200 --> 01:46:58.560]   I had hundreds of papers,
[01:46:58.560 --> 01:47:00.480]   but some of them were the papers,
[01:47:00.480 --> 01:47:02.060]   like the risk and rate stuff that I'm proud of,
[01:47:02.060 --> 01:47:04.520]   but a lot of them were not those things.
[01:47:04.520 --> 01:47:08.040]   So people who just spend their lives
[01:47:08.040 --> 01:47:09.520]   going after the dollars
[01:47:09.520 --> 01:47:12.280]   or going after all the papers in the world,
[01:47:12.280 --> 01:47:14.000]   that's probably not the things
[01:47:14.000 --> 01:47:16.080]   that afterwards you're gonna care about.
[01:47:16.080 --> 01:47:21.520]   When I got the offer from Berkeley before I showed up,
[01:47:21.520 --> 01:47:24.520]   I read a book where they interviewed a lot of people
[01:47:24.520 --> 01:47:25.480]   in all works of life,
[01:47:25.480 --> 01:47:26.980]   and what I got out of that book
[01:47:26.980 --> 01:47:28.760]   was the people who felt good about what they did
[01:47:28.760 --> 01:47:30.760]   was the people who affected people,
[01:47:30.760 --> 01:47:32.920]   as opposed to things that were more transitory.
[01:47:32.920 --> 01:47:34.680]   So I came into this job
[01:47:34.680 --> 01:47:36.560]   assuming that it wasn't gonna be the papers,
[01:47:36.560 --> 01:47:39.000]   it was gonna be relationships with the people over time
[01:47:39.000 --> 01:47:41.320]   that I would value,
[01:47:41.320 --> 01:47:43.600]   and that was a correct assessment.
[01:47:43.600 --> 01:47:45.920]   It's the people you work with,
[01:47:45.920 --> 01:47:47.080]   the people you can influence,
[01:47:47.080 --> 01:47:48.000]   the people you can help,
[01:47:48.000 --> 01:47:49.680]   is the things that you feel good about
[01:47:49.680 --> 01:47:50.520]   towards the end of your career.
[01:47:50.520 --> 01:47:53.500]   It's not the stuff that's more transitory.
[01:47:53.500 --> 01:47:56.340]   - I don't think there's a better way to end it
[01:47:56.340 --> 01:47:58.640]   than talking about your family,
[01:47:58.640 --> 01:48:01.080]   the over 50 years of being married
[01:48:01.080 --> 01:48:02.880]   to your childhood sweetheart.
[01:48:02.880 --> 01:48:04.400]   - What I think I could add is,
[01:48:04.400 --> 01:48:06.800]   when you tell people you've been married 50 years,
[01:48:06.800 --> 01:48:08.200]   they wanna know why.
[01:48:08.200 --> 01:48:09.200]   - How, why?
[01:48:09.200 --> 01:48:11.600]   - Yeah, I can tell you the nine magic words
[01:48:11.600 --> 01:48:13.800]   that you need to say to your partner
[01:48:13.800 --> 01:48:15.960]   to keep a good relationship.
[01:48:15.960 --> 01:48:17.680]   And the nine magic words are,
[01:48:17.680 --> 01:48:21.200]   I was wrong, you were right, I love you.
[01:48:21.200 --> 01:48:22.040]   - Okay.
[01:48:22.040 --> 01:48:22.860]   - And you gotta say all nine.
[01:48:22.860 --> 01:48:25.480]   You can't say, I was wrong, you were right, you're a jerk.
[01:48:25.480 --> 01:48:27.120]   You know, you can't say that.
[01:48:27.120 --> 01:48:30.080]   So yeah, freely acknowledging that you made a mistake,
[01:48:30.080 --> 01:48:31.160]   the other person was right,
[01:48:31.160 --> 01:48:32.920]   and that you love them
[01:48:32.920 --> 01:48:36.480]   really gets over a lot of bumps in the road.
[01:48:36.480 --> 01:48:38.520]   So that's what I pass along.
[01:48:38.520 --> 01:48:39.640]   - Beautifully put.
[01:48:39.640 --> 01:48:41.360]   David, it is a huge honor.
[01:48:41.360 --> 01:48:42.920]   Thank you so much for the book you've written,
[01:48:42.920 --> 01:48:45.120]   for the research you've done, for changing the world.
[01:48:45.120 --> 01:48:46.000]   Thank you for talking today.
[01:48:46.000 --> 01:48:48.080]   - Oh, thanks for the interview.
[01:48:48.080 --> 01:48:49.560]   - Thanks for listening to this conversation
[01:48:49.560 --> 01:48:50.800]   with David Patterson,
[01:48:50.800 --> 01:48:53.280]   and thank you to our sponsors,
[01:48:53.280 --> 01:48:56.740]   the Jordan Harbinger Show and Cash App.
[01:48:56.740 --> 01:48:58.520]   Please consider supporting this podcast
[01:48:58.520 --> 01:49:01.780]   by going to jordanharbinger.com/lex
[01:49:01.780 --> 01:49:06.000]   and downloading Cash App and using code LEXPODCAST.
[01:49:06.000 --> 01:49:08.200]   Click the links, buy the stuff.
[01:49:08.200 --> 01:49:10.040]   It's the best way to support this podcast
[01:49:10.040 --> 01:49:12.000]   and the journey I'm on.
[01:49:12.000 --> 01:49:15.000]   If you enjoy this thing, subscribe on YouTube,
[01:49:15.000 --> 01:49:17.160]   review it with 5,000 F on podcast,
[01:49:17.160 --> 01:49:18.480]   support it on Patreon,
[01:49:18.480 --> 01:49:20.680]   or connect with me on Twitter at Lex Friedman,
[01:49:20.680 --> 01:49:23.360]   spelled without the E,
[01:49:23.360 --> 01:49:24.720]   try to figure out how to do that.
[01:49:24.720 --> 01:49:27.480]   It's just F-R-I-D-M-A-N.
[01:49:27.480 --> 01:49:29.640]   And now let me leave you with some words
[01:49:29.640 --> 01:49:32.400]   from Henry David Thoreau.
[01:49:32.400 --> 01:49:35.600]   Our life is fretted away by detail.
[01:49:35.600 --> 01:49:38.040]   Simplify, simplify.
[01:49:38.040 --> 01:49:42.440]   Thank you for listening and hope to see you next time.
[01:49:42.440 --> 01:49:45.020]   (upbeat music)
[01:49:45.020 --> 01:49:47.600]   (upbeat music)
[01:49:47.600 --> 01:49:57.600]   [BLANK_AUDIO]

