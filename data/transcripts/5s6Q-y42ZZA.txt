
[00:00:00.060 --> 00:00:08.280]   Hi, I'm Ray Myers. I'm currently Chief Architect at All Hands AI, makers of the leading open source coding agent called Open Hands.
[00:00:08.280 --> 00:00:10.840]   But I'd like to talk about something different today.
[00:00:10.840 --> 00:00:17.360]   And I'm proud to be presenting at the online track for AI Engineer World's Fair.
[00:00:17.360 --> 00:00:24.540]   But I feel like I've actually snuck in the back door today because this is not an AI talk.
[00:00:24.540 --> 00:00:28.540]   Perhaps this is not even a programming talk.
[00:00:29.340 --> 00:00:31.560]   This is a talk about empathy.
[00:00:31.560 --> 00:00:35.040]   This is a talk about listening to each other.
[00:00:35.040 --> 00:00:43.000]   But if those things are uncomfortable for you, don't worry, because we will have the pleasant, comforting backdrop of AI and programming.
[00:00:43.000 --> 00:00:52.400]   For starters, let me take you through a day in the life of an AI skeptic, which is the role I so often find myself in.
[00:00:52.400 --> 00:00:58.580]   What happens is someone will say something provocative in public, like you may have seen the CEO of Anthropic,
[00:00:58.680 --> 00:01:04.200]   Dario Amadei say a few months ago at a Council on Foreign Relations interview.
[00:01:05.320 --> 00:01:15.360]   Many interesting things in that interview, but a quote that we saw got shared around a lot was in 12 months, we may be in a world where AI is writing essentially all the code.
[00:01:15.360 --> 00:01:29.280]   Right now, speaking to a general audience, as software engineers, we hear a little bit different when you're saying the part about writing the code, because we understand that the job contains, you know, other factors.
[00:01:29.280 --> 00:01:40.380]   But regardless, I posted a friendly challenge in response to that to do with, like, could you replace the software in one mainframe?
[00:01:40.500 --> 00:01:51.580]   Can we kill one mainframe even, how difficult is that right now with these AI tools that are soon to supposedly write essentially all the code, right?
[00:01:52.300 --> 00:01:55.820]   And the specifics of my challenge are not really that relevant right now.
[00:01:55.820 --> 00:02:01.660]   So, just over the past two years, I have repeatedly said different forms of this.
[00:02:01.660 --> 00:02:07.340]   I have repeatedly said different forms of this LLMs, large language models, break old code.
[00:02:07.340 --> 00:02:31.340]   I say that a lot because I think it is ignored, simply the importance of maintaining old code and keeping it alive, that already is ignored and the extent to which these AI tools are performing much better on writing new code than in editing code that already exists, I feel like that's sort of doubly ignored as a result.
[00:02:31.340 --> 00:02:41.860]   I feel like I raise pretty basic questions and point out pretty obvious limitations a lot of the time and get somewhat extreme reactions, honestly.
[00:02:41.860 --> 00:02:43.540]   You know, I've been called a Luddite.
[00:02:43.540 --> 00:02:46.380]   I've been told I have my head in the sand.
[00:02:46.380 --> 00:02:49.560]   I've been told I'm missing the big picture.
[00:02:49.780 --> 00:03:00.640]   In the case where I posted that challenge, actually, the full quote was, I was completely missing the big picture so much that it physically hurts to read my post.
[00:03:00.640 --> 00:03:05.440]   And, you know, if my posts have hurt you, I'm sorry.
[00:03:05.440 --> 00:03:12.380]   Honestly, it doesn't feel good to be talked to in any of these ways, right?
[00:03:12.440 --> 00:03:16.180]   So you may have had to feel that way at some point as well.
[00:03:16.180 --> 00:03:17.320]   Or maybe you've been told this.
[00:03:17.320 --> 00:03:20.240]   You've been told that you'll be left behind.
[00:03:20.240 --> 00:03:24.060]   I struggle with this one, honestly.
[00:03:24.060 --> 00:03:44.500]   I feel that left behind is verbiage better suited for, you know, some post-apocalyptic religious prophecy in the form of a B-movie franchise starring Kirk Cameron than in some sort of nuanced technical discussion.
[00:03:45.800 --> 00:03:56.060]   I heard this one recently from someone who was saying it with a straight face and is someone whose work I respect, you know, goes back a long time.
[00:03:56.060 --> 00:03:58.800]   They said resistance is futile.
[00:03:58.800 --> 00:04:04.620]   Again, I cannot comprehend what would make someone want to say things like this.
[00:04:04.620 --> 00:04:09.840]   For all the money in the bank, do you recall what that quote is from?
[00:04:09.840 --> 00:04:12.700]   Was it from the hero of that story?
[00:04:12.700 --> 00:04:14.060]   No.
[00:04:14.680 --> 00:04:23.920]   It's a quote from the Borg, from Star Trek, one of the most notorious villains in the entire science fiction genre.
[00:04:23.920 --> 00:04:31.480]   If we find ourselves quoting the Borg in earnest, maybe we should reassess what side we're on.
[00:04:31.480 --> 00:04:40.160]   When Picard was captured and being mind-controlled by the AI, he said resistance is futile as Locutus.
[00:04:40.160 --> 00:04:43.620]   But the real Picard would never say resistance is futile.
[00:04:43.620 --> 00:04:47.660]   Now, Picard would say if you're on the side of truth, you should resist to the last breath.
[00:04:47.660 --> 00:04:49.180]   He embodied that again and again.
[00:04:49.180 --> 00:04:52.860]   But we need to breathe.
[00:04:54.160 --> 00:05:02.500]   I'm responding to emotionally charged rhetoric and now I've started spewing out my own emotionally charged rhetoric in response.
[00:05:02.500 --> 00:05:05.760]   This is not helping me listen to these people.
[00:05:05.760 --> 00:05:07.160]   They mean well.
[00:05:07.700 --> 00:05:09.240]   You know, I've chosen to engage.
[00:05:09.240 --> 00:05:16.980]   I've made the decision to engage in these conversations and I need to be able to try to do that productively.
[00:05:16.980 --> 00:05:19.120]   I need to be able to hear people out.
[00:05:19.120 --> 00:05:23.200]   I think we need to back up and decide what are we even talking about.
[00:05:23.980 --> 00:05:25.460]   It is something very important.
[00:05:25.460 --> 00:05:32.340]   It makes sense that a lot of us feel strongly about it, which then leads to us being in conflict with each other.
[00:05:32.640 --> 00:05:36.320]   Because we're pondering, what is the future of software?
[00:05:36.320 --> 00:05:38.960]   What is it going to look like next?
[00:05:38.960 --> 00:05:40.140]   What can we make it?
[00:05:40.240 --> 00:05:41.780]   I mean, that is deeply interesting.
[00:05:41.780 --> 00:05:58.100]   If there's one thing I am grateful to AI for, it's probably, even more than the technology, the opportunity for us to have all be thrown into this one conversation about the future of the craft.
[00:05:58.100 --> 00:06:02.680]   It's a very difficult conversation to be in of late.
[00:06:02.680 --> 00:06:06.240]   And I'm going to try to make it a little easier with this.
[00:06:06.240 --> 00:06:08.840]   But I think it is an important one worth having.
[00:06:09.640 --> 00:06:16.240]   I've identified six scenarios that seem to be embedded in the views people have been putting out over the last few years.
[00:06:16.240 --> 00:06:20.000]   And I'd like to share them with you for the remainder of this talk.
[00:06:20.000 --> 00:06:21.680]   Let's get started.
[00:06:21.680 --> 00:06:34.740]   These are Extreme Completion, the Devocalypse, the Abstraction Leap, Uncharted Waters, the Review Economy, and the Infinite Pile of Garbage.
[00:06:34.740 --> 00:06:37.560]   We will discuss each of these briefly.
[00:06:38.560 --> 00:06:46.880]   Now, Extreme Completion is probably the most conservative of these views because it is already happening.
[00:06:46.880 --> 00:06:48.540]   We can just see it happen.
[00:06:48.540 --> 00:06:51.440]   There's no real doubt about whether this one will happen.
[00:06:51.440 --> 00:06:55.300]   It's just a matter of, you know, how much with what impact.
[00:06:55.300 --> 00:07:13.180]   And that is simply the autocomplete style editors like Cursor, like GitHub Copilot are just going to continue to help us type more of the typing and be a great convenience, right?
[00:07:13.840 --> 00:07:19.440]   So in that scenario, in Extreme Completion, our job doesn't fundamentally change.
[00:07:19.440 --> 00:07:31.420]   Even as they progress to these agents that can take a few more steps, you can still have, you know, fairly extreme completion with it still needing to be on an engineer's lease most of the time.
[00:07:31.420 --> 00:07:35.600]   Such that you could argue our role is not changing like a great deal.
[00:07:35.600 --> 00:07:41.300]   So pretty much everyone agrees this at least is happening and is a somewhat significant shift, right?
[00:07:41.700 --> 00:07:43.180]   This is what this looks like.
[00:07:43.180 --> 00:07:45.960]   If you're at this conference, you've surely seen it.
[00:07:45.960 --> 00:07:56.040]   This is an example of using the cursor IDE, which is a fork of VS Code with a lot of AI features built in.
[00:07:56.300 --> 00:08:00.520]   So there's a function called clean prime to remove trailing white space.
[00:08:00.520 --> 00:08:10.460]   And I've prompted it here in this little pop-up thing that happens when I hit control K, make the function clean prime prime to remove both leading and trailing white space.
[00:08:10.460 --> 00:08:15.740]   And sort of based on that example, it's going to make me another Haskell function pop out.
[00:08:15.740 --> 00:08:16.300]   There it is.
[00:08:16.300 --> 00:08:17.180]   This is pretty cool.
[00:08:17.180 --> 00:08:26.080]   By the way, the reason I happen to pick the programming language Haskell for this example is that it has a very strong typing system.
[00:08:26.080 --> 00:08:35.400]   I think that type theory is a very promising counterbalance to add more certainty into the flow when we have the uncertainty of LLMs doing code gem.
[00:08:35.400 --> 00:08:46.840]   Regardless, here's another example that is maybe a little more where we think things are going, where I have delegated an entire task via Slack.
[00:08:46.840 --> 00:08:48.380]   So I'm just in my work chat here.
[00:08:48.380 --> 00:08:52.320]   And this is a new experimental way of interacting with the open hands agent.
[00:08:52.500 --> 00:09:01.760]   I've said, hey, there's this pull request here where I've added this thing to a log statement and probably some other things ought to have that too.
[00:09:01.760 --> 00:09:08.560]   Could you just poke around and make a PR that's adding that where it ought to go, right?
[00:09:08.660 --> 00:09:14.280]   And a few minutes later, boom, comes back this 48 file pull request.
[00:09:14.280 --> 00:09:16.300]   Like, this is really cool.
[00:09:16.300 --> 00:09:18.900]   You showed this to me like a couple of years ago.
[00:09:18.900 --> 00:09:21.240]   I would have thought this was unreal that we have this.
[00:09:21.240 --> 00:09:24.980]   Nonetheless, it is a fairly discreet, you know, task.
[00:09:24.980 --> 00:09:28.140]   Not a lot of quote unquote, you know, thinking needed to be done here.
[00:09:28.160 --> 00:09:32.240]   But there's a lot of grunt work that's been taken up by being able to do stuff like this.
[00:09:32.240 --> 00:09:35.720]   This sort of, you know, all purpose tech debt dirt shoveler.
[00:09:35.720 --> 00:09:36.600]   Really neat.
[00:09:36.600 --> 00:09:38.720]   Still, I file it under extreme completion.
[00:09:38.720 --> 00:09:41.920]   It is on a very short leash.
[00:09:41.920 --> 00:09:43.480]   A lot of expertise still involved.
[00:09:43.480 --> 00:09:49.820]   Much more extreme is the scenario of the devocalypse, the developer apocalypse.
[00:09:49.880 --> 00:09:54.440]   At least that's what it is to us as software developers.
[00:09:54.440 --> 00:10:03.320]   If you're not a software developer, if you are dependent on software developers, actually, they don't see this as an apocalypse at all, right?
[00:10:03.320 --> 00:10:11.280]   They see this as the innovators paradise scenario because they're no longer dependent on us to bring their ideas to fruition.
[00:10:11.280 --> 00:10:13.560]   It sounds very nice from their point of view.
[00:10:13.560 --> 00:10:25.560]   And I'll say this, even though we benefit from this not happening at the moment, if it actually is possible, if we can deliver it, I think it is desirable.
[00:10:25.560 --> 00:10:26.880]   Like, we should do that.
[00:10:26.880 --> 00:10:31.100]   I think really that would be programming fulfilling its destiny.
[00:10:31.100 --> 00:10:35.560]   It would be finishing the project of computer science.
[00:10:35.560 --> 00:10:39.520]   You know, the objection to it wouldn't be this isn't desirable.
[00:10:39.520 --> 00:10:44.480]   I think the objection would be it's not feasible to do.
[00:10:44.480 --> 00:10:54.100]   But, of course, because of these different incentives, someone who sees us objecting to it can always say, hey, you just want to keep your job.
[00:10:54.100 --> 00:10:57.700]   And, like, I do want my friends to keep our jobs, right?
[00:10:57.700 --> 00:10:58.860]   Of course, I want that.
[00:10:58.860 --> 00:11:07.520]   But I just don't think that's why I'm saying the things that I'm saying about how some of these solutions look unsustainable.
[00:11:07.520 --> 00:11:12.400]   You know, I think we have real expertise that these critiques come from.
[00:11:12.400 --> 00:11:19.120]   So if you're going to predict the devocalypse, you still must say how we are going to get there, right?
[00:11:19.120 --> 00:11:22.500]   And abstraction leap is one of those ideas.
[00:11:22.500 --> 00:11:24.840]   It has a few flavors I'll talk about.
[00:11:24.840 --> 00:11:40.080]   But basically, if you're a believer in abstraction leap, you think that what we currently think of as code will no longer be the kind of level of abstraction, the substrate in which we do our main work.
[00:11:40.580 --> 00:11:55.700]   Doing something like Java code or Rust code today, you know, that will eventually be in the position of assembly language or JVM bytecode or LLVM bitcode or something to that effect.
[00:11:56.220 --> 00:11:59.220]   Only highly specialized people would need to do that.
[00:11:59.220 --> 00:12:07.240]   Most of us can just live up here doing something more pleasant and productive, whatever that may be.
[00:12:07.240 --> 00:12:12.940]   But internally, it's ultimately code-like as we currently understand it, right?
[00:12:13.000 --> 00:12:14.540]   So how does that happen?
[00:12:14.540 --> 00:12:21.600]   One way people say this will happen is with the prompts as code flavor of the scenario, as I call it.
[00:12:21.600 --> 00:12:29.360]   So natural language instead of source code becomes the main human-facing artifact that we manipulate.
[00:12:29.360 --> 00:12:31.860]   So how does that scale?
[00:12:31.860 --> 00:12:35.640]   Well, maybe it gets some kind of structure, right?
[00:12:35.640 --> 00:12:40.560]   Maybe there are a bunch of little requirements inside folders that are interconnected.
[00:12:40.560 --> 00:12:41.680]   I don't know.
[00:12:41.680 --> 00:12:43.580]   It is structured somehow.
[00:12:43.580 --> 00:12:45.400]   It is tested somehow.
[00:12:45.400 --> 00:12:51.840]   Now, the objection to this, other than maybe, you know, vagueness, would be predictability.
[00:12:51.840 --> 00:13:06.360]   There is serious reason to doubt that LLM prompts constitute an abstraction, at least as far as something that you're able to really build on because of how unpredictable it is.
[00:13:06.360 --> 00:13:14.220]   Perhaps in order to build really large, long-lived projects, this just is not a sturdy enough foundation.
[00:13:14.460 --> 00:13:15.500]   It's not a clean abstraction.
[00:13:15.500 --> 00:13:18.760]   You could say that it will become one.
[00:13:18.760 --> 00:13:21.360]   We have to see that happen.
[00:13:21.360 --> 00:13:28.580]   Or, you know, some people believe that it's possible to make up for that unpredictability with some sort of control.
[00:13:28.580 --> 00:13:32.080]   Here's an attempt to do this, right?
[00:13:32.080 --> 00:13:36.320]   This is actually from a few years ago now, the parcel paper.
[00:13:36.320 --> 00:13:40.900]   And you can go to this GitHub repo, if you like, or you can read the paper.
[00:13:41.560 --> 00:13:52.040]   And this is an example, for instance, where they've got 61 lines of just these text prompts and example input and output of all these different functions.
[00:13:52.040 --> 00:14:01.640]   And from these prompts and examples, it generates a 220-line Python program that functions as a Lisp interpreter, right?
[00:14:01.640 --> 00:14:05.480]   So they've, using these structured prompts, generated an entire Lisp interpreter.
[00:14:05.480 --> 00:14:07.520]   Like, that's pretty neat, right?
[00:14:08.300 --> 00:14:19.460]   And yet, you know, we largely don't believe structured prompts as code as a way to build real applications is a solved problem still.
[00:14:20.300 --> 00:14:28.120]   So, you know, when you try to operationalize this, are we going to be able to make this into a real product that is better than its alternatives?
[00:14:28.120 --> 00:14:29.800]   I think very much still up in the air.
[00:14:29.800 --> 00:14:37.600]   Another flavor of abstraction leap, I think personally is a little more promising, is domain-specific languages or DSLs.
[00:14:37.740 --> 00:14:46.780]   Now, if you believe you're unfamiliar with DSLs, actually, you probably are familiar with them, because in programming, there are many of them that occur, right?
[00:14:46.780 --> 00:14:56.640]   So, CSS, right, that you use to style web pages, art example, or SQL queries, or regular expressions, right?
[00:14:56.640 --> 00:15:00.540]   And these are a domain-specific language for particular programming tasks.
[00:15:00.540 --> 00:15:10.440]   And then you also have ones that are made for particular business domains, even things like Excel, you know, arguably are a domain-specific language.
[00:15:10.440 --> 00:15:14.120]   So, these are very prolific, very successful oftentimes.
[00:15:15.120 --> 00:15:33.500]   And what you do if you're operationalizing this, you know, in an enterprise scenario, is you ultimately are investing in creating a particular specialized programming environment that, you know, optimizes for the kinds of thoughts you usually need to express in your business domain, right?
[00:15:33.500 --> 00:15:44.820]   And that is this upfront investment that can yield great reliability, great quality, great productivity, you know, when it works well.
[00:15:44.820 --> 00:15:47.060]   And it can also backfire, of course.
[00:15:47.060 --> 00:16:02.360]   And over the last 20 years, the kind of risk-to-reward trade-off has been steadily improving with lots of tooling that's made it easier to create these specialized environments, such as language workbenches, right?
[00:16:02.420 --> 00:16:06.440]   So, examples of that are JetBrains MPS, right?
[00:16:06.440 --> 00:16:10.220]   One called LionWeb, one called Xtext.
[00:16:10.220 --> 00:16:13.080]   There's one, you could call it a language workbench.
[00:16:13.080 --> 00:16:18.600]   They would say it is a language-oriented programming environment called Racket, which is a very interesting Lisp dialect.
[00:16:19.980 --> 00:16:32.360]   And there is every reason to believe that language models in various ways would even further improve the cost, you know, risk-reward trade-off of creating these specialized environments.
[00:16:32.360 --> 00:16:38.520]   Can they help us generate, you know, some of the code to process these DSLs?
[00:16:38.520 --> 00:16:50.180]   Or can they help on the side of the editor giving the, you know, business domain users suggestions to allow them to more quickly adapt to these languages?
[00:16:50.360 --> 00:16:54.540]   So, I think this is a very promising area.
[00:16:54.540 --> 00:16:59.700]   I've actually declared it the year of DSLs on my YouTube channel, Craft vs. Cruft.
[00:16:59.700 --> 00:17:02.420]   On the right, you can see my announcement of that.
[00:17:02.420 --> 00:17:06.260]   It's a recent video if you want to dive a little deeper or better yet.
[00:17:06.260 --> 00:17:14.160]   You could watch this talk called Empowerment of Subject Matter Experts by DSL aficionado Marcus Volter.
[00:17:14.160 --> 00:17:17.040]   But again, an abstraction leap scenario.
[00:17:17.580 --> 00:17:22.080]   Ultimately, what's underneath that abstraction is still fundamentally code-like.
[00:17:22.080 --> 00:17:25.200]   It is still basically following the rules as we understand them today.
[00:17:25.200 --> 00:17:28.440]   Next, we have uncharted waters.
[00:17:28.440 --> 00:17:35.780]   Many people argue that the future, the foundation, will not even be code-like.
[00:17:35.780 --> 00:17:39.260]   It will be like nothing we have seen before.
[00:17:39.260 --> 00:17:42.240]   So, what does that mean?
[00:17:43.240 --> 00:17:47.740]   Some say that it will be direct model inference.
[00:17:47.740 --> 00:17:55.940]   So, you will be not just using LLMs in code or using LLMs to code, but LLMs will be like the processors themselves.
[00:17:55.940 --> 00:17:59.860]   That model inference will simply be the new computation.
[00:17:59.860 --> 00:18:09.060]   Or maybe the AI becomes super intelligent and it invents a new programming paradigm we can't even conceive of.
[00:18:09.060 --> 00:18:23.360]   I think uncharted waters are certainly possible, but in order to plan for them, in order to take these possibilities seriously, we need to chart them.
[00:18:23.360 --> 00:18:26.820]   We need to really see what works and what really scales.
[00:18:27.660 --> 00:18:33.540]   And I don't think we've seen anything like this really prove out yet.
[00:18:33.540 --> 00:18:42.440]   Getting back to the mundane, we have the next-to-last scenario, the review economy.
[00:18:42.440 --> 00:18:49.340]   So, in this one, this is created, for instance, by the extreme completion.
[00:18:49.340 --> 00:18:57.620]   And we have very cheap to create all these pull requests, like the one I generated, you know, in that previous slide.
[00:18:58.280 --> 00:19:06.220]   And we're still stuck checking their output, because they're not good enough to just completely approve.
[00:19:06.220 --> 00:19:13.620]   So, ultimately, we are just reading pull requests from these AIs slinging them at us, you know.
[00:19:13.620 --> 00:19:22.420]   Many people find this to be kind of a dismal scenario, like the least fun part of the job has just become our whole job.
[00:19:22.420 --> 00:19:23.280]   Kind of depressing.
[00:19:24.020 --> 00:19:28.140]   I don't see this so much as an endgame, but maybe it will be a pit stop.
[00:19:28.140 --> 00:19:32.400]   There are certainly companies that have already seen, you know, this is some kind of reality.
[00:19:32.400 --> 00:19:34.920]   But I think it's a pit stop along to something better.
[00:19:34.920 --> 00:19:38.560]   I think it is a sign that you're not managing your bottlenecks well.
[00:19:38.560 --> 00:19:47.700]   Pro tip, there's a body of knowledge called Theory of Constraints, introduced by Ellie Goldrath, starting in the novel The Goal.
[00:19:47.700 --> 00:19:50.500]   But there's a lot of work in its sense.
[00:19:50.860 --> 00:19:59.760]   But the thinking processes in Theory of Constraints are really helpful whenever you're in some situation that it looks like this, right?
[00:19:59.760 --> 00:20:06.460]   Where everything's held back by this one choke point, like in this case, manual developer review.
[00:20:06.460 --> 00:20:12.380]   Examples of what, you know, improving that can look like are shifting further left.
[00:20:12.380 --> 00:20:23.240]   Like I mentioned with type theory, like any number of things, how can we reduce the error rate such that we're less bound by needing to review these?
[00:20:23.380 --> 00:20:25.980]   Or maybe it's just a matter of prioritization.
[00:20:25.980 --> 00:20:32.080]   Maybe we need to pick and choose which of these things we're even going to try to review.
[00:20:32.280 --> 00:20:40.840]   And, you know, by shipping a third of those items, we get 90% of the value because we're doing a good job of picking the right ones.
[00:20:40.840 --> 00:20:43.840]   The ones that really are going to give us something.
[00:20:43.840 --> 00:20:49.560]   Last scenario, and the most dismal, is the infinite pile of garbage.
[00:20:50.240 --> 00:21:03.760]   In this scenario, coding assistance made us feel more productive, but ultimately just exploded tech debt and dug us into a hole that even the AI could not dig us out of.
[00:21:04.220 --> 00:21:09.760]   The quality of our products gets worse over time instead of better.
[00:21:09.760 --> 00:21:12.600]   Ultimately, it's a world to hurt.
[00:21:12.600 --> 00:21:19.700]   We hope that this doesn't happen, of course, and there's reason to think it may already be starting to happen.
[00:21:20.060 --> 00:21:42.300]   This is, for example, a group called Uplevel, which did some investigation in which they did this controlled trial and they found that developers who had access to a coding assistant were putting out a significantly higher bug rate and not even having better throughput on their issues.
[00:21:42.300 --> 00:21:43.580]   That's pretty dismal.
[00:21:43.580 --> 00:22:03.920]   GetClear also has a white paper, and they have a number of interesting things in that, one of which is that this was the first year, 2024, this past year, was the first year they've seen where the percentage of code that was copy-pasted exceeded that which was moved in a refactoring.
[00:22:03.920 --> 00:22:07.220]   If you know your way around code quality, that's a big red flag.
[00:22:07.220 --> 00:22:10.340]   Lots of copy and paste creates a lot of risk.
[00:22:10.460 --> 00:22:21.620]   Some people could argue that with these AI tools, the practices, our intuition for what is a good thing or not may need to change, but that needs to be borne out.
[00:22:21.620 --> 00:22:22.820]   You need to prove that.
[00:22:22.820 --> 00:22:27.380]   Unsurprisingly, GitHub actually had the opposite finding.
[00:22:27.380 --> 00:22:33.700]   They did not find that their co-pilot tool that they sell decreases quality.
[00:22:33.700 --> 00:22:35.880]   They found that it increases it.
[00:22:35.880 --> 00:22:40.360]   Now, they were doing a controlled experiment involving a fixed task.
[00:22:40.360 --> 00:22:49.160]   It was not like these other examples, you know, an example on real work code, but there's any number of factors that could impact this.
[00:22:49.160 --> 00:22:54.140]   So they have a blog, does GitHub Copile improve code quality?
[00:22:54.140 --> 00:22:56.240]   Admittedly, much better graphic design.
[00:22:56.240 --> 00:22:57.280]   They say, yes, it does.
[00:22:57.280 --> 00:22:58.580]   So what do we make of this?
[00:22:58.640 --> 00:23:07.600]   I mean, these results are obviously, you know, contradictory and ambiguous, but, you know, even more importantly, they're all from sources that have something to sell.
[00:23:07.600 --> 00:23:13.620]   You know, I have seen some more rigorous academic work start to come together.
[00:23:13.620 --> 00:23:31.120]   I'd love next year to be talking about not some white papers and a blog, but, you know, maybe a meta analysis of multiple different, you know, independent academic studies of what results we have in the wild with AI coding assistance.
[00:23:31.360 --> 00:23:44.320]   Will we be able to do that, I mean, I hope so, but one way or another, you know, do these make things better under certain circumstances and worse under other circumstances?
[00:23:44.320 --> 00:23:45.620]   Does it matter how we use them?
[00:23:45.620 --> 00:23:47.760]   Does it matter what we use them on?
[00:23:47.760 --> 00:23:53.840]   These are all things we need to understand so that we know we're making the situation better and not worse.
[00:23:55.180 --> 00:24:02.360]   People will say the models will get better, but the products we use them to build will get better only if we make them better.
[00:24:02.360 --> 00:24:03.340]   We need to be deliberate.
[00:24:03.340 --> 00:24:06.100]   Again, these are the scenarios.
[00:24:06.100 --> 00:24:15.120]   Extreme completion, devocalypse, abstraction leap, uncharted waters, review economy, and infinite pile of garbage.
[00:24:15.120 --> 00:24:19.100]   Now, these interrelate in various ways, right?
[00:24:19.480 --> 00:24:39.580]   So, for example, by doing extreme completion, many fears, I just mentioned, that we will try to get to devocalypse, but actually we will overinvest in them before we're capable of dealing with their results, and we'll slide right past devocalypse and into the infinite pile of garbage.
[00:24:39.720 --> 00:24:52.020]   Some think, this is maybe the closest to my point of view, that a really promising area would be to combine abstraction leap with maybe domain-specific languages, combine that with the extreme completion.
[00:24:52.020 --> 00:25:06.160]   We have a very nice DSL or, say, a formal methods-based, you know, specification system, and then we combine that with the extreme completion to, you know, help us navigate it ergonomically.
[00:25:06.320 --> 00:25:17.780]   So, this could work very well, so there are these multiple different endgames, and if someone believes in a different one as you, you might both be right, because many of them are going to play out.
[00:25:17.780 --> 00:25:19.100]   The industries are vast.
[00:25:19.100 --> 00:25:23.900]   This is going to impact different areas very differently.
[00:25:23.900 --> 00:25:32.840]   We wouldn't expect the AI coding impact on, like, video game programming to be the same as in healthcare tech, right?
[00:25:34.320 --> 00:25:37.520]   And lastly, I want to leave us with this.
[00:25:37.520 --> 00:25:39.220]   We get a say.
[00:25:39.220 --> 00:25:53.180]   People have spoken about AI coding as though it is some meteor from outside the solar system just coming at us to hit us, and we're these passive observers.
[00:25:53.180 --> 00:25:55.000]   We get a say on what happens.
[00:25:55.000 --> 00:25:58.600]   This is something that we are actively building together.
[00:25:58.980 --> 00:26:02.760]   So, I think we need to ask ourselves, what do we want from software?
[00:26:02.760 --> 00:26:05.160]   What is the goal?
[00:26:05.160 --> 00:26:09.080]   Do we want there to be no programmers?
[00:26:09.080 --> 00:26:12.760]   Or do we want everyone to be a programmer?
[00:26:12.760 --> 00:26:15.080]   I don't think those are the same thing.
[00:26:15.080 --> 00:26:17.240]   Is it somewhere in between?
[00:26:17.240 --> 00:26:17.920]   Where?
[00:26:18.920 --> 00:26:23.400]   Do we want software to be better of higher quality?
[00:26:23.400 --> 00:26:28.500]   Or do we just need more software and we don't care about the quality?
[00:26:28.500 --> 00:26:30.860]   Again, not the same thing.
[00:26:30.860 --> 00:26:36.760]   As the skills required to do our jobs change, what do we want to happen?
[00:26:36.760 --> 00:26:45.200]   Do we think it's a good idea to just let people go who have diligently learned what yesterday was the thing we needed?
[00:26:45.200 --> 00:26:52.640]   Or are we going to figure out how to continue to develop a valuable relationship with those people?
[00:26:53.320 --> 00:26:54.860]   I want many things from software.
[00:26:54.860 --> 00:26:56.340]   I want it to work well.
[00:26:56.340 --> 00:27:02.780]   I want it to provide value for the people who use it and for the people who build it.
[00:27:02.780 --> 00:27:05.780]   And I want people who work together to treat each other well.
[00:27:05.780 --> 00:27:10.580]   Well, thank you very much for having me, and I look forward to your questions.

