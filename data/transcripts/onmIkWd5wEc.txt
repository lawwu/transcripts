
[00:00:00.000 --> 00:00:04.760]   Okay. So now we're trying this a second time.
[00:00:04.760 --> 00:00:07.160]   Very bold.
[00:00:07.160 --> 00:00:13.000]   And we are in fact now live.
[00:00:13.000 --> 00:00:14.520]   Beautiful. Okay.
[00:00:14.520 --> 00:00:20.440]   So welcome everyone to the Weights and Biases Salon.
[00:00:20.440 --> 00:00:24.720]   I have here today two guests who are
[00:00:24.720 --> 00:00:33.000]   the co-hosts of UCLA ACM AI Student Group's podcast, You Belong in AI.
[00:00:33.000 --> 00:00:38.000]   And so we're going to hear from Ava and from Matt a little bit about themselves and
[00:00:38.000 --> 00:00:44.680]   about their work, their careers, such as they are so far as undergraduates.
[00:00:44.680 --> 00:00:47.440]   And then we'll talk a bit more about the podcast,
[00:00:47.440 --> 00:00:50.560]   which its sort of core topic is about.
[00:00:50.560 --> 00:00:52.360]   It's an interview podcast,
[00:00:52.360 --> 00:00:54.560]   not unlike what we're doing right now.
[00:00:54.560 --> 00:01:00.520]   With inspiring figures in AI, not unlike what we're doing right now.
[00:01:00.520 --> 00:01:05.600]   And with a particular focus on representation and inclusion.
[00:01:05.600 --> 00:01:08.760]   And so there's some really great discussions.
[00:01:08.760 --> 00:01:11.200]   I really have enjoyed listening to the podcast.
[00:01:11.200 --> 00:01:15.160]   And I'm really excited to hear from Matt and Ava here today.
[00:01:15.160 --> 00:01:20.080]   I'm super excited.
[00:01:20.080 --> 00:01:21.880]   Let's get into it.
[00:01:21.880 --> 00:01:26.600]   I don't know if we can call ourselves influential figures in AI, but
[00:01:26.600 --> 00:01:28.040]   hopefully one day.
[00:01:28.040 --> 00:01:35.040]   >> It said inspiring, and that's maybe an easier goal than influential.
[00:01:35.040 --> 00:01:36.480]   Influential is a tough one.
[00:01:36.480 --> 00:01:47.160]   So yeah, I guess just to start off, maybe tell some of our viewers here
[00:01:47.160 --> 00:01:52.920]   who might not have seen the podcast or know about y'all UCLA ACMAI.
[00:01:52.920 --> 00:01:57.200]   Maybe I guess starting with Matt,
[00:01:57.200 --> 00:02:01.680]   how'd you become interested in programming and in AI?
[00:02:01.680 --> 00:02:03.520]   How'd you sort of get into this field?
[00:02:03.520 --> 00:02:08.160]   >> Yeah, so I think it started in high school.
[00:02:08.160 --> 00:02:10.040]   I don't remember exact moment in time.
[00:02:10.040 --> 00:02:12.200]   There's no epiphany moment that happened.
[00:02:12.200 --> 00:02:16.320]   But I remember early high school, I tried programming once just on my own.
[00:02:16.320 --> 00:02:17.120]   It wasn't for me.
[00:02:17.120 --> 00:02:21.400]   So I decided just to take a class like my senior year.
[00:02:21.400 --> 00:02:24.440]   And when I took the class, I kind of fell in love with it.
[00:02:24.440 --> 00:02:27.320]   I was always a math person, just very analytical things.
[00:02:27.320 --> 00:02:30.840]   And that just kind of seemed to fit my style when it came to programming.
[00:02:30.840 --> 00:02:33.080]   And when it came to computer science,
[00:02:33.080 --> 00:02:36.120]   there's different fields I was interested in.
[00:02:36.120 --> 00:02:40.720]   I was interested in cybersecurity, but then I was also interested in AI.
[00:02:40.720 --> 00:02:45.760]   And I don't know what about it was it for me with artificial intelligence, but
[00:02:45.760 --> 00:02:50.240]   just seeing or just knowing that I could program something that
[00:02:50.240 --> 00:02:55.360]   had inherent intelligence, I don't know, it just attracted to me.
[00:02:55.360 --> 00:02:57.920]   I don't know, there's just something about it that attracted to me.
[00:02:57.920 --> 00:03:00.280]   I can't even describe it right now, but
[00:03:00.280 --> 00:03:03.480]   I think that's senior year of high school, that's when it hit for me.
[00:03:03.480 --> 00:03:10.080]   >> Yeah, it's interesting that you mentioned that sort of AI was really what
[00:03:10.080 --> 00:03:12.760]   convinced you to try it out.
[00:03:12.760 --> 00:03:14.400]   Ava, did you have a similar experience?
[00:03:15.400 --> 00:03:20.360]   >> A little bit, so I had my first CS experience when I was a freshman in
[00:03:20.360 --> 00:03:24.840]   high school, and my parents are both engineers.
[00:03:24.840 --> 00:03:29.760]   My mom's a software engineer, and so is my uncle.
[00:03:29.760 --> 00:03:31.760]   So I kind of took after them.
[00:03:31.760 --> 00:03:39.000]   And I didn't really like it, but I didn't think I wanted to major in it in college.
[00:03:39.000 --> 00:03:43.520]   I wanted to go down more of a route of electrical engineering, like my dad.
[00:03:44.480 --> 00:03:47.720]   So that's kind of where I got into electrical engineering,
[00:03:47.720 --> 00:03:50.600]   was just doing some other things in high school with it.
[00:03:50.600 --> 00:03:52.800]   But I knew I still liked computer science all along, and
[00:03:52.800 --> 00:03:56.000]   I wanted to get involved in a computer science field.
[00:03:56.000 --> 00:04:00.640]   Also, I've been pretty much told all my life, artificial intelligence is the future,
[00:04:00.640 --> 00:04:04.120]   and you should really get involved in it, because it's going to be everything.
[00:04:04.120 --> 00:04:06.800]   And it kind of is everything already.
[00:04:06.800 --> 00:04:09.760]   But yeah, I knew I wanted to get involved in AI.
[00:04:09.760 --> 00:04:13.600]   I knew I wanted to get involved in CS,
[00:04:13.600 --> 00:04:16.800]   even though I am an electrical engineering major.
[00:04:16.800 --> 00:04:20.040]   And then, yeah, so that's kind of how I got interested in it.
[00:04:20.040 --> 00:04:25.440]   >> Nice, yeah, it's interesting, I guess,
[00:04:25.440 --> 00:04:29.200]   I actually didn't start programming at all until I was, or well,
[00:04:29.200 --> 00:04:35.400]   I did a little bit of MATLAB and R for data analysis when I was in college.
[00:04:35.400 --> 00:04:39.720]   But I didn't really fully start programming until I was already in
[00:04:39.720 --> 00:04:41.200]   graduate school.
[00:04:41.200 --> 00:04:45.400]   And I had been told that artificial intelligence was this big waste of time
[00:04:45.400 --> 00:04:48.400]   that people had kind of tried and failed at.
[00:04:48.400 --> 00:04:51.480]   I remember making fun of one of my friends in college,
[00:04:51.480 --> 00:04:53.480]   because he was working on AI stuff.
[00:04:53.480 --> 00:04:55.840]   And I was like, don't you know that's never going to work?
[00:04:55.840 --> 00:05:02.600]   [LAUGH] And yeah, so it seems like we've had quite different paths.
[00:05:03.600 --> 00:05:08.400]   >> A question actually came in from YouTube from Florian Hofstetter,
[00:05:08.400 --> 00:05:12.160]   which actually is a question I wanted to ask soon,
[00:05:12.160 --> 00:05:16.720]   which is sort of what parts of AI are you most interested in?
[00:05:16.720 --> 00:05:20.440]   I really like computer vision, personally, it's my favorite.
[00:05:20.440 --> 00:05:24.800]   It's like maybe best understood, which I like as a math person.
[00:05:24.800 --> 00:05:25.560]   But what about y'all?
[00:05:25.560 --> 00:05:29.320]   >> Yeah, I can take this on first.
[00:05:29.320 --> 00:05:33.080]   So actually, all this past year in quarantine,
[00:05:33.080 --> 00:05:35.240]   I was trying to figure out this question on my own.
[00:05:35.240 --> 00:05:39.200]   because I knew I was interested in AI, but there's so many subfields and
[00:05:39.200 --> 00:05:41.760]   I didn't know which one I was interested in.
[00:05:41.760 --> 00:05:45.760]   So I kind of took these past nine months just dabbling in everything.
[00:05:45.760 --> 00:05:51.680]   And nothing really hit for me until I took a graphics class last quarter.
[00:05:51.680 --> 00:05:55.840]   And I really loved a professor and he does research and
[00:05:55.840 --> 00:05:59.440]   particularly artificial life and computer graphics.
[00:05:59.440 --> 00:06:02.120]   And I'm actually a really visual person.
[00:06:02.120 --> 00:06:04.320]   I learned a lot by visualizing.
[00:06:04.320 --> 00:06:09.960]   And for me, just the part of graphics, just applying AI to graphics,
[00:06:09.960 --> 00:06:13.720]   there's something I can visually see, I could visually see the results.
[00:06:13.720 --> 00:06:18.080]   Whereas for some other machine learning subfields, it's harder to visually see.
[00:06:18.080 --> 00:06:22.160]   And just being able to create an artificial life character,
[00:06:23.280 --> 00:06:28.000]   in a Unity or Unreal Engine, and just being able to see the results,
[00:06:28.000 --> 00:06:32.360]   just really inspired me to continue pushing on into the subfield.
[00:06:32.360 --> 00:06:39.200]   >> Nice, yeah, there was actually some really, really cool work came out,
[00:06:39.200 --> 00:06:44.360]   maybe last week or earlier this week, very recently on,
[00:06:44.360 --> 00:06:48.040]   people have been doing GANs for rendering and sort of pushing the envelope with that.
[00:06:48.040 --> 00:06:52.960]   But yeah, Vlad Coltune at Intel had some really great work
[00:06:52.960 --> 00:06:58.200]   that just came out that just like really sold me on it.
[00:06:58.200 --> 00:07:01.880]   I hadn't been sold on GANs for rendering, or even really on neural rendering.
[00:07:01.880 --> 00:07:06.000]   I was a little skeptical, but it was really, really impressive results.
[00:07:06.000 --> 00:07:07.240]   So it's an exciting field for sure.
[00:07:07.240 --> 00:07:16.520]   >> So, yeah, so I think first thing that really got me interested in AI and
[00:07:16.520 --> 00:07:22.080]   kind of a reason why I joined ACMI was I took this class in my first quarter
[00:07:22.080 --> 00:07:26.960]   of my senior year at UCLA, and it's called E96C,
[00:07:26.960 --> 00:07:29.840]   Internet of Things and Embedded Machine Learning.
[00:07:29.840 --> 00:07:36.120]   And so we did a bunch of projects with, I didn't 100% understand what I was doing.
[00:07:36.120 --> 00:07:38.800]   It was like an intro course to a very advanced topic,
[00:07:38.800 --> 00:07:41.920]   which was embedded machine learning and Internet of Things.
[00:07:41.920 --> 00:07:44.320]   But I really liked it, it was pretty cool.
[00:07:44.320 --> 00:07:48.800]   So pretty much what we did the entire time was we used this sensor tile code with
[00:07:48.800 --> 00:07:52.320]   like an ARM in it and using embedded machine learning and
[00:07:52.320 --> 00:07:56.320]   Internet of Things technology to pretty much detect movements.
[00:07:56.320 --> 00:08:02.160]   And so you would train the machine and then you would do different movements and
[00:08:02.160 --> 00:08:05.440]   it would be able to detect it based off of the training that you gave it.
[00:08:05.440 --> 00:08:09.080]   So which is pretty much what they do in VR and stuff, and
[00:08:09.080 --> 00:08:10.720]   like different video games and stuff.
[00:08:10.720 --> 00:08:14.000]   And so that's honestly something that I've really been interested in.
[00:08:14.000 --> 00:08:18.640]   I haven't dived too deep in it since I did take that class.
[00:08:18.640 --> 00:08:21.360]   But yeah, that's something that I actually do really like.
[00:08:21.360 --> 00:08:24.480]   And I personally don't play video games all that much, but
[00:08:24.480 --> 00:08:26.000]   the ones that I have I do like.
[00:08:26.000 --> 00:08:30.560]   And I think that it's really cool to make really cool sensorial experiences for
[00:08:30.560 --> 00:08:31.680]   different people.
[00:08:31.680 --> 00:08:35.320]   And I'd love to get involved in that in any way possible when I get older.
[00:08:35.320 --> 00:08:38.800]   >> Nice, yeah.
[00:08:38.800 --> 00:08:42.440]   Yeah, that's cool, definitely a cool domain and
[00:08:42.440 --> 00:08:44.520]   one with a lot of room to grow and improve.
[00:08:46.520 --> 00:08:50.200]   So you've already touched on this a little bit, but I guess there's a lot of
[00:08:50.200 --> 00:08:55.040]   kind of divisions in what kind of careers you might pursue in AI.
[00:08:55.040 --> 00:08:59.160]   And you're definitely at the stage where you are early enough to make these
[00:08:59.160 --> 00:09:02.080]   decisions, but you kind of need to start making those moves.
[00:09:02.080 --> 00:09:06.680]   So are you thinking researcher engineering?
[00:09:06.680 --> 00:09:09.560]   Are you thinking being an individual contributor who writes code,
[00:09:09.560 --> 00:09:13.760]   makes stuff, versus being a manager who makes sure a team is able to deliver
[00:09:13.760 --> 00:09:17.360]   something, versus being a founder who creates a company?
[00:09:17.360 --> 00:09:21.360]   Not just the technology, but also the social structures and
[00:09:21.360 --> 00:09:22.960]   the money and all these other things.
[00:09:22.960 --> 00:09:25.080]   Where are you feeling on those fronts?
[00:09:25.080 --> 00:09:31.520]   >> I, that's a good question.
[00:09:31.520 --> 00:09:35.120]   I would say everyone wants to be a founder.
[00:09:35.120 --> 00:09:40.920]   And I mean, not everyone, but I feel like the founder job sounds pretty cool.
[00:09:40.920 --> 00:09:45.960]   It's like you get a lot of money and you get to make something from the start.
[00:09:45.960 --> 00:09:50.560]   But then I read stories about how stressed Elon Musk was when Tesla and
[00:09:50.560 --> 00:09:55.040]   SpaceX were first starting out and how he didn't sleep for days and
[00:09:55.040 --> 00:09:58.320]   became really depressed for a couple of years.
[00:09:58.320 --> 00:10:00.520]   Then that kind of stuff makes me rethink that.
[00:10:00.520 --> 00:10:04.640]   But in a perfect world, I would love to be a founder.
[00:10:04.640 --> 00:10:07.680]   But before that, I definitely think that I want to go to grad school.
[00:10:07.680 --> 00:10:11.480]   So that's kind of where my focus is right now.
[00:10:11.480 --> 00:10:13.520]   My focus is on just getting to grad school and
[00:10:13.520 --> 00:10:15.560]   then after that figuring out what I want to do from there.
[00:10:15.560 --> 00:10:21.720]   >> Cool, yeah, I've worked with the grad school, PhD founders and
[00:10:21.720 --> 00:10:24.800]   with college dropout founders.
[00:10:24.800 --> 00:10:27.640]   And people can make it work no matter what path they take,
[00:10:27.640 --> 00:10:32.880]   if they're willing to deal with the no sleep and the high responsibility.
[00:10:32.880 --> 00:10:34.820]   >> Yeah.
[00:10:36.080 --> 00:10:39.240]   I guess for myself, yeah, so for myself,
[00:10:39.240 --> 00:10:45.880]   I guess the past few months or so, this is the question I ask myself every day.
[00:10:45.880 --> 00:10:49.320]   Not every day, but pretty often, what path I want to take.
[00:10:49.320 --> 00:10:52.960]   because there's definitely a lot I could take from where I'm at as a second year.
[00:10:52.960 --> 00:10:57.400]   There's obviously the path, like machine learning engineer,
[00:10:57.400 --> 00:11:01.120]   that's attractive, work on real world problems.
[00:11:01.120 --> 00:11:04.360]   Then there's also another path, like I mentioned, about being a founder.
[00:11:04.360 --> 00:11:06.680]   And both of them really attract to me.
[00:11:06.680 --> 00:11:11.320]   I guess, like I said, I hope one day I become a founder, but
[00:11:11.320 --> 00:11:14.440]   I'm really not letting myself push it too hard.
[00:11:14.440 --> 00:11:17.520]   Because if I just push it too hard, I don't want to force anything.
[00:11:17.520 --> 00:11:22.120]   I found that the passionate projects I've been involved with,
[00:11:22.120 --> 00:11:23.080]   they've come naturally.
[00:11:23.080 --> 00:11:27.480]   And I'm just really just waiting to see what opportunity comes upon me or
[00:11:27.480 --> 00:11:28.800]   what ideas I come up with.
[00:11:28.800 --> 00:11:31.960]   But yeah, not forcing anything and just seeing what happens.
[00:11:33.160 --> 00:11:38.400]   >> Nice, yeah, I do think maybe 10 or 15 years ago,
[00:11:38.400 --> 00:11:41.360]   you might have gotten the advice that you should found now,
[00:11:41.360 --> 00:11:45.920]   like found when you're 22, drop out of college and found a company.
[00:11:45.920 --> 00:11:49.320]   But I think the advice you'll get nowadays, at least the advice you're going to get
[00:11:49.320 --> 00:11:53.960]   from me right now, is that you can found a company when you're 40 something years old,
[00:11:53.960 --> 00:11:57.200]   if you want to.
[00:11:57.200 --> 00:12:00.760]   And a lot of very successful founders are people who've built up a big
[00:12:00.760 --> 00:12:02.680]   stable of skills over time.
[00:12:02.680 --> 00:12:08.560]   But like I said, lots of people take different paths to get to that final result.
[00:12:08.560 --> 00:12:19.200]   So pivoting a little bit over towards what you're currently working on rather
[00:12:19.200 --> 00:12:21.760]   than the exciting future ahead.
[00:12:21.760 --> 00:12:25.000]   Maybe tell us a little bit more about the work,
[00:12:25.000 --> 00:12:31.880]   you're both in this outreach group, in the AI group, in the ACM club.
[00:12:31.880 --> 00:12:34.360]   So maybe break that down for
[00:12:34.360 --> 00:12:38.320]   our viewers who are not familiar with the structure of this student group.
[00:12:38.320 --> 00:12:44.320]   >> Yeah, I could take this one really quickly.
[00:12:44.320 --> 00:12:48.360]   So yeah, ACM AI, or just like ACM at UCLA,
[00:12:48.360 --> 00:12:51.840]   we're probably the biggest computer science club at our school.
[00:12:51.840 --> 00:12:57.000]   And within ACM, there are different subcommittees, AI being one of them.
[00:12:57.000 --> 00:13:01.120]   There are other subcommittees like Hack, which focuses on web design or
[00:13:01.120 --> 00:13:04.760]   web applications, other ones like cybersecurity.
[00:13:04.760 --> 00:13:08.840]   And even one of the cooler committees is called Design,
[00:13:08.840 --> 00:13:11.560]   which creates all the cool designs for all of our committees,
[00:13:11.560 --> 00:13:13.920]   like the one I'm using for my background.
[00:13:13.920 --> 00:13:16.960]   So yeah, they make great designs, so shout out to them.
[00:13:16.960 --> 00:13:18.400]   Yeah, we love them so much.
[00:13:18.400 --> 00:13:22.680]   But yeah, ACM, it's one of the bigger clubs on campus for CS.
[00:13:22.680 --> 00:13:30.760]   And our goal is to make learning CS as easy and as inclusive as possible.
[00:13:30.760 --> 00:13:33.560]   We have a lot of workshops across all of our subcommittees.
[00:13:33.560 --> 00:13:36.320]   And we try to reach out to non-CS majors too,
[00:13:36.320 --> 00:13:40.720]   because we know programming could be really useful for a lot of different
[00:13:40.720 --> 00:13:44.280]   industries, and we just want to share the love with programming.
[00:13:44.280 --> 00:13:47.120]   And yeah, that's the best way I could put it.
[00:13:47.120 --> 00:13:56.960]   >> ACM, that's a broader organization that's across multiple campuses,
[00:13:56.960 --> 00:13:59.800]   so this is a chapter of a broader group?
[00:14:00.800 --> 00:14:04.680]   >> Yeah, yeah, there's a couple of different ones.
[00:14:04.680 --> 00:14:08.440]   I think UC Riverside just started one, and so did UCSD.
[00:14:08.440 --> 00:14:10.560]   And so one thing that we've been working on lately.
[00:14:10.560 --> 00:14:15.240]   So there are the subcommittees that Matt talked about, and
[00:14:15.240 --> 00:14:20.720]   then overarching is board, which is, it's not as scary as it sounds.
[00:14:20.720 --> 00:14:22.640]   They're actually all pretty nice.
[00:14:22.640 --> 00:14:27.480]   But they are in charge of some of the more broader organizational things.
[00:14:27.480 --> 00:14:32.600]   So something that they've been interested in doing is collaborating with other UC
[00:14:32.600 --> 00:14:33.720]   chapters of ACM.
[00:14:33.720 --> 00:14:37.080]   So yeah, that's pretty fun.
[00:14:37.080 --> 00:14:38.000]   I'm looking forward to those.
[00:14:38.000 --> 00:14:44.400]   >> I see, and I guess in the Zoom era, it's a little bit easier maybe to
[00:14:44.400 --> 00:14:50.000]   collaborate across campuses, if maybe not quite as fun as the sleepover.
[00:14:50.000 --> 00:14:51.540]   >> Definitely.
[00:14:55.120 --> 00:15:00.360]   >> So the particular thing that the two of you work on with outreach that we're
[00:15:00.360 --> 00:15:06.880]   here to nominally discuss today is the You Belong in AI podcast.
[00:15:06.880 --> 00:15:11.920]   So maybe if you could tell us just a little bit about that podcast.
[00:15:11.920 --> 00:15:18.960]   >> Yeah, so the main goal of the podcast is to bring,
[00:15:18.960 --> 00:15:24.480]   it's an interview style, but the goal is it's mainly centered
[00:15:24.480 --> 00:15:28.680]   around high school students, college students.
[00:15:28.680 --> 00:15:32.360]   Anyone who's really interested in AI, you could be older as well and
[00:15:32.360 --> 00:15:33.880]   interested in artificial intelligence.
[00:15:33.880 --> 00:15:37.720]   But the whole point is just to spread the message that anyone can be in
[00:15:37.720 --> 00:15:39.520]   artificial intelligence.
[00:15:39.520 --> 00:15:43.880]   I feel like sometimes saying the words artificial intelligence can be a buzzword.
[00:15:43.880 --> 00:15:47.280]   And it also can be something that seems super, super complicated.
[00:15:47.280 --> 00:15:49.640]   And there definitely are still barriers to that, and
[00:15:49.640 --> 00:15:51.720]   we never deny that in our podcast.
[00:15:51.720 --> 00:15:56.840]   But our goal is definitely to inspire everyone who wants to be in it to just
[00:15:56.840 --> 00:15:57.800]   try and go for it.
[00:15:57.800 --> 00:16:01.680]   That's the main goal of our podcast.
[00:16:01.680 --> 00:16:06.440]   And so every speaker has a different story, and
[00:16:06.440 --> 00:16:08.880]   they all have different facets of advice.
[00:16:08.880 --> 00:16:12.880]   And they all have definitely cool experiences to talk about.
[00:16:12.880 --> 00:16:15.520]   And they all just kind of relay that onto the audience.
[00:16:15.520 --> 00:16:19.040]   And hopefully, we inspire people to get into AI,
[00:16:19.040 --> 00:16:21.800]   give people advice on how to get into AI.
[00:16:21.800 --> 00:16:25.040]   And yeah, that's pretty much the goal of the podcast.
[00:16:25.040 --> 00:16:29.520]   >> And I guess, so what are some of the reasons why you chose or
[00:16:29.520 --> 00:16:35.920]   are working with this particular podcast format as a form of outreach, right?
[00:16:35.920 --> 00:16:40.000]   There's lots of different ways that one can reach out.
[00:16:40.000 --> 00:16:44.680]   Do you think there's particular benefits for the podcast format for
[00:16:44.680 --> 00:16:50.080]   the style you're going for or the mission that you in particular have?
[00:16:50.080 --> 00:16:56.240]   >> Yeah, so one thing that we like about the podcast is that we could easily
[00:16:56.240 --> 00:17:00.280]   distribute it to whoever we want to distribute it to.
[00:17:00.280 --> 00:17:03.040]   And yeah, sure, it's an interview format.
[00:17:03.040 --> 00:17:08.120]   One of the things for this season in particular that we're trying to focus on
[00:17:08.120 --> 00:17:13.360]   is we really want to get to know who we're interviewing, the journey.
[00:17:13.360 --> 00:17:16.680]   And just like you, just the questions like you asked us earlier,
[00:17:16.680 --> 00:17:19.680]   we really want to understand the journey that they took.
[00:17:19.680 --> 00:17:22.080]   Because we believe that the best way for
[00:17:22.080 --> 00:17:26.560]   our audience to resonate with these inspiring figures is to realize that
[00:17:26.560 --> 00:17:31.240]   maybe they had the same journey or they were in the same position that I was in
[00:17:31.240 --> 00:17:32.200]   high school.
[00:17:32.200 --> 00:17:34.960]   And if they can see that, if they could relate to that,
[00:17:34.960 --> 00:17:38.680]   then maybe there's a greater chance that they could see themselves in AI.
[00:17:38.680 --> 00:17:41.160]   So just that's our main theme.
[00:17:41.160 --> 00:17:45.800]   And yeah, like I said earlier, just the podcast, it's easy to distribute and
[00:17:45.800 --> 00:17:48.440]   just reach out to as many people as possible.
[00:17:48.440 --> 00:17:53.480]   >> Yeah, the conversational format does seem to work nicely to get people to tell
[00:17:53.480 --> 00:17:54.840]   their life stories.
[00:17:54.840 --> 00:17:58.960]   Like the episode with Paco Guzman of Facebook AI,
[00:17:58.960 --> 00:18:03.080]   who is working on under-resourced languages and
[00:18:03.080 --> 00:18:05.560]   making NLP more inclusive.
[00:18:05.560 --> 00:18:09.560]   That is sort of like, well, you asked him, I think,
[00:18:09.560 --> 00:18:14.480]   about his high school experience or his college experience.
[00:18:14.480 --> 00:18:19.040]   And he was like, well, let me go back to I was a child growing up in this place in
[00:18:19.040 --> 00:18:21.320]   Mexico and blah, blah, blah, blah.
[00:18:21.320 --> 00:18:30.160]   So what are some moments in these?
[00:18:30.160 --> 00:18:33.880]   One of the nice things about the conversational format is that you can
[00:18:33.880 --> 00:18:38.640]   have these moments of sort of surprise or serendipity.
[00:18:38.640 --> 00:18:43.160]   So were there any moments that were particularly surprising to you or
[00:18:43.160 --> 00:18:48.240]   particularly serendipitous when you were running the podcast and interviewing folks?
[00:18:48.240 --> 00:18:55.960]   >> Actually, this is something that I haven't told Matt about.
[00:18:55.960 --> 00:19:01.600]   But so the second person that we interviewed, her name was Chinasa.
[00:19:01.600 --> 00:19:03.200]   Her name is Chinasa.
[00:19:03.200 --> 00:19:05.440]   And she goes to Cornell.
[00:19:05.440 --> 00:19:07.000]   She's a graduate student at Cornell.
[00:19:07.000 --> 00:19:08.640]   She's a PhD candidate.
[00:19:08.640 --> 00:19:11.440]   And I was always like, when we were doing the interview, I was like, gosh,
[00:19:11.440 --> 00:19:12.480]   she looks familiar.
[00:19:12.480 --> 00:19:18.800]   And then one day, like probably two weeks after we filmed the podcast episode,
[00:19:18.800 --> 00:19:21.000]   I remembered where I knew her from.
[00:19:21.000 --> 00:19:25.200]   In high school, I did a program, an engineering program at Cornell over
[00:19:25.200 --> 00:19:28.200]   the summer, and she was one of my counselors.
[00:19:28.200 --> 00:19:32.960]   And I remember having a long conversation with her about her being a PhD candidate.
[00:19:32.960 --> 00:19:35.760]   I was like, my God, you're a PhD candidate, that's so cool.
[00:19:35.760 --> 00:19:39.680]   And then I thought that she would be much older because she was a PhD candidate.
[00:19:39.680 --> 00:19:43.600]   But then I found out on the podcast, she just went straight from undergrad to PhD.
[00:19:43.600 --> 00:19:45.600]   So she was actually pretty fairly young.
[00:19:45.600 --> 00:19:50.040]   And she also went to Pomona College, which is an hour away from me.
[00:19:50.040 --> 00:19:53.320]   So I don't know, I found that out a week later,
[00:19:53.320 --> 00:19:56.080]   a week after the podcast was uploaded and everything.
[00:19:56.080 --> 00:19:59.080]   And I was like, wow, I knew her.
[00:19:59.080 --> 00:20:02.360]   And so I think I emailed her and I was like, hey, I remember you from this.
[00:20:02.360 --> 00:20:07.160]   This is super random, but thanks for coming on the podcast again.
[00:20:07.160 --> 00:20:08.760]   >> Nice, did you remember this?
[00:20:08.760 --> 00:20:15.240]   >> I don't remember actually, I don't have a very good memory of it.
[00:20:15.240 --> 00:20:18.000]   But I don't think that she remembered because I think she would have, yeah,
[00:20:18.000 --> 00:20:19.240]   there was a bunch of kids there.
[00:20:19.240 --> 00:20:20.680]   So she might have not remembered, but
[00:20:20.680 --> 00:20:23.880]   I did have a couple good conversations with her there, which was cool.
[00:20:23.880 --> 00:20:29.560]   >> Yeah, I would guess that the counselors I remember from my robotics camp when I
[00:20:29.560 --> 00:20:34.080]   was 12 years old probably do not remember one particular child like me.
[00:20:34.080 --> 00:20:40.680]   But I do remember the counselor Haji who was my favorite.
[00:20:40.680 --> 00:20:45.440]   So maybe there's an asymmetry there.
[00:20:45.440 --> 00:20:46.280]   >> Yeah. >> That's interesting.
[00:20:46.280 --> 00:20:52.640]   What about, I guess, we all consume a lot of media, or
[00:20:52.640 --> 00:20:54.040]   maybe I shouldn't speak for everybody.
[00:20:54.040 --> 00:20:58.560]   But lots of us consume lots of media and don't necessarily make it ourselves,
[00:20:58.560 --> 00:21:02.360]   right, we watch more TV shows than we produce.
[00:21:02.360 --> 00:21:04.440]   I think I can say that confidently.
[00:21:04.440 --> 00:21:07.640]   So I'm curious if the process of putting this whole thing together and
[00:21:07.640 --> 00:21:11.600]   making it work was eye opening or surprising to you in any way about
[00:21:11.600 --> 00:21:18.600]   either about the world of media production and podcasting or more broadly.
[00:21:18.600 --> 00:21:28.400]   >> I guess, yeah, I'm one of those people that consume a lot of media, unfortunately.
[00:21:28.400 --> 00:21:31.080]   I should reduce it, but it's okay.
[00:21:31.080 --> 00:21:37.520]   But I think just from experience, I had a general idea of how it was produced.
[00:21:37.520 --> 00:21:43.440]   And just when I transitioned into actually producing the podcast,
[00:21:43.440 --> 00:21:48.160]   I think just by consuming media just gave me that good idea of how it's produced.
[00:21:48.160 --> 00:21:51.120]   And it translated over, it was what I expected.
[00:21:51.120 --> 00:21:55.240]   Maybe a little bit more on the post-production,
[00:21:55.240 --> 00:21:58.640]   because one of the things we do is we make clips for our podcast and
[00:21:58.640 --> 00:22:01.720]   we post it on YouTube, just so it's more digestible.
[00:22:01.720 --> 00:22:04.280]   Because I know a huge podcast can be an hour and
[00:22:04.280 --> 00:22:06.760]   that might not be suitable for a lot of people.
[00:22:06.760 --> 00:22:09.760]   So we condense it into clips sometimes.
[00:22:09.760 --> 00:22:13.680]   And I thought that was gonna be really quick in post-production, but
[00:22:13.680 --> 00:22:17.080]   that takes two hours and yeah, it just adds to it.
[00:22:17.080 --> 00:22:22.280]   So I guess one thing that we're working towards is just to create a schedule or
[00:22:22.280 --> 00:22:26.960]   just try to not make it all over the place, but
[00:22:26.960 --> 00:22:28.680]   just try to keep it as structured as possible.
[00:22:28.680 --> 00:22:35.600]   >> And I guess in the process, you brought up the difficulty and
[00:22:35.600 --> 00:22:40.040]   the time consuming nature of post-production.
[00:22:40.040 --> 00:22:44.120]   Do you have any podcast production productivity tips?
[00:22:44.120 --> 00:22:48.680]   I've got a few that I've carefully crafted over the last year.
[00:22:48.680 --> 00:22:51.080]   But any thoughts about that?
[00:22:51.080 --> 00:22:56.240]   Have you tried using your computer science skills to make this better and faster?
[00:22:56.240 --> 00:23:04.280]   >> I think Matt, didn't you make an algorithm to email out to a bunch
[00:23:04.280 --> 00:23:07.200]   of different teachers across America,
[00:23:07.200 --> 00:23:11.480]   a bunch of high school teachers to email out our clips?
[00:23:11.480 --> 00:23:12.680]   >> That's true.
[00:23:12.680 --> 00:23:17.560]   It wasn't an algorithm exactly, but it was kind of a custom email list thing.
[00:23:17.560 --> 00:23:23.320]   Because yeah, one of the things that we do is we email to teachers all around the US.
[00:23:23.320 --> 00:23:29.560]   And sometimes they have email forms on their website, and that's hard to automate.
[00:23:29.560 --> 00:23:33.400]   And so I haven't finished it completely, I'm almost there.
[00:23:33.400 --> 00:23:36.760]   But yeah, just try to automate those forms that we have to complete.
[00:23:36.760 --> 00:23:41.640]   Yeah, I'm doing our best, but yeah, just the fact that we're doing it,
[00:23:41.640 --> 00:23:46.160]   it makes reaching out to hundreds of teachers a lot easier rather than just
[00:23:46.160 --> 00:23:48.360]   individually emailing all of them.
[00:23:48.360 --> 00:23:51.360]   because one thing that we like to do is we like to personalize the emails as much as
[00:23:51.360 --> 00:23:56.000]   possible, so that they just seem more receptive to us.
[00:23:56.000 --> 00:24:00.760]   So, but yeah, that's honestly made emailing a lot faster, so.
[00:24:00.760 --> 00:24:03.440]   >> Nice, yeah.
[00:24:03.440 --> 00:24:11.240]   I think one thing that's maybe underappreciated, I think,
[00:24:11.240 --> 00:24:16.040]   by people who are sort of outside the world of programming, is how useful,
[00:24:16.040 --> 00:24:19.480]   even being able to automate a simple form like that is.
[00:24:19.480 --> 00:24:24.040]   There's a pretty well known intro to Python book called Automate the Boring
[00:24:24.040 --> 00:24:28.680]   Stuff with Python, that's intended for people who don't have a computer science
[00:24:28.680 --> 00:24:33.040]   education, don't have strong programming fundamentals to be able to automate away
[00:24:33.040 --> 00:24:34.440]   certain parts of their jobs.
[00:24:34.440 --> 00:24:39.520]   And I found that has been the times that I've been most grateful for
[00:24:39.520 --> 00:24:40.680]   having developed these skills.
[00:24:40.680 --> 00:24:43.960]   When I see something, I have to email 100 people.
[00:24:43.960 --> 00:24:49.360]   And instead of thinking, boy, let me put on the headphones and type this all out.
[00:24:49.360 --> 00:24:51.920]   I think, well, maybe I can automate this.
[00:24:51.920 --> 00:24:57.320]   >> Yeah, and that's kind of why I love programming.
[00:24:57.320 --> 00:25:00.840]   It helps when you least expect it, so, yeah.
[00:25:00.840 --> 00:25:09.280]   >> So one thing actually that I liked about the way you structured your podcast
[00:25:09.280 --> 00:25:15.360]   was you included this nice little segment to break it up called 10 Epochs.
[00:25:15.360 --> 00:25:20.880]   Which is a segment in which the interviewees are asked
[00:25:20.880 --> 00:25:25.240]   ten quick questions and give their quick responses.
[00:25:25.240 --> 00:25:30.800]   So I've stolen this idea from you, at least for now.
[00:25:30.800 --> 00:25:34.560]   If I use it on future episodes, I'll make sure to credit you for this idea.
[00:25:34.560 --> 00:25:38.000]   But I've also stolen some of the questions.
[00:25:38.000 --> 00:25:41.360]   So I'm excited to hear your answers to some of the questions that you asked some
[00:25:41.360 --> 00:25:42.360]   of your guests.
[00:25:42.360 --> 00:25:44.600]   So 10 Epochs, here we go.
[00:25:44.600 --> 00:25:49.480]   And we'll do Ava and then Matt for each of these questions.
[00:25:49.480 --> 00:25:54.040]   All right, so favorite programming language?
[00:25:54.040 --> 00:25:57.680]   >> Not Python.
[00:25:57.680 --> 00:26:01.520]   [LAUGH] >> Ava, I didn't hear you there.
[00:26:01.520 --> 00:26:03.120]   I think you might have been on mute.
[00:26:03.120 --> 00:26:06.680]   >> Okay, I said not Python, but I'm kidding.
[00:26:06.680 --> 00:26:08.040]   I don't like Python.
[00:26:08.040 --> 00:26:12.760]   Personally, I'm just going to say C++ because that's what I've been doing for
[00:26:12.760 --> 00:26:16.440]   a long time with Visual Studios and stuff.
[00:26:16.440 --> 00:26:19.720]   And yeah, I mean, I think that it's just like, it's pretty cool and
[00:26:19.720 --> 00:26:22.000]   I like learning it because I know a lot of people use it.
[00:26:22.000 --> 00:26:24.960]   So I'm just going to say C++.
[00:26:24.960 --> 00:26:26.200]   >> Nice, Matt?
[00:26:26.200 --> 00:26:29.000]   >> Yeah, I guess for me, it has to be Python.
[00:26:29.000 --> 00:26:32.040]   I know it's kind of like popular choice, but
[00:26:32.040 --> 00:26:33.840]   it's what I've used the most and most comfortable with.
[00:26:35.040 --> 00:26:40.880]   >> Sure, the combination of Python and C++ together, that duo,
[00:26:40.880 --> 00:26:44.680]   it's the peanut butter and jelly of machine learning right there.
[00:26:44.680 --> 00:26:52.240]   And I personally thank God every day for the people who like C++ and
[00:26:52.240 --> 00:26:55.960]   enjoy learning and writing C++, so thanks for that.
[00:26:55.960 --> 00:27:00.200]   All right, TensorFlow or PyTorch?
[00:27:03.120 --> 00:27:06.000]   >> I'm just going to say PyTorch just because that's the only really thing that
[00:27:06.000 --> 00:27:06.680]   I've worked with.
[00:27:06.680 --> 00:27:12.720]   >> Yes, for me, I would say PyTorch too.
[00:27:12.720 --> 00:27:17.000]   TensorFlow can get too confusing at times, so yeah, PyTorch.
[00:27:17.000 --> 00:27:23.680]   >> Quick interlude, what do you do when you ask this question and
[00:27:23.680 --> 00:27:27.560]   then the interviewee just gives an answer that you completely disagree with?
[00:27:27.560 --> 00:27:30.440]   I'm not asking because of what's currently happening in this interview.
[00:27:30.440 --> 00:27:34.400]   I'm just thinking because I think in Chinasa's episode,
[00:27:34.400 --> 00:27:38.440]   I think she said MATLAB and TensorFlow.
[00:27:38.440 --> 00:27:43.360]   When you had Dr. Lachelle Hatley on,
[00:27:43.360 --> 00:27:47.720]   she definitely said TensorFlow.
[00:27:47.720 --> 00:27:49.000]   So what's going through your head?
[00:27:49.000 --> 00:27:49.520]   What are you thinking?
[00:27:49.520 --> 00:27:54.680]   >> I mean, yeah, because I know those are coming.
[00:27:54.680 --> 00:27:56.600]   It's bound to happen that someone disagrees with me.
[00:27:56.600 --> 00:27:59.920]   So I prepare mentally for it just in case that happens,
[00:27:59.920 --> 00:28:02.480]   just so I don't garner a wild reaction.
[00:28:02.480 --> 00:28:06.640]   But yeah, I just prepare for it mentally.
[00:28:06.640 --> 00:28:09.600]   Then I actually enjoy when we get different opinions,
[00:28:09.600 --> 00:28:11.280]   just it's not the same answers every time.
[00:28:11.280 --> 00:28:16.920]   >> Yeah, I guess it would be interesting.
[00:28:16.920 --> 00:28:20.080]   I don't know how much of you sort of dug in with people.
[00:28:20.080 --> 00:28:23.760]   I don't know how much of the conversation that you actually have ends up in
[00:28:23.760 --> 00:28:24.720]   the podcast.
[00:28:24.720 --> 00:28:28.880]   So I'm just curious if you've gotten the hotter takes from any of your guests
[00:28:28.880 --> 00:28:32.080]   on these questions of programming language or framework.
[00:28:32.080 --> 00:28:39.080]   >> Most of what we say in the podcast is kind of in there.
[00:28:39.080 --> 00:28:43.120]   So I don't know, for the 10 epochs, we usually don't question anything just
[00:28:43.120 --> 00:28:46.400]   because we want to keep it rolling, I guess.
[00:28:46.400 --> 00:28:50.760]   But yeah, I mean, I don't know, we just kind of like we accept their answer and
[00:28:50.760 --> 00:28:52.800]   just move on.
[00:28:52.800 --> 00:28:55.400]   >> Yeah, also, I guess like you said, for
[00:28:55.400 --> 00:28:59.160]   me the one that caught me off guard the most is probably the MATLAB one.
[00:28:59.160 --> 00:29:03.280]   I'm not a fan of MATLAB too much, but I just thought it was really interesting.
[00:29:03.280 --> 00:29:08.280]   >> Yeah, definitely, my eyebrows raised for sure, because I had some traumatic,
[00:29:08.280 --> 00:29:12.520]   like, it was also my first language, it was what I first started writing in.
[00:29:12.520 --> 00:29:15.800]   And I look back on it and I'm like, man, that was a traumatic experience.
[00:29:15.800 --> 00:29:19.200]   I sure wish I hadn't written all that MATLAB.
[00:29:19.200 --> 00:29:21.440]   But different, yeah.
[00:29:23.680 --> 00:29:27.880]   >> All right, moving on in 10 epochs, pre-pandemic hobby.
[00:29:27.880 --> 00:29:35.200]   If you can even remember that far back.
[00:29:35.200 --> 00:29:36.360]   >> My God, so long ago.
[00:29:36.360 --> 00:29:39.680]   Okay, pre-pandemic hobby.
[00:29:39.680 --> 00:29:44.560]   God, I don't know, my pre-pandemic hobby and
[00:29:44.560 --> 00:29:49.480]   something that lately with vaccines coming out and
[00:29:49.480 --> 00:29:52.640]   everything like that I've been getting into more lately, but hanging out with
[00:29:52.640 --> 00:29:57.840]   friends, I loved having parties at my house almost every weekend,
[00:29:57.840 --> 00:30:02.040]   just inviting a group over and family friends too.
[00:30:02.040 --> 00:30:05.880]   Just being around a lot of people, that was my hobby, I loved it so much.
[00:30:05.880 --> 00:30:12.200]   >> Yeah, not to copy Ava, but yeah, I think my first year at college,
[00:30:12.200 --> 00:30:18.680]   just being in the dorms and just having all your friends five feet away from you.
[00:30:18.680 --> 00:30:21.520]   I don't know, that's an experience that I'll never forget.
[00:30:21.520 --> 00:30:25.120]   And unfortunately, we haven't been able to do that this year, but yeah,
[00:30:25.120 --> 00:30:26.560]   just hanging out with friends, definitely.
[00:30:26.560 --> 00:30:32.680]   >> I think, yeah, Dr.
[00:30:32.680 --> 00:30:37.160]   Hadley said pretty much the same thing, which I think maybe is not an answer
[00:30:37.160 --> 00:30:40.040]   that people would have given for their favorite hobby before the pandemic.
[00:30:40.040 --> 00:30:45.120]   But we've maybe learned that you don't know what you got till it's gone.
[00:30:45.120 --> 00:30:45.680]   >> For sure.
[00:30:48.320 --> 00:30:52.920]   >> Right, what is your favorite computer science class that you have taken?
[00:30:52.920 --> 00:31:01.360]   >> I have not taken too many computer science classes at UCLA.
[00:31:01.360 --> 00:31:06.840]   >> Ooh, you're double E, so I'll accept a double E answer here,
[00:31:06.840 --> 00:31:09.080]   if you have an electrical engineering class you like.
[00:31:09.080 --> 00:31:10.120]   >> Okay, okay, cool.
[00:31:10.120 --> 00:31:12.120]   I'm actually gonna say the one that I'm taking right now.
[00:31:14.120 --> 00:31:18.320]   ECE3, it's like the intro to electrical engineering course.
[00:31:18.320 --> 00:31:22.200]   The professor is just kind of a meme.
[00:31:22.200 --> 00:31:23.760]   He's a cool guy.
[00:31:23.760 --> 00:31:25.320]   He's a pretty fun guy.
[00:31:25.320 --> 00:31:29.160]   And I also really have a lot of respect for him now because we usually have
[00:31:29.160 --> 00:31:32.600]   weekly homeworks that take a good hour to do every week.
[00:31:32.600 --> 00:31:36.080]   And we also have weekly quizzes at 8 AM on Mondays, and
[00:31:36.080 --> 00:31:37.920]   you just cancel them for the rest of the quarter.
[00:31:37.920 --> 00:31:41.080]   So I gotta say, I love him right now.
[00:31:41.080 --> 00:31:42.320]   I'm really into him right now.
[00:31:42.320 --> 00:31:44.200]   [LAUGH] Love that class.
[00:31:44.200 --> 00:31:45.200]   [LAUGH]
[00:31:45.200 --> 00:31:50.640]   >> Yeah, then as for me, also I'm taking it right now.
[00:31:50.640 --> 00:31:56.480]   I'm taking this class, artificial life and computer graphics, which I love.
[00:31:56.480 --> 00:32:01.040]   What I love about it is that there's only 20 people in the class and
[00:32:01.040 --> 00:32:02.720]   the professor speaks his mind.
[00:32:02.720 --> 00:32:05.400]   There's not too many people, he feels really comfortable about it.
[00:32:05.400 --> 00:32:07.040]   And that's what I love.
[00:32:07.040 --> 00:32:11.280]   And he specializes in that research field, which is really cool.
[00:32:11.280 --> 00:32:13.880]   He's done a lot of cool papers in the past and
[00:32:13.880 --> 00:32:17.520]   he explains everything really well and it's really fun.
[00:32:17.520 --> 00:32:19.400]   So yeah, I'd have to say that class.
[00:32:19.400 --> 00:32:24.520]   >> What is the coolest research or
[00:32:24.520 --> 00:32:29.320]   engineering project that you've seen in the past year?
[00:32:29.320 --> 00:32:33.840]   Either something you've done yourself, some friends have done, or
[00:32:33.840 --> 00:32:37.160]   something we'd all know about, like GPT-3 or something.
[00:32:40.840 --> 00:32:45.600]   >> Okay, so I was in a hackathon earlier this year.
[00:32:45.600 --> 00:32:48.760]   So I'm gonna shout out Courtney Givens and
[00:32:48.760 --> 00:32:52.080]   Brady, cannot pronounce his last name.
[00:32:52.080 --> 00:32:56.200]   But so the project that I made for that hackathon was actually pretty cool.
[00:32:56.200 --> 00:33:01.880]   My friend Raj and I, we made a phone box.
[00:33:01.880 --> 00:33:04.400]   So that in the pandemic, I feel like we use our phones a lot.
[00:33:04.400 --> 00:33:06.640]   So you put your phone into this box and
[00:33:06.640 --> 00:33:10.960]   it only lets you take it out after a certain amount of time.
[00:33:10.960 --> 00:33:14.560]   And it only lets you, or sorry, it only lets you take it out if a certain amount
[00:33:14.560 --> 00:33:18.080]   of time hasn't elapsed yet and you're only allowed to take it out a couple of times
[00:33:18.080 --> 00:33:19.400]   a day pretty much.
[00:33:19.400 --> 00:33:22.960]   So it's supposed to limit, it's like your phone is kinda out of sight, out of mind.
[00:33:22.960 --> 00:33:28.200]   And then, so then, but my friend Courtney and
[00:33:28.200 --> 00:33:31.080]   Brady, they made a really cool contraption.
[00:33:31.080 --> 00:33:33.080]   It was called the ZoomStay Clock.
[00:33:33.080 --> 00:33:36.600]   And so it was pretty much like a clock that you can put outside your door.
[00:33:36.600 --> 00:33:41.160]   That links to your computer, so it knows when your Zoom meetings are for class.
[00:33:41.160 --> 00:33:45.160]   And it has a timer that says how much time left you have in your class so
[00:33:45.160 --> 00:33:48.720]   that your parents don't walk in while you're in Zoom.
[00:33:48.720 --> 00:33:51.560]   And barge in and interrupt the class and everything like that.
[00:33:51.560 --> 00:33:56.280]   It'll tell them right there, okay, I got Zoom for 35 more minutes.
[00:33:56.280 --> 00:33:58.960]   So yeah, those are two pretty cool things.
[00:33:58.960 --> 00:34:02.440]   I mean, I made one and then theirs was awesome as well, so yeah.
[00:34:02.440 --> 00:34:06.440]   >> I could for sure use a ZoomStay Clock.
[00:34:06.440 --> 00:34:11.920]   Sometimes when I'm running the show, I put a little on air sign on my door,
[00:34:11.920 --> 00:34:14.400]   but I'd love to automate that, that'd be huge.
[00:34:14.400 --> 00:34:17.720]   >> Yeah, it was pretty cool.
[00:34:17.720 --> 00:34:19.080]   Yeah, they did a really good job with it.
[00:34:19.080 --> 00:34:27.760]   >> And yeah, for myself, I saw this really cool paper,
[00:34:27.760 --> 00:34:32.320]   I think it was maybe like five months ago, where these people,
[00:34:32.320 --> 00:34:36.680]   these researchers at CMU, they were able to, given any picture,
[00:34:36.680 --> 00:34:40.560]   like a picture of a courtyard or the front of a building,
[00:34:40.560 --> 00:34:44.720]   they're able to transform a picture into a 3D environment,
[00:34:44.720 --> 00:34:47.160]   which I thought was really cool.
[00:34:47.160 --> 00:34:51.080]   And I'm interested in graphics, so just being able to take a picture and
[00:34:51.080 --> 00:34:54.800]   just being able to transform it into any 3D environment,
[00:34:54.800 --> 00:34:57.160]   I thought that was really awesome, so I'd have to say that one.
[00:35:01.240 --> 00:35:04.720]   >> I see, yeah, so is this the Neural Radiance Fields paper, or
[00:35:04.720 --> 00:35:07.920]   is this a different, the Neural Radiance Fields was one where you could take
[00:35:07.920 --> 00:35:11.560]   a couple of images kind of sparsely around an object and
[00:35:11.560 --> 00:35:13.440]   then turn it into a 3D model.
[00:35:13.440 --> 00:35:16.480]   Is this in that line of work, or is this something different?
[00:35:16.480 --> 00:35:20.560]   >> I think it's similar, I don't think it was that exact paper, but
[00:35:20.560 --> 00:35:24.840]   I forgot what the paper was called, but it's pretty similar though.
[00:35:24.840 --> 00:35:30.280]   >> Got it, yeah, I can't remember whether the NERF paper was from CMU or not.
[00:35:30.280 --> 00:35:35.120]   I feel like it was from one of the big famous CS departments, but anyway.
[00:35:35.120 --> 00:35:40.000]   Cool paper, a lot of exciting stuff in the neural graphics world for sure.
[00:35:40.000 --> 00:35:47.480]   Hard pivot, startup idea, what is your billion dollar startup idea that you're
[00:35:47.480 --> 00:35:52.480]   sitting on for when you finally decide to pursue your founder dreams?
[00:35:52.480 --> 00:35:55.360]   I'll give you a second to dredge this up.
[00:35:55.360 --> 00:35:59.000]   Also, there's concerns about copyright, so
[00:35:59.000 --> 00:36:03.440]   you might need to insert a small bad idea into your good ideas that no one can steal
[00:36:03.440 --> 00:36:05.640]   it, but yeah.
[00:36:05.640 --> 00:36:07.720]   >> Okay, cool, all right, let me think about it.
[00:36:07.720 --> 00:36:09.480]   I've definitely thought about this before.
[00:36:09.480 --> 00:36:18.200]   This isn't a startup idea, I don't know if we can make this into a corporation.
[00:36:18.200 --> 00:36:21.840]   But this is something that I thought about with my boyfriend over the summer
[00:36:21.840 --> 00:36:23.960]   that we never actually ended up doing.
[00:36:23.960 --> 00:36:26.920]   But I thought it would have been pretty cool at the time, but I don't know,
[00:36:26.920 --> 00:36:30.160]   we got busy and stuff and it just didn't work.
[00:36:30.160 --> 00:36:33.880]   With school starting and everything, but what we wanted to do is we wanted to make
[00:36:33.880 --> 00:36:37.280]   this app, I'm not going to go into too many specifics because I kind of have some
[00:36:37.280 --> 00:36:38.760]   intention of making it.
[00:36:38.760 --> 00:36:43.320]   We wanted to make this app that pretty much, it's like a one stop shop to be able
[00:36:43.320 --> 00:36:47.360]   to support any political issue that you want.
[00:36:47.360 --> 00:36:52.600]   So it's an app and it has a bunch of different links to different types of
[00:36:52.600 --> 00:36:54.360]   issues and ways that you can get involved.
[00:36:54.360 --> 00:36:59.120]   So it can provide you with anything from upcoming rallies in your area,
[00:36:59.120 --> 00:37:02.800]   to petitions that you can sign, to books that you can read,
[00:37:02.800 --> 00:37:05.880]   to corporations that you can support.
[00:37:05.880 --> 00:37:07.480]   That was pretty much our idea.
[00:37:07.480 --> 00:37:12.760]   And especially in May when the Black Lives Matter movement kind of got
[00:37:12.760 --> 00:37:19.120]   a little bit more intense from everything that was happening at the time.
[00:37:19.120 --> 00:37:21.520]   That was definitely something that was on my mind.
[00:37:21.520 --> 00:37:24.120]   And so I think that's kind of where the idea came from.
[00:37:25.120 --> 00:37:32.200]   >> Those are definitely some choppy waters to wade into as your first startup idea.
[00:37:32.200 --> 00:37:35.240]   But I think, yeah, people could definitely benefit from better
[00:37:35.240 --> 00:37:36.360]   organization tools.
[00:37:36.360 --> 00:37:40.120]   Actually, I did read some interesting stuff.
[00:37:40.120 --> 00:37:45.160]   There's some, a lot of the labor unions that are moving to organize,
[00:37:45.160 --> 00:37:50.200]   groups that haven't been organized in the past, like service workers,
[00:37:50.200 --> 00:37:55.880]   food service workers, things like that, have been embracing more digital technology.
[00:37:55.880 --> 00:37:59.040]   And so organizing, the Fight for 15 movement, for example,
[00:37:59.040 --> 00:38:01.160]   used a lot of digital organizing.
[00:38:01.160 --> 00:38:04.720]   So I think it's maybe not a billion dollar startup,
[00:38:04.720 --> 00:38:08.480]   because maybe you can't make a billion dollars helping people.
[00:38:08.480 --> 00:38:10.640]   But it's a good startup idea maybe.
[00:38:16.200 --> 00:38:20.600]   >> And okay, yeah, for me, I'll be straightforward with mine.
[00:38:20.600 --> 00:38:21.960]   I've had this on mind for a while.
[00:38:21.960 --> 00:38:26.480]   For me, I see machine learning engineering.
[00:38:26.480 --> 00:38:30.440]   I feel like a lot of companies employ machine learning engineers to help build
[00:38:30.440 --> 00:38:31.760]   a product.
[00:38:31.760 --> 00:38:36.800]   But I feel like there's less so companies that are just purely based on
[00:38:36.800 --> 00:38:42.000]   machine learning consulting, specifically for automation tasks.
[00:38:42.000 --> 00:38:46.560]   I feel like there aren't that too many companies that do automation for
[00:38:46.560 --> 00:38:50.320]   various industries or various subtasks.
[00:38:50.320 --> 00:38:54.560]   And so I think my idea is kind of like building the Oracle, but
[00:38:54.560 --> 00:38:58.280]   instead of for software, for machine learning and automation.
[00:38:58.280 --> 00:39:02.320]   So that's kind of my idea.
[00:39:02.320 --> 00:39:06.760]   I know there's still time until machine learning can reach that point where it
[00:39:06.760 --> 00:39:10.480]   could automate a lot of different tasks, but that's my idea that I'm aiming for.
[00:39:10.480 --> 00:39:13.880]   >> Nice, ambitious for sure.
[00:39:13.880 --> 00:39:14.380]   >> Yeah.
[00:39:14.380 --> 00:39:21.720]   >> So with the last of my, I don't think I made it to 10 epochs, but
[00:39:21.720 --> 00:39:25.320]   this is going to be the last of our popcorn epoch questions.
[00:39:25.320 --> 00:39:32.760]   What is somebody in, who's somebody in the field of AI who you admire the most?
[00:39:32.760 --> 00:39:36.880]   Maybe not the most,
[00:39:36.880 --> 00:39:40.560]   maybe there's a bunch of people who are all at roughly the same level.
[00:39:40.560 --> 00:39:43.160]   You don't have to worry about offending anybody.
[00:39:43.160 --> 00:39:49.120]   >> I'm going to say Dr. Lachelle Hadley.
[00:39:49.120 --> 00:39:53.200]   She was honestly amazing and her story,
[00:39:53.200 --> 00:40:00.080]   if you haven't listened to the podcast yet, I recommend giving it a listen.
[00:40:00.080 --> 00:40:04.840]   But if you're looking for a more digestible clip on the ACM UCLA YouTube,
[00:40:04.840 --> 00:40:09.440]   there is a clip where she talks about pretty much what she did at Google.
[00:40:09.440 --> 00:40:13.880]   And how right after she talked at Google and some of her experience,
[00:40:13.880 --> 00:40:17.320]   she just pretty much talks about how when she was at Google campus,
[00:40:17.320 --> 00:40:19.880]   like working as a resident for them.
[00:40:19.880 --> 00:40:24.800]   She did experience a lot of discrimination and she did feel a little bit left out.
[00:40:24.800 --> 00:40:28.800]   But she really toughed it out because she knew that what she was doing was important.
[00:40:28.800 --> 00:40:33.640]   And so she was very inspiring and yeah, I have to say probably her.
[00:40:34.640 --> 00:40:39.080]   >> We'll definitely come back to that after we get Matt's answer.
[00:40:39.080 --> 00:40:44.440]   >> So actually, I was still my answer unfortunately, but yeah,
[00:40:44.440 --> 00:40:48.960]   honestly for some of the reasons I was said of course, but
[00:40:48.960 --> 00:40:54.040]   also she really inspired me in terms of teaching.
[00:40:54.040 --> 00:40:58.360]   She's a really personal or she's really relatable with everything she says.
[00:40:58.360 --> 00:41:01.440]   And I really love teaching AI and
[00:41:01.440 --> 00:41:05.680]   just hearing some of her responses about how she teaches or how she approaches it.
[00:41:05.680 --> 00:41:06.600]   Just really inspires me.
[00:41:06.600 --> 00:41:09.040]   So yeah, I'd have to say the show Hadley as well.
[00:41:09.040 --> 00:41:17.800]   >> Yeah, yeah, so that was a really incredible episode of the podcast,
[00:41:17.800 --> 00:41:18.720]   really great interview.
[00:41:18.720 --> 00:41:23.360]   And one of the pieces of it, I'd heard a little bit about that story.
[00:41:23.360 --> 00:41:27.560]   So just for folks who haven't heard this episode or
[00:41:27.560 --> 00:41:34.080]   weren't there when it was recorded, she was part of,
[00:41:34.080 --> 00:41:38.120]   like Google did a program where they wanted to increase their surface area with
[00:41:38.120 --> 00:41:41.160]   historically black colleges and universities in the United States.
[00:41:41.160 --> 00:41:44.320]   They invited people over.
[00:41:44.320 --> 00:41:47.520]   I think at one point it was called Howard West,
[00:41:47.520 --> 00:41:50.120]   was one version of the name of the program.
[00:41:50.120 --> 00:41:53.280]   And they ran into serious issues with discrimination and
[00:41:53.280 --> 00:41:56.720]   basically a casual racism on Google's campus.
[00:41:56.720 --> 00:42:00.360]   Dr. Hadley, of course, gives a much better version of this story than I can give
[00:42:00.360 --> 00:42:03.560]   in the podcast, you should definitely check it out.
[00:42:03.560 --> 00:42:07.760]   But one aspect of the story I didn't know was that the author of the Google
[00:42:07.760 --> 00:42:13.960]   manifesto, James Damore, was actually published it immediately after they left
[00:42:13.960 --> 00:42:19.160]   the campus and attended some of their events, which is just a wild twist
[00:42:19.160 --> 00:42:22.320]   to this story, which I'd never heard anywhere else.
[00:42:24.360 --> 00:42:27.520]   Yeah, I couldn't believe it, honestly, as well.
[00:42:27.520 --> 00:42:32.040]   >> Yeah, our old outreach director, Arjun,
[00:42:32.040 --> 00:42:38.960]   told us you have to get Dr. Lachelle Hadley on your podcast.
[00:42:38.960 --> 00:42:41.800]   She's amazing, and she has such a cool story.
[00:42:41.800 --> 00:42:47.160]   And so I just asked her about it in the interview and she said it.
[00:42:47.160 --> 00:42:51.360]   And it's like one of those problems that kind of makes you like kind of
[00:42:51.360 --> 00:42:53.400]   shaking with anger, I guess.
[00:42:53.400 --> 00:42:56.400]   I don't know, I take that stuff super personally.
[00:42:56.400 --> 00:43:01.800]   I know some people feel for it, but they don't take it very, very personally.
[00:43:01.800 --> 00:43:06.840]   But I just got, I remember I was super shocked, super mad, almost cried.
[00:43:06.840 --> 00:43:10.120]   [LAUGH] Yeah.
[00:43:10.120 --> 00:43:15.040]   >> Yeah, I think, yeah, it's really deeply upsetting,
[00:43:15.040 --> 00:43:20.880]   especially the, I don't know, the sort of disconnected thinking
[00:43:20.880 --> 00:43:24.800]   about these problems and working on these problems with people who really get it.
[00:43:24.800 --> 00:43:30.400]   And know how serious the problems are, and then sort of, I guess,
[00:43:30.400 --> 00:43:34.600]   I don't know, coming across people who are so just like, yeah,
[00:43:34.600 --> 00:43:36.920]   ignorant of the issues, so off base.
[00:43:36.920 --> 00:43:39.400]   Yeah.
[00:43:39.400 --> 00:43:44.440]   I guess on that front,
[00:43:44.440 --> 00:43:50.000]   I really appreciate the work that you're
[00:43:50.000 --> 00:43:55.000]   doing to expose the stories of these people that you're interviewing
[00:43:55.000 --> 00:43:59.320]   to a broader audience to create a more inclusive AI community.
[00:43:59.320 --> 00:44:03.800]   I'm wondering what you think are some of the most critically important issues that
[00:44:03.800 --> 00:44:06.600]   more inclusive community might resolve, right?
[00:44:06.600 --> 00:44:10.320]   Like what are the problems in AI right now and in the future that creating
[00:44:10.320 --> 00:44:13.160]   an inclusive community would help solve?
[00:44:13.160 --> 00:44:19.160]   >> Yeah, I think- >> Go ahead, Matt.
[00:44:19.160 --> 00:44:20.640]   >> Yeah, sorry.
[00:44:20.640 --> 00:44:25.560]   Yeah, I really think that it becomes, the main issue is that a lot of models
[00:44:25.560 --> 00:44:29.520]   nowadays, they're really biased, especially with NLP models.
[00:44:29.520 --> 00:44:33.880]   And I think just our community is inclusive, but
[00:44:33.880 --> 00:44:38.200]   I don't think it's to the extent that it needs to be.
[00:44:38.200 --> 00:44:42.840]   Because right now we have people of all backgrounds in AI that are speaking up,
[00:44:42.840 --> 00:44:45.120]   but I think we need more people.
[00:44:45.120 --> 00:44:47.200]   We need more people from these backgrounds to speak up so
[00:44:47.200 --> 00:44:50.760]   that becomes more prevalent that we need to address these issues.
[00:44:50.760 --> 00:44:54.280]   So I think just biases and models and even biases in data,
[00:44:54.280 --> 00:44:57.520]   I think that's probably the biggest issue.
[00:44:57.520 --> 00:45:01.680]   And just making sure that we're collecting the right kind of data and
[00:45:01.680 --> 00:45:03.720]   we're collecting a different set of data.
[00:45:03.720 --> 00:45:08.360]   And we have people from all different types of backgrounds,
[00:45:08.360 --> 00:45:11.200]   like reviewing the data, reviewing the models, biases,
[00:45:11.200 --> 00:45:13.120]   just making sure that everything's in check.
[00:45:13.120 --> 00:45:14.600]   I think that's the most important part.
[00:45:15.600 --> 00:45:22.120]   >> Mm-hm, yeah, it does seem like the participation, right?
[00:45:22.120 --> 00:45:25.960]   Getting more and more people to be participants and
[00:45:25.960 --> 00:45:32.480]   to have say and stake in these systems seems like the only way.
[00:45:32.480 --> 00:45:36.360]   Rather than waiting for the users of Twitter to discover that
[00:45:36.360 --> 00:45:40.040]   the cropping function seems to be racially biased.
[00:45:40.040 --> 00:45:43.040]   That's something that should be picked up in pre-production and
[00:45:43.040 --> 00:45:46.000]   could be picked up in pre-production if we had a more,
[00:45:46.000 --> 00:45:50.000]   like a community that was more inclusive and more diverse.
[00:45:50.000 --> 00:45:59.240]   >> Yeah, I would say, sorry, I honestly, Matt has said it, but
[00:45:59.240 --> 00:46:01.720]   I think that just really quickly,
[00:46:01.720 --> 00:46:05.600]   the biggest thing is bringing people to the table.
[00:46:05.600 --> 00:46:10.080]   I think that it's quite unfair, I don't know,
[00:46:10.080 --> 00:46:14.280]   I've heard a lot of adults, when I was in high school,
[00:46:14.280 --> 00:46:17.960]   I met a lot of adults that were from lower income communities that said,
[00:46:17.960 --> 00:46:20.480]   if I was a kid and I knew that engineering was a thing or
[00:46:20.480 --> 00:46:24.520]   I knew that computer science was a thing, I would have totally got involved with it.
[00:46:24.520 --> 00:46:29.160]   And I feel like it's that problem that leads us to have these ignorant models put
[00:46:29.160 --> 00:46:33.840]   out and things that are just completely tone deaf, and so I think,
[00:46:33.840 --> 00:46:37.040]   bringing people to the table is really important and
[00:46:37.040 --> 00:46:40.120]   raising their awareness can solve a lot of issues for us.
[00:46:40.120 --> 00:46:46.600]   >> Yeah, I talked with somebody who was building a model that was like a camera to
[00:46:46.600 --> 00:46:51.120]   detect distracted driving, so it could tell whether somebody was paying attention or
[00:46:51.120 --> 00:46:54.120]   not, whether they were looking at their phone, zoning out.
[00:46:54.120 --> 00:46:56.480]   And they really sold it as a safety thing and
[00:46:56.480 --> 00:47:00.920]   I could definitely see the potential safety benefits of something.
[00:47:00.920 --> 00:47:06.880]   But the final use case was a company was going to pay to install it
[00:47:06.880 --> 00:47:10.400]   in the cars of the drivers in their fleet.
[00:47:10.400 --> 00:47:15.200]   And I feel like if you had had enough experience working
[00:47:15.200 --> 00:47:20.320]   this sort of job where you have a manager who's breathing down your neck at all
[00:47:20.320 --> 00:47:28.240]   times, or if you'd worked in sort of out there in the trenches more,
[00:47:28.240 --> 00:47:33.800]   you would be able to more clearly see the potential, the power conflict there,
[00:47:33.800 --> 00:47:40.880]   the ethical issues and the concerns with enforcing this AI surveillance on truck
[00:47:40.880 --> 00:47:41.380]   drivers.
[00:47:41.380 --> 00:47:44.880]   >> Yeah.
[00:47:44.880 --> 00:47:45.880]   >> Sure, that's cool.
[00:47:45.880 --> 00:47:46.480]   That's interesting.
[00:47:46.480 --> 00:47:48.200]   I haven't heard about that.
[00:47:48.200 --> 00:47:49.320]   It's an interesting example.
[00:47:49.320 --> 00:47:56.720]   >> I think Amazon ended up rolling it out to some of their fleets,
[00:47:56.720 --> 00:48:03.400]   but which and got a ton of pushback, pretty much immediately from the drivers and
[00:48:03.400 --> 00:48:04.480]   from people who heard about it.
[00:48:04.480 --> 00:48:08.880]   >> Yeah, to me, just like things like that,
[00:48:08.880 --> 00:48:13.960]   it takes me back to my English class in high school when we read 1984.
[00:48:13.960 --> 00:48:17.840]   I'm not sure if you're all familiar with that book, but
[00:48:17.840 --> 00:48:19.520]   Big Brother is always watching over you.
[00:48:19.520 --> 00:48:23.000]   And yeah, that's the kind of vibes that it gives me.
[00:48:23.000 --> 00:48:26.480]   >> Mm-hm, mm-hm.
[00:48:26.480 --> 00:48:31.720]   I would definitely say, yeah, it's a little, I found it interesting actually that
[00:48:31.720 --> 00:48:35.960]   one of your guests, Paco Guzman, we mentioned him earlier from Facebook,
[00:48:35.960 --> 00:48:40.080]   talked about being very optimistic about the future of AI and machine learning.
[00:48:40.080 --> 00:48:42.800]   And I found that kind of surprising, because nowadays,
[00:48:42.800 --> 00:48:46.720]   I kind of associate optimism about machine learning with people who don't care
[00:48:46.720 --> 00:48:51.840]   about inclusion and don't care so much about biases,
[00:48:51.840 --> 00:48:56.560]   about how machine learning interacts with existing systems of power.
[00:48:56.560 --> 00:49:01.600]   So I'm curious, I mean, where are you on the optimism,
[00:49:01.600 --> 00:49:07.400]   pessimism scale between Paco and me?
[00:49:07.400 --> 00:49:14.120]   And what the future of AI holds for not just inclusion within our community,
[00:49:14.120 --> 00:49:20.600]   but for designing technologies that are beneficial and not harmful?
[00:49:23.760 --> 00:49:28.320]   >> I think that, I don't know, when I think of optimism, yeah,
[00:49:28.320 --> 00:49:29.880]   there is that undertone of it.
[00:49:29.880 --> 00:49:32.760]   I'd like to be optimistic.
[00:49:32.760 --> 00:49:38.000]   I'd like to say that things are going to get better, but [LAUGH] super sad,
[00:49:38.000 --> 00:49:42.120]   coming to realize when you're little, you think that everything is going to be fixed
[00:49:42.120 --> 00:49:42.720]   when you're older.
[00:49:42.720 --> 00:49:46.080]   And now that I'm becoming an adult, I'm 19 years old,
[00:49:46.080 --> 00:49:48.080]   I'm entering the adult realm.
[00:49:48.080 --> 00:49:52.600]   I'm starting to see just, nope, this is never going to,
[00:49:52.600 --> 00:49:56.360]   it's going to definitely decrease, but it's never going to end.
[00:49:56.360 --> 00:50:01.280]   There are always going to be people who just are racist or
[00:50:01.280 --> 00:50:05.120]   prejudiced or biased and don't care.
[00:50:05.120 --> 00:50:10.280]   So I'm optimistic in the sense that it's going to decrease,
[00:50:10.280 --> 00:50:15.560]   but I'm pessimistic in the fact that life is always suffering.
[00:50:15.560 --> 00:50:21.240]   [LAUGH] Yeah, so that's something that I've definitely begun to realize,
[00:50:21.240 --> 00:50:27.640]   but I will always work, my career goal is to always work to make it inclusive.
[00:50:27.640 --> 00:50:31.720]   That's something that I can grow up and work at a chip company and
[00:50:31.720 --> 00:50:36.640]   just be an electrical engineer, but I do want to have a social impact.
[00:50:36.640 --> 00:50:39.200]   I don't want to just be sitting behind a computer designing stuff,
[00:50:39.200 --> 00:50:41.400]   even though there's nothing wrong with that.
[00:50:41.400 --> 00:50:45.920]   I personally, in terms of my life goals, I want to be involved in a company or
[00:50:45.920 --> 00:50:52.240]   in a certain way in my life to be able to increase the applicability of technology
[00:50:52.240 --> 00:50:55.040]   and increase the representation.
[00:50:55.040 --> 00:51:03.960]   >> Yeah, I guess for myself, I don't have a definitive answer to that.
[00:51:03.960 --> 00:51:05.000]   I kind of lie in between.
[00:51:05.000 --> 00:51:11.040]   I think for me, how I approach everything I do is,
[00:51:11.040 --> 00:51:15.320]   I approach everything that I really want to leave everything better than it already
[00:51:15.320 --> 00:51:20.080]   is and kind of like what Abha said, I really want to uplift people,
[00:51:20.080 --> 00:51:24.760]   especially people from diverse backgrounds, including myself.
[00:51:24.760 --> 00:51:29.440]   I really want to uplift these people so that these problems become less prevalent.
[00:51:29.440 --> 00:51:34.880]   And just seeing impacts that I've made, I haven't made too many,
[00:51:34.880 --> 00:51:38.720]   but small impacts I've made to uplift people and
[00:51:38.720 --> 00:51:42.800]   just bring people into tech or whatever it may be.
[00:51:42.800 --> 00:51:47.080]   Now for me, that's really satisfying and just knowing that I'm uplifting people.
[00:51:47.080 --> 00:51:52.280]   And at the end of the day, I can't control the entire world.
[00:51:52.280 --> 00:51:55.720]   But what I can control is the impact I have on other people.
[00:51:55.720 --> 00:52:00.120]   And for me, that's my main goal, just having a positive impact on other people.
[00:52:00.120 --> 00:52:04.160]   And I understand that I can't change the world, but I'm doing the best I can.
[00:52:04.160 --> 00:52:11.440]   [BLANK_AUDIO]
[00:52:11.440 --> 00:52:15.560]   >> As I think, keeping on a slightly maybe tenser or
[00:52:15.560 --> 00:52:20.320]   darker note here, a question I wanted to ask you about,
[00:52:20.320 --> 00:52:25.440]   since this is something that I think about a lot as somebody who
[00:52:25.440 --> 00:52:30.160]   wants to make things better without making them worse, so to speak.
[00:52:30.160 --> 00:52:35.240]   How do you balance the need for representation and exposure?
[00:52:35.240 --> 00:52:39.560]   Exposure of people who are out there who don't know about engineering as
[00:52:39.560 --> 00:52:42.320]   a potential career path, don't know about AI.
[00:52:42.320 --> 00:52:46.200]   We gotta get out there and contact them.
[00:52:46.200 --> 00:52:50.440]   We want to provide role models, right?
[00:52:50.440 --> 00:52:52.880]   Get a hold of the people like Dr.
[00:52:52.880 --> 00:52:56.240]   Hadley, who are these incredible role models.
[00:52:56.240 --> 00:53:00.840]   How do you balance that need with the dangers of demanding that people
[00:53:00.840 --> 00:53:06.040]   provide basically this sort of unpaid labor to resolve the crises of racism and
[00:53:06.040 --> 00:53:11.320]   like cis hetero patriarchy and sort of demanding these things out of other
[00:53:11.320 --> 00:53:15.840]   people when they've been often burdened in so many other ways?
[00:53:15.840 --> 00:53:21.920]   [BLANK_AUDIO]
[00:53:21.920 --> 00:53:24.800]   >> I'm gonna quote Dr. Hadley here.
[00:53:24.800 --> 00:53:29.840]   So she did run into that issue when she went to Google and she wanted to leave
[00:53:29.840 --> 00:53:34.720]   because she's like, it's not my job to make these people not racist.
[00:53:34.720 --> 00:53:38.840]   She stuck it out because her friend told her, or
[00:53:38.840 --> 00:53:46.520]   one of her colleagues told her, this is what we've been working for.
[00:53:46.520 --> 00:53:50.920]   And props to her for sticking it out, but obviously I would have understood if she
[00:53:50.920 --> 00:53:53.440]   wanted to just go home and not deal with that.
[00:53:53.440 --> 00:53:57.560]   And I think that's something that we, it's definitely a hard question.
[00:53:57.560 --> 00:54:01.160]   And I am a female, so I do run into some issues, but
[00:54:01.160 --> 00:54:05.800]   I also understand that I've had a pretty privileged life.
[00:54:05.800 --> 00:54:07.920]   I have had a pretty privileged life.
[00:54:07.920 --> 00:54:10.720]   I haven't had to run into many issues, thankfully, but
[00:54:10.720 --> 00:54:13.000]   I know that's not the same for everyone else.
[00:54:13.000 --> 00:54:15.240]   So I definitely just don't like to talk about it.
[00:54:15.240 --> 00:54:20.320]   Like talk about it like I do know that or anything like that.
[00:54:20.320 --> 00:54:26.080]   But I don't know, I would just say, props to the people who toughen out,
[00:54:26.080 --> 00:54:31.600]   like Dr. Hadley, and go through very uncomfortable situations,
[00:54:31.600 --> 00:54:38.080]   just so that she can validate someone or fix a company's issue.
[00:54:38.080 --> 00:54:41.920]   But also props to the people that don't stand for it too.
[00:54:41.920 --> 00:54:44.960]   It's personal preference, it depends on the person.
[00:54:44.960 --> 00:54:49.440]   And I think that the whole point of this thing is we need to stop telling them.
[00:54:49.440 --> 00:54:54.040]   We need to just stop telling people who are, my belief is that I think we
[00:54:54.040 --> 00:54:58.440]   shouldn't have expectations about model minorities, for example.
[00:54:58.440 --> 00:55:03.520]   I think that they have the rights to make the decision based on what they would like
[00:55:03.520 --> 00:55:06.120]   to do and how they would like to be represented.
[00:55:06.120 --> 00:55:10.680]   And never expect them to fix everything for us because it's not their job.
[00:55:10.680 --> 00:55:16.400]   It's the oppressor's job, if anything, but yeah.
[00:55:16.400 --> 00:55:21.640]   >> Yeah, that was another really great moment, I thought,
[00:55:21.640 --> 00:55:26.280]   of the interview with Dr. Hadley, where she talked, I think her friend called it,
[00:55:26.280 --> 00:55:30.640]   the civil rights struggle of our time, which I thought was very apt.
[00:55:30.640 --> 00:55:35.240]   It does seem like there's a lot of focus and attention converging on the ways
[00:55:35.240 --> 00:55:40.600]   machine learned algorithms are rolling back the successes of labor rights and
[00:55:40.600 --> 00:55:42.240]   civil rights movements of the last century.
[00:55:42.240 --> 00:55:49.160]   Matt, any thoughts about the unpaid labor of diversity and
[00:55:49.160 --> 00:55:55.720]   equity and inclusion or tips for how to go about minimizing that impact and
[00:55:55.720 --> 00:55:56.720]   maximizing the benefit?
[00:55:56.720 --> 00:56:02.080]   >> Yeah, it's definitely a really tough question to answer,
[00:56:02.080 --> 00:56:07.320]   but I think about it a lot for myself.
[00:56:07.320 --> 00:56:13.720]   And like I always said, I understand people that do something about it and
[00:56:13.720 --> 00:56:14.680]   don't do something about it.
[00:56:14.680 --> 00:56:19.280]   But for myself, how I view it is if someone, a minority,
[00:56:19.280 --> 00:56:25.080]   is able to push through all the struggles and get to a good position in life,
[00:56:25.080 --> 00:56:29.480]   that's what they've been working for their entire life.
[00:56:29.480 --> 00:56:30.760]   And they've been going through all their struggles.
[00:56:30.760 --> 00:56:34.280]   And quite frankly, it might be hard for
[00:56:34.280 --> 00:56:39.440]   them to work more than they need to to help uplift others.
[00:56:39.440 --> 00:56:44.920]   But from what I've seen, from my experience, those type of people,
[00:56:44.920 --> 00:56:50.800]   they really love who they grew up with or where they grew up with.
[00:56:50.800 --> 00:56:54.560]   And I always see them going back to the community that helped uplift them.
[00:56:54.560 --> 00:56:56.960]   And that's always really inspiring.
[00:56:56.960 --> 00:56:59.280]   And that inspires me to do the same thing.
[00:56:59.280 --> 00:57:04.080]   And I'm not saying it's necessarily bad culture or
[00:57:04.080 --> 00:57:10.440]   culture that, I'm not saying it's something that everyone needs to follow.
[00:57:10.440 --> 00:57:14.560]   But that's what I see most people do, and that's what's inspired me.
[00:57:14.560 --> 00:57:17.040]   >> Got it, yeah.
[00:57:17.040 --> 00:57:23.200]   I think maybe I've been told that, well,
[00:57:23.200 --> 00:57:27.120]   one, we should probably end on a happier note if possible.
[00:57:27.120 --> 00:57:31.760]   And then also, you're supposed to end videos with a call to action.
[00:57:31.760 --> 00:57:35.800]   I don't know if you've come across this when learning how to do your
[00:57:35.800 --> 00:57:37.400]   media production.
[00:57:37.400 --> 00:57:41.800]   So I'm wondering if you have any thoughts about key steps that you think folks
[00:57:41.800 --> 00:57:47.400]   can take to make our community more representative and more inclusive.
[00:57:47.400 --> 00:57:52.480]   So maybe steps for students and professors, things for engineers,
[00:57:52.480 --> 00:57:54.320]   managers, and founders.
[00:57:54.320 --> 00:57:58.320]   Maybe for broader sort of stakeholders in AI, what they can do,
[00:57:58.320 --> 00:58:01.720]   people who are going to be impacted by these systems,
[00:58:01.720 --> 00:58:03.920]   which is pretty much everybody.
[00:58:03.920 --> 00:58:08.160]   Or other non-technical professionals, lawyers,
[00:58:08.160 --> 00:58:12.000]   doctors who maybe can be allies in this.
[00:58:12.000 --> 00:58:15.960]   Yeah, for any of those groups,
[00:58:15.960 --> 00:58:19.520]   what are your thoughts about key steps that you think they could take?
[00:58:19.520 --> 00:58:28.240]   >> Yeah, I think just in terms of being up to date on what AI is,
[00:58:28.240 --> 00:58:32.360]   if you're part of a different industry, I don't really think that there's a need to
[00:58:32.360 --> 00:58:34.160]   learn all the technical stuff about it.
[00:58:34.160 --> 00:58:38.240]   But just understand there's a lot of issues right now,
[00:58:38.240 --> 00:58:41.400]   especially with biases and machine learning models.
[00:58:41.400 --> 00:58:46.320]   And yeah, I think the biggest thing to take away is that AI isn't perfect.
[00:58:46.320 --> 00:58:48.240]   There are a lot of things we need to work towards.
[00:58:48.240 --> 00:58:52.440]   And the next step towards that is making sure that the people that make these
[00:58:52.440 --> 00:58:55.360]   models come from a diverse set of backgrounds.
[00:58:55.360 --> 00:58:59.000]   And I know we repeated that so many times, but I can't emphasize that enough.
[00:58:59.000 --> 00:59:03.360]   And I just think that the best way to do it is for
[00:59:03.360 --> 00:59:08.240]   the people that have made it to the stage where they are working behind the scenes
[00:59:08.240 --> 00:59:12.160]   with AI, just sharing their story, how they got there.
[00:59:12.160 --> 00:59:17.640]   I think just sharing stories, humans connect by sharing stories.
[00:59:17.640 --> 00:59:20.400]   And if we can inspire others by sharing their story and
[00:59:20.400 --> 00:59:23.440]   uplifting other people to make our community more diverse,
[00:59:23.440 --> 00:59:25.560]   then I think that's the best fit for it.
[00:59:25.560 --> 00:59:33.240]   >> Before we get Ava's answer, I would say I was recently reading
[00:59:33.240 --> 00:59:38.200]   Yeshe Milner, who is the head of Data for Black Lives,
[00:59:38.200 --> 00:59:42.360]   released a really, really great sort of paper,
[00:59:42.360 --> 00:59:47.760]   almost a short book at this point, about data capitalism that I was reading.
[00:59:47.760 --> 00:59:51.960]   And she talks about data washing in it, which is this sort of way that people
[00:59:51.960 --> 00:59:56.800]   take what should be decisions that are made by people about who's paid,
[00:59:56.800 --> 01:00:00.320]   how much, when, and then automate them and
[01:00:00.320 --> 01:00:04.360]   use the automation process to hide their bias.
[01:00:04.360 --> 01:00:07.720]   So I think what you said about one of the most important things that people outside
[01:00:07.720 --> 01:00:12.680]   of our community, what they can do is sort of learn not to trust AI models
[01:00:12.680 --> 01:00:18.040]   a little bit, don't give them that sort of objectivity that they automatically,
[01:00:18.040 --> 01:00:21.520]   statistics can't be racist or whatever attitude.
[01:00:21.520 --> 01:00:26.680]   Like cutting that out seems to be a really important step people can take
[01:00:26.680 --> 01:00:32.400]   to maybe mitigate some of the negative impacts that automation and
[01:00:32.400 --> 01:00:33.240]   machine learning can have.
[01:00:33.240 --> 01:00:42.200]   >> So I think that my call to action or the advice that I can give is,
[01:00:42.200 --> 01:00:49.240]   if you're in a position where you're teaching someone something or
[01:00:49.240 --> 01:00:55.840]   you're giving advice to everyone, don't underlook new inclusive teaching techniques.
[01:00:55.840 --> 01:01:00.560]   It can definitely be hard for individuals from an older generation or
[01:01:00.560 --> 01:01:04.040]   anyone who's just stubborn, frankly,
[01:01:04.040 --> 01:01:12.440]   to learn a new way to explain things, but don't be able to change that.
[01:01:12.440 --> 01:01:17.000]   I think the way that people learn things have such a profound impact on whether or
[01:01:17.000 --> 01:01:18.920]   not they're going to get into it or something like that.
[01:01:18.920 --> 01:01:21.560]   Like if you're a teacher, you already know this, but
[01:01:21.560 --> 01:01:24.280]   you literally have the ability to change lives.
[01:01:24.280 --> 01:01:27.600]   And you probably have one of the most important jobs in society.
[01:01:27.600 --> 01:01:31.120]   And unfortunately, I feel like a lot of people don't recognize that.
[01:01:31.120 --> 01:01:33.560]   But yeah, be open to looking up and
[01:01:33.560 --> 01:01:36.680]   researching about new inclusive teaching techniques.
[01:01:36.680 --> 01:01:41.200]   And also just for anyone in general, reading is super helpful.
[01:01:41.200 --> 01:01:44.400]   I think that reading from the perspective of, again,
[01:01:44.400 --> 01:01:47.800]   individuals who have taken on the responsibility,
[01:01:47.800 --> 01:01:51.240]   who are minorities who have taken that responsibility to try and fix things and
[01:01:51.240 --> 01:01:52.400]   stuff like that.
[01:01:52.400 --> 01:01:55.960]   Reading everything, not reading everything, but reading things that they write and
[01:01:55.960 --> 01:01:57.640]   reading their experiences and
[01:01:57.640 --> 01:02:01.280]   listening to the stories that Matt talked about are super important.
[01:02:01.280 --> 01:02:05.400]   And it just, it makes us empathize with each other a little bit more.
[01:02:05.400 --> 01:02:10.600]   I think that in general, all these issues come from a lack of empathy and
[01:02:10.600 --> 01:02:11.440]   ignorance as well.
[01:02:11.440 --> 01:02:14.360]   But I think that if we would just empathize with each other a little bit more,
[01:02:14.360 --> 01:02:17.440]   a lot of things would be nicer and easier.
[01:02:17.440 --> 01:02:20.400]   And I think the way that you empathize is you learn about people's struggles and
[01:02:20.400 --> 01:02:21.160]   you listen to them.
[01:02:21.160 --> 01:02:25.480]   And reading is the best way to listen to people's stories.
[01:02:25.480 --> 01:02:32.160]   >> Yeah, I guess to close this out here, do you have any quick book recommendations
[01:02:32.160 --> 01:02:35.560]   or any other media recommendations for these kinds of stories?
[01:02:35.560 --> 01:02:40.040]   I know it's growing, there's a lot more stuff than there was five years ago
[01:02:40.040 --> 01:02:44.360]   about this, but it'd be great to know what the good ones are.
[01:02:47.080 --> 01:02:51.840]   >> I would say the only ones that I can really think of right now are our old
[01:02:51.840 --> 01:02:57.120]   outreach director Arjun, they write some pretty great articles.
[01:02:57.120 --> 01:03:01.800]   Arjun Subramanian, I believe is their last name.
[01:03:01.800 --> 01:03:05.480]   Again, I met them online, so I never had to say their last name.
[01:03:05.480 --> 01:03:10.360]   But they write some pretty awesome articles about bias and stuff like that.
[01:03:10.360 --> 01:03:16.000]   And then also another one of our directors at UCLA, her name is Maya Rahman.
[01:03:16.000 --> 01:03:21.160]   She writes some pretty cool articles about bias in Google especially on Medium,
[01:03:21.160 --> 01:03:24.320]   which is like a blog website.
[01:03:24.320 --> 01:03:26.520]   So she has a pretty cool blog as well.
[01:03:26.520 --> 01:03:32.000]   And then one book that I read that probably changed the way I think
[01:03:32.000 --> 01:03:36.600]   about oppression in general, this book, it's very general,
[01:03:36.600 --> 01:03:41.000]   has nothing to do with AI, but it's called Pedagogy of the Oppressed.
[01:03:41.000 --> 01:03:44.600]   And it's an interesting read, it's definitely a cool read.
[01:03:44.600 --> 01:03:48.320]   Unfortunately, I was only able to get to the first section of it,
[01:03:48.320 --> 01:03:53.280]   because it has a couple of sections, but that was super interesting for sure.
[01:03:53.280 --> 01:03:59.480]   >> That is one of my siblings' favorite books also,
[01:03:59.480 --> 01:04:02.040]   they just absolutely love Pedagogy of the Oppressed.
[01:04:02.040 --> 01:04:06.720]   And also, by the way, I put the names for Arjun and for
[01:04:06.720 --> 01:04:09.880]   Maya in the YouTube chat for people who are listening.
[01:04:09.880 --> 01:04:14.480]   So if you check that out, you can get their names.
[01:04:14.480 --> 01:04:14.980]   Matt?
[01:04:14.980 --> 01:04:22.080]   >> Yeah, I don't have anything specific, but just for me,
[01:04:22.080 --> 01:04:27.800]   I already said that I'm a big junkie for media content earlier.
[01:04:27.800 --> 01:04:34.040]   Sometimes on YouTube, I just end up in the YouTube spiral where I keep going
[01:04:34.040 --> 01:04:38.640]   through a lot of stuff, but a lot of things I watch are about just stories
[01:04:38.640 --> 01:04:43.880]   about random people around the world, and from Europe, third world countries,
[01:04:43.880 --> 01:04:45.920]   whatever it may be, in different situations.
[01:04:45.920 --> 01:04:48.480]   I love learning about people and their struggles.
[01:04:48.480 --> 01:04:52.800]   And it's easy to watch people in their prime and
[01:04:52.800 --> 01:04:56.040]   their highlights, like most social medias now.
[01:04:56.040 --> 01:05:01.560]   But for me, I enjoy learning about how people live day to day and
[01:05:01.560 --> 01:05:02.720]   just the struggles that they face.
[01:05:02.720 --> 01:05:05.840]   And ever since I started doing that, maybe six months ago,
[01:05:05.840 --> 01:05:10.320]   I just become more empathetic, more understanding of people.
[01:05:10.320 --> 01:05:11.840]   And I just think that's the best way to do it,
[01:05:11.840 --> 01:05:14.480]   just learning other people's stories, where they come from.
[01:05:14.480 --> 01:05:18.280]   Everyone's different, and just being able to emphasize with someone that,
[01:05:18.280 --> 01:05:20.920]   a complete stranger, cuz you never know where they came from.
[01:05:20.920 --> 01:05:25.840]   I just think that's the best way to approach just everyone or
[01:05:25.840 --> 01:05:26.640]   just life in general.
[01:05:26.640 --> 01:05:34.280]   >> Well, I'd like to thank, to close this out here, thank both of you for
[01:05:34.280 --> 01:05:39.640]   coming on and telling a little bit of your stories in the course of this hour,
[01:05:39.640 --> 01:05:41.800]   and sharing that with our viewers.
[01:05:41.800 --> 01:05:46.240]   Hopefully they find them additionally inspiring, or
[01:05:46.240 --> 01:05:50.280]   they learn a little bit more about how to empathize from your stories.
[01:05:50.280 --> 01:05:54.880]   So thank you so much for coming.
[01:05:54.880 --> 01:05:57.840]   I'm going to close out the live stream.
[01:05:57.840 --> 01:06:03.360]   Thanks to everybody in the audience and to all the folks watching this later.
[01:06:03.360 --> 01:06:06.960]   Check the description of the video for links to some of the things that we.

