
[00:00:00.000 --> 00:00:04.800]   I've had to do plenty of unpopular things. I think anytime you have to run a company that endures
[00:00:04.800 --> 00:00:10.400]   a century and has to endure another century, you will do unpopular things. You have no choice.
[00:00:10.400 --> 00:00:16.000]   And I often felt I had to sacrifice things for the long term. And whether that would have been
[00:00:16.000 --> 00:00:22.240]   really difficult things like job changes or reductions, or whether it would be things like,
[00:00:22.240 --> 00:00:28.320]   "Hey, we're going to change the way we do our semiconductors," and a whole different philosophy,
[00:00:29.360 --> 00:00:33.040]   you have no choice. And in times of crisis as well, you got to be...
[00:00:33.040 --> 00:00:35.040]   I would say it's not a popularity contest.
[00:00:35.040 --> 00:00:41.680]   The following is a conversation with Jeannie Vermetti, who was a longtime CEO,
[00:00:41.680 --> 00:00:46.720]   president, and chairman of IBM. And for many years, she was widely considered to be one of
[00:00:46.720 --> 00:00:52.800]   the most powerful women in the world. She's the author of a new book on power, leadership,
[00:00:52.800 --> 00:01:00.880]   and her life story called "Good Power," coming out on March 7th. She is an incredible leader
[00:01:00.880 --> 00:01:07.200]   and human being, both fearless and compassionate. It was a huge honor and pleasure for me to sit
[00:01:07.200 --> 00:01:12.320]   down and have this chat with her. This is the Lex Friedman Podcast. To support it,
[00:01:12.320 --> 00:01:18.160]   please check out our sponsors in the description. And now, dear friends, here's Jeannie Vermetti.
[00:01:19.280 --> 00:01:24.880]   You worked at IBM for over 40 years, starting as a systems engineer, and you ran the company
[00:01:24.880 --> 00:01:31.840]   as chairman, president, and CEO from 2011 to 2020. IBM is one of the largest tech companies in the
[00:01:31.840 --> 00:01:39.760]   world with, maybe you can correct me on this, with about 280,000 employees. What are the biggest
[00:01:39.760 --> 00:01:44.720]   challenges running the company of that size? Let's start with a sort of big overview question.
[00:01:45.280 --> 00:01:48.640]   The biggest challenges I think are not in running them, it's in changing them.
[00:01:48.640 --> 00:01:54.320]   And that idea to know what you should change and what you should not change. Actually, people don't
[00:01:54.320 --> 00:01:59.360]   always ask that question. What should endure, even if it has to be modernized, but what should endure?
[00:01:59.360 --> 00:02:04.720]   And then I found the hardest part was changing how work got done at such a big company.
[00:02:04.720 --> 00:02:10.480]   What was the parts that you thought should endure? The core of the company that was beautiful and
[00:02:10.480 --> 00:02:13.520]   powerful and could persist through time, that should persist through time.
[00:02:13.520 --> 00:02:16.080]   I'd be interested, do you have a perception of what you think it would be?
[00:02:16.080 --> 00:02:23.040]   Do I have a perception? Well, I'm a romantic for a history of long running companies,
[00:02:23.040 --> 00:02:31.600]   so there's kind of a tradition as a AI person. To me, IBM has some epic sort of research
[00:02:31.600 --> 00:02:38.880]   accomplishments where you show off, you know, Deep Blue and Watson, just impressive big moonshot
[00:02:39.440 --> 00:02:44.400]   challenges and accomplishing those. But that's, I think, probably a small part of what IBM is.
[00:02:44.400 --> 00:02:48.160]   That's mostly like the sexy public facing part.
[00:02:48.160 --> 00:02:54.320]   Yeah. Well, certainly the research part itself is over 3,000, so it's not that small. That's
[00:02:54.320 --> 00:03:01.040]   a pretty big research group. But the part that should endure ends up being a company that does
[00:03:01.040 --> 00:03:09.200]   things that are essential to the world. Meaning, think back, you said you're a romantic. It was
[00:03:09.200 --> 00:03:13.680]   the '30s, the social security system. It was putting the man on the moon. It was, you know,
[00:03:13.680 --> 00:03:21.360]   to this day, banks don't run, railroads don't run. That is at its core, it's doing mission
[00:03:21.360 --> 00:03:26.720]   critical work. And so that part, I think, is at its core, it's a business to business company.
[00:03:26.720 --> 00:03:32.640]   And at its core, it's about doing things that are really important to the world becoming running and
[00:03:32.640 --> 00:03:37.360]   being better. Running the infrastructure of the world, so doing it at scale, doing it reliably.
[00:03:37.360 --> 00:03:41.200]   Yes, secure in this world, that's like everything. And in fact, when I started,
[00:03:41.200 --> 00:03:45.680]   I almost felt people were looking for what that was. And together, we sort of, in a word,
[00:03:45.680 --> 00:03:51.280]   it was to be essential. And the reason I love that word was I can't call myself essential.
[00:03:51.280 --> 00:03:56.240]   You have to determine I am, right? So it was to be essential, even though some of what we did is
[00:03:56.240 --> 00:03:59.600]   exactly what you said, it's below the surface. So many people, because people say to me, well,
[00:03:59.600 --> 00:04:06.240]   what does IBM do now? Right? And over the years, it's changed so much. And today, it's really a
[00:04:06.240 --> 00:04:12.160]   software and consulting company. Consulting is a third of it. And the software is all hybrid cloud
[00:04:12.160 --> 00:04:17.200]   and AI. That would not have been true, as you well know, back even two decades ago, right? So
[00:04:17.200 --> 00:04:22.560]   it changes. But I think at its core, it's that be essential. You said moonshot, can't all be
[00:04:22.560 --> 00:04:26.400]   moonshots because moonshots don't always work, but mission critical work.
[00:04:26.400 --> 00:04:35.280]   So given the size, though, when you started running it, did you feel the sort of thing that
[00:04:35.280 --> 00:04:41.680]   people usually associate with size, which is bureaucracy, and maybe the aspect of size that
[00:04:41.680 --> 00:04:47.920]   hinder progress or hinder pivoting, did you feel that? - You would, for lots of reasons. I think
[00:04:47.920 --> 00:04:53.280]   when you're a big company, sometimes people think of process as the client themselves.
[00:04:53.280 --> 00:04:59.760]   I always say to people, your process is not your customer. There is a real customer here that you
[00:04:59.760 --> 00:05:07.360]   exist for. And that's really easy to fall into because people are a master to this process. And
[00:05:07.360 --> 00:05:13.760]   that's not right. And when you're big, the other thing, and boy, there's a premium on it, is speed,
[00:05:13.760 --> 00:05:20.800]   right? That in our industry, you got to be fast. And go back, like when I took over and it was 2012,
[00:05:20.800 --> 00:05:26.800]   we had a lot of catching up to do and a lot of things to do, and it was moving so fast. And as
[00:05:26.800 --> 00:05:31.920]   you well know, all those trends were happening at once, which made them go even faster. And so
[00:05:31.920 --> 00:05:36.480]   pretty unprecedented, actually, for that many trends to be at one time. And I used to say to
[00:05:36.480 --> 00:05:43.440]   people, "Go faster, go faster, go faster." And honestly, I've tired them out. I mean,
[00:05:43.440 --> 00:05:49.360]   it kind of dawned on me that when you're that big, that's a really valuable lesson. And it taught me
[00:05:49.360 --> 00:05:54.560]   the how's perhaps more important than the what. Because if I didn't do something to change how
[00:05:54.560 --> 00:05:59.360]   work was done, like change those processes or give them new tools, help them with skills,
[00:05:59.360 --> 00:06:05.280]   they couldn't. They'll just do the same thing faster. If someone tells you, "You've got hiking
[00:06:05.280 --> 00:06:09.040]   boots," and they're like, "No, go run a marathon." You're like, "I can't do it in those boots."
[00:06:09.040 --> 00:06:14.880]   But so you've got to do something. And at first, I think the ways for big companies,
[00:06:14.880 --> 00:06:19.280]   I would call them like blunt clubs. You do what everyone does. You reduce layers. Because if you
[00:06:19.280 --> 00:06:25.600]   reduce layers, decisions go faster. It's math. If there's less decision points, things go faster.
[00:06:25.600 --> 00:06:33.440]   You do the blunt club thing. And then after that, though, it did lead me down a long journey of,
[00:06:33.440 --> 00:06:37.600]   they sound like buzzwords, but if you really do them at scale, they're hard, around things like
[00:06:37.600 --> 00:06:44.880]   agile. Because you've really got to change the way work gets done. And we ended up training,
[00:06:44.880 --> 00:06:48.400]   God, hundreds of thousands of people on that stuff to really change it.
[00:06:48.400 --> 00:06:50.480]   - Correctly. - On how to do it correctly.
[00:06:50.480 --> 00:06:55.120]   That's right. Because everybody talks about it. But the idea that you would really have small,
[00:06:55.120 --> 00:07:01.760]   multidisciplinary teams work from the outside in, set those sort of interim steps, take the
[00:07:01.760 --> 00:07:07.280]   feedback pivot, and then do it on not just products, do it on lots of things. It's hard to
[00:07:07.280 --> 00:07:12.560]   do at scale. People always say, "Oh, I got this agile group over here of 40 people." But not when
[00:07:12.560 --> 00:07:15.760]   you're a couple hundred thousand people. You got to get a lot of people to work that way.
[00:07:15.760 --> 00:07:19.600]   - The blunt club thing you're talking about. So flatten the organization as much as possible.
[00:07:19.600 --> 00:07:22.320]   - Yeah, yeah. I probably reduced the layers of management by half.
[00:07:22.320 --> 00:07:31.680]   And so that has lots of benefits, right? Time to a decision, more autonomy to people. And then
[00:07:31.680 --> 00:07:37.520]   the idea of faster clarity of where you're going. Because you're not just filtered through so many
[00:07:37.520 --> 00:07:42.720]   different layers. And I think it's the kind of thing a lot of companies, if you're big,
[00:07:42.720 --> 00:07:46.080]   have to just keep going through. It's kind of like grass grows. It just comes back,
[00:07:46.080 --> 00:07:52.240]   and you got to go back town and work on it. So it's a natural thing. But I hear so many people
[00:07:52.240 --> 00:07:56.480]   talk about it, Lex, this idea of like, "Okay, well, who makes a decision?" You've often heard,
[00:07:56.480 --> 00:08:01.040]   "Nobody can say yes, and everybody can say no." And that's actually what you're trying to get out
[00:08:01.040 --> 00:08:06.320]   of a system like that. - So, I mean, your book in general,
[00:08:06.320 --> 00:08:12.560]   the way you lead is very much about we and us, the power of we. But is there times when a
[00:08:12.560 --> 00:08:17.840]   leader has to step in and be almost autocratic, take control and make hard, unpopular decisions?
[00:08:17.840 --> 00:08:21.600]   - Oh, I am sure you know the answer to that. And it is, of course, yes.
[00:08:21.600 --> 00:08:24.400]   - It's just fun to hear you say it. - It's fun to say it. Yeah. You know,
[00:08:24.400 --> 00:08:29.840]   'cause I actually, A, there's a leader for a time, but then there's a leader for a situation,
[00:08:29.840 --> 00:08:35.440]   right? And so I've had to do plenty of unpopular things. I think anytime you have to run a company
[00:08:35.440 --> 00:08:40.720]   that endures a century and has to endure another century, you will do unpopular things. You have
[00:08:40.720 --> 00:08:46.880]   no choice. And I often felt I had to sacrifice things for the long-term. And whether that would
[00:08:46.880 --> 00:08:53.520]   have been really difficult things like job changes or reductions, or whether it would be things like,
[00:08:53.520 --> 00:08:59.600]   "Hey, we're gonna change the way we do our semiconductors," and a whole different philosophy,
[00:08:59.600 --> 00:09:05.200]   you have no choice. I mean, and in times of crisis as well, you gotta be... I always said it's not a
[00:09:05.200 --> 00:09:10.480]   popularity contest. So that's... None of these jobs are popularity contests. I don't care if
[00:09:10.480 --> 00:09:14.960]   your company's got one person or half a million. They're not popularity contests.
[00:09:14.960 --> 00:09:19.920]   - But psychologically, is it difficult to just sort of step in as a new CEO and...
[00:09:19.920 --> 00:09:25.680]   'Cause you're fighting against tradition, against all these people that act like experts of their
[00:09:25.680 --> 00:09:29.840]   thing, and they are experts of their thing, to step in and say, "We have to do it differently."
[00:09:29.840 --> 00:09:34.400]   - Yeah. When you gotta change a company, it's really tempting to say, "Throw everything else
[00:09:34.400 --> 00:09:39.040]   out." Back to that, what must endure, right? But I know when I took over to start, I knew how much
[00:09:39.040 --> 00:09:44.160]   had to change. The more I got into it, I could see, wow, a lot more had to change, right? 'Cause
[00:09:44.160 --> 00:09:49.280]   we needed a platform. We'd always done our best when we had a platform, a technology platform.
[00:09:49.280 --> 00:09:53.840]   You will go back in time and you'll think of the mainframe systems. You'll think of the PC. You'll
[00:09:53.840 --> 00:10:00.080]   think of perhaps middleware. You could even call services a platform. We needed a platform,
[00:10:00.080 --> 00:10:07.200]   the next platform here to be there. Skills. When I took over, we inventoried who had modern skills
[00:10:07.200 --> 00:10:10.880]   for the future. It was two out of 10 people for the future. Not that they didn't have relevant
[00:10:10.880 --> 00:10:16.160]   skills today, but for the future, two out of 10. Yikes, that's a big problem, right? The speed at
[00:10:16.160 --> 00:10:23.360]   which things were getting done. So you got so much to do, and you say, "Is that a scary thing?" Yes.
[00:10:23.360 --> 00:10:29.760]   Do you have to sometimes dictate? Yes. But I did find, and it is worth it. I know every big company
[00:10:29.760 --> 00:10:36.160]   I know. My good friend that runs General Motors, she's had to change. Go back to what is them,
[00:10:36.160 --> 00:10:42.560]   them. When you do that, that back to be essential, we started with, "Hey, it's be essential." Then
[00:10:42.560 --> 00:10:46.560]   the next thing I did with the team was say, "Okay, now this means new era of computing,
[00:10:46.560 --> 00:10:50.880]   new buyers are out there, and we better have new skills. Okay, now the next thing,
[00:10:50.880 --> 00:10:58.560]   how do you operationalize it?" It just takes some time, but you can engineer that and get people to
[00:10:58.560 --> 00:11:06.560]   build belief. For the skills, that means hiring and it means training? Yes. Oh boy, that's a long,
[00:11:06.560 --> 00:11:11.040]   skills is a really long topic in and of itself. I try to put my view in it. I learned a lot,
[00:11:11.040 --> 00:11:17.840]   and I changed my view on this a lot. I'll go back at my very beginning, say 40 years ago,
[00:11:17.840 --> 00:11:23.600]   I would have said at that point, "Okay." I was always in a hurry. I was interviewing to hire
[00:11:23.600 --> 00:11:27.600]   people. I don't know how you hire people. 40 years ago, I'd be like, "Okay, I got to fit in these
[00:11:27.600 --> 00:11:31.520]   interviews. I got to hire someone to get this done." Then time would go on. I'm like, "Oh,
[00:11:31.520 --> 00:11:35.440]   that's not very good." In fact, someone once said to me, "Hey, hire the best people to work for you,
[00:11:35.440 --> 00:11:39.760]   and your job gets a lot easier. Okay, I should spend more time on this topic, spend more time
[00:11:39.760 --> 00:11:45.520]   on it." Then it was like, "Okay, hire experts." Okay, hired a lot of experts over my life.
[00:11:45.520 --> 00:11:51.760]   Then I was really like an epiphany, and it really happened over my tenure running the
[00:11:51.760 --> 00:11:56.560]   company and having to change skills. If someone's an expert at something and has just done that for
[00:11:56.560 --> 00:12:02.560]   30 years, the odds of them really wanting to change a lot are pretty low. When you're in a
[00:12:02.560 --> 00:12:08.960]   really dynamic industry, that's a problem. Okay, that was my first revelation on this.
[00:12:08.960 --> 00:12:14.960]   Then when I looked to hiring, I can remember when I started my job, we needed cyber people.
[00:12:14.960 --> 00:12:20.880]   I go out there and I look. Unemployment in the US was almost 10%. Can't find them. Okay, it's 10%,
[00:12:20.880 --> 00:12:24.560]   and I can't find the people. Okay, what's the issue? Okay, they're not teaching the right
[00:12:24.560 --> 00:12:31.040]   things. That led me down a path, and it was serendipity that I happened to do a review of
[00:12:31.040 --> 00:12:36.160]   corporate social responsibility. We had this one little fledgling school in a low-income area,
[00:12:36.160 --> 00:12:41.440]   and high school with a community college, we gave them internships, direction on curriculum.
[00:12:41.440 --> 00:12:48.720]   Lo and behold, we could hire these kids. I said, "Hmm, this is not CSR. I just found a new talent
[00:12:48.720 --> 00:12:55.280]   pool," which takes me to now what I'm doing in my post-retirement. I'm like, "This idea that don't
[00:12:55.280 --> 00:13:02.240]   hire just for a college degree," we had 99% of our hires were college and PhDs, and I'm all for it.
[00:13:02.240 --> 00:13:04.480]   So you're very, "Don't, don't."
[00:13:04.480 --> 00:13:05.520]   I'm deeply offended.
[00:13:05.520 --> 00:13:09.280]   No, you should not be. I'm vice chair at Northwestern, one of the vice chairs.
[00:13:09.280 --> 00:13:15.520]   But I said, "I just really like aptitude does not equal access." These people didn't have access,
[00:13:15.520 --> 00:13:22.080]   but they had aptitude. It changed my whole view to skills first. And so now for hiring,
[00:13:22.080 --> 00:13:25.760]   that's kind of a long story to tell you, the number one thing I would hire for now
[00:13:25.760 --> 00:13:29.680]   is somebody's willingness to learn. And you can test, you can try different ways,
[00:13:29.680 --> 00:13:34.640]   but their curiosity and willingness to learn, hands down, I will take that trait over anything
[00:13:34.640 --> 00:13:35.440]   else they have.
[00:13:35.440 --> 00:13:37.200]   So the interview process, the questions you ask-
[00:13:37.200 --> 00:13:38.560]   Changed. Everything changed.
[00:13:38.560 --> 00:13:43.360]   The kind of things you talk to them about is try to get at how curious they are about the work.
[00:13:43.360 --> 00:13:47.200]   And testing. I mean, we triangulated around it lots of ways. And now look,
[00:13:47.200 --> 00:13:52.080]   at the heart of it, what it would do is change. You don't think of buying skills,
[00:13:52.080 --> 00:13:57.760]   you think of building skills. And when you think that way, with so many people, and I think
[00:13:57.760 --> 00:14:02.960]   this country, many developed countries being disenfranchised, you got to bring them back
[00:14:02.960 --> 00:14:06.160]   into the workforce somehow, and they got to get some kind of contemporary skills.
[00:14:06.160 --> 00:14:10.480]   And if you took that approach, you can bring them back into the workforce.
[00:14:10.480 --> 00:14:15.120]   Yeah, I think some interesting combination of humility and passion. Because like you said,
[00:14:15.120 --> 00:14:22.240]   experts sometimes lack humility if they call themselves an expert for a few too many years.
[00:14:22.240 --> 00:14:27.600]   So you have to have that beginner's mind and a passion to be able to aggressively, constantly
[00:14:27.600 --> 00:14:30.320]   be a beginner at everything and learn and learn and learn.
[00:14:30.320 --> 00:14:35.760]   You know, I saw it firsthand when we were beginning this path down the cloud in AI,
[00:14:35.760 --> 00:14:41.840]   and people would say, "Oh, IBM, you know, it's existential. They've got to change." And
[00:14:41.840 --> 00:14:46.560]   all these things. And I did hire a lot of people from outside, very willing to learn new things.
[00:14:46.560 --> 00:14:51.760]   "Come on in. Come on in." And I sometimes say, "Shiny objects. Trained in shiny objects. Come
[00:14:51.760 --> 00:14:56.160]   on in." But I saw something, it was another one of these, "You're not a shiny object. I'm not
[00:14:56.160 --> 00:15:04.640]   saying that." But I learned something. Okay, some of them did fantastic. And others, they're like,
[00:15:04.640 --> 00:15:09.200]   "Well, let me school you on everything." But they didn't realize we did really mission-critical
[00:15:09.200 --> 00:15:13.520]   work. And that'd break a bank. I mean, they would not understand the certain kind of security and
[00:15:13.520 --> 00:15:19.200]   the auditability and everything they had to go on. And then I watched IBM people say, "Oh,
[00:15:19.200 --> 00:15:22.480]   I actually could learn something." Some were like, "Yeah. Okay. I don't know how to do that. That's
[00:15:22.480 --> 00:15:26.800]   a really good thing I could learn." And in the end, there was not like one group was a winner
[00:15:26.800 --> 00:15:30.240]   and one was a loser. The winners were the people who were willing to learn from each other.
[00:15:30.240 --> 00:15:36.560]   I mean, to me, it was very stark example of that point. And I saw it firsthand. So,
[00:15:36.560 --> 00:15:40.240]   that's why I'm so committed to this idea about skills first. And that's how people should be
[00:15:40.240 --> 00:15:47.360]   hired, promoted, paid, you name it. - Yeah. The AI in general, it seems like
[00:15:47.360 --> 00:15:52.960]   nobody really understands now what the future will look like. We're all trying to figure it out. So,
[00:15:52.960 --> 00:15:59.680]   what IBM will look like in 50 years in relation to the software business to AI is unknown. What
[00:15:59.680 --> 00:16:04.240]   Google will look like, what all these companies, we're trying to figure it out. And that means
[00:16:04.240 --> 00:16:11.440]   constantly learning, taking risks, all of those things. And nobody's really skilled in AI.
[00:16:11.440 --> 00:16:16.400]   - You're absolutely right. That's right. I couldn't agree more with you on that.
[00:16:16.400 --> 00:16:25.200]   - You wrote in the book, speaking of hiring, "My drive for perfection often meant I only
[00:16:25.200 --> 00:16:29.600]   focused on what needed to change without acknowledging the positive. This could keep
[00:16:29.600 --> 00:16:34.800]   people from trusting themselves. It could take me a while to learn that just because I could point
[00:16:34.800 --> 00:16:39.840]   something out didn't mean I should. I still spotted errors, but I became more deliberate
[00:16:39.840 --> 00:16:45.680]   about what I mentioned and sent back to get fixed. I also tried to curtail my tendency to micromanage
[00:16:45.680 --> 00:16:51.920]   and let people execute. I had to stop assuming my way was the best or only way. I was learning
[00:16:51.920 --> 00:16:56.720]   that giving other people control builds their confidence and that constantly trying to control
[00:16:56.720 --> 00:17:04.560]   people destroys it." So, what's the right balance between showing the way and helping people find
[00:17:04.560 --> 00:17:13.440]   the way? - That is a good question. Because, like a really flip answer would be, as it gets bigger,
[00:17:13.440 --> 00:17:20.320]   you have no choice but to just, you know, you can't do it. You have to tell or show. I mean,
[00:17:20.960 --> 00:17:26.160]   you've got to let people find their way because it's so big you can't, right? That's an obvious
[00:17:26.160 --> 00:17:34.160]   answer. Scope of work. Bigger it gets, okay, I've got to let more stuff go. But, I have always
[00:17:34.160 --> 00:17:43.760]   believed that a leader's job is to do as well. And I think there's like a few areas that are
[00:17:43.760 --> 00:17:49.360]   really important that you always do. Now, it doesn't mean you're showing. So, like when it has
[00:17:49.360 --> 00:17:54.800]   to do with values and value-based decisions, like I think it's really important to constantly
[00:17:54.800 --> 00:18:02.800]   show people that you walk your talk on that kind of thing. It's super important. And I actually
[00:18:02.800 --> 00:18:08.080]   think it's a struggle young companies have because the values aren't deeply rooted. And when a storm
[00:18:08.080 --> 00:18:15.520]   comes, it's easy to uproot. And so, I always felt like when it was that time, I showed it. I got
[00:18:15.520 --> 00:18:23.120]   taught that so young at IBM and even General Motors. In fact, I do write about that in the book.
[00:18:23.120 --> 00:18:30.480]   First time I was a manager, I had a gentleman telling dirty jokes. And not to me, but to other
[00:18:30.480 --> 00:18:37.840]   people. And it really offended people and some of the women. And this is the very early '80s.
[00:18:38.480 --> 00:18:45.600]   And they came, said something. I talked to my boss. I'm a first-time manager. And he was
[00:18:45.600 --> 00:18:50.880]   unequivocal with what I should do. He said, and this was a top performer, "It stops immediately
[00:18:50.880 --> 00:18:56.400]   or you fire him." So, there are a few areas like that that I actually think you have to always
[00:18:56.400 --> 00:19:02.480]   continue to role model and show. That to me isn't the kind that like when do you let go of stuff.
[00:19:02.480 --> 00:19:06.400]   The values and relationships with clients.
[00:19:06.400 --> 00:19:10.960]   Yeah, whatever you're in service of. And the other thing was, I really felt it was really
[00:19:10.960 --> 00:19:17.440]   important to role model learning. So, I can remember when we started down the journey
[00:19:17.440 --> 00:19:22.960]   and we went on to this thing called the Think Academy. IBM's longtime motto had been Think.
[00:19:22.960 --> 00:19:27.440]   And we said, "Okay, I'm going to make the first Friday of every month compulsory education."
[00:19:27.440 --> 00:19:33.200]   And, okay, I mean everybody. Like everybody, I don't care what your job is. When the whole
[00:19:33.200 --> 00:19:37.280]   company has to transform, everybody's got to kind of have some skin in this game and understand it.
[00:19:37.280 --> 00:19:43.600]   I taught the first hour of every month for four years. Now, okay, I had to learn something.
[00:19:43.600 --> 00:19:49.040]   But it made me learn. But I was like, "Okay, if I can teach this, you can do it." Right? I mean,
[00:19:49.040 --> 00:19:50.080]   you know, that kind of thing.
[00:19:50.080 --> 00:19:53.120]   So, it was a compulsory Thursday night education for you.
[00:19:53.120 --> 00:19:57.200]   I'm a little bit better prepared than that. But yes, you're so right. Yes.
[00:19:57.200 --> 00:19:58.080]   So, you prepare.
[00:19:58.080 --> 00:19:58.880]   Yeah.
[00:19:58.880 --> 00:20:01.840]   So, like personality-wise, you like to prepare?
[00:20:01.840 --> 00:20:06.240]   Yeah. But there's roots in that go back deeply, deeply, deeply, deeply. And I think
[00:20:06.240 --> 00:20:11.760]   it's an interesting reason. So, why do? You're prepared, my friend. Yes, you are. You prepare
[00:20:11.760 --> 00:20:12.480]   for your interviews.
[00:20:12.480 --> 00:20:15.040]   Uh, sure.
[00:20:15.040 --> 00:20:16.080]   The rest you wing?
[00:20:16.080 --> 00:20:16.800]   Yeah, I wing most of it.
[00:20:16.800 --> 00:20:19.920]   But that's okay. I mean, you don't have to prepare everything. I don't prepare everything
[00:20:19.920 --> 00:20:20.400]   either.
[00:20:20.400 --> 00:20:25.680]   No, but I unfortunately wing stuff. I save it to last minute. I push everything. I'm always
[00:20:25.680 --> 00:20:30.160]   almost late. And I don't know why that is. I mean, there's some deep psychological thing we
[00:20:30.160 --> 00:20:34.720]   should probably investigate. But it's probably the anxiety brings out the performance.
[00:20:34.720 --> 00:20:36.400]   That can be. That's very true with some people.
[00:20:36.400 --> 00:20:41.840]   I mean, so, I'm a programmer and engineer at heart. And so, programmers famously overestimate
[00:20:41.840 --> 00:20:47.520]   or underestimate, sorry, how long something's going to take. And so, I just, everything,
[00:20:47.520 --> 00:20:54.240]   always underestimate. And it's almost as if I want to feel this chaos of anxiety of a
[00:20:54.240 --> 00:20:57.840]   deadline or something like this. Otherwise, I'll be lazy sitting on a beach with a pina
[00:20:57.840 --> 00:20:58.960]   colada and relaxing.
[00:20:58.960 --> 00:20:59.280]   Yeah.
[00:20:59.280 --> 00:21:03.600]   I don't know. So, we have to know ourselves. But for you, you like to prepare.
[00:21:03.600 --> 00:21:08.640]   For me. Yeah. It came from a few different places. I mean, one would have been as a kid,
[00:21:08.640 --> 00:21:17.440]   I think, I was not a memorizer. And my brother is brilliant. He can read it once, boom, done.
[00:21:17.440 --> 00:21:21.040]   And so, I always wanted to understand how something happened. It didn't matter what
[00:21:21.040 --> 00:21:24.960]   it was I was doing. Whether it was algebra, theorems, I always wanted, don't give me the
[00:21:24.960 --> 00:21:27.440]   answer. Don't give me the answer. I want to figure it out, figure it out. So, I could
[00:21:27.440 --> 00:21:32.720]   reproduce it again and didn't have to memorize. So, it started with that. And then over time,
[00:21:32.720 --> 00:21:39.360]   okay, so I was in university in the 70s. When I was in engineering school, I was the only woman.
[00:21:39.360 --> 00:21:42.080]   You know, I meet people still to this day and they're like, "Oh, I remember you." I'm like,
[00:21:42.080 --> 00:21:47.360]   "Yeah, sorry, I don't remember you. There were 30 of you, one of me." And I think you already
[00:21:47.360 --> 00:21:50.640]   get that feeling of, okay, I better really study hard because whatever I say is going to be
[00:21:50.640 --> 00:21:58.160]   remembered in this class, good or bad. And it started there. So, in some ways, I did it for
[00:21:58.160 --> 00:22:04.720]   two reasons. Early on, I think it was a shield for confidence. The more I studied, the more prepared
[00:22:04.720 --> 00:22:10.480]   I was, the more confident. That's probably still true to this day. The second reason I did it
[00:22:10.480 --> 00:22:17.040]   evolved over time and became different to prepare. If I was really prepared, then when we're in the
[00:22:17.040 --> 00:22:21.920]   moment, I can really listen to you. See, because I don't have to be doing all this stuff on the fly
[00:22:21.920 --> 00:22:27.920]   in my head. And I could actually take things I know and maybe help the situation. So, it really
[00:22:27.920 --> 00:22:34.720]   became a way that I could be present in the moment. And I think it's something a lot of people
[00:22:34.720 --> 00:22:38.800]   that in the moment, I learned it from my husband. He doesn't prepare by the way at all. So, that's
[00:22:38.800 --> 00:22:43.040]   not it. But I watched the in the moment part. The negative example.
[00:22:43.040 --> 00:22:46.800]   No, no, no. And I'm not going to change that. As he says, he's a type C, I'm an A.
[00:22:46.800 --> 00:22:47.840]   Yes, of course. That's how love works.
[00:22:47.840 --> 00:22:53.440]   And I have been married 43 years and that seems to work. But that idea that you could be in the
[00:22:53.440 --> 00:22:58.560]   moment with people is a really important thing. Yeah. So, the preparation gives you the freedom
[00:22:58.560 --> 00:23:06.000]   to really be present. So, just to linger on, you mentioned your brother. And it seems like in the
[00:23:06.000 --> 00:23:13.360]   book that you really had to work hard when you studied to sort of, given that you weren't good
[00:23:13.360 --> 00:23:17.200]   at memorization, you really truly deeply wanted to understand the stuff and you put in the hard
[00:23:17.200 --> 00:23:23.120]   work. And that seems to persist throughout your career. So, hard work is often associated with,
[00:23:23.120 --> 00:23:29.200]   sort of has negative associations. Well, maybe with burnout, with dissatisfaction.
[00:23:29.200 --> 00:23:35.840]   Is there some aspect of hard work at the core of who you are that led to happiness for you?
[00:23:36.560 --> 00:23:38.000]   Did you enjoy it?
[00:23:38.000 --> 00:23:43.600]   I enjoyed it. So, I'll be the first. And I'm really careful to say that to people because
[00:23:43.600 --> 00:23:48.560]   I don't think everyone should associate, "Gee, to do what you did, there's only one route there."
[00:23:48.560 --> 00:23:53.840]   Right? And that's just not true. And I do it because I like it. In fact, I'm careful. And
[00:23:53.840 --> 00:23:57.760]   as time goes on, you have to be careful as more and more people watch you. Whether you like it,
[00:23:57.760 --> 00:24:01.440]   you're a role model or not. You are a role model for people. Whether you know it, like it, want it,
[00:24:01.440 --> 00:24:05.200]   does not matter. I learned that the hard way. And I would have to say to people, "Hey, just because
[00:24:05.200 --> 00:24:11.360]   I do this does not mean I do it for these reasons." Right? So, be really explicit. And I'd come to
[00:24:11.360 --> 00:24:14.960]   believe, usually when people say the word power, I don't know, do you have a positive or negative
[00:24:14.960 --> 00:24:18.080]   notion when I say the word power? We'll just do a- - Probably negative one, yeah.
[00:24:18.080 --> 00:24:23.840]   - For some stereotype or some view that somebody's abused it in some way. You can read the newspaper,
[00:24:23.840 --> 00:24:29.040]   somebody's doing something. Personal people, like I'll ask people, "Do you want power?" And they're
[00:24:29.040 --> 00:24:34.720]   like, "Oh no, I'd rather do good." And I think the irony is you need power to do good.
[00:24:35.280 --> 00:24:42.640]   And so, that sort of led me down to, as I thought about my own life, right? Because it starts in a,
[00:24:42.640 --> 00:24:48.720]   like many of us, you don't have a lot, but you don't know that because you're like everybody
[00:24:48.720 --> 00:24:54.240]   else around you at that time. And on one end, tragedy, right? My father leaves my mother,
[00:24:54.240 --> 00:25:00.080]   homeless, no money, no food, nothing, four kids. She's never worked a day in her life outside of
[00:25:00.080 --> 00:25:06.080]   a home. And the irony that I hear I would end up as the ninth CEO of one of America's iconic
[00:25:06.080 --> 00:25:11.680]   companies. And now I co-chair this group 110. And that journey, I said, "The biggest thing I learned
[00:25:11.680 --> 00:25:15.920]   was you could do really hard, meaningful things in a positive way." So now you ask me about why
[00:25:15.920 --> 00:25:22.720]   do I work so hard? I ended up writing the book in three pieces for this reason. When you really
[00:25:22.720 --> 00:25:28.880]   think of your life and power, I thought it kind of fell like a pebble in water. Like there's a
[00:25:28.880 --> 00:25:35.360]   ring about you really care about yourself and like the power of yourself, power of me. There's a time
[00:25:35.360 --> 00:25:39.680]   it transcends to that you are working with and for others and another moment when it becomes like
[00:25:39.680 --> 00:25:46.880]   about society. So my hard work, I'd ask you, one day sit really hard and think about when you close
[00:25:46.880 --> 00:25:52.080]   your eyes, who do you see from your early life, right? And what did you learn? And maybe it's not
[00:25:52.080 --> 00:25:59.680]   that hard for you. I mean, it's funny the things then, if I really looked at it, it's no surprise
[00:25:59.680 --> 00:26:05.280]   what I do today. And that hard work part, my great-grandma, as you and I were comparing notes
[00:26:05.280 --> 00:26:11.520]   on Russia, right? And never spoke English, spoke Russian, came here to this country, was a cleaning
[00:26:11.520 --> 00:26:17.120]   person at the Wrigley Building in Chicago. Yet if she hadn't saved every dime she made,
[00:26:17.120 --> 00:26:21.680]   my mother wouldn't have a home and wouldn't have had a car, right? What did I learn from that?
[00:26:22.000 --> 00:26:25.440]   Hard work. In fact, actually, when I went to college, she's like, "You know, you really should
[00:26:25.440 --> 00:26:31.520]   be on a farm. You're so big and strong." That was her view. And then my grandmother,
[00:26:31.520 --> 00:26:37.680]   another tragic life. What did she do though? And think how long, that's in the 40s, the 50s,
[00:26:37.680 --> 00:26:43.200]   she made lampshades. And she taught me how to sew, right? So I could sew clothes when we couldn't
[00:26:43.200 --> 00:26:50.720]   afford them. But my memory of my grandma is working seven days a week, sewing lampshades.
[00:26:51.360 --> 00:26:57.680]   And then here comes my mom and her situation who climbs her way out of it. So I associate that
[00:26:57.680 --> 00:27:07.040]   with, well, strong women, by the way, all strong women. And I associate hard work with how you are
[00:27:07.040 --> 00:27:11.840]   sure you can always take care of yourself. And so I think that the roots go way back there.
[00:27:11.840 --> 00:27:15.040]   And they were always teaching something, right? My great-grandma was teaching me how to cook,
[00:27:15.040 --> 00:27:19.440]   how to work a farm, even though I didn't need to be on a farm. My grandma taught me, you know,
[00:27:19.440 --> 00:27:23.600]   here's how to sew, here's how to run a business. And then my mother would teach us that,
[00:27:23.600 --> 00:27:27.040]   "Look, with just a little bit of education, look at the difference it could make." Right?
[00:27:27.040 --> 00:27:31.760]   So anyways, that's a long answer too. I think that hard work thing is really
[00:27:31.760 --> 00:27:36.000]   deeply rooted from that background. - And it gives you a way out from hard times.
[00:27:36.000 --> 00:27:39.520]   - Yeah. You know, I think I've seen you on other podcasts say,
[00:27:39.520 --> 00:27:46.240]   "I thought I did. Do you want a plan B?" Didn't you say, "No, you would not like a plan B?"
[00:27:46.240 --> 00:27:47.760]   - Yeah, I don't want a plan B. - Because you're like,
[00:27:47.760 --> 00:27:50.960]   "I would prefer my backup against, am I remembering?" - You have a story like that.
[00:27:50.960 --> 00:27:59.520]   You seem to have, at least certain moments in your life, seem to do well in desperate times.
[00:27:59.520 --> 00:28:07.440]   - True enough. True enough, that's true. I learned that very well. But I also think that maybe this
[00:28:07.440 --> 00:28:12.400]   isn't the same kind of plan B. I think of it as, like I was taught, always be able to take care of
[00:28:12.400 --> 00:28:18.960]   yourself. Don't have to rely on someone else. And I think that to me, so that's my plan B,
[00:28:18.960 --> 00:28:24.240]   I can take care of myself. And it's even after what I lived through with my father, I thought,
[00:28:24.240 --> 00:28:30.400]   "Well, this is at a bar for bad. After this, nothing's bad." And that is a very freeing thought.
[00:28:30.400 --> 00:28:35.280]   - The being able to take care of yourself, is that, you mean practically, or do you mean just
[00:28:35.280 --> 00:28:39.040]   a self-belief that I'll figure it out? - I'll figure it out and practically both,
[00:28:39.040 --> 00:28:40.720]   right? - So you wrote,
[00:28:40.720 --> 00:28:48.160]   "I vividly remember the last two weeks of my freshman year when I only had 25 cents left.
[00:28:48.160 --> 00:28:54.640]   I put the quarter in a clear plastic box on my desk and just stared at it. This is it, I thought,
[00:28:54.640 --> 00:29:02.560]   no more money." So do you think there's some aspect of that financial stress, even desperation,
[00:29:02.560 --> 00:29:09.840]   just being hungry, does that play a role in that drive that led to your success to be the CEO
[00:29:09.840 --> 00:29:12.800]   of one of the great companies ever? - It's a really interesting question
[00:29:12.800 --> 00:29:17.520]   because I was just talking to another colleague who's CEO of another great American company this
[00:29:17.520 --> 00:29:24.160]   weekend. And he mentioned to me about all this adversity and he said, or I said to him, I said,
[00:29:24.160 --> 00:29:28.960]   "Do you think part of your success is 'cause you had bad stuff happen?"
[00:29:30.400 --> 00:29:38.400]   And he said, "Yes." And so I guess I'd be lying if I didn't say, I don't think you have to have
[00:29:38.400 --> 00:29:43.840]   tragedy, but it does teach you one really important thing is that there is always a way forward,
[00:29:43.840 --> 00:29:47.440]   always, and it's in your control. - And I think there's probably wisdom
[00:29:47.440 --> 00:29:55.120]   for mentorship there, or whether you're a parent or a mentor, that easy times don't result in growth.
[00:29:55.120 --> 00:29:59.040]   - Yeah, I've heard a lot of my friends and they worry, they say, "Gee, my kids have never had
[00:29:59.040 --> 00:30:06.320]   bad times." And so what happens here? So I don't know, is it required? And why you end up,
[00:30:06.320 --> 00:30:11.760]   not required, but it sure doesn't hurt. - You had this good line about advice you
[00:30:11.760 --> 00:30:17.280]   were given that growth and comfort never coexist, growth and comfort never coexist.
[00:30:17.280 --> 00:30:21.040]   And you have to get used to that thought. - If someone said that they think of me like
[00:30:21.040 --> 00:30:27.200]   one of the more profound sort of lessons I had, and the irony is, it's from my husband,
[00:30:28.240 --> 00:30:32.000]   which is even more funny, actually. - I'm glad you're able to, you could just steal it.
[00:30:32.000 --> 00:30:35.200]   I mean, you don't have to give him credit. - Oh, I have, I have, shamelessly, as he'll tell you.
[00:30:35.200 --> 00:30:42.240]   Okay, so the story behind growth and comfort never coexist, but honestly, I think it's been
[00:30:42.240 --> 00:30:49.680]   a really freeing thought for me, and it's helped me immensely since. Mid-career, and as I write
[00:30:49.680 --> 00:30:55.520]   about it in the book, I'm mid-career, and I'd been running a pretty big business, actually,
[00:30:55.520 --> 00:30:58.880]   and the fella I work for is gonna get a new job, he's gonna get promoted. He calls me and he says,
[00:30:58.880 --> 00:31:04.400]   "Hey, you're gonna get my job. I really want you to have it." And I said to him, "No way." I said,
[00:31:04.400 --> 00:31:09.200]   "I'm not ready for that job. I got a lot more things I gotta learn. That is like a huge job,
[00:31:09.200 --> 00:31:14.160]   round the world, every product line, development, you name it, every function, I can't do it."
[00:31:14.160 --> 00:31:20.640]   He looked at me, he says, "Well, I think you should go to the interview." I went to the interview the
[00:31:20.640 --> 00:31:25.680]   next day, blah, blah, blah. Guy says to me, looks at me and he says, "I wanna offer you that job."
[00:31:25.680 --> 00:31:31.840]   And I said, "I would like to think about it." I said, "I wanna go home and talk to my husband
[00:31:31.840 --> 00:31:38.560]   about it." He kinda looked at me, "Okay." I went home, my husband is sitting there and he says to
[00:31:38.560 --> 00:31:44.640]   me, I went on and on about the story, et cetera, and he says, "Do you think a man would've answered
[00:31:44.640 --> 00:31:51.360]   it that way?" And I said, "Hmm." He says, "I know you." He's like, "Six months, you're gonna be
[00:31:51.360 --> 00:31:56.160]   bored. And all you can think of is what you don't know." And he said, "And I know these other
[00:31:56.160 --> 00:32:01.920]   people. You have way more skill than them and they think they could do it." And he's like, "Why?"
[00:32:01.920 --> 00:32:10.000]   And for me, it internalized this feeling that, and I am gonna say something that's a bit stereotyped,
[00:32:10.000 --> 00:32:15.600]   that it resonates with many, many women, and I'll ask you if it does after, is that they're the most
[00:32:15.600 --> 00:32:21.840]   harsh critic of themselves. And so this idea that I won't grow unless I can feel uncomfortable,
[00:32:21.840 --> 00:32:26.000]   doesn't mean I always have to show it, by the way. So that's why I meant growth and comfort can never
[00:32:26.000 --> 00:32:32.320]   coexist. So I was like, "He's exactly right." Now, the end of that story is I went in and I took the
[00:32:32.320 --> 00:32:37.760]   job. When I went back to the man who was really my mentor looking out for me, and he looked at me and
[00:32:37.760 --> 00:32:43.360]   he said, "Don't ever do that again." And I said, "I understand." Because it was okay to be
[00:32:43.360 --> 00:32:48.960]   uncomfortable. I didn't have to use it. I mean, now I would take stock of the things I can do,
[00:32:48.960 --> 00:32:55.360]   right? And really think, or I look for times to be uncomfortable. Because I know if I am nervous,
[00:32:55.360 --> 00:32:58.960]   like, I don't know if you're nervous to meet me. We never met in real person.
[00:32:58.960 --> 00:33:00.720]   - 100%. I'm still terrified.
[00:33:00.720 --> 00:33:04.160]   - No, you're not. But then you're, it means you're learning something, right?
[00:33:04.160 --> 00:33:04.880]   - Holding it together.
[00:33:04.880 --> 00:33:07.520]   - So that to me matters.
[00:33:07.520 --> 00:33:12.560]   - I think it's interesting. Maybe you could speak to that, the sort of the self-critical
[00:33:12.560 --> 00:33:21.040]   thing inside your brain. Because I think sometimes it's talked about that women have that.
[00:33:21.040 --> 00:33:28.480]   But I have that, definitely. And I think that's not just solely a property of women in the workplace.
[00:33:28.480 --> 00:33:28.800]   - No, I don't think it is either.
[00:33:28.800 --> 00:33:33.360]   - But I also want to sort of push back on the idea that that's a bad thing that you should
[00:33:33.360 --> 00:33:39.760]   silence. Because I think that anxiety, that leads to growth also. That's like this discomfort. So
[00:33:39.760 --> 00:33:44.080]   there's this weird balance you have to have between that self-critical engine and confidence.
[00:33:44.080 --> 00:33:45.440]   - Yeah, I think that's a good point.
[00:33:45.440 --> 00:33:49.200]   - You have to kind of dance. Because if you're super confident, people will value you higher.
[00:33:49.200 --> 00:33:53.120]   That's important. But if you're way too confident, maybe in the short term you'll gain,
[00:33:53.120 --> 00:33:54.400]   but in the long term you won't grow.
[00:33:54.400 --> 00:33:59.520]   - Very good point. So I can't really disagree with that. And to me, even when I took on jobs,
[00:34:00.160 --> 00:34:05.200]   I always felt people say, "Well, what point are you confident enough?" And I came to sort of
[00:34:05.200 --> 00:34:10.640]   believe, again, a theme of my beliefs that if I was willing to ask lots of questions and understood
[00:34:10.640 --> 00:34:15.760]   enough, that's all I needed to know. - Let me ask you about your husband
[00:34:15.760 --> 00:34:16.800]   a little bit. - Oh.
[00:34:16.800 --> 00:34:21.440]   - So you write in the book. You write in the book. He's just jumping around. Like I said,
[00:34:21.440 --> 00:34:23.920]   I'm a bit of a romantic. So how did you meet your husband?
[00:34:23.920 --> 00:34:32.160]   - So I met my husband when I was 19 years old. So I was a young kid. And I met him when I had a
[00:34:32.160 --> 00:34:37.600]   General Motors scholarship. So I was at Northwestern University through my first two years,
[00:34:37.600 --> 00:34:43.440]   had a lot of loans, financial aid. And a professor said, "Hey, you should sign up for this
[00:34:43.440 --> 00:34:47.920]   interview. They're looking to bring forward diverse candidates through their management track.
[00:34:47.920 --> 00:34:51.360]   Now, these programs don't exist anymore like that. They will pay your tuition,
[00:34:51.360 --> 00:34:55.520]   your room and board, your expenses at Northwestern, other Ivy League schools,
[00:34:55.520 --> 00:35:01.680]   these very expensive schools. And I think you'd be a good fit." I am eternally thankful for that
[00:35:01.680 --> 00:35:06.080]   advice. I went and I interviewed. I actually got the scholarship. I mean, without it, I'd have
[00:35:06.080 --> 00:35:11.600]   graduated with hundreds of thousands of dollars of debt. So part of that was in the summer,
[00:35:11.600 --> 00:35:16.720]   I had to work in Detroit. I lived a little room by a cement plant. Not theirs, but I mean,
[00:35:16.720 --> 00:35:18.320]   that's all I could afford. - Very romantic.
[00:35:18.320 --> 00:35:22.960]   - Very, very romantic. And the person who owned the house said, "Hey, I'm having a party. You're
[00:35:22.960 --> 00:35:28.480]   not invited. I'm going to fix you up with someone tonight." And that turned out to be my husband.
[00:35:28.480 --> 00:35:31.680]   And so it was a blind date is how we very first met.
[00:35:31.680 --> 00:35:35.200]   - And then it was over. The story was written. - Yep.
[00:35:35.200 --> 00:35:42.240]   - If it's okay, just zoom out to, you mentioned power and good power a few times. So if we can
[00:35:42.240 --> 00:35:46.880]   just even talk about it. Your book is called "Good Power, Leading Positive Change in Our Lives,
[00:35:46.880 --> 00:35:50.640]   Work, and World." What is good power? What's the essence of good power?
[00:35:50.640 --> 00:35:57.520]   - Yeah. So the essence of it would be doing something hard or meaningful, but in a positive
[00:35:57.520 --> 00:36:05.920]   way. I would also tell you, I hope one day I'm remembered for how I did things, not just for
[00:36:05.920 --> 00:36:09.600]   what I did. I think that could almost be more important. And I think it's a choice we can all
[00:36:09.600 --> 00:36:15.360]   make. So the essence to me of good power, if I had a contrast, good to bad, let's say, would be that
[00:36:15.360 --> 00:36:22.640]   first off, you have to embrace and navigate tension. This is the world we live in. And
[00:36:22.640 --> 00:36:28.720]   by embracing tension, not running from it, you would bridge divides that unites people,
[00:36:28.720 --> 00:36:33.280]   not divides them. It's a hard thing to do, but you can do it. You do it with respect,
[00:36:33.280 --> 00:36:37.440]   which is the opposite of fear. A lot of people think the way to get things done is fear.
[00:36:37.440 --> 00:36:43.200]   And then the third thing would be, you got to celebrate some progress versus perfection.
[00:36:44.000 --> 00:36:48.320]   Because I also think that's what stops a lot of things from happening. Because if you go for
[00:36:48.320 --> 00:36:54.480]   whatever your definition of perfect is, it's either polarization or paralyzation. I mean,
[00:36:54.480 --> 00:37:01.040]   something happens in there versus no, no, no. Don't worry about getting to that actual exact
[00:37:01.040 --> 00:37:06.160]   endpoint. If I keep taking a step forward of progress, really tough stuff can get done.
[00:37:06.160 --> 00:37:12.320]   And so my view of that is like, honestly, I hope it can, I said it's like a memoir with purpose.
[00:37:12.320 --> 00:37:17.360]   I'm only doing it. It was a really hard thing for me to do because I don't actually talk about all
[00:37:17.360 --> 00:37:22.000]   these things. And I had to, nobody cares about your like scientific description of this. They
[00:37:22.000 --> 00:37:27.200]   want the stories in your life to bring it alive. So it's a memoir with purpose. And in the writing
[00:37:27.200 --> 00:37:33.120]   of it, it became the power of me, the power of we, and the power of us. The idea that you build a
[00:37:33.120 --> 00:37:40.000]   foundation when you're young, mostly from my work life, the power of we, which says, I kind of,
[00:37:40.560 --> 00:37:47.200]   in retrospect, could see five principles on how to really drive change that would be done in a
[00:37:47.200 --> 00:37:52.880]   good way. And then eventually you could scale that, the power really of us, which is what I'm
[00:37:52.880 --> 00:37:58.320]   doing about finding better jobs for more people now that I co-chair an organization called 110.
[00:37:58.320 --> 00:38:05.840]   So that essence of navigate tensions, do it respectfully, celebrate progress,
[00:38:06.800 --> 00:38:14.160]   and indulge me one more minute, these sort of, again, it's retrospect that I didn't know this
[00:38:14.160 --> 00:38:18.480]   in the moment. I had to learn it. I learned it. I am blessed by a lot of people I worked with and
[00:38:18.480 --> 00:38:27.280]   around. But some of the principles, like the first one says, if you're going to do something,
[00:38:27.280 --> 00:38:34.000]   change something, do something, you got to be in service of something. Being in service of
[00:38:34.000 --> 00:38:39.680]   is really different than serving, super different. And like, I just had my knee replaced
[00:38:39.680 --> 00:38:44.800]   and I interviewed all these doctors. You can tell the difference that the guy who's going to do a
[00:38:44.800 --> 00:38:47.680]   surgery, hey, my surgery is fine. I really don't care whether you can walk and do the stuff you
[00:38:47.680 --> 00:38:52.240]   wanted to do again, but because my surgery is fine. Your hardware is good. I actually had some
[00:38:52.240 --> 00:38:56.720]   trouble. And I had a doctor who was like, you know, this doesn't sound right. I'm coming to you.
[00:38:56.720 --> 00:39:03.120]   The surgery was fine. It was me that was reacting wrong to it. And he didn't care until I could
[00:39:03.120 --> 00:39:07.680]   walk again. Okay. There's a big difference in those two things. And it's true in any business
[00:39:07.680 --> 00:39:13.360]   you have. A waiter serves your food. Okay. He serves his food. He did his job. Or did he care
[00:39:13.360 --> 00:39:18.000]   he had a good time? So that thought to be in service of, it took me a while to get that,
[00:39:18.000 --> 00:39:21.680]   like to try to write it, to get that across. Cause I think it's like so fundamental.
[00:39:21.680 --> 00:39:28.800]   If people were really in service of something, you got to believe that if I fulfill your needs
[00:39:28.800 --> 00:39:35.280]   at the end of the day, mine will be fulfilled. And that is that essence that makes it so different.
[00:39:35.280 --> 00:39:40.320]   And then the second part, second principle is about building belief, which is I got to hope
[00:39:40.320 --> 00:39:45.200]   you'll voluntarily believe in a new future or some alternate reality. And you will use your
[00:39:45.200 --> 00:39:50.880]   discretionary energy versus me ordering you. You'll get so much more done. Then the third
[00:39:50.880 --> 00:39:55.920]   change and endure. We kind of talked about that earlier, focus more on the how and the skills.
[00:39:56.800 --> 00:40:03.360]   And then the part on good tech and being resilient. So anyways, I just felt that
[00:40:03.360 --> 00:40:06.880]   like good tech, everybody's a tech company. I don't care what you do today.
[00:40:06.880 --> 00:40:12.400]   And there's some fundamental things you got to do. In fact, pick up today's, any newspaper, right?
[00:40:12.400 --> 00:40:18.720]   Chat GPT. You're an AI guy. All right. I believe one of the tenants of good tech is,
[00:40:18.720 --> 00:40:23.280]   it's like responsibility for the longterm. It says, so if you're going to invent something,
[00:40:23.280 --> 00:40:27.120]   you better look at its upside and its downside. Like we did quantum computing.
[00:40:27.120 --> 00:40:33.120]   Great. A lot of great stuff, right? Materials development, risk management calculations,
[00:40:33.120 --> 00:40:38.080]   endless lists one day. On the other side, it can break encryption. That's a bad thing.
[00:40:38.080 --> 00:40:44.960]   So we work equally hard on all the algorithms that would sustain quantum. I think with chat,
[00:40:44.960 --> 00:40:50.880]   okay, great. There's equal in, there are people working on it, but like, okay,
[00:40:50.880 --> 00:40:56.320]   the things that say, hey, I can tell this was written with that, right? Because the implications
[00:40:56.320 --> 00:41:00.160]   on how people learn, right? If this is not a great thing, if all it does is do your homework,
[00:41:00.160 --> 00:41:05.200]   that is not the idea of homework as someone who liked to study so hard. But anyways, you get my
[00:41:05.200 --> 00:41:08.800]   point. It's just the upside and the downside. And that there could be a much larger implications
[00:41:08.800 --> 00:41:14.560]   that are much more difficult to predict. And it's our responsibility to really work hard to figure
[00:41:14.560 --> 00:41:18.400]   that out. I was talking to AI ethics a decade ago, and I'm like, why won't anybody listen to us?
[00:41:20.480 --> 00:41:24.480]   That's another one of those values things that you realize, hey, if I'm going to bring technology in
[00:41:24.480 --> 00:41:30.320]   the world, I better bring it safely. And that to me comes with when you're an older company that's
[00:41:30.320 --> 00:41:34.880]   been around, you realize that society gave you a license to operate and it can take it away.
[00:41:34.880 --> 00:41:41.200]   And we see that happen to companies. And therefore you're like, okay, like why I feel so strong about
[00:41:41.200 --> 00:41:45.920]   skills. Hey, if I'm going to bring in, it's going to create all these new jobs, job dislocation,
[00:41:45.920 --> 00:41:50.560]   then I should help on trying to help people get new skills. Anyways, that's a long answer to what
[00:41:50.560 --> 00:41:57.040]   good tech, but the idea that there's kind of in retrospect, a set of principles you could look at
[00:41:57.040 --> 00:42:01.040]   and maybe learn something from my sort of rocky road through there.
[00:42:01.040 --> 00:42:06.960]   But it started with the power of we, and there's that big leap, I think that propagates through
[00:42:06.960 --> 00:42:11.360]   the things you're saying, which is the leap from focusing on yourself to the focusing on others. So
[00:42:11.360 --> 00:42:16.720]   that having that empathy, you've said at some point in our lives and careers, our attention
[00:42:16.720 --> 00:42:22.000]   turns from ourselves to others. We still have our own goals, but we recognize that our actions
[00:42:22.000 --> 00:42:28.640]   affect many, that it is impossible to achieve anything truly meaningful alone. So it's to you,
[00:42:28.640 --> 00:42:36.480]   I think maybe you can correct me, but ultimate good power is about collaboration. And maybe
[00:42:38.080 --> 00:42:42.240]   in large companies, like delegation on great teams.
[00:42:42.240 --> 00:42:45.520]   The ultimate good power is actually doing something for society. That would be my
[00:42:45.520 --> 00:42:46.480]   ultimate definition of good power, by the way.
[00:42:46.480 --> 00:42:48.080]   So it's about the results of the thing.
[00:42:48.080 --> 00:42:56.800]   Yeah, but how it's done, right? The how it's done. And so, you know, when you said a leap,
[00:42:56.800 --> 00:43:00.480]   do you think people make a leap when they go from thinking about themselves to others?
[00:43:00.480 --> 00:43:04.560]   Do you think it's a leap or do you think it kind of just is a sort of slow point?
[00:43:04.560 --> 00:43:13.040]   I think the leap is in deciding that this is, it's like deciding that you will care about others.
[00:43:13.040 --> 00:43:17.840]   That this is, it's like a leap of going to the gym for the first time. Yes, it takes a long time
[00:43:17.840 --> 00:43:22.000]   to develop that and to actually care, but that decision that I'm going to actually care about
[00:43:22.000 --> 00:43:28.160]   other human beings. Yeah. I think, or at least, like, yeah, it just feels like a deliberate action
[00:43:28.160 --> 00:43:29.280]   you take of empathy.
[00:43:29.280 --> 00:43:32.720]   Yeah, because sometimes I think it happens a little, it's maybe not as deliberate. Yeah,
[00:43:32.720 --> 00:43:36.320]   it's a little bit more gradual because it might happen because you realize that, geez,
[00:43:36.320 --> 00:43:40.240]   I can't get this done alone. So I got to have other people with me. Well, how do I get them
[00:43:40.240 --> 00:43:45.680]   to help me do something? So I think it does help happen a little bit more gradually. And as you
[00:43:45.680 --> 00:43:50.400]   get more confident, you start to not think so much that it's about you. And you start to think
[00:43:50.400 --> 00:43:54.800]   about this other thing you're trying to accomplish. And so that's why I felt it was a little more
[00:43:55.680 --> 00:44:04.960]   gradual. I also felt like I can remember so well, you know, this idea that, again, now we're in the
[00:44:04.960 --> 00:44:11.920]   80s, 90s, I'm a woman, I'm in technology. And I was down in Australia at a conference.
[00:44:11.920 --> 00:44:17.280]   And I gave this great speech, again, me, power of me, you know, I'm thinking I give this great
[00:44:17.280 --> 00:44:21.200]   speech, financial services, this guy, man walks up to me after I think he's going to like ask me
[00:44:21.200 --> 00:44:27.600]   some great question. And he said to me, I wish my daughter could have been here. And in that moment,
[00:44:27.600 --> 00:44:33.200]   and I in at that point, up to then, I'd always been about, look, please don't notice I'm a
[00:44:33.200 --> 00:44:39.440]   woman, do not notice that I am, I just want to be recognized for my work. Crossing over from me to
[00:44:39.440 --> 00:44:46.320]   we, like it or not, I was a role model for some number of people. And maybe I didn't want to be,
[00:44:46.320 --> 00:44:51.120]   but that didn't really matter. So I could either accept that and embrace it or not. I think it's
[00:44:51.120 --> 00:44:55.840]   a good example of that transition. I did have a little epiphany with that happening. And then I'm
[00:44:55.840 --> 00:44:59.680]   like, okay, because I would always be like, no, I won't go on a women's conference. I won't talk
[00:44:59.680 --> 00:45:04.560]   here. I won't, you know, no, no, no. But then I sort of realized, wait a second, you know,
[00:45:04.560 --> 00:45:11.280]   that old saying, you cannot be what you cannot see. And I said to myself, well, oh, wait a second.
[00:45:11.280 --> 00:45:17.440]   Okay. I am in these positions I have a responsibility to, and it's to others. And
[00:45:17.440 --> 00:45:20.720]   that's what I meant. I felt like it can be somewhat gradual that you come and you may
[00:45:20.720 --> 00:45:25.840]   have these like pivotal moments that you see it, but then you feel it and you sort of move over
[00:45:25.840 --> 00:45:31.760]   that transom into the power of we. - You're one of the most powerful tech leaders ever.
[00:45:31.760 --> 00:45:37.120]   And as you mentioned the word power, you know, the old saying goes, power corrupts and absolute
[00:45:37.120 --> 00:45:46.080]   power corrupts. Absolutely. Was there an aspect of power that was, that you had to resist
[00:45:48.000 --> 00:45:55.040]   its ability to corrupt your mind to sort of delude you into thinking you're smarter than you are,
[00:45:55.040 --> 00:45:58.320]   that kind of, all the ways. - That's very dangerous. I agree with, I mean,
[00:45:58.320 --> 00:46:02.240]   I think you got to be careful who you surround yourself with. That's how I would answer that
[00:46:02.240 --> 00:46:06.240]   question. Right. And people who will hold the mirror up to you can be done in a very positive
[00:46:06.240 --> 00:46:11.120]   way, by the way, it doesn't mean, you know, but that we're sycophants, you cannot have that.
[00:46:11.120 --> 00:46:15.040]   Right. I mean, it's like, I always say to someone like, hey, listen to me, tell me,
[00:46:15.040 --> 00:46:19.360]   I mean, tell me what would make me better or do something. Or I have a husband that'll do that for
[00:46:19.360 --> 00:46:23.600]   me quite easily, by the way. He'll always tell me. - He's the one that kind of gives you some
[00:46:23.600 --> 00:46:26.880]   criticism sometimes. - I have been surrounded myself with a number of people that will do that.
[00:46:26.880 --> 00:46:32.160]   And I think you have to have that. I had a woman that worked with me for a very long time. And
[00:46:32.160 --> 00:46:36.880]   at one time we were competitors. And then at some point she started to work for me and stayed with
[00:46:36.880 --> 00:46:40.800]   me for quite a while. And she was one of the few people that would tell me the truth in, you know,
[00:46:40.800 --> 00:46:46.960]   sometimes I'm like, enough already. And she'd be like, do not roll your eyes at this. And you
[00:46:46.960 --> 00:46:51.600]   absolutely have got to have that. And I think it also comes, it'll go back to my complete
[00:46:51.600 --> 00:46:56.400]   commitment to inclusion and diversity, 'cause you gotta have that variety around you. You'll get a
[00:46:56.400 --> 00:47:01.920]   better product and a better answer at the end of the day. And so that, to resist that allure,
[00:47:01.920 --> 00:47:06.240]   I think it's around about who you surround yourself with. I, current politics would say that too.
[00:47:06.240 --> 00:47:10.720]   - So you, you write about, and in general you value diversity a lot. So
[00:47:10.720 --> 00:47:14.800]   can you speak to almost like philosophically, what does diversity mean to you?
[00:47:14.800 --> 00:47:22.960]   - Diversity to me means I'm gonna get a better product, a better answer. I value different views.
[00:47:22.960 --> 00:47:29.280]   And so it's inclusion. So I always say inclusion, diversity is a number, inclusion is a choice.
[00:47:29.280 --> 00:47:33.920]   And you can make that choice every single day. - That's a good line.
[00:47:33.920 --> 00:47:39.120]   - I really believe, and I've witnessed it, that I've had, when my teams are diverse,
[00:47:39.120 --> 00:47:43.760]   I get a better answer. My friends are diverse, I have a better life. I mean, all these kinds of
[00:47:43.760 --> 00:47:49.600]   things. And so I also believe it's like no silver thread, there's no easy way. You have to
[00:47:49.600 --> 00:47:54.320]   authentically believe it. I mean, do you authentically believe that diversity is a good thing?
[00:47:54.320 --> 00:47:59.840]   - Yeah, but I believe that diversity, like broadly-- - A thought, I very broadly define it.
[00:47:59.840 --> 00:48:05.520]   - Yeah, so like there's, sometimes the way diversity is looked at, the way diversity is
[00:48:05.520 --> 00:48:10.160]   used today is like surface level characteristics, which are also important, but they're usually
[00:48:10.160 --> 00:48:14.960]   reflective of something else, which is a diversity of background, a diversity of thought, a diversity
[00:48:14.960 --> 00:48:22.240]   of struggle. Some people that grew up middle-class versus poor, different countries, different
[00:48:22.240 --> 00:48:26.880]   motivations, all of that. Yeah, it's beautiful when different people from very different walks
[00:48:26.880 --> 00:48:31.440]   of life get together. Yeah, it's beautiful to see. But sometimes it's very difficult to get
[00:48:31.440 --> 00:48:37.040]   at that on a sheet of paper of the characteristics that defines the diversity.
[00:48:37.040 --> 00:48:41.040]   - I know, so it is. It's just like, oh, I can't hire exactly for, or if I'm trying to,
[00:48:41.040 --> 00:48:46.560]   but I do know one thing, that when people say, well, I can't find these kind of people I'm
[00:48:46.560 --> 00:48:48.400]   looking for, I'm like, you're just not looking in the right places.
[00:48:48.400 --> 00:48:50.800]   - Right, you have to open up-- - You gotta really open up new pools.
[00:48:50.800 --> 00:48:55.360]   - Yeah, you have to think, like everybody, you don't have to have a PhD, just like you said.
[00:48:55.360 --> 00:48:59.280]   - I'm sorry to say it. I know it's very valuable what you have, trust me, but--
[00:48:59.280 --> 00:49:04.000]   - Well, just like you said, it could even be a negative. So you mentioned,
[00:49:04.000 --> 00:49:10.480]   like for good power, you are a CEO, you were a CEO for a long time of a public company.
[00:49:10.480 --> 00:49:16.240]   Were there times when there was pressure to sacrifice what is good for the world
[00:49:16.240 --> 00:49:20.960]   for the bottom line in order to do what's good for the company?
[00:49:20.960 --> 00:49:25.680]   - There were a lot of times for that. I mean, I think every company faces that today in that
[00:49:25.680 --> 00:49:31.040]   I always felt like there's so much discussion about stakeholder capitalism, right? Do you just
[00:49:31.040 --> 00:49:36.000]   serve a shareholder or do you have multiple? I have always found, and I've been very vocal about
[00:49:36.000 --> 00:49:40.960]   that topic, that when I participated, the Business Roundtable wrote up a new purpose statement that
[00:49:40.960 --> 00:49:45.920]   had multiple stakeholders. I think it's common sense. Like if you're gonna be 100 years old,
[00:49:45.920 --> 00:49:50.800]   you only get there because you actually do at some time balance all these different stakeholders
[00:49:50.800 --> 00:49:55.520]   in what it is that you do, and short-term, long-term, all these trade-offs. And I always say
[00:49:55.520 --> 00:49:59.360]   people who write about it, they write about it black and white, but I have to live in a gray
[00:49:59.360 --> 00:50:04.080]   world. Nothing I've ever done has been in a black and white world, hardly. Maybe things of values
[00:50:04.080 --> 00:50:10.480]   that I had to answer, but most of it is gray. And so I think back lots of different decisions.
[00:50:10.480 --> 00:50:17.120]   I think back, as you would well remember, you're a student of history. IBM was one of the really the
[00:50:17.120 --> 00:50:21.440]   originators of the semiconductor industry, and certainly of commercializing the semiconductor
[00:50:21.440 --> 00:50:28.640]   industry. Great R&D and manufacturing, but it is a game of volume. And so when I came on,
[00:50:28.640 --> 00:50:35.440]   we were still manufacturing R&D and manufacturing our own chips. We were losing a lot of money,
[00:50:35.440 --> 00:50:41.200]   yet here we had to fight a war on cloud and AI. And so, okay, now shareholders would say,
[00:50:41.200 --> 00:50:46.320]   "Fine, shut it down." Okay, those chips also power some of the most important systems that power
[00:50:46.320 --> 00:50:52.480]   these, the banks of today. If I just shut it down, well, what would that do? And so, okay,
[00:50:52.480 --> 00:50:58.400]   the answer wasn't just stop it. The answer wasn't just keep putting money into it. The answer was,
[00:50:58.400 --> 00:51:02.720]   and we had to kind of sit in an uncomfortable spot till we found a way. I mean, it's going to sound
[00:51:02.720 --> 00:51:09.680]   so basic, but you as an engineer understand it. We had to separate. It was a very integrated process
[00:51:09.680 --> 00:51:13.440]   of research, development, and manufacturing. And you'd also, you'd be perfecting things in
[00:51:13.440 --> 00:51:19.280]   manufacturing. And these were very high performance chips. We had to be able to separate those. We
[00:51:19.280 --> 00:51:24.960]   eventually found a way to do that so that we could take the manufacturing elsewhere and we would
[00:51:24.960 --> 00:51:31.280]   maintain the R&D. I think it's a great example of the question you just asked, because people
[00:51:31.280 --> 00:51:37.280]   would have applauded, others would have been, "This is horrible." Or we had a financial roadmap
[00:51:37.280 --> 00:51:41.520]   that had been put in place that said, "I'll make this amount of EPS by this date." There came a
[00:51:41.520 --> 00:51:47.760]   time we couldn't honor it because we had to invest. And so, there's a million of these decisions. I
[00:51:47.760 --> 00:51:53.760]   think most people that run firms, any size firm, they're just one right after another like that.
[00:51:53.760 --> 00:51:58.160]   And you're always making that short and long tension of, "What am I giving up?"
[00:51:58.160 --> 00:52:02.480]   What is that partnership like with the clients? Because you work with gigantic businesses.
[00:52:02.480 --> 00:52:11.200]   And what's it like sort of really forming a great relationship with them, understanding
[00:52:11.200 --> 00:52:14.320]   what their needs are, being in service of their needs?
[00:52:14.320 --> 00:52:22.400]   Yeah. Very simple. Honor your promises. And that happens over time. I mean, in service of,
[00:52:22.400 --> 00:52:27.360]   which is often why you can work with competitors, because if you are really in service of you and
[00:52:27.360 --> 00:52:32.240]   you need something, it takes two of us to do it, that becomes easier to do. Because I really,
[00:52:32.240 --> 00:52:38.960]   we both care, you get what you needed. And so, I can remember during one of the times I was on a
[00:52:38.960 --> 00:52:46.800]   European trip and at the time, a lot of, and this is still true, about views about technology and
[00:52:46.800 --> 00:52:51.600]   national technology giants and global ones and the pros and the cons. And countries want their
[00:52:51.600 --> 00:52:56.560]   own national champions, quite obvious. I mean, if I'm France or Germany. And there was a lot
[00:52:56.560 --> 00:53:01.600]   of discussion about security and data and who was getting access to what. And I can remember being
[00:53:01.600 --> 00:53:06.560]   in one of the, I was with Chancellor Merkel, I had met her many times. She's very well prepared,
[00:53:06.560 --> 00:53:12.240]   very well prepared every time, as you would know. And I started to explain all these things about
[00:53:12.240 --> 00:53:18.080]   why, how, you know, how we don't share data, how, who it belongs to. Our systems never had back
[00:53:18.080 --> 00:53:24.480]   doors. And she sort of stopped me. Like, you're one of the good guys. Like, stop. Now, that wasn't
[00:53:24.480 --> 00:53:30.080]   about me personally. She's talking about a company that's acted consistent with values for decades,
[00:53:30.080 --> 00:53:37.200]   right? So to me, how you work with those big kind of clients is you honor your promises. You say
[00:53:37.200 --> 00:53:42.080]   what you do and you do what you say. And you act with values over a long period of time. And that,
[00:53:42.080 --> 00:53:46.560]   to me, people say we're valued. It is not a fluffy thing. It is not a fluffy thing. It is a,
[00:53:46.560 --> 00:53:53.520]   I mean, if I was starting a company now, I'd spend a lot of time on that. On,
[00:53:53.520 --> 00:53:58.800]   you know, why we do what we do and why some things are tolerable and something,
[00:53:58.800 --> 00:54:03.200]   you know, what's your fundamental beliefs are. And many people sort of zoom past that stage,
[00:54:03.200 --> 00:54:07.040]   right? It's okay for a while. - And never sacrifice that.
[00:54:07.040 --> 00:54:10.400]   - You would never sacrifice that. I don't think you can.
[00:54:10.400 --> 00:54:16.320]   - So there was a lot of pressure when you took over as CEO and there was 22 consecutive quarters
[00:54:16.320 --> 00:54:22.080]   of revenue decline between 2012 and the summer of 2017. So it was a stressful time. Maybe not,
[00:54:22.080 --> 00:54:28.720]   maybe you can correct me on that. So as a CEO, what was it like going through that time,
[00:54:28.720 --> 00:54:33.840]   the decisions, the tensions in terms of investing versus making a profit?
[00:54:33.840 --> 00:54:41.040]   - I always felt that, that sense of urgency was so high. And even if I was calm on the outside,
[00:54:41.040 --> 00:54:46.880]   because you have one of the world's largest pensions. So, so many people depend on you.
[00:54:46.880 --> 00:54:50.720]   You have a huge workforce. They're depending on you. You have clients whose businesses don't run
[00:54:50.720 --> 00:54:59.840]   if you don't perform, et cetera. And shareholders, of course, right? And so, but I also am really
[00:54:59.840 --> 00:55:04.720]   clear. This was perhaps the largest reinvention IBM ever had to undertake. I had a board that
[00:55:04.720 --> 00:55:09.440]   understood that. In fact, some people, some of the headlines were like, this is existential,
[00:55:09.440 --> 00:55:13.600]   right? I mean, nobody gives you a right to exist forever. And there aren't many texts. You're the
[00:55:13.600 --> 00:55:20.240]   student of it. They are gone. They are all gone. And so if we didn't reinvent ourselves,
[00:55:20.240 --> 00:55:26.480]   we were going to be extinct. And so now, but you're big and it's like changing, what's that old
[00:55:26.480 --> 00:55:31.280]   saying? Can I change the wheels while the train's running or something like that? Or the engines
[00:55:31.280 --> 00:55:36.720]   while the plane's flying? And that's what you have to do. And that took time. And so, you know,
[00:55:36.720 --> 00:55:43.040]   Lex, do I wish it would have been faster? Absolutely. But the team worked so hard. And in
[00:55:43.040 --> 00:55:51.840]   that timeframe, 50% of the portfolio was changed. It's a very large company. And if you would,
[00:55:51.840 --> 00:55:56.880]   I also divested $10 billion to businesses. So if you would look at that growth rate without
[00:55:56.880 --> 00:56:01.040]   divestitures and currency, which now today everyone talks about currency. Back then,
[00:56:01.040 --> 00:56:07.120]   we were the only international guy. Net of divestitures and currency, the growth was flat.
[00:56:07.120 --> 00:56:12.000]   Is flat great? No, but flat for a big transformation. I was really proud of the
[00:56:12.000 --> 00:56:17.200]   team for what they did. That is actually pretty miraculous to have made it through that. I had
[00:56:17.200 --> 00:56:21.280]   my little nephew one day and he would see on TV occasionally when there'd be criticism,
[00:56:21.280 --> 00:56:25.440]   and he'd say, "You know, Auntie, does it make you mad when they talk mean?"
[00:56:25.440 --> 00:56:26.000]   CB: Yeah.
[00:56:26.000 --> 00:56:30.080]   DRH: And I just looked at him and I said, "How do you feel?" I said, "Look,
[00:56:30.080 --> 00:56:36.720]   I'm doing what has to be done." And I happened to be the one there. And if you have great conviction,
[00:56:37.360 --> 00:56:43.280]   and I did, a great conviction, I knew it was the right thing. I knew it would be needed for IBM to
[00:56:43.280 --> 00:56:49.120]   live its second century. And my successor, they have picked up, gone forward. I mean, you go back,
[00:56:49.120 --> 00:56:53.440]   we did the acquisition of Red Hat. I mean, we had to find our way on cloud, right? We were
[00:56:53.440 --> 00:56:57.920]   late to it. So we had to find our way. And eventually that led us to hybrid cloud.
[00:56:57.920 --> 00:57:03.440]   We did a lot of work with Red Hat back in 2017. Oh, we'd always done a lot of work with them.
[00:57:03.440 --> 00:57:08.880]   Actually, we were one of the first investors when they were first formed. But that was 2018.
[00:57:08.880 --> 00:57:14.080]   We took quite a hit for even--oh, it was the largest to then software acquisition ever.
[00:57:14.080 --> 00:57:20.720]   But it is the foundation, right, of what is our hybrid cloud play today and doing very, very well.
[00:57:20.720 --> 00:57:26.480]   But it had to take a short-term hit for that, right? Short-term hit for a very large $34 billion
[00:57:26.480 --> 00:57:32.080]   acquisition. But for all of us, it was the right thing to do. So I think when you get really
[00:57:32.080 --> 00:57:35.680]   centered on, you know it's the right thing to do, you just keep going, right?
[00:57:35.680 --> 00:57:39.600]   So the team had the vision, they had the belief, and everything else, the criticism doesn't matter.
[00:57:39.600 --> 00:57:45.120]   So we didn't always have exactly the right--this wasn't a straight arrow. But stay down,
[00:57:45.120 --> 00:57:48.960]   you know you're right, keep going. Okay, made a mistake. There's no bad mistake as long as
[00:57:48.960 --> 00:57:53.600]   you learn from it, right? And keep moving. So yes, did it take longer, but we are the largest
[00:57:53.600 --> 00:57:58.720]   that was there. Could you maybe just on a small tangent, educate me a little bit? So Red Hat
[00:57:58.720 --> 00:58:04.240]   originally is Linux, open source distribution of Linux, but it's also consulting. Well, it's--
[00:58:04.240 --> 00:58:07.440]   A little bit of consulting, but it's mostly software distribution. It's mostly Linux.
[00:58:07.440 --> 00:58:10.960]   It was mostly software. Yeah, absolutely. Absolutely.
[00:58:10.960 --> 00:58:14.080]   So but today, IBM is very much, there's, you know--
[00:58:14.080 --> 00:58:14.480]   Services.
[00:58:14.480 --> 00:58:22.560]   Most IT services in the world is done by IBM. There's so many varied--so basically if you have
[00:58:22.560 --> 00:58:29.040]   issues, problems to solve in business, in the software space, IBM can help.
[00:58:29.040 --> 00:58:35.600]   Yes, and so in that my last year, our services business, we broke it into two pieces. And one
[00:58:35.600 --> 00:58:41.120]   piece was spun off into a company called Kindrel, which is managed outsourcing. Keeping things
[00:58:41.120 --> 00:58:45.840]   running, and they're off creating their own company. What IBM then retained is really the
[00:58:45.840 --> 00:58:53.120]   part I built with PWCC, the big consulting arm. And so today, the IBM of today in 2023 is,
[00:58:53.120 --> 00:58:58.320]   you know, at least ending 2022, was 30% consulting, and the other 70% would be,
[00:58:58.320 --> 00:59:03.760]   what would you consider software cloud AI? So hybrid cloud and AI is the other, and some
[00:59:03.760 --> 00:59:09.760]   hardware, obviously. Still, the mainframe is modernized, alive, and kicking, and still running
[00:59:09.760 --> 00:59:14.720]   some of the most important things of every bank you can think of practically in the world.
[00:59:15.280 --> 00:59:23.600]   And so that is the IBM of today versus perhaps, you know, and Red Hat is a big piece and an
[00:59:23.600 --> 00:59:27.600]   important part of that software portfolio. And they had some services with them for
[00:59:27.600 --> 00:59:33.200]   implementation, but it wasn't a very large part. And it's grown by leaps and bounds,
[00:59:33.200 --> 00:59:36.880]   you know, because originally the belief was everything was going to go to the public cloud.
[00:59:36.880 --> 00:59:44.000]   And at least many people thought that way. We didn't. In fact, I mean, we tried. We
[00:59:44.000 --> 00:59:48.240]   cured a public cloud company. We really tried to work it. But what we found was a lot of the
[00:59:48.240 --> 00:59:53.520]   mission critical work, it was tuned for consumer world. It wasn't tuned for the enterprise.
[00:59:53.520 --> 01:00:00.560]   So then time is elapsing here though, and you got to be at scale. And we didn't have
[01:00:00.560 --> 01:00:04.720]   any application. Remember, we're not an application company. So it wasn't like we had
[01:00:04.720 --> 01:00:09.680]   an office. We didn't have anything that like pulled things out to the cloud. And so as we
[01:00:09.680 --> 01:00:15.120]   look for what our best, what we really back to who you are, we really know mission critical work.
[01:00:15.120 --> 01:00:18.880]   We know where it lives today, and we know how to make it live on the cloud,
[01:00:18.880 --> 01:00:23.360]   which led us down hybrid cloud. You know, that belief that the real world would turn into,
[01:00:23.360 --> 01:00:27.680]   there'll be things on traditional that'll, you'll never move because it doesn't make sense.
[01:00:27.680 --> 01:00:32.000]   There'll be private clouds for, you know, have all the benefit of the cloud, but they just don't
[01:00:32.000 --> 01:00:35.760]   have, you know, infinite expansion. And then there'll be public clouds and you're going to
[01:00:35.760 --> 01:00:40.400]   have to connect them and be secure. And that's what took us down the path with Red Hat, that
[01:00:40.400 --> 01:00:44.320]   belief. - The structure of that is fundamentally different than something that's consumer facing.
[01:00:44.320 --> 01:00:49.520]   So the lesson you learn there is you can't just reuse something that's optimized for consumers.
[01:00:49.520 --> 01:00:52.560]   - Yeah, very interesting point. It doesn't mean consumer companies can't move up in the enterprise
[01:00:52.560 --> 01:00:56.960]   because obviously they have, right? But I think it's very hard to take something from the enterprise
[01:00:56.960 --> 01:00:58.080]   and come on down. - Sure.
[01:00:58.080 --> 01:01:01.920]   - And because it got to be simple, consumable, all the things we talked about already.
[01:01:01.920 --> 01:01:03.920]   - Plus you have to have the relationships with the enterprise.
[01:01:03.920 --> 01:01:08.160]   - Yeah, very different. Yeah. I mean, you know our history, right? At one time we had the PC business
[01:01:08.160 --> 01:01:14.160]   and, you know, the short answer to why we would not do that is it's a consumer facing business.
[01:01:14.160 --> 01:01:18.400]   We were good at the enterprise and that consumer business, A, highly competitive,
[01:01:18.400 --> 01:01:23.760]   got to be low cost, all the things that are not the same muscles necessarily of being in an
[01:01:23.760 --> 01:01:30.080]   innovation driven, you know, technology business. - Yeah, but what is now Lenovo, I guess that's
[01:01:30.080 --> 01:01:31.920]   what's just been done. - That's right, Lenovo acquired it.
[01:01:31.920 --> 01:01:37.440]   - They were extremely good at that, but not as good as you're saying as an enterprise.
[01:01:37.440 --> 01:01:43.040]   - Yeah, Lenovo's very good at their PC world, yes, and they can sell into the enterprise,
[01:01:43.040 --> 01:01:48.000]   right? But you as a consumer can go buy a Lenovo too. So look in China, right? Look in other places.
[01:01:48.000 --> 01:01:52.480]   So that's what I mean by consumer, you know, an end device. And that was a big decision because
[01:01:52.480 --> 01:01:56.480]   it would have been one of the last things that had our logo on it that sits in your hands, right? So
[01:01:56.480 --> 01:02:00.560]   when a new generation says, "Well, what does IBM do?" right? - Was that a difficult decision?
[01:02:00.560 --> 01:02:02.960]   Do you remember? - This is a long time ago now,
[01:02:02.960 --> 01:02:08.800]   it's like 2005. So they're all difficult because it's not only things, it's people.
[01:02:08.800 --> 01:02:16.800]   But it's back to knowing who you are is how I would sum that up as, right? And we were never
[01:02:16.800 --> 01:02:20.560]   great at making a lot of money at that. And you can remember originally it was IBM PC,
[01:02:20.560 --> 01:02:26.960]   then there were IBM clones, or they were called IBM clones back then as the field became highly,
[01:02:26.960 --> 01:02:33.360]   highly competitive. And as things commoditized, we often as they commoditize, we would move them
[01:02:33.360 --> 01:02:39.840]   out and move on to the next innovation level. - But because of that, it's not as public facing.
[01:02:39.840 --> 01:02:41.200]   - That's right. That's absolutely right. - Even though it's one of the most
[01:02:41.200 --> 01:02:44.640]   recognizable logos ever. - Yeah, isn't it true? That is very true.
[01:02:44.640 --> 01:02:50.080]   That is actually a very important point. And that is, you know, branding, as you say,
[01:02:50.080 --> 01:02:55.680]   one of the most recognizable and a very highly ranked brand strength around the world. And so
[01:02:55.840 --> 01:03:02.160]   that's a trade-off. I mean, I can't, you know, because there was a time you'd have something
[01:03:02.160 --> 01:03:06.720]   of IBM in your home or a cash register, as an example, you'd walk into a store, actually,
[01:03:06.720 --> 01:03:12.400]   they're still in places that went to Toshiba. - Can you speak to consulting a little bit? What
[01:03:12.400 --> 01:03:18.240]   does that entail? To train up, to hire a workforce that can be of service to all kinds of different
[01:03:18.240 --> 01:03:22.320]   problems in the software space, in the tech space. What's entailed in that?
[01:03:22.320 --> 01:03:28.640]   - I mean, you have to value a different set of things, right? And so you've got to always stay
[01:03:28.640 --> 01:03:36.320]   ahead. It's about hiring people who are willing to learn. It is about, at the same time, in my view,
[01:03:36.320 --> 01:03:43.440]   it's what really drives you to be customer-centric. - Maybe you can educate me. I think consulting is
[01:03:43.440 --> 01:03:50.800]   a kind of, you roll in and you try to solve problems that businesses have, like with expertise,
[01:03:50.800 --> 01:03:56.880]   right? - Okay. - Is that the process of consulting? - Somewhat, right? So, fair enough. When you say
[01:03:56.880 --> 01:04:00.880]   the word consulting, it's a really broad spectrum. I mean, I think people could be sitting here
[01:04:00.880 --> 01:04:06.080]   thinking it does any, it could be, I just give advice and I leave, to all the way to, I run your
[01:04:06.080 --> 01:04:12.480]   systems, right? And I think it's generally, people use the word to cover everything in that space.
[01:04:12.480 --> 01:04:17.920]   So we sort of fit in the spot, which is, we would come in and live at that intersection of business
[01:04:17.920 --> 01:04:21.280]   and technology. So yeah, we could give you recommendations and then we'd implement them
[01:04:21.280 --> 01:04:24.880]   and see them through, because we had the technology to go to the implementation and see them through.
[01:04:24.880 --> 01:04:29.520]   And at the time back then, that's what, there'd been five of those that had failed,
[01:04:29.520 --> 01:04:35.040]   that the companies had bought other consulting firms. And so we were, okay, that was the great
[01:04:35.040 --> 01:04:38.960]   thing about, I mean, the harrowing thing about it was, here, please go work on this. None of
[01:04:38.960 --> 01:04:43.760]   the others have ever succeeded before. And yet on the other hand, the great promise was, you could
[01:04:43.760 --> 01:04:48.400]   really, clients were dying at that time when we were doing that, to get more value out of their
[01:04:48.400 --> 01:04:52.960]   technology and have it really change the way the business worked. So I think of it as how to
[01:04:52.960 --> 01:04:56.720]   improve business and apply technology and see it all the way through. That's what we do today still.
[01:04:56.720 --> 01:05:01.920]   - Yeah, to see it all the way through. Yes. So let me say, it's almost like a personal question.
[01:05:01.920 --> 01:05:08.000]   So that was a big thing you were a part of that you led in 2002, that you championed and helped,
[01:05:08.000 --> 01:05:14.640]   I should say, negotiate the purchase of Monday, the consulting arm of PricewaterhouseCoopers
[01:05:14.640 --> 01:05:19.840]   for $3.5 billion. So what were some of the challenges of that that you remember,
[01:05:19.840 --> 01:05:29.040]   personal and business? - At that time, PW had to, really had to divest. And so they were in
[01:05:29.040 --> 01:05:33.760]   peril of going to IPO, right? So we sort of swept in at that point and said, and we'd been thinking
[01:05:33.760 --> 01:05:39.680]   about it a long time and started to work on that as an acquisition. So, kind of balancing which way
[01:05:39.680 --> 01:05:44.720]   would they go, IPO or acquisition. And so the challenges are obvious and part of it's why they
[01:05:44.720 --> 01:05:49.520]   went with us as an acquisition. Big difference to be a private firm than a public firm, very big.
[01:05:49.520 --> 01:05:53.600]   I can remember one of the guys, he asked somebody, "How long have you been with IBM?" And the person
[01:05:53.600 --> 01:05:58.400]   answered, "143 quarters." Okay, that's a little enlightening about a business model, right?
[01:05:59.360 --> 01:06:05.040]   So we had the challenges of being private versus public. You have the challenge of when you acquire
[01:06:05.040 --> 01:06:10.400]   something like that, as I say, you acquire hearts, not parts. They could leave. I mean,
[01:06:10.400 --> 01:06:14.480]   you could destroy your value by them leaving. They can walk right out the door. I mean, yes,
[01:06:14.480 --> 01:06:20.400]   you can put lots of restraints, but still, that you have there. And then we had to really build
[01:06:20.400 --> 01:06:25.760]   a new business model that people and clients would see as valuable and be willing to pay for.
[01:06:26.320 --> 01:06:31.760]   And so we had to do something that lived at that intersection and say that how this was unique is
[01:06:31.760 --> 01:06:37.840]   what we were doing. So you had the people aspect, you had that they were going to be public and they
[01:06:37.840 --> 01:06:44.160]   had always been private their whole life. And then you had the business model. So, and the others had
[01:06:44.160 --> 01:06:50.320]   all failed that had tried to do this. So yeah, it was a tough thing to do.
[01:06:50.320 --> 01:06:54.880]   What about the personal side of that? That was a big leap, step up for you. You've been at IBM
[01:06:54.880 --> 01:07:05.360]   for a long time. This is a big sort of leadership, like very impactful, large scale leadership
[01:07:05.360 --> 01:07:10.400]   decisions. What was that like? - So, unlike in my career earlier,
[01:07:10.400 --> 01:07:15.440]   where I said I was changing jobs, I said I wasn't comfortable, et cetera. So now here, fast forward
[01:07:15.440 --> 01:07:21.360]   10 years, and I'm like, okay, honestly, how I felt inside on one hand, I did what I learned,
[01:07:21.360 --> 01:07:26.400]   like inventory what you know how to do. Like you have some good strengths that could work here.
[01:07:26.400 --> 01:07:32.640]   The other part of me said, boy, this is really high profile. And I felt, and I can remember
[01:07:32.640 --> 01:07:38.560]   saying to someone like, this is going to kill me or catapult, probably nothing in between.
[01:07:38.560 --> 01:07:41.520]   - And that wasn't terrifying to you? That was okay? You were okay with that kind of pressure?
[01:07:41.520 --> 01:07:46.560]   - I was okay with that because I felt I knew enough, you know, like these things I had,
[01:07:47.200 --> 01:07:56.480]   and I'll tell you the one thing I felt I knew the best. Consultants of any, worth their weight,
[01:07:56.480 --> 01:08:04.240]   they really do care that they deliver for an end client. And I felt I understood service to a
[01:08:04.240 --> 01:08:09.280]   client so well, that what it meant to really provide value. So I knew we would have like
[01:08:09.280 --> 01:08:14.720]   something that I knew the PwC people, more than anything, wanted to deliver value to those clients
[01:08:14.720 --> 01:08:18.960]   they had, next to then developing their people, that those were like the really two things.
[01:08:18.960 --> 01:08:23.680]   And that I could, and I also knew they felt they could do better if they had more technology.
[01:08:23.680 --> 01:08:28.160]   And we did. So there really was a reason, you know, that I could really believe in. So I
[01:08:28.160 --> 01:08:33.760]   authentically believed back to that point. And I also felt I had built some of those skills to be
[01:08:33.760 --> 01:08:43.200]   able to do that. But I wouldn't call it terrifying, but make no mistake, Lex, it was very hard. And
[01:08:44.080 --> 01:08:48.240]   it turned out to be extremely successful. By the time we ended, it was worth 19 and a half,
[01:08:48.240 --> 01:08:53.360]   well, the time I stepped, I ran it for, oh goodness gracious, quite a long time. I'm
[01:08:53.360 --> 01:08:58.960]   going to say seven or eight, nine years. And we were 19 and a half billion dollars.
[01:08:58.960 --> 01:09:04.960]   It made 2.7 billion in profit. It was very consequential to IBM. But the fact that it was
[01:09:04.960 --> 01:09:10.400]   consequential is also very, I mean, there was a time as we moved through it, I can even remember
[01:09:10.400 --> 01:09:15.440]   it. We just weren't meeting the goals as fast as we should. And some of it was clients were like,
[01:09:15.440 --> 01:09:21.360]   oh, now you're IBM. So, I mean, some things I knew would happen, but they happened so much faster.
[01:09:21.360 --> 01:09:24.880]   It'd be things like clients would say, oh, IBM cares about a quarter. So let's negotiate every
[01:09:24.880 --> 01:09:30.560]   quarter on these prices. And, you know, when they were private, they didn't have these issues. Well,
[01:09:30.560 --> 01:09:34.320]   that had an impact on margins really fast. And so that ability-
[01:09:34.320 --> 01:09:35.760]   So you picked up a lot of challenges.
[01:09:35.760 --> 01:09:41.360]   You pick them up right away. And I thought, oh boy, I mean, if I don't get this turned around,
[01:09:41.360 --> 01:09:46.640]   this is really a problem. And the team learned a lot of lessons. I mean, I learned people I had to
[01:09:46.640 --> 01:09:51.520]   move out, that I learned that when people don't believe they can do something, they probably won't
[01:09:51.520 --> 01:09:56.960]   do it. So, you know, we wanted to run the business at a certain level. I really did have some great
[01:09:56.960 --> 01:10:00.800]   leaders, but they didn't really believe it could do that. And I finally had to come to terms with,
[01:10:00.800 --> 01:10:03.760]   if you don't really believe in something, you really aren't going to probably make it happen
[01:10:03.760 --> 01:10:09.760]   at the end of the day. And so we would change that. We would have to actually get some more
[01:10:09.760 --> 01:10:14.560]   help to help us on doing so. But then it turned. And I can remember the day that we started
[01:10:14.560 --> 01:10:20.960]   really getting the business to hum and start to, it was almost like, finally. And I gave the team
[01:10:20.960 --> 01:10:27.120]   this little plaque, this little, it was kind of corny, paperweight thing. And I'm going to believe,
[01:10:27.120 --> 01:10:33.920]   remember if it was Thomas Edison, and he said, "Many of life's greatest failures are people who
[01:10:33.920 --> 01:10:40.560]   gave up right before they were going to be successful." And it's so true. I mean, there
[01:10:40.560 --> 01:10:45.680]   was also a governor of Texas who's passed, but she had said, someone said, "What's the secret
[01:10:45.680 --> 01:10:50.560]   of your success?" And she said, "It's passion and perseverance when everyone else would have given
[01:10:50.560 --> 01:10:56.480]   up." And I feel that's what that taught me. That taught me, like, no matter how bad this gets,
[01:10:56.480 --> 01:11:01.120]   you are not giving up. Now you can't keep doing the same thing, like the doctor, this hurts,
[01:11:01.120 --> 01:11:06.640]   oh, then stop doing it. You can't keep doing the same thing. We had to keep changing till we found
[01:11:06.640 --> 01:11:14.160]   our right way to get the model to work right. And client work, we never had an issue and kept so
[01:11:14.160 --> 01:11:21.440]   many of the people. And now we are 25 years almost later, and a number of them run parts of the IBM
[01:11:21.440 --> 01:11:27.200]   business still today. So it's that old Maya Angelou saying, when you say, "What do I remember?"
[01:11:27.200 --> 01:11:32.080]   They'll say, "You won't remember the specifics of this, but you'll remember how you felt." And
[01:11:32.080 --> 01:11:35.600]   that's kind of how I feel. And I think they do too, the whole team does, of that. Like,
[01:11:35.600 --> 01:11:40.960]   I'll get anniversary notes still on that, you know, when you've been through something like
[01:11:40.960 --> 01:11:45.360]   that together with people. So during the acquisition, the way you knew the people,
[01:11:46.560 --> 01:11:52.240]   it's the right team are the ones that could believe that this consulting business can grow,
[01:11:52.240 --> 01:11:56.240]   can integrate with IBM and all that. - Yeah, I was lucky. Look, I did things
[01:11:56.240 --> 01:11:59.840]   that helped that. I mean, I knew that people joining us would feel more comfortable if they
[01:11:59.840 --> 01:12:06.160]   had people leading it that they recognized, et cetera. But again, I learned those that didn't
[01:12:06.160 --> 01:12:12.080]   then, I eventually had to take some action out. But PWCC had a lot of really dedicated leaders
[01:12:12.080 --> 01:12:17.040]   to it and I give them a lot of credit. - Well, it's amazing to see a thing that
[01:12:17.040 --> 01:12:22.320]   kind of start at that very stressful time and then it turns out to be a success.
[01:12:22.320 --> 01:12:24.400]   - Yeah. - That's just beautiful to see. So
[01:12:24.400 --> 01:12:28.960]   what about the acquisition itself? Is there something interesting to say about the,
[01:12:28.960 --> 01:12:33.040]   like, what you learned about maybe negotiation? Because there's a lot of money involved too.
[01:12:33.040 --> 01:12:39.520]   - To me, it was a win-win and we both actually cared that customers got value. So there was this
[01:12:39.520 --> 01:12:45.680]   like third thing that had a benefit, not them, there was this third thing. And then next to that--
[01:12:45.680 --> 01:12:50.640]   - I like how you think that people would have the wisdom or what it takes to have great
[01:12:50.640 --> 01:12:54.720]   negotiation. But yeah, so it's a win-win is one of the ways you can have successful negotiation.
[01:12:54.720 --> 01:12:58.480]   - But it's like obvious to even say that, right? I mean, if you can, back to being in service of
[01:12:58.480 --> 01:13:04.080]   something, we were both in service of clients. So in and then, you know, I always say,
[01:13:05.280 --> 01:13:09.920]   when you have a negotiation with someone, okay, both parties always kind of walk away a little
[01:13:09.920 --> 01:13:14.160]   bit. Okay, that's good. If they both walk away going, "God, I should have got a little bit more.
[01:13:14.160 --> 01:13:18.800]   Okay, but it's okay if I should have got." Okay, they're both a little fussy. When one walks away
[01:13:18.800 --> 01:13:22.720]   and thinks they did great and the other one did horrible, they're usually like born bad. I mean,
[01:13:22.720 --> 01:13:27.760]   because they never worked that way. And I've always felt that way with negotiations that
[01:13:29.600 --> 01:13:35.520]   you push too far down and you usually will be sorry you did that, you know?
[01:13:35.520 --> 01:13:40.080]   - So don't push too far. I mean, that's ultimately what collaboration and empathy
[01:13:40.080 --> 01:13:45.120]   means is you're interested in the long-term success of everybody together versus like
[01:13:45.120 --> 01:13:47.040]   your short-term success. - And then you get the discretionary
[01:13:47.040 --> 01:13:50.720]   energy from them versus like, "Okay, you screwed me here. I'm done," right?
[01:13:50.720 --> 01:13:57.440]   - So let's even rewind even back. - Oh no. Oh no. Do you feel like this is
[01:13:57.440 --> 01:14:01.120]   a nostalgia interview? Oh no. - Let me just ask the romantic question.
[01:14:01.120 --> 01:14:04.720]   What did you love most about engineering, computer science, electrical engineering,
[01:14:04.720 --> 01:14:08.160]   so in those early days from your degree to the early work?
[01:14:08.160 --> 01:14:13.760]   - I love that logic part of it, right? And you do get a sense of completion at some point when
[01:14:13.760 --> 01:14:20.080]   you reach certain milestones that, you know, like, yes, it worked or yes, that finite answer to that.
[01:14:20.080 --> 01:14:22.880]   So that's what I loved about it. I loved the problem-solving of it.
[01:14:22.880 --> 01:14:27.040]   - Computing, what led you down that path? Computing in general, what made you fall in
[01:14:27.040 --> 01:14:32.480]   love with computing, with engineering? - It's probably that back to that desire,
[01:14:32.480 --> 01:14:36.160]   wanting to know how things work, right? And so that's like a natural thing. You know,
[01:14:36.160 --> 01:14:39.760]   math, I loved math for that reason. I always wanted to study how did that, you know,
[01:14:39.760 --> 01:14:45.200]   how did it get that to work kind of thing? So it goes back in that time. But I did start,
[01:14:45.200 --> 01:14:48.400]   when I went to, when I started at Northwestern, I was already in the engineering school,
[01:14:48.400 --> 01:14:51.840]   but my first thought was to be a doctor, that that was far more noble, that I should be a
[01:14:51.840 --> 01:14:56.320]   medical doctor until I could not pass human reproduction as a course. And I thought the
[01:14:56.320 --> 01:15:00.880]   irony that I could not, I'm like, I got all these colored pencils, I got all these pictures,
[01:15:00.880 --> 01:15:02.560]   this is not working out for me. - I'm gonna stick to math.
[01:15:02.560 --> 01:15:07.200]   - It was the only course in my four-year college education I had to take pass/fail,
[01:15:07.200 --> 01:15:11.840]   I guess, otherwise I risked, you know, impairing my grade point average.
[01:15:11.840 --> 01:15:16.640]   - Engineering it is. So, but after about 10 years, you jumped from the technical
[01:15:16.640 --> 01:15:22.720]   role of systems engineer to management, to a leadership role. Did you miss at that time
[01:15:22.720 --> 01:15:26.640]   the sort of the technical direct contribution versus being a leader, a manager?
[01:15:26.640 --> 01:15:31.600]   - That's an interesting point. Like I say, I've always been sort of a doer leader, you know, so.
[01:15:31.600 --> 01:15:35.520]   - So you never lost that. - I never really did. Even,
[01:15:35.520 --> 01:15:41.600]   you know, and I think this is really important for today. The best way people learn is experientially,
[01:15:41.600 --> 01:15:44.800]   I think. Now you may, that's being a generalization because there are people
[01:15:44.800 --> 01:15:49.360]   can learn all different ways, right? So I've done things like with my whole team,
[01:15:50.400 --> 01:15:55.520]   they all had to learn how to build cloud applications and call the code off. And so,
[01:15:55.520 --> 01:16:01.920]   you know, I don't care what your job is, write code, you know? And I remember when we were
[01:16:01.920 --> 01:16:06.720]   trying to get the company to understand AI, we did something called a cognitive jam. Okay,
[01:16:06.720 --> 01:16:10.480]   there's a reason we picked the word cognitive, by the way, instead of AI. Today we use the word AI.
[01:16:10.480 --> 01:16:17.920]   It was really symbolic. It was to mean, this is to help you think, not replace your thinking.
[01:16:17.920 --> 01:16:22.960]   There was so much in the zeitgeist about AI being a bad thing at that time. So that was why we picked
[01:16:22.960 --> 01:16:28.000]   a mouthful of a word like cognitive. And it was like, no, no, this is to help you actually. So
[01:16:28.000 --> 01:16:32.800]   do what, you know, do what you do better or do something you haven't yet learned. And we did
[01:16:32.800 --> 01:16:36.720]   something called the cognitive jam, but the whole point was everybody in the company could volunteer,
[01:16:36.720 --> 01:16:41.120]   get on a team. You either had to build something that improved one of our products, or did
[01:16:41.120 --> 01:16:48.000]   something for a client, or did a social, solved a social issue with AI. And again, this goes back
[01:16:48.000 --> 01:16:55.440]   now, 10 years, and people did things from bullying applications to, you know, railroad stuff to
[01:16:55.440 --> 01:17:01.600]   whatever it was, but it got like a hundred thousand people to understand, you know, viscerally what is
[01:17:01.600 --> 01:17:06.720]   AI. So that's a long answer to my belief around experiential. And so do you ever give it up?
[01:17:07.280 --> 01:17:10.960]   I don't think so. Cause I actually think that's pretty good to get your hands dirty
[01:17:10.960 --> 01:17:14.160]   in something. You know, you can't do it, you know, depending what you're doing,
[01:17:14.160 --> 01:17:16.160]   your effort to do that will be less, but.
[01:17:16.160 --> 01:17:20.720]   So even a CEO, you try to get your hands dirty a little bit.
[01:17:20.720 --> 01:17:25.360]   I've played, I mean, still, I'm not saying I'm any good at any of it anymore, but.
[01:17:25.360 --> 01:17:26.240]   But to build up intuition.
[01:17:26.240 --> 01:17:30.400]   But it's that, yeah, it's that really understand, right? And not be afraid of.
[01:17:30.400 --> 01:17:35.600]   Yeah. Like we mentioned at the beginning, IBM research has helped catalyze some of the
[01:17:35.600 --> 01:17:41.600]   biggest accomplishments in computing and artificial intelligence history. So D-Blue,
[01:17:41.600 --> 01:17:49.840]   IBM D-Blue versus Kasparov chess match in '96 and '97. Just to ask kind of like what your
[01:17:49.840 --> 01:17:55.760]   perception is, what your memory is of it, what is that moment? Like this seminal moment, I believe
[01:17:55.760 --> 01:18:01.040]   probably one of the greatest moments in AI history, when the machine first beat a human at a
[01:18:01.040 --> 01:18:02.960]   thing that humans thought.
[01:18:02.960 --> 01:18:07.200]   You make a very interesting point, because it is like one of the first demonstrations of using a
[01:18:07.200 --> 01:18:10.960]   game to like bring something to people's consciousness, right? And to this date,
[01:18:10.960 --> 01:18:18.800]   people use games, right, to demonstrate different things. But at the time, it's funny. I didn't
[01:18:18.800 --> 01:18:23.200]   necessarily think of it so much as AI, and I'll tell you why. I was, and I'm not a chess player,
[01:18:23.200 --> 01:18:27.920]   you might be a chess player, so I'm not expert at it. But I think I understand properly of chess,
[01:18:27.920 --> 01:18:32.640]   that chess has got a finite number of moves that can be made. Therefore, if it's finite,
[01:18:32.640 --> 01:18:37.600]   really what's a demonstration of a supercomputing, right? It's about the amount of time and how fast
[01:18:37.600 --> 01:18:42.160]   it can crunch through to find the right move. So in some ways, I thought of it as almost a
[01:18:42.160 --> 01:18:48.160]   bigger demonstration of that. But it is absolutely, as you said, it was a motivator, one of the big
[01:18:48.160 --> 01:18:53.360]   milestones of AI, because it put in your consciousness that it's man in this other
[01:18:53.360 --> 01:18:55.200]   machine, right? Doing something.
[01:18:55.200 --> 01:19:00.160]   So you saw it as just a challenging computation problem, and this is a way to demonstrate hardware
[01:19:00.160 --> 01:19:01.840]   and software computation at its best.
[01:19:01.840 --> 01:19:02.880]   Yes, I did.
[01:19:02.880 --> 01:19:09.360]   But the thing is, there is a romantic notion that chess is the embodiment of human intellect,
[01:19:09.360 --> 01:19:14.320]   I mean, intelligence, that you can't build a machine that can beat a chess champion in chess.
[01:19:14.320 --> 01:19:18.000]   See, and I was blessed by not being a chess expert, so it wasn't like,
[01:19:18.000 --> 01:19:18.320]   "Oh, I'm not a chess player."
[01:19:18.320 --> 01:19:19.520]   It's just a computation problem.
[01:19:19.520 --> 01:19:21.200]   It was a computation problem to me.
[01:19:21.200 --> 01:19:26.800]   Well, that's probably required to not be paralyzed by the immensity of the task,
[01:19:26.800 --> 01:19:32.720]   so that this is just solvable. But it was a very, very, I think that was a powerful moment,
[01:19:32.720 --> 01:19:39.520]   so speaking just as an AI person, that reinvigorated the dream.
[01:19:39.520 --> 01:19:43.520]   You were a little kid back then, though, right? At 95? You have to be, like, were you...
[01:19:43.520 --> 01:19:45.920]   Do you remember it, actually, at the moment?
[01:19:45.920 --> 01:19:47.280]   Yeah, yeah, yeah, yeah, yeah.
[01:19:47.280 --> 01:19:48.880]   What did you think at the moment about it?
[01:19:49.680 --> 01:19:59.040]   Um, it was awe-inspiring, because, especially sort of growing up in the Soviet Union,
[01:19:59.040 --> 01:20:06.880]   you think, especially of Garry Kasparov and chess, like, your intuition is weak about those things.
[01:20:06.880 --> 01:20:13.680]   I didn't see it as computation. I thought of it as intelligence, because chess,
[01:20:13.680 --> 01:20:15.760]   for a human being, doesn't feel like computation.
[01:20:16.800 --> 01:20:23.840]   It feels like some complicated relationship between memory and patterns and intuition
[01:20:23.840 --> 01:20:28.240]   and guts and instinct and all of those, like...
[01:20:28.240 --> 01:20:30.400]   If you watch someone play, that's what you would conclude, right?
[01:20:30.400 --> 01:20:37.040]   So to see a machine be able to beat a human, I mean, you get a little bit of that with Chagy-Pity,
[01:20:37.040 --> 01:20:41.520]   now, it's like, language was to us humans the thing that we kind of...
[01:20:42.800 --> 01:20:47.200]   Surely, the poetry of language is something only humans can really have.
[01:20:47.200 --> 01:20:51.200]   It's going to be very difficult to replicate the magic of
[01:20:51.200 --> 01:20:55.120]   natural language without deeply understanding language.
[01:20:55.120 --> 01:20:59.600]   But it seems like Chagy-Pity can do some incredible things with language
[01:20:59.600 --> 01:21:04.000]   in natural language dialogue.
[01:21:04.000 --> 01:21:07.440]   But that was the first moment in AI.
[01:21:07.440 --> 01:21:10.960]   Through all the AI winters from the '60s, the promise of the...
[01:21:10.960 --> 01:21:17.920]   It was, wow, this is possible for a simple set of algorithms
[01:21:17.920 --> 01:21:20.160]   to accomplish something that we think of as intelligence.
[01:21:20.160 --> 01:21:23.520]   So that was truly inspiring, that maybe intelligence,
[01:21:23.520 --> 01:21:25.440]   maybe the human mind is just algorithms.
[01:21:25.440 --> 01:21:27.680]   That was the thought at the time.
[01:21:27.680 --> 01:21:32.480]   And of course, now, the funny thing, what happens is the moment you accomplish it,
[01:21:32.480 --> 01:21:35.280]   everyone says, "Oh, it's just brute force algorithms. It's silly."
[01:21:36.400 --> 01:21:37.840]   And this continues.
[01:21:37.840 --> 01:21:43.040]   Every single time you pass a benchmark, a threshold to win a game, people say,
[01:21:43.040 --> 01:21:45.920]   "Oh, well, it's just this."
[01:21:45.920 --> 01:21:49.040]   And I think that's funny.
[01:21:49.040 --> 01:21:54.480]   And there's going to be a moment when we're going to have to contend with AI systems that
[01:21:54.480 --> 01:21:59.040]   exhibit human-like emotions and feelings.
[01:21:59.040 --> 01:22:03.280]   And you have to start to have some difficult discussions about,
[01:22:03.280 --> 01:22:05.200]   well, how do we treat those beings?
[01:22:05.200 --> 01:22:07.280]   And what role do they have in society?
[01:22:07.280 --> 01:22:08.480]   What are the rules around that?
[01:22:08.480 --> 01:22:13.360]   And this is really exciting because that also puts a mirror to ourselves to see,
[01:22:13.360 --> 01:22:17.440]   okay, what's the right way to treat each other as human beings?
[01:22:17.440 --> 01:22:19.360]   Because it's a good test for that.
[01:22:19.360 --> 01:22:23.440]   - It is, because I always say it's a reflection of humanity.
[01:22:23.440 --> 01:22:26.000]   I mean, it's taught by what man...
[01:22:26.000 --> 01:22:28.480]   Bad stuff in the past, you'll teach it bad stuff for the future.
[01:22:29.280 --> 01:22:33.760]   Which is why I think efforts to regulate it are a fool's errand.
[01:22:33.760 --> 01:22:35.040]   You need to regulate uses.
[01:22:35.040 --> 01:22:39.360]   Because it's not the technology itself is not inherently good or bad,
[01:22:39.360 --> 01:22:42.640]   but how it's used or taught can be good or bad for sure.
[01:22:42.640 --> 01:22:49.120]   And so that's, to me, will unveil now a whole different way of having to look at technology.
[01:22:49.120 --> 01:22:52.880]   - What about another magical leap with the early days of Watson
[01:22:52.880 --> 01:22:54.560]   with beating the Jeopardy challenge?
[01:22:54.560 --> 01:22:56.080]   What was your experience like with Watson?
[01:22:56.080 --> 01:22:57.840]   And what's your vision for Watson in general?
[01:22:57.840 --> 01:23:01.280]   - Yeah, and it was really inspired by first chess, right?
[01:23:01.280 --> 01:23:04.080]   And Kasparov, and then you come forward in time.
[01:23:04.080 --> 01:23:09.600]   And I think what Watson did, because you used a really important word,
[01:23:09.600 --> 01:23:12.640]   AI had kind of waxed and waned in these winners, right?
[01:23:12.640 --> 01:23:17.360]   In and out, in and out, popular or not, more money, less money, in and out,
[01:23:17.360 --> 01:23:18.800]   confidence, no confidence.
[01:23:18.800 --> 01:23:24.320]   And so I think that was one of the first times it brought to the forefront of people like,
[01:23:24.320 --> 01:23:26.480]   "Whoa, I could humanized it."
[01:23:26.480 --> 01:23:29.520]   Because here it is playing against these two gentlemen.
[01:23:29.520 --> 01:23:34.480]   And as you did lose at first, and then finally won at the end of the day.
[01:23:34.480 --> 01:23:39.200]   And what it was doing is making you say, "Hey, natural language."
[01:23:39.200 --> 01:23:40.800]   It's actually understanding natural language.
[01:23:40.800 --> 01:23:43.680]   It's one of the first demonstrations of natural language support
[01:23:43.680 --> 01:23:47.200]   and bit of reasoning over lots of data, right?
[01:23:47.200 --> 01:23:50.320]   And so that it could have access to a lot of things,
[01:23:50.320 --> 01:23:51.840]   come up with a conclusion on it.
[01:23:51.840 --> 01:23:54.880]   And to me, that was a really big moment.
[01:23:54.880 --> 01:23:57.520]   And I do think it brought to the conscious of the public,
[01:23:57.520 --> 01:24:00.800]   and in good ways and bad, because it probably set expectations very high of like,
[01:24:00.800 --> 01:24:02.160]   "Whoa, what this could be."
[01:24:02.160 --> 01:24:07.440]   And I still do believe that it has got the ability to change
[01:24:07.440 --> 01:24:11.680]   and help us, man, make better decisions.
[01:24:11.680 --> 01:24:14.240]   That so many decisions are not optimal in this world.
[01:24:14.240 --> 01:24:16.080]   Even medical decisions.
[01:24:16.080 --> 01:24:20.720]   And it's right or wrong what took us down a path of healthcare first with our AI.
[01:24:21.440 --> 01:24:23.120]   And we took many pivots.
[01:24:23.120 --> 01:24:26.240]   And I think there's a really valuable lesson in what we learned.
[01:24:26.240 --> 01:24:29.680]   One is that, I actually don't think the challenges are the technology.
[01:24:29.680 --> 01:24:30.800]   Yes, those are challenges.
[01:24:30.800 --> 01:24:34.800]   But the challenges are the people challenges around this, right?
[01:24:34.800 --> 01:24:36.320]   So do people trust it?
[01:24:36.320 --> 01:24:38.000]   How will they use it?
[01:24:38.000 --> 01:24:43.760]   I mean, I saw that straight up with doctors and like,
[01:24:43.760 --> 01:24:46.960]   meaning they're so busy in the way they've been taught to do something.
[01:24:46.960 --> 01:24:49.840]   Do they really have time to learn another way?
[01:24:49.840 --> 01:24:53.120]   I saw it was a mistake when you put it on top of processes that didn't change,
[01:24:53.120 --> 01:24:54.800]   kind of like paving a cow path.
[01:24:54.800 --> 01:24:56.080]   - Mm-hmm. - Didn't work.
[01:24:56.080 --> 01:25:01.520]   I mean, it was all human change management around it that were really its biggest challenges.
[01:25:01.520 --> 01:25:06.320]   And another valuable lesson, we picked, back to usage, you think of IBM as moonshots.
[01:25:06.320 --> 01:25:08.560]   We picked really hard problems to start with.
[01:25:08.560 --> 01:25:12.400]   I think you see a lot of technology now starts with really simple problems.
[01:25:12.400 --> 01:25:17.280]   And by that, it probably starts to build trust because I start little.
[01:25:17.280 --> 01:25:20.480]   It's like, oh, I'm not ready to outsource my diagnosis to you,
[01:25:20.480 --> 01:25:23.360]   but I'll get some information here about a test question.
[01:25:23.360 --> 01:25:25.040]   So very different thinking.
[01:25:25.040 --> 01:25:26.480]   So a lot of things to learn.
[01:25:26.480 --> 01:25:28.640]   We were making a market at the time.
[01:25:28.640 --> 01:25:33.520]   And when you make a market, choice of problem you work on gets to be very important.
[01:25:33.520 --> 01:25:36.000]   When you're catching up, well, then it's a scale game.
[01:25:36.000 --> 01:25:37.520]   So very different thing.
[01:25:37.520 --> 01:25:42.000]   But Watson proved, I think, I mean, I hope I'm not being too...
[01:25:44.240 --> 01:25:47.760]   I think Watson brought AI back out a winner for the world.
[01:25:47.760 --> 01:25:50.400]   And that since then, there's just been one company after another
[01:25:50.400 --> 01:25:52.160]   and innovations and people working on it.
[01:25:52.160 --> 01:25:55.360]   And I have no regrets of anything that we did.
[01:25:55.360 --> 01:25:56.400]   We learned so much.
[01:25:56.400 --> 01:25:58.960]   I mean, we probably rebuilt it many times over.
[01:25:58.960 --> 01:26:00.160]   It made it more modular.
[01:26:00.160 --> 01:26:05.920]   And today, to IBM, a Watson is more about AI inside of a lot of things,
[01:26:05.920 --> 01:26:10.640]   if you think of it that way, which is more like an ingredient versus it's a thing in and of itself.
[01:26:10.640 --> 01:26:12.720]   And I think that's how it'll bring its real value.
[01:26:12.720 --> 01:26:15.360]   More as an ingredient, and it's so badly needed.
[01:26:15.360 --> 01:26:18.160]   And even back then, the issue was so much data.
[01:26:18.160 --> 01:26:19.200]   What do you ever do with it?
[01:26:19.200 --> 01:26:20.000]   You can't get through it.
[01:26:20.000 --> 01:26:21.040]   You can't use it for anything.
[01:26:21.040 --> 01:26:22.800]   You know this well, it's your profession.
[01:26:22.800 --> 01:26:26.080]   So we have to have it.
[01:26:26.080 --> 01:26:28.080]   So that's going to propel it forward.
[01:26:28.080 --> 01:26:32.320]   - So it's part of the suite of tools that you use when you go to enterprise
[01:26:32.320 --> 01:26:34.080]   and you try to solve problems.
[01:26:34.080 --> 01:26:38.480]   - Yeah, so AI for security, AI in automated operations, AI in your robotics,
[01:26:38.480 --> 01:26:40.080]   AI on your factory floor, you know what I mean?
[01:26:40.080 --> 01:26:41.680]   It's all part of.
[01:26:41.680 --> 01:26:46.160]   And I think, and that's why even to this day, thousands, I mean, thousands and thousands
[01:26:46.160 --> 01:26:50.320]   of clients of IBM still have the Watson components that it's the AI being used.
[01:26:50.320 --> 01:26:53.040]   So it became a platform is how I would say it, right?
[01:26:53.040 --> 01:26:59.520]   And an ingredient that went inside and consultants, like you said, had to learn.
[01:26:59.520 --> 01:27:04.560]   I had, they had to learn, don't just put it on something.
[01:27:04.560 --> 01:27:06.880]   You got to rethink how that thing should work,
[01:27:06.880 --> 01:27:09.760]   because with the AI, it could work entirely differently.
[01:27:09.760 --> 01:27:15.040]   And so I also felt it could open up and still will open up jobs to a lot of people,
[01:27:15.040 --> 01:27:19.200]   because more like an assistant, and it could help me be qualified to do something.
[01:27:19.200 --> 01:27:23.520]   And we even years ago saw this with the French banks, very unionized,
[01:27:23.520 --> 01:27:27.840]   but that idea that you could, in this case, the unions voted for it
[01:27:27.840 --> 01:27:29.840]   because it felt people did a better job.
[01:27:29.840 --> 01:27:33.200]   And so, and that's just part about being really dedicated
[01:27:33.200 --> 01:27:35.200]   to help it help humanity, not destroy it.
[01:27:35.680 --> 01:27:38.800]   Speaking of which, a funny side note.
[01:27:38.800 --> 01:27:42.480]   So Kubrick's 2001 Space Odyssey.
[01:27:42.480 --> 01:27:48.800]   What do you think about the fact that Hal 9000 was named after IBM?
[01:27:48.800 --> 01:27:50.080]   I really don't think it was.
[01:27:50.080 --> 01:27:51.600]   I know there's, I really don't.
[01:27:51.600 --> 01:27:52.080]   I've also-
[01:27:52.080 --> 01:27:53.520]   It could be more fake news.
[01:27:53.520 --> 01:27:54.320]   It's more fake news.
[01:27:54.320 --> 01:27:58.480]   I have done, I've like researched this, tried to find any evidence and people have talked to,
[01:27:58.480 --> 01:28:01.760]   was it really, one letter, it was one letter off, right?
[01:28:01.760 --> 01:28:06.320]   But people don't know H is one letter off of I, A is one letter off of B,
[01:28:06.320 --> 01:28:07.760]   and then L is one letter off of-
[01:28:07.760 --> 01:28:10.960]   That was the, I think that's a solution found afterwards, you know?
[01:28:10.960 --> 01:28:11.680]   Yeah, that's hilarious.
[01:28:11.680 --> 01:28:13.520]   But here's what I think it more was.
[01:28:13.520 --> 01:28:16.960]   I do think it's one of the early demonstrations of evil AI.
[01:28:16.960 --> 01:28:17.280]   Yeah.
[01:28:17.280 --> 01:28:18.560]   Like can be taught bad.
[01:28:18.560 --> 01:28:23.040]   I could push back on that because it's presented as evil in the movie
[01:28:23.040 --> 01:28:25.280]   because it hurts, the AI hurts people.
[01:28:25.280 --> 01:28:30.000]   But it's a really interesting ethical question because the role of Hal 9000
[01:28:30.640 --> 01:28:33.840]   is to carry out a successful mission.
[01:28:33.840 --> 01:28:38.880]   And so the question that is a human question, it's not an AI question,
[01:28:38.880 --> 01:28:40.000]   at what price?
[01:28:40.000 --> 01:28:41.760]   Humans wage war.
[01:28:41.760 --> 01:28:51.520]   They pay very heavy costs for a vision, for a goal of a future that creates a better world.
[01:28:51.520 --> 01:28:54.080]   And so that's the question certainly in space.
[01:28:54.080 --> 01:28:55.520]   Doctors ask that question all the time,
[01:28:55.520 --> 01:28:59.040]   but the limited resources, who do I allocate my time and money and efforts?
[01:28:59.040 --> 01:28:59.760]   I agree.
[01:28:59.760 --> 01:29:02.960]   Like I said, I've spent a decade talking about this question of AI ethics, right?
[01:29:02.960 --> 01:29:06.640]   And that it needs really considerable, not just attention,
[01:29:06.640 --> 01:29:12.000]   because otherwise it will mirror everything we love and everything we don't love.
[01:29:12.000 --> 01:29:14.640]   And again, and that's the beauty in the eye of the beholder, right?
[01:29:14.640 --> 01:29:16.240]   Depending your culture and everything else.
[01:29:16.240 --> 01:29:21.040]   With what you're doing and what you're going to do, how do you think about it?
[01:29:21.040 --> 01:29:24.800]   Do you think about the AI you're going to develop as having guardrails
[01:29:24.800 --> 01:29:27.520]   dictated by some of your beliefs or?
[01:29:28.240 --> 01:29:29.200]   Yeah, for sure.
[01:29:29.200 --> 01:29:33.360]   So there's so many interesting ways to do this the right way,
[01:29:33.360 --> 01:29:35.280]   and I don't think anyone has an answer.
[01:29:35.280 --> 01:29:37.920]   I tend to believe that transparency is really important.
[01:29:37.920 --> 01:29:43.040]   So I think some aspect of your work should be open-sourced
[01:29:43.040 --> 01:29:45.920]   or at least have an open-source competitor
[01:29:45.920 --> 01:29:50.720]   that creates a kind of forcing function for transparency of how you do things.
[01:29:50.720 --> 01:29:54.000]   So the other is, I tend to believe,
[01:29:54.000 --> 01:29:56.720]   maybe it's because of the podcast and I've just talked to a lot of people,
[01:29:57.440 --> 01:29:59.360]   you should know the people involved.
[01:29:59.360 --> 01:30:00.960]   I agree 100%.
[01:30:00.960 --> 01:30:03.520]   As opposed to hide behind a company wall.
[01:30:03.520 --> 01:30:05.920]   Sometimes there's a pressure, you have a PR team,
[01:30:05.920 --> 01:30:09.440]   you have to care for investors and discussion, so on, let's protect,
[01:30:09.440 --> 01:30:12.000]   let's surely not tweet, not this.
[01:30:12.000 --> 01:30:14.320]   Like, and you form this bubble
[01:30:14.320 --> 01:30:17.600]   where you have incredible engineers doing fascinating work
[01:30:17.600 --> 01:30:23.680]   and also doing work that's difficult, complex human questions being answered.
[01:30:23.680 --> 01:30:26.320]   And we don't know about any of them as a society.
[01:30:26.320 --> 01:30:28.720]   And so we can't really have that conversation.
[01:30:28.720 --> 01:30:31.440]   Even though that conversation would be great for hiring,
[01:30:31.440 --> 01:30:35.680]   it would be great for revealing the complexities of what the company is facing.
[01:30:35.680 --> 01:30:37.200]   So when the company makes mistakes,
[01:30:37.200 --> 01:30:42.000]   you understand that it wasn't malevolence or half-assedness
[01:30:42.000 --> 01:30:44.720]   and the decision-making is just a really hard problem.
[01:30:44.720 --> 01:30:47.280]   And so I think transparency is just good for everybody.
[01:30:47.280 --> 01:30:51.520]   And I mean, in general,
[01:30:51.520 --> 01:30:55.520]   just having a lot of public conversations about this is serious stuff.
[01:30:55.520 --> 01:31:00.960]   It's that AI will have a transformative impact on our society
[01:31:00.960 --> 01:31:03.280]   and it might do so very, very quickly
[01:31:03.280 --> 01:31:05.360]   through all kinds of ways we're not expecting,
[01:31:05.360 --> 01:31:07.760]   which is social media recommendation systems.
[01:31:07.760 --> 01:31:11.760]   They at scale have impact on the way we think,
[01:31:11.760 --> 01:31:17.360]   on the way we consume news and our growth,
[01:31:17.360 --> 01:31:19.520]   like the kind of stuff we consume to grow and learn
[01:31:19.520 --> 01:31:22.800]   and become better human beings, all of that, that's all AI.
[01:31:22.800 --> 01:31:28.000]   And then obviously the tools that run companies on which we depend,
[01:31:28.000 --> 01:31:29.600]   the infrastructure in which we depend,
[01:31:29.600 --> 01:31:32.640]   we need to know all about those AI decisions.
[01:31:32.640 --> 01:31:34.160]   And it's not as simple as,
[01:31:34.160 --> 01:31:38.800]   well, we don't want the AI to say these specific set of bad things.
[01:31:38.800 --> 01:31:39.280]   - Right.
[01:31:39.280 --> 01:31:41.760]   - It's unfortunately, I don't believe it's possible to--
[01:31:41.760 --> 01:31:42.320]   - It's not.
[01:31:42.320 --> 01:31:48.480]   - Prevent evil or bad things
[01:31:48.480 --> 01:31:51.760]   by creating a set of cold mathematical rules.
[01:31:51.760 --> 01:31:52.240]   - Yeah.
[01:31:52.240 --> 01:31:54.480]   - Unfortunately, it's all fuzzy and gray areas.
[01:31:54.480 --> 01:31:55.520]   It's all a giant mess.
[01:31:55.520 --> 01:31:56.080]   - It is.
[01:31:56.080 --> 01:31:57.440]   I mean, you think about it like a knife.
[01:31:57.440 --> 01:31:59.440]   A knife can do good and a knife can do bad.
[01:31:59.440 --> 01:32:01.200]   Okay, you can't, it's very hard.
[01:32:01.200 --> 01:32:02.160]   - You can't ban knives.
[01:32:02.160 --> 01:32:03.280]   - You can't ban knives.
[01:32:03.280 --> 01:32:09.200]   And that this is, I think back, it was probably 20, I don't know, 15, 16,
[01:32:09.200 --> 01:32:11.360]   we did principles of trust and transparency.
[01:32:11.360 --> 01:32:14.480]   Notice the word transparency, that belief that with AI,
[01:32:14.480 --> 01:32:15.760]   it should be explainable.
[01:32:15.760 --> 01:32:16.960]   You should know who taught it.
[01:32:16.960 --> 01:32:19.360]   You should know the data that went into training it.
[01:32:19.360 --> 01:32:21.680]   You should know who, how it was written.
[01:32:21.680 --> 01:32:24.160]   If it's being used, you have a right to know these things.
[01:32:24.160 --> 01:32:26.960]   And I think those are pretty, to this day,
[01:32:26.960 --> 01:32:29.920]   really powerful principles to be followed, right?
[01:32:29.920 --> 01:32:31.760]   And part of it, we ended up writing,
[01:32:31.760 --> 01:32:34.240]   'cause here we were when we were working on particularly healthcare,
[01:32:34.240 --> 01:32:38.320]   like, okay, you care who trained it and what, and where did,
[01:32:38.320 --> 01:32:41.280]   and that's sort of, you know, that comes to your mind.
[01:32:41.280 --> 01:32:42.480]   You're like, yeah, that makes a lot of sense
[01:32:42.480 --> 01:32:43.600]   for something important like that.
[01:32:43.600 --> 01:32:47.120]   But it just in general, people won't trust the technologies,
[01:32:47.120 --> 01:32:50.480]   I don't think, unless they have transparency into those things.
[01:32:50.480 --> 01:32:51.840]   In the end, they won't really trust it.
[01:32:51.840 --> 01:32:55.280]   - I think a lot of people would like to know, sort of,
[01:32:55.280 --> 01:32:58.960]   'cause a lot of us, I certainly do,
[01:32:58.960 --> 01:33:02.240]   suffer from imposter syndrome, that self-critical brain.
[01:33:02.240 --> 01:33:05.760]   So, you know, taking that big step into leadership,
[01:33:05.760 --> 01:33:08.240]   did you at times suffer from imposter syndrome?
[01:33:08.240 --> 01:33:09.600]   Like, how did I get here?
[01:33:09.600 --> 01:33:10.880]   Do I really belong here?
[01:33:10.880 --> 01:33:13.440]   Or were you able to summon the courage
[01:33:13.440 --> 01:33:15.680]   and sort of the confidence to really step up?
[01:33:15.680 --> 01:33:19.200]   - Hmm.
[01:33:19.200 --> 01:33:21.600]   I think that's very natural for someone.
[01:33:21.600 --> 01:33:23.600]   Like, no matter, like the bigger the job gets,
[01:33:23.600 --> 01:33:27.440]   you turn and you look to the left and the right
[01:33:27.440 --> 01:33:29.520]   and you see people around you and you think,
[01:33:29.520 --> 01:33:30.960]   what am I doing here, right?
[01:33:30.960 --> 01:33:32.960]   But then you remember what you do
[01:33:32.960 --> 01:33:36.000]   and there's no one else doing it.
[01:33:36.000 --> 01:33:38.240]   And so you get that confidence.
[01:33:38.240 --> 01:33:41.680]   So I do hear a lot of people talk about imposter syndrome,
[01:33:41.680 --> 01:33:42.480]   right?
[01:33:42.560 --> 01:33:47.200]   And I kind of, actually this past year,
[01:33:47.200 --> 01:33:49.280]   I've spent some time helping people on that topic.
[01:33:49.280 --> 01:33:54.720]   And part of the stress, you have to believe
[01:33:54.720 --> 01:33:58.240]   you have a right to be there like anyone else does
[01:33:58.240 --> 01:34:00.000]   if you've prepared for that moment, you know?
[01:34:00.000 --> 01:34:02.160]   And so it's a bit more of a,
[01:34:02.160 --> 01:34:04.800]   I know it's hard to say it,
[01:34:04.800 --> 01:34:06.960]   like a confidence thing more than anything else.
[01:34:06.960 --> 01:34:09.120]   So yes, there are times I look around,
[01:34:09.120 --> 01:34:10.800]   but then I think, wow,
[01:34:10.800 --> 01:34:12.400]   I'm in a position to make something change.
[01:34:12.400 --> 01:34:15.680]   So I can't say I have ever really dwelled
[01:34:15.680 --> 01:34:16.640]   on that feeling for long.
[01:34:16.640 --> 01:34:18.000]   - Okay, so you just focus on the work,
[01:34:18.000 --> 01:34:18.960]   I have an opportunity, I'm gonna stop propagating.
[01:34:18.960 --> 01:34:21.440]   - You know, it's good or bad, I just focus on the work.
[01:34:21.440 --> 01:34:22.960]   Yeah, good or bad, yeah.
[01:34:22.960 --> 01:34:26.320]   - One important lesson you said you learned from your mom
[01:34:26.320 --> 01:34:29.200]   is never let anyone else define you.
[01:34:29.200 --> 01:34:30.560]   Only you define who you are.
[01:34:30.560 --> 01:34:34.320]   So what's the trajectory, let's say,
[01:34:34.320 --> 01:34:36.160]   of your self-definition journey,
[01:34:36.160 --> 01:34:38.160]   of you discovering who you are?
[01:34:40.000 --> 01:34:42.960]   From having that very difficult upbringing.
[01:34:42.960 --> 01:34:45.920]   - You know, they say pivotal moments happen
[01:34:45.920 --> 01:34:47.760]   and you don't realize it when they're happening.
[01:34:47.760 --> 01:34:51.920]   So most of my, I feel like most of my self-discovery,
[01:34:51.920 --> 01:34:55.440]   it's been like something happens in a year or two
[01:34:55.440 --> 01:34:58.160]   or some number later, I look back on it and say,
[01:34:58.160 --> 01:35:00.320]   you know, I learned this from that.
[01:35:00.320 --> 01:35:02.160]   It's like not in the moment always with me.
[01:35:02.160 --> 01:35:04.080]   That could just be how I am.
[01:35:04.080 --> 01:35:06.160]   So I feel like it's been,
[01:35:06.160 --> 01:35:09.600]   know yourself, it's a good thing, right?
[01:35:09.600 --> 01:35:11.520]   I've actually heard you say that on different podcasts
[01:35:11.520 --> 01:35:13.360]   when you ask people questions, you're like,
[01:35:13.360 --> 01:35:16.080]   well, it depends, you know, like know yourself a bit, right?
[01:35:16.080 --> 01:35:18.000]   - It's hard to know who you are though.
[01:35:18.000 --> 01:35:19.680]   There's a lot of things, like you said,
[01:35:19.680 --> 01:35:23.920]   like for me, there's moods when you're super self-critical,
[01:35:23.920 --> 01:35:25.440]   sometimes you're super confident,
[01:35:25.440 --> 01:35:28.080]   and there's many, sometimes you're emotional,
[01:35:28.080 --> 01:35:29.520]   sometimes you're cool under pressure,
[01:35:29.520 --> 01:35:32.480]   and all those are the same human being.
[01:35:32.480 --> 01:35:33.600]   - Yeah, and I think that's fine.
[01:35:33.600 --> 01:35:35.600]   (Lex laughing)
[01:35:35.600 --> 01:35:37.040]   Self-awareness, that's different.
[01:35:37.200 --> 01:35:40.000]   - Was there societal expectations and norms
[01:35:40.000 --> 01:35:44.800]   regarding gender that you felt in your career?
[01:35:44.800 --> 01:35:46.080]   You've spoken to that a little bit,
[01:35:46.080 --> 01:35:47.600]   but was there some aspect of that
[01:35:47.600 --> 01:35:50.480]   that was constraining, empowering, or both?
[01:35:50.480 --> 01:35:55.360]   - You know, I chose to never look at it, okay?
[01:35:55.360 --> 01:35:56.880]   Now, whether that is right or wrong,
[01:35:56.880 --> 01:35:58.720]   and again, I'm a product of the '70s,
[01:35:58.720 --> 01:36:02.480]   and '70s and the '80s where I think I was surrounded,
[01:36:02.480 --> 01:36:04.160]   all the other women around me
[01:36:04.160 --> 01:36:06.240]   viewed our way to get ahead was just to work hard.
[01:36:06.240 --> 01:36:07.200]   Work hard, work hard,
[01:36:07.200 --> 01:36:10.080]   and that was the way you differentiated yourself.
[01:36:10.080 --> 01:36:12.320]   And that's obvious it did help.
[01:36:12.320 --> 01:36:13.520]   I mean, there's no doubt about it.
[01:36:13.520 --> 01:36:15.680]   You're always, you know, you learned a lot of things,
[01:36:15.680 --> 01:36:17.040]   which qualified opened up another door,
[01:36:17.040 --> 01:36:18.080]   opened up another door.
[01:36:18.080 --> 01:36:22.560]   I'm very remindful that I have worked for companies
[01:36:22.560 --> 01:36:25.920]   that are very steeped in those values of equal opportunity.
[01:36:25.920 --> 01:36:28.640]   And so nothing remarkable about that.
[01:36:28.640 --> 01:36:31.040]   And I mean, when I was a wee kid,
[01:36:31.040 --> 01:36:33.840]   I'm taught hire a diverse team.
[01:36:33.840 --> 01:36:36.960]   I get evaluated for it.
[01:36:36.960 --> 01:36:39.920]   I get evaluated if my team has built up their skills.
[01:36:39.920 --> 01:36:42.080]   So this is, you know, when you're really formative,
[01:36:42.080 --> 01:36:44.880]   you're in a culture that that's what it's valuing, right?
[01:36:44.880 --> 01:36:46.480]   So it becomes part of you.
[01:36:46.480 --> 01:36:49.040]   So I say sometimes to Chagrin,
[01:36:49.040 --> 01:36:51.760]   "Did I ever feel I was held back for that reason?"
[01:36:51.760 --> 01:36:53.120]   Now, were there plenty of times when,
[01:36:53.120 --> 01:36:55.680]   you know, I write about a few of the stories in the book,
[01:36:55.680 --> 01:36:58.160]   I'm laying cables at night and the guys are at the bar.
[01:36:58.160 --> 01:37:00.640]   Now, I didn't really wanna go with them to the bar anyways.
[01:37:00.640 --> 01:37:03.360]   They'd be like, "We'll be back to get you, you know, bye."
[01:37:03.360 --> 01:37:04.080]   And I'm like, "Okay."
[01:37:04.080 --> 01:37:06.240]   I mean, I learned a lot.
[01:37:06.240 --> 01:37:07.360]   So it didn't.
[01:37:07.360 --> 01:37:09.280]   Now, all that said,
[01:37:09.280 --> 01:37:11.600]   back to my earlier story about being a role model,
[01:37:11.600 --> 01:37:15.120]   you know, it would be foolish to not believe
[01:37:15.120 --> 01:37:16.800]   that there were times that that mattered.
[01:37:16.800 --> 01:37:21.840]   And I would say two things,
[01:37:21.840 --> 01:37:23.200]   even not that long ago,
[01:37:23.200 --> 01:37:25.600]   you know, a colleague called me
[01:37:25.600 --> 01:37:30.160]   and I was talking about media and about women CEOs
[01:37:30.160 --> 01:37:31.840]   and said, "Do you notice that sometimes
[01:37:31.840 --> 01:37:33.920]   when it's a woman CEO, they call the person by name
[01:37:33.920 --> 01:37:36.400]   and when it's a man, they call the company out,
[01:37:36.400 --> 01:37:39.200]   not the person's name exactly associated with the issue."
[01:37:39.200 --> 01:37:42.880]   And I said, "Yeah, well, I think you have to just understand
[01:37:42.880 --> 01:37:45.840]   much of what you do, it will be magnified
[01:37:45.840 --> 01:37:47.200]   'cause there are so few of you.
[01:37:47.200 --> 01:37:49.920]   And sometimes it will be, you know,
[01:37:49.920 --> 01:37:52.160]   really can be blown out of proportion, right?
[01:37:52.160 --> 01:37:54.240]   And so that can happen
[01:37:54.240 --> 01:37:57.520]   and you gotta learn in which way.
[01:37:57.520 --> 01:37:59.840]   Now, all that said, on gender,
[01:37:59.840 --> 01:38:01.440]   it is an interesting thing with the book
[01:38:01.440 --> 01:38:03.680]   as I've talked to, you know, having a book,
[01:38:03.680 --> 01:38:06.720]   even some of my best friends, the first reaction is,
[01:38:06.720 --> 01:38:08.160]   "I can't wait for my daughter to read it."
[01:38:08.160 --> 01:38:11.040]   I say, "Well, that's interesting,
[01:38:11.040 --> 01:38:12.000]   do you think you could read it?"
[01:38:12.000 --> 01:38:14.320]   - Yeah, it's fascinating.
[01:38:14.320 --> 01:38:15.680]   - It's an interesting reaction.
[01:38:15.680 --> 01:38:17.120]   And here I am 40 years later,
[01:38:17.120 --> 01:38:18.960]   that's an interesting reaction, right?
[01:38:18.960 --> 01:38:21.680]   And I say, "No, the book, I really worked hard
[01:38:21.680 --> 01:38:23.200]   to write it for everyone,
[01:38:23.200 --> 01:38:25.280]   I just happened to be a woman, right?"
[01:38:25.280 --> 01:38:27.920]   But there's still that there.
[01:38:27.920 --> 01:38:30.640]   And so look, until I think people see
[01:38:30.640 --> 01:38:32.960]   and never feel that they have a,
[01:38:32.960 --> 01:38:34.160]   it doesn't even matter whether there's a woman,
[01:38:34.160 --> 01:38:36.080]   it could be another diverse group that feels it.
[01:38:36.080 --> 01:38:38.640]   It's okay to ask those questions.
[01:38:38.640 --> 01:38:40.480]   And that's why actually I'm okay talking about it
[01:38:40.480 --> 01:38:42.800]   because there were times I felt it, right?
[01:38:42.800 --> 01:38:45.360]   There were times in my life on my looks or my weight
[01:38:45.360 --> 01:38:48.320]   or my clothing or endless numbers of things
[01:38:48.320 --> 01:38:49.920]   that people would comment on
[01:38:49.920 --> 01:38:51.680]   that they would not have commented on
[01:38:51.680 --> 01:38:53.600]   if it was someone else.
[01:38:53.600 --> 01:38:58.000]   Now, on the other hand, when there's so few of you
[01:38:58.640 --> 01:39:02.000]   and there's good and bad,
[01:39:02.000 --> 01:39:03.600]   I mean, there's benefit to that too, right?
[01:39:03.600 --> 01:39:07.600]   If you do good work, it's easier to be recognized.
[01:39:07.600 --> 01:39:09.600]   And so a pro and a con,
[01:39:09.600 --> 01:39:13.040]   and I think I've just grown up believing
[01:39:13.040 --> 01:39:15.680]   like my advice to young women going to engineering,
[01:39:15.680 --> 01:39:16.880]   not 'cause you're gonna be an engineer,
[01:39:16.880 --> 01:39:18.240]   it teaches you to solve problems
[01:39:18.240 --> 01:39:20.480]   and anything new job you do is gonna be solving problems.
[01:39:20.480 --> 01:39:24.480]   Things like that are what I take away from that
[01:39:24.480 --> 01:39:25.200]   in that journey.
[01:39:25.200 --> 01:39:27.920]   - It is interesting that I hear from women
[01:39:28.000 --> 01:39:31.120]   that even on this podcast,
[01:39:31.120 --> 01:39:35.760]   when I talk to incredible women like yourself,
[01:39:35.760 --> 01:39:37.920]   it is inspiring to young women to hear.
[01:39:37.920 --> 01:39:39.280]   I mean, you like to see,
[01:39:39.280 --> 01:39:41.280]   you talk to somebody from Turkey
[01:39:41.280 --> 01:39:42.960]   and then Turkish people all get excited.
[01:39:42.960 --> 01:39:44.480]   - It's so true.
[01:39:44.480 --> 01:39:46.800]   - So you get like somebody that looks like you,
[01:39:46.800 --> 01:39:50.240]   somebody that, and the category could be tiny,
[01:39:50.240 --> 01:39:52.400]   it can be huge.
[01:39:52.400 --> 01:39:53.920]   That's just the reality of the world.
[01:39:53.920 --> 01:39:54.960]   - It is the reality of the world.
[01:39:54.960 --> 01:39:56.880]   And the work I do now to put,
[01:39:57.440 --> 01:39:58.560]   this group called One Tent,
[01:39:58.560 --> 01:40:00.800]   put 1 million black employees into the middle class
[01:40:00.800 --> 01:40:01.840]   without college degrees,
[01:40:01.840 --> 01:40:04.160]   get them the right skills, upwardly mobile jobs.
[01:40:04.160 --> 01:40:07.040]   So one of my last years,
[01:40:07.040 --> 01:40:09.440]   we had been working on,
[01:40:09.440 --> 01:40:11.520]   it just did regular leadership session at IBM
[01:40:11.520 --> 01:40:12.720]   and had our black colleagues,
[01:40:12.720 --> 01:40:14.080]   we're talking about what did it feel like
[01:40:14.080 --> 01:40:15.360]   to be a black leader?
[01:40:15.360 --> 01:40:18.160]   And here, these are extremely accomplished people.
[01:40:18.160 --> 01:40:20.320]   And I can remember very well
[01:40:20.320 --> 01:40:21.680]   when telling a story about,
[01:40:21.680 --> 01:40:24.960]   look, I felt if I failed or succeeded,
[01:40:24.960 --> 01:40:26.240]   it's not just me,
[01:40:26.240 --> 01:40:27.760]   it came from a country in Africa.
[01:40:27.760 --> 01:40:30.480]   I feel like the whole country is on my shoulder,
[01:40:30.480 --> 01:40:31.920]   my success or failure.
[01:40:31.920 --> 01:40:32.880]   That's a burden.
[01:40:32.880 --> 01:40:34.720]   I mean, like I don't feel that burden.
[01:40:34.720 --> 01:40:36.880]   Not true, as a woman CEO,
[01:40:36.880 --> 01:40:37.680]   I did feel like,
[01:40:37.680 --> 01:40:40.400]   even the headlines when I was named said,
[01:40:40.400 --> 01:40:42.800]   her appointment will either,
[01:40:42.800 --> 01:40:44.960]   her success or failure will be a statement
[01:40:44.960 --> 01:40:46.400]   for the whole gender kind of thing.
[01:40:46.400 --> 01:40:49.040]   And I didn't dwell on it,
[01:40:49.040 --> 01:40:50.000]   but I meant that's,
[01:40:50.000 --> 01:40:51.200]   but I could see how people,
[01:40:51.200 --> 01:40:52.480]   like you said, it could be a small group,
[01:40:52.480 --> 01:40:53.520]   could be whatever.
[01:40:53.520 --> 01:40:55.600]   And so that is a lot of pressure on people
[01:40:55.600 --> 01:40:56.640]   and they need role models.
[01:40:56.640 --> 01:40:57.760]   You're a role model for people.
[01:40:57.760 --> 01:40:58.800]   Look at what you're able to do.
[01:40:58.800 --> 01:40:59.840]   You do these podcasts,
[01:40:59.840 --> 01:41:01.840]   you understand your science very well,
[01:41:01.840 --> 01:41:02.960]   you're very well prepared,
[01:41:02.960 --> 01:41:05.200]   your ability to translate it to people.
[01:41:05.200 --> 01:41:07.840]   That's not an insignificant thing.
[01:41:07.840 --> 01:41:08.480]   And you may think,
[01:41:08.480 --> 01:41:10.480]   oh, is that about the power of me?
[01:41:10.480 --> 01:41:11.360]   Not really, right?
[01:41:11.360 --> 01:41:13.040]   And you obviously believe,
[01:41:13.040 --> 01:41:14.400]   you don't do this because
[01:41:14.400 --> 01:41:16.000]   you just like sitting at a microphone.
[01:41:16.000 --> 01:41:17.040]   You do it because you think,
[01:41:17.040 --> 01:41:18.720]   okay, if I can get people to say things
[01:41:18.720 --> 01:41:20.160]   that are really valuable to other people,
[01:41:20.160 --> 01:41:21.360]   they're gonna learn something.
[01:41:21.360 --> 01:41:22.240]   I assume that is,
[01:41:22.240 --> 01:41:24.720]   I mean, you never told me my interpretation is,
[01:41:24.720 --> 01:41:26.320]   that's why you do this podcast,
[01:41:26.320 --> 01:41:28.400]   that you feel like in service of other people
[01:41:28.400 --> 01:41:30.720]   that you can bring them something unique
[01:41:30.720 --> 01:41:31.760]   by the way you do this.
[01:41:31.760 --> 01:41:33.840]   Now, I should ask you, why do you do it?
[01:41:33.840 --> 01:41:34.800]   That's my impression.
[01:41:34.800 --> 01:41:36.480]   - By the way, can I just comment on the fact
[01:41:36.480 --> 01:41:38.400]   that you keep asking me really hard questions?
[01:41:38.400 --> 01:41:39.200]   (both laughing)
[01:41:39.200 --> 01:41:40.960]   I really appreciate it.
[01:41:40.960 --> 01:41:41.520]   - It's in your own medicine.
[01:41:41.520 --> 01:41:42.560]   - I love this.
[01:41:42.560 --> 01:41:43.920]   I'm really honored by it.
[01:41:43.920 --> 01:41:48.000]   As a fan of a podcast myself,
[01:41:48.000 --> 01:41:51.920]   what I hope is to talk to people like you
[01:41:51.920 --> 01:41:55.280]   and to show that you are a fascinating
[01:41:55.280 --> 01:41:56.960]   and beautiful human being
[01:41:56.960 --> 01:41:59.760]   outside of your actual accomplishments also.
[01:41:59.760 --> 01:42:02.480]   So sometimes people are very focused on
[01:42:02.480 --> 01:42:05.920]   very specific things about, like you said, science,
[01:42:05.920 --> 01:42:07.840]   like what the actual work is,
[01:42:07.840 --> 01:42:10.640]   whether it's nuclear fusion or it's GIGPT.
[01:42:10.640 --> 01:42:13.200]   I just wanna show that it's,
[01:42:13.200 --> 01:42:17.040]   because I see it at MIT and everywhere,
[01:42:17.040 --> 01:42:19.200]   it's just human beings trying their best.
[01:42:19.200 --> 01:42:22.400]   They're flawed, but just realizing
[01:42:22.400 --> 01:42:25.360]   that all of these very well accomplished people
[01:42:25.360 --> 01:42:26.160]   are all the same.
[01:42:26.160 --> 01:42:28.160]   - Yeah, that's a very good, well said.
[01:42:28.160 --> 01:42:30.640]   - And then so then regular people and young people,
[01:42:30.640 --> 01:42:33.440]   they're able to see, I can do this too.
[01:42:33.440 --> 01:42:34.720]   I can have a very big impact.
[01:42:34.720 --> 01:42:35.120]   - No imposter syndrome, right?
[01:42:35.120 --> 01:42:36.160]   - Yeah, exactly.
[01:42:36.160 --> 01:42:38.240]   It's like we're all kind of imposters.
[01:42:38.240 --> 01:42:39.600]   We're all like trying to figure it out.
[01:42:39.600 --> 01:42:41.680]   - To a certain degree.
[01:42:41.680 --> 01:42:44.000]   - So let me just ask you about family.
[01:42:44.000 --> 01:42:46.080]   You wrote that, "My family still jokes
[01:42:46.080 --> 01:42:49.040]   "that the reason I never had children on my own
[01:42:49.040 --> 01:42:51.600]   "was because I had already raised my family."
[01:42:51.600 --> 01:42:52.720]   They're right.
[01:42:52.720 --> 01:42:54.400]   So this is talking to you about upbringing,
[01:42:54.400 --> 01:42:55.680]   but in general, what was your,
[01:42:55.680 --> 01:42:58.800]   you know, leading a giant company,
[01:42:58.800 --> 01:43:03.600]   what was the right place to find a work-life balance for you
[01:43:03.600 --> 01:43:05.840]   to have time for family,
[01:43:05.840 --> 01:43:09.600]   have time for away from work and be successful?
[01:43:09.600 --> 01:43:10.560]   - So I had to learn that.
[01:43:10.560 --> 01:43:12.320]   And I might've said, you know,
[01:43:12.320 --> 01:43:14.400]   you're the only one that can determine
[01:43:14.400 --> 01:43:15.760]   your own work-life balance.
[01:43:15.760 --> 01:43:16.960]   Companies are innate things.
[01:43:16.960 --> 01:43:18.720]   I mean, they will take everything they can from you.
[01:43:18.720 --> 01:43:19.760]   And it's not a bad thing.
[01:43:19.760 --> 01:43:21.200]   They just will, as will bosses.
[01:43:21.200 --> 01:43:23.760]   I mean, you give it, they'll take it.
[01:43:23.760 --> 01:43:26.000]   And when people ask for, you know, I need a roof,
[01:43:26.000 --> 01:43:28.640]   I'm like, okay, I had to come with terms with,
[01:43:28.640 --> 01:43:31.760]   the criminal was me if I needed that balance.
[01:43:31.760 --> 01:43:33.200]   I had to set those boundaries.
[01:43:33.200 --> 01:43:36.160]   And so when I comment about a family,
[01:43:36.160 --> 01:43:39.840]   because I am in extreme awe of people with children who work,
[01:43:39.840 --> 01:43:43.360]   it is a extremely hard thing to do.
[01:43:43.360 --> 01:43:44.400]   I watch my siblings.
[01:43:44.400 --> 01:43:45.760]   I love my nieces and nephews.
[01:43:45.760 --> 01:43:50.080]   And, whew, you know, A, the emotional,
[01:43:50.080 --> 01:43:52.160]   their pain is your pain every minute of the day.
[01:43:52.160 --> 01:43:54.000]   And then you still have a job on top of it.
[01:43:54.000 --> 01:43:56.960]   And so when my mom had to go back to school
[01:43:56.960 --> 01:43:58.960]   and had to work, I was the one.
[01:43:58.960 --> 01:44:01.440]   And so when she couldn't go to the teacher meeting,
[01:44:01.440 --> 01:44:02.480]   I went to the teacher meeting.
[01:44:02.480 --> 01:44:04.880]   And so in some ways, there's an age gap
[01:44:04.880 --> 01:44:06.720]   between my brother and I and my other two sisters.
[01:44:06.720 --> 01:44:11.280]   And so I'm still, they still call me mama bear even.
[01:44:11.280 --> 01:44:14.240]   I mean, I'm extremely protective of all of them.
[01:44:14.240 --> 01:44:16.880]   And it is as if I had raised them.
[01:44:16.880 --> 01:44:18.480]   And my mom did a great job raising them.
[01:44:18.480 --> 01:44:20.400]   I didn't, but I was there.
[01:44:20.400 --> 01:44:24.640]   And so when it came time to have children,
[01:44:24.640 --> 01:44:28.000]   and my husband came from a family where his father died
[01:44:28.000 --> 01:44:29.280]   and was raised by a single mother,
[01:44:29.280 --> 01:44:34.400]   very similar end point, different reasons why he ended up,
[01:44:34.400 --> 01:44:37.120]   you know, his father did not abandon them.
[01:44:37.120 --> 01:44:41.600]   And I don't want people to believe to do my job,
[01:44:41.600 --> 01:44:42.800]   you can have no children.
[01:44:42.800 --> 01:44:43.600]   That is not right.
[01:44:44.240 --> 01:44:46.240]   I know other great women CEOs,
[01:44:46.240 --> 01:44:49.120]   Marilyn Hewson, who ran Lockheed Martin,
[01:44:49.120 --> 01:44:50.400]   extremely technical company,
[01:44:50.400 --> 01:44:52.400]   Mary Barra, who runs General Motors,
[01:44:52.400 --> 01:44:54.000]   Ellen Coleman, who run DuPont.
[01:44:54.000 --> 01:44:55.520]   These are all my friends to this day.
[01:44:55.520 --> 01:44:59.200]   And they've been fantastic mothers
[01:44:59.200 --> 01:45:01.440]   and husbands, good parents, right?
[01:45:01.440 --> 01:45:04.640]   And so I talk about it because it was a choice we made.
[01:45:04.640 --> 01:45:07.440]   And so, you know, we both felt, look,
[01:45:07.440 --> 01:45:10.160]   we'd reached a point where for his reasons,
[01:45:10.160 --> 01:45:12.880]   what he had to do, I'd already felt that way.
[01:45:12.880 --> 01:45:14.240]   And that we were comfortable
[01:45:14.240 --> 01:45:15.440]   just being great aunts and uncles,
[01:45:15.440 --> 01:45:16.640]   and I'm a great aunt, you know?
[01:45:16.640 --> 01:45:19.040]   Well, I like to think that for my little guys,
[01:45:19.040 --> 01:45:21.920]   and they're older now, but lots of them.
[01:45:21.920 --> 01:45:26.240]   And there's no doubt though, the choices we made,
[01:45:26.240 --> 01:45:29.680]   Mark and I, that that made it easier
[01:45:29.680 --> 01:45:31.040]   for me to focus on work.
[01:45:31.040 --> 01:45:33.920]   I mean, it's just math, you know,
[01:45:33.920 --> 01:45:36.160]   when you've got less people to have to take care of.
[01:45:36.160 --> 01:45:40.000]   And so I'm very considerate of that.
[01:45:40.000 --> 01:45:41.520]   And I think much of it informed
[01:45:41.520 --> 01:45:42.960]   many of the policies I put into,
[01:45:42.960 --> 01:45:44.720]   because I had such great empathy
[01:45:44.720 --> 01:45:46.400]   for those who then still had
[01:45:46.400 --> 01:45:47.920]   these other responsibilities.
[01:45:47.920 --> 01:45:49.920]   And I desperately wanted them all
[01:45:49.920 --> 01:45:51.440]   to stay in the workforce.
[01:45:51.440 --> 01:45:54.240]   So I can remember, and my siblings have been
[01:45:54.240 --> 01:45:56.480]   more successful than I, by the way,
[01:45:56.480 --> 01:45:57.760]   I mean, to my mother's credit.
[01:45:57.760 --> 01:46:02.400]   And my one sister who, you know,
[01:46:02.400 --> 01:46:04.160]   went to Northwestern, has an MBA,
[01:46:04.160 --> 01:46:06.560]   built some of the most sophisticated systems.
[01:46:06.560 --> 01:46:09.840]   She spent her whole career at Accenture
[01:46:09.840 --> 01:46:11.920]   and just recently retired as a chief executive
[01:46:11.920 --> 01:46:13.280]   of all of consulting.
[01:46:13.280 --> 01:46:15.280]   But at one point she took off time
[01:46:15.280 --> 01:46:16.800]   to spend with her family
[01:46:16.800 --> 01:46:18.480]   and then went to go back to work.
[01:46:18.480 --> 01:46:20.000]   She's talking to me and she's like,
[01:46:20.000 --> 01:46:21.360]   "I don't know if I should go back to work.
[01:46:21.360 --> 01:46:22.320]   You know, maybe life's path,
[01:46:22.320 --> 01:46:23.920]   you know, technology goes so fast.
[01:46:23.920 --> 01:46:24.800]   It's been a few years."
[01:46:24.800 --> 01:46:26.640]   I'm sitting there like,
[01:46:26.640 --> 01:46:28.800]   "What are you talking about?"
[01:46:28.800 --> 01:46:30.800]   I'm like, you know, look at her credentials.
[01:46:30.800 --> 01:46:32.640]   They're far outstanding.
[01:46:32.640 --> 01:46:34.640]   I'm like, and I thought to myself,
[01:46:34.640 --> 01:46:36.320]   like, ding, one of those moments,
[01:46:36.320 --> 01:46:38.560]   if my own sister feels that way
[01:46:38.560 --> 01:46:40.080]   with all her credentials,
[01:46:40.080 --> 01:46:42.480]   I'll bet I went back to work the next day
[01:46:42.480 --> 01:46:43.840]   and I said, "Hey, pull for me."
[01:46:43.840 --> 01:46:45.040]   All the people who've left
[01:46:45.040 --> 01:46:46.320]   for parental reasons
[01:46:46.320 --> 01:46:48.640]   and or whatever, family reasons,
[01:46:48.640 --> 01:46:49.360]   didn't come back.
[01:46:49.360 --> 01:46:51.920]   And it began a program of return ships.
[01:46:51.920 --> 01:46:53.040]   And I can't tell you how many,
[01:46:53.040 --> 01:46:53.840]   in men and women,
[01:46:53.840 --> 01:46:56.240]   was because they didn't feel confident to come back.
[01:46:56.240 --> 01:46:58.800]   They thought technology passed them by.
[01:46:58.800 --> 01:47:00.320]   Okay, we said, "It's three months.
[01:47:00.320 --> 01:47:01.600]   You could stay one month, three months,
[01:47:01.600 --> 01:47:02.560]   it doesn't matter."
[01:47:02.560 --> 01:47:03.680]   Well, a lot of people like one day,
[01:47:03.680 --> 01:47:05.200]   they're like, "You're right, not that much happened."
[01:47:05.200 --> 01:47:06.000]   (laughing)
[01:47:06.000 --> 01:47:07.440]   It happened, but I caught up.
[01:47:07.440 --> 01:47:08.560]   I actually know more than I think.
[01:47:08.560 --> 01:47:14.240]   So, it was a long answer to your question about,
[01:47:14.240 --> 01:47:16.240]   I didn't, but I am so empathetic
[01:47:16.240 --> 01:47:19.120]   and I am in awe of what they are able to do.
[01:47:19.120 --> 01:47:21.520]   And it made me then, I think,
[01:47:21.520 --> 01:47:23.440]   more empathetic to the policies and the like
[01:47:23.440 --> 01:47:24.960]   around that topic,
[01:47:24.960 --> 01:47:26.640]   so you could keep great people in the workforce.
[01:47:26.640 --> 01:47:28.560]   - So you mentioned your friends with Mayabara,
[01:47:28.560 --> 01:47:30.480]   the CEO of GM.
[01:47:30.480 --> 01:47:33.360]   - I didn't mean to name drop.
[01:47:33.360 --> 01:47:35.680]   - No, I love her, she's amazing.
[01:47:35.680 --> 01:47:37.040]   So I just wanted to, I'm just curious,
[01:47:37.040 --> 01:47:37.600]   do you guys-- - I'll tell Mary
[01:47:37.600 --> 01:47:39.040]   she should do your podcast.
[01:47:39.040 --> 01:47:40.080]   - We'll make it happen, but--
[01:47:40.080 --> 01:47:41.200]   - She's a great leader.
[01:47:41.200 --> 01:47:43.120]   I tell Mary what I think of her
[01:47:43.120 --> 01:47:45.840]   is I think she's one of the most authentic leaders out there,
[01:47:45.840 --> 01:47:46.800]   most authentic.
[01:47:46.800 --> 01:47:49.920]   - I mean, just very different companies, huge challenges.
[01:47:49.920 --> 01:47:51.360]   - I worked there first, though, remember, right?
[01:47:51.360 --> 01:47:55.120]   So I'm very, in some ways, I'm very beholden, right?
[01:47:55.120 --> 01:47:57.280]   I'm very appreciative of what they did.
[01:47:57.280 --> 01:47:58.560]   I mean, Mary and I are circa the same,
[01:47:58.560 --> 01:48:01.680]   well, I'm a bit older, so, but circa that genre.
[01:48:01.680 --> 01:48:03.040]   - Do you exchange wisdoms?
[01:48:03.040 --> 01:48:04.000]   - Oh yeah, yeah.
[01:48:04.000 --> 01:48:07.040]   When you do anything hard, it takes time and perseverance,
[01:48:07.040 --> 01:48:07.840]   like we talked about.
[01:48:07.840 --> 01:48:13.440]   And you can get that, where do you get the fuel for it?
[01:48:13.440 --> 01:48:15.360]   You can either get it from your attitude,
[01:48:15.360 --> 01:48:18.080]   or you can get it from your network or your relationships.
[01:48:18.080 --> 01:48:21.840]   And I'm a firm believer relationships are from what you give,
[01:48:21.840 --> 01:48:22.880]   not what you get.
[01:48:22.880 --> 01:48:25.680]   Meaning you give, trust me, they will come back
[01:48:25.680 --> 01:48:27.360]   at the time they need to come back to you
[01:48:27.360 --> 01:48:28.880]   at these moments in life.
[01:48:28.880 --> 01:48:33.200]   If you focus on how can I bring lex value,
[01:48:33.200 --> 01:48:35.680]   there'll be a day I need lex, he will be back.
[01:48:35.680 --> 01:48:38.480]   And so to those women,
[01:48:38.480 --> 01:48:40.560]   to me, relationships are not transactional.
[01:48:40.560 --> 01:48:44.320]   And it's a proof that to this day,
[01:48:44.320 --> 01:48:46.800]   even though I'm no longer still active as a CEO,
[01:48:46.800 --> 01:48:48.480]   these are all still my friends.
[01:48:48.480 --> 01:48:51.040]   And we are friends, all of us.
[01:48:51.040 --> 01:48:53.760]   And I can remember some of them
[01:48:53.760 --> 01:48:55.920]   when I first became a CEO calling me and saying,
[01:48:55.920 --> 01:48:59.360]   "Hey, it's a little lonely here, so let me talk to you."
[01:48:59.360 --> 01:49:01.840]   And then when they became, I did the same for them.
[01:49:01.840 --> 01:49:03.040]   And then they remember,
[01:49:03.040 --> 01:49:04.640]   and they do for the next generation.
[01:49:04.640 --> 01:49:11.040]   And so it's a very supportive, almost to a T,
[01:49:11.040 --> 01:49:13.680]   any of the women you could name who have been CEOs,
[01:49:13.680 --> 01:49:17.120]   I would say, almost to a T, have all been very supportive.
[01:49:17.120 --> 01:49:18.400]   In fact, a number of us work on a little,
[01:49:18.400 --> 01:49:20.480]   another non-for-profit right now called Journey,
[01:49:20.480 --> 01:49:22.720]   which some women who had started,
[01:49:22.720 --> 01:49:24.800]   the Fortune's Most Powerful Women had started,
[01:49:24.800 --> 01:49:27.680]   which was, could we get more women,
[01:49:27.680 --> 01:49:29.360]   particularly diverse women, but women in general,
[01:49:29.360 --> 01:49:32.800]   to more quickly be into positions of leadership and power.
[01:49:32.800 --> 01:49:35.520]   And so many of the women you named and more,
[01:49:35.520 --> 01:49:37.600]   we all dedicate time mentoring
[01:49:37.600 --> 01:49:40.720]   in kind of creating this little group of fellows
[01:49:40.720 --> 01:49:41.520]   every year to do this.
[01:49:41.520 --> 01:49:43.680]   - Friendship and love is core to this whole thing,
[01:49:43.680 --> 01:49:46.480]   not just the success, but just the whole human condition.
[01:49:46.480 --> 01:49:49.920]   Let me ask one last question, advice for young people.
[01:49:49.920 --> 01:49:53.760]   You've had a difficult upbringing, a difficult life,
[01:49:53.760 --> 01:49:56.160]   and you've become one of the most successful
[01:49:56.160 --> 01:49:57.520]   human beings in history.
[01:49:57.520 --> 01:49:59.600]   What advice would you give to young people,
[01:49:59.600 --> 01:50:02.000]   or just people in general who are struggling a bit,
[01:50:02.000 --> 01:50:04.400]   trying to figure out how they can have a career
[01:50:04.400 --> 01:50:07.600]   they can be proud of, or maybe a life they can be proud of?
[01:50:07.600 --> 01:50:10.320]   - I feel like a life you can be proud of
[01:50:10.320 --> 01:50:12.320]   is just one if you leave something a little bit better.
[01:50:12.320 --> 01:50:15.040]   It doesn't have to be big.
[01:50:15.040 --> 01:50:18.000]   That's a life well-lived, right?
[01:50:18.000 --> 01:50:19.200]   It was Churchill who said,
[01:50:19.200 --> 01:50:21.760]   you might remember it better than I.
[01:50:21.760 --> 01:50:24.400]   You make a living by what you get,
[01:50:24.400 --> 01:50:26.880]   and you live a life by what you give.
[01:50:26.880 --> 01:50:27.920]   Something to that effect.
[01:50:27.920 --> 01:50:31.360]   But my advice would probably, when I'm asked this,
[01:50:31.360 --> 01:50:35.920]   I would tell them to ask more questions than give answers.
[01:50:35.920 --> 01:50:40.800]   Just focus on being a sponge.
[01:50:40.800 --> 01:50:44.080]   It's funny, I asked my husband the same question the other day.
[01:50:44.080 --> 01:50:46.240]   I said, hey, we're talking to somebody,
[01:50:46.240 --> 01:50:48.160]   and people were asking this,
[01:50:48.160 --> 01:50:51.120]   and he sort of paused for a while,
[01:50:51.120 --> 01:50:53.440]   and he said, I tell him, patience.
[01:50:53.440 --> 01:50:54.240]   I said, what do you mean?
[01:50:54.240 --> 01:50:58.000]   And he said, I see so many young people,
[01:50:58.000 --> 01:51:00.240]   like they're in such a hurry to somewhere, I don't know where.
[01:51:00.240 --> 01:51:03.600]   And that if they just had patience and let life unfold,
[01:51:03.600 --> 01:51:06.480]   I think they may be surprised where they ended up.
[01:51:06.480 --> 01:51:11.120]   And actually, I think that's a really good answer, to be honest.
[01:51:11.120 --> 01:51:13.360]   - Along the way, keep asking questions.
[01:51:13.360 --> 01:51:15.200]   Keep that childlike curiosity.
[01:51:15.200 --> 01:51:17.120]   - I know, it sounds so easy to say.
[01:51:17.120 --> 01:51:19.840]   It's just so, you know, it's--
[01:51:19.840 --> 01:51:22.560]   - Yeah, like you said, the obvious things.
[01:51:22.560 --> 01:51:23.120]   - Yeah.
[01:51:23.120 --> 01:51:25.680]   - I think they tend to be the most profound.
[01:51:25.680 --> 01:51:27.200]   Jeannie, you're an incredible human being.
[01:51:27.200 --> 01:51:28.720]   You're an inspiration to so many.
[01:51:28.720 --> 01:51:33.760]   Thank you for helping run and contribute
[01:51:33.760 --> 01:51:34.960]   to one of the great companies
[01:51:34.960 --> 01:51:37.120]   that brings so much good power to the world.
[01:51:37.120 --> 01:51:39.920]   And thank you for putting in the hard work
[01:51:39.920 --> 01:51:41.520]   of putting it all in the great book.
[01:51:41.520 --> 01:51:43.040]   So, and thank you for talking today.
[01:51:43.040 --> 01:51:44.000]   This is a huge honor.
[01:51:44.000 --> 01:51:44.640]   Thank you for doing it.
[01:51:44.640 --> 01:51:45.760]   - Yeah, you did a lovely job.
[01:51:45.760 --> 01:51:49.440]   - Thanks for listening to this conversation with Jeannie Rometty.
[01:51:49.440 --> 01:51:50.640]   To support this podcast,
[01:51:50.640 --> 01:51:52.800]   please check out our sponsors in the description.
[01:51:52.800 --> 01:51:55.360]   And now let me leave you with some words
[01:51:55.360 --> 01:51:56.640]   from Eleanor Roosevelt.
[01:51:56.640 --> 01:51:59.440]   "Do what you feel in your heart to be right,
[01:51:59.440 --> 01:52:02.480]   for you'll be criticized either way."
[01:52:02.480 --> 01:52:04.640]   Thank you for listening.
[01:52:04.640 --> 01:52:06.480]   I hope to see you next time.
[01:52:06.880 --> 01:52:09.460]   (upbeat music)
[01:52:09.460 --> 01:52:12.040]   (upbeat music)
[01:52:12.040 --> 01:52:22.040]   [BLANK_AUDIO]

