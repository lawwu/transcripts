
[00:00:00.000 --> 00:00:03.440]   [MUSIC PLAYING]
[00:00:03.440 --> 00:00:13.600]   It's like it's a part of human nature,
[00:00:13.600 --> 00:00:17.000]   and you just need to accept it and deal with it.
[00:00:17.000 --> 00:00:21.880]   Potentially, weaned it to your benefit.
[00:00:21.880 --> 00:00:27.080]   But ultimately, I don't think of these things as zero sum.
[00:00:27.080 --> 00:00:32.080]   There's actually more capital in the world,
[00:00:32.080 --> 00:00:34.440]   or at least more capital in Silicon Valley,
[00:00:34.440 --> 00:00:36.640]   than there are productive uses for that capital.
[00:00:36.640 --> 00:00:45.080]   And so hype maybe creates a way to be
[00:00:45.080 --> 00:00:48.560]   able to accumulate into one space all that capital
[00:00:48.560 --> 00:00:52.800]   to put it to productive use or unproductive use.
[00:00:52.800 --> 00:00:53.720]   But it is what it is.
[00:00:53.720 --> 00:01:04.360]   And I think any field that feels that they're not getting
[00:01:04.360 --> 00:01:07.200]   enough capital should maybe think about their marketing
[00:01:07.200 --> 00:01:10.840]   and maybe build something like ChatGPD that goes viral.
[00:01:10.840 --> 00:01:11.560]   Yeah, it's funny.
[00:01:11.560 --> 00:01:15.880]   I always tell younger researchers,
[00:01:15.880 --> 00:01:19.680]   like, you know what's worse than hype is no hype, right?
[00:01:19.680 --> 00:01:22.360]   I mean, I don't really know what's worse than hype.
[00:01:22.360 --> 00:01:29.600]   I remember when I graduated and was looking for a job in AI,
[00:01:29.600 --> 00:01:31.760]   basically all you could do is kind of rank ads
[00:01:31.760 --> 00:01:34.480]   or go to Wall Street and try to pick stocks.
[00:01:34.480 --> 00:01:36.960]   And the applications now are so much cooler.
[00:01:36.960 --> 00:01:40.240]   And I think the hype, to some extent, is really justified.
[00:01:40.240 --> 00:01:43.080]   I mean, these products are amazing.
[00:01:43.080 --> 00:01:46.640]   I think the use cases are still untapped.
[00:01:46.640 --> 00:01:50.040]   And I think it's important to separate from the hype
[00:01:50.040 --> 00:01:54.640]   and enjoy the breakthroughs that we're seeing right now.
[00:01:54.640 --> 00:01:56.440]   I mean, I just can't even believe--
[00:01:56.440 --> 00:01:58.360]   like, that question about what's the most interesting use
[00:01:58.360 --> 00:02:01.280]   case of LLMs, it's like, where do I even begin?
[00:02:01.280 --> 00:02:05.480]   Like, it must be such a fun time to be getting into AI
[00:02:05.480 --> 00:02:09.120]   with so many applications opening up.
[00:02:09.120 --> 00:02:13.960]   Yeah, and for what it's worth, I think it's sort of like more
[00:02:13.960 --> 00:02:15.560]   ML begets more ML.
[00:02:15.560 --> 00:02:19.320]   Like, since we started working Ghostwriter,
[00:02:19.320 --> 00:02:22.360]   we started doing other non-transformer ML
[00:02:22.360 --> 00:02:25.960]   to help us with building Ghostwriter itself.
[00:02:25.960 --> 00:02:27.080]   Yeah, it just came out recently.
[00:02:27.080 --> 00:02:28.600]   Someone reversed the generic quote by--
[00:02:28.600 --> 00:02:31.000]   they found a small linear regression
[00:02:31.000 --> 00:02:33.600]   model on the front end.
[00:02:33.600 --> 00:02:35.920]   And so I think these tools will just
[00:02:35.920 --> 00:02:39.280]   compose in a really powerful way.
[00:02:39.280 --> 00:02:42.320]   So I think it just brings more attention
[00:02:42.320 --> 00:02:43.320]   to the field in general.
[00:02:43.320 --> 00:02:46.680]   [MUSIC PLAYING]
[00:02:46.680 --> 00:02:49.260]   (upbeat music)
[00:02:49.260 --> 00:02:51.840]   (upbeat music)

