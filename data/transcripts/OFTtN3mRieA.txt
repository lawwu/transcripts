
[00:00:00.000 --> 00:00:19.640]   Awesome.
[00:00:19.640 --> 00:00:22.000]   Let me quickly check if I'm live.
[00:00:22.000 --> 00:00:25.400]   And I'll wait to hear an echo.
[00:00:25.400 --> 00:00:32.280]   Just as a reminder, we'll be reading the Swin Transformer V1 paper today.
[00:00:32.280 --> 00:00:35.120]   I can hear an echo, which means things are looking good.
[00:00:35.120 --> 00:00:36.120]   Awesome.
[00:00:36.120 --> 00:00:37.120]   We can get started.
[00:00:37.120 --> 00:00:42.760]   And just a quick mention to everyone.
[00:00:42.760 --> 00:00:45.760]   These are quite stressful times in the world.
[00:00:45.760 --> 00:00:52.680]   And if you are not in a position where you're in duress, please consider thinking about
[00:00:52.680 --> 00:00:59.320]   how you can help those who are in troubled times.
[00:00:59.320 --> 00:01:01.000]   Thanks.
[00:01:01.000 --> 00:01:08.720]   With that, I'd love to get started with the paper reading group.
[00:01:08.720 --> 00:01:13.120]   As a reminder, I always get a few comments and I'm reading them.
[00:01:13.120 --> 00:01:14.600]   This is a paper reading group.
[00:01:14.600 --> 00:01:16.200]   It's not a paper summary group.
[00:01:16.200 --> 00:01:18.680]   That's why it's an hour long.
[00:01:18.680 --> 00:01:20.520]   But I'll always try to start with a summary.
[00:01:20.520 --> 00:01:25.320]   So if you're short on time, the first five minutes should give you the gist of what we're
[00:01:25.320 --> 00:01:26.320]   about to learn.
[00:01:26.320 --> 00:01:27.800]   But thanks again, everyone.
[00:01:27.800 --> 00:01:31.120]   And I'll now get started.
[00:01:31.120 --> 00:01:35.520]   So as I mentioned, we're reading the Swin Transformer V1 paper today.
[00:01:35.520 --> 00:01:39.760]   And we won't cover V2 because I need more content for future PRG sessions.
[00:01:39.760 --> 00:01:40.920]   Just kidding.
[00:01:40.920 --> 00:01:48.600]   I think V2 of this model has some minor improvements.
[00:01:48.600 --> 00:01:52.680]   If you've gone through this paper, you can read them by itself.
[00:01:52.680 --> 00:01:56.480]   And I'll also be releasing the annotated version of this paper.
[00:01:56.480 --> 00:01:59.880]   So just as a reminder, last time we had read through Connext.
[00:01:59.880 --> 00:02:04.240]   And this is a part of the paper reading group where we've been going through a lot of papers,
[00:02:04.240 --> 00:02:06.840]   state of the art, impactful papers, otherwise.
[00:02:06.840 --> 00:02:08.680]   Here's the paper's repository.
[00:02:08.680 --> 00:02:12.820]   I haven't pushed the annotated version of Swin Transformer to it yet.
[00:02:12.820 --> 00:02:17.840]   But if you just go to this link, you can, for any future sessions, hopefully this will
[00:02:17.840 --> 00:02:22.280]   be on time, you can follow along any papers that we're reading.
[00:02:22.280 --> 00:02:27.320]   So to give you a summary of what we'll be going through, we always start with introducing
[00:02:27.320 --> 00:02:28.320]   the session.
[00:02:28.320 --> 00:02:29.320]   I just did that.
[00:02:29.320 --> 00:02:31.840]   I'll now quickly summarize the Swin Transformer architecture.
[00:02:31.840 --> 00:02:34.000]   And then we'll do an implementation walkthrough.
[00:02:34.000 --> 00:02:37.320]   And like always, some people do it, some people don't.
[00:02:37.320 --> 00:02:40.560]   I'll try to leave you with some suggested homework.
[00:02:40.560 --> 00:02:46.040]   So let me quickly stop sharing my screen and switch to the paper that I have just downloaded.
[00:02:46.040 --> 00:02:48.520]   Hopefully, the archive wasn't down today.
[00:02:48.520 --> 00:02:56.680]   So we're used to CNNs, which follow this hierarchical learning way.
[00:02:56.680 --> 00:03:01.200]   And transformers follow this approach where, throw attention modules at anything, you're
[00:03:01.200 --> 00:03:02.200]   good.
[00:03:02.200 --> 00:03:03.200]   Right?
[00:03:03.200 --> 00:03:06.440]   This paper comes in the middle area where it tries to take some things from the CNN
[00:03:06.440 --> 00:03:08.960]   architecture and tries to apply that.
[00:03:08.960 --> 00:03:11.880]   What do I mean by that?
[00:03:11.880 --> 00:03:14.400]   So there are a few problems with the VIT architecture.
[00:03:14.400 --> 00:03:17.600]   And this paper came out when VIT was the state of the art.
[00:03:17.600 --> 00:03:21.680]   So remember, anytime we're reading a paper, it's always directed at what was happening
[00:03:21.680 --> 00:03:24.440]   in the world at that time in research.
[00:03:24.440 --> 00:03:25.440]   Right?
[00:03:25.440 --> 00:03:26.440]   But it's been a year.
[00:03:26.440 --> 00:03:31.520]   And since then, this paper itself has a renewed version and many new architectures have come
[00:03:31.520 --> 00:03:32.520]   out.
[00:03:32.520 --> 00:03:34.580]   So a few things might not be relevant here.
[00:03:34.580 --> 00:03:41.000]   But just to quickly summarize, on the right hand side, you can see VIT.
[00:03:41.000 --> 00:03:45.360]   And if you read this figure definition, I'm going to be reading through that.
[00:03:45.360 --> 00:03:50.880]   So Swin transformer builds hierarchical feature maps by merging patches.
[00:03:50.880 --> 00:03:56.200]   VIT, if you remember, let's see what color am I feeling like today?
[00:03:56.200 --> 00:03:57.840]   Let's start with blue.
[00:03:57.840 --> 00:04:05.480]   So VIT always, VIT, actually the paper had this title, an image is worth 16 by 16.
[00:04:05.480 --> 00:04:09.840]   So VIT would patch your image into 16 by 16 portions.
[00:04:09.840 --> 00:04:14.560]   So right now what you're seeing, hopefully, if you have a nice connection, is in 1080p,
[00:04:14.560 --> 00:04:18.440]   which means this is in 1920 by 1080.
[00:04:18.440 --> 00:04:21.480]   Yeah, 1920 is this.
[00:04:21.480 --> 00:04:26.060]   VIT would cut this into 16 by 16 pixels.
[00:04:26.060 --> 00:04:30.600]   So a lot of pixels, and then process on top of it, right.
[00:04:30.600 --> 00:04:36.160]   And in essence, it would have this representation inside of the neural network as it would learn
[00:04:36.160 --> 00:04:37.440]   it.
[00:04:37.440 --> 00:04:43.120]   So Swin transformer builds this hierarchical feature map by merging image patches in deeper
[00:04:43.120 --> 00:04:47.560]   layers and has a linear computation complexity.
[00:04:47.560 --> 00:04:53.680]   So for those of you that don't know, pardon my underlining skills.
[00:04:53.680 --> 00:04:59.080]   For those of you that don't know, transformers has a quadratic complexity.
[00:04:59.080 --> 00:05:03.960]   Don't worry, this only comes up when you're reading papers and giving DSA interviews,
[00:05:03.960 --> 00:05:06.880]   which none of us like.
[00:05:06.880 --> 00:05:13.560]   Quadratic complexity means as you increase the number of images or the image size, the
[00:05:13.560 --> 00:05:16.760]   computational time increases quadratically.
[00:05:16.760 --> 00:05:17.920]   That's not good, right?
[00:05:17.920 --> 00:05:20.880]   You don't want to wait for weeks, not even days.
[00:05:20.880 --> 00:05:26.840]   So this has a linear computation complexity, which is awesome.
[00:05:26.840 --> 00:05:31.840]   With respect to input image size.
[00:05:31.840 --> 00:05:34.800]   And thus it serves as a good general purpose backbone.
[00:05:34.800 --> 00:05:36.280]   So what does backbone mean?
[00:05:36.280 --> 00:05:38.120]   Let me quickly draw that.
[00:05:38.120 --> 00:05:42.960]   Usually whenever you're working on, I don't know, a SOTA hotdog or not hotdog classifier,
[00:05:42.960 --> 00:05:50.440]   you would have this set of inputs that go through your model.
[00:05:50.440 --> 00:05:53.960]   Let me get rid of my headphones.
[00:05:53.960 --> 00:06:00.040]   And now I rely on the chart to make sure everyone can hear me okay, because I can't hear myself.
[00:06:00.040 --> 00:06:06.480]   So you have this input that goes through a model and then you have output probabilities.
[00:06:06.480 --> 00:06:13.280]   But inside of this model, you usually have a defined input layer and then a certain set.
[00:06:13.280 --> 00:06:15.760]   This could be, you know, a swing transformer.
[00:06:15.760 --> 00:06:18.480]   So this becomes the backbone.
[00:06:18.480 --> 00:06:20.200]   And why do we call it a backbone?
[00:06:20.200 --> 00:06:27.760]   Because inside of this model, we usually chop off the head.
[00:06:27.760 --> 00:06:31.680]   Whenever you're doing transfer learning, let's say this was trained on ImageNet.
[00:06:31.680 --> 00:06:33.800]   So it would have the number of classes of ImageNet.
[00:06:33.800 --> 00:06:38.120]   I say that because I embarrassingly don't remember the number of classes anyways.
[00:06:38.120 --> 00:06:43.460]   And let's say you're applying it to a problem that has, I don't know, 100 classes.
[00:06:43.460 --> 00:06:48.760]   So you'd remove the head and then add a new one that has 100 on here.
[00:06:48.760 --> 00:06:56.240]   A pro tip that I learned recently on Kaggle was, I'll mention it when I switch sharing,
[00:06:56.240 --> 00:07:01.560]   but you can use PyTorch lazy linear layers to not define the last linear layer dimension.
[00:07:01.560 --> 00:07:03.600]   I'll mention it later.
[00:07:03.600 --> 00:07:07.740]   But ideally, you would have to redefine this final layer.
[00:07:07.740 --> 00:07:13.040]   So this model that you use here, after modifying it a bit, is the backbone.
[00:07:13.040 --> 00:07:19.040]   This could be swing transformer, this could be convex, this could be VIT.
[00:07:19.040 --> 00:07:24.840]   If you're super rich and have more computation, I'm just kidding, it takes more to train,
[00:07:24.840 --> 00:07:27.320]   it takes a little bit less.
[00:07:27.320 --> 00:07:28.840]   This was just a joke.
[00:07:28.840 --> 00:07:34.320]   Or maybe a simpler ResNet as well.
[00:07:34.320 --> 00:07:36.920]   So all of these models serve as the backbone.
[00:07:36.920 --> 00:07:41.360]   So the authors are saying because of how this model functions, and we learn about that,
[00:07:41.360 --> 00:07:49.680]   it has a linear complexity and serves as a good backbone for image classification and
[00:07:49.680 --> 00:07:51.640]   recognition tasks.
[00:07:51.640 --> 00:07:55.440]   Why is that important to emphasize with recognition?
[00:07:55.440 --> 00:08:01.040]   So I just told you that VIT works with 16 by 16 pixels, right?
[00:08:01.040 --> 00:08:04.240]   That is still a large number of pixels to be working with.
[00:08:04.240 --> 00:08:09.600]   But with segmentation, you're literally working with every single pixel because remember in
[00:08:09.600 --> 00:08:14.640]   segmentation, you have to literally label every single pixel.
[00:08:14.640 --> 00:08:26.400]   Because of its linear complexity, swing transformers can also be used with segmentation.
[00:08:26.400 --> 00:08:32.680]   And when compared with vision transformers, they produce feature maps of single low resolution
[00:08:32.680 --> 00:08:39.720]   and have quadratic computation complexity to input image size due to computation of
[00:08:39.720 --> 00:08:41.560]   self-attention globally.
[00:08:41.560 --> 00:08:47.200]   So VIT, we've covered this in our paper reading group, I refer you to that.
[00:08:47.200 --> 00:08:54.960]   But VIT has a quadratic complexity because it uses attention in every single layer.
[00:08:54.960 --> 00:08:59.520]   Swing transformer applies a clever approach of dealing with this.
[00:08:59.520 --> 00:09:00.960]   So that is the quick summary.
[00:09:00.960 --> 00:09:11.760]   And what makes this awesome is shifted window.
[00:09:11.760 --> 00:09:19.400]   So that's where the word swing comes from, by the way.
[00:09:19.400 --> 00:09:25.280]   That's what makes this awesome that you first of all start with a set of patches on top
[00:09:25.280 --> 00:09:27.760]   of which you apply attention.
[00:09:27.760 --> 00:09:30.080]   So this somewhat resembles a CNN, right?
[00:09:30.080 --> 00:09:36.880]   Because again, you're applying some form of computation, here it being attention to all
[00:09:36.880 --> 00:09:38.200]   of these layers.
[00:09:38.200 --> 00:09:44.680]   And using some tricks, it's been made linearly scalable.
[00:09:44.680 --> 00:09:47.040]   So that's the summary of this paper.
[00:09:47.040 --> 00:09:52.440]   And now we'll be diving into depth of how this works, what is in the paper, and we'll
[00:09:52.440 --> 00:09:53.600]   be reading through this.
[00:09:53.600 --> 00:09:57.200]   If you're going to comment that it's a long video, the first three minutes were for your
[00:09:57.200 --> 00:09:58.200]   summary.
[00:09:58.200 --> 00:10:01.640]   Please don't leave those comments, they make me stay up at night.
[00:10:01.640 --> 00:10:04.160]   I'm just having fun here.
[00:10:04.160 --> 00:10:10.400]   Coming back to the agenda, now I'll go about introducing and giving some more context.
[00:10:10.400 --> 00:10:14.400]   So hopefully more of you will stay around until the end, but we've just given the shorter
[00:10:14.400 --> 00:10:15.400]   summary.
[00:10:15.400 --> 00:10:19.920]   I've used the swing transformer paper, we'll be reading that.
[00:10:19.920 --> 00:10:23.920]   And I'll be uploading this to the paper's repository soon after we end reading the paper.
[00:10:23.920 --> 00:10:31.200]   I'll be using the incredible TIM repository by Ross, who basically has every single computer
[00:10:31.200 --> 00:10:33.760]   vision model available for you.
[00:10:33.760 --> 00:10:38.840]   And to credit, I've also watched the video, although I don't agree with all of the channel's
[00:10:38.840 --> 00:10:46.400]   name, wordings, but Leticia's video on swing transformer was absolutely incredible, and
[00:10:46.400 --> 00:10:48.560]   I'd encourage you to check that out.
[00:10:48.560 --> 00:10:53.800]   So I've learned through that, and now I'll be reteaching you what I've learned.
[00:10:53.800 --> 00:10:57.180]   This paper talks about the transformer takeover.
[00:10:57.180 --> 00:11:03.280]   So to go down the history lane, the attention is all you need paper came out in around 2017
[00:11:03.280 --> 00:11:09.480]   if I remember correctly.
[00:11:09.480 --> 00:11:17.000]   And then you saw all of these Bert, Roberta, that's not in chronological order, I don't
[00:11:17.000 --> 00:11:22.800]   know, deBerta, all of these NLP models that made so huge progress, right?
[00:11:22.800 --> 00:11:30.200]   And then, of course, someone was at that time thinking, why not apply this to why not apply
[00:11:30.200 --> 00:11:32.000]   this to computer vision?
[00:11:32.000 --> 00:11:33.000]   Right?
[00:11:33.000 --> 00:11:35.840]   I see simply so month in the chat.
[00:11:35.840 --> 00:11:41.240]   Thanks for joining us simply so month is the knowledge of how transformers work necessary
[00:11:41.240 --> 00:11:42.240]   for this.
[00:11:42.240 --> 00:11:47.560]   It will help you, but you can follow along and feel free to ask any questions.
[00:11:47.560 --> 00:11:52.000]   I'll try to make it useful without that.
[00:11:52.000 --> 00:11:56.760]   So we saw this attention and transformer takeover in the NLP world.
[00:11:56.760 --> 00:12:01.060]   Someone was surely thinking, hey, why can't we make this work in images.
[00:12:01.060 --> 00:12:09.720]   So right around in 2020, so VIT, which was titled and images for 16 by 16 paper, and
[00:12:09.720 --> 00:12:14.320]   that established transformers in the computer vision field.
[00:12:14.320 --> 00:12:17.680]   A few months later, we saw Swin transformer.
[00:12:17.680 --> 00:12:24.000]   And as expected, at that time, it would be comparing itself with VID and showcasing how
[00:12:24.000 --> 00:12:25.980]   it's better than that model.
[00:12:25.980 --> 00:12:28.440]   So that's what the paper covers.
[00:12:28.440 --> 00:12:36.480]   To remind you, CNN work like so, so you convolve over the input image, and you generate this
[00:12:36.480 --> 00:12:38.600]   feature map.
[00:12:38.600 --> 00:12:43.920]   And as I mentioned earlier, Swin transformer are somewhat closer to this, right?
[00:12:43.920 --> 00:12:49.880]   So now you have this collection of transformer models, CNN, and there's this fight going
[00:12:49.880 --> 00:12:52.560]   on between both of them.
[00:12:52.560 --> 00:12:58.400]   Swin transformer is more on the CNN side, because it's taking a few tricks from there
[00:12:58.400 --> 00:13:01.240]   to some extent and applying them.
[00:13:01.240 --> 00:13:05.560]   Last week, we learned about convex, which was a CNN that was on the side of transformers.
[00:13:05.560 --> 00:13:08.760]   As you can see, this battle is getting quite interesting.
[00:13:08.760 --> 00:13:12.880]   This comparison is getting quite interesting.
[00:13:12.880 --> 00:13:19.120]   So the question I assume that are addressed in this paper is what can we learn from CNNs?
[00:13:19.120 --> 00:13:23.480]   Or if you read through the paper, I think Leticia mentions in her video, it's very comparable
[00:13:23.480 --> 00:13:27.640]   to a CNN or a unit architecture.
[00:13:27.640 --> 00:13:31.360]   Then the second one is how can we reduce the complexity of transformers?
[00:13:31.360 --> 00:13:41.560]   How can we make VID like models more scalable?
[00:13:41.560 --> 00:13:45.840]   In other words, how can we make it computationally efficient?
[00:13:45.840 --> 00:13:49.080]   And then what the hell does shifted windows stand for?
[00:13:49.080 --> 00:13:57.160]   So what does Swin transformer stand for?
[00:13:57.160 --> 00:14:02.160]   Before we start with the paper reading, I always love to just showcase what's inside
[00:14:02.160 --> 00:14:04.040]   of the model.
[00:14:04.040 --> 00:14:06.040]   For some reason, Google Collab doesn't like me.
[00:14:06.040 --> 00:14:07.040]   So they always kick me off.
[00:14:07.040 --> 00:14:10.140]   Let's see if I can quickly print this.
[00:14:10.140 --> 00:14:16.160]   So what I'm trying to show you here is inside of the PyTorch implementation, what all layers
[00:14:16.160 --> 00:14:18.240]   exist.
[00:14:18.240 --> 00:14:22.920]   So instead of reading the paper, it's always a good idea to just quickly see how many concrude
[00:14:22.920 --> 00:14:26.680]   layers how many nn.linear layers exist inside of it.
[00:14:26.680 --> 00:14:28.920]   At least I understand better that way.
[00:14:28.920 --> 00:14:32.280]   So we will quickly take a look at that.
[00:14:32.280 --> 00:14:46.020]   While this runs and downloads, torch.meshgrid will be required to pass indexing argument.
[00:14:46.020 --> 00:14:47.020]   Interesting.
[00:14:47.020 --> 00:14:52.380]   I have not even used that function recently, if I remember correctly.
[00:14:52.380 --> 00:14:56.980]   Has anyone used torch.meshgrid recently?
[00:14:56.980 --> 00:14:59.780]   I'm just curious about the wording.
[00:14:59.780 --> 00:15:03.020]   I've not seen it in a long time.
[00:15:03.020 --> 00:15:09.780]   So what I've done here is I first of all wrote Swin transformer here and that of course didn't
[00:15:09.780 --> 00:15:10.780]   import.
[00:15:10.780 --> 00:15:13.820]   So then I printed out what all models are available.
[00:15:13.820 --> 00:15:19.100]   And you know what, let's look at Swin tiny patch, Windows 7.
[00:15:19.100 --> 00:15:24.460]   So instead of the base, let's import this one.
[00:15:24.460 --> 00:15:37.900]   So I've listed the models and then I'll try to see what layers exist inside of it.
[00:15:37.900 --> 00:15:41.520]   So we're looking at the Swin tiny.
[00:15:41.520 --> 00:15:49.820]   So the tiny version, remember with any CNN or computer vision architecture, authors always
[00:15:49.820 --> 00:15:55.180]   start with a smaller version and then they try to add many more layers to it to tackle
[00:15:55.180 --> 00:15:56.260]   more tasks.
[00:15:56.260 --> 00:16:02.980]   So this is a smaller version that would be less computationally expensive.
[00:16:02.980 --> 00:16:07.700]   So let's see, we have a patch embed layer and inside of that we have projection and
[00:16:07.700 --> 00:16:08.700]   normalization.
[00:16:08.700 --> 00:16:11.420]   So projections are calculated by Contrudy.
[00:16:11.420 --> 00:16:21.700]   So three channels, 96 kernel size of 4x4, stride of 4x4, followed by layer norms.
[00:16:21.700 --> 00:16:26.160]   We see a dropout of zero, that's interesting.
[00:16:26.160 --> 00:16:30.740]   And inside of the layers, we see the first basic layer, which has a Swin transformer
[00:16:30.740 --> 00:16:40.820]   block followed by an identity and a normalization layer and an MLP.
[00:16:40.820 --> 00:16:44.380]   And then we have another Swin transformer block followed by more dropout, normalization
[00:16:44.380 --> 00:16:46.340]   and MLP.
[00:16:46.340 --> 00:16:50.260]   So let's see what's inside of the Swin transformer block.
[00:16:50.260 --> 00:16:59.020]   You have a layer norm followed by window attention inside of which you call, let's see what is
[00:16:59.020 --> 00:17:00.020]   being called.
[00:17:00.020 --> 00:17:06.180]   I believe this is attention being applied, followed by dropout, a linear layer.
[00:17:06.180 --> 00:17:09.600]   Again even though dropout is being applied, it's at the value zero.
[00:17:09.600 --> 00:17:13.520]   I'm quite curious about that, followed by softmax.
[00:17:13.520 --> 00:17:19.720]   So we learn what window attention is, but it seems like it's an attention layer that's
[00:17:19.720 --> 00:17:30.900]   being applied to smaller parts inside of the input or inside of the Swin transformer block,
[00:17:30.900 --> 00:17:35.580]   followed by which you have a multi-layer perceptron that has simply a linear layer.
[00:17:35.580 --> 00:17:42.020]   JLU, not ReLU, I'm still curious and I haven't found an answer why JLU is being used instead
[00:17:42.020 --> 00:17:43.440]   of ReLU.
[00:17:43.440 --> 00:17:46.960]   So I'd love if anyone knows the answer to that.
[00:17:46.960 --> 00:17:49.580]   Followed by dropout, a linear layer and dropout again.
[00:17:49.580 --> 00:17:52.800]   The same thing repeats itself.
[00:17:52.800 --> 00:18:00.760]   Followed by which you have a basic layer, which has again more Swin transformer blocks
[00:18:00.760 --> 00:18:04.900]   and more layer norm and window attention.
[00:18:04.900 --> 00:18:09.480]   More Swin transformer blocks, then you downsample and repeat the same.
[00:18:09.480 --> 00:18:15.360]   So it looks like this is being repeated a bunch of times.
[00:18:15.360 --> 00:18:17.480]   And let's see.
[00:18:17.480 --> 00:18:21.080]   Finally you get to average pooling and a linear layer.
[00:18:21.080 --> 00:18:26.960]   So the trick I wanted to mention was firetorch lazy layer.
[00:18:26.960 --> 00:18:31.640]   Sorry, it's lazy linear.
[00:18:31.640 --> 00:18:35.600]   Let me drop this in the chart.
[00:18:35.600 --> 00:18:42.640]   The cool thing about this layer is you don't have to explicitly define, if I remember correctly,
[00:18:42.640 --> 00:18:43.840]   the in features.
[00:18:43.840 --> 00:18:51.880]   So it lazily defines the number of features required for input.
[00:18:51.880 --> 00:18:53.620]   And this is cool for lazy people like me.
[00:18:53.620 --> 00:18:57.400]   So it's an awesome trick that you all can use.
[00:18:57.400 --> 00:18:59.500]   Just a quick suggestion.
[00:18:59.500 --> 00:19:04.800]   So so far we've looked at inside of the architecture and it looks like a Swin transformer block.
[00:19:04.800 --> 00:19:06.720]   We'll see what that is.
[00:19:06.720 --> 00:19:10.920]   But looks like it has some normalization, some form of attention, some dropout and more
[00:19:10.920 --> 00:19:15.000]   normalization and MLP is being applied inside of it.
[00:19:15.000 --> 00:19:20.720]   And this is being repeated a bunch of times and you have the classic pooling layer followed
[00:19:20.720 --> 00:19:22.740]   by a linear layer towards the end.
[00:19:22.740 --> 00:19:29.180]   So that's what is happening inside of this architecture.
[00:19:29.180 --> 00:19:32.400]   So again, that was from the temp library.
[00:19:32.400 --> 00:19:38.120]   I installed it and imported the model from there, specifically the tiny model.
[00:19:38.120 --> 00:19:47.200]   Now let's jump back to the paper and hide this panel.
[00:19:47.200 --> 00:19:52.120]   So we've looked at a summary, we've looked inside of the firetorch implementation.
[00:19:52.120 --> 00:20:02.180]   Now we'll go through the paper linearly.
[00:20:02.180 --> 00:20:12.700]   The authors claim challenges that arise when you're trying to take transformers from language
[00:20:12.700 --> 00:20:21.820]   and apply them to images is that there's a large variation in how things appear and they
[00:20:21.820 --> 00:20:24.340]   have a higher resolution.
[00:20:24.340 --> 00:20:31.260]   So again, as a reminder, how I try to read papers in this session is I've already read
[00:20:31.260 --> 00:20:32.260]   them.
[00:20:32.260 --> 00:20:36.800]   So I'll point out the important things in every single line that I think is worth noticing
[00:20:36.800 --> 00:20:39.380]   and I'll go over those points.
[00:20:39.380 --> 00:20:45.620]   So the authors start by saying, hey, transformers work well in NLP.
[00:20:45.620 --> 00:20:47.540]   But you know what vision is more challenging.
[00:20:47.540 --> 00:20:48.980]   There are so many different things.
[00:20:48.980 --> 00:20:52.380]   Saeem looks different one day, one day he has a beard, one day he doesn't, sometimes
[00:20:52.380 --> 00:20:54.220]   even his face ID doesn't work.
[00:20:54.220 --> 00:20:57.720]   No one needed to hear that.
[00:20:57.720 --> 00:21:07.100]   So because of this nature of this thing, computer vision, it's a more challenging thing where
[00:21:07.100 --> 00:21:17.260]   you can not so easily apply attention or transformers.
[00:21:17.260 --> 00:21:24.060]   To address this problem, they propose a hierarchical transformer.
[00:21:24.060 --> 00:21:25.940]   So this is the hierarchy that they're referring to.
[00:21:25.940 --> 00:21:33.260]   If you look over to the right side, this particular image, this hierarchy that's being built,
[00:21:33.260 --> 00:21:41.140]   the shifted windows scheme brings greater efficiency by limiting self-attention computation
[00:21:41.140 --> 00:21:45.620]   to non-overlapping local windows.
[00:21:45.620 --> 00:21:50.580]   In VIT, you were applying attention to almost all of the windows.
[00:21:50.580 --> 00:21:54.700]   Here you apply them to non-overlapping windows.
[00:21:54.700 --> 00:22:00.700]   So these windows that have been created, this window, you would have one attention being
[00:22:00.700 --> 00:22:05.020]   applied here and one more being applied here.
[00:22:05.020 --> 00:22:08.380]   Attention module.
[00:22:08.380 --> 00:22:12.380]   And the cool thing about this architecture is it has the flexibility to model at various
[00:22:12.380 --> 00:22:16.540]   scales and has a linear computational complexity.
[00:22:16.540 --> 00:22:20.900]   And then they talk about its performance on different benchmarks.
[00:22:20.900 --> 00:22:25.700]   You can read through that, but this already has been beaten, so I won't go through the
[00:22:25.700 --> 00:22:28.460]   individual numbers.
[00:22:28.460 --> 00:22:39.500]   Then in the introduction, sorry, one second.
[00:22:39.500 --> 00:22:45.860]   In the introduction, they talk about CNN started with AlexNet and they have recently become
[00:22:45.860 --> 00:22:46.860]   really cool.
[00:22:46.860 --> 00:22:47.860]   This was the backbone.
[00:22:47.860 --> 00:22:55.820]   NLP first saw the transformers because they were really cool of modeling, really good
[00:22:55.820 --> 00:23:01.220]   and cool at modeling long-range dependencies in the data.
[00:23:01.220 --> 00:23:07.340]   And this led to people thinking, hey, let's take this thing and apply it to computer vision.
[00:23:07.340 --> 00:23:09.820]   It's bound to work, right?
[00:23:09.820 --> 00:23:16.180]   And in this paper, they expand on the application such that it can serve as a general purpose
[00:23:16.180 --> 00:23:27.100]   backbone as it does for NLP and CNNs.
[00:23:27.100 --> 00:23:33.700]   So from here, I'll skip the other parts and scroll down to the implementation again, not
[00:23:33.700 --> 00:23:40.540]   implementation, but the diagram where they show how this has been created.
[00:23:40.540 --> 00:23:44.820]   So this is the architecture of Swintiny, the one we looked at.
[00:23:44.820 --> 00:23:51.500]   Now we're looking at it in terms of visual representation.
[00:23:51.500 --> 00:23:58.500]   So you have, as you saw earlier, two successive Swintransformer blocks on the right and on
[00:23:58.500 --> 00:24:00.260]   the left, you can see the complete architecture.
[00:24:00.260 --> 00:24:06.140]   So you have this image that goes through this patch partition, and we'll go back and see
[00:24:06.140 --> 00:24:13.300]   what that is, followed by which looks like you have stages of more patch merging.
[00:24:13.300 --> 00:24:16.300]   So first you partition an image, right?
[00:24:16.300 --> 00:24:20.460]   You take an image and cut it into smaller pieces.
[00:24:20.460 --> 00:24:24.980]   And then you merge them and pass them through a Swintransformer block.
[00:24:24.980 --> 00:24:27.300]   And you repeat this a bunch of times.
[00:24:27.300 --> 00:24:35.220]   If you pay close attention, you can see that the size is being reduced every time.
[00:24:35.220 --> 00:24:41.680]   So it started with h by 4 and w by 4, h is the height and w is the width.
[00:24:41.680 --> 00:24:44.340]   And now it's by 8, by 16, by 32.
[00:24:44.340 --> 00:24:46.580]   So it's getting smaller and smaller.
[00:24:46.580 --> 00:24:49.900]   And on the right, we can see two Swintransformer blocks.
[00:24:49.900 --> 00:24:59.100]   So reading here, MSA stands for multi head self attention with regular and shifted window.
[00:24:59.100 --> 00:25:06.340]   So it looks like inside of this Swintransformer block that occurs in every single stage, twice,
[00:25:06.340 --> 00:25:11.980]   sometimes six times, and most of the times twice, right?
[00:25:11.980 --> 00:25:19.940]   So this is through layer norm, followed by WMSA, followed by more layer norm MLP, and
[00:25:19.940 --> 00:25:20.940]   the same repeats.
[00:25:20.940 --> 00:25:27.620]   Instead, there's one minute difference, but this time it's a shifted window MSA.
[00:25:27.620 --> 00:25:31.820]   So we looked at this inside of PyTorch and here's how it looks visually.
[00:25:31.820 --> 00:25:40.340]   Again, to summarize, you have your image that somehow gets partitioned.
[00:25:40.340 --> 00:25:44.460]   And then you apply Swintransformer blocks.
[00:25:44.460 --> 00:25:46.300]   What are they?
[00:25:46.300 --> 00:25:48.260]   These things on the right.
[00:25:48.260 --> 00:25:52.340]   So some form of normalization followed by attention layers.
[00:25:52.340 --> 00:25:55.740]   To simply understand, you can remember it like so.
[00:25:55.740 --> 00:26:00.540]   And then these patches, so the partitions you created get merged somehow.
[00:26:00.540 --> 00:26:03.560]   We don't know how yet, but they get merged.
[00:26:03.560 --> 00:26:05.980]   And this repeats a few times.
[00:26:05.980 --> 00:26:09.740]   So this is the architecture overview.
[00:26:09.740 --> 00:26:14.180]   Going back to reading in order.
[00:26:14.180 --> 00:26:19.220]   Please let me know if this process of jumping ahead is confusing.
[00:26:19.220 --> 00:26:23.460]   This is how I skimmed through the paper, so I'm just following that approach.
[00:26:23.460 --> 00:26:38.620]   Let's see if there's anything I want to cover here.
[00:26:38.620 --> 00:26:45.340]   So in figure 1a, the one we looked at, Swintransformer constructs a hierarchical representation by
[00:26:45.340 --> 00:26:54.020]   starting from small side patches and gradually merging neighboring patches in deeper transformer
[00:26:54.020 --> 00:26:55.020]   layers.
[00:26:55.020 --> 00:27:00.700]   That's why it's called a hierarchical representation.
[00:27:00.700 --> 00:27:07.340]   And using this, Swintransformer can conveniently leverage techniques for dense prediction such
[00:27:07.340 --> 00:27:15.580]   as FPN unit, etc.
[00:27:15.580 --> 00:27:23.580]   The key design element here is its shift of the window partition between consecutive self-attention
[00:27:23.580 --> 00:27:27.840]   layers.
[00:27:27.840 --> 00:27:35.520]   So as you can see, I'm looking at this figure and I'm looking at layer 1 and layer 2.
[00:27:35.520 --> 00:27:36.520]   This is how we start.
[00:27:36.520 --> 00:27:40.480]   So we create all of these patches.
[00:27:40.480 --> 00:27:46.280]   In layer L, a regular window partitioning scheme is adopted and self-attention is computed
[00:27:46.280 --> 00:27:47.280]   within each window.
[00:27:47.280 --> 00:27:54.040]   So you have attention being applied to this window, to this window, and this window, and
[00:27:54.040 --> 00:27:58.040]   this window.
[00:27:58.040 --> 00:28:03.560]   In the next layer, the partitioning is shifted.
[00:28:03.560 --> 00:28:06.960]   So it's not a perfect square, it's a little shifted this time, right?
[00:28:06.960 --> 00:28:11.680]   So it changes a bit.
[00:28:11.680 --> 00:28:18.260]   And now the self-attention computation crosses the boundaries of the previous windows, providing
[00:28:18.260 --> 00:28:20.620]   connections among them.
[00:28:20.620 --> 00:28:22.360]   This is quite important to understand.
[00:28:22.360 --> 00:28:31.800]   So in any CNN, as you go deeper, right, it starts to build an understanding of more high-level
[00:28:31.800 --> 00:28:34.240]   features.
[00:28:34.240 --> 00:28:41.440]   Here it won't be possible if you keep patching or dividing your images and keep applying
[00:28:41.440 --> 00:28:42.640]   attention to it.
[00:28:42.640 --> 00:28:46.480]   It'll just learn what's inside of that patch, right?
[00:28:46.480 --> 00:28:47.880]   How do you make it more global?
[00:28:47.880 --> 00:28:52.720]   So how do you teach your model to learn what's in the complete image, especially when you're
[00:28:52.720 --> 00:28:54.640]   chopping it up?
[00:28:54.640 --> 00:29:03.040]   So by applying the shifting, where every time inside every next layer, there's a different
[00:29:03.040 --> 00:29:09.760]   window that's created, and then you apply attention to it, the model starts to learn
[00:29:09.760 --> 00:29:12.960]   features across these partitions.
[00:29:12.960 --> 00:29:17.860]   So this is helpful because first of all, you're still creating partitions.
[00:29:17.860 --> 00:29:21.700]   So it makes it easier for batching and computation.
[00:29:21.700 --> 00:29:26.740]   But then since you're applying attention to different windows every time, your model can
[00:29:26.740 --> 00:29:32.720]   still continue learning about all of these features that it's supposed to understand
[00:29:32.720 --> 00:29:33.720]   inside of the image.
[00:29:33.720 --> 00:29:34.720]   This is a hot dog.
[00:29:34.720 --> 00:29:35.720]   This is not a hot dog.
[00:29:35.720 --> 00:29:36.940]   If you look closely, this is a building.
[00:29:36.940 --> 00:29:37.940]   This is not a building.
[00:29:37.940 --> 00:29:39.200]   These are trees.
[00:29:39.200 --> 00:29:51.540]   That's what's visible inside of this layer.
[00:29:51.540 --> 00:29:57.680]   This strategy is also efficient in regards to real world latency.
[00:29:57.680 --> 00:30:06.200]   In contrast, earlier sliding window have a higher latency.
[00:30:06.200 --> 00:30:08.820]   And this approach has a much lower latency.
[00:30:08.820 --> 00:30:14.560]   So it has higher performance compared to the others.
[00:30:14.560 --> 00:30:19.700]   And the shifted window approach also provides benefits to all MLP architectures.
[00:30:19.700 --> 00:30:24.060]   That's another claim they make.
[00:30:24.060 --> 00:30:27.460]   So then they talk about related works.
[00:30:27.460 --> 00:30:36.280]   And they mentioned, I believe all of the ResNets, VGG, GoogleNet, DenseNet, HRNet, EfficientNet.
[00:30:36.280 --> 00:30:40.260]   I've never actually used HRNet.
[00:30:40.260 --> 00:30:45.060]   I'm curious if anyone has, but I've never ever come across someone who's using HRNet.
[00:30:45.060 --> 00:30:47.620]   I've always seen its name, but I've not used it yet.
[00:30:47.620 --> 00:30:51.660]   And I haven't had the opportunity.
[00:30:51.660 --> 00:31:00.060]   And then they talk about the history and general self-attention based backbone architectures.
[00:31:00.060 --> 00:31:04.740]   Self-attention or transformers to complement CNNs.
[00:31:04.740 --> 00:31:10.780]   Transformer based vision backbones where they mentioned the VIT paper.
[00:31:10.780 --> 00:31:13.400]   So again, this is more of a historical catch up.
[00:31:13.400 --> 00:31:18.740]   If you're trying to read the paper, any paper, they always give you more context about what's
[00:31:18.740 --> 00:31:21.600]   happening in the general world at that time.
[00:31:21.600 --> 00:31:25.140]   So this is about, there used to be a thing called CNNs.
[00:31:25.140 --> 00:31:26.680]   Those are boring now.
[00:31:26.680 --> 00:31:29.040]   Now we have attention that's here.
[00:31:29.040 --> 00:31:30.500]   And then we have self-attention.
[00:31:30.500 --> 00:31:33.660]   We have self-attention based backbone.
[00:31:33.660 --> 00:31:35.460]   We have transformers to complement.
[00:31:35.460 --> 00:31:37.300]   We have transformer based vision backbones.
[00:31:37.300 --> 00:31:38.300]   Here's the history.
[00:31:38.300 --> 00:31:39.300]   Here are the current models.
[00:31:39.300 --> 00:31:41.900]   You can read about them.
[00:31:41.900 --> 00:31:50.900]   And then in any paper followed by that, you usually have an architectural overview.
[00:31:50.900 --> 00:32:01.420]   We've already gone through this, but I'll now read through in sequential order.
[00:32:01.420 --> 00:32:08.040]   So we looked at the tiny version of Swinty and it first splits an input RGB image into
[00:32:08.040 --> 00:32:14.220]   non overlapping patches, similar to VIT.
[00:32:14.220 --> 00:32:20.780]   Each patch is treated like a token and its feature is set as a concatenation of the raw
[00:32:20.780 --> 00:32:23.500]   pixel RGB values.
[00:32:23.500 --> 00:32:26.620]   The patch size here is four by four.
[00:32:26.620 --> 00:32:33.940]   So we create four by four patches and does the M feature dimension of each patch is four
[00:32:33.940 --> 00:32:36.980]   by four by three.
[00:32:36.980 --> 00:32:38.480]   It's three because it's RGB.
[00:32:38.480 --> 00:32:41.720]   So you have three channels hence four by four by three.
[00:32:41.720 --> 00:32:46.900]   So you have this input image.
[00:32:46.900 --> 00:32:47.900]   Let's use red.
[00:32:47.900 --> 00:32:55.820]   So I have this input image and now inside of it, you cut out this four by four patch.
[00:32:55.820 --> 00:32:58.700]   You take it out here.
[00:32:58.700 --> 00:33:03.980]   This would have a dimension of.
[00:33:03.980 --> 00:33:06.840]   See I'm bad at counting as well.
[00:33:06.840 --> 00:33:10.660]   So you have this terrible four by four.
[00:33:10.660 --> 00:33:12.680]   Let's assume this is four by four.
[00:33:12.680 --> 00:33:17.380]   So now you have this four by four patch from inside of the image and this would have three
[00:33:17.380 --> 00:33:25.500]   channels in it because remember RGB.
[00:33:25.500 --> 00:33:31.180]   You create all of these patches from inside of the image and then you have a linear embedding
[00:33:31.180 --> 00:33:37.060]   layer applied on this raw valued feature.
[00:33:37.060 --> 00:33:42.260]   Several transformer blocks with modified self attention are applied on these patch tokens.
[00:33:42.260 --> 00:33:50.660]   So now you have all of these tokens and let's say you have a lot of these like so.
[00:33:50.660 --> 00:34:04.200]   So now you apply attention modules on top of them.
[00:34:04.200 --> 00:34:10.020]   To produce a hierarchical representation, the number of tokens is reduced by the patch
[00:34:10.020 --> 00:34:13.340]   merging layers.
[00:34:13.340 --> 00:34:20.360]   The first merging layer concatenates features of each group of two by two neighboring patches.
[00:34:20.360 --> 00:34:30.040]   So next up as you go deeper in the model, you would have this merging layer and now
[00:34:30.040 --> 00:34:37.720]   it creates smaller windows, smaller representation.
[00:34:37.720 --> 00:34:46.920]   So now you'd have a larger, let's say a representation like so.
[00:34:46.920 --> 00:34:54.040]   This reduces the number of tokens by a multiple of four since it's twice down sampling of
[00:34:54.040 --> 00:34:59.160]   resolution and the output dimension is set to 2C.
[00:34:59.160 --> 00:35:02.560]   So C is again a hyper parameter we define.
[00:35:02.560 --> 00:35:11.960]   If you look above, it's an arbitrary dimension that we apply.
[00:35:11.960 --> 00:35:18.320]   So now you have this smaller set of patches that have a hierarchy and keep getting lesser
[00:35:18.320 --> 00:35:22.720]   in number.
[00:35:22.720 --> 00:35:29.220]   And the swing transformer blocks are applied afterwards for feature transformation.
[00:35:29.220 --> 00:35:31.480]   And this is denoted as stage two.
[00:35:31.480 --> 00:35:35.720]   This is repeated a few times you have stage three, stage four.
[00:35:35.720 --> 00:35:39.400]   These stages jointly produce a hierarchical representation.
[00:35:39.400 --> 00:35:48.400]   So you can somewhat assume that after this, you would have just one window and this form
[00:35:48.400 --> 00:35:55.640]   of hierarchy is being generated where the patches keep getting smaller and you can now
[00:35:55.640 --> 00:36:03.200]   represent more and more information and some form of attention is being applied here.
[00:36:03.200 --> 00:36:07.800]   So what is the attention and what is the swing transformer block that seems to be the secret
[00:36:07.800 --> 00:36:09.720]   sauce for making this work?
[00:36:09.720 --> 00:36:14.680]   A swing transformer block is built by replacing standard.
[00:36:14.680 --> 00:36:23.240]   Oops, sorry.
[00:36:23.240 --> 00:36:26.000]   My hand is constantly slipping over my keyboard today.
[00:36:26.000 --> 00:36:29.440]   Sorry about that.
[00:36:29.440 --> 00:36:39.880]   So inside of a swing transformer block, you replace the standard MSA, multi head self-attention
[00:36:39.880 --> 00:36:42.960]   by shifted windows.
[00:36:42.960 --> 00:36:48.680]   We'll just read about that here.
[00:36:48.680 --> 00:36:54.800]   And it consists of a shifted window based MSA followed by two layer MLP with Gelu.
[00:36:54.800 --> 00:36:56.760]   We've already looked at this.
[00:36:56.760 --> 00:37:05.600]   So let's read what a shifted window is.
[00:37:05.600 --> 00:37:14.480]   The problem with general self-attention is it has quadratic complexity with respect to
[00:37:14.480 --> 00:37:16.400]   input tokens.
[00:37:16.400 --> 00:37:23.960]   And this makes it challenging as you go to 1080p and then 4k becomes almost unimaginable.
[00:37:23.960 --> 00:37:27.600]   8k, well forget it.
[00:37:27.600 --> 00:37:33.840]   So how do you deal with this problem?
[00:37:33.840 --> 00:37:41.360]   So they propose to compute self-attention within local windows and not globally.
[00:37:41.360 --> 00:37:46.460]   These windows are arranged evenly in a non-overlapping manner.
[00:37:46.460 --> 00:37:53.920]   So these windows we looked at, these boundaries are inside of the image and they don't overlap.
[00:37:53.920 --> 00:37:59.920]   So this does not happen at all.
[00:37:59.920 --> 00:38:05.000]   So you don't have any overlapping like so.
[00:38:05.000 --> 00:38:10.200]   And let me remove the underlining because I've messed up the text.
[00:38:10.200 --> 00:38:13.040]   Supposing each window has M by M patches.
[00:38:13.040 --> 00:38:20.120]   So these patches we looked at, let's say these are M by M and the computational complexity
[00:38:20.120 --> 00:38:25.560]   of global MSA and window based one can be written like so.
[00:38:25.560 --> 00:38:33.440]   The only thing to note here is this has a lower complexity if I remember compared to
[00:38:33.440 --> 00:38:35.040]   it.
[00:38:35.040 --> 00:38:41.000]   The former is quadratic and the latter is linear when M is fixed.
[00:38:41.000 --> 00:38:43.120]   7 by default.
[00:38:43.120 --> 00:38:47.740]   So we've managed to make it more scalable by using this approach.
[00:38:47.740 --> 00:38:50.080]   What is the approach again?
[00:38:50.080 --> 00:39:00.120]   Using non-overlapping patches, merging them, the size always goes down by 4 and then applying
[00:39:00.120 --> 00:39:07.400]   some form of attention to this, patching it again, merging these, doing it repeatedly
[00:39:07.400 --> 00:39:10.680]   while keeping them non-overlapping.
[00:39:10.680 --> 00:39:15.000]   So this shifted window partitioning in successive blocks, let's see how does it get applied
[00:39:15.000 --> 00:39:18.920]   in successive blocks.
[00:39:18.920 --> 00:39:31.760]   To introduce cross window connections while maintaining the computation, the first module
[00:39:31.760 --> 00:39:34.360]   uses a regular window partitioning.
[00:39:34.360 --> 00:39:40.820]   So the first is simply cutting into 4 or how many partitions you have.
[00:39:40.820 --> 00:39:43.160]   So let's say this is your original image.
[00:39:43.160 --> 00:39:50.240]   The first would have a regular partitioning, the first layer.
[00:39:50.240 --> 00:39:56.740]   Then the next module adopts a windowing configuration that is shifted.
[00:39:56.740 --> 00:40:06.260]   So after this, remember this still remains non-overlapping but now you have like so.
[00:40:06.260 --> 00:40:12.480]   So it overlaps in the previous layer but there is no overlap between these two blue coloured
[00:40:12.480 --> 00:40:14.960]   patches.
[00:40:14.960 --> 00:40:28.520]   With the shifted window partitioning approach, these modules are computed like so and if
[00:40:28.520 --> 00:40:34.680]   you are like me, it's a better idea to skip the math because it always confuses me.
[00:40:34.680 --> 00:40:38.080]   And let's take a look at this image to understand better.
[00:40:38.080 --> 00:40:44.120]   You have this window partitioning and then you have a cyclic shift that helps reduce
[00:40:44.120 --> 00:40:47.240]   the computational load.
[00:40:47.240 --> 00:40:54.340]   Then they talk about how to make efficient batch for this shifted configuration.
[00:40:54.340 --> 00:41:01.560]   The issue with this approach is it's not always computationally efficient.
[00:41:01.560 --> 00:41:08.000]   So they propose this cyclic shifting approach towards the left direction.
[00:41:08.000 --> 00:41:15.480]   So as you can see there is a leftward shift every time in every layer which the authors
[00:41:15.480 --> 00:41:20.320]   claim has more computational efficiency.
[00:41:20.320 --> 00:41:25.440]   With the cyclic shift, I don't like blue, let's use red.
[00:41:25.440 --> 00:41:27.260]   I just don't like blue colour.
[00:41:27.260 --> 00:41:32.760]   With the cyclic shift, the number of batch windows remains the same as the original regular
[00:41:32.760 --> 00:41:38.720]   window partitioning and this is also efficient.
[00:41:38.720 --> 00:41:46.840]   Then they talk about the position bias which I will not cover here.
[00:41:46.840 --> 00:41:51.080]   And now you have this architecture definition.
[00:41:51.080 --> 00:41:57.720]   So to go back and summarize, you take the input, cut it into smaller parts which are
[00:41:57.720 --> 00:42:06.360]   known overlapping, then you apply attention to it, do a cyclic shift and apply these attention
[00:42:06.360 --> 00:42:07.360]   blocks again.
[00:42:07.360 --> 00:42:09.480]   And you keep doing it.
[00:42:09.480 --> 00:42:12.040]   This is more efficient than VITs.
[00:42:12.040 --> 00:42:17.520]   It has a linear computational efficiency and it works really well.
[00:42:17.520 --> 00:42:19.420]   That was a tiny version.
[00:42:19.420 --> 00:42:23.760]   With any architecture you have, small, base and large.
[00:42:23.760 --> 00:42:28.800]   As you can expect, the number C will be increasing here.
[00:42:28.800 --> 00:42:29.800]   What was C?
[00:42:29.800 --> 00:42:36.640]   It was again this.
[00:42:36.640 --> 00:42:40.080]   Output dimension that was being set to 2C.
[00:42:40.080 --> 00:42:47.020]   So whenever you would apply a linear layer on the 4C dimensional concatenated features,
[00:42:47.020 --> 00:42:51.360]   this was making the size smaller and that's what C refers to.
[00:42:51.360 --> 00:42:57.200]   So it's again a number that the authors decide and across different variations of the architecture,
[00:42:57.200 --> 00:42:59.420]   it's being decided like so.
[00:42:59.420 --> 00:43:06.680]   C is the channel number of hidden layers in the first stage.
[00:43:06.680 --> 00:43:09.760]   So let me take a pause and see if there are any questions.
[00:43:09.760 --> 00:43:30.960]   And as a reminder, if you have any questions now is a good time to ask them.
[00:43:30.960 --> 00:43:37.960]   I don't see any questions which means I'm doing a good job or a terrible job.
[00:43:37.960 --> 00:43:41.840]   Awesome, I'll continue nevertheless.
[00:43:41.840 --> 00:43:45.160]   I was just seeing to wait if any questions come in.
[00:43:45.160 --> 00:43:49.600]   As a reminder, I'm happy to answer any questions.
[00:43:49.600 --> 00:43:54.920]   I've only recently reading these papers, so my understanding might be wrong as well.
[00:43:54.920 --> 00:43:58.560]   If there's anything you don't understand, please, please do ask.
[00:43:58.560 --> 00:44:03.160]   If I can't answer it right now, I'll go back, re-read and definitely answer your question
[00:44:03.160 --> 00:44:04.160]   again.
[00:44:04.160 --> 00:44:11.600]   And Sumant, I hope you didn't require the understanding of Transformer so far.
[00:44:11.600 --> 00:44:17.220]   So furthermore, the authors talk about their experiments on ImageNet 1K.
[00:44:17.220 --> 00:44:23.880]   They talk about the settings, what hyperparameters they use, how many epochs they retain for
[00:44:23.880 --> 00:44:27.440]   what form of warmup did they apply.
[00:44:27.440 --> 00:44:33.080]   I don't think this is necessary, so I'm just glancing over the details.
[00:44:33.080 --> 00:44:37.960]   This is important when you're trying to replicate the paper or when you're trying to apply it
[00:44:37.960 --> 00:44:39.840]   to any other problem.
[00:44:39.840 --> 00:44:42.240]   Right now we're just understanding how this works.
[00:44:42.240 --> 00:44:46.200]   And that's my goal here to tell you, hey, this is what SYN Transformer is.
[00:44:46.200 --> 00:44:53.560]   And then you can take these ideas and apply them to your problems of interest or just
[00:44:53.560 --> 00:44:54.960]   try to replicate it.
[00:44:54.960 --> 00:44:59.400]   When you're doing that, that's when these are important.
[00:44:59.400 --> 00:45:04.160]   So I'm skipping again over all of these results.
[00:45:04.160 --> 00:45:12.320]   And the quick summary here is it also worked well on object detection.
[00:45:12.320 --> 00:45:17.360]   And then the authors compare it against the different architectures at that time.
[00:45:17.360 --> 00:45:21.040]   It also performed well on semantic segmentation.
[00:45:21.040 --> 00:45:22.040]   They do the minor comparisons.
[00:45:22.040 --> 00:45:27.000]   The reason I'm not going into these numbers is sometimes these differences are really
[00:45:27.000 --> 00:45:28.000]   small.
[00:45:28.000 --> 00:45:29.840]   As you can see, it's a 2% difference.
[00:45:29.840 --> 00:45:34.840]   And this paper came out in 2021, 2020.
[00:45:34.840 --> 00:45:37.080]   Sorry, I think 2020.
[00:45:37.080 --> 00:45:42.200]   Two years is a lot of time in the world of machine learning.
[00:45:42.200 --> 00:45:49.880]   So these numbers would have changed a lot.
[00:45:49.880 --> 00:45:55.120]   I don't think there's anything else I need to compare or mention here.
[00:45:55.120 --> 00:45:58.760]   So I'll read the conclusion.
[00:45:58.760 --> 00:46:04.480]   The gist of this paper is it produces a hierarchical feature representation and has a linear computational
[00:46:04.480 --> 00:46:08.240]   complexity with respect to the input image size.
[00:46:08.240 --> 00:46:11.360]   SYN Transformer achieves SOTA at that time.
[00:46:11.360 --> 00:46:13.440]   I'm not sure if this is true anymore.
[00:46:13.440 --> 00:46:19.560]   On COCO object detection and semantic segmentation, significantly surpassing at that time the
[00:46:19.560 --> 00:46:23.320]   best methods.
[00:46:23.320 --> 00:46:30.240]   And they conclude because of this, the shifted window self-attention is really effective.
[00:46:30.240 --> 00:46:36.040]   I believe furthermore, they talk about the details of different augmentations that they
[00:46:36.040 --> 00:46:44.760]   had applied, the type of suppression techniques they had applied when doing object detection
[00:46:44.760 --> 00:46:50.960]   or detail of every single layer.
[00:46:50.960 --> 00:46:56.160]   They also create a swim MLP mixer, which was quite interesting to me.
[00:46:56.160 --> 00:47:03.720]   So if you read the MLP mixer paper, they also tried to apply swim layers there.
[00:47:03.720 --> 00:47:07.020]   And I believe that's all I wanted to cover here.
[00:47:07.020 --> 00:47:17.680]   So to summarize again, the architecture takes the image, partitions it into smaller parts
[00:47:17.680 --> 00:47:23.680]   that are non-overlapping, applies attention, merges them and shifts them every time.
[00:47:23.680 --> 00:47:25.240]   How does it shift them?
[00:47:25.240 --> 00:47:29.280]   Cyclically, leftwards and keeps applying attention.
[00:47:29.280 --> 00:47:35.920]   This outperforms VIT and is more scalable than that.
[00:47:35.920 --> 00:47:38.840]   It works better than that.
[00:47:38.840 --> 00:47:40.480]   At that time, it was more accurate.
[00:47:40.480 --> 00:47:47.160]   And it also works well across semantic segmentation and object detection tasks.
[00:47:47.160 --> 00:47:52.640]   So because of the shifted window, which was key to making it work, the name swim transformer
[00:47:52.640 --> 00:47:56.000]   comes from there.
[00:47:56.000 --> 00:47:57.000]   Awesome.
[00:47:57.000 --> 00:48:04.100]   Now I'll glance over any questions and I managed to end a few minutes early so I can always
[00:48:04.100 --> 00:48:11.320]   mention the homework to everyone.
[00:48:11.320 --> 00:48:13.440]   Someone mentioned that you got this emoji.
[00:48:13.440 --> 00:48:16.200]   I've never seen that.
[00:48:16.200 --> 00:48:18.920]   Thank you, Julian.
[00:48:18.920 --> 00:48:28.440]   Again please, if you have any questions, I'm happy to answer them right now.
[00:48:28.440 --> 00:48:33.160]   I'm trying to switch sharing and struggling to find the correct screen.
[00:48:33.160 --> 00:48:40.120]   Now let me hop over to my slides.
[00:48:40.120 --> 00:48:44.440]   I always try to leave a few suggestions.
[00:48:44.440 --> 00:48:51.160]   So I've particularly been curious what happens if you swap out JLU with ReLU.
[00:48:51.160 --> 00:48:54.040]   I have not found an answer yet to this.
[00:48:54.040 --> 00:48:57.400]   Pardon the in-app spacing there.
[00:48:57.400 --> 00:49:01.560]   I would invite you to try this on ImageNet.
[00:49:01.560 --> 00:49:04.160]   I believe that's the correct pronunciation.
[00:49:04.160 --> 00:49:09.360]   This is a more challenging and more accessible version of ImageNet.
[00:49:09.360 --> 00:49:12.280]   I would invite you to try this on active Kaggle competition.
[00:49:12.280 --> 00:49:18.840]   So go to any Kaggle competition, sort by best score, see if they don't use swim transformer,
[00:49:18.840 --> 00:49:19.840]   try to switch the backbone.
[00:49:19.840 --> 00:49:21.640]   You'll get a bunch of errors.
[00:49:21.640 --> 00:49:25.480]   Try to get rid of the errors, try to get the model to train and see if you can improve
[00:49:25.480 --> 00:49:26.480]   the accuracy.
[00:49:26.480 --> 00:49:33.720]   That's a lot of work, but please do tell us if you try that.
[00:49:33.720 --> 00:49:37.800]   And I will be uploading this to the annotated paper repository.
[00:49:37.800 --> 00:49:42.320]   So I invite you to check it out afterwards.
[00:49:42.320 --> 00:49:44.680]   That's all I had for homework suggestions.
[00:49:44.680 --> 00:49:46.460]   I'll again wait a minute.
[00:49:46.460 --> 00:49:50.680]   If not, we can wrap up early today.
[00:49:50.680 --> 00:49:59.680]   I see a question and I'll address that.
[00:49:59.680 --> 00:50:03.240]   As a reminder, this is where you can find these papers.
[00:50:03.240 --> 00:50:14.200]   Tony, I'll just get to your comment.
[00:50:14.200 --> 00:50:19.480]   Can you please give some advice on how to get into a stage when reading this paper becomes
[00:50:19.480 --> 00:50:22.160]   more natural?
[00:50:22.160 --> 00:50:25.160]   You don't need to read papers.
[00:50:25.160 --> 00:50:30.160]   Seriously you don't need to read papers even if you're a machine learning engineer.
[00:50:30.160 --> 00:50:33.680]   That's a broad statement that could get me into trouble.
[00:50:33.680 --> 00:50:39.200]   But what you need is the ability to iterate on ideas faster.
[00:50:39.200 --> 00:50:40.200]   Right?
[00:50:40.200 --> 00:50:47.440]   And that only comes through either applying these techniques a lot and then slowly reading
[00:50:47.440 --> 00:50:48.440]   the paper.
[00:50:48.440 --> 00:50:49.660]   It's always frustrating.
[00:50:49.660 --> 00:50:51.200]   It's always annoying.
[00:50:51.200 --> 00:51:05.640]   This paper was by Microsoft Research and Microsoft Research's papers are more accessible.
[00:51:05.640 --> 00:51:10.520]   So the thing to understand here is some papers are more readable than others.
[00:51:10.520 --> 00:51:14.160]   I have no affiliation with Microsoft by the way.
[00:51:14.160 --> 00:51:16.680]   I just enjoy reading their papers.
[00:51:16.680 --> 00:51:22.520]   But different labs sometimes have a higher bar of accessibility to papers.
[00:51:22.520 --> 00:51:23.520]   Nothing against anyone.
[00:51:23.520 --> 00:51:27.680]   Sometimes they tend to have more jargon.
[00:51:27.680 --> 00:51:29.400]   So it also varies from paper to paper.
[00:51:29.400 --> 00:51:33.080]   I myself struggle a lot sometimes with reading papers.
[00:51:33.080 --> 00:51:34.640]   Honestly there's no cheat code to it.
[00:51:34.640 --> 00:51:38.560]   Please keep coming to our sessions and asking me any questions.
[00:51:38.560 --> 00:51:44.400]   If I can't help you read papers faster, I'm doing a terrible job and tell me and I'll
[00:51:44.400 --> 00:51:50.000]   make sure you're able to read the papers faster than you were earlier.
[00:51:50.000 --> 00:51:52.520]   So that's the only suggestion I have.
[00:51:52.520 --> 00:51:57.680]   Tony mentions they saw a few Kaggle notebooks in the Pet Finder competition.
[00:51:57.680 --> 00:51:58.680]   That is awesome.
[00:51:58.680 --> 00:51:59.680]   Let's see.
[00:51:59.680 --> 00:52:02.880]   Let's try to find them if I can.
[00:52:02.880 --> 00:52:06.600]   Or that might be too slow for me right now.
[00:52:06.600 --> 00:52:08.740]   But Tony says that's why he's here.
[00:52:08.740 --> 00:52:13.120]   So hopefully Tony that helped you understand the architecture better.
[00:52:13.120 --> 00:52:19.200]   And what many people end up doing on Kaggle is also you see these architectures and you
[00:52:19.200 --> 00:52:20.840]   just sort out the backbone.
[00:52:20.840 --> 00:52:26.360]   Sometimes you don't even understand what's going on and you see an improvement in accuracy.
[00:52:26.360 --> 00:52:27.360]   That's cool right?
[00:52:27.360 --> 00:52:31.440]   Your end goal is to improve accuracy on Kaggle and you're able to do it.
[00:52:31.440 --> 00:52:36.160]   But if you want to then pull back the curtains, try to understand why it works.
[00:52:36.160 --> 00:52:39.840]   See if you can improve it even further.
[00:52:39.840 --> 00:52:41.880]   We've covered this in the Chai Time Talks.
[00:52:41.880 --> 00:52:47.000]   Christy, the legend, Kaggle legend spoke about this.
[00:52:47.000 --> 00:52:49.420]   But for that you need to understand the architecture better.
[00:52:49.420 --> 00:52:55.040]   So this wasn't a comment for Tony but it was for the previous comment.
[00:52:55.040 --> 00:52:58.080]   That's why reading papers is somewhat beneficial.
[00:52:58.080 --> 00:52:59.640]   Awesome.
[00:52:59.640 --> 00:53:05.120]   I hope Tony you found this session useful and I hope I was able to explain Twin Transformer.
[00:53:05.120 --> 00:53:08.480]   I'll try to find you on the leaderboard of Pet Finder.
[00:53:08.480 --> 00:53:12.120]   Hopefully you're in the top 100.
[00:53:12.120 --> 00:53:14.480]   I'll try to find your solution as well.
[00:53:14.480 --> 00:53:18.360]   But again, I'll try to wrap up here since I don't see any other questions.
[00:53:18.360 --> 00:53:19.680]   Thanks again everyone for joining.
[00:53:19.680 --> 00:53:23.120]   We'll meet again in one month.
[00:53:23.120 --> 00:53:28.200]   We've seen a lot of CVPR papers come in and I'll try to read the interesting ones and
[00:53:28.200 --> 00:53:30.460]   annotate and upload them here.
[00:53:30.460 --> 00:53:34.200]   So if you're curious of any paper annotations, right now there's just one.
[00:53:34.200 --> 00:53:37.720]   I'll upload the Twin Transformer ASAP there.
[00:53:37.720 --> 00:53:43.920]   Check out this repository and you can follow along in your free time at your own pace.
[00:53:43.920 --> 00:53:45.640]   Thanks again for joining and I'll see you in one month.
[00:53:45.640 --> 00:53:55.640]   [BLANK_AUDIO]

