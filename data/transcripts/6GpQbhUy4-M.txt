
[00:00:00.000 --> 00:00:05.760]   Hey everyone, I'm Aleksa and I am co-founder and CEO of Jervis.
[00:00:05.760 --> 00:00:13.000]   And I will be talking to you about how we made our project management software run with
[00:00:13.000 --> 00:00:19.800]   NLP fast and how we actually don't even need UI that much anymore since we started using
[00:00:19.800 --> 00:00:21.760]   NLP this way.
[00:00:21.760 --> 00:00:26.160]   So first of all, we've all been witnessing some of the big milestones happening, especially
[00:00:26.160 --> 00:00:31.880]   in the last few years in NLP and especially since BERT came out, there has been some increased
[00:00:31.880 --> 00:00:36.000]   interest in conversational AI.
[00:00:36.000 --> 00:00:39.660]   And with this increased interest came more NLP in production.
[00:00:39.660 --> 00:00:44.760]   So conversation, it slowly started turning from, is this even possible to how can we
[00:00:44.760 --> 00:00:48.560]   make it better and faster and actually run in production?
[00:00:48.560 --> 00:00:54.520]   So with Jervis, we've encountered many problems related to, especially to inference speed.
[00:00:54.520 --> 00:00:59.400]   So we had to solve them fast just so we can get them to our users.
[00:00:59.400 --> 00:01:04.360]   And first I will talk a bit about what is Jervis and why the speed is this important
[00:01:04.360 --> 00:01:05.520]   to us.
[00:01:05.520 --> 00:01:10.040]   So Jervis automates repetitive parts of project management using natural language processing
[00:01:10.040 --> 00:01:15.160]   and makes getting information in and out of the software available with a single sentence.
[00:01:15.160 --> 00:01:19.480]   And since there is a gap between what employees want and that is the focus on their work and
[00:01:19.480 --> 00:01:25.280]   what managers want to have as much information to draw conclusions from, this gap becomes
[00:01:25.280 --> 00:01:29.600]   even wider and wider with remote and asynchronous work.
[00:01:29.600 --> 00:01:34.440]   So as we can see on these graphs, what we do is try to take away all the communication
[00:01:34.440 --> 00:01:41.360]   between managers, managers, employees, employees themselves, and go through Jervis, all of
[00:01:41.360 --> 00:01:48.440]   this communication and data processing so that we can just work remotely and asynchronous.
[00:01:48.440 --> 00:01:53.320]   And we do this with a system of smart notifications from which you can update work through either
[00:01:53.320 --> 00:01:59.520]   text or voice and receive status updates without even slightest needs to go to UI and fill
[00:01:59.520 --> 00:02:00.740]   out forms.
[00:02:00.740 --> 00:02:05.880]   So we analyze all this information and present it to managers in a way that they can find
[00:02:05.880 --> 00:02:08.320]   useful and draw conclusions from.
[00:02:08.320 --> 00:02:12.960]   And when you compare this to the current process, which requires you to manually search, enter,
[00:02:12.960 --> 00:02:18.680]   and submit everything, this requires and requires managers to either make sure everybody did
[00:02:18.680 --> 00:02:22.720]   this or they had to work with incomplete data.
[00:02:22.720 --> 00:02:25.480]   We can notice how much time can be saved.
[00:02:25.480 --> 00:02:33.000]   But to do this, we have to have our conversational AI actually work in real time.
[00:02:33.000 --> 00:02:36.940]   And for this, and not only real time, but of course, be accurate.
[00:02:36.940 --> 00:02:42.640]   And for this to be possible, we need both accuracy and speed to be going for us.
[00:02:42.640 --> 00:02:50.240]   So if I'm to be honest, our first version of Jervis, we were first of all researchers
[00:02:50.240 --> 00:02:55.440]   and then I guess gotten more into the development.
[00:02:55.440 --> 00:03:00.480]   So we started thinking like researchers and let's see what's the best accuracy we can
[00:03:00.480 --> 00:03:01.480]   get.
[00:03:01.480 --> 00:03:08.160]   So we started just badly when it comes to productionizing that model that we were making.
[00:03:08.160 --> 00:03:12.920]   And we bought into the hype and thought we could do everything with a large do it all
[00:03:12.920 --> 00:03:18.720]   transformer with some additional layers, like fine tuning and everything.
[00:03:18.720 --> 00:03:24.360]   And we've written all the codes sequentially, especially like the pre and post processing
[00:03:24.360 --> 00:03:25.360]   code.
[00:03:25.360 --> 00:03:28.760]   So we didn't really care about anything away from that model.
[00:03:28.760 --> 00:03:34.160]   So we were just being like, okay, we will build one big model with all the data that
[00:03:34.160 --> 00:03:36.840]   we have gathered crammed up into it.
[00:03:36.840 --> 00:03:40.520]   And this will give us great result because all of this data is connected and all of these
[00:03:40.520 --> 00:03:42.480]   tasks are connected.
[00:03:42.480 --> 00:03:48.520]   And we did get good accuracy, but our whole process from users sending the message to
[00:03:48.520 --> 00:03:52.160]   them receiving the response, it took around 10 seconds.
[00:03:52.160 --> 00:03:54.160]   And that's only for one conversation.
[00:03:54.160 --> 00:03:59.480]   If there are like concurrent conversations going on, this request takes even longer and
[00:03:59.480 --> 00:04:02.440]   performance just started rapidly going down.
[00:04:02.440 --> 00:04:07.120]   So we knew we had to improve this and that we couldn't do like full production with this
[00:04:07.120 --> 00:04:08.640]   type of model.
[00:04:08.640 --> 00:04:13.880]   And I mean, first thing that comes to our mind in this case is just, we were testing
[00:04:13.880 --> 00:04:15.640]   this out on CPU.
[00:04:15.640 --> 00:04:20.700]   Let's see how it works on GPU and see if this is a good enough option.
[00:04:20.700 --> 00:04:27.680]   And it did improve, but not nearly enough for this option to be viable because it will
[00:04:27.680 --> 00:04:32.480]   be a lot expensive since we would still have to have a lot of GPUs for a lot of concurrent
[00:04:32.480 --> 00:04:34.280]   conversations.
[00:04:34.280 --> 00:04:39.160]   And we also like try to code things a little differently, like vectorizing the code wherever
[00:04:39.160 --> 00:04:44.560]   we can, less comprehensive generators, just trying to follow, I guess, good practices
[00:04:44.560 --> 00:04:48.000]   when it comes to running the Python code faster.
[00:04:48.000 --> 00:04:55.920]   But the thing is that we were using too much data science where we didn't have to, and
[00:04:55.920 --> 00:05:02.360]   especially machine learning, and we weren't specializing parts of our system enough.
[00:05:02.360 --> 00:05:07.480]   We wanted to do everything sequentially going to our big model that works so great.
[00:05:07.480 --> 00:05:11.400]   And I mean, everybody gets excited when they get this one big model that can do so many
[00:05:11.400 --> 00:05:17.120]   things, but we knew we couldn't get this to users and get them to use it this way.
[00:05:17.120 --> 00:05:23.700]   So we had to stay back and think how can we redesign the system to work better for us.
[00:05:23.700 --> 00:05:27.280]   And the little things that we didn't care about during the research really started to
[00:05:27.280 --> 00:05:31.560]   matter when it comes to using this model in production.
[00:05:31.560 --> 00:05:39.300]   So wherever it didn't occur to the accuracy, what we started doing is breaking down the
[00:05:39.300 --> 00:05:42.640]   models into atomic inference.
[00:05:42.640 --> 00:05:51.840]   So what I mean by that is we actually wanted one model to do its specific thing.
[00:05:51.840 --> 00:05:54.680]   And the lowest possible specific thing.
[00:05:54.680 --> 00:06:01.160]   If we can boil it down to simple classification, that's what we are just going with.
[00:06:01.160 --> 00:06:07.100]   And we a bit stepped away from deep learning for some of the models.
[00:06:07.100 --> 00:06:12.400]   Some of them, of course, they'll have to use it, especially those BERT-based models.
[00:06:12.400 --> 00:06:20.160]   And in some parts, we even totally stood away from machine learning data science.
[00:06:20.160 --> 00:06:23.520]   And some we just did like statistical models, let's say.
[00:06:23.520 --> 00:06:30.060]   And what it turns out is not that it only made it faster, but it also made it a bit
[00:06:30.060 --> 00:06:36.460]   more accurate when we use statistical models where we can.
[00:06:36.460 --> 00:06:39.980]   But I mean, of course, when there is context involved in everything, we can't.
[00:06:39.980 --> 00:06:46.620]   But for example, if we have to match closest name to the one that was misspelled, statistical
[00:06:46.620 --> 00:06:48.700]   models will still battle on that.
[00:06:48.700 --> 00:06:55.180]   And unless we want to do complete, like for English names, we can do that.
[00:06:55.180 --> 00:07:00.340]   But for some names that aren't that common in data sets, we had to do some statistical
[00:07:00.340 --> 00:07:01.340]   models.
[00:07:01.340 --> 00:07:03.500]   And the second thing we did is just what I mentioned.
[00:07:03.500 --> 00:07:08.660]   So move away from deep learning when needed and when possible.
[00:07:08.660 --> 00:07:12.900]   We also started harnessing power of concurrency.
[00:07:12.900 --> 00:07:19.140]   And this is where a lot of time has to be spent just thinking which parts can actually
[00:07:19.140 --> 00:07:20.500]   be done concurrently.
[00:07:20.500 --> 00:07:26.460]   Because some parts do have some sort of relationship where you have to do them sequentially.
[00:07:26.460 --> 00:07:32.940]   But it's actually much more common that you can do concurrent things that just speed up
[00:07:32.940 --> 00:07:34.420]   things a lot.
[00:07:34.420 --> 00:07:40.180]   And in our case, we also moved away from Python to go to Go for non-machine learning stuff.
[00:07:40.180 --> 00:07:43.060]   Of course, for machine learning, we still kept Python.
[00:07:43.060 --> 00:07:49.220]   And this improves our speed incredibly because it harnessed innate concurrency that Go has.
[00:07:49.220 --> 00:07:53.620]   But I mean, if somebody is more comfortable with Python, they're also like Cython and
[00:07:53.620 --> 00:08:00.940]   some other solutions, Spacey, that speed up execution in Python without having to change
[00:08:00.940 --> 00:08:02.840]   the language.
[00:08:02.840 --> 00:08:08.140]   So with all this in place, we had already improved our model considerably.
[00:08:08.140 --> 00:08:12.860]   But as number of concurrent requests grew, we wanted to do something more just in case.
[00:08:12.860 --> 00:08:18.380]   So we wouldn't fall again into the same thing that we had before where we thought we were
[00:08:18.380 --> 00:08:19.380]   good enough.
[00:08:19.380 --> 00:08:23.880]   But then it turns out when the number of requests increases, we're not.
[00:08:23.880 --> 00:08:33.760]   So what we wanted to just check out, how can we actually build this inference to be better?
[00:08:33.760 --> 00:08:39.640]   So until now, we have building everything around the models themselves to be better
[00:08:39.640 --> 00:08:41.720]   and breaking them down and everything.
[00:08:41.720 --> 00:08:47.560]   But we wanted to see if we can also make those models work faster and infer everything that
[00:08:47.560 --> 00:08:49.620]   we've trained them faster.
[00:08:49.620 --> 00:08:55.580]   And some of the things that we tried next, first, we started using TVM on AWS.
[00:08:55.580 --> 00:09:01.080]   And this helped model inference speed, but we wanted to see if we can push it further.
[00:09:01.080 --> 00:09:06.480]   So we tried ONNX Runtime, which turned out to have massive impact on us.
[00:09:06.480 --> 00:09:11.580]   And for those who might not be familiar with this, ONNX is a format representing ML models
[00:09:11.580 --> 00:09:17.040]   and the ONNX Runtime is a high performance engine for running these models.
[00:09:17.040 --> 00:09:25.160]   And it's better like our inference drastically, so much so that right now, for each of our
[00:09:25.160 --> 00:09:28.560]   models, it's around 10 milliseconds inference.
[00:09:28.560 --> 00:09:33.720]   And for some of them, it's in single digits for some of the simpler models.
[00:09:33.720 --> 00:09:38.520]   And this makes the whole conversation look completely real time.
[00:09:38.520 --> 00:09:44.640]   Even if we have a lot of pre-processing and post-processing after that inference, it still
[00:09:44.640 --> 00:09:48.680]   helps a lot to make it completely real time.
[00:09:48.680 --> 00:09:53.280]   And another thing that we would like to try soon as we start implementing, especially
[00:09:53.280 --> 00:09:59.600]   like document summarization, question answering, and some more complex feature is a TensorRT
[00:09:59.600 --> 00:10:03.200]   to see if this can speed us up even more.
[00:10:03.200 --> 00:10:08.760]   And if someone has good insight on how this works with transformers models, please do
[00:10:08.760 --> 00:10:15.200]   tell us so we would know if we should pursue it further.
[00:10:15.200 --> 00:10:17.360]   And yeah, thank you for attention.
[00:10:17.360 --> 00:10:21.920]   And if you would like to hear more about Jervis or chat about machine learning and conversational
[00:10:21.920 --> 00:10:28.280]   AI in general, you can reach out either by email or my Twitter, border stated here.
[00:10:28.280 --> 00:10:31.120]   And yeah, now I guess we're moving to QA.
[00:10:31.120 --> 00:10:33.360]   Yeah, great.
[00:10:33.360 --> 00:10:35.160]   Thanks a lot for the talk, Alexa.
[00:10:35.160 --> 00:10:41.560]   And yeah, I encourage folks who are in the Zoom or on the YouTube, in the YouTube chat
[00:10:41.560 --> 00:10:43.640]   to post their questions as we're going.
[00:10:43.640 --> 00:10:44.840]   I've got lots of questions.
[00:10:44.840 --> 00:10:46.240]   I'm very excited to ask them.
[00:10:46.240 --> 00:10:51.800]   So the first thing I'd like to ask is sort of towards the end there, more of a detail
[00:10:51.800 --> 00:10:53.360]   oriented question.
[00:10:53.360 --> 00:10:56.840]   But why did you choose the ONNX format?
[00:10:56.840 --> 00:11:00.960]   Like what made you look into that originally?
[00:11:00.960 --> 00:11:05.400]   And also, what do you think is the secret behind why I was able to give you that big
[00:11:05.400 --> 00:11:06.800]   performance boost?
[00:11:06.800 --> 00:11:11.920]   Yeah, so it wasn't like the format itself.
[00:11:11.920 --> 00:11:17.200]   But it's actually the ONNX runtime, which Microsoft developed.
[00:11:17.200 --> 00:11:24.720]   And they especially updated it like a year ago, I think for BERT, which we are using
[00:11:24.720 --> 00:11:25.720]   underneath.
[00:11:25.720 --> 00:11:31.360]   So they even, I mean, this was in research settings in their lab settings, but they got
[00:11:31.360 --> 00:11:37.080]   like nearly 20 times improvement on both CPU and GPU.
[00:11:37.080 --> 00:11:38.340]   So that got us going.
[00:11:38.340 --> 00:11:43.280]   We didn't get, let's say, that much, but we still got like five to 10 times X, which
[00:11:43.280 --> 00:11:45.220]   is pretty, pretty good.
[00:11:45.220 --> 00:11:48.040]   And we're right now able to even run it on CPU.
[00:11:48.040 --> 00:11:52.240]   Like we don't need any GPUs for the inference.
[00:11:52.240 --> 00:11:53.240]   I see.
[00:11:53.240 --> 00:11:59.260]   Yeah, because I guess, so I have come across the ONNX format only as just sort of this
[00:11:59.260 --> 00:12:03.360]   attempt to get everybody to play nicely with each other by sort of some of the people coming
[00:12:03.360 --> 00:12:04.440]   into the field later, right?
[00:12:04.440 --> 00:12:07.900]   You've got TensorFlow and PyTorch, both of their solutions.
[00:12:07.900 --> 00:12:12.220]   And it seems like a lot of other people have been like, okay, can we agree on an open format?
[00:12:12.220 --> 00:12:16.600]   So it's actually, this is great news to me to hear that the open format can also bring
[00:12:16.600 --> 00:12:20.060]   you these performance benefits.
[00:12:20.060 --> 00:12:21.060]   Yeah.
[00:12:21.060 --> 00:12:24.960]   So I mean, Microsoft is the one pushing that format, especially.
[00:12:24.960 --> 00:12:29.140]   So that's probably why they created this runtime.
[00:12:29.140 --> 00:12:34.000]   But when we like research the bit deeper, this runtime really, really helps.
[00:12:34.000 --> 00:12:35.000]   Great.
[00:12:35.000 --> 00:12:36.000]   That's cool.
[00:12:36.000 --> 00:12:43.320]   That's a nice detail about operating these things that I was not aware of.
[00:12:43.320 --> 00:12:47.400]   You mentioned being able to deploy just a CPU being important.
[00:12:47.400 --> 00:12:50.160]   Is that mostly just to reduce cloud costs?
[00:12:50.160 --> 00:12:55.080]   Or is this something that you can see maybe running on unspecialized hardware in phones
[00:12:55.080 --> 00:12:56.400]   or something like that?
[00:12:56.400 --> 00:12:57.400]   Yeah.
[00:12:57.400 --> 00:13:03.800]   So we right now are doing this on AWS and we're just trying to reduce the cost as much
[00:13:03.800 --> 00:13:04.800]   as possible.
[00:13:04.800 --> 00:13:11.320]   And I mean, we didn't even need that last steps that I like put the last slide, but
[00:13:11.320 --> 00:13:14.580]   we still have it like real time even without that.
[00:13:14.580 --> 00:13:19.600]   So when we implemented this, this made it like, it gave us a lot of space to just go
[00:13:19.600 --> 00:13:23.500]   completely away from GPU for now, at least.
[00:13:23.500 --> 00:13:29.020]   But yeah, at some point it would be great to just be able to also do it on the phone
[00:13:29.020 --> 00:13:35.180]   directly so that you don't have to actually have an internet connection to, I mean, if
[00:13:35.180 --> 00:13:39.260]   you're on the airplane and you can't do, you're just remembering something so that you can
[00:13:39.260 --> 00:13:43.400]   do it right away and then it gets uploaded when you get connected to the wifi.
[00:13:43.400 --> 00:13:46.060]   So that is something that we are looking into.
[00:13:46.060 --> 00:13:47.060]   I see.
[00:13:47.060 --> 00:13:48.060]   Yeah.
[00:13:48.060 --> 00:13:50.860]   You also mentioned the sort of one of the pieces of the transition from your original
[00:13:50.860 --> 00:13:55.240]   application to the second app, the like more performant one was you moved from one like
[00:13:55.240 --> 00:13:58.340]   single almighty transformer model to many small models.
[00:13:58.340 --> 00:14:02.940]   So this is something that it seems to have been coming up like quite a bit in a lot of
[00:14:02.940 --> 00:14:07.540]   the conversations I've been having with folks on the salon that people are moving over to
[00:14:07.540 --> 00:14:12.040]   these multi-model model amalgamation type pipelines.
[00:14:12.040 --> 00:14:15.660]   So I was just wondering if you'd comment a little bit more about that process, what worked
[00:14:15.660 --> 00:14:20.520]   well, what didn't, what were the pain points and the tools that you use to just switch
[00:14:20.520 --> 00:14:22.020]   over your, your pipeline?
[00:14:22.020 --> 00:14:26.220]   Cause it's quite a different thing to manage a zoo of models than a single one.
[00:14:26.220 --> 00:14:27.220]   Yeah.
[00:14:27.220 --> 00:14:28.220]   Yeah.
[00:14:28.220 --> 00:14:34.660]   So at first let's say we, we saw that a lot of people in the beginning, since we started
[00:14:34.660 --> 00:14:42.000]   working on this in the early 2019, so a lot of people at that time were just taking it
[00:14:42.000 --> 00:14:43.000]   straight away.
[00:14:43.000 --> 00:14:46.600]   It was probably hype, but everybody was like, yeah, this is a silver bullet.
[00:14:46.600 --> 00:14:49.340]   So we were, we were like kind of onto that hype.
[00:14:49.340 --> 00:14:56.300]   And then we saw that I think Sonos acquired the company for like 50 million did use, especially
[00:14:56.300 --> 00:14:57.740]   like just one model.
[00:14:57.740 --> 00:15:01.100]   And like they used one joint model for like a bunch of NLP tasks.
[00:15:01.100 --> 00:15:06.340]   And we were like, you know, if they can build it that good to be actually acquired, I mean,
[00:15:06.340 --> 00:15:07.740]   there must be something there.
[00:15:07.740 --> 00:15:13.140]   So we started just going straight to, to the joint model that does like a bunch of other
[00:15:13.140 --> 00:15:14.860]   tasks.
[00:15:14.860 --> 00:15:20.260]   But like as, as, as we went going on about it, we noticed that a lot of those things
[00:15:20.260 --> 00:15:23.220]   can be done concurrently, like concurrently.
[00:15:23.220 --> 00:15:29.680]   So what, instead of actually just going everything sequentially to that one model, we started
[00:15:29.680 --> 00:15:36.100]   building a lot of models that can be done in different like timestamps.
[00:15:36.100 --> 00:15:42.600]   And we did like, I guess we did have to work a bit more on the architecture since we have
[00:15:42.600 --> 00:15:47.940]   to have like, we have gRPC communicating between those microservices and everything.
[00:15:47.940 --> 00:15:55.400]   So we did have to do a bit more architecture designing in it and just implementation details.
[00:15:55.400 --> 00:16:02.180]   But it helped us even, even with all the networking costs between those models, it's still, it
[00:16:02.180 --> 00:16:04.500]   still helped us a lot with the speed.
[00:16:04.500 --> 00:16:06.540]   I see.
[00:16:06.540 --> 00:16:11.180]   But that is, I guess, the biggest thing I see as somebody who's more of a like machine
[00:16:11.180 --> 00:16:15.780]   learning researcher than a machine learning engineer or an ML ops person, that seems like
[00:16:15.780 --> 00:16:21.620]   a pretty big hurdle to come over this like ops problem of integrating that whole pipeline.
[00:16:21.620 --> 00:16:28.820]   So it seemed at first actually, but it's not like once you build a communication between
[00:16:28.820 --> 00:16:34.340]   two services, the rest become like incrementally simpler.
[00:16:34.340 --> 00:16:39.140]   So there's a lot of, there's a lot of code that you can basically just copy out.
[00:16:39.140 --> 00:16:44.480]   It's like for the communication and we completely removed states from most of these things.
[00:16:44.480 --> 00:16:47.820]   So we didn't actually have to save states at any point.
[00:16:47.820 --> 00:16:50.400]   It was just basically kicking the states around.
[00:16:50.400 --> 00:16:54.900]   So bunch of codes was similar, just receiving the state, then packing it up for the next
[00:16:54.900 --> 00:16:55.900]   thing.
[00:16:55.900 --> 00:17:00.220]   So it, even though it seems like at first as a big hurdle to overcome, it's actually
[00:17:00.220 --> 00:17:01.220]   not.
[00:17:01.220 --> 00:17:07.600]   We did it like in less than a month, just switching to that.
[00:17:07.600 --> 00:17:12.260]   So that's kind of related to another broader question that I wanted to ask.
[00:17:12.260 --> 00:17:16.440]   You mentioned that early on you were effectively doing like too much data science, too much
[00:17:16.440 --> 00:17:18.500]   machine learning.
[00:17:18.500 --> 00:17:23.680]   So how do we take some of these, and you eventually learned the lesson to kind of pare that back.
[00:17:23.680 --> 00:17:29.120]   So how do we take like that lesson and the lesson of like, it's a good idea to have multiple
[00:17:29.120 --> 00:17:33.740]   smaller services communicating with each other and sort of incorporate them maybe earlier
[00:17:33.740 --> 00:17:34.740]   in our workflow.
[00:17:34.740 --> 00:17:38.340]   Because when I talk to people, it seems like they all, like a lot of them have that same
[00:17:38.340 --> 00:17:44.820]   trajectory of pairing things away and adding more like of a complicated MLOps structure.
[00:17:44.820 --> 00:17:47.860]   So what do you think we can do to get those earlier in our development cycle?
[00:17:47.860 --> 00:17:48.860]   Yeah.
[00:17:48.860 --> 00:17:53.780]   So I mean, for us, it was, it was just, I guess, trying it out and seeing what works,
[00:17:53.780 --> 00:17:59.180]   what doesn't, and probably like everything won't work the same for everybody, but just
[00:17:59.180 --> 00:18:03.980]   trying to think, what can you break down as, as for the like multiple?
[00:18:03.980 --> 00:18:07.860]   Like which parts can you break down into simpler problems?
[00:18:07.860 --> 00:18:12.540]   Like you don't have to have a silver bullet and you usually can't have a silver bullet
[00:18:12.540 --> 00:18:13.540]   for everything.
[00:18:13.540 --> 00:18:20.060]   So just trying to think what, what can you and how can you break down this problem into
[00:18:20.060 --> 00:18:22.260]   much more like smaller problems.
[00:18:22.260 --> 00:18:27.560]   And then also are those smaller problems actually best solved this way?
[00:18:27.560 --> 00:18:35.800]   So don't think like, just, just try not to go with the, with the feeling, let's say,
[00:18:35.800 --> 00:18:39.000]   but actually the reasonable explanation.
[00:18:39.000 --> 00:18:42.600]   So we were like, yeah, machine learning is totally going to kill it here.
[00:18:42.600 --> 00:18:47.840]   But it turns out that it doesn't end like statistical models are much better matching
[00:18:47.840 --> 00:18:53.320]   like misspelled names than, than any deep learning or even like basic machine learning
[00:18:53.320 --> 00:18:54.320]   model.
[00:18:54.320 --> 00:18:55.320]   So just, just-
[00:18:55.320 --> 00:18:58.360]   So just a quick clarification, when you say statistical models, are you talking about
[00:18:58.360 --> 00:19:04.440]   things like, I know like Markov models for language or do you just mean like, yeah, like
[00:19:04.440 --> 00:19:08.760]   probability, like forming a probability distribution over tokens?
[00:19:08.760 --> 00:19:10.320]   Like what are you talking about here?
[00:19:10.320 --> 00:19:11.320]   Yeah.
[00:19:11.320 --> 00:19:13.440]   Like what we use was Gerard-Winkler.
[00:19:13.440 --> 00:19:16.200]   So like, or like Clevens-Stein's distance.
[00:19:16.200 --> 00:19:22.520]   So something, something that's, yeah, that's not really, it's not even, I guess, a complete
[00:19:22.520 --> 00:19:23.520]   model.
[00:19:23.520 --> 00:19:31.200]   It's just computing what's, what's the closest to something else in some sort of way.
[00:19:31.200 --> 00:19:32.200]   I see.
[00:19:32.200 --> 00:19:33.200]   Yeah.
[00:19:33.200 --> 00:19:36.400]   So often people say that ML can replace the need for hand-engineered features, right?
[00:19:36.400 --> 00:19:39.960]   So like the SIFT features or Laplacian pyramids for images.
[00:19:39.960 --> 00:19:42.680]   I know a little bit more about images than I do about NLP.
[00:19:42.680 --> 00:19:44.840]   So those are the examples that immediately come to mind.
[00:19:44.840 --> 00:19:49.120]   Like those have been effectively replaced in a lot of miles models by like the first
[00:19:49.120 --> 00:19:51.780]   couple layers of a conf net essentially.
[00:19:51.780 --> 00:19:55.940]   But that doesn't mean that there aren't tons of places where actually doing something like
[00:19:55.940 --> 00:20:00.420]   that and pulling out those nice features can get you really good performance on one part
[00:20:00.420 --> 00:20:02.060]   of what your model needs to do.
[00:20:02.060 --> 00:20:03.500]   Yeah, yeah, exactly.
[00:20:03.500 --> 00:20:08.660]   And I actually started as a computer like vision engineer as well.
[00:20:08.660 --> 00:20:16.100]   So when it comes to that part, even though we're like for hand gestures, we did use deep
[00:20:16.100 --> 00:20:25.740]   learning, so, but like for, for some kind of, to actually detect some things like movement
[00:20:25.740 --> 00:20:29.700]   or something like that, we used a bit simpler models.
[00:20:29.700 --> 00:20:33.980]   So I mean, it was like five years ago, so I'm not that like great remembering exact
[00:20:33.980 --> 00:20:39.780]   details, but I actually do have like a patent for a shelf during that time.
[00:20:39.780 --> 00:20:44.300]   But yeah, I, I, I'm much more into natural language processing right now.
[00:20:44.300 --> 00:20:45.300]   Yeah.
[00:20:45.300 --> 00:20:48.900]   So I guess I did want to get a little bit of your opinions on what's going on in natural
[00:20:48.900 --> 00:20:52.140]   processing these days, natural language processing these days.
[00:20:52.140 --> 00:20:58.720]   So the like the zeitgeist now seems to be like transformers, transformers are it.
[00:20:58.720 --> 00:21:02.820]   We just need to get like the attention mechanism, we need to find a way to make that efficient.
[00:21:02.820 --> 00:21:08.740]   Then we need to scale that up, work with internet scale data, like, you know, GPT three is the
[00:21:08.740 --> 00:21:10.580]   move now we just need to make it smaller.
[00:21:10.580 --> 00:21:14.540]   Do you think this is the good direction for the NLP community to move in?
[00:21:14.540 --> 00:21:16.740]   Do you think this is, is the right one?
[00:21:16.740 --> 00:21:20.340]   So I think it depends on what actually you want NLP to do.
[00:21:20.340 --> 00:21:26.660]   So if you need NLP to just analyze the text, extract some information from it.
[00:21:26.660 --> 00:21:32.080]   And like, like just those, that sort of things that the transformer can be really great,
[00:21:32.080 --> 00:21:36.500]   but just like you said, like getting more data, making it more sophisticated and everything.
[00:21:36.500 --> 00:21:40.860]   But I don't think it's a solution to a broader problem of language.
[00:21:40.860 --> 00:21:48.020]   So I don't think that actually it can solve communication that isn't, let's say, even
[00:21:48.020 --> 00:21:49.820]   like you don't know what's going to happen.
[00:21:49.820 --> 00:21:56.780]   It's not really predefined, but it's not like an actual conversation going on since like
[00:21:56.780 --> 00:22:01.940]   it can just spew out anything that it read from the internet, which can be good, bad,
[00:22:01.940 --> 00:22:02.940]   terrible.
[00:22:02.940 --> 00:22:05.780]   So whatever is the most common connection with something else.
[00:22:05.780 --> 00:22:11.460]   So it's not like, it's not reason, reason communication per se.
[00:22:11.460 --> 00:22:14.140]   I see.
[00:22:14.140 --> 00:22:17.800]   So do you think that that is something, some of that comes from the choice of using the
[00:22:17.800 --> 00:22:21.980]   cross entropy loss, this sort of like contrastive learning thing where you're trying to say,
[00:22:21.980 --> 00:22:26.220]   okay, like predict this next token by using all these negative examples of like, it's
[00:22:26.220 --> 00:22:30.580]   not, you know, you know, masking things, all these kinds of, of approaches.
[00:22:30.580 --> 00:22:35.580]   So do you think, do you think that's an architectural limitation or do you think that's a problem
[00:22:35.580 --> 00:22:38.620]   specification limitation there?
[00:22:38.620 --> 00:22:44.660]   So for me, I think it's actually like the problem specification since it's, it's completely
[00:22:44.660 --> 00:22:54.940]   different to have some sort of real reasoning to get to real reasoning from that.
[00:22:54.940 --> 00:23:01.260]   What you can do is analyze a lot of things that humans already, like you can use tools
[00:23:01.260 --> 00:23:10.220]   that humans already use to pack it up and get something new there, but you can't actually,
[00:23:10.220 --> 00:23:13.140]   at least for me, can't build totally out of the box.
[00:23:13.140 --> 00:23:20.300]   Like it can be something that satisfies the closest possible thing you want to get to,
[00:23:20.300 --> 00:23:27.100]   but not nothing like, so I'm not sure like how to explain what, where I'm going at, but
[00:23:27.100 --> 00:23:32.220]   like in manufacturing, you see like those shapes that get really good results and everything,
[00:23:32.220 --> 00:23:36.540]   but they're just trying to satisfy a goal that was given to them, but they can like
[00:23:36.540 --> 00:23:41.500]   really think out of the box and be like, maybe we don't actually need this thing at all.
[00:23:41.500 --> 00:23:43.860]   Maybe we can build something completely different.
[00:23:43.860 --> 00:23:46.260]   Like that, that is lacking for me.
[00:23:46.260 --> 00:23:47.260]   I see.
[00:23:47.260 --> 00:23:48.260]   Yeah.
[00:23:48.260 --> 00:23:49.260]   So this, yeah.
[00:23:49.260 --> 00:23:51.900]   Related to the sort of problem of, you know, out of distribution, generalization, sort
[00:23:51.900 --> 00:23:56.820]   of rule extraction, things that people are finding computer vision models approaching
[00:23:56.820 --> 00:24:05.460]   like the problem of self-driving have also run into like that kind of difficulty or issue.
[00:24:05.460 --> 00:24:06.460]   Pretty much.
[00:24:06.460 --> 00:24:10.740]   I haven't like been researching that much self-driving per se.
[00:24:10.740 --> 00:24:14.380]   Like, I mean, I think self-driving can work great if a lot of self-driving cars are out
[00:24:14.380 --> 00:24:18.540]   there and they communicate between themselves and everything is great.
[00:24:18.540 --> 00:24:23.700]   But I haven't researched enough to talk about like real world scenario where there's like
[00:24:23.700 --> 00:24:30.340]   a bunch of random things going on, but in natural language and like in manufacturing,
[00:24:30.340 --> 00:24:36.780]   like I said, it can like build the best goal, like build the best box for this, but it can't
[00:24:36.780 --> 00:24:39.780]   say you may, might not need this box at all.
[00:24:39.780 --> 00:24:42.580]   Just put this box away and do something else.
[00:24:42.580 --> 00:24:45.300]   That's something that I don't think is still available.
[00:24:45.300 --> 00:24:46.300]   Yeah.
[00:24:46.300 --> 00:24:47.300]   That makes sense.
[00:24:47.300 --> 00:24:52.020]   At least, you know, for the, in the early days of computer chess, I don't know if this
[00:24:52.020 --> 00:24:57.860]   remains true, but some of the, even after computers beat humans, the best solution was
[00:24:57.860 --> 00:25:01.060]   a sort of like human computer hybrid.
[00:25:01.060 --> 00:25:07.100]   Not in the Deus Ex, you know, or, you know, cyborg sense, but in the, like just two people,
[00:25:07.100 --> 00:25:10.420]   like a computer and a person working together.
[00:25:10.420 --> 00:25:13.820]   And so it seems like that's kind of the sort of thing that you're going with Jarvis in
[00:25:13.820 --> 00:25:21.980]   that there's a step of automation of information extraction and collection, but the whole process,
[00:25:21.980 --> 00:25:23.580]   the whole process is not being automated, right?
[00:25:23.580 --> 00:25:28.300]   The whole process of managing employees, of managing a project is not automated.
[00:25:28.300 --> 00:25:33.700]   It's still, it's an adjunct, it's a tool that aids somebody who is, you know, an individual
[00:25:33.700 --> 00:25:37.300]   human making those kinds of decisions and capable of that kind of reasoning.
[00:25:37.300 --> 00:25:38.580]   Yeah, exactly.
[00:25:38.580 --> 00:25:43.780]   It's about improving like performance of those people so they can focus on something more
[00:25:43.780 --> 00:25:49.460]   important and like, like some, some different example is just in medical field.
[00:25:49.460 --> 00:25:55.140]   There are some like medical images that machine learning models can just find some things
[00:25:55.140 --> 00:26:00.580]   that doctors can't, but then on the other way, there, there is something that only doctors
[00:26:00.580 --> 00:26:01.580]   can see.
[00:26:01.580 --> 00:26:06.180]   So it can, it can work like best if it's a hybrid thing.
[00:26:06.180 --> 00:26:07.180]   Yeah.
[00:26:07.180 --> 00:26:16.700]   Well, I think actually I have one more question for you, which is, so you're in, have you
[00:26:16.700 --> 00:26:22.060]   been using weights and biases as part of your natural language processing model building
[00:26:22.060 --> 00:26:23.660]   toolkit?
[00:26:23.660 --> 00:26:25.460]   And if so, like what have you found?
[00:26:25.460 --> 00:26:26.460]   What have you found useful?
[00:26:26.460 --> 00:26:27.460]   What have you been using?
[00:26:27.460 --> 00:26:28.460]   Yeah.
[00:26:28.460 --> 00:26:35.740]   So I have been using it and I started actually using it a while ago when we started using
[00:26:35.740 --> 00:26:41.500]   that one big model and it started like, and it started at first because it took a lot
[00:26:41.500 --> 00:26:46.900]   of time to train that big model and then you had to do like a bunch of different tuning
[00:26:46.900 --> 00:26:50.180]   parameters like either write code yourself.
[00:26:50.180 --> 00:26:55.700]   So back then I think it was like mostly just that thing that, that helped the most.
[00:26:55.700 --> 00:26:57.580]   So that's how I started using it.
[00:26:57.580 --> 00:27:00.940]   And I still find just sweeps.
[00:27:00.940 --> 00:27:05.220]   They're really, really best to think about it because it just goes through every single
[00:27:05.220 --> 00:27:10.580]   possibility and logs everything that's happening during that process.
[00:27:10.580 --> 00:27:16.100]   And especially like utilization of CPU, GPU and stuff like that was helpful for us because
[00:27:16.100 --> 00:27:24.460]   we can see if like during that case, would we need like some sort of a change in the
[00:27:24.460 --> 00:27:25.460]   future?
[00:27:25.460 --> 00:27:26.460]   Great.
[00:27:26.460 --> 00:27:27.460]   Yeah.
[00:27:27.460 --> 00:27:29.860]   I'm glad to hear that the tool was useful.
[00:27:29.860 --> 00:27:35.660]   I think the artifacts toolkit is maybe newer than when you were working on that particular
[00:27:35.660 --> 00:27:36.660]   part of the project.
[00:27:36.660 --> 00:27:40.940]   So that's definitely helpful for orchestrating that model model pipeline where are the end
[00:27:40.940 --> 00:27:44.860]   where you have things that are not just deep learning models that you want to, you know,
[00:27:44.860 --> 00:27:49.620]   combine together and communicate with one another.
[00:27:49.620 --> 00:27:55.180]   But yeah, well, thanks so much, Alexa for staying up late over there in Serbia to be
[00:27:55.180 --> 00:27:59.660]   able to come on and talk with folks and answer questions live.
[00:27:59.660 --> 00:28:02.340]   And it's a really cool and exciting tool.
[00:28:02.340 --> 00:28:08.500]   And I look forward to seeing, you know, how you guys grow and make great use of it.
[00:28:08.500 --> 00:28:10.420]   Yeah, thanks so much for having me.
[00:28:10.420 --> 00:28:11.420]   This was really fun.
[00:28:11.420 --> 00:28:14.980]   And it was also fun to hear from Richard, what he's doing.
[00:28:14.980 --> 00:28:21.100]   And like, I mean, just that sort of kind of competition, but it's also like you earn even
[00:28:21.100 --> 00:28:23.460]   if you don't like completely win.
[00:28:23.460 --> 00:28:25.660]   It's really interesting thing to hear.
[00:28:25.660 --> 00:28:27.900]   Yeah, well, thanks for coming, Alexa.
[00:28:27.900 --> 00:28:33.180]   And thanks to our audience on zoom and on YouTube for coming by.
[00:28:33.180 --> 00:28:38.780]   In I hope to see you all again in two weeks, when I'll be giving a talk and Sharaag Agarwal
[00:28:38.780 --> 00:28:43.780]   of Harvard will be here to talk about his variants of gradient paper.
[00:28:43.780 --> 00:28:46.940]   So I will see you all then.
[00:28:46.940 --> 00:28:48.420]   Take care.
[00:28:48.420 --> 00:29:10.620]   Bye.
[00:29:10.620 --> 00:29:13.200]   (upbeat music)
[00:29:13.200 --> 00:29:14.740]   ♪ You got it, you got it ♪

