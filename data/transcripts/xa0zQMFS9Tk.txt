
[00:00:00.000 --> 00:00:08.280]   I love the fact that when you develop really novel AI techniques, that you can apply it
[00:00:08.280 --> 00:00:10.960]   to so many different areas.
[00:00:10.960 --> 00:00:14.800]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:14.800 --> 00:00:16.460]   models work in the real world.
[00:00:16.460 --> 00:00:18.560]   I'm your host, Lukas Biewald.
[00:00:18.560 --> 00:00:23.280]   Richard Socher was the chief scientist at Salesforce, where he oversaw development initiatives
[00:00:23.280 --> 00:00:25.520]   like Einstein's Cloud AI.
[00:00:25.520 --> 00:00:29.860]   As a PhD student at Stanford University, Richard helped create the ImageNet dataset and also
[00:00:29.860 --> 00:00:32.000]   founded the startup Metamind.
[00:00:32.000 --> 00:00:36.640]   Recently, Richard left Salesforce to create his own startup, SuSy, which is on a mission
[00:00:36.640 --> 00:00:40.680]   to build better internet using the latest NLP and AI technology.
[00:00:40.680 --> 00:00:43.040]   I'm excited to talk to him about all these things today.
[00:00:43.040 --> 00:00:48.360]   I was curious how you got inspired by this AI Economist paper and project.
[00:00:48.360 --> 00:00:51.720]   I was trying to read it and I'm not an economist, so I had a whole bunch of basic questions
[00:00:51.720 --> 00:00:58.320]   that are probably pretty embarrassing, but when did you learn so much about economics?
[00:00:58.320 --> 00:00:59.320]   It's such an interesting idea.
[00:00:59.320 --> 00:01:02.800]   I guess maybe you should probably summarize the paper first for those who haven't read
[00:01:02.800 --> 00:01:06.280]   it and then talk about how you got interested in it.
[00:01:06.280 --> 00:01:07.280]   Happy to.
[00:01:07.280 --> 00:01:08.280]   Yeah.
[00:01:08.280 --> 00:01:09.280]   So AI Economist is essentially a framework.
[00:01:09.280 --> 00:01:11.440]   It's more than just a single model or something.
[00:01:11.440 --> 00:01:19.760]   It's a whole framework that tries to model an economy in its most simple forms for now,
[00:01:19.760 --> 00:01:24.560]   though it'll get more realistic, I think, in the next months and years to come.
[00:01:24.560 --> 00:01:29.640]   And then inside that economic simulation, you have a two-level reinforcement learning
[00:01:29.640 --> 00:01:35.800]   setup where you have an AI Economist that basically can set taxes and subsidies and
[00:01:35.800 --> 00:01:44.000]   other kinds of financial instruments in order to optimize an overall objective for the economy,
[00:01:44.000 --> 00:01:50.900]   namely, in our case, productivity times equality, where equality is measured as sort of a one
[00:01:50.900 --> 00:01:55.600]   minus Gini index, which is a measure of equality that's used worldwide.
[00:01:55.600 --> 00:02:03.160]   And productivity makes sense in terms of how much do all the single agents in the simulation
[00:02:03.160 --> 00:02:04.440]   make.
[00:02:04.440 --> 00:02:09.880]   And each single agent is also a reinforcement learning agent, but their goals are just to
[00:02:09.880 --> 00:02:15.720]   maximize their own objectives, which is to maximize their own income and wealth.
[00:02:15.720 --> 00:02:18.280]   And is that 100% realistic?
[00:02:18.280 --> 00:02:19.280]   Of course not.
[00:02:19.280 --> 00:02:23.440]   But also in the simulation, there's mostly just three different types of resources, wood
[00:02:23.440 --> 00:02:26.640]   and stone and space in some ways.
[00:02:26.640 --> 00:02:30.560]   And then agents walk around in this 2D world, grid world.
[00:02:30.560 --> 00:02:34.760]   They can build houses, they can block other agents by building these houses, and they
[00:02:34.760 --> 00:02:40.040]   can trade resources as well to try, you know, you need certain more wood to build the house,
[00:02:40.040 --> 00:02:42.480]   but you have plenty of stone, you can trade it and so on.
[00:02:42.480 --> 00:02:44.440]   So it's sort of some of the fundamentals.
[00:02:44.440 --> 00:02:49.600]   Also, you have utility curves, which is quite common in economic modeling that you wouldn't
[00:02:49.600 --> 00:02:50.600]   have in a game.
[00:02:50.600 --> 00:02:52.400]   What does a utility curve do?
[00:02:52.400 --> 00:02:58.000]   It tells you, for instance, that after a certain amount of work, you have diminishing returns.
[00:02:58.000 --> 00:03:01.920]   Like you could work seven days a week, but most people at some point want to actually
[00:03:01.920 --> 00:03:07.720]   take some time off and not spend all their time just to minimize another little bit of
[00:03:07.720 --> 00:03:08.720]   money.
[00:03:08.720 --> 00:03:13.300]   And so, you know, that and a couple of other things make it quite different to playing
[00:03:13.300 --> 00:03:14.300]   just a game.
[00:03:14.300 --> 00:03:17.920]   And we thought about this too, could we just use civilization or age of empires or some
[00:03:17.920 --> 00:03:22.600]   of these things, but we wanted to one, steer away from just zero sum war games where you
[00:03:22.600 --> 00:03:25.800]   training I just get really, really good at fighting each other.
[00:03:25.800 --> 00:03:31.000]   And instead try to think of a system to try to have an overall improvement for the world
[00:03:31.000 --> 00:03:35.880]   so that if that system actually gets deployed, it would have as is a positive impact versus,
[00:03:35.880 --> 00:03:40.240]   oh, we used it to develop interesting technology that eventually maybe will have this positive
[00:03:40.240 --> 00:03:41.240]   impact.
[00:03:41.240 --> 00:03:44.280]   So that's kind of what AI economist is.
[00:03:44.280 --> 00:03:50.000]   And what's interesting technically and hard about it is that with this two level reinforcement
[00:03:50.000 --> 00:03:55.240]   learning, you're essentially the economist keeps changing the goalposts for all the RL
[00:03:55.240 --> 00:03:56.240]   agents.
[00:03:56.240 --> 00:03:58.040]   And they say, oh, I found this great strategy.
[00:03:58.040 --> 00:04:02.640]   I'm going to sell this, trade this, like collect this resource and build houses in this way
[00:04:02.640 --> 00:04:04.560]   to block off some other person.
[00:04:04.560 --> 00:04:09.600]   And then all of a sudden the economist changes it because they realize like you have a monopoly
[00:04:09.600 --> 00:04:13.240]   and equality is suffering and maybe you're going to tax the person with the monopoly
[00:04:13.240 --> 00:04:17.240]   a little bit higher who blocked all the resources away from the other agents.
[00:04:17.240 --> 00:04:20.480]   And now all of a sudden the agents have to adapt too.
[00:04:20.480 --> 00:04:26.320]   And in almost all RL before you have a fixed objective function, you know, like this is
[00:04:26.320 --> 00:04:29.440]   what you need to fit, like this is how you win go, this is how you win chess, this is
[00:04:29.440 --> 00:04:31.440]   how you win Atari games and so on.
[00:04:31.440 --> 00:04:32.880]   They don't change.
[00:04:32.880 --> 00:04:34.320]   And here the goalpost keeps changing.
[00:04:34.320 --> 00:04:37.840]   So it is a really hard, interesting optimization problem.
[00:04:37.840 --> 00:04:39.640]   So that's kind of what AI economists is.
[00:04:39.640 --> 00:04:41.200]   Now, how did we get to that idea?
[00:04:41.200 --> 00:04:45.640]   It actually came from a couple of different strains.
[00:04:45.640 --> 00:04:50.780]   The first time I had this idea was during my PhD, where I had this idea of essentially
[00:04:50.780 --> 00:04:56.640]   all these different cultures in the world have had their different energy landscapes
[00:04:56.640 --> 00:04:58.040]   on their optimization strategy.
[00:04:58.040 --> 00:05:02.880]   And a lot of them are trying to optimize roughly similar things you would hope, you know, people
[00:05:02.880 --> 00:05:10.000]   want to prosper, people want to have certain amenities and freedoms and so on.
[00:05:10.000 --> 00:05:13.040]   But they all end up in their different local optima.
[00:05:13.040 --> 00:05:16.480]   And I thought about this as like this non-convex objective function that different cultures
[00:05:16.480 --> 00:05:21.080]   go in and try to optimize it and end up in all these different local optima.
[00:05:21.080 --> 00:05:25.080]   And so that was kind of idea, but I didn't quite know how to structure it as like an
[00:05:25.080 --> 00:05:26.080]   AI problem.
[00:05:26.080 --> 00:05:29.840]   I just had some sort of quick little notes and I drew some objective function and I kind
[00:05:29.840 --> 00:05:32.920]   of discarded it and I continued to do NLP research.
[00:05:32.920 --> 00:05:37.440]   And then we hired Stefan Chang, the first author of the AI Economist.
[00:05:37.440 --> 00:05:42.160]   And we've also had Alex Trott in the team already for a year.
[00:05:42.160 --> 00:05:48.000]   And with him, we were working on trying to build houses just from like lots of bricks.
[00:05:48.000 --> 00:05:52.960]   You collect resources and you have a 3D agent, like something like Minecraft that tries to
[00:05:52.960 --> 00:05:54.280]   build a house.
[00:05:54.280 --> 00:05:56.600]   And that house building turned out to be pretty complicated.
[00:05:56.600 --> 00:06:01.200]   But the goal of that house building project was to eventually have multiple agents and
[00:06:01.200 --> 00:06:04.220]   a whole island and like the whole thing.
[00:06:04.220 --> 00:06:08.160]   And then we realized, man, we could spend another two years or so or three years just
[00:06:08.160 --> 00:06:11.960]   trying to build the houses properly before we could get to that AI Economist.
[00:06:11.960 --> 00:06:16.360]   I thought, "Stefan, like, hey, why don't we just go directly, assume the house building
[00:06:16.360 --> 00:06:20.080]   is just one action, build the house in this location and that's it, rather than all the
[00:06:20.080 --> 00:06:25.280]   different 3D structures and so on and finding out what is a good structure for a house."
[00:06:25.280 --> 00:06:28.760]   Just one click and then eventually maybe we can merge the two projects.
[00:06:28.760 --> 00:06:34.940]   But then, yeah, Stefan actually did a phenomenal job deepening our understanding of the literature
[00:06:34.940 --> 00:06:37.400]   and economics and reached out to other economists.
[00:06:37.400 --> 00:06:44.080]   That's how we worked with David Parks in the end from Harvard and really fleshed out how
[00:06:44.080 --> 00:06:49.440]   to make it work in an RL framework and then getting all the technology, like the complex
[00:06:49.440 --> 00:06:50.880]   optimization going and so on.
[00:06:50.880 --> 00:06:53.060]   So I have to give a lot of credit to Stefan.
[00:06:53.060 --> 00:06:57.560]   And Alex eventually said, "You know, screw this simple house building.
[00:06:57.560 --> 00:06:59.560]   This is why I'm kind of wanting to do this anyway."
[00:06:59.560 --> 00:07:02.800]   And he's been interested in economics for a long time too.
[00:07:02.800 --> 00:07:08.040]   So the two of them kind of just jumped in on that project and it became really, really
[00:07:08.040 --> 00:07:09.040]   great.
[00:07:09.040 --> 00:07:10.040]   That's so cool.
[00:07:10.040 --> 00:07:11.040]   Was there a debate over the...
[00:07:11.040 --> 00:07:14.040]   I mean, I thought there's actually a lot of kind of...
[00:07:14.040 --> 00:07:16.000]   Could be a lot of debate in the objective function, right?
[00:07:16.000 --> 00:07:21.920]   You did the economic growth times one minus the Gini coefficient.
[00:07:21.920 --> 00:07:25.960]   I mean, why not like economic growth plus the one minus the Gini?
[00:07:25.960 --> 00:07:29.000]   I could think of many other ways to do it.
[00:07:29.000 --> 00:07:30.000]   And so could we.
[00:07:30.000 --> 00:07:32.840]   And I think eventually you're going to...
[00:07:32.840 --> 00:07:38.360]   I love this project in that it literally covers the whole spectrum from a hardcore optimization
[00:07:38.360 --> 00:07:43.960]   problem that's really technical and sort of min-max and shifting objective function and
[00:07:43.960 --> 00:07:49.620]   energy landscapes and so on, all the way to the most philosophical civilization level
[00:07:49.620 --> 00:07:56.200]   debates and questions of what we need to do in the world and what is economics and politics
[00:07:56.200 --> 00:08:00.380]   and what are we all optimizing and should it be equality?
[00:08:00.380 --> 00:08:05.440]   In some ways you don't want the Gini index to go all the way to its maximum either because
[00:08:05.440 --> 00:08:10.360]   that means sort of there's absolutely everybody's forced to be a hundred percent equal, which
[00:08:10.360 --> 00:08:11.360]   is questionable.
[00:08:11.360 --> 00:08:12.360]   That's the danger these days.
[00:08:12.360 --> 00:08:13.360]   Right.
[00:08:13.360 --> 00:08:17.440]   Which is questionable in terms of monetary things, of course, we should all be equal
[00:08:17.440 --> 00:08:21.800]   in terms of rights and opportunities and so on.
[00:08:21.800 --> 00:08:25.080]   But in terms of financial equality, it's very important.
[00:08:25.080 --> 00:08:27.280]   Thanks for pointing it out.
[00:08:27.280 --> 00:08:31.360]   And in fact, I think actually this kind of work will help with that kind of equality
[00:08:31.360 --> 00:08:35.460]   because we can push for that and improve it a lot.
[00:08:35.460 --> 00:08:37.440]   Does it mean we have to get to the maximum?
[00:08:37.440 --> 00:08:38.440]   Right.
[00:08:38.440 --> 00:08:43.640]   Like maximum would be like infinite productivity and nobody has any difference in the world.
[00:08:43.640 --> 00:08:46.760]   And I think we should also celebrate some types of differences.
[00:08:46.760 --> 00:08:52.640]   But I think economic inequality is, in my eyes, the single biggest issue that we have
[00:08:52.640 --> 00:08:53.800]   in the world right now.
[00:08:53.800 --> 00:08:55.760]   A lot of issues fall from that.
[00:08:55.760 --> 00:09:00.640]   If certain minorities would have more economic equality, they would be better off.
[00:09:00.640 --> 00:09:03.440]   I think we'd have less racism, less sexism.
[00:09:03.440 --> 00:09:12.200]   If people of color and women had the same financial equality as men do, statistically
[00:09:12.200 --> 00:09:13.200]   speaking.
[00:09:13.200 --> 00:09:15.360]   And so I think economic equality is a big part.
[00:09:15.360 --> 00:09:17.080]   A lot of wars get started from that.
[00:09:17.080 --> 00:09:24.800]   A lot of genocides and so many other issues happen from economic inequality.
[00:09:24.800 --> 00:09:26.400]   It's a really tough one.
[00:09:26.400 --> 00:09:30.080]   Now, should it only be productivity times equality?
[00:09:30.080 --> 00:09:31.080]   Maybe not.
[00:09:31.080 --> 00:09:33.840]   Maybe there should be other things like sustainability in there.
[00:09:33.840 --> 00:09:37.240]   So in the simulation, you have trees.
[00:09:37.240 --> 00:09:39.360]   The trees will eventually regrow.
[00:09:39.360 --> 00:09:45.160]   But you can have sort of a tragedy of the common situation where all the agents just
[00:09:45.160 --> 00:09:48.120]   get rid of all the trees and then there are no more trees.
[00:09:48.120 --> 00:09:50.040]   And they all optimize their thing.
[00:09:50.040 --> 00:09:55.520]   Everybody's equal, but then long term, everybody will suffer because there will be stasis and
[00:09:55.520 --> 00:09:58.000]   people won't build anything anymore.
[00:09:58.000 --> 00:10:00.440]   And it'll flatline because they destroyed all their resources.
[00:10:00.440 --> 00:10:03.840]   So I think sustainability is a reasonable one.
[00:10:03.840 --> 00:10:05.720]   And then there are interesting questions.
[00:10:05.720 --> 00:10:14.200]   Clearly utilitarianism isn't, I think at least philosophically, the only answer to this.
[00:10:14.200 --> 00:10:20.320]   So you may need to have other protections in the objective functions and sort of boundary
[00:10:20.320 --> 00:10:21.320]   conditions.
[00:10:21.320 --> 00:10:27.440]   We could talk about just that for hours probably and over drinks in an evening, we could really
[00:10:27.440 --> 00:10:34.680]   spend a lot of fun sort of philosophy and ethics of what we should optimize.
[00:10:34.680 --> 00:10:41.800]   I think what I'm hoping to realistically though, is that when a politician in the future would
[00:10:41.800 --> 00:10:45.360]   say, "I want to help the middle class.
[00:10:45.360 --> 00:10:47.480]   That's one of the things I want to do."
[00:10:47.480 --> 00:10:51.920]   And then eventually either right away during their campaign or later on, they propose to
[00:10:51.920 --> 00:10:54.600]   say, "Now this is what I'm going to do to do this."
[00:10:54.600 --> 00:10:59.480]   And then you run that setup through the simulation and you say, "That is really different from
[00:10:59.480 --> 00:11:05.400]   any of the potential solutions that the simulation would get for helping the middle class.
[00:11:05.400 --> 00:11:08.640]   Why does yours differ so far?
[00:11:08.640 --> 00:11:12.080]   What's your thinking about that will actually help more than these other ways?"
[00:11:12.080 --> 00:11:17.640]   And so hopefully we can agree more easily on the objective function and then we can
[00:11:17.640 --> 00:11:21.560]   disagree less on how to exercise and how to get there.
[00:11:21.560 --> 00:11:25.840]   In your simulation, was there any emergent behavior that surprised you?
[00:11:25.840 --> 00:11:30.920]   Is there anything kind of counterintuitive that you discovered from doing these experiments?
[00:11:30.920 --> 00:11:34.360]   Yeah, there's definitely some things that at first you're like, "Wait, this doesn't
[00:11:34.360 --> 00:11:35.360]   make sense."
[00:11:35.360 --> 00:11:40.600]   We have taxes and we have subsidies and when you look at it, it's actually the lowest income
[00:11:40.600 --> 00:11:47.240]   bracket got taxed a lot and then it actually went down for the middle class of the simulation
[00:11:47.240 --> 00:11:50.280]   and we're like, "Wait a minute, that seems very counterintuitive."
[00:11:50.280 --> 00:11:52.840]   But it turned out they were also getting more subsidies.
[00:11:52.840 --> 00:11:59.640]   So they actually, net net, they were actually much more positive because the subsidies were
[00:11:59.640 --> 00:12:02.920]   also given to that income bracket.
[00:12:02.920 --> 00:12:06.400]   But that at first was kind of counterintuitive, but then once you double clicked on it and
[00:12:06.400 --> 00:12:11.560]   you realize, well, effectively they're actually getting more subsidies than they have to pay
[00:12:11.560 --> 00:12:12.560]   taxes.
[00:12:12.560 --> 00:12:15.560]   So it kind of leveled out and made more sense.
[00:12:15.560 --> 00:12:16.560]   Interesting.
[00:12:16.560 --> 00:12:19.000]   And sorry, I guess this may be a personal question, but you grew up in Eastern Germany,
[00:12:19.000 --> 00:12:20.000]   didn't you?
[00:12:20.000 --> 00:12:21.000]   I did for a couple of years.
[00:12:21.000 --> 00:12:28.240]   Ethiopia for three years and four years or so of East Germany and then Germany reunited.
[00:12:28.240 --> 00:12:29.240]   I see.
[00:12:29.240 --> 00:12:33.120]   Do you feel like that gives you a different perspective maybe than Americans on these
[00:12:33.120 --> 00:12:34.120]   topics?
[00:12:34.120 --> 00:12:40.160]   I mean, I think I was still pretty young when the Berlin Wall came down.
[00:12:40.160 --> 00:12:45.320]   And so I think though culturally, of course, East Germany still had, it wasn't like Germany
[00:12:45.320 --> 00:12:48.560]   reunited and then there's no more differences between East and West.
[00:12:48.560 --> 00:12:53.800]   In fact, some of the issues you see in other countries, you would see between East and
[00:12:53.800 --> 00:12:58.960]   West, like the East still has lower income compared to the West.
[00:12:58.960 --> 00:13:05.240]   And a lot of women actually left East Germany to go to the West.
[00:13:05.240 --> 00:13:09.720]   So there are a couple of counties that have too many men and so on.
[00:13:09.720 --> 00:13:14.920]   So there are still a lot of differences between the East and West still to this day.
[00:13:14.920 --> 00:13:18.960]   But I think, you know, sort of growing up in Germany overall, which is where, you know,
[00:13:18.960 --> 00:13:25.280]   the time I got most of my education was in reunited Germany, probably didn't affect me
[00:13:25.280 --> 00:13:31.840]   like in general, as I grew up, it was a free health care and free education all the way
[00:13:31.840 --> 00:13:39.240]   down to or up to a PhD level, master's level, which is not ever a politicized question.
[00:13:39.240 --> 00:13:42.640]   It was just a given.
[00:13:42.640 --> 00:13:49.600]   And being sort of anti big military intervention was something that was still pretty deeply
[00:13:49.600 --> 00:13:55.640]   ingrained in Germans having, you know, in there twice for a century.
[00:13:55.640 --> 00:14:00.220]   It was clear that that's something we should all try to add as best as we can to avoid.
[00:14:00.220 --> 00:14:07.720]   So there's a lot less sort of pro military conversations going on in Germany.
[00:14:07.720 --> 00:14:09.400]   And so, yeah, so it's kind of interesting.
[00:14:09.400 --> 00:14:11.840]   The whole political discourse in Germany is kind of shifted.
[00:14:11.840 --> 00:14:21.640]   Even the most sort of liberal pro economy, pro sort of companies, types of parties and
[00:14:21.640 --> 00:14:28.840]   on the political spectrum, none of them would ever question free health care or free education
[00:14:28.840 --> 00:14:32.240]   because statistically speaking, it just helps everyone.
[00:14:32.240 --> 00:14:36.500]   And it's kind of an interesting definition of freedom, even, you know, is it more free
[00:14:36.500 --> 00:14:39.640]   that you always have health care no matter what job you have, or is it less free because
[00:14:39.640 --> 00:14:42.160]   you have to pay for it?
[00:14:42.160 --> 00:14:45.280]   It's interesting cultural differences.
[00:14:45.280 --> 00:14:47.760]   So that definitely had a little bit of an impact.
[00:14:47.760 --> 00:14:49.160]   Got it.
[00:14:49.160 --> 00:14:51.600]   Yeah, that makes sense.
[00:14:51.600 --> 00:14:55.080]   I mean, I guess I also want to make sure we cover some of the other papers that since
[00:14:55.080 --> 00:14:59.200]   I have you, I'm just like, I have so many questions.
[00:14:59.200 --> 00:15:03.680]   I was wondering if you could talk about, we've actually been, you know, my company has been
[00:15:03.680 --> 00:15:09.800]   working with a lot of people doing various aspects of kind of protein generation and
[00:15:09.800 --> 00:15:16.440]   folding and I really feel like there's something going on in ML right now with all the applications.
[00:15:16.440 --> 00:15:20.520]   And it's something I know very little about because it didn't feel like a topic when I
[00:15:20.520 --> 00:15:21.520]   was in school.
[00:15:21.520 --> 00:15:25.760]   I'd love to just, if you could just describe, I mean, it's such an intriguing idea that
[00:15:25.760 --> 00:15:29.160]   language modeling techniques could be used for protein generation.
[00:15:29.160 --> 00:15:32.360]   Maybe you could just tell us what you did and kind of what you think about the field
[00:15:32.360 --> 00:15:33.360]   in general.
[00:15:33.360 --> 00:15:34.360]   Sure.
[00:15:34.360 --> 00:15:35.360]   Yeah.
[00:15:35.360 --> 00:15:41.320]   So generally high level protein generation or the progen model that we published is a
[00:15:41.320 --> 00:15:42.320]   language model.
[00:15:42.320 --> 00:15:43.320]   What's the language model?
[00:15:43.320 --> 00:15:46.240]   It's basically just trying to predict the next word in a large unsupervised text.
[00:15:46.240 --> 00:15:51.000]   We take all of Wikipedia, all of, you know, as much of the internet text as you can.
[00:15:51.000 --> 00:15:54.640]   And some people innovate it by taking Reddit data, which is kind of more interesting, but
[00:15:54.640 --> 00:15:57.760]   then also has issues with bias and so on.
[00:15:57.760 --> 00:16:01.600]   And you have a very simple objective function for a large neural network architecture, which
[00:16:01.600 --> 00:16:03.520]   is just try to predict what the next word is.
[00:16:03.520 --> 00:16:07.960]   And people have been doing that for many, many years, almost many decades, because it's
[00:16:07.960 --> 00:16:13.600]   a very helpful way to disambiguate words that sound the same, but actually are written and
[00:16:13.600 --> 00:16:14.720]   mean something different.
[00:16:14.720 --> 00:16:19.600]   So if I want to say the price of wood versus what you tell me something, then the wood
[00:16:19.600 --> 00:16:23.760]   sounds the same in both sentences, but one it's the wood of trees and one it's auxiliary
[00:16:23.760 --> 00:16:24.760]   verb.
[00:16:24.760 --> 00:16:29.040]   And so you basically want to disambiguate like which one is more likely in that context.
[00:16:29.040 --> 00:16:34.440]   And so that was used for speech recognition and still is a lot of speech recognition models
[00:16:34.440 --> 00:16:35.440]   for translation.
[00:16:35.440 --> 00:16:39.440]   You know, you can try to generate a bunch of possible translations for, you know, a
[00:16:39.440 --> 00:16:48.280]   German to English translation, but then try to identify which sentences are the most fluent
[00:16:48.280 --> 00:16:51.300]   sort of for the English language.
[00:16:51.300 --> 00:16:59.940]   And so the sort of interesting novelty that came out recently with GPT and OpenAI is to
[00:16:59.940 --> 00:17:04.420]   actually take these existing models, make them even bigger, and then not just look at
[00:17:04.420 --> 00:17:06.780]   the perplexity numbers going lower and lower.
[00:17:06.780 --> 00:17:10.700]   Perplexity is essentially sort of an inverse metric of the probability that you assign
[00:17:10.700 --> 00:17:11.700]   to each word.
[00:17:11.700 --> 00:17:17.820]   So the less perplexed you are, the more correctly you've assigned probability mass to the word
[00:17:17.820 --> 00:17:20.000]   that actually comes next.
[00:17:20.000 --> 00:17:24.900]   And so as the perplexity reduced more and more, it crossed a threshold.
[00:17:24.900 --> 00:17:28.940]   And OpenAI was clever enough to realize the threshold is so low now we should really look
[00:17:28.940 --> 00:17:32.820]   at what they're generating and see what comes out.
[00:17:32.820 --> 00:17:36.860]   And it turned out that they're actually surprisingly good, better than most anybody in the field
[00:17:36.860 --> 00:17:43.860]   had thought five, 10 years ago in generating fluent paragraphs that actually made sense,
[00:17:43.860 --> 00:17:46.660]   that had some coherence and flow to them.
[00:17:46.660 --> 00:17:50.700]   And of course, after one or two paragraphs, they will repeat themselves and it won't make
[00:17:50.700 --> 00:17:54.100]   that much sense still because they don't have, this is I think actually an interesting question
[00:17:54.100 --> 00:17:57.520]   for the future, what's the next objective function?
[00:17:57.520 --> 00:18:03.800]   Just producing the next word and generating the next word doesn't include the fact that
[00:18:03.800 --> 00:18:09.260]   usually when you try to say stuff, you have a goal in mind to convince somebody of something,
[00:18:09.260 --> 00:18:13.400]   to learn something, to get a message across, to get somebody to do something, all these
[00:18:13.400 --> 00:18:17.880]   different goals that you have as you try to use language, that I think will be the next
[00:18:17.880 --> 00:18:23.600]   level of AI research to identify and understand new objective functions in general.
[00:18:23.600 --> 00:18:27.480]   And it'll actually allow AI to come up with its own objective function.
[00:18:27.480 --> 00:18:30.240]   But that's, anyway, so back to Progen.
[00:18:30.240 --> 00:18:32.280]   So we have, this is fun.
[00:18:32.280 --> 00:18:35.720]   This is like, usually I don't have that technical of an audience to geek out about these things.
[00:18:35.720 --> 00:18:39.400]   It's just like I have to stay a lot more high level for most other interviews.
[00:18:39.400 --> 00:18:44.800]   And so what's cool about Progen is we took this idea of predicting the next word, which
[00:18:44.800 --> 00:18:48.560]   for languages makes sense, humans can do it, but humans can't actually do it for proteins.
[00:18:48.560 --> 00:18:54.760]   We're not built to look at a bunch of different amino acids and protein sequences and then
[00:18:54.760 --> 00:18:58.360]   try to learn what they would look like, what would come next.
[00:18:58.360 --> 00:19:04.920]   And so I love the fact that when you develop really novel AI techniques, that you can apply
[00:19:04.920 --> 00:19:07.880]   to so many different areas.
[00:19:07.880 --> 00:19:13.560]   And I still think that one of the most exciting things is if you find a new model family and
[00:19:13.560 --> 00:19:17.120]   then you apply it to all these different things and you show and eventually have a multitask
[00:19:17.120 --> 00:19:19.280]   model that can do multiple different things.
[00:19:19.280 --> 00:19:28.320]   And so here it made sense to us because again, it's a language that has a meaning.
[00:19:28.320 --> 00:19:31.720]   We just have much harder way of accessing that meaning.
[00:19:31.720 --> 00:19:33.680]   And we have a ton of training data.
[00:19:33.680 --> 00:19:35.720]   Now that sequencing is getting cheaper and cheaper.
[00:19:35.720 --> 00:19:40.880]   There's also an interesting learning about the first time somebody got sequenced, it
[00:19:40.880 --> 00:19:44.600]   was incredibly expensive and that was a white man.
[00:19:44.600 --> 00:19:49.320]   And now for a hundred bucks, anybody can sequence their technology.
[00:19:49.320 --> 00:19:53.080]   So that's actually a great story for technology.
[00:19:53.080 --> 00:20:03.480]   So long story short, we predict each new protein one at a time and then we can generate new
[00:20:03.480 --> 00:20:04.480]   proteins.
[00:20:04.480 --> 00:20:05.480]   And so what does that mean?
[00:20:05.480 --> 00:20:08.080]   And I think that'd be useful for people not familiar with proteins.
[00:20:08.080 --> 00:20:11.880]   Everything in life is governed by proteins.
[00:20:11.880 --> 00:20:14.480]   Every function of your body is governed by proteins.
[00:20:14.480 --> 00:20:19.540]   Deep down and the level below cells and everything, it's all guided by proteins.
[00:20:19.540 --> 00:20:24.600]   And so digestive system, even you could develop proteins that will fight certain types of
[00:20:24.600 --> 00:20:27.240]   cancer, certain types of viruses.
[00:20:27.240 --> 00:20:32.080]   It's actually something we're also working on now to try to do some interesting things
[00:20:32.080 --> 00:20:36.240]   for curing certain kinds of viruses, but it's too early to talk about it right now.
[00:20:36.240 --> 00:20:37.240]   It will take some time.
[00:20:37.240 --> 00:20:39.480]   It's kind of another moonshot.
[00:20:39.480 --> 00:20:42.880]   But there's a really exciting work that you could do.
[00:20:42.880 --> 00:20:47.880]   You can even develop proteins that will eat plastic to try to help with pollution.
[00:20:47.880 --> 00:20:51.720]   This is unlimited, the kinds of stuff you could do with proteins if you understood that
[00:20:51.720 --> 00:20:52.840]   language well.
[00:20:52.840 --> 00:20:58.640]   And so one big important factor for this protein model was that it's also a controllable language
[00:20:58.640 --> 00:21:00.920]   model, that it has these control codes in the beginning.
[00:21:00.920 --> 00:21:04.280]   Because you don't want it to just sort of randomly generate random proteins.
[00:21:04.280 --> 00:21:08.200]   You have an actual goal in mind, like, "Oh, this should bind to this binding site in a
[00:21:08.200 --> 00:21:14.200]   cell," or, "This should try to be able to connect to a plastic," or all these different
[00:21:14.200 --> 00:21:17.320]   kinds of things you could do.
[00:21:17.320 --> 00:21:19.520]   And so we have these control codes.
[00:21:19.520 --> 00:21:23.840]   They basically give you the function and which area of the body and things like that it's
[00:21:23.840 --> 00:21:26.280]   in or what other binding sites it should have.
[00:21:26.280 --> 00:21:29.280]   And then it will actually generate reasonable proteins.
[00:21:29.280 --> 00:21:34.680]   And Ali on our team has just been doing a phenomenal job pushing that line of research.
[00:21:34.680 --> 00:21:36.680]   And he's also the first author of ProGen.
[00:21:36.680 --> 00:21:37.680]   Of course.
[00:21:37.680 --> 00:21:39.960]   I mean, how did you get the training data for that?
[00:21:39.960 --> 00:21:44.320]   I could see how you could get protein data from DNA, but how did you get the data of
[00:21:44.320 --> 00:21:46.400]   what these proteins do and things like that?
[00:21:46.400 --> 00:21:54.120]   So we took a subset of data that had some kind of meta data associated with them.
[00:21:54.120 --> 00:22:02.080]   And what's interesting is there, you actually can look at a lot more training data once
[00:22:02.080 --> 00:22:04.720]   you just say, "Look, any control code goes."
[00:22:04.720 --> 00:22:07.320]   It just goes in here and then we can also use that.
[00:22:07.320 --> 00:22:17.880]   The majority of datasets are still very unstructured slash there's no good sort of, what's the
[00:22:17.880 --> 00:22:22.600]   word I'm looking for, documentation and coherence between these different datasets.
[00:22:22.600 --> 00:22:24.920]   Each different dataset, the proteins have different lengths.
[00:22:24.920 --> 00:22:27.880]   And then some people say, "Oh, it has like these three functions."
[00:22:27.880 --> 00:22:30.560]   And other people say, "Well, I just got this from somewhere."
[00:22:30.560 --> 00:22:35.400]   The next level is actually to try to train it even if you have zero meta data associated
[00:22:35.400 --> 00:22:36.400]   with them.
[00:22:36.400 --> 00:22:42.640]   There are some interesting meta studies that just have a lot of unsupervised sequences
[00:22:42.640 --> 00:22:44.920]   from like soil and all kinds of things.
[00:22:44.920 --> 00:22:49.220]   And so if you could learn from unsupervised sequences, you could train on even more.
[00:22:49.220 --> 00:22:53.560]   But for now, we just took datasets that had at least some kind of meta data associated
[00:22:53.560 --> 00:23:00.700]   with it, even if there's no general nice sort of ImageNet like taxonomy or WordNet like
[00:23:00.700 --> 00:23:02.300]   taxonomy for them.
[00:23:02.300 --> 00:23:07.020]   But any kind of meta data was enough for us to incorporate the data into the model.
[00:23:07.020 --> 00:23:11.240]   And is this the same with like GPT where it's just like predicting the next one and you're
[00:23:11.240 --> 00:23:15.420]   just trying to have the lowest perplexity or the highest probability that-
[00:23:15.420 --> 00:23:16.420]   That's right.
[00:23:16.420 --> 00:23:19.580]   It's a super simple objective function still.
[00:23:19.580 --> 00:23:21.460]   And it's just trying to predict what's the next one.
[00:23:21.460 --> 00:23:26.580]   And what's amazing is actually that we, and we just released this on Twitter today and
[00:23:26.580 --> 00:23:31.660]   on the blog post, we analyzed it and some super fascinating stuff.
[00:23:31.660 --> 00:23:35.820]   So there's sort of a protein folding, which is computationally expensive, it's a really
[00:23:35.820 --> 00:23:36.820]   hard problem.
[00:23:36.820 --> 00:23:41.420]   But what we found is that even though the model goes through sequences, just one at
[00:23:41.420 --> 00:23:48.580]   a time, you can visualize the attention mechanism inside these transformer networks and the
[00:23:48.580 --> 00:23:55.740]   attention mechanisms actually have a very high correlation to folding patterns in 3D.
[00:23:55.740 --> 00:24:01.460]   So the areas of the protein, when they fold around and they actually are close to another
[00:24:01.460 --> 00:24:06.340]   area and they would often fold in a way that they're very close and then also different
[00:24:06.340 --> 00:24:11.340]   binding sites and so on, they're highly, highly correlated with transformer attention.
[00:24:11.340 --> 00:24:16.700]   So I think there's a lot more there to find out and explore.
[00:24:16.700 --> 00:24:23.660]   And were the same mechanisms like attention that make language models work well, was it
[00:24:23.660 --> 00:24:27.500]   the same things that really mattered for the protein prediction or was there any difference
[00:24:27.500 --> 00:24:30.700]   in the kinds of models that work?
[00:24:30.700 --> 00:24:35.220]   So to be honest, these models are so large and we don't want to burn through like a hundred
[00:24:35.220 --> 00:24:37.660]   million dollars to train 10 of them.
[00:24:37.660 --> 00:24:47.300]   We just trained like a tiny samples of a transformer and then we train very, very few, one or two
[00:24:47.300 --> 00:24:56.500]   of the 1.7 billion parameter with 230 billion or yeah, 17 billion or so protein sequences.
[00:24:56.500 --> 00:24:57.500]   I see.
[00:24:57.500 --> 00:24:58.500]   I see.
[00:24:58.500 --> 00:25:02.180]   So yeah, we don't have a huge inflation table where we spend a hundred million dollars on
[00:25:02.180 --> 00:25:05.900]   that one piece of paper that gives all the different numbers on a big table.
[00:25:05.900 --> 00:25:09.380]   These models are so large, you really better not have a bug in that in the beginning and
[00:25:09.380 --> 00:25:11.940]   then realize it later.
[00:25:11.940 --> 00:25:16.740]   But I guess, do these larger models work significantly better than like simpler models?
[00:25:16.740 --> 00:25:17.740]   For sure.
[00:25:17.740 --> 00:25:18.740]   Yeah.
[00:25:18.740 --> 00:25:19.740]   This is really where neural networks shine.
[00:25:19.740 --> 00:25:23.060]   They have so much more expressive power.
[00:25:23.060 --> 00:25:32.380]   They can capture so many more different non-convex, highly complex functions that you need that.
[00:25:32.380 --> 00:25:37.140]   I think this is sort of where you couldn't do this thing with a linear model.
[00:25:37.140 --> 00:25:38.700]   The world is not linear.
[00:25:38.700 --> 00:25:44.100]   It'd be a lot easier to solve all kinds of issues in medicine and so on if everything
[00:25:44.100 --> 00:25:47.980]   was like some nice convex problem in biology, but it's far from that.
[00:25:47.980 --> 00:25:52.020]   So we really need the complexity of these very large models.
[00:25:52.020 --> 00:25:58.300]   Do you have any way to, I feel like GPD 2, one of the coolest things about it was it
[00:25:58.300 --> 00:26:01.940]   produced these sentence where they're so evocative and you kind of have the sense of like, okay,
[00:26:01.940 --> 00:26:08.020]   this thing is not good at long range dependencies, but it's very fluent.
[00:26:08.020 --> 00:26:12.660]   Could a scientist look at the proteins you generate and have some sense of like, these
[00:26:12.660 --> 00:26:17.180]   seem fairly realistic or is there any way to measure that?
[00:26:17.180 --> 00:26:20.300]   It's super fascinating, right?
[00:26:20.300 --> 00:26:27.700]   What is the energy landscape in this discrete protein space that actually makes sense?
[00:26:27.700 --> 00:26:32.140]   When you look at, so biologists already do this, like two years ago, I think the Nobel
[00:26:32.140 --> 00:26:37.940]   Prize in medicine was given to a team that essentially randomly modified existing protein
[00:26:37.940 --> 00:26:40.300]   sequences and then just tested them out.
[00:26:40.300 --> 00:26:44.260]   You can synthesize them and see if they actually have certain properties.
[00:26:44.260 --> 00:26:48.740]   You can try to make them fluorescent or not, and then see how many proteins can I randomly
[00:26:48.740 --> 00:26:53.740]   change to still have that property or have even more of that property, which could be
[00:26:53.740 --> 00:26:57.220]   useful for drugs and drug development and so on.
[00:26:57.220 --> 00:27:03.060]   And so you don't want to steer usually too far away from it.
[00:27:03.060 --> 00:27:08.340]   And then there are a couple of different metrics you can look at of like, as you generated
[00:27:08.340 --> 00:27:15.380]   a new type of protein that doesn't yet exist in nature, how likely would that to be structurally
[00:27:15.380 --> 00:27:18.060]   sound at all?
[00:27:18.060 --> 00:27:23.200]   And we actually, in the paper, we have different experiments where we show that there's a certain
[00:27:23.200 --> 00:27:29.340]   energy that with programs that you can compute that says like this would actually have a
[00:27:29.340 --> 00:27:31.300]   very low or very high energy.
[00:27:31.300 --> 00:27:34.860]   And hence this protein would not just disintegrate and fall apart.
[00:27:34.860 --> 00:27:36.940]   It would actually be structurally sound.
[00:27:36.940 --> 00:27:42.300]   And it turns out that compared to the random baseline, which is relatively easy to beat,
[00:27:42.300 --> 00:27:48.060]   we're so much better and create much more stable proteins that are more likely to actually
[00:27:48.060 --> 00:27:49.060]   work.
[00:27:49.060 --> 00:27:50.060]   That's so cool.
[00:27:50.060 --> 00:27:51.060]   All right.
[00:27:51.060 --> 00:27:52.580]   I'm going to keep jumping around because I have so many questions.
[00:27:52.580 --> 00:27:58.300]   I want to get through, but I'd love to hear about the language model that you came out
[00:27:58.300 --> 00:28:03.340]   with last year, the CTRL and what inspired you to make a new language model and what
[00:28:03.340 --> 00:28:06.220]   it does differently than other options out there.
[00:28:06.220 --> 00:28:07.740]   Yeah, that's a great question.
[00:28:07.740 --> 00:28:13.140]   So control is essentially a controllable language model where instead of just saying, here's
[00:28:13.140 --> 00:28:19.480]   a beginning sentence, now just spitball, randomly generate how that could continue.
[00:28:19.480 --> 00:28:23.640]   Essentially it would make more sense for us to try to create language technology that
[00:28:23.640 --> 00:28:25.320]   we have a little bit more control over.
[00:28:25.320 --> 00:28:31.840]   So we created these control codes that essentially say for this sequence, but also given this
[00:28:31.840 --> 00:28:34.080]   genre, like continue the sentence.
[00:28:34.080 --> 00:28:38.200]   So if you start with a knife and you say the genre is a horror movie, then like the knife
[00:28:38.200 --> 00:28:40.880]   peaked through the door and blah, blah, blah, crazy stuff is happening.
[00:28:40.880 --> 00:28:45.800]   Or if you say a knife and a review story, then it's like, oh, the knife cuts really
[00:28:45.800 --> 00:28:46.800]   well.
[00:28:46.800 --> 00:28:49.440]   My vegetables, my husband loves using it in the kitchen and blah, blah, blah.
[00:28:49.440 --> 00:28:51.880]   So that's kind of a difference.
[00:28:51.880 --> 00:28:54.160]   You have more control over what it would actually generate.
[00:28:54.160 --> 00:28:59.120]   And then the control codes can also be used as task codes.
[00:28:59.120 --> 00:29:04.920]   And you can say the task code or control code is generate this translation of this, and
[00:29:04.920 --> 00:29:10.620]   then it sort of generates the translated sentence after instead of just sort of the next random
[00:29:10.620 --> 00:29:13.540]   possible sentences that might make some sense.
[00:29:13.540 --> 00:29:19.420]   And so at that point, and this has been something I've been trying to work on for a long time
[00:29:19.420 --> 00:29:24.080]   with DECA, the natural language processing, DECA, DECA NLP, and a lot of other projects
[00:29:24.080 --> 00:29:31.600]   is I think we're at that state now in NLP where we can try to just solve a lot of these
[00:29:31.600 --> 00:29:39.160]   standard NLP problems by having a single large multitask model that you have the substrate,
[00:29:39.160 --> 00:29:41.640]   a large complex neural network structure.
[00:29:41.640 --> 00:29:44.040]   It almost doesn't matter anymore these days what it is.
[00:29:44.040 --> 00:29:49.200]   You could probably have a very deep, large stack LSTM, now it would be a transformer,
[00:29:49.200 --> 00:29:52.920]   we'll probably come up with other versions of that, but it's some kind of large general
[00:29:52.920 --> 00:29:55.880]   function approximator, some neural substrate.
[00:29:55.880 --> 00:30:00.480]   And then the novelty is you try to train it to have all these different objective functions,
[00:30:00.480 --> 00:30:02.680]   different tasks, it gets better over time.
[00:30:02.680 --> 00:30:09.320]   And then you can get to transfer learning tasks, you can get to zero shot abilities,
[00:30:09.320 --> 00:30:10.320]   and so on.
[00:30:10.320 --> 00:30:16.680]   So that's been a dream of our first line of that work with Brian McCann on contextual
[00:30:16.680 --> 00:30:21.240]   vectors, COVE, which we trained back then still with translation.
[00:30:21.240 --> 00:30:24.880]   Then Elmo took that idea and replaced translation with language modeling, which is even more
[00:30:24.880 --> 00:30:29.800]   clever because you have even more data that's unsupervised than you have with translation,
[00:30:29.800 --> 00:30:33.540]   even though that's sort of the biggest supervised data set like ImageNet.
[00:30:33.540 --> 00:30:39.240]   And then Elmo of course became BERT with even more novelties on top of it, but still sticking
[00:30:39.240 --> 00:30:42.080]   to language models and taking these contextual vectors.
[00:30:42.080 --> 00:30:47.600]   And so when you have contextual vectors that can get easily fine tuned on multiple tasks,
[00:30:47.600 --> 00:30:53.480]   then you have something like DecaNLP, where you have everything as described as one task,
[00:30:53.480 --> 00:30:57.320]   then you get closer and closer to that step to eventually just having a single model for
[00:30:57.320 --> 00:30:58.520]   all of NLP.
[00:30:58.520 --> 00:31:05.220]   And then my hope is that eventually the NLP community can work in a agglomerative, a kind
[00:31:05.220 --> 00:31:10.460]   of cumulative way where we have a control like language model or question answering
[00:31:10.460 --> 00:31:14.280]   model that you can ask any kind of question and so on.
[00:31:14.280 --> 00:31:19.600]   Or you can even have just a general language model, but you ask it questions and then outcomes,
[00:31:19.600 --> 00:31:23.280]   the next words that come after the question should be the answer if it really learned
[00:31:23.280 --> 00:31:25.900]   something about language and the world and everything.
[00:31:25.900 --> 00:31:28.800]   So there are these equivalent super tasks of NLP.
[00:31:28.800 --> 00:31:35.560]   But long story short is if we're able to do that and every research that we do actually
[00:31:35.560 --> 00:31:41.920]   makes an assisting super model better and better, then we would all of a sudden have
[00:31:41.920 --> 00:31:46.400]   an explosion, I think, in progress in natural language processing.
[00:31:46.400 --> 00:31:51.160]   And we would stop saying, "Oh yes, this paper has a baseline and we're making it a little
[00:31:51.160 --> 00:31:52.160]   bit better."
[00:31:52.160 --> 00:31:54.480]   And then the next paper, we jump back to the baseline, we make it a little bit better in
[00:31:54.480 --> 00:31:55.480]   a different direction.
[00:31:55.480 --> 00:32:00.200]   The next time we again, we keep sort of, we improve our baselines from time to time, but
[00:32:00.200 --> 00:32:05.540]   all these papers do sort of one offs improvements of these baselines versus every time somebody
[00:32:05.540 --> 00:32:10.320]   publishes a good paper, the model overall gets better and then everybody will start
[00:32:10.320 --> 00:32:13.300]   directly from that improved model.
[00:32:13.300 --> 00:32:18.200]   So that's been my dream for the NLP field for a while.
[00:32:18.200 --> 00:32:21.160]   It does kind of seem like NLP is moving in that direction, doesn't it?
[00:32:21.160 --> 00:32:25.800]   With the big multitask baselines.
[00:32:25.800 --> 00:32:26.800]   That's right.
[00:32:26.800 --> 00:32:29.120]   And T5 and all these other large models.
[00:32:29.120 --> 00:32:30.440]   I'm super excited to see it.
[00:32:30.440 --> 00:32:31.800]   I think it's finally happening.
[00:32:31.800 --> 00:32:40.880]   It'll still take some time because just like about 10 years ago, I had my first deep learning
[00:32:40.880 --> 00:32:46.280]   neural network paper at an NLP conference and the reviewers still wanted to reject it.
[00:32:46.280 --> 00:32:49.240]   There are a lot of people like, "Why are you applying neural networks to NLP?
[00:32:49.240 --> 00:32:51.800]   That stuff from the nineties, it doesn't work here."
[00:32:51.800 --> 00:32:55.120]   And I had in the beginning of my PhD, a lot of papers rejected.
[00:32:55.120 --> 00:32:59.960]   And I think part of it is that a lot of people have built their careers and their knowledge
[00:32:59.960 --> 00:33:03.460]   and their academic standing and so on, on feature engineering.
[00:33:03.460 --> 00:33:05.960]   And so when you say, "Oh, you don't need to do feature engineering anymore.
[00:33:05.960 --> 00:33:08.400]   You just now have these models and they learn the features."
[00:33:08.400 --> 00:33:12.040]   It doesn't sound that great if you've done feature engineering for 10 years.
[00:33:12.040 --> 00:33:18.720]   And now we have the last 10 years or so of people doing architecture engineering and
[00:33:18.720 --> 00:33:21.880]   they don't want to hear that the architecture doesn't quite matter anymore.
[00:33:21.880 --> 00:33:24.040]   It's now about the objective functions.
[00:33:24.040 --> 00:33:27.200]   And so let's ignore all these architecture engineering papers.
[00:33:27.200 --> 00:33:34.360]   Let's just assume there's one very large, efficiently trainable neural network architecture,
[00:33:34.360 --> 00:33:41.920]   probably a transformer because it's paralyzable on GPUs nowadays, but it could be LSTMs, whatever.
[00:33:41.920 --> 00:33:46.160]   And we train this really large one and now we become clever about the objective functions
[00:33:46.160 --> 00:33:48.360]   on improving that neural substrate.
[00:33:48.360 --> 00:33:54.040]   Again, it will be a shift and it usually takes the community a couple of years to make these
[00:33:54.040 --> 00:33:56.600]   shifts and young people are jumping on it.
[00:33:56.600 --> 00:34:00.520]   And then people that are older and longer, have been longer in the field will eventually
[00:34:00.520 --> 00:34:04.640]   kind of through their grad students and so on, adjust and then embrace it and then start
[00:34:04.640 --> 00:34:06.720]   doing amazing work in that new area.
[00:34:06.720 --> 00:34:07.920]   So how does control fit into that?
[00:34:07.920 --> 00:34:12.340]   So is that like, it sounds to me like, was that a new architecture or was that really
[00:34:12.340 --> 00:34:13.880]   just adding control codes?
[00:34:13.880 --> 00:34:17.640]   It was mostly adding control codes to a large language model.
[00:34:17.640 --> 00:34:20.280]   That was kind of the main idea.
[00:34:20.280 --> 00:34:23.520]   And it fits into this as a way to unify.
[00:34:23.520 --> 00:34:28.720]   Basically the way I see it is there are three equivalent super tasks of NLP, dialogue systems,
[00:34:28.720 --> 00:34:30.640]   language models, and question answering systems.
[00:34:30.640 --> 00:34:36.040]   You can cast every other NLP problem into any of those three and you can map those three
[00:34:36.040 --> 00:34:37.680]   between one another.
[00:34:37.680 --> 00:34:42.120]   Like in a dialogue, something happens and then you have to generate the answer to what
[00:34:42.120 --> 00:34:44.600]   the previous agent just said.
[00:34:44.600 --> 00:34:48.840]   And in language modeling, you can also cast it as question answering, like asking a question
[00:34:48.840 --> 00:34:53.800]   and then the words that should be predicted after that question should be the answer.
[00:34:53.800 --> 00:34:56.040]   So question answering and language modeling are equivalent.
[00:34:56.040 --> 00:35:02.680]   So we tried this with DECA NLP where we had language modeling, sorry, where we use question
[00:35:02.680 --> 00:35:08.240]   answering as that default framework and with control, it's the acknowledgement that if
[00:35:08.240 --> 00:35:12.820]   you start with a large substrate that can be trained unsupervised from a large amount
[00:35:12.820 --> 00:35:18.360]   of texts, it's like sort of the best single task to then transfer from and do multitask
[00:35:18.360 --> 00:35:19.360]   learning from.
[00:35:19.360 --> 00:35:23.440]   Do you treat the control codes differently than other tokens?
[00:35:23.440 --> 00:35:30.320]   Because I feel like I see a lot of examples where people do a translation by just showing
[00:35:30.320 --> 00:35:35.120]   pairs and then it's like, oh, their language model just starts generating pairs.
[00:35:35.120 --> 00:35:40.280]   Is that the same thing or is it somehow control doing things more systematically?
[00:35:40.280 --> 00:35:45.560]   So in some cases you can actually make those control codes be language themselves.
[00:35:45.560 --> 00:35:49.720]   So you could say like, here's a question, now as a text, generate the answer after you
[00:35:49.720 --> 00:35:51.340]   read that whole thing.
[00:35:51.340 --> 00:35:56.400]   But you can also have control codes that are just like task one, task underscore one is
[00:35:56.400 --> 00:35:57.560]   a control token.
[00:35:57.560 --> 00:36:02.440]   And then what's surprising is with these control tokens, the outputs will be very different,
[00:36:02.440 --> 00:36:03.440]   right?
[00:36:03.440 --> 00:36:07.080]   Like translation all of a sudden, generating a very different output with the same neural
[00:36:07.080 --> 00:36:09.000]   network architecture overall.
[00:36:09.000 --> 00:36:11.520]   It's pretty amazing that that works.
[00:36:11.520 --> 00:36:12.600]   It's amazing that that works.
[00:36:12.600 --> 00:36:17.600]   I mean, I can't believe that that works.
[00:36:17.600 --> 00:36:22.040]   I remember when I was studying this stuff, it was like linguists that wanted to do it
[00:36:22.040 --> 00:36:25.680]   completely explicitly and rule-based versus people doing machine learning.
[00:36:25.680 --> 00:36:29.120]   So I guess we sort of keep going up levels of abstractions.
[00:36:29.120 --> 00:36:31.120]   You know, it's interesting.
[00:36:31.120 --> 00:36:34.040]   Sometimes these rules, I used to just so much discard it.
[00:36:34.040 --> 00:36:40.160]   But when you try to build a real system for a real company, you have a chatbot and that
[00:36:40.160 --> 00:36:46.360]   company has in the end, like everything, if you have the ability to make it into chatbot,
[00:36:46.360 --> 00:36:48.440]   you have some API somewhere, right?
[00:36:48.440 --> 00:36:52.840]   Whether that API is like you click on these fields or you already have it as a program,
[00:36:52.840 --> 00:36:59.040]   it needs to be a structured disambiguated programming language output at some point
[00:36:59.040 --> 00:37:00.560]   that fulfills actions.
[00:37:00.560 --> 00:37:02.000]   Like what order do you want?
[00:37:02.000 --> 00:37:06.880]   We go into our order management system and operate this field and resend a new one that
[00:37:06.880 --> 00:37:08.880]   goes again to some logistics center.
[00:37:08.880 --> 00:37:15.760]   And so when you have these concrete chatbots for a company, I was always thinking, oh,
[00:37:15.760 --> 00:37:16.760]   it should just all be learned.
[00:37:16.760 --> 00:37:19.240]   And then at the very end, it generates some code and so on.
[00:37:19.240 --> 00:37:22.880]   But the truth is companies want to sometimes have control.
[00:37:22.880 --> 00:37:27.800]   They want to say, yeah, maybe there was like bad biases in my past training data, or maybe
[00:37:27.800 --> 00:37:31.800]   we changed the process and now we don't want to do it the way we used to do it.
[00:37:31.800 --> 00:37:33.040]   It's going to be a new process.
[00:37:33.040 --> 00:37:35.760]   Or like in this country, we have some regulations.
[00:37:35.760 --> 00:37:39.560]   So we need to first ask this other question that wasn't in the training data from that
[00:37:39.560 --> 00:37:40.560]   country and so on.
[00:37:40.560 --> 00:37:48.760]   So man, I'm surprised how often when it comes down to real business and products, you have
[00:37:48.760 --> 00:37:51.920]   to still have these rules in there.
[00:37:51.920 --> 00:37:57.480]   I'm curious actually, so you've gone through this transition from mainly academic to startup
[00:37:57.480 --> 00:38:03.200]   founder to big company, C-level executive.
[00:38:03.200 --> 00:38:07.240]   Is there any other surprises like that, like kind of seeing how businesses think about
[00:38:07.240 --> 00:38:10.480]   machine learning versus how academia thinks about it?
[00:38:10.480 --> 00:38:11.480]   Yeah.
[00:38:11.480 --> 00:38:15.400]   One thing I love is that I actually, I still dabble a little bit in all the other ones.
[00:38:15.400 --> 00:38:18.800]   And obviously we're still doing fundamental research now, but also a lot more product
[00:38:18.800 --> 00:38:19.800]   and stuff.
[00:38:19.800 --> 00:38:24.220]   But there are a lot of interesting different mindsets.
[00:38:24.220 --> 00:38:28.120]   In many ways, if you have a domain problem, this is actually something you see even in
[00:38:28.120 --> 00:38:29.120]   research.
[00:38:29.120 --> 00:38:34.000]   So if you work in just biology or you're just trying to solve one particular domain problem
[00:38:34.000 --> 00:38:37.760]   in a particular modality, like language revision, then it's rare.
[00:38:37.760 --> 00:38:45.080]   It's really hard for the people working on those applications to also find out new architectures.
[00:38:45.080 --> 00:38:46.080]   It's just a different mindset.
[00:38:46.080 --> 00:38:47.080]   You're trying to solve the problem.
[00:38:47.080 --> 00:38:55.600]   Like if you try to, for instance, help babies in the ICU, or you try to cure COVID or something,
[00:38:55.600 --> 00:38:56.600]   you don't care.
[00:38:56.600 --> 00:39:02.000]   If you can do it with naive Bayes or an SVM or some like latent derisory allocation or
[00:39:02.000 --> 00:39:07.780]   whatever the hot model used to be, the popular model at the time, it doesn't really matter.
[00:39:07.780 --> 00:39:12.880]   You solve cancer or some specific type of sub-cancer.
[00:39:12.880 --> 00:39:13.880]   And so it's interesting.
[00:39:13.880 --> 00:39:17.440]   You're still starting to throw the kitchen sink at applied problems.
[00:39:17.440 --> 00:39:21.000]   And that's sort of still true even for applied engineering teams.
[00:39:21.000 --> 00:39:25.680]   They say, by the end of this quarter and this sprint planning and so on, you got to have
[00:39:25.680 --> 00:39:31.800]   a solution that works on some level, whether it's like the absolute latest greatest and
[00:39:31.800 --> 00:39:35.200]   like really squeezes out those 2% that depends on the business model.
[00:39:35.200 --> 00:39:39.760]   Like for Google, it makes sense to spend a lot of time on AI because they have clearly
[00:39:39.760 --> 00:39:45.440]   certain AI metrics like recommender systems for advertisement and so on, where an improvement
[00:39:45.440 --> 00:39:48.600]   in an AI metric results in immediately more revenue.
[00:39:48.600 --> 00:39:54.480]   That isn't the case for every AI problem and solution and product out there in the B2B
[00:39:54.480 --> 00:39:55.480]   world.
[00:39:55.480 --> 00:40:00.680]   So if it kind of works, you make the same amount of money than if it works 5% better.
[00:40:00.680 --> 00:40:02.880]   What are the things that Salesforce cares about?
[00:40:02.880 --> 00:40:06.920]   What are the ML applications that are really important inside of your company?
[00:40:06.920 --> 00:40:09.360]   So there are a ton.
[00:40:09.360 --> 00:40:17.640]   There are roughly different groups such as packaged applications that you can sell as
[00:40:17.640 --> 00:40:22.520]   is like a chatbot application or opportunity or lead scoring.
[00:40:22.520 --> 00:40:27.800]   Some of these sometimes go into a second category, which is sort of quality of life and the user
[00:40:27.800 --> 00:40:31.200]   experience where you just kind of make the product a little bit better and we have a
[00:40:31.200 --> 00:40:32.200]   lot of those.
[00:40:32.200 --> 00:40:33.640]   Wait, sorry, what would that be like?
[00:40:33.640 --> 00:40:34.960]   Like make the product a little better?
[00:40:34.960 --> 00:40:41.520]   So for instance, you type in the name of a company to create a new lead object as a salesperson
[00:40:41.520 --> 00:40:43.840]   and it just like finds the company logo.
[00:40:43.840 --> 00:40:44.840]   Just like boom.
[00:40:44.840 --> 00:40:46.600]   And now it looks better in a nice table.
[00:40:46.600 --> 00:40:48.880]   This is like not a feature you could get money for.
[00:40:48.880 --> 00:40:50.560]   Or the search functionality.
[00:40:50.560 --> 00:40:55.520]   Search is like one of the most used features in most, you know, CM software.
[00:40:55.520 --> 00:41:01.080]   But making search like spending another billion dollars on improving search is like questionable.
[00:41:01.080 --> 00:41:04.640]   Like because you make the same amount of money, everybody assumes what searches just kind
[00:41:04.640 --> 00:41:09.120]   of work and you don't pay extra for it for the most part.
[00:41:09.120 --> 00:41:14.960]   And so then, yeah, so you have packaged applications where you clearly make a lot more money, like
[00:41:14.960 --> 00:41:16.520]   a recommendation engine in commerce.
[00:41:16.520 --> 00:41:20.880]   We're one of the largest e-commerce platforms in the United States, which many people don't
[00:41:20.880 --> 00:41:23.360]   know because nobody goes to Salesforce.com to buy their shoes.
[00:41:23.360 --> 00:41:26.720]   But you go to adidas.com, which runs on Salesforce.
[00:41:26.720 --> 00:41:32.680]   And so there you have recommendation engines as clearly like sort of almost an obvious
[00:41:32.680 --> 00:41:33.680]   kind of task.
[00:41:33.680 --> 00:41:38.240]   Everybody knows you should use recommendation engines in e-commerce.
[00:41:38.240 --> 00:41:42.480]   But those are sort of packaged applications that you can sell as is.
[00:41:42.480 --> 00:41:44.040]   Then you have these quality of life features.
[00:41:44.040 --> 00:41:48.520]   You have things like you want to improve your operational efficiency, like make recommendations
[00:41:48.520 --> 00:41:53.680]   for your own salespeople or learn how to work with your data centers more efficiently and
[00:41:53.680 --> 00:41:55.760]   things like that.
[00:41:55.760 --> 00:41:59.960]   And then you have, we have in the company also a lot of platforms.
[00:41:59.960 --> 00:42:06.080]   So where we enable our hundreds of thousands of customers to build their own AI applications
[00:42:06.080 --> 00:42:09.320]   with their own data without us interfering.
[00:42:09.320 --> 00:42:13.400]   And so there you also have interesting problems because you have to not just build one app,
[00:42:13.400 --> 00:42:19.720]   but you have to build an engine such that a lot of admins and with low or no code, you
[00:42:19.720 --> 00:42:25.040]   can create an AI application, some prediction model, some recommendation model, some OCR
[00:42:25.040 --> 00:42:30.880]   model to read in forms from some complex form, boom, directly into digital form, which is
[00:42:30.880 --> 00:42:34.720]   surprisingly still necessary a lot of times these days.
[00:42:34.720 --> 00:42:38.080]   So there's so many different applications.
[00:42:38.080 --> 00:42:40.320]   That's kind of why it's so exciting here.
[00:42:40.320 --> 00:42:44.640]   How do you even, like in your team, how do you pick what to work on?
[00:42:44.640 --> 00:42:48.240]   Is it driven by research interests?
[00:42:48.240 --> 00:42:50.320]   So I'm wearing these different hats.
[00:42:50.320 --> 00:42:57.120]   And so on the research side, we go mostly for impact on the AI community as a whole.
[00:42:57.120 --> 00:43:02.200]   And then, so that's one of our objective functions, just impact in AI research.
[00:43:02.200 --> 00:43:07.080]   Another one could be impact down the line eventually on products.
[00:43:07.080 --> 00:43:12.920]   So we have things that we work on in medicine where we don't currently work in medicine,
[00:43:12.920 --> 00:43:16.200]   but maybe down the line that could be used.
[00:43:16.200 --> 00:43:22.120]   We have things that we work on in the AI economist or progen where maybe eventually the world
[00:43:22.120 --> 00:43:24.440]   will improve, but it's not really clear.
[00:43:24.440 --> 00:43:30.160]   So there's sort of pure AI research impact on the world and all our stakeholders in the
[00:43:30.160 --> 00:43:31.840]   community and so on.
[00:43:31.840 --> 00:43:36.800]   Then on real products, so a lot of natural language processing research is surprisingly
[00:43:36.800 --> 00:43:38.120]   close.
[00:43:38.120 --> 00:43:44.880]   You can do some fundamental research in semantic parsing, learning how to really disambiguate
[00:43:44.880 --> 00:43:50.440]   a sentence into a query that could be used to get the answer from a database.
[00:43:50.440 --> 00:43:54.520]   And that is fundamental research, but it's also pretty applied and could be used for
[00:43:54.520 --> 00:44:00.080]   Tableau and a lot of other exciting areas inside the CRM where people need to find answers
[00:44:00.080 --> 00:44:01.080]   in the database.
[00:44:01.080 --> 00:44:04.000]   So that is kind of the two different worlds on the research side.
[00:44:04.000 --> 00:44:10.680]   And then on the product side and the large engineering groups, it's very customer driven
[00:44:10.680 --> 00:44:13.920]   and sometimes it's driven by what we think the future will be like.
[00:44:13.920 --> 00:44:21.200]   So we announced, for instance, at Dreamforce last year, a first agent over the phone.
[00:44:21.200 --> 00:44:26.080]   So like a kind of agent that you can just pick up the phone and have a natural conversation
[00:44:26.080 --> 00:44:27.080]   with.
[00:44:27.080 --> 00:44:30.880]   So Mark and I were on the keynote stage and showing sort of what that would look like.
[00:44:30.880 --> 00:44:36.440]   And so that is obviously something that maybe customers aren't even thinking yet about.
[00:44:36.440 --> 00:44:40.040]   They're like, because they're not sure it's even possible, but we're working on those
[00:44:40.040 --> 00:44:42.800]   kinds of things because we think it will be possible soon.
[00:44:42.800 --> 00:44:45.560]   And we're now making it possible.
[00:44:45.560 --> 00:44:46.560]   Cool.
[00:44:46.560 --> 00:44:47.560]   All right.
[00:44:47.560 --> 00:44:48.560]   Well, we're running out of time.
[00:44:48.560 --> 00:44:52.840]   We always end with two questions that I didn't warn you about, but I'm kind of curious what
[00:44:52.840 --> 00:44:53.840]   you'll say.
[00:44:53.840 --> 00:45:01.000]   So the first question is, what's something, an aspect of machine learning that you think
[00:45:01.000 --> 00:45:04.440]   practitioners are not paying enough attention to?
[00:45:04.440 --> 00:45:11.640]   I think now that AI has reached that deep impact level on the world, you really need
[00:45:11.640 --> 00:45:15.720]   to think about the biases in a holistic way.
[00:45:15.720 --> 00:45:24.560]   Like the systems, the people, the structures that are using AI for something.
[00:45:24.560 --> 00:45:26.960]   Are we thinking enough about the bias?
[00:45:26.960 --> 00:45:33.960]   And as AI has a bigger and bigger impact on people's lives, I think the bar needs to increase
[00:45:33.960 --> 00:45:34.960]   more and more.
[00:45:34.960 --> 00:45:40.840]   Like a loan application AI that decides who should be able to start a business and so
[00:45:40.840 --> 00:45:41.840]   on.
[00:45:41.840 --> 00:45:47.360]   So you really need to pay a lot of attention to the biases in the datasets, the biases
[00:45:47.360 --> 00:45:53.520]   of how those datasets are created by the people, the sort of hidden agendas and what the status
[00:45:53.520 --> 00:45:56.360]   quo is and so on.
[00:45:56.360 --> 00:46:04.560]   And how do you improve the world in the end versus entrenching it in the current system
[00:46:04.560 --> 00:46:08.280]   and just keeping the current system the way it is.
[00:46:08.280 --> 00:46:12.120]   And I think that's sort of something that a lot of practitioners still need to work
[00:46:12.120 --> 00:46:13.200]   on.
[00:46:13.200 --> 00:46:15.680]   And now also more researchers need to work on.
[00:46:15.680 --> 00:46:19.280]   Because even when we play around with like, "Oh, it's just a cute little artsy research
[00:46:19.280 --> 00:46:21.280]   project, this deep pixelization."
[00:46:21.280 --> 00:46:27.880]   It actually is, it turns out it's like another deeply rooted bias that is there and that
[00:46:27.880 --> 00:46:28.880]   gets exposed in that.
[00:46:28.880 --> 00:46:31.000]   And I think we should all work on that.
[00:46:31.000 --> 00:46:36.520]   Do you have any suggested reading material for people who want to get more educated on
[00:46:36.520 --> 00:46:38.040]   the topic where you'd point them?
[00:46:38.040 --> 00:46:39.120]   Yeah, for sure.
[00:46:39.120 --> 00:46:44.480]   I think Timnit Gebru right now is really one of the leaders in that area.
[00:46:44.480 --> 00:46:48.440]   And she has given this great tutorial at CVPR and the slides are online.
[00:46:48.440 --> 00:46:52.880]   There are a bunch of papers from a lot of other people.
[00:46:52.880 --> 00:46:57.800]   At our team, we also have Kathy Baxter and she's sort of looked at more of the applied
[00:46:57.800 --> 00:47:05.640]   side of AI a lot more, making sure that AI systems are explainable, transparent, that
[00:47:05.640 --> 00:47:11.040]   they have feedback loops in them, that people can give feedback to when an important decision
[00:47:11.040 --> 00:47:12.960]   about them was made in an automated fashion.
[00:47:12.960 --> 00:47:18.880]   They think it's wrong, that they're able to fix it and sort of escalate it to humans or
[00:47:18.880 --> 00:47:22.020]   improve that data.
[00:47:22.020 --> 00:47:26.280]   Making sure they're explainable, you actually understand how it came about that this decision
[00:47:26.280 --> 00:47:28.040]   was made about you and so on.
[00:47:28.040 --> 00:47:33.560]   So there are a couple of different things like making sure, even sounds kind of crazy,
[00:47:33.560 --> 00:47:38.560]   but I think we need to even think about human rights when it comes to AI applications that
[00:47:38.560 --> 00:47:39.560]   we work on.
[00:47:39.560 --> 00:47:44.320]   And so I think Kathy Baxter has a lot of materials online, interviews and materials.
[00:47:44.320 --> 00:47:49.560]   We have some trailheads also on Salesforce, the Salesforce learning platform on ethics
[00:47:49.560 --> 00:47:51.680]   and ethics in AI in particular.
[00:47:51.680 --> 00:47:59.000]   And then Timnit Gebru has a lot of great materials on research in AI and the systemic issues
[00:47:59.000 --> 00:48:01.000]   as well as sort of other concrete issues.
[00:48:01.000 --> 00:48:02.000]   Cool.
[00:48:02.000 --> 00:48:06.240]   Yeah, we'll put this in the notes and totally agree.
[00:48:06.240 --> 00:48:10.080]   The final question, so you're coming from kind of a research perspective, but you're
[00:48:10.080 --> 00:48:13.760]   at a company that does lots of applied machine learning.
[00:48:13.760 --> 00:48:21.000]   When you look at that path from taking a research thing to deployed inside the Salesforce product,
[00:48:21.000 --> 00:48:23.400]   what's the biggest challenge that you see in that process?
[00:48:23.400 --> 00:48:26.320]   Where do things get bogged down the most?
[00:48:26.320 --> 00:48:28.800]   Boy, it's interesting.
[00:48:28.800 --> 00:48:33.560]   I feel like we're finally really getting into a groove and we're getting a lot of features
[00:48:33.560 --> 00:48:35.600]   out much, much more quickly than we used to.
[00:48:35.600 --> 00:48:43.000]   And I think part of it is just that the two different sides of the pure researchers/research
[00:48:43.000 --> 00:48:50.120]   engineers/data scientists/data engineers, they have a certain way they see of where's
[00:48:50.120 --> 00:48:53.640]   the complexity of deploying an actual AI product.
[00:48:53.640 --> 00:49:00.320]   And then you have engineers and the truth is though that somewhere between 5 to 20%
[00:49:00.320 --> 00:49:02.840]   of an AI product is actual AI.
[00:49:02.840 --> 00:49:09.080]   And then somewhere between 95 to 80% is just relatively standard, but still very hard software
[00:49:09.080 --> 00:49:10.080]   engineering.
[00:49:10.080 --> 00:49:18.760]   And everybody can nowadays quickly hack together a quick TensorFlow image classifier, right?
[00:49:18.760 --> 00:49:23.080]   It's like, oh, and you feel like after 10 minutes, you're an AI expert and so cool and
[00:49:23.080 --> 00:49:26.040]   you're super smart and you know AI now and so on.
[00:49:26.040 --> 00:49:30.480]   But then when you actually want to deploy that in a large context, now you have load
[00:49:30.480 --> 00:49:35.440]   balancing, you have security, you have privacy, and you have all these issues.
[00:49:35.440 --> 00:49:39.760]   Now somebody from GDPR from Europe says, "I want you to delete this picture.
[00:49:39.760 --> 00:49:40.880]   Now you need to retrain it."
[00:49:40.880 --> 00:49:44.520]   If that happens every day, are you retraining a whole huge model every day?
[00:49:44.520 --> 00:49:49.040]   Because somebody you're asking to take their data out of the thing that eventually found
[00:49:49.040 --> 00:49:50.040]   in your classifier.
[00:49:50.040 --> 00:49:52.360]   How do you update the classifier continuously?
[00:49:52.360 --> 00:49:55.880]   How do you make sure that as you update the classifier, if you've had something like FDA
[00:49:55.880 --> 00:50:02.200]   approval or HIPAA compliance, how do you make sure that the new classifier is still compliant
[00:50:02.200 --> 00:50:04.320]   with all the various regulations you have?
[00:50:04.320 --> 00:50:10.160]   So there's so much complexity on the engineering and productionizing of AI.
[00:50:10.160 --> 00:50:15.200]   And that is what a lot of people who are super deep AI experts often underestimate.
[00:50:15.200 --> 00:50:16.200]   Cool.
[00:50:16.200 --> 00:50:18.480]   Well, great answer and great talking to you, Richard.
[00:50:18.480 --> 00:50:19.920]   Thank you so much for doing this.
[00:50:19.920 --> 00:50:20.920]   Pleasure.
[00:50:20.920 --> 00:50:21.920]   Great questions.
[00:50:21.920 --> 00:50:25.440]   I'm going to geek out a little bit and go deep into some of these papers.
[00:50:25.440 --> 00:50:26.440]   Totally.
[00:50:26.440 --> 00:50:27.440]   Yeah.
[00:50:27.440 --> 00:50:28.440]   Thanks so much.
[00:50:28.440 --> 00:50:32.240]   When we first started making these videos, we didn't know if anyone would be interested
[00:50:32.240 --> 00:50:35.320]   or want to see them, but we made them for fun.
[00:50:35.320 --> 00:50:38.280]   And we started off by making videos that would teach people.
[00:50:38.280 --> 00:50:42.240]   And now we get these great interviews with real industry practitioners.
[00:50:42.240 --> 00:50:45.920]   And I love making this available to the whole world so everyone can watch these things for
[00:50:45.920 --> 00:50:46.920]   free.
[00:50:46.920 --> 00:50:49.280]   The more feedback you give us, the better stuff we can produce.
[00:50:49.280 --> 00:50:52.200]   So please subscribe, leave a comment, engage with us.
[00:50:52.200 --> 00:50:53.400]   We really appreciate it.

